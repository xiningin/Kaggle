{"cell_type":{"dd92fafc":"code","e2ff261c":"code","a7f89a90":"code","c95006b1":"code","156414e6":"code","34b822fa":"code","69048833":"code","6acdb9ce":"code","52fccfb3":"code","fa53d710":"code","d8be34a5":"code","04346bfd":"code","1a4d7f97":"code","ba0628a0":"code","e3c5fc72":"code","3e9b7a38":"code","177ccda9":"code","f7ad0f76":"code","5099978e":"code","2b68e483":"code","ff27f5c7":"code","9b265167":"code","582287d3":"code","86545a2d":"code","04ffb67e":"code","312aa062":"code","987aa26c":"code","67cb02bb":"code","30d18027":"code","d050bb71":"code","8d9c8ed5":"code","62ae465e":"code","f96fe4c4":"code","a0ad7797":"code","a541fef4":"code","e88d9b70":"code","6e0b92d1":"code","2f4294ba":"code","9b9b8dae":"code","89ce44f3":"code","f84fd2de":"code","5008d67d":"code","e204c348":"code","9e591ff2":"code","7bf830bf":"code","f6a38b9b":"code","f4da2fa6":"code","cdf42bd9":"code","ec9543d8":"code","513ec05f":"code","177ae41c":"code","7f839d11":"code","d96ade6d":"code","6ef46cf1":"code","d06a61a1":"code","3c5e0fdf":"code","07034c30":"code","8aca502c":"code","318a7b06":"code","4a4669b6":"code","6c338820":"code","5513be39":"code","6dd6063b":"code","e4e0a41b":"code","27b8aaa0":"code","004df66f":"code","e491d9dd":"code","4ab8c395":"code","d51f5039":"code","a65dc63a":"code","99978961":"code","72eabe63":"code","26a6d3b6":"code","72257f6b":"code","fefba0c6":"code","a13fa281":"code","1f27c686":"code","279fd1fe":"code","4e687bec":"code","d16f296a":"code","2ed64dee":"code","dc07ea6f":"code","e6aafa13":"code","7f2053f7":"code","ec1abfdf":"code","3f9e9a26":"code","61c71478":"code","37497405":"code","2ff7a39c":"code","652657b1":"code","229af551":"code","4b6db4a8":"code","89900840":"code","cf5dd047":"markdown","3ae1b3ae":"markdown","2d44b803":"markdown","95ad5f07":"markdown","c0b61040":"markdown"},"source":{"dd92fafc":"import numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\n \nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.decomposition import PCA\n \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n \nimport warnings\nwarnings.filterwarnings('ignore')\n\ndata_dir = '..\/input\/lish-moa\/'\n\ntrain_features = pd.read_csv(data_dir + 'train_features.csv')\ntrain_targets_scored = pd.read_csv(data_dir + 'train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv(data_dir + 'train_targets_nonscored.csv')\ntrain_drug = pd.read_csv(data_dir + 'train_drug.csv')\ntest_features = pd.read_csv(data_dir + 'test_features.csv')\nsample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n\nprint('train_features: {}'.format(train_features.shape))\nprint('train_targets_scored: {}'.format(train_targets_scored.shape))\nprint('train_targets_nonscored: {}'.format(train_targets_nonscored.shape))\nprint('train_drug: {}'.format(train_drug.shape))\nprint('test_features: {}'.format(test_features.shape))\nprint('sample_submission: {}'.format(sample_submission.shape))\n","e2ff261c":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\n\nprint('GENES: {}'.format(GENES[:10]))\nprint('CELLS: {}'.format(CELLS[:10]))","a7f89a90":"!pip install ..\/input\/iterativestratification\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","c95006b1":"for col in (GENES + CELLS):\n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","156414e6":"n_comp_GENES = 500 ## pca features\nn_comp_CELLS = 50 ## pca features","34b822fa":"g_pca = PCA(n_components=n_comp_GENES, random_state=42)\ntrain_features = pd.concat((train_features, pd.DataFrame(g_pca.fit_transform(train_features[GENES]), columns=[f'pca_G-{i}' for i in range(n_comp_GENES)])), axis=1)\ntest_features = pd.concat((test_features, pd.DataFrame(g_pca.transform(test_features[GENES]), columns=[f'pca_G-{i}' for i in range(n_comp_GENES)])), axis=1)\n\nc_pca = PCA(n_components=n_comp_CELLS, random_state=42)\ntrain_features = pd.concat((train_features, pd.DataFrame(c_pca.fit_transform(train_features[CELLS]), columns=[f'pca_C-{i}' for i in range(n_comp_CELLS)])), axis=1)\ntest_features = pd.concat((test_features, pd.DataFrame(c_pca.transform(test_features[CELLS]), columns=[f'pca_C-{i}' for i in range(n_comp_CELLS)])), axis=1)","69048833":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything(seed=42)","6acdb9ce":"print('train_features: {}'.format(train_features.shape))\nprint('test_features: {}'.format(test_features.shape))","52fccfb3":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train.merge(train_targets_nonscored, on='sig_id')\ntrain = train.merge(train_drug, on='sig_id')\ntrain = train[train['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)","fa53d710":"train = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","d8be34a5":"target_cols = [x for x in train_targets_scored.columns if x != 'sig_id']\naux_target_cols = [x for x in train_targets_nonscored.columns if x != 'sig_id']\nall_target_cols = target_cols + aux_target_cols\n\nnum_targets = len(target_cols)\nnum_aux_targets = len(aux_target_cols)\nnum_all_targets = len(all_target_cols)\n\nprint('num_targets: {}'.format(num_targets))\nprint('num_aux_targets: {}'.format(num_aux_targets))\nprint('num_all_targets: {}'.format(num_all_targets))","04346bfd":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)\n        }\n        \n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n\n        return dct\n","1a4d7f97":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        final_loss += loss.item()\n        \n    final_loss \/= len(dataloader)\n    return final_loss\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n\n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    return preds\n","ba0628a0":"import torch\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n            \n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","e3c5fc72":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets):\n        super(Model, self).__init__()\n        self.hidden_size = [1500, 1250, 1000, 750]\n        self.dropout_value = [0.5, 0.35, 0.3, 0.25]\n\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.Linear(num_features, self.hidden_size[0])\n        \n        self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n        self.dropout2 = nn.Dropout(self.dropout_value[0])\n        self.dense2 = nn.Linear(self.hidden_size[0], self.hidden_size[1])\n\n        self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n        self.dropout3 = nn.Dropout(self.dropout_value[1])\n        self.dense3 = nn.Linear(self.hidden_size[1], self.hidden_size[2])\n\n        self.batch_norm4 = nn.BatchNorm1d(self.hidden_size[2])\n        self.dropout4 = nn.Dropout(self.dropout_value[2])\n        self.dense4 = nn.Linear(self.hidden_size[2], self.hidden_size[3])\n\n        self.batch_norm5 = nn.BatchNorm1d(self.hidden_size[3])\n        self.dropout5 = nn.Dropout(self.dropout_value[3])\n        self.dense5 = nn.utils.weight_norm(nn.Linear(self.hidden_size[3], num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = F.leaky_relu(self.dense3(x))\n\n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        x = F.leaky_relu(self.dense4(x))\n\n        x = self.batch_norm5(x)\n        x = self.dropout5(x)\n        x = self.dense5(x)\n        return x\n    \nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing \/ (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n            \n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))    \n","3e9b7a38":"class FineTuneScheduler:\n    def __init__(self, epochs):\n        self.epochs = epochs\n        self.epochs_per_step = 0\n        self.frozen_layers = []\n\n    def copy_without_top(self, model, num_features, num_targets, num_targets_new):\n        self.frozen_layers = []\n\n        model_new = Model(num_features, num_targets)\n        model_new.load_state_dict(model.state_dict())\n\n        # Freeze all weights\n        for name, param in model_new.named_parameters():\n            layer_index = name.split('.')[0][-1]\n\n            if layer_index == 5:\n                continue\n\n            param.requires_grad = False\n\n            # Save frozen layer names\n            if layer_index not in self.frozen_layers:\n                self.frozen_layers.append(layer_index)\n\n        self.epochs_per_step = self.epochs \/\/ len(self.frozen_layers)\n\n        # Replace the top layers with another ones\n        model_new.batch_norm5 = nn.BatchNorm1d(model_new.hidden_size[3])\n        model_new.dropout5 = nn.Dropout(model_new.dropout_value[3])\n        model_new.dense5 = nn.utils.weight_norm(nn.Linear(model_new.hidden_size[-1], num_targets_new))\n        model_new.to(DEVICE)\n        return model_new\n\n    def step(self, epoch, model):\n        if len(self.frozen_layers) == 0:\n            return\n\n        if epoch % self.epochs_per_step == 0:\n            last_frozen_index = self.frozen_layers[-1]\n            \n            # Unfreeze parameters of the last frozen layer\n            for name, param in model.named_parameters():\n                layer_index = name.split('.')[0][-1]\n\n                if layer_index == last_frozen_index:\n                    param.requires_grad = True\n\n            del self.frozen_layers[-1]  # Remove the last layer as unfrozen\n","177ccda9":"def process_data(data):\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data","f7ad0f76":"feature_cols = [c for c in process_data(train).columns if c not in all_target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold', 'sig_id', 'drug_id']]\nnum_features = len(feature_cols)\nnum_features","5099978e":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 25\nBATCH_SIZE = 128\n\nWEIGHT_DECAY = {'ALL_TARGETS': 1e-5, 'SCORED_ONLY': 3e-6}\nMAX_LR = {'ALL_TARGETS': 1e-2, 'SCORED_ONLY': 3e-3}\nDIV_FACTOR = {'ALL_TARGETS': 1e3, 'SCORED_ONLY': 1e2}\nPCT_START = 0.1","2b68e483":"model = Model(num_features, num_all_targets)\n","ff27f5c7":"from sklearn.model_selection import KFold\n\ndef make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH):\n    vc = train.drug_id.value_counts()\n    vc1 = vc.loc[vc <= DRUG_THRESH].index.sort_values()\n    vc2 = vc.loc[vc > DRUG_THRESH].index.sort_values()\n\n    for seed_id in range(SEEDS):\n        kfold_col = 'kfold_{}'.format(seed_id)\n        \n        # STRATIFY DRUGS 18X OR LESS\n        dct1 = {}\n        dct2 = {}\n\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = train.groupby('drug_id')[target_cols].mean().loc[vc1]\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.index[idxV].values}\n            dct1.update(dd)\n\n        # STRATIFY DRUGS MORE THAN 18X\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = train.loc[train.drug_id.isin(vc2)].reset_index(drop=True)\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.sig_id[idxV].values}\n            dct2.update(dd)\n\n        # ASSIGN FOLDS\n        train[kfold_col] = train.drug_id.map(dct1)\n        train.loc[train[kfold_col].isna(), kfold_col] = train.loc[train[kfold_col].isna(), 'sig_id'].map(dct2)\n        train[kfold_col] = train[kfold_col].astype('int8')\n        \n    return train\n\nSEEDS = 7\nNFOLDS = 7\nDRUG_THRESH = 18\n\ntrain = make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH)\ntrain.head()\n","9b265167":"model_path = '..\/input\/models-approach-v2\/'","582287d3":"def run_inference(fold_id, seed_id):\n    test_ = process_data(test)\n\n    # Load the fine-tuned model with the best loss\n    model = Model(num_features, num_targets)\n    model.load_state_dict(torch.load(f\"{model_path}\/SCORED_ONLY_FOLD{fold_id}_{seed_id}.pth\"))\n    model.to(DEVICE)\n\n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    predictions = np.zeros((len(test_), num_targets))\n    predictions = inference_fn(model, testloader, DEVICE)\n    return predictions\n","86545a2d":"from time import time\n\n# Averaging on multiple SEEDS\nSEED = [0, 1, 2, 3, 4, 5, 6]\nfold =[0,1,2,3,4,5,6]\npredictions = np.zeros((len(test), len(target_cols)))\n\ntime_begin = time()\nfor fold_id in fold:\n    for seed_id in SEED:\n        predictions_ = run_inference(fold_id, seed_id)\n        predictions += predictions_ \/ len(SEED)\n\ntime_diff = time() - time_begin\n\ntest[target_cols] = predictions","04ffb67e":"test[target_cols]=test[target_cols]\/7","312aa062":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)","987aa26c":"sub.shape","67cb02bb":"sub.head(5)","30d18027":"train_features = pd.read_csv(data_dir + 'train_features.csv')\ntrain_targets_scored = pd.read_csv(data_dir + 'train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv(data_dir + 'train_targets_nonscored.csv')\ntrain_drug = pd.read_csv(data_dir + 'train_drug.csv')\ntest_features = pd.read_csv(data_dir + 'test_features.csv')\nsample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n\nprint('train_features: {}'.format(train_features.shape))\nprint('train_targets_scored: {}'.format(train_targets_scored.shape))\nprint('train_targets_nonscored: {}'.format(train_targets_nonscored.shape))\nprint('train_drug: {}'.format(train_drug.shape))\nprint('test_features: {}'.format(test_features.shape))\nprint('sample_submission: {}'.format(sample_submission.shape))","d050bb71":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\n\nprint('GENES: {}'.format(GENES[:10]))\nprint('CELLS: {}'.format(CELLS[:10]))","8d9c8ed5":"for col in (GENES + CELLS):\n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","62ae465e":"n_comp_GENES = 100 ## pca features\nn_comp_CELLS = 50 ## pca features","f96fe4c4":"g_pca = PCA(n_components=n_comp_GENES, random_state=42)\ntrain_features = pd.concat((train_features, pd.DataFrame(g_pca.fit_transform(train_features[GENES]), columns=[f'pca_G-{i}' for i in range(n_comp_GENES)])), axis=1)\ntest_features = pd.concat((test_features, pd.DataFrame(g_pca.transform(test_features[GENES]), columns=[f'pca_G-{i}' for i in range(n_comp_GENES)])), axis=1)\n\nc_pca = PCA(n_components=n_comp_CELLS, random_state=42)\ntrain_features = pd.concat((train_features, pd.DataFrame(c_pca.fit_transform(train_features[CELLS]), columns=[f'pca_C-{i}' for i in range(n_comp_CELLS)])), axis=1)\ntest_features = pd.concat((test_features, pd.DataFrame(c_pca.transform(test_features[CELLS]), columns=[f'pca_C-{i}' for i in range(n_comp_CELLS)])), axis=1)","a0ad7797":"from sklearn.feature_selection import VarianceThreshold\n\nvar_thresh = VarianceThreshold(0.8)\ndata = train_features.append(test_features)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\nprint('train_features: {}'.format(train_features.shape))\nprint('test_features: {}'.format(test_features.shape))","a541fef4":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train.merge(train_targets_nonscored, on='sig_id')\ntrain = train.merge(train_drug, on='sig_id')\ntrain = train[train['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)","e88d9b70":"train = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","6e0b92d1":"target_cols = [x for x in train_targets_scored.columns if x != 'sig_id']\naux_target_cols = [x for x in train_targets_nonscored.columns if x != 'sig_id']\nall_target_cols = target_cols + aux_target_cols\n\nnum_targets = len(target_cols)\nnum_aux_targets = len(aux_target_cols)\nnum_all_targets = len(all_target_cols)\n\nprint('num_targets: {}'.format(num_targets))\nprint('num_aux_targets: {}'.format(num_aux_targets))\nprint('num_all_targets: {}'.format(num_all_targets))","2f4294ba":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)\n        }\n        \n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n\n        return dct\n","9b9b8dae":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        final_loss += loss.item()\n        \n    final_loss \/= len(dataloader)\n    return final_loss\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n\n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    return preds\n","89ce44f3":"import torch\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n            \n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss\n","f84fd2de":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets):\n        super(Model, self).__init__()\n        self.hidden_size = [1500, 1250, 1000, 750]\n        self.dropout_value = [0.5, 0.35, 0.3, 0.25]\n\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.Linear(num_features, self.hidden_size[0])\n        \n        self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n        self.dropout2 = nn.Dropout(self.dropout_value[0])\n        self.dense2 = nn.Linear(self.hidden_size[0], self.hidden_size[1])\n\n        self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n        self.dropout3 = nn.Dropout(self.dropout_value[1])\n        self.dense3 = nn.Linear(self.hidden_size[1], self.hidden_size[2])\n\n        self.batch_norm4 = nn.BatchNorm1d(self.hidden_size[2])\n        self.dropout4 = nn.Dropout(self.dropout_value[2])\n        self.dense4 = nn.Linear(self.hidden_size[2], self.hidden_size[3])\n\n        self.batch_norm5 = nn.BatchNorm1d(self.hidden_size[3])\n        self.dropout5 = nn.Dropout(self.dropout_value[3])\n        self.dense5 = nn.utils.weight_norm(nn.Linear(self.hidden_size[3], num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = F.leaky_relu(self.dense3(x))\n\n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        x = F.leaky_relu(self.dense4(x))\n\n        x = self.batch_norm5(x)\n        x = self.dropout5(x)\n        x = self.dense5(x)\n        return x\n    \nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing \/ (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n            \n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))    \n","5008d67d":"class FineTuneScheduler:\n    def __init__(self, epochs):\n        self.epochs = epochs\n        self.epochs_per_step = 0\n        self.frozen_layers = []\n\n    def copy_without_top(self, model, num_features, num_targets, num_targets_new):\n        self.frozen_layers = []\n\n        model_new = Model(num_features, num_targets)\n        model_new.load_state_dict(model.state_dict())\n\n        # Freeze all weights\n        for name, param in model_new.named_parameters():\n            layer_index = name.split('.')[0][-1]\n\n            if layer_index == 5:\n                continue\n\n            param.requires_grad = False\n\n            # Save frozen layer names\n            if layer_index not in self.frozen_layers:\n                self.frozen_layers.append(layer_index)\n\n        self.epochs_per_step = self.epochs \/\/ len(self.frozen_layers)\n\n        # Replace the top layers with another ones\n        model_new.batch_norm5 = nn.BatchNorm1d(model_new.hidden_size[3])\n        model_new.dropout5 = nn.Dropout(model_new.dropout_value[3])\n        model_new.dense5 = nn.utils.weight_norm(nn.Linear(model_new.hidden_size[-1], num_targets_new))\n        model_new.to(DEVICE)\n        return model_new\n\n    def step(self, epoch, model):\n        if len(self.frozen_layers) == 0:\n            return\n\n        if epoch % self.epochs_per_step == 0:\n            last_frozen_index = self.frozen_layers[-1]\n            \n            # Unfreeze parameters of the last frozen layer\n            for name, param in model.named_parameters():\n                layer_index = name.split('.')[0][-1]\n\n                if layer_index == last_frozen_index:\n                    param.requires_grad = True\n\n            del self.frozen_layers[-1]  # Remove the last layer as unfrozen","e204c348":"def process_data(data):\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data","9e591ff2":"feature_cols = [c for c in process_data(train).columns if c not in all_target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold', 'sig_id', 'drug_id']]\nnum_features = len(feature_cols)\nnum_features","7bf830bf":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 24\nBATCH_SIZE = 128\n\nWEIGHT_DECAY = {'ALL_TARGETS': 1e-5, 'SCORED_ONLY': 3e-6}\nMAX_LR = {'ALL_TARGETS': 1e-2, 'SCORED_ONLY': 3e-3}\nDIV_FACTOR = {'ALL_TARGETS': 1e3, 'SCORED_ONLY': 1e2}\nPCT_START = 0.1","f6a38b9b":"model = Model(num_features, num_all_targets)\n","f4da2fa6":"from sklearn.model_selection import KFold\n\ndef make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH):\n    vc = train.drug_id.value_counts()\n    vc1 = vc.loc[vc <= DRUG_THRESH].index.sort_values()\n    vc2 = vc.loc[vc > DRUG_THRESH].index.sort_values()\n\n    for seed_id in range(SEEDS):\n        kfold_col = 'kfold_{}'.format(seed_id)\n        \n        # STRATIFY DRUGS 18X OR LESS\n        dct1 = {}\n        dct2 = {}\n\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = train.groupby('drug_id')[target_cols].mean().loc[vc1]\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.index[idxV].values}\n            dct1.update(dd)\n\n        # STRATIFY DRUGS MORE THAN 18X\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = train.loc[train.drug_id.isin(vc2)].reset_index(drop=True)\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.sig_id[idxV].values}\n            dct2.update(dd)\n\n        # ASSIGN FOLDS\n        train[kfold_col] = train.drug_id.map(dct1)\n        train.loc[train[kfold_col].isna(), kfold_col] = train.loc[train[kfold_col].isna(), 'sig_id'].map(dct2)\n        train[kfold_col] = train[kfold_col].astype('int8')\n        \n    return train\n\nSEEDS = 7\nNFOLDS = 7\nDRUG_THRESH = 18\n\ntrain = make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH)\ntrain.head()","cdf42bd9":"model_path = '..\/input\/pytorch-transfer-learning-with-k-folds-by-drug-ids\/'","ec9543d8":"def run_inference(fold_id, seed_id):\n    test_ = process_data(test)\n\n    # Load the fine-tuned model with the best loss\n    model = Model(num_features, num_targets)\n    model.load_state_dict(torch.load(f\"{model_path}\/SCORED_ONLY_SEED_{seed_id}_FOLD{fold_id}_.pth\"))\n\n    model.to(DEVICE)\n\n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    predictions = np.zeros((len(test_), num_targets))\n    predictions = inference_fn(model, testloader, DEVICE)\n    return predictions","513ec05f":"from time import time\n\n# Averaging on multiple SEEDS\nSEED = [0, 1, 2, 3, 4, 5, 6]\nfold =[0,1,2,3,4,5,6]\npredictions = np.zeros((len(test), len(target_cols)))\n\ntime_begin = time()\nfor fold_id in fold:\n    for seed_id in SEED:\n        predictions_ = run_inference(fold_id, seed_id)\n        predictions += predictions_ \/ len(SEED)\n\ntime_diff = time() - time_begin\n\ntest[target_cols] = predictions","177ae41c":"test[target_cols]=test[target_cols]\/7\n","7f839d11":"sub2 = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)","d96ade6d":"sub2.head(5)","6ef46cf1":"# TabNet\n!pip install --no-index --find-links \/kaggle\/input\/pytorchtabnet\/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet","d06a61a1":"import sys\nsys.path.append('..\/input\/iterativestratification')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","3c5e0fdf":"### General ###\nimport os\nimport copy\nimport tqdm\nimport pickle\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsys.path.append(\"..\/input\/rank-gauss\")\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = '1'\n\n### Data Wrangling ###\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\n### Machine Learning ###\nfrom sklearn import preprocessing\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nfrom pickle import load,dump\n\n### Deep Learning ###\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nfrom torch.nn import functional as F\nfrom torch.nn.modules.loss import _WeightedLoss\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n# Tabnet \nfrom pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n### Make prettier the prints ###\nfrom colorama import Fore\nc_ = Fore.CYAN\nm_ = Fore.MAGENTA\nr_ = Fore.RED\nb_ = Fore.BLUE\ny_ = Fore.YELLOW\ng_ = Fore.GREEN\n","07034c30":"from sklearn.preprocessing import QuantileTransformer\ntrain_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\ndf = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","8aca502c":"train_features2=train_features.copy()\ntest_features2=test_features.copy()","318a7b06":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","4a4669b6":"qt = QuantileTransformer(n_quantiles=100,random_state=42,output_distribution='normal')\ntrain_features[GENES+CELLS] = qt.fit_transform(train_features[GENES+CELLS])\ntest_features[GENES+CELLS] = qt.transform(test_features[GENES+CELLS])","6c338820":"n_comp_GENES=80\nn_comp_CELLS=10","5513be39":"g_pca = PCA(n_components=n_comp_GENES, random_state=42)\ntrain_features = pd.concat((train_features, pd.DataFrame(g_pca.fit_transform(train_features[GENES]), columns=[f'pca_G-{i}' for i in range(n_comp_GENES)])), axis=1)\ntest_features = pd.concat((test_features, pd.DataFrame(g_pca.transform(test_features[GENES]), columns=[f'pca_G-{i}' for i in range(n_comp_GENES)])), axis=1)\n\nc_pca = PCA(n_components=n_comp_CELLS, random_state=42)\ntrain_features = pd.concat((train_features, pd.DataFrame(c_pca.fit_transform(train_features[CELLS]), columns=[f'pca_C-{i}' for i in range(n_comp_CELLS)])), axis=1)\ntest_features = pd.concat((test_features, pd.DataFrame(c_pca.transform(test_features[CELLS]), columns=[f'pca_C-{i}' for i in range(n_comp_CELLS)])), axis=1)","6dd6063b":"from sklearn.feature_selection import VarianceThreshold\n\nc_n = [f for f in list(train_features.columns) if f not in ['sig_id', 'cp_type', 'cp_time', 'cp_dose']]\nmask = (train_features[c_n].var() >= 0.85).values\ntmp = train_features[c_n].loc[:, mask]\ntrain_features = pd.concat([train_features[['sig_id', 'cp_type', 'cp_time', 'cp_dose']], tmp], axis=1)\ntmp = test_features[c_n].loc[:, mask]\ntest_features = pd.concat([test_features[['sig_id', 'cp_type', 'cp_time', 'cp_dose']], tmp], axis=1)","e4e0a41b":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntarget = train[train_targets_scored.columns]","27b8aaa0":"train = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","004df66f":"target_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n","e491d9dd":"target=target[target_cols]\n","4ab8c395":"train = pd.get_dummies(train, columns=['cp_time','cp_dose'])\ntest_ = pd.get_dummies(test, columns=['cp_time','cp_dose'])","d51f5039":"feature_cols = [c for c in train.columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['sig_id']]","a65dc63a":"train = train[feature_cols]\ntest = test_[feature_cols]","99978961":"X_test = test.values\n","72eabe63":"class LogitsLogLoss(Metric):\n\n    def __init__(self):\n        self._name = \"logits_ll\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_pred):\n        logits = 1 \/ (1 + np.exp(-y_pred))\n        aux = (1 - y_true) * np.log(1 - logits + 5e-5) + y_true * np.log(logits + 5e-5)\n        return np.mean(-aux)\n","26a6d3b6":"seed = 42\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\nset_seed(seed)","72257f6b":"MAX_EPOCH = 200\n\ntabnet_params = dict(\n    n_d = 32,\n    n_a = 32,\n    n_steps = 1,\n    gamma = 1.3,\n    lambda_sparse = 0,\n    optimizer_fn = optim.Adam,\n    optimizer_params = dict(lr = 2e-2, weight_decay = 1e-5),\n    mask_type = \"entmax\",\n    scheduler_params = dict(mode = \"min\", patience = 5, min_lr = 1e-5, factor = 0.9),\n    scheduler_fn = ReduceLROnPlateau,\n    seed = seed,\n    verbose = 10\n)\n","fefba0c6":"test_cv_preds = []\n\nNB_SPLITS = 10\nmskf = MultilabelStratifiedKFold(n_splits = NB_SPLITS, random_state = 0, shuffle = True)\nSEED = [20,21,22]\nfor s in SEED:\n    tabnet_params['seed'] = s\n    for fold_nb, (train_idx, val_idx) in enumerate(mskf.split(train, target)):\n        \n        model = TabNetRegressor()\n        ### Predict on test ###\n        model.load_model(f\"..\/input\/moa-tabnet-correct\/TabNet_seed_{s}_fold_{fold_nb+1}.zip\")\n        preds_test = model.predict(X_test)\n        test_cv_preds.append(1 \/ (1 + np.exp(-preds_test)))\n\ntest_preds_all = np.stack(test_cv_preds)\n","a13fa281":"all_feat = [col for col in df.columns if col not in [\"sig_id\"]]\n# To obtain the same lenght of test_preds_all and submission\ntest = pd.read_csv(\"..\/input\/lish-moa\/test_features.csv\")\nsig_id = test[test[\"cp_type\"] != \"ctl_vehicle\"].sig_id.reset_index(drop = True)\ntmp = pd.DataFrame(test_preds_all.mean(axis = 0), columns = all_feat)\ntmp[\"sig_id\"] = sig_id\n\nsub3 = pd.merge(test[[\"sig_id\"]], tmp, on = \"sig_id\", how = \"left\")\nsub3.fillna(0, inplace = True)\nsub3.head(5)","1f27c686":"sub","279fd1fe":"sub2","4e687bec":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA\nfrom tensorflow.keras import layers,regularizers,Sequential,Model,backend,callbacks,optimizers,metrics,losses\nimport tensorflow as tf\nimport sys\nimport json\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","d16f296a":"train_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\nnon_ctl_idx = train_features.loc[train_features['cp_type']!='ctl_vehicle'].index.to_list()\ntrain_features = train_features.drop(['sig_id','cp_type','cp_dose','cp_time'],axis=1)\ntrain_targets_scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_scored = train_targets_scored.drop('sig_id',axis=1)\nlabels_train = train_targets_scored.values\n\n# Drop training data with ctl vehicle\n\ntrain_features = train_features.iloc[non_ctl_idx]\nlabels_train = labels_train[non_ctl_idx]\n\n# Import test data\n\ntest_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\ntest_features = test_features.drop(['sig_id','cp_dose','cp_time'],axis=1)\n\n# Import predictors from public kernel\n\njson_file_path = '..\/input\/t-test-pca-rfe-logistic-regression\/main_predictors.json'\n\nwith open(json_file_path, 'r') as j:\n    predictors = json.loads(j.read())\n    predictors = predictors['start_predictors']\n","2ed64dee":"# Create g-mean, c-mean, genes_pca (2 components), cells_pca (all components)\n\ncs = train_features.columns.str.contains('c-')\ngs = train_features.columns.str.contains('g-')\n\ndef preprocessor(train,test):\n    \n    # PCA\n    \n    n_gs = 10 # No of PCA comps to include\n    n_cs = 90 # No of PCA comps to include\n    \n    pca_cs = PCA(n_components = n_cs)\n    pca_gs = PCA(n_components = n_gs)\n\n    train_pca_gs = pca_gs.fit_transform(train[:,gs])\n    train_pca_cs = pca_cs.fit_transform(train[:,cs])\n    test_pca_gs = pca_gs.transform(test[:,gs])\n    test_pca_cs = pca_cs.transform(test[:,cs])\n    \n    # c-mean, g-mean\n    \n    train_c_mean = train[:,cs].mean(axis=1)\n    test_c_mean = test[:,cs].mean(axis=1)\n    train_g_mean = train[:,gs].mean(axis=1)\n    test_g_mean = test[:,gs].mean(axis=1)\n    \n    # Append Features\n    \n    train = np.concatenate((train,train_pca_gs,train_pca_cs,train_c_mean[:,np.newaxis]\n                            ,train_g_mean[:,np.newaxis]),axis=1)\n    test = np.concatenate((test,test_pca_gs,test_pca_cs,test_c_mean[:,np.newaxis],\n                           test_g_mean[:,np.newaxis]),axis=1)\n    \n    # Scaler for numerical values\n\n    # Scale train data\n    scaler = preprocessing.StandardScaler()\n\n    train = scaler.fit_transform(train)\n\n    # Scale Test data\n    test = scaler.transform(test)\n    \n    return train, test","dc07ea6f":"n_labels = train_targets_scored.shape[1]\nn_train = train_features.shape[0]\nn_test = test_features.shape[0]\n\n\n# Prediction Clipping Thresholds\n\np_min = 0.0005\np_max = 0.9995\n\n# OOF Evaluation Metric with clipping and no label smoothing\n\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -backend.mean(y_true*backend.log(y_pred) + (1-y_true)*backend.log(1-y_pred))\n","e6aafa13":"def build_model(n_features, n_features_2, n_labels, label_smoothing = 0.0005):    \n    input_1 = layers.Input(shape = (n_features,), name = 'Input1')\n    input_2 = layers.Input(shape = (n_features_2,), name = 'Input2')\n\n    head_1 = Sequential([\n        layers.BatchNormalization(),\n        layers.Dropout(0.2),\n        layers.Dense(512, activation=\"elu\"), \n        layers.BatchNormalization(),\n        layers.Dense(256, activation = \"elu\")\n        ],name='Head1') \n\n    input_3 = head_1(input_1)\n    input_3_concat = layers.Concatenate()([input_2, input_3])\n\n    head_2 = Sequential([\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(512, \"relu\"),\n        layers.BatchNormalization(),\n        layers.Dense(512, \"elu\"),\n        layers.BatchNormalization(),\n        layers.Dense(256, \"relu\"),\n        layers.BatchNormalization(),\n        layers.Dense(256, \"elu\")\n        ],name='Head2')\n\n    input_4 = head_2(input_3_concat)\n    input_4_avg = layers.Average()([input_3, input_4]) \n\n    head_3 = Sequential([\n        layers.BatchNormalization(),\n        layers.Dense(256, kernel_initializer='lecun_normal', activation='selu'),\n        layers.BatchNormalization(),\n        layers.Dense(n_labels, kernel_initializer='lecun_normal', activation='selu'),\n        layers.BatchNormalization(),\n        layers.Dense(n_labels, activation=\"sigmoid\")\n        ],name='Head3')\n\n    output = head_3(input_4_avg)\n\n\n    model = Model(inputs = [input_1, input_2], outputs = output)\n    model.compile(optimizer='adam', loss=losses.BinaryCrossentropy(label_smoothing=label_smoothing), metrics=logloss)\n    \n    return model\n","7f2053f7":"n_seeds = 7\nnp.random.seed(1)\nn_folds = 10\nseeds = np.random.randint(0,100,size=n_seeds)\ny_pred = np.zeros((n_test,n_labels))\n\"\"\nfor seed in seeds:\n    fold = 0\n    kf = KFold(n_splits=n_folds,shuffle=True,random_state=seed)\n    for train, test in kf.split(train_features):\n        X_train, X_test = preprocessor(train_features.iloc[train].values,\n                                       train_features.iloc[test].values)\n        _,data_test = preprocessor(train_features.iloc[train].values,\n                                   test_features.drop('cp_type',axis=1).values)\n        X_train_2 = train_features.iloc[train][predictors].values\n        X_test_2 = train_features.iloc[test][predictors].values\n        data_test_2 = test_features[predictors].values\n        y_train = labels_train[train]\n        y_test = labels_train[test]\n        n_features = X_train.shape[1]\n        n_features_2 = X_train_2.shape[1]\n\n        model = build_model(n_features, n_features_2, n_labels)\n        \n#         reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_logloss', factor=0.1, patience=2, mode='min', min_lr=1E-5)\n#         early_stopping = callbacks.EarlyStopping(monitor='val_logloss', min_delta=1E-5, patience=10, mode='min',restore_best_weights=True)\n\n#         hist = model.fit([X_train,X_train_2],y_train, batch_size=128, epochs=192,verbose=1,validation_data = ([X_test,X_test_2],y_test),\n#                          callbacks=[reduce_lr, early_stopping])\n#         hists.append(hist)\n        \n        # Save Model\n        model.load_weights('..\/input\/resnet-model\/TwoHeads_seed_'+str(seed)+'_fold_'+str(fold)+'.h5')\n\n        # OOF Score\n        \n        # Run prediction\n        y_pred += model.predict([data_test,data_test_2])\/(n_folds*n_seeds)\n\n        fold += 1\n","ec1abfdf":"\nsub4 = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\nsub4.iloc[:,1:] = y_pred\n\n# Set ctl_vehicle to 0\nsub4.iloc[test_features['cp_type'] == 'ctl_vehicle',1:] = 0","3f9e9a26":"submission = sub.copy()","61c71478":"submission.iloc[:, 1:] = (sub.iloc[:, 1:] + sub2.iloc[:, 1:]+ sub3.iloc[:, 1:]+ sub4.iloc[:, 1:])\/4","37497405":"submission.head(5)","2ff7a39c":"#submission.iloc[:,1:] = np.clip(submission.iloc[:,1:],0.0005,0.9995)","652657b1":"submission.iloc[test_features['cp_type'] == 'ctl_vehicle',1:] = 0","229af551":"submission.head(5)","4b6db4a8":"submission.to_csv(\"submission.csv\",index=False)","89900840":"submission.shape","cf5dd047":"# TABNET MODEL","3ae1b3ae":"# ENSEMBLING","2d44b803":"# 1836 MODEL","95ad5f07":"# KERAS MODEL","c0b61040":"# 1840 MODEL"}}