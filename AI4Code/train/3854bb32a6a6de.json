{"cell_type":{"c6a2e423":"code","522830d3":"code","515c6a08":"code","18fbe6a1":"code","b1225d4c":"code","8f4b5792":"code","d2d2b4e3":"code","bc709c07":"code","2f7a9ba9":"code","4094709f":"code","6e81ce3b":"code","e08e629f":"code","2e64ee6f":"code","77bef57e":"code","53b65424":"code","0a21f31d":"code","c0ecca17":"code","7d948f97":"code","c25f986a":"code","9d850507":"code","0a785b0e":"code","8273514f":"code","995ccc3b":"markdown","347f2c58":"markdown","44ee79ae":"markdown","244fd799":"markdown","fe7976f0":"markdown","28fb1a4b":"markdown","115d204b":"markdown","87cc0011":"markdown","e0f4a4a7":"markdown"},"source":{"c6a2e423":"#we made our first submission to Kaggle, getting an accuracy score of 75.6%. While this is a good\n#start, there is definitely room for improvement. There are two main areas we can focus on to boost the accuracy of\n#our predictions:\n\n# Improving the features we train our model on\n# Improving the model itself\n\n#we're going to focus working with the features used in our model.\n#We'll start by looking at feature selection. Feature selection is important because it helps to exclude features which\n#are not good predictors, or features that are closely related to each other. Both of these will cause our model to be\n#less accurate, particularly on previously unseen data.\n\n#we trained our model using data about the age, sex and class of the passengers on the Titanic,we ll add some more\n#features\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","522830d3":"#code snippet:\nimport pandas as pd\n\ntrain = pd.read_csv('train.csv')\nholdout = pd.read_csv('test.csv')\n\ndef process_age(df):\n    df[\"Age\"] = df[\"Age\"].fillna(-0.5)\n    cut_points = [-1,0,5,12,18,35,60,100]\n    label_names = [\"Missing\",\"Infant\",\"Child\",\"Teenager\",\"Young Adult\",\"Adult\",\"Senior\"]\n    df[\"Age_categories\"] = pd.cut(df[\"Age\"],cut_points,labels=label_names)\n    return df\n\ndef create_dummies(df,column_name):\n    dummies = pd.get_dummies(df[column_name],prefix=column_name)\n    df = pd.concat([df,dummies],axis=1)\n    return df\n\ntrain=process_age(train)\nholdout=process_age(holdout)\nfor each in [\"Age_categories\",\"Pclass\",\"Sex\"]:\n    train=create_dummies(train,each)\n    holdout=create_dummies(holdout,each)\n    \nprint(train.columns)","515c6a08":"#Our model in the previous attempt based on three columns from the original data: Age, Sex, and Pclass.As we saw\n#when we printed the column names in the previous screen, there are a number of other columns that we haven't yet used\n#To make it easier to reference, the output from the previous screen is copied below:\nprint(train.columns)\n#The last nine rows of the output are dummy columns we created, but in the first three rows we can see there are a\n#number of features we haven't yet utilized. We can ignore PassengerId, since this is just a column Kaggle have added\n#to identify each passenger and calculate scores.Here is a list of the remaining cols:\n\n# SibSp - The number of siblings or spouses the passenger had aboard the Titanic\n# Parch - The number of parents or children the passenger had aboard the Titanic\n# Ticket - The passenger's ticket number\n# Fare - The fair the passenger paid\n# Cabin - The passengers cabin number\n# Embarked - The port where the passenger embarked (C=Cherbourg, Q=Queenstown, S=Southampton)\n\n#both the Name and Ticket columns look to be unique to each passenger.we ll investigate them further\n#We can use the Dataframe.describe() method to give us some more information on the values within each remaining column\ncolumns = ['SibSp','Parch','Fare','Cabin','Embarked']\nprint(train[columns].describe(include='all',percentiles=[]))\n#Of these, SibSp, Parch and Fare look to be standard numeric columns with no missing values. Cabin has values for only\n#204 of the 891 rows,and even then most of the values are unique, so for now we will leave this column also.Embarked\n#looks to be a standard categorical column with 3 unique values, much like PClass was, except that there are 2 missing\n#values. We will fill these two missing values with the most common value, \"S\" which occurs 644 times.\n\n#Looking at our numeric columns, we can see a big difference between the range of each. SibSp has values between 0-8,\n#Parch between 0-6, and Fare is on a dramatically different scale, with values ranging from 0-512. In order to make\n#sure these values are equally weighted within our model, we'll need to rescale the data.\n\n#Within scikit-learn, the preprocessing.minmax_scale() function allows us to quickly and easily rescale our data:\n\n# from sklearn.preprocessing import minmax_scale\n# columns = [\"column one\", \"column two\"]\n# data[columns] = minmax_scale(data[columns])\n\n#Let's process the Embarked, SibSp, Parch and Fare columns in both our train and holdout dataframes.","18fbe6a1":"#code snippet:\nfrom sklearn.preprocessing import minmax_scale\n# The holdout set has a missing value in the Fare column which\n# we'll fill with the mean.\nholdout[\"Fare\"] = holdout[\"Fare\"].fillna(train[\"Fare\"].mean())\n\ntrain[\"Embarked\"]=train[\"Embarked\"].fillna(\"S\")\nholdout[\"Embarked\"] = holdout[\"Embarked\"].fillna(\"S\")\n\ntrain=create_dummies(train,\"Embarked\")\nholdout = create_dummies(holdout,\"Embarked\")\n\ncolumns=[\"SibSp\",\"Parch\",\"Fare\"]\nfor col in columns:\n    train[col + \"_scaled\"]=minmax_scale(train[col])\n    holdout[col + \"_scaled\"] = minmax_scale(holdout[col])","b1225d4c":"#In order to select the best-performing features, we need a way to measure which of our features are relevant to our\n#outcome - in this case, the survival of each passenger. One effective way is by training a logistic regression model\n#using all of our features, and then looking at the coefficients of each feature.\n\n#The scikit-learn LogisticRegression class has an attribute in which coefficients are stored after the model is fit,\n#LogisticRegression.coef_. We first need to train our model, after which we can access this attribute.\n\n\n#The coef() method returns a NumPy array of coefficients, in the same order as the features that were used to fit the\n#model. To make these easier to interpret, we can convert the coefficients to a pandas series, adding the column names\n#as the index:\n","8f4b5792":"import matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\n\ncolumns = ['Age_categories_Missing', 'Age_categories_Infant',\n       'Age_categories_Child', 'Age_categories_Teenager',\n       'Age_categories_Young Adult', 'Age_categories_Adult',\n       'Age_categories_Senior', 'Pclass_1', 'Pclass_2', 'Pclass_3',\n       'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S',\n       'SibSp_scaled', 'Parch_scaled', 'Fare_scaled']\n\n\n\nlr=LogisticRegression()\nlr.fit(train[columns],train[\"Survived\"])\ncoefficients=lr.coef_\nfeature_importance=pd.Series(coefficients[0],index=columns)\nfeature_importance.plot.barh()\n\n","d2d2b4e3":"#To make things easier to interpret, we'll alter the plot to show all positive values, and have sorted the bars in\n#order of size:\nordered_feature_importance = feature_importance.abs().sort_values()\nordered_feature_importance.plot.barh()\nplt.show()\n#We'll train a new model with the top 8 scores and check our accuracy using cross validation.","bc709c07":"#code snippet:\nfrom sklearn.model_selection import cross_val_score\n\ncolumns = ['Age_categories_Infant', 'SibSp_scaled', 'Sex_female', 'Sex_male',\n       'Pclass_1', 'Pclass_3', 'Age_categories_Senior', 'Parch_scaled']\nall_X = train[columns]\nall_y = train['Survived']\n\nlr = LogisticRegression()\nscores = cross_val_score(lr, all_X, all_y, cv=10)\naccuracy = scores.mean()\nprint(accuracy)\n","2f7a9ba9":"#The cross validation score of 81.48% is marginally higher than the cross validation score for the model we created in\n#the previous mission, which had a score of 80.2%.\n\n#Lets see if this translate to unseen data,Let's train a model using the columns from the previous step, make some\n#predictions on the holdout data and submit it to Kaggle for scoring.","4094709f":"#code snippet:\ncolumns = ['Age_categories_Infant', 'SibSp_scaled', 'Sex_female', 'Sex_male',\n       'Pclass_1', 'Pclass_3', 'Age_categories_Senior', 'Parch_scaled']\n\nall_X = train[columns]\nall_y = train['Survived']\nlr = LogisticRegression()\nlr.fit(all_X,all_y)\nholdout_predictions = lr.predict(holdout[columns])\n\nholdout_ids = holdout[\"PassengerId\"]\nsubmission_df = {\"PassengerId\": holdout_ids,\n                 \"Survived\": holdout_predictions}\nsubmission = pd.DataFrame(submission_df)\n\nsubmission.to_csv(\"submission_1.csv\",index=False)","6e81ce3b":"#we'll see that the score is 77.0% It's only a small improvement, but we're moving in the right direction.\n\n#A lot of the gains in accuracy in machine learning come from Feature Engineering. Feature engineering is the practice\n#of creating new features from our existing data.\n\n#One common way to engineer a feature is using a technique called binning.Binning is when we take a continuous feature\n#like the fare a passenger paid for their ticket, and separate it out into several ranges (or 'bins'),turning it into\n#a categorical variable.\n\n#This can be useful when there are patterns in the data that are non-linear and we're using a linear model (like\n#logistic regression). We actually used binning in the previous mission when we dealt with the Age column, although we\n#didn't use the term.\n\n#Let's look at histograms of the Fare column for passengers who died and survived, and see if there are patterns that\n#we can use when creating our bins.\n\n#Looking at the values, it looks like we can separate the feature into four bins to capture some patterns from the data\n#0-12\n# 12-50\n# 50-100\n# 100+\n#we can use the pandas.cut() function to create our bins.","e08e629f":"#code snippet:\ndef process_age(df,cut_points,label_names):\n    df[\"Age\"] = df[\"Age\"].fillna(-0.5)\n    df[\"Age_categories\"] = pd.cut(df[\"Age\"],cut_points,labels=label_names)\n    return df\ndef process_fare(df,cut_points,label_names):\n    df[\"Fare_categories\"] = pd.cut(df[\"Fare\"],cut_points,labels=label_names)\n    return df\n\ncut_points = [0,12,50,100,1000]\nlabel_names = [\"0-12\",\"12-50\",\"50-100\",\"100+\"]\n\ntrain = process_fare(train,cut_points,label_names)\nholdout = process_fare(holdout,cut_points,label_names)\n\ntrain = create_dummies(train,\"Fare_categories\")\nholdout = create_dummies(holdout,\"Fare_categories\")","2e64ee6f":"#Another way to engineer features is by extracting data from text columns. Name and Cabin columns weren't useful by\n#themselves, but what if there is some data there we could extract? Let's take a look at a random sample of rows from\n#those two columns:\n\n#we can see that the format of the cabin numbers is one letter followed by two numbers. It seems like the letter is\n#representative of the type of cabin, which could be useful data for us. We can use the pandas Series.str accessor and\n#then subset the first character using brackets:\n\n\n\n#Looking at the Name column, There is a title like 'Mr' or 'Mrs' within each, as well as some less common titles, like\n#the 'Countess' from the final row of our table above.By spending some time researching the different titles, we can\n#categorize these into six types:\n\n# Mr\n# Mrs\n# Master\n# Miss\n# Officer\n# Royalty\n\n#We can use the Series.str.extract method and a regular expression to extract the title from each name and then use the\n#Series.map() method and a predefined dictionary to simplify the titles.","77bef57e":"#code snippet:\ntitles = {\n    \"Mr\" :         \"Mr\",\n    \"Mme\":         \"Mrs\",\n    \"Ms\":          \"Mrs\",\n    \"Mrs\" :        \"Mrs\",\n    \"Master\" :     \"Master\",\n    \"Mlle\":        \"Miss\",\n    \"Miss\" :       \"Miss\",\n    \"Capt\":        \"Officer\",\n    \"Col\":         \"Officer\",\n    \"Major\":       \"Officer\",\n    \"Dr\":          \"Officer\",\n    \"Rev\":         \"Officer\",\n    \"Jonkheer\":    \"Royalty\",\n    \"Don\":         \"Royalty\",\n    \"Sir\" :        \"Royalty\",\n    \"Countess\":    \"Royalty\",\n    \"Dona\":        \"Royalty\",\n    \"Lady\" :       \"Royalty\"\n}\n\nextracted_titles = train[\"Name\"].str.extract(' ([A-Za-z]+)\\.',expand=False)\ntrain[\"Title\"] = extracted_titles.map(titles)\nextracted_titles = holdout[\"Name\"].str.extract(' ([A-Za-z]+)\\.',expand=False)\nholdout[\"Title\"] = extracted_titles.map(titles)\n\ntrain[\"Cabin_type\"] = train[\"Cabin\"].str[0]\ntrain[\"Cabin_type\"] = train[\"Cabin_type\"].fillna(\"Unknown\")\n\nholdout[\"Cabin_type\"] = holdout[\"Cabin\"].str[0]\nholdout[\"Cabin_type\"] = holdout[\"Cabin_type\"].fillna(\"Unknown\")\n\nfor column in [\"Title\",\"Cabin_type\"]:\n    train = create_dummies(train,column)\n    holdout = create_dummies(holdout,column)","53b65424":"#We now have 34 possible feature columns we can use to train our model. One thing to be aware of as we start to add\n#more features is a concept called collinearity. Collinearity occurs where more than one feature contains data that\n#are similar.\n\n#The effect of collinearity is that our model will overfit - we may get great results on our test data set, but \n#then the model performs worse on unseen data (like the holdout set).\n\n#One easy way to understand collinearity is with a simple binary variable like the Sex column in our dataset. Every\n#passenger in our data is categorized as either male or female, so 'not male' is exactly the same as 'female'.\n\n#As a result, when we created our two dummy columns from the categorical Sex column, we've actually created two columns\n#with identical data in them. This will happen whenever we create dummy columns, and is called the dummy variable trap.\n#The easy solution is to choose one column to drop any time our make dummy columns.\n\n#A common way to spot collinearity is to plot correlations btw each pair of variables in a heatmap\n\n#The darker squares, whether the darker red or darker blue, indicate pairs of columns that have higher correlation and\n#may lead to collinearity. The easiest way to produce this plot is using the DataFrame.corr() method to produce a\n#correlation matrix, and then use the Seaborn library's seaborn.heatmap() function to plot the values:\n\n#We've created a function containing that code to make it easier for you to plot the correlations between the features\n#in our data.","0a21f31d":"#code snippet:\nimport numpy as np\nimport seaborn as sns\n\ndef plot_correlation_heatmap(df):\n    corr = df.corr()\n    \n    sns.set(style=\"white\")\n    mask = np.zeros_like(corr, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n\n    f, ax = plt.subplots(figsize=(11, 9))\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n    plt.show()\n\ncolumns = ['Age_categories_Missing', 'Age_categories_Infant',\n       'Age_categories_Child', 'Age_categories_Teenager',\n       'Age_categories_Young Adult', 'Age_categories_Adult',\n       'Age_categories_Senior', 'Pclass_1', 'Pclass_2', 'Pclass_3',\n       'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S',\n       'SibSp_scaled', 'Parch_scaled', 'Fare_categories_0-12',\n       'Fare_categories_12-50','Fare_categories_50-100', 'Fare_categories_100+',\n       'Title_Master', 'Title_Miss', 'Title_Mr','Title_Mrs', 'Title_Officer',\n       'Title_Royalty', 'Cabin_type_A','Cabin_type_B', 'Cabin_type_C', 'Cabin_type_D',\n       'Cabin_type_E','Cabin_type_F', 'Cabin_type_G', 'Cabin_type_T', 'Cabin_type_Unknown']\n\nplot_correlation_heatmap(train[columns])","c0ecca17":"#We can see that there is a high correlation between Sex_female\/Sex_male and Title_Miss\/Title_Mr\/Title_Mrs. We will\n#remove the columns Sex_female and Sex_male since the title data may be more nuanced.\n\n#Apart from that, we should remove one of each of our dummy variables to reduce the collinearity in each. We'll remove:\n\n# Pclass_2\n# Age_categories_Teenager\n# Fare_categories_12-50\n# Title_Master\n# Cabin_type_A\n\n#we manually used the logit coefficients to select the most relevant features. An method is to use one of sklearn's\n#inbuilt feature selection classes. We will be using the feature_selection.RFECV class which performs recursive feature\n#elimination with cross-validation.\n\n#The RFECV class starts by training a model using all of our features and scores it using cross validation. It then\n#uses the logit coefficients to eliminate the least important feature, and trains and scores a new model. At the end,\n#the class looks at all the scores, and selects the set of features which scored highest.\n\n#Like the LogisticRegression class, RFECV must first be instantiated and then fit. The first parameter when creating\n#the RFECV object must be an estimator, and we need to use the cv parameter to specific the number of folds for cv\n\n\n# #Once the RFECV object has been fit, we can use the RFECV.support_ attribute to access a boolean mask of True and False\n# #values which we can use to generate a list of optimized columns:\n","7d948f97":"#code snippet:\nfrom sklearn.feature_selection import RFECV\n\ncolumns = ['Age_categories_Missing', 'Age_categories_Infant',\n       'Age_categories_Child', 'Age_categories_Young Adult',\n       'Age_categories_Adult', 'Age_categories_Senior', 'Pclass_1', 'Pclass_3',\n       'Embarked_C', 'Embarked_Q', 'Embarked_S', 'SibSp_scaled',\n       'Parch_scaled', 'Fare_categories_0-12', 'Fare_categories_50-100',\n       'Fare_categories_100+', 'Title_Miss', 'Title_Mr', 'Title_Mrs',\n       'Title_Officer', 'Title_Royalty', 'Cabin_type_B', 'Cabin_type_C',\n       'Cabin_type_D', 'Cabin_type_E', 'Cabin_type_F', 'Cabin_type_G',\n       'Cabin_type_T', 'Cabin_type_Unknown']\n\nall_X = train[columns]\nall_y = train[\"Survived\"]\n\nlr=LogisticRegression()\nselector=RFECV(lr,cv=10)\nselector.fit(all_X,all_y)\n\noptimized_columns=all_X.columns[selector.support_]\noptimized_columns\n\n#feature selection-> R","c25f986a":"# The RFECV() selector returned only four columns:\n\n# ['SibSp_scaled', 'Title_Mr', 'Title_Officer', 'Cabin_type_Unknown']\n# Let's train a model using cross validation using these columns and check the score.","9d850507":"#CODE Snippet:\nall_X = train[optimized_columns]\nall_y = train[\"Survived\"]\nlr = LogisticRegression()\nscores = cross_val_score(lr, all_X, all_y, cv=10)\naccuracy = scores.mean()\naccuracy","0a785b0e":"#This four-feature model scores 82.3%, a modest improvement compared to the 81.5% from our earlier model. Let's train\n#these columns on the holdout set, save a submission file and see what score we get from Kaggle.","8273514f":"#code snippet:\nlr = LogisticRegression()\nlr.fit(all_X,all_y)\nholdout_predictions = lr.predict(holdout[optimized_columns])\n\nholdout_ids = holdout[\"PassengerId\"]\nsubmission_df = {\"PassengerId\": holdout_ids,\n                 \"Survived\": holdout_predictions}\nsubmission = pd.DataFrame(submission_df)\n\nsubmission.to_csv(\"submission_2.csv\",index=False)","995ccc3b":"## Determining the Most Relevant Features","347f2c58":"## Engineering Features From Text Columns","44ee79ae":"## Submitting our Model to Kaggle - Attempt2","244fd799":"## Finding Correlated Features","fe7976f0":"## Training A Model Using Optimized Columns","28fb1a4b":"## Final Feature Selection using RFECV","115d204b":"## Feature Engineering Using Binning","87cc0011":"## Submitting Improved Model to Kaggle - Attempt1.5","e0f4a4a7":"## Training a model using relevant features."}}