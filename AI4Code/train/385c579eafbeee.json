{"cell_type":{"60ce3708":"code","f223fa65":"code","98d680e1":"code","5dcddf74":"code","4d1b8e37":"code","f23b2682":"code","ad7d27ff":"code","7245094f":"code","c6db6e4a":"code","aed33c52":"code","6ca184de":"code","36236602":"code","1578900f":"code","08597de9":"code","c0f6f8e0":"code","370807d4":"code","6cbfe804":"code","c3d8a427":"code","a2da50f3":"code","f8f628ca":"markdown","c17af42e":"markdown","ee754512":"markdown","aa73db38":"markdown","a9d8d46d":"markdown","6bf1ce64":"markdown","7ab32102":"markdown","43dcf504":"markdown","30260774":"markdown","144ea8b7":"markdown","57634bb0":"markdown","01cf87d6":"markdown","f09bad60":"markdown","a9bf6c49":"markdown","49b3f358":"markdown","7f136ee2":"markdown","78617cca":"markdown","f0e2e60e":"markdown","a1895c91":"markdown","461080bf":"markdown","9a837ed3":"markdown","f6607252":"markdown","6f9fd443":"markdown","4faebc33":"markdown","817d4078":"markdown","0cb1ab1d":"markdown","0404a049":"markdown"},"source":{"60ce3708":"# importar bibliotecas\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n#%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import GridSearchCV\nfrom numpy import sqrt, mean \n\n","f223fa65":"# listar datasets\nimport os\nprint(os.listdir(\"..\/input\"))","98d680e1":"train = pd.read_csv('..\/input\/train-call-data-v1csv\/train_call_data_v1.csv')\ntrain.head()","5dcddf74":"train['Event Clearance Description'].value_counts()\/len(train)*100","4d1b8e37":"# Crimes com mais ocorr\u00eancias\ncounts = train['Final Call Type'].value_counts()\nkeys = counts.keys().values\nvalues = counts.values\ndfCount = pd.DataFrame()\ndfCount['Final Call Type'] = keys\ndfCount['Total'] = values\n\n# graph\ntop10 = dfCount[:10]\nsns.set(style=\"whitegrid\")\nax = sns.barplot(x=\"Total\", y=\"Final Call Type\", data=top10)\nax.set_title('MOST COMMON CALL TYPE')\n\nplt.show()","f23b2682":"# Setores com mais ocorr\u00eancias\ncounts = train['Sector'].value_counts()\nkeys = counts.keys().values\nvalues = counts.values\ndfCount = pd.DataFrame()\ndfCount['Sector'] = keys\ndfCount['Total'] = values\n\n# graph\ntop10 = dfCount[:10]\nsns.set(style=\"whitegrid\")\nax = sns.barplot(x=\"Total\", y=\"Sector\", data=top10)\nax.set_title('MOST COMMON SECTOR')\n\nplt.show()","ad7d27ff":"# No setor com mais ocorr\u00eancias, a principal origem s\u00e3o abordagens feitas pelos pr\u00f3pios policiais\nprint('SECTOR: KING')\ndfKingSector = train[train['Sector']=='KING'].copy()\ndfKingSectorCounts = dfKingSector['Call Type'].value_counts()\nprint(dfKingSectorCounts)\n\n# J\u00e1 no setor com menos ocorr\u00eancias, a principal origem s\u00e3o as chamadas para a central 911\nprint('\\n\\nSECTOR: WILLIAM')\ndfWilliamSector = train[train['Sector']=='WILLIAM'].copy()\ndfWilliamSectorCounts = dfWilliamSector['Call Type'].value_counts()\nprint(dfWilliamSectorCounts)\n\n# DataFrame para o resultado em gr\u00e1fico\ndfCallTypes = pd.DataFrame(columns=['Sector', 'Call Type', 'Occurrences'])\n\ndfCallTypes = dfCallTypes.append(\n    pd.DataFrame(\n        {'Sector':'KING', 'Call Type': dfKingSectorCounts.index.tolist(), 'Occurrences': dfKingSectorCounts.values}))\n\ndfCallTypes = dfCallTypes.append(\n    pd.DataFrame(\n        {'Sector':'WILLIAM', 'Call Type': dfWilliamSectorCounts.index.tolist(), 'Occurrences': dfWilliamSectorCounts.values}))\n\nsns.set(style=\"whitegrid\")\nax = sns.barplot(y='Call Type', x='Occurrences', data=dfCallTypes, hue='Sector', palette=\"Blues_d\")\nax.set_title('MOST COMMON CALL TYPE FOR SECTOR K AND W')\n\nplt.show()","7245094f":"def slug_column_name(dataframe):\n    dataframe.columns = dataframe.columns.str.lower()\\\n        .str.replace(' ', '_')\\\n        .str.replace('-', '_')\\\n        .str.replace('(', '_')\\\n        .str.replace(')', '_')\\\n        .str.replace(',', '_')\\\n        .str.replace('.', '_')\n    return dataframe\n\ndef set_onehotencoding(dataframe, column, prefix):\n    cols = pd.get_dummies(dataframe[column], prefix=prefix)\n    dataframe.drop(columns=[column], inplace=True)\n    return pd.concat([dataframe,cols],axis=1)\n\ndef drop_columns(dataframe, columns): \n    return dataframe.drop(columns=columns)\n\ndef set_dateproperty(dataframe, column, prefix):\n    dataframe[prefix + '_timestamp'] = dataframe[column].apply(lambda x: pd.Timestamp(x))\n    dataframe[prefix + '_hour'] = dataframe[prefix + '_timestamp'].dt.hour\n    dataframe[prefix + '_day'] = dataframe[prefix + '_timestamp'].dt.day\n    dataframe[prefix + '_dayofweek'] = dataframe[prefix + '_timestamp'].dt.dayofweek\n    dataframe[prefix + '_month'] = dataframe[prefix + '_timestamp'].dt.month\n    dataframe.drop(columns=[prefix + '_timestamp'], inplace=True)\n    return dataframe\n\ndfTrainProcessed = train.pipe(set_onehotencoding, column='Call Type', prefix='call_type')\\\n    .pipe(set_onehotencoding, column='Priority', prefix='priority')\\\n    .pipe(set_onehotencoding, column='Initial Call Type', prefix='initial_call_type')\\\n    .pipe(set_onehotencoding, column='Final Call Type', prefix='final_call_type')\\\n    .pipe(set_onehotencoding, column='Precinct', prefix='precinct')\\\n    .pipe(set_onehotencoding, column='Sector', prefix='sector')\\\n    .pipe(set_onehotencoding, column='Beat', prefix='beat')\\\n    .pipe(set_dateproperty, column='Original Time Queued', prefix='original_time_queued')\\\n    .pipe(drop_columns, columns=['Unnamed: 0', 'CAD Event Number', 'Event Clearance Description', 'Original Time Queued', 'Arrived Time'])\\\n    .pipe(slug_column_name)\\\n    .copy()\n\ndfTrainProcessed.head()","c6db6e4a":"# fun\u00e7\u00e3o para plotar o gr\u00e1fico das 20 features mais importantes\ndef feature_importances_graph(model, df):\n    importances = model.feature_importances_\n    feature_names = df.columns\n    limit = 20\n\n    # Sort feature importances in descending order\n    indices = np.argsort(importances)[::-1]\n    indices = indices[0:limit]\n\n    # Rearrange feature names so they match the sorted feature importances\n    names = [feature_names[i] for i in indices]\n\n    # Create plot\n    plt.figure(figsize=(20, 10))\n\n    # Create plot title\n    plt.title(\"Feature Importance\")\n\n    # Add bars\n    plt.bar(range(df.shape[1])[0:limit], importances[indices])\n\n    # Add feature names as x-axis labels\n    plt.xticks(range(df.shape[1])[:limit], names, rotation=90)\n\n    # Show plot\n    plt.show()","aed33c52":"# Converte os valores em 'labels', ou seja, agora a coluna `Event Clearance Description` \n# possue valores num\u00e9ricos para identificar cada classe \u00fanica, possibilitando a aplica\u00e7\u00e3o do modelo escolhido\nle = LabelEncoder()\n_train = train.copy()\n_train['Event Clearance Description'] = le.fit_transform(_train['Event Clearance Description'])\n\n# define a classe alvo, e os preditores (j\u00e1 resultantes do pr\u00e9-processamento realizado nas quest\u00f5es anteriores)\nclasse = _train['Event Clearance Description'].values\npreditores = dfTrainProcessed\n\n# aplica o StandardScaler no dataset\nscaler = StandardScaler()\npreditores = scaler.fit_transform(preditores)\n\n# divide o dataset em conjuntos de treino e teste\nX_train, X_test, y_train, y_test = train_test_split(preditores,\\\n   classe, test_size=0.3, random_state=100)","6ca184de":"# cria uma nova inst\u00e2ncia do modelo, aplica o treinamento e, em seguida, a predi\u00e7\u00e3o\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train, y_train)\ndtc_y_predict = dtc.predict(X_test)\n\nfeature_importances_graph(dtc, dfTrainProcessed)\n\n# avalia a acur\u00e1cia \naccuracy_score(y_test, dtc_y_predict)","36236602":"# cria uma nova instancia do modelo, aplica o treinamento e, em seguida, a predi\u00e7\u00e3o\nrfc = RandomForestClassifier(n_estimators=100, random_state=100)\n\nrfc.fit(X_train, y_train)\nrfc_y_predict = rfc.predict(X_test)\n\nfeature_importances_graph(rfc, dfTrainProcessed)\n\n# avalia a acur\u00e1cia \naccuracy_score(y_test, rfc_y_predict)","1578900f":"# cross validation\ncv_results = cross_validate(dtc, preditores, classe, cv=3)\nsorted(cv_results.keys()) \nmean(cv_results['test_score'])","08597de9":"# grid search\ndtc = DecisionTreeClassifier()\n\nparams_dtc = {'max_depth': np.arange(3, 10)}\n\ncv_dtc = GridSearchCV(dtc, params_dtc)\ncv_dtc.fit(X_train, y_train)\ncv_dtc.best_params_","c0f6f8e0":"# cross validation\ncv_results = cross_validate(rfc, preditores, classe, cv=3)\nsorted(cv_results.keys()) \nmean(cv_results['test_score'])","370807d4":"# grid search\nrfc = RandomForestClassifier()\n\nparams_rfc = { \n    'n_estimators': [100,200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\n\ncv_rfc = GridSearchCV(estimator=rfc, param_grid=params_rfc,cv= 10, n_jobs=3)\ncv_rfc.fit(X_train, y_train)\ncv_rfc.best_params_","6cbfe804":"# carrega o dataset test_call_data\ntest = pd.read_csv('..\/input\/test-call-datacsv\/test_call_data.csv')\n\n# aplica o mesmo pipeline de pr\u00e9-processamento do treino\ndfTestProcessed = test.pipe(set_onehotencoding, column='Call Type', prefix='call_type')\\\n    .pipe(set_onehotencoding, column='Priority', prefix='priority')\\\n    .pipe(set_onehotencoding, column='Initial Call Type', prefix='initial_call_type')\\\n    .pipe(set_onehotencoding, column='Final Call Type', prefix='final_call_type')\\\n    .pipe(set_onehotencoding, column='Precinct', prefix='precinct')\\\n    .pipe(set_onehotencoding, column='Sector', prefix='sector')\\\n    .pipe(set_onehotencoding, column='Beat', prefix='beat')\\\n    .pipe(set_dateproperty, column='Original Time Queued', prefix='original_time_queued')\\\n    .pipe(drop_columns, columns=['Unnamed: 0', 'CAD Event Number', 'Event Clearance Description', 'Original Time Queued', 'Arrived Time'])\\\n    .pipe(slug_column_name)\\\n    .copy()\n\ndfTestProcessed.head()","c3d8a427":"# prepara valores para aplicar o modelo\nle = LabelEncoder()\n_test = test.copy()\n_test['Event Clearance Description'] = le.fit_transform(_test['Event Clearance Description'])\n_test.head()\n\nclasse = _test['Event Clearance Description'].values\npreditores = dfTestProcessed\n\ntrain_columns = dfTrainProcessed.columns\ntest_columns = dfTestProcessed.columns\n\n# seleciona apenas as features equivalentes em ambos os datasets\ncolumns_to_use = [x for x in test_columns if x in train_columns]","a2da50f3":"X_train = dfTrainProcessed[columns_to_use]\ny_train =  le.fit_transform(train['Event Clearance Description'].values)\n\npreditores = dfTestProcessed[columns_to_use]\n\nscaler = StandardScaler()\npreditores = scaler.fit_transform(preditores)\n\nrfc = RandomForestClassifier(n_estimators=100, random_state=100)\n\nrfc.fit(X_train, y_train)\nrfc_y_predict = rfc.predict(preditores)\n\naccuracy_score(classe, rfc_y_predict)","f8f628ca":"### 6.2. Random Forest Classifier","c17af42e":"![title](https:\/\/doity.com.br\/media\/doity\/eventos\/evento-20025-logo_organizador.png)\n\n# Prova de Descoberta do Conhecimento\n\n* **Prof. Cleilton Lima Rocha**\n* **emails:** climarocha@gmail.com\n* **deadline: 20\/5 \u00e0s 12h**","ee754512":"## * **9. Realize a predi\u00e7\u00e3o sobre os dado test_call_data, como o seu modelo saiu? (1,0 ponto)**","aa73db38":"Obs.: Os gr\u00e1ficos foram plotados anteriormente. \n\nPodemos ver que as features mais importantes s\u00e3o relacionadas \u00e0s datas e horas, em seguida vemos as de prioridade (espec\u00edficamente priority_7) e os tipos de chamados. ","a9d8d46d":"## * **4. Selecione duas solu\u00e7\u00f5es candidatas e justifique suas escolhas. Mostre os pontos negativos e positivos de cada modelo. (2,0 pontos)**","6bf1ce64":"## * **5. Construa os modelos de aprendizagem de m\u00e1quina para cada modelo (2,0 ponto)**    ","7ab32102":"* **1.1. Como est\u00e1 o balanceamento das classes?**","43dcf504":"### 6.1. Decision Tree Classifier","30260774":"#### Foi realizado um pr\u00e9-processamento para deixar todas as vari\u00e1veis na mesma unidade de medida, conforme podemos ver na sa\u00edda acima.\n\n#### Geralmente as fontes de dados submetidos aos modelos de machine learning tem um grande volume de informa\u00e7\u00f5es,mas nem tudo \u00e9 relevante para a solu\u00e7\u00e3o. Este processo \u00e9 fundamental para, escolher as melhores features dentres os dados brutos a fim de aumentar a prcis\u00e3o da solu\u00e7\u00e3o ou viabilizar modelos mais simples para a resolu\u00e7\u00e3o do problema.","144ea8b7":"### 5.1. Decision Tree Classifier","57634bb0":"## * **7. Defina uma m\u00e9trica de avalia\u00e7\u00e3o e avalie as solu\u00e7\u00f5es candidatas. Justifique a escolha da sua m\u00e9trica. (1,0 ponto)**","01cf87d6":"##  * **3. Realize o tratamento que voc\u00ea julgar mais adequado aos dados. (2,0 pontos)**","f09bad60":"### Conforme o g\u00e1fico, podemos ver que o setor K tem quase o dobro de chamados do setor W.","a9bf6c49":"R.: Conforme os valores apresentados, podemos constatar h\u00e1 um desbalanceamento das classes, contendo apenas sete alvos, onde uma \u00fanica classe concentra mais de 40% dos registros.","49b3f358":"Para este projeto exploraremos os dados Call_Data dispon\u00edvel dispon\u00edvel na pasta.\n\n\nPara facilitar a administra\u00e7\u00e3o da seguran\u00e7a p\u00fablica, o Departamento de Pol\u00edcia de Seattle dividiu a cidade em 5 partes, cada uma com uma delegacia. Cada delegacia foi subdividida em setores, e estes foram divididos em beats (hondas). A administra\u00e7\u00e3o tem um dataset chamado Call_Data, para obter maiores informa\u00e7\u00f5es acesse este [link](https:\/\/data.seattle.gov\/Public-Safety\/Call-Data\/33kz-ixgy).\n\nO objetivo do nosso projeto \u00e9 apoiar os policiais quanto as medidas prescritivas que eles devem tomar ao tentarem resolver uma chamada. Para isto eles t\u00eam dispon\u00edvel o hist\u00f3rico de tudo o que j\u00e1 foi resolvido, por ele e por seus colegas, e sua solu\u00e7\u00e3o de Data Science capaz de prever a vari\u00e1vel alvo da nossa prova ser\u00e1 **Event Clearance Description**.\n\nBoa prova e hands on!\n\n**PS.:**\n* Quando houver necessidade de splitar os dados aplique a propor\u00e7\u00e3o 70 para treino e 30 para teste\n* Quando houver necessidade de utilizar o random_state defina o valor 100\n* O t\u00edtulo do email deve ser \"Prova KDD - Turma 2 - [Membros da equipe]\"\n* Envie o c\u00f3digo fonte e o report (File ==> Download As ==> Html ou PDF), com o nome dos membros da dupla, para um dos meus emails, climarocha@gmail.com at\u00e9 o dia **20\/5 \u00e0s 12h**.","7f136ee2":"## * **8. Escolha um dos modelos, por exemplo o melhor modelo, e fa\u00e7a uma an\u00e1lise do overfitting e underfitting. Justique sua resposta com base em experimentos. (1,5 pontos)**","78617cca":"##  * **6. Para cada modelo aplique uma combina\u00e7\u00e3o aos hiperpar\u00e2metros com o GridSearch e aplique tamb\u00e9m o CrossValidation (2,0 pontos)**","f0e2e60e":"## * **1. Importe o data set train_call_data e considere a vari\u00e1vel alvo 'Event Clearance Description'(0,5 pontos)** \n      ","a1895c91":"Escolhemos o segundo modelo utilizado, o Random Forest, com melhor acur\u00e1cia. Com o pr\u00e9-processamento, obtivemos um dataset com mais de 600 features, com uma acur\u00e1cia de 77%. Uma vantagem do Random Forest, \u00e9 a possibilidade de obter as features mais importantes, com isso selecionamos as 90 mais importantes e rodamos o modelo novamente, obtendo 74% de acur\u00e1cia.","461080bf":"### 5.0. Preparar dados para aplicar os modelos","9a837ed3":"## * **10. Se seu modelo permitir analisar a import\u00e2ncia das features, analise-o e tente justificar de forma subjetiva a import\u00e2ncia da feature. Por exemplo, a feature_chamadas_a_noite possui um alto coeficiente, pois h\u00e1 uma tend\u00eancia dos crimes acontecerem a noite, n\u00e3o t\u00e3o simples assim :P. (1,0 ponto)**","f6607252":"Utilizamos a acuracia para selecionar o melhor modelo, conforme os resultados, chegamos a conclus\u00e3o que o Random Forest teve o melhor desempenho, com uma acur\u00e1cia de 77%, contra 70% do modelo Decision Tree.","6f9fd443":"## * **2. Realize o EDA que voc\u00ea julgar necess\u00e1rio (an\u00e1lise explorat\u00f3ria dos dados), o objetivo do EDA \u00e9 mostrar alguns insights sobre os dados (1,0 pontos)**","4faebc33":"### Setor K \u00e9 onde ocorre o maior n\u00famero de chamados, por\u00e9m s\u00e3o resultados de abordagens feitas pelos pr\u00f3pios policiais. J\u00e1 no setor W, com menos chamados, observamos um comportamento oposto.","817d4078":"### Solu\u00e7\u00e3o 1: Decision Tree:\nEste modelo de solu\u00e7\u00e3o foi escolhido pela sua f\u00e1cil compreens\u00e3o e pela sua aplicabilidade em explora\u00e7\u00e3o de dados,que \u00e9 o cerne do problema proposto. Uma de suas desvantagens \u00e9 a m\u00e1 utiliza\u00e7\u00e3o de vari\u00e1veis num\u00e9ricas cont\u00ednuas (nosso modelo possui poucas), pois a \u00e1rvore perde informa\u00e7\u00f5es ao utilizar muitas categorias. Outro problema \u00e9 o Over fitting, mas este pode ser solucionado com aplica\u00e7\u00e3o de restri\u00e7\u00f5es aos dados.\n\n\n### Solu\u00e7\u00e3o 2: Random Forest:\nO algor\u00edtimo random forest \u00e9 capaz de resolver problemas de classifica\u00e7\u00e3o e regress\u00e3o atrav\u00e9s de estimativas. Ele identifica as vari\u00e1veis mais significativas mesmo com um grande volume de dados, utilizando um grau de importancia para as vari\u00e1veis. Al\u00e9m disto, possui m\u00e9todos para equilibrar erros em conjuntos de dados onde as classes s\u00e3o desequilibradas. Sua principal limita\u00e7\u00e3o \u00e9 o caso do algor\u00edtimo gerar muitas \u00e1rvorres, pois o tornar\u00e1 lento e ineficiente em tempo real.","0cb1ab1d":"# Quest\u00f5es\n\n * **1. Importe o data set train_call_data e considere a vari\u00e1vel alvo 'Event Clearance Description'(0,5 pontos)** \n     * **1.1. Como est\u00e1 o balanceamento das classes?**\n     * **P.S.: N\u00e3o \u00e9 obrigat\u00f3rio aplicar o undersampling and oversampling sobre o dataset**. \n     * **P.S.: Voc\u00ea pode usar qualquer um dos datasets de treinamento a v1 ou a v2**. \n * **2. Realize o EDA que voc\u00ea julgar necess\u00e1rio (an\u00e1lise explorat\u00f3ria dos dados), o objetivo do EDA \u00e9 mostrar alguns insights sobre os dados (1,0 pontos)**\n     * *Utilize recursos visuais, por exemplo gr\u00e1ficos* \n * **3. Realize o tratamento que voc\u00ea julgar mais adequado aos dados. (2,0 pontos)**\n     * *P.S.: Explique, com suas palavras, porque o processo de feature engineering \u00e9 necess\u00e1rio*\n     * *P.S.: A cria\u00e7\u00e3o de um pipeline lhe dar\u00e1 pontos extras e melhorar\u00e1 o reaproveitamento de c\u00f3digo *\n * **4. Selecione duas solu\u00e7\u00f5es candidatas e justifique suas escolhas. Mostre os pontos negativos e positivos de cada modelo. (2,0 pontos)**\n * **5. Construa os modelos de aprendizagem de m\u00e1quina para cada modelo (2,0 ponto)**    \n * **6. Para cada modelo aplique uma combina\u00e7\u00e3o aos hiperpar\u00e2metros com o GridSearch e aplique tamb\u00e9m o CrossValidation (2,0 pontos)**\n     * P.S.: Explique, com suas palavras, a necessidade de utilizar GridSearch e CrossValidation\n     * P.S.: Explique a import\u00e2ncia para de no m\u00ednimo um hiperpar\u00e2metro para cada modelo\n * **7. Defina uma m\u00e9trica de avalia\u00e7\u00e3o e avalie as solu\u00e7\u00f5es candidatas. Justifique a escolha da sua m\u00e9trica. (1,0 ponto)**\n * **8. Escolha um dos modelos, por exemplo o melhor modelo, e fa\u00e7a uma an\u00e1lise do overfitting e underfitting. Justique sua resposta com base em experimentos. (1,5 pontos)**\n     * *Analise no m\u00ednimo 2 hiperpar\u00e2metros e tamb\u00e9m o n\u00famero de amostras utilizado no treinamento* \n     * *Utilize recursos visuais, por exemplo gr\u00e1ficos, se voc\u00ea achar neccess\u00e1rio* \n * **9. Realize a predi\u00e7\u00e3o sobre os dado test_call_data, como o seu modelo saiu? (1,0 ponto)**\n * **10. Se seu modelo permitir analisar a import\u00e2ncia das features, analise-o e tente justificar de forma subjetiva a import\u00e2ncia da feature. Por exemplo, a feature_chamadas_a_noite possui um alto coeficiente, pois h\u00e1 uma tend\u00eancia dos crimes acontecerem a noite, n\u00e3o t\u00e3o simples assim :P. (1,0 ponto)**\n * **11. Aplique clusteriza\u00e7\u00e3o, preferencialmente o KMeans sobre o dado, e comunique suas novas descobertas, sinta-se a vontade para apresentar uma solu\u00e7\u00e3o com recursos visuais (2,0 pontos)**","0404a049":"## 5.2. Random Forest Classifier"}}