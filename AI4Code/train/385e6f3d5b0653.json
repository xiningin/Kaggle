{"cell_type":{"7943e64c":"code","76c9ef91":"code","b9a83834":"code","482f4f66":"code","df08758e":"code","2214b3bb":"code","9aa82ab3":"code","08aedcbd":"code","5c83843f":"code","0643eadd":"code","33737ed7":"code","1d8d7363":"code","7c045635":"code","45a05563":"code","4735f026":"code","811bf246":"code","63819270":"code","cf349e16":"code","cb56eab1":"code","9ae29ef8":"code","43578973":"code","2a3eefdc":"code","432fda5f":"code","10e2d0f4":"code","fd78ed0c":"code","1d044012":"code","8b9f63e6":"code","db69655e":"code","8a3e143e":"code","67a58a83":"code","a3787bc3":"code","3f2e4e63":"code","39feeb1a":"code","a697b5fa":"code","cff359a9":"code","3790e81e":"code","ee59f4e6":"code","f076a8a6":"code","b65cc23b":"code","db18955b":"code","e3e4e1dd":"code","f96d0642":"code","cf7a4abe":"code","7988d46c":"code","2e4e55bf":"code","7cc95387":"code","9b5d0747":"code","59d9a063":"code","bedf97a1":"code","711f776e":"code","8a71a23a":"code","e5cc223a":"code","4163baf9":"code","acc6e513":"code","9201a7e8":"code","1caa9707":"code","3fb7bc0e":"code","3d0888b3":"code","2ec86b7e":"code","565c3cda":"code","730d4f5f":"code","4cda641c":"code","f9ffa7bb":"code","477b1600":"code","ae653f9e":"code","46ac8555":"markdown","1edca047":"markdown","97002701":"markdown","01a46695":"markdown","1680ccb2":"markdown","c28c64b5":"markdown","c5a45c92":"markdown","8d1098f7":"markdown","eec7f611":"markdown","21700e85":"markdown","328fb463":"markdown","c7402cb5":"markdown","77ae001e":"markdown","326545da":"markdown","34364ce0":"markdown","e69fd22b":"markdown","31bd50e7":"markdown","39e163de":"markdown","eaa3cb2e":"markdown","1ba9562e":"markdown","35d652c1":"markdown","63182441":"markdown","a8ec43ff":"markdown","4c9f7662":"markdown","24c99f22":"markdown","75830d5a":"markdown","d99e67e5":"markdown","987bfb2f":"markdown","57c6df18":"markdown","87011e4c":"markdown","03ecb7c9":"markdown","3b463982":"markdown","d5cd5810":"markdown","d5b57e22":"markdown","98b6fe71":"markdown"},"source":{"7943e64c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","76c9ef91":"train_df_full = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df_full = pd.read_csv('..\/input\/titanic\/test.csv')","b9a83834":"import seaborn as sns\n\nsns.pairplot(train_df_full, hue='Survived')","482f4f66":"train_df_full.head()","df08758e":"train_df_full.describe().transpose()","2214b3bb":"train_df_full.columns","9aa82ab3":"train_df_full.info()","08aedcbd":"train_df_full.isnull().sum()","5c83843f":"# Visualize missing data \n\nimport missingno as msno\nimport matplotlib.pyplot as plt\nmsno.matrix(train_df_full)\nplt.show()\n","0643eadd":"test_df_full.head()","33737ed7":"test_df_full.info()","1d8d7363":"# Visualize missing data in the test data\n\nmsno.matrix(test_df_full)\nplt.show()\n","7c045635":"train_df = train_df_full.drop(['PassengerId'], axis=1)\ntest_df = test_df_full.drop(['PassengerId'], axis=1)","45a05563":"train_df[\"Embarked\"] = train_df[\"Embarked\"].transform(lambda x: x.fillna(x.mode()[0]))\ntest_df[\"Embarked\"] = test_df[\"Embarked\"].transform(lambda x: x.fillna(x.mode()[0]))","4735f026":"train_df['Fare'] = train_df['Fare'].fillna(train_df['Fare'].mean())\ntest_df['Fare'] = test_df['Fare'].fillna(train_df['Fare'].mean())","811bf246":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.scatterplot(x=\"Age\", y='Pclass', data=train_df, hue=\"Survived\")\nplt.show()\n","63819270":"train_df.columns","cf349e16":"from sklearn.ensemble import RandomForestRegressor\n\n# split the dataset into train and test sets\ntrain_age = train_df.loc[train_df.Age.notnull()][['Age','Pclass']]\ntrain_age.isnull().sum()\ntest_age = train_df.loc[train_df.Age.isnull()][['Age','Pclass']]\n\n\nX_train = train_age.drop('Age', axis=1)\ny_train = train_age.Age\nX_test = test_age.drop('Age', axis=1)\n\nrf = RandomForestRegressor(n_estimators=2500, n_jobs=-1)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\n\ntrain_df.loc[train_df.Age.isnull(), 'Age'] = y_pred","cb56eab1":"#test set \n\ntest_set_age = test_df.loc[test_df.Age.notnull()][['Age','Pclass']]\ntest_set_age.isnull().sum()\npredict_age = test_df.loc[test_df.Age.isnull()][['Age','Pclass']]\n\n\nX_test_set = test_set_age.drop('Age', axis=1)\nX_test_set_predict_age = predict_age.drop('Age', axis=1)\n\ny_test_pred = rf.predict(X_test_set_predict_age)\n\ntest_df.loc[test_df.Age.isnull(), 'Age'] = y_test_pred\n","9ae29ef8":"print(train_df['Age'].isnull().sum())\n","43578973":"print(test_df['Age'].isnull().sum())","2a3eefdc":"train_df['Family_Size']  = train_df['SibSp'] + train_df['Parch']\ntrain_df['Family_Size'].value_counts()","432fda5f":"test_df['Family_Size']  = test_df['SibSp'] + test_df['Parch'] \ntest_df['Family_Size'].value_counts()","10e2d0f4":"train_df['IsAlone'] = 0\ntrain_df.loc[train_df['Family_Size'] == 0, 'IsAlone'] = 1\n\ntest_df['IsAlone'] = 0\ntest_df.loc[test_df['Family_Size'] == 0, 'IsAlone'] = 1","fd78ed0c":"sns.histplot(data=train_df, x = 'Age', hue='Survived', bins = [0,16,30,45,65,80])\n\nplt.show()","1d044012":"sns.displot( train_df , x = 'Age', hue='Survived' , kind = 'kde')","8b9f63e6":"age_bin_labels = [1, 2, 3,4]\ntrain_df['Binned_Age'] =  pd.qcut(train_df['Age'], q=4, labels=age_bin_labels).astype(int)\ntest_df['Binned_Age'] =  pd.qcut(test_df['Age'], q=4, labels=age_bin_labels).astype(int)\n","db69655e":"sns.countplot(x=\"Binned_Age\", hue=\"Survived\", data=train_df)","8a3e143e":"train_df['Binned_Age'].value_counts()","67a58a83":"test_df['Binned_Age'].value_counts()","a3787bc3":"train_df[train_df['Binned_Age'].isnull()]","3f2e4e63":"sns.histplot(data=train_df, x = 'Fare', hue='Survived', bins=4)\n\nplt.show()","39feeb1a":"sns.displot( train_df , x = 'Fare', hue='Survived' , kind = 'kde')","a697b5fa":"labels=  [1, 2, 3]\n\ntrain_df['Binned_Fare'] =  pd.qcut(train_df['Fare'], q=3, labels=labels).astype(int)\ntest_df['Binned_Fare'] =  pd.qcut(test_df['Fare'], q=3, labels=labels).astype(int)","cff359a9":"sns.countplot(x=\"Binned_Fare\", hue=\"Survived\", data=train_df)","3790e81e":"test_df['Binned_Fare'].value_counts()","ee59f4e6":"train_df['Binned_Fare'].value_counts()","f076a8a6":"sns.countplot(x=\"Sex\", hue=\"Survived\", data=train_df)","b65cc23b":"sns.countplot(x=\"Embarked\", hue=\"Survived\", data=train_df)","db18955b":"train_df['Title'] = train_df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\ntest_df['Title'] = test_df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)","e3e4e1dd":"train_df = train_df.drop(['Fare', 'Age','Name','Ticket','Cabin', 'SibSp', 'Parch'], axis=1)\ntest_df = test_df.drop(['Fare', 'Age','Name','Ticket','Cabin', 'SibSp', 'Parch'], axis=1)","f96d0642":"train_df['Title'].value_counts()","cf7a4abe":"train_df['Title'] = train_df['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n    'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\ntrain_df['Title'] = train_df['Title'].replace(['Mlle','Ms'], 'Miss')\ntrain_df['Title'] = train_df['Title'].replace('Mme', 'Mrs')\n \ntest_df['Title'] = test_df['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n    'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\ntest_df['Title'] = test_df['Title'].replace(['Mlle','Ms'], 'Miss')\ntest_df['Title'] = test_df['Title'].replace('Mme', 'Mrs')    \n    ","7988d46c":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\ntrain_df['Title'] = train_df['Title'].map(title_mapping)\ntest_df['Title'] = test_df['Title'].map(title_mapping)   ","2e4e55bf":"y = train_df['Survived']\nX = train_df.drop(['Survived'], axis=1)\nX_test = test_df","7cc95387":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nX['Sex'] = le.fit_transform(X['Sex'])\nprint(le.classes_)\nX['Embarked'] = le.fit_transform(X['Embarked'])\nprint(le.classes_)\n\nX_test['Sex'] = le.fit_transform(X_test['Sex'])\nX_test['Embarked'] = le.fit_transform(X_test['Embarked'])","9b5d0747":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42)","59d9a063":"X_train.describe()","bedf97a1":"X_test.describe()","711f776e":"GridSearch = False","8a71a23a":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nif GridSearch == True:\n    param_grid = {'n_neighbors': np.arange(1,50), 'p':[1,2]}\n    knn = KNeighborsClassifier()\n    knn_cv = GridSearchCV(knn, param_grid, cv=10)\n    knn_cv.fit(X_train, y_train)\n    print(knn_cv.best_params_)\n    knn_cv.predict(X_val)\n    print(knn_cv.score(X_val, y_val))\n    \n#{'n_neighbors': 28, 'p': 1}\n#0.8161434977578476","e5cc223a":"from sklearn.linear_model import LogisticRegression\n\nif GridSearch == True:\n    param_grid = {'C':[0.01, 0.1,0.2,0.25,0.3,0.4, 1, 2],'max_iter': [150,200,250, 300, 1000,1500], 'random_state':[1,123, 21,42,10] } \n    lr = LogisticRegression()\n    lr_cv = GridSearchCV(lr, param_grid, cv=10)\n    lr_cv.fit(X_train, y_train)\n    print(lr_cv.best_params_)\n    lr_cv.predict(X_val)\n    print(lr_cv.score(X_val, y_val))\n\n# {'C': 0.1, 'max_iter': 150, 'random_state': 1}\n#0.820627802690583","4163baf9":"from sklearn.ensemble import RandomForestClassifier\n\nif GridSearch == True:\n    param_grid = {'n_estimators':[25,50,60],'max_features': [3,4,5], 'max_depth':[5,6,7], 'min_samples_leaf':[8,9,10,11],'random_state':[1,42] } \n    rf = RandomForestClassifier(n_jobs=-1)\n    rf_cv =  GridSearchCV(rf, param_grid)\n    rf_cv.fit(X_train, y_train)\n    print(rf_cv.best_params_)\n    rf_cv.predict(X_val)\n    print(rf_cv.score(X_val, y_val))\n    \n#{'max_depth': 6, 'max_features': 3, 'min_samples_leaf': 9, 'n_estimators': 25, 'random_state': 42}\n#0.8340807174887892","acc6e513":"from sklearn.linear_model import SGDClassifier\n\nif GridSearch == True:\n    param_grid = {'alpha':[0.01, 0.1,0.2, 1, 2],'max_iter': [70,75,80,90,120,150,200], 'random_state':[1,123, 21,42,10] } \n    sgd = SGDClassifier()\n    sgd_cv =  GridSearchCV(sgd, param_grid, cv=5)\n    sgd_cv.fit(X_train, y_train)\n    print(sgd_cv.best_params_)\n    sgd_cv.predict(X_val)\n    print(sgd_cv.score(X_val, y_val))\n    \n#{'alpha': 0.01, 'max_iter': 70, 'random_state': 21}\n#0.8026905829596412 ","9201a7e8":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nif GridSearch == True:\n    param_grid = {'base_estimator': [LogisticRegression(random_state=1), LogisticRegression(random_state=42), DecisionTreeClassifier()],'learning_rate':[0.1, 0.3, 0.6, 0.9, 1.1,1.3], 'n_estimators':[55,60,65,70],\n                 'random_state':[1,123, 21,42,10]}\n    adb = AdaBoostClassifier()\n    adb_cv = GridSearchCV(adb, param_grid, cv=6)\n    adb_cv.fit(X_train, y_train)\n    print(adb_cv.best_params_)\n    adb_cv.predict(X_val)\n    print(adb_cv.score(X_val, y_val))\n    \n# {'base_estimator': LogisticRegression(random_state=1), 'learning_rate': 0.3, 'n_estimators': 70, 'random_state': 1}\n# 0.8295964125560538","1caa9707":"from sklearn.ensemble import BaggingClassifier\n\nif GridSearch == True:\n    param_grid = {'base_estimator': [LogisticRegression(random_state=1), DecisionTreeClassifier()],'max_features':[3,4,5,6],'max_samples':[30,40,50,60],\n                  'n_estimators':[20,30,40,50],'random_state':[1,123, 21,42,10] } \n\n    bc = BaggingClassifier(n_jobs=-1)\n    bc_cv = GridSearchCV(bc, param_grid, cv=5)\n    bc_cv.fit(X_train, y_train)\n    print(bc_cv.best_params_)\n    bc_cv.predict(X_val)\n    print(bc_cv.score(X_val, y_val))\n\n# {'base_estimator': DecisionTreeClassifier(), 'max_features': 6, 'max_samples': 60, 'n_estimators': 20, 'random_state': 21}\n# 0.8071748878923767","3fb7bc0e":"import xgboost as xgb\n\nif GridSearch == True:\n    param_grid = {'n_estimators':[300,500,1000], 'objective':['binary:logistic'],'learning_rate':[0.1,1,1.5],'max_depth':[5,10,15] }\n    #xgb_clf = xgb.XGBClassifier(base_score=0.5, objective='binary:logistic', n_estimators=300,seed=1, learning_rate=0.1, max_depth=10)\n    xgb = xgb.XGBClassifier()\n    xgb_cv =  GridSearchCV(xgb, param_grid, cv=5) \n    xgb_cv.fit(X_train, y_train)\n    print(xgb_cv.best_params_)\n    xgb_cv.predict(X_val)\n    print(xgb_cv.score(X_val,y_val))\n\n# {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 1000, 'objective': 'binary:logistic'}\n# 0.8161434977578476","3d0888b3":"from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import VotingClassifier, RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\nfrom sklearn.linear_model import SGDClassifier\nimport xgboost as xgb\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nknn = KNeighborsClassifier(n_neighbors=28, p=1)\nlr = LogisticRegression(C=0.1,random_state=1, max_iter=150)\nrf = RandomForestClassifier(n_estimators=50,max_features=4, max_depth=5, min_samples_leaf=11, random_state=1, n_jobs=-1)\n#rf = RandomForestClassifier(n_estimators=25,max_features=3, max_depth=6, min_samples_leaf=9, random_state=42, n_jobs=-1)\n\nsgd = SGDClassifier(alpha=0.1, max_iter=50, n_jobs=-1, random_state=0)\nadb = AdaBoostClassifier(base_estimator=LogisticRegression(random_state=1),\n                   learning_rate=0.9, n_estimators=21, random_state=1)\n#adb = AdaBoostClassifier(base_estimator=LogisticRegression(random_state=1),\n #                  learning_rate=0.3, n_estimators=70, random_state=1)\n\nbc = BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_features=5,\n                  max_samples=40, n_estimators=20, random_state=1)\n#bc = BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_features=6,\n #                 max_samples=60, n_estimators=20, random_state=21)\nxgb_clf = xgb.XGBClassifier(base_score=0.5, objective='binary:logistic', n_estimators=1000,seed=1, learning_rate=1, max_depth=5)\n\nclassifiers = [('Logistic Regression', lr),\n              ('Random Forest', rf),\n              ('AdaBoostClassifier', adb),\n              #('sgd', sgd),\n               #('knn', knn),\n               #('xgb', xgb_clf),\n              ('BaggingClassifier', bc)]  \n\nfor clf_name , clf in classifiers:\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_val)\n    print('{:s} : {:.4f}'.format(clf_name, accuracy_score(y_val, y_pred)))","2ec86b7e":"vc = VotingClassifier(estimators=classifiers)\nvc.fit(X, y)\n\ny_pred = vc.predict(X_val)\nprint('VotingClassifier_score: {:.4f}'.format(vc.score(X_val, y_val)))\n\nprint('VotingClassifier: {:.4f}'.format(accuracy_score(y_val, y_pred)))\n","565c3cda":"from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_val, y_pred))","730d4f5f":"print(classification_report(y_val, y_pred))","4cda641c":"y_pred.shape","f9ffa7bb":"y_pred_test = vc.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId':test_df_full.PassengerId.astype(np.int32), 'Survived':y_pred_test})\noutput.to_csv('my_submission.csv', index=False)","477b1600":"y_pred_test","ae653f9e":"y_pred_test.shape","46ac8555":"One intersting feature is the family size. It consists of the addition of 'SibSp' and 'Parch' columns. i.e. the number of siblings \/ spouses and parents \/ children aboard the Titanic. ","1edca047":"PAY ATTENTION to train the model  using the train set only. The test set is used for prediction. ","97002701":"Now we explore the Age variable by displaying it's histogram and distribution.","01a46695":"# **Feature Engineering**","1680ccb2":"Another intersting feature is whether or not (0 or 1) the person is alone aboard the Titanic. Although ostensibly it does not provide any new information, it does improve the score.    ","c28c64b5":"The same for the Fare variable","c5a45c92":"As shown above, this data set contains 12 columns. i.e 11 features + 1 label. \n* 5 categorical variables: Name, Sex, Ticket, Cabin, Embarked\n* 6 numerical variables: PassengerId, Pclass, Age, Sibsp, Parch, Fare\n* 1 label variable - Survived","8d1098f7":"# Results","eec7f611":"One option to fill the missing values can be calculating the mean age at each Pclass, and replace missing age values according to the their suitable Pclass. \nAnother way, which I find as a better choice, would be to use a regression process. I did a process of trial and error in order to find the features which would yeild the best score; in the end, I left only one feature - the Pclass.  ","21700e85":"# Data preproccesing ","328fb463":"Notice the relationship between Age and Pclass, and between Age and Fare, based on the survived label\n1. Pclass 1 has highest survival chances and Pclass 3 has the lowest chance. \n2. The higher the price and the older the age (25-50 years old), the higher the chance of surviving.  ","c7402cb5":"We will now remove all the columns we no longer need","77ae001e":"## Label Encoding\nMost machine learning models require the categorical variables to be in one-hot encoding representation, or to be mapped into numbers. We will apply this below","326545da":"# Overview\nThis notebook is organized as follows :\n\n1.  Reading the data\n2.  EDA - Exploratory data analysis\n3.  Data preproccesing\n4.  Feature Engineering\n5.  Training models\n7.  Results\n","34364ce0":"Load the train and the test sets","e69fd22b":"We using a voting classifier as an ensmelber for all our models","31bd50e7":"# **EDA - Exploratory data analysis**","39e163de":"We will split the data into train and validation sets","eaa3cb2e":"Since the 'Fare' column is a continuous variable and there is only 1 missing values in this column, an appropriate choice would be to replace the missing value by the mean fare.\nPAY ATTENTION to calculate the mean fare using the train set only!","1ba9562e":"Going over the 5 categorical variables: Name, Sex, Ticket, Cabin, Embarked: \n\n1. From the Name variable we can extract the prefix of the name. The rest is irrelevant.\n2. The Sex variable is important. We will keep it.\n3. The Ticket variable is also irrelevant, it contains many different categories and does not provide any relevant information.\n4. The Cabin column should be dropped later due to many missing values and many different categories.\n5. The Embarked column contain only 3 categories and we may be able to extract relevant information from it, so we will keep it.\n\n","35d652c1":"## Dealing with missing values\nSince the 'Embarked' column is a categorical variable and there are only 2 missing values in this column, an appropriate choice would be to replace the missing values by the most frequent value of that column.    ","63182441":"Create a new dataframe for the rest of the processing, keeping the 'PassengerId' column in the full dataframe for the final stage of writing to the csv file.  ","a8ec43ff":"It can be seen that for each age group, we get a similar behavior, so we will prefer to discretize the age column.","4c9f7662":"# Test set","24c99f22":"Now we have to deal with the 'Age' column. This column contains many missing values, both in the train and the test sets. We saw earlier that there is an intersting relationship between the 'Pclass' and the 'Age'. Let's plot these variables using the seaborn library.   ","75830d5a":"## HyperParameter Tuning\nSince grid search tuning can take a long time to run, we don't necessarily want to run it every time.\n\nInstead of commenting it out, we use a flag to control whether to run it or not.","d99e67e5":"## Fill missing values using RandomForestRegressor","987bfb2f":"# Reading the data","57c6df18":"# Training models ","87011e4c":"The same conclusions for the test set - most of the cabin column is missing, as well as many values in the Age column. here we also have one missing value in the Fare column.  ","03ecb7c9":"White lines indicate missing values, while black lines represent existing information.\n\nAs one can see, the 'Cabin' and 'Age' columns have many missing value(687 and 177 respectively), scattered throughout the columns, and the 'Embarked' column has only 2 missing values. \n\nWe start to consider to drop the 'Cabin' column. ","3b463982":"Explore the data. \n\nFirst I visualized the relationships between each variable to the other variables in the data using Pairplot. \nThen I used methods like head(), describe(), info() ect. to display the first five rows of each data set, display basic statistical details and general information about the data.\n\nAfterwards I visualized the missing data using the missingno library in order the decide which colunmns to drop and how to deal with the missing values, together with the pairplot and other visualizations shown below. ","d5cd5810":"Making sure we didn't miss any row in our discretization process","d5b57e22":"An appropriate number of quantile-based bins could be 3. ","98b6fe71":"# Train set\n## Visualizing the data "}}