{"cell_type":{"bf856f3b":"code","859e8dbc":"code","d14a33f3":"code","4ab808c9":"code","d4e00fe7":"code","73a6e606":"code","20efafd4":"code","2d9a74b5":"code","1866e30e":"code","c64e842c":"code","71e79b39":"code","7b6503ee":"code","fe3dde14":"code","f0a83322":"code","056b627b":"code","7aadb3bb":"code","5241d4ba":"code","c517bce9":"code","99c50fe0":"code","547f1379":"code","1e7848d6":"code","a3d9e04e":"code","0dc46854":"code","c3f0133a":"code","b755bda9":"code","dae4536a":"code","691db367":"code","b1a779b0":"code","a0a0dd3a":"code","8340ff82":"code","0e38dde0":"code","99c1853c":"code","4e7beca5":"code","b0b512ee":"code","24a790ca":"code","afb88384":"code","26453513":"code","e34952e0":"code","a4c15433":"code","e0c066f5":"code","d63cec2b":"code","1cb2733b":"code","eb5e7598":"code","bb832c2d":"code","1e4ce008":"code","c3886b80":"code","b8f920f6":"code","bc72e058":"code","6740e2cf":"code","648d608b":"code","b9b015e4":"code","d443709a":"code","eed2e59d":"code","61438756":"code","dc6b9169":"code","a0408c22":"markdown","bd68bdb2":"markdown","1a2b1110":"markdown","2739ddba":"markdown","ef463bf5":"markdown","38999c3e":"markdown","97b4349d":"markdown","dc887afc":"markdown","14770f5f":"markdown","0504c373":"markdown","6a6396ab":"markdown","bf121d91":"markdown","799d6467":"markdown","3aabf8bf":"markdown","7351fe10":"markdown","ef3b1b7f":"markdown","7fbfa70e":"markdown","228969bb":"markdown","feca14db":"markdown","23bf1bad":"markdown","ec1a4362":"markdown","c48c3876":"markdown","15804f76":"markdown","5428e292":"markdown","ffa85dce":"markdown"},"source":{"bf856f3b":"# import libraries\n\nimport numpy as np \nimport pandas as pd \nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve, KFold, train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.metrics import roc_curve, auc\n\nimport hyperopt\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\nSEED = 42","859e8dbc":"# load data\ntrain=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","d14a33f3":"# scatter plot function\ndef scatter_plot(var):\n    data = pd.concat([train['SalePrice'], train[var]], axis=1)\n    data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000), color=\"Purple\")","4ab808c9":"# plot GrLivArea to vs sale price to identify outliers\nscatter_plot('GrLivArea')","d4e00fe7":"# remove the two data points on the bottom right part of the plot\ntrain = train[train.GrLivArea < 4676]","73a6e606":"# merge train and test\ndf_all = train.append(test, ignore_index=True)\n\n# create indexes to separate data later on\ntrain_idx = len(train)\ntest_idx = len(df_all) - len(test)\n\n# save and drop id column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\ndf_all.drop(\"Id\", axis = 1, inplace = True)\n\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","20efafd4":"sns.distplot(train['SalePrice'] , fit=norm, color=\"Purple\");\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\n#print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nax = fig.add_subplot(111)\nres = stats.probplot(train['SalePrice'], plot=plt)\nax.get_lines()[0].set_markerfacecolor('Purple')\nax.get_lines()[0].set_markeredgecolor('Purple')\nplt.show()","2d9a74b5":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log(train[\"SalePrice\"])\ndf_all[\"SalePrice\"] = np.log(df_all[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm, color=\"Purple\");\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\n#print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nax = fig.add_subplot(111)\nres = stats.probplot(train['SalePrice'], plot=plt)\nax.get_lines()[0].set_markerfacecolor('Purple')\nax.get_lines()[0].set_markeredgecolor('Purple')\nplt.show()","1866e30e":"# rank features by % of missing values\ndef missing_percentage(df):\n    \"\"\"This function takes a DataFrame(df) as input and returns two columns, total missing values and total missing values percentage\"\"\"\n    ## the two following line may seem complicated but its actually very simple. \n    total = df.isnull().sum().sort_values(ascending = False)[df.isnull().sum().sort_values(ascending = False) != 0]\n    percent = round(df.isnull().sum().sort_values(ascending = False)\/len(df)*100,2)[round(df.isnull().sum().sort_values(ascending = False)\/len(df)*100,2) != 0]\n    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])\n\nmissing_percentage(df_all)","c64e842c":"# Show unique values in a given column\n#uniques = df_all['Functional'].unique()\n#uniques\n\n#df_all['Utilities'].value_counts()","71e79b39":"df_all['MSZoning'] = df_all.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\ndf_all[\"LotFrontage\"] = df_all.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))","7b6503ee":"for col in (\n    'PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'MasVnrType',\n    'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n    'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'\n    ):\n    df_all[col] = df_all[col].fillna(\"None\")","fe3dde14":"for col in (\n    'LotFrontage', 'MasVnrArea',\n    'GarageYrBlt', 'GarageArea', 'GarageCars',\n    'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath',\n    ):\n    df_all[col] = df_all[col].fillna(0)","f0a83322":"df_all['Electrical'] = df_all['Electrical'].fillna(df_all['Electrical'].mode()[0])\ndf_all['KitchenQual'] = df_all['KitchenQual'].fillna(df_all['KitchenQual'].mode()[0])\ndf_all['Exterior1st'] = df_all['Exterior1st'].fillna(df_all['Exterior1st'].mode()[0])\ndf_all['Exterior2nd'] = df_all['Exterior2nd'].fillna(df_all['Exterior2nd'].mode()[0])\ndf_all['SaleType'] = df_all['SaleType'].fillna(df_all['SaleType'].mode()[0])\ndf_all['Functional'] = df_all['Functional'].fillna(df_all['Functional'].mode()[0])","056b627b":"# all but one row has the same value so just drop Utilities\ndf_all = df_all.drop(['Utilities'], axis=1)","7aadb3bb":"# check that only Sale Price values are missing\nmissing_percentage(df_all)","5241d4ba":"#uniques = df_all['MSZoning'].unique()\n#uniques","c517bce9":"# Adding total bathrooms feature \ndf_all['TotalPorchArea'] = df_all['OpenPorchSF'] + df_all['EnclosedPorch'] + df_all['3SsnPorch'] + df_all['ScreenPorch']\n\n# Adding total sqfootage feature \ndf_all['TotalSF'] = df_all['TotalBsmtSF'] + df_all['GrLivArea']\n\n# Adding total bathrooms feature \ndf_all['TotalBath'] = df_all['BsmtFullBath'] + df_all['BsmtHalfBath'] + df_all['FullBath'] + df_all['HalfBath']\n\n# Garage Age\ndef garage_age(row):\n    if row['GarageYrBlt'] == 0:\n        return 0\n    return (row['YrSold'] - row['GarageYrBlt'])\ndf_all['GarageAge'] = df_all.apply(garage_age, axis=1)\n\ndf_all['haspool'] = df_all['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['has2ndfloor'] = df_all['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['hasgarage'] = df_all['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['hasbsmt'] = df_all['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['hasfireplace'] = df_all['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","99c50fe0":"# House Age \/ Remodelled and remodelling age\n\n# House Age at sale point\ndf_all['HouseAge'] = df_all['YrSold'] - df_all['YearBuilt']\n\n# Remodelled?\ndef remodelled(row):\n    if row['YearRemodAdd'] > row['YearBuilt']:\n        return 1\n    return 0\ndf_all['Remodelled'] = df_all.apply(remodelled, axis=1)\n\n# Modelling or remodelling age\ndef last_modelled(row):\n    year = max(row['YearBuilt'], row['YearRemodAdd'])\n    return row['YrSold'] - year\ndf_all['LastModelled'] = df_all.apply(last_modelled, axis=1)","547f1379":"df_all['MSSubClass'] = df_all['MSSubClass'].astype(str)\ndf_all['YrSold'] = df_all['YrSold'].astype(str)\ndf_all['MoSold'] = df_all['MoSold'].astype(str)","1e7848d6":"df_all.shape","a3d9e04e":"#correlation matrix\ncorrmat = df_all.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True, cmap=\"BuPu\");","0dc46854":"label_features = ['FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold']\n\nfor feature in label_features:        \n    df_all[feature] = LabelEncoder().fit_transform(df_all[feature])","c3f0133a":"cat_features = []\nfor i in df_all.columns:\n    if df_all[i].dtype == \"object\":\n        if i not in label_features:\n            cat_features.append(i)\n\nencoded_features = []\n\nfor feature in cat_features:\n    encoded_feat = OneHotEncoder().fit_transform(df_all[feature].values.reshape(-1, 1)).toarray()\n    n = df_all[feature].nunique()\n    cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n    encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n    encoded_df.index = df_all.index\n    encoded_features.append(encoded_df)\n    \nlen_cat = len(cat_features)\n\ndf_all = pd.concat([df_all, *encoded_features[:len_cat]], axis=1)","b755bda9":"# drop the old columns that have been one-hot encoded\ndf_all.drop(cat_features, axis=1, inplace=True)","dae4536a":"df_all.shape","691db367":"# create X, y and X_test dfs for modelling\nX = df_all[:train_idx].drop(['SalePrice'], axis=1)\ny = df_all['SalePrice'][:train_idx]\nX_test = df_all[train_idx:].drop(['SalePrice'], axis=1)","b1a779b0":"# ML imports\n\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","a0a0dd3a":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, X.values, y, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","8340ff82":"# Create X datasets that have been scaled with a Robust Scalar first\n\ntransformer = RobustScaler().fit(X)\nXt = transformer.transform(X)\nXt_test = transformer.transform(X_test)","0e38dde0":"# Lasso\n\nmax_evals = 10\n\nspace={\n        'alpha': hp.uniform(\"alpha\", 0.0001, 0.1),\n        'max_iter': hp.uniform(\"max_iter\", 200, 2000),\n    }\n\n# Regression: \ndef hyperparameter_tuning(space):\n    model=Lasso(\n        alpha =(space['alpha']),\n        max_iter =int(space['max_iter']),\n        random_state = 1\n        )\n    \n    scores = cross_val_score(model, Xt, y, scoring='neg_mean_squared_error', cv=KFold(n_splits=5,shuffle=True))\n    # print(scores)\n    \n    \n    # mse= metrics.mean_squared_error(y_test, pred)\n    print (\"SCORE: %0.4f\" % (scores.mean()))\n    #change the metric if you like\n    return {'loss':-scores.mean(), 'status': STATUS_OK }\n\ntrials = Trials()\nbest = fmin(fn=hyperparameter_tuning,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=max_evals,\n            trials=trials)\n\nprint (best)","99c1853c":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nscore = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","4e7beca5":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nscore = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","b0b512ee":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nscore = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","24a790ca":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, \n                             #silent=1,\n                             random_state =7, nthread = -1)\nscore = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","afb88384":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\nscore = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","26453513":"clf = RandomForestRegressor(n_estimators = 100,\n                   )\nscores = cross_val_score(clf, X, y, cv=5, scoring='neg_root_mean_squared_error')\nprint(scores.mean())","e34952e0":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1) ","a4c15433":"averaged_models = AveragingModels(models = (\n                                            lasso,\n                                            ENet,\n                                            GBoost,  \n                                            model_lgb,\n                                            model_xgb\n                                            ))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","e0c066f5":"averaged_models.fit(X, y)\nav_prices = np.exp(averaged_models.predict(X_test))","d63cec2b":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = av_prices\nimport os\nos.chdir('\/kaggle\/working')\nsub.to_csv('av_submission.csv',index=False)","1cb2733b":"# create train and validation set for blending\nX_train, X_t, y_train, y_t = train_test_split(X, y, test_size=0.33, random_state=42)","eb5e7598":"# train models using only the training set\nlasso_b = lasso.fit(X_train, y_train)\nenet_b = ENet.fit(X_train, y_train)\ngb_b = GBoost.fit(X_train, y_train)\nxgb_b = model_xgb.fit(X_train, y_train)\nlgb_b = model_lgb.fit(X_train, y_train)","bb832c2d":"# get validation set predictions from our models\ny_pred_lasso = pd.DataFrame(lasso_b.predict(X_t))\ny_pred_enet = pd.DataFrame(enet_b.predict(X_t))\ny_pred_gb = pd.DataFrame(gb_b.predict(X_t))\ny_pred_xgb = pd.DataFrame(xgb_b.predict(X_t))\ny_pred_lgb = pd.DataFrame(lgb_b.predict(X_t))","1e4ce008":"from sklearn.metrics import mean_squared_error\n\ny_pred=pd.DataFrame()\nbest_score = 100\nblend_increments = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\nbest_increments = []\n\nfor i in blend_increments:\n    for j in blend_increments:\n        for k in blend_increments:\n            for l in blend_increments:\n                for m in blend_increments:\n                    if (i+j+k+l) > 0:\n                        y_pred['SalePrice'] = (i * y_pred_lasso[0] + j * y_pred_enet[0] + k * y_pred_gb[0] + l * y_pred_xgb[0] + m * y_pred_lgb[0])\n                        y_pred['SalePrice'] = y_pred['SalePrice'] \/ (i+j+k+l+m)\n                        rms = np.sqrt(mean_squared_error(y_pred['SalePrice'], y_t))\n                        if rms < best_score:\n                            best_score = rms\n                            best_increments = [i, j, k, l, m]\n    \nprint(best_score)\nprint(best_increments)\n    \n#y_pred['SalePrice'] = 0.5 * y_pred_xgb[0] + 0.5 * y_pred_lgbm[0]\n#y_pred['Id'] = test['Id']","c3886b80":"# fir out models on the whole dataset\nlasso_b = lasso.fit(X, y)\nenet_b = ENet.fit(X, y)\ngb_b = GBoost.fit(X, y)\nxgb_b = model_xgb.fit(X, y)\nlgb_b = model_lgb.fit(X, y)","b8f920f6":"# make predictions on the true test set\nX_test = df_all[train_idx:].drop(['SalePrice'], axis=1)\ny_pred_lasso = pd.DataFrame(lasso_b.predict(X_test))\ny_pred_enet = pd.DataFrame(enet_b.predict(X_test))\ny_pred_gb = pd.DataFrame(gb_b.predict(X_test))\ny_pred_xgb = pd.DataFrame(xgb_b.predict(X_test))\ny_pred_lgb = pd.DataFrame(lgb_b.predict(X_test))","bc72e058":"y_pred = pd.DataFrame()","6740e2cf":"y_pred['SalePrice'] = np.exp((0.476 * y_pred_lasso[0] + 0 * y_pred_enet[0] + 0.381 * y_pred_gb[0] + 0.143 * y_pred_xgb[0] + 0 * y_pred_lgb[0]))","648d608b":"y_pred['Id'] = test_ID","b9b015e4":"import os\nos.chdir('\/kaggle\/working')\ny_pred.to_csv('blend_submission.csv',index=False)","d443709a":"from sklearn.ensemble import StackingRegressor\n\nestimators = [\n     ('lasso', lasso),\n     ('ENet', ENet),\n     ('model_gb', GBoost),\n     ('model_lgb', model_lgb),\n     ('model_xgb', model_xgb),\n ]\n\nreg = StackingRegressor(\n     estimators=estimators,\n     final_estimator=model_lgb,\n    passthrough = True\n    \n)\n#from sklearn.model_selection import train_test_split\n#X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n#reg.fit(X_train, y_train)\n\n\nscore = rmsle_cv(reg)\nprint(\"Ensemble score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","eed2e59d":"reg.fit(X, y)\nstack_prices = np.exp(reg.predict(X_test))","61438756":"stack_prices.shape","dc6b9169":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = stack_prices\nimport os\nos.chdir('\/kaggle\/working')\nsub.to_csv('stack_submission.csv',index=False)","a0408c22":"Let's look at a correlation heat map of the variables we have so far. There are a lot of vaiables so it's difficult to get much detail from the map but, as expected, there are a lot of correlated variables.","bd68bdb2":"# Outlier removal\n\nLet's look and see if there are any data points that really don't fit the overall pattern and remove them. These can have a big effect on regression tasks.","1a2b1110":"# Load Data","2739ddba":"# Handling Missing Data\n\nLots of columns have missing values. We need to fill these in with sensible estimates for our model to work.","ef463bf5":"Drop columns","38999c3e":"Iterate through every combination of proportion for each model to see which gives the best score.","97b4349d":"The two points on the bottom right of the scatter plot are properties with extremely large square footage of living area but fairly low sale price. Since most other data points diaply a reasonably nice correlation between living area and price it would be best to remove these two points. These two properties could be something like very rural areas where the same property size rules don't apply and \/ or properties in a very bad condition.","dc887afc":"**Label encoding** - for categorical variables with ordered values (ordinal)","14770f5f":"Let's test out some common models for regression.","0504c373":"**Numerical Features** - Replace missing values with 0","6a6396ab":"# Ensembling\n\nWe'll use a Stacking Regressor model with out lgb as a meta predictor. Passthrough is set to true so that the original features can be used in the predictions.","bf121d91":"# Feature Engineering\n\nLet's construct some new features from the ones we're given.","799d6467":"Define a method to carry out cross validation","3aabf8bf":"Turn categorical variables from integers to strings","7351fe10":"**Categorical Features** - replace missing values with \"None\"","ef3b1b7f":"# Modelling\n\nWe'll train a few models, see what kind of validation scores they get and then look at a few ways of combining these individual models - Averaging, Blending and Stacking.","7fbfa70e":"Fill missing values with the most common values using the **groupby function.**","228969bb":"Random Forest seems to perform significantly worse than linear or gradient boosting models for this problem, even when optimizing hyperparameters.","feca14db":"**One-Hot Encoding** - for categorical variables with no order","23bf1bad":"# Averaging\n\nTake clones of each model, ask them to predict each value and then take the average.","ec1a4362":"Replace missing values with **most common** value","c48c3876":"# Combine Train and Test\n\nCombine the datasets so that any feature engineering we do is applied to both.","15804f76":"Bayesian Hyperparameter Optimization: This code is more of an example as it can take a long time to work with the gradient boosting models. This is an effective method to find tune hyperparameters.","5428e292":"# Blending\n\nUsing a validation set, we'll try blending different proportions of the predictions from all the models to see which mixture give the lowest score.","ffa85dce":"# Transforming target distribution\n\nThe Sale Price target variable is skewed towards the high price end of the scale - it has a long tail of higher prices. For prediction purposes, it's better for the target variable to have a normal distribution. We can transform the target distribution by taking the log of Sale price."}}