{"cell_type":{"f2601084":"code","9bf90b37":"code","e469a6f9":"code","9b07276b":"code","12b8ad71":"code","2aa41190":"code","104b9b8b":"code","84712fcd":"code","6fbf781b":"code","2f070647":"code","42926c6f":"code","a152884f":"code","89f5075e":"code","bfeb4178":"code","615e1d93":"markdown","4e1017b5":"markdown","3fa4e21c":"markdown","2ff77ce8":"markdown"},"source":{"f2601084":"#loading necessary libraries :\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport random\nimport json\nimport pickle\nimport pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense , Activation , Dropout\nfrom tensorflow.keras.optimizers import SGD","9bf90b37":"# Defining a fonction for lemmatizing the sentences :\nlemmatizer = WordNetLemmatizer()","e469a6f9":"# Loading the json file that contains the intents\nintents = json.loads(open('..\/input\/intentsjson\/intents.json').read())","9b07276b":"words = []\nclasses = []\ndocuments = []\nfor intent in intents['intents']:\n    for pattern in intent['patterns']:\n        word_list = nltk.word_tokenize(pattern)\n        words.extend(word_list)\n        documents.append((word_list, intent['tag']))\n        if intent['tag'] not in classes:\n            classes.append(intent['tag'])","12b8ad71":"# Before procceding further we need first to clean the words list from marks like (? ; , ...)\nmarks = ['?' , ';' , ',' , '!' , '\/' , '\"\"' , '.' ,  ':' , '(' , ')' , '=' ]\nwords = [lemmatizer.lemmatize(word) for word in words if word not in marks]","2aa41190":"#Cleaning and sorting the two lists : words and classes\nWords = sorted(set(words))\nClasses = sorted(set(classes))","104b9b8b":"data = []\nempty = np.zeros(len(Classes))\nfor document in documents:\n    bag = []\n    patterns = document[0]\n    patterns = [lemmatizer.lemmatize(word.lower()) for word in patterns]\n    for word in Words:\n        bag.append(1) if word in patterns else bag.append(0)\n    target = list(empty)\n    target[Classes.index(document[1])] = 1\n    data.append([bag , target])","84712fcd":"random.shuffle(data)","6fbf781b":"data = np.array(data)","2f070647":"x_train = list(data[:,0])\ny_train = list(data[:,1])","42926c6f":"Model = Sequential()\nModel.add(Dense(128, input_shape=(len(x_train[0]),), activation = 'relu'))\nModel.add(Dropout(0.5))\nModel.add(Dense(64 , activation = 'relu'))\nModel.add(Dropout(0.5))\nModel.add(Dense(len(y_train[0]) , activation = 'softmax'))","a152884f":"sgd = SGD(lr = 0.01 , decay = 1e-6 , momentum = 0.9 , nesterov = True )\nModel.compile(loss='categorical_crossentropy' , optimizer=sgd , metrics=['accuracy'])","89f5075e":"Model.fit(np.array(x_train) , np.array(y_train) , epochs=200 , batch_size=10 , verbose=1)\nprint('Done')","bfeb4178":"def Clean_up(sentence):\n    sentence_words = nltk.word_tokenize(sentence)\n    sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]\n    return sentence_words\n\n\ndef bag_words(sentence):\n    sentence_words = Clean_up(sentence)\n    bag = [0] * len(Words) \n    for w in sentence_words:\n        for i , word in enumerate(Words):\n            if word == w:\n                bag[i]=1\n    return np.array(bag)\n\ndef pred_class(sentence):\n    bow = bag_words(sentence)\n    res = Model.predict(np.array([bow]))[0]\n    Error_Threshold = 0.25\n    results = [[i , r] for i , r in enumerate(res) if r > Error_Threshold ]\n    \n    results.sort(key=lambda x: x[1], reverse=True)\n    return_list = []\n    for r in results:\n        return_list.append({'intent' : Classes[r[0]] , 'probability' : str(r[1])})\n    return return_list\n\ndef get_responses(intents_list , intents_json):\n    tag = intents_list[0]['intent']\n    list_of_intents = intents_json['intents']\n    for i in list_of_intents:\n        if i['tag'] == tag:\n            result = random.choice(i['responses'])\n            break\n    return result\n\n\nprint('ChatBot is ready...')\n\nwhile True:\n   message = input(\"\")\n   ints = pred_class(message)\n   res = get_responses(ints , intents)\n   print(res)\n\n","615e1d93":"#### Now that our model is fited we could prepare the console for our first conversation with the chatbot","4e1017b5":"#### Creating lists for words, classes and documents where words contains all the words we used in the patterns in the intents file, classes contains the classes or tags and the documents list will contain the words from the words list with there specific classes ","3fa4e21c":"####  once we have our features and target variable we are ready to fit the model","2ff77ce8":"#### It's time for our data preparation for applying the machine learning model so for that we need to convert words to 0 and 1 so that the model could understand it's a method called bag of words"}}