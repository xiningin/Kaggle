{"cell_type":{"40cb12b8":"code","ac2bc090":"code","e4a29341":"code","61777d23":"code","71caa36b":"code","ec4b4fbd":"code","389e36a7":"code","790d0cde":"code","3483b52b":"code","d3cd9f26":"code","5850e7ae":"code","67f573c0":"markdown","941b7f22":"markdown"},"source":{"40cb12b8":"#Required Libraries\nimport gym\nimport numpy as np\nimport random as rm","ac2bc090":"\n\nenv = gym.make('FrozenLake-v0')\nenv.reset()\nenv.render()\n ","e4a29341":"def extract_policy(value_table, gamma):\n \n    #Initialize the policy with zero\n    policy = np.zeros(env.observation_space.n) \n    \n    \n    for state in range(env.observation_space.n):\n        \n        #Initialize the Q table for a state\n        Q_table = np.zeros(env.action_space.n)\n        \n        #Compute Q value for all ations in the state\n        for action in range(env.action_space.n):\n            for next_sr in env.P[state][action]: \n                trans_prob, next_state, reward, _ = next_sr \n                Q_table[action] += (trans_prob * (reward + gamma * value_table[next_state]))\n        \n        #Select the action which has maximum Q value as an optimal action of the state\n        policy[state] = np.argmax(Q_table)\n    \n    return policy","61777d23":"def compute_value_function(policy, gamma):\n    \n    #Initialize value table with zeros\n    value_table = np.zeros(env.observation_space.n)\n    \n    #Set the threshold\n    threshold = 1e-10\n    \n    while True:\n        \n        #Copy the value table to the updated_value_table\n        updated_value_table = np.copy(value_table)\n\n        #Select the action according to the policy and compute the value table\n        for state in range(env.observation_space.n):\n            action = policy[state]\n            \n            #Build the value table with the selected action\n            value_table[state] = sum([trans_prob * (reward_prob + gamma * updated_value_table[next_state]) \n                        for trans_prob, next_state, reward_prob, _ in env.P[state][action]])\n            \n        if (np.sum((np.fabs(updated_value_table - value_table))) <= threshold):\n            break\n            \n    return value_table","71caa36b":"def policy_iteration(env, gamma):\n    \n    #Initialize policy with zeros\n    old_policy = np.zeros(env.observation_space.n)   \n    max_iters = 10000\n    \n    for i in range(max_iters):\n        \n        #Compute the value function\n        new_value_function = compute_value_function(old_policy, gamma)\n        \n        #Extract new policy from the computed value function\n        new_policy = extract_policy(new_value_function, gamma)\n   \n        #Check Convergence\n        if (np.all(old_policy == new_policy)):\n            print ('Policy-Iteration converged at step #', (i+1))\n            break\n        old_policy = new_policy\n        \n    return new_policy","ec4b4fbd":"\noptimal_policy = policy_iteration(env, gamma = 0.9)\nprint (optimal_policy)","389e36a7":"#Run so many episodes\ndef run_episodes(env, policy, num_games = 1000):\n    \n    tot_rew = 0\n    state = env.reset()\n\n    for _ in range(num_games):\n        done = False\n        \n        while not done:\n            #Select the action accordingly to the policy\n            next_state, reward, done, _ = env.step(policy[state])\n                \n            state = next_state\n            tot_rew += reward \n            if done:\n                state = env.reset()\n\n    print('Won {} of {} games!'.format(tot_rew, num_games))","790d0cde":"env.reset()\nrun_episodes(env, optimal_policy, num_games = 1000)","3483b52b":"# Custom Map\ncustom_map = [\n    'SFFFH', \n    'FFHFF',\n    'HFFFH',\n    'HFFFH',\n    'HHHFG'\n]\n\nenv = gym.make('FrozenLake-v0', desc = custom_map)\nenv.reset()\nenv.render()","d3cd9f26":"optimal_policy = policy_iteration(env, gamma = 0.9)\nprint (optimal_policy)","5850e7ae":"env.reset()\nrun_episodes(env, optimal_policy, num_games = 1000)","67f573c0":"## Policy Iteration","941b7f22":"# Try another env"}}