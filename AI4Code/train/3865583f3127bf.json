{"cell_type":{"52aa2a6e":"code","cba00995":"code","f248513a":"code","114f8055":"code","457d21e0":"code","ee2c1f4a":"code","81e74046":"code","43576420":"code","fb7e3384":"code","3462c063":"code","d4f2ecd4":"code","87819ba6":"code","4444e69e":"code","a794caca":"code","6a026cd1":"code","1e10dcf9":"code","6c71831b":"code","84507316":"code","7dc2ee3f":"code","7229d3aa":"code","6cc16a40":"code","4e1fbeaa":"code","21056948":"code","bd0b3dd7":"code","4d62d073":"code","04c87646":"code","5af35519":"code","0fb344f6":"code","71e01aa4":"code","4bb0742c":"code","d49683c8":"code","56bbd042":"code","97e8f4cf":"code","8a14d4ce":"code","8110db2d":"code","8e02120a":"code","50494893":"code","fe20cca1":"code","7e867592":"code","a8c6068a":"code","37251d7b":"code","101fedba":"code","3e4e8d34":"code","0515c809":"code","19f2a650":"code","b73c8918":"code","262e0d85":"code","ebd74818":"code","8c6a8a47":"code","3793dbfd":"code","6714b2c5":"code","a15b5920":"code","1a8c8f04":"code","92cffc0b":"code","5bd5d66d":"code","78a92bc0":"code","8d094003":"code","fa5ba03d":"code","4d8b0114":"code","d5f6ac96":"code","08b34b74":"code","f3d22d4c":"code","4b863521":"code","705257cf":"markdown","b9db8af9":"markdown","f8963323":"markdown","298911b5":"markdown","21f4320f":"markdown","fa68a367":"markdown","3ce6b4bf":"markdown","93c0b538":"markdown","834855c6":"markdown","2fab5441":"markdown","ea6b7f2a":"markdown","01b78e89":"markdown","9c84a1f4":"markdown","5474eee0":"markdown","772442e2":"markdown","b1599444":"markdown","598395d1":"markdown","ae94575b":"markdown","37e1cd34":"markdown","79700806":"markdown","75b6992b":"markdown","53261171":"markdown","d3df2970":"markdown","f262b17c":"markdown","0555dfc1":"markdown","8c88fac4":"markdown","bb74db5e":"markdown","04538557":"markdown","ca280e45":"markdown","1f218b0e":"markdown","38aee95a":"markdown","fd46da96":"markdown","5156f6d1":"markdown","2a8e5ca3":"markdown","025cb2b4":"markdown","67830bab":"markdown","7dcd2732":"markdown","d4ae41a5":"markdown","5c0dcf47":"markdown","477651b6":"markdown","461cab2c":"markdown","008017f0":"markdown","a4fe7823":"markdown","61e60a53":"markdown","4fc64dd3":"markdown","43accb3c":"markdown","81da2979":"markdown","9c934085":"markdown","86cd7f75":"markdown","d62f5309":"markdown","e0463cbc":"markdown","e57adf46":"markdown","59d10d5f":"markdown","5b8880ac":"markdown","735024e0":"markdown","a631c8de":"markdown","a0d8a2e9":"markdown","9a0e071f":"markdown","78217735":"markdown","cafe1d25":"markdown"},"source":{"52aa2a6e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport re #regex applies a regular expression to a string and returns the matching substrings. \nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport nltk \nfrom sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing\nimport nltk.corpus\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import BlanklineTokenizer\nfrom nltk.tokenize import TweetTokenizer\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('stopwords')\nfrom nltk.stem import WordNetLemmatizer\nimport string\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('indonesian'))\nimport re\nimport scikitplot as skplt\nfrom nltk.tokenize import word_tokenize\nimport gensim\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nfrom nltk.stem import PorterStemmer \nfrom nltk.tokenize import word_tokenize\nfrom wordcloud import WordCloud\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pickle\nfrom multiprocessing import Pool","cba00995":"train_data = pd.read_csv('..\/input\/stmmnlp\/primary.csv')\ntest_data  = pd.read_csv('..\/input\/stmmnlp\/pred.csv')\n","f248513a":"train_data.head(10)","114f8055":"test_data.head(10)","457d21e0":"print('There are {} rows and {} columns in train'.format(train_data.shape[0],train_data.shape[1]))\nprint('There are {} rows and {} columns in train'.format(test_data.shape[0],test_data.shape[1]))","ee2c1f4a":"train_data.dtypes","81e74046":"#Let's display one the tweets existed in the text column \ntrain_data['text'][11]","43576420":"x=train_data.label.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","fb7e3384":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train_data[train_data['label']==1]['text'].str.len()\nax1.hist(tweet_len,color='blue')\nax1.set_title('Clickbait')\ntweet_len=train_data[train_data['label']==0]['text'].str.len()\nax2.hist(tweet_len,color='CRIMSON')\nax2.set_title('Non Clickbait')\nfig.suptitle('Characters in tweets')\nplt.show()","3462c063":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train_data[train_data['label']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='blue')\nax1.set_title('Clickbait')\ntweet_len=train_data[train_data['label']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='CRIMSON')\nax2.set_title('Non Clickbait')\nfig.suptitle('Words in a Headline')\nplt.show()\n","d4f2ecd4":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=train_data[train_data['label']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('Clickbait')\nword=train_data[train_data['label']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Non Clickbait')\nfig.suptitle('Average word length in each Headline')","87819ba6":"\ncorpus=[]\n    \nfor x in train_data['text'].str.split():\n    for i in x:\n        corpus.append(i)\n","4444e69e":"#corpus\n#the output is hidden.","a794caca":"dic=defaultdict(int)\ndic=defaultdict(int)\nfor word in corpus:\n    if word not in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:30] \n    \n\n\nx,y=zip(*top)\nplt.rcParams[\"figure.figsize\"] = (20,10)\nplt.bar(x,y , color ='red')","6a026cd1":"from nltk.corpus import stopwords\nstop = stopwords.words('indonesian')\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:30] \n    \n\n\nx,y=zip(*top)\nplt.rcParams[\"figure.figsize\"] = (20,10)\n#There is also this workaround in case you want to change the size without using the figure environment.\n#So in case you are using plt.plot() for example, you can set a tuple with width and height.\nplt.bar(x,y , color ='green')","1e10dcf9":"plt.figure(figsize=(10,5))\nimport string\ndic=defaultdict(int)\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n      \nx,y=zip(*dic.items())\nplt.barh(x,y ,color = 'purple')","6c71831b":"from collections import Counter\n\nwords = train_data[train_data.label==0].text.apply(lambda x: [word.lower() for word in x.split()])\nh_words = Counter()\n\nfor text_ in words:\n    h_words.update(text_)\n    \nprint(h_words.most_common(50))","84507316":"words = train_data[train_data.label==1].text.apply(lambda x: [word.lower() for word in x.split()])\nh_words = Counter()\n\nfor text_ in words:\n    h_words.update(text_)\n    \nprint(h_words.most_common(50))","7dc2ee3f":"''''missing_cols = ['keyword', 'location']\n\nfig, axes = plt.subplots(ncols=2, figsize=(17, 4), dpi=100)\n\nsns.barplot(x=train_data[missing_cols].isnull().sum().index, y=train_data[missing_cols].isnull().sum().values, ax=axes[0])\nsns.barplot(x=train_data[missing_cols].isnull().sum().index, y=train_data[missing_cols].isnull().sum().values, ax=axes[1])\n\naxes[0].set_ylabel('Missing Value Count', size=15, labelpad=20)\naxes[0].tick_params(axis='x', labelsize=15)\naxes[0].tick_params(axis='y', labelsize=15)\naxes[1].tick_params(axis='x', labelsize=15)\naxes[1].tick_params(axis='y', labelsize=15)\n\naxes[0].set_title('Training Set', fontsize=13)\naxes[1].set_title('Test Set', fontsize=13)\n\nplt.show()\n\nfor df in [train_data, test_data]:\n    for col in ['keyword', 'location']:\n        df[col] = df[col].fillna(f'no_{col}')'''\n","7229d3aa":"print(f'Number of unique values in keyword = {train_data[\"keyword\"].nunique()} (Training) - {test_data[\"keyword\"].nunique()} (Test)')\nprint(f'Number of unique values in location = {train_data[\"location\"].nunique()} (Training) - {test_data[\"location\"].nunique()} (Test)')","6cc16a40":"train_data['target_mean'] = train_data.groupby('keyword')['target'].transform('mean')\n\nfig = plt.figure(figsize=(8, 72), dpi=100)\n\nsns.countplot(y=train_data.sort_values(by='target_mean', ascending=False)['keyword'],\n              hue=train_data.sort_values(by='target_mean', ascending=False)['target'])\n\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=12)\nplt.legend(loc=1)\nplt.title('Target Distribution in Keywords')\n\nplt.show()\n\ntrain_data.drop(columns=['target_mean'], inplace=True)\n","4e1fbeaa":"def  clean_text(df, text_field, new_text_field_name):\n    df[new_text_field_name] = df[text_field].str.lower() #Convert strings in the Series\/Index to lowercase.\n    \n    # remove numbers\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n    #remove url\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"https?:\/\/\\S+|www\\.\\S+\", \"\", elem))\n    #remove HTML tags\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"<.*?>\", \"\", elem))\n    #remove emojis \n    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", \"\", elem))\n    return df\ndata_clean = clean_text(train_data, 'text', 'text_clean')\ndata_clean_test = clean_text(test_data,'text', 'text_clean')\ndata_clean.head()","21056948":"from nltk.corpus import stopwords\nstop = stopwords.words('indonesian')\ndata_clean['text_clean'] = data_clean['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ndata_clean.head()","bd0b3dd7":"\nfrom nltk.tokenize import sent_tokenize, word_tokenize\ndata_clean['text_tokens'] = data_clean['text_clean'].apply(lambda x: word_tokenize(x))\ndata_clean.head()","4d62d073":"from nltk.stem import PorterStemmer \nfrom nltk.tokenize import word_tokenize\ndef word_stemmer(text):\n    stem_text = [PorterStemmer().stem(i) for i in text]\n    return stem_text\ndata_clean['text_clean_tokens'] = data_clean['text_tokens'].apply(lambda x: word_stemmer(x))\ndata_clean.head()","04c87646":"\nfrom nltk.stem import WordNetLemmatizer\ndef word_lemmatizer(text):\n    lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]\n    return lem_text\ndata_clean['text_clean_tokens'] = data_clean['text_tokens'].apply(lambda x: word_lemmatizer(x))\ndata_clean.head()","5af35519":"'''!pip3 install pyspellchecker==20.2.2\nfrom spellchecker import SpellChecker\n\nspell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n\ndata_clean['text_clean'] = data_clean['text_clean'].apply(lambda x: correct_spellings(x))'''","0fb344f6":"X_train, X_test, Y_train, Y_test = train_test_split(data_clean['text_clean'], \n                   \n                                                    data_clean['label'], \n                                                    test_size = 0.2,\n                                                    random_state = 10)","71e01aa4":"print(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","4bb0742c":"vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1))\nvectorized = vectorizer.fit_transform(X_train)\npd.DataFrame(vectorized.toarray(), \n            index=['sentence '+str(i) \n                   for i in range(1, 1+len(X_train))],\n            columns=vectorizer.get_feature_names())","d49683c8":"#Only alphabet, contains at least 3 letters\nvectorizer = CountVectorizer(analyzer='word', \n                              token_pattern=r'\\b[a-zA-Z]{3,}\\b',  \n                              ngram_range=(1, 1))\nvectorized = vectorizer.fit_transform(X_train)\npd.DataFrame(vectorized.toarray(), \n             index=['sentence '+str(i) \n                    for i in range(1, 1+len(X_train))],\n             columns=vectorizer.get_feature_names())","56bbd042":"vectorizer = CountVectorizer(analyzer='word', \n                              token_pattern=r'\\b[a-zA-Z]{3,}\\b',  \n                              ngram_range=(2, 2))  # only bigrams\nvectorized = vectorizer.fit_transform(X_train)\npd.DataFrame(vectorized.toarray(), \n             index=['sentence '+str(i) \n                    for i in range(1, 1+len(X_train))],\n             columns=vectorizer.get_feature_names())","97e8f4cf":"# consider both unigrams and bigrams, occur at least twice\nvectorizer = CountVectorizer(analyzer='word', \n                              token_pattern=r'\\b[a-zA-Z]{3,}\\b',  \n                              ngram_range=(1, 2),                                             min_df = 2)  \nvectorized = vectorizer.fit_transform(X_train)\npd.DataFrame(vectorized.toarray(), \n             index=['sentence '+str(i) \n                    for i in range(1, 1+len(X_train))],\n             columns=vectorizer.get_feature_names())","8a14d4ce":"from sklearn.feature_extraction.text import (CountVectorizer, \n                                             TfidfVectorizer,\n                                             TfidfTransformer)\nvectorizer = CountVectorizer(analyzer='word', \n                              token_pattern=r'\\b[a-zA-Z]{3,}\\b',  \n                              ngram_range=(1, 1) \n                              )  \ncount_vectorized = vectorizer.fit_transform(X_train)\ntfidf = TfidfTransformer(smooth_idf=True, use_idf=True)\ntrain_features = tfidf.fit_transform(count_vectorized).toarray()\n\npd.DataFrame(train_features, \n             index=['sentence '+str(i) \n                    for i in range(1, 1+len(X_train))],\n             columns=vectorizer.get_feature_names())","8110db2d":"# Convert a collection of text documents to a matrix of token counts\ntfidf = feature_extraction.text.TfidfVectorizer(encoding='utf-8',\n                       ngram_range=(1,1),\n                       max_features=5000,\n                       norm='l2',\n                       sublinear_tf=True)","8e02120a":"train_features = tfidf.fit_transform(X_train).toarray()","50494893":"print(train_features[1])\nprint(train_features.shape)","fe20cca1":"pd.DataFrame(train_features, \n             index=['sentence '+str(i) \n                    for i in range(1, 1+len(X_train))],\n             columns=tfidf.get_feature_names())","7e867592":"dic_vocabulary = tfidf.vocabulary_","a8c6068a":"word = \"bukti\"\ndic_vocabulary[word]\n#If the word exists in the vocabulary, \n#this command prints a number N, \n#meaning that the Nth feature of the matrix is that word.","37251d7b":"test_features = tfidf.transform(X_test).toarray()\nprint(test_features.shape)","101fedba":"train_labels = Y_train\ntest_labels = Y_test","3e4e8d34":"\"\"\"from sklearn import feature_selection \ny = data_clean['target']\nX_names = tfidf.get_feature_names()\np_value_limit = 0.95\ndtf_features = pd.DataFrame()\nfor cat in np.unique(y):\n    chi2, p = feature_selection.chi2(X_train, y==cat)\n    dtf_features = dtf_features.append(pd.DataFrame(\n                   {\"feature\":X_names, \"score\":1-p, \"y\":cat}))\n    dtf_features = dtf_features.sort_values([\"y\",\"score\"], \n                    ascending=[True,False])\n    dtf_features = dtf_features[dtf_features[\"score\"]>p_value_limit]\nX_names = dtf_features[\"feature\"].unique().tolist()\nlen(X_names)\"\"\"","0515c809":"import pandas as pd\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score","19f2a650":"mnb_classifier = MultinomialNB()","b73c8918":"mnb_classifier.fit(train_features,train_labels)","262e0d85":"mnb_prediction = mnb_classifier.predict(test_features)","ebd74818":"training_accuracy = accuracy_score(train_labels, mnb_classifier.predict(train_features))\nprint(training_accuracy)","8c6a8a47":"testing_accuracy = accuracy_score(test_labels, mnb_prediction)\nprint(testing_accuracy)","3793dbfd":"print(classification_report(test_labels, mnb_prediction))","6714b2c5":"conf_matrix = confusion_matrix(test_labels, mnb_prediction)\nprint(conf_matrix)","a15b5920":"import seaborn as sns\nsns.heatmap(conf_matrix\/np.sum(conf_matrix),annot=True, fmt='.2%', cmap='Blues')","1a8c8f04":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier().fit(train_features, train_labels)","92cffc0b":"predicts = knn.predict((test_features))\nprint(classification_report(test_labels, predicts)) ","5bd5d66d":"test_vectorizer =tfidf.transform( data_clean_test['text_clean']).toarray()","78a92bc0":"test_vectorizer.shape","8d094003":"final_predictions = mnb_classifier.predict(test_vectorizer)","fa5ba03d":"final_predictions","4d8b0114":"submission_df = pd.DataFrame()","d5f6ac96":"submission_df['id'] = data_clean_test['id']\nsubmission_df['target'] = final_predictions","08b34b74":"submission_df","f3d22d4c":"submission_df['target'].value_counts()","4b863521":"submission = submission_df.to_csv('Hasil Pred.csv',index = False)","705257cf":"<font size='5' color='Black'> WORK IN PROGRESS ...<\/font>","b9db8af9":"<a id=\"3\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center>  Exploratory Data Analysis <\/center><\/h2>","f8963323":"<a id=\"4.9\"><\/a>\n<font color=\"blue\" size=+2.5><b> Bag of words <\/b><\/font>","298911b5":"1. ***Missing Values***\n\nBoth training and test set have same ratio of missing values in keyword and location.\n\n0.8% of keyword is missing in both training and test set\n33% of location is missing in both training and test set\nSince missing value ratios between training and test set are too close, they are most probably taken from the same sample. Missing values in those features are filled with no_keyword and no_location respectively.","21f4320f":"Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph","fa68a367":"<font size=\"+3\" color=Green><b> <center><u>Text classification step by step<\/u><\/center><\/b><\/font>","3ce6b4bf":"In details about each target ","93c0b538":"Consider only unigrams\/bigrams\/\u2026 tokens\n* ngram-range=(1, 1): unigram only\n* ngram-range=(2, 2): bigrams only\n* ngram-range=(1, 2): both unigrams and bigrams","834855c6":"1. With Tfidftransformer you will systematically compute word counts using CountVectorizer and then compute the Inverse Document Frequency (IDF) values and only then compute the TF-IDF scores.\n2. With Tfidfvectorizer on the contrary, you will do all three steps at once. Under the hood, it computes the word counts, IDF values, and TF-IDF scores all using the same data set.","2fab5441":"Natural Language Processing (NLP) is a subfield of computer science, artificial intelligence, information engineering, and human-computer interaction. This field focuses on how to program computers to process and analyze large amounts of natural language data. It is difficult to perform as the process of reading and understanding languages is far more complex than it seems at first glance.","ea6b7f2a":"**The regular expression**\n\nabove is meant to find any four digits at the beginning of a string, which suffices for our case. The above is a raw string (meaning that a backslash is no longer an escape character), which is standard practice with regular expressions.\nregex = r'^(\\d{4})'\n","01b78e89":"<a id=\"2.1\"><\/a>\n<font color=\"blue\" size=+2.5><b> Import Libraries <\/b><\/font>","9c84a1f4":"**Number of words in a tweet**","5474eee0":"<a id=\"5.0\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center>  Model training <\/center><\/h2>","772442e2":"<a id=\"4.0\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center>  Data Cleaning  <\/center><\/h2>","b1599444":"<font color=\"black\" size=+1.5><b>CountVectorizer<\/b><\/font>\n\nCountVectorizer converts a collection of text documents to a matrix of token counts: the occurrences of tokens in each document. This implementation produces a sparse representation of the counts.","598395d1":"# Fitting the Test data for submission","ae94575b":"<a id=\"3.3\"><\/a>\n<font color=\"blue\" size=+2.5><b> Explaratory analysis of Other Variables <\/b><\/font>","37e1cd34":"The distribution of both seems to be almost same.120 t0 140 characters in a tweet are the most common among both.","79700806":"**Keyword and Location**","75b6992b":"**Frequencies**\nNow we want to count the frequency of each word in our corpus.","53261171":"<a id=\"5.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Model : Multinomial NB <\/b><\/font>","d3df2970":"**What are Stop words?**\n\n\nStop Words: A stop word is a commonly used word (such as \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cin\u201d) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\n\nWe would not want these words to take up space in our database, or taking up valuable processing time. For this, we can remove them easily, by storing a list of words that you consider to stop words. NLTK(Natural Language Toolkit) in python has a list of stopwords stored in 16 different languages. You can find them in the nltk_data directory. home\/pratima\/nltk_data\/corpora\/stopwords is the directory address.(Do not forget to change your home directory name)","f262b17c":"<a id=\"5.1\"><\/a>\n<font color=\"blue\" size=+2.5><b> reduce the matrix dimensionality <\/b><\/font>","0555dfc1":"![image.png](attachment:image.png)","8c88fac4":"\n<a id=\"1\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center>  Introduction  <\/center><\/h2>","bb74db5e":"**Average word length in a tweet**","04538557":"![image.png](attachment:image.png)","ca280e45":"Note that for each sentence in the corpus, the position of the tokens (words in our case) is completely ignored. When constructing this bag-of-words representation, the default configuration tokenize the string by extracting words of at least 2 alphanumeric characters (punctuation is completely ignored and always treated as a token separator)","1f218b0e":"<a id=\"2.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Import Dataset <\/b><\/font>\n","38aee95a":" in order to reduce the dimensionality of our matrix ! [Feature matrix shape: Number of documents x Length of vocabulary ] we can carry out some Feature Selection, the process of selecting a subset of relevant variables. I will proceed as follows:\n\ntreat each category as binary (for example, the \u201cTech\u201d category is 1 for the Tech news and 0 for the others);\nperform a Chi-Square test to determine whether a feature and the (binary) target are independent;\nkeep only the features with a certain p-value from the Chi-Square test.","fd46da96":"Consider only certain pattern\nWe can also specify the desired pattern for our token using token_pattern argument. In the following example, we will only consider tokens consists of at least 3 alphabets.","5156f6d1":" Tfidftransformer ","2a8e5ca3":"Tfidfvectorizer","025cb2b4":"<a id=\"4.2\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center>  Tokenizing  <\/center><\/h2>","67830bab":"<a id=\"3.1\"><\/a>\n<font color=\"blue\" size=+2.5><b> Target variable distribution <\/b><\/font>","7dcd2732":"First,we will do very basic analysis,that is character level,word level and sentence level analysis.","d4ae41a5":"<a id=\"4.4\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center>  Lemmatization  <\/center><\/h2>","5c0dcf47":"2. Cardinality and Target Distribution\n\nLocations are not automatically generated, they are user inputs. That's why location is very dirty and there are too many unique values in it. It shouldn't be used as a feature.\n\nFortunately, there is signal in keyword because some of those words can only be used in one context. Keywords have very different tweet counts and target means. keyword can be used as a feature by itself or as a word added to the text. Every single keyword in training set exists in test set. If training and test set are from the same sample, it is also possible to use target encoding on keyword.","477651b6":"In order to know the position of a certain word, we can look it up in the vocabulary:","461cab2c":"<h2 class=\"list-group-item list-group-item-action active\"  data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\" > <center>Objective  <\/center><\/h2>\n\nGoals of this kernel are the following:\n- Basic Exploratory Data Analysis.\n- Beginners guide to clean the dataset.\n- Feature Analysis & extraction \n- Modelling and ecvaluation metrics \n- submitting the results ","008017f0":"<a id=\"4.1\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center>  Removing Stop words <\/center><\/h2>","a4fe7823":"Before we begin with anything else,let's check the class distribution.There are only two classes 0 and 1.","61e60a53":"<a id=\"4.3\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center>  Stemming  <\/center><\/h2>","4fc64dd3":"**TfidfTransformer v.s. Tfidfvectorizer**\n\n\nBoth TfidfTransformer and Tfidfvectorizer modules can convert a collection of raw documents to a matrix of TF-IDF features. However,","43accb3c":"![image.png](attachment:image.png)","81da2979":"<a id=\"2\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center>  Load and Check Data  <\/center><\/h2>","9c934085":"<a id=\"5.0\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center>  Feature Extraction : tf-idf <\/center><\/h2>","86cd7f75":"There are more tweets with class 0 ( No disaster) than class 1 ( disaster tweets)","d62f5309":"<a id=\"4.5\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center>  Splitting the data <\/center><\/h2>","e0463cbc":"<a id=\"top\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center> Table of content <\/center><\/h2>\n\n<font color=\"blue\" size=+1><b>Introduction<\/b><\/font>\n* [About Data ](#1.1)\n    \n<font color=\"blue\" size=+1><b> Load and Check Data <\/b><\/font>\n* [Importing Library](#2.1)\n* [Load Dataset](#2.2)\n\n<font color=\"blue\" size=+1><b> Exploratory Data Analysis <\/b><\/font>\n* [Target variable distribution](#3.1)\n* [Explaratory analysis of tweets](#3.2)    \n* [Explaratory analysis of Other Variables](#3.3)   \n\n\n<font color=\"blue\" size=+1><b> Data Preparation <\/b><\/font>\n    \n* [Data cleaning ](#4.0)\n        \n* [Removing STOP words ](#4.1)\n           \n* [Tokonezing](#4.2)\n        \n* [Stemming](#4.3)\n        \n* [Lemmatization](#4.4)\n        \n* [Splitting the data](#4.5)\n    \n<font color=\"blue\" size=+1><b> Feature Extraction <\/b><\/font>\n* [Bag of words](#4.9)\n* [Tf-idf Vectorizer](#5.0)\n* [reduce the dimensionality of the Matrix](#5.1)\n    \n    \n<font color=\"blue\" size=+1><b> Model Training <\/b><\/font>\n    \n* [MultinomialNB ](#5.2)\n\n\n","e57adf46":"Note 1000, be, is, of are removed from the original feature space.","59d10d5f":"<font color=\"black\" size=+1.5><b> Vectorization is the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the Bag of Words or \u201cBag of n-grams\u201d representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document. <\/b><\/font>","5b8880ac":"# Visualizing scikit model performance","735024e0":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Explaratory analysis of tweets <\/b><\/font>","a631c8de":"\n<font size='5' color='red'>  if you liked this kernel,please upvote it <3 <\/font>\n    \n\n","a0d8a2e9":"**Consider only tokens with certain frequency**\n\n\nWe can also make the vectorizer to ignore terms that have a document frequency strictly lower than a specified threshold by setting min_df = threshold or max_df = threshold for higher frequency terms.","9a0e071f":"Number of characters in tweets","78217735":"Let's take a look to the punctuations in our tweets : ","cafe1d25":"<a id=\"1.1\"><\/a>\n<font color=\"blue\" size=+2.5><b> About Data<\/b><\/font>\n<br\/>\n<br\/>\n* **What files do I need?**\n\nYou'll need train.csv, test.csv and sample_submission.csv.\n\n* **What should I expect the data format to be?**\n\nEach sample in the train and test set has the following information:\n\n1. The text of a tweet\n2. A keyword from that tweet (although this may be blank!)\n3. The location the tweet was sent from (may also be blank)\n\n*** What am I predicting?**\n\nYou are predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.\n\n* **Files**\n\n1. train.csv - the training set\n2. test.csv - the test set\n3. sample_submission.csv - a sample submission file in the correct format\n\n## The columns in this dataset are:\n\n* id - a unique identifier for each tweet\n* text - the text of the tweet\n* location - the location the tweet was sent from (may be blank)\n* keyword - a particular keyword from the tweet (may be blank)\n* target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n\n\n"}}