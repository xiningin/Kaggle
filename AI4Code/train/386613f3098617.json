{"cell_type":{"640f3346":"code","fc8b304d":"code","e3f62f14":"code","c9415e24":"code","a4bd45ad":"code","e212908e":"code","e0493de6":"code","6e0515c2":"code","39874b04":"code","6c564bbc":"code","318de33e":"code","ed57c43f":"code","17e42e62":"code","298bd492":"code","e0bd60c5":"code","f73e9c37":"code","dbf1b069":"code","ed6c44c1":"code","5d8997a3":"code","b45c1671":"code","3d20cdc2":"code","f6153cb5":"code","fb3e9419":"code","fbb68841":"code","fa1274fe":"code","6786ae24":"code","a38cba1f":"code","78d0be4d":"code","297b44c0":"code","29339e99":"code","46a78ff5":"code","6e2d3b47":"code","56060e88":"code","cf3cef9a":"code","bb455e6c":"code","ee11cd28":"code","201dfac4":"code","2a69ba2d":"code","fdea24df":"code","bb7621ee":"code","56089b9a":"code","876e4490":"code","7756baf6":"code","c2aa2a1f":"code","60e25a6b":"code","ca9a70bb":"code","9fce230f":"code","1436a63e":"code","cef8c157":"markdown","c0fc592e":"markdown","0fd9f276":"markdown","27a7d049":"markdown","c8235d0b":"markdown","7ac23302":"markdown","95ee62b2":"markdown","b64d5cce":"markdown","fcdb2b7f":"markdown","8b2a3e21":"markdown","73d087a4":"markdown","5467b5a2":"markdown","b1be05ac":"markdown","5f0e6976":"markdown","663943e8":"markdown","c5598bf4":"markdown","71f65cbe":"markdown","c84e58ae":"markdown","53cc7137":"markdown","a687ad0a":"markdown","605dc99b":"markdown","d152c108":"markdown","b52349da":"markdown","f37b5d52":"markdown","929fdabe":"markdown","ab843db4":"markdown","1f571d9e":"markdown","78974c5e":"markdown","70e4867d":"markdown","1c14fbfb":"markdown","d4d2fc48":"markdown"},"source":{"640f3346":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fc8b304d":"df=pd.read_csv(\"..\/input\/star-type-classification\/Stars.csv\") #Import data","e3f62f14":"df.head()","c9415e24":"df.dtypes","a4bd45ad":"df.isnull().sum()","e212908e":"df.describe()","e0493de6":"for i in \"Color\",\"Spectral_Class\",\"Type\":\n    print(i+\":\"+str(df[i].unique())+\"\\n\") \n    ","6e0515c2":"for i in [\"Yellowish White\",\"Blue White\",\"Blue white\",\"Blue-white\",\"Whitish\",\"yellow-white\",\"white\",\"Blue-White\"]:\n    df.loc[df[\"Color\"]==i,\"Color\"]=\"White\"","39874b04":"df[\"Color\"].unique()","6c564bbc":"df.loc[df[\"Color\"]=='Pale yellow orange',\"Color\"]=\"Orange\"","318de33e":"df[\"Color\"].unique()","ed57c43f":"for i in ['White-Yellow','yellowish','Yellowish']:\n        df.loc[df[\"Color\"]==i,\"Color\"]=\"Yellow\"","17e42e62":"df[\"Color\"].unique()","298bd492":"df.loc[df[\"Color\"]=='Orange-Red',\"Color\"]=\"Red\"","e0bd60c5":"df[\"Color\"].unique()","f73e9c37":"df.duplicated().sum()","dbf1b069":"df.head(10)","ed6c44c1":"data=df.groupby(\"Type\").mean()","5d8997a3":"sns.barplot(x=data.index,y=\"Temperature\",data=data)\nplt.title(\"Temperature vs Type\")","b45c1671":"sns.barplot(x=data.index,y=\"L\",data=data)\nplt.title(\"L vs Type\")","3d20cdc2":"sns.barplot(x=data.index,y=\"R\",data=data)\nplt.title(\"R vs Type\")","f6153cb5":"sns.barplot(x=data.index,y=\"A_M\",data=data)\nplt.title(\"A_M vs Type\")","fb3e9419":"data2=df[\"Color\"].value_counts()\ndata2.head()","fbb68841":"sns.barplot(x=data2.index,y=df[\"Color\"].value_counts())\nplt.title(\"Numbers of stars by their colors\")","fa1274fe":"plt.figure(figsize=(10,10))\nsns.heatmap(df.corr(),annot=True)","6786ae24":"def outlier_graph(data,column):\n    plt.figure(figsize=(5,3))\n    sns.boxplot(data[column])\n    plt.title(\"{} distribution\".format(column))","a38cba1f":"for i in [\"Temperature\",\"L\",\"R\",\"A_M\"]:\n    outlier_graph(df,i)","78d0be4d":"df.head()","297b44c0":"Temperature_Cat=[]\nfor i in df[\"Temperature\"]:\n    if i > 6000:\n        Temperature_Cat.append(\"Temp_High\")\n    else:\n        Temperature_Cat.append(\"Temp_Low\")\nlen(Temperature_Cat)\ndf[\"Temperature_Cat\"]=Temperature_Cat        ","29339e99":"L_Cat=[]\nfor i in df[\"L\"]:\n    if i >= 100000:\n        L_Cat.append(\"L_High\")\n    elif 25000<= i < 100000:\n        L_Cat.append(\"L_Moderate\")\n    else:\n        L_Cat.append(\"L_Low\")        \nlen(L_Cat)\ndf[\"L_Cat\"]=L_Cat  ","46a78ff5":"R_Cat=[]\nfor i in df[\"R\"]:\n    if i > 400:\n        R_Cat.append(\"R_High\")\n    else:\n        R_Cat.append(\"R_Low\")\nlen(R_Cat)\ndf[\"R_Cat\"]=R_Cat        ","6e2d3b47":"A_M_Cat=[]\nfor i in df[\"A_M\"]:\n    if i > 0:\n        A_M_Cat.append(\"A_M_High\")\n    else:\n        A_M_Cat.append(\"A_M_Low\")\nlen(A_M_Cat)\ndf[\"A_M_Cat\"]=A_M_Cat        ","56060e88":"df.head()","cf3cef9a":"df.isnull().sum()","bb455e6c":"df=pd.get_dummies(data=df,columns=[\"Color\",\"Spectral_Class\",\"Temperature_Cat\",\"L_Cat\",\"R_Cat\",\"A_M_Cat\"],drop_first=True)","ee11cd28":"df.head()","201dfac4":"from sklearn.preprocessing import MinMaxScaler,StandardScaler,QuantileTransformer\nfrom sklearn.model_selection import GridSearchCV, cross_val_score,StratifiedKFold,train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, recall_score,f1_score\nfrom sklearn.metrics import confusion_matrix","2a69ba2d":"x=df.drop([\"Type\"],axis=1)\ny=df[\"Type\"]","fdea24df":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 42)\nprint(\"x_train\",len(x_train))\nprint(\"x_test\",len(x_test))\nprint(\"y_train\",len(y_train))\nprint(\"y_test\",len(y_test))","bb7621ee":"Classifiers=[]\nScores=[]","56089b9a":"for i in [5,6,7,8,9,10]:\n    knn=KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train,y_train)\n    Scores.append(cross_val_score(knn, x_test, y_test, cv=5).mean())\n    Classifiers.append(\"Knn{}\".format(str(i)))\n    plt.subplots()\n    sns.heatmap(confusion_matrix(y_test, knn.predict(x_test)),annot=True)\n    plt.title(\"Knn{}\".format(str(i)))","876e4490":"svc=SVC(random_state = 5)\nsvc.fit(x_train,y_train)\nScores.append(cross_val_score(svc, x_test, y_test, cv=5).mean())\nClassifiers.append(\"Svc\")\nsns.heatmap(confusion_matrix(y_test,svc.predict(x_test)),annot=True)","7756baf6":"for i in [30,60,80,100]:\n    rf=RandomForestClassifier(n_estimators=i,random_state = 5)\n    rf.fit(x_train,y_train)\n    Scores.append(cross_val_score(rf, x_test, y_test, cv=5).mean())\n    plt.subplots()\n    sns.heatmap(confusion_matrix(y_test, rf.predict(x_test)),annot=True)\n    plt.title(\"Rf{}\".format(str(i)))\n    Classifiers.append(\"Rf{}\".format(str(i)))","c2aa2a1f":"dtc=DecisionTreeClassifier(random_state = 5)\ndtc.fit(x_train,y_train)\nScores.append(cross_val_score(dtc, x_test, y_test, cv=5).mean())\nsns.heatmap(confusion_matrix(y_test, dtc.predict(x_test)),annot=True)\nClassifiers.append(\"Dtc\")","60e25a6b":"graph_data= pd.DataFrame(list(zip(Classifiers,Scores)),columns =['Classifiers', 'Scores']) \ngraph_data=graph_data.sort_values(\"Scores\",ascending=False)\nplt.figure(figsize=(16,8))\nsns.barplot(x=graph_data[\"Classifiers\"],y=graph_data[\"Scores\"])","ca9a70bb":"graph_data.head(20)","9fce230f":"Last_Model = VotingClassifier(estimators = [('dtc', DecisionTreeClassifier(random_state = 5)),\n                                        ('Rf60', RandomForestClassifier(n_estimators=60,random_state = 5)),\n                                        ('Rf30', RandomForestClassifier(n_estimators=30,random_state = 5))],\n                                        voting = \"hard\", n_jobs = -1)\nLast_Model = Last_Model.fit(x_train, y_train)\nprint(accuracy_score(Last_Model.predict(x_test),y_test))","1436a63e":"sns.heatmap(confusion_matrix(y_test, Last_Model.predict(x_test)),annot=True)","cef8c157":"We need to adjust Color variable.","c0fc592e":"# **Cross Validation & Modelling**","0fd9f276":"SVC","27a7d049":"I will try KNN, SVC,Random Forest,Decision Tree. I won't use  Hyperparameter Tuning or Grid Search since our variables are pretty clear and I don't want to push too hard my model, I will go basic. ","c8235d0b":"# **Modelling**","7ac23302":"Visualize Scores.","95ee62b2":"We are ready for modelling.","b64d5cce":"Our accuracy seems %100 but it is because we have only 240 data, If we would have more it will decrease.","fcdb2b7f":"We have no problem all the predicts are correct but still we need bigger dataset to create better model. 100% accuracy can be misleading.","8b2a3e21":"First of all my aim is creating new variables as categorical variable and  getting a good model.","73d087a4":"Still I can separte 0-1-2-3-4 and 5 by using this information.","5467b5a2":"# **Ensemble Model**","b1be05ac":"Decision Tree","5f0e6976":"I can create a new variable by using this information, I can create a temp variable which contains\nlow-temp and and high-temp. low-temp ones are type 1 and type 2 while others are high temperature.","663943e8":"Random Forest","c5598bf4":"We can see that Random Forest and Decision Tree are the best models.We can combine them.","71f65cbe":"Now we need to look for outliers.","c84e58ae":"Still we can see the correlations by looking this correlation map. A_M variable highly negative correlated with Type so it is important for us.","53cc7137":"It is good that we don't have any null value.","a687ad0a":"# **Creating New Variables**","605dc99b":"Start with KNN","d152c108":"I deliberately dropped the first values because if other values are 0, our model will know that dropped one is 1 so it is unnecessary to put all values and putting all the values in would inflate our model.","b52349da":"Now we created all of our variables, now we can create dummies and start modelling.","f37b5d52":"Color adjustment is done.","929fdabe":"For type 0,1,2 L variable is incredibly low, while for type 3 it is  moderate and for 4-5 it is high. I can create new categorical variable by looking this information.","ab843db4":"First of all I want to look again to the table to visualize easily.","1f571d9e":"# ***Reading Data***","78974c5e":"There are too many outilers in R and L but remove them can lead to wrong model to us because variables already seperated in non-uniform way. Removing these outlierswill cause a lack of information so I don't touch them.","70e4867d":"This information still leads to us a new categorical variable.","1c14fbfb":"# **Visualization**","d4d2fc48":"As we can see some variables are highly spread."}}