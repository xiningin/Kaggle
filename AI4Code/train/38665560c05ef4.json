{"cell_type":{"87faed49":"code","27f6aec7":"code","4528e2da":"code","dcede720":"code","f3c8011d":"code","1acf2811":"code","9367c490":"code","1c9994de":"code","5635d494":"code","02ec5968":"code","df0dd5b6":"code","44af57f8":"code","bea935f6":"code","7fdb4b3d":"code","e292af60":"code","eac16a62":"code","87d9c775":"code","717d81a1":"markdown","9ad6cf32":"markdown","b4697751":"markdown","f95cac37":"markdown"},"source":{"87faed49":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","27f6aec7":"df=pd.read_csv('\/kaggle\/input\/wine-dataset-for-clustering\/wine-clustering.csv')","4528e2da":"df.head()","dcede720":"df.isnull().sum()\ndata=df.copy()\ndata.head()","f3c8011d":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\ndata[data.columns]=sc.fit_transform(data)","1acf2811":"data","9367c490":"data[data.columns]","1c9994de":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(data)\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2'])","5635d494":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"darkgrid\", rc={'figure.figsize':(9,6)}, font_scale=2)\nplt.scatter(x=principalComponents[:,0], y=principalComponents[:,1], color=\"black\", lw=2)\nplt.xlabel(\"Principle Component 1\")\nplt.ylabel(\"Principle Component 2\")\nplt.title(\"Strongest Principle Components\")\nplt.show()","02ec5968":"#Visualizing the ELBOW method to get the optimal value of K \nfrom sklearn.cluster import KMeans\nwcss=[] # within cluster sum of sqaures\nfor i in range(1,11):\n    kmeans = KMeans(n_clusters= i, init='k-means++', random_state=0)\n    kmeans.fit(data)\n    wcss.append(kmeans.inertia_)\n\n#inertia_ is the formula used to segregate the data points into clusters\nimport matplotlib.pyplot as plt\nplt.plot(range(1,11), wcss)\nplt.title('The Elbow Method')\nplt.xlabel('no of clusters')\nplt.ylabel('wcss')\nplt.show()","df0dd5b6":"kmeans= KMeans(n_clusters=3, random_state=17, init='k-means++')\nkmeans_labels= kmeans.fit_predict(data)\ncentroids = kmeans.cluster_centers_","44af57f8":"centroids_pca = pca.transform(centroids)\npd.Series(kmeans_labels).value_counts()","bea935f6":"centroids_df = pd.DataFrame(sc.inverse_transform(centroids), columns= df.columns)\ncentroids_df","7fdb4b3d":"sns.set(style='darkgrid', rc={'figure.figsize':(12,8)},font_scale=2)\nplt.scatter(x=principalComponents[:,0], y=principalComponents[:,1], c=kmeans_labels, cmap=\"CMRmap\", lw=4)\nplt.scatter(x=centroids_pca[:,0], y=centroids_pca[:,1], marker=\"+\", s=500, linewidths=3, lw=4, color=\"blue\", zorder=10)\nplt.xlabel(\"Principle Component 1\")\nplt.ylabel(\"Principle Component 2\")\nplt.title(\"Clustered Data\")","e292af60":"from sklearn.metrics import silhouette_score\nsilhouette = silhouette_score(data, kmeans.labels_, metric='euclidean')","eac16a62":"silhouette","87d9c775":"sns.set(style=\"darkgrid\", font_scale=1.5, rc={'figure.figsize':(20,20)})\nax=df.hist(bins=20, color=\"green\")","717d81a1":"## Applying PCA","9ad6cf32":"## Applying K-Means","b4697751":"## Scaling the data","f95cac37":"## Finding the best number for K"}}