{"cell_type":{"3928b93e":"code","46586eee":"code","ac2b7787":"code","92d8a082":"code","a2343136":"code","24343050":"code","4173ebf2":"code","af9c2901":"code","28d9ee13":"code","0cc76275":"code","cfe527f6":"code","816a48eb":"code","c9e46ed9":"code","78ca6ead":"code","61550993":"code","4284dc37":"code","fd4cb14a":"code","f05f0f26":"code","e218e6aa":"code","4f66300e":"code","d669a863":"code","b6bff05d":"code","2626df72":"code","baddd347":"code","de2cdbb7":"code","f8169f21":"code","fbb5f3cb":"code","55f9efdb":"code","265c486c":"code","ca70032d":"code","7911e075":"code","98e86978":"code","e9a909ff":"code","571fbf46":"code","7a25bbf1":"code","d1f0456b":"code","29fb4f71":"code","c01b20e0":"code","97d89e8b":"code","2b5a33e1":"code","e68e7caa":"markdown","b3db8e8c":"markdown","279deb5c":"markdown","692564a6":"markdown","2b1f654b":"markdown","aab33b26":"markdown","f172ebd7":"markdown","c4f1234f":"markdown","ca949777":"markdown","47cb2a06":"markdown","75393e07":"markdown","a66ece87":"markdown","d5c42878":"markdown","7bad92e4":"markdown","655e98aa":"markdown","6efeba1b":"markdown","0a04a0bb":"markdown","926ebfff":"markdown","ac99bea0":"markdown","5897251b":"markdown","31a84391":"markdown","26a81e0e":"markdown","5c083d87":"markdown","72e8cdd5":"markdown","efc77af4":"markdown"},"source":{"3928b93e":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.stats import rankdata\nimport lightgbm as lgb\nfrom sklearn import metrics\nimport gc\nimport warnings\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nimport itertools\nfrom sklearn import metrics\nfrom scipy.stats import norm, rankdata\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\npd.set_option('display.max_columns', 200)\n# below is to have multiple outputs from same Jupyter cells\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nwarnings.filterwarnings('ignore')\nfrom feature_selector import FeatureSelector","46586eee":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')","ac2b7787":"train_df = train_df.sample(n = 20000, random_state = 42)\ntest_df = test_df.sample(n = 20000, random_state = 42)","92d8a082":"print(\"Train Shape\\n\")\nprint(\"*\"*80)\ntrain_df.shape\nprint(\"*\"*80)\nprint(\"Test Shape\\n\")\nprint(\"*\"*80)\ntest_df.shape","a2343136":"print(\"Train Describe\\n\")\ntrain_df.describe() \nprint(\"Test Describe\\n\")\ntest_df.describe()","24343050":"train_df[\"target\"].value_counts()\/train_df.shape[0]*100\nfig,ax= plt.subplots()\nsns.countplot(data=train_df,x=\"target\",ax=ax)\nax.set(xlabel=\"Target\",\n       ylabel=\"Count\", \n       Title = \"Target Distribution\"\n       )","4173ebf2":"train_df.isnull().sum().sum()\ntest_df.isnull().sum().sum()","af9c2901":"# reduce memory\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\/2.0\n            c_max = df[col].max()\/2.0\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","28d9ee13":"def plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(10,10,figsize=(18,22))\n\n    for feature in features:\n        i += 1\n        plt.subplot(10,10,i)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show();","0cc76275":"def plot_feature_boxplot(df1,df2,label1,label2,features,target):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(100,2,figsize=(10,180))\n\n    for feature in features:\n        i += 1\n        plt.subplot(100,2,i)\n        sns.boxplot(y=df1[feature], x=target, showfliers=False)\n        plt.title(feature+'_train', fontsize=10)\n        plt.ylabel('')\n        plt.xlabel('')\n        i += 1\n        plt.subplot(100,2,i)\n        sns.boxplot(df2[feature],orient='v',color='r')\n        plt.title(feature+'_test', fontsize=10)\n        plt.ylabel('')\n        plt.xlabel('')\n\n        #locs, labels = plt.xticks()\n        #plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        #plt.tick_params(axis='y', which='major', labelsize=6)\n        #plt.gca().axes.get_xaxis().set_visible(False)\n        #plt.gca().axes.get_yaxis().set_visible(False)\n    plt.tight_layout()  \n    plt.show();\n    \n    ","cfe527f6":"def plot_feature_violinplot(df1,df2,label1,label2,features,target):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(100,2,figsize=(10,180))\n\n    for feature in features:\n        i += 1\n        plt.subplot(100,2,i)\n        sns.violinplot(y=df1[feature], x=target, showfliers=False)\n        plt.title(feature+'_train', fontsize=10)\n        plt.ylabel('')\n        plt.xlabel('')\n        i += 1\n        plt.subplot(100,2,i)\n        sns.violinplot(df2[feature],orient='v',color='r')\n        plt.title(feature+'_test', fontsize=10)\n        plt.ylabel('')\n        plt.xlabel('')\n\n        #locs, labels = plt.xticks()\n        #plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        #plt.tick_params(axis='y', which='major', labelsize=6)\n        #plt.gca().axes.get_xaxis().set_visible(False)\n        #plt.gca().axes.get_yaxis().set_visible(False)\n    plt.tight_layout()  \n    plt.show();\n    ","816a48eb":"def plot_binned_feature_target_violinplot(df,features,target):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(200,1,figsize=(8,380))\n\n    for feature in features:\n        bins = np.nanpercentile(df[feature], range(0,101,10))\n        df[feature+\"_binned\"] = pd.cut(df[feature],bins=bins)\n        i += 1\n        plt.subplot(200,1,i)\n        sns.violinplot(y=df[feature+\"_binned\"], x=target, showfliers=False)\n        plt.title(feature+'_binned & Target', fontsize=12)\n        plt.ylabel('')\n        plt.xlabel('')\n       \n        locs, labels = plt.xticks()\n        plt.xticks([0.0,1.0])\n        plt.tick_params(axis='y', which='major', labelsize=8)\n        #ax.set_xticks([0.15, 0.68, 0.97])\n        #plt.gca().axes.get_xaxis().set_visible(False)\n        #plt.gca().axes.get_yaxis().set_visible(False)\n    plt.tight_layout()  \n    plt.show();","c9e46ed9":"def add_new_feature_row(df,features):\n    for feature in features:\n        df[feature+\"_pct\"] = df[feature].pct_change()\n        df[feature+\"_diff\"] = df[feature].diff()\n        df.drop(feature,axis=1)\n    return df\n    ","78ca6ead":"def add_feature_df(df,features):\n    # count +ve and -ve\n    df['count+'] = np.array(df>0).sum(axis=1)\n    df['count-'] = np.array(df<0).sum(axis=1)\n    #sum\n    #df['sum_outside'] = df.sum(axis=1)\n        \n    for feature in features:\n        #normalize\n        #df[feature+'_norm'] = (df[feature] - df[feature].mean())\/df[feature].std()\n        #percentage change row wise\n        #df[feature+\"_pct\"] = df[feature].pct_change() # didnt give boost\n        #diff change row wise\n        #df[feature+\"_diff\"] = df[feature].diff() # didnt give boost\n        # Square\n        #df[feature+'^2'] = df[feature] * df[feature]\n        # Cube\n        #df[feature+'^3'] = df[feature] * df[feature] * df[feature]\n        # 4th power\n        #df[feature+'^4'] = df[feature] * df[feature] * df[feature] * df[feature]\n        # Cumulative percentile (not normalized)\n        #df[feature+'_cp'] = rankdata(df[feature]).astype('float32')\n        # Cumulative normal percentile, probabilites\n        #df[feature+'_cnp'] = norm.cdf(df[feature]).astype('float32')\n        # sqrt\n        #df[feature+'_sqrt'] = np.sqrt(df[feature])\n        #binning\n        #bins = np.nanpercentile(df[feature], range(0,101,10))\n        #df[feature+\"_binned\"] = pd.cut(df[feature],bins=bins)\n        #rounding\n        #df[feature+'_r2'] = np.round(df[feature], 2)\n        #rounding\n        #df['r1_'+feature] = np.round(df[feature], 1)\n        #exp\n        #df['exp_'+feature] = np.exp(df[feature])\n        #exp and feature\n        #df['xintoexp_'+feature] = np.exp(df[feature])*df[feature]\n        #sum\n        #df['sum_inside'] = df[[feature]].sum(axis=1)\n        #max\n        #df['max'] = df[[feature]].max(axis=1)\n        #min\n        #df['min'] = df[[feature]].min(axis=1)\n        #max\n        #df['std'] = df[[feature]].std(axis=1)\n        #skew\n        #df['skew'] = df[[feature]].skew(axis=1)\n        #kurt\n        #df['kurt'] = df[[feature]].kurtosis(axis=1)\n        #median\n        #df['med'] = df[[feature]].median(axis=1)\n        #tanh\n        df['tanh_'+feature] = np.tanh(df[feature])\n        \n    return df\n    ","61550993":"import gc\ngc.collect()","4284dc37":"test_df['target']= np.nan\ncombine_df = train_df.append(test_df,ignore_index=True)","fd4cb14a":"#features = train_df.columns[~train_df.columns.isin(['target','ID_code'])]\n#combine_df = add_feature_df(combine_df,features)","f05f0f26":"#combine_df.head()","e218e6aa":"train_df = combine_df[combine_df['target'].notnull()].reset_index(drop=True)\ntest_df = combine_df[combine_df['target'].isnull()].reset_index(drop=True)\n","4f66300e":"# features will have added cols defined as part of feature engineering\nfeatures = train_df.columns[~train_df.columns.isin(['target','ID_code'])]","d669a863":"# Features are in train and labels are in train_labels\nfs = FeatureSelector(data = train_df[features], labels = train_df[\"target\"])","b6bff05d":"fs.identify_all(selection_params = {'missing_threshold': 0.6,    \n                                    'correlation_threshold': 0.95, \n                                    'task': 'classification',    \n                                    'eval_metric': 'auc', \n                                    'cumulative_importance': 0.95})","2626df72":"collinear_features = fs.ops['collinear']\nfs.record_collinear.head(10)\n","baddd347":"fs.feature_importances.head(200)","de2cdbb7":"# plot the feature importances\nfs.plot_feature_importances(threshold = 0.99, plot_n = 25)","f8169f21":"# list of zero importance features\nzero_importance_features = fs.ops['zero_importance']\nzero_importance_features","fbb5f3cb":"train_removed_all = fs.remove(methods = 'all', \n                                          keep_one_hot=False)","55f9efdb":"fs.ops # stats for which all are removed by what method.","265c486c":"train_removed_all.head()","ca70032d":"feat_sel=train_removed_all.columns.values\n","7911e075":"feat_sel\n","98e86978":"with open (\"feat_sel.csv\",\"w\")as fp:\n   for line in feat_sel:\n       fp.write(str(line)+\"\\n\")\n\n","e9a909ff":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\ntest_df['target']= np.nan\ncombine_df = train_df.append(test_df,ignore_index=True)\nfeatures = train_df.columns[~train_df.columns.isin(['target','ID_code'])]\ncombine_df = add_feature_df(combine_df,features)\ntrain_df = combine_df[combine_df['target'].notnull()].reset_index(drop=True)\ntest_df = combine_df[combine_df['target'].isnull()].reset_index(drop=True)","571fbf46":"#test_df = test_df.drop(\"target\",axis=1)\npredictors = train_removed_all.columns\nnfold = 5\ntarget = 'target'","7a25bbf1":"'''param = {\n     'num_leaves': 18,\n     'max_bin': 63,\n     'min_data_in_leaf': 5,\n     'learning_rate': 0.010614430970330217,\n     'min_sum_hessian_in_leaf': 0.0093586657313989123,\n     'feature_fraction': 0.056701788569420042,\n     'lambda_l1': 0.060222413158420585,\n     'lambda_l2': 4.6580550589317573,\n     'min_gain_to_split': 0.29588543202055562,\n     'max_depth': 49,\n     'save_binary': True,\n     'seed': 1337,\n     'feature_fraction_seed': 1337,\n     'bagging_seed': 1337,\n     'drop_seed': 1337,\n     'data_random_seed': 1337,\n     'objective': 'binary',\n     'boosting_type': 'gbdt',\n     'verbose': 1,\n     'metric': 'auc',\n     'is_unbalance': True,\n     'boost_from_average': False\n}\n'''\nparam = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.4,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.05,\n    'learning_rate': 0.01,\n    'max_depth': -1,  \n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary', \n    'verbosity': 1\n}\n\nnfold = 10\n\nskf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=42)\n\noof = np.zeros(len(train_df))\npredictions = np.zeros(len(test_df))\nfeature_importance_df = pd.DataFrame()\n\ni = 1\nfor train_index, valid_index in skf.split(train_df, train_df.target.values):\n    print(\"\\nfold {}\".format(i))\n    xg_train = lgb.Dataset(train_df.iloc[train_index][predictors].values,\n                           label=train_df.iloc[train_index][target].values,\n                           feature_name='auto',\n                           categorical_feature = 'auto',\n                           free_raw_data = False\n                           )\n    print(\"after lgb train\")\n    xg_valid = lgb.Dataset(train_df.iloc[valid_index][predictors].values,\n                           label=train_df.iloc[valid_index][target].values,\n                           feature_name='auto',\n                           categorical_feature = 'auto',\n                           free_raw_data = False\n                           )   \n    print(\"after lgb test\")\n    nround = 1000000\n    clf = lgb.train(param, \n                    xg_train, \n                    nround, \n                    valid_sets = [xg_train,xg_valid], \n                    early_stopping_rounds=3000,\n                    verbose_eval=1000)\n    oof[valid_index] = clf.predict(train_df.iloc[valid_index][predictors].values, num_iteration=nround) \n    print(\"after lgb fit\")\n    predictions += clf.predict(test_df[predictors], num_iteration=nround) \/ nfold\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = predictors\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] =  i = i + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n   \n\n\nprint(\"\\n\\nCV AUC: {:<0.4f}\".format(metrics.roc_auc_score(train_df.target.values, oof)))\n","d1f0456b":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:150].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(14,28))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance (averaged\/folds)')\nplt.tight_layout()\nplt.savefig('FI.png')","29fb4f71":" # Get feature importances\nimp_df = pd.DataFrame()\nimp_df[\"feature\"] = predictors\nimp_df[\"importance_gain\"] = clf.feature_importance(importance_type='gain')\nimp_df[\"importance_split\"] = clf.feature_importance(importance_type='split')\nimp_df['trn_score'] = roc_auc_score(train_df['target'], clf.predict(train_df.loc[:,predictors]))","c01b20e0":"imp_df.sort_values(by=\"importance_gain\",ascending=False).to_csv(\"imp_lgb.csv\", index=False)\n","97d89e8b":"imp_df.sort_values(by=\"importance_gain\",ascending=False)[:150]","2b5a33e1":"sub_df = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\nsub_df[\"target\"] = predictions\nsub_df.to_csv(\"sant_lgb.csv\", index=False)\nsub_df[:10]","e68e7caa":"Read files ","b3db8e8c":"Number of rows and columns in Dataset","279deb5c":"Basic statistics for datasets","692564a6":"<div id = FeatLink>\n** Feature Engineering **\n    <\/div>","2b1f654b":"None of dataset has any missing values. ","aab33b26":"<div id = ModLink>\n** Modeling **\n    <\/div>","f172ebd7":"To plot violinplot of features of two datasets, along with class split  **plot_feature_violinplot**","c4f1234f":"This is model lifted and shifted from [Fayaz's](https:\/\/www.kaggle.com\/fayzur\/lightgbm-customer-transaction-prediction) kernel.","ca949777":"Feature Importance as per model","47cb2a06":"<div id=\"FuncLink\">\n** Utility Functions for EDA and Feature Engineering **\n    <\/div>","75393e07":"Distribution of target in training dataset. This shows its a imbalance data set, with 90% of data being 0 and 10% as 1.","a66ece87":"To plot violinplot of binned features for training along with class split  **plot_binned_feature_target_violinplot**","d5c42878":"To plot boxplot of features of two datasets, along with class split  **plot_feature_boxplot**","7bad92e4":"To reduce memory footprint","655e98aa":"-  [Import and Read](#LibLink)\n-  [Basic EDA](#EDALink)\n-  [Functions](#FuncLink)\n-  [Plotting](#PlotLink)\n-  [Corr and Bin](#CorLink)\n-  [Features](#FeatLink)\n-  [Model](#ModLink)","6efeba1b":"Submission file","0a04a0bb":"<div id=\"LibLink\">\n**Import libraries**\n<\/div>","926ebfff":"Separate out train and test. Append new features created to training dataset.","ac99bea0":"To plot distributions features of two datasets  **plot_feature_distribution**","5897251b":"Looking at output of describe for both df, data seems to be similar in both the datasets (test and train).  Another point is test is of same size as train. we need to find a way to extract some info from test data.","31a84391":"To normailize features using combined dataset **add_feature_df**","26a81e0e":"To add new features row wise  **add_new_feature_row**","5c083d87":"<div id=\"EDALink\">\n **Basic EDA**\n <\/div>","72e8cdd5":"I am trying to analyze data thru diff views to get a clue which features may be impacting target. My intenetion is to keep things simple and easily comprehendable. I myself get lost sometimes in good kernels which are bit low on structure part.  I have tried to keep it structured and scalable for new features and models. ","efc77af4":"### Missing values"}}