{"cell_type":{"566d486a":"code","792df482":"code","5e4d24ea":"code","0a71ad6d":"code","ae3b1dc6":"code","d3151dd0":"code","9eeec51a":"code","3e572c75":"code","b0ef67c2":"code","17f65ea5":"code","09b9c3a6":"code","75ab225f":"code","43c43c8a":"code","dd6fb853":"code","678d172e":"code","3d9f1a19":"code","e0faad11":"code","2d1a70ed":"code","75d2596b":"code","3fc7e655":"code","8c9bdb58":"code","4f860915":"code","977ceb63":"code","9a23b5ef":"code","9b415e70":"code","f65faa57":"code","cf171c05":"code","f6dc9b06":"code","ad3b677c":"code","c2575a16":"code","b35a8471":"code","91822df4":"code","6cf3955d":"code","597fa053":"markdown","59214d02":"markdown","25bf7667":"markdown","8503405e":"markdown","915b9d07":"markdown","9788f587":"markdown","478904b2":"markdown","efb5ee46":"markdown","a1e2096f":"markdown","9049e278":"markdown","9f5f6e4e":"markdown","e38c80e5":"markdown","30391366":"markdown","5f165ab9":"markdown","1eb20cdd":"markdown","47f5be26":"markdown","b5929928":"markdown","80286e78":"markdown","f98e254d":"markdown","15cd2efb":"markdown","8f9904f6":"markdown","e8663dd2":"markdown","405e4524":"markdown","245034d6":"markdown","f9ed651f":"markdown","be6c5c82":"markdown","337694f2":"markdown","32e91e33":"markdown","4ebb3710":"markdown","b84a8ec9":"markdown","8a021fe7":"markdown","c3c7ff35":"markdown"},"source":{"566d486a":"#importing libraries\nimport numpy as np \nimport pandas as pd\nfrom math import *\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport matplotlib.pyplot as plt\n\npd.set_option('display.max_columns', 100)\nimport warnings\nwarnings.filterwarnings('ignore')        \n\n#reading test and train dataset files\ntrain = pd.read_csv(\"..\/input\/blackfriday\/train.csv\")\ntest = pd.read_csv(\"..\/input\/blackfriday\/test.csv\")\n\n#creating dataframe for the required output\nsubmission = pd.DataFrame()\nsubmission['User_ID'] = test['User_ID']\nsubmission['Product_ID'] = test['Product_ID']","792df482":"gender_dict = {'F':0, 'M':1}\nage_dict = {'0-17':0, '18-25':1, '26-35':2, '36-45':3, '46-50':4, '51-55':5, '55+':6}\ncity_dict = {'A':0, 'B':1, 'C':2}\nstay_dict = {'0':0, '1':1, '2':2, '3':3, '4+':4}\n\ntrain[\"Gender\"] = train[\"Gender\"].apply(lambda x: gender_dict[x])\ntest[\"Gender\"] = test[\"Gender\"].apply(lambda x: gender_dict[x])\n\ntrain[\"Age\"] = train[\"Age\"].apply(lambda x: age_dict[x])\ntest[\"Age\"] = test[\"Age\"].apply(lambda x: age_dict[x])\n\ntrain[\"City_Category\"] = train[\"City_Category\"].apply(lambda x: city_dict[x])\ntest[\"City_Category\"] = test[\"City_Category\"].apply(lambda x: city_dict[x])\n\ntrain[\"Stay_In_Current_City_Years\"] = train[\"Stay_In_Current_City_Years\"].apply(lambda x: stay_dict[x])\ntest[\"Stay_In_Current_City_Years\"] = test[\"Stay_In_Current_City_Years\"].apply(lambda x: stay_dict[x])","5e4d24ea":"train[\"Age_Count\"] = train.groupby(['Age'])['Age'].transform('count')\nage_count_dict = train.groupby(['Age']).size().to_dict()\ntest['Age_Count'] = test['Age'].apply(lambda x:age_count_dict.get(x,0))\n\ntrain[\"Occupation_Count\"] = train.groupby(['Occupation'])['Occupation'].transform('count')\noccupation_count_dict = train.groupby(['Occupation']).size().to_dict()\ntest['Occupation_Count'] = test['Occupation'].apply(lambda x:occupation_count_dict.get(x,0))\n\ntrain[\"Product_Category_1_Count\"] = train.groupby(['Product_Category_1'])['Product_Category_1'].transform('count')\npc1_count_dict = train.groupby(['Product_Category_1']).size().to_dict()\ntest['Product_Category_1_Count'] = test['Product_Category_1'].apply(lambda x:pc1_count_dict.get(x,0))\n\ntrain[\"Product_Category_2_Count\"] = train.groupby(['Product_Category_2'])['Product_Category_2'].transform('count')\npc2_count_dict = train.groupby(['Product_Category_2']).size().to_dict()\ntest['Product_Category_2_Count'] = test['Product_Category_2'].apply(lambda x:pc2_count_dict.get(x,0))\n\ntrain[\"Product_Category_3_Count\"] = train.groupby(['Product_Category_3'])['Product_Category_3'].transform('count')\npc3_count_dict = train.groupby(['Product_Category_3']).size().to_dict()\ntest['Product_Category_3_Count'] = test['Product_Category_3'].apply(lambda x:pc3_count_dict.get(x,0))\n\ntrain[\"User_ID_Count\"] = train.groupby(['User_ID'])['User_ID'].transform('count')\nuserID_count_dict = train.groupby(['User_ID']).size().to_dict()\ntest['User_ID_Count'] = test['User_ID'].apply(lambda x:userID_count_dict.get(x,0))\n\ntrain[\"Product_ID_Count\"] = train.groupby(['Product_ID'])['Product_ID'].transform('count')\nproductID_count_dict = train.groupby(['Product_ID']).size().to_dict()\ntest['Product_ID_Count'] = test['Product_ID'].apply(lambda x:productID_count_dict.get(x,0))","0a71ad6d":"train[\"User_ID_MinPrice\"] = train.groupby(['User_ID'])['Purchase'].transform('min')\nuserID_min_dict = train.groupby(['User_ID'])['Purchase'].min().to_dict()\ntest['User_ID_MinPrice'] = test['User_ID'].apply(lambda x:userID_min_dict.get(x,0))\n\ntrain[\"User_ID_MaxPrice\"] = train.groupby(['User_ID'])['Purchase'].transform('max')\nuserID_max_dict = train.groupby(['User_ID'])['Purchase'].max().to_dict()\ntest['User_ID_MaxPrice'] = test['User_ID'].apply(lambda x:userID_max_dict.get(x,0))\n\ntrain[\"User_ID_MeanPrice\"] = train.groupby(['User_ID'])['Purchase'].transform('mean')\nuserID_mean_dict = train.groupby(['User_ID'])['Purchase'].mean().to_dict()\ntest['User_ID_MeanPrice'] = test['User_ID'].apply(lambda x:userID_mean_dict.get(x,0))\n\n\ntrain[\"Product_ID_MinPrice\"] = train.groupby(['Product_ID'])['Purchase'].transform('min')\nproductID_min_dict = train.groupby(['Product_ID'])['Purchase'].min().to_dict()\ntest['Product_ID_MinPrice'] = test['Product_ID'].apply(lambda x:productID_min_dict.get(x,0))\n\ntrain[\"Product_ID_MaxPrice\"] = train.groupby(['Product_ID'])['Purchase'].transform('max')\nproductID_max_dict = train.groupby(['Product_ID'])['Purchase'].max().to_dict()\ntest['Product_ID_MaxPrice'] = test['Product_ID'].apply(lambda x:productID_max_dict.get(x,0))\n\ntrain[\"Product_ID_MeanPrice\"] = train.groupby(['Product_ID'])['Purchase'].transform('mean')\nproductID_mean_dict = train.groupby(['Product_ID'])['Purchase'].mean().to_dict()\ntest['Product_ID_MeanPrice'] = test['Product_ID'].apply(lambda x:productID_mean_dict.get(x,0))","ae3b1dc6":"userID_25p_dict = train.groupby(['User_ID'])['Purchase'].apply(lambda x:np.percentile(x,25)).to_dict()\ntrain['User_ID_25PercPrice'] = train['User_ID'].apply(lambda x:userID_25p_dict.get(x,0))\ntest['User_ID_25PercPrice'] = test['User_ID'].apply(lambda x:userID_25p_dict.get(x,0))\n\nuserID_75p_dict = train.groupby(['User_ID'])['Purchase'].apply(lambda x:np.percentile(x,75)).to_dict()\ntrain['User_ID_75PercPrice'] = train['User_ID'].apply(lambda x:userID_75p_dict.get(x,0))\ntest['User_ID_75PercPrice'] = test['User_ID'].apply(lambda x:userID_75p_dict.get(x,0))\n\nproductID_25p_dict = train.groupby(['Product_ID'])['Purchase'].apply(lambda x:np.percentile(x,25)).to_dict()\ntrain['Product_ID_25PercPrice'] = train['Product_ID'].apply(lambda x:productID_25p_dict.get(x,0))\ntest['Product_ID_25PercPrice'] = test['Product_ID'].apply(lambda x:productID_25p_dict.get(x,0))\n\nproductID_75p_dict = train.groupby(['Product_ID'])['Purchase'].apply(lambda x:np.percentile(x,75)).to_dict()\ntrain['Product_ID_75PercPrice'] = train['Product_ID'].apply(lambda x:productID_75p_dict.get(x,0))\ntest['Product_ID_75PercPrice'] = test['Product_ID'].apply(lambda x:productID_75p_dict.get(x,0))\n\n\ntrain[\"Product_Cat1_MinPrice\"] = train.groupby(['Product_Category_1'])['Purchase'].transform('min')\npc1_min_dict = train.groupby(['Product_Category_1'])['Purchase'].min().to_dict()\ntest['Product_Cat1_MinPrice'] = test['Product_Category_1'].apply(lambda x:pc1_min_dict.get(x,0))\n\ntrain[\"Product_Cat1_MaxPrice\"] = train.groupby(['Product_Category_1'])['Purchase'].transform('max')\npc1_max_dict = train.groupby(['Product_Category_1'])['Purchase'].max().to_dict()\ntest['Product_Cat1_MaxPrice'] = test['Product_Category_1'].apply(lambda x:pc1_max_dict.get(x,0))\n\ntrain[\"Product_Cat1_MeanPrice\"] = train.groupby(['Product_Category_1'])['Purchase'].transform('mean')\npc1_mean_dict = train.groupby(['Product_Category_1'])['Purchase'].mean().to_dict()\ntest['Product_Cat1_MeanPrice'] = test['Product_Category_1'].apply(lambda x:pc1_mean_dict.get(x,0))\n\npc1_25p_dict = train.groupby(['Product_Category_1'])['Purchase'].apply(lambda x:np.percentile(x,25)).to_dict()\ntrain['Product_Cat1_25PercPrice'] = train['Product_Category_1'].apply(lambda x:pc1_25p_dict.get(x,0))\ntest['Product_Cat1_25PercPrice'] = test['Product_Category_1'].apply(lambda x:pc1_25p_dict.get(x,0))\n\npc1_75p_dict = train.groupby(['Product_Category_1'])['Purchase'].apply(lambda x:np.percentile(x,75)).to_dict()\ntrain['Product_Cat1_75PercPrice'] = train['Product_Category_1'].apply(lambda x:pc1_75p_dict.get(x,0))\ntest['Product_Cat1_75PercPrice'] = test['Product_Category_1'].apply(lambda x:pc1_75p_dict.get(x,0))\n\n\ntrain[\"Product_Cat2_MinPrice\"] = train.groupby(['Product_Category_2'])['Purchase'].transform('min')\npc2_min_dict = train.groupby(['Product_Category_2'])['Purchase'].min().to_dict()\ntest['Product_Cat2_MinPrice'] = test['Product_Category_2'].apply(lambda x:pc2_min_dict.get(x,0))\n\ntrain[\"Product_Cat2_MaxPrice\"] = train.groupby(['Product_Category_2'])['Purchase'].transform('max')\npc2_max_dict = train.groupby(['Product_Category_2'])['Purchase'].max().to_dict()\ntest['Product_Cat2_MaxPrice'] = test['Product_Category_2'].apply(lambda x:pc2_max_dict.get(x,0))\n\ntrain[\"Product_Cat2_MeanPrice\"] = train.groupby(['Product_Category_2'])['Purchase'].transform('mean')\npc2_mean_dict = train.groupby(['Product_Category_2'])['Purchase'].mean().to_dict()\ntest['Product_Cat2_MeanPrice'] = test['Product_Category_2'].apply(lambda x:pc2_mean_dict.get(x,0))\n\npc2_25p_dict = train.groupby(['Product_Category_2'])['Purchase'].apply(lambda x:np.percentile(x,25)).to_dict()\ntrain['Product_Cat2_25PercPrice'] = train['Product_Category_2'].apply(lambda x:pc2_25p_dict.get(x,0))\ntest['Product_Cat2_25PercPrice'] = test['Product_Category_2'].apply(lambda x:pc2_25p_dict.get(x,0))\n\npc2_75p_dict = train.groupby(['Product_Category_2'])['Purchase'].apply(lambda x:np.percentile(x,75)).to_dict()\ntrain['Product_Cat2_75PercPrice'] = train['Product_Category_2'].apply(lambda x:pc2_75p_dict.get(x,0))\ntest['Product_Cat2_75PercPrice'] = test['Product_Category_2'].apply(lambda x:pc2_75p_dict.get(x,0))\n\n\ntrain[\"Product_Cat3_MinPrice\"] = train.groupby(['Product_Category_3'])['Purchase'].transform('min')\npc3_min_dict = train.groupby(['Product_Category_3'])['Purchase'].min().to_dict()\ntest['Product_Cat3_MinPrice'] = test['Product_Category_3'].apply(lambda x:pc3_min_dict.get(x,0))\n\ntrain[\"Product_Cat3_MaxPrice\"] = train.groupby(['Product_Category_3'])['Purchase'].transform('max')\npc3_max_dict = train.groupby(['Product_Category_3'])['Purchase'].max().to_dict()\ntest['Product_Cat3_MaxPrice'] = test['Product_Category_3'].apply(lambda x:pc3_max_dict.get(x,0))\n\ntrain[\"Product_Cat3_MeanPrice\"] = train.groupby(['Product_Category_3'])['Purchase'].transform('mean')\npc3_mean_dict = train.groupby(['Product_Category_3'])['Purchase'].mean().to_dict()\ntest['Product_Cat3_MeanPrice'] = test['Product_Category_3'].apply(lambda x:pc3_mean_dict.get(x,0))\n\npc3_25p_dict = train.groupby(['Product_Category_3'])['Purchase'].apply(lambda x:np.percentile(x,25)).to_dict()\ntrain['Product_Cat3_25PercPrice'] = train['Product_Category_3'].apply(lambda x:pc3_25p_dict.get(x,0))\ntest['Product_Cat3_25PercPrice'] = test['Product_Category_3'].apply(lambda x:pc3_25p_dict.get(x,0))\n\npc3_75p_dict = train.groupby(['Product_Category_3'])['Purchase'].apply(lambda x:np.percentile(x,75)).to_dict()\ntrain['Product_Cat3_75PercPrice'] = train['Product_Category_3'].apply(lambda x:pc3_75p_dict.get(x,0))\ntest['Product_Cat3_75PercPrice'] = test['Product_Category_3'].apply(lambda x:pc3_75p_dict.get(x,0))","d3151dd0":"#seperating the dependant variable \ntrain_y = train[\"Purchase\"]\ntrain.drop([\"Purchase\"], axis=1, inplace=True)\n\n#filling missing values in product categories 2 & 3 by by any constant number say -999\ntrain.fillna(-999, inplace=True)\ntest.fillna(-999, inplace=True)\n\n#label encoding User ID and Product ID\nfrom sklearn.preprocessing import LabelEncoder\ncat_columns_list = [\"User_ID\", \"Product_ID\"]\nfor var in cat_columns_list:\n    lb = LabelEncoder()\n    full_var_data = pd.concat((train[var],test[var]),axis=0).astype('str')\n    temp = lb.fit_transform(np.array(full_var_data))\n    train[var] = lb.transform(np.array( train[var] ).astype('str'))\n    test[var] = lb.transform(np.array( test[var] ).astype('str'))","9eeec51a":"X_train,X_test,Y_train,Y_test = train_test_split(train,train_y,test_size=0.2,random_state=42)","3e572c75":"dtr = DecisionTreeRegressor(max_depth = 10)\ndtr.fit(X_train,Y_train)\ny_pred = dtr.predict(X_test)\n\n# finding the mean_squared error\nmse = mean_squared_error(Y_test, y_pred)\nprint(\"RMSE Error:\", np.sqrt(mse))\n\n# finding the r2 score or the variance\nr2 = r2_score(Y_test, y_pred)\nprint(\"R2 Score:\", r2)","b0ef67c2":"linear_test = test\nlinear_train = train\n\n#dropping User ID and Product ID\nlinear_train = pd.concat([linear_train,train_y],1)\nlinear_train = linear_train.drop(['User_ID', 'Product_ID'],1)\nlinear_test = linear_test.drop(['User_ID', 'Product_ID'],1)\n\n#removing rows corresponding to the unique categorical values only available in train set, not in test set\nlinear_train = linear_train.drop(linear_train[(linear_train['Product_Category_1']==19) | (linear_train['Product_Category_1']==20) | (linear_train['Product_Category_3']==2) | (linear_train['Product_Category_3']==7)].index)\n\nsns.boxplot(linear_train['Purchase'])\n\nq1 = linear_train['Purchase'].quantile(0.25)\nq3 = linear_train['Purchase'].quantile(0.75)\niqr = q3-q1 #Interquartile range\nfence_low  = q1-1.5*iqr\nfence_high = q3+1.5*iqr\nlinear_train = linear_train[(linear_train['Purchase'] > fence_low) & (linear_train['Purchase'] < fence_high)]\n\n#separating dependant variable\nlinear_train_y = linear_train[\"Purchase\"]\nlinear_train.drop([\"Purchase\"], axis=1, inplace=True)\n\n#chanding data type of categorical variable to string so as to one hot encode them using get_dummies\nstrlist = ['Gender', 'Age', 'Occupation', 'City_Category',\n       'Stay_In_Current_City_Years', 'Marital_Status', 'Product_Category_1',\n       'Product_Category_2', 'Product_Category_3']\nfor s in strlist:\n    linear_train[s] = linear_train[s].astype(str)\n    linear_test[s] = linear_test[s].astype(str)","17f65ea5":"#One Hot encoding categorical variables\nlinear_train = pd.get_dummies(linear_train, drop_first = True)\nlinear_test = pd.get_dummies(linear_test, drop_first = True)","09b9c3a6":"#Scaling all the variables in range of 0 to 1\nscaler = MinMaxScaler()\nlinear_train = scaler.fit_transform(linear_train)\nlinear_test = scaler.transform(linear_test)","75ab225f":"X_train,X_test,Y_train,Y_test = train_test_split(linear_train,linear_train_y,test_size=0.2,random_state=42)","43c43c8a":"from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, Y_train)\ny_pred = regressor.predict(X_test)\n\n# finding the mean_squared error\nmse = mean_squared_error(Y_test, y_pred)\nprint(\"RMSE Error:\", np.sqrt(mse))\n\n# finding the r2 score or the variance\nr2 = r2_score(Y_test, y_pred)\nprint(\"R2 Score:\", r2)","dd6fb853":"regressor = Ridge(alpha = 10)\nregressor.fit(X_train, Y_train)\ny_pred = regressor.predict(X_test)\n\n# finding the mean_squared error\nmse = mean_squared_error(Y_test, y_pred)\nprint(\"RMSE Error:\", np.sqrt(mse))\n\n# finding the r2 score or the variance\nr2 = r2_score(Y_test, y_pred)\nprint(\"R2 Score:\", r2)","678d172e":"regressor = Lasso(alpha = 0.1)\nregressor.fit(X_train, Y_train)\ny_pred = regressor.predict(X_test)\n\n# finding the mean_squared error\nmse = mean_squared_error(Y_test, y_pred)\nprint(\"RMSE Error:\", np.sqrt(mse))\n\n# finding the r2 score or the variance\nr2 = r2_score(Y_test, y_pred)\nprint(\"R2 Score:\", r2)","3d9f1a19":"regressor = ElasticNet(alpha = 0.05)\nregressor.fit(X_train, Y_train)\nregressor.fit(X_train, Y_train)\ny_pred = regressor.predict(X_test)\n\n# finding the mean_squared error\nmse = mean_squared_error(Y_test, y_pred)\nprint(\"RMSE Error:\", np.sqrt(mse))\n\n# finding the r2 score or the variance\nr2 = r2_score(Y_test, y_pred)\nprint(\"R2 Score:\", r2)","e0faad11":"alist = ['User_ID',\n'Product_ID',\n'Gender',\n'Age',\n'Occupation',\n'City_Category',\n'Stay_In_Current_City_Years',\n'Marital_Status',\n'Product_Category_1',\n'Product_Category_2',\n'Product_Category_3',\n'Age_Count',\n'Occupation_Count',\n'Product_Category_1_Count',\n'Product_Category_2_Count',\n'Product_Category_3_Count',\n'User_ID_Count',\n'Product_ID_Count']\n         \nblist = ['User_ID_MinPrice',\n'User_ID_MaxPrice',\n'User_ID_MeanPrice',\n'Product_ID_MinPrice',\n'Product_ID_MaxPrice',\n'Product_ID_MeanPrice']\n\nclist = ['User_ID_25PercPrice',\n'User_ID_75PercPrice',\n'Product_ID_25PercPrice',\n'Product_ID_75PercPrice',\n'Product_Cat1_MinPrice',\n'Product_Cat1_MaxPrice',\n'Product_Cat1_MeanPrice',\n'Product_Cat1_25PercPrice',\n'Product_Cat1_75PercPrice',\n'Product_Cat2_MinPrice',\n'Product_Cat2_MaxPrice',\n'Product_Cat2_MeanPrice',\n'Product_Cat2_25PercPrice',\n'Product_Cat2_75PercPrice',\n'Product_Cat3_MinPrice',\n'Product_Cat3_MaxPrice',\n'Product_Cat3_MeanPrice',\n'Product_Cat3_25PercPrice',\n'Product_Cat3_75PercPrice']","2d1a70ed":"#XGB model 1 dataframe\ntrain1 = train[alist+blist]\ntest1 = test[alist+blist]\n\n#XGB model 2 dataframe \ntrain2 = train[alist+clist]\ntest2 = test[alist+clist]","75d2596b":"X_train,X_test,Y_train,Y_test = train_test_split(train1,train_y,test_size=0.2,random_state=42)","3fc7e655":"params = {}\nparams[\"eta\"] = 0.05\nparams[\"min_child_weight\"] = 10\nparams[\"subsample\"] = 0.8\nparams[\"colsample_bytree\"] = 0.7\nparams[\"max_depth\"] = 10\nparams[\"seed\"] = 0\nplst = list(params.items())\nnum_rounds = 750","8c9bdb58":"#Takes 11 minutes to train\nxgtrain = xgb.DMatrix(X_train, label=Y_train)\nxgtest = xgb.DMatrix(X_test)\nmodel1 = xgb.train(plst, xgtrain, num_rounds)","4f860915":"pred_test_y = model1.predict(xgtest)\nfrom sklearn.metrics import mean_squared_error, r2_score\nrmse = np.sqrt(mean_squared_error(pred_test_y, Y_test))\nprint(\"RMSE Score: \",rmse)\nr2 = r2_score(Y_test, pred_test_y)\n\nprint(\"R2 Score:\", r2)\n\n#This will print\n#RMSE Score:  2407.2232066688994\n#R2 Score: 0.7693750669848086","977ceb63":"feature_important = model1.get_score(importance_type='gain')\nkeys = list(feature_important.keys())\nvalues = list(feature_important.values())\ntotal = sum(values)\nnew = [value * 100. \/ total for value in values]\nnew = np.round(new,2)\n\nfeature_importances = pd.DataFrame()\nfeature_importances['Features'] = keys\nfeature_importances['Importance (%)'] = new\nfeature_importances = feature_importances.sort_values(['Importance (%)'],ascending=False).reset_index(drop=True)\nfeature_importances\nfeature_importances.style.set_properties(**{'font-size':'10pt'})","9a23b5ef":"plt.figure(figsize=(20, 8))\nsns.barplot(data=feature_importances, x='Importance (%)', y='Features');\nplt.title('Feature importance',fontsize=24)\nplt.xlabel('Importance (%)',fontsize=20)\nplt.yticks(fontsize=15)\nplt.xticks(fontsize=15)\nplt.ylabel('Features',fontsize=20)","9b415e70":"#This will take 17 mins to run\nxgtrain = xgb.DMatrix(train, label=train_y)\nxgtest = xgb.DMatrix(test)\nmodel1 = xgb.train(plst, xgtrain, num_rounds)\n\n#prediction1\npred_xgb_m1 = model1.predict(xgtest)\nsubmission['Purchase'] = pred_xgb_m1\nsubmission.to_csv('xgb_model1.csv',index=False)","f65faa57":"X_train,X_test,Y_train,Y_test = train_test_split(train2,train_y,test_size=0.2,random_state=42)","cf171c05":"#These parameters are found using GridSearchCV by checking R2 score & RMSE on validation set\nparams = {}\nparams[\"eta\"] = 0.03\nparams[\"min_child_weight\"] = 10\nparams[\"subsample\"] = 0.8\nparams[\"colsample_bytree\"] = 0.7\nparams[\"max_depth\"] = 10\nparams[\"seed\"] = 0\nplst = list(params.items())\nnum_rounds = 1100\nimport xgboost as xgb\nxgtrain = xgb.DMatrix(X_train, label=Y_train)\nxgtest = xgb.DMatrix(X_test)","f6dc9b06":"#Takes 18 minutes to train\nmodel2 = xgb.train(plst, xgtrain, num_rounds)","ad3b677c":"pred_test_y = model2.predict(xgtest)\nfrom sklearn.metrics import mean_squared_error, r2_score\nrmse = np.sqrt(mean_squared_error(pred_test_y, Y_test))\nprint(\"RMSE Score: \",rmse)\nr2 = r2_score(Y_test, pred_test_y)\n\nprint(\"R2 Score:\", r2)\n\n#This will print\n#RMSE Score:  2415.988043238194\n#R2 Score: 0.7676925725333128","c2575a16":"feature_important = model2.get_score(importance_type='gain')\nkeys = list(feature_important.keys())\nvalues = list(feature_important.values())\ntotal = sum(values)\nnew = [value * 100. \/ total for value in values]\nnew = np.round(new,2)\n\nfeature_importances = pd.DataFrame()\nfeature_importances['Features'] = keys\nfeature_importances['Importance (%)'] = new\nfeature_importances = feature_importances.sort_values(['Importance (%)'],ascending=False).reset_index(drop=True)\nfeature_importances\nfeature_importances.style.set_properties(**{'font-size':'10pt'})","b35a8471":"plt.figure(figsize=(20, 11))\nsns.barplot(data=feature_importances, x='Importance (%)', y='Features');\nplt.title('Feature importance',fontsize=24)\nplt.xlabel('Importance (%)',fontsize=20)\nplt.yticks(fontsize=15)\nplt.xticks(fontsize=15)\nplt.ylabel('Features',fontsize=20)","91822df4":"xgtrain = xgb.DMatrix(train2, label=train_y)\nxgtest = xgb.DMatrix(test2)\nmodel2 = xgb.train(plst, xgtrain, num_rounds)\n\n#prediction2\npred_xgb_m2 = model2.predict(xgtest)\nsubmission['Purchase'] = pred_xgb_m2\nsubmission.to_csv('xgb_model2.csv',index=False)","6cf3955d":"submission['Purchase'] = 0.3*pred_xgb_m1 + 0.7*pred_xgb_m2\nsubmission.to_csv('A016AmanJain.csv',index=False)","597fa053":"**Label encoding** following columns : Gender, Age, City Category,Stay in Current City Years","59214d02":"Getting the feature importance for XG Boost model **1** ,\n* type of feature importance = 'gain\u2019: the average gain across all splits the feature is used in","25bf7667":"Learning rate is arbitrarily set and then other parameters are found using GridSearchCV by checking R2 score & RMSE on validation set, and some parameters are set as per the general standard","8503405e":"Plotting the Feature importane on a scale of 0 to 100 (total combined features impoprtance =100)","915b9d07":"Creating **2 XGBoost Models** *both with* **different learning rates** *and* **different set of features** for both of them\n1. XG Boost model 1 (learning rate = **0.5**), and features set = **alist + blist** (mentioned below)\n1. XG Boost model 2 (learinng rate = **0.3**), and features set = **alist + **clist** (mentioned below)","9788f587":"Getting the feature importance for XG Boost model **2** ,\n* type of feature importance = 'gain\u2019: the average gain across all splits the feature is used in","478904b2":"**2. Linear Regression**","efb5ee46":"Also, creating **25th and 75th percentile** columns on **Purchase** feature for each category","a1e2096f":"Creating a **Count Column** for each caetgory","9049e278":"> *For All the models,*\n1. Splitting Train set into train and validation set in 80:20 ratio\n1. **Hyperparameter** have been found by checking **R2 & RMSE score on validation set** using **GridSearchCV**\n1. Training the model with complete Train set and getting **predictions** for '**Purchase' feature** of Test set in a **.csv file**","9f5f6e4e":"**Training the XG Boost model 1** with the above mentioned hyperparameters","e38c80e5":"** 1. Decision Tree Regressor**","30391366":"**Ensemble Model**","5f165ab9":"Generating XG Boost model 2 predictions","1eb20cdd":"**7. XG Boost model 2** (same as above, but with different hyperparameters and different set of features)","47f5be26":"1. **One Hot Encoding** all Categorical features for all Linear models\n1. Removing rows corresponding to the unique categorical values only available in train set, not in test set\n1. **Removing outliers** rows by performing **boxplot analysis on Purchase** feature","b5929928":"*Checking R2 and RMSE score on validation set for the below 4 linear models*","80286e78":"Checking the R2 and RMSE score on validation set using XG Boost model **1**","f98e254d":"**5. Elastic Net**","15cd2efb":"Plotting the Feature importane on a scale of 0 to 100 (total combined features impoprtance =100)","8f9904f6":"Boring Stuff Over, Now the fun begins","e8663dd2":"**Feature Engineering**","405e4524":"**6. XG Boost model 1**","245034d6":"Splitting into train and validation to calculate R2 & RMSE score on validation set","f9ed651f":"**4. Lasso**","be6c5c82":"Checking the R2 and RMSE score on validation set using XG Boost model **2**","337694f2":"Creating **mean, max, min columns** on **Purchase** feature for each category","32e91e33":"Weighted Average of the above 2 XG Boost models","4ebb3710":"Generating predctions of Purchase feature on test set and exporting it to a csv file","b84a8ec9":"**2. Linear Regression**","8a021fe7":"Splitting into train and validation and appliing 4 Linear Models: ","c3c7ff35":"**3. Ridge**"}}