{"cell_type":{"c4d3294d":"code","33d1acf5":"code","8ea1e72b":"code","27d2ee62":"code","ee145b5b":"code","f38a246f":"code","12bb5867":"code","98d2ec11":"code","d95e7412":"code","c6176ac9":"code","ba0051a0":"code","e7675a45":"code","fa9d0581":"code","9bd7dbc3":"code","f7963bd6":"code","5656d57c":"code","8c02ecf2":"code","08e49cd2":"code","e9601a80":"code","da077734":"code","fc900d71":"code","b11017ce":"code","8a343eb5":"code","4a4b83e6":"code","2d62cd1b":"code","fd36f1e0":"code","9d46928b":"code","2228b148":"code","39de47d9":"code","d19860d5":"code","9851c261":"code","551ac1db":"code","73fdb442":"code","85de94ca":"code","e13029f9":"code","514a8396":"code","a446cdc6":"markdown","34ab7e64":"markdown","5817c54a":"markdown","cb092e15":"markdown","723b1390":"markdown","e49e0f24":"markdown","4017c627":"markdown","dbeffd7f":"markdown","e1b52f25":"markdown","35ccb4f1":"markdown","161bb597":"markdown","6fe6994b":"markdown","58547b41":"markdown","860cdb1d":"markdown","e3729648":"markdown","a496b115":"markdown","0e844b2a":"markdown","9fb98dac":"markdown","acc178f9":"markdown","a255f795":"markdown","a5dc129a":"markdown","f4c39884":"markdown","bb8e34f3":"markdown","1b6756ac":"markdown","6cd83038":"markdown","3673f9bd":"markdown","4d74890b":"markdown"},"source":{"c4d3294d":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nfrom sklearn.linear_model import Lasso, Ridge, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor","33d1acf5":"sns.set_style('darkgrid')","8ea1e72b":"df = pd.read_csv('..\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv')\ndf.shape","27d2ee62":"df.head()","ee145b5b":"# Serial No. is not needed. let's drop it\ndf.drop('Serial No.', axis=1, inplace=True)\n\n# make sure it is dropped\ndf.head()","f38a246f":"df.shape","12bb5867":"# check data types of all features\ndf.dtypes","98d2ec11":"# is there any nulls?\ndf.isnull().sum()","d95e7412":"# what about NA?\ndf.isna().sum()","c6176ac9":"df.rename(columns={'GRE Score':'GRE', \n                   'TOEFL Score':'TOEFL', \n                   'University Rating':'uni_rating',\n                   'Chance of Admit ':'admit_prob'}, inplace=True)\ndf.head(10)","ba0051a0":"df.tail(10)","e7675a45":"# let's check summary of all feature distribution\ndf.describe()","fa9d0581":"# Plot Admission chance vs GRE score\nplt.scatter(df.GRE, df.admit_prob, alpha=0.25)\nplt.title('Admission Chance vs GRE Score')\nplt.xlabel('GRE Score')\nplt.ylabel('Admission Chance')\nplt.show()","9bd7dbc3":"# Plot Admission chance vs TOEFL score\nplt.scatter(df.TOEFL, df.admit_prob, alpha=0.25)\nplt.title('Admission Chance vs TOEFL Score')\nplt.xlabel('TOEFL Score')\nplt.ylabel('Admission Chance')\nplt.show()","f7963bd6":"# Plot Admission chance vs CGPA\nplt.scatter(df.CGPA, df.admit_prob, alpha=0.25)\nplt.title('Admission Chance vs CGPA')\nplt.xlabel('CGPA')\nplt.ylabel('Admission Chance')\nplt.show()","5656d57c":"# Let's look at all distributions\ndf.hist(figsize=(9,9), xrot=45)\nplt.show()","8c02ecf2":"sns.lmplot(x='GRE', y='admit_prob', hue='Research', data=df, fit_reg=False, \n           scatter_kws={'alpha':0.25, 's':100})\nplt.xlabel('GRE Score')\nplt.ylabel('Admission Chance')\nplt.show()","08e49cd2":"sns.lmplot(x='TOEFL', y='admit_prob', hue='Research', data=df, fit_reg=False,\n           scatter_kws={'alpha':0.25, 's':100})\nplt.xlabel('TOEFL Score')\nplt.ylabel('Admission Chance')\nplt.show()","e9601a80":"sns.lmplot(x='CGPA', y='admit_prob', hue='Research', data=df, fit_reg=False,\n           scatter_kws={'alpha':0.25, 's':100})\nplt.xlabel('CGPA')\nplt.ylabel('Admission Chance')\nplt.show()","da077734":"df.corr()","fc900d71":"sns.set_style('white')\nplt.figure(figsize=(7,6))\n\ndf_corr = df.corr()\n\nbool_mask = np.zeros_like(df_corr)\nbool_mask[np.triu_indices_from(df_corr)]=1\n\nsns.heatmap(df_corr, cmap='RdBu_r', annot=True, mask=bool_mask, cbar=False)\n\nplt.show()","b11017ce":"# Check if there are any duplicate records\ndf.duplicated().sum()","8a343eb5":"# let's create X and y to separate input data from target variable\ny = df.admit_prob\nX = df.drop('admit_prob', axis=1)","4a4b83e6":"# split total data into train and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.2, \n                                                    random_state=1234)","2d62cd1b":"print(len(X_train), len(X_test), len(y_train), len(y_test))","fd36f1e0":"X_train.head()","9d46928b":"y_train.head()","2228b148":"# Create pipelines dictionary\npipelines = {\n    'lasso': make_pipeline(StandardScaler(), Lasso(random_state=1234)),\n    'ridge': make_pipeline(StandardScaler(), Ridge(random_state=1234)),\n    'enet': make_pipeline(StandardScaler(), ElasticNet(random_state=1234)),\n    'rf': make_pipeline(StandardScaler(), RandomForestRegressor(random_state=1234)),\n    'gb': make_pipeline(StandardScaler(), GradientBoostingRegressor(random_state=1234))\n}","39de47d9":"# Lasso hyperparameters\nlasso_hyperparameters = {\n    'lasso__alpha': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10]\n}\n\n# Ridge hyperparameters\nridge_hyperparameters = {\n    'ridge__alpha': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100]\n}\n\n# Elastic Net hyperparameters\nenet_hyperparameters = {\n    'elasticnet__alpha': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10],\n    'elasticnet__l1_ratio': [0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 0.9]\n}\n\n# RandomForest hyperparameters\nrf_hyperparameters = {\n    'randomforestregressor__n_estimators': [100, 200, 500, 1000],\n    'randomforestregressor__max_features': ['auto', 'sqrt', 0.33, 0.25]\n}\n\n# GradientBoost hyperparameters\ngb_hyperparameters = {\n    'gradientboostingregressor__n_estimators': [100, 200, 500],\n    'gradientboostingregressor__learning_rate': [0.05, 0.1, 0.2, 0.3],\n    'gradientboostingregressor__max_depth': [1, 3, 5, 7]\n}","d19860d5":"# hyperparameters dictionary\nhyperparameters = {\n    'lasso': lasso_hyperparameters,\n    'ridge': ridge_hyperparameters,\n    'enet': enet_hyperparameters,\n    'rf': rf_hyperparameters,\n    'gb': gb_hyperparameters\n}","9851c261":"import time\n\n# Create empty dictionary\nfitted_models = {}\n\n# Loop through model pipelines, tuning each one and storing in fitted_models\nfor name,pipeline in pipelines.items():\n    start = time.time()\n    model = GridSearchCV(pipeline, hyperparameters[name], cv=10, n_jobs=-1)\n    \n    # Fit model on X_train, y_train\n    model.fit(X_train, y_train)\n    \n    # Store a model in fitted_models\n    fitted_models[name] = model\n    \n    end = time.time()\n    print(name, ' has been fitted. Took ', np.around(end - start, decimals=2), ' seconds')","551ac1db":"# let's check that the models have been fitted correctly\nfrom sklearn.exceptions import NotFittedError\n\nfor name, model in fitted_models.items():\n    try:\n        pred = model.predict(X_test)\n        print(name, 'has been fitted.')\n    except NotFittedError as e:\n        print(repr(e))","73fdb442":"# let's evaluate fitted models and print performance scores\nfor name,model in fitted_models.items():\n    pred = model.predict(X_test)\n    print()\n    print(name)\n    print('------------')\n    print('R2 :', np.around(r2_score(y_test, pred), decimals=4))\n    print('MSE :', np.around(mean_squared_error(y_test, pred), decimals=4))\n    print('Holdout R2 :', np.around(model.best_score_, decimals=4))","85de94ca":"sns.set_style('darkgrid')\nplt.scatter(fitted_models['ridge'].predict(X_test), y_test, alpha=0.25, s=100)\nplt.title('Predicted vs Actual')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()","e13029f9":"fitted_models['ridge'].best_params_","514a8396":"fitted_models['ridge'].best_estimator_","a446cdc6":"Let's look at relation of few important features with target feature","34ab7e64":"All graphs above clearly show Research has low influence on Admission chance.","5817c54a":"## Library Imports\n\nLet's import libraries first","cb092e15":"CGPA higher than 8.5 has good chance of admission","723b1390":"This dataset is created for prediction of Graduate Admissions from an Indian perspective.\n\nThe dataset contains several parameters which are considered important during the application for Masters Programs.\nThe parameters included are :\n\n1. GRE Scores ( out of 340 )\n2. TOEFL Scores ( out of 120 )\n3. University Rating ( out of 5 )\n4. Statement of Purpose and Letter of Recommendation Strength ( out of 5 )\n5. Undergraduate GPA ( out of 10 )\n6. Research Experience ( either 0 or 1 )\n7. Chance of Admit ( ranging from 0 to 1 )\n\nSo, let's get started.","e49e0f24":"Ridge regression model is the clear winner among all models. It has the highest R2 score, highest Holdout R2 and lowest MSE.","4017c627":"Few key findings discovered from this dataset are:\n\n1. CGPA is the most important factor affecting student's rate of admission.\n2. GRE and TOEFL are the second important factors having high impact on graduate admission.\n3. Research is not much deciding factor while considering graduate admission.","dbeffd7f":"## Insights & Analysis","e1b52f25":"### Winner Selection","35ccb4f1":"Since this problem is the regression one, Let's try few widely used regression algorithms available. The algorithms Let's try Lasso regression, Ridge regression, ElasticNet regression, RandomForest regression and GradientBoosting regression.\n\nThe key hyperparameters for each of the algorithm are as follows:\n\nLasso: alpha\nRidge: alpha\nElasticNet: alpha, l1_ratio\nRandomForest: n_estimators, max_features\nGradientBoost: n_estimators, learning_rate, max_depth","161bb597":"## Data cleaning","6fe6994b":"## Model Training","58547b41":"Let's see Predicted vs Actual graph of winning model","860cdb1d":"Let's check the hyperparametrs of winning model.","e3729648":"Looks like GRE score of 320 or higher has good chance of admission.","a496b115":"### Hyperparameter Tuning","0e844b2a":"Looks like all features are numeric. Not a single categorical one. That's good. Since the target feature is Numeric, this is the regression problem.","9fb98dac":"Let's check whether Research has any influence on chance of admission","acc178f9":"## Algorithm Selection","a255f795":"### Pre-Processing & Pipelines","a5dc129a":"Thank you for reading. If you like it, please upvote. It will motivate me to keep adding good quality content.","f4c39884":"## Exploratory Analysis\n\nLet's load the dataset","bb8e34f3":"### Data Splitting","1b6756ac":"Looks like TOEFL score of higher than 110 has good chance of admission.","6cd83038":"It clearly shows that GRE, TOEFL, CGPA are highly correlated with one another and with target features. CGPA is the most correlated with target feature.","3673f9bd":"Let's rename the columns for simplicity","4d74890b":"Finally, let's look how all features are correlated with one another"}}