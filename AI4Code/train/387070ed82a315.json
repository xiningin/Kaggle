{"cell_type":{"3714bca4":"code","e8378690":"code","33c12763":"code","9a72c5ea":"code","f0975662":"code","d14e6747":"code","eae875c9":"code","123847a6":"code","5f3fc18f":"code","a64afafe":"code","b0a4aece":"code","dc07491a":"code","88b7a95e":"code","de049b60":"code","545a1632":"code","dcc1a5c7":"code","73ed7394":"code","9f7b6936":"code","0f6ed54c":"code","b2ec5ab9":"code","3c777850":"code","a2e3124c":"code","10c2713b":"code","51004944":"code","82de3e7c":"code","acf54544":"code","1d4cab28":"code","379afba3":"code","86cbb47e":"markdown","d30bf847":"markdown","3de2afec":"markdown","ff0a6ecb":"markdown","c2f9c5d4":"markdown"},"source":{"3714bca4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport json\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e8378690":"!pip install transformers -q\n!pip install wandb -q","33c12763":"import torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n\n# Importing the T5 modules from huggingface\/transformers\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration","9a72c5ea":"# Checking out the GPU we have access to. This is output is from the google colab version. \n!nvidia-smi","f0975662":"# # Setting up the device for GPU usage\nfrom torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'\n\n# Preparing for TPU usage\n# import torch_xla\n# import torch_xla.core.xla_model as xm\n# device = xm.xla_device()\n\nprint(device)","d14e6747":"train_df = pd.read_json(\"\/kaggle\/input\/naturalqa\/nq-open-master\/NQ-open.train.jsonl\", orient='columns', lines=True)\ntrain_df.head()","eae875c9":"dev_df = pd.read_json(\"\/kaggle\/input\/naturalqa\/nq-open-master\/NQ-open.dev.jsonl\", orient='columns', lines=True)\ndev_df.head()","123847a6":"number_of_answers = pd.Series([len(train_df['answer'][i]) for i in range(len(train_df['answer']))])\nnumber_of_answers.value_counts()","5f3fc18f":"train_df['number_of_answers']=number_of_answers\ntrain_df.head()","a64afafe":"sample_questions = train_df['question'].head()\nsample_answers = train_df['answer'].head()\nsample_answers","b0a4aece":"tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")","dc07491a":"print(sample_questions[0])\ntokenizer.encode(sample_questions[0])","88b7a95e":"sample_answers[4]","de049b60":"multi_answers = train_df.query('number_of_answers>1')['answer']\nmulti_answers.head()","545a1632":"answers = ' <sep> '.join(multi_answers[7])\nanswers","dcc1a5c7":"# Creating a custom dataset for reading the dataframe and loading it into the dataloader to pass it to the neural network at a later stage for finetuning the model and to prepare it for predictions\n\nclass CustomDataset(Dataset):\n\n    def __init__(self, dataframe, tokenizer, source_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.source_len = source_len\n        self.question = self.data.question\n        self.answer = self.data.answer\n\n    def __len__(self):\n        return len(self.question)\n\n    def __getitem__(self, index):\n        question = str(self.question[index])\n        question = 'trivia question: '+' '.join(question.split())+'?'\n        answer = ' <sep> '.join(self.answer[index])\n        answer = ' '.join(answer.split())\n        \n        print(question,\":\",answer)\n        \n        source = self.tokenizer.batch_encode_plus([question], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n        target = self.tokenizer.batch_encode_plus([answer], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n        \n        source_ids = source['input_ids'].squeeze()\n        source_mask = source['attention_mask'].squeeze()\n        target_ids = target['input_ids'].squeeze()\n        target_mask = target['attention_mask'].squeeze()\n\n        return {\n            'source_ids': source_ids.to(dtype=torch.long), \n            'source_mask': source_mask.to(dtype=torch.long), \n            'target_ids': target_ids.to(dtype=torch.long),\n            'target_ids_y': target_ids.to(dtype=torch.long)\n        }","73ed7394":"# Creating the training function. This will be called in the main function. It is run depending on the epoch value.\n# The model is put into train mode and then we wnumerate over the training loader and passed to the defined network \n\ndef train(epoch, tokenizer, model, device, loader, optimizer):\n    model.train()\n    for _,data in enumerate(loader, 0):\n        y = data['target_ids'].to(device, dtype = torch.long)\n        y_ids = y[:, :-1].contiguous()\n        lm_labels = y[:, 1:].clone().detach()\n        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n        ids = data['source_ids'].to(device, dtype = torch.long)\n        mask = data['source_mask'].to(device, dtype = torch.long)\n        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n        loss = outputs[0]\n\n        if _%500==0:\n            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        # xm.optimizer_step(optimizer)\n        # xm.mark_step()","9f7b6936":"def validate(epoch, tokenizer, model, device, loader):\n    model.eval()\n    predictions = []\n    actuals = []\n    with torch.no_grad():\n        for _, data in enumerate(loader, 0):\n            y = data['target_ids'].to(device, dtype = torch.long)\n            ids = data['source_ids'].to(device, dtype = torch.long)\n            mask = data['source_mask'].to(device, dtype = torch.long)\n                \n            generated_ids = model.generate(\n                input_ids = ids,\n                attention_mask = mask, \n                max_length=150, \n                repetition_penalty=2.5, \n                length_penalty=1.0, \n                early_stopping=True\n                )\n            \n            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True) for t in y]\n            \n            if _%100==0:\n                print(f'Completed {_}')\n\n            predictions.extend(preds)\n            actuals.extend(target)\n    return predictions, actuals","0f6ed54c":"TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\nVALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\nTRAIN_EPOCHS = 2\n# number of epochs to train (default: 10)\nVAL_EPOCHS = 1 \nLEARNING_RATE = 1e-4    # learning rate (default: 0.01)\nSEED = 42               # random seed (default: 42)\nMAX_LEN = 512\n\n# Set random seeds and deterministic pytorch for reproducibility\ntorch.manual_seed(SEED) # pytorch random seed\nnp.random.seed(SEED) # numpy random seed\ntorch.backends.cudnn.deterministic = True\n\n# tokenzier for encoding the text\ntokenizer = T5Tokenizer.from_pretrained(\"t5-base\")","b2ec5ab9":"# Creation of Dataset and Dataloader\n# Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \ntrain_size = 0.95\ntrain_df = pd.read_json(\"\/kaggle\/input\/naturalqa\/nq-open-master\/NQ-open.train.jsonl\", orient='columns', lines=True)\ntrain_df = train_df[:10000]\ntrain_dataset=train_df.sample(frac=train_size, random_state = SEED).reset_index(drop=True)\nval_dataset=train_df.drop(train_dataset.index).reset_index(drop=True)\nval_dataset = val_dataset\nprint(\"FULL Dataset: {}\".format(train_df.shape))\nprint(\"TRAIN Dataset: {}\".format(train_dataset.shape))\nprint(\"TEST Dataset: {}\".format(val_dataset.shape))","3c777850":"# Creating the Training and Validation dataset for further creation of Dataloader\ntraining_set = CustomDataset(train_dataset[:20], tokenizer, MAX_LEN)\nval_set = CustomDataset(val_dataset[:20], tokenizer, MAX_LEN)\n\n# Defining the parameters for creation of dataloaders\ntrain_params = {\n    'batch_size': TRAIN_BATCH_SIZE,\n    'shuffle': True,\n    'num_workers': 0\n    }\n\nval_params = {\n    'batch_size': VALID_BATCH_SIZE,\n    'shuffle': False,\n    'num_workers': 0\n    }\n\n# Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\ntraining_loader = DataLoader(training_set, **train_params)\nval_loader = DataLoader(val_set, **val_params)","a2e3124c":"for itm in training_loader:\n    print()","10c2713b":"for itm in val_loader:\n    print()","51004944":"# Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n# Further this model is sent to device (GPU\/TPU) for using the hardware.\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\nmodel = model.to(device)\n\n# Defining the optimizer that will be used to tune the weights of the network in the training session. \noptimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)","82de3e7c":"# Training loop\n# print('Initiating Fine-Tuning for the model on our dataset')\n\n# for epoch in range(TRAIN_EPOCHS):\n#     train(epoch, tokenizer, model, device, training_loader, optimizer)","acf54544":"# Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n# Saving the dataframe as predictions.csv\n# print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n# predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n# final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})","1d4cab28":"#  final_df.to_csv('predictions.csv')","379afba3":"# for i in range(20):\n#     print(\"Actual Answer: \", final_df['Actual Text'][i],\"\\nPredicted Answer: \", final_df['Generated Text'][i])\n#     print()","86cbb47e":"## DataProcessing","d30bf847":"## Training and Validation","3de2afec":"## Imports","ff0a6ecb":"## Data Preprocessing","c2f9c5d4":"## Data Exploration"}}