{"cell_type":{"4dd95a4b":"code","761b24c0":"code","cb983fc4":"code","b6e637d4":"code","d2eebbdc":"code","e2f240a5":"code","4f6ebd46":"code","f2d659a6":"code","1082fdd4":"code","10abb944":"code","7b5d8612":"code","f0985b1e":"code","93a3a670":"code","8a17f721":"code","16db4968":"code","b4875a66":"code","67232594":"code","09ad9d6c":"code","36941a9d":"code","02bc3a1d":"code","144ca36d":"code","e6539414":"code","354883ab":"code","b745eb36":"code","aa914c4b":"code","89081ce0":"code","b13ae64e":"code","7ea227a7":"code","1081eb1e":"code","c9e6fb66":"code","ec81f530":"code","887f76c4":"code","075dec79":"code","ee316915":"code","030a4701":"code","a9e9b2a5":"code","5e4aed1a":"code","d89ae2ba":"code","1d3e886e":"code","4337ebba":"code","1a74da26":"code","a43bd649":"code","043f88ed":"code","794519ca":"code","cd9e0c9e":"code","69a7f369":"code","b30801c7":"code","765caff4":"code","945dda42":"code","89990bfe":"code","73315373":"code","f1e5a501":"code","3b2fab79":"code","89594efc":"code","eea6fae4":"markdown","49780acd":"markdown","f4728345":"markdown","0c5d169a":"markdown","bf7471f1":"markdown","e7ce9733":"markdown","ba5d086a":"markdown","31c89a4d":"markdown","42a17cea":"markdown","b04a0c50":"markdown","8a942dde":"markdown","7e8efa9b":"markdown","825db9c7":"markdown","69e81382":"markdown","3365718d":"markdown","93db7ebd":"markdown","9d812e85":"markdown","89e9df7e":"markdown","b288cc60":"markdown","6d6e09f7":"markdown","b399d540":"markdown","e5ddef45":"markdown","0a40f8bf":"markdown","bc206188":"markdown","293711be":"markdown","8674cd0f":"markdown","085ab843":"markdown","d6c78640":"markdown","107bf2bb":"markdown","a12e16cf":"markdown","5656b76e":"markdown","73922bd6":"markdown","b7eb0fdc":"markdown","1685d22b":"markdown","b633c4dc":"markdown","d2fc0cdb":"markdown","ff88d5ab":"markdown","7b1b8aef":"markdown","bb5b858f":"markdown","95d0effa":"markdown","f61940c6":"markdown","8ce37d4d":"markdown","45c01499":"markdown"},"source":{"4dd95a4b":"# Importing the libraries\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nimport plotly\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport matplotlib.gridspec as gridspec\n\nfrom collections import Counter\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\n\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.metrics import classification_report,accuracy_score, roc_curve, precision_score\nfrom sklearn.metrics import confusion_matrix, recall_score, roc_auc_score, f1_score, precision_recall_curve\nfrom sklearn.model_selection import cross_val_score\nfrom imblearn.pipeline import make_pipeline\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.ensemble import IsolationForest","761b24c0":"data = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","cb983fc4":"data.shape","b6e637d4":"data.head().style.set_properties(**{\"background-color\":\"black\",\n                           \"color\" : \"lawngreen\"})","d2eebbdc":"data.info()","e2f240a5":"data.columns","4f6ebd46":"data[['Time', 'Amount']].describe().style.set_properties(**{\"background-color\":\"black\",\n                           \"color\" : \"yellow\"})","f2d659a6":"# Descriptive statistics of  of frauds transactions\n\nsummary = (data[data['Class'] == 1].describe().transpose().reset_index())\n\nsummary = summary.rename(columns = {\"index\" : \"feature\"})\nsummary = np.around(summary,3)\n\nval_lst = [summary['feature'], summary['count'],\n           summary['mean'],summary['std'],\n           summary['min'], summary['25%'],\n           summary['50%'], summary['75%'], summary['max']]\n\ntrace  = go.Table(header = dict(values = summary.columns.tolist(),\n                                line = dict(color = ['#506784']),\n                                fill = dict(color = ['skyblue']),\n                               ),\n                  cells  = dict(values = val_lst,\n                                line = dict(color = ['#506784']),\n                                fill = dict(color = [\"lightgrey\",'#F5F8FF'])\n                               ),\n                  columnwidth = [150,60,100,100,80,80,80,80,80])\nlayout = go.Layout(dict(title = \"Summary for fraudulent Transactions\"))\nfigure = go.Figure(data=[trace],layout=layout)\npy.iplot(figure)\n","1082fdd4":"# Descriptive statistics of geniune transactions\n\nsummary = (data[data['Class'] == 0].describe().transpose().reset_index())\n\nsummary = summary.rename(columns = {\"index\" : \"feature\"})\nsummary = np.around(summary,3)\n\nval_lst = [summary['feature'], summary['count'],\n           summary['mean'],summary['std'],\n           summary['min'], summary['25%'],\n           summary['50%'], summary['75%'], summary['max']]\n\ntrace  = go.Table(header = dict(values = summary.columns.tolist(),\n                                line = dict(color = ['#506784']),\n                                fill = dict(color = ['lightgreen']),\n                               ),\n                  cells  = dict(values = val_lst,\n                                line = dict(color = ['#506784']),\n                                fill = dict(color = [\"lightgrey\",'#F5F8FF'])\n                               ),\n                  columnwidth = [130,100,100,100,80,80,80,80,80])\nlayout = go.Layout(dict(title = \"Summary for Geniune Transactions\"))\nfigure = go.Figure(data=[trace],layout=layout)\npy.iplot(figure)\n","10abb944":"# Percentages of fraudulent and non-fradulent transactions in data\n\nprint(f'Percent of Non-Fraudulent Transactions = {round(data[\"Class\"].value_counts()[0]\/len(data) * 100,3)}%')\nprint(f'Percent of Fraudulent Transactions = {round(data[\"Class\"].value_counts()[1]\/len(data) * 100,3)}%')","7b5d8612":"# plotting a pie chart for fraud and non-fraud transactions\n\nfraud_or_not = data[\"Class\"].value_counts().tolist()\n\nlabels = ['Not Fraud','Frauds']\nvalues = [fraud_or_not[0], fraud_or_not[1]]\ncolors = ['skyblue', 'red']\n\ntrace = go.Pie(labels=labels, values=values, textinfo='value', \n               textfont=dict(size=20),\n               marker=dict(colors=colors, line=dict(color='#000000', width=0.1)))\n\nplotly.offline.iplot([trace], filename='styled_pie_chart')\n","f0985b1e":"print('\\n\\033[1m  Fraudulent Transaction Distribution by amount \\033[0m')\nprint(\"-\"*50)\nprint(data[(data['Class'] == 1)]['Amount'].value_counts().head())","93a3a670":"corr = data.corrwith(data['Class']).reset_index()\n\ncorr.columns = ['Index','Correlations']\ncorr = corr.set_index('Index')\ncorr = corr.sort_values(by=['Correlations'], ascending = False)\n\nplt.figure(figsize=(8, 15))\nfig = sns.heatmap(corr, annot=True, fmt=\"g\", cmap='Set3', linewidths=0.4, linecolor='green')\n\nplt.title(\"Correlation of Variables with Class\", fontsize=20)\nplt.show()","8a17f721":"# Heatmap for explainatory variables\n\nplt.subplots(figsize=(12,8))\nplt.title(\"Correlation Matrix\", fontsize=15)\nsns.heatmap(data[['Time', 'Amount','Class']].corr(),linewidths=0.9, cmap=\"Paired\", linecolor='green',annot=True, annot_kws={'size':16},);","16db4968":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(20,10))\n\nfraud_transactions = data.Time[data.Class == 1]\nnormal_transactions = data.Time[data.Class == 0]\n\nax1.hist(fraud_transactions, bins = 50, color='deeppink', edgecolor=\"black\")\nax1.set_xlim([min(fraud_transactions), max(fraud_transactions)])\nax1.set_title('Fraudulent Transactions', fontsize=15)\nax1.set_ylabel(\"Number of Transactions\",  fontsize=13)\n\nax2.hist(normal_transactions, bins = 50, color='deepskyblue', edgecolor=\"black\")\nax2.set_xlim([min(normal_transactions), max(normal_transactions)])\nax2.set_title('Normal Transactions',  fontsize=15)\n\nax2.set_xlabel('Time (in Seconds)',  fontsize=13)\nax2.set_ylabel('Number of Transactions',  fontsize=13)\n\nplt.show()","b4875a66":"# converting seconds to time delta to extract hours and mins\n\ntimedelta = pd.to_timedelta(data['Time'], unit='s')\n\ndata['mins'] = (timedelta.dt.components.minutes).astype(int)\ndata['hours'] = (timedelta.dt.components.hours).astype(int)","67232594":"# Countplots for hours vs count of transactions\n\nfig, axs = plt.subplots(3, figsize=(12,20))\n\nfig.subplots_adjust(hspace=.5)\n\nsns.countplot(data['hours'], ax = axs[0], palette=\"Pastel1\")\naxs[0].set_title(\"Distribution of Total Transactions\",fontsize=20)\naxs[0].set_facecolor(\"black\")\n\nsns.countplot(data[(data['Class'] == 1)]['hours'], ax=axs[1], palette='Pastel2')\naxs[1].set_title(\"Distribution of Fraudulent Transactions\", fontsize=20)\naxs[1].set_facecolor('black')\n\nsns.countplot(data[(data['Class'] == 0)]['hours'], ax=axs[2], palette='Set3')\naxs[2].set_title(\"Distribution of Normal Transactions\", fontsize=20)\naxs[2].set_facecolor(\"black\")\n\nplt.show()","09ad9d6c":"# Exploring the distribuition by Class types throught hours and minutes\n\nplt.figure(figsize=(20,8))\n\nsns.distplot(data[data['Class'] == 0]['hours'], bins=24, color='m')\nsns.distplot(data[data['Class'] == 1][\"hours\"], bins=24, color='r')\n\nplt.title('Fraudulent and Normal Transactions by Hours', fontsize=20)\n\nplt.xlabel(\"Time in Hours\", fontsize=13)\nplt.xlim([-1,25])\nplt.show()","36941a9d":"#Exploring the distribuition by Class types throught hours and minutes\n\nplt.figure(figsize=(20,8))\n\nsns.distplot(data[data['Class'] == 0][\"mins\"], bins =60, color='deeppink')\nsns.distplot(data[data['Class'] == 1][\"mins\"], bins =60, color='coral')\n\nplt.title('Fraud and Normal Transactions by minutes', fontsize=20)\nplt.xlim([-1,61])\nplt.xlabel(\"Time in minutes\", fontsize=12)\nplt.show()","02bc3a1d":"# Scatter plot of Class vs Amount and Time for Normal Transactions \n\nplt.figure(figsize=(20,8))\n\nfig = plt.scatter(x=data[data['Class'] == 0]['Time'], y=data[data['Class'] == 0]['Amount'], color=\"dodgerblue\", s=50, edgecolor='black')\nplt.title(\"Time vs Transaction Amount in Normal Transactions\", fontsize=20)\nplt.xlabel(\"Time (in seconds)\", fontsize=13)\nplt.ylabel(\"Amount of Transaction\", fontsize=13)\n\nplt.show()","144ca36d":"# Scatter plot of Class vs Amount and Time for Fraudulent Transactions \n\nplt.figure(figsize=(20,8))\n\nfig = plt.scatter(x=data[data['Class'] == 1]['Time'], y=data[data['Class'] == 1]['Amount'], color=\"c\", s=80)\nplt.title(\"Time vs Transaction Amount in Fraud Cases\", fontsize=20)\nplt.xlabel(\"Time (in seconds)\", fontsize=13)\nplt.ylabel(\"Amount of Transaction\", fontsize=13)\n\nplt.show()","e6539414":"#Looking the V's features\ncolumns = data.iloc[:,1:29].columns\n\nfrauds = data.Class == 1\nnormals = data.Class == 0\n\ngrid = gridspec.GridSpec(14, 2)\nplt.figure(figsize=(20,20*4))\n\nfor n, col in enumerate(data[columns]):\n    ax = plt.subplot(grid[n])\n    sns.distplot(data[col][frauds], color='deeppink', kde_kws={\"color\": \"k\", \"lw\": 1.5},  hist_kws=dict(alpha=1)) \n    sns.distplot(data[col][normals],color='darkturquoise', kde_kws={\"color\": \"k\", \"lw\": 1.5},  hist_kws=dict(alpha=1))\n    ax.set_ylabel('Density', fontsize=13)\n    ax.set_title(str(col), fontsize=20)\n    ax.set_xlabel('')\nplt.show()","354883ab":"# copy of data for future use\n\ntemp = data.copy()","b745eb36":"# Finding the 3rd and 1st Quantile for Amount Column\n\nQ3 = np.percentile(data['Amount'], 75)\nQ1 = np.percentile(data['Amount'], 25)\n\n# setting the cutoff\ncutoff = 5.0\n\n# computing the interquartile range\nIQR = (Q3 - Q1)\n\n# computing lower bound and upper bound\nlower_bound = Q1 - (IQR * cutoff)\nupper_bound = Q3 + (IQR * cutoff)\n\n# creating a filter to remove values less than lower bound and greater than\n# upper bound\nfilter_data = (data['Amount'] < lower_bound) | (data['Amount'] > upper_bound)\n\n# filtering data\noutliers = data[filter_data]['Amount']\nfraud_outliers = data[(data['Class'] == 1) & filter_data]['Amount']\nnormal_outliers = data[(data['Class'] == 0) & filter_data]['Amount']\n\nprint(f\"Total Number of Outliers : {outliers.count()}\")\nprint(f\"Number of Outliers in Fraudulent Class : {fraud_outliers.count()}\")\nprint(f\"No of Outliers in Normal Class : {normal_outliers.count()}\")\nprint(f\"Percentage of Fraud amount outliers : {round((fraud_outliers.count()\/outliers.count())*100,2)}%\")","aa914c4b":"data = data.drop(outliers.index)\ndata.reset_index(inplace=True, drop=True)","89081ce0":"data.head().style.set_properties(**{\"background-color\":\"black\",\n                           \"color\" : \"skyblue\"})","b13ae64e":"data.shape","7ea227a7":"# applying log transformation of Amount column\n\ndata['Amount'] = np.log(data['Amount'] + 0.001)","1081eb1e":"# Box Plot for transformed Amount feature with class\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x =\"Class\", y=\"Amount\", data=data, palette='Set2');\nplt.xlabel(\"Amount\", fontsize=13)\nplt.ylabel(\"Class\", fontsize=13)\nplt.title(\"Box Plot for Transformed Amount Feature\", fontsize=20);","c9e6fb66":"# scaling the time column\n\nrobust_scaler = RobustScaler()\ndata['Time'] = robust_scaler.fit_transform(data['Time'].values.reshape(-1,1))","ec81f530":"X = data.drop(['Class','hours','mins'], 1)\nY = data.Class","887f76c4":"print(f'Original dataset shape : {Counter(Y)}')\n\nsmote = SMOTE(random_state=42)\nX_res, y_res = smote.fit_resample(X, Y)\n\nprint(f'Resampled dataset shape {Counter(y_res)}')","075dec79":"# creating a random sample of 5000 points \n\nX_vis = X_res.sample(5000, random_state=42)\ny_vis = y_res.sample(5000, random_state=42)\n\nprint(X_vis.shape)\nprint(y_vis.shape)","ee316915":"# training the t-SNE model to reduce dimensionality\n# to 3\n\ntsne3d = TSNE(\n    n_components=3,\n    random_state=42,\n    verbose=2,\n).fit_transform(X_vis)","030a4701":"# plotting a 3D scatter plot \n\ntrace1 = go.Scatter3d(\n    x=tsne3d[:,0],\n    y=tsne3d[:,1],\n    z=tsne3d[:,2],\n    mode='markers',\n    marker=dict(\n        color = y_vis,\n        colorscale = ['deeppink', 'deepskyblue'],\n        colorbar = dict(title = 'Fraud'),\n        line=dict(color='rgb(255, 255, 255)'),\n        opacity=0.5\n    )\n)\n\ndata=[trace1]\n\nlayout = dict(title = 'TSNE (T-Distributed Stochastic Neighbourhood Embedding)',\n              showlegend= False, height=800, width=800,)\n\nfig=dict(data=data, layout=layout)\npy.iplot(fig, filename='3DBubble')","a9e9b2a5":"# creating instance of statrifiedkfold split for 5 splits \nstrat = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\n# splitting the data\nfor train_index, test_index in strat.split(X, Y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = Y.iloc[train_index], Y.iloc[test_index]","5e4aed1a":"# Turning the splits into an array\n\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values","d89ae2ba":"# Creating a function to plot correlation matrix and roc_auc_curve\n\ndef show_metrics(model, y_test, y_pred):\n    fig = plt.figure(figsize=(20, 8))\n\n    # Confusion matrix\n    ax = fig.add_subplot(121)\n    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, annot_kws={\"size\": 16}, fmt='g', \n                cmap='Set3', linewidths=1, linecolor='white')\n\n    \n    # labels, title and ticks\n    ax.set_xlabel('Predicted labels', fontsize=15);\n    ax.set_ylabel('True labels', fontsize=15); \n    ax.set_title(\"Confusion Matix\", fontsize=20) \n    ax.xaxis.set_ticklabels(['No Fraud', 'Fraud'], fontsize=12); \n    ax.yaxis.set_ticklabels(['Fraud', 'No Fraud'], fontsize=12);\n\n    # ROC Curve\n    fig.add_subplot(122)\n    \n    \n    auc_roc = roc_auc_score(y_test, model.predict(original_Xtest))\n    fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(original_Xtest)[:,1])\n\n    plt.plot(fpr, tpr, color='darkturquoise', lw=2, marker='o', label='Trained Model (area = {0:0.3f})'.format(auc_roc))\n    plt.plot([0, 1], [0, 1], color='deeppink', lw=2, linestyle='--', label= 'No Skill (area = 0.500)')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate', fontsize=13)\n    plt.ylabel('True Positive Rate', fontsize=13)\n    plt.title('Receiver operating characteristic', fontsize=20)\n    plt.legend(loc=\"lower right\")\n    plt.show()","1d3e886e":"# List to append the score and then find the average\naccuracy_lst = []\nprecision_lst = []\nrecall_lst = []\nf1_lst = []\nauc_lst = []\n\n# specifying the parameter grid for logistic regression\nlog_reg_params = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\n# Applying RandomsearchCV to find best model\nrand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)\n\n\n# iterating over all the splits\nfor train, test in strat.split(original_Xtrain, original_ytrain):\n    \n    # create pipeline with smote and the model \n    # sampling_strategy = minority because we want to only resample the minority class\n    pipeline = make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg)\n    \n    # fit the pipeline\n    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n    best_est = rand_log_reg.best_estimator_\n    prediction = best_est.predict(original_Xtrain[test])\n    \n    # finding mean for all the necessary measures to evaluate performance\n    accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    precision_lst.append(precision_score(original_ytrain[test], prediction))\n    recall_lst.append(recall_score(original_ytrain[test], prediction))\n    f1_lst.append(f1_score(original_ytrain[test], prediction))\n    auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n\nprint(\"Accuracy: {0:0.2f}%\".format(np.mean(accuracy_lst)*100))\nprint(\"Precision: {0:0.2f}\".format(np.mean(precision_lst)))\nprint(\"Recall: {0:0.2f}\".format(np.mean(recall_lst)))\nprint(\"f1 Score: {0:0.2f}\".format(np.mean(f1_lst)))","4337ebba":"y_pred = best_est.predict(original_Xtest)","1a74da26":"show_metrics(best_est, original_ytest, y_pred)","a43bd649":"# Random forest Classifier\nrf_cfl = RandomForestClassifier(n_estimators = 200, \n                                 max_features = 3, \n                                 min_samples_leaf = 1, \n                                 min_samples_split = 2, \n                                 n_jobs = -1,\n                                random_state = 42)\n\nrf_cfl.fit(original_Xtrain, original_ytrain)\ny_pred = rf_cfl.predict(original_Xtest)","043f88ed":"show_metrics(rf_cfl, original_ytest, y_pred)","794519ca":"print('Accuracy :{0:0.5f}'.format(accuracy_score(y_pred , original_ytest))) \nprint('AUC : {0:0.5f}'.format(roc_auc_score(original_ytest , y_pred)))\nprint('Precision : {0:0.5f}'.format(precision_score(original_ytest , y_pred)))\nprint('Recall : {0:0.5f}'.format(recall_score(original_ytest , y_pred)))\nprint('F1 : {0:0.5f}'.format(f1_score(original_ytest , y_pred)))","cd9e0c9e":"# Uncomment and run to find the best model using ExtraTrees (Takes a lot of time!)\n\n\n\n# accuracy_lst = []\n# precision_lst = []\n# recall_lst = []\n# f1_lst = []\n# auc_lst = []\n\n# # specifying the parameter grid for extra trees\n# extra_trees_params = {\"n_estimators\": [30,50,70,100], 'max_depth': [30,50,70,100], 'min_samples_split':[1,2,3,4,5]}\n\n# # Applying RandomsearchCV to find best model\n# rand_extra_trees = RandomizedSearchCV(ExtraTreesClassifier(random_state=42), extra_trees_params, n_iter=4)\n\n# # iterating over all the splits\n# for train, test in strat.split(original_Xtrain, original_ytrain):\n    \n#     # creating pipeline\n#     pipeline = make_pipeline(SMOTE(sampling_strategy='minority'), rand_extra_trees)\n#     model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n#     best_est = rand_extra_trees.best_estimator_\n#     prediction = best_est.predict(original_Xtrain[test])\n    \n#     # computing performance measures\n#     accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n#     precision_lst.append(precision_score(original_ytrain[test], prediction))\n#     recall_lst.append(recall_score(original_ytrain[test], prediction))\n#     f1_lst.append(f1_score(original_ytrain[test], prediction))\n#     auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n\n# print(\"Accuracy: {0:0.2f}%\".format(np.mean(accuracy_lst)*100))\n# print(\"Precision: {0:0.2f}\".format(np.mean(precision_lst)))\n# print(\"Recall: {0:0.2f}\".format(np.mean(recall_lst)))\n# print(\"f1 Score: {0:2f}\".format(np.mean(f1_lst)))","69a7f369":"# training using the best model\n\nbest_model = ExtraTreesClassifier(max_depth=50, n_estimators=70, min_samples_split=5, random_state=42)","b30801c7":"fitted_model = best_model.fit(original_Xtrain, original_ytrain)\n\npredictions = fitted_model.predict(original_Xtest)","765caff4":"show_metrics(fitted_model, original_ytest, predictions)","945dda42":"# utility function to compute accuracy of normal transactions\ndef normal_accuracy(values):\n    \n    tp=list(values).count(1)\n    total=values.shape[0]\n    accuracy=np.round(tp\/total,4)\n    \n    return accuracy\n\n# utility function to compute accuracy of fraud transactions\ndef fraud_accuracy(values):\n    \n    tn=list(values).count(-1)\n    total=values.shape[0]\n    accuracy=np.round(tn\/total,4)\n    \n    return accuracy","89990bfe":"# create inliers and outliers data\n\ninliers = temp[temp.Class==0]\nins = inliers.drop(['Class'], axis=1)\n\noutliers = temp[temp.Class==1]\nouts = outliers.drop(['Class'], axis=1)\n","73315373":"# training of isolation forest\n\nISF = IsolationForest(random_state=42)\nISF.fit(ins)\n\nnormal_isf = ISF.predict(ins)\nfraud_isf = ISF.predict(outs)\n\nin_accuracy_isf=normal_accuracy(normal_isf)\nout_accuracy_isf=fraud_accuracy(fraud_isf)\nprint(f\"Accuracy in Detecting Normal Cases: {in_accuracy_isf*100}%\")\nprint(f\"Accuracy in Detecting Fraud Cases: {round(out_accuracy_isf*100, 2)}%\")","f1e5a501":"# dropping useless columns\n\nnew_data = temp.drop(['Time', 'Amount', 'V27', 'V28', 'V25', 'V23', 'V7', 'V13', 'V20','V22','mins','hours'], 1)","3b2fab79":"# create inliers and outliers data\n\ninliers = new_data[new_data.Class==0]\nins = inliers.drop(['Class'], axis=1)\n\noutliers = new_data[new_data.Class==1]\nouts = outliers.drop(['Class'], axis=1)","89594efc":"# training of isolation forest on new data\n\nISF = IsolationForest(random_state=42)\nISF.fit(ins)\n\nnormal_isf = ISF.predict(ins)\nfraud_isf = ISF.predict(outs)\n\nin_accuracy_isf=normal_accuracy(normal_isf)\nout_accuracy_isf=fraud_accuracy(fraud_isf)\nprint(f\"Accuracy in Detecting Normal Cases: {in_accuracy_isf*100}%\")\nprint(f\"Accuracy in Detecting Fraud Cases: {out_accuracy_isf*100}%\")","eea6fae4":"> &#9658; **Precision  and accuracy are good but the recall is pretty low due the which the model is not performing well on fraudulent data.**","49780acd":"<h1><font color=red> Modelling <\/font><\/h1>\n\n&#9658; **In this section, we will finally apply models and classify whether a certain transaction done a particular time is fraud or geniune. Thus, this is a binary classification problem.**\n\n&#9658; **Important thing to note here is that we did SMOTE but we won't use that data, Why?**\n\n**If we used that data to predict the classes then it will result in a problem know as 'Data Leakage' which is another term for using test data for prediction or cross validation. So, this sounds like a good point to use Pipelines. Pipelines make our life easier by specifying what order should the operations be done on the data.**\n\n&#9658; **One thing we should keep in mind that we might get very high accuracy but we should focus on optimising out f1_score and recall as we want to perform better on fraud cases as they are the most important.**","f4728345":"<h1><font color=red>Data Cleaning and Preprocessing<\/font><\/h1>","0c5d169a":"<h2><font color=blue> Handling Class Imbalance <\/font><\/h2>\n\n&#9658; **Imbalanced data is a problem in supervised learning problems which can result is high bias towards majority class. As we have already seen that this data is severly imbalanced so to balance it we can use various techniques such as:** \n\n- **Oversampling**\n- **Undersampling** \n- **SMOTE**\n\n&#9658; **Out of all these three SMOTE is the most effective so we will go with it, In this technique, instead of simply duplicating data from the minority class, we synthesize new data from the minority class. This is a type of data augmentation for tabular data can be very effective. This approach to synthesizing new data is called the Synthetic Minority Oversampling TEchnique, or SMOTE for short.**","bf7471f1":"&#9658; Best Model : ExtraTreesClassifier(max_depth=50, n_estimators=70, min_samples_split=5, random_state=42)\n\n&#9658; Results: \n\n> **Accuracy: 99.92%**<br>\n> **Precision: 0.79**<br>\n> **Recall: 0.80**<br>\n> **f1 Score: 0.78**<br>","e7ce9733":"<h2><font color=blue> Isolation Forest<\/font><\/h2>\n\n&#9658; **Isolation Forest is an unsupervised anomaly detection algorithm that uses the two properties \u201cFew\u201d and \u201cDifferent\u201d of anomalies to detect their existence.**<br>\n\n&#9658; **Since anomalies are few and different, they are more susceptible to isolation. This algorithm isolates each point in the data and splits them into outliers or inliers. This split depends on how long it takes to separate the points. If we try to separate a point which is obviously a non-outlier, it\u2019ll have many points in its round, so that it will be really difficult to isolate. On the other hand, if the point is an outlier, it\u2019ll be alone and we\u2019ll find it very easily.**<br>\n","ba5d086a":"<h1><font color=red>Correlation of Features with Response<\/font><\/h1>\n\n**Let's plot the correlation of our response i.e Class with all the other features in the data.**","31c89a4d":"### <h3><font color=red>OBSERVATIONS<\/font><\/h3>\n\n&#9658; **Due to confidentiality issue, original features V1, V2,... V28 have been transformed using PCA, however, my guess is that these features might be credit card number, expiry date, CVV, cardholder name, transaction location, transaction date-time, etc.**\n\n&#9658; **Only features which have not been transformed with PCA are 'Time',&nbsp; 'Amount' and 'Class'.**<br>\n\n> &#9678; **'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset.**<br><br>\n> &#9678; **The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning.**<br><br>\n> &#9678; **The Feature 'Class' is the response or target variable and it takes value 1 in case of fraud and 0 otherwise.**<br>","42a17cea":"> &#9658; **Logistic regression did really well on the fraud class giving almost 95% accuracy and algo does well on the fraud class, predicting 78 fraud cases are correct but 12 cases as wrong so we still have a room for improvement. It we applied model to the imbalanced data instead then it results would be pathetic. You can try that for yourself.**","b04a0c50":"<h3 align=\"center\"><font color=red>OBSERVATIONS<\/font><\/h3>\n\n#### <font color=blue>FRAUDULENT<\/font>\n\n>&#9658; **There are two peaks between 40000 seconds and 100000 seconds which were the maximum number of fraudulent transaction at any time.**\n\n\n#### <font color=blue>NORMAL<\/font>\n\n>&#9658; **Normal transactions have not much to uncover except the fact that there were less transactions somewhere around 20000 seconds and 100000 seconds which is not very useful.**","8a942dde":"<h2><font color=blue> Random Forest Classifier <\/font><\/h2>","7e8efa9b":">&#9658; **We have successfully removed the outliers now and we let's scale the Amount feature so that our model isn't baised towards amount feature.**","825db9c7":"&#9658; **Let's do some feature engineering on time and transform it to minutes and hours to uncover some of the hidden patterns.**","69e81382":"<h3><font color=red>OBSERVATIONS<\/font><\/h3>\n\n&#9658; **Mean transaction is somewhere is 88 and standard deviation is around 250.**<br><br>\n&#9658; **The median is 22 which is very less as compared to mean which signifies that there are outliers or our data is highly positive skewed which is effecting the amount and thus the mean.**<br><br>\n&#9658; **The maximum transaction that was done is of 25,691 and minimum is 0.**\n","3365718d":">&#9658; **Now let's scale the time column but we will use normal scaling this time.**","93db7ebd":"> &#9658; **Using Extra Tree classifier we got 99.9% accuracy! But wait, let's look at the recall - only 80% which is even lower than our Logistic regression model. So, extra trees is performing bad on fraud data so let's try other models.**","9d812e85":">&#9658; **Let's look at the distribution of number of transactions for each hour for total, fraudulent and normal transactions.**","89e9df7e":"<h2><font color=blue> Extra Trees Classifier <\/font><\/h2>\n\n&#9658; **Extra Trees Classifier is a ensemble model which is a randomised version of Random Forest and makes learning faster. So, let's apply this model and see how well it can perform.**","b288cc60":"<h2><font color=blue> Dimensionality Reduction and Clustering <\/font><\/h2>\n\n&#9658; **The dimesionality reduction I will use is t-SNE. It is state-of-the-art in visualising high dimensional data and can help us understand how our model will perform or how seperable the data is using clusters.**","6d6e09f7":"&#9658; **This looks much better! We are doing pretty well on the fraud cases but we are doing little less on normal transactions   but that is okay as ~95% is still good.**","b399d540":"> &#9658; **As expected, t-SNE is performing well, seperating both the classes. The clusters are not perfectly seperated but it is able to seperate almost 80% of the sample data which is cool.**","e5ddef45":"<h1><font color=red>Exploratory Data Analysis<\/font><\/h1>","0a40f8bf":"<h2><font color=blue>Feature Scaling<\/font><\/h2>\n\n&#9658; **As the amount column is highly skewed so it will be better to apply log transoformation as it can result in nearly normal distribution which is suited for most of the algorithms.**","bc206188":"&#9658; **V17, V14, V12 and V10 are negatively correlated. Notice how the lower these values are, the more likely the end result will be a fraud transaction.**<br>\n\n\n&#9658; **V2, V4, V11, and V19 are positively correlated. Notice how the higher these values are, the more likely the end result will be a fraud transaction.**\n\n\n&#9658; **Sadly, features that show correlation with response variable are not explainatory so we don't get much insights from this plot. But, we still have a lot to analyse so let's dive straight in!**","293711be":"<h3><font color=blue>Class and Amount vs Time<\/font><\/h3>","8674cd0f":"&#9658; **Note that we will take the fresh data for performing anomaly detection. Also, we won't do outlier detection either, as out algorithm would already be doing that!**","085ab843":"<h2><font color=blue> Logistic Regression <\/font><\/h2>","d6c78640":"&#9658; **Isolation forest successfully classified ~85% of the fraudulent transactions correction and it gives 96% on normal cases too which is pretty good.**\n\n&#9658; **Let's us give a final try by dropping some of the columns which we found useless during EDA. According to graphs, the correlation between amound and time is not very significant. Also, we saw some of the V's which were overlapping like crazy so they are definately are of no help to use so we will drop them and try our Isolation forest again on the new data.**","107bf2bb":"<h3><font color=red>OBSERVATIONS<\/font><\/h3>\n\n&#9658; **There are 113 fraud transactions for just one dollar and 27 fraud transaction for 99.99 dollars. Also, there are 27 fraud transaction for zero amount.<br>**\n\n&#9658; **The reason for zero transaction can be the zero Authorization which is an account verification method for credit cards that is used to verify a cardholders information without charging the consumer.**","a12e16cf":">&#9658; **We can clearly see that now the data is completely balanced so let's use some visualisation technique to visualize this data.**\n\n>&#9658; **Note that, as our data has a lot of columns and humans can only understand 3D so we will use Dimensionality reduction technique to reduce our data to 3D and then plot it. So, let's get started!**","5656b76e":"<h3 align=\"center\"><font color=red>OBSERVATIONS<\/font><\/h3>\n\n#### <font color=blue>FRAUDULENT<\/font>\n\n>&#9658; **Mean transaction is around 122 and standard deviation is around 256.**<br><br>\n>&#9658; **Maximum Transaction was 2125 and minimum was 0.**<br>\n\n#### <font color=blue>NORMAL<\/font>\n\n> &#9658; **Mean transaction is around 88 and standard deviation is around 250.**<br><br>\n> &#9658; **Maximum Transaction was 25691 and minimum was 0.**<br>","73922bd6":"<h2><font color=blue>Outlier Removal<\/font><\/h2>\n\n&#9658; **As we already saw that amount column has a extreme outliers so it necessary to remove them as they can effect the model's performance. We will used Interquartile range to detect outliers which removes anything below the lower limit (25 percentile) and anything above upper limit (75 Percentile).**\n\n&#9658; **Note that, the data we have for fraudulent cases is very low so we wanna keep our cutoff a bit high so as avoid removing much of the fraud cases. Here, as the data is skewed (kind of exponential) so having high cutoff will help us. Let's take the cutoff value as 5.0 instead of 1.5 which is usually used.**","b7eb0fdc":"<h3><font color=red>Descriptive Statistics<\/font><\/h3>\n\n&#9658; **As most of the columns V1, V2,... V28 are transformed using PCA so neither features make much sense and nor will the descriptive statistics so we will leave them and consider only Time and Amount which makes sense.**","1685d22b":"<h3><font color=red>OBSERVATIONS<\/font><\/h3>\n\n&#9658; **The plots of both hours and minutes doesn't have any interesting trend.**","b633c4dc":"<h2><font color=blue> Splitting the data <\/font><\/h2>\n\n&#9658; **As our data is imbalanced so we will not use train_test_split and instead we will use stratified split which will take the representative of respective populations i.e Fraud Transactions and Normal Transactions.**","d2fc0cdb":"<h3 align=\"center\"><font color=red>OBSERVATIONS<\/font><\/h3>\n\n#### <font color=blue>FRAUDULENT<\/font>\n\n>&#9658; **There are much more outliers as compared to normal transactions.**<br><br>\n>&#9658; **The plot seems to not have any inherent pattern.**<br><br>\n\n#### <font color=blue>NORMAL<\/font>\n\n> &#9658; **There are a less number of outliers as compared to fraudulent transactions.**<br><br>\n> &#9658; **There are a lot of transactions with amount less than 5000.**<br>","ff88d5ab":"<h2><font color=red>Analysis of Time Column<\/font><\/h2>","7b1b8aef":"&#9658; **We can clearly see that we don't have any null values in the data which is bound to happen in such datasets as each and every information is very necessary else the transaction isn't processed.**","bb5b858f":"<h3><font color=blue>Class vs Time<\/font><\/h3>\n\n**Let's plot the distribution of the classes with respect to time for both fraudulent and normal transactions.**","95d0effa":"<h3><font color=red>OBSERVATIONS<\/font><\/h3>\n\n&#9658; **For some of the features we can observe a good selectivity in terms of distribution for the two values of Class: V4, V11 have clearly separated distributions for Class values 0 and 1, V12, V14, V18 are partially separated, V1, V2, V3, V10 have a quite distinct profile, whilst V20-V28 have similar profiles for the two values of Class and thus not very useful in differentiation of both the classes.**\n\n&#9658; **In general, with just few exceptions (Time and Amount), the features distribution for legitimate transactions (values of Class = 0) is centered around 0, sometime with a long queue at one of the extremities. In the same time, the fraudulent transactions (values of Class = 1) have a skewed (asymmetric) distribution.**","f61940c6":"Content\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders.\nThis dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise","8ce37d4d":"> **Now, Let's try descriptive statistics filtering by fraudulent and non-fraudulent transactions.**","45c01499":"<h3><font color=red>OBSERVATIONS<\/font><\/h3>\n \n&#9658; **This dataset has 492 frauds out of 284,315 transactions. Thus, the dataset is highly unbalanced, the positive class (frauds) account for 0.173% of all transactions.**\n\n&#9658; **Most of the transactions are non-fraud which is obvious. If we use this data for our predictive models and analysis, our algorithms will probably overfit to the non-fraudulent transactions and will answer in non-fraudulent all the time which can result in actual frauds to slip by!**\n\n&#9658; **Note that our task is not to find the obvious, rather we have to find the anomalies and signs of fraud! Thus, we will take care of this imbalance during preprocessing.**"}}