{"cell_type":{"a8368a51":"code","c7cb07a8":"code","feea3578":"code","76d89228":"code","44d24bd8":"code","029904b5":"code","baf892a1":"code","f2763400":"markdown","0eb12e67":"markdown","142d4650":"markdown","ef0f91c4":"markdown","1f910965":"markdown","23492796":"markdown"},"source":{"a8368a51":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cv2\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D # <--- This is important for 3d plotting \nimport os\nimport cv2\nimport ntpath\nfrom sklearn.linear_model import LogisticRegression\n#from sklearn import svm, datasets\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom scipy import interp\nfrom sklearn import svm\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom scipy import interp\nfrom itertools import cycle\nfrom sklearn.decomposition import PCA\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","c7cb07a8":"files=[] #store the filenames here\nlabels1=[] #store the labels here\ndirname='\/kaggle\/input'\nfor dirname, _, filenames in os.walk(dirname):\n    for filename in filenames:\n        if filename.endswith('.png'):\n            files.append(os.path.join(dirname, filename))            \n            if filename.startswith('NL'):                \n                labels1.append('NL')     #Normal retina      \n                \n            elif filename.startswith('ca'):  #Cataract              \n                labels1.append('ca')\n            elif filename.startswith('Gl'):  #Glaucoma              \n                labels1.append('Gl')\n            elif filename.startswith('Re'):  #Retina Disease              \n                labels1.append('Re')\n            \n\n\n# Shuffle the data\ncombined = list(zip(files,labels1)) # combine the lists\nnp.random.shuffle(combined) # shuffle two lists together to keep order\nfiles[:],labels1[:] = zip(*combined) #unzip the shuffled lists","feea3578":"########################################################################\n#########The following function normalizes the image histogram##########\n#######################################################################\n\ndef normalize_histograms(im): #normalizes the histogram of images\n    im1=im.copy()\n    for i in range(3):\n        imi=im[:,:,i]\n        #print(imi.shape)\n        minval=np.min(imi)\n        maxval=np.max(imi)\n        #print(minval,maxval)\n        imrange=maxval-minval\n        im1[:,:,i]=(255\/(imrange+0.0001)*(imi-minval)) # imi-minval will turn the color range between 0-imrange, and the scaleing will stretch the range between 0-255\n    return im1\n\n######################################################################\n# This following function reads the images from file,\n#auto crops the image to its relevant content, then normalizes\n#the histograms of the cropped images\n######################################################################\n\ndef read_and_process_image(filename):\n        im=cv2.imread(filename) #read image from file \n        # The following steps re needed for auto cropping the black paddings in the images\n        \n        gray = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY) # convert 2 grayscale\n        _,thresh = cv2.threshold(gray,10,255,cv2.THRESH_BINARY) # turn it into a binary image\n        contours,hierarchy = cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE) # find contours\n        if len(contours) != 0:\n            #find the biggest area\n            cnt = max(contours, key = cv2.contourArea)\n                      \n            #find the bounding rect\n            x,y,w,h = cv2.boundingRect(cnt)                  \n\n            crop = im[y:y+h,x:x+w]# crop image\n            #crop1=cv2.resize(crop,(im_size,im_size)) # resize to im_size X im_size size\n            crop=normalize_histograms(crop)\n            return crop\n        else:\n            return( normalize_histograms(im))\n\n\n##################################################################################\n#### The following functions are for extracting features from the images #########\n##################################################################################\n        \n# histogram statistics (mean, standard deviations, energy, entropy, log-kurtosis)\n\n\ndef histogram_statistics(hist):\n    #hist= cv2.calcHist([gr],[0],None,[256],[0,256])\n    hist=hist\/np.sum(hist)#probabilities\n    hist=hist.reshape(-1)\n    hist[hist==0]=10**-20 # replace zeros with a small number\n    mn=np.sum([i*hist[i] for i in range(len(hist))]) # mean\n    std_dev=np.sqrt(np.sum([((i-mn)**2)*hist[i] for i in range(len(hist))])) # standard deviation\n    energy=np.sum([hist[i]**2 for i in range(len(hist))]) #energy\n    entropy=np.sum([hist[i]*np.log(hist[i]) for i in range(len(hist))]) #entropy\n    kurtosis=np.log(np.sum([(std_dev**-4)*((i-mn)**-4)*hist[i] for i in range(len(hist))])) # kurtosis\n    return[mn,std_dev,energy,entropy,kurtosis]\n\n#################################################################\n# create thresholding based features, the idea is to hand engineer some features based on adaptive thresholding.\n#After looking at the images it appeared  that adaptive thresholding may\n#leave different artifacts in the processed images, we can extract several features from these artifacts            \n##################################################################\n\ndef thresholding_based_features(im,imsize,quartiles):\n    im=cv2.resize(im,(imsize,imsize))\n    gray=cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)\n    w=11 #window\n    t=5#threshold\n    counts=[]\n    th = cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,w,t) # adaptive gaussian threshold the image\n    th=cv2.bitwise_not(th)    #invert the image (the black pixels will turn white and the white pixels will turn black)\n    contours,hierarchy = cv2.findContours(th,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE) #find cntours in the image\n    #print(len(contours))\n    \n    q=np.zeros(len(quartiles)) # quartiles of contours will be stored here\n    \n\n    for cnt in contours:\n        area=cv2.contourArea(cnt) # calculate the area of the contours\n        if area<40000: #Exclude contours that are too big, generally these are the image outlines\n            counts.append(area) \n    if len(counts)>1:\n        q=np.quantile(np.array(counts),quartiles) # contour quartiles\n    \n    return (q,len(counts),np.sum(th)\/(255*th.shape[0]*th.shape[1]))# return the contour quartiles, number of contours, proportion of white pixels in the thresholded images\n    #counts.append(np.sum(th)\/(normalizing_factor*(th.shape[0]*th.shape[1])))\n\n##########################################################################\n############ The following code creates the various features #############\n##########################################################################\n    \n# color averages\nB=[] \nG=[]\nR=[]\n\n#mini 16 bin histograms\nhist_B=[]\nhist_G=[]\nhist_R=[]\n\n#statistics fom full 256 bin shitogram\nhist_feat_B=[]\nhist_feat_G=[]\nhist_feat_R=[]\nhist_feat_GS=[]\n\n#thresholding based features\nmean_pixels=[] #proportion of white pixels\ncontour_quartiles=[] # contour area quartiles\nno_of_contours=[] #total number of contours\n\n\nquartiles=np.arange(0.1,1,0.1) # contour area quartiles\nbins=16 # mini histogram bins\n\nfor f in files:\n    im=read_and_process_image(f)\n    #im_yuv = cv2.cvtColor(im, cv2.COLOR_BGR2YUV)\n\n    # equalize the histogram of the Y channel\n    #im_yuv[:,:,0] = cv2.equalizeHist(im_yuv[:,:,0])\n\n    # convert the YUV image back to RGB format\n    #im = cv2.cvtColor(im_yuv, cv2.COLOR_YUV2BGR)\n    \n    #median color\n    B.append(np.median(im[:,:,0]))\n    G.append(np.median(im[:,:,1]))\n    R.append(np.median(im[:,:,2]))\n    \n    #histograms\n    hist_B.append(cv2.calcHist([im],[0],None,[bins],[0,256])\/(im.size\/3))\n    hist_G.append(cv2.calcHist([im],[1],None,[bins],[0,256])\/(im.size\/3))\n    hist_R.append(cv2.calcHist([im],[2],None,[bins],[0,256])\/(im.size\/3))\n    \n    \n    #more histogram features\n    \n    hist_feat_B.append(histogram_statistics(cv2.calcHist([im],[0],None,[256],[0,256])))\n    hist_feat_G.append(histogram_statistics(cv2.calcHist([im],[1],None,[256],[0,256])))\n    hist_feat_R.append(histogram_statistics(cv2.calcHist([im],[2],None,[256],[0,256])))\n    \n    gr=cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)\n    gr=cv2.equalizeHist(gr)\n    hist_feat_GS.append(histogram_statistics(cv2.calcHist([gr],[0],None,[256],[0,256])))\n    \n    #threshold featues\n    q,nc,m=thresholding_based_features(im,256,quartiles)\n    mean_pixels.append(m)\n    contour_quartiles.append(q)\n    no_of_contours.append(nc)\n\n#create feature vectors\nwidth_of_features=3*bins+len(quartiles)+2+20 #20 features are histogram statistics\n\nX=np.zeros((len(files),width_of_features)) # this is where all features will be stored\n\nfor i in range(len(files)):\n    X[i,0:bins]=hist_B[i].reshape(-1)\n    X[i,bins:2*bins]=hist_G[i].reshape(-1)\n    X[i,2*bins:3*bins]=hist_R[i].reshape(-1)\n    X[i,3*bins:3*bins+len(quartiles)]=contour_quartiles[i].reshape(-1)\n    X[i,3*bins+len(quartiles)]=mean_pixels[i]\n    X[i,3*bins+len(quartiles)+1]=no_of_contours[i]\n    start=3*bins+len(quartiles)+2\n    X[i,start:start+5]=hist_feat_B[i]\n    X[i,start+5:start+10]=hist_feat_G[i]\n    X[i,start+10:start+15]=hist_feat_R[i]\n    X[i,start+15:start+20]=hist_feat_B[i]\n\n    \n#######################################################################\n########### Divide the dataset into 70%train and 30% test data########\n######################################################################\n\n#train test devide (70:30)\nindex=int(len(labels1)*0.7)\n\nX_train=X[:index,:]\nY_train1=labels1[:index]\n\nX_test=X[index:,:]\nY_test1=labels1[index:]\n","76d89228":"#pca = PCA(n_components=30, svd_solver='arpack')\n#pca.fit(X_train)\n#X_train=pca.transform(X_train)\n#X_test=pca.transform(X_test)","44d24bd8":"\n#########################################################################\n################### train a multiclass logistic regression ##############\n#########################################################################\n\nlr = LogisticRegression(multi_class='multinomial', solver='lbfgs',C=1, penalty='l2', fit_intercept=True, max_iter=5000, random_state=42)\nlr.fit(X_train,Y_train1)\n\nY_testp_proba=lr.predict_proba(X_test)\nY_testp=lr.predict(X_test)\nY_trainp=lr.predict(X_train)\nY_trainp_proba=lr.predict_proba(X_train)\n#percent accuracy\ntest_accuracy=np.sum(np.array(Y_test1)==Y_testp)\/len(Y_testp)\ntrain_accuracy=np.sum(np.array(Y_train1)==Y_trainp)\/len(Y_trainp)\nprint('LR ',train_accuracy,test_accuracy)\n\n\n\n\n\n#########################################################################\n################### train a multiclass random forest ####################\n#########################################################################\n\n\nforest=RandomForestClassifier(n_estimators=100, max_depth=3,random_state=42)\nforest.fit(X_train,Y_train1)\n\nY_testp_forest=forest.predict(X_test)\nY_trainp_forest=forest.predict(X_train)\nY_testp_proba_forest=forest.predict_proba(X_test)\nY_trainp_proba_forest=forest.predict_proba(X_train)\n\ntest_accuracy=np.sum(np.array(Y_test1)==Y_testp_forest)\/len(Y_testp)\ntrain_accuracy=np.sum(np.array(Y_train1)==Y_trainp_forest)\/len(Y_trainp)\nprint('RF: ',train_accuracy,test_accuracy)\n\n#########################################################################\n################### train a multiclass gradient boosting classifier #####\n#########################################################################\n\n\ngb=GradientBoostingClassifier(learning_rate=0.01, n_estimators=100, subsample=1, max_depth=2,  random_state=42 )\ngb.fit(X_train,Y_train1)\n\nY_testp_gb=gb.predict(X_test)\nY_trainp_gb=gb.predict(X_train)\nY_testp_proba_gb=gb.predict_proba(X_test)\nY_trainp_proba_gb=gb.predict_proba(X_train)\n\ntest_accuracy=np.sum(np.array(Y_test1)==Y_testp_gb)\/len(Y_testp)\ntrain_accuracy=np.sum(np.array(Y_train1)==Y_trainp_gb)\/len(Y_trainp)\nprint('GB ',train_accuracy,test_accuracy)\n\n\n\n#########################################################################\n################### train a multiclass gSVM classifier ###################\n#########################################################################\n\n\nsvm1 = svm.SVC(gamma='scale',probability=True)\nsvm1.fit(X_train, Y_train1)\nY_testp_svm=svm1.predict(X_test)\nY_trainp_svm=svm1.predict(X_train)\nY_testp_proba_svm=svm1.predict_proba(X_test)\nY_trainp_proba_svm=svm1.predict_proba(X_train)\n\ntest_accuracy=np.sum(np.array(Y_test1)==Y_testp_svm)\/len(Y_testp)\ntrain_accuracy=np.sum(np.array(Y_train1)==Y_trainp_svm)\/len(Y_trainp)\nprint('SVM ',train_accuracy,test_accuracy)\n\n","029904b5":"#########################################################\n############### One hot encoder#########################\n##########################################################\n\ndef one_hot_encode(Y,classes=None):\n    if classes is None:\n        classes=np.unique(Y)\n    output=np.zeros((len(Y),len(classes)))\n    for i,c in enumerate(classes):\n        #print(i,c)\n        output[Y==c,i]=1\n        #print(np.sum(output))\n    return output\n\n############################################################################\n########### the following function plots multiclass ROCs###################\n############################################################################\n\n#multiclass classification micro vs macro rocs\ndef multiclass_roc(y_test,y_score,n_classes,lw,classes):\n    # Compute ROC curve and ROC area for each class\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n    \n    # Compute micro-average ROC curve and ROC area\n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n    \n    # First aggregate all false positive rates\n    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n    \n    # Then interpolate all ROC curves at this points\n    mean_tpr = np.zeros_like(all_fpr)\n    for i in range(n_classes):\n        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n    \n    # Finally average it and compute AUC\n    mean_tpr \/= n_classes\n    \n    fpr[\"macro\"] = all_fpr\n    tpr[\"macro\"] = mean_tpr\n    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n    \n    plt.figure()\n    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n             label='micro-average ROC curve (area = {0:0.2f})'\n                   ''.format(roc_auc[\"micro\"]),\n             color='deeppink', linestyle=':', linewidth=4)\n    \n    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n             label='macro-average ROC curve (area = {0:0.2f})'\n                   ''.format(roc_auc[\"macro\"]),\n             color='navy', linestyle=':', linewidth=4)\n    \n    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n    for i, color in zip(range(n_classes), colors):\n        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n                 label='ROC curve of class '+classes[i]+' (area = {1:0.2f})'\n                 ''.format(i, roc_auc[i]))\n    \n    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Extension of Receiver operating characteristic to multi-class')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n\n\n\n\n#Logistic Regression\nmulticlass_roc(one_hot_encode(np.array(Y_test1),lr.classes_),Y_testp_proba,4,2,lr.classes_)\nmulticlass_roc(one_hot_encode(np.array(Y_train1),lr.classes_),Y_trainp_proba,4,2,lr.classes_)\n\n#Random Forest\nmulticlass_roc(one_hot_encode(np.array(Y_test1),lr.classes_),Y_testp_proba_forest,4,2,forest.classes_)\nmulticlass_roc(one_hot_encode(np.array(Y_train1),lr.classes_),Y_trainp_proba_forest,4,2,forest.classes_)\n\n#Gradient Boosting\nmulticlass_roc(one_hot_encode(np.array(Y_test1),lr.classes_),Y_testp_proba_gb,4,2,gb.classes_)\nmulticlass_roc(one_hot_encode(np.array(Y_train1),lr.classes_),Y_trainp_proba_gb,4,2,gb.classes_)\n\n#SVM\nmulticlass_roc(one_hot_encode(np.array(Y_test1),lr.classes_),Y_testp_proba_svm,4,2,gb.classes_)\nmulticlass_roc(one_hot_encode(np.array(Y_train1),lr.classes_),Y_trainp_proba_svm,4,2,gb.classes_)\n\n","baf892a1":"#all diseases\ndiseases=np.unique(labels1)  \n\nfor d in diseases:\n    if d!='NL': # not for normal\n        \n        Y_train2=np.where(np.array(Y_train1)==d,d,'ALL') # relabel training data\n        Y_test2=np.where(np.array(Y_test1)==d,d,'ALL')   #relabel test data\n        \n        #logistic regression\n        \n        lr = LogisticRegression(multi_class='multinomial', solver='lbfgs',C=1, penalty='l2', fit_intercept=True, max_iter=5000, random_state=42)\n        lr.fit(X_train,Y_train2)\n        Y_testp=lr.predict(X_test)\n        Y_trainp=lr.predict(X_train)\n        #percent accuracy\n        test_accuracy=np.sum(np.array(Y_test2)==Y_testp)\/len(Y_testp)\n        print('LR '+d,': ',test_accuracy)\n\n        #random forest\n        \n        forest=RandomForestClassifier(n_estimators=100, max_depth=3,random_state=42)\n        forest.fit(X_train,Y_train2)        \n        Y_testp_forest=forest.predict(X_test)        \n        test_accuracy=np.sum(np.array(Y_test2)==Y_testp_forest)\/len(Y_testp)\n        print('RF '+d,': ',test_accuracy)\n        \n\n        #gradient boosting\n        gb=GradientBoostingClassifier(learning_rate=0.015, n_estimators=100, subsample=1, max_depth=2,  random_state=42 )\n        gb.fit(X_train,Y_train2)\n        #models[d]=gb\n        Y_testp=gb.predict(X_test)\n        test_accuracy=np.sum(np.array(Y_test2)==Y_testp)\/len(Y_testp)\n        print('GB '+d,': ',test_accuracy)\n        \n       \n        #SVM\n        svm1 = svm.SVC(gamma='scale',probability=True)\n        svm1.fit(X_train, Y_train2)\n        Y_testp_svm=svm1.predict(X_test)\n        \n        test_accuracy=np.sum(np.array(Y_test2)==Y_testp_svm)\/len(Y_testp)\n        print('SVM '+d,': ',test_accuracy)\n        \n        print('.................')\n","f2763400":"# Plot ROC curves for the above models","0eb12e67":"# Create training and testing data","142d4650":"# Read the data files","ef0f91c4":"# Train binary classifiers on the same datasets","1f910965":"# Perform PCA on the features","23492796":"# Build multiclass classification models"}}