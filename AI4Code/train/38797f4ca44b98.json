{"cell_type":{"141dd37d":"code","fdfcbdc6":"code","386ee4ea":"code","c2f0c158":"code","7ef4053d":"code","a96cf882":"code","5086ff06":"code","61f66fef":"code","15af45a1":"code","dbdda814":"code","7ba79d5a":"code","ae7eaf42":"code","7d59cde6":"code","e1cc933f":"code","bdf6bc58":"code","bcb6afc6":"code","f2834efd":"code","c723f637":"code","9e27be92":"code","d7c39f1a":"code","fb229a3c":"code","afa5b88a":"code","20d7f2c3":"code","52ab3d4d":"code","25e04d9c":"code","cd14f0c0":"code","f916af81":"code","9aa16f89":"code","acb53458":"code","14c13bcf":"code","cb0e7517":"code","71dfe29b":"code","d7e2c87d":"code","8248a867":"code","4cb56ffb":"markdown","0c475864":"markdown","4d575ac9":"markdown","ce548aff":"markdown","b2efbb26":"markdown","a659c426":"markdown","afca86f7":"markdown","731fce3a":"markdown","281fb88d":"markdown","f7b2051b":"markdown","dcefb049":"markdown","c707744d":"markdown","7f0f2e2b":"markdown","b7b70cdb":"markdown","d76a2271":"markdown","d95f584c":"markdown","8e288204":"markdown","e8940459":"markdown","b5e1a3d3":"markdown"},"source":{"141dd37d":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\ntqdm.pandas()\n\nfrom flashtext import KeywordProcessor\nfrom sklearn.impute import KNNImputer\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split,cross_validate,ShuffleSplit\nfrom sklearn.metrics import accuracy_score\n\nimport warnings\nwarnings.filterwarnings('ignore')","fdfcbdc6":"df_train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/titanic\/test.csv\")","386ee4ea":"scaler = MinMaxScaler()\n\n#Normalising the Titles\ndict_title = {\n    \"Officer\" : [\"Capt\",\"Col\",\"Major\",\"Dr\",\"Rev\",],\n    \"Royalty\": [\"Jonkheer\",\"Don\",\"Dona\", \"Sir\",\"the Countess\",\"Lady\",],\n    \"Mrs\": [\"Mme\",\"Ms\",\"Mrs\",],\n    \"Mr\" : [\"Mr\"],\n    \"Miss\": [\"Miss\",\"Mlle\",],\n    \"Master\" : [\"Master\"],\n}\n\nkp_title_norm = KeywordProcessor()\nkp_title_norm.add_keywords_from_dict(dict_title)\n\ndef pipeline(df_):\n    \"\"\"\n    Pipeline for Feature Engineering, Missing value imputation, and Scaling\n    \"\"\"\n    \n    ## Missing value \n    # Fill missing value of \"Embarked\" by mode\n    df_['Embarked'].fillna(df_['Embarked'].mode()[0], inplace=True)\n\n    ## Creating one hot encoder\n    df_[\"Sex_bin\"] = pd.get_dummies(df_[\"Sex\"], prefix=['Sex_bin'], drop_first=True)\n    df_temp = pd.get_dummies(df_[\"Embarked\"], prefix='Embarked', drop_first=True)\n    df_ = df_.join(df_temp)\n\n    df_temp = pd.get_dummies(df_[\"Pclass\"], prefix='Pclass', drop_first=True)\n    df_ = df_.join(df_temp)\n\n    count_Ticket = dict(df_.Ticket.value_counts())\n    df_[\"count_Ticket\"] = df_[\"Ticket\"].apply(lambda x: count_Ticket[x])\n    \n    df_['Title'] = df_['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n    df_[\"Title_norm\"] = df_[\"Title\"].apply(lambda x: kp_title_norm.extract_keywords(x)[0])\n\n    df_temp = pd.get_dummies(df_[\"Title_norm\"], prefix='Title_norm',)\n    df_ = df_.join(df_temp)\n\n    df_['Is_Married'] = 0\n    df_['Is_Married'].loc[df_['Title'] == 'Mrs'] = 1\n\n    df_.drop([\"Name\", 'Sex', \"Embarked\", \"Pclass\", \"Ticket\", \"Cabin\", \"Title\", \"Title_norm\"], axis = 1, inplace = True)\n\n    # Fill missing value of \"Embarked\" by  KNNimputer\n    list_columns_temp = list(df_.columns)\n    imputer = KNNImputer(n_neighbors=2)\n    df_ = pd.DataFrame(imputer.fit_transform(df_), columns= list_columns_temp)\n\n    df_[['Age', 'Fare']] = scaler.fit_transform(df_[['Age', 'Fare']])\n    \n    return df_","c2f0c158":"df_train_ = df_train.copy()\ndf_test_ = df_test.copy()\ndf_train__preprocessed = pipeline(df_train_)","7ef4053d":"df_train__preprocessed.head(2)","a96cf882":"del df_train__preprocessed[\"PassengerId\"]\ncol_df = list(df_train__preprocessed.columns)\ncol_X = [elem for elem in col_df if elem != \"Survived\"]\ncol_Y = [\"Survived\"]","5086ff06":"# create training and testing vars\nX_train, X_val, y_train, y_val = train_test_split(df_train__preprocessed[col_X], df_train__preprocessed[col_Y], test_size=0.2)\nprint (X_train.shape, y_train.shape)\nprint (X_val.shape, y_val.shape)","61f66fef":"## Predicting on the Test Data\ndef get_predictions(clf_, if_probability=False):\n    df= pd.DataFrame()\n    df[\"PassengerId\"] = df_test_[\"PassengerId\"]\n    df[\"Survived\"] = clf_.predict(df_test__preprocessed[col_X])\n    if not if_probability:\n        df[\"Survived\"] = df[\"Survived\"].apply(lambda x: int(x))\n    else:\n        predict_train = clf_.predict(df_test__preprocessed[col_X])\n        df[\"Survived\"] = [1 if (i>0.5) else 0 for i in predict_train]\n    return df","15af45a1":"from sklearn.linear_model import LogisticRegression\n\n# Initialize the model\nLogReg = LogisticRegression(max_iter=60)\n\n# fit the model with the training data\nLogReg.fit(X_train,y_train)\n\n# # coefficeints of the trained model\n# print('Coefficient of model :', LogReg.coef_)\n\n# # intercept of the model\n# print('Intercept of model',LogReg.intercept_)\n\n# predict the target on the train dataset\npredict_train = LogReg.predict(X_train)\n\n# Accuray Score on train dataset\naccuracy_train = accuracy_score(y_train,predict_train)\nprint('accuracy_score on train dataset : ', accuracy_train)\n\n# predict the target on the test dataset\npredict_test = LogReg.predict(X_val)\n\n# Accuracy Score on test dataset\naccuracy_test = accuracy_score(y_val,predict_test)\nprint('accuracy_score on Validation dataset : ', accuracy_test)","dbdda814":"df_test__preprocessed = pipeline(df_test_)\ndf_predict_test_LogReg = get_predictions(LogReg)\npath_LogReg_predict = \"LogReg_predict_17_june.csv\"\ndf_predict_test_LogReg.to_csv(path_LogReg_predict, index=False)","7ba79d5a":"from sklearn.tree import DecisionTreeClassifier\n\n# Create Decision Tree classifer object\nDecisionTree_clf = DecisionTreeClassifier(criterion=\"entropy\",)\n\n# Train Decision Tree Classifer\nDecisionTree_clf.fit(X_train,y_train)\n\n# predict the target on the train dataset\npredict_train = DecisionTree_clf.predict(X_train)\n\n# Accuray Score on train dataset\naccuracy_train = accuracy_score(y_train,predict_train)\nprint('accuracy_score on train dataset : ', accuracy_train)\n\n# predict the target on the test dataset\npredict_test = DecisionTree_clf.predict(X_val)\n\n# Accuracy Score on test dataset\naccuracy_test = accuracy_score(y_val,predict_test)\nprint('accuracy_score on Validation dataset : ', accuracy_test)","ae7eaf42":"# We can clearly see that we have overfitted our model, Hence, we will tune it to get better results.\n\n# ## Predicting on the Test Data\ndf_predict_test_DecisionTree = get_predictions(DecisionTree_clf)\n\npath_DecisionTree_predict = \"DecisionTree_predict_17_june.csv\"\ndf_predict_test_DecisionTree.to_csv(path_DecisionTree_predict, index=False)","7d59cde6":"# Necessary imports \nfrom scipy.stats import randint \nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.model_selection import RandomizedSearchCV \n  \n# Creating the hyperparameter grid  \nparam_dist = {\"max_depth\": [3, 4,6, None], \n              \"max_features\": randint(1, 17), \n              \"min_samples_leaf\": randint(1, 5), \n              \"criterion\": [\"gini\", \"entropy\"]} \n  \n# Instantiating Decision Tree classifier \nDecisionTree_clf = DecisionTreeClassifier() \n  \n# Instantiating RandomizedSearchCV object \nDecisionTree_clf_cv = RandomizedSearchCV(DecisionTree_clf, param_dist, cv = 5) \n\n# Train Decision Tree Classifer\nDecisionTree_clf_cv.fit(X_train,y_train)\n\n# Print the tuned parameters and score \nprint(\"Tuned Decision Tree Parameters: {}\".format(DecisionTree_clf_cv.best_params_)) \nprint(\"Best score is {}\".format(DecisionTree_clf_cv.best_score_)) \n\n# predict the target on the train dataset\npredict_train = DecisionTree_clf_cv.predict(X_train)\n# print('Target on train data',predict_train) \n\n# Accuray Score on train dataset\naccuracy_train = accuracy_score(y_train,predict_train)\nprint('accuracy_score on train dataset : ', accuracy_train)\n\n# predict the target on the test dataset\npredict_test = DecisionTree_clf_cv.predict(X_val)\n# print('Target on test data',predict_test) \n\n# Accuracy Score on test dataset\naccuracy_test = accuracy_score(y_val,predict_test)\nprint('accuracy_score on Validation dataset : ', accuracy_test)","e1cc933f":"# Create Decision Tree classifer object\n\nbest_criterion_ = DecisionTree_clf_cv.best_params_[\"criterion\"]\nbest_max_depth_ = DecisionTree_clf_cv.best_params_[\"max_depth\"]\nbest_max_features_ = DecisionTree_clf_cv.best_params_[\"max_features\"]\nbest_min_samples_leaf_ = DecisionTree_clf_cv.best_params_[\"min_samples_leaf\"]\n\nDecisionTree_clf_tuned = DecisionTreeClassifier(criterion=best_criterion_, max_depth= best_max_depth_, max_features= best_max_features_,\n                                                min_samples_leaf= best_min_samples_leaf_)\n\n# Train Decision Tree Classifer\nDecisionTree_clf_tuned.fit(X_train,y_train)\n\n# predict the target on the train dataset\npredict_train = DecisionTree_clf_tuned.predict(X_train)\n# print('Target on train data',predict_train) \n\n# Accuray Score on train dataset\naccuracy_train = accuracy_score(y_train,predict_train)\nprint('accuracy_score on train dataset : ', accuracy_train)\n\n# predict the target on the test dataset\npredict_test = DecisionTree_clf_tuned.predict(X_val)\n# print('Target on test data',predict_test) \n\n# Accuracy Score on test dataset\naccuracy_test = accuracy_score(y_val,predict_test)\nprint('accuracy_score on Validation dataset : ', accuracy_test)","bdf6bc58":"df_predict_test_DecisionTree_tuned = get_predictions(DecisionTree_clf_tuned)\npath_DecisionTree_Tuned_predict = \"DecisionTree_Tuned_predict_17_june.csv\"\ndf_predict_test_DecisionTree_tuned.to_csv(path_DecisionTree_Tuned_predict, index=False)","bcb6afc6":"#Import Library\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create Linear RandomForestClassifier object\nrandom_forest_classifier = RandomForestClassifier(n_estimators=100)\n\n# Train Classifer\nrandom_forest_classifier.fit(X_train,y_train)\n\n# predict the target on the train dataset\npredict_train = random_forest_classifier.predict(X_train)\n\n# Accuray Score on train dataset\naccuracy_train = accuracy_score(y_train,predict_train)\nprint('accuracy_score on train dataset : ', accuracy_train)\n\n# predict the target on the test dataset\npredict_test = random_forest_classifier.predict(X_val)\n\n# Accuracy Score on test dataset\naccuracy_test = accuracy_score(y_val,predict_test)\nprint('accuracy_score on Validation dataset : ', accuracy_test)","f2834efd":"#Import Library\nfrom sklearn import svm\n\n# Create Linear SVM object\nSVM_classifier = svm.LinearSVC(random_state=20)\n\n# Train Decision Tree Classifer\nSVM_classifier.fit(X_train,y_train)\n\n# predict the target on the train dataset\npredict_train = SVM_classifier.predict(X_train)\n\n# Accuray Score on train dataset\naccuracy_train = accuracy_score(y_train,predict_train)\nprint('accuracy_score on train dataset : ', accuracy_train)\n\n# predict the target on the test dataset\npredict_test = SVM_classifier.predict(X_val)\n\n# Accuracy Score on test dataset\naccuracy_test = accuracy_score(y_val,predict_test)\nprint('accuracy_score on Validation dataset : ', accuracy_test)","c723f637":"df_predict_test_SVM_classifier = get_predictions(SVM_classifier)\n# df_predict_test_SVM_classifier.head(2)\n\npath_SVM_classifier_predict = \"SVM_classifier_predict_17_june.csv\"\ndf_predict_test_SVM_classifier.to_csv(path_SVM_classifier_predict, index=False)","9e27be92":"import lightgbm as lgb\n\n# Create Light GB object\nLGB_classifier = lgb.LGBMClassifier()\n\n# fit the model with the training data\nLGB_classifier.fit(X_train,y_train)\n\n# predict the target on the train dataset\npredict_train = LGB_classifier.predict(X_train)\n\n# Accuray Score on train dataset\naccuracy_train = accuracy_score(y_train,predict_train)\nprint('accuracy_score on train dataset : ', accuracy_train)\n\n# predict the target on the test dataset\npredict_test = LGB_classifier.predict(X_val)\n\n# Accuracy Score on test dataset\naccuracy_test = accuracy_score(y_val,predict_test)\nprint('accuracy_score on Validation dataset : ', accuracy_test)","d7c39f1a":"df_predict_test_LGB_classifier = get_predictions(LGB_classifier)\n\npath_LGB_classifier_predict = \"LGB_classifier_predict_17_june.csv\"\ndf_predict_test_LGB_classifier.to_csv(path_LGB_classifier_predict, index=False)","fb229a3c":"from xgboost import XGBClassifier\n\n# fit model on training data\nXGB_classifier = XGBClassifier()\n\n# fit the model with the training data\nXGB_classifier.fit(X_train,y_train)\n\n# predict the target on the train dataset\npredict_train = XGB_classifier.predict(X_train)\n\n# Accuray Score on train dataset\naccuracy_train = accuracy_score(y_train,predict_train)\nprint('accuracy_score on train dataset : ', accuracy_train)\n\n# predict the target on the test dataset\npredict_test = XGB_classifier.predict(X_val)\n\n# Accuracy Score on test dataset\naccuracy_test = accuracy_score(y_val,predict_test)\nprint('accuracy_score on Validation dataset : ', accuracy_test)","afa5b88a":"df_predict_test_XGB_classifier = get_predictions(XGB_classifier)\npath_XGB_classifier_predict = \"XGB_classifier_predict_17_june.csv\"\ndf_predict_test_XGB_classifier.to_csv(path_XGB_classifier_predict, index=False)","20d7f2c3":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n\n## Hyper Parameter Optimization\nparams={\n \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n    \n}\nclassifier=XGBClassifier()\nXGB_clf_cv=RandomizedSearchCV(classifier,param_distributions=params,n_iter=5,scoring='roc_auc', cv=5)\nXGB_clf_cv.fit(X_train,y_train)\n\n\n# Print the tuned parameters and score \nprint(\"Tuned Decision Tree Parameters: {}\".format(XGB_clf_cv.best_params_)) \nprint(\"Best score is {}\".format(XGB_clf_cv.best_score_)) \n\n# predict the target on the train dataset\npredict_train = XGB_clf_cv.predict(X_train)\n\n# Accuray Score on train dataset\naccuracy_train = accuracy_score(y_train,predict_train)\nprint('accuracy_score on train dataset : ', accuracy_train)\n\n# predict the target on the test dataset\npredict_test = XGB_clf_cv.predict(X_val)\n# print('Target on test data',predict_test) \n\n# Accuracy Score on test dataset\naccuracy_test = accuracy_score(y_val,predict_test)\nprint('accuracy_score on Validation dataset : ', accuracy_test)","52ab3d4d":"# Create Decision Tree classifer object\nczzcc\nbest_criterion_ = DecisionTree_clf_cv.best_params_[\"criterion\"]\nbest_max_depth_ = DecisionTree_clf_cv.best_params_[\"max_depth\"]\nbest_max_features_ = DecisionTree_clf_cv.best_params_[\"max_features\"]\nbest_min_samples_leaf_ = DecisionTree_clf_cv.best_params_[\"min_samples_leaf\"]\n\nDecisionTree_clf_tuned = DecisionTreeClassifier(criterion=best_criterion_, max_depth= best_max_depth_, max_features= best_max_features_,\n                                                min_samples_leaf= best_min_samples_leaf_)\n\n# Train Decision Tree Classifer\nDecisionTree_clf_tuned.fit(X_train,y_train)\n\n# predict the target on the train dataset\npredict_train = DecisionTree_clf_tuned.predict(X_train)\n# print('Target on train data',predict_train) \n\n# Accuray Score on train dataset\naccuracy_train = accuracy_score(y_train,predict_train)\nprint('accuracy_score on train dataset : ', accuracy_train)\n\n# predict the target on the test dataset\npredict_test = DecisionTree_clf_tuned.predict(X_val)\n# print('Target on test data',predict_test) \n\n# Accuracy Score on test dataset\naccuracy_test = accuracy_score(y_val,predict_test)\nprint('accuracy_score on Validation dataset : ', accuracy_test)","25e04d9c":"import tensorflow as tf","cd14f0c0":"## Single Hidden layer\n\nann_classifier = tf.keras.models.Sequential()\nann_classifier.add(tf.keras.layers.Dense(units=6, activation='relu'))\n# ann_classifier.add(tf.keras.layers.Dense(units=6, activation='relu'))\nann_classifier.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\nann_classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\nann_classifier.fit(X_train, y_train, batch_size = 32, epochs = 100)\n\n\n# # fit the model with the training data\n# XGB_classifier.fit(X_train,y_train)\n\n# predict the target on the train dataset\npredict_train = ann_classifier.predict(X_train)\npredict_train = [1 if (i>0.5) else 0 for i in predict_train ]\n\n# Accuray Score on train dataset\naccuracy_train = accuracy_score(y_train,predict_train)\nprint('accuracy_score on train dataset : ', accuracy_train)\n\n# predict the target on the test dataset\npredict_test = ann_classifier.predict(X_val)\npredict_test = [1 if (i>0.5) else 0 for i in predict_test ]\n\n# Accuracy Score on test dataset\naccuracy_test = accuracy_score(y_val,predict_test)\nprint('accuracy_score on Validation dataset : ', accuracy_test)","f916af81":"df_predict_test_ANN_classifier = get_predictions(ann_classifier, if_probability=True)\npath_ANN_classifier_predict = \"ANN_classifier_predict_17_june.csv\"\ndf_predict_test_ANN_classifier.to_csv(path_ANN_classifier_predict, index=False)","9aa16f89":"## Multi-Hidden layer\n\nann_classifier_2 = tf.keras.models.Sequential()\nann_classifier_2.add(tf.keras.layers.Dense(units=12, activation='relu'))\nann_classifier_2.add(tf.keras.layers.Dense(units=8, activation='relu'))\nann_classifier_2.add(tf.keras.layers.Dense(units=6, activation='relu'))\nann_classifier_2.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\nann_classifier_2.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\nann_classifier_2.fit(X_train, y_train, batch_size = 32, epochs = 100)\n\n\n# predict the target on the train dataset\npredict_train = ann_classifier_2.predict(X_train)\npredict_train = [1 if (i>0.5) else 0 for i in predict_train ]\n\n# Accuray Score on train dataset\naccuracy_train = accuracy_score(y_train,predict_train)\nprint('accuracy_score on train dataset : ', accuracy_train)\n\n# predict the target on the test dataset\npredict_test = ann_classifier_2.predict(X_val)\npredict_test = [1 if (i>0.5) else 0 for i in predict_test ]\n\n# Accuracy Score on test dataset\naccuracy_test = accuracy_score(y_val,predict_test)\nprint('accuracy_score on Validation dataset : ', accuracy_test)","acb53458":"df_predict_test_ANN_classifier_2 = get_predictions(ann_classifier_2, if_probability=True)\npath_ANN_classifier_2_predict = \"ANN_classifier_2_predict_17_june.csv\"\ndf_predict_test_ANN_classifier_2.to_csv(path_ANN_classifier_2_predict, index=False)","14c13bcf":"# from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process, neural_network\nfrom sklearn import ensemble\n\nvote_est = [\n    (\"LogReg\", LogisticRegression(max_iter=60)),\n    (\"DecisionTree_clf_tuned\", DecisionTreeClassifier(criterion=best_criterion_, max_depth= best_max_depth_, max_features= best_max_features_,\n                                                min_samples_leaf= best_min_samples_leaf_)),\n    (\"SVC\", svm.SVC(max_iter = 500000,probability=True,kernel='linear',C=0.025)),\n    (\"LGB_classifier\", lgb.LGBMClassifier()),\n    (\"XGB_classifier\", XGBClassifier()),]","cb0e7517":"#Hard Vote\ncv_split = ShuffleSplit(n_splits = 10, test_size = .3, train_size = .7, random_state = 0 ) \nvote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\n\nvote_hard_cv = cross_validate(vote_hard, df_train__preprocessed[col_X], \n                              df_train__preprocessed[col_Y], cv  = cv_split, return_train_score=True)\n\nvote_hard.fit(df_train__preprocessed[col_X], df_train__preprocessed[col_Y])\n\nprint(\"Hard Voting Training w\/bin score mean: {:.2f}\". format(vote_hard_cv['train_score'].mean()*100)) \nprint(\"Hard Voting Test w\/bin score mean: {:.2f}\". format(vote_hard_cv['test_score'].mean()*100))\nprint(\"Hard Voting Test w\/bin score 3*std: +\/- {:.2f}\". format(vote_hard_cv['test_score'].std()*100*3))\nprint('-'*10)","71dfe29b":"df_predict_test_Ensemble_hard_vote_classifier = get_predictions(vote_hard)\npath_Ensemble_hard_vote_classifier_predict = \"Ensemble_hard_vote_classifier_predict_17_june.csv\"\ndf_predict_test_Ensemble_hard_vote_classifier.to_csv(path_Ensemble_hard_vote_classifier_predict, index=False)","d7e2c87d":"#Soft Vote\nvote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\n\nvote_soft_cv = cross_validate(vote_soft, df_train__preprocessed[col_X], \n                              df_train__preprocessed[col_Y], cv=cv_split, return_train_score=True)\n\nvote_soft.fit(df_train__preprocessed[col_X], df_train__preprocessed[col_Y])\n\nprint(\"Soft Voting Training w\/bin score mean: {:.2f}\". format(vote_soft_cv['train_score'].mean()*100)) \nprint(\"Soft Voting Test w\/bin score mean: {:.2f}\". format(vote_soft_cv['test_score'].mean()*100))\nprint(\"Soft Voting Test w\/bin score 3*std: +\/- {:.2f}\". format(vote_soft_cv['test_score'].std()*100*3))\nprint('-'*10)","8248a867":"df_predict_test_Ensemble_soft_vote_classifier = get_predictions(vote_soft)\npath_Ensemble_soft_vote_classifier_predict = \"Ensemble_soft_vote_classifier_predict_17_june.csv\"\ndf_predict_test_Ensemble_soft_vote_classifier.to_csv(path_Ensemble_soft_vote_classifier_predict, index=False)","4cb56ffb":"## **Thanks \u30c3**","0c475864":"## **Decision Tree:**","4d575ac9":"Basic LightGBM: 0.75598","ce548aff":"## ***Final Submission Scores*:**\n* Logistic Regression: 0.78468\n* Decision Tree: 0.77990\n* SVM: 0.78947\n* Ligt Gradient Boosting: 0.75598\n* XGBoost: 0.75598\n* Single Hidden layer ANN: 0.80861\n* Multi-Hidden layer ANN: 0.78468\n* Ensemble:\n    * Hard: 0.78468\n    * Soft: 0.77990","b2efbb26":"## **LightGBM:**","a659c426":"Basic Logistic Regression Score: 0.78468","afca86f7":"## **ANN:**","731fce3a":"Ensemble (Hard Vote) :0.78468","281fb88d":"Multi-Hidden layer: 0.78468","f7b2051b":"Ensemble (Soft Vote) :0.77990","dcefb049":"## **Pre-Processing**","c707744d":"## **Voting Ensemble**","7f0f2e2b":"## **Logistic Regression:**","b7b70cdb":"## **SVM:**","d76a2271":"Basic SVM Classifier: 0.78947","d95f584c":"Basic XGBoost: 0.75598","8e288204":"Tuned Decision Tree Classifier: 0.77990","e8940459":"Single-Hidden layer: 0.80861","b5e1a3d3":"## **Xgboost:**"}}