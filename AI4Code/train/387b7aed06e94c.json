{"cell_type":{"6c367253":"code","dd28ef58":"code","9c05f7fe":"code","199d99fd":"code","a889c91e":"code","f00f9544":"code","972aeefa":"code","a0609df6":"code","ea03028f":"code","655c49bc":"code","8ed58a83":"code","ada13fdf":"code","2f14ad32":"code","91bf4fa1":"code","e01e813f":"code","3884464a":"code","5e342f9f":"code","c8970968":"code","436b9e2e":"code","fe3e9b88":"code","23244525":"code","30b038a8":"code","2d2afae4":"code","a5fedc76":"code","f994aac2":"code","9153b4ba":"code","21c9a205":"code","b89d2286":"code","716a4137":"markdown","ab7e281c":"markdown","8eebf32e":"markdown"},"source":{"6c367253":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are avai_lable in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","dd28ef58":"! pip install pyspark","9c05f7fe":"from pyspark.sql import SparkSession","199d99fd":"spark = SparkSession.builder.appName(\"task1\").getOrCreate()","a889c91e":"train_path = \"..\/input\/train.csv\"","f00f9544":"from pyspark.sql.functions import lag,date_format,udf\nfrom pyspark.sql.types import IntegerType,ArrayType,TimestampType","972aeefa":"df = spark.read.csv(train_path,inferSchema=True,header=True)\n#df = df.filter(\"Store = 1\")","a0609df6":"from pyspark.sql import Window\nfrom pyspark.sql.functions import lag,date_format,col,sum,weekofyear,year,mean,round,when,collect_list,dayofweek,expr","ea03028f":"df = df.withColumn(\"year\",year(df[\"date\"]))","655c49bc":"df = df.withColumn(\"week_id\",weekofyear(df[\"date\"]))","8ed58a83":"def date_range(min_date,max_date):\n    base = min_date\n    numdays =  ((max_date-min_date).days) +1\n    date_list = []\n    for x in range(0, numdays):\n        date_list.append([base + timedelta(days=x)])\n    return date_list","ada13fdf":"col_name=\"Date\"\nfrom pyspark.sql.functions import min,max,col\nfrom datetime import datetime,timedelta\nmin_date = df.select(min(col(col_name))).collect()[0][0]\nmax_date = df.select(max(col(col_name))).collect()[0][0]\ndate_list = date_range(min_date,max_date)\nfull_date = spark.createDataFrame(date_list,[col_name])","2f14ad32":"store = df.select(\"store\").distinct()","91bf4fa1":"store_date = store.crossJoin(full_date)","e01e813f":"missing_data = store_date.select(\"store\",\"date\").exceptAll(df.select(\"store\",\"date\"))","3884464a":"missing_data = missing_data.select(\"store\",dayofweek(\"date\").alias(\"DayOfWeek\"),\"date\",expr(\"0\").alias(\"sales\"),expr(\"0\").alias(\"customers\"),expr(\"0\").alias(\"open\"),expr(\"-1\").alias(\"promo\"),expr(\"1\").alias(\"StateHoliday\"),expr(\"1\").alias(\"SchoolHoliday\"),year(\"date\").alias(\"year\"),weekofyear(\"date\").alias(\"week_id\"))","5e342f9f":"#missing_data.count()","c8970968":"comp = df.union(missing_data)","436b9e2e":"close = df.filter(\"open=1\").groupBy(\"year\",\"week_id\",\"store\").agg(mean(\"sales\").alias(\"avg_sales\"),round(mean(\"Customers\"),0).alias(\"avg_customers\")).orderBy(\"store\",\"year\",\"week_id\")\n","fe3e9b88":"#close.show()","23244525":"new_df = comp.join(close,on=((comp.year==close.year) & (comp.week_id==close.week_id) & (comp.Store==close.store) ))\nnew_df = new_df.select(comp.Store,comp.DayOfWeek,comp.Date,when(comp.Sales==0,close.avg_sales).otherwise(comp.Sales).alias(\"sales\"), when(comp.Customers==0,close.avg_customers).otherwise(comp.Customers).alias(\"customers\"),comp.Promo,comp.StateHoliday,comp.SchoolHoliday,comp.year,comp.week_id ).orderBy(\"Store\",\"year\",\"week_id\")","30b038a8":"#new_df.show()","2d2afae4":"gdf = new_df.groupBy(\"year\",\"week_id\",\"store\").agg(sum(\"sales\").alias(\"weekly_sales\")).orderBy(\"year\",\"week_id\",\"store\")","a5fedc76":"#gdf.show(2000)","f994aac2":"win = Window.partitionBy(\"store\").orderBy(\"year\",\"week_id\",\"store\")\ncuml_sales = gdf.withColumn(\"cuml_sales\",sum(\"weekly_sales\").over(win))\n#cuml_sales.show()","9153b4ba":"cuml_sales.orderBy(\"store\",\"year\",\"week_id\").show(1000)","21c9a205":"#new_df.filter(\"store=1 and week_id=2 and year=2013\").select(sum(\"sales\")).show()","b89d2286":"#new_df.filter(\"store=1 and week_id=1 and year=2013\").select(\"store\",\"week_id\",\"year\",\"sales\").show()","716a4137":"new gdf","ab7e281c":"## <center style='color:#e74c3c'>is date continuous<\/center>","8eebf32e":"## new imputation"}}