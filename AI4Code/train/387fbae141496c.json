{"cell_type":{"acc11ef0":"code","47306d54":"code","7acf265d":"code","e832ab2a":"code","5dbbc7a5":"code","440bda48":"code","01bd5199":"code","3a7657bb":"code","faf807fa":"code","932e71a3":"code","9c48e51a":"code","1b6ab7a7":"code","fd57aa0d":"code","794c1c72":"code","247b30be":"code","9762eea8":"code","19ed8124":"code","cd4db4b7":"code","5da88717":"code","76ed7702":"code","455fa21e":"code","969b1b0f":"code","7c870b9b":"code","b643c5ae":"code","65199743":"code","bb688e1a":"code","251df802":"code","b4fbb544":"code","a995e693":"code","44303e89":"code","4dca2c0b":"code","98a92888":"code","c2ad4169":"code","0d605fca":"code","eb758db2":"code","af8a664e":"code","376b783c":"code","0baaffa8":"code","ec8769b0":"markdown","9c7e5a8f":"markdown","8099073f":"markdown","4773f8b4":"markdown","d0e7f4e2":"markdown","d148ab4f":"markdown","b57220cb":"markdown","72aa671c":"markdown","ef02b055":"markdown","2892b159":"markdown","93942e58":"markdown","1554d883":"markdown","22aea243":"markdown","5006d8ae":"markdown","3970eec8":"markdown","345866fb":"markdown","6eacc4af":"markdown"},"source":{"acc11ef0":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","47306d54":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing, impute\nfrom IPython.display import display\n\nfrom sklearn.dummy import DummyClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n\nimport optuna\nimport statsmodels.api as sm\nimport scipy.stats as stats\n\nplt.style.use('ggplot')","7acf265d":"random_state = 42\nrng = np.random.RandomState(random_state)\n\nfrom optuna.samplers import TPESampler\nsampler = TPESampler(seed=random_state)","e832ab2a":"df_train = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv', index_col=0)\nX_test = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv', index_col=0)\n\ncolumn_y = df_train.columns.difference(X_test.columns)[0]\nX = df_train.drop(columns=column_y)\ny = df_train[column_y]\n\nX_proc = X.assign(missing_sum=X.isna().sum(axis=1))\nX_test_proc = X_test.assign(missing_sum=X_test.isna().sum(axis=1))\n\ndel df_train, X, X_test","5dbbc7a5":"# df_train = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv', index_col=0)\n# y = df_train['claim']\n# del df_train\n\n# X_proc = pd.read_csv('\/kaggle\/input\/tps-092021-processing\/proc.csv', index_col=0)\n# X_test_proc = pd.read_csv('\/kaggle\/input\/tps-092021-processing\/proc_test.csv', index_col=0)\n\n# X_proc = X_proc.filter(regex=\"^f\\d+$|^missing_sum$\")\n# X_test_proc = X_test_proc.filter(regex=\"^f\\d+$|^missing_sum$\")","440bda48":"def submit(y_pred, name):\n    ys = pd.DataFrame(y_pred, index=X_test_proc.index, columns=[y.name])\n    ys.to_csv(f\"{name}.csv\")\n    return ys","01bd5199":"# def get_folds(random_state=random_state):\n#     return StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n\n# folds = get_folds()","3a7657bb":"# fixed_folds = []\n\n# for train_index, test_index in get_folds().split(X_proc, y):\n#     fixed_folds.append((train_index, test_index))","faf807fa":"xgb_params = {\n    \"objective\":\"binary:logistic\",\n    \"eval_metric\":\"auc\",\n    'gpu_id':0,\n    'tree_method':'gpu_hist',\n    #\"predictor\": \"gpu_predictor\",\n    \"verbosity\":0,\n}","932e71a3":"# dtrain = xgb.DMatrix(X_proc, label=y)","9c48e51a":"# eval_hist = {}\n# for eta in np.geomspace(0.01, 1, 5):\n#     eval_hist[eta] = xgb.cv(\n#         params={'eta':eta, **xgb_params},\n#         dtrain=dtrain,\n#         num_boost_round=10_000,\n#         early_stopping_rounds=50,\n#         #folds=folds,\n#         nfold=5,\n#         stratified=True,\n#         shuffle=True,\n#         metrics='auc',\n#         verbose_eval=False,\n#         seed=random_state,\n#     )","1b6ab7a7":"# r_eval_hist_train, r_eval_hist_test = {}, {}\n# for eta, d in eval_hist.items():\n#     r_eval_hist_train[eta] = pd.Series(d['train-auc-mean'])\n#     r_eval_hist_test[eta] = pd.Series(d['test-auc-mean'])\n\n# r_eval_hist_train = pd.DataFrame(r_eval_hist_train)\n# r_eval_hist_test = pd.DataFrame(r_eval_hist_test)\n# r_eval_hist_test.plot(style='-')","fd57aa0d":"# pd.concat([r_eval_hist_test.idxmax().rename('num_boost'),\n#            r_eval_hist_test.max().rename('min_test_score'),\n#            r_eval_hist_train.where(r_eval_hist_test.apply(lambda x: x==x.max())).agg('sum').rename('min_train_score')], axis=1)","794c1c72":"xgb_params['eta'] = 0.1\nxgb_params['booster'] = 'gbtree'","247b30be":"# def objective(trial, dtrain=dtrain):    \n#     params = {\n#         **xgb_params,\n#         \"max_depth\":trial.suggest_int(\"max_depth\", 2, 8),\n#         \"min_child_weight\":trial.suggest_int(\"min_child_weight\", 1, 8),\n#         \"colsample_bytree\":trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n#         \"subsample\":trial.suggest_float(\"subsample\", 0.2, 1.0),\n#         \"reg_alpha\":trial.suggest_float(\"reg_alpha\", 1e-4, 1e2, log=True),\n#         \"reg_lambda\":trial.suggest_float(\"reg_lambda\", 1e-4, 1e2, log=True),\n#     }\n    \n#     pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"test-auc\")\n#     history = xgb.cv(\n#         params,\n#         dtrain=dtrain,\n#         num_boost_round=500,\n#         early_stopping_rounds=50,\n#         #folds=folds,\n#         nfold=5,\n#         stratified=True,\n#         shuffle=True,\n#         metrics='auc',\n#         verbose_eval=False,\n#         callbacks=[pruning_callback],\n#         seed=random_state,\n#     )\n    \n#     trial.set_user_attr(\"n_estimators\", len(history))\n    \n#     best_score = history[\"test-auc-mean\"].values[-1]\n#     return best_score\n\n# pruner = optuna.pruners.MedianPruner(n_warmup_steps=10)\n# xgb_study = optuna.create_study(pruner=pruner, direction=\"maximize\", sampler=sampler)\n# xgb_study.optimize(objective, n_trials=10)","9762eea8":"# print(\"Number of finished trials: {}\".format(len(xgb_study.trials)))\n\n# print(\"Best trial:\")\n# xgb_trial = xgb_study.best_trial\n\n# print(\"  Value: {}\".format(xgb_trial.value))\n\n# print(\"  Params: \")\n# for key, value in xgb_trial.params.items():\n#     print(\"    {}: {}\".format(key, value))\n    \n# print(\"  Number of estimators: {}\".format(xgb_trial.user_attrs[\"n_estimators\"]))","19ed8124":"# display(optuna.visualization.plot_optimization_history(xgb_study))\n# display(optuna.visualization.plot_param_importances(xgb_study))","cd4db4b7":"xgb_params = {**xgb_params}#, **xgb_trial.params}\n\nxgb_params['n_estimators'] = 500 #xgb_trial.user_attrs[\"n_estimators\"] * 1.1\nxgb_params['learning_rate'] = xgb_params.pop('eta')\n\nxgbm = xgb.XGBClassifier(**xgb_params, use_label_encoder=False)\nxgbm.fit(X_proc, y)\ny_pred = xgbm.predict_proba(X_test_proc)[:, 1]","5da88717":"submit(y_pred, 'xgbm')","76ed7702":"lgb_params = {\n    \"objective\":\"binary\",\n    \"metric\": \"auc\",\n    \"verbose\":-1,\n    \"random_state\": random_state,\n}\n\n# lgb_params[\"num_threads\"] = 4\nlgb_params = {\n    \"device\":'gpu',\n    'gpu_platform_id': 0,\n    'gpu_device_id': 0,\n    #\"gpu_use_dp\":False,\n    #\"max_bin\":63,\n**lgb_params}","455fa21e":"# dtrain = lgb.Dataset(X_proc, y)\n\n# eval_hist = {}\n# for learning_rate in np.geomspace(1e-2, 1e-1, 3):\n#     eval_hist[learning_rate] = lgb.cv(\n#         params={\"learning_rate\":learning_rate, **lgb_params},\n#         train_set=dtrain,\n#         num_boost_round=1_000,\n#         early_stopping_rounds=100,\n#         #folds=folds,\n#         nfold=5,\n#         stratified=True,\n#         shuffle=True,\n#         metrics='auc',\n#         verbose_eval=False,\n#     )","969b1b0f":"# r_eval_hist = {}\n# for eta, d in eval_hist.items():\n#     r_eval_hist[eta] = pd.Series(d['auc-mean'])\n    \n# r_eval_hist = pd.DataFrame(r_eval_hist)\n# r_eval_hist.plot()","7c870b9b":"# pd.concat([r_eval_hist.idxmax().rename('num_boost'),\n#            r_eval_hist.max().rename('max_test_score')], axis=1)","b643c5ae":"lgb_params['learning_rate'] = 0.03","65199743":"# import optuna\n# from optuna.integration import lightgbm as lgb_opt\n\n# dtrain = lgb_opt.Dataset(X_proc, y)\n\n# study = optuna.create_study(direction='maximize', sampler=sampler)\n# lgb_tuner = lgb_opt.LightGBMTunerCV(\n#     params=lgb_params,\n#     train_set=dtrain,\n#     #folds=folds,\n#     nfold=5,\n#     stratified=True,\n#     shuffle=True,\n#     #feature_name='auto',\n#     #categorical_feature=columns_proc_category,\n#     return_cvbooster=True,\n#     verbose_eval=-1,\n#     show_progress_bar=False,\n#     optuna_seed=random_state,\n#     early_stopping_rounds=100,\n#     num_boost_round=1_000,\n#     study=study,\n#     time_budget=3600*5\n# )\n# lgb_tuner.run()","bb688e1a":"# optuna.visualization.plot_optimization_history(study)","251df802":"# print(\"Best score:\", lgb_tuner.best_score)\n# best_params = lgb_tuner.best_params\n# print(\"Best params:\", best_params)\n# print(\"  Params: \")\n# for key, value in best_params.items():\n#     print(\"    {}: {}\".format(key, value))","b4fbb544":"# best_params = {\n#     'device': \"gpu\",\n#     'gpu_platform_id': 0,\n#     'gpu_device_id': 0,\n#     'objective': \"binary\",\n#     'metric': \"auc\",\n#     'verbose': -1,\n#     'random_state': 42,\n#     'learning_rate': 0.1,\n#     'feature_pre_filter': False,\n#     'lambda_l1': 7.747577106473336,\n#     'lambda_l2': 3.404677878190547e-05,\n#     'num_leaves': 7,\n#     'feature_fraction': 1.0,\n#     'bagging_fraction': 0.8391963650875751,\n#     'bagging_freq': 5,\n#     'min_child_samples': 100,\n# }","a995e693":"best_params = {\n    'device': \"gpu\",\n    'gpu_platform_id': 0,\n    'gpu_device_id': 0,\n    'objective': \"binary\",\n    'metric': \"auc\",\n    'verbose': -1,\n    'random_state': 42,\n    'learning_rate': 0.03,\n    'feature_pre_filter': False,\n    'lambda_l1': 7.160696576959302,\n    'lambda_l2': 0.05505687419638068,\n    'num_leaves': 41,\n    'feature_fraction': 0.984,\n    'bagging_fraction': 0.8994655844810855,\n    'bagging_freq': 2,\n    'min_child_samples': 20, # default\n}","44303e89":"lgb_params = {**best_params, \"n_estimators\":1000}\nlgbm = lgb.LGBMClassifier(**lgb_params).fit(X_proc, y)\ny_pred = lgbm.predict_proba(X_test_proc)[:, 1]","4dca2c0b":"submit(y_pred, 'lgbm')","98a92888":"model_dict = {\n    'lgb': lgbm,\n    'xgb': xgbm,\n}","c2ad4169":"from sklearn.model_selection import cross_val_predict\n\nX_out_all = X_proc.copy()\nX_out_test_all = X_test_proc.copy()\nX_out, X_out_test = pd.DataFrame(), pd.DataFrame()\n\nfinal_score = {}\n\nfor name, model in model_dict.items():\n    method = 'predict_proba'\n    s_train = cross_val_predict(model, X_proc, y,\n                                n_jobs=1, cv=5,\n                                method=method)\n    s_train = s_train[:, 1]\n    s_test = model.predict_proba(X_test_proc)[:, 1]\n    \n    final_score[name] = roc_auc_score(y, s_train)\n    print(name, final_score[name])\n    \n    X_out[f'out_{name}'] = pd.Series(s_train, X_proc.index)\n    X_out_test[f'out_{name}'] = pd.Series(s_test, X_test_proc.index)\n    \nX_out_all = X_out_all.join(X_out)\nX_out_test_all = X_out_test_all.join(X_out_test)","0d605fca":"y_mean = X_out.mean(axis=1)\ny_mean_test = X_out_test.mean(axis=1)\n\nprint(\"final score with mean:\", roc_auc_score(y, y_mean))\nsubmit(y_mean_test, 'final_mean')","eb758db2":"final_estimator = LogisticRegression(penalty='none').fit(X_out, y)\n\ny_pred = final_estimator.predict_proba(X_out)[:, 1]\ny_pred_test = final_estimator.predict_proba(X_out_test)[:, 1]\n\nprint(\"final 'training' score on training data:\", roc_auc_score(y, y_pred))\nsubmit(y_pred_test, 'final')","af8a664e":"r = cross_val_score(final_estimator, X_out, y, cv=5, scoring='roc_auc')\n\nprint(\"final score in old-out\/CV:\", r.mean())","376b783c":"y_pred = pd.Series(y_pred, index=y.index, name='predictions')","0baaffa8":"from sklearn.metrics import roc_curve, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay\n\n# fig, axs = plt.subplots(1, 2, figsize=(12,, 12))\n\ncm = confusion_matrix(y, y_pred > 0.5)\ncm_display = ConfusionMatrixDisplay(cm).plot()\n\nfpr, tpr, _ = roc_curve(y, y_pred > 0.5)\nroc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()","ec8769b0":"# Modeling","9c7e5a8f":"# Output visualization","8099073f":"```\nnum_boost\tmax_test_score\n0.010000\t999\t0.813118\n0.031623\t963\t0.814037\n0.100000\t201\t0.813316\n```","4773f8b4":"Manual finish, the study could not run until the end, see version 3 of this notebook","d0e7f4e2":"# Imports","d148ab4f":"> Your notebook tried to allocate more memory than is available. It has restarted.","b57220cb":"### Hyperparameter Opt. part 2 with Optuna","72aa671c":"# Data Collection","ef02b055":"### Hyperparameter Opt. part 1","2892b159":"# Stacking","93942e58":"At first I filled the NaN with IterativeImputer and Ridge but it reduce the score. The pattern is in the NaN and XGB and LGB can use the NaN values to improve their scores. So now I let the NaN values in X","1554d883":"I predict y from each model by hold-out like [sklearn.ensemble.StackingRegressor](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.StackingRegressor.html) for proper Stacking (and avoid overfitting)\n\nI dot not use it because I want to optimize the `final_estimator` and need the data to feed it","22aea243":"Our final leaderboard score should be close to the value below:","5006d8ae":"## Folds","3970eec8":"```\n\t\tnum_boost\tmin_test_score\tmin_train_score\n0.010000\t3875\t0.814530\t0.870770\n0.031623\t1370\t0.814381\t0.875110\n0.100000\t360\t0.813164\t0.865133\n0.316228\t75\t0.809961\t0.844073\n1.000000\t7\t0.805810\t0.816480\n```","345866fb":"## LightGBM Hyperparameter Opt.","6eacc4af":"## XGB Hyperparameter Opt.\n\nallocate too much memory (?)"}}