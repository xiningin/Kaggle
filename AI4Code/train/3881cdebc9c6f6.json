{"cell_type":{"864fa1da":"code","b4030096":"code","2bf468a6":"code","fcb0a7b8":"code","71271777":"code","f9a88564":"code","d0606ae2":"code","638e2360":"code","46651286":"code","4d1b3e46":"code","061bd536":"code","e92489ec":"code","2236bf1b":"code","063e596d":"code","dd853ea8":"code","c409ed29":"code","92238dc9":"code","d079d239":"code","482abcf4":"code","686aca78":"code","684895da":"code","f756569b":"code","3dd8816e":"code","0f7b5df3":"code","6e54e2a9":"code","aa71949b":"code","a1da857f":"code","0da90276":"code","201d985e":"code","d6d58345":"code","02dfc15f":"markdown","2f4a9b7a":"markdown","9bc47985":"markdown","5ea892b5":"markdown","d1c0d1ad":"markdown","a94b9693":"markdown","92dd1f6b":"markdown","4edf70b0":"markdown","2db9c885":"markdown","13f0597a":"markdown","1fd7a24b":"markdown","913be562":"markdown","f2f24f7a":"markdown","7a142daa":"markdown","860d3416":"markdown","eae643cc":"markdown","484b8b01":"markdown"},"source":{"864fa1da":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b4030096":"data=pd.read_csv(\"\/kaggle\/input\/star-dataset\/6 class csv.csv\")\ndata.head()","2bf468a6":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport math as m","fcb0a7b8":"data.describe()\n#Strange results for Luminosity and Radius: The quartiles have a sudden change in the order of magnitude","71271777":"data.info()","f9a88564":"sns.pairplot(data)","d0606ae2":"#checking out the relation of Log(T) with all variables\ntemp_log=data[\"Temperature (K)\"]\ntemp_log=temp_log.apply(m.log10)\ndata2=data\ndata2[\"Temperature (K)\"]=temp_log\nsns.pairplot(data2)","638e2360":"#Rescaling Luminosity and Radius\nrad_log=data[\"Radius(R\/Ro)\"]\nlum_log=data[\"Luminosity(L\/Lo)\"]\nrad_log=rad_log.apply(m.log10)\nlum_log=lum_log.apply(m.log10)\ndata4=data\ndata4[\"Radius(R\/Ro)\"]=rad_log\ndata4[\"Luminosity(L\/Lo)\"]=lum_log\nsns.pairplot(data4)\n","46651286":"data4[\"log(L\/Lo)\"]=data4[\"Luminosity(L\/Lo)\"]\ndata4[\"log(R\/Ro)\"]=data4[\"Radius(R\/Ro)\"]\ndata4.drop([\"Luminosity(L\/Lo)\",\"Radius(R\/Ro)\"], axis=1, inplace=True)\ndata4.describe()\n#Luminosity and Radius look better know","4d1b3e46":"#from now on the datax dataset is refering to the data that has model x for spectral class\ndata1=data4.drop([\"Star color\",\"Spectral Class\"], axis=1)\ndata2=data4.drop(\"Star color\", axis=1)","061bd536":"#Checking all categories in both features\ndata4[\"Spectral Class\"].unique()","e92489ec":"data4[\"Star color\"].unique()","2236bf1b":"#For data2 I'll use the following dictionary\nsp_class={\"O\":0,\"B\":1,\"A\":2,\"F\":3,\"G\":4,\"K\":5,\"M\":6}\ndata2[\"Spectral Class\"]=data2[\"Spectral Class\"].map(sp_class)","063e596d":"#making dummies for light color and for spectral class\ndumm_light=pd.get_dummies(data4[\"Star color\"], drop_first=True)\ndumm_class=pd.get_dummies(data4[\"Spectral Class\"], drop_first=True)","dd853ea8":"data1=data1.join(dumm_light)\ndata1=data1.join(dumm_class)\ndata2=data2.join(dumm_light)","c409ed29":"#checking datasets\ndata1.head()","92238dc9":"data2.head()","d079d239":"#sklearn imports\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split","482abcf4":"#first organize the test train splits\nX1=data1.drop(\"Star type\", axis=1)\ny1=data1[\"Star type\"]\nX2=data2.drop(\"Star type\", axis=1)\ny2=data2[\"Star type\"]\nX1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.3)\nX2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.3)","686aca78":"lr1=LogisticRegression(max_iter=10000)\nlr1.fit(X1_train,y1_train)\nresult_lr1=lr1.predict(X1_test)\nprint(confusion_matrix(y1_test,result_lr1))\nprint(\"\\n\")\nprint(classification_report(y1_test,result_lr1))","684895da":"lr2=LogisticRegression(max_iter=10000)\nlr2.fit(X2_train,y2_train)\nresult_lr2=lr2.predict(X2_test)\nprint(confusion_matrix(y2_test,result_lr2))\nprint(\"\\n\")\nprint(classification_report(y2_test,result_lr2))","f756569b":"lda1=LinearDiscriminantAnalysis()\nlda1.fit_transform(X1_train,y1_train)\nresult_lda1=lda1.predict(X1_test)\nprint(confusion_matrix(y1_test,result_lda1))\nprint(\"\\n\")\nprint(classification_report(y1_test,result_lda1))","3dd8816e":"lda2=LinearDiscriminantAnalysis()\nlda2.fit_transform(X2_train,y2_train)\nresult_lda2=lda2.predict(X2_test)\nprint(confusion_matrix(y2_test,result_lda2))\nprint(\"\\n\")\nprint(classification_report(y2_test,result_lda2))","0f7b5df3":"#Decision of K value\nimport numpy as np\nerror_rate1=[]\nerror_rate2=[]\nfor i in range(1,50):\n    knn1=KNeighborsClassifier(n_neighbors=i)\n    knn2=KNeighborsClassifier(n_neighbors=i)\n    knn1.fit(X1_train,y1_train)\n    knn2.fit(X2_train,y2_train)\n    res_knn1_i=knn1.predict(X1_test)\n    res_knn2_i=knn2.predict(X2_test)\n    error_rate1.append(np.mean(res_knn1_i!=y1_test))\n    error_rate2.append(np.mean(res_knn2_i!=y2_test))","6e54e2a9":"n_values=range(1,50)\nplt.fig_size=(12,20)\nplt.plot(n_values,error_rate1)\nplt.ylabel(\"Error rate\")\nplt.xlabel(\"K value\")\nneighbors1=error_rate1.index(min(error_rate1))+1","aa71949b":"n_values=range(1,50)\nplt.plot(n_values,error_rate2)\nplt.ylabel(\"Error rate\")\nplt.xlabel(\"K value\")\nneighbors2=error_rate2.index(min(error_rate2))+1","a1da857f":"knn1=KNeighborsClassifier(n_neighbors=neighbors1)\nknn1.fit(X1_train,y1_train)\nresult_knn1=knn1.predict(X1_test)\nprint(confusion_matrix(y1_test,result_knn1))\nprint(\"\\n\")\nprint(classification_report(y1_test,result_knn1))","0da90276":"knn2=KNeighborsClassifier(n_neighbors=neighbors2)\nknn2.fit(X2_train,y2_train)\nresult_knn2=knn2.predict(X2_test)\nprint(confusion_matrix(y2_test,result_knn2))\nprint(\"\\n\")\nprint(classification_report(y2_test,result_knn2))","201d985e":"rf1=RandomForestClassifier(n_estimators=300)\nrf1.fit(X1_train,y1_train)\nresult_rf1=rf1.predict(X1_test)\nprint(confusion_matrix(y1_test,result_rf1))\nprint(\"\\n\")\nprint(classification_report(y1_test,result_rf1))","d6d58345":"rf2=RandomForestClassifier(n_estimators=300)\nrf2.fit(X2_train,y2_train)\nresult_rf2=rf2.predict(X2_test)\nprint(confusion_matrix(y2_test,result_rf2))\nprint(\"\\n\")\nprint(classification_report(y2_test,result_rf2))","02dfc15f":"# Starting with some metrics about the dataset","2f4a9b7a":"# Model 1: Logistic Regression","9bc47985":"# Model 2: Linear Discriminant Analysis","5ea892b5":"# Changing the scale made the data look a lot better afterall, especially when looking the new relation of both parameters with the Star Type variable.\nWith this in mind, the model fitting will be done using the data4 dataframe","d1c0d1ad":"# Second Step: Regularized Variables\nIn physics, it is very common for regularized variables to have logarithmic realtions with other parameters. A very know example is the equation for sound intensity:\nI=10log(I\/I_0) (I given in decibels)\n\nIt may seem a long shot to make this relationship in this dataset based on this, but two arguments hold this hypothesis:\n* Looking at the wide range of values both variables assume: As mentioned earlier, the high concentration of values around 0 seemed strange, and a exponential rework could improve the looks of those distributions;\n* If you look into the dataset description, you can see the in the last figure: Luminosity has a logarithmic scale. This alone supports the hypothesis, but the first point is what is usaully observable when dealing with data from a field of study that you don't have much affinity.\n\nWe will now check if these assumptions can improve subsequent model fitting","a94b9693":"# As this is my very first attempt of model selection and fitting in Kaggle, I will be using different models and cross validation to select the best performing model","92dd1f6b":"# Thanks for reading my kernel!","4edf70b0":"# Model 3: K Nearest Neighbors","2db9c885":"# Model selection allowed us to see many important facts about the data:\n\n* The results given show that the previous assumption of correlation between Temperature and Absolute Magnitude is probably not true, given that overall all models performed very well on the dataset\n* Usually not very well suited for more complex datasets, the logistic regression model has performed very well on this data, probably resulting from the logarithmic rescaling of Luminosity and Radius. (I chose not to explore this possibility, if anyone would like to try I would appreciate the feedback, although I strongly believe that the values will exceed Python's mathematical limit due to the size of the parameters)\n* KNN model has the worst performance from all models studied, which is expected as a unsupervised learning model is applied onto a supervised dataset\n* The cardinal relation on Spectral Class has improved model performance independent of the model chosen, showing that the relation indicated on the Hertzsprung-Russel Diagram had a very positive effect on the data. A relation could have been built for light but the subjectiveness and need to select one value of frequency for a spectrum has made this task difficult and prone to error\n* The linear models had some trouble in Star Types 3 and 4 probably because of overlapping values in some parameters as Temperature and Luminosity. If these models were used, extra care should be taken when dealing with these types\n* The random forest 2 classifier outperformed all models studied, but when thinking of easier scientific interpretation of the model, the logistic regression 2 model could be implemented without significant loss of performance compared to the huge interpretational gain","13f0597a":"# Two observations can be derived from the pairplot:\n* There is a possible relation between Temperature and Absolute Magnitude.\n* Both regularized parameters (Luminosity and Radius) have a lot of observations concentrated around 0","1fd7a24b":"# First Step: Searching for correlation between Temperature and Absolute Magnitude","913be562":"# Model 4: Random Forest","f2f24f7a":"Up to this point all numeric parameters have been treated, but the categorical features (Star Color and Spectral Class) have not. Both of them have a cardinal relation: Light color is related to light frequency and Spectral Class has an order according to the Hertzsprung-Russel Diagram given in the dataset description.\n\n* I'm not sure the relation between a scale difference in the Spectral Class can be accurately described by a unitary difference (i.e. making O=0, B=1...), but given the lack of supplemental info it's the approach I'm going to take\n* The light color can be associated with its frequency, but since the color descriptions are not about isolated lights but the percieved light color, this approach becomes unfeasible\n\nKeeping this in mind, I've decided to make 2 models for class: One with dummy variables (model 1) and another with their respective ordinal relations (model 2). The light color feature will be treated only as dummy","7a142daa":"In both cases there is a relationship that can be seen but given the distributions no conclusion can be drawn. Given the wide range of possible mathematical relations between these variables, when building and assessing model performance an attempt to check this correlation will be made by adding and removing temperature from the estimators.","860d3416":"# Obs: On the many runs through this notebook I have had a few splits that gave 1.00 score to many of the metrics in the models, and so I chose to reroll the train_test_split cell in order to remove this possible overfit from the result analysis. Only rf2 kept scoring 1.00 in all scenarios","eae643cc":"# We can now proceed to model comparison","484b8b01":"# Welcome to my first kernel! \n# I hope the workflow is detailed enough for anyone to understand and I hope you can learn along with my discoveries on this dataset. Any feedback is greatly appreciated!"}}