{"cell_type":{"83489fd3":"code","e0d1dae6":"code","e7093083":"code","93133433":"code","c2e770df":"code","46a9a2f0":"code","bcf92b99":"code","44ab26f0":"code","55a4e85e":"code","f10ac081":"code","e6c39698":"code","03fe4175":"code","9f19a3b2":"code","a5e41447":"code","d6928351":"code","5c213e37":"code","b7d21fd6":"code","1cab9317":"code","dccc08e0":"code","7271428e":"code","6652e78e":"code","f427f1ad":"code","f3eeec08":"code","c0289e57":"code","9bdb5f45":"code","e248515b":"code","3ceb2a38":"code","36fb1a21":"code","812bb834":"code","054e40e5":"code","263c58b9":"code","14322dca":"code","a1b4115b":"code","6b2da40e":"code","93158a15":"code","e42b74a8":"code","c429db78":"code","0fe7e489":"code","e4d7465d":"code","527b5244":"code","46b16e5d":"code","29a45066":"code","cf1c9196":"code","0bf069cb":"code","9dbb3abf":"code","c5801c12":"code","a43c4fba":"code","84378bff":"code","641a1e29":"code","bd0e443c":"code","9b973596":"code","41beb1ac":"code","b34caf3c":"code","ce4a7a2c":"code","46fbb4a7":"code","8006604a":"code","0f7739b3":"code","36871598":"code","127947d1":"code","a45beb47":"code","a59d112a":"code","959b3518":"code","4e4d9884":"code","b6e39b49":"code","96a3e6f5":"code","a275d97b":"code","dd9fee5b":"code","34428c5a":"code","afb579d9":"code","40968c8d":"code","a20c3a22":"code","bd612f05":"code","a0d17658":"code","8d032856":"code","a98cbaa8":"code","8f15109a":"code","3ed567d9":"code","7910faf3":"code","04555b52":"code","1c5d2a9f":"code","6a3b876b":"code","77aa4fb9":"code","f4bdaf77":"code","ab54fd65":"code","fa85959b":"code","5c0aef44":"code","0b6a768a":"code","8246eec7":"code","eb2ec64c":"code","f05ad300":"code","81c0517e":"code","1716c65f":"code","85ea583a":"code","678199a5":"code","765d00dd":"code","386f1c73":"code","128da2fb":"code","24214bc9":"code","a1889abd":"markdown","db67858f":"markdown","bf0feb87":"markdown","d206962d":"markdown","686734a0":"markdown","38c261d5":"markdown","be4c3e9d":"markdown","95dd75d4":"markdown","366ec62e":"markdown","9971d3b8":"markdown","1793676d":"markdown","e6d4068a":"markdown","1cb77759":"markdown","f5edcf24":"markdown","e74d4746":"markdown","64195b63":"markdown","5965ca50":"markdown","27b76839":"markdown","b616a03c":"markdown","e702688b":"markdown","607a8ad1":"markdown","8fccdcfa":"markdown","151d92a7":"markdown","66ce503f":"markdown","df1a3c6b":"markdown","55cc331e":"markdown","52826e07":"markdown","fb9add40":"markdown","24b1d9ff":"markdown","d0146050":"markdown","ad8db101":"markdown","a67e6c8f":"markdown","048a4d57":"markdown","9e4ded44":"markdown","25832f13":"markdown","82211295":"markdown","9058924c":"markdown","5583af3b":"markdown","a9013962":"markdown","f9e0a683":"markdown","b80e8f44":"markdown","756372f6":"markdown","9ff41fb5":"markdown","bf6f3fd8":"markdown","77ac450a":"markdown","90721e94":"markdown","2e516637":"markdown","55ae9148":"markdown","4bb622a3":"markdown","14cf2539":"markdown","61f63437":"markdown","cc6678a2":"markdown","9f114e4f":"markdown","42b300b1":"markdown","1b902ae1":"markdown","f068f925":"markdown","575bcbac":"markdown","1c899739":"markdown","e334e313":"markdown"},"source":{"83489fd3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e0d1dae6":"import matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\nimport gc\nimport lightgbm as lgb\nimport time\n# import datetime\n# import xgboost as xgb\n# import time\n# import itertools\n# from sklearn.linear_model import LinearRegression\n# from sklearn.model_selection import train_test_split\n# from sklearn.preprocessing import OneHotEncoder\n# from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\n\n%matplotlib inline\nsns.set()","e7093083":"INPUT_DIR = '\/kaggle\/input\/m5-forecasting-accuracy'\n\ncalendar_df = pd.read_csv(f\"{INPUT_DIR}\/calendar.csv\")\nsell_prices_df = pd.read_csv(f\"{INPUT_DIR}\/sell_prices.csv\")\nsales_train_validation_df = pd.read_csv(f\"{INPUT_DIR}\/sales_train_validation.csv\")\nsample_submission_df = pd.read_csv(f\"{INPUT_DIR}\/sample_submission.csv\")","93133433":"# Calendar data type cast -> Memory Usage Reduction\ncalendar_df[[\"month\", \"snap_CA\", \"snap_TX\", \"snap_WI\", \"wday\"]] = calendar_df[[\"month\", \"snap_CA\", \"snap_TX\", \"snap_WI\", \"wday\"]].astype(\"int8\")\ncalendar_df[[\"wm_yr_wk\", \"year\"]] = calendar_df[[\"wm_yr_wk\", \"year\"]].astype(\"int16\") \ncalendar_df[\"date\"] = calendar_df[\"date\"].astype(\"datetime64\")\n\nnan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\nfor feature in nan_features:\n    calendar_df[feature].fillna('unknown', inplace = True)\n\ncalendar_df[[\"weekday\", \"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]] = calendar_df[[\"weekday\", \"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]] .astype(\"category\")","c2e770df":"calendar_df.head()","46a9a2f0":"sales_train_validation_df.head()","bcf92b99":"# Sales Training dataset cast -> Memory Usage Reduction\nsales_train_validation_df.loc[:, \"d_1\":] = sales_train_validation_df.loc[:, \"d_1\":].astype(\"int16\")","44ab26f0":"# Make ID column to sell_price dataframe\nsell_prices_df.loc[:, \"id\"] = sell_prices_df.loc[:, \"item_id\"] + \"_\" + sell_prices_df.loc[:, \"store_id\"] + \"_validation\"","55a4e85e":"sell_prices_df = pd.concat([sell_prices_df, sell_prices_df[\"item_id\"].str.split(\"_\", expand=True)], axis=1)\nsell_prices_df = sell_prices_df.rename(columns={0:\"cat_id\", 1:\"dept_id\"})\nsell_prices_df[[\"store_id\", \"item_id\", \"cat_id\", \"dept_id\"]] = sell_prices_df[[\"store_id\",\"item_id\", \"cat_id\", \"dept_id\"]].astype(\"category\")\nsell_prices_df = sell_prices_df.drop(columns=2)","f10ac081":"def make_dataframe():\n    # Wide format dataset \n    df_wide_train = sales_train_validation_df.drop(columns=[\"item_id\", \"dept_id\", \"cat_id\", \"state_id\",\"store_id\", \"id\"]).T\n    df_wide_train.index = calendar_df[\"date\"][:1913]\n    df_wide_train.columns = sales_train_validation_df[\"id\"]\n    \n    # Making test label dataset\n    df_wide_test = pd.DataFrame(np.zeros(shape=(56, len(df_wide_train.columns))), index=calendar_df.date[1913:], columns=df_wide_train.columns)\n    df_wide = pd.concat([df_wide_train, df_wide_test])\n\n    # Convert wide format to long format\n    df_long = df_wide.stack().reset_index(1)\n    df_long.columns = [\"id\", \"value\"]\n\n    del df_wide_train, df_wide_test, df_wide\n    gc.collect()\n    \n    df = pd.merge(pd.merge(df_long.reset_index(), calendar_df, on=\"date\"), sell_prices_df, on=[\"id\", \"wm_yr_wk\"])\n    df = df.drop(columns=[\"d\"])\n#     df[[\"cat_id\", \"store_id\", \"item_id\", \"id\", \"dept_id\"]] = df[[\"cat_id\"\", store_id\", \"item_id\", \"id\", \"dept_id\"]].astype(\"category\")\n    df[\"sell_price\"] = df[\"sell_price\"].astype(\"float16\")   \n    df[\"value\"] = df[\"value\"].astype(\"int32\")\n    df[\"state_id\"] = df[\"store_id\"].str[:2].astype(\"category\")\n\n\n    del df_long\n    gc.collect()\n\n    return df\n\ndf = make_dataframe()","e6c39698":"df.dtypes","03fe4175":"def add_date_feature(df):\n    df[\"year\"] = df[\"date\"].dt.year.astype(\"int16\")\n    df[\"month\"] = df[\"date\"].dt.month.astype(\"int8\")\n    df[\"week\"] = df[\"date\"].dt.week.astype(\"int8\")\n    df[\"day\"] = df[\"date\"].dt.day.astype(\"int8\")\n    df[\"quarter\"]  = df[\"date\"].dt.quarter.astype(\"int8\")\n    return df","9f19a3b2":"df = add_date_feature(df)\ndf.head()","a5e41447":"temp_series = df.groupby([\"cat_id\", \"date\"])[\"value\"].sum()\ntemp_series","d6928351":"plt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"FOODS\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"FOODS\"].values, label=\"FOODS\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].values, label=\"HOUSEHOLD\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].values, label=\"HOBBIES\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"# of sold items\")\nplt.title(\"Total Item Sold Transition of each Category\")\nplt.legend()\n","5c213e37":"temp_series = temp_series.loc[temp_series.index.get_level_values(\"date\") >= \"2015-01-01\"]\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"FOODS\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"FOODS\"].values, label=\"FOODS\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].values, label=\"HOUSEHOLD\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].values, label=\"HOBBIES\")\nplt.xlabel(\"Year-Month\")\nplt.ylabel(\"# of sold items\")\nplt.title(\"Total Item Sold Transition of each Category from 2015\")\nplt.legend()","b7d21fd6":"# Plot only December, 2015\ntemp_series = temp_series.loc[(temp_series.index.get_level_values(\"date\") >= \"2015-12-01\") & (temp_series.index.get_level_values(\"date\") <= \"2015-12-31\")]\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"FOODS\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"FOODS\"].values, label=\"FOODS\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].values, label=\"HOUSEHOLD\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].values, label=\"HOBBIES\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"# of sold items\")\nplt.title(\"Total sold item per day in December, 2015\")\nplt.legend()","1cab9317":"temp_series.loc[(temp_series.index.get_level_values(\"date\") >= \"2015-12-24\") & (temp_series.index.get_level_values(\"date\") <= \"2015-12-26\")]","dccc08e0":"temp_series = df.groupby([\"cat_id\", \"wday\"])[\"value\"].sum()\ntemp_series","7271428e":"plt.figure(figsize=(6, 4))\nleft = np.arange(1,8) \nwidth = 0.3\nweeklabel = [\"Saturday\", \"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"]    # Please Confirm df\n\n\nplt.bar(left, temp_series[temp_series.index.get_level_values(\"cat_id\") == \"FOODS\"].values, width=width, label=\"FOODS\")\nplt.bar(left + width, temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].values, width=width, label=\"HOUSEHOLD\")\nplt.bar(left + width + width, temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].values, width=width, label=\"HOBBIES\")\nplt.legend(bbox_to_anchor=(1.01, 1.01))\nplt.xticks(left, weeklabel, rotation=60)\nplt.xlabel(\"day of week\")\nplt.ylabel(\"# of sold items\")\nplt.title(\"Total sold item in each daytype\")","6652e78e":"temp_series = df.groupby([\"state_id\", \"date\"])[\"value\"].sum()\ntemp_series","f427f1ad":"plt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"state_id\") == \"CA\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"state_id\") == \"CA\"].values, label=\"CA\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"state_id\") == \"TX\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"state_id\") == \"TX\"].values, label=\"TX\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"state_id\") == \"WI\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"state_id\") == \"WI\"].values, label=\"WI\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"# of sold items\")\nplt.title(\"Total Item Sold Transition of each State\")\nplt.legend()","f3eeec08":"temp_series = df.groupby([\"store_id\", \"date\"])[\"value\"].sum()\n\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_1\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_1\"].values, label=\"CA_1\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_2\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_2\"].values, label=\"CA_2\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_3\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_3\"].values, label=\"CA_3\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_4\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_4\"].values, label=\"CA_4\")\n\nplt.xlabel(\"Year\")\nplt.ylabel(\"# of sold items\")\nplt.title(\"Total Item Sold Transition of each Store in CA\")\nplt.legend()","c0289e57":"temp_series = df.groupby([\"store_id\", \"date\"])[\"item_id\"].count()\n\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_1\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_1\"].values, label=\"CA_1\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_2\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_2\"].values, label=\"CA_2\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_3\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_3\"].values, label=\"CA_3\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_4\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_4\"].values, label=\"CA_4\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"# of item entries\")\nplt.title(\"Total item entries in each CA stores\")\nplt.legend()","9bdb5f45":"temp_series = df.groupby([\"store_id\", \"date\"])[\"value\"].std()\ntemp_series","e248515b":"plt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_1\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_1\"].values, label=\"CA_1\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_2\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_2\"].values, label=\"CA_2\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_3\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_3\"].values, label=\"CA_3\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_4\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_4\"].values, label=\"CA_4\")\n\nplt.xlabel(\"Year\")\nplt.ylabel(\"Standard deviation of sold items\")\nplt.title(\"Standard deviation of sold items in CA stores\")\nplt.legend()","3ceb2a38":"temp_series = df.groupby([\"store_id\", \"date\"])[\"value\"].sum()\n\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_1\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_1\"].values, label=\"WI_1\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_2\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_2\"].values, label=\"WI_2\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_3\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_3\"].values, label=\"WI_3\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"# of sold items\")\nplt.title(\"Total item sold in each WI stores\")\nplt.legend()","36fb1a21":"temp_series = df.groupby([\"store_id\", \"date\"])[\"item_id\"].count()\n\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_1\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_1\"].values, label=\"WI_1\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_2\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_2\"].values, label=\"WI_2\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_3\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_3\"].values, label=\"WI_3\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"# of item entries\")\nplt.title(\"Total item entries in each WI stores\")\nplt.legend()","812bb834":"temp_series = df.groupby([\"store_id\", \"date\"])[\"value\"].sum()\n\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_1\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_1\"].values, label=\"TX_1\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_2\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_2\"].values, label=\"TX_2\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_3\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_3\"].values, label=\"TX_3\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Total sold item per day\")\nplt.title(\"Total item sold in each TX stores\")\nplt.legend()","054e40e5":"temp_series = df.groupby([\"store_id\", \"date\"])[\"item_id\"].count()\n\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_1\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_1\"].values, label=\"TX_1\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_2\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_2\"].values, label=\"TX_2\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_3\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_3\"].values, label=\"TX_3\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Total item entries\")\nplt.title(\"Total item entries in each TX stores\")\nplt.legend()","263c58b9":"temp_series = df.groupby([\"store_id\", \"date\"])[\"value\"].sum()\ntemp_series","14322dca":"# Find the day when items are sold less than 1000 of each store\n# Let's take a look at TX_2 for example\ntemp_series.loc[(temp_series.values < 1000) & (temp_series.index.get_level_values(\"date\") <= \"2016-04-22\")].loc[\"TX_2\"]","a1b4115b":"# Find the day when items are sold most of each store\ntemp_series.groupby([\"store_id\"]).idxmax()","6b2da40e":"temp_series = temp_series.reset_index()\ntemp_series","93158a15":"plt.plot(temp_series[(temp_series[\"store_id\"] == \"CA_1\") & ((temp_series[\"date\"] >= \"2013-07-15\") & (temp_series[\"date\"] <= \"2013-10-15\"))][\"date\"],\n         temp_series[(temp_series[\"store_id\"] == \"CA_1\") & ((temp_series[\"date\"] >= \"2013-07-15\") & (temp_series[\"date\"] <= \"2013-10-15\"))][\"value\"])\nplt.xticks(rotation=60)\nplt.ylabel(\"# of sold items\")\nplt.xlabel(\"date\")\nplt.title(\"Item sold transition around its most sold day in CA_1 store\")","e42b74a8":"# import statsmodels.api as sm","c429db78":"# item_id = \"HOBBIES_1_008\"\n# temp_df = df.loc[df.item_id == item_id, [\"date\",\"store_id\", \"value\"]]","0fe7e489":"# store_list = [\"CA_1\", \"CA_2\", \"CA_3\", \"CA_4\", \"TX_1\", \"TX_2\", \"TX_3\", \"WI_1\", \"WI_2\"]\n\n# # temp_df.drop(columns=\"sell_price\", inplace=True)\n# temp_df_wide = pd.pivot_table(temp_df, index='date', columns='store_id', values=\"value\")\n# temp_df_wide.plot(figsize=(12, 4))\n# plt.legend(bbox_to_anchor=(1.01, 1.01))","e4d7465d":"# diff_cols = [\"diff_\" + store for store in store_list]\n\n# for store in store_list:\n#     col = \"diff_\" + store\n#     temp_df_wide.columns = temp_df_wide.columns.add_categories(col)\n#     temp_df_wide[col] = np.log(temp_df_wide[store] + 0.1).diff() * 100\n    \n#     std_col = \"std_\" + col\n    \n#     temp_df_wide.columns = temp_df_wide.columns.add_categories(std_col)\n#     temp_df_wide[std_col] = (temp_df_wide[col] - temp_df_wide[col].mean()) \/ temp_df_wide[col].std()\n","527b5244":"# std_cols = [\"std_diff_\" + store for store in store_list]","46b16e5d":"# endog = temp_df_wide.loc[:, std_cols]\n\n# # Create the model\n# mod = sm.tsa.DynamicFactor(endog, k_factors=1, factor_order=3, error_order=3)\n# initial_res = mod.fit(method='powell', disp=False)\n# res = mod.fit(initial_res.params, disp=False)","29a45066":"# print(res.summary(separate_params=False))","cf1c9196":"# from pandas_datareader.data import DataReader\n\n# fig, ax = plt.subplots(figsize=(13,3))\n\n# # Plot the factor\n# dates = endog.index._mpl_repr()\n# ax.plot(dates, res.factors.filtered[0], label='Factor')\n# ax.legend()\n\n# # Retrieve and also plot the NBER recession indicators\n# rec = DataReader('USREC', 'fred', start=temp_df_wide.index.min(), end=temp_df_wide.index.max())\n# ylim = ax.get_ylim()\n# ax.fill_between(dates[:-3], ylim[0], ylim[1], rec.values[:-4,0], facecolor='k', alpha=0.1);","0bf069cb":"# This doesn't seem make sense.\n# res.plot_coefficients_of_determination(figsize=(8,2));","9dbb3abf":"temp_series = df.groupby([\"store_id\", \"cat_id\"])[\"value\"].sum()","c5801c12":"store_id_list_by_state = [[\"CA_1\", \"CA_2\", \"CA_3\", \"CA_4\"], [\"TX_1\", \"TX_2\", \"TX_3\"], [\"WI_1\", \"WI_2\", \"WI_3\"]] ","a43c4fba":"fig, axs = plt.subplots(3, 4, figsize=(16, 12), sharey=True) \n\nfor row in range(len(store_id_list_by_state)):\n    for col in range(len(store_id_list_by_state[row])):\n        axs[row, col].bar(x=temp_series[temp_series.index.get_level_values(\"store_id\") == store_id_list_by_state[row][col]].index.get_level_values(\"cat_id\"),\n                          height=temp_series[temp_series.index.get_level_values(\"store_id\") == store_id_list_by_state[row][col]].values,\n                         color=[\"orange\", \"green\", \"blue\"], label=[\"FOODS\", \"HOBBIES\", \"HOUSEHOLD\"])\n        axs[row, col].set_title(store_id_list_by_state[row][col])\n        axs[row, col].set_ylabel(\"# of items\")\n\nfig.suptitle(\"Each category item sold in each store\")","84378bff":"fig, axs = plt.subplots(3, 4, figsize=(16, 12), sharey=True) \n\nfor row in range(len(store_id_list_by_state)):\n    for col in range(len(store_id_list_by_state[row])):\n        axs[row, col].bar(x=temp_series[temp_series.index.get_level_values(\"store_id\") == store_id_list_by_state[row][col]].index.get_level_values(\"cat_id\"),\n                          height=temp_series[temp_series.index.get_level_values(\"store_id\") == store_id_list_by_state[row][col]].values \/ temp_series[temp_series.index.get_level_values(\"store_id\") == store_id_list_by_state[row][col]].sum(),\n                         color=[\"orange\", \"green\", \"blue\"], label=[\"FOODS\", \"HOBBIES\", \"HOUSEHOLD\"])\n        axs[row, col].set_title(store_id_list_by_state[row][col])\n        axs[row, col].set_ylabel(\"% of each category\")\n\nfig.suptitle(\"Each category item sold percentage in each store\")","641a1e29":"cat_id = \"FOODS\"\n\ntemp_series = df.groupby([\"store_id\", \"cat_id\", \"wday\"])[\"value\"].sum()\ntemp_series = temp_series[temp_series.index.get_level_values(\"cat_id\") == cat_id]\ntemp_series","bd0e443c":"weekday = [\"Sat\", \"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"]","9b973596":"# Combine all these three figures.\ncat_list = [\"FOODS\", \"HOBBIES\", \"HOUSEHOLD\"]\ncolor_list = [\"orange\", \"green\", \"blue\"]\ntemp_series = df.groupby([\"store_id\", \"cat_id\", \"wday\"])[\"value\"].sum()\nwidth = 0.25\n\nfig, axs = plt.subplots(3, 4, figsize=(20, 12), sharey=True) \n\nfor row in range(len(store_id_list_by_state)):\n    for col in range(len(store_id_list_by_state[row])):\n        for i, cat in enumerate(cat_list):\n            height_numerator = temp_series[(temp_series.index.get_level_values(\"cat_id\") == cat) & (temp_series.index.get_level_values(\"store_id\") == store_id_list_by_state[row][col])].values\n            height_denominater = height_numerator.sum()\n\n            axs[row, col].bar(x=temp_series[(temp_series.index.get_level_values(\"cat_id\") == cat) & (temp_series.index.get_level_values(\"store_id\") == store_id_list_by_state[row][col])].index.get_level_values(\"wday\") + width * (i-1),\n                              height=height_numerator \/ height_denominater,\n                             tick_label=weekday, color=color_list[i], width=width, label=cat)\n            axs[row, col].set_title(store_id_list_by_state[row][col])\n            axs[row, col].legend()\n            \nfig.suptitle(\"HOBBIES item sold in each store in each day\")","41beb1ac":"fig, axs = plt.subplots(1, 3, sharey=True)\nfig.suptitle(\"Snap Purchase Enable Day Count of each store\")\n\nsns.countplot(x=\"snap_CA\", data =calendar_df, ax=axs[0])\nsns.countplot(x=\"snap_TX\", data =calendar_df, ax=axs[1])\nsns.countplot(x=\"snap_WI\", data =calendar_df, ax=axs[2])","b34caf3c":"temp_df = calendar_df.groupby([\"year\"])[[\"snap_CA\", \"snap_TX\", \"snap_WI\"]].sum()\ntemp_df","ce4a7a2c":"# This cell is just visuallizing the above dataframe.\nplt.bar(temp_df.index, temp_df.snap_CA)\nplt.ylabel(\"# of snap purchase allowed day\")\nplt.xlabel(\"Year\")\nplt.title(\"Snap Purchase allowed day yearly transition\")","46fbb4a7":"temp_df = calendar_df[calendar_df[\"year\"] == 2015].groupby([\"month\"])[[\"snap_CA\", \"snap_TX\", \"snap_WI\"]].sum()\ntemp_df","8006604a":"# Just visualizing the above dataframe\nplt.bar(temp_df.index, temp_df.snap_CA)\nplt.ylabel(\"# of snap purchase allowed day\")\nplt.xlabel(\"Month\")\nplt.title(\"Snap Purchase allowed day monthly trend\")","0f7739b3":"temp_df = calendar_df[calendar_df[\"year\"] == 2015].groupby([\"weekday\"])[[\"snap_CA\", \"snap_TX\", \"snap_WI\"]].sum()\ntemp_df","36871598":"plt.bar(temp_df.index, temp_df.snap_CA)\nplt.xticks(rotation=60)\nplt.ylabel(\"# of snap purchase allowed day\")\nplt.xlabel(\"Day type\")\nplt.title(\"Snap Purchase allowed day weekly trend\")","127947d1":"# Make temp dataframe with necessary information\ntemp_df = df.groupby([\"date\", \"state_id\"])[[\"value\"]].sum()\ntemp_df = temp_df.reset_index()\ntemp_df = temp_df.merge(calendar_df[[\"date\", \"snap_CA\", \"snap_TX\", \"snap_WI\"]], on=\"date\")\ntemp_df","a45beb47":"np.argmax(temp_df.groupby([\"date\", \"state_id\"])[\"value\"].sum())","a59d112a":"temp_df = temp_df[(temp_df.date >= \"2016-02-15\") & (temp_df.date <= \"2016-03-25\") & (temp_df.state_id == \"CA\")]\ntemp_df","959b3518":"fig, ax1 = plt.subplots()\nplt.xticks(rotation=60)\nax1.plot(\"date\", \"value\", data=temp_df[temp_df.state_id == \"CA\"])\nax2 = ax1.twinx()  \nax2.scatter(\"date\", \"snap_CA\", data=temp_df[temp_df.state_id == \"CA\"])","4e4d9884":"plt.figure(figsize=(8, 6))\nsns.countplot(x=\"event_type_1\", data=calendar_df[calendar_df[\"event_name_1\"] != \"unknown\"])\nplt.xticks(rotation=90)\nplt.title(\"Event Type Count in event name 1 column\")","b6e39b49":"# Let's check the distribution of snap purchase day and event day\n# Accirding to the graph, Snap CA is allowed especially when sport event occurs.\n\nplt.figure(figsize=(8, 6))\nsns.countplot(x=\"event_type_1\", data=calendar_df[calendar_df[\"event_name_1\"] != \"unknown\"], hue=\"snap_CA\")\nplt.xticks(rotation=90)\nplt.legend(bbox_to_anchor=(1.01, 1.01))\nplt.title(\"Snap Purchse allowed day Count in each event category\")","96a3e6f5":"temp_series = df.groupby([\"cat_id\", \"event_type_1\"])[\"value\"].mean()\ntemp_series","a275d97b":"plt.bar(x=temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].index.get_level_values(\"event_type_1\"), \n        height=temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].values)\nplt.title(\"HOBBIES Item Sold mean in each event type\")\nplt.ylabel(\"Item sold mean\")\nplt.xlabel(\"Event Type\")","dd9fee5b":"# find out most sold item for example\ndf[df[\"value\"] == df[\"value\"].max()]","34428c5a":"target_id = \"FOODS_3_090_CA_3_validation\"\ntemp_df = df[df[\"id\"] == target_id]\ntemp_df","afb579d9":"weekday = [\"Sat\", \"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"]\n\n# Create one hot weekday column from wday column to calculate correlation later. \nfor idx, val in enumerate(weekday):\n    temp_df.loc[:, val] = (temp_df[\"wday\"] == idx + 1).astype(\"int8\")\n\ntemp_df\n# sns.heatmap(temp_df[[\"value\", \"snap_CA\", ]].corr(), annot=True)","40968c8d":"# Create Event Flag (Any events occur: 1, otherwise: 0)\n# Create Each Event Type Flag\ntemp_df.loc[:, \"is_event_day\"] = (temp_df[\"event_name_1\"] != \"unknown\").astype(\"int8\")\ntemp_df.loc[:, \"is_sport_event\"] = (temp_df[\"event_type_1\"] == \"Sporting\").astype(\"int8\")\ntemp_df.loc[:, \"is_cultural_event\"] = (temp_df[\"event_type_1\"] == \"Cultural\").astype(\"int8\")\ntemp_df.loc[:, \"is_national_event\"] = (temp_df[\"event_type_1\"] == \"National\").astype(\"int8\")\ntemp_df.loc[:, \"is_religious_event\"] = (temp_df[\"event_type_1\"] == \"Religious\").astype(\"int8\")\n\ntemp_df.head()","a20c3a22":"# Plot Heatmap with these columns made in previous cells\nplt.figure(figsize=(14, 10))\nsns.heatmap(temp_df[[\"value\", \"sell_price\", \"snap_CA\", \"is_event_day\", \"is_sport_event\", \"is_cultural_event\", \"is_national_event\", \"is_religious_event\"] + weekday].corr(), annot=True)\nplt.title(\"Heatmap with values, snap_CA,  event_flag and weekday columns\")","bd612f05":"df.groupby(\"cat_id\")[\"sell_price\"].mean()","a0d17658":"df.groupby(\"cat_id\")[\"sell_price\"].describe()","8d032856":"sns.boxplot(data=df, x=\"cat_id\", y='sell_price')\nplt.title(\"Boxplot of sell prices in each category\")","a98cbaa8":"plt.figure(figsize=(12, 6))\nsns.boxplot(data=df, x=\"cat_id\", y='sell_price', hue=\"store_id\")\nplt.title(\"Boxplot of sell prices in each store\")","8f15109a":"# One Item Sell Price Transition\nsns.lineplot(data=df[df[\"item_id\"] == \"FOODS_3_090\"], x='date', y='sell_price', hue=\"store_id\")\nplt.legend(bbox_to_anchor=(1.01, 1.01))\nplt.title(\"Sell price change of 'FOODS_3_090' in each store\")","3ed567d9":"df[\"is_event_day\"] = (df[\"event_name_1\"] != \"unknown\").astype(\"int8\")\ndf.head()","7910faf3":"sns.heatmap(df[df[\"item_id\"] == \"FOODS_3_090\"][[\"value\", \"sell_price\", \"is_event_day\"]].corr(), annot=True)\nplt.title(\"Heatmap of value, sell_price and event flag\")","04555b52":"temp_df = df.groupby([\"date\", \"cat_id\"])[\"sell_price\"].mean()\ntemp_df","1c5d2a9f":"plt.figure(figsize=(8,4))\nsns.lineplot(x=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"FOODS\"].index.get_level_values(\"date\"), y=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"FOODS\"].values, label=\"FOODS\")\nsns.lineplot(x=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].index.get_level_values(\"date\"), y=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].values, label=\"HOUSEHOLD\")\nsns.lineplot(x=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOBBIES\"].index.get_level_values(\"date\"), y=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOBBIES\"].values, label=\"HOBBIES\")\nplt.legend()\nplt.xlabel(\"Year\")\nplt.ylabel(\"Mean price\")\nplt.title(\"Mean price transition of each category\")","6a3b876b":"temp_df = df.groupby([\"date\", \"cat_id\"])[\"item_id\"].count()","77aa4fb9":"plt.figure(figsize=(8,4))\nsns.lineplot(x=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"FOODS\"].index.get_level_values(\"date\"), y=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"FOODS\"].values, label=\"FOODS\")\nsns.lineplot(x=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].index.get_level_values(\"date\"), y=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].values, label=\"HOUSEHOLD\")\nsns.lineplot(x=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOBBIES\"].index.get_level_values(\"date\"), y=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOBBIES\"].values, label=\"HOBBIES\")\nplt.legend()\nplt.xlabel(\"Year\")\nplt.ylabel(\"Registered Item Counts\")\nplt.title(\"Registered Item Counts Transition in each category\")","f4bdaf77":"sns.jointplot(df[\"value\"], df[\"sell_price\"])","ab54fd65":"df[\"sell_price_diff\"] = df.groupby(\"id\")[\"sell_price\"].transform(lambda x: x - x.mean()).astype(\"float32\")","fa85959b":"sns.lineplot(df[df[\"item_id\"] == \"FOODS_3_090\"][\"date\"],df[df[\"item_id\"] == \"FOODS_3_090\"][\"sell_price_diff\"], hue=df[\"store_id\"]) \nplt.legend(bbox_to_anchor=(1.01, 1.01))","5c0aef44":"df[\"lag_1\"] = df.groupby(\"id\")[\"value\"].transform(lambda x: x.shift(1)).astype(\"float32\")\ndf[\"lag_7\"] = df.groupby(\"id\")[\"value\"].transform(lambda x: x.shift(7)).astype(\"float32\")","0b6a768a":"# plt.figure(figsize=(8, 8))\n# sns.pairplot(df[[\"cat_id\", \"value\", \"lag_1\"]], hue=\"cat_id\")","8246eec7":"sns.pairplot(df[[\"cat_id\", \"value\", \"lag_1\", \"lag_7\"]], hue=\"cat_id\")","eb2ec64c":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder","f05ad300":"pca = PCA()","81c0517e":"temp_df = df.loc[df[\"date\"] >= \"2015-01-01\", [\"sell_price\", \"dept_id\", \"value\", \"state_id\", \"cat_id\"]]","1716c65f":"# For memory usage reduction\ndel df\ngc.collect()","85ea583a":"temp_df.loc[temp_df[\"cat_id\"] == \"HOBBIES\", \"cat_color\"]  = \"orange\"\ntemp_df.loc[temp_df[\"cat_id\"] == \"FOODS\", \"cat_color\"]  = \"blue\"\ntemp_df.loc[temp_df[\"cat_id\"] == \"HOUSEHOLD\", \"cat_color\"]  = \"green\"\ncolor = temp_df[\"cat_color\"]","678199a5":"le = LabelEncoder()\ntemp_df[\"enc_state_id\"] = le.fit_transform(temp_df[\"state_id\"])\ntemp_df.drop(columns=[\"state_id\", \"cat_id\", \"cat_color\"], inplace=True)","765d00dd":"# temp_df = temp_df.apply(lambda x: (x-x.mean()\/ x.std(), axis=0))","386f1c73":"pca.fit(temp_df)\nfeature = pca.transform(temp_df)","128da2fb":"plt.figure(figsize=(6, 6))\nplt.scatter(feature[:, 0], feature[:, 1], alpha=0.8, c=color)\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.show()","24214bc9":"del temp_df\ngc.collect()","a1889abd":"## Sell Price Analysis","db67858f":"All categories have similar trends though the volume of these values are different.  \nFrom 2015, the number of registered items is seemed to be relatively constant.","bf0feb87":"## Apply Dynamic Factor Analysis Trial","d206962d":"1. Oops, in 2015, it seems some extreme points exist.  \n   For exmaple, around Febrary, TX_2 has almost 0 item sold. (I assume this store is closed exceptionally.)\n   In contrast, in one summer day of that year, TX_3 increased its total item sold exprosively.\n   \n2. TX_2 has most item sold especially before 2014.","686734a0":"## PCA Trial","38c261d5":"# Future Work\nIf I have time, I'd like to tackle with the following things.\n\n1. Apply Dynamic Analysis and find out the relationship among state and stores.\n2. Make One item or one store prediction model for beginners like me to learn how to use lightgbm as a regressor.\n3. Check out the pre-processing effect. Is that effective considering the noise samples like Christmas or other irregularly days.\n4. More detailed analysis and find out some useful information for making prediction.","be4c3e9d":"This result is also interesting.  \nOf course, when the price of item is expensive, less items are sold. (left top field)  \nAnd when the price gets lower, more items are sold.  (Right bottom field)  \nHowever, the relationship is not likely linear but likely to be inverse propotion.","95dd75d4":"The most sold out item in this dataseet is FOODS_3_090_CA_3_validation","366ec62e":"OK, total count in one year is almost the same in all years and all states.\nHow about monthly distribution?","9971d3b8":"1. OK, the total count of snap purchase enable day looks similar in these three stores.\n\n2. The total count of snap purchase enable day is about one half of that of non-enable day.\n\nNext Let's see whether snap purchase is how-distributed in one year.","1793676d":"## One Item Features Analysis","e6d4068a":"3. From around 2015 Spring or Summer, CA_2 increased its sold record rapidly. We have to investigate the reasons.\n\n-> It is because item registered in CA_2 increased rapidly.  \nAfter summer in 2015, all stores in CA have similar registered item count","1cb77759":"# References\nFollowing notebooks are the great notebooks in this competition. \nFor whom hasn't check these notebooks, I strongly recommend you to take a look at these notebooks.\n(I'm sorry if I missed some other great kernels, I'll take a lookt at other notebooks if I have enough time.)\n\nData Visualization:\n\n- **M5 Forecasting - Starter Data Exploration**  \n  https:\/\/www.kaggle.com\/robikscube\/m5-forecasting-starter-data-exploration  \n\n-  **Back to (predict) the future - Interactive M5 EDA**  \n   https:\/\/www.kaggle.com\/headsortails\/back-to-predict-the-future-interactive-m5-eda\n\nMaking Prediction:\n\n- **M5 - Three shades of Dark: Darker magic**  \n  https:\/\/www.kaggle.com\/kyakovlev\/m5-three-shades-of-dark-darker-magic\n\n-  **Very fst Model**  \n   https:\/\/www.kaggle.com\/ragnar123\/very-fst-model","f5edcf24":"**Point of the graph**\n1. FOODS is the most sold item category of these three categories.  \n   HOUSEHOLD is the 2nd one, and HOBBIES are the least sold one.  \n\n\n2. FOODS category appearently has some periodical feature.   \n   During one year, it seems more items are sold in summer than in winter, however, we have to verify this.  \n   As for more short time interval, it seems the trend has monthly or weekly features. (Let's take a look below)\n\n3. HOUSEHOLD category items sold is gradually increasing from 2011.  \n   However, it may be because some items are not in the store in 2011.  \n   So we have to take the total item in the store into account.\n   Periodical Features are not so clear in this category compared to FOODS.\n  \n4. In HOBBIES category, periodical features are less appearent like HOUSEHOLD category.\n\n5. In some point (around the end of year), all categories don't have any sold.  So I think we have to consider whether we take these days into account when training models.\n\nSo let's take a look at the latest year, 2015!","e74d4746":"# Item Sold relation Analysis\nUnder Construction...  \n(I tried to apply Dynamic Factor Analysis and execute the codes of this tutorial, but it seems I couldn't get informative outcome:\nhttps:\/\/www.statsmodels.org\/dev\/examples\/notebooks\/generated\/statespace_dfm_coincident.html\n\nTo Whoever can understand this method more specifically, I appreciate your comments.","64195b63":"# Snap Purchase Analysis\nLet's see how snap purchase allowed day is distributed.","5965ca50":"We can find the following things.\n1. Regarding value and other columns correlation:\n   - snap purchase and other events flag has little correltion.\n   - Saturday has the most positive effect on values, and Tuesday has the most negative effect.  \n     (We've previously seen Saturday is the most item sold day in one week [here](#Item-Sold-in-each-day-type).)\n     \n2. Regarding snap_CA and weekdays columns correlation:\n   - As we've previously seen, snap_CA is uniformly distributed in each day type.  \n     Thus, the correlation between snap_CA and weekdays columns (ex. Monday, Tuesday, ...) are almost 0.\n\n3. Others:\n   - Looking at event and sunday correlation,it is just 0.089.  \n     I thought most part of events oocur on Sunday, but it wasn't so much as I had exoected.\n   - Regarding sell price, we look below.","27b76839":"OK, event tyoe distributes like the graph above.   \n(Most of the values are actually \"unknown\", but for visualization, I omitted unknown value)","b616a03c":"1. OK. Total snap purchase allowed day of each state is the same in all years.\n\n2. From 2011 to 2015, there are about 120 days when snap purchase is allowed.  \n   (As for 2016, we only have the first half of whole year.)","e702688b":"1. Three stores in CA have similar amount of item sold record.  \n   CA_3 has more item sold a little bit compared to others.  \n\n2. The standard deviation of each store seems different, confirm it later.\n\n3. From around 2015 Spring or Summer, CA_2 increased its sold record rapidly. We have to investigate the reasons.\n\n","607a8ad1":"Let's check the sales of event day!","8fccdcfa":"1. CA is the most sold state of these three states.  \n   TX and WI are not so different except for the year 2011 and 2012.\n  \n2. All three states have some periodical features as we've already seen in category-based item sold graph. \n\nFirst, let's focus on stores in CA.","151d92a7":"All store has lower price than mean value in the latter half of 2013.  ","66ce503f":"As we've already seen above, the registered item count trends are different in each store.  \nWI_2 increased its item register around summer in 2012, then WI_3 increased around November of that year.  \n\nFrom 2013, all stores have similar trend.  Next, stores in TX states!","df1a3c6b":"# Data Visualization Notebook\n\n\nHere is table of contents in this notebook:\n- [Import Libraries and Data Input](#Import-Libraries-and-Data-Input)\n- [Data Cleaning](#Data-Cleaning)\n- [Data Visualization](#Data-Visualization)\n   -  [Total Item Sold Transition](#Total-Item-Sold-Transition)\n   -  [Item Sold in each day type](#Item-Sold-in-each-day-type)\n   -  [Item sold in each State and Store](#Item-sold-in-each-State-and-Store)\n   -  [Item Sold relation Analysis](#Item-Sold-relation-Analysis)\n   -  [Store Analysis](#Store-Analysis)\n   -  [Snap Purchase Analysis](#Snap-Purchase-Analysis)\n   -  [Event Pattern Analysis](#Event-Pattern-Analysis)\n   -  [One Item Features Analysis](#One-Item-Features-Analysis)\n   -  [Sell Price Analysis](#Sell-Price-Analysis)\n   -  [Sell price and value relationship](#Sell-price-and-value-relationship)\n   -  [Relationship of Lag Variables](#Relationship-of-Lag-Variables)\n   -  [PCA Trial](#PCA-Trial)\n  \n- [Summary](#Summary)\n- [Future Work](#Future-Work)\n- [References](#References)","55cc331e":"Event Day flag and Sell Price don't have so strong relationship.  \nHowever, when we buy items for some events, we perhaps buy items 1 week ~ 1 day before the event, not on the same day.  \nSo we have to take this into consideration.  (I'll tackle with this analysis later.)","52826e07":"# Event Pattern Analysis\nLet's check event pattern in event_name_1 column.  \n(As for event_name_2 column, there are much less non-null values compeared to event_name_1 column.","fb9add40":"OK, total count in one month is the same through the whole year.\nHow about weekly distribution?","24b1d9ff":"Some items are sold even on Christmas Day, but I think these are completely noisy values.   \n\nUntil now, we can find the items sold have something weekly fetures. So let's think this:   \n**Next Question: Which day of the week is the items sold most?**","d0146050":"# Data Cleaning\nFirst, let's combine all three dataframe.  \nThe important thing is changing data format from wide to long to make prediction model easier  \n(Though this notebook doesn't dive into predicition model itself.)\n\n","ad8db101":"On Christmas Day, the items sold are seemed to be 0, let's check it with *.loc* method","a67e6c8f":"In above figure, each plot means whether the day allows snap purchase or not in CA.  \nAs you can see, snap purchase enable day is not regularly distributed like one day in three consective days.  \n(ex. 2016-03-01 > 2016-03-04 > 2016-03-07 > ...)  \nIt is actually biasedly distributed like the figure above.  \n(i.e. Snap purchase Enable Day continues from 2016-03-01 to 2016-03-10)  \nAnd on these days, sales are also increased.  ","048a4d57":"## Sell price and value relationship","9e4ded44":"## Item sold in each State and Store","25832f13":"# Summary\n   In this notebook, through some easy data visualization, we found some points regarding this dataset like below.\n   1. The transition of all items sold in each category \n      - Some periodical effect. (Weekly and monthly)\n      - On christmas day, there are almost no sales\n   2. Which category is the most sold one?  \n      - FOODS is the most sold item category of these three categories.\n      - HOUSEHOLD category is the 2nd one, and the HOBBIES are the least sold one.\n   3. Which day type is the most sold day? \n      - Saturday and Sunday is the most item sold day types.\n      - In contrast, on weekdays like Tuesday, there are less item sold.\n   4. The transition in all stores by each state\n      - In CA, CA_3 store is the most sold store.  \n      - In other states, not so much difference appeared.  \n      - These sales trasition often corresponds to the registered item entries.\n   5. Snap purchase allowed day visuaizaiton\n      - The total count of Snap purchase allowed day in whole year is almost the same from 2011.\n      - The total count of Snap purchase allowed day in one month is 10 in every month.\n      - The total count of Snap purchase allowed day in each day type is almost uniformly distributed.\n      - However, there are some biased patterns regarding snap purchase allowed flag.\n        (i.e. it is not like one day in three consective days regularly, but all days in one week and none in next week)\n    \n   And finally we visualize some points by using heatmap.","82211295":"1. As we can probraly guess, Saturday or Sunday is the day which the items are most sold.  \n   Tuesday or Wednesday is the least sold days.  \n   -> Later, we visualize these correlation factors with heatmap. Looking forward to it!\n\n2. HOBBIES are not so day dependent compared to FOODS or HOUSEHOLD.  ","9058924c":"1. Oops, in 2015, it seems some extreme points exist.  \n   For exmaple, around Febrary, TX_2 has almost 0 item sold. (I assume this store is closed exceptionally.)\n   \n   -> On 2015-03-24, TX_2 has very little item sold. ","5583af3b":"Though the item is same, sell price is slightly different in each store and each season. -> Some promotion season?","a9013962":"# Import Libraries and Data Input","f9e0a683":"1. Since CA_3 is the most sold store in CA, standard deviation of this store is also higher than others.  \n   Expecially, around the end of 2011, sold item deviation gets higher than usual. \n   \n \nLet's check other state, WI next!","b80e8f44":"1. In all categories, the periodical trends is seemed weekly.  \n   In previous graph, we can't easily recognize that HOUSEHOLD and HOBBIES have weekly features, but in this graph we can.\n   \n2. The day when all item sold is 0 is seemed to be Christmas Day, not new year's day, confirm it below.","756372f6":"Compared to other states, TX stores have similar tendency regarding registered entries.","9ff41fb5":"# Data Visualization\n## Total Item Sold Transition","bf6f3fd8":"Regarding weekly trend, we can find no biased distribution.  \nThis is also almost uniformly distributed like year total and month total.","77ac450a":"# Store Analysis","90721e94":"I don't understand why mean = NaN when using *.describe* method, however, *.mean()* method accurately calculate category's sell price mean.  \nLet's plot it with some ways!","2e516637":"I thought when some cultual or sporting event occurs, HOBBIES item are more likely to be sold.  \nHowever, this plot doesn't mean this hypothesis clearly.","55ae9148":"## Relationship of Lag Variables","4bb622a3":"## Discount Season Presumption\nIs there a period when all items have lower price than usual in Walmart?","14cf2539":"1. Stores in WI have similar item sold count.  \n   Before 2013, WI_3 is the most sold store in WI, but WI_2 gradually increases its proportion. (Especially around summer in 2012)\n   \n2. In some point, WI_1 rapidly increase its sold item count. (Around on November 2012)\n\n-> Total Sold out count depends on the number of entries at that day.  So Now we check the total item entries in each store as we did in CA stores.","61f63437":"We've already seen HOUSEHOLD has some exceptionally expensive sell price.  \nNow we found these items aren't sold anywhere, but only in some stores.","cc6678a2":"## Item Sold in each day type","9f114e4f":"Find the most item sold day for example and take a look at the relationship between snap purchase allowed flag and values.","42b300b1":"# Acknowledgment\n\nI apologize that my english are somewhat wrong and my codes are not so beautiful one like others' codes.  \nHowever, I tried hard to make simle codes as much as I can especially for beginners like me to learn how to use matplotlib and seaborn to do data visualization.  \nAny comments or upvotes can be my very strong motivation towards much harder work! Thank you!","1b902ae1":"The mean of item price in each category seems to have some change in these dataset periods.  \nIs this simply because the expensive item increased in later periods?","f068f925":"Maybe 1 day or 1 week lag variables are important in this case.  \nI'll find the strength of correlation between the sold of the day and past few days in my next version.","575bcbac":"Through the year, we have 10 snap purchase allowed days in one month.  \nThis tendency is the same from 2012 to 2015.  \n(In 2011, no snap days in January)","1c899739":"The price of some Household category is super expensive like over 100 \\$.  \nOn the other hand, foods are mostly around 5 to 10 \\$ and don't have a large deviation.","e334e313":"From above things, we may think \"oh, snap_enable day is distributed uniformly, like 1 day in 3 consecutive days.\" because all of these barplot show it's not so different in any month, any day.  \nHowever, it is **completely different** as I'll show you below.  \n(i.e. snap purchase enable day is distributed biasedly.)"}}