{"cell_type":{"40e87be2":"code","3d2fa660":"code","a903aa72":"code","c768dce9":"code","b6648f43":"code","d12f3115":"code","62aeec18":"code","177854ba":"code","199bea12":"code","74f4a71c":"code","f93e74ae":"code","8eb5707d":"code","39dcb735":"code","5a381262":"code","3fe8a386":"code","67194cb7":"code","fd18129c":"code","aed7853d":"code","a6607d94":"code","2587bbfa":"code","57622341":"code","b3af5f9c":"code","2106f08b":"code","7f59e750":"code","c4e68428":"code","f6a5131d":"code","93a6b6d4":"code","c2f73fbc":"code","57674bac":"code","d8de89df":"code","bcd44005":"code","7ccaf68c":"code","674f52ad":"code","c179dad6":"code","130bc596":"code","6fc58d99":"code","c5dc0238":"code","324a6023":"code","158c851d":"code","ceacbf47":"code","ea1477f1":"code","ad7c86c4":"code","cd933956":"code","e6428ee5":"code","85fe4e8c":"code","0c6dc0ed":"code","fcce39ff":"code","87e2d40e":"code","82d48d2a":"code","d993a72d":"code","11729e7f":"code","d5a7a68f":"code","0e22e4ee":"code","126118b8":"code","b942328f":"code","8b3a94ca":"code","73c15a59":"code","b0de2d9b":"code","b517f839":"code","26c825db":"markdown","da91eae8":"markdown","0e431515":"markdown","1424a097":"markdown","d3eab626":"markdown","5870936c":"markdown","9b66939e":"markdown","771636de":"markdown","847acdc6":"markdown","68f95070":"markdown","37667031":"markdown","c6ae45e6":"markdown"},"source":{"40e87be2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3d2fa660":"\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom sklearn import preprocessing \nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom category_encoders import *\nfrom sklearn.preprocessing import LabelEncoder\n%matplotlib inline\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import datasets, linear_model, metrics\nfrom sklearn.metrics import  confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, plot_confusion_matrix\nfrom sklearn.linear_model import LinearRegression","a903aa72":"!pip install openpyxl","c768dce9":"df=pd.read_excel('..\/input\/air-quality-time-series-data-uci\/AirQualityUCI.xlsx')\n\ndf","b6648f43":"#getting the names of columns\ndf.columns","d12f3115":"#getting the shape of the dataset\ndf.shape","62aeec18":"#Checkoing Skewness of data\ndf.skew()","177854ba":"#deleting null values\ndatacleaned=df.dropna()\ndatacleaned.isnull().sum()","199bea12":"#checking correlation\ndf.corr()","74f4a71c":"#Global declartions of function names for EDA\nglobal Head\nglobal Size\nglobal Column_names\nglobal Describe\nglobal Shape\nglobal Count\nglobal Value_count\nglobal ISNULL\nglobal Tail\nglobal Ndim\nglobal Nunique\nglobal Memory_usage\nglobal Duplicated\nglobal ISNA\nglobal DTYPES\nglobal CORR\nglobal Info\nglobal operations\n        \n\n        ","f93e74ae":" def Head(value=5):\n            print('\\033[1m'+'displaying the', value, 'rows'+'\\033[0m')\n            a=df.head(value)\n            return a\n            print(\"--------------------------------------------------------------------------\")\nHead()","8eb5707d":" def Shape():\n            print('\\033[1m'+'Shape of the Data set'+'\\033[0m')\n            b=df.shape\n            print(b,'\\n')\n            print(\"--------------------------------------------------------------------------\")\n\nShape()","39dcb735":"def Column_names():\n    print('\\033[1m'+'Column Names in the Data set'+'\\033[0m')\n    c=df.columns\n    print(c,'\\n')\n    print(\"--------------------------------------------------------------------------\")\nColumn_names()","5a381262":"def Describe():\n    print('\\033[1m'+\"The Description of our dataset is:\"+'\\033[0m')\n    des=df.describe()\n    return(des)\n    print(\"--------------------------------------------------------------------------\")\nDescribe()","3fe8a386":"def Size():\n    print('\\033[1m'+\"The size of dataset is :\"+'\\033[0m')\n    siz=df.size\n    print(siz,'\\n')\n    print(\"--------------------------------------------------------------------------\")\nSize()","67194cb7":"def Count():\n    print('\\033[1m'+\"The count of non null values are:\"+'\\033[0m')\n    co=df.count()\n    print(co,'\\n')\n    print(\"--------------------------------------------------------------------------\")\nCount()","fd18129c":"def Value_count():\n    print('\\033[1m'+\"The count of unique  values \"+'\\033[0m')\n    co1=df.value_counts().sum()\n    return co1\n    print(\"--------------------------------------------------------------------------\")\nValue_count()","aed7853d":"def ISNULL():\n    print('\\033[1m'+\"Detection of missing values\"+'\\033[0m')\n    co2=df.isnull().sum()\n    print(co2,'\\n')\n    print(\"--------------------------------------------------------------------------\")\nISNULL()","a6607d94":" def Tail():\n    print('\\033[1m'+\"The last five rows of the dataframe are\"+'\\033[0m')\n    co3=df.tail()\n    return(co3)\n    print(\"--------------------------------------------------------------------------\")\nTail()","2587bbfa":"def Ndim():\n    print('\\033[1m'+\"The dimensions of data set are:\"+'\\033[0m')\n    co4=df.ndim\n    print(co4,'\\n')\n    print(\"--------------------------------------------------------------------------\")\nNdim()","57622341":"def Nunique():\n    print('\\033[1m'+\"Total number of unique values are:\"+'\\033[0m')\n    co5=df.nunique()\n    print(co5,'\\n')\n    print(\"--------------------------------------------------------------------------\")\nNunique()","b3af5f9c":"def Memory_usage():\n    print('\\033[1m'+\"The total memory used is :\"+'\\033[0m')\n    co6=df.memory_usage()\n    print(co6,'\\n')\n    print(\"--------------------------------------------------------------------------\")\nMemory_usage()","2106f08b":"def Duplicated():\n    print('\\033[1m'+\"Total number of duplicate rows\"+'\\033[0m')\n    co7=df.duplicated().count()\n    return(co7)\n    print(\"--------------------------------------------------------------------------\")\nDuplicated()","7f59e750":"def ISNA():\n    print('\\033[1m'+\"The detected missing values are :\"+'\\033[0m')\n    co8=df.isna().sum()\n    return(co8)\n    print(\"--------------------------------------------------------------------------\")\nISNA()","c4e68428":"def DTYPES():\n    print('\\033[1m'+\"The datatypes are :\"+'\\033[0m')\n    co9=df.dtypes\n    print(co9,'\\n')\n    print(\"--------------------------------------------------------------------------\")\nDTYPES()","f6a5131d":"def Info():\n    print('\\033[1m'+\"The info of data set is :\"+'\\033[0m')\n    co11=df.info()\n    print(\"--------------------------------------------------------------------------\")\nInfo()","93a6b6d4":"def operations(df,x):\n    if df[x].dtype==\"float64\":\n        print('\\033[1m'+'', x, 'rows'+'\\033[0m')\n        print('\\033[1m'+\"It is a quantitaive data \\n\"+'\\033[0m')\n        print(\"The mean is :\\n\",df[x].mean())\n        print(\"The median is :\\n\",df[x].median())\n        print(\"The Standard Deviation is \\n\",df[x].std())\n        q1=df[x].quantile(0.25)\n        q2=df[x].quantile(0.5)\n        q3=df[x].quantile(0.75)\n        IQR=q3-q1\n        LLP=q1-1.5*IQR\n        ULP=q3+1.5*IQR\n        print(\"The quartiles are q1 : \\n\",q1)\n        print(\"The quartiles are q2 : \\n\",q2)\n        print(\"The quartiles are q3 :\\n \",q3)\n        print(\"The Uppler limit point of the data is \\n\",ULP)\n        print(\"The lower limit point of the data is \\n \",LLP)\n        if df[x].min()>LLP and df[x].max()<ULP:\n            print(\"The outliers are not present \\n\")\n            print(\"--------------------------------------------------------------------------\")\n\n        else:\n\n            print(\"The outliers are present \\n\")\n            print(\"The outliers are :\")\n            print(df[df[x].values>ULP][x])\n            print(df[df[x].values<LLP][x])\n\n            print(\"--------------------------------------------------------------------------\")\n\n\n    elif df[x].dtype==\"int64\":\n        print('\\033[1m'+'', x, 'rows'+'\\033[0m')\n        print('\\033[1m'+\"It is a quantitaive data \\n\"+'\\033[0m')\n        print(\"The mean is : \\n\",df[x].mean())\n        print(\"The median is : \\n\",df[x].median())\n        print(\"The Standard Deviation is \\n\",df[x].std())\n        q1=df[x].quantile(0.25)\n        q2=df[x].quantile(0.5)\n        q3=df[x].quantile(0.75)\n        IQR=q3-q1\n        LLP=q1-1.5*IQR\n        ULP=q3+1.5*IQR\n        print(\"The quartiles are q1 : \\n\",q1)\n        print(\"The quartiles are q2 : \\n\",q2)\n        print(\"The quartiles are q3 : \\n\",q3)\n        print(\"The Uppler limit point of the data is \\n\",ULP)\n        print(\"The lower limit point of the data is \\n\",LLP)\n        if df[x].min()>LLP and df[x].max()<ULP:\n            print(\"The outliers are not present \\n\")\n\n            print(\"--------------------------------------------------------------------------\")\n\n        else:\n\n            print(\"The outliers are present \\n\")\n            print(\"The outliers are :\")\n            print(df[df[x].values>ULP][x])\n            print(df[df[x].values<LLP][x])\n            print(\"--------------------------------------------------------------------------\")\n\n\n\n\n\n\n\n    else:\n\n        print('\\033[1m'+\"The data is Qualitative \\n\"+'\\033[0m')\n\n\n        if df[x].nunique()==1:\n            print('\\033[1m'+\"The data is singular \\n\"+'\\033[0m')\n            print(\"The mode is :\",df[x].mode())\n            print(\"The count of mode is \\n\",df[x].value_counts())\n        elif df[x].nunique()==2:\n            print('\\033[1m'+\"The data is Binary \\n\"+'\\033[0m')\n            print(\"The mode is :\",df[x].mode())\n            print(\"The count of mode is \\n\",df[x].value_counts())\n        elif df[x].nunique()>2:\n            print('\\033[1m'+\"The data is Multi \\n\"+'\\033[0m')\n            print(\"The mode is :\",df[x].mode())\n            print(\"The count of mode is \\n\",df[x].value_counts())\n\n        print(\"--------------------------------------------------------------------------\")\n\nc=df.columns\nfor i in c:\n    operations(df,i)\n    print(\"\\n\")\n\n\n","c2f73fbc":"def Summary():\n        print('\\033[1m'+\"The Summary of data is  \\n\"+'\\033[0m')\n        print(\"The shape of the datset is :\",df.shape)\n        print(\"The sixe o the data set is :\",df.size)\n        print(\"The dimensions of the dataset are:\",df.ndim)\n        print(\"The memory usage of the data set are\",df.memory_usage())\n        print(\"The data types of the dataset are:\",df.dtypes)\n        print(\"--------------------------------------------------------------------------\")\n\nSummary()     ","57674bac":" def Column_Summary():\n        print('\\033[1m'+\"The Column wise Summary of data is  \\n\"+'\\033[0m')\n        k=df.columns\n        for i in k:\n            print('\\033[1m'+'', i, 'rows'+'\\033[0m')\n            print(\"The Shape of the column \",i,\"is \",df[i].shape)\n            print(\"The Size of the column \",i,\"is \",df[i].size)\n            print(\"The Dimensions of the column \",i,\"is \",df[i].ndim)\n            print(\"The Memory used by the column \",i,\"is \",df[i].memory_usage())\n            print(\"The Data types  of the column \",i,\"is \",df[i].dtypes)\n            print(\"--------------------------------------------------------------------------\")\nColumn_Summary()","d8de89df":"#getting the datatypes if our dataset\ndf.dtypes","bcd44005":"#taking only numerical columns in list x for plotting distribution plot\nx = df.drop(['Date','Time'],axis = 1)\nx.dtypes","7ccaf68c":"x","674f52ad":"\n\n#using for loop plotting the distribution plot for each numerical column\nfor i in x:\n    sns.distplot(df[i],kde=True)\n    plt.show()\n    ","c179dad6":"plt.figure(figsize=(10,7))\nsns.histplot(data=df)\nplt.show()\n\n","130bc596":"plt.figure(figsize=(10,16))\nax = sns.heatmap(df.corr(),annot = True, cmap = 'viridis')\nplt.show()","6fc58d99":"x.columns","c5dc0238":"df3=x.drop(['T', 'RH', 'AH'],axis=1)","324a6023":"df3.head()","158c851d":"for i in df3.columns:\n    plt.figure(figsize=(10,10))\n    plt.gcf().text(.5, .9, \"Scatter Plot\", fontsize = 40, color='Black' ,ha='center', va='center')\n    sns.scatterplot(x=df[i] , y=df['T'])\n    plt.show()","ceacbf47":"for i in x.columns:\n    plt.figure(figsize=(16,6))\n    plt.title(\"Air quality vs diffrent Parameters\",fontsize = 14)\n    sns.set(rc={\"axes.facecolor\":\"#283747\", \"axes.grid\":False,'xtick.labelsize':10,'ytick.labelsize':10})\n    plt.xticks(rotation=45) # Rotating X tickts by 45 degrees\n    sns.lineplot(x =df['Date'], y =df[i])\n    plt.show()","ea1477f1":"def count_outliers(data,col):\n        q1 = data[col].quantile(0.25,interpolation='nearest')\n        q2 = data[col].quantile(0.5,interpolation='nearest')\n        q3 = data[col].quantile(0.75,interpolation='nearest')\n        q4 = data[col].quantile(1,interpolation='nearest')\n        IQR = q3 -q1\n        global LLP\n        global ULP\n        LLP = q1 - 1.5*IQR\n        ULP = q3 + 1.5*IQR\n        if data[col].min() > LLP and data[col].max() < ULP:\n            print(\"No outliers in\",i)\n        else:\n            print(\"There are outliers in\",i)\n            x = data[data[col]<LLP][col].size\n            y = data[data[col]>ULP][col].size\n            a.append(i)\n            print('Count of outliers are:',x+y)\nglobal a\na = []\nfor i in x.columns:\n    count_outliers(df,i)","ad7c86c4":"#Replacing outliers with null values\nfor i in x.columns:\n    Q1 = np.percentile(df[i], 25,interpolation = 'midpoint')\n    Q3 = np.percentile(df[i], 75,interpolation = 'midpoint')\n    IQR = Q3 - Q1\n    upper1 = np.where(df[i] >= (Q3+1.5*IQR))\n    lower1 = np.where(df[i] <= (Q1-1.5*IQR))\n    for j in range(len(upper1[0])):\n        df.replace(df[i][upper1[0][j]],np.nan, inplace = True)\n    for k in range(len(lower1[0])):\n        df.replace(df[i][lower1[0][k]],np.nan, inplace = True)","cd933956":"df.isnull().sum()","e6428ee5":"#replacing outliers with mean of outliers of that column\nfor t in x.columns:\n    q1=df[t].quantile(0.25)\n    q2=df[t].quantile(0.5)\n    q3=df[t].quantile(0.75)\n    IQR=q3-q1\n    LLP=q1-1.5*IQR\n    ULP=q3+1.5*IQR\n    l=[]\n    m=[]\n    df3 = df[df[t].values>ULP][t]\n    for i in df3:\n        l.append(i)\n    df4 = df[df[t].values<LLP][t]\n    for i in df4:\n        m.append(i)\n    k = 0\n    p = 0\n    if len(l)>0:\n        k=sum(l)\/len(l)\n    if len(m)>0:\n        p=sum(m)\/len(m)\n    u=(k+p)\/2\n    for i in df.columns:\n        if df[i].isnull().sum()>=1:\n            df[i].fillna(u,inplace = True)","85fe4e8c":"df.isnull().sum()","0c6dc0ed":"#checking statistical analysis after preproccessing\ndf.describe()","fcce39ff":"#Encoding the date time column\ndef LABEL_ENCODING(c1):\n    from sklearn import preprocessing\n    # label_encoder object knows how to understand word labels.\n    label_encoder = preprocessing.LabelEncoder()\n \n    # Encode labels in column 'species'.\n    df[c1]= label_encoder.fit_transform(df[c1])\n \n    df[c1].unique()\n    return df","87e2d40e":"LABEL_ENCODING('Date')","82d48d2a":"LABEL_ENCODING('Time')","d993a72d":"\n#plotting histogram for correlation values\ncorrmat=df.corr()\ntop_corr_feature=corrmat.index\nplt.figure(figsize=(30,20))\ng=sns.heatmap(df[top_corr_feature].corr(),annot=True,cmap='viridis')","11729e7f":"#getting the pairplot for the entire dataframe\nsns.pairplot(df)","d5a7a68f":"def count_outliers(data,col):\n        q1 = data[col].quantile(0.25,interpolation='nearest')\n        q2 = data[col].quantile(0.5,interpolation='nearest')\n        q3 = data[col].quantile(0.75,interpolation='nearest')\n        q4 = data[col].quantile(1,interpolation='nearest')\n        IQR = q3 -q1\n        global LLP\n        global ULP\n        LLP = q1 - 1.5*IQR\n        ULP = q3 + 1.5*IQR\n        if data[col].min() > LLP and data[col].max() < ULP:\n            print(\"No outliers in\",i)\n        else:\n            print(\"There are outliers in\",i)\n            x = data[data[col]<LLP][col].size\n            y = data[data[col]>ULP][col].size\n            l=  data[data[col]<LLP][col].mean()\n            m=  data[data[col]<LLP][col].mean()\n            a.append(i)\n            \nglobal a\na = []\nfor i in x.columns:\n    count_outliers(df,i)","0e22e4ee":"for i in x:\n    sns.boxplot(df[i])\n    plt.show()","126118b8":"\n#dropping the target variable and selecting features\nfeature=df\nfeature=feature.drop('Date',axis=1)","b942328f":"feature.head()","8b3a94ca":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(feature)","73c15a59":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nlabel=df['Date']\nX_train,X_test,y_train,y_test=train_test_split(feature,label,test_size=.3)\n\nforest= RandomForestRegressor(n_estimators =40, random_state = 0)\nforest.fit(X_train,y_train)  \ny_pred = forest.predict(X_test)\nforest.score(X_test,y_test)","b0de2d9b":"print(y_pred)","b517f839":"from sklearn import metrics\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n","26c825db":"# Relation Graphs After Preproccessing","da91eae8":"# Time Series Plots\n","0e431515":"# Importing Libraries","1424a097":"# Loading Data","d3eab626":"# Feature Selection and Data Modelling","5870936c":"# Data Visualization","9b66939e":"# EDA SUMMARY","771636de":"# Realtion Plots","847acdc6":"# Data Cleaning and Preproccessing","68f95070":"# Exploratory Data Analysis","37667031":"# From the above obseration we can see that there are no outliers","c6ae45e6":"# Count of Outliers"}}