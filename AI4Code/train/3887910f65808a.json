{"cell_type":{"97fb6a6f":"code","906e4494":"code","f40471b4":"code","b321750b":"code","5c7cbbdd":"code","90ebac96":"code","fa464969":"code","7d427ff6":"code","e8daad84":"code","fabbe9da":"code","cae3f0c7":"code","9d75097d":"code","4f890a71":"code","c123b07c":"code","3c9cf882":"code","522e44c7":"code","d8a3d9f2":"code","272fdcde":"code","7d686609":"code","5af791d6":"code","094645c3":"code","e725d271":"code","f7c7e2f7":"code","c4defe4b":"code","afaf8516":"code","aecb81d4":"code","337c50f5":"code","d3b61f16":"code","e3625498":"code","3e95adf1":"code","a7819722":"code","1e8b9e1d":"code","f822ec92":"code","6d27d961":"code","bea068a4":"code","1508d4f0":"code","6a82b4e0":"code","f24c2244":"code","314e4141":"code","8d64efd1":"code","1e689878":"code","f6987d1f":"markdown","b4b01e53":"markdown","6ddd7c40":"markdown","cf027d17":"markdown","85059224":"markdown","7c7823d8":"markdown","aedb21a0":"markdown","306e6d40":"markdown","a63aca3c":"markdown","b1edffe8":"markdown","4318d570":"markdown","fabb7a8a":"markdown","51955030":"markdown","a08f9f36":"markdown","23de95c2":"markdown","830f523a":"markdown","ecc9ef12":"markdown","ed29f713":"markdown","9b337a60":"markdown","da1eea78":"markdown","3ada655d":"markdown","0cf41d06":"markdown","e3458aa3":"markdown","8651a236":"markdown","d0197f38":"markdown","49aa899e":"markdown","f3dc2b4b":"markdown","653d0755":"markdown"},"source":{"97fb6a6f":"import os\n\nimport random\n\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport missingno\n\nimport sklearn\nimport sklearn.model_selection\n\nfrom surprise import NormalPredictor\nfrom surprise import Dataset\nfrom surprise import Reader\nfrom surprise.model_selection import cross_validate, train_test_split, GridSearchCV\nfrom surprise import KNNWithMeans, SVD\nfrom surprise import accuracy\n\nimport pickle\n\n#Settings\npd.set_option('display.max_columns', None)\nsns.set(color_codes=True)\n\n#To be avoided in your notebook.\nimport warnings\nwarnings.filterwarnings(\"ignore\")","906e4494":"[print(x) for x in os.listdir('..\/input\/news-portal-user-interactions-by-globocom')];","f40471b4":"PATH_ARTICLES_DATA = \"..\/input\/news-portal-user-interactions-by-globocom\/articles_metadata.csv\"\narticles_df = pd.read_csv(PATH_ARTICLES_DATA)","b321750b":"articles_df.head(5)","5c7cbbdd":"missingno.matrix(articles_df);","90ebac96":"articles_df['words_count'].describe()","fa464969":"fig, ax = plt.subplots(figsize=(12, 8))\n\nsns.histplot(articles_df['words_count'],\n            color=\"deepskyblue\",\n            edgecolor=\"black\",\n            alpha=0.7,\n            ax=ax)\n\nplt.xlim([0,400])\nplt.title('words per article distribution');\nplt.show()","7d427ff6":"print(f'Empty article(s) : {articles_df[articles_df[\"words_count\"] == 0].count()[0]}')","e8daad84":"fig, ax = plt.subplots(figsize=(12, 8))\n\nsns.histplot(articles_df['category_id'],\n            color=\"deepskyblue\",\n            edgecolor=\"black\",\n            alpha=0.7,\n            ax=ax)\n\nplt.title('categories distribution');\nplt.show()","fabbe9da":"PATH_CLICK_SAMPLE_DATA = \"..\/input\/news-portal-user-interactions-by-globocom\/clicks_sample.csv\"\nclicks_samp_df = pd.read_csv(PATH_CLICK_SAMPLE_DATA)","cae3f0c7":"clicks_samp_df.head(5)","9d75097d":"[print(x) for x in clicks_samp_df.columns];","4f890a71":"%%time\n\nCLICK_FILES_PATH = \"..\/input\/news-portal-user-interactions-by-globocom\/clicks\/clicks\/\"\n\n#Get all .csv contain in clicks folder\nclick_files = [CLICK_FILES_PATH + x for x in os.listdir(CLICK_FILES_PATH)];\n\n#Sort all files PATH contain in list.\nclick_files.sort()\n\n#Check if all files are present.\nprint(\"Total PATH contain in list : \", len(click_files))\n\nlist_click_file_to_df = [pd.read_csv(x, index_col=None, header=0) for x in click_files];\n\n#Remove unused columns\nlist_click_file_to_df_cleaned = [x.drop(columns = ['session_id',\n                                                   'session_start',\n                                                   'session_size',\n                                                   'click_timestamp',\n                                                   'click_environment',\n                                                   'click_deviceGroup',\n                                                   'click_os',\n                                                   'click_country',\n                                                   'click_region',\n                                                   'click_referrer_type']) for x in list_click_file_to_df]\n\nall_clicks_df = pd.concat(list_click_file_to_df_cleaned, axis=0, ignore_index=True)\n\nprint(f\"DataFrame shape : {all_clicks_df.shape}\")","c123b07c":"#Create a map to convert article_id to category\ndict_article_categories = articles_df.set_index('article_id')['category_id'].to_dict()\n\n#Get Categorie associate for each article\nall_clicks_df['category_id'] = all_clicks_df['click_article_id'].map(dict_article_categories).astype(int)\nall_clicks_df['total_click'] = all_clicks_df.groupby(['user_id'])['click_article_id'].transform('count')\nall_clicks_df['total_click_by_category_id'] = all_clicks_df.groupby(['user_id','category_id'])['click_article_id'].transform('count')\nall_clicks_df['rating'] = all_clicks_df['total_click_by_category_id'] \/ all_clicks_df['total_click']\n\nprint(f'Shape before dropping duplicate : {all_clicks_df.shape}')\nall_clicks_df = all_clicks_df.drop(['click_article_id'], axis=1)\nall_clicks_df = all_clicks_df.drop_duplicates()\nprint(f'Shape after dropping duplicate : {all_clicks_df.shape}')","3c9cf882":"#Remove not used columns\nall_clicks_df = all_clicks_df.drop(['total_click', 'total_click_by_category_id'], axis=1)\n\nall_clicks_df[all_clicks_df['user_id'] == 0]","522e44c7":"#keep ~42 000 users to build our model.\ndf = all_clicks_df[all_clicks_df['user_id'] < 42000]\nprint(f'size of our sample : {df.shape}')","d8a3d9f2":"# A reader is still needed but only the rating_scale param is requiered.\nreader = Reader(rating_scale=(0, 1))\n\n#Split our Dataframe 75% train \/ 25% test\ndata = Dataset.load_from_df(df[['user_id', 'category_id', 'rating']], reader)\ntrain_set, test_set = train_test_split(data, test_size=.25)\nprint(f'Size of test set : {len(test_set)}')","272fdcde":"%%time\n\nmodel = SVD().fit(train_set)","7d686609":"predict = model.test(test_set)\n\n#Print our Results\naccuracy.rmse(predict)\naccuracy.mse(predict)\naccuracy.mae(predict);\n\nprint(f'Prediction rating for User 0 with cat 281 : {model.predict(0, 281).est}')\nprint(f'Current rating : {df[(df[\"user_id\"] == 0) & (df[\"category_id\"] == 281)][\"rating\"].values}')","5af791d6":"#Function from https:\/\/github.com\/NicolasHug\/Surprise\/blob\/master\/examples\/top_n_recommendations.py\ndef get_top_n(predictions, n=10):\n    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n    Args:\n        predictions(list of Prediction objects): The list of predictions, as\n            returned by the test method of an algorithm.\n        n(int): The number of recommendation to output for each user. Default\n            is 10.\n    Returns:\n    A dict where keys are user (raw) ids and values are lists of tuples:\n        [(raw item id, rating estimation), ...] of size n.\n    \"\"\"\n\n    # First map the predictions to each user.\n    top_n = defaultdict(list)\n    for uid, iid, true_r, est, _ in predictions:\n        top_n[uid].append((iid, est))\n\n    # Then sort the predictions for each user and retrieve the k highest ones.\n    for uid, user_ratings in top_n.items():\n        user_ratings.sort(key=lambda x: x[1], reverse=True)\n        top_n[uid] = user_ratings[:n]\n\n    return top_n","094645c3":"top_n = get_top_n(predict, n=10)","e725d271":"#Make simple recommendation for user.\ndef make_recommendation(user_ID, top_n, df_rating, articles_df):\n    \"\"\"Return a list of recommanded articles based on the taste of the user and all recommended categories\n    Args:\n        user_id -> user id used for recommendation\n        top_n -> top-N recommendation for each user from a set of predictions\n        df_rating -> df used to train our algo\n        articles_df -> df with metadata of all articles.\n    Returns:\n        list(recommanded articles), list(recommanded categories)\n    \"\"\"\n    #Get top 5 cat and adding it to our list\n    recommanded_cat = [iid for iid, _ in top_n[user_ID]]\n    \n    #If we don't have any recommandation, use our data.\n    if not recommanded_cat:\n        recommanded_cat = df[df['user_id'] == user_ID].nlargest(1, ['rating'])['category_id'].values\n    \n    #Select 5 randoms articles for each recommanded cat.\n    random_articles_by_cat = [articles_df[articles_df['category_id'] == x]['article_id'].sample(5).values for x in recommanded_cat]\n    \n    #Select one of the recommanded cat and return 5 articles.\n    rand_category = random.sample(random_articles_by_cat, 1)\n    \n    return rand_category[0], recommanded_cat","f7c7e2f7":"recommendation, categories = make_recommendation(1, top_n, df, articles_df)\nprint(f'recommanded categories for user_id[1] : {categories}')\nprint(f'recommanded articles for user_id[1] : {recommendation}')","c4defe4b":"%%time \n\n# To use item-based cosine similarity\nsim_options = {\n    \"name\": \"cosine\",\n    \"user_based\": False,  # Compute similarities between items\n}\n\nmodel = KNNWithMeans(sim_options=sim_options).fit(train_set)","afaf8516":"predict = model.test(test_set)\n\n#Print our Results\naccuracy.rmse(predict)\naccuracy.mse(predict)\naccuracy.mae(predict);","aecb81d4":"print(f'Prediction rating for User 0 with cat 281 : {model.predict(0, 281).est}')\nprint(f'Current rating : {df[(df[\"user_id\"] == 0) & (df[\"category_id\"] == 281)][\"rating\"].values}')","337c50f5":"top_n = get_top_n(predict, n=10)","d3b61f16":"recommendation, categories = make_recommendation(3, top_n, df, articles_df)\n\nprint(f'recommanded categories for user_id[0] : {categories}')\nprint(f'recommanded articles for user_id[0] : {recommendation}')","e3625498":"df_cb = df.copy()\n\n#Add number total of rating for each\nnum_ratings = pd.DataFrame(df_cb.groupby('category_id').count()['rating']).reset_index()\ndf_cb = pd.merge(left=df_cb, right=num_ratings, on='category_id')\ndf_cb.rename(columns={'rating_x': 'rating', 'rating_y': 'numRatings'}, inplace=True)","3e95adf1":"#Category_id\ndef get_similar_category(category_id, df, n_ratings_filter=50, n_recommendations=5):\n    matrix = df.pivot_table(\n        index='user_id',\n        columns='category_id',\n        values='rating'\n    )\n    \n    similar = matrix.corrwith(matrix[category_id])\n    corr_similar = pd.DataFrame(similar, columns=['correlation'])\n    corr_similar.dropna(inplace=True)\n    \n    orig = df.copy()\n    \n    corr_with_category = pd.merge(\n        left=corr_similar, \n        right=orig,\n        on='category_id')[['category_id', 'correlation', 'numRatings']].drop_duplicates().reset_index(drop=True)\n    \n    result = corr_with_category[corr_with_category['numRatings'] > n_ratings_filter].sort_values(by='correlation', ascending=False)\n    \n    result = result[result['category_id'] != category_id]\n    return result.head(n_recommendations)","a7819722":"result = get_similar_category(412, df_cb)\n\n#Top 5 corr categories with category_id = 1\nresult.head(5)","1e8b9e1d":"train, test = sklearn.model_selection.train_test_split(df_cb, test_size=0.3, random_state=42, shuffle=True)","f822ec92":"%%time\n\nall_categories = train['category_id'].unique()\npredictions = {}\n\nfor val in all_categories:\n    predictions[val] = get_similar_category(val, train)","6d27d961":"train[train['user_id'] == 2]","bea068a4":"predictions[332]","1508d4f0":"test[test['user_id'] == 2]","6a82b4e0":"%%time\n\n# Retrieve the trainset.\ntrain_set = data.build_full_trainset()\n\n# To use item-based cosine similarity\nsim_options = {\n    \"name\": \"cosine\",\n    \"user_based\": False,  # Compute similarities between items\n}\n\nmodel = KNNWithMeans(sim_options=sim_options).fit(train_set)","f24c2244":"def predict_best_category_for_user(user_id, model, article_df):\n    predictions = {}\n    \n    #Category 1 to 460\n    for i in range(1, 460):\n        _, cat_id, _, est, err = model.predict(user_id, i)\n        \n        #Keep prediction only if we could keep it.\n        if (err != True):\n            predictions[cat_id] = est\n    \n    best_cats_to_recommend = dict(sorted(predictions.items(), key=lambda x: x[1], reverse=True)[:5])\n    \n    recommended_articles = []\n    for key, _ in best_cats_to_recommend.items():\n        recommended_articles.append(int(articles_df[articles_df['category_id'] == key]['article_id'].sample(1).values))\n    \n    #return random_articles_for_best_cat, best_cat_to_recommend\n    return recommended_articles, best_cats_to_recommend","314e4141":"results, recommended_cats = predict_best_category_for_user(1, model, articles_df)\n\nprint(f'Reco cat : {recommended_cats}')\nprint(f'5 randoms articles from cat {results}')","8d64efd1":"# Save to file in the current working directory\npkl_filename = \"pickle_surprise_model_KNNWithMeans.pkl\"\n\nwith open(pkl_filename, 'wb') as file:\n    pickle.dump(model, file)\n\n# Load from file\nwith open(pkl_filename, 'rb') as file:\n    pickle_model = pickle.load(file)","1e689878":"results, recommended_cats = predict_best_category_for_user(1, pickle_model, articles_df)\n\nprint(f'Reco cat : {recommended_cats}')\nprint(f'5 randoms articles from cat {results}')","f6987d1f":"## Dataset Content\n\n* **articles_metadata.csv** : CSV file with metadata information about all (364047) published articles\n* **articles_embeddings.pickle** : Pickle (Python 3) of a NumPy matrix containing the Article Content Embeddings (250-dimensional vectors), trained upon articles' text and metadata by the CHAMELEON's ACR module (see paper for details) for 364047 published articles.\n* **clicks** : Folder with CSV files (one per hour), containing user sessions interactions in the news portal.\n* **clicks_sample.csv** : CSV file containing user sessions interactions in the news portal.","b4b01e53":"### 2.3.3 Prediction with testSet","6ddd7c40":"## 2.1 Data Preparation\nFor our recommendation model we need to find a scoring system to help the model understand the preferences of each user.\n\nI decided to proceed like that : **Rating = Total_Click_Per_Cat_By_User \/ Total_Click_Done_By_User**","cf027d17":"# 2. Build our Recommendation Models\n\n**[Helpful link : build-recommendation-engine-collaborative-filtering](https:\/\/realpython.com\/build-recommendation-engine-collaborative-filtering\/)**","85059224":"## 2.3 KNNWithMeans (Collaborative model)\n### 2.3.1 Train Model","7c7823d8":"### 2.4.1 Content-based","aedb21a0":"## 2.4 Content-Based Prediction\nSimple content-based prediction.\n\n[towardsdatascience article that helped me](https:\/\/towardsdatascience.com\/recommender-system-in-python-part-1-preparation-and-analysis-d6bb7939091e)","306e6d40":"## **1.2 clicks_{}.csv**","a63aca3c":"after a few tries with different users i realize that the lack of information does not allow me to use this method in an efficient way...","b1edffe8":"### 2.4.2 Evaluation","4318d570":"### 2.3.3 Predictions with test set","fabb7a8a":"## 2.2 SVD (baseline)\n### 2.3.1 Train Model","51955030":"### Content","a08f9f36":"### Missing Values","23de95c2":"No value are missing.","830f523a":"**articles_metadata.csv contain 5 columns :**\n* **article_id :** ID of the article\n* **category_id :** Category ID of the article\n* **created_at_ts :** Date of creation (timestamp)\n* **publisher_id :** ID of the publisher\n* **words_count :** Total word contain in the article","ecc9ef12":"## 1.1 **articles_metadata.csv**","ed29f713":"### 2.3.2 Results","9b337a60":"# **SLAP LIKE NOW if your like this notebook ;D**\nThis notebook is a test for a project, I know it's far from perfect and I'll have to go back over it later to make it cleaner. I hope it will help some people :)","da1eea78":"pretty good ! now let's see if our model will predictions will be different from SVD","3ada655d":"# 1. Exploratory Data Analysis","0cf41d06":"**clicks_{}.csv contain 12 columns :**\n* **user_id :** user ID\n* **session_id :** Session ID\n* **session_start :** Start of the session (timestamp)\n* **session_size :** number of click\/session\n* **words_count :** Total word contain in the article\n* **click_article_id :** article ID user clicked\n* **click_timestamp :** When user clicked (timestamp)\n* **click_environment :** user env when click\n* **click_deviceGroup :** user device\n* **click_os :** user OS\n* **click_country :** localisation (country) when user clicked\n* **click_region :** localisation (region) when user clicked\n* **click_referrer_type :** ?","e3458aa3":"### Content","8651a236":"### 2.3.2 Results","d0197f38":"We don't need Total_click and total_click_by_category_id anymore.","49aa899e":"Recommended categories are the same but it's not the same order let's try another model.","f3dc2b4b":"### Distribution\n#### let's concat all clicks files and observe our datas","653d0755":"# 3. Select best model and deployment"}}