{"cell_type":{"b4ae3e0d":"code","6af1050e":"code","1356301b":"code","083e3745":"code","a20d22cb":"code","70c02c17":"code","1d4b7fd6":"code","67bb151e":"code","c9d61350":"code","234be70f":"code","e0421c17":"code","3166f0d8":"code","b267d62d":"code","215ab751":"markdown","adb31ef6":"markdown","8f98d3f8":"markdown","ca5784b7":"markdown","c4e07f7c":"markdown","a26dc5fb":"markdown","be941da8":"markdown","e40dd2ef":"markdown","1dadcdce":"markdown","77784e75":"markdown"},"source":{"b4ae3e0d":"from itertools import islice\nimport json\nimport time\nimport typing as tp\nimport urllib.error\nimport urllib.parse\nimport urllib.request\n\nimport pandas as pd\nimport seaborn as sns","6af1050e":"convoy_dataset = pd.read_csv(\"..\/input\/the-2022-trucker-strike-on-reddit\/the-2022-trucker-strike-on-reddit-comments.csv\")","1356301b":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nSOCIALGREP_API_KEY = user_secrets.get_secret(\"SOCIALGREP_API_KEY\")\n\nRETRIES = 3\nRETRY_SECONDS = 3\nMAX_ITERATIONS = 250000  # Our search can run forever - we cap it to ensure it will not.\nAPI_REQUEST_TEMPLATE = \"https:\/\/socialgrep.com\/api\/v1\/search\/{search_type}?{query}\"\n\ndef search_socialgrep(search_type: str, query: str):\n    class SearchIterator:\n        def __iter__(_) -> tp.Iterator[dict]:\n            iterations = 0\n            after = None\n            while True:\n                response_buffer, after = _fetch_socialgrep(search_type, query, after)\n                for i in response_buffer:\n                    yield i\n                    iterations += 1\n                    if iterations > MAX_ITERATIONS:\n                        return\n                if after is None:\n                    return\n\n    return SearchIterator()\n\ndef _fetch_socialgrep(search_type: str, query: str, after: tp.Optional[int]):\n    map_query: tp.Dict[str, tp.Any] = {\"query\": query}\n    if after:\n        map_query[\"after\"] = after\n    text_query = urllib.parse.urlencode(map_query)\n    last_exception: tp.Optional[urllib.error.HTTPError] = None\n    for attempt_no in range(RETRIES + 1):\n        try:\n            with urllib.request.urlopen(urllib.request.Request(\n                API_REQUEST_TEMPLATE.format(\n                    search_type=search_type,\n                    query=text_query,\n                ),\n                headers={\n                    \"Accept\": \"application\/json\",\n                    \"Authorization\": f\"Bearer {SOCIALGREP_API_KEY}\",\n                    \"User-Agent\": \"Kaggle Dataset Creator\"\n                }\n            )) as result:\n                json_result = json.loads(result.read())\n                if \"sort_key\" in json_result:\n                    return json_result[\"data\"], json_result[\"sort_key\"][0][0]\n                return json_result[\"data\"], None\n        except urllib.error.HTTPError as e:\n            if e.code == 400:\n                raise ValueError(\"Invalid search query\")\n            last_exception = e\n            if attempt_no < RETRIES:\n                time.sleep(RETRY_SECONDS)\n\n        assert last_exception is not None\n        raise last_exception\n        \ndef translate_to_export(source_row):\n    row = source_row.copy()\n    row[\"subreddit.id\"] = row[\"subreddit\"][\"id\"]\n    row[\"subreddit.name\"] = row[\"subreddit\"][\"name\"]\n    row[\"subreddit.nsfw\"] = row[\"subreddit\"][\"nsfw\"]\n    if \"score\" not in row:\n        row[\"score\"] = None\n    del row[\"subreddit\"]\n    for k in row:\n        if row[k] is None:\n            row[k] = \"\"\n        elif isinstance(row[k], bool):\n            row[k] = str(row[k]).lower()\n        else:\n            row[k] = str(row[k])\n    return row","083e3745":"dataset_comments = [translate_to_export(i) for i in search_socialgrep(\"comments\", f\"\/r\/ottawa,before:2019-02-01,after:2019-01-01\")]\nregular_dataset = pd.DataFrame(dataset_comments, columns=[\"type\", \"id\", \"subreddit.id\", \"subreddit.name\", \"subreddit.nsfw\", \"created_utc\", \"permalink\", \"body\", \"sentiment\", \"score\"])\nregular_dataset.head()","a20d22cb":"convoy_dataset[\"timestamp\"] = pd.to_datetime(convoy_dataset[\"created_utc\"],unit='s')\nconvoy_dataset[\"hour\"] = convoy_dataset[\"timestamp\"].dt.hour\nconvoy_counts = convoy_dataset.groupby([\"hour\"]).count()\n\nregular_dataset[\"timestamp\"] = pd.to_datetime(regular_dataset[\"created_utc\"],unit='s')\nregular_dataset[\"hour\"] = regular_dataset[\"timestamp\"].dt.hour\nregular_counts = regular_dataset.groupby([\"hour\"]).count()\n\nresult = {}\nfor i in convoy_counts.iloc:\n    key = i.name\n    value = i[\"id\"]\n    result[key] = value \/ sum([i[\"id\"] for i in convoy_counts.iloc]) * 100\n\nresult_df = []\nfor i in regular_counts.iloc:\n    key = i.name\n    value = i[\"id\"]\n    result_df.append({\"key\": key, \"Jan 2019\": value \/ sum([i[\"id\"] for i in regular_counts.iloc]) * 100, \"Convoy\": result[key]})\n\nresult_df = pd.DataFrame(result_df)\ntidy = result_df.melt(id_vars='key', var_name='Period').rename(columns=str.title)\nplot = sns.lineplot(x=\"Key\", y=\"Value\", hue=\"Period\", data=tidy)\nplot.set_xlabel(\"Hour (UTC)\")\nplot.set_ylabel(\"Comment percentage\")\nNone","70c02c17":"convoy_dataset[\"timestamp\"] = pd.to_datetime(convoy_dataset[\"created_utc\"],unit='s')\nconvoy_dataset[\"hour\"] = convoy_dataset[\"timestamp\"].dt.hour\nconvoy_counts = convoy_dataset.groupby([\"hour\"]).count()\n\nregular_dataset[\"timestamp\"] = pd.to_datetime(regular_dataset[\"created_utc\"],unit='s')\nregular_dataset[\"hour\"] = regular_dataset[\"timestamp\"].dt.hour\nregular_counts = regular_dataset.groupby([\"hour\"]).count()\n\nresult = {}\nfor i in convoy_counts.iloc:\n    key = i.name\n    value = i[\"id\"]\n    result[key] = value \/ sum([i[\"id\"] for i in convoy_counts.iloc]) * 100\n\nresult_df = []\nfor i in regular_counts.iloc:\n    key = (i.name + 1) % 24\n    value = i[\"id\"]\n    result_df.append({\"key\": key, \"Jan 2019 (adjusted)\": value \/ sum([i[\"id\"] for i in regular_counts.iloc]) * 100, \"Convoy\": result[key]})\n\nresult_df = pd.DataFrame(result_df)\ntidy = result_df.melt(id_vars='key', var_name='Period').rename(columns=str.title)\nplot = sns.lineplot(x=\"Key\", y=\"Value\", hue=\"Period\", data=tidy)\nplot.set_xlabel(\"Hour (UTC)\")\nplot.set_ylabel(\"Comment percentage\")\nNone","1d4b7fd6":"new_dataset_comments = [translate_to_export(i) for i in search_socialgrep(\"comments\", f\"\/r\/ottawa,after:2022-01-25\")]","67bb151e":"potential_megathreads = list(search_socialgrep(\"posts\", \"convoy,megathread,\/r\/ottawa\"))[::-1]\npotential_megathreads = [i for i in potential_megathreads if i[\"title\"].startswith('Convoy Megathread #')]\n\n# The first trucker protest megathread has a different name: https:\/\/old.reddit.com\/r\/ottawa\/comments\/sdcm6x\/\nmegathread_ids = set([\"sdcm6x\"] + [i[\"id\"] for i in potential_megathreads])","c9d61350":"new_dataset_comments_filtered = [i for i in new_dataset_comments if i[\"permalink\"].split(\"\/\")[6] not in megathread_ids]","234be70f":"print(len(new_dataset_comments_filtered) \/ len(new_dataset_comments))","e0421c17":"non_convoy_dataset = pd.DataFrame(new_dataset_comments_filtered, columns=[\"type\", \"id\", \"subreddit.id\", \"subreddit.name\", \"subreddit.nsfw\", \"created_utc\", \"permalink\", \"body\", \"sentiment\", \"score\"])\nnon_convoy_dataset.head()\n\nnon_convoy_dataset[\"timestamp\"] = pd.to_datetime(non_convoy_dataset[\"created_utc\"],unit='s')\nnon_convoy_dataset[\"hour\"] = non_convoy_dataset[\"timestamp\"].dt.hour\nnon_convoy_counts = non_convoy_dataset.groupby([\"hour\"]).count()\n\nresult = {}\nfor i in convoy_counts.iloc:\n    key = i.name\n    value = i[\"id\"]\n    result[key] = value \/ sum([i[\"id\"] for i in convoy_counts.iloc]) * 100\n    \n    \nresult_nc = {}\nfor i in non_convoy_counts.iloc:\n    key = i.name\n    value = i[\"id\"]\n    result_nc[key] = value \/ sum([i[\"id\"] for i in non_convoy_counts.iloc]) * 100\n\nresult_df = []\nfor i in regular_counts.iloc:\n    key = i.name\n    value = i[\"id\"]\n    result_df.append({\"key\": key, \"January 2019\": value \/ sum([i[\"id\"] for i in regular_counts.iloc]) * 100, \"Convoy\": result[key], \"Non-Convoy\": result_nc[key]})\n\nresult_df = pd.DataFrame(result_df)\ntidy = result_df.melt(id_vars='key', var_name='Period').rename(columns=str.title)\nplot = sns.lineplot(x=\"Key\", y=\"Value\", hue=\"Period\", data=tidy)\nplot.set_xlabel(\"Hour (UTC)\")\nplot.set_ylabel(\"Comment percentage\")\nNone","3166f0d8":"from collections import Counter\nfrom nltk.tokenize import wordpunct_tokenize\n\nN_DIMENSIONS = 100\n\ndef tokenize_dataframe(df):\n    return [wordpunct_tokenize(i[\"body\"]) for i in df.iloc]\n\ndef create_corpus_for_delta(tokenized):\n    return [i[0] for i in Counter([j for i in tokenized for j in i]).most_common(N_DIMENSIONS)]\n\ndef delta(tokenized, corpus):\n    flat_tokenized = [j for i in tokenized for j in i]\n    counts = Counter(flat_tokenized)\n    delta_vector = []\n    for i in corpus:\n        delta_vector.append(counts[i] \/ len(flat_tokenized) if i in counts else 0)\n    return delta_vector\n\ndef l1_distance(delta_1, delta_2):\n    return sum([abs(delta_1[i] - delta_2[i]) for i in range(N_DIMENSIONS)])\n\n\nregular_test = regular_dataset.sample(frac=0.2, axis=0)\nregular_train = regular_dataset.drop(index=regular_test.index)\n\nregular_tokens = tokenize_dataframe(regular_train)\nregular_test_tokens = tokenize_dataframe(regular_test)\nnon_convoy_tokens = tokenize_dataframe(non_convoy_dataset)\nconvoy_tokens = tokenize_dataframe(convoy_dataset)\n\ncorpus = create_corpus_for_delta(regular_tokens)","b267d62d":"regular_delta = delta(regular_tokens, corpus)\nregular_test_delta = delta(regular_test_tokens, corpus)\nnon_convoy_delta = delta(non_convoy_tokens, corpus)\nconvoy_delta = delta(convoy_tokens, corpus)\n\nprint(f\"Delta between Jan 2019 and Jan 2019 (reference): {l1_distance(regular_delta, regular_test_delta)}\")\nprint(f\"Delta between Jan 2019 and non-convoy comments: {l1_distance(regular_delta, non_convoy_delta)}\")\nprint(f\"Delta between Jan 2019 and convoy comments: {l1_distance(regular_delta, convoy_delta)}\")\nprint(f\"Delta between non-convoy and convoy comments: {l1_distance(non_convoy_delta, convoy_delta)}\")","215ab751":"## First, let's compare their time distributions.\nThat way we can see if posters from another timezone have arrived. It's not a surefire method - some have complained about the protests interfering with their sleep schedule - but it's one way to research the data nonetheless.","adb31ef6":"The Freedom Convoy has been a news sensation for the past week. \n\n\n### On Reddit?\n\nThis notebook will investigate the convoy's effect on the subreddit \/r\/Ottawa. To do this, we will investigate the [The 2022 Freedom Convoy on Reddit](https:\/\/www.kaggle.com\/pavellexyr\/the-2022-trucker-strike-on-reddit) dataset.\n\n### Brigading?\n\nThe social media network site Reddit consists of many small subforum communities called _subreddits_. While people usually keep to their own subreddits, sometimes major events can cause them to flock to other communities to talk to a different audience. \n\n![brigading.png](attachment:f71f6943-7709-498f-8e28-5e3056ed8675.png)\n\n_Not a cozy time._\n\nSince the subreddits may have different values and community norms, sometimes these arrivals can introduce friction into a subreddit. This is called _brigading_.\n\n---\n\nIn this notebook, we will investigate some methods used to detect anomalies in forum posts to determine if \/r\/Ottawa's convoy megathreads have had any noticeable shift in comment posting. From this we can make some assumptions about brigading.\n\n## Let's start by making two datasets.\n\nOne will be the convoy dataset from Kaggle. Let's create the second one using [SocialGrep's](https:\/\/socialgrep.com) API.","8f98d3f8":"We can see the difference pretty clearly.\n\nOf course, that does not mean much - a lot of [confounding](https:\/\/en.wikipedia.org\/wiki\/Confounding) factors could be at play - for one, modern stylometry methods do not work as well when the topics discussed are different. In that way, the value of the above cells is mostly educational.\n\n---\n\nLooking at the above two metrics, we see that the convoy megathreads aren't a normal slice of \/r\/Ottawa's life. ","ca5784b7":"We see that even after adjustment, the graph is a bit more flat - the orange line curves down a little bit later than usual, and picks up earlier.\n\nIt makes sense for more people to post in the convoy megathread in the off hours - not only might they be attracting outside attention, but also the protest goes on into the night - some people might have trouble sleeping and turn to Reddit. (Or to the streets, like [this guy](https:\/\/www.reddit.com\/r\/PublicFreakout\/comments\/sie642\/freedom_truckers_have_been_honking_their_horns\/).)\n\n### What's up with that hour, though?\nTo find out, we can look at all the other comments in the subreddit at the time. If the distribution is closer to the convoy's, we can assume it's just Ottawa citizens being groggy. If not, the convoy threads are being brigaded - judging by the time zone, by someone in the CST area.","c4e07f7c":"While it's strange to see even the non-convoy threads pick up in activity an hour later than they used to, we can clearly see that the convoy megathread is still thriving even after the rest of \/r\/Ottawa goes to bed. Given that, a brigade is pretty likely.\n\n---\n\n## Writing styles can also be compared.\n\nAnother way to identify authorship in disputed cases is _stylometric analysis_ - by analyzing the way text is written, we can infer certain things about the author's identity.\n\nA classic example in this field is the [Federalist papers problem](https:\/\/en.wikipedia.org\/wiki\/The_Federalist_Papers#Disputed_essays) - a set of essays written by three people. Authorship of some is disputed - to solve the disputes, machine learning methods are usually used.\n\nCommunity authorship is a topic less researched (The author doesn't know much on the topic - do you? Comment with some papers!), but we will attempt it nonetheless.\n\n### Burrows's Delta\n\nA common way to compare the authorship of two papers is to use the so-called Delta method, first proposed by John F. Burrows in [this paper](https:\/\/sci-hub.se\/https:\/\/academic.oup.com\/dsh\/article-abstract\/17\/3\/267\/929277?login=false) - the simplified version we are going to use will simply create a 100-dimensional vector of frequencies of top 100 words in the tokenized corpus, and then look at the distance between the reference corpus from Jan 2019 and the convoy corpus provided in the dataset. ","a26dc5fb":"We will take January of 2019 in \/r\/Ottawa as a reference interval.","be941da8":"Taking a couple cells from the [Convoy Megathread reproduction notebook](https:\/\/www.kaggle.com\/pavellexyr\/the-2022-freedom-convoy-on-reddit-reproduce-it), let's find out which threads were designated for discussion of the strike - and remove them from our dataset.","e40dd2ef":"_(Only about 20% of the comments are not part of the megathreads!)_","1dadcdce":"### The difference is visible.\n\nThat's strange! Those differ by one hour exactly. Take a look at how well they match if we shift Jan 2019 one hour forward:","77784e75":"We won't share our `SOCIALGREP_API_KEY` here - if you would like to reproduce the results, it can be gotten from [this](https:\/\/socialgrep.com\/api\/keys) page.\n\n_(And as a little bonus for reading this all the way up to here, [contact us](https:\/\/socialgrep.com\/contact) with the message `Give me some Kaggle credits :)` before March 2022 to get some API credits to do your NLP project with.)_ "}}