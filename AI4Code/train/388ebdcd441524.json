{"cell_type":{"2db23d5c":"code","fb1afbae":"code","3697301a":"code","91ea5073":"code","9a45f9e6":"code","5b120ce1":"code","541ec0ad":"code","45dcc7ca":"code","e1c74e62":"code","0453a202":"code","87ee2c09":"code","21b23f09":"code","585f97bf":"markdown","f78d6efe":"markdown","dcb63dcd":"markdown","58ed5944":"markdown"},"source":{"2db23d5c":"import pandas as pd\nimport numpy as np\n\nimport os \n\nimport matplotlib.pyplot as plt\nfrom PIL import Image \nfrom glob import glob\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nimport warnings\nwarnings.filterwarnings('ignore')","fb1afbae":"train_path = '..\/input\/fruits\/fruits-360\/Training\/'\ntest_path = '..\/input\/fruits\/fruits-360\/Test\/'","3697301a":"img = load_img(train_path + \"Apple Braeburn\/0_100.jpg\", target_size=(128,128))","91ea5073":"image_shape = img_to_array(img)","9a45f9e6":"className = glob(train_path + '\/*')\nnumber_of_class = len(className)\nprint(number_of_class)","5b120ce1":"batch_size = 32","541ec0ad":"model = Sequential()\nmodel.add(Conv2D(32, (3,3), input_shape=image_shape.shape))\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D())\n\nmodel.add(Conv2D(32, (3,3),))\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D())\n\nmodel.add(Conv2D(64, (3,3),))\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D())\n\nmodel.add(Flatten())\nmodel.add(Dense(1024))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(number_of_class))\nmodel.add(Activation(\"softmax\"))","45dcc7ca":"train_datagen = ImageDataGenerator(rescale = 1.\/255,\n                  shear_range = 0.3,\n                  horizontal_flip=True,\n                  vertical_flip=False,\n                  zoom_range = 0.3\n                  )\ntest_datagen  = ImageDataGenerator(rescale = 1.\/255)\n\ntrain_generator = train_datagen.flow_from_directory(train_path,\n                                                    target_size = image_shape.shape[:2],\n                                                    batch_size = batch_size,\n                                                    color_mode= \"rgb\",\n                                                    class_mode = \"categorical\")\ntest_generator = test_datagen.flow_from_directory(test_path,\n                                                  target_size = image_shape.shape[:2],\n                                                  batch_size = batch_size,\n                                                  color_mode= \"rgb\",\n                                                  class_mode = \"categorical\")","e1c74e62":"model.compile(loss = \"categorical_crossentropy\",\n             optimizer = \"adam\", #rmsprop\n             metrics = [\"accuracy\"])","0453a202":"early = EarlyStopping(monitor='val_loss',patience=5)\nreduce = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.2, mil_lr=0.001)","87ee2c09":"hist = model.fit_generator(generator = train_generator,\n                           steps_per_epoch = 1600 \/\/ batch_size,\n                           epochs = 50,\n                           validation_data = test_generator,\n                           validation_steps = 800 \/\/ batch_size,\n                           callbacks=[early, reduce])","21b23f09":"plt.style.use('fivethirtyeight')\n\nfig, axes = plt.subplots(1, 2, figsize=(15,5))\naxes[0].plot(hist.history['accuracy'])\naxes[0].plot(hist.history['val_accuracy'])\naxes[0].set_xlabel('Epochs')\naxes[0].set_ylabel('Accuracy')\naxes[0].legend(['Accuracy in Train','Accuracy in Test'])\naxes[0].grid(True)\n\naxes[1].plot(hist.history['loss'])\naxes[1].plot(hist.history['val_loss'])\naxes[1].set_xlabel('Epochs')\naxes[1].set_ylabel('Erro')\naxes[1].legend(['Erro in Train','Erro in Test'])\naxes[1].grid(True)","585f97bf":"## Number of class","f78d6efe":"## Shape images","dcb63dcd":"# <p style=\"background-color:#80ccff; font-family:newtimeroman; font-size:150%; text-align:center; border-radius:  80px 5px; padding-top:8px; padding-bottom:8px;\">Read Dataset<\/p>","58ed5944":"# <p style=\"background-color:#80ccff; font-family:newtimeroman; font-size:150%; text-align:center; border-radius:  80px 5px; padding-top:8px; padding-bottom:8px;\">Model<\/p>"}}