{"cell_type":{"e9ef55f8":"code","8e985a28":"code","8efb6dd4":"code","1b23b0df":"code","04212c1c":"code","c42c5880":"code","ff261433":"code","d4b70ff8":"code","49ed67db":"code","575e4a9b":"code","d9ef0018":"code","017cf9f0":"code","115c6aa0":"code","dbed2087":"code","aeb655d4":"code","770246e8":"code","6ff9f8b7":"code","5fa5f654":"code","933da69a":"code","b168ac67":"code","4ee9a449":"code","f0916988":"code","35fcdfdc":"code","12ce31e7":"code","d89bf7a6":"code","78cbd625":"code","a29699b4":"code","9f03f971":"code","a5a8828b":"code","8067ceb6":"code","5cea8b45":"code","39bf94ec":"code","0314be4d":"code","fa5c03be":"code","416a7636":"code","9062f86a":"code","e8d2e8cc":"code","955b3acd":"code","feef1f41":"code","aba8e5f4":"code","b79cf9c2":"code","478b4675":"code","d0071848":"code","543648b6":"code","c12d07d4":"code","d0255de7":"code","86ce1bff":"code","0a121176":"code","b284e2f6":"code","fa8329dc":"code","15e617a1":"code","efcd5c67":"code","1ec086ba":"code","40137ca3":"code","d7174c34":"code","e1249d61":"code","2dcd103f":"code","aaf97f28":"code","26577562":"code","e950ddfe":"code","a2d93f37":"code","c2367186":"code","2d6c6543":"code","9eca2cc7":"code","cad40213":"code","cc1a59cd":"code","0a06c8e9":"code","239a98d3":"markdown","7b925eb3":"markdown","7588afcc":"markdown","c8611c69":"markdown","3ff00da9":"markdown","ea5f3ac4":"markdown","4316d6a2":"markdown","5bec3ce8":"markdown","bc365546":"markdown","75e8dfbb":"markdown","51aeac8f":"markdown","6549ed67":"markdown","d75dc771":"markdown","7f09c73a":"markdown","0253f1bd":"markdown"},"source":{"e9ef55f8":"# Basic Libraries\nimport numpy as np\nimport pandas as pd\nimport operator\nimport re\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(\"ignore\")\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\n# Preprocessing\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler \nfrom sklearn.pipeline import _name_estimators\nfrom sklearn.base import BaseEstimator\nfrom sklearn.base import ClassifierMixin\nfrom sklearn.base import clone\nfrom sklearn.externals import six\n\n# Evaluation\nfrom sklearn import metrics\nfrom sklearn import linear_model, datasets \nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import train_test_split, cross_val_score \nfrom sklearn.neighbors import LocalOutlierFactor\n\n# Classifier (machine learning algorithm) \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.base import BaseEstimator\nfrom sklearn.base import ClassifierMixin\nfrom sklearn.externals import six\nfrom sklearn.base import clone\nfrom sklearn.pipeline import _name_estimators","8e985a28":"# from google.colab import drive \n# drive.mount('\/content\/gdrive')\n# dataset = pd.read_csv(\"gdrive\/My Drive\/Colab Notebooks\/Churn_Modelling.csv\", header = 0)","8efb6dd4":"dataset = pd.read_csv('..\/input\/Churn_Modelling.csv', header = 0)","1b23b0df":"# Tmp data\ndataset_tmp = dataset.copy()\ndataset_tmp.head()","04212c1c":"class MajorityVoteClassifier(BaseEstimator, ClassifierMixin):\n    \"\"\" A majority vote ensemble classifier\n    Parameters\n    classifiers : array-like, shape = [n_classifiers]  Different classifiers for the ensemble\n    vote : str, {'classlabel', 'probability'} (default='label')\n      If 'classlabel' the prediction is based on the argmax of class labels. Else if 'probability', the argmax of the sum of probabilities is used to predict the class label (recommended for calibrated classifiers).\n    weights : array-like, shape = [n_classifiers], optional (default=None)\n      If a list of `int` or `float` values are provided, the classifiers are weighted by importance; Uses uniform weights if `weights=None`.\n    \"\"\"\n    def __init__(self, classifiers, vote='classlabel', weights=None):\n        self.classifiers = classifiers\n        self.named_classifiers = {key: value for key, value in _name_estimators(classifiers)}\n        self.vote = vote\n        self.weights = weights\n    def fit(self, X, y):\n        \"\"\" Fit classifiers. Parameters\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features] Matrix of training samples.\n        y : array-like, shape = [n_samples] Vector of target class labels.\n        Returns self : object\n        \"\"\"\n        if self.vote not in ('probability', 'classlabel'):\n            raise ValueError(\"vote must be 'probability' or 'classlabel'\" \"; got (vote=%r)\" % self.vote)\n        if self.weights and len(self.weights) != len(self.classifiers):\n            raise ValueError('Number of classifiers and weights must be equal''; got %d weights, %d classifiers' %   \n            (len(self.weights), len(self.classifiers)))\n        # Use LabelEncoder to ensure class labels start with 0, which is important for np.argmax call in self.predict\n        self.lablenc_ = LabelEncoder()\n        self.lablenc_.fit(y)\n        self.classes_ = self.lablenc_.classes_\n        self.classifiers_ = []\n        for clf in self.classifiers:\n            fitted_clf = clone(clf).fit(X, self.lablenc_.transform(y))\n            self.classifiers_.append(fitted_clf)\n        return self\n    def predict(self, X):\n        \"\"\" Predict class labels for X.\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features] Matrix of training samples.\n        Returns ----------\n        maj_vote : array-like, shape = [n_samples] Predicted class labels.\n        \"\"\"\n        if self.vote == 'probability':\n            maj_vote = np.argmax(self.predict_proba(X), axis=1)\n        else:  # 'classlabel' vote\n            #  Collect results from clf.predict calls\n            predictions = np.asarray([clf.predict(X) for clf in self.classifiers_]).T\n            maj_vote = np.apply_along_axis( lambda x: np.argmax(np.bincount(x, weights=self.weights)),\n                                      axis=1,\n                                      arr=predictions)\n        maj_vote = self.lablenc_.inverse_transform(maj_vote)\n        return maj_vote\n    def predict_proba(self, X):\n        \"\"\" Predict class probabilities for X.\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and n_features is the number of features.\n        Returns\n        avg_proba : array-like, shape = [n_samples, n_classes] Weighted average probability for each class per sample.\n        \"\"\"\n        probas = np.asarray([clf.predict_proba(X)  for clf in self.classifiers_])\n        avg_proba = np.average(probas, axis=0, weights=self.weights)\n        return avg_proba\n    def get_params(self, deep=True):\n        \"\"\" Get classifier parameter names for GridSearch\"\"\"\n        if not deep:\n            return super(MajorityVoteClassifier, self).get_params(deep=False)\n        else:\n            out = self.named_classifiers.copy()\n            for name, step in six.iteritems(self.named_classifiers):\n                for key, value in six.iteritems(step.get_params(deep=True)):\n                    out['%s__%s' % (name, key)] = value\n            return out\n\n# Split Train and Test and check shape \ndef SplitDataFrameToTrainAndTest(DataFrame, TrainDataRate, TargetAtt):\n    # gets a random TrainDataRate % of the entire set\n    training = DataFrame.sample(frac=TrainDataRate, random_state=1)\n    # gets the left out portion of the dataset\n    testing = DataFrame.loc[~DataFrame.index.isin(training.index)]\n\n    X_train = training.drop(TargetAtt, 1)\n    y_train = training[[TargetAtt]]\n    X_test = testing.drop(TargetAtt, 1)\n    y_test = testing[[TargetAtt]]\n    return X_train, y_train, X_test, y_test\n    \ndef PrintTrainTestInformation(X_train, y_train, X_test, y_test):\n    print(\"Train rows and columns : \", X_train.shape)\n    print(\"Test rows and columns : \", X_test.shape)\n\ndef DrawJointPlot(DataFrame, XAtt, yAtt, bins = 20):\n    sns.set(color_codes=True)\n    sns.distplot(data[XAtt], bins=bins);\n    df = pd.DataFrame(DataFrame, columns=[XAtt,yAtt])\n    df = df.reset_index(drop=True)\n    sns.jointplot(x=XAtt, y=yAtt, data=df)\n    \ndef DrawBoxplot2(DataFrame, xAtt, yAtt, hAtt=\"N\/A\"):\n    plt.figure()\n    if(hAtt == \"N\/A\"):\n        sns.boxplot(x=xAtt, y=yAtt,  data=DataFrame)\n    else:\n        sns.boxplot(x=xAtt, y=yAtt,  hue=hAtt,  data=DataFrame)\n    plt.show()\n    \ndef DrawBarplot(DataFrame, att):\n    Distribution = DataFrame[att].value_counts()\n    Distribution = pd.DataFrame({att:Distribution.index, 'Freq':Distribution.values})\n    Distribution = Distribution.sort_values(by=att, ascending=True)\n    plt.bar(Distribution[att], Distribution[\"Freq\"])\n    plt.xticks(Distribution[att])\n    plt.ylabel('Frequency')\n    plt.title('Barplot of ' + att)\n    plt.show()   \n    \ndef DrawCountplot(DataFrame, att, hatt=\"N\/A\"):\n    if(hatt == \"N\/A\"):\n        sns.countplot(x=att, data=DataFrame)\n    else:\n        sns.countplot(x=att, hue=hatt, data=DataFrame)\n    plt.show()\n    \ndef DrawHistogram(DataFrame, att):\n    plt.figure()\n    DataFrame[att].hist(edgecolor='black', bins=20)\n    plt.title(att)\n    plt.show()\n    \n# Detect outlier in each feature\ndef DetectOutlierByIQR(DataFrame, AttList, Rate = 3.0):\n    OutlierIdx = []\n    for att in AttList:\n        AttData = DataFrame.loc[:, att]\n        lowerq = AttData.quantile(0.25)\n        upperq = AttData.quantile(0.75)\n        IQR = upperq - lowerq\n        threshold_upper = (IQR * Rate) + upperq\n        threshold_lower = lowerq - (IQR * Rate)\n        AttOutlierIdx = set(AttData[AttData.apply(lambda x: x > threshold_upper\n                                                    or x < threshold_lower)].index.get_values())\n        OutlierIdx = set(OutlierIdx) | AttOutlierIdx\n        # print(\"Min, Max and IQR : %f, %f, and %f\" % (AttData.min(), AttData.max(), IQR))\n        # print(\"Upper Fence and Lower Fence : %f and %f\" % (threshold_lower, threshold_upper))\n        # print(\"OutlierIdx : \" + str(OutlierIdx))\n        # print(att + \" \"  + str(len(AttOutlierIdx)) + \" Outlier Idx : \" + str(AttOutlierIdx))\n\n    OutlierIdx = list(OutlierIdx)\n    OutlierIdx = sorted(OutlierIdx)\n    return OutlierIdx\n      \n# Detect outlier in group features\ndef DetectOutlierByLOF(DataFrame, AttList, LOFThresh=3.0, neighbors = 10):\n    clf = LocalOutlierFactor(n_neighbors=neighbors)\n    AttData = DataFrame.loc[:, AttList].values\n    y_pred = clf.fit_predict(AttData)\n    AttData_scores = -1 * clf.negative_outlier_factor_\n    LOFFactorData = pd.DataFrame(AttData_scores, columns=['LOF'])\n    LOFFactorData = LOFFactorData.sort_values('LOF', ascending=False)\n    LOFFactorData = LOFFactorData.reset_index(drop=False)\n    # print(LOFFactorData.loc[0:10, :])\n    OutlierThreshold = LOFThresh\n    SuspectOutlierData = LOFFactorData[LOFFactorData['LOF'].apply(lambda x: x > OutlierThreshold)]\n    OutlierIdx = SuspectOutlierData.loc[:, 'index'].tolist()\n    # print(\"OutlierIdx : \" + str(OutlierIdx))\n    return OutlierIdx, LOFFactorData\n      \ndef RemoveRowsFromDataFrame(DataFrame, RowIdxList = []):\n    DataFrame = DataFrame.drop(RowIdxList)\n    DataFrame = DataFrame.reset_index(drop=True)\n    return DataFrame\n\ndef NaiveBayesLearning(DataTrain, TargetTrain):\n    NBModel = GaussianNB()\n    NBModel.fit(DataTrain, TargetTrain.values.ravel())\n    return NBModel\n\ndef NaiveBayesTesting(NBModel,DataTest, TargetTest):\n    PredictTest = NBModel.predict(DataTest)\n    Accuracy = accuracy_score(TargetTest, PredictTest)\n    return Accuracy, PredictTest\n\ndef LogisticRegressionLearning(DataTrain, TargetTrain):\n    logreg = LogisticRegression()\n    # Training by Logistic Regression\n    logreg.fit(DataTrain, TargetTrain.values.ravel())\n    return logreg\n\ndef LogisticRegressionTesting(LRModel,DataTest, TargetTest):\n    logreg = LRModel\n    PredictTest = logreg.predict(DataTest)\n    Accuracy = accuracy_score(TargetTest, PredictTest)\n    # print('Logistic regression accuracy: {:.3f}'.format(Accuracy))\n    return Accuracy, PredictTest\n\ndef RandomForestLearning(DataTrain, TargetTrain):\n    rf = RandomForestClassifier()\n    rf.fit(DataTrain, TargetTrain.values.ravel())\n    return rf\n\ndef RandomForestTesting(RFModel,DataTest, TargetTest):\n    PredictTest = RFModel.predict(DataTest)\n    Accuracy = accuracy_score(TargetTest, PredictTest)\n    # print('Random Forest Accuracy: {:.3f}'.format(accuracy_score(TargetTest, PredictTest)))\n    return Accuracy, PredictTest\n\ndef SVMLearning(DataTrain, TargetTrain, ClassifierType = \" \"):\n    if(ClassifierType == 'Linear'):\n        svc = SVC(kernel=\"linear\", C=0.025)\n        # print('SVM Linear processing')\n    # Radial basis function kernel\n    elif (ClassifierType == 'RBF'):\n        svc = SVC(gamma=2, C=1)\n        # print('SVM RBF processing')\n    else:\n        svc = SVC()\n        # print('SVM Default processing')\n    svc.fit(DataTrain, TargetTrain.values.ravel())\n    return svc\n\ndef SVMTesting(SVMModel, DataTest, TargetTest):\n    PredictTest = SVMModel.predict(DataTest)\n    Accuracy = accuracy_score(TargetTest, PredictTest)\n    # print('Support Vector Machine Accuracy: {:.3f}'.format(accuracy_score(TargetTest, PredictTest)))\n    return Accuracy, PredictTest\n\ndef KNNLearning(DataTrain, TargetTrain, K = 3):\n    neigh = KNeighborsClassifier(n_neighbors=K)\n    neigh.fit(DataTrain, TargetTrain.values.ravel())\n    return neigh\n\ndef KNNTesting(KNNModel,DataTest, TargetTest):\n    PredictTest = KNNModel.predict(DataTest)\n    Accuracy = accuracy_score(TargetTest, PredictTest)\n    # print('KNN Accuracy: {:.3f}'.format(accuracy_score(TargetTest, PredictTest)))\n    return Accuracy, PredictTest\n\ndef ANNLearning(DataTrain, TargetTrain):\n    ANNModel = MLPClassifier(alpha=1)\n    ANNModel.fit(DataTrain, TargetTrain.values.ravel())\n    return ANNModel\n\ndef ANNTesting (ANNModel, DataTest, TargetTest):\n    PredictTest = ANNModel.predict(DataTest)\n    Accuracy = accuracy_score(TargetTest, PredictTest)\n    # print('Neural Net Accuracy: {:.3f}'.format(Accuracy))\n    return Accuracy, PredictTest\n\n# Continuous Data Plot\ndef ContPlot(df, feature_name, target_name, palettemap, hue_order, feature_scale): \n    df['Counts'] = \"\" # A trick to skip using an axis (either x or y) on splitting violinplot\n    fig, [axis0,axis1] = plt.subplots(1,2,figsize=(10,5))\n    sns.distplot(df[feature_name], ax=axis0);\n    sns.violinplot(x=feature_name, y=\"Counts\", hue=target_name, hue_order=hue_order, data=df,\n                   palette=palettemap, split=True, orient='h', ax=axis1)\n    axis1.set_xticks(feature_scale)\n    plt.show()\n\n# Categorical\/Ordinal Data Plot\ndef CatPlot(df, feature_name, target_name, palettemap): \n    fig, [axis0,axis1] = plt.subplots(1,2,figsize=(10,5))\n    df[feature_name].value_counts().plot.pie(autopct='%1.1f%%',ax=axis0)\n    sns.countplot(x=feature_name, hue=target_name, data=df,\n                  palette=palettemap,ax=axis1)\n    plt.show()\n\ndef MachineLearningModelEvaluate(X_train, y_train, X_test, y_test):\n    NBModel = NaiveBayesLearning(X_train, y_train)\n    NBAccuracy,NBPredictTest = NaiveBayesTesting(NBModel,X_test, y_test)\n    print('Naive Bayes accuracy: {:.3f}'.format(NBAccuracy))\n\n    LRModel = LogisticRegressionLearning(X_train, y_train)\n    LRAccuracy,LRPredictTest = LogisticRegressionTesting(LRModel,X_test, y_test)\n    print('Logistic Regression accuracy: {:.3f}'.format(LRAccuracy))\n\n    RFModel = RandomForestLearning(X_train, y_train)\n    RFAccuracy,RFPredictTest = RandomForestTesting(RFModel,X_test, y_test)\n    print('Random Forest accuracy: {:.6f}'.format(RFAccuracy))\n\n    LiSVMModel = SVMLearning(X_train, y_train)\n    LiSVMAccuracy,LiSVMPredictTest = SVMTesting(LiSVMModel, X_test, y_test)\n    print('Linear SVM accuracy: {:.6f}'.format(LiSVMAccuracy))\n\n    RBFSVMModel = SVMLearning(X_train, y_train, 'RBF')\n    RBFSVMAccuracy,RBFSVMPredictTest = SVMTesting(RBFSVMModel, X_test, y_test)\n    print('RBF SVM accuracy: {:.6f}'.format(RBFSVMAccuracy))\n\n    KNNModel = KNNLearning(X_train, y_train)\n    KNNAccuracy,KNNPredictTest = KNNTesting(KNNModel,X_test, y_test)\n    print('K Nearest Neighbor accuracy: {:.6f}'.format(KNNAccuracy))\n\n    ANNModel = ANNLearning(X_train, y_train)\n    ANNAccuracy, ANNPredictTest = ANNTesting(ANNModel, X_test, y_test)\n    print('ANN accuracy: {:.6f}'.format(ANNAccuracy))\n    ","c42c5880":"# Checking the percentage of missing values in each variable\n(dataset.isnull().sum()\/len(dataset)*100)","ff261433":"# Split Train and Test and check shape \ndata_train, target_train, data_test, target_test = SplitDataFrameToTrainAndTest(dataset, 0.6, 'Exited')\nPrintTrainTestInformation(data_train, target_train, data_test, target_test)","d4b70ff8":"# Check column types\ndata_train.info()","49ed67db":"print(\" List of unique values in Surname : \")\nprint(dataset['Surname'].unique())\nprint(\" List of unique values in Geography : \")\nprint(dataset['Geography'].unique())\nprint(\" List of unique values in Gender : \")\nprint(dataset['Gender'].unique())\n\n#Special Field\nprint(\" List of unique values in NumOfProducts : \")\nprint(dataset['NumOfProducts'].unique())","575e4a9b":"# Numerical data distribution \ndata_train.describe()","d9ef0018":"data_train.describe(include=['O'])","017cf9f0":"dataset.hist(bins=10, figsize=(20,15))","115c6aa0":"DrawBarplot(data_train, 'Geography')","dbed2087":"DrawBoxplot2(dataset, xAtt = 'Exited', yAtt='CreditScore')\nDrawBoxplot2(dataset, xAtt = 'Exited', yAtt='CreditScore', hAtt='Gender')","aeb655d4":"DrawCountplot(dataset, 'Geography', 'Exited')\nDrawCountplot(dataset, 'Age', 'Exited')","770246e8":"dataset['CategoricalCreditScore'] = pd.qcut(dataset['CreditScore'], 3)\nprint (dataset[['CategoricalCreditScore', 'Exited']].groupby(['CategoricalCreditScore'], as_index=False).mean())","6ff9f8b7":"ContPlot(dataset[['Age','Exited']].copy().dropna(axis=0), \n          'Age', 'Exited', {0: \"black\", 1: \"orange\"} , [1, 0], range(0,100,10))\n\ndataset['CategoricalAge'] = pd.qcut(dataset['Age'], 5, duplicates='drop')\nprint (dataset[['CategoricalAge', 'Exited']].groupby(['CategoricalAge'], as_index=False).mean())","5fa5f654":"ContPlot(dataset[['Balance','Exited']].copy().dropna(axis=0), \n          'Balance', 'Exited', {0: \"black\", 1: \"orange\"} , [1, 0], range(0,100,10))\n\ndataset['CategoricalBalance'] = pd.qcut(dataset['Balance'], 3, duplicates='drop')\nprint (dataset[['CategoricalBalance', 'Exited']].groupby(['CategoricalBalance'], as_index=False).mean())\n","933da69a":"data_encoder = dataset.copy()\ndata_encoder['Geography'] = LabelEncoder().fit_transform(data_encoder['Geography'])\n# data_encoder['Surname'] = LabelEncoder().fit_transform(data_encoder['Surname'])\n# data_encoder['Gender'] = LabelEncoder().fit_transform(data_encoder['Gender'])\ndata_encoder = data_encoder.join(pd.get_dummies(data_encoder['Gender'], prefix='Gender'))\ndata_encoder = data_encoder.drop('Gender', axis=1)\n\ndata_encoder.loc[ data_encoder['Balance'] <= 118100.59, 'Balance'] = 0\ndata_encoder.loc[ data_encoder['Balance'] > 118100.59, 'Balance'] = 1\n\ndata_encoder.head(10)","b168ac67":"AttList = [\"RowNumber\", \"CustomerId\", \"Surname\", \"CategoricalCreditScore\", \"CategoricalAge\", \"CategoricalBalance\"]\ndata_encoder = data_encoder.drop(AttList, axis=1)\ndata_encoder.head()","4ee9a449":"# Split Train and Test and check shape \ndata_train_encoder, target_train_encoder, data_test_encoder, target_test_encoder = SplitDataFrameToTrainAndTest(data_encoder, 0.6, 'Exited')\nPrintTrainTestInformation(data_train_encoder, target_train_encoder, data_test_encoder, target_test_encoder)","f0916988":"X_train = data_train_encoder\ny_train = target_train_encoder\nX_test = data_test_encoder\ny_test = target_test_encoder","35fcdfdc":"MachineLearningModelEvaluate(X_train, y_train, X_test, y_test)","12ce31e7":"## get the most important variables. \ncorr = dataset.corr()**2\ncorr.Exited.sort_values(ascending=False)","d89bf7a6":"# Heatmeap to see the correlation between features. \n# Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(dataset.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# plot\nplt.subplots(figsize = (10,8))\nsns.heatmap(dataset.corr(), annot=True, mask = mask, cmap = 'RdBu_r', linewidths=0.1, linecolor='white', vmax = .9, square=True)\nplt.title(\"Correlations Among Features\", y = 1.03,fontsize = 20);","78cbd625":"print(dataset[['NumOfProducts', 'Exited']].groupby(['NumOfProducts'], as_index=False).mean().sort_values(by='Exited', ascending=False))\nCatPlot(dataset, 'NumOfProducts','Exited', {0: \"black\", 1: \"orange\"} )","a29699b4":"print(dataset[['IsActiveMember', 'Exited']].groupby(['IsActiveMember'], as_index=False).mean().sort_values(by='Exited', ascending=False))\nCatPlot(dataset, 'IsActiveMember','Exited', {0: \"black\", 1: \"orange\"} )","9f03f971":"# https:\/\/seaborn.pydata.org\/generated\/seaborn.pairplot.html\nsns.pairplot(dataset, vars=[\"NumOfProducts\", \"IsActiveMember\", \"Balance\"], hue=\"Exited\")","a5a8828b":"AttList = [\"CreditScore\",\t\"Age\",\t\"Tenure\",\t\"Balance\",\t\"NumOfProducts\",\t\"HasCrCard\",\t\"IsActiveMember\",\t\"EstimatedSalary\"]\ncorrelation_matrix = dataset[AttList].corr().round(2)\n# annot = True to print the values inside the square\nsns.heatmap(data=correlation_matrix, annot=True)","8067ceb6":"data_encoder_feselection = data_encoder.copy()\n# AttList = [\"Surname\", \"RowNumber\", \"CustomerId\"]\n# data_encoder_feselection = data_encoder_feselection.drop(AttList,  axis=1)\nprint(data_encoder_feselection.shape)\ndata_encoder_feselection.head()","5cea8b45":"# Split Train and Test and check shape \ndata_train_encoder_feselection, target_train_encoder_feselection, data_test_encoder_feselection, target_test_encoder_feselection = SplitDataFrameToTrainAndTest(data_encoder_feselection, 0.6, 'Exited')\nPrintTrainTestInformation(data_train_encoder_feselection, target_train_encoder_feselection, data_test_encoder_feselection, target_test_encoder_feselection)","39bf94ec":"# Retest all traditional classification approaches\nX_train = data_train_encoder\ny_train = target_train_encoder\nX_test = data_test_encoder\ny_test = target_test_encoder\n\nMachineLearningModelEvaluate(X_train, y_train, X_test, y_test)","0314be4d":"# Retest all traditional classification approaches\nX_train = data_train_encoder_feselection\ny_train = target_train_encoder_feselection\nX_test = data_test_encoder_feselection\ny_test = target_test_encoder_feselection\n\nMachineLearningModelEvaluate(X_train, y_train, X_test, y_test)","fa5c03be":"model = RandomForestRegressor(random_state=1, max_depth=10)\nmodel.fit(data_train_encoder,target_train_encoder.values.ravel())\n\nprint(data_train_encoder.shape)\nfeatures = data_train_encoder.columns\nimportances = model.feature_importances_\nindices = np.argsort(importances)[-len(features):]  # top features\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","416a7636":"# Get numerical feature importances\nfeature_list = list(data_train_encoder.columns)\nimportances = list(model.feature_importances_)\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];","9062f86a":"# Split Train and Test and check shape \nAttSelection = [\"Age\", \"NumOfProducts\", \"EstimatedSalary\", \"CreditScore\", \"Tenure\", \"Geography\", \"Balance\",\n                \"Exited\"]\n\ndata_train_encoder_feselection02, target_train_encoder_feselection02, data_test_encoder_feselection02, target_test_encoder_feselection02 = SplitDataFrameToTrainAndTest(data_encoder[AttSelection], 0.6, 'Exited')\nPrintTrainTestInformation(data_train_encoder_feselection02, target_train_encoder_feselection02, data_test_encoder_feselection02, target_test_encoder_feselection02)","e8d2e8cc":"# Retest all traditional classification approaches\nX_train = data_train_encoder_feselection02\ny_train = target_train_encoder_feselection02\nX_test = data_test_encoder_feselection02\ny_test = target_test_encoder_feselection02\n\nMachineLearningModelEvaluate(X_train, y_train, X_test, y_test)","955b3acd":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nimport pandas as pd\nfrom sklearn.svm import SVR\n\n# Retest all traditional classification approaches\nX_train = data_train_encoder\ny_train = target_train_encoder\nX_test = data_test_encoder\ny_test = target_test_encoder\n\nLRModel = LogisticRegressionLearning(X_train, y_train)\nmodel = LRModel\nrfe = RFE(model, 10)\nrfe = rfe.fit(X_train, y_train.values.ravel())\n\nfeature_list = list(X_train.columns)\nRankStatistics = pd.DataFrame(columns=['Attributes', 'Ranking', 'Support'])\nfor i, att, rank, suppport in zip(range(len(feature_list)), feature_list, rfe.ranking_, rfe.support_):\n    RankStatistics.loc[i] = [att, rank, suppport]\nRankStatistics = RankStatistics.sort_values('Ranking')\n\nRankStatistics","feef1f41":"# Split Train and Test and check shape \nAttSelection = RankStatistics[(RankStatistics[\"Support\"] == True)]\nAttSelection = list(filter(lambda a: a not in [\"CustomerId\", \"Surname\"], AttSelection[\"Attributes\"]))\nAttSelection = AttSelection + ['Exited']\n\ndata_train_encoder_feselection03, target_train_encoder_feselection03, data_test_encoder_feselection03, target_test_encoder_feselection03 = SplitDataFrameToTrainAndTest(data_encoder[AttSelection], 0.6, 'Exited')\nPrintTrainTestInformation(data_train_encoder_feselection03, target_train_encoder_feselection03, data_test_encoder_feselection03, target_test_encoder_feselection03)","aba8e5f4":"# Retest all traditional classification approaches\nX_train = data_train_encoder_feselection03\ny_train = target_train_encoder_feselection03\nX_test = data_test_encoder_feselection03\ny_test = target_test_encoder_feselection03\n\nMachineLearningModelEvaluate(X_train, y_train, X_test, y_test)","b79cf9c2":"# Feature Reduction: Dimensionality Reduction with PCA.\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nAttRemoved = [\"RowNumber\", \"CustomerId\", \"Surname\", \"HasCrCard\", \"Gender_Male\", \"Gender_Female\"]\nDataFrame = data_encoder\nhr_vars = DataFrame.columns.values.tolist()\nhr_vars = list(filter(lambda a: a not in AttRemoved, hr_vars))\ntargets = ['Exited']\nfeatures = [i for i in hr_vars if i not in targets]\n\n# Separating out the features\nx = DataFrame.loc[:, features].values\n# Separating out the target\ny = DataFrame.loc[:, ['Exited']].values\n# Standardizing the features\nx = StandardScaler().fit_transform(x)\n\nnSelectedFeature = len(hr_vars) - 1\nSelectedAttList = []\nfor i in range(1, nSelectedFeature + 1):\n    SelectedAttList.append(\"principal component\" + str(i))\n\n\npca = PCA(n_components=nSelectedFeature)\nprincipalComponents = pca.fit_transform(x)\nprincipalDf = pd.DataFrame(data=principalComponents, columns=SelectedAttList)\nPCAdf = pd.concat([principalDf, DataFrame[targets]], axis=1)\nPCAdf = PCAdf.dropna()\nPCAdata = PCAdf\n\nPCAdata.head(10)","478b4675":"PCAdata_train, PCAtarget_train, PCAdata_test, PCAtarget_test = SplitDataFrameToTrainAndTest(PCAdata, 0.6, 'Exited')\nPrintTrainTestInformation(PCAdata_train, PCAtarget_train, PCAdata_test, PCAtarget_test)","d0071848":"# Retest all traditional classification approaches\nX_train = PCAdata_train\ny_train = PCAtarget_train\nX_test = PCAdata_test\ny_test = PCAtarget_test\n\nMachineLearningModelEvaluate(X_train, y_train, X_test, y_test)","543648b6":"import matplotlib.pyplot as plt\n\ncum_explained_var = []\nfor i in range(0, len(pca.explained_variance_ratio_)):\n    if i == 0:\n        cum_explained_var.append(pca.explained_variance_ratio_[i])\n    else:\n        cum_explained_var.append(pca.explained_variance_ratio_[i] +\n                                 cum_explained_var[i - 1])\n\nx_val = range(1, len(cum_explained_var) + 1)\ny_val = cum_explained_var\n\nfig = plt.figure()\nplt.plot(x_val, y_val)\nplt.plot(x_val, y_val, 'or')\nplt.title(\"PCA Accumulative Explained Variance\")\nplt.xticks(range(1, len(cum_explained_var) + 1))\nplt.grid(True)\nplt.show()","c12d07d4":"AttSelection = PCAdata.columns.values.tolist()\nAttSelection = AttSelection[:15]\nif AttSelection[len(AttSelection)-1] != 'Exited' :\n    AttSelection = AttSelection + ['Exited']\nprint(AttSelection)\n\nPCAdata_train_feReduction, PCAtarget_train_feReduction, PCAdata_test_feReduction, PCAtarget_test_feReduction = SplitDataFrameToTrainAndTest(PCAdata[AttSelection], 0.6, 'Exited')\nPrintTrainTestInformation(PCAdata_train_feReduction, PCAtarget_train_feReduction, PCAdata_test_feReduction, PCAtarget_test_feReduction)","d0255de7":"# Retest all traditional classification approaches\nX_train = PCAdata_train_feReduction\ny_train = PCAtarget_train_feReduction\nX_test = PCAdata_test_feReduction\ny_test = PCAtarget_test_feReduction\n\nMachineLearningModelEvaluate(X_train, y_train, X_test, y_test)","86ce1bff":"data_encoder.head()","0a121176":"data_encoder.info()","b284e2f6":"CheckOutlierAtt = ['CreditScore', 'Geography']\nLOFOutlierIdx01,LOFFactorData01 = DetectOutlierByLOF(data_encoder, AttList=CheckOutlierAtt, LOFThresh=3.0, neighbors = 10)\n\nprint(\"Size of LOFOutlierIdx : \" + str(len(LOFOutlierIdx01)))\nprint(LOFFactorData01.head())\n","fa8329dc":"CheckOutlierAtt = ['Age', 'Tenure', 'Balance']\nLOFOutlierIdx02,LOFFactorData02 = DetectOutlierByLOF(data_encoder, AttList=CheckOutlierAtt, LOFThresh=3.0, neighbors = 10)\n\nprint(\"Size of LOFOutlierIdx : \" + str(len(LOFOutlierIdx02)))\nprint(LOFFactorData02.head())","15e617a1":"CheckOutlierAtt = ['HasCrCard', 'IsActiveMember', 'EstimatedSalary']\nLOFOutlierIdx03,LOFFactorData03 = DetectOutlierByLOF(data_encoder, AttList=CheckOutlierAtt, LOFThresh=3.0, neighbors = 10)\n\nprint(\"Size of LOFOutlierIdx : \" + str(len(LOFOutlierIdx03)))\nprint(LOFFactorData03.head())\n","efcd5c67":"print('LOFOutlierIdx01 :' + str(LOFOutlierIdx01))\nprint('LOFOutlierIdx02 :' + str(LOFOutlierIdx02))\nprint('LOFOutlierIdx03 :' + str(LOFOutlierIdx03))","1ec086ba":"OutlierIndex = set(LOFOutlierIdx01 + LOFOutlierIdx02 + LOFOutlierIdx03)\nOutlierIndex = list(OutlierIndex)\nprint(len(OutlierIndex))\nprint('OutlierIdx : ' + str(OutlierIndex))","40137ca3":"data_encoder_mining = data_encoder.copy()\nprint(data_encoder_mining.shape)\ndata_encoder_mining = RemoveRowsFromDataFrame(data_encoder_mining,OutlierIndex)\nprint(data_encoder_mining.shape)\n\n# feature selection\n# AttList = [\"Surname\", \"RowNumber\", \"CustomerId\"]\n# data_encoder_mining = data_encoder_mining.drop(AttList,  axis=1)\n# print(data_encoder_mining.shape)\n","d7174c34":"# Split Train and Test and check shape \ndata_train_encoder_mining, target_train_encoder_mining, data_test_encoder_mining, target_test_encoder_mining = SplitDataFrameToTrainAndTest(data_encoder_mining, 0.6, 'Exited')\nPrintTrainTestInformation(data_train_encoder_mining, target_train_encoder_mining, data_test_encoder_mining, target_test_encoder_mining)","e1249d61":"# Retest all traditional classification approaches\nX_train = data_train_encoder_mining\ny_train = target_train_encoder_mining\nX_test = data_test_encoder_mining\ny_test = target_test_encoder_mining\n\nMachineLearningModelEvaluate(X_train, y_train, X_test, y_test)","2dcd103f":"# Retest all traditional classification approaches\n# X_train = data_train_encoder_mining\n# y_train = target_train_encoder_mining\n# X_test = data_test_encoder_mining\n# y_test = target_test_encoder_mining\n\nX_train = PCAdata_train_feReduction\ny_train = PCAtarget_train_feReduction\nX_test = PCAdata_test_feReduction\ny_test = PCAtarget_test_feReduction\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import ModelCheckpoint\n\nseed = 42\nnp.random.seed(seed)\n\n## Create our model\nmodel = Sequential()\n\n# 1st layer: 23 nodes, input shape[1] nodes, RELU\nmodel.add(Dense(23, input_dim=X_train.shape[1], kernel_initializer='uniform', activation='relu'))\n# 2nd layer: 17 nodes, RELU\nmodel.add(Dense(17, kernel_initializer='uniform', activation = 'relu'))\n# 3nd layer: 15 nodes, RELU\nmodel.add(Dense(15, kernel_initializer='uniform', activation='relu'))\n# 4nd layer: 11 nodes, RELU\nmodel.add(Dense(11, kernel_initializer='uniform', activation='relu'))\n# 5nd layer: 9 nodes, RELU\nmodel.add(Dense(9, kernel_initializer='uniform', activation='relu'))\n# 6nd layer: 7 nodes, RELU\nmodel.add(Dense(7, kernel_initializer='uniform', activation='relu'))\n# 7nd layer: 5 nodes, RELU\nmodel.add(Dense(5, kernel_initializer='uniform', activation='relu'))\n# 8nd layer: 2 nodes, RELU\nmodel.add(Dense(2, kernel_initializer='uniform', activation='relu'))\n# output layer: dim=1, activation sigmoid\nmodel.add(Dense(1, kernel_initializer='uniform', activation='sigmoid' ))\n# Compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nNB_EPOCHS = 100\nBATCH_SIZE = 23\n\n# checkpoint: store the best model\nckpt_model = 'pima-weights.best.hdf5'\ncheckpoint = ModelCheckpoint(ckpt_model, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\ncallbacks_list = [checkpoint]\n\nprint('Starting training...')\n# train the model, store the results for plotting\nhistory = model.fit(X_train,\n                    y_train,\n                    validation_data=(X_test, y_test),\n                    epochs=NB_EPOCHS,\n                    batch_size=BATCH_SIZE,\n                    callbacks=callbacks_list,\n                    verbose=0)\n","aaf97f28":"X = data_encoder_mining.copy()\nX = X.drop('Exited', 1)\ny = data_encoder_mining[['Exited']]\nX.head()","26577562":"X = PCAdata.copy()\nX = X.drop('Exited', 1)\ny = PCAdata[['Exited']]\nX.head()\n\nX_train = PCAdata_train_feReduction\ny_train = PCAtarget_train_feReduction\nX_test = PCAdata_test_feReduction\ny_test = PCAtarget_test_feReduction\n","e950ddfe":"NBModel = NaiveBayesLearning(X_train, y_train)\nLRModel = LogisticRegressionLearning(X_train, y_train)\nRFModel = RandomForestLearning(X_train, y_train)\nLiSVMModel = SVMLearning(X_train, y_train)\nRBFSVMModel = SVMLearning(X_train, y_train, 'RBF')\nKNNModel = KNNLearning(X_train, y_train)\nANNModel = ANNLearning(X_train, y_train)","a2d93f37":"from sklearn import model_selection\nprint('5-fold cross validation:\\n')\nlabels = ['NaiveBayesLearning', 'LogisticRegressionLearning', 'RandomForestLearning', \n          'SVMLearningLinear', 'SVMLearningRBF', 'KNNLearning', 'ANNLearning']\nfor clf, label in zip([NBModel, LRModel, RFModel, LiSVMModel, RBFSVMModel, KNNModel, ANNModel], labels):\n    scores = model_selection.cross_val_score(clf, X, y.values.ravel(), cv=5, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n","c2367186":"from mlxtend.classifier import EnsembleVoteClassifier\neclf = EnsembleVoteClassifier(clfs=[RFModel, \n                                    LiSVMModel, \n                                    ANNModel], weights=[1,1,1])\n\nlabels = ['RandomForestLearning', 'SVMLearningLinear', 'ANNModel', 'Ensemble']\nfor clf, label in zip([RFModel, LiSVMModel, ANNModel, eclf], labels):\n    scores = model_selection.cross_val_score(clf, X, y.values.ravel(), cv=5,scoring='accuracy')\n    print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))","2d6c6543":"# Majority Rule (hard) Voting\n\nmv_clf = MajorityVoteClassifier(classifiers=[RFModel, LiSVMModel, ANNModel])\n\nlabels = ['RandomForestLearning', 'SVMLearningLinear', 'ANN', 'Majority voting']\nall_clf = [RFModel, LiSVMModel, ANNModel, mv_clf]\n\nfor clf, label in zip(all_clf, labels):\n    scores = cross_val_score(estimator=clf, X=X, y=y.values.ravel(), cv=5, scoring='accuracy')\n    print(\"ROC AUC: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))","9eca2cc7":"# Split Train and Test and check shape \ndata_train_encoder_mining, target_train_encoder_mining, data_test_encoder_mining, target_test_encoder_mining = SplitDataFrameToTrainAndTest(data_encoder_mining, 0.6, 'Exited')\nPrintTrainTestInformation(data_train_encoder_mining, target_train_encoder_mining, data_test_encoder_mining, target_test_encoder_mining)\n\n# Retest all traditional classification approaches\nX_train = data_train_encoder_mining\ny_train = target_train_encoder_mining\nX_test = data_test_encoder_mining\ny_test = target_test_encoder_mining","cad40213":"tree = DecisionTreeClassifier(criterion='entropy', max_depth=None, random_state=1)\nbag = BaggingClassifier(base_estimator=RFModel,\n                        n_estimators=1000, \n                        max_samples=1.0, \n                        max_features=1.0, \n                        bootstrap=True, \n                        bootstrap_features=False, \n                        n_jobs=1, \n                        random_state=1)\n\ntree = tree.fit(X_train, y_train.values.ravel())\ny_train_pred = tree.predict(X_train)\ny_test_pred = tree.predict(X_test)\n\ntree_train = accuracy_score(y_train, y_train_pred)\ntree_test = accuracy_score(y_test, y_test_pred)\nprint('Decision tree train\/test accuracies %.3f\/%.3f'\n      % (tree_train, tree_test))\n\nbag = bag.fit(X_train, y_train.values.ravel())\ny_train_pred = bag.predict(X_train)\ny_test_pred = bag.predict(X_test)\n\nbag_train = accuracy_score(y_train, y_train_pred) \nbag_test = accuracy_score(y_test, y_test_pred) \nprint('Bagging train\/test accuracies %.3f\/%.3f'\n      % (bag_train, bag_test))","cc1a59cd":"from sklearn.ensemble import AdaBoostClassifier\n\ntree = DecisionTreeClassifier(criterion='entropy', max_depth=None, random_state=1)\nada = AdaBoostClassifier(base_estimator=tree, n_estimators=500, learning_rate=0.1, random_state=1)\ntree = tree.fit(X_train, y_train.values.ravel())\ny_train_pred = tree.predict(X_train)\ny_test_pred = tree.predict(X_test)\n\ntree_train = accuracy_score(y_train, y_train_pred)\ntree_test = accuracy_score(y_test, y_test_pred)\nprint('Decision tree train\/test accuracies %.3f\/%.3f'% (tree_train, tree_test))\n\nada = ada.fit(X_train, y_train.values.ravel())\ny_train_pred = ada.predict(X_train)\ny_test_pred = ada.predict(X_test)\n\nada_train = accuracy_score(y_train, y_train_pred) \nada_test = accuracy_score(y_test, y_test_pred) \nprint('AdaBoost train\/test accuracies %.3f\/%.3f'\n      % (ada_train, ada_test))","0a06c8e9":"from mlxtend.classifier import StackingClassifier\nimport matplotlib.gridspec as gridspec\nimport itertools\nfrom mlxtend.plotting import plot_learning_curves\nfrom mlxtend.plotting import plot_decision_regions\n\nlr = LogisticRegression()\nsclf = StackingClassifier(classifiers=[RFModel, LiSVMModel, ANNModel], meta_classifier=lr)\n\nlabel = ['RandomForestLearning', 'SVMLearningLinear', 'ANN', 'Stacking Classifier']\nclf_list = [RFModel, LiSVMModel, ANNModel, sclf]\n\n\nclf_cv_mean = []\nclf_cv_std = []\nfor clf, label in zip(clf_list, label):\n    scores = cross_val_score(clf, X, y.values.ravel(), cv=5, scoring='accuracy')\n    print(\"Accuracy: %.2f (+\/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n    clf_cv_mean.append(scores.mean())\n    clf_cv_std.append(scores.std())        \n    clf.fit(X, y.values.ravel())","239a98d3":"# Functions","7b925eb3":"# **Approach 1 ** (Feature Selection)","7588afcc":"# Data Science\n\n## Bank Customer Churn Modeling\n\nnhan.cao@beesightsoft.com\n","c8611c69":"# Correlation","3ff00da9":"# Read data\nhttps:\/\/www.kaggle.com\/barelydedicated\/bank-customer-churn-modeling","ea5f3ac4":"# **Encoder **","4316d6a2":"# Checking missing values\n\n- Fill missing value: Median \/ Mode, Label Encode \/ Dummies","5bec3ce8":"# *Outlier Removal Approach*","bc365546":"# **Approach 2 (Feature Reduction)**","75e8dfbb":"# Summaries\n\n### Using Bagging on RandomForest can make up to 87.4%\n","51aeac8f":"# *Bagging Boosting and Stacking*","6549ed67":"# **Neural Network Approach**","d75dc771":"## **Classification by trainditional models** ","7f09c73a":"## Feature Importances","0253f1bd":"## **Preparation and EDA**"}}