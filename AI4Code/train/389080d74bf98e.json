{"cell_type":{"885d889c":"code","60a5b7f8":"code","08716cd0":"code","2a6a49d5":"code","4ab7626a":"code","cd863e18":"code","f617a4e1":"code","d8363f86":"code","93e7b99d":"code","13730f13":"code","33361192":"code","e15c8110":"code","e52044b3":"code","3df9133e":"code","04a37c6b":"code","b320f58a":"code","cdc272ab":"code","28ad3175":"code","04ce831c":"code","d5753631":"code","c2e8fa0f":"code","6b63fd48":"code","7bc07fef":"code","dfbc0ec0":"code","7396bfe4":"code","c1da063b":"code","147d5767":"code","3dfbe21e":"code","a3917d0c":"code","54d6a46a":"code","e1aae97b":"code","4d14e03f":"code","3fc8e1e7":"code","f79afa73":"code","aa595da2":"code","b1bd647d":"code","df0e9b76":"markdown","fb2ac929":"markdown","12f5cf19":"markdown","bf3c00ad":"markdown","5b0a1510":"markdown","9e25e951":"markdown","7e2da476":"markdown","b4b6cc26":"markdown","bdca188d":"markdown","85324f25":"markdown","941f3882":"markdown","217e042f":"markdown","553d00ca":"markdown","286deb81":"markdown","8f6092c3":"markdown","698d2371":"markdown","e268666a":"markdown","a6d5602e":"markdown"},"source":{"885d889c":"# to load, access, process and dump json files\nimport json\n# regular repression\nimport re\n# to parse HTML contents\nfrom bs4 import BeautifulSoup\n\n# for numerical analysis\nimport numpy as np \n# to store and process in a dataframe\nimport pandas as pd \n\n# for ploting graphs\nimport matplotlib.pyplot as plt\n# advancec ploting\nimport seaborn as sns\n# to create word clouds\nfrom wordcloud import WordCloud, STOPWORDS \n\n# To encode values\nfrom sklearn.preprocessing import LabelEncoder\n# Convert a collection of text documents to a matrix of token counts\nfrom sklearn.feature_extraction.text import CountVectorizer\n# confusion matrix\nfrom sklearn.metrics import confusion_matrix\n# train test split\nfrom sklearn.model_selection import train_test_split\n\n# for deep learning \nimport tensorflow as tf\n# to tokenize text\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n# to pad sequence \nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","60a5b7f8":"# import data\ndf = pd.read_json('..\/input\/news-category-dataset\/News_Category_Dataset_v2.json', lines=True)\n# shape\nprint('No. of rows and columns :', df.shape)\n# show first few rows\ndf.head()","08716cd0":"# Stopwords list from https:\/\/github.com\/Yoast\/YoastSEO.js\/blob\/develop\/src\/config\/stopwords.js\nstopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]","2a6a49d5":"# to plot n-gram\n# ==============\n\ndef category_ngram(category, n):\n    \n    temp_df = df[df['category'] == category]\n    \n    word_vectorizer = CountVectorizer(ngram_range=(n, n), analyzer='word')\n    sparse_matrix = word_vectorizer.fit_transform(temp_df['headline'])\n    \n    frequencies = sum(sparse_matrix).toarray()[0]\n    \n    return pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\\\n            .sort_values(by='frequency', ascending=False) \\\n            .reset_index() \\\n            .head(10)","4ab7626a":"# to plot wordcloud\n# =================\n\ndef plot_wordcloud(headlines, cmap):\n    fig, ax = plt.subplots(figsize=(8, 6))\n    wc = WordCloud(max_words = 1000, background_color ='white', stopwords = stopwords, \n                   min_font_size = 10, colormap=cmap)\n    wc = wc.generate(headlines)\n    plt.axis('off')\n    plt.imshow(wc)","cd863e18":"# to plot model accuracy and loss\n# ===============================\n\ndef plot_history(history):\n    \n    plt.figure(figsize=(20, 5))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'], label='Training Accuracy', c='dodgerblue', lw='2')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', c='orange', lw='2')\n    plt.title('Accuracy', loc='left', fontsize=16)\n    plt.xlabel(\"Epochs\")\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='Training Loss', c='dodgerblue', lw='2')\n    plt.plot(history.history['val_loss'], label='Validation Loss', c='orange', lw='2')\n    plt.title('Loss', loc='left', fontsize=16)\n    plt.xlabel(\"Epochs\")\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.show()","f617a4e1":"# to plot confusion matrix\n# ========================\n\ndef plot_cm(pred, ticklabels, figsize):\n      \n    fig, ax = plt.subplots(1, 1, figsize=(figsize, figsize))\n\n    cm = confusion_matrix(validation_labels, pred)\n    sns.heatmap(cm, annot=True, cbar=False, fmt='1d', cmap='Blues', ax=ax)\n\n    ax.set_xlabel('Predicted')\n    ax.set_ylabel('Actual')\n    ax.set_xticklabels(ticklabels, rotation=90)\n    ax.set_yticklabels(ticklabels, rotation=0)\n\n    plt.show()","d8363f86":"count_df = pd.DataFrame(df['category'].value_counts()).reset_index()\nprint('There are', len(count_df), 'news categories')\n\nsns.set_style('darkgrid')\nplt.figure(figsize=(10, 12))\nsns.barplot(data=count_df, y='index', x='category', palette='Dark2')\nplt.title('No. news in each category', loc='left', fontsize=20)\nplt.xlabel(\"\")\nplt.ylabel(\"\")\nplt.show()","93e7b99d":"# container for dfs\ndfs = []\n\n# loop through each category and get 1004 rows and append to dfs\nfor category in df['category'].unique():\n    temp = df[df['category']==category]\n    dfs.append(temp.sample(1004))\n    \n# concatenate dataframes\ndf = pd.concat(dfs)\n\n# shuffle dataframe\ndf = df.sample(frac=1).reset_index(drop=True)\n\n# first few rows\ndf.head()","13730f13":"# removing non alphanumeric character\ndef alpha_num(text):\n    return re.sub(r'[^A-Za-z0-9 ]', '', text)\n\n# removing the stopwords from text\ndef remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stopwords:\n            final_text.append(i.strip())\n    return \" \".join(final_text)","33361192":"# apply preprocessing steps\n\ndf['headline'] = df['headline'].str.lower()\ndf['headline'] = df['headline'].apply(alpha_num)\ndf['headline'] = df['headline'].apply(remove_stopwords)\n\ndf.head()","e15c8110":"# unique news category\ndf['category'].unique()","e52044b3":"# most frequent unigrams of news belongs 'SCIENCE' category\ncategory_ngram('SCIENCE', 1)","3df9133e":"# most frequent bigrams of news belongs 'SPORTS' category\ncategory_ngram('SPORTS', 2)","04a37c6b":"# most frequent trigrams of news belongs 'POLITICS' category\ncategory_ngram('POLITICS', 3)","b320f58a":"# container for sentences\nnews = np.array([headline for headline in df['headline']])\n\n# container for labels\nlabels = np.array([label for label in df['category']])","cdc272ab":"# Label encoding news category\nenc = LabelEncoder()\nlabels = enc.fit_transform(labels)\n# print(enc.classes_)","28ad3175":"# parameters\n\nvocab_size = 1000\nembedding_dim = 16\nmax_length = 120\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"\ntraining_portion = .8","04ce831c":"# train-test split\ntrain_sentences, validation_sentences, train_labels, validation_labels = train_test_split(news, labels, \n                                                                                          test_size=0.33, \n                                                                                          stratify=labels)","d5753631":"# tokenize sentences\ntokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(train_sentences)\nword_index = tokenizer.word_index\n\n# convert train dataset to sequence and pad sequences\ntrain_sequences = tokenizer.texts_to_sequences(train_sentences)\ntrain_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)\n\n# convert validation dataset to sequence and pad sequences\nvalidation_sequences = tokenizer.texts_to_sequences(validation_sentences)\nvalidation_padded = pad_sequences(validation_sequences, padding=padding_type, maxlen=max_length)","c2e8fa0f":"# model initialization\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(41, activation='softmax')\n])\n\n# compile model\nmodel.compile(loss='sparse_categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n# model summary\nprint(model.summary())","6b63fd48":"# fit model\nnum_epochs = 100\nhistory = model.fit(train_padded, train_labels, \n                    epochs=num_epochs, verbose=2, \n                    validation_split=0.33)\n\n# predict values\npred = model.predict(validation_padded)\npred = np.array([np.argmax(i) for i in pred])","7bc07fef":"# plot history\nplot_history(history)","dfbc0ec0":"# plot confusion matrix\nplot_cm(pred, enc.classes_, 20)","7396bfe4":"# reviews on which we need to predict\nsentence = [\"Rojer federer is into the wimbledon finals\", \n            \"NASA to launch two more satellites this year\"]\n\n# convert to a sequence\nsequences = tokenizer.texts_to_sequences(sentence)\n\n# pad the sequence\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\n# preict the label\npred = model.predict(padded)\npred = np.array([np.argmax(i) for i in pred])\nfor i in range(len(sentence)):\n    print(sentence[i], '\\t:\\t', enc.classes_[i])","c1da063b":"# model initialization\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(41, activation='softmax')\n])\n\n# compile model\nmodel.compile(loss='sparse_categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n# model summary\nprint(model.summary())","147d5767":"# fit model\nnum_epochs = 50\nhistory = model.fit(train_padded, train_labels, \n                    epochs=num_epochs, verbose=2, \n                    validation_split=0.33)\n\n# predict values\npred = model.predict(validation_padded)\npred = np.array([np.argmax(i) for i in pred])","3dfbe21e":"# plot history\nplot_history(history)","a3917d0c":"# plot confusion matrix\nplot_cm(pred, enc.classes_, 20)","54d6a46a":"# reviews on which we need to predict\nsentence = [\"Rojer federer is into the wimbledon finals\", \n            \"NASA to launch two more satellites this year\"]\n\n# convert to a sequence\nsequences = tokenizer.texts_to_sequences(sentence)\n\n# pad the sequence\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\n# preict the label\nprint(model.predict(padded))","e1aae97b":"# model initialization\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n    tf.keras.layers.GlobalMaxPooling1D(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(41, activation='softmax')\n])\n\n# compile model\nmodel.compile(loss='sparse_categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n# model summary\nprint(model.summary())","4d14e03f":"# fit model\nnum_epochs = 50\nhistory = model.fit(train_padded, train_labels, \n                    epochs=num_epochs, verbose=2, \n                    validation_split=0.33)\n\n# predict values\npred = model.predict(validation_padded)\npred = np.array([np.argmax(i) for i in pred])","3fc8e1e7":"# plot history\nplot_history(history)","f79afa73":"# plot confusion matrix\nplot_cm(pred, enc.classes_, 20)","aa595da2":"# reviews on which we need to predict\nsentence = [\"The movie was very touching and heart whelming\", \n            \"I have never seen a terrible movie like this\"]\n\n# convert to a sequence\nsequences = tokenizer.texts_to_sequences(sentence)\n\n# pad the sequence\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\n# preict the label\nprint(model.predict(padded))","b1bd647d":"# reviews on which we need to predict\nsentence = [\"Rojer federer is into the wimbledon finals\", \n            \"NASA to launch two more satellites this year\"]\n\n# convert to a sequence\nsequences = tokenizer.texts_to_sequences(sentence)\n\n# pad the sequence\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\n# preict the label\nprint(model.predict(padded))","df0e9b76":"## Label encode target","fb2ac929":"## About the Data\n  \n> Dataset contains around 200k news headlines from the year 2012 to 2018 obtained from HuffPost. \n\n## Why Classifying ?\n  \n> The model trained on this dataset could be used to identify tags for untracked news articles or to identify the type of language used in different news articles.","12f5cf19":"## Data","bf3c00ad":"## With Convolution","5b0a1510":"## Utility Funtion","9e25e951":"## Equalize no. of news category","7e2da476":"## Stopwords","b4b6cc26":"## Import Libraries","bdca188d":"## Data Preprocessing","85324f25":"## Get text and labels","941f3882":"## With LSTM","217e042f":"From : https:\/\/www.kaggle.com\/madz2000\/sarcasm-detection-with-glove-word2vec-82-accuracy","553d00ca":"### No. of headlines in each category","286deb81":"## EDA","8f6092c3":"## Tokenize and Sequence text","698d2371":"## With Word Embedding","e268666a":"## Model Parameters","a6d5602e":"## Train Test Split"}}