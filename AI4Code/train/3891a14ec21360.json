{"cell_type":{"738d76f0":"code","d92a9492":"code","b56457aa":"code","a7185098":"code","4ff53366":"code","bbdcd3de":"code","ec63bafa":"code","391b9196":"code","d8ff0820":"code","ccca291b":"code","2f1f77eb":"code","1fc81080":"code","34a11599":"code","8f4b36a9":"code","e498f7cc":"code","fd43d713":"code","1bef4136":"code","9c7f9c7e":"code","48473106":"markdown","fc13e420":"markdown","047253aa":"markdown","50deb84a":"markdown","124383f0":"markdown","faabf89e":"markdown","b27b839c":"markdown","1c5c6f52":"markdown","ab76275c":"markdown","666b9612":"markdown","6ed413c1":"markdown","dcec6d8b":"markdown","fe380b30":"markdown","e58e2713":"markdown","edc2d91a":"markdown","f5694bb4":"markdown","9470705f":"markdown","64a11f67":"markdown","a0320083":"markdown","c3bbf6bb":"markdown","457b5d79":"markdown","d4571079":"markdown","c8a85340":"markdown"},"source":{"738d76f0":"# Install cord19q project\n!pip install git+https:\/\/github.com\/neuml\/cord19q\n\n# Install scispacy model\n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.5\/en_core_sci_md-0.2.5.tar.gz","d92a9492":"import os\nimport shutil\n\nfrom paperetl.cord19.execute import Execute as Etl\n\n# Copy study design models locally\nos.mkdir(\"cord19q\")\nshutil.copy(\"..\/input\/cord19-study-design\/attribute\", \"cord19q\")\nshutil.copy(\"..\/input\/cord19-study-design\/design\", \"cord19q\")\n\n# Build SQLite database for metadata.csv and json full text files\nEtl.run(\"..\/input\/CORD-19-research-challenge\", \"cord19q\", \"cord19q\", \"..\/input\/cord-19-article-entry-dates\/entry-dates.csv\", False)","b56457aa":"import shutil\n\nfrom paperai.index import Index\n\n# Copy vectors locally for predictable performance\nshutil.copy(\"..\/input\/cord19-fasttext-vectors\/cord19-300d.magnitude\", \"\/tmp\")\n\n# Build the embeddings index\nIndex.run(\"cord19q\", \"\/tmp\/cord19-300d.magnitude\")","a7185098":"import os\n\n# Workaround for mdv terminal width issue\nos.environ[\"COLUMNS\"] = \"80\"\n\nfrom paperai.highlights import Highlights\nfrom paperai.tokenizer import Tokenizer\n\nfrom wordcloud import WordCloud\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport plotly.graph_objects as go\nimport pycountry\n\n# Tokenizes text and removes stopwords\ndef tokenize(text, case_sensitive=False):\n    # Get list of accepted tokens\n    tokens = [token for token in Tokenizer.tokenize(text) if token not in Highlights.STOP_WORDS]\n    \n    if case_sensitive:\n        # Filter original tokens to preserve token casing\n        return [token for token in text.split() if token.lower() in tokens]\n\n    return tokens\n    \n# Country data\ncountries = [c.name for c in pycountry.countries]\ncountries = countries + [\"USA\"]\n\n# Lookup country name for alpha code. If already an alpha code, return value\ndef countryname(x):\n    country = pycountry.countries.get(alpha_3=x)\n    return country.name if country else x\n    \n# Resolve alpha code for country name\ndef countrycode(x):\n    return pycountry.countries.get(name=x).alpha_3\n\n# Tokenize and filter only country names\ndef countrynames(x):\n    return [countryname(country) for country in countries if country.lower() in x.lower()]\n\n# Word Cloud colors\ndef wcolors(word=None, font_size=None, position=None,  orientation=None, font_path=None, random_state=None):\n    colors = [\"#7e57c2\", \"#03a9f4\", \"#011ffd\", \"#ff9800\", \"#ff2079\"]\n    return np.random.choice(colors)\n\n# Word Cloud visualization\ndef wordcloud(df, title = None):\n    # Set random seed to have reproducible results\n    np.random.seed(64)\n    \n    wc = WordCloud(\n        background_color=\"white\",\n        max_words=200,\n        max_font_size=40,\n        scale=5,\n        random_state=0\n    ).generate_from_frequencies(df)\n\n    wc.recolor(color_func=wcolors)\n    \n    fig = plt.figure(1, figsize=(15,15))\n    plt.axis('off')\n\n    if title:\n        fig.suptitle(title, fontsize=14)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wc),\n    plt.show()\n\n# Dataframe plot\ndef plot(df, title, kind=\"bar\", color=\"bbddf5\"):\n    # Remove top and right border\n    ax = plt.axes()\n    ax.spines[\"top\"].set_visible(False)\n    ax.spines[\"right\"].set_visible(False)\n\n    # Set axis color\n    ax.spines['left'].set_color(\"#bdbdbd\")\n    ax.spines['bottom'].set_color(\"#bdbdbd\")\n\n    df.plot(ax=ax, title=title, kind=kind, color=color);\n\n# Pie plot\ndef pie(labels, sizes, title):\n    patches, texts = plt.pie(sizes, colors=[\"#4caf50\", \"#ff9800\", \"#03a9f4\", \"#011ffd\", \"#ff2079\", \"#7e57c2\", \"#fdd835\"], startangle=90)\n    plt.legend(patches, labels, loc=\"best\")\n    plt.axis('equal')\n    plt.tight_layout()\n    plt.title(title)\n    plt.show()\n    \n# Map visualization\ndef mapplot(df, title, bartitle):\n    fig = go.Figure(data=go.Choropleth(\n        locations = df[\"Code\"],\n        z = df[\"Count\"],\n        text = df[\"Country\"],\n        colorscale = [(0,\"#fffde7\"), (1,\"#f57f17\")],\n        showscale = False,\n        marker_line_color=\"darkgray\",\n        marker_line_width=0.5,\n        colorbar_title = bartitle,\n    ))\n\n    fig.update_layout(\n        title={\n            'text': title,\n            'y':0.9,\n            'x':0.5,\n            'xanchor': 'center',\n            'yanchor': 'top'},\n        geo=dict(\n            showframe=False,\n            showcoastlines=False,\n            projection_type='equirectangular'\n        )\n    )\n    \n    fig.show(config={\"displayModeBar\": False, \"scrollZoom\": False})","4ff53366":"import pandas as pd\nimport sqlite3\n\n# Connect to database\ndb = sqlite3.connect(\"cord19q\/articles.sqlite\")\n\n# Articles\npd.set_option(\"max_colwidth\", 125)\narticles = pd.read_sql_query(\"select * from articles where tags is not null LIMIT 5\", db)\narticles","bbdcd3de":"# Connect to database\ndb = sqlite3.connect(\"cord19q\/articles.sqlite\")\n\n# Sections\npd.set_option(\"max_colwidth\", 125)\nsections = pd.read_sql_query(\"select * from sections where tags is not null LIMIT 5\", db)\nsections","ec63bafa":"# Connect to database\ndb = sqlite3.connect(\"cord19q\/articles.sqlite\")\n\n# Select data\narticles = pd.read_sql_query(\"select title from articles where tags is not null and title is not null\", db)\n\n# Build word frequencies on filtered tokens\nfreqs = pd.Series(np.concatenate([tokenize(x) for x in articles.Title])).value_counts()\nwordcloud(freqs, \"Most frequent words in article titles tagged as COVID-19\")","391b9196":"# Connect to database\ndb = sqlite3.connect(\"cord19q\/articles.sqlite\")\n\nsections = pd.read_sql_query(\"select text from sections where tags is not null\", db)\n\n# Filter tokens to only country names. Build dataframe of Country, Count, Code\nmentions = pd.Series(np.concatenate([countrynames(x) for x in sections.Text])).value_counts()\nmentions = mentions.rename_axis(\"Country\").reset_index(name=\"Count\")\nmentions[\"Code\"] = [countrycode(x) for x in mentions[\"Country\"]]\n\n# Set max to 1000 to allow shading for multiple countries\nmentions[\"Count\"] = mentions[\"Count\"].clip(upper=2500)\n\nmapplot(mentions, \"Tagged Articles by Country Mentioned\", \"Articles by Country\")","d8ff0820":"# Connect to database\ndb = sqlite3.connect(\"cord19q\/articles.sqlite\")\n\narticles = pd.read_sql_query(\"select source from articles where tags is not null\", db)\n\nfreqs = articles.Source.value_counts().sort_values(ascending=True)\nplot(freqs, \"Tagged Articles by Source\", \"barh\", \"#1976d2\")","ccca291b":"# Connect to database\ndb = sqlite3.connect(\"cord19q\/articles.sqlite\")\n\narticles = pd.read_sql_query(\"select case when (Publication = '' OR Publication IS NULL) THEN '[None]' ELSE Publication END AS Publication from articles where tags is not null\", db)\n\nfreqs = articles.Publication.value_counts().sort_values(ascending=True)[-20:]\n\nplot(freqs, \"Tagged Articles by Publication\", \"barh\", \"#7e57c2\")","2f1f77eb":"# Connect to database\ndb = sqlite3.connect(\"cord19q\/articles.sqlite\")\n\narticles = pd.read_sql_query(\"select strftime('%Y-%m', published) as Published from articles where tags is not null and published >= '2020-01-01' order by published\", db)\n\nfreqs = articles.Published.value_counts().sort_index()\nplot(freqs, \"Tagged Articles by Publication Month\", \"bar\", \"#ff9800\")","1fc81080":"# Connect to database\ndb = sqlite3.connect(\"cord19q\/articles.sqlite\")\n\narticles = pd.read_sql_query('select count(*) as count, case when design=1 then \"systematic review\" when design in (2, 3) then \"control trial\" ' + \n                             'when design in (4, 5) then \"prospective studies\" when design=6 then \"retrospective studies\" ' +\n                             'when design in (7, 8) then \"case series\" else \"modeling\" end as design from articles ' +\n                             'where tags is not null and design > 0 group by design', db)\n\narticles = articles.groupby([\"design\"]).sum().reset_index()\n\n# Plot a pie chart of study types\npie(articles[\"design\"], articles[\"count\"], \"Tagged Articles by Study Design\")\n","34a11599":"from paperai.embeddings import Embeddings\n\nembeddings = Embeddings()\nembeddings.load(\"cord19q\")\n\nvectors = embeddings.vectors\n\npd.DataFrame(embeddings.vectors.most_similar(\"covid-19\", topn=10), columns=[\"key\", \"value\"])\n","8f4b36a9":"vectors.similarity(\"coronavirus\", [\"sars\", \"influenza\", \"ebola\", \"phone\"])","e498f7cc":"sentence1 = \"Range of incubation periods for the disease in humans\"\nsentence2 = \"The incubation period of 2019-nCoV is generally 3-7 days but no longer than 14 days, and the virus is infective during the incubation period\"\n\nembeddings.similarity(Tokenizer.tokenize(sentence1), [Tokenizer.tokenize(sentence2)])","fd43d713":"sentence1 = \"Range of incubation periods for the disease in humans\"\nsentence2 = \"The medical profession is short on facemasks during this period, more are needed\"\n\nembeddings.similarity(Tokenizer.tokenize(sentence1), [Tokenizer.tokenize(sentence2)])","1bef4136":"from paperai.query import Query\n\n# Execute a test query\nQuery.run(\"antiviral covid-19 success treatment\", 5, \"cord19q\")","9c7f9c7e":"%%capture --no-display\n\nfrom paperai.report.execute import Execute as Report\nfrom IPython.display import display, Markdown\n\nquery = \"\"\"\nname: query\n\nantiviral covid-19 success treatment:\n    query: antiviral covid-19 success treatment\n    columns:\n        - name: Date\n        - name: Study\n        - name: Study Type\n        - name: Sample Size\n        - name: Study Population\n        - name: Matches\n        - name: Entry\n\"\"\"\n\n# Execute report query\nReport.run(query, 10, \"md\", \"cord19q\")\n\n# Render report\ndisplay(Markdown(filename=\"query.md\"))","48473106":"## Tagged Articles by Source\nThe following graph shows the articles grouped by the source field in the metadata.","fc13e420":"The vector model is good at identifying near matches, which helps increase the accuracy of the overall model. Notice that the top hits are typos (covid-10 mistyped 0 instead of 9). \n\nBelow shows similarity for a list of terms, numbers look overall as expected, model has learned an association between the various diseases and knows phone is not related.","047253aa":"# Exploring the data\nThe articles database has a copy of all articles that were found in metadata.csv. Pure duplicate articles (based on the sha hash) are filtered out. In addition to the metadata and text fields, a field named tags is added. Each article is tagged based on the topic. The only tag at this time is COVID-19 for articles that directly mention COVID-19 and related terms. This field is important as the embedding index and all model searches will go against the subset of data tagged as COVID-19.","50deb84a":"## Run a query\nRun a full query to ensure model is working.","124383f0":"## Articles Table\nA sample of the articles table is shown below.","faabf89e":"## Exploration Takeaways\nGiven the urgency to find any data to help, many of the tagged articles are recent. Publications by nature put hypothesises and theories through a rigorious scientific method\/peer review to ensure accuracy and reliability. It's a balancing act of not holding on to data that can help against making sure decisions are based on accurate data. Given that all searches are against this subset of data, conclusions should be carefully drawn. ","b27b839c":"# Build Embedding Index\n\nAn embeddings index is created with [FastText](https:\/\/fasttext.cc\/) + [BM25](https:\/\/en.wikipedia.org\/wiki\/Okapi_BM25). Background on this method can be found in this [Medium article](https:\/\/towardsdatascience.com\/building-a-sentence-embedding-index-with-fasttext-and-bm25-f07e7148d240) and an existing repository using this method [codequestion](https:\/\/github.com\/neuml\/codequestion).\n\nThe embeddings index takes each COVID-19 tagged, not labeled a question\/fragment, having a detected study type, tokenizes the text, and builds a sentence embedding. A sentence embedding is a BM25 weighted combination of the FastText vectors for each token in the sentence. The embeddings index takes the full corpus of these embeddings and builds a [Faiss](https:\/\/github.com\/facebookresearch\/faiss) index to enable similarity searching. \n\nImportant source files to highlight\n\n* Indexing Process -> [index.py](https:\/\/github.com\/neuml\/paperai\/blob\/master\/src\/python\/paperai\/index.py)\n* Tokenizer -> [tokenizer.py](https:\/\/github.com\/neuml\/paperai\/blob\/master\/src\/python\/paperai\/tokenizer.py)\n* Embeddings Model -> [embeddings.py](https:\/\/github.com\/neuml\/paperai\/blob\/master\/src\/python\/paperai\/embeddings.py)\n* BM25 Scoring -> [scoring.py](https:\/\/github.com\/neuml\/paperai\/blob\/master\/src\/python\/paperai\/scoring.py)\n\nFastText vectors trained on the full CORD-19 corpus are required. A [dataset with pre-trained vectors](https:\/\/www.kaggle.com\/davidmezzetti\/cord19-fasttext-vectors) is included and used in this notebook. Building the vectors takes a couple of hours when locally trained and would most likely take much longer within a notebook. \n\nVectors can optionally be (re)built by running the following command with the project and articles.sqlite database installed locally:\n\n```\npython -m paperai.vectors\n```\n\nThe following code builds the embeddings index using fastText vectors trained on the full CORD-19 dataset. Alternatively, any [pymagnitude vector file](https:\/\/github.com\/plasticityai\/magnitude#pre-converted-magnitude-formats-of-popular-embeddings-models) can be used the build the sentence embeddings.","1c5c6f52":"## Sections Table\nIn addition to the articles table, another table named sections is also created. The full text content is stored here. Each row is a single sentence from an article. Sentences are parsed using [NTLK's](https:\/\/www.nltk.org\/) sent_tokenize method. The article id and tags are also stored with each section. The sections schema and sample rows are shown below.","ab76275c":"## Tagged Articles by Publication\nThe graph below shows the articles grouped by publication. Only the Top 20 publications are shown and many articles have no publication.","666b9612":"# Testing the model\n\nNow that both the articles.sqlite database and embeddings index are both created, lets test that everything is working properly.","6ed413c1":"# cord19q: COVID-19 Open Research Dataset (CORD-19) Analysis\n\n![CORD19](https:\/\/pages.semanticscholar.org\/hs-fs\/hubfs\/covid-image.png?width=300&name=covid-image.png)\n\n***NOTE: There is a [Report Builder Notebook](https:\/\/www.kaggle.com\/davidmezzetti\/cord-19-report-builder) that runs on a prebuilt model. If you just want to try this out without a full build, this is the best choice. Links to [task submissions](#Round-1-Tasks) are available at the bottom of this notebook.***\n\nCOVID-19 Open Research Dataset (CORD-19) is a free resource of scholarly articles, aggregated by a coalition of leading research groups, covering COVID-19 and the coronavirus family of viruses. The dataset can be found on [Semantic Scholar](https:\/\/pages.semanticscholar.org\/coronavirus-research) and there is a CORD-19 challenge on [Kaggle](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge).\n\nThe cord19q project builds an index over the CORD-19 dataset to assist with analysis and data discovery. A series of COVID-19 related research topics were explored to identify relevant articles and help find answers to key scientific questions.  \n\n*An example result snippet is shown below for one of the task questions. For each task question, a highlights section is built with a summary of the results along with each matching article and each article's text matches.*\n\n>## Effectiveness of drugs being developed and tried to treat COVID-19 patients.\n>\n>#### Highlights<br\/>\n>- Currently, there are no approved drugs to treat the infection. [Arya et al](https:\/\/doi.org\/10.26434\/chemrxiv.11860011.v2)<br\/>\n>- Antiviral drugs: lopinavir\/ritonavir and ribavirin had been tried to treat SARS disease with apparent favorable clinical response. [Wu et al](https:\/\/doi.org\/10.1097\/jcma.0000000000000270)<br\/>\n>- Although antiviral drugs, including osehamivir and ribavirin had been applied to our patients, to date no effective antiviral to treat COVID-19 has been identified. [Xu et al](https:\/\/doi.org\/10.1101\/2020.03.03.20030668)<br\/>\n>\n>#### Articles<br\/>\n>|Date|Authors|Title|LOE & Sample|Matches|\n>|----|----|----|----|----|\n>|2020|Wang et al|[Science in the fight against the novel coronavirus disease](https:\/\/doi.org\/10.1097\/cm9.0000000000000777)<br\/>Chin Med J (Engl)|II. Randomized Controlled Trial<br\/><br\/>They performed a metagenomic analysis of respiratory tract specimens obtained from five patients suffering from the pneumonia in question and identified the virus now known as 2019-nCoV as the causative agent.|As specific therapies targeting 2019-nCoV are lacking, it may be useful to repurpose drugs already licensed for marketing or clinical trials to treat COVID-19 patients in an emergency response; researchers are actively working to identify such drugs.<br\/><br\/> Clinical trials are also underway to validate the effectiveness of various other licensed drugs against COVID-19.|","dcec6d8b":"## Sentence Embeddings\nAt the highest level, the model builds embeddings for each sentence in the corpus. For input queries, it compares each sentence against the input query. Faiss enables that similarity search to be fast. An example of how this works at a small level below.","fe380b30":"# Building task reports\nTask reports are an aggregation of each question within a task in the challenge. For each question, a query is run and the top articles are returned. For each article, text matches are shown as bulleted points and these are the best matching sentences within the article. The full list of result sentences are also analyzed and run through a [textrank algorithm](https:\/\/en.wikipedia.org\/wiki\/Automatic_summarization#TextRank_and_LexRank). Highlights or top sentences within the results are also shown within the report. \n\nImportant source files to highlight\n* Report Process -> [execute.py](https:\/\/github.com\/neuml\/cord19q\/blob\/master\/src\/python\/cord19q\/report\/execute.py)\n* Textrank algorithm to highlight best sentences -> [highlights.py](https:\/\/github.com\/neuml\/cord19q\/blob\/master\/src\/python\/cord19q\/highlights.py)\n\nQueries use a YAML formatted syntax that allows customizing the query string and result columns. The following example shows how to build a task report.","e58e2713":"This notebook requires Internet connectivity to be enabled. If this notebook is copied, the GitHub project could also be forked for an edited notebook to modify the Python code. Would simply just need to update the pip install command above to the new repository location.","edc2d91a":"# Build SQLite articles database\n\nThe raw CORD-19 data is stored across a metadata.csv file and json files with the full text. This project uses [SQLite](https:\/\/www.sqlite.org\/index.html) to aggregate and store the merged content.\n\nThe ETL process transforms the csv\/json files into a SQLite database. The process iterates over each row in metadata.csv, extracts the column data and ensures it is not a pure duplicate (using the sha hash). This process will also load the full text if available. \n\n## Tagging\nArticles are tagged based on keyword matches. The only tag at this time is COVID-19 and articles are tagged with this if the article text contains any of the following regular expressions. \n\n>2019[\\-\\s]?n[\\-\\s]?cov, 2019 novel coronavirus, coronavirus 2(?:019)?, coronavirus disease (?:20)?19, covid(?:[\\-\\s]?(?:20)?19)?, n\\s?cov[\\-\\s]?2019, sars[\\-\\s]cov-?2, wuhan (?:coronavirus|cov|pneumonia)\n\nCredit to [@ajrwhite](https:\/\/www.kaggle.com\/ajrwhite) and his [notebook](https:\/\/www.kaggle.com\/ajrwhite\/covid-19-thematic-tagging-with-regular-expressions) in helping to develop this list.\n\n## Study Design\nAdditional metadata is parsed out of the article to derive information on the study design. The models referenced in this section are [available as a dataset](https:\/\/www.kaggle.com\/davidmezzetti\/cord19-study-design). \n\n### Design Type\nThe full text is analyzed to determine a design type for the backing study in the article using a machine learning model. The model has a pre-defined vocabulary and features are a count of each of these defined keywords. A Random Forest Classifier is then trained using the feature set and is used to predict study design labels. \n\nCredit to [@savannareid](https:\/\/www.kaggle.com\/savannareid) for developing the keywords to use with this method. The keywords can be found in this [domain dictionary](https:\/\/docs.google.com\/spreadsheets\/d\/1t2e3CHGxHJBiFgHeW0dfwtvCG4x0CDCzcTFX7yz9Z2E\/edit#gid=389064679). More details on deriving a study design can be found in [this discussion](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/139355).\n\n### Attribute Type Detection and Extraction\nAdditionally, the full text is analyzed to identify study metadata for the backing study in the article also using a machine learning model. The model has a combination of features including a TF-IDF vector of the text elements and Natural Lanuage Processing (NLP) elements. The NLP features are built from entity, part of speech and dependency labels extracted with [scispacy](https:\/\/allenai.github.io\/scispacy\/). scispacy has been pretrained on medical articles and has good detection on articles in this dataset. A Logistic Regression Classifier is then trained using the feature set and is used to predict attribute labels.\n\nBased on the attribute type, further extraction is used via NLP. An example of this is with the sample size. Given a sentence \"34 patients were enrolled\", the logic will take the token patients and use dependency labels to extract the associated number (34) of patients to use as the sample size.\n\nAnother example of extraction is with risk factors with an example being the odds ratio of hypertension within a study. Using the detected entities, logic runs to find the matching statistic for a topic (such as hypertension) within a text section. The current process can currently only extract statistics, not calculate statistics from lower level data.\n\n## Grammar Labels\nThe title, abstract and full-text fields are tokenized into sentences. Linguistic rules are used to label each sentence to help identify concise, data-driven statements. \n\nFor the linguistic rules process, it has two basic rules right now.\n\n1. *QUESTION*: Sentence ending in a '?' mark\n2. *FRAGMENT*: Less informative\/incomplete statements. Acceptable sentences have the following structure.\n  - At least one nominal subject noun\/proper noun AND\n  - At least one action\/verb AND\n  - At least 5 words\n\nImportant source files to highlight\n- ETL Process -> [execute.py](https:\/\/github.com\/neuml\/paperetl\/blob\/master\/src\/python\/paperetl\/cord19\/execute.py)\n- Linguistic Rules -> [grammar.py](https:\/\/github.com\/neuml\/paperetl\/blob\/master\/src\/python\/paperetl\/grammar.py)\n- Study Design Model -> [design.py](https:\/\/github.com\/neuml\/paperetl\/blob\/master\/src\/python\/paperetl\/study\/design.py)\n- Attribute Model -> [attribute.py](https:\/\/github.com\/neuml\/paperetl\/blob\/master\/src\/python\/paperetl\/study\/attribute.py)\n- Sample Size Extraction -> [sample.py](https:\/\/github.com\/neuml\/paperetl\/blob\/master\/src\/python\/paperetl\/study\/sample.py)","f5694bb4":"Upon completion, a database named articles.sqlite will be stored in the output directory under a sub-folder named cord19q.","9470705f":"## Tagged Articles by Publication Month\nThe following graph shows articles by publication month. All of the articles have a publication date of 2020 or later (or the date is null). Many publication dates only include the year but there is a significant portion of articles this month, which shows the rapid pace things are moving. Also note that some publication dates are in the future. The articles have been released early to help find answers.","64a11f67":"# Round 1 Tasks\n\nThe following is a list of submitted [Round 1](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/148807) task notebooks. These notebooks are no longer updated as of 2020-05-02. \n\n- [What is known about transmission, incubation, and environmental stability?](https:\/\/www.kaggle.com\/davidmezzetti\/cord-19-transmission-incubation-environment)\n- [What do we know about COVID-19 risk factors?](https:\/\/www.kaggle.com\/davidmezzetti\/cord-19-risk-factors?scriptVersionId=33173909)\n- [What do we know about virus genetics, origin, and evolution?](https:\/\/www.kaggle.com\/davidmezzetti\/cord-19-virus-genetics-origin-and-evolution)\n- [What do we know about vaccines and therapeutics?](https:\/\/www.kaggle.com\/davidmezzetti\/cord-19-vaccines-and-therapeutics)\n- [What do we know about non-pharmaceutical interventions?](https:\/\/www.kaggle.com\/davidmezzetti\/cord-19-non-pharmaceutical-interventions)\n- [What has been published about medical care?](https:\/\/www.kaggle.com\/davidmezzetti\/cord-19-medical-care)\n- [What do we know about diagnostics and surveillance?](https:\/\/www.kaggle.com\/davidmezzetti\/cord-19-diagnostics-and-surveillance)\n- [What has been published about information sharing and inter-sectoral collaboration?](https:\/\/www.kaggle.com\/davidmezzetti\/cord-19-sharing-and-collaboration)\n- [What has been published about ethical and social science considerations?](https:\/\/www.kaggle.com\/davidmezzetti\/cord-19-ethical-and-social-science-considerations)\n\n# Round 2 Tasks\n\nThe following notebooks map to the tables in the [Kaggle COVID-19 Literature Review](https:\/\/www.kaggle.com\/covid-19-contributions) and [Round 2](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/150921).\n\n- [Task 1: Population](https:\/\/www.kaggle.com\/davidmezzetti\/cord-19-population)\n- [Task 2: Relevant Factors](https:\/\/www.kaggle.com\/davidmezzetti\/cord-19-relevant-factors)\n- [Task 3: Patient Descriptions](https:\/\/www.kaggle.com\/davidmezzetti\/cord-19-patient-descriptions)\n- [Task 4: Models and Open Questions](https:\/\/www.kaggle.com\/davidmezzetti\/cord-19-models-and-open-questions)\n- [Task 5: Materials](https:\/\/www.kaggle.com\/davidmezzetti\/cord-19-materials)\n- [Task 6: Diagnostics](https:\/\/www.kaggle.com\/davidmezzetti\/cord-19-diagnostics)\n- [Task 7: Therapeutics](https:\/\/www.kaggle.com\/davidmezzetti\/cord-19-therapeutics)\n- [Task 8: Risk Factors](https:\/\/www.kaggle.com\/davidmezzetti\/cord-19-risk-factors)\n- [Full Task CSV Export List](https:\/\/www.kaggle.com\/davidmezzetti\/cord-19-task-csv-exports)\n\n","a0320083":"## Tagged Articles by Country Mentioned\nThe following map shows the Articles by Country mentioned. China is mentioned significantly more and it's count is clipped in this graphic to allow showing distribution across the globe.","c3bbf6bb":"## Most Frequent Words in Tagged Articles\nThe following wordcloud shows the most frequent words within the titles of tagged articles.","457b5d79":"# Install from GitHub\n\n![](http:\/\/)Full source code for [cord19q](https:\/\/github.com\/neuml\/cord19q) is on GitHub and be installed into this notebook as follows:","d4571079":"## Word Embeddings\nThe foundation of sentence embeddings are word embeddings. As previously explained, sentence embeddings are just word embeddings joined together (each token weighted by a BM25 index). ","c8a85340":"## Tagged Articles by Study Design\nThe chart below shows articles grouped by study design type. The study design gives researchers insight into the overall structure and quality of a study. The more rigor and hard data that goes into a study, the more reliable. This is a distinction compared to many other search systems, where we look for the best matching text. Credibility of the information is very important \nin helping judge whether the conclusions are reliable. \n\nThe medical field is rightfully a skeptical field. Many technologists are accustomed to running a web search and quickly trying the top results until you get to something that works. Lets be glad our doctors don't do the same. "}}