{"cell_type":{"3581fc33":"code","8944d88a":"code","8562466d":"code","2d760295":"code","301b1f09":"code","02e95352":"code","0cb6d385":"code","7bb7be66":"code","5b847d6b":"code","0aba5d54":"code","08fea025":"code","88cdb732":"code","cce6028b":"code","41634f53":"code","ef1ec50f":"code","58f8050a":"code","d7be047c":"code","d989215a":"code","e2732873":"code","273cdb72":"markdown","e6723f21":"markdown","945108c9":"markdown","e0de8aa7":"markdown","17d3ece7":"markdown","7632bf91":"markdown","bd0b8f36":"markdown","64c86aec":"markdown","053e0662":"markdown","03a22f67":"markdown","bafbe8e1":"markdown","e4501049":"markdown","60ab93e9":"markdown","78f9d509":"markdown","c96a6d00":"markdown","44e09b59":"markdown","06b6a170":"markdown","d4865bdc":"markdown"},"source":{"3581fc33":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8944d88a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport sys\nfrom imblearn.over_sampling import SMOTE\n\nwarnings.filterwarnings(\"ignore\")\nsns.set()\nnp.set_printoptions(threshold=sys.maxsize)","8562466d":"data = pd.read_csv('\/kaggle\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\ndata.drop('enrollee_id', axis=1, inplace=True)\ndata.head(3)","2d760295":"sns.countplot(data=data, x='target')\nplt.show()","301b1f09":"from sklearn.datasets import make_classification\nX_ex, y_ex = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n                           n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=111)\n\ndata_ex = pd.DataFrame(np.concatenate([X_ex, y_ex.reshape(-1,1)],axis=1))\n\nzero = data_ex[data_ex[2]==0.0]\none = data_ex[data_ex[2]==1.0]\n\nX_ex_oversampled, y_ex_oversampled = SMOTE().fit_resample(X_ex, y_ex)\ndata_ex_oversampled = pd.DataFrame(np.concatenate([X_ex_oversampled, y_ex_oversampled.reshape(-1,1)],axis=1))\n\nzero_oversampled = data_ex_oversampled[data_ex_oversampled[2]==0.0]\none_oversampled = data_ex_oversampled[data_ex_oversampled[2]==1.0]\n\nfig, ax = plt.subplots(1,2, sharey=True)\nfig.set_size_inches(13,5)\nax[0].scatter(zero[0], zero[1], label='class 0')\nax[0].scatter(one[0], one[1], label='class 1')\nax[0].set_title('Original Data')\nax[0].set_ylabel('feature_1')\nax[0].set_xlabel('feature_0')\nax[1].scatter(zero_oversampled[0], zero_oversampled[1], label='class 0')\nax[1].scatter(one_oversampled[0], one_oversampled[1], label='class 1')\nax[1].set_title('Oversampled Data')\nax[1].set_xlabel('feature_0')\nplt.legend(bbox_to_anchor=(1.3,1))\nplt.show()","02e95352":"numeric = data.select_dtypes(exclude='object')\ncategory = data.select_dtypes(include='object')","0cb6d385":"missing = pd.DataFrame(data.isnull().sum()\/len(data), columns=['Missing'])\n\ncm = sns.light_palette(\"green\", as_cmap=True)\nmissing.style.background_gradient(cmap=cm)","7bb7be66":"category_notNull = category.fillna('No')","5b847d6b":"category_notNull['company_size'] = category_notNull['company_size'].replace('10\/49', '10-49')","0aba5d54":"from sklearn.preprocessing import OrdinalEncoder\n\nOrdinal_encoder = OrdinalEncoder([\n    ['No', 'Primary School',  'High School', 'Graduate', 'Masters', 'Phd'],\n    'No,<1,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,>20'.split(','),\n    ['No','<10', '10-49', '50-99', '100-500', '500-999' ,'1000-4999', '5000-9999', '10000+'],\n    ['No','1', '2', '3', '4', '>4', 'never']\n])\n\ncategory_notNull_ordinal = category_notNull[['education_level', 'experience', 'company_size', 'last_new_job']]\n\ncategory_notNull_ordinalEncoded = Ordinal_encoder.fit_transform(category_notNull_ordinal)","08fea025":"from sklearn.preprocessing import OneHotEncoder\n\none_how_columns = [ col for col in category_notNull.columns if col not in ['education_level', 'experience', 'company_size', 'last_new_job']]\n\nohe = OneHotEncoder(sparse=False).fit(category_notNull.loc[:, one_how_columns])\n\ncategory_notNull_onehotEncoded = ohe.transform(category_notNull.loc[:, one_how_columns])","88cdb732":"from sklearn.model_selection import train_test_split\n\ncategory_preprocessed = np.concatenate([category_notNull_onehotEncoded, category_notNull_ordinalEncoded], axis=1)\n\nX = np.concatenate([numeric.drop('target', axis=1).values, category_preprocessed], axis=1)\ny = numeric['target'].values\n\n\nX, y = SMOTE().fit_resample(X, y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)","cce6028b":"X_train[:5,:5]","41634f53":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler().fit(X_train[:,:2])\n\nX_train_scaled = X_train.copy()\nX_train_scaled[:,:2] = scaler.transform(X_train[:,:2])\n\nX_test_scaled = X_test.copy()\nX_test_scaled[:,:2] = scaler.transform(X_test[:,:2])","ef1ec50f":"from sklearn.metrics import confusion_matrix, classification_report, plot_roc_curve, roc_auc_score, roc_curve \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\nestimators = {\n    'Logistic Regression': [LogisticRegression()],\n    'Decision Tree' :[DecisionTreeClassifier()],\n    'Random Forest' :[RandomForestClassifier()],\n    'Gradient Boost' :[GradientBoostingClassifier()],\n    'XG Boost': [XGBClassifier()],\n}\n\n\ndef mfit(estimators, X_train, y_train):\n    for m in estimators:\n        estimators[m][0].fit(X_train, y_train)\n        print(m+' fitted')\n\nmfit(estimators, X_train_scaled, y_train)","58f8050a":"\ndef mpredict(estimators, X_test, y_test):\n    outcome = dict()\n    r_a_score = dict()\n    for m in estimators:\n        y_pred = estimators[m][0].predict(X_test)\n        r_a_score[m] = roc_auc_score(y_test, y_pred)\n        outcome[m] = [y_pred, confusion_matrix(y_pred,y_test), classification_report(y_pred,y_test)]\n    return outcome, r_a_score\n\noutcome, r_a_score = mpredict(estimators, X_test_scaled, y_test)","d7be047c":"for m in outcome:\n    print('------------------------'+m+'------------------------')\n    print(outcome[m][1])\n    print(outcome[m][2])","d989215a":"print('roc_auc_score')\nfor m in r_a_score:\n    print('------------------------'+m+'------------------------')\n    print(r_a_score[m])\n","e2732873":"fig, ax = plt.subplots()\nfig.set_size_inches(13,6)\n\nfor m in estimators:\n    y_pred = estimators[m][0].predict_proba(X_test_scaled)\n    fpr, tpr, _ = roc_curve(y_test, y_pred[:,1].ravel())\n    plt.plot(fpr,tpr, label=m)\nplt.xlabel('False-Positive rate')\nplt.ylabel('True-Positive rate')\nplt.legend()\nplt.show()","273cdb72":"## Be aware of imbalancing, we'll deal with it later.","e6723f21":"## Imbalanced dataset\nA problem with imbalanced classification is that there are too few examples of the minority class for a model to effectively learn the decision boundary.\n\nOne way to solve this problem is to oversample the examples in the minority class. This can be achieved by simply duplicating examples from the minority class in the training dataset prior to fitting a model. This can balance the class distribution but does not provide any additional information to the model. \n\nInstead, new samples can be synthesized from the existing samples. This is a type of data augmentation for the minority class and is referred to as the Synthetic Minority Oversampling Technique, or SMOTE\n\nSMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and creating a new sample at a point along that line.","945108c9":"# 1) Missing values","e0de8aa7":"First two columns are numerical data, others is encoded data.","17d3ece7":"## 2.1) Ordinal encode","7632bf91":"### Clean the data a little bit.","bd0b8f36":"# 3) Splitting the data","64c86aec":"- enrollee_id : Unique ID for candidate\n- city: City code\n- city_ development _index : Developement index of the city (scaled)\n- gender: Gender of candidate\n- relevent_experience: Relevant experience of candidate\n- enrolled_university: Type of University course enrolled if any\n- education_level: Education level of candidate\n- major_discipline :Education major discipline of candidate\n- experience: Candidate total experience in years\n- company_size: No of employees in current employer's company\n- company_type : Type of current employer\n- lastnewjob: Difference in years between previous job and current job\n- training_hours: training hours completed\n\ntarget: 0 \u2013 Not looking for job change, 1 \u2013 Looking for a job change","053e0662":"## 2.2) One hot encode","03a22f67":"# Looking to ROC curve","bafbe8e1":"# 4) Scale","e4501049":"### Deal with NaN in categorical features.","60ab93e9":"Edit, 05-Jan-2021, Deal with imbalancing data by oversampling -> accuracy, precision, recall, auc socre, and roc curve are improved","78f9d509":"# 2) Encode categorical features","c96a6d00":"# 5) Building some models. <br>\nI'll use default parameter.","44e09b59":"# 6) Let's predict!","06b6a170":"**First, I'll separate categorical columns and numerical columns into two DataFrame named \"category\", \"numeric\" respectively.**","d4865bdc":"\"SMOTE first selects a minority class instance a at random and finds its k nearest minority class neighbors. The synthetic instance is then created by choosing one of the k nearest neighbors b at random and connecting a and b to form a line segment in the feature space. The synthetic instances are generated as a convex combination of the two chosen instances a and b.\"\n\n[reference](https:\/\/machinelearningmastery.com\/smote-oversampling-for-imbalanced-classification\/)"}}