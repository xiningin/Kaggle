{"cell_type":{"81303db4":"code","885d4d3d":"code","c96dd66b":"code","a3137a51":"code","348379c9":"code","36bf22f8":"code","1519f940":"code","fdb9e3a8":"code","537f62a9":"code","ffe73f8e":"code","bbe39af0":"code","cb040e47":"code","ab3f6a4b":"code","82e58f95":"code","cca451d7":"code","b6a7264b":"code","faab4c49":"code","d8fe5a7e":"code","c08cd056":"code","7690ee7f":"code","8c60b2cd":"code","601ee5b1":"code","647696cf":"code","117f1cfb":"code","723d4a08":"code","f6a11b70":"code","d57a6f86":"code","df40af03":"code","89a4569e":"code","3b9d9a10":"code","05c08e77":"code","5f33efc3":"code","437d4378":"code","b29d14ee":"code","b86465fd":"code","b4d5b29a":"code","d402ae71":"code","33324774":"code","e4231d9b":"code","7faefb96":"code","523177a0":"code","af1d9cc4":"code","93edf318":"code","4ed9465f":"code","d2fbbe72":"code","e3347757":"code","cc0c1c99":"code","acbb7574":"code","67109f43":"code","d3471df7":"code","3659ae0f":"code","89b678db":"code","226ddd08":"code","c8912c6f":"code","bab2faf5":"code","48664f5b":"code","c66e65b8":"code","70c88f9c":"code","4470431c":"code","e06f1578":"code","1a0a80d2":"code","825dd769":"code","409df23c":"code","cf34c6c5":"code","1157a5b8":"code","56845793":"code","2370f198":"code","6c5163f6":"code","edc5b5df":"code","ac21064d":"code","f4cf1e5d":"code","b422a155":"code","9a8c620a":"code","cdcf76f4":"code","e457cde2":"code","537e0048":"code","364e84c8":"code","2a6145a8":"code","b6707abd":"code","410e15ab":"code","2a743f6d":"code","2805f2ab":"code","01cfdb82":"code","340dd2d0":"code","bd42e3a4":"code","8bd08c11":"code","9e8f8101":"code","9b09b4b1":"code","71b787a9":"code","40fe2139":"code","9b7414c9":"code","d58caa55":"code","a3098919":"code","4237cc36":"code","36d66668":"code","72a5fd36":"code","00434749":"code","df0be3f8":"code","10223e18":"code","5ce134ce":"code","ea2e3991":"code","c0a3ec2c":"code","e7fa5107":"code","993e52c0":"code","7505b179":"code","157f4a06":"code","8314078d":"code","5c96e7f2":"code","dcad1c3d":"code","2ee2963f":"code","f225b0e9":"code","748881c7":"code","4ba49d3d":"code","d7d6cd1e":"code","89c2c448":"code","85879b48":"code","150b0347":"code","6f7903b5":"code","17a58f46":"code","404a362c":"code","cac075f1":"code","f2a9c62d":"code","eb3bc146":"code","dee27889":"code","d017b0bf":"code","dc033342":"code","a1e15ae9":"code","f9918bd4":"code","132b0089":"code","35049415":"code","cb4680a0":"code","41dafa00":"markdown","a58b838d":"markdown","6ebce79c":"markdown","b6d88b07":"markdown","81a47ee9":"markdown","a8ac9a33":"markdown","26d94d89":"markdown","0809165f":"markdown","973eef4c":"markdown","a1a063ac":"markdown","2b013aa9":"markdown","4853abe8":"markdown","6dd1dc88":"markdown","cef4d7dd":"markdown","770f3607":"markdown","19060f76":"markdown","45965abd":"markdown","2472576d":"markdown","ae6c2990":"markdown"},"source":{"81303db4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","885d4d3d":"!pip install -qU xlrd openpyxl","c96dd66b":"train_data=pd.read_excel('..\/input\/flight-price\/Data_Train.xlsx')","a3137a51":"train_data.head()","348379c9":"train_data.info()","36bf22f8":"train_data.isnull().sum()","1519f940":"train_data.dropna(inplace=True)","fdb9e3a8":"train_data.isnull().sum()","537f62a9":"train_data.dtypes","ffe73f8e":"def change_into_datetime(col):\n    train_data[col]=pd.to_datetime(train_data[col])\n    ","bbe39af0":"train_data.columns","cb040e47":"for i in ['Date_of_Journey','Dep_Time', 'Arrival_Time']:\n    change_into_datetime(i)","ab3f6a4b":"train_data.dtypes","82e58f95":"train_data['Journey_day']=train_data['Date_of_Journey'].dt.day","cca451d7":"train_data['Journey_month']=train_data['Date_of_Journey'].dt.month","b6a7264b":"train_data.head()","faab4c49":"## Since we have converted Date_of_Journey column into integers, Now we can drop as it is of no use.\ntrain_data.drop('Date_of_Journey', axis=1, inplace=True)","d8fe5a7e":"train_data.head()","c08cd056":"def extract_hour(df,col):\n    df[col+\"_hour\"]=df[col].dt.hour","7690ee7f":"def extract_min(df,col):\n    df[col+\"_minute\"]=df[col].dt.minute","8c60b2cd":"def drop_column(df,col):\n    df.drop(col,axis=1,inplace=True)","601ee5b1":"# Departure time is when a plane leaves the gate. \n# Similar to Date_of_Journey we can extract values from Dep_Time\nextract_hour(train_data,'Dep_Time')","647696cf":"# Extracting Minutes\nextract_min(train_data,'Dep_Time')","117f1cfb":"# Now we can drop Dep_Time as it is of no use\ndrop_column(train_data,'Dep_Time')","723d4a08":"train_data.head()","f6a11b70":"# Arrival time is when the plane pulls up to the gate.\n# Similar to Date_of_Journey we can extract values from Arrival_Time\n\n# Extracting Hours\nextract_hour(train_data,'Arrival_Time')\n\n# Extracting minutes\nextract_min(train_data,'Arrival_Time')\n\n# Now we can drop Arrival_Time as it is of no use\ndrop_column(train_data,'Arrival_Time')","d57a6f86":"train_data.head()","df40af03":"'2h 50m'.split(' ')","89a4569e":"duration=list(train_data['Duration'])\n\nfor i in range(len(duration)):\n    if len(duration[i].split(' '))==2:\n        pass\n    else:\n        if 'h' in duration[i]:                   # Check if duration contains only hour\n            duration[i]=duration[i] + ' 0m'      # Adds 0 minute\n        else:\n            duration[i]='0h '+ duration[i]       # if duration contains only second, Adds 0 hour\n    ","3b9d9a10":"train_data['Duration']=duration","05c08e77":"train_data.head()","5f33efc3":"'2h 50m'.split(' ')[1][0:-1]","437d4378":"def hour(x):\n    return x.split(' ')[0][0:-1]\n","b29d14ee":"def min(x):\n    return x.split(' ')[1][0:-1]","b86465fd":"train_data['Duration_hours']=train_data['Duration'].apply(hour)\ntrain_data['Duration_mins']=train_data['Duration'].apply(min)","b4d5b29a":"train_data.head()","d402ae71":"train_data.drop('Duration',axis=1,inplace=True)","33324774":"train_data.head()","e4231d9b":"train_data.dtypes","7faefb96":"train_data['Duration_hours']=train_data['Duration_hours'].astype(int)\ntrain_data['Duration_mins']=train_data['Duration_mins'].astype(int)","523177a0":"train_data.dtypes","af1d9cc4":"train_data.head()","93edf318":"train_data.dtypes","4ed9465f":"cat_col=[col for col in train_data.columns if train_data[col].dtype=='O']\ncat_col","d2fbbe72":"cont_col=[col for col in train_data.columns if train_data[col].dtype!='O']\ncont_col","e3347757":"categorical=train_data[cat_col]\ncategorical.head()","cc0c1c99":"categorical['Airline'].value_counts()","acbb7574":"plt.figure(figsize=(15,5))\nsns.boxplot(y='Price',x='Airline',data=train_data.sort_values('Price',ascending=False))","67109f43":"plt.figure(figsize=(15,5))\nsns.boxplot(y='Price',x='Total_Stops',data=train_data.sort_values('Price',ascending=False))","d3471df7":"len(categorical['Airline'].unique())","3659ae0f":"# As Airline is Nominal Categorical data we will perform OneHotEncoding\nAirline=pd.get_dummies(categorical['Airline'], drop_first=True)\nAirline.head()","89b678db":"categorical['Source'].value_counts()","226ddd08":"# Source vs Price\n\nplt.figure(figsize=(15,5))\nsns.catplot(y='Price',x='Source',data=train_data.sort_values('Price',ascending=False),kind='boxen')","c8912c6f":"# As Source is Nominal Categorical data we will perform OneHotEncoding\n\n\nSource=pd.get_dummies(categorical['Source'], drop_first=True)\nSource.head()","bab2faf5":"categorical['Destination'].value_counts()","48664f5b":"# As Destination is Nominal Categorical data we will perform OneHotEncoding\n\nDestination=pd.get_dummies(categorical['Destination'], drop_first=True)\nDestination.head()","c66e65b8":"categorical['Route']","70c88f9c":"categorical['Route_1']=categorical['Route'].str.split('\u2192').str[0]\ncategorical['Route_2']=categorical['Route'].str.split('\u2192').str[1]\ncategorical['Route_3']=categorical['Route'].str.split('\u2192').str[2]\ncategorical['Route_4']=categorical['Route'].str.split('\u2192').str[3]\ncategorical['Route_5']=categorical['Route'].str.split('\u2192').str[4]","4470431c":"categorical.head()","e06f1578":"import warnings \nfrom warnings import filterwarnings\nfilterwarnings('ignore')","1a0a80d2":"categorical['Route_1'].fillna('None',inplace=True)\ncategorical['Route_2'].fillna('None',inplace=True)\ncategorical['Route_3'].fillna('None',inplace=True)\ncategorical['Route_4'].fillna('None',inplace=True)\ncategorical['Route_5'].fillna('None',inplace=True)","825dd769":"categorical.head()","409df23c":"#now extract how many categories in each cat_feature\nfor feature in categorical.columns:\n    print('{} has total {} categories \\n'.format(feature,len(categorical[feature].value_counts())))","cf34c6c5":"### as we will see we have lots of features in Route , one hot encoding will not be a better option lets appply Label Encoding","1157a5b8":"from sklearn.preprocessing import LabelEncoder\nencoder=LabelEncoder()","56845793":"categorical.columns","2370f198":"for i in ['Route_1', 'Route_2', 'Route_3', 'Route_4','Route_5']:\n    categorical[i]=encoder.fit_transform(categorical[i])","6c5163f6":"categorical.head()","edc5b5df":"# Additional_Info contains almost 80% no_info,so we can drop this column\n# we can drop Route as well as we have pre-process that column\n    \ndrop_column(categorical,'Route')\ndrop_column(categorical,'Additional_Info')","ac21064d":"categorical.head()","f4cf1e5d":"categorical['Total_Stops'].value_counts()","b422a155":"categorical['Total_Stops'].unique()","9a8c620a":"# As this is case of Ordinal Categorical type we perform LabelEncoder\n# Here Values are assigned with corresponding key\n\ndict={'non-stop':0, '2 stops':2, '1 stop':1, '3 stops':3, '4 stops':4}","cdcf76f4":"categorical['Total_Stops']=categorical['Total_Stops'].map(dict)","e457cde2":"categorical.head()","537e0048":"train_data[cont_col]","364e84c8":"# Concatenate dataframe --> categorical + Airline + Source + Destination\n\ndata_train=pd.concat([categorical,Airline,Source,Destination,train_data[cont_col]],axis=1)\ndata_train.head()","2a6145a8":"drop_column(data_train,'Airline')\ndrop_column(data_train,'Source')\ndrop_column(data_train,'Destination')","b6707abd":"data_train.head()","410e15ab":"pd.set_option('display.max_columns',35)","2a743f6d":"data_train.head()","2805f2ab":"data_train.columns","01cfdb82":"def plot(df,col):\n    fig,(ax1,ax2)=plt.subplots(2,1)\n    sns.distplot(df[col],ax=ax1)\n    sns.boxplot(df[col],ax=ax2)\n    ","340dd2d0":"plt.figure(figsize=(30,20))\nplot(data_train,'Price')","bd42e3a4":"data_train['Price']=np.where(data_train['Price']>=40000,data_train['Price'].median(),data_train['Price'])","8bd08c11":"plt.figure(figsize=(30,20))\nplot(data_train,'Price')","9e8f8101":"### separate your independent & dependent data","9b09b4b1":"X=data_train.drop('Price',axis=1)\nX.head()","71b787a9":"y=data_train['Price']\ny","40fe2139":"##type(X)","9b7414c9":"##type(y)","d58caa55":"##X.isnull().sum()","a3098919":"##y.isnull().sum()","4237cc36":"#### as now we dont have any missing value in data, we can definitely go ahead with Feature Selection","36d66668":"###np.array(X)","72a5fd36":"##np.array(y)","00434749":"from sklearn.feature_selection import mutual_info_classif","df0be3f8":"# mutual_info_classif()","10223e18":"# mutual_info_classif(np.array(X),np.array(y))","5ce134ce":"X.dtypes","ea2e3991":"mutual_info_classif(X,y)","c0a3ec2c":"imp=pd.DataFrame(mutual_info_classif(X,y),index=X.columns)\nimp","e7fa5107":"imp.columns=['importance']\nimp.sort_values(by='importance',ascending=False)","993e52c0":"from sklearn.model_selection import train_test_split","7505b179":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)","157f4a06":"from sklearn import metrics\n##dump your model using pickle so that we will re-use\nimport pickle\ndef predict(ml_model,dump):\n    model=ml_model.fit(X_train,y_train)\n    print('Training score : {}'.format(model.score(X_train,y_train)))\n    y_prediction=model.predict(X_test)\n    print('predictions are: \\n {}'.format(y_prediction))\n    print('\\n')\n    r2_score=metrics.r2_score(y_test,y_prediction)\n    print('r2 score: {}'.format(r2_score))\n    print('MAE:',metrics.mean_absolute_error(y_test,y_prediction))\n    print('MSE:',metrics.mean_squared_error(y_test,y_prediction))\n    print('RMSE:',np.sqrt(metrics.mean_squared_error(y_test,y_prediction)))\n    sns.distplot(y_test-y_prediction)\n    \n    if dump==1:\n        ##dump your model using pickle so that we will re-use\n        file=open('.\/model.pkl','wb')\n        pickle.dump(model,file)","8314078d":"from sklearn.ensemble import RandomForestRegressor","5c96e7f2":"predict(RandomForestRegressor(),1)","dcad1c3d":"\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor","2ee2963f":"predict(DecisionTreeRegressor(),0)","f225b0e9":"predict(LinearRegression(),0)","748881c7":"from sklearn.model_selection import RandomizedSearchCV","4ba49d3d":"# Number of trees in random forest\nn_estimators=[int(x) for x in np.linspace(start=100,stop=1200,num=6)]\n\n# Number of features to consider at every split\nmax_features=['auto','sqrt']\n\n# Maximum number of levels in tree\nmax_depth=[int(x) for x in np.linspace(5,30,num=4)]\n\n# Minimum number of samples required to split a node\nmin_samples_split=[5,10,15,100]","d7d6cd1e":"# Create the random grid\n\nrandom_grid={\n    'n_estimators':n_estimators,\n    'max_features':max_features,\n'max_depth':max_depth,\n    'min_samples_split':min_samples_split\n}","89c2c448":"random_grid","85879b48":"### initialise your estimator\nreg_rf=RandomForestRegressor()","150b0347":"# Random search of parameters, using 3 fold cross validation\n\nrf_random=RandomizedSearchCV(estimator=reg_rf,param_distributions=random_grid,cv=3,verbose=2,n_jobs=-1)","6f7903b5":"rf_random.fit(X_train,y_train)","17a58f46":"rf_random.best_params_","404a362c":"prediction=rf_random.predict(X_test)","cac075f1":"sns.distplot(y_test-prediction)","f2a9c62d":"metrics.r2_score(y_test,prediction)","eb3bc146":"print('MAE',metrics.mean_absolute_error(y_test,prediction))\nprint('MSE',metrics.mean_squared_error(y_test,prediction))\nprint('RMSE',np.sqrt(metrics.mean_squared_error(y_test,prediction)))","dee27889":"!pip install pickle","d017b0bf":"import pickle","dc033342":"# open a file, where you want to store the data\nfile=open('rf_random.pkl','wb')","a1e15ae9":"# dump information to that file\npickle.dump(rf_random,file)","f9918bd4":"model=open('rf_random.pkl','rb')\nforest=pickle.load(model)","132b0089":"y_prediction=forest.predict(X_test)","35049415":"y_prediction","cb4680a0":"metrics.r2_score(y_test,y_prediction)","41dafa00":"#### Perform Total_Stops vs Price Analysis","a58b838d":"#### Hyperparameter Tuning\n    1.Choose following method for hyperparameter tuning\n        a.RandomizedSearchCV --> Fast way to Hypertune model\n        b.GridSearchCV--> Slow way to hypertune my model\n    \n    2.Assign hyperparameters in form of dictionary\n    3.Fit the model\n    4.Check best paramters and best score","6ebce79c":"### Feature Selection\n    Finding out the best feature which will contribute and have good relation with target variable. \n    \n### Why to apply Feature Selection?\n    To select important features to get rid of curse of dimensionality ie..to get rid of duplicate features","b6d88b07":"##### Conclusion-->  From graph we can see that Jet Airways Business have the highest Price., Apart from the first Airline almost all are having similar median","81a47ee9":"#### From description we can see that Date_of_Journey is a object data type,\n     Therefore, we have to convert this datatype into timestamp so as to use this column properly for prediction,bcz our \n     model will not be able to understand Theses string values,it just understand Time-stamp\n    For this we require pandas to_datetime to convert object data type to datetime dtype.\n\n\n    dt.day method will extract only day of that date\n    dt.month method will extract only month of that date","a8ac9a33":"#### Importing dataset\n    1.Since data is in form of excel file we have to use pandas read_excel to load the data\n    2.After loading it is important to check null values in a column or a row\n    3.If it is present then following can be done,\n        a.Filling NaN values with mean, median and mode using fillna() method\n        b.If Less missing values, we can drop it as well\n","26d94d89":"##### Save the model to reuse it again","0809165f":"#### dealing with Outliers","973eef4c":"\n#### We are using 2 main Encoding Techniques to convert Categorical data into some numerical format\n    Nominal data --> data are not in any order --> OneHotEncoder is used in this case\n    Ordinal data --> data are in order -->       LabelEncoder is used in this case","a1a063ac":"#### as less missing values,I can directly drop these","2b013aa9":"### Handling Categorical Data","4853abe8":"### outlier detection","6dd1dc88":"####  Feature Selection using Information Gain,","cef4d7dd":"#### Lets Apply pre-processing on duration column,Separate Duration hours and minute from duration","770f3607":"#### Airline vs Price Analysis","19060f76":"#### play with multiple Algorithms","45965abd":"#### import randomforest class","2472576d":"### I wanted to find mutual information scores or matrix to get to know about the relationship between all features.","ae6c2990":"#### split dataset into train & test"}}