{"cell_type":{"33024b8a":"code","966f4e92":"code","9c80d4f9":"code","643208d6":"code","824acb38":"code","e25e1357":"code","c0fbfbb1":"code","a9b1037c":"code","a0135d50":"code","462744b4":"code","1059b7bd":"code","dd75abfd":"code","11b71dad":"code","3770527c":"code","12cf0e4a":"code","65bb6ccc":"code","bf817d31":"code","72ca48b4":"code","d8f74776":"code","a3e3bf61":"code","51fbb35c":"code","4708ecb0":"code","d1162a56":"code","98f29863":"code","35e57ac3":"code","3c238563":"code","e38008fd":"code","272a5a47":"code","95d95cdc":"code","0d65973e":"code","ed4f8c89":"code","fdcee883":"code","8f38e9ce":"code","143cc403":"code","d7801588":"code","2f2784d5":"code","3dd957bc":"code","d1ce3997":"code","39305491":"code","354d4bae":"code","666fc353":"code","d6cccaf8":"code","0962e13b":"code","320fc542":"code","f85e8559":"code","b096b45c":"code","8309be2f":"code","b5c6a7b2":"code","55cb6766":"code","e13e9d5d":"code","fb8eea86":"code","b0e28915":"code","1935641a":"code","89cf492b":"code","55b1a5ba":"code","ebad9b61":"code","8b17317d":"markdown","0bfe6976":"markdown","3df0e6c4":"markdown","b0c19818":"markdown","19ad5b6b":"markdown","62ebcfd2":"markdown","3845f514":"markdown","31b8319d":"markdown","9113b706":"markdown","afb855c7":"markdown","9ce7db8c":"markdown","f8cda895":"markdown","8aa16846":"markdown","3b5f1c70":"markdown","e8ec58bb":"markdown","bbc5c64c":"markdown"},"source":{"33024b8a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\u2248\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","966f4e92":"import pandas as pd\nimport numpy as np\nimport string \nstring.punctuation\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstopwords.words('english')\nimport re\nimport unicodedata\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer\n","9c80d4f9":"df_train=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf_train.head()","643208d6":"df_test=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ndf_test.head()","824acb38":"df_sample=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\ndf_sample.head()","e25e1357":"df_train.info()","c0fbfbb1":"df_train['target'].value_counts()","a9b1037c":"df_train['target'].value_counts().plot(kind='bar')","a0135d50":"df_train.head()","462744b4":"df_train['keyword'].value_counts()","1059b7bd":"df_train.groupby(['keyword']).mean()","dd75abfd":"df_train.describe(include='all')","11b71dad":"df_train.describe()","3770527c":"EMOJIS = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\nURLPATTERN        = r\"((http:\/\/)[^ ]*|(https:\/\/)[^ ]*|( www\\.)[^ ]*)\"\nUSERPATTERN       = '@[^\\s]+'\nSEQPATTERN   = r\"(.)\\1\\1+\"\nSEQREPLACE = r\"\\1\\1\"","12cf0e4a":"def tweets_cleaning(text):\n    lowercase = text.lower()\n    punc_removal = [char for char in lowercase if char not in string.punctuation]\n    punc_removal_joined = ''.join(punc_removal)\n    url_removal = re.sub(r'(https|http)?:\\\/\\\/(\\w|\\.|\\\/|\\?|\\=|\\&|\\%)*\\b', '', punc_removal_joined, flags=re.MULTILINE)\n    for emoji in EMOJIS.keys():\n        url_removal = url_removal.replace(emoji, \"EMOJI\" + EMOJIS[emoji])  \n   # emoji_removal = url_removal.encode('ascii', 'ignore').decode('ascii')\n    emoji_removal=url_removal\n    stopwords_removal = [word for word in emoji_removal.split() if word not in stopwords.words('english')]\n    return stopwords_removal","65bb6ccc":"df_train['cleaned_text']=df_train['text'].apply(tweets_cleaning).astype(str)","bf817d31":"df_test['cleaned_text']=df_test['text'].apply(tweets_cleaning).astype(str)","72ca48b4":"df_train.head()","d8f74776":"df_train_clean=df_train[['cleaned_text','target']]","a3e3bf61":"df_train_clean.head()","51fbb35c":"max_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(df_train_clean['cleaned_text'].values)\nX = tokenizer.texts_to_sequences(df_train_clean['cleaned_text'].values)\nX = pad_sequences(X)","4708ecb0":"embed_dim = 128\nlstm_out = 196\n\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","d1162a56":"Y = pd.get_dummies(df_train_clean['target']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","98f29863":"batch_size = 16\nmodel.fit(X_train, Y_train, epochs = 19, batch_size=batch_size, verbose = 2)","35e57ac3":"df_test.head()","3c238563":"df_test.info()","e38008fd":"max_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(df_test['cleaned_text'].values)\nX = tokenizer.texts_to_sequences(df_test['cleaned_text'].values)\nX = pad_sequences(X)","272a5a47":"pred=model.predict(X)","95d95cdc":"labels=[0,1]","0d65973e":"pred.shape","ed4f8c89":"pred","fdcee883":"tweet_predict=[]","8f38e9ce":"df_test.head()","143cc403":"df_test['id'][0]","d7801588":"idx=0\nfor p in pred:\n    predict=[]\n    t_id=df_test['id'][idx]\n    idx+=1\n    target=labels[np.argmax(p)]\n    predict.append(t_id)\n    predict.append(target)\n    tweet_predict.append(predict)\n    ","2f2784d5":"df_sub=pd.DataFrame(tweet_predict,columns=[\"id\",\"target\"])","3dd957bc":"df_sub.head()","d1ce3997":"df_sub['target'].value_counts()","39305491":"df_sub.to_csv('submission_lstm.csv',index=False)","354d4bae":"def save_prediction(pred,model):\n    df_test=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\n    submission_list=[]\n    idx=0\n    tweet_predict=[]\n    for p in pred:\n        predict=[]\n        t_id=df_test['id'][idx]\n        idx+=1\n        target=p\n        predict.append(t_id)\n        predict.append(target)\n        tweet_predict.append(predict)\n    df_sub=pd.DataFrame(tweet_predict,columns=['id','target'])\n    df_sub.to_csv('submission_'+model+\".csv\",index=False)\n    print(model+\"- model prediction\")\n    print(df_sub['target'].value_counts())\n    \n        ","666fc353":"df_train[\"kfold\"]=-1","d6cccaf8":"df_train=df_train.sample(frac=1).reset_index(drop=True)","0962e13b":"y=df_train.target","320fc542":"kf=model_selection.StratifiedKFold(n_splits=4)","f85e8559":"for f,(t_,v_) in enumerate(kf.split(X=df_train,y=y)):\n    df_train.loc[v_,'kfold']=f","b096b45c":"df_train.head()","8309be2f":"df_train['kfold'].value_counts()","b5c6a7b2":"for fold_ in range(5):\n    train_df=df_train[df_train.kfold!=fold_].reset_index(drop=True)\n    if fold_==4:\n        train_df=df_train.reset_index(drop=True)\n        test_df=df_test.reset_index(drop=True)\n    else:\n        test_df=df_train[df_train.kfold==fold_].reset_index(drop=True)\n    ##### initilize CountVectorizer with NLTK's word_tokenize\n    ##### function as tokenizer\n    count_vec=CountVectorizer(\n        tokenizer=word_tokenize,\n        token_pattern=None)\n    #### fit count_vec on training data reviews\n    count_vec.fit(train_df.text)\n    #transform training and validation data reviews\n    x_train=count_vec.transform(train_df.text)\n    x_test=count_vec.transform(test_df.text)\n    # initialize logistic regression model\n    model_logistic=linear_model.LogisticRegression(max_iter=1000)\n    # fit the model on training data reviews and sentiment\n    model_logistic.fit(x_train,train_df.target)\n    preds_log=model_logistic.predict(x_test)\n    ##Calculate accuracy\n    if fold_!=4:\n        accuracy=metrics.accuracy_score(test_df.target,preds_log)\n        print(f\"Fold:{fold_}\")\n        print(f\"Accuracy={accuracy}\")\n        print(\"\")\n    ","55cb6766":"set(preds_log)","e13e9d5d":"len(preds_log)","fb8eea86":"save_prediction(preds_log,model='logistic')","b0e28915":"from sklearn import naive_bayes","1935641a":"for fold_ in range(5):\n    train_df=df_train[df_train.kfold!=fold_].reset_index(drop=True)\n    if fold_==4:\n        train_df=df_train.reset_index(drop=True)\n        test_df=df_test.reset_index(drop=True)\n    else:\n        test_df=df_train[df_train.kfold==fold_].reset_index(drop=True)\n    ##### initilize CountVectorizer with NLTK's word_tokenize\n    ##### function as tokenizer\n    count_vec=CountVectorizer(\n        tokenizer=word_tokenize,\n        token_pattern=None)\n    #### fit count_vec on training data reviews\n    count_vec.fit(train_df.text)\n    #transform training and validation data reviews\n    x_train=count_vec.transform(train_df.text)\n    x_test=count_vec.transform(test_df.text)\n    # initialize naive bayes model\n    model_naive=naive_bayes.MultinomialNB()\n    # fit the model on training data reviews and sentiment\n    model_naive.fit(x_train,train_df.target)\n    preds_naive=model_naive.predict(x_test)\n    ##Calculate accuracy\n    if fold_!=4:\n        accuracy=metrics.accuracy_score(test_df.target,preds_naive)\n        print(f\"Fold:{fold_}\")\n        print(f\"Accuracy={accuracy}\")\n        print(\"\")\n    ","89cf492b":"save_prediction(preds_naive,model='naive')","55b1a5ba":"from sklearn.feature_extraction.text import TfidfVectorizer","ebad9b61":"for fold_ in range(5):\n    train_df=df_train[df_train.kfold!=fold_].reset_index(drop=True)\n    if fold_==4:\n        train_df=df_train.reset_index(drop=True)\n        test_df=df_test.reset_index(drop=True)\n    else:\n        test_df=df_train[df_train.kfold==fold_].reset_index(drop=True)\n\n    ##### initilize TfidfVectorizer with NLTK's word_tokenize\n    ##### function as tokenizer\n    tfidf_vec=TfidfVectorizer(\n        tokenizer=word_tokenize,\n        token_pattern=None)\n    #### fit count_vec on training data reviews\n    tfidf_vec.fit(train_df.text)\n    #transform training and validation data reviews\n    x_train=tfidf_vec.transform(train_df.text)\n    x_test=tfidf_vec.transform(test_df.text)\n    # initialize logistic regression model\n    model_logistic=linear_model.LogisticRegression(max_iter=1000)\n    # fit the model on training data reviews and sentiment\n    model_logistic.fit(x_train,train_df.target)\n    preds_logtfidf=model_logistic.predict(x_test)\n    ##Calculate accuracy\n    if fold_!=4:\n        accuracy=metrics.accuracy_score(test_df.target,preds_logtfidf)\n        print(f\"Fold:{fold_}\")\n        print(f\"Accuracy={accuracy}\")\n        print(\"\")\n    ","8b17317d":"### LSTM Ends","0bfe6976":"# Logistic regression +CounteVcetorizer","3df0e6c4":"# Naive bayes","b0c19818":"#### randomize the row of the data","19ad5b6b":"### Prediction","62ebcfd2":"### Tokenization","3845f514":"### Prediction","31b8319d":"#### Fetch labels","9113b706":"### Clean the text","afb855c7":"## Date preprocessing","9ce7db8c":"#### initiate the kfold class from model_selection module","f8cda895":"## LSTM Model training","8aa16846":"## Save prediction","3b5f1c70":"# Logistic +TF-IDF","e8ec58bb":"## Predictions","bbc5c64c":"##### We create a new column called kfold and fill it with -1"}}