{"cell_type":{"e134f50b":"code","4d611100":"code","3f48e443":"code","04ca0185":"code","a0b0fbbc":"code","26d3aa0b":"code","5edf5356":"code","02a5dadd":"code","4f44dd97":"code","2aef15ab":"code","4b844ccd":"code","aef36c9c":"code","452ae646":"code","d11b1b4b":"code","542cb9c5":"code","fb3f1da4":"code","c8d4347f":"code","b2744f03":"code","bb31f784":"code","c74222e4":"code","48e5c857":"code","52098a12":"code","48b1f32c":"code","1c5964a1":"code","7e381baf":"code","60c4b1c2":"code","8d787b51":"code","1f9a9c3e":"code","f758f29d":"code","e9c0602b":"code","5ba4e904":"code","c130ef29":"code","119ff00f":"code","eaf59a08":"code","3a8a1936":"code","fd5ba2ab":"markdown","fc02d809":"markdown","38cf3899":"markdown","7c88f1f5":"markdown","44fcb5a3":"markdown","681117b6":"markdown","2abd18cd":"markdown","00c91211":"markdown","2c53f1f2":"markdown"},"source":{"e134f50b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4d611100":"import matplotlib.pyplot as plt\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib notebook\nfrom sklearn.linear_model import LogisticRegression\nimport sklearn.model_selection\nimport sklearn.preprocessing as preproc\nfrom sklearn.feature_extraction import text\nimport pickle\nimport warnings\nwarnings.filterwarnings(\"ignore\")","3f48e443":"df = pd.read_csv('..\/input\/amazon-fine-food-reviews\/Reviews.csv') ","04ca0185":"df = df.dropna()\ndf.shape","a0b0fbbc":"df.head(2)","26d3aa0b":"df.drop_duplicates(subset={'UserId', 'ProfileName', 'Time', 'Text'}, inplace=True)","5edf5356":"df.shape","02a5dadd":"%matplotlib inline\nax = df['Score'].value_counts().plot(kind='bar', figsize=(6,6))\nfig = ax.get_figure()\nax.set_title(\"Amazon's Fine Food Reviews Dataset\")\nax.set_xlabel('Score')\nax.set_ylabel('Value Counts');","4f44dd97":"df['Label'] = 0\ndf.loc[df['Score'] > 3, ['Label']] = 1\n\nlen(df[df['Label'] == 1])\/len(df)","2aef15ab":"# A list of contractions from http:\/\/stackoverflow.com\/questions\/19790188\/expanding-english-language-contractions-in-python\ncontractions = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"i'd\": \"i would\",\n\"i'll\": \"i will\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'll\": \"it will\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"needn't\": \"need not\",\n\"oughtn't\": \"ought not\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"she'd\": \"she would\",\n\"she'll\": \"she will\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"that'd\": \"that would\",\n\"that's\": \"that is\",\n\"there'd\": \"there had\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'll\": \"they will\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'll\": \"we will\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"who'll\": \"who will\",\n\"who's\": \"who is\",\n\"won't\": \"will not\",\n\"wouldn't\": \"would not\",\n\"you'd\": \"you would\",\n\"you'll\": \"you will\",\n\"you're\": \"you are\"\n}","4b844ccd":"def clean_text(text, remove_stopwords = True):\n    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n    \n    # Convert words to lower case\n    text = text.lower()\n    \n    # Replace contractions with their longer forms \n    if True:\n        text = text.split()\n        new_text = []\n        for word in text:\n            if word in contractions:\n                new_text.append(contractions[word])\n            else:\n                new_text.append(word)\n        text = \" \".join(new_text)\n    \n    # Format words and remove unwanted characters\n    text = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n    text = re.sub(r'\\<a href', ' ', text)\n    text = re.sub(r'&amp;', '', text) \n    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]\/]', ' ', text)\n    text = re.sub(r'<br \/>', ' ', text)\n    text = re.sub(r'\\'', ' ', text)\n    \n    # remove stop words\n    if remove_stopwords:\n        text = text.split()\n        stops = set(stopwords.words(\"english\"))\n        text = [w for w in text if not w in stops]\n        text = \" \".join(text)\n\n    # Tokenize each word\n    text =  nltk.WordPunctTokenizer().tokenize(text)\n        \n    return text","aef36c9c":"df['Text_Cleaned'] = list(map(clean_text, df.Text))","452ae646":"def lemmatized_words(text):\n    lemm = nltk.stem.WordNetLemmatizer()\n    df['lemmatized_text'] = list(map(lambda word:\n                                     list(map(lemm.lemmatize, word)),\n                                     df.Text_Cleaned))\n    \n\nlemmatized_words(df.Text_Cleaned)","d11b1b4b":"pd.set_option('max_colwidth', 500)\ndf[['Score', 'Text', 'Label', 'Text_Cleaned', 'lemmatized_text']].sample(3)","542cb9c5":"bow_converter = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\nx = bow_converter.fit_transform(df['Text_Cleaned'])\n\nwords = bow_converter.get_feature_names()\nlen(words)","fb3f1da4":"bigram_converter = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[2,2], lowercase=False) \nx2 = bigram_converter.fit_transform(df['Text_Cleaned'])\nbigrams = bigram_converter.get_feature_names()\nlen(bigrams)","c8d4347f":"trigram_converter = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[3,3], lowercase=False) \nx3 = trigram_converter.fit_transform(df['Text_Cleaned'])\ntrigrams = trigram_converter.get_feature_names()\nlen(trigrams)","b2744f03":"print(len(words), len(bigrams), len(trigrams))","bb31f784":"sns.set_style(\"white\")\ncounts = [len(words), len(bigrams), len(trigrams)]\nplt.plot(counts, color='blue')\nplt.plot(counts, 'bo')\n#plt.margins(0.1)\nplt.ticklabel_format(style = 'plain')\nplt.xticks(range(3), ['unigram', 'bigram', 'trigram'])\nplt.tick_params(labelsize=14)\nplt.title('Number of ngrams in Amazon Fine Food Reviews Dataset', {'fontsize':16})\nplt.show()","c74222e4":"training_data, test_data = sklearn.model_selection.train_test_split(df, train_size = 0.7, random_state=42)","48e5c857":"bow_transform = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[3,3], lowercase=False) ","52098a12":"X_tr_bow = bow_transform.fit_transform(training_data['Text_Cleaned'])","48b1f32c":"len(bow_transform.vocabulary_)","1c5964a1":"X_te_bow = bow_transform.transform(test_data['Text_Cleaned'])","7e381baf":"y_tr = training_data['Label']\ny_te = test_data['Label']","60c4b1c2":"tfidf_transform = text.TfidfTransformer(norm=None)\nX_tr_tfidf = tfidf_transform.fit_transform(X_tr_bow)\nX_te_tfidf = tfidf_transform.transform(X_te_bow)","8d787b51":"def simple_logistic_classify(X_tr, y_tr, X_test, y_test, description, _C=1.0):\n    model = LogisticRegression(C=_C).fit(X_tr, y_tr)\n    score = model.score(X_test, y_test)\n    print('Test Score with', description, 'features', score)\n    return model","1f9a9c3e":"model_bow = simple_logistic_classify(X_tr_bow, y_tr, X_te_bow, y_te, 'bow')\nmodel_tfidf = simple_logistic_classify(X_tr_tfidf, y_tr, X_te_tfidf, y_te, 'tf-idf')","f758f29d":"param_grid_ = {'C': [1e-5, 1e-3, 1e-1, 1e0, 1e1, 1e2]}\nbow_search = sklearn.model_selection.GridSearchCV(LogisticRegression(), cv=5, param_grid=param_grid_)\ntfidf_search = sklearn.model_selection.GridSearchCV(LogisticRegression(), cv=5,\n                                   param_grid=param_grid_)","e9c0602b":"bow_search.fit(X_tr_bow, y_tr)","5ba4e904":"bow_search.best_score_","c130ef29":"tfidf_search.fit(X_tr_tfidf, y_tr)","119ff00f":"tfidf_search.best_score_","eaf59a08":"search_results = pd.DataFrame.from_dict({'bow': bow_search.cv_results_['mean_test_score'],\n                               'tfidf': tfidf_search.cv_results_['mean_test_score']})\nsearch_results","3a8a1936":"ax = sns.boxplot(data=search_results, width=0.4)\nax.set_ylabel('Accuracy', size=14)\nax.tick_params(labelsize=14)\nplt.savefig('tfidf_gridcv_results.png')","fd5ba2ab":"# Classification with Logistic Regression","fc02d809":"- Deduplication Removing the dublicate rows, which share the same UserId, ProfileName, Time, and Text","38cf3899":"# Data Cleansing","7c88f1f5":"# Text Preprocessing","44fcb5a3":"# Inspecting Dataset","681117b6":"- Dataset has 5-star rating system and ratings are highly unbalanced. Due to high number of 5-star rating, I have decided to label score 4 and 5 as positive and else negative.","2abd18cd":"# Importing Libraries","00c91211":"# Bag of Words Transformation","2c53f1f2":"# Tf-Idf Tranformation"}}