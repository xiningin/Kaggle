{"cell_type":{"54501d12":"code","923dd6d0":"code","da763ce7":"code","9c131ac7":"code","8a07d827":"code","7ea42f63":"code","f5ae355a":"code","2741f875":"code","86231de8":"code","997e7f76":"code","6d17316c":"code","019d6354":"code","98f6d364":"code","6a39543d":"code","2ecf868c":"code","d220abce":"code","e1208257":"code","267157ea":"code","83d4e167":"code","66c7b347":"code","2e86c815":"code","56647033":"code","bb1a2b5e":"code","8a4b42f5":"code","a25a551e":"code","692ba687":"code","c27a4b43":"code","4fb73e2d":"code","e8bff2c8":"code","0158b49b":"code","4f55b0a6":"code","fcc9be52":"markdown","a71bcb4d":"markdown","6accb87e":"markdown","4f7a4ff2":"markdown","e20fee82":"markdown","9430e00b":"markdown","8b4395aa":"markdown","8e00a578":"markdown","96ee372f":"markdown","c7cb1557":"markdown","bf2e1d96":"markdown","c7ca8278":"markdown","28a9bdea":"markdown","61210e0e":"markdown","c8ffacf3":"markdown","09c2b787":"markdown","cebb63e6":"markdown","0f73d325":"markdown","8df696e0":"markdown","86fde487":"markdown","4d17d81d":"markdown"},"source":{"54501d12":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","923dd6d0":"data = pd.read_csv(\"\/kaggle\/input\/loan-default-prediction\/train_v2.csv.zip\")\nt = pd.read_csv(\"\/kaggle\/input\/loan-default-prediction\/test_v2.csv.zip\")\ndata.shape","da763ce7":"data.head()","9c131ac7":"data.info()","8a07d827":"data.select_dtypes(include=['object']).head()","7ea42f63":"invalid = data.select_dtypes(include=['object']).columns\ndata.drop(invalid, axis=1, inplace=True)\nt.drop(invalid, axis=1, inplace=True)\nt_id = t['id'].copy\nt.drop('id', axis=1, inplace = True)","f5ae355a":"data.describe()","2741f875":"t.describe()","86231de8":"missing = data.isnull().sum()\nmissing = pd.DataFrame(missing[missing!=0])\nmissing.columns = ['No. of missing values']\nmissing['Percentage'] = 100*missing['No. of missing values']\/data.id.count()\nmissing.sort_values(by=\"Percentage\", ascending=False)","997e7f76":"correlations = data.iloc[:,1:752].corr()\ncorrelations.head()","6d17316c":"x = data.iloc[:,1:751].copy()\ny = data.iloc[:,751].copy()\ny.value_counts()","019d6354":"y[y>0] = 1\ny.value_counts()","98f6d364":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, stratify = y, random_state=0)","6a39543d":"[X_train.shape, X_test.shape, y_train.shape, y_test.shape]","2ecf868c":"X_train = X_train.fillna(X_train.mean())\nX_test = X_test.fillna(X_train.mean())\nt = t.fillna(X_train.mean())\n[X_train.isnull().sum().sum(), X_test.isnull().sum().sum(), t.isnull().sum().sum()]","d220abce":"from sklearn.preprocessing import StandardScaler\nscalar= StandardScaler()\nscalar.fit(X_train)\nX_train = scalar.transform(X_train)\nX_test = scalar.transform(X_test)\nX_t = scalar.transform(t)","e1208257":"from sklearn.decomposition import PCA\npca = PCA()\npca.fit(X_train)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');","267157ea":"np.cumsum(pca.explained_variance_ratio_)[200]","83d4e167":"final_pca = PCA(n_components=200)\nfinal_pca.fit(X_train)\nX_train = final_pca.transform(X_train)\nX_train = pd.DataFrame(data = X_train)\nX_test = final_pca.transform(X_test)\nX_test = pd.DataFrame(data = X_test)\nX_t = final_pca.transform(X_t)\nX_t = pd.DataFrame(data = X_t)","66c7b347":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(solver= 'saga', class_weight='balanced',max_iter=500, random_state=1).fit(X_train, y_train)\nmodel.coef_[0]","2e86c815":"y_pred = model.predict(X_test)\ny_pred","56647033":"import sklearn.metrics as sm\nc = pd.DataFrame(sm.confusion_matrix(y_test, y_pred), index=['Actual non defaulter','Actual defaulter'])\nc.columns = ['Predicted non defaulter','Predicted defaulter']\nc['Actual Total'] = c.sum(axis=1)\nc.loc['Predicted Total',:] = c.sum(axis = 0)\nc","bb1a2b5e":"print([\"The accuracy on the validation data is \" + str(round(sm.accuracy_score(y_test, y_pred)*100,ndigits = 2)) + \"%\"])","8a4b42f5":"print(\"The sensitivity (true positive rate) is \" + str(round(100*c.iloc[1,1]\/c.iloc[1,2], ndigits=2)) + \"%\")","a25a551e":"ns_fpr, ns_tpr, _ = sm.roc_curve(y_test, np.zeros(len(y_test)))\nlr_probs = model.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nlr_probs = lr_probs[:, 1]\nlr_fpr, lr_tpr, _ = sm.roc_curve(y_test, lr_probs)\n# plot the roc curve for the model\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\nplt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic Regression')\n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","692ba687":"print(\"The Area under ROC curve is \" + str(round(100 * sm.roc_auc_score(y_test, y_pred), ndigits=2)) + \"%\")","c27a4b43":"print(sm.classification_report(y_test, y_pred))","4fb73e2d":"pred = model.predict(X_t)\nsns.countplot(pred);","e8bff2c8":"submission = pd.read_csv(\"..\/input\/loan-default-prediction\/sampleSubmission.csv\")\nsubmission['loss'] = pred","0158b49b":"submission.head()","4f55b0a6":"submission.to_csv(\"submit.csv\", index=False)","fcc9be52":"Now we can use these variables to fit the model with 200 independent variables to predict loss.","a71bcb4d":"These columns seem to be incorrect, so we drop them.","6accb87e":"## Correlations","4f7a4ff2":"## Classification Report","e20fee82":"# Logistic Regression","9430e00b":"# Standardization of Variables\nPCA is effected by scale so we need to scale the features in the data before applying PCA. We can transform the data onto unit scale (mean = 0 and variance = 1) which is a requirement for the optimal performance of many machine learning algorithms. StandardScaler helps standardize the dataset\u2019s features. ","8b4395aa":"# Missing value treatment\nSince the percentage of missing values is small, we impute them by the mean of the column.","8e00a578":"# Train test split\nBefore we go for data transformation and model building, it is necessary to divide the data into train and test.","96ee372f":"# Exploratory Data Analysis\n\n## Describe the numeric columns","c7cb1557":"# Reading the input files","bf2e1d96":"# Principal Component Analysis\n* Given a collection of points in two, three, or higher dimensional space, a \"best fitting\" line can be defined as one that minimizes the average squared distance from a point to the line. The next best-fitting line can be similarly chosen from directions perpendicular to the first. Repeating this process yields an orthogonal basis in which different individual dimensions of the data are uncorrelated. These basis vectors are called principal components, and several related procedures principal component analysis (PCA).\n* PCA is a method used to reduce number of variables in the data by extracting the important ones from a large pool. It reduces the dimension of the data with an aim to retain as much information as possible. In other words, this method combines highly correlated variables together to form a smaller number of an artificial set of variables which is called \u201cprincipal components\u201d that account for most variance in the data.","c7ca8278":"# Validation on test data","28a9bdea":"We first convert y to binary","61210e0e":"## Accuracy","c8ffacf3":"98.27% of variation is explained by 100 components.","09c2b787":"## AUC","cebb63e6":"As we can see in the above output, there are many features that have very high correlations among themselves. This is the motivation behind performing Principal Component Analysis (PCA) in the further step to reduce the dimensions.","0f73d325":"## Missing values","8df696e0":"# Model Evaluation\n## Confusion Matrix","86fde487":"## Sensitivity","4d17d81d":"# Prediction on given test data"}}