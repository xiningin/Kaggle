{"cell_type":{"5cf5c71b":"code","a42a8c28":"code","d0d2ca81":"code","f675bee9":"code","5dbcd106":"code","33d3d063":"code","8e860b1d":"code","f14e2890":"code","a7e72c16":"code","c58bb119":"code","4c9f642f":"code","6c96c0c3":"code","a6706054":"code","1ec640ad":"code","8c4f06f2":"code","6817a659":"code","2ce36e1b":"code","dc6b4eae":"code","89b370af":"code","057f737d":"code","98511779":"code","ff2eba81":"code","1ff88c71":"code","24abcdd3":"code","3cdf804a":"code","2fe6bf84":"code","a21eece7":"code","147ddb74":"code","fc6d96bb":"code","6c574446":"code","6504cbb3":"code","97a7c41e":"code","46dabd30":"code","889c97f8":"code","ec0c1517":"code","f70cd03b":"code","cdc9cddd":"code","e06c2a0a":"code","5aeeb36b":"code","ac2f3b22":"code","f98c388a":"code","18fadcc8":"code","22dc462b":"code","45d5af33":"code","ea047c4b":"code","14a337f6":"code","0efbc584":"code","3c6fbef7":"code","e0f5676e":"code","f9829f76":"code","cfe9826e":"code","27af2a69":"code","abcc5490":"code","7b64741b":"code","27bdb3a3":"code","2b41ed27":"code","0422189c":"code","e35e0885":"code","d98314d4":"code","cbf273c2":"code","dda0d053":"code","b7f280ac":"code","0f049cf0":"code","62a1bfb6":"code","b35e2483":"code","3f3ad5ac":"code","f46e3d27":"code","c62a5915":"code","b42e7ce5":"code","fede01d7":"code","9dacdaee":"code","89c763ef":"code","afffa4b0":"code","28f4b271":"code","37a9892f":"code","2cd193d3":"code","2f962b2c":"code","1f221a9c":"code","a91e2e00":"code","26d53b27":"code","55b6b050":"code","63810eb1":"code","d3b6e2cb":"code","8b9b3eb7":"code","0e689641":"code","9c1f1246":"code","5e9810e2":"code","21d28539":"code","53700648":"code","170b9311":"code","131088fc":"code","594aec6d":"code","97285471":"code","55eef4f2":"markdown","35f784a0":"markdown","d674ba80":"markdown","f4d6c78a":"markdown","340dbbae":"markdown","bd8ea230":"markdown","4c4ff1f0":"markdown","b30ccd82":"markdown","06a5b44d":"markdown","523b670c":"markdown","0186e013":"markdown","9dd2b48d":"markdown","a7d6aeeb":"markdown","8fda07a0":"markdown","4fd33d15":"markdown","a4cafc4b":"markdown","2874225c":"markdown","175e0af1":"markdown","1c978239":"markdown","2138ca48":"markdown","b9890cf6":"markdown","17a8c2de":"markdown","c9f1bdbe":"markdown","67d967aa":"markdown","c636f358":"markdown","58d5b409":"markdown","ab717e5d":"markdown","dc9b988b":"markdown","b468e66c":"markdown","f8b0d019":"markdown","40e0cf0f":"markdown","e1f81869":"markdown","df3e8802":"markdown","82857e20":"markdown","83200deb":"markdown","10d21c7d":"markdown","9bc27754":"markdown","e1bc9b9b":"markdown","72cc531e":"markdown","63a21237":"markdown","8f0d8a3e":"markdown","8a6f48d1":"markdown","2c521278":"markdown","3f505ee6":"markdown","89637ddd":"markdown","bf3033a7":"markdown","87da6a99":"markdown","8c016f96":"markdown","8f153fc3":"markdown","f0f4fdec":"markdown","0c13475a":"markdown","4078914e":"markdown","98fd59d9":"markdown","a775bd5a":"markdown","66b8e05f":"markdown","d3e4e288":"markdown","6be23b2b":"markdown","2e5d56b7":"markdown","789a09e2":"markdown"},"source":{"5cf5c71b":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn')\nimport seaborn as sns\n\nimport re\nimport xgboost as xgb\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","a42a8c28":"df_initial = pd.read_csv('..\/input\/listings_summary.csv')\n\n# checking shape\nprint(\"The dataset has {} rows and {} columns.\".format(*df_initial.shape))\n\n# ... and duplicates\nprint(\"It contains {} duplicates.\".format(df_initial.duplicated().sum()))","d0d2ca81":"df_initial.head(1)","f675bee9":"# check the columns we currently have\ndf_initial.columns","5dbcd106":"# define the columns we want to keep\ncolumns_to_keep = ['id', 'space', 'description', 'host_has_profile_pic', 'neighbourhood_group_cleansed', \n                   'latitude', 'longitude', 'property_type', 'room_type', 'accommodates', 'bathrooms',  \n                   'bedrooms', 'bed_type', 'amenities', 'square_feet', 'price', 'cleaning_fee', \n                   'security_deposit', 'extra_people', 'guests_included', 'minimum_nights',  \n                   'instant_bookable', 'is_business_travel_ready', 'cancellation_policy']\n\ndf_raw = df_initial[columns_to_keep].set_index('id')\nprint(\"The dataset has {} rows and {} columns - after dropping irrelevant columns.\".format(*df_raw.shape))","33d3d063":"df_raw.room_type.value_counts(normalize=True)","8e860b1d":"df_raw.property_type.value_counts(normalize=True)","f14e2890":"df_raw[['price', 'cleaning_fee', 'extra_people', 'security_deposit']].head(3)","a7e72c16":"# checking Nan's in \"price\" column\ndf_raw.price.isna().sum()","c58bb119":"# Nan's in \"cleaning_fee\" column\ndf_raw.cleaning_fee.isna().sum()","4c9f642f":"df_raw.cleaning_fee.fillna('$0.00', inplace=True)\ndf_raw.cleaning_fee.isna().sum()","6c96c0c3":"df_raw.security_deposit.isna().sum()","a6706054":"df_raw.security_deposit.fillna('$0.00', inplace=True)\ndf_raw.security_deposit.isna().sum()","1ec640ad":"df_raw.extra_people.isna().sum()","8c4f06f2":"# clean up the columns (by method chaining)\ndf_raw.price = df_raw.price.str.replace('$', '').str.replace(',', '').astype(float)\ndf_raw.cleaning_fee = df_raw.cleaning_fee.str.replace('$', '').str.replace(',', '').astype(float)\ndf_raw.security_deposit = df_raw.security_deposit.str.replace('$', '').str.replace(',', '').astype(float)\ndf_raw.extra_people = df_raw.extra_people.str.replace('$', '').str.replace(',', '').astype(float)","6817a659":"df_raw['price'].describe()","2ce36e1b":"red_square = dict(markerfacecolor='r', markeredgecolor='r', marker='.')\ndf_raw['price'].plot(kind='box', xlim=(0, 1000), vert=False, flierprops=red_square, figsize=(16,2));","dc6b4eae":"df_raw.drop(df_raw[ (df_raw.price > 400) | (df_raw.price == 0) ].index, axis=0, inplace=True)","89b370af":"df_raw['price'].describe()","057f737d":"print(\"The dataset has {} rows and {} columns - after being price-wise preprocessed.\".format(*df_raw.shape))","98511779":"df_raw.isna().sum()","ff2eba81":"# drop columns with too many Nan's\ndf_raw.drop(columns=['square_feet', 'space'], inplace=True)","1ff88c71":"# drop rows with NaN's in bathrooms and bedrooms\ndf_raw.dropna(subset=['bathrooms', 'bedrooms', ], inplace=True)","24abcdd3":"df_raw.host_has_profile_pic.unique()","3cdf804a":"# replace host_has_profile_pic Nan's with no\ndf_raw.host_has_profile_pic.fillna(value='f', inplace=True)\ndf_raw.host_has_profile_pic.unique()","2fe6bf84":"df_raw.isna().sum()","a21eece7":"print(\"The dataset has {} rows and {} columns - after having dealt with missing values.\".format(*df_raw.shape))","147ddb74":"from geopy.distance import great_circle","fc6d96bb":"def distance_to_mid(lat, lon):\n    berlin_centre = (52.5027778, 13.404166666666667)\n    accommodation = (lat, lon)\n    return great_circle(berlin_centre, accommodation).km","6c574446":"df_raw['distance'] = df_raw.apply(lambda x: distance_to_mid(x.latitude, x.longitude), axis=1)","6504cbb3":"df_raw.head(2)","97a7c41e":"#list(df_raw.description[:10])","46dabd30":"df_raw.description.isna().sum()","889c97f8":"# extract numbers \ndf_raw['size'] = df_raw['description'].str.extract('(\\d{2,3}\\s?[smSM])', expand=True)\ndf_raw['size'] = df_raw['size'].str.replace(\"\\D\", \"\")\n\n# change datatype of size into float\ndf_raw['size'] = df_raw['size'].astype(float)\n\nprint('NaNs in size_column absolute:     ', df_raw['size'].isna().sum())\nprint('NaNs in size_column in percentage:', round(df_raw['size'].isna().sum()\/len(df_raw),3), '%')","ec0c1517":"df_raw[['description', 'size']].head(10)","f70cd03b":"#list(df_raw.description[:10])","cdc9cddd":"# drop description column\ndf_raw.drop(['description'], axis=1, inplace=True)","e06c2a0a":"df_raw.info()","5aeeb36b":"# filter out sub_df to work with\nsub_df = df_raw[['accommodates', 'bathrooms', 'bedrooms',  'price', 'cleaning_fee', \n                 'security_deposit', 'extra_people', 'guests_included', 'distance', 'size']]","ac2f3b22":"# split datasets\ntrain_data = sub_df[sub_df['size'].notnull()]\ntest_data  = sub_df[sub_df['size'].isnull()]\n\n# define X\nX_train = train_data.drop('size', axis=1)\nX_test  = test_data.drop('size', axis=1)\n\n# define y\ny_train = train_data['size']","f98c388a":"print(\"Shape of Training Data:\", train_data.shape)\nprint(\"Shape of Test Data:    \",test_data.shape)\nprint(\"\\nShape of X_train:\", X_train.shape)\nprint(\"Shape of X_test:\", X_test.shape)\nprint(\"\\nShape of y_train:\", y_train.shape)","18fadcc8":"# import Linear Regression\nfrom sklearn.linear_model import LinearRegression\n\n# instantiate\nlinreg = LinearRegression()\n\n# fit model to training data\nlinreg.fit(X_train, y_train)","22dc462b":"# making predictions\ny_test = linreg.predict(X_test)","45d5af33":"y_test = pd.DataFrame(y_test)\ny_test.columns = ['size']\nprint(y_test.shape)\ny_test.head()","ea047c4b":"print(X_test.shape)\nX_test.head()","14a337f6":"# make the index of X_test to an own dataframe\nprelim_index = pd.DataFrame(X_test.index)\nprelim_index.columns = ['prelim']\n\n# ... and concat this dataframe with y_test\ny_test = pd.concat([y_test, prelim_index], axis=1)\ny_test.set_index(['prelim'], inplace=True)\ny_test.head()","0efbc584":"new_test_data = pd.concat([X_test, y_test], axis=1)","3c6fbef7":"print(new_test_data.shape)\nnew_test_data.head()","e0f5676e":"new_test_data['size'].isna().sum()","f9829f76":"# combine train and test data back to a new sub df\nsub_df_new = pd.concat([new_test_data, train_data], axis=0)\n\nprint(sub_df_new.shape)\nsub_df_new.head()","cfe9826e":"sub_df_new['size'].isna().sum()","27af2a69":"# prepare the multiple columns before concatening\ndf_raw.drop(['accommodates', 'bathrooms', 'bedrooms', 'price', 'cleaning_fee', \n             'security_deposit', 'extra_people', 'guests_included', 'distance', 'size'], \n            axis=1, inplace=True)","abcc5490":"# concate back to complete dataframe\ndf = pd.concat([sub_df_new, df_raw], axis=1)\n\nprint(df.shape)\ndf.head(2)","7b64741b":"df['size'].isna().sum()","27bdb3a3":"df['size'].describe()","2b41ed27":"red_square = dict(markerfacecolor='r', markeredgecolor='r', marker='.')\ndf['size'].plot(kind='box', xlim=(0, 1000), vert=False, flierprops=red_square, figsize=(16,2));","0422189c":"df.drop(df[ (df['size'] == 0.) | (df['size'] > 300.) ].index, axis=0, inplace=True)","e35e0885":"print(\"The dataset has {} rows and {} columns - after being engineered.\".format(*df.shape))","d98314d4":"from collections import Counter","cbf273c2":"results = Counter()\ndf['amenities'].str.strip('{}')\\\n               .str.replace('\"', '')\\\n               .str.lstrip('\\\"')\\\n               .str.rstrip('\\\"')\\\n               .str.split(',')\\\n               .apply(results.update)\n\nresults.most_common(30)","dda0d053":"# create a new dataframe\nsub_df = pd.DataFrame(results.most_common(30), columns=['amenity', 'count'])","b7f280ac":"# plot the Top 20\nsub_df.sort_values(by=['count'], ascending=True).plot(kind='barh', x='amenity', y='count',  \n                                                      figsize=(10,7), legend=False, color='darkgrey',\n                                                      title='Amenities')\nplt.xlabel('Count');","0f049cf0":"df['Laptop_friendly_workspace'] = df['amenities'].str.contains('Laptop friendly workspace')\ndf['TV'] = df['amenities'].str.contains('TV')\ndf['Family_kid_friendly'] = df['amenities'].str.contains('Family\/kid friendly')\ndf['Host_greets_you'] = df['amenities'].str.contains('Host greets you')\ndf['Smoking_allowed'] = df['amenities'].str.contains('Smoking allowed')","62a1bfb6":"df.drop(['amenities'], axis=1, inplace=True)","b35e2483":"df.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4, figsize=(10,7), \n        c=\"price\", cmap=\"gist_heat_r\", colorbar=True, sharex=False);","3f3ad5ac":"df['neighbourhood_group_cleansed'].value_counts().sort_values().plot(kind='barh', color='darkgrey')\nplt.title('Number of Accommodations per District');","f46e3d27":"# group_by neighbourhood groups, take the median price and store new values in sub_df \ndf_grouped = pd.DataFrame(df.groupby(['neighbourhood_group_cleansed'])['price'].agg(np.median))\ndf_grouped.reset_index(inplace=True)\n\n# plot this \ndf_grouped.sort_values(by=['price'], ascending=True)\\\n          .plot(kind='barh', x='neighbourhood_group_cleansed', y='price', \n                figsize=(10,6), legend=False, color='salmon')\n\nplt.xlabel('\\nMedian Price', fontsize=12)\nplt.ylabel('District\\n', fontsize=12)\nplt.title('\\nMedian Prices by Neighbourhood\\n', fontsize=14, fontweight='bold');","c62a5915":"red_square = dict(markerfacecolor='salmon', markeredgecolor='salmon', marker='.')\n\ndf.boxplot(column='price', by='neighbourhood_group_cleansed', \n           flierprops=red_square, vert=False, figsize=(10,8))\n\nplt.xlabel('\\nMedian Price', fontsize=12)\nplt.ylabel('District\\n', fontsize=12)\nplt.title('\\nBoxplot: Prices by Neighbourhood\\n', fontsize=14, fontweight='bold')\n\n# get rid of automatic boxplot title\nplt.suptitle('');","b42e7ce5":"df.plot.scatter(x=\"distance\", y=\"price\", figsize=(9,6), c='dimgrey')\nplt.title('\\nRelation between Distance & Median Price\\n', fontsize=14, fontweight='bold');","fede01d7":"sns.jointplot(x=df[\"distance\"], y=df[\"price\"], kind='hex')\nplt.title('\\nRelation between Distance & Median Price\\n', fontsize=14, fontweight='bold');","9dacdaee":"sns.set_style(\"white\")\ncmap = sns.cubehelix_palette(rot=-.2, as_cmap=True)\n\nfig, ax = plt.subplots(figsize=(12,7))\nax = sns.scatterplot(x=\"size\", y=\"price\", size='cleaning_fee', sizes=(5, 200),\n                      hue='size', palette=cmap,  data=df)\n\nplt.title('\\nRelation between Size & Median Price\\n', fontsize=14, fontweight='bold')\n\n# putting legend out of the plot\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.);","89c763ef":"plt.figure(figsize=(6,6))\nsns.heatmap(df.groupby(['neighbourhood_group_cleansed', 'bedrooms']).price.median().unstack(), \n            cmap='Reds', annot=True, fmt=\".0f\")\n\nplt.xlabel('\\nBedrooms', fontsize=12)\nplt.ylabel('District\\n', fontsize=12)\nplt.title('\\nHeatmap: Median Prices by Neighbourhood and Number of Bedrooms\\n\\n', fontsize=14, fontweight='bold');","afffa4b0":"# group_by neighbourhood groups, take the median price and store new values in sub_df \ndf_grouped = pd.DataFrame(df.groupby(['cancellation_policy'])['price'].agg(np.median))\ndf_grouped.reset_index(inplace=True)\n\n# plot this \ndf_grouped.sort_values(by=['price'], ascending=True)\\\n          .plot(kind='barh', x='cancellation_policy', y='price', \n                figsize=(9,5), legend=False, color='darkblue')\n\nplt.xlabel('\\nMedian Price', fontsize=12)\nplt.ylabel('Cancellation Policy\\n', fontsize=12)\nplt.title('\\nMedian Prices by Cancellation Policy\\n', fontsize=14, fontweight='bold');","28f4b271":"df.columns","37a9892f":"df.info()","2cd193d3":"df.drop(['latitude', 'longitude', 'neighbourhood_group_cleansed', 'property_type'], axis=1, inplace=True)","2f962b2c":"for col in ['host_has_profile_pic', 'room_type', 'bed_type', 'instant_bookable', \n            'is_business_travel_ready', 'cancellation_policy']:\n    df[col] = df[col].astype('category')","1f221a9c":"# define our target\ntarget = df[[\"price\"]]\n\n# define our features \nfeatures = df.drop([\"price\"], axis=1)","a91e2e00":"num_feats = features.select_dtypes(include=['float64', 'int64', 'bool']).copy()\n\n# one-hot encoding of categorical features\ncat_feats = features.select_dtypes(include=['category']).copy()\ncat_feats = pd.get_dummies(cat_feats)","26d53b27":"features_recoded = pd.concat([num_feats, cat_feats], axis=1)","55b6b050":"print(features_recoded.shape)\nfeatures_recoded.head(2)","63810eb1":"# import train_test_split function\nfrom sklearn.model_selection import train_test_split\n# import metrics\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# split our data\nX_train, X_test, y_train, y_test = train_test_split(features_recoded, target, test_size=0.2)","d3b6e2cb":"# scale data\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test  = sc.transform(X_test)","8b9b3eb7":"# create a baseline\nbooster = xgb.XGBRegressor()","0e689641":"from sklearn.model_selection import GridSearchCV\n\n# create Grid\nparam_grid = {'n_estimators': [100, 150, 200],\n              'learning_rate': [0.01, 0.05, 0.1], \n              'max_depth': [3, 4, 5, 6, 7],\n              'colsample_bytree': [0.6, 0.7, 1],\n              'gamma': [0.0, 0.1, 0.2]}\n\n# instantiate the tuned random forest\nbooster_grid_search = GridSearchCV(booster, param_grid, cv=3, n_jobs=-1)\n\n# train the tuned random forest\nbooster_grid_search.fit(X_train, y_train)\n\n# print best estimator parameters found during the grid search\nprint(booster_grid_search.best_params_)","9c1f1246":"# instantiate xgboost with best parameters\nbooster = xgb.XGBRegressor(colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, \n                           max_depth=6, n_estimators=200, random_state=4)\n\n# train\nbooster.fit(X_train, y_train)\n\n# predict\ny_pred_train = booster.predict(X_train)\ny_pred_test = booster.predict(X_test)","5e9810e2":"RMSE = np.sqrt(mean_squared_error(y_test, y_pred_test))\nprint(f\"RMSE: {round(RMSE, 4)}\")","21d28539":"r2 = r2_score(y_test, y_pred_test)\nr2\nprint(f\"r2: {round(r2, 4)}\")","53700648":"xg_train = xgb.DMatrix(data=X_train, label=y_train)","170b9311":"params = {'colsample_bytree':0.6, 'gamma':0.2, 'learning_rate':0.05, 'max_depth':6}\n\ncv_results = xgb.cv(dtrain=xg_train, params=params, nfold=3,\n                    num_boost_round=200, early_stopping_rounds=10, \n                    metrics=\"rmse\", as_pandas=True)","131088fc":"cv_results.head()","594aec6d":"cv_results.tail()","97285471":"# plot the important features\nfeat_importances = pd.Series(booster.feature_importances_, index=features_recoded.columns)\nfeat_importances.nlargest(15).sort_values().plot(kind='barh', color='darkgrey', figsize=(10,5))\nplt.xlabel('Relative Feature Importance with XGBoost');","55eef4f2":"As we see, the **most important features are size, distance, and cleaning fee**, which account for approximately 45% of the daily price. Other top features are the number of people the apartment accommodates, other fees such as security deposit or the price for extra people, minimum night stay and bathrooms.","35f784a0":"We shouldn't miss investigating the `price` - it might need some cleaning to be of use to us:","d674ba80":"*Back to: <a href='#Table of contents'> Table of contents<\/a>*\n### 4. Modeling the Data \n<a id='4. Modeling the Data'><\/a>","f4d6c78a":"**> Price Differences by Accommodation Size**","340dbbae":"No, so we don't need to drop any rows. And what about the `cleaning_fee`?","bd8ea230":"Let's imagine we are in the shoes of someone who'd like to offer their home. Fixed features of our property include its rooms, size, and location. We also can decide on how we want to be listed: with a picture or not, how many minimum nights we want a guest to stay, whether we are instantly bookable, how we handle cancellations, etc. But we can neither be a \"super host\", nor do we have any reviews yet to show - although they can be very important for setting a price. So, let's focus only on features we can influence:","4c4ff1f0":"*Back to: <a href='#Table of contents'> Table of contents<\/a>*\n### 3. Exploratory Data Analysis (EDA)\n<a id='3. Exploratory Data Analysis (EDA)' ><\/a>","b30ccd82":"As we work with the distance to the center, let's drop the `neighbourhood_group_cleansed`. Furthermore, `property_types` may not be that helpful.","06a5b44d":"And how many different **property types** are we up against?","523b670c":"**> Price Differences on a Map**","0186e013":"*Back to: <a href='#Table of contents'> Table of contents<\/a>*\n#### 2.2. Cleaning Price Columns\n<a id='2.2. Cleaning Price Columns'><\/a>","9dd2b48d":"*Back to: <a href='#Table of contents'> Table of contents<\/a>*\n#### 2.6. Feature Engineering 3: Lodging Amenities\n<a id='2.6. Feature Engineering 3: Lodging Amenities'><\/a>","a7d6aeeb":"Comparing the results, we did a pretty good job with the first 4 records, but filtered an incorrect number for the last record. Okay, let's keep that in mind: there may be mistakes in the size we engineered from the text! ","8fda07a0":"*Back to: <a href='#Table of contents'> Table of contents<\/a>*\n#### 2.5. Feature Engineering 2: Lodging Size\n<a id='2.5. Feature Engineering 2: Lodging Size'><\/a>","4fd33d15":"#### 4.4. Cross Validation\n<a id='4.4. Cross Validation'><\/a>","a4cafc4b":"Let's add columns with amenities that are somewhat unique and not offered by all hosts: \n- a laptop-friendly workspace\n- a TV\n- kid friendly accommodation \n- smoker friendly and \n- being greeted by the host.\n\nAfter doing this, let's drop the original column:","2874225c":"Once we have completed our new train_data, we stack test_data and train_data back to a new sub_dataframe across the rows:","175e0af1":"Now let's convert all string columns into categorical ones:","1c978239":"Now that we have predicted the missing sizes, let's cast `y_test` into a dataframe with a column `size`:","2138ca48":"#### 4.2. Splitting and Scaling the Data\n<a id='4.2. Splitting and Scaling the Data'><\/a>","b9890cf6":"Let's first check if there are any null values in the `price` column:","17a8c2de":"*Back to: <a href='#Table of contents'> Table of contents<\/a>*\n### 2. Preprocessing the Data \n<a id='2. Preprocessing the Data'><\/a>","c9f1bdbe":"**> Price Differences by Number of Bedrooms**","67d967aa":"Let's remove the dollar signs in all four columns and convert the string values into numerical ones:","c636f358":"## Predicting the price for an Airbnb Host in Berlin\n\nAirbnb has successfully disrupted the traditional hospitality industry as more and more travelers decide to use Airbnb as their primary accommodation provider. Since its inception in 2008, Airbnb has seen an enormous growth, with the number of rentals listed on its website growing exponentially each year.\n\nIn Germany, no city is more popular than Berlin. That implies that Berlin is one of the hottest markets for Airbnb in Europe, with over 22,552 listings as of November 2018. With a size of 891 km\u00b2, this means there are roughly 25 homes being rented out per km\u00b2 in Berlin on Airbnb!\n\nConsidering the possibility that I might have to relocate for a new data science job, but want to keep my current flat in Berlin (which is quite cheap!), I might wonder if it could be worth it to offer my jewel on Airbnb. Could this perhaps be a profitable option? However, it is difficult for potential hosts to know what the true value of their home is, and how in-demand their home might be. And since location and furniture are obviously fixed for the most part, is there anything else a host can influence - such as description, communication patterns, and\/or additional services to boost their earnings?\n\nThe following question will drive this project:\n\n> **Can we determine a fairly spot-on daily price for a new accommodation that fits into its specific market environment and competitors in Berlin?** <br>\n\nThe question focuses on the accommodation features and decisions a new host can make with regards to initial presentation, i.e. posting a picture of him- or herself on the website, determining a minimum length of stay, offering instant bookings etc. A machine learning algorithm will be applied to try to get an answer. \n\n### The dataset\n\nIn the first notebook, I will perform an analysis of the detailed Berlin listings data, sourced from the Inside Airbnb website, in order to understand the rental landscape and try to recommend a price for a newbie entering the market. The dataset is named `listings.csv.gz` and was scraped on November 07th 2018.","58d5b409":"The `description` column seems to be rich in content. Let's extract \n- all double-digit or three-digit numbers \n- that are followed by one of the two characters \"s\" or \"m\" (covering \"sqm\", \"square meters\", \"m2\" etc.) and \n- may or may not be connected by white space. \n\nSingle- or more than three-digit numbers for accommodation sizes are quite unlikely.\n\nI know, it's a bold move - but let's give it a try...","ab717e5d":"A bit messy, as expected! \n\n75% of the apartments charge up to 68\u20ac - but the maximum value is 9000\u20ac. Let's decide on a limit of 400\u20ac, after which the outliers seem to \"fringe\", and drop all records that charge more than that. Oddly enough, we have prices set to zero. So let's drop those records, too:","dc9b988b":"By the way, how many different **room types** do we have?","b468e66c":"#### 4.1. Preparing Target and Features\n<a id='4.1. Preparing Target and Features'><\/a>","f8b0d019":"Location is always an important factor in lodging services. To make it more descriptive, I decided to calculate each accommodation's distance to the so-called centroid of Berlin instead of just relying on the neighbourhoods or areas. \n\nFor our convenience, let's write a quick function that does this, apply it to each accommodation, and store the values in a new column:","40e0cf0f":"Some of the important hyperparameters to tune an XGBoost are:\n- `n_estimators` $\\;\\;\\;\\;\\;$ = Number of trees one wants to build.\n- `learning_rate` $\\;\\;\\;\\;$= Rate at which our model learns patterns in data. After every round, it shrinks the feature weights to reach the best optimum.\n- `max_depth` $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$= Determines how deeply each tree is allowed to grow during any boosting round.\n- `colsample_bytree` = Percentage of features used per tree. \n- `gamma` $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$= Specifies the minimum loss reduction required to make a split.","e1f81869":"***Predicting missing values with regression***","df3e8802":"The same is true for the `security_deposit`:","82857e20":"One of the most important pieces of information for predicting the rate is the size. Since the column `square_feet` was heavily filled with null values, we dropped it in the previous section. (Besides, size in Germany is expressed in square meters, not in square feet.)\n\nLet's check, if the column `description` reveals any information about size instead: ","83200deb":"***Recoding Categorical Features***","10d21c7d":"## Table of Contents\n<a id='Table of contents'><\/a>\n\n### <a href='#1. Obtaining and Viewing the Data'> 1. Obtaining and Viewing the Data <\/a>\n\n### <a href='#2. Preprocessing the Data'> 2. Preprocessing the Data <\/a>\n* <a href='#2.1. Deciding which columns to work with'> 2.1. Deciding which columns to work with <\/a>\n* <a href='#2.2. Cleaning Price Columns'> 2.2. Cleaning Price Columns <\/a>\n* <a href='#2.3. Dealing with Missing Values'> 2.3. Dealing with Missing Values <\/a>\n* <a href='#2.4. Feature Engineering 1: Distance to Centroid of Berlin'> 2.4. Feature Engineering 1: Distance to Centroid of Berlin <\/a>\n* <a href='#2.5. Feature Engineering 2: Lodging Size'> 2.5. Feature Engineering 2: Lodging Size <\/a>\n* <a href='#2.6. Feature Engineering 3: Lodging Amenities'> 2.6. Feature Engineering 3: Lodging Amenities <\/a>\n\n### <a href='#3. Exploratory Data Analysis (EDA)'> 3. Exploratory Data Analysis (EDA) <\/a>\n\n### <a href='#4. Modeling the Data'> 4. Modeling the Data <\/a>\n* <a href='#4.1. Preparing Target and Features'> 4.1. Preparing Target and Features <\/a>\n* <a href='#4.2. Splitting and Scaling the Data'> 4.2. Splitting and Scaling the Data <\/a>\n* <a href='#4.3. Training an XGBoost Regressor'> 4.3. Training an XGBoost Regressor <\/a>\n* <a href='#4.4. Cross Validation'> 4.4. Cross Validation <\/a>\n\n### <a href='#5. Interpreting the Data'> 5. Interpreting the Data <\/a>\n\n### <a href='#6. Appendix'> 6. Appendix <\/a>","9bc27754":"Finally, the last big step: merging our sub_dataframe with the remaining columns from our original dataframe - this time, across the columns:","e1bc9b9b":"*Back to: <a href='#Table of contents'> Table of contents<\/a>*\n#### 2.3. Dealing with Missing Values\n<a id='2.3. Dealing with Missing Values'><\/a>","72cc531e":"*Back to: <a href='#Table of contents'> Table of contents<\/a>*\n#### 2.4. Feature Engineering 1: Distance to Centroid of Berlin\n<a id='2.4. Feature Engineering 1: Distance to Centroid of Berlin'><\/a>","63a21237":"**> Price Differences by Neighbourhood**","8f0d8a3e":"*Back to: <a href='#Table of contents'> Table of contents<\/a>*\n### 5. Interpreting the Data \n<a id='5. Interpreting the Data'><\/a>","8a6f48d1":"### 1. Obtaining and Viewing the Data \n<a id='1. Obtaining and Viewing the Data'><\/a>","2c521278":"*Back to: <a href='#Table of contents'> Table of contents<\/a>*\n### 6. Appendix \n<a id='6. Appendix'><\/a>","3f505ee6":"**> Price Differences by Accommodation Distance to Center of Berlin**","89637ddd":"All resources used in this notebook are listed below.\n\nData\n- http:\/\/insideairbnb.com\/get-the-data.html\n\nImputing missing values with Linear Regression\n- https:\/\/towardsdatascience.com\/the-tale-of-missing-values-in-python-c96beb0e8a9d\n\nXGBoost\n- https:\/\/www.datacamp.com\/community\/tutorials\/xgboost-in-python\n- https:\/\/www.kaggle.com\/marcelo06\/cross-validation-with-xgboost-python\n\nGeocoding\n- https:\/\/pypi.org\/project\/geopy\/\n\nCentroid of Berlin (in German!)\n- https:\/\/www.tagesspiegel.de\/berlin\/bezirke\/friedrichshain-kreuzberg\/reise-zum-mittelpunkt-berlins-am-ruhepol\/9837502.html\n\nVisualizations\n- https:\/\/python-graph-gallery.com\n- https:\/\/www.kaggle.com\/kostyabahshetsyan\/boston-airbnb-visualization\n\nInspiration for Analysis\n- https:\/\/towardsdatascience.com\/digging-into-airbnb-data-reviews-sentiments-superhosts-and-prices-prediction-part1-6c80ccb26c6a\n- https:\/\/www.kaggle.com\/ibjohnsson\/predicting-listing-prices\n- https:\/\/www.kaggle.com\/mathvv\/prediction-on-house-prices-xgboost-tutorial\n- https:\/\/towardsdatascience.com\/improving-airbnb-yield-prediction-with-text-mining-9472c0181731\n- https:\/\/github.com\/joaeechew\/airbnb_nlp\/blob\/master\/Capstone%20Project.ipynb","bf3033a7":"Machine learning algorithms generally need all data - including categorical data - in numeric form. To satisfy these algorithms, categorical features are converted into separate binary features called dummy variables. Therefore, we have to find a way to represent these variables as numbers before handing them off to the model. One typical way of doing this in one-hot encoding, which creates a new column for each unique category in a categorical variable. Each observation receives a 1 in the column for its corresponding category (= \"HOT\") and a 0 in all other new columns. To conduct one-hot encoding, we use the pandas get_dummies function.","87da6a99":"**> Price Differences by Cancellation Policy**","8c016f96":"To be on the safe side, let\u2019s remove all outliers over 300 square meters, and all those with 0:","8f153fc3":"One of the challenges in building models is mixing features that have different scales. Look at our dataset and compare bathrooms with size or maximum_nights. When we mix units with ranges that have different orders of magnitude, our models may not be able to find the proper coefficients. To account for this problem, we standardize or normalize the features.","f0f4fdec":"***Investigating sizes***\n\nLet's thoroughly examine our new `size` column:","0c13475a":"We can see that our average error (RMSE) in the initial XGBoost is around 22\u20ac, which improves to 17.5\u20ac by cross validation. Given the fact that after cleaning up the price column, 50% of our lodgings cost only up to 45\u20ac and 75% of our lodgings up to 70\u20ac - even the improved standard deviation of 17\u20ac is quite a massive inaccuracy that doesn't help much in recommending a price.\n\nIt turns out that the price is dependent not only on geography, size, and features. It stands to reason that \n- the quality of presentation (e.g. pictures), \n- availability, \n- the number and content of reviews, \n- communication (e.g. acceptance rate, host response time) or \n- status ((whether or not the host is a super host)) \n\nmight have a substantial influence too. But the purpose of this analysis was to recommend a price to a \"rookie\" without any reviews or status. With this in mind, we might say that we can't recommend an exact price, but rather a range..\n\nThe next step (and maybe an idea for the reader) would be to start all over again and include the features mentioned above to try to find out if accuracy improves. That might help a beginner on Airbnb better know what price to aim for.\n\nWith what we have done here, we have explained 71% of the variance (R^2) with the most important accommodation features, as pictured below:","4078914e":"As calculated further up, half of our records still don't have a size. That means we have a problem! Dropping these records isn't an option as we would loose too much valuable information. Simply replacing it with the mean or median makes no sense. That leaves a third option: predict the missing value with a Machine Learning Algorithm. To not make it too complicated, we'll only use numerical features. Next, we have to split our data into \n- a) a training set where we have sizes and \n- b) a test set where we don't.","98fd59d9":"#### 2.1. Deciding which columns to work with \n<a id='2.1. Deciding which columns to work with'><\/a>","a775bd5a":"I'm interested in what amenities hosts offer their guests, and in order to enrich our prediction, whether we can determine what some of the more special and\/or rare amenities might be that make a property more desirable.","66b8e05f":"There are plenty of Nan's. It's more than likely that these hosts do not charge any extra cleaning fee. So let's simply replace these null values with $0.00:","d3e4e288":"#### 4.3. Training an XGBoost Regressor\n<a id='4.3. Training an XGBoost Regressor'><\/a>","6be23b2b":"To combine `y_test` and `X_test` back to our full test_dataframe, we have to create a bit of a circuit. As we can see here, the `X_test` dataframe has a specific index that's not compatible with `y_test`, where the numbers simply start from 0 onwards. So we need to provide `y_test` with the `X_test` index to make sure everyting fits together.","2e5d56b7":"In order to build more robust models, it is common to conduct a k-fold cross validation where all the entries in the original training dataset are used for both training and validation. XGBoost supports k-fold cross validation via the cv method. All we have to do is specify the `nfolds` parameter, which is the number of cross validation rounds you want to build. \n\nAlso, it supports many other parameters:\n- `num_boost_round` $\\;\\;\\;\\;\\;\\;\\;\\;$ = Specifies the number of trees to build (analogous to n_estimators).\n- `metrics` $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ = Specifies the evaluation metrics to be checked during CV.\n- `as_pandas` $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$= Returns the results in a decent pandas DataFrame.\n- `early_stopping_rounds` = Finishes model training early if the hold-out metric does not improve for a given number of rounds. \n\nWe will have to first convert the dataset into an optimized data structure called DMatrix so that XGBoost's cross validation method is supported.","789a09e2":"***Extracting size from text***"}}