{"cell_type":{"aab66e87":"code","523166f9":"code","c22f6641":"code","fcb94fc3":"code","1101a33e":"code","2d3df7de":"code","74c3f163":"code","5435b208":"code","184e5afb":"code","446ca224":"code","5fac0732":"code","b4c5b21d":"code","5c93dc2a":"code","6edf8791":"code","7b1bb766":"code","bc83ad0a":"code","80e29206":"code","11baee73":"code","c0aa0524":"code","4d100f74":"code","c614ef65":"markdown","4be58817":"markdown","b509cdd4":"markdown","9ecffe73":"markdown","e3262467":"markdown","2aef235a":"markdown","4ee7e263":"markdown","54f8e83a":"markdown","2ca50c34":"markdown","4020a730":"markdown","a394c799":"markdown","bc430c1f":"markdown","76b18cb3":"markdown","20fbd719":"markdown","e5a13b82":"markdown","11342dd2":"markdown","7390bab6":"markdown","b53d4ffc":"markdown","c23e5440":"markdown","fc0d852c":"markdown","7e9082d5":"markdown"},"source":{"aab66e87":"import os\nimport pandas as pd\nimport numpy as np\nnp.set_printoptions(precision=4)\n\nimport catboost\nfrom catboost import CatBoostClassifier","523166f9":"from catboost.datasets import amazon\n(train_df, test_df) = amazon()","c22f6641":"y = train_df.ACTION\nX = train_df.drop('ACTION', axis=1)\n\n## categorical features\ncat_features = list(range(0, X.shape[1]))\nprint(cat_features)","fcb94fc3":"## Check the balance of the model\nprint('Labels: {}'.format(set(y)))\nprint('Zero count = {}, One count = {}'.format(len(y) - sum(y), sum(y)))","1101a33e":"from sklearn.model_selection import train_test_split\nfrom catboost import Pool\ndata = train_test_split(X, y, test_size=0.2, random_state=0)\nX_train, X_validation, y_train, y_validation = data\n\ntrain_pool = Pool(\n    data=X_train, \n    label=y_train, \n    cat_features=cat_features\n)\n\nvalidation_pool = Pool(\n    data=X_validation, \n    label=y_validation, \n    cat_features=cat_features\n)","2d3df7de":"#### Start training a simple model\nmodel = CatBoostClassifier(iterations=1000, learning_rate=0.01)\nmodel.fit(train_pool, verbose=100);","74c3f163":"print(model.predict(X_validation))","5435b208":"## get probabilities\nprint(model.predict_proba(X_validation))","184e5afb":"## get feature importance\nnp.array(model.get_feature_importance(prettified=True))","446ca224":"import eli5\neli5.explain_weights(model) # clf is the model fitted ","5fac0732":"# training the random forest model\nfrom sklearn.ensemble import RandomForestClassifier\nrf_model = RandomForestClassifier(n_estimators=200,max_depth=5, min_samples_leaf=100,n_jobs=-1, random_state=10)\nrf_model.fit(X_train, y_train)","b4c5b21d":"eli5.show_prediction(rf_model, X_train.iloc[1], feature_names = list(X.columns)) ","5c93dc2a":"import lime\nimport lime.lime_tabular\nexplainer = lime.lime_tabular.LimeTabularExplainer(training_data=np.array(X_train),\n                                                   feature_names=X_train.columns)","6edf8791":"# storing a new observation\ni = 6\nX_observation = X_validation.iloc[[i], :]","7b1bb766":"rf_model.predict_proba(X_observation)[0,1]","bc83ad0a":"# explanation using the random forest model\nexplanation = explainer.explain_instance(X_observation.values[0], rf_model.predict_proba)\nexplanation.show_in_notebook(show_table=True, show_all=False)\nprint(explanation.score)","80e29206":"shap_values = model.get_feature_importance(\n    validation_pool, \n    'ShapValues'\n)\nexpected_value = shap_values[0,-1]\nshap_values = shap_values[:,:-1]\nprint(shap_values.shape)\n\nimport shap\n\nshap.initjs()\nshap.force_plot(expected_value, shap_values[1,:], X_validation.iloc[1,:])","11baee73":"shap.summary_plot(shap_values, X_validation)","c0aa0524":"X_train.columns.tolist()","4d100f74":"import yellowbrick\nfrom yellowbrick.features import JointPlotVisualizer\nvisualizer = JointPlotVisualizer(columns=['ROLE_ROLLUP_1','ROLE_ROLLUP_2'])\nvisualizer.fit_transform(X_train, y_train)\nvisualizer.show()\n","c614ef65":"<a id='2'><\/a>\n# <p style=\"background-color:maroon; font-family:newtimeroman; font-size:150%;text-align:center; border-radius: 15px 50px;color:white\">2. Introduction \ud83d\udc8e<\/p>\n\nAre you able to decipher a deep neural network? Building a complex and dense machine learning model has the ability to achieve the accuracy we want, but is it practical? Can you illustrate how the black-box model arrived at the final result by opening it up?\n\nMachine learning is being used by a wide range of companies to direct their strategy and improve their bottom line. It's important to develop a model that we can communicate to our clients and stakeholders.\n\nSo, how do we go about creating interpretable machine learning models? This is what we'll discuss in this post. We'll start with an overview of interpretable machine learning and why it's relevant. Then we'll learn how to construct machine learning models using a basic framework for interpretable machine learning.","4be58817":"## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:center; border-radius: 15px 50px\">Yellowbrick<\/p>","b509cdd4":"## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:center; border-radius: 15px 50px\">SHAP - SHapley Additive Explaination<\/p>\n### Local Interpretation ->","9ecffe73":"<a id='4'><\/a>\n# <p style=\"background-color:maroon; font-family:newtimeroman; font-size:150%;text-align:center; border-radius: 15px 50px;color:white\">4. Python Implementation \ud83d\udc8e<\/p>","e3262467":"## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:center; border-radius: 15px 50px\">ELI5 - Explain me like i am 5<\/p>\n### Global Interpretation ->","2aef235a":"<a id='3.6'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px\">3.6 List of libraries for interpretation \ud83d\udee0<\/p>","4ee7e263":"<a id='3.4'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px\">3.4 Inherently Interpretable Models \ud83d\udee0<\/p>\n\n\n<a id='3.4.1'><\/a>\n## <p style=\"background-color:pink; font-family:newtimeroman; font-size:100%; text-align:center; border-radius: 15px 50px\">3.4.1. Logistic Regression: \ud83d\udee0<\/p>\n\nFor linear models such as a linear and logistic regression, we can get the importance from the weights\/coefficients of each feature.\nBelow is the equation:\n<b><center>y=w1*x1+w2*x2+w3*x3+....+error<\/center><\/b>\n\nw1,w2,w3,... are essentially the weights given by the model to the variables x1,x2,x3 respectively.Also this is a model-specific technique that can be used for both global and local explanations.\n\n*Refer the link to know more on regression:*\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2015\/11\/beginners-guide-on-logistic-regression-in-r\/?utm_source=blog&utm_medium=decoding-black-box-step-by-step-guide-interpretable-machine-learning-models-python\n\n<a id='3.4.2'><\/a>\n## <p style=\"background-color:pink; font-family:newtimeroman; font-size:100%; text-align:center; border-radius: 15px 50px\">3.4.2. Feature Importance: \ud83d\udee0<\/p>\n\n![Screenshot 2021-05-18 at 3.15.39 PM.png](attachment:1028fa5e-55da-4854-aa2a-7aa90eb1f0f1.png)\n\nSince each feature is used once in our case, we calculate the importance\n\n* For X[2] :\nfeature_importance = (4 \/ 4) * (0.375 \u2013 (0.75 * 0.444)) = 0.042\n* For X[1] :\nfeature_importance = (3 \/ 4) * (0.444 \u2013 (2\/3 * 0.5)) = 0.083\n* For X[0] :\nfeature_importance = (2 \/ 4) * (0.5) = 0.25\n\n<a id='3.4.3'><\/a>\n## <p style=\"background-color:pink; font-family:newtimeroman; font-size:100%; text-align:center; border-radius: 15px 50px\">3.4.3. Tree Ensembles: \ud83d\udee0<\/p>\n\nFor Random forest and Gradient Boosting Machines, we can use the same feature importance. But this time, we will take its average across all trees. Let us look at the steps involved:\n\nGo through each tree in the ensemble\nFind the feature importance by using the technique explained in the above section\nTake the average of all feature importance across all trees using this formula:\n\n<b><center>Feature imp. = Sum of feature imp. of all estimators\/ number of trees<\/center><\/b>","54f8e83a":"## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:center; border-radius: 15px 50px\">LIME - Local interpretable Model agnostic explaination<\/p>","2ca50c34":"<a id='1'><\/a>\n# <p style=\"background-color:maroon; font-family:newtimeroman; font-size:150%;text-align:center; border-radius: 15px 50px;color:white\">1. Overview \ud83d\udc8e<\/p>\n\n* Every data scientist should be familiar with the principle of interpretable machine learning.\n* How do you build machine learning models that are easy to understand? This notebook will serve as a foundation.\n* Lets code and implement this.\n\nSo what we do ususlly...\nAs a data scientist , we cannot claim that we have used a complex ensemble of machine learning models and is achieving excellent results in terms of an evaluation metric such as ROC-AUC Score, Accuracy, RMSE, and so on. Your client will be unable to make sense of that statement.\n\nThe next question arises...So what excatly do we do then?\nIn most data science ventures, we need to build reliable and interpretable machine learning models. In this post, we'll look at some Python libraries that we can use to partially or fully interpret machine learning or deep learning models.\n\nSo lets begin this journey....\n","4020a730":"<a id='3'><\/a>\n# <p style=\"background-color:maroon; font-family:newtimeroman; font-size:150%;text-align:center; border-radius: 15px 50px;color:white\">3. Points to Cover \ud83d\udc8e<\/p>\n\n1. What is Interpretable Machine Learning?\n2. Why is there a need for interpretation?\n3. Framework for Interpretable Machine Learning\n4. Few Inherently Interpretable Models\n5. Model Agnostic Techniques for Interpretable Machine Learning\n6. List of libraries for interpreting ML models.","a394c799":"## <p style=\"background-color:pink; font-family:newtimeroman; font-size:120%; text-align:center; border-radius: 15px 50px;\">Contents<\/p>\n\n* [1. Overview \ud83d\udc8e](#1)\n* [2. Introduction ](#2)\n* [3. Points to Cover \u2699\ufe0f](#3)\n    * [3.1 What is interpretable Machine Learning \ud83d\udee0](#3.1)\n    * [3.2 Why is there a need of Interpretation \ud83d\udee0](#3.2)\n    * [3.3 Framework for interpretable Machine Learning \ud83d\udee0](#3.3)\n    * [3.4 Inherently Interpretable Models \ud83d\udee0](#3.4)\n        * [3.4.1 Logistic regression \ud83d\udee0](#3.4.1)\n        * [3.4.2 Feature Importance \ud83d\udee0](#3.4.2)\n        * [3.4.3 Feature Importance \ud83d\udee0](#3.4.3)\n    * [3.5 Model Agnostic Techniques \ud83d\udee0](#3.5)\n    * [3.6 List of libraries for interpretation \ud83d\udee0](#3.6)\n* [4. Python Implementation \ud83d\udcca](#4)","bc430c1f":"### Global Interpretation ->","76b18cb3":"# <p style=\"background-color:pink; font-family:newtimeroman; font-size:80%;text-align:center;color:white\">Create the Pool<\/p>","20fbd719":"## <p style=\"background-color:pink; font-family:newtimeroman; font-size:120%; text-align:center; border-radius: 15px 50px;\">Interpretation of ML Models<\/p>\n<img align='middle' width=\"1000\" height=\"100\" src=\"https:\/\/miro.medium.com\/max\/742\/0*Pnypj3flKzC2dR8F.png\">","e5a13b82":"* ELI5\n* LIME\n* SHAP\n* Yellowbrick\n\nLet's start by explaination first then implement the same in the notebook.\n\n<b><span style=\"color:blue\">1. ELI5<\/span><\/b>:\n\nELI5 is an acronym for \u2018Explain like I am a 5-year old\u2019. Python has ELI5 methods to show the functionality for both:\n\n* Global interpretation-Look at a model\u2019s parameters and figure out at a global level how the model works\n* Local interpretation-Look at a single prediction and identify features leading to that prediction.\n\nFor Global Interpretation, ELI5 has: *eli5.show_weights(classifier)*\nFor Local Interpretation,ELI5 has: eli5.show_predictions(cls,train_datapoint,featurenames)\n\nELI5 documentation:\nhttps:\/\/eli5.readthedocs.io\/en\/latest\/index.html#\n\n<b><span style=\"color:blue\">2. LIME<\/span><\/b>:\n\nLIME is an acronym for Local Inerpretabl Model Agnostic Explaination:\n\nThe idea behind LIME (Local Interpretable Model-Agnostic Explanations) is to provide the reasons why a prediction was made. Taking the same example, if a machine learning model predicts that a movie is going to be a blockbuster, LIME highlights the characteristics of the movie that would make it a super hit. Features like genre and actor might contribute to the movie doing well, while others like running time, director, etc. might work against it.\n\nThe creators of LIME outline four basic criteria for explanations that must be satisfied:\n\n* Interpretable: The explanation must be easy to understand depending on the target demographic\n* Local fidelity: The explanation should be able to explain how the model behaves for individual predictions\n* Model-agnostic: The method should be able to explain any model\n* Global perspective: The model, as a whole, should be considered while explaining it.\n\nLIME has lime.lime_tabular.LimeTabularExplainer(np.array(X_train),\n                                                feature_names,\n                                                class_names,\n                                                categorical_features, \n                                                mode)\nwhere,\n* np.array(X_train): The training data\n* class_names: The target variable(for regression), different classes in the target variable(for regression)\n* categorical_features: List of all the column names which are categorical\n* mode: For a regression problem: 'regression', and for a classification problem, 'classification\n\nWe can also use lime for image and text data using lime.lime_tabular.LimeImageExplainer() and lime.lime_tabular.LimeTextExplainer()\n\n<b><span style=\"color:blue\">3. SHAP<\/span><\/b>: \n\nSHAP is an scronym for SHapley Additive exPlanations.The SHAP library uses Shapley values at its core and is aimed at explaining individual predictions.Simply put, Shapley values are derived from Game Theory, where each feature in our data is a player, and the final reward is the prediction. Depending on the reward, Shapley values tell us how to distribute this reward among the players fairly.The best part about SHAP is that it offers a special module for tree-based models. Considering how popular tree-based models are in hackathons and in the industry, this module makes fast computations, even considering dependent features.\n\nFor global level predictions:\nshap.summary_plot(shap_values, features, feature_names)\n* features: Our training set of independent variables\n* feature_names: list of column names from the above training set\n\nKnow more about SHAP:\nhttps:\/\/github.com\/slundberg\/shap\n\n<b><span style=\"color:blue\">4. Yellowbrick<\/span><\/b>:\n\nThe Yellowbrick library is based on the scikit-learn and matplotlib libraries. This makes it compatible with most of scikit-learn\u2019s models. We can even use the same parameters that we used in our machine learning models (based on scikit-learn, of course)\n\nYellowbrick uses the concept of \u2018Visualisers\u2019. Visualizers are a set of tools that help us visualize the features in our data considering individual datapoints. Think of it as a dashboard for all your features. The main Visualisers offered by Yellowbrick are:\n\nfrom yellowbrick.features import JointPlotVisualizer\n\nvisualizer = JointPlotVisualizer(columns=['col1', 'col2'])\nvisualizer.fit_transform(X, y)\nvisualizer.show()\n\nKnow more about Yellowbrick:\nhttps:\/\/www.scikit-yb.org\/en\/latest\/quickstart.html","11342dd2":"### Local Interpretation ->","7390bab6":"<a id='3.1'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px\">3.1 What is Interpretable Machine Learning? \ud83d\udee0<\/p>\n\nMachine learning-powered applications have become an ever-increasing part of our lives, from image and facial recognition systems to conversational applications, autonomous machines, and personalized systems.\n\nThe sort of decisions and predictions being made by these machine learning-enabled systems are becoming much more profound, and in many cases, critical to life, death, and personal wellness. The need to trust these AI-based systems is paramount.\n\nInterpretation of a machine learning model is the process wherein we try to understand the predictions of a machine learning model.\n\n![Screenshot 2021-05-18 at 2.42.24 PM.png](attachment:2ffb277e-c751-43e1-bce1-7d94c8315858.png)\n\nPredictive modeling lifecycle involved 2 stages:\n* Where we keep an eye on the evaluation metric and experiment with different feature engineering, feature selection, and algorithm selection ideas in order to develop and create more robust models.\n* The second stage, which is the subject of this essay, is to analyse the models using the predictions and parameters to figure out why the classifier, for example, chose a particular class.","b53d4ffc":"<a id='3.5'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px\">3.5 Model Agnostic Techniques \ud83d\udee0<\/p>\n\nSo far, we've covered linear and logistic regression, as well as decision trees, using model-specific techniques. We also discussed the function significance methods that ensemble methods employ. What about other models, I'm sure you're wondering?\n\nWe now know that certain models, such as random forest and gradient boosting, are difficult to interpret.\n\nWe did use feature importance for these techniques. But it does not tell us whether a particular feature affects the target positively or negatively. And that is VERY important in certain cases.\n\nSome machine learning models are even harder to interpret. For example, a deep neural network model can have millions of learned parameters and it essentially ends up being an extreme version of a black-box model.\n\n![Screenshot 2021-05-18 at 3.22.53 PM.png](attachment:626ec87b-ff70-42c5-8d4b-b949941cace0.png)\n\nTypes of Model agnostic techniques:\n* Global Surrogate Method\n* LIME (Local interpretation Model agnostic explanation)","c23e5440":"<a id='3.2'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px\">3.2 Why is there a need for interpretation? \ud83d\udee0<\/p>\n\n* Checking causality of features & Debugging models\nHaving an interpretable model, in this case, enables us to test the causality of the features, test its reliability and ultimately can help us to debug the model appropriately.\n\n* Fairness\nThe model might learn the bias and predict accordingly (and this bias has unfortunately happened in certain real-world scenarios). Now, if there is no way to interpret our model at this stage, the model might end up providing false insights at the cost of compromising on fairness.","fc0d852c":"<a id='3.3'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px\">3.3 Framework for Interpretable Machine Learning \ud83d\udee0<\/p>\n\nWe can think of interpretability under two structures:\n1. Scope: If we're trying to view all data points globally, the significance of each variable, or clarify a specific prediction that's local?\n2. Model: Whether we're talking about a technique that can be applied to any kind of model (model agnostic) or one that is specifically designed for a specific class of algorithms (model specific).","7e9082d5":"Most of the variables have high negative impact for few data objects in Validation while for many data points it has small positive imapct."}}