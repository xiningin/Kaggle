{"cell_type":{"435216a4":"code","59326597":"code","344540ca":"code","8d8708e8":"code","f418dc45":"code","b54d7fab":"code","892e1220":"code","111822ad":"code","affaf7fd":"code","595a3256":"code","32abfa2d":"code","62d8e718":"code","d54d6c9c":"code","dd89af5c":"code","ea845cff":"code","6968e4a6":"code","4fc6d7c6":"code","9172d4ea":"code","e0a555c8":"code","7219c708":"code","28871fd7":"code","116004b8":"code","1a5f3820":"markdown","e5178c53":"markdown","d7d04f3d":"markdown","255ac5ff":"markdown","a4816a5d":"markdown","69dcb4fd":"markdown","74d72ce9":"markdown","d7ad4c8d":"markdown","d3c5b5aa":"markdown","32d7b827":"markdown","9d44ec94":"markdown","39d395bf":"markdown","7f13fd40":"markdown","5c23c4ba":"markdown","72995ddf":"markdown","c382313e":"markdown","3996224b":"markdown","146e045c":"markdown","a0d16f7c":"markdown","73848cef":"markdown","702156d4":"markdown","13c2e01b":"markdown"},"source":{"435216a4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import chi2_contingency\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve, auc, make_scorer\nfrom scikitplot.metrics import plot_roc\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, minmax_scale, Normalizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nplt.style.use('default')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","59326597":"df = pd.read_csv('\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\ndf","344540ca":"df.info()","8d8708e8":"df.isnull().any()","f418dc45":"delete_item = df[df.duplicated()]\n\ndf.drop(index = delete_item.index, inplace=True)","b54d7fab":"sns.countplot(x = 'output',data=df);","892e1220":"plt.figure(figsize=(25,10))\n\nplt.subplot(241)\n\nsns.countplot(x = 'sex', data=df)\n\nplt.subplot(242)\nsns.countplot(x = 'cp', data=df)\n\nplt.subplot(243)\nsns.countplot(x = 'fbs', data=df)\n\nplt.subplot(244)\nsns.countplot(x = 'restecg', data=df)\n\nplt.subplot(245)\nsns.countplot(x = 'exng', data=df)\n\nplt.subplot(246)\nsns.countplot(x = 'slp', data=df)\n\nplt.subplot(247)\nsns.countplot(x = 'caa', data=df)\n\nplt.subplot(248)\nsns.countplot(x = 'thall', data=df);\n\n\n","111822ad":"plt.figure(figsize=(15,15))\n\nplt.subplot(321)\nsns.histplot(x = 'age', data=df)\n\nplt.subplot(322)\nsns.histplot(x = 'trtbps', data=df)\n\nplt.subplot(323)\nsns.histplot(x = 'chol', data=df)\n\nplt.subplot(324)\nsns.histplot(x = 'thalachh', data=df)\n\nplt.subplot(325)\nsns.histplot(x = 'oldpeak', data=df);\n","affaf7fd":"risky = df[df['output'] == 1]\n\nnot_risky = df[df['output'] == 0]\n","595a3256":"plt.figure(figsize=(20,7))\n\nplt.subplot(131)\nsns.countplot(data=df,x='sex', hue='output')\n\nplt.subplot(132)\nsns.countplot(data=df,x='fbs', hue='output')\n\nplt.subplot(133)\nsns.countplot(data=df,x='exng', hue='output');","32abfa2d":"df","62d8e718":"plt.figure(figsize=(25,10))\n\nplt.subplot(331)\nsns.kdeplot(data=df, x='age',hue='output',fill=True,palette=[\"blue\",\"orange\"], alpha=.5, linewidth=0)\n\nplt.subplot(332)\nsns.kdeplot(data=df, x='cp',hue='output',fill=True,palette=[\"blue\",\"orange\"], alpha=.5, linewidth=0)\n\nplt.subplot(333)\nsns.kdeplot(data=df, x='trtbps',hue='output',fill=True,palette=[\"blue\",\"orange\"], alpha=.5, linewidth=0)\n\nplt.subplot(334)\nsns.kdeplot(data=df, x='chol',hue='output',fill=True,palette=[\"blue\",\"orange\"], alpha=.5, linewidth=0)\n\nplt.subplot(335)\nsns.kdeplot(data=df, x='restecg',hue='output',fill=True,palette=[\"blue\",\"orange\"], alpha=.5, linewidth=0)\n\nplt.subplot(336)\nsns.kdeplot(data=df, x='thalachh',hue='output',fill=True,palette=[\"blue\",\"orange\"], alpha=.5, linewidth=0)\n\nplt.subplot(337)\nsns.kdeplot(data=df, x='oldpeak',hue='output',fill=True,palette=[\"blue\",\"orange\"], alpha=.5, linewidth=0)\n\nplt.subplot(338)\nsns.kdeplot(data=df, x='slp',hue='output',fill=True,palette=[\"blue\",\"orange\"], alpha=.5, linewidth=0)\n\nplt.subplot(339)\nsns.kdeplot(data=df, x='caa',hue='output',fill=True,palette=[\"blue\",\"orange\"], alpha=.5, linewidth=0);","d54d6c9c":"modeling_df = df.copy()\n\ncat_cols = ['sex','cp','fbs','restecg','exng','slp','caa','thall']\nnum_cols = ['age','trtbps','chol','thalachh','oldpeak']\n\n\n\npd.get_dummies(modeling_df, columns = cat_cols, drop_first = True)\n\n\n\nX = modeling_df.drop(['output'],axis=1)\ny = modeling_df[['output']]\n\n\n\n\n\nX_train, X_val, y_train, y_val = train_test_split(X,y, test_size=.10, random_state=0)","dd89af5c":"X_train.value_counts()","ea845cff":"print('All Positive model equals:',130\/y_train.size)\n\nprint('All Negative model equals:',111\/y_train.size)","6968e4a6":"def plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()","4fc6d7c6":"def fit_model(model):\n    \n    model.fit(X_train, y_train)\n    val_preds = model.predict(X_val)\n    print(pd.DataFrame(confusion_matrix(y_val,val_preds),\\\n            columns=[\"Predicted No\", \"Predicted Yes\"],\\\n            index=[\"No\",\"Yes\"]))\n    print('\\n')\n    print(classification_report(y_val, val_preds))\n    \n    probs = model.predict_proba(X_val)\n    probs = probs[:, 1]\n    fpr, tpr, thresholds = roc_curve(y_val, probs)\n    plot_roc_curve(fpr,tpr)\n    print('auc score: '+ str(roc_auc_score(y_val,val_preds)))","9172d4ea":"log_model = LogisticRegression(max_iter=700)\n\nfit_model(log_model)","e0a555c8":"knn_model = KNeighborsClassifier(n_neighbors=50)\n\nfit_model(knn_model)","7219c708":"tree_model = DecisionTreeClassifier(criterion='entropy', max_depth=5, max_leaf_nodes=10)\n\nfit_model(tree_model)","28871fd7":"rf_model = RandomForestClassifier(n_estimators=1000, criterion = \"entropy\")\n\nfit_model(rf_model)","116004b8":"xgb = XGBClassifier(max_depth = 100,learning_rate = .07, booster = \"gblinear\" )\n\nfit_model(xgb)","1a5f3820":"# Conclusion\n\nOut of all the models that we tried, the xgb classifier outperformed the rest. It out-performed the baseline model accuracy by about 40%. It performed the best at predicting people who are more risk of having a heart attack. We had a very small data set to work with and a very small test set to work with so we should question how well this model may do on new unforseen data. ","e5178c53":"Since the all positive model has a higher accuracy we will be using it for our baseline. This means that out model must beat an accuracy score of 53.94%.","d7d04f3d":"This function efficiently trains each model on the training data and makes predictions for our validation set.","255ac5ff":"### Target Variable","a4816a5d":"## Establishing Baseline Performance\n\u200b\nTo understand if our model holds any weight, we need to establish a baseline model to test our models against.","69dcb4fd":"## Bivariate Analysis","74d72ce9":"## Model Selection\n\n### Defining model functions","d7ad4c8d":"# Import Libraries and Read Data","d3c5b5aa":"### Categorical Variables","32d7b827":"## Next Steps\n\nThe next step would be to collect more data. We have a very small data set to work with and there is a lot of potential to make a great model if we had more data to train it on. With an increase in records, Deep learning methods would also be interesting to apply here. ","9d44ec94":"## Model Fitting","39d395bf":"### Random Forest Classifier","7f13fd40":"This is a function to plot the ROC curve for each model.","5c23c4ba":"### Logistic Regression","72995ddf":"# EDA\n## Univariate Analysis\n\n","c382313e":"# Predicting Heart Attacks\n\nIn this notebook, we predict which people are at risk for heart attacks.","3996224b":"### Numerical Variables","146e045c":"## Model Preparation\n\nTo prepare the model we must get dummy variables for all the categorical variables. We also select a lower test size because there is not a lot of training data so we want to use as much as possible for training.","a0d16f7c":"# Model Building\n","73848cef":"## The Data\n\nWe use the \"Heart Attack Analysis & Prediction Dataset\" provided by Rashik Rahman.\n\nAge : Age of the patient\n\nSex : Sex of the patient\n\nexang: exercise induced angina (1 = yes; 0 = no)\n\nca: number of major vessels (0-3)\n\ncp : Chest Pain type chest pain type\n- Value 1: typical angina\n- Value 2: atypical angina\n- Value 3: non-anginal pain\n- Value 4: asymptomatic\n\ntrtbps : resting blood pressure (in mm Hg)\n\nchol : cholestoral in mg\/dl fetched via BMI sensor\n\nfbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n\nrest_ecg : resting electrocardiographic results\n\n- Value 0: normal\n- Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n- Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n- thalach : maximum heart rate achieved\n\ntarget : 0= less chance of heart attack 1= more chance of heart attack","702156d4":"### Decision Tree Classifier","13c2e01b":"### K-nearest Neighbors"}}