{"cell_type":{"b1ad89bb":"code","1ee9c6f6":"code","5197d6f7":"code","fbdfd888":"code","067f62ac":"code","a16f2df2":"code","a8a795c8":"code","4133df78":"code","08f0c9d4":"code","94208a5f":"code","db99ff60":"code","abc4e3e1":"code","26121279":"code","c9f022d8":"code","80db2341":"code","187bda0e":"code","e5f88ccf":"code","27461035":"code","3a854560":"code","7e377001":"code","94f34ca2":"code","3fb40c09":"code","f336dd8a":"code","b3f75d99":"code","a468287c":"code","fd22701b":"code","b313cda4":"markdown"},"source":{"b1ad89bb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1ee9c6f6":"#Import \ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","5197d6f7":"#Check train\ntrain.isnull().sum()","fbdfd888":"#Check test\ntest.isnull().sum()","067f62ac":"from sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import KBinsDiscretizer, OneHotEncoder\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import cross_val_score","a16f2df2":"full_data = [train, test]","a8a795c8":"for dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n    \n# Create new feature IsAlone from FamilySize\nfor dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n    \n# Remove all NULLS in the Embarked column\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')","4133df78":"import re","08f0c9d4":"def get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\n# Create a new feature Title, containing the titles of passenger names\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n","94208a5f":"for dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\ndataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\ndataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\ndataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')","db99ff60":"for dataset in full_data:\n    dataset['Cabin'] = dataset['Cabin'].astype(str).str[0]","abc4e3e1":"train.head()","26121279":"y = train['Survived']\nX = train.drop(['Survived','Name','Ticket'], axis = 1)","c9f022d8":"numerical_features = [c for c, dtype in zip(X.columns, X.dtypes)\n                     if dtype.kind in ['i','f'] and c !='PassengerId']\ncategorical_features = [c for c, dtype in zip(X.columns, X.dtypes)\n                     if dtype.kind not in ['i','f']]","80db2341":"numerical_features","187bda0e":"categorical_features","e5f88ccf":"#import train_test_split library\nfrom sklearn.model_selection import train_test_split\n\n# create train test split\nX_train, X_val, y_train, y_val = train_test_split( X,  y, test_size=0.3, random_state=0, stratify = y)","27461035":"preprocessor = make_column_transformer(\n    \n    (make_pipeline(\n    SimpleImputer(strategy = 'median')\n        #,KBinsDiscretizer(n_bins=3)\n    ), numerical_features),\n    \n    (make_pipeline(\n    SimpleImputer(strategy = 'constant', fill_value = 'missing'),\n    OneHotEncoder(categories = 'auto', handle_unknown = 'ignore')), categorical_features),\n)","3a854560":"#from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier","7e377001":"model_pipeline = make_pipeline(preprocessor,RandomForestClassifier(n_estimators = 200) )","94f34ca2":"model_pipeline.fit(X_train, y_train)","3fb40c09":"X_prediction = model_pipeline.predict(X_train)","f336dd8a":"print(f'Train : {model_pipeline.score(X_train, y_train):.3f}')","b3f75d99":"print(f'Test : {model_pipeline.score(X_val, y_val):.3f}')","a468287c":"submission_prediction = model_pipeline.predict(test.drop(['Name','Ticket'], axis = 1)).astype(int)\n#","fd22701b":"AllSub = pd.DataFrame({ 'PassengerId': test['PassengerId'],\n                       'Survived' : submission_prediction\n    \n})\n\nAllSub.to_csv(\"Solution_Pipeline_RF_IMproved.csv\", index = False)","b313cda4":"# Feature Engineering"}}