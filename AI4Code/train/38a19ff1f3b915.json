{"cell_type":{"448b7e74":"code","d0e8fd39":"code","d3b97792":"code","7e0a73da":"code","fb95891b":"code","15cac0d2":"code","0faad791":"code","2a0afa6d":"code","ba009984":"code","ad8a624e":"code","0a2ac3cc":"code","f975823b":"code","00e63f55":"code","a76913ab":"code","901f565b":"code","a7bdc8e7":"code","845514d9":"code","5f86f7f5":"code","0e6e7a47":"code","df3f126e":"code","b4aee48a":"code","ffef3c89":"code","58540299":"code","3d938905":"code","f5be49f2":"code","7fd9c8e8":"code","a9430446":"code","44d03800":"code","f81973cd":"code","4e8b2eac":"code","c00c1382":"code","4c447e78":"code","cfa69bf1":"code","2f18d8a4":"code","2721d58f":"code","9e006f18":"code","d6922366":"code","61cc0944":"code","0e29502b":"markdown","5b054758":"markdown","3fc01a97":"markdown"},"source":{"448b7e74":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #Libraries for visualization\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import metrics\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom yellowbrick.classifier import ConfusionMatrix\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score","d0e8fd39":"colnames=['Letter', 'X-box', 'Y-box', 'width','height','onpix total','x-bar mean','y-bar mean','x var','y var','xy correlation','mean x x y','mean x y y ','edge','correlation x-ege','edge bTt','correlation y-ege'] \nletters = pd.read_csv('\/kaggle\/input\/letter-recognition.data',names = colnames, header=None)\nletters","d3b97792":"letters.isnull().sum()\n#nema prazno :)","7e0a73da":"letters['LetterInt'] = letters['Letter'].apply(lambda x: ord(x.lower())-96)\nletters = letters.drop('Letter',axis=1)","fb95891b":"letters","15cac0d2":"columns = list(letters.columns)\ndata = letters.values\n\nfig = plt.figure(figsize=(15, 50))\nfig.subplots(17\/\/2+1, ncols=2)\nfor feat_i in range(17): \n    ax = plt.subplot(17\/\/2+1,2, feat_i+1)\n    plt.title(columns[feat_i]) \n    sns.distplot(data[:,feat_i], color = \"navy\")\nplt.show()","0faad791":"#Naive Bayes\n\ntraining_points = np.array(letters[:15000].drop(['LetterInt'], 1))\ntraining_labels = np.array(letters[:15000]['LetterInt'])\n\nclf = GaussianNB()\nclf.fit(training_points, training_labels)\n\ntest_points = np.array(letters[15000:].drop(['LetterInt'], 1))\ntest_labels = np.array(letters[15000:]['LetterInt'])\n\nexpected = test_labels\npredicted = clf.predict(test_points)\n\n# summarize the fit of the model\ntarget_names=['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\nprint(metrics.classification_report(expected, predicted,target_names=target_names))","2a0afa6d":"#KNN\ntraining_points = np.array(letters[:15000].drop(['LetterInt'], 1))\ntraining_labels = np.array(letters[:15000]['LetterInt'])\n\nneigh = KNeighborsClassifier(n_neighbors=26)\nneigh.fit(training_points, training_labels) \n\ntest_points = np.array(letters[15000:].drop(['LetterInt'], 1))\ntest_labels = np.array(letters[15000:]['LetterInt'])\n\nexpected = test_labels\npredicted = clf.predict(test_points)\n\n# summarize the fit of the model\ntarget_names=['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\nprint(metrics.classification_report(expected, predicted,target_names=target_names))","ba009984":"training_points = np.array(letters[:15000].drop(['LetterInt'], 1))\ntraining_labels = np.array(letters[:15000]['LetterInt'])\n\nclf = SVC()\nclf.fit(training_points, training_labels) \n\ntest_points = np.array(letters[15000:].drop(['LetterInt'], 1))\ntest_labels = np.array(letters[15000:]['LetterInt'])\n\nexpected = test_labels\npredicted = clf.predict(test_points)\n\n# summarize the fit of the model\ntarget_names=['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\nprint(metrics.classification_report(expected, predicted,target_names=target_names))\n","ad8a624e":"X=letters.drop(['LetterInt'],axis=1)\ny=letters['LetterInt']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","0a2ac3cc":"#Using Pearson Correlation\nplt.figure(figsize=(12,10))\ncor = letters.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","f975823b":"#Correlation with output variable\ncor_target = abs(cor[\"LetterInt\"])\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.2]\nrelevant_features\n#drop all other features apart from these","00e63f55":"print(letters[[\"y-bar mean\",\"mean x x y\"]].corr())\nprint(letters[[\"mean x x y\",\"correlation x-ege\"]].corr())\n\n#Moze y-bar mean da se dropne\n\n#So Pearson \"mean x x y\",\"correlation x-ege\"","a76913ab":"import statsmodels.api as sm\nX = letters.drop(\"LetterInt\",1)   #Feature Matrix\ny = letters[\"LetterInt\"]          #Target Variable\n\n#Adding constant column of ones, mandatory for Ordinary Least Squares model\nX_1 = sm.add_constant(X)\n#Fitting sm.OLS model\nmodel = sm.OLS(y,X_1).fit()\nmodel.pvalues\n\n\n#which is greater than 0.05. Hence we will remove this feature and build the model once again. This is an iterative process and can be performed at once with the help of loop.","901f565b":"l1 = letters.drop(['height','mean x x y','correlation x-ege','x var','edge'],axis=1)","a7bdc8e7":"from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\nreg = LassoCV()\nreg.fit(X, y)\nprint(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\nprint(\"Best score using built-in LassoCV: %f\" %reg.score(X,y))\ncoef = pd.Series(reg.coef_, index = X.columns)","845514d9":"print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","5f86f7f5":"imp_coef = coef.sort_values()\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Feature importance using Lasso Model\")","0e6e7a47":"letters['LetterInt'].value_counts() #Class distribution","df3f126e":"plt.figure()\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=6)\nproj = pca.fit_transform(X)\nplt.scatter(proj[:, 0], proj[:, 1], c=y, cmap=\"Paired\")\nplt.colorbar()","b4aee48a":"from sklearn.manifold import TSNE\ntsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\ntsne_results = tsne.fit_transform(letters)","ffef3c89":"#2d vizuelizacija \n#letters['tsne-2d-one'] = tsne_results[:,0]\n#letters['tsne-2d-two'] = tsne_results[:,1]\nlabel = np.array( letters['LetterInt'])\nplt.figure(figsize=(16,10))\nsns.scatterplot(\n    x=tsne_results[:,0], y=tsne_results[:,1],\n    hue = label,\n    palette=sns.color_palette(\"hls\", 26),\n    data=X,\n    legend=\"full\",\n    alpha=0.3\n)","58540299":"letters.describe().T","3d938905":"l = letters.drop(['Y-box','correlation y-ege','x var','edge bTt','y-bar mean','y var','width'],axis=1)\nl","f5be49f2":"training_points = np.array(l[:15000].drop(['LetterInt'], 1))\ntraining_labels = np.array(l[:15000]['LetterInt'])\n\nclf = SVC()\nclf.fit(training_points, training_labels) \n\n\ntest_points = np.array(l[15000:].drop(['LetterInt'], 1))\ntest_labels = np.array(l[15000:]['LetterInt'])\n\nexpected = test_labels\npredicted = clf.predict(test_points)\n\n# summarize the fit of the model\ntarget_names=['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\nprint(metrics.classification_report(expected, predicted,target_names=target_names))\n\n","7fd9c8e8":"training_points = np.array(l1[:15000].drop(['LetterInt'], 1))\ntraining_labels = np.array(l1[:15000]['LetterInt'])\n\nclf = SVC()\nclf.fit(training_points, training_labels) \n\n\ntest_points = np.array(l1[15000:].drop(['LetterInt'], 1))\ntest_labels = np.array(l1[15000:]['LetterInt'])\n\nexpected = test_labels\npredicted = clf.predict(test_points)\n\naccuracy = clf.score(test_points, test_labels)\n\nprint(float(accuracy))\n\n# summarize the fit of the model\ntarget_names=['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\nprint(metrics.classification_report(expected, predicted,target_names=target_names))\n\n","a9430446":"clf = GaussianNB()\nclf.fit(training_points, training_labels)\n# predicts = clf.predict(test_points)\n\naccuracy = clf.score(test_points, test_labels)\n\nprint(float(accuracy))\n\nexpected = test_labels\npredicted = clf.predict(test_points)\n\n# summarize the fit of the model\ntarget_names=['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\nprint(metrics.classification_report(expected, predicted,target_names=target_names))","44d03800":"neigh = KNeighborsClassifier(n_neighbors=26)\nneigh.fit(training_points, training_labels) \n\naccuracy = neigh.score(test_points, test_labels)\nprint(float(accuracy))\n\nexpected = test_labels\npredicted = clf.predict(test_points)\n\n# summarize the fit of the model\ntarget_names=['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\nprint(metrics.classification_report(expected, predicted,target_names=target_names))","f81973cd":"pca = PCA(n_components=6)\nl_pca = pca.fit_transform(X)\n\npca.explained_variance_ratio_","4e8b2eac":"pca_letters=pd.DataFrame(data=l_pca[0:,0:],\n                       index=[i for i in range(l_pca.shape[0])],\n                       columns=['f'+str(i) for i in range(l_pca.shape[1])])\npca_letters['LetterInt']=letters['LetterInt']\npca_letters","c00c1382":"#Plotting the Cumulative Summation of the Explained Variance\npca = PCA(n_components=16)\npca = PCA().fit(X)\nplt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Variance (%)') #for each component\nplt.title('Pulsar Dataset Explained Variance')\nplt.show()","4c447e78":"training_points = np.array(pca_letters[:15000].drop(['LetterInt'], 1))\ntraining_labels = np.array(pca_letters[:15000]['LetterInt'])\n\nclf = SVC()\nclf.fit(training_points, training_labels) \n\n\ntest_points = np.array(pca_letters[15000:].drop(['LetterInt'], 1))\ntest_labels = np.array(pca_letters[15000:]['LetterInt'])\n\nexpected = test_labels\npredicted = clf.predict(test_points)\n\n# summarize the fit of the model\ntarget_names=['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\nprint(metrics.classification_report(expected, predicted,target_names=target_names))\n\n","cfa69bf1":"clf = GaussianNB()\nclf.fit(training_points, training_labels)\n# predicts = clf.predict(test_points)\n\naccuracy = clf.score(test_points, test_labels)\n\nprint(float(accuracy))\n\nexpected = test_labels\npredicted = clf.predict(test_points)\n\n# summarize the fit of the model\ntarget_names=['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\nprint(metrics.classification_report(expected, predicted,target_names=target_names))","2f18d8a4":"pca_letters = letters\npca = PCA(n_components=6)\nl_pca = pca.fit_transform(pca_letters)\n\npca.explained_variance_ratio_\n\npca_letters['pca-one'] = l_pca[:,0]\npca_letters['pca-two'] = l_pca[:,1]\npca_letters['pca-three'] = l_pca[:,2]\npca_letters['pca-four'] = l_pca[:,3]\npca_letters['pca-five'] = l_pca[:,4]\npca_letters['pca-six'] = l_pca[:,5]\npca_letters","2721d58f":"training_points = np.array(pca_letters[:15000].drop(['LetterInt'], 1))\ntraining_labels = np.array(pca_letters[:15000]['LetterInt'])\n\ntest_points = np.array(pca_letters[15000:].drop(['LetterInt'], 1))\ntest_labels = np.array(pca_letters[15000:]['LetterInt'])\n","9e006f18":"clf = GaussianNB()\nclf.fit(training_points, training_labels)\nexpected = test_labels\npredicted = clf.predict(test_points)\n\n# accuracy: (tp + tn) \/ (p + n)\naccuracy = accuracy_score(expected, predicted)\nprint('Accuracy: %f' % accuracy)\n# precision tp \/ (tp + fp)\nprecision = precision_score(expected, predicted,average=\"macro\")\nprint('Precision: %f' % precision)\n# recall: tp \/ (tp + fn)\nrecall = recall_score(expected, predicted,average=\"macro\")\nprint('Recall: %f' % recall)\n# f1: 2 tp \/ (2 tp + fp + fn)\nf1 = f1_score(expected, predicted,average=\"macro\")\nprint('F1 score: %f' % f1)","d6922366":"clf = SVC()\nclf.fit(training_points, training_labels) \n\nexpected = test_labels\npredicted = clf.predict(test_points)\n\n\naccuracy = accuracy_score(expected, predicted)\nprint('Accuracy: %f' % accuracy)\nprecision = precision_score(expected, predicted,average=\"macro\")\nprint('Precision: %f' % precision)\nrecall = recall_score(expected, predicted,average=\"macro\")\nprint('Recall: %f' % recall)\nf1 = f1_score(expected, predicted,average=\"macro\")\nprint('F1 score: %f' % f1)","61cc0944":"neigh = KNeighborsClassifier(n_neighbors=26)\nneigh.fit(training_points, training_labels) \n\nexpected = test_labels\npredicted = neigh.predict(test_points)\n\naccuracy = accuracy_score(expected, predicted)\nprint('Accuracy: %f' % accuracy)\nprecision = precision_score(expected, predicted,average=\"macro\")\nprint('Precision: %f' % precision)\nrecall = recall_score(expected, predicted,average=\"macro\")\nprint('Recall: %f' % recall)\nf1 = f1_score(expected, predicted,average=\"macro\")\nprint('F1 score: %f' % f1)","0e29502b":"Lasso regularization. If the feature is irrelevant, lasso penalizes it\u2019s coefficient and make it 0. Hence the features with coefficient = 0 are removed and the rest are taken.","5b054758":"1.\tlettr\tcapital letter\t(26 values from A to Z) \n2.\tx-box\thorizontal position of box\t(integer) \n3.\ty-box\tvertical position of box\t(integer) \n4.\twidth\twidth of box\t(integer) \n5.\thigh height of box\t(integer) \n6.\tonpix\ttotal # on pixels\t(integer) \n7.\tx-bar\tmean x of on pixels in box\t(integer) \n8.\ty-bar\tmean y of on pixels in box\t(integer) \n9.\tx2bar\tmean x variance\t(integer) \n10.\ty2bar\tmean y variance\t(integer) \n11.\txybar\tmean x y correlation\t(integer) \n12.\tx2ybr\tmean of x * x * y\t(integer) \n13.\txy2br\tmean of x * y * y\t(integer) \n14.\tx-ege\tmean edge count left to right\t(integer) \n15.\txegvy\tcorrelation of x-ege with y\t(integer) \n16.\ty-ege\tmean edge count bottom to top\t(integer) \n17.\tyegvx\tcorrelation of y-ege with x\t(integer)","3fc01a97":"The correlation coefficient has values between -1 to 1\n\u2014 A value closer to 0 implies weaker correlation (exact 0 implying no correlation)\n\u2014 A value closer to 1 implies stronger positive correlation\n\u2014 A value closer to -1 implies stronger negative correlation"}}