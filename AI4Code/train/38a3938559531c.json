{"cell_type":{"0afe4e09":"code","5602581e":"code","7b770c82":"code","cae843a4":"code","82bdcc1c":"code","8aff7018":"code","0354be77":"code","891eeee3":"code","b6d554f8":"code","1b49de56":"code","58b75df5":"code","f95565da":"code","83527ad6":"code","fa92ed81":"code","62f15230":"code","58d47e6c":"code","fcdb3a5c":"code","7c76e911":"code","4a43189b":"code","19fa480b":"code","67588484":"code","b77c832e":"code","8d5a5e58":"code","1251ac77":"code","88b60c4d":"code","ad299d0e":"code","f6cbecc3":"code","6249743c":"code","e8ada9f1":"code","6d2df2f8":"code","0eeb6045":"code","277d734b":"code","d7579c12":"code","21d55a8f":"code","3859e681":"code","fe33ddb4":"code","1ad9515c":"code","522eeb65":"code","38283e10":"code","8141aef4":"code","cdc1fd70":"code","7a0dddf0":"code","e3e1cad2":"code","fca84298":"code","b11c44ca":"code","19948553":"code","28508532":"code","8f664acf":"code","30cbf3bd":"code","bbba976b":"code","06b9eb49":"code","c8e31466":"code","7878fe42":"code","1750dfe1":"code","ce3d4a6c":"code","46ef7b47":"markdown","0e64e934":"markdown","c2211ff2":"markdown","7cd63ef3":"markdown","6312c7db":"markdown","36e43bc2":"markdown","1acd64f3":"markdown","1cfdfc2d":"markdown","e4c86a6e":"markdown","f0f1a3c5":"markdown","b9901284":"markdown","e99fc7c9":"markdown","cdee2144":"markdown","5e7b0999":"markdown","1b03c7fe":"markdown","76c8b5e1":"markdown","bb6ef767":"markdown","62d7ec75":"markdown","dd2b2206":"markdown","d3384b27":"markdown","842b9e9e":"markdown","4b553ad9":"markdown","988cd99e":"markdown","4afb63a6":"markdown","52382c22":"markdown","c1520d40":"markdown","646b2df3":"markdown"},"source":{"0afe4e09":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import KFold, cross_val_score\n\nimport warnings\nimport time\nwarnings.filterwarnings(\"ignore\")\n\n# statistics\nfrom scipy import stats\nfrom scipy.stats import norm, skew, boxcox_normmax #for some statistics\nfrom scipy.special import boxcox1p\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Configs\npd.options.display.float_format = '{:,.3f}'.format\nsns.set(style=\"whitegrid\")\nplt.style.use('seaborn')\nseed = 42\nnp.random.seed(seed)","5602581e":"file_path = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv'\ndf_train = pd.read_csv(file_path)\nprint(\"Train DataSet = {} rows and {} columns\".format(df_train.shape[0], df_train.shape[1]))\nprint(\"Columns:\", df_train.columns.tolist())\ndf_train.head()","7b770c82":"file_path = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv'\ndf_test = pd.read_csv(file_path)\nprint(\"Test DataSet = {} rows and {} columns\".format(df_test.shape[0], df_test.shape[1]))\nprint(\"Columns:\", df_test.columns.tolist())\ndf_test.head()","cae843a4":"quantitative = [f for f in df_train.columns if df_train.dtypes[f] != 'object']\nquantitative.remove('SalePrice')\nquantitative.remove('Id')\nqualitative = [f for f in df_train.columns if df_train.dtypes[f] == 'object']\nprint(\"Qualitative Variables: (Numerics)\", \"\\n\\n=>\", qualitative,\n      \"\\n\\nQuantitative Variable: (Strings)\\n=>\", quantitative)\n","82bdcc1c":"def eda_numerical_feat(series, title=\"\", with_label=True, number_format=\"\"):\n    f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18, 5), sharex=False)\n    print(series.describe())\n    if(title != \"\"):\n        f.suptitle(title, fontsize=18)\n    sns.distplot(series, ax=ax1)\n    sns.boxplot(series, ax=ax2)\n    if(with_label):\n        describe = series.describe()\n        labels = { 'min': describe.loc['min'], 'max': describe.loc['max'], \n              'Q1': describe.loc['25%'], 'Q2': describe.loc['50%'],\n              'Q3': describe.loc['75%']}\n        if(number_format != \"\"):\n            for k, v in labels.items():\n                height = 0.3\n                if(k == 'Q2'):\n                    height = -0.3\n                ax2.text(v, height, k + \"\\n\" + number_format.format(v), ha='center', va='center', fontweight='bold',\n                         size=12, color='white', bbox=dict(facecolor='#445A64'))\n        else:\n            for k, v in labels.items():\n                ax2.text(v, 0.3, k + \"\\n\" + str(v), ha='center', va='center', fontweight='bold',\n                     size=8, color='white', bbox=dict(facecolor='#445A64'))\n    plt.show()","8aff7018":"def plot_top_rank_correlation(my_df, column_target, top_rank=5):\n    corr_matrix = my_df.corr()\n    f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18, 6), sharex=False)\n\n    ax1.set_title('Top {} Positive Corr to {}'.format(top_rank, column_target))\n    ax2.set_title('Top {} Negative Corr to {}'.format(top_rank, column_target))\n    \n    cols_top = corr_matrix.nlargest(top_rank+1, column_target)[column_target].index\n    cm = np.corrcoef(my_df[cols_top].values.T)\n    mask = np.zeros_like(cm)\n    mask[np.triu_indices_from(mask)] = True\n    hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f',\n                     annot_kws={'size': 8}, yticklabels=cols_top.values,\n                     xticklabels=cols_top.values, mask=mask, ax=ax1)\n    \n    cols_bot = corr_matrix.nsmallest(top_rank, column_target)[column_target].index\n    cols_bot  = cols_bot.insert(0, column_target)\n    cm = np.corrcoef(my_df[cols_bot].values.T)\n    mask = np.zeros_like(cm)\n    mask[np.triu_indices_from(mask)] = True\n    hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f',\n                     annot_kws={'size': 8}, yticklabels=cols_bot.values,\n                     xticklabels=cols_bot.values, mask=mask, ax=ax2)\n    \n    plt.show()","0354be77":"def test_normal_distribution(serie, series_name='series', thershold=0.4):\n    f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18, 6), sharex=False)\n    f.suptitle('{} is a Normal Distribution?'.format(series_name), fontsize=18)\n    ax1.set_title(\"Histogram to \" + series_name)\n    ax2.set_title(\"Q-Q-Plot to \"+ series_name)\n    \n    # calculate normal distrib. to series\n    mu, sigma = norm.fit(serie)\n    print('Normal dist. (mu= {:,.2f} and sigma= {:,.2f} )'.format(mu, sigma))\n    \n    # skewness and kurtoise\n    skewness = serie.skew()\n    kurtoise = serie.kurt()\n    print(\"Skewness: {:,.2f} | Kurtosis: {:,.2f}\".format(skewness, kurtoise))\n    # evaluate skeness\n    # If skewness is less than \u22121 or greater than +1, the distribution is highly skewed.\n    # If skewness is between \u22121 and \u2212\u00bd or between +\u00bd and +1, the distribution is moderately skewed.\n    # If skewness is between \u2212\u00bd and +\u00bd, the distribution is approximately symmetric.\n    pre_text = '\\t=> '\n    if(skewness < 0):\n        text = pre_text + 'negatively skewed or left-skewed'\n    else:\n        text =  pre_text + 'positively skewed or right-skewed\\n'\n        text += pre_text + 'in case of positive skewness, log transformations usually works well.\\n'\n        text += pre_text + 'np.log(), np.log1(), boxcox1p()'\n    if(skewness < -1 or skewness > 1):\n        print(\"Evaluate skewness: highly skewed\")\n        print(text)\n    if( (skewness <= -0.5 and skewness > -1) or (skewness >= 0.5 and skewness < 1)):\n        print(\"Evaluate skewness: moderately skewed\")\n        print(text)\n    if(skewness >= -0.5 and skewness <= 0.5):\n        print('Evaluate skewness: approximately symmetric')\n    # evaluate kurtoise\n    #     Mesokurtic (Kurtoise next 3): This distribution has kurtosis statistic similar to that of the normal distribution.\n    #         It means that the extreme values of the distribution are similar to that of a normal distribution characteristic. \n    #         This definition is used so that the standard normal distribution has a kurtosis of three.\n    #     Leptokurtic (Kurtosis > 3): Distribution is longer, tails are fatter. \n    #         Peak is higher and sharper than Mesokurtic, which means that data are heavy-tailed or profusion of outliers.\n    #         Outliers stretch the horizontal axis of the histogram graph, which makes the bulk of the data appear in a \n    #         narrow (\u201cskinny\u201d) vertical range, thereby giving the \u201cskinniness\u201d of a leptokurtic distribution.\n    #     Platykurtic: (Kurtosis < 3): Distribution is shorter, tails are thinner than the normal distribution. The peak\n    #         is lower and broader than Mesokurtic, which means that data are light-tailed or lack of outliers.\n    #         The reason for this is because the extreme values are less than that of the normal distribution.\n    print('evaluate kurtoise')\n    if(kurtoise > 3 + thershold):\n        print(pre_text + 'Leptokurtic: anormal: Peak is higher')\n    elif(kurtoise < 3 - thershold):\n        print(pre_text + 'Platykurtic: anormal: The peak is lower')\n    else:\n        print(pre_text + 'Mesokurtic: normal: the peack is normal')\n    \n    # shapiro-wilki test normality\n    # If the P-Value of the Shapiro Wilk Test is larger than 0.05, we assume a normal distribution\n    # If the P-Value of the Shapiro Wilk Test is smaller than 0.05, we do not assume a normal distribution\n    #     print(\"Shapiro-Wiki Test: Is Normal Distribution? {}\".format(stats.shapiro(serie)[1] < 0.01) )\n    #     print(stats.shapiro(serie))\n\n    \n    # ax1 = histogram\n    sns.distplot(serie , fit=norm, ax=ax1)\n    ax1.legend(['Normal dist. ($\\mu=$ {:,.2f} and $\\sigma=$ {:,.2f} )'.format(mu, sigma)],\n            loc='best')\n    ax1.set_ylabel('Frequency')\n    # ax2 = qq-plot\n    stats.probplot(df_train['SalePrice'], plot=ax2)\n    plt.show()\n    ","891eeee3":"def plot_model_score_regression(models_name_list, model_score_list, title=''):\n    fig = plt.figure(figsize=(15, 6))\n    ax = sns.pointplot( x = models_name_list, y = model_score_list, \n        markers=['o'], linestyles=['-'])\n    for i, score in enumerate(model_score_list):\n        ax.text(i, score + 0.002, '{:.6f}'.format(score),\n                horizontalalignment='left', size='large', \n                color='black', weight='semibold')\n    plt.ylabel('Score', size=20, labelpad=12)\n    plt.xlabel('Model', size=20, labelpad=12)\n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n\n    plt.title(title, size=20)\n\n    plt.show()","b6d554f8":"plot_top_rank_correlation(df_train, 'SalePrice', 7)","1b49de56":"data = pd.concat([df_train['SalePrice'], df_train['GarageArea']], axis=1)\ndata.plot.scatter(x='GarageArea', y='SalePrice', alpha=0.3, ylim=(0,800000));","58b75df5":"data = pd.concat([df_train['SalePrice'], df_train['GarageCars']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=df_train['GarageCars'], y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","f95565da":"data = pd.concat([df_train['SalePrice'], df_train['GrLivArea']], axis=1)\ndata.plot.scatter(x='GrLivArea', y='SalePrice', alpha=0.3, ylim=(0,800000));","83527ad6":"data = pd.concat([df_train['SalePrice'], df_train['LotArea']], axis=1)\ndata.plot.scatter(x='LotArea', y='SalePrice', alpha=0.3, ylim=(0,800000));","fa92ed81":"f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18, 5), sharex=False)\n\n\nvar = 'GrLivArea'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000), ax=ax1);\nax1.set_title('SalesPrice x GrLivArea')\n\n#scatter plot totalbsmtsf\/saleprice\nvar = 'TotalBsmtSF'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000), ax=ax2);\nax2.set_title('SalesPrice x TotalBsmtSF')","62f15230":"#box plot overallqual\/saleprice\nvar = 'OverallQual'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","58d47e6c":"data = pd.concat([df_train['SalePrice'], df_train['YearBuilt']], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=df_train['YearBuilt'], y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=45);","fcdb3a5c":"print(df_train.shape[0])","7c76e911":"rows_before = df_train.shape[0]\n# Remove outliers\ndf_train.drop(df_train[(df_train['OverallQual']<5) & (df_train['SalePrice']>200000)].index, inplace=True)\ndf_train.drop(df_train[(df_train['GrLivArea']>4500) & (df_train['SalePrice']<300000)].index, inplace=True)\n# eu adicinei, pois nao segue a reta  normal\ndf_train.drop(df_train[(df_train['LotArea']>100000)].index, inplace=True)\ndf_train.reset_index(drop=True, inplace=True)\nrows_after = df_train.shape[0]\nprint(\"Qtd Row removed outiliers:\", rows_before - df_train.shape[0])","4a43189b":"eda_numerical_feat(df_train['SalePrice'], \"SalePrice\", number_format='{:.0f}')\n\n## color subtitulos","19fa480b":"test_normal_distribution(df_train['SalePrice'] , 'SalePrice')","67588484":"# log(1+x) transform\ndf_train[\"SalePrice\"] = np.log1p(df_train[\"SalePrice\"])","b77c832e":"test_normal_distribution(df_train['SalePrice'] , 'SalePrice')","8d5a5e58":"# Split features and labels\ny_train = df_train['SalePrice'].reset_index(drop=True)\ntrain_features = df_train.drop(['SalePrice'], axis=1)\ntest_features = df_test\n\n# Combine train and test features in order to apply the feature transformation pipeline to the entire dataset\ndf_all = pd.concat([train_features, test_features]).reset_index(drop=True)\ndf_all.shape","1251ac77":"# ntrain = df_train.shape[0]\n# ntest = df_test.shape[0]\n# y_train = df_train.SalePrice.values\n# df_all = pd.concat((df_train, df_test)).reset_index(drop=True)\n# df_all.drop(['SalePrice'], axis=1, inplace=True)\n# print(\"all_data size is : {}\".format(df_all.shape))","88b60c4d":"sns.heatmap(df_train.isnull(), cbar=False)","ad299d0e":"def df_rating_missing_data(my_df):\n    \"\"\"Create DataFrame with Missing Rate\n    \"\"\"\n    all_data_nan = (my_df.isnull().sum() \/ len(my_df)) * 100\n    all_data_nan = all_data_nan.drop(all_data_nan[all_data_nan == 0].index).sort_values(ascending=False)[:30]\n    return pd.DataFrame({'Missing Ratio' :all_data_nan})  ","f6cbecc3":"df_missing_data = df_rating_missing_data(df_train)\ndf_missing_data","6249743c":"f, ax = plt.subplots(figsize=(15, 4))\nplt.xticks(rotation='90')\nsns.barplot(x=df_missing_data.index, y=df_missing_data['Missing Ratio'])\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","e8ada9f1":"# Some of the non-numeric predictors are stored as numbers; convert them into strings \ndf_all['MSSubClass'] = df_all['MSSubClass'].apply(str)\ndf_all['YrSold'] = df_all['YrSold'].astype(str)\ndf_all['MoSold'] = df_all['MoSold'].astype(str)","6d2df2f8":"# the data description states that NA refers to typical ('Typ') values\ndf_all['Functional'] = df_all['Functional'].fillna('Typ')\n# Replace the missing values in each of the columns below with their mode\ndf_all['Electrical'] = df_all['Electrical'].fillna(\"SBrkr\")\ndf_all['KitchenQual'] = df_all['KitchenQual'].fillna(\"TA\")\ndf_all['Exterior1st'] = df_all['Exterior1st'].fillna(df_all['Exterior1st'].mode()[0])\ndf_all['Exterior2nd'] = df_all['Exterior2nd'].fillna(df_all['Exterior2nd'].mode()[0])\ndf_all['SaleType'] = df_all['SaleType'].fillna(df_all['SaleType'].mode()[0])\ndf_all['MSZoning'] = df_all.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\n# the data description stats that NA refers to \"No Pool\"\ndf_all[\"PoolQC\"] = df_all[\"PoolQC\"].fillna(\"None\")\n# Replacing the missing values with 0, since no garage = no cars in garage\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    df_all[col] = df_all[col].fillna(0)\n# Replacing the missing values with None\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    df_all[col] = df_all[col].fillna('None')\n# NaN values for these categorical basement df_all, means there's no basement\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    df_all[col] = df_all[col].fillna('None')\n\n# Group the by neighborhoods, and fill in missing value by the median LotFrontage of the neighborhood\ndf_all['LotFrontage'] = df_all.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n# We have no particular intuition around how to fill in the rest of the categorical df_all\n# So we replace their missing values with None\nobjects = []\nfor i in df_all.columns:\n    if df_all[i].dtype == object:\n        objects.append(i)\ndf_all.update(df_all[objects].fillna('None'))\n\n# And we do the same thing for numerical df_all, but this time with 0s\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in df_all.columns:\n    if df_all[i].dtype in numeric_dtypes:\n        numeric.append(i)\ndf_all.update(df_all[numeric].fillna(0))","0eeb6045":"df_rating_missing_data(df_all)","277d734b":"numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in df_all.columns:\n    if df_all[i].dtype in numeric_dtypes:\n        numeric.append(i)","d7579c12":"# Create box plots for all numeric features\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=df_all[numeric] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","21d55a8f":"# Find skewed numerical features\nskew_features = df_all[numeric].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nprint(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\nskewness = pd.DataFrame({'Skew' :high_skew})\nskew_features.head(10)","3859e681":"# Normalize skewed features\nfor i in skew_index:\n    df_all[i] = boxcox1p(df_all[i], boxcox_normmax(df_all[i] + 1))","fe33ddb4":"# Let's make sure we handled all the skewed values\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=df_all[skew_index] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","1ad9515c":"df_all['BsmtFinType1_Unf'] = 1*(df_all['BsmtFinType1'] == 'Unf')\ndf_all['HasWoodDeck'] = (df_all['WoodDeckSF'] == 0) * 1\ndf_all['HasOpenPorch'] = (df_all['OpenPorchSF'] == 0) * 1\ndf_all['HasEnclosedPorch'] = (df_all['EnclosedPorch'] == 0) * 1\ndf_all['Has3SsnPorch'] = (df_all['3SsnPorch'] == 0) * 1\ndf_all['HasScreenPorch'] = (df_all['ScreenPorch'] == 0) * 1\ndf_all['YearsSinceRemodel'] = df_all['YrSold'].astype(int) - df_all['YearRemodAdd'].astype(int)\ndf_all['Total_Home_Quality'] = df_all['OverallQual'] + df_all['OverallCond']\ndf_all = df_all.drop(['Utilities', 'Street', 'PoolQC',], axis=1)\ndf_all['TotalSF'] = df_all['TotalBsmtSF'] + df_all['1stFlrSF'] + df_all['2ndFlrSF']\ndf_all['YrBltAndRemod'] = df_all['YearBuilt'] + df_all['YearRemodAdd']\n\ndf_all['Total_sqr_footage'] = (df_all['BsmtFinSF1'] + df_all['BsmtFinSF2'] +\n                                 df_all['1stFlrSF'] + df_all['2ndFlrSF'])\ndf_all['Total_Bathrooms'] = (df_all['FullBath'] + (0.5 * df_all['HalfBath']) +\n                               df_all['BsmtFullBath'] + (0.5 * df_all['BsmtHalfBath']))\ndf_all['Total_porch_sf'] = (df_all['OpenPorchSF'] + df_all['3SsnPorch'] +\n                              df_all['EnclosedPorch'] + df_all['ScreenPorch'] +\n                              df_all['WoodDeckSF'])\ndf_all['TotalBsmtSF'] = df_all['TotalBsmtSF'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\ndf_all['2ndFlrSF'] = df_all['2ndFlrSF'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\ndf_all['GarageArea'] = df_all['GarageArea'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\ndf_all['GarageCars'] = df_all['GarageCars'].apply(lambda x: 0 if x <= 0.0 else x)\ndf_all['LotFrontage'] = df_all['LotFrontage'].apply(lambda x: np.exp(4.2) if x <= 0.0 else x)\ndf_all['MasVnrArea'] = df_all['MasVnrArea'].apply(lambda x: np.exp(4) if x <= 0.0 else x)\ndf_all['BsmtFinSF1'] = df_all['BsmtFinSF1'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\n\ndf_all['haspool'] = df_all['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['has2ndfloor'] = df_all['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['hasgarage'] = df_all['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['hasbsmt'] = df_all['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['hasfireplace'] = df_all['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","522eeb65":"def logs(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(np.log(1.01+res[l])).values)   \n        res.columns.values[m] = l + '_log'\n        m += 1\n    return res\n\nlog_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n                 'TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n                 'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n                 'TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF',\n                 'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','YearRemodAdd','TotalSF']\n\ndf_all = logs(df_all, log_features)","38283e10":"def squares(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(res[l]*res[l]).values)   \n        res.columns.values[m] = l + '_sq'\n        m += 1\n    return res \n\nsquared_features = ['YearRemodAdd', 'LotFrontage_log', \n              'TotalBsmtSF_log', '1stFlrSF_log', '2ndFlrSF_log', 'GrLivArea_log',\n              'GarageCars_log', 'GarageArea_log']\ndf_all = squares(df_all, squared_features)","8141aef4":"print('before', df_all.shape)\ndf_all = pd.get_dummies(df_all).reset_index(drop=True)\nprint('after encoded categorical features', df_all.shape)\n# Remove any duplicated column names\ndf_all = df_all.loc[:,~df_all.columns.duplicated()]\nprint('after remove duplicate', df_all.shape)","cdc1fd70":"X_train = df_all.iloc[:len(y_train), :]\nX_test = df_all.iloc[len(y_train):, :]\nX_train.shape, y_train.shape, X_test.shape","7a0dddf0":"from sklearn.model_selection import GridSearchCV, KFold, cross_val_score, train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, RobustScaler, scale\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import ElasticNet, LassoCV, BayesianRidge, LassoLarsIC\nfrom sklearn.linear_model import Ridge, RidgeCV, ElasticNet, ElasticNetCV\nfrom sklearn.kernel_ridge import KernelRidge\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor, BaggingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.svm import SVR\n\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor","e3e1cad2":"# Setup cross validation folds\nkf = KFold(n_splits=4, random_state=42, shuffle=True)\n\n# Define error metrics\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=X_train):\n    rmse = np.sqrt(-cross_val_score(model, X, y_train, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","fca84298":"# Light Gradient Boosting Regressor\nlightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=7000,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)\n\n# XGBoost Regressor\nxgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=6000,\n                       max_depth=4,\n                       min_child_weight=0,\n                       gamma=0.6,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:squarederror',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006,\n                       random_state=42)\n\n# Ridge Regressor\nridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kf))\n\n# setup models    \nlasso_alphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n\nelastic_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\nelastic_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\n\n# Lasso Regressor\nlasso = make_pipeline(RobustScaler(),\n                      LassoCV(max_iter=1e7, alphas=lasso_alphas2,\n                              random_state=42, cv=kf))\n# Elastic Net Regressor\nelasticnet = make_pipeline(RobustScaler(),  \n                           ElasticNetCV(max_iter=1e7, alphas=elastic_alphas,\n                                        cv=kf, l1_ratio=elastic_l1ratio))\n\n# Kernel Ridge\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n\n# Support Vector Regressor\nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n\n# Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(n_estimators=6000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)  \n\n# Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=1200,\n                          max_depth=15,\n                          min_samples_split=5,\n                          min_samples_leaf=5,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=42)\n\n# Stack up all the models above, optimized using xgboost\nstack_gen = StackingCVRegressor(regressors = (xgboost, lightgbm, svr, ridge, gbr, rf),\n                                meta_regressor = xgboost,\n                                use_features_in_secondary=True)","b11c44ca":"regressor_models = {\n    'LightGB': lightgbm, # 20s\n    'XGBoost': xgboost, # 340s = 5min 40s\n    'SVM_Regressor': svr, # 6s\n    'Ridge': ridge, # 6s\n    'RandomForest': rf, # 146s = 2min 20s\n    'GradientBoosting': gbr, # 93s = 1min 30s\n    # 'stack_gen': stack_gen, # N\u00e2o tem como fazer, esse CV \u00e9 para avaliar os outros, nao tem como aplicar o CV ao Stack\n    ## ADD++\n    'Lasso': lasso, # 15s\n    'KernelRidge': KRR, # 1.88s\n    'ElasticNet': elasticnet # 40s\n}\n\nscores = {}\n\n## Cross Validation\nt_start = time.time()\n\nfor model_name, model in regressor_models.items():\n    print(model_name)\n    t0 = time.time()\n    score = cv_rmse(model)\n    t1 = time.time()\n    m, s = score.mean(), score.std()\n    scores[model_name] = [m,s]\n    print('\\t=> mean {:.5f}, std: {:.5f}'.format(m, s))\n    print(\"\\t=> took {:,.3f} s\".format(t1 - t0))\n    \nt_ending = time.time()\nprint('took', t_ending - t_start)","19948553":"plot_model_score_regression(list(scores.keys()),\n                            [score for score, _ in scores.values()],\n                            'Best Individual Models by Mean score (RMSLE)')","28508532":"plot_model_score_regression(list(scores.keys()), \n                            [score for _, score in scores.values()],\n                            'Best Individual Models by std score (RMSLE)')","8f664acf":"# Def Stack Model: Stack up all the models above, optimized using  ['xgboost'\/'elasticnet']\nstack_gen = StackingCVRegressor(regressors = (xgboost, lightgbm, svr, ridge, gbr, rf),\n                                meta_regressor = elasticnet,\n                                use_features_in_secondary=True)\n\n# train a model and show the time\ndef fit_a_model(model, model_name):\n    t0 = time.time()\n    if(model_name == 'Stack'):\n        a_model = model.fit( np.array(X_train), np.array(y_train) )\n    else:\n        a_model =  model.fit( X_train, y_train )\n    t1 = time.time()\n    print(\"{} took {:,.3f} s\".format(model_name, t1 - t0))\n    return a_model\n\nlgb_model   = fit_a_model(lightgbm, 'LightGB') # 3.7s\nsvr_model   = fit_a_model(svr, 'SVM_R') # 1.8s\nridge_model = fit_a_model(ridge, 'Ridge') # 1.8s\ngbr_model   = fit_a_model(gbr, 'GradientBoost') # 30s\nlasso_model = fit_a_model(lasso, 'Lasso') # 2.8s\nkridg_model = fit_a_model(KRR, 'KernelRidge') # 1.2\nelast_model = fit_a_model(elasticnet, 'ElasticNet') # 8s\n# more time\nrf_model    = fit_a_model(rf, 'RandomForest') # 51s\nxgb_model   = fit_a_model(xgboost, 'XGboost') # 116s = 2min\nstack_model = fit_a_model(stack_gen, 'Stack') # 1.087s = 18min","30cbf3bd":"# Blend models in order to make the final predictions more robust to overfitting\ndef blended_predictions(X):\n    return ((0.10 * ridge_model.predict(X)) + \\\n#             (0.15 * kridg_model.predict(X)) + \\\n            (0.30 * gbr_model.predict(X)) + \\\n            (0.15 * elast_model.predict(X)) + \\\n            (0.15 * lgb_model.predict(X)) + \\\n#             (0.05 * rf_model.predict(X)) + \\\n            (0.30 * stack_model.predict(np.array(X))))\n\n\n# LAST BEST\n#     return ((0.10 * ridge_model.predict(X)) + \\\n#             (0.15 * svr_model.predict(X)) + \\\n#             (0.15 * gbr_model.predict(X)) + \\\n#             (0.15 * elast_model.predict(X)) + \\\n#             (0.15 * lgb_model.predict(X)) + \\\n# #             (0.05 * rf_model.predict(X)) + \\\n#             (0.30 * stack_model.predict(np.array(X))))\n\n# Original\n#     return ((0.10 * ridge_model.predict(X)) + \\\n#             (0.15 * svr_model.predict(X)) + \\\n#             (0.10 * gbr_model.predict(X)) + \\\n#             (0.15 * xgb_model.predict(X)) + \\\n#             (0.10 * lgb_model.predict(X)) + \\\n#             (0.05 * rf_model.predict(X)) + \\\n#             (0.35 * stack_model.predict(np.array(X))))\n\n# Others Ideas: Use stack with others models\n\n# Get final precitions from the blended model\nblended_score = rmsle(y_train, blended_predictions(X_train))\nscores['blended'] = (blended_score, 0)\nprint('RMSLE score on train data to Blend Model:')\nprint(blended_score)\n\n# Mesmo tentando outras coisa, nehum resultado foi melhor do que o Original\n\n# Original: 0.07972798382117793\n# try1:     0.07926790876360021 (change xgb => elastic)\n# try2:     0.07791727684371352 (remove rf_model and add 0.5 in lgb_model)\n# try3:     0.0761370032672606  (remove 0.5 from stack and add in gbr_model)\n# try4:     0.0723571304918255  (remove from pipe_line the secoond and the rf_model and put all (0.15 in gbd))","bbba976b":"# Evaluate Stack\nstack_score = rmsle(y_train, stack_model.predict(np.array(X_train)))\nscores['stack'] = (stack_score, 0)\nprint('RMSLE score on train data to Stack Model:\\n\\t=>', stack_score)","06b9eb49":" plot_model_score_regression(list(scores.keys()), [score for score, _ in scores.values()])","c8e31466":"# Read in sample_submission dataframe\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.shape","7878fe42":"# Append predictions from blended models\n# We Apply expm1 beacuse, our model generate Sale\u1e54rice conveterted by log1, this is a inverse operation\nsubmission.iloc[:,1] = np.floor(np.expm1(blended_predictions(X_test)))","1750dfe1":"# Fix outleir predictions\nq1 = submission['SalePrice'].quantile(0.0045)\nq2 = submission['SalePrice'].quantile(0.99)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\nsubmission.to_csv(\"submission_regression11.csv\", index=False)","ce3d4a6c":"# Scale predictions (BEST)\nsubmission['SalePrice'] *= 1.001619\nsubmission.to_csv(\"submission_regression33.csv\", index=False)","46ef7b47":"## Snippets <a id='index02'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n","0e64e934":"## Import Libs and DataSet <a id='index01'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n","c2211ff2":"<h1 style=\"text-align: center;\">House Prices: EDA and Regression<\/h1>\n\n<h3 align=\"center\">Made by \ud83d\ude80 <a href=\"https:\/\/www.kaggle.com\/rafanthx13\"> Rafael Morais de Assis<\/a><\/h3>\n\n<img src=\"https:\/\/www.laoistoday.ie\/wp-content\/uploads\/2018\/09\/house-prices-up2-1.jpg\" \/>\n\nCreated: 2020-08-24; \n\nLast updated: 2020-08-24;\n\nin progres.....\n\n**Next Ideas**\n+ Change Pre-Processing, Test other values in subimition, tests other combinations of models and weight\n\n## References\n\n+ https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n+ https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n+ https:\/\/www.kaggle.com\/lavanyashukla01\/how-i-made-top-0-3-on-a-kaggle-competition\n+ https:\/\/www.kaggle.com\/jesucristo\/1-house-prices-solution-top-1","7cd63ef3":"## Feature engineering <a id='index13'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\n### Create New Features <a id='index14'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n","6312c7db":"### Encoded Categorical Features <a id='index15'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","36e43bc2":"### Join Models in Blend Model <a id='index32'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","1acd64f3":"## Kaggle Description\n\nhttps:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data\n\n### Competition Desrciption\n\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\n### The Goal\n\nEach row in the dataset describes the characteristics of a house.\n\nOur goal is to predict the SalePrice, given these features.\n\nOur models are evaluated on the Root-Mean-Squared-Error (RMSE) between the log of the SalePrice predicted by our model, and the log of the actual SalePrice. Converting RMSE errors to a log scale ensures that errors in predicting expensive houses and cheap houses will affect our score equally.\n\n### File Description\n\n\n+ `train.csv` - the training set\n+ `test.csv` - the test set\n+ `data_description.txt` - full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here\n+ `sample_submission.csv` - a benchmark submission from a linear regression on year and month of sale, lot square footage, and number of bedrooms\n\n### DataSet Description\n\n| Column                                                                                                          | Type\/Values | \\|\\|\\| | Column                                                                 | Type\/Values |\n|-----------------------------------------------------------------------------------------------------------------|-------------|--------|------------------------------------------------------------------------|-------------|\n| SalePrice - the property's sale price in dollars.<br>This is the target variable that you're trying to predict. |             | \\|\\|\\| | HeatingQC: Heating quality and condition                               |             |\n| MSSubClass: The building class                                                                                  |             | \\|\\|\\| | CentralAir: Central air conditioning                                   |             |\n| MSZoning: The general zoning classification                                                                     |             | \\|\\|\\| | Electrical: Electrical system                                          |             |\n| LotFrontage: Linear feet of street connected to property                                                        |             | \\|\\|\\| | 1stFlrSF: First Floor square feet                                      |             |\n| LotArea: Lot size in square feet                                                                                |             | \\|\\|\\| | 2ndFlrSF: Second floor square feet                                     |             |\n| Street: Type of road access                                                                                     |             | \\|\\|\\| | LowQualFinSF: Low quality finished square feet (all floors)            |             |\n| Alley: Type of alley access                                                                                     |             | \\|\\|\\| | GrLivArea: Above grade (ground) living area square feet                |             |\n| LotShape: General shape of property                                                                             |             | \\|\\|\\| | BsmtFullBath: Basement full bathrooms                                  |             |\n| LandContour: Flatness of the property                                                                           |             | \\|\\|\\| | BsmtHalfBath: Basement half bathrooms                                  |             |\n| Utilities: Type of utilities available                                                                          |             | \\|\\|\\| | FullBath: Full bathrooms above grade                                   |             |\n| LotConfig: Lot configuration                                                                                    |             | \\|\\|\\| | HalfBath: Half baths above grade                                       |             |\n| LandSlope: Slope of property                                                                                    |             | \\|\\|\\| | Bedroom: Number of bedrooms above basement level                       |             |\n| Neighborhood: Physical locations within <br>Ames city limits                                                    |             | \\|\\|\\| | Kitchen: Number of kitchens                                            |             |\n| Condition1: Proximity to main road or railroad                                                                  |             | \\|\\|\\| | KitchenQual: Kitchen quality                                           |             |\n| Condition2: Proximity to main road or railroad <br>(if a second is present)                                     |             | \\|\\|\\| | TotRmsAbvGrd: Total rooms above grade <br>(does not include bathrooms) |             |\n| BldgType: Type of dwelling                                                                                      |             | \\|\\|\\| | Functional: Home functionality rating                                  |             |\n| HouseStyle: Style of dwelling                                                                                   |             | \\|\\|\\| | Fireplaces: Number of fireplaces                                       |             |\n| OverallQual: Overall material and finish quality                                                                |             | \\|\\|\\| | FireplaceQu: Fireplace quality                                         |             |\n| OverallCond: Overall condition rating                                                                           |             | \\|\\|\\| | GarageType: Garage location                                            |             |\n| YearBuilt: Original construction date                                                                           |             | \\|\\|\\| | GarageYrBlt: Year garage was built                                     |             |\n| YearRemodAdd: Remodel date                                                                                      |             | \\|\\|\\| | GarageFinish: Interior finish of the garage                            |             |\n| RoofStyle: Type of roof                                                                                         |             | \\|\\|\\| | GarageCars: Size of garage in car capacity                             |             |\n| RoofMatl: Roof material                                                                                         |             | \\|\\|\\| | GarageArea: Size of garage in square feet                              |             |\n| Exterior1st: Exterior covering on house                                                                         |             | \\|\\|\\| | GarageQual: Garage quality                                             |             |\n| Exterior2nd: Exterior covering on house <br>(if more than one material)                                         |             | \\|\\|\\| | GarageCond: Garage condition                                           |             |\n| MasVnrType: Masonry veneer type                                                                                 |             | \\|\\|\\| | PavedDrive: Paved driveway                                             |             |\n| MasVnrArea: Masonry veneer area in square feet                                                                  |             | \\|\\|\\| | WoodDeckSF: Wood deck area in square feet                              |             |\n| ExterQual: Exterior material quality                                                                            |             | \\|\\|\\| | OpenPorchSF: Open porch area in square feet                            |             |\n| ExterCond: Present condition of <br>the material on the exterior                                                |             | \\|\\|\\| | EnclosedPorch: Enclosed porch area in square feet                      |             |\n| Foundation: Type of foundation                                                                                  |             | \\|\\|\\| | 3SsnPorch: Three season porch area in square feet                      |             |\n| BsmtQual: Height of the basement                                                                                |             | \\|\\|\\| | ScreenPorch: Screen porch area in square feet                          |             |\n| BsmtCond: General condition of the basement                                                                     |             | \\|\\|\\| | PoolArea: Pool area in square feet                                     |             |\n| BsmtExposure: Walkout or garden level basement walls                                                            |             | \\|\\|\\| | PoolQC: Pool quality                                                   |             |\n| BsmtFinType1: Quality of basement finished area                                                                 |             | \\|\\|\\| | Fence: Fence quality                                                   |             |\n| BsmtFinSF1: Type 1 finished square feet                                                                         |             | \\|\\|\\| | MiscFeature: Miscellaneous feature not <br>covered in other categories |             |\n| BsmtFinType2: Quality of second <br>finished area (if present)                                                  |             | \\|\\|\\| | MiscVal: $Value of miscellaneous feature                               |             |\n| BsmtFinSF2: Type 2 finished square feet                                                                         |             | \\|\\|\\| | MoSold: Month Sold                                                     |             |\n| BsmtUnfSF: Unfinished square feet of basement area                                                              |             | \\|\\|\\| | YrSold: Year Sold                                                      |             |\n| TotalBsmtSF: Total square feet of basement area                                                                 |             | \\|\\|\\| | SaleType: Type of sale                                                 |             |\n| Heating: Type of heating                                                                                        |             | \\|\\|\\| | SaleCondition: Condition of sale                                       |             |","1cfdfc2d":"### See 'SalePrice' as normal distribution <a id='index07'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","e4c86a6e":"### Transform 'SalePrice' in a 'correct' normal distribution <a id='index08'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","f0f1a3c5":"## Recreate Train nad Test DataSets <a id='index16'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","b9901284":"## Table Of Contents (TOC) <a id=\"top\"><\/a>\n\n+ [Import Libs and DataSet](#index01) \n+ [Snippets](#index02)\n+ [EDA on 'SalePrice': the target to be predicted](#index03)\n  - [Top Correlation with 'SalePrice'](#index04)\n  - [Outiliers to 'SalePrice' to top corr features](#index05)\n  - [Remove Outiliers](#index06)\n  - [See 'SalePrice' as normal distribution](#index07)\n  - [Transform 'SalePrice' in a 'correct' normal distribution](#index08)\n+ [Data Cleaning](#index09)\n  - [Join Train and Test Datasets to cleaning](#index10)\n  - [Missing Data](#index11)\n  - [Fix skewness in features to be normal distributions](#index12)\n+ [Feature engineering](#index13)\n  - [Create New Features](#index14)\n  - [Encoded Categorical Features](#index15)\n+ [Recreate Train nad Test DataSets](#index16)\n+ [Developing models](#index17)\n  - [Evaluate models with CrossValidation](#index18)\n  - [Fit Models](#index19)\n  - [Join Models in Blend Model](#index32)\n  - [Evaluate Blend, Stack and all others models](#index33)\n+ [Submit Prediction](#index20)\n","e99fc7c9":"## Missing Data <a id='index11'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","cdee2144":"## Submit Prediction <a id='index20'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n","5e7b0999":"## Data Cleaning <a id='index09'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\n### Join Train and Test Datasets to cleaning <a id='index10'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","1b03c7fe":"## Developing models <a id='index17'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","76c8b5e1":"### Outiliers to 'SalePrice' to top corr features <a id='index05'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","bb6ef767":"## EDA on 'SalePrice': the target to be predicted <a id='index03'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\n","62d7ec75":"### Top Correlation with 'SalePrice' <a id='index04'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","dd2b2206":"### Evaluate models with CrossValidation <a id='index018'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\nCross valaidation serve para avaliar o mdoelo para novos dados. \n\nDiferente de t reinar e testar, queremos saber para o nosso modelo o quao bom ele \u00e9 pra tetar em outros dados, pois sua estratgia \u00e9 treinar\/testar sobre dados diversos.\n\nExemplop:\n\nImagina que voc\u00ea faz um modello e elete tem 90% de acerto no conjutno de teste.\n\nDepois, voc\u00ca aplica esse modelo em produ\u00e7\u00e2o e tem resultados de 70%.\n\nO que pode ter acontecido?\n\nTalvez para os dados de testse ele se sai muito bem, mas para dados difernetes do de testse saia ruim (Overfittin).\n\nENtao usamos Cross validation com K-Fold: \n+ Dividmiso o conjunto de treino em K partes, \n+ usasmos a maior parte para trieno e uma pequena para tests\n+ Obtemos resultados avaliandao treinamento e tests variandos ambos.\n+ Assim fazemos a media e temos mais ideia de quais modelos s\u00e2o mais est\u00e1veis (vendo o desvio padrao) e quais tem melhores resultados na media (avalaindao a media)","d3384b27":"### Evaluate Blend, Stack and all others models <a id='index33'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","842b9e9e":"OverallQual and GrLivArea tem as maiores correla\u00e7\u00f5es, depois GarageCars e GarageArea","4b553ad9":"## Abstract\n\nObjetivo\n+ Com base em 79 features, vamos tentar prever o pre\u00e7o de casas (regress\u00e3o)\n\nConhecimentos aprendidos neste notebook:\n+ Distribui\u00e7\u00e2o Normal: skewness, kurtoise, boxcox, teste de normalidade\n+ Diversars t\u00e9cnicas de regress\u00e3o\n+ Feature engineering: Diversass ideias e forma para este dataset\n+ Data Missing e como lidar\n+ An\u00e1lise explorat\u00f3ria para ver tendencias e retirar outiliers\n\nComo foi feito\n+ Cross Validation: Using 12-fold cross-validation\n+ Models: On each run of cross-validation I fit 7 models (ridge, svr, gradient boosting, random forest, xgboost, lightgbm regressors)\n+ Stacking: In addition, I trained a meta StackingCVRegressor optimized using xgboost\n+ Blending: All models trained will overfit the training data to varying degrees. Therefore, to make final predictions, I blended their predictions together to get more robust predictions.","988cd99e":"## Box Cow to target (A normal Distribution)\n\nhttp:\/\/www.portalaction.com.br\/analise-de-capacidade\/411-transformacao-de-box-cox\n\nQuando a distribui\u00e7\u00e3o normal n\u00e3o se ad\u00e9qua aos dados, muitas vezes \u00e9 \u00fatil aplicar a transforma\u00e7\u00e3o de Box-Cox para obtermos a normalidade. Considerando X1, ..., Xn os dados originais, a transforma\u00e7\u00e3o de Box-Cox consiste em encontrar um \u03bb tal que os dados transformados Y1, ..., Yn se aproximem de uma distribui\u00e7\u00e3o normal. Esta transforma\u00e7\u00e3o \u00e9 dada por\n\nEsse comportamento, posteriormente foi apresentado como a Curva de Gauss. Que mostrava que grande parte dos eventos ficam em torno de um valor m\u00e9dio, com uma certa variabilidade. Voc\u00ea sabe o que \u00e9 uma curva de distribui\u00e7\u00e3o normal? Ou o que essa hist\u00f3ria que te contei tem haver com isso? Sabe qual a sua import\u00e2ncia e para que serve? E como calcular?\n\nLeia mais em: https:\/\/www.voitto.com.br\/blog\/artigo\/distribuicao-normal\n\nhttps:\/\/www.voitto.com.br\/blog\/artigo\/distribuicao-normal","4afb63a6":"### Fix skewness in features to be normal distributions <a id='index12'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","52382c22":"### Fit models <a id='index19'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","c1520d40":"### Remove Outiliers <a id='index06'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","646b2df3":"considera\u00e7oes"}}