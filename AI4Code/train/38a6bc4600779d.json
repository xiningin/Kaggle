{"cell_type":{"ee56d055":"code","13ee3090":"code","a280cf67":"code","c628502d":"code","70aa5e9f":"code","bf34729d":"code","d5a76cc7":"code","7cc41564":"code","77951b47":"code","22fc30e0":"code","e0bbcc70":"code","38445cfd":"code","ab90570f":"code","9dc88981":"code","e40f48f1":"code","ade697cd":"code","86ab20e4":"code","229a0d65":"markdown","4cddff03":"markdown","e2e406e5":"markdown","401ee3bc":"markdown","e2ec88fa":"markdown","6d3607ab":"markdown","99543431":"markdown","689242bf":"markdown","6506b0f2":"markdown","9244b0c8":"markdown","80ccae29":"markdown","e446905c":"markdown"},"source":{"ee56d055":"!pip install albumentations > \/dev\/null 2>&1\n!pip install pretrainedmodels > \/dev\/null 2>&1\n!pip install catalyst > \/dev\/null 2>&1","13ee3090":"import numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader,Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport time \nfrom PIL import Image\ntrain_on_gpu = True\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\nfrom sklearn.metrics import accuracy_score\nimport cv2\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport albumentations\nimport pretrainedmodels\nfrom albumentations.pytorch import ToTensor\n\nimport collections","a280cf67":"from catalyst.dl.utils import UtilsFactory\nfrom catalyst.dl.experiments import SupervisedRunner\nfrom catalyst.dl.callbacks import EarlyStoppingCallback, OneCycleLR, InferCallback","c628502d":"data_transforms = albumentations.Compose([\n    albumentations.HorizontalFlip(),\n    albumentations.VerticalFlip(),\n    albumentations.RandomBrightness(),\n    albumentations.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225],\n    ),\n    ToTensor()\n    ])\ndata_transforms_test = albumentations.Compose([\n    albumentations.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225],\n    ),\n    ToTensor()\n    ])","70aa5e9f":"train_df = pd.read_csv('..\/input\/train.csv')\ntrain, valid = train_test_split(train_df.has_cactus, stratify=train_df.has_cactus, test_size=0.1)\n# creating dict with image names and labels\nimg_class_dict = {k:v for k, v in zip(train_df.id, train_df.has_cactus)}","bf34729d":"class CactusDataset(Dataset):\n    def __init__(self, datafolder, datatype='train', transform = transforms.Compose([transforms.CenterCrop(32),transforms.ToTensor()]), labels_dict={}):\n        self.datafolder = datafolder\n        self.datatype = datatype\n        self.image_files_list = [s for s in os.listdir(datafolder)]\n        self.transform = transform\n        self.labels_dict = labels_dict\n        if self.datatype == 'train':\n            self.labels = [np.float32(labels_dict[i]) for i in self.image_files_list]\n        else:\n            self.labels = [np.float32(0.0) for _ in range(len(self.image_files_list))]\n\n    def __len__(self):\n        return len(self.image_files_list)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.datafolder, self.image_files_list[idx])\n        img = cv2.imread(img_name)[:,:,::-1]\n        image = self.transform(image=img)\n        image = image['image']\n        label = self.labels[idx]\n        \n        return image, label","d5a76cc7":"dataset = CactusDataset(datafolder='..\/input\/train\/train', datatype='train', transform=data_transforms, labels_dict=img_class_dict)\ntest_set = CactusDataset(datafolder='..\/input\/test\/test', datatype='test', transform=data_transforms_test)","7cc41564":"loaders = collections.OrderedDict()\n\ntrain_sampler = SubsetRandomSampler(list(train.index))\nvalid_sampler = SubsetRandomSampler(list(valid.index))\nbatch_size = 512\nnum_workers = 0\ntrain_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\nvalid_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=num_workers)\n\nloaders[\"train\"] = train_loader\nloaders[\"valid\"] = valid_loader\nloaders[\"test\"] = test_loader","77951b47":"class Flatten(nn.Module):\n    def forward(self, input):\n        return input.view(input.size(0), -1)\n    \nclass Net(nn.Module):\n    def __init__(\n            self,\n            num_classes: int,\n            p: float = 0.2,\n            pooling_size: int = 2,\n            last_conv_size: int = 1664,\n            arch: str = \"densenet169\",\n            pretrained: str = \"imagenet\") -> None:\n        \"\"\"A simple model to finetune.\n        \n        Args:\n            num_classes: the number of target classes, the size of the last layer's output\n            p: dropout probability\n            pooling_size: the size of the result feature map after adaptive pooling layer\n            last_conv_size: size of the flatten last backbone conv layer\n            arch: the name of the architecture form pretrainedmodels\n            pretrained: the mode for pretrained model from pretrainedmodels\n        \"\"\"\n        super().__init__()\n        net = pretrainedmodels.__dict__[arch](pretrained=pretrained)\n        modules = list(net.children())[:-1]  # delete last layer\n        # add custom head\n        modules += [nn.Sequential(\n            Flatten(),\n            nn.BatchNorm1d(1664),\n            nn.Dropout(p),\n            nn.Linear(1664, num_classes)\n        )]\n        self.net = nn.Sequential(*modules)\n\n    def forward(self, x):\n        logits = self.net(x)\n        return torch.squeeze(logits)","22fc30e0":"# experiment setup\nnum_epochs = 10\nlogdir = \".\/logs\/simple\"\n\n# model, criterion, optimizer\nmodel = Net(num_classes=1)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.SGD(model.parameters(), momentum=0.99, lr=1e-2)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=2)","e0bbcc70":"# model runner\nrunner = SupervisedRunner()\n\n# model training\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler, \n    loaders=loaders,\n    logdir=logdir,\n    callbacks=[\n        OneCycleLR(\n            cycle_len=num_epochs, \n            div_factor=3,\n            increase_fraction=0.3,\n            momentum_range=(0.95, 0.85))\n    ],\n    num_epochs=num_epochs,\n    verbose=False\n)","38445cfd":"# plotting training progress\nUtilsFactory.plot_metrics(logdir=logdir)","ab90570f":"model = Net(num_classes=1)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.SGD(model.parameters(), momentum=0.99, lr=1e-2)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=2)\n\nnum_epochs = 20\nlogdir1 = \".\/logs\/simple1\"\n# model runner\nrunner1 = SupervisedRunner()\n\n# model training\nrunner1.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler, \n    loaders=loaders,\n    callbacks=[\n        EarlyStoppingCallback(patience=4, min_delta=0.01)\n    ],\n    logdir=logdir1,\n    num_epochs=num_epochs,\n    verbose=False\n)","9dc88981":"# plotting training progress\nUtilsFactory.plot_metrics(logdir=logdir1)","e40f48f1":"test_loader = collections.OrderedDict([(\"infer\", loaders[\"test\"])])\nrunner.infer(\n    model=model,\n    loaders=test_loader,\n    callbacks=[InferCallback()],\n)","ade697cd":"test_img = os.listdir('..\/input\/test\/test')\ntest_df = pd.DataFrame(test_img, columns=['id'])\ntest_preds = pd.DataFrame({'imgs': test_df.id.values, 'preds': runner.callbacks[0].predictions[\"logits\"]})\ntest_preds.columns = ['id', 'has_cactus']\ntest_preds.to_csv('sub.csv', index=False)\ntest_preds.head()","86ab20e4":"runner1.infer(\n    model=model,\n    loaders=test_loader,\n    callbacks=[InferCallback()],\n)\ntest_preds['has_cactus'] = runner1.callbacks[0].predictions[\"logits\"]\ntest_preds.to_csv('sub1.csv', index=False)","229a0d65":"Importing catalyst:\n- importing utils;\n- SupervisedRunner - a convenient wrapper for supervised models;\n- callbacks contain schedulers, metrics and many other useful things;","4cddff03":"### Training with early stopping","e2e406e5":"### Neural net architecture","401ee3bc":"![](https:\/\/i.imgur.com\/RBVgWdU.png)\n\nCurrently there are many DL frameworks which have their own strengths and advantages. Also there is a number of higher-level libraries, which provide an easier way to train models and make experiments.\n\n[Catalyst](https:\/\/github.com\/catalyst-team\/catalyst) provides high-level utils for PyTorch DL & RL research. It was developed with a focus on reproducibility, fast experimentation and code\/ideas reusing. Being able to research\/develop something new, rather then write another regular train loop.\n\nIt is supposed to be run from command line and customized with configuration files, but in this kernel I'll show how to use it in Jupyter Notebooks (or Kaggle Kernel).","e2ec88fa":"### Inference\nMaking inference is easy - we simply need to use a special callback","6d3607ab":"### Creating Catalyst data loaders\n\nWe crete data loaders in a usual way, but then we combine loaders into one dictionary for convenience.","99543431":"### Training model with SupervisedRunner","689242bf":"Reading and preparing data","6506b0f2":"#### Preparing for training","9244b0c8":"Defining tranformations using albumentations:","80ccae29":"### Training with one cycle","e446905c":"### Writing custom dataset class\n\nOne interesting point worth noticing: Pytorch automatically converts numpy data types into Pytorch datatypes as a result sometimes the type of Pytorch tensor can be incompatible with the loss and we would need to convert datatypes. I define datatype for labels as `np.float32` to avoid such a problem."}}