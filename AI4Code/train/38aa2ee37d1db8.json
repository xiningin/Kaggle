{"cell_type":{"30cef3dc":"code","3aa8f4ac":"code","1439ebb9":"code","e84cf9b8":"code","91f1a207":"code","4ae91d2a":"code","6fe35d7a":"code","3e999f11":"code","47308b63":"code","b91b520a":"code","19748b0e":"code","5a21622f":"code","a452c3a3":"code","851d63ce":"code","8e3c78da":"code","266b6ece":"code","bae00890":"code","5c23b44a":"code","1bdec98d":"code","5a0c2023":"code","23e13173":"code","5006cd81":"code","a8a1a2d9":"code","cf95448a":"code","e6593593":"code","28ec9712":"code","86faaca2":"code","73c306fa":"code","99dabd3a":"code","0c91eb67":"code","8e1254af":"code","29b03255":"code","b91ef03e":"code","13f8b304":"code","b3eec9c2":"code","a1e94d8f":"code","8c2e2345":"code","221dcb74":"code","9090c577":"code","5ea1734f":"code","73ceb0b0":"code","2a7d8338":"code","1e3dd19c":"code","5e222adb":"code","f093c35c":"code","63a254c1":"code","d2879773":"code","720fc1bb":"markdown","e126d905":"markdown","da04b8be":"markdown","2a34bcb5":"markdown","ad3dfccb":"markdown","d60fa8e2":"markdown","36079794":"markdown","1b21d2d0":"markdown","6facab5e":"markdown"},"source":{"30cef3dc":"# Install the required package\n!pip install bert-for-tf2","3aa8f4ac":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras.models import Model\nimport pandas as pd\nimport numpy as np\nimport bert\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import  Model\nfrom tensorflow.keras.layers import Input, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt","1439ebb9":"def grader_tf_version():\n    assert((tf.__version__)>'2')\n    return True\ngrader_tf_version()","e84cf9b8":"#reading data\nreviews=pd.read_csv('\/kaggle\/input\/imdb-data\/Reviews.csv')","91f1a207":"#extract data whose Score is not equal to 3 \nreviews=reviews.loc[reviews['Score']!=3]","4ae91d2a":"#Create a function who return 1 for Score>4 and 0 for Score<=2\ndef partition(x):\n    if x>3:\n        return 1\n    else:\n        return 0\nscore=reviews['Score']\npositiveNegative = score.map(partition) \n\nreviews['Score']=positiveNegative\n\nreviews.head()","6fe35d7a":"reviews[\"Score\"].value_counts()","3e999f11":"#select only Score and text columns\nreviews=reviews[['Score','Text']]","47308b63":"reviews.head(3)","b91b520a":"def grader_reviews():\n    temp_shape = (reviews.shape == (525814, 2)) and (reviews.Score.value_counts()[1]==443777)\n    assert(temp_shape == True)\n    return True\ngrader_reviews()","19748b0e":"#select 25000 data point whose score is 0 \nneg_review=reviews[reviews['Score']==0]\nneg_review=neg_review[:25000]","5a21622f":"#select 25000 data point whose score is 1 \npos_review=reviews[reviews['Score']==1]\npos_review=pos_review[:25000]","a452c3a3":"#adding positive & negative review data\nr=pos_review.append(neg_review)","851d63ce":"#shuffle the data\ndf = r.sample(frac=1).reset_index(drop=True)","8e3c78da":"df=df[['Score','Text']]","266b6ece":"df.head()\n#len(df)","bae00890":"#remove html tags from data\nfrom tqdm import tqdm\nimport re\nfor i in tqdm(range(len(df))):\n  cleanr = re.compile('<.*?>')\n  df['Text'][i] = re.sub(cleanr, '', df['Text'][i])","5c23b44a":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(df['Text'],df['Score'],test_size=.20,random_state=33,shuffle=False)","1bdec98d":"#Plot Bar graph on y_train,y_test\n#For Y_train\nimport matplotlib.pyplot as plt\ncount_1=Y_train.value_counts()[1]\ncount_0=Y_train.value_counts()[0]\nfig = plt.figure()\nax = fig.add_axes([0,1,1,1])\nlabel = ['0', '1']\nvalues = [count_0,count_1]\nax.bar(label,values)\nplt.show()","5a0c2023":"#For Y_test\nimport matplotlib.pyplot as plt\ncount_1=Y_test.value_counts()[1]\ncount_0=Y_test.value_counts()[0]\nfig = plt.figure()\nax = fig.add_axes([0,1,1,1])\nlabel = ['0', '1']\nvalues = [count_0,count_1]\nax.bar(label,values)\nplt.show()","23e13173":"from tensorflow.keras.layers import Input, Dense, Activation, Dropout,LSTM,Embedding,Bidirectional,Conv1D,GlobalMaxPool1D","5006cd81":"# Functions for constructing BERT Embeddings: input_ids, input_masks, input_segments and Inputs\nMAX_SEQ_LEN=256# max sequence length\n\ndef get_masks(tokens):\n    \"\"\"Masks: 1 for real tokens and 0 for paddings\"\"\"\n    return [1]*len(tokens) + [0] * (MAX_SEQ_LEN - len(tokens))\n \ndef get_segments(tokens):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"  \n    segments = []\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            current_segment_id = 1\n    return segments + [0] * (MAX_SEQ_LEN - len(tokens))\n\ndef get_ids(tokens, tokenizer):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens,)\n    input_ids = token_ids + [0] * (MAX_SEQ_LEN - len(token_ids))\n    return input_ids\n\ndef create_single_input(sentence, tokenizer, max_len):\n    \"\"\"Create an input from a sentence\"\"\"\n    stokens = tokenizer.tokenize(sentence)\n    stokens = stokens[:max_len]\n    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n \n    ids = get_ids(stokens, tokenizer)\n    masks = get_masks(stokens)\n    segments = get_segments(stokens)\n\n    return ids, masks, segments\n \ndef convert_sentences_to_features(sentences, tokenizer):\n    \"\"\"Convert sentences to features: input_ids, input_masks and input_segments\"\"\"\n    input_ids, input_masks, input_segments = [], [], []\n \n    for sentence in tqdm(sentences,position=0, leave=True):\n        ids,masks,segments=create_single_input(sentence,tokenizer,MAX_SEQ_LEN-2)\n        assert len(ids) == MAX_SEQ_LEN\n        assert len(masks) == MAX_SEQ_LEN\n        assert len(segments) == MAX_SEQ_LEN\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n\n    return [np.asarray(input_ids, dtype=np.int32), \n          np.asarray(input_masks, dtype=np.int32), \n          np.asarray(input_segments, dtype=np.int32)]\n\ndef create_tonkenizer(bert_layer):\n    \"\"\"Instantiate Tokenizer with vocab\"\"\"\n    vocab_file=bert_layer.resolved_object.vocab_file.asset_path.numpy()\n    do_lower_case=bert_layer.resolved_object.do_lower_case.numpy() \n    tokenizer=bert.bert_tokenization.FullTokenizer(vocab_file,do_lower_case)\n    return tokenizer","a8a1a2d9":"def nlp_model(callable_object):\n    # Load the pre-trained BERT base model\n    bert_layer = hub.KerasLayer(handle=callable_object, trainable=True)  \n   \n    # BERT layer three inputs: ids, masks and segments\n    input_ids = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"input_ids\")           \n    input_masks = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"input_masks\")       \n    input_segments = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"segment_ids\")\n    \n    inputs = [input_ids, input_masks, input_segments] # BERT inputs\n    pooled_output, sequence_output = bert_layer(inputs) # BERT outputs\n    \n    # Add a hidden layer\n    x = Dense(units=768, activation='relu')(pooled_output)\n    x = Dropout(0.1)(x)\n \n    # Add output layer\n    outputs = Dense(1, activation=\"sigmoid\")(x)\n\n    # Construct a new model\n    model = Model(inputs=inputs, outputs=outputs)\n    return model\n\nmodel = nlp_model(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\")\nmodel.summary()","cf95448a":"tokenizer = create_tonkenizer(model.layers[3])","e6593593":"#it has to give no error \ndef grader_tokenize(tokenizer):\n    out = False\n    try:\n        out=('[CLS]' in tokenizer.vocab) and ('[SEP]' in tokenizer.vocab)\n    except:\n        out = False\n    assert(out==True)\n    return out\ngrader_tokenize(tokenizer)","28ec9712":"X_train_tokens,X_train_mask,X_train_segment = convert_sentences_to_features(X_train, tokenizer)","86faaca2":"X_test_tokens, X_test_mask, X_test_segment= convert_sentences_to_features(X_test, tokenizer)","73c306fa":"max_seq_length=256\ndef grader_alltokens_train():\n    out = False\n    \n    if type(X_train_tokens) == np.ndarray:\n        \n        temp_shapes = (X_train_tokens.shape[1]==max_seq_length) and (X_train_mask.shape[1]==max_seq_length) and \\\n        (X_train_segment.shape[1]==max_seq_length)\n        \n        segment_temp = not np.any(X_train_segment)\n        \n        mask_temp = np.sum(X_train_mask==0) == np.sum(X_train_tokens==0)\n        \n        no_cls = np.sum(X_train_tokens==tokenizer.vocab['[CLS]'])==X_train_tokens.shape[0]\n        \n        no_sep = np.sum(X_train_tokens==tokenizer.vocab['[SEP]'])==X_train_tokens.shape[0]\n        \n        out = temp_shapes and segment_temp and mask_temp and no_cls and no_sep\n      \n    else:\n        print('Type of all above token arrays should be numpy array not list')\n        out = False\n    assert(out==True)\n    return out\n\ngrader_alltokens_train()","99dabd3a":"def grader_alltokens_test():\n    out = False\n    if type(X_test_tokens) == np.ndarray:\n        \n        temp_shapes = (X_test_tokens.shape[1]==max_seq_length) and (X_test_mask.shape[1]==max_seq_length) and \\\n        (X_test_segment.shape[1]==max_seq_length)\n        \n        segment_temp = not np.any(X_test_segment)\n        \n        mask_temp = np.sum(X_test_mask==0) == np.sum(X_test_tokens==0)\n        \n        no_cls = np.sum(X_test_tokens==tokenizer.vocab['[CLS]'])==X_test_tokens.shape[0]\n        \n        no_sep = np.sum(X_test_tokens==tokenizer.vocab['[SEP]'])==X_test_tokens.shape[0]\n        \n        out = temp_shapes and segment_temp and mask_temp and no_cls and no_sep\n      \n    else:\n        print('Type of all above token arrays should be numpy array not list')\n        out = False\n    assert(out==True)\n    return out\ngrader_alltokens_test()","0c91eb67":"X_train_pooled_output=model.predict([X_train_tokens,X_train_mask,X_train_segment])","8e1254af":"!rm -rf .\/logs\/ \n!mkdir .\/logs\/","29b03255":"from tensorflow.keras.callbacks import TensorBoard\n\ntensorboard_callback = tf.keras.callbacks.TensorBoard('logs', histogram_freq=1)\n#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1)\nBATCH_SIZE = 8\nEPOCHS = 3\n\n# Use Adam optimizer to minimize the categorical_crossentropy loss\nopt = Adam(learning_rate=2e-5)\nmodel.compile(optimizer=opt, \n              loss='binary_crossentropy', \n              metrics=['accuracy'])\n\n# Fit the data to the model\nhistory = model.fit([X_train_tokens, X_train_mask, X_train_segment], Y_train,\n                    validation_data=([X_test_tokens, X_test_mask, X_test_segment], Y_test),\n                    epochs=EPOCHS,\n                    batch_size=BATCH_SIZE\n                    verbose = 1)","b91ef03e":"import pickle\nPkl_Filename = \"model.h5\"  \n\nwith open(Pkl_Filename, 'wb') as file:  \n    pickle.dump(model, file)\nmodel.save('model.h5')","13f8b304":"# Getting score metrics from our model\nscores = model.evaluate([X_test_tokens, X_test_mask, X_test_segment], Y_test, verbose=0)\n# Displays the accuracy of correct sentiment prediction over test data\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","b3eec9c2":"y_test_pred=model.predict(X_test)","a1e94d8f":"Y_test.shape","8c2e2345":"Y_test=Y_test.values.reshape(-1,1)","221dcb74":"for i in range(len(y_test_pred)):\n    if i>=.5:\n        y_test_pred[i]=1\n    else:\n        y_test_pred[i]=0","9090c577":"# Predict on test dataset\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(np.argmax(Y_test,axis=1), y_test_pred))","5ea1734f":"from sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\n#train_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(Y_test, y_test_pred)\n#plt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"a: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()\nauc_score=auc(test_fpr, test_tpr)\nprint('{0:.2f}'.format(auc_score*100))","73ceb0b0":"test_data=pd.read_csv('\/kaggle\/input\/test-data\/test.csv')","2a7d8338":"test_data.head(3)","1e3dd19c":"from tqdm import tqdm\nimport re\nfor i in tqdm(range(len(test_data))):\n    cleanr = re.compile('<.*?>')\n    test_data['Text'][i] = re.sub(cleanr, '', test_data['Text'][i])","5e222adb":"# Now do tokenization\ntest_tokens,test_mask,test_segment=convert_sentences_to_features(test_data['Text'], tokenizer)","f093c35c":"test_pred=model.predict([test_tokens,test_mask,test_segment])","63a254c1":"for i in range(len(test_pred)):\n    if i>=.5:\n        test_pred[i]=1\n    else:\n        test_pred[i]=0","d2879773":"test_pred","720fc1bb":"Model is overfit over the data.","e126d905":"#Grader Function-3","da04b8be":"<pre><font size=6>Part-1: Preprocessing<\/font><\/pre>","2a34bcb5":"*Part-2: Creating BERT Model*","ad3dfccb":"* \n<font size=4>Grader function 1 <\/font>","d60fa8e2":"**We are select 25000 data point of positive and negative to balance the data**","36079794":"**Create Model**","1b21d2d0":"Remove all the html tags","6facab5e":"Grader Function 2"}}