{"cell_type":{"3751de3d":"code","68149520":"code","3795fd08":"code","c453c59b":"code","c54046f1":"code","470122b7":"code","fbc10bdb":"code","d8259ab2":"code","2f782b3f":"code","051200d5":"code","7d1df260":"code","30cb5c07":"code","5d97fd63":"code","b010460d":"code","6cd731b8":"code","8431b68b":"code","175789f5":"code","ba8d235d":"code","8d3188ef":"code","770206e0":"code","c4e0baa0":"code","2c318265":"markdown","74b925b9":"markdown","86f18c4f":"markdown","35d088b7":"markdown","18aee3a7":"markdown","ba8b6f92":"markdown","bf3411cc":"markdown","2fc33610":"markdown","71d392eb":"markdown","140b156e":"markdown","875e26f7":"markdown"},"source":{"3751de3d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","68149520":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler,StandardScaler,OneHotEncoder\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error,auc\n\nimport warnings\nwarnings.filterwarnings('ignore')","3795fd08":"train = pd.read_csv('\/kaggle\/input\/the-great-indian-hiring-hackathon\/Participants_Data_TGIH\/Train.csv')\ntest = pd.read_csv('\/kaggle\/input\/the-great-indian-hiring-hackathon\/Participants_Data_TGIH\/Test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/the-great-indian-hiring-hackathon\/Participants_Data_TGIH\/Sample Submission.csv')","c453c59b":"IQR = train['Quantity'].quantile(0.75) - train['Quantity'].quantile(0.25)\n\nlower = -10000\nupper = 6000\n\noutliers = np.where(train['Quantity'] > upper , True , np.where(train['Quantity'] < lower , True, False))\n\ntrain = train.loc[~(outliers),]\n\ntrain.drop_duplicates(inplace=True)","c54046f1":"train_df = train.drop('UnitPrice',axis=1)\ny= train['UnitPrice']","470122b7":"train_idx = len(train)","fbc10bdb":"train_df['is_train'] = 1\ntest['is_train'] = 0\n\ndf_combine = pd.concat([train_df,test],axis=0,ignore_index=True)\nprint(df_combine.head())\n\ndf_combine['CustomerID'] = df_combine['CustomerID'].astype('int64')","d8259ab2":"# Adding a feature named customer invoice id\ndf_combine['customer_invoice_id'] = df_combine['CustomerID'].astype(str)+\"_\"+df_combine['InvoiceNo'].astype(str)\n\n#Adding a feature named total visited - total number of transactions done by the customer\ntotal_visited = df_combine.groupby(by=['CustomerID'],as_index=False)['InvoiceNo'].count()\ndf_combine['visit_count'] = 0\n\nfor idx,count in zip(total_visited.CustomerID,total_visited.InvoiceNo):\n    cus_idx = df_combine[df_combine['CustomerID']==idx]['visit_count'].index\n    df_combine['visit_count'].loc[cus_idx] = count\n\n# Based on the feature visit count we will create a new column frequent - if he visits store more than 52times which is weekly once in a full year\ndf_combine['is_frequent'] = np.where(df_combine['visit_count']>=52,1,0)\n# if visited more than 52 times frequent customer \n# if visited less than 52 times non frequent customer\n\n\n# Adding a feature number_of_times_ordered - where it contains the number of orders \n# totally placed for that stock\norder_count = df_combine.groupby(by=['StockCode'],as_index=False)['InvoiceNo'].count()\ndf_combine['stock_order_count'] = 0\n\nfor idx,count in zip(order_count.StockCode,order_count.InvoiceNo):\n    stk_idx = df_combine[df_combine['StockCode']==idx]['stock_order_count'].index\n    df_combine['stock_order_count'].loc[stk_idx] = count\n    \n\n    \ndf_combine.head()","2f782b3f":"# Adding new features using the InvoiceDate column\n\ndf_combine['InvoiceDate'] = pd.to_datetime(df_combine.InvoiceDate, format='%Y-%m-%d %H:%M:%S')\n\ndf_combine['day'] = df_combine['InvoiceDate'].dt.day\ndf_combine['month'] = df_combine['InvoiceDate'].dt.month_name()\ndf_combine['year'] = df_combine['InvoiceDate'].dt.year\ndf_combine['week'] = df_combine['InvoiceDate'].dt.week\ndf_combine['day_of_week'] = df_combine['InvoiceDate'].dt.dayofweek\ndf_combine['is_working_day'] = np.where(df_combine['day_of_week'].isin([0,1,2,3,4,6]),1,0)\ndf_combine['year_month'] = df_combine['year'].astype(str)+\"_\"+df_combine['month']\ndf_combine['is_month_start'] = df_combine['InvoiceDate'].dt.is_month_start\ndf_combine['is_month_end'] = df_combine['InvoiceDate'].dt.is_month_end\ndf_combine['quarter'] = df_combine['InvoiceDate'].dt.quarter\ndf_combine['is_year_start'] = df_combine['InvoiceDate'].dt.is_year_start\ndf_combine['is_year_end'] = df_combine['InvoiceDate'].dt.is_year_end\n\n#Extracting time features\ndf_combine['hour'] = df_combine['InvoiceDate'].dt.hour\n\n\ndf_combine = df_combine.drop('InvoiceDate',axis=1)","051200d5":"df_combine.drop('is_train',axis=1,inplace=True)","7d1df260":"ohe = OneHotEncoder()\ncols = df_combine.columns.values\n\ndf_combine_ohe = ohe.fit_transform(df_combine[cols])","30cb5c07":"train_df = df_combine_ohe[:train_idx]\ntest_df = df_combine_ohe[train_idx:]","5d97fd63":"X = train_df","b010460d":"def rmse(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))","6cd731b8":"lasso = Lasso(alpha=0.0005, random_state = 1,max_iter=100)\n\nENet = ElasticNet(alpha = 0.0005, l1_ratio=0.9, random_state = 3,max_iter=100)\n\nGBoost = GradientBoostingRegressor(n_estimators = 100,learning_rate=0.05,\n                                   max_depth = 10, random_state=5)\n\nmodel_rf = RandomForestRegressor(max_depth=17,n_estimators=100)","8431b68b":"models = [lasso,ENet,GBoost,model_rf]\nscores={}\nfor model in models:\n    print(model)\n    model.fit(X,y)\n    tr_pred = model.predict(X)\n    scores[model] = rmse(y,tr_pred)\n    print(scores[model])","175789f5":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=3):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","ba8d235d":"y_train = y.reset_index(drop=True)","8d3188ef":"\nstacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, model_rf),\n                                                 meta_model = lasso)\n\n\nstacked_averaged_models.fit(X, y_train)\n\nstacked_train_pred = stacked_averaged_models.predict(X)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test_df))\nprint(rmse(y_train, stacked_train_pred))","770206e0":"stacked_pred_ = stacked_averaged_models.predict(test_df)\nstacked_pred_","c4e0baa0":"submission['UnitPrice'] = stacked_pred_.round(2)\n# scored 23.09\nsubmission.to_csv('avg_model_sub.csv',index=False)","2c318265":"Adding new features based on the `InvoiceDate`","74b925b9":"### **If you find this notebook helpful please upvote**","86f18c4f":"# Model building\n\nBelow are the different base models which will be trained seperately on the whole training dataset.","35d088b7":"## OneHotEncoding","18aee3a7":"# Feature Engineering","ba8b6f92":"# Importing libraries","bf3411cc":"# Combining train and test dataset","2fc33610":"# Removing the outliers\n\nIn the data exploration phase it has been found that there are some outliers present in the `Quantity` column so removing those outliers.","71d392eb":"> The above submission scored 23.09 in the public leaderboard\n\nStacked Regression algorithm refernce - https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard","140b156e":"# Stacked Regressor","875e26f7":"# Predicting for test data"}}