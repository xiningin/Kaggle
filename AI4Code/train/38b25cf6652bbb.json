{"cell_type":{"6d90a445":"code","18761174":"code","52c3345a":"code","2551bf96":"code","7f9234ed":"code","7388009f":"code","59aac9f3":"code","aedb3e14":"code","b03c47e0":"code","e21103db":"code","16bf7725":"code","14f77910":"code","6a364b04":"code","cf1405f8":"code","f3c609ef":"code","23852de2":"code","a15d3ccb":"code","223cf80a":"code","bf323646":"code","5ca621c2":"code","39c5b2ab":"code","05c832d5":"code","7b1b01a3":"code","8d9bbed9":"code","dea8e6bb":"code","e3f9c35f":"markdown","a3d38b2c":"markdown","4c95c9b9":"markdown","eaea6af2":"markdown","ad30027b":"markdown"},"source":{"6d90a445":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","18761174":"df = pd.read_csv('\/kaggle\/input\/stop-words-in-28-languages\/gujarati.txt')\ndf","52c3345a":"#Codes by Ragnar https:\/\/www.kaggle.com\/rowhitswami\/starter-load-stopwords\n\ndef get_stopwords_list(stop_file_path):\n    \"\"\"load stop words \"\"\"\n    \n    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n        stopwords = f.readlines()\n        stop_set = set(m.strip() for m in stopwords)\n        return list(frozenset(stop_set))","2551bf96":"stopwords_path = \"\/kaggle\/input\/stop-words-in-28-languages\/gujarati.txt\"\nstopwords = get_stopwords_list(stopwords_path)","7f9234ed":"stopwords[0:10]","7388009f":"print(f\"Total number of stopwords: {len(stopwords)}\")","59aac9f3":"from gensim.models import Word2Vec\nimport gensim","aedb3e14":"corpus = ['\u0a85\u0aa8\u0ac1\u0a9a\u0acd\u0a9b\u0ac7\u0aa6 \u0ae7: \u0aaa\u0acd\u0ab0\u0aa4\u0abf\u0ab7\u0acd\u0aa0\u0abe \u0a85\u0aa8\u0ac7 \u0a85\u0aa7\u0abf\u0a95\u0abe\u0ab0\u0acb\u0aa8\u0ac0 \u0aa6\u0ac3\u0ab7\u0acd\u0a9f\u0abf\u0a8f \u0ab8\u0ab0\u0acd\u0ab5', '\u0aae\u0abe\u0aa8\u0ab5\u0acb \u0a9c\u0aa8\u0acd\u0aae\u0aa5\u0ac0 \u0ab8\u0acd\u0ab5\u0aa4\u0a82\u0aa4\u0acd\u0ab0 \u0a85\u0aa8\u0ac7 \u0ab8\u0aae\u0abe\u0aa8 \u0ab9\u0acb\u0aaf \u0a9b\u0ac7', '\u0aa4\u0ac7\u0aae\u0aa8\u0abe\u0aae\u0abe\u0a82 \u0ab5\u0abf\u0a9a\u0abe\u0ab0\u0ab6\u0a95\u0acd\u0aa4\u0abf \u0a85\u0aa8\u0ac7 \u0a85\u0a82\u0aa4\u0a83\u0a95\u0ab0\u0aa3 \u0ab9\u0acb\u0aaf']","b03c47e0":"stop_words = ['\u0aaa\u0acd\u0ab0\u0aaf\u0abe\u0ab8 \u0a95\u0ab0\u0acb',\n '\u0ab2\u0ac7\u0ab5\u0abe\u0aae\u0abe\u0a82',\n '\u0aaa\u0abe\u0a82\u0a9a\u0aae\u0acb',\n '\u0ab8\u0aae\u0abe\u0aa8',\n '\u0a9b\u0aa4\u0abe\u0a82',\n '\u0aa8 \u0ab9\u0acb\u0aa4',\n '\u0a9f\u0ac0\u0ab5\u0ac0',\n '\u0ab0\u0acb',\n '\u0aaa\u0ac1\u0ab0\u0ac1\u0ab7\u0acb',\n '\u0a95\u0acb\u0a88 \u0aa6\u0abf\u0ab5\u0ab8']","e21103db":"def remove_stop_words(corpus):\n    results = []\n    for text in corpus:\n        tmp = text.split(' ')\n        for stop_word in stop_words:\n            if stop_word in tmp:\n                tmp.remove(stop_word)\n        results.append(\" \".join(tmp))\n        \n    return results","16bf7725":"corpus = remove_stop_words(corpus)","14f77910":"words = []\nfor text in corpus:\n    for word in text.split(' '):\n        words.append(word)\n        \nwords = set(words)","6a364b04":"words","cf1405f8":"\"\"\"Data Generation\"\"\"\n\nword2int = {}\n\nfor i,word in enumerate(words):\n    word2int[word] = i\n    \nsentences = []\nfor sentence in corpus:\n    sentences.append(sentence.split())\n    \nWINDOW_SIZE = 2\n\ndata = []\nfor sentence in sentences:\n    for idx, word in enumerate(sentence):\n        for neighbor in sentence[max(idx - WINDOW_SIZE, 0): min(idx + WINDOW_SIZE, len(sentence) + 1)]:\n            if neighbor !=word:\n                data.append([word, neighbor])","f3c609ef":"for text in corpus:\n    print(text)\n\ndf = pd.DataFrame(data, columns = ['input', 'label'])","23852de2":"df.head(10)","a15d3ccb":"df.shape","223cf80a":"word2int","bf323646":"import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nx = tf.placeholder(shape=[None, 2], dtype=tf.float32)","5ca621c2":"\"\"\"Define Tensorflow Graph\"\"\"\n\nONE_HOT_DIM = len(words)\n\n# function to convert numbers to one hot vectors\ndef to_one_hot_encoding(data_point_index):\n    one_hot_encoding = np.zeros(ONE_HOT_DIM)\n    one_hot_encoding[data_point_index] = 1\n    return one_hot_encoding\n\nX = [] # input word\nY = [] # target word\n\nfor x, y in zip(df['input'], df['label']):\n    X.append(to_one_hot_encoding(word2int[ x ]))\n    Y.append(to_one_hot_encoding(word2int[ y ]))\n\n# convert them to numpy arrays\nX_train = np.asarray(X)\nY_train = np.asarray(Y)\n\n# making placeholders for X_train and Y_train\nx = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\ny_label = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n\n# word embedding will be 2 dimension for 2d visualization\nEMBEDDING_DIM = 2 \n\n# hidden layer: which represents word vector eventually\nW1 = tf.Variable(tf.random_normal([ONE_HOT_DIM, EMBEDDING_DIM]))\nb1 = tf.Variable(tf.random_normal([1])) #bias\nhidden_layer = tf.add(tf.matmul(x,W1), b1)\n\n# output layer\nW2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, ONE_HOT_DIM]))\nb2 = tf.Variable(tf.random_normal([1]))\nprediction = tf.nn.softmax(tf.add( tf.matmul(hidden_layer, W2), b2))\n\n# loss function: cross entropy\nloss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), axis=[1]))\n\n# training operation\ntrain_op = tf.train.GradientDescentOptimizer(0.05).minimize(loss)","39c5b2ab":"\"\"\"Training\"\"\"\n\nsess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init) \n\niteration = 20000\nfor i in range(iteration):\n    # input is X_train which is one hot encoded word\n    # label is Y_train which is one hot encoded neighbor word\n    sess.run(train_op, feed_dict={x: X_train, y_label: Y_train})\n    if i % 3000 == 0:\n        print('iteration '+str(i)+' loss is : ', sess.run(loss, feed_dict={x: X_train, y_label: Y_train}))","05c832d5":"# Now the hidden layer (W1 + b1) is actually the word look up table\nvectors = sess.run(W1 + b1)\nprint(vectors)","7b1b01a3":"\"\"\"Word Vector in Table\"\"\"\n\nw2v_df = pd.DataFrame(vectors, columns = ['x1', 'x2'])\nw2v_df['word'] = words\nw2v_df = w2v_df[['word', 'x1', 'x2']]\nw2v_df","8d9bbed9":"\"\"\"Word Vector in 2D Chart\"\"\"\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\n\nfor word, x1, x2 in zip(w2v_df['word'], w2v_df['x1'], w2v_df['x2']):\n    ax.annotate(word, (x1,x2 ))\n    \nPADDING = 1.0\nx_axis_min = np.amin(vectors, axis=0)[0] - PADDING\ny_axis_min = np.amin(vectors, axis=0)[1] - PADDING\nx_axis_max = np.amax(vectors, axis=0)[0] + PADDING\ny_axis_max = np.amax(vectors, axis=0)[1] + PADDING\n \nplt.xlim(x_axis_min,x_axis_max)\nplt.ylim(y_axis_min,y_axis_max)\nplt.rcParams[\"figure.figsize\"] = (20,20)\n\nplt.show()","dea8e6bb":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('\u0a97\u0ac1\u0a9c\u0ab0\u0abe\u0aa4\u0ac0 \u0ab2\u0abf\u0aaa\u0abf guj\u01cer\u0101t\u012b lipi Gujarati script, @mpwolke was here' )","e3f9c35f":"#Maybe anybody tell me how to plot charts with the right FONT to read Gujarati.","a3d38b2c":"#Gujar\u0101t\u012b   \u0a97\u0ac1\u0a9c\u0ab0\u0abe\u0aa4\u0ac0 (Gujarati)\n\nGujar\u0101t\u012b is an Indo-Aryan language spoken by about 46 million people in the Indian states of Gujarat, Maharashtra, Rajasthan, Karnataka and Madhya Pradesh, and also in Bangladesh, Fiji, Kenya, Malawi, Mauritius, Oman, Pakistan, R\u00e9union, Singapore, South Africa, Tanzania, Uganda, United Kingdom, USA, Zambia and Zimbabwe.\n\nhttps:\/\/omniglot.com\/writing\/gujarati.htm","4c95c9b9":"#Text by https:\/\/r12a.github.io\/scripts\/gujarati\/","eaea6af2":"#Codes by Anil Govind   https:\/\/www.kaggle.com\/anilreddy8989\/stopwords-word2vector","ad30027b":"![](https:\/\/www.ritiriwaz.com\/wp-content\/uploads\/2020\/05\/Gujarati.jpg)ritiriwaz.com"}}