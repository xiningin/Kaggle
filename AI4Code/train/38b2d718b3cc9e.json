{"cell_type":{"923b2643":"code","7d7e9e84":"code","739a4fd2":"code","80094985":"code","b98ad2e2":"code","b633681d":"code","2528f01a":"code","7133ada1":"code","0938fe41":"code","eb0425e4":"code","ca149bac":"code","4c1e77a7":"code","7ee8335e":"code","bdf068b4":"code","73a53065":"code","1ab4bdca":"code","c3220ba2":"code","d5fd9bac":"code","eb041b23":"code","6b43d3e1":"code","2489ed5f":"code","2f7bcdbd":"code","e87f260c":"code","d1cc5fc0":"code","cc01f975":"code","74702d41":"code","a8f2524e":"code","7bdcfad0":"code","f49327dd":"code","aadfb811":"code","883467a7":"code","c2504286":"code","6012e7ec":"code","8bce26ea":"code","c25016d9":"code","ef7ea5b0":"code","f608dc41":"code","8e053d92":"code","a9bf90a5":"code","45198784":"code","27a832fa":"code","5a127b5d":"markdown","a145db1f":"markdown","71ae4744":"markdown","792dab73":"markdown","24e8deb2":"markdown","cf0fcbe0":"markdown","a5348702":"markdown","d748c54b":"markdown","c90ced4b":"markdown","5e412f6b":"markdown","be481292":"markdown","712671bf":"markdown","a9a3f51f":"markdown","a70e3bec":"markdown","d588c254":"markdown","f34c6609":"markdown"},"source":{"923b2643":"import json\nimport pickle\nimport re\nfrom glob import glob\nimport string\nimport regex\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom wordcloud import STOPWORDS\nimport warnings\nwarnings.simplefilter('ignore')\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.metrics import log_loss\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer \nfrom sklearn.feature_extraction.text import TfidfTransformer\nimport torch\nimport transformers\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom transformers import BertTokenizer, BertForMaskedLM\nfrom transformers import RobertaTokenizer, RobertaModel\nfrom tqdm import tqdm\ntqdm.pandas()\nfrom sklearn.model_selection import train_test_split\nfrom bs4 import BeautifulSoup\nfrom sklearn.metrics.pairwise import cosine_similarity \nfrom sklearn.metrics.pairwise import linear_kernel\nfrom transformers import AutoTokenizer, AutoModel","7d7e9e84":"pd.set_option('display.max_columns', 150)\npd.set_option('display.max_rows', 150)","739a4fd2":"df = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\nprint(df.shape)\ndf_sub = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")","80094985":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","b98ad2e2":"df = reduce_mem_usage(df)\ndf_sub = reduce_mem_usage(df_sub)","b633681d":"print(df.shape)\nprint(df_sub.shape)","2528f01a":"df['severe_toxic'] = df.severe_toxic * 1.5\ndf['toxic'] = df.toxic * 0.32\ndf['obscene'] = df.obscene * 0.16\ndf['threat'] = df.threat * 1.5\ndf['insult'] = df.insult * 0.64\ndf['identity_hate'] = df.identity_hate * 1.5\ndf['y'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) ).astype(int)\ndf['y'] = df['y']\/df['y'].max()\n\ndf = df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\ndf.sample(5)","7133ada1":"# df['severe_toxic'] = df.severe_toxic * 2.5\n# # df['toxic'] = df.toxic * 0.32\n# # df['obscene'] = df.obscene * 0.16\n# df['threat'] = df.threat * 3\n# # df['insult'] = df.insult * 0.64\n# df['identity_hate'] = df.identity_hate * 2\n# df['y'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) ).astype(int)\n# df['y'] = df['y']\/df['y'].max()\n\n# df = df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\n# df.sample(5)","0938fe41":"df['y'].value_counts()","eb0425e4":"min_len = (df['y'] >= 0.1).sum()\ndf_y0_undersample = df[df['y'] == 0].sample(n=min_len, random_state=201)\ndf = pd.concat([df[df['y'] >= 0.1], df_y0_undersample])\ndf['y'].value_counts()","ca149bac":"total_data = pd.concat([df, df_sub],ignore_index=True)","4c1e77a7":"display(total_data.head(2))\ntotal_data_idx = df.shape[0]","7ee8335e":"print(total_data.shape)\nprint(df_sub.shape)\nprint(total_data_idx)","bdf068b4":"def text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?:\/\/\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text\n","73a53065":"tqdm.pandas()\ntotal_data['text'] = total_data['text'].progress_apply(text_cleaning)","1ab4bdca":"# # word_count\n# total_data['word_count'] = total_data['text'].apply(lambda x: len(str(x).split()))\n\n# # unique_word_count\n# total_data['unique_word_count'] = total_data['text'].apply(lambda x: len(set(str(x).split())))\n\n# # stop_word_count\n# total_data['stop_word_count'] = total_data['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n# # mean_word_length\n# total_data['mean_word_length'] = total_data['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\n# # char_count\n# total_data['char_count'] = total_data['text'].apply(lambda x: len(str(x)))\n\n# # punctuation_count\n# total_data['punctuation_count'] = total_data['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))","c3220ba2":"# total_data_1 = total_data[total_data['y'] > 0.2]\n# total_data_1 = total_data_1.reset_index()","d5fd9bac":"# vectorizer = CountVectorizer()\n# transformer = TfidfTransformer()\n# tf_title = vectorizer.fit_transform(total_data_1['text'])\n# tfid_title = transformer.fit_transform(tf_title)\n# tfid_title.shape","eb041b23":"# similarity_values=[]\n# for i in range(0,len(total_data['text'])):\n#     title_term = total_data['text'][i]\n#     title_tf = vectorizer.transform([title_term])\n#     title_tfidf = transformer.transform(title_tf)\n#     similarity_title = cosine_similarity(title_tfidf, tfid_title)[0]\n#     topn_indices = np.argsort(similarity_title)[::-1][:1]\n#     value = similarity_title[topn_indices]\n#     similarity_values.append(value)","6b43d3e1":"# total_data['text_sim'] = similarity_values\n# total_data['text_sim'] = total_data['text_sim'].astype(float)","2489ed5f":"# total_data[\"num_text\"] = total_data[\"text\"].apply(lambda x: len(x))","2f7bcdbd":"total_data['Fuck'] = total_data['text'].str.contains('Fuck')\ntotal_data.replace({'Fuck':{False:0,True:1}},inplace = True)\nprint(total_data['Fuck'].value_counts())\n\ntotal_data['Nigger'] = total_data['text'].str.contains('Nigger')\ntotal_data.replace({'Nigger':{False:0,True:1}},inplace = True)\nprint(total_data['Nigger'].value_counts())\n\ntotal_data['Suck'] = total_data['text'].str.contains('Suck')\ntotal_data.replace({'Suck':{False:0,True:1}},inplace = True)\nprint(total_data['Suck'].value_counts())\n\ntotal_data['Don'] = total_data['text'].str.contains('Don')\ntotal_data.replace({'Don':{False:0,True:1}},inplace = True)\nprint(total_data['Don'].value_counts())\n\ntotal_data['Shit'] = total_data['text'].str.contains('Shit')\ntotal_data.replace({'Shit':{False:0,True:1}},inplace = True)\nprint(total_data['Shit'].value_counts())\n\ntotal_data['Fat'] = total_data['text'].str.contains('Fat')\ntotal_data.replace({'Fat':{False:0,True:1}},inplace = True)\nprint(total_data['Fat'].value_counts())\n\ntotal_data['Gay'] = total_data['text'].str.contains('Gay')\ntotal_data.replace({'Gay':{False:0,True:1}},inplace = True)\nprint(total_data['Gay'].value_counts())\n\ntotal_data['Faggot'] = total_data['text'].str.contains('Faggot')\ntotal_data.replace({'Faggot':{False:0,True:1}},inplace = True)\nprint(total_data['Faggot'].value_counts())\n\ntotal_data['Moron'] = total_data['text'].str.contains('Moron')\ntotal_data.replace({'Moron':{False:0,True:1}},inplace = True)\nprint(total_data['Moron'].value_counts())\n\ntotal_data['Ass'] = total_data['text'].str.contains('Ass')\ntotal_data.replace({'Ass':{False:0,True:1}},inplace = True)\nprint(total_data['Ass'].value_counts())\n\ntotal_data['Sucks'] = total_data['text'].str.contains('Sucks')\ntotal_data.replace({'Sucks':{False:0,True:1}},inplace = True)\nprint(total_data['Sucks'].value_counts())\n\ntotal_data['Jew'] = total_data['text'].str.contains('Jew')\ntotal_data.replace({'Jew':{False:0,True:1}},inplace = True)\nprint(total_data['Jew'].value_counts())\n\ntotal_data['Pig'] = total_data['text'].str.contains('Pig')\ntotal_data.replace({'Pig':{False:0,True:1}},inplace = True)\nprint(total_data['Pig'].value_counts())\n\ntotal_data['Stupid'] = total_data['text'].str.contains('Stupid')\ntotal_data.replace({'Stupid':{False:0,True:1}},inplace = True)\nprint(total_data['Stupid'].value_counts())\n\ntotal_data['Die'] = total_data['text'].str.contains('Die')\ntotal_data.replace({'Die':{False:0,True:1}},inplace = True)\nprint(total_data['Die'].value_counts())","e87f260c":"total_data.head(2)","d1cc5fc0":"class BertSequenceVectorizer:\n    def __init__(self):\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.model_name = \"..\/input\/roberta-base\"\n#         self.model_name = \"..\/input\/distil-roberta-base\"\n#         self.tokenizer = RobertaTokenizer.from_pretrained(self.model_name)\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n     \n#         self.bert_model = transformers.RobertaModel.from_pretrained(self.model_name)  \n        self.bert_model = transformers.AutoModel.from_pretrained(self.model_name)      \n        self.bert_model = self.bert_model.to(self.device)\n        self.max_len = 128\n#         self.max_len = 256\n        \n    def vectorize(self, sentence : str) -> np.array:\n        inp = self.tokenizer.encode(sentence)\n        len_inp = len(inp)\n\n        if len_inp >= self.max_len:\n            inputs = inp[:self.max_len]\n            masks = [1] * self.max_len\n        else:\n            inputs = inp + [0] * (self.max_len - len_inp)\n            masks = [1] * len_inp + [0] * (self.max_len - len_inp)\n\n        inputs_tensor = torch.tensor([inputs], dtype=torch.long).to(self.device)\n        masks_tensor = torch.tensor([masks], dtype=torch.long).to(self.device)\n        \n        seq_out = self.bert_model(inputs_tensor, masks_tensor)[0]\n        pooled_out = self.bert_model(inputs_tensor, masks_tensor)[1]\n\n        if torch.cuda.is_available():    \n            return seq_out[0][0].cpu().detach().numpy() \n        else:\n            return seq_out[0][0].detach().numpy()","cc01f975":"BSV = BertSequenceVectorizer()\ntotal_data['text'] = total_data['text'].progress_apply(lambda x: BSV.vectorize(x) if x is not np.nan else np.array([0]*768))","74702d41":"bert = pd.DataFrame(total_data['text'].tolist())\nbert.columns = ['text_bertvec_'+str(col) for col in bert.columns]","a8f2524e":"text_bert_df = pd.DataFrame(bert)\ntext_bert_df.head()","7bdcfad0":"total_data.reset_index(drop=True, inplace=True)\ntotal_data.head()","f49327dd":"concat_df = pd.concat([total_data, text_bert_df], axis=1)\nconcat_df.shape","aadfb811":"concat_df.head(2)","883467a7":"# cat_cols = ['num_text','word_count','unique_word_count','stop_word_count','mean_word_length','char_count',\n#             'punctuation_count','Fuck', 'Nigger', 'Suck', 'Don', 'Shit', 'Fat',\n#             'Gay', 'Faggot', 'Moron', 'Ass','Sucks','Jew','Pig','Stupid','Die']\n# num_cols = list(list(text_bert_df.columns)) \n# feat_cols = cat_cols + num_cols\n# TARGET = 'y'","c2504286":"cat_cols = ['Fuck', 'Nigger', 'Suck', 'Don', 'Shit', 'Fat',\n            'Gay', 'Faggot', 'Moron', 'Ass','Sucks','Jew','Pig','Stupid','Die']\nnum_cols = list(list(text_bert_df.columns)) \nfeat_cols = cat_cols + num_cols\nTARGET = 'y'","6012e7ec":"train_df = concat_df.iloc[:11000, :]\nval_df = concat_df.iloc[11000:total_data_idx, :]\ntest_df = concat_df.iloc[total_data_idx:, :]\nprint(train_df.shape, val_df.shape, test_df.shape)","8bce26ea":"train_x = train_df[feat_cols]\ntrain_y = train_df[TARGET]\nval_x = val_df[feat_cols]\nval_y = val_df[TARGET]\ntest_x = test_df[feat_cols]\ntest_y = test_df[TARGET]","c25016d9":"print(train_x.shape)\nprint(train_y.shape)\nprint(val_x.shape)\nprint(val_y.shape)","ef7ea5b0":"params = {   \n    'objective': 'regression',\n#     'objective': 'regression_l1',\n    'metric': 'rmse',\n    'num_leaves': 32,\n    'max_depth': 7,\n    \"feature_fraction\": 0.8,\n    'subsample_freq': 1,\n    \"bagging_fraction\": 0.98,\n    'min_data_in_leaf': 2,\n    'learning_rate': 0.05,\n    \"boosting\": \"gbdt\",\n    \"lambda_l1\": 0.2,\n    \"lambda_l2\": 10,\n    \"verbosity\": -1,\n    \"random_state\": 42,\n    \"num_boost_round\": 8000,\n    \"early_stopping_rounds\": 100\n}\n\ntrain_data = lgb.Dataset(train_x, label=train_y)\nval_data = lgb.Dataset(val_x, label=val_y)\n\nmodel = lgb.train(\n    params,\n    train_data, \n    categorical_feature = cat_cols,\n    valid_names = ['train', 'valid'],\n    valid_sets =[train_data, val_data], \n    verbose_eval = 100,\n)\n\nval_pred = model.predict(val_x, num_iteration=model.best_iteration)\n\npred_df = pd.DataFrame(sorted(zip(val_x.index, val_pred, val_y)), columns=['index', 'predict', 'actual'])","f608dc41":"# lgb.plot_importance(model, figsize=(12,8), max_num_features=70, importance_type='gain')\n# plt.tight_layout()\n# plt.show()","8e053d92":"test_pred = model.predict(test_x, num_iteration=model.best_iteration)","a9bf90a5":"# sub_df.iloc[:, 1:] = test_pred\ndf_sub['score'] = test_pred","45198784":"df_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)  ","27a832fa":"df_sub","5a127b5d":"<h3>BERT<\/h3>","a145db1f":"<h3>Word count<\/h3>","71ae4744":"<h3>Undersampling for train data<\/h3>","792dab73":"<h3>Read Training data<\/h3>","24e8deb2":"# Imports","cf0fcbe0":"<h3>Reduce memory usage<\/h3>","a5348702":"<h3>Prediction<\/h3>","d748c54b":"<h3>Cos similarity<\/h3>","c90ced4b":"<h3>Text cleaning<\/h3>","5e412f6b":"<h3>Flag whether to include toxic words<\/h3>","be481292":"<h3>Prepare train and test data<\/h3>","712671bf":"<h3>Prepare train and test data<\/h3>","a9a3f51f":"<h3>Merge train and df_sub to prepare total_data<\/h3>","a70e3bec":"<h3>Number of words for text<\/h3>","d588c254":"<h3>LightGBM<\/h3>","f34c6609":"### LightGBM_BERT_Simple_Baseline[0.765]"}}