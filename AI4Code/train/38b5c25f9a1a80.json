{"cell_type":{"d428f7e3":"code","de2dff6d":"code","5aeaae4e":"code","fc3cbb2a":"code","79f02334":"code","29583dd1":"code","4660f012":"code","7f876aab":"code","762a8dd7":"code","c85817c1":"code","b8f2244d":"code","f333099a":"code","3564cb57":"code","29507bf2":"code","667a68cb":"code","6e3c391a":"code","efc9248e":"code","aa474e27":"code","20ff2ca3":"code","67f14105":"markdown","775045ce":"markdown","87c5fbc3":"markdown","4b25cb31":"markdown","dd8b7eaf":"markdown","6bd12bac":"markdown","6781c046":"markdown","7eb885e0":"markdown","885e2f31":"markdown","eb6c4ca5":"markdown","8507f941":"markdown","906faf48":"markdown","1b97ba6f":"markdown","ac45cf92":"markdown"},"source":{"d428f7e3":"conda env create -f environments.yml","de2dff6d":"conda env create -f environments-gpu.yml","5aeaae4e":"# http:\/\/pytorch.org\/\nfrom os.path import exists\nfrom wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\nplatform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\ncuda_output = !ldconfig -p|grep cudart.so|sed -e 's\/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$\/cu\\1\\2\/'\naccelerator = cuda_output[0] if exists('\/dev\/nvidia0') else 'cpu'","fc3cbb2a":"# !pip install -q http:\/\/download.pytorch.org\/whl\/{accelerator}\/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n!wget https:\/\/d17h27t6h515a5.cloudfront.net\/topher\/2016\/December\/584f6edd_data\/data.zip","79f02334":"!unzip data.zip","29583dd1":"conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch","4660f012":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils import data\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\nimport cv2\nimport numpy as np\nimport csv","7f876aab":"# Step1: Read from the log file\nsamples = []\nwith open('data\/driving_log.csv') as csvfile:\n    reader = csv.reader(csvfile)\n    next(reader, None)\n    for line in reader:\n        samples.append(line)","762a8dd7":"train_len = int(0.8*len(samples))\nvalid_len = len(samples) - train_len\ntrain_samples, validation_samples = data.random_split(samples, lengths=[train_len, valid_len])","c85817c1":"# Step3a: Define the augmentation, transformation processes, parameters and dataset for dataloader\ndef augment(imgName, angle):\n  name = 'data\/IMG\/' + imgName.split('\/')[-1]\n  current_image = cv2.imread(name)\n  current_image = current_image[65:-25, :, :]\n  if np.random.rand() < 0.5:\n    current_image = cv2.flip(current_image, 1)\n    angle = angle * -1.0  \n  return current_image, angle","b8f2244d":"class Dataset(data.Dataset):\n\n    def __init__(self, samples, transform=None):\n        self.samples = samples\n        self.transform = transform\n\n    def __getitem__(self, index):\n        \n        batch_samples = self.samples[index]\n        steering_angle = float(batch_samples[3])\n        center_img, steering_angle_center = augment(batch_samples[0], steering_angle)\n        left_img, steering_angle_left = augment(batch_samples[1], steering_angle + 0.4)\n        right_img, steering_angle_right = augment(batch_samples[2], steering_angle - 0.4)\n        center_img = self.transform(center_img)\n        left_img = self.transform(left_img)\n        right_img = self.transform(right_img)\n        return (center_img, steering_angle_center), (left_img, steering_angle_left), (right_img, steering_angle_right)\n      \n    def __len__(self):\n        return len(self.samples)","f333099a":"# Step3b: Creating generator using the dataloader to parallasize the process\ntransformations = transforms.Compose([transforms.Lambda(lambda x: (x \/ 255.0) - 0.5)])\n\nparams = {'batch_size': 32,\n          'shuffle': True,\n          'num_workers': 4}\n\ntraining_set = Dataset(train_samples, transformations)\ntraining_generator = DataLoader(training_set, **params)\n\nvalidation_set = Dataset(validation_samples, transformations)\nvalidation_generator = DataLoader(validation_set, **params)","3564cb57":"# Step4: Define the network\nclass NetworkDense(nn.Module):\n\n    def __init__(self):\n        super(NetworkDense, self).__init__()\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(3, 24, 5, stride=2),\n            nn.ELU(),\n            nn.Conv2d(24, 36, 5, stride=2),\n            nn.ELU(),\n            nn.Conv2d(36, 48, 5, stride=2),\n            nn.ELU(),\n            nn.Conv2d(48, 64, 3),\n            nn.ELU(),\n            nn.Conv2d(64, 64, 3),\n            nn.Dropout(0.25)\n        )\n        self.linear_layers = nn.Sequential(\n            nn.Linear(in_features=64 * 2 * 33, out_features=100),\n            nn.ELU(),\n            nn.Linear(in_features=100, out_features=50),\n            nn.ELU(),\n            nn.Linear(in_features=50, out_features=10),\n            nn.Linear(in_features=10, out_features=1)\n        )\n        \n    def forward(self, input):  \n        input = input.view(input.size(0), 3, 70, 320)\n        output = self.conv_layers(input)\n        print(output.shape)\n        output = output.view(output.size(0), -1)\n        output = self.linear_layers(output)\n        return output","29507bf2":"class NetworkLight(nn.Module):\n\n    def __init__(self):\n        super(NetworkLight, self).__init__()\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(3, 24, 3, stride=2),\n            nn.ELU(),\n            nn.Conv2d(24, 48, 3, stride=2),\n            nn.MaxPool2d(4, stride=4),\n            nn.Dropout(p=0.25)\n        )\n        self.linear_layers = nn.Sequential(\n            nn.Linear(in_features=48*4*19, out_features=50),\n            nn.ELU(),\n            nn.Linear(in_features=50, out_features=10),\n            nn.Linear(in_features=10, out_features=1)\n        )\n        \n\n    def forward(self, input):\n        input = input.view(input.size(0), 3, 70, 320)\n        output = self.conv_layers(input)\n        print(output.shape)\n        output = output.view(output.size(0), -1)\n        output = self.linear_layers(output)\n        return output","667a68cb":"# Step5: Define optimizer\nmodel = NetworkLight()\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\ncriterion = nn.MSELoss()","6e3c391a":"# Step6: Check the device and define function to move tensors to that device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \nprint('device is: ', device)\n\ndef toDevice(datas, device):\n  \n  imgs, angles = datas\n  return imgs.float().to(device), angles.float().to(device)","efc9248e":"max_epochs = 22\nfor epoch in range(max_epochs):\n    model.to(device)\n    \n    # Training\n    train_loss = 0\n    model.train()\n    for local_batch, (centers, lefts, rights) in enumerate(training_generator):\n        # Transfer to GPU\n        centers, lefts, rights = toDevice(centers, device), toDevice(lefts, device), toDevice(rights, device)\n        \n        # Model computations\n        optimizer.zero_grad()\n        datas = [centers, lefts, rights]        \n        for data in datas:\n            imgs, angles = data\n#             print(\"training image: \", imgs.shape)\n            outputs = model(imgs)\n            loss = criterion(outputs, angles.unsqueeze(1))\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.data.item()\n            \n        if local_batch % 100 == 0:\n            print('Loss: %.3f '\n                % (train_loss\/(local_batch+1)))","aa474e27":"    model.eval()\n    valid_loss = 0\n    with torch.set_grad_enabled(False):\n        for local_batch, (centers, lefts, rights) in enumerate(validation_generator):\n            # Transfer to GPU\n            centers, lefts, rights = toDevice(centers, device), toDevice(lefts, device), toDevice(rights, device)\n        \n            # Model computations\n            optimizer.zero_grad()\n            datas = [centers, lefts, rights]        \n            for data in datas:\n                imgs, angles = data\n#                 print(\"Validation image: \", imgs.shape)\n                outputs = model(imgs)\n                loss = criterion(outputs, angles.unsqueeze(1))\n                \n                valid_loss += loss.data.item()\n\n            if local_batch % 100 == 0:\n                print('Valid Loss: %.3f '\n                    % (valid_loss\/(local_batch+1)))","20ff2ca3":"# Step8: Define state and save the model wrt to state\nstate = {\n        'model': model.module if device == 'cuda' else model,\n        }\n\ntorch.save(state, 'model.h5')","67f14105":"### elu activation:\nThe activation function used here is elu (exponential linear unit). Unlike relu (rectified linear unit), elu speeds up the training process and also solves the vanishing gradient problem.","775045ce":"#  <span style=\"color: red;\"> Using Nvidia\u2019s research to build a CNN for autonomous driving in Pytorch <\/span>\n\n![Automatic_Car-compressed.jpg](attachment:Automatic_Car-compressed.jpg)\n","87c5fbc3":"20\u201330% of training data as the validation set to compare the validation loss and training loss so that we can avoid overfitting.So we are dividing it in 20:80","4b25cb31":"# Validation","dd8b7eaf":"# Model Architecture\n\n![image.png](attachment:image.png)","6bd12bac":"Now we define the Dataloader class and pass on this augment function to the input batch samples, concatenate the steering data and images and return it.","6781c046":"# Reading and splitting the data","7eb885e0":"###  Reference :\n1. https:\/\/towardsdatascience.com\/deep-learning-for-self-driving-cars-7f198ef4cfa2\n2. https:\/\/github.com\/ManajitPal\/DeepLearningForSelfDrivingCars","885e2f31":"# Training","eb6c4ca5":"### Loading Images in Dataloader\nNow that we have made the samples, it is time to read the images and augment them. This is an important step as this will help to generalize our model. But the process is computationally heavy and time-consuming even for GPUs. The trick is to parallelize this process by taking data in batches, augmenting them and sending to the model to train. Keras achieves this process using python generators and the fit_generator function. In Pytorch, we shall use the Dataset class and the Dataloader function to achieve this.","8507f941":"# Import Libraries","906faf48":"## Saving the Model","1b97ba6f":"# Optimizer and Criterion","ac45cf92":"![image.png](attachment:image.png)"}}