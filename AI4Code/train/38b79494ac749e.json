{"cell_type":{"0fd0cdef":"code","348ec3c0":"code","c71e02ef":"code","937cf24f":"code","d35ac9c5":"code","a83b00f7":"code","4060d6b5":"code","c1249b9f":"code","5b4aecab":"code","97e62a5f":"code","783ac5da":"code","34a36f15":"code","33db32fc":"code","9ba38caf":"code","a122c5cb":"code","6c112760":"code","3d9df812":"code","4d15ed34":"code","f450281b":"code","e902569a":"code","92ccfbf5":"code","26665b14":"code","a0bc50b3":"code","56a67cb7":"code","4fc4f86a":"code","2a918953":"markdown","4497758b":"markdown","ecd11430":"markdown","749d3b32":"markdown","c6b86591":"markdown","770cf6af":"markdown","36c78197":"markdown","77c3eb3f":"markdown","eeace8d9":"markdown","95865d37":"markdown","279b24d9":"markdown","a64749db":"markdown","376e1665":"markdown","b9e5c112":"markdown","e6356bb1":"markdown","7026b1ba":"markdown","ea0e6ca4":"markdown","8805aac9":"markdown","44a9979f":"markdown","26526040":"markdown","7ece1e9b":"markdown","1d6d318f":"markdown","7f294cb5":"markdown","e3c5798e":"markdown","fc0e9c07":"markdown","672b1973":"markdown","0deaa73e":"markdown","12a33e0a":"markdown","2be1ab6d":"markdown","7e3ef14d":"markdown","c7781743":"markdown","9e502a58":"markdown","d8ba37e3":"markdown","3411919f":"markdown","2d0986b5":"markdown","d018c732":"markdown","13610613":"markdown","b67a85b0":"markdown","d0ec7f1c":"markdown","81a5a7eb":"markdown","cc7c799f":"markdown","c7b07fa8":"markdown","6a858606":"markdown","6ee60d70":"markdown","d0a6e5fd":"markdown"},"source":{"0fd0cdef":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","348ec3c0":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import set_config","c71e02ef":"set_config(display='diagram')","937cf24f":"SEED = 42\nRANGE = (-5, 5)\nN_SAMPLES = 50\nDEGREES = np.linspace(0, 15, 1 + 15, dtype=int)\nALPHAS = np.linspace(0, 0.5, 1 + 40)","d35ac9c5":"def target_function(x):\n    return 2 * x + 10 * np.sin(x)\n\ndef generate_samples():\n    \"\"\"Generate noisy samples.\"\"\"\n    np.random.seed(SEED)\n    x = np.random.uniform(*RANGE, size=N_SAMPLES)\n    y = target_function(x) + np.random.normal(scale=4, size=N_SAMPLES)\n    return x.reshape(-1, 1), y\n\nX, y = generate_samples()","a83b00f7":"def plot_scatter(x, y, title=None, label='Noisy samples'):\n    plt.scatter(x, y, label=label)\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.grid(True)\n    plt.title(title)\n    plt.legend(loc='lower right')\n\nplot_scatter(X, y)","4060d6b5":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=SEED)\n\nplot_scatter(X_train, y_train, label='Training set')\nplot_scatter(X_valid, y_valid, label='Validation set')","c1249b9f":"PolynomialFeatures(degree=4).fit_transform(X=[\n    [1],\n    [3],\n    [4],\n])","5b4aecab":"def make_model(degree, alpha=0, penalty=None):\n    # linear regression\n    if alpha == 0:\n        regressor = LinearRegression(fit_intercept=False)\n    \n    # lasso regression\n    elif penalty == 'L1':\n        regressor = Lasso(fit_intercept=False, alpha=alpha, random_state=SEED, max_iter=50000)        \n    \n    # ridge regression\n    elif penalty == 'L2':\n        regressor = Ridge(fit_intercept=False, alpha=alpha, random_state=SEED, max_iter=50000)        \n    \n    return Pipeline([\n        ('pol', PolynomialFeatures(degree)),\n        ('sca', StandardScaler()),\n        ('reg', regressor)\n    ])\n\ndisplay(make_model(2))\ndisplay(make_model(2, penalty='L1', alpha=0.1))\ndisplay(make_model(2, penalty='L2', alpha=0.1))","97e62a5f":"def plot_fit(model):\n    degree = model['pol'].degree\n    X_range = np.linspace(*RANGE, 1000).reshape(-1, 1)\n    y_pred = model.predict(X_range)\n    plot_scatter(X_train, y_train, label='Training sample')\n    plot_scatter(X_valid, y_valid, label='Validation sample')\n    plt.plot(X_range, target_function(X_range), c='green', alpha=0.2, lw=5, label='Target function')\n    plt.plot(X_range, y_pred, c='red', label='Hypothesis')\n    plt.ylim((min(y) - 3, max(y) + 3))\n    plt.legend(loc='best')    \n    plt.title(f'Polynomial approximation: degree={degree}')\n    plt.show()\n\nplot_fit(make_model(degree=2).fit(X_train, y_train))","783ac5da":"for degree in [0, 1, 2, 3, 4, 5, 10, 15, 20]:\n    plot_fit(make_model(degree).fit(X_train, y_train))","34a36f15":"def rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\ndef plot_fitting_graph(x, metric_train, metric_valid, xlabel, ylabel, \n                       custom_metric=None, custom_label='', custom_scale='log', title='Fitting graph'):\n    plt.figure(figsize=(9, 4.5))\n    plt.plot(x, metric_train, label='Training')\n    plt.plot(x, metric_valid, color='C1', label='Validation')\n    plt.axvline(x[np.argmin(metric_valid)], color='C1', lw=10, alpha=0.2)\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.grid(True)\n    plt.xticks(x, rotation='vertical')\n    plt.legend(loc='center left')        \n    if custom_metric:\n        plt.twinx()\n        plt.yscale(custom_scale)\n        plt.plot(x, custom_metric, alpha=0.2, lw=4, ls='dotted', color='black', label=custom_label) \n        plt.legend(loc='center right')         \n    plt.show()\n    \nrmse_train, rmse_valid = [], []\nfor degree in DEGREES:\n    reg = make_model(degree).fit(X_train, y_train)\n    rmse_train.append(rmse(reg.predict(X_train), y_train))\n    rmse_valid.append(rmse(reg.predict(X_valid), y_valid))\n    \nplot_fitting_graph(DEGREES, rmse_train, rmse_valid, xlabel='Complexity (degree)', ylabel='Error (RMSE)', \n                   title='Least squares polynomial regression')","33db32fc":"DEGREES[np.argmin(rmse_valid)]","9ba38caf":"rmse_train, rmse_valid = [], []\nfor degree in DEGREES:\n    results = cross_validate(make_model(degree), \n                             X, y, cv=5,\n                             return_train_score=True,\n                             scoring='neg_root_mean_squared_error')\n    rmse_train.append(-np.mean(results['train_score']))\n    rmse_valid.append(-np.mean(results['test_score']))\n    \nplot_fitting_graph(DEGREES, rmse_train, rmse_valid, xlabel='Complexity (degree)', ylabel='Error (RMSE)',\n                   title='Least squares polynomial regression')","a122c5cb":"(make_model(degree=1).fit(X_train, y_train)['reg'].coef_,\n make_model(degree=2).fit(X_train, y_train)['reg'].coef_,\n make_model(degree=5).fit(X_train, y_train)['reg'].coef_,\n make_model(degree=10).fit(X_train, y_train)['reg'].coef_)","6c112760":"rmse_train, rmse_valid, avg_coef = [], [], []\nfor degree in DEGREES:\n    results = cross_validate(make_model(degree),\n                             X, y, cv=5,\n                             return_train_score=True, return_estimator=True,\n                             scoring='neg_root_mean_squared_error')\n    rmse_train.append(-np.mean(results['train_score']))\n    rmse_valid.append(-np.mean(results['test_score']))        \n    avg_coef.append(        \n        # average over CV folds\n        np.mean([            \n            # mean absolute value of weights\n            np.mean(np.abs(model['reg'].coef_))\n            for model in results['estimator']\n        ]))\n    \nplot_fitting_graph(DEGREES, rmse_train, rmse_valid,\n                   xlabel='Complexity (degree)', ylabel='Error (RMSE)',\n                   custom_metric=avg_coef, custom_label='avg(|$w_i$|)',\n                   title='Least squares polynomial regression')","3d9df812":"rmse_train, rmse_valid = [], []\nfor alpha in ALPHAS:    \n    results = cross_validate(make_model(degree=15, penalty='L1', alpha=alpha), \n                             X, y, cv=5,\n                             return_train_score=True,\n                             scoring='neg_root_mean_squared_error')\n    rmse_train.append(-np.mean(results['train_score']))\n    rmse_valid.append(-np.mean(results['test_score']))\n    \nplot_fitting_graph(ALPHAS, rmse_train, rmse_valid,\n                   xlabel='Regularization strength (alpha)', ylabel='Error (RMSE)',\n                   title='Lasso polynomial regression (L1): degree=15')","4d15ed34":"rmse_train, rmse_valid = [], []\nfor alpha in ALPHAS:    \n    results = cross_validate(make_model(degree=15, penalty='L2', alpha=alpha), \n                             X, y, cv=5,\n                             return_train_score=True,\n                             scoring='neg_root_mean_squared_error')\n    rmse_train.append(-np.mean(results['train_score']))\n    rmse_valid.append(-np.mean(results['test_score']))\n    \nplot_fitting_graph(ALPHAS, rmse_train, rmse_valid, \n                   xlabel='Regularization strength (alpha)', ylabel='Error (RMSE)', \n                   title='Ridge polynomial regression (L2): degree=15')","f450281b":"rmse_train, rmse_valid, avg_coef = [], [], []\nfor alpha in ALPHAS:    \n    results = cross_validate(make_model(degree=15, penalty='L1', alpha=alpha), \n                             X, y, cv=5,\n                             return_train_score=True,return_estimator=True,\n                             scoring='neg_root_mean_squared_error')\n    rmse_train.append(-np.mean(results['train_score']))\n    rmse_valid.append(-np.mean(results['test_score'])) \n    avg_coef.append(        \n        np.mean([            \n            np.mean(np.abs(model['reg'].coef_))\n            for model in results['estimator']\n        ]))\n    \nplot_fitting_graph(ALPHAS, rmse_train, rmse_valid,\n                   xlabel='Regularization strength (alpha)', ylabel='Error (RMSE)',\n                   custom_metric=avg_coef, custom_label='avg(|$w_i$|)',\n                   title='Lasso polynomial regression (L1): degree=15')","e902569a":"rmse_train, rmse_valid, avg_coef = [], [], []\nfor alpha in ALPHAS:    \n    results = cross_validate(make_model(degree=15, penalty='L2', alpha=alpha), \n                             X, y, cv=5,\n                             return_train_score=True,return_estimator=True,\n                             scoring='neg_root_mean_squared_error')\n    rmse_train.append(-np.mean(results['train_score']))\n    rmse_valid.append(-np.mean(results['test_score'])) \n    avg_coef.append(        \n        np.mean([            \n            np.mean(np.abs(model['reg'].coef_))\n            for model in results['estimator']\n        ]))\n    \nplot_fitting_graph(ALPHAS, rmse_train, rmse_valid,\n                   xlabel='Regularization strength (alpha)', ylabel='Error (RMSE)',\n                   custom_metric=avg_coef, custom_label='avg(|$w_i$|)',\n                   title='Lasso polynomial regression (L2): degree=15')","92ccfbf5":"## your observations\/conclusions\n\u0421\u0440\u0435\u0434\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f L1 \u0438 L2 \u043e\u0447\u0435\u043d\u044c \u0431\u043b\u0438\u0437\u043a\u0438 \u0438\u043f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u043d\u0435 \u043e\u0442\u043b\u0438\u0447\u0430\u044e\u0442\u0441\u044f. ","26665b14":"## your code\n\nrmse_train, rmse_valid, avg_coef, alp = [], [], [], []\nq=0\nfor alpha in ALPHAS:    \n    results = cross_validate(make_model(degree=15, penalty='L1', alpha=alpha), \n        X, y,cv=5,\n        return_train_score=True,return_estimator=True,\n        scoring='neg_root_mean_squared_error')\n    alp.append([[]])\n    for i in range(5):\n        alp[q].append([])\n        for e in range(16):\n            alp[q][i].append(alpha)\n    q+=1\n    avg_coef.append([(np.abs(model['reg'].coef_))\n        for model in results['estimator']])\nalpl=[]\ncoefl=[]\nfor q in range(41):\n    for e in range(5):\n        for w in range(16):\n            if avg_coef[q][e][w]>50:\n                avg_coef[q][e][w]=50\n            if alp[q][e][w]>50:\n                alp[q][e][w]=50\n            coefl.append(avg_coef[q][e][w])\n            alpl.append(alp[q][e][w])\nplot_scatter(alpl,coefl, label='Training')","a0bc50b3":"## your code\nrmse_train, rmse_valid, avg_coef, alp = [], [], [], []\nq=0\nfor alpha in ALPHAS:    \n    results = cross_validate(make_model(degree=15, penalty='L2', alpha=alpha), \n        X, y,cv=5,\n        return_train_score=True,return_estimator=True, scoring='neg_root_mean_squared_error')\n    alp.append([[]])\n    for i in range(5):\n        alp[q].append([])\n        for e in range(16):\n            alp[q][i].append(alpha)\n    q+=1\n    avg_coef.append([(np.abs(model['reg'].coef_))\n        for model in results['estimator']])\nalpl=[]\ncoefl=[]\nfor q in range(41):\n    for e in range(5):\n        for w in range(16):\n            if avg_coef[q][e][w]>50:\n                avg_coef[q][e][w]=50\n            if alp[q][e][w]>50:\n                alp[q][e][w]=50\n            coefl.append(avg_coef[q][e][w])\n            alpl.append(alp[q][e][w])\nplot_scatter(alpl,coefl, label='Training')","56a67cb7":"## your findings\/conclusions\n\n\u0412\u0438\u0434\u043d\u043e,\u0447\u0442\u043e L1 \u0437\u0430\u043d\u0443\u043b\u044f\u0435\u0442 \u043a\u043e\u044d\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u044b, \u0430 L2 \u0443\u043c\u0435\u043d\u044c\u0448\u0430\u0435\u0442 \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u044b \u043f\u043e \u043e\u0442\u043d\u043e\u0449\u0435\u043d\u0438\u044e \u043a \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c, \u0431\u043b\u0438\u0437\u043a\u0438\u043c \u043a \u043d\u0443\u043b\u044e, \u0438 \u0441\u0430\u043c\u044b\u0435 \u0431\u043e\u043b\u044c\u0448\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0443 \u0432\u0435\u0441\u0430 1 \u0438 3","4fc4f86a":"## your thoughts\n1) \u041c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0438\u0433\u0440\u0430\u0435\u0442 \u0432\u0430\u0436\u043d\u0443\u044e \u0440\u043e\u043b\u044c \u0432 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430\u0445, \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u0440\u0430\u0441\u0441\u0447\u0435\u0442\u0435 \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u0439. \u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, k-\u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0438\u0435 \u0441\u043e\u0441\u0435\u0434\u0438 (\u0415\u0432\u043a\u043b\u0438\u0434\u043e\u0432\u0430 \u043c\u0435\u0440\u0430 \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u044f) \u0447\u0443\u0432\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u044b \u043a \u0432\u0435\u043b\u0438\u0447\u0438\u043d\u0430\u043c, \n\u0438, \u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e, \u0434\u043e\u043b\u0436\u043d\u044b \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u0442\u044c\u0441\u044f, \u0447\u0442\u043e\u0431\u044b \u0432\u0441\u0435 \u043e\u0431\u044a\u0435\u043a\u0442\u044b \u0432 \u043d\u0435\u0439 \u0432\u0437\u0432\u0435\u0448\u0438\u0432\u0430\u043b\u0438\u0441\u044c \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u043e. \u041d\u0430\u043f\u0440\u043e\u0442\u0438\u0432, \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043d\u0435 \u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432, \u043f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 \u043e\u043d\u0438 \u043d\u0435 \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u043c\u043e\u0434\u0435\u043b\u044f\u043c\u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0440\u0430\u0441\u0441\u0447\u0442\u0435\u0430 \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u044f \n\u0438 \u043c\u043e\u0433\u0443\u0442 \u043e\u0431\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0442\u044c \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0439 \u043d\u0430\u0431\u043e\u0440 \u0444\u0443\u043d\u043a\u0446\u0438\u0439.\n2) \u041c\u0430\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u0442\u043e\u0433\u043e, \u0447\u0442\u043e \u0431\u044b \u0438\u0437-\u0437\u0430 \u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0434\u0440\u0443\u0433\u0438\u0445 \u043c\u0435\u043d\u0449\u0438\u0445 \u043d\u0435 \u0431\u044b\u043b\u043e \u0432\u043b\u0438\u044f\u043d\u0438\u044f \u043d\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442. \n3) \u0421\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442 \u0434\u0432\u0430 \u043f\u043e\u0434\u0445\u043e\u0434\u0430 \u043a \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044e - \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0438 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u0430\u0446\u0438\u044f.\n4) \u0412\u043e \u0432\u0440\u0435\u043c\u044f \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u0430\u0446\u0438\u0438 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0441\u0432\u043e\u0434\u044f\u0442\u0441\u044f \u043a \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u043c\u0443 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044e \u0441\u043e \u0441\u0440\u0435\u0434\u043d\u0438\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\u043c 0 \u0438 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u043c \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u0435\u043c 1. \u0421\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u0430\u0446\u0438\u044f \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u0443\u044e \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043e\u0431 \u0430\u0443\u0442\u043b\u0430\u0439\u0435\u0440\u0430\u0445 \u0438 \u0434\u0435\u043b\u0430\u0435\u0442 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u043c\u0435\u043d\u0435\u0435 \u0447\u0443\u0432\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u043c \u043a \u043d\u0438\u043c.","2a918953":"We observe the following:\n\n1. **Underfitting** (degree < 5): The model is not able to fit the data properly. The fit is bad for both the training and the validation set.\n\n2. **Fit is just right** (degree = 5): The model is able to capture the underlying data distribution. The fit is good for both the training and the validation set.\n\n3. **Overfitting** (degree > 5): The model starts fitting the noise in the dataset. While the fit for the training data gets even better, the fit for the validation set gets worse.\n\n4. As the order of polynomial increases, the linear model coefficients become more likely to take on **large values**.","4497758b":"### Cross-validation","ecd11430":"We can investigate the shape of the fitted curve for different values of `degree`:","749d3b32":"Ideally, we would choose the the model parameters such that we have the best model performance. However, we want to make sure that we really have the best validation performance. When we do `train_test_split` we randomly split the data into two parts. What could happen is that we got lucky and split the data such that it favours the validation error. This is especially dangerous if we are dealing with small datasets. One way to check if that's the case is to run the experiment several times for different, random splits. However, there is an even more systematic way of doing this: [cross-validation](https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html).","c6b86591":"### L2 - Ridge regression","770cf6af":"There are two major ways to build a machine learning model with the ability to generalize well on unseen data:\n1. Train the simplest model possible for our purpose (according to Occam\u2019s Razor).\n2. Train a complex or more expressive model on the data and perform regularization.\n\nRegularization is a method used to reduce the variance of a machine learning model. In other words, it is used to reduce overfitting. Regularization penalizes a model for being complex. For linear models, it means regularization forces model coefficients to be smaller in magnitude.\n\nLet's pick a polynomial model of degree **15** (which tends to overfit strongly) and try to regularize it using **L1** and **L2** penalties.","36c78197":"Let's pick a target function $ f(x) = 2\\cdot x + 10\\cdot sin(x) $ and generate some noisy samples to learn from.","77c3eb3f":"### Model coefficients","eeace8d9":"### Plot samples","95865d37":"#### L2","279b24d9":"### Fitting graph","a64749db":"#### L2","376e1665":"### Split","b9e5c112":"### From underfitting to overfitting","e6356bb1":"### Fit","7026b1ba":"## Part 3: Homework assignment (10 points)","ea0e6ca4":"As a general rule, it is recommended to scale input features before fitting a regularized model so that the features\/inputs take values in similar ranges. One common way of doing so is to standardize the inputs and that is exactly what our pipeline  second step (`StandardScaler`) is responsible for. \n\nWhy is scaling important? What are the underlying reasons?","8805aac9":"## Part 2: Regularization","44a9979f":"Let's inspect our regression model coefficients:","26526040":"1. We can control the regularization strength by changing the hyperparameter `alpha`.\n2. Regularized version of the model performs pretty well. Even in case the original original (unregularized) model is heavily overfitting due to excessive complexity.","7ece1e9b":"Hmm... it looks like high degree polynomials are coming with much bigger regression coefficients. \n\nWe are going to plot the mean absolute value of $w_i$ as a function of degree to reveal the relationship:","1d6d318f":"### Model","7f294cb5":"#### Summary","e3c5798e":"### Summary","fc0e9c07":"# Overfitting and\u00a0Regularization","672b1973":"<img src=\"https:\/\/scikit-learn.org\/stable\/_images\/grid_search_cross_validation.png\" width=50% \/>","0deaa73e":"#### Summary","12a33e0a":"#### L1","2be1ab6d":"#### L1","7e3ef14d":"## Settings","c7781743":"Let's try to approximate our target function $ f(x) = 2\\cdot x + 10\\cdot sin(x) $ with polynomials of different degree. \n\nA polynomial of degree $n$ has the form:\n$ h(x) = w_0 + w_1\\cdot x + w_2\\cdot x^2 +\\ldots + w_n\\cdot x^n $.\n\n$x^i$ values could easily be generated by `PolynomialFeatures`, while $w_i$ are the unknown paramaters to be estimated using `LinearRegression`.","9e502a58":"### Excercise 2 - Sparsity (4 points)","d8ba37e3":"### Generate samples","3411919f":"### Excercise 1 - Magnitude (4 points)","2d0986b5":"### Summary","d018c732":"In the next step we calculate the training and the validation error for each `degree` and plot them in a single graph. The resulting graph is called the fitting graph.","13610613":"Lasso can also be used for **feature selection** since L1 is [more likely to produce zero coefficients](https:\/\/explained.ai\/regularization\/).\n\nIs it indeed happening? \n\nPlease do a discovery on your own and find that out empirically (both for **L1** and **L2**). Let's use `degree=15` and `alpha` from `ALPHAS`.","b67a85b0":"Let's fit a model and plot the hypothesis it learns:","d0ec7f1c":"### Excercise 3 - Scaling (2 points)","81a5a7eb":"As discussed earlier, regularization methods are expected to constraint the weights (model coefficients). \n\nIs it indeed happening? \n\nPlease do a discovery on your own and find that out empirically (both for **L1** and **L2**). Let's use `degree=15` and `alpha` from `ALPHAS`.","cc7c799f":"### L1 - Lasso regression","c7b07fa8":"## Part 1: Underfitting vs. overfitting","6a858606":"### Sweet spot","6ee60d70":"What is the optimal `degree` to go with?","d0a6e5fd":"## Imports"}}