{"cell_type":{"208ed254":"code","3f187f71":"code","ba2bff66":"code","e6b8f137":"code","d0129ddf":"code","48bc2e50":"code","37ce4474":"code","e4edd653":"code","6996b187":"code","36808bf3":"code","acaccf10":"code","829e21c2":"code","88ae23b1":"code","70adbab7":"code","f93a075d":"code","8be75442":"code","11fd820b":"code","6221c9d5":"code","4d84c1fa":"code","89fdaf3d":"code","a3303a82":"code","a78adfa6":"code","7584289b":"code","e157bf26":"code","1872267d":"code","f4968784":"code","06b15f8a":"code","7eb1a661":"code","f5c767f6":"code","c9d43817":"code","dcf29442":"code","1fab0a9c":"code","7f986fc0":"code","469d863a":"code","b4e03573":"code","ba0bde62":"code","36ab5d40":"code","4d7a4700":"code","5c59f483":"code","2508a1a9":"code","b92a73f4":"code","71a67f3f":"code","affa0393":"code","201a6c01":"code","f48c135b":"code","a9f22a09":"markdown","5cf16301":"markdown","f5c22da2":"markdown","45ddb427":"markdown","8fe67338":"markdown","19a0d3d2":"markdown","98a88407":"markdown","d38369c8":"markdown"},"source":{"208ed254":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","3f187f71":"df = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")","ba2bff66":"df.head()","e6b8f137":"df.info()","d0129ddf":"df.describe().transpose()","48bc2e50":"plt.figure(figsize=(10,6))\nsns.countplot(x=\"target\", data=df)","37ce4474":"plt.figure(figsize=(10,6))\nsns.countplot(x=\"sex\", hue=\"target\", data=df)","e4edd653":"plt.figure(figsize=(10,6))\nsns.distplot(df[\"age\"])","6996b187":"plt.figure(figsize=(24,8))\nsns.countplot(x=\"age\", hue=\"target\", data=df)","36808bf3":"plt.figure(figsize=(10,6))\nsns.countplot(x=\"cp\", hue=\"target\", data=df)","acaccf10":"plt.figure(figsize=(10,6))\nsns.countplot(x=\"fbs\", hue=\"target\", data=df)","829e21c2":"plt.figure(figsize=(10,6))\nsns.countplot(x=\"restecg\", hue=\"target\", data=df)","88ae23b1":"plt.figure(figsize=(10,6))\nsns.countplot(x=\"exang\", hue=\"target\", data=df)","70adbab7":"plt.figure(figsize=(10,6))\nsns.countplot(x=\"slope\", hue=\"target\", data=df)","f93a075d":"plt.figure(figsize=(10,6))\nsns.countplot(x=\"ca\", hue=\"target\", data=df)","8be75442":"plt.figure(figsize=(10,6))\nsns.countplot(x=\"thal\", hue=\"target\", data=df)","11fd820b":"f, ax = plt.subplots(figsize=(16, 12))\nplt.title('Pearson Correlation Matrix', fontsize=25)\n\nsns.heatmap(df.corr(), linewidths=0.25, vmax=0.7, square=True, cmap=\"BuGn\",\n            linecolor='w', annot=True, annot_kws={\"size\":8}, cbar_kws={\"shrink\": .9});","6221c9d5":"cp_dummies = pd.get_dummies(df[\"cp\"], drop_first=True, prefix=\"cp\")\nrestecg_dummies = pd.get_dummies(df[\"restecg\"], drop_first=True, prefix=\"restecg\")\nslope_dummies = pd.get_dummies(df[\"slope\"], drop_first=True, prefix=\"slope\")\nca_dummies = pd.get_dummies(df[\"ca\"], drop_first=True, prefix=\"ca\")\nthal_dummies = pd.get_dummies(df[\"thal\"], drop_first=True, prefix=\"thal\")\n                            ","4d84c1fa":"concat_df = [df, cp_dummies, restecg_dummies, slope_dummies, ca_dummies, thal_dummies]\ndf = pd.concat(concat_df, axis=1)\ndf = df.drop([\"cp\", \"restecg\", \"slope\", \"ca\", \"thal\"], axis=1)","89fdaf3d":"df.head()","a3303a82":"X = df.drop(\"target\", axis=1).values\ny = df[\"target\"].values","a78adfa6":"from sklearn.model_selection import train_test_split","7584289b":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1\/3, random_state=0)","e157bf26":"X_train.shape","1872267d":"X_test.shape","f4968784":"from sklearn.preprocessing import StandardScaler, MinMaxScaler","06b15f8a":"scaler = MinMaxScaler()","7eb1a661":"X_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","f5c767f6":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis","c9d43817":"classifiers = [\n    (\"Logistic Regression\", LogisticRegression(random_state=0)),\n    (\"KNN\", KNeighborsClassifier()),\n    (\"Support Vector Machine\", SVC(kernel = 'rbf',random_state=0)),\n    (\"Naive Bayes\", GaussianNB()),\n    (\"Random Forest\", RandomForestClassifier(random_state=0)),\n    (\"Ada Boost\", AdaBoostClassifier(random_state=0)),\n    (\"Gradient Boosting\", GradientBoostingClassifier(random_state=0)),\n    (\"XGBoost\", XGBClassifier(random_state=0)),\n    (\"LDA\", LinearDiscriminantAnalysis())\n    \n]","dcf29442":"from sklearn.model_selection import cross_val_score","1fab0a9c":"columns = [\"Classifier\", \"Validation Score\", \"+\/-\"]\nmodel_comparison = pd.DataFrame(columns=columns)\nrow_index = 0\n\nfor name, clf in classifiers:\n    scores = cross_val_score(clf, X_train, y_train, cv=5)\n    model_comparison.loc[row_index, \"Classifier\"] = name\n    model_comparison.loc[row_index, \"Validation Score\"] = round(scores.mean(), 4)\n    model_comparison.loc[row_index, \"+\/-\"] = round(scores.std()*2, 4)\n    row_index += 1\n    \nmodel_comparison.sort_values(by=[\"Validation Score\"], ascending=False, inplace=True)\nmodel_comparison.reset_index(drop=True, inplace=True)\nmodel_comparison","7f986fc0":"classifiers = [\n    (\"Logistic Regression\", LogisticRegression(random_state=0)),\n    (\"KNN\", KNeighborsClassifier()),\n    (\"Support Vector Machine\", SVC(random_state=0)),\n    (\"Naive Bayes\", GaussianNB()),\n    (\"Random Forest\", RandomForestClassifier(random_state=0)),\n    (\"Ada Boost\", AdaBoostClassifier(random_state=0)),\n    (\"Gradient Boosting\", GradientBoostingClassifier(random_state=0)),\n    (\"XGBoost\", XGBClassifier(random_state=0)),\n    (\"LDA\", LinearDiscriminantAnalysis())\n    \n]","469d863a":"parameters = [\n    [{\n        #LR\n        \"solver\": [\"lbfgs\", \"liblinear\"],\n        \"penalty\": [\"l2\"],\n    }],\n    [{\n        #KNN\n        \"n_neighbors\": [3 , 4, 5, 6,  7, 8, 9, 10, 12, 15],\n        \"weights\": [\"uniform\", \"distance\"],\n        \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n    }],\n    [{\n        #SVM\n        \"kernel\": [\"linear\", \"rbf\"],\n        \"gamma\": [\"scale\", \"auto\"],\n        \"C\": [1,2,3],\n        \"decision_function_shape\": [\"ovo\", \"ovr\"]\n    }],\n    [{\n        #NB\n    }],\n    [{\n        #RF\n        #\"bootstrap\": [True, False],  \n        #\"min_samples_leaf\": [1, 2, 4, 6],\n        #\"min_samples_split\": [2, 4, 6, 10],\n        #\"n_estimators\": [50, 100, 200, 300],\n        #\"criterion\": [\"gini\", \"entropy\"],\n        #\"max_depth\": [20, 30, 40, None],\n        #\"max_features\": [\"auto\", \"log2\"]\n        \"bootstrap\": [True],\n        \"min_samples_leaf\": [1],\n        \"min_samples_split\": [4],\n        \"n_estimators\": [50],\n        \"criterion\": [\"entropy\"],\n        \"max_depth\": [20],\n        \"max_features\": [\"auto\"]\n    }],\n    [{\n        #ADA\n        \"n_estimators\": [25, 50, 75, 100],\n        \"learning_rate\": [0.25, 0.5, 1],\n        \n    }],\n    [{\n        #GB\n        #\"learning_rate\": [0.05, 0.1, 0.2, 0.25, 0.3],\n        #\"n_estimators\": [50, 100, 150, 200],\n        #\"criterion\": [\"friedman_mse\", \"mse\", \"mae\"],\n        #\"max_depth\": [2, 3, 4, 5]\n        \"learning_rate\": [0.25],\n        \"n_estimators\": [150],\n        \"criterion\": [\"friedman_mse\"],\n        \"max_depth\": [3]\n        \n        \n    }],\n    [{\n        #XGB\n        \"learning_rate\": [0.025, 0.05, 0.1, 0.2, 0.3, 0.4],\n        \"max_depth\": [2,4,6,8,10]\n    }],\n    [{\n        # LDA\n        \"solver\": [\"svd\", \"lsqr\"]\n    }]\n]","b4e03573":"from sklearn.model_selection import GridSearchCV\nimport time","ba0bde62":"columns = [\"Classifier\", \"Grid Search Score\"]\nmodel_comparison = pd.DataFrame(columns=columns)\nrow_index = 0\n\nfor (name, clf), param in zip(classifiers, parameters):\n    start = time.perf_counter()\n    grid_search = GridSearchCV(estimator=clf, param_grid=param, cv=5)\n    grid_search.fit(X_train, y_train)\n    stop = time.perf_counter() - start\n    best_params = grid_search.best_params_\n    best_score = round(grid_search.best_score_, 4)\n    model_comparison.loc[row_index, \"Classifier\"] = name\n    model_comparison.loc[row_index, \"Grid Search Score\"] = best_score\n    row_index += 1\n    print(\"Best parameters for {} are: {}.\".format(name, best_params))\n    print(\"Best score of {} is: {}.\".format(name, best_score))\n    print(\"Run time of {} is {:.2f} second\".format(name, stop))","36ab5d40":"model_comparison.sort_values(by=[\"Grid Search Score\"], ascending=False, inplace=True)\nmodel_comparison.reset_index(drop=True, inplace=True)\nmodel_comparison","4d7a4700":"from sklearn.ensemble import VotingClassifier","5c59f483":"voting_classifiers = [\n    (\"Logistic Regression\", LogisticRegression(penalty=\"l2\", solver=\"liblinear\", random_state=0)),\n    (\"KNN\", KNeighborsClassifier(algorithm=\"auto\", n_neighbors=10, weights=\"distance\")),\n    (\"Support Vector Machine\", SVC(C=1, decision_function_shape=\"ovo\", gamma=\"scale\", kernel=\"linear\", probability=True, random_state=0)),\n    #(\"Naive Bayes\", GaussianNB()),\n    (\"Random Forest\", RandomForestClassifier(bootstrap=True, criterion=\"entropy\", max_depth=20, max_features=\"auto\",\n                                             min_samples_leaf=1, min_samples_split=4, n_estimators=50, random_state=0)),\n    (\"Ada Boost\", AdaBoostClassifier(learning_rate=1, n_estimators=50, random_state=0)),\n    (\"Gradient Boosting\", GradientBoostingClassifier(criterion=\"friedman_mse\", learning_rate=0.25, max_depth=3, \n                                                     n_estimators=150, random_state=0)),\n    #(\"XGBoost\", XGBClassifier(learning_rate=0.05, max_depth=2, random_state=0)),\n    (\"LDA\", LinearDiscriminantAnalysis(solver=\"svd\"))\n    \n]","2508a1a9":"hard_voting = VotingClassifier(estimators = voting_classifiers, voting=\"hard\")\nscores = cross_val_score(hard_voting, X_train, y_train, cv=5)\nprint(\"Hard voting score is {:.4f}\".format(scores.mean()))","b92a73f4":"soft_voting = VotingClassifier(estimators = voting_classifiers, voting=\"soft\")\nscores = cross_val_score(soft_voting, X_train, y_train, cv=5)\nprint(\"Soft voting score is {:.4f}\".format(scores.mean()))","71a67f3f":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report","affa0393":"soft_voting = VotingClassifier(estimators = voting_classifiers, voting=\"soft\")\nsoft_voting.fit(X_train, y_train)\ny_pred = soft_voting.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Soft voting score on test data is {:.4f}\".format(accuracy))","201a6c01":"print(classification_report(y_test, y_pred, digits = 4 ))","f48c135b":"cm = confusion_matrix(y_test, y_pred)\nprint(cm)","a9f22a09":"# **Models**","5cf16301":"# **Train Test Split**","f5c22da2":"# **Scaling the Data**","45ddb427":"# **Grid Search**","8fe67338":"# **Evaluation**","19a0d3d2":"# **Creating Dummy Variables**","98a88407":"# **Voting**","d38369c8":"# **Data Analysis**"}}