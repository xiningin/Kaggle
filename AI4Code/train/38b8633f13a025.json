{"cell_type":{"379599a7":"code","b6af9483":"code","ad7d1fd7":"code","7c629972":"code","26e4944b":"code","62ca68ac":"code","8c024504":"code","c747b9a3":"code","bd82d437":"code","7bc32316":"code","cbd57178":"code","0a2bdde0":"code","b4b270e4":"code","4e91c4f6":"code","b24c8ed8":"code","d3954acf":"code","30a41f01":"code","c32de53c":"code","bcc18b6b":"code","0f7850cf":"code","d233b8c7":"code","9a379a94":"code","b200678a":"code","e1a66ffd":"code","8cd6689f":"code","15d36efd":"code","9fad64be":"code","64345d6d":"code","da1732b7":"code","05b296b3":"code","abbf886d":"code","a033d131":"code","431d7a2d":"code","998e4673":"code","0f827c5a":"code","2b2fbb87":"code","7073f24e":"code","4602f828":"code","fba7b2a3":"code","8a871251":"markdown","4e736de6":"markdown","54468bc1":"markdown","8d7f4b9c":"markdown","a8f61fb4":"markdown","71c54a84":"markdown","9a7b2e1e":"markdown","2b8c63b5":"markdown","61a5d617":"markdown","228d3142":"markdown"},"source":{"379599a7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n# Basic\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport json\nimport os\nimport random\nfrom tqdm.autonotebook import tqdm\nimport string\nimport re\nfrom functools import partial\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b6af9483":"import functools\nfrom IPython.core.display import display, HTML\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)\n#Metadata_df=pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/all_sources_metadata_2020-03-13.csv')\ntrain_df = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/train.csv')\nsample_sub = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv')\ntrain_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\ntest_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/test'\n\n#Metadata_df = pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv')","ad7d1fd7":"print('shape' ,train_df.shape)","7c629972":"print('info',train_df.info())","26e4944b":"train_df.head()","62ca68ac":"\ndef read_append_return(filename, train_files_path=train_files_path, output='text'):\n    json_path = os.path.join(train_files_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data","8c024504":"tqdm.pandas()\ntrain_df['text'] = train_df['Id'].progress_apply(read_append_return)","c747b9a3":"pd.set_option('display.max_colwidth', None)\ntrain_df.text[train_df['Id']=='0007f880-0a9b-492d-9a58-76eb0b0e0bd7']","bd82d437":"for col in train_df.columns:\n    print(f\"{col}: {len(train_df[col].unique())}\")","7bc32316":"#textprops={'color':\"b\"}\nimport matplotlib.pyplot as plt\ntrain_df['pub_title'].value_counts().iloc[[0,1,2,3,4,5,6,7,8,9]].plot.pie(figsize = (10,10),autopct = '%.2f%%',\n                                                                           title = 'Top Ten Pub Title')\nplt.title(\"Top Ten Pub Title\", bbox={'facecolor':'0.8', 'pad':5})\nplt.savefig(\"Top Ten Pub Title\"+\".png\", bbox_inches='tight')","cbd57178":"\ntrain_df['dataset_title'].value_counts().iloc[[0,1,2,3,4,5,6,7,8,9]].plot.pie(figsize = (10,10), autopct = '%.2f%%',\n                                                                           title = 'Top Ten Dataset title')\nplt.title(\"Top Ten Data set title\", bbox={'facecolor':'0.8', 'pad':5})\nplt.savefig(\"Top Ten Data set title\"+\".png\", bbox_inches='tight')","0a2bdde0":"\ntrain_df['dataset_label'].value_counts().iloc[[0,1,2,3,4,5,6,7,8,9]].plot.pie(figsize = (10,10), autopct = '%.2f%%',\n                                                                           title = 'Top Ten Dataset Label')\nplt.title(\"Top Ten Dataset Label\", bbox={'facecolor':'0.8', 'pad':5})\nplt.savefig(\"Top Ten Dataset Label\"+\".png\", bbox_inches='tight')","b4b270e4":"duplicateRowsDF = train_df['Id'][train_df.duplicated(['Id'])]\nprint(\"Duplicate Rows except first occurrence based on all columns are :\")\nprint(duplicateRowsDF)","4e91c4f6":"dups_ID = train_df.pivot_table(index=['Id'], aggfunc='size')\nprint (dups_ID)","b24c8ed8":"from gensim.parsing.preprocessing import remove_stopwords\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom collections import Counter \nimport time\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom gensim import corpora,models,similarities\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom gensim.summarization import summarize\nfrom gensim.summarization import keywords\nporter = PorterStemmer()","d3954acf":"# getting the lower case , removing stopwords and gettting the word origin\ndef lower_case(input_str):\n    input_str = input_str.lower()\n    return input_str\ndef stemSentence(sentence):\n    token_words=word_tokenize(sentence)\n    token_words\n    stem_sentence=[]\n    for word in token_words:\n        stem_sentence.append(porter.stem(word))\n        stem_sentence.append(\" \")\n    return \"\".join(stem_sentence)\n\ndef text_preprocessing(text):\n    text= re.sub('[^a-zA-z0-9\\s]','',text)\n    text=lower_case(text)\n    text=remove_stopwords(text)\n    text=stemSentence(text)    \n    return text","30a41f01":"'''Combined_data_processing=train_df.copy()\nCombined_data_processing['pub_title']=Combined_data_processing['pub_title'].apply(lambda x: text_preprocessing(x))\nCombined_data_processing['dataset_title']=Combined_data_processing['dataset_title'].apply(lambda x: text_preprocessing(x))\nCombined_data_processing['dataset_label']=Combined_data_processing['dataset_label'].apply(lambda x: text_preprocessing(x))\nCombined_data_processing['text']=Combined_data_processing['text'].apply(lambda x: text_preprocessing(x))'''","c32de53c":"#Combined_data_processing.to_csv('processed_train.csv')","bcc18b6b":"Combined_data_processing=pd.read_csv('..\/input\/coleridge-cosine-similarity-gensim\/processed_train.csv')\nCombined_data_processing.head()","0f7850cf":"'''KeyWords=[]\nfor i in range(0,len(Combined_data_processing)):\n    KeyWords.append(remove_stopwords(str(keywords(Combined_data_processing['text'][i]))))'''","d233b8c7":"'''corpus_text = [] # intializing list of collection of  clean reviews\n\nfor i in range(0, 33384):\n    for word in str(KeyWords['KeyWords'][i]).split():\n        corpus_text.append(word)'''","9a379a94":"\n#len(KeyWords)","b200678a":" #removing the words less than four chars \n#corpus_text_modified=[i for i in corpus_text if 5 <=  len(i)]","e1a66ffd":"'''unique_string=(\" \").join(KeyWords)\nwordcloud = WordCloud(width = 1000, height = 500 ,max_words=10000).generate(unique_string)\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.savefig(\"your_file_name\"+\".png\", bbox_inches='tight')\nplt.show()\nplt.close()'''","8cd6689f":"corpus_pub_title = [] # intializing list of collection of  clean reviews\n\nfor i in range(0, len(Combined_data_processing)):\n    for word in str(Combined_data_processing['pub_title'][i]).split():\n        corpus_pub_title.append(word)","15d36efd":"''''removed_words= ['nan', 'missing','miss']  \nfor word in list(corpus_pub_title):  # iterating on a copy since removing will mess things up\n    if word in removed_words:\n        corpus_pub_title.remove(word)","9fad64be":"corpus_pub_title=[i for i in corpus_pub_title if 3 <=  len(i)]","64345d6d":"unique_string=(\" \").join(corpus_pub_title)\nwordcloud = WordCloud(background_color='white',width = 1000, height = 500 ,max_words=10000).generate(unique_string)\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.savefig(\"Authors_Words\"+\".png\", bbox_inches='tight')\nplt.show()\nplt.close()","da1732b7":"corpus_dataset_title = [] # intializing list of collection of  clean reviews\n\nfor i in range(0, len(Combined_data_processing)):\n    for word in str(Combined_data_processing['dataset_title'][i]).split():\n        corpus_dataset_title.append(word)","05b296b3":"unique_string=(\" \").join(corpus_dataset_title)\nwordcloud = WordCloud(background_color='white',width = 1000, height = 500 ,max_words=10000).generate(unique_string)\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.savefig(\"Titles_Words\"+\".png\", bbox_inches='tight')\nplt.show()\nplt.close()","abbf886d":"corpus_dataset_label = [] # intializing list of collection of  clean reviews\n\nfor i in range(0, len(Combined_data_processing)):\n    for word in str(Combined_data_processing['dataset_label'][i]).split():\n        corpus_dataset_label.append(word)","a033d131":"unique_string=(\" \").join(corpus_dataset_label)\nwordcloud = WordCloud(background_color='white',width = 1000, height = 500 ,max_words=30000).generate(unique_string)\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.savefig(\"abstract_words\"+\".png\", bbox_inches='tight')\nplt.show()\nplt.close()","431d7a2d":"title_font = {'fontname':'Arial', 'size':'18', 'color':'black', 'weight':'normal',\n              'verticalalignment':'bottom'} # Bottom vertical alignment for more space\naxis_font = {'fontname':'Arial', 'size':'14'}\n#corpus_title=[i for i in corpus_title if 4 <=  len(i)]\ncounter_title = Counter(corpus_dataset_title) \n  \n# most_common() produces k frequently encountered \n# input values and their respective counts. \nmost_occur = dict(counter_title.most_common(100)) \n  \nprint(most_occur) \n\nlabels, values = zip(*most_occur.items())\n\n# sort your values in descending order\nindSort = np.argsort(values)[::-1]\n\n# rearrange your data\nlabels = np.array(labels)[indSort]\nvalues = np.array(values)[indSort]\n\nindexes = np.arange(len(labels))\nplt.figure(figsize=(20,40))\nplt.barh(labels, values)\nplt.title('The most frequent words in Articles Titles',**title_font,bbox={'facecolor':'0.8', 'pad':5})\nplt.xlabel('Number of Words',**axis_font,color='black')\nplt.ylabel('Top 100 Words in Titles',**axis_font,color='black')\nplt.xticks(color='black')\nplt.yticks(color='black')\nplt.savefig(\"title most frequent Words\"+\".png\", bbox_inches='tight')\n# add labels\n#plt.xticks(indexes + bar_width, labels)\nplt.show()","998e4673":"counter_abstract = Counter(corpus_dataset_label) \n  \n# most_common() produces k frequently encountered \n# input values and their respective counts. \nmost_occur = dict(counter_abstract.most_common(100)) \n  \nprint(most_occur) \n\nlabels, values = zip(*most_occur.items())\n\n# sort your values in descending order\nindSort = np.argsort(values)[::-1]\n\n# rearrange your data\nlabels = np.array(labels)[indSort]\nvalues = np.array(values)[indSort]\n\nindexes = np.arange(len(labels))\nplt.figure(figsize=(20,40))\nplt.barh(labels, values)\nplt.title('The most frequent words in Articles Abstract',**title_font,bbox={'facecolor':'0.8', 'pad':5})\nplt.xlabel('Number of Words',**axis_font,color='black')\nplt.ylabel('Top 100 Words in Abstracts',**axis_font,color='black')\nplt.xticks(color='black')\nplt.yticks(color='black')\nplt.savefig(\"abstract most frequent Words\"+\".png\", bbox_inches='tight')\n# add labels\n#plt.xticks(indexes + bar_width, labels)\nplt.show()","0f827c5a":"title_font = {'fontname':'Arial', 'size':'18', 'color':'black', 'weight':'normal',\n              'verticalalignment':'bottom'} # Bottom vertical alignment for more space\naxis_font = {'fontname':'Arial', 'size':'14'}\n#corpus_authors=[i for i in corpus_authors if 2 <=  len(i)]\ncorpus_authors = Counter(corpus_pub_title) \n  \n# most_common() produces k frequently encountered \n# input values and their respective counts. \nmost_occur = dict(corpus_authors.most_common(100)) \n  \nprint(most_occur) \n\nlabels, values = zip(*most_occur.items())\n\n# sort your values in descending order\nindSort = np.argsort(values)[::-1]\n\n# rearrange your data\nlabels = np.array(labels)[indSort]\nvalues = np.array(values)[indSort]\n\nindexes = np.arange(len(labels))\nplt.figure(figsize=(20,40))\nplt.barh(labels, values)\nplt.title('The most frequent words in Articles Abstract',**title_font,bbox={'facecolor':'0.8', 'pad':5})\nplt.xlabel('Number of Words',**axis_font,color='black')\nplt.ylabel('Top 100 Words in Abstracts',**axis_font,color='black')\nplt.xticks(color='black')\nplt.yticks(color='black')\nplt.savefig(\"abstract most frequent Words\"+\".png\", bbox_inches='tight')\n# add labels\n#plt.xticks(indexes + bar_width, labels)\nplt.show()","2b2fbb87":"#Implementing TFIDF \nvectorizer = TfidfVectorizer(max_features=5000)\nX = vectorizer.fit_transform(Combined_data_processing['text'].values)\n\n#implementing K-Means\nk = 9\nkmeans = MiniBatchKMeans(n_clusters=k)\ny_pred = kmeans.fit_predict(X)\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=3)\npca_result = pca.fit_transform(X.toarray())\n            \nprint(pca.components_)\nprint(pca.explained_variance_)","7073f24e":"#implementing K-Means Clustering\ntitle_font = {'fontname':'Arial', 'size':'18', 'color':'black', 'weight':'normal',\n              'verticalalignment':'bottom'} # Bottom vertical alignment for more space\naxis_font = {'fontname':'Arial', 'size':'14'}\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(15,15)})\n\n# colors\npalette = sns.color_palette(\"muted\", len(set(y_pred)))\n\n# plot\nsns.scatterplot(pca_result[:,0], pca_result[:,1], hue=y_pred,sizes =100, legend='full',palette=palette)\nplt.title(\"PCA - Clustered (K-Means)\",**title_font,bbox={'facecolor':'0.8', 'pad':5})\nplt.savefig(\"Kmeans\"+\".png\", bbox_inches='tight')\nplt.show()","4602f828":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nax = plt.figure(figsize=(16,10)).gca(projection='3d')\nax.scatter(\n    xs=pca_result[:,0], \n    ys=pca_result[:,1], \n    zs=pca_result[:,2], \n    c=y_pred, \n    cmap='tab10'\n)\nax.set_xlabel('pca-1')\nax.set_ylabel('pca-2')\nax.set_zlabel('pca-3')\n\nplt.title(\"PCA - Clustered (K-Means)-3D\",**title_font,bbox={'facecolor':'0.9', 'pad':2})\nplt.savefig(\"Kmeans_3D\"+\".png\", bbox_inches=\"tight\")\nplt.show()","fba7b2a3":"#Getting the best number of clusters using elbow method\nimport time\ndef elbow_plot(data, start_K, end_K, step):\n    '''\n    Generate an elbow plot to find optimal number of clusters\n    graphing K values from start_K to end_K every step value\n    \n    INPUT: \n        data: Demographics DataFrame\n        start_K: Inclusive starting value for cluster number\n        end_K: Exclusive stopping value for cluster number\n        step: Step value between start_K and end_K\n    OUTPUT: Trimmed and cleaned demographics DataFrame\n    '''\n    score_list = []\n\n    for i in range(start_K, end_K, step):\n        print(i)\n        start = time.time()\n        kmeans = MiniBatchKMeans(i)\n        model = kmeans.fit(data)\n        score = model.score(data)\n        score_list.append(abs(score))\n        end = time.time()\n        elapsed_time = end - start\n        print(elapsed_time)\n\n    plt.plot(range(start_K, end_K, step), \n    score_list, linestyle='--', marker='o', color='b');\n    plt.xlabel('# of clusters K');\n    plt.ylabel('Sum of squared errors');\n    plt.title('SSE vs. K');\n    plt.savefig('elbow_plot.png')\nelbow_plot(pca_result, 1, 20, 1)","8a871251":"# 6. Modelling","4e736de6":"# 5.  Most Frequent Words","54468bc1":"# 7. Model Evaluation","8d7f4b9c":"# 2.Data Visualization","a8f61fb4":"# to be continued......","71c54a84":"##  Fetching Data and Data Cleaing","9a7b2e1e":"# 4.Word Clouds","2b8c63b5":"## Conclusion\n1. -This scatter plot is generated from Articles text , each article text is a feature. \n1. -usinig features vector TfidfVectorizer. \n1. -Dimensionality Reduction using PCA.\n1. -generating clustering using k-Means where k=9 (the best value as elbow plot).\n1. -Topic Modeling is done on each cluster to get the keywords per cluster.","61a5d617":"# 1.Rendering Data","228d3142":"# 3.Text Preprocessing"}}