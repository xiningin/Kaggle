{"cell_type":{"9fe9aaa3":"code","550ad023":"code","ae85c3ad":"code","6e5da717":"code","1298cd6d":"code","7b93dd82":"code","1a08b9d1":"code","4c3d665f":"code","4f1fb66c":"code","5144936d":"code","a3f2eb39":"code","df9e7c00":"code","a0d9faca":"markdown","f903811e":"markdown","8a32882a":"markdown","1b7a10fd":"markdown","08d16f9b":"markdown","f00b340f":"markdown","7a4c9fd7":"markdown","933a8f22":"markdown","286d6e53":"markdown"},"source":{"9fe9aaa3":"# evaluate an ridge regression model on the dataset\nimport numpy as np\nfrom pandas import read_csv\n\n# load the dataset\nurl = 'https:\/\/raw.githubusercontent.com\/jbrownlee\/Datasets\/master\/housing.csv'\ninput_df = read_csv(url, header=None)\n#print(input_df)\n\ndata = input_df.values\n\nX = data[0:35, 12] # choose one feature\ny = data[0:35, 13] # target","550ad023":"import seaborn as sns\nsns.scatterplot(X,y)","ae85c3ad":"# Simple linear regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nlin_reg = LinearRegression()\n\n# reshape\nX_re = X[:, np.newaxis]\ny_re = y[:, np.newaxis]\n\nlin_reg.fit(X_re, y_re)\n\ny_pred = lin_reg.predict(X_re)\n\n# Plot fit over data\nplt.scatter(X_re, y_re, s=10)\nplt.plot(X_re, y_pred, color='r')\nplt.show()\n\n# Model accuracy\nrmse = np.sqrt(mean_squared_error(y,y_pred))\nr2 = r2_score(y,y_pred)\nprint(\"RMSE: \" + str(rmse))\nprint(\"R2 score: \" + str(r2))","6e5da717":"# Polynomial regression\nimport operator\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nlin_reg = LinearRegression()\n\n# reshape\nX_re = X[:, np.newaxis]\ny_re = y[:, np.newaxis]\n\n# polynomial transformation\npolynomial_features= PolynomialFeatures(degree=2)\nX_poly = polynomial_features.fit_transform(X_re)\n\nlin_reg.fit(X_poly, y_re)\n\ny_poly_pred = lin_reg.predict(X_poly)\n\n# Plot fit over data:\nplt.scatter(X_re, y_re, s=10)\nsort_axis = operator.itemgetter(0)\nsorted_zip = sorted(zip(X_re, y_poly_pred), key=sort_axis)\nX_re_sort, y_poly_pred = zip(*sorted_zip)\nplt.plot(X_re_sort, y_poly_pred, color='r')\nplt.show()\n\n# Model accuracy\nrmse = np.sqrt(mean_squared_error(y_re, y_poly_pred))\nr2 = r2_score(y_re, y_poly_pred)\nprint(\"RMSE: \" + str(rmse))\nprint(\"R2 score: \" + str(r2))","1298cd6d":"# Model with ridge regularization\nfrom sklearn.linear_model import Ridge\n\n# define model with alpha=1\nridge_reg = Ridge(alpha=1.0)\n\n# reshape\nX_re = X[:, np.newaxis]\ny_re = y[:, np.newaxis]\n\n# fit ridge model\nridge_reg.fit(X_re, y_re)\n\n# predict y\ny_ridge_pred = ridge_reg.predict(X_re)\n\n# Plot fit over data\nplt.scatter(X_re, y_re, s=10)\nplt.plot(X_re, y_ridge_pred, color='r')\nplt.show()\n\n# Model accuracy\nrmse = np.sqrt(mean_squared_error(y_re, y_ridge_pred))\nr2 = r2_score(y_re, y_ridge_pred)\nprint(\"RMSE: \" + str(rmse))\nprint(\"R2 score: \" + str(r2))","7b93dd82":"# define model with very large alpha\nridge_reg = Ridge(alpha=10000)\n\n# reshape\nX_re = X[:, np.newaxis]\ny_re = y[:, np.newaxis]\n\n# fit ridge model\nridge_reg.fit(X_re, y_re)\n\n# predict y\ny_ridge_pred = ridge_reg.predict(X_re)\n\n# Plot fit over data\nplt.scatter(X_re, y_re, s=10)\nplt.plot(X_re, y_ridge_pred, color='r')\nplt.show()\n\n# Model accuracy\nrmse = np.sqrt(mean_squared_error(y_re, y_ridge_pred))\nr2 = r2_score(y_re, y_ridge_pred)\nprint(\"RMSE: \" + str(rmse))\nprint(\"R2 score: \" + str(r2))\n\nprint(\"\\nData mean is : \" + str(np.mean(y_re)))","1a08b9d1":"# reshape\nX_re = X[:, np.newaxis]\ny_re = y[:, np.newaxis]\n\n# polynomial transformation\npolynomial_features= PolynomialFeatures(degree=10)\nX_poly = polynomial_features.fit_transform(X_re)\n\n#----------\n\n# Model without regularization\n\nlin_reg = LinearRegression()\nlin_reg.fit(X_poly, y_re)\n\ny_poly_pred = lin_reg.predict(X_poly)\n\n# Plot fit over data\nplt.scatter(X_re, y_re, s=10)\nsort_axis = operator.itemgetter(0)\nsorted_zip = sorted(zip(X_re, y_poly_pred), key=sort_axis)\nX_re_sort, y_poly_pred = zip(*sorted_zip)\nplt.plot(X_re_sort, y_poly_pred, color='r')\nplt.show()\n\n# Model accuracy\nrmse = np.sqrt(mean_squared_error(y_re, y_poly_pred))\nr2 = r2_score(y_re, y_poly_pred)\nprint(\"RMSE: \" + str(rmse))\nprint(\"R2 score: \" + str(r2))\n\n","4c3d665f":"# Model with ridge regularization\nfrom sklearn.linear_model import Ridge\n\n# define model with alpha=xxx\nridge_reg = Ridge(alpha=10)\n\n# fit ridge model\nridge_reg.fit(X_poly, y_re)\n\n# predict y\ny_ridge_poly_pred = ridge_reg.predict(X_poly)\n\n# Plot fit over data\nplt.scatter(X_re, y_re, s=10)\nsort_axis = operator.itemgetter(0)\nsorted_zip = sorted(zip(X_re, y_ridge_poly_pred), key=sort_axis)\nX_re_sort, y_ridge_poly_pred = zip(*sorted_zip)\nplt.plot(X_re_sort, y_ridge_poly_pred, color='r')\nplt.show()\n\n# Model accuracy\nrmse = np.sqrt(mean_squared_error(y_re, y_ridge_poly_pred))\nr2 = r2_score(y_re, y_ridge_poly_pred)\nprint(\"RMSE: \" + str(rmse))\nprint(\"R2 score: \" + str(r2))","4f1fb66c":"# Polynomial regression as before but using Lasso regularization\n\nfrom sklearn.linear_model import Lasso\n\n# define model with alpha=10\nlasso_reg = Lasso(alpha=10)\n\n# fit ridge model\nlasso_reg.fit(X_poly, y_re)\n\n# predict y\ny_lasso_poly_pred = lasso_reg.predict(X_poly)\n\n# Plot fit over data\nplt.scatter(X_re, y_re, s=10)\nsort_axis = operator.itemgetter(0)\nsorted_zip = sorted(zip(X_re, y_lasso_poly_pred), key=sort_axis)\nX_re_sort, y_lasso_poly_pred = zip(*sorted_zip)\nplt.plot(X_re_sort, y_lasso_poly_pred, color='r')\nplt.show()\n\n# Model accuracy\nrmse = np.sqrt(mean_squared_error(y_re, y_lasso_poly_pred))\nr2 = r2_score(y_re, y_lasso_poly_pred)\nprint(\"RMSE: \" + str(rmse))\nprint(\"R2 score: \" + str(r2))","5144936d":"from sklearn.linear_model import ElasticNet\n\n# define elastic net model\nelastic_net = ElasticNet(alpha=1, l1_ratio=0.5) # Lasso 50 : 50 Ridge\n\n# fit model\nelastic_net.fit(X_re, y_re)\n\n# predict y\ny_elas_pred = elastic_net.predict(X_re)\n\n# Plot fit over data\nplt.scatter(X_re, y_re, s=10)\nsort_axis = operator.itemgetter(0)\nsorted_zip = sorted(zip(X_re, y_elas_pred), key=sort_axis)\nX_re_sort, y_lasso_poly_pred = zip(*sorted_zip)\nplt.plot(X_re_sort, y_elas_pred, color='r')\nplt.show()\n\n# Model accuracy\nrmse = np.sqrt(mean_squared_error(y_re, y_elas_pred))\nr2 = r2_score(y_re, y_elas_pred)\nprint(\"RMSE: \" + str(rmse))\nprint(\"R2 score: \" + str(r2))","a3f2eb39":"# define elastic net model\nelastic_net = ElasticNet(alpha=1, l1_ratio=0.1) # Lasso 10 : 90 Ridge\n\n# fit model\nelastic_net.fit(X_re, y_re)\n\n# predict y\ny_elas_pred = elastic_net.predict(X_re)\n\n# Plot fit over data\nplt.scatter(X_re, y_re, s=10)\nsort_axis = operator.itemgetter(0)\nsorted_zip = sorted(zip(X_re, y_elas_pred), key=sort_axis)\nX_re_sort, y_lasso_poly_pred = zip(*sorted_zip)\nplt.plot(X_re_sort, y_elas_pred, color='r')\nplt.show()\n\n# Model accuracy\nrmse = np.sqrt(mean_squared_error(y_re, y_elas_pred))\nr2 = r2_score(y_re, y_elas_pred)\nprint(\"RMSE: \" + str(rmse))\nprint(\"R2 score: \" + str(r2))","df9e7c00":"# define elastic net model\nelastic_net = ElasticNet(alpha=1, l1_ratio=0.9) # Lasso 90 : 10 Ridge\n\n# fit model\nelastic_net.fit(X_re, y_re)\n\n# predict y\ny_elas_pred = elastic_net.predict(X_re)\n\n# Plot fit over data\nplt.scatter(X_re, y_re, s=10)\nsort_axis = operator.itemgetter(0)\nsorted_zip = sorted(zip(X_re, y_elas_pred), key=sort_axis)\nX_re_sort, y_lasso_poly_pred = zip(*sorted_zip)\nplt.plot(X_re_sort, y_elas_pred, color='r')\nplt.show()\n\n# Model accuracy\nrmse = np.sqrt(mean_squared_error(y_re, y_elas_pred))\nr2 = r2_score(y_re, y_elas_pred)\nprint(\"RMSE: \" + str(rmse))\nprint(\"R2 score: \" + str(r2))","a0d9faca":"### Polynomial Regression","f903811e":"# Regularized Linear Models\n\nSimply speaking, a good model generalized well. However, a **high-bias model** is usually the result of a model that misses important information and thus underfits while a **high-variance model** is too sensitive, thus has high variance and overfits the training data. \n\nThere is a **trade-off between bias and variance** since reducing the model complexity reduces variance but increases bias. The **irreducible error**, however, is the result not of the model but noisy data and can be reduced by cleaning up the data.\n\n**Regularization** is a way to reduce overfitting by constraining the weights of a linear model. \n\nThis notebook shows **basic code example for different regularization methods** for regression problems using scikit learn. I hope it helps you to **get started implementing regularization** into your regression models.\n\nNote: Updated code and other notebooks on machine learning for beginners may be available in my GitHub repo https:\/\/github.com\/JonasSchroeder\/handsonmachinelearning where I take notes on my journey.\n\n### Table of Contents\n0. Linear Regression\n1. Ridge Regression (Tikhonov Regularization)\n2. Lasso Regression\n3. Elastic Net\n4. Other Regression Models with Penalty Hyperparameter\n4. Application Examples","8a32882a":"### Simple Linear Regression","1b7a10fd":"# 0. Linear Regression","08d16f9b":"# 1. Ridge Regression (Tikhonov Regularization)\n\nDuring training the model, a regularization term is added to the cost function which keeps the weights as small as possible (thus preventing overfitting). This adapted cost function is called ridge regression cost function, it includes **regularizer penalty**, which is moderated by alpha (or lamda).\n\nThus, when we fit a model with ridge regression, we minimize the cost function: \n### cost = sum of squared residuals + lamda * (slope)^2\n\n\nThis introduces a small amount of Bias but reduces the Variance significantly and thus reduces the chance of overfitting the training data.\n\nA ridge model with **alpha=0 is equivalent to the simple linear regression model** (see example below).\n\nThe **larger the hyperparameter alpha, the closer the weights end up at zero** and the result is a flat line through the data's mean.\n\nThis leads to a ridge regression model having smaller weights than without regularization (or same weights when lambda = 1).","f00b340f":"# 2. Lasso Regression\n\n**Lasso Regression** stands for **Least Absolute Shrinkage and Selection Operator Regression**. Like Ridge, the model is regularized by adding a regularization term to the cost function, but it uses the L1 norm of the weight vector instead of half the square of the L2 norm.\n\nLasso regression tends to eliminate the weights of the least important features by setting them to zero. Thus, **Lasso performs feature selection** and outputs a sparse model with only a few nonzero feature weights.","7a4c9fd7":"# 3. Elastic Net\n\nElastic Net is the middle ground between Ridge Regression and Lasso Regression. Its **regularization term is a mixture of those of Ridge and Lasso** and the mix ratio can be controlled by r.\n\nWhen r=0, Elastic Net is equivalent to Lasso Regression\nWhen r=1, Elastic Net is equivalent to Ridge Regression\n\nWe should avoid using Linear Regression without regularization. **Ridge Regression** is a good default but when we think that only a small number if features are relevant, we could use **Lasso Regression**. However, Lasso can behave too erratically when the number of features is greater than the number of training instances or when several features are strongly correlated. In that case, **Elastic Net** can be used instead of Lasso to control the behavior to be less extreme.","933a8f22":"## Why is Ridge called an L2 regularization? Why Lasso L1?\n\nWhen we talk about norms in machine learning, we are talking about mathematical ways to express distance between two vectors.\n\n**Manhattan distance** or **taxicab geometry** measure distance by summing up the right-angle distances (like a cab driving around a block).\n\n**Euclidean distance** is the simplest form, it is how we would measure the distance between two points (diagonal distance)\n\nWhen we fit a model, we have a cost function which measures the distance between the predicted and actual point. Thus, our definition of distance is relevant for the cost function.\n\nEuclidean distance is also called the **L2 norm** and Manhattan distance is known as **L1 norm**. \n\nThus, **Ridge Regression uses Euclidean Distance** and **Lasso Regressionen uses Manhattan distance** when expressing the regularization term which is added to the cost function.\n\nMore: https:\/\/www.kaggle.com\/residentmario\/l1-norms-versus-l2-norms\n\n\n## When do we use Ridge, when Lasso?\n\n- L1 is more robust against outliers (extreme values) than L2 since L2 squares values\n- L2 is more stable than L1 (? resistance to horizontal adjustments ?)\n- there is ony one numeric solution for L2 (one diagonal), but many for L1 (many ways to get from A to B in Manhattan)\n- L1 is more difficult to compute than L2, since L2 has a closed form solution but L1 can only be solved piece-wise\n- L1 leads to more sparsity (elimination of many feature weights) then L2","286d6e53":"By controlling the model weights, ridge regularization reduces the variance of a model.\nLet's try ridge regularization after polynomial transformation with degree=10 versus without regularization."}}