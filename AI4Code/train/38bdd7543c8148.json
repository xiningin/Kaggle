{"cell_type":{"fc801739":"code","b01419b5":"code","89d00d3c":"code","1769bce3":"code","7cae25f5":"code","10e85907":"code","e1e9c072":"code","03f58826":"code","33b017d4":"code","d3d23595":"code","d067112d":"code","161711f5":"code","dab68d40":"code","2e3d310e":"code","2c71ddc3":"code","a06055f4":"code","d11e46bc":"code","5e8ef23e":"code","3413b8b7":"code","9859fc21":"code","c4afa804":"code","be31fe40":"code","95ac39c5":"code","f999b604":"code","806e5385":"code","1262e12e":"code","108635fc":"code","e8ff3268":"code","26727c93":"code","f1015ad5":"code","79084d03":"code","20aba8a2":"code","f2eb90a8":"code","df917683":"code","0bbe10a0":"code","a7f1d3cc":"code","0808d827":"code","07b55d48":"code","d93a5ab5":"code","bc4118a5":"code","d7d24151":"code","f523f520":"code","8e8e4fe9":"code","5c709ba2":"code","fc009be4":"code","6af84529":"code","9e086514":"code","c845e429":"code","a1116244":"code","ad658b4a":"code","b8709bc2":"code","7927e0a2":"code","98ebce99":"code","2d08a732":"code","007afde0":"code","78f27bed":"code","9aea322e":"code","8da52c5f":"markdown","d70baf99":"markdown","e0abe3eb":"markdown","7ef55ee7":"markdown","076bbc7e":"markdown","833f80be":"markdown","06464d19":"markdown","14248128":"markdown","b93366d9":"markdown","57548105":"markdown","0ab11340":"markdown","da208e76":"markdown","0644939d":"markdown","66e65c0d":"markdown","8efd572c":"markdown","1d9f9eae":"markdown","5ae68e6f":"markdown","ac94a0f3":"markdown","69b13734":"markdown","41451e5b":"markdown","9b561c15":"markdown","6c15b8a9":"markdown","feb207bc":"markdown","83f4923b":"markdown","300a121f":"markdown","747d2cc1":"markdown","3d424bf6":"markdown","19a76f1a":"markdown","28e7cd0a":"markdown","42c25517":"markdown","d15094c1":"markdown","92fe1660":"markdown","ea94bab4":"markdown","b6e3507b":"markdown","d7bfa9ba":"markdown","eab0cae4":"markdown","7f43c98d":"markdown","6224fade":"markdown"},"source":{"fc801739":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport matplotlib.gridspec as grid_spec\nimport seaborn as sns\nfrom imblearn.over_sampling import SMOTE\nimport scikitplot as skplt\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder\nfrom sklearn.model_selection import train_test_split,cross_val_score\n\n\nfrom sklearn.linear_model import LinearRegression,LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import accuracy_score, recall_score, roc_auc_score, precision_score, f1_score\nimport warnings\nwarnings.filterwarnings('ignore')\n!pip install pywaffle","b01419b5":"df = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf.head(3)","89d00d3c":"df.isnull().sum()","1769bce3":"# A really fantsatic and intelligent way to deal with blanks, from Thoman Konstantin in: https:\/\/www.kaggle.com\/thomaskonstantin\/analyzing-and-modeling-stroke-data\n\nDT_bmi_pipe = Pipeline( steps=[ \n                               ('scale',StandardScaler()),\n                               ('lr',DecisionTreeRegressor(random_state=42))\n                              ])\nX = df[['age','gender','bmi']].copy()\nX.gender = X.gender.replace({'Male':0,'Female':1,'Other':-1}).astype(np.uint8)\n\nMissing = X[X.bmi.isna()]\nX = X[~X.bmi.isna()]\nY = X.pop('bmi')\nDT_bmi_pipe.fit(X,Y)\npredicted_bmi = pd.Series(DT_bmi_pipe.predict(Missing[['age','gender']]),index=Missing.index)\ndf.loc[Missing.index,'bmi'] = predicted_bmi","7cae25f5":"print('Missing values: ',sum(df.isnull().sum()))","10e85907":"variables = [variable for variable in df.columns if variable not in ['id','stroke']]\n\nconts = ['age','avg_glucose_level','bmi']","e1e9c072":"fig = plt.figure(figsize=(12, 12), dpi=150, facecolor='#fafafa')\ngs = fig.add_gridspec(4, 3)\ngs.update(wspace=0.1, hspace=0.4)\n\nbackground_color = \"#fafafa\"\n\nplot = 0\nfor row in range(0, 1):\n    for col in range(0, 3):\n        locals()[\"ax\"+str(plot)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(plot)].set_facecolor(background_color)\n        locals()[\"ax\"+str(plot)].tick_params(axis='y', left=False)\n        locals()[\"ax\"+str(plot)].get_yaxis().set_visible(False)\n        for s in [\"top\",\"right\",\"left\"]:\n            locals()[\"ax\"+str(plot)].spines[s].set_visible(False)\n        plot += 1\n\nplot = 0\nfor variable in conts:\n        sns.kdeplot(df[variable] ,ax=locals()[\"ax\"+str(plot)], color='#0f4c81', shade=True, linewidth=1.5, ec='black',alpha=0.9, zorder=3, legend=False)\n        locals()[\"ax\"+str(plot)].grid(which='major', axis='x', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n        #locals()[\"ax\"+str(plot)].set_xlabel(variable) removed this for aesthetics\n        plot += 1\n        \nax0.set_xlabel('Age')\nax1.set_xlabel('Avg. Glucose Levels')\nax2.set_xlabel('BMI')\n\n\nax0.text(-20, 0.022, 'Numeric Variable Distribution', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-20, 0.02, 'We see a positive skew in BMI and Glucose Level', fontsize=13, fontweight='light', fontfamily='serif')\n\nplt.show()","03f58826":"fig = plt.figure(figsize=(12, 12), dpi=150,facecolor=background_color)\ngs = fig.add_gridspec(4, 3)\ngs.update(wspace=0.1, hspace=0.4)\n\n\nplot = 0\nfor row in range(0, 1):\n    for col in range(0, 3):\n        locals()[\"ax\"+str(plot)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(plot)].set_facecolor(background_color)\n        locals()[\"ax\"+str(plot)].tick_params(axis='y', left=False)\n        locals()[\"ax\"+str(plot)].get_yaxis().set_visible(False)\n        for s in [\"top\",\"right\",\"left\"]:\n            locals()[\"ax\"+str(plot)].spines[s].set_visible(False)\n        plot += 1\n\nplot = 0\n\ns = df[df['stroke'] == 1]\nns = df[df['stroke'] == 0]\n\nfor feature in conts:\n        sns.kdeplot(s[feature], ax=locals()[\"ax\"+str(plot)], color='#0f4c81', shade=True, linewidth=1.5, ec='black',alpha=0.9, zorder=3, legend=False)\n        sns.kdeplot(ns[feature],ax=locals()[\"ax\"+str(plot)], color='#9bb7d4', shade=True, linewidth=1.5, ec='black',alpha=0.9, zorder=3, legend=False)\n        locals()[\"ax\"+str(plot)].grid(which='major', axis='x', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n        #locals()[\"ax\"+str(plot)].set_xlabel(feature)\n        plot += 1\n\nax0.set_xlabel('Age')\nax1.set_xlabel('Avg. Glucose Levels')\nax2.set_xlabel('BMI')\n        \nax0.text(-20, 0.056, 'Numeric Variables by Stroke & No Stroke', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-20, 0.05, 'Age looks to be a prominent factor - this will likely be a salient feautre in our models', \n         fontsize=13, fontweight='light', fontfamily='serif')\n\nplt.show()\n","33b017d4":"str_only = df[df['stroke'] == 1]\nno_str_only = df[df['stroke'] == 0]","d3d23595":"# Setting up figure and axes\n\nfig = plt.figure(figsize=(10,16),dpi=150,facecolor=background_color) \ngs = fig.add_gridspec(4, 2)\ngs.update(wspace=0.5, hspace=0.2)\nax0 = fig.add_subplot(gs[0, 0:2])\nax1 = fig.add_subplot(gs[1, 0:2]) \n\nax0.set_facecolor(background_color)\nax1.set_facecolor(background_color)\n\n# glucose\n\nsns.regplot(no_str_only['age'],y=no_str_only['avg_glucose_level'],  \n            color='lightgray',\n            logx=True,\n            ax=ax0)\n\nsns.regplot(str_only['age'],y=str_only['avg_glucose_level'],  \n            color='#0f4c81',\n            logx=True,scatter_kws={'edgecolors':['black'], \n                                              'linewidth': 1},\n            ax=ax0)\n\nax0.set(ylim=(0, None))\nax0.set_xlabel(\" \",fontsize=12,fontfamily='serif')\nax0.set_ylabel(\"Avg. Glucose Level\",fontsize=10,fontfamily='serif',loc='bottom')\n\nax0.tick_params(axis='x', bottom=False)\nax0.get_xaxis().set_visible(False)\n\nfor s in ['top','left','bottom']:\n    ax0.spines[s].set_visible(False)\n\n\n# bmi\nsns.regplot(no_str_only['age'],y=no_str_only['bmi'],  \n            color='lightgray',\n            logx=True,\n            ax=ax1)\n\nsns.regplot(str_only['age'],y=str_only['bmi'],  \n            color='#0f4c81', scatter_kws={'edgecolors':['black'], \n                                              'linewidth': 1},\n            logx=True,\n            ax=ax1)\n\nax1.set_xlabel(\"Age\",fontsize=10,fontfamily='serif',loc='left')\nax1.set_ylabel(\"BMI\",fontsize=10,fontfamily='serif',loc='bottom')\n\n\nfor s in ['top','left','right']:\n    ax0.spines[s].set_visible(False)\n    ax1.spines[s].set_visible(False)\n\n    \nax0.text(-5,350,'Strokes by Age, Glucose Level, and BMI',fontsize=18,fontfamily='serif',fontweight='bold')\nax0.text(-5,320,'Age appears to be a very important factor',fontsize=14,fontfamily='serif')\n\n\nax0.tick_params(axis=u'both', which=u'both',length=0)\nax1.tick_params(axis=u'both', which=u'both',length=0)\n\n\n\nplt.show()","d067112d":"fig = plt.figure(figsize=(10, 5), dpi=150,facecolor=background_color)\ngs = fig.add_gridspec(2, 1)\ngs.update(wspace=0.11, hspace=0.5)\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(background_color)\n\n\ndf['age'] = df['age'].astype(int)\n\nrate = []\nfor i in range(df['age'].min(), df['age'].max()):\n    rate.append(df[df['age'] < i]['stroke'].sum() \/ len(df[df['age'] < i]['stroke']))\n\nsns.lineplot(data=rate,color='#0f4c81',ax=ax0)\n\nfor s in [\"top\",\"right\",\"left\"]:\n    ax0.spines[s].set_visible(False)\n\nax0.tick_params(axis='both', which='major', labelsize=8)\nax0.tick_params(axis=u'both', which=u'both',length=0)\n\nax0.text(-3,0.055,'Risk Increase by Age',fontsize=18,fontfamily='serif',fontweight='bold')\nax0.text(-3,0.047,'As age increase, so too does risk of having a stroke',fontsize=14,fontfamily='serif')\n\n\nplt.show()","161711f5":"from pywaffle import Waffle\n\nfig = plt.figure(figsize=(7, 2),dpi=150,facecolor=background_color,\n    FigureClass=Waffle,\n    rows=1,\n    values=[1, 19],\n    colors=['#0f4c81', \"lightgray\"],\n    characters='\u2b24',\n    font_size=20,vertical=True,\n)\n\nfig.text(0.035,0.78,'People Affected by a Stroke in our dataset',fontfamily='serif',fontsize=15,fontweight='bold')\nfig.text(0.035,0.65,'This is around 1 in 20 people [249 out of 5000]',fontfamily='serif',fontsize=10)\n\nplt.show()","dab68d40":"# Drop single 'Other' gender\nno_str_only = no_str_only[(no_str_only['gender'] != 'Other')]","2e3d310e":"fig = plt.figure(figsize=(22,15))\ngs = fig.add_gridspec(3, 3)\ngs.update(wspace=0.35, hspace=0.27)\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[0, 1])\nax2 = fig.add_subplot(gs[0, 2])\nax3 = fig.add_subplot(gs[1, 0])\nax4 = fig.add_subplot(gs[1, 1])\nax5 = fig.add_subplot(gs[1, 2])\nax6 = fig.add_subplot(gs[2, 0])\nax7 = fig.add_subplot(gs[2, 1])\nax8 = fig.add_subplot(gs[2, 2])\n\nbackground_color = \"#f6f6f6\"\nfig.patch.set_facecolor(background_color) # figure background color\n\n\n# Plots\n\n## Age\n\n\nax0.grid(color='gray', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\npositive = pd.DataFrame(str_only[\"age\"])\nnegative = pd.DataFrame(no_str_only[\"age\"])\nsns.kdeplot(positive[\"age\"], ax=ax0,color=\"#0f4c81\", shade=True, ec='black',label=\"positive\")\nsns.kdeplot(negative[\"age\"], ax=ax0, color=\"#9bb7d4\", shade=True, ec='black',label=\"negative\")\n#ax3.text(0.29, 13, 'Age', \n #        fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\nax0.yaxis.set_major_locator(mtick.MultipleLocator(2))\nax0.set_ylabel('')    \nax0.set_xlabel('')\nax0.text(-20, 0.0465, 'Age', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\n\n\n\n# Smoking\npositive = pd.DataFrame(str_only[\"smoking_status\"].value_counts())\npositive[\"Percentage\"] = positive[\"smoking_status\"].apply(lambda x: x\/sum(positive[\"smoking_status\"])*100)\nnegative = pd.DataFrame(no_str_only[\"smoking_status\"].value_counts())\nnegative[\"Percentage\"] = negative[\"smoking_status\"].apply(lambda x: x\/sum(negative[\"smoking_status\"])*100)\n\nax1.text(0, 4, 'Smoking Status', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\nax1.barh(positive.index, positive['Percentage'], color=\"#0f4c81\", zorder=3, height=0.7)\nax1.barh(negative.index, negative['Percentage'], color=\"#9bb7d4\", zorder=3,ec='black', height=0.3)\nax1.xaxis.set_major_formatter(mtick.PercentFormatter())\nax1.xaxis.set_major_locator(mtick.MultipleLocator(10))\n\n##\n# Ax2 - GENDER \npositive = pd.DataFrame(str_only[\"gender\"].value_counts())\npositive[\"Percentage\"] = positive[\"gender\"].apply(lambda x: x\/sum(positive[\"gender\"])*100)\nnegative = pd.DataFrame(no_str_only[\"gender\"].value_counts())\nnegative[\"Percentage\"] = negative[\"gender\"].apply(lambda x: x\/sum(negative[\"gender\"])*100)\n\nx = np.arange(len(positive))\nax2.text(-0.4, 68.5, 'Gender', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\nax2.grid(color='gray', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\nax2.bar(x, height=positive[\"Percentage\"], zorder=3, color=\"#0f4c81\", width=0.4)\nax2.bar(x+0.4, height=negative[\"Percentage\"], zorder=3, color=\"#9bb7d4\", width=0.4)\nax2.set_xticks(x + 0.4 \/ 2)\nax2.set_xticklabels(['Male','Female'])\nax2.yaxis.set_major_formatter(mtick.PercentFormatter())\nax2.yaxis.set_major_locator(mtick.MultipleLocator(10))\nfor i,j in zip([0, 1], positive[\"Percentage\"]):\n    ax2.annotate(f'{j:0.0f}%',xy=(i, j\/2), color='#f6f6f6', horizontalalignment='center', verticalalignment='center')\nfor i,j in zip([0, 1], negative[\"Percentage\"]):\n    ax2.annotate(f'{j:0.0f}%',xy=(i+0.4, j\/2), color='#f6f6f6', horizontalalignment='center', verticalalignment='center')\n\n    \n\n# Heart Dis\n\npositive = pd.DataFrame(str_only[\"heart_disease\"].value_counts())\npositive[\"Percentage\"] = positive[\"heart_disease\"].apply(lambda x: x\/sum(positive[\"heart_disease\"])*100)\nnegative = pd.DataFrame(no_str_only[\"heart_disease\"].value_counts())\nnegative[\"Percentage\"] = negative[\"heart_disease\"].apply(lambda x: x\/sum(negative[\"heart_disease\"])*100)\n\nx = np.arange(len(positive))\nax3.text(-0.3, 110, 'Heart Disease', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\nax3.grid(color='gray', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\nax3.bar(x, height=positive[\"Percentage\"], zorder=3, color=\"#0f4c81\", width=0.4)\nax3.bar(x+0.4, height=negative[\"Percentage\"], zorder=3, color=\"#9bb7d4\", width=0.4)\nax3.set_xticks(x + 0.4 \/ 2)\nax3.set_xticklabels(['No History','History'])\nax3.yaxis.set_major_formatter(mtick.PercentFormatter())\nax3.yaxis.set_major_locator(mtick.MultipleLocator(20))\nfor i,j in zip([0, 1], positive[\"Percentage\"]):\n    ax3.annotate(f'{j:0.0f}%',xy=(i, j\/2), color='#f6f6f6', horizontalalignment='center', verticalalignment='center')\nfor i,j in zip([0, 1], negative[\"Percentage\"]):\n    ax3.annotate(f'{j:0.0f}%',xy=(i+0.4, j\/2), color='#f6f6f6', horizontalalignment='center', verticalalignment='center')\n    \n\n## AX4 - TITLE\n\nax4.spines[\"bottom\"].set_visible(False)\nax4.tick_params(left=False, bottom=False)\nax4.set_xticklabels([])\nax4.set_yticklabels([])\nax4.text(0.5, 0.6, 'Can we see patterns for\\n\\n patients in our data?', horizontalalignment='center', verticalalignment='center',\n         fontsize=22, fontweight='bold', fontfamily='serif', color=\"#323232\")\n\nax4.text(0.15,0.57,\"Stroke\", fontweight=\"bold\", fontfamily='serif', fontsize=22, color='#0f4c81')\nax4.text(0.41,0.57,\"&\", fontweight=\"bold\", fontfamily='serif', fontsize=22, color='#323232')\nax4.text(0.49,0.57,\"No-Stroke\", fontweight=\"bold\", fontfamily='serif', fontsize=22, color='#9bb7d4')\n\n\n# Glucose\n\nax5.grid(color='gray', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\npositive = pd.DataFrame(str_only[\"avg_glucose_level\"])\nnegative = pd.DataFrame(no_str_only[\"avg_glucose_level\"])\nsns.kdeplot(positive[\"avg_glucose_level\"], ax=ax5,color=\"#0f4c81\",ec='black', shade=True, label=\"positive\")\nsns.kdeplot(negative[\"avg_glucose_level\"], ax=ax5, color=\"#9bb7d4\", ec='black',shade=True, label=\"negative\")\nax5.text(-55, 0.01855, 'Avg. Glucose Level', \n         fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\nax5.yaxis.set_major_locator(mtick.MultipleLocator(2))\nax5.set_ylabel('')    \nax5.set_xlabel('')\n\n\n\n## BMI\n\n\nax6.grid(color='gray', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\npositive = pd.DataFrame(str_only[\"bmi\"])\nnegative = pd.DataFrame(no_str_only[\"bmi\"])\nsns.kdeplot(positive[\"bmi\"], ax=ax6,color=\"#0f4c81\", ec='black',shade=True, label=\"positive\")\nsns.kdeplot(negative[\"bmi\"], ax=ax6, color=\"#9bb7d4\",ec='black', shade=True, label=\"negative\")\nax6.text(-0.06, 0.09, 'BMI', \n         fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\nax6.yaxis.set_major_locator(mtick.MultipleLocator(2))\nax6.set_ylabel('')    \nax6.set_xlabel('')\n\n\n# Work Type\n\npositive = pd.DataFrame(str_only[\"work_type\"].value_counts())\npositive[\"Percentage\"] = positive[\"work_type\"].apply(lambda x: x\/sum(positive[\"work_type\"])*100)\npositive = positive.sort_index()\n\nnegative = pd.DataFrame(no_str_only[\"work_type\"].value_counts())\nnegative[\"Percentage\"] = negative[\"work_type\"].apply(lambda x: x\/sum(negative[\"work_type\"])*100)\nnegative = negative.sort_index()\n\nax7.bar(negative.index, height=negative[\"Percentage\"], zorder=3, color=\"#9bb7d4\", width=0.05)\nax7.scatter(negative.index, negative[\"Percentage\"], zorder=3,s=200, color=\"#9bb7d4\")\nax7.bar(np.arange(len(positive.index))+0.4, height=positive[\"Percentage\"], zorder=3, color=\"#0f4c81\", width=0.05)\nax7.scatter(np.arange(len(positive.index))+0.4, positive[\"Percentage\"], zorder=3,s=200, color=\"#0f4c81\")\n\nax7.yaxis.set_major_formatter(mtick.PercentFormatter())\nax7.yaxis.set_major_locator(mtick.MultipleLocator(10))\nax7.set_xticks(np.arange(len(positive.index))+0.4 \/ 2)\nax7.set_xticklabels(list(positive.index),rotation=0)\nax7.text(-0.5, 66, 'Work Type', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\n\n\n\n# hypertension\n\npositive = pd.DataFrame(str_only[\"hypertension\"].value_counts())\npositive[\"Percentage\"] = positive[\"hypertension\"].apply(lambda x: x\/sum(positive[\"hypertension\"])*100)\nnegative = pd.DataFrame(no_str_only[\"hypertension\"].value_counts())\nnegative[\"Percentage\"] = negative[\"hypertension\"].apply(lambda x: x\/sum(negative[\"hypertension\"])*100)\n\nx = np.arange(len(positive))\nax8.text(-0.45, 100, 'Hypertension', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\nax8.grid(color='gray', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\nax8.bar(x, height=positive[\"Percentage\"], zorder=3, color=\"#0f4c81\", width=0.4)\nax8.bar(x+0.4, height=negative[\"Percentage\"], zorder=3, color=\"#9bb7d4\", width=0.4)\nax8.set_xticks(x + 0.4 \/ 2)\nax8.set_xticklabels(['No History','History'])\nax8.yaxis.set_major_formatter(mtick.PercentFormatter())\nax8.yaxis.set_major_locator(mtick.MultipleLocator(20))\nfor i,j in zip([0, 1], positive[\"Percentage\"]):\n    ax8.annotate(f'{j:0.0f}%',xy=(i, j\/2), color='#f6f6f6', horizontalalignment='center', verticalalignment='center')\nfor i,j in zip([0, 1], negative[\"Percentage\"]):\n    ax8.annotate(f'{j:0.0f}%',xy=(i+0.4, j\/2), color='#f6f6f6', horizontalalignment='center', verticalalignment='center')\n\n\n# tidy up\n\n\n\nfor s in [\"top\",\"right\",\"left\"]:\n    for i in range(0,9):\n        locals()[\"ax\"+str(i)].spines[s].set_visible(False)\n        \nfor i in range(0,9):\n        locals()[\"ax\"+str(i)].set_facecolor(background_color)\n        locals()[\"ax\"+str(i)].tick_params(axis=u'both', which=u'both',length=0)\n        locals()[\"ax\"+str(i)].set_facecolor(background_color) \n\n        \nplt.show()","2c71ddc3":"# Encoding categorical values\n\ndf['gender'] = df['gender'].replace({'Male':0,'Female':1,'Other':-1}).astype(np.uint8)\ndf['Residence_type'] = df['Residence_type'].replace({'Rural':0,'Urban':1}).astype(np.uint8)\ndf['work_type'] = df['work_type'].replace({'Private':0,'Self-employed':1,'Govt_job':2,'children':-1,'Never_worked':-2}).astype(np.uint8)","a06055f4":"# Inverse of Null Accuracy\nprint('Inverse of Null Accuracy: ',249\/(249+4861))\nprint('Null Accuracy: ',4861\/(4861+249))","d11e46bc":"X  = df[['gender','age','hypertension','heart_disease','work_type','avg_glucose_level','bmi']]\ny = df['stroke']\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.3, random_state=42)","5e8ef23e":"X_test.head(2)","3413b8b7":"# Our data is biased, we can fix this with SMOTE\n\noversample = SMOTE()\nX_train_resh, y_train_resh = oversample.fit_resample(X_train, y_train.ravel())","9859fc21":"# Models\n\n# Scale our data in pipeline, then split\n\nrf_pipeline = Pipeline(steps = [('scale',StandardScaler()),('RF',RandomForestClassifier(random_state=42))])\nsvm_pipeline = Pipeline(steps = [('scale',StandardScaler()),('SVM',SVC(random_state=42))])\nlogreg_pipeline = Pipeline(steps = [('scale',StandardScaler()),('LR',LogisticRegression(random_state=42))])\n\n\n\n#X = upsampled_df.iloc[:,:-1] # X_train_resh\n#Y = upsampled_df.iloc[:,-1]# y_train_resh\n\n#retain_x = X.sample(100)\n#retain_y = Y.loc[X.index]\n\n#X = X.drop(index=retain_x.index)\n#Y = Y.drop(index=retain_x.index)","c4afa804":"rf_cv = cross_val_score(rf_pipeline,X_train_resh,y_train_resh,cv=10,scoring='f1')\nsvm_cv = cross_val_score(svm_pipeline,X_train_resh,y_train_resh,cv=10,scoring='f1')\nlogreg_cv = cross_val_score(logreg_pipeline,X_train_resh,y_train_resh,cv=10,scoring='f1')","be31fe40":"print('Mean f1 scores:')\nprint('Random Forest mean :',cross_val_score(rf_pipeline,X_train_resh,y_train_resh,cv=10,scoring='f1').mean())\nprint('SVM mean :',cross_val_score(svm_pipeline,X_train_resh,y_train_resh,cv=10,scoring='f1').mean())\nprint('Logistic Regression mean :',cross_val_score(logreg_pipeline,X_train_resh,y_train_resh,cv=10,scoring='f1').mean())\n\n","95ac39c5":"rf_pipeline.fit(X_train_resh,y_train_resh)\nsvm_pipeline.fit(X_train_resh,y_train_resh)\nlogreg_pipeline.fit(X_train_resh,y_train_resh)\n\n#X = df.loc[:,X.columns]\n#Y = df.loc[:,'stroke']\n\nrf_pred   =rf_pipeline.predict(X_test)\nsvm_pred  = svm_pipeline.predict(X_test)\nlogreg_pred   = logreg_pipeline.predict(X_test)\n\nrf_cm  = confusion_matrix(y_test,rf_pred )\nsvm_cm = confusion_matrix(y_test,svm_pred)\nlogreg_cm  = confusion_matrix(y_test,logreg_pred )\n\nrf_f1  = f1_score(y_test,rf_pred)\nsvm_f1 = f1_score(y_test,svm_pred)\nlogreg_f1  = f1_score(y_test,logreg_pred)","f999b604":"print('Mean f1 scores:')\n\nprint('RF mean :',rf_f1)\nprint('SVM mean :',svm_f1)\nprint('LR mean :',logreg_f1)","806e5385":"from sklearn.metrics import plot_confusion_matrix, classification_report\n\nprint(classification_report(y_test,rf_pred))\n\nprint('Accuracy Score: ',accuracy_score(y_test,rf_pred))","1262e12e":"# Pretty good accuracy, but poor recall!\n# Unscaled and not upsampled negative\n\nfrom sklearn.model_selection import GridSearchCV\n\nn_estimators =[64,100,128,200]\nmax_features = [2,3,5,7]\nbootstrap = [True,False]\n\nparam_grid = {'n_estimators':n_estimators,\n             'max_features':max_features,\n             'bootstrap':bootstrap}","108635fc":"rfc = RandomForestClassifier()","e8ff3268":"#grid = GridSearchCV(rfc,param_grid)\n\n#grid.fit(X_train,y_train)","26727c93":"#grid.best_params_\n\n#{'bootstrap': True, 'max_features': 2, 'n_estimators': 100}","f1015ad5":"# Let's use those params now\n\nrfc = RandomForestClassifier(max_features=2,n_estimators=100,bootstrap=True)\n\nrfc.fit(X_train_resh,y_train_resh)\n\nrfc_tuned_pred = rfc.predict(X_test)","79084d03":"print(classification_report(y_test,rfc_tuned_pred))\n\nprint('Accuracy Score: ',accuracy_score(y_test,rfc_tuned_pred))\nprint('F1 Score: ',f1_score(y_test,rfc_tuned_pred))","20aba8a2":"penalty = ['l1','l2']\nC = [0.001, 0.01, 0.1, 1, 10, 100] \n\nlog_param_grid = {'penalty': penalty, \n                  'C': C}\nlogreg = LogisticRegression()\ngrid = GridSearchCV(logreg,log_param_grid)","f2eb90a8":"#grid.fit(X_train_resh,y_train_resh)","df917683":"#grid.best_params_\n\n#output:\n# {'C': 0.1, 'penalty': 'l2'}","0bbe10a0":"# Let's use those params now\n\nlogreg_pipeline = Pipeline(steps = [('scale',StandardScaler()),('LR',LogisticRegression(C=0.1,penalty='l2',random_state=42))])\n\nlogreg_pipeline.fit(X_train_resh,y_train_resh)\n\n#logreg.fit(X_train_resh,y_train_resh)\n\nlogreg_tuned_pred   = logreg_pipeline.predict(X_test)","a7f1d3cc":"print(classification_report(y_test,logreg_tuned_pred))\n\nprint('Accuracy Score: ',accuracy_score(y_test,logreg_tuned_pred))\nprint('F1 Score: ',f1_score(y_test,logreg_tuned_pred))","0808d827":"#source code: https:\/\/www.kaggle.com\/prashant111\/extensive-analysis-eda-fe-modelling\n# modified\n\nfrom sklearn.preprocessing import binarize\n\nfor i in range(1,6):\n    \n    cm1=0\n    y_pred1 = logreg_pipeline.predict_proba(X_test)[:,1]\n    y_pred1 = y_pred1.reshape(-1,1)\n    y_pred2 = binarize(y_pred1, i\/10)\n    y_pred2 = np.where(y_pred2 == 1, 1, 0)\n    cm1 = confusion_matrix(y_test, y_pred2)\n        \n    print ('With',i\/10,'threshold the Confusion Matrix is ','\\n\\n',cm1,'\\n\\n',\n            'with',cm1[0,0]+cm1[1,1],'correct predictions, ', '\\n\\n', \n           \n            cm1[0,1],'Type I errors( False Positives), ','\\n\\n',\n           \n            cm1[1,0],'Type II errors( False Negatives), ','\\n\\n',\n           \n           'Accuracy score: ', (accuracy_score(y_test, y_pred2)), '\\n\\n',\n           'F1 score: ', (f1_score(y_test, y_pred2)), '\\n\\n',\n           'Sensitivity: ',cm1[1,1]\/(float(cm1[1,1]+cm1[1,0])), '\\n\\n',\n           \n           'Specificity: ',cm1[0,0]\/(float(cm1[0,0]+cm1[0,1])),'\\n\\n',\n          \n            '====================================================', '\\n\\n')","07b55d48":"# source code: https:\/\/www.kaggle.com\/ilyapozdnyakov\/rain-in-australia-precision-recall-curves-viz\n# heeavily modified plotting\n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_recall_curve\n\nns_probs = [0 for _ in range(len(y_test))]\nlr_probs = logreg_pipeline.predict_proba(X_test)\nlr_probs = lr_probs[:, 1]\nns_auc = roc_auc_score(y_test, ns_probs)\nlr_auc = roc_auc_score(y_test, lr_probs)\n\n# calculate roc curves\nns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\nlr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n\n\n\ny_scores = logreg_pipeline.predict_proba(X_train)[:,1]\nprecisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)\n\n\n\n# Plots\n\nfig = plt.figure(figsize=(12,4))\ngs = fig.add_gridspec(1,2, wspace=0.1,hspace=0)\nax = gs.subplots()\n\nbackground_color = \"#f6f6f6\"\nfig.patch.set_facecolor(background_color) # figure background color\nax[0].set_facecolor(background_color) \nax[1].set_facecolor(background_color)\n\nax[0].grid(color='gray', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\nax[1].grid(color='gray', linestyle=':', axis='y',  dashes=(1,5))\n\n\n\ny_scores = logreg_pipeline.predict_proba(X_train)[:,1]\n\n\nprecisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)\n\nax[0].plot(thresholds, precisions[:-1], 'b--', label='Precision',color='#9bb7d4')\nax[0].plot(thresholds, recalls[:-1], '.', linewidth=1,label='Recall',color='#0f4c81')\nax[0].set_ylabel('True Positive Rate',loc='bottom')\nax[0].set_xlabel('Thresholds',loc='left')\n#plt.legend(loc='center left')\nax[0].set_ylim([0,1])\n\n\n# plot the roc curve for the model\nax[1].plot(ns_fpr, ns_tpr, linestyle='--', label='Dummy Classifer',color='gray')\nax[1].plot(lr_fpr, lr_tpr, marker='.', linewidth=2,color='#0f4c81')\nax[1].set_xlabel('False Positive Rate',loc='left')\nax[1].set_ylabel('')\nax[1].set_ylim([0,1])\n\nfor s in [\"top\",\"right\",\"left\"]:\n    ax[0].spines[s].set_visible(False)\n    ax[1].spines[s].set_visible(False)\n    \n    \nax[0].text(-0.1,2,'Model Selection: Considerations',fontsize=18,fontfamily='serif',fontweight='bold')\nax[0].text(-0.1,1.26,\n'''\nHere we observe how our Logistic Regression model performs when we change the threshold.\n\nWe'd like a model that predicts all strokes, but in reality, this would come at a cost.\nIn fact we can create a model that succeeds in that goal, but it would mean predicting\nmost people to have a stroke - which in itself would have negative effects.\n\nTherefore, we need to choose a model which not only predicts, correctly, those who have\nstrokes, but also those who do not.\n''',fontsize=14,fontfamily='serif')\n\n\nax[0].text(-0.1,1.1,'Precision & Recall',fontsize=14,fontfamily='serif',fontweight='bold')\nax[1].text(-0.1,1.1,'ROC: True Positives & False Positives',fontsize=14,fontfamily='serif',fontweight='bold')\n\nax[1].tick_params(axis='y', colors=background_color)\n\nplt.show()","d93a5ab5":"# defining parameter range \n#svm_param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n          #    'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n           #   'kernel': ['rbf']} \n\n#svm = SVC(random_state=42)\n\n#grid = GridSearchCV(svm,svm_param_grid)\n","bc4118a5":"#grid.fit(X_train_resh,y_train_resh)","d7d24151":"#grid.best_params_\n\n#output:\n# {'C': 1000, 'gamma': 0.01, 'kernel': 'rbf'}","f523f520":"# Let's use those params now\n\nsvm_pipeline = Pipeline(steps = [('scale',StandardScaler()),('SVM',SVC(C=1000,gamma=0.01,kernel='rbf',random_state=42))])\n\nsvm_pipeline.fit(X_train_resh,y_train_resh)\n\nsvm_tuned_pred   = svm_pipeline.predict(X_test)","8e8e4fe9":"print(classification_report(y_test,svm_tuned_pred))\n\nprint('Accuracy Score: ',accuracy_score(y_test,svm_tuned_pred))\nprint('F1 Score: ',f1_score(y_test,svm_tuned_pred))","5c709ba2":"# Make dataframes to plot\n\nrf_df = pd.DataFrame(data=[f1_score(y_test,rf_pred),accuracy_score(y_test, rf_pred), recall_score(y_test, rf_pred),\n                   precision_score(y_test, rf_pred), roc_auc_score(y_test, rf_pred)], \n             columns=['Random Forest Score'],\n             index=[\"F1\",\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])\n\nsvm_df = pd.DataFrame(data=[f1_score(y_test,svm_pred),accuracy_score(y_test, svm_pred), recall_score(y_test, svm_pred),\n                   precision_score(y_test, svm_pred), roc_auc_score(y_test, svm_pred)], \n             columns=['Support Vector Machine (SVM) Score'],\n             index=[\"F1\",\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])\n\nlr_df = pd.DataFrame(data=[f1_score(y_test,logreg_tuned_pred),accuracy_score(y_test, logreg_tuned_pred), recall_score(y_test, logreg_tuned_pred),\n                   precision_score(y_test, logreg_tuned_pred), roc_auc_score(y_test, logreg_tuned_pred)], \n             columns=['Tuned Logistic Regression Score'],\n             index=[\"F1\",\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])\n\n","fc009be4":"df_models = round(pd.concat([rf_df,svm_df,lr_df], axis=1),3)\nimport matplotlib\ncolors = [\"lightgray\",\"lightgray\",\"#0f4c81\"]\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\nbackground_color = \"#fbfbfb\"\n\nfig = plt.figure(figsize=(10,8)) # create figure\ngs = fig.add_gridspec(4, 2)\ngs.update(wspace=0.1, hspace=0.5)\nax0 = fig.add_subplot(gs[0, :])\n\nsns.heatmap(df_models.T, cmap=colormap,annot=True,fmt=\".1%\",vmin=0,vmax=0.95, linewidths=2.5,cbar=False,ax=ax0,annot_kws={\"fontsize\":12})\nfig.patch.set_facecolor(background_color) # figure background color\nax0.set_facecolor(background_color) \n\nax0.text(0,-2.15,'Model Comparison',fontsize=18,fontweight='bold',fontfamily='serif')\nax0.text(0,-0.9,'Random Forest performs the best for overall Accuracy,\\nbut is this enough? Is Recall more important in this case?',fontsize=14,fontfamily='serif')\nax0.tick_params(axis=u'both', which=u'both',length=0)\n\n\nplt.show()","6af84529":"\n# Plotting our results\n\ncolors = [\"lightgray\",\"#0f4c81\",\"#0f4c81\",\"#0f4c81\",\"#0f4c81\",\"#0f4c81\",\"#0f4c81\",\"#0f4c81\"]\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\n\nbackground_color = \"#fbfbfb\"\n\nfig = plt.figure(figsize=(10,14)) # create figure\ngs = fig.add_gridspec(4, 2)\ngs.update(wspace=0.1, hspace=0.8)\nax0 = fig.add_subplot(gs[0, :])\nax1 = fig.add_subplot(gs[1, :])\nax2 = fig.add_subplot(gs[2, :])\nax0.set_facecolor(background_color) # axes background color\n\n# Overall\nsns.heatmap(rf_cm, cmap=colormap,annot=True,fmt=\"d\", linewidths=5,cbar=False,ax=ax0,\n            yticklabels=['Actual Non-Stroke','Actual Stroke'],xticklabels=['Predicted Non-Stroke','Predicted Stroke'],annot_kws={\"fontsize\":12})\n\nsns.heatmap(logreg_cm, cmap=colormap,annot=True,fmt=\"d\", linewidths=5,cbar=False,ax=ax1,\n            yticklabels=['Actual Non-Stroke','Actual Stroke'],xticklabels=['Predicted Non-Stroke','Predicted Stroke'],annot_kws={\"fontsize\":12})\n\nsns.heatmap(svm_cm, cmap=colormap,annot=True,fmt=\"d\", linewidths=5,cbar=False,ax=ax2,\n            yticklabels=['Actual Non-Stroke','Actual Stroke'],xticklabels=['Predicted Non-Stroke','Predicted Stroke'],annot_kws={\"fontsize\":12})\n\n\n\nax0.tick_params(axis=u'both', which=u'both',length=0)\nbackground_color = \"#fbfbfb\"\nfig.patch.set_facecolor(background_color) # figure background color\nax0.set_facecolor(background_color) \nax1.tick_params(axis=u'both', which=u'both',length=0)\nax1.set_facecolor(background_color) \nax2.tick_params(axis=u'both', which=u'both',length=0)\nax2.set_facecolor(background_color)\n\nax0.text(0,-0.75,'Random Forest Performance',fontsize=18,fontweight='bold',fontfamily='serif')\nax0.text(0,-0.2,'The model has the highest accuracy, and predicts non-Strokes well.\\nThe recall is poor though.',fontsize=14,fontfamily='serif')\n\nax1.text(0,-0.75,'Logistic Regression Performance',fontsize=18,fontweight='bold',fontfamily='serif')\nax1.text(0,-0.2,'This model predicts strokes with most success.\\nHowever, it gives a lot of false-positives.',fontsize=14,fontfamily='serif')\n\nax2.text(0,-0.75,'Support Vector Machine Performance',fontsize=18,fontweight='bold',fontfamily='serif')\nax2.text(0,-0.2,'A very similar performance to Logistic Regression.\\nThe recall is slightly less though.',fontsize=14,fontfamily='serif')\n\n\nplt.show()","9e086514":"colors = [\"lightgray\",\"lightgray\",\"#0f4c81\"]\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\nbackground_color = \"#fbfbfb\"\n\nfig = plt.figure(figsize=(10,8)) # create figure\ngs = fig.add_gridspec(4, 2)\ngs.update(wspace=0.1, hspace=0.5)\nax0 = fig.add_subplot(gs[0, :])\nax1 = fig.add_subplot(gs[1, :])\n\nsns.heatmap(lr_df.T, cmap=colormap,annot=True,fmt=\".1%\",vmin=0,vmax=0.95,yticklabels='', linewidths=2.5,cbar=False,ax=ax0,annot_kws={\"fontsize\":12})\nfig.patch.set_facecolor(background_color) # figure background color\nax0.set_facecolor(background_color) \nax1.set_facecolor(background_color) \n\nax0.text(0,-2,'Tuned Logistic Regression Overview',fontsize=18,fontweight='bold',fontfamily='serif')\nax0.text(0,-0.3,\n'''\nA reminder of the results that the tuned model acheived.\nThe results are not perfect, but they do the best job at predicting those that will\nsuffer a stroke without sacrificing overall accuracy too much.\n\nIt has the highest f1 score of all models too - which is a weighted average of both\nprecision and recall.\n''',fontsize=14,fontfamily='serif')\nax0.tick_params(axis=u'both', which=u'both',length=0)\n\n\n\n# Overall\n\nsns.heatmap(logreg_cm, cmap=colormap,annot=True,fmt=\"d\", linewidths=5,cbar=False,ax=ax1,\n            yticklabels=['Actual Non-Stroke','Actual Stroke'],vmax=500,vmin=0,xticklabels=['Predicted Non-Stroke','Predicted Stroke'],annot_kws={\"fontsize\":12})\nax0.tick_params(axis=u'both', which=u'both',length=0)\nax1.tick_params(axis=u'both', which=u'both',length=0)\nplt.show()","c845e429":"\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'Feature':df.columns, 'Importance':m.feature_importances_}).sort_values('Importance', ascending=False)\n\n\nfi = rf_feat_importance(rf_pipeline['RF'], X)\nfi[:10].style.background_gradient(cmap=colormap)","a1116244":"background_color = \"#fbfbfb\"\n\nfig, ax = plt.subplots(1,1, figsize=(10, 8),facecolor=background_color)\n\ncolor_map = ['lightgray' for _ in range(10)]\ncolor_map[0] = color_map[1] = color_map[2] =  '#0f4c81' # color highlight\n\nsns.barplot(data=fi,x='Importance',y='Feature',ax=ax,palette=color_map)\nax.set_facecolor(background_color) \nfor s in ['top', 'left', 'right']:\n    ax.spines[s].set_visible(False)\n    \nfig.text(0.12,0.92,\"Feature Importance: Random Forest Stroke Prediction\", fontsize=18, fontweight='bold', fontfamily='serif')\n\n    \nplt.xlabel(\" \", fontsize=12, fontweight='light', fontfamily='serif',loc='left',y=-1.5)\nplt.ylabel(\" \", fontsize=12, fontweight='light', fontfamily='serif')\n\n\nfig.text(1.1, 0.92, 'Insight', fontsize=18, fontweight='bold', fontfamily='serif')\n\nfig.text(1.1, 0.315, '''\nIt is always interesting to view what features\na predictive model utilises the most, that is, \nwhat features are the most important. \nThis not only helps understand how the model\nworks, but importantly can help us to explain\nthe model results.\n\nIn this case, we see that Age, Average Glucose Level,\nand BMI are the most important factors for our model.\n\nOne also notices just how important Age is for our model,\nit is by far the most significant variable.\n\nIt is also interesting that Work Type is more salient\nthan Gender - this is a surprise.\n\nHaving a history of Heart Disease and Hypertension\nare also low in the importance ranking which again\nis very surprising.\n'''\n         , fontsize=14, fontweight='light', fontfamily='serif')\n\nax.tick_params(axis=u'both', which=u'both',length=0)\n\n\nimport matplotlib.lines as lines\nl1 = lines.Line2D([0.98, 0.98], [0, 1], transform=fig.transFigure, figure=fig,color='black',lw=0.2)\nfig.lines.extend([l1])\n\n\nplt.show()","ad658b4a":"# great resource: https:\/\/www.kaggle.com\/dansbecker\/advanced-uses-of-shap-values\nimport shap  \n\nexplainer = shap.TreeExplainer(rfc)\n\n# calculate shap values. This is what we will plot.\nshap_values = explainer.shap_values(X_test)\n","b8709bc2":"# custom colour plot\ncolors = [\"#9bb7d4\", \"#0f4c81\"]           \ncmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\nshap.summary_plot(shap_values[1], X_test,cmap=cmap,alpha=0.4)\n","7927e0a2":"\nshap.dependence_plot('age', shap_values[1], X_test, interaction_index=\"age\",cmap=cmap,alpha=0.4,show=False)\nplt.title(\"Age dependence plot\",loc='left',fontfamily='serif',fontsize=15)\nplt.ylabel(\"SHAP value for the 'Age' feature\")\nplt.show()","98ebce99":"shap.dependence_plot('bmi', shap_values[1], X_test, interaction_index=\"bmi\",cmap=cmap,alpha=0.4,show=False)\nplt.title(\"BMI dependence plot\",loc='left',fontfamily='serif',fontsize=15)\nplt.ylabel(\"SHAP value for the 'BMI' feature\")\nplt.show()","2d08a732":"import lime\nimport lime.lime_tabular\n\n# LIME has one explainer for all the models\nexplainer = lime.lime_tabular.LimeTabularExplainer(X.values, feature_names=X.columns.values.tolist(),\n                                                  class_names=['stroke'], verbose=True, mode='classification')","007afde0":"# Choose the jth instance and use it to predict the results for that selection\nj = 1\nexp = explainer.explain_instance(X.values[j], logreg_pipeline.predict_proba, num_features=5)\n","78f27bed":"# Show the predictions\nexp.show_in_notebook(show_table=True)","9aea322e":"import eli5\n\ncolumns_ = ['gender', 'age', 'hypertension', 'heart_disease', 'work_type',\n       'avg_glucose_level', 'bmi']\n\neli5.show_weights(logreg_pipeline.named_steps[\"LR\"], feature_names=columns_)","8da52c5f":"# SHAP explained\n\nThe plot above shows the effext of each data point on our predictions. \n\nFor example, for age, the top left point reduced the prediction by 0.6.\n\nThe color shows whether that feature was high or low for that row of the dataset\nHorizontal location shows whether the effect of that value caused a higher or lower prediction.\n\n\nWe can also see how our Random Forest Model is heavily skewed in favour of predicting no-strokes.\n\n# There's more? SHAP dependence plots\n\nWe can also focus on how the impact of each variable changes, as the variable itself changes.\n\nFor instance, Age. When this variable increases, the SHAP value also increases - pushing the patient closer to  our 1 condition (stroke). \n\nThis is also shown with colour -  pink\/red representing those who suffered a stroke.","d70baf99":"# Good accuracy, poor recall\n\nI will try using a grid search to find the optimal parameters for our Random Forest","e0abe3eb":"# Random Forest performed the best \n\nOften, tree methods will be the model of choice. \n\n# Now let's try it on the unseen negative data\n","7ef55ee7":"# General Overview\n\nWe've assessed a few variables so far, and gained some powerful insights. \n\nI'll now plot several variables in one place, so we can spot interesting trends or features.\n\nI will split the data in to 'Stroke' and 'No-Stroke' so we can see if these two populations differ in any meaningful way.","076bbc7e":"# One-Step Further: Logistic Regression with LIME\n\nWhen it comes to model interpretation, sometimes it is useful to unpack and focus on one example at a time.\n\nThe LIME package enables just that.\n\nLime stands for Local Interpretable Model-agnostic Explanations - here's an example:","833f80be":"# Bonus: Dive in to Random Forest's performance\n\nAs the model with the highest 'accuracy' (not the best metric for this project), I thought I would include a little more analysis so we can see how the Random Forest makes its predictions.","06464d19":"# What about Logistic Regression?\n\nLogistic Regression had the highest f1 score above, so perhaps we can tune that for better results.","14248128":"Let's plot this with the top 3 features highlighted","b93366d9":"# Selection\n\nI would opt for Logistic Regression. \n\nIt Has a decent accuracy, and the best recall. I feel that on balance it provides the best overall results.","57548105":"This needs to be considered when modelling of course, but also when formulating risk.\n\nStrokes are still relatively rare, we are not saying anything is guaranteed, just that risk is increasing.","0ab11340":"# Model Success\n\nSo all of our models have quite a high accuracy, the highest being 95% (Tuned Random Forest).\n\nBut the recall of Strokes is quite poor across the board.\n\nResults always need to be considered carefully - ask yourself: 'Why do I need to predict this value?'\nIn our case, I would assume it would be to offer medical advice \/ preventative treatment to those we predict will have a stroke, therefore, in the real-world, I would probably select the model with the highest recall.\n\n\nThe model's can be considered a success - that is, healthcare professionals would be better equipped with this model than without it.\n\n\nSeeing as Random Forest did have the highest accuracy, I will delve deeper in to the model and how it works - woth feature importance & LIME.\n\nHowever, the actual selection of model would be up for debate due to the recall variance. \n\n\n\n # Which model would you choose?\n \n I am curious to hear your opinions.","da208e76":"One can see how, by manipulating the threshold, we can catch more strokes. \n\nHowever, one needs to be careful with this approach. We could just change the threshold such that every patient is predicted to have a stroke so as not to miss any - but this helps no one.\n\nThe art is in finding the balance between 'hits' and 'misses'. \n\nF1 score is a decent starting point for this as it is the weighted average of several metrics.\n\nHere's a chart showing what I mean","0644939d":"# Model by Model Confusion Matrix\n\nNow we have selected our models, we can view how they performed in each prediction.\n\nA great way to visualise where your data performs well, and where it performs poorly.\n\n\n\n","66e65c0d":"# Insights\n\nThe plots above are quite enlightening.\n\nAs discussed earlier, we again note the importance of Age, amongst other things.","8efd572c":"So the hyper-parameter tuning has helped the Logisitc Regression model. It's recall score is much better than Random Forest's - even if the overall acuracy is down.\n\nHowever, we can maniupulate the threshold that our model uses to classify stroke vs no-stroke.\n\nLet's try that...","1d9f9eae":"# Thank you for reading\n","5ae68e6f":"# Conclusion\n\nWe started by exploring our data and noticed that certain features, such as Age, looked to be good indicators for predicting a stroke.\n\nAfter extensive visualization, we went on to try multiple models.\n\nRandom Forest, SVM, and Logistic Regression were all tried. \n\nI then tried hyperparameter tuning on all models to see if I could improve their results.\n\nWhile Random Fornegative had the highest accuracy, the Tuned Logistic Regression model provided the best recall and f1 score.\n\n**I therefore selected the Tuned Logistic Regression as my model**.\n\nHowever I wasn't done. To try to understand how Random Forest was using our data to get the highest accuracy score we looked at feature importance. I also introduced SHAP. This can be useful in understanding how models make predictions, and also where they might be going wrong. \n\nFinally, I used LIME & ELI5 on our chosen Logistic Regression model to show how the features interact with one another to produce a final prediction in the model. \n\nThis is a very powerful tool which can be used to help explain and demonstrate how machine learning models work to business stakeholders.\n","ac94a0f3":"We've **replaced all missing values**\n\nNow we can move to the next step","69b13734":"# Model preparation","41451e5b":"# Modelling\n\n# Can we predict whether or not an indiviudal will suffer a stroke?\n\nFirst, I will use the SMOTE (Synthetic Minority Over-sampling Technique) to balance our dataset.\n\nCurrently, as I mentioned above, there are many more negative examples of a stroke and this could hinder our model.\n\nThis can be addressed using SMOTE.","9b561c15":"Our data is now equal","6c15b8a9":"As we suspected, Age is a big factor, and also has slight relationships with BMI & Avg. Glucose levels.\n\nWe might understand intuitively that **as Age increases, the risk of having a stroke increases too, but can ve visualise this?**","feb207bc":"# Model Interpretation\n\nI'll use some valuable tools which help to uncover the supposed \"Black Box\" of machine learning algorithms.\n\nAs I always say, the models we create need to be sold to business stakeholders. And if business stakeholders don't understand what we're creating, they may be less likely to back the project.","83f4923b":"# Models \n\nI will model Random Forest, SVM, and Logisitc Regression for this classificatioin task. \n\nIn addition, I will utilise 10 fold cross validation.","300a121f":"# We can also use ELI5 for feature explanation\n\nELI5 stands for Explain it like I am 5 - a quirky name!\n\nHere we see the coefficient for each variable - in other words, what our Logistic model puts most value in.","747d2cc1":"# Insight\n\nBased on the above plots, it seems clear that **Age is a big factor** in stroke patients - the older you get the more at risk you are.\n\nThough less obvious, there are also differences in Avg. Glucose Levels and BMI.\n\nLet's explore those variables further...","3d424bf6":"# Exploring the data\n\nWe have now dealt with the missing values in the data.\n\nNext, I want to **explore** the data.\n\nDoes age makes one more likely to suffer a stroke? What about gender? Or BMI?\n\nThese are all questions that can be explored and answered with some data visulization. \n\nFirst, let's look at the numeric\/continuous variable distribtion","19a76f1a":"# Baseline\n\nFor such an imbalanced dataset, a useful baseline can be to beat the 'Null Accuracy', and in our case, since we're looking for the positive ('stroke'), I will take the inverse of that.\nIn other words, always predicting the most common outcome. \n\nFor this case, 249\/(249+4861) = 0.048\n\nSo a good target to beat would be 5%~ for recall for positive stroke patients.","28e7cd0a":"# Missing Data","42c25517":"This confirms what our intuitions told us. The older you get, the more at risk you get.\n\nHowever, you may have notices the low risk values on the y-axis. This is because the **dataset is highly imbalanced**.\n\nOnly 249 strokes are in our dataset which totals 5000 - around 1 in 20. ","d15094c1":"# Now we have a decision to make...\n\nThe tuned Random Forest gave us a much higher accuracy score of around 94%, but with a recall for Stroke patients of 2%. \n\nThe original model had an accuracy of 88%, but a recall for stroke patients of 24%.\n\nThis is often where domain knowledge comes in to play. \n\n**In my opinion**, the model is better off predicting those who will suffer a stroke, rather than predicting who will not. \n\n","92fe1660":"# For a browse\n\nhttps:\/\/www.kaggle.com\/joshuaswords","ea94bab4":"So we've **gained some understanding** on the distributiona of our numeric variables, **but we can add more information** to this plot. \n\nLet's see how the distribution of our numeric variables is different for **those that have strokes, and those that do not.**\n\nThis could be important for modelling later on","b6e3507b":"# We can also use SHAP\n\nSHAP Values (SHapley Additive exPlanations) break down a prediction to show the impact of each feature. \n\nIt interprets the impact of having a certain value for a given feature in comparison to the prediction we'd make if that feature took some baseline value (e.g. zero)\n\n\nIn this case I will use it for the Random Forest Model. It can be used for any type of model but it is by far the quickest with tree based models.\n\nIt is possible to change the colour values in SHAP plots too\n","d7bfa9ba":"# Welcome\n\nToday I will attempt to **predict whether or not an individual will suffer a stroke.**\n\nFirst, I will perform extensive data visualization. This will help me to see if there are any features that look to be indicative of a stroke, or indeed of not having a stroke.\n\nNext, I will build multiple models and select the best performing one. I will use f1 score as my primary metric as our dataset is imbalanced (though I will also resolve this with SMOTE).\n\n# Model Interpretation\n\n**I will also delve in to Model Interpretation**\nThis is incfedibly important in industry. Often we need to explain very technical algorithms to a non-technical audience, so any tool that can help this process should be mastered.\n\n\n\n# Please consider upvoting if you find my work useful\n\nLet's go...\n","eab0cae4":"The same plot, but with a more interesting varibale. \n\nHere we see a clear cutoff point for when strokes become far more common - after a BMI of around 30 or so.\n\nSuch is the power of SHAP visualization.","7f43c98d":"# For completeness, I will try to optimize SVM also","6224fade":"**How can we deal with blanks in our data?**\n\nThere are many ways. One can simply drop these records, fill the blanks with the mean, the median, or even simply the record before or after the missing value.\n\nBut there are other, more unusual ways. \n\nHere I will use a **Decision Tree to predict the missing BMI**\n\nOther interesting methods to explore could include using K-Nearest-Neighbours to fill the gaps."}}