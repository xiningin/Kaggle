{"cell_type":{"97f6b5f3":"code","28aad674":"code","2f18f1a7":"code","6bc92704":"code","8a883f80":"code","842f1fd0":"code","da549ee1":"code","04627f81":"code","145ff177":"code","6034675d":"code","3c36cebe":"code","328eec56":"code","9aa0fb5c":"code","567850e7":"code","80e2b0d2":"code","505f29b7":"code","80058cb9":"code","9ff3519d":"code","63f0381a":"code","d801a576":"code","189b905b":"markdown","f97d7328":"markdown","e4e7c1c4":"markdown","9a4addde":"markdown","4c7d9f37":"markdown","824eff77":"markdown","bf9bf5ed":"markdown","7de63fa1":"markdown","80fa6feb":"markdown","3c9f4fe3":"markdown","04172a1d":"markdown","2e26d95a":"markdown","af549475":"markdown","3aeb3017":"markdown","db291abe":"markdown","ff9dcfba":"markdown"},"source":{"97f6b5f3":"Xs = [\n        [0], \n        [1], \n        [2], \n        [3],\n        [4]\n]\nys = [0, 0, 1, 1, 2]\n\nfrom sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(Xs, ys)\n\nprint(neigh.predict([[4.1]])) #close to target `2`","28aad674":"neigh = KNeighborsClassifier(n_neighbors=3, weights='distance')\nneigh.fit(Xs, ys)\n\nprint(neigh.predict([[4.1]])) #close to target `2`","2f18f1a7":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split as split","6bc92704":"iris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = split(X, y, test_size=0.33, random_state=42)","8a883f80":"print(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","842f1fd0":"class Distances:\n    @staticmethod\n    def norm(X, p):\n        \"\"\"\n        X : 1-D Vector (np.ndarray)\n        P : Int (>= 1)\n        \n        Returns L2 Norm of 1-D Vector (np.ndarray)\n        \"\"\"\n        scalar = np.sum(np.power(X, p))\n        return np.power(scalar, (1\/p))\n    \n    @staticmethod\n    def euclidean(X1, X2):\n        \"\"\"\n        X1 - 1-D vector of len `m dims` (np.ndarray)\n        X2 - 1-D vector of len `m dims` (np.ndarray)\n        \"\"\"\n        if (X1.shape != X2.shape): \n            raise Exception('X1 and X2 must be 1-D vecs of same dims')\n        \n        # calculate diffs\n        X2_minus_X1 = X2-X1\n        # calculate L2 norm and return\n        return Distances.norm(X2_minus_X1, p=2)\n    \n    @staticmethod\n    def mse(X1, X2):\n        \"\"\"\n        X1 - 1-D vector of len `m dims` (list \/ np.ndarray)\n        X2 - 1-D vector of len `m dims` (list \/ np.ndarray)\n        \"\"\"\n        # validate\n        if (len(X1) != len(X2)): \n            raise Exception('X1 and X2 must be 1-D vecs of same dims')\n            \n        # claculate diffs\n        X2_minus_X1 = X2-X1\n        # return loss\n        dists = Distances.norm(X2_minus_X1, p=2)\n        # return mse (l2 square)\n        return (1\/len(X1)) * np.power(dists, 2)","da549ee1":"class KNN:\n    \"\"\"\n    TIME  : ~ O(mn)\n    SPACE : ~ O(mn) whole training set needed!\n    \"\"\"\n    def __init__(self, k=5, distance_metric=Distances.euclidean):\n        self.k = k\n        self.dist = distance_metric\n    \n    def fit(self, X_train, y_train):\n        \"\"\" Simply store \n        \n        SPACE: O(mn)\n        \"\"\"\n        self.X_train = X_train\n        self.y_train = y_train\n    \n    def predict(self, X_test):\n        y_pred = [self._predict(x_q) for x_q in X_test]\n        return y_pred\n    \n    def _predict(self, x_q):\n        # 1. get sorted distances and their idxs\n        asc_sorted_dist_idxs, asc_sorted_dists = self.__get_sorted_distance_idxs(x_q)\n        # 2. get top k classes w\/ least distances\n        top_k_ys, top_k_wts = self.__get_top_k_nearest_classes_w_wts(asc_sorted_dist_idxs, asc_sorted_dists)\n        # 3. majority vote from top_k_ys\n        pred_class = self.__majority_vote(top_k_ys, top_k_wts, n_top=1)\n        return pred_class[0]\n    \n    # ==================================================\n    # Helpers start\n    # ==================================================    \n    def __get_sorted_distance_idxs(self, x_q):\n        \"\"\" Returns indices of `n` sorted distances \n        \n        TIME  : O(nm)\n        \"\"\"\n        # validate\n        if (x_q.shape != self.X_train[0].shape): raise Exception('x_q and samples of X must be vecs of same dims')    \n        \n        # calculate distances\n        dists = np.array([self.dist(x_q, x_i) for x_i in self.X_train])\n        asc_idxs = np.argsort(dists) # O(nlogn). Could be made 0(1) by tracking the smallest in loop above\n        return asc_idxs, dists[asc_idxs]\n            \n    def __get_top_k_nearest_classes_w_wts(self, asc_dist_idxs, asc_sorted_dists):\n        \"\"\" \n        + Returns `k` class labels from which majority vote is taken \n        + `y` must have numeric class labels starting from `0`\n        \"\"\"\n        top_k_ys     = self.y_train[asc_dist_idxs][:self.k]\n        top_k_dists  = asc_sorted_dists[:self.k]\n        top_k_wts    = np.array([(1\/dist) for dist in top_k_dists])\n        return top_k_ys, top_k_wts\n        \n    @staticmethod\n    def __majority_vote(top_k_ys, top_k_wts, n_top=1):\n        \"\"\" `top_k_ys` must be a subset of numeric class labels starting from `0` \"\"\"\n        labels, cnts = np.unique(top_k_ys, return_counts=True)\n        # descending (need highest)\n        desc_cnts_idxs = np.argsort(cnts)[::-1]\n        # return mostly occuring label(s)\n        return labels[desc_cnts_idxs][:n_top]\n        \n    # ==================================================\n    # Helpers end\n    # ==================================================\n","04627f81":"def accuracy(Y1, Y2):\n    \"\"\" \n    Y1 and Y2 are 1-D vecs of same dims\n    of class labels in numeric representation (whole numbers) \n    \"\"\"\n    # validate\n    if len(Y1) != len(Y2): raise Exception(\"Y1 and Y2 are 1-D vecs of same dims\")\n        \n    true_if_same_else_false  = (Y1 == Y2)\n    num_correct              = np.sum(true_if_same_else_false)\n    percent_of_correct       = num_correct \/ len(Y1) \n    \n    return percent_of_correct","145ff177":"for k in range(1, 15):\n\n    # 1. initialize\n    clf = KNN(k=k, distance_metric=Distances.euclidean)\n    \n    # 2. Train (simply store)\n    clf.fit(X_train, y_train)\n    \n    # 3. get preds\n    y_pred = clf.predict(X_test)\n    \n    # check results and print\n    print(f\"k: {k} \\tacc: {accuracy(y_pred, y_test)}\")","6034675d":"class KNNRegression(KNN):\n    \"\"\"\n    Override `__majority_vote` with averages\/medians\n    \"\"\"\n    @staticmethod\n    def __majority_vote(top_k_ys, top_k_wts, n_top=1):\n        \"\"\"\n        `top_k_ys` ~ Continous rvs unlike numeric in classification above\n        \"\"\"\n        return np.mean(top_k_ys) # use np.median (robust to outliers)","3c36cebe":"np.random.seed(101)\ner = np.random.normal(0, 10, 100)\nxs = np.arange(0, 100, 1)\nys = 3*xs + 4 + er\n\nplt.scatter(xs, ys)\nplt.show()","328eec56":"X_train, X_test, y_train, y_test = split(xs, ys, test_size=0.33, random_state=42)\n\nX_train  = X_train.reshape(len(X_train), 1)\nX_test   = X_test.reshape(len(X_test), 1)","9aa0fb5c":"print(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","567850e7":"for k in range(1, 15):\n\n    # 1. initialize\n    clf = KNNRegression(k=k, distance_metric=Distances.euclidean)\n    \n    # 2. Train (simply store)\n    clf.fit(X_train, y_train)\n    \n    # 3. get preds\n    y_pred = clf.predict(X_test)\n    \n    # check results and print\n    print(f\"k: {k} \\terror: {Distances.mse(y_pred, y_test)}\")","80e2b0d2":"# Train w\/ k=1 (selected by hyperparam tuning)\n\nclf = KNNRegression(k=2, distance_metric=Distances.euclidean)\nclf.fit(X_train, y_train)\ny_test_preds = clf.predict(X_test)","505f29b7":"plt.figure(figsize=(10, 5))\n\nplt.scatter(np.squeeze(X_train), y_train, label=\"Train set\")\nplt.scatter(np.squeeze(X_test), y_test, label=\"Ground truth\")\nplt.scatter(np.squeeze(X_test), y_test_preds, label=\"Predictions\")\n\nplt.legend()\nplt.show()","80058cb9":"class WeightedKNN(KNN):\n    \"\"\"\n    Override `__majority_vote`\n    \"\"\"\n    @staticmethod\n    def __majority_vote(top_k_ys, top_k_wts, n_top=1):\n        \"\"\"\n        `top_k_ys` ~ Continous rvs unlike numeric in classification above\n        `top_k_wts` as in table above (1\/dists)\n        \"\"\"\n        # descending (need highest)\n        desc_idxs = np.argsort(top_k_wts)[::-1]\n        \n        return top_k_ys[desc_idxs][:n_top]","9ff3519d":"iris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = split(X, y, test_size=0.33, random_state=42)","63f0381a":"ks     = []\naccs   = {\n    'train' : [],\n    'cv'    : []\n}\nfor k in range(1, 150):\n    if (k%5 == 0):\n        # 1. initialize\n        clf = WeightedKNN(k=k, distance_metric=Distances.euclidean)\n\n        # 2. Train (simply store)\n        clf.fit(X_train, y_train)\n\n        # 3. get preds and acc for train and c.v\n        # train\n        y_pred_train   = clf.predict(X_train)\n        acc_train      = accuracy(y_pred_train, y_train)\n        # cross-val\n        y_pred_cv      = clf.predict(X_test)\n        acc_cv         = accuracy(y_pred_cv, y_test)\n        \n        # check results and print\n        print(f\"k: {k} \\ttrain_acc: {acc_train} \\tcv_acc: {acc_cv}\")\n\n        # history (for plots)\n        ks.append(k)\n        accs['train'].append(acc_train)\n        accs['cv'].append(acc_cv)","d801a576":"plt.plot(ks, accs['train'], label=\"Train Acc\")\nplt.plot(ks, accs['cv'], label=\"Validation Acc\")\nplt.plot(\n    ks, \n    [1 - acc for acc in accs['train']], \n    label=\"Train error\"\n)\nplt.plot(\n    ks, \n    [1 - acc for acc in accs['cv']], \n    label=\"Validation error\"\n)\n\nplt.xlabel(\"k\")\nplt.ylabel(\"accuracy \/ error\")\nplt.title(\"Hyperparameter Tuning\")\nplt.legend()\nplt.grid()\nplt.show()","189b905b":"# Test","f97d7328":"> **Not intuitive because of majority vote!** `4.1` is loser to `class 2` not `class 1`","e4e7c1c4":"# 02. Raw KNN Implementation (Regression w\/ Mean or Median)","9a4addde":"# 01. Raw KNN Implementation (Classification)","4c7d9f37":"> `k = 2` fits the best","824eff77":"> `k=10` fits the best\n\n*Why not chose initial `k`s even if their acc is low? [Cz, if k is less, overfits](https:\/\/www.kaggle.com\/l0new0lf\/knn-underfitting-overfitting)*\n\n**More scientific approach below(using plots and training set as well)**","bf9bf5ed":"# Algorithm\n\n\n\n## 01. KNN Classification (Regular)\n\n- Get distances of all points in training set from query point\n- Select `k` shortest distances and get respective classes\n- Majority vote from the `k` class labels (corresponding to `k` shortest distances)\n\n## 02. KNN Regression\n\n\n- **Mean:**\n  - Instead of taking majority vote in last step, take average\n  - Not robust to outliers\n  \n- **Median:**\n  - Instead of taking majority vote in last step, take median\n  - Robust to outliers\n  \n  \n## 03. Weighted KNN\n\n> If 3 classes of top-5 are `0`s and are **extremely far** and rest 2 classes of top-5 are `1`s and are **extremely closer to query point**, \n>\n> - Regular KNN predicts `class 0` (Majority vote by **max count of labels**)\n> - Regular KNN predicts `class 1` (Majority vote by **max of weights' sums of labels**)\n\n*Weighted KNN Makes more sense!!*\n\n**WEIGHTS**\n\nChose weights such that,\n$$Weight \\propto \\frac{1}{Distance}$$\nOne simple example is,\n$$Weight = \\frac{1}{Distance}$$\n\n| top-5-K Labels | distances from x_q | weights (Closer Dist $\\Rightarrow$ Larger Weight) |\n| --- | --- | --- |\n| 0 | 100 | $ \\frac{1}{100} = 0.010 $ |\n| 0 |  90 | $ \\frac{1}{90}  = 0.011 $ |\n| 0 |  60 | $ \\frac{1}{60}  = 0.016 $ |\n| 1 |   7 | $ \\frac{1}{7} = 0.143 $ |\n| 1 |   5 | $ \\frac{1}{5} = 0.200 $ |\n\n| Label | simple counts | sum of wts |\n| --- | --- | --- |\n|   0 |   3 | 0.010 + 0.011 + 0.016 = 0.037 |\n|   1 |   2 | 0.143 + 0.200 = 0.343 |\n\n- Prediction(Regular KNN) = MAX(simple counts) = `class 0`\n- Prediction(Weighted KNN) = MAX(sum of wts) = `class 1`","7de63fa1":"## **Which k to chose?**\n\n> - The one with High Generalisation\n> - Sweet spot (low bias and low variance)\n\n*The fig above is unlike usual fig in which generally, `val error > train error` @initial stages of training.* Chose `~sqrt(n=150) = 10` as optimal k","80fa6feb":"> Note how close predictons are to ground truths","3c9f4fe3":"> *Know more about distance metrics [here](https:\/\/www.kaggle.com\/l0new0lf\/ml-ds-dl-notebooks#05.-Metrics)*","04172a1d":"**Test**\n\n> Use MSE instead of acc. and plots to see how better model fits","2e26d95a":"> **Intuitive!!**","af549475":"# 03. Weighted KNN","3aeb3017":"**Prepare Dataset**","db291abe":"# Dataset","ff9dcfba":"**Cross validation**"}}