{"cell_type":{"6a06a505":"code","67b9c6aa":"code","02fc4d58":"code","628b7ba5":"code","6335cbb6":"code","9662276f":"code","51823e99":"code","2fc6d3cc":"code","05fe84d0":"code","86c2f501":"code","ef397a0b":"code","96cf0d36":"code","8a472c84":"code","161378b0":"code","cd88b2ca":"code","29bdcab7":"code","f73a2942":"code","37abc1c1":"code","30ab7d78":"code","d340c637":"code","a3dee189":"code","34250cb7":"code","e540131b":"code","ba994636":"code","940c39ad":"code","230ccd55":"code","6ee608fe":"code","d66955f6":"code","e1e7979f":"code","4a2f7fec":"markdown","0e416d75":"markdown","36dab220":"markdown","6419c4b5":"markdown","211dcc62":"markdown","232b5ffe":"markdown","01eed5fd":"markdown","1204c317":"markdown","9c4695a6":"markdown","04bf176a":"markdown","ec35a034":"markdown","2dd10d3d":"markdown","6e06ec1f":"markdown","b84697bd":"markdown","7286c260":"markdown","89e62b8d":"markdown","4a38b2c9":"markdown","766d2be0":"markdown","52bcfa7f":"markdown","c0941885":"markdown","52422e13":"markdown","5b27e90a":"markdown","94757c4f":"markdown"},"source":{"6a06a505":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","67b9c6aa":"import matplotlib.pyplot as plt\nimport numpy.linalg as nla\n\nfrom itertools import combinations\n\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.tsa.stattools import adfuller\n\nfrom statsmodels.tsa.vector_ar.var_model import VAR","02fc4d58":"df = pd.read_csv('\/kaggle\/input\/daily-climate-time-series-data\/DailyDelhiClimateTrain.csv')\n\n# Set the date column as index and change type to datetime (daily frequency)\ndf = df.set_index('date')\ndf.index = pd.to_datetime(df.index)\ndf = df.asfreq('D')\n\ndf.tail()","628b7ba5":"for quant in df.columns:\n    df[quant].plot()\n    plt.title(quant)\n    plt.xlabel('date')\n    plt.show()","6335cbb6":"k = 0.2\nhigh_pressure = df['meanpressure'] > df['meanpressure'].mean() + k*df['meanpressure'].std()\nlow_pressure = df['meanpressure'] < df['meanpressure'].mean() - k*df['meanpressure'].std()\n\ndf['meanpressure'][high_pressure | low_pressure] = None\ndf['meanpressure'].fillna(method='ffill',inplace=True)\n\ndf['meanpressure'].plot()\nplt.title('meanpressure')\nplt.xlabel('date')\nplt.show()","9662276f":"df.describe()","51823e99":"# Plot the acf and the Dickley-Fuller test for the columns of the passed dataframe\ndef stationary_test(df,columns,treshold = 0.01):\n\n    for quant in columns:\n        \n        # Remove null values (useful when plotting diff series)\n        time_series = df[quant][df[quant].notnull()]\n\n        # Autocorrelation function\n        plot_acf(time_series,title='Autocorrelation function - {}'.format(quant))\n        plt.xlabel('lags')\n        plt.show()\n\n        # Dickely-Fuller test statistics\n        dickley_fuller_test = adfuller(time_series)\n        pvalue = dickley_fuller_test[1]\n\n        if pvalue < treshold:\n            print('Time series \"{}\" is stationay (NULL HP rejected - pvalue = {:.4f})'.format(quant,pvalue))\n        else:\n            print('Time series \"{}\" might have unit root (NULL HP cannot be rejected - pvalue = {:.4f})'.format(quant,pvalue))","2fc6d3cc":"stationary_test(df, df.columns)","05fe84d0":"df['diff_meantemp'] = df['meantemp'].diff()\ndf['diff_meanpressure'] = df['meanpressure'].diff()","86c2f501":"diff_columns = ['diff_meantemp','diff_meanpressure']\nstationary_test(df, diff_columns)","ef397a0b":"train_mean = df.mean()\ntrain_std = df.std()","96cf0d36":"def standardize(time_series):\n    return (time_series - time_series.mean())\/(time_series.std())\n\ndf = df.apply(standardize)","8a472c84":"df.columns","161378b0":"columns = ['humidity', 'wind_speed','diff_meantemp','diff_meanpressure']\n\nendogenous = df[columns].dropna() # Remove the first row since we are using diff time series as well\ntime_dates = endogenous.index","cd88b2ca":"model = VAR(endogenous,\n            dates=time_dates,\n            freq=df.index.freq\n           )\n\nresults = model.fit(maxlags=25,\n                    ic='aic',\n                    verbose=1\n                   )","29bdcab7":"print(results.summary()._resid_info())","f73a2942":"normality = results.test_normality()\nprint(normality.summary())","37abc1c1":"# Residuals for the 4 equations we have\nresiduals = results.resid.to_numpy()\nN,_ = residuals.shape\n\n# Covariance matrix for the residuals\nres_cov = np.cov(residuals.T)\n\n# Let's consider the distribution of the residuals along the direction where the variance is higher (PCA)\neigvals, eigvecs = nla.eigh(res_cov)\npc_residuals = residuals @ eigvecs[:,-2:]\n\n# Let's plot the distribution\nplt.hist2d(*pc_residuals.T,bins=30,density=True)\nplt.title('residual distribution - 2D ')\nplt.colorbar()\nplt.show()","30ab7d78":"for quant in columns:\n    plot_acf(results.resid[quant],title='Autocorrelation function - residual {}'.format(quant))","d340c637":"treshold = 0.05\n\n# Check if the lags of a quantity help predict its current value \nfor quant in columns:\n    \n    causality = results.test_causality(quant, quant, kind='f')\n\n    if causality.pvalue < treshold:\n        print('{} lags might Granger-cause its current value (NULL HP rejected - pval = {:.4f})'.format(quant, causality.pvalue))\n        print('---')\n\ncausally_related = []\n        \n# Check if one quantity causes the other\nfor quantA, quantB in combinations(columns,2):\n    \n    bidirectional = [(quantA,quantB),(quantB,quantA)]\n    \n    for caused, causing in bidirectional:\n    \n        causality = results.test_causality(caused, causing, kind='f')\n\n        if causality.pvalue < treshold:\n            print('{} might Granger-cause {} (NULL HP rejected - pval = {:.4f})'.format(causing, caused, causality.pvalue))\n            print('---')\n            causally_related.append((causing, caused))","a3dee189":"# Impulse response function for the model we trained\nirf = results.irf(periods=10)\n\nfor causing, caused in causally_related:\n    irf.plot(orth=True, impulse=causing, response=caused, figsize=(12,4))\n    plt.suptitle('')\n    plt.xlabel('steps ahead')\n    plt.show()","34250cb7":"df_test = pd.read_csv('\/kaggle\/input\/daily-climate-time-series-data\/DailyDelhiClimateTest.csv')\n\n# Set the date column as index and change type to datetime (daily frequency)\ndf_test = df_test.set_index('date')\ndf_test.index = pd.to_datetime(df_test.index)\ndf_test = df_test.asfreq('D')\n\ndf_test.tail()","e540131b":"for quant in df_test.columns:\n    df_test[quant].plot()\n    plt.title(quant)\n    plt.xlabel('date')\n    plt.show()","ba994636":"df_test['meanpressure'][df_test['meanpressure'] < 900] = None\ndf_test['meanpressure'].plot()\nplt.title('meanpressure')\nplt.xlabel('date')\nplt.show()","940c39ad":"df_test.describe()","230ccd55":"# Get the diff for temperature and pressure\ndf_test['diff_meantemp'] = df_test['meantemp'].diff()\ndf_test['diff_meanpressure'] = df_test['meanpressure'].diff()\n\n# Drop the temperature and pressure\ndf_test.drop(columns = ['meantemp','meanpressure'], inplace=True)","6ee608fe":"for quant in df_test.columns:\n    mean = train_mean[quant]\n    std = train_std[quant]\n    df_test[quant] = df_test[quant].apply(lambda val : (val-mean)\/std)","d66955f6":"N_test,_ = df_test.shape\n\nestimate,lower,upper = results.forecast_interval(y=results.endog,steps=N_test)\n\nfor i,quant in enumerate(df_test.columns):\n    df_test[quant].plot(figsize=(12,4))\n    plt.plot(df_test.index,estimate[:,i],'k--',label='mean estimate')\n    plt.plot(df_test.index,lower[:,i],'-.',c='gray',label='CI 95%')\n    plt.plot(df_test.index,upper[:,i],'-.',c='gray')\n    plt.legend()\n    plt.title('{} forcast - multiple steps ahead'.format(quant))\n    plt.show()","e1e7979f":"# Join the training and test datasets\nfull_df = pd.concat((df[df_test.columns],df_test))\nfull_df.fillna(method='ffill',inplace=True)\n\n_,p = full_df.shape\n\nendogenous_var = full_df.to_numpy()\n\nestimate_array = np.empty(shape=(0,p))\nlower_array = np.empty(shape=(0,p))\nupper_array = np.empty(shape=(0,p))\n\n# Compute the forcast one step ahead\nfor i in range(N_test,0,-1):\n\n    estimate,lower,upper = results.forecast_interval(y=endogenous_var[:-i],steps=1)\n    \n    estimate_array = np.vstack((estimate_array,estimate))\n    lower_array = np.vstack((lower_array,lower))\n    upper_array = np.vstack((upper_array,upper))\n    \n# Plot the forcast \nfor i,quant in enumerate(df_test.columns):\n    df_test[quant].plot(figsize=(12,4))\n    plt.plot(df_test.index,estimate_array[:,i],'k--',label='mean estimate')\n    plt.plot(df_test.index,lower_array[:,i],'-.',c='gray',label='CI 95%')\n    plt.plot(df_test.index,upper_array[:,i],'-.',c='gray')\n    plt.legend()\n    plt.title('{} forcast - 1 step ahead'.format(quant))\n    plt.show()","4a2f7fec":"# Stationarity\n\nFrom the above plots, we see that the time-series are not stationary. We now sistematically check using the acf and the Dickley-Fuller test. Recall, NULL HP is that a unit root is present (that is, the time-series is not stationary)","0e416d75":"The residuals are not Normally distributed. We cannot visualize the distribution in 4D,","36dab220":"# Analysis of the residuals\n\n## Correlations\n\nLet's check the properties of the residuals, starting with thier correlation matrix.\n\nThis information will be helpful when we try to study the impulse response function (see below), since one cannot really assume that the shock is only applied to one of the error, if there are correlations between errors.\n\nFrom the result below, we see that indeed there are correlations between errors.","6419c4b5":"Let's now take the difference for temperature and pressure","211dcc62":"Some of the results of the causality test seem to make sense. For example, the change in temperature on previous days might indeed influence the current humidity, and wind during previous days might influence the change in temperature.\n\n## Impulse response\n\nTo better undesrstand how the change of one quantity in the past influence another quantity in the present, we can use Impule Response. The idea is that we start with a unit change for a single quantity at $n$ lags, and study how this shock propagate in another variable.\n\nWe will consider the impulse response function for those variables that are Grenger-causally related.","232b5ffe":"## Normality\n\nWe can now use the Jarque-Bera test for Normality of the residuals, that conside how skewed a the residual distribution is, and how fat the tails are, compared to the Gaussian distribution.","01eed5fd":"Before moving on, we standardize the time series (this is not strictly necessary, but might be helpful when we try to understand the contribution of different terms to a certain quantity).\n\nBefore doing so, let's record the mean and std of the training variables, to properly compare the results with the test set during the forcast section.","1204c317":"We see how, for example, a past higher humidity negatively affects the future wind speed, and how a past positive change in temperature positively affects the future humidity.","9c4695a6":"It is immediately clear that there are outlier in the presure (0 atm or 8 atm is a bit to low\/high for life on Earth I think). We can remove the outliers and replace them with the value of the pressure on the previous day.","04bf176a":"Let's plot the test time series,","ec35a034":"Are the diff time series stationary now?","2dd10d3d":"As a first step, let us visualize the data to get a better idea pf what we are dealing with,","6e06ec1f":"## Out-of-sample forcasting (full test series)\n\nLet use our model for out-of-sample forcasting, using only the information we have from the training series. In this way, we predict the next N_test stesp ahead in the future, with N_test ~ 100 here.","b84697bd":"# Vector AutoRegression (VAR)\n\nIn this notebook we fit a VAR model to climate data collected in Delhi between 2013 and 2017. Specifically, we will\n\n- analyse the time series, clean them and remove possible outliers.\n- check for stationarity and differencing the variables that have unit root.\n- fit a model and chose it using a multi-variate version of AIC.\n- check the residuals (correlations, autocorrelations, normality).\n- check causality to better understand the model (Granger-causality, impulse response functions).\n- use the model to forcast climate conditions in Delhi.\n\n## Load the time series and analyse it\n\nLet's load the data for training the VAR model. The four quantities collected are\n\n- Mean temperature averaged out from multiple 3 hour intervals in a day.\n- Humidity value for the day (units are grams of water vapor per cubic meter volume of air).\n- Wind speed measured in kmph.\n- Pressure reading of weather (measure in atm).","7286c260":"# Forcasting\n\nLet's now use our model to do some forcasting. We can compare our prediction with a test set that we have not yet considered.\n\nBefore we can use the test set, the usual checks are necessary, in particularly to be sure that no outliers are present.","89e62b8d":"The plots of the residuals show that there is no autocorrelations in each residuals, as we would expect if the model used gave a good fit to the data.\n\n# Causality tests\n\nWe can test for causality between the quantities (that is, if the lags of a quantity, the causing one, are helpful for estimating the current value of another quantity, the casued one).\n\nThis test is known as **Granger-causality**. It is performed as an F test, comparing the sum of residuals for the casued quantity between the unrestricted model and the restricted one, with the coefficients of the causing lags set to 0.\n\nThe NULL HP is that the quantity A is not causing quantity B (the RSS of the unrestricted\/unrestricted models are not statistically different).","4a38b2c9":"Again, pressure is behaving weirdly. Let set the values below 0.9 atm to None (we do not need to replace them, since we do not train a model with them).","766d2be0":"Based on the test and the acfs, we can procede by first differencing meantemperature and meanpressure, so that the four time-series are all stationary.","52bcfa7f":"# Fitting the model\n\nWe are now going to fit a VAR (vector auto-regression) model to the data. In order to select the optimal lag, we use two multivariate information criterion (AIC and BIC). ","c0941885":"This marginal distribution of a linear combination of residuals is probably not telling us much about the overall distribution of the residuals, but we can see, at least, that this marginal distribution is indeed a bit skewed.\n\n## Autocorrelations\n\nLet's check the autocorrelation functions of the different residuals (they are not white noise since they are correlated, but are autocorrelations present as well?)","52422e13":"As we might have expected, the model is not great at forcasting very far away in the future. We can now compare its preformance on the test set in forcasting one step ahead.\n\n## Out-of-sample forcasting (1 step ahead)\n\nWe repeat the study but this time we use the test values up to a cetrain time $t$ to estimate the value at time $t+1$. As we see below, our model is able to predict pretty well one day in advance.","5b27e90a":"Let's test for stationary of each time-series individually","94757c4f":"We can now standardize the data using the same parameters used for the training set,"}}