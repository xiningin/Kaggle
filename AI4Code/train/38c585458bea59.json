{"cell_type":{"27d33bc9":"code","d232e144":"code","4a1efb57":"code","2879e994":"code","b61cd405":"code","d3d970ae":"code","6b3d9ec4":"code","4fe1478b":"code","4ee77769":"code","ddb7f3e0":"code","e4b01137":"code","7ef307ac":"code","a1a05081":"code","56981c98":"code","f188aa98":"code","b2857417":"code","154d32b5":"code","56086f64":"markdown","3492331f":"markdown","a0971a3b":"markdown","7fd961f8":"markdown","d8db8d69":"markdown","bcb6a0e3":"markdown","5dabdb6b":"markdown"},"source":{"27d33bc9":"from tensorflow.keras.layers import Dense, Input, BatchNormalization, Dropout, concatenate\nfrom tensorflow.keras.models import Sequential, Model\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\nfrom sklearn.model_selection import train_test_split","d232e144":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4a1efb57":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","2879e994":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","b61cd405":"# preprocessing \n\ndef refactor(data):   \n    \n    #Sex onehot transform\n    data['Sex'] = data['Sex'].astype(\"category\")\n    data['Sex'] = data['Sex'].cat.codes\n    \n    #Cabin onehot transform\n    data['Cabin'] = data['Cabin'].astype(\"category\")\n    data['Cabin'] = data['Cabin'].cat.codes\n    \n    #Embarked onehot transform\n    data['Embarked'] = data['Embarked'].astype(\"category\")\n    data['Embarked'] = data['Embarked'].cat.codes\n    \n    \n    #return refactored data\n    return data","d3d970ae":"train_data_categorical = refactor(train_data)\ntest_data_categorical  = refactor(test_data)","6b3d9ec4":"#create imputer\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\n\n#look at the current data\ntrain_data_categorical.head()","4fe1478b":"for key in train_data_categorical.keys():\n    try:\n        print(key, \" correlation: \", train_data_categorical[key].corr(train_data_categorical['Survived']))\n    except:\n        pass","4ee77769":"features = ['Pclass', 'Sex', 'Fare', 'Cabin', 'Embarked']\n\nX, y = train_data_categorical[features].to_numpy(), train_data_categorical['Survived'].to_numpy()","ddb7f3e0":"print(\"X shape: \", X.shape)\nprint(\"y shape: \", y.shape)","e4b01137":"# split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y)","7ef307ac":"inputs = Input(shape=[5,])\nnormalization = BatchNormalization()(inputs)\n\n#path 1\npath1 = Dense(100, kernel_initializer='he_uniform', activation='selu')(normalization)\npath1 = BatchNormalization()(path1)\npath1 = Dense(100, kernel_initializer='he_uniform', activation='selu')(path1)\npath1 = BatchNormalization()(path1)\npath1 = Dense(50, kernel_initializer='he_uniform', activation='selu')(path1)\npath1 = BatchNormalization()(path1)\n\n#path 2\npath2 = Dense(100, kernel_initializer='he_uniform', activation='selu')(normalization)\npath2 = BatchNormalization()(path2)\npath2 = Dense(50, kernel_initializer='he_uniform', activation='selu')(path2)\npath2 = BatchNormalization()(path2)\n\n#cistern\nconcat = concatenate([path1, path2])\nconcat = BatchNormalization()(concat)\ncenter1 = Dense(50, kernel_initializer='he_uniform', activation='selu')(concat)\ncenter1 = BatchNormalization()(center1)\ncenter1 = Dense(50, kernel_initializer='he_uniform', activation='selu')(center1)\ncenter1 = BatchNormalization()(center1)\noutputs = Dense(1, kernel_initializer='he_uniform', activation='sigmoid')(center1)\n\nmodel = Model(inputs=[inputs], outputs=[outputs])\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nmodel.summary()","a1a05081":"model.fit(X_train, y_train, batch_size=32, epochs=20)","56981c98":"# 7. Evaluate the model","f188aa98":"model.evaluate(X_test, y_test)","b2857417":"X_pred = test_data_categorical[features].to_numpy()\ntest_predictions = [1 if s >= 0.5 else 0 for s in model.predict(X_pred)]\nmy_submission = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': test_predictions})\nmy_submission.to_csv('final_submission.csv', index=False)","154d32b5":"my_submission.head()","56086f64":"# 5. Build the model","3492331f":"# 1. Import your data and variables","a0971a3b":"# 3. Refactor your data","7fd961f8":"# 8. Send predictions to kaggle","d8db8d69":"# 2. Get the data:","bcb6a0e3":"# 4. Visualize the Data:","5dabdb6b":"# 6. Fit the model"}}