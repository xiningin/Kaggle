{"cell_type":{"aec8e4b5":"code","2fe5cd0c":"code","e386edaa":"code","1e294176":"code","5389061b":"code","140f6089":"code","be68d929":"code","8f2ce41e":"code","c1fee3b0":"code","dd9fc3d3":"code","511886e3":"code","08c450cc":"code","f58da7ce":"code","2a1bff20":"code","0363c689":"code","39909d62":"code","d24c549b":"code","19bb8295":"markdown","11168172":"markdown","f210cc2b":"markdown","c4348dc1":"markdown","35c775cf":"markdown","33ed58cf":"markdown","3d67604f":"markdown","f1d976ed":"markdown","0e20411c":"markdown","12718578":"markdown","24cc4414":"markdown","42ca7082":"markdown","ee3d9856":"markdown"},"source":{"aec8e4b5":"!pip install yake\n!pip install langdetect","2fe5cd0c":"import numpy as np\nimport pandas as pd\nimport os\nimport argparse\nimport json\nimport torch\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nfrom scipy import sparse\nfrom texttable import Texttable\nimport yake\n\nimport community\nfrom tqdm import tqdm\nfrom itertools import combinations\nimport itertools\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\nfrom itertools import permutations, combinations\n\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport numpy as np\nnp.random.seed(2018)\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport ast\nfrom collections import Counter","e386edaa":"read_data_All = pd.read_pickle('\/kaggle\/input\/inputs-covid-clustering\/ALL_study_with_wiki_columns.pkl')\nref_table = pd.read_pickle('\/kaggle\/input\/inputs-covid-clustering\/res_ref.pkl')\nmain_Data_csv = pd.read_csv('\/kaggle\/input\/inputs-covid-clustering\/title_rp.csv')\nedge_list = pd.read_pickle('\/kaggle\/input\/inputs-covid-clustering\/result__1.pkl')","1e294176":"#Functions to Extract Wikifier Keywords and the Category of the Keywords in a List Format\n\ndef extract_response_title(response):\n    all_wrds=[]\n    try:\n        for annotation in response[\"annotations\"]:\n            if annotation[\"cosine\"]>0.1:\n                if len(annotation[\"dbPediaTypes\"])>0:\n                    all_wrds.append(annotation[\"title\"])\n    except:\n        1==1\n    return [key for key, value in Counter(all_wrds).most_common()] \n\ndef extract_response_val(response):\n    all_wrds=[]\n    try:\n        for annotation in response[\"annotations\"]:\n            if annotation[\"cosine\"]>0.1:\n                all_wrds.append(annotation[\"dbPediaTypes\"])\n    except:\n        1==1\n    all_wrds=[k for j in all_wrds for k in j ]\n    return [key for key, value in Counter(all_wrds).most_common()]  ","5389061b":"read_data_All['title_kw']=read_data_All['all_txt_resp'].apply(lambda x: extract_response_title(x))\nread_data_All['Cat_mach']=read_data_All['all_txt_resp'].apply(lambda x: extract_response_val(x))\nread_data_All=read_data_All[['title','title_kw','Cat_mach']]\nread_data_All.head(2)","140f6089":"main_Data_csv_new = main_Data_csv.merge(ref_table,left_on='sha',right_on='paper_id',how='inner')\nmain_Data_csv_new = main_Data_csv_new.merge(main_Data_csv_new[['pid','title_x']].drop_duplicates(),left_on=['ref_title_y'],right_on=['title_x'],how='inner')\nmain_Data_csv_new = main_Data_csv_new[['pid_x','pid_y']]\n\nedge_list.columns = ['pid_x','pid_y']\nedge_list = edge_list.append(main_Data_csv_new)\nedge_list = edge_list.merge(main_Data_csv,left_on='pid_x',right_on='pid',how='left')\nread_data_All = read_data_All.merge(main_Data_csv[['pid','title']],on='title',how='right')\n\n#Save to Pickle Files\n# ref_table.to_pickle('Graph_adj_matrix.pkl')\n# read_data_All.to_pickle('Graph_kw_matrix.pkl')","be68d929":"# Find out the PIDs of Research Papers which occurs more than once in the Repository\n\nread_data_All['title_kw'] = read_data_All['title_kw'].apply(lambda x:str(x))\nread_data_All['Cat_mach'] = read_data_All['Cat_mach'].apply(lambda x:str(x))\ncheck_ = read_data_All.groupby(by = 'pid', as_index = False)['title_kw'].count().sort_values(by = 'title_kw', ascending = False)\nremove_pid = list(check_[check_['title_kw'] > 1]['pid'])\n\n#Save to Pickle Files\npd.DataFrame(remove_pid).to_pickle('Remove_PID.pkl')\n\n#Remove these PIDs from the Final Graph Edge List\nskills_cluster_file = edge_list[['pid_x','pid_y']]\nskills_cluster_file = skills_cluster_file[~skills_cluster_file['pid_x'].isin(remove_pid)].reset_index(drop = True)\nskills_cluster_file = skills_cluster_file[~skills_cluster_file['pid_y'].isin(remove_pid)].reset_index(drop = True)","8f2ce41e":"graph = nx.from_edgelist(skills_cluster_file.values.tolist())","c1fee3b0":"class EgoNetSplitter(object):\n    \"\"\"\n    A lightweight implementation of \"Ego-Splitting Framework: from Non-Overlapping to Overlapping Clusters\". \n    Paper: https:\/\/www.eecs.yorku.ca\/course_archive\/2017-18\/F\/6412\/reading\/kdd17p145.pdf\n    Video: https:\/\/www.youtube.com\/watch?v=xMGZo-F_jss\n    Slides: https:\/\/epasto.org\/papers\/kdd2017-Slides.pdf\n    \"\"\"\n    def __init__(self, graph, resolution):\n        \"\"\"\n        :param graph: Networkx object.\n        :param resolution: Resolution parameter of Python Louvain.\n        \"\"\"\n        self.graph = graph\n        self.resolution = resolution\n\n    def create_egonet(self, node):\n        \"\"\"\n        Creating an ego net, extracting personas and partitioning it.\n        :param node: Node ID for egonet (ego node).\n        \"\"\"\n        ego_net_minus_ego = self.graph.subgraph(self.graph.neighbors(node))\n        components = {i: nodes for i, nodes in enumerate(nx.connected_components(ego_net_minus_ego))}\n        new_mapping = {}\n        personalities = []\n        for k, v in components.items():\n            personalities.append(self.index)\n            for other_node in v:\n                new_mapping[other_node] = self.index \n            self.index = self.index +1\n        self.components[node] = new_mapping\n        self.personalities[node] = personalities\n\n    def create_egonets(self):\n        \"\"\"\n        Creating an egonet for each node.\n        \"\"\"\n        self.components = {}\n        self.personalities = {}\n        self.index = 0\n        print(\"Creating egonets.\")\n        for node in tqdm(self.graph.nodes()):\n            self.create_egonet(node)\n\n    def map_personalities(self):\n        \"\"\"\n        Mapping the personas to new nodes.\n        \"\"\"\n        self.personality_map = {persona: node for node in self.graph.nodes() for persona in self.personalities[node]}\n\n    def create_persona_graph(self):\n        \"\"\"\n        Create a persona graph using the egonet components.\n        \"\"\"\n        print(\"Creating the persona graph.\")\n        self.persona_graph_edges = [(self.components[edge[0]][edge[1]], self.components[edge[1]][edge[0]]) for edge in tqdm(self.graph.edges())]\n        self.persona_graph = nx.from_edgelist(self.persona_graph_edges)\n\n    def create_partitions(self):\n        \"\"\"\n        Creating a non-overlapping clustering of nodes in the persona graph.\n        \"\"\"\n        print(\"Clustering the persona graph.\")\n        self.partitions = community.best_partition(self.persona_graph, resolution=self.resolution)\n        self.overlapping_partitions = {node: [] for node in self.graph.nodes()}\n        for node, membership in self.partitions.items():\n            self.overlapping_partitions[self.personality_map[node]].append(membership)","dd9fc3d3":"def create_clusters_(x):\n    print('Resolution : ',x)\n    splitter = EgoNetSplitter(graph, x)\n    splitter.create_egonets()\n    splitter.map_personalities()\n    splitter.create_persona_graph()\n    splitter.create_partitions()\n\n    b = list(splitter.overlapping_partitions.values())\n    \n    ego_focal_points = []\n    for vals in b:\n        ego_focal_points.extend(vals)\n\n    c = list(set(ego_focal_points))\n\n    node_df = pd.DataFrame({'nodes':list(splitter.overlapping_partitions.keys()), 'focal_points':list(splitter.overlapping_partitions.values())})\n    for vals in c:\n        node_df[vals] = node_df['focal_points'].apply(lambda x:x.count(vals))\n        \n    node_df['count_focal_points'] = node_df['focal_points'].apply(lambda x:len(x))\n\n    cl_ = list(node_df['focal_points'])\n\n    c_ = []\n    for ele in cl_:\n        c_.extend(ele)\n\n    c_ = list(set(c_))\n    num_nodes = max(c_)\n\n    sk_ = []\n    for i in range(num_nodes):\n        sk_.append(list(node_df[node_df[i] == 1]['nodes']))\n\n    sk__ = pd.DataFrame(columns = ['nodes_in_cluster'])\n    sk__['nodes_in_cluster'] = sk_\n\n    sk__['cluster'] = sk__.index\n    sk__['total_cluster_size']=sk__['nodes_in_cluster'].apply(lambda x:len(x))\n    \n    print(sk__[sk__['total_cluster_size'] > 5].shape)\n    file_name = 'Save_Clusters_Res_'+str(x)+'.pkl'\n    sk__[sk__['total_cluster_size'] > 5].sort_values(by = 'total_cluster_size', ascending = False).to_pickle(file_name)\n    print('----------------------------------------------------------------------')\n    \nlist_res = [0.1, 0.8, 0.03]\nfor res in list_res:\n    create_clusters_(res)","511886e3":"def unstack_(dataframe, column_name):\n    lst_col = column_name\n    dataframe = pd.DataFrame({col:np.repeat(dataframe[col].values, dataframe[lst_col].str.len())for col in dataframe.columns.difference([lst_col])}).assign(**{lst_col:np.concatenate(dataframe[lst_col].values)})[dataframe.columns.tolist()]\n    return dataframe","08c450cc":"#Function to extract Keywords using yake package\ndef yake_generator(text):\n    text = re.sub(r'\u00c2',\"\",text)\n    text = re.sub(r'\u00a9',\"\",text)\n    text = re.sub(r'\u00ae',\"\",text)\n    text = re.sub(r'\u00e2',\"\",text)\n    text = re.sub(r'\u00a2',\"\",text)\n    text = re.sub(r'\u20ac',\"\",text)\n    language = \"en\"\n    max_ngram_size = 4\n    deduplication_thresold = 0.7\n    deduplication_algo = 'jaro' #'jaro'\n    windowSize = 3\n    numOfKeywords = 10\n\n    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_thresold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n    keywords = custom_kw_extractor.extract_keywords(text)\n    keywords1=[]\n    for kw in keywords:\n        strt=kw[1]\n        keywords1.append(strt)\n    keywrd_ext=[]\n    for wrd in keywords1:\n        wrd=re.sub('\\s+',' ',wrd).strip()\n        pat = re.compile(r\"([.()!?,;\/\\'])\")\n        wrd=pat.sub(\"\", wrd)\n        wrd=re.sub('@','',wrd).strip()\n        keywrd_ext.append(wrd)\n        \n    final_final_list=[]\n    for k in keywrd_ext:\n        doc = word_tokenize(k)\n        li=[w for w in doc if w.isalpha()]\n        wrd=\"\"\n        if len(li)>0:\n            wrd=\" \".join(li)\n            final_final_list.append(wrd)\n    return final_final_list\n","f58da7ce":"read_data_All.head()","2a1bff20":"read_data_All = read_data_All[~read_data_All['pid'].isin(remove_pid)].reset_index(drop = True)\nread_data_All['title_'] = read_data_All['title'].apply(lambda x:x.strip('Chapter '))\nread_data_All['title_'] = read_data_All['title_'].apply(lambda x:x.replace('volume',''))\nread_data_All['title_'] = read_data_All['title_'].apply(lambda x:x.replace('Volume',''))\nread_data_All['title_'] = read_data_All['title_'].apply(lambda x:x.replace('ontents',''))\nread_data_All['keywords'] = read_data_All['title_'].apply(lambda x:yake_generator(x))\n\n#Finding out the Language in which the Research Paper is written\n#read_data_All['lang'] = read_data_All['title_'].apply(lambda x: detect(str(x))) \n\nread_data_All['title_kw'] = read_data_All['title_kw'].apply(lambda x:x[2:-2].split(\"', '\"))\nread_data_All['Cat_mach'] = read_data_All['Cat_mach'].apply(lambda x:x[2:-2].split(\"', '\"))","0363c689":"def generate_detailed_clusters_keywords(dataframe):\n    print('File Name : ', dataframe)\n    cluster_1 = pd.read_pickle(dataframe)\n    cluster_1 = cluster_1[['nodes_in_cluster','cluster','total_cluster_size']]\n\n    #cluster_1['nodes_in_cluster'] =  cluster_1['nodes_in_cluster'].apply(lambda x: ast.literal_eval(x))\n    skills_in_cluster_int = cluster_1['nodes_in_cluster'].to_list()\n    \n    print('----------------------------------')\n    print('Gettin Top N Research Papers')\n    cluster_node = []\n\n    for each_cluster in skills_in_cluster_int:\n        H = graph.subgraph(each_cluster)\n        pr = nx.pagerank_numpy(H, alpha=0.9)\n        ## You can take Top K from the Dictionary 'pr'.\n        top_rank = list(pd.DataFrame({'nodes':list(pr.keys()),'values':list(pr.values())}).sort_values(by = 'values', ascending = False).reset_index(drop = True)[:5]['nodes'])\n        cluster_node.append(top_rank)\n\n    cluster_1['Top_rank_node'] = cluster_node\n    \n    print('----------------------------------')\n    print('Unstacking & Merging')\n    \n    cluster_1 = unstack_(cluster_1, 'nodes_in_cluster')\n    cluster_1['top_nodes'] = cluster_1.apply(lambda z:1 if z.Top_rank_node.count(z.nodes_in_cluster) != 0 else 0, axis = 1)\n    cluster_1 = cluster_1[['nodes_in_cluster', 'cluster', 'total_cluster_size', 'top_nodes']]\n    \n    cluster_1 = cluster_1.merge(read_data_All, right_on = 'pid', left_on = 'nodes_in_cluster', how = 'left')\n    cluster_1 = cluster_1[['nodes_in_cluster', 'cluster', 'total_cluster_size', 'top_nodes',\n           'title', 'title_kw', 'Cat_mach', 'keywords']]\n    \n    save_file_name = dataframe.strip('.xlsx')+('_dc.xlsx')\n    cluster_1.to_excel(save_file_name)\n    \n    print('----------------------------------')\n    print('Getting Top Keywords')\n    \n    skills_in_cluster_int = list(cluster_1['cluster'].unique())\n    \n    cluster_dict = []\n    for each_cluster in skills_in_cluster_int:\n        keywords_extracted = list(itertools.chain(*list(cluster_1[cluster_1['cluster'] == each_cluster]['keywords'])))\n        keywords_extracted_unique = list(set(keywords_extracted))\n        counts = []\n        for words_ in keywords_extracted_unique:\n            counts.append(keywords_extracted.count(words_))\n\n        kc = pd.DataFrame({'keywords':keywords_extracted_unique,'count':counts}).sort_values(by = 'count', ascending = False)\n        kc = kc[kc['count'] > 2].reset_index(drop = True)\n\n\n        cluster_dict.append(dict(zip(list(kc['keywords']), list(kc['count']))))\n        \n    KW = pd.DataFrame({'cluster':skills_in_cluster_int,'keywords':cluster_dict})\n    \n    print('----------------------------------')\n    print('Getting Top Title Keywords')\n    \n    skills_in_cluster_int = list(cluster_1['cluster'].unique())\n    \n    cluster_dict = []\n    for each_cluster in skills_in_cluster_int:\n        keywords_extracted = list(itertools.chain(*list(cluster_1[cluster_1['cluster'] == each_cluster]['title_kw'])))\n        keywords_extracted_unique = list(set(keywords_extracted))\n        counts = []\n        for words_ in keywords_extracted_unique:\n            counts.append(keywords_extracted.count(words_))\n\n        kc = pd.DataFrame({'keywords':keywords_extracted_unique,'count':counts}).sort_values(by = 'count', ascending = False)\n        kc = kc[kc['count'] > 2].reset_index(drop = True)\n\n\n        cluster_dict.append(dict(zip(list(kc['keywords']), list(kc['count']))))\n        \n    TKW = pd.DataFrame({'cluster':skills_in_cluster_int,'titlekeywords':cluster_dict})\n    \n    print('----------------------------------')\n    print('Getting Top Title Match')\n    \n    skills_in_cluster_int = list(cluster_1['cluster'].unique())\n    \n    cluster_dict = []\n    for each_cluster in skills_in_cluster_int:\n        keywords_extracted = list(itertools.chain(*list(cluster_1[cluster_1['cluster'] == each_cluster]['Cat_mach'])))\n        keywords_extracted_unique = list(set(keywords_extracted))\n        counts = []\n        for words_ in keywords_extracted_unique:\n            counts.append(keywords_extracted.count(words_))\n\n        kc = pd.DataFrame({'keywords':keywords_extracted_unique,'count':counts}).sort_values(by = 'count', ascending = False)\n        kc = kc[kc['count'] > 2].reset_index(drop = True)\n\n\n        cluster_dict.append(dict(zip(list(kc['keywords']), list(kc['count']))))\n        \n    TKM = pd.DataFrame({'cluster':skills_in_cluster_int,'title_match':cluster_dict})\n    \n    T = KW.merge(TKW).merge(TKM)\n    save_file_name = dataframe.strip('.xlsx')+('_topkw.xlsx')\n    T.to_excel(save_file_name)\n    print('----------------------------------')\n    ","39909d62":"def _generate_filename(x):\n    return 'Save_Clusters_Res_'+str(x)+'.pkl'\nlist_res_ = list(map(_generate_filename,list_res))\n\nfor files in list_res_:\n    generate_detailed_clusters_keywords(files)","d24c549b":"#New Repositiory of Research Papers with CORD_UID variable for all the Research Papers\nmetadata = pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv')\nmetadata = metadata[['cord_uid','title']]\nmetadata.columns = ['cord_uid', 'Title']\n\n#Read the File with the Final Clusters\ncluster_file_all = pd.read_csv('\/kaggle\/input\/inputs-covid-clustering\/Final_Clusters_Keywords.csv')\n\n#Merge both the Files and Save\ncluster_file_all.merge(metadata, how = 'left').to_csv(('Final_Clusters_Keywords_UID.csv'))","19bb8295":"## Additional Operations:\nAdditionally we are also performing the following Operations to analyze the clusters formed in a better and efficient way.\n\n1. Extract Additional Keywords from all the Titles of the Research Papers. We are using 'yake' for this (https:\/\/pypi.org\/project\/yake-github\/)\n2. Extract the top 5 Research Papaers from each Cluster based on Page Rank algorithm.\n\nTo know more about Page Rank Algorithms, please refer to:\n\n1. https:\/\/www.geeksforgeeks.org\/page-rank-algorithm-implementation\/\n2. https:\/\/www.link-assistant.com\/news\/google-page-rank-2019.html\n3. https:\/\/www.searchenginewatch.com\/2018\/10\/25\/googles-pagerank-algorithm-explained\/","11168172":"![image.png](attachment:image.png)","f210cc2b":"### Build the Final Network from the Edge List\nThe Ego-Split Algorithm code takes the **edge list** of the graph in a csv file. Every row indicates an edge between two nodes separated by a comma. The first row is a header. Nodes should be indexed starting with 0. ","c4348dc1":"### Merge all the Information to save the final Graph Edges along with detailed Information like Research Paper Title, Keywords Extracted, Category of Keywords Extracted etc.","35c775cf":"## Additional Changes based on Updates in the Research Papers Repository","33ed58cf":"## Step II: Ego-Split Algorithm\n\nReference: https:\/\/github.com\/benedekrozemberczki\/EgoSplitting","3d67604f":"## Methodology: Graph Based Overlapping Clustering\n\nReference: https:\/\/github.com\/benedekrozemberczki\/EgoSplitting\n\nWe are using the a NetworkX implementation of \"Ego-splitting Framework\" to categorize the Research Papers into clusters.  This framework is developed by Alessandro Epasto, Silvio Lattanzi and Renato Paes Leme for detecting clusters in complex networks which leverage the local structures known as ego-nets (i.e. the subgraph induced by the neighborhood of each node) to de-couple overlapping clusters. Ego-splitting is a highly scalable and flexible framework, with provable theoretical guarantees, that reduces the complex overlapping clustering problem to a simpler and more amenable non-overlapping (partitioning)problem.\n\nEgo-splitting framework works in two steps: a local ego-net analysis and a global graph partitioning\n\nEgo-Splitting Framework Paper: https:\/\/www.eecs.yorku.ca\/course_archive\/2017-18\/F\/6412\/reading\/kdd17p145.pdf","f1d976ed":"## Objective: The objective of this step is that given the repository of Research Papers, if we can utilize an Unsupervised method to cluster them into insightful groups based on genre.\n\nFor example, Research Papers regarding topics concerning impacts on Mass Gatherings or Travelling Abroad durings pandemics may be clustered together to form a cluster. \n\nBelow are few examples of Research Paper titles:\n1. Yellow fever outbreaks, vaccine shortages and the Hajj and Olympics: call for global vigilance\n2. Chapter 7 The Games of the XXVII Olympiad in Sydney (2000)\n3. Mass gatherings and respiratory disease\n4. Assessment of Temporary Community-Based Health Care Facilities During Arbaeenia Mass Gathering at Karbala, Iraq: Cross-Sectional Survey Study","0e20411c":"## Step I: Build the Network for the Research Paper Repository\nIn this network, **each Node is a Research Paper** and there will be an edge between two Nodes only if there exists some kind of relationship between the two Research Papers. The relationships can exist in two cases:\n1. If one Research Paper is referenced in the other Research Paper\n2. If the cosine similarity of the document embedding of the two Research Papers is higher than or equal to 0.9\n\nCreating the Embedding for the Research Papers are mentioned in this Notebook: https:\/\/mentionthenameoftheNotebook\n\nEmbedding Used: BioSentVec (https:\/\/github.com\/ncbi-nlp\/BioSentVec)","12718578":"## Function to Trying out Clustering at Different Resolutions","24cc4414":"## ~CODE~","42ca7082":"## Inputs:\n1. **ALL_study_with_wiki_columns.pkl:** This file contains keywords extracted from the Title of the Research Paper based on Wikifier (Find the notebook for the same here: https:\/\/mentionthenameoftheNotebook)\n2. **res_ref.csv:** This file contains the relationship among research papers depending on whether one is referenced in another\n3. **title.csv:** This file contains the Title of the Research Papers along with their distinct PID (Identity Variable)\n4. **result.pkl:** This file contains contains the edgelist of the network (Notebook: https:\/\/mentionthenameoftheNotebook)","ee3d9856":"![image.png](attachment:image.png)"}}