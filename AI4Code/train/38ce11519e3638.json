{"cell_type":{"a7aee80d":"code","302ad169":"code","f4742b93":"code","a650b2dc":"code","651c0a89":"code","b6bcafe4":"code","1a7cdd1c":"code","70c32c6f":"code","04a0079e":"code","144a0196":"code","95b6b6e7":"code","95d594bc":"code","aee4abaf":"code","c310bb73":"code","e5841e38":"code","e9371d9c":"code","cdb25aff":"code","882dc78e":"code","59a3690c":"code","d5739a3c":"code","12f62b09":"code","b31363d3":"markdown","aff89132":"markdown","26a5b4e2":"markdown","829e8ff0":"markdown","1a336e50":"markdown"},"source":{"a7aee80d":"# import Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib as pyplot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# to see all the comands result in a single kernal \nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n# to increase no. of rows and column visibility in outputs\npd.set_option('display.max_rows', 2000)\npd.set_option('display.max_columns', 2000)\npd.set_option('display.width', 2000)\n\n#To ignore warnings\nimport warnings\nwarnings.simplefilter('ignore')","302ad169":"#Import data\ntrain = pd.read_csv(r'..\/input\/tabular-playground-series-jan-2021\/train.csv')\ntest = pd.read_csv(r'..\/input\/tabular-playground-series-jan-2021\/test.csv')\nsample = pd.read_csv(r'..\/input\/tabular-playground-series-jan-2021\/sample_submission.csv')","f4742b93":"# Having a look at data and its shape \ntrain.head()\ntest.head()\ntrain.shape ,test.shape ,sample.shape","a650b2dc":"# Missing value check \ntrain.isna().sum().sum()\ntest.isna().sum().sum()","651c0a89":"# Analysing Distribution of Variables\ntrain.describe().T\ntest.describe().T","b6bcafe4":"#Target Distribution check\ntrain['target'].describe([0,0.01,0.03,0.05,0.1,0.25,0.5,0.75,0.95,0.97,0.99,1])","1a7cdd1c":"# found out that there is no. missing value and only one address as object type variable\n# Target varibale distribution \ntrain['target'].plot(kind = 'density', title = 'target Distribution')","70c32c6f":"# Checking correlation\nplt.figure(figsize=(15, 8))\nsns.heatmap(train.corr(),annot=True)","04a0079e":"#Importing Packages\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV,KFold\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor","144a0196":"# Code for cross validation\ndef run_gradient_boosting(clf,k, fit_params, train, test, features):\n  N_SPLITS = k\n  oofs = np.zeros(len(train))\n  preds = np.zeros((len(test)))\n\n  folds = KFold(n_splits = N_SPLITS,random_state=2021)\n\n  for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['target'])):\n    print(f'\\n------------- Fold {fold_ + 1} -------------')\n    target=train['target']\n    X_trn, y_trn = train[features].iloc[trn_idx], target.iloc[trn_idx]\n\n    X_val, y_val = train[features].iloc[val_idx], target.iloc[val_idx]\n\n    X_test = test[features]\n    \n    _ = clf.fit(X_trn, y_trn, eval_set = [(X_val, y_val)], **fit_params)\n    preds_val = clf.predict(X_val)\n    preds_test = clf.predict(X_test)\n    fold_score =  np.sqrt(mean_squared_error(y_val,preds_val)) \n    print(f'\\n RMLSE score for validation set is {fold_score}')\n    oofs[val_idx] = ((preds_val))\n    preds += preds_test \/ N_SPLITS\n\n\n  oofs_score = np.sqrt(mean_squared_error(target, oofs))\n  print(f'\\n\\n rmlse score for oofs is {oofs_score}')\n\n  return oofs, preds\n","95b6b6e7":"train.columns","95d594bc":"model_col=['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14']\nmodel_col_1=['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14','xgb','lgb','cb']","aee4abaf":"from sklearn.model_selection import GridSearchCV,StratifiedKFold,KFold\n\nxgb = XGBRegressor(n_estimators = 8000,\n#                        learning_rate = 0.05,\n#                       colsample_bytree = 0.76,\n                        )\nfit_params = {'verbose': False, 'early_stopping_rounds': 100}\nTARGET_COL= 'target'\nfeatures =model_col\nxgb_oofs, xgb_preds = run_gradient_boosting(xgb,5, fit_params, train, test, features)","c310bb73":"lgb = LGBMRegressor(n_estimators=5000, importance_type='gain',\n#                          learning_rate = 0.05,\n#                          colsample_bytree = 0.76,\n                        )\nfit_params = {'verbose': False, 'early_stopping_rounds': 100}\nTARGET_COL= 'target'\nfeatures =model_col\nlgb_oofs, lgb_preds = run_gradient_boosting(lgb,5, fit_params, train, test, features)","e5841e38":"from catboost import CatBoostRegressor\ncb = CatBoostRegressor(iterations=10000,\n                       learning_rate = 0.1,\n#                         colsample_bytree = 0.76,\n                        )\nfit_params = {'verbose': False , 'early_stopping_rounds': 100}\nTARGET_COL= 'target'\nfeatures =model_col\ncb_oofs, cb_preds = run_gradient_boosting(cb,5, fit_params, train, test, features)","e9371d9c":"# Feature Importance \n# you can also change cb to xgb or lgb to see their imporatance of variable\nfeat_importances = pd.Series(cb.feature_importances_, index=model_col)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()","cdb25aff":"train['xgb']=(xgb_oofs) \ntrain['lgb']=(lgb_oofs)\ntrain['cb']=(cb_oofs)\ntest['xgb']=(xgb_preds)\ntest['lgb']=(lgb_preds)\ntest['cb']=(cb_preds)","882dc78e":"np.sqrt(mean_squared_error(train['target'],(xgb_oofs)))\nnp.sqrt(mean_squared_error(train['target'],(lgb_oofs)))\nnp.sqrt(mean_squared_error(train['target'],(cb_oofs)))\nt=0.33*train['xgb']+0.33*train['lgb']+0.34*train['cb']\nnp.sqrt(mean_squared_error(train['target'],t))\ntest['target']=0.33*test['xgb']+0.33*test['lgb']+0.34*test['cb']","59a3690c":"output = pd.DataFrame({\"id\":test.id, \"target\":test.target})\noutput.to_csv('Ensembeling.csv', index=False)","d5739a3c":"lgb2 = LGBMRegressor(n_estimators=5000, importance_type='gain',\n#                          learning_rate = 0.05,\n#                          colsample_bytree = 0.76,\n                        )\nfit_params = {'verbose': False, 'early_stopping_rounds': 100}\nTARGET_COL= 'target'\nfeatures =model_col_1\nlgb2_oofs, lgb2_preds = run_gradient_boosting(lgb2,5, fit_params, train, test, features)","12f62b09":"train['lgb2']=(lgb2_oofs)\ntest['lgb2']=(lgb2_preds)\noutput = pd.DataFrame({\"id\":test.id, \"target\":test.target})\noutput.to_csv('Stacking.csv', index=False)","b31363d3":"# Competition- [Tabular Playground Series - Jan 2021](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2021\/overview)","aff89132":"# Modeling","26a5b4e2":"# Competition","829e8ff0":"### Approach(TOP-19%)\n* I have use Models - Extreme Gradient Boosting,Light Gradient Boosting and Catboost Regressor.\n* There is only early stopping criteria is met with no other hyperparameter tuning, and 5 fold Cross-validation is used for making model robust.\n* My first approach(Ensembeling) is submitting average prediction of all these models- Out of fold cross validation score-0.6982 , Public leaderboard Score-0.69971\n* Second approach(Stacking) I have added output of these 3 models as 3 more features in the model and train Light Gradient Boosting model, Out of fold cross validation score-0.6978 , Public leaderboard Score-0.69908","1a336e50":"# Basic Visualization"}}