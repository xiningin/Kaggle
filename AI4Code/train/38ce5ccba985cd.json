{"cell_type":{"3ed3f656":"code","7b416665":"code","d0770429":"code","c64184e0":"code","50e83142":"code","34b1167d":"code","4b589419":"code","cb914637":"code","54082de8":"code","08b50e62":"code","9588536c":"code","e00e1e09":"code","8eef3e0d":"code","0cce4b48":"code","374cfd46":"code","60a7f45d":"code","68886495":"code","42c61ccf":"markdown","38b503cb":"markdown","67301128":"markdown","7095a00b":"markdown","5273476a":"markdown","b7edace9":"markdown","df2546aa":"markdown","e35a8f0c":"markdown","a1a005bd":"markdown"},"source":{"3ed3f656":"from IPython.display import YouTubeVideo      \nYouTubeVideo('fYEf8kjPuV8')","7b416665":"import spacy\nfrom spacy import displacy\nnlp = spacy.load(\"en_core_web_sm\")\nimport random    \nimport datetime as dt","d0770429":"doc1 = nlp('John lives in New York, where is Mr.Abhishek its already 11PM, he likes burger so he went to KFC')\nfor i in doc1.ents:\n  print(i.text,'-',i.label_)","c64184e0":"spacy.explain('GPE')","50e83142":"#Visualize\ndisplacy.render(doc1,style='ent',jupyter=True)","34b1167d":"doc = nlp('Hi i am Jhon')","4b589419":"doc","cb914637":"nlp.pipeline","54082de8":"doc.ents","08b50e62":"nlp.remove_pipe('ner')","9588536c":"doc = nlp('Hi i am Jhon')\ndoc.ents","e00e1e09":"# Training  data\ntrain = [(\"I love burger\", {\"entities\" : [(7,13 , \"FOOD\")]}),\n         (\"pizza with more cheese\",  {\"entities\" : [(0,5, \"FOOD\")]}),\n         (\"chips is soo crispy\", {\"entities\" : [(0,5 , \"FOOD\")]}),                          \n        ]","8eef3e0d":"train","0cce4b48":"def create_blank_nlp(train_data):\n    nlp = spacy.blank(\"en\")\n    ner = nlp.create_pipe(\"ner\")\n    nlp.add_pipe(ner, last=True)\n    ner = nlp.get_pipe(\"ner\")\n    for _, annotations in train_data:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n    return nlp ","374cfd46":"nlp = create_blank_nlp(train)\noptimizer = nlp.begin_training()  \nfor i in range(7):\n    random.shuffle(train)\n    losses = {}\n    for text, annotations in train:\n        nlp.update([text], [annotations], sgd=optimizer, losses=losses)\n    print(f\"Losses at iteration {i} - {dt.datetime.now()}\", losses)","60a7f45d":"doc = nlp(\"yummy burger\")\ndisplacy.render(doc, style=\"ent\")","68886495":"doc = nlp(\"I ordered pizza\")\ndisplacy.render(doc, style=\"ent\")","42c61ccf":"# Custom Named Entity Recognition","38b503cb":"# Named Entity Recognition\n\n  NER is a process where an algorthm takes a string as input and indentifies relevant nouns like people, places, organization","67301128":"## Remove Pipeline component","7095a00b":"### Use Cases\n\n1. Customer support Services - automtically categorize incoming customer chats into relevant dept\n  To understand better - Let me explain with Bank Domain\n  \n2. Content Classification    - Automatically scan documents and extract important entities pople, organisation,places...\n3. Health Care - by etracting essential information from lab reports\n4. Analyze Resume\n3. Proile Extraction\n4. Efficient Search Algorithm\n5. Research Papers","5273476a":"## Pre trained Method","b7edace9":"![s4.jpg](attachment:s4.jpg)","df2546aa":"But do you guys noticed one thing???????","e35a8f0c":"### Libraries\n\n1. NLTK\n2. spaCy\n3. Standford Core NLP\n4. Allen NLP\n5. Flair\n6. Polyglot","a1a005bd":"FYI: This is just a simple example, to make it more accurate train the data as many as possible"}}