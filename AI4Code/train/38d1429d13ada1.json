{"cell_type":{"a89eee47":"code","1095e758":"code","5939e50d":"code","29891d89":"code","cab13ee5":"code","3ee74784":"code","b4e804c6":"code","2a8700fb":"code","13788f45":"code","3fc3d108":"code","c430d5ff":"code","ae7c7776":"code","046ef89a":"code","5190a0cd":"code","447cf4f1":"code","6d93e2a4":"code","31dc03df":"code","b39a240e":"code","11eb9eb6":"code","22da9b87":"code","4f2c52e3":"code","9d9e3f8d":"code","a41a02f8":"code","de8273b2":"code","99a7fa7f":"code","bf0622c8":"code","86458531":"code","3a7a007d":"code","f3da98df":"code","ba74c1db":"code","40a9a5e5":"code","597aceb7":"code","7fcbb4ff":"code","3798fdba":"code","27f4fe2f":"code","7c2d1d1f":"code","478faa83":"code","c9a903d4":"code","2b5ba7b2":"code","18e4567b":"code","0d61fabc":"code","68595eca":"code","ce3e48ce":"code","79250ce3":"code","c7bb5733":"code","b67cdf3a":"markdown","b6b59071":"markdown","c4114a80":"markdown","9f26251f":"markdown","fb7e83b6":"markdown","ebb8acff":"markdown","1a5f2cb1":"markdown","dbebaff0":"markdown","b3382a15":"markdown","ec52d20a":"markdown","269a0e91":"markdown","d3f4eb5b":"markdown","9a8690a9":"markdown","9e5451ed":"markdown","3331988b":"markdown","1aa73faf":"markdown","888c43ff":"markdown","de7ba2dd":"markdown","117e5626":"markdown","fabb4e0e":"markdown","ac222c31":"markdown","ea25a096":"markdown","020ce247":"markdown","6e989f0f":"markdown","b7f90a9d":"markdown","b23b6792":"markdown","daf62934":"markdown","b29a6c68":"markdown","f9224393":"markdown","9d9c476c":"markdown","ff088b4e":"markdown","7deefe0f":"markdown","12c89fd4":"markdown","78904664":"markdown","29d37d86":"markdown","c84c9bb8":"markdown","9cdaac85":"markdown","26b248b2":"markdown","1f40a998":"markdown","dab6749e":"markdown","ecc95740":"markdown"},"source":{"a89eee47":"#Import Libraries\nimport numpy as np \nimport pandas as pd \n\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix\n\n","1095e758":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\nall_data = [train,test]","5939e50d":"train.head()\n\n","29891d89":"train.info()","cab13ee5":"train.describe()\n","3ee74784":"print (train[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean())\n","b4e804c6":"print (train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean())\n","2a8700fb":"grid = sns.FacetGrid(train, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();\n","13788f45":"print(pd.crosstab(train.Survived, train.Pclass))\n\nplt.figure(figsize=(12,5))\n\nsns.countplot(x=\"Pclass\", data=train, hue=\"Survived\",palette=\"hls\")\nplt.xlabel('PClass',fontsize=17)\nplt.ylabel('Count', fontsize=17)\nplt.title('Class Distribuition by Survived or not', fontsize=20)\n\nplt.show()\n","3fc3d108":"for dataset in all_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\nprint (train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean())","c430d5ff":"print(pd.crosstab(train.FamilySize, train.Survived))\nsns.factorplot(x=\"FamilySize\",y=\"Survived\", data=train, kind=\"bar\",size=6, aspect=1.6)\nplt.show()\n","ae7c7776":"train = train.drop(['Parch', 'SibSp'], axis=1)\ntest = test.drop(['Parch', 'SibSp'], axis=1)\nall_data = [train, test]\n\ntrain.head()\n","046ef89a":"g = sns.FacetGrid(train, col='Survived')\ng.map(plt.hist, 'Age', bins=25)\n","5190a0cd":"print(pd.crosstab(train.Survived,train.Sex))\n\nplt.figure(figsize=(12,5))\nsns.countplot(x=\"Sex\", data=train, hue=\"Survived\",palette=\"hls\")\nplt.title('Sex Distribuition by survived or not', fontsize=20)\nplt.xlabel('Sex Distribuition',fontsize=17)\nplt.ylabel('Count', fontsize=17)\n\nplt.show()\n","447cf4f1":"print(pd.crosstab(train.Survived, train.Embarked))\n\nplt.figure(figsize=(12,5))\n\nsns.countplot(x=\"Embarked\", data=train, hue=\"Survived\",palette=\"hls\")\nplt.title('Class Distribuition by survived or not',fontsize=20)\nplt.xlabel('Embarked',fontsize=17)\nplt.ylabel('Count', fontsize=17)\n\nplt.show()\n","6d93e2a4":"print('Shape Befor drop: ', train.shape)\ntrain = train.drop(['Ticket', 'Cabin'], axis=1)\ntest = test.drop(['Ticket', 'Cabin'], axis=1)\nall_data = [train, test]\nprint('Shape After drop: ',train.shape)\n","31dc03df":"for dataset in all_data:\n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\ntrain.head()","b39a240e":"guess_ages = np.zeros((2,3))\nguess_ages\n","11eb9eb6":"for dataset in all_data:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_data = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_data.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain.head()\n","22da9b87":"train['Age_group'] = pd.cut(train['Age'], 5)\ntrain[['Age_group', 'Survived']].groupby(['Age_group'], as_index=False).mean().sort_values(by='Age_group', ascending=True)\n","4f2c52e3":"for dataset in all_data:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ntrain.head()\n","9d9e3f8d":"train = train.drop(['Age_group'], axis=1)\nall_data = [train, test]\ntrain.head()\n","a41a02f8":"for dataset in all_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\nprint (train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean())\n\n","de8273b2":"for dataset in all_data:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain.head()\n","99a7fa7f":"train = train.drop(['Fare'], axis=1)\ntest = test.drop(['Fare'], axis=1)\nall_data = [train,test]\ntrain.head()","bf0622c8":"# retain the new Title feature for model training.\nfor dataset in all_data:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train['Title'], train['Sex'])\n","86458531":"for dataset in all_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n","3a7a007d":"plt.figure(figsize=(12,5))\n\n#Plotting the result\nsns.countplot(x='Title', data=train, palette=\"hls\")\nplt.xlabel(\"Title\", fontsize=16) #seting the xtitle and size\nplt.ylabel(\"Count\", fontsize=16) # Seting the ytitle and size\nplt.title(\"Title Name Count\", fontsize=20) \nplt.xticks(rotation=45)\nplt.show()\n","f3da98df":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in all_data:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain.head()\n","ba74c1db":"train = train.drop(['Name', 'PassengerId'], axis=1)\ntest = test.drop(['Name'], axis=1)\nall_data = [train, test]\n","40a9a5e5":"train.head()\n","597aceb7":"test.head()","7fcbb4ff":"X_train = train.drop(\"Survived\", axis=1)\nY_train = train[\"Survived\"]\nX_test  = test.drop(\"PassengerId\", axis=1).copy()\n","3798fdba":"#Apply RandomForestClassifier\nrandom_forest= RandomForestClassifier(n_estimators=100,\n                             max_features='auto',\n                             criterion='entropy',\n                             max_depth=10)\nrandom_forest.fit(X_train, Y_train)\n\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(round(acc_random_forest,2,), \"%\")\n\n\n","27f4fe2f":"#Apply GradientBoostingClassifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nclf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n                                 max_depth=1, random_state=0).fit(X_train, Y_train)\ny_prediction= clf.predict(X_test)\nclf.score(X_train, Y_train)\nacc_clf = round(clf.score(X_train, Y_train) * 100, 2)\nprint(round(acc_clf,2,), \"%\")\n","7c2d1d1f":"#Apply LGBMClassifier\n\nfrom lightgbm import LGBMClassifier\nmodel = LGBMClassifier().fit(X_train, Y_train)\ny_predict= model.predict(X_test)\nmodel.score(X_train, Y_train)\nacc_model = round(model.score(X_train, Y_train) * 100, 2)\nprint(round(acc_model,2,), \"%\")\n\n","478faa83":"#Apply Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\n\nY_pred = logreg.predict(X_test)\n\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nprint(round(acc_log,2,), \"%\")\n","c9a903d4":"# Apply Decision Tree\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\n\nY_pred = decision_tree.predict(X_test)\n\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nprint(round(acc_decision_tree,2,), \"%\")\n","2b5ba7b2":"from xgboost import XGBClassifier\n\nparams_xgb = {'colsample_bylevel': 0.7, 'learning_rate': 0.03, 'max_depth': 3, \n              'n_estimators': 400, 'reg_lambda': 15, 'subsample': 0.5}\nxgb = XGBClassifier(**params_xgb)\ny_preds = xgb.fit(X_train, Y_train).predict(X_test)\nacc_xgb = round(xgb.score(X_train, Y_train) * 100, 2)\nprint(round(acc_xgb,2,), \"%\")\n\n","18e4567b":"results = pd.DataFrame({\n    'Model': ['LGBMClassifier', 'Logistic Regression', \n              'Random Forest', 'Boosting', \n              'Decision Tree','xgb'],\n    'Score': [ acc_model,acc_log,\n              acc_random_forest, acc_clf,\n              acc_decision_tree,acc_xgb]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(7)\n","0d61fabc":"from sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(rf, X_train, Y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())\n","68595eca":"importances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(random_forest.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\nimportances.head(15)\n\n","ce3e48ce":"importances.plot.bar()\n","79250ce3":"params_xgb = {'colsample_bylevel': 0.7, 'learning_rate': 0.03, 'max_depth': 3, \n              'n_estimators': 400, 'reg_lambda': 15, 'subsample': 0.5}\nxgb = XGBClassifier(**params_xgb)\n\ny_preds = xgb.fit(X_train, Y_train).predict(X_test)\nprint(\"Score: \",xgb.score, 4*100, \"%\")\n\n\n\n","c7bb5733":"submission = pd.DataFrame({\n        \"PassengerId\": test['PassengerId'],\n        \"Survived\":  y_preds\n    })\n\nsubmission.to_csv('submission.csv', index=False)\n","b67cdf3a":"## 1. Introduction\n\n> This is my first work of machine learning in kaggle. the notebook is written in python. \nIn this kernel I will go through the whole process of creating a machine learning model on the famous Titanic dataset, which is used by many people all over the world.\n\n> feel free to fork this kernel to play around with the code and test it for yourself. If you plan to use any part of this code, please reference this kernel! I will be glad to answer any questions you may have in the comments. Thank You!\n","b6b59071":"Let us drop Parch, SibSpeatures in favor of FamilySize.\n\n","c4114a80":"## Gathering Data\n\n> We downloaded two files (train.csv) & (test.csv) and We have to read them","9f26251f":" ### Prop3: Missing value in Age\n \nWe have plenty of missing values in this feature. \nWe can generate random numbers between (mean - std) and (mean + std). then we part age into 5 range.\n\n","fb7e83b6":"### Prop4: Missing value in Embarked:\nOur training dataset has two missing values. We simply fill these with the most common occurance.\n\n","ebb8acff":"## 3. Cleaning","1a5f2cb1":"#### Let us replace Age with ordinals based on these groups.\n\n","dbebaff0":"### Prop5: categorical feature (Embarked)\n\nWe can now convert the EmbarkedFill","b3382a15":"### Show Titles names in a graph","ec52d20a":"### Prop2: categorical feature (Sex)\n\nLet us start by converting Sex feature to a new feature called Gender where female=1 and male=0.\n\n","269a0e91":"## Problems:\n - Missing Value in (Age, Cabin, Embarked)\n \n - Categorical data in columns (Sex, Embarked)\n \n - mix of numeric and alphanumeric data types in Ticket column\n \n - Name feature may contain errors or typos as there are several ways used to describe a name including titles, round brackets, and quotes used for alternative or short names.\n \n ","d3f4eb5b":"### Test","9a8690a9":"## 5. Sex","9e5451ed":"## 6. Embarked","3331988b":"## Exploration\n\n","1aa73faf":"Looking the graphs, is clear that 3st class and Embarked at Southampton have a high probabilities to not survive\n\n","888c43ff":"#### remove the Age_group column.","de7ba2dd":"We can replace many titles with a more common name","117e5626":"We note that we have: \n1. mix of numeric and alphanumeric data types in Ticket column\n2. Categorical data in Embarked and Sex columns\n","fabb4e0e":"### Above we can see that: \n1. Infants (Age <=4) had high survival rate.\n2. Oldest passengers (Age = 80) survived.\n3. Large number of 15-25 year olds did not survive.\n4. Most passengers are in 15-35 age range.","ac222c31":"Here we can create a new feature who contain SibSp(siblings\/spouse ) and Parch (children\/parents) family_size","ea25a096":"### Submission\n","020ce247":"### Prop1: Cabine and Ticket Data[](http:\/\/)\n* I think that the Cabin and Ticket features not impact in survive so I decided to drob these features","6e989f0f":"### Prop6: Name \n- Most titles band Age groups accurately. For example: Master title has Age mean of 5 years.\n- Survival among Title Age bands varies slightly.\n- Certain titles mostly survived (Mme, Lady, Sir) or did not (Don, Rev, Jonkheer).","b7f90a9d":"## 2. Assessment Data","b23b6792":"## 4. Applying ML Models:","daf62934":"We have a new probleme here, there is missing value in columns('Age' - 'Cabine' - 'Embarked')","b29a6c68":"It's meaning that the females were rescued at the expense of the males\n","f9224393":"## 1. Sex","9d9c476c":"It's meaning that dies to mens are much higher than female\n","ff088b4e":"## Feature Importance","7deefe0f":"### The Best Model?\n","12c89fd4":"### 4. Age","78904664":"### 3. SibSp and Parch","29d37d86":"- we can drop the Name feature now from our data.\n- We also don't need the PassengerId column in the training dataset.","c84c9bb8":"The title was the most impact in survive! ","9cdaac85":"## 2. Pclass","26b248b2":"We can convert the categorical titles to ordinal.\n\n","1f40a998":"### Another graph for representation","dab6749e":"Most family suvived who are consisting of Four people","ecc95740":"### Prop6: Fare\nI think Fare column it's not important so i decided to drop this"}}