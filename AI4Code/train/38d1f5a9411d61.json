{"cell_type":{"d504730e":"code","722d8912":"code","0cbe059b":"code","fbb988b7":"code","59b215c3":"code","096e2e06":"code","54aa9db1":"code","22b12dc5":"code","cc09e55e":"code","e1fc506a":"code","c472826d":"code","8b81a54c":"code","9337b521":"code","750008f4":"code","293899f9":"code","e8650a13":"code","1df62621":"code","2c8b8f00":"code","2789a243":"code","9107d538":"code","0e0df239":"code","e08e0ac1":"code","e9313916":"code","c9ad85fc":"code","a339c629":"code","3b3c41d3":"code","df2737fe":"code","255697d7":"code","b0b6fed2":"code","22d20c20":"code","2fb62b6e":"code","ce831994":"code","e9c24276":"code","f9d4767c":"code","60ebc062":"code","16e36f4a":"code","6b4b7cdf":"code","ec795acf":"code","ef2df988":"code","265f424f":"code","f369a0be":"code","a90a7bff":"code","fd6d2bd3":"code","4bbe1b68":"code","35d5e675":"code","51886078":"code","6e9eefb4":"code","f369f1e2":"code","df806fce":"code","c1c3f2a2":"code","5f576cc8":"markdown","b0460363":"markdown","1b70dee1":"markdown","7abd64aa":"markdown","9c47a53c":"markdown","ef05535c":"markdown","a8e02ccf":"markdown","d4c6fa23":"markdown","1ab5dd08":"markdown","1a49819f":"markdown","bd86cfae":"markdown","13a5d5d6":"markdown","d4c977f9":"markdown","b5bb26a8":"markdown","505c6821":"markdown","25af7a54":"markdown","479bc9bb":"markdown","41b77c11":"markdown"},"source":{"d504730e":"import gc\nimport os\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\n\nimport holidays\nfrom datetime import date\nimport dateutil.easter as easter\n\nfrom collections import defaultdict\nle = defaultdict(LabelEncoder)\n\nimport warnings\nwarnings.simplefilter('ignore')\nnp.warnings.filterwarnings('ignore')","722d8912":"import matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nimport seaborn as sns\nfrom learntools.time_series.style import *  # plot style settings\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True, figsize=(12, 8))\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=16,\n    titlepad=10,\n)\nplot_params = dict(\n    color=\"0.75\",\n    style=\".-\",\n    markeredgecolor=\"0.25\",\n    markerfacecolor=\"0.25\",\n    legend=False,\n)\n%config InlineBackend.figure_format = 'retina'","0cbe059b":"# -----------------------------------------------------------------\n# Some parameters to config \nSEED = 42\nN_ESTIMATORS = 1000\nVERBOSE = 0\n\nID = \"row_id\"\nINPUT = \"..\/input\/tabular-playground-series-jan-2022\"\nPSEUDO_DIR = \"..\/input\/tps-jan-2022-pseudo-labels\/pseudo_labels_1.csv\"\nPSEUDO_DIR2 = \"..\/input\/tps-jan-2022-pseudo-labels\/pseudo_labels_2.csv\"\n\n# time series data common new feature  \nDATE = \"date\"\nYEAR = \"year\"\nMONTH = \"month\"\nWEEK = \"week\"\nDAY = \"day\"\nDAYOFYEAR = \"dayofyear\"\nDAYOFMONTH = \"dayofMonth\"\nDAYOFWEEK = \"dayofweek\"\nWEEKDAY = \"weekday\"","fbb988b7":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nseed_everything(SEED)","59b215c3":"# https:\/\/www.kaggle.com\/c\/web-traffic-time-series-forecasting\/discussion\/36414\ndef smape_loss(y_true, y_pred):\n    \"\"\"\n    SMAPE Loss\n    Parameters\n    ----------\n    y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n        Ground truth (correct) target values.\n    y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n        Estimated target values.\n    Returns\n    -------\n    loss : float or ndarray of floats\n        If multioutput is 'raw_values', then mean absolute error is returned\n        for each output separately.\n        If multioutput is 'uniform_average' or an ndarray of weights, then the\n        weighted average of all output errors is returned.\n        SMAPE output is non-negative floating point. The best value is 0.0.\n\n    \"\"\"\n    denominator = (np.abs(y_true) + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)\n","096e2e06":"def plot_periodogram(ts, detrend='linear', ax=None):\n    from scipy.signal import periodogram\n    fs = pd.Timedelta(\"1Y\") \/ pd.Timedelta(\"1D\")\n    freqencies, spectrum = periodogram(\n        ts,\n        fs=fs,\n        detrend=detrend,\n        window=\"boxcar\",\n        scaling='spectrum',\n    )\n    if ax is None:\n        _, ax = plt.subplots()\n    ax.step(freqencies, spectrum, color=\"purple\")\n    ax.set_xscale(\"log\")\n    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n    ax.set_xticklabels(\n        [\n            \"Annual (1)\",\n            \"Semiannual (2)\",\n            \"Quarterly (4)\",\n            \"Bimonthly (6)\",\n            \"Monthly (12)\",\n            \"Biweekly (26)\",\n            \"Weekly (52)\",\n            \"Semiweekly (104)\",\n        ],\n        rotation=30,\n    )\n    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n    ax.set_ylabel(\"Variance\")\n    ax.set_title(\"Periodogram\")\n    return ax","54aa9db1":"def fourier_features(index, freq, order):\n    time = np.arange(len(index), dtype=np.float32)\n    k = 2 * np.pi * (1 \/ freq) * time\n    features = {}\n    for i in range(1, order + 1):\n        features.update({\n            f\"sin_{freq}_{i}\": np.sin(i * k),\n            f\"cos_{freq}_{i}\": np.cos(i * k),\n        })\n    return pd.DataFrame(features, index=index)\n\n# Compute Fourier features to the 4th order (8 new features) for a\n# series y with daily observations and annual seasonality:\n#\n# fourier_features(y, freq=365.25, order=4)\n\ndef get_basic_ts_features(df):\n    \n    gdp_df = pd.read_csv('..\/input\/gdp-20152019-finland-norway-and-sweden\/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\n    gdp_df.set_index('year', inplace=True)\n    gdp_exponent = 1.2121103201489674 # see https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model for an explanation\n    # gdp_exponent = 1.3458829179823577  # UPDATE: value from version17\n\n    def get_gdp(row):\n        country = 'GDP_' + row.country\n        return gdp_df.loc[row.date.year, country]\n    \n    df['gdp'] = np.log1p(df.apply(get_gdp, axis=1))\n    \n    for country in ['Finland', 'Norway', 'Sweden']:\n        df[country] = df.country == country\n    for store in ['KaggleMart', 'KaggleRama']:\n        df[store] = df['store'] == store\n    for product in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n        df[product] = df['product'] == product\n    \n    df[YEAR] = df[DATE].dt.year\n    df[MONTH] = df[DATE].dt.month\n    df[WEEK] = df[DATE].dt.week\n    df[DAY] = df[DATE].dt.day\n    # df[DAYOFYEAR] = df[DATE].dt.dayofyear\n    # df[DAYOFMONTH] = df[DATE].dt.days_in_month\n    # df[DAYOFWEEK] = df[DATE].dt.dayofweek\n    df[WEEKDAY] = df[DATE].dt.weekday\n\n    df['wd4'] = df[DATE].dt.weekday == 4\n    df['wd56'] = df[DATE].dt.weekday >= 5\n#     df.loc[(df.date.dt.year != 2016) & (df.date.dt.month >=3), DAYOFYEAR] += 1 # fix for leap years\n    \n    # 21 days cyclic for lunar\n    dayofyear = df.date.dt.dayofyear\n    # for k in range(1, 32, 4):\n    for k in [7, 14, 21, 28, 30, 31, 91]:\n        df[f'sin{k}'] = np.sin(dayofyear \/ 365 * 2 * math.pi * k)\n        df[f'cos{k}'] = np.cos(dayofyear \/ 365 * 2 * math.pi * k)\n        df[f'Finland_sin{k}'] = df[f'sin{k}'] * df['Finland']\n        df[f'Finland_cos{k}'] = df[f'cos{k}'] * df['Finland']\n        df[f'Norway_sin{k}'] = df[f'sin{k}'] * df['Norway']\n        df[f'Norway_cos{k}'] = df[f'cos{k}'] * df['Norway']\n        df[f'Sweden_sin{k}'] = df[f'sin{k}'] * df['Sweden']\n        df[f'Sweden_cos{k}'] = df[f'cos{k}'] * df['Sweden']\n        df[f'storeMart_sin{k}'] = df[f'sin{k}'] * df['KaggleMart']\n        df[f'storeMart_cos{k}'] = df[f'cos{k}'] * df['KaggleMart']\n        df[f'storeRama_sin{k}'] = df[f'sin{k}'] * df['KaggleRama']\n        df[f'storeRama_cos{k}'] = df[f'cos{k}'] * df['KaggleRama']\n        df[f'mug_sin{k}'] = df[f'sin{k}'] * df['Kaggle Mug']\n        df[f'mug_cos{k}'] = df[f'cos{k}'] * df['Kaggle Mug']\n        df[f'hat_sin{k}'] = df[f'sin{k}'] * df['Kaggle Hat']\n        df[f'hat_cos{k}'] = df[f'cos{k}'] * df['Kaggle Hat']\n        df[f'sticker_sin{k}'] = df[f'sin{k}'] * df['Kaggle Sticker']\n        df[f'sticker_cos{k}'] = df[f'cos{k}'] * df['Kaggle Sticker']\n    \n#     df = pd.concat([df, pd.DataFrame({f'fin{ptr[1]}':\n#                                       (df.date == pd.Timestamp(ptr[0])) & (df.country == 'Finland')\n#                                       for ptr in holidays.Finland(years = [2015,2016,2017,2018,2019]).items()})], axis=1)\n#     df = pd.concat([df, pd.DataFrame({f'nor{ptr[1]}':\n#                                       (df.date == pd.Timestamp(ptr[0])) & (df.country == 'Norway')\n#                                       for ptr in holidays.Norway(years = [2015,2016,2017,2018,2019]).items()})], axis=1)\n#     df = pd.concat([df, pd.DataFrame({f'swe{ptr[1]}':\n#                                       (df.date == pd.Timestamp(ptr[0])) & (df.country == 'Sweden')\n#                                       for ptr in holidays.Sweden(years = [2015,2016,2017,2018,2019]).items()})], axis=1)\n\n#     for ptr in holidays.Finland(years = [2015,2016,2017,2018,2019]).items():\n#         df[f\"fin{ptr[1]}\"] = (df.date == pd.Timestamp(ptr[0])) & (df.country == 'Finland')\n#     for ptr in holidays.Norway(years = [2015,2016,2017,2018,2019]).items():\n#         df[f\"nor{ptr[1]}\"] = (df.date == pd.Timestamp(ptr[0])) & (df.country == 'Norway')\n#     for ptr in holidays.Sweden(years = [2015,2016,2017,2018,2019]).items():\n#         df[f\"swe{ptr[1]}\"] = (df.date == pd.Timestamp(ptr[0])) & (df.country == 'Sweden')\n    \n    # End of year\n    # Dec\n    for d in range(24, 32):\n        df[f\"dec{d}\"] = (df.date.dt.month == 12) & (df.date.dt.day == d)\n    for d in range(24, 32):\n        df[f\"n-dec{d}\"] = (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Norway')\n    # Jan\n    for d in range(1, 14):\n        df[f\"f-jan{d}\"] = (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Finland')\n    for d in range(1, 10):\n        df[f\"n-jan{d}\"] = (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Norway')\n    for d in range(1, 15):\n        df[f\"s-jan{d}\"] = (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Sweden')\n    # May\n    for d in list(range(1, 10)):\n        df[f\"may{d}\"] = (df.date.dt.month == 5) & (df.date.dt.day == d)\n    for d in list(range(19, 26)):\n        df[f\"may{d}\"] = (df.date.dt.month == 5) & (df.date.dt.day == d) & (df.country == 'Norway')\n    # June\n    for d in list(range(8, 14)):\n        df[f\"june{d}\"] = (df.date.dt.month == 6) & (df.date.dt.day == d) & (df.country == 'Sweden')\n    \n    #Swedish Rock Concert\n    #Jun 3, 2015 \u2013 Jun 6, 2015\n    #Jun 8, 2016 \u2013 Jun 11, 2016\n    #Jun 7, 2017 \u2013 Jun 10, 2017\n    #Jun 6, 2018 \u2013 Jun 10, 2018\n    #Jun 5, 2019 \u2013 Jun 8, 2019\n    swed_rock_fest  = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-6')),\n                                         2016: pd.Timestamp(('2016-06-11')),\n                                         2017: pd.Timestamp(('2017-06-10')),\n                                         2018: pd.Timestamp(('2018-06-10')),\n                                         2019: pd.Timestamp(('2019-06-8'))})\n\n    df = pd.concat([df, pd.DataFrame({f\"swed_rock_fest{d}\":\n                                      (df.date - swed_rock_fest == np.timedelta64(d, \"D\")) & (df.country == 'Sweden')\n                                      for d in list(range(-3, 3))})], axis=1)\n\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    for d in list(range(-4, 6)):\n        df[f\"wed_june{d}\"] = (df.date - wed_june_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n        \n    # First Sunday of November\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    df = pd.concat([df, pd.DataFrame({f\"sun_nov{d}\":\n                                      (df.date - sun_nov_date == np.timedelta64(d, \"D\")) & (df.country == 'Norway')\n                                      for d in list(range(0, 9))})], axis=1)\n    # First half of December (Independence Day of Finland, 6th of December)\n    df = pd.concat([df, pd.DataFrame({f\"dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Finland')\n                                      for d in list(range(6, 14))})], axis=1)\n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    df = pd.concat([df, pd.DataFrame({f\"easter{d}\":\n                                      (df.date - easter_date == np.timedelta64(d, \"D\"))\n                                      for d in list(range(-2, 11)) + list(range(40, 48)) + list(range(50, 59))})], axis=1)\n    \n    return df  ","22b12dc5":"def feature_engineer(df):\n    df = get_basic_ts_features(df)\n    return df","cc09e55e":"for ptr in holidays.Norway(years=[2019], observed=True).items():\n    print(ptr)","e1fc506a":"from pathlib import Path\n\ndef load_data():\n    # Read data\n    data_dir = Path(INPUT)\n    \n    df_train = pd.read_csv(\n        Path(INPUT) \/ \"train.csv\", parse_dates=[DATE],\n        usecols=['date', 'country', 'store', 'product', 'num_sold'],\n        dtype={\n            'country': 'category',\n            'store': 'category',\n            'product': 'category',\n            'num_sold': 'float32',\n        },\n        infer_datetime_format=True,)\n    \n    df_test = pd.read_csv(Path(INPUT) \/ \"test.csv\", index_col=ID, parse_dates=[DATE])\n    \n    column_y = df_train.columns.difference(df_test.columns)[0]  # column_y target_col label_col\n    df_train[DATE] = pd.to_datetime(df_train[DATE])\n    df_test[DATE] = pd.to_datetime(df_test[DATE])\n    \n    return df_train, df_test, column_y\n","c472826d":"def process_data(df_train, df_test):\n    df_train = feature_engineer(df_train)\n    df_test = feature_engineer(df_test)\n\n    return df_train, df_test","8b81a54c":"%%time\ntrain_df, test_df, column_y = load_data()   \ntrain_df, test_df = process_data(train_df, test_df)","9337b521":"train_data = train_df.copy()\ntrain_data[DATE] = train_df.date.dt.to_period('D')\ntest_data = test_df.copy()\ntest_data[DATE] = test_df.date.dt.to_period('D')","750008f4":"df_pseudolabels = pd.read_csv(PSEUDO_DIR, index_col=ID)\ndf_pseudolabels[DATE] = pd.to_datetime(test_df[DATE])\ndf_pseudolabels.to_csv(\"pseudo_labels_v0.csv\", index=True)\n\ntest_data[column_y] = df_pseudolabels[column_y].astype(np.float32)\ntrain_data = pd.concat([train_data, test_data], axis=0)\ntrain_df = pd.concat([train_df, test_data], axis=0)","293899f9":"X = train_data.set_index([DATE]).sort_index()\nX_test = test_data.set_index([DATE]).sort_index()","e8650a13":"train_data = train_data.set_index(['date', 'country', 'store', 'product']).sort_index()","1df62621":"kaggle_sales_2015 = (\n    train_data\n    .groupby(['country', 'store', 'product', 'date'])\n    .mean()\n    .unstack(['country', 'store', 'product'])\n    .loc['2015']\n)","2c8b8f00":"kaggle_sales_2016 = (\n    train_data\n    .groupby(['country', 'store', 'product', 'date'])\n    .mean()\n    .unstack(['country', 'store', 'product'])\n    .loc['2016']\n)","2789a243":"kaggle_sales_2017 = (\n    train_data\n    .groupby(['country', 'store', 'product', 'date'])\n    .mean()\n    .unstack(['country', 'store', 'product'])\n    .loc['2017']\n)","9107d538":"kaggle_sales_2018 = (\n    train_data\n    .groupby(['country', 'store', 'product', 'date'])\n    .mean()\n    .unstack(['country', 'store', 'product'])\n    .loc['2018']\n)","0e0df239":"frames = [kaggle_sales_2015, kaggle_sales_2016, kaggle_sales_2017, kaggle_sales_2018]\nkaggle_sales = pd.concat(frames)","e08e0ac1":"kaggle_sales","e9313916":"gc.collect()","c9ad85fc":"# Check NA\nmissing_val = X.isnull().sum()\nprint(missing_val[missing_val > 0])","a339c629":"train_data.groupby(column_y).apply(lambda s: s.sample(min(len(s), 5)))","3b3c41d3":"fig_dims = (50,30)\nax = kaggle_sales.num_sold.plot(title='Sales Trends', figsize=fig_dims)\n_ = ax.set(ylabel=\"Numbers sold\")","df2737fe":"plot_periodogram(X[column_y]);","255697d7":"# Plot all num_sold_true and num_sold_pred (five years) for one country-store-product combination\ndef plot_five_years_combination(engineer, country='Norway', store='KaggleMart', product='Kaggle Hat'):\n    demo_df = pd.DataFrame({'row_id': 0,\n                            'date': pd.date_range('2015-01-01', '2019-12-31', freq='D'),\n                            'country': country,\n                            'store': store,\n                            'product': product})\n    demo_df.set_index('date', inplace=True, drop=False)\n    demo_df = engineer(demo_df)\n    demo_df[GROUP_INDEX] = demo_df[GROUP_INDEX].apply(lambda x: le[x.name].transform(x))\n\n#     display(demo_df)\n    demo_df['num_sold'] = np.expm1(model.predict(preproc.transform(demo_df[features])))\n    train_subset = train_df[(train_df.country == country) & (train_df.store == store) & (train_df['product'] == product)]\n    plt.figure(figsize=(24, 8))\n    plt.plot(demo_df[DATE], demo_df.num_sold, label='prediction', alpha=0.5)\n    plt.scatter(train_subset[DATE], train_subset.num_sold, label='true', alpha=0.5, color='red', s=2)\n    plt.grid(True)\n#     plt.grid(which='major',axis ='y', linewidth='0.7', color='black')\n    plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\n    plt.legend()\n    plt.title(f'{country} {store} {product} Predictions and true for five years')\n    plt.show()\n    return demo_df['num_sold']","b0b6fed2":"def plot_true_vs_prediction(df_true, df_hat):\n    plt.figure(figsize=(20, 13))\n    plt.scatter(np.arange(len(df_hat)), np.log1p(df_hat), label='prediction', alpha=0.5, color='blue', s=3) #np.arange(len(df_hat))\n    plt.scatter(np.arange(len(df_true)), np.log1p(df_true), label='Pseudo\/true', alpha=0.5, color='red', s=7) #np.arange(len(df_true))\n    plt.legend()\n    plt.title(f'Predictions VS Pseudo-label {column_y} (LOG)') #{df_true.index[0]} - {df_true.index[-1]}\n    plt.show()","22d20c20":"def plot_residuals(y_residuals):\n    plt.figure(figsize=(13, 3))\n    plt.scatter(np.arange(len(y_residuals)), np.log1p(y_residuals), label='residuals', alpha=0.1, color='blue', s=5)\n    plt.legend()\n    plt.title(f'Linear Model residuals {column_y} (LOG)') #{df_true.index[0]} - {df_true.index[-1]}\n    plt.tight_layout()\n    plt.show()","2fb62b6e":"def plot_oof(y_true, y_predict):\n    # Plot y_true vs. y_pred\n    plt.figure(figsize=(5, 5))\n    plt.scatter(y_true, y_predict, s=1, color='r', alpha=0.5)\n#     plt.scatter(np.log1p(y_true), np.log1p(y_predict), s=1, color='g', alpha=0.3)\n    plt.plot([plt.xlim()[0], plt.xlim()[1]], [plt.xlim()[0], plt.xlim()[1]], '--', color='k')\n    plt.gca().set_aspect('equal')\n    plt.xlabel('y_true')\n    plt.ylabel('y_pred')\n    plt.title('OOF Predictions')\n    plt.show()","ce831994":"def find_min_SMAPE(y_true, y_predict):\n    # loss_correction = 1\n    scores = []\n    # float step\n    for WEIGHT in np.arange(0.97, 1.02, 0.0001):\n        y_hat = y_predict.copy()\n        y_hat *= WEIGHT\n        scores.append(np.array([WEIGHT, np.mean(smape_loss(y_true, y_hat))]))\n        \n    scores = np.vstack(scores)\n    min_SMAPE = np.min(scores[:, 1])\n    print(f'min SMAPE {min_SMAPE:.5f}')\n    for x in scores:\n        if x[1] == min_SMAPE:\n            loss_correction = x[0]\n            print(f'loss_correction: {x[0]:.5f}')\n            \n    plt.figure(figsize=(5, 3))\n    plt.plot(scores[:, 0],scores[:, 1])\n    plt.scatter([loss_correction], [min_SMAPE], color='g')\n    plt.ylabel(f'SMAPE')\n    plt.xlabel(f'loss_correction: {loss_correction:.5f}')\n    plt.legend()\n    plt.title(f'min SMAPE:{min_SMAPE:.5f} scaling')\n    plt.show()\n    \n    return loss_correction","e9c24276":"def evaluate_SMAPE(y_va, y_va_pred):\n    # loss_correction = 1\n    # Evaluation: Execution time and SMAPE\n    smape_before_correction = np.mean(smape_loss(y_va, y_va_pred))\n    smape = np.mean(smape_loss(y_va, y_va_pred))\n    loss_correction = find_min_SMAPE(y_va, y_va_pred)\n    y_va_pred *= loss_correction\n    \n    print(f\"SMAPE (before correction: {smape_before_correction:.5f})\")\n    print(f'Min SMAPE: {np.mean(smape_loss(y_va, y_va_pred))}')\n    \n    return loss_correction","f9d4767c":"GROUP_INDEX = ['country', 'store', 'product']\n\n# Target series\ny = X.loc[:, column_y]\n\n# X_1: Features for Linear Regression\nfourier = CalendarFourier(freq=\"A\", order=10)  # 10 sin\/cos pairs for Annual seasonality\n\ndp = DeterministicProcess(\n    index=X.index,\n    constant=True,               # dummy feature for bias (y-intercept)\n    order=1,                     # trend (order 1 means linear)\n    seasonal=True,               # weekly seasonality (indicators)\n    additional_terms=[fourier],  # annual seasonality (fourier)\n    drop=True,                   # drop terms to avoid collinearity\n)\n\nX_1 = dp.in_sample()  # create features for dates in tunnel.index\n\n\n# X_2: Features for XGBoost\nX_2 = X.drop(column_y, axis=1)\n# Encoding the variable\nX_2[GROUP_INDEX] = X_2[GROUP_INDEX].apply(lambda x: le[x.name].fit_transform(x))\n# Using the dictionary to label future data\nX_test[GROUP_INDEX] = X_test[GROUP_INDEX].apply(lambda x: le[x.name].transform(x))\n\n# Label encoding for seasonality\n# X_2[\"day\"] = X_2.index.dayofyear  # values are day of the month","60ebc062":"features = X_2.columns","16e36f4a":"X_test_1 = dp.out_of_sample(365)","6b4b7cdf":"from pyearth import Earth\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge, HuberRegressor, RidgeCV\nfrom sklearn.svm import LinearSVR\nfrom sklearn.ensemble import StackingRegressor","ec795acf":"# You'll add fit and predict methods to this minimal class\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n\nclass BoostedHybrid(BaseEstimator, RegressorMixin):\n    def __init__(self, model_1, model_2, scaler):\n        self.model_1 = model_1\n        self.model_2 = model_2\n        self.scaler = scaler\n        self.y_columns = None  # store column names from fit method\n    def fit(self, X, y): #, X_1_valid, y_valid\n        \"\"\"A reference implementation of a fitting function.\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            The training input samples.\n        y : array-like, shape (n_samples,) or (n_samples, n_outputs)\n            The target values (class labels in classification, real numbers in\n            regression).\n        Returns\n        -------\n        self : object\n            Returns self.\n        \"\"\"\n        X, y = check_X_y(X, y, accept_sparse=True)\n        # Train model_1\n        self.model_1.fit(X, y)\n\n        # Make predictions\n        y_fit = self.model_1.predict(X)\n        # Compute residuals\n        y_resid = y - y_fit\n        \n        # Make predictions\n#         y_valid_fit = self.model_1.predict(X_1_valid)\n        # Compute residuals\n#         y_valid_resid = y_log_valid - y_valid_fit\n\n        # Train model_2 on residuals , eval_set=[(X_1_valid, y_valid_resid)]\n        self.model_2.fit(X, y_resid)\n        \n        # Model2 prediction\n        y_fit2 = self.model_2.predict(X)\n        # Compute noise\n        y_resid2 = y_resid - y_fit2\n        \n        # Save data for question checking\n        self.y = y\n        self.y_fit = y_fit\n        self.y_resid = y_resid\n        self.y_fit2 = y_fit2\n        self.y_resid2 = y_resid2\n\n        self.is_fitted_ = True\n        \n        return self\n    \n    def predict(self, X):\n        \"\"\" A reference implementation of a predicting function.\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            The training input samples.\n        Returns\n        -------\n        y : ndarray, shape (n_samples,)\n            Returns an array of ones.\n        \"\"\"\n        X = check_array(X, accept_sparse=True)\n        check_is_fitted(self, 'is_fitted_')\n        # Predict with model_1\n        y_predict = self.model_1.predict(X)\n        # Add model_2 predictions to model_1 predictions\n        y_predict += self.model_2.predict(X)\n\n        return y_predict\n","ef2df988":"TRAIN_END_DATE = \"2019-12-31\"\nVALID_START_DATE = \"2015-01-01\"\nVALID_END_DATE = \"2018-12-31\"\n\ny_train, y_valid = y[:TRAIN_END_DATE], y[VALID_START_DATE:VALID_END_DATE]\nX1_train, X1_valid = X_1[:TRAIN_END_DATE], X_1[VALID_START_DATE:VALID_END_DATE]\nX2_train, X2_valid = X_2.loc[:TRAIN_END_DATE], X_2.loc[VALID_START_DATE:VALID_END_DATE]","265f424f":"preproc = StandardScaler()","f369a0be":"def model_fit_eval(hybrid_model, X_train, y_train, X_valid, y_valid, scaler):\n    # test_pred_list = []\n    # Boosted Hybrid\n    hybrid_model.fit(X_train, y_train) #, X_valid, y_valid\n    y_va_pred = hybrid_model.predict(X_valid)\n    \n    # loss_correction = 1\n    ###### Preprocess the validation data\n    y_va = np.expm1(y_valid.copy())\n    # Inference for validation\n    y_va_pred = np.expm1(hybrid_model.predict(X_valid))\n    loss_correction = evaluate_SMAPE(y_va, y_va_pred)\n    \n    ###### Visualize and evual\n    plot_oof(y_va, y_va_pred)\n    plot_true_vs_prediction(y_va, y_va_pred)\n#     plot_residuals(hybrid_model.y_resid)\n#     plot_residuals(hybrid_model.y_resid2)\n#     plot_residuals(model.y_resid3)\n    \n    ###### Validate against 2019 PSEU #######\n    # loss_correction = 1\n    ###### Preprocess the validation data\n    y_va = df_pseudolabels[column_y].values.reshape(-1, 1)\n    \n    # Inference test 2019 for validation\n    y_va_pred = np.expm1(hybrid_model.predict(scaler.transform(X_test[features])))\n    \n    # Evaluation: Execution time and SMAPE\n    smape_before_correction = np.mean(smape_loss(y_va, y_va_pred.reshape(-1, 1)))\n    smape = np.mean(smape_loss(y_va, y_va_pred.reshape(-1, 1)))\n    print(f'***********Test Data*****************')\n    loss_correction = find_min_SMAPE(y_va, y_va_pred.reshape(-1, 1))\n#     y_va_pred *= loss_correction\n    \n    ### Mean test prediction ###\n    # test_pred_list.append(y_va_pred)\n\n    print(f'SMAPE (before correction: {smape_before_correction:.5f})')\n    print(f'Min SMAPE: {np.mean(smape_loss(y_va, y_va_pred.reshape(-1, 1)*loss_correction))}')\n    \n    # return hybrid_model, test_pred_list, loss_correction\n    return y_va_pred","a90a7bff":"test_pred_list = []\nmodel_list = []\n\nfor seed in range(15):\n    estimator_stack = []\n\n    param1 = {\n        'loss_function': 'MultiRMSE',\n        'eval_metric': 'MultiRMSE',\n        'n_estimators': N_ESTIMATORS,\n        'od_type': 'Iter',\n        'od_wait': 20,\n        'random_state': seed,\n        'verbose': VERBOSE\n    }\n\n    # Try different combinations of the algorithms above KNeighborsRegressor\n    models_1 = [\n        Earth(verbose=VERBOSE),\n        Ridge(random_state=seed),\n        HuberRegressor(epsilon=1.20, max_iter=N_ESTIMATORS),\n        # LinearSVR(max_iter=N_ESTIMATORS, random_state=seed, verbose=VERBOSE),\n    ]\n\n    models_2 = [\n        XGBRegressor(\n            objective='reg:squarederror',\n            tree_method='gpu_hist',\n            eval_metric=smape_loss,\n            n_estimators=N_ESTIMATORS,\n            random_state=seed\n        ),\n        LGBMRegressor(\n            objective='regression',\n            n_estimators=N_ESTIMATORS,\n            random_state=seed\n        ),\n        CatBoostRegressor(**param1),\n    ]\n\n    for model_1 in models_1:\n        for model_2 in models_2:\n            model1_name = type(model_1).__name__\n            model2_name = type(model_2).__name__\n\n            hybrid_model = BoostedHybrid(\n                    model_1 = model_1,\n                    model_2 = model_2,\n                    scaler = preproc\n            )\n\n            print(f'******************Stacking {model1_name:>15} with {model2_name:<18}*************************')\n            estimator_stack.append((f'model_{model1_name}_{model2_name}', hybrid_model))\n\n\n    X2 = preproc.fit_transform(X2_train[features])\n    model = StackingRegressor(estimators=estimator_stack, final_estimator=RidgeCV(), n_jobs=-1, verbose=VERBOSE)\n    test_pred = model_fit_eval(\n        model,\n        X2,\n        np.log1p(y_train),\n        preproc.transform(X2_valid[features]),\n        np.log1p(y_valid),\n        preproc\n    )\n    \n    model_list.append(model)\n    test_pred_list.append(test_pred)","fd6d2bd3":"for country in np.unique(train_df['country']):\n    for product in np.unique(train_df['product']):\n        for store in np.unique(train_df['store']):\n            y_fit = plot_five_years_combination(feature_engineer, country=country, product=product, store=store)","4bbe1b68":"y_pred = sum(test_pred_list) \/ len(test_pred_list) # model.predict(X_test[features])","35d5e675":"%%time\nLOSS_CORRECTION = 1\n\n###### Preprocess the validation data\ny_va = df_pseudolabels[column_y].values.reshape(-1, 1)\n\n# Inference for validation\ny_va_pred = y_pred.copy().reshape(-1, 1) #model.predict(X_test[features])\n\n# Evaluation: Execution time and SMAPE\nsmape_before_correction = np.mean(smape_loss(y_va, y_va_pred))\nsmape = np.mean(smape_loss(y_va, y_va_pred))\nLOSS_CORRECTION = find_min_SMAPE(y_va, y_va_pred)\ny_va_pred *= LOSS_CORRECTION\n\nprint(f\" SMAPE: {smape:.5f} (before correction: {smape_before_correction:.5f})\")\nprint(np.mean(smape_loss(y_va, y_va_pred)))","51886078":"for model in model_list:\n    plot_oof(y_va, y_va_pred)\n    plot_true_vs_prediction(y_va, y_va_pred)\n    plot_residuals(model.estimators_[0].y_resid)\n    plot_residuals(model.estimators_[0].y_resid2)\n    # plot_residuals(model.y_resid3)","6e9eefb4":"from math import ceil, floor, sqrt\n# from https:\/\/www.kaggle.com\/fergusfindley\/ensembling-and-rounding-techniques-comparison\ndef geometric_round(arr):\n    result_array = arr\n    result_array = np.where(result_array < np.sqrt(np.floor(arr)*np.ceil(arr)), np.floor(arr), result_array)\n    result_array = np.where(result_array >= np.sqrt(np.floor(arr)*np.ceil(arr)), np.ceil(arr), result_array)\n\n    return result_array","f369f1e2":"sub = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv\")","df806fce":"# Inference for test\ntest_prediction_list = []\ntest_prediction_list.append(y_pred) # * LOSS_CORRECTION)\n\ntest_prediction_list.append(df_pseudolabels[column_y].values)\n\ndf_pseudolabels1 = pd.read_csv(PSEUDO_DIR2, index_col=ID)\ntest_prediction_list.append(df_pseudolabels1[column_y].values)\n\ntest_prediction_list = np.median(test_prediction_list, axis=0) # median is better https:\/\/www.kaggle.com\/saraswatitiwari\/tabular-playground-series-22\n\nif len(test_prediction_list) > 0:\n    # Create the submission file\n    submission = pd.DataFrame(data=np.zeros((sub.shape[0], 2)), index=sub.index.tolist(), columns=[ID,column_y])\n    submission[ID] = sub[ID]\n    submission[column_y] = test_prediction_list\n    submission[column_y] = geometric_round(submission[column_y]).astype(int) # https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/299162\n    submission.to_csv('submission.csv', index=False)\n\n    # Plot the distribution of the test predictions\n    plt.figure(figsize=(16,3))\n    plt.hist(\n        train_df[column_y],\n        bins=np.linspace(0, 3000, 201),\n        density=True,\n        label='Training')\n    plt.hist(\n        submission[column_y],\n        bins=np.linspace(0, 3000, 201),\n        density=True,\n        rwidth=0.5,\n        label='Test predictions')\n    plt.xlabel(column_y)\n    plt.ylabel('Frequency')\n    plt.legend()\n    plt.show()","c1c3f2a2":"display(submission.head(30))\ndisplay(submission.tail(30))","5f576cc8":"# Data Pipeline","b0460363":"# Components and Residuals #\n\nSo that we can design effective hybrids, we need a better understanding of how time series are constructed. We've studied up to now three patterns of dependence: trend, seasons, and cycles. Many time series can be closely described by an additive model of just these three components plus some essentially unpredictable, entirely random *error*:\n\n```\nseries = trend + seasons + cycles + error\n```\n\nEach of the terms in this model we would then call a **component** of the time series.\n\nThe **residuals** of a model are the difference between the target the model was trained on and the predictions the model makes -- the difference between the actual curve and the fitted curve, in other words. Plot the residuals against a feature, and you get the \"left over\" part of the target, or what the model failed to learn about the target from that feature.","1b70dee1":"# Libraries","7abd64aa":"# What is Seasonality? #\n\nWe say that a time series exhibits **seasonality** whenever there is a regular, periodic change in the mean of the series. Seasonal changes generally follow the clock and calendar -- repetitions over a day, a week, or a year are common. Seasonality is often driven by the cycles of the natural world over days and years or by conventions of social behavior surrounding dates and times.\n### Choosing Fourier features with the Periodogram\n\nHow many Fourier pairs should we actually include in our feature set? We can answer this question with the periodogram. The **periodogram** tells you the strength of the frequencies in a time series. Specifically, the value on the y-axis of the graph is `(a ** 2 + b ** 2) \/ 2`, where `a` and `b` are the coefficients of the sine and cosine at that frequency (as in the *Fourier Components* plot above).\n\n<figure style=\"padding: 1em;\">\n<img src=\"https:\/\/i.imgur.com\/PK6WEe3.png\" width=600, alt=\"\">\n<figcaption style=\"textalign: center; font-style: italic\"><center>Periodogram for the <em>Wiki Trigonometry<\/em> series.<\/center><\/figcaption>\n<\/figure>\n\nFrom left to right, the periodogram drops off after *Quarterly*, four times a year. That was why we chose four Fourier pairs to model the annual season. The *Weekly* frequency we ignore since it's better modeled with indicators.\n\n### Computing Fourier features (optional)\n\nKnowing how Fourier features are computed isn't essential to using them, but if seeing the details would clarify things, the cell hidden cell below illustrates how a set of Fourier features could be derived from the index of a time series. (We'll use a library function from `statsmodels` for our applications, however.)","9c47a53c":"# Submission\nOnce you're satisfied with everything, it's time to create your final predictions! This cell will:\n\n- use the best trained model to make predictions from the test set\n- save the predictions to a CSV file\n","ef05535c":"# Training","a8e02ccf":"# Data\/Feature Engineering","d4c6fa23":"## Pseudolabeling","1ab5dd08":"The periodogram agrees with the seasonal plots above: a strong semiweekly season and a weaker annual season. The weekly season we'll model with indicators and the annual season with Fourier features. From right to left, the periodogram falls off between Bimonthly (6) and Monthly (12), so let's use 10 Fourier pairs.\n\nWe'll create our seasonal features using DeterministicProcess, the same utility we used in Lesson 2 to create trend features. To use two seasonal periods (weekly and annual), we'll need to instantiate one of them as an \"additional term\":","1a49819f":"# **Acknowledgements:**\n* Kaggle's [time series course](https:\/\/www.kaggle.com\/learn\/time-series).\n* [TPS2201_Hybrid_Time_Series notebook](https:\/\/www.kaggle.com\/teckmengwong\/tps2201-hybrid-time-series) by [Teck Meng Wong](https:\/\/www.kaggle.com\/teckmengwong).\n* [Many great notebooks](https:\/\/www.kaggle.com\/ambrosm\/code) by [AmbrosM's](https:\/\/www.kaggle.com\/ambrosm) ","bd86cfae":"## Using StandardScaler","13a5d5d6":"# Inference validation","d4c977f9":"# Inference year 2019 test data","b5bb26a8":"# Fine tuning","505c6821":"# Load Data #\n\nAnd now we can call the data loader and get the processed data splits:","25af7a54":"# Loss function SMAPE","479bc9bb":"Now let's look at the periodogram:","41b77c11":"# Hybrid Models\nLinear regression excels at extrapolating trends, but can't learn interactions. XGBoost excels at learning interactions, but can't extrapolate trends. We'll learn how to create \"hybrid\" forecasters that combine complementary learning algorithms and let the strengths of one make up for the weakness of the other."}}