{"cell_type":{"5a95b4b8":"code","bf6eb1c2":"code","bbfa2c39":"code","d2b458d2":"code","eed6f4da":"code","2a2ee225":"code","944e4741":"code","44c5bac9":"code","d53166a4":"code","136a5f45":"code","dbd15d63":"code","d19392c5":"code","3c9542dd":"code","f45b389b":"code","62258836":"code","52e684c2":"code","02cd441b":"code","b59b603f":"code","b0e9390e":"code","8a846dcc":"code","6266cac7":"code","3fd41fb8":"code","7453b909":"code","50fae7a5":"code","4af7a3b5":"code","e4521f29":"code","01f47b50":"code","90f021bd":"code","8a77d994":"markdown","dd0e8186":"markdown","a7f77f1e":"markdown","c22dafc1":"markdown","a2316a59":"markdown"},"source":{"5a95b4b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","bf6eb1c2":"BT_train=pd.read_csv(\"\/kaggle\/input\/scm-final-evaluation-challenge\/new_BT_train.csv\")\nBT_test=pd.read_csv(\"\/kaggle\/input\/scm-final-evaluation-challenge\/new_BT_test.csv\")\nAT_train=pd.read_csv(\"\/kaggle\/input\/scm-final-evaluation-challenge\/new_AT_train.csv\")\nAT_test=pd.read_csv(\"\/kaggle\/input\/scm-final-evaluation-challenge\/new_AT_test.csv\")","bbfa2c39":"import pandas_profiling\nAT_train.profile_report()","d2b458d2":"BT_train.profile_report()","eed6f4da":"TestFeature=AT_train\nTestFeatureTest=AT_test","2a2ee225":"Distrib=AT_train\nDistrib=Distrib.drop(['label'], axis='columns', inplace=False).unstack()\nDistrib2=BT_train\nDistrib2=Distrib2.drop(['label'], axis='columns', inplace=False).unstack()","944e4741":"# After training histogram\nDistrib.hist()\n","44c5bac9":"#Before training histogram\nDistrib2.hist()","d53166a4":"# importing necessary libraries \nfrom sklearn import datasets \nfrom sklearn.metrics import confusion_matrix, classification_report \nfrom sklearn.model_selection import train_test_split","136a5f45":"Y=TestFeature[\"label\"]\ndata=TestFeature\nX=data.drop(['label'], axis='columns', inplace=False)","dbd15d63":"X_train, X_val, Y_train, Y_val = train_test_split(X, Y)","d19392c5":"from sklearn.tree import DecisionTreeClassifier \nfrom sklearn.model_selection import cross_validate\ndtree_model = DecisionTreeClassifier(criterion='entropy',splitter=\"best\").fit(X_train, Y_train) \ndtree_predictions = dtree_model.predict(X_val) \nprint(\"decision tree validation score :\",dtree_model.score(X_val,Y_val))\nprint(\"decision tree training score :\",dtree_model.score(X_train,Y_train))\ny_true, y_pred = Y_val , dtree_predictions\nprint('Results on the test set:')\nprint(classification_report(y_true, y_pred))\ncv_results = cross_validate(dtree_model, X_train, Y_train, cv=5)\nnp.mean(cv_results['test_score'])","3c9542dd":"! pip install -q scikit-plot","f45b389b":"from sklearn.ensemble import AdaBoostClassifier\nAda_model = AdaBoostClassifier(dtree_model,n_estimators=100).fit(X_train, Y_train) \nAda_predictions = Ada_model.predict(X_val) \nprint(\"Adaboost validation score :\",Ada_model.score(X_val,Y_val))\nprint(\"Adaboost training score :\",Ada_model.score(X_train,Y_train))\ny_true, y_pred = Y_val , Ada_predictions\nprint('Results on the test set:')\nprint(classification_report(y_true, y_pred))\ncv_results = cross_validate(Ada_model, X_train, Y_train, cv=5)\nprint(np.mean(cv_results['test_score']))","62258836":"import scikitplot as skplt\n\nskplt.metrics.plot_confusion_matrix(\n    Y_val, \n    Ada_predictions,normalize=True,\n    figsize=(10,10))","52e684c2":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import normalize\nfrom sklearn.neighbors import KNeighborsClassifier","02cd441b":"RandomForest=RandomForestClassifier(criterion='entropy',n_estimators=200)\nRandomForest.fit(X_train,Y_train)\nRandomForest_predictions = RandomForest.predict(X_val)\nprint(\"RandomForest validation score:\",RandomForest.score(X_val,Y_val))\nprint(\"RandomForest training score:\",RandomForest.score(X_train,Y_train))\ny_true, y_pred = Y_val , RandomForest.predict(X_val)\nprint('Results on the test set:')\nprint(classification_report(y_true, y_pred))\nfrom sklearn.model_selection import cross_validate\ncv_results = cross_validate(RandomForest, X_train, Y_train, cv=5)\nprint(np.mean(cv_results['test_score']))","b59b603f":"#Import libraries:\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics   #Additional scklearn functions\nfrom sklearn.model_selection import GridSearchCV #Perforing grid search","b0e9390e":"xgb1 = XGBClassifier(objective= 'multi:softmax',probability=True,n_estimators=100,colsample_bytree=0.3,colsample_bylevel=0.7,max_depth=6)\nxgb1.fit(X_train,Y_train)\nprint(\"XGB validation score:\",xgb1.score(X_val,Y_val))\nprint(\"XGB training score:\",xgb1.score(X_train,Y_train))\ny_true, y_pred = Y_val , xgb1.predict(X_val)\nprint('Results on the test set:')\nprint(classification_report(y_true, y_pred))\ncv_results = cross_validate(xgb1, X_train, Y_train, cv=5)\nprint(np.mean(cv_results['test_score']))\nprint(xgb1.get_params)","8a846dcc":"skplt.metrics.plot_confusion_matrix(\n    Y_val, \n    xgb1.predict(X_val),normalize=True,\n    figsize=(10,10))","6266cac7":"from sklearn import svm\nSVC= svm.SVC(probability=True,gamma='auto')\nSVC.fit(X_train,Y_train)\ny_true, y_pred = Y_val , SVC.predict(X_val)\nfrom sklearn.metrics import classification_report\nprint('Results on the test set:')\nprint(classification_report(y_true, y_pred))\nprint(\"SVC validation score:\",SVC.score(X_val,Y_val))\nprint(\"SVC training score:\",SVC.score(X_train,Y_train))\ncv_results = cross_validate(SVC, X_train, Y_train, cv=5)\nprint(np.mean(cv_results['test_score']))","3fd41fb8":"from sklearn.neighbors import KNeighborsClassifier\nKNN = KNeighborsClassifier(n_neighbors=7)\nKNN.fit(X_train, Y_train)\ny_true, y_pred = Y_val , KNN.predict(X_val)\nprint('Results on the test set:')\nprint(classification_report(y_true, y_pred))\nprint(\"KNN validation score:\",KNN.score(X_val,Y_val))\nprint(\"KNN training score:\",KNN.score(X_train,Y_train))\ncv_results = cross_validate(KNN, X_train, Y_train, cv=5)\nprint(np.mean(cv_results['test_score']))","7453b909":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import cross_val_score\nVote = VotingClassifier(estimators=[('SVM', SVC),(\"xgb\",xgb1),(\"Adaboost\",Ada_model)],voting='soft')\nfor clf, label in zip([SVC,xgb1,Ada_model,Vote], ['SVM','XGB',\"Adaboost\",'vote']):\n    scores = cross_val_score(clf, X_train, Y_train, scoring='accuracy', cv=5)\n    print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))","50fae7a5":"Vote.fit(X_train,Y_train)\ny_true, y_pred = Y_val , Vote.predict(X_val)\nprint('Results on the test set:')\nprint(classification_report(y_true, y_pred))\nfrom sklearn.model_selection import cross_validate\ncv_results = cross_validate(KNN, X_train, Y_train, cv=5)\nprint(\"Vote validation score:\",Vote.score(X_val,Y_val))\nprint(\"Vote training score:\",Vote.score(X_train,Y_train))\nprint(np.mean(cv_results['test_score']))","4af7a3b5":"skplt.metrics.plot_confusion_matrix(\n    Y_val, \n    Vote.predict(X_val),normalize=True,\n    figsize=(10,10))","e4521f29":"resultat=Vote.predict(AT_test)\nsubmission=pd.DataFrame()\nsubmission['id']=range(320)\nsubmission['label']=resultat","01f47b50":"submission","90f021bd":"filename = 'xgb.csv'\n\nsubmission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","8a77d994":"## Decision Tree Baseline Model","dd0e8186":"## Voting classifier\n","a7f77f1e":"## Building the model\n","c22dafc1":"# **Data Loading**","a2316a59":"# Statistical Analysis\n"}}