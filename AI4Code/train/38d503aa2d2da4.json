{"cell_type":{"e62900e2":"code","f385ac8e":"code","1f6301c8":"markdown"},"source":{"e62900e2":"import keras.backend as K\n# For scoring coding on continuous manner.\ndef QWKloss_score(y_true, y_pred):\n    N = K.cast(K.shape(y_true)[0], 'float32')\n    \n    WC = (y_pred - y_true)**2 \/ N\n    WE = (y_pred - K.transpose(y_true))**2 \/ (N**2)\n    \n    k = K.sum(WC) \/ K.sum(WE)\n    \n    return 1-k","f385ac8e":"import numpy as np\nfrom sklearn.metrics import cohen_kappa_score\n\n# check ~ comparing sklearn\nfor _ in range(10):\n    N = 1000\n    y_true = np.random.randint(5, size=(N))\n    y_pred = np.random.randint(5, size=(N))  # cast to int is necessary for sklearn. but youcan use float in Keras use.\n\n    skl = cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\n    s = QWKloss_score(K.variable(y_true.reshape(-1, 1)), K.variable(y_pred.reshape(-1, 1)))\n    org = K.get_value(s)\n    \n    print(skl, org)","1f6301c8":"I implemented continuous approximation of quadratic weighted cohen's kappa for regressor on Keras.  \nThe below function can be used as regressor loss function (but for loss, shoud change 1-k to k for smaller the better).  \nThe performance of this loss is same level as binary_crossentropy in this competiton.  \n\nI don't know the reason, there seems some local optima around 1.0, then when you use this, recommend to use other loss for the warming up (1 or 2 epochs). "}}