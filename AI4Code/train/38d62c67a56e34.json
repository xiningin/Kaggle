{"cell_type":{"5085c283":"code","a8871f60":"code","ba75c92f":"code","7046631c":"code","c9f1c5f0":"code","e51bf2d7":"code","9bba9ca3":"code","a4d46c30":"code","469c8758":"code","3e8c7663":"code","99e28f8e":"code","ab5bf752":"code","a2613712":"code","f633d836":"code","2ef9f74d":"code","bbbb1464":"code","50c9b6c6":"code","eeafc01f":"code","8fc100ca":"markdown","bb46e68f":"markdown","8bfbf534":"markdown","2278f4e0":"markdown","437e55f0":"markdown","b7ce904e":"markdown","941fb5cf":"markdown","4f5b6976":"markdown","236de5b7":"markdown","261bcc76":"markdown"},"source":{"5085c283":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns","a8871f60":"import os\nimport missingno as msno\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndf = pd.read_csv(\"..\/input\/credit-card-customers\/BankChurners.csv\")\n\n\n#Remove the ID column and the last two columns\ndf = df.iloc[:,1:-2]\n#Check out the missing values\ndf.replace([\"Unknown\",\"NaN\"],np.nan, inplace=True)\n#Visualize the missing value\nmsno.matrix(df) \n# We have missing values in education, maritial and income randomly distributed\n# We can delete rows with more than 2 missing values\ndf[\"num_missing\"] = df.apply(lambda x: x.isnull().sum(), axis=1)\n# I checked the maximum missing number per row is 2. \ndf = df[df[\"num_missing\"]!=2]\n# I will put the missing value back to Unknown here since they might contain info\ndf.fillna(\"Unknown\", inplace=True)","ba75c92f":"# For all the categorical data, we create order for those which has order meaning and encode the rest\neducation_dic = {'Uneducated':0, 'High School':1, 'Unknown':2, 'College':3, 'Graduate':4, 'Post-Graduate':5,\\\n    'Doctorate':6}\nincome_dic = {'Less than $40K':0, '$40K - $60K':1, \"Unknown\":2, '$60K - $80K':3, '$80K - $120K':4,'$120K +':5 }\nchurn_dic = {'Existing Customer':0, 'Attrited Customer':1}\ncard_dic = {'Blue':0, 'Silver':1, 'Gold':2, 'Platinum':3}\n\ndf[\"Education_Level\"].replace(education_dic, inplace=True)\ndf[\"Income_Category\"].replace(income_dic, inplace=True)\ndf[\"Attrition_Flag\"].replace(churn_dic, inplace=True)\ndf[\"Card_Category\"].replace(card_dic, inplace=True)","7046631c":"# For sure those numbers might not be in the correct magitude. We can adjust those later after we see the feature importance.\ndf = pd.get_dummies(df)\ndf.shape","c9f1c5f0":"#Correlation Plot\nfrom string import ascii_letters\nsns.set(style=\"white\")\n# Generate a large random dataset\nrs = np.random.RandomState(22)\n# Compute the correlation matrix\ncorr = df.corr()\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","e51bf2d7":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df,df[\"Attrition_Flag\"], test_size=0.2,\n                                                    random_state=42)\nX_train.shape, X_test.shape # We have 7845 in train and 1962 in test\n","9bba9ca3":"from sklearn.utils import resample\nX = X_train.copy()\n# separate minority and majority classes\nnon_churn = X[X[\"Attrition_Flag\"]==0]\nchurn     = X[X[\"Attrition_Flag\"]==1]\n\n# upsample minority\nchurn_upsampled = resample(churn,\n                          replace=True, # sample with replacement\n                          n_samples=len(non_churn), # match number in majority class\n                          random_state=1) # reproducible results\n\nupsampled = pd.concat([non_churn,churn_upsampled])\n\n# check new class counts\n# print(upsampled[\"Attrition_Flag\"].value_counts()) #6561 for each category","a4d46c30":"X_train = upsampled.drop(\"Attrition_Flag\",axis=1)\ny_train = upsampled[\"Attrition_Flag\"].astype(int)\nX_test = X_test.drop(\"Attrition_Flag\", axis=1)\ny_test = y_test.astype(int)","469c8758":"#Lets do a simple logistic regression here first \nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score\n\nfrom sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(solver = \"lbfgs\",random_state=42).fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\n# Model Performance\ndef prediction_result(y_test, y_pred):\n    print(\"Accuracy : \", accuracy_score(y_test, y_pred) *  100)\n    print(\"Recall : \", recall_score(y_test, y_pred) *  100)\n    print(\"Precision : \", precision_score(y_test, y_pred) *  100)\n    print(confusion_matrix(y_test, y_pred))\n    print(classification_report(y_test, y_pred))\nprediction_result(y_test,y_pred)","3e8c7663":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators = 50, random_state = 0).fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\n# Model Performance\nprediction_result(y_test,y_pred)","99e28f8e":"import xgboost as xgb\nclf = xgb.XGBClassifier( \n    n_estimatoryhs=1000,\n    max_depth=3, \n    learning_rate=0.02, \n    subsample=0.8,\n    colsample_bytree=0.4, \n    missing=-1, \n)\n\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\n# Model Performance\nprediction_result(y_test,y_pred)","ab5bf752":"from sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nX_train, X_test, y_train, y_test = train_test_split(df,df[\"Attrition_Flag\"], test_size=0.2,\n                                                    random_state=40)\nprint(X_train.shape, y_train.shape)\nX_train.shape, X_test.shape # We have 7845 in train and 1962 in test\n\noversample = SMOTE()\nX_train_SMOTE, y_train_SMOTE = oversample.fit_resample(X_train,y_train)\n\nprint(\"After Upsampling:-\")\nprint(X_train_SMOTE.shape, y_train_SMOTE.shape)\n\nX_train_SMOTE = X_train_SMOTE.drop(\"Attrition_Flag\",axis=1)\nX_test = X_test.drop(\"Attrition_Flag\", axis=1)","a2613712":"def prediction_result(y_test, y_pred):\n    print(\"Accuracy : \", accuracy_score(y_test, y_pred) *  100)\n    print(\"Recall : \", recall_score(y_test, y_pred) *  100)\n    print(\"Precision : \", precision_score(y_test, y_pred) *  100)\n    print(confusion_matrix(y_test, y_pred))\n    print(classification_report(y_test, y_pred))\n    \n# Logistic Regression\nclf = LogisticRegression(solver = \"lbfgs\",random_state=42).fit(X_train_SMOTE, y_train_SMOTE)\ny_pred = clf.predict(X_test)\nprint(\"LOGISTIC RESULT\")\nprint(prediction_result(y_test,y_pred))\n\n# Random Forest\nclf = RandomForestClassifier(n_estimators = 50, random_state = 0).fit(X_train_SMOTE, y_train_SMOTE)\ny_pred = clf.predict(X_test)\nprint(\"RANDOM FOREST RESULT\")\nprint(prediction_result(y_test,y_pred))\n\n# XGBoost\nimport xgboost as xgb\nclf = xgb.XGBClassifier( \n    n_estimatoryhs=1000,\n    max_depth=3, \n    learning_rate=0.02, \n    subsample=0.8,\n    colsample_bytree=0.4, \n    missing=-1, \n)\n\nclf.fit(X_train_SMOTE, y_train_SMOTE)\ny_pred = clf.predict(X_test)\n\n# Model Performance\nprint(\"XGBOOST RESULT\")\nprint(prediction_result(y_test,y_pred))","f633d836":"import h2o\nh2o.init()","2ef9f74d":"from h2o.automl import H2OAutoML\nh2o_df = h2o.H2OFrame(df)\nh2o_df[\"Attrition_Flag\"] = h2o_df[\"Attrition_Flag\"].asfactor()\n# h2o_df.describe()\ntrain, test = h2o_df.split_frame(ratios=[.8])\n\n# Identify predictors and response\nx = train.columns\ny = \"Attrition_Flag\"\nx.remove(y)\n\ntrain[y] = train[y].asfactor()\ntest[y] = test[y].asfactor()\n\naml = H2OAutoML(max_runtime_secs=600,\n                exclude_algos=['DeepLearning'],\n                seed=1,\n                stopping_metric='AUC',\n                sort_metric='AUC',\n                balance_classes=True,\n                project_name='Churn_Prediction'\n)\n\n%time aml.train(x=x, y=y, training_frame=train)","bbbb1464":"# View the AutoML Leaderboard\nlb = aml.leaderboard\nlb.head(rows=lb.nrows)  # Print all rows instead of default","50c9b6c6":"model = h2o.get_model('StackedEnsemble_AllModels_AutoML_20201222_163314')\nmodel.model_performance(test)","eeafc01f":"model2 = h2o.get_model(\"XGBoost_grid__1_AutoML_20201222_163314_model_4\")\nmodel2.varimp_plot(num_of_features=20)","8fc100ca":"## SMOTE and Model Prediction","bb46e68f":"## Variable Importance for Churn Prediction","8bfbf534":"### Random Forest","2278f4e0":"## Upsampling with H2O AutoML","437e55f0":"### The result is not as good as upsampling(out of my expectation). ","b7ce904e":"### XGBoost","941fb5cf":"## Upampling & Model Prediction","4f5b6976":"### Logistic Model","236de5b7":"### The recall is 258\/(258+36)=87%. It can be improved to 1 with a lower threshold of 0.012851.","261bcc76":"### I ran into a error because h2o take the task as a regression task. I have no idea what happened since I already specified the target column as \"enum\". No related info online and the error disappeared after I restarted the kernel. "}}