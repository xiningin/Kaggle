{"cell_type":{"da5a7b80":"code","7211e14c":"code","2e0230a8":"code","78f05d67":"code","85ba0086":"code","89b72d81":"code","551fd4b7":"code","27d28fca":"code","d380cdd3":"code","479bc6ef":"code","9cd82c35":"code","a02185c2":"code","b0ab2c2a":"code","89ac60b4":"code","873579a8":"code","73b233da":"code","a9b9cd36":"code","40403931":"code","850ca449":"code","cf8bb0b1":"code","d6caeef7":"code","036b05d0":"code","07175a09":"code","b34175ac":"code","0a17db1e":"code","99095d8b":"code","5453849c":"code","16529414":"code","78e5abec":"code","8070090c":"code","622580b8":"markdown","78888823":"markdown","32f60e51":"markdown","e1883e83":"markdown","749404aa":"markdown","fd0fbefc":"markdown","145bf325":"markdown","c811619b":"markdown","b0c1ec5a":"markdown"},"source":{"da5a7b80":"import numpy as np\nimport tensorflow as tf","7211e14c":"import gensim\nmodel = gensim.models.Word2Vec.load('Twt-CBOW')","2e0230a8":"corpus_raw = 'He is the king . The king is royal . She is the royal queen'\ncorpus_raw = corpus_raw.lower()","78f05d67":"words = []\nfor word in corpus_raw.split():\n    if word != '.': \n        words.append(word)","85ba0086":"words = set(words) ","89b72d81":"words","551fd4b7":"word2int = {}\nint2word = {}\nvocab_size = len(words)\n\nfor i,word in enumerate(words):\n    word2int[word] = i\n    int2word[i] = word","27d28fca":"print(word2int['queen'])","d380cdd3":"print(int2word[2])","479bc6ef":"raw_sentences = corpus_raw.split('.')\nsentences = []\nfor sentence in raw_sentences:\n    sentences.append(sentence.split())\n                 ","9cd82c35":"raw_sentences","a02185c2":"print(sentences)","b0ab2c2a":"data = []\nWINDOW_SIZE = 2\n\ndef to_one_hot(data_point_index, vocab_size):\n    temp = np.zeros(vocab_size)\n    temp[data_point_index] = 1\n    return temp\n","89ac60b4":"for sentence in sentences:\n    for word_index, word in enumerate(sentence):\n        for nb_word in sentence[max(word_index - WINDOW_SIZE, 0) : min(word_index + WINDOW_SIZE, len(sentence)) + 1] : \n            if nb_word != word:\n                data.append([word, nb_word])","873579a8":"data","73b233da":"x_train = [] \ny_train = [] \nfor data_word in data:\n    x_train.append(to_one_hot(word2int[ data_word[0] ], vocab_size))\n    y_train.append(to_one_hot(word2int[ data_word[1] ], vocab_size))\n","a9b9cd36":"x_train = np.asarray(x_train)\ny_train = np.asarray(y_train)","40403931":"print(x_train.shape, y_train.shape)","850ca449":"x_train[:5,:]","cf8bb0b1":"y_train[:5,:]","d6caeef7":"model = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(5, input_dim=x_train.shape[1]))\nmodel.add(tf.keras.layers.Dense(y_train.shape[1], activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])","036b05d0":"history = model.fit(\n    x_train,\n    y_train,\n    epochs=10000,\n    shuffle=True,\n    verbose=1,\n)","07175a09":"weights_list = model.get_weights()\nlen(weights_list)","b34175ac":"for i in range(len(weights_list)):\n    print(weights_list[i].shape)","0a17db1e":"word2int['queen']","99095d8b":"word2int","5453849c":"vectors","16529414":"vectors = weights_list[0]\nprint(vectors[ word2int['queen'] ])","78e5abec":"def euclidean_dist(vec1, vec2):\n    return np.sqrt(np.sum((vec1-vec2)**2))\n\ndef find_closest(word_index, vectors):\n    min_dist = 10000 # to act like positive infinity\n    min_index = -1\n    query_vector = vectors[word_index]\n    for index, vector in enumerate(vectors):\n        if euclidean_dist(vector, query_vector) < min_dist and not np.array_equal(vector, query_vector):\n            min_dist = euclidean_dist(vector, query_vector)\n            min_index = index\n    return min_index","8070090c":"print(int2word[find_closest(word2int['queen'], vectors)])","622580b8":"# Load Courps","78888823":"## <center> Word2Vec from Scratch using Keras","32f60e51":"##  Make a Dictionary","e1883e83":"##  One Hot Encoding OHE","749404aa":"## Tokenization","fd0fbefc":"#  Model Training Keras","145bf325":"##  Windowing","c811619b":"#  Import Libiraries","b0c1ec5a":"##  Input Output Encoding"}}