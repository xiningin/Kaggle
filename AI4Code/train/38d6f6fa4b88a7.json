{"cell_type":{"eab3052d":"code","8b8f9d17":"code","dad67123":"code","babb461f":"code","9ef00549":"code","bf931565":"code","5f3cf7da":"code","7c2792b4":"code","61e6541c":"code","2a943738":"code","18e8fef0":"code","7fd29f80":"code","15ef890d":"code","3e5533a4":"code","26071df6":"code","82b32063":"code","f118e5b5":"code","a7bcf801":"code","b5f35d70":"code","e701f87f":"code","c4c49518":"code","c77bd53b":"code","1f1ad8e5":"code","c6ecbb63":"code","d26ad1db":"code","d298cc8c":"code","77a2ac12":"code","3f143c26":"code","4e21d73a":"code","19b93cd7":"code","388b6724":"code","3351b221":"code","c33afa8a":"code","0095c421":"code","1d728bd4":"code","1e87c503":"code","4088d708":"code","ea9f59c3":"code","60d142c2":"code","25b373bf":"code","91457d99":"markdown","06de610e":"markdown"},"source":{"eab3052d":"!pip install vaderSentiment\n!pip install textstat\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport sys\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport nltk\nfrom nltk.stem.porter import *\nimport string\nimport re\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\nfrom textstat.textstat import *\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import classification_report\nfrom sklearn.svm import LinearSVC\nimport matplotlib.pyplot as plt\nimport seaborn\n%matplotlib inline\n","8b8f9d17":"nRowsRead = None # specify 'None' if want to read whole file\n# labeled_data.csv may have more rows in reality, but we are only loading\/previewing the first 1000 rows\ndf1 = pd.read_csv('\/kaggle\/input\/labeled_data.csv', delimiter=',', nrows = nRowsRead)\ndf1.dataframeName = 'labeled_data.csv'\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')","dad67123":"df1.head(5)","babb461f":"df1.describe()","9ef00549":"df1.columns","bf931565":"df1['class'].value_counts()","5f3cf7da":"df1['class'].hist()","7c2792b4":"a = df1.loc[df1['class'] == 0]","61e6541c":"a[\"tweet\"]","2a943738":"!pip install -U nltk","18e8fef0":"import nltk\nnltk.download('wordnet')","7fd29f80":"!git clone https:\/\/github.com\/jasonwei20\/eda_nlp.git","15ef890d":"!cd eda_nlp","3e5533a4":"!ls","26071df6":"tweets=df1.tweet","82b32063":"stopwords=stopwords = nltk.corpus.stopwords.words(\"english\")\n\nother_exclusions = [\"#ff\", \"ff\", \"rt\"]\nstopwords.extend(other_exclusions)\n\nstemmer = PorterStemmer()\n\n\ndef preprocess(text_string):\n    \"\"\"\n    Accepts a text string and replaces:\n    1) urls with URLHERE\n    2) lots of whitespace with one instance\n    3) mentions with MENTIONHERE\n\n    This allows us to get standardized counts of urls and mentions\n    Without caring about specific people mentioned\n    \"\"\"\n    space_pattern = '\\s+'\n    giant_url_regex = ('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    mention_regex = '@[\\w\\-]+'\n    parsed_text = re.sub(space_pattern, ' ', text_string)\n    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n    parsed_text = re.sub(mention_regex, '', parsed_text)\n    return parsed_text\n\ndef tokenize(tweet):\n    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n    tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n    tokens = [stemmer.stem(t) for t in tweet.split()]\n    return tokens\n\ndef basic_tokenize(tweet):\n    \"\"\"Same as tokenize but without the stemming\"\"\"\n    tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", tweet.lower())).strip()\n    return tweet.split()\n\nvectorizer = TfidfVectorizer(\n    tokenizer=tokenize,\n    preprocessor=preprocess,\n    ngram_range=(1, 3),\n    stop_words=stopwords,\n    use_idf=True,\n    smooth_idf=False,\n    norm=None,\n    decode_error='replace',\n    max_features=10000,\n    min_df=5,\n    max_df=0.75\n    )\n","f118e5b5":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","a7bcf801":"#Construct tfidf matrix and get relevant scores\ntfidf = vectorizer.fit_transform(tweets).toarray()\nvocab = {v:i for i, v in enumerate(vectorizer.get_feature_names())}\nidf_vals = vectorizer.idf_\nidf_dict = {i:idf_vals[i] for i in vocab.values()} #keys are indices; values are IDF scores","b5f35d70":"#Get POS tags for tweets and save as a string\ntweet_tags = []\nfor t in tweets:\n    tokens = basic_tokenize(preprocess(t))\n    tags = nltk.pos_tag(tokens)\n    tag_list = [x[1] for x in tags]\n    tag_str = \" \".join(tag_list)\n    tweet_tags.append(tag_str)","e701f87f":"#We can use the TFIDF vectorizer to get a token matrix for the POS tags\npos_vectorizer = TfidfVectorizer(\n    tokenizer=None,\n    lowercase=False,\n    preprocessor=None,\n    ngram_range=(1, 3),\n    stop_words=None,\n    use_idf=False,\n    smooth_idf=False,\n    norm=None,\n    decode_error='replace',\n    max_features=5000,\n    min_df=5,\n    max_df=0.75,\n    )","c4c49518":"#Construct POS TF matrix and get vocab dict\npos = pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()\npos_vocab = {v:i for i, v in enumerate(pos_vectorizer.get_feature_names())}","c77bd53b":"#Now get other features\nsentiment_analyzer = VS()\n\ndef count_twitter_objs(text_string):\n    \"\"\"\n    Accepts a text string and replaces:\n    1) urls with URLHERE\n    2) lots of whitespace with one instance\n    3) mentions with MENTIONHERE\n    4) hashtags with HASHTAGHERE\n\n    This allows us to get standardized counts of urls and mentions\n    Without caring about specific people mentioned.\n    \n    Returns counts of urls, mentions, and hashtags.\n    \"\"\"\n    space_pattern = '\\s+'\n    giant_url_regex = ('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    mention_regex = '@[\\w\\-]+'\n    hashtag_regex = '#[\\w\\-]+'\n    parsed_text = re.sub(space_pattern, ' ', text_string)\n    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n\ndef other_features(tweet):\n    \"\"\"This function takes a string and returns a list of features.\n    These include Sentiment scores, Text and Readability scores,\n    as well as Twitter specific features\"\"\"\n    sentiment = sentiment_analyzer.polarity_scores(tweet)\n    \n    words = preprocess(tweet) #Get text only\n    \n    syllables = textstat.syllable_count(words)\n    num_chars = sum(len(w) for w in words)\n    num_chars_total = len(tweet)\n    num_terms = len(tweet.split())\n    num_words = len(words.split())\n    avg_syl = round(float((syllables+0.001))\/float(num_words+0.001),4)\n    num_unique_terms = len(set(words.split()))\n    \n    ###Modified FK grade, where avg words per sentence is just num words\/1\n    FKRA = round(float(0.39 * float(num_words)\/1.0) + float(11.8 * avg_syl) - 15.59,1)\n    ##Modified FRE score, where sentence fixed to 1\n    FRE = round(206.835 - 1.015*(float(num_words)\/1.0) - (84.6*float(avg_syl)),2)\n    \n    twitter_objs = count_twitter_objs(tweet)\n    retweet = 0\n    if \"rt\" in words:\n        retweet = 1\n    features = [FKRA, FRE,syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,\n                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],\n                twitter_objs[2], twitter_objs[1],\n                twitter_objs[0], retweet]\n    #features = pandas.DataFrame(features)\n    return features\n\ndef get_feature_array(tweets):\n    feats=[]\n    for t in tweets:\n        feats.append(other_features(t))\n    return np.array(feats)\n","1f1ad8e5":"other_features_names = [\"FKRA\", \"FRE\",\"num_syllables\", \"avg_syl_per_word\", \"num_chars\", \"num_chars_total\", \\\n                        \"num_terms\", \"num_words\", \"num_unique_words\", \"vader neg\",\"vader pos\",\"vader neu\", \\\n                        \"vader compound\", \"num_hashtags\", \"num_mentions\", \"num_urls\", \"is_retweet\"]","c6ecbb63":"feats = get_feature_array(tweets)","d26ad1db":"#Now join them all up\nM = np.concatenate([tfidf,pos,feats],axis=1)","d298cc8c":"M.shape","77a2ac12":"#Finally get a list of variable names\nvariables = ['']*len(vocab)\nfor k,v in vocab.items():\n    variables[v] = k\n\npos_variables = ['']*len(pos_vocab)\nfor k,v in pos_vocab.items():\n    pos_variables[v] = k\n\nfeature_names = variables+pos_variables+other_features_names\n","3f143c26":"X = pd.DataFrame(M)\ny = df1['class'].astype(int)","4e21d73a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)\n","19b93cd7":"from sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.pipeline import Pipeline","388b6724":"pipe = Pipeline(\n        [('select', SelectFromModel(LogisticRegression(class_weight='balanced',\n                                                  penalty=\"l1\", C=0.01, solver='liblinear', max_iter=10000))),\n        ('model', LogisticRegression(class_weight='balanced',penalty='l2'))])\n","3351b221":"param_grid = [{}]","c33afa8a":"grid_search = GridSearchCV(pipe, \n                           param_grid,\n                           cv=StratifiedKFold(n_splits=5, \n                            random_state=42).split(X_train, y_train), \n                           verbose=2)","0095c421":"model = grid_search.fit(X_train, y_train)","1d728bd4":"y_preds = model.predict(X_test)","1e87c503":"report = classification_report( y_test, y_preds )","4088d708":"print(report)","ea9f59c3":"from sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test,y_preds)\nmatrix_proportions = np.zeros((3,3))\nfor i in range(0,3):\n    matrix_proportions[i,:] = confusion_matrix[i,:]\/float(confusion_matrix[i,:].sum())\nnames=['Hate','Offensive','Neither']\nconfusion_df = pd.DataFrame(matrix_proportions, index=names,columns=names)\nplt.figure(figsize=(5,5))\nseaborn.heatmap(confusion_df,annot=True,annot_kws={\"size\": 12},cmap='gist_gray_r',cbar=False, square=True,fmt='.2f')\nplt.ylabel(r'True categories',fontsize=14)\nplt.xlabel(r'Predicted categories',fontsize=14)\nplt.tick_params(labelsize=12)\n\n#Uncomment line below if you want to save the output\n#plt.savefig('confusion.pdf')","60d142c2":"#True distribution\ny.hist()","25b373bf":"pd.Series(y_preds).hist()","91457d99":"### Running the model\nThe best model was selected using a GridSearch with 5-fold CV.","06de610e":"### Evaluation"}}