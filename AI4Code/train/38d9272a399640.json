{"cell_type":{"524db4d8":"code","3f9f1d31":"code","a282f873":"code","ad4d0f7d":"code","a18e2195":"code","803ae289":"code","8911ba55":"code","2b3f0c8f":"code","cdecf3ca":"code","297f520c":"code","24c5fe6b":"code","2567fcb5":"code","85eec109":"code","80e8ae7f":"code","2b3e14e9":"code","0a45007c":"code","a766905d":"code","e875a7ec":"code","2af6c5e3":"code","6d417010":"code","e026b1cc":"code","f5b2d187":"code","5a0debce":"code","54578144":"code","7f8b28e1":"code","3363c63a":"code","4777ada8":"code","1c2c6344":"code","435c793b":"code","7100fca4":"code","2c8afd37":"code","532c8489":"code","3af1c406":"code","e56dd652":"code","145a0f4d":"code","5d8136db":"code","9a6dbe9d":"code","393dfac0":"code","253c2cd9":"code","2b680f6b":"code","8eea014b":"code","b83bae2d":"code","1f8a8f68":"code","d5222a85":"code","c0d1ed83":"code","0df2135f":"code","8100f7da":"markdown","ae785a3e":"markdown","e8900ef1":"markdown","1df759dc":"markdown","37916956":"markdown","d73f2edf":"markdown","fe678136":"markdown","be75e642":"markdown","4924d57b":"markdown","8e4b63af":"markdown","5b0303dc":"markdown","e3387dfa":"markdown","5117091a":"markdown","1b7566a2":"markdown","5a519950":"markdown","d9a9e8a5":"markdown","4e1f30aa":"markdown","bdc855b9":"markdown","49b10688":"markdown","2419a48d":"markdown","bd294d43":"markdown","0dcfaa0f":"markdown","cd602b30":"markdown","45690fa7":"markdown","59b0e8c4":"markdown","c4cb58c9":"markdown","28f3d192":"markdown","0fe9a6c0":"markdown","a208f855":"markdown","f73090e5":"markdown","1695955b":"markdown","4395714d":"markdown","2dc7d32b":"markdown","6fbd1014":"markdown","ff2c813f":"markdown","f0519341":"markdown","966c2117":"markdown","52bc7135":"markdown","94fc3de3":"markdown","fe3e7d65":"markdown","df412b2a":"markdown","9864423f":"markdown","f2dd85b3":"markdown","b9af13ff":"markdown","26749040":"markdown","ef18b8f0":"markdown","243a23b7":"markdown","2c2b55ca":"markdown","b1eb4ce5":"markdown","c41a235d":"markdown","b772a021":"markdown","5095e4dd":"markdown","9500699c":"markdown","22dfadf1":"markdown","6c2cebc0":"markdown","9d710d4a":"markdown","e7613970":"markdown","3e2158ae":"markdown","902af884":"markdown","7d58992c":"markdown","b1d5847b":"markdown","58eff1c4":"markdown","89d90ed2":"markdown","31a8995a":"markdown","39103edb":"markdown","43a4ad4b":"markdown","e9dc38c8":"markdown","ba6be7c0":"markdown"},"source":{"524db4d8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom IPython.display import display","3f9f1d31":"warnings.filterwarnings('ignore') # ignore warnings.\n%config IPCompleter.greedy = True # autocomplete feature.\npd.options.display.max_rows = None # set maximum rows that can be displayed in notebook.\npd.options.display.max_columns = None # set maximum columns that can be displayed in notebook.\npd.options.display.precision = 2 # set the precision of floating point numbers.","a282f873":"# # Check the encoding of data. Use ctrl+\/ to comment\/un-comment.\n\n# import chardet\n\n# rawdata = open('candy-data.csv', 'rb').read()\n# result = chardet.detect(rawdata)\n# charenc = result['encoding']\n# print(charenc)\n# print(result) # It's utf-8 with 99% confidence.","ad4d0f7d":"df = pd.read_csv('..\/input\/candy-data.csv', encoding='utf-8')\ndf.drop_duplicates(inplace=True) # drop duplicates if any.\ndf.shape # num rows x num columns.","a18e2195":"(df.isnull().sum()\/len(df)*100).sort_values(ascending=False)","803ae289":"df.head()","8911ba55":"df['winpercent'] = df['winpercent']\/100","2b3f0c8f":"df['sugarbyprice'] = df['sugarpercent'].div(df['pricepercent']) # higher value means the candy is sweet as well as cheap.\ndf['winbyprice'] = df['winpercent'].div(df['pricepercent']) # higher value means the candy is more liked as well as cheap.","cdecf3ca":"categorival_vars = ['chocolate', 'fruity', 'caramel', 'peanutyalmondy', 'nougat', 'crispedricewafer', 'hard', 'bar',\n                    'pluribus']\nnumerical_vars = ['sugarpercent', 'pricepercent', 'winpercent', 'sugarbyprice', 'winbyprice']","297f520c":"df['competitorname'] = df['competitorname'].str.replace('\u00d5', \"'\") # Special character was appearing in name of candy.\ndf.sort_values(by=['winpercent', 'sugarpercent'], ascending=False).head(10)","24c5fe6b":"df[df['chocolate']==0].sort_values(by=['winpercent', 'sugarpercent'], ascending=False).head(10)","2567fcb5":"df.sort_values(by=['winbyprice', 'winpercent'], ascending=False).head(10)","85eec109":"df.sort_values(by=['sugarpercent', 'winpercent'], ascending=False).head(10)","80e8ae7f":"df[(df['chocolate']==1)&(df['fruity']==1)]","2b3e14e9":"plt.figure(figsize = (20,8))        \nsns.heatmap(df.corr(),annot=True, cmap = 'coolwarm')","0a45007c":"# Improting the PCA module. \n\nfrom sklearn.decomposition import PCA # import.\npca = PCA(svd_solver='randomized', random_state=123) #instantiate.\npca.fit(df.drop('competitorname', axis=1)) # fit.","a766905d":"# Making the screeplot - plotting the cumulative variance against the number of components\n\nfig = plt.figure(figsize = (20,5))\nax = plt.subplot(121)\nplt.plot(pca.explained_variance_ratio_)\nplt.xlabel('principal components')\nplt.ylabel('explained variance')\n\nax2 = plt.subplot(122)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\n\nplt.show()","e875a7ec":"# what percentage of variance in data can be explained by first 2,3 and 4 principal components respectively?\n(pca.explained_variance_ratio_[0:2].sum().round(3),\npca.explained_variance_ratio_[0:3].sum().round(3),\npca.explained_variance_ratio_[0:4].sum().round(3))","2af6c5e3":"# we'll use first 2 principal components as it retains 95% of variance.\n\ndf_pca_2_comp = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], 'Feature':df.drop(\n                              'competitorname', axis=1).columns})\n# df_pca_2_comp","6d417010":"# we can visualize what the principal components seem to capture.\n\nfig = plt.figure(figsize = (6,6))\nplt.scatter(df_pca_2_comp.PC1, df_pca_2_comp.PC2)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nfor i, txt in enumerate(df_pca_2_comp.Feature):\n    plt.annotate(txt, (df_pca_2_comp.PC1[i],df_pca_2_comp.PC2[i]))\nplt.tight_layout()\nplt.show()","e026b1cc":"df_pca = pca.transform(df.drop('competitorname', axis=1)) # our data transformed with new features as principal components.\ndf_pca = df_pca[:, 0:2] # Since we require first two principal components only.","f5b2d187":"from sklearn.preprocessing import StandardScaler\n\nstandard_scaler = StandardScaler()\ndf_s = standard_scaler.fit_transform(df_pca) # s in df_s stands for scaled.","5a0debce":"sns.pairplot(pd.DataFrame(df_s)) # Try to get some intuiton of data.","54578144":"from sklearn.neighbors import NearestNeighbors\nfrom random import sample\nfrom numpy.random import uniform\nimport numpy as np\nfrom math import isnan\n \ndef hopkins(X):\n    d = X.shape[1]\n    #d = len(vars) # columns\n    n = len(X) # rows\n    m = int(0.1 * n) \n    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n \n    rand_X = sample(range(0, n, 1), m)\n \n    ujd = []\n    wjd = []\n    for j in range(0, m):\n        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\n        ujd.append(u_dist[0][1])\n        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)\n        wjd.append(w_dist[0][1])\n \n    H = sum(ujd) \/ (sum(ujd) + sum(wjd))\n    if isnan(H):\n        print(ujd, wjd)\n        H = 0\n \n    return H","7f8b28e1":"hopkins(pd.DataFrame(df_s))","3363c63a":"from sklearn.cluster import KMeans # import.\n\n# silhouette scores to choose number of clusters.\nfrom sklearn.metrics import silhouette_score\ndef sil_score(df):\n    sse_ = []\n    for k in range(2, 15):\n        kmeans = KMeans(n_clusters=k, random_state=123).fit(df_s) # fit.\n        sse_.append([k, silhouette_score(df, kmeans.labels_)])\n    plt.plot(pd.DataFrame(sse_)[0], pd.DataFrame(sse_)[1])\n\nsil_score(df_s)","4777ada8":"# sum of squared distances.\n\ndef plot_ssd(df):\n    ssd = []\n    for num_clusters in list(range(1,19)):\n        model_clus = KMeans(n_clusters = num_clusters, max_iter=50, random_state=123)\n        model_clus.fit(df)\n        ssd.append(model_clus.inertia_)\n    plt.plot(ssd)\n\nplot_ssd(df_s)","1c2c6344":"# K-means with K=2.\nkm2c = KMeans(n_clusters=2, max_iter=50, random_state=93)\nkm2c.fit(df_s)","435c793b":"# creation of data frame with original features for analysis of clusters formed.\n\ndf_dummy = pd.DataFrame.copy(df)\ndfkm2c = pd.concat([df_dummy, pd.Series(km2c.labels_)], axis=1)\ndfkm2c.rename(columns={0:'Cluster ID'}, inplace=True)\n# dfkm2c.head()","7100fca4":"# creation of data frame with features as principal components for analysis of clusters formed.\n\ndf_dummy = pd.DataFrame.copy(pd.DataFrame(df_s))\ndfpcakm2c = pd.concat([df_dummy, pd.Series(km2c.labels_)], axis=1)\ndfpcakm2c.columns = ['PC1', 'PC2', 'Cluster ID']","2c8afd37":"sns.pairplot(data=dfpcakm2c, vars=['PC1', 'PC2'], hue='Cluster ID')","532c8489":"# K-means with K=5.\nkm5c = KMeans(n_clusters=5, max_iter=50, random_state=123)\nkm5c.fit(df_s)","3af1c406":"# creation of data frame with original features for analysis of clusters formed.\n\ndf_dummy = pd.DataFrame.copy(df)\ndfkm5c = pd.concat([df_dummy, pd.Series(km5c.labels_)], axis=1) # df-dataframe, km-kmeans, 5c-5clusters.\ndfkm5c.rename(columns={0:'Cluster ID'}, inplace=True)\n# dfkm5c.head()","e56dd652":"# creation of data frame with features as principal components for analysis of clusters formed.\n\ndf_dummy = pd.DataFrame.copy(pd.DataFrame(df_s))\ndfpcakm5c = pd.concat([df_dummy, pd.Series(km5c.labels_)], axis=1)\ndfpcakm5c.columns = ['PC1', 'PC2', 'Cluster ID']","145a0f4d":"sns.pairplot(data = dfpcakm5c, vars=['PC1', 'PC2'], hue='Cluster ID')","5d8136db":"dfkm5c.groupby('Cluster ID').mean()","9a6dbe9d":"dfkm5c[dfkm5c['Cluster ID']!=0]","393dfac0":"dfkm5c['Cluster ID'] = dfkm5c['Cluster ID'].map(lambda x: 1 if (x!=0) else 0)","253c2cd9":"dfkm5c.groupby('Cluster ID').mean()","2b680f6b":"X = df.drop(['competitorname', 'winpercent', 'sugarpercent', 'pricepercent', 'sugarbyprice', 'winbyprice'], axis=1)\ny = df['winpercent']\n\nfrom sklearn.preprocessing import MinMaxScaler\nminmax_scaler = MinMaxScaler()","8eea014b":"from sklearn import linear_model # import.\nlr_rdg = linear_model.Ridge(random_state=123) # instantiate.\n\n# Perform cross-validation.\nfrom sklearn.model_selection import GridSearchCV\nhyperparameters = {'alpha': [0.01, 0.1, 1, 10, 100, 1000]}\nmodel_cv = GridSearchCV(estimator = lr_rdg, param_grid = hyperparameters, cv=10, scoring= 'neg_mean_absolute_error')\n#lr_rdg.get_params().keys() # hyperparameters that we can set.\n\nmodel_cv.fit(X, y) # fit.","b83bae2d":"cv_results = pd.DataFrame(model_cv.cv_results_)\n# cv_results.head()\n\n# Plotting mean test and train scoes with alpha.\ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n\n# Plotting.\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\n\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","1f8a8f68":"model_cv.best_params_","d5222a85":"alpha = 1\nridge = linear_model.Ridge(alpha=alpha)\nridge.fit(X, y)","c0d1ed83":"ridge.intercept_ # constant term.","0df2135f":"for x,y in zip(X.columns, ridge.coef_): # coefficients of features.\n    print(x, y*100)","8100f7da":"<hr>","ae785a3e":"**Visualize Principal Components.**","e8900ef1":"3. Top `winbyprice` competitors.","1df759dc":"<hr>","37916956":"**Results.**","d73f2edf":"**Some Notebook Settings.**","fe678136":"**Scale Data.**","be75e642":"Maximum silhouette score at k=2.","4924d57b":"Let's put clusters other than 0 into one cluster and then analyze again.","8e4b63af":"Hopkin's Statstic will tell us if the data is clusterable or not. If it is less than 0.5, clusters are not statistically significant.","5b0303dc":"So, Cluster ID 0 contains competitors which are more chocolaty and more pricey.","e3387dfa":"## Predicting the win percentage.","5117091a":"<hr>","1b7566a2":"**Deriving new features.**","5a519950":"**Scaling.**","d9a9e8a5":"1. Top 10 winner candies.","4e1f30aa":"Tootsie Roll Midgies seems to give a bang for buck.","bdc855b9":"**Perform PCA.**","49b10688":"First 2 or 3 components are suggested by using the elbow method.","2419a48d":"1. It is to be noted that only Cluster ID 4 (Dum Dums) and 1 (Tootsie Roll Midgies) are far away from Cluster ID 0.<br>\n2. 'Dum Dums' and 'Tootsie Roll Midgies' are sort of opposite of each other. The first one is fruity and the second one chocolaty.<br>\n3. Cluster ID 0 contains competitors which are mostly chocolaty, sugary and more favourable. Cluster ID 1, although being chocolaty has a low sugar percentile.<br>\n4. All the chocolates which don't belong to Cluster ID 0 have made the top 10 list of `winbyprice`. They are all cheap.","bd294d43":"Reese's seem to be a favourite. Note that all the top competitors are chocolaty as well. Also, Reese's Miniatures is very cheap when compared to top competitors and overall as well.","0dcfaa0f":"## Analysis of Clusters.","cd602b30":"Sour Patch Kids has a high `winbyprice`. They are cheap as well as a favourite.","45690fa7":"## Clustering of Data.","59b0e8c4":"2. Competitors which are not chocolaty but winners.","c4cb58c9":"One cluster is very clearly visible. Seems to me that the second cluster will contain the data points not in the first cluster. Two clusters might suffice.","28f3d192":"We have a total of 12 variables that describe a chocolate. 9 of them are categorical and the rest, i.e. 3 are numerical variables.","0fe9a6c0":"**K-means with 5 clusters.**","a208f855":"**Is the data clusterable?**","f73090e5":"**Import Libraries.**","1695955b":"5. Which candies are both chocolaty as well as fruity?","4395714d":"**Transform Data.**","2dc7d32b":"## Data Preparation.","6fbd1014":"<hr>","ff2c813f":"<hr>","f0519341":"4. Top 10 sugary candies.","966c2117":"**K-Means with 2 clusters.**","52bc7135":"<hr>","94fc3de3":"<hr>","fe3e7d65":"We do have some correlation between features. We can use PCA for treating correlation as well as dimensionality reduction.","df412b2a":"**Check for missing values.**","9864423f":"**Some Questions one might ask.**","f2dd85b3":"Elbow seems to form at 2.","b9af13ff":"**Scree Plot.**","26749040":"**Check Encoding of Data.**","ef18b8f0":"No missing values.","243a23b7":"<hr>","2c2b55ca":"## Preliminary Steps.","b1eb4ce5":"**Clustering.**","c41a235d":"## Data Understanding.","b772a021":"**Percentage of Variance retained.**","5095e4dd":"Should we drop correlated variables before performing K-means? -> https:\/\/stats.stackexchange.com\/questions\/62253\/do-i-need-to-drop-variables-that-are-correlated-collinear-before-running-kmeans","9500699c":"scikit has 4 steps -> import, instantiate, fit, transform.","22dfadf1":"Yes, Hopkin's statistic claims that this data is indeed highly clusterable.","6c2cebc0":"**Ridge Linear Regression.**","9d710d4a":"**Cross-Validation.**","e7613970":"<hr>","3e2158ae":"<hr>","902af884":"1. chocolate: Does it contain chocolate?\n2. fruity: Is it fruit flavored?\n3. caramel: Is there caramel in the candy?\n4. peanutalmondy: Does it contain peanuts, peanut butter or almonds?\n5. nougat: Does it contain nougat?\n6. crispedricewafer: Does it contain crisped rice, wafers, or a cookie component?\n7. hard: Is it a hard candy?\n8. bar: Is it a candy bar?\n9. pluribus: Is it one of many candies in a bag or box?\n10. sugarpercent: The percentile of sugar it falls under within the data set.\n11. pricepercent: The unit price percentile compared to the rest of the set.\n12. winpercent: The overall win percentage according to 269,000 matchups.","7d58992c":"<hr>","b1d5847b":"Let's see how cluster 0 differs from the rest.","58eff1c4":"**Read Data.**","89d90ed2":"<hr>","31a8995a":"## Principal Components Analysis.","39103edb":"Except `sugarbyprice` and `winbyprice`, all the other features seem to be clustered.","43a4ad4b":"These coefficients sort of matches with the analysis done by FiveThirtyEight -> https:\/\/fivethirtyeight.com\/features\/the-ultimate-halloween-candy-power-ranking\/","e9dc38c8":"**Visualize Principal Components Loadings.**","ba6be7c0":"**Correlation Heatmap.**"}}