{"cell_type":{"57edee96":"code","69379052":"code","29fcabf4":"code","be10cd13":"code","7b838e26":"code","a425c601":"code","4da655db":"code","55bd8f07":"code","70b47a37":"code","2346d4a5":"code","126a6fff":"code","94d7f89c":"code","7da53187":"code","4da5c808":"code","87f84a90":"code","9cce4139":"code","5bd2f800":"code","a842367b":"code","73b70dc6":"code","4c3f4438":"code","e82ee78f":"code","f7b4ec98":"code","3ec31859":"code","41bf0e27":"code","50c92197":"code","b8fabb02":"code","055bad0d":"code","08ce4f84":"code","3e5d0e7c":"code","a10bd780":"code","87d340f3":"code","f3e68889":"code","7b1acd18":"code","707b7694":"code","f765e2c3":"code","31f2675c":"code","66638cbd":"code","0ab3258d":"code","5fb44c8c":"code","277ba56c":"code","c0cd3922":"code","b1b5a647":"code","6cbbf118":"code","1badf34c":"code","80323d49":"code","5e448792":"code","d8503a02":"code","66953258":"code","232afeab":"code","d7af3fd5":"code","1033d1fe":"code","6c4a81dc":"code","be8a6d9a":"code","4e002987":"code","e778d449":"code","fcd7a572":"code","cedac4b0":"code","186654ce":"code","758178ae":"code","f3845802":"code","85cee5ca":"code","8c0a9cca":"markdown","034411e4":"markdown","2dc74232":"markdown","1b2bca9d":"markdown","fb9a174d":"markdown","7edeb059":"markdown","340da7f0":"markdown","0bc1da81":"markdown","4b39d459":"markdown","185fde00":"markdown","7a37dabf":"markdown","3b4125d6":"markdown","4a3b5f32":"markdown","9cd13685":"markdown","cf35a482":"markdown","0f370b2b":"markdown","d499c437":"markdown","2fb3b211":"markdown","9d0389a2":"markdown","6fd67d30":"markdown","56d69db2":"markdown","270f941a":"markdown","d5af7a83":"markdown","c09ce3ad":"markdown","575e8358":"markdown","24acd06d":"markdown","2f0cceeb":"markdown","28a3ae20":"markdown","fa5a2846":"markdown","e7bd14f5":"markdown","d9b35cc5":"markdown","203082fb":"markdown","03e18cdf":"markdown"},"source":{"57edee96":"import os, datetime\n\nimport numpy as np \nimport pandas as pd \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\n\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","69379052":"#data = pd.read_csv('datasets\/life_expectancy.csv')\ndata = pd.read_csv('..\/input\/life-expectancy-dataset\/life_expectancy.csv')\n\ndata.sample(5)","29fcabf4":"data.shape","be10cd13":"data.info()","7b838e26":"data.isna().sum()","a425c601":"countries = data['Country'].unique()\n\nna_cols = ['Life expectancy ', 'Adult Mortality', 'Alcohol', 'Hepatitis B',\n           ' BMI ', 'Polio', 'Total expenditure','Diphtheria ', 'GDP',\n           ' thinness  1-19 years', ' thinness 5-9 years', 'Population',\n           'Income composition of resources']\n\nfor col in na_cols:\n    for country in countries:\n        data.loc[data['Country']== country, col] = data.loc[data['Country'] == country, col]\\\n                                                        .fillna(data[data['Country'] == country][col].mean())","4da655db":"data.isna().sum()","55bd8f07":"data = data.dropna() # drop all rows with missing field values or NaN values using dropna()\n\ndata.shape","70b47a37":"data['Status'].value_counts()","2346d4a5":"data['Country'].value_counts()","126a6fff":"plt.figure(figsize=(10, 8))\n\ndata.boxplot('Life expectancy ')\n\nplt.show()","94d7f89c":"plt.figure(figsize=(10, 8))\n\ndata['Life expectancy '].plot.kde()\n\nplt.show()","7da53187":"plt.figure(figsize=(8, 6))\n\nsns.boxplot('Status', 'Life expectancy ', data = data)\n\nplt.xlabel('Status', fontsize = 16)\nplt.ylabel('Total expenditure', fontsize = 16)\n\nplt.show()","4da5c808":"plt.figure(figsize=(8, 6))\n\nsns.boxplot('Status', 'Total expenditure', data = data)\n\nplt.xlabel('Status', fontsize = 16)\nplt.ylabel('Total expenditure', fontsize = 16)\n\nplt.show()","87f84a90":"data_corr = data[['Life expectancy ', \n                  'Adult Mortality', \n                  'Schooling', \n                  'Total expenditure', \n                  'Diphtheria ', \n                  'GDP',\n                  'Population']].corr()\n\ndata_corr","9cce4139":"fig, ax = plt.subplots(figsize=(12, 8))\n\nsns.heatmap(data_corr, annot=True)\n\nplt.show()","5bd2f800":"features = data.drop('Life expectancy ', axis=1)\n\ntarget = data[['Life expectancy ']]","a842367b":"features.columns","73b70dc6":"target.sample(5)","4c3f4438":"features = features.drop('Country', axis=1)\n\nfeatures.columns","e82ee78f":"categorical_features = features['Status'].copy()\n\ncategorical_features.head()","f7b4ec98":"categorical_features = pd.get_dummies(categorical_features)\n\ncategorical_features.head()","3ec31859":"numeric_features = features.drop(['Status'], axis=1)\n\nnumeric_features.head()","41bf0e27":"numeric_features.describe().T","50c92197":"# we use the standard scale of fit_transform to standardize our numeric_features\nstandardScaler = StandardScaler()\n\nnumeric_features = pd.DataFrame(standardScaler.fit_transform(numeric_features), \n                                columns=numeric_features.columns,\n                                index=numeric_features.index)\n\nnumeric_features.describe().T","b8fabb02":"processed_features = pd.concat([numeric_features, categorical_features], axis=1,\n                               sort=False)\n\nprocessed_features.head()","055bad0d":"processed_features.shape","08ce4f84":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(processed_features, \n                                                    target, \n                                                    test_size = 0.2, \n                                                    random_state=1)","3e5d0e7c":"(x_train.shape, x_test.shape), (y_train.shape, y_test.shape)","a10bd780":"def build_single_layer_model():\n    \n    # instantiate the sequential model, tf.keras.Sequential\n    model = tf.keras.Sequential()\n\n    # add a initial dense layer with 32 neurons to the model, using model.add \n    # input_shape = shape of the input data refers to the number of features that we feed in during training\n    # to access the shape of the input data, we access the 2nd element of the shape array using x_train.shape[1]\n    # activation function for this dense layer I've specified as the sigmoid activation function\n    model.add(tf.keras.layers.Dense(32, \n                                    input_shape = (x_train.shape[1],), \n                                    activation = 'sigmoid'))\n\n    # This is our output or prediction layer. \n    # It has exactly one neuron because we have a single predicted output value \n    # in our regression model, the life expectancy\n    model.add(tf.keras.layers.Dense(1))\n    \n    # The optimizer that we have chosen here is the Adam optimizer (adaptive moment estimation) \n    # with a learning_rate of 0.01.\n    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n    \n    # we invoke the model.compile method to configure the parameters for our model\n    # mean squared error = loss function\n    # we track the mean absolute error, mean squared error metrics (mae and mse)\n    model.compile(loss = 'mse', \n                  metrics = ['mae', 'mse'], \n                  optimizer = optimizer)\n\n    return model","87d340f3":"model = build_single_layer_model()\n\nmodel.summary()","f3e68889":"tf.keras.utils.plot_model(model)","7b1acd18":"x_train.shape","707b7694":"num_epochs = 100\n\ntraining_history = model.fit(x_train, \n                             y_train,\n                             epochs = num_epochs, \n                             validation_split = 0.2, \n                             verbose = True)","f765e2c3":"plt.figure(figsize=(16, 8))\n\nplt.subplot(1, 2, 1)\n\nplt.plot(training_history.history['mae'])\nplt.plot(training_history.history['val_mae'])\n\nplt.title('Model MAE')\nplt.ylabel('mae')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'])\n\nplt.subplot(1, 2, 2)\n\nplt.plot(training_history.history['loss'])\nplt.plot(training_history.history['val_loss'])\n\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'])","31f2675c":"model.evaluate(x_test, y_test)","66638cbd":"y_pred = model.predict(x_test)\n\nr2_score(y_test, y_pred)","0ab3258d":"pred_results = pd.DataFrame({'y_test': y_test.values.flatten(),\n                             'y_pred': y_pred.flatten()}, index = range(len(y_pred)))\n\npred_results.sample(10)","5fb44c8c":"plt.figure(figsize=(10, 8))\n\nplt.scatter(y_test, y_pred, s=100, c='blue')\n\nplt.xlabel('Actual life expectancy values')\nplt.ylabel('Predicted life expectancy values')\nplt.show()","277ba56c":"# Load the extension and start TensorBoard\n\n%load_ext tensorboard\n%tensorboard --logdir logs","c0cd3922":"def build_multiple_layer_model():\n    \n    model = keras.Sequential([layers.Dense(32, input_shape = (x_train.shape[1],), activation = 'relu'),\n                              layers.Dense(16, activation = 'relu'),\n                              layers.Dense(4, activation = 'relu'),\n                              layers.Dense(1)])\n    \n    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n        \n    model.compile(loss = 'mse', metrics = ['mae', 'mse'], optimizer = optimizer)\n    \n    return model","b1b5a647":"model = build_multiple_layer_model()\n\ntf.keras.utils.plot_model(model, show_shapes=True)","6cbbf118":"logdir = os.path.join(\"seq_logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n \ntensorboard_callback = keras.callbacks.TensorBoard(logdir, histogram_freq = 1)","1badf34c":"training_history = model.fit(x_train, \n                             y_train, \n                             validation_split = 0.2, \n                             epochs = 500,\n                             batch_size = 100,\n                             callbacks = [tensorboard_callback])","80323d49":"plt.figure(figsize=(16, 8))\n\nplt.subplot(1, 2, 1)\n\nplt.plot(training_history.history['mae'])\nplt.plot(training_history.history['val_mae'])\n\nplt.title('Model MAE')\nplt.ylabel('mae')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'])\n\nplt.subplot(1, 2, 2)\n\nplt.plot(training_history.history['loss'])\nplt.plot(training_history.history['val_loss'])\n\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'])","5e448792":"# Load the extension and start TensorBoard\n\n%load_ext tensorboard","d8503a02":"%tensorboard --logdir seq_logs --port 6500 ","66953258":"model.evaluate(x_test, y_test)","232afeab":"y_pred = model.predict(x_test)\n\nr2_score(y_test, y_pred)","d7af3fd5":"def build_model_with_sgd():\n    \n    model = keras.Sequential([layers.Dense(32, input_shape = (x_train.shape[1],), activation = 'relu'),\n                              layers.Dense(16, activation = 'relu'),\n                              layers.Dense(4, activation = 'relu'),\n                              layers.Dense(1)])\n    \n    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.001)\n        \n    model.compile(loss = 'mse', metrics = ['mae', 'mse'], optimizer = optimizer)\n    \n    return model","1033d1fe":"model_sgd = build_model_with_sgd()\n\ntf.keras.utils.plot_model(model_sgd, show_shapes=True)","6c4a81dc":"training_history = model_sgd.fit(x_train, \n                                 y_train, \n                                 validation_split = 0.2, \n                                 epochs = 100,\n                                 batch_size = 100)","be8a6d9a":"plt.figure(figsize=(16, 8))\n\nplt.subplot(1, 2, 1)\n\nplt.plot(training_history.history['mae'])\nplt.plot(training_history.history['val_mae'])\n\nplt.title('Model MAE')\nplt.ylabel('mae')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'])\n\nplt.subplot(1, 2, 2)\n\nplt.plot(training_history.history['loss'])\nplt.plot(training_history.history['val_loss'])\n\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'])","4e002987":"model_sgd.evaluate(x_test, y_test)","e778d449":"y_pred = model_sgd.predict(x_test)\n\nr2_score(y_test, y_pred)","fcd7a572":"def build_model_with_rmsprop():\n    \n    model = keras.Sequential([layers.Dense(16, input_shape = (x_train.shape[1],), activation = 'elu'),\n                              layers.Dense(8, activation = 'elu'),\n                              layers.Dense(4, activation = 'elu'),\n                              layers.Dense(1)])\n    \n    optimizer = tf.keras.optimizers.RMSprop(learning_rate = 0.001)\n        \n    model.compile(loss = 'mse', metrics = ['mae', 'mse'], optimizer = optimizer)\n    \n    return model","cedac4b0":"model_rmsprop = build_model_with_rmsprop()","186654ce":"training_history = model_rmsprop.fit(x_train, \n                                 y_train, \n                                 validation_split = 0.2, \n                                 epochs = 100, \n                                 batch_size=100)","758178ae":"plt.figure(figsize=(16, 8))\n\nplt.subplot(1, 2, 1)\n\nplt.plot(training_history.history['mae'])\nplt.plot(training_history.history['val_mae'])\n\nplt.title('Model MAE')\nplt.ylabel('mae')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'])\n\nplt.subplot(1, 2, 2)\n\nplt.plot(training_history.history['loss'])\nplt.plot(training_history.history['val_loss'])\n\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'])","f3845802":"model_rmsprop.evaluate(x_test, y_test)","85cee5ca":"y_pred = model_rmsprop.predict(x_test)\n\nr2_score(y_test, y_pred)","8c0a9cca":"\n\n#### Evaluating the model","034411e4":"#### filling nan with country's mean","2dc74232":"### Schooling and Life Expectancy correlation?\n\nSchooling has a positive correlation coefficient of 0.75. Greater the number of years of schooling in the country, greater the life expectancy of that country","1b2bca9d":"### Correlation between dataset features\n\nHow are each feature correlated with other features? In other words how are various columns in our dataset correlated with one another. Let's use correlation coefficient to measure this. Correlation coefficient is a measure of the linear relationship that exists between variables and is a value between \u20111 and 1","fb9a174d":"#### Evaluating the model","7edeb059":"### 2nd model","340da7f0":"### Developed vs. Developing Countries and corresponding life expectancy\n\nWe're interested in how the life expectancy varies depending on country status","0bc1da81":"Code below requires pydot and graphviz to generate model install - https:\/\/graphviz.gitlab.io\/download\/","4b39d459":"### Training the model","185fde00":"Now all of our numeric values have a mean value very close to 0 and our standard deviation (std column) is very close to 1","7a37dabf":"### Using RMSprop as loss function","3b4125d6":"Go to port localhost:6500 and show the graph\n\n- Explore the scalars tab, show only train and validation data (using checkbox, I think in the recording you do this at the very end, just do it together with the other stuff at the beginning)\n- You can show just one of the scalars, don't need to show all\n- Explore Graphs, this part was proper\n- Explore distributions, can only show one of the scalars, don't need to show all\n- Explore histograms, can only show one of the kernels\/biases don't need to show all","4a3b5f32":"#### Evaluating the model","9cd13685":"### 1st model","cf35a482":"Model with multiple layers and relu activation","0f370b2b":"#### Evaluating the model","d499c437":"We see that 'Adult mortality' and 'Schooling' is highly corelated with 'Life expectancy'","2fb3b211":"### Using Feature Standardization in sklearn\n\nWhen we look at the mean and standard deviation (std columns) above, you'll see that all features have very different values for mean and standard deviation. \n\nMachine learning models, especially neural network models, tend to be far more robust, when they are trained using numeric features that have the same scale. \n\nTo achieve this is by processing numeric_features using  standardization. \n\nStandardization is a column\u2011wise operation where for every value in a column, you subtract the mean of that column from each value and divide by the standard deviation. \n\nThis expresses all of our data in terms of standard deviations from the mean or z\u2011scores.","9d0389a2":"Model with one layer and sigmoid activation \n\nWe've explored and processed our data, we are ready to build our regression model using the sequential API available in Keras","6fd67d30":"### Data Visualization","56d69db2":"### Importing Libraries","270f941a":"The country name doesn't matter since the rest of the values are related to certain country. ","d5af7a83":"Using .add() method","c09ce3ad":"### Using SGD (stochastic gradient descent) as loss function","575e8358":"### Developed vs. Developing Countries and corresponding life expectancy\n\nFrom our boxplots, clearly developed countries have a much higher life expectancy as compared with developing countries","24acd06d":"There are many null values. Since the life expectancy depends on the status of country and particular country's detail, I thought, it made sense to fill the nan values with mean value with respect to country. Taking simple mean or interpolating didn't make much sense.","2f0cceeb":"### Processing the null values","28a3ae20":"Dataset: https:\/\/www.kaggle.com\/kumarajarshi\/life-expectancy-who","fa5a2846":"#### For using tensorboard","e7bd14f5":"### Splitting into test and train","d9b35cc5":"### Understanding the dataset","203082fb":"Columns:\n    - Country \n    - Year\n    - Status: Developed or Developing status\n    - Life expectancy: Life Expectancy in age \n    - Adult Mortality: Adult Mortality Rates of both sexes (probability of dying between 15 and 60 years per 1000 population) \n    - infant deaths: Number of Infant Deaths per 1000 population\n    - Alcohol: Alcohol, recorded per capita (15+) consumption (in litres of pure alcohol)\n    - percentage expenditure: Expenditure on health as a percentage of Gross Domestic Product per capita(%)\n    - Hepatitis B: Hepatitis B (HepB) immunization coverage among 1-year-olds (%)\n    - Measles: Measles number of reported cases per 1000 population\n    - BMI: Average Body Mass Index of entire population\n    - under-five deaths: Number of under-five deaths per 1000 population\n    - Polio: Polio (Pol3) immunization coverage among 1-year-olds (%)\n    - Total expenditure: General government expenditure on health as a percentage of total government expenditure (%)\n    - Diphtheria: Diphtheria tetanus toxoid and pertussis (DTP3) immunization coverage among 1-year-olds (%)\n    - HIV\/AIDS: Deaths per 1 000 live births HIV\/AIDS (0-4 years)\n    - GDP: Gross Domestic Product per capita (in USD)\n    - Population: Population of the country\n    - thinness 1-19 years: Prevalence of thinness among children and adolescents for Age 10 to 19 (%)\n    - thinness 5-9 years: Prevalence of thinness among children for Age 5 to 9(%)\n    - Income composition of resources: Human Development Index in terms of income composition of resources (index ranging from 0 to 1)\n    - Schooling: Number of years of Schooling(years)\n   ","03e18cdf":"### Splitting the data"}}