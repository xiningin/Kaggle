{"cell_type":{"fc9525e3":"code","8c265a61":"code","44783253":"code","8828df65":"code","d644a2fa":"code","22a94cdd":"code","6c0f7e22":"code","396d3564":"code","fa5f2388":"code","5c657e39":"code","02799c17":"code","294fc26d":"code","90c9ba86":"code","27685c60":"code","38a2f861":"code","e635aac6":"code","d0762b65":"code","323ee976":"code","fd612323":"code","25c4437d":"code","838931e9":"code","8c391d3a":"code","56fec173":"code","948c47b5":"code","8e4e57c3":"code","ef7c8435":"code","97cbe807":"code","84141346":"code","5fab7889":"code","77b8788f":"code","cbdd9e77":"code","be7824eb":"markdown","ce1ba4d8":"markdown","4775c1e4":"markdown","2c558dc3":"markdown","f05c479f":"markdown","c9c39016":"markdown","7993e4e9":"markdown","59d2b692":"markdown","fc8ee5be":"markdown","f503753e":"markdown","e32557ea":"markdown","acb21795":"markdown","363712a9":"markdown","6c9f1b7e":"markdown","96462dbc":"markdown","f3d83ee1":"markdown","7f693859":"markdown","6ede2309":"markdown"},"source":{"fc9525e3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nfrom scipy import stats\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nimport seaborn as sns\n\n\nimport ipywidgets as widgets\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\n\nfrom datetime import date\nimport holidays\nimport calendar\nimport dateutil.easter as easter\n\nfrom collections import defaultdict\nle = defaultdict(LabelEncoder)\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True, figsize=(12, 8))\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=16,\n    titlepad=10,\n)\nplot_params = dict(\n    color=\"0.75\",\n    style=\".-\",\n    markeredgecolor=\"0.25\",\n    markerfacecolor=\"0.25\",\n    legend=False,\n)\n%config InlineBackend.figure_format = 'retina'\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport gc\nimport os\nimport math\nimport random\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8c265a61":"# Admin\nID = \"row_id\"            # Id id x X index\nINPUT = \"..\/input\/tabular-playground-series-jan-2022\"\n\n# time series data common new feature\nDATE = \"date\"\nYEAR = \"year\"\nQUARTER = \"quarter\"\nMONTH = \"month\"\nWEEK = \"week\"\nDAY = \"day\"\nDAYOFYEAR = \"dayofyear\"\nWEEKOFYEAR = \"weekofyear\"\nDAYOFMONTH = \"dayofMonth\"\nDAYOFWEEK = \"dayofweek\"\nWEEKDAY = \"weekday\"","44783253":"from pathlib import Path\n\n\ndef load_data():\n    # Read data\n    data_dir = Path(INPUT)\n    df_train = pd.read_csv(data_dir \/ \"train.csv\", parse_dates=[DATE],\n                    usecols=['date', 'country', 'store', 'product', 'num_sold'],\n                    dtype={\n                        'country': 'category',\n                        'store': 'category',\n                        'product': 'category',\n                        'num_sold': 'float64',\n                    },\n                    infer_datetime_format=True,)\n    df_test = pd.read_csv(data_dir \/ \"test.csv\", index_col=ID, parse_dates=[DATE])\n    column_y = df_train.columns.difference(\n        df_test.columns)[0]  # column_y target_col label_col\n    df_train[DATE] = pd.to_datetime(df_train[DATE])\n    df_test[DATE] = pd.to_datetime(df_test[DATE])\n    return df_train, df_test, column_y\n","8828df65":"def process_data(df_train, df_test):\n    # Preprocessing\n    if FEATURE_ENGINEERING:\n        df_train = feature_engineer(df_train)\n        df_test = feature_engineer(df_test)\n\n    return df_train, df_test","d644a2fa":"train_df, test_df, column_y = load_data()","22a94cdd":"train_df","6c0f7e22":"df = train_df.groupby(['country', 'store', 'product', 'date']).mean().unstack(['country', 'store', 'product']) #.loc['2015']","396d3564":"train_subset = train_df[(train_df.country == 'Norway') & (train_df.store == 'KaggleMart') & (train_df['product'] == 'Kaggle Hat')].copy()","fa5f2388":"train_subset","5c657e39":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nax.plot(train_subset[column_y])\nax.set_xlabel('Time')\nax.set_ylabel('Kaggle Sales')\nfig.autofmt_xdate()\nplt.tight_layout()","02799c17":"train_subset.index = pd.DatetimeIndex(train_subset.date).to_period('D')","294fc26d":"from statsmodels.tsa.stattools import adfuller\nADF_result = adfuller(train_df[column_y])\nprint(f'ADF Statistic: {ADF_result[0]}')\nprint(f'p-value: {ADF_result[1]}')","90c9ba86":"ADF_result = adfuller(train_subset[column_y])\nprint(f'ADF Statistic: {ADF_result[0]}')\nprint(f'p-value: {ADF_result[1]}')","27685c60":"kaggle_2015_sales_diff = np.diff(train_subset[column_y], n=1)","38a2f861":"fig, ax = plt.subplots()\nax.plot(kaggle_2015_sales_diff)\nax.set_xlabel('Differenced volume of Kaggle sales. Notice how the trend component was stabilized since values')\nax.set_ylabel('Kaggle Sales')\nfig.autofmt_xdate()\nplt.tight_layout()","e635aac6":"ADF_result = adfuller(kaggle_2015_sales_diff)\nprint(f'ADF Statistic: {ADF_result[0]}')\nprint(f'p-value: {ADF_result[1]}')","d0762b65":"kaggle_2015_sales_diff2 = np.diff(kaggle_2015_sales_diff, n=1)\nADF_result = adfuller(kaggle_2015_sales_diff2)\nprint(f'ADF Statistic: {ADF_result[0]}')\nprint(f'p-value: {ADF_result[1]}')","323ee976":"from statsmodels.graphics.tsaplots import plot_acf\nplot_acf(kaggle_2015_sales_diff, lags=31);\nplt.tight_layout()","fd612323":"df_diff = pd.DataFrame({'kaggle_2015_sales_diff': kaggle_2015_sales_diff})\ntrain = df_diff[:int(0.9*len(df_diff))]\ntest = df_diff[int(0.9*len(df_diff)):]\nprint(len(train))\nprint(len(test))","25c4437d":"from statsmodels.graphics.tsaplots import plot_pacf\nplot_pacf(train_subset[column_y], lags=20) #train_subset[column_y] kaggle_2015_sales_diff\nplt.tight_layout()","838931e9":"from statsmodels.tsa.arima_process import ArmaProcess\nimport numpy as np\nnp.random.seed(42)\nar1 = np.array([1, -0.33])\nma1 = np.array([1, 0.9])\nARMA_1_1 = ArmaProcess(ar1, ma1).generate_sample(nsample=1000)","8c391d3a":"from itertools import product\nps = range(0, 8, 1)\nqs = range(0, 8, 1)\norder_list = list(product(ps, qs))","56fec173":"from typing import Union\nfrom tqdm.notebook import tqdm\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\ndef optimize_ARMA(endog: Union[pd.Series, list], order_list: list):\n    results = []\n    for order in tqdm(order_list):\n        try:\n            model = SARIMAX(endog, order=(order[0], 0, order[1]),simple_differencing=False).fit(disp=False)\n        except:\n            continue\n        aic = model.aic\n        results.append([order, aic])\n    result_df = pd.DataFrame(results)\n    result_df.columns = ['(p,q)', 'AIC']\n    #Sort in ascending order, lower AIC is better\n    result_df = result_df.sort_values(by='AIC',\n        ascending=True).reset_index(drop=True)\n    return result_df","948c47b5":"result_df = optimize_ARMA(train_subset[column_y], order_list) #A\nresult_df","8e4e57c3":"train_set = train_df[(train_df.country == 'Norway') & (train_df.store == 'KaggleMart') & (train_df['product'] == 'Kaggle Hat')].copy()","ef7c8435":"from statsmodels.tsa.seasonal import STL\ndecomposition = STL(train_set[column_y], period=12).fit()\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(nrows=4, ncols=1, sharex=True,\nfigsize=(10,8))\nax1.plot(decomposition.observed)\nax1.set_ylabel('Observed')\nax2.plot(decomposition.trend)\nax2.set_ylabel('Trend')\nax3.plot(decomposition.seasonal)\nax3.set_ylabel('Seasonal')\nax4.plot(decomposition.resid)\nax4.set_ylabel('Residuals')\nfig.autofmt_xdate()\nplt.tight_layout()","97cbe807":"ps = range(0, 13, 1)\nqs = range(0, 13, 1)\nPs = [0]\nQs = [0]\nd = 2\nD = 0\ns = 12\nARIMA_order_list = list(product(ps, qs, Ps, Qs))","84141346":"from typing import Union\nfrom tqdm.notebook import tqdm\n\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\ndef optimize_SARIMA(endog: Union[pd.Series, list], order_list: list, d: int, D: int, s: int) -> pd.DataFrame:\n    results = []\n    for order in tqdm(order_list):\n        try:\n            model = SARIMAX(\n                endog,\n                order=(order[0], d, order[1]),\n                seasonal_order=(order[2], D, order[3], s),\n                simple_differencing=False).fit(disp=False)\n        except:\n            continue\n        aic = model.aic\n        results.append([order, model.aic])\n        \n    result_df = pd.DataFrame(results)\n    result_df.columns = ['(p,q,P,Q)', 'AIC']\n    #Sort in ascending order, lower AIC is better\n    result_df = result_df.sort_values(by='AIC',\n        ascending=True).reset_index(drop=True)\n    return result_df ","5fab7889":"train = train_subset[column_y]\nARIMA_result_df = optimize_SARIMA(train, ARIMA_order_list, d, D, s) #B\nARIMA_result_df","77b8788f":"ARIMA_model = SARIMAX(train, order=(9,11,0), simple_differencing=False)\nARIMA_model_fit = ARIMA_model.fit(disp=False)\nARIMA_model_fit.plot_diagnostics(figsize=(12,10))","cbdd9e77":"from statsmodels.stats.diagnostic import acorr_ljungbox\nresiduals = ARIMA_model_fit.resid\nlbvalue, pvalue = acorr_ljungbox(residuals, np.arange(1, 11, 1))\nprint(pvalue)","be7824eb":"With a transformation applied to our series, we can test for stationarity again using the ADF\ntest. This time, make sure to run the test on the differenced data stored in the\n*kaggle_2015_sales_diff* variable.","ce1ba4d8":"This gives an ADF statistic of -10.5 and a p-value of 9.7 \u00d7 10 \u221219 . Therefore, with a large\nnegative ADF statistic and a p-value much smaller than 0.05, we can say that our series is\nstationary.\nOur next step is to plot the autocorrelation function. The statsmodels library\nconveniently includes the plot_acf function for us. We simply pass in our differenced series\nand specify the number of lags in the lags parameter. Remember that the number of lags\ndetermines the range of values on the x-axis.","4775c1e4":"Since we have significant autocorrelation coefficients inteval of 7, assumming that we have\na stationary moving average process of order 7. Therefore, we can use a 7-order\nmoving average model, or MA(7) model, to forecast our stationary time series.","2c558dc3":"Let\u2019s plot the PACF and see if the coefficients become abruptly non-significant after lag 2. If that is the case, then we know that we can use the PACF plot to determine the order of a stationary autoregressive process, just like we can use the ACF plot to determine the order of a stationary moving average process.","f05c479f":"The next step is learning how to treat time series where you cannot infer an order from the ACF plot nor from the PACF plot. This means that both figures exhibit a slowly decaying pattern or a sinusoidal pattern. In such case, we are in the presence of an autoregressive moving average process or ARMA. This denotes the combination of both the autoregressive and moving average processes.\n\nWe will examine the autoregressive moving average process or ARMA(p,q), where p denotes the order of the autoregressive portion, and q denotes the order of the moving average portion. Furthermore, we will not be able to use the ACF and PACF plots to determine the orders q and p respectively, as both plots will show either a slowly decaying or sinusoidal pattern. Thus, we will define a general modeling procedure that will\nallow us to model such complex time series. This procedure involves model selection using the Akaike\u2019s Information Criterion or AIC, which will determine the optimal combination of p and q for our series. Then, we must evaluate the models\u2019 validity using residual analysis, by studying the correlogram, Q-Q plot and density plot of the model\u2019s residuals in order to assess if they closely resemble white noise. If that is the case, we can move on to forecasting our time series using the ARMA(p,q) model.","c9c39016":"With time series decomposition, we can clearly identify and visualize the seasonal component of a time series. Here, we decompose the dataset for air passengers using the STL function from the statsmodels library.","7993e4e9":"This results in an ADF statistic of -2.5 and a p-value of 0.11. Here, the ADF statistic is not a\nlarge negative number and the p-value is greater than 0.05. Therefore, our time series is not\nstationary and we must apply transformations to make it stationary.\nIn order to make our series stationary, we will try to stabilize the trend by applying a\nfirst-order differencing. We can do so by using the diff method from the numpy library.\nRemember that this method takes in a parameter n that specifies the order of differencing.\nIn this case, because it is a first-order differencing, n will be equal to 1.","59d2b692":"## Partial autocorrelation\nPartial autocorrelation measures the correlation between lagged values in a time series when we remove the influence of other correlated lagged values. We can plot the partial autocorrelation function to determine the order of a stationary AR(p) process. The coefficients will be non-significant after lag *p*.","fc8ee5be":"# Seasonal","f503753e":"# Autoregressive process\nAn autoregressive process is a regression of a variable against itself. In time series, this means that the present value is linearly dependent on its past values.\n\nThe autoregressive process is denoted as AR(p) where p is the order. The general expression of an AR(p) model is:\ny t = C + \u03d5 1 y t \u22121 + \u03f5 t + \u03d5 2 y t \u22122 + \u03f5 t +\u22c5\u22c5\u22c5 + \u03d5 p y t \u2212p + \u03f5","e32557ea":"# Load Data\nThe first step is to gather the data. Then, we test for stationarity. In the event\nwhere our series is not stationary, we apply transformations, such as differencing, until the\nseries is stationary. Then, we plot the ACF and look for significant autocorrelation\ncoefficients. In the case of a random walk, we will not see significant coefficients after lag 0.\nOn the other hand, if we see significant coefficients, then we must check if they become\nabruptly non-significant after some lag q . If that is the case, then we know that we have a\nmoving average process of order q . Otherwise, we must follow a different set of steps to\ndiscover the underlying process of our time series.","acb21795":"# Chapter 2\n\nPreviously, we covered the moving average process, also denoted as MA(q), where q is the order. We learned that in a moving average process, the present value is linearly dependent on current and past error terms. Therefore, if we predict more than q steps ahead, the prediction will fall flat and will return only the mean of the series because the error terms are not observed in the data and must be recursively estimated. Finally, we saw that we can determine the order of a stationary MA(q) process by studying the ACF plot;\nthe autocorrelation coefficients will be significant up until lag q. In the case where the autocorrelation coefficients slowly decay or exhibit a sinusoidal pattern, then we are possibly in the presence of an autoregressive process.\n\nHere, we will first define the autoregressive process. Then, we will define the partial autocorrelation function and use it to find the order of the underlying autoregressive process of our dataset. Finally, we will use the AR(p) model to produce forecasts.","363712a9":"# EDA Times series\n\nReading Times Series Forecasting in Python by @Marco Peixeiro","6c9f1b7e":"- There are significant autocorrelation coefficients after lag 0. Therefore, it is not a random walk.\n- The coefficients do not become abruptly non-significant after a certain lag, which means that it is not a purely moving average process either.\n- We notice that there are significant coefficients after lag 0. In fact, they are significant up until lag 30.\n- Notice the coefficients 7, 14, 21, 28(weekly, weekend). \n- Negative coefficients on weekdays Monday-Thursday.\n- Non-significant coefficients remain in the shaded area of the plot.","96462dbc":"# Opps, this is for year 2019 365 days one-shot.\nI stop here for now. :S","f3d83ee1":"The next step is to test for stationarity. We intuitively know that the series is not stationary\nsince there is an observable trend till 2018. Still, we will use the ADF test to make\nsure. Again, we use the adfuller function from the statsmodels library and extract the ADF\nstatistic and p-value. If the ADF statistic is a large negative number and the p-value is\nsmaller than 0.05, then our series is stationary. Otherwise, we must apply transformations.","7f693859":"For the forecast horizon the moving average model bring in a particularity. The MA(q) model\ndoes not allow us to forecast 50 steps into future in one shot. Remember that the moving\naverage model is linearly dependent on past error terms. Those terms are not observed in\nthe dataset and must therefore be recursively estimated. This means that for an MA(q)\nmodel, we can only forecast q steps into the future. Any prediction made beyond that point\nwill not have past error terms and the model will only predict the mean. Therefore, there is\nno added value in forecasting beyond q steps into the future because the predictions will fall\nflat, as only the mean is returned, which is equivalent to a baseline model.","6ede2309":"## Plot of subset year 2015-2018 Norway KaggleMart Kaggle Hat"}}