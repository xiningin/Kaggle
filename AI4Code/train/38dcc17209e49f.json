{"cell_type":{"f3e2dc12":"code","6e42e95b":"code","eb3d1cf6":"code","dc421cbc":"code","b59ace49":"code","8bc771f4":"code","2b5288a3":"code","6e4959df":"code","ae22346d":"code","15588869":"code","d8058665":"code","f07dd9e5":"code","a4982b4f":"code","9be64840":"code","ce87ae59":"code","9584785d":"code","6a4467d7":"code","d7f4ed80":"code","794edbe8":"code","2c5dbab5":"code","6b3ce88b":"code","bb2e0135":"code","a5c50228":"code","201525d9":"code","b4ccdc0d":"code","04c691d6":"code","b2dc62ee":"code","f8c13f07":"code","7400d127":"code","17da3a6a":"code","83b63430":"code","acbc4abd":"code","2981fbfc":"code","1df8e79f":"markdown","79d220a8":"markdown","7d9e9dad":"markdown","755cb1c4":"markdown","7cec1b0b":"markdown","ccfa6db9":"markdown","ad4b15f8":"markdown","3e181127":"markdown","83c32cf4":"markdown","a0898dd5":"markdown","3a362a22":"markdown","c8508ff3":"markdown","4eab0826":"markdown","006e1eea":"markdown","3969ce35":"markdown","f1c0b86b":"markdown","423bc9b1":"markdown","9026b111":"markdown","13f52596":"markdown","3f5f0e14":"markdown","daa40c44":"markdown","c97c6403":"markdown","747d3360":"markdown","28b091ef":"markdown","b0124024":"markdown"},"source":{"f3e2dc12":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","6e42e95b":"# Reading the main data file \"application_train\/test.csv\" for baseline model\ntrain_data = pd.read_csv(\"..\/input\/application_train.csv\")\nprint('training data shape: ', train_data.shape)\n\ntest_data = pd.read_csv(\"..\/input\/application_test.csv\")\nprint('testing data shape: ', test_data.shape)","eb3d1cf6":"# Looking at different features in training data\ntrain_data.head()","dc421cbc":"# Looking at overall statistics of training data\ntrain_data.describe()","b59ace49":"#Lets first drop the target column from the train data\ntarget = train_data['TARGET']\n#train_data = train_data.drop(columns = ['TARGET'])\n#print('training data shape: ', train_data.shape)","8bc771f4":"# Lets look the the target distribution in the data set\ntarget.value_counts()","2b5288a3":"# Now plot the histogram to visualize it further\ntarget.plot.hist()","6e4959df":"train_data.dtypes.value_counts()","ae22346d":"# Utility function to change Object type training data to categorical data\ndef one_hot_encoding(train_data, test_data):\n    \"\"\"\n    examine columns with object type in training and test data\n    do one hot encoding of such columns\n    \"\"\"\n\n    encoded_train_data = pd.get_dummies(train_data)\n    encoded_test_data = pd.get_dummies(test_data)\n    return encoded_train_data, encoded_test_data","15588869":"encoded_train_data, encoded_test_data = one_hot_encoding(train_data, test_data)\nprint('Training Features shape after one hot encoding: ', encoded_train_data.shape)\nprint('Testing Features shape after one hot encoding: ', encoded_test_data.shape) ","d8058665":"align_train_data, align_test_data = encoded_train_data.align(encoded_test_data, join = 'inner', axis = 1)\nalign_train_data['TARGET'] = target\nprint('Training Features shape: ', align_train_data.shape)\nprint('Testing Features shape: ', align_test_data.shape)","f07dd9e5":"missing_val_count_by_column = align_train_data.isnull().sum()\nmissing_val_count_by_column = missing_val_count_by_column[missing_val_count_by_column > 0]\nprint('Number of columns with missing values: ', missing_val_count_by_column.shape[0])\nmissing_val_count_by_column.head()","a4982b4f":"align_train_data.describe()","9be64840":"align_test_data.describe()","ce87ae59":"col_with_mean = align_train_data.mean(axis = 0)\ncol_with_negative_mean = col_with_mean[col_with_mean < 0]\ncol_with_negative_mean","9584785d":"col_name_with_negative_mean = ['DAYS_BIRTH','DAYS_REGISTRATION','DAYS_ID_PUBLISH','DAYS_LAST_PHONE_CHANGE']\nalign_train_data[col_name_with_negative_mean] = align_train_data[col_name_with_negative_mean]\/-365\nalign_test_data[col_name_with_negative_mean] = align_test_data[col_name_with_negative_mean]\/-365\nalign_train_data.describe()","6a4467d7":"import matplotlib.pyplot as plt\nalign_train_data['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","d7f4ed80":"col_with_anom = align_train_data['DAYS_EMPLOYED'] >= 36500\nprint('Number of anomalies : ', col_with_anom.sum())\nalign_train_data['DAYS_EMPLOYED'][col_with_anom].value_counts()","794edbe8":"# Replace the anomalous values with nan\nalign_train_data['DAYS_EMPLOYED'].replace({365243 : np.nan}, inplace=True)\nalign_test_data['DAYS_EMPLOYED'].replace({365243 : np.nan}, inplace=True)\nalign_train_data['DAYS_EMPLOYED'].describe()","2c5dbab5":"align_train_data['DAYS_EMPLOYED'] = abs(align_train_data['DAYS_EMPLOYED'])\nalign_test_data['DAYS_EMPLOYED'] = abs(align_test_data['DAYS_EMPLOYED'])\n\n#Now plot histogram to visualize days of employment distribution\nalign_train_data['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","6b3ce88b":"correlations = align_train_data.corr()['TARGET'].sort_values()\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","bb2e0135":"feature_col = ['EXT_SOURCE_3','EXT_SOURCE_2','EXT_SOURCE_1','DAYS_BIRTH','DAYS_EMPLOYED']\nX = align_train_data[feature_col]\nY = align_train_data['TARGET']\nX_test = align_test_data[feature_col]\nprint('Training Features shape : ', X.shape)\nprint('Testing Features shape : ', X_test.shape) ","a5c50228":"# have a final look at train  data to check if we miss something\nX.describe()","201525d9":"# have a final look at test data to check if we miss something\nX_test.describe()","b4ccdc0d":"# Utility functions to fix missing values    \ndef impute_missing_values(train_data, test_data):\n    \"\"\"\n    check for the missing values and impute it with mean values of the columns\n    in both train and test data\n    \"\"\"\n    from sklearn.preprocessing import Imputer\n    imputer = Imputer(strategy = 'median')\n    imputed_train_data = imputer.fit_transform(train_data)\n    imputed_test_data = imputer.transform(test_data)\n    return imputed_train_data, imputed_test_data\n","04c691d6":"#impute the missing values\nX, X_test = impute_missing_values(X, X_test)\nprint('training data shape after imputing missing values: ', X.shape)\nprint('testing data shape after imputing missing values: ', X_test.shape)","b2dc62ee":"# Utility functions to normalize data   \ndef normalize_values(train_data, test_data):\n    \"\"\"\n    Normalizing the feature data values using MinMaxScaler library \n    in both train and test data\n    \"\"\"\n    \n    from sklearn.preprocessing import MinMaxScaler\n    scaler = MinMaxScaler(feature_range = (0, 1)) \n    normalized_train_data = scaler.fit_transform(train_data)\n    normalized_test_data = scaler.transform(test_data)\n    return normalized_train_data, normalized_test_data","f8c13f07":"#normalize data\nX, X_test = normalize_values(X, X_test)\nprint('training data shape after normalizing values: ', X.shape)\nprint('testing data shape after normalizing values: ', X_test.shape)","7400d127":"#Utility function to create logistic regression model\nimport time   \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_absolute_error\n    \ndef create_logictic_regression_model(X, Y):\n    \"\"\"\n    split data into training and validation data, for both features and target\n    The split is based on a random number generator. Supplying a numeric value to\n    the random_state argument guarantees we get the same split every time we\n    run this script.\n    \n    create model using sklearn logistic regression library and measuere model performance\n    \"\"\"\n\n    print('Starting Logistic regression model training...')\n    t0 = time.time()\n    \n    #X_train, X_val, Y_train, Y_val = train_test_split(X, Y, random_state = 0, test_size=0.2)\n    \n    # Make the model with the specified regularization parameter\n    model = LogisticRegression(C = 0.0001)\n    model.fit(X,Y)\n\n    # Train on the training data\n    #model.fit(X_train, Y_train)\n    \n    #accuracy = model.score(X_val, Y_val)\n    #print('Accuray of logistic regression model is : ', accuracy)\n    \n    #Y_pred = model.predict(X_val)\n    #mae = mean_absolute_error(Y_val, Y_pred)\n    #print('mean absoute error of logistic regression model is : ', mae)\n    \n    t1 = time.time()\n    print('Time elapsed during logistic regression model training is : ', t1-t0)\n    \n    return model","17da3a6a":"# create logistic regression model\nlogistic_regr_model = create_logictic_regression_model(X,Y)","83b63430":"# generate the prediction for test data\n# Make sure to select the second column only\nlogistic_regr_pred = logistic_regr_model.predict_proba(X_test)[:,1]","acbc4abd":"# Submission dataframe\nsubmit = align_test_data[['SK_ID_CURR']]\nsubmit['TARGET'] = logistic_regr_pred\nsubmit.head()","2981fbfc":"# Save the submission to a csv file\nsubmit.to_csv('log_reg_baseline.csv', index = False)","1df8e79f":"Here we obsereve that few features have negative mean values which looks suspective. This might be a sign of data anomalies.\nBelow are the features we need to look at - ","79d220a8":"When it comes time to build our machine learning models, we will have to fill in these missing values (known as imputation). In later work, we will use models such as XGBoost that can handle missing values with no need for imputation. Another option would be to drop columns with a high percentage of missing values, although it is impossible to know ahead of time if these columns will be helpful to our model. Therefore, we will keep all of the columns for now.","7d9e9dad":"The predictions represent a probability between 0 and 1 that the loan will not be repaid. If we were using these predictions to classify applicants, we could set a probability threshold for determining that a loan is risky.","755cb1c4":"### Column Types\nLet's look at the number of columns of each data type. int64 and float64 are numeric variables (which can be either discrete or continuous). object columns contain strings and are categorical features. .","7cec1b0b":"### Handling Missing value","ccfa6db9":"It seems all the days are past number of days relative to the present day when loan application is registered. We are just going to change it in years to find the age of applicants.","ad4b15f8":"### Normalize tarining and test data","3e181127":"### Aligning Training and Testing Data\nThere need to be the same features (columns) in both the training and testing data. One-hot encoding has created more columns in the training data because there were some categorical variables with categories not represented in the testing data. To remove the columns in the training data that are not in the testing data, we need to align the dataframes. When we do the align, we must make sure to set axis = 1 to align the dataframes based on the columns and not on the rows!","83c32cf4":"### Correlation\nNow we will look at the correlation of these features with target value in training dataset...","a0898dd5":"Here we observe that it is an imbalanced class problem. There are far more loans that were repaid on time than loans that were not repaid.","3a362a22":"### Examine the Distribution of the Target Column\nThe target is what we are asked to predict: either a 0 for the loan was repaid on time, or a 1 indicating the client had payment difficulties. We can first examine the number of loans falling into each category.","c8508ff3":"Here we see that all the anomalies are subjected to only one number - 365243. We will replace this number to NaN.","4eab0826":"### Back to Exploratory Data Analysis\nLets look at the statistics of columns again to have a deeper look of training data and identify all important features","006e1eea":"The training and testing datasets now have the same features which is required for machine learning. The number of features has grown significantly due to one-hot encoding. At some point we probably will want to try dimensionality reduction (removing features that are not relevant) to reduce the size of the datasets.","3969ce35":"Age looks reasonable now. If we look at the DAYS_EMPLOYED column, it seems very unreasonable. The maximum number is close to 1000 years.","f1c0b86b":"### One Hot Encoding for all categorical data","423bc9b1":"### Examine Missing Values\nNow we will examine the columns with missing values","9026b111":"### Encoding Categorical Variables\nBefore we go any further, we need to deal with these categorical variables. A machine learning model unfortunately cannot deal with categorical variables (except for some models such as LightGBM). Therefore, we have to find a way to encode (represent) these variables as numbers before handing them off to the model. There are two main ways to carry out this process:\n\n1. Label encoding: assign each unique category in a categorical variable with an integer. No new columns are created.\n\n2. One-hot encoding: create a new column for each unique category in a categorical variable. Each observation recieves a 1 in the column for its corresponding category and a 0 in all other new columns.\n\nThe problem with label encoding is that it gives the categories an arbitrary ordering. The value assigned to each of the categories is random and does not reflect any inherent aspect of the category. Therefore, when we perform label encoding, the model might use the relative value of the feature (for example programmer = 4 and data scientist = 1) to assign weights which is not what we want. If we only have two unique values for a categorical variable (such as Male\/Female), then label encoding is fine, but for more than 2 unique categories, one-hot encoding is the safe option.\nThe only downside to one-hot encoding is that the number of features (dimensions of the data) can explode with categorical variables with many categories. To deal with this, we can perform one-hot encoding followed by PCA or other dimensionality reduction methods to reduce the number of dimensions.","13f52596":"Here we go..\nWe have bunch of entries which seems unreasonable. We have to remove those. Lets consider the maximum age of the person who is applying is 100.","3f5f0e14":"Here are the observations from the first look at data - \n\n1. There are few columns with object type - need to change into categorical data\n2. There are many columns with missing values - need to fix it\n3. There are few columns with high standard deviation - need to normalize the data","daa40c44":"Now that the model has been trained, we can use it to make predictions. We want to predict the probabilities of not paying a loan, so we use the model predict.proba method. This returns an m x 2 array where m is the number of observations. The first column is the probability of the target being 0 and the second column is the probability of the target being 1 (so for a single row, the two columns must sum to 1). We want the probability the loan is not repaid, so we will select the second column.","c97c6403":"### Creating Baseline model","747d3360":"The predictions must be in the format shown in the sample_submission.csv file, where there are only two columns: SK_ID_CURR and TARGET. We will create a dataframe in this format from the test set and the predictions called submit.","28b091ef":"As usual, the day counts are negative since it is relative to the date of load application. Let's make it postitive for better visualization.","b0124024":"We have all the feature correlation with the target. We are going to consider only those features whose abs correlation is more than 0.7.\nSo, below are the features we are going to consider for our model - \n1. EXT_SOURCE_3\/2\/1\n2. DAYS_BIRTH\n3. DAYS_EMPLOYED"}}