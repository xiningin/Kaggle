{"cell_type":{"e73e890f":"code","d00b00dd":"code","4a2c4148":"code","dca7c0d5":"code","435bd703":"code","38021b4f":"code","df07ab53":"code","4fcec8ea":"code","bca8d32b":"code","0500e4e3":"code","c0ad8fe2":"code","1f17abeb":"code","6017e6d4":"code","cbdbcffd":"code","7da9379b":"code","14dcd7f3":"code","a55050ec":"code","7c6e46c7":"markdown","58201c92":"markdown","fd58a311":"markdown","cf089698":"markdown","c2baeeed":"markdown","a22b9af7":"markdown"},"source":{"e73e890f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/working\/checkPointFolder'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d00b00dd":"song_lyrics = open('..\/input\/\/poetry\/Kanye_West.txt','rb').read().decode(encoding='utf-8')\n","4a2c4148":"#adding all vocabularies\nvocab = sorted(set(song_lyrics))\n","dca7c0d5":"char2idx = {u: i for i, u in enumerate(vocab)}\nidx2char = np.array(vocab)","435bd703":"\ntext_as_integer = np.array([char2idx[text] for text in song_lyrics])\nchar_data_set = tf.data.Dataset.from_tensor_slices(text_as_integer)","38021b4f":"# sequence_length is the size of the training sequence that is fed into Neural network at an instance\nsequence_length = 256\nexamples_per_epoch = len(song_lyrics) \/\/ sequence_length","df07ab53":"sequence_text = char_data_set.batch(sequence_length+1, drop_remainder=True)\n\nfor item in sequence_text.take(5):\n    print(repr(''.join(idx2char[item.numpy()])))","4fcec8ea":"# divide the sequence into input and output sampel\ndef split_input(chunk):\n    inp = chunk[:-1]\n    out = chunk[1:]\n    return inp, out\ndataset = sequence_text.map(split_input)","bca8d32b":"\nfor j, k in dataset.take(1):\n    print(repr(''.join(idx2char[j.numpy()])))\n    print(repr(''.join(idx2char[k.numpy()])))\n    ","0500e4e3":"BATCH_SIZE= 64\ndataset= dataset.shuffle(100000).batch(BATCH_SIZE,drop_remainder=True)","c0ad8fe2":"def loss(labels, logits):\n    return keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)","1f17abeb":"#create the checkpoint for callback function\nfilepath = os.path.join('checkPointFolder\/', \"ckpt{epoch}\")\ncheckpoint_callback = keras.callbacks.ModelCheckpoint(filepath, save_weights_only=True)","6017e6d4":"vocabulary_size = len(vocab)\nembedding_dim = 512\nrnn_units = 1024\n\ndef createModel(batchSize):    \n    model = keras.Sequential()\n    model.add(keras.layers.Embedding(vocabulary_size, embedding_dim, batch_input_shape=[batchSize, None]))\n    model.add(keras.layers.GRU(rnn_units, return_sequences=True,stateful=True, recurrent_initializer='glorot_uniform'))\n    model.add(keras.layers.Dense(vocabulary_size))\n    return model\n","cbdbcffd":"model = createModel(BATCH_SIZE)\nmodel.compile(optimizer=\"adam\", loss=loss)\nmodel.fit(dataset,epochs=40,callbacks=[checkpoint_callback])","7da9379b":"#load the model and build model\npredictionModel = createModel(1)\npredictionModel.load_weights(tf.train.latest_checkpoint(\"\/kaggle\/working\/checkPointFolder\"))\npredictionModel.build(tf.TensorShape([1,None]))\npredictionModel.summary()","14dcd7f3":"def generate_text(model_hat, start_string):\n    # Evaluation step (generating text using the learned model)\n\n    # Number of characters to generate\n    num_generate = 1000\n\n    # Converting our start string to numbers (vectorizing)\n    input_eval = [char2idx[s] for s in start_string]\n    input_eval = tf.expand_dims(input_eval, 0)\n\n    # Empty string to store our results\n    text_generated = []\n\n    # Low temperatures results in more predictable text.\n    # Higher temperatures results in more surprising text.\n    # Experiment to find the best setting.\n    temperature = 1.0\n\n    # Here batch size == 1\n    model_hat.reset_states()\n    for i in range(num_generate):\n        predictions = model_hat(input_eval)\n        # remove the batch dimension\n        predictions = tf.squeeze(predictions, 0)\n\n        # using a categorical distribution to predict the word returned by the model\n        predictions = predictions \/ temperature\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n        # We pass the predicted word as the next input to the model\n        # along with the previous hidden state\n        input_eval = tf.expand_dims([predicted_id], 0)\n\n        text_generated.append(idx2char[predicted_id])\n\n    return (start_string + ''.join(text_generated))\n","a55050ec":"out = generate_text(predictionModel, \"Lamborgini\")\nprint(out)","7c6e46c7":"# Load the song \"Gold-digger\" by Kanye West","58201c92":"# Lets see how the training sample looks like, we will extract 1 sequence from the sample using dataset.take(n) method","fd58a311":"### Convert all the charchters in the original song lyirics into integers and create a Dataset object","cf089698":"## Create the Vocabulary","c2baeeed":"### Create the numerical index for each charachter in the vocabulary","a22b9af7":"## Lets generate a new Song using the starter word \"Lamborgini\""}}