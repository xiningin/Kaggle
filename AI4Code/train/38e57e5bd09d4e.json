{"cell_type":{"dd8111a7":"code","ffdd3bd9":"code","07807872":"code","b2f1e5a0":"code","4d8f51b2":"code","da791086":"code","6a68d4e7":"code","85e2068b":"code","533bc412":"code","96dad4c8":"code","7c696acd":"code","171f8f04":"code","f705ad31":"code","3e0fd97d":"code","dbd3c35b":"code","dce31217":"code","2e068328":"code","15c9c1ca":"code","ee73b748":"code","c3ea33ba":"code","58b477ce":"code","2c4851d4":"code","210aeaf9":"code","e25e4cb0":"code","5c083c2e":"code","2e3c8fb8":"code","619225b1":"code","48753001":"code","79755bc4":"code","a8007179":"code","725d9419":"code","1f454309":"code","451ae810":"code","9c6db5ef":"code","0755d6fc":"code","9491af39":"code","98aee765":"code","ac2879f0":"code","8db43e13":"code","004ccff1":"code","2f35630a":"code","8a6d0d3b":"code","f9b7f8a3":"code","79f2fd80":"code","a4a35429":"code","230555f4":"code","d3a601b3":"code","187e853e":"code","ac425145":"code","77eb0b4c":"code","bfbfb997":"markdown","e19cb819":"markdown","85ce6ef5":"markdown","6da1b071":"markdown","2ae6db5b":"markdown","07741016":"markdown","5872e3ee":"markdown","016259ad":"markdown","0310b9b9":"markdown","8dc51994":"markdown","cb9a1ed7":"markdown","0c62a916":"markdown","0ba2db69":"markdown","50d87dd0":"markdown","464c6144":"markdown","4eae16d0":"markdown","167771dd":"markdown","3ee121fc":"markdown","075f6a41":"markdown"},"source":{"dd8111a7":"import zipfile\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom transformers import BertTokenizer, BertModel, logging\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom pytorch_lightning.metrics.functional import accuracy\nfrom collections import defaultdict\n\nlogging.set_verbosity_error()","ffdd3bd9":"train_csv_path = '..\/input\/d\/julian3833\/jigsaw-toxic-comment-classification-challenge\/train.csv'\nsample_sub_path = '..\/input\/jigsaw-toxic-severity-rating\/sample_submission.csv'\ntest_csv_path = '..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv'","07807872":"train_df = pd.read_csv(train_csv_path)\ntrain_df.head()","b2f1e5a0":"test_df = pd.read_csv(test_csv_path)\ntest_df.head()","4d8f51b2":"sample_sub_df = pd.read_csv(sample_sub_path)\nsample_sub_df.head()","da791086":"train_df.comment_text.isnull().sum()","6a68d4e7":"test_df.text.isnull().sum()","85e2068b":"def add_all(row):\n    toxicity = row[2:].sum()\n    if toxicity > 0:\n        return 1\n    else:\n        return 0\n\ntrain_df['toxic_nontoxic'] = train_df.apply(add_all, axis='columns')\ntrain_df.head()","533bc412":"ax = sns.countplot(train_df.toxic_nontoxic)\nplt.xlabel('Toxic vs Non toxic')\nax.set_xticklabels(['Non Toxic', 'Toxic'])","96dad4c8":"train_df.toxic_nontoxic.value_counts()","7c696acd":"toxic_train_df = train_df[train_df.toxic_nontoxic == 1]\nnontoxic_train_df = train_df[train_df.toxic_nontoxic == 0]\ntoxic_train_df.shape, nontoxic_train_df.shape","171f8f04":"sample = 16000\ntoxic_train_df = toxic_train_df.sample(frac=1).reset_index(drop=True)\nnontoxic_train_df = nontoxic_train_df.sample(frac=1).reset_index(drop=True)\n\nnew_train_df = pd.concat([toxic_train_df[:sample], nontoxic_train_df[:sample]])\nnew_train_df = new_train_df.sample(frac=1).reset_index(drop=True)\nnew_train_df.head()","f705ad31":"ax = sns.countplot(new_train_df.toxic_nontoxic)\nplt.xlabel('Toxic vs Non toxic')\nax.set_xticklabels(['Non Toxic', 'Toxic'])","3e0fd97d":"new_train_df = new_train_df.drop(['toxic_nontoxic'], axis=1)\nnew_train_df.head()","dbd3c35b":"# Assign weights to each category\nweights_per_category = {'toxic': 0.5,\n                        'severe_toxic': 1.5,\n                        'obscene': 0.25,\n                        'threat': 1.5,\n                        'insult': 0.8,\n                        'identity_hate': 1.5}","dce31217":"for category in weights_per_category:\n    new_train_df[category] = new_train_df[category] * weights_per_category[category]","2e068328":"new_train_df['score'] = new_train_df.loc[:, 'toxic':'identity_hate'].mean(axis=1)\nnew_train_df.head()","15c9c1ca":"# Defining tokenizer instance from BertTokenizer\n# we will use \"bert base uncased\" pretrained model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","ee73b748":"# tokenizer encode example on sample text\nidx = 2\nsample_text = new_train_df.loc[idx, 'comment_text']\nprint('Sample Text: ')\nprint(sample_text)\n\n# we will use encode_plus method from tokenizer instance\n# https:\/\/huggingface.co\/docs\/transformers\/v4.15.0\/en\/internal\/tokenization_utils#transformers.PreTrainedTokenizerBase.encode_plus\n\nencoded_sample = tokenizer.encode_plus(\n    sample_text,\n    add_special_tokens=True,     # [CLS], [SEP], [PAD] tokens\n    max_length=100,   # to be determined next!!\n    return_token_type_ids=False,\n    padding='max_length',\n    return_attention_mask=True,\n    return_tensors='pt'    # pytorch tensor\n)\n\n# encoded_sample is a dictionary \n\nencoded_sample.keys()","c3ea33ba":"# Let's see the encoded token ids of the sample text\nprint('Encoded tokens of sample text:')\nprint(encoded_sample['input_ids'])\nprint(f\"Shape of encoded tokens: {encoded_sample['input_ids'].shape}\")","58b477ce":"# Let's see the attention mask (contains 1(token to be considered) and 0(token not to be considered, in case of padding))\nattention_mask = encoded_sample['attention_mask']\nprint(attention_mask)\nprint(f'Shape of attention mask: {attention_mask.shape}')","2c4851d4":"# Let's see the encoded tokens of the sample text\nencoded_tokens = tokenizer.convert_ids_to_tokens(encoded_sample['input_ids'].squeeze())\nprint(encoded_tokens)","210aeaf9":"token_lens = []\nfor i in tqdm(range(len(new_train_df))):\n    comment_text = new_train_df.loc[i, 'comment_text']\n    # encode comment_text (using .encode method to get only the token ids)\n    tokens = tokenizer.encode(comment_text, max_length=512, truncation=True)\n    token_lens.append(len(tokens))","e25e4cb0":"sns.distplot(token_lens)\nplt.xlim([0,512])\nplt.xlabel('Token count')","5c083c2e":"MAX_LEN = 350","2e3c8fb8":"train_df, val_df = train_test_split(new_train_df, test_size=0.2)\ntrain_df.shape, val_df.shape","619225b1":"class CustomDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        comment_text = self.df.iloc[index, 1]\n        target = self.df.iloc[index, -1]\n        \n        encoding = self.tokenizer.encode_plus(comment_text, \n                                             add_special_tokens=True,\n                                             max_length=self.max_len,\n                                             return_token_type_ids=False,\n                                             padding='max_length',\n                                             truncation=True,\n                                             return_attention_mask=True,\n                                             return_tensors='pt'\n                                             )\n        \n        input_ids = encoding['input_ids'].squeeze()    # Shape: (max_length)\n        attention_mask = encoding['attention_mask'].squeeze()    # Shape: (max_length)\n        target = torch.tensor(target)\n            \n        return input_ids, attention_mask, target","48753001":"train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\nval_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)","79755bc4":"input_ids, attention_mask, targets = train_dataset[0]\ninput_ids.shape, attention_mask.shape, targets.shape","a8007179":"input_ids, attention_mask, targets = val_dataset[0]\ninput_ids.shape, attention_mask.shape, targets.shape","725d9419":"BATCH_SIZE = 16","1f454309":"train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, pin_memory=True)","451ae810":"data = next(iter(train_loader))\ninput_ids, attention_mask, targets = data\ninput_ids.shape, attention_mask.shape, targets.shape","9c6db5ef":"DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'","0755d6fc":"# Instantiating pretrained bertmodel\nbert_model = BertModel.from_pretrained('bert-base-uncased')","9491af39":"output = bert_model(input_ids=input_ids,\n                   attention_mask=attention_mask,\n                   return_dict=True)\noutput.keys()","98aee765":"# bert_model outputs two tensors last_hidden_state and pooler_output\n# Let's see their dimensions\n\nlast_hidden_state, pooler_output = output['last_hidden_state'], output['pooler_output']\n\nlast_hidden_state.shape, pooler_output.shape","ac2879f0":"bert_model.config.hidden_size","8db43e13":"linear_layer = nn.Linear(768, 1)\noutput = linear_layer(pooler_output)\noutput.shape","004ccff1":"criterion = nn.MSELoss()","2f35630a":"loss = criterion(output.squeeze(), targets)\nprint(f'Loss on sample batch: {loss.item()}')","8a6d0d3b":"class Net(nn.Module):\n    def __init__(self, bert_model):\n        super(Net, self).__init__()\n        self.bert_model = bert_model\n        self.fcdense = nn.Linear(self.bert_model.config.hidden_size, 1)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, input_ids, attention_mask):\n        bert_out = self.bert_model(input_ids, attention_mask, return_dict=True)\n        pooler_output =  bert_out['pooler_output']    # (batch_size, 768)\n        output = self.fcdense(pooler_output)       # (batch_size, 1)\n        return output","f9b7f8a3":"model = Net(bert_model=bert_model).to(DEVICE)\noutput = model(input_ids.to(DEVICE), attention_mask.to(DEVICE))\nprint(output.shape)","79f2fd80":"# Freeze bert layers\nfor name, param in model.named_parameters(): \n    if 'fcdense' in name:\n        param.requires_grad = True\n    else:\n        param.requires_grad = False\n    #print(name, param.requires_grad)","a4a35429":"def train_epoch(model, train_loader, criterion, optimizer, DEVICE):\n    model.train()\n    \n    losses = []\n    \n    for batch_idx, data in enumerate(tqdm(train_loader)):\n        input_ids, attention_mask, targets = data\n        input_ids = input_ids.to(DEVICE)   # (batch_size, seq_len)\n        attention_mask = attention_mask.to(DEVICE)   # (batch_size, seq_len)\n        targets = targets.to(DEVICE)  # (batch_size,)\n\n        output = model(input_ids, attention_mask)   # (batch_size, 1)\n\n        loss = criterion(output.squeeze().float(), targets.float())\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        losses.append(loss.item())\n\n    return np.mean(losses)","230555f4":"def val_epoch(model, val_loader, criterion, DEVICE):\n    model.eval()\n    \n    losses = []\n    \n    with torch.no_grad():\n        for batch_idx, data in enumerate(tqdm(val_loader)):\n            input_ids, attention_mask, targets = data\n            input_ids = input_ids.to(DEVICE)   # (batch_size, seq_len)\n            attention_mask = attention_mask.to(DEVICE)   # (batch_size, seq_len)\n            targets = targets.to(DEVICE)  # (batch_size,)\n\n            output = model(input_ids, attention_mask)   # (batch_size, 1)\n\n            loss = criterion(output.squeeze().float(), targets.float())\n\n            losses.append(loss.item())\n\n    return np.mean(losses)","d3a601b3":"EPOCHS = 25\nLEARNING_RATE = 2e-5\n\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\nbest_val_loss = np.inf\n\nfor epoch in range(EPOCHS):\n    print(f'Epoch: {epoch+1}\/{EPOCHS}')\n    print('-' * 10)\n    \n    print('Training')\n    train_loss = train_epoch(model,train_loader,criterion,optimizer,DEVICE)\n    \n    print('Validating')\n    val_loss = val_epoch(model,val_loader,criterion,DEVICE)\n    \n    print(f'Train Loss: {train_loss}\\t Val Loss: {val_loss}')\n    \n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), 'toxicity_best_model.pth.tar')","187e853e":"tokenizer.save_pretrained('.\/tokenizer_pretrained\/')","ac425145":"bert_model.save_pretrained('.\/bert_model_pretrained\/')","77eb0b4c":"print('Done!')","bfbfb997":"### Importing libraries","e19cb819":"### Building Train and Val DataLoaders","85ce6ef5":"### Dataset path\n\nIn this notebook, I have added dataset from \nhttps:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\/data","6da1b071":"### Determining the max length of the tokens to be encoded from comment_text (using BertTokenizer)","2ae6db5b":"### Let's train the model","07741016":"### Now let's build the model architecture","5872e3ee":"### Spliting newly sampled dataframe into training and validation data frames","016259ad":"### Checking NaN","0310b9b9":"**last_hidden_state** contains the hidden representations for each token in each sequence of the batch. So the size is (batch_size, seq_len, hidden_size)\n\n**pooler_output** contains a \"representation\" of each sequence in the batch, and is of size (batch_size, hidden_size)\n\n768 is the hidden size\n\n[More about bert model output](https:\/\/github.com\/huggingface\/transformers\/issues\/7540)","8dc51994":"### Building Custom Dataset\n","cb9a1ed7":"We see that the dataset is highly imbalanced. Around 16k comments are toxic. We will sample 16k non toxic comments to create new balanced training dataset.\n","0c62a916":"### Checking for imbalance in the training dataset","0ba2db69":"We see that a good fit for max length of tokens would be around 350","50d87dd0":"### Awesome! Now let's define the loss function and print the loss value corresponding to the sample batch we used","464c6144":"We are interested in the pooler_output.\nSo we will feed the pooler_output to linear layer to output a score","4eae16d0":"### Double Awesome!!","167771dd":"### Let's try running a sample batch on BertModel","3ee121fc":"### Create a score column for each comment text","075f6a41":"### Save the bert_model and bert_tokenizer"}}