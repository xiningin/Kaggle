{"cell_type":{"c0bc6ad4":"code","c5abb657":"code","a8a83187":"code","dc0c0c38":"code","4b51dca9":"code","f62dcf0d":"code","694d3801":"code","9d99d3c9":"code","71517820":"code","41af3071":"code","10c1b21b":"code","ab713a2f":"code","1149decc":"code","a5c99462":"code","8e355192":"code","3a2713da":"code","fd53def9":"code","2e33f245":"code","b6e42dc8":"code","9e1d8d61":"code","d64afe1a":"code","b27b81e2":"code","a64c7549":"code","f7600a67":"code","5e83fb1b":"code","ed488c10":"code","5578f2a2":"code","cec9e0cf":"code","8f605053":"code","6ae65a81":"markdown","199a48c1":"markdown","102ea583":"markdown","e60f2205":"markdown","ce15326c":"markdown","7833136f":"markdown","b6bf369d":"markdown"},"source":{"c0bc6ad4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c5abb657":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\n\nseed = random.seed(100)","a8a83187":"path = '..\/input\/water-potability\/water_potability.csv'\ndataLoad = pd.read_csv(path)\ndataLoad.head()","dc0c0c38":"dataLoad.isnull().sum()","4b51dca9":"dataLoad.info()","f62dcf0d":"dataLoad.describe()","694d3801":"a = dataLoad['Potability'].value_counts()\nsns.barplot(x=a.index, y=a)","9d99d3c9":"for i in dataLoad.columns:\n    sns.histplot(x=i, data=dataLoad)\n    plt.show()","71517820":"a = dataLoad.copy()\na.drop('Potability', axis=1, inplace=True)\na","41af3071":"from scipy import stats\n\nfor i in a.columns:\n    stat, p = stats.shapiro(a[i])\n    \n    if p > 0.05 :\n        print('{} feature does have normal distribution (p : {})'.format(i, round(p, 3)))\n    else:\n        print('{} feature doesn\\'t have normal distribution (p : {})'.format(i, round(p, 3)))\n         ","10c1b21b":"ls =[]\ncol=[c for c in a.columns]\n\nfor m in range(3):\n    for n in range(3):\n        ls.append((m, n))\n\nfigh, axes = plt.subplots(3, 3, figsize=(18, 15))\nfor num in range(9):\n    sns.boxplot(ax=axes[ls[num][0], ls[num][1]], y=col[num], data=a)\n    ","ab713a2f":"ls1 =[]\ncol1=[c for c in a.columns]\n\nfor m in range(3):\n    for n in range(3):\n        ls.append((m, n))\n\nfigh, axes1 = plt.subplots(3, 3, figsize=(18, 15))\nfor num1 in range(9):\n    sns.boxplot(ax=axes1[ls[num1][0], ls[num1][1]], y=a[col1[num1]], x=dataLoad['Potability'])","1149decc":"sns.pairplot(a)","a5c99462":"dataCorr =a.corr()\nplt.figure(figsize=(15, 8))\nsns.heatmap(dataCorr, annot=True)","8e355192":"dataPre = pd.concat([a, dataLoad['Potability']], axis=1)\n\nQ3 = a.quantile(0.75)\nQ1 = a.quantile(0.25)\n\nIQR = Q3-Q1\ndataCleaned = dataPre[~((dataPre < (Q1-1.5*IQR)) | (dataPre > (Q3+1.5*IQR))).any(axis=1)]\ndataCleaned.head()","3a2713da":"dataCleaned.isnull().sum()","fd53def9":"from sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy='mean', missing_values=np.nan)\ndataImputed = pd.DataFrame(imputer.fit_transform(dataCleaned), columns=[dataCleaned.columns])\ndataImputed.head()","2e33f245":"dataImputed.isnull().sum()","b6e42dc8":"from sklearn.model_selection import train_test_split\n\ny=dataImputed['Potability']\nX=dataImputed.drop('Potability', axis=1)\n\n\nx_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.2)\nx_Train, x_valid, y_Train, y_valid = train_test_split(x_train, y_train, test_size=0.2)","9e1d8d61":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodelBase = keras.Sequential([layers.Dense(units=16, activation='relu', input_shape=[9]),\n                             layers.Dense(units=1, activation='sigmoid')])\n","d64afe1a":"modelBase.compile(optimizer='adam',\n                 loss='binary_crossentropy',\n                 metrics=['binary_accuracy'])\n\nearlyStopping = keras.callbacks.EarlyStopping(patience=5, min_delta=0.001, restore_best_weights=True)\n\nhistory=modelBase.fit(x_Train, y_Train,\n                     validation_data=(x_valid, y_valid),\n                     epochs=200,\n                     batch_size=256,\n                     callbacks=[earlyStopping],\n                     verbose = 0)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title='Cross-Entropy')\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title='Binary-Accuracy')\n\nprint('best val loss : ', history_df['val_loss'].min())\nprint('best val accuracy : ', history_df['val_binary_accuracy'].max())","b27b81e2":"model1 = keras.Sequential([layers.Dense(units=16, activation='relu', input_shape=[9]),\n                           layers.Dense(units=16, activation='relu'),\n                           layers.Dense(units=1, activation='sigmoid')])\n                            ","a64c7549":"model1.compile(optimizer='adam',\n               loss='binary_crossentropy',\n                metrics=['binary_accuracy'])\n\nhistory1=model1.fit(x_Train, y_Train, validation_data=(x_valid, y_valid),\n          epochs=200, batch_size=256, callbacks=[earlyStopping], verbose=0)\n\nhistory1_df = pd.DataFrame(history1.history)\n\nhistory1_df.loc[:, ['loss', 'val_loss']].plot(title='Cross-Entropy')\nhistory1_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title='Binary-accuaracy')\n\nprint('best val loss : ', history1_df['val_loss'].min())\nprint('best val accuracy : ', history1_df['val_binary_accuracy'].max())","f7600a67":"model2 = keras.Sequential([layers.Dense(units=32, activation='relu', input_shape=[9]),\n                          layers.Dense(units=32, activation='relu'),\n                          layers.Dense(units=1, activation='sigmoid')])","5e83fb1b":"model2.compile(optimizer='adam',\n               loss='binary_crossentropy',\n                metrics=['binary_accuracy'])\n\nhistory2=model2.fit(x_Train, y_Train, validation_data=(x_valid, y_valid),\n          epochs=200, batch_size=256, callbacks=[earlyStopping], verbose=0)\n\nhistory2_df = pd.DataFrame(history2.history)\n\nhistory2_df.loc[:, ['loss', 'val_loss']].plot(title='Cross-Entropy')\nhistory2_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title='Binary-accuaracy')\n\nprint('best val loss : ', history2_df['val_loss'].min())\nprint('best val accuracy : ', history2_df['val_binary_accuracy'].max())","ed488c10":"model3 = keras.Sequential([layers.BatchNormalization(),\n                           layers.Dense(units=512, activation='relu', input_shape=[9]),\n                           layers.BatchNormalization(),\n                           layers.Dropout(0.3),\n                           layers.Dense(units=512, activation='relu'),\n                           layers.BatchNormalization(),\n                           layers.Dropout(0.3),\n                           layers.Dense(units=1, activation='sigmoid')])","5578f2a2":"model3.compile(optimizer='adam',\n               loss='binary_crossentropy',\n                metrics=['binary_accuracy'])\n\nhistory3=model3.fit(x_Train, y_Train, validation_data=(x_valid, y_valid),\n          epochs=200, batch_size=512, callbacks=[earlyStopping], verbose=0)\n\nhistory3_df = pd.DataFrame(history3.history)\n\nhistory3_df.loc[:, ['loss', 'val_loss']].plot(title='Cross-Entropy')\nhistory3_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title='Binary-accuracy')\n\nprint('best val loss : ', history3_df['val_loss'].min())\nprint('best val accuracy : ', history3_df['val_binary_accuracy'].max())","cec9e0cf":"model4 = keras.Sequential([layers.BatchNormalization(),\n                           layers.Dense(units=512, activation='relu', input_shape=[9]),\n                           layers.BatchNormalization(),\n                           layers.Dropout(0.3),\n                           layers.Dense(units=512, activation='relu'),\n                           layers.BatchNormalization(),\n                           layers.Dropout(0.3),\n                           layers.Dense(units=1, activation='sigmoid')])","8f605053":"model4.compile(optimizer='adam',\n               loss='binary_crossentropy',\n                metrics=['binary_accuracy'])\n\nhistory4=model4.fit(x_Train, y_Train, validation_data=(x_valid, y_valid),\n          epochs=200, batch_size=512, callbacks=[earlyStopping], verbose=1)\n\nhistory4_df = pd.DataFrame(history4.history)\n\nhistory4_df.loc[:, ['loss', 'val_loss']].plot(title='Cross-Entropy')\nhistory4_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title='Binary-accuracy')\n\nprint('best val loss : ', history4_df['val_loss'].min())\nprint('best val accuracy : ', history4_df['val_binary_accuracy'].max())","6ae65a81":"## Data Splitting & Data preprocessing","199a48c1":"## EDA (Exploratory Data Analysis)","102ea583":"## Making Baseline ","e60f2205":"## Univariate Analysis","ce15326c":"## Data Cleaning","7833136f":"##### ","b6bf369d":"## Bivariate Analysis"}}