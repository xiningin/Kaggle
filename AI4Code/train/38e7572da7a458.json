{"cell_type":{"2321e8a2":"code","c3579f66":"code","08d589bc":"code","e192bef6":"code","a4d00ad2":"code","11c57714":"code","57e623af":"code","fde8fd90":"code","e4a7927c":"code","21fb579d":"code","444eefd1":"code","79e9f8d4":"code","ce7ba673":"code","4317db49":"code","55a938a5":"code","faad94da":"markdown","ff1246c2":"markdown","fcb1e8b3":"markdown","953b3eeb":"markdown","7fcc270d":"markdown","21f218b0":"markdown","98bf13c1":"markdown"},"source":{"2321e8a2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# NLTK modules\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\n\nimport re\n\nfrom gensim.models import Word2Vec # Word2Vec module\n\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c3579f66":"news_data = pd.read_csv('\/kaggle\/input\/bbc-fulltext-and-category\/bbc-text.csv')","08d589bc":"print(f\"Shape : {news_data.shape}, \\n\\nColumns: {news_data.columns}, \\n\\nCategories: {news_data.category.unique()}\")\n\n# print sample data\nnews_data.head().append(news_data.tail())","e192bef6":"# Plot category data\nplt.figure(figsize=(10,6))\nsns.countplot(news_data.category)\nplt.show()","a4d00ad2":"class DataPreparation:\n    def __init__(self, data, column='text'):\n        self.df = data\n        self.column = column\n    \n    def preprocess(self):\n        self.tokenize()\n        self.remove_stopwords()\n        self.remove_non_words()\n        self.lemmatize_words()\n        \n        return self.df\n    \n    def tokenize(self):\n        self.df['clean_text'] = self.df[self.column].apply(nltk.word_tokenize)\n        print(\"Tokenization is done.\")\n    \n    def remove_stopwords(self):\n        stopword_set = set(nltk.corpus.stopwords.words('english'))\n        \n        rem_stopword = lambda words: [item for item in words if item not in stopword_set]\n        \n        self.df['clean_text'] = self.df['clean_text'].apply(rem_stopword)\n        print(\"Remove stopwords done.\")\n    \n    def remove_non_words(self):\n        \"\"\"\n            Remove all non alpha characters from the text data\n            :numbers: 0-9\n            :punctuation: All english punctuations\n            :special characters: All english special characters\n        \"\"\"\n        regpatrn = '[a-z]+'\n        rem_special_chars = lambda x: [item for item in x if re.match(regpatrn, item)]\n        self.df['clean_text'] = self.df['clean_text'].apply(rem_special_chars)\n        print(\"Removed non english characters is done.\")\n        \n    def lemmatize_words(self):\n        lemma = nltk.stem.wordnet.WordNetLemmatizer()\n        \n        on_word_lemma = lambda x: [lemma.lemmatize(w, pos='v') for w in x]\n        \n        self.df['clean_text'] = self.df['clean_text'].apply(on_word_lemma)\n        print(\"Lemmatization on the words.\")","11c57714":"data_prep = DataPreparation(news_data)\n\ncleanse_df = data_prep.preprocess()","57e623af":"cleanse_df['clean_text']","fde8fd90":"vec_model = Word2Vec(cleanse_df['clean_text'])\n\nw2v = dict(zip(vec_model.wv.index2word, vec_model.wv.syn0))","e4a7927c":"class Vectorizer(object):\n    def __init__(self, vec):\n        self.vec = vec\n        self.dim = len(vec.values())\n        \n    def fit(self, X, y):\n        return self\n    \n    def transform(self, X):\n        return np.array([np.mean([self.vec[w] for w in words if w in self.vec] or [np.zeros(self.dim)], axis=0) for words in X])\n    \n\n    \n\n# Classifier class\nclass Classifier(object):\n    def __init__(self, model, param):\n        self.model = model\n        self.param = param\n        self.gsearch = GridSearchCV(self.model, self.param, cv=5, error_score=0, refit=True)\n        \n    def fit(self, X, y):\n        return self.gsearch.fit(X, y)\n    \n    def predict(self, X):\n        return self.gsearch.predict(X)\n\nclf_models = {\n    'Naive Bayes': GaussianNB(),\n    'SVC': SVC(),\n    'Decision Tree': DecisionTreeClassifier(),\n    'SGD Classifier': SGDClassifier(),\n    'Perceptron': MLPClassifier()\n}\n\nclf_params = {\n    'Naive Bayes': {},\n    'SVC' : {'kernel': ['linear', 'rbf']},\n    'Decision Tree': {'min_samples_split': [2, 5]},\n    'SGD Classifier': { 'penalty': ['l2', 'l1', 'elasticnet'] },\n    'Perceptron': {'activation': ['tanh', 'relu']}\n}","21fb579d":"X_train, X_valid, y_train, y_valid = train_test_split(cleanse_df['clean_text'], cleanse_df['category'], test_size=0.2, shuffle=True)","444eefd1":"# Iterate through the model names\nfor key in clf_models.keys():\n    \n    clf = Pipeline([('Word2Vec', Vectorizer(w2v)), ('Classifier', Classifier(clf_models[key], clf_params[key]))])\n    \n    # Fitting the data\n    clf.fit(X_train, y_train)\n    \n    y_preds = clf.predict(X_valid)\n    \n    \n    print(key, \":\")\n    print(\"Accuracy: %1.3f \\tPrecision: %1.3f \\tRecall: %1.3f \\tF1-Score: %1.3f\\n\" % (accuracy_score(y_valid, y_preds),\n                                                                                     precision_score(y_valid, y_preds, average='macro'),\n                                                                                     recall_score(y_valid, y_preds, average='macro'),\n                                                                                     f1_score(y_valid, y_preds, average='macro')))","79e9f8d4":"def vectorize(vector, X_train, X_test):\n    vector_fit = vector.fit(X_train)\n    \n    X_train_vec = vector_fit.transform(X_train)\n    X_test_vec = vector_fit.transform(X_test)\n    \n    print(\"Vectorization is completed.\")\n    return X_train_vec, X_test_vec\n\ndef label_encoding(y_train):\n    \"\"\"\n        Encode the given list of class labels\n        :y_train_enc: returns list of encoded classes\n        :labels: actual class labels\n    \"\"\"\n    lbl_enc = LabelEncoder()\n    \n    y_train_enc = lbl_enc.fit_transform(y_train)\n    labels = lbl_enc.classes_\n    \n    return y_train_enc, labels\n\ndef algorithm_stack(models, params, X_train, X_test, y_train, y_test):\n    \n    if not set(models.keys()).issubset(set(params.keys())):\n        raise ValueError('Keys do not match')\n        \n    for key in models.keys():\n        model = models[key]\n        param = params[key]\n        \n        gs = GridSearchCV(model, param, cv=5, error_score=0, refit=True)\n        gs.fit(X_train, y_train)\n        \n        y_pred = gs.predict(X_test)\n        \n        print(key, \":\")\n        print(\"Accuracy: %1.3f \\tPrecision: %1.3f \\tRecall: %1.3f \\tF1-Score: %1.3f\\n\" % (accuracy_score(y_test, y_pred),\n                                                                                     precision_score(y_test, y_pred, average='macro'),\n                                                                                     recall_score(y_test, y_pred, average='macro'),\n                                                                                     f1_score(y_test, y_pred, average='macro')))\n    return\n","ce7ba673":"# Encode the class labels\ny_enc_train, labels = label_encoding(news_data['category'])\n\n# Split from the loaded dataset\nX_train, X_valid, y_train, y_test = train_test_split(news_data['text'], y_enc_train, test_size=0.2, shuffle=True)\n\n# TFIDFVectorizer \nX_train_vec, X_valid_vec = vectorize(TfidfVectorizer(), X_train, X_valid)\n\nprint(X_train_vec.shape, X_valid_vec.shape)\n\nclf_models = {\n    'Naive Bayes': MultinomialNB(),\n    'SVC': SVC(),\n    'Decision Tree': DecisionTreeClassifier(),\n    'SGD Classifier': SGDClassifier(),\n    'Perceptron': MLPClassifier()\n}\n\n\n# Modified parameters\nclf_params = {\n    'Naive Bayes': {'alpha': [0.5, 1], 'fit_prior': [True, False] },\n    'SVC' : {'kernel': ['linear', 'rbf']},\n    'Decision Tree': {'min_samples_split': [1, 2, 5]},\n    'SGD Classifier': { 'penalty': ['l2', 'l1', 'elasticnet'] },\n    'Perceptron': {'alpha': [0.0001, 0.001], 'activation': ['tanh', 'relu']}\n}","4317db49":"y_train","55a938a5":"algorithm_stack(clf_models, clf_params, X_train_vec, X_valid_vec, y_train, y_test)","faad94da":"## Classification with Custom Vectorizer","ff1246c2":"# Text Classification - BBC News Data\n\n## Overview\n\nThe following notebook is created out of inspiration from the source [here](https:\/\/colab.research.google.com\/github\/srushtidhope\/bbc-text-classification\/blob\/master\/bbc_text_classification.ipynb#scrollTo=22L7TrqYtFiz) \n","fcb1e8b3":"## Vectorization using TFIDF","953b3eeb":"## Split the dataset","7fcc270d":"## Loading Dataset","21f218b0":"## Feature Engineering","98bf13c1":"## Data Preparation"}}