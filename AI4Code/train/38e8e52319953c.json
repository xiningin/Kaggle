{"cell_type":{"2f39ac0c":"code","cbea9c5e":"code","658d1878":"code","c4824751":"code","ab119959":"code","3ceb59b3":"code","65f16405":"code","4e159739":"code","67517bd0":"code","bf4d7721":"code","00589b16":"code","7de09268":"code","977cdf69":"code","75e17834":"code","d25100f7":"code","82a57765":"code","9420c3a6":"code","68ffbf7c":"code","6be9d05c":"code","76d1c134":"code","47fa657a":"markdown","4e5d8def":"markdown","3117b25f":"markdown","da871bab":"markdown","06e0cdc2":"markdown","da03cc65":"markdown","9d7cb699":"markdown","f939abc4":"markdown","de718e09":"markdown","fb52f027":"markdown","469ef989":"markdown","106b53dd":"markdown","de829659":"markdown","759ff92b":"markdown","1a9308d2":"markdown","5b3a0a56":"markdown","bae9e265":"markdown","150b1a18":"markdown","9f286c48":"markdown","cf2d5274":"markdown","1ef2adeb":"markdown","9107dd2f":"markdown","c199e422":"markdown","bfe9acdf":"markdown","66482602":"markdown","d9c19564":"markdown","98383c2c":"markdown","073c49f4":"markdown","462492cd":"markdown","d4362526":"markdown"},"source":{"2f39ac0c":"# Base Libraries\nimport matplotlib.pyplot as plt  \nimport pandas as pd\nimport numpy as np\nfrom numpy import set_printoptions\nfrom numpy import mean\n# Models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import StackingClassifier\n# Metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import plot_confusion_matrix","cbea9c5e":"creditcard = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv', sep=',')\n\ncreditcard.head(10)","658d1878":"array = creditcard.values\nX = array[:,0:30]\nY = array[:,30]\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, stratify=Y)","c4824751":"# feature extraction\nfi = ExtraTreesClassifier(n_estimators=10)\nfi.fit(X, Y)\nprint(fi.feature_importances_)","ab119959":"array = creditcard.values\nX = array[:, [17,14,10,12,11]]\nY = array[:,30]\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, stratify=Y)","3ceb59b3":"knn = KNeighborsClassifier(algorithm='auto')\nparams_knn = {\n    'n_neighbors': (1,30, 1),\n    'leaf_size': (20,40,1),\n    'p': (1,2),\n    'weights': ('uniform', 'distance'),\n    'metric': ('minkowski', 'chebyshev')}\nknn_gs = GridSearchCV(\n    estimator=knn,\n    param_grid=params_knn,\n    scoring = 'accuracy',\n    n_jobs = -1,\n    cv = 5)\n\n%time knn_gs.fit(X_train, y_train)\ny_pred = knn_gs.predict(X_test)","65f16405":"knn_best = knn_gs.best_estimator_\nprint(knn_gs.best_params_)\nfpr, tpr, _ = roc_curve(y_test, y_pred)\n\nknn_auc = auc(fpr, tpr)","4e159739":"rf = RandomForestClassifier()\nparams_rf = {\"n_estimators\": np.arange(1,30,1)}\nrf_gs = GridSearchCV(rf, params_rf, cv=5)\n\n%time rf_gs.fit(X_train, y_train)\ny_pred = rf_gs.predict(X_test)","67517bd0":"rf_best = rf_gs.best_estimator_\nprint(rf_gs.best_params_)\n\nfpr, tpr, _ = roc_curve(y_test, y_pred)\n\nrf_auc = auc(fpr, tpr) ","bf4d7721":"log_reg = LogisticRegression(max_iter=1000)\n%time log_reg.fit(X_train, y_train)\ny_pred = log_reg.predict(X_test)","00589b16":"fpr, tpr, _ = roc_curve(y_test, y_pred)\n\nlr_auc = auc(fpr, tpr)","7de09268":"estimators=[(\"knn\", knn_best), (\"rf\", rf_best), (\"log_reg\", log_reg)]","977cdf69":"ensemble_class = VotingClassifier(estimators, voting=\"hard\")\n\nensemble_class.fit(X_train, y_train)\n\nensemble_class.score(X_test, y_test)\ny_pred = ensemble_class.predict(X_test)","75e17834":"fpr, tpr, _ = roc_curve(y_test, y_pred)\n\nensemble_class_auc = auc(fpr, tpr) ","d25100f7":"ensemble_stack = StackingClassifier(estimators=estimators)\n\nensemble_stack.fit(X_train, y_train)\n\nensemble_stack.score(X_test, y_test)\ny_pred = ensemble_stack.predict(X_test)","82a57765":"fpr, tpr, _ = roc_curve(y_test, y_pred)\n\nensemble_stack_auc = auc(fpr, tpr)","9420c3a6":"print(\"KNN Score: {}\".format(knn_best.score(X_test, y_test)))\nprint(\"RF Score: {}\".format(rf_best.score(X_test, y_test)))\nprint(\"Log_Reg Score: {}\".format(log_reg.score(X_test, y_test)))\nprint(\"Ensemble Voting Score: {}\".format(ensemble_class.score(X_test, y_test)))\nprint(\"Ensemble Stacking Score: {}\".format(ensemble_stack.score(X_test, y_test)))","68ffbf7c":"print(\"KNN AUC: {}\".format(knn_auc))\nprint(\"RF AUC: {}\".format(rf_auc))\nprint(\"Log_Reg AUC: {}\".format(lr_auc))\nprint(\"Ensemble Voting AUC: {}\".format(ensemble_class_auc))\nprint(\"Ensemble Stacking AUC: {}\".format(ensemble_stack_auc))","6be9d05c":"fig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nmodels = ['KNN', 'LR', 'Log_Reg', 'Voting', 'Stacking']\nvalues = [knn_auc, rf_auc, lr_auc, ensemble_class_auc, ensemble_stack_auc]\nax.bar(models, values)\nplt.show()","76d1c134":"plot_confusion_matrix(knn_gs, X_test, y_test)  \nplt.show() ","47fa657a":"### Logistic Regression","4e5d8def":"### Voting Classifier","3117b25f":"Tree-based estimators can be used to compute impurity-based feature importances, which in turn can be used to discard irrelevant features (https:\/\/scikit-learn.org\/stable\/modules\/feature_selection.html). This technique will help find the best variables to be used in the models.","da871bab":"### K-Nearest Neighbor","06e0cdc2":"This is my first Kaggle Contribution, using a very well-rated dataset in the community: the **Credit Card Fraud Dataset**. Sometimes, use all variables in a classification can be time consuming, and create overfitting. The objective of this work is to find, through Feature Selection, the most significant variables for the recognition of credit card fraud patterns, and subsequently to test the efficiency of some Machine Learning models (using this variables) working both separately and together, through Ensemble Learning.","da03cc65":"Stacked generalization is a method for combining estimators to reduce their biases. More precisely, the predictions of each individual estimator are stacked together and used as input to a final estimator to compute the prediction. This final estimator is trained through cross-validation (https:\/\/scikit-learn.org\/stable\/modules\/ensemble.html). Here, we create our stacking classifier, inputing our models, fit on training and test our model in data.","9d7cb699":"Here, the best model is saved and the best parameters are shown, together with model's Area Under the Curve (AUC - https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.auc.html). ","f939abc4":"## Models","de718e09":"Here, the best model is saved and the best parameters are shown, together with model's Area Under the Curve (AUC - https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.auc.html). ","fb52f027":"This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019, \u2018saga\u2019 and \u2018lbfgs\u2019 solvers. Note that regularization is applied by default. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance (https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html). Here, we create a new logistic regression model and just fit the model to the training data","469ef989":"Classifier implementing the k-nearest neighbors vote (https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html). First, a new kNN model is created, then we create a dictionary of all values we want to test for n_neighbors. GridSearch is used to test all values for n_neighbors, and finally we fit the model to the training data.","106b53dd":"The idea behind the VotingClassifier is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses (https:\/\/scikit-learn.org\/stable\/modules\/ensemble.html). Here, we create our voting classifier, inputing our models, fit on training and test our model in data.","de829659":"## Feature Selection (Importance)","759ff92b":"A Random Forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control overfitting (https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html). Below, we create a new Random Forest Classifier, a dictionary of all values we want to test for n_estimators and we use gridsearch to test all values for n_estimators, finally fitting the model to training data.","1a9308d2":"\nAs we can see, the data set contains the time, 28 references regarding the card's consumption behavior, the amount spent and the binary classification (fraudulent or not). Below, the data is splitted between train and test sets (70\/30 classic distribution)","5b3a0a56":"When using AUC, it is possible to see that the objective was achieved: to find a percentage of precision using only the variables chosen by the Feature Selection (which possibly also decreased overfitting). Of all models, kNN had the best results (89.52%), while Logistic Regression was the one with the worst (77.69%)","bae9e265":"This plotted results present well the difference between the models.","150b1a18":"For this notebook, three classification models were tested: **K-Nearest Neighbor**, **Random Forest** and **Logistic Regression**, all using the sklearn library. Initially, X and y receive new values, based on the most representative variables found by Feature Selection.","9f286c48":"Here, the best model is saved and the best parameters are shown, together with model's Area Under the Curve (AUC - https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.auc.html). ","cf2d5274":"The confusion matrix shows that the classification of the model is quite efficient, with a high accuracy of true positives and negatives","1ef2adeb":"Here, the best model is saved and the best parameters are shown, together with model's Area Under the Curve (AUC - https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.auc.html). ","9107dd2f":" ## **Credit Card Fraud Classification with Features Selection and Ensemble Learning**","c199e422":"First, the libraries and the dataset for this Notebook are imported[](http:\/\/)","bfe9acdf":"### Random Forest","66482602":"## Model Metrics","d9c19564":"In this section, all models are compared and its results shown. Considering the score, all models are very similar, so we need more metrics to see its differences.","98383c2c":"The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability \/ robustness over a single estimator (https:\/\/scikit-learn.org\/stable\/modules\/ensemble.html). Below, we create a dictionary of our models","073c49f4":"Here, the best model is saved and the best parameters are shown, together with model's Area Under the Curve (AUC - https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.auc.html). ","462492cd":"## Ensemble (Voting\/Stacking Classifiers)","d4362526":"### Stacking Classifier"}}