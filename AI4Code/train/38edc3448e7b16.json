{"cell_type":{"54609a95":"code","aa16054a":"code","9375db27":"code","81c1026c":"code","79462aa4":"code","05264312":"code","bd7d1de7":"code","12f58fe5":"code","72e0597d":"code","d5a74af7":"code","e409358f":"code","aaa4c47a":"code","ac9b49a1":"code","54cd2908":"code","76f5f457":"code","17a87514":"code","e4f73217":"code","f1f20e20":"code","bc0b3f2e":"code","44b38979":"code","89c04a5d":"code","22db6018":"code","26e0a397":"code","35ead346":"code","2cc1f4c7":"code","22bd61ad":"markdown","a4a4e6ae":"markdown","077e8b53":"markdown","efde3776":"markdown","665d9ff7":"markdown","f16c198b":"markdown"},"source":{"54609a95":"# Import the libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report,accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\n# Read the dataset\ndata = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")","aa16054a":"data.info()","9375db27":"#Removing the time column since it would not help in model training\ndata.drop([\"Time\"],axis=1,inplace = True)","81c1026c":"# Lets check the missing values in case any\ndata.isnull().sum()","79462aa4":"# Correlation of variables with the target class.\ncorr = data.corr()\ncorr[\"Class\"]","05264312":"# Removing the variables which can affect the output stream\n#These are features either are too negatively correlated with the target or too much positive with target(may surpress other features)\nCOL=[\"V3\",\"V10\",\"V12\",\"V14\",\"V16\",\"V17\",\"V18\"]\ndata.drop(COL,axis=1,inplace=True)","bd7d1de7":"# Lets normalise the amount column\n# Need not perform scaling of other features since all values have been scaled because of the PCA applied before the data was released\nfrom sklearn.preprocessing import StandardScaler\ndata['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\ndata = data.drop(['Amount'],axis=1)","12f58fe5":"#Checking the ratio of classes with respect to each other \ndata[\"Class\"].value_counts()","72e0597d":"# Since our classes are highly skewed we should make them equivalent in order to have a normal distribution of the classes.\n\n# Lets shuffle the data before creating the subsamples\n\ndf = data.sample(frac=1)\n\n# NO of fraud instances :: 492 rows.\nfraud_df = df.loc[df['Class'] == 1]\nnon_fraud_df = df.loc[df['Class'] == 0][:492]\n\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n\n# Shuffle dataframe rows\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)\n\nnew_df.head()","d5a74af7":"# Lets split the data into features and target\nfeatures = new_df.drop([\"Class\"],axis =1)\ntarget = new_df[\"Class\"]\nfrom sklearn.model_selection import train_test_split\n\n# Splitting the data into train and test\nX_train,X_test, Y_train,Y_test = train_test_split(features, target, test_size = .3, random_state =5)","e409358f":"# Logistic Regression \n# Solver is liblinear since its supports both l1 and l2 penalities\nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\"solver\" :['liblinear']}\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg.fit(X_train, Y_train)\n# We automatically get the logistic regression with the best parameters.\n\nprint(grid_log_reg.best_estimator_)\n\nknears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\ngrid_knears.fit(X_train, Y_train)\n# KNears best estimator\nprint(grid_knears.best_estimator_)\n\n# Support Vector Classifier\nsvc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(X_train, Y_train)\n\n# SVC best estimator\n\nprint(grid_svc.best_estimator_)\n\n# DecisionTree Classifier\ntree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n              \"min_samples_leaf\": list(range(5,7,1))}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(X_train, Y_train)\n\n# tree best estimator\nprint(grid_tree.best_estimator_)","aaa4c47a":"\nk = KFold(n_splits = 5)\nclassifiers = {\n    'Logistic Regression': LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n                                               intercept_scaling=1, l1_ratio=None, max_iter=100,\n                                               multi_class='auto', n_jobs=None, penalty='l1',\n                                               random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                                               warm_start=False),\n               'Support Vector Machine' : SVC(C=0.9, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n                                                decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n                                                max_iter=-1, probability=False, random_state=None, shrinking=True,\n                                                tol=0.001, verbose=False),\n               'Random Forest Classifier': RandomForestClassifier(),\n               'Decision Tree Algorithm':DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n                                                                   max_depth=2, max_features=None, max_leaf_nodes=None,\n                                                                   min_impurity_decrease=0.0, min_impurity_split=None,\n                                                                   min_samples_leaf=5, min_samples_split=2,\n                                                                   min_weight_fraction_leaf=0.0, presort='deprecated',\n                                                                   random_state=None, splitter='best'),\n               'KNN':KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n                                             metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n                                             weights='uniform')\n              }\ndef model(m):\n    for i in m:\n        print(i)\n        print('-'*100)\n        m[i].fit(X_train,Y_train.values.ravel())\n        prediction = m[i].predict(X_test)\n        print('Classification Report')        \n        cr = classification_report(Y_test,prediction,output_dict=True)\n        print(pd.DataFrame(cr).transpose())\n        print('='*100)\n        accuracy = accuracy_score(Y_test.values.ravel(),prediction)\n        print('Accuracy Score :  ',accuracy)\n        print('='*100)\n        print('Cross Validation Score')\n        cv=cross_val_score(m[i],X_train,Y_train,cv=k,scoring='accuracy')\n        print(cv.mean())\n        print('-'*100)","ac9b49a1":"model(classifiers)","54cd2908":"from sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef plot_learning_curve(estimator1, estimator2, estimator3, estimator4, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(20,14), sharey=True)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    # First Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax1.set_title(\"Logistic Regression Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n    \n    # Second Estimator \n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax2.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax2.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax2.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax2.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax2.set_title(\"Knears Neighbors Learning Curve\", fontsize=14)\n    ax2.set_xlabel('Training size (m)')\n    ax2.set_ylabel('Score')\n    ax2.grid(True)\n    ax2.legend(loc=\"best\")\n    \n    # Third Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator3, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax3.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax3.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax3.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax3.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax3.set_title(\"Support Vector Classifier \\n Learning Curve\", fontsize=14)\n    ax3.set_xlabel('Training size (m)')\n    ax3.set_ylabel('Score')\n    ax3.grid(True)\n    ax3.legend(loc=\"best\")\n    \n    # Fourth Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator4, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax4.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax4.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax4.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax4.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax4.set_title(\"Decision Tree Classifier \\n Learning Curve\", fontsize=14)\n    ax4.set_xlabel('Training size (m)')\n    ax4.set_ylabel('Score')\n    ax4.grid(True)\n    ax4.legend(loc=\"best\")\n    return plt","76f5f457":"cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve(classifiers[\"Logistic Regression\"],classifiers[\"KNN\"], classifiers[\"Support Vector Machine\"], classifiers[\"Decision Tree Algorithm\"], X_train, Y_train, (0.87, 1.01), cv=cv, n_jobs=4)","17a87514":"from sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_predict\n# Create a DataFrame with all the scores and the classifiers names.\n\nlog_reg_pred = cross_val_predict(classifiers[\"Logistic Regression\"], X_train, Y_train, cv=5,\n                             method=\"decision_function\")\n\nknears_pred = cross_val_predict(classifiers[\"KNN\"], X_train, Y_train, cv=5)\n\nsvc_pred = cross_val_predict(classifiers[\"Support Vector Machine\"], X_train, Y_train, cv=5,\n                             method=\"decision_function\")\n\ntree_pred = cross_val_predict(classifiers[\"Decision Tree Algorithm\"], X_train, Y_train, cv=5)\nfrom sklearn.metrics import roc_auc_score\n\nprint('Logistic Regression: ', roc_auc_score(Y_train, log_reg_pred))\nprint('KNears Neighbors: ', roc_auc_score(Y_train, knears_pred))\nprint('Support Vector Classifier: ', roc_auc_score(Y_train, svc_pred))\nprint('Decision Tree Classifier: ', roc_auc_score(Y_train, tree_pred))","e4f73217":"import keras\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Activation\nfrom keras.layers.core import Dense\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy\n\n#Size of the input of the input layer\nn_inputs = X_train.shape[1]\n\nundersample_model = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n    Dense(32, activation='relu',kernel_regularizer=keras.regularizers.l2(0.01)),\n    Dense(2, activation='softmax') #2 neurons because we have 2 output classes \n])","f1f20e20":"undersample_model.summary()","bc0b3f2e":"keras.utils.plot_model(undersample_model)","44b38979":"#using sparse_categorial because there is a lot of sparse of values in the target so it is preferred to use this loss\n# using accuracy since it is classification problem \nundersample_model.compile(Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","89c04a5d":"#Spliting into train, Dev set and test set\n# Train = 60% Valid = 30% Test = 1% on original dataset \nX= data.drop([\"Class\"],axis = 1)\nY = data[\"Class\"]\norg_x_train_f,org_x_test,org_y_train_f,org_y_test = train_test_split(X,Y,test_size = .1,random_state = 1)\norg_x_train,org_x_valid,org_y_train,org_y_valid = train_test_split(org_x_train_f,org_y_train_f,test_size = .3,random_state = 1)","22db6018":"# Using random shuffling in the dataset while fitting\nhistory=undersample_model.fit(X_train, Y_train, batch_size=64, epochs=20, shuffle=True, verbose=2,validation_data = (org_x_valid,org_y_valid))","26e0a397":"#Lets check out the weights and biases in each layer\nfor i in range(3):\n    print('Layer',i+1)\n    print(\"-\"*10)\n    hidden_layer  = undersample_model.layers[i]\n    weights,biases = hidden_layer.get_weights()\n    print('Weights')\n    print(weights)\n    print(\"Weight shape\",weights.shape)\n    print(\"*\"*10)\n    print('Bias')\n    print(biases)\n    print(\"Bias shape\",biases.shape)\n    print(\"=\"*10)","35ead346":"\npd.DataFrame(history.history).plot(figsize=(10,10))\nplt.grid(True)\nplt.gca().set_ylim(0,1)\nplt.show()","2cc1f4c7":"# Evaluation on test test \nacc=undersample_model.evaluate(org_x_test,org_y_test)\nprint('Loss : {} ,Accuracy : {}'.format(acc[0],acc[1]*100))","22bd61ad":"<h1> ROC_AUC_SCORE <\/h1>","a4a4e6ae":"<h1> Lets Train Neural Networks <\/h1>","077e8b53":"<h1> GRID SEARCH CV <\/h1>\nLets apply the grid search in other to obtain our the best hyperparamter for our models \n","efde3776":"The results are quite promising since the loss is decreasing and overfitting doesn't seems to occur","665d9ff7":"<h1>LETS Train those models<\/h1>\nWe would be deploying cross validation score in order to get a much better result","f16c198b":"#After all the models are trained we see that we are able to obtain a perfect accuracy of 94.9 % on artifical neural networks.\nLoss is also reported to be less. \n#This is a good report that we have obtained over all the project.\n#It would be a preferred model to be deployed. "}}