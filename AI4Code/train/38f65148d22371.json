{"cell_type":{"bcd29060":"code","a7b05d16":"code","23c4dd94":"code","91ca4dcf":"code","5f92e77f":"code","80c33fd8":"code","773ab5e1":"code","a0b6ddfa":"code","5211a596":"code","54b9a89b":"code","767b5f90":"code","db2e8e54":"code","192df0d2":"code","74932eed":"code","fc85221d":"code","2ceb900f":"code","88a84121":"code","95e56bdb":"code","e9c3002c":"code","ab1d2615":"code","60edd6ca":"code","c5638d4a":"code","e8c735a5":"code","fb32d872":"code","67f996f2":"code","4ef83643":"code","76c8e6f5":"code","453152a7":"code","32ddb9de":"code","f3544351":"code","d6e959e1":"code","b233e195":"code","45cf3aab":"code","c7bdee13":"code","940a3595":"code","ebd83741":"code","f564cf8b":"code","d7ba61c2":"code","c3f8789e":"code","63a4e3c3":"code","b039d9da":"code","2656a45f":"code","7cc3ea0e":"code","21d474e5":"code","3d5f5706":"code","87aea015":"code","623e2934":"code","30eead26":"code","d69cb290":"code","3d61e5bf":"code","e0c8ae51":"code","605feb58":"code","dec8c00f":"code","68e1c163":"code","3565a2b9":"code","8e3fbd2f":"code","6de5e20e":"code","d72c3325":"code","621bdeb6":"code","d0980214":"code","b3ad7795":"code","f0bc18b5":"code","88ff6483":"code","e005bf75":"code","3338efdc":"code","122c630c":"code","f21a61dc":"code","4f0363f8":"code","4dfcc766":"code","419888da":"code","641bac1e":"code","6e00dbe9":"code","1aefe27c":"code","e1d82175":"code","cce39e43":"code","3fe857b5":"code","9950531f":"code","64dd9e5a":"code","b471d39e":"code","a1c08890":"code","47caebde":"code","a95235eb":"code","1631216f":"code","1cab467c":"code","d9243e7d":"code","890808fa":"code","579bc38b":"code","c016bba6":"code","b80e36bc":"code","33998dc6":"code","7033d5ef":"code","7f68868b":"markdown","82f210e0":"markdown","f147de09":"markdown","d373ff3a":"markdown","73ef8369":"markdown","4be13752":"markdown","6c01bc31":"markdown","b5f560f3":"markdown","c75d1de5":"markdown","c4cc3e28":"markdown","312e6d9e":"markdown","88d018ff":"markdown","d13a0431":"markdown","980c5301":"markdown","efdd7d41":"markdown","613d098c":"markdown","c0567bf1":"markdown","0dcf5fda":"markdown","189178c6":"markdown","5ca39466":"markdown","7a74172d":"markdown","ae02e038":"markdown","ad2b73f7":"markdown","785188e2":"markdown","dcb6c1fa":"markdown","b08a4caa":"markdown","a879d062":"markdown","8214dcb8":"markdown","1f8380fd":"markdown","4f54379e":"markdown","2d3621ac":"markdown","e013a0b3":"markdown","93e140d6":"markdown","c5935fef":"markdown","7389cca2":"markdown","e4b0d44a":"markdown"},"source":{"bcd29060":"import re\nimport nltk\nimport pandas\nimport matplotlib\nimport numpy as np\nfrom collections import OrderedDict, Counter\nfrom matplotlib import pyplot as plt\nmatplotlib.style.use('ggplot')\nfrom wordcloud import WordCloud, ImageColorGenerator\nfrom PIL import Image","a7b05d16":"df = pandas.read_csv('..\/input\/raw-data\/google_jobs.csv')\ndf.head(10)","23c4dd94":"def count_keywords_freq(df: pandas.DataFrame, col_name: str, keywords: list, none=False):\n    \"\"\" Given a list of keywords and count their frequency in the specified pandas dataframe.\n    :param df: target pandas dataframe.\n    :param col_name: target column name.\n    :param keywords: a list of keywords.\n    :param none: count the items that contain no specified keywords at all.\n    :return: keyword frequency dict.\n    \"\"\"\n    freq = {keyword: 0 for keyword in keywords}\\\n    \n    for keyword in keywords:\n        freq[keyword] = df[col_name].str.contains(keyword, regex=False).sum()\n    \n    if none is True:\n        freq['None'] = 0\n        for col in df[col_name]:\n            freq['None'] += 0 if type(col) is str and any(w in col for w in keywords) else 1\n    \n    return freq","91ca4dcf":"keywords = ['PhD', 'Master', 'MBA', 'BA', 'BS', 'Bachelor']\n\n# Count keyword frequency.\nmin_degree_reqs = count_keywords_freq(df, 'minimum_qual', keywords, none=True)\npref_degree_reqs = count_keywords_freq(df, 'preferred_qual', keywords, none=False)\n\nprint(\"Min: \" + str(min_degree_reqs))\nprint(\"Pref: \" + str(pref_degree_reqs))","5f92e77f":"# Convert the above dicts into pandas DataFrames.\nmin_degree_df = pandas.DataFrame.from_dict(min_degree_reqs, orient='index', columns=['Count'])\npref_degree_df = pandas.DataFrame.from_dict(pref_degree_reqs, orient='index', columns=['Count'])\n\nmin_degree_df","80c33fd8":"# Define bar colors\ncolors = ['#958090', '#83A2BE', '#7DAEA9', '#B4BF86', '#CBB079', '#B77A76', '#707070', '#AAAAAA']\nmin_labels = list(min_degree_reqs.keys())\nmin_values = list(min_degree_reqs.values())\npref_labels = list(pref_degree_reqs.keys())\npref_values = list(pref_degree_reqs.values())\n\nplt.figure(figsize=(15, 5))\nplt.subplot(121)\nplt.bar(min_labels, min_values, color=colors, width=0.5)\nplt.xlabel('Degree')\nplt.ylabel('Number of jobs')\nplt.title('Minimum Degree Requirements')\n\nplt.subplot(122)\nplt.bar(pref_labels, pref_values, color=colors, width=0.5)\nplt.xlabel('Degree')\nplt.ylabel('Number of jobs')\nplt.title('Preferred Degree Requirements')\n\nplt.show()","773ab5e1":"def extract_experience(df: pandas.DataFrame, col_name: str, end_year=20):\n    \"\"\" Extract years of experiences required.\n    :param df: target dataframe.\n    :param col_name: name of the column that contains strings\n                     like `4 years of experience in ...`\n    :param end_year: the last year in the list returned.\n    :return: a list of years of exp required (indexed by years)\n    \"\"\"\n    exp_list = [0] * (end_year + 1)\n    \n    for col in df[col_name]:\n        if type(col) is not str:\n            continue\n        exp_required = re.findall('\\d+ year', col)\n        exp_required = exp_required + re.findall('\\d+\\+ year', col)\n        year = 0 if not exp_required else int(exp_required[0].replace(' year', '').replace('+', ''))\n        exp_list[year] += 1\n        \n    return exp_list","a0b6ddfa":"min_exp_list = extract_experience(df, 'minimum_qual')\npref_exp_list = extract_experience(df, 'preferred_qual')","5211a596":"colors = ['#958090', '#83A2BE', '#7DAEA9', '#B4BF86', '#CBB079', '#B77A76', '#707070', '#AAAAAA']\nlabels = np.arange(len(min_exp_list))\n\nplt.figure(figsize=(16, 5))\nplt.subplot(121)\nplt.bar(np.arange(11), min_exp_list[0:11], color=colors, width=0.5)\nplt.xticks(labels[0:11])\nplt.xlabel('Year(s)')\nplt.ylabel('Number of jobs')\nplt.title('Minimum Experience Requirements')\n\nplt.subplot(122)\nplt.bar(labels[0:11], pref_exp_list[0:11], color=colors, width=0.5)\nplt.xticks(labels[0:11])\nplt.xlabel('Year(s)')\nplt.ylabel('Number of jobs')\nplt.title('Preferred Experience Requirements')\n\nplt.show()","54b9a89b":"lang_colors = {\n    'C++': '#F34B7D',\n    'Java': '#B07219',\n    'Python': '#3572A5',\n    'JavaScript': '#F1E05A',\n    'Go': '#375EAB',\n    'PHP': '#4F5D95',\n    'SQL': '#494D5C',\n    'Ruby': '#701516',\n    'Swift': '#FFAC45',\n    'Kotlin': '#F18E33',\n    'C#': '#178600',\n    'Objective C': '#438EFF'\n}\n\nlangs = lang_colors.keys()","767b5f90":"# Count keyword frequency.\nmin_lang_reqs = count_keywords_freq(df, 'minimum_qual', langs)\npref_lang_reqs = count_keywords_freq(df, 'preferred_qual', langs)\n\n# Some manual correction.\nmin_lang_reqs['Java'] -= min_lang_reqs['JavaScript']\npref_lang_reqs['Java'] -= pref_lang_reqs['JavaScript']\nmin_lang_reqs['Go'] -= df['minimum_qual'].str.contains('Google').sum()\npref_lang_reqs['Go'] -= df['preferred_qual'].str.contains('Google').sum()\n\n# Sort the dicts.\nmin_lang_reqs = dict(sorted(min_lang_reqs.items(), key=lambda kv: kv[1], reverse=True))\npref_lang_reqs = dict(sorted(pref_lang_reqs.items(), key=lambda kv: kv[1], reverse=True))\n\n# Create DataFrame from dict.\nmin_lang_df = pandas.DataFrame.from_dict(min_lang_reqs, orient='index', columns=['Count'])\npref_lang_df = pandas.DataFrame.from_dict(pref_lang_reqs, orient='index', columns=['Count'])\n\npref_lang_reqs","db2e8e54":"min_labels = list(min_lang_reqs.keys())\nmin_values = list(min_lang_reqs.values())\nmin_colors = [lang_colors[k] for k, v in min_lang_reqs.items()]\n\npref_labels = list(pref_lang_reqs.keys())\npref_values = list(pref_lang_reqs.values())\npref_colors = [lang_colors[k] for k, v in pref_lang_reqs.items()]\n\n\nplt.figure(figsize=(21, 5))\nplt.subplot(121)\nplt.bar(min_labels, min_values, color=min_colors, width=0.5)\nplt.title('Top 10 Programming Languages in Minimum Quals')\n\nplt.subplot(122)\nplt.bar(pref_labels, pref_values, color=pref_colors, width=0.5)\nplt.title('Top 10 Programming Languages in Preferred Quals')\n\nplt.show()","192df0d2":"df[df['preferred_qual'].str.contains('Python')]['preferred_qual'].tolist()[0:3]","74932eed":"def import_terms(tokenizer: nltk.tokenize.MWETokenizer, term_file_path: str):\n    \"\"\" Import all user-defined untokenizable terms from a file into nltk MWETokenizer.\n    :param tokenizer: nltk MWETokenizer instance.\n    :param text_file_path: path to the file.\n    \"\"\"\n    with open(term_file_path, 'r') as f:\n        for line in f:\n            tokenizer.add_mwe(line.strip().split())","fc85221d":"def tokenize(tokenizer: nltk.tokenize.MWETokenizer, s: str, lowercase=True, preserve_case_words=[]):\n    \"\"\" Tokenize given string using nltk MWETokenizer.\n    :param case: convert all tokens into lowercase.\n    :param exclude_words: words that should preserve their cases.\n    :return: a list of tokens.\n    \"\"\"\n    tokens = nltk.tokenize.word_tokenize(s)\n    tokens = tokenizer.tokenize(tokens)\n    \n    # Remove tokens that are either purely digits or purely punctuations.\n    tokens = list(filter(lambda token: not token.isdigit() and re.search('[a-zA-Z]', token), tokens))\n\n    # Since nltk MWETokenizer will not split tokens that contain a slash,\n    # we'll have to do it ourselves.\n    for token in tokens:\n        if '\/' in token:\n            tokens += token.split('\/')\n            tokens.remove(token)\n            \n    # Lowercase conversion.\n    tokens = [token.lower() if token not in preserve_case_words else token for token in tokens]\n    return tokens","2ceb900f":"def create_word_freq_dict(tokenizer, df, col_name, lowercase=True, preserve_case_words=[]):\n    \"\"\" Create a word frequency dict\n    :param tokenizer: nltk MWETokenizer.\n    :param df: source pandas dataframe.\n    :param col_name: name of the column to create wfm from.\n    :param lowercase: convert all tokens into lowercase.\n    :param preserve_case_words: words that should preserve their cases.\n    :return: a word frequency dict (dict of dict, separated by job indices).\n    \"\"\"\n    freq = {}\n    \n    for i, col in enumerate(df[col_name]):\n        if type(col) is str:\n            freq[i] = {}\n            words = tokenize(tokenizer, col, lowercase=True, preserve_case_words=preserve_case_words)\n            for word in words:\n                if word in freq:\n                    freq[i][word] += 1\n                else:\n                    freq[i][word] = 1\n                \n    return freq","88a84121":"def create_wfm(word_frequency_dict: dict):\n    \"\"\" Create word frequency matrix from the specified word frequency dict \"\"\"\n    dwf_list = [pandas.DataFrame(list(freq.values()), index=freq.keys()) for freq in word_frequency_dict.values()]\n    wfm = pandas.concat(dwf_list, axis=1, sort=True)\n    wfm = np.transpose(wfm).fillna(0)\n    wfm.index = word_frequency_dict.keys()\n    return wfm","95e56bdb":"# Initialize nltk MWETokenizer.\ntokenizer = nltk.tokenize.MWETokenizer(separator=' ')\nimport_terms(tokenizer, '..\/input\/cs_terms_and_stopwords\/cs_terms.txt')\n\n# Words that needs to preserve case.\npreserve_case_words = list(langs) + ['.Net', '.NET']\nmin_qual_wfd = create_word_freq_dict(tokenizer, df, 'minimum_qual', True, preserve_case_words)\npref_qual_wfd = create_word_freq_dict(tokenizer, df, 'preferred_qual', True, preserve_case_words)\nresp_qual_wfd = create_word_freq_dict(tokenizer, df, 'responsibilities', True, preserve_case_words)\n\nlist(min_qual_wfd[0].items())[0:10]","e9c3002c":"min_qual_wfm = create_wfm(min_qual_wfd)\npref_qual_wfm = create_wfm(pref_qual_wfd)\nresp_wfm = create_wfm(resp_qual_wfd)\n\n# Row: job, Column: word frequency\npref_qual_wfm.head(5)","ab1d2615":"def create_tfm(wfm):\n    tfm = wfm.copy()\n    for i in range(0, len(tfm)):\n        tfm.iloc[i] = tfm.iloc[i] \/ tfm.iloc[i].sum()\n    return tfm","60edd6ca":"min_qual_tfm = create_tfm(min_qual_wfm)\npref_qual_tfm = create_tfm(pref_qual_wfm)\nresp_tfm = create_tfm(resp_wfm)\n\nmin_qual_tfm.head()","c5638d4a":"min_qual_df = (min_qual_wfm > 0).sum()\npref_qual_df = (pref_qual_wfm > 0).sum()\nresp_df = (resp_wfm > 0).sum()","e8c735a5":"N = len(df)\nN","fb32d872":"def create_tfidfm(tfm, N, df):\n    tfidfm = tfm.copy()\n    for i in range(0, len(tfidfm)):\n        # Add 0.01 so that those extremely frequent words won't be completely neglected.\n        tfidfm.iloc[i] = tfidfm.iloc[i] * np.log10(N \/ df) + 0.01\n    return tfidfm","67f996f2":"min_qual_tfidfm = create_tfidfm(min_qual_tfm, N, min_qual_df)\npref_qual_tfidfm = create_tfidfm(pref_qual_tfm, N, pref_qual_df)\nresp_tfidfm = create_tfidfm(resp_tfm, N, resp_df)","4ef83643":"mask = np.array(Image.open('..\/input\/wordcloud-mask\/google-icon.png'))\nplt.figure(figsize=(18,6))\n\nplt.subplot(131)\ntfidf_dict = min_qual_tfidfm.to_dict(orient='records')\nwordcloud = WordCloud(mask=mask, background_color=\"white\").fit_words(tfidf_dict[0])\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.margins(x=0, y=0)\nplt.title('Minimum qual',size=24)\n\nplt.subplot(132)\ntfidf_dict = pref_qual_tfidfm.to_dict(orient='records')\nwordcloud = WordCloud(mask=mask, background_color=\"white\").fit_words(tfidf_dict[0])\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.margins(x=0, y=0)\nplt.title('Preferred qual',size=24)\n\nplt.subplot(133)\ntfidf_dict = resp_tfidfm.to_dict(orient='records')\nwordcloud = WordCloud(mask=mask, background_color=\"white\").fit_words(tfidf_dict[0])\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.margins(x=0, y=0)\nplt.title('Responsibilities',size=24)\n\nplt.show()","76c8e6f5":"def create_wordclouds(dataframe, mask_img_path, *col_names):\n    \"\"\" Create wordclouds from the given column names \"\"\"\n    mask = np.array(Image.open(mask_img_path))\n    plt.figure(figsize=(18,6))\n    \n    # Initialize nltk MWETokenizer.\n    tokenizer = nltk.tokenize.MWETokenizer(separator=' ')\n    import_terms(tokenizer, '..\/input\/cs_terms_and_stopwords\/cs_terms.txt')\n    \n    # Words that needs to preserve case.\n    preserve_case_words = list(langs) + ['.Net', '.NET']\n    \n    for i, col_name in enumerate(col_names, 1):\n        wfd = create_word_freq_dict(tokenizer, dataframe, col_name, True, preserve_case_words)\n        wfm = create_wfm(wfd)\n        tfm = create_tfm(wfm)\n        df = (wfm > 0).sum()\n        N = len(dataframe)\n        tfidfm = create_tfidfm(tfm, N, df)\n        \n        plt.subplot(1, len(col_names), i)\n        tfidf_dict = tfidfm.to_dict(orient='records')\n        wordcloud = WordCloud(mask=mask, background_color=\"white\").fit_words(tfidf_dict[0])\n        plt.imshow(wordcloud, interpolation=\"bilinear\")\n        plt.axis(\"off\")\n        plt.margins(x=0, y=0)\n        plt.title(col_name, size=24)\n\n    plt.show()\n    \n# Usage: create_wordclouds(df, '..\/input\/wordcloud-mask\/google-icon.png', 'minimum_qual', 'preferred_qual', 'responsibilities')","453152a7":"keyword_df = df.copy()\n\n# Replace NaN columns with empty strings.\nkeyword_df = keyword_df.replace(np.nan, '', regex=True)\n\n# Split job titles by comma into separate columns.\nkeyword_df['title'] = keyword_df['title'].str.split(',', expand=True)[0]\n\n# Combine minimum_qual, preferred_qual and responsibilities into a single column.\nkeyword_df['text'] = list(keyword_df['minimum_qual'] + keyword_df['preferred_qual'] + keyword_df['responsibilities'])\n\n# Drop unused columns.\nkeyword_df = keyword_df.drop(['location', 'minimum_qual', 'preferred_qual', 'responsibilities'], axis=1)\nkeyword_df.head()","32ddb9de":"def get_keywords(tokens, num, stopwords=[]):\n    if len(stopwords) > 0:\n        for stopword in stopwords:\n            for token in tokens:\n                if token == stopword:\n                    tokens.remove(token)\n    return [t[0] for t in Counter(tokens).most_common(num)]","f3544351":"stopwords = []\nwith open('..\/input\/cs_terms_and_stopwords\/stopwords.txt', 'r') as f:\n    for line in f:\n        if not line.startswith('#'):\n            stopwords.append(line.strip())","d6e959e1":"# Clean up the text.\ntokenizer = nltk.tokenize.MWETokenizer(separator=' ')\nimport_terms(tokenizer, '..\/input\/cs_terms_and_stopwords\/cs_terms.txt')\nkeyword_df['text'] = keyword_df['text'].apply(lambda s: ' '.join(tokenize(tokenizer, s, lowercase=True, preserve_case_words=['Go', '.NET'])))\n\n# Create keyword columns.\nkeyword_df['keyword'] = list(','.join(get_keywords(tokenize(tokenizer, col, lowercase=True, preserve_case_words=['Go']), 3, stopwords)) for col in keyword_df['text'])\nkeyword_df.head(10)","b233e195":"keywords_array=[]\nfor index, row in keyword_df.iterrows():\n    keywords = row['keyword'].split(',')\n    for kw in keywords:\n        keywords_array.append((kw.strip(' '), row['keyword']))\n\nkw_df = pandas.DataFrame(keywords_array).rename(columns={0:'keyword', 1:'keywords'})\nkw_df.head(10)","45cf3aab":"document = kw_df.keywords.tolist()\nnames = kw_df.keyword.tolist()\n\ndocument_array = []\nfor item in document:\n    items = item.split(',')\n    document_array.append((items))\n\noccurrences = OrderedDict((name, OrderedDict((name, 0) for name in names)) for name in names)\n\n# Find the co-occurrences:\nfor l in document_array:\n    for i in range(len(l)):\n        for item in l[:i] + l[i + 1:]:\n            occurrences[l[i]][item] += 1\n\nco_occur = pandas.DataFrame.from_dict(occurrences)\nco_occur.head(10)","c7bdee13":"# Write to CSV and import it into Gephi as spreadsheet.\n#co_occur.to_csv('keyword_co_occur_google.csv')","940a3595":"def create_term_term_matrix(dataframe):\n    keyword_df = dataframe.copy()\n\n    # Replace NaN columns with empty strings.\n    keyword_df = keyword_df.replace(np.nan, '', regex=True)\n\n    # Split job titles by comma into separate columns.\n    keyword_df['title'] = keyword_df['title'].str.split(',', expand=True)[0]\n\n    # Combine minimum_qual, preferred_qual and responsibilities into a single column.\n    keyword_df['text'] = list(keyword_df['minimum_qual'] + keyword_df['preferred_qual'] + keyword_df['responsibilities'])\n\n    # Drop unused columns.\n    keyword_df = keyword_df.drop(['location', 'minimum_qual', 'preferred_qual', 'responsibilities'], axis=1)\n    keyword_df.head()\n    \n    stopwords = []\n    with open('..\/input\/cs_terms_and_stopwords\/stopwords.txt', 'r') as f:\n        for line in f:\n            if not line.startswith('#'):\n                stopwords.append(line.strip())\n                \n    # Clean up the text.\n    tokenizer = nltk.tokenize.MWETokenizer(separator=' ')\n    import_terms(tokenizer, '..\/input\/cs_terms_and_stopwords\/cs_terms.txt')\n    keyword_df['text'] = keyword_df['text'].apply(lambda s: ' '.join(tokenize(tokenizer, s, lowercase=True, preserve_case_words=['Go', '.NET'])))\n    \n    # Create keyword columns.\n    keyword_df['keyword'] = list(','.join(get_keywords(tokenize(tokenizer, col, lowercase=True, preserve_case_words=['Go']), 3, stopwords)) for col in keyword_df['text'])\n    \n    keywords_array=[]\n    for index, row in keyword_df.iterrows():\n        keywords = row['keyword'].split(',')\n        for kw in keywords:\n            keywords_array.append((kw.strip(' '), row['keyword']))\n\n    kw_df = pandas.DataFrame(keywords_array).rename(columns={0:'keyword', 1:'keywords'})\n    \n    document = kw_df.keywords.tolist()\n    names = kw_df.keyword.tolist()\n\n    document_array = []\n    for item in document:\n        items = item.split(',')\n        document_array.append((items))\n\n    occurrences = OrderedDict((name, OrderedDict((name, 0) for name in names)) for name in names)\n\n    # Find the co-occurrences:\n    for l in document_array:\n        for i in range(len(l)):\n            for item in l[:i] + l[i + 1:]:\n                occurrences[l[i]][item] += 1\n\n    return pandas.DataFrame.from_dict(occurrences)","ebd83741":"df = pandas.read_csv('..\/input\/raw-data\/facebook_jobs.csv')\ndf.head(10)","f564cf8b":"keywords = ['PhD', 'Master', 'MBA', 'BA', 'BS', 'Bachelor']\n\n# Count keyword frequency.\nmin_degree_reqs = count_keywords_freq(df, 'minimum_qual', keywords, none=True)\npref_degree_reqs = count_keywords_freq(df, 'preferred_qual', keywords, none=False)\n\nprint(\"Minimum: \" + str(min_degree_reqs))\nprint(\"Preferred: \" + str(pref_degree_reqs))","d7ba61c2":"min_degree_df = pandas.DataFrame.from_dict(min_degree_reqs, orient='index', columns=['Count'])\npref_degree_df = pandas.DataFrame.from_dict(pref_degree_reqs, orient='index', columns=['Count'])\n\nmin_degree_df","c3f8789e":"colors = ['#958090', '#83A2BE', '#7DAEA9', '#B4BF86', '#CBB079', '#B77A76', '#707070', '#AAAAAA']\n\nmin_labels = list(min_degree_reqs.keys())\nmin_values = list(min_degree_reqs.values())\n\npref_labels = list(pref_degree_reqs.keys())\npref_values = list(pref_degree_reqs.values())\n\n\nplt.figure(figsize=(15, 5))\n\nplt.subplot(121)\nplt.bar(min_labels, min_values, color=colors, width=0.5)\nplt.xlabel('Degree')\nplt.ylabel('Number of jobs')\nplt.title('Minimum Degree Requirements')\n\nplt.subplot(122)\nplt.bar(pref_labels, pref_values, color=colors, width=0.5)\nplt.xlabel('Degree')\nplt.ylabel('Number of jobs')\nplt.title('Preferred Degree Requirements')\n\nplt.show()","63a4e3c3":"# Initialize a list with 0 from index 0 to 20.\nmin_exp_list = extract_experience(df, 'minimum_qual')\npref_exp_list = extract_experience(df, 'preferred_qual')","b039d9da":"colors = ['#958090', '#83A2BE', '#7DAEA9', '#B4BF86', '#CBB079', '#B77A76', '#707070', '#AAAAAA']\nlabels = np.arange(len(min_exp_list))\n\nplt.figure(figsize=(16, 5))\n\nplt.subplot(121)\nplt.bar(np.arange(11), min_exp_list[0:11], color=colors, width=0.5)\nplt.xticks(labels[0:11])\nplt.xlabel('Year(s)')\nplt.ylabel('Number of jobs')\nplt.title('Minimum Experience Requirements')\n\nplt.subplot(122)\nplt.bar(labels[0:11], pref_exp_list[0:11], color=colors, width=0.5)\nplt.xticks(labels[0:11])\nplt.xlabel('Year(s)')\nplt.ylabel('Number of jobs')\nplt.title('Preferred Experience Requirements')\n\nplt.show()","2656a45f":"print(df[df['title'] == 'Software Engineer, Linux Userspace'].iloc[0].minimum_qual)","7cc3ea0e":"print(df[df['title'] == 'Software Engineer, Linux Userspace'].iloc[0].preferred_qual)","21d474e5":"print(df[df['title'] == 'Software Engineer, Linux Userspace'].iloc[0].responsibilities)","3d5f5706":"# Count keyword frequency.\nmin_lang_reqs = count_keywords_freq(df, 'minimum_qual', langs)\npref_lang_reqs = count_keywords_freq(df, 'preferred_qual', langs)\n\n# Some manual correction.\nmin_lang_reqs['Java'] -= min_lang_reqs['JavaScript']\npref_lang_reqs['Java'] -= pref_lang_reqs['JavaScript']\n\n# Sort the dicts.\nmin_lang_reqs = dict(sorted(min_lang_reqs.items(), key=lambda kv: kv[1], reverse=True))\npref_lang_reqs = dict(sorted(pref_lang_reqs.items(), key=lambda kv: kv[1], reverse=True))\n\n# Create DataFrame from dict.\nmin_lang_df = pandas.DataFrame.from_dict(min_lang_reqs, orient='index', columns=['Count'])\npref_lang_df = pandas.DataFrame.from_dict(pref_lang_reqs, orient='index', columns=['Count'])\n\nmin_lang_reqs","87aea015":"min_labels = list(min_lang_reqs.keys())\nmin_values = list(min_lang_reqs.values())\nmin_colors = [lang_colors[k] for k, v in min_lang_reqs.items()]\n\npref_labels = list(pref_lang_reqs.keys())\npref_values = list(pref_lang_reqs.values())\npref_colors = [lang_colors[k] for k, v in pref_lang_reqs.items()]\n\n\nplt.figure(figsize=(21, 5))\n\nplt.subplot(121)\nplt.bar(min_labels, min_values, color=min_colors, width=0.5)\nplt.title('Top 10 Programming Languages in Minimum Quals')\n\nplt.subplot(122)\nplt.bar(pref_labels, pref_values, color=pref_colors, width=0.5)\nplt.title('Top 10 Programming Languages in Preferred Quals')\n\nplt.show()","623e2934":"create_wordclouds(df, '..\/input\/wordcloud-mask\/fb-icon.png', 'minimum_qual', 'preferred_qual', 'responsibilities')","30eead26":"co_occur = create_term_term_matrix(df)\n#co_occur.to_csv('keyword_co_occur_facebook.csv')","d69cb290":"df = pandas.read_csv('..\/input\/raw-data\/apple_jobs.csv')\ndf.head(10)","3d61e5bf":"keywords = ['PhD', 'Master', 'MBA', 'BA', 'BS', 'Bachelor']\n\n# Count keyword frequency.\ndegree_reqs = count_keywords_freq(df, 'education&experience', keywords, none=True)\n\nprint(\"Education & Experience: \" + str(degree_reqs))","e0c8ae51":"degree_df = pandas.DataFrame.from_dict(degree_reqs, orient='index', columns=['Count'])\ndegree_df","605feb58":"colors = ['#958090', '#83A2BE', '#7DAEA9', '#B4BF86', '#CBB079', '#B77A76', '#707070', '#AAAAAA']\n\ndegree_labels = list(degree_reqs.keys())\ndegree_values = list(degree_reqs.values())\n\nplt.figure(figsize=(15, 5))\nplt.bar(degree_labels, degree_values, color=colors, width=0.5)\nplt.xlabel('Degree')\nplt.ylabel('Number of jobs')\nplt.title('Degree Requirements')\nplt.show()","dec8c00f":"min_exp_list = extract_experience(df, 'minimum_qual', end_year=25)","68e1c163":"colors = ['#958090', '#83A2BE', '#7DAEA9', '#B4BF86', '#CBB079', '#B77A76', '#707070', '#AAAAAA']\nlabels = np.arange(len(min_exp_list))\n\nplt.figure(figsize=(16, 5))\nplt.subplot(1, 2, 1) # Drawing the 1st subplot.\nplt.bar(np.arange(26), min_exp_list[0:26], color=colors, width=0.5)\nplt.xticks(labels[0:26])\nplt.xlabel('Year(s)')\nplt.ylabel('Number of jobs')\nplt.title('Experience Requirements')\nfor i, v in enumerate(min_exp_list[0:26]):\n    plt.text(i - 0.3, v+25, str(v), color='black')\nplt.show()","3565a2b9":"print(df[df['title'] == 'Software Engineer'].iloc[11].minimum_qual)","8e3fbd2f":"print(df[df['title'] == 'Software Engineer'].iloc[11].responsibilities)","6de5e20e":"print(df[df['title'] == 'Software Engineer'].iloc[11]['education&experience'])","d72c3325":"# Count keyword frequency.\nmin_lang_reqs = count_keywords_freq(df, 'minimum_qual', langs)\npref_lang_reqs = count_keywords_freq(df, 'preferred_qual', langs)\n\n# Sort the dicts.\nmin_lang_reqs = dict(sorted(min_lang_reqs.items(), key=lambda kv: kv[1], reverse=True))\npref_lang_reqs = dict(sorted(pref_lang_reqs.items(), key=lambda kv: kv[1], reverse=True))\n\n# Create DataFrame from dict.\nmin_lang_df = pandas.DataFrame.from_dict(min_lang_reqs, orient='index', columns=['Count'])\npref_lang_df = pandas.DataFrame.from_dict(pref_lang_reqs, orient='index', columns=['Count'])\n\nprint(min_lang_reqs)\nprint(pref_lang_reqs)","621bdeb6":"min_labels = list(min_lang_reqs.keys())\nmin_values = list(min_lang_reqs.values())\nmin_colors = [lang_colors[k] for k, v in min_lang_reqs.items()]\n\npref_labels = list(pref_lang_reqs.keys())\npref_values = list(pref_lang_reqs.values())\npref_colors = [lang_colors[k] for k, v in pref_lang_reqs.items()]\n\n\nplt.figure(figsize=(21, 5))\n\nplt.subplot(121)\nplt.bar(min_labels, min_values, color=min_colors, width=0.5)\nplt.title('Top 10 Programming Languages in Minimum Quals')\n\nplt.subplot(122)\nplt.bar(pref_labels, pref_values, color=pref_colors, width=0.5)\nplt.title('Top 10 Programming Languages in Preferred Quals')\n\nplt.show()","d0980214":"create_wordclouds(df, '..\/input\/wordcloud-mask\/apple-icon.png', 'minimum_qual', 'preferred_qual', 'responsibilities')","b3ad7795":"co_occur = create_term_term_matrix(df)\n#co_occur.to_csv('keyword_co_occur_apple.csv')","f0bc18b5":"df_apple = pandas.read_csv('..\/input\/raw-data\/apple_jobs.csv')\ndf_google= pandas.read_csv('..\/input\/raw-data\/google_jobs.csv')\ndf_facebook= pandas.read_csv('..\/input\/raw-data\/facebook_jobs.csv')","88ff6483":"loc_apple = []\nloc_google = []\nloc_facebook = []\n\nfor a in df_apple['location']:\n    loc_apple.append(a)\nfor b in df_google['location']:\n    loc_google.append(b)\nfor c in df_facebook['location']:\n    loc_facebook.append(c)\n    \nprint(loc_google[0:5])","e005bf75":"import pickle\n\nloc_google_dic = {}\nnum_google_dic = {}\nloc_facebook_dic = {}\nnum_facebook_dic = {}\nloc_apple_dic = {}\nnum_apple_dic = {}\n\nwith open('..\/input\/map-data\/map_locations_data.pkl', 'rb') as f:\n    data = pickle.load(f)\n    loc_google_dic = data['loc_google_dic']\n    num_google_dic = data['num_google_dic']\n    loc_facebook_dic = data['loc_facebook_dic']\n    num_facebook_dic = data['num_facebook_dic']\n    loc_apple_dic = data['loc_apple_dic']\n    num_apple_dic = data['num_apple_dic']","3338efdc":"def merge_two_dicts(x, y):\n    \"\"\" Given two dicts, merge them into a new dict as a shallow copy \"\"\"\n    z = x.copy()\n    z.update(y)\n    return z","122c630c":"company_locations_colors = {\n    'google': 'green',\n    'apple': 'red',\n    'facebook': 'blue'\n}","f21a61dc":"loc_arr = []\nnum_arr = []\nlong_arr = []\nlati_arr = []\n\nfor key, value in loc_apple_dic.items():\n    loc_arr.append(key)\n    num_arr.append(num_apple_dic[key])\n    long_arr.append(loc_apple_dic[key][1])\n    lati_arr.append(loc_apple_dic[key][0])\ncompany_apple_locations={'apple': {'long':long_arr,'lati':lati_arr,'num':num_arr,'loc':loc_arr }}","4f0363f8":"loc_arr = []\nnum_arr = []\nlong_arr = []\nlati_arr = []\n\nfor key, value in loc_google_dic.items():\n    loc_arr.append(key)\n    num_arr.append(num_google_dic[key])\n    long_arr.append(loc_google_dic[key][1])\n    lati_arr.append(loc_google_dic[key][0])\ncompany_google_locations={'google': {'long':long_arr,'lati':lati_arr,'num':num_arr,'loc':loc_arr }}","4dfcc766":"loc_arr = []\nnum_arr = []\nlong_arr = []\nlati_arr = []\n\nfor key, value in loc_facebook_dic.items():\n    loc_arr.append(key)\n    num_arr.append(num_facebook_dic[key])\n    long_arr.append(loc_facebook_dic[key][1])\n    lati_arr.append(loc_facebook_dic[key][0])\ncompany_facebook_locations={'facebook': {'long':long_arr,'lati':lati_arr,'num':num_arr,'loc':loc_arr }}","419888da":"tem = merge_two_dicts(company_apple_locations,company_google_locations)\ncompany_locations = merge_two_dicts(tem,company_facebook_locations)\ncompany_locations['google']['loc'][0]","641bac1e":"import pandas as pd # Reading csv file \nfrom shapely.geometry import Point # Shapely for converting latitude\/longtitude to geometry\nimport geopandas as gpd # To create GeodataFrame\n\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nbase = world.plot(color='white', edgecolor='black',figsize=(18,16))\nbase.legend(['google'], title='legend')\n\n\nlon = company_locations['google']['long']\nlat = company_locations['google']['lati']\nkey = company_locations['google']['loc']\nnum = company_locations['google']['num']\ngeometry = [Point(xy) for xy in zip(lon, lat)]\n    \n# Coordinate reference system : WGS84\ncrs = {'init': 'epsg:4326'}\n    \n# Creating a Geographic data frame \nfor a in range(0,len(lon)):\n    gdf_google_dot = gpd.GeoDataFrame(key, crs=crs, geometry=geometry)\n    size = 5 + num[a]*0.4\n    base.scatter(lon[a],lat[a], marker='o', color='green',s=size)\n    \nbase.set_title('Google Job Opportunities')\nbase.legend(['Google'])","6e00dbe9":"world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nbase = world.plot(color='white', edgecolor='black',figsize=(18,16))\nbase.legend(['apple'], title='legend')\n\n\nlon = company_locations['apple']['long']\nlat = company_locations['apple']['lati']\nkey = company_locations['apple']['loc']\nnum = company_locations['apple']['num']\ngeometry = [Point(xy) for xy in zip(lon, lat)]\n    \n# Coordinate reference system : WGS84\ncrs = {'init': 'epsg:4326'}\n    \n# Creating a Geographic data frame \nfor a in range(0,len(lon)):\n    gdf_apple_dot = gpd.GeoDataFrame(key, crs=crs, geometry=geometry)\n    size = 5 + num[a]*0.4\n    base.scatter(lon[a],lat[a], marker='o', color='red',s=size)\n    \nbase.set_title('Apple Job Opportunities')\nbase.legend(['Apple'])","1aefe27c":"world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nbase = world.plot(color='white', edgecolor='black',figsize=(18,16))\nbase.legend(['facebook'], title='legend')\n\n\nlon = company_locations['facebook']['long']\nlat = company_locations['facebook']['lati']\nkey = company_locations['facebook']['loc']\nnum = company_locations['facebook']['num']\ngeometry = [Point(xy) for xy in zip(lon, lat)]\n    \n# Coordinate reference system : WGS84\ncrs = {'init': 'epsg:4326'}\n    \n# Creating a Geographic data frame \nfor a in range(0,len(lon)):\n    gdf_facebook_dot = gpd.GeoDataFrame(key, crs=crs, geometry=geometry)\n    size = 5 + num[a]*0.4\n    base.scatter(lon[a],lat[a], marker='o', color='blue',s=size)\n    \nbase.set_title('Facebook Job Opportunities')\nbase.legend(['Facebook'])","e1d82175":"world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nbase = world.plot(color='white', edgecolor='black',figsize=(18,16))\nbase.legend(['google', 'apple', 'facebook'], title='legend')\n\nfor company, color in company_locations_colors.items():\n    lon = company_locations[company]['long']\n    lat = company_locations[company]['lati']\n    key = company_locations[company]['loc']\n    num = company_locations[company]['num']\n    geometry = [Point(xy) for xy in zip(lon, lat)]\n    \n    df = pandas.DataFrame(\n    {'City':key,\n     'Company':company,\n     'Number of jobs':num})\n    \n    # Coordinate reference system : WGS84\n    crs = {'init': 'epsg:4326'}\n    \n    # Creating a Geographic data frame \n    if company is 'google':\n        gdf_google = gpd.GeoDataFrame(df, crs=crs, geometry=geometry)\n        gdf_google.plot(ax=base, marker='o', color=color, markersize=20)\n    if company is 'apple':\n        gdf_apple = gpd.GeoDataFrame(df, crs=crs, geometry=geometry)\n        gdf_apple.plot(ax=base, marker='o', color=color, markersize=20)\n    if company is 'facebook':\n        gdf_facebook = gpd.GeoDataFrame(df, crs=crs, geometry=geometry)\n        gdf_facebook.plot(ax=base, marker='o', color=color, markersize=20)\n    \nbase.set_title('Google, Apple, Facebook Global Job Opportunities')\nbase.legend(['Google', 'Apple', 'Facebook'])","cce39e43":"from bokeh.io import output_file, output_notebook, show\nfrom bokeh.plotting import figure, output_file, show, ColumnDataSource\noutput_notebook()","3fe857b5":"def convert_GeoPandas_to_Bokeh_format(gdf):\n    \"\"\"\n    Function to convert a GeoPandas GeoDataFrame to a Bokeh\n    ColumnDataSource object.\n    \n    :param: (GeoDataFrame) gdf: GeoPandas GeoDataFrame with polygon(s) under\n                                the column name 'geometry.'\n                                \n    :return: ColumnDataSource for Bokeh.\n    \"\"\"\n    gdf_new= gdf.drop('geometry', axis=1).copy()\n\n    gdf_new['x'] = gdf.apply(getGeometryCoords, \n                             geom='geometry', \n                             coord_type='x', \n                             shape_type='polygon', \n                             axis=1)\n    \n    gdf_new['y'] = gdf.apply(getGeometryCoords, \n                             geom='geometry', \n                             coord_type='y', \n                             shape_type='polygon', \n                             axis=1)\n    \n    return ColumnDataSource(gdf_new)\n\n\ndef getGeometryCoords(row, geom, coord_type, shape_type):\n    \"\"\"\n    Returns the coordinates ('x' or 'y') of edges of a Polygon exterior.\n    \n    :param: (GeoPandas Series) row : The row of each of the GeoPandas DataFrame.\n    :param: (str) geom : The column name.\n    :param: (str) coord_type : Whether it's 'x' or 'y' coordinate.\n    :param: (str) shape_type\n    \"\"\"\n    \n    # Parse the exterior of the coordinate\n    if shape_type == 'polygon':\n        try:\n            exterior = row[geom].exterior\n        except:\n            exterior = row[geom].geom[0].exterior\n        \n        if coord_type == 'x':\n            # Get the x coordinates of the exterior\n            return list( exterior.coords.xy[0] )    \n        \n        elif coord_type == 'y':\n            # Get the y coordinates of the exterior\n            return list( exterior.coords.xy[1] )\n\n    elif shape_type == 'point':\n        exterior = row[geom]\n    \n        if coord_type == 'x':\n            # Get the x coordinates of the exterior\n            return  exterior.coords.xy[0][0] \n\n        elif coord_type == 'y':\n            # Get the y coordinates of the exterior\n            return  exterior.coords.xy[1][0]","9950531f":"world.head()","64dd9e5a":"def explode(gdf):\n    gs = gdf.explode()\n    gdf2 = gs.reset_index().rename(columns={0: 'geometry'})\n    gdf_out = gdf2.merge(gdf.drop('geometry', axis=1), left_on='level_0', right_index=True)\n    gdf_out = gdf_out.set_index(['level_0', 'level_1']).set_geometry('geometry')\n    gdf_out.crs = gdf.crs\n    return gdf_out","b471d39e":"world_Source = convert_GeoPandas_to_Bokeh_format(explode(world))","a1c08890":"from bokeh.models import (\n    Range1d,\n    GeoJSONDataSource,\n    HoverTool,\n    LinearColorMapper,\n    GMapPlot, GMapOptions, ColumnDataSource, \n    Circle, DataRange1d, PanTool, WheelZoomTool, BoxSelectTool\n)","47caebde":"gdf_google['x'] = gdf_google.apply(getGeometryCoords, \n                                 geom='geometry', \n                                 coord_type='x', \n                                 shape_type='point',\n                                 axis=1)\n                                 \ngdf_google['y'] = gdf_google.apply(getGeometryCoords, \n                                 geom='geometry', \n                                 coord_type='y', \n                                 shape_type='point',\n                                 axis=1)\n\ngdf_google = gdf_google.drop(['geometry'],axis=1)\n\npoint_source_google = ColumnDataSource(data=dict(x=gdf_google['x'],\n                                      y=gdf_google['y'],\n                                      City=gdf_google['City'].values,\n                                      Company=gdf_google['Company'].values,\n                                      NumberofJobs=gdf_google['Number of jobs'].values))","a95235eb":"gdf_apple['x'] = gdf_apple.apply(getGeometryCoords, \n                                 geom='geometry', \n                                 coord_type='x', \n                                 shape_type='point',\n                                 axis=1)\n                                 \ngdf_apple['y'] = gdf_apple.apply(getGeometryCoords, \n                                 geom='geometry', \n                                 coord_type='y', \n                                 shape_type='point',\n                                 axis=1)\n\ngdf_apple = gdf_apple.drop(['geometry'],axis=1)\n\npoint_source_apple = ColumnDataSource(data=dict(x=gdf_apple['x'],\n                                      y=gdf_apple['y'],\n                                      City=gdf_apple['City'].values,\n                                      Company=gdf_apple['Company'].values,\n                                      NumberofJobs=gdf_apple['Number of jobs'].values))","1631216f":"gdf_facebook['x'] = gdf_facebook.apply(getGeometryCoords, \n                                 geom='geometry', \n                                 coord_type='x', \n                                 shape_type='point',\n                                 axis=1)\n                                 \ngdf_facebook['y'] = gdf_facebook.apply(getGeometryCoords, \n                                 geom='geometry', \n                                 coord_type='y', \n                                 shape_type='point',\n                                 axis=1)\n\ngdf_facebook = gdf_facebook.drop(['geometry'],axis=1)\n\npoint_source_facebook = ColumnDataSource(data=dict(x=gdf_facebook['x'],\n                                      y=gdf_facebook['y'],\n                                      City=gdf_facebook['City'].values,\n                                      Company=gdf_facebook['Company'].values,\n                                      NumberofJobs=gdf_facebook['Number of jobs'].values))","1cab467c":"TOOLS = \"pan,wheel_zoom,box_zoom,reset,hover,save\"","d9243e7d":"Elevated = figure(title=\"Google, Apple, Facebook Locations across the Globe\",\n            tools=TOOLS,\n            x_axis_location=None, \n            y_axis_location=None, plot_width=900, plot_height=450)   ","890808fa":"Elevated.multi_line('x', \n                    'y', \n                    source=world_Source, \n                    color=\"gray\", \n                    line_width=1)","579bc38b":"Elevated.circle('x', \n                'y', \n                source=point_source_google, \n                size=5,\n                color='green')","c016bba6":"Elevated.circle('x', \n                'y', \n                source=point_source_apple, \n                size=5,\n                color='red')","b80e36bc":"Elevated.circle('x', \n                'y', \n                source=point_source_facebook, \n                size=5,\n                color='blue')","33998dc6":"hover = Elevated.select_one(HoverTool)\nhover.point_policy = \"follow_mouse\"\n\nTOOLTIPS = \"\"\"\n    <div>\n        <div>\n            <span style=\"font-size: 15px; font-weight: bold;\">City:<\/span>\n            <span style=\"font-size: 17px; font-weight: bold;\">@City<\/span>\n        <\/div>\n        <div>\n            <span style=\"font-size: 15px;\">Company:<\/span>\n            <span style=\"font-size: 15px; color: #696;\">@Company<\/span>\n        <\/div>\n        <div>\n            <span style=\"font-size: 15px;\">Location:<\/span>\n            <span style=\"font-size: 15px; color: #696;\">($x, $y)<\/span>\n        <\/div>\n        <div>\n            <span style=\"font-size: 15px;\">NumberofJobs:<\/span>\n            <span style=\"font-size: 15px; color: #696;\">@NumberofJobs<\/span>\n        <\/div>\n    <\/div>\n\"\"\"\nhover.tooltips=TOOLTIPS","7033d5ef":"show(Elevated)","7f68868b":"### 1.4 Keyword Extraction: TF-IDF Wordclouds\nWe want to know what are the most important keywords in the job descriptions, so here we'll extract keywords from minimum qualifications, preferred qualifications and responsibilities, and then create three wordclouds using TF-IDF.","82f210e0":"It seems that lots of job positions doesn't mention work experience. We'll explore further.","f147de09":"### 1.2 Work Experience Requirements\nThe following function works this way: For each column under the specified column name,\n* 3 years -> extract `3`\n* 3-5 years -> extract `5`\n* 5+ years -> extract `5`","d373ff3a":"# Get a Software Engineer Job at Big Tech Companies\n\n> Authors: [Anita Chang](https:\/\/github.com\/a19990506), [Ariel Hsu](https:\/\/github.com\/aarriiel), [Marco Wang](https:\/\/github.com\/aesophor)\n>\n> Supervisor: [Pecu Tsai](https:\/\/github.com\/pecu)\n>\n> Github Repo: https:\/\/github.com\/aesophor\/NTU-CSX-DataScience2019\n> \n> Please refer to **[this external page](https:\/\/github.com\/aesophor\/NTU-CSX-DataScience2019\/blob\/master\/hw4-final-project\/README_EN.md)** to see all these graphs without code.\n\n## Intention \nAs a CS major student, working as a **software engineer** at one of the big tech companies has always been my dream. \nHowever, I wonder...\n\n* What if I'm not graduated from a prestigious university?\n* What if I don't have any work experience?\n* Being familiar with which programming languages (besides C++\/Java\/Python) gives me an edge?\n* Which frameworks\/skills are preferred in Google\/Facebook\/Apple?\n\nTherefore, my friends and I decided to collect all job descriptions relavant to the keyword `software` \nfrom the websites of **Google, Facebook and Apple** ([web scrapers source code](https:\/\/github.com\/aesophor\/NTU-CSX-DataScience2019\/tree\/master\/hw4-final-project\/data)), then perform some analysis and visualization. Not\nsure if you would like it, but we do hope that our work can benefit other people (even if just a few!)\n\nThe data used in this kernel were collected on Jan 22, 2019. \n\n## Outline\n1. **Google**    \n  1.1 Degree Requirements    \n  1.2 Work Experience Requirements    \n  1.3 Frequently Appeared Programming Languages in Job Descriptions    \n  1.4 Keyword Extraction: TF-IDF Wordclouds    \n  1.5 Keyword Extraction: Term-Term Matrix Visualization    \n2. **Facebook**    \n  2.1 Degree Requirements    \n  2.2 Work Experience Requirements    \n  2.3 Frequently Appeared Programming Languages in Job Descriptions    \n  2.4 Keyword Extraction: TF-IDF Wordclouds    \n  2.5 Keyword Extraction: Term-Term Matrix Visualization    \n3. **Apple**    \n  3.1 Degree Requirements    \n  3.2 Work Experience Requirements    \n  3.3 Frequently Appeared Programming Languages in Job Descriptions    \n  3.4 Keyword Extraction: TF-IDF Wordclouds    \n  3.5 Keyword Extraction: Term-Term Matrix Visualization\n4. **Global Job Opportunities at Google, Facebook and Apple**\n5. **Conclusion**\n    \n## 1. Google\n### 1.1 Degree Requirements\n","73ef8369":"### 3.2 Work Experience Requirements","4be13752":"### 2.5 Keyword Extraction: Term-Term Matrix Visualization","6c01bc31":"In summary, in order to have advantages over other applicants at Google...\n* have at least a bachelor degree, but further education is preferred.\n* master at least one general purpose programming language, such as Java, C\/C++, etc.\n* have experience with [GWT](http:\/\/www.gwtproject.org\/overview.html), JSP, OOP, etc.\n* have experience with Unix\/Linux open source developments && kernel\/device drivers.\n* have experience building large software systems.\n* have competent communication skills (both written and verbal).\n\nAlso it seems that Google cares a lot about UX and is looking forward to redesigning UI.","b5f560f3":"![](https:\/\/raw.githubusercontent.com\/aesophor\/NTU-CSX-DataScience2019\/master\/hw4-final-project\/out_graphs\/facebook_keyword_relations.png)","c75d1de5":"Most jobs at Google require at least a **Bachelor** degree, but they prefer applicants with **further education** (e.g., Master and PhD). However, there are 131 jobs that doesn't mention degree requirements at all.","c4cc3e28":"Lots of job positions doesn't mention work experience. However, my guess is that I'll need some practical experience in building systems (since getting a software engineer job position at Google is quite difficult). We'll create wordclouds later, so let's carry on for now.","312e6d9e":"### 1.5 Keyword Extraction: Term-Term Matrix Visualization\nWe also created a term-term matrix visualization. For each job position, we extract the top 3 most important keywords (we borrowed a smart stopwords list from [here](https:\/\/github.com\/aneesha\/RAKE\/blob\/master\/SmartStoplist.txt)), create a term-term matrix and perform visualization with Gephi.\n\nReference: https:\/\/zhuanlan.zhihu.com\/p\/30023387?fbclid=IwAR1XnO0HXkhLMDrdQrS9dCYew9uk5aucQZzptR96JZWLnKW2nF9I3EVlMzM","88d018ff":"Now we're going to create an enhanced version of this map with the ability to interact with its users!","d13a0431":"We'll also wrap all of these in a function.","980c5301":"* programming: Java\/JSP\/OOP\/modular\/asp.net\/JavaScript\/CSS\/HTML\/Ajax\n* tools: gwt\/xul\/xaml\/flex\n* goal: accelerate\/next-generation\/redesign\/user interface","efdd7d41":"### 2.2 Work Experience Requirements","613d098c":"Hmm... so some job positions don't require educational degree and work experience. I found an interesting example which doesn't mention educational degree, but **requires practical experience** from the applicant (e.g., C\/C++, open source, Linux userspace, Linux kernel skills). Personally, these skills can be self-taught or gathered from previous work experience.","c0567bf1":"* library: boost\/openmp\n* tools: git\n* machine learning: openml\n* hardware related: i2c\/uart\/spi\/low-level\/device drivers\n* parallel computation: mpi\/cuda","0dcf5fda":"![](https:\/\/raw.githubusercontent.com\/aesophor\/NTU-CSX-DataScience2019\/master\/hw4-final-project\/out_graphs\/google_keyword_relations.png)","189178c6":"## 2. Facebook\n### 2.1 Degree Requirements","5ca39466":"* criteria: hands-on experience\n* management: pdm\/lims\/tech-stack\/ERP\/OEM\n* familiar with: python\/perl\/\\*nix\n* ability: debugging\/deployment\/schematics\/assist","7a74172d":"### 3.4 Keyword Extraction: TF-IDF Wordclouds","ae02e038":"### 2.4 Keyword Extraction: TF-IDF Wordclouds","ad2b73f7":"Lots of job positions mention \"BS\" and no experience. I found an interesting example, let me sum it up real quick:\n* Bachelor's degree in CS or equivalent is required; Master's degree preferred\n* programming paradigm: OOP\n* have an understand of: scalable distributed systems (scalability and performance matters a lot)\n* core skills: strong problem solving\/debugging\/communicate effectively (written and verbal)\n* mindsets: results oriented\/deadline driven\/have desire to work in dynamic and challenging environment","785188e2":"### 3.5 Keyword Extraction: Term-Term Matrix Visualization","dcb6c1fa":"![](https:\/\/raw.githubusercontent.com\/aesophor\/NTU-CSX-DataScience2019\/master\/hw4-final-project\/out_graphs\/apple_keyword_relations.png)","b08a4caa":"## 5. Conclusion\nConclusively, in order to have an advantage among all job applicants at Google, Facebook and Apple...\n* Having at least a **Bachelor degree** would be safer (unless you have some jaw-dropping personal projects); **further education is a plus**!\n* Having **practical experience** is vital. Previous work experience isn't necessary for every job, but most job positions metion lots of skills you need to have, and I believe a good way to build up those experience is through **personal projects**.\n* In big tech companies, they care a lot about system **scalability, stability and performance** (every msec counts).\n* Having **competent communication skills** (both written and verbal) is important, since you will be working in a TEAM with lots of amazing engineers!\n* Google cares a lot about UI\/UX and is currently looking forward to redesigning it. Facebook has lots of job positions related to hardware and firmware. In Apple, some well-known management methodologies (e.g., pdm, lims, tech-stack, ERP...) are mentioned.\n* master at least one general purpose programming language, such as Java, C\/C++, Python etc. Being very good at one programming language is very important.\n* have experience with **OOP** and Unix\/Linux **open source** developments.\n---\n**Last but not least, a shoutout to our supervisor, Pecu Tsai, as well as her TAs, who spent their time training us the skills we need!**","a879d062":"Finally, export the term-term matrix to a csv file and import it as a `spreadsheet` in Gephi.","8214dcb8":"### 3.3 Frequently Appeared Programming Languages in Job Descriptions","1f8380fd":"## 4. Global Job Opportunities at Google, Facebook and Apple\nWe are students from Taiwan, and we harbor ambitions of working abroad at a big tech company in a team with some amazing software engineers! Therefore, we decide to visualize all job opportunities in a world map.","4f54379e":"Here we'll create the following function (since it will be reused for many times).","2d3621ac":"I wonder what Google engineers are using Python for, so I randomly pick three job positions and inpsect their preferred qualifications.\n\nAlso it seems that I need to \n* master at least one general purpose programming language, such as Java, C\/C++, ..., etc. \n* have experience with Unix\/Linux open source developments && kernel\/device drivers.\n* have experience building large software systems.\n* have competent communication skills (both written and verbal).","e013a0b3":"## 3. Apple\n### 3.1 Degree Requirements","93e140d6":"In Facebook, PHP is used quite often! Anyway, let's dig deeper and create wordclouds!","c5935fef":"Now we can create the following function which allows us to create wordclouds easily!","7389cca2":"### 2.3 Frequently Appeared Programming Languages in Job Descriptions","e4b0d44a":" ### 1.3 Frequently Appeared Programming Languages in Job Descriptions"}}