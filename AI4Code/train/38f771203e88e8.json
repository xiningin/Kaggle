{"cell_type":{"24651f11":"code","fbff7d0e":"code","9a7399e3":"code","eeac6bc0":"code","696b668c":"code","1bc0b90d":"code","54e1f7f7":"code","0585feb6":"code","94057eaf":"code","994fb7a7":"code","58d4c9fc":"code","0d35210f":"code","7787c3d4":"code","3cfcd46a":"code","c4e13be3":"code","0a6f6b4f":"code","8ebae57b":"code","43ad1ff0":"code","efc21d06":"code","ad77bc23":"code","6f5bce43":"code","3e3c5e9c":"code","75b128dd":"code","15102584":"code","b433e364":"code","a48a1816":"code","04749119":"code","a2aa7b22":"code","0d39f12f":"code","c10b3a38":"code","e4bc972d":"code","e3414354":"code","b96c9ca8":"code","aeffaf8d":"code","4a3822b2":"code","04123672":"code","4a236ef6":"code","81fa4892":"code","3c8a6018":"code","aa190fee":"code","5aad32b7":"code","233b0a03":"code","c2a83d9c":"code","8cd82333":"markdown","f9d88d13":"markdown","6df8f028":"markdown","93afe693":"markdown","223d0f22":"markdown","68a005b7":"markdown","ad35ee4f":"markdown","33a9eb3b":"markdown","8089d545":"markdown","110eda0d":"markdown","99ee5061":"markdown","3da40203":"markdown"},"source":{"24651f11":"import os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.image import imread","fbff7d0e":"my_data_dir = '..\/input\/flowers-recognition\/flowers'","9a7399e3":"os.listdir(my_data_dir)","eeac6bc0":"dandelion_image_path = my_data_dir+'\/dandelion\/'+os.listdir(my_data_dir+'\/dandelion')[0]\ndandelion_img= imread(dandelion_image_path)\nplt.imshow(dandelion_img)\nplt.show()\ndandelion_img.shape","696b668c":"rose_image_path = my_data_dir+'\/rose\/'+os.listdir(my_data_dir+'\/rose')[0]\nrose_img = imread(rose_image_path)\nplt.imshow(rose_img)\nplt.show()\nrose_img.shape","1bc0b90d":"tulip_image_path = my_data_dir+'\/tulip\/'+os.listdir(my_data_dir+'\/tulip')[0]\ntulip_img = imread(tulip_image_path)\nplt.imshow(tulip_img)\nplt.show()\ntulip_img.shape","54e1f7f7":"sunflower_image_path = my_data_dir+'\/sunflower\/'+os.listdir(my_data_dir+'\/sunflower')[0]\nsunflower_img = imread(sunflower_image_path)\nplt.imshow(sunflower_img)\nplt.show()\nsunflower_img.shape","0585feb6":"daisy_image_path = my_data_dir+'\/daisy\/'+os.listdir(my_data_dir+'\/daisy')[0]\ndaisy_img = imread(daisy_image_path)\nplt.imshow(daisy_img)\nplt.show()\ndaisy_img.shape","94057eaf":"print(\"Total dandelionlen images:\", len(os.listdir(my_data_dir+'\/dandelion')))\nprint(\"Total rose images:\", len(os.listdir(my_data_dir+'\/rose')))\nprint(\"Total daisy images:\", len(os.listdir(my_data_dir+'\/daisy')))\nprint(\"Total sunflower images:\", len(os.listdir(my_data_dir+'\/sunflower')))\nprint(\"Total tulip images:\", len(os.listdir(my_data_dir+'\/tulip')))","994fb7a7":"dim1 = []\ndim2 = []\nfor image_filename in os.listdir(my_data_dir+'\/tulip'):\n    \n    img = imread(my_data_dir+'\/tulip'+'\/'+image_filename)\n    d1,d2,colors = img.shape\n    dim1.append(d1)\n    dim2.append(d2)","58d4c9fc":"sns.jointplot(dim1,dim2)\nplt.show()","0d35210f":"print(np.mean(dim1))\nprint(np.mean(dim2))","7787c3d4":"image_shape = (224,224,3)","3cfcd46a":"from tensorflow.keras.preprocessing.image import ImageDataGenerator","c4e13be3":"image_gen = ImageDataGenerator(rotation_range=20, # rotate the image 20 degrees\n                               width_shift_range=0.10, # Shift the pic width by a max of 5%\n                               height_shift_range=0.10, # Shift the pic height by a max of 5%\n                               rescale=1\/255, # Rescale the image by normalzing it.\n                               shear_range=0.1, # Shear means cutting away part of the image (max 10%)\n                               zoom_range=0.1, # Zoom in by 10% max\n                               horizontal_flip=True, # Allo horizontal flipping\n                               fill_mode='nearest' # Fill in missing pixels with the nearest filled value\n                              )","0a6f6b4f":"plt.imshow(image_gen.random_transform(dandelion_img))\nplt.show()","8ebae57b":"plt.imshow(image_gen.random_transform(dandelion_img))\nplt.show()","43ad1ff0":"plt.imshow(image_gen.random_transform(dandelion_img))\nplt.show()","efc21d06":"image_gen.flow_from_directory(my_data_dir)","ad77bc23":"from tensorflow.keras.applications import DenseNet201\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.regularizers import l2","6f5bce43":"model = DenseNet201(include_top = False,weights = 'imagenet',input_shape = (224,224,3))","3e3c5e9c":"model.summary()","75b128dd":"for ix in range(len(model.layers)):\n    print(ix,model.layers[ix])","15102584":"for layer in model.layers[:703]:\n    layer.trainable = False\nfor i, layer in enumerate(model.layers):\n    print(i, layer.name, layer.trainable)","b433e364":"av1 = Flatten()(model.output)\nfc1 = Dense(256,activation='relu',kernel_regularizer= l2(0.01),input_dim=256)(av1)\nd1 = Dropout(0.5)(fc1)\nfc2 = Dense(128,activation='relu',kernel_regularizer= l2(0.01),input_dim=128)(d1)\nd2 = Dropout(0.5)(fc2)\nfc3 = Dense(5,activation = 'softmax')(d2)\n\n\nmodel_den = Model(model.input,fc3)\nmodel_den.summary()","a48a1816":"from tensorflow.keras.applications.resnet50 import ResNet50","04749119":"model1 = ResNet50(include_top=False, input_shape=(224,224,3), weights='imagenet')","a2aa7b22":"model1.summary()","0d39f12f":"for ix in range(len(model1.layers)):\n    print(ix,model1.layers[ix])","c10b3a38":"for layer in model1.layers[:171]:\n    layer.trainable = False\nfor i, layer in enumerate(model1.layers):\n    print(i, layer.name, layer.trainable)","e4bc972d":"av1 = Flatten()(model1.output)\nfc1 = Dense(256,activation='relu',kernel_regularizer= l2(0.01),input_dim=256)(av1)\nd1 = Dropout(0.5)(fc1)\nfc2 = Dense(128,activation='relu',kernel_regularizer= l2(0.01),input_dim=128)(d1)\nd2 = Dropout(0.5)(fc2)\nfc3 = Dense(5,activation = 'softmax')(d2)\n\n\nmodel_R = Model(model1.input,fc3)\nmodel_R.summary()","e3414354":"import tensorflow as tf\nmodels = [model_R,model_den]\nmodel_input = tf.keras.Input(shape=(224,224, 3))\nmodel_outputs = [model(model_input) for model in models]\nensemble_output = tf.keras.layers.Average()(model_outputs)\nensemble_model = tf.keras.models.Model(inputs=model_input, outputs=ensemble_output, name='ensemble')","b96c9ca8":"ensemble_model.summary()","aeffaf8d":"adam = Adam(learning_rate=0.0001)\nensemble_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),optimizer = adam,metrics=['accuracy'])","4a3822b2":"tf.keras.utils.plot_model(ensemble_model, 'model1.png', show_shapes= True)","04123672":"from tensorflow.keras.callbacks import ModelCheckpoint","4a236ef6":"filepath=\"Flowers_ensemble_model.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=2, save_best_only=True, mode='min',patience=5)\ncallbacks_list = [checkpoint]","81fa4892":"from keras.preprocessing import image_dataset_from_directory\ntrain_image_gen=image_dataset_from_directory(my_data_dir,validation_split=0.2,batch_size=32,subset='training',image_size=(224,224),seed=42)","3c8a6018":"validation_image_gen=image_dataset_from_directory(my_data_dir,validation_split=0.2,seed=42,batch_size=32,image_size=(224,224),subset='validation')","aa190fee":"train_image_gen.class_names","5aad32b7":"import warnings\nwarnings.filterwarnings('ignore')","233b0a03":"hist = ensemble_model.fit(train_image_gen,\n                    validation_data = validation_image_gen,\n                    shuffle = True,\n                    epochs = 10,callbacks=callbacks_list)","c2a83d9c":"plt.figure(1, figsize = (15, 5))\nplt.subplot(1,2,1)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.plot( hist.history[\"loss\"], label = \"Training Loss\")\nplt.plot( hist.history[\"val_loss\"], label = \"Validation Loss\")\nplt.grid(True)\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.plot( hist.history[\"accuracy\"], label = \"Training Accuracy\")\nplt.plot( hist.history[\"val_accuracy\"], label = \"Validation Accuracy\")\nplt.grid(True)\nplt.legend()\nplt.show()","8cd82333":"# Train Model","f9d88d13":"# Fine tuning","6df8f028":"# Resnet50 Model","93afe693":"# Data Load","223d0f22":"# Flowers Prediction Using Ensembel Models\n\n* Total 4317 photos with 5 classes\n* Using Densenet201 and Resnet50\n* Ensembling both\n* Fine tuning\n* To avoid overfitting Regularization,Droupouts and ImageDataGenrator","68a005b7":"# Visualize Images","ad35ee4f":"# Early Stoping","33a9eb3b":"# DenseNet201 Model","8089d545":"# Images Genrator","110eda0d":"# Ensembel DenseNet201 and Resnet50","99ee5061":"# Resnet50 Model","3da40203":"# Load Libraries"}}