{"cell_type":{"b2be5c52":"code","708ed531":"code","2c1f36d4":"code","6dc7a899":"code","178024da":"code","cf8b3bf6":"code","7f18e737":"code","f4031690":"code","ee02bbbe":"code","9f1f0f0c":"code","b7a59e6e":"code","bc66798a":"code","64976a29":"code","61f67e78":"code","a8efc2b0":"code","9e31e280":"code","5d1c5e63":"code","01eda978":"code","dd6081d6":"code","bf774d68":"code","d8193d9e":"code","c891af03":"code","ed2fcf41":"code","ee590063":"code","79b85c20":"code","d38683a4":"code","6d5f17af":"code","844a8f00":"code","b8560041":"markdown","50ef7ed3":"markdown","810438f0":"markdown","d1a90dc9":"markdown","e53281fc":"markdown","6055d667":"markdown","16caf602":"markdown","365ddc96":"markdown","6b817b55":"markdown"},"source":{"b2be5c52":"import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt \nplt.rc(\"font\", size=14)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nsns.set(style=\"white\")\nsns.set(style=\"whitegrid\", color_codes=True)\n\nimport warnings\nwarnings.simplefilter(action='ignore')","708ed531":"df=pd.read_csv(\"..\/input\/bankcsv\/banking.csv\", header=0)\ndf.head()","2c1f36d4":"df['education'].unique()","6dc7a899":"data = df.copy()\n\n#The education column of the dataset has many categories and we need to reduce the categories for a better modelling. \n#Group all the basic educations together\ndata['education']=np.where(data['education'] =='basic.9y', 'Basic', data['education'])\ndata['education']=np.where(data['education'] =='basic.6y', 'Basic', data['education'])\ndata['education']=np.where(data['education'] =='basic.4y', 'Basic', data['education'])","178024da":"data['y'].value_counts()","cf8b3bf6":"count_no_sub = len(data[data['y']==0])\ncount_sub = len(data[data['y']==1])\n\npct_of_no_sub = count_no_sub\/(count_no_sub+count_sub)\nprint(\"percentage of no subscription is\", pct_of_no_sub*100)\n\npct_of_sub = count_sub\/(count_no_sub+count_sub)\nprint(\"percentage of subscription\", pct_of_sub*100)","7f18e737":"data.groupby('y').mean()","f4031690":"data.groupby('job').mean()","ee02bbbe":"data.groupby('marital').mean()","9f1f0f0c":"data.groupby('education').mean()","b7a59e6e":"%matplotlib inline\npd.crosstab(data.job,data.y).plot(kind='bar')\nplt.title('Purchase Frequency for Job Title')\nplt.xlabel('Job')\nplt.ylabel('Frequency of Purchase')\nplt.savefig('purchase_fre_job')","bc66798a":"table=pd.crosstab(data.marital,data.y)\ntable.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\nplt.title('Stacked Bar Chart of Marital Status vs Purchase')\nplt.xlabel('Marital Status')\nplt.ylabel('Proportion of Customers')\nplt.savefig('mariral_vs_pur_stack')","64976a29":"table=pd.crosstab(data.education,data.y)\ntable.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\nplt.title('Stacked Bar Chart of Education vs Purchase')\nplt.xlabel('Education')\nplt.ylabel('Proportion of Customers')\nplt.savefig('edu_vs_pur_stack')","61f67e78":"pd.crosstab(data.month,data.y).plot(kind='bar')\nplt.title('Purchase Frequency for Month')\nplt.xlabel('Month')\nplt.ylabel('Frequency of Purchase')\nplt.savefig('pur_fre_month_bar')","a8efc2b0":"data.age.hist()\nplt.title('Histogram of Age')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.savefig('hist_age')","9e31e280":"pd.crosstab(data.poutcome,data.y).plot(kind='bar')\nplt.title('Purchase Frequency for Poutcome')\nplt.xlabel('Poutcome')\nplt.ylabel('Frequency of Purchase')\nplt.savefig('pur_fre_pout_bar')","5d1c5e63":"cat_vars=['job','marital','education','default','housing','loan','contact','month','day_of_week','poutcome']\nfor var in cat_vars:\n    cat_list='var'+'_'+var\n    cat_list = pd.get_dummies(data[var], prefix=var)\n    data1=data.join(cat_list)\n    data=data1\ncat_vars=['job','marital','education','default','housing','loan','contact','month','day_of_week','poutcome']\ndata_vars=data.columns.values.tolist()\nto_keep=[i for i in data_vars if i not in cat_vars]\n\ndata_final=data[to_keep]\ndata_final.columns.values","01eda978":"X = data_final.loc[:, data_final.columns != 'y']\ny = data_final.loc[:, data_final.columns == 'y']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","dd6081d6":"from imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\n\nos = SMOTE(random_state = 0)","bf774d68":"os_data_X,os_data_y=os.fit_sample(X_train, y_train)\ncolumns = X_train.columns\nos_data_X = pd.DataFrame(data=os_data_X,columns=columns )\nos_data_y= pd.DataFrame(data=os_data_y,columns=['y'])","d8193d9e":"print(\"length of oversampled data is \",len(os_data_X))\nprint(\"Number of no subscription in oversampled data\",len(os_data_y[os_data_y['y']==0]))\nprint(\"Number of subscription\",len(os_data_y[os_data_y['y']==1]))","c891af03":"from sklearn.feature_selection import RFECV\nfrom sklearn.linear_model import LogisticRegression\n\nrfecv = RFECV(estimator=LogisticRegression(), step=1, cv=10, scoring='roc_auc')\nrfecv.fit(os_data_X, os_data_y.values.ravel())\n\nprint(\"Optimal number of features: %d\" % rfecv.n_features_)\nprint('Selected features: %s' % list(os_data_X.columns[rfecv.support_]))\n\n# Plot number of features VS. cross-validation scores\nplt.figure(figsize=(10,6))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","ed2fcf41":"#RFE has helped us choose the following columns\nselected_features = ['emp_var_rate', 'cons_price_idx', 'job_admin.', 'job_blue-collar', 'job_entrepreneur', 'job_housemaid', 'job_management', 'job_self-employed', 'job_services', 'job_student', 'job_technician', 'job_unemployed', 'job_unknown', 'marital_divorced', 'marital_married', 'marital_single', 'marital_unknown', 'education_Basic', 'education_high.school', 'education_professional.course', 'education_university.degree', 'education_unknown', 'default_no', 'default_unknown', 'housing_no', 'housing_unknown', 'housing_yes', 'loan_no', 'loan_unknown', 'loan_yes', 'contact_cellular', 'contact_telephone', 'month_apr', 'month_aug', 'month_dec', 'month_jul', 'month_jun', 'month_may', 'month_nov', 'month_oct', 'month_sep', 'day_of_week_fri', 'day_of_week_mon', 'day_of_week_thu', 'day_of_week_tue', 'day_of_week_wed', 'poutcome_failure', 'poutcome_nonexistent']","ee590063":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score \nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\n\n# create X (features) and y (response)\nos_data_X = os_data_X[selected_features]\nX_test = X_test[selected_features]","79b85c20":"# check classification scores of logistic regression\nlogreg = LogisticRegression()\nlogreg.fit(os_data_X, os_data_y)\ny_pred = logreg.predict(X_test)\ny_pred_proba = logreg.predict_proba(X_test)[:, 1]\n[fpr, tpr, thr] = roc_curve(y_test, y_pred_proba)\n\nprint('Train\/Test split results:')\nprint(logreg.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(y_test, y_pred))\nprint(logreg.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_test, y_pred_proba))\nprint(logreg.__class__.__name__+\" auc is %2.3f\" % auc(fpr, tpr))\n\nidx = np.min(np.where(tpr > 0.95)) # index of the first threshold for which the sensibility > 0.95\n\nplt.figure()\nplt.plot(fpr, tpr, color='coral', label='ROC curve (area = %0.3f)' % auc(fpr, tpr))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot([0,fpr[idx]], [tpr[idx],tpr[idx]], 'k--', color='blue')\nplt.plot([fpr[idx],fpr[idx]], [0,tpr[idx]], 'k--', color='blue')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)\nplt.ylabel('True Positive Rate (recall)', fontsize=14)\nplt.title('Receiver operating characteristic (ROC) curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\nprint(\"Using a threshold of %.3f \" % thr[idx] + \"guarantees a sensitivity of %.3f \" % tpr[idx] +  \n      \"and a specificity of %.3f\" % (1-fpr[idx]) + \n      \", i.e. a false positive rate of %.2f%%.\" % (np.array(fpr[idx])*100))","d38683a4":"confusion_matrix(y_test, y_pred)","6d5f17af":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","844a8f00":"# 10-fold cross-validation logistic regression\nlogreg = LogisticRegression()\n# Use cross_val_score function\n# We are passing the entirety of X and y since cross validation takes care of splitting the data\n# cv=10 for 10 folds\n# scoring = {'accuracy', 'neg_log_loss', 'roc_auc'} for evaluation metric - althought they are many\nscores_accuracy = cross_val_score(logreg, X, y, cv=10, scoring='accuracy')\nscores_log_loss = cross_val_score(logreg, X, y, cv=10, scoring='neg_log_loss')\nscores_auc = cross_val_score(logreg, X, y, cv=10, scoring='roc_auc')\n\nprint('K-fold cross-validation results:')\nprint(logreg.__class__.__name__+\" average accuracy is %2.3f\" % scores_accuracy.mean())\nprint(logreg.__class__.__name__+\" average log_loss is %2.3f\" % -scores_log_loss.mean())\nprint(logreg.__class__.__name__+\" average auc is %2.3f\" % scores_auc.mean())","b8560041":"## K-fold cross-validation","50ef7ed3":"# Data Exploration","810438f0":"# One Hot Encoding","d1a90dc9":"Marital status doesn't seem to be a good predictor","e53281fc":"# Visualizations","6055d667":"# Logistic Regression","16caf602":"# Cleaning Dataframe and Manipulating","365ddc96":" The classification goal is to predict whether the client will subscribe (1\/0) to a term deposit (so this is our variable y).","6b817b55":"# Logistic Regression & Results"}}