{"cell_type":{"1d9c2f89":"code","5c58939e":"code","a555153d":"code","10dc44f0":"code","44ccda07":"code","f501977f":"code","8346fad2":"code","d1cd5629":"code","0f4b4203":"code","fe00666c":"code","daaf8df2":"code","4dc453a0":"code","281a7a65":"code","987debfa":"code","8168e213":"code","f2c75e8f":"code","45a5d839":"code","741bef1a":"code","007fc638":"code","7412f961":"code","26287545":"markdown","bb0217e5":"markdown","69e5bf7a":"markdown","066c2daa":"markdown","2150d381":"markdown","516fb8f4":"markdown","2c8b299b":"markdown","28d6f75a":"markdown","59d372af":"markdown","dbb1afe7":"markdown","f1c476cf":"markdown","d9cc24e1":"markdown","8a2a8157":"markdown","192772c9":"markdown","b1d0481d":"markdown","d149cdad":"markdown","6a0750d2":"markdown","bc8bcda9":"markdown","4b6be2b9":"markdown"},"source":{"1d9c2f89":"#installing sklearn extra to run KMedoids\n!pip install https:\/\/github.com\/scikit-learn-contrib\/scikit-learn-extra\/archive\/master.zip\n\n","5c58939e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy.stats import zscore\n\n#data visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\n#clustering \nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom sklearn_extra.cluster import KMedoids\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os, gc, sys\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a555153d":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\n\n# function to read the data and merge it (ignoring some columns, this is a very fst model)\ndef read_data():\n    print('Reading files...')\n    INPUT_DIR = \"\/kaggle\/input\/m5-forecasting-accuracy\"\n    calendar = pd.read_csv(os.path.join(INPUT_DIR, 'calendar.csv'))\n    calendar = reduce_mem_usage(calendar)\n    print('Calendar has {} rows and {} columns'.format(calendar.shape[0], calendar.shape[1]))\n#     sell_prices = pd.read_csv(os.path.join(INPUT_DIR, 'sell_prices.csv'))\n#     sell_prices = reduce_mem_usage(sell_prices)\n#     print('Sell prices has {} rows and {} columns'.format(sell_prices.shape[0], sell_prices.shape[1]))\n    sales_train_validation = pd.read_csv(os.path.join(INPUT_DIR, 'sales_train_validation.csv'))\n    print('Sales train validation has {} rows and {} columns'.format(sales_train_validation.shape[0], sales_train_validation.shape[1]))\n    submission = pd.read_csv(os.path.join(INPUT_DIR, 'sample_submission.csv'))\n    return calendar, sales_train_validation, submission\n\n\n\ndef melt_and_merge(calendar, sales_train_validation, submission, nrows = 55000000, merge = False):\n    \n    # melt sales data, get it ready for training\n    data = pd.melt(sales_train_validation, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n    print('Melted sales train validation has {} rows and {} columns'.format(sales_train_validation.shape[0], sales_train_validation.shape[1]))\n    \n    # get only a sample for fst training\n    data = data.loc[nrows:]\n    \n    # drop some calendar features\n    calendar.drop(['weekday', 'wday', 'month', 'year'], inplace = True, axis = 1)\n    \n    \n    if merge:\n        # notebook crash with the entire dataset (maybee use tensorflow, dask, pyspark xD)\n        data = pd.merge(data, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\n        data.drop(['d', 'day'], inplace = True, axis = 1)\n        print('Our final dataset to train has {} rows and {} columns'.format(data.shape[0], data.shape[1]))\n    else: \n        pass\n    \n    gc.collect()\n    \n    #convert date to pandas datetime\n    data['date'] = pd.to_datetime(data['date'])\n\n    \n    return data\n        \ncalendar, sales_train_validation, submission = read_data()\ndata = melt_and_merge(calendar, sales_train_validation, submission, nrows = 25000000, merge = True)","10dc44f0":"a = data.groupby([pd.Grouper(key='date', freq='M'),'state_id'])['demand'].sum().reset_index(name='sum_of_sales')\npx.line(a, x='date', y='sum_of_sales',  color='state_id', title='Sum of Sales Monthly per State')","44ccda07":"a = data.groupby([pd.Grouper(key='date', freq='W'),'state_id'])['demand'].sum().reset_index(name='sum_of_sales')\npx.line(a, x='date', y='sum_of_sales',  color='state_id', title='Sum of Sales Weekly per State')","f501977f":"a = data.groupby([pd.Grouper(key='date', freq='M'),'state_id'])['demand'].sum().reset_index(name='sum_of_sales')\npx.box(a, x='state_id', y='sum_of_sales', title=' Sales vs. State name ', color='state_id')","8346fad2":"a = data.groupby([pd.Grouper(key='date', freq='M'),'cat_id'])['demand'].sum().reset_index(name='sum_of_sales')\npx.line(a, x='date', y='sum_of_sales',  color='cat_id', title='Sum of Sales monthly based on category')","d1cd5629":"a = data.groupby([pd.Grouper(key='date', freq='W'),'cat_id'])['demand'].sum().reset_index(name='sum_of_sales')\npx.line(a, x='date', y='sum_of_sales',  color='cat_id', title='Sum of Sales Weekly based on category')","0f4b4203":"a = data.groupby([pd.Grouper(key='date', freq='M'),'cat_id'])['demand'].sum().reset_index(name='sum_of_sales')\npx.box(a, x='cat_id', y='sum_of_sales', title=' Sales vs. Category name ', color='cat_id')","fe00666c":"a = data.groupby([pd.Grouper(key='date', freq='W'),'store_id'])['demand'].sum().reset_index(name='sum_of_sales')\npx.line(a, x='date', y='sum_of_sales',  color='store_id', title='Sum of Sales Weekly based on Store')","daaf8df2":"a = data.groupby(['date', 'store_id'])['demand'].sum().reset_index(name='sum_of_sales')\npx.line(a, x='date', y='sum_of_sales',  color='store_id', title='Sum of Sales per Store ')","4dc453a0":"a = data.groupby([pd.Grouper(key='date', freq='M'),'store_id'])['demand'].sum().reset_index(name='sum_of_sales')\npx.box(a, x='store_id', y='sum_of_sales', title=' Sales vs. Store name in each state', color='store_id')","281a7a65":"a = data.groupby([pd.Grouper(key='date', freq='M'), 'state_id', 'dept_id'])['demand'].sum().reset_index(name='sum_of_sales')\npx.line(a, x='date', y='sum_of_sales',  facet_col='state_id', color='dept_id', title='Sum of Sales per state\/dept id')","987debfa":"df = data.groupby([pd.Grouper(key='date', freq='M'),'id'])['demand'].sum().reset_index(name='sales')\ndf.head()","8168e213":"pivot_df = df.pivot(index='id', columns='date', values='sales')\nsales_matrix = pivot_df.values\n#changing values of 0 to 0.0001 to deal with 0 divition\nmax_values = np.max(sales_matrix, axis=1)[:,None]\nmax_values = np.array(max_values, dtype='float64')\nmax_values[np.argmin(max_values)][0] = 0.0001 ","f2c75e8f":"#normalize with the highest number of sales per product to transform into relative\nnormalized_sales_matrix = np.divide(sales_matrix, max_values)\npivot_df[pivot_df.columns] = normalized_sales_matrix\npivot_df.head()","45a5d839":"di = dict()\nfor column in pivot_df.columns:\n    di[column] = pivot_df[pivot_df[column] == 0].shape[0]\ncols = sorted(di, key=di.get, reverse=True)","741bef1a":"nclusters = 10\ndef cluster(matrix, n_clusters=8):\n    kmeans = KMeans(n_clusters)\n    kmedoids = KMedoids(n_clusters)\n    labels_kmeans = kmeans.fit_predict(matrix)\n    print(labels_kmeans)\n    labels_kmedoids = kmedoids.fit_predict(matrix)\n    print(labels_kmedoids)\n    cluster_df = pd.DataFrame()\n    cluster_df['id'] = pivot_df.index\n    cluster_df['cluster_kmeans'] = labels_kmeans\n    cluster_df['cluster_kmedoids'] = labels_kmedoids\n    return cluster_df\n\ncluster_df = cluster(pivot_df[cols].values, nclusters)\npivot_df['cluster_means'] = cluster_df['cluster_kmeans'].values\npivot_df['cluster_kmedoids'] = cluster_df['cluster_kmedoids'].values\npivot_df.head()","007fc638":"fig, ax = plt.subplots(nrows=int(nclusters\/2), ncols=2, figsize=(15,18))\n\nx, y = 0, 0\nfor j in range(0, nclusters):\n    if j == 0:\n        x = 0\n        y = 0\n    elif j % 2 == 0:\n        x += 1 \n        y = 0\n    else:\n        y += 1\n    a = pivot_df[pivot_df['cluster_means'] == j]\n    print(a.shape)\n    for i in range(0, 50):\n        \n        ax[x, y].plot(df['date'].unique(), a.iloc[i, :-2].values, color='lightgrey');\n    ax[x, y].plot(df['date'].unique(), a.iloc[:, :-2].values.mean(axis=0), color='red')\n    ax[x, y].set_title('cluster:' + str(j))\n\nplt.ylim(0, 5000)\nplt.show()","7412f961":"fig, ax = plt.subplots(nrows=int(nclusters\/2), ncols=2, figsize=(15,18))\n\nx, y = 0, 0\nfor j in range(0, nclusters):\n    if j == 0:\n        x = 0\n        y = 0\n    elif j % 2 == 0:\n        x += 1 \n        y = 0\n    else:\n        y += 1\n    a = pivot_df[pivot_df['cluster_kmedoids'] == j]\n    print(a.shape)\n\n    for i in range(0, 50):\n        \n        ax[x, y].plot(df['date'].unique(), a.iloc[i, :-2].values, color='lightgrey');\n    ax[x, y].plot(df['date'].unique(), a.iloc[:, :-2].values.mean(axis=0), color='red')\n    ax[x, y].set_title('cluster:' + str(j))\n\nplt.ylim(0, 5000)\nplt.show()","26287545":"## Clustering Time Series","bb0217e5":"## EDA","69e5bf7a":"As most of us have noticed, California state has the highest sales compared to 2 other states. Looking at weekly plot, we see that the sales in 3 states follow somehow a similar pattern in the sales flunctuations.","066c2daa":"Installing scikit-learn extra package for K-medoids usage","2150d381":"The above plots shows the weekly and daily sales in store level. in the previous plots, we saw that CA stores sell the most. But from this graphs, we see that store <code>\"CA_3\"<\/code> has the biggest influence in these aggregations, as the other CA stores are in the same level with stores in TX, WI. In Summer 2015, weekly sales at <code> CA_2<\/code> has increased on average 10k. We can assume that a development occured on the shop (the shop became bigger and offering more) or marketing strategy updates. The same happends to <code> WI_2<\/code> but the increase is more steadily.\n","516fb8f4":"## Read Data and Merge Files","2c8b299b":" ### EDA on sales based on store_id","28d6f75a":" ### EDA on sales based on cat id","59d372af":"Hey guys, this is my first notebook in kaggle. Please rate it as you like it!","dbb1afe7":"The stores in California have the highest variance in sales, meaning that the monthly sales have flunctuations(not consistent) where they might be external factors related to that. On the other hand the stores in Texas and Wisconsin show a consistency, without a lot of variance in their sales. ","f1c476cf":"This graphs shows the monthly sales based on department and state level. Department <code> FOODS_3 <\/code> and <code> HOUSEHOLD_1<\/code> seems to remain top-2 in all 3 states. We can assume that these dep includes neccessary or more essential products for the house. However, in <code> CA <\/code> we see that HOBBIES_1 has also a great demand compared to other states, competing with FOODS_2 category. ","d9cc24e1":"### Kmeans clustering","8a2a8157":"1. ### EDA on sales based on state id","192772c9":"### Kmedoids Clusters","b1d0481d":"![](http:\/\/)The above plot shows the distribution of sales over time for each category and in state level. Well, It is obvious that food category takes the lead, having a high variance in sales.","d149cdad":"# M5 Forecasting Challenge","6a0750d2":"The goal of this notebook is to provide a simple EDA to the data in this competition and extending it with a cluster analysis on the sales. I will show some vizualization using Plotly and matplotlib. clustering time series analysis will give us insights about the trend and patterns that each product follows throughtout the months, which probably can be converted to new feature to fit in our models. ","bc8bcda9":"# The dataset <a id=\"1\"><\/a>\n\nThe dataset consists of four .csv files.\n\n* <code>calendar.csv<\/code> - Contains the dates on which products are sold. The dates are in a <code>yyyy\/dd\/mm<\/code> format.\n\n* <code>sales_train_validation.csv<\/code> - Contains the historical daily unit sales data per product and store <code>[d_1 - d_1913]<\/code>.\n\n* <code>submission.csv<\/code> - Demonstrates the correct format for submission to the competition.\n\n* <code>sell_prices.csv<\/code> - Contains information about the price of the products sold per store and date.\n\n\nIn this kernel, we won't use the price data and validation and we focused more to demand attribute and training data.   \n","4b6be2b9":"We will cluster the products based on its demand (monthly demand). First, we transform the data in a matrix (products numbers x months) and the cell value \"demand\". So in this way we have will have 1 row with monthly sales for each products. We will cluster these products in order to analyse their trend over time. This will give us more insights over sales and the trend for every specific product. For instance, some products have an increase in the first 6 months, and then start decreasing. These products will be in the same cluster which will explain this trend. \nI have applied Kmeans and Kmedoids to see the differences between, but likely they seems to follow the same pattern .\n"}}