{"cell_type":{"2dbf7bc3":"code","1bff41e4":"code","d75a6f04":"code","af068f81":"code","dd9b1101":"code","98d9e7f8":"code","4875296e":"code","d6891470":"code","a06ff787":"code","1678a4af":"code","b4278403":"code","ab37b42c":"code","4114932a":"code","e6734f83":"code","29bb89ea":"code","f2f25cb5":"code","b8f8a46f":"code","b70bffa1":"code","c2b4c6eb":"code","19eba8db":"code","55067f69":"code","5454d2c6":"code","d2c8f3b5":"code","bc628d19":"code","397ffbfd":"code","b245bf16":"code","1ff60060":"code","3ce1fd2d":"code","bd411021":"code","e517a364":"code","c300ec0d":"code","4b41786d":"code","c44127fa":"markdown","713e593e":"markdown","3d7d37f6":"markdown","bb8503ba":"markdown","b6c8d66a":"markdown","10db5316":"markdown","ab951f7e":"markdown"},"source":{"2dbf7bc3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom keras.layers import Dense,Dropout,SimpleRNN,LSTM\nfrom keras.models import Sequential\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score","1bff41e4":"data = pd.read_csv(\"..\/input\/UTX_2006-01-01_to_2018-01-01.csv\")        # loading data (can be any stock)","d75a6f04":"data.head()    # EDA operations","af068f81":"data.info()","dd9b1101":"# I picked the \"Low\" column to work on\n\nLow_data = data.iloc[:,3:4].values","98d9e7f8":"Low_data           # checking the data","4875296e":"plt.figure(figsize=(14,10))                 # Visualizing the data\nplt.plot(Low_data,c=\"red\")\nplt.title(\"Microsoft Stock Prices\",fontsize=16)\nplt.xlabel(\"Days\",fontsize=16)\nplt.ylabel(\"Scaled Price\",fontsize=16)\nplt.grid()\nplt.show()","d6891470":"scaler = MinMaxScaler(feature_range=(0,1))           # Scaling the data between 1 and 0","a06ff787":"Low_scaled = scaler.fit_transform(Low_data)","1678a4af":"step_size = 21                      # days that are used for the following prediction\n\ntrain_x = []\ntrain_y = []","b4278403":"for i in range(step_size,3019):                # making feature and the label lists\n    train_x.append(Low_scaled[i-step_size:i,0])\n    train_y.append(Low_scaled[i,0])","ab37b42c":"train_x = np.array(train_x)                   # converting our lists to the arrays\ntrain_y = np.array(train_y)","4114932a":"print(train_x.shape)                                # checking the shape","e6734f83":"test_x = train_x[2500:]            # last 419 days are going to be used in test \ntrain_x = train_x[:2500]           # first 2500 days are going to be used in training\ntest_y = train_y[2500:]  \ntrain_y = train_y[:2500]","29bb89ea":"train_x = np.reshape(train_x, (2500, step_size, 1))           # reshaping them for the Keras model\ntest_x = np.reshape(test_x, (498, step_size, 1))","f2f25cb5":"print(train_x.shape)\nprint(test_x.shape)                             # checking the shapes again","b8f8a46f":"# Now we can start to create our models, starting with RNN","b70bffa1":"rnn_model = Sequential()","c2b4c6eb":"rnn_model.add(SimpleRNN(40,activation=\"tanh\",return_sequences=True, input_shape=(train_x.shape[1],1)))\nrnn_model.add(Dropout(0.15))\n\nrnn_model.add(SimpleRNN(40,activation=\"tanh\",return_sequences=True))\nrnn_model.add(Dropout(0.15))\n\nrnn_model.add(SimpleRNN(40,activation=\"tanh\",return_sequences=False))\nrnn_model.add(Dropout(0.15))\n\nrnn_model.add(Dense(1))","19eba8db":"rnn_model.compile(optimizer=\"adam\",loss=\"MSE\")","55067f69":"rnn_model.fit(train_x,train_y,epochs=20,batch_size=25)","5454d2c6":"rnn_predictions = rnn_model.predict(test_x)\n\nrnn_score = r2_score(test_y,rnn_predictions)","d2c8f3b5":"# Now with the next model, LSTM","bc628d19":"lstm_model = Sequential()","397ffbfd":"lstm_model.add(LSTM(40,activation=\"tanh\",return_sequences=True, input_shape=(train_x.shape[1],1)))\nlstm_model.add(Dropout(0.15))\n\nlstm_model.add(LSTM(40,activation=\"tanh\",return_sequences=True))\nlstm_model.add(Dropout(0.15))\n\nlstm_model.add(LSTM(40,activation=\"tanh\",return_sequences=False))\nlstm_model.add(Dropout(0.15))\n\nlstm_model.add(Dense(1))","b245bf16":"lstm_model.compile(optimizer=\"adam\",loss=\"MSE\")","1ff60060":"lstm_model.fit(train_x,train_y,epochs=20,batch_size=25)","3ce1fd2d":"lstm_predictions = lstm_model.predict(test_x)\n\nlstm_score = r2_score(test_y,lstm_predictions)","bd411021":"# Trainings are done. Now we can continue with Evaluation results","e517a364":"print(\"R^2 Score of RNN\",rnn_score)\nprint(\"R^2 Score of LSTM\",lstm_score)","c300ec0d":"lstm_predictions = scaler.inverse_transform(lstm_predictions)\nrnn_predictions = scaler.inverse_transform(rnn_predictions)\ntest_y = scaler.inverse_transform(test_y.reshape(-1,1))","4b41786d":"plt.figure(figsize=(16,12))\n\nplt.plot(test_y, c=\"blue\",linewidth=2, label=\"original\")\nplt.plot(lstm_predictions, c=\"green\",linewidth=2, label=\"LSTM\")\nplt.plot(rnn_predictions, c=\"red\",linewidth=2, label=\"RNN\")\nplt.legend()\nplt.title(\"COMPARISONS\",fontsize=20)\nplt.grid()\nplt.show()","c44127fa":"# **Introduction**\n### Investing your money on a stock is always risky. If you want to make a profit, you have to be careful and do smart predictions.\n### But what about if we train a RNN for making predictions for future? (Many-to-One RNN)\n### In this Notebook I will apply very similar Sequential models. One with <a href=\"#rnn\">SimpleRNN<\/a> layers and the other with <a href=\"#lstm\">LSTM<\/a> layers. \n### Then we can test how good are they for making good predictions and <a href=\"#eva\">Evaluate<\/a> the results.","713e593e":"### So after all, I think I am not able to be the wolf of the wall street (YET). But even this simple RNN model can help a lot to make better predicitons.\nThanks for reading.\n\n## Reference\nhttps:\/\/www.kaggle.com\/thebrownviking20\/intro-to-recurrent-neural-networks-lstm-gru","3d7d37f6":"<a id=\"eva\"><\/a>\n# Evaluation","bb8503ba":"<a id=\"rnn\"><\/a>\n# RNN","b6c8d66a":"<a id=\"lstm\"><\/a>\n# LSTM","10db5316":"# Data PreProcess","ab951f7e":"# Data"}}