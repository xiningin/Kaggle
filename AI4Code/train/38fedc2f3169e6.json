{"cell_type":{"56bfbb6a":"code","111c83bd":"code","f5a9ad6d":"code","a7b02a2a":"code","1391c0cc":"code","d26afc3e":"code","effb8f59":"code","81d9bba7":"code","1d115004":"code","3b39a269":"code","165ab9dc":"code","73b21491":"code","e821aed0":"code","33bf74ac":"code","1aeeb180":"code","e3cddf98":"code","eb01ed29":"code","6756aa60":"code","800bf386":"code","40604ac9":"code","b274de19":"code","db1585dd":"code","2f66608a":"code","30734efe":"code","815ff085":"code","b1af50d6":"code","241cf4a3":"code","bb758f23":"code","8250e7c1":"code","f7dbf542":"code","422798a2":"code","2d6d87eb":"code","a54c2d51":"code","494f540f":"code","ed0927d0":"code","e0546004":"code","0ae9421b":"code","561f16a8":"code","e1919bed":"code","3098fcb6":"code","a0251fea":"code","5dd7f231":"code","01835937":"code","650d8e2d":"code","bc953808":"code","264babb2":"code","0b526067":"code","b5c8bf07":"code","1f3bc91d":"code","0a3810ae":"code","17e342a5":"code","d9b04ee5":"code","7ad8b480":"code","cef1168f":"code","d61f7477":"code","c58a604d":"code","e5f750e9":"code","2db5129a":"code","e7105d0d":"code","c7de6cb8":"code","618388af":"code","5565c5c4":"code","9c54410d":"code","56a0285e":"code","c0ebd29b":"code","a9e951d8":"code","eb87b2b5":"code","2d6f710b":"code","51defe52":"code","efc83191":"code","a0efe5b2":"code","d5dbaa3e":"code","d7d3babb":"code","ad6ab56d":"code","79d5ad41":"code","c26e1ea7":"code","eb200cde":"code","7d68d708":"code","4dbcaf89":"code","538385a7":"code","7b061f6e":"code","89831e1e":"code","8a994a56":"code","9c7f19b6":"code","70e5a397":"code","9b835414":"code","b7cc06b8":"code","af6b9a6d":"code","e2274fb2":"code","c1732bd1":"code","778043db":"code","c5b66a57":"code","f4b13a56":"code","e4f2afc7":"code","ab4273db":"code","c018f4ab":"code","1ab53f21":"code","9213b921":"code","096cf1ef":"code","6b1f7837":"code","7754070f":"code","447ec0fb":"markdown","46d6be48":"markdown","a6fa7083":"markdown","22274433":"markdown","545c0c31":"markdown","f1839f2d":"markdown","ae74ec20":"markdown","78204a14":"markdown","fb90384d":"markdown","4f06c3bd":"markdown","c06bd53f":"markdown","e2dda7f1":"markdown","057c2c27":"markdown","1fdf722e":"markdown","e0df081f":"markdown","aa3fee0d":"markdown","2ca19fe2":"markdown","0d695224":"markdown","766bc712":"markdown","87b59a80":"markdown","1f392542":"markdown","83661155":"markdown","0f024efc":"markdown","2f67f9a3":"markdown","866cb74f":"markdown","7b839a84":"markdown","4dd8855b":"markdown","d7cc9fd8":"markdown","19a1628a":"markdown","7b2353ba":"markdown","0159ba62":"markdown","988223b6":"markdown","8b2624c2":"markdown","9b61403a":"markdown","5ade40df":"markdown","cd69622c":"markdown","433e3522":"markdown","f7766de3":"markdown","2f54c701":"markdown","f462013e":"markdown","60d94438":"markdown","c3fad741":"markdown","ad83e79c":"markdown","7559d947":"markdown","01d3d1c2":"markdown"},"source":{"56bfbb6a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas_profiling as pp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.metrics import accuracy_score,confusion_matrix\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport time\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","111c83bd":"pip install openpyxl","f5a9ad6d":"df = pd.read_excel('\/kaggle\/input\/flight-fare-prediction-mh\/Data_Train.xlsx')\ndf.head(10)","a7b02a2a":"df.info()","1391c0cc":"df.describe()","d26afc3e":"df.shape","effb8f59":"pp.ProfileReport(df)","81d9bba7":"df.dropna(inplace = True)","1d115004":"df.isnull().sum()","3b39a269":"df.dtypes","165ab9dc":"df.head()","73b21491":"def transform_to_datetime(column):\n    df[column] = pd.to_datetime(df[column])","e821aed0":"df.columns","33bf74ac":"for i in ['Date_of_Journey','Dep_Time','Arrival_Time']:\n    transform_to_datetime(i)","1aeeb180":"df.dtypes","e3cddf98":"df['Journey_Day'] = df['Date_of_Journey'].dt.day\ndf['Journey_Month'] = df['Date_of_Journey'].dt.month","eb01ed29":"df.head()","6756aa60":"df.drop('Date_of_Journey', axis=1, inplace = True)","800bf386":"def extract_hour(data,column):\n    data[column +'_hour'] = data[column].dt.hour\n    \ndef extract_min(data,column):\n    data[column +'_min'] = data[column].dt.minute\n    \ndef drop_col(data,column):\n    data.drop(column, axis = 1, inplace = True)","40604ac9":"# For \"Dep_Time\" variable\n# Extracting Hour from the variable\nextract_hour(df,'Dep_Time')\n\n# Extracting Minute from the variable\nextract_min(df,'Dep_Time')\n\n# Dropping the variable\ndrop_col(df,'Dep_Time')","b274de19":"# For \"Arrival_Time\" variable\n# Extracting Hour from the variable\nextract_hour(df,'Arrival_Time')\n\n# Extracting Minute from the variable\nextract_min(df,'Arrival_Time')\n\n# Dropping the variable\ndrop_col(df,'Arrival_Time')","db1585dd":"df.head()","2f66608a":"# To make it consistent add 0 mins in case it is just hour component to make it equivalent values across for us to handle\nduration = list(df['Duration'])\nfor i in range(len(duration)):\n    if len(duration[i].split(' ')) == 2:\n        pass\n    else:\n        if 'h' in duration[i]:               # Check if Duration contains only hour component\n             duration[i]=duration[i] + ' 0m' # Adds 0 minute to it\n        else:\n             duration[i]='0h '+ duration[i]","30734efe":"df['Duration'] = duration","815ff085":"df.head()","b1af50d6":"# Write functions to transform and handle\n\ndef hour(x):\n    return x.split(' ')[0][0:-1]\n\ndef minutes(x):\n    return x.split(' ')[1][0:-1]","241cf4a3":"df['Duration_hour'] = df['Duration'].apply(hour)\ndf['Duration_min'] = df['Duration'].apply(minutes)","bb758f23":"df.head()","8250e7c1":"drop_col(df,'Duration')","f7dbf542":"df.head()","422798a2":"# Convert data types\n\ndf['Duration_hour'] = df['Duration_hour'].astype(int)\ndf['Duration_min'] = df['Duration_min'].astype(int)","2d6d87eb":"df.dtypes","a54c2d51":"categorical_column = [column for column in df.columns if df[column].dtype=='object']\ncategorical_column","494f540f":"continuous_column =[column for column in df.columns if df[column].dtype!='object']\ncontinuous_column","ed0927d0":"categorical = df[categorical_column]","e0546004":"categorical.head()","0ae9421b":"categorical['Airline'].value_counts()","561f16a8":"plt.figure(figsize=(15,8))\nsns.boxplot(x='Airline',y='Price',data=df.sort_values('Price',ascending=False))","e1919bed":"plt.figure(figsize=(15,8))\nsns.boxplot(x='Total_Stops',y='Price',data=df.sort_values('Price',ascending=False))","3098fcb6":"Airline = pd.get_dummies(categorical['Airline'],drop_first=True)","a0251fea":"Airline.head()","5dd7f231":"categorical['Source'].value_counts()","01835937":"plt.figure(figsize=(15,15))\nsns.catplot(x='Source',y='Price',data=df.sort_values('Price',ascending=False),kind='boxen')","650d8e2d":"source = pd.get_dummies(categorical['Source'],drop_first=True)\nsource.head()","bc953808":"categorical['Destination'].value_counts()","264babb2":"plt.figure(figsize=(15,8))\nsns.boxplot(x='Destination',y='Price',data=df.sort_values('Price',ascending=False))","0b526067":"destination = pd.get_dummies(categorical['Destination'],drop_first=True)\ndestination.head()","b5c8bf07":"categorical['Route'].value_counts()","1f3bc91d":"categorical['Route1']=categorical['Route'].str.split('\u2192').str[0]\ncategorical['Route2']=categorical['Route'].str.split('\u2192').str[1]\ncategorical['Route3']=categorical['Route'].str.split('\u2192').str[2]\ncategorical['Route4']=categorical['Route'].str.split('\u2192').str[3]\ncategorical['Route5']=categorical['Route'].str.split('\u2192').str[4]","0a3810ae":"categorical.head()","17e342a5":"drop_col(categorical,'Route')","d9b04ee5":"categorical.isnull().sum()","7ad8b480":"categorical.columns","cef1168f":"for i in ['Route3', 'Route4', 'Route5']:\n    categorical[i].fillna('None',inplace=True)","d61f7477":"categorical.isnull().sum()","c58a604d":"for i in categorical.columns:\n    print('{} has total {} categories'.format(i,len(categorical[i].value_counts())))","e5f750e9":"df.plot.hexbin(x='Arrival_Time_hour',y='Price',gridsize=15)","2db5129a":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()","e7105d0d":"for i in ['Route1', 'Route2', 'Route3', 'Route4', 'Route5']:\n    categorical[i]=encoder.fit_transform(categorical[i])","c7de6cb8":"categorical.head()","618388af":"drop_col(categorical,'Additional_Info')","5565c5c4":"categorical['Total_Stops'].unique()","9c54410d":"dict={'non-stop':0, '2 stops':2, '1 stop':1, '3 stops':3, '4 stops':4}\ncategorical['Total_Stops']=categorical['Total_Stops'].map(dict)","56a0285e":"categorical['Total_Stops']","c0ebd29b":"drop_col(categorical,'Source')\ndrop_col(categorical,'Destination')\ndrop_col(categorical,'Airline')","a9e951d8":"categorical.head()","eb87b2b5":"final_df = pd.concat([categorical,Airline,source,destination,df[continuous_column]],axis=1)","2d6f710b":"final_df.head()","51defe52":"final_df.info()","efc83191":"def plot(data,col):\n    fig,(ax1,ax2)=plt.subplots(2,1)\n    sns.distplot(data[col],ax=ax1)\n    sns.boxplot(data[col],ax=ax2)","a0efe5b2":"plot(final_df,'Price')","d5dbaa3e":"final_df['Price']=np.where(final_df['Price']>=40000,final_df['Price'].median(),final_df['Price'])","d7d3babb":"plot(final_df,'Price')","ad6ab56d":"X = final_df.drop('Price',axis=1)\ny = df['Price']","79d5ad41":"from sklearn.feature_selection import mutual_info_classif","c26e1ea7":"%time\n\nmutual_info_classif(X,y)","eb200cde":"importance = pd.DataFrame(mutual_info_classif(X,y),index=X.columns)\nimportance","7d68d708":"importance.columns=['importance']\nimportance.sort_values(by='importance',ascending=False)","4dbcaf89":"X.shape","538385a7":"duplicate_columns = X.columns[X.columns.duplicated()]\nduplicate_columns","7b061f6e":"X = X.loc[:,~X.columns.duplicated()]","89831e1e":"X.shape","8a994a56":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=123) # We start with 80-20 split","9c7f19b6":"# We will try with GBM, RF and XGB regressors\n\nfrom sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\nimport xgboost as xgb\nfrom xgboost import XGBRegressor","70e5a397":"%time \n\nml_model1 = GradientBoostingRegressor()\nmodel1 = ml_model1.fit(X_train,y_train)\npredictions = model1.predict(X_test)\n\nR2Score = r2_score(y_test,predictions)\nMAE = format(mean_absolute_error(y_test,predictions))\nMSE = format(mean_squared_error(y_test,predictions))\nRMSE = format(np.sqrt(mean_squared_error(y_test,predictions)))\n\nsns.distplot(y_test-predictions)\n\nresults_df = pd.DataFrame(data=[[\"GBM Regressor\", \"{:.2f}\".format(float(RMSE)), \"{:.3f}\".format(float(R2Score))]], \n                          columns=['Model\/Algorithm', 'RMSE', 'R2 Score'])\nresults_df\n","9b835414":"%time\n\nml_model2 = RandomForestRegressor()\nmodel2 = ml_model2.fit(X_train,y_train)\npredictions = model2.predict(X_test)\n\nR2Score = r2_score(y_test,predictions)\nMAE = format(mean_absolute_error(y_test,predictions))\nMSE = format(mean_squared_error(y_test,predictions))\nRMSE = format(np.sqrt(mean_squared_error(y_test,predictions)))\n\nsns.distplot(y_test-predictions)\n\nnew_results_df = pd.DataFrame(data=[[\"RF Regressor\", \"{:.2f}\".format(float(RMSE)), \"{:.3f}\".format(float(R2Score))]], \n                          columns=['Model\/Algorithm', 'RMSE', 'R2 Score'])\nresults_df = results_df.append(new_results_df, ignore_index=True)\n\nresults_df\n","b7cc06b8":"%time\n\nml_model3 = XGBRegressor(learning_rate=0.1, n_estimators=200)\nmodel3 = ml_model3.fit(X_train,y_train)\npredictions = model3.predict(X_test)\n\nR2Score = r2_score(y_test,predictions)\nMAE = format(mean_absolute_error(y_test,predictions))\nMSE = format(mean_squared_error(y_test,predictions))\nRMSE = format(np.sqrt(mean_squared_error(y_test,predictions)))\n\nsns.distplot(y_test-predictions)\n\nnew_results_df = pd.DataFrame(data=[[\"XGBoost Regressor\", \"{:.2f}\".format(float(RMSE)), \"{:.3f}\".format(float(R2Score))]], \n                          columns=['Model\/Algorithm', 'RMSE', 'R2 Score'])\nresults_df = results_df.append(new_results_df, ignore_index=True)\n\nresults_df","af6b9a6d":"from sklearn.model_selection import RandomizedSearchCV\n\nrandom_grid = {\n    'n_estimators' : [100, 120, 150, 180, 200,220],\n    'max_features':['auto','sqrt'],\n    'max_depth':[5,10,15,20],\n    }\n\nrf = RandomForestRegressor()\nrf_random = RandomizedSearchCV(estimator=rf,param_distributions=random_grid,cv=4,verbose=2,n_jobs=-1)\n\nrf_random.fit(X_train,y_train)\n\n# best parameter\nrf_random.best_params_","e2274fb2":"#predicting the values\nprediction = rf_random.predict(X_test)\n\n#distribution plot between actual value and predicted value\nsns.displot(y_test-prediction)","c1732bd1":"# Adding into Results Dataframe for easy reading of outcome analysis\nR2Score = r2_score(y_test,prediction)\nRMSE = format(np.sqrt(mean_squared_error(y_test,prediction)))\n\nnew_results_df = pd.DataFrame(data=[[\"RF Regressor (Tuned)\", \"{:.2f}\".format(float(RMSE)), \"{:.3f}\".format(float(R2Score))]], \n                          columns=['Model\/Algorithm', 'RMSE', 'R2 Score'])\nresults_df = results_df.append(new_results_df, ignore_index=True)\n\nresults_df","778043db":"%time\n\nparams = {'max_depth': [1, 2, 3, 4, 5], \n          'learning_rate': [ 0.1], \n          'n_estimators': [100, 200, 300, 400, 500], \n          'reg_lambda': [0.001, 0.1, 1.0, 10.0, 100.0],\n          'objective' : ['reg:squarederror'],\n          'colsample_bytree': [0.3]\n         }\n\nml_model5 = RandomizedSearchCV(XGBRegressor(), params, n_iter=20, scoring='neg_mean_absolute_error', cv=5, n_jobs=1, random_state=124)\nml_model5.fit(X_train, y_train)\n\npredictions = ml_model5.predict(X_test)\n#predictions\n\nR2Score = r2_score(y_test,predictions)\nMAE = format(mean_absolute_error(y_test,predictions))\nMSE = format(mean_squared_error(y_test,predictions))\nRMSE = format(np.sqrt(mean_squared_error(y_test,predictions)))\n\nsns.distplot(y_test-predictions)\n\nnew_results_df = pd.DataFrame(data=[[\"XGBoost Regressor (Tuned)\", \"{:.2f}\".format(float(RMSE)), \"{:.3f}\".format(float(R2Score))]], \n                          columns=['Model\/Algorithm', 'RMSE', 'R2 Score'])\nresults_df = results_df.append(new_results_df, ignore_index=True)\n\nresults_df","c5b66a57":"import shap","f4b13a56":"shap.initjs()","e4f2afc7":"explainer = shap.TreeExplainer(model3)  # consider the latest model which has shown the better model performance so far\nshap_values = explainer.shap_values(X_test)","ab4273db":"X_shap = pd.DataFrame(shap_values)\nX_shap.head()","c018f4ab":"print('Expected Value from SHAP: ', explainer.expected_value)","1ab53f21":"shap.summary_plot(shap_values, X_test, plot_type='bar', color = 'blue')","9213b921":"shap.force_plot(explainer.expected_value,shap_values[0,:],X_test.iloc[0,:])","096cf1ef":"shap.force_plot(explainer.expected_value, shap_values[:1000,:], X_test.iloc[:1000,:])","6b1f7837":"shap.summary_plot(shap_values,X_test)","7754070f":"shap.dependence_plot(ind='Route2', interaction_index='Journey_Day',\n                     shap_values=shap_values, \n                     features=X_test,  \n                     display_features=X_test)","447ec0fb":"### From above chart, we observe that most of median values are near vicinity. New Delhi, Cochin, Bangalore are relatively higher and closer. Rest 3 are relatively lower.","46d6be48":"# 4.2 Algorithms \/ Methods","a6fa7083":"#### Applying Label Encoder","22274433":"### Analysis: Source vs Price","545c0c31":"# 3.5 Feature Selection","f1839f2d":"# 1. Import Libraries","ae74ec20":"# 2. Get Dataset","78204a14":"### Check for Duplicate Feature Names (if any) prior to running algorithms","fb90384d":"# 5. MODEL TUNING and EVALUATION","4f06c3bd":"#### Time variables will be handled (e.g. Dep_Time, Arrival_Time) as follows","c06bd53f":"# 4.1 Split the dataset","e2dda7f1":"#### Handle Categorical Values","057c2c27":"### Random Forest now has slightly better r2 Score with some improvement after tuning.\n\n### However, the XGBoost Algorithm remains the best with higher R2 Score and lower RMSE. That's the best model so far. We will perform further experiments on it's tuning and other methods subsequently.\n\n### Best Model so far: XGBoost Regressor: (RMSE:1525.74 , R2Score:87.9%)\n","1fdf722e":"#### We don't need \"Additional_Info\" variable. Let us delete it.","e0df081f":"# 3.3 Handle Categorical and Continuous variables\n\n#### Now Let us find out Categorical and Continuous variables","aa3fee0d":"# 6. Interpretation with SHAP","2ca19fe2":"#### Let's Encode \"Total_Stops\"","0d695224":"#### Selection of features: We will extract day and month from \"Date_of_Journey\" and store into 2 separate variables and drop the parent variable as we no longer need the same.","766bc712":"#### This does not give any better R2 Score. Hence the XGBoost regressor model with 87.9% R2 Score is the best model so far.","87b59a80":"#### Now, we will encode the \"Source\" variable","1f392542":"### Let us now consolidate our data frame","83661155":"#### What we are getting here is a matrix of SHAP values with the same shape as the original X_test set. Each row adds up to the difference between the model output for that sample and the expected value of the model output (which is stored as expected_value attribute of the explainer). \n\n#### Usually, the difference helps us in explaining why the model is inclined on predicting a specific class outcome.","0f024efc":"#### Let's now transform \"Duration\" variable and extract \"Hour\" and \"Min\" from it","2f67f9a3":"### Feature Importance with SHAP for the xgb model","866cb74f":"### From the above chart, we can observe that - \"Non-stop\" travel has lower price. However all of them have similar median with some variation.\n\n#### As Airline is Nominal Categorical variable, we will perform OneHotEncoding.","7b839a84":"# 3.1 AutoEDA: Using Pandas Profile Report","4dd8855b":"# 5.1 Tuning of Random Forest Regressor","d7cc9fd8":"### Analysis on \"Route\" variable","19a1628a":"### Analysis: Airline vs Price","7b2353ba":"# 3.4 Detect Outliers and Treat\n","0159ba62":"### Since we have some duplicate features, we need to handle them","988223b6":"# 3. Data Understanding and EDA","8b2624c2":"# 4. MODELING","9b61403a":"# Flight Fare Prediction ","5ade40df":"#### Now, there are no missing values. We can proceed with next steps.","cd69622c":"### Analysis: Destination vs Price","433e3522":"# 3.2 Handle DATE columns\n\n#### We need to transform DATE fields as they are of type objects","f7766de3":"### From above chart, we can observe that - Jet Airways has relatively very high price compared to rest carriers.","2f54c701":"### Analysis: Total_Stops vs Price","f462013e":"### There are outliers. We will apply median values to replace them","60d94438":"#### Now, we will encode the \"Destination\" variable","c3fad741":"# 5.2 Tuning of XGBoost Regressor","ad83e79c":"#### XGBoost Regressor has RMSE = 1525.74 and r2 Score = 87.9%\n#### RandomForest Regressor has RMSE = 1730.62 and r2 Score = 84.4%\n#### GBM Regressor has RMSE = 1878.10 and r2 Score = 81.7%\n\n### So XGBoost is the best with r2 Score = 87.9%","7559d947":"## Get the data frame - X (Independent) and Y (Target) variables","01d3d1c2":"#### The \"Pandas_Profiling\" has provided us with some quick insights. \n#### There are 2 missing values. So we can remove them."}}