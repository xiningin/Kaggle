{"cell_type":{"a935c2d9":"code","4d4bb7b6":"code","37275efc":"code","da356dae":"code","bfb7f155":"code","ab5d2506":"code","7e90a8e2":"code","93ab7a73":"code","28c5fe2d":"code","08026390":"code","2fc8e64f":"code","29cecb57":"code","3fd5ad47":"code","db82e45f":"code","720d644f":"code","26d82123":"code","14723036":"code","abbd78d1":"code","c729ce71":"code","c3184af9":"code","c272047b":"code","31159c98":"code","4722baf7":"code","dcaf0852":"code","ee1173f3":"code","a01eb2ad":"code","f85f4e98":"code","b8e76eec":"code","211af6b3":"code","1e78486d":"code","0cfaaddd":"code","c52d8b28":"code","589489c1":"code","75dd83fd":"code","26acd781":"code","171084e7":"code","9f1c09f1":"code","52d94eca":"code","6185a9bd":"code","bec18b49":"code","4b88b643":"code","b546b1c2":"code","72b94de9":"code","e95909c6":"code","bc037748":"code","678e613c":"code","571f37e5":"code","7b1e6c91":"code","9c70b3f2":"code","f1f664ff":"code","d13fcc3a":"code","ef2c01a1":"code","37912e22":"code","12388ec0":"code","372b1b37":"code","8ed4d7fc":"code","0f9503e1":"code","9b831806":"code","1d1ca46a":"code","e53d7e44":"code","acb6a8dd":"code","2b0f3ef7":"code","d4c18ba8":"code","9ee88c08":"code","68447076":"code","7702c921":"markdown","b05166ed":"markdown","09c7105f":"markdown","dedfbd13":"markdown","4c02fe15":"markdown","e2c7646a":"markdown","90024208":"markdown","644ca7af":"markdown","ba24d842":"markdown","92552181":"markdown","9444513a":"markdown","dde6b556":"markdown","ebbbb567":"markdown","9af3f857":"markdown","2d455ff1":"markdown","83ea0d68":"markdown","3cefb401":"markdown","16f67a9b":"markdown"},"source":{"a935c2d9":"import pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\nfrom scipy.stats import norm","4d4bb7b6":"# Read in the dataset and take a look at the top few rows\ndf = pd.read_csv('..\/input\/ab_data.csv')\ndf.head()","37275efc":"df.info()\n#The total of users is 294,478","da356dae":"# check the number of unique users in the dataset.\ndf.user_id.nunique()","bfb7f155":"#Check the proportion of users converted\np= df.query('converted == 1').user_id.nunique()\/df.shape[0]\n\nprint(\"The proportion of users converted is {0:.2%}\".format(p))","ab5d2506":"# Check the number of times the new_page and treatment don't line up.\nl = df.query('(group == \"treatment\" and landing_page != \"new_page\" ) \\\n         or (group != \"treatment\" and landing_page == \"new_page\")').count()[0]\nprint(\"The number of times the new_page and treatment don't line up is {}\".format(l))","7e90a8e2":"#Check missing values\ndf.isnull().sum()","93ab7a73":"df2 =df.drop(df.query('(group == \"treatment\" and landing_page != \"new_page\" ) \\\n                      or (group != \"treatment\" and landing_page == \"new_page\") or (group == \"control\" and landing_page != \"old_page\") or (group != \"control\" and landing_page == \"old_page\")').index)","28c5fe2d":"# Double Check all of the correct rows were removed \ndf2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]","08026390":"#Check the number of unique user_ids are in df2\ndf2.user_id.nunique()","2fc8e64f":"df2.head()","29cecb57":"df['landing_page'].value_counts().plot(kind='bar', figsize=(8,8));","3fd5ad47":"df['landing_page'].value_counts().plot(kind='pie', figsize=(8,8));","db82e45f":"lan_rev =df.groupby('landing_page').sum()['converted']\n\nind = np.arange(len(lan_rev))  # the x locations for the groups\nwidth = 0.35  \n\nplt.subplots(figsize=(18,10))\ngen_bars =plt.bar(ind, lan_rev, width, color='g', alpha=.7)\n#adv_bars =plt.bar(ind, adv, width, color='b', alpha=.7, label=\"Adventure\")\nplt.ylabel('converted',size=14) # title and labels\nplt.xlabel('landing_page',size=14)\nplt.title('Conversion by landing_page',size=18)\nlocations = ind + width \/ 2  # xtick locations\nlabels = ['old_page', 'new_page']  # xtick labels\nplt.xticks(locations, labels)","720d644f":"#Check duplicates rows\ndf2.user_id.duplicated().sum()","26d82123":"#Check the repeated user_id\ndf2[df2.duplicated(['user_id'],keep=False)]['user_id']\nprint(\"The user_id repeated is 773192\")","14723036":"# Check the row information for the repeat user_id\ndf2.query('user_id == 773192')","abbd78d1":"#Remove the duplicated rows\ndf2 = df2.drop(df2.query('user_id == 773192 and timestamp == \"2017-01-09 05:37:58.781806\"').index)","c729ce71":"#Check if there is any repeated user_id \ndf2.user_id.duplicated().sum()","c3184af9":"# Calculate the probability of an individual converting regardless of the page they receive\ndf_prob =df2.query('converted == 1').user_id.nunique()\/df2.user_id.nunique()\ndf_prob\n\nprint(\"The probability of an individual converting regardless of the page they receive is {0:.2%}\".format(df_prob))","c272047b":"# Calculate the probabilty the individual was in the control group to convert\np_cont = df2.query('converted == 1 and group == \"control\"').user_id.nunique() \\\n\/df2.query('group == \"control\"').user_id.nunique()\n\nprint(\"The probability they converted based on control group is {0:.2%}\".format(p_cont))","31159c98":"# Calculate the probabilty the individual was in the treatment group to convert\np_treat = df2.query('converted == 1 and group == \"treatment\"').user_id.nunique() \\\n\/df2.query('group == \"treatment\"').user_id.nunique()\n\nprint(\"The probability they converted based on treatment group is {0:.2%}\".format(p_treat))","4722baf7":"# Calculate the probabilty that an individual received the new page\np_n = df2.query('landing_page == \"new_page\"').user_id.nunique()\/df2.user_id.nunique()\n#The probability that an individual received the new page is 50.00%\nprint(\"The probability that an individual received the new page is {0:.2%}\".format(p_n))","dcaf0852":"# Since P_new and P_old both have \"true\" success rates equally, their converted rate \n#will have the same result.\np_new = df2.converted.mean()\n\nprint(\"The convert rate for p_new under the null is {0:.4}\".format(p_new))","ee1173f3":"# Since P_new and P_old both have \"true\" success rates equally, their converted rate \n#will have the same result.\np_old = df2.converted.mean()\n\nprint(\"The convert rate for p_old under the null is {0:.4}\".format(p_old))","a01eb2ad":"# Count the total unique users with new page\nn_new = df2.query('landing_page == \"new_page\" ').count()[0]\nn_new","f85f4e98":"# Count the total unique users with old page\nn_old = df2.query('landing_page == \"old_page\" ').count()[0]\nn_old","b8e76eec":"#Simulate n_new transactions with a convertion rate of  p_new under the null. \n#Store these n_new 1's and 0's in new_page_converted\n\nnew_page_converted = np.random.choice([0,1],n_new, p=(p_new,1-p_new))\nnew_page_converted","211af6b3":"#Simulate n_new transactions with a convert rate of  p_old under the null. \n#Store these  n_new 1's and 0's in old_page_converted\n\nold_page_converted = np.random.choice([0,1],n_old, p=(p_old,1-p_old))\nold_page_converted","1e78486d":"# Find the difference between p_new and p_old\n#For discovering the difference between p_new and p_old, it is necessary to find out the mean \n#of new_page_converted and old_page_converted.\nnew_page_converted.mean()","0cfaaddd":"old_page_converted.mean()","c52d8b28":"#diff_conv is the difference between p_new and p_old.\ndiff_conv = new_page_converted.mean() - old_page_converted.mean()\ndiff_conv","589489c1":"# Simulate 10,000 p_new - p_old values with random binomial\n\nnew_converted_simulation = np.random.binomial(n_new, p_new,  10000)\/n_new\nold_converted_simulation = np.random.binomial(n_old, p_old,  10000)\/n_old\np_diffs = new_converted_simulation - old_converted_simulation","75dd83fd":"p_diffs = np.array(p_diffs)","26acd781":"plt.hist(p_diffs);","171084e7":"# Calculate actual difference observed\nnew_convert = df2.query('converted == 1 and landing_page == \"new_page\"').count()[0]\/n_new\nold_convert = df2.query('converted == 1 and landing_page == \"old_page\"').count()[0]\/n_old\nobs_diff = new_convert - old_convert\nobs_diff","9f1c09f1":"#Check the proportion of the p_diffs are greater than the actual difference observed in ab_data.\nnull_vals = np.random.normal(0, p_diffs.std(), p_diffs.size)","52d94eca":"plt.hist(null_vals);\nplt.axvline(x=obs_diff, color='red')","6185a9bd":"(null_vals > obs_diff).mean()","bec18b49":"convert_old = df2.query('converted == 1 and landing_page == \"old_page\"').count()[0]\nconvert_new = df2.query('converted == 1 and landing_page == \"new_page\"').count()[0]\nn_old = df2.query('landing_page == \"old_page\" ').count()[0]\nn_new = df2.query('landing_page == \"new_page\" ').count()[0]","4b88b643":"convert_old,convert_new,n_old,n_new","b546b1c2":"z_score, p_value = sm.stats.proportions_ztest(np.array([convert_new,convert_old]),\\\n                                              np.array([n_new,n_old]), alternative = 'larger')","72b94de9":"z_score, p_value","e95909c6":"norm.cdf(z_score)\n#0.09494168724097551 # Tells us how significant our z-score is","bc037748":"norm.ppf(1-(0.05\/2))\n# 1.959963984540054 # Tells us what our critical value at 96% confidence is","678e613c":"df2.head()","571f37e5":"#Create intercept and dummies columns\ndf2['intercept'] = 1\ndf2[['ab_page','old_page']] = pd.get_dummies(df2['landing_page'])\ndf2 = df2.drop('old_page', axis = 1)","7b1e6c91":"df2.head()","9c70b3f2":"#Create a model\nlog = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])\nresults = log.fit()\nresults.summary()","f1f664ff":"countries_df = pd.read_csv('..\/input\/countries.csv')\n","d13fcc3a":"#Merge the countries data frame with df2\ndf3 = df2.merge(countries_df, on='user_id', how='inner')\ndf3.head()","ef2c01a1":"df3.country.unique()","37912e22":"### Create the necessary dummy variables\ndf3[['US', 'CA', 'UK']] = pd.get_dummies(df3['country'])\ndf3 = df3.drop(['country', 'US'], axis = 1)","12388ec0":"df3.head()","372b1b37":"#Print Summary\nlog2 = sm.Logit(df3['converted'], df3[['intercept','ab_page','CA','UK']])\nresults2 = log2.fit()\nresults2.summary()","8ed4d7fc":"# For better visualizing the coef, we exponentiated them with numpy.\n1\/np.exp(-0.0149),np.exp(0.0408), np.exp(0.0506)","0f9503e1":"df3.head()","9b831806":"#For understanding the interaction between page and country we need two create \n#two columns that multiple ab_page to the country.\n\ndf3['CA_new_page']=df3['ab_page']*df3['CA']\ndf3['UK_new_page']=df3['ab_page']*df3['UK']","1d1ca46a":"df3.head()","e53d7e44":"### Print Summary\nlog3 = sm.Logit(df3['converted'], df3[['intercept', 'ab_page', 'CA', 'UK','CA_new_page','UK_new_page']])","acb6a8dd":"results3 = log3.fit()\nresults3.summary()","2b0f3ef7":"# For better visualizing the coef, we exponentiated them with numpy.\n1\/np.exp(-0.0674),np.exp(0.0118), np.exp(0.0175),np.exp(0.0783),np.exp(0.0469)","d4c18ba8":"X = df3[['CA','UK','CA_new_page','UK_new_page']]\ny = df3['converted']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)","9ee88c08":"log_mod = LogisticRegression()\nlog_mod.fit(X_train, y_train)\npreds = log_mod.predict(X_test)\nconfusion_matrix(y_test, preds)","68447076":"accuracy_score(y_test, preds)","7702c921":"#### Regression analysis I\n1- The **p-value** associated with ab_page is 0.19 and the **p-value** in **Part II** was 0.90. So, in both cases, we fail to reject the null hyphothesis because these two p-values are greater than 0.05(Type Error I).\n\n2- The difference lies in what each test assumes for their hypothesis. In Part II, the hyphothesis is to analyze if the old page is better unless the new page proves to be definitely greater at a Type I error rate of 5%. In other words,we were concerned with which page had a higher conversion rate, so a one-tailed test. While in the Part III hyphotheses, there is a significant difference in conversion based on which page a customer receives.  The nature of a regression test is not concerned with which had a positive or negative change, specifically. It is concerned with if the condition had any effect at all, so a two-tailed test.","b05166ed":"## A\/B Test Results an e-commerce website\n\n\n\n## Table of Contents\n- [Introduction](#intro)\n- [Part I Wrangle data & Exploratory Data Analysis](#wrangle)\n- [Part II A\/B Test](#ab_test)\n- [Part III Regression](#regression)\n- [Conclusion](#conclusion)\n\n<a id='intro'><\/a>\n### Introduction\n\n For this project, I will be working to understand the results of an A\/B test run by an e-commerce website. My goal is to help the company understand if they should implement the new page, keep the old page, or perhaps run the experiment longer to make their decision.","09c7105f":"<a id='regression'><\/a>\n### Part III - A regression approach\n\nIn this final part, I will confirm that the result acheived in the previous A\/B test can also be acheived by performing logistic regression. I will use **statsmodels** to fit the regression model to see if there is a significant difference in conversion based on which page a customer receives. <br><br> ","dedfbd13":"#### Regression approach II (add countries)\nNow, I will analyze if the countries have an impact on conversion. So, I will along with testing if the conversion rate changes for different pages, also add an effect based on which country a user lives. I will read in the **countries.csv** dataset and merge together my datasets on the approporiate rows. ","4c02fe15":"#### Regression analysis III\n\n>For each 1 unit decrease in new_page, convert is 1.5 times as likely holding all else constant.\n\n>For each 1 unit increase in CA, convert is 1.2 times as likely holding all else constant.\n\n>For each 1 unit increase in UK, convert is 1.7 times as likely holding all else constant.\n\n>For each 1 unit increase in CA new_page, convert is 8.1 times as likely holding all else constant.\n\n>For each 1 unit increase in UK new_page, convert is 4.8 times as likely holding all else constant.","e2c7646a":"#### Note\nFor the rows where **treatment** is not aligned with **new_page** or **control** is not aligned with **old_page**, we can not be sure if this row truly received the new or old page. So, I will drop these rows and create a new dataframe.  ","90024208":"<a id='ab_test'><\/a>\n### Part II - A\/B Test\n\nSince we do not have  sufficient evidence to say that the new treatment page leads to more conversions than the control page with probability tests, I will run a hypothesis test continuously as each observation was observed with the time stamp associated with each event. \n \n`1.`I will consider making the decision only based on all the data provided. Further, I want to assume that the old page is better unless the new page proves to be definitely better at a Type I error rate of 5%. The null and alternative hypotheses follow below:\n\n> $$H_0:  P_{new} - P_{old}  \\leq  0$$\n\n\n> $$H_1: P_{new} - P_{old} > 0$$","644ca7af":"`2.` I will assume under the null hypothesis, $p_{new}$ and $p_{old}$ both have \"true\" success rates equal to the **converted** success rate regardless of page -- that is $p_{new}$ and $p_{old}$ are equal. Furthermore, I will assume they are equal to the **converted** rate in **ab_data.csv** regardless of the page. <br><br>\n\nI wil use a sample size for each page equal to the ones in **ab_data.csv**.  <br><br>\n\nI will perform the sampling distribution for the difference in **converted** between the two pages over 10,000 iterations of calculating an estimate from the null.  <br><br>","ba24d842":"<img src=\"https:\/\/i.imgur.com\/UoJrYZQ.png\"\/>\n","92552181":"<a id='conclusion'><\/a>\n### Conclusion\n\nThe work described in this notebook is based on a database providing details on the conversion rate of two groups(treatment group that holds the new page and control group that holds the old page), on an E-commerce platform from 2017-01-02 to 2017-01-24. The goal was to decide whether the E-commerce website should keep the old page or change to a new. \n\nRegarding the quality of the data, we had only 1 row duplicated in a sample with 294,478 rows. So, that problem did not affect the results.  \n\nTo achieve our goal, we performed the following tests: A\/B test with z-test and logistic regression models. In A\/B test, we found the p-value is higher than type error I, and because this, we fail to reject the null hyphotesis. Then, we saw the z-score was 1.31 which does not exceed the critical value of 1.96, so we fail again to reject the null hyphothesis. \n\nAfter that, we used stasmodels to fit the regression model and we found there is a significant difference in conversion based on which page a customer receives. In the first experiment with regression, we analyzed the individual factors of country and page on conversion. \n\nThe coefficient of our explanatory variables presented the following results: For each 1 unit decrease in new_page, conversion is 1.5 times as likely holding all else constant; For each 1 unit increase in CA, conversion is 4.1 times as likely holding all else constant; For each 1 unit increase in UK, conversion is 5.2 times as likely holding all else constant. In the last test we interpreted interaction between page and country to see if there significant effects on conversion, we discovered the following results: For each 1 unit increase in CA new_page, conversion is 8.1 times as likely holding all else constant. For each 1 unit increase in UK new_page, conversion is 4.8 times as likely holding all else constant.\n\nFinally, I strongly recommend gathering more data per period of at least 4 months. I consider the period of 22 days insufficient for making the decision about whether we should keep the old page or change to the new page, even if all tests indicated that we should keep the old one. ","9444513a":"<a id='wrangle'><\/a>\n#### Part I - Wrangle data & Exploratory Data Analysis\n\nTo get started, let's import our libraries.","dde6b556":"### Probability results\n>It seems to be that there is insufficient evidence to say that the new treatment page leads to more conversions than the control page. The difference of probability between control (12.04%) and treatment groups (11.88%) is tiny, especially when we compare them with the probability of individual conversion (11.96%)","ebbbb567":"`3.` Now, I will use  stats.proportions_ztest to compute my test statistic and p-value for evaluating if there is a statistically significance difference in conversion rates of the new page and the conversion rates of the old page.\n\nFirst, I will calculate the number of conversions for each page, as well as the number of individuals who received each page. The `n_old` and `n_new` refer the the number of rows associated with the old page and new pages, respectively.","9af3f857":"#### Z-test analysis\n>Since the z-score of 1.31 does not exceed the critical value of 1.96, we fail to reject the null hypothesis. The conversion rates of the old page is greater than or equal to the conversion rates of the new. Moreover, there was not a significant difference between the conversion rates of the new page and the conversion rates of the old page (>0.15%).","2d455ff1":"#### Regression analysis II\n\n>For each 1 unit decrease in new_page, convert is 1.5 times as likely holding all else constant.\n\n>For each 1 unit increase in CA, convert is 4.1 times as likely holding all else constant.\n\n>For each 1 unit increase in UK, convert is 5.2 times as likely holding all else constant.","83ea0d68":"#### Considering other things that might influence whether or not an individual converts.\nThere are many aspects that may influence whether or not an individual converts. For instance, we could consider factors as country, age, gender, city, hour or weekday and try to understand the correlation between them and the effects under the two groups. Another thing to deal with is the Simpson's paradox, in which a trend appears in several different groups of data, but disappears or reverses when these groups are combined. It is sometimes given the descriptive title of reversal paradox or amalgamation paradox.","3cefb401":"#### Sampling distribution analysis\n\n>1- The proportion of the conversion rate differences were greater than the actual observed difference. The p-value is extremely large (90%) than the type I error rate (5%).That means we fail to reject the null hypothesis.  \n\n>2- According to Wikipedia, p-value is the probability for a given statistical model that, when the null hypothesis is true, the statistical summary (such as the sample mean difference between two compared groups) would be the same as or of greater magnitude than the actual observed results.\n\n>3- When the p-value is low (in this project less than 5%), it suggests that the null hypothesis is not true, and we need to consider the alternative hypothesis. Finally, the p-value of 90% indicates that the actual page should be maintained.","16f67a9b":"Regression approach III (interaction between page and country)\n\nNow, I will look at an interaction between page and country to see if there significant effects on conversion.  "}}