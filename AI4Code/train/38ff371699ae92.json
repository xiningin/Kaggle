{"cell_type":{"3aade721":"code","04944f44":"code","3d4b4787":"code","f8820df1":"code","c06b1bfa":"code","982867f3":"code","f4dc3141":"code","234ba7f4":"code","6e1d2436":"code","25ad5606":"code","151dc9cc":"code","86054965":"code","88fcdce5":"code","1ef78902":"code","493e481e":"code","cccb9b5b":"code","4ff5ee99":"code","9298487c":"code","ba32c8e8":"code","d2a145b6":"code","e39fc669":"code","8a70f2a8":"code","d1416531":"code","921c7b27":"code","7e768663":"code","005bea0f":"code","3257f26c":"code","5b7c27c8":"code","e815581e":"code","e487fa87":"code","6d56d19e":"code","ca28df9e":"code","8a43e58d":"code","be3fcdbb":"code","d841af3d":"code","00d29edf":"code","aa346e4e":"code","32ded548":"code","10f7fbdc":"code","8205ff18":"code","2fa913f6":"code","a3d8471d":"code","e29d1d43":"code","2e521091":"code","01f60d96":"code","52e3cccd":"code","f4c7775c":"code","87ef01b6":"code","53bdee6b":"code","e5815ea1":"code","b629d23f":"code","b06bd41f":"code","503bcf8e":"code","8bf44b23":"code","54110c27":"code","5c84124e":"code","de4c234f":"code","8a4aaedd":"code","ef8ebcd0":"code","edfaa3ef":"code","b0b7abe2":"code","b047b15b":"code","8746e819":"code","9e69e152":"code","33e5293b":"code","a1008439":"code","1fb1041b":"code","92ee532a":"code","c724a328":"code","cb60aa0a":"code","1a38737a":"code","a1eef72e":"markdown","e1ab87ce":"markdown","6947185c":"markdown","265c23d9":"markdown","6d61f058":"markdown","e666f097":"markdown","68f97862":"markdown","aae83739":"markdown","0bce1063":"markdown","14ab1f82":"markdown","5088289d":"markdown","94d3cfa1":"markdown","22457190":"markdown","18c40411":"markdown","e39e7cf8":"markdown","1100c45f":"markdown","64bf92f8":"markdown","7203c535":"markdown","6f08de84":"markdown","1fd89c9a":"markdown","e15eead3":"markdown","730a1198":"markdown","df726ce1":"markdown","64a091c9":"markdown","f06b0a31":"markdown","02c99820":"markdown","e4ceb64e":"markdown","83cb4438":"markdown","ab1f3a73":"markdown","fca5701f":"markdown","d0c26e27":"markdown","f1b710b2":"markdown","ed10b3be":"markdown","2fc9d44a":"markdown","2becdae6":"markdown","55ca0e13":"markdown","c679d16a":"markdown","d8d3e365":"markdown"},"source":{"3aade721":"import warnings\nwarnings.filterwarnings('ignore', 'SettingWithCopyWarning')","04944f44":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport lightgbm as lgb\nfrom xgboost import XGBClassifier\n\nfrom statsmodels.distributions.empirical_distribution import ECDF\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import metrics, model_selection\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import classification_report, roc_auc_score\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.mixture import GaussianMixture\n\nfrom IPython.display import display, Markdown, Latex","3d4b4787":"# matplotlib\nplt.rc('font', size=15)\nplt.rc('axes', titlesize=18)  \nplt.rc('xtick', labelsize=10)  \nplt.rc('ytick', labelsize=10)\n\n# seaborn\nsns.set(font_scale = 1.2)\nsns.set_style(\"whitegrid\")","f8820df1":"class Cfg:\n    RANDOM_STATE = 2021\n    TRAIN_DATA = '..\/input\/tabular-playground-series-nov-2021\/train.csv'\n    TEST_DATA = '..\/input\/tabular-playground-series-nov-2021\/test.csv'\n    SUBMISSION = '..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv'    \n    SUBMISSION_FILE = 'submission.csv'\n    TEST_SIZE = 0.4\n    SAMPLE_FRAC = 0.03\n    N_FEATURE = 5 # 285\n    \n    INDEX = 'id'\n    TARGET = 'target'\n    FEATURES = ['f{}'.format(i) for i in range(0, 100)]\n    \n    @staticmethod\n    def set_seed():\n        random.seed(Cfg.RANDOM_STATE)\n        np.random.seed(Cfg.RANDOM_STATE)\n\nCfg.set_seed()","c06b1bfa":"def read_data(\n    train_file:str=Cfg.TRAIN_DATA, \n    test_file:str=Cfg.TEST_DATA\n) -> (pd.DataFrame, pd.DataFrame):\n    \"\"\"Reads the train and test data files\n    \"\"\"\n    # read csv files\n    train_df = pd.read_csv(train_file).set_index(Cfg.INDEX).astype(np.float32)\n    train_df[Cfg.TARGET] = train_df[Cfg.TARGET].astype(np.short, copy=False)\n    test_df = pd.read_csv(test_file).set_index(Cfg.INDEX).astype(np.float32)\n    \n    return train_df, test_df","982867f3":"%%time\ntrain_data, test_data = read_data()","f4dc3141":"train_data.head()","234ba7f4":"test_data.head()","6e1d2436":"print('Train data: {} rows'.format(len(train_data)))\nprint('Test data: {} rows'.format(len(test_data)))","25ad5606":"pd.DataFrame({\n    'data_set': ['train', 'test'],\n    'missing_values': [\n        train_data.isna().sum().sum(), \n        test_data.isna().sum().sum()\n    ]\n}).set_index('data_set')","151dc9cc":"def get_sample_data(\n    data,\n    split_target=True,\n    features=Cfg.FEATURES,\n    target=Cfg.TARGET,\n    frac=Cfg.SAMPLE_FRAC, \n    random_state=Cfg.RANDOM_STATE):\n    \"\"\"Select a sample subset from data\n    \"\"\"\n    idx = data.sample(frac=frac, random_state=random_state).index\n\n    if split_target:\n        X_data = data.iloc[idx][features]\n        y_data = data.iloc[idx][target]\n    \n        return X_data, y_data\n    \n    return train_data.iloc[idx]","86054965":"stat_data = train_data.describe().drop('count')\nstat_data.loc['var'] = stat_data.T['std']**2\n\nstat_data.T.style.bar(\n    subset=['mean'], \n    color='Bules'\n).background_gradient(subset=['50%'], cmap='Blues')","88fcdce5":"def plot_count(\n    data:pd.DataFrame, \n    feature:str, \n    title='Countplot',\n    ax=None):\n    \"\"\"\n    \"\"\"\n    if ax == None:\n        fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n    \n    sns.countplot(\n        data=train_data,\n        x=feature, \n        ax=ax)\n    \n    ax.set_title(title)\n\n    ax.set_xlabel('Feature {}'.format(feature))\n    ax.set_ylabel('Count')\n\n    return ax","1ef78902":"plot_count(train_data, Cfg.TARGET, title='Target countplot');","493e481e":"def plot_pdf(\n    data:pd.DataFrame, \n    feature:str,\n    target=Cfg.TARGET,\n    title='Histplot',\n    bins=70,\n    ax=None):\n    \"\"\" Plots the estimated pdf. \n    \"\"\"\n    if ax == None:\n        fig, ax = plt.subplots(1, 1)\n    \n    # plot pdf\n    sns.histplot(\n        data=data,\n        x=feature,\n        hue=target,\n        bins=bins,\n        legend=True,\n        kde=True,\n        ax=ax)\n    \n    mean = np.mean(data[feature])\n    ax.vlines(\n        mean, 0, 1, \n        transform=ax.get_xaxis_transform(), \n        color='red', ls=':')\n    \n    ax.set_title(title)\n    \n    ax.set_xlabel('Feature {}'.format(feature))\n    ax.set_ylabel('Count')\n    \n    return ax","cccb9b5b":"def plot_boxplot(\n    data:pd.DataFrame, \n    feature:str, \n    title='Boxplot',\n    ax=None):\n\n    if ax == None:\n        fig, ax = plt.subplots(1, 1)\n    \n    ax = sns.boxplot(\n        x=Cfg.TARGET, \n        y=feature,\n        data=data\n    )\n    \n    ax.set_title(title)\n    \n    ax.set_xlabel('Target {}'.format(Cfg.TARGET))\n    ax.set_ylabel('Feature {}'.format(feature))\n    \n    return ax","4ff5ee99":"def plot_ecdf(\n    data:pd.DataFrame, \n    feature:str, \n    title='Empirical distribution',\n    ax=None):\n    \"\"\"Displays the ECDF\n    \"\"\"    \n    if ax == None:\n        fig, ax = plt.subplots(1, 1)\n        \n    target_0 = data[data[Cfg.TARGET] == 0][feature]\n    target_1 = data[data[Cfg.TARGET] == 1][feature]\n    \n    ecdf_0 = ECDF(target_0)\n    ecdf_1 = ECDF(target_1)\n\n    ax.plot(ecdf_0.x, ecdf_0.y)\n    ax.plot(ecdf_1.x, ecdf_1.y)\n    \n    ax.set_title(title)\n    ax.set_xlabel('Feature {}'.format(feature))\n    ax.set_ylabel('ecdf')\n\n    return ax","9298487c":"for feature in Cfg.FEATURES:\n    display(Markdown('### Feature `{}`'.format(feature)))\n \n    info = np.round(train_data[feature].describe(), 4)\n    \n    format_str = '* mean: {}\\n* std: {}\\n* min: {}\\n* 25%: {}\\n* 50%: {}\\n* 75%: {}\\n* max: {}'\n    display(Markdown(format_str.format(\n        info['mean'], \n        info['std'], \n        info['min'], \n        info['25%'], \n        info['50%'], \n        info['75%'], \n        info['max'])))\n    \n    fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n\n    plot_pdf(train_data, feature, ax=ax[0])\n    plot_ecdf(train_data, feature, ax=ax[1])\n    plot_boxplot(train_data, feature, ax=ax[2])\n    \n    plt.show()","ba32c8e8":"# Features with a bimodel distribution\nBIMODAL_DIST = [\n    'f1', 'f3', 'f5', 'f6', 'f7', 'f8', 'f10', 'f11', \n    'f13', 'f14', 'f15', 'f17', 'f18', 'f22', 'f25', \n    'f26', 'f29', 'f34', 'f37', 'f38', 'f40', 'f41', \n    'f43', 'f45', 'f47', 'f50', 'f54', 'f55', 'f57', \n    'f65', 'f66', 'f67', 'f70', 'f71', 'f74', 'f77', \n    'f80', 'f82', 'f85', 'f86', 'f91', 'f96', 'f97'\n]","d2a145b6":"bimodal_df = train_data[BIMODAL_DIST]\nbimodal_stats = bimodal_df.describe().T\n\nmu = np.round(bimodal_stats['mean'].mean(), 3)\nstd =  np.round(bimodal_stats['mean'].std(), 3)\n\nprint(f'The average mean of all bimodal features is {mu} with a standard deviation of {std}.')","e39fc669":"fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n\nplot_pdf(bimodal_stats, 'mean', target=None, bins=12, title='mean of bimodale features', ax=ax[0])\nplot_pdf(bimodal_stats, 'std', target=None, bins=12, title='std of bimodale features', ax=ax[1])\n\nplt.show()","8a70f2a8":"def plot_model_proba(proba, ax=None):\n    \"\"\"\n    \"\"\"\n    if ax == None:\n        fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n\n    sns.histplot(\n        data=proba,\n        legend=True,\n        bins=100,\n        kde=True,\n        ax=ax\n    )\n\n    ax.set_xlabel('Prediction probapility')\n    ax.set_ylabel('Probabitity')","d1416531":"def plot_roc(model, X_val, y_val, ax=None):\n    \"\"\"Displays the ROC\n    \"\"\"\n    if ax == None:\n        fig, ax = plt.subplots(1, 1)\n    \n    metrics.plot_roc_curve(model, X_val, y_val, ax=ax)","921c7b27":"from sklearn import metrics\n\ndef plot_confusion_matrix(model, X_val, y_val, ax=None):\n    \"\"\"Displays confusion matrix\n    \"\"\"\n    if ax == None:\n        fig, ax = plt.subplots(1, 1)\n\n    metrics.plot_confusion_matrix(\n        model, \n        X_val, \n        y_val, \n        cmap=plt.cm.Blues,\n        normalize='true', \n        ax=ax\n    ) ","7e768663":"def display_model_result(model, X_val, y_val, y_pred, y_pred_proba=np.array([])):\n    \"\"\"\n    \"\"\"\n    n_to_show = 3 if len(y_pred_proba) != 0 else 2\n    figsize = (15, 5) if len(y_pred_proba) != 0 else (10, 5)\n        \n    fig, ax = plt.subplots(1, n_to_show, figsize=figsize)\n\n    plot_roc(model, X_val, y_val, ax=ax[0])\n    plot_confusion_matrix(model, X_val, y_val, ax=ax[1])\n    \n    if len(y_pred_proba) != 0:\n        plot_model_proba(y_pred_proba, ax=ax[2])\n\n    plt.tight_layout()\n    plt.show()\n\n    print(classification_report(y_val, y_pred))","005bea0f":"X_data, y_data = get_sample_data(train_data, features=BIMODAL_DIST, frac=1)\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_data,\n    y_data,\n    test_size=Cfg.TEST_SIZE, \n    random_state=Cfg.RANDOM_STATE)","3257f26c":"%%time\n\nmodel = make_pipeline(\n    StandardScaler(), \n    LogisticRegression(C=0.2, solver='liblinear')\n)\n\ny_pred = model.fit(X_train, y_train).predict(X_val)\ny_pred_proba = model.predict_proba(X_val)[:, 1]\n\ndisplay_model_result(model, X_val, y_val, y_pred, y_pred_proba)","5b7c27c8":"LOW_VARIANCE_FEATURES = list(set(Cfg.FEATURES) - set(BIMODAL_DIST))","e815581e":"X_data, y_data = get_sample_data(train_data, features=LOW_VARIANCE_FEATURES, frac=1)\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_data,\n    y_data,\n    test_size=Cfg.TEST_SIZE, \n    random_state=Cfg.RANDOM_STATE)","e487fa87":"%%time\n\nmodel = make_pipeline(\n    StandardScaler(), \n    LogisticRegression(C=0.2, solver='liblinear')\n)\n\ny_pred = model.fit(X_train, y_train).predict(X_val)\ny_pred_proba = model.predict_proba(X_val)[:, 1]\n\ndisplay_model_result(model, X_val, y_val, y_pred, y_pred_proba)","6d56d19e":"def get_gaussian_mixture(data):\n    \"\"\"\n    \"\"\"\n    gm = GaussianMixture(n_components=2, random_state=Cfg.RANDOM_STATE).fit(data)\n    gm_proba = np.round(gm.predict_proba(data)[:, 0], 4)\n    \n    df = pd.DataFrame({\n        'id': data.index,\n        'gm_proba': gm_proba,\n        'gm': (gm_proba > 0.5).astype(np.int),\n        'target': data[Cfg.TARGET]\n    }).set_index('id')\n    \n    return df","ca28df9e":"%%time\n\nbimodal_data = get_gaussian_mixture(train_data[BIMODAL_DIST + [Cfg.TARGET]])\nbimodal_data.head()","8a43e58d":"print(classification_report(bimodal_data['target'], bimodal_data['gm']))","be3fcdbb":"fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n\nN, _ = bimodal_data.shape\nsns.heatmap(\n    pd.crosstab(bimodal_data['gm'], bimodal_data['target']) \/ N,\n    cmap='Blues_r',\n    annot=True, \n    ax=ax\n)\n\nplt.tight_layout()\nplt.show()","d841af3d":"high_corr = train_data.sample(frac=0.01).corr().abs()[[Cfg.TARGET]]\nhigh_corr.columns = ['corr']\nhigh_corr = high_corr.sort_values(by='corr', ascending=False).head(10)\n\nhigh_corr","00d29edf":"df = train_data.sample(frac=0.001)[high_corr.index]\ng = sns.pairplot(\n    data=df, \n    hue=Cfg.TARGET,\n    corner=True)\n\ng.fig.set_size_inches(15, 15)\n\nfig.tight_layout()\nplt.show()","aa346e4e":"from sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.inspection import permutation_importance","32ded548":"X_data, y_data = get_sample_data(train_data, frac=0.1)\nX_train, X_val, y_train, y_val = train_test_split(\n    X_data,\n    y_data,\n    test_size=Cfg.TEST_SIZE, \n    random_state=Cfg.RANDOM_STATE)\n\nprint(f'train size: {X_train.shape[0]} rows')\nprint(f'val size  : {X_val.shape[0]} rows')","10f7fbdc":"baseline_model = RandomForestClassifier(\n    n_estimators=30, \n    random_state=Cfg.RANDOM_STATE).fit(X_train, y_train)","8205ff18":"%%time\nresult = permutation_importance(baseline_model, X_val, y_val, n_repeats=10, random_state=Cfg.RANDOM_STATE)","2fa913f6":"n_to_show = 20\n\nimportance_df = pd.DataFrame({\n    'feature': X_val.columns.tolist(),\n    'weight': result.importances_mean,\n    'std': result.importances_std\n}).set_index('feature').sort_values(by='weight', ascending=False)\n\nimportance_df.head(n_to_show)","a3d8471d":"fig, ax = plt.subplots(1, 1, figsize=(15, 8))\n\ndf = importance_df.head(n_to_show)\nsns.barplot(\n    data=df,\n    x='weight', \n    y=df.index, \n    palette='Blues_r',\n    ax=ax) \n\nax.set_title('Permutation importance')\nax.set_xlabel(\"Weights\")\nax.set_ylabel(\"Features\")\n\nplt.show()","e29d1d43":"rfe = RFECV(\n    estimator=baseline_model, \n    cv=StratifiedKFold(2),\n    scoring='accuracy',\n    min_features_to_select=1,\n    step=3, \n    verbose=0\n)","2e521091":"%%time\n\nrfe.fit(X_data, y_data);\nprint('Optimal number of features: {}'.format(rfe.n_features_))","01f60d96":"fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n\nsns.lineplot(\n    x=range(1, len(rfe.grid_scores_) + 1),\n    y=rfe.grid_scores_,\n    ax=ax\n)\n\nax.set_title('Recursive feature elimination')\nax.set_xlabel('Number of features selected')\nax.set_ylabel('Cross validation score (accuracy)')\n\nplt.show()","52e3cccd":"X_data, y_data = train_data[Cfg.FEATURES], train_data[Cfg.TARGET] \nlda = LinearDiscriminantAnalysis()\n\nlda_data = pd.DataFrame({\n    'lda': lda.fit_transform(X_data, y_data).reshape(-1),\n    'target': y_data\n})","f4c7775c":"fig, ax = plt.subplots(1, 1, figsize=(20, 3))\n\nsns.scatterplot(\n    data=lda_data.sample(frac=0.6),\n    x='lda',\n    y=0,\n    hue='target',\n    ax=ax,\n    alpha=0.4\n)\nax.set_title('LDA')\nax.get_yaxis().set_visible(False)\n\nplt.tight_layout()\nplt.show()","87ef01b6":"fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n\nplot_pdf(lda_data, 'lda', ax=ax[0])\nplot_pdf(lda_data[lda_data[Cfg.TARGET] == 0], 'lda', ax=ax[1])\nplot_pdf(lda_data[lda_data[Cfg.TARGET] == 1], 'lda', ax=ax[2])\n\nplt.tight_layout()\nplt.show()","53bdee6b":"from sklearn.decomposition import PCA","e5815ea1":"def plot_pca(data, x, y, ax=None):\n    if ax == None:\n        fig, ax = plt.subplots(1, 1)\n        \n    sns.scatterplot(\n        data=data,\n        x=x, \n        y=y,\n        hue=Cfg.TARGET,\n        legend='brief',\n        alpha=0.4,\n        ax=ax)","b629d23f":"X_data, y_data = get_sample_data(train_data, frac=0.2)\n\nn_components=100\npca = make_pipeline(\n    StandardScaler(), \n    PCA(n_components=n_components, random_state=Cfg.RANDOM_STATE)\n)\n\npca_df = pd.DataFrame(\n    pca.fit_transform(X_data, y_data), \n    columns=['pc{}'.format(i) for i in range(1, n_components + 1)])\n\npca_df[Cfg.TARGET] = y_data.values","b06bd41f":"fig, ax = plt.subplots(1, 5, figsize=(28, 5))\n\nplot_pca(pca_df, 'pc1', 'pc2', ax=ax[0])\nplot_pca(pca_df, 'pc2', 'pc3', ax=ax[1])\nplot_pca(pca_df, 'pc3', 'pc4', ax=ax[2])\nplot_pca(pca_df, 'pc4', 'pc5', ax=ax[3])\nplot_pca(pca_df, 'pc5', 'pc6', ax=ax[4])\n\nplt.tight_layout()\nplt.show()","503bcf8e":"fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n\nsns.lineplot(\n    x=range(1, n_components+1),\n    y=pca['pca'].explained_variance_ratio_,\n    ax=ax[0])\n\nax[0].set_title('Explained variance ratio')\nax[0].set_xlabel(\"# of components\")\nax[0].set_ylabel(\"Variance ratio\")\n\nsns.lineplot(\n    x=range(1, n_components+1),\n    y=np.cumsum(pca['pca'].explained_variance_ratio_),\n    ax=ax[1])\n\nax[1].set_title('Cumulative explained variance ratio')\nax[1].set_xlabel(\"# of components\")\nax[1].set_ylabel(\"Cumulative ratio\") \n\nplt.tight_layout()\nplt.show()","8bf44b23":"def add_agg_features(data):\n    \"\"\"Create aggregated features\n    \"\"\"\n    df = data.copy()\n    \n    df.loc[:, 'min'] = np.min(data[Cfg.FEATURES], axis=1)\n    df.loc[:, 'max'] = np.max(data[Cfg.FEATURES], axis=1)\n    df.loc[:, 'var'] = np.var(data[Cfg.FEATURES], axis=1)\n    df.loc[:, 'std'] = np.std(data[Cfg.FEATURES], axis=1)\n    df.loc[:, 'sum'] = np.sum(data[Cfg.FEATURES], axis=1)\n    df.loc[:, 'mean'] = np.mean(data[Cfg.FEATURES], axis=1)\n    \n    return df","54110c27":"def add_gaussian_mixture(data):\n    \"\"\"\n    \"\"\"\n    df = data.copy()\n    \n    gm = GaussianMixture(n_components=2, random_state=Cfg.RANDOM_STATE).fit(df[BIMODAL_DIST])\n    df.loc[:, 'gm'] =  gm.predict_proba(df[BIMODAL_DIST])[:, 0]\n    \n    return df","5c84124e":"feature_engineering = make_pipeline(\n    FunctionTransformer(add_agg_features),\n    FunctionTransformer(add_gaussian_mixture),\n    StandardScaler()\n)","de4c234f":"X_data, y_data = get_sample_data(train_data, frac=1)\n\nX_train, X_val, y_train, y_val = train_test_split(\n    train_data[Cfg.FEATURES],\n    train_data[Cfg.TARGET],\n    test_size=Cfg.TEST_SIZE, \n    random_state=Cfg.RANDOM_STATE)\n\nprint(f'train size: {X_train.shape[0]} rows')\nprint(f'val size  : {X_val.shape[0]} rows')","8a4aaedd":"%%time\n\nlr_model = make_pipeline(\n    feature_engineering, \n    LogisticRegression(C=0.2, solver='liblinear'))\n\ny_pred = lr_model.fit(X_train, y_train).predict(X_val)\ny_pred_proba = lr_model.predict_proba(X_val)[:, 1]\n\ndisplay_model_result(lr_model, X_val, y_val, y_pred, y_pred_proba)","ef8ebcd0":"%%time\n\nlda_model = make_pipeline(\n    feature_engineering, \n    LinearDiscriminantAnalysis())\n\ny_pred = lda_model.fit(X_train, y_train).predict(X_val)\ny_pred_proba = lda_model.predict_proba(X_val)[:, 1]\n\ndisplay_model_result(lda_model, X_val, y_val, y_pred, y_pred_proba)","edfaa3ef":"%%time\n\nfrom sklearn.linear_model import SGDClassifier\n\nsgd_model = make_pipeline(\n    feature_engineering, \n    SGDClassifier(loss='log'))\n\ny_pred = sgd_model.fit(X_train, y_train).predict(X_val)\ny_pred_proba = sgd_model.predict_proba(X_val)[:, 1]\n\ndisplay_model_result(sgd_model, X_val, y_val, y_pred, y_pred_proba)","b0b7abe2":"%%time\n\nfrom sklearn.linear_model import RidgeClassifier\n\nridge_model = make_pipeline(\n    feature_engineering,  \n    RidgeClassifier())\n\ny_pred = ridge_model.fit(X_train, y_train).predict(X_val)\ndisplay_model_result(ridge_model, X_val, y_val, y_pred)","b047b15b":"%%time\n\ndt_model = make_pipeline(\n    feature_engineering,  \n    DecisionTreeClassifier(max_depth=5)\n)\n\ny_pred = dt_model.fit(X_train, y_train).predict(X_val)\ny_pred_proba = dt_model.predict_proba(X_val)[:, 1]\n\ndisplay_model_result(dt_model, X_val, y_val, y_pred, y_pred_proba)","8746e819":"%%time\n\nrf_model = make_pipeline(\n    feature_engineering, \n    RandomForestClassifier(\n        max_depth=5, \n        n_estimators=20)\n)\n\ny_pred = rf_model.fit(X_train, y_train).predict(X_val)\ny_pred_proba = rf_model.predict_proba(X_val)[:, 1]\n\ndisplay_model_result(rf_model, X_val, y_val, y_pred, y_pred_proba)","9e69e152":"%%time\n\nlgbm_model = make_pipeline(\n    feature_engineering,  \n    lgb.LGBMClassifier(\n        learning_rate=0.05,\n        n_estimators=1000,\n        reg_lambda = 1)\n)\n\ny_pred = lgbm_model.fit(X_train, y_train).predict(X_val)\ny_pred_proba = lgbm_model.predict_proba(X_val)[:, 1]\n\ndisplay_model_result(lgbm_model, X_val, y_val, y_pred, y_pred_proba)","33e5293b":"%%time\n\nxgb_model = make_pipeline(\n    feature_engineering,  \n    XGBClassifier(\n        n_estimators=100,\n        use_label_encoder=False,\n        eval_metric='rmse',\n        random_state=Cfg.RANDOM_STATE)\n)\n\ny_pred = xgb_model.fit(X_train, y_train).predict(X_val)\ny_pred_proba = xgb_model.predict_proba(X_val)[:, 1]\n\ndisplay_model_result(xgb_model, X_val, y_val, y_pred, y_pred_proba)","a1008439":"%%time\n\nada_model = make_pipeline(\n    StandardScaler(), \n    AdaBoostClassifier())\n\ny_pred = ada_model.fit(X_train, y_train).predict(X_val)\ny_pred_proba = ada_model.predict_proba(X_val)[:, 1]\n\ndisplay_model_result(xgb_model, X_val, y_val, y_pred, y_pred_proba)","1fb1041b":"estimators = [\n    ('lda', LinearDiscriminantAnalysis()),\n    ('lr',  LogisticRegression(C=0.2, solver='liblinear')),\n    ('sgd',  SGDClassifier(loss='log')),\n    ('ridge', RidgeClassifier())\n]\n\nmodel = make_pipeline(\n    feature_engineering, \n    StackingClassifier(\n        estimators=estimators, \n        final_estimator=LogisticRegression(),\n        cv=3,\n        verbose=0)\n)","92ee532a":"%%time\n\ny_pred = model.fit(X_train, y_train).predict(X_val)\ny_pred_proba = model.predict_proba(X_val)[:, 1]\n\ndisplay_model_result(model, X_val, y_val, y_pred, y_pred_proba)","c724a328":"y_pred_submission = model.predict_proba(test_data)[:, 1]","cb60aa0a":"submission_data = pd.DataFrame({\n    Cfg.INDEX: test_data.index,\n    Cfg.TARGET: y_pred_submission,\n}).set_index(Cfg.INDEX)\n\nsubmission_data","1a38737a":"# save submission file\nsubmission_data.to_csv(Cfg.SUBMISSION_FILE)","a1eef72e":"## Model `LogisticRegression`","e1ab87ce":"### Summary\n\n* There any some features with a bimodal distribution, e.g. `f1`, `f3`, etc.\n* Some distributions have a low variance, e.g. `f2`, `f4`, etc. Features with low variance usually provide less information. ","6947185c":"# Exploratory data analysis (EDA)","265c23d9":"# Submission","6d61f058":"## Features with low variance\n\nFeatures with low variance usually provide less information. ","e666f097":"### Notice\n\n* There are 43 features with bimodal distribution.\n* These features result in an accuracy = 0.68 for the target variable.\n* The average mean of all bimodal features is 2.555 with a standard deviation of 0.076.\n\n","68f97862":"## Correlation\n\nNow we will identify the features that have a high correlation with the target variable.","aae83739":"# Linear discriminant analysis (LDA)","0bce1063":"## Model `LinearDiscriminant`","14ab1f82":"# Setup","5088289d":"### Notice\n\n* There are no missing values in both data sets.","94d3cfa1":"## Gaussian mixture\n\nA bimodal distribution most commonly arises as a mixture of two different unimodal distributions (see: https:\/\/www.wikiwand.com\/en\/Multimodal_distribution). ","22457190":"### Notice\n\n* The distribution of the target is balanced.","18c40411":"### Notice\n\n* Unfortunately, it is not possible to make a prediction based on the gaussian mixture feature `gm`.","e39e7cf8":"## Model `LGBM`","1100c45f":"# Modeling","64bf92f8":"## Model `RandomForest`","7203c535":"## Model `AdaBoost`","6f08de84":"## Model `XGB`","1fd89c9a":"#  Feature engineering","e15eead3":"Thank you for reading.","730a1198":"## Features `f0` - `f99`","df726ce1":"![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/28009\/logos\/header.png?)","64a091c9":"# Read data","f06b0a31":" ## Permutation importance","02c99820":"## Target variable","e4ceb64e":"### Notice\n\n* There are 57 features with bimodal distribution.\n* These features result in an accuracy = 0.58 for the target variable","83cb4438":"## Model `DecisionTree`","ab1f3a73":"### Model `SGD`","fca5701f":"### Notice\n\n* The training data contains 600,000 rows.\n\n* The test data contains 540,000 rows.\n\n* There are 100 features `f0` - `f99`\n\n* The target variable `target` is binary (1\/0)","d0c26e27":"# Feature importance","f1b710b2":"### Model `Ridge`","ed10b3be":"## Features with bimodal distribution\n","2fc9d44a":"## Baseline model","2becdae6":"## Stacking model ","55ca0e13":"## Recursive feature elimination (RFE)","c679d16a":"# Missing values","d8d3e365":"# Principal component analysis (PCA)"}}