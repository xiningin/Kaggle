{"cell_type":{"960888f7":"code","57f62ec6":"code","6549080d":"code","d964b13e":"code","8be6c9cd":"code","a70cab79":"code","4f769976":"code","0b7be651":"markdown","549b4815":"markdown","0af42ee6":"markdown","2e94fff0":"markdown","7314f617":"markdown","055c0635":"markdown","6c7c0690":"markdown","9ed9e336":"markdown","6067eea5":"markdown","754ec67c":"markdown"},"source":{"960888f7":"from keras.layers import Dense, Dropout, ReLU, LeakyReLU, BatchNormalization\nfrom keras.models import Sequential, Model, Input\nfrom keras.optimizers import Adam\nfrom keras.datasets import mnist\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nimport pandas as pd\nwarnings.filterwarnings(\"ignore\")","57f62ec6":"x_train = pd.read_csv(\"..\/input\/fashionmnist\/fashion-mnist_train.csv\")\nx_test = pd.read_csv(\"..\/input\/fashionmnist\/fashion-mnist_test.csv\")\n\nx_train = np.array(x_train)\nx_test = np.array(x_test)\n\nx_train = x_train.astype(\"float32\")\/255.0\nx_test = x_test.astype(\"float32\")\/255.0\n\nx_train = x_train[:,:-1]\nx_test = x_test[:,:-1]\n\nprint(x_train.shape)\nprint(x_test.shape)","6549080d":"def create_generator():\n    generator = Sequential()\n    generator.add(Dense(512, input_dim=100))\n    generator.add(ReLU())\n\n    generator.add(Dense(1024))\n    generator.add(ReLU())\n\n    generator.add(Dense(512))\n    generator.add(ReLU())\n\n    #set output sizes to 784 to match our data\n    generator.add(Dense(784, activation=\"tanh\"))\n\n\n    #it will be fake and real two classes, we will build our model similar to classification.\n    generator.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=0.001, beta_1 = 0.5))\n    return generator\n\ng = create_generator()\ng.summary()","d964b13e":"def create_discriminator():\n    discriminator = Sequential()\n    discriminator.add(Dense(1024, input_dim=784))\n    discriminator.add(ReLU())\n    discriminator.add(Dropout(0.4))\n    \n    discriminator.add(Dense(512))\n    discriminator.add(ReLU())\n    discriminator.add(Dropout(0.4))\n\n    discriminator.add(Dense(512))\n    discriminator.add(ReLU())\n\n    discriminator.add(Dense(1, activation=\"sigmoid\"))\n\n    discriminator.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=0.001, beta_1=0.5))\n\n    return discriminator\n\nd = create_discriminator()\nd.summary()","8be6c9cd":"def create_gan(generator, discriminator):\n    discriminator.trainable = False\n\n    #lets get started to specify an input and give it to generator\n    gan_input = Input(shape=(100,))\n    #generator will give us a value after it taked this part\n    x = generator(gan_input)\n    #the value we get will be entered into the discriminator and checked \n    # and returned to us a GAN output\n    gan_output = discriminator(x)\n\n    #now we can build a gun model\n    gan = Model(inputs=gan_input, outputs=gan_output)\n    gan.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n\n    #as a result, let's turn our gan model\n    return gan\n\ngan = create_gan(g, d)\ng.summary()","a70cab79":"epochs = 50\nbatch_size = 256\nacc_list = []\nhsitory = []\n\ndef gan_train(g, d, gan):\n    for e in range(epochs):\n        print(\"Epoch continues : \", e+1)\n        for _ in range(batch_size):\n            noise = np.random.normal(0,1,[batch_size, 100])\n            generated_img = g.predict(noise)\n            \n            batch_img = x_train[np.random.randint(low=0, high=x_train.shape[0], size=batch_size)]\n            \n            x = np.concatenate([batch_img, generated_img])\n\n\n            y_disc = np.zeros(batch_size*2)\n            y_disc[:batch_size] = 1\n\n            d.trainable = True\n            d.train_on_batch(x, y_disc)\n\n            noise = np.random.normal(0,1,[batch_size, 100])\n            y_gen = np.ones(batch_size)\n\n            d.trainable = False\n\n            history = gan.train_on_batch(noise, y_gen)\n        acc_list.append(history[0])\n        history.append(history)\n \n    print(\"Training Done...\")\n\ngan_train(g,d,gan)","4f769976":"noise = np.random.normal(loc=0, scale=1, size=[100,100])\ngenerated_images = g.predict(noise)\ngenerated_images = generated_images.reshape(100,28,28)\nplt.imshow(generated_images[66], interpolation=\"nearest\")\nplt.title(\"The Picture\\nof GAN\")\nplt.show()\ngan","0b7be651":"### RESHAPE DATA\n","549b4815":"## IMPORTING DATA","0af42ee6":"## GAN Model Training","2e94fff0":"## IMPORTING LIBRARIES","7314f617":"\n## MODEL BUILDING\n\nThe GANs model consists of two networks, Generator and Discriminator. We will set up these networks separately and then create our overall model.\n\nLet's write our function to create a generator network.","055c0635":"# CREATING THE GAN MODEL\n\nAfter creating our Generator and Discriminator networks, we can now combine them into a single structure and create our GAN model.\n\nBefore we get started, we need to know that we don't want to develop the discriminator part so that the generator part, that is, the producer part of our network, can update itself. Because if we train the discriminator part that will make the fake \/ real distinction, this part of the network will improve itself, so no matter how realistic the fake data generated, it will realize that the data is fake and the generator part will always fail. Therefore, the producer network will see every output unsuccessful and will have difficulty developing itself. However, when he approaches the truth, he needs to know that he is successful and update himself accordingly. For these reasons, our discriminator network will be closed to train process.\n","6c7c0690":"# CODING OF GAN ARCHITECTURE\nIf we understand how it works, we can start building an exemplary GAN architecture.\n\nIn this study, we will try to observe the results of the GAN network using mnist dataset.","9ed9e336":"# GENERATIVE ADVERSERIAL NETWORK (GANs)\n\nWHAT ARE GANs?\n\nGAN is an artificial neural network architecture. But we call it GAN's. Because, in fact, it is the combination of two separate network structures. The GANs generate new data that has the same statistics as the data given to it.\n\nWe said that GAN consists of two separate networks. One of these two networks receives real data and sends it to a controller structure. The second network structure creates another data which should represent the original data and sends this data to the same supervisory structure. In the supervisory network or structure part, these data are tested and as a result of the test, the similarity of the data from the generative structure to the original data is measured. If the counterfeit network is not successful enough, it tries to generate new data by updating their weights. As a result, the weights of the simulated network are updated after a certain location and learn to create a dataset that is very similar to the original data or well represents the original data.\n\nIn other words, one of the networks uses the original data and the other tries to emulate the same data. While both networks try to prove to the supervisory structure that their data is original, our copycat network becomes able to create the original data from scratch.\n\nIf we represent this situation with an image, it would look like this:\n\n![alt text](https:\/\/miro.medium.com\/max\/958\/1*-gFsbymY9oJUQJ-A3GTfeg.png)\n\nPicture Source: https:\/\/medium.com\/@devnag\/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f\n\n**GAN SAMPLE STRUCTURE**\n![alt text](https:\/\/miro.medium.com\/max\/1741\/1*t78gwhhw-hn1CgXc1K89wA.png)\n\nPicture Source: https:\/\/medium.com\/datadriveninvestor\/generative-adversarial-network-gan-using-keras-ce1c05cfdfd3\n\n**Imroving of GAN Epoch by Epoch**\n![alt text](https:\/\/miro.medium.com\/max\/1952\/1*xm6_ZfvfKSHe2KS49DT8TQ.png)\n\n\nPicture Source: https:\/\/medium.com\/datadriveninvestor\/generative-adversarial-network-gan-using-keras-ce1c05cfdfd3\n\n\nAs a result of this structure, our ultimate goal is to produce new data. The more similar it is to the new data produced, the more successful our network is. The new data we produce in a good structure is too high quality to be distinguished from the reality.\n\nAs an example, let's look at human faces that were actually produced with GANs (These people never lived in our world):\n\nB\n\n![alt text](https:\/\/wp-assets.futurism.com\/2018\/12\/ai1.jpg)\n\nPicture Source: https:\/\/futurism.com\/incredibly-realistic-faces-generated-neural-network","6067eea5":"# Visualization of GAN Results\n\nIt would be unreasonable to expect a very good result because we only did 50 epochs. The higher the number of Epoch, the better we can get.","754ec67c":"\nNow let's write the part of our Discriminator network.\n\nBut the part that needs to be understood here is that the Discriminator network should match the images that the input shape will take. Because we will give the pictures directly to this network,  itself will not produce like the generator.\n"}}