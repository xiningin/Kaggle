{"cell_type":{"16373a17":"code","3cad355d":"code","ead39524":"code","e7cff9b2":"code","9f2bacc9":"code","1577f579":"code","e25f953a":"code","062c9666":"code","38dcc235":"code","44775ff8":"code","5f0543cb":"code","bd06d69c":"code","440ba9a3":"code","3a9d6cce":"code","87932545":"code","e9eb22a3":"code","9205089a":"code","3d25b090":"code","8e04886e":"code","e9d10c33":"code","7aecaaf3":"code","b8d24fc0":"code","6bc077bc":"code","d3484ed6":"code","a20466cf":"code","0b9d257c":"code","9360c93d":"code","4c7458e5":"code","9a16314d":"code","56bb846c":"code","f67488e3":"code","e2ab370e":"code","5428fac8":"code","2b26e44d":"code","af2ee650":"code","ef2db61c":"markdown","4887fc49":"markdown","2962f612":"markdown","200929ff":"markdown","8c328eca":"markdown","b19a47f5":"markdown","4a6786f9":"markdown","f0ad2c18":"markdown","50b5ca9d":"markdown","33233417":"markdown","4e5482f1":"markdown","cf711b0d":"markdown","678719c8":"markdown","13d821ab":"markdown","82f77da1":"markdown","c29444a2":"markdown","b6521a74":"markdown","c6f83377":"markdown","bdd43b7d":"markdown","a26a5702":"markdown","52a2abca":"markdown","5667c18d":"markdown","8b3bad12":"markdown","c8de46e5":"markdown","054a749c":"markdown","2643f6e9":"markdown","fde17bc3":"markdown"},"source":{"16373a17":"# Importing libraries\nfrom fastai.imports import *\nfrom fastai.structured import *\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\n\nfrom IPython.display import display","3cad355d":"# importing data (only 10 million data points)\nPATH = '..\/input'\ndf_raw = pd.read_csv(f'{PATH}\/train.csv', nrows=10000000)","ead39524":"# Function to set display options\ndef display_all(df):\n    with pd.option_context('display.max_rows',1000):\n        with pd.option_context('display.max_columns',1000):\n            display(df)","e7cff9b2":"display_all(df_raw.head(5))","9f2bacc9":"add_datepart(df_raw,'pickup_datetime',drop=True, time=True)","1577f579":"display_all(df_raw.head(5))","e25f953a":"def distance(data):\n    data['longitutde_traversed'] = (data.dropoff_longitude - data.pickup_longitude).abs()\n    data['latitude_traversed'] = (data.dropoff_latitude - data.pickup_latitude).abs()","062c9666":"distance(df_raw)","38dcc235":"display_all(df_raw.head(2).T)","44775ff8":"df_raw.isnull().sum()","5f0543cb":"df_raw.dropna(axis=0, how='any', inplace=True)","bd06d69c":"df_raw.shape","440ba9a3":"key = df_raw.key\ndf_raw.drop('key', axis=1, inplace = True)","3a9d6cce":"df_raw.passenger_count.value_counts()","87932545":"df_raw = df_raw[(df_raw.passenger_count>0)&(df_raw.passenger_count<10)]","e9eb22a3":"len(df_raw)","9205089a":"df_raw.reset_index(drop=True, inplace=True)","3d25b090":"outliers = []\n# For each feature find the data points with extreme high or low values\nfor feature in df_raw.keys():\n    Q1 = np.percentile(df_raw[feature],25,axis=0)\n    Q3 = np.percentile(df_raw[feature],75,axis=0)\n    step = 2*(Q3-Q1)\n    feature_outlier = df_raw[~((df_raw[feature] >= Q1 - step) & (df_raw[feature] <= Q3 + step))]\n    outliers += feature_outlier.index.tolist()","8e04886e":"len(outliers)\/len(df_raw)","e9d10c33":"outliers = []\n# For each feature find the data points with extreme high or low values\nfor feature in ['longitutde_traversed','latitude_traversed']:\n    Q1 = np.percentile(df_raw[feature],25,axis=0)\n    Q3 = np.percentile(df_raw[feature],75,axis=0)\n    step = 10*(Q3-Q1)\n    feature_outlier = df_raw[~((df_raw[feature] >= Q1 - step) & (df_raw[feature] <= Q3 + step))]\n    outliers += feature_outlier.index.tolist()","7aecaaf3":"len(outliers)\/len(df_raw)","b8d24fc0":"df = df_raw.drop(df_raw.index[outliers]).reset_index(drop = True)","6bc077bc":"len(df)","d3484ed6":"y = df_raw.fare_amount\ndf_raw.drop('fare_amount', axis=1, inplace = True)","a20466cf":"X_train, X_valid, y_train, y_valid = train_test_split(df_raw, y, test_size = 10000)","0b9d257c":"def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    print(res)","9360c93d":"set_rf_samples(10000)","4c7458e5":"m = RandomForestRegressor(n_jobs=-1)\n%time m.fit(X_train, y_train)\nprint_score(m)","9a16314d":"fi = rf_feat_importance(m,X_train)\nfi[:10]","56bb846c":"def plot_fi(fi): return fi.plot('cols','imp','barh',figsize=(12,8),legend=False)\nplot_fi(fi)","f67488e3":"test_set = pd.read_csv(f'{PATH}\/test.csv')","e2ab370e":"test_key = test_set.key\ntest_set.drop('key', axis = 1, inplace = True)","5428fac8":"add_datepart(test_set,'pickup_datetime',drop=True, time=True)\ndistance(test_set)","2b26e44d":"test_predictions = m.predict(test_set)","af2ee650":"submission = pd.DataFrame({'key': test_key, \n                           'fare_amount': test_predictions})\nsubmission.to_csv('submissions.csv', index=False)","ef2db61c":"There are a lot of improvements that can be done:\n\n    1) The obvious one, tuning the model itself rather than using the default Random Forest settings.\n    2) Using the feature importance chart to explore relevant features further and have better pre-processing (almost non-existent here)\n    3) Creating more features, such as the whether the pickup time is at night or day, distance traversed(rather than lat.\/long. traversed), etc.\n    4) If we really want to dig deep, we can use external data sources to come up with features as well.","4887fc49":"Next, let's create a scorer function","2962f612":"## Feature Importance","200929ff":"Even after using 10 times the IQR, about 1% of the data is considered outliers. Let's remove it. (Note: We can have a look at these points to confirm if they are outliers or not)","8c328eca":"## Predictions on test set","b19a47f5":"Hello everyone. This is my first ever kernel on kaggle and is pretty rudimentary. Any criticism, positive or negative is highly appreciated.","4a6786f9":"Let's have a look at the dataset.","f0ad2c18":"The plot shows that the fare is highly dependent on the distance traversed features(longitude_traversed, latitude_traversed). Whereas, there are a few features which have almost a non-existent relationship with the model and may not be needed at all.","50b5ca9d":"## Naive Random Forest","33233417":"## Data Quality check and Outlier Detection","4e5482f1":"Let's drop the key column, as it is same as the pickup_datetime columns.","cf711b0d":"Let's perform the same feature engineering functions as training set to the test set as well","678719c8":"So there are taxis with 208, 129, 51 and 49 passengers as well, which is quite far fetched. Moreover, there are 35263 records of taxis with no passengers at all, let's remove all these data points as well.","13d821ab":"Now, let's have a look at the passenger_count column","82f77da1":"Based on the simple model above, let's have a look at the important features for our prediction.","c29444a2":"## Feature Engineering","b6521a74":"### Submission","c6f83377":"Since we have ample amount of data, let's just drop these few missing lines of data. (Note: We have 45 million more data rows in the training set)","bdd43b7d":"So with a little feature engineering, even an untuned naive Random Forest seems to be performing better than a Linear Regression. Let's make a submission based on this model.","a26a5702":"Let's define a function which creates some more new features based on the pickup and dropoff lat. and long.","52a2abca":"Now, let's do some feature engineering based on the pickup_datetime column.","5667c18d":"Now, instead of using all the data for training the model, let's only use the first 10000 rows for each tree in the forest. This will save us the hassle of waiting for long time to see our result\/changes.","8b3bad12":"## Improvements","c8de46e5":"Woah, this doesn't seem right! More than 50% of points are considered outliers according to this condition. I guess, we can't use datetime and basic lat.\/long. features for outlier detection. Moreover, let's increase the step size to see what we consider outliers.","054a749c":"Let's split our data into training and validation sets. We will have a validation set of about 10000 rows (Same as test set).","2643f6e9":"Next, let's see if there are any missing values in the dataset.","fde17bc3":"Next, let's remove any outliers in the data."}}