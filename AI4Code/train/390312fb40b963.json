{"cell_type":{"7fb384de":"code","2bc971a9":"code","41c7ba91":"code","9515df55":"code","ca9dae9b":"code","a05b9da2":"code","05574ea6":"code","7cd7a1ce":"code","fe4b0cdf":"code","24b7570c":"code","42c1ebe5":"code","e013ff31":"code","dc3bc2e1":"code","7d51e368":"code","6b6cf333":"code","18fad3df":"code","4308e900":"code","9d05b0c3":"code","6866b98c":"code","1f4d7e3f":"code","6defed39":"code","bc9bba00":"code","4177315f":"code","86dfb0f1":"code","14c4cd5d":"code","660d723a":"code","ea64c7e8":"code","28a6bbc5":"code","4c59b601":"code","895e6807":"code","5adcdf5a":"code","27a08385":"markdown","2e2e0242":"markdown","5961c707":"markdown","30f7acff":"markdown","25d9ed24":"markdown","79ff30c2":"markdown","190fd19b":"markdown","60f3bac9":"markdown","f128fb30":"markdown","6468bf43":"markdown","d844ae90":"markdown","3dc9907a":"markdown","2ef22046":"markdown","dcb4e656":"markdown","4ac854b9":"markdown","5e113fe5":"markdown","450efb85":"markdown","27e4c1a7":"markdown","d7c8cc37":"markdown","380372f4":"markdown","9e660977":"markdown","66d34107":"markdown","04b2a663":"markdown","4d21f653":"markdown","716285de":"markdown","66b5f427":"markdown","750aa4ee":"markdown","879f0bfa":"markdown","a82bd999":"markdown"},"source":{"7fb384de":"import os\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential, Model\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers import Input, Lambda, Dense, Flatten, Activation, Dropout\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import RMSprop\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom keras import applications","2bc971a9":"train = list(os.walk('..\/input\/butterfly-classification-dataset\/Train')) #gives the list of all directories and subdirectories","41c7ba91":"label_names = train[0][1]\ndict_labels = dict(zip(label_names, list(range(len(label_names)))))\nprint(dict_labels)","9515df55":"def dataset(path): #put path till the highest directory level\n    images = []\n    labels = []\n    for folder in tqdm(os.listdir(path)):\n        value_of_label = dict_labels[folder] #dict_labels is the dictionary whose key:value pairs are classes:numbers representing them\n\n        for file in (os.listdir(os.path.join(path, folder))):\n            path_of_file = os.path.join(os.path.join(path, folder), file)\n\n            image = cv2.imread(path_of_file)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            image = cv2.resize(image, (150, 150))\n            images.append(image)\n            labels.append(value_of_label)\n\n    images = np.array(images, dtype = 'float32')\/255.0\n    labels = np.array(labels)\n\n    return images, labels\n\nimages, labels = dataset('..\/input\/butterfly-classification-dataset\/Train')\nimages, labels = shuffle(images, labels)","ca9dae9b":"images.shape","a05b9da2":"species = train[0][1]\nno_of_butterflies = []\nfor _ in range(1, 51):\n    no_of_butterflies.append(len(train[_][0]))\nplt.figure(figsize = (8, 8))\nsns.barplot(y = species, x = no_of_butterflies);","05574ea6":"plt.figure(figsize = (10,10))\nfor _ in range(25):\n    plt.subplot(5, 5, _+1)\n    plt.yticks([])\n    plt.xticks([])\n    plt.grid(False)\n    data = images[_]\n    plt.xlabel(label_names[labels[_]])\n    plt.imshow(data);","7cd7a1ce":"image_size = (224, 224)\nbatch_size = 64\ntrain_datagen = ImageDataGenerator(rescale = 1.\/255,\n                            shear_range = 0.4,\n                            zoom_range = 0.4,\n                            horizontal_flip = True,\n                            vertical_flip = True,\n                            validation_split = 0.2)","fe4b0cdf":"train_ds = train_datagen.flow_from_directory('..\/input\/butterfly-classification-dataset\/Train',\n                                      target_size = image_size,\n                                      batch_size = batch_size,\n                                      class_mode = 'categorical',\n                                      subset = 'training',\n                                      color_mode=\"rgb\",)\n\nval_ds = train_datagen.flow_from_directory('..\/input\/butterfly-classification-dataset\/Train',\n                                      target_size = image_size,\n                                      batch_size = batch_size,\n                                      class_mode = 'categorical',\n                                      subset = 'validation',\n                                      color_mode=\"rgb\")","24b7570c":"train_ds.class_indices","42c1ebe5":"fig, ax = plt.subplots(nrows=1, ncols=5, figsize=(15,15))\n\nfor i in range(5):\n    image = next(train_ds)[0][0]\n    image = np.squeeze(image)\n    ax[i].imshow(image)\n    ax[i].axis(False)","e013ff31":"vgg_base = applications.VGG16(weights = 'imagenet', include_top = False, input_shape = (224, 224, 3))\nvgg_base.trainable = False","dc3bc2e1":"inputs = Input(shape=(224, 224, 3))\n\nx = vgg_base(inputs, training=False)\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dense(1024, activation = 'relu')(x)\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(50, activation = 'sigmoid')(x)\nvgg_model = Model(inputs, outputs)\nvgg_model.summary","7d51e368":"vgg_model.compile(\n    optimizer=keras.optimizers.Adam(),\n    loss= keras.losses.CategoricalCrossentropy(from_logits = True),\n    metrics= [keras.metrics.CategoricalAccuracy()],\n)","6b6cf333":"epochs = 25\nvgg_model.fit(train_ds, epochs=epochs, validation_data=val_ds)","18fad3df":"vgg_model.save('vgg.hdf5') #this saves the model with the weights","4308e900":"xcep_base = applications.Xception(weights = 'imagenet', include_top = False, input_shape = (224, 224, 3))\nxcep_base.trainable = False\n\ninputs = Input(shape=(224, 224, 3))\n\nx = xcep_base(inputs, training=False)\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dense(1024, activation = 'relu')(x)\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(50, activation = 'sigmoid')(x)\nxcep_model = Model(inputs, outputs)","9d05b0c3":"xcep_model.compile(\n    optimizer=keras.optimizers.Adam(),\n    loss= keras.losses.CategoricalCrossentropy(from_logits = True),\n    metrics= [keras.metrics.CategoricalAccuracy()],\n)","6866b98c":"epochs = 25\nxcep_model.fit(train_ds, epochs=epochs, validation_data=val_ds)","1f4d7e3f":"xcep_model.save('xcep.hdf5')","6defed39":"res_base = applications.ResNet152V2(weights = 'imagenet', include_top = False, input_shape = (224, 224, 3))\nres_base.trainable = False\n\ninputs = Input(shape=(224, 224, 3))\n\nx = res_base(inputs, training=False)\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dense(1024, activation = 'relu')(x)\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(50, activation = 'sigmoid')(x)\nres_model = Model(inputs, outputs)","bc9bba00":"res_model.compile(\n    optimizer=keras.optimizers.Adam(),\n    loss= keras.losses.CategoricalCrossentropy(from_logits = True),\n    metrics= [keras.metrics.CategoricalAccuracy()],\n)","4177315f":"epochs = 25\nres_model.fit(train_ds, epochs=epochs, validation_data=val_ds)","86dfb0f1":"res_model.save('res.hdf5')","14c4cd5d":"incep_base = applications.InceptionV3(weights = 'imagenet', include_top = False, input_shape = (224, 224, 3))\nincep_base.trainable = False\n\ninputs = Input(shape=(224, 224, 3))\n\nx = incep_base(inputs, training=False)\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dense(1024, activation = 'relu')(x)\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(50, activation = 'sigmoid')(x)\nincep_model = Model(inputs, outputs)","660d723a":"incep_model.compile(\n    optimizer=keras.optimizers.Adam(),\n    loss= keras.losses.CategoricalCrossentropy(from_logits = True),\n    metrics= [keras.metrics.CategoricalAccuracy()],\n)","ea64c7e8":"epochs = 25\nincep_model.fit(train_ds, epochs=epochs, validation_data=val_ds)","28a6bbc5":"incep_model.save('incep.hdf5')","4c59b601":"def predictor(img, model):\n    image = cv2.imread(img)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = cv2.resize(image, (224, 224))\n    image = np.array(image, dtype = 'float32')\/255.0\n    plt.imshow(image)\n    image = image.reshape(1, 224,224,3)\n    \n    label_names = train_ds.class_indices\n    dict_class = dict(zip(list(range(len(label_names))), label_names))\n    clas = model.predict(image).argmax()\n    name = dict_class[clas]\n    print('The given image is of \\nClass: {0} \\nSpecies: {1}'.format(clas, name))","895e6807":"predictor('..\/input\/butterfly-classification-dataset\/Train\/an 88\/076.jpg', res_model) #example image","5adcdf5a":"#modelSaved = keras.models.load_model('.\/inception.hdf5') ","27a08385":"# Loading model","2e2e0242":"This portion is just for visualization and data exploration. \n\nThe visualizations could have been done with Tensorflow's ImageDataGenerator, but exploration with the OS module and CV2 seemed easier.","5961c707":"In machine learning, Transfer Learning is the process of using knowledge previously acquired by solving a problem on a new (current) problem, with or without changes to the knowledge.\n\nThis is possible because when a model is trained, the initial neurons of a CNN architecture extract only the high level information, which gets narrowed down to information about the specific classes with each neural layer. For example, if a model is trained to recognize flowers, it can be further trained to classify different flower species.\n\nThis is needed because it brings trained state-of-the-art models like VGG16, InceptionNet at the disposal of common users. These models have been trained for hundreds of hours on millions of images which is not possible for a dork sitting in his basement on his i5 processor.\n\nSo transfer learning comes to the rescue. From this notebook, you will learn how to make an image classifier using 4 models. The method of training the models is the same, so it shouldn't be much difficult if you learn just one.","30f7acff":"The best way to prepare the images for the model is by using the ImageDataGenerator. Create an object of the ImageDataGenerator and initialize the various parameters. \n\nIf the number of training samples is low, like in the current dataset, it's always better to set the shear_range, zoom_range, and the other image augmentations. This helps to avoid overfitting due to low data and makes the model more robust.","25d9ed24":"## Sample images","79ff30c2":"# Transfer learning","190fd19b":"# Introduction","60f3bac9":"And that's how the model is trained, the next 3 models have also been trained in a similar way.\n\nYou can try modifying the layers, input shape, activation function and see what you get.","f128fb30":"# Dataset","6468bf43":"### Xception","d844ae90":"### VGG16","3dc9907a":"# Testing Model on Own Image","2ef22046":"Use the following code to load a saved model.","dcb4e656":"In the summary, it can be seen that the base is VGG16, while the top is the one that is added.","4ac854b9":"### ResNet","5e113fe5":"This function preprocesses the image so that it can be fed into the trained models. \n\nYou can train the above models and pick an image and plug it into the predictor function to get the output.","450efb85":"The following cell imports the necessary libraries required for this notebook.","27e4c1a7":"* Check out my Github where I have deployed this project as an app on Heroku\n* The dataset for this project is available on my Kaggle profile","d7c8cc37":"# Load images using ImageDatagenerator","380372f4":"## External","9e660977":"### Inception V3","66d34107":"The first model here is the VGG16. It was trained on the imagenet dataset with 1000 classes. The include_top parameter is set False because our dataset has only 50 classes so the output layer has to be modified accordingly.\n\nAlso, trainable is set to False as we do not want to train the base model. ","04b2a663":"### Some augmented images","4d21f653":"## This barplot shows the number of images of each species","716285de":"# Data Visualization","66b5f427":"After this, use the flow_from_directory method to get the images from the directories to feed it into the models.","750aa4ee":"THe next function takes in the outermost directory and returns np array of the images contained in each subdirectory along with their labels","879f0bfa":"The directory structure is in the following form:\n    \n```\nTrain   \n\u2502\n\u2514\u2500\u2500\u2500adonis\n\u2502   \u2502   000.jpg\n\u2502   \u2502   001.jpg\n\u2502   \u2502   ...\n\u2502\n\u2514\u2500\u2500\u2500american anoot\n\u2502   \u2502   000.jpg\n\u2502   \u2502   001.jpg\n\u2502   \u2502   ...\n\u2502\n\u2514\u2500\u2500\u2500an 88\n\u2502   \u2502   000.jpg\n\u2502   \u2502   001.jpg\n\u2502   \u2502   ...\n...\n...\n\n...\n\u2502\n\u2514\u2500\u2500\u2500zebra long wing\n    \u2502   000.jpg\n    \u2502   001.jpg\n    \u2502   ...\n```","a82bd999":"# Importing Necessary Libraries"}}