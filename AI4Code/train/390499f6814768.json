{"cell_type":{"cf049c1b":"code","13a9adac":"code","b08a8d73":"code","a08f1016":"code","6a0bcc03":"code","01fc25c5":"code","0fad1795":"code","4a0e9370":"code","ddb4a5f1":"code","1f7a85bb":"code","a381b1ce":"code","7b29444e":"code","3ac4fe21":"code","ec575f0c":"code","6ca6e341":"code","0e6efa66":"code","9cf976f5":"code","1e3b2cba":"code","c2fc626e":"code","0355f097":"code","7c7367d9":"code","74610311":"code","9308c43e":"code","4d76508d":"code","ac8be74c":"code","456b07e0":"code","0c047000":"code","a6350529":"code","e6d9916f":"code","e50e6d18":"code","8bdd1dfd":"code","3048f0a8":"code","a4ab513e":"code","94a63090":"code","9b967a4d":"code","afefc9a6":"code","3d2c28e9":"markdown","5a99b6e1":"markdown","0828574f":"markdown","53425cc3":"markdown","b45848ef":"markdown","f028c619":"markdown","6a7d47ce":"markdown","038d378d":"markdown","922caf86":"markdown","58624be3":"markdown","6f813a6a":"markdown","474951f3":"markdown","c456019d":"markdown","b41baea7":"markdown","0a58a028":"markdown","739dba17":"markdown","5ee0c49c":"markdown","f348adcc":"markdown","d74c2674":"markdown","3518757b":"markdown","27addca2":"markdown","beedaf00":"markdown","12fc3b3e":"markdown","60844087":"markdown","ba0fb337":"markdown","7be05d4c":"markdown","788842c8":"markdown"},"source":{"cf049c1b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","13a9adac":"df_2019 = pd.read_csv('\/kaggle\/input\/flight-delay-prediction\/Jan_2019_ontime.csv')\ndf_2020 = pd.read_csv('\/kaggle\/input\/flight-delay-prediction\/Jan_2020_ontime.csv')","b08a8d73":"df_2020.head()","a08f1016":"df_2020.tail()","6a0bcc03":"print(df_2020.shape)\nprint(df_2020['Unnamed: 21'].isnull().sum())","01fc25c5":"def bar_plot(variable):\n    var = df_2020[variable] # get feature\n    varValue = var.value_counts() # count number of categorical variable(value\/sample)\n    \n    plt.figure(figsize = (9,6))\n    plt.bar(varValue.index,varValue)\n    plt.xticks(varValue.index,varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{} \\n {}\".format(variable,varValue))","0fad1795":"bar_plot('CANCELLED')","4a0e9370":"print(df_2020.columns)\nprint(df_2020.shape[1])","ddb4a5f1":"df_2020.info()","1f7a85bb":"column_names = df_2020.columns\nj=0\nfor i in df_2020.columns:\n    print(\"  {} has got {} Null Sample \" .format(df_2020.columns[j],df_2020[i].isnull().sum()))\n    j=j+1","a381b1ce":"import missingno as msno\nplt.figure(figsize=(4,4))\nmsno.bar(df_2020)","7b29444e":"msno.heatmap(df_2020) ","3ac4fe21":"#Data Preprocessing\ndf_2020 = df_2020.drop(['Unnamed: 21'],axis=1)\ndf_2020.shape","ec575f0c":"#Drop NaN TAIL_NUM rows\ndf_2020 = df_2020.dropna(subset=['TAIL_NUM'])\nprint(df_2020['TAIL_NUM'].isna().sum())\nprint(df_2020.shape)","6ca6e341":"df_2020['DEP_DEL15'] = df_2020['DEP_DEL15'].replace(np.NaN,0)\ndf_2020['DEP_DEL15'].isnull().sum()","0e6efa66":"df_2020['ARR_DEL15'] = df_2020['ARR_DEL15'].replace(np.NaN,0)\ndf_2020['ARR_DEL15'].isnull().sum()","9cf976f5":"from sklearn.impute import SimpleImputer\nimp_mean = SimpleImputer(missing_values=np.nan,strategy='mean')\n#DEP_TIME\n\ndf_2020['DEP_TIME'] = imp_mean.fit_transform(df_2020[['DEP_TIME']])\n#ARR_TIME\n\ndf_2020['ARR_TIME'] = imp_mean.fit_transform(df_2020[['ARR_TIME']])","1e3b2cba":"column_names = df_2020.columns\nj=0\nfor i in df_2020.columns:\n    print(\"  {} has got {} NaN Sample \" .format(df_2020.columns[j],df_2020[i].isnull().sum()))\n    j=j+1","c2fc626e":"df_2020.shape","0355f097":"import seaborn as sns\nf,ax= plt.subplots(figsize=(15,15))\nsns.heatmap(df_2020.corr(),linewidths=.5,annot=True,fmt='.4f',ax=ax)\nplt.show()","7c7367d9":"df_2020 = df_2020.drop(['DEST_AIRPORT_SEQ_ID'],axis=1)\ndf_2020 = df_2020.drop(['ORIGIN_AIRPORT_SEQ_ID'],axis=1)\nprint(df_2020.shape)","74610311":"bar_plot('CANCELLED')","9308c43e":"y = df_2020.CANCELLED\ndf_2020 = df_2020.drop('CANCELLED',axis=1)\nX = df_2020","4d76508d":"categorical_columns = ['OP_CARRIER','OP_UNIQUE_CARRIER','TAIL_NUM','ORIGIN','DEST','DEP_TIME_BLK']\nfor col in categorical_columns:\n    X_encoded = pd.get_dummies(X[col],prefix_sep = '_')\n    df_2020 = df_2020.drop([col],axis=1)\n\ndf_2020 = pd.concat([df_2020, X_encoded], axis=1)","ac8be74c":"X = df_2020","456b07e0":"\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,shuffle=True,random_state=42)","0c047000":"from sklearn.tree import DecisionTreeClassifier\nclf_dt = DecisionTreeClassifier(random_state = 0)\nmodel_dt = clf_dt.fit(X_train, y_train) ","a6350529":"from sklearn import tree\ntree.plot_tree(model_dt) ","e6d9916f":"from sklearn import metrics\ny_pred = model_dt.predict(X_test)\nprint(metrics.classification_report(y_test,y_pred))","e50e6d18":"y_test.value_counts()","8bdd1dfd":"from sklearn.ensemble import RandomForestClassifier\nclf_rf = RandomForestClassifier(max_depth=50)\nmodel_rf = clf_rf.fit(X_train, y_train)","3048f0a8":"from sklearn import metrics\ny_pred = model_rf.predict(X_test)\nprint(metrics.classification_report(y_test,y_pred))","a4ab513e":"from sklearn.ensemble import AdaBoostClassifier\nclf_ab = RandomForestClassifier()\nmodel_ab = clf_ab.fit(X_train, y_train)","94a63090":"from sklearn import metrics\ny_pred = model_ab.predict(X_test)\nprint(metrics.classification_report(y_test,y_pred))","9b967a4d":"import xgboost as xgb\nclf_xgb = xgb.XGBClassifier()\nmodel_xgb = clf_xgb.fit(X_train, y_train)","afefc9a6":"from sklearn import metrics\ny_pred = model_xgb.predict(X_test)\nprint(metrics.classification_report(y_test,y_pred))","3d2c28e9":"# **CHECK NaN VALUES**","5a99b6e1":"# **Unnamed: 21 already empty. We will clean this column at preprocessing step.**","0828574f":"# if not type 15 min delay : we filled 0","53425cc3":"# **XGBoost Classifier**","b45848ef":"# We cleaned data but we lost (607346 - 600271 = 7075) sample","f028c619":"# CORRELATION MATRIX","6a7d47ce":"# **ARR_DEL15**","038d378d":"# Please report your suggestions. These are important for me to see my mistakes and improve my self.","922caf86":"# **22 column and their names**","58624be3":"# **--------------------------------------------------------------------------------------------------------**","6f813a6a":"# TRAIN - TEST SPLIT","474951f3":"# **if not type 15min delay :  we filled with 0**","c456019d":"# **I'm looking my target feature**","b41baea7":"# **We applied One-Hot Encoder for categorical columns**","0a58a028":"# **We filled NaN values with column's mean**","739dba17":"# TAIL NUM","5ee0c49c":"# **How much NaN Value**","f348adcc":"# **Seperated train and test data**","d74c2674":"# **DEP_TIME and ARR_TIME**","3518757b":"# Data set is imbalance. So we can't trust accuracy metric. We will check other metrics.","27addca2":"# **DEP_DEL15**","beedaf00":"# **Unnamed: 21**","12fc3b3e":"# **Random Forest Classifier**","60844087":"# **Decision Tree Classifier**","ba0fb337":"# DEST_AIRPORT_ID - DEST_AIRPORT_SEQ_ID  and  ORIGIN_AIRPORT_ID - ORIGIN_AIRPORT_SEQ_ID  They are looking same so I gonna drop each one of them","7be05d4c":"# **Ada Boost Classifier**","788842c8":"# **-O-O- PREPROCESSING -0-0-**"}}