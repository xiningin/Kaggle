{"cell_type":{"b0522089":"code","0fc87db7":"code","5dc1d86f":"code","45e4cd40":"code","c1af543e":"code","8a20de48":"code","ac01855d":"code","abb64496":"code","1f83a40d":"code","9966c8f6":"code","01a5131b":"code","5dd0cb7f":"code","cb0e7c6d":"code","9274d33a":"code","31642a0f":"code","9c5bc736":"code","c567b270":"code","24de3d98":"code","0b9a2eb6":"code","cc7ad26c":"code","5afe9a1d":"code","d7d778a3":"code","8c1415a0":"markdown","703d3245":"markdown","64e4485b":"markdown","b1b9b920":"markdown","bda96ee7":"markdown"},"source":{"b0522089":"!pip install whatlies","0fc87db7":"from whatlies.embedding import Embedding\nfrom whatlies.embeddingset import EmbeddingSet\nfrom whatlies.transformers import Umap,Pca,Tsne\nimport os,io\nimport torch\nimport numpy as np","5dc1d86f":"def load_vec(emb_path, nmax=150000):\n    vectors = []\n    word2id = {}\n    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n        next(f)\n        for i, line in enumerate(f):\n            word, vect = line.rstrip().split(' ', 1)\n            vect = np.fromstring(vect, sep=' ')\n            assert word not in word2id, 'word found twice'\n            vectors.append(vect)\n            word2id[word] = len(word2id)\n            if len(word2id) == nmax:\n                break\n    id2word = {v: k for k, v in word2id.items()}\n    embeddings = np.vstack(vectors)\n    embeddings = embeddings \/np.linalg.norm(embeddings,ord=2,axis=1,keepdims=True)\n    return embeddings, id2word, word2id","45e4cd40":"\nsrc_path = '\/kaggle\/input\/fasttext-wikipedia\/wiki.id.vec'\ntgt_path = '\/kaggle\/input\/fasttext-wikipedia\/wiki.en.vec'\nnmax = 2000  # maximum number of word embeddings to load\n\nsrc_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\ntgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)","c1af543e":"\nsrc_embeddings = (src_embeddings - src_embeddings.mean(axis=0))\ntgt_embeddings = (tgt_embeddings - tgt_embeddings.mean(axis=0))","8a20de48":"emb_set_id= []\nemb_set_en= []\nfor i in range(nmax):\n    emb_set_id.append(Embedding(src_id2word[i],src_embeddings[i] ))\n    emb_set_en.append(Embedding(tgt_id2word[i],tgt_embeddings[i] ))\n\nemb_set_id = EmbeddingSet(*emb_set_id)\nemb_set_en = EmbeddingSet(*emb_set_en)\nemb_set_id = emb_set_id.add_property('Language', lambda d: 'Indonesia')\nemb_set_en = emb_set_en.add_property('Language', lambda d: 'English')\n\nmonolingual_we = emb_set_en.merge(emb_set_id)\nplot =monolingual_we.transform(Umap(2,metric='cosine',n_neighbors=5)).plot_interactive(color=\"Language\",annot=False)\nplot.properties(title=\"Unaligned Indonesia-English Embeddings, 2000 most frequent words\", height=600, width=600)","ac01855d":"# plot","abb64496":"# df = monolingual_we.transform(Umap(3,metric='cosine',n_neighbors=5)).to_dataframe()\n","1f83a40d":"# df.columns = df.columns.astype(str)\n# df['word'] = df.index\n# df['language'] = range(1813)","9966c8f6":"!mkdir \/kaggle\/tmp\/\n%cd \/kaggle\/tmp\n!pip install faiss-gpu\n!git clone https:\/\/github.com\/Lungaville\/MUSE.git\n%cd .\/MUSE\n","01a5131b":"\nEMBEDDING_DIMENSION=300","5dd0cb7f":"class Dictionary():\n    def __init__(self,lang,method):\n        self.word2id= {}\n        self.id2word = []\n        self.lang = lang\n        self.emb_matrix = []\n        self.load_emb(method)\n        \n    def load_emb(self,method):\n        lang = self.lang\n        filename=\"\"\n        if lang is \"id\":\n            filename=\"vectors-id.pth\"\n        if lang is \"en\":\n            filename=\"vectors-en.pth\"\n        load_emb = torch.load(f'\/kaggle\/input\/aligned-cross-lingual-word-embedding-iden\/{method}\/{filename}')\n        self.word2id= load_emb['dico'].word2id\n        self.id2word= list(load_emb['dico'].id2word.values())\n        word_embedding = load_emb['vectors'].cpu().detach().numpy()\n        word_embedding = word_embedding \/ np.linalg.norm(word_embedding,ord=2,axis=1,keepdims=True)\n#         unit length normalization\n#         word_embedding = word_embedding \/ np.linalg.norm(word_embedding,axis=0)\n        self.emb_matrix= list(word_embedding)\n#         del load_emb\n        self.word2id[\"<UKN>\"] = len(self.id2word)\n        self.emb_matrix.append(np.zeros( EMBEDDING_DIMENSION))\n        self.id2word.append(\"<UKN>\")\n","cb0e7c6d":"emb_id_bilingual = Dictionary(\"id\",'bilingual')\nemb_id_identical = Dictionary(\"id\",'identical')\nemb_id_unsupervised = Dictionary(\"id\",'unsupervised')\nemb_en = Dictionary(\"en\",\"bilingual\")\n","9274d33a":"# clwe['you']","31642a0f":"\nselected_embedding =emb_id_unsupervised\nwhatlies_emb_id = []\nwhatlies_emb_en= []\nfor i in range(2000):\n    word = selected_embedding.id2word[i]\n    emb =  selected_embedding.emb_matrix[i]\n    whatlies_emb_id.append(Embedding(word,emb) )\nfor i in range(2000):\n    word = emb_en.id2word[i]\n    emb =  emb_en.emb_matrix[i]\n    whatlies_emb_en.append(Embedding(word,emb) )\nemb_set_id = EmbeddingSet(*whatlies_emb_id)\nemb_set_en = EmbeddingSet(*whatlies_emb_en)\nemb_set_id = emb_set_id.add_property('Language', lambda d: 'Indonesia')\nemb_set_en = emb_set_en.add_property('Language', lambda d: 'English')\n\nclwe = emb_set_en.merge(emb_set_id)\nclwe =clwe.transform(Umap(2,metric='cosine',n_neighbors=15))\nplot_annot_false = clwe.plot_interactive(color=\"Language\",annot=False)\nplot_annot_false.properties(title=\"CLWE Indonesia-English, 2000 most frequent words\", height=600, width=600)\n","9c5bc736":"plot_annot_true = clwe.plot_interactive(color=\"Language\",annot=False)\nplot_annot_true.properties(title=\"CLWE Indonesia-English, 2000 most frequent words\", height=600, width=600)\n","c567b270":"def get_embeddings_from_dictionary(word,dico):\n    idx =dico.word2id[word]\n    emb = dico.emb_matrix[idx]\n    return Embedding(word,emb)\n\nnumbers_id = [\"ihsg\",\"saham\",\"kata\",\"gaji\",\"diskon\",\"binatang\",\"ular\",\"botol\",\"baju\",\"cemilan\",\"makan\",\"jalan\",\"minum\",\"anjing\"]\nnumbers_en = [\"dow\",\"stock\",\"word\",\"salary\",\"discount\",\"animal\",\"snake\",\"bottle\",\"shirt\",\"snack\",\"eat\",\"walk\",\"drink\",\"dog\"]\nembeddings_set_numbers_id  = []\nembeddings_set_numbers_en  = []\nfor word in numbers_id:\n    x= get_embeddings_from_dictionary(word,emb_id_identical)\n    embeddings_set_numbers_id.append(x)\nfor word in numbers_en:\n    x= get_embeddings_from_dictionary(word,emb_en)\n    embeddings_set_numbers_en.append(x)\n\nembeddings_set_numbers_id = EmbeddingSet(*embeddings_set_numbers_id)\nembeddings_set_numbers_en = EmbeddingSet(*embeddings_set_numbers_en)\nembeddings_set_numbers_id = embeddings_set_numbers_id.add_property('Language', lambda d: 'Indonesia')\nembeddings_set_numbers_en = embeddings_set_numbers_en.add_property('Language', lambda d: 'English')\n\nclwe_number = embeddings_set_numbers_en.merge(embeddings_set_numbers_id)\nplot =clwe_number.transform(Umap(2,metric='cosine',n_neighbors=5)).plot_interactive(color=\"Language\")\nplot.properties(title=\"CLWE Indonesia-English, Numbers\", height=600, width=900)\n","24de3d98":"\ndef load_dictionary(path, word2id1, word2id2):\n    \"\"\"\n    Return a torch tensor of size (n, 2) where n is the size of the\n    loader dictionary, and sort it by source word frequency.\n    \"\"\"\n    assert os.path.isfile(path)\n\n    unique_id = set()\n    unique_en = set()\n    not_found = 0\n    not_found1 = 0\n    not_found2 = 0\n\n    with io.open(path, 'r', encoding='utf-8') as f:\n        for index, line in enumerate(f):\n            assert line == line.lower()\n            parts = line.rstrip().split()\n            if len(parts) < 2:\n                logger.warning(\"Could not parse line %s (%i)\", line, index)\n                continue\n            word1, word2 = parts\n            if word1 in word2id1 and word2 in word2id2:\n#                 pairs.append((word1, word2))\n                unique_id.add(word1)\n                unique_en.add(word2)\n            else:\n                not_found += 1\n                not_found1 += int(word1 not in word2id1)\n                not_found2 += int(word2 not in word2id2)\n        print(not_found1)\n        return list(unique_id), list(unique_en)\ndef load_dictionary_1_1(path, word2id1, word2id2):\n    \"\"\"\n    Return a torch tensor of size (n, 2) where n is the size of the\n    loader dictionary, and sort it by source word frequency.\n    \"\"\"\n    assert os.path.isfile(path)\n\n    pairs = []\n    unique_indonesia= []\n    unique_id = set()\n    unique_en = set()\n    not_found = 0\n    not_found1 = 0\n    not_found2 = 0\n\n    with io.open(path, 'r', encoding='utf-8') as f:\n        for index, line in enumerate(f):\n            assert line == line.lower()\n            parts = line.rstrip().split()\n            if len(parts) < 2:\n                logger.warning(\"Could not parse line %s (%i)\", line, index)\n                continue\n            word1, word2 = parts\n            if word1 in word2id1 and word2 in word2id2:\n                if word1 not in unique_indonesia:\n                    unique_indonesia.append(word1)\n                    unique_id.add(word1)\n                    unique_en.add(word2)\n            else:\n                not_found += 1\n                not_found1 += int(word1 not in word2id1)\n                not_found2 += int(word2 not in word2id2)\n\n    # sort the dictionary by source word frequencies\n    pairs = sorted(pairs, key=lambda x: word2id1[x[0]])\n    dico = torch.LongTensor(len(pairs), 2)\n    for i, (word1, word2) in enumerate(pairs):\n        dico[i, 0] = word2id1[word1]\n        dico[i, 1] = word2id2[word2]\n\n    return list(unique_id), list(unique_en)","0b9a2eb6":"bilingual_dictionary = load_dictionary('\/kaggle\/input\/bilingual-dictionary-iden\/id-en.5000-6500.txt',emb_id_identical.word2id,emb_en.word2id)","cc7ad26c":"print(len(bilingual_dictionary[1]))","5afe9a1d":"words_id = bilingual_dictionary[0]\nwords_en = bilingual_dictionary[1]\nembeddings_set_words_id  = []\nembeddings_set_words_en  = []\nfor word in words_id:\n    x= get_embeddings_from_dictionary(word,emb_id_bilingual)\n    embeddings_set_words_id.append(x)\nfor word in words_en:\n    x= get_embeddings_from_dictionary(word,emb_en)\n    embeddings_set_words_en.append(x)\n\nembeddings_set_words_id = EmbeddingSet(*embeddings_set_words_id)\nembeddings_set_words_en = EmbeddingSet(*embeddings_set_words_en)\nembeddings_set_words_id = embeddings_set_words_id.add_property('Language', lambda d: 'Indonesia')\nembeddings_set_words_en = embeddings_set_words_en.add_property('Language', lambda d: 'English')\n\nclwe_dico = embeddings_set_words_en.merge(embeddings_set_words_id)\ntransform_umap =clwe_dico.transform(Umap(2,metric='cosine',n_neighbors=15, min_dist=0.05,random_state=42))\nplot_annot_false = transform_umap.plot_interactive(color=\"Language\",annot=False)\nplot_annot_false.properties(title=\"CLWE Indonesia-English, Dico\", height=600, width=600)\n","d7d778a3":"plot_annot_true = transform_umap.plot_interactive(color=\"Language\",annot=True)\nplot_annot_true.properties(title=\"Kluster nama negara\", height=800, width=800)","8c1415a0":"# Plot 2000 most frequent word after alignment","703d3245":"# Plot some common words\n### The 2000 frequents word from two embeddings is different. It is better if we had a bilingual dictionary data.","64e4485b":"# First lets try to visualize monolingual embeddings","b1b9b920":"# Reference\n\n1. https:\/\/github.com\/facebookresearch\/MUSE\n2. https:\/\/umap-learn.readthedocs.io\/en\/latest\/\n3. https:\/\/rasahq.github.io\/whatlies\/","bda96ee7":"# Plot using bilingual dictionary data, from MUSE library"}}