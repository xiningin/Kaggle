{"cell_type":{"86c150db":"code","08f193f6":"code","227c5aff":"code","fd0bb32e":"code","758ba513":"code","4ca8da4c":"code","05cd644f":"code","0dfcb8d3":"code","14461e03":"code","04ae54a6":"code","936bb99a":"code","31d359b5":"code","85e59472":"code","02358680":"code","6eb8d179":"code","4d8c395c":"code","e4183c63":"code","74f93875":"code","0a145b7b":"code","ce33e23f":"code","c6149b3e":"code","99d846f2":"code","16f5f738":"code","1cee4347":"code","65d38504":"code","a9ab2fd5":"code","9f0a59a6":"code","0a562fea":"code","99fb4974":"code","29380a43":"code","9a973663":"code","352233ab":"code","2cc05816":"code","192915dc":"code","b2bf45aa":"code","7630f36c":"code","5bd17623":"code","77931a01":"code","0053e7eb":"code","b6d8f11f":"code","72c65a9b":"code","e8e249c9":"code","8136d7ff":"code","76add15a":"code","65008fc3":"code","0f5efbef":"code","cfd501cd":"code","d5d15080":"code","1b886737":"code","d4bdd8e5":"code","801847ae":"code","05cfb108":"code","e4f4e96c":"code","a3811b55":"code","0c3c9e9d":"code","a97e1b52":"code","38dbd92f":"code","aa37c6d7":"code","6ebe9890":"code","184aa3d9":"code","fe09eaeb":"code","c2368b51":"code","cd086528":"code","40893a41":"code","fde35f05":"code","5b104830":"code","63fc446e":"code","1bdb3935":"code","de1f3e5a":"code","42be2715":"markdown","6bb8d13a":"markdown","f4679de1":"markdown","fb41384d":"markdown","04907a6f":"markdown","258281fc":"markdown","dd6de8ce":"markdown","36182239":"markdown","e600f413":"markdown","6fb65a09":"markdown","29bab5c6":"markdown","988aabeb":"markdown","b66d1301":"markdown","8a447871":"markdown","d39b4c87":"markdown"},"source":{"86c150db":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport math, time, random, datetime\nimport catboost\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier, Pool, cv\n\n# Let's be rebels and ignore warnings for now\nimport warnings\nwarnings.filterwarnings('ignore')","08f193f6":"train = pd.read_csv('..\/input\/titanic\/train.csv')","227c5aff":"train.head()","fd0bb32e":"train.isnull().sum()","758ba513":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","4ca8da4c":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',data=train)","05cd644f":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Sex',data=train,palette='RdBu_r')","0dfcb8d3":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Pclass',data=train,palette='rainbow')","14461e03":"sns.distplot(train['Age'].dropna(),kde=False,color='darkred',bins=40)","04ae54a6":"sns.countplot(x='SibSp',data=train)","936bb99a":"import cufflinks as cf\ncf.go_offline()","31d359b5":"train['Fare'].iplot(kind='hist',bins=30,color='green')","85e59472":"\nplt.figure(figsize=(12, 7))\nsns.boxplot(x='Pclass',y='Age',data=train,palette='winter')","02358680":"def impute_age(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Age):\n\n        if Pclass == 1:\n            return 37\n\n        elif Pclass == 2:\n            return 29\n\n        else:\n            return 24\n\n    else:\n        return Age","6eb8d179":"train['Age'] = train[['Age','Pclass']].apply(impute_age,axis=1)","4d8c395c":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","e4183c63":"train.drop('Cabin',axis=1,inplace=True)","74f93875":"train.head()","0a145b7b":"train.dropna(inplace=True)","ce33e23f":"train.info()","c6149b3e":"train.head()","99d846f2":"new_train=pd.DataFrame()","16f5f738":"new_train['Survived'] = train['Survived']\nnew_train['Pclass'] = train['Pclass']\nnew_train['Sex'] = train['Sex']\nnew_train['SibSp'] = train['SibSp']\nnew_train['Parch'] = train['Parch']\nnew_train['Fare'] = train['Fare']\nnew_train['Embarked']=train['Embarked']","1cee4347":"new_train.head()","65d38504":"# One hot encode the categorical columns\ndf_embarked_one_hot = pd.get_dummies(new_train['Embarked'], \n                                     prefix='embarked')\n\ndf_sex_one_hot = pd.get_dummies(new_train['Sex'], \n                                prefix='sex')\n\ndf_plcass_one_hot = pd.get_dummies(new_train['Pclass'], \n                                   prefix='pclass')","a9ab2fd5":"train_enc = pd.concat([new_train, \n                        df_embarked_one_hot, \n                        df_sex_one_hot, \n                        df_plcass_one_hot], axis=1)\n\n# Drop the original categorical columns (because now they've been one hot encoded)\ntrain_enc = train_enc.drop(['Pclass', 'Sex', 'Embarked'], axis=1)","9f0a59a6":"train_enc","0a562fea":"# Seclect the dataframe we want to use first for predictions\nselected_df = train_enc","99fb4974":"selected_df.head()","29380a43":"# Split the dataframe into data and labels\nX_train = selected_df.drop('Survived', axis=1) # data\ny_train = selected_df.Survived # labels","9a973663":"# Shape of the data (without labels)\nX_train.shape","352233ab":"X_train.head()","2cc05816":"# Shape of the labels\ny_train.shape","192915dc":"# Function that runs the requested algorithm and returns the accuracy metrics\ndef fit_ml_algo(algo, X_train, y_train, cv):\n    \n    # One Pass\n    model = algo.fit(X_train, y_train)\n    acc = round(model.score(X_train, y_train) * 100, 2)\n    \n    # Cross Validation \n    train_pred = model_selection.cross_val_predict(algo, \n                                                  X_train, \n                                                  y_train, \n                                                  cv=cv, \n                                                  n_jobs = -1)\n    # Cross-validation accuracy metric\n    acc_cv = round(metrics.accuracy_score(y_train, train_pred) * 100, 2)\n    \n    return train_pred, acc, acc_cv","b2bf45aa":"# Logistic Regression\nstart_time = time.time()\ntrain_pred_log, acc_log, acc_cv_log = fit_ml_algo(LogisticRegression(), \n                                                               X_train, \n                                                               y_train, \n                                                                    10)\nlog_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_log)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_log)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=log_time))\n","7630f36c":"# k-Nearest Neighbours\nstart_time = time.time()\ntrain_pred_knn, acc_knn, acc_cv_knn = fit_ml_algo(KNeighborsClassifier(), \n                                                  X_train, \n                                                  y_train, \n                                                  10)\nknn_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_knn)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_knn)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=knn_time))","5bd17623":"# Gaussian Naive Bayes\nstart_time = time.time()\ntrain_pred_gaussian, acc_gaussian, acc_cv_gaussian = fit_ml_algo(GaussianNB(), \n                                                                      X_train, \n                                                                      y_train, \n                                                                           10)\ngaussian_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_gaussian)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_gaussian)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=gaussian_time))","77931a01":"# Linear SVC\nstart_time = time.time()\ntrain_pred_svc, acc_linear_svc, acc_cv_linear_svc = fit_ml_algo(LinearSVC(),\n                                                                X_train, \n                                                                y_train, \n                                                                10)\nlinear_svc_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_linear_svc)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_linear_svc)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=linear_svc_time))","0053e7eb":"# Stochastic Gradient Descent\nstart_time = time.time()\ntrain_pred_sgd, acc_sgd, acc_cv_sgd = fit_ml_algo(SGDClassifier(), \n                                                  X_train, \n                                                  y_train,\n                                                  10)\nsgd_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_sgd)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_sgd)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=sgd_time))","b6d8f11f":"# Decision Tree Classifier\nstart_time = time.time()\ntrain_pred_dt, acc_dt, acc_cv_dt = fit_ml_algo(DecisionTreeClassifier(), \n                                                                X_train, \n                                                                y_train,\n                                                                10)\ndt_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_dt)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_dt)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=dt_time))","72c65a9b":"# Gradient Boosting Trees\nstart_time = time.time()\ntrain_pred_gbt, acc_gbt, acc_cv_gbt = fit_ml_algo(GradientBoostingClassifier(), \n                                                                       X_train, \n                                                                       y_train,\n                                                                       10)\ngbt_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_gbt)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_gbt)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=gbt_time))","e8e249c9":"# View the data for the CatBoost model\nX_train.head()","8136d7ff":"# View the labels for the CatBoost model\ny_train.head()","76add15a":"# Define the categorical features for the CatBoost model\ncat_features = np.where(X_train.dtypes != np.float)[0]\ncat_features","65008fc3":"# Use the CatBoost Pool() function to pool together the training data and categorical feature labels\ntrain_pool = Pool(X_train, \n                  y_train,\n                  cat_features)","0f5efbef":"y_train.head()\n","cfd501cd":"# CatBoost model definition\ncatboost_model = CatBoostClassifier(iterations=1000,\n                                    custom_loss=['Accuracy'],\n                                    loss_function='Logloss')\n\n# Fit CatBoost model\ncatboost_model.fit(train_pool,\n                   plot=True)\n\n# CatBoost accuracy\nacc_catboost = round(catboost_model.score(X_train, y_train) * 100, 2)","d5d15080":"# How long will this take?\nstart_time = time.time()\n\n# Set params for cross-validation as same as initial model\ncv_params = catboost_model.get_params()\n\n# Run the cross-validation for 10-folds (same as the other models)\ncv_data = cv(train_pool,\n             cv_params,\n             fold_count=10,\n             plot=True)\n\n# How long did it take?\ncatboost_time = (time.time() - start_time)\n\n# CatBoost CV results save into a dataframe (cv_data), let's withdraw the maximum accuracy score\nacc_cv_catboost = round(np.max(cv_data['test-Accuracy-mean']) * 100, 2)","1b886737":"# Print out the CatBoost model metrics\nprint(\"---CatBoost Metrics---\")\nprint(\"Accuracy: {}\".format(acc_catboost))\nprint(\"Accuracy cross-validation 10-Fold: {}\".format(acc_cv_catboost))\nprint(\"Running Time: {}\".format(datetime.timedelta(seconds=catboost_time)))","d4bdd8e5":"models = pd.DataFrame({\n    'Model': ['KNN', 'Logistic Regression', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree', 'Gradient Boosting Trees',\n              'CatBoost'],\n    'Score': [\n        acc_knn, \n        acc_log,  \n        acc_gaussian, \n        acc_sgd, \n        acc_linear_svc, \n        acc_dt,\n        acc_gbt,\n        acc_catboost\n    ]})\nprint(\"---Reuglar Accuracy Scores---\")\nmodels.sort_values(by='Score', ascending=False)\n","801847ae":"cv_models = pd.DataFrame({\n    'Model': ['KNN', 'Logistic Regression', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree', 'Gradient Boosting Trees',\n              'CatBoost'],\n    'Score': [\n        acc_cv_knn, \n        acc_cv_log,      \n        acc_cv_gaussian, \n        acc_cv_sgd, \n        acc_cv_linear_svc, \n        acc_cv_dt,\n        acc_cv_gbt,\n        acc_cv_catboost\n    ]})\nprint('---Cross-validation Accuracy Scores---')\ncv_models.sort_values(by='Score', ascending=False)","05cfb108":"# Feature Importance\ndef feature_importance(model, data):\n    \"\"\"\n    Function to show which features are most important in the model.\n    ::param_model:: Which model to use?\n    ::param_data:: What data to use?\n    \"\"\"\n    fea_imp = pd.DataFrame({'imp': model.feature_importances_, 'col': data.columns})\n    fea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False]).iloc[-30:]\n    _ = fea_imp.plot(kind='barh', x='col', y='imp', figsize=(20, 10))\n    return fea_imp\n    ","e4f4e96c":"# Plot the feature importance scores\nfeature_importance(catboost_model, X_train)","a3811b55":"metrics = ['Precision', 'Recall', 'F1', 'AUC']\n\neval_metrics = catboost_model.eval_metrics(train_pool,\n                                           metrics=metrics,\n                                           plot=True)\n\nfor metric in metrics:\n    print(str(metric)+\": {}\".format(np.mean(eval_metrics[metric])))","0c3c9e9d":"# We need our test dataframe to look like this one\nX_train.head()","a97e1b52":"test=pd.read_csv('..\/input\/titanic\/test.csv')\ngender_submission=pd.read_csv('..\/input\/titanic\/gender_submission.csv')","38dbd92f":"# Our test dataframe has some columns our model hasn't been trained on\ntest.head()","aa37c6d7":"# One hot encode the columns in the test data frame (like X_train)\ntest_embarked_one_hot = pd.get_dummies(test['Embarked'], \n                                       prefix='embarked')\n\ntest_sex_one_hot = pd.get_dummies(test['Sex'], \n                                prefix='sex')\n\ntest_plcass_one_hot = pd.get_dummies(test['Pclass'], \n                                   prefix='pclass')","6ebe9890":"# Combine the test one hot encoded columns with test\ntest = pd.concat([test, \n                  test_embarked_one_hot, \n                  test_sex_one_hot, \n                  test_plcass_one_hot], axis=1)","184aa3d9":"# Let's look at test, it should have one hot encoded columns now\ntest.head()","fe09eaeb":"# Create a list of columns to be used for the predictions\nwanted_test_columns = X_train.columns\nwanted_test_columns","c2368b51":"# Make a prediction using the CatBoost model on the wanted columns\npredictions = catboost_model.predict(test[wanted_test_columns])","cd086528":"# Our predictions array is comprised of 0's and 1's (Survived or Did Not Survive)\npredictions[:20]","40893a41":"# Create a submisison dataframe and append the relevant columns\nsubmission = pd.DataFrame()\nsubmission['PassengerId'] = test['PassengerId']\nsubmission['Survived'] = predictions # our model predictions on the test dataset\nsubmission.head()","fde35f05":"# What does our submission have to look like?\ngender_submission.head()","5b104830":"# Let's convert our submission dataframe 'Survived' column to ints\nsubmission['Survived'] = submission['Survived'].astype(int)\nprint('Converted Survived column to integers.')","63fc446e":"# Are our test and submission dataframes the same length?\nif len(submission) == len(test):\n    print(\"Submission dataframe is the same length as test ({} rows).\".format(len(submission)))\nelse:\n    print(\"Dataframes mismatched, won't be able to submit to Kaggle.\")","1bdb3935":"# for Kaggle submisison\nsubmission.to_csv('catboost_submission.csv', index=False)\nprint('Submission CSV is ready!')","de1f3e5a":"# Check the submission csv to make sure it's in the right format\nsubmissions_check = pd.read_csv(\".\/catboost_submission.csv\")\nsubmissions_check.head()","42be2715":"# Exploratory Data Analysis\n\nLet's begin some exploratory data analysis! We'll start by checking out missing data!\n\n## Missing Data\n\nWe can use seaborn to create a simple heatmap to see where we are missing data!","6bb8d13a":"#### We understand that people belonging to class 3 are the ones who were given less priority","f4679de1":"### Heatmap to check null values","fb41384d":"### Cufflinks for plots ","04907a6f":"\n\n## Converting Categorical Features \n\nWe'll need to convert categorical features to dummy variables using pandas! Otherwise our machine learning algorithm won't be able to directly take in those features as inputs.","258281fc":"Now let's check that heat map again!","dd6de8ce":"\n\n## Import Libraries\nLet's import some libraries to get started!","36182239":"## The Data\n\nLet's start by reading in the titanic_train.csv file into a pandas dataframe.","e600f413":"Now apply that function!","6fb65a09":"We can see the wealthier passengers in the higher classes tend to be older, which makes sense. We'll use these average age values to impute based on Pclass for Age.","29bab5c6":"#### From the above graph we understand that \"Sex\" was a important factor on titanic\n#### Number of females survived are more compared to male ","988aabeb":"## Age distribution of the passengers","b66d1301":"### First 5 rows of the Dataset","8a447871":"___\n## Data Cleaning\nWe want to fill in missing age data instead of just dropping the missing age data rows. One way to do this is by filling in the mean age of all the passengers (imputation).\nHowever we can be smarter about this and check the average age by passenger class. For example:\n","d39b4c87":"Great! Let's go ahead and drop the Cabin column and the row in Embarked that is NaN."}}