{"cell_type":{"3b30a6f5":"code","d3643f5a":"code","9db562fe":"code","83a69dd5":"code","6a71426b":"code","dd35a38d":"code","8a53aa20":"code","a680cba3":"code","4c200a1e":"code","20480d2d":"code","0f47c441":"code","31696cd8":"code","5cbe12fe":"code","9774b9e6":"code","f73c1cf5":"code","8ad6cb67":"code","26596711":"code","a8dfad13":"code","0802e1fa":"code","68af13b2":"code","62bf3625":"code","fbe23010":"code","4d11cc22":"code","16f34ca9":"code","6a051872":"code","78bf3c92":"code","090b568f":"code","7b2310a2":"code","47af6a85":"code","a2360f22":"code","af7c0297":"code","6219fdf6":"code","a5824764":"code","33024c6d":"code","211508eb":"code","b0b49c86":"code","a07d3712":"code","e1886e31":"code","569652c4":"code","5e5e49d5":"code","1302a659":"code","ec96a2d1":"code","9b275e3d":"code","86e627b9":"code","a0854e20":"code","eb7a583e":"code","56de2f5e":"code","4542fe0b":"code","70579030":"code","4300d0d9":"code","af9f8ecd":"code","b89fda50":"code","e46ba285":"code","bc9de84b":"code","7bbec9a7":"code","461ccc66":"code","a2a6eecc":"code","8fee1d37":"code","92c26583":"code","5b0e1cb2":"code","8663b290":"code","790a0569":"code","0daa93d3":"code","495dc9a4":"code","19462555":"code","85662504":"code","8efba79d":"code","d20ee5d9":"code","0f84d880":"code","50a30376":"code","49286a19":"code","eb2c7408":"code","b9bf805f":"code","b27dd1ab":"code","084f16d2":"code","4c89318e":"code","c2daf05e":"code","c01ff69d":"code","70e6be65":"code","260320c6":"code","be0773ad":"code","25b0f390":"code","7e1bb6c4":"markdown","1f7d521e":"markdown","2ecf76ff":"markdown","c12ee64d":"markdown","4691df07":"markdown","74829d7a":"markdown","a736949a":"markdown","2c457f8f":"markdown","b3708c6e":"markdown","fdf42de6":"markdown","7c31f9ce":"markdown","d6dc5b8c":"markdown","02081d16":"markdown","89e88941":"markdown","72521340":"markdown","f961950d":"markdown","4038b65e":"markdown","356d9eb2":"markdown","b88e498f":"markdown","48d4d355":"markdown","f203843b":"markdown","b6b4b769":"markdown","077c3da9":"markdown","a08ee1c2":"markdown","07b370a4":"markdown","b40d1fab":"markdown","05af2f58":"markdown","25ca7e70":"markdown","8b4c6fda":"markdown","ef821f29":"markdown","c658855c":"markdown","fe49189c":"markdown","e52f01f7":"markdown","5521678c":"markdown","e3e725aa":"markdown","5e27b273":"markdown","18f55de8":"markdown"},"source":{"3b30a6f5":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","d3643f5a":"#Data Visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"white\")","9db562fe":"from imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom category_encoders import CatBoostEncoder","83a69dd5":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")","6a71426b":"train.head()","dd35a38d":"print('Concise summary of training data:\\n')\nprint(train.info())\nprint('-'*50,'\\nConcise summary of testing data:\\n')\nprint(test.info())","8a53aa20":"#dropping PassengerId as it provides no purpose.\ntrain.drop('PassengerId',1,inplace=True)\nPID = test.PassengerId\ntest.drop('PassengerId',1,inplace=True)\n#We will work through rest of the columns.","a680cba3":"# categorical data\ntrain.select_dtypes(\"object\").columns","4c200a1e":"print(\"Survival Percentage in Titanic:\")\nprint(round(train.Survived.sum()*100\/len(train),2))\nsns.countplot(y=train.Survived, orient='h', palette='Accent')","20480d2d":"sns.countplot(train.Pclass, hue=train.Survived)","0f47c441":"#let's extract class 3 and drop the column Pclass.\ntrain['isPclass3'] = train['Pclass']==3\ntest['isPclass3'] = test['Pclass']==3","31696cd8":"print(train.Name.sample(10))\nprint('\\nTotal no of unique names:',train.Name.nunique())","5cbe12fe":"#Extracting title from name\ntitle = train.Name.str.extract('([A-Za-z]+)\\.')\nttitle = test.Name.str.extract('([A-Za-z]+)\\.')\nplt.xticks(rotation=90)\nsns.countplot(title[0], palette='tab20', hue=train.Survived)\nplt.xlabel('Title')","9774b9e6":"title = title[0].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'Countess', 'Dona'], 'Miss\/Mrs\/Ms')\nttitle = ttitle[0].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'Countess', 'Dona'], 'Miss\/Mrs\/Ms')\n\ntitle = title.replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Misc')\nttitle = ttitle.replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Misc')\n","f73c1cf5":"train = train.join(title).rename(columns = {0:'Title'}) #append the generated feature to our dataset\ntest = test.join(ttitle).rename(columns = {0:'Title'}) #append the generated feature to our dataset","8ad6cb67":"plt.xticks(rotation=90)\nsns.countplot('Title',data=train,hue='Survived', palette='Set2')","26596711":"sns.countplot(train.Sex, hue=train.Survived)","a8dfad13":"train['isMale'] = train.Sex=='male'\ntest['isMale'] = test.Sex=='male'","0802e1fa":"Ticket = train.Ticket.str.strip().str[0]\nTicket_test = test.Ticket.str.strip().str[0]","68af13b2":"plt.figure(figsize=(14,6))\n\nsns.countplot('Ticket',data=train.drop('Ticket',1).join(Ticket),hue='Survived', palette=\"Accent\")","62bf3625":"train = train.drop('Ticket',1).join(Ticket).rename(columns={'Ticket':'Ticket'})\ntest = test.drop('Ticket',1).join(Ticket_test).rename(columns={'Ticket':'Ticket'})","fbe23010":"train['Ticket_preferred']=train.Ticket.apply(lambda s: s in ['3','A','S','C','7','W','4'])\ntest['Ticket_preferred']=test.Ticket.apply(lambda s: s in ['3','A','S','C','7','W','4'])","4d11cc22":"print(\"Total no of null values in Cabin column:\",train.Cabin.isnull().sum())","16f34ca9":"Cabin = train.Cabin.str[0]\nCabin_test = test.Cabin.str[0]\nCabin.value_counts()","6a051872":"#let's append it to our dataset\ntrain = train.drop('Cabin',1).join(Cabin)\ntest = test.drop('Cabin',1).join(Cabin_test)","78bf3c92":"sns.countplot(train.Cabin, hue=train.Survived)","090b568f":"train['Cabin_preferred']=train.Cabin.apply(lambda s: s in ['C','E','D','B','F'])\ntest['Cabin_preferred']=test.Cabin.apply(lambda s: s in ['C','E','D','B','F'])","7b2310a2":"sns.countplot(train.Embarked, hue=train.Survived, palette='seismic')","47af6a85":"print('Total no of missing values in Embarked:',train.Embarked.isna().sum())","a2360f22":"train.Embarked.fillna('S',inplace=True)","af7c0297":"train['ifEmbarkedS'] = train.Embarked =='S'\ntest['ifEmbarkedS'] = test.Embarked =='S'","6219fdf6":"train.drop('Pclass',1, inplace = True)\ntest.drop('Pclass',1, inplace = True)\n\ntrain.drop('Name',1, inplace = True)\ntest.drop('Name',1, inplace = True)\n\ntrain.drop('Title',1, inplace = True)\ntest.drop('Title',1, inplace = True)\n\ntrain.drop('Sex',1, inplace = True)\ntest.drop('Sex',1, inplace = True)\n\ntrain = train.drop('Ticket',1)\ntest = test.drop('Ticket',1)\n\ntrain = train.drop('Cabin',1)\ntest = test.drop('Cabin',1)\n\ntrain = train.drop('Embarked',1)\ntest = test.drop('Embarked',1)","a5824764":"#descriptive statistics of numerical data\ntrain.drop(['Survived'],1).describe()","33024c6d":"print('Total no of missing values in Age:',train.Age.isna().sum())","211508eb":"train.loc[:,'Age'] = train.loc[:,'Age'].fillna(train.groupby(['isMale','Ticket_preferred'])['Age'].transform('median'))\ntrain.loc[:,'Age'] = train.loc[:,'Age'].fillna(train['Age'].median())","b0b49c86":"test.loc[:,'Age'] = test.loc[:,'Age'].fillna(test.groupby(['isMale','Ticket_preferred'])['Age'].transform('median'))\ntest.loc[:,'Age'] = test.loc[:,'Age'].fillna(test['Age'].median())","a07d3712":"sns.distplot(train[train.Survived==0].Age,color='r')\nsns.distplot(train[train.Survived==1].Age,color='g')","e1886e31":"train[['isPclass3','Age']].groupby('isPclass3').mean()","569652c4":"train['Family_size'] = train['SibSp'] + train['Parch'] + 1\ntest['Family_size'] = test['SibSp'] + test['Parch'] + 1","5e5e49d5":"sns.distplot(train[train.Survived==0].Family_size,color='r')\nsns.distplot(train[train.Survived==1].Family_size,color='g')","1302a659":"train['isAlone']= train.Family_size==1\ntest['isAlone']= test.Family_size==1","ec96a2d1":"#let's drop sibsp, parch and family_size as their work is done.\ntrain.drop(['SibSp','Parch','Family_size'],1,inplace=True)\ntest.drop(['SibSp','Parch','Family_size'],1,inplace=True)","9b275e3d":"sns.boxplot(train.Fare, hue=train.Survived)","86e627b9":"train[train.Fare>400].Fare = np.nan\ntrain.loc[:,'Fare'] = train.loc[:,'Fare'].fillna(train.groupby(['isMale','Ticket_preferred'])['Fare'].transform('mean'))\ntest.loc[:,'Fare'] = test.loc[:,'Fare'].fillna(test.groupby(['isMale','Ticket_preferred'])['Fare'].transform('mean'))","a0854e20":"fig, ax = plt.subplots(1,2, figsize=(16,6))\nsns.distplot(train[train.Survived==0].Fare,color='r', bins=50, ax=ax[0])\nsns.distplot(train[train.Survived==1].Fare,color='g', bins=100, ax=ax[0])\nax[0].axis(xmin=-10,xmax=55)\n\nsns.distplot(train[train.Survived==0].Fare,color='r', bins=75, ax=ax[1])\nsns.distplot(train[train.Survived==1].Fare,color='g', bins=100, ax=ax[1])\nax[1].axis(xmin=45,xmax=150)","eb7a583e":"#binning age\ntrain=train.drop('Age',1).join(pd.cut(train.Age, range(0,81,10), True, range(8), ).astype('int64')).rename(columns={'Age':'Age_bin'})\ntest=test.drop('Age',1).join(pd.cut(test.Age, range(0,81,10), True, range(8), ).astype('int64')).rename(columns={'Age':'Age_bin'})","56de2f5e":"n=9\nsns.countplot(pd.qcut(train.Fare, n, range(n)).astype('int64'), hue=train.Survived)","4542fe0b":"#binning Fare\ntrain.Fare, bins = pd.qcut(train.Fare, n, range(n), True)\ntest.Fare = pd.cut(test.Fare, bins, True, range(n))","70579030":"train.Fare = train.Fare.astype('int64')\ntest.Fare = test.Fare.fillna(method='bfill').astype('int64')","4300d0d9":"# Custom Label Encoder for handling unknown values\nclass LabelEncoderExt(object):\n    def __init__(self):\n        self.label_encoder = LabelEncoder()\n\n    def fit(self, data):\n        self.label_encoder = self.label_encoder.fit(list(data) + ['Unknown'])\n        self.classes_ = self.label_encoder.classes_\n        return self\n\n    def transform(self, data):\n        new_data = list(data)\n        for unique_item in np.unique(data):\n            if unique_item not in self.label_encoder.classes_:\n                new_data = ['Unknown' if x==unique_item else x for x in new_data]\n        return self.label_encoder.transform(new_data)","af9f8ecd":"# from itertools import combinations\n\n# object_cols = train.select_dtypes(\"object\").columns\n# object_cols_test = test.select_dtypes(\"object\").columns\n\n# low_cardinality_cols = [col for col in object_cols if train[col].nunique() < 15]\n\n# interactions = pd.DataFrame(index=train.index)\n# interactions_test = pd.DataFrame(index=test.index)\n\n# # Iterate through each pair of features, combine them into interaction features\n# for features in combinations(low_cardinality_cols,2):\n    \n#     new_interaction = train[features[0]].map(str)+\"_\"+train[features[1]].map(str)\n#     new_interaction_test = test[features[0]].map(str)+\"_\"+test[features[1]].map(str)\n    \n#     encoder = LabelEncoderExt()\n#     encoder.fit(new_interaction)\n#     interactions[\"_\".join(features)] = encoder.transform(new_interaction)\n#     interactions_test[\"_\".join(features)] = encoder.transform(new_interaction_test)","b89fda50":"# train = train.join(interactions) #append to the dataset\n# test = test.join(interactions_test) #append to the dataset","e46ba285":"train.info()","bc9de84b":"test.info()","7bbec9a7":"print(train.isna().sum(), test.isna().sum())","461ccc66":"X_train = train.drop('Survived',1)\ny_train = train.Survived\nX_test = test","a2a6eecc":"seed = 42","8fee1d37":"sampler = SMOTE(random_state = seed)\nX_train, y_train = sampler.fit_resample(X_train, y_train)","92c26583":"y_train.value_counts()","5b0e1cb2":"print(X_train.shape,X_test.shape)","8663b290":"# pcorr = X_train.corrwith(y_train)\n# imp_corr_cols = pcorr[(pcorr>0.1) | (pcorr<-0.1)].index\n\n# X_train = X_train[imp_corr_cols]\n# X_test = X_test[imp_corr_cols]","790a0569":"from numpy import mean\nfrom numpy import std\n\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier, StackingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import GridSearchCV","0daa93d3":"models = {\n    'LR':make_pipeline(StandardScaler(),LogisticRegression(random_state=seed)),\n    'SVC':make_pipeline(StandardScaler(),SVC(random_state=seed)),\n    'AB':AdaBoostClassifier(random_state=seed),\n    'ET':ExtraTreesClassifier(random_state=seed),\n    'GB':GradientBoostingClassifier(random_state=seed),\n    'RF':RandomForestClassifier(random_state=seed),\n    'XGB':XGBClassifier(random_state=seed),\n    'LGBM':LGBMClassifier(random_state=seed)\n    }","495dc9a4":"# evaluate a give model using cross-validation\ndef evaluate_model(model):\n    cv = StratifiedKFold(shuffle=True, random_state=seed)\n    scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n    return scores","19462555":"# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model)\n    results.append(scores)\n    names.append(name)\n    print('*%s %.3f (%.3f)' % (name, mean(scores), std(scores)))","85662504":"plt.boxplot(results, labels=names, showmeans=True)\nplt.show()","8efba79d":"feat_imp=[]\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    if name not in ['LR', 'SVC', 'KNN']: #since they do not have feature importance\n        feat_imp.append(pd.Series(model.feature_importances_, index=X_train.columns))","d20ee5d9":"feat_imp[-1]=feat_imp[-1].apply(lambda x: x\/1000)\navg_feat_imp = pd.DataFrame(feat_imp).mean()\nplt.figure(figsize=(16,6))\nplt.xticks(rotation=90)\nplt.xlabel('Average Feature Importance')\nplt.plot(avg_feat_imp.sort_values(ascending=False))","0f84d880":"# impcols = avg_feat_imp.sort_values(ascending=False).index[:15]\n# X_train = X_train[impcols]\n# X_test = X_test[impcols]","50a30376":"models = {\n    'SVC':make_pipeline(StandardScaler(),SVC(random_state=seed)),\n    'AB':AdaBoostClassifier(random_state=seed),\n    'ET':ExtraTreesClassifier(n_jobs=-1, random_state=seed),\n    'GB':GradientBoostingClassifier(random_state=seed),\n    'RF':RandomForestClassifier(n_jobs=-1, random_state=seed),\n    'XGB':XGBClassifier(n_jobs=-1, random_state=seed),\n    'LGBM':LGBMClassifier(n_jobs=-1, random_state=seed)\n    }","49286a19":"sns.heatmap(X_train.join(y_train).corr().apply(abs), cmap = 'bwr')","eb2c7408":"for name, model in models.items():\n    print(name,'parameters:')\n    print(model.get_params())\n    print('='*140)","b9bf805f":"params = {\n    'SVC':{'svc__gamma':[0.01,0.02,0.05,0.08,0.1], 'svc__C':range(1,8)},\n    \n    'AB':{'learning_rate': [0.05, 0.1, 0.2, 0.5], 'n_estimators': range(50,501,100)},\n    \n    'ET':{'max_depth':[5,8,10,12], 'min_samples_split': [5,8,10,12],\n          'n_estimators': [500,1000,1500,2000]},\n    \n    'GB':{'learning_rate': [0.1, 0.2, 0.5], 'max_depth':[3,5,8,10],\n          'min_samples_split': [5,8,10,12], 'n_estimators': [50,100,200,500],\n          'subsample':[0.5,0.7,0.9]},\n    \n    'RF':{'max_depth':[3,5,10,12,15], 'n_estimators': [50,100,500,1000],\n          'min_samples_split': [4,8,10]},\n    \n    'XGB':{'max_depth':range(3,10,2), 'n_estimators': range(50,201,50),\n           'learning_rate': [0.05, 0.08, 0.1, 0.15], 'subsample':[0.5,0.7,0.9]},\n    \n    'LGBM':{'max_depth':range(3,10,2), 'n_estimators': range(50,201,50),\n            'learning_rate': [0.05, 0.08, 0.1, 0.15, 0.2],'subsample':[0.5,0.7,0.9],\n           'num_leaves': range(15,51,10)}\n}","b27dd1ab":"# evaluate the models and store results\nbest_params = params\nnames= list()\nfor name, param_grid, model in zip(params.keys(), params.values(), models.values()):\n    gscv = GridSearchCV(model, param_grid, n_jobs=-1, verbose=3, cv=4)\n    gscv.fit(X_train,y_train)\n    names.append(name)\n    best_params[name] = gscv.best_params_\n    print(name)\n    print(\"best score:\",gscv.best_score_)\n    print(\"best params:\",gscv.best_params_)","084f16d2":"base_models = [\n    ('SVC',make_pipeline(StandardScaler(),SVC(random_state=seed))),\n    ('AB',AdaBoostClassifier(random_state=seed)),\n    ('ET',ExtraTreesClassifier(random_state=seed)),\n    ('GB',GradientBoostingClassifier(random_state=seed)),\n    ('RF',RandomForestClassifier(random_state=seed)),\n    ('XGB',XGBClassifier(random_state=seed)),\n    ('LGBM',LGBMClassifier(random_state=seed))\n]","4c89318e":"for model, param in zip(base_models,best_params.values()):\n    model[1].set_params(**param)","c2daf05e":"clf = StackingClassifier(estimators=base_models)","c01ff69d":"score = evaluate_model(clf)","70e6be65":"print(score)\nprint(mean(score))","260320c6":"clf.fit(X_train,y_train)","be0773ad":"y_preds = clf.predict(X_test)","25b0f390":"Submission = pd.DataFrame({ 'PassengerId': PID,'Survived': y_preds })\nSubmission.to_csv('Submission.csv', index = False)","7e1bb6c4":"***Survived - Our dependent variable***","1f7d521e":"***Ticket***\n\nLet's observe and see if we can create any feature from the tickets.","2ecf76ff":"## Age","c12ee64d":"As we can see every name is unique and therefore it wouldn't provide any help in our prediction models unless we engineer some stuff out of it such as **title** (Mr., Mrs, etc). Also tried extracting last name but didn't provide any help to our model.","4691df07":"*Though the dtype is int64 but Pclass and Survived are also categorical.*","74829d7a":"While studying about this dataset from various notebooks, I came across creation of Family_size. I feel it would be a good idea to include it.\n\nLet's create Family_size by adding SibSp and Parch.","a736949a":"Passengers who embarked from port `S` had low chances of survival \n(might be because most of them belonged to 3rd Pclass).\n\nPassengers embarked from port `C` had high chances of survival.","2c457f8f":"Let's drop the original columns as we have extracted features from them.","b3708c6e":"We can see that cheap fare passengers had a very low survival rate(obvious).","fdf42de6":"We can observe very **low survival rate** among passengers who were travelling **alone**(might be due to preference to family people in rescue operation).","7c31f9ce":"**We can observe that Title and Sex are quite similar and highly correlate. This wouldn't be useful.\nWe ll drop title.**","d6dc5b8c":"***Cabin***","02081d16":"Let's Zoom in.","89e88941":"## Cleaning, Analysis and Feature Engineering","72521340":"It's obvious and verified through the visualization that the survival rate of `first class` was **high**.\nWhile survival rate of the `third class` passengers was **very low**.","f961950d":"***Name***","4038b65e":"We can see almost every `cabintype` has high survival rate.\n\nThis might be due to them being in `first` and `second`.","356d9eb2":"### Binning Fare in train and test data using pd.qcut()","b88e498f":"This is large number considering the size of our dataset. So we need to be careful while filling the missing values.\n\nWe generated a new feature - Title and Ticket_numtype. It should be useful in imputing these missing values in Age.","48d4d355":"It is a lot of `null` values. Let's try to create `features` from the available values and we ll see afterwards if they are useful.","f203843b":"### ***Let's analyse categorical features***","b6b4b769":"***Sex***","077c3da9":"Our process of feature engineering is complete.\n\nNow let's complete our preprocessing by converting categorical data into numerical data.","a08ee1c2":"***Embarked***","07b370a4":"We can observe that **Children** had **high chances of survival** (they might have been given preference during rescue operation).\n\nWhereas people around the age of `20 - 30` had very **low chance of survival** (The reason might be that many of them belonged to the third class as we can see the average age of 3rd Pclass))\n\nAlso `old people (age 60+)` too had **low survival rate.**","b40d1fab":"***Feature Selection using mean of feature importance of different models***","05af2f58":"## Model imports","25ca7e70":"The Passengers with Ticket starting with 3 had **low survival rate**. While Passengers with ticket starting with `1` had **high survival rate**.","8b4c6fda":"*Let's iterate through rest of the columns.*\n\n***Pclass***","ef821f29":"## Let's start","c658855c":"### ***Now, let's analyse the numerical features***","fe49189c":"***Fare***","e52f01f7":"The features extracted from `ticket` seem to be **useful**.\nLet's append them to our dataframe.\nDon't worry, we will see which features to keep during **feature selection**.","5521678c":"Let's create some artificial features by combining the categorical columns.","e3e725aa":"## Imports","5e27b273":"From `Title` and `Sex`, We can clearly see that `Female` passengers had **high chances of survival** than `Male`.","18f55de8":"We can clearly see the **mode** of the Embarked column is `S`.\n\nSo, let's fill the missing values with `S`."}}