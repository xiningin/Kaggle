{"cell_type":{"06f64847":"code","cd347038":"code","b49aecf4":"code","143152fb":"code","c7803671":"code","4e2c2468":"code","190a7f8e":"code","21026655":"code","7d58a047":"code","b6f22f5a":"code","b0c794c9":"code","d726518b":"code","029b4feb":"code","dc8a9c02":"code","77317ffd":"code","8ec87d25":"code","991e7c7a":"code","d8c72eea":"code","07f3f4a9":"code","4b2c0254":"code","df84dcf1":"code","f28db968":"code","9cb7bffd":"code","fe98d696":"code","8350ebdf":"code","d09f2c27":"code","b9b34d7a":"code","04c99ffd":"code","621a050a":"code","7df1c9b1":"code","f6f45ecd":"code","0a909afd":"code","cd82e5ae":"code","b80a065c":"code","f5b03a88":"code","fc2f8168":"code","efbf2f2a":"code","d5eba939":"code","287bd18c":"code","4917af7c":"code","6fbcb332":"code","1a0fd479":"code","45090405":"code","7444237e":"code","d508fc6b":"code","60953c9b":"code","6799344e":"code","18e4a427":"code","d0bedb42":"code","a506d5eb":"code","cf877f0c":"code","44a1ab11":"code","58cb6af9":"code","89141a9c":"code","ad6e5e90":"code","dc1b827e":"code","aeeaedac":"code","603b01ef":"code","7b67edd0":"code","3d2dc256":"code","6cb57e35":"code","3573d009":"code","1a540423":"code","b1acaa38":"code","ccc51e9b":"code","39366b4f":"code","d1ac9e5a":"code","9de8208c":"code","76dab5aa":"code","cb8144b0":"code","6893ce34":"code","90ac1463":"code","7aaac851":"code","7bab4948":"code","6b7d71e1":"code","76bb66ae":"code","bf568efe":"code","fd10a69c":"code","5d57e799":"code","c668fcb7":"code","0f4057b9":"code","ebe97b86":"code","93b8ec3f":"code","51e1d702":"code","87297007":"code","83d21f4a":"code","2a329633":"code","9a9a0338":"code","c2613a2b":"code","abd703ec":"code","b372aa1f":"code","74b0713c":"code","8c772f48":"code","a24ee23f":"code","8cdbefb9":"code","71ee9199":"code","f4d2ae87":"code","aeaebb7a":"code","2d99345e":"code","73919330":"code","8cc48e1e":"code","693568eb":"code","79cded5f":"code","618efe93":"code","0aa5a796":"code","516262bc":"code","0e090aab":"code","f82eb06e":"code","f6ce531b":"code","f52eb93e":"code","0bb0b5a4":"code","cbc10b90":"code","c681c799":"code","9997216e":"code","2ac55385":"code","74c2579e":"code","112dbe4e":"code","f9dd62b6":"code","91671702":"code","68fa1cd6":"code","ce125170":"code","d1245360":"code","338e1b04":"code","508b929d":"code","aa24e66b":"code","758736ab":"code","1b4efac8":"code","2c2c3e0c":"code","c3faa7fc":"code","7febb077":"code","bcc1f510":"code","679abd3c":"code","6c8278c8":"code","bff2368b":"code","3451e8b7":"code","0e8a20a0":"markdown","20983693":"markdown","b9a93f05":"markdown","78eed362":"markdown","ac8504db":"markdown","69ebb80f":"markdown","61ed2b86":"markdown","c4013ea2":"markdown","273a4d0c":"markdown","266bc2b4":"markdown","1abd22db":"markdown","4fb2311e":"markdown","29e88567":"markdown","a9464b12":"markdown","1f133383":"markdown","3d57269d":"markdown","ec9213f5":"markdown","f9822e2a":"markdown","40e35ca4":"markdown","3f1066f6":"markdown","813f1cc8":"markdown","54b0d759":"markdown","22bf8ddb":"markdown","ce67150d":"markdown","0c9a4c30":"markdown","49236392":"markdown","06b7493d":"markdown","7dc56465":"markdown","3834f4c5":"markdown","0f55debf":"markdown","5111e1df":"markdown","fa8eb5ce":"markdown","2fa204f7":"markdown","19bf8d72":"markdown"},"source":{"06f64847":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/\"))\n\n# Any results you write to the current directory are saved as output.","cd347038":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport missingno as msno\nimport gc","b49aecf4":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats('pdf', 'png')\npd.options.display.float_format = '{:.2f}'.format\nrc={'savefig.dpi': 75, 'figure.autolayout': False, 'figure.figsize': [12, 8], 'axes.labelsize': 18,\\\n   'axes.titlesize': 18, 'font.size': 18, 'lines.linewidth': 2.0, 'lines.markersize': 8, 'legend.fontsize': 16,\\\n   'xtick.labelsize': 16, 'ytick.labelsize': 16}\n\nsns.set(style='dark',rc=rc)","143152fb":"default_color = '#56B4E9'\ncolormap = plt.cm.cool","c7803671":"# Setting working directory\n\npath = '..\/input\/'\npath_result = '..\/output\/'","4e2c2468":"train = pd.read_csv(path + 'train_data.csv')\ntest = pd.read_csv(path + 'teste_data.csv')\ntrain = train.rename(columns={\"default\": \"target\", \"ids\":\"id\"})\ntest = test.rename(columns={\"ids\":\"id\"})","190a7f8e":"train.shape","21026655":"test.head()","7d58a047":"test.shape","b6f22f5a":"test_id = test.id","b0c794c9":"train.head()","d726518b":"train.describe()","029b4feb":"def missing_values_table(df):\n        mis_val = df.isnull().sum()\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        return mis_val_table_ren_columns","dc8a9c02":"missing_values_table(train)","77317ffd":"missingValueColumns = train.columns[train.isnull().any()].tolist()\ndf_null = train[missingValueColumns]","8ec87d25":"msno.bar(df_null,figsize=(20,8),color=default_color,fontsize=18,labels=True)","991e7c7a":"msno.heatmap(df_null,figsize=(20,8),cmap=colormap)","d8c72eea":"msno.dendrogram(df_null,figsize=(20,8))","07f3f4a9":"sorted_data = msno.nullity_sort(df_null, sort='descending') # or sort='ascending'\nmsno.matrix(sorted_data,figsize=(20,8),fontsize=14)","4b2c0254":"train = train.dropna(subset=['target'])","df84dcf1":"missingValueColumns = train.columns[train.isnull().any()].tolist()\ndf_null = train[missingValueColumns]","f28db968":"msno.bar(df_null,figsize=(20,8),color=default_color,fontsize=18,labels=True)","9cb7bffd":"msno.heatmap(df_null,figsize=(20,8),cmap=colormap)","fe98d696":"msno.dendrogram(df_null,figsize=(20,8))","8350ebdf":"sorted_data = msno.nullity_sort(df_null, sort='descending') # or sort='ascending'\nmsno.matrix(sorted_data,figsize=(20,8),fontsize=14)","d09f2c27":"plt.figure(figsize=(15,5))\n\nax = sns.countplot('target',data=train,color=default_color)\nfor p in ax.patches:\n    ax.annotate('{:.2f}%'.format(100*p.get_height()\/len(train['target'])), (p.get_x()+ 0.3, p.get_height()+0.2))","b9b34d7a":"def get_meta(train):\n    data = []\n    for col in train.columns:\n        # Defining the role\n        if col == 'target':\n            role = 'target'\n        elif col == 'id':\n            role = 'id'\n        else:\n            role = 'input'\n\n        # Defining the level\n        if col == 'target' or 'facebook' in col or col == 'gender' or 'bin_' in col:\n            level = 'binary'\n        elif train[col].dtype == np.object or col == 'id':\n            level = 'nominal'\n        elif train[col].dtype == np.float64:\n            level = 'interval'\n        elif train[col].dtype == np.int64:\n            level = 'ordinal'\n\n        # Initialize keep to True for all variables except for id\n        keep = True\n        if col == 'id':\n            keep = False\n\n        # Defining the data type \n        dtype = train[col].dtype\n\n        # Creating a Dict that contains all the metadata for the variable\n        col_dict = {\n            'varname': col,\n            'role'   : role,\n            'level'  : level,\n            'keep'   : keep,\n            'dtype'  : dtype\n        }\n        data.append(col_dict)\n    meta = pd.DataFrame(data, columns=['varname', 'role', 'level', 'keep', 'dtype'])\n    meta.set_index('varname', inplace=True)\n    return meta","04c99ffd":"meta_data = get_meta(train)\nmeta_data","621a050a":"meta_counts = meta_data.groupby(['role', 'level']).agg({'dtype': lambda x: x.count()}).reset_index()\nmeta_counts","7df1c9b1":"fig,ax = plt.subplots()\nfig.set_size_inches(20,5)\nsns.barplot(data=meta_counts[(meta_counts.role != 'target') & (meta_counts.role != 'id') ],x=\"level\",y=\"dtype\",ax=ax,color=default_color)\nax.set(xlabel='Variable Type', ylabel='Count',title=\"Variables Count Across Datatype\")","f6f45ecd":"col_ordinal   = meta_data[(meta_data.level == 'ordinal') & (meta_data.keep)].index\ncol_nominal   = meta_data[(meta_data.level == 'nominal') & (meta_data.keep)& (meta_data.role != 'target')& (meta_data.role != 'id')].index\ncol_interval = meta_data[(meta_data.level == 'interval') & (meta_data.keep)].index\ncol_binary    = meta_data[(meta_data.level == 'binary') & (meta_data.keep) & (meta_data.role != 'target')].index","0a909afd":"list(col_nominal)","cd82e5ae":"def new_missing_columns(df):\n    for i in missingValueColumns:\n        if 'target' not in i:\n            new_col = 'bin_missing_'+ i\n            df[new_col] = np.where(df[i].isnull(), True, False)\n    return df","b80a065c":"train = new_missing_columns(train)\ntest = new_missing_columns(test)","f5b03a88":"def count_label_encoding(train, test,col):\n    for i in col:\n        df1 = train[i].value_counts().reset_index(name='freq_'+ i).rename(columns={'index': 'lc_'+ i})\n        train = pd.merge(train,df1,left_on=i, right_on='lc_'+ i, how='left')\n        test = pd.merge(test,df1,left_on=i, right_on='lc_'+ i, how='left')\n        \n    for i in list(train):\n        if 'lc_' in i:\n            train = train.drop(i, axis = 1)\n            test = test.drop(i, axis = 1)\n    return train, test","fc2f8168":"train, test = count_label_encoding(train, test,col_nominal)\ntrain, test = count_label_encoding(train, test,col_binary)","efbf2f2a":"meta_data = get_meta(train)\ncol_ordinal   = meta_data[(meta_data.level == 'ordinal') & (meta_data.keep)& (meta_data.role != 'target')].index\ncol_nominal   = meta_data[(meta_data.level == 'nominal') & (meta_data.keep)& (meta_data.role != 'target')].index\ncol_interval = meta_data[(meta_data.level == 'interval') & (meta_data.keep)& (meta_data.role != 'target')].index\ncol_binary    = meta_data[(meta_data.level == 'binary') & (meta_data.keep) & (meta_data.role != 'target')].index","d5eba939":"meta_counts = meta_data.groupby(['role', 'level']).agg({'dtype': lambda x: x.count()}).reset_index()\nmeta_counts","287bd18c":"fig,ax = plt.subplots()\nfig.set_size_inches(20,5)\nsns.barplot(data=meta_counts[(meta_counts.role != 'target') & (meta_counts.role != 'id') ],x=\"level\",y=\"dtype\",ax=ax,color=default_color)\nax.set(xlabel='Variable Type', ylabel='Count',title=\"Variables Count Across Datatype\")","4917af7c":"plt.figure(figsize=(18,16))\nplt.title('Pearson correlation of continuous features', y=1.05, size=15)\nsns.heatmap(train[col_interval].corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True, fmt = '.2f')","6fbcb332":"from sklearn.model_selection import train_test_split","1a0fd479":"train.head()","45090405":"X = pd.concat([train[col_interval],train[col_ordinal],pd.get_dummies(train[col_binary])], axis=1)\ny = pd.DataFrame(train.target)\nX.fillna(-1, inplace=True) \ny.fillna(-1, inplace=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","7444237e":"y.shape","d508fc6b":"X.shape","60953c9b":"X.head()","6799344e":"plt.figure(figsize=(18,16))\nplt.title('Pearson correlation of continuous features', y=1.05, size=15)\nsns.heatmap(X.corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=False, fmt = '.1f')","18e4a427":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=150, max_depth=8, min_samples_leaf=30, max_features=0.2, n_jobs=-1, random_state=0)\nrf.fit(X_train, y_train['target'])\nfeatures = X_train.columns.values\nprint(\"----- Training Done -----\")","d0bedb42":"from sklearn.metrics import accuracy_score, roc_auc_score","a506d5eb":"acc = accuracy_score(y_train, rf.predict(X_train))\nauc = roc_auc_score(y_train, rf.predict(X_train))\nprint(\"Accuracy: %.4f\" % acc)\nprint(\"AUC: %.4f\" % auc)","cf877f0c":"acc = accuracy_score(y_test, rf.predict(X_test))\nauc = roc_auc_score(y_test, rf.predict(X_test))\nprint(\"Accuracy: %.4f\" % acc)\nprint(\"AUC: %.4f\" % auc)","44a1ab11":"def get_feature_importance_df(feature_importances, \n                              column_names, \n                              top_n=25):\n    \"\"\"Get feature importance data frame.\n \n    Parameters\n    ----------\n    feature_importances : numpy ndarray\n        Feature importances computed by an ensemble \n            model like random forest or boosting\n    column_names : array-like\n        Names of the columns in the same order as feature \n            importances\n    top_n : integer\n        Number of top features\n \n    Returns\n    -------\n    df : a Pandas data frame\n \n    \"\"\"\n     \n    imp_dict = dict(zip(column_names, \n                        feature_importances))\n    top_features = sorted(imp_dict, \n                          key=imp_dict.get, \n                          reverse=True)[0:top_n]\n    top_importances = [imp_dict[feature] for feature \n                          in top_features]\n    df = pd.DataFrame(data={'feature': top_features, \n                            'importance': top_importances})\n    return df","58cb6af9":"feature_importance = get_feature_importance_df(rf.feature_importances_, features)\nfeature_importance","89141a9c":"fig,ax = plt.subplots()\nfig.set_size_inches(20,10)\nsns.barplot(data=feature_importance[:10],x=\"feature\",y=\"importance\",ax=ax,color=default_color,)\nax.set(xlabel='Variable name', ylabel='Importance',title=\"Variable importances\")","ad6e5e90":"from xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier","dc1b827e":"def cross_val_model(X,y, model, n_splits=3):\n    from sklearn.model_selection import StratifiedKFold\n    from sklearn.model_selection import cross_val_score\n    X = np.array(X)\n    y = np.array(y)\n    \n\n    folds = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=2017).split(X, y))\n\n    for j, (train_idx, test_idx) in enumerate(folds):\n        X_train = X[train_idx]\n        y_train = y[train_idx]\n        X_holdout = X[test_idx]\n        y_holdout = y[test_idx]\n\n        print (\"Fit %s fold %d\" % (str(model).split('(')[0], j+1))\n        m = model.fit(X_train, y_train)\n        cross_score = cross_val_score(model, X_holdout, y_holdout, cv=3, scoring='roc_auc')\n        print(\"    cross_score: %.5f\" % cross_score.mean())\n    return m","aeeaedac":"#RandomForest params\nrf_params = {}\nrf_params['n_estimators'] = 200\nrf_params['max_depth'] = 6\nrf_params['min_samples_split'] = 70\nrf_params['min_samples_leaf'] = 30","603b01ef":"rf_model = RandomForestClassifier(**rf_params, random_state=29,n_jobs = -1)","7b67edd0":"cross_val_model(X_train, y_train['target'], rf_model)","3d2dc256":"# XGBoost params\nxgb_params = {}\nxgb_params['learning_rate'] = 0.02\nxgb_params['n_estimators'] = 1000\nxgb_params['max_depth'] = 4\nxgb_params['subsample'] = 0.9\nxgb_params['colsample_bytree'] = 0.9","6cb57e35":"XGB_model = XGBClassifier(**rf_params, random_state=29,n_jobs=-1)","3573d009":"cross_val_model(X_train, y_train['target'], XGB_model)","1a540423":"train_ext = train.copy()\ntest_ext = test.copy()\n#train_ext.fillna(-1,inplace = True)\nmissing_values_table(train_ext)","b1acaa38":"train.head()","ccc51e9b":"def create_extra_features(train_ext):\n    train_ext['null_sum'] = train_ext[train_ext==-1].count(axis=1)\n    #train_ext['bin_sum']  = train_ext[col_binary].sum(axis=1)\n    train_ext['ord_sum']  = train_ext[col_ordinal].sum(axis=1)\n    train_ext['interval_median']  = train_ext[col_interval].sum(axis=1)\n    train_ext['new_amount_borrowed_by_income']  = train_ext['amount_borrowed']\/train_ext['income']\n    train_ext['new_amount_borrowed_by_months']  = train_ext['amount_borrowed']\/train_ext['borrowed_in_months']\n    return train_ext","39366b4f":"train_ext = create_extra_features(train_ext)\ntest_ext = create_extra_features(test_ext)\n\ntrain_backup = train_ext.copy()\ntest_backup = test_ext.copy()","d1ac9e5a":"meta_data = get_meta(train_ext)\ncol_ordinal   = meta_data[(meta_data.level == 'ordinal') & (meta_data.keep)& (meta_data.role != 'target')].index\ncol_nominal   = meta_data[(meta_data.level == 'nominal') & (meta_data.keep)& (meta_data.role != 'target')].index\ncol_interval = meta_data[(meta_data.level == 'interval') & (meta_data.keep)& (meta_data.role != 'target')].index\ncol_binary    = meta_data[(meta_data.level == 'binary') & (meta_data.keep) & (meta_data.role != 'target')].index\n#meta_data","9de8208c":"meta_counts = meta_data.groupby(['role', 'level']).agg({'dtype': lambda x: x.count()}).reset_index()\nmeta_counts","76dab5aa":"ids_targets = meta_data[meta_data['role'] != 'input'].index","cb8144b0":"train_ext.head()","6893ce34":"test_ext.head()","90ac1463":"train_ext.fillna(-1, inplace = True)\n\nX_ext = pd.concat([train_ext[col_interval],train_ext[col_ordinal], pd.get_dummies(train_ext[col_binary])], axis=1)\nX_ext.head()","7aaac851":"X_ext = X_ext.drop(columns = ['facebook_profile_False','gender_f'], axis=1)","7bab4948":"test_ext = pd.concat([test_ext[col_interval],test_ext[col_ordinal], pd.get_dummies(test_ext[col_binary])], axis=1)\ntest_ext.fillna(-1, inplace = True)\n#X_ext = X_ext.drop(columns = ids_targets, axis =1)\ny_ext = pd.DataFrame(train_ext.target) #train_lc.target_default.ravel(order='K') #pd.DataFrame(train_ext.target)\ny_ext=y_ext.astype('bool')\ny_ext = y_ext.values\ny_ext = y_ext.reshape(-1)","6b7d71e1":"test_ext['gender_-1'] = 0\ntest_ext['facebook_profile_-1'] = 0\ntest_ext=test_ext.drop(columns = ['facebook_profile_False', 'gender_f'], axis = 1)\ntest_ext.head()","76bb66ae":"cols = list(X_ext)\ntest_ext = test_ext[cols]","bf568efe":"X_ext.head()","fd10a69c":"test_ext.head()","5d57e799":"from sklearn.utils.multiclass import type_of_target\ntype_of_target(y_ext)","c668fcb7":"X_ext.shape","0f4057b9":"test_ext.shape","ebe97b86":"X_train, X_test, y_train, y_test = train_test_split(X_ext, y_ext, test_size=0.2, random_state=42)","93b8ec3f":"X_ext.head()","51e1d702":"test_ext.head()","87297007":"cross_val_model(X_ext, y_ext, rf_model)","83d21f4a":"cross_val_model(X_ext, y_ext, XGB_model)","2a329633":"gc.collect","9a9a0338":"from sklearn.model_selection import GridSearchCV","c2613a2b":"tuned_parameters = [{'max_depth': [4,5,6,7,8,9,10],\n                     'max_features': [4,5,6,7,8,9,10],\n                    'n_estimators':[10,25,50,75]}]\n\nclf = GridSearchCV(RandomForestClassifier(random_state=29), tuned_parameters, cv=3, scoring='roc_auc')\nclf.fit(X_train, y_train)","abd703ec":"print(\"Best parameters set found on development set:\")\nprint()\nprint(clf.best_params_)\nprint()\nprint(\"Grid scores on development set:\")\nprint()\nmeans = clf.cv_results_['mean_test_score']\nstds = clf.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, clf.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n        % (mean, std * 2, params))\nprint()\n\nprint(\"Detailed classification report:\")\nprint()\nprint(\"The model is trained on the full development set.\")\nprint(\"The scores are computed on the full evaluation set.\")\nprint()\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nacc = accuracy_score(y_test, clf.predict(X_test))\nauc = roc_auc_score(y_test, clf.predict(X_test))\nprint(\"Accuracy: %.4f\" % acc)\nprint()\nprint(\"AUC: %.4f\" % auc)\nprint()","b372aa1f":"from hyperopt.pyll.base import scope\nfrom hyperopt.pyll.stochastic import sample\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe","74b0713c":"import random\nimport itertools\nN_HYPEROPT_PROBES = 10\nEARLY_STOPPING = 80\nHOLDOUT_SEED = 123456\nHOLDOUT_SIZE = 0.10\nHYPEROPT_ALGO = tpe.suggest  #  tpe.suggest OR hyperopt.rand.suggest\nDATASET = 'clean' # 'raw' | 'clean' | 'extended'\nSEED0 = random.randint(1,1000000000)\nNB_CV_FOLDS = 5","8c772f48":"obj_call_count = 0\ncur_best_score = 0","a24ee23f":"space_RF ={\n    'n_estimators'           : hp.choice('n_estimators',         np.arange(10, 200,  dtype=int)),       \n    'max_depth'              : hp.choice(\"max_depth\",            np.arange(3, 15,    dtype=int)),\n    'min_samples_split'      : hp.choice(\"min_samples_split\",    np.arange(20, 100,    dtype=int)),\n    'min_samples_leaf'       : hp.choice(\"min_samples_leaf\",    np.arange(10, 100,    dtype=int)),\n    'criterion'              : hp.choice('criterion', [\"gini\", \"entropy\"]),\n    'class_weight'           : hp.choice('class_weight', ['balanced_subsample', None]),\n    'n_jobs'                 : -1,\n    'oob_score'              : True,\n    'random_state'           :  hp.randint('random_state',2000000)\n   }\n#{'class_weight': 1, 'criterion': 1, 'max_depth': 9, 'min_samples_leaf': 74, 'min_samples_split': 12, 'n_estimators': 134, 'random_state': 1433254}\n#Params: class_weight=balanced_subsample criterion=entropy max_depth=11 min_samples_leaf=2 min_samples_split=29 n_estimators=89 n_jobs=-1 oob_score=True\n#Params: class_weight=balanced_subsample criterion=entropy max_depth=10 min_samples_leaf=2 min_samples_split=17 n_estimators=38 n_jobs=-1 oob_score=True","8cdbefb9":"def objective_RF(space):\n    import uuid\n    from sklearn.model_selection import cross_val_score\n    from sklearn.ensemble import RandomForestClassifier\n    \n    global obj_call_count, cur_best_score, X_train, y_train, test, X_test, y_test, test_id\n\n    \n    obj_call_count += 1\n    print('\\nLightGBM objective call #{} cur_best_score={:7.5f}'.format(obj_call_count,cur_best_score) )\n\n    sorted_params = sorted(space.items(), key=lambda z: z[0])\n    print('Params:', str.join(' ', ['{}={}'.format(k, v) for k, v in sorted_params if not k.startswith('column:')]))\n\n\n    params = sample(space)\n        \n    mdl = RandomForestClassifier(**params)\n    \n    cv_score = cross_val_score(mdl, X_train, y_train).mean()\n\n    print( 'CV finished ; \\n cv_score={:7.5f}'.format(cv_score ) )\n    \n    _model = mdl.fit(X_train, y_train)\n    \n    predictions = _model.predict_proba(X_test)[:,1]#(X_test)\n    \n    score = roc_auc_score(y_test, predictions)\n    print('valid score={}'.format(score))\n    \n    \n    do_submit = score > 0.64\n\n    if score > cur_best_score:\n        cur_best_score = score\n        print('NEW BEST SCORE={}'.format(cur_best_score))\n        do_submit = True\n\n    if do_submit:\n        submit_guid = uuid.uuid4()\n\n        print('Compute submissions guid={}'.format(submit_guid))\n\n        y_submission = _model.predict_proba(test_ext)[:,1] #, num_iteration=n_rounds)\n        submission_filename = 'rf_score_{:13.11f}_submission_guid_{}.csv'.format(score,submit_guid)\n        pd.DataFrame(\n        {'ids':test_id, 'prob':y_submission}\n        ).to_csv(submission_filename, index=False)\n       \n    loss = 1 - score\n    return {'loss': loss, 'status': STATUS_OK}","71ee9199":"trials = Trials()\nbest = fmin(fn=objective_RF,\n                     space=space_RF,\n                     algo=HYPEROPT_ALGO,\n                     max_evals=N_HYPEROPT_PROBES,\n                     trials=trials,\n                     verbose=1)\n\nprint('-'*50)\nprint('The best params for RF:')\nprint( best )\nprint('\\n\\n')","f4d2ae87":"space_XGB ={\n    'max_depth'        : hp.choice(\"max_depth\", np.arange(5, 15,dtype=int)), \n    'learning_rate'    : hp.loguniform('learning_rate', -4.9, -3.0),\n    'n_estimators'     : hp.choice('n_estimators', np.arange(10, 100,dtype=int)),\n    'objective'        : 'binary:logistic',\n    'booster'          : 'gbtree',       \n    'reg_alpha'        :  hp.uniform('reg_alpha', 1e-5, 1e-1),\n    'reg_lambda'       :  hp.uniform('reg_lambda', 1e-5, 1e-1), \n    'colsample_bytree' : hp.uniform('colsample_bytree', 0.5, 0.8),\n    'min_child_weight ': hp.uniform('min_child_weight', 0.5, 0.8),   \n    'random_state'     :  hp.randint('random_state',2000000)\n   }","aeaebb7a":"def objective_XGB(space):\n    import uuid\n    from sklearn.model_selection import cross_val_score\n    from xgboost import XGBClassifier\n   \n    global obj_call_count, cur_best_score, X_train, y_train, test, X_test, y_test, test_id\n\n    \n    obj_call_count += 1\n    print('\\nLightGBM objective call #{} cur_best_score={:7.5f}'.format(obj_call_count,cur_best_score) )\n\n    sorted_params = sorted(space.items(), key=lambda z: z[0])\n    print('Params:', str.join(' ', ['{}={}'.format(k, v) for k, v in sorted_params if not k.startswith('column:')]))\n\n\n    params = sample(space)\n        \n    mdl = XGBClassifier(**params)\n    \n    cv_score = cross_val_score(mdl, X_train, y_train).mean()\n\n    \n    print( 'CV finished ; \\n cv_score={:7.5f}'.format(cv_score ) )\n    \n    _model = mdl.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], eval_metric='auc',verbose=True,early_stopping_rounds =30)\n    \n    params.update({'n_estimators': _model.best_iteration})\n    \n    predictions = _model.predict_proba(X_test)[:,1]#(X_test)\n    \n    score = roc_auc_score(y_test, predictions)\n    print('valid score={}'.format(score))\n    \n    \n    do_submit = score > 0.64\n\n    if score > cur_best_score:\n        cur_best_score = score\n        print('NEW BEST SCORE={}'.format(cur_best_score))\n        do_submit = True\n\n    if do_submit:\n        submit_guid = uuid.uuid4()\n\n        print('Compute submissions guid={}'.format(submit_guid))\n\n        y_submission = _model.predict_proba(test_ext)[:,1]#, num_iteration=n_rounds)\n        submission_filename = 'xgb_score_{:13.11f}_submission_guid_{}.csv'.format(score,submit_guid)\n        pd.DataFrame(\n        {'ids':test_id, 'prob':y_submission}\n        ).to_csv(submission_filename, index=False)\n       \n    loss = 1 - score\n    return {'loss': loss, 'status': STATUS_OK}","2d99345e":"trials = Trials()\nbest = fmin(fn=objective_XGB,\n                     space=space_XGB,\n                     algo=HYPEROPT_ALGO,\n                     max_evals=N_HYPEROPT_PROBES,\n                     trials=trials,\n                     verbose=1)\n\nprint('-'*50)\nprint('The best params for XGB:')\nprint( best )\nprint('\\n\\n')","73919330":"train_stack = train_backup.copy()\ntest_stack  = test_backup.copy()","8cc48e1e":"train_stack.shape","693568eb":"test_stack.shape","79cded5f":"meta_data = get_meta(train_stack)\ncol_ordinal   = meta_data[(meta_data.level == 'ordinal') & (meta_data.keep)& (meta_data.role != 'target')].index\ncol_nominal   = meta_data[(meta_data.level == 'nominal') & (meta_data.keep)& (meta_data.role != 'target')].index\ncol_interval = meta_data[(meta_data.level == 'interval') & (meta_data.keep)& (meta_data.role != 'target')].index\ncol_binary    = meta_data[(meta_data.level == 'binary') & (meta_data.keep) & (meta_data.role != 'target')].index\n#meta_data","618efe93":"one_hot = {c: list(train_stack[c].unique()) for c in col_nominal}","0aa5a796":"train_stack = train_stack.replace(-1, np.NaN)\n\nd_median    = train_stack.median(axis=0)\nd_mean      = train_stack.mean(axis=0)\n\ntrain_stack = train_stack.fillna(-1)","516262bc":"from sklearn import preprocessing","0e090aab":"def transform(df, ohe, d_median, d_mean):\n    \n   \n    dcol = [c for c in d_median.index if c in d_mean.index and c !='target']\n    \n    #df['ps_car_13_x_ps_reg_03'] = df['ps_car_13'] * df['ps_reg_03']\n    #df['negative_one_vals'] = np.sum((df[dcol]==-1).values, axis=1)\n    \n    for c in dcol:\n        if 'bin_' not in c:\n            df[c+'_median_range'] = (df[c].values > d_median[c])#.astype(np.int)\n            df[c+'_mean_range']   = (df[c].values > d_mean[c])#.astype(np.int)\n\n    for c in one_hot:\n        if len(one_hot[c])>2 and len(one_hot[c]) < 7:\n            for val in one_hot[c]:\n                df[c+'_oh_' + str(val)] = (df[c].values == val)#.astype(np.int)\n    return df","f82eb06e":"train_stack = transform(train_stack, one_hot,d_median, d_mean)","f6ce531b":"test_stack = transform(test_stack, one_hot, d_median, d_mean)","f52eb93e":"\ntrain_stack = create_extra_features(train_stack)\ntest_stack = create_extra_features(test_stack)\ntrain_stack['bin_sum']  = train_stack[col_binary].sum(axis=1)\ntest_stack['bin_sum']  = test_stack[col_binary].sum(axis=1)","0bb0b5a4":"col = [c for c in train_stack.columns if c not in ['id','target']]\ncol = [c for c in col if not c.startswith('ps_calc_')] ## Droping ps_cal_ vars","cbc10b90":"dups = train_stack[train_stack.duplicated(subset=col, keep=False)]","c681c799":"train_stack = train_stack[~(train_stack.index.isin(dups.index))]","9997216e":"target_stack = train_stack['target']","2ac55385":"train_stack = train_stack[col]","74c2579e":"target_stack.shape","112dbe4e":"test_stack = test_stack[col]","f9dd62b6":"train_stack.shape","91671702":"test_stack.shape","68fa1cd6":"meta_data = get_meta(train_stack)\ncol_ordinal   = meta_data[(meta_data.level == 'ordinal') & (meta_data.keep)& (meta_data.role != 'target')].index\ncol_nominal   = meta_data[(meta_data.level == 'nominal') & (meta_data.keep)& (meta_data.role != 'target')].index\ncol_interval = meta_data[(meta_data.level == 'interval') & (meta_data.keep)& (meta_data.role != 'target')].index\ncol_binary    = meta_data[(meta_data.level == 'binary') & (meta_data.keep) & (meta_data.role != 'target')].index\n#meta_data","ce125170":"X_stack = pd.concat([train_stack[col_interval],train_stack[col_ordinal], pd.get_dummies(train_stack[col_binary])], axis=1)\ntest_stack_val = pd.concat([test_stack[col_interval],test_stack[col_ordinal], pd.get_dummies(test_stack[col_binary])], axis=1)\ny_stack = target_stack","d1245360":"X_stack.shape","338e1b04":"test_stack_val.shape","508b929d":" X_stack =  X_stack.drop(columns=['gender_-1','facebook_profile_-1'], axis = 1)\n","aa24e66b":"cross_val_model(X_stack, y_stack, rf_model)","758736ab":"cross_val_model(X_stack, y_stack, XGB_model)","1b4efac8":"class Ensemble(object):\n\n    def __init__(self, n_splits, stacker, base_models):\n        self.n_splits = n_splits\n        self.stacker = stacker\n        self.base_models = base_models\n\n    def fit_predict(self, X, y, T):\n        from sklearn.model_selection import StratifiedKFold\n        from sklearn.model_selection import cross_val_score\n        X = np.array(X)\n        y = np.array(y)\n        T = np.array(T)\n\n        folds = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=2017).split(X, y))\n\n        S_train = np.zeros((X.shape[0], len(self.base_models)))\n        S_test = np.zeros((T.shape[0], len(self.base_models)))\n        for i, clf in enumerate(self.base_models):\n\n            S_test_i = np.zeros((T.shape[0], self.n_splits))\n\n            for j, (train_idx, test_idx) in enumerate(folds):\n                X_train = X[train_idx]\n                y_train = y[train_idx]\n                X_holdout = X[test_idx]\n\n                print (\"Fit %s fold %d\" % (str(clf).split('(')[0], j+1))\n                clf.fit(X_train, y_train)\n                y_pred = clf.predict_proba(X_holdout)[:,1]                \n\n                S_train[test_idx, i] = y_pred\n                S_test_i[:, j] = clf.predict_proba(T)[:,1]\n            S_test[:, i] = S_test_i.mean(axis=1)\n\n        results = cross_val_score(self.stacker, S_train, y, cv=3, scoring='roc_auc')\n        print(\"Stacker score: %.5f\" % (results.mean()))\n\n        self.stacker.fit(S_train, y)\n        res = self.stacker.predict_proba(S_test)[:,1]\n        return res","2c2c3e0c":"#RandomForest params\nrf_params = {}\nrf_params['n_estimators'] = 80\nrf_params['max_depth'] = 12\nrf_params['min_samples_split'] = 50\nrf_params['min_samples_leaf'] = 23\n#rf_params['class_weight'] = \"balanced_subsample\"# \"balanced\" # \"balanced_subsample\"\n#rf_params['criterion'] = 1\n#{'class_weight': 1, 'criterion': 1, 'max_depth': 10, 'min_samples_leaf': 23, 'min_samples_split': 88, 'n_estimators': 66, 'random_state': 584867}\n#{'class_weight': 0, 'criterion': 1, 'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 15, 'n_estimators': 31}\n#{'class_weight': 1, 'criterion': 1, 'max_depth': 7, 'min_samples_leaf': 1, 'min_samples_split': 25, 'n_estimators': 52}","c3faa7fc":"# XGBoost params\nxgb_params = {}\nxgb_params['learning_rate'] =0.03660642032718193\nxgb_params['n_estimators'] = 70\nxgb_params['max_depth'] = 7\nxgb_params['reg_alpha'] = 0.1\nxgb_params['reg_lambda'] = 0.1\nxgb_params['colsample_bytree'] = 0.6162725690461764 \nxgb_params['min_child_weight'] =  0.751826989118936\n#{'colsample_bytree': 0.6162725690461764, 'learning_rate': 0.07660642032718193, 'max_depth': 1, 'min_child_weight': 0.751826989118936, 'n_estimators': 51, 'random_state': 2943, 'reg_alpha': 8.447744027604217e-05, 'reg_lambda': 2.506380824011793e-05}\n#{'colsample_bytree': 0.6669680642534331, 'learning_rate': 0.0027697150000431693, 'max_depth': 2, 'min_child_weight': 0.7842089630474731, 'n_estimators': 58, 'random_state': 194789, 'reg_alpha': 6.334122926125054e-05, 'reg_lambda': 7.725227814541321e-05}\n#{'colsample_bytree': 0.7185209051997172, 'learning_rate': 0.09634564047154007, 'max_depth': 1, 'min_child_weight': 0.7765683660381831, 'n_estimators': 60, 'random_state': 1791482, 'reg_alpha': 1.5998181299665275e-05, 'reg_lambda': 9.446368653609355e-05}\n#{'colsample_bytree': 0.785981949747911, 'learning_rate': 0.07697973917507268, 'max_depth': 0, 'min_child_weight': 0.7528834859046539, 'n_estimators': 48, 'random_state': 1038594, 'reg_alpha': 9.730513129698628e-05, 'reg_lambda': 9.804649087783435e-05}","7febb077":"rf_model = RandomForestClassifier(**rf_params, random_state=584867)","bcc1f510":"xgb_model = XGBClassifier(**xgb_params, random_state=2943)","679abd3c":"log_model = LogisticRegression(random_state=29)","6c8278c8":"stack = Ensemble(n_splits=3,\n        stacker = log_model,\n        base_models = (rf_model, xgb_model))","bff2368b":"X_stack.fillna(-1, inplace = True)\ntest_stack_val.fillna(-1,inplace=True)\ny_pred = stack.fit_predict(X_stack, target_stack, test_stack_val)","3451e8b7":"sub = pd.DataFrame()\nsub['ids'] = test_id\nsub['prob'] = y_pred\nsub.to_csv('stacked_main.csv', index=False)","0e8a20a0":"## Loading files","20983693":"## Target Analysis","b9a93f05":"# Input variables analysis","78eed362":"## Categorical features analysis","ac8504db":"get_meta(train_ext)","69ebb80f":"# Baseline Models","61ed2b86":"train.fillna(-1, inplace=True) \ntest.fillna(-1, inplace=True)\n#I should improve this...","c4013ea2":"# Ensemble CV","273a4d0c":"# Testing feature importance with random forests","266bc2b4":"## Continuous features analysis","1abd22db":"Question: if test_ext and X_ext have different columns, why didn't the code break? ","4fb2311e":"lb = preprocessing.LabelBinarizer()\ny_ext = lb.fit_transform(y_ext)","29e88567":"## Missing Values","a9464b12":"## XGBoost Model","1f133383":"from sklearn import preprocessing","3d57269d":"## XGBoost","ec9213f5":"## Extra features effect","f9822e2a":"list(test_ext)","40e35ca4":"## Tuning Parameters","3f1066f6":"%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfor i in list(col_nominal):\n\n    carrier_count = train[i].value_counts()\n    sns.set(style=\"darkgrid\")\n    sns.barplot(carrier_count.index, carrier_count.values, alpha=0.9)\n    plt.title('Frequency Distribution of '+i)\n    plt.ylabel('Number of Occurrences', fontsize=12)\n    plt.xlabel(i, fontsize=12)\n    plt.show()","813f1cc8":"train.dtypes","54b0d759":"Ops! Target has missing values!\n\n**For next versions**: iterate predictions: predict target missing values, model again...","22bf8ddb":"## Initialize","ce67150d":"# Make submission","0c9a4c30":"## Data Analysis","49236392":"Missing correlation heat map before droping missing values in target.","06b7493d":"## Parameter optimization","7dc56465":"list(train_ext)","3834f4c5":"# Stacking Models","0f55debf":"meta_data = get_meta(train)","5111e1df":"## Random Forest Model","fa8eb5ce":"### Random Forest","2fa204f7":"## Testing combined transformations without stacking","19bf8d72":"## Count label encoding"}}