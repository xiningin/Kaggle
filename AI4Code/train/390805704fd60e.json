{"cell_type":{"59a08a03":"code","87bbf00c":"code","e731fd0d":"code","54279f42":"code","fa01d1eb":"code","e75fc5eb":"code","19ce0d31":"code","282af446":"code","787cb1be":"code","825f6826":"code","ec524b19":"code","0fffa22d":"code","270a9576":"code","4a5a2c9c":"code","86d743dd":"code","df6fb174":"code","b42c0a34":"code","f2ba50f6":"code","dc82aa39":"code","55e62ee8":"code","db5c6c8d":"code","822d3a55":"code","9242fa91":"code","08991414":"code","e75ea4c7":"code","18bb1a48":"code","f28061b3":"code","38db83f7":"code","902c1ef3":"code","410bd9aa":"code","c2213ce8":"code","139bb023":"code","aba29aa9":"code","40374a39":"code","1aecc0d4":"code","4ddd8674":"code","4a253d97":"code","38b9b262":"code","11e28806":"code","ec7a5e13":"code","16ee3821":"code","fa329f53":"code","a5817d2c":"code","bd00b24c":"code","321c10a8":"code","94f0d015":"code","5154072a":"code","48b3db7f":"code","de654fa6":"code","2273d7d2":"code","f27781cb":"code","abc2b290":"code","96ed698c":"code","be4561b7":"code","0057c7af":"markdown","c281608a":"markdown","75fa4c10":"markdown","ffe05e21":"markdown","184b7d52":"markdown","628b10fb":"markdown","e5360088":"markdown","0db91bbd":"markdown","7e5972ec":"markdown","b135816a":"markdown","a54ae177":"markdown","f4505b4c":"markdown","89d1dd28":"markdown","1935b024":"markdown","62a0150e":"markdown","f86d883b":"markdown","cc9fdb2a":"markdown","9734221e":"markdown","f72f446c":"markdown","bf757369":"markdown","21ed714c":"markdown","5fec433b":"markdown","74696b17":"markdown","c08d3cf2":"markdown","bc0bbade":"markdown","037c80fd":"markdown","8b659a25":"markdown","ee059b7d":"markdown","2e9e6fca":"markdown","ce43bd3d":"markdown","247c1d65":"markdown","186939a4":"markdown","9610baea":"markdown","8b94addd":"markdown","a32dc045":"markdown","93f891da":"markdown","7fe498c9":"markdown","1c1495c0":"markdown","ec34b0ad":"markdown","d97483da":"markdown","9b4e042a":"markdown","5cf17985":"markdown","bcd3468e":"markdown","072a0e2d":"markdown","ce1fb413":"markdown","73adb514":"markdown","bd3f4401":"markdown","9931084f":"markdown","19b18e92":"markdown","5448783d":"markdown","8929f808":"markdown","9e6e5016":"markdown","3d87f95f":"markdown","caac3acf":"markdown","4bd29fd7":"markdown","9add407a":"markdown","03a7ca0f":"markdown","ee855081":"markdown","eee28b00":"markdown","97b21c5e":"markdown","05b29b82":"markdown","55303b8e":"markdown","ef8e4f48":"markdown","2d11afd9":"markdown","3e63f78b":"markdown","548aee37":"markdown","8c41e448":"markdown","74e9304f":"markdown","9318b23e":"markdown","16fdcaae":"markdown"},"source":{"59a08a03":"#Libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy as sp\nfrom scipy import stats\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n#Load the data \ndata = pd.read_csv(\"\/kaggle\/input\/bank-marketing\/bank-additional-full.csv\",sep=\";\")\n\n","87bbf00c":"#Plot functions for categorical variables\n\ndef categorical_var_plot(label,dataset,target_variable,text_size):\n    \"\"\" This function is defined in order to make three different plots:\n    - Pie: Shows the percentage of each category inside the variable.\n    - Bar plot: Shows the percentage of clients that has a term deposit by category, also has a red line which shows the average rate of people which hire a term deposit.\n    - Count Plot: Shows the percentage of each category inside the variable, differenciated by if the person has hire a term a deposit.\n    \n    Args:\n    label: Category selected\n    dataset: Dataset\n    target_variable: target variable of the project\n    text_size: set text size of the plot\"\"\"\n    \n    #Define colors\n    colors_list= sns.color_palette(\"hls\",12)\n    #Set Text Size\n    sns.set(font_scale=text_size)\n    #Plot size\n    fig = plt.figure(figsize=(50,20))\n    \n    plt.suptitle(t=label.title())\n    \n    \n    # Subplot 1: Pie Plot\n    a0= plt.subplot2grid((2,2),(0,0))\n    plt.pie(dataset[label].value_counts(),\n                       radius= 1,\n                       autopct='%1.1f%%',# to label the wedges with their percentage \n                       startangle=30,    \n                       shadow=True,\n                       pctdistance=1.16,    # the ratio between the center of each pie slice and the start of the text generated by autopct \n                       colors=colors_list, # add custom colors\n                       )\n    plt.title(\"Distribution\")\n    plt.legend(labels=pd.DataFrame(data[label].value_counts()).index, loc=\"upper right\", bbox_to_anchor=(1, 0, 0.5, 1))\n    \n    #Subplot 2 : Bar Plot\n    #group by target and label\n    var_gby= data.groupby([label,target_variable]).size()\n    var_size= len(data[label].unique())*2\n    table=[]\n    #iterate in order to find the percentage of clients that a has a term deposit\n    for i in range(0,var_size,2):\n        a =(var_gby[i+1]\/(var_gby[i]+var_gby[i+1]))*100\n        table.append(a)\n    # Sort the names\n    index_vab = np.sort(data[label].unique())\n\n    table1= pd.DataFrame({\"Names\":index_vab,\"Data\": table})\n    a1= plt.subplot2grid((2,2),(0,1))\n    a1 = sns.barplot( x=\"Names\", y=\"Data\",data=table1,palette=colors_list)\n\n    plt.axhline(11.3, color='r', linestyle='--')\n    plt.title(\"Percentaje of clients that have a term deposit by category\")\n    plt.xlabel(\"Categories\")\n    plt.ylabel(\"Percentage(%)\")\n    #Add the percentage of each class\n    for p in a1.patches:\n        a1.annotate('{:.1f}%'.format(p.get_height()), (p.get_x()+0.1, p.get_height()+0.5))\n \n    # Subplot 3: Count Plot\n    a2 = plt.subplot2grid((2,2),(1,1))\n    a2 = sns.countplot(x=label,data=dataset,palette = colors_list ,hue=target_variable, order =np.sort(data[label].value_counts().index))\n    \n    for p in a2.patches:\n        a2.annotate('{:.1f}%'.format(100*p.get_height()\/len(dataset[label])*1.), (p.get_x()+0.1, p.get_height()+10))\n        \n    plt.title (\"Distribution dif by if the client has suscribed the term deposit\")\n    plt.ylabel('Number of samples')\n    plt.xlabel('Categories')\n    \n \n   \n\n    \n    plt.show()\n    \ndef categorical_var_plot2(label,dataset,target_variable,text_size):\n    \"\"\" \n     This function is defined in order to make two different plots:\n    - Pie: Shows the percentage of each category inside the variable.\n    - Count Plot: Shows the percentage of each category inside the variable, differenciated by if the person has hire a term a deposit.\n    \n    Args:\n    label: Category selected\n    dataset: Dataset\n    target_variable: target variable of the project\n    text_size: text size of the plot\"\"\"\n    colors_list= sns.color_palette(\"hls\",12)\n    sns.set(font_scale=text_size)\n    fig = plt.figure(figsize=(30,15))\n    plt.suptitle(t=label.title())\n\n    # Subplot 1: Pie Plot\n    a0= plt.subplot2grid((2,2),(0,0))\n    plt.pie(dataset[label].value_counts(),\n                       radius= 1,\n                       autopct='%1.1f%%',# to label the wedges with their percentage \n                       startangle=30,    \n                       shadow=True,\n                       pctdistance=1.16,    # the ratio between the center of each pie slice and the start of the text generated by autopct \n                       colors=colors_list, # add custom colors\n                       )\n    plt.title(\"Distribution\")\n    plt.legend(labels=pd.DataFrame(data[label].value_counts()).index, loc=\"upper right\", bbox_to_anchor=(1, 0, 0.5, 1))\n \n    # Subplot 2: Count Plot\n    a2 = plt.subplot2grid((2,2),(0,1))\n    a2 = sns.countplot(x=label,data=dataset,palette = colors_list ,hue=target_variable, order =np.sort(data[label].value_counts().index))\n    for p in a2.patches:\n        a2.annotate('{:.1f}%'.format(100*p.get_height()\/len(dataset[label])*1.), (p.get_x()+0.1, p.get_height()+10))\n        \n    plt.title (\"Distribution dif by if the client has suscribed the term deposit\")\n    plt.ylabel('Number of samples')\n    plt.xlabel('Categories')\n    \n    plt.show()\n    \ndef continuos_var_plot(label,dataset):\n    colors_list = sns.color_palette(\"hls\",12)\n    fig, (ax1,ax2) = plt.subplots(nrows = 2, ncols = 1, figsize = (40,30))\n    sns.distplot(dataset[label],hist=True,color=\"firebrick\" ,ax =ax1)\n    ax1.set_title(\"DISTRIBUTION\")\n    \n    sns.boxplot(x = dataset[label], orient = 'h',ax=ax2,palette=colors_list,whis=True)\n    ax2.set_xlabel(label)\n    ax2.set_ylabel(\"Distribution\")\n    ax2.set_title(\"Distribution\")\n    plt.show()\n    ","e731fd0d":"\ndata.info()\n\ndata.describe(include=\"all\")","54279f42":"#Pie plot of the target variable\n\ncolors_list = sns.color_palette(\"hls\",12)\n\ndata.y.value_counts().plot(kind='pie',\n                            figsize=(15, 8),\n                            autopct='%1.1f%%',# to label the wedges with their percentage \n                            startangle=30,    \n                            shadow=True,\n                            labels=None,         # turn off labels on pie chart\n                            pctdistance=1.12,    # the ratio between the center of each pie slice and the start of the text generated by autopct \n                            colors=colors_list, # add custom colors\n                            explode = [0, 0.1]\n                           \n                            )\nplt.legend(labels=(\"No\",\"Yes\"), loc='upper left')\nplt.title(\"Target Variable:Has the client subscribed a term deposit? \")\n\n#variable \n#y = pd.get_dummies(data.y)\n#y =np.asarray(y.yes)\n\n","fa01d1eb":"#Plot the distribution of age\nfig, (ax1, ax2,ax3) = plt.subplots(nrows = 3, ncols = 1, figsize = (40,20))\n\n\nsns.distplot(data.age,hist=True,color=\"firebrick\" ,ax =ax1)\nax1.set_title(\"AGE DISTRIBUTION\")\n\n\nsns.countplot(x=\"age\",hue=\"y\", data=data, ax=ax2, palette=colors_list)\nax2.set_title(\"Age distribution dif by if the client has subscribed a term deposit\")\n\nsns.boxplot(x = data.age, orient = 'h',ax=ax3,palette=colors_list,whis=True)\nax3.set_xlabel(\"Age\")\nax3.set_title(\"Age Distribution\")\n\n\n\n\n","e75fc5eb":"#Resume of information about the variable\ndata.age.describe()\n","19ce0d31":"\n#Define IQR\nIQR = data.age.quantile(0.75)-data.age.quantile(0.25)\n# Find the outliers lower limit \noutliers = data.age.quantile(0.75)+1.5*IQR\n# Find the distribution acummulated until outliers\nquantile_outlier = stats.percentileofscore(data.age,outliers)\nprint(\"Ages above:\",outliers, \"are outliers and acumulate the\",(100-quantile_outlier),\"% of the distribution.\")\nprint(\"Amount of different values of age in the sample:\",len(data.age.unique()))","282af446":"categorical_var_plot(\"job\",data,\"y\",1.4)","787cb1be":"categorical_var_plot(\"marital\",data,\"y\",1.6)","825f6826":"categorical_var_plot(\"education\",data,\"y\",1.5)\n\n","ec524b19":"default_gby= data.groupby([\"default\",\"y\"]).size()\ndefault_gby","0fffa22d":"categorical_var_plot2(\"default\",data,\"y\",1.4) ","270a9576":"categorical_var_plot(\"housing\",data,\"y\",1.5)","4a5a2c9c":"categorical_var_plot(\"loan\",data,\"y\",1.5)\n","86d743dd":"categorical_var_plot(\"contact\",data,\"y\",1.5)","df6fb174":"categorical_var_plot(\"month\",data,\"y\",1.4)","b42c0a34":"categorical_var_plot(\"day_of_week\",data,\"y\",1.5)","f2ba50f6":"fig, (ax1,ax2) = plt.subplots(nrows = 2, ncols = 1, figsize = (40,30))\n\nsns.distplot(data.duration,hist=True,color=\"firebrick\" ,ax =ax1)\nax1.set_title(\"Duration DISTRIBUTION\")\n\n\nsns.boxplot(x = data.duration, orient = 'h',ax=ax2,palette=colors_list,whis=True)\nax2.set_xlabel(\"duration of the call\")\nax2.set_ylabel(\"distribution\")\nax2.set_title(\"Duration Distribution\")\n\n","dc82aa39":"#Drop duration from the dataset\ndata= data.drop(columns=\"duration\")\n","55e62ee8":"continuos_var_plot(\"campaign\",data)","db5c6c8d":"pdays = data.groupby([\"pdays\",\"y\"]).size()\nprint(pdays)\n\n\nfig = plt.figure(figsize=(30,15))\ndata.pdays.plot(kind='hist', figsize=(10, 6)),\n\nplt.title(\"pdays\")\nplt.show()","822d3a55":"previous = data.groupby([\"previous\",\"y\"]).size()\nprint(previous)\n\n\nfig = plt.figure(figsize=(30,15))\ndata.previous.plot(kind='hist', figsize=(10, 6)),\n\nplt.title(\"previous\")\nplt.show()","9242fa91":"categorical_var_plot(\"poutcome\",data,\"y\",1.5)","08991414":"bin_labels = [0,1, 2, 3, 4]\ndata[\"age\"] = pd.qcut(data[\"age\"], q=[0, .25, .5, .75,(quantile_outlier\/100),1],labels=bin_labels).astype(\"float64\")\ndata.age.value_counts()","e75ea4c7":"#variable \n\ndata.y=pd.get_dummies(data.y,prefix=\"y\",drop_first=True)","18bb1a48":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import OrdinalEncoder\n\nscaler = MinMaxScaler()\nenc=OrdinalEncoder()\ncategorical_cols = [cname for cname in data if\n                    data[cname].dtype == \"object\"]\nX = data[categorical_cols].values\n\nX= enc.fit_transform(X)\nX=scaler.fit_transform(X)\n\ncategorical=pd.DataFrame(X,columns=categorical_cols)\ncategorical.head()\n\nnumerical_cols = [cname for cname in data if \n                data[cname].dtype in ['int64', 'float64']]\n\nX2=data[numerical_cols].values\nX2=scaler.fit_transform(X2)\nnumerical=pd.DataFrame(X2,columns=numerical_cols)\n\ny =data.y\n\n\ndata_1 = pd.concat([numerical,categorical,y],axis=1)\nprint(data_1.head())\n\ndataset = np.asarray(data_1)\n\n","f28061b3":"def confusion_matrix(actual,prediction):\n    TP = 0\n    FP = 0\n    FN = 0\n    TN = 0\n    for i in range(len(actual)):\n        if actual[i] == prediction[i] and actual[i] == 0:\n            TN +=1\n        elif actual[i] != prediction[i] and actual[i] == 1:\n            FN +=1\n        elif actual[i] != prediction[i] and actual[i] == 0:\n            FP+=1\n        else:\n            TP+=1\n    conf_matrix=([TN,FP,],[FN,TP])\n    return conf_matrix\n\ndef confusion_matrix_mean(confusions_matrix,n_folds):\n    TP=0\n    FP=0\n    FN=0\n    TN=0\n    \n    for i in range(n_folds):\n        TN += (confusions_matrix[i][0][0])\/n_folds\n        FP += (confusions_matrix[i][0][1])\/n_folds\n        FN += (confusions_matrix[i][1][0])\/n_folds\n        TP += (confusions_matrix[i][1][1])\/n_folds\n        \n    confusion_matrix= ([int(TN),int(FP)],\n                       [int(FN),int(TP)])\n    return confusion_matrix\n\ndef plot_cm(cm):\n  plt.figure(figsize=(5,5))\n  sns.heatmap(cm, annot=True, fmt=\"d\")\n  plt.title(\"Confusion matrix\")\n  plt.ylabel('Actual label')\n  plt.xlabel('Predicted label')\n\n  print('Clients which do not suscribed a term deposit, Correctly detected (True Negatives): ', cm[0][0])\n  print('Clients which do not suscribed a term deposit, Incorrectly Detected (False Positives): ', cm[0][1])\n  print('Cients which suscribed a term deposit, Incorrectly predicted (False Negatives): ', cm[1][0])\n  print('Clients which suscribed a term deposit, Correctly Predicted(True Positives): ', cm[1][1])\n  print('Total ', np.sum(cm[1]))","38db83f7":"def accuracy_metric(actual, predicted):\n    correct = 0\n    for i in range(len(actual)):\n        if actual[i] == predicted[i]:\n            correct += 1\n    return correct \/ float(len(actual)) * 100.0","902c1ef3":"def Precision(actual,predicted):\n    FP= 0\n    TP= 0\n    for i in range(len(actual)):\n        if actual[i] != predicted[i] and actual[i] == 0:\n            FP +=1\n        elif actual[i] == predicted[i] and actual[i] == 1:\n            TP +=1\n        else:\n            continue\n    if (FP+TP) > 0:\n        PREC = TP\/(FP+TP)\n    else: PREC = 0\n    return PREC","410bd9aa":"def Recall(actual,predicted):\n    FN= 0\n    TP= 0\n    for i in range(len(actual)):\n        if actual[i] != predicted[i] and actual[i] == 1:\n            FN +=1\n        elif actual[i] == predicted[i] and actual[i] == 1:\n            TP +=1\n        else:\n            continue\n    if (FN+TP) > 0:\n        REC = TP\/(FN+TP)\n    else: REC = 0\n    return REC\n        \n    \n    ","c2213ce8":"def F1_SCORE (PR,REC):\n    f1= 2*(PR*REC)\/(PR+REC)\n    return f1","139bb023":"#libraries\nimport random\nfrom random import seed\nfrom random import randrange\nfrom csv import reader\nfrom sklearn.metrics import precision_recall_curve\n\n\n \ndef train_test_split(dataset, split):\n    \"\"\" This function is defined in order to  split the data into train and test sets.\n    - dataset: the entire dataset, included the target variable in the last column\n    - split: value between 0 and 1, which defines the percentage of sample that will be used to train the model later \"\"\"\n    \n    train = list()\n    train_size = split * len(dataset)\n    dataset_copy = list(dataset)\n    while len(train) < train_size:\n        index = randrange(len(dataset_copy))\n        train.append(dataset_copy.pop(index))\n    test=np.asarray(dataset_copy)\n    train=np.asarray(train)\n    y_train= np.ravel(train[:,[-1]])\n    X_train = train[:, :-1]\n    y_test =np.ravel(test [:,[-1]])\n    X_test = test[:, :-1]\n    return X_train,X_test,y_train,y_test\n\n#function to split the data into 4 randomly sets\ndef cross_validation_split(dataset,n_folds):\n    \"\"\" This function is defined in order to split the data into n randomly sets\n    - dataset:the entire dataset, included the target variable in the last column\n    - n_folds: amount of sets in which we want to split the sample\"\"\"\n    data_split=list()\n    data_to_split= list(dataset)\n    fold_size = int(len(dataset))\/n_folds\n    for i in range(n_folds):\n        fold=list()\n        while len(fold) < fold_size:\n            index= randrange(len(data_to_split))\n            fold.append(data_to_split.pop(index))\n        data_split.append(fold)\n    return data_split\n\n\ndef evaluate_algorithm(dataset, algorithm, n_folds):\n    \"\"\" This function is defined in order to run the selected model through cross validation and calculate the different metrics defined for each split of the data selected. Then, the function returns the average metrics.\n     - dataset:the entire dataset, included the target variable in the last column\n     - algorithm: the model that we want to apply to the train sample\n     - n_folds: amount of sets in which we want to split the sample \"\"\"\n    # Split the dat\n    folds = cross_validation_split(dataset,n_folds)\n    scores= []\n    conf=[]\n    recall=[]\n    precision=[]\n    #conf.clear()\n    \n    for i in range(n_folds):\n        #Prepare the data\n        fold = folds[i]\n        train = list(folds)\n        test = train.pop(i)\n        train = sum(train, [])\n        test=np.asarray(test)\n        train=np.asarray(train)\n        y_train= np.ravel(train[:,[-1]])\n        X_train = train[:, :-1]\n        y_test =np.ravel(test [:,[-1]])\n        X_test = test[:, :-1]\n        #Define the model\n        model =  algorithm\n        #Fit the model\n        model.fit(X_train, y_train)\n        #Predict the model \n        model_pred = model.predict(X_test)\n        #Calculate Accuracy\n        accuracy = accuracy_metric(y_test,model_pred)\n        scores.append(accuracy)\n        #Confusion Matrix\n        cm_results = confusion_matrix(y_test,model_pred)\n        conf.append(cm_results)\n        #Precision \n        pr = Precision(y_test,model_pred)\n        precision.append(pr)\n        #Recall\n        rec = Recall(y_test,model_pred)\n        recall.append(rec)\n        #Precision Recall curve\n        # predict probabilities\n        probs = model.predict_proba(X_test)\n        # keep probabilities for the positive outcome only\n        probs = probs[:,1]\n        graph_precision, graph_recall,thres = precision_recall_curve(y_test, probs)\n        \n    acc_mean = np.mean(scores) \n    cm_mean= confusion_matrix_mean(conf,n_folds)\n    recall_mean=np.mean(recall)\n    precision_mean=np.mean(precision)\n    f1 = F1_SCORE(precision_mean,recall_mean)\n    \n    \n    return acc_mean,cm_mean,recall_mean,precision_mean,f1,graph_precision, graph_recall\n\n\n","aba29aa9":"#Set Seed\nrandom.seed(40)\nrandom_state = 100\nnp.random.state = random_state","40374a39":"\nfrom sklearn.neighbors import KNeighborsClassifier\nX_train,X_test,y_train,y_test = train_test_split(np.asarray(dataset), 0.75)\nscores_knn=[]\nscores_f1=[]\nk=[]\nfor i in range(1,30,2):\n    neigh = KNeighborsClassifier(n_neighbors = i)\n    neigh.fit(X_train, y_train)\n    model_pred = neigh.predict(X_test)\n    accuracy = accuracy_metric(y_test,model_pred)\n    scores_knn.append(accuracy\/100)\n    recall = Recall(y_test,model_pred)\n    precision = Precision(y_test,model_pred)\n    f1= F1_SCORE(precision,recall)\n    k.append(i)\n    scores_f1.append(f1)\n    print(\"K=\",i,\"F1_SCORE:\",f1)\n    \nplt.plot(k, scores_knn)\nplt.plot(k,scores_f1)\nplt.xlabel('K')\nplt.ylabel(\"F1 Score\")\nplt.show()","1aecc0d4":"from sklearn.neighbors import KNeighborsClassifier\n\n\nneigh = KNeighborsClassifier(n_neighbors=9)\nKNN_acc, KNN_cm, KNN_rec,KNN_pr,KNN_f1,KNN_graph_precision, KNN_graph_recall = evaluate_algorithm(np.asarray(dataset), neigh, n_folds=4)\nKNN_acc, KNN_cm, KNN_rec,\nprint(\"F1 Score:\",KNN_f1,\"\\n\",\"Confusion Matrix:\",KNN_cm,\"\\n\",\"Accuracy:\",KNN_acc)","4ddd8674":"from sklearn import svm\nfrom sklearn.svm import SVC\n\nsvm_rbf = svm.SVC(kernel='rbf',probability=True)\nsvm_lin = svm.SVC(kernel='linear',probability=True)\nsvm_poly = svm.SVC(kernel='poly',degree=3,probability=True)\nkernels = [svm_rbf,svm_lin,svm_poly]\n\nSVM_pr=[]\nSVM_rec=[]\nSVM_f1_score=[]\nSVM_graph_precision=[]\nSVM_graph_recall = []\nSVM_acc=[]\n\nfor model in kernels:\n    model.fit(X_train, y_train)\n    model_pred = model.predict(X_test)\n    model_pr = Precision(y_test,model_pred)\n    SVM_pr.append(model_pr)\n    model_rec = Recall(y_test,model_pred)\n    SVM_rec.append(model_rec)\n    model_f1_score = F1_SCORE(model_rec,model_pr)\n    SVM_f1_score.append(model_f1_score)\n    model_acc = accuracy_metric(y_test,model_pred)\n    SVM_acc.append(model_acc)\n    model_proba = model.predict_proba(X_test)\n    model_proba=model_proba[:,1]\n    model_graph_precision, model_graph_recall,model_thres = precision_recall_curve(y_test, model_proba)\n    SVM_graph_precision.append(model_graph_precision)\n    SVM_graph_recall.append(model_graph_recall)\n    \n    print(model,\"Precision \",model_pr,\"Recall \",model_rec,\"F1 Score:\",model_f1_score,\"Accuracy:\",model_acc)\n","4a253d97":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\n\nLR_acc, LR_cm,LR_rec,LR_pr,LR_f1,LR_graph_precision, LR_graph_recall = evaluate_algorithm(dataset, lr, 4)\n\nprint(\"F1 Score:\",LR_f1,\"\\n\",\"Confusion Matrix:\",LR_cm,\"\\n\",\"Accuracy:\",LR_acc)\n\n","38b9b262":"from sklearn.tree import DecisionTreeClassifier\nfor c in [\"gini\",\"entropy\"]:\n    DTC = DecisionTreeClassifier(criterion = c)\n    DTC.fit(X_train, y_train)\n    model_pred = DTC.predict(X_test)\n    accuracy = accuracy_metric(y_test,model_pred)\n    DTC_recall = Recall(y_test,model_pred)\n    DTC_precision = Precision(y_test,model_pred)\n    DTC_f1_score=F1_SCORE(DTC_precision,DTC_recall)\n    print(\"Criterion:\",c,\" F1 Score:\",DTC_f1_score,\"Accuracy:\",accuracy)\n","11e28806":"DTC = DecisionTreeClassifier(criterion = \"entropy\")\n\n\nDTC_acc, DTC_cm, DTC_rec,DTC_pr,DTC_f1,DTC_graph_precision, DTC_graph_recall = evaluate_algorithm(dataset, DTC, 4)\nprint(\"F1 Score:\",DTC_f1,\"\\n\",\"Confusion Matrix:\",DTC_cm,\"\\n\",\"Accuracy:\",DTC_acc)","ec7a5e13":"from sklearn.ensemble import RandomForestClassifier\nRFC_acc=[]\nfor n in range (100,600,100):\n    RFC = RandomForestClassifier(n_estimators = n)#criterion = entopy,gini\n    RFC.fit(X_train, y_train)\n    rfc_pred = RFC.predict(X_test)\n    accuracy = accuracy_metric(y_test,rfc_pred)\n    RFC_precision=Precision(y_test,rfc_pred)\n    RFC_recall=Recall(y_test,rfc_pred)\n    RFC_f1=F1_SCORE(RFC_precision,RFC_recall)\n    print(\"n_estimators:\",n,\"F1 Score:\",RFC_f1,\"Accuracy:\",accuracy)","16ee3821":"RFC = RandomForestClassifier(n_estimators = 500)#criterion = entopy,gin\nRFC.fit(X_train, y_train)\nrfc_pred = RFC.predict(X_test)\nRFC_acc = accuracy_metric(y_test,rfc_pred)\nRFC_pr=Precision(y_test,rfc_pred)\nRFC_rec=Recall(y_test,rfc_pred)\nRFC_cm= confusion_matrix(y_test,rfc_pred)\nRFC_f1=F1_SCORE(RFC_precision,RFC_recall)\n#Precision Recall curve\n# predict probabilities\nprobs = RFC.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:,1]\nRFC_graph_precision, RFC_graph_recall,RFC_thres = precision_recall_curve(y_test, probs)\n    ","fa329f53":"from sklearn.ensemble import GradientBoostingClassifier\n\nGBC = GradientBoostingClassifier()\n\nGBC_acc, GBC_cm,GBC_rec,GBC_pr,GBC_f1,GBC_graph_precision, GBC_graph_recall= evaluate_algorithm(dataset, GBC, 4)\n\nprint(\"F1 Score:\",GBC_f1,\"\\n\",\"Confusion Matrix:\",GBC_cm,\"\\n\",\"Accuracy:\",GBC_acc)\n\n","a5817d2c":"from xgboost import XGBClassifier\n\nXGBC = XGBClassifier()\n\nXGBC_acc, XGBC_cm, XGBC_rec,XGBC_pr,XGBC_f1, XGBC_graph_precision, XGBC_graph_recall= evaluate_algorithm(dataset, XGBC, 4)\n\nprint(\"F1 Score:\",XGBC_f1,\"\\n\",\"Confusion Matrix:\",XGBC_cm,\"\\n\",\"Accuracy:\",XGBC_acc)\n","bd00b24c":"from sklearn.naive_bayes import GaussianNB\n\nGNB = GaussianNB()\n\nGNB_acc, GNB_cm , GNB_rec,GNB_pr,GNB_f1,GNB_graph_precision, GNB_graph_recall = evaluate_algorithm(dataset, GNB, 4)\n\nprint(\"F1 Score:\",GNB_f1,\"\\n\",\"Confusion Matrix:\",GNB_cm,\"\\n\",\"Accuracy:\",GNB_acc)","321c10a8":"models_acc =(LR_acc,KNN_acc,SVM_acc[2],\nDTC_acc,RFC_acc,GBC_acc,XGBC_acc,GNB_acc)\nmodels_cm = [LR_cm,KNN_cm,#SVM_cm,\n             DTC_cm,RFC_cm,GBC_cm,XGBC_cm,GNB_cm]\nPR=[LR_pr,KNN_pr,SVM_pr[2],\n             DTC_pr,RFC_pr,GBC_pr,XGBC_pr,GNB_pr]\nREC=[LR_rec,KNN_rec,SVM_rec[2],\n             DTC_rec,RFC_rec,GBC_rec,XGBC_rec,GNB_rec]\nF1=[LR_f1,KNN_f1,SVM_f1_score[2],\n             DTC_f1,RFC_f1,GBC_f1,XGBC_f1,GNB_f1]\n\n    ","94f0d015":"Summary = pd.DataFrame(columns=[\"Logistic Regression\",\"KNN\",\"SVM\",\n                                \"DecisionTree\",\"Random Forest\",\n                                \"Gradient Boosting\",\"XGBoost\",\"Gaussian Naive Bayes\"],data=[F1,PR,REC,models_acc], index=([\"F1\",\"PR\",\"REC\",\"Accuracy\"])).transpose()\nSummary.sort_values(by=[\"F1\"],ascending=False,inplace=True)\nSummary","5154072a":"plt.figure(figsize=(30,15))\nplt.plot(KNN_graph_recall, KNN_graph_precision, marker='.', label='KNN')\nplt.plot(LR_graph_recall, LR_graph_precision, marker='.', label='Logistic Regression')\nplt.plot(SVM_graph_precision[2], SVM_graph_recall[2], marker='.', label='SVM')\nplt.plot(DTC_graph_recall, DTC_graph_precision, marker='.', label='DTC')\nplt.plot(RFC_graph_recall, RFC_graph_precision, marker='.', label='RFC')\nplt.plot(GBC_graph_recall, GBC_graph_precision, marker='.', label='GBC')\nplt.plot(XGBC_graph_recall, XGBC_graph_precision, marker='.', label='XGBC')\nplt.plot(GNB_graph_recall, GNB_graph_precision, marker='.', label='GNB')\nplt.title(\"Precision-Recall Curve\")\n# axis labels\nplt.xlabel('Recall')\nplt.ylabel('Precision')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","48b3db7f":"random_state = 40\nnp.random.state = random_state","de654fa6":"from numpy import argmax\n\ndef evaluate_diff_threshold(algorithm,dataset,train_size):\n    \"\"\"\n    This function is defined in order to calculate the threshold which maximizes the f-score \n    algorithm:the model that we want to apply to the train sample\n    dataset:the entire dataset, included the target variable in the last column\n    train_size: value between 0 and 1, which defines the percentage of sample, that will be used to train the model later\n    \"\"\"\n    X_train,X_test,y_train,y_test =train_test_split(np.asarray(dataset), train_size)\n    model=algorithm\n    model.fit(X_train, y_train)\n    model_probs = model.predict_proba(X_test)\n    model_probs = model_probs[:,1]\n    model_precision, model_recall,model_thres = precision_recall_curve(y_test, model_probs)\n    model_fscore = F1_SCORE(model_precision,model_recall)\n    # locate the index of the largest f score\n    ix = argmax(model_fscore)\n    p=model_thres[ix]\n    #Precision\n    model_pr = Precision(y_test,model_probs>p)\n    #Recall\n    model_rec = Recall(y_test,model_probs>p)\n    model_f1_score= F1_SCORE(model_pr,model_rec)\n    #Confusion Matrix\n    model_cm=confusion_matrix(y_test,model_probs>p)\n    print(\" Best Threshold:\",model_thres[ix],\"\\n\",\"F-Score=:\",model_f1_score,\"\\n\",\"Recall:\",model_rec,\"\\n\",\"Precision:\",model_pr) \n\n    plot_cm(model_cm)\n    \n    return model_pr,model_rec,model_f1_score,model_cm, model_precision, model_recall, p\n\n    ","2273d7d2":"GBC = GradientBoostingClassifier()\nGBC_pr,GBC_rec,GBC_f1_score,GBC_cm,GBC_curve_pr,GBC_curve_rec,p =  evaluate_diff_threshold(GBC,dataset,0.75)","f27781cb":"XGBC = XGBClassifier()\nXGBC_pr,XGBC_rec,XGBC_f1_score,XGBC_cm,XGBC_curve_pr,XGBC_curve_rec,q =  evaluate_diff_threshold(XGBC,dataset,0.75)\n","abc2b290":"from sklearn.svm import SVC\nSVC = SVC(kernel='poly',degree=3,probability=True,class_weight=\"balanced\")\nSVC_pr,SVC_rec,SVC_f1_score,SVC_cm,SVC_curve_pr,SVC_curve_rec,r =  evaluate_diff_threshold(SVC,dataset,0.75)","96ed698c":"LR = LogisticRegression(class_weight=\"balanced\")\nLR_pr,LR_rec,LR_f1_score,LR_cm,LR_curve_pr,LR_curve_rec,s =  evaluate_diff_threshold(LR,dataset,0.75)","be4561b7":"import matplotlib.patches as patches\n    \n\nfig, (ax1, ax2) = plt.subplots(1, 2,figsize=(30,15))\n \nplt.suptitle(\"Precision-Recall Curve\")\n\n\n\nax1.figsize=(30,15)\nax1.plot(GBC_curve_rec, GBC_curve_pr, marker='.',label='GBC',linewidth=1,zorder=1),\nax1.plot(XGBC_curve_rec, XGBC_curve_pr, marker=\".\", label='XGBC',linewidth=1,zorder=1),\nax1.plot(LR_curve_rec, LR_curve_pr, marker=\".\", label='LR',linewidth=1,zorder=1),\nax1.plot(SVC_curve_rec,SVC_curve_pr, marker=\".\", label='SVM',linewidth=1,zorder=1)\nax1.add_patch(patches.Rectangle((0.4, 0.4),0.2,0.2,edgecolor = 'blue',facecolor = 'lightgray',\n        fill=True\n     ) )\nax1.arrow(0.65,0.5,0.3,0,width=0.01)\n\n# axis labels\nax1.set_xlabel('Recall')\nax1.set_ylabel('Precision')\n# show the legend\nax1.legend()\n\n\nax2.set_facecolor(\"lightgray\")\nax2.scatter(GBC_rec, GBC_pr, marker='X',s=100, color='black', label='Best',zorder=2),\nax2.plot(GBC_curve_rec, GBC_curve_pr, marker='.',label='GBC',linewidth=1,zorder=1),\nax2.scatter(XGBC_rec, XGBC_pr, marker='X',s=100, color='black',zorder=2),\nax2.plot(XGBC_curve_rec, XGBC_curve_pr, marker=\".\", label='XGBC',linewidth=1,zorder=1),\nax2.scatter(LR_rec, LR_pr, marker='X',s=100, color='black',zorder=2),\nax2.plot(LR_curve_rec, LR_curve_pr, marker=\".\", label='LR',linewidth=1,zorder=1),\nax2.scatter(SVC_rec, SVC_pr, marker='X',s=100, color='black',zorder=2),\nax2.plot(SVC_curve_rec,SVC_curve_pr, marker=\".\", label='SVM',linewidth=1,zorder=1)\nax2.set_xlim(0.4,0.6)\nax2.set_ylim(0.4,0.6)\n\n# axis labels\nax2.set_xlabel('Recall')\nax2.set_ylabel('Precision')\n# show the legend\nax2.legend()\n\n\n","0057c7af":"> <a id=\"Related\"> <\/a>\n**IV. Related with the last contact of the current campaign**\n","c281608a":"><a id=\"DWID\"> <\/a>\n6. Dealing with Imbalanced Data\n\nIn order to deal with the imbalanced data, we are going to use two techniques:","75fa4c10":"> <a id=\"ACC\"> <\/a>\nII. Accuracy","ffe05e21":"> <a id=\"SVM\"> <\/a>\nII. SVM","184b7d52":"Additionally to find the best threshold, we are going to incorporate the weights of the classes  into the cost function Intuitively, we want to give higher weight to minority class and lower weight to majority class that at the end result is a classifier which can learn equally from all classes. Another technique used for this is Resampling to get more balanced data, that is out of the scope of our analysis.\n\n\n","628b10fb":"> <a id=\"PR\"> <\/a>\nIII. Precision","e5360088":"> <a id=\"LR\"> <\/a>\nIII. Logistic Regression","0db91bbd":"As we can see in the table above, the model that has a higher f1-score is Naives Bayes (0.405), with a recall of 0.5 wichs means that half of the clients which suscribed a term where classified correctly. On the other hand, the precision value is 0.33, which means that for all the clients predicted as positive, just one third of them have actually sucribed a term deposit. The second model with higher f1-score is Random Forest. This model shows the opposite behavior to Naive Bayes, since just above a half of the clients predicted as positive actually are, and the 30% of the clients that have sucribed a term deposit where classified correctly. \nThese numbers are not sufficient to conclude a great analysis and must be improved. \n\n","7e5972ec":"> <a id=\"Month\"> <\/a>\nB. Month\n\nMonth: last contact month of year (categorical: 'jan', 'feb', 'mar',\u2026, 'nov', 'dec')\n\nThis variable shows different rates for each moth. However, since the months which have a higher rate of clientes that hire a term deposit were the months with less calls done, it can not be confirmed a seasonly on the data.","b135816a":"<a id=\"Libraries_and_definition_of_functions_to_be_used\"><\/a> \n# **1. Libraries, import the data and definition of functions to be used for plot**\n\nIn order to start with the analysis, the first step to do is to import all the libraries, then import the data and define some functions that will be useful for the analysis.\n","a54ae177":"> <a id=\"Poutcome\"> <\/a>\nD. Poutcome\n\nPoutcome: outcome of the previous marketing campaign (categorical:\n'failure','nonexistent','success')","f4505b4c":"\n\nPrecision ($PR$) is a measure of the accuracy provided that a class label has been predicted.\n\n$PR = \\frac {TP}{TP+FP}$\n\n\n\n","89d1dd28":"As we can see in the graphic above, when the value of K increase, the f-score increase until K=9 and then strat to fall. Therefore, we choose a value of K= 9.","1935b024":"The following function was defined in order to split the data and run the models.\n","62a0150e":"> <a id=\"NB\"> <\/a>\nVIII. NAIVE BAYES","f86d883b":"> <a id=\"Previous\"> <\/a>\nC. Previous\n\n\nPrevious: number of contacts performed before this campaign and for\nthis client (numeric)\n","cc9fdb2a":"> <a id=\"RF\"> <\/a>\nV. RANDOM FOREST","9734221e":"> <a id=\"DTC\"> <\/a>\nIV. DECISION TREE","f72f446c":"> <a id=\"Pdays\"> <\/a>\nB. Pdays\n\nPdays: number of days that passed by after the client was last\ncontacted from a previous campaign (numeric; 999 means client was not\npreviously contacted)","bf757369":" > <a id=\"Default\"> <\/a>\n E. Default\n \n  Default: has credit in default? (categorical: 'no', 'yes', 'unknown')\n\n  \n  Regardind the variable default, the graphics don\u00b4t bring any relevant information. Also, there are only three cases that the person has a default credit, and none of them have a term deposit. \n  ","21ed714c":"And Recall($REC$) is the true positive rate.\n\n$REC =\\frac {TP}{TP+FN}$\n","5fec433b":"As for some models predicted previously, we can predict the probability of each client to suscribed a term deposit, it is important to note that the classifier.predict() function of scikit learn has a threshold of 0.5 by default for classification. A value above that threshold indicates \"suscribed\"; a value below indicates \"not suscribed\".\nIn order to be able to analyze if through the modification of the threshold,the results can be improved, we define the precision-recall curve(PCR). The same shows the relationship between precision and recall for every possible threshold. The PRC is a graph with: \n\n\u2022 The x-axis showing recall.\n\n\u2022 The y-axis showing precision\n\nThus every point on the PRC represents a chosen cut-off even though you cannot see this cut-off. What you can see is the precision and the recall that you will get when you choose this cut-off. \n\n\n\n","74696b17":"> <a id=\"CM\"> <\/a>\nI. Confusion Matrix","c08d3cf2":"><a id=\"FBCT\"> <\/a>\nI.Finding the Best Classification Threshold ","bc0bbade":"> <a id=\"bank_client_data\"> <\/a>\n**Bank Client Data**\n\n> <a id=\"Age\"> <\/a>\n**A. Age**\n\nAge (numeric)\n\nThe information and graphs made are not sufficient to conclude if age has an important impact in the target variable. However, the distribution analysis performed is important, in order to form groups since there are 78 different values of the age variable. This transformation will be shown, in the section Data Cleaning.\n\n","037c80fd":"In the following code, we proceed to separate the variables in categorical and numerical. First of all, for the categorical variables we apply an ordinal encoding,so for each unique category value an integer value is assigned. Then we apply the function MinMaxScaler(), so the values are in the same range, because ML algorithms works better when features are relatively on a similar scale and close to Normal Distribution. For the numeric values, we only apply the MinMaxScaler(), for the reason explained before.\n","8b659a25":"> <a id=\"Housing\"> <\/a>\n**F. Housing**\n\nHousing: has housing loan? (categorical: 'no', 'yes', 'unknown')\n\nInside this variable, each category has a similar rate of hiring a term deposit.","ee059b7d":"> <a id=\"Metrics\"> <\/a>\n# 4. Metrics","2e9e6fca":"\n\nEach confusion matrix row shows the Actual\/True labels in the test set and the columns show the predicted labels by classifier.\n\nIn the specific case of a binary classifier, we can interpret these numbers as the count of true positives ($TP$), false positives($FP$), true negatives($TN$), and false negatives($FN$).","ce43bd3d":"Gradient boosting classifiers are a group of machine learning algorithms that combine many weak learning models together to create a strong predictive model.\nGradient Boosting for classification.\n\nGB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.\n\n","247c1d65":"> <a id=\"Social\"> <\/a>\nVI. Social and economic context attributes","186939a4":"A decision tree is a decision support tool that is built using recursive partitioning to classify the data.\nThe algorithm chooses the most predictive feature to split the data on. The objetive is to determine \u201cwhich attribute is the best, or more predictive, to split data based on the feature.\nThe functions to measure the quality of a split are: \u201cgini\u201d for the Gini impurity and \u201centropy\u201d for the information gain.","9610baea":"> <a id=\"Campaign\"> <\/a>\nA. Campaign\n\nCampaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n","8b94addd":"SVM works by mapping data to a high-dimensional feature space so that data points can be categorized, even when the data are not otherwise linearly separated. A separator between the categories is found, then the data is transformed in such a way that the separator could be drawn as a hyperplane. Following this, characteristics of new data can be used to predict the group to which a new record should belong.\nThe SVM algorithm offers a choice of kernel functions for performing its processing. Basically, mapping data into a higher dimensional space is called kernelling. The mathematical function used for the transformation is known as the\u00a0kernel\u00a0function, and can be of different types, such as:\n\n    1.Linear\n    2.Polynomial\n    3.Radial basis function (RBF)\n    4.Sigmoid\nEach of these functions has its characteristics, its pros and cons, and its equation, but as there is no easy way of knowing which function performs best with any given dataset, we usually choose different functions in turn and compare the results.\n\nSince SVM takes a lot of time for training with large datasets, we are not going to apply cross validation.","a32dc045":"The graphic above shows that when the threshold decrease, the True Positive rate increase, but of course, at the cost of some more False Positives. For some cases, as the KNN curve, each graphic consists of two points effectively (since KNN predicts binary values) so this curve is not very useful or meaningful. However, for other cases like, GBC, XGBC, Logarithmic Regression and SVM, they show a remarkable improvement in the F1 Score. In the next lines we are going to define a function in order to find the thresold that maximizes F1 Score for each one.","93f891da":"> <a id=\"Job\"> <\/a>\n**B. Job**\n\nJob : type of job (categorical: 'admin.', 'blue-collar', 'entrepreneur', 'housemaid', 'management', 'retired', 'self-employed', 'services', 'student', 'technician', 'unemployed', 'unknown')\n\n\n\nRegarding the variable job, the conlusions that we can get are:\n\n1. The main difference between the clients that have a term deposit is between de admin and blue collar, because it represents 25.3% and 22.5 % respectively, and the percentaje that has a term deposit is almost the double for the group admin.\n2. The groups students and retired have rates of acquire a term deposit, 31.4% and 25.2% respectively, that are greater than the medium rate (11.3%). This is an important fact to analyze, but we are not in conditions that are  decisive characteristics because the available data for both classes is really small.","7fe498c9":" Then we need to convert the target variable into a dummy\n","1c1495c0":"><a id=\"IWC\"> <\/a>\nII. Incorporate Weights of the Classes.\n","ec34b0ad":"> <a id=\"Marital\"> <\/a>\n**C. Marital**\n\nMarital : marital status (categorical: 'divorced', 'married', 'single', 'unknown' ; note: 'divorced' means divorced or widowed)\n\n\nIn relation to the variable Marital, the percentage of clients that have a term deposit are similar, with a little major difference for single clients.","d97483da":"> <a id=\"REC\"> <\/a>\nIV. Recall","9b4e042a":"The F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (which represents perfect precision and recall) and its worst at 0.\n\n$ F1SCORE = 2 \\times \\frac {PR\\times REC}{PR+REC}$","5cf17985":"> <a id=\"Contact\"> <\/a>\nA. Contact\n\nContact: contact communication type (categorical: 'cellular','telephone'\n\nThe following graphics show that if the contact has been done by cellphone, the probability of hiring a term deposit is higher.\n","bcd3468e":"> <a id=\"F1\"> <\/a>\nV. F1 Score","072a0e2d":"# BANK MARKETING# \n\nThe data used in this analysis is related with direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to identify existing clients that have higher chance to subscribe for a term deposit, in order to know in which potential clients the bank should focus marketing efforts. The importance of this product is that term deposits allow banks to hold onto a deposit for a specific amount of time, so banks can invest in higher gain financial products to make a profit. \n\nSince it is a learning project,the following objectives have been define:\n\n- Definition of relevant functions to carry out the study, which are currently available in libraries. The objective of this is to affirm the concepts acquired. Special emphasis will be placed on the definition of metric functions and sample split.\n\n\n- Make a brief introduction of the different techniques used to classify data and apply them through the Scikit Learn library.\n\nThe report is divided into the following sections:\n\n\n1. [Libraries, import the data and definition of functions to be used for plot](#Libraries_and_definition_of_functions_to_be_used)<br>\n2. [First approach to the variables](#First_approach_to_the_variables)<br>\n\n    I. [Descriptive Statics](#Descriptive_Statics) <br> \n    \n    II. [y(Target Variable)](#y_Target_Variable) <br>\n    \n    III. **[Bank client data](#bank_client_data)**<br>\n    A. [Age](#Age)<br>\n    B. [Job](#Job)<br>\n    C. [Marital](#Marital)<br>\n    D. [Education](#Education)<br>\n    E. [Default](#Default)<br>\n    F. [Housing](#Housing)<br>\n    G. [Loan](#Loan)<br>    \n    \n    IV. **[Related with the last contact of the current campaign](#Related)**<br>\n    A. [Contact](#Contact)<br>\n    B. [Month](#Month)<br>\n    C. [Day of Week](#DOW)<br>\n    D. [Duration](#Duration)<br>\n    \n    V. **[Other attributes](#Other_att)**<br>\n    A. [Campaign](#Campaign)<br>\n    B. [Pdays](#Pdays)<br>\n    C. [Previous](#Previous)<br>\n    D. [Poutcome](#Poutcome)<br>\n    \n    VI.**[Social and economic context attributes](#Social)**<br>\n   \n3. [Data Cleaning](#Data_Cleaning) <br>\n\n4. [Metrics](#Metrics)<br>\n    I. [Confusion Matrix](#CM)<br>\n    II.[Accuracy](#ACC) <br>\n    III. [Precision](#PR)<br>\n    IV. [Recall](#REC)<br>\n    V. [F1 Score](#F1)<br>\n    VI. [Other Functions](#OT)<br>\n\n5. [Models](#Models)<br>\n    I. [KNN](#KNN) <br>\n    II.[SVM](#SVM) <br>\n    III. [LOGISTIC REGRESSION](#LR)<br>\n    IV. [DECISION TREE](#DTC)<br>\n    V. [RANDOM FOREST](#RF)<br>\n    VI. [GRADIENT BOOSTING](#GB)<br>\n    VII. [XGBOOST](#XGB)<br>\n    VIII. [NAIVE BAYES](#NB)<br>\n    IX. [SUMMARY](#SUM)<br>\n\n6.  [Dealing with Imbalanced Data](#DWID)<br>\n    I.[Finding the Best Classification Threshold](#FBCT)<br>\n    II.[Incorporate Weights of the Classes](#IWC)<br>\n    III.[Conclusion](#CONC)<br>\n    \n\n","ce1fb413":"> <a id=\"SUM\"> <\/a>\nIX. SUMMARY","73adb514":"> <a id=\"Other_att\"> <\/a>\n**Other attributes:**\n\n","bd3f4401":"Logistic Regression is a variation of Linear Regression, useful when the observed dependent variable, <b>y<\/b>, is categorical. It produces a formula that predicts the probability of the class label as a function of the independent variables.\n\nLogistic regression fits a special s-shaped curve by taking the linear regression and transforming the numeric estimate into a probability with the following function, which is called sigmoid function \ud835\udf0e:\n\n$$\n\u210e_\\theta(\ud835\udc65) = \\sigma({\\theta^TX}) =  \\frac {e^{(\\theta_0 + \\theta_1  x_1 + \\theta_2  x_2 +...)}}{1 + e^{(\\theta_0 + \\theta_1  x_1 + \\theta_2  x_2 +\\cdots)}}\n$$\nOr:\n$$\nProbabilityOfaClass_1 =  P(Y=1|X) = \\sigma({\\theta^TX}) = \\frac{e^{\\theta^TX}}{1+e^{\\theta^TX}} \n$$\n\nIn this equation, ${\\theta^TX}$ is the regression result (the sum of the variables weighted by the coefficients), `exp` is the exponential function and $\\sigma(\\theta^TX)$ is the sigmoid or logistic function , also called logistic curve. It is a common \"S\" shape (sigmoid curve).\n\nSo, briefly, Logistic Regression passes the input through the logistic\/sigmoid but then treats the result as a probability.\nThe objective of __Logistic Regression__ algorithm, is to find the best parameters \u03b8, for $\u210e_\\theta(\ud835\udc65)$ = $\\sigma({\\theta^TX})$, in such a way that the model best predicts the class of each case.","9931084f":"Accuracy ($ACC$) is the  number of correct predictions, divided by the hotal number of predictions. It is the fraction of predictions our model got right.\n\n $ACC = \\frac {TP + TN}{TN+FN+TP+FP}$","19b18e92":"<a id=\"First_approach_to_the_variables\"><\/a>\n# **2. First approach to the variables**\n<a id= \"Descriptive_Statics\"><\/a>\n    \n    I. Descriptive Statics:\n    \n- There are ten categorical variables, ten numerical variables and the target variable which is categorical, since is a classification problem. \n- There are no missing values.  \n    ","5448783d":"1. After the analysis I carry out, it can be concluded that the best model that predicts if the client will hire a term deposit is grandient boosting, since it has a  F-Score of 0.514, with a recall of 0.577, wichs means that the 57.7% of the clients which suscribed a term where classified correctly. On the other hand, the precision value is 0.463, which means that for all the clients predicted as positive, 46.3% of them have actually sucribed a term deposit. This means that nearly one of two clients, for whom the marketing campaign will be focused might hire a term deposit. This implies a remarkable improvement because in this analysis we can also improve the true positive rate.\n2. The other models improved(XGBC,Logarithmic Regression and SVM),also return similar results with a better performance than the one achived with the first analysis.\n \n","8929f808":"> <a id=\"Loan\"> <\/a>\nG. Loan\n\nLoan: has personal loan? (categorical: 'no', 'yes', 'unknown')\n \nInside this variable, each category has a similar rate of hiring a term deposit.\n ","9e6e5016":"Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean\/average prediction (regression) of the individual trees. Cross-validation is not necessary when using random forest, because multiple bagging in process of training random forest prevents over-fitting.","3d87f95f":"> <a id=\"Education\"> <\/a>\nD. Education\n\nEducation (categorical: 'basic.4y', 'basic.6y', 'basic.9y', 'high.school', 'illiterate', 'professional.course', 'university.degree', 'unknown')\n\nIn regard to the variable Education, it is important to note that the clients which have an University Degree,have a sligher trend to acquire a term deposit than the other categories.\n\n","caac3acf":"Naive Bayes is essentially a technique for assigning classifiers to a finite set. However, there is no single algorithm for training these classifiers, so Naive Bayes assumes that the value of a specific feature is independent from the value of any other feature, given the class variable. ","4bd29fd7":"> <a id=\"Duration\"> <\/a>\nD. Duration\n\nDuration: last contact duration, in seconds (numeric). \n\nThis attribute highly affects the output target (e.g., ifduration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known.\nThus, this input would be discarded if the intention is to have a realistic predictive model.","9add407a":"The following variables were included, which are not directly related with the clients: \n\n- Nr.employed: number of employees - quarterly indicator (numeric)\n- Cons.conf.idx: consumer confidence index - monthly indicator\n(numeric)\n- Emp.var.rate: employment variation rate - quarterly indicator\n(numeric)\n- Cons.price.idx: consumer price index - monthly indicator (numeric)\n- Euribor3m: euribor 3 month rate - daily indicator (numeric)\n\n\n","03a7ca0f":"> <a id=\"OT\"> <\/a>\nVI.Other functions","ee855081":"XGBoost\n\nXGBoost is a specific implementation of the Gradient Boosting method which uses more accurate approximations to find the best tree model. While regular gradient boosting uses the loss function of our base model (e.g. decision tree) as a proxy for minimizing the error of the overall model, XGBoost uses the 2nd order derivative as an approximation. Also, it use an advanced regularization (L1 & L2), which improves model generalization.","eee28b00":"![image.png](attachment:image.png)","97b21c5e":"The following functions were defined to generate to plot and make a first approach of the variables.","05b29b82":"> <a id=\"XGB\"> <\/a>\nVII. XGBOOST","55303b8e":"> <a id=\"Model\"> <\/a>\n\n# 5. Models\n\nIn order to carry out a good analysis, the metric that we are going to choose is crucial. Our main objective in this project is to find the clients most likely to subcribe a term deposit, in order to carry out specific marketing campaigns efficiently, Since that, we are going to focus on the f1 score to evaluate the models. As it was defined as the harmonic mean of Precision and Recall, it penalizes the extreme values. So, it is a good measure to reduce the incorrectly classified cases, in order to achieve a balance between false negatives and false positives.\n\n","ef8e4f48":"> <a id=\"DOW\"> <\/a>\nC. Day of Week\n\nDayofweek: last contact day of the week (categorical:'mon','tue','wed','thu','fri')\n\nInside this variable each category has a similar rate of hiring a term deposit.","2d11afd9":"<a id= \"y_Target_Variable\"><\/a>\nII. y (Target Variable)\n\nIn the following graphic, it can be clearly seen that most of the clients (88,7%) have not suscribed a term deposit. This is an important fact, because since is an imbalanced dataset, metrics like accuracy can create a misconception of the analysis. For example, if we predict that all the clients are not going to suscribed a term deposit, we achived an accuracy of 88.7%, but this not concurs with our analysis objetive.","3e63f78b":"As it was previously said, since there are 78 different values of the variable age, the quantiles and outliers were used as limits to form 5 groups. In the case of the first 3 groups, they will have a similar size, but it is not equal because the limit of the bin is included in the group, e.g. In the group 0, that has a superior limit of 32, we see that we have a lot of clients with this age (almost the maximum of the sample), so the group 0 will have a bigger size.","548aee37":"Functions to model and measure","8c41e448":"><a id=\"CONC\"> <\/a>\nIII. Conclusion","74e9304f":"> <a id=\"GB\"> <\/a>\nVI. GRADIENT BOOSTING","9318b23e":"> <a id=\"KNN\"> <\/a>\nI. KNN\n\nThe k-nearest-neighbors algorithm is a classification algorithm that takes a bunch of labelled points\nand uses them to learn how to label other points.\nThis algorithm classifies cases based on their similarity to other cases. In k-nearest neighbors, data points that are near each other are said to be \u201cneighbors\u201d.\nK-nearest neighbors is based on this paradigm: \u201cSimilar cases with the same class labels are near each other[.\n\nIn the following lines we run the model for different k groups of neighbors, in order to determine which value its more suitable for this data,\n","16fdcaae":"> <a id=\"Data_Cleaning\"> <\/a>\n# 3. Data Cleaning"}}