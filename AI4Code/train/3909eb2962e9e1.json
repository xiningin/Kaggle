{"cell_type":{"57904fb8":"code","b2661091":"code","1616be77":"code","59a5dabe":"code","5d404ee4":"code","08f76b4d":"code","e04b7281":"code","887b8254":"code","6a5629c1":"code","06590413":"code","16ab16db":"code","da53cab3":"code","4d2557a6":"code","a490b7b6":"code","674bf019":"code","701704b7":"code","a4ad75cb":"code","b5b4d4e7":"code","96447729":"code","43070e53":"code","c8927660":"code","158d7ac0":"code","ea7727e2":"code","56f4cfe9":"code","66716830":"code","014348df":"code","4aefde73":"code","4ff37e6d":"code","2e92febf":"code","7ed61238":"code","1bc56e45":"code","95ee735c":"code","e990cb84":"code","8d2777b0":"code","3876c67a":"code","d0f8d2c4":"code","de0a7307":"code","1af0e776":"code","374f79e8":"code","154e729d":"code","50c3c824":"code","b9a8bee2":"code","9f7fc12f":"code","dd5c9b2f":"code","87293007":"code","1565c8a6":"code","d890201e":"code","71845e4d":"code","d6bf8ed7":"code","997a2d5e":"code","e25368ee":"code","0fcd6c2e":"code","0b757b23":"code","4d7482aa":"code","ee743220":"markdown","e602a7eb":"markdown","c3a1721c":"markdown","1ef834b9":"markdown","8ae17bf9":"markdown","8ec488ca":"markdown","ccecf06f":"markdown","cc80abef":"markdown","c10bef8c":"markdown","c7e278e8":"markdown","a4ea9c62":"markdown","a2bacb1e":"markdown","af567dac":"markdown","2758f074":"markdown","a083885f":"markdown","83a20ffd":"markdown","c6d21dca":"markdown","fbc16634":"markdown","7d2f6560":"markdown","a2a760ed":"markdown","7da36a3c":"markdown","fee80c3f":"markdown","7a3777ea":"markdown","de90ae85":"markdown","685e08ce":"markdown","adb4bda5":"markdown","0f5ad8a6":"markdown","21064be2":"markdown","63a41283":"markdown","5b2b4107":"markdown","588c223b":"markdown"},"source":{"57904fb8":"from IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:90% !important; }<\/style>\"))","b2661091":"%matplotlib inline\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nsns.set() # set seaborn default","1616be77":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\n#from enum import Enum\nclass Columns:\n    # existing features\n    PassengerId = \"PassengerId\"\n    Survived = \"Survived\"\n    Pclass = \"Pclass\"\n    Name = \"Name\"\n    Sex = \"Sex\"\n    Age = \"Age\"\n    SibSp = \"SibSp\"\n    Parch = \"Parch\"\n    Ticket = \"Ticket\"\n    Fare = \"Fare\"\n    Cabin = \"Cabin\"\n    Embarked = \"Embarked\"\n    \n    # new features\n    Title = \"Title\"\n    FareBand = \"FareBand\"\n    Family = \"Family\"\n    Deck = \"Deck\" # get character from existing 'Cabin' values\n    CabinExists = \"CabinExists\"","59a5dabe":"train.head()","5d404ee4":"test.head()","08f76b4d":"print(train[[Columns.Pclass, Columns.Survived]].head())\ntrain[[Columns.Pclass, Columns.Survived]].groupby([Columns.Pclass]).mean().plot.bar()","e04b7281":"train[[Columns.Sex, Columns.Survived]].groupby([Columns.Sex]).mean().plot.bar()","887b8254":"fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(10, 8))\nsns.countplot(x=Columns.Sex, hue=Columns.Survived, data=train, ax=ax[0])\nsns.countplot(x=Columns.Sex, hue=Columns.Pclass, data=train, ax=ax[1])\nsns.countplot(x=Columns.Pclass, hue=Columns.Survived, data=train, ax=ax[2])","6a5629c1":"train[Columns.Age].plot.kde()","06590413":"df = train[train[Columns.Age].isnull() == False] # drop rows have no age\n\nbincount = 12\nage_min = int(df[Columns.Age].min())\nage_max = int(df[Columns.Age].max())\nprint(\"Age :\", age_min, \" ~ \", age_max)\ngap = int((age_max - age_min) \/ bincount)\nprint('gap:', gap)\n\nbins = [-1]\nfor i in range(bincount):\n    bins.append(i * gap)\nbins.append(np.inf)\nprint(bins)\n\n_df = df\n_df['AgeGroup'] = pd.cut(_df[Columns.Age], bins)\nfig, ax = plt.subplots(figsize=(20, 10))\nsns.countplot(x='AgeGroup', hue=Columns.Survived, data=_df, ax=ax)","16ab16db":"fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(16, 20))\nsns.violinplot(x=Columns.Pclass, y=Columns.Age, hue=Columns.Survived, data=train, scale='count', split=True, ax=ax[0])\nsns.violinplot(x=Columns.Sex, y=Columns.Age, hue=Columns.Survived, data=train, scale='count', split=True, ax=ax[1])\nsns.violinplot(x=Columns.Pclass, y=Columns.Sex, hue=Columns.Survived, data=train, scale='count', split=True, ax=ax[2])\n","da53cab3":"_train = train\n_train['Family'] = _train[Columns.SibSp] + _train[Columns.Parch] + 1\n_train[['Family', Columns.Survived]].groupby('Family').mean().plot.bar()","4d2557a6":"train[Columns.Age].plot.hist()","a490b7b6":"sns.countplot(x='Family', data=_train)","674bf019":"sns.countplot(x='Family', hue=Columns.Survived, data=_train)","701704b7":"train_len = train.shape[0]\n\nmerged = train.append(test, ignore_index=True) \nprint(\"train len : \", train.shape[0])\nprint(\"test len : \", test.shape[0])\nprint(\"merged len : \", merged.shape[0])","a4ad75cb":"merged[Columns.Family] = merged[Columns.Parch] + merged[Columns.SibSp] + 1\nif Columns.Parch in merged:    \n    merged = merged.drop([Columns.Parch], axis=1)\nif Columns.SibSp in merged:\n    merged = merged.drop([Columns.SibSp], axis=1)\n    \nmerged.head()","b5b4d4e7":"most_embarked_label = merged[Columns.Embarked].value_counts().index[0]\n\nmerged = merged.fillna({Columns.Embarked : most_embarked_label})\nmerged.describe(include=\"all\")","96447729":"merged[Columns.Title] = merged.Name.str.extract('([A-Za-z]+)\\. ', expand=False)\n\nprint(\"initial titles : \", merged[Columns.Title].value_counts().index)\n#initial titles :  Index(['Mr', 'Miss', 'Mrs', 'Master', 'Dr', 'Rev', 'Col', 'Ms', 'Mlle', 'Major',\n#                         'Sir', 'Jonkheer', 'Don', 'Mme', 'Countess', 'Lady', 'Dona', 'Capt'],\n\nmerged[Columns.Title] = merged[Columns.Title].replace(['Lady', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\nmerged[Columns.Title] = merged[Columns.Title].replace(['Countess', 'Lady', 'Sir'], 'Royal')\nmerged[Columns.Title] = merged[Columns.Title].replace(['Miss', 'Mlle', 'Ms', 'Mme'], 'Mrs')\n\nprint(\"Survival rate by title:\")\nprint(\"========================\")\nprint(merged[[Columns.Title, Columns.Survived]].groupby(Columns.Title).mean())\n\nidxs = merged[Columns.Title].value_counts().index\nprint(idxs)\n\nmapping = {}\nfor i in range(len(idxs)):\n    mapping[idxs[i]] = i + 1\nprint(\"Title mapping : \", mapping)\nmerged[Columns.Title] = merged[Columns.Title].map(mapping)\n\nif Columns.Name in merged:\n    merged = merged.drop([Columns.Name], axis=1)\n    \nmerged.head()","43070e53":"sns.countplot(x=Columns.Title, hue=Columns.Survived, data=merged)\nprint(merged[Columns.Title].value_counts())","c8927660":"mapping = {'male':0, 'female':1}\nmerged[Columns.Sex] = merged[Columns.Sex].map(mapping)","158d7ac0":"merged.head(n=10)","ea7727e2":"# {'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Rare': 5, 'Royal': 6}\n\nmapping = {1:21, 2:28, 3:28, 4:40, 5:50, 6:60}\ndef guess_age(row):\n    return mapping[row[Columns.Title]]\n\ndef fixup_age(df):\n    for idx, row in df[df[Columns.Age].isnull() == True].iterrows():\n        df.loc[idx, Columns.Age] = guess_age(row)\n    return df\n    \nmerged = fixup_age(merged)\nmerged.describe(include='all')","56f4cfe9":"def make_deck(df):\n    df[Columns.Deck] = df[Columns.Cabin].str.extract('([A-Za-z]+)', expand=True)\n    return df\n\nmerged = make_deck(merged)\nmerged.describe(include='all')","66716830":"merged[[Columns.Deck, Columns.Fare]].groupby(Columns.Deck).mean().sort_values(by=Columns.Fare)","014348df":"sns.countplot(x=Columns.Deck, hue=Columns.Survived, data=merged)","4aefde73":"print(\"total survived rate: \", merged[Columns.Survived].mean())\nprint(\"deck survived rate: \", merged[merged[Columns.Deck].isnull() == False][Columns.Survived].mean())\nprint(\"no deck survived rate: \", merged[merged[Columns.Deck].isnull()][Columns.Survived].mean())\n\nfig, ax = plt.subplots(2, 1, figsize=(16, 16))\nmerged[[Columns.Deck, Columns.Survived]].groupby(Columns.Deck).mean().plot.bar(ax=ax[0])\n\ndef generate_fare_group(df, slicenum):\n    if \"FareGroup\" in df:\n        df.drop(\"FareGroup\", axis=1)    \n    _min = int(df[Columns.Fare].min())\n    _max = int(df[Columns.Fare].max())\n    print(\"Fare :\", _min, \" ~ \", _max)\n    gap = int((_max - _min) \/ slicenum)\n    print('gap:', gap)\n\n    bins = [-1]\n    for i in range(slicenum):\n        bins.append(i * gap)\n    bins.append(np.inf)\n    print(bins)\n    df['FareGroup'] = pd.cut(df[Columns.Fare], bins)    \n    return df\n\ndf = generate_fare_group(merged.copy(), 16)\n\nsns.countplot(x=\"FareGroup\", hue=Columns.Survived, data=df[df[Columns.Deck].isnull()], ax=ax[1])","4ff37e6d":"merged[Columns.CabinExists] = (merged[Columns.Cabin].isnull() == False)\nmerged[Columns.CabinExists] = merged[Columns.CabinExists].map({True:1, False:0})","2e92febf":"merged.head()","7ed61238":"merged[merged[Columns.Fare].isnull()]","1bc56e45":"merged.loc[merged[Columns.Fare].isnull(), [Columns.Fare]] = merged[Columns.Fare].mean()","95ee735c":"merged.head()","e990cb84":"sns.distplot(merged[Columns.Fare])","8d2777b0":"'''\nlog\ub97c \ucde8\ud558\ub294 \ubc29\ubc95\n'''\n\n#merged[Columns.Fare] = merged[Columns.Fare].map(lambda i : np.log(i) if i > 0 else 0)\n\n'''\n\ub4f1\uae09\uc744 4\ub2e8\uacc4\ub85c \ub098\ub204\ub294 \ubc29\ubc95\n'''\nmerged[Columns.FareBand] = pd.qcut(merged[Columns.Fare], 4, labels=[1,2,3,4]).astype('float')\n#merged[Columns.Fare] = merged[Columns.FareBand]\n\nmerged.head(n=20)\n","3876c67a":"merged[Columns.Fare] = merged[Columns.FareBand]\nmerged = merged.drop([Columns.FareBand], axis=1)\nmerged.head()","d0f8d2c4":"merged.head()","de0a7307":"sns.distplot(merged[Columns.Fare])","1af0e776":"merged.head()","374f79e8":"if Columns.Ticket in merged:\n    merged = merged.drop(labels=[Columns.Ticket], axis=1)\nif Columns.Cabin in merged:\n    merged = merged.drop(labels=[Columns.Cabin], axis=1)\nif Columns.Deck in merged:\n    merged = merged.drop(labels=[Columns.Deck], axis=1)","154e729d":"merged.describe(include='all')","50c3c824":"merged.head()","b9a8bee2":"merged = pd.get_dummies(merged, columns=[Columns.Pclass], prefix='Pclass')\nmerged = pd.get_dummies(merged, columns=[Columns.Title], prefix='Title')\nmerged = pd.get_dummies(merged, columns=[Columns.Embarked], prefix='Embarked')\nmerged = pd.get_dummies(merged, columns=[Columns.Sex], prefix='Sex')\nmerged = pd.get_dummies(merged, columns=[Columns.CabinExists], prefix='CabinExists')\nmerged.head()","9f7fc12f":"from sklearn.preprocessing import MinMaxScaler\n\nclass NoColumnError(Exception):\n    \"\"\"Raised when no column in dataframe\"\"\"\n    def __init__(self, value):\n        self.value = value\n    # __str__ is to print() the value\n    def __str__(self):\n        return(repr(self.value))\n\n# normalize AgeGroup\ndef normalize_column(data, columnName):\n    scaler = MinMaxScaler(feature_range=(0, 1))    \n    if columnName in data:\n        aaa = scaler.fit_transform(data[columnName].values.reshape(-1, 1))\n        aaa = aaa.reshape(-1,)\n        #print(aaa.shape)\n        data[columnName] = aaa\n        return data\n    else:\n        raise NoColumnError(str(columnName) + \" is not exists!\")\n\ndef normalize(dataset, columns):\n    for col in columns:\n        dataset = normalize_column(dataset, col)\n    return dataset","dd5c9b2f":"merged.head()","87293007":"merged = normalize(merged, [Columns.Age, Columns.Fare, Columns.Family])","1565c8a6":"merged.head(n=10)","d890201e":"train = merged[:train_len]\ntest = merged[train_len:]\ntest = test.drop([Columns.Survived], axis=1)\n\ntrain = train.drop([Columns.PassengerId], axis=1)\n\ntest_passenger_id = test[Columns.PassengerId]\ntest = test.drop([Columns.PassengerId], axis=1)\n\nprint(train.shape)\nprint(test.shape)","71845e4d":"train_X = train.drop([Columns.Survived], axis=1).values\ntrain_Y = train[Columns.Survived].values.reshape(-1, 1)\nprint(train_X.shape)\nprint(train_Y.shape)","d6bf8ed7":"test.shape","997a2d5e":"test.describe(include='all')","e25368ee":"train.head()","0fcd6c2e":"test.head()","0b757b23":"import lightgbm as lgb\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit, RepeatedStratifiedKFold\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn import linear_model\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\n\nimport warnings\n\nfrom bayes_opt import BayesianOptimization\n\nn_splits = 6\n\ndef get_param(learning_rate, max_depth, lambda_l1, lambda_l2, \n              bagging_fraction, bagging_freq, colsample_bytree, subsample_freq, feature_fraction):\n    params = {'n_estimators':5000,\n                'boosting_type': 'gbdt',\n                'objective': 'regression',\n                'metric': 'rmse',\n                #'eval_metric': 'cappa',\n                'subsample' : 1.0,\n                'subsample_freq' : subsample_freq,\n                'feature_fraction' : feature_fraction,\n                'n_jobs': -1,\n                'seed': 42,                    \n                'learning_rate': learning_rate,\n                'max_depth': int(max_depth),\n                'lambda_l1': lambda_l1,\n                'lambda_l2': lambda_l2,\n                'bagging_fraction' : bagging_fraction,\n                'bagging_freq': int(bagging_freq),\n                'colsample_bytree': colsample_bytree,\n                'early_stopping_rounds': 100,\n                'verbose' : 0\n              }\n    return params\n\ndef opt_test_func(learning_rate, max_depth, lambda_l1, lambda_l2, \n             bagging_fraction, bagging_freq, colsample_bytree, subsample_freq, feature_fraction):\n    \n    params = get_param(learning_rate, max_depth, lambda_l1, lambda_l2, \n             bagging_fraction, bagging_freq, colsample_bytree, subsample_freq, feature_fraction)\n    acc, _ = train(params)\n    return acc\n    \ndef train(params):\n    models = []\n    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=7)\n    oof = np.zeros(len(train_X))\n\n    for train_idx, test_idx in kfold.split(train_X, train_Y):\n        X_train, y_train = train_X[train_idx], train_Y[train_idx]\n        X_valid, y_valid = train_X[test_idx], train_Y[test_idx]\n\n        y_train = y_train.reshape(-1)\n        y_valid = y_valid.reshape(-1)        \n\n        model = lgb.LGBMClassifier()\n        model.set_params(**params)\n\n        eval_set = [(X_valid, y_valid)]\n        eval_names = ['valid']\n        model.fit(X=X_train, y=y_train, eval_set=eval_set, eval_names=eval_names, verbose=0)\n        \n        pred = model.predict(X_valid).reshape(len(test_idx))\n        oof[test_idx] = pred\n        \n        models.append(model)\n    \n    \n    result = np.equal(oof, train_Y.reshape(-1))    \n    accuracy = np.count_nonzero(result.astype(int)) \/ oof.shape[0]\n    #print(\"accuracy : \", accuracy)    \n    return accuracy, models\n\ndef get_optimized_hyperparameters():    \n    bo_params = {'learning_rate' : (0.001, 0.1),\n                   'max_depth': (10, 20),\n                   'lambda_l1': (1, 10),\n                   'lambda_l2': (1, 10),\n            'bagging_fraction': (0.8, 1.0),\n                'bagging_freq': (1, 10),\n            'colsample_bytree': (0.7, 1.0),\n              'subsample_freq': (1, 10),\n            'feature_fraction': (0.9, 1.0)\n            }\n    \n    optimizer = BayesianOptimization(opt_test_func, bo_params, random_state=1030)\n    \n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore')\n        init_points = 16\n        n_iter = 16\n        optimizer.maximize(init_points = init_points, n_iter = n_iter, acq='ucb', xi=0.0, alpha=1e-6)\n        return optimizer.max['params']\n\ndef predict(models, test):\n    preds = []\n    for model in models:\n        pred = model.predict(test).reshape(test.shape[0])\n        preds.append(pred)\n    \n    preds = np.array(preds)\n    preds = np.mean(preds, axis=0) > 0.5\n    \n    return preds\n\nparams = get_optimized_hyperparameters()\nparams = get_param(**params)\nacc, models = train(params)\nprint(\"train accuracy : \", acc)\n\ntest_pred = predict(models,test)\nprint(test_pred.shape)\nsubmission = pd.DataFrame({\"PassengerId\" : test_passenger_id, \"Survived\":test_pred.reshape(-1).astype(np.int)})\n\n\n","4d7482aa":"submission = pd.DataFrame({\"PassengerId\" : test_passenger_id, \"Survived\":test_pred.reshape(-1).astype(np.int)})\nsubmission.to_csv('submission.csv', index=False)","ee743220":"## features :\n\n- **PassengerId** <br>\n- **Survived** : Survival (1: survived, 0 : not)<br>\n- **Pclass** : boarding pass class (1 : 1st, 2 : 2nd ,3 : 3rd)<br>\n- **Name** <br>\n- **Sex** <br>\n- **Age**  <br>\n- **SibSp** : number of siblings + spouses accompanying<br>\n- **Parch** : number of parents + children accompanying<br>\n- **Ticket** : ticket number<br>\n- **Fare** <br>\n- **Cabin** : cabin number<br>\n- **Embarked** : embarked port (C : Cherbourg, Q : Queenstown, S : Southampton)<br>\n\n","e602a7eb":"add the existence of Cabin as feature","c3a1721c":"make 'Family' and drop 'Parch'\/'SibSp'","1ef834b9":"After finish data manipulation, do below.\n\n- remove unnecessary columns\n- scaling\n- detach train\/test\n- separate train as input\/label(survived)","8ae17bf9":"Survival rate by Pclass","8ec488ca":"fix 'Embarked'","ccecf06f":"Fill null 'Fare' column","cc80abef":"detach merged to train\/test","c10bef8c":"Numerical data need to be scalied.","c7e278e8":"# kaggle Titanic : LGBM + BayesianOpt Baseline\n\nThis notebook is a very basic and simple introduction of how using bayesian optimization for selecting best hyperparameter values and LightGBM for training\/predicting.\n\n\n**1. [EDA](#data_analysis)** <br>\n**2. [modify data](#data_manipulation)** <br>\n**3. [modeling\/train\/submit](#modelling_training_submit)** <br>\n\n<a id=\"Introduction\"><\/a> <br> \n","a4ea9c62":"get survival rate change by Pclass\/Age","a2bacb1e":"remove unnecessary columns","af567dac":"survival rate by age","2758f074":"survival rate by sex.","a083885f":"Extract alphabet from 'Cabin' and make 'Deck'","83a20ffd":"Survived count by Deck and Fare","c6d21dca":"Show relation of Deck and Survived","fbc16634":"<a id=\"data_analysis\"><\/a> <br> \n# **1. EDA:**\n\nload .csv files","7d2f6560":"#### Check for features that do not have data.","a2a760ed":"<a id=\"modelling_training_submit\"><\/a> <br> \n# **3. Model train\/submit:**\n\n","7da36a3c":"change category features to one-hot encoding <br>\n- Pclass\n- Embarked\n- Title","fee80c3f":"## merge train\/test and manipulate and detach last.","7a3777ea":"extract Title from Name and change to number value<br>\nremove Name","de90ae85":"survived count by family count","685e08ce":"survival rate by age group(bar graph)","adb4bda5":"survival rate by number of family ( Parch + SibSp + 1(self))","0f5ad8a6":"<a id=\"data_manipulation\"><\/a> <br> \n# **2. Data manipulation:**\n\nprocess empty column values <br>\n\n###### complement & process existing features\n**Age** : need to fix null values<br>\n**Cabin** : too much null values<br>\n**Embarked** : since there are few nulls, seems to be no problem filling rouch values<br>\n**Parse, SibSp** : sum up 'Family' and delete.\n\n###### add new features\n**Family** : Parch + SibSp + 1(self) <br>\n**Title** : extract from 'Name'.","21064be2":"Just use average value.","63a41283":"The distribution of 'Fare' is in high skewness. It is said to adversely affect the learning of the model. <br>\nuse log value or band rate","5b2b4107":"### Check features","588c223b":"Fill empty Age columns"}}