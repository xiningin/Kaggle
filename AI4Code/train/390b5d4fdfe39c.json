{"cell_type":{"2b794c23":"code","316801e9":"code","8e907f22":"code","35a70db9":"code","9eef0314":"code","6953cf13":"code","9dac7e8a":"code","39e8707c":"code","d7249acd":"code","8cdf9779":"code","7d4f4b18":"code","a3e3cce9":"code","0805cc5b":"code","f0a66162":"code","2172b887":"code","5d8a4e2d":"code","ae568aa4":"code","bb138d81":"code","1366846f":"code","f97931bf":"code","7cbabf40":"markdown","aa718264":"markdown","50744824":"markdown","a5a0187f":"markdown","7a59c5e6":"markdown"},"source":{"2b794c23":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","316801e9":"import torch\nimport torchvision\nimport torch.nn.functional as F\nfrom torch import optim, nn\nfrom torchvision import transforms, models, datasets\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm import tqdm\nimport shutil \n\n%matplotlib inline","8e907f22":"print(torch.cuda.is_available())\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(device)","35a70db9":"path = \"\/kaggle\/input\/digit-recognizer\/\"\ntrain_df = pd.read_csv(path + 'train.csv')\ntest_df = pd.read_csv(path + 'test.csv')\ntrain_df.shape, test_df.shape","9eef0314":"train_classes = train_df['label'].values\ntrain_img = (train_df.iloc[:, 1:].values).astype('float32')\ntest_img = (test_df.iloc[:, :].values).astype('float32')\ntrain_img.shape","6953cf13":"X_train, X_test, y_train, y_test = train_test_split(\n                                    train_img, train_classes, stratify = train_classes,\n                                    test_size = 0.2, random_state = 42)\nX_train.shape, y_train.shape, X_test.shape, test_img.shape","9dac7e8a":"train = np.array(X_train).reshape(33600, 1, 28, 28)\nval = X_test.reshape(8400, 1, 28, 28)\ntest = test_img.reshape(28000, 1, 28, 28)\n\ntrain.shape","39e8707c":"for i in range(3):\n    plt.subplot(330 + (i + 1))\n    plt.imshow(train[i].squeeze(), cmap=plt.get_cmap('gray'))\n    plt.title(y_train[i])","d7249acd":"# train\nx_train_tensor = torch.tensor(train)\/255\ny_train_tensor = torch.tensor(y_train)\ntrain_tensor = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n\n# val\nx_test_tensor = torch.tensor(val)\/255\ny_test_tensor = torch.tensor(y_test)\nval_tensor = torch.utils.data.TensorDataset(x_test_tensor, y_test_tensor)\n\n# test\ntest_tensor = torch.tensor(test)\/255","8cdf9779":"train_loader = torch.utils.data.DataLoader(train_tensor, batch_size=64, num_workers= 4, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_tensor, batch_size = 64, num_workers=4, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_tensor, batch_size= 64, shuffle=False)","7d4f4b18":"class Neural(nn.Module):\n    def __init__(self, model, output):\n        super(Neural, self).__init__()\n        self.model = nn.Sequential(\n                nn.Conv2d(1, 64, kernel_size = (7,7), stride = (2, 2), padding = (3, 3), bias = False),\n                *(list(model.children())[1:-1]),\n                nn.AdaptiveMaxPool2d(output_size = 1),\n                nn.Flatten(),\n                nn.BatchNorm1d(512, eps=1e-05, momentum=0.1, track_running_stats = True, affine = True),\n                nn.Dropout(p = 0.25, inplace = True),\n                )\n        self.fc = nn.Sequential(\n                nn.Linear(in_features=512, out_features = 128, bias=True),\n                nn.BatchNorm1d(128, eps=1e-05, momentum=0.1, track_running_stats = True, affine = True),\n                nn.Dropout(p=0.3, inplace = True),\n                nn.ReLU(inplace = True),\n                nn.Linear(128, output, bias = True),\n                nn.Softmax(dim = 1))\n\n    \n    def forward(self, x):\n\n        x = self.model(x)\n        x = self.fc(x)\n        \n        return x\n    \nresnet_model = models.resnet18(pretrained=True)\nmodel = Neural(resnet_model, 10)\nmodel","a3e3cce9":"opt = optim.Adam(model.parameters(), lr=0.02)\nloss_func = nn.CrossEntropyLoss()\nscheduler = optim.lr_scheduler.OneCycleLR(opt, max_lr=0.003, steps_per_epoch=len(train_loader.dataset), epochs=10)\nmodel.to(device);","0805cc5b":"%time\ndef train_model(models, loss, optimizer, train_data, val_data, scheduler=None, num_epoch=15):\n    \n    train_loss = []\n    train_acc = []\n    val_loss = []\n    val_acc = []\n    \n    best_train_acc, best_val_acc = 0, 0\n    \n    for epoch in tqdm(range(num_epoch)):\n        \n        models.train()\n        _train_loss, _train_acc = 0, 0\n            \n        for batch, (xb, yb) in enumerate(train_data):\n            xb, yb = xb.to(device), yb.to(device)\n            pred = models(xb)\n            yb = torch.squeeze(yb).long()\n\n            _loss = loss(pred, yb)\n            \n            _, pred = torch.max(pred, dim=1)\n            \n            _train_acc += torch.sum(pred==yb).item()\n            _train_loss += _loss.item()\n            _loss.backward()\n            \n            \n            optimizer.step()\n            optimizer.zero_grad()\n            if scheduler is not None:\n                scheduler.step()  \n            if batch % 200 == 0:\n                print(str(epoch) + \"  loss:  \" + str(_loss.item()))\n         \n        accuracy = 100 * _train_acc \/ len(train_data.dataset)\n        print('Best accuracy train epoch: ' + str(epoch) + ' ' + str(accuracy))\n        train_loss.append(_train_loss \/ len(train_data.dataset))\n        train_acc.append(accuracy)\n        models.eval()\n        with torch.no_grad():\n            _val_loss, _val_acc = 0, 0\n            for val_x, val_y in val_data:\n                val_x, val_y = val_x.to(device), val_y.to(device)\n                pred = models(val_x)\n                _val_loss += loss(pred, val_y).item()\n                _, preds = torch.max(pred, 1)\n                _val_acc += torch.sum(preds == val_y.data)\n                \n            val_accuracy = 100 * _val_acc.item() \/ len(val_data.dataset)\n            print(\"Best accuracy val epoch \" + str(epoch) + ' = ' + str(val_accuracy))\n            if val_accuracy >= best_val_acc:\n                print('Save best model')\n                torch.save(model.state_dict(), f\"test.pth\")\n                best_val_acc = val_accuracy\n            val_loss.append(_val_loss \/ len(val_data.dataset))\n            val_acc.append(val_accuracy)\n            \n         \n    return models, train_loss, train_acc, val_acc, val_loss\n\nnew_model, train_loss, train_acc, val_acc, val_loss = train_model(model, loss_func, opt, train_loader, val_loader, scheduler=scheduler, num_epoch=10)","f0a66162":"def show_loss_and_accuracy(train_loss, val_loss, train_acc, val_acc, title):\n\n    fig, (ax1, ax2) = plt.subplots(figsize=(20, 12), nrows=2)\n\n    x = list(range(len(train_loss)))\n\n    ax1.plot(x, train_loss,  label='train_loss')\n    ax1.plot(x, val_loss, label='val_loss')\n    ax1.set_title(\"loss in train and val\")\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('loss')\n    ax1.legend()\n\n    ax2.plot(x, train_acc, label='train_accuracy')\n    ax2.plot(x, val_acc, label='val_accuracy')\n    ax2.set_title(\"Accuracy in train and val\")\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Accuracy')\n\n    fig.show((ax1, ax2))\n        \n        \nshow_loss_and_accuracy(train_loss, val_loss, train_acc, val_acc, title='loss')","2172b887":"count = 0\nfor n, param in enumerate(new_model.parameters()):\n    count += 1\n    if n >= 50:\n        param.requires_grad = True\n\ncount","5d8a4e2d":"opt = optim.Adam(new_model.parameters(), lr=0.001)\nloss_func = nn.CrossEntropyLoss()\nscheduler = optim.lr_scheduler.OneCycleLR(opt, max_lr=0.0001, steps_per_epoch=len(train_loader.dataset), epochs=10)\nnew_model.to(device);","ae568aa4":"new_model, train_loss, train_acc, val_acc, val_loss = train_model(new_model, loss_func, opt, train_loader, val_loader, scheduler=scheduler, num_epoch=10)","bb138d81":"def predicts(model, data):\n    model.eval()\n    test_preds = torch.LongTensor()\n    for i, item in enumerate(data):\n        item = item.to(device)\n        out = model(item)\n        _, preds = torch.max(out.data, 1)\n        test_preds = torch.cat((test_preds, preds.cpu()), dim = 0)\n        \n    return test_preds","1366846f":"test_set_pred = predicts(new_model, test_loader)\n\nsubm_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/sample_submission.csv')\n\nsubm_df['Label'] = test_set_pred\nsubm_df.head()","f97931bf":"subm_df.to_csv('submit_test.csv', index = False)","7cbabf40":"Convert data in float","aa718264":"### Resnet model","50744824":"### Load data","a5a0187f":"### Visualization data","7a59c5e6":"Split data in 80% to train and 20% to val"}}