{"cell_type":{"78bcc6cf":"code","e9cda6bd":"code","5b9f81b7":"code","a323882b":"code","096b3532":"code","6e781710":"code","0f0f540a":"code","e435bf9f":"code","2a1acc61":"code","a6823cf3":"code","ca53f1bf":"code","8e17e98b":"code","2922876d":"code","b0bba95b":"code","fd6e3d74":"code","9097f798":"code","c1990775":"code","d7839800":"code","a52a3253":"markdown","9c7b1ff1":"markdown","d2d69f0e":"markdown","43737958":"markdown","8b0c869d":"markdown","4c34b351":"markdown","ebb2c778":"markdown","b6576f4e":"markdown","06eaa810":"markdown","3c4d1f7a":"markdown","e89ede7e":"markdown","c68ec8d2":"markdown","e3d60151":"markdown","c4cff540":"markdown","5c686476":"markdown","3850ba08":"markdown","b97d9246":"markdown","4a1ed4be":"markdown"},"source":{"78bcc6cf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e9cda6bd":"import numpy as np #linear algebra\nimport pandas as pd #data preprocessing, csv file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans","5b9f81b7":"data = pd.read_csv(\"..\/input\/Iris.csv\", index_col = 0)\ndata.head(5)","a323882b":"print(\"Iris-setosa\")\ndata1 = data[data[\"Species\"] == \"Iris-setosa\"]\ndata1.describe()","096b3532":"print(\"Iris-versicolor\")\ndata2 = data[data[\"Species\"] == \"Iris-versicolor\"]\ndata2.describe()","6e781710":"print(\"Iris-virginica\")\ndata3 = data[data[\"Species\"] == \"Iris-virginica\"]\ndata3.describe()","0f0f540a":"X = data.iloc[:, 0:4]\nf, ax = plt.subplots(figsize=(10, 8))\ncorr = X.corr()\nprint(corr)\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), \n            cmap=sns.diverging_palette(220, 10, as_cmap=True), square=True, ax=ax)","e435bf9f":"sns.pairplot(data, hue = \"Species\", size=3, diag_kind=\"kde\")","2a1acc61":"PCA = PCA(n_components = 4)\nPCA.fit(X)\nprint(\"explained variance ratio: %s\"\n      % str(PCA.explained_variance_ratio_))\nplt.plot(np.r_[[0], np.cumsum(PCA.explained_variance_ratio_)])\nplt.xlim(0,5)\nplt.xlabel(\"number of components\")\nplt.ylabel(\"cumulative explained variance\");","a6823cf3":"y = data[\"Species\"]\ny = y.map({\"Iris-setosa\" : 0, \"Iris-virginica\" : 1, \"Iris-versicolor\" : 2})\n\nX_r = PCA.transform(X)\nplt.scatter(X_r[y == 0, 0], X_r[y == 0, 1], alpha=.8, color = 'blue',\n              label=\"Iris-setosa\")\nplt.scatter(X_r[y == 1, 0], X_r[y == 1, 1], alpha=.8, color = \"green\",\n              label = \"Iris-virginica\")\nplt.scatter(X_r[y == 2, 0], X_r[y == 2, 1], alpha =.8, color = 'orange', \n              label = \"Iris-versicolor\")\nplt.legend(loc='best', shadow=False, scatterpoints=1)\nplt.xlabel(\"First component\")\nplt.ylabel(\"Second component\")\nplt.title(\"PCA of IRIS datset\")","ca53f1bf":"components = pd.DataFrame(PCA.components_, columns = X.columns, index=[1, 2, 3, 4])\ncomponents","8e17e98b":"list_nclusters = range(2, 10)\nlist_init_seed = range(1, 100)\ntotal = len(list_nclusters)\ninertia = np.zeros(total)\nncluster = np.zeros(total)\nnseed = np.zeros(total)\ninertia_aux = np.inf\ncount = -1\n\nfor n in list_nclusters:\n    count = count + 1\n    for s in list_init_seed:\n        cl = KMeans(n_clusters = n, random_state =s)\n        cl.fit(X_r[:, 0:2])\n        if  (cl.inertia_ < inertia_aux) :\n            inertia[count] = cl.inertia_\n            inertia_aux = cl.inertia_\n            ncluster[count] = n\n            nseed[count] = s","2922876d":"matrix = np.matrix(np.c_[inertia, ncluster, nseed])\nmodels = pd.DataFrame(data = matrix, columns = ['Inertia', 'Number of clusters', 'random seed'])\nmodels","b0bba95b":"models.plot(kind = 'scatter', x = 1, y=0)","fd6e3d74":"c1 = KMeans(n_clusters = 3, random_state = 1)\nc1.fit(X_r[:, 0:2])\ncluster = cl.predict(X_r[:, 0:2])\nplt.scatter(X_r[cluster ==0, 0], X_r[cluster == 0, 1], alpha =.8, color = 'orange', label = \"Cluster 1\")\nplt.scatter(X_r[cluster ==1, 0], X_r[cluster == 1, 1], alpha =.8, color = 'blue', label = \"Cluster 2\")\nplt.scatter(X_r[cluster ==2, 0], X_r[cluster == 2, 1], alpha =.8, color = 'green', label = \"Cluster 3\")\nplt.legend(loc=\"best\", shadow=False, scatterpoints =1)\nplt.xlabel(\"First component\")\nplt.ylabel('second component')\nplt.title(\"PCA of IRIS dataset\")\n","9097f798":"c = pd.DataFrame(cluster, columns = ['Cluster'], index = range(1, len(cluster) + 1))\nc.head(n=5)\nc[\"Cluster\"] = c[\"Cluster\"].map({0: 'Cluster 1', 1:'Cluster 2', 2: \"Cluster 3\"})\nsns.pairplot(pd.concat([data, c], axis = 1), hue = \"Cluster\", size=3, diag_kind= \"kde\")","c1990775":"c1 = KMeans(n_clusters= 4, random_state =2)\nc1.fit(X_r[:, 0:2])\ncluster = cl.predict(X_r[:, 0:2])\nplt.scatter(X_r[cluster == 0, 0], X_r[cluster ==0, 1], alpha =.8, color = 'orange', label =\"cluster 1\")\nplt.scatter(X_r[cluster == 1, 0], X_r[cluster ==1, 1], alpha =.8, color = 'blue', label =\"cluster 2\")\nplt.scatter(X_r[cluster == 2, 0], X_r[cluster ==2, 1], alpha =.8, color = 'red', label =\"cluster 3\")\nplt.scatter(X_r[cluster == 3, 0], X_r[cluster ==3, 1], alpha =.8, color = 'green', label =\"cluster 4\")\nplt.legend(loc='best', shadow = False, scatterpoints=1)\nplt.xlabel(\"First component\")\nplt.ylabel(\"Second component\")\nplt.title(\"PCA of IRIS dataset\")","d7839800":"c = pd.DataFrame(cluster, columns = ['Cluster'], index = range(1, len(cluster) + 1))\nc.head(n=5)\nc['Cluster'] = c['Cluster'].map({0: 'Cluster 1', 1:'Cluster 2', 2:'Cluster 3', 3:'Cluster 4'})\nsns.pairplot(pd.concat([data, c], axis = 1), hue = \"Cluster\", size=3, diag_kind=\"kde\")","a52a3253":"**now , we're going to select the number of clusters, let's see the objective function best value!**","9c7b1ff1":"**K-Means**","d2d69f0e":"**Principal Component Analysis**","43737958":"loading the data","8b0c869d":"*The K-means unsupervised algorithm involves randomly selecting K initial centroids where K is a user defined number of desired clusters.\n\nThe problem is computationally difficult (NP-hard); however, there are efficient heuristic algorithms that are commonly employed and converge quickly to a local optimum.\n\nWe apply differenet algorithms chaging the parameters number of clusters (n) and random_seed (s). We save the best models, taking those that minimize the objective function.*","4c34b351":"*The variables PetalLengthCm is highly correlated with the first components(92.4% variance)*","ebb2c778":"> The sam eplots with four clusters!","b6576f4e":"**The 98% of the variance is expalined, approxmately, by the first two components.\n\n**Lets see the species represented by this components! **","06eaa810":"It is a statistical procedure that uses an orthogonal transformation to convert set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables","3c4d1f7a":"Different statisstics by species","e89ede7e":"> PCA is a method which allows you to identify the \"Directions\" in which the most of the variations in the data are present","c68ec8d2":"**Correlation between variables**","e3d60151":"**Kernel density estimations**","c4cff540":"**Import libraries to look at iris data**","5c686476":"**What do the components mean? Let's see the correlation between variables and components**","3850ba08":"let's see the best models!!!","b97d9246":"Three and four clusters would be good.\n\nwe create different plots in order to explore the clusters","4a1ed4be":"Kernal density with three clusters"}}