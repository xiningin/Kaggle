{"cell_type":{"c5e2014a":"code","a5af432a":"code","7bd28f31":"code","6526e89c":"code","4d02389f":"code","cdbc795e":"code","66c522cf":"code","2ffd4787":"code","bf240b7b":"code","0a539b73":"code","03960e04":"code","8a2ab023":"code","a0684205":"code","c5efaef0":"code","11d4df1b":"code","d965a2f1":"code","ee94fa9e":"code","767808d7":"code","85e24b23":"code","4a27464f":"code","ddaee802":"code","554cb898":"code","97ecb9ae":"code","275d4c11":"code","b607c350":"code","7a19f408":"code","a5dda5c3":"code","726ed47d":"code","877a6973":"code","adaceccd":"code","2daea112":"code","670c9841":"code","27809607":"code","e4fef463":"markdown","565a4894":"markdown","ee0d17b9":"markdown","f6b11f40":"markdown","19d1b0b1":"markdown","ed5efeec":"markdown","f9a14f3a":"markdown","2d0dc060":"markdown","87cc6629":"markdown","a398032a":"markdown","93d66223":"markdown","226e488b":"markdown","c1de3325":"markdown","d5627b4f":"markdown","bbf90ac9":"markdown","958f707e":"markdown","0b78c1f8":"markdown","ad05fbf9":"markdown","c5c44a53":"markdown","934da68f":"markdown","6e0b5c65":"markdown","1520c16b":"markdown","7e6f1caf":"markdown","3f382d49":"markdown","c1ee495d":"markdown","dd60452c":"markdown","99b3a32f":"markdown","5b18f55b":"markdown","67da3c3e":"markdown","ab784e87":"markdown","be4c404b":"markdown","c27787a2":"markdown","943b84ea":"markdown","c4a769ff":"markdown","8ab7338f":"markdown","a4ac7721":"markdown","f6bd4552":"markdown","ecd703bd":"markdown","6537bba2":"markdown","5c5211b8":"markdown","2b9f4677":"markdown","02ccb6b7":"markdown","b58cd340":"markdown","0f2b7544":"markdown","9d9a02ca":"markdown","02cc62ed":"markdown"},"source":{"c5e2014a":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom scipy.stats import zscore","a5af432a":"data_set = pd.read_csv('..\/input\/water-potability\/water_potability.csv') \ndata_set","7bd28f31":"data_set.info()","6526e89c":"data_set.isnull().sum()","4d02389f":"data_set = data_set.fillna(data_set.mean())","cdbc795e":"data_set.isnull().sum()","66c522cf":"data_set = (data_set - data_set.min()) \/ (data_set.max() - data_set.min())\ndata_set","2ffd4787":"data_set.plot(kind = 'box',\n              figsize = (18,8)\n)\nplt.grid()","bf240b7b":"z_scores = zscore(data_set)\nDatos_limpios = (np.abs(z_scores) < 2.8).all(axis=1)\ndata_set = data_set[Datos_limpios]","0a539b73":"data_set.plot(kind = 'box',\n                figsize = (18,8)\n)\nplt.grid()","03960e04":"data_set = data_set.reset_index()\ndel data_set['index']\ndata_set","8a2ab023":"corr = data_set.corr()\nfig, ax = plt.subplots(figsize=(10,6)) \nsns.heatmap(corr,\n            xticklabels = True,\n            yticklabels = True,\n            annot = True,\n            ax=ax\n)\nplt.title('Correlaci\u00f3n de las variables')","a0684205":"from sklearn.model_selection import train_test_split\nfrom tensorflow.keras import datasets, layers, models, metrics, optimizers\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_validate","c5efaef0":"data_set_X = data_set.drop(['Potability'], axis = 1).values","11d4df1b":"data_set_Y = data_set['Potability'].values\ndata_set_Y","d965a2f1":"X_train, X_test, y_train, y_test = train_test_split(data_set_X, data_set_Y, test_size = 0.2, random_state = 0)","ee94fa9e":"param_GB = {'n_estimators': range(100, 1000, 100),\n            'max_depth': range(2, 6),\n            'loss': ['deviance', 'exponential']}","767808d7":"GradBoost = GradientBoostingClassifier()","85e24b23":"gsearch_GB = GridSearchCV(GradBoost, param_grid=param_GB, scoring='r2', cv=5, return_train_score=True)","4a27464f":"gsearch_GB.fit(X_train, y_train)","ddaee802":"gsearch_GB.best_params_, gsearch_GB.best_score_","554cb898":"gsearch_GB.best_estimator_.score(X_test, y_test)","97ecb9ae":"param_KNN = {'n_neighbors': range(1, 100),\n            'weights': ['uniform','distance']}","275d4c11":"KNN = KNeighborsClassifier()","b607c350":"gsearch_KNN = GridSearchCV(KNN, param_grid=param_KNN, scoring='r2', cv=5, return_train_score=True)","7a19f408":"gsearch_KNN.fit(X_train, y_train)","a5dda5c3":"gsearch_KNN.best_params_, gsearch_KNN.best_score_","726ed47d":"gsearch_KNN.best_estimator_.score(X_test, y_test)","877a6973":"X_train, X_test, y_train, y_test = train_test_split(data_set_X, data_set_Y, test_size = 0.2, random_state = 0)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.125, random_state = 0)","adaceccd":"import os\nos.environ[\"KMP_SETTINGS\"] = \"false\"","2daea112":"model = models.Sequential()\n\nmodel.add(layers.Dense(16, activation = 'relu', input_dim = 9))\nmodel.add(layers.Dense(12, activation = 'relu'))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(1, activation = 'sigmoid'))\n\nmodel.summary()","670c9841":"model.compile(optimizer = 'adam',\n              loss = 'binary_crossentropy',\n              metrics = ['accuracy'])\nepochs = 100\n\nhistory = model.fit(X_train,\n                    y_train,\n                    epochs = epochs,\n                    validation_data = (X_val,y_val),\n                    batch_size = 32)","27809607":"model.evaluate(X_test,y_test)","e4fef463":"Se limpian los outliers por el metodo Z-Scores, el cual elimina filas que contengan valores en alguna columna alejados un cierto n\u00famero de desviaciones estandar de la media de la respectiva columna.","565a4894":"Se definen los parametros de compilado y se entrena el modelo","ee0d17b9":"Se prueba el clasificador que tiene los mejores parametros con los datos de test","f6b11f40":"Se prueba el modelo con el conjunto de test","19d1b0b1":"### An\u00e1lisis de correlaci\u00f3n","ed5efeec":"Se revisa una vez mas el gr\u00e1fico de boxplot","f9a14f3a":"Se imprimen los mejores parametros encontrados junto con el promedio de scores de las 5 validaciones cruzadas hechas","2d0dc060":"Se crea el clasificador y se configura la busqueda de los hiperparametros","87cc6629":"Se importan las librerias necesarias para: \n*   Importar el dataset\n*   Analizar los datos\n*   Limpiar datos curruptos\n*   Eliminar outliers\n\n","a398032a":"Se revisa el formato de los datos que contiene cada columna","93d66223":"Se prueba el clasificador que tiene los mejores parametros con los datos de test","226e488b":"### Eliminaci\u00f3n de outliers","c1de3325":"Se realiza la division entre datos de entrenamiento y de test, asignando el 80% de datos para entrenamiento y el 20% para testear los modelos","d5627b4f":"Se imprimen los mejores parametros encontrados junto con el promedio de scores de las 5 validaciones cruzadas hechas","bbf90ac9":"\n\n<p align=justify>Una red neuronal es un modelo simplificado que intenta emular el funcionamiento de un cerebro humano. Generalmente son organizadas en capas, teniendo la capa de entrada, las capas ocultas y la capa de salida. Estas capas estan conectadas entre si y sus conexiones tienen pesos asociados, los cuales se autoajustan cuando la red realiza predicciones incorrectas para un registro al momento en que esta es entrenada.<\/p>","958f707e":"Se extraen las columnas que se usar\u00e1n para predecir la potabilidad del agua","0b78c1f8":"### Importar datos","ad05fbf9":"Se crea el modelo de la red neuronal","c5c44a53":"Se realizan las particiones de datos en 70% para entrenamiento, 20% para test y 10% para validaci\u00f3n.","934da68f":"Se imprime el data set una vez ya limpio","6e0b5c65":"Se calcula la correlaci\u00f3n de las variables entre cada una de ellas y se grafican los resultados en la siguiente tabla","1520c16b":"### Limpieza de datos corruptos","7e6f1caf":"### Algoritmo no visto en clases (KNN)","3f382d49":"Se revisan las variables del dataset en busca de outliers, para esto se utiliza el grafico boxplot de cada columna del data set","c1ee495d":"Se vuelve a revisar por valores NaN en el data set","dd60452c":"Se entrena el modelo con los datos de entrenamiento","99b3a32f":"Se limpian los valores NaN, reemplazandolos con el valor medio de su respectiva columna.","5b18f55b":"### Algoritmo visto en clases (Gradient Boosting)","67da3c3e":"Se crea el clasificador y se configura la busqueda de los hiperparametros\n","ab784e87":"### Divisi\u00f3n de datos","be4c404b":"Se genera el rango de variaci\u00f3n de los parametros para Gradient Boosting","c27787a2":"Se normalizan los datos con el metodo Min-Max","943b84ea":"Se importa el dataset al entorno de trabajo","c4a769ff":"## Algoritmos","8ab7338f":"Gradient Boosting consiste en algoritmos d\u00e9biles (arboles de decisi\u00f3n), que por si solos no entregan buenos resultados, pero que en forma conjunta constituyen un algoritmo fuerte y robusto capaz de resolver problemas de regresi\u00f3n y clasificaci\u00f3n.","a4ac7721":"### Red Neuronal","f6bd4552":"Se extrae la columna que indica la potabilidad del agua, la cual seria la columna a predecir","ecd703bd":"Los resultados obtenidos con los 3 algoritmos no muestran buenas predicciones, rondando entre el 60% y 70% de accuracy. \nComparando con otros trabajos realizados en Kaggle vemos que todos los trabajos rondan los mismos porcentajes de predicci\u00f3n. Uno en particular logra sobresalir con un 76%, debido que hizo uso de una t\u00e9cnica adicional. Observ\u00f3 que hab\u00eda un desbalance en los datos, esto es que hab\u00eda muchas m\u00e1s muestras de agua no potable que potable, por lo que hizo un sobre-muestreo mediante Synthetic Minority Oversampling Technique (SMOT) para afectar y mejorar el rendimiento. Finalizando, una hip\u00f3tesis que se cree es que a pesar que la correlaci\u00f3n entre los predictores es baja, no entregan la suficiente informaci\u00f3n, que sea relevante, para poder pronosticar si el agua es potable o no, obteni\u00e9ndose resultados menores al 80% con la mayor\u00eda de los clasificadores.\n","6537bba2":"# Proyecto 2: An\u00e1lisis de algoritmos de machine learning\n\n\n*   Hern\u00e1n Mangas\n*   Brayan Figueroa\n","5c5211b8":"## Pre-procesamiento de datos","2b9f4677":"Se importan las librerias necesarias para:\n*   Dividir los datos en conjuntos de entrenamiento y test\n*   Entrenar y utilizar **Redes Neuronales**\n*   Entrenar y utilizar el algoritmo **Gradient Boosting**\n*   Entrenar y utilizar el algoritmo **KNN**","02ccb6b7":"Se entrena el modelo con los datos de entrenamiento","b58cd340":"## Comentarios y Conclusiones Finales","0f2b7544":"Se revisa por el n\u00famero de valores NaN en cada columna","9d9a02ca":"* Tomando en cuenta la tabla, los valores de las correlaciones estan cercanos a 0, por lo tanto todas las variables aportan informaci\u00f3n y se tomaran en cuenta para el entrenamiento de los algoritmos.","02cc62ed":"K-Nearest Neighbor (KNN) consiste en realizar una predicci\u00f3n de acuerdo a los \u201ck\u201d datos m\u00e1s cercanos y clasificar el punto de inter\u00e9s seg\u00fan la mayor cantidad de datos que le rodean."}}