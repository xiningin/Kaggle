{"cell_type":{"eb2e09ff":"code","acfb37c4":"code","7ac38b0f":"code","4bf61db8":"code","748d7dd2":"code","a895d7f9":"code","799366b7":"code","c978e8d6":"code","889f282c":"code","54a79f95":"code","73e10cda":"markdown","60d5d64a":"markdown","5dc047a2":"markdown","9b89f9fd":"markdown","d31bbe07":"markdown","91c4c821":"markdown"},"source":{"eb2e09ff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","acfb37c4":"input_files = ['\/kaggle\/input\/us-election-2020-presidential-debates\/us_election_2020_vice_presidential_debate.csv',\n              '\/kaggle\/input\/us-election-2020-presidential-debates\/us_election_2020_biden_town_hall.csv',\n              '\/kaggle\/input\/us-election-2020-presidential-debates\/us_election_2020_2nd_presidential_debate.csv',\n              '\/kaggle\/input\/us-election-2020-presidential-debates\/us_election_2020_1st_presidential_debate.csv',\n              '\/kaggle\/input\/us-election-2020-presidential-debates\/us_election_2020_trump_town_hall.csv']\ndfs = [pd.read_csv(file) for file in input_files]\ndf = pd.concat(dfs)\ndf","7ac38b0f":"#t is a function that takes a series of iterables and returns one iterable. It groups all the iterables together and produces a single iterable as output.\nfrom itertools import chain\nprint(df['speaker'].unique())\n# seperating trump speech from Biden speech\ntrump_df = df[df['speaker'].str.contains('Trump')]\nbiden_df = df[df['speaker'].str.contains('Biden')]\n\ntrump_text_df = trump_df.drop(['speaker', 'minute'], axis=1)\nbiden_text_df = biden_df.drop(['speaker', 'minute'], axis=1)\n\ntrump_text = list(chain(*trump_text_df.astype(str).values.tolist()))\nbiden_text = list(chain(*biden_text_df.astype(str).values.tolist()))\n\nprint(trump_text[::100])\nprint(biden_text[::100])","4bf61db8":"from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(trump_text + biden_text)\nvocab_size = len(tokenizer.word_index) + 1\nprint('vocab_size :' ,vocab_size)\n\n","748d7dd2":"from nltk import sent_tokenize\nimport tensorflow\nfrom tensorflow import keras\nfrom tensorflow.keras.utils import to_categorical\nmax_sentence_length = 4\n\ndef get_sequences(text):\n    input_sequence = list()\n    output_sequence = list()\n    slice_size = max_sentence_length + 1\n    for s in text:\n        s = s.lower()\n        sentences = sent_tokenize(s)\n        seq = tokenizer.texts_to_sequences(sentences)\n        #print(seq)\n        for s in seq:\n            tokens = [s[i:i+slice_size] for i in range(len(s) - slice_size + 1)]\n            for t in tokens:\n                if t:\n                    input_sequence.append(t[:-1])\n                    output_sequence.append(t[-1])\n    return np.array(input_sequence), to_categorical(output_sequence, num_classes=vocab_size)\n            ","a895d7f9":"trump_input , trump_output =  get_sequences(trump_text)\nbiden_input , biden_output =  get_sequences(biden_text)\n\n#print(trump_input[1],(biden_output[1]))\nprint(len(trump_input),len(biden_input))\ntrump_output\n","799366b7":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense,LSTM,Embedding , Dropout,Bidirectional\n\nearly_stopping = tensorflow.keras.callbacks.EarlyStopping(monitor ='accuracy',patience = 5,restore_best_weights=True)\ndef train_model(input_data,output_data,epochs):\n    model = Sequential([\n        Embedding(vocab_size,50,input_length = max_sentence_length),\n#        Bidirectional(LSTM(150,return_sequences=True)),\n#       Dropout(0.3),\n        Bidirectional(LSTM(150,return_sequences=True)),\n        Dropout(0.3),\n        Bidirectional(LSTM(100)),\n        Dropout(0.2),\n        Dense(100,activation = \"relu\"),\n        Dense(vocab_size,activation=\"sigmoid\")\n    ])\n    model.compile(loss=\"categorical_crossentropy\",\n                 optimizer = 'adam',\n                 metrics=['accuracy'])\n    model.fit(input_data,output_data,batch_size=64,callbacks = early_stopping,epochs=epochs)\n    return model\n","c978e8d6":"trump_model = train_model(trump_input,trump_output,150)","889f282c":"def predict(model,seed):\n    words = seed.split(' ')\n    for i in range(4):\n        input_vectors = tokenizer.texts_to_sequences([' '.join(words)])\n        pred = model.predict(input_vectors)\n        y = np.argmax(pred,axis=1)\n        for key ,value in tokenizer.word_index.items():\n            if y == value:\n                word = key\n                break\n        print(word,end=\" \")\n        words = words[1:]\n        words.append(word)\n# Range(n)    n will decide the next number of words to be predicted","54a79f95":"predict(trump_model,'We have the greatest')","73e10cda":"# Tokenizing","60d5d64a":"# Model Creation","5dc047a2":"# Predicting the next word","9b89f9fd":"**What if you needed to tokenize a sentence provided in a string and then put it in according to the required training data ? \\\nFor That visit https:\/\/www.kaggle.com\/sagarsethi3598\/jack-ma-next-word**","d31bbe07":"# Creating the dataframe","91c4c821":"**In this notebook I've used 2020 election president candidates speeches**"}}