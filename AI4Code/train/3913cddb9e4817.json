{"cell_type":{"0808a812":"code","1a8bc42e":"code","2e99f3f6":"code","92f4b0c0":"code","19e113a1":"code","294b3fd5":"code","a4fae89b":"code","4c3b24a3":"code","af90499c":"code","da863788":"code","c41ec6f1":"code","962d22a3":"code","d33edac4":"code","0a800c08":"code","ce9cd210":"code","c68b7b6b":"code","8fa3a923":"code","787fb2cf":"code","0b08c8c1":"code","1fb069d7":"code","5814400a":"code","b80207a4":"code","c04d79a1":"code","99fd5114":"code","2f315d60":"code","63999ea1":"code","54d6acc6":"code","b8d3364d":"code","6bc697e4":"code","53247128":"code","2690a9da":"code","89b28fad":"code","07d32d88":"code","b4c428a9":"code","7dd687c5":"code","134edd9f":"code","4df5997d":"code","8c3755fa":"code","3efba260":"code","af7d2b12":"code","31f673d7":"code","3bc1f107":"code","9b03c720":"code","a1f36e1a":"code","0e370def":"code","090c621b":"code","2e2429b3":"code","71026008":"code","ac43d0be":"code","366e1d07":"markdown","c9b54450":"markdown","0b280439":"markdown","d98d996d":"markdown","a1de2bf2":"markdown","01c82711":"markdown","51819899":"markdown","ef968c50":"markdown","258988cb":"markdown","13385de0":"markdown","ea9bed90":"markdown","a81cb2b4":"markdown","73e5844c":"markdown","88462d22":"markdown","9db1f07e":"markdown","fec63b9a":"markdown","57870db3":"markdown","f1a26d88":"markdown","ee8f08b2":"markdown","b075fc4b":"markdown","86f9566f":"markdown","c35acd14":"markdown","39a22af5":"markdown","5ff857cd":"markdown","eb3b708e":"markdown","9370fd81":"markdown","bf9252fd":"markdown","b7e3fff0":"markdown","28baf7e4":"markdown"},"source":{"0808a812":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.datasets import make_hastie_10_2\nimport os","1a8bc42e":"from sklearn.preprocessing import StandardScaler as ss","2e99f3f6":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import auc, roc_curve\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.model_selection import train_test_split","92f4b0c0":"from sklearn.decomposition import PCA","19e113a1":"os.chdir(\"..\/input\")\nos.listdir()","294b3fd5":"data = pd.read_csv(\"..\/input\/data.csv\")","a4fae89b":"data.shape","4c3b24a3":"data.columns","af90499c":"data.dtypes","da863788":"data.head()","c41ec6f1":"data.shape","962d22a3":"df=data.drop(['id','Unnamed: 32'],axis=1)","d33edac4":"df.shape","0a800c08":"df.head()","ce9cd210":"X = df.loc[: , 'radius_mean':'fractal_dimension_worst']\ny = df.loc[:, 'diagnosis']","c68b7b6b":"df['diagnosis'].replace('M',1,inplace=True)\ndf['diagnosis'].replace('B',0,inplace=True)","8fa3a923":"scale = ss()\nX = scale.fit_transform(X)","787fb2cf":"pca = PCA()\nout = pca.fit_transform(X)\nout.shape ","0b08c8c1":"pca.explained_variance_ratio_","1fb069d7":"pca.explained_variance_ratio_.cumsum()","5814400a":"final_data = out[:, :10]","b80207a4":"final_data.shape","c04d79a1":"final_data[:5,:]","99fd5114":"pcdf = pd.DataFrame( data =  final_data,\n                    columns = ['pc1', 'pc2','pc3', 'pc4','pc5','pc6','pc7','pc8','pc9','pc10'])","2f315d60":"pcdf['target'] = data['diagnosis'].map({'M': 1, \"B\" : 0 })","63999ea1":"pcdf.head()","54d6acc6":"X = pcdf.loc[: , 'pc1':'pc10']\ny = pcdf.loc[:,'target']","b8d3364d":"X.head()","6bc697e4":"y.head()","53247128":"X_train, X_test, y_train, y_test = train_test_split(X ,y, test_size = 0.2,shuffle=True)","2690a9da":"X_train.shape","89b28fad":"X_test.shape","07d32d88":"y_train.shape","b4c428a9":"y_test.shape","7dd687c5":"dt = DecisionTreeClassifier()\nrf = RandomForestClassifier(n_estimators=100)\nxg = XGBClassifier(learning_rate=0.5,\n                   reg_alpha= 5,\n                   reg_lambda= 0.1\n                   )\ngbm = GradientBoostingClassifier()\net = ExtraTreesClassifier()\nknn = KNeighborsClassifier()","134edd9f":"dt1 = dt.fit(X_train,y_train)\nrf1 = rf.fit(X_train,y_train)\nxg1 = xg.fit(X_train,y_train)\ngbm1 = gbm.fit(X_train,y_train)\net1 = et.fit(X_train,y_train)\nknn1 = knn.fit(X_train,y_train)","4df5997d":"y_pred_dt = dt1.predict(X_test)\ny_pred_rf = rf1.predict(X_test)\ny_pred_xg= xg1.predict(X_test)\ny_pred_gbm= gbm1.predict(X_test)\ny_pred_et= et1.predict(X_test)\ny_pred_knn= knn1.predict(X_test)\ny_pred_dt","8c3755fa":"y_pred_dt_prob = dt1.predict_proba(X_test)\ny_pred_rf_prob = rf1.predict_proba(X_test)\ny_pred_xg_prob = xg1.predict_proba(X_test)\ny_pred_gbm_prob= gbm1.predict_proba(X_test)\ny_pred_et_prob= et1.predict_proba(X_test)\ny_pred_knn_prob= knn1.predict_proba(X_test)","3efba260":"print (accuracy_score(y_test,y_pred_dt))\nprint (accuracy_score(y_test,y_pred_rf))\nprint (accuracy_score(y_test,y_pred_xg))\nprint (accuracy_score(y_test,y_pred_gbm))\nprint (accuracy_score(y_test,y_pred_et))\nprint (accuracy_score(y_test,y_pred_knn))","af7d2b12":"confusion_matrix(y_test,y_pred_dt)\nconfusion_matrix(y_test,y_pred_rf)\nconfusion_matrix(y_test,y_pred_xg)\nconfusion_matrix(y_test,y_pred_gbm)\nconfusion_matrix(y_test,y_pred_et)\nconfusion_matrix(y_test,y_pred_knn)\ntn,fp,fn,tp= confusion_matrix(y_test,y_pred_dt).flatten()","31f673d7":"fpr_dt, tpr_dt, thresholds = roc_curve(y_test,\n                                 y_pred_dt_prob[: , 1],\n                                 pos_label= 1\n                                 )","3bc1f107":"fpr_rf, tpr_rf, thresholds = roc_curve(y_test,\n                                 y_pred_rf_prob[: , 1],\n                                 pos_label= 1\n                                 )","9b03c720":"fpr_xg, tpr_xg, thresholds = roc_curve(y_test,\n                                 y_pred_xg_prob[: , 1],\n                                 pos_label= 1\n                                 )","a1f36e1a":"fpr_gbm, tpr_gbm,thresholds = roc_curve(y_test,\n                                 y_pred_gbm_prob[: , 1],\n                                 pos_label= 1\n                                 )","0e370def":"fpr_et, tpr_et,thresholds = roc_curve(y_test,\n                                 y_pred_et_prob[: , 1],\n                                 pos_label= 1\n                                 )","090c621b":"fpr_knn, tpr_knn,thresholds = roc_curve(y_test,\n                                 y_pred_knn_prob[: , 1],\n                                 pos_label= 1\n                                 )","2e2429b3":"p_dt,r_dt,f_dt,_ = precision_recall_fscore_support(y_test,y_pred_dt)\np_rf,r_rf,f_rf,_ = precision_recall_fscore_support(y_test,y_pred_rf)\np_gbm,r_gbm,f_gbm,_ = precision_recall_fscore_support(y_test,y_pred_gbm)\np_xg,r_xg,f_xg,_ = precision_recall_fscore_support(y_test,y_pred_xg)\np_et,r_et,f_et,_ = precision_recall_fscore_support(y_test,y_pred_et)\np_knn,r_knn,f_knn,_ = precision_recall_fscore_support(y_test,y_pred_knn)\np_dt,r_dt,f_dt","71026008":"print (auc(fpr_dt,tpr_dt))\nprint (auc(fpr_rf,tpr_rf))\nprint (auc(fpr_gbm,tpr_gbm))\nprint (auc(fpr_xg,tpr_xg))\nprint (auc(fpr_et,tpr_et))\nprint (auc(fpr_knn,tpr_knn))","ac43d0be":"fig = plt.figure(figsize=(12,10))          # Create window frame\nax = fig.add_subplot(111)   # Create axes\n# 9.2 Also connect diagonals\nax.plot([0, 1], [0, 1], ls=\"--\")   # Dashed diagonal line\n# 9.3 Labels etc\nax.set_xlabel('False Positive Rate')  # Final plot decorations\nax.set_ylabel('True Positive Rate')\nax.set_title('ROC curve for multiple models')\n# 9.4 Set graph limits\nax.set_xlim([0.0, 1.0])\nax.set_ylim([0.0, 1.0])\n\n# 9.5 Plot each graph now\nax.plot(fpr_dt, tpr_dt, label = \"dt\")\nax.plot(fpr_rf, tpr_rf, label = \"rf\")\nax.plot(fpr_xg, tpr_xg, label = \"xg\")\nax.plot(fpr_gbm, tpr_gbm, label = \"gbm\")\nax.plot(fpr_et, tpr_et, label = \"et\")\nax.plot(fpr_knn, tpr_knn, label = \"knn\")\n# 9.6 Set legend and show plot\nax.legend(loc=\"lower right\")\nplt.show()","366e1d07":"#### Calculate the cumulative sum of each column, by this we can decide that how many PCs (Principal components) we need to consider to get desired variance. As I told before in this case we are considering the .95 variance, so we can take first 10 columns as PCs.","c9b54450":"#### Calculate the variance of each columns in the predictors.","0b280439":"#### Calculate the Precision, Recall and F1 Score","d98d996d":"#### Simple Exploration of data","a1de2bf2":"#### Get the file and read it","01c82711":"#### Prepare the confusion matrix. This uses the y_test and y_pred_dt. y_test is having actual values and y_pred_dt is having predicted values. Once confusion matrix prepared, we will come to know TP,TN,FP,FN values.","51819899":"# Attribute Information:\n1) ID number                                   \n2) Diagnosis (M = malignant, B = benign)       \n\nAll attributes are real values. Some of the attributes are as follows:\n\n    a) radius (mean of distances from center to points on the perimeter) \n    b) texture (standard deviation of gray-scale values)\n    c) perimeter\n    d) area\n    e) smoothness (local variation in radius lengths)\n    f) compactness (perimeter^2 \/ area - 1.0)\n    g) concavity (severity of concave portions of the contour)\n    h) concave points (number of concave portions of the contour)\n    i) symmetry\n    j) fractal dimension (\"coastline approximation\" - 1)\n\nPerform modeling using six algorithms (click to read its documentation in sklearn):\n\n           i) Decision Trees,\n           ii) Random Forest,\n           iii) ExtraTreesClassifier,\n           iv) Gradient Boosting Machine\n           v) XGBoost, and\n           vi) KNeighborsClassifier \nIn this assignment, we will be doing the following activities.\n\n        i)  Read dataset. Check if any column has any missing variable.\n        ii) Drop any column not needed (ID column, for example)\n        iii)Segregate dataset into predictors (X) and target (y)\n        iv) Map values in ' y ' (target) from 'M' and 'B' to 1 and 0\n        v)  Scale all numerical features in X  using sklearn's StandardScaler class\n        vi) Perform PCA on numeric features, X. Only retain as many PCs, to explain 95% of the variance   \n        vii)Use PCs from (vi) as your explanatory variables. This is our new X.\n        viii)Split X,y into train and test datasets in the ratio of 80:20.\n        ix) Perform modeling on (X_train,y_train) using above listed algorithms (six).\n        x) Make predictions on test (X_test) for each one of the models. Compare the output of predictions \n        in each case with actual (y_test)\n        xi) Compare the performance of each of these models by calculating metrics::\n             a) accuracy,\n             b) Precision & Recall,\n             c) F1 score,\n             d) AUC\n        xii) Also draw ROC curve for each","ef968c50":"#### Scale all numerical features in X  using sklearn's StandardScaler class","258988cb":"#### Import StandardScaler function to scale the numerical features.","13385de0":"#### Assign the first 10 columns of the 'out' to final_data. 'out' has the fit and transformed values after we performed the PCA of feature columns.","ea9bed90":"#### Calculate the accuracy score using the sklearn's accuracy_score function. accuracy score is calcuated by y_test,y_pred_dt. y_test is having actual values and y_pred_dt is having predicted values. This is nothingbut how accurate the model is predicting.","a81cb2b4":"#### Below is the plotting of ROC curve for all the models, we can see that KNN is having more AUC and we can say that KNN is better in this case.","73e5844c":"##### Droping the column id and Unnamed: 32 (null column) as these won't play any role while predicting the breast camcer.","88462d22":"#### Calculate the AUC(Area Under the ROC Curve). More the AUC, more the better model.","9db1f07e":"#### As this is our new file, we need to separate the X(Predictors) and y(target) . These 10 PCs will be our new predictors.","fec63b9a":"#### Import PCA from skelarn","57870db3":"##### We need to create the new Dataframe with 10 PCs as predictors and with target column. This will be our new data file. As showed below we are creating the dataframe with values in the final_data and with column names as PC1 through PC10. And 'target' as target column.","f1a26d88":"#### Once the training the model is done, Make the prediction on the test data. ","ee8f08b2":"#### ROC Graph","b075fc4b":"#### Import the metrics from sklearn","86f9566f":"#### As I told in the begining, here we are using multiple classifier models for prediction. We are creating the default classifier. ","c35acd14":"### Import the required libraries and classifiers from the sklearn. ","39a22af5":"# Predicting breast cancer using PCA and Multiple classification models.","5ff857cd":"#### Train the data using data in the X_train and y_train.","eb3b708e":"#### Performing PCA on numeric columns. \n###### PCA is helpfull to reduce the dimentionality of the feature columns. As of now we have 30 features in the data, so we need to reduce the number of feature columns, at the same time we need to take care about the variance in data also. In this case we are considering the variance as .95. PCA transforms the the existing set of features into new set of features(reduced in number) and these are called Principal Componants. \n","9370fd81":"#### Once we get the final predictors and target, we need to split the data into train and test data. For this we can use sklearn's train_test_split function. Here I am using test_size = 0.2, that means I am using 20% of the data as test data and other 80% data for training the model. shuffle=True make sure the data are shuffled before the split, so that random data will go into train and test splits.","bf9252fd":"#### Calculate the prediction probability of the models.","b7e3fff0":"#### Separate the predictors and target. Here our aim to predict whether a breast cancer cell is benign or malignant. So the column 'diagnosis' is the target and rest other 30 columns are act as predictors(features).","28baf7e4":"#### Map the values in target column with 1 and 0, here we are mapping M as 1 and B as 0. "}}