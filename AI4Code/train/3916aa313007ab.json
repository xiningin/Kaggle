{"cell_type":{"721593f6":"code","273e1f38":"code","a9d5f67b":"code","5f1743a2":"code","b7e36db5":"code","b14c25a9":"code","aab11e6c":"code","cf82a5df":"code","2852efa4":"code","bfac55e6":"code","639f9303":"code","2647d880":"code","33d68024":"code","9bdd2b2b":"code","0de02356":"code","14421a48":"code","3e509e84":"code","6eb40c76":"code","b9f6c18a":"code","77404edf":"code","21a95871":"code","2529c0bf":"code","c72affac":"code","1ba92345":"code","b56fd258":"code","0b032df6":"code","d896bb75":"code","855bff60":"code","4ee2d927":"code","35633366":"code","7a16f0e0":"code","e7ed5b33":"code","8b55f41c":"code","ff9e2481":"code","2d0730ca":"code","a1f659dd":"code","ec89b33b":"code","1c42d276":"code","46db4535":"code","fd24dd23":"code","8c9a8efa":"code","cdc1fb63":"code","65a5f223":"code","45ebca8c":"code","5cbdb595":"code","69fcb59c":"code","fcda51a5":"code","3c097497":"code","6f85999e":"code","9a97d238":"code","494e6a8e":"code","7f85e377":"code","41f40fc3":"code","5a0904d1":"code","b1400a22":"code","54e07a6f":"code","7a335326":"code","228b99b2":"code","af500ce9":"code","a3619e62":"code","1831eb1d":"code","a178885f":"code","e0985c98":"code","ffbedb5b":"code","91d35680":"code","7f09fc1e":"code","dcde8c71":"code","58a73778":"code","d700fb5d":"code","5070ae80":"code","dbfcf34f":"code","5e69e1cb":"code","67f5b401":"code","8e42c96d":"code","a923aa1f":"code","55aa3a63":"code","0c916502":"code","9ec89e78":"code","46de2c77":"code","99efb6f3":"code","837384dd":"code","c0ce2fee":"code","8260f8ff":"code","faa05d15":"code","65632f0b":"code","4d485971":"code","81c9ce11":"code","3c6653a2":"code","61bb6b55":"code","7d073a10":"code","3c15c2ea":"code","28311edd":"code","cef4830a":"code","0d56f9fc":"code","80a6718a":"code","0ebc7d31":"code","5ff33c2c":"code","1a9af7af":"code","12d705ec":"code","d3e5c327":"markdown","6f77496e":"markdown","87b0b8da":"markdown","8ab881e9":"markdown","2b84692d":"markdown","42837f20":"markdown","e42cfbbf":"markdown","705f368b":"markdown","9f4f045e":"markdown","1936567e":"markdown","8885a03e":"markdown","18db94c1":"markdown","bce77835":"markdown","77a05f7a":"markdown","195e70e5":"markdown","7ebe8300":"markdown","b4838dda":"markdown","cbfc5899":"markdown","76217835":"markdown","9130a843":"markdown","ce2e78ed":"markdown","b926348c":"markdown","20f44156":"markdown","01093afb":"markdown","19f1f135":"markdown","cd281461":"markdown","96f913e8":"markdown","9e061c61":"markdown","aae9f678":"markdown","e4b1d794":"markdown","60255a73":"markdown","536015b9":"markdown","b1a54699":"markdown","b2a0ec69":"markdown","a7b89214":"markdown","18015d9f":"markdown","a1dc26bc":"markdown","29b0853e":"markdown"},"source":{"721593f6":"# Check the versions of libraries\n\n# Python version\nimport sys\nprint('Python: {}'.format(sys.version))\n# scipy\nimport scipy\nprint('scipy: {}'.format(scipy.__version__))\n# numpy\nimport numpy\nprint('numpy: {}'.format(numpy.__version__))\n# matplotlib\nimport matplotlib\nprint('matplotlib: {}'.format(matplotlib.__version__))\n# pandas\nimport pandas\nprint('pandas: {}'.format(pandas.__version__))\n# scikit-learn\nimport sklearn\nprint('sklearn: {}'.format(sklearn.__version__))","273e1f38":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n## Load metrics for predictive modeling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, auc","a9d5f67b":"# Load dataset train and test\ntrain_titanic = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_titanic = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# Create titanis list with both dataset to use same cleansing methods\ntitanic_list = [train_titanic, test_titanic]","5f1743a2":"# Check dataframe structure\nfor information in titanic_list:\n    information.info()\n    print('_'*40)","b7e36db5":"# Check dataframe basic stats data\nfor stats in titanic_list:\n    print(stats)\n    print('_'*40)","b14c25a9":"# Check test dataframe basic stats data\nfor descrip in titanic_list:\n    print(descrip.describe())\n    print('_'*40)","aab11e6c":"# Check null and NA values for both dataset\nfor nuls in titanic_list:\n    print(nuls.isna().sum())\n    print('_'*40)","cf82a5df":"# Table of relative frequency\nfor nuls in titanic_list:\n    print(nuls.isnull().sum()\/len(nuls)*100)\n    print('_'*40)","2852efa4":"# Check first 10 elements\nfor passenger in titanic_list:\n    print(passenger['PassengerId'].head(10))\n    print('_'*40)","bfac55e6":"# Remove PassengerId variable only for train dataset\ntitanic_list[0].drop(['PassengerId'], axis=1, inplace=True)","639f9303":"# Check train dataset\ntitanic_list[0].head()","2647d880":"sns.barplot(x=\"Survived\", data=titanic_list[0])","33d68024":"titanic_list[0].describe()['Survived']","9bdd2b2b":"titanic_list[0][['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean()","0de02356":"sns.barplot(x=\"Pclass\", y=\"Survived\", data=titanic_list[0])","14421a48":"# Check the survived ratio with sex\ntitanic_list[0][[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean()","3e509e84":"sns.barplot(x=\"Sex\", y=\"Survived\", data=titanic_list[0])","6eb40c76":"# Convert categorical variable to binary variable - female 1 and male 0\nfor genre in titanic_list:\n    genre['Sex'] = genre['Sex'].map( {'female': 1, 'male': 0} ).astype(int)","b9f6c18a":"# Check Sex features\ntitanic_list[0].head()","77404edf":"titanic_list[1].head()","21a95871":"titanic_list[0][[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean()","2529c0bf":"sns.barplot(x=\"SibSp\", y=\"Survived\", data=titanic_list[0])","c72affac":"titanic_list[0][[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean()","1ba92345":"sns.barplot(x=\"Parch\", y=\"Survived\", data=titanic_list[0])","b56fd258":"# Create a new feature\nfor famsize in titanic_list:\n    famsize['FamilySize'] = famsize['SibSp'] + famsize['Parch'] + 1","0b032df6":"titanic_list[0][[\"FamilySize\", \"Survived\"]].groupby(['FamilySize'], as_index=False).mean()","d896bb75":"sns.barplot(x=\"FamilySize\", y=\"Survived\", data=titanic_list[0])","855bff60":"for alone in titanic_list:\n    alone['IsAlone'] = 0\n    alone.loc[alone['FamilySize'] == 1, 'IsAlone'] = 1\n\n# Check new feature with predictor    \ntitanic_list[0][['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","4ee2d927":"sns.barplot(x=\"IsAlone\", y=\"Survived\", data=titanic_list[0])","35633366":"# Check new features in dataset train\ntitanic_list[1].head()","7a16f0e0":"# We remove Ticket variable in both traing and test dataset\nfor tick in titanic_list:\n    tick.drop(['Ticket'], axis=1, inplace=True)","e7ed5b33":"# We check the dataset again - train\ntitanic_list[0].head(10)","8b55f41c":"# ...and test dataset\ntitanic_list[1].head(10)","ff9e2481":"# Check ratio Embarked and Survived variable\ntitanic_list[0][['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean()","2d0730ca":"# Fill na or null values with the most frequent value, C\nfreq_port = titanic_list[0].Embarked.dropna().mode()[0]\nfreq_port","a1f659dd":"# Assign result on the dataset\nfor port in titanic_list:\n    port['Embarked'] = port['Embarked'].fillna(freq_port)","ec89b33b":"sns.barplot(x=\"Embarked\", y=\"Survived\", data=titanic_list[0])","1c42d276":"sns.distplot(titanic_list[0]['Fare'], fit=norm)","46db4535":"for f in titanic_list:\n    f['Fare'] = np.log1p(f['Fare'])\nsns.distplot(titanic_list[0]['Fare'], fit=norm)","fd24dd23":"for faregr in titanic_list:\n    faregr['FareGroup'] = pd.qcut(faregr['Fare'], 7, labels=['A', 'B', 'C', 'D', 'E', 'F', 'G'])\n\n\ntitanic_list[0][['FareGroup', 'Survived']].groupby(['FareGroup'], as_index=False).mean()","8c9a8efa":"sns.barplot(x=\"FareGroup\", y=\"Survived\", data=titanic_list[0])","cdc1fb63":"# We remove the variable Fare\nfor fares in titanic_list:\n    fares.drop(['Fare'], axis=1, inplace=True)","65a5f223":"for cab in titanic_list:\n    cab['InCabin'] = ~cab['Cabin'].isnull()","45ebca8c":"sns.barplot(x=\"InCabin\", y=\"Survived\", data=titanic_list[0])\nplt.show()","5cbdb595":"# We remove the variable Cabin\nfor cabin in titanic_list:\n    cabin.drop(['Cabin'], axis=1, inplace=True)","69fcb59c":"bins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]\nlabels = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\nfor age in titanic_list:\n    age[\"Age\"] = age[\"Age\"].fillna(-0.5)\n    age['AgeGroup'] = pd.cut(age[\"Age\"], bins, labels = labels)","fcda51a5":"sns.barplot(x=\"AgeGroup\", y=\"Survived\", data=titanic_list[0])\nplt.show()","3c097497":"# We remove the variable Age\n#for a in titanic_list:\n#    a.drop(['Age'], axis=1, inplace=True)","6f85999e":"# Check the names\ntitanic_list[0]['Name'].head(10)","9a97d238":"# Create the function to extract the title\nimport re\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\n# Apply get_title function\nfor title in titanic_list:\n    title['Title'] = title['Name'].apply(get_title)\n\n# Check the results\npd.crosstab(titanic_list[0]['Title'], titanic_list[0]['Sex'])","494e6a8e":"# Create a categorization on train dataset\nfor t in titanic_list:\n    t['Title'] = t['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    t['Title'] = t['Title'].replace('Mlle', 'Miss')\n    t['Title'] = t['Title'].replace('Ms', 'Miss')\n    t['Title'] = t['Title'].replace('Mme', 'Mrs')\n\n# We create a relative table\ntitanic_list[0][['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","7f85e377":"sns.barplot(x=\"Title\", y=\"Survived\", data=titanic_list[0])\nplt.show()","41f40fc3":"# Remove Name variable\nfor name in titanic_list:\n    name.drop(['Name'], axis=1, inplace=True)","5a0904d1":"# Check all values and new features\ntitanic_list[0].head(10)","b1400a22":"titanic_list[1].head(10)","54e07a6f":"correlation_matrix = titanic_list[0].corr()\ncorrelation_matrix\n\nplt.figure(figsize=(10,10))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(correlation_matrix, annot=True)","7a335326":"# The procedure is very simple, binarizing categorical variable for training dataset\ncols = ['Pclass', 'Embarked', 'FareGroup', 'AgeGroup', 'Title']\ntitanic_categorical = titanic_list[0][cols]\ntitanic_categorical = pd.concat([pd.get_dummies(titanic_categorical[col], prefix=col) for col in titanic_categorical], axis=1)\ntitanic_categorical.head()\ntrain_titanic = pd.concat([titanic_list[0][titanic_list[0].columns[~titanic_list[0].columns.isin(cols)]], titanic_categorical], axis=1)\ntrain_titanic.head()","228b99b2":"correlation_matrix = train_titanic.corr()\ncorrelation_matrix\n\n#plt.figure(figsize=(10,10))\nfig, ax = plt.subplots(figsize=(20, 10))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(correlation_matrix, annot=True)","af500ce9":"# Binarizing variable for testing dataset\ntitanic_categorical = titanic_list[1][cols]\ntitanic_categorical = pd.concat([pd.get_dummies(titanic_categorical[col], prefix=col) for col in titanic_categorical], axis=1)\ntest_titanic = pd.concat([titanic_list[1][titanic_list[1].columns[~titanic_list[1].columns.isin(cols)]], titanic_categorical], axis=1)\ntest_titanic.head()","a3619e62":"# Backup train and test dataset\ntrain_bak = train_titanic\ntest_bak = test_titanic","1831eb1d":"# Version 1 \n# Modeling with only high correlation features - Drop SibSb - Parch\nfor feature in train_titanic, test_titanic:\n    feature.drop(['SibSp'], axis=1, inplace=True)\n    feature.drop(['Parch'], axis=1, inplace=True)","a178885f":"train_titanic.head(10)","e0985c98":"test_titanic.head(10)","ffbedb5b":"# Split and drop Survived variable\nX_train = train_titanic.drop('Survived', axis=1)\ny_train = train_titanic['Survived']\n\n# Drop PassengerId variable on test dataset\nids = test_titanic[['PassengerId']] # create a sub-dataset for submission file and saving it \ntest_titanic = test_titanic.drop('PassengerId', axis=1).copy()\n\n# Create train and test 80-20 with seed fixed to 42 for validation the model\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.20, random_state=42)","91d35680":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","7f09fc1e":"X_train.head()","dcde8c71":"y_train.head()","58a73778":"X_test.head()","d700fb5d":"y_test.head()","5070ae80":"# Create a performance_auc dict\nperformance_auc = {}","dbfcf34f":"model = LogisticRegression().fit(X_train, y_train)\nmodel","5e69e1cb":"predicted_log = model.predict(X_test)\npredicted_log","67f5b401":"# Confidence score\nlogreg_score = round(model.score(X_train,y_train) * 100, 2)\n\nprint(logreg_score)\n\nprint(classification_report(y_test, predicted_log))","8e42c96d":"# Create a confusion matrix\nmatrix = confusion_matrix(y_test, predicted_log)\nsns.heatmap(matrix, annot=True, fmt=\"d\", cmap='Blues', square=True)\nplt.xlabel(\"predicted\")\nplt.ylabel(\"actual\")\nplt","a923aa1f":"# Visualize results by ROC graph\nfpr, tpr, thresholds = roc_curve(y_test, predicted_log)\nroc_auc = auc(fpr, tpr)\nperformance_auc['Logistic Regression'] = roc_auc\n\n# Plotting\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()","55aa3a63":"model = DecisionTreeClassifier().fit(X_train, y_train)\nmodel","0c916502":"predicted_dt = model.predict(X_test)\npredicted_dt","9ec89e78":"# Confidence score\ndectree_score = round(model.score(X_train,y_train) * 100, 2)\nprint(dectree_score)\nprint(classification_report(y_test, predicted_dt))","46de2c77":"# Create a confusion matrix\nmatrix = confusion_matrix(y_test, predicted_dt)\nsns.heatmap(matrix, annot=True, fmt=\"d\", cmap='Blues', square=True)\nplt.xlabel(\"predicted\")\nplt.ylabel(\"actual\")\nplt","99efb6f3":"# Visualize results by ROC graph\nfpr, tpr, thresholds = roc_curve(y_test, predicted_dt)\nroc_auc = auc(fpr, tpr)\nperformance_auc['Decision Tree'] = roc_auc\n\n# Plotting\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()","837384dd":"pd.concat((pd.DataFrame(X_train.iloc[:, 1:].columns, columns = ['variable']), \n           pd.DataFrame(model.feature_importances_, columns = ['importance'])), \n          axis = 1).sort_values(by='importance', ascending = False)[:20]","c0ce2fee":"model = RandomForestClassifier(1000).fit(X_train, y_train)\nmodel","8260f8ff":"predicted_rf = model.predict(X_test)\npredicted_rf","faa05d15":"# Confidence score\nrandfor_score = round(model.score(X_train,y_train) * 100, 2)\nprint(randfor_score)\nprint(classification_report(y_test, predicted_rf))","65632f0b":"matrix = confusion_matrix(y_test, predicted_rf)\nsns.heatmap(matrix, annot=True, fmt=\"d\", cmap='Blues', square=True)\nplt.xlabel(\"predicted\")\nplt.ylabel(\"actual\")\nplt","4d485971":"fpr, tpr, thresholds = roc_curve(y_test, predicted_rf)\nroc_auc = auc(fpr, tpr)\nperformance_auc['Random Forests'] = roc_auc\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()","81c9ce11":"pd.concat((pd.DataFrame(X_train.iloc[:, 1:].columns, columns = ['variable']), \n           pd.DataFrame(model.feature_importances_, columns = ['importance'])), \n          axis = 1).sort_values(by='importance', ascending = False)[:20]","3c6653a2":"model = KNeighborsClassifier(3).fit(X_train, y_train)\nmodel","61bb6b55":"predicted_knn = model.predict(X_test)\npredicted_knn","7d073a10":"# Confidence score\nknn_score = round(model.score(X_train,y_train) * 100, 2)\nprint(knn_score)\nprint(classification_report(y_test, predicted_knn))","3c15c2ea":"matrix = confusion_matrix(y_test, predicted_knn)\nsns.heatmap(matrix, annot=True, fmt=\"d\", cmap='Blues', square=True)\nplt.xlabel(\"predicted\")\nplt.ylabel(\"actual\")\nplt","28311edd":"fpr, tpr, thresholds = roc_curve(y_test, predicted_knn)\nroc_auc = auc(fpr, tpr)\nperformance_auc['k-nearest neighbours'] = roc_auc\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()","cef4830a":"model = SVC(probability=True).fit(X_train, y_train)\nmodel","0d56f9fc":"predicted_sv = model.predict(X_test)\npredicted_sv","80a6718a":"# Confidence score\nsvm_score = round(model.score(X_train,y_train) * 100, 2)\nprint(svm_score)\nprint(classification_report(y_test, predicted_sv))","0ebc7d31":"matrix = confusion_matrix(y_test, predicted_sv)\nsns.heatmap(matrix, annot=True, fmt=\"d\", cmap='Blues', square=True)\nplt.xlabel(\"predicted\")\nplt.ylabel(\"actual\")\nplt","5ff33c2c":"fpr, tpr, thresholds = roc_curve(y_test, predicted_sv)\nroc_auc = auc(fpr, tpr)\nperformance_auc['SVM'] = roc_auc\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()","1a9af7af":"perf = pd.DataFrame.from_dict(performance_auc, orient='index')\nperf['Model'] = perf.index\nperf['AUC'] = perf[0]\nplt.xlabel('AUC')\nplt.title('Classifier AUC')\nsns.set_color_codes(\"muted\")\nsns.barplot(x='AUC', y='Model', data=perf, color=\"b\")","12d705ec":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Decision Tree'],\n    'Score': [svm_score, \n              knn_score, \n              logreg_score, \n              randfor_score,\n              dectree_score ]})\nmodels.sort_values(by='Score', ascending=False)","d3e5c327":"# First Kaggle challenge - Titanic\n\n## Machine Learning from Disaster\n\n- v.022020\n- author: marcusRB\n- [Kaggle - Titanic challenge](https:\/\/www.kaggle.com\/c\/titanic\/)\n\n```\n####\nIn this version I use only few feature, I try an another cleansing method.\nI use same ML algorithms\n####\n\n```\n\nThis is the legendary Titanic ML competition \u2013 the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\n### The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n\n### What Data Will I Use in This Competition?\nIn this competition, you\u2019ll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\nTrain.csv will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the \u201cground truth\u201d.\n\nThe `test.csv` dataset contains similar information but does not disclose the \u201cground truth\u201d for each passenger. It\u2019s your job to predict these outcomes.\n\nUsing the patterns you find in the train.csv data, predict whether the other 418 passengers on board (found in test.csv) survived.","6f77496e":"## Train and Test data\n\nDividimos los datos en dos conjuntos, de entrenamiento y de test. Con el conjunto de entrenamiento crearemos el modelo predictivo, y con el de test, lo evaluaremos para ver qu\u00e9 rendimiento tiene.","87b0b8da":"## Workflow stages\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\n\n1. Question or problem definition.\n2. Acquire training and testing data.\n3. Wrangle, prepare, cleanse the data.\n4. Analyze, identify patterns, and explore the data.\n5. Model, predict and solve the problem.\n6. Visualize, report, and present the problem solving steps and final solution.\n7. Supply or submit the results.","8ab881e9":"#### `Parch`\n\nFather and childs of passenger. Numerical variable","2b84692d":"#### `FamilySize`\n\nCreate new feature, called FamilySize, where we summarize `SibSp` and `Parch` as numerical variable.","42837f20":"#### `IsAlone`\n\nWe create new feature caracterized if passanger travel alone or not, based on familySize. The binary feature is called `IsAlone`.","e42cfbbf":"### Logistic Regression\n\nIn statistics, the logistic model (or logit model) is used to model the probability of a certain class or event existing such as pass\/fail, win\/lose, alive\/dead or healthy\/sick. This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc. Each object being detected in the image would be assigned a probability between 0 and 1 and the sum adding to one.\n\nsource [wikipedia - Logistic Regression](https:\/\/en.wikipedia.org\/wiki\/Logistic_regression)","705f368b":"## Variable correlation\n\n- What is Correlation?\n- Test Dataset\n- Covariance\n- Pearson\u2019s Correlation\n- Spearman\u2019s Correlation\n\nSi visualizamos la matriz de correlaci\u00f3n entre las variables, vemos que las mas correlacionadas con la que queremos predecir son `Sex`, `Pclass` i `isAlone`.","9f4f045e":"#### `Survived`\n\nThis is our depedent variable or predictor, it check if passenger survived (`1`) or not (`0`). Almost 38% of passenger survived.","1936567e":"### Random forest\n\n[Random forests](https:\/\/en.wikipedia.org\/wiki\/Random_forest) or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.","8885a03e":"We need to check those 3 features, but it must probable remove `Cabin`, there are many null values.","18db94c1":"#### `SibSp`\n\nNumerical feature. Indicate a sibling of passenger.","bce77835":"### k-nearest neighbors\n\nIn pattern recognition, the k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression.[1] In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:\n\nIn k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\nIn k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors.\nk-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until classification.\n\nsource [wikipedia - k-nearest neighbors](https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm)","77a05f7a":"***\n\n### EDA, Visualization and transformation data\n\nWe analyze all variable one by one and check null value, errors or we create new variables.","195e70e5":"Create groups for all frequents titles and the other will be `Rare`.","7ebe8300":"#### `Fare`\n\nThis continuous numerical variable is ticket fare of the passenger.","b4838dda":"***\n\n## Load dataset\n\nKaggle we provide two datasets: train and test in csv extension. So, we check and analyze only train file.","cbfc5899":"#### `Pclass`\n\nTicket class. This is a categorical feature with 3 different values, first class, second class and third class. Exist high correlation between this feature with dependent variable.","76217835":"### Decision tree\n\nA [decision tree](https:\/\/en.wikipedia.org\/wiki\/Decision_tree) is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.\n\nDecision trees are commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal, but are also a popular tool in machine learning.\n","9130a843":"We transform to categorical variable","ce2e78ed":"After check the variable, has a normal distribution. We apply a logarithm to normalize. ","b926348c":"#### `Ticket`\n\nTicket number of the passanger. In first instance isn't important for the model. We removed it.","20f44156":"#### `PassangerId`\nId of the passenger. We remove it because haven't predictive weight on our model.","01093afb":"#### `Name`\n\nCategorical variable with the name of the passenger. We extract from title names like as `Mr`, `Miss` or `Master`.","19f1f135":"### `Embarked`\n\nThis feature is Port of Embarkation. There are three categorical variables: `C` for Cherbourg, `Q` for Queenstown, `S` for Southampton.","cd281461":"## Conclusion and Model Evaluation\n\nDespu\u00e9s de aplicar diferentes modelos de clasificaci\u00f3n, y viendo que no hay demasiadas diferencias de rendimiento entre ellos, el que mejor resultado nos ha dado con la m\u00e9trica que hemos escogido ha sido el de Support Vector Machines.\n","96f913e8":"## Check the versions of libraries","9e061c61":"## Binarizing categorical variables\n\nDe cada variable categ\u00f3rica haremos `m` variables binarias, donde `m` es el numero de categor\u00edas de la variable.","aae9f678":"#### `Age`\n\nNumerical variable with age of the passenger. We transform in categorical variable and grouped.","e4b1d794":"We have prepared our dataset and ready for to apply algorithms to predict variable `Survived`:\n\n- Logistic Regression\n- Decision Tree\n- Random Forests\n- k-nearest neigbours\n- Support Vector Machines\n\nWe use two metrics: confusion matrix and ROC curve. Only for the Kaggle competition also compare the results with AUC performance.","60255a73":"### Support Vector Machines\n\nIn machine learning, support-vector machines (SVMs, also support-vector networks[1]) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on the side of the gap on which they fall.\n\nsource [wikipedia - SVM](https:\/\/en.wikipedia.org\/wiki\/Support-vector_machine)","536015b9":"***\n\n## Data description\n\nNuestro conjunto de datos tiene 12 columnas o varables, de las cuales 3 (Age, Cabin y Embarked) tienen valores no disponibles. La variable que queremos predecir es Survived, que nos indica si el pasajero sobrevivi\u00f3 a la tragedia del Titanic.","b1a54699":"## Kaggle Submission\n\n### Test made and submitted with Random forest model and SVM with 79% accuracy","b2a0ec69":"## Predictive Modeling\n\n[Classification](https:\/\/en.wikipedia.org\/wiki\/Statistical_classification) problems are one of the most common in machine learning. This is [supervised learning](https:\/\/en.wikipedia.org\/wiki\/Supervised_learning), that is, algorithms that, based on a set of tagged data, generalize a model that makes the most accurate prediction of the tag in a new set of data of the same type.\n\nIn this activity we will focus on showing different measures that we can use to evaluate a classification model.\n\n","a7b89214":"#### `Sex`\n\nPassenger genre. It's a categorical feature with two values, `male` y `female`. We converted it a dummy or binary value.","18015d9f":"***\n\n## Import Libraries","a1dc26bc":"#### `Cabin`\n\nWe transform this feature in binary variable, so it inform if he was or not in the cabin.","29b0853e":"```\nX_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape\n```"}}