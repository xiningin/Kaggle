{"cell_type":{"5a64e1df":"code","c41d4a93":"code","f58b0e41":"code","17f1738f":"code","00ec20b0":"code","ab40240e":"code","00a626fc":"code","5f8794a9":"code","8c04c001":"code","3416e047":"code","cd85de49":"code","4332823c":"code","14684d6b":"code","e2bcf998":"code","4354e147":"code","34b3f2b2":"code","6104f1ea":"code","6dec6b3e":"code","353ba04c":"code","fa081874":"code","082b5c71":"code","d6db1155":"code","dfa35eb2":"code","91b6fb4e":"markdown","88920991":"markdown","93eb1f4d":"markdown","a28119d5":"markdown","b1edc0a2":"markdown","4aafa6d3":"markdown","b8d8253f":"markdown","4a204d84":"markdown","af524e36":"markdown","df08a39c":"markdown","7f3ab622":"markdown","5c87581e":"markdown","8a08248f":"markdown","03cbdf03":"markdown","8867e753":"markdown","be0db52a":"markdown","36fa5958":"markdown"},"source":{"5a64e1df":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nimport xgboost as xgb\ncolor = sns.color_palette()\n\n%matplotlib inline\n\npd.options.mode.chained_assignment = None  # default='warn'\npd.options.display.max_columns = 999\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","c41d4a93":"train_df = pd.read_csv(\"..\/input\/train.csv.zip\")\ntest_df = pd.read_csv(\"..\/input\/test.csv.zip\")\nprint(\"Train shape : \", train_df.shape)\nprint(\"Test shape : \", test_df.shape)","f58b0e41":"\ntrain_df.head()","17f1738f":"plt.figure(figsize=(8,6))\nplt.scatter(range(train_df.shape[0]), np.sort(train_df.y.values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.show()","00ec20b0":"ulimit = 180\ntrain_df['y'].ix[train_df['y']>ulimit] = ulimit\n\nplt.figure(figsize=(12,8))\nsns.distplot(train_df.y.values, bins=50, kde=False)\nplt.xlabel('y value', fontsize=12)\nplt.show()","ab40240e":"dtype_df = train_df.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df.groupby(\"Column Type\").aggregate('count').reset_index()","00a626fc":"dtype_df.ix[:10,:]","5f8794a9":"missing_df = train_df.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df = missing_df.ix[missing_df['missing_count']>0]\nmissing_df = missing_df.sort_values(by='missing_count')\nmissing_df","8c04c001":"unique_values_dict = {}\nfor col in train_df.columns:\n    if col not in [\"ID\", \"y\", \"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"]:\n        unique_value = str(np.sort(train_df[col].unique()).tolist())\n        tlist = unique_values_dict.get(unique_value, [])\n        tlist.append(col)\n        unique_values_dict[unique_value] = tlist[:]\nfor unique_val, columns in unique_values_dict.items():\n    print(\"Columns containing the unique values : \",unique_val)\n    print(columns)\n    print(\"--------------------------------------------------\")\n        ","3416e047":"var_name = \"X0\"\ncol_order = np.sort(train_df[var_name].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.stripplot(x=var_name, y='y', data=train_df, order=col_order)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()","cd85de49":"var_name = \"X1\"\ncol_order = np.sort(train_df[var_name].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.stripplot(x=var_name, y='y', data=train_df, order=col_order)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()","4332823c":"var_name = \"X2\"\ncol_order = np.sort(train_df[var_name].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.boxplot(x=var_name, y='y', data=train_df, order=col_order)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()","14684d6b":"var_name = \"X3\"\ncol_order = np.sort(train_df[var_name].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.violinplot(x=var_name, y='y', data=train_df, order=col_order)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()","e2bcf998":"var_name = \"X4\"\ncol_order = np.sort(train_df[var_name].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.violinplot(x=var_name, y='y', data=train_df, order=col_order)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()","4354e147":"var_name = \"X5\"\ncol_order = np.sort(train_df[var_name].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.boxplot(x=var_name, y='y', data=train_df, order=col_order)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()","34b3f2b2":"var_name = \"X6\"\ncol_order = np.sort(train_df[var_name].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.boxplot(x=var_name, y='y', data=train_df, order=col_order)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()","6104f1ea":"var_name = \"X8\"\ncol_order = np.sort(train_df[var_name].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.boxplot(x=var_name, y='y', data=train_df, order=col_order)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()","6dec6b3e":"zero_count_list = []\none_count_list = []\ncols_list = unique_values_dict['[0, 1]']\nfor col in cols_list:\n    zero_count_list.append((train_df[col]==0).sum())\n    one_count_list.append((train_df[col]==1).sum())\n\nN = len(cols_list)\nind = np.arange(N)\nwidth = 0.35\n\nplt.figure(figsize=(6,100))\np1 = plt.barh(ind, zero_count_list, width, color='red')\np2 = plt.barh(ind, one_count_list, width, left=zero_count_list, color=\"blue\")\nplt.yticks(ind, cols_list)\nplt.legend((p1[0], p2[0]), ('Zero count', 'One Count'))\nplt.show()","353ba04c":"\nzero_mean_list = []\none_mean_list = []\ncols_list = unique_values_dict['[0, 1]']\nfor col in cols_list:\n    zero_mean_list.append(train_df.ix[train_df[col]==0].y.mean())\n    one_mean_list.append(train_df.ix[train_df[col]==1].y.mean())\n\nnew_df = pd.DataFrame({\"column_name\":cols_list+cols_list, \"value\":[0]*len(cols_list) + [1]*len(cols_list), \"y_mean\":zero_mean_list+one_mean_list})\nnew_df = new_df.pivot('column_name', 'value', 'y_mean')\n\nplt.figure(figsize=(8,80))\nsns.heatmap(new_df)\nplt.title(\"Mean of y value across binary variables\", fontsize=15)\nplt.show()","fa081874":"var_name = \"ID\"\nplt.figure(figsize=(12,6))\nsns.regplot(x=var_name, y='y', data=train_df, scatter_kws={'alpha':0.5, 's':30})\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()","082b5c71":"plt.figure(figsize=(6,10))\ntrain_df['eval_set'] = \"train\"\ntest_df['eval_set'] = \"test\"\nfull_df = pd.concat([train_df[[\"ID\",\"eval_set\"]], test_df[[\"ID\",\"eval_set\"]]], axis=0)\n\nplt.figure(figsize=(12,6))\nsns.violinplot(x=\"eval_set\", y='ID', data=full_df)\nplt.xlabel(\"eval_set\", fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of ID variable with evaluation set\", fontsize=15)\nplt.show()","d6db1155":"for f in [\"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"]:\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train_df[f].values)) \n        train_df[f] = lbl.transform(list(train_df[f].values))\n        \ntrain_y = train_df['y'].values\ntrain_X = train_df.drop([\"ID\", \"y\", \"eval_set\"], axis=1)\n\n# Thanks to anokas for this #\ndef xgb_r2_score(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'r2', r2_score(labels, preds)\n\nxgb_params = {\n    'eta': 0.05,\n    'max_depth': 6,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'silent': 1\n}\ndtrain = xgb.DMatrix(train_X, train_y, feature_names=train_X.columns.values)\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=100, feval=xgb_r2_score, maximize=True)\n\n# plot the important features #\nfig, ax = plt.subplots(figsize=(12,18))\nxgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nplt.show()","dfa35eb2":"from sklearn import ensemble\nmodel = ensemble.RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_leaf=4, max_features=0.2, n_jobs=-1, random_state=0)\nmodel.fit(train_X, train_y)\nfeat_names = train_X.columns.values\n\n## plot the importances ##\nimportances = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\nindices = np.argsort(importances)[::-1][:20]\n\nplt.figure(figsize=(12,12))\nplt.title(\"Feature importances\")\nplt.bar(range(len(indices)), importances[indices], color=\"r\", align=\"center\")\nplt.xticks(range(len(indices)), feat_names[indices], rotation='vertical')\nplt.xlim([-1, len(indices)])\nplt.show()","91b6fb4e":"Good to see that there are no missing values in the dataset :) \n\n**Integer Columns Analysis:**","88920991":"So majority of the columns are integers with 8 categorical columns and 1 float column (target variable)","93eb1f4d":"Categorical occupy the top spots followed by binary variables. \n\nLet us also build a Random Forest model and check the important variables.","a28119d5":"**More to come. Stay tuned.!**\n\n**Please upvote if you like it.!**","b1edc0a2":"Binary variables which shows a good color difference in the above graphs between 0 and 1 are likely to be more predictive given the the count distribution is also good between both the classes (can be seen from the previous graph). We will dive more into the important variables in the later part of the notebook.\n\n**ID variable:**\n\nOne more important thing we need to look at it is ID variable. This will give an idea of how the splits are done across train and test (random or id based) and also to help see if ID has some potential prediction capability (probably not so useful for business)\n\nLet us first see how the 'y' variable changes with ID variable.","4aafa6d3":"Seems like a single data point is well above the rest. \n\nNow let us plot the distribution graph.","b8d8253f":"**Binary Variables:**\n\nNow we can look into the binary variables. There are quite a few of them as we have seen before. Let us start with getting the number of 0's and 1's in each of these variables.","4a204d84":"**Target Variable:**\n\n\"y\" is the variable we need to predict. So let us do some analysis on this variable first.","af524e36":"X0 to X8 are the categorical columns.\n\n**Missing values:**\n\nLet us now check for the missing values.","df08a39c":"There seems to be a slight decreasing trend with respect to ID variable. Now let us see how the IDs are distributed across train and test.","7f3ab622":"Seems like a random split of ID variable between train and test samples.\n\n**Important Variables:**\n\nNow let us run and xgboost model to get the important variables.","5c87581e":"Wow the number of rows are small with 388 columns. We should try not to overfit :)\n\nLet us look at the top few rows.","8a08248f":"So all the integer columns are binary with some columns have only one unique value 0. Possibly we could exclude those columns in our modeling activity.\n\nNow let us explore the categorical columns present in the dataset.","03cbdf03":"Now let us check the mean y value in each of the binary variable.","8867e753":"Quite a few differences in the important variables between xgboost and random forest. Not sure why though.!","be0db52a":"Now let us have a look at the data type of all the variables present in the dataset.","36fa5958":"In this notebook, let us explore the dataset that is given for this competition.\n\n**Objective:**\n\nThis dataset contains an anonymized set of variables that describe different Mercedes cars. The ground truth is labeled 'y' and represents the time (in seconds) that the car took to pass testing. \n\nLet us first import the necessary modules."}}