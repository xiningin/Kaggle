{"cell_type":{"6b83994d":"code","b33e28d6":"code","42776733":"code","0da562c3":"code","78179b33":"code","5191c320":"code","133df9f2":"code","b5f91745":"code","6d399466":"code","7a7507b3":"code","e91423cc":"code","aed01f60":"code","106569ed":"code","275cf070":"code","92bcaef0":"code","38d7dddc":"code","dbffb906":"code","f6f5e814":"code","f4355bb9":"code","99179ae6":"code","fd42b885":"code","fa44aae4":"code","3cce5ba2":"code","beeb6709":"code","65a6b5f2":"code","e85477a5":"code","f25226c6":"code","bd2ddc43":"code","03f349cf":"code","fcd273f6":"code","9836621b":"code","a567b295":"code","f3d8fc01":"code","b291b9bf":"code","e7a1fe07":"code","79536b08":"markdown","15e1e1a5":"markdown","de56a54d":"markdown","b0bb70a9":"markdown","2d83aae7":"markdown","d15bc3ce":"markdown","0e6e5cb8":"markdown","a3f6208c":"markdown","7dec1310":"markdown","5410ac1e":"markdown","b61053c4":"markdown","a2e0e723":"markdown","7aed8e39":"markdown","d90200c5":"markdown","256314e6":"markdown"},"source":{"6b83994d":"import numpy as np, pandas as pd, os, matplotlib.pyplot as plt, seaborn as sns\nimport json, re, gc                              #garbage collector\nfrom sklearn.preprocessing import LabelEncoder\nfrom ast import literal_eval\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV #Experimented hyperparams a bit with this\n\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        pass\ngc.enable()\nsns.set(style='whitegrid',palette='deep',font_scale=1.1,rc={'figure.figsize':[8,6]})\npd.set_option('float_format', '{:f}'.format)     #to display full numbers in dataframe and not just exponentiated form ","b33e28d6":"''' THIS WAS AN INITIAL APPROACH TO STANDARDIZE DATA, SKIP OVER THIS CELL '''\n\ndef drop_messy_cols(df):\n    #print(df.dtypes)\n    numeric_cols=list(df.select_dtypes(include=['int32','int64','float32']).columns)\n    categorical_cols=list(df.select_dtypes(include=['object']).columns)\n    bool_cols=list(df.select_dtypes(include=['bool']).columns)\n\n\n    ''' TO FIND WHICH CATEGORICAL COLUMNS HAVE LOW CARDINALITY '''\n    low_cardinality_cols=[col for col in categorical_cols if df[col].nunique()<10]\n    print('\\n########################## LOW CARDINALITY COLUMNS: ##########################\\n',df[low_cardinality_cols].nunique())\n\n\n    ''' TO FIND WHICH CATEGORICAL COLUMNS HAVE V HIGH CARDINALITY '''\n    high_cardinality_cols=[col for col in categorical_cols if df[col].nunique()>10 and col not in [\n                                        'totals.transactionRevenue','fullVisitorId', 'totals.hits', 'device.operatingSystem', 'geoNetwork.metro'\n                                        'totals.pageviews','trafficSource.source']\n                          ]\n    print('\\n########################## HIGH CARDINALITY COLUMNS: ##########################\\n',df[high_cardinality_cols].nunique())\n\n\n\n    ''' TO FIND WHICH COLUMNS HAVE <na> IN LARGE NUMBERS '''\n    high_na_cols=[col for col in df.columns if df[col].isna().sum()>10000 and col not in ['totals.transactionRevenue']]\n    print('\\n########################## COLUMNS WITH \\'na\\' IN LARGE NUMBERS: ##########################\\n',df[high_na_cols].isna().sum())\n\n\n\n    _COLS_TO_DROP = ['visitNumber', 'totals.newVisits', 'totals.bounces', 'trafficSource.isTrueDirect', 'geoNetwork.city', 'geoNetwork.networkDomain',\n                     'geoNetwork.region', 'geoNetwork.subContinent', 'trafficSource.adContent', 'trafficSource.campaign', 'trafficSource.keyword', \n                     'trafficSource.referralPath', 'device.isMobile'\n                    ]\n\n    df.drop(columns=_COLS_TO_DROP , inplace=True, axis=1)\n    return df\n\n\n\ndef preprocess_and_impute(train_df, test_df):\n    full_df=pd.concat([train_df,test_df],axis=0)\n    \n    ''' DROP COLS WITH (V HIGH CARDINALITY & NOT MUCH RELEVANCE) , (COLS WITH CONSTANT VAL) , (V HIGH NUMBER OF <na> VALS) '''\n    full_df=drop_messy_cols(full_df)\n    \n    ''' IDENTIFY COUNTRY_NAMES WITH LESS THAN 10 TRANSACTIONS AND SET THEM TO <Other> TO AVOID HIGH CARDINALITY '''\n    _CNTRYS_LT_4_TRANSACTIONS = list(full_df.groupby('geoNetwork.country')['totals.transactions'].sum()[full_df.groupby('geoNetwork.country')['totals.transactions'].sum()<4].index)\n    full_df['geoNetwork.country'][full_df['geoNetwork.country'].isin(_CNTRYS_LT_4_TRANSACTIONS)]='Other'\n    \n    \n    ''' I IDENTIFIED BROWSER NAMES WHICH SEEM LEGIT, AND SET THE REST TO <Others> TO AVOID HIGH CARDINALITY '''\n    _VALID_BROWSERS = [ 'Chrome', 'Safari', 'Firefox', 'Internet Explorer', 'Android Webview', 'Edge', 'Samsung Internet']\n    full_df['device.browser'][~full_df['device.browser'].isin(_VALID_BROWSERS)]='Other'\n    \n    \n    ''' I IDENTIFIED OS NAMES WHICH SEEM LEGIT, AND SET THE REST TO <Other> TO AVOID HIGH CARDINALITY '''\n    _VALID_OPERATING_SYSTEMS = ['Windows', 'Macintosh', 'Android', 'iOS', 'Linux', 'Chrome OS', 'Windows Phone', 'Samsung']\n    full_df['device.operatingSystem'][~full_df['device.operatingSystem'].isin(_VALID_OPERATING_SYSTEMS)]='Other'\n    \n    \n    ''' IDENTIFY METRO_NAMES LEADING TO 0 TRANSACTIONS AND SET THEM TO <Other> TO AVOID HIGH CARDINALITY '''\n    _SPARSE_METROS= list(full_df.groupby('geoNetwork.metro')['totals.transactions'].sum()[full_df.groupby('geoNetwork.metro')['totals.transactions'].sum()<1].index)\n    print('\\nFound {} sparse-metros. The entire dataset still has {} metros.'.format(len(set(_SPARSE_METROS)), len(set(full_df['geoNetwork.metro']))-len(set(_SPARSE_METROS))))\n    full_df['geoNetwork.metro'][full_df['geoNetwork.metro'].isin(_SPARSE_METROS)]='Other'\n    \n    \n    \n    ''' IDENTIFY TRAFFIC SOURCES LEADING TO 0 TRANSACTIONS, AND SET THEM TO <Other> TO AVOID HIGH CARDINALITY '''\n    _INEFFECTIVE_TRAFFIC_SOURCES=list(full_df.groupby('trafficSource.source')['totals.transactions'].sum()[full_df.groupby('trafficSource.source')['totals.transactions'].sum()<1].index)\n    print('\\nFound {} ineffective traffic-sources. The entire dataset still has {} traffic-sources.'.format(len(set(_INEFFECTIVE_TRAFFIC_SOURCES)), len(set(full_df['trafficSource.source']))-len(set(_INEFFECTIVE_TRAFFIC_SOURCES))))\n    full_df['trafficSource.source'][full_df['trafficSource.source'].isin(_INEFFECTIVE_TRAFFIC_SOURCES)]='Other'\n    \n    full_df['trafficSource.source'][full_df['trafficSource.source'].str.contains('google')]='google'\n    full_df['trafficSource.source'][full_df['trafficSource.source'].str.contains('facebook')]='facebook'\n    full_df['trafficSource.source'][full_df['trafficSource.source'].str.contains('yahoo')]='yahoo'\n    print('\\n\\nLIST OF trafficSource.source: ',list(full_df['trafficSource.source'].value_counts().index))\n    \n    \n    full_df['date.year']=full_df['date'].dt.year\n    full_df['date.month']=full_df['date'].dt.month\n    full_df['date.weekday']=full_df['date'].dt.weekday\n    full_df['date.day']=full_df['date'].dt.day\n\n    del(full_df['date'])\n    ''' ENCODE THE FINAL CATEGORICAL COLS INTO LABEL-ENCODINGS OF <n-1> VALUES EACH '''\n    _CAT_COLS_FOR_ENCODER = ['channelGrouping', 'date.year','date.month', 'device.browser', 'device.deviceCategory', 'device.operatingSystem', \n                             'geoNetwork.continent', 'geoNetwork.country', 'geoNetwork.metro', 'trafficSource.medium', 'trafficSource.source']\n    for col in _CAT_COLS_FOR_ENCODER:\n        lb = LabelEncoder()\n        full_df[col]=lb.fit_transform(full_df[col])\n    return full_df","42776733":"def load_df_without_hits(path=dirname+'\/train_v2.csv'):\n    ans=pd.DataFrame()\n    dfs=pd.read_csv(path, parse_dates=['date'],\n                    converters={ j: json.loads for j in _JSON_COLUMNS },\n                    dtype={'fullVisitorId':str}, chunksize=100000\n                   )\n    \n    for df in dfs:\n        df.reset_index(drop=True, inplace=True)\n        #print(df.hits[0])\n        \n        ''' PROCESS COLS WITH JSON DATA '''        \n        for col in _JSON_COLUMNS:\n            column_as_df= pd.json_normalize(df[col])\n            #print(column_as_df.columns)\n            column_as_df.columns=[f'{col}.{subcolumn}' for subcolumn in column_as_df.columns]\n            df=df.drop(col, axis=1).merge(column_as_df, right_index=True, left_index=True)\n        \n        ''' REPLACE STRANGE <unknown> VALUES INTO <na> '''\n        df=df.replace( _COL_VALUES_TO_BE_REPLACED , 'NA')\n        \n        \n        \n        ''' CONVERT <object> dtype COLS INTO <integer> VALUE COLS WHEREVER NEEDED '''\n        for col in _COLS_TO_CONVERT_TO_INT:\n            df[col] = df[col].fillna(1).astype(int)\n        \n        \n        ''' IMPUTE totals.transactionRevenue WITH 0 VALUE '''\n        df['totals.transactionRevenue'] = df['totals.transactionRevenue'].fillna(0).astype(float)\n        df['totals.totalTransactionRevenue'] = df['totals.totalTransactionRevenue'].fillna(0).astype(float)\n        \n        \n        ''' CHOOSE ONLY A FEW OF THE COLS INCLUDING NESTED JSON COLS '''\n        use_df = df[_FEATURES_WITHOUT_HITS]\n        del df\n        gc.collect()\n        \n        ans = pd.concat([ans, use_df], axis = 0).reset_index(drop = True)\n        if ans.shape[0]%200000==0:\n            print(f\"Loaded {os.path.basename(path)} and now df= {ans.shape}\")    \n    return ans\n\n\n\n\n\n\n''' IDENTIFIED COLUMN NAMES WITH NESTED JSON DATA '''\n_JSON_COLUMNS = [\n    'device','geoNetwork','totals','trafficSource'\n    ]\n\n\n\n''' TO KEEP ONLY A FEW OF THE COLS INCLUDING NESTED JSON COLS '''\n_FEATURES_WITHOUT_HITS = [\n    'channelGrouping', 'date', 'fullVisitorId', 'socialEngagementType', 'visitId', 'visitNumber', 'visitStartTime', \n    'device.browser', 'device.deviceCategory', 'device.isMobile', 'device.operatingSystem',\n    'geoNetwork.city', 'geoNetwork.continent', 'geoNetwork.country', 'geoNetwork.metro', 'geoNetwork.networkDomain', 'geoNetwork.region', 'geoNetwork.subContinent',        \n    'totals.hits', 'totals.pageviews', 'totals.transactionRevenue','totals.totalTransactionRevenue', 'totals.sessionQualityDim', 'totals.transactions', 'totals.bounces', 'totals.newVisits', 'totals.timeOnSite',\n    'trafficSource.medium', 'trafficSource.source', 'trafficSource.isTrueDirect', 'trafficSource.keyword','trafficSource.campaign','trafficSource.referralPath', 'trafficSource.adContent', \n    'trafficSource.adwordsClickInfo.slot'\n    ]\n\n\n\n''' TO CONVERT <object> dtype COLS INTO <integer> VALUE COLS WHEREVER NEEDED '''\n_COLS_TO_CONVERT_TO_INT =  [\n    'totals.pageviews', 'totals.hits', 'totals.sessionQualityDim',\n    'totals.visits', 'totals.timeOnSite', 'totals.transactions'\n    ]\n\n\n\n\n''' TO REPLACE STRANGE <unknown> VALUES TO <na> '''\n_COL_VALUES_TO_BE_REPLACED = {\n            'device.operatingSystem' :      ['not available in demo dataset','(not set)'],\n            'device.browser' :              ['not available in demo dataset','(not set)','0'],\n            'geoNetwork.city' :             ['not available in demo dataset','(not set)'],\n            'geoNetwork.metro' :            ['not available in demo dataset', '(not set)'], \n            'geoNetwork.networkDomain' :    ['unknown.unknown', '(not set)', 'not.configured'],\n            'geoNetwork.region' :           ['not available in demo dataset','(not set)'],\n            'geoNetwork.continent' :        ['not available in demo dataset','(not set)'],\n            'geoNetwork.subContinent' :     ['not available in demo dataset','(not set)'],\n            'geoNetwork.country' :          ['not available in demo dataset','(not set)'],\n            'trafficSource.campaign' :      ['not available in demo dataset','(not set)'],\n            'trafficSource.medium' :        ['(none)','(not set)'],\n            'trafficSource.source' :        '(not set)'\n            }","0da562c3":"%%time\nprint('{} Reading the 27 Gig \\'train_v2.csv\\' will take ~18 minutes time! Grab a coffee in the meanwhile. {}\\n'.format('#'*26 , '#'*26))\ntrain = load_df_without_hits(path=dirname+'\/train_v2.csv')\nprint(list(train.columns))\ntrain_bkp=train.copy()","78179b33":"%%time\nprint('{} Reading the 7 Gig \\'test_v2.csv\\' will take ~4 minutes time! Go hydrate in the meanwhile :) {}\\n'.format('#'*26 , '#'*26))\ntest = load_df_without_hits(path=dirname+'\/test_v2.csv')\nprint(list(test.columns))","5191c320":"train_shape = train.shape\ntest_shape = test.shape\nprint('Train_Shape=',train_shape)\nprint('Test_Shape=',test_shape)","133df9f2":"full_df=pd.concat([train,test],axis=0)\n_VALID_BROWSERS = [ 'Chrome', 'Safari', 'Firefox', 'Internet Explorer', 'Android Webview', 'Edge', 'Samsung Internet']\nfull_df['device.browser'].loc[~full_df['device.browser'].isin(_VALID_BROWSERS)]='Other'\n\n_VALID_OPERATING_SYSTEMS = ['Windows', 'Macintosh', 'Android', 'iOS', 'Linux', 'Chrome OS', 'Windows Phone', 'Samsung']\nfull_df['device.operatingSystem'].loc[~full_df['device.operatingSystem'].isin(_VALID_OPERATING_SYSTEMS)]='Other'\n\nfull_df['date.weekday']=full_df['date'].dt.weekday\nfull_df['date.day']=full_df['date'].dt.day\nfull_df['date.month']=full_df['date'].dt.month\nfull_df['date.year']=full_df['date'].dt.year\n\nprint(full_df.nunique()[full_df.nunique()==1].index)\ndel(full_df['totals.bounces'])\ndel(full_df['totals.newVisits'])\ndel(full_df['socialEngagementType'])\ndel(full_df['trafficSource.isTrueDirect'])","b5f91745":"full_df[['fullVisitorId','channelGrouping','date','visitNumber','totals.transactions','totals.transactionRevenue']][\n    full_df['fullVisitorId']=='9974232250427988367'].sort_values(by=['date','visitNumber'])","6d399466":"visitors_in_both_years= set(full_df['fullVisitorId'][full_df['date.year']==2016]).intersection(set(full_df['fullVisitorId'][full_df['date.year']==2017]))\ntmp_df=full_df[['fullVisitorId', 'totals.transactionRevenue']][full_df['fullVisitorId'].isin(visitors_in_both_years)]\nprint(tmp_df.groupby('fullVisitorId')['totals.transactionRevenue'].count()[(tmp_df.groupby('fullVisitorId')['totals.transactionRevenue'].count()>1) \n                                                                      & (tmp_df.groupby('fullVisitorId')['totals.transactionRevenue'].sum()!=0)])\ndel(tmp_df)\ndel(visitors_in_both_years)\ndel(full_df['visitId'])","7a7507b3":"sns.distplot(full_df['totals.transactionRevenue'], norm_hist=True, kde=True,kde_kws={'bw': 0.1}, \n             bins=20, hist_kws={'alpha':1}).set(xlabel='Total Revenue',ylabel='Count of records in dataset')\nplt.show()","e91423cc":"print(full_df.dtypes)\nfull_df.head(15)","aed01f60":"tmp_dist=pd.DataFrame(columns=['user_level_transactions'], data=full_df.groupby('fullVisitorId')['totals.transactionRevenue'].sum().values)\n\ntmp_dist['user_level_transactions']=np.where(tmp_dist['user_level_transactions']==0, \n                                             'Users without a single transaction', 'Users with one or more transaction(s)')\n\nplt.figure(figsize=(8,8))\nplt.pie(x=tmp_dist['user_level_transactions'].value_counts(), colors=['orange','yellow'],\n        labels=tmp_dist['user_level_transactions'].value_counts().index, autopct='%.2f%%')\n\nplt.gcf().gca().add_artist(plt.Circle(xy=(0,0),radius=0.7,color='white'))\nplt.title('Highly imbalanced dataset', fontdict={'fontweight':'bold'})\ndel(tmp_dist)\nplt.show()\n''' General data distribution '''","106569ed":"''' Relationship between channelGrouping and transactionRevenue '''\nfig, ax=plt.subplots(1,2, figsize=(12,10))\n\nsns.countplot(x='channelGrouping',data=full_df, palette='twilight_shifted_r', ax=ax[0], order=full_df['channelGrouping'].value_counts(ascending=False).index).set_xticklabels(ax[0].get_xticklabels(),rotation=90)\nax[0].set_title('# of visits vs \\'channelGrouping\\'', fontdict={'fontweight':'bold'})\nsns.barplot(data=pd.pivot_table(data=full_df,index='channelGrouping',values='totals.transactionRevenue',aggfunc='mean').reset_index().sort_values(by='totals.transactionRevenue',ascending=False),\n               x='channelGrouping', y='totals.transactionRevenue', palette='twilight_shifted_r', ax=ax[1]).set_xticklabels(ax[1].get_xticklabels(),rotation=90)\nax[1].set_title('Mean Revenue vs \\'channelGrouping\\'', fontdict={'fontweight':'bold'})\n\n\n\nplt.show()\n''' <Referral> and <Direct> channels produce most revenue; <Organic Search> and <Social> lead to most visits but lead to very little revenue in the RHS plot '''","275cf070":"plt.figure(figsize=(16,6))\nyearly_channelwise_revenue = full_df.groupby(['date.year','channelGrouping'])['totals.transactionRevenue'].mean().reset_index()\nsns.barplot(data=yearly_channelwise_revenue, x='date.year', y='totals.transactionRevenue', hue='channelGrouping', palette='twilight_shifted_r')\nplt.show()\ndel(yearly_channelwise_revenue)\n''' <Referral> gives most revenue each year, <Direct> is 2nd most for 2016,2018, and <Display> is most for 2017 '''","92bcaef0":"plt.figure(figsize=(18,8))\nmonthwise_revenue = pd.pivot_table(data=full_df, index=['date.year', 'date.month'], \n                                   values='totals.transactionRevenue', aggfunc='mean').reset_index().sort_values(by='date.month')\nmonthwise_revenue['date.month'].replace({1:'Jan',2:'Feb',3:'Mar',4:'Apr',5:'May',6:'June',7:'July',8:'Aug',9:'Sept',10:'Oct',11:'Nov',12:'Dec'},inplace=True)\n\nsns.barplot(data=monthwise_revenue, x='date.year', y='totals.transactionRevenue', hue='date.month', palette='icefire')\nplt.show()\ndel(monthwise_revenue)\n''' Most Revenue: August 2016, April 2017, and Sept 2018; August to November general trend of decreasing revenue '''","38d7dddc":"plt.figure(figsize=(16,6))\nfull_df.groupby('date')['totals.transactionRevenue'].mean().plot()\nplt.title('Revenue vs {}'.format('date'.capitalize()), fontdict={'fontweight':'bold'})\nplt.show()\n''' Revenue-Spikes around Sept 2016, Feb 2017, Apr 2017, Aug-Sept 2017, and Aug 2018 '''","dbffb906":"fig, ax= plt.subplots(1,2, figsize=(18,6))\n\nweekday_revenue_dist = pd.pivot_table(data=full_df,index=['date.weekday'],values='totals.transactionRevenue',aggfunc='mean').reset_index()\nsns.lineplot(data=weekday_revenue_dist,x='date.weekday',y='totals.transactionRevenue', palette='Accent', ax=ax[0])\n\nweekly_revenue_dist=pd.pivot_table(data=full_df,index=['date.weekday','date.year'],values='totals.transactionRevenue',aggfunc='mean').reset_index()\nsns.lineplot(data=weekly_revenue_dist,x='date.weekday',y='totals.transactionRevenue', hue='date.year', palette='Accent', ax=ax[1])\nplt.show()\ndel(weekly_revenue_dist)\ndel(weekday_revenue_dist)\n''' Overall sales dip on Saturday; On average: 2016 sales peak on Monday, 2017 sales peak on Wednesday, 2018 sales peak on Friday '''","f6f5e814":"categorical_variables=['device.browser','device.isMobile','device.operatingSystem','device.deviceCategory']\ncolor_schemes=['ocean_r', 'twilight_r', 'muted', 'autumn_r']\ni=0\nfig, ax = plt.subplots(2, 2, figsize=(16,12))\nplt.subplots_adjust(wspace=.5, hspace=.7)\nfor variable, subplot in zip(categorical_variables, ax.flatten()):\n    tmp = full_df.groupby(variable)['totals.transactionRevenue'].mean().reset_index().sort_values(ascending=False,by='totals.transactionRevenue')\n    c=sns.barplot(data=tmp,x=variable,y='totals.transactionRevenue', ax=subplot, palette=color_schemes[i])\n    c.set_xticklabels(subplot.get_xticklabels(),rotation=90)\n    c.set_title('Revenue vs {}'.format(variable.replace('device.','').capitalize()), fontdict={'fontweight':'bold'})\n    i+=1\nplt.show()\n''' <Browser> Firefox,Chrome,Edge ;  <isMobile> False ;  <OperatingSystem> ChromeOs,Mac,Linux,Windows ;  <DeviceType> Desktop give most revenue '''","f4355bb9":"fig, ax= plt.subplots(1,2, figsize=(18,6))\n\nop_ch_device_0 = pd.pivot_table(data=full_df,index=['device.deviceCategory','device.operatingSystem','channelGrouping'],\n                              values='totals.transactionRevenue',aggfunc='mean').reset_index()\nop_ch_device=op_ch_device_0[op_ch_device_0['totals.transactionRevenue']>0]\nsns.barplot(data=op_ch_device, x='device.operatingSystem',y='totals.transactionRevenue', \n            hue='device.deviceCategory', palette='Accent', ax=ax[0], dodge=False).set_xticklabels(ax[0].get_xticklabels(),rotation=90)\nax[0].set_title('Revenue vs OS-wise devices', fontdict={'fontweight':'bold'})\nax[0].set_ylim([0,2e7])\nsns.barplot(data=op_ch_device, x='device.operatingSystem',y='totals.transactionRevenue', \n            hue='channelGrouping', palette='Accent_r', ax=ax[1], dodge=False).set_xticklabels(ax[1].get_xticklabels(),rotation=90)\nax[1].set_title('Revenue vs OS-wise channel', fontdict={'fontweight':'bold'})\nax[1].set_ylim([0,2e7])\nplt.show()\nprint(''' LHS: Chrome\/Mac\/Windows with Desktop gives most revenue ;  \n RHS: Referral visits leading to overall most revenue are spread across OS's ;\n RHS: Windows with Display channel gives most revenue ;''')\ndel(op_ch_device)\ndel(op_ch_device_0)","99179ae6":"plt.figure(figsize=(24,6))\ncitywise_revenue=pd.pivot_table(data=full_df, index=['geoNetwork.city'], values='totals.transactionRevenue', \n                                aggfunc='mean').sort_values(by='totals.transactionRevenue', ascending=False).reset_index()\nsns.barplot(data=citywise_revenue.head(50), x='geoNetwork.city', y='totals.transactionRevenue', palette='Greens_r')\nplt.xticks(rotation=90)\nplt.show()\n''' <City> On average, Fort Collins produces most revenue '''","fd42b885":"fig, ax= plt.subplots(2,1, figsize=(18,15))\nplt.subplots_adjust(hspace=.7)\ncountrywise_revenue = pd.pivot_table(data=full_df, index=['geoNetwork.country'], values='totals.transactionRevenue', \n                                aggfunc='mean').sort_values(by='totals.transactionRevenue', ascending=False).reset_index()\nsns.barplot(data=countrywise_revenue.head(50), x='geoNetwork.country',y='totals.transactionRevenue', \n            palette='Accent', ax=ax[0], dodge=False).set_xticklabels(ax[0].get_xticklabels(),rotation=90)\nax[0].set_title('I. Revenue vs Country', fontdict={'fontweight':'bold'})\n\ncontinentwise_revenue = pd.pivot_table(data=full_df, index=['geoNetwork.continent'], values='totals.transactionRevenue', \n                                aggfunc='mean').sort_values(by='totals.transactionRevenue', ascending=False).reset_index()\nsns.barplot(data=continentwise_revenue, x='geoNetwork.continent',y='totals.transactionRevenue', \n            palette='Accent', ax=ax[1], dodge=False).set_xticklabels(ax[1].get_xticklabels(),rotation=90)\nax[1].set_title('II. Revenue vs Continent', fontdict={'fontweight':'bold'})\n\nplt.show()\ndel(continentwise_revenue)\ndel(countrywise_revenue)\n''' American countries give most revenue ; Anguila, Venezuela, Curacao, USA, Kenya, St.Lucia give most revenue '''","fa44aae4":"numerical_variables=['totals.hits','totals.pageviews','totals.sessionQualityDim', 'totals.timeOnSite']\nfig, ax = plt.subplots(2,2, figsize=(16, 12))\nplt.subplots_adjust(hspace=1)\n\nfor variable, subplot in zip(numerical_variables, ax.flatten()):\n    tmp=pd.pivot_table(data=full_df, index=variable, values='totals.transactionRevenue',\n                       aggfunc='mean').reset_index()\n    sns.scatterplot(data=tmp, x=variable, y='totals.transactionRevenue', ax=subplot)\n    subplot.set_title('Revenue vs {}'.format(variable.replace('totals.','').capitalize()), fontdict={'fontweight':'bold'})\n    subplot.set_ylim(ymin=0,ymax=tmp['totals.transactionRevenue'].quantile(0.90))\n    \nfig.tight_layout(pad=3.0)\nplt.show()\n''' As <totals.hits>, <pageviews>, <sessionQuality> increases, so does chance of increasing Revenue '''","3cce5ba2":"def standardize_traffic_source(val):\n    if 'google' in val:\n        return 'google'\n    elif 'youtube' in val:\n        return 'youtube'\n    elif 'facebook' in val:\n        return 'facebook'\n    elif 'yahoo' in val:\n        return 'yahoo'\n    elif 'baidu' in val:\n        return 'baidu'\n    elif 'aol' in val:\n        return 'aol'\n    elif 'quora' in val:\n        return 'quora'\n    elif 'reddit' in val:\n        return 'reddit'\n    elif 'pinterest' in val:\n        return 'pinterest'\n    elif 'bing' in val:\n        return 'bing'\n    elif 'wow' in val:\n        return 'wow'\n    elif 'blackboard' in val:\n        return 'blackboard'\n    elif len(val)>20:\n        return 'Other'\n    else:\n        return val\n\nfull_df['trafficSource.source']=full_df['trafficSource.source'].apply(standardize_traffic_source)\ncategorical_variables=[#'trafficSource.medium','trafficSource.adContent','trafficSource.keyword','trafficSource.campaign',\n                       'trafficSource.medium','trafficSource.source']\nfig, ax = plt.subplots(2,1, figsize=(16, 12))\nplt.subplots_adjust(hspace=.5)\n\nfor variable, subplot in zip(categorical_variables, ax.flatten()):\n    tmp=pd.pivot_table(data=full_df, index=variable, values='totals.transactionRevenue',\n                       aggfunc='mean').reset_index().sort_values(by='totals.transactionRevenue', ascending=False)\n    sns.barplot(data=tmp.head(50), x=variable, y='totals.transactionRevenue', ax=subplot).set_xticklabels(subplot.get_xticklabels(),rotation=90)\n    subplot.set_title('Revenue vs {}'.format(variable.replace('trafficSource.','').capitalize()), fontdict={'fontweight':'bold'})\n\ndel(tmp)\nplt.show()\n''' cpm: Cost payable by advertiser per thousand Impressions made by publisher (GStore) ; cpc: Cost payable per Click by advertiser to publisher (GStore) '''","beeb6709":"def standardize_traffic_keyword(keyword):\n    keyword=str(keyword).lower()\n    if 'goo' in keyword or 'stor' in keyword:\n        return 'google'\n    elif 'yo' in keyword or 'yuotub' in keyword or 'yuo tub' in keyword or 'tub' in keyword or 'yutob' in keyword:\n        return 'youtube'\n    elif 'tshirt' in keyword or 't shirt' in keyword or 't-shirt' in keyword:\n        return 'tshirt'\n    elif 'shirts' in keyword or 'shirt' in keyword:\n        return 'shirts'\n    elif 'lamp' in keyword or 'lamps' in keyword:\n        return 'lamps'\n    elif 'android' in keyword:\n        return 'android'\n    elif 'merchandi' in keyword:\n        return 'merchandise'\n    elif bool(re.search(r'\\d', keyword)):\n        return 'other'\n    elif 'apparel' in keyword:\n        return 'apparel'\n    elif len(keyword)>28:\n        return 'other'\n    else:\n        return keyword\n\n\nfull_df['trafficSource.keyword']=full_df['trafficSource.keyword'].fillna('other')\nfull_df['trafficSource.keyword']=full_df['trafficSource.keyword'].apply(standardize_traffic_keyword)\nplt.figure(figsize=(24,6))\nkeywordwise_revenue=pd.pivot_table(data=full_df, index=['trafficSource.keyword'], values='totals.transactionRevenue', \n                                aggfunc='mean').sort_values(by='totals.transactionRevenue', ascending=False).reset_index()\nsns.barplot(data=keywordwise_revenue.head(50), x='trafficSource.keyword', y='totals.transactionRevenue', palette='Greens_r')\nplt.title('Revenue vs {}'.format('Keyword'), fontdict={'fontweight':'bold'})\nplt.xticks(rotation=90)\nplt.show()\ndel(keywordwise_revenue)\n''' (not set): a visit to this site from another referral site ; (not provided): a visit to this site via organic search but encrypted via https '''","65a6b5f2":"full_df['trafficSource.adwordsClickInfo.slot']=full_df['trafficSource.adwordsClickInfo.slot'].fillna('No Traffic-ad Slot')\nplt.figure(figsize=(10,6))\ntraffic_slotwise_revenue=pd.pivot_table(data=full_df, index=['trafficSource.adwordsClickInfo.slot'], values='totals.transactionRevenue', \n                                aggfunc='mean').sort_values(by='totals.transactionRevenue', ascending=False).reset_index()\nsns.barplot(data=traffic_slotwise_revenue.head(50), x='trafficSource.adwordsClickInfo.slot', y='totals.transactionRevenue', palette='Greens_r')\nplt.title('Revenue vs {}'.format('adword click info slot'.capitalize()), fontdict={'fontweight':'bold'})\nplt.xticks(rotation=90)\nplt.show()\ndel(traffic_slotwise_revenue)","e85477a5":"def preprocess_and_standardize(train_df, test_df):\n    full_df=pd.concat([train_df,test_df],axis=0)\n\n    ''' SET DATE FEATURES AS SEPERATE COLUMNS '''\n    full_df['date.weekday']=full_df['date'].dt.weekday\n    full_df['date.day']=full_df['date'].dt.day\n    full_df['date.month']=full_df['date'].dt.month\n    full_df['date.year']=full_df['date'].dt.year\n    \n    ''' DROP COLS WITH NOT MUCH RELEVANCE , AND COLS WITH CONSTANT VAL '''\n    print('Columns with cardinality=1 ie Constant value : ',full_df.nunique()[full_df.nunique()==1].index)\n    _COLS_TO_DROP=[ 'trafficSource.adContent', 'visitId', 'device.isMobile', 'trafficSource.referralPath', \n                    'totals.totalTransactionRevenue', 'visitNumber', 'visitStartTime',\n                    'totals.bounces','totals.newVisits','socialEngagementType','trafficSource.isTrueDirect',]\n    for c in _COLS_TO_DROP:\n        try:\n            del(full_df[c])\n        except:\n            pass\n    print('{} I) DROPPED UNNECESSARY COLUMNS {}'.format('#'*26 , '#'*26))\n\n    \n    ''' IDENTIFIED BROWSER NAMES WHICH SEEM LEGIT, AND SET THE REST TO <Others> TO AVOID HIGH CARDINALITY '''\n    _VALID_BROWSERS = [ 'Chrome', 'Safari', 'Firefox', 'Internet Explorer', 'Android Webview', 'Edge', 'Samsung Internet']\n    full_df['device.browser'].loc[~full_df['device.browser'].isin(_VALID_BROWSERS)]='Other'\n\n    ''' IDENTIFIED OS NAMES WHICH SEEM LEGIT, AND SET THE REST TO <Others> TO AVOID HIGH CARDINALITY '''\n    _VALID_OPERATING_SYSTEMS = ['Windows', 'Macintosh', 'Android', 'iOS', 'Linux', 'Chrome OS', 'Windows Phone', 'Samsung']\n    full_df['device.operatingSystem'].loc[~full_df['device.operatingSystem'].isin(_VALID_OPERATING_SYSTEMS)]='Other'\n\n    ''' STANDARDIZE TRAFFIC SOURCES INFO '''\n    full_df['trafficSource.keyword']=full_df['trafficSource.keyword'].fillna('other')\n    full_df['trafficSource.keyword']=full_df['trafficSource.keyword'].apply(standardize_traffic_keyword)\n    full_df['trafficSource.source']=full_df['trafficSource.source'].apply(standardize_traffic_source)\n    full_df['trafficSource.adwordsClickInfo.slot']=full_df['trafficSource.adwordsClickInfo.slot'].fillna('No Traffic-ad Slot')\n    \n    print('{} II) STANDARDIZED COLUMN VALUES {}'.format('#'*26 , '#'*26))\n    \n    ''' ENCODE THE FINAL CATEGORICAL COLS INTO LABEL-ENCODINGS OF <n-1> VALUES EACH '''\n    _CAT_COLS_FOR_ENCODER = ['channelGrouping', 'device.browser', 'device.deviceCategory', 'device.operatingSystem', \n                         'geoNetwork.city','geoNetwork.continent', 'geoNetwork.country', 'geoNetwork.metro', 'geoNetwork.region',\n                         'geoNetwork.subContinent','geoNetwork.networkDomain','trafficSource.medium', 'trafficSource.source',\n                         'trafficSource.campaign','trafficSource.keyword','trafficSource.adwordsClickInfo.slot']\n    \n    print('{} III) ENCODING RELEVANT COLUMNS TO LABELS {}'.format('#'*26 , '#'*26))\n    for col in _CAT_COLS_FOR_ENCODER:\n        print('Converting <',col,'> to labels.')\n        if full_df[col].dtype.name=='object':\n            lb = LabelEncoder()\n            full_df[col]=lb.fit_transform(full_df[col])\n    \n    return full_df\n\n\n\n\n\n\ndef return_aggregated_df(input_df, i=0, create_test=False, verbose=True):\n    \n    if create_test==False:\n        ''' EXTRACT FEATURES FROM PREVIOUS TIMEFRAME '''\n        feature_df = input_df[(pd.to_datetime(input_df['date']) > time_from.index[i]) & (pd.to_datetime(input_df['date']) <= time_end.index[i])].copy()\n    else:\n        ''' USE THE INPUT DATASET AS IS '''\n        feature_df = input_df\n    \n    if verbose==True:\n        print('{} A) CREATED BASE-FEATURE DATAFRAME {}'.format('#'*26 , '#'*26))\n    \n    _COLS_TO_MEDIAN_VAL =  ['channelGrouping' , 'device.browser' , 'device.deviceCategory' , 'device.operatingSystem' , 'geoNetwork.city' , 'geoNetwork.country' , \n                            'geoNetwork.metro' , 'geoNetwork.networkDomain' , 'geoNetwork.region' , 'geoNetwork.subContinent', 'geoNetwork.continent' , \n                            'trafficSource.medium' , 'trafficSource.source' , 'trafficSource.campaign' , 'trafficSource.keyword' , 'trafficSource.adwordsClickInfo.slot']\n    _COLS_TO_SUM_VAL =     ['totals.hits' , 'totals.sessionQualityDim' , 'totals.transactions', 'totals.pageviews' ]\n    _COLS_TO_MEAN_VAL =    ['date.year', 'date.month', 'date.day', 'date.weekday']\n    _COLS_TO_STD_DEV_VAL = ['totals.hits' , 'totals.pageviews' , 'totals.sessionQualityDim']\n    \n    ans=pd.concat([\n        feature_df.groupby('fullVisitorId')[_COLS_TO_MEDIAN_VAL].median().add_suffix('_median'),\n        feature_df.groupby('fullVisitorId')[_COLS_TO_MEAN_VAL].mean().add_suffix('_mean'),\n        feature_df.groupby('fullVisitorId')[_COLS_TO_SUM_VAL].sum().add_suffix('_sum'),\n        feature_df.groupby('fullVisitorId')[_COLS_TO_STD_DEV_VAL].std(ddof=0).add_suffix('_stdDev'),\n        feature_df.groupby('fullVisitorId')['date'].apply(lambda x: (x.max()-x.min())\/\/10**9).rename('time_diff')#,\n        #feature_df.groupby('fullVisitorId')['date'].agg(['first','last']).apply(lambda x: (x.astype(int) - time_from.values[i].astype(int))\/\/10**9).abs().add_suffix('_time')\n    ], axis=1).reset_index()\n    if verbose==True:\n        print('{} B) AGGREGATED THE BASE FEATURES {}'.format('#'*26 , '#'*26))\n    \n    if create_test==False:\n        ''' EXTRACT TARGET VARIABLE FROM FUTURE TIMEFRAME'''\n        target_df  = input_df[(input_df['date'] > pd.Timestamp(time_from.values[i])) \n                              & (input_df['date'] <= pd.Timestamp(time_from.values[i+1]))].copy()\n        ans=ans.merge(target_df.groupby('fullVisitorId')['totals.transactionRevenue'].sum().reset_index(), how = 'left', on = 'fullVisitorId')\n        if verbose==True:\n            print('{} C) MERGED THE FUTURE TIMEFRAME TARGET WITH AGGREGATED FEATURES {}'.format('#'*26 , '#'*26))\n        ans['totals.transactionRevenue']=ans['totals.transactionRevenue'].fillna(0).apply(lambda y: np.log(y) if y>0 else y)\n    \n    return ans","f25226c6":"full_df = preprocess_and_standardize(train,test)\n\n''' WE USE THE BELOW DATE-RANGE VARIABLES FOR CREATING CV-FOLDS BASED ON TIME  '''\noverall_period = pd.date_range(start='2016-08-01',end='2018-12-01', freq='2MS')  #Catch the start-date of every alternate month from Aug-1st-2016 to Dec-1st-2018\ntrain_period = overall_period.to_series().shift(periods=-220, freq='d',axis= 0)  #Shift the index of 'overall_period' by -220 days (will be used for features of dataset)\ntime_from = train_period[train_period.index>np.datetime64('2016-08-01')]         #Select only those values where index > Aug-2016\ntime_end = overall_period.to_series().shift(periods=-50, freq='d',axis= 0)[4:]   #Shift the index of 'overall_period' by only -50 days (will be used for target variable of dataset)\n\nprint('[time_from] <Aug-2016 to May-2018> :\\n',time_from)\nprint('\\n[time_end] <Feb-2017 to Oct-2018> :\\n',time_end)\nprint(\"\\nNOTE:\\nWe\\'ll set aside a validation-dataset of features from 170 days with start-date as {}\\'th entry of [time_from]=\\'{}\\' and end-date as {}\\'th entry of [time_end]=\\'{}\\''.\".format(i,time_from.index[i],i,time_end.index[i]))\n\n\n''' SET ASIDE ONE VALIDATION DATASET '''\ni=4\nval_df = return_aggregated_df(full_df, i)\nvalidation_y=val_df['totals.transactionRevenue']\nvalidation_x=val_df.copy().drop(['fullVisitorId','totals.transactionRevenue'],inplace=False,axis=1).astype(int)\nprint('{} V) CREATED VALIDATION-DATASET FOR APPROACH 2 {}'.format('#'*26 , '#'*26))","bd2ddc43":"submissions_df=pd.DataFrame()\n\ntest_df = full_df.iloc[train_shape[0]:, :] #SHAPE = <1708337, 15> with 296530 unique fullVisitorIds\ntest_x=return_aggregated_df(test_df,i=0,create_test=True )\n\nsubmissions_df['fullVisitorId']=test_x['fullVisitorId'].copy()\ntest_x=test_x.drop(['fullVisitorId'],axis=1).astype(int)\nprint('{} IV) CREATED TEST-DATASET FOR APPROACH 2 {}'.format('#'*26 , '#'*26))","03f349cf":"tmp=pd.DataFrame()\ntmp['features']=full_df.corr()['totals.transactionRevenue'].sort_values(ascending=False).index\ntmp['feature_correlation']=full_df.corr()['totals.transactionRevenue'].sort_values(ascending=False).values\nplt.figure(figsize=(10,12))\nplt.title('Feature Correlation with Target Label', fontdict={'fontweight':'bold'})\nsns.barplot(x='feature_correlation',y='features',data=tmp, palette='Accent')\nplt.show()","fcd273f6":"val_df[['fullVisitorId','totals.transactions_sum','totals.transactionRevenue']][val_df['totals.transactionRevenue']>0].head(10)","9836621b":"''' HYBRID APPROACH '''\ntry:\n    del(test_df)\n    del(tmp)\n    del(val_df)\nexcept:\n    pass\nsubmission_catboost_preds = np.zeros(len(test_x))\nsubmission_xgb_preds = np.zeros(len(test_x))\nsubmission_lgb_preds = np.zeros(len(test_x))\n\nkfold=list(range(time_from.shape[0]-1))\nkfold.remove(4)\nfor fold in kfold:\n    fold_df = return_aggregated_df(full_df, fold, verbose=False)\n    train_y = fold_df['totals.transactionRevenue']\n    train_x = fold_df.copy().drop(['fullVisitorId','totals.transactionRevenue'],inplace=False,axis=1).astype(int)\n    print('\\n{} {}.1) CREATED TRAIN SET OF <{}> FOR THIS FOLD {}'.format(26*'#', fold, train_x.shape, 26*'#'))\n    \n    try:\n        del(fold_df)\n    except:\n        pass\n    \n    print('\\n{} {}.2) TRAINING CatBoostRegressor FOR THIS FOLD {}'.format(26*'#', fold, 26*'#'))\n    catboost = CatBoostRegressor(iterations=1100,learning_rate=0.04, depth=10, bagging_temperature = 0.2, od_wait=20,\n                                 eval_metric='RMSE', random_seed=0, od_type='Iter')\n    catboost.fit(train_x , train_y, eval_set=[(validation_x, validation_y)],early_stopping_rounds=100, use_best_model=True, verbose=100)\n    \n    predictions = catboost.predict(validation_x)\n    predictions[predictions<0]=0\n    print('{}{}.3) CATBOOST REGRESSION LOSS = {}{}\\n\\n'.format('>'*26, fold, mean_squared_error(validation_y, predictions)**.5, '<'*26))\n    \n\n    submission_preds_raw=catboost.predict(test_x)\n    submission_preds_raw[submission_preds_raw<0]=0\n    submission_catboost_preds += submission_preds_raw \/ len(kfold)\n\n\n    print('\\n{} {}.4) TRAINING XGBRegressor FOR THIS FOLD {}'.format(26*'#', fold, 26*'#'))\n    xgb = XGBRegressor(n_estimators=1000, max_depth=20, learning_rate=0.07, booster='gbtree', random_state=0, importance_type='total_gain',\n                       objective= 'reg:squarederror', gamma=1.45, alpha=0.0, subsample=0.67, colsample_bytree=0.054, colsample_bylevel= 0.50)\n    xgb.fit(train_x , train_y ,eval_set=[(validation_x, validation_y)], early_stopping_rounds=100, eval_metric='rmse', verbose=100)\n    predictions = xgb.predict(validation_x)\n    predictions[predictions<0]=0\n    print('{}{}.5) XGB LOSS = {}{}\\n\\n'.format('>'*26, fold, mean_squared_error(validation_y, predictions)**.5, '<'*26))\n\n    submission_preds_raw=xgb.predict(test_x)\n    submission_preds_raw[submission_preds_raw<0]=0\n    submission_xgb_preds +=  submission_preds_raw \/ len(kfold)\n    \n    \n    \n    print('\\n{} {}.6) TRAINING LGBRegressor FOR THIS FOLD {}'.format(26*'#', fold, 26*'#'))\n    lgb_reg = lgb.LGBMRegressor(n_estimators=1100, objective='regression', max_depth=-1, learning_rate=0.08, min_child_samples=40, boosting='gbdt', \n                                feature_fraction=0.9, bagging_freq=10, bagging_fraction=0.8, bagging_seed=3, metric='rmse', lambda_l1=0, lambda_l2=1, verbosity=100)\n    lgb_reg.fit(train_x, train_y, eval_set= [(validation_x, validation_y)], eval_metric='rmse', verbose=100 ,early_stopping_rounds = 100)\n    predictions = lgb_reg.predict(validation_x.values, num_iteration=lgb_reg.best_iteration_)\n    predictions[predictions<0]=0\n    print('{}{}.7) LGB LOSS = {}{}\\n\\n'.format('>'*26, fold, mean_squared_error(validation_y.values, predictions)**.5, '<'*26))\n    submission_preds_raw=lgb_reg.predict(test_x.values, num_iteration=lgb_reg.best_iteration_)\n    submission_preds_raw[submission_preds_raw<0]=0\n    submission_lgb_preds +=  submission_preds_raw \/ len(kfold)","a567b295":"feature_importances=pd.DataFrame()\nfeature_importances['features']=train_x.columns\nfeature_importances['feature_importance']=catboost.get_feature_importance()\n\nplt.figure(figsize=(10,12))\nsns.barplot(x='feature_importance',y='features',data=feature_importances.sort_values('feature_importance',ascending=False), palette='twilight')\nplt.title('Feature Importance as per Catboost-Regressor', fontdict={'fontweight':'bold'})\nplt.show()","f3d8fc01":"feature_importances=pd.DataFrame()\nfeature_importances['features']=train_x.columns\nfeature_importances['feature_importance']=xgb.feature_importances_\n\nplt.figure(figsize=(10,12))\nplt.title('Feature Importance as per XGBRegressor', fontdict={'fontweight':'bold'}) \nsns.barplot(x='feature_importance',y='features',data=feature_importances.sort_values('feature_importance',ascending=False), palette='twilight_shifted_r')\nplt.show()","b291b9bf":"feature_importances=pd.DataFrame()\nfeature_importances['features']=train_x.columns\nfeature_importances['feature_importance']=lgb_reg.feature_importances_\nplt.figure(figsize=(10,12)) \nplt.title('Feature Importance as per LGBRegressor', fontdict={'fontweight':'bold'})\nsns.barplot(x='feature_importance',y='features',data=feature_importances.sort_values('feature_importance',ascending=False), palette='twilight_r')\nplt.show()","e7a1fe07":"submissions_df['PredictedLogRevenue'] = submission_lgb_preds*0.4 + submission_catboost_preds*0.5 + submission_xgb_preds*0.1\nsubmissions_df.to_csv('result.csv', index=False)\nprint(\"Done\")","79536b08":"# PROCESSING THE 'hits' COLUMN IN 'train_v2.csv'\nWe'll need to create this as a function so as to have a final consolidated 'train' dataframe after concatenation.","15e1e1a5":"This initial approach of encoding categorical data was leading to these observations:\n![image.png](attachment:image.png)","de56a54d":"## RECAP FOR SUMMARY OF PROBLEM STATEMENT\n### Re-stating all the facts we know at hand:\n\n* train_v2.csv - contains user transactions from August 1st 2016 to April 30th 2018. (637 days)\n\n* test_v2.csv - contains user transactions from May 1st 2018 to October 15th 2018. (almost 170 days)\n\n* Because we are predicting the log of the total revenue per user, not all rows in test_v2.csv will correspond to a row in the submission, but all unique fullVisitorIds will correspond to a row in the final submission.\n\n* END GOAL is to predict the log of the total revenue per user for the period of December 1st 2018 to January 31st 2019.\n\n* The target-prediction period is thus 47 days after max-available train-data: Dec 1st 2018 to Oct 15th 2018 (almost 50 days later)\n\n* How will we test-validation accuracy, since we're essentially trying to perform look-ahead regression? \n\n![image.png](attachment:image.png)","b0bb70a9":"# LOADING 'test_v2.csv' WITHOUT 'hits' COLUMN\nInvoking the above functions to read chunks of the huge file: _test_v2.csv_","2d83aae7":"# LOADING 'train_v2.csv' WITHOUT 'hits' COLUMN\nInvoking the above functions to read chunks of the huge file: _train_v2.csv_","d15bc3ce":"# ETL FUNCTIONS AND DATA-WRANGLING\n\n\nUtility variables to note in below code-cell:\n*  **_JSON_COLUMNS** : [ ] variable of 4 columns that after EDA, have JSON nested data values\n\n*  **_FEATURES_WITHOUT_HITS** : [ ] variable of 29 columns which by my intuition, should be relevant to feeding into the regression model ahead\n\n*  **_COL_VALUES_TO_BE_REPLACED** : { } variable with 11 key-value pairs of column-names and data-values to be replaced by 'NA\n*  **_COLS_TO_CONVERT_TO_INT** : [ ] variable of 6 columns to be converted into integer dtype.","0e6e5cb8":"reader=pd.read_csv(dirname+'\/train_v2.csv', usecols=[4,10,6], chunksize=100000)\nhit_features=['fullVisitorId','visitId','hits']\nfor chunk in reader:\n    \n    chunk=chunk[hit_features]\n    print(chunk.columns,'Shape of chunk=', chunk.shape)\n    chunk['hits'][chunk['hits']=='[]'] = '[{}]'      # SET THE 'hits' COL VALUE = '[{}]' , WHERE IT IS JUST '[]'\n    \n    print('This next line takes a hell of a lot of time!')\n    chunk['hits'] = chunk['hits'].apply(literal_eval).str[0]\n    #print(chunk.hits[0])                            # UNCOMMENT THIS TO VIEW THE RAW JSON INSIDE 'hits' COLUMN, FIRST ROW\n    \n    chunk=pd.json_normalize(chunk['hits'])\n    if 'product' in chunk.columns:\n        chunk['v2ProductName']   =   chunk['product'].apply(lambda x: [p['v2ProductName'] for p in x] if type(x)==list else [])\n        chunk['v2ProductCategory'] = chunk['product'].apply(lambda x: [p['v2ProductCategory'] for p in x] if type(x)==list else [])\n        del(chunk['product'])     #del is an inplace operation instead of chunk.drop('col',axis=1)\n    if 'promotion' in chunk.columns:\n        chunk['promoId']         =   chunk['promotion'].apply(lambda x: [p['promoId'] for p in x] if type(x)==list else [])\n        chunk['promoName']         = chunk['promotion'].apply(lambda x: [p['promoName'] for p in x] if type(x)==list else [])\n        del(chunk['promotion'])   #del is an inplace operation instead of chunk.drop('col',axis=1)\n    gc.collect()\n    chunk.to_csv('..\/input\/output\/Chunk_Hits001.csv',index=False)   # WRITING TO CSV SO THAT I CAN MANUALLY READ THE CSV FILE\n    break","a3f6208c":"# GROUPING DATA BASED ON VISITORIDs\nPrior to feeding data into the regression model, we need to ensure just 1 record per 'fullVisitorId'. The combination of ('fullVisitorId' , 'visitId') isn't feasible for this aggregation, since I feel we're trying to predict only based on the relevant 'fullVisitorId'.\n\nWe'd thus need to perform some aggregation of the other columns for each chunk of 'fullVisitorId' value. Here are my suggestions:\n\n\n### A. median\/middle VALUE FOR ORIGINALLY string DTYPE:\n> 'channelGrouping' , 'device.browser' , 'device.deviceCategory' , 'device.operatingSystem' , 'geoNetwork.city' , 'geoNetwork.country' , \n  'geoNetwork.metro' , 'geoNetwork.networkDomain' , 'geoNetwork.region' , 'geoNetwork.subContinent', 'geoNetwork.continent' , \n  'trafficSource.medium' , 'trafficSource.source' , 'trafficSource.campaign' , 'trafficSource.keyword' , 'trafficSource.adwordsClickInfo.slot'\n\n### B. sum VALUE FOR ORIGINALLY integer DTYPE:\n> 'totals.hits' , 'totals.sessionQualityDim' , 'totals.transactions', 'totals.pageviews'\n\n### C. mean VALUE FOR ORIGINALLY integer DTYPE:\n> 'date.year', 'date.month', 'date.day', 'date.weekday'\n\n### D. std_dev VALUE FOR ORIGINALLY integer DTYPE:\n> 'totals.hits' , 'totals.pageviews' , 'totals.sessionQualityDim'\n\n### E. time_diff VALUE FOR ORIGINALLY string DTYPE (to be added in later):\n> We'll try creating time-dependent features like _'time_difference'_ based on max and min 'date' for a particular 'fullvisitorId' in the dataset.","7dec1310":"# INTRODUCTION TO THE DATA\n\n![image.png](attachment:image.png)\n\n\n### NOTE-\n* For __pd.read_csv()__, use a parameter _'chunksize'_ since we have a very large file size, a _'parse_dates'_ parameter to recognize _'date'_ values, and a _'converters'_ parameter to transform values in certain columns, using a dict of functions.\n\n* In the below code-cell, I convert values in each value of the _JSON_COLUMN_ columns into a json-interepreted format.\n\n* Data-extraction-strategy inspired from [Burrito Dan (excellent name)](https:\/\/www.kaggle.com\/burritodan\/gstore-pythonic-flatten-only-hits-column?select=hits-00000.csv) and [Prachi Panchal](https:\/\/www.kaggle.com\/ppanchak\/gstore-predictions).","5410ac1e":"# PROBLEM DESCRIPTION\nYou\u2019re challenged to analyze a Google Merchandise Store (also known as GStore, where Google swag is sold) customer dataset to predict revenue per customer. Hopefully, the outcome will be more actionable operational changes and a better use of marketing budgets for those companies who choose to use data analysis on top of GA data.\n\n[Google analytics customer revenue prediction](https:\/\/www.kaggle.com\/c\/ga-customer-revenue-prediction)\n\nEach row in the _train_v2.csv_ and _test_v2.csv_ dataset is one visit to the store. \n\n_train_v2.csv_ - contains user transactions from August 1st 2016 to April 30th 2018.\n\n_test_v2.csv_  - contains user transactions from May 1st 2018 to October 15th 2018.\n\nBecause we are predicting **the log of the total revenue per user**, be aware that not all rows in _test_v2.csv_ will correspond to a row in the submission, but all unique **fullVisitorIds** will correspond to a row in the submission.\n\nWe're trying to predict **the log of the total revenue per user** for the period of December 1st 2018 to January 31st 2019.","b61053c4":"# APPROACH: LOOKAHEAD REGRESSION PREDICTIONS\n* Now we can finally experiment with Regression models. Although the distribution of data is likely to give pretty poor results!\n\n* I found an interesting blog-post which deals with [explaination of xgboost from the official author](https:\/\/homes.cs.washington.edu\/~tqchen\/2016\/03\/10\/story-and-lessons-behind-the-evolution-of-xgboost.html).\n\n* Also spent some time understanding [gradient boosting](https:\/\/en.wikipedia.org\/wiki\/Gradient_boosting).\n\nWe can also attempt to fit the lgb, xgb & catboost model on the entire train-dataset, just like a normal problem statement, without considering specific timeframes. During the time of _'submission.csv'_ creation, we'd just need to group all 'fullVisitorId's and aggregate their predictions using sum().\n\nHowever this approach gives pretty poor results on the Private LB ~ 1.5\n\nInstead we can select the chunk of data from lower timeframe -> extract its features -> merge this chunk with a future timeframe's target-variable ie 'totals.transactionRevenue' -> test accuracy of our regression model.\n\n![image.png](attachment:image.png)","a2e0e723":"try:\n    del(fulltrain_df)\nexcept:\n    pass\nsubmission_catboost_preds = np.zeros(len(test_x))\nsubmission_xgb_preds = np.zeros(len(test_x))\nsubmission_lgb_preds = np.zeros(len(test_x))\n\nkfold=KFold(n_splits=5, shuffle=True, random_state=100)\n\nfor fold, (train_indices,validation_indices) in enumerate(kfold.split(fulltrain_x.values,fulltrain_y.values)):\n    train_y = fulltrain_y.iloc[train_indices]\n    train_x = fulltrain_x.iloc[train_indices].astype(int)\n    validation_y = fulltrain_y.iloc[validation_indices]\n    validation_x = fulltrain_x.iloc[validation_indices].astype(int)\n    print('\\n{} {}.1) CREATED TRAIN AND DEV SETS FOR THIS FOLD {}'.format(26*'#', fold, 26*'#'))\n    \n    \n    print('\\n{} {}.2) TRAINING CatBoostRegressor FOR THIS FOLD {}'.format(26*'#', fold, 26*'#'))\n    catboost = CatBoostRegressor(iterations=1000,learning_rate=0.03, depth=10, eval_metric='RMSE', random_seed = 0,\n                                 bagging_temperature = 0.2, od_type='Iter', od_wait=20)\n    catboost.fit(train_x , train_y, eval_set=[(validation_x, validation_y)],early_stopping_rounds=70, use_best_model=True, verbose=100)\n    \n    predictions = catboost.predict(validation_x)\n    predictions[predictions<0]=0\n    print('{}{}.3) CATBOOST REGRESSION LOSS ={}{}\\n\\n'.format('>'*26, fold, mean_squared_error(validation_y, predictions)**.5, '<'*26))\n    \n    submission_preds_raw=catboost.predict(test_x)\n    submission_preds_raw[submission_preds_raw<0]=0\n    submission_catboost_preds += submission_preds_raw \/ kfold.n_splits\n    \n    \n    \n    print('\\n{} {}.4) TRAINING XGBRegressor FOR THIS FOLD {}'.format(26*'#', fold, 26*'#'))\n    xgb = XGBRegressor(n_estimators=1000, max_depth=20, learning_rate=0.07, booster='gbtree', random_state=0, importance_type='total_gain',\n                       objective= 'reg:squarederror', gamma=1.45, alpha=0.0, subsample=0.67, colsample_bytree=0.054, colsample_bylevel= 0.50)\n    xgb.fit(train_x , train_y ,eval_set=[(validation_x, validation_y)], early_stopping_rounds=70, eval_metric='rmse', verbose=100)\n    predictions = xgb.predict(validation_x)\n    predictions[predictions<0]=0\n    print('{}{}.5) XGB LOSS ={}{}\\n\\n'.format('>'*26, fold, mean_squared_error(validation_y, predictions)**.5, '<'*26))\n\n    submission_preds_raw=xgb.predict(test_x)\n    submission_preds_raw[submission_preds_raw<0]=0\n    submission_xgb_preds +=  submission_preds_raw \/ kfold.n_splits\n    \n    \n\n    print('\\n{} {}.6) TRAINING LGBRegressor FOR THIS FOLD {}'.format(26*'#', fold, 26*'#'))\n    trn_data = lgb.Dataset(fulltrain_x.iloc[train_indices], label=fulltrain_y.iloc[train_indices])\n    val_data = lgb.Dataset(fulltrain_x.iloc[validation_indices], label=fulltrain_y.iloc[validation_indices])\n    clf = lgb.train(params={\n                        'objective':'regression', 'max_depth':-1, 'learning_rate':0.05, 'min_child_samples':40, 'boosting':\"gbdt\", 'feature_fraction':0.8, \n                        'bagging_freq':1, 'bagging_fraction':0.8, 'bagging_seed':3,'metric':'rmse', 'lambda_l1':1, 'lambda_l2':1, 'verbosity':100\n                    },train_set=trn_data, num_boost_round=10000, valid_sets = [trn_data, val_data], verbose_eval=500 ,early_stopping_rounds = 500)\n    predictions = clf.predict(validation_x.values, num_iteration=clf.best_iteration)\n    predictions[predictions<0]=0\n    print('{}{}.7) LGB LOSS ={}{}\\n\\n'.format('>'*26, fold, mean_squared_error(validation_y.values, predictions)**.5, '<'*26))\n        \n    submission_preds_raw=clf.predict(test_x.values, num_iteration=clf.best_iteration)\n    submission_preds_raw[submission_preds_raw<0]=0\n    submission_lgb_preds +=  submission_preds_raw \/ kfold.n_splits","7aed8e39":"# MEAT OF THE ENTIRE NOTEBOOK: EXPLORATORY DATA-ANALYSIS\nBelow I'll attempt to find some aggregation logics so that we have a unique entity per (fullVisitorId, visitId) combination. I'd much rather do this in SQL since I'm proficient there, but here goes.\n\n'totals.transactionRevenue' is captured in a cumulative manner for each year. So for one visitor's yearly data, we can consider max() as his total contribution towards GStore's revenue.\n\nAt the end of each year, that user's transactionRevenue value seems to be reset.","d90200c5":"> A 'hit': It is a user-interaction that results in data being sent to Google-Analytics.\n\n> Each time the tracking code is triggered by a user\u2019s behavior (for example, user loads a page on a website or a screen in a mobile app), Google-Analytics records that activity. Each interaction is packaged into a hit and sent to Google\u2019s servers.\u201c\n\n\nThe 'hits' column was specifically pretty complex in structure. Hence I'll attempt to parse & load it seperately later and merge it into the main train\/test dataframe. Here's a single entry to show you the content:\n\n![image.png](attachment:image.png)","256314e6":"# STRAIGHTFORWARD APPROACH DOESNT GIVE GREAT PERFORMANCE\nHence commented this out for now."}}