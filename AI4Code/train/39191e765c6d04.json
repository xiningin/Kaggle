{"cell_type":{"04475719":"code","a0c36176":"code","dbc18446":"code","ef28314f":"code","099ae4e4":"code","8728a132":"code","538e2e3b":"code","5674ff8e":"code","4e05f54c":"code","5b5d70d3":"code","8bf71424":"code","e23afd18":"code","a2a3b160":"code","a22b8041":"code","a338b395":"code","e2ff403b":"code","16460fa0":"code","d3c7a24b":"code","3f251a50":"code","9b971d6d":"code","d5ddc34e":"code","5d68b4eb":"code","8f5bae04":"code","d1a45f90":"code","6b9b7d1a":"code","363efe4f":"code","0683a493":"code","9ac16311":"code","875c448c":"code","008a3b63":"code","91ab3cc6":"code","3b2e8703":"code","633ed055":"code","212f5b75":"code","29001900":"code","0d93711e":"code","8fb36967":"code","d5c20710":"code","7df0ead6":"code","378fb6cc":"code","69d2d3b2":"code","ab1cd9d1":"code","2cd91646":"code","5c71eeb4":"code","fc6ec498":"code","68357f09":"code","bd1558bb":"code","07423123":"code","7fdd1976":"code","57704a5b":"code","f73bc572":"code","5471f50d":"code","0397ba13":"code","9cb5524a":"code","7cfe8fea":"code","d0353d4b":"code","d236b393":"code","74643ef2":"code","f3aadb40":"code","6010b9d1":"code","f3786b6b":"code","247a0bcb":"code","54747d90":"code","431dbd4d":"code","71415b12":"code","8d9838c7":"code","ba2d9511":"code","bb4b7bea":"code","b96ba5e6":"code","29c1f627":"code","9020797c":"code","37b81555":"code","c0faf8f6":"code","4eb4c01d":"code","e74f4404":"code","f955c5cf":"code","0659e9f8":"code","7eebfca3":"code","4d3c5905":"code","470e5d25":"code","d21cb3e3":"code","46961a90":"code","0603e6e1":"code","b55a0d8f":"code","43217793":"code","f7286774":"code","12c3bc4b":"code","82d142bc":"code","2de6dad0":"code","72f44433":"code","609e0e9d":"code","105db918":"code","bb229217":"code","c979bd84":"code","7a9eafc6":"code","3252314d":"code","aa893983":"code","08cd3db7":"code","62cf973b":"code","f50ada70":"code","7d43419b":"code","33aa91f8":"code","390b4311":"code","ece7ddba":"code","24376741":"code","4d819b45":"code","35d06ed2":"code","a43f1810":"code","1c1eadd1":"code","00ba5e27":"code","5824957e":"code","7609fccd":"code","d69207e6":"code","d4f7e9a3":"code","468add12":"code","85854df8":"code","51173ad7":"code","b3c157d4":"code","6a8c49a2":"code","f7c4c86c":"code","7860d7ab":"code","c1e22428":"code","a194aa63":"code","683f002c":"code","6ae4bdc9":"code","19bb4dc9":"code","31fa6fef":"code","ad299a2d":"code","1dea57de":"code","364a2f68":"code","5841acef":"code","faa97c45":"code","63042cb5":"code","9a4d8e65":"code","0d7946cc":"code","7b11235b":"code","9cc29a6c":"code","f20afcc7":"code","3628a09e":"code","76cd1153":"code","a63da5a4":"code","047df17c":"code","0449f3db":"code","d8317f18":"code","35349870":"code","bf68db69":"code","9e9b3d8b":"code","ad4d4535":"code","09a5865b":"markdown","1194c47b":"markdown","c6e335e9":"markdown","8abe89dc":"markdown","f7c2c9bc":"markdown","4c19fdac":"markdown","ac6f0ddb":"markdown","f519ccf2":"markdown","6d5cb1b4":"markdown","0e568a88":"markdown","701907d1":"markdown","da1a1492":"markdown","185dd2a0":"markdown","c51cddbd":"markdown","3e41285a":"markdown","149e0b17":"markdown","4290f000":"markdown","b07c115b":"markdown","f39c0db8":"markdown","673a16fa":"markdown","6794cca4":"markdown","c46d6e04":"markdown","f3a18b87":"markdown","f211e376":"markdown","a6c0022e":"markdown","f6861e80":"markdown","48e62316":"markdown","d0f30d89":"markdown","1630fae0":"markdown","1f63b13f":"markdown","2441a28d":"markdown","47644af2":"markdown","af2d664a":"markdown","ca298bd8":"markdown","6daab620":"markdown","487be290":"markdown","449234c0":"markdown","c3ceefa5":"markdown","a932c389":"markdown","b49f63f3":"markdown","2a6ac1b4":"markdown","6fa55066":"markdown","5b801ada":"markdown","f721f1db":"markdown","21c30b2c":"markdown","efe4e157":"markdown","a67420f4":"markdown","c5fc70af":"markdown","d808fc46":"markdown","e8308674":"markdown","fcfbce4b":"markdown","706fe4ad":"markdown","f790374e":"markdown"},"source":{"04475719":"#library\nimport os\nfrom datetime import datetime\nimport time\nfrom sklearn.preprocessing import StandardScaler\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\n\nimport statsmodels.api as sm\nfrom statsmodels.tsa.ar_model import AutoReg\nfrom statsmodels.tsa.arima_model import ARMA\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom scipy import stats\nfrom itertools import product\n\nfrom plotly.subplots import make_subplots\n\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\n\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n        \n        \ninfo = pd.read_csv(\"\/kaggle\/input\/g-research-crypto-forecasting\/asset_details.csv\")\nctrain = pd.read_csv(\"\/kaggle\/input\/g-research-crypto-forecasting\/train.csv\")\n","a0c36176":"ctrain.head()","dbc18446":"info.Asset_Name.unique()","ef28314f":"ctrain.columns","099ae4e4":"#time frame that we have\npd.to_datetime(ctrain['timestamp'], unit='s')","8728a132":"#Dictionary for assets\nrename_dict = {}\nasset_details = info \nfor a in asset_details['Asset_ID']:\n    rename_dict[a] = asset_details[asset_details.Asset_ID == a].Asset_Name.values[0]\n\ndisplay(rename_dict)","538e2e3b":"# Convert timestamp\ntrain  = ctrain.copy()\ntrain['timestamp'] = train['timestamp'].astype('datetime64[s]')\n#supplemental_train['timestamp'] = supplemental_train['timestamp'].astype('datetime64[s]')\n#example_test['timestamp'] = example_test['timestamp'].astype('datetime64[s]')\n\n#train['date'] = read_DatetimeIndex(train['timestamp']).date\n#supplemental_train['date'] = pd.DatetimeIndex(supplemental_train['timestamp']).date\n#example_test['date'] = pd.DatetimeIndex(example_test['timestamp']).date\n\n# Resample\ntrain_daily = pd.DataFrame()\n\nfor asset_id in asset_details.Asset_ID:\n    train_single = train[train.Asset_ID == asset_id].copy()\n\n    train_single_new = train_single[['timestamp','Count']].resample('D', on='timestamp').sum()\n    train_single_new['Open'] = train_single[['timestamp','Open']].resample('D', on='timestamp').first()['Open']\n    train_single_new['High'] = train_single[['timestamp','High']].resample('D', on='timestamp').max()['High']\n    train_single_new['Low'] = train_single[['timestamp','Low']].resample('D', on='timestamp').min()['Low']\n    train_single_new['Close'] = train_single[['timestamp','Close']].resample('D', on='timestamp').last()['Close']\n    train_single_new['Volume'] = train_single[['timestamp','Volume']].resample('D', on='timestamp').sum()['Volume']\n    # train_single_new['VWAP']\n    #train_single_new['Target'] = train_single[['timestamp','Target']].resample('D', on='timestamp').mean()['Target']\n    train_single_new['Asset_ID'] = asset_id\n\n    train_daily = train_daily.append(train_single_new.reset_index(drop=False))\ntrain_daily = train_daily.sort_values(by = ['timestamp', 'Asset_ID']).reset_index(drop=True)\n\ntrain_daily = train_daily.pivot(index='timestamp', columns='Asset_ID')[['Count', 'Open', 'High', 'Low', 'Close', 'Volume']]\ntrain_daily = train_daily.reset_index(drop=False)\n\ndisplay(train_daily.head(10))","5674ff8e":"#visualize Bitcoin for recent data rows - last 200 rows\ncrypto_df = ctrain\n\ncrypto_df.index = pd.to_datetime(crypto_df.timestamp, unit='s')\nbtc = crypto_df[crypto_df[\"Asset_ID\"]==1] # Asset_ID = 1 for Bitcoin\nbtc_mini = btc.iloc[-200:] # Select recent data rows\n\n\nimport plotly.graph_objects as go\n\nfig = go.Figure(data=[go.Candlestick(x=btc_mini.index, open=btc_mini['Open'], high=btc_mini['High'], low=btc_mini['Low'], close=btc_mini['Close'])])\nfig.show()","4e05f54c":"data = ctrain\n\nasset_count= []\nfor i in range(14):\n    count = (data[\"Asset_ID\"]==i).sum()\n    asset_count.append(count)\nfig = px.bar(x = asset_details.sort_values(\"Asset_ID\")[\"Asset_Name\"],\n             y = asset_count , \n             color = asset_count ,\n             color_continuous_scale=\"Emrld\") \nfig.update_xaxes(title=\"Assets\")\nfig.update_yaxes(title = \"Number of Rows\")\nfig.update_layout(showlegend = True,\n    title = {\n        'text': 'Data Distribution ',\n        'y':0.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\nfig.show()","5b5d70d3":"# Impute Missing Time Value\n\ndef c_time_sub(asset_id, data=ctrain):\n    df=data[ctrain[\"Asset_ID\"]==asset_id].set_index(\"timestamp\")\n    df=df.reindex(range(df.index[0],df.index[-1]+60,60), method=\"pad\")\n    df.index = pd.to_datetime(df.index, unit='s')\n    return df\n\n\n#  Bitcoin\nbtc=c_time_sub(asset_id=1)\n\n#  Ethereum\neth=c_time_sub(asset_id=6)\n\n#  Cardano\nada=c_time_sub(asset_id=3)\n","8bf71424":"btc.head()","e23afd18":"import time\n\n# define function to compute log returns\ndef log_return(series, periods=1):\n    return np.log(series).diff(periods=periods)\ndef plot_corr_2021(ctrain, info):\n    # auxiliary function, from datetime to timestamp\n    totimestamp = lambda s: np.int32(time.mktime(datetime.strptime(s, \"%d\/%m\/%Y\").timetuple()))\n    # create dataframe with returns for all assets\n    \n    crypto_df = ctrain\n    all_assets_2021 = pd.DataFrame([])\n    \n    for asset_id, asset_name in rename_dict.items():\n        print(asset_name)\n        asset = crypto_df[crypto_df[\"Asset_ID\"]==asset_id].set_index(\"timestamp\")\n        asset = asset.loc[totimestamp('01\/01\/2021'):]\n        asset = asset.reindex(range(asset.index[0],asset.index[-1]+60,60),method='pad')\n        lret = log_return(asset.Close.fillna(0))[1:]\n        \n        all_assets_2021 = all_assets_2021.join(lret, rsuffix=asset_name, how=\"outer\")\n    all_assets_2021.rename(columns = {'Close':'CloseBitcoin Cash'}, inplace=True)\n    sns.heatmap(all_assets_2021.corr());\n    \"\"\"plt.imshow(all_assets_2021.corr());\n    plt.yticks(info.Asset_ID.values, info.Asset_Name.values);\n    plt.xticks(info.Asset_ID.values, info.Asset_Name.values, rotation='vertical');\n    plt.colorbar();\n    plt.savefig('corr.jpg')\"\"\"\n    all_assets_2021_corr =  all_assets_2021.corr()\n    \"\"\"all_assets_2021_corr.columns = info.Asset_Name.values\n    all_assets_2021_corr.index = info.Asset_Name.values\"\"\"\n    return all_assets_2021, all_assets_2021_corr\nall_assets_2021, all_assets_2021_corr = plot_corr_2021(ctrain, info)","a2a3b160":"all_assets_2021_corr.columns","a22b8041":"#the top correlation\ndef get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_abs_correlations(df, n=5, ascending=False):\n    au_corr = df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=ascending)\n    return au_corr[0:n]\npd.DataFrame(get_top_abs_correlations(all_assets_2021_corr, n=10, ascending=False), columns=['Correlation'])\n","a338b395":"#least correlated\npd.DataFrame(get_top_abs_correlations(all_assets_2021_corr, n=10, ascending=True), columns=['Correlation'])\n","e2ff403b":"rename_dict[2]\n","16460fa0":"#correlations over time\n# join two asset in single DataFrame\ndef corr_over_time(crypto_df, asset_id_1 =1, asset_id_2 = 6):\n    btc = crypto_df[crypto_df[\"Asset_ID\"]==asset_id_1].set_index(\"timestamp\")\n    eth= crypto_df[crypto_df[\"Asset_ID\"]==asset_id_2].set_index(\"timestamp\")\n\n    lret_btc_long = log_return(btc.Close)[1:]\n    lret_eth_long = log_return(eth.Close)[1:]\n    lret_btc_long.rename('lret_btc', inplace=True)\n    lret_eth_long.rename('lret_eth', inplace=True)\n    two_assets = pd.concat([lret_btc_long, lret_eth_long], axis=1)\n\n    # group consecutive rows and use .corr() for correlation between columns\n    corr_time = two_assets.groupby(two_assets.index\/\/(10000*60)).corr().loc[:,\"lret_btc\"].loc[:,\"lret_eth\"]\n    return corr_time\n\n    #corr_time.plot();\n    \"\"\"plt.xticks([])\n    plt.ylabel(\"Correlation\")\n    plt.title(\"Correlation between BTC and ETH over time\");\"\"\"\n\nfig, ax = plt.subplots(nrows=14, ncols=14, figsize=(30, 25))\nfor i in range(0,13):\n    for j in range(i+1,14):\n        g = sns.lineplot(data = corr_over_time(ctrain,asset_id_1 =i, asset_id_2 = j),ax=ax[i,j])\n        g.set(xlabel=rename_dict[i], ylabel=rename_dict[j])\n","d3c7a24b":"# Plot the Closing Price for BTC, ETH, ADA (Cardano)\nf= plt.figure(figsize=(10,12))  \n\ndef gplot(no , data, price, label, ylabel, color):\n    ax=f.add_subplot(no)\n    plt.plot(data[price], label=label, color=color)\n    plt.legend()\n    plt.xlabel(\"Time\")\n    plt.ylabel(ylabel)\n    return plt\n\ngplot(no=311, data=btc, price=\"Close\" , label=\"BTC 2021 Overall Performance\", ylabel=\"BTC Closing Price\", color=\"Lightskyblue\")\ngplot(no=312, data=eth, price=\"Close\" ,label=\"ETH 2021 Overall Performance\", ylabel=\"ETH Closing Price\", color=\"Coral\")\ngplot(no=313, data=ada, price=\"Close\" ,label=\"Cardano 2021 Overall Performance\", ylabel=\"ADA Closing Price\", color=\"khaki\")\n\nplt.tight_layout()\nplt.show()","3f251a50":"#Target : 15 minute resudualized returns\n#Residual Return: An asset's residual return equals its excess return minus beta times the benchmark excess return.\n\nf= plt.figure(figsize=(10,12))  \ngplot(no=311, data=btc, price=\"Target\" , label=\"BTC 2021 15min Return Residue\", ylabel=\"BTC residual return\", color=\"Aqua\")\ngplot(no=312, data=eth, price=\"Target\" ,label=\"ETH 2021 15min Return Residue\", ylabel=\"ETH residual return\", color=\"Pink\")\ngplot(no=313, data=ada, price=\"Target\" ,label=\"ADA 2021 15min Return Residue\", ylabel=\"ADA residual return\", color=\"gold\")\n\n\nplt.tight_layout()\nplt.show()","9b971d6d":"#candlestick\n\ndef c_chart(data,label):\n    candlestick = go.Figure(data = [go.Candlestick(x =data.index, \n                                               open = data[('Open')], \n                                               high = data[('High')], \n                                               low = data[('Low')], \n                                               close = data[('Close')])])\n    candlestick.update_xaxes(title_text = 'Time',\n                             rangeslider_visible = True)\n\n    candlestick.update_layout(\n    title = {\n        'text': '{:} Candelstick Chart'.format(label),\n        \"y\":0.8,\n        \"x\":0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n\n    candlestick.update_yaxes(title_text = 'Price in USD', ticksuffix = '$')\n    return candlestick\n\n%matplotlib inline\nbtc_candle=c_chart(btc[-90:], label=\"BTC Price\")\nbtc_candle.show()\n\neth_candle=c_chart(eth[-90:], label=\"ETH Price \")\neth_candle.show()","d5ddc34e":"ctrain.head()","5d68b4eb":"btc.head()","8f5bae04":"rename_dict","d1a45f90":"\ndef mov_avg(df, a = 'W',Asset_ID =1, N = 10):\n    \n    df = df[df.Asset_ID == Asset_ID]\n    \n    df.set_index(df['timestamp'], inplace=True)\n    df.drop('timestamp', axis=1, inplace=True)\n    df.index = pd.to_datetime(df.index, unit='s')\n\n    #weekly\n    convertion={\n        'Open':'first',\n        'High':'max',\n        'Low':'min',\n        'Close':'mean',\n        'Volume':'sum',    \n    }\n    ds_df = df.resample(a).apply(convertion)\n    #avg # of mins in a week\n\n    \n    \n    #Simple ma\n    ds_df['rolling_mean' + str(a) + '_' + str(5)] = ds_df.Close.rolling(window=5).mean()\n    ds_df['rolling_mean' + str(a) + '_' + str(10)] = ds_df.Close.rolling(window=10).mean()\n    fig = go.Figure(go.Candlestick(x=ds_df.index,open=ds_df['Open'],high=ds_df['High'],low=ds_df['Low'],close=ds_df['Close']))\n    fig.update_layout(title='SMA '+str(rename_dict[Asset_ID])+' Close', yaxis_title=str(rename_dict[Asset_ID]))\n    fig.update_yaxes(type=\"log\")\n    fig.add_trace(go.Scatter(x=ds_df.index, y=ds_df['Close'],mode='lines',name='Close'))\n    fig.add_trace(go.Scatter(x=ds_df.index, y=ds_df['rolling_mean' + str(a) + '_' + str(5)], mode='lines', name='SMA MEAN of 5 ' + str(a),line=dict(color='royalblue', width=2)))\n    fig.add_trace(go.Scatter(x=ds_df.index, y=ds_df['rolling_mean' + str(a) + '_' + str(10)], mode='lines', name='SMA MEAN of 10 ' + str(a), line=dict(color='#555555', width=2)))\n    fig.show()\n    \n    \n    #Exp ma\n    ewma = pd.Series.ewm\n    ds_df['rolling_ema_'+ str(N)]  = ds_df.Close.ewm(min_periods=N, span=N).mean()\n    fig = go.Figure(go.Candlestick(x=ds_df.index,open=ds_df['Open'],high=ds_df['High'],low=ds_df['Low'],close=ds_df['Close']))\n    fig.update_layout(title='EMA ' +str(rename_dict[Asset_ID])+' Close', yaxis_title=str(rename_dict[Asset_ID]))\n    fig.update_yaxes(type=\"log\")\n    fig.add_trace(go.Scatter(x=ds_df.index, y=ds_df['Close'],mode='lines',name='Close'))\n    fig.add_trace(go.Scatter(x=ds_df.index, y=ds_df['rolling_ema_' + str(N)], mode='lines', name='EMA MEAN span of '+str(N)+' rolled by '+ str(a),line=dict(color='royalblue', width=2)))\n    fig.show()\n    \n    \n \n\n    \n    \n    \n    \n    \n    \nmov_avg(ctrain, Asset_ID =1 )\n","6b9b7d1a":"#ETH\nmov_avg(ctrain, Asset_ID =6 )","363efe4f":"info = pd.read_csv(\"\/kaggle\/input\/g-research-crypto-forecasting\/asset_details.csv\")\nctrain = pd.read_csv(\"\/kaggle\/input\/g-research-crypto-forecasting\/train.csv\")","0683a493":"ctrain.head()","9ac16311":"import time\n\n# define function to compute log returns\ndef log_return(series, periods=1):\n    return np.log(series).diff(periods=periods)\ndef logret_all_assets(ctrain, info):\n    # auxiliary function, from datetime to timestamp\n    totimestamp = lambda s: np.int32(time.mktime(datetime.strptime(s, \"%d\/%m\/%Y\").timetuple()))\n    # create dataframe with returns for all assets\n    \n    crypto_df = ctrain\n    all_assets_2021 = pd.DataFrame([])\n    \n    for asset_id, asset_name in rename_dict.items():\n        #print(asset_name)\n        asset = crypto_df[crypto_df[\"Asset_ID\"]==asset_id].set_index(\"timestamp\")\n        #asset = asset.loc[totimestamp('01\/01\/2021'):]\n        asset = asset.reindex(range(asset.index[0],asset.index[-1]+60,60),method='pad')\n        lret = log_return(asset.Close.fillna(0))[1:]\n        \n        all_assets_2021 = all_assets_2021.join(lret, rsuffix=asset_name, how=\"outer\")\n    all_assets_2021.rename(columns = {'Close':'Close Bitcoin Cash'}, inplace=True)\n    all_assets_2021.index  = pd.to_datetime(all_assets_2021.index, unit='s')\n    return all_assets_2021\nall_assets = logret_all_assets(ctrain, info)","875c448c":"#logret of all assets\nall_assets.head()","008a3b63":"all_assets.to_csv('logret_all_years_assets.csv')","91ab3cc6":"sns.boxplot(data = all_assets)","3b2e8703":"all_assets.head()","633ed055":"stat = all_assets.describe()","212f5b75":"stat","29001900":"stat.loc['mean',:].sort_values()","0d93711e":"\n\nstat.loc['std',:].sort_values()","8fb36967":"all_assets.describe()","d5c20710":"all_assets.columns","7df0ead6":"# CREATE A FUNCTION THAT CALCULATE REALIZED VOLATILITY for btc\n# FROM minute-by-minute LOG RETURNS\ndef realized_volatility_daily(series_log_return):\n    \"\"\"\n    Get the daily realized volatility which is calculated as the square root\n    of sum of squares of log returns within a specific window interval \n    \"\"\"\n    n = len(series_log_return)\n    series_log_return = series_log_return.fillna(0)\n    return np.sqrt(np.sum(series_log_return**2)\/(n - 1))\n\nk =24\nintervals = [ 7*k]\n#, 60 *k, 180 *k, 365*k\nvols_df = {}\n\n# ITERATE OVER intervals LIST\nfor i in intervals:\n    # GET DAILY LOG RETURNS USING THAT INTERVAL\n    vols = all_assets.CloseBitcoin.rolling(window=i)\\\n                         .apply(realized_volatility_daily).values\n\n    vols_df[i] = vols\n\n# CONVERT vols_df FROM DICTIONARY TO PANDAS DATAFRAME\nvols_df = pd.DataFrame(vols_df, columns=intervals, index=df.index)","378fb6cc":"vols_df","69d2d3b2":"# CHANGING MATPLOTLIB STYLE\nplt.style.use(['fivethirtyeight'])\n\nfig, ax = plt.subplots(figsize=(18,7))\n\nfor i in intervals:\n    if i == 7:\n        alpha = 0.5\n        lw = 1\n    else:\n        alpha = 1.0\n        lw = 2\n    ax.plot(vols_df[i], label=f'{i}-Day Interval Realized Volatility', \n            alpha=alpha, lw=lw)\n\nax.set_title('Realized Volatility Using Different Interval Windows', fontsize=21)\n\nplt.legend(loc='best', prop={'size': 14})\nplt.savefig(os.path.join(directory_to_img, 'diff_intervals.png'), \n            dpi=300, bbox_inches='tight')\nplt.show();","ab1cd9d1":"sns.boxplot(data = all_assets_2021)","2cd91646":"stat_2021 = all_assets_2021.describe()","5c71eeb4":"stat_2021.loc['mean',:].sort_values()","fc6ec498":"stat_2021.loc['mean',:].sort_values()","68357f09":"sns.pairplot(all_assets_2021)","bd1558bb":"# Imports\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n%config InlineBackend.figure_format='retina'\nwarnings.filterwarnings( \"ignore\", module = \"matplotlib\\..*\" )\n\n\ndef vis_pca(all_assets):\n    X_std = StandardScaler().fit_transform(all_assets.fillna(0))\n    pca = PCA()\n    X_std_pca = pca.fit_transform(X_std)\n    plot = plt.scatter(X_std_pca[:,0], X_std_pca[:,1])\n    #plt.legend(handles=plot.legend_elements()[0])\n    plt.show()\n\n    # Plot the explained variances\n    features = range(pca.n_components_)\n    plt.bar(features, pca.explained_variance_ratio_, color='black')\n    plt.xlabel('PCA features')\n    plt.ylabel('variance %')\n    plt.xticks(features)\n    plt.show()\nvis_pca(all_assets)","07423123":"\n\nvis_pca(all_assets.resample('M').mean())","7fdd1976":"vis_pca(all_assets.resample('W').mean())","57704a5b":"\n\n\n\nX_std = StandardScaler().fit_transform(all_assets.fillna(0))\npca = PCA()\nX_std_pca = pca.fit_transform(X_std)\nplot = plt.scatter(X_std_pca[:,0], X_std_pca[:,1])\n#plt.legend(handles=plot.legend_elements()[0])\nplt.show()\n\n# Plot the explained variances\nfeatures = range(pca.n_components_)\nplt.bar(features, pca.explained_variance_ratio_, color='black')\nplt.xlabel('PCA features')\nplt.ylabel('variance %')\nplt.xticks(features)\n\n","f73bc572":"btc.head()","5471f50d":"#freq https:\/\/stackoverflow.com\/questions\/17001389\/pandas-resample-documentation\n#explanation https:\/\/towardsdatascience.com\/time-series-decomposition-and-statsmodels-parameters-69e54d035453\nplt.rcParams[\"figure.figsize\"]=(15,7)\n\ndef season_df(data, label, a):\n    df=data.resample(a).mean()\n    seasonal_decompose(df.Close).plot()\n    print(label)\n    return plt.show()\nseason_df(data=btc, label=\"BTC Seasonal Decomposition\",a =\"M\")\n","0397ba13":"season_df(data=btc, label=\"BTC Seasonal Decomposition\",a =\"D\")","9cb5524a":"season_df(data=btc, label=\"BTC Seasonal Decomposition\",a =\"H\")","7cfe8fea":"\n# CALCULATE PRICE RETURNS AS DAILY PERCENTAGE CHANGE USING pct_change()\nbtc['returns'] = 100 * btc.Close.pct_change().dropna()\n\n# CALCULATE LOG RETURNS BASED ON ABOVE FORMULA\nbtc['log_returns'] = np.log(btc.Close\/btc.Close.shift(1))\nbtc.dropna(inplace=True)\n\n# PLOT DISTRIBUTION PLOTS OF RETURNS & LOG RETURNS\n# AND VISUALLY COMPARE THEM WITH THE STANDARD NORMAL DISTRIBUTION\ndf = btc.copy()\nwith sns.axes_style(\"darkgrid\"):\n    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(18,12))\n\n    axes[0][0].plot(df.returns, color='blue')\n    axes[0][0].set_title('Returns')\n\n    sns.distplot(df.returns, norm_hist=True, fit=stats.norm, color='blue',\n                bins=50, ax=axes[0][1])\n    axes[0][1].set_title('Returns')\n\n    axes[1][0].plot(df.log_returns, color='green')\n    axes[1][0].set_title('Log Returns')\n\n    sns.distplot(df.log_returns, norm_hist=True, fit=stats.norm, color='green',\n                bins=50, ax=axes[1][1])\n    axes[1][1].set_title('Log Returns')\n    plt.tight_layout()\n\n    fig.show();","d0353d4b":"from scipy.stats import kurtosis, skew\n\n#returns\nprint( 'returns: excess kurtosis of normal distribution (should be 0): {}'.format( kurtosis(df.returns) ))\nprint( 'returns: skewness of normal distribution (should be 0): {}'.format( skew(df.returns) ))\n\n#log_returns\nprint( 'log_returns: excess kurtosis of normal distribution (should be 0): {}'.format( kurtosis(df.log_returns) ))\nprint( 'log_returns: skewness of normal distribution (should be 0): {}'.format( skew(df.log_returns) ))","d236b393":"btc.head()","74643ef2":"#Box-Cox Transformation\nbtc_month=btc.resample(\"M\").mean()\nbtc_month[\"close_box\"], lmbda=stats.boxcox(btc_month.Close)\nprint(\"Dickey\u2013Fuller test: p=%f\" % adfuller(btc_month.close_box)[1])\n","f3aadb40":"# Seasonal differentiation (12 months)\nbtc_month['box_diff_seasonal_12'] = btc_month.close_box - btc_month.close_box.shift(12)\nprint(\"Dickey\u2013Fuller test: p=%f\" % adfuller(btc_month.box_diff_seasonal_12[12:])[1])","6010b9d1":"# Seasonal differentiation (3 months)\nbtc_month['box_diff_seasonal_3'] = btc_month.close_box - btc_month.close_box.shift(3)\nprint(\"Dickey\u2013Fuller test: p=%f\" % adfuller(btc_month.box_diff_seasonal_3[3:])[1])","f3786b6b":"btc_month.box_diff_seasonal_12.head()","247a0bcb":"# Regular differentiation (second diff for 12 months)\nbtc_month['box_diff2'] = btc_month.box_diff_seasonal_12 - btc_month.box_diff_seasonal_12.shift(1)\n\n# STL-decomposition\nseasonal_decompose(btc_month.box_diff2[13:]).plot()   \nprint(\"Dickey\u2013Fuller test: p=%f\" % adfuller(btc_month.box_diff2[13:])[1])\n\nplt.show()","54747d90":"#autocorrelation_plot(btc_month.close)\nplot_acf(btc_month.Close[13:].values.squeeze(), lags=12)\n\nplt.tight_layout()\nplt.show()","431dbd4d":"# Initial approximation of parameters using Autocorrelation and Partial Autocorrelation Plots\nax = plt.subplot(211)\n# Plot the autocorrelation function\n#sm.graphics.tsa.plot_acf(btc_month.box_diff2[13:].values.squeeze(), lags=48, ax=ax)\nplot_acf(btc_month.box_diff2[13:].values.squeeze(), lags=12, ax=ax)\nax = plt.subplot(212)\n#sm.graphics.tsa.plot_pacf(btc_month.box_diff2[13:].values.squeeze(), lags=48, ax=ax)\nplot_pacf(btc_month.box_diff2[13:].values.squeeze(), lags=12, ax=ax)\nplt.tight_layout()\nplt.show()","71415b12":"#start=datetime(2018, 1, 31), end=datetime(2021, 5, 31)\n\nbtc_month.index","8d9838c7":"qs = range(0, 3)\nps = range(0, 3)\nd=1\nparameters = product(ps, qs)\nparameters_list = list(parameters)\nlen(parameters_list)\n\n# Model Selection\nresults = []\nbest_aic = float(\"inf\")\nwarnings.filterwarnings('ignore')\nfor param in parameters_list:\n    try:\n        model = SARIMAX(btc_month['2018-01-31':'2021-05-31'].close_box, order=(param[0], d, param[1])).fit(disp=-1)\n    except ValueError:\n        print('bad parameter combination:', param)\n        continue\n    aic = model.aic\n    if aic < best_aic:\n        best_model = model\n        best_aic = aic\n        best_param = param\n    results.append([param, model.aic])","ba2d9511":"result_table = pd.DataFrame(results)\nresult_table.columns = ['parameters', 'aic']\nprint(result_table.sort_values(by = 'aic', ascending=True).head())\nprint(best_model.summary())","bb4b7bea":"print(\"Dickey\u2013Fuller test:: p=%f\" % adfuller(best_model.resid[13:])[1])\n","b96ba5e6":"best_model.plot_diagnostics(figsize=(15, 12))\nplt.show()","29c1f627":"btc_month.head()","9020797c":"#one-step ahead predictions\ndef invboxcox(y,lmbda):\n    if lmbda == 0:\n        return(np.exp(y))\n    else:\n        return(np.exp(np.log(lmbda*y+1)\/lmbda))\n# Prediction\nbtc_month_pred = btc_month[['Close']]\n\nbtc_month_pred['forecast'] = invboxcox(best_model.predict(start=datetime(2018, 1, 31), end=datetime(2021, 9, 21)), lmbda)\nplt.figure(figsize=(18,10))\nbtc_month_pred.Close.plot()\nbtc_month_pred.forecast.plot(color='r', ls='--', label='Predicted Close')\nplt.legend()\nplt.title('Bitcoin monthly forecast')\nplt.ylabel('USD')\nplt.show()","37b81555":"##Addd SARIMAX (week 6)\n##Add rmse\n##Add other metric that is used in the competition","c0faf8f6":"# Initial approximation of parameters\nfrom itertools import product\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\n\nQs = range(0, 2)\nqs = range(0, 3)\nPs = range(0, 3)\nps = range(0, 3)\nD=1\nd=1\nparameters = product(ps, qs, Ps, Qs)\nparameters_list = list(parameters)\nlen(parameters_list)\n\n# Model Selection\nresults = []\nbest_aic = float(\"inf\")\nwarnings.filterwarnings('ignore')\nfor param in parameters_list:\n    try:\n#        model = SARIMAX(btc_month.close_box, order=(param[0], d, param[1]), seasonal_order=(param[2], D, param[3], 12)).fit(disp=-1)\n        model = SARIMAX(btc_month.close_box, order=(param[0], d, param[1]), seasonal_order=(param[2], D, param[3], 4)).fit(disp=-1)\n    except ValueError:\n        print('bad parameter combination:', param)\n        continue\n    aic = model.aic\n    if aic < best_aic:\n        best_model = model\n        best_aic = aic\n        best_param = param\n    results.append([param, model.aic])","4eb4c01d":"# Best Models\nresult_table = pd.DataFrame(results)\nresult_table.columns = ['parameters', 'aic']\nprint(result_table.sort_values(by = 'aic', ascending=True).head())\nprint(best_model.summary())","e74f4404":"print(\"Dickey\u2013Fuller test:: p=%f\" % adfuller(best_model.resid[13:])[1])\n","f955c5cf":"best_model.plot_diagnostics(figsize=(15, 12))\nplt.show()","0659e9f8":"# STL-decomposition\nplt.figure(figsize=(15,7))\nplt.subplot(211)\nbest_model.resid[13:].plot()\nplt.ylabel(u'Residuals')\nax = plt.subplot(212)\n#sm.graphics.tsa.plot_acf(best_model.resid[13:].values.squeeze(), lags=48, ax=ax)\nplot_acf(best_model.resid[13:].values.squeeze(), lags=12, ax=ax)\n\nprint(\"Dickey\u2013Fuller test:: p=%f\" % adfuller(best_model.resid[13:])[1])\n\nplt.tight_layout()\nplt.show()","7eebfca3":"btc_month.columns","4d3c5905":"btc_month.index","470e5d25":"# Prediction\nbtc_month2 = btc_month[['Close']]\ndate_list = [datetime(2018, 1, 31), datetime(2018, 4, 30), datetime(2018, 5, 31), datetime(2018, 6, 30)]\nfuture = pd.DataFrame(index=date_list, columns= btc_month.columns)\nbtc_month2 = pd.concat([btc_month2, future])\n\nbtc_month2['forecast'] = invboxcox(best_model.predict(start=0, end=75), lmbda)\n\nplt.figure(figsize=(15,7))\nbtc_month2.Close.plot()\nbtc_month2.forecast.plot(color='r', ls='--', label='forecast')\nplt.legend()\nplt.title('Bitcoin Monthly Close Forecast')\nplt.ylabel('USD')\nplt.savefig('bitcoin_monthly_forecast.png')\nplt.show()","d21cb3e3":"y_forecasted = btc_month2.forecast\ny_truth = btc_month2['2015-01-01':'2017-01-01'].Close\n\n# Compute the root mean square error\nrmse = np.sqrt(((y_forecasted - y_truth) ** 2).mean())\nprint('Mean Squared Error: {}'.format(round(rmse, 2)))","46961a90":"import os\nimport gc\nimport traceback\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport gresearch_crypto\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nimport pandas as pd, numpy as np\nfrom tensorflow.keras import layers\nimport tensorflow_probability as tfp\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import RobustScaler\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\npd.set_option('display.max_columns', None)","0603e6e1":"train_df = pd.read_csv('\/kaggle\/input\/g-research-crypto-forecasting\/train.csv')","b55a0d8f":"# Impute Missing Time Value\n\"\"\"def c_time_sub(asset_id, data=ctrain):\n    df=data[ctrain[\"Asset_ID\"]==asset_id].set_index(\"timestamp\")\n    df=df.reindex(range(df.index[0],df.index[-1]+60,60), method=\"pad\")\n    return df\"\"\"\n\ntrain_df.index=train_df['timestamp']\ntrain = train_df.sort_index()\nind = train.index.unique()\ndef reindex(df):\n    df = df.reindex(range(ind[0],ind[-1]+60,60),method='nearest')\n    df = df.fillna(method=\"ffill\").fillna(method=\"bfill\")\n    return df\ntrain = train.groupby('Asset_ID').apply(reindex).reset_index(0, drop=True).sort_index()\ngc.collect()","43217793":"train = train.reset_index(drop = True)\ntrain[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target']] = train[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target']].astype(np.float32)\ntrain['Target'] = train['Target'].fillna(0)\n\nVWAP_max = np.max(train[np.isfinite(train.VWAP)].VWAP)\nVWAP_min = np.min(train[np.isfinite(train.VWAP)].VWAP)\ntrain['VWAP'] = np.nan_to_num(train.VWAP, posinf=VWAP_max, neginf=VWAP_min)\ndf = train[['Asset_ID', 'Target']].copy()\n\ntimes = dict((t,i) for i,t in enumerate(df.index.unique()))\n\ndf['id'] = df.index.map(times)\ndf['id'] = df['id'].astype(str) + '_' + df['Asset_ID'].astype(str)\nids = df.id.copy()\ndel df\n\n\ntrain = get_features(train)\n\ntrain_features = [i for i in train.columns if i not in ['Target', 'date', 'timestamp', 'Asset_ID', 'groups']]\ntrain.head()","f7286774":"train['Target'] = train['Target'].fillna(0)\n","12c3bc4b":"train['Target'].head()","82d142bc":"DEVICE = \"TPU\" #or \"GPU\"\n\nSEED = 42\n\nEPOCHS = 10\nDEBUG = True\nN_ASSETS = 1\nWINDOW_SIZE = 15\nBATCH_SIZE = 1024\nPCT_VALIDATION = 10 # last 10% of the data are used as validation set","2de6dad0":"train = train_df.copy()\n\ntargets = train['Target'].to_numpy().reshape(-1, N_ASSETS)\nfeatures = train.columns.drop(['Asset_ID', 'Target','timestamp'])\ntrain = train[features]\ntrain = train.values\ntrain = train.reshape(-1, N_ASSETS, train.shape[-1])","72f44433":"class sample_generator(keras.utils.Sequence):\n    def __init__(self, x_set, y_set, batch_size, length):\n        self.x, self.y = x_set, y_set\n        self.batch_size = batch_size\n        self.length = length\n        self.size = len(x_set)\n    def __len__(self): return int(np.ceil(len(self.x) \/ float(self.batch_size)))\n    def __getitem__(self, idx):\n        batch_x=[]\n        batch_y=[]\n        for i in range(self.batch_size):\n            start_ind = self.batch_size*idx + i\n            end_ind = start_ind + self.length \n            if end_ind <= self.size:\n                batch_x.append(self.x[start_ind : end_ind])\n                batch_y.append(self.y[end_ind -1])\n        return np.array(batch_x), np.array(batch_y)","609e0e9d":"X_train, X_test = train[:-len(train)\/\/PCT_VALIDATION], train[-len(train)\/\/PCT_VALIDATION:]\ny_train, y_test = targets[:-len(train)\/\/PCT_VALIDATION], targets[-len(train)\/\/PCT_VALIDATION:]","105db918":"train_generator = sample_generator(X_train, y_train, length = WINDOW_SIZE, batch_size = BATCH_SIZE)\nval_generator = sample_generator(X_test, y_test, length = WINDOW_SIZE, batch_size = BATCH_SIZE)\nprint(f'Sample shape: {train_generator[0][0].shape}')\nprint(f'Target shape: {train_generator[0][1].shape}')","bb229217":"#Correlations for predicted and real\ndef MaxCorrelation(y_true,y_pred): \n    return -tf.math.abs(tfp.stats.correlation(y_pred,y_true, sample_axis=None, event_axis=None))\ndef Correlation(y_true,y_pred): \n    return tf.math.abs(tfp.stats.correlation(y_pred,y_true, sample_axis=None, event_axis=None))\n\n#Masked losses\ndef masked_mse(y_true, y_pred):\n    mask = tf.math.not_equal(y_true, 0.)\n    y_true_masked = tf.boolean_mask(y_true, mask)\n    y_pred_masked = tf.boolean_mask(y_pred, mask)\n    return tf.keras.losses.mean_squared_error(y_true = y_true_masked, y_pred = y_pred_masked)\n\n\ndef masked_mae(y_true, y_pred):\n    mask = tf.math.not_equal(y_true, 0.)\n    y_true_masked = tf.boolean_mask(y_true, mask)\n    y_pred_masked = tf.boolean_mask(y_pred, mask)\n    return tf.keras.losses.mean_absolute_error(y_true = y_true_masked, y_pred = y_pred_masked)\n\ndef masked_cosine(y_true, y_pred):\n    mask = tf.math.not_equal(y_true, 0.)\n    y_true_masked = tf.boolean_mask(y_true, mask)\n    y_pred_masked = tf.boolean_mask(y_pred, mask)\n    return tf.keras.losses.cosine_similarity(y_true_masked, y_pred_masked)","c979bd84":"#Long Short-Term Memory layer\ndef get_squence_model(x):\n    x = layers.LSTM(units=32, return_sequences=True)(x)\n    return x\n\n#Model\ndef get_model(n_assets = N_ASSETS):\n    x_input = keras.Input(shape=(train_generator[0][0].shape[1], n_assets, train_generator[0][0].shape[-1]))\n    branch_outputs = []\n    for i in range(n_assets):\n        a = layers.Lambda(lambda x: x[:,:, i])(x_input) # Slicing the ith asset:\n        a = layers.Masking(mask_value = 0., )(a)\n        a = get_squence_model(a)\n        a = layers.GlobalAvgPool1D()(a)\n        branch_outputs.append(a)\n    x = layers.Concatenate()(branch_outputs)\n    x = layers.Dense(units = 128)(x)\n    out = layers.Dense(units = n_assets)(x)\n    model = keras.Model(inputs=x_input, outputs=out)\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3), loss = masked_cosine, metrics=[Correlation])\n    return model\n\nmodel = get_model()\nmodel.summary()","7a9eafc6":"tf.keras.utils.plot_model(get_model(n_assets=1), show_shapes=True)","3252314d":"print(features)","aa893983":"tf.random.set_seed(0)\nestop = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 7, verbose = 0, mode = 'min',restore_best_weights = True)\nscheduler = keras.optimizers.schedules.ExponentialDecay(1e-3, (0.5 * len(X_train) \/ BATCH_SIZE), 1e-3)\nlr = keras.callbacks.LearningRateScheduler(scheduler, verbose = 1)\n\nhistory = model.fit(train_generator, validation_data = (val_generator), epochs = EPOCHS, callbacks = [lr, estop])\n","08cd3db7":"fig, ax = plt.subplots(1, 2, figsize=(16, 8))\nhistories = pd.DataFrame(history.history)\nepochs = list(range(1,len(histories)+1))\nloss = histories['loss']\nval_loss = histories['val_loss']\nCorrelation = histories['Correlation']\nval_Correlation = histories['val_Correlation']\nax[0].plot(epochs, loss, label = 'Train Loss')\nax[0].plot(epochs, val_loss, label = 'Val Loss')\nax[0].set_title('Losses')\nax[0].set_xlabel('Epoch')\nax[0].legend(loc='upper right')\nax[1].plot(epochs, Correlation, label = 'Train Correlation')\nax[1].plot(epochs, val_Correlation, label = 'Val Correlation')\nax[1].set_title('Correlations')\nax[1].set_xlabel('Epoch')\nax[1].legend(loc='upper right')\nfig.show()\ngc.collect()","62cf973b":"env = gresearch_crypto.make_env()\niter_test = env.iter_test()\n\nfor (test_df, sample_prediction_df) in iter_test:\n    test_df = get_features(test_df)\n    test_data = test_df.merge(example, how='outer', on='Asset_ID').sort_values('asset_order')\n    test = np.array(test_data[features].fillna(0))\n    test = test.reshape(-1, 1, N_ASSETS, test.shape[-1])\n    test_sample = np.hstack([test_sample, test])[:,-1 * WINDOW_SIZE:]\n    y_pred = model.predict(test_sample).squeeze().reshape(-1, 1).squeeze()\n    test_data['Target'] = y_pred\n    for _, row in test_df.iterrows():\n        try: sample_prediction_df.loc[sample_prediction_df['row_id'] == row['row_id'], 'Target'] = test_data.loc[test_data['row_id'] == row['row_id'], 'Target'].item()\n        except: sample_prediction_df.loc[sample_prediction_df['row_id'] == row['row_id'], 'Target'] = 0\n    env.predict(sample_prediction_df)","f50ada70":"#library\nimport os\nfrom datetime import datetime\nimport time\nfrom sklearn.preprocessing import StandardScaler\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\n\nimport statsmodels.api as sm\nfrom statsmodels.tsa.ar_model import AutoReg\nfrom statsmodels.tsa.arima_model import ARMA\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom scipy import stats\nfrom itertools import product\n\n\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\n\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n        \n        \ninfo = pd.read_csv(\"\/kaggle\/input\/g-research-crypto-forecasting\/asset_details.csv\")\nctrain = pd.read_csv(\"\/kaggle\/input\/g-research-crypto-forecasting\/train.csv\")\n","7d43419b":"info = pd.read_csv(\"\/kaggle\/input\/g-research-crypto-forecasting\/asset_details.csv\")\nctrain = pd.read_csv(\"\/kaggle\/input\/g-research-crypto-forecasting\/train.csv\")\n","33aa91f8":"def preprocess_data(df,asset_id =1):    \n    df=df[df[\"Asset_ID\"]==asset_id].set_index(\"timestamp\")\n    df=df.reindex(range(df.index[0],df.index[-1]+60,60), method=\"pad\")\n    df.index = pd.to_datetime(df.index, unit='s')\n\n    # CALCULATE PRICE RETURNS AS DAILY PERCENTAGE CHANGE USING pct_change()\n    df['returns'] = 100 * df.Close.pct_change().dropna()\n\n    # CALCULATE LOG RETURNS BASED ON ABOVE FORMULA\n    df['log_returns'] = np.log(df.Close\/df.Close.shift(1))\n\n    # DROPPING THE 1ST ROW OF DATA \n    # BECAUSE I SHIFTED IT FORWARD TO CALCULATE RETURNS\/LOG RETURNS\n    df.dropna(inplace=True)\n    \n    #The Open\/High\/Low\/Close prices are usually very similar and highly correlated to each other. Therefore, instead of keeping all of them in the dataset, I would add 2 more features:\n    df['HL_sprd'] = np.log((df.High - df.Low) \/ df.Close)\n    df['CO_sprd'] = (df.Close - df.Open) \/ df.Open\n    df['Volume'] = np.log(df.Volume)\n    features = ['HL_sprd', 'CO_sprd', 'Volume','log_returns']\n    x_train_processed = df[features]     \n    y_train_processed = df['Target']\n    print()\n    return df,x_train_processed,y_train_processed","390b4311":"df, X_btc, y_btc  = preprocess_data(ctrain,1)","ece7ddba":"X_btc.head()","24376741":"y_btc.head()","4d819b45":"from statsmodels.tsa.stattools import adfuller\n# LOG RETURNS\nadfuller_results = adfuller(btc.log_returns.dropna())\n\nprint(f'ADF Statistic: {adfuller_results[0]}')\nprint(f'p-value: {adfuller_results[1]}')\nprint('Critical Values:')\nfor key, value in adfuller_results[4].items():\n    print(f'{key}: {value:.4f}')","35d06ed2":"# RETURNS\nadfuller_results = adfuller(df.returns.dropna())\n\nprint(f'ADF Statistic: {adfuller_results[0]}')\nprint(f'p-value: {adfuller_results[1]}')\nprint('Critical Values:')\nfor key, value in adfuller_results[4].items():\n    print(f'{key}: {value:.4f}')","a43f1810":"#SPLIT\ndef custom_split(X_btc,y_btc):   \n    PCT_VALIDATION = 10 # last 10% of the data are used as validation set\n\n\n    X_train, X_test = X_btc[:-len(X_btc)\/\/PCT_VALIDATION], X_btc[-len(X_btc)\/\/PCT_VALIDATION:]\n    y_train, y_test = y_btc[:-len(y_btc)\/\/PCT_VALIDATION], y_btc[-len(y_btc)\/\/PCT_VALIDATION:]\n    return X_train, X_test, y_train, y_test\n\nX_train, X_test, y_train, y_test = custom_split(X_btc,y_btc)   ","1c1eadd1":"##think about it later\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n\n\ndef transform_volatility_to_scaler(scaler, tf_series):\n    '''\n    Transform a series to a fitted scaler\n    '''\n    idx = tf_series.index\n    output = pd.Series(scaler.transform(tf_series.values.reshape(-1,1))[:,0],\n                       index=idx)\n    return output\n\n# INITIATE SCALER\nscaler = MinMaxScaler()\n\n# FIT SCALER TO CURRENT VOLATILITY IN TRAINING SET\nscaler = scaler.fit(X_train.values.reshape(-1,1))","00ba5e27":"\"\"\"\"# TRANSFORM TRAINING CURRENT & FUTURE VOLATILITIES \nx_train_scaled = transform_volatility_to_scaler(scaler_vol, x_train)\ny_train_scaled = transform_volatility_to_scaler(scaler_vol, y_train)\n\n# TRANSFORMING VALIDATION CURRENT & FUTURE VOLATILITIES\nx_val_scaled = transform_volatility_to_scaler(scaler_vol, x_val)\ny_val_scaled = transform_volatility_to_scaler(scaler_vol, y_val)\n\n# TRANSFORMING TEST CURRENT & FUTURE VOLATILITIES\nx_test_scaled = transform_volatility_to_scaler(scaler_vol, x_test)\ny_test_scaled = transform_volatility_to_scaler(scaler_vol, y_test)\"\"\"\"","5824957e":"# DEFINE ROOT MEAN SQUARED PERCENTAGE ERROR FUNCTION\ndef RMSPE(y_true, y_pred):\n    \"\"\"\n    Compute Root Mean Squared Percentage Error between 2 arrays\n    \"\"\"\n    output = np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n    return output\n\n\n\n# DEFINE ROOT MEAN SQUARED ERROR FUNCTION\ndef RMSE(y_true, y_pred):\n    \"\"\"\n    Compute Root Mean Squared Error between 2 arrays\n    \"\"\"\n    output = np.sqrt(mse(y_true, y_pred))\n    return output\n\n# INITIATE A DATAFRAME FOR MODEL PERFORMANCE TRACKING & COMPARISON \nperf_df = pd.DataFrame(columns=['Model', 'Validation RMSPE', 'Validation RMSE'])\n\n# A FUNCTION THAT LOGS MODEL NAME, RMSE AND RMPSE INTO perf_df\n# FOR EASY COMPARISON LATER\ndef log_perf(y_true, y_pred, model_name):\n    perf_df.loc[len(perf_df.index)] = [model_name, \n                                       RMSPE(y_true, y_pred), \n                                       RMSE(y_true, y_pred)]\n    return perf_df\n\n\n# PLOTTING MODEL PREDICTIONS VS. TARGET VALUES\ndef viz_model(y_true, y_pred, model_name):\n    sns.set_context(\"paper\", font_scale=1.7)\n    plt.rcParams[\"axes.grid\"] = False\n\n    with sns.axes_style(\"whitegrid\"):\n        plt.figure(figsize=(18,7))\n        plt.plot(x_val_scaled, color='gray',  ls=':',\n                label=f\"Scaled Current Daily Volatility\")\n        \n        plt.plot(y_true, color='blue', lw=2, \n                label=f\"Target Volatility\")\n        plt.plot(y_pred, color='orange', lw=2.5,\n                label=f'Forecasted Volatility')\n        \n        # plt.plot(lr_val, color='gray', alpha=0.4,\n        #         label='Daily Log Returns')\n\n        plt.title(f'{model_name} \\non Validation Data')\n        plt.legend(loc='best', frameon=True)\n\n","7609fccd":"import tensorflow as tf\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras import backend as K\nfrom decimal import Decimal\n# CONVERT A SERIES TO MATRIX DATASET\n\nseed = 20\n\n\ndef windowed_dataset(x_series, y_series, lookback_window):\n    dataX, dataY = [], []\n    for i in range((lookback_window-1), len(x_series)):\n        start_idx = x_series.index[i-lookback_window+1]\n        end_idx = x_series.index[i]\n        a = x_series[start_idx:end_idx].values\n        dataX.append(a)\n        dataY.append(y_series[end_idx])\n\n    return np.array(dataX), np.array(dataY)\n\n\ntf.keras.backend.clear_session()\n\n# SET SEED FOR REPRODUCIBILITY\nnp.random.seed(seed)\n\nn_past = 30\nbatch_size = 64\nn_dims = X_btc.shape[1]\n\nmat_X_train, mat_y_train = windowed_dataset(X_train, y_train, n_past)","d69207e6":"import os\nimport gc\nimport traceback\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport gresearch_crypto\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nimport pandas as pd, numpy as np\nfrom tensorflow.keras import layers\nimport tensorflow_probability as tfp\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import RobustScaler\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\npd.set_option('display.max_columns', None)","d4f7e9a3":"        \nasset_details = pd.read_csv(\"\/kaggle\/input\/g-research-crypto-forecasting\/asset_details.csv\")\ntrain = pd.read_csv(\"\/kaggle\/input\/g-research-crypto-forecasting\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/g-research-crypto-forecasting\/example_test.csv\")\n","468add12":"#Params\nDEBUG = True\nEPOCHS = 200\nN_ASSETS = 14\nWINDOW_SIZE = 15\nBATCH_SIZE = 1024\nPCT_VALIDATION = 10 # last 10% of the data are used as validation set","85854df8":"import gc \n\ndef time_change(train):\n    # Convert timestamp\n    train['timestamp'] = pd.to_datetime(train['timestamp'], unit='s')\n\n    train.index = train.timestamp\n    train.sort_index(inplace=True)\n    train['is_real'] = True\n    ind = train.index.unique()\n\n    def reindex(df):\n        res = df.reindex(pd.date_range(ind.min(), ind.max(), freq='min'))\n        res['is_real'].fillna(False, inplace=True)\n        res['timestamp'] = res.index\n        res = res.fillna(method=\"ffill\").fillna(method=\"bfill\")\n        return res\n    train = train.groupby('Asset_ID').apply(reindex).reset_index(0, drop=True).sort_values(by=['timestamp', 'Asset_ID'])\n    gc.collect()\n    return train\n\ndef vmap_change(train):\n    #VWAP  - The average price of the asset over the time interval, weighted by volume.\n    VWAP_max = np.max(train[np.isfinite(train.VWAP)].VWAP)\n    VWAP_min = np.min(train[np.isfinite(train.VWAP)].VWAP)\n    train['VWAP'] = np.nan_to_num(train.VWAP, posinf=VWAP_max, neginf=VWAP_min)\n    \n    if DEBUG:\n        train = train[-500010:]\n    return train\n\ndef target_train_shape_change(train):\n    targets = train['Target'].to_numpy().reshape(-1, N_ASSETS)\n    feature_cols = train.columns.drop(['Asset_ID', 'Target', 'timestamp', 'is_real'])\n\n    #non-real data features are set to 0 which are then masked by the model\n    for col in feature_cols:\n        train[col] = train[col] * train.is_real\n        \n    train_data = train[feature_cols].values\n    train_data = train_data.reshape(-1, N_ASSETS, train_data.shape[-1])\n    return targets, train_data\n\n","51173ad7":"train.columns","b3c157d4":"def get_features(df):\n    df_feat = df[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']].copy()","6a8c49a2":"targets, train_data = target_train_shape_change(vmap_change(time_change(train)))","f7c4c86c":"#Samples with a duration of WINDOW_SIZE records (minutes) will be formed from the train array. \n#Each sample has a target vector corresponding to the final index if WINDOW_SIZE record.\n\nclass sample_generator(keras.utils.Sequence):\n    def __init__(self, x_set, y_set, batch_size, length):\n        self.x, self.y = x_set, y_set\n        self.batch_size = batch_size\n        self.length = length\n        self.size = len(x_set)\n    def __len__(self): return int(np.ceil(len(self.x) \/ float(self.batch_size)))\n    def __getitem__(self, idx):\n        batch_x = []\n        batch_y = []\n        for i in range(self.batch_size):\n            start_ind = self.batch_size * idx + i\n            end_ind = start_ind + self.length \n            if end_ind <= self.size:\n                batch_x.append(self.x[start_ind : end_ind])\n                batch_y.append(self.y[end_ind -1])\n        return np.array(batch_x), np.array(batch_y)","7860d7ab":"X_train, X_test = train_data[:-len(train_data)\/\/PCT_VALIDATION], train_data[-len(train_data)\/\/PCT_VALIDATION:]\ny_train, y_test = targets[:-len(train_data)\/\/PCT_VALIDATION], targets[-len(train_data)\/\/PCT_VALIDATION:]","c1e22428":"train_generator = sample_generator(X_train, y_train, length=WINDOW_SIZE, batch_size=BATCH_SIZE)\nval_generator = sample_generator(X_test, y_test, length=WINDOW_SIZE, batch_size=BATCH_SIZE)\nprint(f'Sample shape: {train_generator[0][0].shape}')\nprint(f'Target shape: {train_generator[0][1].shape}')","a194aa63":"#Correlations for predicted and real\ndef MaxCorrelation(y_true,y_pred): \n    return -tf.math.abs(tfp.stats.correlation(y_pred, y_true, sample_axis=None, event_axis=None))\n\ndef Correlation(y_true,y_pred): \n    return tf.math.abs(tfp.stats.correlation(y_pred, y_true, sample_axis=None, event_axis=None))\n\n#Masked losses\ndef masked_mse(y_true, y_pred):\n    mask = tf.math.not_equal(y_true, 0.)\n    y_true_masked = tf.boolean_mask(y_true, mask)\n    y_pred_masked = tf.boolean_mask(y_pred, mask)\n    return tf.keras.losses.mean_squared_error(y_true=y_true_masked, y_pred=y_pred_masked)\n\ndef masked_mae(y_true, y_pred):\n    mask = tf.math.not_equal(y_true, 0.)\n    y_true_masked = tf.boolean_mask(y_true, mask)\n    y_pred_masked = tf.boolean_mask(y_pred, mask)\n    return tf.keras.losses.mean_absolute_error(y_true=y_true_masked, y_pred=y_pred_masked)\n\ndef masked_cosine(y_true, y_pred):\n    mask = tf.math.not_equal(y_true, 0.)\n    y_true_masked = tf.boolean_mask(y_true, mask)\n    y_pred_masked = tf.boolean_mask(y_pred, mask)\n    return tf.keras.losses.cosine_similarity(y_true_masked, y_pred_masked)","683f002c":"train_generator[0][0].shape[1]","6ae4bdc9":"#Model\ndef get_model_1_layer(n_assets=N_ASSETS):\n    x_input = keras.Input(shape=(train_generator[0][0].shape[1], n_assets, train_generator[0][0].shape[-1]))\n    branch_outputs = []\n    \n    for i in range(n_assets):\n        a = layers.Lambda(lambda x: x[:,:, i])(x_input) # Slicing the ith asset:\n        a = layers.Masking(mask_value = 0., )(a)\n        a = layers.LSTM(units=32, return_sequences=True)(a)\n        a = layers.GlobalAvgPool1D()(a)\n        branch_outputs.append(a)\n        \n    x = layers.Concatenate()(branch_outputs)\n    x = layers.Dense(units = 128)(x)\n    out = layers.Dense(units = n_assets)(x)\n    model = keras.Model(inputs=x_input, outputs=out)\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3), loss=masked_mse, metrics=[Correlation])\n    return model\n\nmodel_1_layer = get_model_1_layer()\nmodel_1_layer.summary()","19bb4dc9":"tf.keras.utils.plot_model(get_model_1_layer(n_assets=1), show_shapes=True)","31fa6fef":"tf.random.set_seed(0)\n\n#Stop training when a monitored metric has stopped improving\nestop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, verbose=0, mode='min', restore_best_weights=True)\n\nscheduler = keras.optimizers.schedules.ExponentialDecay(1e-3, (0.5 * len(X_train) \/ BATCH_SIZE), 1e-3)\nlr = keras.callbacks.LearningRateScheduler(scheduler, verbose=1)","ad299a2d":"history_1_layer = model_1_layer.fit(train_generator, validation_data=(val_generator), epochs=EPOCHS, callbacks=[lr, estop])\n\n","1dea57de":"def plot_training_history(history):\n    fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n    histories = pd.DataFrame(history.history)\n    epochs = list(range(1,len(histories)+1))\n    loss = histories['loss']\n    val_loss = histories['val_loss']\n    correlation = histories['Correlation']\n    val_correlation = histories['val_Correlation']\n    ax[0].plot(epochs, loss, label='Train Loss')\n    ax[0].plot(epochs, val_loss, label='Val Loss')\n    ax[0].set_title('Losses')\n    ax[0].set_xlabel('Epoch')\n    ax[0].legend(loc='upper right')\n    ax[1].plot(epochs, correlation, label='Train Correlation')\n    ax[1].plot(epochs, val_correlation, label='Val Correlation')\n    ax[1].set_title('Correlations')\n    ax[1].set_xlabel('Epoch')\n    ax[1].legend(loc='upper right')\n    fig.show()\ngc.collect()\n\n","364a2f68":"plot_training_history(history_1_layer)","5841acef":"predictions_1_layer = model_1_layer.predict(val_generator)","faa97c45":"perf_df = pd.DataFrame(columns=['Model', 'asset', 'corr','weights'])# A FUNCTION THAT LOGS MODEL NAME, RMSE AND RMPSE INTO perf_df\n\nprint('Asset:    Corr. coef.')\nprint('---------------------')\nfor i in range(N_ASSETS):\n    # drop first 14 values in the y_test, since they are absent in val_generator labels\n    y_true = np.squeeze(y_test[WINDOW_SIZE - 1:, i])\n    y_pred = np.squeeze(predictions_1_layer[:, i])\n    real_target_ind = np.argwhere(y_true != 0)\n    asset_name = asset_details[asset_details.Asset_ID == i]['Asset_Name'].item()\n    weights = asset_details[asset_details.Asset_ID == i]['Weight'].item()\n    correl = np.corrcoef(y_pred[real_target_ind].flatten(), y_true[real_target_ind].flatten())[0,1]\n    perf_df = perf_df.append([pd.Series(['1 layer LSTM',asset_name,correl,weights ],index = perf_df.columns)], ignore_index=True)\n    \n    print(f\"{asset_name}: {correl:.4f}\")\n    plt.plot(y_true, label='Target')\n    plt.plot(y_pred, label='Prediction')\n    plt.xlabel('Time')\n    plt.ylabel('Target')\n    plt.legend()\n    plt.show()","63042cb5":"perf_df","9a4d8e65":"perf_df = perf_df.append([pd.Series(['1 layer LSTM', 'WEIGTED', (perf_df['corr']*perf_df['weights']).sum(), 1 ],index = perf_df.columns)], ignore_index=True)","0d7946cc":"perf_df.to_csv('Performance.csv')","7b11235b":"perf_df","9cc29a6c":"import datetime\ntimestamp = datetime.datetime.now()\nmodel_1_layer.save(f\"model_1_layer{timestamp.strftime('%s')}\")","f20afcc7":"model_1_layer.save('model_1_layer.h5')","3628a09e":"# load data that is just before the test set\nsup = pd.read_csv('\/kaggle\/input\/g-research-crypto-forecasting\/supplemental_train.csv').sort_values(by=['timestamp', 'Asset_ID'])\n\n# only the last 15 min are useful\nsup = sup[-WINDOW_SIZE * (N_ASSETS):]\n\n# prepare the sup data. Test data will be appended\ntest_sample = np.array(get_features(sup))\n#test_sample = np.array(sup[feature_cols])\ntest_sample = test_sample.reshape(-1, (N_ASSETS), test_sample.shape[-1])\ntest_sample = np.expand_dims(test_sample, axis=0)\n\n\nenv = gresearch_crypto.make_env()\niter_test = env.iter_test()\n\n\"\"\"\nenv = gresearch_crypto.make_env()\niter_test = env.iter_test()\n\"\"\"\nfor (test_df, sample_prediction_df) in iter_test:\n    # get the features\n    test_df = get_features(test_df)\n    \n    # always sort by time, then by Asset_ID\n    test_df = test_df.sort_values(by=['timestamp', 'Asset_ID'])\n    \n    # to map to the corresponding rows    \n    asset_id_row_id_map = {a_id: r_id for a_id, r_id in test_df[['Asset_ID', 'row_id']].values}\n    \n    # matrix of features\n    test = np.array(test_df[feature_cols].fillna(0))\n    \n    # reshaping (similar to the train)\n    test = test.reshape(-1, 1, N_ASSETS, test.shape[-1])\n    \n    # Stack the test data to previous data, keep only last 15 min \n    test_sample = np.hstack([test_sample, test])[:,-WINDOW_SIZE:]\n    \n    y_pred = model.predict(test_sample).squeeze().reshape(-1, 1).squeeze()\n    \n    for i, p in enumerate(y_pred):\n        sample_prediction_df.loc[sample_prediction_df['row_id'] == asset_id_row_id_map[i], 'Target'] = p\n    env.predict(sample_prediction_df)","76cd1153":"#Model\ndef get_model(n_assets=N_ASSETS):\n    x_input = keras.Input(shape=(train_generator[0][0].shape[1], n_assets, train_generator[0][0].shape[-1]))\n    branch_outputs = []\n\n    for i in range(n_assets):\n        a = layers.Lambda(lambda x: x[:,:, i])(x_input) # Slicing the ith asset:\n        a = layers.Masking(mask_value = 0., )(a)\n        a = layers.Bidirectional(layers.LSTM(32, return_sequences=True))(a)\n        a = layers.Bidirectional(layers.LSTM(16))(a)\n        #a = layers.GlobalAvgPool1D()(a)\n        branch_outputs.append(a)\n    \n    x = layers.Concatenate()(branch_outputs)\n    x = layers.Dense(units = 128)(x)\n    out = layers.Dense(units = n_assets)(x)\n    model = keras.Model(inputs=x_input, outputs=out)\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3), loss=masked_mse, metrics=[Correlation])\n    return model\n\nmodel = get_model()\nmodel.summary()\n\ntf.keras.utils.plot_model(get_model(n_assets=1), show_shapes=True)\n\n#print(features)\n\ntf.random.set_seed(0)\nestop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, verbose=0, mode='min', restore_best_weights=True)\nscheduler = keras.optimizers.schedules.ExponentialDecay(1e-3, (0.5 * len(X_train) \/ BATCH_SIZE), 1e-3)\nlr = keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n\n\n#train and test\n\n\nhistory = model.fit(train_generator, validation_data=(val_generator), epochs=EPOCHS, callbacks=[lr, estop])\n\nplot_training_history(history)\n\npredictions = model.predict(val_generator)\n\n","a63da5a4":"plot_training_history(history)","047df17c":"# DEFINE ROOT MEAN SQUARED ERROR FUNCTION\ndef RMSE(y_true, y_pred):\n    \"\"\"\n    Compute Root Mean Squared Error between 2 arrays\n    \"\"\"\n    output = np.sqrt(mse(y_true, y_pred))\n    return output\n# INITIATE A DATAFRAME FOR MODEL PERFORMANCE TRACKING & COMPARISON \nperf_df = pd.DataFrame(columns=['Model', 'Validation RMSPE', 'Validation RMSE'])\n\n# A FUNCTION THAT LOGS MODEL NAME, RMSE AND RMPSE INTO perf_df\n# FOR EASY COMPARISON LATER\ndef log_perf(y_true, y_pred, model_name):\n    perf_df.loc[len(perf_df.index)] = [model_name, \n                                       RMSPE(y_true, y_pred), \n                                       RMSE(y_true, y_pred)]\n    return perf_df\n","0449f3db":"y_test","d8317f18":"predictions[:,0]","35349870":"print('Asset:    Corr. coef.')\nprint('---------------------')\nfor i in range(N_ASSETS):\n    # drop first 14 values in the y_test, since they are absent in val_generator labels\n    y_true = np.squeeze(y_test[WINDOW_SIZE - 1:, i])\n    y_pred = np.squeeze(predictions[:, i])\n    real_target_ind = np.argwhere(y_true != 0)\n    asset_name = asset_details[asset_details.Asset_ID == i]['Asset_Name'].item()\n    print(f\"{asset_name}: {np.corrcoef(y_pred[real_target_ind].flatten(), y_true[real_target_ind].flatten())[0,1]:.4f}\")\n    plt.plot(y_true, label='Target')\n    plt.plot(y_pred, label='Prediction')\n    plt.xlabel('Time')\n    plt.ylabel('Target')\n    plt.legend()\n    plt.show()","bf68db69":"# load data that is just before the test set\nsup = pd.read_csv('\/kaggle\/input\/g-research-crypto-forecasting\/supplemental_train.csv').sort_values(by=['timestamp', 'Asset_ID'])\n\n# only the last 15 min are useful\nsup = sup[-WINDOW_SIZE * (N_ASSETS):]\n\n# prepare the sup data. Test data will be appended\ntest_sample = np.array(get_features(sup))\n#test_sample = np.array(sup[feature_cols])\ntest_sample = test_sample.reshape(-1, (N_ASSETS), test_sample.shape[-1])\ntest_sample = np.expand_dims(test_sample, axis=0)\n\n\nenv = gresearch_crypto.make_env()\niter_test = env.iter_test()\n\n\"\"\"\nenv = gresearch_crypto.make_env()\niter_test = env.iter_test()\n\"\"\"\nfor (test_df, sample_prediction_df) in iter_test:\n    # get the features\n    test_df = get_features(test_df)\n    \n    # always sort by time, then by Asset_ID\n    test_df = test_df.sort_values(by=['timestamp', 'Asset_ID'])\n    \n    # to map to the corresponding rows    \n    asset_id_row_id_map = {a_id: r_id for a_id, r_id in test_df[['Asset_ID', 'row_id']].values}\n    \n    # matrix of features\n    test = np.array(test_df[feature_cols].fillna(0))\n    \n    # reshaping (similar to the train)\n    test = test.reshape(-1, 1, N_ASSETS, test.shape[-1])\n    \n    # Stack the test data to previous data, keep only last 15 min \n    test_sample = np.hstack([test_sample, test])[:,-WINDOW_SIZE:]\n    \n    y_pred = model.predict(test_sample).squeeze().reshape(-1, 1).squeeze()\n    \n    for i, p in enumerate(y_pred):\n        sample_prediction_df.loc[sample_prediction_df['row_id'] == asset_id_row_id_map[i], 'Target'] = p\n    env.predict(sample_prediction_df)","9e9b3d8b":"test_df.head()","ad4d4535":"iter_test = env.iter_test()","09a5865b":"## Submit to Kaggle","1194c47b":"p - the number of lag observations to include in the model, or lag order. (AR)\nd - the number of times that the raw observations are differenced, or the degree of differencing. (I)\nq - the size of the moving average window, also called the order of moving average.(MA)","c6e335e9":"## Time history of 3 coins and its return\n\nref: https:\/\/www.kaggle.com\/fangya\/cryptocurrency-data-visualization-arima","8abe89dc":"### Closing Price for BTC, ETH, ADA","f7c2c9bc":"## Multivariate 2-Layered Bidirectional LSTM Model","4c19fdac":"## Metrics and Loss functions","ac6f0ddb":"Feature eng","f519ccf2":"# Box-Cox Transformation for Arima\nSometimes after applying Box-Cox with a particular value of lambda the process may look stationary. It is sometimes possible that even if after applying the Box-Cox transformation the series does not appear to be stationary, diagnostics from ARIMA modeling can then be used to decide if differencing or seasonal differencing might be useful to to remove polynomial trends or seasonal trends respectively. After that the result might be an ARMA model that is stationary. If diagnostics confirm the orders p an q for the ARMA model, the AR and MA parameters can then be estimated.\n\nhttps:\/\/www.kaggle.com\/taniaj\/cryptocurrency-predictions-with-arima\n","6d5cb1b4":"1. Data visualtion for 14 popular cryptocurrency\n2. Price History for selected individual cryptocurrency\n3. Basic Arima Model for price prediction","0e568a88":"### LSTM\nversion on git https:\/\/github.com\/ayman-mezghani\/ml4f-project\/blob\/master\/g-research.ipynb","701907d1":"For all time minutes logreturn  ","da1a1492":"The p-value indicates that series is stationary as the computed p-value is lower than the significance level alpha = 0.05.","185dd2a0":"Dodgecoin has ","c51cddbd":"### BTC Seasonal Decomposition\n","3e41285a":"### 4.7. Multivariate 2-Layered Bidirectional LSTM with Dropout\n\nMy first Multivariate LSTM model would be relatively simple with 2 hidden layers of Bidirectional LSTM (quite similar to the Univariate structure above). However, I will also include Dropout layers in between. Having more features means the model would be more prone to **overfitting**, and Dropout layers can help reduce that.","149e0b17":"### Arima","4290f000":"The p-value indicates that series is still not stationary.\n\n","b07c115b":"Ref: https:\/\/www.kaggle.com\/fangya\/cryptocurrency-data-visualization-arima\n\nhttps:\/\/www.kaggle.com\/iamleonie\/to-the-moon-g-research-crypto-forecasting-eda#Data-Overview","f39c0db8":"AutoRegressive Integrated Moving Average\n\nARIMA models are denoted with the notation ARIMA(p, d, q). These parameters account for seasonality, trend, and noise in datasets:","673a16fa":"## Technical Analysis charts\nAcross the industry, there are hundreds of patterns and signals that have been developed by researchers to support technical analysis trading, it would be a bit tedious to code them. \nFortunately there is a technical analysis library in python available(created by <a href=\"https:\/\/github.com\/bukosabino\" >bukosabino<\/a>), the library has implemented 42 indicators:\n\n\n<b>Volume:<\/b>\n<ul>\n<li>Money Flow Index (MFI)<\/li>\n<li>Accumulation\/Distribution Index (ADI)<\/li>\n<li>On-Balance Volume (OBV)<\/li>\n<li>Chaikin Money Flow (CMF)<\/li>\n<li>Force Index (FI)<\/li>\n<li>Ease of Movement (EoM, EMV)<\/li>\n<li>Volume-price Trend (VPT)<\/li>\n<li>Negative Volume Index (NVI)<\/li>\n<li>Volume Weighted Average Price (VWAP)<\/li>\n<\/ul>\n<b>Volatility:<\/b>\n<ul>\n<li>Average True Range (ATR)<\/li>\n<li>Bollinger Bands (BB)<\/li>\n<li>Keltner Channel (KC)<\/li>\n<li>Donchian Channel (DC)<\/li>\n<li>Ulcer Index (UI)<\/li>\n<\/ul>\n\n<b>Trend:<\/b>\n<ul>\n<li>Simple Moving Average (SMA)<\/li>\n<li>Exponential Moving Average (EMA)<\/li>\n<li>Weighted Moving Average (WMA)<\/li>\n<li>Moving Average Convergence Divergence (MACD)<\/li>\n<li>Average Directional Movement Index (ADX)<\/li>\n<li>Vortex Indicator (VI)<\/li>\n<li>Trix (TRIX)<\/li>\n<li>Mass Index (MI)<\/li>\n<li>Commodity Channel Index (CCI)<\/li>\n<li>Detrended Price Oscillator (DPO)<\/li>\n<li>KST Oscillator (KST)<\/li>\n<li>Ichimoku Kink\u014d Hy\u014d (Ichimoku)<\/li>\n<li>Parabolic Stop And Reverse (Parabolic SAR)<\/li>\n<li>Schaff Trend Cycle (STC)<\/li>\n<\/ul>\n\n<b>Momentum:<\/b>\n<ul>\n<li>Relative Strength Index (RSI)<\/li>\n<li>Stochastic RSI (SRSI)<\/li>\n<li>True strength index (TSI)<\/li>\n<li>Ultimate Oscillator (UO)<\/li>\n<li>Stochastic Oscillator (SR)<\/li>\n<li>Williams %R (WR)<\/li>\n<li>Awesome Oscillator (AO)<\/li>\n<li>Kaufman's Adaptive Moving Average (KAMA)<\/li>\n<li>Rate of Change (ROC)<\/li>\n<li>Percentage Price Oscillator (PPO)<\/li>\n<li>Percentage Volume Oscillator (PVO)<\/li>\n<\/ul>\n<b>Others:<\/b>\n<ul>\n<li>Daily Return (DR)<\/li>\n<li>Daily Log Return (DLR)<\/li>\n<li>Cumulative Return (CR)<\/li>\n<\/ul>\n\n\n\n<b>Documentation:<\/b>\n<ul>\n    <li><a href = \"https:\/\/technical-analysis-library-in-python.readthedocs.io\/en\/latest\/\">Technical Analysis library documentation<\/a><\/li>\n<\/ul>\n<\/div>\n\n","6794cca4":"Scaler","c46d6e04":"Other technical analysis could be found here https:\/\/www.kaggle.com\/shakshyathedetector\/crypto-prediction-technical-analysis-features#Moving-average\nThere are:\n- Moving average convergence divergence (MACD) is a trend-following momentum indicator that shows the relationship between two moving averages of prices. The MACD is calculated by subtracting the 26-day exponential moving average (EMA) from the 12-day EMA\n- Bollinger Bands are a type of statistical chart characterizing the prices and volatility over time of a financial instrument or commodity, using a formulaic method propounded by John Bollinger in the 1980s. Financial traders employ these charts as a methodical tool to inform trading decisions, control automated trading systems, or as a component of technical analysis. Bollinger Bands display a graphical band (the envelope maximum and minimum of moving averages, similar to Keltner or Donchian channels) and volatility (expressed by the width of the envelope) in one two-dimensional chart.\n- A Volume Moving Average is the simplest volume-based technical indicator. Similar to a price moving average, a VMA is an average volume of a security (stock), commodity, index or exchange over a selected period of time. Volume Moving Averages are used in charts and in technical analysis to smooth and describe a volume trend by filtering short term spikes and gaps.","f3a18b87":"## Cryptocurrency Log Return Correlation Plot for 2021","f211e376":"The p-value indicates that series is still not stationary.\n\n","a6c0022e":"### Analysis of Results \nThe coef column shows the weight (i.e. importance) of each feature and how each one impacts the time series. The P>|z| column informs us of the significance of each feature weight. Here, each weight has a p-value lower or close to 0.05, so it is reasonable to retain all of them in our model.\n\nWhen fitting seasonal ARIMA models (and any other models for that matter), it is important to run model diagnostics to ensure that none of the assumptions made by the model have been violated. The plot_diagnostics object allows us to quickly generate model diagnostics and investigate for any unusual behavior.","f6861e80":"### Autocorrelation \nAutocorrelation is the correlation of a time series with the same time series lagged. It summarizes the strength of a relationship with an observation in a time series with observations at prior time steps.\n\nWe create autocorrelation factor (ACF) and partial autocorrelation factor (PACF) plots to identify patterns in the above data which is stationary on both mean and variance. The idea is to identify presence of AR and MA components in the residuals.","48e62316":"# Seasonal differentiation \n\nOne method of differencing data is seasonal differencing, which involves computing the difference between an observation and the corresponding observation in the previous year.","d0f30d89":"## Statistical analysis","1630fae0":"## Training\nOur model will be trained for the number of FOLDS and EPOCHS you chose in the configuration above. Each fold the model with lowest validation loss will be saved and used to predict OOF and test. Adjust the variable VERBOSE. The variable VERBOSE=1 or 2 will display the training and validation loss for each epoch as text.","1f63b13f":"## Candlestick charts","2441a28d":"## Training Data Distribution among differnet Assets ","47644af2":"Fig shows that the first one component explain the majority of the variance in our data. \n","af2d664a":"There is a positive correlation with the first 9 lags that is perhaps significant for the first 3 lags.\n\nA good starting point for the AR parameter of the model may be 3.\n\nLets try out autocorrelation on the differences","ca298bd8":"There are not many spikes in the plots outside the insignificant zone (shaded) so there may not be enough information available in the residuals to be extracted by AR and MA models.","6daab620":"### LSTM\n","487be290":"### Parameter Selection\nWe will iteratively explore different combinations of parameters. For each combination we fit a new ARIMA model with SARIMAX() and assess its overall quality.\n\nWe will use the AIC (Akaike Information Criterion) value, returned with ARIMA models fitted using statsmodels. The AIC measures how well a model fits the data while taking into account the overall complexity of the model. A model that fits the data very well while using lots of features will be assigned a larger AIC score than a model that uses fewer features to achieve the same goodness-of-fit. Therefore, we are interested in finding the model that yields the lowest AIC value.","449234c0":"As we can see ETH, and BTC residual return are relatively stable compared to ADA. This might be a good implication that if the investor would take short time trading opportunities, ADA is a better choice.\n\nIf the investor is risk averse, BTC or ETH will be a better fit","c3ceefa5":"### Regular differentiation \nSometimes it may be necessary to difference the data a second time to obtain a stationary time series, which is referred to as second order differencing","a932c389":"The illustration of BTC and ETH prices groupped by year [9] https:\/\/www.kaggle.com\/iamleonie\/to-the-moon-g-research-crypto-forecasting-eda#Data-Overview","b49f63f3":"- skewness:\nreturns - positive, log_returns - negative\n\n- positive kurtosis (leptokurtic) - Both Returns & Log Returns show higher peak with thicker tails than the standard normal distribution.\n","2a6ac1b4":"Our primary concern is to ensure that the residuals of our model are uncorrelated and normally distributed with zero-mean. If the seasonal ARIMA model does not satisfy these properties, it is a good indication that it can be further improved.\nIn the histogram (top right), the KDE line should follow the N(0,1) line (normal distribution with mean 0, standard deviation 1) closely. This is an indication whether the residuals are normally distributed or not.\nIn the Q-Q-plot the ordered distribution of residuals (blue dots) should follow the linear trend of the samples taken from a standard normal distribution with N(0, 1). Again, this is an indication whether the residuals are normally distributed.\n\nThe standardized residual(top left) plot doesn't display any obvious seasonality. \nThis is confirmed by the Correlogram (bottom right), which shows that the time series residuals have low correlation with lagged versions of itself. (?)","6fa55066":"## ARIMA Model \n","5b801ada":"## Multivariate 1-Layered LSTM with Gloobal Average Pool","f721f1db":"In statistics and econometrics, an augmented Dickey\u2013Fuller test (ADF) tests the null hypothesis that a unit root is present in a time series sample.","21c30b2c":"The p-value indicates that series is stationary as the computed p-value is lower than the significance level alpha = 0.05.","efe4e157":"https:\/\/www.investopedia.com\/articles\/investing\/102715\/computing-historical-volatility-excel.asp\n r (t) = ln (P (t) \/ P (t-1)) ->  \n \nThe total return R at time t is:  \nR = r1 + r2 + r3 + 2 + ... +rt-1+ rt,\n\n","a67420f4":"The corr changes from time ","c5fc70af":"## SARIMAX","d808fc46":"Split","e8308674":"A linear regression model is constructed including the specified number and type of terms, and the data is prepared by a degree of differencing in order to make it stationary, i.e. to remove trend and seasonal structures that negatively affect the regression model. A value of 0 for a parameter indicates to not use that element of the model.","fcfbce4b":"## Dataset creation\nSamples with a duration of WINDOW_SIZE records (minutes) will be formed from the train array. Each sample has a target vector corresponding to the final index if WINDOW_SIZE record.","706fe4ad":"Conclusion: We may consider trying to standardise the distribution further. But lets go ahead and do a prediction anyway...","f790374e":"## Moving average"}}