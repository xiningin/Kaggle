{"cell_type":{"32e1504d":"code","faf51615":"code","f278d520":"code","068e6844":"code","f3ca4f41":"code","77864afa":"code","44588794":"code","83dd4d54":"code","0a641ea7":"code","5e1341be":"code","752ac373":"code","46233a42":"code","06abfc66":"code","d4ce0061":"code","f876822e":"code","aca7a33d":"code","039bb1ee":"code","6410d75c":"code","32282c0f":"code","67c5bdcd":"code","42c1480b":"code","2703a56b":"code","9b28ce3b":"code","348534be":"code","6afad797":"code","e931960c":"code","9f08398a":"code","b26b5b97":"code","ff140cc7":"code","1f058e83":"code","c22e7329":"code","f19b1c7e":"code","3c5a42f3":"code","d1fd1022":"code","a2cf28a7":"code","681f262a":"code","283ca1d1":"code","1bafc32d":"code","40eb330d":"code","efeb95e7":"code","b3cf7bc3":"code","13639ee3":"code","a1f5b44c":"markdown","0558679c":"markdown","0eae72ea":"markdown","78fb3cd0":"markdown","c079d318":"markdown","d55d858b":"markdown","792f7242":"markdown","6d1181f2":"markdown","edf0cbf3":"markdown","267db449":"markdown","2bdeb9d0":"markdown","bcff7d77":"markdown","80d5c788":"markdown","fa2a3865":"markdown","d7c24193":"markdown","f6093cbf":"markdown","9f7b5d9b":"markdown","89e5b944":"markdown","e4daf75d":"markdown","45045cf7":"markdown","b8ec5a27":"markdown","61b001bd":"markdown","6ec95d00":"markdown","1cc6e887":"markdown","ff229c26":"markdown","03f76a9e":"markdown"},"source":{"32e1504d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neighbors import DistanceMetric\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier","faf51615":"#Loading the data and displaying them\ncustomers_df = pd.read_csv(\"\/kaggle\/input\/predicting-churn-for-bank-customers\/Churn_Modelling.csv\")\ncustomers_df.head()","f278d520":"#Describing the information of the dataset to see the type, null and size of the columns\ncustomers_df.info()","068e6844":"#Verifying and making sure that there is no nan in the dataset\ncustomers_df.isna().sum()","f3ca4f41":"#Looking for duplicates in the dataset\ncustomers_df[customers_df.duplicated(keep=\"first\")].count()","77864afa":"exited_customers = customers_df[customers_df[\"Exited\"] == 1]\nremained_customers = customers_df[customers_df[\"Exited\"] == 0]\n\nlabels = ['Exited', 'Stayed']\nsizes = [len(exited_customers), len(remained_customers)]\n\nexplode = (0, 0.1)  \nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax1.axis('equal')  \nplt.tight_layout()\nplt.title(\"Cutomers who stayed and those who remained percentage\")\nplt.show()","44588794":"\nexited_Male = exited_customers[exited_customers[\"Gender\"] == \"Male\"]\nremained_Male = remained_customers[remained_customers[\"Gender\"] == \"Male\"]\nexited_Female = exited_customers[exited_customers[\"Gender\"] == \"Female\"]\nremained_Female = remained_customers[remained_customers[\"Gender\"] == \"Female\"]\n\ngroup_names=['Exited', 'Stayed']\ngroup_size=[len(exited_customers),len(remained_customers)]\nsubgroup_names=['Male', 'Female', 'Male', 'Female']\nsubgroup_size=[len(exited_Male),len(exited_Female),len(remained_Male),len(remained_Female)]\n \n# Create colors\na, b, c=[plt.cm.Blues, plt.cm.Reds, plt.cm.Greens]\n \n# First Ring (outside)\nfig, ax = plt.subplots()\nax.axis('equal')\nmypie, _ = ax.pie(group_size, radius=1.3, labels=group_names, colors=[ b(0.6),c(0.7)] )\nplt.setp( mypie, width=0.3, edgecolor='white')\n \n# Second Ring (Inside)\nmypie2, _ = ax.pie(subgroup_size, radius=1.3-0.3, labels=subgroup_names, labeldistance=0.7, colors=[a(0.6),c(0.3),a(0.6),c(0.3)])\nplt.setp( mypie2, width=0.4, edgecolor='white')\nplt.margins(0,0)\nplt.title(\"Distribution of Male and Female who exited\/stayed\")\n \n# show it\nplt.show()\n","83dd4d54":"   #\nfig, axarr = plt.subplots(2, 2, figsize=(20, 12))\nsns.countplot(x='Geography', hue = 'Exited',data = customers_df, ax=axarr[0][0])\nsns.countplot(x='Gender', hue = 'Exited',data = customers_df, ax=axarr[0][1])\nsns.countplot(x='HasCrCard', hue = 'Exited',data = customers_df, ax=axarr[1][0])\nsns.countplot(x='IsActiveMember', hue = 'Exited',data = customers_df, ax=axarr[1][1])","0a641ea7":"exit_probability = len(exited_customers)\/(len(exited_customers)+len(remained_customers))\nexit_probability","5e1341be":"exited_Female = exited_customers[exited_customers[\"Gender\"] == \"Female\"]\nfemale_exit_probability = len(exited_Female)\/len((customers_df[customers_df[\"Gender\"] == \"Female\"]))\nfemale_exit_probability","752ac373":"exited_Male = exited_customers[exited_customers[\"Gender\"] == \"Male\"]\nmale_exit_probability = len(exited_Male)\/len((customers_df[customers_df[\"Gender\"] == \"Male\"]))\nmale_exit_probability","46233a42":"exited_Credit_card_owner = exited_customers[exited_customers[\"HasCrCard\"] == 1]\ncredit_card_owner_exit_probability = len(exited_Credit_card_owner)\/len((customers_df[customers_df[\"HasCrCard\"] == 1]))\ncredit_card_owner_exit_probability","06abfc66":"exited_Credit_card_no_owner = exited_customers[exited_customers[\"HasCrCard\"] == 0]\ncredit_card_no_owner_exit_probability = len(exited_Credit_card_no_owner)\/len((customers_df[customers_df[\"HasCrCard\"] == 0]))\ncredit_card_no_owner_exit_probability","d4ce0061":"exited_active_customer = exited_customers[exited_customers[\"IsActiveMember\"] == 1]\nactive_customer_exit_probability = len(exited_active_customer)\/len((customers_df[customers_df[\"IsActiveMember\"] == 1]))\nactive_customer_exit_probability","f876822e":"exited_non_active_customer = exited_customers[exited_customers[\"IsActiveMember\"] == 0]\nnon_active_customer_exit_probability = len(exited_non_active_customer) \/ len((customers_df[customers_df[\"IsActiveMember\"] == 0]))\nnon_active_customer_exit_probability","aca7a33d":"exited_french_customer = exited_customers[exited_customers[\"Geography\"] == \"France\"]\nfrench_customer_exit_probability = len(exited_french_customer) \/ len((customers_df[customers_df[\"Geography\"] == \"France\"]))\nfrench_customer_exit_probability","039bb1ee":"exited_spain_customer = exited_customers[exited_customers[\"Geography\"] == \"Spain\"]\nspain_customer_exit_probability = len(exited_spain_customer) \/ len((customers_df[customers_df[\"Geography\"] == \"Spain\"]))\nspain_customer_exit_probability ","6410d75c":"exited_germany_customer = exited_customers[exited_customers[\"Geography\"] == \"Germany\"]\ngermany_customer_exit_probability = len(exited_germany_customer) \/ len((customers_df[customers_df[\"Geography\"] == \"Germany\"]))\ngermany_customer_exit_probability ","32282c0f":"exited_Teenagers = exited_customers[exited_customers[\"Age\"] < 20]\ntotal_Teenagers = customers_df[customers_df[\"Age\"] < 20]\nprint(\"Probability of tennagers to churn = \",len(exited_Teenagers)\/len(total_Teenagers))\nexited_adults_less_than_35 = exited_customers[(exited_customers[\"Age\"] < 35) & (exited_customers[\"Age\"] >= 20)]\ntotal_adults_less_than_35 = customers_df[(customers_df[\"Age\"] < 35) & (customers_df[\"Age\"] >= 20)]\nprint(\"Probability of adults between 20 than 35 years to churn = \",len(exited_adults_less_than_35)\/len(total_adults_less_than_35))\nexited_adults_less_than_40 = exited_customers[(exited_customers[\"Age\"] < 40) & (exited_customers[\"Age\"] >= 35)]\ntotal_adults_less_than_40 = customers_df[(customers_df[\"Age\"] < 40) & (customers_df[\"Age\"] >= 35)]\nprint(\"Probability of adults between 35 than 40 years to churn = \",len(exited_adults_less_than_40)\/len(total_adults_less_than_40))\nexited_adults_less_than_50 = exited_customers[(exited_customers[\"Age\"] < 50) & (exited_customers[\"Age\"] >= 40)]\ntotal_adults_less_than_50 = customers_df[(customers_df[\"Age\"] < 50) & (customers_df[\"Age\"] >= 40)]\nprint(\"Probability of adults between 40 than 50 years to churn = \",len(exited_adults_less_than_50)\/len(total_adults_less_than_50))\nexited_adults_greater_than_50 = exited_customers[(exited_customers[\"Age\"] > 50)]\ntotal_adults_greater_than_50 = customers_df[(customers_df[\"Age\"] > 50)]\nprint(\"Probability of adults over 50 years to churn = \",len(exited_adults_greater_than_50)\/len(total_adults_greater_than_50))","67c5bdcd":"#Correlation on the heatmap\n\ncorr = customers_df.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","42c1480b":"#Displaying the correlation on a heatmap using different colors and annotations\nfig, ax = plt.subplots(figsize=(20,10))\nsns.heatmap(corr, annot=True, annot_kws={\"size\": 12},ax=ax)","2703a56b":"# Creating dummy variables\ngender_dummies = pd.get_dummies(customers_df[\"Gender\"])\ngeography_dummies = pd.get_dummies(customers_df[\"Geography\"])\n# Adding them to our dataframe and removing unecessary dataframes\nnew_customers_df = pd.concat([customers_df,gender_dummies,geography_dummies],axis=1)\n#Removing unecessary columns\nnew_customers_df = new_customers_df.drop([\"RowNumber\", \"CustomerId\",\"Surname\",\"Geography\",\"Gender\"], axis=1)\nnew_customers_df.head()","9b28ce3b":"X = new_customers_df.drop([\"Exited\"],axis=1)\ny = new_customers_df[\"Exited\"]","348534be":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n#Splitting the training set and the testing set with 70 to 30 percentage\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# Building the logistic and fitting the model \nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)","6afad797":"#Predicting using our built model\ny_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))","e931960c":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n#Building and printing the confusion matrix\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion matrix for the logistic regression\")\nprint(confusion_matrix)\nprint(classification_report(y_test, y_pred))","9f08398a":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nlogit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","b26b5b97":"max_depths = [30,20,15,10,9,8,7,6,5,4,3,2,1]\nmodels = [tree.DecisionTreeClassifier(max_depth=x).fit(X_train,y_train) for x in max_depths]\nprint(len(X_train))\nprint(len(X_test))\n\nfor m,d in zip(models,max_depths):\n    print('depth = ',d,\n          'train_accuracy = ',round(m.score(X_train,y_train),3),\n          'valid_accuracy = ',round(m.score(X_test,y_test),3))","ff140cc7":"# Building the model and pruning at depth 6 \nDTreemodel = tree.DecisionTreeClassifier(max_depth=6).fit(X_train,y_train) \n# Printing the accuracy both training and testing\nprint('train_accuracy = ',round(DTreemodel.score(X_train,y_train),3),\n          'valid_accuracy = ',round(DTreemodel.score(X_test,y_test),3))\n# Predict on the testing set\ny_pred_D_Tree = DTreemodel.predict(X_test)","1f058e83":"# Calculating the random forest confusion matrix then the classification report\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test, y_pred_D_Tree)\nprint(\"The confusion matrix is as follow:\")\nprint(confusion_matrix)\nprint(classification_report(y_test, y_pred_D_Tree))","c22e7329":"# Area under the curve, ROC analysis\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nDT_roc_auc = roc_auc_score(y_test, DTreemodel.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, DTreemodel.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Decision Trees  (area = %0.2f)' % DT_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","f19b1c7e":"# Looping through different number of neighbors\nn = np.arange(1,21)\nmodels = [KNeighborsClassifier(n_neighbors=x).fit(X_train,y_train) for x in n]\nscores_train = [models[x-1].score(X_train,y_train) for x in n]\nscores_valid = [models[x-1].score(X_test,y_test) for x in n]\n\nfig, ax = plt.subplots(figsize=(8,8))\nax.plot(n,scores_train,label='train')\nax.plot(n,scores_valid,label='valid')\nax.grid(linestyle='--')\nax.legend()\nax.set_xticks(n)\nax.set_xlabel('n neighbors')\nax.set_ylabel('accuracy')\nax.set_title('Accuracy of KNN with number of neighbors')\nplt.close('all')\nfig","3c5a42f3":"# Loop trough different distance metrics to find the best \nn = np.arange(1,21)\n\nmodels_minkowski = [KNeighborsClassifier(\n    n_neighbors=x,\n    metric='minkowski').fit(X_train,y_train) for x in n]\nmodels_euclidean = [KNeighborsClassifier(\n    n_neighbors=x,\n    metric='euclidean').fit(X_train,y_train) for x in n]\nmodels_manhattan = [KNeighborsClassifier(\n    n_neighbors=x,\n    metric='manhattan').fit(X_train,y_train) for x in n]\n\n\nvalid_minkowski = [models_minkowski[x-1].score(X_test,y_test) for x in n]\nvalid_euclidean = [models_euclidean[x-1].score(X_test,y_test) for x in n]\nvalid_manhattan = [models_manhattan[x-1].score(X_test,y_test) for x in n]\ntrain_manhattan = [models_manhattan[x-1].score(X_test,y_test) for x in n]\n\nfig, ax = plt.subplots(figsize=(8,8))\nax.plot(n,valid_minkowski,label='minkowski')\nax.plot(n,valid_euclidean,label='euclidean')\nax.plot(n,valid_manhattan,label='manhattan')\nax.grid(linestyle='--')\nax.legend()\nax.set_xticks(n)\nax.set_xlabel('n neighbors')\nax.set_ylabel('accuracy')\nax.set_title('Accuracy of KNN Validation')\n\nplt.close('all')\nfig","d1fd1022":"valid_manhattan[19]","a2cf28a7":"KNNmodel = KNeighborsClassifier(n_neighbors=20, metric='manhattan').fit(X_train,y_train)\ny_pred_KNN = KNNmodel.predict(X_test)","681f262a":"# Calculating the KNN confusion matrix then the classification report\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test, y_pred_KNN)\nprint(\"The confusion matrix is as follow:\")\nprint(confusion_matrix)\nprint(classification_report(y_test, y_pred_KNN))","283ca1d1":"# Area under the curve, ROC analysis\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nKNN_roc_auc = roc_auc_score(y_test, KNNmodel.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, KNNmodel.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='KNN  (area = %0.2f)' % KNN_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","1bafc32d":"n = [1,10,50,80,100,200,300,400,450,500,550,600,650,700,800,900,1000]\nmodels = [RandomForestClassifier(n_estimators=x,max_depth=4,random_state=3).fit(X_train,y_train) for x in n]","40eb330d":"train_acc = [models[x].score(X_train,y_train) for x in range(len(n))]\nvalid_acc = [models[x].score(X_test,y_test) for x in range(len(n))]\n\nfig, ax = plt.subplots(figsize=(8,8))\nax.plot(range(len(n)),train_acc,label='train')\nax.plot(range(len(n)),valid_acc,label='valid')\nax.set_xticks(range(len(n)))\nax.set_xticklabels(n)\nax.set_xlabel('trees')\nax.set_ylabel('accuracy')\nax.grid(linestyle='--')\nax.legend()\nax.set_title('Random Forest accuracy with different trees')\n\nplt.close('all')\nfig.savefig('rf.png',bbox_inches='tight')\nfig","efeb95e7":"RF_model = RandomForestClassifier(n_estimators=1000,max_depth=6,random_state=3).fit(X_train,y_train)\ny_pred_RF = RF_model.predict(X_test)","b3cf7bc3":"# Calculating the random forest confusion matrix then the classification report\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test, y_pred_RF)\nprint(\"The confusion matrix is as follow:\")\nprint(confusion_matrix)\nprint(classification_report(y_test, y_pred_RF))","13639ee3":"# Area under the curve, ROC analysis\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nRF_roc_auc = roc_auc_score(y_test, RF_model.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, RF_model.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Random forest  (area = %0.2f)' % RF_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","a1f5b44c":"From the heatmap we can see that Age is positivily correlated with our target value but with a weak correlation. This is followed by the ActiveMember which is a weak negative correlation, followed by balance with 0.12. Balance and Number of products are also negatively correlated which we do not want. ","0558679c":"### The probability for having a female exit","0eae72ea":"The ensembles uses use multiple learning algorithms to obtain better predictive performance than could be obtained using only one. We are going to look at the random forest which is an ensemble of decision trees. We will find out which number of trees is better by looping through different numbers","78fb3cd0":" # The probability for customers to churn is:","c079d318":"### Based on the graph,an esemble of trees(Random forest) of 1000 trees, provides the highest testing accuracy ","d55d858b":"### The probability for having a French customer exit","792f7242":"### The probability for having a Germany customer exit","6d1181f2":"The KNN is a non parametric model that with an algorithm thatassumes that similar things exist in close proximity. In order to find the perfect distance metrics and number of neighbors, we will loop through different neighborghs, and by the help of a graph, we will choose which one gives the greatest accuracy, then do the same using different distance metrics","edf0cbf3":"Both the accuracy and confusion matrix for the decision tree is way better compared to our previous logistic regression model","267db449":"From the confusion matrix of logistic regression, we can see that we have 2352 correct prediction and 621 incorect  predictions. \nRecall is the number of true positive over the number of true negative divided by the number of true positive plus thhe number of false negative, is very low on the people who exited\n\nPrecision which is defined as the number true positives divided by the number of true positives plus the number of false positives is also low. \n\nF1 score which is the balance between precision and recall is also low for the exited customers.\n\nThese metrics will help us identify a better model beyond just looking at the accuracy","2bdeb9d0":"# KNN","bcff7d77":"Decision trees may become more powerfull when it is pruned as when you prune, it reduces the size of the tree by removing sections of the tree that provide little power to classify instance, thus improves accuracy and reduces overfitting. We will try different trees while pruning on different depth with accuracy on our explanotory variable to find out the ideal depth to cut our tree","80d5c788":"### The probability for having a non active customer exit","fa2a3865":"### The probability for having an active customer exit","d7c24193":"### The probability for having a credit card owner exit","f6093cbf":"### The probability for having a Spain customer exit","9f7b5d9b":"### The probability for having someone who doesn't have a credit card owner exit","89e5b944":"### The probability for having a group exit by age","e4daf75d":"# Data Distribution","45045cf7":"The accuracy is better than logistic regression but not above the decision tree","b8ec5a27":"# Logistic regression","61b001bd":"We have seen that we have categorical values in our data, so we need to create dummy variables for models that can not work on categorical values such as logistics regression","6ec95d00":"# Decision trees","1cc6e887":"# Ensembles for classification","ff229c26":"#### If we prune the tree at the depth of 6 that's where we get the highest accuracy","03f76a9e":"### The probability for having a female exit"}}