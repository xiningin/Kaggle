{"cell_type":{"ac39a021":"code","d9433048":"code","39bf27f7":"code","3a37803e":"code","ac812c50":"code","156699d6":"code","a63d3ad2":"code","249ba41f":"code","b97eebaa":"code","4ea390a2":"code","c38408bc":"code","6c77adee":"code","179d192a":"code","86dbe389":"code","c1766d7e":"code","98a14c6a":"code","a4a42586":"code","f73a2271":"code","a634053c":"code","5fbb4950":"code","4dcd8791":"code","6fa4de5c":"code","cabffd9e":"code","35260c24":"code","b8b5d65e":"code","2273bac1":"code","7b555b7c":"code","f62dc03d":"code","16e16c29":"code","4046b0ce":"code","26e13862":"code","4e2a42b9":"code","353d711b":"code","1ea8076d":"code","9fcbf079":"code","fe9f1312":"code","cd9f0b5f":"code","18c78a8c":"code","2e62d7a3":"code","016a38ce":"code","b4767c77":"code","2cd81b81":"code","7b2636d2":"code","d961049a":"code","dd91104c":"code","e1c4e8ca":"code","9862bf41":"code","dcd1e4ca":"code","b5dfa15e":"code","f0b3f9e3":"code","b5eb2811":"code","0b8d3a5a":"code","da5c4dad":"code","a1e5e667":"code","30001094":"markdown","0d58a0ca":"markdown","02fd6cd4":"markdown","99963220":"markdown","2dbd3e03":"markdown","098ef646":"markdown","12942cc6":"markdown","bdcf897b":"markdown","21c45c2e":"markdown","02e8f8bf":"markdown"},"source":{"ac39a021":"import os\nimport numpy as np \nimport pandas as pd \nimport re\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n!pip install neattext\nimport neattext as ntx\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d9433048":"path='\/kaggle\/input\/all-covid19-vaccines-tweets\/vaccination_all_tweets.csv'\ndata = pd.read_csv(path)","39bf27f7":"data.head()","3a37803e":"display(data.shape, str(data.shape[0])+\" tweets in dataset\")   ","ac812c50":"data.info()","156699d6":"data.isna().sum()","a63d3ad2":"data['date'] = pd.to_datetime(data['date']).dt.date  #converting date column to date format\ndata.head()","249ba41f":"# Visulizing Tweet Count vs Location  \nplt.figure(figsize=(15,10))\ndata['user_location'].value_counts().nlargest(20).plot(kind='bar')\nplt.xticks(rotation=60)","b97eebaa":"data=data.drop_duplicates('text')             #dropping duplicate tweets\ndata.shape","4ea390a2":"data.source.value_counts()","c38408bc":"#Visualizing Tweet Platform-wise Distribution \nplt.figure(figsize=(15,10))\ndata['source'].value_counts().nlargest(6).plot(kind='bar')\nplt.xticks(rotation=80)","6c77adee":"len(data['date'].unique())  #Number of days considered","179d192a":"data.sort_values(by=['date'], ascending=[True]).head(2)","86dbe389":"data.drop(columns={\"id\",\"user_name\",\"user_description\",\"user_created\",\"user_followers\",\\\n                   \"user_friends\",\"user_favourites\",\"user_verified\",\"hashtags\",\"source\",\"retweets\",\"favorites\",\"is_retweet\"},inplace=True)\n# dropping unnecessary ","c1766d7e":"pd.set_option('display.max_colwidth', 700)\ndata.head()","98a14c6a":"# Cleaning the data using neattext library\ndata['clean_data']=data['text'].apply(ntx.remove_hashtags)\ndata['clean_data']=data['clean_data'].apply(ntx.remove_urls)\ndata['clean_data']=data['clean_data'].apply(ntx.remove_userhandles)\ndata['clean_data']=data['clean_data'].apply(ntx.remove_multiple_spaces)\ndata['clean_data']=data['clean_data'].apply(ntx.remove_special_characters)","a4a42586":"data[['clean_data','text']].head()","f73a2271":"import nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nlemmatizer = WordNetLemmatizer()\nnltk.download('stopwords')\nnltk.download('wordnet')","a634053c":"#stopwords are the words which won't bring about any changes to the polarity of the tweet\nstop_words = stopwords.words('english')   \nlen(stop_words),stop_words[5:10]","5fbb4950":"# function to remove stopwords\ndef stopWords(tweet):\n  clean_tweet = tweet\n  clean_tweet = \" \".join(word for word in clean_tweet.split() if word not in stop_words)\n# clean_tweet = \" \".join(lemmatizer.lemmatize(word) for word in clean_tweet.split())\n  return clean_tweet","4dcd8791":"data['clean_data'] = data['clean_data'].apply(lambda x: stopWords(x))","6fa4de5c":"data.head(2)","cabffd9e":"from textblob import TextBlob\n# Function to assign polarity and subjectivity to the tweets\ndef blob_fun(text):\n  senti = TextBlob(text)\n  senti_polarity = senti.sentiment.polarity\n  senti_subjectivity = senti.sentiment.subjectivity\n\n  if senti_polarity > 0:\n    res = 'Positive'\n\n  elif senti_polarity < 0:\n    res = 'Negative'\n\n  elif senti_polarity == 0:\n    res =\"Neutral\"\n\n  result = {'polarity':senti_polarity,'subjectivity':senti_subjectivity,'sentiment':res}\n\n  return result","35260c24":"blob_fun(data['clean_data'][5])","b8b5d65e":"# but this isn't always right as shown in the example below\nblob_fun('thank god,i tested negative for covid')","2273bac1":"data['results'] = data['clean_data'].apply(blob_fun)","7b555b7c":"data.drop(columns={\"user_location\",'text'},inplace=True)","f62dc03d":"data.head(2)","16e16c29":"data = data.join(pd.json_normalize(data=data['results']))","4046b0ce":"\ndata.head()","26e13862":"# categorized tweets in seperate Series\npositive_tweet =  data[data['sentiment'] == 'Positive']['clean_data']\nnegative_tweet =  data[data['sentiment'] == 'Negative']['clean_data']\nneutral_tweet =  data[data['sentiment'] == 'Neutral']['clean_data']","4e2a42b9":"from wordcloud import WordCloud\n# Function for creating WordClouds\ndef cloud_of_Words(tweet_cat,title):\n    forcloud = ' '.join([tweet for tweet in tweet_cat])\n    wordcloud = WordCloud(width =500,height = 300,random_state =5,max_font_size=110).generate(forcloud)\n    plt.imshow(wordcloud, interpolation ='bilinear')\n    plt.title(title)\n    plt.axis('off')\n    plt.show()\n    plt.figure(figsize = (10,8))","353d711b":"plt.figure(figsize = (10,8))\n# Creating wordclouds for positive, negative, neutral tweets\ncloud_of_Words(positive_tweet, 'Positive')\ncloud_of_Words(negative_tweet, 'Negative')\ncloud_of_Words(neutral_tweet, 'Neutral')","1ea8076d":"# Breaking down the tweets into words in seperate categories\npositive_tokens = [token for line in positive_tweet for token in line.split()]\nnegative_tokens = [token for line in negative_tweet for token in line.split()]\nneutral_tokens = [token for line in neutral_tweet for token in line.split()]","9fcbf079":"from collections import Counter\n# to get most used words\ndef get_maxtoken(tweets,num=30):\n  word_tokens = Counter(tweets)\n  max_common = word_tokens.most_common(num)\n  return dict(max_common)","fe9f1312":"def token_df_vis(x, title):\n  df = pd.DataFrame(get_maxtoken(x).items(),columns=['words','count'])\n  # plt.figure(figsize = (20,5))\n  # plt.title(title)\n  # plt.xticks(rotation=45)\n  fig = px.bar(df,x='words',y='count',title = title)\n  fig.show()","cd9f0b5f":"token_df_vis(positive_tokens,'Positive')\ntoken_df_vis(negative_tokens,'Negative')\ntoken_df_vis(neutral_tokens,'Neutral')","18c78a8c":"fig = px.scatter(data,x='polarity',y='subjectivity')\nfig.show()","2e62d7a3":"def percent(x,y):\n  return print(\"Percentage of \"+y+\" tweets :\",round(len(x)\/data.shape[0]*100,3),\"%\")","016a38ce":"percent(positive_tweet, 'positive')\npercent(negative_tweet, 'negative')\npercent(neutral_tweet, 'neutral')","b4767c77":"data['sentiment'].value_counts().plot(kind='bar')","2cd81b81":"data.columns","7b2636d2":"deep = data.drop(columns=\"results\")\ndeep.head(2)","d961049a":"# creating reference tags for 5 vaccines -> Pfizer, Covaxin(Bharat Biotech), Sputnik,AstraZenca(Covishield),Moderna\npfizer_refs = [\"Pfizer\",\"pfizer\",\"Pfizer\u2013BioNTech\",\"pfizer-bioNtech\",\"BioNTech\",\"biontech\"]\nbbiotech_refs = [\"covax\",\"covaxin\",\"Covax\",\"Covaxin\",\"Bharat Biotech\",\"bharat biotech\",\"BharatBiotech\",\"bharatbiotech\"]\nsputnik_refs = [\"russia\",\"sputnik\",\"Sputnik\",\"V\"]\nastra_refs = ['sii','SII','adar poonawalla','Covishield','covishield','astra','zenca','Oxford\u2013AstraZeneca','astrazenca','oxford-astrazenca','serum institiute']\nmoderna_refs = ['moderna','Moderna','mRNA-1273','Spikevax']","dd91104c":"def refer(tweet, refs):\n  flag =0\n  for ref in refs:\n    if tweet.find(ref) != -1:\n      flag =1\n  return flag\n\ndeep['pfizer'] = deep['clean_data'].apply(lambda x : refer(x, pfizer_refs))\ndeep['bbiotech'] = deep['clean_data'].apply(lambda x : refer(x, bbiotech_refs))\ndeep['sputnik'] = deep['clean_data'].apply(lambda x : refer(x, sputnik_refs))\ndeep['astra'] = deep['clean_data'].apply(lambda x : refer(x, astra_refs))\ndeep['moderna'] = deep['clean_data'].apply(lambda x : refer(x, moderna_refs))","e1c4e8ca":"display(deep.pfizer.value_counts(),deep.bbiotech.value_counts(),deep.sputnik.value_counts(),deep.astra.value_counts(),deep.moderna.value_counts())","9862bf41":"deep[deep['bbiotech']==1].head()    #what the dataset looks like","dcd1e4ca":"deep[deep['pfizer']==1].head()","b5dfa15e":"def stats(a,b,c,d,e):\n  for i in a,b,c,d,e:\n     display(deep[deep[i]==1][[i,'polarity','subjectivity']].groupby(i).agg([np.mean,np.max,np.min,np.median]))","f0b3f9e3":"stats('pfizer','bbiotech','sputnik','astra','moderna')","b5eb2811":"pfizer = deep[deep['pfizer']==1][['date','polarity']]\nbbiotech = deep[deep['bbiotech']==1][['date','polarity']]\nsputnik = deep[deep['sputnik']==1][['date','polarity']]\nastra = deep[deep['astra']==1][['date','polarity']]\nmoderna = deep[deep['moderna']==1][['date','polarity']]\n\npfizer = pfizer.sort_values(by='date',ascending=True)\nbbiotech = bbiotech.sort_values(by='date',ascending=True)\nsputnik = sputnik.sort_values(by='date',ascending=True)\nastra = astra.sort_values(by='date',ascending=True)\nmoderna = moderna.sort_values(by='date',ascending=True)\n\npfizer['Avg Polarity'] = pfizer.polarity.rolling(20, min_periods=3).mean()\nbbiotech['Avg Polarity'] = bbiotech.polarity.rolling(20, min_periods=3).mean()\nsputnik['Avg Polarity'] = sputnik.polarity.rolling(20, min_periods=3).mean()\nastra['Avg Polarity'] = astra.polarity.rolling(5, min_periods=3).mean()\nmoderna['Avg Polarity'] = moderna.polarity.rolling(20, min_periods=3).mean()","0b8d3a5a":"bbiotech.head(10)","da5c4dad":"a,b,c,d,e = pfizer,bbiotech,sputnik,astra,moderna\nfig = px.line(a, x=\"date\", y=\"Avg Polarity\", title='Pfizer')\nfig.show()\nfig = px.line(b, x=\"date\", y=\"Avg Polarity\", title='Bharat Biotech')\nfig.show()\nfig = px.line(c, x=\"date\", y=\"Avg Polarity\", title='Sputnik')\nfig.show()\nfig = px.line(d, x=\"date\", y=\"Avg Polarity\", title='AstraZence\/Covishield')\nfig.show()\nfig = px.line(e, x=\"date\", y=\"Avg Polarity\", title='Moderna')\nfig.show()","a1e5e667":"total=pd.DataFrame()\ntotal['date'] = sorted(deep['date'].unique())\nsenti=list()\nfor date in total['date']:\n    senti.append(deep[deep['date']==date].polarity.mean())\ntotal['Sentiment']=senti\nfig = px.line(total, x=\"date\", y=\"Sentiment\", title='Overall Sentiment around Vaccines')\nfig.show()   ","30001094":"# Assigning Polarity & Subjectivity to tweets","0d58a0ca":"# Vaccine-wise analysis","02fd6cd4":"# Importing Modules & Data","99963220":"# Visualizing the vaccine Polarity (Moving Average) vs Time","2dbd3e03":"We can see that there is a lot of missing data in user_location, description,sources","098ef646":"The above three wordclouds have the similar words as expected because our main analysis is Covid Vaccine","12942cc6":"Shows the spread of our tweets on polarity vs subjectivity ","bdcf897b":"# Visualizing Overall Vaccine Polarity","21c45c2e":"# Understanding & Preprocessing Data","02e8f8bf":"It's visible that the dataset has covid vaccine related tweets from 12th December ,2020"}}