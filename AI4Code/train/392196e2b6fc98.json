{"cell_type":{"6a771ce6":"code","e0118cc0":"code","83a50b8f":"code","92db60b6":"code","cee90ea7":"code","b7449ae2":"code","03c44ae8":"code","f84de237":"code","a600b8e9":"code","96085c4f":"code","082bf85f":"code","14fce6a2":"code","d2abb8c9":"code","83bb9c84":"code","456e9ac6":"code","43af2eea":"code","16d3532e":"code","438da2a7":"code","b67aa367":"code","1e0ccff7":"code","b2d98800":"code","9ac882e1":"code","a3a766de":"code","eadb0a81":"code","05c79fb2":"code","ec0fb63c":"code","c7e3850e":"code","2f10b71b":"code","2739d3ba":"code","53c6e7f0":"code","2c97aa3d":"code","08167342":"code","a39fc8ce":"code","576b0802":"code","09f30263":"code","a368600a":"code","f0e86e47":"code","c5a2e1bf":"code","d80e436b":"code","4308e131":"code","d7f38fd5":"code","01fef81e":"code","0c3618af":"code","0848c572":"code","c2dee82b":"code","6fefc62b":"code","dec21d87":"code","86d64e62":"code","04f1bfc4":"code","7f718d79":"code","aac09903":"code","d5fddca1":"code","565a42e5":"code","12eeadd9":"code","83d302df":"code","e644aefb":"code","a6672d64":"code","2e1ee10d":"code","12bd4754":"code","535fb2af":"code","0b8b5bb9":"code","24453a89":"code","e724d467":"code","58c08dbe":"code","deff1635":"code","ea13ba59":"code","c39bf615":"code","337323a2":"code","59573350":"code","27f13a85":"code","33d23b17":"code","768cb738":"code","81f69be1":"code","c5589312":"code","007421c6":"code","3620aff6":"code","0a9b386b":"code","359ac296":"code","12bdd290":"code","9e8c0cf5":"code","674c9b65":"code","dceae768":"code","a13053da":"code","9f2619d1":"code","7f4af629":"code","c1f4cfe9":"code","b721077b":"code","e530e302":"code","d314138b":"code","59eb776c":"code","1e4ba349":"code","f5bbe8e4":"code","c950d790":"code","2efc68bd":"code","d1b942a8":"code","002de47e":"code","1e0d780a":"code","611888c5":"code","edb1df40":"code","6d4d1611":"code","d3879eff":"code","73cffa24":"code","30e29f2b":"code","56a0ea02":"code","0baae260":"code","c3538adf":"code","cc68d8dd":"code","741175da":"code","fadc1e9c":"code","e88a673e":"code","33fd716d":"code","9bec3b3f":"code","f53cf0a2":"code","fd94b62d":"code","a64e04bd":"code","88a5d4f8":"markdown","d5d71a2f":"markdown","d368a73d":"markdown","2214a7ca":"markdown","188248d2":"markdown","deb24a93":"markdown","f44f58b5":"markdown","c03bc0d8":"markdown","bcca432b":"markdown","9cf32606":"markdown","85bd5155":"markdown","50e5d9d3":"markdown","d6cf37a7":"markdown","6593ee7e":"markdown","89fb313c":"markdown","c175169e":"markdown","3fda537c":"markdown","572cbc10":"markdown","2b952fe7":"markdown","32443683":"markdown","25764690":"markdown","ad52a0b0":"markdown","500443b9":"markdown","2cafe982":"markdown","b6a288b7":"markdown","e51e9776":"markdown","729c4757":"markdown","b97a899d":"markdown","c47769dc":"markdown","d79e8709":"markdown","79adcd54":"markdown","60845ce4":"markdown","e889a2d2":"markdown","6c122f36":"markdown","c1689189":"markdown","f976bc9b":"markdown","870e1e48":"markdown","06273ec7":"markdown","b12d5119":"markdown","d3c46d8c":"markdown","c74b2cc5":"markdown","8585f3ea":"markdown","9408edc6":"markdown","885df3ac":"markdown","c9596ede":"markdown","17a0b39b":"markdown","2c168411":"markdown","f51ab76a":"markdown","17239cd1":"markdown","0efdba1a":"markdown","3c04a96d":"markdown","ceb89cfe":"markdown","b95b8a28":"markdown","96df732b":"markdown","0cebedb7":"markdown","e819c613":"markdown","c8e5d61e":"markdown"},"source":{"6a771ce6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport catboost\nfrom catboost import CatBoostRegressor\nfrom catboost import Pool, cv\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import product\nimport gc\nimport shap\nshap.initjs()\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e0118cc0":"items = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')\nitem_categories = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\ntrain = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nshops = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')\ntest = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv').set_index('ID')","83a50b8f":"items.head()\nsample_submission.head()\nitem_categories.head()\ntrain.head()\nshops.head()\ntest.head()","92db60b6":"train.head()\ntrain.shape\ntrain.isnull().sum()\ntrain.info()\ntrain.describe()","cee90ea7":"test.head()\ntest.shape\ntest.isnull().sum()\ntest.info()\ntest.describe()","b7449ae2":"sns.histplot(train, x=\"item_price\")\n","03c44ae8":"plt.figure(figsize=(10,4))\nsns.boxplot(x=train[\"item_price\"])\n","f84de237":"train_item_price_gt_100K = train[train['item_price'] >= 100000]\ntrain_item_price_gt_100K","a600b8e9":"train = train[train['item_price'] <= 100000]","96085c4f":"plt.figure(figsize=(10,4))\nsns.boxplot(x=train[\"item_price\"])","082bf85f":"plt.xlim(0,10000)\nplt.ylim(0, 50000)\nsns.histplot(train, x=\"item_price\")","14fce6a2":"train_item_cnt_day_gt_1000 = train[train.item_cnt_day > 1000]\ntrain_item_cnt_day_gt_1000","d2abb8c9":"train = train[train.item_cnt_day < 1000]","83bb9c84":"plt.figure(figsize=(10,4))\nsns.boxplot(x=train.item_cnt_day)","456e9ac6":"train.item_cnt_day.value_counts()","43af2eea":"train.item_cnt_day[train.item_cnt_day < 0].value_counts()","16d3532e":"print('% of error points {}'.format(round(100 * len(train[train.item_cnt_day < 0]) \/ len(train), 5)))","438da2a7":"train_item_cnt_day_lt_0 = train[train.item_cnt_day < 0]","b67aa367":"print('{} % of the test set exists in the shop ids of error points'.format(test.shop_id.isin(train_item_cnt_day_lt_0.shop_id).sum() \/ len(test)))","1e0ccff7":"print('{} % of the test set exists in the shop ids of error points'.format(test.item_id.isin(train_item_cnt_day_lt_0.item_id).sum() \/ len(test)))","b2d98800":"train[train.item_price < 0].value_counts()","9ac882e1":"train_shop_id_eq_32_item_id_2973 = train[(train.shop_id==32) & (train.item_id==2973)]","a3a766de":"sns.histplot(data=train_shop_id_eq_32_item_id_2973, x='item_price', bins=10)","eadb0a81":"train.loc[train.item_price < 0, 'item_price'] = train[(train.shop_id==32) & (train.item_id==2973)]['item_price'].median()","05c79fb2":"train[train.item_price < 0].value_counts()","ec0fb63c":"# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\ntrain.loc[train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","c7e3850e":"shops_item_cnt_day_lt_0  = list(train_item_cnt_day_lt_0.shop_id.unique())\nfor i, shop in enumerate(shops_item_cnt_day_lt_0):\n    plt.figure(i)\n    plt.title('Shop #{}'.format(shop))\n    sns.histplot(data=train[train.shop_id == shop], x = 'item_cnt_day')","2f10b71b":"new_item_cnt_day = []\nfor row in train.itertuples():\n    if row.item_cnt_day < 0:\n        item_cnt_day = train[train.shop_id == row.shop_id]['item_cnt_day'].median()\n    else: \n        item_cnt_day = row.item_cnt_day\n    new_item_cnt_day.append(item_cnt_day)\n    \ntrain.item_cnt_day = new_item_cnt_day","2739d3ba":"train.date = pd.to_datetime(train.date,  format='%d.%m.%Y')\n# Lets see the max and minumum date\ntrain.date.min()\ntrain.date.max()\n# Train shape\ntrain.shape","53c6e7f0":"train.sort_values(by=['date'], ascending=True, inplace=True)\ntrain.tail()","2c97aa3d":"train['month'] = train['date'].dt.month\ntrain['days_in_month'] = train['date'].dt.daysinmonth","08167342":"train.head()\ntrain.tail()","a39fc8ce":"train.drop(['date'], inplace=True, axis = 1)","576b0802":"train.head()","09f30263":"# We will add more element to this dict as we go forward\ndtypes_dict = {'date_block_num': np.int8, \n              'shop_id': np.int8, \n              'item_id': np.int16, \n              'item_price': np.float32, \n              'item_cnt_day': np.int8,\n               'month': np.int8,\n               'days_in_month': np.int8\n              }","a368600a":"train = train.astype(dtypes_dict)\ntrain.info()","f0e86e47":"shops.head()\nshops.tail()\nshops.isnull().sum()","c5a2e1bf":"shops.loc[shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', 'shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops[['shop_id','city_code']]","d80e436b":"item_categories.head()\nitem_categories.tail()","4308e131":"item_categories['split'] = item_categories['item_category_name'].str.split('-')\nitem_categories['category'] = item_categories['split'].map(lambda x: x[0].strip())\nitem_categories['category_code'] = LabelEncoder().fit_transform(item_categories['category'])","d7f38fd5":"# if subtype is nan then take the \nitem_categories['sub_category'] = item_categories['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\nitem_categories['sub_category_code'] = LabelEncoder().fit_transform(item_categories['sub_category'])\n","01fef81e":"# Final item_categories dataset\nitem_categories = item_categories[['item_category_id','category_code', 'sub_category_code']]","0c3618af":"items.head()\nitems.tail()","0848c572":"items.drop(['item_name'], axis=1, inplace=True)","c2dee82b":"comb = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = train[train.date_block_num==i]\n    comb.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n    \ncomb = pd.DataFrame(np.vstack(comb), columns=cols)\ncomb['date_block_num'] = comb['date_block_num'].astype(np.int8)\ncomb['shop_id'] = comb['shop_id'].astype(np.int8)\ncomb['item_id'] = comb['item_id'].astype(np.int16)\ncomb.sort_values(cols,inplace=True)","6fefc62b":"comb.head()\ncomb.shape","dec21d87":"train['revenue'] = train['item_price'] *  train['item_cnt_day']","86d64e62":"group = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\n\ncomb = pd.merge(comb, group, on=cols, how='left')\ncomb['item_cnt_month'] = (comb['item_cnt_month']\n                                .fillna(0)\n                                .clip(0,20) # NB clip target here\n                                .astype(np.float16))","04f1bfc4":"comb.head()","7f718d79":"comb.shape","aac09903":"test['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)","d5fddca1":"comb = pd.concat([comb, test], ignore_index=True, sort=False, keys=cols)\ncomb.fillna(0, inplace=True)","565a42e5":"comb.head()\ncomb.tail()","12eeadd9":"comb = pd.merge(comb, shops, on=['shop_id'], how='left')\ncomb = pd.merge(comb, items, on=['item_id'], how='left')\ncomb = pd.merge(comb, item_categories, on=['item_category_id'], how='left')\n# Lets see the dataframe now\ncomb.head()\ncomb.shape\ncomb.info()","83d302df":"# Lets update the dtypes_dict\ndtypes_dict['city_code'] = np.int8\ndtypes_dict['item_category_id'] = np.int8\ndtypes_dict['category_code'] = np.int8\ndtypes_dict['sub_category_code'] = np.int8\ndtypes_dict['item_cnt_month'] = np.int8","e644aefb":"dtypes_dict","a6672d64":"comb = comb.astype(dict((k, dtypes_dict[k]) for k in comb.columns))\ncomb.info()","2e1ee10d":"def create_lag_features(df, time_feature, keys, lags, on):\n    \"\"\"\n    This function is to create lage features of a specific target for multiple lags\n    df : pd.DataFrame, DataFrame in which we are trying to create lag features\n    time_feature: list, \n    keys: list, list of static columns that will not contribute in creating lags\n    lags: list, list of window periods to create lags on\n    on : list, target feature \n    \"\"\"\n    temp = df[keys+on]\n    for lag in lags:\n        shifted_temp = temp.copy()\n        shifted_temp.columns = keys+[on[0]+'_lag_'+str(lag)]\n        shifted_temp[time_feature] +=lag\n        df = pd.merge(df, shifted_temp, on = keys, how = 'left')\n    return df","12bd4754":"comb = create_lag_features(df = comb, keys=['date_block_num','shop_id','item_id'], time_feature = 'date_block_num',lags = [1, 2, 3, 6, 12],  on = ['item_cnt_month'])","535fb2af":"comb['item_cnt_month_lag_1'] = comb['item_cnt_month_lag_1'].astype(np.float16)\ncomb['item_cnt_month_lag_2'] = comb['item_cnt_month_lag_2'].astype(np.float16)\ncomb['item_cnt_month_lag_3'] = comb['item_cnt_month_lag_3'].astype(np.float16)\ncomb['item_cnt_month_lag_6'] = comb['item_cnt_month_lag_6'].astype(np.float16)\ncomb['item_cnt_month_lag_12'] = comb['item_cnt_month_lag_12'].astype(np.float16)\n\n\ncomb.head()\ncomb.info()","0b8b5bb9":"group = comb.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'avg_item_cnt_per_month' ]\ngroup.reset_index(inplace=True)\ncomb = pd.merge(comb, group, on=['date_block_num'], how='left')\ncomb.head()","24453a89":"comb['avg_item_cnt_per_month'] = comb['avg_item_cnt_per_month'].astype(np.float16)\ncomb = create_lag_features(df = comb, keys=['date_block_num','shop_id','item_id'], time_feature = 'date_block_num',lags = [1],  on = ['avg_item_cnt_per_month'])\ncomb.head()","e724d467":"group = comb.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'avg_item_cnt_per_month_per_shop' ]\ngroup.reset_index(inplace=True)\ncomb = pd.merge(comb, group, on=['date_block_num','shop_id'], how='left')\ncomb.head()","58c08dbe":"comb['avg_item_cnt_per_month_per_shop'] = comb['avg_item_cnt_per_month_per_shop'].astype(np.float16)\n# matrix = lag_feature(matrix, [1,2,3,6,12], 'date_shop_avg_item_cnt')\ncomb = create_lag_features(df = comb, keys=cols, time_feature = 'date_block_num',lags = [1,2,3,6,12],  on = ['avg_item_cnt_per_month_per_shop'])\ncomb.head()\n# matrix.drop(['date_shop_avg_item_cnt'], axis=1, inplace=True)","deff1635":"comb.info()","ea13ba59":"group = comb.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'avg_item_cat_id_item_cnt_per_month' ]\ngroup.reset_index(inplace=True)\ncomb = pd.merge(comb, group, on=['date_block_num','item_category_id'], how='left')\ncomb.head()","c39bf615":"comb['avg_item_cat_id_item_cnt_per_month'] = comb['avg_item_cat_id_item_cnt_per_month'].astype(np.float16)\ncomb = create_lag_features(df = comb, keys=cols, time_feature = 'date_block_num',lags = [1],  on = ['avg_item_cat_id_item_cnt_per_month'])\ncomb.head()\n# matrix = lag_feature(matrix, [1], 'date_cat_avg_item_cnt')\n\n# matrix.drop(['date_cat_avg_item_cnt'], axis=1, inplace=True)","337323a2":"group = comb.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['avg_item_cnt_month_per_shop_per_item_cat']\ngroup.reset_index(inplace=True)\ncomb = pd.merge(comb, group, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\ncomb.head()\n","59573350":"comb['avg_item_cnt_month_per_shop_per_item_cat'] = comb['avg_item_cnt_month_per_shop_per_item_cat'].astype(np.float16)\ncomb = create_lag_features(df = comb, keys=cols, time_feature = 'date_block_num',lags = [1],  on = ['avg_item_cnt_month_per_shop_per_item_cat'])\ncomb.head()","27f13a85":"group = comb.groupby(['date_block_num', 'shop_id', 'category_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['avg_item_cnt_month_per_cat_per_shop']\ngroup.reset_index(inplace=True)\ncomb = pd.merge(comb, group, on=['date_block_num', 'shop_id', 'category_code'], how='left')\ncomb.head()","33d23b17":"comb['avg_item_cnt_month_per_cat_per_shop'] = comb['avg_item_cnt_month_per_cat_per_shop'].astype(np.float16)\ncomb = create_lag_features(df = comb, keys=cols, time_feature = 'date_block_num',lags = [1],  on = ['avg_item_cnt_month_per_cat_per_shop'])\ncomb.head()","768cb738":"comb.info()","81f69be1":"group = comb.groupby(['date_block_num', 'shop_id', 'sub_category_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['avg_item_cnt_month_per_sub_cat_per_shop']\ngroup.reset_index(inplace=True)\ncomb = pd.merge(comb, group, on=['date_block_num', 'shop_id', 'sub_category_code'], how='left')\ncomb.head()","c5589312":"comb['avg_item_cnt_month_per_sub_cat_per_shop'] = comb['avg_item_cnt_month_per_sub_cat_per_shop'].astype(np.float16)\ncomb = create_lag_features(df = comb, keys=cols, time_feature = 'date_block_num',lags = [1],  on = ['avg_item_cnt_month_per_sub_cat_per_shop'])\ncomb.head()","007421c6":"group = comb.groupby(['date_block_num', 'city_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'avg_item_cnt_month_per_city' ]\ngroup.reset_index(inplace=True)\ncomb = pd.merge(comb, group, on=['date_block_num', 'city_code'], how='left')\ncomb.head()","3620aff6":"comb['avg_item_cnt_month_per_city'] = comb['avg_item_cnt_month_per_city'].astype(np.float16)\ncomb = create_lag_features(df = comb, keys=cols, time_feature = 'date_block_num',lags = [1],  on = ['avg_item_cnt_month_per_city'])\ncomb.head()","0a9b386b":"group = comb.groupby(['date_block_num', 'item_id', 'city_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'avg_item_cnt_month_per_item_per_city' ]\ngroup.reset_index(inplace=True)\ncomb = pd.merge(comb, group, on=['date_block_num', 'item_id', 'city_code'], how='left')\ncomb.head()","359ac296":"comb['avg_item_cnt_month_per_item_per_city'] = comb['avg_item_cnt_month_per_item_per_city'].astype(np.float16)\ncomb = create_lag_features(df = comb, keys=cols, time_feature = 'date_block_num',lags = [1],  on = ['avg_item_cnt_month_per_item_per_city'])\ncomb.head()","12bdd290":"group = comb.groupby(['date_block_num', 'category_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'avg_item_cnt_month_per_cat_per_month' ]\ngroup.reset_index(inplace=True)\ncomb = pd.merge(comb, group, on=['date_block_num', 'category_code'], how='left')\ncomb.head()","9e8c0cf5":"comb['avg_item_cnt_month_per_cat_per_month'] = comb['avg_item_cnt_month_per_cat_per_month'].astype(np.float16)\n# matrix = lag_feature(matrix, [1], 'date_type_avg_item_cnt')\n# matrix.drop(['date_type_avg_item_cnt'], axis=1, inplace=True)\ncomb = create_lag_features(df = comb, keys=cols, time_feature = 'date_block_num',lags = [1],  on = ['avg_item_cnt_month_per_cat_per_month'])\ncomb.head()","674c9b65":"del train_item_cnt_day_gt_1000, train_item_cnt_day_lt_0, train_item_price_gt_100K, train_shop_id_eq_32_item_id_2973, shops, item_categories, group\ngc.collect()","dceae768":"group = comb.groupby(['date_block_num', 'sub_category_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'avg_item_cnt_month_per_sub_cat_per_month' ]\ngroup.reset_index(inplace=True)\ncomb = pd.merge(comb, group, on=['date_block_num', 'sub_category_code'], how='left')\ncomb.head()","a13053da":"comb['avg_item_cnt_month_per_sub_cat_per_month'] = comb['avg_item_cnt_month_per_sub_cat_per_month'].astype(np.float16)\ncomb = create_lag_features(df = comb, keys=cols, time_feature = 'date_block_num',lags = [1],  on = ['avg_item_cnt_month_per_sub_cat_per_month'])\ncomb.head()","9f2619d1":"# import sys\n# def sizeof_fmt(num, suffix='B'):\n#     ''' by Fred Cirera,  https:\/\/stackoverflow.com\/a\/1094933\/1870254, modified'''\n#     for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n#         if abs(num) < 1024.0:\n#             return \"%3.1f %s%s\" % (num, unit, suffix)\n#         num \/= 1024.0\n#     return \"%.1f %s%s\" % (num, 'Yi', suffix)\n\n# for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n#                          key= lambda x: -x[1])[:10]:\n#     print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))","7f4af629":"# del group, items, sales\n# gc.collect()","c1f4cfe9":"# comb.to_pickle('\/kaggle\/input\/competitive-data-science-predict-future-sales\/comb.pkl')","b721077b":"# del comb\n# gc.collect()","e530e302":"# comb = pd.read_pickle('\/kaggle\/input\/competitive-data-science-predict-future-sales\/comb.pkl')\n# comb.head()","d314138b":"group = train.groupby(['item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['avg_item_price']\ngroup.reset_index(inplace=True)\n\ncomb = pd.merge(comb, group, on=['item_id'], how='left')\ncomb['avg_item_price'] = comb['avg_item_price'].astype(np.float16)","59eb776c":"comb.head()","1e4ba349":"group = train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['avg_item_price_per_month']\ngroup.reset_index(inplace=True)\n\ncomb = pd.merge(comb, group, on=['date_block_num','item_id'], how='left')\ncomb['avg_item_price_per_month'] = comb['avg_item_price_per_month'].astype(np.float16)\ncomb.head()","f5bbe8e4":"lags = [1,2,3,4,5,6]\ncomb = create_lag_features(df = comb, keys=cols, time_feature = 'date_block_num',lags = lags,  on = ['avg_item_price_per_month'])\ncomb.head()","c950d790":"for i in lags:\n    comb['delta_price_lag_'+str(i)] = \\\n        (comb['avg_item_price_per_month_lag_'+str(i)] - comb['avg_item_price']) \/ comb['avg_item_price']","2efc68bd":"def select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_'+str(i)]:\n            return row['delta_price_lag_'+str(i)]\n    return 0","d1b942a8":"comb['delta_price_lag'] = comb.apply(select_trend, axis=1)\ncomb['delta_price_lag'] = comb['delta_price_lag'].astype(np.float16)\ncomb['delta_price_lag'].fillna(0, inplace=True)\ncomb.head()","002de47e":"fetures_to_drop = ['avg_item_price', 'avg_item_price_per_month']\nfor i in lags:\n    fetures_to_drop += ['avg_item_price_per_month_lag_'+str(i)]\n    fetures_to_drop += ['delta_price_lag_'+str(i)]\n\ncomb.drop(fetures_to_drop, axis=1, inplace=True)\ncomb.head()","1e0d780a":"group = train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\ngroup.columns = ['sum_revenue_per_month_per_shop']\ngroup.reset_index(inplace=True)\n\ncomb = pd.merge(comb, group, on=['date_block_num','shop_id'], how='left')\ncomb['sum_revenue_per_month_per_shop'] = comb['sum_revenue_per_month_per_shop'].astype(np.float32)\ncomb.head()","611888c5":"group = group.groupby(['shop_id']).agg({'sum_revenue_per_month_per_shop': ['mean']})\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\ncomb = pd.merge(comb, group, on=['shop_id'], how='left')\ncomb['shop_avg_revenue'] = comb['shop_avg_revenue'].astype(np.float32)\n\n","edb1df40":"comb['delta_revenue'] = (comb['sum_revenue_per_month_per_shop'] - comb['shop_avg_revenue']) \/ comb['shop_avg_revenue']\ncomb['delta_revenue'] = comb['delta_revenue'].astype(np.float16)\n\ncomb = create_lag_features(df = comb, keys=cols, time_feature = 'date_block_num',lags = [1],  on = ['delta_revenue'])\ncomb.head()","6d4d1611":"month_dict = dict(train[['date_block_num', 'month']].values)\nmonth_dict[34] = 11\ndays_in_month_dict = dict(train[['month', 'days_in_month']].values)\ncomb['month'] = comb.date_block_num.map(month_dict)\ncomb['days_in_month'] = comb.month.map(days_in_month_dict)\ncomb.head()","d3879eff":"cache = {}\ncomb['item_shop_last_sale'] = -1\ncomb['item_shop_last_sale'] = comb['item_shop_last_sale'].astype(np.int8)\nfor idx, row in comb.iterrows():    \n    key = str(row.item_id)+' '+str(row.shop_id)\n    if key not in cache:\n        if row.item_cnt_month!=0:\n            cache[key] = row.date_block_num\n    else:\n        last_date_block_num = cache[key]\n        comb.at[idx, 'item_shop_last_sale'] = row.date_block_num - last_date_block_num\n        cache[key] = row.date_block_num      \n        \ncomb.head()","73cffa24":"comb['item_last_sale'] = -1\ncomb['item_last_sale'] = comb['item_last_sale'].astype(np.int8)\nfor idx, row in comb.iterrows():    \n    key = row.item_id\n    if key not in cache:\n        if row.item_cnt_month!=0:\n            cache[key] = row.date_block_num\n    else:\n        last_date_block_num = cache[key]\n        if row.date_block_num>last_date_block_num:\n            comb.at[idx, 'item_last_sale'] = row.date_block_num - last_date_block_num\n            cache[key] = row.date_block_num       \ncomb.head()","30e29f2b":"comb['item_shop_first_sale'] = comb['date_block_num'] - comb.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\ncomb['item_first_sale'] = comb['date_block_num'] - comb.groupby('item_id')['date_block_num'].transform('min')\ncomb.head()","56a0ea02":"comb = comb[comb.date_block_num > 11]","0baae260":"comb.head()","c3538adf":"def fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df","cc68d8dd":"comb = fill_na(comb)\ncomb.head()","741175da":"X_train = comb[comb.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = comb[comb.date_block_num < 33]['item_cnt_month']\nX_valid = comb[comb.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = comb[comb.date_block_num == 33]['item_cnt_month']\nX_test = comb[comb.date_block_num == 34].drop(['item_cnt_month'], axis=1)","fadc1e9c":"catboost = CatBoostRegressor(iterations=1000, \n                            learning_rate=0.1, \n                            max_depth= 8, \n                            loss_function='RMSE', \n                            early_stopping_rounds=10, \n                            verbose=True, \n                            )","e88a673e":"catboost.fit(X= X_train, \n             y=Y_train, \n            eval_set=(X_valid, Y_valid ),\n            verbose=True, \n            plot = True)","33fd716d":"mean_squared_error(Y_valid, catboost.predict(X_valid).clip(0,20))**0.5","9bec3b3f":"y_pred = catboost.predict(X_test).clip(0,20)","f53cf0a2":"def plot_feature_importance(importance,names,model_type):\n    \n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n    \n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n    \n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n    \n    #Define size of bar plot\n    plt.figure(figsize=(12,10))\n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    #Add chart labels\n    plt.title(model_type + 'FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')","fd94b62d":"plot_feature_importance(catboost.feature_importances_, X_train.columns,'CATBOOST')\n","a64e04bd":"submission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": y_pred\n})\nsubmission.to_csv('catboost_submission.csv', index=False)\n","88a5d4f8":"Lets see the `item_categories`","d5d71a2f":"Lets convert `date` column in the `train` dataset","d368a73d":"Lets create some lag features for the target!","2214a7ca":"Lets see `items` dataset","188248d2":"Because we have generated lags up to 12 month, we have a lot of NaNs so, I will keep only the data for more than 11 months.","deb24a93":"What is the percentage of these points of the total dataset?","f44f58b5":"Lets see more about `item_price`","c03bc0d8":"Lets drop the `item_name`","bcca432b":"Of course, I don't understand Russian but lets try searching and translating some names.\n\nThe first sylbus of the shope name includes the city! Lets extract them as a new feature.","9cf32606":"Now, lets merge construct a monthly dataset by grouping the `train` then merging it with `comb`. ","85bd5155":"Lets investigate a little bit!","50e5d9d3":"Now, lets calculate `avg_item_cnt_month_per_sub_cat_per_shop`","d6cf37a7":"Read the datasets","6593ee7e":"Now, lets construct a `comb` datafram that has all the combinations of `shop_id` and `item_id` for each month.","89fb313c":"I'm thinking of removing these points but we may have some shops and products in the test set that are using the same rows! Lets see more.","c175169e":"`avg_shop_revenue`","3fda537c":"`sum_revenue_per_month_per_shop`","572cbc10":"Now, we will create `avg_item_cat_id_item_cnt_per_month`","2b952fe7":"Now, lets caculate the `revenue`","32443683":"Lets see more about the `train` dataset","25764690":"Lets see more about `shops` dataset","ad52a0b0":"Lets dig deeper into `item_cmt_day`","500443b9":"Now, lets set the data types for all columns for better memory use!","2cafe982":"This is perfect!","b6a288b7":"Great we don't have any NaNs! Now, we will investigate the values of the colums in more details.","e51e9776":"Now, lets add the `month` and `days_in_month` features","729c4757":"Yeah! we have some outliers! Let's see the rows with price greater than 100000","b97a899d":"I think `median` will be better!","c47769dc":"Now, will create `avg_item_cnt_month_per_cat_per_shop`","d79e8709":"`item_last_sale`","79adcd54":"Let's remove that point!","60845ce4":"Lets drop the `date` column as we will work monthly.","e889a2d2":"Now, lets append the `test` set to add features to all of the data.","6c122f36":"Now, lets create `avg_item_cnt_per_month_per_shop`","c1689189":"`item_category_name` has the category and sub-category on it. ","f976bc9b":"We have negative values for `item_cnt_day`! it seems like they are error points!","870e1e48":"Lets sort `train` according to date","06273ec7":"Lets calculate, `avg_item_cnt_month_per_city`","b12d5119":"`avg_item_cnt_month_per_cat_per_month`","d3c46d8c":"Lets fix some duplidations issues in the `shop_id`. Find out more in this [kernel](www.kaggle.com\/dlarionov\/feature-engineering-xgboost?scriptVersionId=4396431&cellId=12)","c74b2cc5":"Now! its impossible to remove these points but lets have another look on the `item_id` !","8585f3ea":"`item_shop_last_sale`","9408edc6":"Nice!","885df3ac":"Lets merge `items`, `item_categories`, and `shops` to the `comb`.","c9596ede":"`avg_item_cnt_month_per_sub_cat_per_month`","17a0b39b":"Lets see about `test` dataset","2c168411":"Also, we have one point in `item_price` in negative! Now! Let's see the best imputation technique.","f51ab76a":"Lets create `avg_item_cnt_per_month` with some lag features","17239cd1":"Great! Lets see the `item_cnt_day` best imputation technique","0efdba1a":"Now, will create `avg_item_cnt_month_per_shop_per_item_cat`","3c04a96d":"`item_shop_fist_sale` and `item_first_sale`","ceb89cfe":"Lets calculate, `avg_item_cnt_month_per_item_per_city`","b95b8a28":"Lets create some `Mean Encoded Features`","96df732b":"Lets see more info about `item_cnt_day`","0cebedb7":"No data leakage found. The data exactly matches the description!","e819c613":"The graph is heavily skewed to the right! Looks like we have some outliers! Lets dig deeper.","c8e5d61e":"I would go with `median` imputation. `mean` imputation here will be very unrealistic"}}