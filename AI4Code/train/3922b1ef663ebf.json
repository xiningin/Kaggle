{"cell_type":{"efacb803":"code","65a2e4ff":"code","53cb8273":"code","248bc4c8":"code","b945b4b6":"code","110c9eb5":"code","4a7f297e":"code","8c8d7aad":"code","9dbe7861":"code","7826ead2":"markdown","e91ad180":"markdown","65c902af":"markdown","1e779320":"markdown","2def9b2f":"markdown","0a95d69e":"markdown","6e104001":"markdown"},"source":{"efacb803":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import Sequential, layers, callbacks\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\n\nimport lightgbm as lgb","65a2e4ff":"train = pd.read_csv('..\/input\/train.csv', index_col=0)\ntest = pd.read_csv('..\/input\/test.csv', index_col=0)\n\ntarget = train.target.values\ntrain.drop('target', axis=1, inplace=True)\ntrain.shape, target.shape, test.shape, ","53cb8273":"pos_idx = (target == 1)\nneg_idx = (target == 0)\nstats = []\nfor col in train.columns:\n    stats.append([\n        train.loc[pos_idx, col].mean(),\n        train.loc[pos_idx, col].std(),\n        train.loc[neg_idx, col].mean(),\n        train.loc[neg_idx, col].std()\n    ])\n    \nstats_df = pd.DataFrame(stats, columns=['pos_mean', 'pos_sd', 'neg_mean', 'neg_sd'])\nstats_df.head()","248bc4c8":"zval1 = (train.values - stats_df.neg_mean.values) \/ stats_df.neg_sd.values\nzval2 = (train.values - stats_df.pos_mean.values) \/ stats_df.pos_sd.values\ntr_zval = np.column_stack([zval1, zval2])\ntr_zval.shape","b945b4b6":"zval1 = (test.values - stats_df.neg_mean.values) \/ stats_df.neg_sd.values\nzval2 = (test.values - stats_df.pos_mean.values) \/ stats_df.pos_sd.values\nte_zval = np.column_stack([zval1, zval2])\nte_zval.shape","110c9eb5":"def augment(X, y, times=8):\n    # up-sample positive samples\n    pos_idx = (y == 1)\n    nsample = times * pos_idx.sum()\n    \n    X_sample = []\n    for i in range(X.shape[1]):\n        X_sample.append(np.random.choice(X[pos_idx, i], size=nsample))\n        \n    X_sample = np.column_stack(X_sample)\n    \n    # shuffle\n    idx = np.arange(X.shape[0] + nsample)\n    np.random.shuffle(idx)\n    \n    return np.vstack([X, X_sample])[idx], np.hstack([y, np.ones(nsample)])[idx]","4a7f297e":"nfold = 5\nkfold = StratifiedKFold(n_splits=nfold, shuffle=True)\n\nnn_oof_tr, nn_oof_te = np.zeros(tr_zval.shape[0]), 0\nlgb_oof_tr, lgb_oof_te = np.zeros(tr_zval.shape[0]), 0\nnb_oof_tr, nb_oof_te = np.zeros(tr_zval.shape[0]), 0\n\nfor tr_idx, va_idx in kfold.split(tr_zval, target):\n    X_tr, y_tr = tr_zval[tr_idx], target[tr_idx] \n    X_va, y_va = tr_zval[va_idx], target[va_idx]\n    # data augment\n    X_tr, y_tr = augment(X_tr, y_tr)\n    \n    # NN model\n    model = Sequential([\n        layers.Dense(16, input_shape=(400, 1), activation='relu'),\n        layers.Flatten(),\n        layers.Dense(1, activation='sigmoid')\n    ])\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_crossentropy'])\n    earlystop = callbacks.EarlyStopping(patience=10)\n    # reshape data for nn input\n    X_tr_re = X_tr[:, :, np.newaxis]\n    X_va_re = X_va[:, :, np.newaxis]\n    model.fit(X_tr_re, y_tr, validation_data=(X_va_re, y_va), epochs=100, verbose=2, batch_size=256, callbacks=[earlystop])\n    # get oof\n    nn_oof_tr[va_idx] = model.predict(X_va_re).flatten()\n    X_te_re = te_zval[:, :, np.newaxis]\n    nn_oof_te += model.predict(X_te_re).flatten() \/ nfold\n    \n    # LGB model\n    param = {\n        'objective': 'binary',\n        'boost': 'gbdt',\n        'metric': 'auc',\n        'learning_rate': 0.01,\n        'num_leaves': 13,\n        'max_depth': -1,\n        'feature_fraction': 0.05,\n        'bagging_freq': 5,\n        'bagging_fraction': 0.4,\n        'min_data_in_leaf': 80,\n        'min_sum_hessian_in_leaf': 10,\n        'num_threads': 4\n    }\n    trn_data = lgb.Dataset(X_tr, y_tr)\n    val_data = lgb.Dataset(X_va, y_va)\n    clf = lgb.train(param, trn_data, 100000, valid_sets=(val_data), early_stopping_rounds=600, verbose_eval=600)\n    # get oof\n    lgb_oof_tr[va_idx] = clf.predict(X_va)\n    lgb_oof_te += clf.predict(te_zval) \/ nfold\n    \n    # Naive Bayes model\n    bayes = GaussianNB()\n    bayes.fit(X_tr, y_tr)\n    nb_oof_tr[va_idx] = bayes.predict_proba(X_va)[:, 1]\n    nb_oof_te += bayes.predict_proba(te_zval)[:, 1] \/ nfold","8c8d7aad":"lr = LogisticRegression()\nlr.fit(np.column_stack([nn_oof_tr, lgb_oof_tr, nb_oof_tr]), target)\npred = lr.predict_proba(np.column_stack([nn_oof_te, lgb_oof_te, nb_oof_te]))[:, 1]","9dbe7861":"pd.DataFrame({\n    'ID_code': test.index,\n    'target': pred\n}).to_csv('sub.csv', index=False)","7826ead2":"### Data augment","e91ad180":"### Standerize train\/test data","65c902af":"Based on [@Branden Murray](https:\/\/www.kaggle.com\/brandenkmurray)'s hypothesis **For each feature they had a distribution for target==0 and a distribution for target==1 and they randomly sampled from each and then put it together**, we can up-sample positive samples to get a balanced dataset. Since the positive\/negative ratio is 1\/9, I'm going to augment the positive sample 8 times to make them balance.\n\nWe only apply up-sample to train fold, not valid fold.","1e779320":"### Stacking","2def9b2f":"### Cross validation for NN\/LGB\/NaiveBayes models","0a95d69e":"In the kernel [A proof of synthetic data](https:\/\/www.kaggle.com\/jiazhuang\/a-proof-of-synthetic-data), I have tried to calculate Z-value for each sample, which I think is also a good way to standerize data. So in this kernel, I will use the Z-value as features to train a few models, including lightgbm, naive bayes, neural network, then stacking them together using logistic regression.","6e104001":"### Calculate mean\/sd of positive and negative samples for each feature"}}