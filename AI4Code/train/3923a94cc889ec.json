{"cell_type":{"ca80cee1":"code","81a9697a":"code","9bc225e6":"code","610fd847":"code","c09ca42f":"code","a513e405":"code","fd1a4074":"code","953eb372":"code","15b48485":"code","2cd0fb5a":"code","d504a239":"code","82a2d318":"code","29d2c899":"code","cb4af8bc":"code","2dbd5fd6":"code","3afecd05":"code","b94fb908":"code","7111d29b":"code","247eb32d":"code","65e09306":"code","e114cbce":"code","a3962c35":"code","b45f6eb0":"code","e81bd64e":"code","4bc94af7":"code","aa5c67be":"code","1cfc3b5e":"code","4d331fd8":"code","b3a5a1e8":"code","7844d2dd":"code","da0646e5":"code","6bc7f932":"code","69ef43d8":"code","45f5f6a1":"code","f7f34e07":"markdown","8a6afc26":"markdown","6f174dc0":"markdown","d9b7969d":"markdown","432bff38":"markdown","3848d0be":"markdown","00d0a827":"markdown","8699f826":"markdown","e4a71673":"markdown","36d29e43":"markdown","419c0eff":"markdown","8bc44227":"markdown","cb049bf5":"markdown","224a799f":"markdown","4538d297":"markdown","92b073b5":"markdown","b60c1b8f":"markdown","30fc80f3":"markdown","9fc2f958":"markdown","55297c59":"markdown","a830c131":"markdown","b896c9d9":"markdown","4e5b0b2b":"markdown","f1ac8cdf":"markdown","567648fa":"markdown","4fe28ae8":"markdown","1593d71d":"markdown","d25886e1":"markdown","8400f3ad":"markdown","f7f9f680":"markdown","6ae76083":"markdown"},"source":{"ca80cee1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\n%matplotlib inline","81a9697a":"import os # accessing directory structure\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9bc225e6":"# import data from Excel csv sheet\ndf = pd.read_csv('..\/input\/machine-learning-for-diabetes-with-python\/diabetes_data.csv')\n\n# show first 5 records of dataset\ndf.head()","610fd847":"# return the object type, which is dataframe\ntype(df)","c09ca42f":"# display the number of entries, the number and names of the column attributes, the data type and\n    # digit placings, and the memory space used\ndf.info()","a513e405":"# identify impossible values and outliers using boxplot\ndf.boxplot(rot = 0, boxprops = dict(color = 'blue'), return_type = 'axes', figsize = (30, 8))\nplt.title(\"Box Plot of Diabetes Data\") # title of plot\nplt.suptitle(\"\")\nplt.xlabel(\"Attribute\") # x axis label\nplt.ylabel(\"Measurements (cm)\") # y axis label\nplt.show()","fd1a4074":"# summary statistics of the attributes, including measures of central tendency and \n    # measures of dispersion\ndf.describe() ","953eb372":"# smooth impossible values by replacing the value with the mean value\ndf['Glucose'] = df['Glucose'].replace(0, df.Glucose.mean())\ndf['BloodPressure'] = df['BloodPressure'].replace(0, df.BloodPressure.mean())\ndf['SkinThickness'] = df['SkinThickness'].replace(0, df.SkinThickness.mean())\ndf['Insulin'] = df['Insulin'].replace(0, df.Insulin.mean())\ndf['BMI'] = df['BMI'].replace(0, df.BMI.mean())","15b48485":"# confirm smoothed impossible values\n# summary statistics of the attributes, including measures of central tendency and \n    # measures of dispersion\ndf.describe() ","2cd0fb5a":"# detect duplicated records\ndf[df.duplicated(subset = None, keep = False)]","d504a239":"# import searborn library for more variety of data visualisation using fewer syntax and interesting default themes\nimport seaborn as sns \n\n# visualise pairs plot or scatterplot matrix in relation to diabetes outcome\ng = sns.pairplot(df, hue = 'Outcome', palette = 'PuBu')\ng = g.map_upper(plt.scatter)\ng = g.map_lower(sns.kdeplot)","82a2d318":"# display the number of entries, the number and names of the column attributes, the data type and\n    # digit placings, and the memory space used\ndf.info()","29d2c899":"# list and count the target class label names and their frequency\nfrom collections import Counter\ncount = Counter(df['Outcome'])\ncount.items()","cb4af8bc":"# count of each target class label\nplt.figure(figsize = (5, 5))\nax = sns.countplot(df['Outcome'], palette = 'PuBu')\nax.set_xticklabels(ax.get_xticklabels(), rotation = 0, ha = \"right\")\nplt.suptitle(\"Count of Diabetes Outcome\")\nplt.show()","2dbd5fd6":"# compare linear relationships between attributes using correlation coefficient generated using\n    # correlation matrix\nsns.heatmap(df.corr(), cmap = 'PuBu', annot = True)\nplt.show()","3afecd05":"# summarise main characteristics by displaying the summary statistics of the attributes, including \n    # measures of central tendency, and measures of dispersion\ndf.describe() ","b94fb908":"# classify and model the data using k-Nearest Neighbour (KNN), Decision Tree (DT), and Naive Bayes (NB)\n    # machine learning algorithms\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nimport math\n\ndf['Outcome'] = df.Outcome.astype(str)\ndf['Outcome'] = df.Outcome.astype(object)\n\n# split dataset into attributes and labels\nX = df.iloc[:, :-1].values # the attributes\ny = df.iloc[:, 8].values # the labels\n\n# choose appropriate range of training set proportions\nt = [0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2]\n\n# plot decision tree based on information gain\nDT = DecisionTreeClassifier(splitter = 'best', criterion = 'entropy', min_samples_leaf = 2)\n\n# use Gaussian method to support continuous data values\nNB = GaussianNB()\n\n# choose recommended optimal number of clusters of sqrt(number of records)\nKNN = KNeighborsClassifier(n_neighbors = math.ceil(math.sqrt(768)))\n\n# find best training set proportion for the chosen models\nplt.figure()\nfor s in t:\n    scores = []\n    for i in range(1,1000):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1-s, random_state = 987)\n        DT.fit(X_train, y_train) # consider DT scores\n        scores.append(DT.score(X_test, y_test))\n        NB.fit(X_train, y_train) # consider NB scores\n        scores.append(NB.score(X_test, y_test))\n        KNN.fit(X_train, y_train) # consider KNN scores\n        scores.append(KNN.score(X_test, y_test))\n    plt.plot(s, np.mean(scores), 'bo')\nplt.xlabel('Training Set Proportion') # x axis label\nplt.ylabel('Accuracy'); # y axis label","7111d29b":"# choose train test splits from original dataset as 80% train data and 20% test data for highest accuracy\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=987)\n\n# find optimal k number of clusters\nk_range = range(1, 25)\nscores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors = k)\n    knn.fit(X_train, y_train)\n    scores.append(knn.score(X_test, y_test))\nplt.figure()\nplt.xlabel('k') # x axis label\nplt.ylabel('Accuracy') # y axis label\nplt.scatter(k_range, scores) # scatter plot\nplt.xticks([0, 5, 10, 15, 20, 25]);","247eb32d":"# number of records in training set\nlen(X_train)","65e09306":"# count each outcome in training set\ncount = Counter(y_train)\nprint(count.items())","e114cbce":"# using k-Nearest Neighbour (KNN) classifier\n# choose 7 as the optimal number of clusters\nclassifierKNN = KNeighborsClassifier(n_neighbors = 15)\nclassifierKNN.fit(X_train, y_train)\n\n# using Euclidean distance metric\nclassifierKNN.effective_metric_","a3962c35":"# using Naive Bayes (NB) classifier\nclassifierNB = GaussianNB()\nclassifierNB.fit(X_train, y_train)\n\n# show prior probability of each class\nclassifierNB.class_prior_","b45f6eb0":"# using Decision Tree (DT) classifier\nclassifierDT = DecisionTreeClassifier(splitter = 'best', criterion='entropy', min_samples_leaf = 2)\nclassifierDT.fit(X_train, y_train)\n\n# plot decison tree\nfrom sklearn import tree\nfig = plt.figure(figsize = (55, 20))\nfn = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\nDT = tree.plot_tree(classifierDT,\n                    feature_names = fn,  \n                    class_names = y,\n                    filled = True)\n# outputs all extracted rules","e81bd64e":"# identifies the important features\nclassifierDT.feature_importances_","4bc94af7":"# number of records in test set\nlen(X_test)","aa5c67be":"# count each outcome in test set\ncount = Counter(y_test)\nprint(count.items())","1cfc3b5e":"# use the chosen three models to make predictions on test data\ny_predKNN = classifierKNN.predict(X_test)\ny_predDT = classifierDT.predict(X_test)\ny_predNB = classifierNB.predict(X_test)","4d331fd8":"# for k-Nearest Neighbours model\n# using confusion matrix\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test, y_predKNN))\nprint(classification_report(y_test, y_predKNN))\n\n# using accuracy performance metric\nfrom sklearn.metrics import accuracy_score\nprint(\"Train Accuracy: \", accuracy_score(y_train, classifierKNN.predict(X_train)))\nprint(\"Test Accuracy: \", accuracy_score(y_test, y_predKNN))","b3a5a1e8":"# for Naive Bayes model\n# using confusion matrix\nprint(confusion_matrix(y_test, y_predNB))\nprint(classification_report(y_test, y_predNB))\n\n# using accuracy performance metric\nprint(\"Train Accuracy: \", accuracy_score(y_train, classifierNB.predict(X_train)))\nprint(\"Test Accuracy: \", accuracy_score(y_test, y_predNB))","7844d2dd":"# for Decision Tree model\n# using confusion matrix\nprint(confusion_matrix(y_test, y_predDT))\nprint(classification_report(y_test, y_predDT))\n\n# using accuracy performance metric\nprint(\"Train Accuracy: \", accuracy_score(y_train, classifierDT.predict(X_train)))\nprint(\"Test Accuracy: \", accuracy_score(y_test, y_predDT))","da0646e5":"# data to plot\nn_groups = 3\nalgorithms = ('k-Nearest Neighbour (KNN)', 'Decision Tree (DT)', 'Naive Bayes (NB)')\ntrain_accuracy = (accuracy_score(y_train, classifierKNN.predict(X_train))*100, \n                  accuracy_score(y_train, classifierDT.predict(X_train))*100, \n                  accuracy_score(y_train, classifierNB.predict(X_train))*100)\ntest_accuracy = (accuracy_score(y_test, y_predKNN)*100, \n                 accuracy_score(y_test, y_predDT)*100, \n                 accuracy_score(y_test, y_predNB)*100)\n\n# create plot\nfig, ax = plt.subplots(figsize=(15, 5))\nindex = np.arange(n_groups)\nbar_width = 0.3\nopacity = 0.8\nrects1 = plt.bar(index, train_accuracy, bar_width, alpha = opacity, color='Cornflowerblue', label='Train')\nrects2 = plt.bar(index + bar_width, test_accuracy, bar_width, alpha = opacity, color='Teal', label='Test')\nplt.xlabel('Algorithm') # x axis label\nplt.ylabel('Accuracy (%)') # y axis label\nplt.ylim(0, 115)\nplt.title('Comparison of Algorithm Accuracies') # plot title\nplt.xticks(index + bar_width * 0.5, algorithms) # x axis data labels\nplt.legend(loc = 'upper right') # show legend\nfor index, data in enumerate(train_accuracy):\n    plt.text(x = index - 0.035, y = data + 1, s = round(data, 2), fontdict = dict(fontsize = 8))\nfor index, data in enumerate(test_accuracy):\n    plt.text(x = index + 0.25, y = data + 1, s = round(data, 2), fontdict = dict(fontsize = 8))\nplt.show()","6bc7f932":"df.describe() ","69ef43d8":"# new data\nnewdata = [[1, 50, 80, 33, 70, 30, 0.55, 20]]\n\n# compute probabilities of assigning to each of the two classes of outcome\nprobaNB = classifierNB.predict_proba(newdata)\nprobaNB.round(4) # round probabilities to four decimal places, if applicable","45f5f6a1":"# make prediction of class label\npredNB = classifierNB.predict(newdata)\npredNB","f7f34e07":"Through this, it is found that there are no other noises of impossible values.\n\nThe Winsorisation method is popularly chosen to handle numerical outliers, where outlier values are replaced with the minimum or maximum non-outlier value identified using the interquartile range (IQR) method. The acceptable value range to not be considered an outlier is [Q1-1.5IQR, Q3+1.5IQR], where Q1 is the first quartile of 25 percentile, Q3 is the third quartile of 75 percentile, and IQR is (Q3 \u2013 Q1).\n\nIn addition, impossible and extreme numerical values can be assumed as incorrect data entries, where they are identified as differing from the mean attribute value by a comparatively large margin.\n\nIn this case, no values were considered as outliers or impossible and extreme values, since all numerical values are reasonable and within an expected range in relation to the Diabetes Prediction's absence or present case study.\n\nDuplicated rows or records will not be dropped from the dataset in this case. There is no certain redundancy which causes inaccurate results and outcomes, since the dataset has no unique identfier that denotes separate entities. Despite this, the dataset will still be checked for duplicated rows.","8a6afc26":"The model performance is evaluated and validated by using the test set of 154 records to predict the classifications of these new unbiased data that were not used to train the model. The confusion matrix is then used to determine the performance metrics of accuracy, precision, recall, and F1-score, based on those classifications. The supports are 92 instances for the target class label of 0 and 62 for 1. This process ensures that the models are useful by being generalisable even when the specific training data used is extended to include new test data, or in technical terms \u2018avoid over fitting\u2019.","6f174dc0":"# Exploratory Data Analysis (EDA)\n\nEDA aims to perform initial investigations on data before formal modeling and graphical representations and visualisations, in order to discover patterns, look over assumptions, and test hypothesis. The summarised information on main characteristics and hidden trends in data can help the doctor to identify concern areas and problems, and the resolution of these can boost their accuracy in diagosing diabetes.\n\nTaking a closer look at the target class labels, as well as their frequency of occurences :","d9b7969d":"# Supervised Machine Learning on Diabetes Prediction","432bff38":"The class prior indicates the probability of a observation belonging to a specific class if no information was given. The probability of prediction is 66.45% as 0 and only 33.55% as 1.\n\nDecision tree is constructed based on parameters of best split strategy, and the criterion of entropy which utilises information gain to iteratively select the next node according to higher feature importance to optimise the quality of splits. The minimum number of leaves are restricted to 2. The outputs are the classification rules as extracted from the decision tree. These are determined by the flow sequence from the root node and the corresponding branches to the internal or decision nodes, then stopping when the leaf node representing the class label is reached.","3848d0be":"The dataset contains 768 rows of records and 9 columns of attributes. The data types of the attributes consist of 1 quantitative discrete binary, 6 quantitative discrete numerical integers, and 2 quantitative continuous numerical float with 64 digit placings.\n\nThe memory space usage is at least 54.1 kilobytes (KB).","00d0a827":"Gaussian Na\u00efve Bayes is suitable for continuous data types, and the prior probabilities and likelihoods are computed in order to predict the posterior probability of a data point belonging to each of the three classes. The outputs are the aforementioned posterior probabilities, and the assigned class membership is selected as the class with the highest posterior probability.","8699f826":"# Neo Ann Yi","e4a71673":"'Pregnancies' is the number of pregnancies to date, with a realistic range of 0 to 17.\n\n'Glocose' is is the plasma glucose concentration over 2 hours in an oral glucose tolerance test, measured in milligrams per decilitre (mg\/dL). A blood sugar level less than 140 mg\/dL is normal. A reading of more than 200 mg\/dL indicates diabetes, whereas that between 140 and 199 mg\/dL (indicates prediabetes. The data ranges from 0 to 199, which indicates that the impossible value of 0 should be smoothed.\n\n'BloodPressure' is the diastolic blood pressure, measured in millimeters of mercury (mm Hg). A normal blood pressure would have a reading of less than 80 mmHg, and this may vary from 90 to 120 mmHg for a healthy young person. A reading of more than 140 mmHg indicates high blood pressure. The data ranges from 0 to 122, which indicates that the impossible value of 0 should be smoothed.\n\n'SkinThickness' is the triceps skin fold thickness, measured in millimeters (mm). The data ranges from 0 to 99, which indicates that the impossible value of 0 should be smoothed.\n\n'Insulin' is the 2-hour serum insulin, measured in micrometre units per millilitre (mu U\/ml). The data ranges from 0 to 846, which indicates that the impossible value of 0 should be smoothed.\n\n'BMI' is the body mass index (BMI) for weight in kg and height in m (kg\/m^2). The data ranges from 0 to 67, which indicates that the impossible value of 0 should be smoothed. BMI should not be close to zero unless the person is grossly underweight which could be life-threatening. \n\n'DiabetesPedigreeFunction' is a function that scores likelihood of diabetes based on family history, with a realistic range of 0.08 to 2.42. \n\n'Age' in years has a realistic range of 21 to 81.\n\n'Outcome' is the target class label, where 0 represents absence and 1 represents presence of diabetes.\n\nAll impossible values will be smoothed by replacing them with the mean value.","36d29e43":"# Data Modelling\n\nThe dataset is split into two separate sets - the training set and test set. They both consist of the same attributes, but not the same attribute values. The training set is used to train and construct the classification models. The test set is used to predict the classifications of the new unbiased data that were not used to train the model, before evaluating the model performance based on the performance metrics of accuracy, precision, recall, and F1-score of those classifications.\n\nThe target labels have uneven distribution. In order to ensure that the training and test sets are unbiased and representative of the two classes, the list of random numbers starting from the random selected position of 987 is used to perform random splitting. An accuracy graph is plotted to find the most accurate training set proportion, after taking the chosen Decision Tree (DT), Naive Bayes (NB), and KNN model scores into consideration. ","419c0eff":"The dataframe format type will facilitate the use of a wider variety of syntax and methods for data analysis, including describe() and info().\n\nRegarding the attributes included in Diabetes Prediction dataset, there are 8 different diabetes predictors, recorded as 'Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', and 'Age'. The target class label is 'outcome', where 0 represents absence and 1 represents presence. Since the data is found to be labeled, supervised machine learning methods will be used to model the data later on.","8bc44227":"Almost all predictors have weak linear correlations, which is indicative that most are more likely to have non-linear relationships. Notably, when Glucose and BMI increases by 1 unit each, the positive Outcome of diabetes increases by 0.49 units and 0.31 units respectively. The population is thus advised to reduce their glucose intake and BMI readings in order to minimise the chances of diabetes.\n\nThus far, analysis is mostly focused on the relationship between the various diabetes features and the target feature which is diabetes outcome. This is because the classification purpose will be mostly interested in these types of correlation and their strengths, in order for accurate predictions. \n\nmatplotlib.pyplot graphics library is imported for the visualisations of figures. It is convenient as it has good reproducibility of scientific figures, for example when regenerating a figure using updated data, appearance, latex labels and texts, and aspects such as orientation. %matplotlib inline is used to configure the output of the figures to be embedded in the Jupyter notebook file, instead of opening a new window each time.\n\nAll data visualisation include suitable graphs that are descriptive and comparative to effectively communicate both abstract and concrete ideas. \n\nPreviously, pairs plot or scatterplot matrix was plotted. Histogram density plots are chosen to illustrate the overall data distribution, as well as the data distributions of diabetes outcome based on the predictor features. Kernel density estimate (KDE) plots will visualise the overall distribution through a continuous probability density curve. Two histogram density plots and their continuous probability density curves are generated for the two target feature values in the same figure space, and clearly differentiates them by specifying different RGB hex colour codes in its parameters. The plot range is limited to 0 and above for meaningful visualisations, since it is impossible for the predictor features to have a negative value.\n\nAll the overall KDE distribution curves have positive leptokurtic kurtosis and right or positive skewness, since the data instances available for the 0 class is approximately twice that of 1. ALl attributes except for 'BloodPressure' have bimodal or multimodel data distribution as there are two distinct peaks also known as local maxima. 'BloodPressure' have approximate normal data distribution as the curves are approximately symmetric, unimodal, asymptotic, and their mean, median, and mode are similar.\n\nThe histogram density plots and their respective highest point in the curves show the patterns that diabetes patients generally have higher numbers of Pregnancies, higher Glucose and BMI readings, and older in Age. \n\nThe difference in the overlapping or overplotting histogram density plots clearly shows that the target class of strongly correlated features can be predicted more easily, and more useful meaning can be extracted. Its data points are less scattered and thus have less overlapping or overplotting areas, which means that they better follow their respective common relationship or pattern. It can thus be seen that the ranking of correlation to diabetes outcome, in decreasing order, is 'Glucose', 'Age', 'Pregnancies', 'BMI', 'DiabetesPedigreeFunction', 'BloodPressure', 'SkinThickness', and 'Insulin'. 'Glucose' is thus the best predictor of diabetes outcome in this case.\n\nLastly, the summary statistics will be considered.","cb049bf5":"# Model Interpretation\n\nThe final crucial step of a data science project is the interpretation of the models and data, in terms of its predictive power and thus its ability to generalise unseen future data.\n\nThe NB model is now ready to be deployed to predict new value instances. To do so, a data frame is created to describe the characteristics of a number of diabetes factors. The maximum and minimum values of each feature is considered before creating the data frame, in order to ensure that all values are reliable and acceptable.","224a799f":"Feature importance will calculate the decrease in node impurity weighted by the probability of reaching that node. The node probability is the number of samples that reach the node, divided by the total number of samples. Higher values indicate higher feature importance in the DT prediction model.\n\nData Modelling is now complete.","4538d297":"There are no duplications in the dataset.\n\nData integration is not needed, since only one dataset is used with no schema integrations, and thus no discernable entity identification issues or data value conflicts.\n\nData transformation will check overall range of values for the entire dataset. All values should fall under an acceptable small range to allow easy visualisations and modelling. It is found that all values already fall under an acceptable range of [0, 846], so there is no need for data transformation to scale the values into a comparable range for easy visualisations and modelling.\n\nData reduction may involve dropping redundant attributes through attribute dimensionality reduction. However, there are no related cases detected thus far.\n\nA correlation heatmap is used to list all the correlation coefficients in order to identify multicollinearity, in other words high intercorrelation above an absolute value of 0.5 between the a pair of attributes. For a pair of attributes with multicollinearity, one of them will be dropped since it would be redudant to include both of them with almost mirroring values and thus almost perfect descriptions of each other. Another reason is to prevent overfitting.\n\nThe correlation will compare and describe the linear connection and relationship between pairs of features, through the type of correlation and its strength. A positive correlation indicates that both features will change their values in the same direction, while a negative correlation indicates that both will change in opposite directions. The larger the correlation strength, the stronger the connection and relationship.\n\nHowever, Decision Tree, k-Nearest Neighbours, and Naive Bayes models are chosen as most appropriate classification models, and they are all immune to multicollinearity. The first two are non-parametric models - Decision Tree only examines one of the features at a time during the splitting process, while KNN examines features all together. Na\u00efve Bayes assumes all features are conditionally independent. Due to these reasons, the only predictors that are considered to be dropped will be if their intercorrelations are above 0.95 and thus almost perfect descriptions of each other. It would be redundant to include both of them. Therefore, no attributes were removed as to not lose relevant information and degrade the overall supervised machine learning prediction model.\n\nPairs plot or scatterplot matrix are used to identify and remove attributes with weak class-attribute relationship. This is of most use and interest for classification purposes. Scatter plots on the upper triangle will visualize the relationships between two variables. Kernel density estimate (KDE) plots, which will be discussed in further detail in the next section, will illustrate the univariate distribution of a single variable in relation to the target variable. 2-D kernel density plots on the lower triangle will illustrate the density of single variable in relation to the target variable. ","92b073b5":"The predicted class for the specified example is assigned as 0, as its probability of 98.92% is almost equal to a 100% certainty and very much higher that that of class 0 of diabetes outcome. It is also safe to intepret this result as having 74.68% accuracy, 74% precision and F1-score, and 75% recall, based on the NB model's performance metrics.\n\nPossible improvements can be to include other strong predictors of diabetes outcome outside of the aforementioned 8, such as weight and skin dryness. The presence of these symptoms can be very relevant and thus useful to include in the model design.","b60c1b8f":"describe() is used to obtain summary statistics including measures of central tendency such as mean and median, and measures of dispersion such as standard deviation, which are useful in providing a quick and simple description of the dataset and its characteristics. ","30fc80f3":"The training subset takes up 614 instances, whereas the test subset takes up the remaining 154 instances.","9fc2f958":"According to the graph, the training subset should take up 80% of the dataset which is 119 instances, whereas the test subset will take up 20% which is 30 instances. The machine Learning models of DT, NB, and KNN are now fitted to the training dataset.\n\nFor the KNN model, the optimal value of k number of nearest neighbours is found by plotting an accuracy graph.","55297c59":"# Model Evaluation","a830c131":"The maximum and minimum values are checked to ensure that all noises of impossible values have been smoothed.","b896c9d9":"The target class label will also have uneven distribution, where 0 has 408 instances, and 1 has 206.\n\nThe accuracy graph identifies the optimal value of k as 15 in order to obtain slight above 70% accuracy, the highest possible for this dataset. This is done to obtain a k value that is large enough to minimise error rate and sensitivity to noise, but not too large such that the boundaries are over-smoothed or overfitted with points from the other classes. The chosen k value is also appropriate since 15 is not a multiple of the 2 classes, which is a requirement when selecting k value. The KNN model parameters are thus the value of k of 15, and Euclidean distance metric to compute the distance between data points. The output is the assigned class membership based on the majority vote on the data point\u2019s k number of neighbors.","4e5b0b2b":"The dataset contains 768 rows of records and 9 columns of attributes. The data types of the attributes consist of 1 quantitative discrete binary, 6 quantitative discrete numerical integers, and 2 quantitative continuous numerical float with 64 digit placings.\n\nThe memory space usage is at least 54.1 kilobytes (KB).\n\nThrough this, it is found that there are no null values present in the dataset.\n\nNext, noises of impossible values are checked by analysing the maximum and minimum values using box plots and summary statistics.","f1ac8cdf":"There are two classes of diabetes outcome with quantitative discrete binary data values, where 1 has 268 instances, and 0 has 500. This clearly illustrates that data available for the target class of 1 is approximately half the proportion than that for 0, which will be taken note of for further data visualisations and analysis later on. \n\nMoving on to analyse the quantitative attributes of diabetes predictors, their linear relationships and their strengths can be compared using a correlation heatmap.","567648fa":"Task Highlights :\n\n> Perform Supervised Machine Learning on Diabetes Prediction dataset (https:\/\/www.kaggle.com\/rahulsah06\/machine-learning-for-diabetes-with-python as at April 1, 2021)\n\n> Perform data analysis\n\n> Make use of different algorithms to predict target label\n\n> Show 3 different algorithms' accuracies with the help of graphs\n\nThe main problems to be solved by this data science task have been properly framed, in terms of client's goals, background information, and purpose of task. This ensures that the task is understood and explored to better inform the decision-making process on the possible range of approaches and solutions to the problems.\n\nThis task will extract relevant, representative, and sufficient case study data from a reputable and reliable online source. Appropriate preprocessing adjustments and data exploration will be performed on the data to ensure reliable and reasonable outcomes and outputs. For the data mining and modelling process, the popular classifier models of Decision Tree, Random Forest, and Multilayer Perceptron Neural Network will be fitted, analysed, and evaluated in terms of the performance metrics of accuracy, precision, recall, and F1-score in predicting the classifications of diabetes. All significant interpretations and observations will be noted and considered for future improvements. \n\nFollowing the purpose of this task, the primary focus will be on diabetes-related factors, which are a range of health measurements including 'Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', and 'Age' as predictors. Analysing these will help to identify concern areas and assess and predict the diabetes outcome.\n\n#### Note that throughout this task, the important points are differentiated using the Indented Quotes format.","4fe28ae8":"# Data Preprocessing\n\nData preprocessing is a data mining technique that transforms raw data into an understandable format. This process has four main stages \u2013 data cleaning, data integration, data transformation, and data reduction.\n\nData cleaning will filter, detect, and handle dirty data to ensure quality data and quality analysis results. In this case, there may be noises of impossible and extreme values and outliers, and missing values. The errors may include inconsistent data and redundant attributes and data.\n\nAs the first step, null values within the dataset will be identified, and appropriately replaced if possible.","1593d71d":"# Data Collection\n\nThe first step of a data science task is to obtain, gather, and measure the necessary and targeted data from available internal or external data sources, and then compiled into an established system. In this case, version 1 of Diabetes Machine Learning and Prediction dataset by Rahul in Kaggle is used. The Excel csv file that was extracted as at 1 April 2021 for the purpose of this task is available at https:\/\/www.kaggle.com\/rahulsah06\/machine-learning-for-diabetes-with-python.","d25886e1":"A similar example is demonstrated below, using 'Pregnancies' of 1, 'Glucose' of 50 mg\/dL, 'BloodPressure' of 80 mm Hg, 'SkinThickness' of 33 mm, 'Insulin' of 70 mu U\/ml, 'BMI' of 30 kg\/m^2, 'DiabetesPedigreeFunction' of 0.55, and 'Age' of 20 years old. These new data instances will be passed to the NB model classifier to predict its class label of diabetes outcome.","8400f3ad":"NB model has the overall performance metrics of 74.68% accuracy, 74% precision and F1-score, and 75% recall. The KNN model achieved worse as 70.13% accuracy, 70% precision and recall, and 69% F1-score. On the other hand, the overall performance metrics of the DT model is the best, with 78.57% accuracy, 78% precision and F1-score, and 79% recall. For all three chosen models, the diabetes outcome that was best classified is 0. \n\nNext, the degree of overfitting must also be taken into consideration before the best model is chosen. Disregarding KNN due to its comparatively poor overall performance metrics, NB model only has a small difference between the train and test accuracy and thus no overfitting, and the 74.68% train accuracy indicates that the learnt rules are not specific for the train set and will generalize well beyond the train set to the test set. Due to this, NB is chosen as the final best model over DT, since DT overfits the data with a large difference between the train and test accuracy.\n\nAccuracy indicates the overall proportion of correct predictions for all the three classes. The train accuracy is measured based on examples that the model was constructed on, while the test accuracy is based on those it has yet to see. NB achieved 74.68% for both train accuracy and test accuracy.\n\nHowever, it will be misleading to solely base decisions on this, as the dataset used is relatively small and biased. Recall and precision metrics are thus also considered to measure model performance.\n\nRecall indicates the proportion of correct predictions for each individual class, out of the corresponding actual class. In other words, the proportion of all actual classes that were predicted correctly. It was found that 75% of all actual classes were predicted correctly by NB.\n\nPrecision indicates the proportion of correct predictions for each individual class, out of the corresponding predicted class. In other words, the proportion of all predicted classes were actually predicted correctly. It was found that 74% of all predicted classes were actually predicted correctly by NB.\n\nHowever, recall and precision have an inverse relationship. In order to make them comparable for cases where they are both important, F1 score is introduced. F1-score, also known as F-score or F-measure, is used to make precision and recall comparable in cases where they are both important, by measuring their harmonic mean. This allows it to consider both metrics and punish extreme values more heavily. F-score is more useful for biased datasets that are common in real-life scenarios, as well as in this case study. This occurs when the counts of, in this case FP of 20 and FN of 13, are very different, but they must still be properly considered since they are crucial conditions in the prediction. False negative (FN) and false positive (FP) are both the worst error in diabetes prediction, as the implications of a wrong classification for a FN for a patient misdiagnosed as not having diabetes is that the patient to lose out on immediate treatment and medications and may lead to death if the mistake is discovered too late. On the other hand, a FP for a patient misdiagnosed as having diabetes will cause the patient to be subjected to unneeded treatments and medications which may lead to other health problems. Therefore, F-score will compute the overall quality of translations produced by the chosen machine learning engine, which is 74% by the NB model.\n\nThe confusion matrix tabulates the predicted class vertically and the actual class horizontally.\n\nIn conclusion, the NB model is chosen as the final model for the prediction of diabetes outcome.","f7f9f680":"All of them were analysed, and it was found that all features have acceptably clear class-attribute relationship with relatively distinguishable class boundaries as well as acceptable degree of overlapping or overplotting areas. Therefore, no attributes were removed as they are all able to allow relatively accurate predictions for classification purposes.\n\nThe final dataset information is summarised below.","6ae76083":"# Thank you !"}}