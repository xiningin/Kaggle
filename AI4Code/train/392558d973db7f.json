{"cell_type":{"0d14f0cf":"code","ce3b2923":"code","4c885a26":"code","afe920e7":"code","ab467610":"code","ca4396aa":"code","e6163898":"code","e44a0ab7":"code","c5bc2f18":"code","b0c8c3d8":"code","611fa5ee":"code","c5767e2f":"code","d651335f":"code","b804b30f":"code","2e22210b":"code","fcfbd4bb":"code","9ea98b40":"code","6f9665e3":"code","ccec8b80":"code","2723a541":"code","3fc11339":"code","ea50e39d":"code","426c2d17":"markdown","cdd2e876":"markdown","99b708ee":"markdown","9765a1f3":"markdown","2b9c294c":"markdown","3551260a":"markdown","a2494e97":"markdown","38dae540":"markdown","4bf37851":"markdown"},"source":{"0d14f0cf":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set()\n\nimport lightgbm as lgb\nimport xgboost as xgb\n\n# sklearn tools for model training and assesment\nfrom sklearn.feature_selection import SelectFromModel,RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom sklearn.metrics import roc_curve, auc, accuracy_score,roc_auc_score\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.base import clone\n\nimport gc\nimport os\nprint(os.listdir(\"..\/input\"))\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ce3b2923":"train=pd.read_csv('..\/input\/train.csv')\ntest=pd.read_csv('..\/input\/test.csv')\ntrain.head()","4c885a26":"train.info()","afe920e7":"sns.countplot(train['target'])","ab467610":"# Checking missing values\nprint(train.isnull().values.any())\nprint(test.isnull().values.any())","ca4396aa":"# Features that have high correlations with the target\nfeatures=[]\ncor=[]\nfor feature in train.iloc[:,2:].columns:\n    if (train['target'].corr(train[feature])>0.05)|(train['target'].corr(train[feature])<-0.05):\n        features.append(feature)\n        cor.append(train['target'].corr(train[feature]))\n\ndf_corr=pd.DataFrame({'Features': features,'Correlations':cor}).sort_values(by='Correlations').set_index('Features')\n\ndf_corr.plot(kind='barh',figsize=(10,8))","e6163898":"# Feature with high skewness\nfeaturesSkew=[]\nskewness=[]\n\nfor feature in train.iloc[:,2:].columns:\n    if (train[feature].skew()>=0.5) | (train[feature].skew()<=-0.5) :\n        featuresSkew.append(feature)\n        skewness.append(train[feature].skew())\n\ndf_skew=pd.DataFrame({'Features':featuresSkew,'Skewness':skewness})\ndf_skew","e44a0ab7":"import featuretools as ft\nes = ft.EntitySet(id='Santander')\n\nes.entity_from_dataframe(dataframe=train[features],\n                         entity_id='train',\n                         make_index = True,\n                         index='index')\n\nfm, feat= ft.dfs(entityset=es, \n                 target_entity='train',\n                 trans_primitives=['multiply_numeric','add_numeric'],\n                 max_depth=1)","c5bc2f18":"train=pd.concat((train,fm.iloc[:,len(features):]),axis=1)\n# release some memory\ndel fm\ngc.collect()\nfm=pd.DataFrame()\ntrain.info()","b0c8c3d8":"es.entity_from_dataframe(dataframe=test[features],\n                         entity_id='test',\n                         make_index = True,\n                         index='index')\n\nfm_test, feat= ft.dfs(entityset=es, \n                 target_entity='test',\n                 trans_primitives=['multiply_numeric','add_numeric'],\n                 max_depth=1)","611fa5ee":"test=pd.concat((test,fm_test.iloc[:,len(features):]),axis=1)\n# release some memory\ndel fm_test\ngc.collect()\nfm_test=pd.DataFrame()\ntest.info()","c5767e2f":"# Cross validate model with Kfold stratified cross val\nrandom_state = 222\nkfold = StratifiedKFold(n_splits=12,shuffle=True,random_state=random_state)\npred_val = np.zeros(len(train))\npred_test_baseline = np.zeros(len(test))\nfeature_base=train.columns.tolist()[2:202]","d651335f":"# https:\/\/www.kaggle.com\/jesucristo\/90-lines-solution-0-901-fast\nparam = {\n        'bagging_freq': 5,\n        'bagging_fraction': 0.4,\n        'boost_from_average':'false',\n        'boost': 'gbdt',\n        'feature_fraction': 0.05,\n        'learning_rate': 0.01,\n        'max_depth': -1,  \n        'metric':'auc',\n        'min_data_in_leaf': 80,\n        'min_sum_hessian_in_leaf': 10.0,\n        'num_leaves': 13,\n        'num_threads': 8,\n        'tree_learner': 'serial',\n        'objective': 'binary', \n        'verbosity': 1\n    }","b804b30f":"# Split X and y\nX_train=train.iloc[:,2:]\ny_train=train['target']\nX_test=test.iloc[:,1:]","2e22210b":"# Baseline model: LightGBM with no feature engineering and tunning.\nfor foldIdx, (trn_idx, val_idx) in enumerate(kfold.split(X_train.loc[:,feature_base], y_train)):\n    print(\"Fold {}\".format(foldIdx))\n    lgbm_base=lgb.LGBMClassifier(n_estimators=100000,random_state=random_state,**param)\n    lgbm_base.fit(X_train.iloc[trn_idx][feature_base],y_train[trn_idx],\n                  eval_set=[(X_train.iloc[trn_idx][feature_base],y_train[trn_idx]),(X_train.iloc[val_idx][feature_base],y_train[val_idx])],\n                  early_stopping_rounds = 3000,\n                  verbose=5000)\n    pred_val[val_idx] = lgbm_base.predict_proba(X_train.loc[val_idx,feature_base], num_iteration=lgbm_base.best_iteration_)[:,1]\n    pred_test_baseline += lgbm_base.predict_proba(X_test[feature_base],num_iteration=lgbm_base.best_iteration_)[:,1] \/ kfold.n_splits","fcfbd4bb":"# Evaluation\nprint('AUC score: %.5f' % roc_auc_score(train['target'],pred_val))","9ea98b40":"# Feature selection: Drop features that are highly correlated\ncorr_matrix=X_train.corr().abs()\nupper=corr_matrix.where(np.triu(np.ones(corr_matrix.shape),k=1).astype(np.bool))","6f9665e3":"# Find index of feature columns with correlation greater than 0.8\nto_drop = [column for column in upper.columns if any(upper[column] > 0.7)]","ccec8b80":"features_selected=[fea for fea in X_train.columns if fea not in to_drop]","2723a541":"# Fit the lightGBM with embeded_lgb_feature\npred_val = np.zeros(len(train))\npred_test = np.zeros(len(test))\nfor foldIdx, (trn_idx, val_idx) in enumerate(kfold.split(X_train.loc[:,features_selected], y_train)):\n    print(\"Fold {}\".format(foldIdx))\n    lgbm=lgb.LGBMClassifier(n_estimators=100000,random_state=random_state,**param)\n    lgbm.fit(X_train.iloc[trn_idx][features_selected],y_train[trn_idx],\n                  eval_set=[(X_train.iloc[trn_idx][features_selected],y_train[trn_idx]),(X_train.iloc[val_idx][features_selected],y_train[val_idx])],\n                  early_stopping_rounds = 3000,\n                  verbose=5000)\n    pred_val[val_idx] = lgbm.predict_proba(X_train.loc[val_idx,features_selected], num_iteration=lgbm.best_iteration_)[:,1]\n    pred_test += lgbm.predict_proba(X_test[features_selected],num_iteration=lgbm.best_iteration_)[:,1] \/ kfold.n_splits","3fc11339":"# Evaluation\nprint('AUC score: %.5f' % roc_auc_score(y_train,pred_val))","ea50e39d":"# Submission\nsubmission_baseline = pd.DataFrame({'ID_code': test.ID_code.values,\n                           'target':pred_test_baseline})\nsubmission_baseline.to_csv(\"LGBM_baseline.csv\", index=False)\nsubmission = pd.DataFrame({'ID_code': test.ID_code.values,\n                           'target':pred_test})\nsubmission.to_csv(\"LGBM_V3.csv\", index=False)","426c2d17":"## The Target","cdd2e876":"# Reference\n\n[What is the acceptable range of skewness and kurtosis for normal distribution of data?](https:\/\/codeburst.io\/2-important-statistics-terms-you-need-to-know-in-data-science-skewness-and-kurtosis-388fef94eeaa)\n\n[Auto feature engineering with feature tool](https:\/\/docs.featuretools.com\/loading_data\/using_entitysets.html)\n\n[Auto feature engineering Kaggle case](https:\/\/www.kaggle.com\/willkoehrsen\/featuretools-for-good)\n\n[How to choose metrics for imbalance dataset](https:\/\/towardsdatascience.com\/what-metrics-should-we-use-on-imbalanced-data-set-precision-recall-roc-e2e79252aeba)\n\n[Santander Magic LGB](https:\/\/www.kaggle.com\/jesucristo\/santander-magic-lgb)\n\n[6 Ways for Feature Selection](https:\/\/www.kaggle.com\/sz8416\/6-ways-for-feature-selection)\n","99b708ee":"# Feature Engineering","9765a1f3":"# Load Data & Overview","2b9c294c":"There is no transformation needed.","3551260a":"# EDA","a2494e97":"The target is imbalance, and we will use AUC as the metric according to the requirement.  I tried SMOTE over sampling before but didn't help.","38dae540":"The AUC score is not better with more features and the model tended to overfit. I will explore more ideas on 1)feature engineering, 2)feature selection and 3)ensemble modeling. ","4bf37851":"# Prediction"}}