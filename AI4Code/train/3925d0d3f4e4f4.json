{"cell_type":{"d5aedb6f":"code","fbc3de8a":"code","b88a7d77":"code","eedfa344":"code","99ebcf61":"code","bb706f91":"code","f3b66378":"code","b21dc794":"code","c764181e":"code","c3b41702":"code","5470578e":"code","4456f4f3":"code","bf73f0d0":"code","9219ff50":"code","4a76d8f8":"code","c2741826":"code","058d947a":"code","746ebc83":"code","2b467ab3":"code","8f2e47b6":"code","ebe995a5":"code","f128a2ee":"code","7bf67606":"code","70cedf4c":"code","249ac4c3":"code","a0ad846d":"code","72e32766":"code","9d2e31b7":"code","3914e795":"code","a96bf324":"code","754c5e22":"code","9f8977bf":"code","fb64367b":"code","cf1d122f":"code","3222bf77":"code","421897a0":"code","504ccbdb":"code","fa5010c6":"code","806fb2e2":"markdown","6ec0e180":"markdown","308d1538":"markdown","d028b887":"markdown","03574b9c":"markdown","32c211d6":"markdown","47f73d05":"markdown","d27ce6f3":"markdown","719c2e57":"markdown","6643a74c":"markdown","70fad7d0":"markdown","e8e04c2b":"markdown","37e2cbda":"markdown","7dbfbe9a":"markdown","127f7518":"markdown","3e855ac8":"markdown","399342bd":"markdown","cf557caa":"markdown","b715a0e2":"markdown","541e0edf":"markdown","61d0829d":"markdown","a3731ad1":"markdown","d7600a74":"markdown","6b1806d6":"markdown"},"source":{"d5aedb6f":"!pip freeze > kaggle_image_requirements.txt","fbc3de8a":"!wget https:\/\/www.openml.org\/data\/get_csv\/3622\/dataset_189_baseball.arff # simple OpenML dataset","b88a7d77":"import pandas as pd\nraw_baseball_data = pd.read_csv('dataset_189_baseball.arff', dtype=str) # .arff format is apprently mostly equivalent for .csv for our purposes (look up Weka if really curious)\n\n# expand default pandas display options to make things more clearly visible when printed\npd.set_option('display.max_colwidth', 300)\n\nprint(raw_baseball_data.head())","eedfa344":"!ls ..\/input\/20022018-bc-public-libraries-open-data-v182 # see BC public libdary data 2002-2018 path","99ebcf61":"raw_data = pd.read_csv('..\/input\/20022018-bc-public-libraries-open-data-v182\/2002-2018-bc-public-libraries-open-data-csv-v18.2.csv', dtype=str)\nprint(raw_data.head())","bb706f91":"COLUMNS = [\"PCT_ELEC_IN_TOT_VOLS\",\"TOT_AV_VOLS\"] # lots of columns in this data set, let's just focus on these two\nraw_library_data = raw_data[COLUMNS]\nprint(raw_library_data)","f3b66378":"!pip install git+https:\/\/github.com\/algorine\/simon","b21dc794":"from Simon import Simon # SIMOn model class \nfrom Simon.Encoder import Encoder # SIMOn data encoder class","c764181e":"!wget https:\/\/raw.githubusercontent.com\/algorine\/simon\/master\/Simon\/scripts\/pretrained_models\/Base.pkl # pretrained SIMOn model configuration - Encoder, etc\n!wget https:\/\/raw.githubusercontent.com\/algorine\/simon\/master\/Simon\/scripts\/pretrained_models\/text-class.17-0.04.hdf5 # corresponding model weights\n!ls","c3b41702":"checkpoint_dir = \"\" # model weight are at the current level\nexecution_config = \"Base.pkl\" # name of our pretrained model configuration that was downloaded\n\nClassifier = Simon(encoder={}) # create text classifier instance for loading encoder from model configs\nconfig = Classifier.load_config(execution_config, checkpoint_dir) # load model config\nencoder = config['encoder'] # get encoder\ncheckpoint = config['checkpoint'] # get checkpoint name\n\nprint(checkpoint) # the name of the checkpoint is stored in config as well, here we double-check it to make sure we got the right one","5470578e":"max_len = 20 # maximum length of each tabular cell\nmax_cells = encoder.cur_max_cells # 500, maximum number of cells in a column\nprint(max_cells)","4456f4f3":"Categories = encoder.categories\ncategory_count = len(Categories) # number of handled categories\nprint(encoder.categories)","bf73f0d0":"X_baseball = encoder.encodeDataFrame(raw_baseball_data) # encode data (standardization, transposition, conversion to Numpy array)\n\nprint(X_baseball[0].shape)\nprint(X_baseball.shape) # display shape of encoded data\nprint(X_baseball[0]) # display encoded first column","9219ff50":"X_library = encoder.encodeDataFrame(raw_library_data) # encode data (standardization, transposition, conversion to Numpy array)\n\nprint(X_library[0])\nprint(X_library[0].shape)\nprint(X_library.shape)","4a76d8f8":"from Simon.DataGenerator import DataGenerator # Simulated\/Fake data generation utility (using the library Faker)\n\n# define appropriate parameters for the simulated data\ndata_cols = 5 # number of columns to generate\ndata_count = 10 # number if cells\/rows per column\n\ntry_reuse_data = False # don't reuse data\nsimulated_data, header = DataGenerator.gen_test_data((data_count, data_cols), try_reuse_data)\n\nprint(\"SIMULATED DATA:\") # display results\nprint(simulated_data)\nprint(\"SIMULATED DATA HEADER:\")\nprint(header)","c2741826":"model = Classifier.generate_model(max_len, max_cells, category_count) # generate model\n\nClassifier.load_weights(checkpoint, None, model, checkpoint_dir) # load weights\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy']) # compile model","058d947a":"p_threshold = 0.5 # probability threshold for deciding membership of class\n\nmodel.summary() # display model architecture","746ebc83":"# alternative visualizationm method\nfrom keras.utils.vis_utils import plot_model\nplot_model(model,to_file='model.png') # visualize model architecture (for display here)\nplot_model(model,to_file='model.svg') # higher resolution version for later\n\nfrom IPython.display import Image # display image in notebook\nImage(retina=True, filename='model.png') ","2b467ab3":"!ls","8f2e47b6":"y = model.predict(X_baseball) # predict classes\nresult = encoder.reverse_label_encode(y,p_threshold) # reverse encode labels\nprint(\"Recall that the column headers were:\") # display the output\nprint(list(raw_baseball_data))\nprint(\"The predicted classes and probabilities are respectively:\")\nprint(result)","ebe995a5":"X = encoder.encodeDataFrame(raw_library_data) # encode data using original frame\ny = model.predict(X) # predict classes\n\nresult = encoder.reverse_label_encode(y,p_threshold) # reverse encode labels\n\nprint(\"Recall that the column headers were:\")\nprint(list(raw_library_data))\nprint(\"The predicted class\/probability:\")\nprint(result)","f128a2ee":"# let's recall what the raw library data columns look like\nprint(raw_library_data)\nprint(raw_library_data.shape)","7bf67606":"# turn into two lists\npercent_value_list = raw_library_data['PCT_ELEC_IN_TOT_VOLS'].values.tolist()\nint_value_list = raw_library_data['TOT_AV_VOLS'].values.tolist()","70cedf4c":"# Break it up into individual sample columns of size 20 cells each\noriginal_length = raw_data.shape[0] # original length, 1207\nchunk_size = 20 # length of each newly generated column\nheader_list = list(range(2*original_length\/\/chunk_size)) # list of indices of new columns\nnew_raw_data = pd.DataFrame(columns = header_list) # initialize new DataFrame to hold new data\nfor i in range(original_length\/\/chunk_size): # populate new DataFrame\n    new_raw_data[i] = percent_value_list[i:i+chunk_size] # percent\n    new_raw_data[original_length\/\/chunk_size+i] = int_value_list[i:i+chunk_size] # integer\nprint(new_raw_data.head())","249ac4c3":"# let's create a corresponding header for our training data\nheader = [(\"percent\",),]*(original_length\/\/chunk_size)\nheader.extend([(\"int\",),]*(original_length\/\/chunk_size))\nprint(header)","a0ad846d":"# new categories\nprint(encoder.categories)","72e32766":"import numpy as np\n\n# but first grab last layer weights for initialization\nprint(\"WEIGHTS::\")\nold_weights = model.layers[8].get_weights()\nprint(old_weights[1]) # biases\nprint(\"Layer Name::\")\nprint(model.layers[8].name) # name of last layer\nprint(\"SHAPE::\")\nprint(model.layers[8].get_weights()[0].shape) # shape of weight matrix in last layer","9d2e31b7":"# find old weight index for closest category - text\nold_category_index = encoder.categories.index('text')\n\n# update the encoder with new category list, sort it alphabetically, find index of new category\nencoder.categories.append(\"percent\")\nencoder.categories.sort()\nnew_category_index = encoder.categories.index('percent')\n\nprint(encoder.categories)","3914e795":"# perform the initialization\n# the most similar class is text, as our experiment above showed, so we use that as a proxy for % when initializing\nnew_weights = np.copy(old_weights) # important to perform the copy operation\n\n# initialize new weights\nnew_weights[0] = np.insert(new_weights[0], new_category_index, old_weights[0][:,old_category_index], axis=1) # weights\nnew_weights[1] = np.insert(new_weights[1], new_category_index, 0) # biases\n\nnew_weights[0].shape","a96bf324":"# rebuild model using new category_count in last layer\nmodel = Classifier.generate_transfer_model(max_len, max_cells, category_count, category_count+1, checkpoint, checkpoint_dir) # generate model\n\n\n# only the last layer should be trainable - this was already done by the function above, but we repeat for illustrative purposes\nfor layer in model.layers:\n    layer.trainable = False\nmodel.layers[-1].trainable = True\n\nmodel.layers[8].set_weights(new_weights)","754c5e22":"# compile model\nmodel.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n\n# see updated model summary, pay attention to dimension of output layer\nprint(model.summary())","9f8977bf":"# encode data (standardization, transposition, conversion to Numpy array)\nX = encoder.encodeDataFrame(new_raw_data)\nprint(\"Encoded data shape\")\nprint(X.shape)\n\n# encode labels\ny = encoder.label_encode(header)\n\n# visualize\nprint(y)","fb64367b":"# Prepare data in the expected format -> 60\/30\/10 train\/validation\/test data split\ndata = Classifier.setup_test_sets(X, y)","cf1d122f":"import time\n\n# Train\nbatch_size = 4\nnb_epoch = 10\nstart = time.time()\nhistory = Classifier.train_model(batch_size, checkpoint_dir, model, nb_epoch, data) # train model\nend = time.time()\nprint(\"Time for training is %f sec\"%(end-start))","3222bf77":"import matplotlib.pyplot as plt\n\ndf_history = pd.DataFrame(history.history)\n\nfig,ax = plt.subplots()\nplt.plot(range(df_history.shape[0]),df_history['val_acc'],'bs--',label='validation')\nplt.plot(range(df_history.shape[0]),df_history['acc'],'r^--',label='training')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.title('SIMOn Percent Transfer Classification Training')\nplt.legend(loc='best')\nplt.grid()\nplt.show()\n# Save figures\nfig.savefig('SIMOnConvergence.eps', format='eps')\nfig.savefig('SIMOnConvergence.pdf', format='pdf')\nfig.savefig('SIMOnConvergence.png', format='png')\nfig.savefig('SIMOnConvergence.svg', format='svg')","421897a0":"# Test trained model on test set\ny = model.predict(data.X_test) # predict classes\nresult = encoder.reverse_label_encode(y,p_threshold) # reverse encode labels\nprint(\"The predicted classes and probabilities are respectively:\")\nprint(result)\nprint(\"True labels\/probabilities, for comparision:\")\nprint(encoder.reverse_label_encode(data.y_test,p_threshold))","504ccbdb":"X = encoder.encodeDataFrame(raw_library_data) # repeat test on the raw two columns, as a final check\ny = model.predict(X) # predict classes\n\nresult = encoder.reverse_label_encode(y,p_threshold) # reverse encode labels\n\nprint(\"The predicted class\/probability:\")\nprint(result)","fa5010c6":"# Make figures downloadable to local system in interactive mode\nfrom IPython.display import HTML\ndef create_download_link(title = \"Download file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}<\/a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\ncreate_download_link(filename='SIMOnConvergence.svg')","806fb2e2":"Make the required imports","6ec0e180":"Define decision threshold, display model architecture","308d1538":"Let's also get another dataset, for reasons that will be very clear later. Without getting into too many details, we suffice it to mention here that this dataset will be used to expand our SIMOn classifier beyond the set of classes the pretrained model we will use was designed to detect.\n\nThis is a dataset of public library data in British Columbia, obtained from https:\/\/catalogue.data.gov.bc.ca\/dataset\/bc-public-libraries-statistics-2002-present and attached to this notebook, just to ensure it is available for readers of the book easily.\n\nLet's display its path to make sure we know where it is.","d028b887":"Visualize convergence","03574b9c":"Gets integer column right, but percent is recognized as \"text\". This is because that class was not trained for in the current model... \n\nLet's try fine-tuning the float category to detect percentages... \n\nThe percent column here is rather large - ~1200 rows, which can be broken up into >50 columns if each one is length 20  ","32c211d6":"Make a prediction on the BC library, and let's look at it","47f73d05":"We \"standardize\" the data into the expected length, i.e.:\n\n*truncate columns that are too long and replicate cells in shorter columns such that all columns are of length max_cells*\n\nThis is necessary because convolutional neural networks require a fixed length input\n\nWe also convert the data from a DataFrame into a Numpy array, and transpose it such that the first dimension corresponds to sample columns, as per convention\n\nAll these steps are accomplished within the function *encodeDataFrame*\n\n\nHandle baseball data first:","d27ce6f3":"# Encode Data as Numbers","719c2e57":" Let's first download a simple OpenML dataset of baseball stats...","6643a74c":"# Preprocess Tabular Datasets","70fad7d0":"Make prediction for the OpenML baseball dataset","e8e04c2b":"Install SIMOn as a first step.","37e2cbda":"Let's load this dataset and take a peek","7dbfbe9a":"What are the pretrained categories?","127f7518":"# Generate Model and Predict Tabular Data Column Types\n\nGenerate model, load weights into it, compile it...","3e855ac8":"Handle library data next:","399342bd":"# Preliminaries\nWrite requirements to file, anytime you run it, in case you have to go back and recover dependencies.\n\nLatest known such requirements are hosted for each notebook in the companion github repo, and can be pulled down and installed here if needed. Companion github repo is located at https:\/\/github.com\/azunre\/transfer-learning-for-nlp","cf557caa":"Let's select a pair of columns to focus on (of type 'int' and 'percent')","b715a0e2":"Next, we want to load the SIMOn model\n\nFirst load configurations and encoder","541e0edf":"Now, let's download a basic pretrained SIMOn model","61d0829d":"Next, we define some hyperparameters to model data","a3731ad1":"Load and take a peek at the dataset","d7600a74":"# Obtain Sample Tabular Datasets","6b1806d6":"Generate some representative simulated data for illustrative purposes. The downloaded pretrained model was first trained on data of this sort, before transferring to a smaller set of hand-labeled examples"}}