{"cell_type":{"5ee7f77e":"code","4568650f":"code","fdfb98b5":"code","5e1147c0":"code","6c8bcb74":"code","9320fb22":"code","65fab71a":"code","d51b43ea":"code","0b5c2494":"code","0b499e5a":"code","a58fd018":"code","aec6936d":"code","7c284a1f":"code","ba9251c4":"code","c65385dc":"code","cfbcf6ba":"code","8b70ca6f":"code","c37fe57a":"code","9d6f67f0":"code","8d59da94":"code","aebe5913":"code","6733fb14":"code","3df09171":"code","d6b2e248":"code","a3d4f8f5":"code","20ec7d91":"code","44faf0ad":"code","3bcbc421":"code","4e8a3898":"code","35d06055":"code","176d545d":"code","88f98881":"code","0dc71725":"code","ef6dfed7":"code","4a9bb4c2":"code","8f564809":"code","2cff5af5":"code","1e770edd":"code","97ebb61f":"code","6ef74791":"code","ca234cea":"code","097525c7":"code","0105a161":"code","054922a3":"code","4e5f6ba7":"code","d414569b":"code","126ff41e":"code","28d0c3b7":"code","ad7be2a5":"code","29eedbfd":"code","4cd16472":"code","10f1c3fe":"code","27138a2f":"code","f2b3c4a8":"code","a3f4ac57":"code","a43979fa":"code","a481143a":"code","819828a9":"code","7539b7ba":"code","84fb8eeb":"code","c82a6a94":"code","400cde48":"code","73fef101":"code","b3f76386":"code","a4b3a32b":"code","aa5b4d9f":"code","12be62bf":"code","81ac98a5":"code","b08f42b1":"code","b5aa1e72":"code","844bec52":"code","910da1ba":"code","ec2d3d73":"code","e02ef436":"code","0ae830a3":"code","d2c168a7":"code","125b6224":"code","71ea037d":"code","5d0fe54d":"code","3c548c9b":"code","123839d2":"markdown","11965d37":"markdown","af5293cf":"markdown","d0bedd92":"markdown","8537bd86":"markdown","e28d221b":"markdown","af5b7484":"markdown","beda174f":"markdown","1b82a05c":"markdown","767ee133":"markdown","fe792216":"markdown","66194427":"markdown","4be873e9":"markdown","eee4d5d4":"markdown","b614018f":"markdown","26a1b3c2":"markdown","121122a8":"markdown","e87a74b6":"markdown","1296b1e5":"markdown","968e6cac":"markdown","edc05e7c":"markdown","d0d6aa3b":"markdown","4e1f2670":"markdown","8afbc300":"markdown","9a05358a":"markdown","41a213fe":"markdown","6fd9e5ac":"markdown","d560711d":"markdown","88627e30":"markdown","9affce64":"markdown","41ba4227":"markdown","6f0b2a45":"markdown","cb2935f4":"markdown","dbe3fdb5":"markdown","b406bd73":"markdown","1de51c71":"markdown","9cc91e2f":"markdown","6c1cece0":"markdown","41af54d7":"markdown"},"source":{"5ee7f77e":"import numpy as np\nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport gc\nfrom sklearn.model_selection import cross_val_score,GridSearchCV,train_test_split\nfrom sklearn.tree import DecisionTreeClassifier,ExtraTreeClassifier\nfrom sklearn.metrics import roc_curve,roc_auc_score,classification_report,mean_squared_error,accuracy_score\nfrom sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier,BaggingClassifier,VotingClassifier,AdaBoostClassifier","4568650f":"import lightgbm as lgb\nfrom sklearn.metrics import precision_recall_curve,roc_auc_score,classification_report,roc_curve\nfrom tqdm import tqdm","fdfb98b5":"from subprocess import check_output\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nsongs = pd.read_csv('..\/input\/songs.csv')\nmembers = pd.read_csv('..\/input\/members.csv')\nsample = pd.read_csv('..\/input\/sample_submission.csv')","5e1147c0":"train.head()","6c8bcb74":"test.head()","9320fb22":"songs.head()","65fab71a":"members.head()","d51b43ea":"sample.head()\nmembers.shape\ntrain.info()\nprint(\"\\n\")\nsongs.info()\nprint(\"\\n\")\nmembers.info()","0b5c2494":"plt.figure(figsize=(20,15))\nsns.set(font_scale=2)\nsns.countplot(x='source_type',hue='source_type',data=train)\nsns.set(style=\"darkgrid\")\nplt.xlabel('source types',fontsize=30)\nplt.ylabel('count',fontsize=30)\nplt.xticks(rotation='45')\nplt.title('Count plot source types for listening music',fontsize=30)\nplt.tight_layout()","0b499e5a":"plt.figure(figsize=(20,15))\nsns.set(font_scale=2)\nsns.countplot(y='source_screen_name',data=train,facecolor=(0,0,0,0),linewidth=5,edgecolor=sns.color_palette('dark',3))\nsns.set(style=\"darkgrid\")\nplt.xlabel('source types',fontsize=30)\nplt.ylabel('count',fontsize=30)\nplt.xticks(rotation='45')\nplt.title('Count plot for which  screen using ',fontsize=30)\nplt.tight_layout()","a58fd018":"plt.figure(figsize=(20,15))\nsns.set(font_scale=2)\nsns.countplot(x='source_system_tab',hue='source_system_tab',data=train)\nsns.set(style=\"darkgrid\")\nplt.xlabel('source types',fontsize=30)\nplt.ylabel('count',fontsize=30)\nplt.xticks(rotation='45')\nplt.title('Count plot for system tab there are using',fontsize=30)\nplt.tight_layout()","aec6936d":"import matplotlib as mpl\n\nmpl.rcParams['font.size'] = 40.0\nlabels = ['Male','Female']\nplt.figure(figsize = (12, 12))\nsizes = pd.value_counts(members.gender)\npatches, texts, autotexts = plt.pie(sizes, \n                                    labels=labels, autopct='%.0f%%',\n                                    shadow=False, radius=1,startangle=90)\nfor t in texts:\n    t.set_size('smaller')\nplt.legend()\nplt.show()\n","7c284a1f":"import matplotlib.pyplot as plt\nmpl.rcParams['font.size'] = 40.0\nplt.figure(figsize = (20, 20)) \n# Make data: I have 3 groups and 7 subgroups\ngroup_names=['explore','my library','search','discover','radio','listen with','notification','settings']\ngroup_size=pd.value_counts(train.source_system_tab)\nprint(group_size)\nsubgroup_names=['Male','Female']\nsubgroup_size=pd.value_counts(members.gender)\n \n# Create colors\na, b, c,d,e,f,g,h=[plt.cm.autumn, plt.cm.GnBu, plt.cm.YlGn,plt.cm.Purples,plt.cm.cool,plt.cm.RdPu,plt.cm.BuPu,plt.cm.bone]\n \n# First Ring (outside)\nfig, ax = plt.subplots()\nax.axis('equal')\nmypie, texts= ax.pie(group_size, radius=3.0,labels=group_names, colors=[a(0.6), b(0.6), c(0.6),d(0.6), e(0.6), f(0.6),g(0.6)])\nplt.setp( mypie, width=0.3, edgecolor='white')\n \n# Second Ring (Inside)\n#mypie2, texts1 = ax.pie(subgroup_size, radius=3.0-0.3, labels=subgroup_names, labeldistance=0.7, colors=[h(0.5), b(0.4)])\n#plt.setp( mypie2, width=0.3, edgecolor='white')\n#plt.margins(0,0)\n#for t in texts:\n #   t.set_size(25.0)\n#for t in texts1:\n \n    #t.set_size(25.0)    \nplt.legend() \n# show it\nplt.show()\n","ba9251c4":"print(members.describe())","c65385dc":"print(songs.describe())","cfbcf6ba":"mpl.rcParams['font.size'] = 40.0\nplt.figure(figsize = (20, 20)) \nsns.distplot(members.registration_init_time)\nsns.set(font_scale=2)\nplt.ylabel('ecdf',fontsize=50)\nplt.xlabel('registration time ' ,fontsize=50)\n\n","8b70ca6f":"members.describe()","c37fe57a":"songs.describe()","9d6f67f0":"train.describe()","8d59da94":"train.info()","aebe5913":"members.info()","6733fb14":"train_members = pd.merge(train, members, on='msno', how='inner')\ntrain_merged = pd.merge(train_members, songs, on='song_id', how='outer')\nprint(train_merged.head())","3df09171":"test_members = pd.merge(test, members, on='msno', how='inner')\ntest_merged = pd.merge(test_members, songs, on='song_id', how='outer')\nprint(test_merged.head())\nprint(len(test_merged.columns))","d6b2e248":"del train_members\ndel test_members","a3d4f8f5":"ax = sns.countplot(y=train_merged.dtypes, data=train_merged)","20ec7d91":"print(train_merged.columns.to_series().groupby(train_merged.dtypes).groups)\nprint(test_merged.columns.to_series().groupby(test_merged.dtypes).groups)","44faf0ad":"msno.heatmap(train_merged)\n#msno.matrix(train_merged)","3bcbc421":"#msno.dendrogram(train_merged)","4e8a3898":"#--- Function to check if missing values are present and if so print the columns having them ---\ndef check_missing_values(df):\n    print (df.isnull().values.any())\n    if (df.isnull().values.any() == True):\n        columns_with_Nan = df.columns[df.isnull().any()].tolist()\n    print(columns_with_Nan)\n    for col in columns_with_Nan:\n        print(\"%s : %d\" % (col, df[col].isnull().sum()))\n    \ncheck_missing_values(train_merged)\ncheck_missing_values(test_merged)","35d06055":"#--- Function to replace Nan values in columns of type float with -5 ---\ndef replace_Nan_non_object(df):\n    object_cols = list(df.select_dtypes(include=['float']).columns)\n    for col in object_cols:\n        df[col]=df[col].fillna(np.int(-5))\n       \nreplace_Nan_non_object(train_merged) \nreplace_Nan_non_object(test_merged)  ","176d545d":"#--- memory consumed by train dataframe ---\nmem = train_merged.memory_usage(index=True).sum()\nprint(\"Memory consumed by training set  :   {} MB\" .format(mem\/ 1024**2))\n \n#--- memory consumed by test dataframe ---\nmem = test_merged.memory_usage(index=True).sum()\nprint(\"Memory consumed by test set      :   {} MB\" .format(mem\/ 1024**2))","88f98881":"def change_datatype(df):\n    float_cols = list(df.select_dtypes(include=['float']).columns)\n    for col in float_cols:\n        if ((np.max(df[col]) <= 127) and(np.min(df[col] >= -128))):\n            df[col] = df[col].astype(np.int8)\n        elif ((np.max(df[col]) <= 32767) and(np.min(df[col] >= -32768))):\n            df[col] = df[col].astype(np.int16)\n        elif ((np.max(df[col]) <= 2147483647) and(np.min(df[col] >= -2147483648))):\n            df[col] = df[col].astype(np.int32)\n        else:\n            df[col] = df[col].astype(np.int64)\n\nchange_datatype(train_merged)\nchange_datatype(test_merged)","0dc71725":"data = train_merged.groupby('target').aggregate({'msno':'count'}).reset_index()\na4_dims = (15, 8)\nfig, ax = plt.subplots(figsize=a4_dims)\nax = sns.barplot(x='target', y='msno', data=data)","ef6dfed7":"mpl.rcParams['font.size'] = 40.0\nplt.figure(figsize = (20, 20)) \ndata=train_merged.groupby('source_system_tab').aggregate({'msno':'count'}).reset_index()\nsns.barplot(x='source_system_tab',y='msno',data=data)","4a9bb4c2":"data = train_merged.groupby('source_screen_name').aggregate({'msno':'count'}).reset_index()\na4_dims = (15, 7)\nfig, ax = plt.subplots(figsize=a4_dims)\nax = sns.barplot(x='source_screen_name', y='msno', data=data)\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)","8f564809":"data = train_merged.groupby('source_type').aggregate({'msno':'count'}).reset_index()\na4_dims = (15, 7)\nfig, ax = plt.subplots(figsize=a4_dims)\nax = sns.barplot(x='source_type', y='msno', data=data)\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)","2cff5af5":"data = train_merged.groupby('language').aggregate({'msno':'count'}).reset_index()\na4_dims = (15, 7)\nfig, ax = plt.subplots(figsize=a4_dims)\nax = sns.barplot(x='language', y='msno', data=data)\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)","1e770edd":"data = train_merged.groupby('registered_via').aggregate({'msno':'count'}).reset_index()\na4_dims = (15, 7)\nfig, ax = plt.subplots(figsize=a4_dims)\nax = sns.barplot(x='registered_via', y='msno', data=data)\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)","97ebb61f":"print(train_merged.columns)\ndata = train_merged.groupby('city').aggregate({'msno':'count'}).reset_index()\na4_dims = (15, 7)\nfig, ax = plt.subplots(figsize=a4_dims)\nax = sns.barplot(x='city', y='msno', data=data)\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)","6ef74791":"a4_dims = (15, 7)\nfig, ax = plt.subplots(figsize=a4_dims)\nax=sns.countplot(x=\"source_system_tab\",data=train_merged,palette=['lightblue','orange','green'],hue=\"target\")\nplt.xlabel(\"source_screen_tab\")\nplt.ylabel(\"count\")\nplt.title(\"source_system_tab vs target \")\nplt.show()","ca234cea":"a4_dims = (15, 7)\nfig, ax = plt.subplots(figsize=a4_dims)\nax=sns.countplot(x=\"source_screen_name\",data=train_merged,palette=['#A8B820','yellow','#98D8D8'],hue=\"target\")\nplt.xlabel(\"source_screen_name\")\nplt.ylabel(\"count\")\nplt.title(\"source_screen_name vs target \")\nplt.xticks(rotation='90')\nplt.show()","097525c7":"a4_dims = (15, 7)\nfig, ax = plt.subplots(figsize=a4_dims)\nax=sns.countplot(x=\"gender\",data=train_merged,palette=['#705898','#7038F8','yellow'],hue=\"target\")\nplt.xlabel(\"male female participation\")\nplt.ylabel(\"count\")\nplt.title(\"male female participation vs target \")\nplt.xticks(rotation='90')\nplt.legend(loc='upper left')\nplt.show()","0105a161":"a4_dims = (15, 7)\nfig, ax = plt.subplots(figsize=a4_dims)\nax=sns.heatmap(data=train_merged.corr(),annot=True,fmt=\".2f\")","054922a3":"a4_dims = (15, 7)\nfig, ax = plt.subplots(figsize=a4_dims)\nax=sns.boxplot(x=\"gender\",y=\"city\",data=train_merged,palette=['blue','orange','green'],hue=\"target\")\nplt.xlabel(\"gender\")\nplt.ylabel(\"city\")\nplt.title(\"city vs registered_via  \")\nplt.show()","4e5f6ba7":"ax=sns.lmplot(x=\"bd\",y=\"registered_via\",data=train_merged,palette=['blue','orange','green'],hue=\"target\",fit_reg=False)\nplt.xlabel(\"bd age group\")\nplt.ylabel(\"registred_via\")\nplt.title(\" bd age group vs registration_via \")\nplt.show()","d414569b":"ax=sns.lmplot(x=\"bd\",y=\"city\",data=train_merged,palette=['blue','orange','green'],hue=\"target\",fit_reg=False)\nplt.xlabel(\"bd age group\")\nplt.ylabel(\"city\")\nplt.title(\"bd (age group) vs city \")\nplt.show()","126ff41e":"#remomving outlier from bd age group column","28d0c3b7":"a4_dims = (15, 7)\nfig, ax = plt.subplots(figsize=a4_dims)\nax=sns.boxplot(x=\"bd\",y=\"gender\",data=train_merged,palette=['blue','orange','green'])\nplt.xlabel(\"bd age group\")\nplt.ylabel(\"gender\")\nplt.title(\"bd age group vs gender \")\nplt.show()","ad7be2a5":"train_merged.describe()\ndef remove_outlier(df_in, col_name):\n\n    #q1 = df_in[col_name].quantile(0.25)\n    #q3 = df_in[col_name].quantile(0.75)\n    #iqr = q3-q1 #Interquartile range\n    fence_low  = 12\n    fence_high = 45\n    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n    return df_out\ndf_final_train=remove_outlier(train_merged,'bd')","29eedbfd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport lightgbm as lgb\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","4cd16472":"print('Loading data...')\ndata_path = '..\/input\/'\ntrain = pd.read_csv(data_path + 'train.csv', dtype={'msno' : 'category',\n                                                'source_system_tab' : 'category',\n                                                  'source_screen_name' : 'category',\n                                                  'source_type' : 'category',\n                                                  'target' : np.uint8,\n                                                  'song_id' : 'category'})\ntest = pd.read_csv(data_path + 'test.csv', dtype={'msno' : 'category',\n                                                'source_system_tab' : 'category',\n                                                'source_screen_name' : 'category',\n                                                'source_type' : 'category',\n                                                'song_id' : 'category'})\nsongs = pd.read_csv(data_path + 'songs.csv',dtype={'genre_ids': 'category',\n                                                  'language' : 'category',\n                                                  'artist_name' : 'category',\n                                                  'composer' : 'category',\n                                                  'lyricist' : 'category',\n                                                  'song_id' : 'category'})\nmembers = pd.read_csv(data_path + 'members.csv',dtype={'city' : 'category',\n                                                      'bd' : np.uint8,\n                                                      'gender' : 'category',\n                                                      'registered_via' : 'category'},\n                     parse_dates=['registration_init_time','expiration_date'])\nsongs_extra = pd.read_csv(data_path + 'song_extra_info.csv')\nprint('Done loading...')","10f1c3fe":"song_cols = ['song_id', 'artist_name', 'genre_ids', 'song_length', 'language']\ntrain = train.merge(songs[song_cols], on='song_id', how='left')\ntest = test.merge(songs[song_cols], on='song_id', how='left')\n\nmembers['registration_year'] = members['registration_init_time'].apply(lambda x: int(str(x)[0:4]))\n#members['registration_month'] = members['registration_init_time'].apply(lambda x: int(str(x)[4:6]))\n#members['registration_date'] = members['registration_init_time'].apply(lambda x: int(str(x)[6:8]))\n\nmembers['expiration_year'] = members['expiration_date'].apply(lambda x: int(str(x)[0:4]))\nmembers['expiration_month'] = members['expiration_date'].apply(lambda x: int(str(x)[4:6]))\n#members['expiration_date'] = members['expiration_date'].apply(lambda x: int(str(x)[6:8]))\n\n# exepting some unimportanat features\n\n\n# Convert date to number of days\nmembers['membership_days'] = (members['expiration_date'] - members['registration_init_time']).dt.days.astype(int)\n\n#members = members.drop(['registration_init_time'], axis=1)\n#members = members.drop(['expiration_date'], axis=1)","27138a2f":"# categorize membership_days \nmembers['membership_days'] = members['membership_days']\/\/200\nmembers['membership_days'] = members['membership_days'].astype('category')","f2b3c4a8":"member_cols = ['msno','city','registered_via', 'registration_year', 'expiration_year', 'membership_days']\n\ntrain = train.merge(members[member_cols], on='msno', how='left')\ntest = test.merge(members[member_cols], on='msno', how='left')\n","a3f4ac57":"train.info()\n","a43979fa":"def isrc_to_year(isrc):\n    if type(isrc) == str:\n        if int(isrc[5:7]) > 17:\n            return int(isrc[5:7])\/\/5\n        else:\n            return int(isrc[5:7])\/\/5\n    else:\n        return np.nan\n#categorize song_year per 5years\n\nsongs_extra['song_year'] = songs_extra['isrc'].apply(isrc_to_year)\nsongs_extra.drop(['isrc', 'name'], axis = 1, inplace = True)","a481143a":"train = train.merge(songs_extra, on = 'song_id', how = 'left')\ntest = test.merge(songs_extra, on = 'song_id', how = 'left')\n","819828a9":"train['genre_ids'] = train['genre_ids'].str.split('|').str[0]","7539b7ba":"temp_song_length = train['song_length']","84fb8eeb":"train.drop('song_length', axis = 1, inplace = True)\ntest.drop('song_length',axis = 1 , inplace =True)","c82a6a94":"train.head()","400cde48":"song_count = train.loc[:,[\"song_id\",\"target\"]]\n\n# measure repeat count by played songs\nsong_count1 = song_count.groupby([\"song_id\"],as_index=False).sum().rename(columns={\"target\":\"repeat_count\"})\n\n# count play count by songs\nsong_count2 = song_count.groupby([\"song_id\"],as_index=False).count().rename(columns = {\"target\":\"play_count\"})","73fef101":"song_repeat = song_count1.merge(song_count2,how=\"inner\",on=\"song_id\")\nsong_repeat[\"repeat_percentage\"] = round((song_repeat['repeat_count']*100) \/ song_repeat['play_count'],1)\nsong_repeat['repeat_count'] = song_repeat['repeat_count'].astype('int')\nsong_repeat['repeat_percentage'] = song_repeat['repeat_percentage'].replace(100.0,np.nan)\n#cuz most of 100.0 are played=1 repeated=1 values. I think it is not fair compare with other played a lot songs","b3f76386":"train = train.merge(song_repeat,on=\"song_id\",how=\"left\")\ntest = test.merge(song_repeat,on=\"song_id\",how=\"left\")","a4b3a32b":"# type cast\ntest['song_id'] = test['song_id'].astype('category')\ntest['repeat_count'] = test['repeat_count'].fillna(0)\ntest['repeat_count'] = test['repeat_count'].astype('int')\ntest['play_count'] = test['play_count'].fillna(0)\ntest['play_count'] = test['play_count'].astype('int')\n#train['repeat_percentage'].replace(100.0,np.nan)\n","aa5b4d9f":"artist_count = train.loc[:,[\"artist_name\",\"target\"]]\n\n# measure repeat count by played songs\nartist_count1 = artist_count.groupby([\"artist_name\"],as_index=False).sum().rename(columns={\"target\":\"repeat_count_artist\"})\n\n# measure play count by songs\nartist_count2 = artist_count.groupby([\"artist_name\"],as_index=False).count().rename(columns = {\"target\":\"play_count_artist\"})\n\nartist_repeat = artist_count1.merge(artist_count2,how=\"inner\",on=\"artist_name\")\n","12be62bf":"artist_repeat[\"repeat_percentage_artist\"] = round((artist_repeat['repeat_count_artist']*100) \/ artist_repeat['play_count_artist'],1)\nartist_repeat['repeat_count_artist'] = artist_repeat['repeat_count_artist'].fillna(0)\nartist_repeat['repeat_count_artist'] = artist_repeat['repeat_count_artist'].astype('int')\nartist_repeat['repeat_percentage_artist'] = artist_repeat['repeat_percentage_artist'].replace(100.0,np.nan)","81ac98a5":"#use only repeat_percentage_artist\ndel artist_repeat['repeat_count_artist']\n#del artist_repeat['play_count_artist']","b08f42b1":"#merge it with artist_name to train dataframe\ntrain = train.merge(artist_repeat,on=\"artist_name\",how=\"left\")\ntest = test.merge(artist_repeat,on=\"artist_name\",how=\"left\")","b5aa1e72":"train.info()","844bec52":"del train['artist_name']\ndel test['artist_name']","910da1ba":"msno_count = train.loc[:,[\"msno\",\"target\"]]\n\n# count repeat count by played songs\nmsno_count1 = msno_count.groupby([\"msno\"],as_index=False).sum().rename(columns={\"target\":\"repeat_count_msno\"})\n\n# count play count by songs\nmsno_count2 = msno_count.groupby([\"msno\"],as_index=False).count().rename(columns = {\"target\":\"play_count_msno\"})\n\nmsno_repeat = msno_count1.merge(msno_count2,how=\"inner\",on=\"msno\")","ec2d3d73":"msno_repeat[\"repeat_percentage_msno\"] = round((msno_repeat['repeat_count_msno']*100) \/ msno_repeat['play_count_msno'],1)\nmsno_repeat['repeat_count_msno'] = msno_repeat['repeat_count_msno'].fillna(0)\nmsno_repeat['repeat_count_msno'] = msno_repeat['repeat_count_msno'].astype('int')\n#msno_repeat['repeat_percentage_msno'] = msno_repeat['repeat_percentage_msno'].replace(100.0,np.nan)\n# it can be meaningful so do not erase 100.0 ","e02ef436":"#merge it with msno to train dataframe\ntrain = train.merge(msno_repeat,on=\"msno\",how=\"left\")\ntest = test.merge(msno_repeat,on=\"msno\",how=\"left\")\n","0ae830a3":"import gc\n#del members, songs; gc.collect();\n\nfor col in train.columns:\n    if train[col].dtype == object:\n        train[col] = train[col].astype('category')\n        test[col] = test[col].astype('category')","d2c168a7":"train['song_year'] = train['song_year'].astype('category')\ntest['song_year'] = test['song_year'].astype('category')","125b6224":"train.head()","71ea037d":"drop_list = ['repeat_count','repeat_percentage',\n             'repeat_percentage_artist',\n             'repeat_count_msno','repeat_percentage_msno'\n            ]\ntrain = train.drop(drop_list,axis=1)\ntest = test.drop(drop_list,axis=1)","5d0fe54d":"test['play_count_msno'] = test['play_count_msno'].fillna(0)\ntest['play_count_msno'] = test['play_count_msno'].astype('int')\n\n\ntrain['play_count_artist'] = train['play_count_artist'].fillna(0)\ntest['play_count_artist'] = test['play_count_artist'].fillna(0)\ntrain['play_count_artist'] = train['play_count_artist'].astype('int')\ntest['play_count_artist'] = test['play_count_artist'].astype('int')\n","3c548c9b":"from sklearn.model_selection import KFold\n# Create a Cross Validation with 3 splits\nkf = KFold(n_splits=3)\n\npredictions = np.zeros(shape=[len(test)])\n\n# For each KFold\nfor train_indices ,validate_indices in kf.split(train) : \n    train_data = lgb.Dataset(train.drop(['target'],axis=1).loc[train_indices,:],label=train.loc[train_indices,'target'])\n    val_data = lgb.Dataset(train.drop(['target'],axis=1).loc[validate_indices,:],label=train.loc[validate_indices,'target'])\n\n    params = {\n            'objective': 'binary',\n            'boosting': 'gbdt',\n            'learning_rate': 0.2 ,\n            'verbose': 0,\n            'num_leaves': 2**8,\n            'bagging_fraction': 0.95,\n            'bagging_freq': 1,\n            'bagging_seed': 1,\n            'feature_fraction': 0.9,\n            'feature_fraction_seed': 1,\n            'max_bin': 256,\n            'num_rounds': 80,\n            'metric' : 'auc'\n        }\n    # Train the model    \n    lgbm_model = lgb.train(params, train_data, 100, valid_sets=[val_data])\n    predictions += lgbm_model.predict(test.drop(['id'],axis=1))\n    del lgbm_model\n    # We get the ammount of predictions from the prediction list, by dividing the predictions by the number of Kfolds.\npredictions = predictions\/3\n\nINPUT_DATA_PATH = '..\/input\/'\n\n# Read the sample_submission CSV\nsubmission = pd.read_csv(INPUT_DATA_PATH + '\/sample_submission.csv')\n# Set the target to our predictions\nsubmission.target=predictions\n# Save the submission file\nsubmission.to_csv('submission.csv',index=False)","123839d2":"# as we can see that mean age group we have 24 to 27 with max is 50 in female case and in male case 48 about age group is max and  min in female it is about 16 and in male case 18 \n\n# one more observation we can see that female outlier are more there reason behind this logic females always tend fill up the things in hurry way because in male we can't see male with 100 , as if this bit funny logic , apart from this it all due unclean data that's it which we have to remove outliers","11965d37":"# As we can see we have male users more now visualization has to be done in this manner like from how many types genders which are popular ways to go back in there playlist","af5293cf":"# so we can decision tree was much better than extra trees classifier","d0bedd92":"# as we can see that new user are about 5500 and old users about 15000  , \n# *-5 are those values which are empty \n","8537bd86":"# doing stats test on members.csv","e28d221b":"# AS we  have imported necessary modules now we can start with\n\n# EDA (Exploratory Data Analysis) with wrangling of data and visualizing\n\n# The necessary thing for out statiscal analysis as well insights of our data \n\n# Last steps would be data imputations , merging , cross validations , \n\n# Hyperparmaters tuning and visualization of every algowhat we used what are\n\n# It's result with Time consumption of algos producing the results","af5b7484":"# Data conversion of int , float and categorical has to be done to reduce the data size for computation as well as storage ","beda174f":"# missing values from the heatmap also showing one thing that information which are missing and has positive correlation are gender with 4 variables of train.csv and rest of varibales with members.csv","1b82a05c":"# Now checking missing values and replacing them with some unique values ","767ee133":"# First visualization we can see as if local library are more perffered than any other source types as well after that online playlist and local playlist and other features are showing less importance but can't say anything right now as we handn't deal with cleaning , imputing , stats \n\n# But as far we are sure are answers for buliding this systems in revolving maximum issues around local library see next what other result say ","fe792216":"In this task, you will be asked to predict the chances of a user listening to a song repetitively after the first observable listening event within a time window was triggered. If there are recurring listening event(s) triggered within a month after the user\u2019s very first observable listening event, its target is marked 1, and 0 otherwise in the training set. The same rule applies to the testing set.\n\nKKBOX provides a training data set consists of information of the first observable listening event for each unique user-song pair within a specific time duration. Metadata of each unique user and song pair is also provided. The use of public data to increase the level of accuracy of your prediction is encouraged.\n\nThe train and the test data are selected from users listening history in a given time period. Note that this time period is chosen to be before the WSDM-KKBox Churn Prediction time period. The train and test sets are split based on time, and the split of public\/private are based on unique user\/song pairs.\n\nTables train.csv\nmsno: user id\nsong_id: song id\nsource_system_tab: the name of the tab where the event was triggered. System tabs are used to categorize KKBOX mobile apps functions. For example, tab my library contains functions to manipulate the local storage, and tab search contains functions relating to search.\nsource_screen_name: name of the layout a user sees.\nsource_type: an entry point a user first plays music on mobile apps. An entry point could be album, online-playlist, song .. etc.\ntarget: this is the target variable. target=1 means there are recurring listening event(s) triggered within a month after the user\u2019s very first observable listening event, target=0 otherwise .\ntest.csv\nid: row id (will be used for submission)\nmsno: user id\nsong_id: song id\nsource_system_tab: the name of the tab where the event was triggered. System tabs are used to categorize KKBOX mobile apps functions. For example, tab my library contains functions to manipulate the local storage, and tab search contains functions relating to search.\nsource_screen_name: name of the layout a user sees.\nsource_type: an entry point a user first plays music on mobile apps. An entry point could be album, online-playlist, song .. etc.\nsample_submission.csv\nsample submission file in the format that we expect you to submit\n\nid: same as id in test.csv\ntarget: this is the target variable. target=1 means there are recurring listening event(s) triggered within a month after the user\u2019s very first observable listening event, target=0 otherwise .\nsongs.csv\nThe songs. Note that data is in unicode.\n\nsong_id\nsong_length: in ms\ngenre_ids: genre category. Some songs have multiple genres and they are separated by |\nartist_name\ncomposer\nlyricist\nlanguage\nmembers.csv\nuser information.\n\nmsno\ncity\nbd: age. Note: this column has outlier values, please use your judgement.\ngender\nregistered_via: registration method\nregistration_init_time: format %Y%m%d\nexpiration_date: format %Y%m%d\nsong_extra_info.csv\nsong_id\nsong name - the name of the song.\nisrc - International Standard Recording Code, theoretically can be used as an identity of a song. However, what worth to note is, ISRCs generated from providers have not been officially verified; therefore the information in ISRC, such as country code and reference year, can be misleading\/incorrect. Multiple songs could share one ISRC since a single recording could be re-published several times.","66194427":"# now we can see on thing that music users vary age form 0 to 100 we can see here are outliers to in bd but interesting information are that most users age group of younsters and 30+ age group form 5 to 10 registered_via index","4be873e9":"# new user are coming form discover and my llibrary and old ones are from my library","eee4d5d4":"# Inferences we can draw from this chart that among Men exploration method is only way they are using while females are using every possible way to get back their music of choices , in real world this thing also very much similar that men focuses in one direction in depth whereas women focuses in every possible direction but not in depth\n\n# We are moving in right direction of building a good accurate systems","b614018f":"# Now moving toward ML approach or machine learning ","26a1b3c2":"# Now we ultimate clean data set ","121122a8":"# new female users are more than male users about 500 to 600","e87a74b6":"# now some statistics inferences \n# as we have numeric data in tow csv files and rest of the files with categorical data so members.csv file with 2 columns in numeric and song.csv ","1296b1e5":"# as we can see that accuracy is more but still auc_score is very less ","968e6cac":"# Clean the test data set","edc05e7c":"# with outlier as we can we didn't remove till now we will remove bd outliers at final stages before applying Ml but that last results insights are telling we have age group 20 to 30+ ages and city index we most 5 to 14 ","d0d6aa3b":"# Analysis on missing values \n","4e1f2670":"# most users 7 and 9 ways to get registered","8afbc300":"# we can see that in members and songs csv files large differences bet min and max values which gives inferences that there are outliers in the csv files which has to be removed before making system ","9a05358a":"# here we can see that most of our user are between 5 to 14 no of cities might be female ratio is same ","41a213fe":"# but not yet preprocssed so now preprocssing task  ","6fd9e5ac":"# Second Visualization is telling us that most of the users are listenning local      \n# playlist more means the app which is provided by the company they are using them apart from this we can also see that after this most the users are coming back to the songs by online playlist sources \n\n# Very less from the other different sources means are outliers , variance and std deviations are in 2 areas local libs and online playlist \n","d560711d":"# no of users are 1,13,5 are containig maximum values ","88627e30":"# I'm Coming ####head - ROHAN (Data Scientist at Databytes Analytics)","9affce64":"# so anyone who has installed KKBOX app we can see most of the users are going back to there songs via my library rather discovering them means there are different sources they can go back but most preffered one is my library \n\n# now doing some visualiaztion in members.csv","41ba4227":"# A strong nullity correlation here we can see \n# song id -> lang, song_len,artist name, genre_id\n# composer -> lyricst\n# gender -> with song_id\n\n# from heatmap we can say if gender is missing 70% missing values will be in msno , target city etc ","6f0b2a45":"# local playlist among new user and old one more most common way to get back their songs","cb2935f4":"# avg no of male users are 13 to 15 city no ","dbe3fdb5":"# MY approch make machine learn till not get generalize models and i called it S.E.D.A.C.R.O.M.L","b406bd73":"# i guess we have almost clean data in this data cleanning i used brute force approach as i get 0.75 percentile values as 0 so there is now way taking standard deviations so i used lookup approach to remove outliers special in this bd age group let's hope my system accurarcy don't went down ","1de51c71":"# inferences we can drawn from above two result that maximum registration were done in time period of 2012 to 2016 and most this righthand skewed graph one more thing before applying we have to normalize it ","9cc91e2f":"# Why i'm making all the classification with trees only there is reason behind that other trees algos are best for classifications , improving loss function in gradient boosters and xgboost that's why and another more reason like why not svm is not made for this type working , powerfull in case of text audio , images ","6c1cece0":"# as we can see lot of missing values are coming up but when common thing we notice that most of the missing values are arrived from members and songs\n","41af54d7":"# here decision tree at it's final showoff can expect too much from this one \n"}}