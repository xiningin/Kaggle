{"cell_type":{"b3f1fb01":"code","45a5b69c":"code","61392c11":"code","880b0546":"code","350c43f5":"code","5cd65b4d":"code","6ad4f6a8":"code","e57b1f3e":"code","769648aa":"code","2bfb8b78":"code","30fcb3e3":"code","e9a551cf":"code","0286fcc6":"code","043ffdc7":"code","d47f41ca":"code","5422e596":"code","048878c7":"code","6ec80285":"markdown","2111b422":"markdown","ef0cd3a3":"markdown","7b32e36a":"markdown","99a6d4d5":"markdown","1a5c91c2":"markdown","b7819818":"markdown","6684aebf":"markdown","e714bdd8":"markdown","c9176ff9":"markdown","80d14cea":"markdown","69fac896":"markdown","764ce4bd":"markdown"},"source":{"b3f1fb01":"import numpy as np\nimport pandas as pd\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom nltk import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom collections import defaultdict\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer","45a5b69c":"train_orig = pd.read_csv(\"..\/input\/quora-question-pairs-data-cleaning\/train_orig.csv\")\ntest_orig = pd.read_csv(\"..\/input\/quora-question-pairs-data-cleaning\/test_orig.csv\")\ntrain_stop = pd.read_csv(\"..\/input\/quora-question-pairs-data-cleaning\/train_stop.csv\")\ntest_stop = pd.read_csv(\"..\/input\/quora-question-pairs-data-cleaning\/test_stop.csv\")\ntrain_stem = pd.read_csv(\"..\/input\/quora-question-pairs-data-cleaning\/train_stem.csv\")\ntest_stem = pd.read_csv(\"..\/input\/quora-question-pairs-data-cleaning\/test_stem.csv\")\ntrain_lem = pd.read_csv(\"..\/input\/quora-question-pairs-data-cleaning\/train_lem.csv\")\ntest_lem = pd.read_csv(\"..\/input\/quora-question-pairs-data-cleaning\/test_lem.csv\")","61392c11":"train_orig = train_orig.fillna(\"\")\ntest_orig = test_orig.fillna(\"\")\ntrain_stop = train_stop.fillna(\"\")\ntest_stop = test_stop.fillna(\"\")\ntrain_stem = train_stem.fillna(\"\")\ntest_stem = test_stem.fillna(\"\")\ntrain_lem = train_lem.fillna(\"\")\ntest_lem = test_lem.fillna(\"\")","880b0546":"train = pd.DataFrame(index = train_orig.index)\ntest = pd.DataFrame(index = test_orig.index)\ntrainlabel = train_orig[[\"is_duplicate\"]]","350c43f5":"len1 = train_orig[\"question1\"].apply(len)\nlen2 = train_orig[\"question2\"].apply(len)\ntrain[\"diff_char\"] = abs(len1 - len2)\ntrain[\"diff_char_rate\"] = 2 * abs(len1 - len2) \/ (len1 + len2)\n\nlen3 = test_orig[\"question1\"].apply(len)\nlen4 = test_orig[\"question2\"].apply(len)\ntest[\"diff_char\"] = abs(len3 - len4)\ntest[\"diff_char_rate\"] = 2 * abs(len3 - len4) \/ (len3 + len4)\n\ndel len1, len2, len3, len4","5cd65b4d":"def words_count(text):\n    wordlist = word_tokenize(text)\n    count = len(wordlist)\n    return count\n\ncount1 = train_orig[\"question1\"].apply(words_count)\ncount2 = train_orig[\"question2\"].apply(words_count)\ntrain[\"diff_words\"] = abs(count1 - count2)\ntrain[\"diff_words_rate\"] = 2 * abs(count1 - count2) \/ (count1 + count2)\n\ncount3 = test_orig[\"question1\"].apply(words_count)\ncount4 = test_orig[\"question2\"].apply(words_count)\ntest[\"diff_words\"] = abs(count3 - count4)\ntest[\"diff_words_rate\"] = 2 * abs(count3 - count4) \/ (count3 + count4)","6ad4f6a8":"def shared_words_count(text1, text2):\n    wordlist1 = word_tokenize(text1)\n    wordlist2 = word_tokenize(text2)\n    wordset1 = set(wordlist1)\n    wordset2 = set(wordlist2)\n    inter = wordset1 & wordset2\n    union = wordset1 | wordset2\n    count = len(inter)\n    rate = 2 * count \/ (len(wordset1) + len(wordset2) + 1)  # \u4e3a\u4e86\u9632\u6b62wordset1\u548cwordset2\u540c\u65f6\u4e3a\u7a7a\uff0c\u4e5f\u5373text1\u548ctext2\u90fd\u662f\u53ea\u5305\u542b\u4e00\u4e2a\u7a7a\u683c\u7684\u5b57\u7b26\u4e32\n    jaccard = count \/ (len(union) + 1)\n    return pd.Series([count, rate, jaccard])","e57b1f3e":"share_train_orig = train_orig[[\"question1\", \"question2\"]].apply(lambda x: shared_words_count(x[0], x[1]), axis = 1)\nshare_train_stop = train_stop[[\"question1\", \"question2\"]].apply(lambda x: shared_words_count(x[0], x[1]), axis = 1)\nshare_train_stem = train_stem[[\"question1\", \"question2\"]].apply(lambda x: shared_words_count(x[0], x[1]), axis = 1)\nshare_train_lem = train_lem[[\"question1\", \"question2\"]].apply(lambda x: shared_words_count(x[0], x[1]), axis = 1)\nshare_words_train = pd.concat([share_train_orig, share_train_stop, share_train_stem, share_train_lem], axis = 1)\nshare_words_train.columns = [\"share_words_count_orig\", \"share_words_rate_orig\", \"jaccard_orig\", \n                             \"share_words_count_stop\", \"share_words_rate_stop\", \"jaccard_stop\", \n                             \"share_words_count_stem\", \"share_words_rate_stem\", \"jaccard_stem\",\n                             \"share_words_count_lem\", \"share_words_rate_lem\", \"jaccard_lem\"]\ntrain = pd.concat([train, share_words_train], axis = 1)\n\nshare_test_orig = test_orig[[\"question1\", \"question2\"]].apply(lambda x: shared_words_count(x[0], x[1]), axis = 1)\nshare_test_stop = test_stop[[\"question1\", \"question2\"]].apply(lambda x: shared_words_count(x[0], x[1]), axis = 1)\nshare_test_stem = test_stem[[\"question1\", \"question2\"]].apply(lambda x: shared_words_count(x[0], x[1]), axis = 1)\nshare_test_lem = test_lem[[\"question1\", \"question2\"]].apply(lambda x: shared_words_count(x[0], x[1]), axis = 1)\nshare_words_test = pd.concat([share_test_orig, share_test_stop, share_test_stem, share_test_lem], axis = 1)\nshare_words_test.columns = [\"share_words_count_orig\", \"share_words_rate_orig\", \"jaccard_orig\", \n                            \"share_words_count_stop\", \"share_words_rate_stop\", \"jaccard_stop\", \n                            \"share_words_count_stem\", \"share_words_rate_stem\", \"jaccard_stem\",\n                            \"share_words_count_lem\", \"share_words_rate_lem\", \"jaccard_lem\"]\ntest = pd.concat([test, share_words_test], axis = 1)\n\ndel share_train_orig, share_train_stop, share_train_stem, share_train_lem, share_words_train\ndel share_test_orig, share_test_stop, share_test_stem, share_test_lem, share_words_test","769648aa":"wordbag = pd.concat([train_orig[\"question1\"], train_orig[\"question2\"]], axis = 0)\ntfidf = TfidfVectorizer(analyzer = \"word\", stop_words = \"english\", lowercase = True)\ntfidf.fit(wordbag)\n\ndel wordbag","2bfb8b78":"tfidf_q1_train = tfidf.transform(train_orig[\"question1\"])\ntfidf_q2_train = tfidf.transform(train_orig[\"question2\"])\n\ndiff = tfidf_q1_train - tfidf_q2_train\ndiff_tfidf_L1_train = np.sum(np.abs(diff), axis = 1)  # \u7edf\u4e00\u7528numpy\u7684\u51fd\u6570\u6bd4\u8f83\u597d\ndiff_tfidf_L2_train = np.sum(diff.multiply(diff), axis = 1)\ndiff_tfidf_L1_norm_train = 2 * np.array(np.sum(np.abs(diff), axis = 1)) \/ pd.DataFrame(count1 + count2).values\ndiff_tfidf_L2_norm_train = 2 * np.array(np.sum(diff.multiply(diff), axis = 1)) \/ pd.DataFrame(count1 + count2).values\n# tfidf_q1_train\u548ctfidf_q2_train\uff0c\u4ee5\u53cadiff\u90fd\u662f\u7a00\u758f\u77e9\u9635\n# \u8f6c\u6362\u6210\u6570\u7ec4\u518d\u505a\u5bf9\u5e94\u5143\u7d20\u7684\u8fd0\u7b97\u5c06\u4f1a\u62a5\u9519\uff0c\u53ef\u4ee5\u7528matrix\u5bf9\u8c61\u81ea\u5e26\u7684\u65b9\u6cd5multiply\u5b9e\u73b0\ncos_tfidf_train = np.sum(tfidf_q1_train.multiply(tfidf_q2_train), axis = 1)  # \u7531\u4e8e\u8bcd\u7684tfidf\u8868\u793a\u662f\u7ecf\u8fc7\u6807\u51c6\u5316\u7684\uff0c\u6240\u4ee5\u5185\u79ef\u5373\u4e3a\u5939\u89d2\u4f59\u5f26\u503c\n\ntrain[\"diff_tfidf_L1\"] = diff_tfidf_L1_train\ntrain[\"diff_tfidf_L2\"] = diff_tfidf_L2_train\ntrain[\"diff_tfidf_L1_norm\"] = diff_tfidf_L1_norm_train\ntrain[\"diff_tfidf_L2_norm\"] = diff_tfidf_L2_norm_train\ntrain[\"cos_tfidf\"] = cos_tfidf_train\n\ndel tfidf_q1_train, tfidf_q2_train, diff, diff_tfidf_L1_train, diff_tfidf_L2_train\ndel diff_tfidf_L1_norm_train, diff_tfidf_L2_norm_train, cos_tfidf_train","30fcb3e3":"tfidf_q1_test = tfidf.transform(test_orig[\"question1\"])\ntfidf_q2_test = tfidf.transform(test_orig[\"question2\"])\n\ndiff = tfidf_q1_test - tfidf_q2_test\ndiff_tfidf_L1_test = np.sum(np.abs(diff), axis = 1)\ndiff_tfidf_L2_test = np.sum(diff.multiply(diff), axis = 1)\ndiff_tfidf_L1_norm_test = 2 * np.array(np.sum(np.abs(diff), axis = 1)) \/ pd.DataFrame(count3 + count4).values\ndiff_tfidf_L2_norm_test = 2 * np.array(np.sum(diff.multiply(diff), axis = 1)) \/ pd.DataFrame(count3 + count4).values\ncos_tfidf_test = np.sum(tfidf_q1_test.multiply(tfidf_q2_test), axis = 1)\n\ntest[\"diff_tfidf_L1\"] = diff_tfidf_L1_test\ntest[\"diff_tfidf_L2\"] = diff_tfidf_L2_test\ntest[\"diff_tfidf_L1_norm\"] = diff_tfidf_L1_norm_test\ntest[\"diff_tfidf_L2_norm\"] = diff_tfidf_L2_norm_test\ntest[\"cos_tfidf\"] = cos_tfidf_test\n\ndel tfidf_q1_test, tfidf_q2_test, diff, diff_tfidf_L1_test, diff_tfidf_L2_test\ndel diff_tfidf_L1_norm_test, diff_tfidf_L2_norm_test, cos_tfidf_test\ndel count1, count2, count3, count4","e9a551cf":"tr = pd.read_csv(\"..\/input\/quora-question-pairs\/train.csv\")\nte = pd.read_csv(\"..\/input\/quora-question-pairs\/test.csv\")\n\nques = pd.concat([tr[[\"question1\", \"question2\"]], te[[\"question1\", \"question2\"]]], \n                 axis = 0).reset_index(drop = \"index\")\nq_dict = defaultdict(set)\nfor i in range(ques.shape[0]):\n        q_dict[ques.question1[i]].add(ques.question2[i])\n        q_dict[ques.question2[i]].add(ques.question1[i])\n\ndef q1_q2_intersect(row):\n    return len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']])))\n\ntrain[\"q1_q2_intersect\"] = tr.apply(q1_q2_intersect, axis=1, raw=True)\ntest[\"q1_q2_intersect\"] = te.apply(q1_q2_intersect, axis=1, raw=True)\n\ndel ques","0286fcc6":"def q1_freq(row):\n    return(len(q_dict[row[\"question1\"]]))\ndef q2_freq(row):\n    return(len(q_dict[row[\"question2\"]]))\n\ntrain[\"q1_freq\"] = tr.apply(q1_freq, axis=1, raw=True)\ntrain[\"q2_freq\"] = tr.apply(q2_freq, axis=1, raw=True)\ntrain[\"q1_q2_freq_average\"] = (train[\"q1_freq\"] + train[\"q2_freq\"]) \/ 2\n\ntest[\"q1_freq\"] = te.apply(q1_freq, axis=1, raw=True)\ntest[\"q2_freq\"] = te.apply(q2_freq, axis=1, raw=True)\ntest[\"q1_q2_freq_average\"] = (test[\"q1_freq\"] + test[\"q2_freq\"]) \/ 2\n\ndel tr, te, q_dict","043ffdc7":"def same_start_word(row):\n    wordlist1 = word_tokenize(row[\"question1\"])\n    wordlist2 = word_tokenize(row[\"question2\"])\n    if wordlist1 and wordlist2:  # \u4e3a\u4e86\u9632\u6b62question1\u6216question2\u662f\u53ea\u5305\u542b\u5206\u9694\u7b26\u7684\u7a7a\u95ee\u9898\n        return int(wordlist1[0] == wordlist2[0])\n    else:\n        return 0\n\ntrain[\"same_start_word\"] = train_orig.apply(same_start_word, axis = 1)\ntest[\"same_start_word\"] = test_orig.apply(same_start_word, axis = 1)","d47f41ca":"sentiment_analyzer = SentimentIntensityAnalyzer()\ndef sentiment_analyze(row):\n    sen1 = sentiment_analyzer.polarity_scores(row[\"question1\"])\n    sen2 = sentiment_analyzer.polarity_scores(row[\"question2\"])\n    diff_neg = np.abs(sen1[\"neg\"] - sen2[\"neg\"])\n    diff_neu = np.abs(sen1[\"neu\"] - sen2[\"neu\"])\n    diff_pos = np.abs(sen1[\"pos\"] - sen2[\"pos\"])\n    diff_com = np.abs(sen1[\"compound\"] - sen2[\"compound\"])\n    return pd.Series([diff_neg, diff_neu, diff_pos, diff_com])","5422e596":"sen_train = train_orig.apply(sentiment_analyze, axis = 1)\nsen_train.columns = [\"diff_sen_neg\", \"diff_sen_neu\", \"diff_sen_pos\", \"diff_sen_com\"]\ntrain = pd.concat([train, sen_train], axis = 1)\n\nsen_test = test_orig.apply(sentiment_analyze, axis = 1)\nsen_test.columns = [\"diff_sen_neg\", \"diff_sen_neu\", \"diff_sen_pos\", \"diff_sen_com\"]\ntest = pd.concat([test, sen_test], axis = 1)\n\ndel sen_train, sen_test","048878c7":"train.to_csv(\"train.csv\", index = False)\ntest.to_csv(\"test.csv\", index = False)\ntrainlabel.to_csv(\"trainlabel.csv\", index = False)","6ec80285":"## TF-IDF\u76f8\u4f3c\u5ea6\n\n\u672c\u6587\u7528\u95ee\u9898\u7684TF-IDF\u5411\u91cf\u7684\u5dee\u76841-\u8303\u6570\u548c2-\u8303\u6570\uff0c\u4ee5\u53ca\u5939\u89d2\u4f59\u5f26\u503c\u8861\u91cf\u95ee\u9898\u5bf9\u7684TF-IDF\u76f8\u4f3c\u5ea6\u3002\n\n\u82e5\u4ec5\u7edf\u8ba1\u8bad\u7ec3\u96c6\u4e0a\u5355\u8bcd\u7684\u8bcd\u9891\uff0c\u90a3\u4e48\u5f53\u6d4b\u8bd5\u96c6\u4e2d\u51fa\u73b0\u8bad\u7ec3\u96c6\u4e0a\u4ece\u672a\u51fa\u73b0\u8fc7\u7684\u5355\u8bcd\u65f6\uff0cTF-IDF\u7684\u8ba1\u7b97\u4fbf\u4f1a\u51fa\u73b0\u95ee\u9898\u3002Scikit-Learn\u4e2d\u7684tfidf.transform\u51fd\u6570\u4f1a\u628a\u5176\u9891\u6570\u4ece0\u8c03\u6574\u81f31\uff0c\u6362\u8a00\u4e4b\uff0c\u8ba4\u4e3a\u8be5\u8bcd\u81f3\u5c11\u51fa\u73b0\u4e86\u4e00\u6b21\uff0c\u7136\u540e\u6b63\u5e38\u5c55\u5f00\u8ba1\u7b97\u3002\u53c8\u56e0\u4e3a\u6d4b\u8bd5\u96c6\u4e2d\u7684\u95ee\u9898\u5305\u542b\u673a\u5668\u751f\u6210\u7684\u5e72\u6270\u9879\uff0c\u4f1a\u4f7f\u8bcd\u9891\u7684\u4f30\u8ba1\u4ea7\u751f\u504f\u5dee\uff0c\u56e0\u6b64\u7edf\u8ba1\u8bcd\u9891\u65f6\u4e0d\u5e94\u628a\u6d4b\u8bd5\u96c6\u4e2d\u7684\u95ee\u9898\u4e5f\u5305\u542b\u5728\u5185\u3002","2111b422":"\u7b2c\u4e00\u6b65\u5bfc\u5165\u6570\u636e\u6e05\u6d17\u6240\u5f97\u5230\u7684\u8bad\u7ec3\u96c6\u3002","ef0cd3a3":"## \u5171\u4eab\u5355\u8bcd\u4e2a\u6570\/\u6bd4\u4f8b\u4e0eJaccard\u76f8\u4f3c\u5ea6\n\n\u7531\u4e8e\u672c\u6587\u8ba4\u4e3a\u8fd9\u4e09\u4e2a\u7279\u5f81\u8f83\u4e3a\u91cd\u8981\uff0c\u56e0\u6b64\u672c\u6587\u5bf9\u56db\u79cd\u8bad\u7ec3\u96c6\u90fd\u8fdb\u884c\u4e86\u8ba1\u7b97\u3002","7b32e36a":"## \u60c5\u611f\u76f8\u4f3c\u5ea6\n\n\u60c5\u611f\u4e5f\u662f\u5206\u6790\u8bed\u4e49\u7684\u91cd\u8981\u4e00\u4e2a\u7ef4\u5ea6\u3002NLTK\u4e2d\u6709\u51fd\u6570SentimentIntensityAnalyzer\u53ef\u4ee5\u5bf9\u5b57\u7b26\u4e32\u7684\u60c5\u611f\u505a\u7b80\u8981\u7684\u8bc4\u5206\uff0c\u5176\u4e2dneg\u4ee3\u8868\u8d1f\u9762\u611f\u60c5\uff0cneu\u4ee3\u8868\u4e2d\u6027\uff0cpos\u4ee3\u8868\u6b63\u9762\uff0ccompound\u4ee3\u8868\u60c5\u611f\u590d\u6742\u5ea6\u3002\u5bf9\u95ee\u9898\u5bf9\u7684\u60c5\u611f\u8bc4\u5206\u4f5c\u5dee\uff0c\u53ef\u4ee5\u5f97\u5230\u65b0\u7684\u7279\u5f81\u3002","99a6d4d5":"## \u7b2c\u4e00\u4e2a\u5355\u8bcd\u7684\u5dee\u5f02\n\n\u95ee\u9898\u7684\u7b2c\u4e00\u4e2a\u5355\u8bcd\u901a\u5e38\u90fd\u662f\u7591\u95ee\u526f\u8bcd\uff0c\u5982what,when,how,which\uff0c\u4ed6\u4eec\u90fd\u6709\u660e\u663e\u4e0d\u540c\u7684\u542b\u4e49\u3002","1a5c91c2":"\u7b2c\u4e8c\u90e8\u5206\u7684\u7279\u5f81\u63d0\u53d6\u8be6\u89c1[Quora Question Pairs: Feature Extraction 2](https:\/\/www.kaggle.com\/benjaminkz\/quora-question-pairs-feature-extraction-2)\u3002","b7819818":"## \u95ee\u9898\u51fa\u73b0\u9891\u6570\n\n\u88ab\u63d0\u95ee\u6b21\u6570\u8d8a\u591a\u7684\u95ee\u9898\uff0c\u5219\u8d8a\u6709\u53ef\u80fd\u662f\u91cd\u590d\u7684\u95ee\u9898\u3002","6684aebf":"\u5b9e\u6d4b\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u90e8\u5206\u6837\u672c\u7684question1\u548cquestion2\u7279\u5f81\u53d8\u4e3a\u53ea\u542b\u4e00\u4e2a\u7a7a\u683c\u7684\u5b57\u7b26\u4e32\uff0c\u8f93\u51fa\u518d\u8bfb\u53d6\u4f1a\u88ab\u89c6\u4e3aNaN\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u91cd\u65b0\u7528\u7a7a\u5b57\u7b26\u4e32\u586b\u5145\u3002","e714bdd8":"## \u5355\u8bcd\u603b\u6570\u5dee\u5f02\u4e0e\u5dee\u5f02\u6bd4\u4f8b","c9176ff9":"## \u95ee\u9898\u5bf9\u7684\u8def\u5f84\u6570\u91cf\n\n\u5229\u7528\u56fe\u7684\u89c2\u70b9\u5206\u6790\u95ee\u9898\u4e4b\u95f4\u7684\u8054\u7cfb\u3002\u5c06\u6bcf\u4e00\u4e2a\u95ee\u9898\u89c6\u4e3a\u4e00\u4e2a\u7ed3\u70b9\uff0c\u82e5\u4e24\u4e2a\u95ee\u9898\u51fa\u73b0\u5728\u540c\u4e00\u4e2a\u6837\u672c\u4e2d\uff0c\u5219\u4e3a\u76f8\u5e94\u7ed3\u70b9\u6dfb\u52a0\u4e00\u6761\u8fb9\uff0c\u5982\u662f\u53ef\u4ee5\u5f97\u5230\u4e00\u5f20\u65e0\u5411\u56fe\u3002\u5982\u679c\u95ee\u9898\u5bf9\u7684\u540c\u65f6\u51fa\u73b0\u4e0d\u662f\u5076\u7136\u9020\u6210\u7684\uff0c\u90a3\u4e48\u95ee\u9898\u5bf9\u4e4b\u95f4\u7684\u8fde\u901a\u8def\u5f84\u8d8a\u591a\uff0c\u5219\u8054\u7cfb\u8d8a\u7d27\u5bc6\uff0c\u7edf\u8ba1\u6bcf\u4e2a\u95ee\u9898\u5bf9\u7684\u8def\u5f84\u6570\u91cf\u53ef\u4ee5\u5f97\u5230\u4e00\u4e2a\u65b0\u7684\u7279\u5f81\u3002\u56ff\u4e8e\u7b97\u6cd5\u590d\u6742\u5ea6\u9650\u5236\uff0c\u4e0b\u9762\u7684\u4ee3\u7801\u4ec5\u7edf\u8ba1\u4e86\u901a\u8fc7\u4e00\u4e2a\u7ed3\u70b9\u7684\u8def\u5f84\u6570\u91cf\u3002","80d14cea":"\u6240\u6709\u751f\u6210\u7684\u7279\u5f81\u5b58\u653e\u5728train\u548ctest\u4e2d\u3002","69fac896":"# 2.1 \u7279\u5f81\u63d0\u53d6\u4e00\n\n\u7279\u5f81\u63d0\u53d6\u5206\u6210\u4e24\u4e2a\u90e8\u5206\u8fdb\u884c\uff0c\u8fd9\u6837\u505a\u662f\u56e0\u4e3a\u5bfc\u5165\u8bad\u7ec3\u597d\u7684GloVe\u540e\u5185\u5b58\u4e25\u91cd\u7d27\u5f20\u3002\u7b2c\u4e00\u90e8\u5206\u63d0\u53d6\u4e869\u5927\u7c7b\uff0c\u603b\u517130\u4e2a\u96f6\u6563\u7684\u7279\u5f81\uff0c\u8d39\u65f6\u7ea63h\u3002\u7b2c\u4e8c\u90e8\u5206\u63d0\u53d6GloVe\u76f8\u5173\u7684\u7279\u5f81\u3002\n\n- \u5b57\u6bcd\u603b\u6570\u5dee\u5f02\/\u5b57\u6bcd\u603b\u6570\u5dee\u5f02\u6bd4\u4f8b\n- \u5355\u8bcd\u603b\u6570\u5dee\u5f02\/\u5355\u8bcd\u603b\u6570\u5dee\u5f02\u6bd4\u4f8b\n- \u5171\u4eab\u5355\u8bcd\u4e2a\u6570\/\u5171\u4eab\u5355\u8bcd\u6bd4\u4f8b\n- Jaccard\u76f8\u4f3c\u5ea6\n- TF-IDF\u76f8\u4f3c\u5ea6\n- \u95ee\u9898\u5bf9\u7684\u8def\u5f84\u6570\u91cf\n- \u95ee\u9898\u51fa\u73b0\u9891\u6570\n- \u7b2c\u4e00\u4e2a\u5355\u8bcd\u7684\u5dee\u5f02\n- \u60c5\u611f\u76f8\u4f3c\u5ea6","764ce4bd":"## \u5b57\u6bcd\u603b\u6570\u5dee\u5f02\u4e0e\u5dee\u5f02\u6bd4\u4f8b"}}