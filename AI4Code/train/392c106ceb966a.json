{"cell_type":{"eee18b23":"code","87258a1f":"code","07143d63":"code","d78cb46b":"code","6b0b02d5":"code","495f194b":"code","98097455":"code","6d61e72b":"code","24a781a5":"code","07b8e9ef":"code","7406ebab":"code","60776410":"code","ecb98218":"code","fe9e464b":"code","e26be560":"code","10704782":"code","ed239a6a":"code","ca8b4b63":"code","3e25fceb":"code","0184ec6c":"code","8b003dcc":"code","c844066c":"code","6042fba1":"code","0c739236":"code","1e0af406":"markdown","ab304ceb":"markdown","2260cc8d":"markdown","4290ddf1":"markdown","9c5e41e5":"markdown","2b5dce51":"markdown","c7c1edd7":"markdown","6e01f2cd":"markdown","cf9a0c1a":"markdown","ffc55087":"markdown","dea22b1c":"markdown"},"source":{"eee18b23":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","87258a1f":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n\nstop_words = set(stopwords.words('english'))\nsnowball_stemmer = SnowballStemmer(\"english\")","07143d63":"train_df = pd.read_csv(\"..\/input\/train.csv\")\n#test_df = pd.read_csv(\"..\/input\/test.csv\")\n","d78cb46b":"train_df.columns","6b0b02d5":"train_df.head(10)","495f194b":"train_df['target'] = [ 1 if target>=0.5 else 0 for target in train_df['target'] ]\n#train_df.head(10)","98097455":"train_df['target'].value_counts()","6d61e72b":"x_label = ('Toxic comments', 'Non-toxic comments')\ny_axis =[ value_count\/train_df.shape[0]*100 for value_count in train_df['target'].value_counts().tolist() ]\n#y_label = np.array(y_axis)\nbar_plot = plt.bar(x_label, y_axis)\nbar_plot[0].set_color('r')\nbar_plot[1].set_color('g')\nplt.ylabel('Total data range', fontsize=10)","24a781a5":"X = train_df['comment_text']\ny = train_df['target']","07b8e9ef":"import re\ndef pre_processing(X):\n    X = X.str.lower()\n    X = X.str.replace(r'\\r', ' ')\n    X = X.str.replace(r'\\n', ' ')\n    X = X.str.replace('[^a-zA-Z0-9 ]', '')\n    #X = re.sub('[^a-zA-Z0-9 \\n\\.]','', X)\n    return X","7406ebab":"X = pre_processing(X)","60776410":"def nlp_preprocessing(document): \n    words = [snowball_stemmer.stem(word) for word in document.split() \n                 if word not in stop_words]\n    doc = ' '.join(words)\n    return doc","ecb98218":"x = X.apply(lambda document: nlp_preprocessing(document))","fe9e464b":"vectorizer = TfidfVectorizer(min_df=0.01)\nx = vectorizer.fit_transform(x)","e26be560":"lencoder= LabelEncoder()\nenc_y = lencoder.fit_transform(y)","10704782":"import tensorflow as tf\nimport keras.backend as K\nfrom keras import layers, models, optimizers, regularizers\nfrom keras.layers import Bidirectional, LSTM\nfrom keras.models import Sequential","ed239a6a":"nlp_input = layers.Input((x.shape[1], ), sparse=True)\nhidden_layer_1 = layers.Dense(500, activation=\"relu\")(nlp_input)\n\nhidden_drop_1 = layers.Dropout(0.3)(hidden_layer_1)\noutput_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_drop_1)\n\nclassifier_1 = models.Model(inputs = nlp_input, outputs = output_layer)\nclassifier_1.compile(optimizer=optimizers.adam(lr=0.001, amsgrad=True), \n                   loss='binary_crossentropy', \n                   metrics=['accuracy'])","ca8b4b63":"from keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\nmc = ModelCheckpoint('model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n\n                       ","3e25fceb":"classifier_1.fit(x, enc_y, batch_size = 256, epochs = 10, callbacks=[es, mc], verbose=1  )","0184ec6c":"test_df = pd.read_csv(\"..\/input\/test.csv\")","8b003dcc":"test_X = pre_processing(test_df['comment_text'])\ntest_x = test_X.apply(lambda document: nlp_preprocessing(document))\ntest_x = vectorizer.transform(test_x)","c844066c":"y = classifier_1.predict(test_x)\ny = np.where(y>=0.5,1,0)\n","6042fba1":"sub_df = pd.read_csv('..\/input\/sample_submission.csv')\nsub_df['prediction'] = y","0c739236":"sub_df.to_csv('submission.csv',index = False)","1e0af406":"Label Encoding the target\/output variable:","ab304ceb":"Vecortizing the text feature using TF-IDF Vectorizer:","2260cc8d":"NLP Preprocessing:","4290ddf1":"Data Preprocesing:\n\nOn **comment_text** column:\n1. convert characters to lower\n2. remove numbers\n\nSpecial characters and repeated letters can also be removed.","9c5e41e5":"**Check the class imbalance:**\n\nSince we converted the target variable to binary class, we can easily identify the class counts.","2b5dce51":"Prediction for test data:","c7c1edd7":"Loading input data for analysis:","6e01f2cd":"Building Model:\n","cf9a0c1a":"Model for textual feature:","ffc55087":"**Convertion of target to Binary class:**\n\nFrom the Background information provided, target with value >=0.5 can be considered toxic","dea22b1c":"To get an idea about the data:"}}