{"cell_type":{"c5a77b10":"code","118db0db":"code","62b6accc":"code","78ddd024":"code","a5fac881":"code","f517ce41":"code","d2adefc6":"code","05936d31":"code","e074de03":"code","76885ec4":"code","4468eaff":"code","200e321c":"code","0c426781":"code","7a6562f2":"code","a0052438":"code","af67883f":"code","61bbe67a":"code","a7a7710d":"code","7bbad4d9":"code","80dc2b8c":"code","a26a0dfa":"code","1e3161c8":"code","2116768b":"code","61c151bc":"code","e47fee2e":"code","843debb1":"code","02e6e269":"code","026592af":"code","9a3a66c2":"code","a13c9791":"code","94d2f9e0":"code","793c62d5":"code","5866a980":"code","e1aed9fd":"code","12dcdb40":"code","3cd45bdd":"code","0352d6b9":"code","f5dcb168":"code","443f2c41":"code","b65cb973":"code","e37ab736":"code","ba15bea6":"code","5042e22e":"code","de00f2c3":"code","82faaa6a":"code","66cc41b5":"code","dee1dceb":"code","148ae145":"code","52934667":"code","b237cef7":"code","e47a1c37":"code","967b67b0":"code","1b967226":"code","c76b14bd":"code","f79ed534":"code","a584ea40":"code","80eff7a1":"code","9dcdacf1":"code","3ef906d5":"code","f883a397":"code","35ddb5a4":"code","66069bb5":"code","b9196097":"code","d2a6de02":"code","5bda1499":"code","fb823ca9":"code","d94629c6":"code","ca219821":"code","6b3d7ea6":"code","e2a356a5":"code","614718ba":"code","19013544":"code","26feedc4":"code","8c674d1a":"code","f643f189":"code","67683af3":"code","b827b0d6":"code","11239b8f":"code","645a0e4b":"code","fab423a8":"markdown","d4f98713":"markdown","4868eb72":"markdown","f46bf146":"markdown","58f5265f":"markdown","9327c81d":"markdown","90d8000c":"markdown","17b69fe9":"markdown","fd47773b":"markdown","6964572f":"markdown","14e615cd":"markdown","ed994c8e":"markdown","4e37db83":"markdown","e0807d14":"markdown","2f38281f":"markdown","21a33715":"markdown","f862e01c":"markdown","2498afdb":"markdown","1b9e99a9":"markdown","b074aa4c":"markdown","27bbe620":"markdown","90a5bfc9":"markdown","c53dc54b":"markdown"},"source":{"c5a77b10":"import os\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 10)\npd.set_option('display.width', 1000)","118db0db":"from plotly.offline import init_notebook_mode, plot, iplot\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)","62b6accc":"train_df = pd.read_csv('..\/input\/restaurantrecommendationdata\/train_100k.csv')","78ddd024":"test_df = pd.read_csv('..\/input\/restaurantrecommendationdata\/test_100k.csv')","a5fac881":"print(train_df.shape)\nprint(test_df.shape)","f517ce41":"train_df.restaurent_rating = train_df.restaurent_rating.astype('float')\ntest_df.restaurent_rating = test_df.restaurent_rating.astype('float')","d2adefc6":"train_df.head()","05936d31":"test_df.head()","e074de03":"train_df.info()","76885ec4":"test_df.info()","4468eaff":"train_explicit = train_df[['customer_id','restaurant_id','restaurent_rating']]\ntrain_implicit = train_df[['customer_id','restaurant_id','restaurent_rating','restaurent_tag_name']]","200e321c":"test_explicit = test_df[['customer_id','restaurant_id','restaurent_rating']]\ntest_implicit = test_df[['customer_id','restaurant_id','restaurent_rating','restaurent_tag_name']]","0c426781":"train_explicit.head()","7a6562f2":"train_implicit.head()","a0052438":"train_explicit.restaurant_id.value_counts().sort_values(ascending=True)[:10]","af67883f":"train_explicit.customer_id.value_counts().sort_values(ascending=False)[:10]","61bbe67a":"train_explicit.restaurent_rating.value_counts()","a7a7710d":"test_explicit.restaurant_id = test_explicit.restaurant_id.astype('int64')","7bbad4d9":"test_explicit.restaurant_id.value_counts()[:10]","80dc2b8c":"test_explicit.customer_id.value_counts()[:10]","a26a0dfa":"test_explicit.restaurent_rating.value_counts()","1e3161c8":"rating_average = train_explicit.groupby('restaurant_id')['restaurent_rating'].mean()\nrating_count = train_explicit.groupby('restaurant_id')['restaurent_rating'].count()","2116768b":"rating_average = rating_average.to_frame()\nrating_average.rename(columns={'restaurent_rating':'rating_average'}, inplace= True)","61c151bc":"rating_count = rating_count.to_frame()\nrating_count.rename(columns={'restaurent_rating':'rating_count'}, inplace= True)","e47fee2e":"df2= rating_average.merge(rating_count,on='restaurant_id')","843debb1":"C= df2['rating_average'].mean()\nC","02e6e269":"m= df2['rating_count'].quantile(0.3)\nm","026592af":"restaurants = df2.copy().loc[df2['rating_count'] >= m]\nrestaurants.shape","9a3a66c2":"def weighted_rating(x, m=m, C=C):\n    v = x['rating_count']\n    R = x['rating_average']\n    return (v\/(v+m) * R) + (m\/(m+v) * C)","a13c9791":"restaurants['score'] = restaurants.apply(weighted_rating, axis=1)","94d2f9e0":"restaurants = restaurants.sort_values('score', ascending=False)\nrestaurants.index.to_list()[:5]","793c62d5":"print('>> Importing Libraries')\n\nimport pandas as pd\n\nfrom surprise import Reader, Dataset, SVD\n\nfrom surprise.accuracy import rmse, mae\nfrom surprise.model_selection import cross_validate\n\nprint('>> Libraries imported.')","5866a980":"train_explicit.isna().sum()","e1aed9fd":"n_restaurants = train_explicit['restaurant_id'].nunique()\nn_customers = train_explicit['customer_id'].nunique()\nprint(f'Number of unique restaurants: {n_restaurants}')\nprint(f'Number of unique n_customers: {n_customers}')","12dcdb40":"available_ratings = train_explicit['restaurent_rating'].count()\ntotal_ratings = n_restaurants*n_customers\nmissing_ratings = total_ratings - available_ratings\nsparsity = (missing_ratings\/total_ratings) * 100\nprint(f'Sparsity: {sparsity}')","3cd45bdd":"train_explicit['restaurent_rating'].value_counts().plot(kind='bar')","0352d6b9":"filter_restaurants = train_explicit['restaurant_id'].value_counts() > 5\nfilter_restaurants = filter_restaurants[filter_restaurants].index.tolist()","f5dcb168":"filter_customers = train_explicit['customer_id'].value_counts() > 5\nfilter_customers = filter_customers[filter_customers].index.tolist()","443f2c41":"print(f'Original shape: {train_explicit.shape}')\ndf = train_explicit[(train_explicit['restaurant_id'].isin(filter_restaurants)) & (train_explicit['customer_id'].isin(filter_customers))]\nprint(f'New shape: {df.shape}')","b65cb973":"df.head()","e37ab736":"cols = ['customer_id','restaurant_id','restaurent_rating']","ba15bea6":"reader = Reader(rating_scale = (0.5, 5))\ndata = Dataset.load_from_df(df[cols], reader)","5042e22e":"trainset = data.build_full_trainset()\nantiset = trainset.build_anti_testset()","de00f2c3":"algo = SVD(n_epochs =25, verbose = True)","82faaa6a":"cross_validate(algo, data, measures = ['RMSE', 'MAE'], cv=5, verbose= True)\nprint('>> Training Done')","66cc41b5":"predictions = algo.test(antiset)","dee1dceb":"predictions[0]","148ae145":"from collections import defaultdict\ndef get_top_n(predictions, n=5):\n    top_n = defaultdict(list)\n    for uid, iid, _, est, _ in predictions:\n        top_n[uid].append((iid, est))\n        \n    for uid, user_ratings in top_n.items():\n        user_ratings.sort(key = lambda x: x[1], reverse = True)\n        top_n[uid] = user_ratings[:n]\n        \n    return top_n\n\ntop_n = get_top_n(predictions, n=5)\n\nfor uid, user_ratings in top_n.items():\n    print(uid, [iid for (iid, rating) in user_ratings])","52934667":"from surprise import Reader\nfrom surprise import Dataset\nfrom surprise.model_selection import cross_validate\nfrom surprise import NormalPredictor\nfrom surprise import KNNBasic\nfrom surprise import KNNWithMeans\nfrom surprise import KNNWithZScore\nfrom surprise import KNNBaseline\nfrom surprise import SVD\nfrom surprise import BaselineOnly\nfrom surprise import SVDpp\nfrom surprise import NMF\nfrom surprise import SlopeOne\nfrom surprise import CoClustering\nfrom surprise.accuracy import rmse\nfrom surprise import accuracy\nfrom surprise.model_selection import train_test_split\nfrom surprise.model_selection import GridSearchCV","b237cef7":"df = train_explicit","e47a1c37":"data = df['restaurent_rating'].value_counts().sort_index(ascending=False)\ntrace = go.Bar(x = data.index,\n               text = ['{:.1f} %'.format(val) for val in (data.values \/ df.shape[0] * 100)],\n               textposition = 'auto',\n               textfont = dict(color = '#000000'),\n               y = data.values,\n               )\n# Create layout\nlayout = dict(title = 'Distribution Of {} restaurant-ratings'.format(df.shape[0]),\n              xaxis = dict(title = 'Rating'),\n              yaxis = dict(title = 'Count'))\n# Create plot\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)","967b67b0":"data = df.groupby('restaurant_id')['restaurent_rating'].count().clip(lower=5000)\n\n# Create trace\ntrace = go.Histogram(x = data.values,\n                     name = 'Ratings',\n                     xbins = dict(start = 5000,\n                                  end = 6000,\n                                  size = 100))\n# Create layout\nlayout = go.Layout(title = 'Distribution Of Number of Ratings Per restaurant',\n                   xaxis = dict(title = 'Number of Ratings Per Restaurant'),\n                   yaxis = dict(title = 'Count'),\n                   bargap = 0.2)\n\n# Create plot\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)","1b967226":"df.groupby('restaurant_id')['restaurent_rating'].count().reset_index().sort_values('restaurent_rating', ascending=False)[:10]","c76b14bd":"data = df.groupby('customer_id')['restaurent_rating'].count()\n\n# Create trace\ntrace = go.Histogram(x = data.values,\n                     name = 'Ratings',\n                     xbins = dict(start = 0,\n                                  end = 50,\n                                  size = 2))\n# Create layout\nlayout = go.Layout(title = 'Distribution Of Number of Ratings Per Customer',\n                   xaxis = dict(title = 'Ratings Per Customer'),\n                   yaxis = dict(title = 'Count'),\n                   bargap = 0.2)\n\n# Create plot\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)","f79ed534":"df.groupby('customer_id')['restaurent_rating'].count().reset_index().sort_values('restaurent_rating', ascending=False)[:10]","a584ea40":"filter_restaurants = train_explicit['restaurant_id'].value_counts() > 5\nfilter_restaurants = filter_restaurants[filter_restaurants].index.tolist()","80eff7a1":"filter_customers = train_explicit['customer_id'].value_counts() > 5\nfilter_customers = filter_customers[filter_customers].index.tolist()","9dcdacf1":"print(f'Original shape: {train_explicit.shape}')\ndf = train_explicit[(train_explicit['restaurant_id'].isin(filter_restaurants)) & (train_explicit['customer_id'].isin(filter_customers))]\nprint(f'New shape: {df.shape}')","3ef906d5":"reader = Reader(rating_scale=(0.5, 5))\ndata = Dataset.load_from_df(df[['customer_id', 'restaurant_id', 'restaurent_rating']], reader)","f883a397":"benchmark = []\nfor algorithm in [SVD(), SVDpp(), SlopeOne(), NMF(), NormalPredictor(), KNNBaseline(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), BaselineOnly(), CoClustering()]:\n    results = cross_validate(algorithm, data, measures=['RMSE'], cv=3, verbose=False)\n    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n    benchmark.append(tmp)\n    \npd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse')  ","35ddb5a4":"print('Using ALS')\nbsl_options = {'method': 'als',\n               'n_epochs': 5,\n               'reg_u': 12,\n               'reg_i': 5}\nalgo = BaselineOnly(bsl_options=bsl_options)\ncross_validate(algo, data, measures=['RMSE'], cv=5, verbose=False)","66069bb5":"trainset, testset = train_test_split(data, test_size=0.25)\nalgo = BaselineOnly(bsl_options=bsl_options)\npredictions = algo.fit(trainset).test(testset)\naccuracy.rmse(predictions)","b9196097":"\ndef get_Iu(uid):\n    \"\"\" return the number of items rated by given user\n    args: \n      uid: the id of the user\n    returns: \n      the number of items rated by the user\n    \"\"\"\n    try:\n        return len(trainset.ur[trainset.to_inner_uid(uid)])\n    except ValueError: # user was not part of the trainset\n        return 0\n    \ndef get_Ui(iid):\n    \"\"\" return number of users that have rated given item\n    args:\n      iid: the raw id of the item\n    returns:\n      the number of users that have rated the item.\n    \"\"\"\n    try: \n        return len(trainset.ir[trainset.to_inner_iid(iid)])\n    except ValueError:\n        return 0\n    \ndf = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details'])\ndf['Iu'] = df.uid.apply(get_Iu)\ndf['Ui'] = df.iid.apply(get_Ui)\ndf['err'] = abs(df.est - df.rui)\nbest_predictions = df.sort_values(by='err')[:10]\nworst_predictions = df.sort_values(by='err')[-10:]","d2a6de02":"best_predictions","5bda1499":"worst_predictions","fb823ca9":"df = train_implicit[(train_implicit['restaurant_id'].isin(filter_restaurants)) & (train_implicit['customer_id'].isin(filter_customers))]","d94629c6":"df = df.sample(n=10000)","ca219821":"df['restaurent_tag_name'] = df['restaurent_tag_name'].apply(lambda x: x.replace(',',' '))","6b3d7ea6":"df['restaurent_tag_name'].head()","e2a356a5":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(stop_words='english')\ndf['restaurent_tag_name'] = df['restaurent_tag_name'].fillna('')\ntfidf_matrix = tfidf.fit_transform(df['restaurent_tag_name'])\ntfidf_matrix.shape","614718ba":"from sklearn.feature_extraction.text import CountVectorizer\n\ncount = CountVectorizer(stop_words='english')\ndf['restaurent_tag_name'] = df['restaurent_tag_name'].fillna('')\ncount_matrix = count.fit_transform(df['restaurent_tag_name'])\ncount_matrix.shape","19013544":"from sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics.pairwise import linear_kernel","26feedc4":"cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)","8c674d1a":"cosine_sim2 = cosine_similarity(count_matrix, count_matrix)","f643f189":"df = df.reset_index()\nindices = pd.Series(df.index, index=df['restaurent_tag_name']).drop_duplicates()","67683af3":"indices.index.unique()","b827b0d6":"def get_recommendations(variable,indices,df,cosine_sim):\n    idx = indices[variable]\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1].all(), reverse=True)\n    sim_scores = sim_scores[1:5]\n    df_indices = [i[0] for i in sim_scores]\n    return df['restaurant_id'].iloc[df_indices]","11239b8f":"get_recommendations('Asian Dimsum Grills Japanese Rice Soups',indices,df,cosine_sim)","645a0e4b":"get_recommendations('Asian Dimsum Grills Japanese Rice Soups',indices,df,cosine_sim2)","fab423a8":"## Trending Now\nTop 5 trending restaurant to recommend","d4f98713":"### Ratings Distribution","4868eb72":"### Dimensionality Reduction","f46bf146":"# Recommender System\n\nWe are dealing with three entities: **user**, **restaurant** and **food item**. As our system is growing exponentially with this large amount of data (x*y*z where x=# of users, y= # of restaurants, z= # of food items), to make it more efficient, we are incorporating recommendation as a type of information filtering system in a sense to provide more relevant and personalized information to the user based on their interests and preferences.\n\n> Our system aims to make recommendations for \n    - Trending food items or restaurants based on demographic filtering\n    - Similar food items or restaurants based on content based filtering\n    - Personalized food items or restaurants based on collaborative filtering\n\n#### Final system developed in this AI programm\nIn this prototype development program we have used 2 datasets one is from Kaggle (Restaurant recommendation challenge) and another is Yelp open dataset. With this we have developed a recommendation engine for 3 different types of recommendations: demographic filtering (Explicit), content based filtering (implicit) and collaborative filtering (Explicit).\n\n#### Future works and improvement on the developed system\nWe have been able to make recommendations based on 3 approaches: demographic filtering (Explicit), content based filtering (implicit) and collaborative filtering (Explicit) where are facing challenges with lacks of data quality, data sparsity and being a new system not all food items, businesses and users being rated. In future we have to solve these problems and have a plan to make a hybrid recommendation engine with explicit(ratings and reviews) and implicit(sessions, purchases, views, clicks) features and latent factors(feature engineered events)\n\n","58f5265f":"# Demographic Filtering","9327c81d":"### Recommending top 5 restaurents based on predictions","90d8000c":"# Contentbased Filtering","17b69fe9":"### Create surprise dataset","fd47773b":"## Utility Codes\n```\ntrain_df.dropna()\ntest_df.dropna()\n\ntrain_df = train_df.sample(n=100000)\ntest_df = test_df.sample(n=100000)\n\ntrain_df.to_csv('train_100k.csv', sep=',', encoding='utf-8', index=False)\ntest_df.to_csv('test_50k.csv', sep=',', encoding='utf-8', index=False)\n\ncolumns = ['customer_id','gender','latitude_x','longitude_x','latitude_y','longitude_y','vendor_category_en','delivery_charge','serving_distance','commission','is_akeed_delivering','discount_percentage','language','rank','vendor_rating','vendor_tag_name','id_obj']\ntrain_df_sf = train_df[columns]\ntest_df_sf = test_df[columns]\n\ntrain_df_sf.rename(columns={'is_akeed_delivering':'delivery_available','vendor_rating':'restaurent_rating','vendor_tag_name':'restaurent_tag_name','id_obj':'restaurant_id'}, inplace=True)\ntest_df_sf.rename(columns={'is_akeed_delivering':'delivery_available','vendor_rating':'restaurent_rating','vendor_tag_name':'restaurent_tag_name','id_obj':'restaurant_id'}, inplace=True)\n\ndef clean_string(string):\n    string = str(string)\n    if '?' in string or string.lower()=='nan' or string.strip(' ')=='':\n        return np.nan\n    string = string.strip(' ').lower()\n    return string\n\ntrain_df_sf.loc[:, 'gender'] = train_df_sf['gender'].apply(clean_string)\ntest_df_sf.loc[:, 'gender'] = test_df_sf['gender'].apply(clean_string)\n\nimport re\ndef check_num(string):\n    if string is None:\n        return float(0)\n    string = str(string)\n    regex = r'-?[0-9]*.[0-9]*'\n    m = re.match(regex, string)\n    if m is None:\n        return float(0)\n    try:\n        return float(string[:6])\n    except:\n        return float(0)\n        \ntrain_df_sf.loc[:,'latitude_x'] = train_df_sf['latitude_x'].apply(check_num)\ntrain_df_sf.loc[:,'longitude_x'] = train_df_sf['longitude_x'].apply(check_num)\n\ntrain_df_sf.loc[:,'latitude_y'] = train_df_sf['latitude_y'].apply(check_num)\ntrain_df_sf.loc[:,'longitude_y'] = train_df_sf['longitude_y'].apply(check_num)\n\ntest_df_sf.loc[:,'latitude_x'] = test_df_sf['latitude_x'].apply(check_num)\ntest_df_sf.loc[:,'longitude_x'] = test_df_sf['longitude_x'].apply(check_num)\n\ntest_df_sf.loc[:,'latitude_y'] = test_df_sf['latitude_y'].apply(check_num)\ntest_df_sf.loc[:,'longitude_y'] = test_df_sf['longitude_y'].apply(check_num)\n```","6964572f":"### Number of restaurants\/customers","14e615cd":"### Sparsity of our data\nSparsity (%) = (No of missing values\/ (Total Values))X100","ed994c8e":"### Check for Missing Data","4e37db83":"## Predictions","e0807d14":"### Create Train-set and Prediction-set","2f38281f":"### Creating the model","21a33715":"### Training the model","f862e01c":"## EDA (Exploratory data analysis)","2498afdb":"### Importing Libraries","1b9e99a9":"## Creating and training the model","b074aa4c":"### Ratings Distribution","27bbe620":"## EDA","90a5bfc9":"# Approach 2: Collaborative Filtering","c53dc54b":"# Approach 1 : Collaborative Filtering"}}