{"cell_type":{"5e287675":"code","10398736":"code","a4107a9b":"code","2fa78c90":"code","b3472f60":"code","0e7ac751":"code","a741b70d":"code","97827362":"code","f69b63f9":"code","caa731cc":"code","aaa107a2":"code","d9c3a7b1":"markdown","c198c37c":"markdown","9df55d04":"markdown","e53ec246":"markdown","b7e204ab":"markdown","af52a2ea":"markdown","df5fe562":"markdown"},"source":{"5e287675":"!pip install transformers\nimport transformers","10398736":"import os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport random\nimport re\nimport time\nimport warnings\nimport string\n\nimport tensorflow_hub as hub\nfrom tqdm.notebook import tqdm\nimport tensorflow as tf\nfrom keras import backend as K\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras.layers import Dense, Input, Conv1D, GlobalMaxPooling1D, Dropout, BatchNormalization, Average\nfrom tensorflow.keras.activations import sigmoid\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import AUC\nfrom tensorflow.keras.regularizers import l1, l2\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.svm import SVC\n\npd.set_option('max_colwidth', 500)\npd.set_option('display.float_format', lambda x: '%.4f' % x)\n\n\ntf.random.set_seed(42)\nrandom.seed(42)\n\nwarnings.filterwarnings(\"ignore\")","a4107a9b":"import pandas as pd\ntrain = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n\nids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\ntrain.loc[train['id'].isin(ids_with_target_error),'target'] = 0","2fa78c90":"%%time\n\nfrom transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)\n\n\ndef greed_encode(data, max_len) :\n    input_ids = []\n    attention_masks = []\n  \n    for i in range(len(data.text)):\n        \n        encoded = tokenizer.encode_plus(data.text[i], add_special_tokens=True, max_length=max_len, pad_to_max_length=True)\n         \n        tok_len = sum(encoded['attention_mask'])\n        if tok_len > max_len*.8:\n            all_encode = tokenizer.encode_plus(data.text[i], add_special_tokens=True)\n            all_ids = all_encode['input_ids']\n            all_attention = all_encode['attention_mask']  \n            max_len_half = int(max_len\/2)\n            input_ids.append(all_ids[:max_len_half] + all_ids[-max_len_half:])\n            attention_masks.append(all_attention[:max_len_half] + all_attention[-max_len_half:])\n            \n        else:  \n            input_ids.append(encoded['input_ids'])\n            attention_masks.append(encoded['attention_mask'])\n    \n    return np.array(input_ids),np.array(attention_masks)\n\n\ntrain_input_ids,train_attention_masks = greed_encode(train,50)\ntest_input_ids,test_attention_masks = greed_encode(test,50)\ny_train = train.target","b3472f60":"N_SAMPLES = 8\ndef create_model(bert_model, MAX_LEN=50):\n    input_ids = layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_ids')\n    attention_mask = layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name='attention_mask')\n\n    last_hidden_state, _ = bert_model({'input_ids': input_ids, 'attention_mask': attention_mask})\n    last_hidden_state = Dropout(0.1)(last_hidden_state)\n    x_avg = layers.GlobalAveragePooling1D()(last_hidden_state)\n    x_max = layers.GlobalMaxPooling1D()(last_hidden_state)\n    x = layers.Concatenate()([x_avg, x_max])\n    \n    samples = []    \n    for n in range(N_SAMPLES):\n        sample_mask = layers.Dense(64, activation='relu', name = f'dense_{n}')\n        sample = layers.Dropout(.5)(x)\n        sample = sample_mask(sample)\n        sample = layers.Dense(1, activation='sigmoid', name=f'sample_{n}')(sample)\n        samples.append(sample)\n    \n    output = layers.Average(name='output')(samples)\n    \n    model = Model(inputs=[input_ids, attention_mask], outputs=output)\n    model.compile(Adam(lr=1e-5), loss = BinaryCrossentropy(label_smoothing=0.1), metrics=['accuracy'])\n    #model.compile(tfa.optimizers.RectifiedAdam(learning_rate=1e-5,min_lr=6e-6,total_steps=2000), loss = BinaryCrossentropy(label_smoothing=0.1), metrics=['accuracy'])\n    return model","0e7ac751":"from transformers import TFBertModel\nbert_model = TFBertModel.from_pretrained('bert-large-uncased')\n\nmodel = create_model(bert_model)\nmodel.summary()","a741b70d":"def lgb_svc_cv(model):\n    cls_layer_model = Model(model.input, outputs=model.get_layer(f'dense_1').output)\n    X_train = cls_layer_model.predict([train_input_ids,train_attention_masks])\n    X_test = cls_layer_model.predict([test_input_ids,test_attention_masks])\n    y_train = train.target.values\n    \n    N_FOLDS = 5\n    print(f'LGBM')\n    folds = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n    oof = np.zeros(len(X_train))\n    sub = np.zeros(len(X_test))\n    params = {'boosting_type': 'dart'}\n    for fold_, (train_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n        X_train_cv, y_train_cv = pd.DataFrame(X_train).loc[train_idx], pd.DataFrame(y_train).loc[train_idx]\n        X_val, y_val = pd.DataFrame(X_train).loc[val_idx], pd.DataFrame(y_train).loc[val_idx]\n        train_data = lgb.Dataset(X_train_cv, label=y_train_cv)\n        val_data = lgb.Dataset(X_val, label=y_val)\n        watchlist = [train_data, val_data]\n        clf = lgb.train(params, train_set = train_data, valid_sets=watchlist)\n        oof[val_idx] = clf.predict(X_val)\n        sub += clf.predict(X_test)\/folds.n_splits\n        \n    sub_all_lgb = sub\n    print(accuracy_score(y_train, np.round(oof).astype(int)),'\\n')        \n        \n\n    print(f'SVC')\n    folds = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n    oof = np.zeros(len(X_train))\n    sub = np.zeros(len(X_test))\n    scores = [0 for _ in range(folds.n_splits)]\n    for fold_, (train_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n        X_train_cv, y_train_cv = pd.DataFrame(X_train).loc[train_idx], pd.DataFrame(y_train).loc[train_idx]\n        X_val, y_val = pd.DataFrame(X_train).loc[val_idx], pd.DataFrame(y_train).loc[val_idx]\n        clf = SVC(kernel='rbf', C=1.75, gamma = 0.1, probability = True).fit(X_train_cv, y_train_cv)\n        oof[val_idx] = clf.predict_proba(X_val)[:,1]\n        sub += clf.predict_proba(X_test)[:,1]\/folds.n_splits\n\n    sub_all_svc = sub    \n    print(accuracy_score(y_train, np.round(oof).astype(int)),'\\n')\n    return sub_all_lgb, sub_all_svc","97827362":"oof_preds = np.zeros(train_input_ids.shape[0])\ntest_preds = np.zeros(test.shape[0])\nall_preds = pd.DataFrame()\n\nfold_hist = {}\nn_splits = 2\nfolds = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n\nfor i, (trn_idx, val_idx) in enumerate(folds.split(train_input_ids)):\n    modelstart = time.time()\n    bert_model = TFBertModel.from_pretrained('bert-large-uncased')\n    model = create_model(bert_model)\n    \n    es = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, verbose=1,\n                                 mode='min', baseline=None, restore_best_weights=True)\n    def lr_sc(epoch):\n        return 1.5e-5\/(epoch + 1)\n    scheduler = LearningRateScheduler(lr_sc)\n    \n    \n    history = model.fit(\n        x=[train_input_ids[trn_idx], train_attention_masks[trn_idx]], y=y_train[trn_idx],\n        validation_data=([train_input_ids[val_idx], train_attention_masks[val_idx]], y_train[val_idx]),\n        epochs=3,\n        batch_size=16,\n        callbacks=[scheduler, es]\n    )\n\n    best_index = np.argmin(history.history['val_loss'])\n    fold_hist[i] = history\n    \n    oof_preds[val_idx] = model.predict([train_input_ids[val_idx], train_attention_masks[val_idx]]).ravel()\n    all_preds[str(i) + '_fold_NN'] = model.predict([test_input_ids,test_attention_masks]).reshape(-1)\n    sc = accuracy_score(y_train[val_idx], (oof_preds[val_idx] > 0.5).astype(int))\n    print(\"\\nFOLFD {} in {:.1f} min - Avg Acc NN {:.5f} - Best Epoch {}\".format(i, (time.time() - modelstart)\/60, sc, best_index + 1),'\\n')\n    \n    \n    # grab last layer and use LGBM and SVC\n    all_preds[str(i) + '_fold_lgb'], all_preds[str(i) + '_fold_svc'] = lgb_svc_cv(model)\n    \n    \n    del model\n    K.clear_session()\n    gc.collect()","f69b63f9":"test_pred = all_preds[['0_fold_NN','1_fold_NN']].mean(axis = 1)*.8 + all_preds[['0_fold_lgb','1_fold_lgb']].mean(axis = 1)*.1 + all_preds[['0_fold_svc','1_fold_svc']].mean(axis = 1)*.1","caa731cc":"submission = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\nsubmission['target'] = np.round(test_pred).astype(int)\nsubmission.head(10)","aaa107a2":"submission.to_csv('submission.csv', index=False)","d9c3a7b1":"Training...","c198c37c":"__I used 3 model for predictions - NN + LGB + SVC. The idea is simple - fine-tune BERT and grab CLS embeddings to LGB and SVM classifier.__","9df55d04":"Multi-Sample Dropout technique - [Multi-Sample Dropout for Accelearted Training and Better Generalization](https:\/\/arxiv.org\/abs\/1905.09788). The loss is calculated for each sample, and then the sample losses are averaged to obtain the final loss. Multi-sample dropout does not significantly increase computation cost per iteration because most of the computation time is consumed in the convolution layers before the dropout layer,\nwhich are not duplicated. \nFirstly, I used 5 multi-sample with different dropouts for each sample, but 0.5 dropout for each sample works for me.","e53ec246":"<span style=\"color:blue\">*Please, check it out and help people to know - we don't have huge amount of disasters in our lives.*\ud83d\udc4c","b7e204ab":"LGB & SVC functions","af52a2ea":"I don't have large experience in transformers, so if you have any proposal, caveats, ideas, additions, conspiracy theory, that can improve training and score - you are welcome!\ud83d\ude09","df5fe562":"I choose 50 tokens from the sentence. So if the length of text bigger then 80% of max_len (50*0.8 = 40) I use the first 25 + last 25 tokens, so in this way, we have a better context. \nThis technique will be useful in text with different, long lengths of the sentence, in this situation is not so critical. BTW, it improves training."}}