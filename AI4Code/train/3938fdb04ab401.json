{"cell_type":{"d2676fb8":"code","fd77c9f5":"code","7a7f8f50":"code","b5b9212e":"code","96abe721":"code","eef56bfc":"code","5ba620d1":"code","e09a4ee9":"code","d385313a":"code","890734ed":"code","a92a3dca":"code","626a91bb":"code","9d8ea9e1":"code","363dde75":"code","d0cb3dce":"code","63305c3c":"code","2ea03e0d":"code","33e13f8d":"code","e407f272":"code","a48032fa":"code","d1381487":"code","b45492c7":"code","059295cf":"code","c98e3d89":"code","ca8bc46e":"code","e1fa2e72":"code","6e880b85":"code","6e48ab1b":"code","926aa713":"code","3ac5d40c":"code","0db0777f":"code","7d7ad097":"code","b71e064e":"code","3c1a1e7c":"code","293d2d2b":"code","a0364abf":"code","133bc138":"code","80b1ba7f":"code","f88e257e":"markdown","7b82bc82":"markdown","68f0908d":"markdown","3c5204af":"markdown","43bbb307":"markdown","5dc7d9d6":"markdown","66054c28":"markdown","989de549":"markdown","24bfba26":"markdown","431ee41b":"markdown","d98d21fc":"markdown","62658c96":"markdown","f23a3636":"markdown","652b07bc":"markdown","ba86ea1b":"markdown","8c185206":"markdown"},"source":{"d2676fb8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, precision_recall_curve\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom spacy.lang.en import English\nfrom spacy.lang.en.stop_words import STOP_WORDS\nlemma = WordNetLemmatizer()\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Bidirectional, LSTM, Dropout, BatchNormalization\nfrom keras.layers.embeddings import Embedding","fd77c9f5":"nltk.download('stopwords')","7a7f8f50":"embeddings_index = dict()\nf = open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.50d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()","b5b9212e":"train = pd.read_csv('..\/input\/news-popularity-in-multiple-social-media-platforms\/train_file.csv')","96abe721":"train.head()","eef56bfc":"missing_val = pd.DataFrame(train.isnull().sum())\nmissing_val = missing_val.reset_index()\nmissing_val","5ba620d1":"train[train['Source'].isna()]","e09a4ee9":"train.dropna(inplace=True)","d385313a":"train.info()","890734ed":"train.describe().T","a92a3dca":"train['Topic'].value_counts()","626a91bb":"import nltk\nstopwords = nltk.corpus.stopwords.words('english')\nstopwords.extend(['Palestinian','Palestine','Microsoft','Economy','Obama','Barack'])","9d8ea9e1":"from wordcloud import WordCloud\nplt.figure(figsize=(12,6))\ntext = ' '.join(train.Title[train['Topic']=='economy'])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)","363dde75":"from wordcloud import WordCloud\nplt.figure(figsize=(12,6))\ntext = ' '.join(train.Title[train['Topic']=='obama'])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)","d0cb3dce":"from wordcloud import WordCloud\nplt.figure(figsize=(12,6))\ntext = ' '.join(train.Title[train['Topic']=='microsoft'])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)","63305c3c":"from wordcloud import WordCloud\nplt.figure(figsize=(12,6))\ntext = ' '.join(train.Title[train['Topic']=='palestine'])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)","2ea03e0d":"sns.set(style='darkgrid',palette='Set1')\n_ = sns.jointplot(x='SentimentTitle',y='SentimentHeadline',data=train,kind = 'reg')\n_.annotate(stats.pearsonr)\nplt.show()","33e13f8d":"# Bar graph exploring total sentiment for the different topics\n\ntrain.groupby('Topic').agg('sum')[['SentimentHeadline', 'SentimentTitle']].plot(kind='bar', figsize=(25, 7),\n                                                          stacked=True, color=['b', 'r', 'g']);","e407f272":"plt.figure(figsize=(15,15))\n_ = sns.heatmap(train[['Facebook','GooglePlus','LinkedIn','SentimentTitle','SentimentHeadline']].corr(), square=True, cmap='Blues',linewidths=0.5,linecolor='w',annot=True)\nplt.title('Correlation matrix ')\n\nplt.show()","a48032fa":"X_train_title = train.loc[:,'Title'].values\ny_train_title = train.loc[:,['SentimentTitle']].values\n\nX_train_headline = train.loc[:,'Headline'].values\ny_train_headline = train.loc[:,['SentimentHeadline']].values","d1381487":"title_df=pd.DataFrame()\ntitle_df['X_train_title']=X_train_title\ntitle_df['y_train_title']=y_train_title\n\nheadline_df=pd.DataFrame()\nheadline_df['X_train_headline']=X_train_headline\nheadline_df['y_train_headline']=y_train_headline","b45492c7":"from nltk.corpus import stopwords\ndef preprocess_text(texts):\n    texts = texts.lower() \n    texts = re.sub(r'[^\\x00-\\x7F]+',' ', texts) \n    splitwords = texts.split()\n    splitwords = filter(lambda x: x[0]!= '@' , texts.split()) \n    splitwords = [word for word in splitwords if word not in set(stopwords.words('english'))] \n    texts = \" \".join(splitwords)\n    return texts","059295cf":"title_df['X_train_title'] = title_df.X_train_title.apply(preprocess_text)\ndisplay(title_df.head())","c98e3d89":"headline_df['X_train_headline'] = headline_df.X_train_headline.apply(preprocess_text)\ndisplay(headline_df.head())","ca8bc46e":"#Creating Embeddings for the titles\n\nmax_len_title = title_df.X_train_title.apply(lambda x: len(x.split())).max()\n\ntok_title = Tokenizer()\ntok_title.fit_on_texts(title_df.X_train_title)\nvocab_size_title = len(tok_title.word_index) + 1\nencoded_title = tok_title.texts_to_sequences(title_df.X_train_title)\npadded_title = pad_sequences(encoded_title, maxlen=max_len_title, padding='post')\n\nvocab_size_title = len(tok_title.word_index) + 1\n\ntitle_embedding_matrix = np.zeros((vocab_size_title, 50))\nfor word, i in tok_title.word_index.items():\n    t_embedding_vector = embeddings_index.get(word)\n    if t_embedding_vector is not None:\n        title_embedding_matrix[i] = t_embedding_vector","e1fa2e72":"#Creating Embeddings for the Headlines\n\nmax_len_headline = headline_df.X_train_headline.apply(lambda x: len(x.split())).max()\n\ntok_headline = Tokenizer()\ntok_headline.fit_on_texts(headline_df.X_train_headline)\nvocab_size_headline = len(tok_headline.word_index) + 1\nencoded_headline = tok_headline.texts_to_sequences(headline_df.X_train_headline)\npadded_headline = pad_sequences(encoded_headline, maxlen=max_len_headline, padding='post')\n\nvocab_size_headline = len(tok_headline.word_index) + 1\n\nheadline_embedding_matrix = np.zeros((vocab_size_headline, 50))\nfor word, i in tok_headline.word_index.items():\n    h_embedding_vector = embeddings_index.get(word)\n    if h_embedding_vector is not None:\n        headline_embedding_matrix[i] = h_embedding_vector","6e880b85":"x_train_title, x_valid_title, Y_train_title, y_valid_title = train_test_split(padded_title, y_train_title, shuffle = True, test_size = 0.15)\n\nx_train_headline, x_valid_headline, Y_train_headline, y_valid_headline = train_test_split(padded_headline, y_train_headline, shuffle = True, test_size = 0.15)","6e48ab1b":"import math\nfrom math import exp\nfrom keras import backend as K","926aa713":"def mod_tanh(x):\n    return K.tanh(0.6*x)","3ac5d40c":"# Model for title\ntitle_model = Sequential()\ntitle_model.add(Embedding(vocab_size_title, 50, input_length=max_len_title, weights=[title_embedding_matrix], trainable=True))\ntitle_model.add(Bidirectional(LSTM(20, return_sequences=True)))\ntitle_model.add(Dropout(0.3))\ntitle_model.add(BatchNormalization())\ntitle_model.add(Bidirectional(LSTM(20, return_sequences=True)))\ntitle_model.add(Dropout(0.3))\ntitle_model.add(BatchNormalization())\ntitle_model.add(Bidirectional(LSTM(20)))\ntitle_model.add(Dropout(0.3))\ntitle_model.add(BatchNormalization())\ntitle_model.add(Dense(64, activation='relu'))\ntitle_model.add(Dense(64, activation='relu'))\ntitle_model.add(Dense(1, activation=mod_tanh))\ntitle_model.compile(loss='mse', optimizer='adam', metrics=['mse', 'mae'])","0db0777f":"# Model for Headline\nheadline_model = Sequential()\nheadline_model.add(Embedding(vocab_size_headline, 50, input_length=max_len_headline, weights=[headline_embedding_matrix], trainable=True))\nheadline_model.add(Bidirectional(LSTM(20, return_sequences=True)))\nheadline_model.add(Dropout(0.3))\nheadline_model.add(BatchNormalization())\nheadline_model.add(Bidirectional(LSTM(20, return_sequences=True)))\nheadline_model.add(Dropout(0.3))\nheadline_model.add(BatchNormalization())\nheadline_model.add(Bidirectional(LSTM(20)))\nheadline_model.add(Dropout(0.3))\nheadline_model.add(BatchNormalization())\nheadline_model.add(Dense(64, activation='relu'))\nheadline_model.add(Dense(64, activation='relu'))\nheadline_model.add(Dense(1, activation=mod_tanh))\nheadline_model.compile(loss='mse', optimizer='adam', metrics=['mse', 'mae'])","7d7ad097":"title_model.fit(x_train_title, Y_train_title, epochs = 10)","b71e064e":"headline_model.fit(x_train_headline, Y_train_headline, epochs = 10)","3c1a1e7c":"title_valid_pred = title_model.predict(x_valid_title)","293d2d2b":"headline_valid_pred = headline_model.predict(x_valid_headline)","a0364abf":"from sklearn.metrics import mean_absolute_error\nmae_title=mean_absolute_error(y_valid_title,title_valid_pred)\nmae_headline=mean_absolute_error(y_valid_headline,headline_valid_pred)","133bc138":"score=1-((0.4*mae_title)+(0.6*mae_headline))","80b1ba7f":"print(\"Score = {} \\nScore(out of 100%) = {}%\".format(score,round(score*100, 2)))","f88e257e":"### Now we shall predict on the validation sets and then see what score we obtain","7b82bc82":"### Title model training","68f0908d":"### Text preprocessing function","3c5204af":"### We achieved a score of 93.15%\n\n### This score is an indication of how close our predicted values were to the target values. It cannot exacly be termed as accurcacy, because this is not a classification problem. Our sentiment score is a real number between -1 and 1","43bbb307":"### EDA & Data Visualization\n\n**NOTE:** I used the same EDA as in my other notebook where I used Custom Transformers in scikit-learn","5dc7d9d6":"### Creating separate dataframes for Title and Headline","66054c28":"### Calculating the Mean Absolute errors for both Title and Headline sentiments","989de549":"### Separating Title and headline, so that they can be trained separately","24bfba26":"### Here we caclulate our final score. Score is calulated as\n\n**max(0, 1 - ((0.4(mean abs error of title)+(0.6(mean abs error of headline)))**","431ee41b":"### Applying preprocessor function to the title and headline text","d98d21fc":"### Using GloVe to create word embeddings for our Title and Headline columns","62658c96":"### Defining separate LSTM Networks for Title and Headline\n\n**Some key novelties in the network:**\n- The loss function used for the network is `mean squared error`, the reason being that the output was required to be continuous\n- The activation function used in the last layer of the network was a custom `tanh` function defined above, because the outputs were required in the range of [-1, 1]","f23a3636":"Here, I have predicted the sentiment score of the Title and the Headline of the news articles.\n\nThe target columns are:\n\n- `SentimentTitle`, which is the sentiment score of the Title\n- `SentimentHeadline`, which is the sentiment score of the Headline\n\nI have used `GloVe Embeddings` for the words and created a `BiLSTM` Network to predict the sentiment","652b07bc":"### Creating training and testing sets from our data for both title and headline respectively. I have used 15% of the data for testing","ba86ea1b":"### Headline model training","8c185206":"### Defining a custom activation function by changing the pre-existing tanh parameter"}}