{"cell_type":{"2768c80b":"code","f6cba89c":"code","787c146e":"code","c7c8fba1":"code","75b063ce":"code","aaef102b":"code","c87cf7e3":"code","ce315df8":"code","58a31a04":"code","3b00b114":"code","c500b549":"code","285fce87":"code","dd2831f0":"code","e74d444f":"code","ec43337a":"code","25062694":"code","84de335d":"code","197a3806":"code","c66e6f0e":"code","22eed35e":"code","150ad16c":"code","3aa9e72b":"code","c459841b":"code","fbaed2a1":"code","7205be16":"code","25011773":"code","dd233162":"code","fcc3f9e0":"code","64b0ab32":"code","6f1b42f7":"code","5587dc97":"code","c3c5285c":"code","7f917b06":"code","c789ee54":"code","4b685f4c":"code","a4f77237":"code","f75c6677":"code","4e835558":"code","a9050a38":"code","3ee76617":"code","e3948e58":"markdown","7e52c925":"markdown","544af0fb":"markdown","58f4664b":"markdown","2d676be0":"markdown","08f0495f":"markdown","f9388c0c":"markdown","dee3f402":"markdown","3603df50":"markdown","13d1494d":"markdown","c5374525":"markdown","72c56c06":"markdown","d638f8a9":"markdown","fe389c11":"markdown","754c5273":"markdown","d9d21fa5":"markdown","a261eaad":"markdown","76b1d27e":"markdown","171c6461":"markdown","5e3f6541":"markdown","4dff1f4e":"markdown","e7101dfe":"markdown","d7f92aab":"markdown","b7dde01f":"markdown","0d793fb5":"markdown","0c21b3a4":"markdown","bb8fcf87":"markdown","b6729b2a":"markdown","a528cfd1":"markdown","5f644937":"markdown","2a0da476":"markdown","d30924c3":"markdown","9f33a20e":"markdown","412fe1b9":"markdown","026ff11f":"markdown"},"source":{"2768c80b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom datetime import datetime\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.pipeline import make_pipeline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Any results you write to the current directory are saved as output.","f6cba89c":"print(\"imported data..\")\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nprint(\"Train set size:\", train.shape)\nprint(\"Test set size:\", test.shape)","787c146e":"train.head()","c7c8fba1":"test.head()","75b063ce":"train_ID = train['Id']\ntest_ID = test['Id']\n# Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)","aaef102b":"#Before the normalisation\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.distplot(train['SalePrice'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\nsns.despine(trim=True, left=True)\nplt.show()\n\n# Deleting the more visibly obvious outliers\n# 4500 exceeds the central tendecy of of the houses in that price point and all the houses in the dataset\n# will deal with more subtle outlers later.\ntrain = train[train.GrLivArea < 4500]\ntrain.reset_index(drop=True, inplace=True)","c87cf7e3":"\n#We use the numpy fuction log1p \ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm, color=\"b\");\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\nsns.despine(trim=True, left=True)\n\nplt.show()","ce315df8":"y = train.SalePrice.reset_index(drop=True)\ntrain_features = train.drop(['SalePrice'], axis=1)\ntest_features = test\n\n# concatinate the train and the test set as features for tranformation to avoid mismatch\nfeatures = pd.concat([train_features, test_features]).reset_index(drop=True)\nprint('Features size:', features.shape)","58a31a04":"\n# function for determining the threshold of missing values\ndef percent_missing(df):\n    data = pd.DataFrame(df)\n    df_cols = list(pd.DataFrame(data))\n    dict_x = {}\n    for i in range(0, len(df_cols)):\n        dict_x.update({df_cols[i]: round(data[df_cols[i]].isnull().mean()*100,2)})\n    \n    return dict_x\n\nmissing = percent_missing(features)\ndf_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\nprint('Percent of missing data')\ndf_miss[0:10]","3b00b114":"# visualising missing values\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nsns.set_color_codes(palette='deep')\nmissing = round(train.isnull().mean()*100,2)\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar(color=\"b\")\n# Tweak the visual presentation\nax.xaxis.grid(False)\nax.set(ylabel=\"Percent of missing values\")\nax.set(xlabel=\"Features\")\nax.set(title=\"Percent missing data by feature\")\nsns.despine(trim=True, left=True)","c500b549":"\n# Some of the non-numeric predictors are stored as numbers; we convert them into strings \nfeatures['MSSubClass'] = features['MSSubClass'].apply(str)\nfeatures['YrSold'] = features['YrSold'].astype(str)\nfeatures['MoSold'] = features['MoSold'].astype(str)\n\n# data description says NA means typical\nfeatures['Functional'] = features['Functional'].fillna('Typ')\n# has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.\nfeatures['Electrical'] = features['Electrical'].fillna(\"SBrkr\")\n#  Only one NA value, We set 'TA' (which is the most frequent) for the missing value in KitchenQual\nfeatures['KitchenQual'] = features['KitchenQual'].fillna(\"TA\")\n\n# Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string\nfeatures['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\nfeatures['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\n# Fill in again with most frequent\nfeatures['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\n\n# data description says NA means \"No Pool\", majority of houses have no Pool at all in general.\nfeatures[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\")\n\n# Replacing missing data with 0 (Since No garage = no cars in such garage.)\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    features[col] = features[col].fillna(0)\n# Replacing missing data with None\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    features[col] = features[col].fillna('None')\n# For all these categorical basement-related features, NaN means that there is no basement\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    features[col] = features[col].fillna('None')\n\n# 'RL' is by far the most common value. So we can fill in missing values with 'RL'\nfeatures['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\n# group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nfeatures['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\nprint('Features size:', features.shape)","285fce87":"# Filling the rest of the categorical features\nobjects = []\nfor i in features.columns:\n    if features[i].dtype == object:\n        objects.append(i)\nfeatures.update(features[objects].fillna('None'))","dd2831f0":"# Filling in the rest of the NA's\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics.append(i)\nfeatures.update(features[numerics].fillna(0))\n","e74d444f":"# Check the state of missing values\nmissing = percent_missing(features)\ndf_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\nprint('Percent of missing data')\ndf_miss[0:10]","ec43337a":"# import statsmodels.formula.api as sm\n# def backwardElimination(x, sl):\n#     numVars = len(x[0])\n#     for i in range(0, numVars):\n#         regressor_OLS = sm.OLS(y, x).fit()\n#         maxVar = max(regressor_OLS.pvalues).astype(float)\n#         if maxVar > sl:\n#             for j in range(0, numVars - i):\n#                 if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n#                     x = np.delete(x, j, 1)\n#     regressor_OLS.summary()\n#     return x\n\n# #start elimination with 5% significance level for best parameters\n# SL = 0.01\n# X_opt = features\n# y = y\n# features = backwardElimination(X_opt, SL)","25062694":"# First we need to find all numeric features in the data\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics2 = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics2.append(i)","84de335d":"# Box plots for all our numeric features\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=features[numerics2] , orient=\"h\", palette=\"Set1\")\n# Tweak the visual presentation\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","197a3806":"# Find the skewed  numerical features\nskew_features = features[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\nprint(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\nskewness = pd.DataFrame({'Skew' :high_skew})\nskew_features.head(10)\n","c66e6f0e":"# Normalise skewed features\nfor i in skew_index:\n    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))\n    ","22eed35e":"sns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=features[skew_index] , orient=\"h\", palette=\"Set1\")\n# Tweak the visual presentation\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)\n\n","150ad16c":"# Calculating totals before droping less significant columns\n\n#  Adding total sqfootage feature \nfeatures['TotalSF']=features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']\n#  Adding total bathrooms feature\nfeatures['Total_Bathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +\n                               features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))\n#  Adding total porch sqfootage feature\nfeatures['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                              features['EnclosedPorch'] + features['ScreenPorch'] +\n                              features['WoodDeckSF'])\n","3aa9e72b":"# Not normaly distributed can not be normalised and has no central tendecy\nfeatures = features.drop(['MasVnrArea', 'OpenPorchSF', 'WoodDeckSF', 'BsmtFinSF1','2ndFlrSF'], axis=1)\n","c459841b":"# Check the distribution after dopping the skewed features\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics3 = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics3.append(i)\n        \nskew_features = features[numerics3].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=features[skew_index] , orient=\"h\", palette=\"Set1\")\n# Tweak the visual presentation\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)\n","fbaed2a1":"# Adding new simplified features (1 = present, 0 = not present)\nfeatures['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\nprint('Features size:', features.shape)","7205be16":"# Encoding the finalized features\nfinal_features = pd.get_dummies(features).reset_index(drop=True)\nprint('Features size:', features.shape)\nfinal_features.head()","25011773":"# Spliting the data back to train(X,y) and test(X_sub)\nX = final_features.iloc[:len(y), :]\nX_test = final_features.iloc[len(X):, :]\nprint('Features size for train(X,y) and test(X_test):')\nprint('X', X.shape, 'y', y.shape, 'X_test', X_test.shape)\n","dd233162":"# Finding numeric features\nsns.set_style(\"dark\")\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics4 = []\nfor i in X.columns:\n    if X[i].dtype in numeric_dtypes:\n        if i in ['TotalSF', 'Total_Bathrooms','Total_porch_sf','haspool','hasgarage','hasbsmt','hasfireplace']:\n            pass\n        else:\n            numerics4.append(i)     \n# visualising some more outliers in the data values\nfig, axs = plt.subplots(ncols=2, nrows=0, figsize=(12, 80))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\n\ncmap = sns.cubehelix_palette(dark=0.3, light=0.8, as_cmap=True)\n\nfor i, feature in enumerate(list(X[numerics4]), 1):    \n    plt.subplot(len(list(numerics4)), 4, i)\n    sns.scatterplot(x=feature, y='SalePrice', hue='SalePrice', size='SalePrice', palette=cmap, data=train)\n        \n    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n    plt.ylabel('SalePrice', size=15, labelpad=12.5)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(loc='best', prop={'size': 10})\n        \nplt.show()","fcc3f9e0":"# Removes outliers \noutliers = [30, 88, 462, 631, 1322]\nX = X.drop(X.index[outliers])\ny = y.drop(y.index[outliers])","64b0ab32":"\n# Removes colums where the threshold of zero's is (> 99.95), means has only zero values \noverfit = []\nfor i in X.columns:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(X) * 100 > 99.95:\n        overfit.append(i)\n\noverfit = list(overfit)\noverfit.append('MSZoning_C (all)')\n\nX = X.drop(overfit, axis=1).copy()\nX_test = X_test.drop(overfit, axis=1).copy()\n\nprint('X', X.shape, 'y', y.shape, 'X_test', X_test.shape)","6f1b42f7":"kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\n# model scoring and validation function\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y,scoring=\"neg_mean_squared_error\",cv=kfolds))\n    return (rmse)\n\n# rmsle scoring function\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","5587dc97":"# Grid search CV\n# from sklearn.model_selection import GridSearchCV\n# param = {'alpha':[1.0,0.0,0.1,0.01,0.001]} \n# model = Lasso()\n# grid_search = GridSearchCV(estimator=reg, param_grid=param, cv=5) \n# grid_search.fit(X, y)\n# print(grid_search.best_params_)\n# print(grid_search.best_estimator_)","c3c5285c":"\n# setup models hyperparameters using a pipline\n# The purpose of the pipeline is to assemble several steps that can be cross-validated together, while setting different parameters.\n# This is a range of values that the model considers each time in runs a CV\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n\n\n# Kernel Ridge Regression : made robust to outliers\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\n\n# LASSO Regression : made robust to outliers\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, \n                    alphas=alphas2,random_state=42, cv=kfolds))\n\n# Elastic Net Regression : made robust to outliers\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, \n                         alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))\n\n# store models, scores and prediction values \nmodels = {'Ridge': ridge,\n          'Lasso': lasso, \n          'ElasticNet': elasticnet}\npredictions = {}\nscores = {}\n\nfor name, model in models.items():\n    \n    model.fit(X, y)\n    predictions[name] = np.expm1(model.predict(X))\n    \n    score = cv_rmse(model, X=X)\n    scores[name] = (score.mean(), score.std())\n    ","7f917b06":"# get the performance of each model on training data(validation set)\nprint('---- Score with CV_RMSLE-----')\nscore = cv_rmse(ridge)\nprint(\"Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = cv_rmse(lasso)\nprint(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = cv_rmse(elasticnet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\n\n\n","c789ee54":"#Fit the training data X, y\nprint('----START Fit----',datetime.now())\nprint('Elasticnet')\nelastic_model = elasticnet.fit(X, y)\nprint('Lasso')\nlasso_model = lasso.fit(X, y)\nprint('Ridge')\nridge_model = ridge.fit(X, y)\n\n","4b685f4c":"# model blending function using fitted models to make predictions\ndef blend_models(X):\n    return ((elastic_model.predict(X)) + (lasso_model.predict(X)) + (ridge_model.predict(X)))\/3\nblended_score = rmsle(y, blend_models(X))\nprint('RMSLE score on train data:')\nprint(rmsle(y, blend_models(X)))","a4f77237":"# visualise model performance\nsns.set_style(\"white\")\nfig, axs = plt.subplots(ncols=0, nrows=3, figsize=(8, 7))\nplt.subplots_adjust(top=3.5, right=2)\n\nfor i, model in enumerate(models, 1):\n    plt.subplot(3, 1, i)\n    plt.scatter(predictions[model], np.expm1(y))\n    plt.plot([0, 800000], [0, 800000], '--r')\n\n    plt.xlabel('{} Predictions (y_pred)'.format(model), size=15)\n    plt.ylabel('Real Values (y_train)', size=13)\n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n\n    plt.title('{} Predictions vs Real Values'.format(model), size=15)\n    plt.text(0, 700000, 'Mean RMSE: {:.6f} \/ Std: {:.6f}'.format(scores[model][0], scores[model][1]), fontsize=15)\n    ax.xaxis.grid(False)\n    sns.despine(trim=True, left=True)\nplt.show()","f75c6677":"scores['Blender'] = (blended_score, 0)\nsns.set_style(\"white\")\nfig = plt.figure(figsize=(24, 12))\n\nax = sns.pointplot(x=list(scores.keys()), y=[score for score, _ in scores.values()], markers=['o'], linestyles=['-'])\nfor i, score in enumerate(scores.values()):\n    ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\nplt.ylabel('Score (RMSE)', size=20, labelpad=12.5)\nplt.xlabel('Model', size=20, labelpad=12.5)\nplt.tick_params(axis='x', labelsize=13.5)\nplt.tick_params(axis='y', labelsize=12.5)\n\nplt.title('Scores of Models', size=20)\n\nplt.show()","4e835558":"# get the target variable\/ y_test with X_test\nprint('Predict submission')\ny_test_r = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsubmission = pd.read_csv(\"..\/input\/sample_submission.csv\")\ny_test = np.log1p(y_test_r.iloc[:,1].values)\nsubmission.iloc[:,1] = np.expm1(blend_models(X_test))\nblended_score = rmsle(y_test, blend_models(X_test))\n","a9050a38":"submission.to_csv(\"new_submission.csv\", index=False)\nprint('Save submission', datetime.now(),)","3ee76617":"submission.head()","e3948e58":"# Evaluate Model Performance","7e52c925":"We check the data after normalisation, is there any remaining skewed values?","544af0fb":"Outlier removal is usually safe, for outliers that are very visible in certain features. We decided to delete these outliers as they are  likely to introduce bias in our regression functions.\n","58f4664b":"Created 3 models RidgeCV, LassoCV, ElasticNetCV and are linear models with built-in cross validation.\nWe a going to use these nodels together to make the final prediction.","2d676be0":"> The graph shows that our data now looks more normal","08f0495f":"# Ensemble prediction on blended models","f9388c0c":"**Data Processing**","dee3f402":"Separating the training(X,y) and the testing(X_sub) set","3603df50":">  [MasVnrArea, OpenPorchSF, WoodDeckSF, BsmtFinSF1] could not be normalised therefore call to be droped.","13d1494d":"Now we use 10-fold stacking , we first split the training data into 10 folds. Then we will do 10 iterations. In each iteration, we train every base model on 9 folds and predict on the remaining fold (holdout fold).","c5374525":"# Model Predictions","72c56c06":"# Feature Engineering","d638f8a9":"> The graph shows that our data is skewed to the right.","fe389c11":"We use the kfolds Cross Validation function where K=10 which is the number of holdout sets. The function has no shuffle attribute, we add then one line of code shuffle=True, in order to shuffle the dataset prior to cross-validation.\n","754c5273":"After imputing features with missing values, is there any remaining missing values?","d9d21fa5":"# Submission","a261eaad":"Visualize the destributions of the numeric features. This will allow us to visually see the ditributions of all our numeric data.","76b1d27e":" > The visualisation show that is some data that is not normaly distributed(Skewed)","171c6461":">  No more missing values","5e3f6541":"> The are no more skewed numerical variables","4dff1f4e":"This is to make sure that SalesPrice values are distributed normaly using function log1p which  applies log(1+x) to all elements of the column which fixes the skeweness of the distribution.","e7101dfe":"# More feature engineering","d7f92aab":"Looking at the distribution of the numeric features. Ideally we would like all our data to be normaly distributed. Id these is not normally distributed that could violate the assumptions of parametric statistics. If the data deviate strongly from the assumptions of a parametric procedure, using the parametric procedure could lead to incorrect conclusions.  ","b7dde01f":"# Finalising features","0d793fb5":"Lets look at the distribution of our target variable to see if it fits a normal distribution, due to the parametric nature of our analysis procedure.","0c21b3a4":"# Import Data","bb8fcf87":"Since area related features are very important to determine house prices, we add a few more features which is the total area of floors, bathrooms and porch area of each house before we continue droping these numeric colum","b6729b2a":"# House Prices: Advanced Regression Techniques\n\n**Competition Description**\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.","a528cfd1":"Another difficulty was to find the most optimal hyperparameters.Applying Grid Search was onr of the methods we tried to get the best hyperparameters for our base models. Although this is very time consuming and computationally expensive the use of a pipeline seemed to be much more easier and effective. (Expand below to view commented code.)\n\n","5f644937":"All of the models individually achieved scores between 0.10 and 0.13, but when the predictions of those models are blended, they get about 0.090. That's because those models are actually overfitting to certain degree. They are very good at predicting a subset of houses, and they fail at predicting the rest of the dataset. When their predictions are blended, those models complement each other.","2a0da476":"We now start the process of preparing our features, we first find, the percentage of missing data in each column and we determine whether the threshold of missing values is acceptable or not.","d30924c3":"We use the scipy function boxcox1p which computes the Box-Cox transformation. The goal is to find a simple transformation that leads to normality. ","9f33a20e":"# Start Model Building","412fe1b9":"One of the other methods that we tried that did not work well in selecting the feature and improving the accuracy was Backward Elimination with Adjusted R-squared. (Expand below to view commented code.)","026ff11f":"These visualisation helps determine which values need to be imputed, We impute them by proceeding sequentially through features with missing values."}}