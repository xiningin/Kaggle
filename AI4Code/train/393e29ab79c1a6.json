{"cell_type":{"c1f35f2b":"code","a2fbbb46":"code","439138fa":"code","75a7d8f8":"code","0ed9ee54":"code","970891ef":"code","19975e30":"code","70decfe9":"code","cc274794":"code","9a569c36":"code","69c17f5b":"code","e5585224":"markdown","9cbf623b":"markdown","16f5b0c8":"markdown","e09678fd":"markdown","92a19921":"markdown","aa28f10c":"markdown","1e2ef3ba":"markdown","37a36587":"markdown"},"source":{"c1f35f2b":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split","a2fbbb46":"df = pd.read_csv('..\/input\/prostate-cancer\/Prostate_Cancer.csv')\ndf.head()","439138fa":"labels = df['diagnosis_result']\nfeatures = df.drop(columns=['id','diagnosis_result'])","75a7d8f8":"# normalize\nfeatures = (features - features.mean())\/features.std()\nfeatures.head()\n\n# replace label malignant and benignant with 0 and 1\n# malignant - cancer - 1\n# benignant - non-cancerous - 0\ncategories_to_idx = {'M' : 1, 'B' : -1}\nidx_to_categories = { 1 : 'Malignant', -1 : 'Benignant'}\n\nlabels = pd.DataFrame(labels.replace(categories_to_idx))","0ed9ee54":"sns.pairplot(pd.concat([pd.DataFrame(labels.replace(idx_to_categories)), features], axis=1), hue=\"diagnosis_result\")","970891ef":"X_train, X_test, y_train, y_test = train_test_split(features.values, labels.values.reshape(-1), test_size=0.33, random_state=42)","19975e30":"class Adaboost():\n    '''\n        AdaBoost Implementation\n    '''\n    \n    class Decision():\n        def __init__(self):\n            self.polarity = 1\n            self.feature_idx = None\n            self.threshold = None\n            self.alpha = None\n\n        def predict(self, X):\n            n_samples = X.shape[0]\n            X_column = X[:, self.feature_idx]\n            predictions = np.ones(n_samples)\n            if self.polarity == 1:\n                predictions[X_column < self.threshold] = -1\n            else:\n                predictions[X_column > self.threshold] = -1\n\n            return predictions\n\n    def __init__(self, n_clf=5):\n        self.n_clf = n_clf\n        \n    def predict(self, X):\n        clf_preds = [clf.alpha * clf.predict(X) for clf in self.clfs]\n        y_pred = np.sum(clf_preds, axis=0)\n        y_pred = np.sign(y_pred)\n\n        return y_pred\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n\n        # Initialize weights to 1\/N\n        w = np.full(n_samples, (1 \/ n_samples))\n\n        self.clfs = []\n        # Iterate through classifiers\n        for _ in range(self.n_clf):\n            clf = self.Decision()\n\n            min_error = float('inf')\n            # greedy search to find best threshold and feature\n            for feature_i in range(n_features):\n                X_column = X[:, feature_i]\n                thresholds = np.unique(X_column)\n\n                for threshold in thresholds:\n                    # predict with polarity 1\n                    p = 1\n                    predictions = np.ones(n_samples)\n                    predictions[X_column < threshold] = -1\n\n                    # Error = sum of weights of misclassified samples\n                    misclassified = w[y != predictions]\n                    error = sum(misclassified)\n\n                    if error > 0.5:\n                        error = 1 - error\n                        p = -1\n\n                    # store the best configuration\n                    if error < min_error:\n                        clf.polarity = p\n                        clf.threshold = threshold\n                        clf.feature_idx = feature_i\n                        min_error = error\n\n            # calculate alpha\n            EPS = 1e-10\n            clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) \/ (min_error + EPS))\n\n            # calculate predictions and update weights\n            predictions = clf.predict(X)\n\n            w *= np.exp(-clf.alpha * y * predictions)\n            # Normalize to one\n            w \/= np.sum(w)\n\n            # Save classifier\n            self.clfs.append(clf)","70decfe9":"adaboost = Adaboost()\nadaboost.fit(X_train, y_train)","cc274794":"def get_accuracy(X, y):\n    predictions = adaboost.predict(X)\n    accuracy = len(np.where(predictions == y)[0]) \/ len(y)\n    return accuracy","9a569c36":"accuracy = get_accuracy(X_train, y_train)\nprint(\"Train Accuracy: {0:1.4f}\".format(accuracy))","69c17f5b":"accuracy = get_accuracy(X_test, y_test)\nprint(\"Test Accuracy: {0:1.4f}\".format(accuracy))","e5585224":"# Prostate Cancer \ud83c\udf97 detection using Adaboost from Scratch","9cbf623b":"# Prepare Data","16f5b0c8":"# Adaboost implementation","e09678fd":"# Adaboost definition","92a19921":"# Analyze Results","aa28f10c":"![adaboost.png](attachment:adaboost.png)","1e2ef3ba":"AdaBoost, short for Adaptive Boosting, is a machine learning meta-algorithm formulated by Yoav Freund and Robert Schapire, who won the 2003 G\u00f6del Prize for their work. It can be used in conjunction with many other types of learning algorithms to improve performance. The output of the other learning algorithms ('weak learners') is combined into a weighted sum that represents the final output of the boosted classifier. AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers. AdaBoost is sensitive to noisy data and outliers. In some problems it can be less susceptible to the overfitting problem than other learning algorithms. The individual learners can be weak, but as long as the performance of each one is slightly better than random guessing, the final model can be proven to converge to a strong learner. \n\nRef. [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/AdaBoost)","37a36587":"# Adaboost Algorithm"}}