{"cell_type":{"88933ad2":"code","0c8c8ea6":"code","9a83ba6c":"code","3c6c1b5f":"code","c8f2283e":"code","6f85502a":"code","9fa09703":"code","16735121":"code","d7c0c2f2":"code","26bee193":"code","20a54dcf":"code","0edae525":"code","e0a6a0d0":"code","637222f2":"code","7663a507":"code","96ff563d":"code","d8e559f1":"code","236ea32b":"code","bd9c4c67":"code","9f1939d6":"code","ab85d95b":"code","2bf6cc72":"code","a6bc63dd":"code","489877ec":"code","438c780f":"code","0a4f31cf":"code","51465105":"code","ffcf0911":"code","8c224fa3":"code","c1768290":"code","8eb8efe1":"code","e0c246ea":"code","bf2a2c2e":"code","406d5570":"code","db38d843":"code","cc4d4a53":"code","ff649c72":"code","1ab63f49":"code","e224bcab":"code","b05f2eec":"code","1f17961b":"code","5457f2f5":"code","4de51441":"code","193f1e60":"code","6126b1b3":"code","5df750e2":"code","80e5eaa3":"code","141a4716":"code","78edd19e":"code","318fb010":"code","9ab80ed6":"code","59a4f502":"code","4c2af043":"code","1a6e7fd0":"code","84029489":"code","02983157":"code","4c3c529f":"code","6bcd57bd":"code","a32ee2d9":"code","80b9acd4":"code","ed0ecc10":"code","38a61d84":"code","2f288338":"code","5d829fe3":"code","7b18176e":"code","52bec5ca":"code","6b6142cc":"code","ae2c6393":"code","6963bdd7":"code","18087c79":"code","56f9fd17":"code","070cf6ec":"code","b00b7a9e":"code","6de39ef7":"code","5d9aa214":"code","51e864f9":"code","36b748d4":"code","af24a122":"code","b31769b1":"code","e3ec39ba":"code","64fa45ce":"code","f448df08":"code","0dd9e78f":"code","befc65ea":"code","ce724567":"code","3fc2d4a8":"code","538554df":"code","11a2ad16":"code","a6d89c87":"code","01886426":"code","bace8189":"code","ca74ceb2":"code","5d625603":"code","0fe8a97c":"code","a12a10e6":"code","e09e5387":"code","08e53925":"code","99b567eb":"code","e36eb15b":"code","16d497ab":"code","609ac34c":"code","968726a3":"code","e2465104":"code","1b5122fd":"code","89ff7aff":"code","9ff20c42":"code","3b8ffe26":"code","a577138e":"code","41025f10":"markdown","1e002ee0":"markdown","1bb04c50":"markdown","0f8ca8c8":"markdown","b1228d1c":"markdown","d7d8fcd0":"markdown","0b7fe877":"markdown","050982f8":"markdown","12b026d3":"markdown","39541cf4":"markdown","271617a7":"markdown","a9b28e57":"markdown","b770b61b":"markdown","0eab8588":"markdown","2c9764c8":"markdown","420cfded":"markdown","556c631a":"markdown","85472850":"markdown","4bbc56d7":"markdown","b383e66a":"markdown","e4e3b79d":"markdown","f24e11c6":"markdown","c16df0bc":"markdown","f2669ef9":"markdown","683482d5":"markdown","9075f1e0":"markdown","cd9cb5ad":"markdown","7b52baf9":"markdown","9b9fcdf8":"markdown","acc90e85":"markdown","828cd01c":"markdown","ae5641a8":"markdown","759a9905":"markdown","5b8d74eb":"markdown","8477ca05":"markdown","94c36a66":"markdown","3ec22309":"markdown","77767e8c":"markdown","5c377a36":"markdown","efa4d984":"markdown","7178fd65":"markdown","19a28d84":"markdown","4a49ac1b":"markdown","cbb45952":"markdown","8fa8882f":"markdown","0269b782":"markdown","57231e29":"markdown","a4e60161":"markdown","0b30d927":"markdown","20615d65":"markdown","156ac82e":"markdown","81fb59d5":"markdown"},"source":{"88933ad2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport datetime\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport re\nfrom scipy import stats\n\nmatplotlib.rcParams['figure.figsize'] = (10, 5)\nmatplotlib.rcParams['font.size'] = 12\n\nimport random\nrandom.seed(1)\nimport time\n\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import get_scorer\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import VotingClassifier\nimport lightgbm as lgb\nfrom sklearn.externals.joblib import Parallel, delayed\nfrom sklearn.base import clone\n\nimport pickle","0c8c8ea6":"import warnings\nwarnings.filterwarnings('ignore')","9a83ba6c":"matplotlib.rcParams['figure.figsize'] = (10, 5)\nmatplotlib.rcParams['font.size'] = 12","3c6c1b5f":"from kaggle.competitions import twosigmanews\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()\nprint('Done!')","c8f2283e":"(market_train_orig, news_train_orig) = env.get_training_data()","6f85502a":"market_train_df = market_train_orig.copy()\nnews_train_df = news_train_orig.copy()\nprint('Market train shape: ',market_train_df.shape)\nprint('News train shape: ', news_train_df.shape)","9fa09703":"market_train_df.describe()","16735121":"news_train_df.describe()","d7c0c2f2":"# Sort values by time then extract date\nnews_train_df = news_train_df.sort_values(by='time')\nnews_train_df['date'] = news_train_df['time'].dt.date","26bee193":"# Function to plot time series data\ndef plot_vs_time(data_frame, column, calculation='mean', span=10):\n    if calculation == 'mean':\n        group_temp = data_frame.groupby('date')[column].mean().reset_index()\n    if calculation == 'count':\n        group_temp = data_frame.groupby('date')[column].count().reset_index()\n    if calculation == 'nunique':\n        group_temp = data_frame.groupby('date')[column].nunique().reset_index()\n    group_temp = group_temp.ewm(span=span).mean()\n    fig = plt.figure(figsize=(10,3))\n    plt.plot(group_temp['date'], group_temp[column])\n    plt.xlabel('Time')\n    plt.ylabel(column)\n    plt.title('%s versus time' %column)","20a54dcf":"plot_vs_time(news_train_df, 'sourceId', calculation='count', span=10)\nplt.title('News count vs time')\nplt.ylabel('Count')","0edae525":"# Plot time evolution of several parameters\n\ncolumns = ['urgency', 'takeSequence', 'companyCount','marketCommentary','sentenceCount',\\\n           'firstMentionSentence','relevance','sentimentClass','sentimentWordCount','noveltyCount24H', 'volumeCounts24H']\n\nfor column in columns:\n    plot_vs_time(news_train_df, column)","e0a6a0d0":"time_delay = (pd.to_datetime(news_train_df['time']) - pd.to_datetime(news_train_df['firstCreated']))\ntime_delay_log10 = np.log10(time_delay.dt.total_seconds()\/60+1)","637222f2":"plt.hist(time_delay_log10, bins=np.arange(0,2.5,0.25), rwidth=0.7)\nplt.xlabel('$Log_{10}$(Time delay in minutes +1)')\nplt.ylabel('Counts')\nplt.title('Delay time distribution')","7663a507":"time_delay_min = time_delay.dt.total_seconds()\/60\ntime_delay_df = time_delay_min.to_frame().join(news_train_df['date'].to_frame())\ntime_delay_df.columns = ['delay','date']\nplot_vs_time(time_delay_df, 'delay')\nplt.ylabel('Delay (minutes)')","96ff563d":"urgency_count = news_train_df.groupby('urgency')['sourceId'].count()\nurgency_count = urgency_count\/urgency_count.sum()\nprint('Urgency ratio')\nurgency_count.sort_values(ascending=True)\ndel urgency_count","d8e559f1":"take_sequence = news_train_df.groupby('takeSequence')['sourceId'].count()","236ea32b":"take_sequence = take_sequence.sort_values(ascending= False)\ntake_sequence[:10].plot.barh()\nplt.xlabel('Count')\nplt.ylabel('Take sequence')\nplt.title('Top 10 take sequence')\nplt.gca().invert_yaxis()\ndel take_sequence","bd9c4c67":"provider_count = news_train_df.groupby('provider')['sourceId'].count()","9f1939d6":"provider_sort = provider_count.sort_values(ascending= False)\nprovider_sort[:10].plot.barh()\nplt.xlabel('Count')\nplt.ylabel('Provider')\nplt.title('Top 10 news provider')\nplt.gca().invert_yaxis()\ndel provider_count","ab85d95b":"# Extract data from a single cell\ndef contents_to_list(contents):\n    text = contents[1:-1]\n    text = re.sub(r\",\",' ',text)\n    text = re.sub(r\"'\",\"\", text)\n    text_list = text.split('  ')\n    return text_list\n\n# Put data from columns into dict\ndef get_content_dict(content_column):\n    content_dict = {}\n    for i in range(len(content_column)):\n        this_cell = content_column[i]\n        content_list = contents_to_list(this_cell)        \n        for content in content_list:\n            if content in content_dict.keys():\n                content_dict[content] += 1\n            else:\n                content_dict[content] = 1\n    return content_dict\n","2bf6cc72":"subjects = news_train_df.sample(n=10000, random_state=1)['subjects']\nsubjects_dict = get_content_dict(subjects)","a6bc63dd":"subjects_df = pd.Series(subjects_dict).sort_values(ascending=False)\nsubjects_df[:15].plot.barh()\nplt.ylabel('Subjects')\nplt.xlabel('Counts')\nplt.title('Top subjects for 10k data')\nplt.gca().invert_yaxis()\ndel subjects_df","489877ec":"audiences = news_train_df.sample(n=10000, random_state=1)['audiences']\naudiences_dict = get_content_dict(audiences)","438c780f":"audiences_df = pd.Series(audiences_dict).sort_values(ascending=False)\naudiences_df[:15].plot.barh()\nplt.ylabel('Audiences')\nplt.xlabel('Counts')\nplt.title('Top audiences for 10k data')\nplt.gca().invert_yaxis()","0a4f31cf":"news_train_df['companyCount'].hist(bins=np.arange(0,30,1))\nplt.xlabel('Company count')\nplt.title('Company count distribution')","51465105":"head_line = news_train_df.groupby('headlineTag')['sourceId'].count()","ffcf0911":"head_line_sort = head_line.sort_values(ascending= False)\nhead_line_sort[:10].plot.barh()\nplt.xlabel('Count')\nplt.ylabel('Head line')\nplt.title('Top 10 head lines')\nplt.gca().invert_yaxis()\ndel head_line","8c224fa3":"news_train_df['firstMentionSentence'].hist(bins=np.arange(0,20,1))\nplt.xlabel('First mention sentence')\nplt.ylabel('Count')\nplt.title('First mention sentence distribution')","c1768290":"sentence_urgency = news_train_df.groupby('firstMentionSentence')['urgency'].mean()\nsentence_urgency.head(5)\ndel sentence_urgency","8eb8efe1":"news_train_df['relevance'].hist(bins=np.arange(0,1.01,0.05))\nplt.xlabel('Relevance')\nplt.ylabel('Count')\nplt.title('Relevance distribution')","e0c246ea":"sentence_relevance = news_train_df.groupby('firstMentionSentence')['relevance'].mean()\nsentence_relevance[:15].plot.barh()\nplt.xlabel('Relevance')\nplt.title('Relevance by sentence')\nplt.gca().invert_yaxis()\ndel sentence_relevance","bf2a2c2e":"sentimentWordCount = news_train_df.groupby('sentimentWordCount')['sourceId'].count().reset_index()\nplt.plot(sentimentWordCount['sentimentWordCount'], sentimentWordCount['sourceId'])\nplt.xlim(0,300)\nplt.xlabel('Sentiment words count')\nplt.ylabel('Count')\nplt.title('Sentiment words count distribution')\ndel sentimentWordCount","406d5570":"sentimentWordRatio = news_train_df.groupby('sentimentWordCount')['relevance'].mean()\nplt.plot(sentimentWordRatio)\nplt.xlim(0,2000)\nplt.ylabel('Relevance')\nplt.xlabel('Sentiment word count')\nplt.title('Sentiment word count and relevance')\ndel sentimentWordRatio","db38d843":"news_train_df['sentimentRatio'] = news_train_df['sentimentWordCount']\/news_train_df['wordCount']\nnews_train_df['sentimentRatio'].hist(bins=np.linspace(0,1.001,40))\nplt.xlabel('Sentiment ratio')\nplt.ylabel('Count')\nplt.title('Sentiment ratio distribution')","cc4d4a53":"news_train_df.sample(n=10000, random_state=1).plot.scatter('sentimentRatio', 'relevance')\nplt.title('Relevance vs sentiment ratio of 10k samples')","ff649c72":"asset_name = news_train_df.groupby('assetName')['sourceId'].count()\nprint('Total number of assets: ',news_train_df['assetName'].nunique())","1ab63f49":"asset_name = asset_name.sort_values(ascending=False)\nasset_name[:10].plot.barh()\nplt.gca().invert_yaxis()\nplt.xlabel('Count')\nplt.title('Top 10 assets news')","e224bcab":"for i, j in zip([-1, 0, 1], ['negative', 'neutral', 'positive']):\n    df_sentiment = news_train_df.loc[news_train_df['sentimentClass'] == i, 'assetName']\n    print(f'Top mentioned companies for {j} sentiment are:')\n    print(df_sentiment.value_counts().head(5))\n    print('')","b05f2eec":"# Function to remove outliers\ndef remove_outliers(data_frame, column_list, low=0.02, high=0.98):\n    temp_frame = data_frame\n    for column in column_list:\n        this_column = data_frame[column]\n        quant_df = this_column.quantile([low,high])\n        low_limit = quant_df[low]\n        high_limit = quant_df[high]\n        temp_frame[column] = data_frame[column].clip(lower=low_limit, upper=high_limit)\n    return temp_frame","1f17961b":"# Remove outlier\ncolumns_outlier = ['takeSequence', 'bodySize', 'sentenceCount', 'wordCount', 'sentimentWordCount', 'firstMentionSentence','noveltyCount12H',\\\n                  'noveltyCount24H', 'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', 'volumeCounts12H', 'volumeCounts24H',\\\n                  'volumeCounts3D','volumeCounts5D','volumeCounts7D']\nnews_rmv_outlier = remove_outliers(news_train_df, columns_outlier)","5457f2f5":"# Plot correlation\ncolumns_corr = ['urgency', 'takeSequence', 'companyCount','marketCommentary','sentenceCount',\\\n           'firstMentionSentence','relevance','sentimentClass','sentimentWordCount','noveltyCount24H',\\\n           'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D','volumeCounts24H','volumeCounts3D','volumeCounts5D','volumeCounts7D']\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(18,15))\nsns.heatmap(news_rmv_outlier[columns_corr].astype(float).corr(), linewidths=0.1, vmax=1.0, vmin=-1., square=True, cmap=colormap, linecolor='white', annot=True)\nplt.title('Pair-wise correlation')","4de51441":"print('Check null data:')\nmarket_train_df.isna().sum()","193f1e60":"# Sort data\nmarket_train_df = market_train_df.sort_values('time')\nmarket_train_df['date'] = market_train_df['time'].dt.date\n\n# Fill nan\nmarket_train_fill = market_train_df\ncolumn_market = ['returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\ncolumn_raw = ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1','returnsClosePrevRaw10', 'returnsOpenPrevRaw10']\nfor i in range(len(column_raw)):\n    market_train_fill[column_market[i]] = market_train_fill[column_market[i]].fillna(market_train_fill[column_raw[i]])","6126b1b3":"plot_vs_time(market_train_fill, 'assetCode', 'count')\nplt.title('Number of asset codes versus time')","5df750e2":"# Inspired by https:\/\/www.kaggle.com\/artgor\/eda-feature-engineering-and-everything\nfor i in [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]:\n    price_df = market_train_fill.groupby('date')['close'].quantile(i).reset_index()\n    plt.plot(price_df['date'], price_df['close'], label='%.2f quantile' %i)\nplt.legend(loc='best')\nplt.xlabel('Time')\nplt.ylabel('Price')\nplt.title('Market close price by quantile')","80e5eaa3":"for i in [0.05, 0.25, 0.5, 0.75, 0.95]:\n    price_df = market_train_fill.groupby('date')['returnsClosePrevRaw1'].quantile(i).reset_index()\n    plt.plot(price_df['date'], price_df['returnsClosePrevRaw1'], label='%.2f quantile' %i)\nplt.legend(loc='best')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.title('returnsClosePrevRaw1 by quantile')\n","141a4716":"for i in [0.05, 0.25, 0.5, 0.75, 0.95]:\n    price_df = market_train_fill.groupby('date')['returnsOpenPrevRaw10'].quantile(i).reset_index()\n    plt.plot(price_df['date'], price_df['returnsOpenPrevRaw10'], label='%.2f quantile' %i)\nplt.legend(loc=1)\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.title('returnsOpenPrevRaw10 by quantiles')","78edd19e":"for i in [0.05, 0.25, 0.5, 0.75, 0.95]:\n    price_df = market_train_fill.groupby('date')['returnsOpenPrevMktres10'].quantile(i).reset_index()\n    plt.plot(price_df['date'], price_df['returnsOpenPrevMktres10'], label='%.2f quantile' %i)\nplt.legend(loc=1)\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.title('returnsOpenPrevMktres10 by quantiles')","318fb010":"for i in [0.05, 0.25, 0.5, 0.75, 0.95]:\n    price_df = market_train_fill.groupby('date')['returnsOpenNextMktres10'].quantile(i).reset_index()\n    plt.plot(price_df['date'], price_df['returnsOpenNextMktres10'], label='%.2f quantile' %i)\nplt.legend(loc=1)\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.title('returnsOpenNextMktres10 by quantiles')","9ab80ed6":"for i in [0.05, 0.25, 0.5, 0.75, 0.95]:\n    price_df = market_train_fill.groupby('date')['volume'].quantile(i).reset_index()\n    plt.plot(price_df['date'], price_df['volume'], label='%.2f quantile' %i)\nplt.legend(loc='best')\nplt.xlabel('Time')\nplt.ylabel('Volumes')\nplt.title('Market trade volumes by quantile')","59a4f502":"column_mkt_raw_diff = []\nfor i in range(len(column_market)):\n    this_raw = column_raw[i]\n    this_market = column_market[i]\n    new_column_name = 'mkt_raw_diff'+this_raw.replace('returns','').replace('Raw','')\n    column_mkt_raw_diff.append(new_column_name)\n    market_train_fill[new_column_name] = market_train_fill[this_market] - market_train_fill[this_raw]","4c2af043":"market_train_fill[column_mkt_raw_diff].describe()","1a6e7fd0":"assetCode_df = market_train_df.groupby('assetCode')['volume'].sum().sort_values(ascending=False)\nprint('There are %i unique asset code' %len(assetCode_df))","84029489":"unknown_name = market_train_fill[market_train_fill['assetName']=='Unknown']\nunknown_count = unknown_name['assetCode'].value_counts().sort_values(ascending=False)","02983157":"print('There are %i unique asset code with unknown asset name' %len(unknown_count))","4c3c529f":"unknown_count[:15].plot.barh()\nplt.ylabel('assetCode')\nplt.xlabel('Counts')\nplt.title('Top 15 asset code with Unknown asset name')\nplt.gca().invert_yaxis()","6bcd57bd":"assetCode_df[:15].plot.barh()\nplt.ylabel('assetCode')\nplt.xlabel('Trading volume')\nplt.title('Top 15 asset code by volume')\nplt.gca().invert_yaxis()","a32ee2d9":"assetName_Volume = market_train_df.groupby('assetName')['volume'].sum().sort_values(ascending=False)\nassetName_Volume[:15].plot.barh()\nplt.ylabel('assetName')\nplt.xlabel('Trading volume')\nplt.title('Top 15 asset name by volume')\nplt.gca().invert_yaxis()\ndel assetName_Volume","80b9acd4":"assetName_code = market_train_df.groupby('assetName')['assetCode'].nunique().reset_index().sort_values(by='assetCode',ascending=False)","ed0ecc10":"assetCodeCount = assetName_code.groupby('assetCode')['assetName'].count().reset_index()\nassetCodeCount.columns = ['assetCodeNo', 'counts']\nassetCodeCount.head()\ndel assetCodeCount","38a61d84":"columns_corr_market = ['volume', 'open', 'close','returnsClosePrevRaw1','returnsOpenPrevRaw1',\\\n           'returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevMktres10','returnsOpenPrevRaw10',\\\n           'returnsClosePrevMktres10', 'returnsOpenPrevMktres10', 'returnsOpenNextMktres10']\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(18,15))\nsns.heatmap(market_train_fill[columns_corr_market].astype(float).corr(), linewidths=0.1, vmax=1.0, vmin=-1., square=True, cmap=colormap, linecolor='white', annot=True)\nplt.title('Pair-wise correlation')","2f288338":"assetCode = 'Bank of America Corp'\nthisAssetMark_df = market_train_fill[market_train_fill['assetName']==assetCode].sort_values(by='date',ascending=True) \nthisAssetMark_df['diff_open_close'] = thisAssetMark_df['open'] - thisAssetMark_df['close']\nthisAssetNews_df = news_rmv_outlier[news_rmv_outlier['assetName']==assetCode]\n# Trading volume vs time\nthisAssetMark_df.plot(x='date', y='volume')\nplt.title('Trading volume vs time')\n# Price vs time\nthisAssetMark_df.plot(x='date', y='open')\nplt.title('Open price vs time')\n# Return vs time\nthisAssetMark_df.plot(x='date', y=['returnsOpenPrevRaw1', 'returnsOpenPrevRaw10','returnsOpenNextMktres10'], alpha=0.8)\nplt.title('Return vs time')","5d829fe3":"news_volume = thisAssetNews_df.groupby('date')['sourceId'].count().reset_index()\nnews_volume = news_volume.ewm(span=10).mean()\nnews_volume.plot(x='date',y='sourceId')\nplt.title('News volume vs time')","7b18176e":"news_urgency = thisAssetNews_df.groupby('date')['urgency'].mean().reset_index()\nnews_urgency = news_urgency.ewm(span=10).mean()\nnews_urgency.plot(x='date',y='urgency')\nplt.title('News urgency vs time')","52bec5ca":"news_relevance = thisAssetNews_df.groupby('date')['relevance'].mean().reset_index()\nnews_relevance = news_relevance.ewm(span=10).mean()\nnews_relevance.plot(x='date',y='relevance')\nplt.title('Relevance vs time')","6b6142cc":"news_sentiment = thisAssetNews_df.groupby('date')['sentimentClass','sentimentNegative','sentimentNeutral','sentimentPositive'].mean().reset_index()\nnews_sentiment = news_sentiment.ewm(span=10).mean()\nnews_sentiment.plot(x='date',y=['sentimentClass','sentimentNegative','sentimentNeutral','sentimentPositive'], alpha=0.8)\nplt.title('Sentiment vs time')","ae2c6393":"# Merge news and market data. Only keep numeric columns\nthisAssetMark_number = thisAssetMark_df[columns_corr_market+['date']]\nthisAssetMark_number = thisAssetMark_number.groupby('date').mean().reset_index()\nthisAssetNews_number = thisAssetNews_df[columns_corr+['date']]\nthisAssetNews_number = thisAssetNews_number.groupby('date').mean().reset_index()\nthisAssetNews_number['news_volume'] = thisAssetNews_df.groupby('date')['sourceId'].count().reset_index()['sourceId']\nthisAssetMerge = pd.merge(thisAssetMark_number, thisAssetNews_number, how='left', on = 'date')","6963bdd7":"columns_corr_merge = ['volume','open','close','returnsOpenPrevRaw1','returnsOpenPrevMktres1','returnsOpenPrevRaw10','returnsOpenPrevMktres10',\\\n                     'returnsOpenNextMktres10','news_volume','urgency','sentenceCount','relevance','sentimentClass',\\\n                     'noveltyCount24H','noveltyCount5D','volumeCounts24H','volumeCounts5D']\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(18,15))\nsns.heatmap(thisAssetMerge[columns_corr_merge].astype(float).corr(), linewidths=0.1, vmax=1.0, vmin=-1., square=True, cmap=colormap, linecolor='white', annot=True)\nplt.title('Pair-wise correlation market and news')","18087c79":"del thisAssetMark_df\ndel news_relevance\ndel market_train_fill\ndel news_train_df\ndel news_rmv_outlier","56f9fd17":"market_train_orig = market_train_orig.sort_values('time')\nnews_train_orig = news_train_orig.sort_values('time')\nmarket_train_df = market_train_orig.copy()\nnews_train_df = news_train_orig.copy()\ndel market_train_orig\ndel news_train_orig","070cf6ec":"market_train_df = market_train_df.loc[market_train_df['time'].dt.date>=datetime.date(2009,1,1)]\nnews_train_df = news_train_df.loc[news_train_df['time'].dt.date>=datetime.date(2009,1,1)]","b00b7a9e":"market_train_df['close_open_ratio'] = np.abs(market_train_df['close']\/market_train_df['open'])\nthreshold = 0.5\nprint('In %i lines price increases by 50%% or more in a day' %(market_train_df['close_open_ratio']>=1.5).sum())\nprint('In %i lines price decreases by 50%% or more in a day' %(market_train_df['close_open_ratio']<=0.5).sum())","6de39ef7":"market_train_df = market_train_df.loc[market_train_df['close_open_ratio'] < 1.5]\nmarket_train_df = market_train_df.loc[market_train_df['close_open_ratio'] > 0.5]\nmarket_train_df = market_train_df.drop(columns=['close_open_ratio'])","5d9aa214":"column_market = ['returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\ncolumn_raw = ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1','returnsClosePrevRaw10', 'returnsOpenPrevRaw10']\nfor i in range(len(column_raw)):\n    market_train_df[column_market[i]] = market_train_df[column_market[i]].fillna(market_train_df[column_raw[i]])","51e864f9":"print('Removing outliers ...')\ncolumn_return = column_market + column_raw + ['returnsOpenNextMktres10']\norig_len = market_train_df.shape[0]\nfor column in column_return:\n    market_train_df = market_train_df.loc[market_train_df[column]>=-2]\n    market_train_df = market_train_df.loc[market_train_df[column]<=2]\nnew_len = market_train_df.shape[0]\nrmv_len = np.abs(orig_len-new_len)\nprint('There were %i lines removed' %rmv_len)","36b748d4":"print('Removing strange data ...')\norig_len = market_train_df.shape[0]\nmarket_train_df = market_train_df[~market_train_df['assetCode'].isin(['PGN.N','EBRYY.OB'])]\n#market_train_df = market_train_df[~market_train_df['assetName'].isin(['Unknown'])]\nnew_len = market_train_df.shape[0]\nrmv_len = np.abs(orig_len-new_len)\nprint('There were %i lines removed' %rmv_len)","af24a122":"# Function to remove outliers\ndef remove_outliers(data_frame, column_list, low=0.02, high=0.98):\n    for column in column_list:\n        this_column = data_frame[column]\n        quant_df = this_column.quantile([low,high])\n        low_limit = quant_df[low]\n        high_limit = quant_df[high]\n        data_frame[column] = data_frame[column].clip(lower=low_limit, upper=high_limit)\n    return data_frame","b31769b1":"# Remove outlier\ncolumns_outlier = ['takeSequence', 'bodySize', 'sentenceCount', 'wordCount', 'sentimentWordCount', 'firstMentionSentence','noveltyCount12H',\\\n                  'noveltyCount24H', 'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', 'volumeCounts12H', 'volumeCounts24H',\\\n                  'volumeCounts3D','volumeCounts5D','volumeCounts7D']\nprint('Clipping news outliers ...')\nnews_train_df = remove_outliers(news_train_df, columns_outlier)","e3ec39ba":"asset_code_dict = {k: v for v, k in enumerate(market_train_df['assetCode'].unique())}\ndrop_columns = [col for col in news_train_df.columns if col not in ['sourceTimestamp', 'urgency', 'takeSequence', 'bodySize', 'companyCount', \n               'sentenceCount', 'firstMentionSentence', 'relevance','firstCreated', 'assetCodes']]\ncolumns_news = ['firstCreated','relevance','sentimentClass','sentimentNegative','sentimentNeutral',\n               'sentimentPositive','noveltyCount24H','noveltyCount7D','volumeCounts24H','volumeCounts7D','assetCodes','sourceTimestamp',\n               'assetName','audiences', 'urgency', 'takeSequence', 'bodySize', 'companyCount', \n               'sentenceCount', 'firstMentionSentence','time']","64fa45ce":"# Data processing function\ndef data_prep(market_df,news_df):\n    market_df['date'] = market_df.time.dt.date\n    market_df['close_to_open'] = market_df['close'] \/ market_df['open']\n    market_df.drop(['time'], axis=1, inplace=True)\n    \n    news_df = news_df[columns_news]\n    news_df['sourceTimestamp']= news_df.sourceTimestamp.dt.hour\n    news_df['firstCreated'] = news_df.firstCreated.dt.date\n    news_df['assetCodesLen'] = news_df['assetCodes'].map(lambda x: len(eval(x)))\n    news_df['assetCodes'] = news_df['assetCodes'].map(lambda x: list(eval(x))[0])\n    news_df['asset_sentiment_count'] = news_df.groupby(['assetName', 'sentimentClass'])['time'].transform('count')\n    news_df['len_audiences'] = news_train_df['audiences'].map(lambda x: len(eval(x)))\n    kcol = ['firstCreated', 'assetCodes']\n    news_df = news_df.groupby(kcol, as_index=False).mean()\n    market_df = pd.merge(market_df, news_df, how='left', left_on=['date', 'assetCode'], \n                            right_on=['firstCreated', 'assetCodes'])\n    del news_df\n    market_df['assetCodeT'] = market_df['assetCode'].map(asset_code_dict)\n    market_df = market_df.drop(columns = ['firstCreated','assetCodes','assetName']).fillna(0) \n    return market_df","f448df08":"print('Merging data ...')\nmarket_train_df = data_prep(market_train_df, news_train_df)\nmarket_train_df.head()","0dd9e78f":"market_train_df = market_train_df.loc[market_train_df['date']>=datetime.date(2009,1,1)]","befc65ea":"num_columns = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1', 'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', \n               'returnsClosePrevMktres10', 'returnsOpenPrevMktres10', 'close_to_open', 'sourceTimestamp', 'urgency', 'companyCount', 'takeSequence', 'bodySize', 'sentenceCount',\n               'relevance', 'sentimentClass', 'sentimentNegative', 'sentimentNeutral', 'sentimentPositive',\n               'noveltyCount24H','noveltyCount7D','volumeCounts24H','volumeCounts7D','assetCodesLen', 'asset_sentiment_count', 'len_audiences']\ncat_columns = ['assetCodeT']\nfeature_columns = num_columns+cat_columns","ce724567":"# Scaling of data\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\ndata_scaler = StandardScaler()\n#market_train_df[num_columns] = data_scaler.fit_transform(market_train_df[num_columns])\n#data_scaler = MinMaxScaler()\nmarket_train_df[num_columns] = data_scaler.fit_transform(market_train_df[num_columns])","3fc2d4a8":"from sklearn.model_selection import train_test_split\n\nmarket_train_df = market_train_df.reset_index()\nmarket_train_df = market_train_df.drop(columns='index')\n\n# Random train-test split\ntrain_indices, val_indices = train_test_split(market_train_df.index.values,test_size=0.1, random_state=92)","538554df":"# Extract X and Y\ndef get_input(market_train, indices):\n    X = market_train.loc[indices, feature_columns].values\n    y = market_train.loc[indices,'returnsOpenNextMktres10'].map(lambda x: 0 if x<0 else 1).values\n    #y = market_train.loc[indices,'returnsOpenNextMktres10'].map(lambda x: convert_to_class(x)).values\n    r = market_train.loc[indices,'returnsOpenNextMktres10'].values\n    u = market_train.loc[indices, 'universe']\n    d = market_train.loc[indices, 'date']\n    return X,y,r,u,d\n\n# r, u and d are used to calculate the scoring metric\nX_train,y_train,r_train,u_train,d_train = get_input(market_train_df, train_indices)\nX_val,y_val,r_val,u_val,d_val = get_input(market_train_df, val_indices)","11a2ad16":"# Set up decay learning rate\ndef learning_rate_power(current_round):\n    base_learning_rate = 0.19000424246380565\n    min_learning_rate = 0.01\n    lr = base_learning_rate * np.power(0.995,current_round)\n    return max(lr, min_learning_rate)","a6d89c87":"from scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\ntune_params = {'n_estimators': [200,500,1000,2500,5000],\n              'max_depth': sp_randint(4,12),\n              'colsample_bytree':sp_uniform(loc=0.8, scale=0.15),\n              'min_child_samples':sp_randint(60,120),\n              'subsample': sp_uniform(loc=0.75, scale=0.25),\n              'reg_lambda':[1e-3, 1e-2, 1e-1, 1]}\n\nfit_params = {'early_stopping_rounds':40,\n              'eval_metric': 'accuracy',\n              'eval_set': [(X_train, y_train), (X_val, y_val)],\n              'verbose': 20,\n              'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_power)]}","01886426":"lgb_clf = lgb.LGBMClassifier(n_jobs=4, objective='binary',random_state=1)\ngs = RandomizedSearchCV(estimator=lgb_clf, \n                        param_distributions=tune_params, \n                        n_iter=40,\n                        scoring='f1',\n                        cv=5,\n                        refit=True,\n                        random_state=1,\n                        verbose=True)","bace8189":"lgb_clf = lgb.LGBMClassifier(n_jobs=4,\n                             objective='multiclass',\n                            random_state=100)\nopt_params = {'n_estimators':500,\n              'boosting_type': 'dart',\n              'objective': 'binary',\n              'num_leaves':2452,\n              'min_child_samples':212,\n              'reg_lambda':0.01}\nlgb_clf.set_params(**opt_params)\nlgb_clf.fit(X_train, y_train,**fit_params)","ca74ceb2":"print('Training accuracy: ', accuracy_score(y_train, lgb_clf.predict(X_train)))\nprint('Validation accuracy: ', accuracy_score(y_val, lgb_clf.predict(X_val)))","5d625603":"features_imp = pd.DataFrame()\nfeatures_imp['features'] = list(feature_columns)[:]\nfeatures_imp['importance'] = lgb_clf.feature_importances_\nfeatures_imp = features_imp.sort_values(by='importance', ascending=False).reset_index()\n\ny_plot = -np.arange(15)\nplt.figure(figsize=(10,6))\nplt.barh(y_plot, features_imp.loc[:14,'importance'].values)\nplt.yticks(y_plot,(features_imp.loc[:14,'features']))\nplt.xlabel('Feature importance')\nplt.title('Features importance')\nplt.tight_layout()","0fe8a97c":"# Rescale confidence\ndef rescale(data_in, data_ref):\n    scaler_ref =  StandardScaler()\n    scaler_ref.fit(data_ref.reshape(-1,1))\n    scaler_in = StandardScaler()\n    data_in = scaler_in.fit_transform(data_in.reshape(-1,1))\n    data_in = scaler_ref.inverse_transform(data_in)[:,0]\n    return data_in","a12a10e6":"def confidence_out(y_pred):\n    confidence = np.zeros(y_pred.shape[0])\n    for i in range(len(confidence)):\n        if y_pred[i,:].argmax() != 1:\n            confidence[i] = y_pred[i,2]-y_pred[i,0]\n    return confidence","e09e5387":"y_pred_proba = lgb_clf.predict_proba(X_val)\npredicted_return = y_pred_proba[:,1] - y_pred_proba[:,0]\n#predicted_return = confidence_out(y_pred_proba)\npredicted_return = rescale(predicted_return, r_train)","08e53925":"# distribution of confidence that will be used as submission\nplt.hist(predicted_return, bins='auto', label='Predicted confidence')\nplt.hist(r_val, bins='auto',alpha=0.8, label='True market return')\nplt.title(\"predicted confidence\")\nplt.legend(loc='best')\nplt.xlim(-1,1)\nplt.show()","99b567eb":"# calculation of actual metric that is used to calculate final score\nr_val = r_val.clip(-1,1) # get rid of outliers.\nx_t_i = predicted_return * r_val * u_val\ndata = {'day' : d_val, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_valid = mean \/ std\nprint('Validation score', score_valid)","e36eb15b":"# This code is inspired from this kernel: https:\/\/www.kaggle.com\/skooch\/lgbm-w-random-split-2\nclfs = []\nfor i in range(20):\n    clf = lgb.LGBMClassifier(learning_rate=0.1, random_state=1200+i, silent=True,\n                             n_jobs=4, n_estimators=2500)\n    clf.set_params(**opt_params)\n    clfs.append(('lgbm%i'%i, clf))\n\ndef split_data(X, y, test_percentage=0.2, seed=None):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_percentage)\n    return X_train, y_train, X_test, y_test \n\ndef _parallel_fit_estimator(estimator, X, y, sample_weight=None, **fit_params):\n    \n    # randomly split the data so we have a test set for early stopping\n    X_train, y_train, X_test, y_test = split_data(X, y, seed=1992)\n    \n    # update the fit params with our new split\n    fit_params[\"eval_set\"] = [(X_train,y_train), (X_test,y_test)]\n    \n    # fit the estimator\n    if sample_weight is not None:\n        estimator.fit(X_train, y_train, sample_weight=sample_weight, **fit_params)\n    else:\n        estimator.fit(X_train, y_train, **fit_params)\n    return estimator","16d497ab":"class VotingClassifierLGBM(VotingClassifier):\n    '''\n    This implements the fit method of the VotingClassifier propagating fit_params\n    '''\n    def fit(self, X, y, sample_weight=None, **fit_params):\n        \n        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\n            raise NotImplementedError('Multilabel and multi-output'\n                                      ' classification is not supported.')\n\n        if self.voting not in ('soft', 'hard'):\n            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\"\n                             % self.voting)\n\n        if self.estimators is None or len(self.estimators) == 0:\n            raise AttributeError('Invalid `estimators` attribute, `estimators`'\n                                 ' should be a list of (string, estimator)'\n                                 ' tuples')\n\n        if (self.weights is not None and\n                len(self.weights) != len(self.estimators)):\n            raise ValueError('Number of classifiers and weights must be equal'\n                             '; got %d weights, %d estimators'\n                             % (len(self.weights), len(self.estimators)))\n\n        if sample_weight is not None:\n            for name, step in self.estimators:\n                if not has_fit_parameter(step, 'sample_weight'):\n                    raise ValueError('Underlying estimator \\'%s\\' does not'\n                                     ' support sample weights.' % name)\n        names, clfs = zip(*self.estimators)\n        self._validate_names(names)\n\n        n_isnone = np.sum([clf is None for _, clf in self.estimators])\n        if n_isnone == len(self.estimators):\n            raise ValueError('All estimators are None. At least one is '\n                             'required to be a classifier!')\n\n        self.le_ = LabelEncoder().fit(y)\n        self.classes_ = self.le_.classes_\n        self.estimators_ = []\n\n        transformed_y = self.le_.transform(y)\n\n        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n                delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,\n                                                 sample_weight=sample_weight, **fit_params)\n                for clf in clfs if clf is not None)\n\n        return self","609ac34c":"vc = VotingClassifierLGBM(clfs, voting='soft')\nvc.fit(X_train, y_train, **fit_params)\nfilename = 'VotingClassifierLGBM.sav'\npickle.dump(vc, open(filename, 'wb'))","968726a3":"vc = pickle.load(open(filename, 'rb'))\nvc.voting = 'soft'\npredicted_class = vc.predict(X_val)\npredicted_return = vc.predict_proba(X_val)\n#predicted_return = confidence_out(predicted_return)\npredicted_return = vc.predict_proba(X_val)[:,1]*2-1\npredicted_return = rescale(predicted_return, r_train)","e2465104":"plt.hist(predicted_class, bins='auto')","1b5122fd":"vc.voting = 'soft'\nglobal_accuracy_soft = accuracy_score(y_val, predicted_class)\nglobal_f1_soft = f1_score(y_val, predicted_class)\nprint('Accuracy score clfs: %f' % global_accuracy_soft)\nprint('F1 score clfs: %f' % global_f1_soft)","89ff7aff":"# distribution of confidence that will be used as submission\nplt.hist(predicted_return, bins='auto', label='Prediciton')\nplt.hist(r_val, bins='auto',alpha=0.8, label='True data')\nplt.title(\"predicted confidence\")\nplt.legend(loc='best')\nplt.xlim(-1,1)\nplt.show()","9ff20c42":"# calculation of actual metric that is used to calculate final score\nr_val = r_val.clip(-1,1) # get rid of outliers. Where do they come from??\nx_t_i = predicted_return * r_val * u_val\ndata = {'day' : d_val, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_valid = mean \/ std\nprint('Validation score', score_valid)","3b8ffe26":"days = env.get_prediction_days()\nn_days = 0\nprep_time = 0\nprediction_time = 0\npackaging_time = 0\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days +=1\n    if n_days % 50 == 0:\n        print(n_days,end=' ')\n\n    t = time.time()\n    column_market = ['returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\n    column_raw = ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1','returnsClosePrevRaw10', 'returnsOpenPrevRaw10']\n    market_obs_df['close_open_ratio'] = np.abs(market_obs_df['close']\/market_obs_df['open'])\n    for i in range(len(column_raw)):\n        market_obs_df[column_market[i]] = market_obs_df[column_market[i]].fillna(market_obs_df[column_raw[i]])\n\n    market_obs_df = market_obs_df[market_obs_df.assetCode.isin(predictions_template_df.assetCode)]\n    market_obs_df = market_obs_df[market_obs_df.assetCode.isin(asset_code_dict.keys())]\n    market_obs = data_prep(market_obs_df, news_obs_df)\n    market_obs[num_columns] = data_scaler.transform(market_obs[num_columns])\n    X_live = market_obs[feature_columns].values\n    prep_time += time.time() - t\n\n    t = time.time()\n    lp = vc.predict_proba(X_live)\n    prediction_time += time.time() -t\n\n    t = time.time()\n    confidence = lp[:,1] - lp[:,0]\n    #confidence = confidence_out(lp)\n    confidence = rescale(confidence, r_train)\n    preds = pd.DataFrame({'assetCode':market_obs['assetCode'],'confidence':confidence})\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    env.predict(predictions_template_df)\n    packaging_time += time.time() - t\n\nenv.write_submission_file()","a577138e":"plt.hist(confidence, bins='auto')\nplt.title(\"predicted confidence\")\nplt.show()","41025f10":"### Dig deeper to a single asset\n\nLet's take a closer look to a single asset. Here I choose the one with largest trading volumen: 'Bank of America Corp'","1e002ee0":"**Sentiment word count and relevance**","1bb04c50":"Running the parameters search will take another 3 hours, so we will straight away use the best parameters ","0f8ca8c8":"### Plot data versus time","b1228d1c":"### Asset Name","d7d8fcd0":"### Time delay","0b7fe877":"We then perform feature selection . Feature scaling is not needed since we plan to use lightgbm - a tree-based model, which do not require standardization.\n\nI tried using a regressor model, but a problem is that it gives close-to-0 values for most of prediction. Thus, I convert this problem into a classification problem: 0 for negative return and 1 for positive return","050982f8":"# Complete EDA + features engineering + voting LightGBM\nNguyen Dang Minh, PhD\n\n* [Loading the data](#load_data)\n* [Exploring news data](#explore_news)\n* [Exploring market data](#explore_market)\n* [Preprocessing](#preprocessing)\n* [Features engineering](#feature_engineering)\n* [Building model](#building_model)\n* [Making submission](#making_submission)","12b026d3":"There is a maximum peak every quarter (time for quaterly financial report) and a minimum peak at the end of the year (time for Christmast holliday.)","39541cf4":"**First sentence and urgency**","271617a7":"* **Remove strange data**: Here we remove data with unknown asset name or asset codes with strange behavior. For more details, see here: https:\/\/www.kaggle.com\/nareyko\/market-return-estimation-and-bad-data-detection","a9b28e57":"### News data\n* **Remove outliers**: apply a clip filter to reduce too extreme data","b770b61b":"<a id='voting_ensemble'><\/a>\n### Voting ensemble\nNow we construct an ensemble of multiple classifier and use soft voting to get the final result","0eab8588":"<a id='data_selection'><\/a>\n\n### Data selection\n\nLooking at the statistics, most data behave homogeneously after 2009 (volume increase, price increase, etc.). However, before 2009, due to the burst of the housing bubble that leads to the financial crisis in 2008, the data behaves differently. So the question to make the right prediction for this problem is: **Will there be a financial crisis in the next 6 months?** If the answer is **Yes**, then we include data before 2009. If the answer is **No**, then we exclude them.\n\nIn this notebook, I choose **No** as the answer and proceed from that.","2c9764c8":"**Some preprocessing:**\n* Sort data in chronological order\n* All NAN data comes from the market adjusted column. We fill them up with the raw value data","420cfded":"### Urgency","556c631a":"<a id='building_model'><\/a>\n\n## Building model\n\nHere we use lightgbm classifier as our model\n\n### Parameters tuning","85472850":"It can be seen that trading volume is strongly associated with price, i.e. trade increase when price hits bottom. Return is also strongly fluctuated at such time","4bbc56d7":"### Take sequence","b383e66a":"The difference between raw return and market adjusted returns are negligible, but there are some extreme values. Those values are noise and needs to be taken care of","e4e3b79d":"### Asset name","f24e11c6":"This concludes my work for this problem. Please let me know if you have any suggestion. Thank you","c16df0bc":"**Sentiment ratio**","f2669ef9":"<a id='visualinzg_result'><\/a>\n\n### Visualizing the result","683482d5":"### Asset codes","9075f1e0":"**The vast majority of companies has only one asset code**. One '*company*' that has 110 actually is the  'Unknown' category. Magically, some companies don't even have any asset code. Currently I have no explanation for this.","cd9cb5ad":"### Audiences","7b52baf9":"The news increases in volumes and urgency as price drops","9b9fcdf8":"### Subjects","acc90e85":"* **Outliers-Returns:** Return should not exceed 50% or falls below 50%. If it does, it is either noise, or extreme data that will confuse our prediction later on. We remove these extreme data.","828cd01c":"### Company Count","ae5641a8":"In this notebook, I  will present my statistical analysis on both the news and market data of the Kaggle problem: [Using News to Predict Stock Movements](http:\/\/https:\/\/www.kaggle.com\/c\/two-sigma-financial-news)","759a9905":"<a id='feature_engineering'><\/a>\n\n## Features engineering\n\n### Data processing function\nHere we make a function process both market and news data, then merge them.\n","5b8d74eb":"**First sentence and relevance**","8477ca05":"### Correlations","94c36a66":"Sentiments are mostly negative. Sentiment drops as price drops, which is expected.\n\nNow let's merge the news and market data and see their correlations","3ec22309":"### Remove outliers and plot correlation","77767e8c":"<a id='making_submission'><\/a>\n## Making submission","5c377a36":"### Market data\n* **Outliers - Open to close:** the difference between open price and close price cannot be too much difference (market would corrupt otherwise). We treat these outliers by clipping the close-to-open ratio","efa4d984":"Most headlines are blank. This properties may not be important.","7178fd65":"<a id='load_data'><\/a>\n\n## Load the data","19a28d84":"### Evolutions over time","4a49ac1b":"* **Fill nulls - Market values:** All null data comes from market adjusted columns. We fill them up with the raw values in the same row","cbb45952":"<a id='preprocessing'><\/a>\n\n## Preprocessing","8fa8882f":"### Providers","0269b782":"This concludes the exploratory analysis. I will now proceed on data preprocessing and model building","57231e29":"<a id='explore_market'><\/a>\n\n## Explore market data","a4e60161":"The volume ranking by coorperation seems to be the same as the rank of asset codes they own, e.g. the one with most popular codes has the most trading volume","0b30d927":"### First sentence - Urgency - relevance - sentiment Word Count","20615d65":"### Difference between raw values and market adjusted values\n\nLet see if there's any difference between raw return and market adjusted return","156ac82e":"<a id='explore_news'><\/a>\n\n## Explore news data","81fb59d5":"### Head line tag"}}