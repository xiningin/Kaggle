{"cell_type":{"f4150f26":"code","504aa219":"code","b35cd008":"code","b79ec7e5":"code","ee594e60":"code","ccb267e1":"code","9fd19a60":"code","1d616de8":"code","465ba798":"code","9277ff30":"code","beaa005d":"code","10d21a64":"code","110afde7":"code","ccb79ed2":"code","ac925f5f":"code","6a565b52":"code","f8c1d80a":"code","39f7f369":"code","de5058de":"code","58dfab3f":"code","ba8ea3df":"code","9fb27b1d":"code","4e53ba0f":"code","085557fb":"code","c74ecc4f":"code","3ec58a45":"code","aed3e113":"code","71941171":"code","21ae52ca":"code","9ff5963f":"code","8b5f2243":"code","fbcaba2d":"code","009e0154":"markdown","1122b2c5":"markdown","009e028c":"markdown","83f2308a":"markdown","fcb7f0a2":"markdown","bcd43429":"markdown","9e2ac0bd":"markdown","e821b087":"markdown","d8c92dcf":"markdown","9dfd6fc9":"markdown","0d43a454":"markdown","efa0c04c":"markdown","5eb0bc36":"markdown","e2a64997":"markdown","c170d950":"markdown","64e2c24f":"markdown","64025c1e":"markdown","ff62f46b":"markdown","1774c292":"markdown","ff628827":"markdown","ec7d40ef":"markdown","39d21967":"markdown","e61c07a4":"markdown","a1c7a27e":"markdown","df2051e7":"markdown","dd051ba3":"markdown","a610f727":"markdown","c240736d":"markdown","0d8b97b5":"markdown","8323a2fe":"markdown"},"source":{"f4150f26":"%load_ext autoreload\n%autoreload 2","504aa219":"from itertools import groupby\nfrom pathlib import Path\nfrom typing import cast, Iterable, Iterator, Optional, Tuple\nfrom typing_extensions import Literal\n\nfrom IPython.display import display\nimport numpy as np\nimport pandas as pd\nfrom sklearn.utils import indexable\nfrom sklearn.utils.validation import _num_samples\n\nfrom matplotlib.patches import Patch\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n%matplotlib inline","b35cd008":"!ls -alh ..","b79ec7e5":"RANDOM_STATE = 42\n\nIMAGES_PATH = Path('..\/images')\nIMAGES_PATH.mkdir(exist_ok=True)","ee594e60":"rng = np.random.default_rng(RANDOM_STATE)\n\nstart_date = '2021-01-01'\nend_date = '2021-12-01'\n\nunique_months = pd.date_range(start_date, end_date, freq='MS').strftime('%Y-%m')\ntimes = rng.integers(low=1, high=5, size=len(unique_months))\nmonths = unique_months.repeat(times)\nmonths","ccb267e1":"groups, _ = pd.factorize(months)\ngroups","9fd19a60":"rng = np.random.default_rng(RANDOM_STATE)\n\ndata_size = len(groups)\n\nnum_feature = rng.random(data_size)\nX = pd.DataFrame({\n    'num_feature': num_feature\n}, index=months)\n\ntargets = rng.choice([0, 1], size=data_size)\ny = pd.Series(targets, name='target')\n\ndisplay(X.head(), y.head())","1d616de8":"class GroupTimeSeriesSplit:\n    \"\"\"Time series cross validation with custom grouping.\"\"\"\n\n    def __init__(\n        self,\n        test_size: int,\n        train_size: Optional[int] = None,\n        n_splits: Optional[int] = None,\n        gap: int = 0,\n        shift_size: int = 1,\n        window: Literal['rolling', 'expanding'] = 'rolling'\n    ) -> None:\n        \"\"\"Initializes cross validation parameters.\n\n        Args:\n            test_size (int):\n                Size of test dataset.\n            train_size (Optional[int], optional):\n                Size of train dataset. Defaults to None.\n            n_splits (int, optional):\n                Number of splits. Defaults to None.\n            gap (int, optional):\n                Gap size. Defaults to 0.\n            shift_size (int, optional):\n                Step to shift for the next fold. Defaults to 1.\n            window (str):\n                Type of the window. Defaults to 'rolling'.\n        \"\"\"\n        if (train_size is None) and (n_splits is None):\n            raise ValueError('Either train_size or n_splits have to be defined')\n\n        if window not in ['rolling', 'expanding']:\n            raise ValueError('Window can be either \"rolling\" or \"expanding\"')\n\n        if (train_size is not None) and (window == 'expanding'):\n            raise ValueError('Train size can be specified only with rolling window')\n\n        self.test_size = test_size\n        self.train_size = train_size\n        self.n_splits = n_splits\n        self.gap = gap\n        self.shift_size = shift_size\n        self.window = window\n\n    def split(self,\n              X: Iterable,\n              y: Optional[Iterable] = None,\n              groups: Optional[Iterable] = None) -> Iterator[Tuple[np.ndarray, np.ndarray]]:\n        \"\"\"Calculates train\/test indices based on split parameters.\n\n        Args:\n            X (Iterable):\n                Dataset with features.\n            y (Iterable):\n                Dataset with target.\n            groups (Iterable):\n                Array with group numbers.\n\n        Yields:\n            Iterator[Tuple[np.ndarray, np.ndarray]]:\n                Train\/test dataset indices.\n        \"\"\"\n        test_size = self.test_size\n        gap = self.gap\n        shift_size = self.shift_size\n\n        # Convert to indexable data structures with additional lengths consistency check\n        X, y, groups = indexable(X, y, groups)\n\n        # Check if groups are specified\n        if groups is None:\n            raise ValueError('Groups must be specified')\n\n        # Check if groups are sorted in dataset\n        group_seqs = [group[0] for group in groupby(groups)]\n        unique_groups, group_starts_idx = np.unique(groups, return_index=True)\n        n_groups = _num_samples(unique_groups)\n        self._n_groups = n_groups\n\n        if group_seqs != sorted(unique_groups):\n            raise ValueError('Groups must be presorted in increasing order')\n\n        # Create mapping between groups and its start indices in array\n        groups_dict = dict(zip(unique_groups, group_starts_idx))\n\n        # Calculate number of samples\n        n_samples = _num_samples(X)\n\n        # Calculate remaining split params\n        self._calculate_split_params()\n        train_size = cast(int, self.train_size)\n        n_splits = cast(int, self.n_splits)\n        train_start_idx = self._train_start_idx\n\n        # Calculate start\/end indices for initial train\/test datasets\n        train_end_idx = train_start_idx + train_size\n        test_start_idx = train_end_idx + gap\n        test_end_idx = test_start_idx + test_size\n\n        # Process each split\n        for _ in range(n_splits):\n            # Calculate train indices range\n            train_idx = np.r_[slice(groups_dict[train_start_idx], groups_dict[train_end_idx])]\n\n            # Calculate test indices range\n            if test_end_idx < n_groups:\n                test_idx = np.r_[slice(groups_dict[test_start_idx], groups_dict[test_end_idx])]\n            else:\n                test_idx = np.r_[slice(groups_dict[test_start_idx], n_samples)]\n\n            # Yield train\/test indices range\n            yield (train_idx, test_idx)\n\n            # Shift train dataset start index by shift size for rolling window\n            if self.window == 'rolling':\n                train_start_idx = train_start_idx + shift_size\n\n            # Shift train dataset end index by shift size\n            train_end_idx = train_end_idx + shift_size\n\n            # Shift test dataset indices range by shift size\n            test_start_idx = test_start_idx + shift_size\n            test_end_idx = test_end_idx + shift_size\n\n    def get_n_splits(\n        self, X: Iterable, y: Optional[Iterable] = None, groups: Optional[Iterable] = None\n    ) -> int:\n        \"\"\"Calculates number of splits given specified parameters.\n\n        Args:\n            X (Iterable):\n                Dataset with features. Defaults to None.\n            y (Optional[Iterable], optional):\n                Dataset with target. Defaults to None.\n            groups (Optional[Iterable], optional):\n                Array with group numbers. Defaults to None.\n\n        Returns:\n            int:\n                Calculated number of splits.\n        \"\"\"\n        if self.n_splits is not None:\n            return self.n_splits\n        else:\n            raise ValueError('Number of splits is not defined')\n\n    def _calculate_split_params(self) -> None:\n        train_size = self.train_size\n        test_size = self.test_size\n        n_splits = self.n_splits\n        gap = self.gap\n        shift_size = self.shift_size\n        n_groups = self._n_groups\n\n        not_enough_data_error = (\n            'Not enough data to split number of groups ({0})'\n            ' for number splits ({1})'\n            ' with train size ({2}),'\n            ' test size ({3}), gap ({4}), shift_size ({5})'\n        )\n\n        if (train_size is None) and (n_splits is not None):\n            train_size = n_groups - gap - test_size - (n_splits - 1) * shift_size\n            self.train_size = train_size\n\n            if train_size <= 0:\n                raise ValueError(\n                    not_enough_data_error.format(\n                        n_groups, n_splits, train_size, test_size, gap, shift_size\n                    )\n                )\n            train_start_idx = 0\n        elif (n_splits is None) and (train_size is not None):\n            n_splits = (n_groups - train_size - gap - test_size) \/\/ shift_size + 1\n            self.n_splits = n_splits\n\n            if self.n_splits <= 0:\n                raise ValueError(\n                    not_enough_data_error.format(\n                        n_groups, n_splits, train_size, test_size, gap, shift_size\n                    )\n                )\n            train_start_idx = n_groups - train_size - gap - test_size - (n_splits - 1) * shift_size\n        elif (n_splits is not None) and (train_size is not None):\n            train_start_idx = n_groups - train_size - gap - test_size - (n_splits - 1) * shift_size\n\n            if train_start_idx < 0:\n                raise ValueError(\n                    not_enough_data_error.format(\n                        n_groups, n_splits, train_size, test_size, gap, shift_size\n                    )\n                )\n\n        self._train_start_idx = train_start_idx\n","465ba798":"def test_cv(X, y, groups, **cv_args):\n    cv = GroupTimeSeriesSplit(**cv_args)\n    \n    for train_idx, test_idx in cv.split(X, groups=groups):\n        print('TRAIN INDICES:', train_idx)\n        print('TEST INDICES:', test_idx)\n        print('TRAIN LENGTH:', len(train_idx))\n        print('TEST LENGTH:', len(test_idx))\n        print('TRAIN GROUPS:', groups[train_idx]) \n        print('TEST GROUPS:', groups[test_idx])\n        print('TRAIN GROUP SIZE:', len(set(groups[train_idx]))) \n        print('TEST GROUP SIZE:', len(set(groups[test_idx])))\n        print('TRAIN GROUP MONTHS:', X.index[train_idx].values) \n        print('TEST GROUP MONTHS:', X.index[test_idx].values)\n        print()","9277ff30":"def plot_cv_indices(cv, X, y, groups, n_splits,\n                    image_file_path=None):\n    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n    \n    fig, ax = plt.subplots(figsize=(12, 4))\n    cmap_data = plt.cm.tab20\n    cmap_cv = plt.cm.coolwarm\n    lw = 10\n    marker_size = 200\n    \n    for split_idx, (train_idx, test_idx) in enumerate(cv.split(X=X, y=y, groups=groups)):\n        indices = np.array([np.nan] * len(X))\n        indices[test_idx] = 1\n        indices[train_idx] = 0\n        \n        ax.scatter(\n            range(len(X)),\n            [split_idx + 0.5] * len(X),\n            c=indices,\n            marker=\"_\",\n            lw=lw,\n            cmap=cmap_cv,\n            vmin=-0.4,\n            vmax=1.4,\n            s=marker_size\n        )\n    ax.scatter(\n        range(len(X)), [split_idx + 1.5] * len(X), \n        c=groups, marker=\"_\", lw=lw, \n        cmap=cmap_data, s=marker_size\n    )\n\n    yticklabels = list(range(n_splits)) + ['group']\n    ax.set(\n        yticks=np.arange(n_splits + 1) + 0.5,\n        yticklabels=yticklabels,\n        ylabel='CV iteration',\n        ylim=[n_splits + 1.2, -0.2],\n        xlim=[-0.5, len(indices) - 0.5]\n    )\n    ax.legend(\n        [Patch(color=cmap_cv(0.2)), Patch(color=cmap_cv(0.8))],\n        [\"Training set\", \"Testing set\"],\n        loc=(1.02, 0.8),\n        fontsize=13\n    )\n    \n    ax.set_title('{}\\n{}'.format(type(cv).__name__, cv_args), fontsize=15)\n    ax.xaxis.set_major_locator(MaxNLocator(min_n_ticks=len(X), integer=True))\n    ax.set_xlabel(xlabel='Sample index', fontsize=13)\n    ax.set_ylabel(ylabel='CV iteration', fontsize=13)\n    ax.tick_params(axis='both', which='major', labelsize=13)\n    ax.tick_params(axis='both', which='minor', labelsize=13)\n    \n    plt.tight_layout()\n    if image_file_path:\n        plt.savefig(image_file_path, bbox_inches='tight')\n    plt.show()\n    \ndef plot_cv(X, y, groups, image_file_path=None, **cv_args):\n    cv = GroupTimeSeriesSplit(**cv_args)\n    cv._n_groups = len(np.unique(groups))\n    cv._calculate_split_params()\n    n_splits = cv.n_splits\n    \n    plot_cv_indices(cv, X, y, groups, n_splits,\n                    image_file_path=image_file_path)","beaa005d":"cv_args = {\n    'test_size': 3, \n    'n_splits': 1\n}","10d21a64":"test_cv(X, y, groups, **cv_args)\nplot_cv(X, y, groups, \n        image_file_path=IMAGES_PATH\/'train_test_split.png',\n        **cv_args)","110afde7":"cv = GroupTimeSeriesSplit(**cv_args)\ntrain_idx, test_idx = next(cv.split(X, groups=groups))","ccb79ed2":"X_train = X.iloc[train_idx]\ny_train = y.iloc[train_idx]\ngroups_train = groups[train_idx]","ac925f5f":"rng = np.random.default_rng(RANDOM_STATE)\n\nunsorted_groups_train = groups_train.copy()\nrng.shuffle(unsorted_groups_train)\n\nunsorted_groups_train","6a565b52":"try:\n    test_cv(X_train, y_train, unsorted_groups_train, **cv_args)\nexcept ValueError as e:\n    print(e)","f8c1d80a":"cv_args = {\n    'test_size': 3\n}\n\ntry:\n    test_cv(X_train, y_train, groups_train, **cv_args)\nexcept ValueError as e:\n    print(e)","39f7f369":"cv_args = {\n    'test_size': 3,\n    'n_splits': 3,\n    'window': 'bad_name'\n}\n\ntry:\n    test_cv(X_train, y_train, groups_train, **cv_args)\nexcept ValueError as e:\n    print(e)","de5058de":"cv_args = {\n    'test_size': 3,\n    'train_size': 3,\n    'window': 'expanding'\n}\n\ntry:\n    test_cv(X_train, y_train, groups_train, **cv_args)\nexcept ValueError as e:\n    print(e)","58dfab3f":"cv_args = {\n    'test_size': 3,\n    'train_size': 7, \n    'n_splits': 1\n}\n\ntry:\n    test_cv(X_train, y_train, groups_train, **cv_args)\nexcept ValueError as e:\n    print(e)","ba8ea3df":"cv_args = {\n    'test_size': 3,\n    'n_splits': 10\n}\n\ntry:\n    test_cv(X_train, y_train, groups_train, **cv_args)\nexcept ValueError as e:\n    print(e)","9fb27b1d":"cv_args = {\n    'test_size': 3,\n    'n_splits': 3,\n    'shift_size': 10\n}\n\ntry:\n    test_cv(X_train, y_train, groups_train, **cv_args)\nexcept ValueError as e:\n    print(e)","4e53ba0f":"cv_args = {\n    'test_size': 3,\n    'n_splits': 3,\n    'gap': 10\n}\n\ntry:\n    test_cv(X_train, y_train, groups_train, **cv_args)\nexcept ValueError as e:\n    print(e)","085557fb":"cv_args = {\n    'test_size': 3,\n    'train_size': 3\n}\n\ntest_cv(X_train, y_train, groups_train, **cv_args)\nplot_cv(X_train, y_train, groups_train, \n        image_file_path=IMAGES_PATH\/'test_size_train_size_cv.png',\n        **cv_args)","c74ecc4f":"cv_args = {\n    'test_size': 3,\n    'n_splits': 3\n}\n\ntest_cv(X_train, y_train, groups_train, **cv_args)\nplot_cv(X_train, y_train, groups_train, \n        image_file_path=IMAGES_PATH\/'test_size_n_splits_cv.png',\n        **cv_args)","3ec58a45":"cv_args = {\n    'test_size': 3,\n    'n_splits': 3,\n    'gap': 2\n}\n\ntest_cv(X_train, y_train, groups_train, **cv_args)\nplot_cv(X_train, y_train, groups_train, \n        image_file_path=IMAGES_PATH\/'test_size_n_splits_gap_cv.png',\n        **cv_args)","aed3e113":"cv_args = {\n    'test_size': 3,\n    'n_splits': 3,\n    'shift_size': 2\n}\n\ntest_cv(X_train, y_train, groups_train, **cv_args)\nplot_cv(X_train, y_train, groups_train, \n        image_file_path=IMAGES_PATH\/'test_size_n_splits_shift_size_cv.png',\n        **cv_args)","71941171":"cv_args = {\n    'test_size': 3,\n    'n_splits': 3,\n    'window': 'expanding'\n}\n\ntest_cv(X_train, y_train, groups_train, **cv_args)\nplot_cv(X_train, y_train, groups_train, \n        image_file_path=IMAGES_PATH\/'test_size_n_splits_expanding_window_cv.png',\n        **cv_args)","21ae52ca":"cv_args = {\n    'test_size': 3,\n    'train_size': 3,\n    'n_splits': 4\n}\n\ntest_cv(X_train, y_train, groups_train, **cv_args)\nplot_cv(X_train, y_train, groups_train, \n        image_file_path=IMAGES_PATH\/'test_size_train_size_n_splits_full_data_cv.png',\n        **cv_args)","9ff5963f":"cv_args = {\n    'test_size': 3,\n    'train_size': 3,\n    'n_splits': 2\n}\n\ntest_cv(X_train, y_train, groups_train, **cv_args)\nplot_cv(X_train, y_train, groups_train, \n        image_file_path=IMAGES_PATH\/'test_size_train_size_n_splits_partial_data_cv.png',\n        **cv_args)","8b5f2243":"cv_args = {\n    'test_size': 3,\n    'n_splits': 2\n}\ncv = GroupTimeSeriesSplit(**cv_args)\n\nprint('First usage')\nfor split in cv.split(X_train, groups=groups_train):\n    print(split)\n    \nprint('Repeated usage')\nfor split in cv.split(X_train, groups=groups_train):\n    print(split)","fbcaba2d":"cv_args = {\n    'test_size': 2,\n    'train_size': 4,\n    'n_splits': 3, \n    'shift_size': 2,\n    'gap': 2,\n    'window': 'rolling'\n}\n\nplot_cv(X, y, groups, **cv_args)","009e0154":"**Generate monthly groups**","1122b2c5":"**test_size + train_size + n_splits (partial usage of data)**","009e028c":"**Not enough data (too many number of splits)**","83f2308a":"**test_size + n_splits + gap**","fcb7f0a2":"- Groups are not sorted\n- Both train_size and n_splits are specified\n- Incorrect window name\n- Specify train_size with expanding window\n- Not enough data (too many number of splits)\n- Not enough data (too large value of shift size)\n- Not enough data (too large value of gap)\n- Repeated usage of the same cv object\n- (TODO) Incorrect range of parameters","bcd43429":"I've shared my implementation of group time series cross-validation which is compatible with sklearn.  \nImplementation contains ideas from different libraries and also additional functionality. Implemented using Python standard library + numpy + sklearn helper functions.  \nFeel free to give feedback.\n\n**Article**  \nhttps:\/\/medium.com\/@labdmitriy\/advanced-group-time-series-validation-bb00d4a74bcc  \n**GitHub repository with source code**  \nhttps:\/\/github.com\/labdmitriy\/ml-lab\n\nHere I am reproducing Jupyter notebook (with minimal modifications to run as kernel) which is provided in repository for functionality testing and splits visualization.","9e2ac0bd":"**Groups are not sorted**","e821b087":"### Failed cases","d8c92dcf":"- test_size + train_size \n- test_size + n_splits\n- test_size + n_splits + gap\n- test_size + n_splits + shift_size\n- test_size + n_splits + expanding window\n- test_size + train_size + n_splits\n- Repeated usage of the same object","9dfd6fc9":"**test_size + n_splits + shift_size**","0d43a454":"**Not enough data (too large values of shift size)**","efa0c04c":"**test_size + n_splits + expanding window**","5eb0bc36":"## Check train\/test split","e2a64997":"### Monthly data","c170d950":"**Specify train_size with expanding window**","64e2c24f":"### Success cases","64025c1e":"## Check cross-validation split","ff62f46b":"**Both train_size and n_splits are not specified**","1774c292":"**Incorrect window name**","ff628827":"**test_size + n_splits**","ec7d40ef":"## Split visualization","39d21967":"## Define helper function","e61c07a4":"### Features and targets","a1c7a27e":"**Not enough data (too large values of gap)**","df2051e7":"## Prepare sample data","dd051ba3":"**Not enough data (too large train_size and\/or n_splits)**","a610f727":"**test_size + train_size + n_splits (full usage of data)**","c240736d":"**Repeated usage of the same cv object**","0d8b97b5":"## Define validation class","8323a2fe":"**test_size + train_size**"}}