{"cell_type":{"1026125b":"code","14196ed8":"code","a2171474":"code","bd8127b2":"code","3567c02d":"code","476e1159":"code","aa5ec55e":"code","304eb839":"code","22c4cf83":"code","de2a3182":"code","d84dc4b0":"code","bce2dead":"code","1def9e1d":"code","814ae1a6":"code","5b04be24":"code","ae550d43":"code","2bb794dc":"code","5e7173ac":"code","892e3eaa":"code","00531ae3":"code","b222a0da":"code","5b123d25":"code","d8b896e5":"code","dff07836":"code","07be9c77":"code","fed19bfb":"code","40470401":"code","283be860":"code","969a3a6c":"code","0f65efcc":"code","c6b92dd6":"code","898bb0dc":"code","ef6e91c5":"code","0d0f1396":"code","60641003":"code","f4806f18":"code","64c3b525":"code","07d0a2fd":"code","cd92ddc7":"code","5b978e94":"code","9f53b2ed":"markdown","cb97d06b":"markdown","4a205d9e":"markdown","15d93fb2":"markdown","efd8e26a":"markdown","a69fd0ac":"markdown","ce9f5410":"markdown","2216ee17":"markdown","787e195e":"markdown","aba27149":"markdown","be37ca80":"markdown","127ccd82":"markdown","180c5110":"markdown","da6210f4":"markdown","864bdbe4":"markdown","4e54ad26":"markdown","1e91e828":"markdown","5155aad8":"markdown","a6e831dd":"markdown","b137b785":"markdown","a8fd21d7":"markdown","081d0df0":"markdown","fc3f61bb":"markdown","928d85a3":"markdown","23487b5e":"markdown","8e6d2993":"markdown","f9c10f2c":"markdown","804f333b":"markdown","a6c3f57d":"markdown"},"source":{"1026125b":"from IPython.display import YouTubeVideo\nYouTubeVideo(\"JC84GCU7zqA\")","14196ed8":"from IPython.display import YouTubeVideo\nYouTubeVideo(\"kBjYK3K3P6M\")","a2171474":"!pip install  efficientnet\n\nimport efficientnet.tfkeras as efn\nimport re\nimport math\nimport numpy as np\nimport seaborn as sns\n\nfrom kaggle_datasets import KaggleDatasets\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.metrics import TruePositives, FalsePositives, FalseNegatives\nprint(\"Tensorflow version \" + tf.__version__)","bd8127b2":"# This is basically -1\nAUTO = tf.data.experimental.AUTOTUNE\nAUTO\n","3567c02d":"# Cluster Resolver for Google Cloud TPUs.\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n\n# Connects to the given cluster.\ntf.config.experimental_connect_to_cluster(tpu)\n\n# Initialize the TPU devices.\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# TPU distribution strategy implementation.\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)","476e1159":"# Configurations\nIMAGE_SIZE = [512, 512]\nEPOCHS = 30\nBATCH_SIZE = 32 * strategy.num_replicas_in_sync\nLEARNING_RATE = 1e-3\nTTA_NUM = 5","aa5ec55e":"print(\"Batch size used: \", BATCH_SIZE)","304eb839":"# As TPUs require access to the GCS path\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('tpu-getting-started')\nMORE_IMAGES_GCS_DS_PATH = KaggleDatasets().get_gcs_path('tf-flower-photo-tfrec')","22c4cf83":"GCS_PATH_SELECT = { # available image sizes\n    192: GCS_DS_PATH + '\/tfrecords-jpeg-192x192',\n    224: GCS_DS_PATH + '\/tfrecords-jpeg-224x224',\n    331: GCS_DS_PATH + '\/tfrecords-jpeg-331x331',\n    512: GCS_DS_PATH + '\/tfrecords-jpeg-512x512'\n}\nGCS_PATH = GCS_PATH_SELECT[IMAGE_SIZE[0]]\n\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/train\/*.tfrec')\nVALIDATION_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/val\/*.tfrec')\nTEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/test\/*.tfrec') # predictions on this dataset should be submitted for the competition\n\nMOREIMAGES_PATH_SELECT = {\n    192: '\/tfrecords-jpeg-192x192',\n    224: '\/tfrecords-jpeg-224x224',\n    331: '\/tfrecords-jpeg-331x331',\n    512: '\/tfrecords-jpeg-512x512'\n}\nMOREIMAGES_PATH = MOREIMAGES_PATH_SELECT[IMAGE_SIZE[0]]\n\nIMAGENET_FILES = tf.io.gfile.glob(MORE_IMAGES_GCS_DS_PATH + '\/imagenet' + MOREIMAGES_PATH + '\/*.tfrec')\nINATURELIST_FILES = tf.io.gfile.glob(MORE_IMAGES_GCS_DS_PATH + '\/inaturalist' + MOREIMAGES_PATH + '\/*.tfrec')\nOPENIMAGE_FILES = tf.io.gfile.glob(MORE_IMAGES_GCS_DS_PATH + '\/openimage' + MOREIMAGES_PATH + '\/*.tfrec')","de2a3182":"SKIP_VALIDATION = True\n\nif SKIP_VALIDATION:\n    TRAINING_FILENAMES = TRAINING_FILENAMES + VALIDATION_FILENAMES + IMAGENET_FILES + INATURELIST_FILES + OPENIMAGE_FILES\n\n","d84dc4b0":"CLASSES = ['pink primrose',    'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',     'wild geranium',     'tiger lily',           'moon orchid',              'bird of paradise', 'monkshood',        'globe thistle',         # 00 - 09\n           'snapdragon',       \"colt's foot\",               'king protea',      'spear thistle', 'yellow iris',       'globe-flower',         'purple coneflower',        'peruvian lily',    'balloon flower',   'giant white arum lily', # 10 - 19\n           'fire lily',        'pincushion flower',         'fritillary',       'red ginger',    'grape hyacinth',    'corn poppy',           'prince of wales feathers', 'stemless gentian', 'artichoke',        'sweet william',         # 20 - 29\n           'carnation',        'garden phlox',              'love in the mist', 'cosmos',        'alpine sea holly',  'ruby-lipped cattleya', 'cape flower',              'great masterwort', 'siam tulip',       'lenten rose',           # 30 - 39\n           'barberton daisy',  'daffodil',                  'sword lily',       'poinsettia',    'bolero deep blue',  'wallflower',           'marigold',                 'buttercup',        'daisy',            'common dandelion',      # 40 - 49\n           'petunia',          'wild pansy',                'primula',          'sunflower',     'lilac hibiscus',    'bishop of llandaff',   'gaura',                    'geranium',         'orange dahlia',    'pink-yellow dahlia',    # 50 - 59\n           'cautleya spicata', 'japanese anemone',          'black-eyed susan', 'silverbush',    'californian poppy', 'osteospermum',         'spring crocus',            'iris',             'windflower',       'tree poppy',            # 60 - 69\n           'gazania',          'azalea',                    'water lily',       'rose',          'thorn apple',       'morning glory',        'passion flower',           'lotus',            'toad lily',        'anthurium',             # 70 - 79\n           'frangipani',       'clematis',                  'hibiscus',         'columbine',     'desert-rose',       'tree mallow',          'magnolia',                 'cyclamen ',        'watercress',       'canna lily',            # 80 - 89\n           'hippeastrum ',     'bee balm',                  'pink quill',       'foxglove',      'bougainvillea',     'camellia',             'mallow',                   'mexican petunia',  'bromelia',         'blanket flower',        # 90 - 99\n           'trumpet creeper',  'blackberry lily',           'common tulip',     'wild rose']               \nprint(f\"No of Flower classes in dataset: {len(CLASSES)}\")","bce2dead":"# Learning rate schedule for TPU, GPU and CPU.\n# Using an LR ramp up because fine-tuning a pre-trained model.\n# Starting with a high LR would break the pre-trained weights.\n\nLR_START = 0.00001\nLR_MAX = 0.00005 * strategy.num_replicas_in_sync\nLR_MIN = 0.00001\nLR_RAMPUP_EPOCHS = 5\nLR_SUSTAIN_EPOCHS = 0\nLR_EXP_DECAY = .7\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) \/ LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n    \nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n\nrng = [i for i in range(EPOCHS)]\ny = [lrfn(x) for x in rng]\nplt.plot(rng, y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))\n\n","1def9e1d":"def display_confusion_matrix(cmat, score, precision, recall):\n    plt.figure(figsize=(15,15))\n    ax = plt.gca()\n    ax.matshow(cmat, cmap='Reds')\n    ax.set_xticks(range(len(CLASSES)))\n    ax.set_xticklabels(CLASSES, fontdict={'fontsize': 7})\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n    ax.set_yticks(range(len(CLASSES)))\n    ax.set_yticklabels(CLASSES, fontdict={'fontsize': 7})\n    plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n    titlestring = \"\"\n    if score is not None:\n        titlestring += 'f1 = {:.3f} '.format(score)\n    if precision is not None:\n        titlestring += '\\nprecision = {:.3f} '.format(precision)\n    if recall is not None:\n        titlestring += '\\nrecall = {:.3f} '.format(recall)\n    if len(titlestring) > 0:\n        ax.text(101, 1, titlestring, fontdict={'fontsize': 18, 'horizontalalignment':'right', 'verticalalignment':'top', 'color':'#804040'})\n    plt.show()\n    \ndef display_training_curves(training, validation, title, subplot):\n    with plt.xkcd():\n        if subplot%10==1: # set up the subplots on the first call\n            plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n            plt.tight_layout()\n        ax = plt.subplot(subplot)\n        ax.set_facecolor('#F8F8F8')\n        ax.plot(training)\n        ax.plot(validation)\n        ax.set_title('model '+ title)\n        ax.set_ylabel(title)\n        #ax.set_ylim(0.28,1.05)\n        ax.set_xlabel('epoch')\n        ax.legend(['train', 'valid.'])","814ae1a6":"def display_one_flower(image, title, subplot, red=False, titlesize=16):\n    plt.subplot(*subplot)\n    plt.axis('off')\n    plt.imshow(image)\n    if len(title) > 0:\n        plt.title(title, fontsize=int(titlesize) if not red else int(titlesize\/1.2), color='red' if red else 'black', fontdict={'verticalalignment':'center'}, pad=int(titlesize\/1.5))\n    return (subplot[0], subplot[1], subplot[2]+1)\n\ndef display_batch_of_images(databatch, predictions=None):\n    \"\"\"This will work with:\n    display_batch_of_images(images)\n    display_batch_of_images(images, predictions)\n    display_batch_of_images((images, labels))\n    display_batch_of_images((images, labels), predictions)\n    \"\"\"\n    # data\n    images, labels = batch_to_numpy_images_and_labels(databatch)\n    if labels is None:\n        labels = [None for _ in enumerate(images)]\n        \n    # auto-squaring: this will drop data that does not fit into square or square-ish rectangle\n    rows = int(math.sqrt(len(images)))\n    cols = len(images)\/\/rows\n        \n    # size and spacing\n    FIGSIZE = 13.0\n    SPACING = 0.1\n    subplot=(rows,cols,1)\n    if rows < cols:\n        plt.figure(figsize=(FIGSIZE,FIGSIZE\/cols*rows))\n    else:\n        plt.figure(figsize=(FIGSIZE\/rows*cols,FIGSIZE))\n    \n    # display\n    for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])):\n        title = '' if label is None else CLASSES[label]\n        correct = True\n        if predictions is not None:\n            title, correct = title_from_label_and_target(predictions[i], label)\n        dynamic_titlesize = FIGSIZE*SPACING\/max(rows,cols)*40+3 # magic formula tested to work from 1x1 to 10x10 images\n        subplot = display_one_flower(image, title, subplot, not correct, titlesize=dynamic_titlesize)\n    \n    #layout\n    plt.tight_layout()\n    if label is None and predictions is None:\n        plt.subplots_adjust(wspace=0, hspace=0)\n    else:\n        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n    plt.show()\n    \n# Visualize model predictions\ndef dataset_to_numpy_util(dataset, N):\n    dataset = dataset.unbatch().batch(N)\n    for images, labels in dataset:\n        numpy_images = images.numpy()\n        numpy_labels = labels.numpy()\n        break;  \n    return numpy_images, numpy_labels\n\ndef title_from_label_and_target(label, correct_label):\n    label = np.argmax(label, axis=-1)\n    correct = (label == correct_label)\n    return \"{} [{}{}{}]\".format(CLASSES[label], str(correct), ', shoud be ' if not correct else '',\n                                CLASSES[correct_label] if not correct else ''), correct\n\ndef display_one_flower_eval(image, title, subplot, red=False):\n    plt.subplot(subplot)\n    plt.axis('off')\n    plt.imshow(image)\n    plt.title(title, fontsize=14, color='red' if red else 'black')\n    return subplot+1\n\ndef display_9_images_with_predictions(images, predictions, labels):\n    subplot=331\n    plt.figure(figsize=(13,13))\n    for i, image in enumerate(images):\n        title, correct = title_from_label_and_target(predictions[i], labels[i])\n        subplot = display_one_flower_eval(image, title, subplot, not correct)\n        if i >= 8:\n            break;\n              \n    plt.tight_layout()\n    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n    plt.show()","5b04be24":"def decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0  # convert image to floats in [0, 1] range\n    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n    return image\n\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    label = tf.cast(example['class'], tf.int32)\n    return image, label # returns a dataset of (image, label) pairs\n\ndef read_unlabeled_tfrecord(example):\n    UNLABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"id\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n        # class is missing, this competitions's challenge is to predict flower classes for the test dataset\n    }\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    idnum = example['id']\n    return image, idnum # returns a dataset of image(s)\n\n\n\ndef data_augment(image, label, seed=2020):\n    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below),\n    # this happens essentially for free on TPU. Data pipeline code is executed on the \"CPU\" part\n    # of the TPU while the TPU itself is computing gradients.\n    image = tf.image.random_flip_left_right(image, seed=seed)\n#     image = tf.image.random_flip_up_down(image, seed=seed)\n#     image = tf.image.random_brightness(image, 0.1, seed=seed)\n    \n#     image = tf.image.random_jpeg_quality(image, 85, 100, seed=seed)\n#     image = tf.image.resize(image, [530, 530])\n#     image = tf.image.random_crop(image, [512, 512], seed=seed)\n    #image = tf.image.random_saturation(image, 0, 2)\n    return image, label   \n\ndef get_training_dataset():\n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_validation_dataset(ordered=False):\n    dataset = load_dataset(VALIDATION_FILENAMES, labeled=True, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_train_valid_datasets():\n    dataset = load_dataset(TRAINING_FILENAMES + VALIDATION_FILENAMES, labeled=True)\n    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_test_dataset(ordered=False):\n    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef count_data_items(filenames):\n    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)","ae550d43":"models = []\nhistories = []\n","2bb794dc":"# No of images in dataset\nNUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\nNUM_VALIDATION_IMAGES = count_data_items(VALIDATION_FILENAMES)\nNUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\nSTEPS_PER_EPOCH = (NUM_TRAINING_IMAGES + NUM_VALIDATION_IMAGES) \/\/ BATCH_SIZE\nprint('Dataset: {} training images, {} unlabeled test images'.format(NUM_TRAINING_IMAGES+NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))","5e7173ac":"def load_dataset(filenames, labeled=True, ordered=False):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n    return dataset\n\ndef get_training_dataset_preview(ordered=True):\n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\n# Visualization utility functions\nnp.set_printoptions(threshold=15, linewidth=80)\n\ndef batch_to_numpy_images_and_labels(data):\n    images, labels = data\n    numpy_images = images.numpy()\n    numpy_labels = labels.numpy()\n    if numpy_labels.dtype == object: # binary string in this case, these are image ID strings\n        numpy_labels = [None for _ in enumerate(numpy_images)]\n    # If no labels, only image IDs, return None for labels (this is the case for test data)\n    return numpy_images, numpy_labels\n\ndef title_from_label_and_target(label, correct_label):\n    if correct_label is None:\n        return CLASSES[label], True\n    correct = (label == correct_label)\n    return \"{} [{}{}{}]\".format(CLASSES[label], 'OK' if correct else 'NO', u\"\\u2192\" if not correct else '',\n                                CLASSES[correct_label] if not correct else ''), correct\n\n\n# Visualize model predictions\ndef dataset_to_numpy_util(dataset, N):\n    dataset = dataset.unbatch().batch(N)\n    for images, labels in dataset:\n        numpy_images = images.numpy()\n        numpy_labels = labels.numpy()\n        break;  \n    return numpy_images, numpy_labels","892e3eaa":"train_dataset = get_training_dataset_preview(ordered=True)\ny_train = next(iter(train_dataset.unbatch().map(lambda image, label: label).batch(NUM_TRAINING_IMAGES))).numpy()\nprint('Number of training images %d' % NUM_TRAINING_IMAGES)","00531ae3":"display_batch_of_images(next(iter(train_dataset.unbatch().batch(20))))","b222a0da":"# Label distribution\ntrain_stack = np.asarray([[label, (y_train == index).sum()] for index, label in enumerate(CLASSES)])\n\nfig, (ax1) = plt.subplots(1, 1, figsize=(24, 32))\n\nax1 = sns.barplot(x=train_stack[...,1], y=train_stack[...,0], order=CLASSES,ax=ax1)\nax1.set_title('Training labels', fontsize=30)\nax1.tick_params(labelsize=16)\n","5b123d25":"# peer at test data\ntest_dataset = get_test_dataset()\ntest_dataset = test_dataset.unbatch().batch(20)\ntest_batch = iter(test_dataset)","d8b896e5":"# run this cell again for next set of images\ndisplay_batch_of_images(next(test_batch))","dff07836":"# def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n#     rotation = math.pi * rotation \/ 180.\n#     shear = math.pi * shear\/ 180.\n    \n#     c1 = tf.math.cos(rotation)\n#     c2 = tf.math.sin(rotation)\n#     one = tf.constant([1], dtype='float32')\n#     zero = tf.constant([0], dtype='float32')\n#     rotation_mat = tf.reshape(tf.concat([c1, s1, zero, -s1, c1, zero, \\\n#                                          zero, zero, one], axis=0), [3,3])\n    \n#     # shear matrix\n#     c2 = tf.math.cos(shear)\n#     s2 = tf.math.sin(shear)\n#     shear_mat = tf.reshape(tf.concat([one, s2, zero, zero, c2, \\\n#                                          zero, zero, zero, one], axis=0), [3,3])\n    \n#     zoom_mat = tf.reshape(tf.concat([one\/height_zoom, zero, zero, zero, \\\n#                                     oneb\/width_zoom, zero, zero, zero, one], axis=0), [3,3])\n    \n#     shift_mat = tf.reshape(tf.concat([one_zero, height_shift, zero, one, width_shift, zero, \\\n#                                       zero, one], axis=0), [3,3])\n    \n#     return K.dot(K.dot(rotation_mat, shear_mat), K.dot(zoom_mat, shift_mat))","07be9c77":"# def transform(image, label):\n#     DIM = 512\n#     XIM = DIM%2\n    \n#     rot = 15. * tf.random.normal([1], dtype='float32')\n#     shr = 5.*tf.random.normal([1], dtype='float32')\n#     h_zoom = 1.0 + tf.random.normal([1], dtype='float32') \/ 1.0\n#     w_zoom = 1.0 + tf.random.normal([1], dtype='float32')\/1.0\n    \n#     h_shift = 16. * tf.random.normal([1],dtype='float32') \n#     w_shift = 16. * tf.random.normal([1],dtype='float32') \n  \n#     # GET TRANSFORMATION MATRIX\n#     m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n#     # LIST DESTINATION PIXEL INDICES\n#     x = tf.repeat( tf.range(DIM\/\/2,-DIM\/\/2,-1), DIM )\n#     y = tf.tile( tf.range(-DIM\/\/2,DIM\/\/2),[DIM] )\n#     z = tf.ones([DIM*DIM],dtype='int32')\n#     idx = tf.stack( [x,y,z] )\n    \n#     # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n#     idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n#     idx2 = K.cast(idx2,dtype='int32')\n#     idx2 = K.clip(idx2,-DIM\/\/2+XDIM+1,DIM\/\/2)\n    \n#     # FIND ORIGIN PIXEL VALUES           \n#     idx3 = tf.stack( [DIM\/\/2-idx2[0,], DIM\/\/2-1+idx2[1,]] )\n#     d = tf.gather_nd(image,tf.transpose(idx3))\n        \n#     return tf.reshape(d,[DIM,DIM,3]), label","fed19bfb":"# row = 3; col = 4;\n# all_elements = get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug=False).unbatch()\n# one_element = tf.data.Dataset.from_tensors( next(iter(all_elements)) )\n# augmented_element = one_element.repeat().map(transform).batch(row*col)\n\n# for (img,label) in augmented_element:\n#     plt.figure(figsize=(15,int(15*row\/col)))\n#     for j in range(row*col):\n#         plt.subplot(row,col,j+1)\n#         plt.axis('off')\n#         plt.imshow(img[j,])\n#     plt.show()\n#     break\n","40470401":"# Need this line so Google will recite some incantations\n# for Turing to magically load the model onto the TPU\nwith strategy.scope():\n    enet = efn.EfficientNetB3(\n        input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n        weights='imagenet',\n        include_top=False\n    )\n    \n    enet.trainable = True\n    model1 = tf.keras.Sequential([\n        enet,\n        tf.keras.layers.GlobalMaxPooling2D(name=\"Layer1\"),\n        tf.keras.layers.Dropout(0.),\n        tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n    ])\n        \n# METRICS = ['TruePositives','FalsePositives', 'FalseNegatives']\nmodel1.compile(\n    optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n    loss = 'sparse_categorical_crossentropy',\n    metrics = \"sparse_categorical_accuracy\"\n)\n\nmodel1.summary()\n\nmodels.append(model1)","283be860":"# schedule = StepDecay(initAlpha=1e-4, factor=0.25, dropEvery=15)\n\n# callbacks = [LearningRateScheduler(schedule)]","969a3a6c":"# Visualising the Model architecture\ntf.keras.utils.plot_model(\n    model1, to_file='model.png', show_shapes=True, show_layer_names=True,\n)\n","0f65efcc":"%%time\nCheckpoint=tf.keras.callbacks.ModelCheckpoint(f\"Enet_model.h5\", monitor='val_accuracy', verbose=1, save_best_only=True,\n       save_weights_only=True,mode='max')\n\ntrain_history1 = model1.fit(\n    get_training_dataset(), \n    steps_per_epoch=STEPS_PER_EPOCH,\n    epochs=EPOCHS,\n    callbacks=[lr_callback, Checkpoint, keras.callbacks.EarlyStopping(\n        monitor=\"val_loss\",\n        min_delta=1e-2,\n        patience=2,\n        verbose=1,\n    )],\n)\n\nhistories.append(train_history1)\n\n","c6b92dd6":"def plot_training(H):\n\t# construct a plot that plots and saves the training history\n\twith plt.xkcd():\n\t\tplt.figure()\n\t\tplt.plot(H.history[\"loss\"], label=\"train_loss\")\n\t\tplt.plot(H.history[\"sparse_categorical_accuracy\"], label=\"train_accuracy\")\n\t\tplt.title(\"Training Loss and Accuracy\")\n\t\tplt.xlabel(\"Epoch #\")\n\t\tplt.ylabel(\"Loss\/Accuracy\")\n\t\tplt.legend(loc=\"lower left\")\n\t\tplt.show()","898bb0dc":"plot_training(train_history1)","ef6e91c5":"with strategy.scope():\n    densenet = tf.keras.applications.DenseNet201(input_shape=[*IMAGE_SIZE, 3], weights='imagenet', include_top=False)\n    densenet.trainable = True\n    \n    model2 = tf.keras.Sequential([\n        densenet,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n    ])\n        \nmodel2.compile(\n    optimizer=tf.keras.optimizers.Adam(),\n    loss = 'sparse_categorical_crossentropy',\n    metrics=['sparse_categorical_accuracy']\n)\nmodel2.summary()","0d0f1396":"# Visualising the Model architecture\ntf.keras.utils.plot_model(\n    model1, to_file='model.png', show_shapes=True, show_layer_names=True,\n)","60641003":"%%time\nCheckpoint=tf.keras.callbacks.ModelCheckpoint(f\"Dnet_model.h5\", monitor='val_accuracy', verbose=1, save_best_only=True,\n       save_weights_only=True,mode='max')\ntrain_history2 = model2.fit(get_training_dataset(), \n                    steps_per_epoch=STEPS_PER_EPOCH,\n                    epochs=15, \n                    callbacks = [lr_callback, Checkpoint, keras.callbacks.EarlyStopping(\n        # Stop training when `val_loss` is no longer improving\n        monitor=\"val_loss\",\n        # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n        min_delta=1e-2,\n        # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n        patience=2,\n        verbose=1,\n    )])\n\nhistories.append(train_history2)","f4806f18":"plot_training(train_history2)","64c3b525":"if not SKIP_VALIDATION:\n    cmdataset = get_validation_dataset(ordered=True) # since we are splitting the dataset and iterating separately on images and labels, order matters.\n    images_ds = cmdataset.map(lambda image, label: image)\n    labels_ds = cmdataset.map(lambda image, label: label).unbatch()\n    cm_correct_labels = next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))).numpy() # get everything as one batch\n    m1 = model1.predict(images_ds)\n    m2 = model2.predict(images_ds)\n    scores = []\n    for alpha in np.linspace(0,1,100):\n        cm_probabilities = alpha*m1+(1-alpha)*m2\n        cm_predictions = np.argmax(cm_probabilities, axis=-1)\n        scores.append(f1_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro'))\n\n    best_alpha = np.argmax(scores)\/100\nelse:\n    best_alpha = 0.51  # change to value calculated with SKIP_VALIDATION=False\n    \nprint('Best alpha: ' + str(best_alpha))","07d0a2fd":"if not SKIP_VALIDATION:\n    cmat = confusion_matrix(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)))\n    score = f1_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\n    precision = precision_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\n    recall = recall_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\n    #cmat = (cmat.T \/ cmat.sum(axis=1)).T # normalized\n    display_confusion_matrix(cmat, score, precision, recall)\n    print('f1 score: {:.3f}, precision: {:.3f}, recall: {:.3f}'.format(score, precision, recall))","cd92ddc7":"def predict_tta(model, n_iter):\n    probs  = []\n    for i in range(n_iter):\n        test_ds = get_test_dataset(ordered=True) # since we are splitting the dataset and iterating separately on images and ids, order matters.\n        test_images_ds = test_ds.map(lambda image, idnum: image)\n        probs.append(model.predict(test_images_ds,verbose=0))\n        \n    return probs","5b978e94":"test_ds = get_test_dataset(ordered=True) # since we are splitting the dataset and iterating separately on images and ids, order matters.\n\nprint('Calculating predictions...')\ntest_images_ds = test_ds.map(lambda image, idnum: image)\nprobs1 = np.mean(predict_tta(model1, TTA_NUM), axis=0)\nprobs2 = np.mean(predict_tta(model2, TTA_NUM), axis=0)\nprobabilities = best_alpha*probs1 + (1-best_alpha)*probs2\npredictions = np.argmax(probabilities, axis=-1)\n\nprint('Generating submission file...')\ntest_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\ntest_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\nnp.savetxt('submission.csv', np.rec.fromarrays([test_ids, predictions]), fmt=['%s', '%d'], delimiter=',', header='id,label', comments='')\n","9f53b2ed":"# What's EfficientNet? <a class=\"anchor\" id=\"efficentnet\"><\/a>\n\nMost of people in this competition has used Efficient Net for this competiton. I was curious about what's Efficient and why everyone is getting good scores with \nthis new Image architecture.\n\n<html>\n<body>\n\n<p><font size=\"4\" color=\"Blue\"> Finally I stumbled in this ICML 2019 paper, Efficient Net<\/font><\/p>\n<\/body>\n<\/html>\nIn the paper \u201cEfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\u201d, we propose a novel model scaling method that uses a simple yet highly effective compound coefficient to scale up CNNs in a more structured manner. Unlike conventional approaches that arbitrarily scale network dimensions, such as width, depth and resolution, our method uniformly scales each dimension with a fixed set of scaling coefficients. Powered by this novel scaling method and recent progress on AutoML, we have developed a family of models, called EfficientNets, which superpass state-of-the-art accuracy with up to 10x better efficiency (smaller and faster).\n\n![Screenshot_2020-02-26%20EfficientNet%20Rethinking%20Model%20Scaling%20for%20Convolutional%20Neural%20Networks%20-%20tan19a%20pdf.png](attachment:Screenshot_2020-02-26%20EfficientNet%20Rethinking%20Model%20Scaling%20for%20Convolutional%20Neural%20Networks%20-%20tan19a%20pdf.png)","cb97d06b":"## Augmentations","4a205d9e":"# DensenetNet model <a class=\"anchor\" id=\"densenet\"><\/a>","15d93fb2":"## Training accuracy and loss curves","efd8e26a":"# Exploring Flower(EDA)<a class=\"anchor\" id=\"eda\"><\/a>","a69fd0ac":"## Importing library","ce9f5410":"Heavily borrowed from [dimitreoliveira EDA kernel](https:\/\/www.kaggle.com\/dimitreoliveira\/flower-classification-with-tpus-eda-and-baseline\/notebook)","2216ee17":"[submission](submission.csv)","787e195e":"<html>\n<body>\n\n<p><font size=\"4\" color=\"Blue\"> The following video from Kaggle explains the main components of TPU like systolic arrays and bfloat16 number formats, and how these two components of TPUs help reduce deep learning model training times <\/font><\/p>\n<\/body>\n<\/html>","aba27149":"### Dataset functions","be37ca80":"The following code does random rotations, shear, zoom, and shift using the GPU\/TPU. When an image gets moved away from an edge revealing blank space, the blank space is filled by stretching the colors on the original edge. Change the variables in function transform() below to control the desired amount of augmentation. Here's a diagram illustrating the mathematics.","127ccd82":"## Find best alpha","180c5110":"<html>\n<body>\n\n<p><font size=\"4\" color=\"Blue\"> This video explains in detail about main differences between TPUv2 and TPUv3 <\/font><\/p>\n<\/body>\n<\/html>\n\n","da6210f4":"## Submitting Model Predicions <a class=\"tpu\" id=\"submit\"><\/a>\n\nEnsembling tricks and TTA borrowed from [notebook](https:\/\/www.kaggle.com\/atamazian\/flower-classification-ensemble-effnet-densenet).","864bdbe4":"### Visualisation Helper Fuctions","4e54ad26":"### Table of contents\n\n1. [What are Tensor Processing Units?](#tpu)\n2. [Peeking into TPU Hardware](#peek) \n3. [Exploring flowers](#eda) \n4. [What's EfficentNet](#efficentnet)\n5. [Densenet](#densenet)\n5. [Final model submission](#submit)","1e91e828":"# Helper functions","5155aad8":"## Confusion Matrix","a6e831dd":"## Test time augmentation(TTA)","b137b785":"As the image show's,  EfficientNet tops the current state of the art both in accuracy and in computational efficiency. How did they do this?\n\n## Lesson 1\n\nThey learned that CNN\u2019s must be scaled up in multiple dimensions. Scaling CNN\u2019s only in one direction (eg depth only) will result in rapidly deteriorating gains relative to the computational increase needed.Most CNN\u2019s are typically scaled up by adding more layers or deeper . e.g. ResNet18, ResNet34, ResNet152, etc. The numbers represent the total number of blocks (layers) and in general, the more layers the more \u2018power\u2019 the CNN has. Going wider is another often used scaling method, and tends to capture finer details and can be easier to train. However, it\u2019s benefits quickly saturate as well.\n\nThere are three scaling dimensions of a CNN: depth, width, and resolution. Depth simply means how deep the networks is which is equivalent to the number of layers in it. Width simply means how wide the network is. One measure of width, for example, is the number of channels in a Conv layer whereas Resolution is simply the image resolution that is being passed to a CNN. The figure below(from the paper itself) will give you a clear idea of what scaling means across different dimensions. We will discuss these in detail as well.\n\n![Screenshot_2020-02-27%20EfficientNet%20Rethinking%20Model%20Scaling%20for%20Convolutional%20Neural%20Networks.png](attachment:Screenshot_2020-02-27%20EfficientNet%20Rethinking%20Model%20Scaling%20for%20Convolutional%20Neural%20Networks.png)\n\n## Lesson 2: Compound Scaling\n\nIn order to scale up efficiently, all dimensions of depth, width and resolution have to be scaled together, and there is an optimal balance for each dimension relative to the others. Intuition says that as the resolution of the images is increased, depth and width of the network should be increased as well. As the depth is increased, larger receptive fields can capture similar features that include more pixels in an image. Also, as the width is increased, more fine-grained features will be captured. To validate this intuition, the authors ran a number of experiments with different scaling values for each dimension. \n\nThese results lead to our second observation: It is critical to balance all dimensions of a network (width, depth, and resolution) during CNNs scaling for getting improved accuracy and efficiency.\n\nThe authors discovered that there is a synergy in scaling multiple dimensions together, and after an extensive grid search derived the theoretically optimal formula of \u201ccompound scaling\u201d using the following co-efficients: Depth = 1.20, Width = 1.10, Resolution = 1.15\n\nThe authors proposed a simple yet very effective scaling technique which uses a compound coefficient \u0278 to uniformly scale network width, depth, and resolution in a principled way\n\n![Screenshot_2020-02-27%20EfficientNet%20Rethinking%20Model%20Scaling%20for%20Convolutional%20Neural%20Networks%281%29.png](attachment:Screenshot_2020-02-27%20EfficientNet%20Rethinking%20Model%20Scaling%20for%20Convolutional%20Neural%20Networks%281%29.png)\n\n\u0278 is a user-specified coefficient that controls how many resources are available whereas \u03b1, \u03b2, and \u03b3 specify how to assign these resources to network depth, width, and resolution respectively.\n\nIn a CNN, Conv layers are the most compute expensive part of the network. Also, FLOPS of a regular convolution op is almost proportional to d, w\u00b2, r\u00b2, i.e. doubling the depth will double the FLOPS while doubling width or resolution increases FLOPS almost by four times. Hence, in order to make sure that the total FLOPS don\u2019t exceed 2^\u03d5, the constraint applied is that (\u03b1 * \u03b2\u00b2 * \u03b3\u00b2) \u2248 2\n","a8fd21d7":"### References\n\n- https:\/\/codelabs.developers.google.com\/codelabs\/keras-flowers-tpu\/#0\n- https:\/\/arxiv.org\/abs\/1905.11946\n- https:\/\/ai.googleblog.com\/2019\/05\/efficientnet-improving-accuracy-and.html\n- https:\/\/medium.com\/@lessw\/efficientnet-from-google-optimally-scaling-cnn-model-architectures-with-compound-scaling-e094d84d19d4\n- https:\/\/medium.com\/@nainaakash012\/efficientnet-rethinking-model-scaling-for-convolutional-neural-networks-92941c5bfb95\n- https:\/\/www.kaggle.com\/mmmarchetti\/flowers-on-tpu-ii","081d0df0":"### Model functions","fc3f61bb":"### TPU Utilisation\n\n![Screenshot_2020-03-03%20Introduction%20kernel%20what's%20EfficientNet%20Kaggle.png](attachment:Screenshot_2020-03-03%20Introduction%20kernel%20what's%20EfficientNet%20Kaggle.png)","928d85a3":"# What are TPU's? <a class=\"tpu\" id=\"prepare\"><\/a>\n\n\n\nTPU's are holy grail of computers for any Machine Learning Practitioners! A tensor processing unit (TPU) is an AI accelerator application-specific integrated circuit (ASIC) developed by Google specifically for neural network machine learning. \nTPUs are hardware accelerators specialized in deep learning tasks. In this code lab, you will see how to use them with Keras and Tensorflow 2. Cloud TPUs are available in a base configuration with 8 cores and also in larger configurations called \"TPU pods\" of up to 2048 cores. The extra hardware can be used to accelerate training by increasing the training batch size.\n\n\n## Why TPUs?\n\nModern GPUs are organized around programmable \"cores\", a very flexible architecture that allows them to handle a variety of tasks such as 3D rendering, deep learning, physical simulations, etc.. TPUs on the other hand pair a classic vector processor with a dedicated matrix multiply unit and excel at any task where large matrix multiplications dominate, such as neural networks.\n","23487b5e":"## EfficientNet Architecture\n\nThe effectiveness of model scaling also relies heavily on the baseline network. So, to further improve performance, we have also developed a new baseline network by performing a neural architecture search using the AutoML MNAS framework, which optimizes both accuracy and efficiency (FLOPS). The resulting architecture uses mobile inverted bottleneck convolution (MBConv), similar to MobileNetV2 and MnasNet, but is slightly larger due to an increased FLOP budget. We then scale up the baseline network to obtain a family of models, called EfficientNets. \n\nThe MBConv block is nothing fancy but an Inverted Residual Block (used in MobileNetV2) with a Squeeze and Excite block injected sometimes.\n\nNow we have the base network, we can search for optimal values for our scaling parameters. If you revisit the equation, you will quickly realize that we have a total of four parameters to search for: \u03b1, \u03b2, \u03b3, and \u03d5. In order to make the search space smaller and making the search operation less costly, the search for these parameters can be completed in two steps.\n\n- Fix \u03d5 =1, assuming that twice more resources are available, and do a small grid search for \u03b1, \u03b2, and \u03b3. For baseline network B0, it turned out the optimal values are \u03b1 =1.2, \u03b2 = 1.1, and \u03b3 = 1.15 such that \u03b1 * \u03b2\u00b2 * \u03b3\u00b2 \u2248 2\n- Now fix \u03b1, \u03b2, and \u03b3 as constants (with values found in above step) and experiment with different values of \u03d5. The different values of \u03d5 produce EfficientNets B1-B7.\n\n\n\nThe effectiveness of model scaling also relies heavily on the baseline network. So, to further improve performance, we have also developed a new baseline network by performing a neural architecture search using the AutoML MNAS framework, which optimizes both accuracy and efficiency (FLOPS). The resulting architecture uses mobile inverted bottleneck convolution (MBConv), similar to MobileNetV2 and MnasNet, but is slightly larger due to an increased FLOP budget. We then scale up the baseline network to obtain a family of models, called EfficientNets. \n\n![Screenshot_2020-02-27%20EfficientNet%20Improving%20Accuracy%20and%20Efficiency%20through%20AutoML%20and%20Model%20Scaling.png](attachment:Screenshot_2020-02-27%20EfficientNet%20Improving%20Accuracy%20and%20Efficiency%20through%20AutoML%20and%20Model%20Scaling.png)","8e6d2993":"## Introduction\n\nWelcome to Petals to Metal: Flower Competition, here are you are challenged to build a deep learning model for classifying flowers\n\n![images.jpeg](attachment:images.jpeg)\n\n","f9c10f2c":"Unhide the code to see the classes in Dataset","804f333b":"# Peeking into TPU Hardware [source](https:\/\/codelabs.developers.google.com\/codelabs\/keras-flowers-tpu\/#2)<a class=\"anchor\" id=\"peek\"><\/a>\n\n\n\n## MXU and VPU\n\n\nA TPU v3 core is made of a Matrix Multiply Unit (MXU) which runs matrix multiplications and a Vector Processing Unit (VPU) for all other tasks such as activations, softmax, etc. The VPU handles float32 and int32 computations. The MXU on the other hand operates in a mixed precision 16-32 bit floating point format. The give image below shows a TPUv3 which operates at 420 TeraFlops and 128 GB HBM.\n\n![Screenshot_2020-02-26%20Keras%20and%20modern%20convnets,%20on%20TPUs.png](attachment:Screenshot_2020-02-26%20Keras%20and%20modern%20convnets,%20on%20TPUs.png)\n\n## Mixed precision floating point and bfloat16\n\nThe MXU computes matrix multiplications using bfloat16 inputs and float32 outputs. Intermediate accumulations are performed in float32 precision.\n\nNeural network training is typically resistant to the noise introduced by a reduced floating point precision. There are cases where noise even helps the optimizer converge. 16-bit floating point precision has traditionally been used to accelerate computations but float16 and float32 formats have very different ranges. Reducing the precision from float32 to float16 usually results in over and underflows. Solutions exist but additional work is typically required to make float16 work.\n\nThat is why Google introduced the bfloat16 format in TPUs. bfloat16 is a truncated float32 with exactly the same exponent bits and range as float32. This, added to the fact that TPUs compute matrix multiplications in mixed precision with bfloat16 inputs but float32 outputs, means that, typically, no code changes are necessary to benefit from the performance gains of reduced precision.\n\n\n> The use of bfloat16\/float32 mixed precision is the default on TPUs. No code changes are necessary in your Tensorflow code to enable it\n\n## Systolic arrays\n\nCPUs are made to run pretty much any calculation. Therefore, CPU store values in registers and a program sends a set of instructions to the Arithmetic Logic Unit to read a given register, perform an operation and register the output into the right register. This comes at some cost in terms of power and chip area.\n\nFor an MXU, matrix multiplication reuses both inputs many times, \n\n\n\n## Under the hood: XLA\n\nTensorflow programs define computation graphs. The TPU does not directly run Python code, it runs the computation graph defined by your Tensorflow program. Under the hood, a compiler called XLA (accelerated Linear Algebra compiler) transforms the Tensorflow graph of computation nodes into TPU machine code. This compiler also performs many advanced optimizations on your code and your memory layout. The compilation happens automatically as work is sent to the TPU. You do not have to include XLA in your build chain explicitly.\n\n\n## Using TPUs in Keras\n\nTPUs are supported through the Keras API as of Tensorflow 2.1. Keras support works on TPUs and TPU pods.\n\nDon't worry TPU is also supported in Pytorch, check out @abhishek, 4X Kaggle grandmaster's video on [training BERT's in TPU](https:\/\/www.youtube.com\/watch?v=s-3zts7FTDA)\n\n\n[Do check out System Architecture of TPU](https:\/\/cloud.google.com\/tpu\/docs\/system-architecture) gives more detials of TPU configurations and various versions of TPU ","a6c3f57d":"## Dataset Processing"}}