{"cell_type":{"d89e88d5":"code","2db9b9b9":"code","e812bf80":"code","24e1d206":"code","65fe93db":"code","1f02da24":"markdown","cfc57e18":"markdown","72f9a9da":"markdown"},"source":{"d89e88d5":"import time\nimport random","2db9b9b9":"class Env():\n    def __init__(self, length, height):\n        # define the height and length of the map\n        self.length = length\n        self.height = height\n        # define the agent's start position\n        self.x = 0\n        self.y = 0\n\n    def render(self, frames=50):\n        for i in range(self.height):\n            if i == 0: # cliff is in the line 0\n                line = ['S'] + ['x']*(self.length - 2) + ['T'] # 'S':start, 'T':terminal, 'x':the cliff\n            else:\n                line = ['.'] * self.length\n            if self.x == i:\n                line[self.y] = 'o' # mark the agent's position as 'o'\n            print(''.join(line))\n        print('\\033['+str(self.height+1)+'A')  # printer go back to top-left \n        time.sleep(1.0 \/ frames)\n\n    def step(self, action):\n        \"\"\"4 legal actions, 0:up, 1:down, 2:left, 3:right\"\"\"\n        change = [[0, 1], [0, -1], [-1, 0], [1, 0]]\n        self.x = min(self.height - 1, max(0, self.x + change[action][0]))\n        self.y = min(self.length - 1, max(0, self.y + change[action][1]))\n\n        states = [self.x, self.y]\n        reward = -1\n        terminal = False\n        if self.x == 0: # if agent is on the cliff line \"SxxxxxT\"\n            if self.y > 0: # if agent is not on the start position \n                terminal = True\n                if self.y != self.length - 1: # if agent falls\n                    reward = -100\n        return reward, states, terminal\n\n    def reset(self):\n        self.x = 0\n        self.y = 0","e812bf80":"class Q_table():\n    def __init__(self, length, height, actions=4, alpha=0.1, gamma=0.9):\n        self.table = [0] * actions * length * height # initialize all Q(s,a) to zero\n        self.actions = actions\n        self.length = length\n        self.height = height\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def _index(self, a, x, y):\n        \"\"\"Return the index of Q([x,y], a) in Q_table.\"\"\"\n        return a * self.height * self.length + x * self.length + y\n\n    def _epsilon(self):\n        return 0.1\n        # version for better convergence:\n        # \"\"\"At the beginning epsilon is 0.2, after 300 episodes decades to 0.05, and eventually go to 0.\"\"\"\n        # return 20. \/ (num_episode + 100)\n\n    def take_action(self, x, y, num_episode):\n        \"\"\"epsilon-greedy action selection\"\"\"\n        if random.random() < self._epsilon():\n            return int(random.random() * 4)\n        else:\n            actions_value = [self.table[self._index(a, x, y)] for a in range(self.actions)]\n            return actions_value.index(max(actions_value))\n\n    def max_q(self, x, y):\n        actions_value = [self.table[self._index(a, x, y)] for a in range(self.actions)]\n        return max(actions_value)\n\n    def update(self, a, s0, s1, r, is_terminated):\n        # both s0, s1 have the form [x,y]\n        q_predict = self.table[self._index(a, s0[0], s0[1])]\n        if not is_terminated:\n            q_target = r + self.gamma * self.max_q(s1[0], s1[1])\n        else:\n            q_target = r\n        self.table[self._index(a, s0[0], s0[1])] += self.alpha * (q_target - q_predict)","24e1d206":"def cliff_walk():\n    env = Env(length=12, height=4)\n    table = Q_table(length=12, height=4)\n    for num_episode in range(3000):\n        # within the whole learning process\n        episodic_reward = 0\n        is_terminated = False\n        s0 = [0, 0]\n        while not is_terminated:\n            # within one episode\n            action = table.take_action(s0[0], s0[1], num_episode)\n            r, s1, is_terminated = env.step(action)\n            table.update(action, s0, s1, r, is_terminated)\n            episodic_reward += r\n            # env.render(frames=100)\n            s0 = s1\n        if num_episode % 20 == 0:\n            print(\"Episode: {}, Score: {}\".format(num_episode, episodic_reward))\n        env.reset()","65fe93db":"cliff_walk()","1f02da24":"# Model-Free TD Control: Q-Learning and SARSA\n\n---\n\n## Terminology in Today's Topic\n\n- **model-free**: different from the DP, in Q-Learning, we do all the things within an unknown MDP, just what the model-free means.\n\n- **prediction & control**: model-free prediction means *estimating* the value function of an unknown MDP, Model-free control means *optimizing* the value function of an unknown MDP.\n\n- **on-policy & off-policy**: on-policy learning is to learn about policy $\\pi$ from experience sample from $\\pi$, off-policy learning is to learn about policy $\\pi$ *from other policy* $\\mu$'s experience, e.g. learn from a coach\n\n- **TD-learning**: temporal-difference learning is a combination of Monte Carlo(MC) ideas and dynamic programming(DP) ideas. Like MC methods, TD methods can learn directly from raw experience without a model of the environment's dynamics. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome like MC.\n\nNow it's time for introducing Q-Learning algorithm, which is an off-policy TD control algorithm, one of the early breakthroughs in model-free RL, defined by\n\n$$Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\left[ r_{t+1} + \\gamma \\max_{a\\in \\mathcal{A}}Q(s_{t+1},a_t) - Q(s_t,a_t) \\right]$$\n\nNext, we will implement Q-learning in cliff-walking task.\n\n## Cliff Walking\n\n![cliff.png](attachment:cliff.png)\n\nThis is a standard undiscounted, episodic task, with start and\ngoal states, and the usual actions causing movement up, down, right, and left. Reward is \u22121 on all\ntransitions except those into the region marked \"The Cliff.\" Stepping into this region incurs a reward\nof \u2212100 and sends the agent instantly back to the start.","cfc57e18":"## Q-Learning Algorithm\n\nFirst, Recall the $\\epsilon$-greedy action selection:\n- with probability $\\epsilon$, choose an action at random\n- with probability $1-\\epsilon$, choose the greedy action\n\n![Q-learning.png](attachment:Q-learning.png)","72f9a9da":"## Practice\n\nmodify the code to SARSA, unlike Q-learning which is on-policy algorithm, and see the different conduct of the agent when algorithm converges.\n\n![SARSA.png](attachment:SARSA.png)"}}