{"cell_type":{"6fe7b07b":"code","067d06c2":"code","ad5d6271":"code","19836ca4":"code","bacc240b":"code","3daf31bc":"code","aecb31b5":"code","340e974b":"code","9957ef13":"code","d7965fdf":"code","cec7de89":"code","41b33578":"code","7117919f":"code","fdfd7d1d":"code","08b1d428":"code","34ca0244":"code","a3472efa":"code","198370bd":"code","62ee35af":"code","abeaa63a":"code","32e25095":"code","c9c6f76e":"code","6f443ac3":"code","97bcebc5":"code","51414744":"code","3a151ad2":"code","ac958af3":"code","0bbb0cc4":"code","6bde1db1":"code","16ed8510":"code","0211f494":"code","4a86ed86":"code","648dc47f":"code","abca8547":"code","a89c0353":"code","40b91f4d":"code","7f72d06f":"markdown","e0b409db":"markdown","d5bf43cb":"markdown","304c135f":"markdown","ea9df63b":"markdown","194a2da4":"markdown","ee54819a":"markdown","b7283b2b":"markdown","5df3150b":"markdown","30ba7180":"markdown","335f7844":"markdown","004e07d4":"markdown","24b1b6c5":"markdown"},"source":{"6fe7b07b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n","067d06c2":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ad5d6271":"data = pd.read_csv('\/kaggle\/input\/car-price-prediction\/CarPrice_Assignment.csv', index_col = 'car_ID')","19836ca4":"data.head()","bacc240b":"data.describe()","3daf31bc":"data.info()","aecb31b5":"data.nunique()","340e974b":"# adding new column - car brand (instead of car name)\ndata['CarBrand']=data['CarName'].apply(lambda x: x.split()[0])\n#deleting carname column\ndel data['CarName']\n                                   ","9957ef13":"data['CarBrand'].value_counts()","d7965fdf":"data=data.replace('vokswagen','volkswagen')\ndata=data.replace('toyouta','toyota')\ndata=data.replace('porcshce', 'porsche')\ndata=data.replace('Nissan','nissan')\ndata=data.replace('maxda', 'mazda')\ndata=data.replace('vw','volkswagen')","cec7de89":"data['CarBrand'].value_counts()","41b33578":"data = data.drop_duplicates()","7117919f":"# Finding numerical and categorical features count\ndata1 = data.drop(['price'], axis=1)\nnumerical_features = [col for col in data1 if data1[col].dtype != 'object']\ncategorical_features = [col for col in data1 if data1[col].dtype == 'object']","fdfd7d1d":"print(\"Numerical_features_count =\", len(numerical_features))\nprint(\"Categorical_features_count =\",len(categorical_features))","08b1d428":"print(f'Numerical Columns:  {data[numerical_features].columns}')\nprint('\\n')\nprint(f'Categorical Columns: {data[categorical_features].columns}')","34ca0244":"# For checking the skewedness of data's target, Drawing the target distribution\ntarget = data['price']\nfig=plt.figure(figsize=(8,4))\nsns.distplot(target)\nplt.title(\"target(price) distribution\")\n","a3472efa":"print(\"Skewnwess =\", target.skew())","198370bd":"%config InlineBackend.figure_format = 'png'\nsns.pairplot(data[numerical_features])","62ee35af":"#categorical columns distribution\nf, axes = plt.subplots(2,5 , figsize=(30, 30))\nfor i, feature in enumerate(categorical_features):\n    sns.countplot(data = data, x = feature,ax=axes[i%2, i\/\/2])","abeaa63a":"corr_matrix = data[numerical_features].corr()\nsns.heatmap(corr_matrix, annot = True)","32e25095":"data = data.drop(['citympg'], axis=1)","c9c6f76e":"# distribution of different brands\ndata.groupby('CarBrand')['price'].mean().sort_values().plot(kind='bar')","6f443ac3":"# Extracting the target and features variable\ny = data['price']\nx = data.drop(['price'], axis=1)\nprint(x.shape)\nprint(y.shape)","97bcebc5":"# Encoding     \nx = pd.get_dummies(x, columns =  categorical_features, drop_first=True)\nx.head()","51414744":"print(x.shape)\nprint(y.shape)","3a151ad2":"#power transforming target variable\ny=np.log1p(y)","ac958af3":"fig=plt.figure(figsize=(8,4))\nsns.distplot(y)\nplt.title(\"Target distribution\")\nprint(y.shape)","0bbb0cc4":"print(y.skew()) # Here we can see skewedness of target variable decreased.","6bde1db1":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx = scaler.fit_transform(x)","16ed8510":"# Division into train and test data\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 42)","0211f494":"score_r2 = []\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(x_train, y_train)\ny_pred_lr = lr.predict(x_test)\nr2_linear = r2_score(y_pred_lr, y_test)\nscore_r2.append(r2_linear)\nprint(\"R2_sccore =\",r2_linear)\nprint(\"Score_linear_reg =\", lr.score(x_test, y_test))\n","4a86ed86":"from sklearn.linear_model import LassoCV\nlassocv=LassoCV()\nlassocv.fit(x_train, y_train)\ny_pred_lasso = lassocv.predict(x_test)\nr2_lasso = r2_score(y_pred_lasso, y_test)\nscore_r2.append(r2_lasso)\nprint(\"R2_score =\", r2_lasso)\nprint(\"Score_lassocv =\",lassocv.score(x_test, y_test))","648dc47f":"from sklearn.linear_model import Ridge\nrr = Ridge()\nrr.fit(x_train, y_train)\ny_pred_ridge = rr.predict(x_test)\nr2_ridge = r2_score(y_pred_ridge, y_test)\nscore_r2.append(r2_ridge)\nprint(\"R2_score =\", r2_ridge)\nprint(\"Score_Ridge =\",rr.score(x_test, y_test))","abca8547":"from xgboost import XGBRegressor\nxgb = XGBRegressor(random_state=42)\nxgb.fit(x_train, y_train)\ny_pred_xgb = xgb.predict(x_test)\nr2_xgb = r2_score(y_pred_xgb, y_test)\nscore_r2.append(r2_xgb)\nprint(\"R2_score\", r2_xgb)\nprint(\"Score_xgb +\",xgb.score(x_test, y_test))","a89c0353":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators = 220, random_state = 42)\nrf.fit(x_train, y_train)\ny_pred_rf = rf.predict(x_test)\nr2_rf = r2_score(y_pred_rf, y_test)\nscore_r2.append(r2_rf)\nprint(\"R2_score =\", r2_rf)\nprint(\"Score_RandomForest =\",rf.score(x_test, y_test))","40b91f4d":"model_names = ['LinearRegression', 'Lassocv', 'RidgeRegression', 'XGB', 'RandomForest']\nresult_df = pd.DataFrame({'R2_score':score_r2}, index  = model_names)\nresult_df","7f72d06f":"# Data Visulization","e0b409db":"**From the table we can see that Lassocv is the best model**","d5bf43cb":"**Here we can see that citympg and highwaympg have high correlation, so dropping any one helps in good prediction.**","304c135f":"# **Data Overview**","ea9df63b":"**4.Random Forest Regressor**","194a2da4":"# Data preprocessing","ee54819a":"**3. XGB Regressor**","b7283b2b":"We can see there are no any Null value present in data and both numerical and catogiracl features are there in the dataset","5df3150b":"**2.Ridge Regression**","30ba7180":"**We can see some names are incorrect like toyouta, vokswagen etc , so let's correct it**","335f7844":"# Building Model","004e07d4":"**BaseLine Model = Linear Regression**","24b1b6c5":"**1. Lasso**"}}