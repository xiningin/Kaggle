{"cell_type":{"6e74fac2":"code","eb3e4969":"code","88d64362":"code","3882bcb7":"code","cc39a0cc":"code","b85b48c1":"code","cc8ccdac":"code","cb1208f0":"markdown","f5bf953c":"markdown","266924e8":"markdown","e03ffe37":"markdown","b0a8a794":"markdown","40fe552b":"markdown","31959d8b":"markdown","bbe3da22":"markdown","3de47914":"markdown","e1c26cf8":"markdown","ffdb72a1":"markdown","7c1ee9d8":"markdown","a3a665af":"markdown","7b152733":"markdown","b8f096bf":"markdown","1d1c08ad":"markdown"},"source":{"6e74fac2":"%%capture\n!pip install hub\n!pip install --upgrade scikit-learn\n\nimport os\nimport hub\nimport glob\nimport torch\nimport PIL.Image\nimport numpy as np\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom skimage import img_as_float\nfrom skimage.transform import resize\nfrom hub.schema import ClassLabel, Image\nfrom torchvision import models, transforms\n\nN_CLASSES = 2\n\n#!hub login -u <username> -p <password","eb3e4969":"%%capture\n\n# Create the dataset Schema\nschema = {\n    \"image\": Image(shape=(None, None, None), max_shape=(3000, 3000, 3), dtype=\"uint8\"),\n    \"label\": ClassLabel(num_classes=2),\n}\n\n# A Tag to identify our dataset\ntag = \"sauravmaheshkar\/hot-dog-not-hot-dog-train\"\n\n# Length of the Dataset\nlen_ds = 498\n\nds = hub.Dataset(tag, mode=\"w+\", shape=(len_ds,), schema=schema)","88d64362":"# Transform function\n@hub.transform(schema=schema, scheduler=\"threaded\", workers=8)\ndef fill_ds(filename):\n    # If the image is in the Not Hot Dog Folder\n    if os.path.basename(os.path.dirname(filename)) == \"not_hot_dog\":\n        label = 0\n    else:\n        label = 1\n    image = np.array(PIL.Image.open(filename))\n    if len(image.shape) == 2:\n        image = np.expand_dims(image, -1)\n    return {\n        \"image\": image,\n        \"label\": label,\n    }","3882bcb7":"# Get the filenames\nfile_list = glob.glob(\"..\/input\/hot-dog-not-hot-dog\/train\/*\/*.jpg\")\n\n# Apply Transformation\nds = fill_ds(file_list)\n\n# Upload to Hub\nds = ds.store(tag)\nds.flush()","cc39a0cc":"%%time\n# Fetch the old Dataset\ntrain_dataset = hub.Dataset(\"sauravmaheshkar\/hot-dog-not-hot-dog-train\")\n\n# Schema with New Image Size\nnew_schema = {\n    \"resized_image\": Image(shape=(224, 224, 3), dtype=\"uint8\"),\n    \"label\": ClassLabel(num_classes=2)\n}\n\n# Transformation Pipeline\ntfms = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n])\n\n# New hub transform for resizing and normalization\n@hub.transform(schema=new_schema, scheduler=\"threaded\", workers=8)\ndef resize_transform(index):\n    image = resize(train_dataset['image', index].compute(), (224, 224, 3), anti_aliasing=True)\n    image = tfms(image)\n    image = img_as_float(image.view(224, 224, 3)) \n    label = int(train_dataset['label', index].compute())\n    return {\n        \"resized_image\": image,\n        \"label\": label\n    }\n\n# New Resized Dataset instance\nresized_ds = resize_transform(range(498))\n\n# Upload Resized Dataset to Hub\nurl = \"sauravmaheshkar\/resized-hot-dog-not-hot-dog\"\npytorch_dataset = resized_ds.store(url)\npytorch_dataset.flush()","b85b48c1":"%%capture\n\n# Import Resnet18 from torchvision.models\nmodel = models.resnet18(pretrained=True)\n\n# Add a Pooling Layer\nmodel.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n\n# Add a FC Layer\nmodel.fc = torch.nn.Linear(model.fc.in_features, N_CLASSES)","cc8ccdac":"# Fetch Resized Dataset\npytorch_dataset = hub.Dataset(\"sauravmaheshkar\/resized-hot-dog-not-hot-dog\")\n\n# Convert to Pytorch Compatible Format with output type as list\npytorch_dataset = pytorch_dataset.to_pytorch(output_type = list)\n\n# Creating DataLoader\ntrain_loader = torch.utils.data.DataLoader(pytorch_dataset, batch_size=32, num_workers=4)\n\n# Some Hyperparameters\nn_epochs = 20\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.003)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Training\nfor epoch in range(n_epochs):\n    print(f\"Epoch {epoch}\")\n    # Setting Running Loss to Zero\n    running_loss = 0.0\n    for i, data in enumerate(train_loader, 0):\n        # Get image, label pair\n        X, y = data\n        # Convert into proper format and dtype\n        y = y.type(torch.LongTensor).to(device)\n        X = X.permute(0, 3, 1, 2).float().to(device)\n        # Set gradients to Zero\n        optimizer.zero_grad()\n        # Get output from the model\n        outputs = model(X)\n        # Calculate the loss\n        loss = criterion(outputs, y)\n        # Perform Backprop\n        loss.backward()\n        optimizer.step()\n\n        # Update the Loss\n        running_loss += loss.item()\n    print(f\"Loss {loss.item()}\")\nprint(\"Finished Training\")","cb1208f0":"Schema's are an essential part of the Hub ecosystem. They define the structure, shapes, data types, meta information(image channels, class names, etc.) and special serialization\/deserialization methods. Currently there are about a dozen available in Hub such as Image, Mask, Segmentation and Text. For the complete list, please visit this [link](https:\/\/docs.activeloop.ai\/en\/latest\/concepts\/features.html#available-schemas).\n\nAs we're trying to build a Binary Image Classifier, our dataset schema contains only 2 components, namely:\n\n1. `image`\n2. `label`\n\n\nWe define a Hub schema as a standard python dict. We use the `ClassLabel` and `Image` from `hub.schema` and specify some parameters such as `shape`, `max_shape`, `dtype` and `num_classes`.\n\nNext up we'll create a instance of a Hub `Dataset` by specifying the:\n\n* **name\/path**: Either the file path to the dataset or the tag that will be used to identify the dataset on activeloop platform, such as `sauravmaheshkar\/hot-dog-not-hot-dog-train`.\n* **mode**: Reading (`r`) & writing (`w`) mode\n* **shape**: The Shape of the Dataset (follows numpy shape convention, such as `(4,)`)\n* **schema**: The Schema for the dataset defined as a `dict`","f5bf953c":"To find out more about Hub and how to use it in your own projects, visit the [activeloopai\/Hub](https:\/\/github.com\/activeloopai\/Hub) GitHub repository. For more advanced data pipelines like uploading large datasets or applying many transformations, please refer to the [documentation](https:\/\/docs.activeloop.ai\/en\/latest\/).","266924e8":"<a id='train'><\/a>\n\n\n# Training \ud83d\udcaa\ud83c\udffb\n\nNow that we have resized and normalized our images and created a model, the next step is to train the model. We'll fetch the resized dataset and use the `.to_pytorch()` function to convert the dataset into pytorch compatible format. We'll create a DataLoader instance from the converted dataset and simply train our model.","e03ffe37":"The uploaded datasets will be available in the Activeloop\u2019s visualization app for free. Here's the link for the datasets:-\n\n* [hot-dog-not-hot-dog-train](https:\/\/app.activeloop.ai\/datasets\/explore?tag=sauravmaheshkar%2Fhot-dog-not-hot-dog-train)\n* [hot-dog-not-hot-dog-test](https:\/\/app.activeloop.ai\/datasets\/explore?tag=sauravmaheshkar%2Fhot-dog-not-hot-dog-test)","b0a8a794":"Let's look at what else we can do with `@hub.transform()`. An essential part of any Computer Vision pipeline is image pre-processing. In this Kernel we're going to use a `Resnet18` to train a Binary Image Classifier. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\n\nThus, we'll create another dataset, this time with a Image Size of `(224, 224, 3)`, and then normalize the images using `torchvision.transforms`. ","40fe552b":"![](https:\/\/github.com\/SauravMaheshkar\/Hot-Dog-Not-Hot-Dog\/blob\/main\/assets\/Banner.png?raw=true)","31959d8b":"<a id='transform'><\/a>\n# Transform \ud83d\udc4e\ud83c\udffb -> \ud83d\udc4d\n\nHub Transform provides a functionality to modify the samples of the dataset or create a new dataset from the existing one. To apply these modifications one needs to add a `@hub.transform` decorator to any custom function. User defined transform function is applied to every sample in the input. It takes in an iterator or a Hub dataset, and output another dataset with the specified schema. There are optimizations done behind the scenes such as chunking to efficiently process and store the data.\n\nIn this transform we associate a label `0` if the image is in the `not_hot_dog` folder and `1` if the it is in the `hot_dog` folder.","bbe3da22":"[Link](https:\/\/app.activeloop.ai\/datasets\/explore?tag=sauravmaheshkar%2Fresized-hot-dog-not-hot-dog) to the Resized Dataset on activeloop platform.","3de47914":"<a id='upload'><\/a>\n# Upload \u2b06\ufe0f to Hub\n\nNow that we've created a `hub.Dataset` instance and created our transform function (`fill_ds`), we'll just pass in the file names and then upload our dataset to the platform.","e1c26cf8":"# Table of Contents\n\n1. [Packages \ud83d\udce6 and Basic Setup](#basic)\n2. [\ud83d\ude80 Getting Started](#start)\n3. [\ud83c\udfd7 Building the Schema](#schema)\n4. [Transform \ud83d\udc4e\ud83c\udffb -> \ud83d\udc4d](#transform)\n5. [Upload \u2b06\ufe0f to Hub](#upload)\n6. [Advanced \ud83e\udd13 Transformations](#adv)\n7. [The Model \ud83d\udc77\u200d\u2640\ufe0f](#model)\n8. [Training \ud83d\udcaa\ud83c\udffb](#train)","ffdb72a1":"<a id='start'><\/a>\n\n# \ud83d\ude80 Getting Started \n\nYou can access public datasets in Hub by following a straight-forward convention. For example to get the first 1000 images of the MNIST database, we can run the following snippet:\n\n```\nfrom hub import Dataset\n\nmnist = Dataset(\"activeloop\/mnist\")\nmnist[\"image\"][0:1000].compute()\n```\n\nIn this kernel however we'll create our own Dataset and then upload it to the Activeloop platform. Let's get started.","7c1ee9d8":"<a id = 'schema'><\/a>\n\n# \ud83c\udfd7 Building the Schema ","a3a665af":"[![hub](https:\/\/img.shields.io\/badge\/powered%20by-hub%20-ff5a1f.svg)](https:\/\/github.com\/activeloopai\/Hub)\n\n\nHave you ever tried using not so popular datasets such as `CelebA` or `ag_news` using services such as **Tensorflow Datasets(tfds)** or **torchvision.datasets** ? Well I have and most of the time, you'll run into several errors finally resorting to traditional methods such as `zip` and `tar`. (Unless they're available on Kaggle)\n\nAndrej Karpathy famously referred to neural networks as **Software 2.0**, referring to the fact that the code behind many of the applications currently in use is much more abstract, such as the weights of a neural network. Shouldn't we have a new paradigm for our data as well ? Version control provided an excellent way to manage Software 1.0, but don't we need a better way to store datasets as well, a system where repositories are datasets and commits are made up of additions and edits of the labels.\n\nIntroducing [Hub](https:\/\/github.com\/activeloopai\/Hub). Hub allows you to store your (even petabyte-scale) datasets on the cloud, so you can seamlessly access and work with it from any machine. You can even version control datasets similarly to git version control. Each version doesn't store the exact copy but rather the diifferences.","7b152733":"<a id = 'basic'> <\/a>\n\n# Packages \ud83d\udce6 and Basic Setup\n\nUsing Activeloop\/Hub you can work with public or your own data, locally or on any cloud. In this tutorial notebook, we'll upload the Hot-Dog-Not-Hot-Dog dataset to the Activeloop\/Hub platform, as well as visualize it within the web app.\n\nTo load a public dataset, one needs to write dozens of lines of code and spend hours accessing and understanding the API as well as downloading the data. With Hub, you only need two lines of code, and you can get started working on your dataset in under three minutes.\n\n\nFirst things first, we install the python package using pip\n\n```\npip install hub\n```\n\nYou'll also need to register and login into Hub. You can register for an account at this link https:\/\/app.activeloop.ai\/","b8f096bf":"<a id = 'model'> <\/a>\n# The Model \ud83d\udc77\u200d\u2640\ufe0f\n\n\n## Transfer Learning\n\nThe main aim of transfer learning (TL) is to implement a model quickly i.e. instead of creating a DNN (dense neural network) from scratch, the model will transfer the features it has learned from the different dataset that has performed the same task. This transaction is also known as **knowledge transfer**.\n\n## Resnet18\nA residual network, or ResNet for short, is a DNN that helps to build deeper neural networks by utilizing skip connections or shortcuts to jump over some layers. This helps solve the problem of vanishing gradients.\n\nThere are different versions of ResNet, including ResNet-18, ResNet-34, ResNet-50, and so on. The numbers denote layers, although the architecture is the same.\n\n![](https:\/\/i.imgur.com\/XwcnU5x.png)\n\nIn the end, we just add a Adaptive Pooling Layer and a Fully Connected Layer with output dimensions equal to the number of classes.","1d1c08ad":"<a id='adv'><\/a>\n# Advanced \ud83e\udd13 Transformations\n"}}