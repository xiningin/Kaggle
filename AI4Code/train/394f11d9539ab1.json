{"cell_type":{"f37ec64c":"code","0c45a4f5":"code","b62633c4":"code","a1d94902":"code","ce7644f5":"code","19cc2a31":"markdown","913818cb":"markdown","f1f24f36":"markdown","7674b9ac":"markdown","32f9f799":"markdown","4e73ebff":"markdown","b61b24af":"markdown","25664b40":"markdown"},"source":{"f37ec64c":"import numpy as np\nimport pandas as pd \nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn import neighbors\nfrom sklearn.svm import SVR\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","0c45a4f5":"def data_prep():\n    df = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\") #calling the dataset\n\n    #we added variables that would enable our models to explain better\n    #values of some variables on year basis\n    df[\"Mean_CAtBat\"] = df[\"CAtBat\"] \/ df[\"Years\"]\n    df[\"Mean_CHits\"] = df[\"CHits\"] \/ df[\"Years\"]\n    df[\"Mean_CHmRun\"] = df[\"CHmRun\"] \/ df[\"Years\"]\n    df[\"Mean_Cruns\"] = df[\"CRuns\"] \/ df[\"Years\"]\n    df[\"Mean_CRBI\"] = df[\"CRBI\"] \/ df[\"Years\"]\n    df[\"Mean_CWalks\"] = df[\"CWalks\"] \/ df[\"Years\"]\n\n    #variables that affect the model less were removed from the dataset\n    df = df.drop(['AtBat','Hits','HmRun','Runs','RBI','Walks','Assists',\n                  'Errors',\"PutOuts\",'League','NewLeague', 'Division'], axis=1)\n\n    #missing data filled in according to KNN\n    from sklearn.impute import KNNImputer\n    imputer = KNNImputer(n_neighbors = 4)\n    df_filled = imputer.fit_transform(df)\n    df = pd.DataFrame(df_filled,columns = df.columns)\n\n    #Suppression for contradictory observations in Salary variable\n    Q1 = df.Salary.quantile(0.25)\n    Q3 = df.Salary.quantile(0.75)\n    IQR = Q3-Q1\n    lower = Q1 - 1.5*IQR\n    upper = Q3 + 1.5*IQR\n    df.loc[df[\"Salary\"] > upper,\"Salary\"] = upper\n    \n    y = df[\"Salary\"]\n    X = df.drop(\"Salary\",axis=1)\n    \n    #standardizing distributions\n    from sklearn.preprocessing import RobustScaler\n    scaler = RobustScaler()\n    X = scaler.fit_transform(X)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                        test_size=0.20, \n                                                        random_state=46)\n    \n    return X_train, X_test, y_train, y_test","b62633c4":"def modeling(X_train, X_test, y_train, y_test):\n    #KNN\n    \n    knn_params = {'n_neighbors': 6}\n\n    knn_tuned = KNeighborsRegressor(**knn_params).fit(X_train, y_train)\n    \n    #test error\n    y_pred = knn_tuned.predict(X_test)\n    knn_final = np.sqrt(mean_squared_error(y_test, y_pred))\n    print(\"final rmse KNN\")\n    print(knn_final)\n    \n    #SVR\n    svr_params = {'C': 1000}\n\n    svr_tuned = SVR(**svr_params).fit(X_train, y_train)\n    \n    #test error\n    y_pred = svr_tuned.predict(X_test)\n    svr_final = np.sqrt(mean_squared_error(y_test, y_pred))\n    print(\"final rmse SVR\")\n    print(svr_final)\n    \n    #CART\n    cart_params = {'max_depth': 5, 'min_samples_split': 50}\n    \n    cart_tuned = DecisionTreeRegressor(**cart_params).fit(X_train, y_train)\n    \n    #test error\n    y_pred = cart_tuned.predict(X_test)\n    cart_final = np.sqrt(mean_squared_error(y_test, y_pred))\n    print(\"final rmse CART\")\n    print(cart_final)\n    \n    \n    #RANDOM FOREST\n    rf_params = {'max_depth': 5, 'max_features': 2, 'min_samples_split': 2, 'n_estimators': 900}\n\n    rf_tuned = RandomForestRegressor(**rf_params).fit(X_train, y_train)\n\n    #test error\n    y_pred = rf_tuned.predict(X_test)\n    rf_final = np.sqrt(mean_squared_error(y_test, y_pred))\n    print(\"final rmse RF\")\n    print(rf_final)\n    \n    #GBM\n    \n    gbm_params = {'learning_rate': 0.01,\n    'loss': 'lad', \n    'max_depth': 5,\n    'n_estimators': 500,\n    'subsample': 0.5}\n    \n    gbm_tuned = GradientBoostingRegressor(**gbm_params).fit(X_train, y_train)\n\n    #test error\n    y_pred = gbm_tuned.predict(X_test)\n    gbm_final = np.sqrt(mean_squared_error(y_test, y_pred))\n    print(\"final rmse GBM\")\n    print(gbm_final)\n    \n    #XGBoost    \n    xgb_params = {'colsample_bytree': 1,\n    'learning_rate': 0.01,\n    'max_depth': 2,\n    'n_estimators': 500}\n    \n    xgb_tuned = XGBRegressor(**xgb_params).fit(X_train, y_train)\n    \n    y_pred = xgb_tuned.predict(X_test)\n    xgb_final = np.sqrt(mean_squared_error(y_test, y_pred))\n    print(\"final rmse XGBoost\")\n    print(xgb_final)\n    \n    #Light GBM\n    lgbm_params = {'colsample_bytree': 1,\n    'learning_rate': 0.01,\n    'max_depth': 5,\n    'n_estimators': 200}\n    \n    lgbm_tuned = LGBMRegressor(**lgbm_params).fit(X_train, y_train)\n    \n    y_pred = lgbm_tuned.predict(X_test)\n    lbg_final = np.sqrt(mean_squared_error(y_test, y_pred))\n    print(\"final rmse LightGBM\")\n    print(lbg_final)\n    \n    #CatBoost\n    catb_params = {'depth': 10, 'iterations': 500, 'learning_rate': 0.1}\n    \n    catb_tuned = CatBoostRegressor(**catb_params).fit(X_train, y_train)\n    \n    y_pred = catb_tuned.predict(X_test)\n    cat_final = np.sqrt(mean_squared_error(y_test, y_pred))\n    print(\"final rmse CatBoost\")\n    print(cat_final)\n    \n    #Neural Networks\n    mlp_params = {'alpha': 0.1, 'hidden_layer_sizes': (1000, 100, 10)}\n    \n    mlp_tuned = MLPRegressor(**mlp_params).fit(X_train, y_train)\n    \n    y_pred = mlp_tuned.predict(X_test)\n    neural_final = np.sqrt(mean_squared_error(y_test, y_pred))\n    print(\"final rmse Neural Networks\")\n    print(neural_final)\n    \n    results = pd.DataFrame({\"Test Tuned Error\":[knn_final, svr_final, cart_final, \n                                                rf_final,gbm_final, xgb_final, \n                                                lbg_final, cat_final, neural_final]})\n    results.index= [\"KNN\", \"SVR\",\"CART\",\"Random Forests\",\"GBM\",\"XGBoost\", \"LightGBM\", \n                    \"CatBoost\", \"Neural Networks\"]\n    print(results)","a1d94902":"X_train, X_test, y_train, y_test = data_prep()","ce7644f5":"modeling(X_train, X_test, y_train, y_test)","19cc2a31":"![baseball.jpg](attachment:baseball.jpg)","913818cb":"# The Goal of the Project\n\nThe aim of this study is to apply nonlinear regression models to the \"Hitters\" dataset and compare the models. Also in this dataset, the model that gives the best results for the estimation of player salaries has been determined.","f1f24f36":"# Modeling","7674b9ac":"### Dataset Information: \n\nThis dataset was originally taken from the StatLib library which is maintained at Carnegie Mellon University. This is part of the data that was used in the 1988 ASA Graphics Section Poster Session. The salary data were originally from Sports Illustrated, April 20, 1987. The 1986 and career statistics were obtained from The 1987 Baseball Encyclopedia Update published by Collier Books, Macmillan Publishing Company, New York.\n\nLink to the Data Set: https:\/\/www.kaggle.com\/floser\/hitters\n\n### Attribute Information:\n\nA data frame with 322 observations of major league players on the following 20 variables.\n\n- AtBat: Number of times at bat in 1986\n- Hits: Number of hits in 1986\n- HmRun: Number of home runs in 1986\n- Runs: Number of runs in 1986\n- RBI: Number of runs batted in in 1986\n- Walks: Number of walks in 1986\n- Years: Number of years in the major leagues\n- CAtBat: Number of times at bat during his career\n- CHits: Number of hits during his career\n- CHmRun: Number of home runs during his career\n- CRuns: Number of runs during his career\n- CRBI: Number of runs batted in during his career\n- CWalks: Number of walks during his career\n- League: A factor with levels A and N indicating player\u2019s league at the end of 1986\n- Division: A factor with levels E and W indicating player\u2019s division at the end of 1986\n- PutOuts: Number of put outs in 1986\n- Assists: Number of assists in 1986\n- Errors: Number of errors in 1986\n- Salary: 1987 annual salary on opening day in thousands of dollars\n- NewLeague: A factor with levels A and N indicating player\u2019s league at the beginning of 1987","32f9f799":"# Installation of Libraries","4e73ebff":"# **Conclusion**\n\n  - Our aim is to compare the models and minimize error scores for dataset. This was done with two functions.\n    \n  - Data preprocessing and modeling were defined by two functions. We run all processes and models with two functions. We have given the values we obtained in data preprocessing as an input to the modeling function. So we got the modeling results.\n    \n  - Hitters Data Set read.\n  \n  - NA values filled with the KNN algorithm.\n  \n  - X variables standardized with RobustScaler.\n  \n  - Predetermined optimum values are assigned as hyperparameter values for each model.\n    \n  - It was determined that the model with the least rmse in the dataset was the GBM model.","b61b24af":"# Data Preproccesing","25664b40":"# Resources\n\n- https:\/\/www.kaggle.com\/ahmetcankaraolan\/salary-predict-with-nonlinear-regression-models\n\n- https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.RobustScaler.html"}}