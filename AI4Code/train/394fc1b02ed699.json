{"cell_type":{"9c8345f3":"code","fb966aab":"code","8cad4a8f":"code","b61a5c2d":"code","3e9f24fb":"code","99feb85b":"code","d47a3fb5":"code","2940a767":"code","3bb45b82":"code","e3c3e805":"markdown","4256b2a0":"markdown","422f443b":"markdown","feb72a4d":"markdown","79fc4d27":"markdown","ae01bab2":"markdown"},"source":{"9c8345f3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport warnings\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","fb966aab":"papers = pd.read_csv(\"..\/input\/papers.csv\")","8cad4a8f":"papers.head(5)","b61a5c2d":"papers_copy = papers.copy()\npapers_copy.drop(columns = ['id','event_type','pdf_name'],inplace=True)\npapers_copy.head(3)\n# We can now go on and prepare the daraset for modelling","3e9f24fb":"groups = papers_copy.groupby(\"year\")\ncounts = groups.size()\ncounts.plot(kind='bar',figsize=(9,7));\nplt.title(\"ML Publications per year\");\nplt.xlabel(\"Years..!!\");","99feb85b":"#Preprocessing the Text Data\n#1. Remove any punctutations\n#2. Perform LowerCasing\n#3. Print title before and after modifications\nimport re\nprint(\"Before MODIFICATION:\")\nprint(papers_copy['title'].head())\npapers_copy['title_processed'] = papers_copy['title'].map(lambda x: re.sub('[,\\.!?]','',x))\npapers_copy['title_processed'] = papers_copy['title_processed'].map(lambda x:x.lower())\nprint(\"After MODIFICATION\")\nprint(papers_copy['title_processed'].head())","d47a3fb5":"import wordcloud\nlong_string = \"\".join(papers_copy.title_processed)\nwordcloud = wordcloud.WordCloud()\nwordcloud.generate(long_string)\nwordcloud.to_image()\n\n#Word cloud tends out to be a very helpful tool to visualize the mmost used words in the particular feature.","2940a767":"from sklearn.feature_extraction.text import CountVectorizer\ndef plot_10_most_common_words(count_data,count_vectorizer):\n    words = count_vectorizer.get_feature_names()\n    total_counts = np.zeros(len(words))\n    for t in count_data:\n        total_counts +=t.toarray()[0]\n    count_dict = (zip(words,total_counts))\n    count_dict= sorted(count_dict,key=lambda x:x[1],reverse = True)[0:10]\n    words = [w[0] for w in count_dict]\n    counts = [w[1] for w in count_dict]\n    x_pos = np.arange(len(words))\n    \n    plt.bar(x_pos,counts,align='center')\n    plt.xticks(x_pos,words,rotation=90)\n    plt.ylabel('counts')\n    plt.xlabel('words')\n    plt.title('10 Most Common Words..!!')\n    plt.show()\n    \n#Initialize the count vectorizer with the English Stop Words:\ncount_vectorizer = CountVectorizer(stop_words = 'english')\n#Fit and transform the processed titles\ncount_data = count_vectorizer.fit_transform(papers_copy['title_processed'])\n#Visualize the most 10-most common words\nplot_10_most_common_words(count_data,count_vectorizer)","3bb45b82":"warnings.simplefilter(\"ignore\",DeprecationWarning)\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA\n#Load the LDA model from sklearn\ndef print_topics(model,count_vectorizer,n_top_words):\n    words = count_vectorizer.get_feature_names()\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"\\nTopic #%d:  \"% topic_idx)\n        print(\" \".join([words[i] for i in topic.argsort()[:- n_top_words -1: -1]]))\n#Tweak the two parameters below:\nnumber_topics =10\nnumber_words=6\n#Create and fit the LDA Model\nlda = LDA(n_components = number_topics)\nlda.fit(count_data)\n#Print the topics found by the LDA Model:\nprint(\"Topics Found: \")\nprint_topics(lda,count_vectorizer,number_words)","e3c3e805":" # We find that:\n*  id\n*  event_type, and \n*  pdf_name\nare redundant and not important for prediction.\n# So we prepare the data by droping these columns in a new copied Data Frame","4256b2a0":"# We see that between 2007 to 2017...the publications got almost tripled while between 1987 to 2007, there was a small significant change","422f443b":"# Now we will use Word Cloud to Visualize the preprocessed text data:\n","feb72a4d":"# Analyzing trends with LDA","79fc4d27":"## Visualize the number of publications per year","ae01bab2":"# Prepare the Text for LDA Ananlysis:\n## LDA: Latent Dirichlet Allocation\n### It performs the \"topic detection\" for the large data sets, determining main \"topics\" that are in a large unlabelled set of texts"}}