{"cell_type":{"ee8406a7":"code","43410349":"code","d4151ae4":"code","e65dd7d0":"code","04d92499":"code","0147c801":"code","95c08227":"code","8c8e74d6":"code","93a85402":"code","7a2fce1a":"code","43e7ea2d":"code","cb134cf1":"code","b16cf623":"code","d119eea0":"code","65c1c629":"code","b2005d16":"code","2eb23a98":"code","208ea0bf":"code","a6432ba0":"code","3af0ee82":"markdown","24b73694":"markdown","36f02572":"markdown","0e32fe6c":"markdown"},"source":{"ee8406a7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","43410349":"dataset = pd.read_csv('\/kaggle\/input\/on-time-graduation-classification\/data_lulus_tepat_waktu.csv')","d4151ae4":"dataset.head()","e65dd7d0":"dataset.shape","04d92499":"dataset.info()","0147c801":"dataset.isnull().values.any()","95c08227":"ket = {\"Ya\" : 0, \"Tidak\" : 1}\ndataset[\"tepat\"] = dataset[\"tepat\"].map(ket)","8c8e74d6":"dataset['tepat'].value_counts()","93a85402":"print('Ya', round(dataset['tepat'].value_counts()[0]\/len(dataset) * 100,2), '% of the dataset')\nprint('Tidak', round(dataset['tepat'].value_counts()[1]\/len(dataset) * 100,2), '% of the dataset')","7a2fce1a":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ncolors = [\"#0101DF\", \"#DF0101\"]\n\nsns.countplot('tepat', data=dataset, palette=colors)\nplt.title('Class Distributions \\n (0: YA || 1: Tidak)', fontsize=14)","43e7ea2d":"# Class count\ncount_class_0, count_class_1 = dataset.tepat.value_counts()\n\n# Divide by class\ndf_class_0 = dataset[dataset['tepat'] == 0]\ndf_class_1 = dataset[dataset['tepat'] == 1]","cb134cf1":"df_class_1_over = df_class_1.sample(count_class_0, replace=True)\ndf_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n\nprint('Random over-sampling:')\nprint(df_test_over.tepat.value_counts())\n\ndf_test_over.tepat.value_counts().plot(kind='bar', title='Count (tepat)');","b16cf623":"df_test_over.tepat.value_counts()","d119eea0":"x = df_test_over.iloc[:, :-1].values\ny = df_test_over.iloc[:, -1].values","65c1c629":"from sklearn.model_selection import train_test_split\n\nvalidation_size = 0.20\nnum_trees = 100\nseed = 5\nx_train, x_test, y_train, y_test=train_test_split(x, y, test_size=validation_size, random_state=seed)","b2005d16":"#Check Algorithms\nmodels = []\nmodels.append(( 'LR' , LogisticRegression()))\nmodels.append(( 'LDA' , LinearDiscriminantAnalysis()))\nmodels.append(( 'KNN' , KNeighborsClassifier()))\nmodels.append(( 'CART' , DecisionTreeClassifier()))\nmodels.append(( 'NB' , GaussianNB()))\nmodels.append(('RF', RandomForestClassifier(n_estimators=num_trees, random_state=seed)))\n# evaluate each model\nresults = []\nnames = []\nfor name, model in models:\n  kfold = KFold(n_splits=2, random_state=seed)\n  cv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring= 'accuracy' )\n  results.append(cv_results)\n  names.append(name)\n  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n  print(msg)","2eb23a98":"rf = RandomForestClassifier(n_estimators=num_trees, random_state=seed)\nrf.fit(x_train, y_train)\npredictions = rf.predict(x_test)\nprint(accuracy_score(y_test, predictions))\nprint(confusion_matrix(y_test, predictions))\nprint(classification_report(y_test, predictions))","208ea0bf":"# ip1 = 3.17\n# ip2 = 3.02\n# ip3 = 3.28\n# ip4 = 2.96\n\nprediction_rf=rf.predict([[3.17,3.02,3.28,2.96]])\nscore1 = rf.score(x_test, y_test)\nif prediction_rf[0] == 0:\n    pred = \"Tepat Waktu\"\nelse:\n    pred = \"Tidak Tepat Waktu\"\nprint('Prediksi :',pred)\nprint(\"Test score: {0:.2f} %\".format(100 * score1)) ","a6432ba0":"prediction_rf=rf.predict([[3.07,3.04,3.39,3.55]])\nscore1 = rf.score(x_test, y_test)\nif prediction_rf[0] == 0:\n    pred = \"Tepat Waktu\"\nelse:\n    pred = \"Tidak Tepat Waktu\"\nprint('Prediksi :',pred)\nprint(\"Test score: {0:.2f} %\".format(100 * score1)) ","3af0ee82":"# **RF paling besar 94%**","24b73694":"# Dataset tidak balance, jomplang bet perbedaannya.\n\nkita Resampling datanya. Resampling ada 2 :\n\n\n* Random Oversampling: Randomly duplicate examples in the minority class.\n* Random Undersampling: Randomly delete examples in the majority class.\n\ndisini kita menggunakan random over sampling\n\n[Referensi dari sini le, buat resampling nya](http:\/\/https:\/\/www.kaggle.com\/rafjaa\/resampling-strategies-for-imbalanced-datasets\/log)","36f02572":"# kita test","0e32fe6c":"gas kan le, kalo udah balance"}}