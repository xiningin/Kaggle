{"cell_type":{"f1a9fce9":"code","f4ebbf4b":"code","0773af59":"code","0025e8d0":"code","1d1cbc36":"code","c44b7258":"code","af197dca":"code","d652862e":"code","aa27c2b7":"code","e074e098":"code","a6d55b23":"code","71607f97":"code","70cc097d":"code","b818f8be":"code","71b87d06":"code","3b8a26d0":"code","cb22b658":"code","c71232e8":"code","4608b4d8":"code","8221503a":"code","3e71a91d":"code","f652b2b2":"code","7270c86e":"code","04cd7727":"code","c71b502d":"code","a6262ca6":"code","e8507148":"code","43fefb9e":"code","2563ba1f":"code","9b291758":"code","31c074f5":"code","cec0fe56":"code","4b06c620":"code","79dc2ec9":"code","99dbcaec":"code","4aaa998b":"code","70a90fd3":"code","7d04798a":"code","962781c6":"code","4842c044":"code","735dbb32":"code","bd47b38f":"code","41329ec6":"code","6542d6e1":"code","4eeb1972":"code","5dfae389":"code","aaeba0dd":"markdown","d4ea8c83":"markdown","48b2d883":"markdown","368efe75":"markdown","8a6c8d7b":"markdown","0bc1ced0":"markdown","bce184aa":"markdown","aee766f0":"markdown","6742c268":"markdown","a7482f08":"markdown","9bcf944a":"markdown","508bce71":"markdown","ccfd233e":"markdown","762dc8f3":"markdown","803fef20":"markdown","4fc0fb36":"markdown","b9f1925b":"markdown","34a17a78":"markdown","1a56219f":"markdown","a91ded08":"markdown","df061b15":"markdown","0fb50600":"markdown","56eebc3a":"markdown","567331b0":"markdown","54f582ab":"markdown","1a303c67":"markdown"},"source":{"f1a9fce9":"import pickle\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns","f4ebbf4b":"# Copy from previous notebook\ndef create_sub(preds, fn=None):\n    df =  pd.DataFrame({'Id': range(len(preds)), 'Prediction': preds})\n    return df","0773af59":"# if the pre-processed data cannot be loaded, we can repeat the preprocessing steps from before:\n\ndf_train = pd.read_csv('\/kaggle\/input\/kit-mathsee-ml-2021\/pp_train.csv', index_col=0)\ndf_test = pd.read_csv('\/kaggle\/input\/kit-mathsee-ml-2021\/pp_test.csv', index_col=0)\n\ndef preproc(df_in, means=None, stds=None, drop_vars=['time', 'station']):\n    df = df_in.copy()\n    if 't2m_obs' in df.columns: df.dropna(subset=['t2m_obs'], inplace=True)\n    df['sm_fc_mean'].replace(np.nan, df['sm_fc_mean'].mean(), inplace=True)\n    \n    y = df.pop('t2m_obs') if 't2m_obs' in df.columns else None\n    X = df.drop(drop_vars, 1)\n    \n    if means is None: means = X.mean()\n    if stds  is None: stds  = X.std()\n    \n    X = (X - means) \/ stds\n    return X, y, means, stds\n\nsplit_date = '2015-01-01'\nX_train, y_train, means, stds = preproc(df_train[df_train.time < split_date])\nX_valid, y_valid, _, _ = preproc(df_train[df_train.time >= split_date], means, stds)\nX_test, _, _, _ = preproc(df_test, means, stds)\nX_train.shape, X_valid.shape, X_test.shape","0025e8d0":"from keras.models import Sequential\nfrom keras.layers import *\nfrom keras.optimizers import SGD, Adam","1d1cbc36":"X_train.shape","c44b7258":"ln = Sequential([Dense(1, input_shape=(22,), activation='linear')])","af197dca":"ln.summary()","d652862e":"ln.compile(optimizer = Adam(1e-1), loss = 'mse')","aa27c2b7":"ln.fit(X_train, y_train, batch_size = 10000, epochs=12, validation_data=(X_valid, y_valid))","e074e098":"from sklearn.metrics import r2_score\ndef print_scores(m, X_train=X_train, X_valid=X_valid):\n    preds = m.predict(X_valid, 10000)\n    print('Train R2 = ', r2_score(y_train, m.predict(X_train, 10000)), \n          ', Valid R2 = ', r2_score(y_valid, preds), ', Valid MSE = ', \n          m.evaluate(X_valid, y_valid, 10000, 0))","a6d55b23":"print_scores(ln)","71607f97":"nn = Sequential([\n    Dense(256, input_shape=(22,), activation='relu'),\n    Dense(1, activation='linear')\n])","70cc097d":"nn.summary()","b818f8be":"nn.compile(optimizer = Adam(1e-3), loss = 'mse')","71b87d06":"nn.fit(X_train, y_train, batch_size = 1024, epochs=12, validation_data=(X_valid, y_valid))","3b8a26d0":"print_scores(nn)","cb22b658":"model = Sequential([\n    Dense(256, input_shape=(22,), activation='relu'),\n    Dense(256, activation='relu'),\n    Dense(256, activation='relu'),\n    Dense(1, activation='linear')\n])","c71232e8":"model.summary()","4608b4d8":"model.compile(optimizer = Adam(1e-4), loss = 'mse')","8221503a":"h = model.fit(X_train, y_train, 1024, epochs=30, validation_data=(X_valid, y_valid))","3e71a91d":"plt.plot(h.history['loss'][1:])\nplt.plot(h.history['val_loss'][1:])","f652b2b2":"import tensorflow as tf\nearly_stopping = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)","7270c86e":"model = Sequential([\n    Dense(256, input_shape=(22,), activation='relu'),\n    Dense(256, activation='relu'),\n    Dense(256, activation='relu'),\n    Dense(1, activation='linear')\n])","04cd7727":"model.compile(optimizer = Adam(1e-4), loss = 'mse')","c71b502d":"model.fit(X_train, \n          y_train, \n          1024, \n          epochs=30, \n          validation_data=(X_valid, y_valid), \n          callbacks=[early_stopping])","a6262ca6":"print_scores(model)","e8507148":"split_date = '2015-01-01'","43fefb9e":"df_train = df_train.dropna(subset=['t2m_obs'])\n\nstations = df_train.station\nstations_train = df_train[df_train.time < split_date]['station']\nstations_valid = df_train[df_train.time >= split_date]['station']\nstations_test = df_test.station","2563ba1f":"stations_train.head()","9b291758":"unique_stations = pd.concat([df_train.station, df_test.station]).unique()","31c074f5":"# dictionary: maps station id to integer value\nstat2id = {s: i for i, s in enumerate(unique_stations)}","cec0fe56":"ids = stations.apply(lambda x: stat2id[x])","4b06c620":"ids_train = ids[df_train.time < split_date]\nids_valid = ids[df_train.time >= split_date]\nids_test = stations_test.apply(lambda x: stat2id[x])","79dc2ec9":"ids_train.head()","99dbcaec":"from keras.models import Model","4aaa998b":"features_in = Input(shape=(22,))\nid_in = Input(shape=(1,))","70a90fd3":"emb = Embedding(len(unique_stations), 2)(id_in)\nemb = Flatten()(emb)","7d04798a":"x = Concatenate()([features_in, emb])","962781c6":"x = Dense(100, activation='relu')(x)\nout = Dense(1, activation='linear')(x)","4842c044":"model = Model(inputs=[features_in, id_in], outputs=out)","735dbb32":"model.summary()","bd47b38f":"tf.keras.utils.plot_model(model)","41329ec6":"model.compile(optimizer = Adam(1e-3), loss = 'mse')","6542d6e1":"model.fit([X_train, ids_train], y_train, 1024, 10, \n          validation_data=([X_valid, ids_valid], y_valid))","4eeb1972":"print_scores(model, X_train=[X_train, ids_train], X_valid=[X_valid, ids_valid])","5dfae389":"# Submit to Kaggle\ndf_sub = create_sub(model.predict([X_test, ids_test], 10000).squeeze(), 'nn_emb.csv'); df_sub.head()","aaeba0dd":"Next we want to concatenate the embedding features and the regular features together.","d4ea8c83":"Recall that with the sklearn linear regression algorithm we got a score of around 3.24. This indicates that we are doing pretty much the same here. But we can do better!","48b2d883":"## Your turn\n\nNow it's your turn. We learned lots of thing just now, so try to recreate the basic steps from this notebook:\n\n1. Train a linear regression using Keras and achieve a similar score to our sklearn linear regression.\n2. Train a neural network by adding a hidden layer.\n3. Add more layers and make them bigger. Prevent overfitting by using early stopping.\n4. Add an embedding for the station IDs.\n\nFurther challenges: If you are done with all of that, it's time to train the best possible model. This will be the last session with this dataset. Here are some ideas:\n\n1. Play around with the hyper-parameters of the NNs and see what works best.\n2. You may try to use the time information as well, or think about other useful features you might be able to engineer.\n3. Create a super-ensemble of NNs, RFs and LRs. You can also train several models of each and average the predictions.\n4. If you want to look under the hood of the embedding layer, you could try to visualize the two embedding features on a map. Did the network learn anything meteorologically reasonable?\n\n","368efe75":"We can see that the model has 23 parameters, 22 coefficients plus one bias term.\n\nNext we need to compile the model, which basically means telling Keras which optimizer to use and which loss function to minimize. In the background it also randomly initializes the weights and biases at this stage.\n\nWe will use the Adam optimizer, which is a fancy version of SGD: https:\/\/arxiv.org\/abs\/1412.6980","8a6c8d7b":"But can you do better?","0bc1ced0":"Next we build an embedding layer for the station ID input. At its core an embedding is a lookup table. For each station ID the embedding table stores a certain number of (in our case 2) parameters.\n\nAn embedding is a mapping from an integer to a vector of real numbers. In our case the vector has length two. The elements are also called latent features. These are then concatenates with the regular features and passed through one hidden layer as before.\n\nThe latent features are updated along with the weights and biases during training and can now represent station-specific information.","bce184aa":"# Notebook 3 - Neural networks with Keras\n\nOn to the most exciting machine learning technique: Neural networks. as you will see they are just as easy to use as the sklearn methods.\n\nThere are several deep learning libraries for Python. The three most commonly used are:\n\n- Keras: High level library based on Tensorflow (or others) that is easy to use and flexible enough for most standard users. It has a great documentation and online support.\n- Tensorflow: Google's neural network library. Most widely used in ML research. Flexible and powerful but also (unnecessarily?) complicated.\n- Pytorch: The newcomer developed by Facebook. Flexible like Tensorflow but with a nicer, more Pythonic API.\n\nHere we will use Keras which is a great start for most tasks.","aee766f0":"Technically early stopping is a callback. A callback is a function to be called during training, e.g. at the end of each epoch. This needs to be passed to the `.fit` method.","6742c268":"Finally, let's train the model.","a7482f08":"Now we can build a standard neural network.","9bcf944a":"*Some notes on the results*\n\n- We see that we are overfitting. This makes sense in a model with 6k parameters. There are techniques to prevent overfitting in neural networks, most notably dropout and weight decay (L2 regularization). We will cover those later. For some reason, I found those techniques not to work for this model and dataset. \n- The score is quite significantly better than our best single random forest model. This suggests that the nonlinear computing power of the neural network is useful.\n- This is not yet a deep neural network. DNNs have several hidden layers. Again, I found that for this dataset a DNN didn't perform better. This might be because the nonlinearities are not very strong, but feel free to prove me wrong.","508bce71":"There are two ways to build a model in Keras. We will start with the easier one, a sequential model. This means that it is a succession of layers. For linear regression we only have one linear layer that maps the inputs to the outputs. Layers where all inputs are connected to all outputs are called fully-connected or Dense layers.","ccfd233e":"Now we can train\/fit the model. For this we specify the training data, the batch size, the number of epochs. Optionally, we can pass on the validation data, so that we get a validation score every epoch.","762dc8f3":"So far we have built Keras models using the Sequential API which is great for building models that go straight from a single input to a single output. Now we have two inputs however. For more complex models, there is another API, the functional API.\n\nThe functional API works by defining the layers separately with inputs and outputs.\n\nFirst we start with two input layers, one for the regular features and one for the station ID.\n\n","803fef20":"## Neural network with a hidden layer\n\nNow let's actually build a neural network with a hidden layer. For this we simply add another Dense layer but this time with a non-linear activation function. In our case this is a Rectified Linear Unit or relu. There is no set rule for how many hidden layers or nodes to use. For this we just need to employ trial-and-error.","4fc0fb36":"Now we can train the model. Because we now have two inputs we need to pass a list of the two inputs to the fit function.","b9f1925b":"Wow, this model is pretty good. Turns out the station ID is a really important predictor.","34a17a78":"So now we have a pretty good model. To create a better model, you can try fiddling with the hyper-parameters, i.e. number of layers, number of neurons in each layer, type of activation function, learning rate, etc.\n\nBut actually, there is one feature that we have not used so far and that is the station ID. Station ID is a categorical feature rather than a continuous feature, so we have to treat it differently.\n\nIn neural networks, categorical features can be dealt with using something called embeddings.","1a56219f":"Early stopping has a `patience` parameter which indicates how long to wait for a decrease in score before stopping. Setting this to >1 makes sense because the validation score can fluctuate quite a bit sometimes.\n\n`restore_best_weights` means that the parameters of the network with the lowest validation score are restored.","a91ded08":"## Linear regression: The Keras way\n\nFirst, let's build our own linear regression algorithm with stochastic gradient descent. This should give us the same solution as the sklearn linear regression we did in the last notebook.","df061b15":"Finally, to create a model with the functional API, we need to connect the inputs and outputs.","0fb50600":"We can also plot the model.","56eebc3a":"What we see here is a classic case of overfitting. The training error continues to go down but the validation erro increases again. This happens when we overparameterize the problem, i.e. we have a model that is more complex than the problem we are trying to solve.\n\nOne way to deal with this is to go back to using a smaller neural network. But if we look at the validation scores the \"too big\" model actually gives a better validation score after a few epochs.\n\nThis is one of the key insights in deep learning. Overparameterizing a problem actually yields better scores if you figure out how to deal with the overfitting problem.\n\n### Early stopping\n\nThe easiest way to deal with overfitting is to simply say. I train until the validation score of my model stops decreasing. This is called early stopping.","567331b0":"## Early stopping and overfitting\/regularization\n\nNow we can add more hidden layers very easily. We can also increase the number of hidden neurons.","54f582ab":"## Embeddings\n\nWe saw in the previous notebook that the station information might be really important. We could train a separate neural network for each station but that would reduce the amount of training data for each individual model and probably lead to stronger overfitting. \n\nHere is a different method of using categorical variables in neural networks. Don't worry if you don't understand embeddings right away. The concept is easy but it takes a while to wrap your head around it (it did for me anyways). ","1a303c67":"The `stations_*` vectors contain the station ID for each sample. However the station IDs are kind of random with gaps between the individual stations. We need a continuous range of IDs. Therefore, we create a dictionary that maps from the actual station IDs to the desired continuous range of IDs."}}