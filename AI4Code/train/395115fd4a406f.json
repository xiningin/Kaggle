{"cell_type":{"66e8ae4e":"code","b6999d97":"code","e72fbff5":"code","70750b0e":"code","2dedac92":"code","bffd4c60":"code","e80f83d2":"code","659955c6":"code","a8b353bd":"code","fb342bfc":"code","da775e11":"code","a14993fc":"code","c0af3a33":"code","cd0c74bd":"code","6950d268":"code","b6bda9d6":"code","2751a54b":"code","da87717d":"code","25d5edd2":"code","0c5e79b1":"code","e8686318":"code","f24ef87d":"code","48caf782":"code","88d5b54e":"code","c95ee641":"code","50d024f5":"code","d27a4954":"code","fed51bf4":"code","a87e5ea2":"code","fc141508":"code","b851d56a":"code","b228dca5":"code","36300833":"code","6ea564a0":"code","c69b9f80":"code","108e7b15":"code","154d0af1":"code","ca03287b":"code","79c67122":"code","c86ae7ac":"code","122390d4":"code","ef7ef593":"code","975ddaa5":"code","4ef3727c":"code","0dc25d08":"code","8aa38b98":"code","2b9f90ac":"code","1744420c":"code","a0caf774":"code","d749f17e":"code","f7a8c4cd":"code","b7289716":"code","4d3117ae":"code","562e9a1c":"code","42a89b0d":"code","49fc7190":"code","c67adff4":"code","d46e0be8":"code","7f5e2503":"code","2e75fb9b":"code","7935bd6c":"code","3ebbd167":"code","2a8d7e1d":"code","0c63c1eb":"code","0468b489":"code","f480c9df":"code","1b1a61f5":"code","0df5526c":"code","8e0daf49":"code","a036b0e3":"code","647cd9cb":"code","34d1ae78":"code","c3acfde3":"code","c3469693":"code","9abff2e8":"code","a6c1e90b":"code","7e063790":"code","ac97fe99":"markdown","2b8ad1c3":"markdown","cd4a005f":"markdown","87018f22":"markdown","d4de0a61":"markdown","6acd7841":"markdown","2b6c28b3":"markdown","0ef91661":"markdown","370f248e":"markdown","a28433f7":"markdown","03be2f87":"markdown"},"source":{"66e8ae4e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b6999d97":"df = pd.read_csv(\"\/kaggle\/input\/vehicle-dataset-from-cardekho\/car data.csv\")\ndf.head()","e72fbff5":"print(df['Seller_Type'].unique())\nprint(df['Fuel_Type'].unique())\nprint(df['Transmission'].unique())\nprint(df['Owner'].unique())","70750b0e":"df.isnull().sum()\n","2dedac92":"df.isna().sum()","bffd4c60":"df.describe()","e80f83d2":"df.columns","659955c6":"final_dataset=df[['Year','Selling_Price','Present_Price','Kms_Driven','Fuel_Type','Seller_Type','Transmission','Owner']]\n","a8b353bd":"final_dataset['Current Year']=2020\n","fb342bfc":"final_dataset.head()\n","da775e11":"final_dataset['no_year']=final_dataset['Current Year']- final_dataset['Year']\n","a14993fc":"final_dataset.drop(['Year'],axis=1,inplace=True)\n","c0af3a33":"final_dataset.head()\n","cd0c74bd":"final_dataset=pd.get_dummies(final_dataset,drop_first=True)\n","6950d268":"final_dataset.head()\n","b6bda9d6":"final_dataset=final_dataset.drop(['Current Year'],axis=1)\n","2751a54b":"import seaborn as sns\nimport matplotlib.pyplot as plt","da87717d":"final_dataset.corr()\n","25d5edd2":"sns.pairplot(final_dataset)\n\n","0c5e79b1":"corrmat = df.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","e8686318":"X=final_dataset.iloc[:,1:]\ny=final_dataset.iloc[:,0]","f24ef87d":"X['Owner'].unique()\n","48caf782":"X.head()\n","88d5b54e":"from sklearn.ensemble import ExtraTreesRegressor\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesRegressor()\nmodel.fit(X,y)","c95ee641":"print(model.feature_importances_)\n","50d024f5":"feat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(5).plot(kind='barh')\nplt.show()","d27a4954":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","fed51bf4":"from sklearn.ensemble import RandomForestRegressor\n","a87e5ea2":"regressor=RandomForestRegressor()\n","fc141508":"n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\nprint(n_estimators)","b851d56a":"from sklearn.model_selection import RandomizedSearchCV\n","b228dca5":"n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# max_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","36300833":"random_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\n\nprint(random_grid)","6ea564a0":"rf = RandomForestRegressor()\n","c69b9f80":"rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)","108e7b15":"rf_random.fit(X_train,y_train)\n","154d0af1":"rf_random.best_params_\n","ca03287b":"rf_random.best_score_\n","79c67122":"predictions=rf_random.predict(X_test)\n","c86ae7ac":"sns.distplot(y_test-predictions)\n","122390d4":"plt.scatter(y_test,predictions)\n","ef7ef593":"from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))","975ddaa5":"import pickle\n# open a file, where you ant to store the data\nfile = open('random_forest_regression_model.pkl', 'wb')\n\n# dump information to that file\npickle.dump(rf_random, file)","4ef3727c":"from sklearn.linear_model import LinearRegression\n","0dc25d08":"model = LinearRegression()\nmodel.fit(X_train,y_train)\npreds = model.predict(X_test)","8aa38b98":"sns.distplot(y_test-preds)","2b9f90ac":"print(\"MAE:\", metrics.mean_absolute_error(y_test,preds))\nprint(\"MSE:\", metrics.mean_squared_error(y_test,preds))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, preds)))","1744420c":"from sklearn.linear_model import Ridge\n","a0caf774":"model1 = Ridge()\nmodel1.fit(X_train,y_train)\npredz = model.predict(X_test)\nsns.distplot(y_test-predz)","d749f17e":"alpha = [x for x in np.linspace(start = .5,stop = 1.5,num = 5)]\nfit_intercept = [\"True\",\"False\"]\nnormalize = [\"True\",\"False\"]\nmax_iter = [int(x) for x in np.linspace(start = 800,stop = 1500,num = 5)]\nsolver = ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag']\n","f7a8c4cd":"param_grid = {\n    \"alpha\":alpha,\n    \"fit_intercept\":fit_intercept,\n    \"normalize\":normalize,\n    \"max_iter\":max_iter,\n    'solver':solver\n}","b7289716":"rf = Ridge()","4d3117ae":"rf_final = RandomizedSearchCV(estimator = rf,param_distributions = param_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)","562e9a1c":"rf_final.fit(X_train,y_train)","42a89b0d":"rf_final.best_params_","49fc7190":"rf_final.best_score_","c67adff4":"preds1 = rf_final.predict(X_test)","d46e0be8":"sns.distplot(y_test-preds1)","7f5e2503":"from sklearn.linear_model import ElasticNet\n","2e75fb9b":"model = ElasticNet()\nmodel.fit(X_train,y_train)\npreds2 = model.predict(X_test)\nsns.distplot(y_test-preds2)","7935bd6c":"alpha = [x for x in np.linspace(start = .5,stop = 1.5,num = 5)]\nl1_ratio = [x for x in np.linspace(start= 0.2,stop = 0.9,num = 5)]\nfit_intercept = [\"True\",\"False\"]\nnormalize = [\"True\",\"False\"]\nmax_iter = [int(x) for x in np.linspace(start = 800,stop = 1500,num = 5)]\nselection = ['cyclic','random']","3ebbd167":"param_grid = {\n    \"alpha\":alpha,\n    \"l1_ratio\":l1_ratio,\n    \"fit_intercept\":fit_intercept,\n    \"normalize\":normalize,\n    \"max_iter\":max_iter,\n    \"selection\":selection\n}","2a8d7e1d":"rf = ElasticNet()","0c63c1eb":"rf_final2 = RandomizedSearchCV(estimator = rf,param_distributions = param_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)","0468b489":"rf_final2.fit(X_train,y_train)","f480c9df":"rf_final2.best_params_","1b1a61f5":"rf_final2.best_score_","0df5526c":"preds = rf_final2.predict(X_test)\nsns.distplot(y_test-preds)","8e0daf49":"import xgboost as xgb\n\nmodel = xgb.XGBRegressor()\nmodel.fit(X_train,y_train)\npred3 = model.predict(X_test)\nsns.distplot(y_test-pred3)","a036b0e3":"from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error,mean_absolute_error\nprint(mean_squared_error(y_test,pred3))\nprint(\"MAE\", mean_absolute_error(y_test,pred3))","647cd9cb":"sns.scatterplot(y_test,pred3)","34d1ae78":"from scipy.stats import uniform, randint\n\nxgb_model = xgb.XGBRegressor()\n\nparams = {\n    \"colsample_bytree\": uniform(0.7, 0.3),\n    \"gamma\": uniform(0, 0.5),\n    \"learning_rate\": uniform(0.03, 0.3), # default 0.1 \n    \"max_depth\": randint(2, 6), # default 3\n    \"n_estimators\": randint(100, 150), # default 100\n    \"subsample\": uniform(0.6, 0.4)\n}\n\nsearch = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=42, n_iter=200, cv=3, verbose=1, n_jobs=1, return_train_score=True)\n\nsearch.fit(X_train, y_train)\n\n# report_best_scores(search.cv_results_, 1)","c3acfde3":"search.best_params_","c3469693":"search.best_score_","9abff2e8":"predss = search.predict(X_test)\nsns.distplot(y_test-predss)","a6c1e90b":"print(\"MAE:\", metrics.mean_absolute_error(y_test,predss))\nprint(\"MSE:\", metrics.mean_squared_error(y_test,predss))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predss)))","7e063790":"sns.scatterplot(y_test,predss)","ac97fe99":"Before Optimization","2b8ad1c3":"Not a good approach!!","cd4a005f":"# Elastic Net","87018f22":"# Xgboost","d4de0a61":"In this kernel I will be performing some basic yet important parts of a machine learning development pipeline.I build this to refresh my concepts and want to share with anyone who wish the same.\nIt includes\n* Feature Engineering\n* Exploratory Data Analysis\n* Different Regression model implementaion and comparison","6acd7841":"# Ridge Regression","2b6c28b3":"# Linear Regression","0ef91661":"# Extra Trees Regression","370f248e":"For Deployment of this particular project \nRefer [here](https:\/\/github.com\/ananyd36\/mldeploy)\n> Thanks","a28433f7":"After Optimization","03be2f87":"Hyperparameter Tuning"}}