{"cell_type":{"ab56a5cc":"code","7f1e8945":"code","43a43ce4":"code","1397436b":"code","b7dcd97c":"code","a5633feb":"code","dfa76eb2":"code","033f155c":"code","276fccd2":"code","9c0b2ca2":"code","39c69174":"code","74a047ff":"code","93b4f463":"code","2ff5d3a9":"code","b887ae8f":"code","bb8d4752":"code","49a8242d":"code","43c9f19f":"code","4a73dbc0":"code","2c7fe504":"code","b225aee3":"code","71242a75":"code","297f8f76":"code","edd2649f":"code","aed874c0":"code","bba9bb9b":"code","31148f67":"code","cbb93116":"code","37ddf48b":"code","ef6f94d1":"code","ed8b05ff":"code","d0bc3e46":"code","0b0df489":"code","a9a30e72":"markdown","28449b17":"markdown","19b0b2c5":"markdown","1f7dd9bc":"markdown","1afb3ff2":"markdown","7b0673f9":"markdown","c86f882c":"markdown","74288e80":"markdown","4828ce4f":"markdown","49ef8509":"markdown","dc3ec671":"markdown","f100cda1":"markdown","3845f51e":"markdown","8dc55d1a":"markdown","e05808d5":"markdown","4f19f079":"markdown","fda462c0":"markdown","06ce7ba8":"markdown","3f754a6c":"markdown","b382a1f0":"markdown","2d4d9112":"markdown","b0cbf74b":"markdown","0e6f8ede":"markdown","da8b1a98":"markdown","a4717189":"markdown","2d0223d3":"markdown","d298ebe4":"markdown","915afa82":"markdown","b2062c9f":"markdown","180d77cf":"markdown","91cf0c32":"markdown","b9e1346d":"markdown","ff6e4d21":"markdown"},"source":{"ab56a5cc":"class Config:\n    vocab_size = 15000 # Vocabulary Size\n    sequence_length = 100 # Length of sequence\n    batch_size = 1024\n    validation_split = 0.15\n    embed_dim = 256\n    latent_dim = 256\n    oov_token = \"<OOV>\" # Out of Word token\n    bos_token = \"<BOS>\" # Begin of sequence token\n    eos_token = \"<EOS>\" # End of Sequence token\n    epochs = 50 # Number of Epochs to train\n    model_path = \"model.h5\"\nconfig = Config()","7f1e8945":"import pandas as pd\nimport tensorflow as tf\nimport pathlib\nimport random\nimport string\nimport re\nimport sys\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\nimport sklearn\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import TweetTokenizer \nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom scipy.stats import rankdata\nimport json","43a43ce4":"class Tokenizer:\n    \n    def __init__(self, vocab_size = None, oov_token = None, bos_token = None, eos_token = None, max_length = 10000):\n        self.vocab_size = vocab_size\n        self.oov_token = oov_token\n        self.max_length = max_length\n        self.bos_token = bos_token\n        self.eos_token = eos_token\n        \n    stopwords = set([\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ])\n    \n    tweet_tokenizer = TweetTokenizer() \n    \n    stemmer = PorterStemmer()\n    \n    lemmatizer = WordNetLemmatizer()\n    \n    @staticmethod\n    def preprocess_string(text):\n        # Convert sentences to lowercase.\n        text = text.lower()\n        # Remove puntuations, but ? and ! are usually enmotional so I won't remove it.\n        text = re.sub(r'[\\n| |.|\\\"|,|:|\\(|\\)|#|\\{|\\}|\\*|\\\/|\\$|\\\u2014|~|;|=|\\[\uff5c\\]|\\-]+', \" \", text)\n        # Remove Digits\n        text = re.sub(\"[0-9]+\", \" \", text)\n        text = re.sub(\"[ ]+\", \" \", text)\n        text = text.strip(\" \")\n        # Convert sentences to tokens\n        items = Tokenizer.tweet_tokenizer.tokenize(text)\n        # Remove stop words\n        new_items = []\n        for item in items:\n            if item not in Tokenizer.stopwords:\n                new_item = Tokenizer.lemmatizer.lemmatize(item)\n                new_item = Tokenizer.stemmer.stem(new_item)\n                new_items.append(new_item)\n        return new_items\n        \n    def fit_transform(self, texts):\n        current_index = 1\n        word_index = {self.oov_token: current_index}\n        if self.bos_token != None:\n            current_index += 1\n            word_index[self.bos_token] = current_index\n        if self.eos_token != None:\n            current_index += 1\n            word_index[self.eos_token] = current_index\n\n        word_count = {}\n        for i in range(len(texts)):\n            text = texts[i]\n            for item in text:\n                if item in word_count:\n                    word_count[item] += 1\n                else:\n                    word_count[item] = 1\n        word_count_df = pd.DataFrame({\"key\": word_count.keys(), \"count\": word_count.values()})\n        word_count_df.sort_values(by=\"count\", ascending=False, inplace=True)\n        self.word_count_df = word_count_df\n        vocab = list(word_index.keys())\n        vocab += list(word_count_df[\"key\"][0: self.vocab_size - len(word_index)])\n        vocab = set(vocab)\n        self.vocab = vocab\n        \n        sentences = []\n        offset = 1 if self.eos_token != None else 0\n        for i in range(len(texts)):\n            text = texts[i]\n            sentence = []\n            if self.bos_token != None:\n                sentence.append(word_index[self.bos_token])\n            for item in text:\n                if item in self.vocab:\n                    if item in word_index:\n                        sentence.append(word_index[item])\n                    else:\n                        current_index += 1\n                        word_index[item] = current_index\n                        sentence.append(word_index[item])\n                else:\n                    sentence.append(word_index[self.oov_token])\n            if len(sentence) <= self.max_length - offset:\n                if self.eos_token != None:\n                    sentence.append(word_index[self.eos_token])\n                sentence += [0] * (self.max_length - len(sentence))\n            elif len(sentence) > self.max_length - offset:\n                sentence = sentence[:self.max_length - offset]\n                if self.eos_token != None:\n                    sentence.append(word_index[self.eos_token])\n            sentences.append(sentence)\n        self.word_index = word_index\n        self.index_word = dict({word_index[key]: key for key in word_index.keys()})\n        return sentences\n    \n    def save(self, path):\n        dic = {\n            \"vocab_size\": self.vocab_size,\n            \"oov_token\": self.oov_token,\n            \"max_length\":  self.max_length,\n            \"vocab\": list(self.vocab),\n            \"index_word\": self.index_word,\n            \"word_index\": self.word_index\n        }\n        if self.bos_token is not None:\n            dic[\"bos_token\"] = self.bos_token\n        if self.eos_token is not None:\n            dic[\"eos_token\"] = self.eos_token\n        res = json.dumps(dic)\n        with open(path, \"w+\") as f:\n            f.write(res)\n            \n    def load(self, path):\n        with open(path, \"r\") as f:\n            dic = json.load(f)\n        self.vocab_size = dic[\"vocab_size\"]\n        self.oov_token = dic[\"oov_token\"]\n        self.max_length = dic[\"max_length\"]\n        self.vocab = set(dic[\"vocab\"])\n        self.index_word = dic[\"index_word\"]\n        self.word_index = dic[\"word_index\"]\n        if \"bos_token\" in dic:\n            self.bos_token = dic[\"bos_token\"]\n        if \"eos_token\" in dic:\n            self.eos_token = dic[\"eos_token\"]\n            \n    def transform(self, texts):\n        sentences = []\n        offset = 1 if self.eos_token != None else 0\n        for i in range(len(texts)):\n            text = texts[i]\n            sentence = []\n            if self.bos_token != None:\n                sentence.append(self.word_index[self.bos_token])\n            for item in text:\n                if item in self.vocab:\n                    sentence.append(self.word_index[item])\n                else:\n                    sentence.append(self.word_index[self.oov_token])\n            if len(sentence) == self.max_length - offset:\n                if self.eos_token != None:\n                    sentence.append(self.word_index[self.eos_token])\n            elif len(sentence) < self.max_length - offset:\n                if self.eos_token != None:\n                    sentence.append(self.word_index[self.eos_token])\n                sentence += [0] * (self.max_length - len(sentence))\n            elif len(sentence) > self.max_length - offset:\n                sentence = sentence[:self.max_length - offset]\n                if self.eos_token != None:\n                    sentence.append(self.word_index[self.eos_token])\n            sentences.append(sentence)\n        return sentences\n            ","1397436b":"validation_data = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\nvalidation_data.head()","b7dcd97c":"train = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\ntrain.head()","a5633feb":"stemmer = PorterStemmer()\nprint(stemmer.stem(\"going\"))\nprint(stemmer.stem(\"dogs\"))\nprint(stemmer.stem(\"leaves\"))","dfa76eb2":"lemmatizer = WordNetLemmatizer()\nprint(lemmatizer.lemmatize(\"going\"))\nprint(lemmatizer.lemmatize(\"dogs\"))\nprint(lemmatizer.lemmatize(\"leaves\"))\nprint(stemmer.stem(\"leaf\"))","033f155c":"sys.setrecursionlimit(100000)\nimport time\nbegin = time.time()\nuse_external_dataset = True\nif use_external_dataset:\n    train = train[[\"comment_text\", \"toxic\"]]\n    train.columns = [\"text\", \"label\"]\n    # Add More toxic data to mitigate class imbalance problem\n    train = train.append(pd.DataFrame({\"text\": validation_data[\"more_toxic\"], \"label\": [1] * len(validation_data)}))\nelse:\n    data = pd.DataFrame({\"text\": validation_data[\"less_toxic\"], \"label\": [0] * len(validation_data)})\n    data = data.append(pd.DataFrame({\"text\": validation_data[\"more_toxic\"], \"label\": [1] * len(validation_data)}))\n    text = data[\"text\"].unique()\n    grouped = data.groupby(\"text\")\n    label = list(grouped.mean()[\"label\"])\n    text_label_dict = dict({key: value for key, value in zip(text, label)})\n    index_label = sorted(grouped.mean()[\"label\"].unique())\n    data[\"average_value\"] = data[\"text\"].apply(lambda text: text_label_dict[text])\n    data[\"class\"] = data[\"average_value\"].apply(lambda value: index_label.index(value))\n    classes = sorted(data[\"class\"].unique())\n    print(\"Classes:\", classes)\n    train = data[[\"text\", \"label\"]]\ntokens = []\nlast_index = len(train) - 1\nfor i in range(len(train)):\n    tokens.append(Tokenizer.preprocess_string(train.iloc[i][\"text\"]))\n    if (i + 1) % 10000 == 0 or i == last_index:\n        current = time.time() - begin\n        print(\"%.2fs-%.2fs: %.2f%%\" % (current, current * len(train) \/ i, i \/ len(train) * 100))\ntrain[\"token\"] = tokens\ntrain[\"token_length\"] = train[\"token\"].apply(len)\ntrain = sklearn.utils.shuffle(train)","276fccd2":"train[[\"token_length\"]].describe()","9c0b2ca2":"train[\"token_length\"][train[\"token_length\"] <= 100].hist()","39c69174":"train[\"label\"].hist()","74a047ff":"tokenizer = Tokenizer(\n    vocab_size=config.vocab_size, \n    oov_token=config.oov_token, \n    bos_token=config.bos_token,\n    eos_token=config.eos_token,\n    max_length=config.sequence_length\n)\nsequences = tokenizer.fit_transform(list(train[\"token\"]))\ntrain[\"sequence\"] = sequences\ntrain.head()","93b4f463":"len(tokenizer.index_word)","2ff5d3a9":"tokenizer.save(\"tokenizer.json\")","b887ae8f":"new_tokenizer = Tokenizer()\nnew_tokenizer.load(\"tokenizer.json\")","bb8d4752":"word_count_seldom_appear = {\"word_count\": [], \"num_words\": []}\nfor i in range(1, 10):\n    word_count_seldom_appear[\"word_count\"].append(i)\n    word_count_seldom_appear[\"num_words\"].append(len(tokenizer.word_count_df[tokenizer.word_count_df[\"count\"] <= i]))\nsns.barplot(x=\"word_count\", y=\"num_words\", data=pd.DataFrame(word_count_seldom_appear))","49a8242d":"X_train, X_val, y_train, y_val = train_test_split(train[\"sequence\"], train[\"label\"], test_size=config.validation_split)","43c9f19f":"X_train.shape, y_train.shape, X_val.shape, y_val.shape","4a73dbc0":"def make_dataset(X, y, batch_size, mode):\n    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n    if mode == \"train\":\n       dataset = dataset.shuffle(256) \n    dataset = dataset.batch(batch_size)\n    dataset = dataset.cache().prefetch(16).repeat(1)\n    return dataset","2c7fe504":"train_ds = make_dataset(list(X_train), list(y_train), batch_size=config.batch_size, mode=\"train\")\nvalid_ds = make_dataset(list(X_val), list(y_val), batch_size=config.batch_size, mode=\"valid\")","b225aee3":"for batch in train_ds.take(1):\n    print(batch)","71242a75":"class_weight =  dict(len(train) \/ train[\"label\"].value_counts())\nclass_weight","297f8f76":"class FNetEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, dropout_rate=0.1, **kwargs):\n        super(FNetEncoder, self).__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.dense_proj = keras.Sequential(\n            [\n                layers.Dense(dense_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n\n    def call(self, inputs):\n        # Casting the inputs to complex64\n        inp_complex = tf.cast(inputs, tf.complex64)\n        # Projecting the inputs to the frequency domain using FFT2D and\n        # extracting the real part of the output\n        fft = tf.math.real(tf.signal.fft2d(inp_complex))\n        proj_input = self.layernorm_1(inputs + fft)\n        proj_output = self.dense_proj(proj_input)\n       \n        layer_norm = self.layernorm_2(proj_input + proj_output)\n        output = self.dropout(layer_norm)\n        return output","edd2649f":"class PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n        super(PositionalEmbedding, self).__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(\n            input_dim=vocab_size, output_dim=embed_dim\n        )\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=embed_dim\n        )\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)\n","aed874c0":"def get_fnet_classifier(config):\n    inputs = keras.Input(shape=(config.sequence_length), dtype=\"int64\", name=\"encoder_inputs\")\n    x = PositionalEmbedding(config.sequence_length, config.vocab_size, config.embed_dim)(inputs)\n    x = FNetEncoder(config.embed_dim, config.latent_dim)(x)\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dropout(0.3)(x)\n    for i in range(3):\n        x = layers.Dense(100, activation=\"relu\")(x)\n        x = layers.Dropout(0.3)(x)\n    output = layers.Dense(1, activation=\"sigmoid\")(x)\n    fnet = keras.Model(inputs, output, name=\"fnet\")\n    return fnet","bba9bb9b":"fnet = get_fnet_classifier(config)","31148f67":"fnet.summary()","cbb93116":"keras.utils.plot_model(fnet, show_shapes=True)","37ddf48b":"fnet.compile(\n    \"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\", tf.keras.metrics.AUC()]\n)","ef6f94d1":"checkpoint = keras.callbacks.ModelCheckpoint(config.model_path, monitor=\"val_accuracy\",save_weights_only=True, save_best_only=True)\nearly_stopping = keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=10)\nreduce_lr = keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", patience=5, min_delta=1e-4, min_lr=1e-6)\nfnet.fit(train_ds, epochs=config.epochs, validation_data=valid_ds, callbacks=[checkpoint, reduce_lr], class_weight=class_weight)\nfnet.save_weights(\"model_latest.h5\")","ed8b05ff":"fnet.load_weights(config.model_path)","d0bc3e46":"from sklearn.metrics import classification_report\ny_pred = np.array(fnet.predict(valid_ds) > 0.5, dtype=int)\ncls_report = classification_report(y_val, y_pred)\nprint(cls_report)","0b0df489":"test = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-severity-rating\/sample_submission.csv\")\ntest[\"text_preprocessed\"] = test[\"text\"].apply(Tokenizer.preprocess_string)\ntest_sequences = tokenizer.transform(list(test[\"text_preprocessed\"]))\nprint(test_sequences[0])\ntest_ds = tf.data.Dataset.from_tensor_slices((test_sequences)).batch(config.batch_size).prefetch(1)\nscore = fnet.predict(test_ds).reshape(-1)\nsample_submission[\"score\"] = rankdata(score, method='ordinal')\nsample_submission.to_csv(\"submission.csv\", index=False)\nsample_submission.head()","a9a30e72":"<a id=\"7.3\"><\/a>\n### 7.3 FNet Classification Model","28449b17":"<a id=\"6.6\"><\/a>\n### 6.6 Train Validation Split","19b0b2c5":"<a id=\"7.2\"><\/a>\n### 7.2 Positional Embedding","1f7dd9bc":"<a id=\"7.1\"><\/a>\n### 7.1 FNet Encoder","1afb3ff2":"# Jigsaw Toxicity Training with FNet\n## Table of Contents\n* [1. Overview](#1.)\n* [2. Configuration](#2.)\n* [3. Setup](#3.)\n* [4. Tools](#4.)\n* [5. Import datasets](#5.)\n* [6. EDA & Preprocessing](#6.)\n    * [6.1 Learn about Stemming](#6.1)\n    * [6.2 Learn about Lemmatisation](#6.2)\n    * [6.3 Select trainng data](#6.3)\n    * [6.4 Statistic info of Token length](#6.4)\n    * [6.5 Build a Tokenizer](#6.5)\n    * [6.6 Train Validation Split](#6.6)\n    * [6.7 Create TensorFlow Dataset](#6.7)\n    * [6.8 Calculate Class weight](#6.8)\n* [7. Model Development](#7.)\n    * [7.1 FNet Encoder](#7.1)\n    * [7.2 Positional Embedding](#7.2)\n    * [7.3 FNet Classification Model](#7.3)\n    * [7.4 Model Training](#7.4)\n* [8. Submission](#8.)\n* [9. References](#9.)","7b0673f9":"Let's see what this data look like.","c86f882c":"\n<a id=\"7.4\"><\/a>\n### 7.4 Model Training","74288e80":"### Classification Report","4828ce4f":"<a id=\"6.4\"><\/a>\n### 6.4 Statistic info of Token length\nAverage Token length is 39. Most are under 100, so choosing 100 as sequence length is enough.","49ef8509":"One of the way is to label `less_toxic` as 0 and `more_toxic` as 1, and FNet can get 0.749 score. I tried grouping the duplicated comment together and replace the label with average value, but got a worse 0.49 score instead. I also tried to convert the average value to a class value, but still can't learn any important information from it. So I am going to keep every variable we may use in the future to a data table.\n\n\nAnother way is to use external dataset from [Toxic Comment Classification Challenge](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge). Since there is a class imbalance problem, I also add more_toxic data from this dataset and label it as 1.","dc3ec671":"<font color=\"red\" size=\"3\">If you found it useful and would like to back me up, just upvote.<\/font>","f100cda1":"<a id=\"6.1\"><\/a>\n#### 6.1 Learn about Stemming\n\nStemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma.","3845f51e":"<a id=\"8.\"><\/a>\n## 8. Submission","8dc55d1a":"Save the Tokenzier:","e05808d5":"Load the Tokenizer:","4f19f079":"<a id=\"7.5\"><\/a>\n### 7.5 Evaluation","fda462c0":"<a id=\"6.8\"><\/a>\n### 6.8  Calculate Class weight","06ce7ba8":"<a id=\"7.\"><\/a>\n## 7. Model Development","3f754a6c":"<a id=\"6.7\"><\/a>\n### 6.7 Create TensorFlow Dataset","b382a1f0":"<a id=\"1.\"><\/a>\n## 1. Overview\nIn this Notebook, I will develop a Jigsaw Toxicity Prediction Model using FNet from scratch.\nThe FNet Model was able to achieve 92-97% of BERT's accuracy while training 80% faster on GPUs and almost 70% faster on TPUs. So that we use use it to do quick experiment.\n\nI build this sample referring to [Text Generation using FNet](https:\/\/keras.io\/examples\/nlp\/text_generation_fnet\/), ranking of toxicity can be calcualated via probability of binary classficiation.\n\nI use dataset from [Toxic Comment Classification Challenge](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge) and combine with more toxic data in this dataset for training.\n\nApart from building Model using FNet, I also try to build a custom Tokenizer to vectorize texts.\n\nCurrently this notebook can get a 0.758 LB, not a very good score. Using a pretrained Model and better text preprocessing method could improve the LB score.","2d4d9112":"<a id=\"6.5\"><\/a>\n### 6.5 Build a Tokenizer","b0cbf74b":"<a id=\"6.3\"><\/a>\n### 6.3 Select Traning Data","0e6f8ede":"Let's visualize the Model.","da8b1a98":"<a id=\"5.\"><\/a>\n## 5. Import datasets","a4717189":"\n<a id=\"9.\"><\/a>\n## 9. References\n- [FNet: Mixing Tokens with Fourier Transforms](https:\/\/arxiv.org\/abs\/2105.03824v3)\n- [Attention Is All You Need](https:\/\/arxiv.org\/abs\/1706.03762v5)\n- [Text Generation using FNet](https:\/\/keras.io\/examples\/nlp\/text_generation_fnet\/)\n- [English-Spanish Translation: FNet](https:\/\/www.kaggle.com\/lonnieqin\/english-spanish-translation-fnet)","2d0223d3":"<a id=\"6.\"><\/a>\n### 6. EDA & Preprocessing","d298ebe4":"<a id=\"6.2\"><\/a>\n### 6.2 Learn about Lemmatisation\n\nLemmatisation in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.","915afa82":"Number of words:","b2062c9f":"<a id=\"3.\"><\/a>\n## 3. Setup","180d77cf":"### Tokenizer class\nThis class can help you build a vocabulary by fitting a sequence of text. It's similar to Tokenizer in TensorFlow, it can also support padding sequences and adding Begin-of-Sentence token and End-of-Sentence token at the same time. I build this class to have fun and it's more flexible to custimize in the future. It accepts 5 parameters: vocaulbary size, out of word token, Begin-of-Sentence token (can be null), End-of-Sentence token (can be null), max sequence length.\n\n`fit_transform` can build a vocuabury from a list of tokens like:\n```python\n[\n    [\"1\", \"2\", \"3\", \"4\", \"5\"],\n    [\"1\", \"2\", \"3\", \"4\", \"5\"]\n]\n```\nand return vectors like\n```python\n[\n    [1, 2, 3, 4, 5],\n    [1, 2, 3, 4, 5]\n]\n```\n\n`transform` method is similar to `fit_transform` without building Vocabulary.\n","91cf0c32":"<a id=\"4.\"><\/a>\n## 4. Tools","b9e1346d":"<a id=\"2.\"><\/a>\n## 2. Configuration","ff6e4d21":"Number of Words that seldom appear:"}}