{"cell_type":{"c01c56b1":"code","f4648447":"code","e4641487":"code","73a7a155":"code","d1f8e2b6":"code","a14d3741":"code","38447e3c":"code","bf93158d":"code","2780af51":"code","49fcee3a":"code","619cc828":"markdown","dd6a837f":"markdown","fc7e91ab":"markdown","52548633":"markdown","73a2d434":"markdown","9376ae57":"markdown","379e33cc":"markdown","3f7b3878":"markdown","2ea3d122":"markdown","c3b4a266":"markdown"},"source":{"c01c56b1":"import numpy as np\nimport pandas as pd\nimport os, math, sys\nimport glob, itertools\nimport argparse, random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision.models import vgg19\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.utils import save_image, make_grid\n\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.model_selection import train_test_split\n\nrandom.seed(42)\nimport warnings\nwarnings.filterwarnings(\"ignore\")","f4648447":"# load pretrained models\nload_pretrained_models = True\n# number of epochs of training\nn_epochs = 15\n# size of the batches\nbatch_size = 16\n# name of the dataset\ndataset_name = \"..\/input\/celeba-dataset\/img_align_celeba\/img_align_celeba\"\n# adam: learning rate\nlr = 0.00008\n# adam: decay of first order momentum of gradient\nb1 = 0.5\n# adam: decay of first order momentum of gradient\nb2 = 0.999\n# number of cpu threads to use during batch generation\nn_cpu = 4\n# dimensionality of the latent space\nlatent_dim = 100\n# size of each image dimension\nimg_size = 128\n# size of random mask\nmask_size = 64\n# number of image channels\nchannels = 3\n# interval between image sampling\nsample_interval = 500\n\ncuda = True if torch.cuda.is_available() else False\nos.makedirs(\"images\", exist_ok=True)\nos.makedirs(\"saved_models\", exist_ok=True)\n\n# Calculate output dims of image discriminator (PatchGAN)\npatch_h, patch_w = int(mask_size \/ 2 ** 3), int(mask_size \/ 2 ** 3)\npatch = (1, patch_h, patch_w)","e4641487":"class ImageDataset(Dataset):\n    def __init__(self, root, transforms_=None, img_size=128, mask_size=64, mode=\"train\"):\n        self.transform = transforms.Compose(transforms_)\n        self.img_size = img_size\n        self.mask_size = mask_size\n        self.mode = mode\n        self.files = sorted(glob.glob(\"%s\/*.jpg\" % root))\n        self.files = self.files[:-4000] if mode == \"train\" else self.files[-4000:]\n\n    def apply_random_mask(self, img):\n        \"\"\"Randomly masks image\"\"\"\n        y1, x1 = np.random.randint(0, self.img_size - self.mask_size, 2)\n        y2, x2 = y1 + self.mask_size, x1 + self.mask_size\n        masked_part = img[:, y1:y2, x1:x2]\n        masked_img = img.clone()\n        masked_img[:, y1:y2, x1:x2] = 1\n\n        return masked_img, masked_part\n\n    def apply_center_mask(self, img):\n        \"\"\"Mask center part of image\"\"\"\n        # Get upper-left pixel coordinate\n        i = (self.img_size - self.mask_size) \/\/ 2\n        masked_img = img.clone()\n        masked_img[:, i : i + self.mask_size, i : i + self.mask_size] = 1\n\n        return masked_img, i\n\n    def __getitem__(self, index):\n\n        img = Image.open(self.files[index % len(self.files)])\n        img = self.transform(img)\n        if self.mode == \"train\":\n            # For training data perform random mask\n            masked_img, aux = self.apply_random_mask(img)\n        else:\n            # For test data mask the center of the image\n            masked_img, aux = self.apply_center_mask(img)\n\n        return img, masked_img, aux\n\n    def __len__(self):\n        return len(self.files)","73a7a155":"transforms_ = [\n    transforms.Resize((img_size, img_size), Image.BICUBIC),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n]\ndataloader = DataLoader(\n    ImageDataset(dataset_name, transforms_=transforms_),\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=n_cpu,\n)\ntest_dataloader = DataLoader(\n    ImageDataset(dataset_name, transforms_=transforms_, mode=\"val\"),\n    batch_size=12,\n    shuffle=True,\n    num_workers=1,\n)","d1f8e2b6":"class Generator(nn.Module):\n    def __init__(self, channels=3):\n        super(Generator, self).__init__()\n\n        def downsample(in_feat, out_feat, normalize=True):\n            layers = [nn.Conv2d(in_feat, out_feat, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2))\n            return layers\n\n        def upsample(in_feat, out_feat, normalize=True):\n            layers = [nn.ConvTranspose2d(in_feat, out_feat, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n            layers.append(nn.ReLU())\n            return layers\n\n        self.model = nn.Sequential(\n            *downsample(channels, 64, normalize=False),\n            *downsample(64, 64),\n            *downsample(64, 128),\n            *downsample(128, 256),\n            *downsample(256, 512),\n            nn.Conv2d(512, 4000, 1),\n            *upsample(4000, 512),\n            *upsample(512, 256),\n            *upsample(256, 128),\n            *upsample(128, 64),\n            nn.Conv2d(64, channels, 3, 1, 1),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, channels=3):\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, stride, normalize):\n            \"\"\"Returns layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 3, stride, 1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        layers = []\n        in_filters = channels\n        for out_filters, stride, normalize in [(64, 2, False), (128, 2, True), (256, 2, True), (512, 1, True)]:\n            layers.extend(discriminator_block(in_filters, out_filters, stride, normalize))\n            in_filters = out_filters\n\n        layers.append(nn.Conv2d(out_filters, 1, 3, 1, 1))\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, img):\n        return self.model(img)","a14d3741":"def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n\ndef save_sample(batches_done):\n    samples, masked_samples, i = next(iter(test_dataloader))\n    samples = Variable(samples.type(Tensor))\n    masked_samples = Variable(masked_samples.type(Tensor))\n    i = i[0].item()  # Upper-left coordinate of mask\n    # Generate inpainted image\n    gen_mask = generator(masked_samples)\n    filled_samples = masked_samples.clone()\n    filled_samples[:, :, i : i + mask_size, i : i + mask_size] = gen_mask\n    # Save sample\n    sample = torch.cat((masked_samples.data, filled_samples.data, samples.data), -2)\n    save_image(sample, \"images\/%d.png\" % batches_done, nrow=6, normalize=True)\n\n    \n# Loss function\nadversarial_loss = torch.nn.MSELoss()\npixelwise_loss = torch.nn.L1Loss()\n\n# Initialize generator and discriminator\ngenerator = Generator(channels=channels)\ndiscriminator = Discriminator(channels=channels)\n\n# Load pretrained models\nif load_pretrained_models:\n    generator.load_state_dict(torch.load(\"..\/input\/context-encoder-gan-for-image-inpainting-pytorch\/saved_models\/generator.pth\"))\n    discriminator.load_state_dict(torch.load(\"..\/input\/context-encoder-gan-for-image-inpainting-pytorch\/saved_models\/discriminator.pth\"))\n    print(\"Using pre-trained Context-Encoder GAN model!\")\n\nif cuda:\n    generator.cuda()\n    discriminator.cuda()\n    adversarial_loss.cuda()\n    pixelwise_loss.cuda()\n\n# Initialize weights\ngenerator.apply(weights_init_normal)\ndiscriminator.apply(weights_init_normal)\n\n# Optimizers\noptimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n\nTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor","38447e3c":"gen_adv_losses, gen_pixel_losses, disc_losses, counter = [], [], [], []\n\nfor epoch in range(n_epochs):\n    \n    ### Training ###\n    gen_adv_loss, gen_pixel_loss, disc_loss = 0, 0, 0\n    tqdm_bar = tqdm(dataloader, desc=f'Training Epoch {epoch} ', total=int(len(dataloader)))\n    for i, (imgs, masked_imgs, masked_parts) in enumerate(tqdm_bar):\n\n        # Adversarial ground truths\n        valid = Variable(Tensor(imgs.shape[0], *patch).fill_(1.0), requires_grad=False)\n        fake = Variable(Tensor(imgs.shape[0], *patch).fill_(0.0), requires_grad=False)\n\n        # Configure input\n        imgs = Variable(imgs.type(Tensor))\n        masked_imgs = Variable(masked_imgs.type(Tensor))\n        masked_parts = Variable(masked_parts.type(Tensor))\n\n        ## Train Generator ##\n        optimizer_G.zero_grad()\n\n        # Generate a batch of images\n        gen_parts = generator(masked_imgs)\n\n        # Adversarial and pixelwise loss\n        g_adv = adversarial_loss(discriminator(gen_parts), valid)\n        g_pixel = pixelwise_loss(gen_parts, masked_parts)\n        # Total loss\n        g_loss = 0.001 * g_adv + 0.999 * g_pixel\n\n        g_loss.backward()\n        optimizer_G.step()\n\n        ## Train Discriminator ##\n        optimizer_D.zero_grad()\n\n        # Measure discriminator's ability to classify real from generated samples\n        real_loss = adversarial_loss(discriminator(masked_parts), valid)\n        fake_loss = adversarial_loss(discriminator(gen_parts.detach()), fake)\n        d_loss = 0.5 * (real_loss + fake_loss)\n\n        d_loss.backward()\n        optimizer_D.step()\n        \n        gen_adv_loss, gen_pixel_loss, disc_loss\n        gen_adv_losses, gen_pixel_losses, disc_losses, counter\n        \n        gen_adv_loss += g_adv.item()\n        gen_pixel_loss += g_pixel.item()\n        gen_adv_losses.append(g_adv.item())\n        gen_pixel_losses.append(g_pixel.item())\n        disc_loss += d_loss.item()\n        disc_losses.append(d_loss.item())\n        counter.append(i*batch_size + imgs.size(0) + epoch*len(dataloader.dataset))\n        tqdm_bar.set_postfix(gen_adv_loss=gen_adv_loss\/(i+1), gen_pixel_loss=gen_pixel_loss\/(i+1), disc_loss=disc_loss\/(i+1))\n        \n        # Generate sample at sample interval\n        batches_done = epoch * len(dataloader) + i\n        if batches_done % sample_interval == 0:\n            save_sample(batches_done)\n            \n    torch.save(generator.state_dict(), \"saved_models\/generator.pth\")\n    torch.save(discriminator.state_dict(), \"saved_models\/discriminator.pth\")","bf93158d":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=counter, y=gen_adv_losses, mode='lines', name='Gen Adv Loss'))\n\nfig.update_layout(\n    width=1000,\n    height=500,\n    title=\"Generator Adversarial Loss\",\n    xaxis_title=\"Number of training examples seen\",\n    yaxis_title=\"Gen Adversarial Loss (MSELoss)\"),\nfig.show()","2780af51":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=counter, y=gen_pixel_losses, mode='lines', name='Gen Pixel Loss', marker_color='orange'))\n\nfig.update_layout(\n    width=1000,\n    height=500,\n    title=\"Generator Pixel Loss\",\n    xaxis_title=\"Number of training examples seen\",\n    yaxis_title=\"Gen Pixel Loss (L1 Loss)\"),\nfig.show()","49fcee3a":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=counter, y=disc_losses, mode='lines', name='Discriminator Loss', marker_color='seagreen'))\n\nfig.update_layout(\n    width=1000,\n    height=500,\n    title=\"Discriminator Loss\",\n    xaxis_title=\"Number of training examples seen\",\n    yaxis_title=\"Disc Loss (MSELoss)\"),\nfig.show()","619cc828":"### Settings \u2699\ufe0f","dd6a837f":"<h3><center>Model Architecture<\/center><\/h3>\n<img src=\"https:\/\/miro.medium.com\/max\/700\/1*fJpamgw0yBZZRNEuex07hw.png\" width=\"1000\" height=\"1000\"\/>\n<h4><\/h4>\n<h4><center>Image Source:  <a href=\"https:\/\/arxiv.org\/abs\/1609.04802\">Context Encoders: Feature Learning by Inpainting [Deepak Pathak et al.]<\/a><\/center><\/h4>","fc7e91ab":"### Define Model Classes","52548633":"### Libraries \ud83d\udcda\u2b07","73a2d434":"### Train Context-Encoder GAN","9376ae57":"### Work in Progress ...","379e33cc":"### Get Train\/Test Dataloaders","3f7b3878":"<h4><center>Semantic Inpainting results on held-out images by Context Encoder.<\/center><\/h4>\n<img src=\"http:\/\/people.eecs.berkeley.edu\/~pathak\/context_encoder\/resources\/result_fig.jpg\" width=\"900\" height=\"900\"\/>\n<h4><\/h4>\n<h4><center>Image Source:  <a href=\"https:\/\/arxiv.org\/abs\/1609.04802\">Context Encoders: Feature Learning by Inpainting [Deepak Pathak et al.]<\/a><\/center><\/h4>","2ea3d122":"### Define Dataset Class","c3b4a266":"## Introduction\n\n### In this notebook we use [Context-Conditional GAN (CCGAN)](https:\/\/arxiv.org\/abs\/1604.07379) to perform Image Inpainting on [CelebA dataset](http:\/\/mmlab.ie.cuhk.edu.hk\/projects\/CelebA.html)."}}