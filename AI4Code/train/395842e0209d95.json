{"cell_type":{"e755da7c":"code","2a46b720":"code","40824186":"code","84752ecf":"code","b21d0f51":"code","e0647f4e":"code","31dfb4aa":"code","68d259a7":"code","57508add":"code","76ccbb7b":"code","5b9ed5d4":"code","bae72b1f":"code","30e15884":"code","dd6d046d":"code","9035e816":"code","32e89ffb":"code","5802f67a":"code","99c69685":"code","79550569":"code","19b85b49":"code","337be31f":"code","d3443efa":"code","192ce380":"code","162a800a":"code","62a641ea":"code","af2a96f4":"code","6d840110":"code","5c9eb4f1":"code","773574a5":"code","1fb31d16":"code","53fbcee0":"code","e1cc3a83":"code","e59d07f3":"code","3869c336":"code","12866949":"code","53df1073":"code","6d1c763d":"code","a5fc484a":"code","b3b3c5d7":"code","3b58d0d2":"code","bce10260":"code","97724276":"code","40b68f55":"code","a7d7f1d1":"code","64adf905":"code","180b7bda":"code","685a7307":"code","a6643223":"code","73549bb1":"code","e7c23ee6":"code","b077b2f0":"code","a5382bea":"code","00ead03d":"code","0bdf7c96":"code","ddef44fe":"markdown","d126f4ae":"markdown","7614c093":"markdown","a26e7bf7":"markdown","1191dbb4":"markdown","46aa8d80":"markdown","01fa12c9":"markdown"},"source":{"e755da7c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2a46b720":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os \nfrom sklearn.preprocessing import MinMaxScaler","40824186":"df = pd.read_csv(\"..\/input\/prices.csv\")","84752ecf":"df['date'] = pd.to_datetime(df['date'])\ndf.info()","b21d0f51":"df.head(20)","e0647f4e":" df_pivot = df.pivot('date','symbol','close').reset_index()\n df_pivot.head()","31dfb4aa":"# set the index\ndf_pivot.set_index('date', inplace=True)","68d259a7":" df_pivot.head()","57508add":"df_pivot.dropna(axis=1, how='any', inplace=True)","76ccbb7b":" df_pivot.head()","5b9ed5d4":"df_pivot.shape","bae72b1f":"SEQ_LEN = 60  # how long of a preceeding sequence to collect for RNN\nFUTURE_PERIOD_PREDICT = 30  # how far into the future are we trying to predict?\nSTOCK_TO_PREDICT = 'GE'","30e15884":"def classify(current, future):\n    if float(future) > float(current):\n        return 1\n    else:\n        return 0","dd6d046d":"df_pivot['future'] = df_pivot[STOCK_TO_PREDICT].shift(-FUTURE_PERIOD_PREDICT)","9035e816":"df_pivot['target'] = list(map(classify, df_pivot[STOCK_TO_PREDICT], df_pivot['future']))","32e89ffb":"times = sorted(df_pivot.index.values)  # get the times\nlast_10pct = sorted(df_pivot.index.values)[-int(0.1*len(times))]  # get the last 10% of the times\nlast_20pct = sorted(df_pivot.index.values)[-int(0.2*len(times))]  # get the last 20% of the times\n\ntest_df = df_pivot[(df_pivot.index >= last_10pct)]\nvalidation_df = df_pivot[(df_pivot.index >= last_20pct) & (df_pivot.index < last_10pct)]  \ntrain_df = df_pivot[(df_pivot.index < last_20pct)]  # now the train_df is all the data up to the last 20%","5802f67a":"from collections import deque\nimport numpy as np\nimport random","99c69685":"from sklearn import preprocessing  \n\ndef preprocess_df(df):\n    df = df.drop(columns=[\"future\"])  # don't need this anymore.\n\n    \n    df.dropna(inplace=True)  # remove the nas created by pct_change\n    df[STOCK_TO_PREDICT] = preprocessing.scale(df[STOCK_TO_PREDICT].values)  # scale between 0 and 1.\n\n    df.dropna(inplace=True)  # cleanup again... jic.\n\n\n    sequential_data = []  # this is a list that will CONTAIN the sequences\n    prev_days = deque(maxlen=SEQ_LEN)  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in\n\n    for i in df.values:  # iterate over the values\n        prev_days.append([n for n in i[:-1]])  # store all but the target\n        if len(prev_days) == SEQ_LEN:  # make sure we have 60 sequences!\n            sequential_data.append([np.array(prev_days), i[-1]])  # append those bad boys!\n\n    random.shuffle(sequential_data)  # shuffle for good measure.\n\n    buys = []  # list that will store our buy sequences and targets\n    sells = []  # list that will store our sell sequences and targets\n\n    for seq, target in sequential_data:  # iterate over the sequential data\n        if target == 0:  # if it's a \"not buy\"\n            sells.append([seq, target])  # append to sells list\n        elif target == 1:  # otherwise if the target is a 1...\n            buys.append([seq, target])  # it's a buy!\n\n    random.shuffle(buys)  # shuffle the buys\n    random.shuffle(sells)  # shuffle the sells!\n\n    lower = min(len(buys), len(sells))  # what's the shorter length?\n\n    buys = buys[:lower]  # make sure both lists are only up to the shortest length.\n    sells = sells[:lower]  # make sure both lists are only up to the shortest length.\n\n    sequential_data = buys+sells  # add them together\n    random.shuffle(sequential_data)  # another shuffle, so the model doesn't get confused with all 1 class then the other.\n\n    X = []\n    y = []\n\n    for seq, target in sequential_data:  # going over our new sequential data\n        X.append(seq)  # X is the sequences\n        y.append(target)  # y is the targets\/labels (buys vs sell\/notbuy)\n\n    return np.array(X), y  # return X and y...and make X a numpy array!","79550569":"X_train, y_train = preprocess_df(train_df)\nX_val, y_val = preprocess_df(validation_df)\nX_test, y_test = preprocess_df(test_df)","19b85b49":"print(f\"train data: {len(X_train)} validation: {len(X_val)}, test: {len(X_test)}\")\nprint(f\"Train Dont buys: {y_train.count(0)}, buys: {y_train.count(1)}\")\nprint(f\"Validation Dont buys: {y_val.count(0)}, buys: {y_val.count(1)}\")\nprint(f\"Test Dont buys: {y_test.count(0)}, buys: {y_test.count(1)}\")","337be31f":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Flatten\nfrom keras.layers import GRU\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom keras.optimizers import Adam ","d3443efa":"EPOCHS = 100  # how many passes through our data\nBATCH_SIZE = 32  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.\nimport time\n\nNAME = f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\"  # a unique name for the model","192ce380":"lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.1, epsilon=0.0001, patience=3, verbose=1)\ncheckpoint = ModelCheckpoint(NAME, monitor='val_loss', verbose=1, save_best_only=True, mode='min')","162a800a":"baseline = Sequential()\nbaseline.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False, activation='relu'))\nbaseline.add(Dropout(0.2))\nbaseline.add(BatchNormalization())  #normalizes activation outputs, same reason you want to normalize your input data.\n\nbaseline.add(Dense(2, activation='softmax'))","62a641ea":"baseline.compile(loss='sparse_categorical_crossentropy', optimizer=Adam() , metrics = ['accuracy'])","af2a96f4":"baseline.fit(X_train, y_train,\n                    batch_size=BATCH_SIZE,\n                    epochs=EPOCHS,\n                    validation_data=(X_val, y_val),\n                    callbacks = [checkpoint, lr_reduce])","6d840110":"y_prob = baseline.predict(X_test) \npredicted_stock_price_baseline = y_prob.argmax(axis=-1)","5c9eb4f1":"plt.figure(figsize = (18,9))\nplt.plot(y_test, color = 'black', label = 'GE Stock Price')\nplt.plot(predicted_stock_price_baseline, color = 'green', label = 'Predicted GE Mid Price')\nplt.title('GE Close Price Prediction', fontsize=30)\n#plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)\nplt.xlabel('Date')\nplt.ylabel('GE Close Price')\nplt.legend(fontsize=18)\nplt.show()","773574a5":"from sklearn.metrics import accuracy_score","1fb31d16":"accuracy_score(y_test, predicted_stock_price_baseline)","53fbcee0":"filepath=\"LSTM_GRU.hdf5\"","e1cc3a83":"model = Sequential()\nmodel.add(GRU(256 , input_shape = (X_train.shape[1], X_train.shape[2]) , return_sequences=True))\nmodel.add(Dropout(0.4))\nmodel.add(LSTM(256))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(64 ,  activation = 'relu'))\nmodel.add(Dense(2, activation='softmax'))\nmodel.summary()","e59d07f3":"model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam() , metrics = ['accuracy'])","3869c336":"history_lstm = model.fit(X_train, y_train,\n                    batch_size=BATCH_SIZE,\n                    epochs=EPOCHS,\n                    validation_data=(X_val, y_val), \n                    callbacks = [checkpoint , lr_reduce])","12866949":"y_prob = model.predict(X_test) \npredicted_stock_price_gru = y_prob.argmax(axis=-1)","53df1073":"plt.figure(figsize = (18,9))\nplt.plot(y_test, color = 'black', label = 'GE Stock Price')\nplt.plot(predicted_stock_price_gru, color = 'green', label = 'Predicted GE Close Price')\nplt.title('GE Close Price Prediction', fontsize=30)\n#plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)\nplt.xlabel('Date')\nplt.ylabel('GE Close Price')\nplt.legend(fontsize=18)\nplt.show()","6d1c763d":"accuracy_score(y_test, predicted_stock_price_gru)","a5fc484a":"from keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints\nfrom keras.engine.input_layer import Input\nfrom keras import backend as K\nfrom keras.models import Model","b3b3c5d7":"# https:\/\/www.kaggle.com\/qqgeogor\/keras-lstm-attention-glove840b-lb-0-043\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","3b58d0d2":"inp = Input(shape = (X_train.shape[1], X_train.shape[2]))\nx = LSTM(128, return_sequences=True)(inp)\nx = Dropout(0.2)(x)\nx = BatchNormalization()(x)\nx = Attention(SEQ_LEN)(x)\nx = Dense(32, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(2, activation=\"softmax\")(x)\nmodel_lstm_attention = Model(inputs=inp, outputs=x)","bce10260":"model_lstm_attention.compile(loss='sparse_categorical_crossentropy', optimizer=Adam() , metrics = ['accuracy'])\n\nmodel_lstm_attention.summary()\n\n","97724276":"model_lstm_attention.fit(X_train, y_train,\n                    batch_size=BATCH_SIZE,\n                    epochs=EPOCHS,\n                    validation_data=(X_val, y_val), \n                    callbacks = [checkpoint , lr_reduce]\n             )","40b68f55":"y_prob = model_lstm_attention.predict(X_test) \npredicted_stock_price_attention = y_prob.argmax(axis=-1)","a7d7f1d1":"plt.figure(figsize = (18,9))\nplt.plot(y_test, color = 'black', label = 'GE Stock Price')\nplt.plot(predicted_stock_price_attention, color = 'green', label = 'Predicted GE Close Price')\nplt.title('GE Close Price Prediction', fontsize=30)\n#plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)\nplt.xlabel('Date')\nplt.ylabel('GE Close Price')\nplt.legend(fontsize=18)\nplt.show()","64adf905":"accuracy_score(y_test, predicted_stock_price_attention)","180b7bda":"# https:\/\/www.kaggle.com\/shujian\/transformer-with-lstm\n\nimport random, os, sys\nimport numpy as np\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.callbacks import *\nfrom keras.initializers import *\nimport tensorflow as tf\nfrom keras.engine.topology import Layer\n\ntry:\n    from dataloader import TokenList, pad_to_longest\n    # for transformer\nexcept: pass\n\n\n\nembed_size = 60\n\nclass LayerNormalization(Layer):\n    def __init__(self, eps=1e-6, **kwargs):\n        self.eps = eps\n        super(LayerNormalization, self).__init__(**kwargs)\n    def build(self, input_shape):\n        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n                                     initializer=Ones(), trainable=True)\n        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n                                    initializer=Zeros(), trainable=True)\n        super(LayerNormalization, self).build(input_shape)\n    def call(self, x):\n        mean = K.mean(x, axis=-1, keepdims=True)\n        std = K.std(x, axis=-1, keepdims=True)\n        return self.gamma * (x - mean) \/ (std + self.eps) + self.beta\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\nclass ScaledDotProductAttention():\n    def __init__(self, d_model, attn_dropout=0.1):\n        self.temper = np.sqrt(d_model)\n        self.dropout = Dropout(attn_dropout)\n    def __call__(self, q, k, v, mask):\n        attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])\/self.temper)([q, k])\n        if mask is not None:\n            mmask = Lambda(lambda x:(-1e+10)*(1-x))(mask)\n            attn = Add()([attn, mmask])\n        attn = Activation('softmax')(attn)\n        attn = self.dropout(attn)\n        output = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])\n        return output, attn\n\nclass MultiHeadAttention():\n    # mode 0 - big martixes, faster; mode 1 - more clear implementation\n    def __init__(self, n_head, d_model, d_k, d_v, dropout, mode=0, use_norm=True):\n        self.mode = mode\n        self.n_head = n_head\n        self.d_k = d_k\n        self.d_v = d_v\n        self.dropout = dropout\n        if mode == 0:\n            self.qs_layer = Dense(n_head*d_k, use_bias=False)\n            self.ks_layer = Dense(n_head*d_k, use_bias=False)\n            self.vs_layer = Dense(n_head*d_v, use_bias=False)\n        elif mode == 1:\n            self.qs_layers = []\n            self.ks_layers = []\n            self.vs_layers = []\n            for _ in range(n_head):\n                self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n                self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n                self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=False)))\n        self.attention = ScaledDotProductAttention(d_model)\n        self.layer_norm = LayerNormalization() if use_norm else None\n        self.w_o = TimeDistributed(Dense(d_model))\n\n    def __call__(self, q, k, v, mask=None):\n        d_k, d_v = self.d_k, self.d_v\n        n_head = self.n_head\n\n        if self.mode == 0:\n            qs = self.qs_layer(q)  # [batch_size, len_q, n_head*d_k]\n            ks = self.ks_layer(k)\n            vs = self.vs_layer(v)\n\n            def reshape1(x):\n                s = tf.shape(x)   # [batch_size, len_q, n_head * d_k]\n                x = tf.reshape(x, [s[0], s[1], n_head, d_k])\n                x = tf.transpose(x, [2, 0, 1, 3])  \n                x = tf.reshape(x, [-1, s[1], d_k])  # [n_head * batch_size, len_q, d_k]\n                return x\n            qs = Lambda(reshape1)(qs)\n            ks = Lambda(reshape1)(ks)\n            vs = Lambda(reshape1)(vs)\n\n            if mask is not None:\n                mask = Lambda(lambda x:K.repeat_elements(x, n_head, 0))(mask)\n            head, attn = self.attention(qs, ks, vs, mask=mask)  \n                \n            def reshape2(x):\n                s = tf.shape(x)   # [n_head * batch_size, len_v, d_v]\n                x = tf.reshape(x, [n_head, -1, s[1], s[2]]) \n                x = tf.transpose(x, [1, 2, 0, 3])\n                x = tf.reshape(x, [-1, s[1], n_head*d_v])  # [batch_size, len_v, n_head * d_v]\n                return x\n            head = Lambda(reshape2)(head)\n        elif self.mode == 1:\n            heads = []; attns = []\n            for i in range(n_head):\n                qs = self.qs_layers[i](q)   \n                ks = self.ks_layers[i](k) \n                vs = self.vs_layers[i](v) \n                head, attn = self.attention(qs, ks, vs, mask)\n                heads.append(head); attns.append(attn)\n            head = Concatenate()(heads) if n_head > 1 else heads[0]\n            attn = Concatenate()(attns) if n_head > 1 else attns[0]\n\n        outputs = self.w_o(head)\n        outputs = Dropout(self.dropout)(outputs)\n        if not self.layer_norm: return outputs, attn\n        # outputs = Add()([outputs, q]) # sl: fix\n        return self.layer_norm(outputs), attn\n\nclass PositionwiseFeedForward():\n    def __init__(self, d_hid, d_inner_hid, dropout=0.1):\n        self.w_1 = Conv1D(d_inner_hid, 1, activation='relu')\n        self.w_2 = Conv1D(d_hid, 1)\n        self.layer_norm = LayerNormalization()\n        self.dropout = Dropout(dropout)\n    def __call__(self, x):\n        output = self.w_1(x) \n        output = self.w_2(output)\n        output = self.dropout(output)\n        output = Add()([output, x])\n        return self.layer_norm(output)\n\nclass EncoderLayer():\n    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):\n        self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n    def __call__(self, enc_input, mask=None):\n        output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)\n        output = self.pos_ffn_layer(output)\n        return output, slf_attn\n\n\ndef GetPosEncodingMatrix(max_len, d_emb):\n    pos_enc = np.array([\n        [pos \/ np.power(10000, 2 * (j \/\/ 2) \/ d_emb) for j in range(d_emb)] \n        if pos != 0 else np.zeros(d_emb) \n            for pos in range(max_len)\n            ])\n    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2]) # dim 2i\n    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2]) # dim 2i+1\n    return pos_enc\n\ndef GetPadMask(q, k):\n    ones = K.expand_dims(K.ones_like(q, 'float32'), -1)\n    mask = K.cast(K.expand_dims(K.not_equal(k, 0), 1), 'float32')\n    mask = K.batch_dot(ones, mask, axes=[2,1])\n    return mask\n\ndef GetSubMask(s):\n    len_s = tf.shape(s)[1]\n    bs = tf.shape(s)[:1]\n    mask = K.cumsum(tf.eye(len_s, batch_shape=bs), 1)\n    return mask\n\nclass Transformer():\n    def __init__(self, len_limit, embedding_matrix, d_model=embed_size, \\\n              d_inner_hid=512, n_head=10, d_k=64, d_v=64, layers=2, dropout=0.1, \\\n              share_word_emb=False, **kwargs):\n        self.name = 'Transformer'\n        self.len_limit = len_limit\n        self.src_loc_info = False # True # sl: fix later\n        self.d_model = d_model\n        self.decode_model = None\n        d_emb = d_model\n\n        pos_emb = Embedding(len_limit, d_emb, trainable=False, \\\n                            weights=[GetPosEncodingMatrix(len_limit, d_emb)])\n\n        i_word_emb = Embedding(max_features, d_emb, weights=[embedding_matrix]) # Add Kaggle provided embedding here\n\n        self.encoder = Encoder(d_model, d_inner_hid, n_head, d_k, d_v, layers, dropout, \\\n                               word_emb=i_word_emb, pos_emb=pos_emb)\n\n        \n    def get_pos_seq(self, x):\n        mask = K.cast(K.not_equal(x, 0), 'int32')\n        pos = K.cumsum(K.ones_like(x, 'int32'), 1)\n        return pos * mask\n\n    def compile(self, active_layers=999):\n        src_seq_input = Input(shape=(None, ))\n        x = Embedding(max_features, embed_size, weights=[embedding_matrix])(src_seq_input)\n        \n        # LSTM before attention layers\n        x = Bidirectional(LSTM(128, return_sequences=True))(x)\n        x = Bidirectional(LSTM(64, return_sequences=True))(x) \n        \n        x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, d_k=64, d_v=64, dropout=0.1)(x, x, x)\n        \n        avg_pool = GlobalAveragePooling1D()(x)\n        max_pool = GlobalMaxPooling1D()(x)\n        conc = concatenate([avg_pool, max_pool])\n        conc = Dense(64, activation=\"relu\")(conc)\n        x = Dense(2, activation=\"softmax\")(conc)   \n        \n        \n        self.model = Model(inputs=src_seq_input, outputs=x)\n        self.model.compile(optimizer = 'Adamax', loss = 'binary_crossentropy', metrics=['accuracy'])","685a7307":"def build_model():\n    inp = Input(shape = (X_train.shape[1], X_train.shape[2]))\n    \n    # LSTM before attention layers\n    x = Bidirectional(LSTM(128, return_sequences=True))(inp)\n    x = Bidirectional(LSTM(64, return_sequences=True))(x) \n        \n    x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, d_k=64, d_v=64, dropout=0.1)(x, x, x)\n        \n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    conc = concatenate([avg_pool, max_pool])\n    conc = Dense(64, activation=\"sigmoid\")(conc)\n    x = Dense(2, activation=\"softmax\")(conc)      \n\n    model = Model(inputs = inp, outputs = x)\n    model.compile(\n        loss = \"binary_crossentropy\", \n        #optimizer = Adam(lr = config[\"lr\"], decay = config[\"lr_d\"]), \n        optimizer = \"RMSprop\",\n        metrics=['accuracy'])\n    \n    return model","a6643223":"from keras.utils import to_categorical\n\ny_train_cat = to_categorical(y_train, num_classes=None)\ny_val_cat = to_categorical(y_val, num_classes=None)\ny_test_cat = to_categorical(y_test, num_classes=None)","73549bb1":"multi_head = build_model()","e7c23ee6":"multi_head.summary()","b077b2f0":"multi_head.fit(X_train, y_train_cat,\n                    epochs=EPOCHS,\n                    validation_data=(X_val, y_val_cat), \n                    callbacks = [checkpoint , lr_reduce]\n             )","a5382bea":"y_prob = multi_head.predict(X_test) \npredicted_stock_price_transformer = y_prob.argmax(axis=-1)","00ead03d":"plt.figure(figsize = (18,9))\nplt.plot(y_test, color = 'black', label = 'GE Stock Price')\nplt.plot(predicted_stock_price_transformer, color = 'green', label = 'Predicted GE Close Price')\nplt.title('GE Close Price Prediction', fontsize=30)\n#plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)\nplt.xlabel('Date')\nplt.ylabel('GE Close Price')\nplt.legend(fontsize=18)\nplt.show()","0bdf7c96":"accuracy_score(y_test, predicted_stock_price_transformer)","ddef44fe":"# 3. [Attention]()","d126f4ae":" # [Multi-Head Attention]()","7614c093":"I'm going to use the New York Stock Exchange database in order to predict General Eletric's stock price in 30 days (buy or sell) from the 60 previous days worth of data from companies in the S&P500. \n\n 1. [LSTM]()\n 2. [LSTM + GRU]()\n 3. [Attention]()\n 4. [Multi-Head Attention]()","a26e7bf7":"# 1. [LSTM]()","1191dbb4":"![](https:\/\/i.imgur.com\/gLH0p3a.jpg)","46aa8d80":"# Work in progress, any suggestion for improvements is welcomed. ","01fa12c9":"# 2. [LSTM + GRU]()"}}