{"cell_type":{"7a77a344":"code","9696d874":"code","998ef52b":"code","5656edf2":"code","8fbbd417":"code","f21f7f0e":"code","7c6290f3":"code","879b55ab":"code","a8f7570a":"code","26fa64a8":"markdown","adff5ebf":"markdown","7186f519":"markdown","0fa48e97":"markdown","bc57a08c":"markdown","22f4aeb2":"markdown","8fbd0efb":"markdown","36c80ea0":"markdown","2ce947fc":"markdown"},"source":{"7a77a344":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport spacy\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9696d874":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntrain.head()","998ef52b":"test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ntest.head()","5656edf2":"#train set info\ntrain.info()\n#test set info\ntest.info()","8fbbd417":"#remove urls from the text\nimport re\ndef remove_urls(text):\n    #text = re.sub(r'^https?:\\\/\\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n    #return text\n    b = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return b.sub(r'',text)\n\ntrain['text'] = train['text'].apply(lambda x : remove_urls(x))\ntest['text'] = test['text'].apply(lambda x : remove_urls(x))\n\ntrain['text'].head()\ntest['text'].head()\n","f21f7f0e":"from spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport string\nfrom collections import Counter\n\n\nstopwords = STOP_WORDS\npunctuations = string.punctuation\n\nparser =  English()\n\ndef spacy_tokenizer(sentence):\n    mytokens = parser(sentence)\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n    return mytokens\n\ncount_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1, 2), stop_words = stopwords)\n\n#Example:\n#most frequently used 1-gram, 2-gram words in disaster tweets\n#disaster_tweets = train[train['target']==1]\n#X = count_vector.fit_transform(disaster_tweets['text'])\n#Counter(count_vector.vocabulary_).most_common(50)\n\n","7c6290f3":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n\nX = train.text\ny = train.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\nX_train.head()\n\nclf = LogisticRegression()\npipe = Pipeline([\n    ('count_vector', CountVectorizer()),  \n    ('clf', LogisticRegression()) \n])\npipe.fit(X_train,y_train)\n","879b55ab":"from sklearn import metrics\npredicted = pipe.predict(X_test)\nprint(\"accuracy :\",  metrics.accuracy_score(predicted, y_test))","a8f7570a":"submission = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\nsubmission[\"target\"] = pipe.predict(test.text)\nsubmission.to_csv(\"twitter-disaster-classifier-shajan-v2.csv\", index=False)\n","26fa64a8":"# Submission","adff5ebf":"# ML model","7186f519":"# Disaster tweet identification using logistic regression\n\n## Overview\n\nThis notebook demonstrates identification of disaster tweets in twitter messages using logistic regression\n\n## Steps\n\n* Load training and test data\n* Cleaning of datasets\n* Extract features (using CountVectorizer)\n* Fit the model and predict\n\n## Python packages used in this notebook\n* Pandas\n* spaCy\n* sklearn","0fa48e97":"# Prepare\/clean the dataset","bc57a08c":"# Load Test Set","22f4aeb2":"# Predict and get the accuracy","8fbd0efb":"# Load Training Set","36c80ea0":"# Verify train and test set","2ce947fc":"# Feature Extraction"}}