{"cell_type":{"9c1539f7":"code","cbdbd489":"code","e02f530e":"code","f20ad6d5":"code","e37062ce":"code","8d411c8a":"code","914453bb":"code","ca80316f":"code","814dd630":"code","d7bf34d8":"code","a41a0b88":"code","2886892f":"code","a0e6af13":"code","f41fe1e9":"code","76badb20":"code","55ef9f07":"code","a551ccbf":"code","7599dcbc":"code","0c1a1c78":"code","73594516":"code","5d5347ee":"code","5e20f963":"code","5b32aab6":"code","05ea836b":"code","2b876a5c":"markdown"},"source":{"9c1539f7":"#Kaggle data import code\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","cbdbd489":"#necessary Imports\nimport pandas as pd\nimport numpy as np\nfrom statsmodels.tsa.arima_model import ARMA\nfrom statsmodels.tsa.ar_model import AutoReg\n\nfrom statsmodels.tsa.statespace.varmax import VARMAX\nfrom sklearn.metrics import mean_squared_log_error\nfrom fbprophet import Prophet\nN = 13","e02f530e":"#Load datasets and fill missing values by 0\nfull_train = pd.read_csv('\/kaggle\/input\/demand-forecasting\/train_0irEZ2H.csv', parse_dates=['week'])\nfull_test = pd.read_csv('\/kaggle\/input\/demand-forecasting\/test_nfaJ3J5.csv', parse_dates=['week'])\n\n\nfull_train.fillna(0, inplace=True)\nfull_test.fillna(0,inplace= True)","f20ad6d5":"full_train.info()","e37062ce":"#Get STORE_ITEM pair\ndataset = [v for k, v in full_train.groupby(['sku_id','store_id'])]\ntest_dataset = [v for k, v in full_test.groupby(['sku_id','store_id'])]\nlen(test_dataset)","8d411c8a":"#Create units_sold column in test data with zeroes\nfor i in range(len(test_dataset)):\n    test_dataset[i]['units_sold'] = np.zeros(len(test_dataset[0]))","914453bb":"test_dataset[0]","ca80316f":"dataset[0][['week','units_sold']][:104]","814dd630":"len(dataset[i].iloc[104:])","d7bf34d8":"train_list = []\nvalidation_list = []\ntest_list =[]\nleng = len(dataset)\nfor i in range(leng):\n    train_list.append(dataset[i][['record_ID','week','units_sold']][:104])\n    validation_list.append(dataset[i][['record_ID','week','units_sold']][104:])\n    test_list.append(test_dataset[i][['record_ID','units_sold']])\n    \n    ","a41a0b88":"test_list[0]","2886892f":"merged_train = pd.concat(train_list)\nmerged_train.to_csv(\"train_dataset_merged.csv\")","a0e6af13":"merged_validation = pd.concat(validation_list)\nmerged_validation.to_csv(\"validation_dataset_merged.csv\")","f41fe1e9":"merged_test = pd.concat(test_list)\nmerged_test.to_csv(\"test_dataset_merged.csv\")","76badb20":"data = pd.read_csv(\".\/train_dataset_merged.csv\")","55ef9f07":"testsdg = pd.read_csv(\"..\/input\/demand-forecasting\/train_0irEZ2H.csv\",parse_dates=['week'])\nfuture_pd = testsdg['week'].sort_values().unique()\nfuture_pd = pd.DataFrame({'ds': future_pd})\nfuture_pd.columns","a551ccbf":"train_dataset_merged = pd.read_csv(\".\/train_dataset_merged.csv\",parse_dates=['week'])\ntrain_dates =  pd.DataFrame({'ds': train_dataset_merged['week'].sort_values().unique() }).squeeze()\n\ntrain_dates","7599dcbc":"120120\/104","0c1a1c78":"start = 0\nds_y = pd.DataFrame()\nds_y['ds'] = train_dates\nds_y['y'] = data['units_sold'][start:start+104]\nds_y","73594516":"\ndef prophet(ds_y,future_pd):\n    model = Prophet(\n    interval_width=0.95,\n    growth='linear',\n    daily_seasonality=False,\n    weekly_seasonality=True,\n    yearly_seasonality=True,\n    seasonality_mode='multiplicative')\n    model.fit(ds_y)\n    forecast_pd = model.predict(future_pd)\n    print(forecast_pd)\n    return forecast_pd['yhat']\ndef VARMAX(data):\n    model = VARMAX(data)\n    model_fit = model.fit(disp=False)\n    # make prediction\n    yhat = model_fit.forecast()\n    return yhat\n\ndef ARIMA(data):\n    model = ARMA(data, order=(1, 1))\n    model_fit = model.fit(disp=False)\n    # make prediction\n    yhat = model_fit.predict(len(data), len(data)+12)\n \n    return yhat\n\ndef Autoreg(data):\n    model = AutoReg(data, lags=1)\n    model_fit = model.fit()\n    print(len(data))\n    yhat = model_fit.predict(start=\"16\/07\/2013\", end=\"01\/10\/2013\")\n    return yhat","5d5347ee":"start = 0 \npred_values_list =[]\nfor i in range(int(len(data)\/104)):\n    ds_y = pd.DataFrame()\n    ds_y['ds'] = train_dates\n    ds_y['y'] = data['units_sold'][start:start+104].values\n    pred_values_list.append(prophet(ds_y,future_pd))\n    start+=104","5e20f963":"pred_values_list","5b32aab6":"x = pd.concat(pred_values_list)\nx = x.apply(np.floor)\nx = x.abs()\nany(x<0)","05ea836b":"x.to_csv(\"pred.csv\")","2b876a5c":"**Problem Statement -** https:\/\/datahack.analyticsvidhya.com\/contest\/janatahack-demand-forecasting\/#ProblemStatement\n\n\nOne of the largest retail chains in the world wants to use their vast data source to build an efficient forecasting model to predict the sales for each SKU in its portfolio at its 76 different stores using historical sales data for the past 3 years on a week-on-week basis. Sales and promotional information is also available for each week - product and store wise. \n\nHowever, no other information regarding stores and products are available. Can you still forecast accurately the sales values for every such product\/SKU-store combination for the next 12 weeks accurately? If yes, then dive right in!"}}