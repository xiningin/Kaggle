{"cell_type":{"1580f03d":"code","c7cca39c":"code","e653c4dd":"code","41333bcb":"code","0f5c6f7a":"code","f214923f":"code","25020a91":"code","d2b21da9":"code","5540d6f3":"code","b8388707":"code","a4a737ec":"code","65a4e7b5":"code","c405903e":"code","1c47448a":"code","38edc88d":"code","0fcde074":"code","45ce72e1":"code","361e69fd":"code","2a360706":"code","022cfb8a":"code","4f1d1cb5":"code","326e8c04":"code","0f8068d0":"code","a81b4d37":"code","5a228d90":"code","fd0e9e86":"code","87995d31":"code","d21f6fcf":"code","2ce4d411":"code","d5db07f9":"code","a06f028a":"code","f8e8c41d":"code","8a7a92a1":"code","aa6d34e8":"code","6ad01929":"code","3fbd1afb":"code","5d208893":"code","0dbbab8c":"code","654fd7f8":"code","14711fe5":"code","9f22d4c0":"code","27d93317":"code","79973e9c":"code","e6930e6b":"code","d5881604":"code","1c047c7b":"code","b2ee6cf6":"code","e9612f83":"markdown","1aa950e1":"markdown","8b5b93c1":"markdown","8f0e826d":"markdown","11fab01d":"markdown","a3ef98d3":"markdown","2ffdd94e":"markdown","78d654ef":"markdown","7b65f607":"markdown","7a3fda92":"markdown","ba0ff660":"markdown","b2b73f34":"markdown","212ba5d5":"markdown","13b4b4f4":"markdown","d34fdbfc":"markdown","6aaaaa65":"markdown"},"source":{"1580f03d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c7cca39c":"housing=pd.read_csv(\"..\/input\/california-housing-prices\/housing.csv\")\nhousing.head()","e653c4dd":"housing.info()","41333bcb":"housing[\"ocean_proximity\"].value_counts()","0f5c6f7a":"housing.describe()","f214923f":"import matplotlib.pyplot as plt\nhousing.hist(bins=50,figsize=(20,15))\nplt.show()","25020a91":"from sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)","d2b21da9":"housing[\"income_cat\"] = np.ceil(housing[\"median_income\"] \/ 1.5)\nhousing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)","5540d6f3":"from sklearn.model_selection import StratifiedShuffleSplit\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]\n","b8388707":" housing[\"income_cat\"].value_counts() \/ len(housing)","a4a737ec":"for set in (strat_train_set, strat_test_set):\n set.drop([\"income_cat\"], axis=1, inplace=True)","65a4e7b5":"housing = strat_train_set.copy()","c405903e":"housing.plot(kind=\"scatter\",x=\"longitude\",y=\"latitude\")","1c47448a":"housing.plot(kind=\"scatter\",x=\"longitude\",y=\"latitude\",alpha=0.1)","38edc88d":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n s=housing[\"population\"]\/100, label=\"population\",\n c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n)\nplt.legend()\n","0fcde074":"corr_matrix = housing.corr()\n","45ce72e1":"corr_matrix[\"median_house_value\"].sort_values(ascending=False)","361e69fd":"housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n alpha=0.1)\n","2a360706":"housing[\"rooms_per_household\"] = housing[\"total_rooms\"]\/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]\/housing[\"total_rooms\"]\nhousing[\"population_per_household\"]=housing[\"population\"]\/housing[\"households\"]\n","022cfb8a":"corr_matrix = housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","4f1d1cb5":"housing = strat_train_set.drop(\"median_house_value\", axis=1)\nhousing_labels = strat_train_set[\"median_house_value\"].copy()","326e8c04":"median = housing[\"total_bedrooms\"].median()\nhousing[\"total_bedrooms\"].fillna(median) ","0f8068d0":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')","a81b4d37":"housing_num = housing.drop(\"ocean_proximity\", axis=1)\n","5a228d90":"imputer.fit(housing_num)\n","fd0e9e86":"X = imputer.transform(housing_num)","87995d31":"housing_tr = pd.DataFrame(X, columns=housing_num.columns)\n","d21f6fcf":"from sklearn.preprocessing import LabelBinarizer\nencoder = LabelBinarizer()\nhousing_cat=housing[\"ocean_proximity\"]\nhousing_cat_1hot = encoder.fit_transform(housing_cat)\nhousing_cat_1hot","2ce4d411":"from sklearn.base import BaseEstimator, TransformerMixin\nrooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):  \n    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self # nothing else to do\n    def transform(self, X, y=None):\n        rooms_per_household = X[:, rooms_ix] \/ X[:, household_ix]\n        population_per_household = X[:, population_ix] \/ X[:, household_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] \/ X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household,bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)\n\n","d5db07f9":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nnum_pipeline = Pipeline([\n ('imputer', SimpleImputer(strategy=\"mean\")),\n ('attribs_adder', CombinedAttributesAdder()),\n ('std_scaler', StandardScaler()),\n ])\nhousing_num_tr = num_pipeline.fit_transform(housing_num)","a06f028a":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names=attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names].values\n","f8e8c41d":"from sklearn.base import TransformerMixin #gives fit_transform method for free\nclass MyLabelBinarizer(TransformerMixin):\n    def __init__(self, *args, **kwargs):\n        self.encoder = LabelBinarizer(*args, **kwargs)\n    def fit(self, x, y=0):\n        self.encoder.fit(x)\n        return self\n    def transform(self, x, y=0):\n        return self.encoder.transform(x)","8a7a92a1":"from sklearn.pipeline import FeatureUnion\nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\nnum_pipeline = Pipeline([\n ('selector', DataFrameSelector(num_attribs)),\n ('imputer', SimpleImputer(strategy=\"mean\")),\n ('attribs_adder', CombinedAttributesAdder()),\n ('std_scaler', StandardScaler()),\n ])\ncat_pipeline = Pipeline([\n ('selector', DataFrameSelector(cat_attribs)),\n ('label_binarizer', MyLabelBinarizer()),\n ])\nfull_pipeline = FeatureUnion(transformer_list=[\n (\"num_pipeline\", num_pipeline),\n (\"cat_pipeline\", cat_pipeline),\n ])\n","aa6d34e8":"housing_prepared = full_pipeline.fit_transform(housing)\nhousing_prepared","6ad01929":"from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)\n","3fbd1afb":"some_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\\t\", lin_reg.predict(some_data_prepared))\nprint(\"Labels:\\t\\t\", list(some_labels))\n","5d208893":"from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n","0dbbab8c":"from sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(housing_prepared, housing_labels)","654fd7f8":"housing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse\n","14711fe5":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels,\nscoring=\"neg_mean_squared_error\", cv=10)\nrmse_scores = np.sqrt(-scores)","9f22d4c0":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())","27d93317":"lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)","79973e9c":"from sklearn.ensemble import RandomForestRegressor\nforest_reg = RandomForestRegressor()\nforest_reg.fit(housing_prepared, housing_labels)","e6930e6b":"for_scores=cross_val_score(forest_reg, housing_prepared, housing_labels,scoring=\"neg_mean_squared_error\", cv=10)\nfor_rmse_scores = np.sqrt(-for_scores)\ndisplay_scores(for_rmse_scores)","d5881604":"from sklearn.model_selection import GridSearchCV\nparam_grid = [\n {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n ]\nforest_reg = RandomForestRegressor()\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n scoring='neg_mean_squared_error')\ngrid_search.fit(housing_prepared, housing_labels)","1c047c7b":"grid_search.best_params_\n","b2ee6cf6":"final_model = grid_search.best_estimator_\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse) \nprint(final_rmse)","e9612f83":"Create features to make it easier to develop the model","1aa950e1":"Use pipeline to get all together","8b5b93c1":"In this project, I will be analysing and training a machine learning model on the california housing prices(Hands on Machine Learning book)","8f0e826d":"Stratify the data after spliting it from train and test split","11fab01d":"Use best parameters to get finally model","a3ef98d3":"Get DecisionTreeRegressor and compare it to Linear Regression","2ffdd94e":"Use Label Binarizer for preprocessing","78d654ef":"Check correlation for data and highest corr is median house value","7b65f607":"First, I will read the the data from housing csv and do some EDA. By getting the info,description, and more. ","7a3fda92":"Remove missing data using a simpleImputer","ba0ff660":"Get mean squared error","b2b73f34":"Create a transformation method","212ba5d5":"Get predictions using Linear Regression","13b4b4f4":"Plot to check the data","d34fdbfc":"Plot a histogram of the data for more visualzation ","6aaaaa65":"More plotting to see relationships between longitude and latitude and use alpha to make it more clearer"}}