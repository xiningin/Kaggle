{"cell_type":{"8ecebf06":"code","c4e29d7c":"code","da6312c0":"code","0d83f8fc":"code","9934a2f3":"code","05d67796":"code","117f9243":"code","c6cf08c3":"code","03976479":"code","aa729585":"code","87492dea":"code","5780f51f":"code","14b5168e":"code","b97f2dc1":"code","778f88cf":"code","18c1a92e":"code","e6a9625b":"code","c583bc1d":"code","a39b2643":"code","f2242384":"code","6ff984cc":"code","2b6e1e0f":"code","af251227":"code","d6a0c027":"code","df8dd308":"code","5aec1fbd":"code","c9c980f5":"code","6cc2299e":"code","fbce6691":"code","df09f72a":"code","c043fe00":"code","eae57429":"code","c0aa757e":"code","049b0165":"code","8fec7ccf":"code","6bfc61bd":"code","69e6131f":"code","6fa78918":"code","f7396ccc":"code","7f52b640":"code","d2556f09":"code","6ec74328":"code","8ac0ef21":"code","3cfaedbf":"code","5fbe39b8":"code","14bea3ec":"code","525e17e3":"code","aac83043":"code","0708639a":"code","dfde601a":"code","7e595a3a":"code","da81161f":"code","22011182":"code","6fca26e2":"code","8f836245":"code","204e232d":"code","c780ab8f":"markdown","7f1b8860":"markdown","c52d6a24":"markdown","acac6dad":"markdown","16807183":"markdown","ce62544c":"markdown","4a449dd2":"markdown","f5290cff":"markdown","517d716c":"markdown","14b0cb7f":"markdown","f14b3805":"markdown","ead44e23":"markdown","c70d0216":"markdown","53c03d84":"markdown","8fa83d2f":"markdown","f6dbe566":"markdown","c68b9fd3":"markdown","dfb32e48":"markdown","c646638c":"markdown","98020ebd":"markdown"},"source":{"8ecebf06":"# Common imports\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# to make this notebook's output identical at every run\nnp.random.seed(42)\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n","c4e29d7c":"# On Kaggle Notebook, after adding the data, you can import the data as Pandas DataFrame as follows\nimport pandas as pd\nincome_df = pd.read_csv(\"..\/input\/income-classification\/income_evaluation.csv\")\nincome_df.head()","da6312c0":"income_df.info()","0d83f8fc":"income_df.columns = income_df.columns.str.strip()\nincome_df.columns","9934a2f3":"continuous_features=income_df.describe()\ncontinuous_features","05d67796":"categorical_features=income_df.describe(include=['object'])\ncategorical_features","117f9243":"income_df.isna().sum()","c6cf08c3":"for col in income_df.columns:\n    print(\"Column:\",col, \"\\n\\n\", income_df[col].value_counts(),\"\\n\")","03976479":"income_df['workclass'].replace(' ?', np.NaN, inplace=True)\nincome_df['occupation'].replace(' ?', np.NaN, inplace=True)\nincome_df['native-country'].replace(' ?', np.NaN, inplace=True)","aa729585":"income_df.drop([\"fnlwgt\",\"education\"],axis=1,inplace=True)","87492dea":"income_df.isna().sum()","5780f51f":"income_df.columns = income_df.columns.str.strip()\nincome_df.columns","14b5168e":"list(categorical_features.drop(\"education\",axis=1))","b97f2dc1":"for col in income_df.describe(include=['object']).columns:\n    income_df[col] = income_df[col].str.strip()","778f88cf":"income_df[\"income\"].unique()","18c1a92e":"# income_df['income'].replace(' <=50K', 0, inplace=True)\n# income_df['income'].replace(' >50K', 1, inplace=True)\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\nincome_df['income']= label_encoder.fit_transform(income_df['income'])","e6a9625b":"income_df[\"income\"].value_counts()","c583bc1d":"categorical_features.columns","a39b2643":"%matplotlib inline\nimport matplotlib.pyplot as plt\nincome_df.hist(bins=50, figsize=(20,15))\nplt.show()","f2242384":"import warnings\nwarnings.filterwarnings('ignore')","6ff984cc":"sns.countplot(x='workclass',data=income_df)","2b6e1e0f":"#To split the data using train_test_split with stratified parameter\nfrom sklearn.model_selection import train_test_split\nstrat_train_set, strat_test_set = train_test_split(income_df, test_size=0.2, random_state=42, \n                                         stratify = income_df[\"income\"])","af251227":"# revert to a clean training set \n# separate the predictors and the labels\ntrain = strat_train_set.drop(\"income\", axis=1) # drop labels for training set\ntrain_labels = strat_train_set[\"income\"].copy()","d6a0c027":"# revert to a clean test set \n# separate the predictors and the labels\ntest = strat_test_set.drop(\"income\", axis=1) # drop labels for test set\ntest_labels = strat_test_set[\"income\"].copy()","df8dd308":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler,RobustScaler,Normalizer,MaxAbsScaler\n\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('std_scaler', RobustScaler()),\n    ])\n\n# df_num_tr = num_pipeline.fit_transform(df_train[['age','fnlwgt','education-num','capital-gain','capital-loss','hours-per-week']])\n","5aec1fbd":"cat_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n        (\"cat\", OneHotEncoder())\n    ])\n","c9c980f5":"categorical_features","6cc2299e":"# categorical_features.drop(\" income\", axis=1)","fbce6691":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nnum_attribs = list(continuous_features.drop(\"fnlwgt\",axis=1))\ncat_attribs = list(categorical_features.drop([\"income\",\"education\"], axis=1))\n\n#using ColumnTransformer to merge the numerical and categorical attributes\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", cat_pipeline, cat_attribs),\n    ])\n\ntrain_prepared = full_pipeline.fit_transform(train)","df09f72a":"#to get the shape of train data\ntrain_prepared.shape","c043fe00":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_validate\n\n# log_reg=LogisticRegression()\n# log_reg.fit(train_prepared,train_labels)\nscores = cross_validate(LogisticRegression(),\n                        train_prepared, train_labels, cv=10, scoring=('roc_auc', 'average_precision','recall'))\n\nprint(\"ROC_AUC: \",scores['test_roc_auc'].mean())\nprint(\"Average Precision: \",scores['test_average_precision'].mean())\nprint(\"Average Recal: \",scores['test_recall'].mean())","eae57429":"# SVM\nfrom sklearn.svm import LinearSVC, SVC\nscores = cross_validate(LinearSVC(),\n                        train_prepared, train_labels, cv=10, scoring=('roc_auc', 'average_precision','recall'))\n\nprint(\"ROC_AUC: \",scores['test_roc_auc'].mean())\nprint(\"Average Precision: \",scores['test_average_precision'].mean())\nprint(\"Average Recall: \",scores['test_recall'].mean())","c0aa757e":"# Naive Bayes\nfrom sklearn.naive_bayes import BernoulliNB\nscores = cross_validate(BernoulliNB(),\n                        train_prepared, train_labels, cv=10, scoring=('roc_auc', 'average_precision','recall'))\n\n\nprint(\"ROC_AUC: \",scores['test_roc_auc'].mean())\nprint(\"Average Precision: \",scores['test_average_precision'].mean())\nprint(\"Average Recal: \",scores['test_recall'].mean())","049b0165":"# K nearest neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\nscores = cross_validate(KNeighborsClassifier(),\n                        train_prepared, train_labels, cv=10, scoring=('roc_auc', 'average_precision','recall'))\n\nprint(\"ROC_AUC: \",scores['test_roc_auc'].mean())\nprint(\"Average Precision: \",scores['test_average_precision'].mean())\nprint(\"Average Recal: \",scores['test_recall'].mean())","8fec7ccf":"#LogisticRegression\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score\nlog_reg=LogisticRegression()\nlog_reg.fit(train_prepared,train_labels)\n\ntest_prepared=full_pipeline.transform(test)\nprediction = log_reg.predict(test_prepared)\n\nprint(\"Accuracy\",accuracy_score(test_labels,prediction))\nprint(\"precision\",precision_score(test_labels,prediction))\nprint(\"Recall\",recall_score(test_labels,prediction))\n\n","6bfc61bd":"#LinearSVC\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,mean_squared_error\nsvc=LinearSVC()\nsvc.fit(train_prepared,train_labels)\n\ntest_prepared=full_pipeline.transform(test)\nprediction = svc.predict(test_prepared)\n\nprint(\"Accuracy\",accuracy_score(test_labels,prediction))\nprint(\"precision\",precision_score(test_labels,prediction))\nprint(\"Recall\",recall_score(test_labels,prediction))\n\n","69e6131f":"#BernoulliNB()\n# from sklearn.metrics import accuracy_score,precision_score,recall_score\n\nbnb=BernoulliNB()\nbnb.fit(train_prepared,train_labels)\n\ntest_prepared=full_pipeline.transform(test)\nprediction = bnb.predict(test_prepared)\n\nprint(\"Accuracy\",accuracy_score(test_labels,prediction))\nprint(\"Precision\",precision_score(test_labels,prediction))\nprint(\"Recall\",recall_score(test_labels,prediction))\n\n","6fa78918":"# #KNeighborsClassifier()\n# # from sklearn.metrics import accuracy_score,precision_score,recall_score\n\n# knn=KNeighborsClassifier()\n# knn.fit(train_prepared,train_labels)\n\n# test_prepared=full_pipeline.transform(test)\n# prediction = knn.predict(test_prepared)\n\n# print(\"Accuracy\",accuracy_score(test_labels,prediction))\n# print(\"Precision\",precision_score(test_labels,prediction))\n# print(\"Recall\",recall_score(test_labels,prediction))\n","f7396ccc":"from sklearn.model_selection import GridSearchCV\n\n# Create hyperparameter options\nhyperparameters = dict(C=np.logspace(0, 4, 10), penalty=['l1', 'l2'])\nlog_reg=LogisticRegression()\ngrid_search = GridSearchCV(log_reg, hyperparameters, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\ngrid_search.fit(train_prepared, train_labels)","7f52b640":"grid_search.best_params_","d2556f09":"grid_search.best_estimator_","6ec74328":"prediction=grid_search.predict(test_prepared)\n\nprint(\"Accuracy\",accuracy_score(test_labels,prediction))\nprint(\"Precision\",precision_score(test_labels,prediction))\nprint(\"Recall\",recall_score(test_labels,prediction))\n","8ac0ef21":"# from sklearn.linear_model import LogisticRegression\n# from sklearn.metrics import accuracy_score\n# from sklearn.learning_curve import validation_curve\n# # learning_curve import validation_curve\n\n# C_param_range = [0.001,0.01,0.1,1,10,100]\n\n# sepal_acc_table = pd.DataFrame(columns = ['C_parameter','Accuracy'])\n# sepal_acc_table['C_parameter'] = C_param_range\n\n# plt.figure(figsize=(10, 10))\n\n# j = 0\n# for i in C_param_range:\n    \n#     # Apply logistic regression model to training data\n#     lr = LogisticRegression(penalty = 'l2', C = i,random_state = 0)\n#     lr.fit(train_prepared,train_labels)\n#     # Predict using model\n#     y_pred_sepal = lr.predict(test_prepared)\n    \n#     # Saving accuracy score in table\n#     sepal_acc_table.iloc[j,1] = accuracy_score(y_test_sepal,y_pred_sepal)\n#     j += 1\n    \n#     # Printing decision regions\n#     plt.subplot(3,2,j)\n#     plt.subplots_adjust(hspace = 0.4)\n#     plot_decision_regions(X = X_combined_sepal_standard\n#                       , y = Y_combined_sepal\n#                       , classifier = lr\n#                       , test_idx = range(105,150))\n#     plt.xlabel('Sepal length')\n#     plt.ylabel('Sepal width')\n#     plt.title('C = %s'%i)","3cfaedbf":"#pip install learning_curve","5fbe39b8":"k_range = [5, 7, 9]\np_values=[1, 2, 5]\nprint(k_range)\nparam_grid = dict(n_neighbors=k_range,p=p_values)\n\nprint(param_grid)\n# instantiate the grid\ngrid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=10, scoring='accuracy')\n\ngrid_search.fit(train_prepared, train_labels)","14bea3ec":"# prediction=grid_search.predict(test_prepared)\n\n# print(\"Accuracy\",accuracy_score(test_labels,prediction))\n# print(\"Precision\",precision_score(test_labels,prediction))\n# print(\"Recall\",recall_score(test_labels,prediction))\n","525e17e3":"from sklearn.naive_bayes import BernoulliNB\nfrom sklearn.preprocessing import PowerTransformer\n\n\nnb_classifier = BernoulliNB()\n\nparams_NB = {'alpha': [0.001,0.01,0.1,0.5,1]}\n\ngs_NB = GridSearchCV(estimator=nb_classifier, \n                     param_grid=params_NB, \n                     cv=5,\n                     verbose=1, \n                     scoring='accuracy')\n\ngs_NB.fit(train_prepared, train_labels)\n# # Data_transformed = PowerTransformer().fit_transform(train_prepared)\n\n# gs_NB.fit(Data_transformed, target);","aac83043":"nb_classifier.get_params()","0708639a":"gs_NB.best_estimator_","dfde601a":"prediction=gs_NB.predict(test_prepared)\n\nprint(\"Accuracy\",accuracy_score(test_labels,prediction))\nprint(\"Precision\",precision_score(test_labels,prediction))\nprint(\"Recall\",recall_score(test_labels,prediction))\n","7e595a3a":"# defining parameter range \nparam_grid = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf','linear']}  \n  \ngrid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3) \n  \n# fitting the model for grid search \n# grid.fit(X_train, y_train)\ngrid.fit(train_prepared, train_labels)","da81161f":"prediction=grid.predict(test_prepared)\n\nprint(\"Accuracy\",accuracy_score(test_labels,prediction))\nprint(\"Precision\",precision_score(test_labels,prediction))\nprint(\"Recall\",recall_score(test_labels,prediction))\n","22011182":"# On Kaggle Notebook, after adding the data, you can import the data as Pandas DataFrame as follows\nimport pandas as pd\nactivity_train = pd.read_csv(\"..\/input\/human-activity-recognition-with-smartphones\/train.csv\")\nactivity_test = pd.read_csv(\"..\/input\/human-activity-recognition-with-smartphones\/test.csv\")","6fca26e2":"activity_train.shape","8f836245":"activity_test.shape","204e232d":"activity_train","c780ab8f":"#### Question 2 (Explore the data) [10 points]:\nCreate plots to visualize the distribution of the target \"income\" and the distributions of feature variables. Create plots to visualize the relationships between the target variable and the feature variables. Discuss whether a feature is useful to predict the target. Discuss your observations about the variables and wranglings on the data for the classification task. Use evidence in data analyses to support your discussions. ","7f1b8860":"### Data for Multiple-class Classification in this assignment:\n* URL: https:\/\/www.kaggle.com\/uciml\/human-activity-recognition-with-smartphones\n* On Kaggel Notebook, you can add the data set by searching the above URL\n* Column \u201cActivity\u201d is the target label to classify.","c52d6a24":"Are there any missing values? ","acac6dad":"### Data for Binary Classification in this assignment:\n* URL: https:\/\/www.kaggle.com\/lodetomasi1995\/income-classification\n* On Kaggel Notebook, you can add the data set by searching the above URL\n* Column \u201cincome\u201d is the target label to classify.\n","16807183":"Wrangle the labels of data if necessary.\n\nWe have in starting of the column name of all the features so we striped the space.\nChanged the target label [' <=50K', ' >50K'] to [0,1]","ce62544c":"### Validation on Testset","4a449dd2":"#### Question 4 (Compare Models) [15 points]: \nClassify \"income\" by building and evaluating the following 4 types of models: Logistic Regression, Support Vector Machines, Na\u00efve Bayes and K-Nearest Neighbors. Apply cross-validation with Pipeline, ColumnTransformer and OneHotEncoder in the modeling process. Compare the performance of different models on the Test data. Discuss your results and findings.","f5290cff":"The info() method is useful to get a quick description of the data, in particular the total number of rows, each attribute\u2019s type, and the number of nonnull values in each attribute.\n\nThere are 32,561 instances in the dataset, and 15 data features which is listed above.\n\nThe 15 attributes are \\\n' age', ' workclass', ' fnlwgt', ' education', ' education-num',' marital-status', ' occupation', ' relationship', ' race', ' sex', ' capital-gain', ' capital-loss', ' hours-per-week', ' native-country',' income'.\n\n' income' is the target variable.","517d716c":"#### Question 1 (Explore the data) [5 points]:\nExamine the features of the data. Identify which features are continuous and which features are categorical. Compute the statistics of features. Are there any missing values? Wrangle the labels of data if necessary.","14b0cb7f":"Now we have null values in three features:\\\nworkclass\\\noccupation\\\nnative-country\n","f14b3805":"There are no missing values but after checking the value_counts for all the Features we got to know that have ' ?' symbol for three coumns : workclass, occupation, native-country, so replaced symbol with np.NaN values","ead44e23":"#### Question 3 (Compare Models) [10 points]:\nApply different scalers on features and compare the performance of different models. Does scaling affect the results? Why?","c70d0216":"#### Question 1 (Explore the data) [10 points]:\nCreate plots to visualize the distribution of the label \"Activity\". Is the distribution of each class is balanced?\nIf imbalanced, what strategies would you adopt and why? Continue to explore the distributions of feature variables, and the relationships between the target variable and the feature variables. Discuss your observations about the variables and wranglings on the data for the classification task. Use evidence in data analyses to support your discussions. ","53c03d84":"__Identify which features are continuous and which features are categorical and Statistics of features:__","8fa83d2f":"The describe() method is a simple way for calculating statistical data like percentile, mean and standard deviation of the numerical values of the Series or DataFrame. It analyzes both numeric and object series and also the DataFrame column sets of mixed data types.\n\ndescribe() method have parameter `include` specifies, List of data types to be included while describing dataframe. Default is None and which only provide statistics of numerical features (Continous features).\n\n* Continous features: age, fnlwgt, education-num, capital-gain, capital-loss, hours-per-week.\n\n* Categorical features: workclass, education, marital-status, occupation, relationship, race, sex, native-country, income.","f6dbe566":"#### Question 2 (Compare Models) [15 points]:\nClassify \"Activity\" by building and evaluating the following 4 types of models using cross-validation: Logistic Regression, Support Vector Machines, Na\u00efve Bayes and K-Nearest Neighbors. Which models are incapable of handling multiple classes natively? What strategy\/strategies can be applied to perform multiclass classification for those models? Compare the performance of different models on Test data. Discuss your results and findings.","c68b9fd3":"#### Question 4 (Fine-tune Models) [15 points]:\nChoose and tune parameters of each of the models above. Discuss the selected parameters and tuning process.  Evaluate the performance of the best estimator of each type of model on the Test data. Does tuning imporve results? Discuss your results and findings.","dfb32e48":"#### Question 3 (Prepare the data) [5 points]: \nSplit the data into train and test data, What parameters do you use for splitting the data? Why? Discuss your observations and suggestions on prepare feature variables for the prediction task.","c646638c":"##### <h1 style=\"text-align:center\"> Drexel University <\/h1>\n<h2 style = \"text-align:center\"> College of Computing and Informatics<\/h2>\n<h2 style = \"text-align:center\">INFO T780: Applied Machine Learning<\/h2>\n<h3 style = \"text-align:center\">Assignment 2<\/h3>\n<h4> <\/h4>\n<div style=\"text-align:center; border-style:solid; padding: 10px\">\n<div style=\"font-weight:bold\">Due Date: Sunday, August 2, 2020<\/div>\nThis assignment counts for 15% of the final grade\n<\/div>\n\n<h3 style=\"color:red; font-weight:bold; text-decoration: underline\">DON'T FORGET TO PUT YOUR TEAM NUMBER AND MEMBERS' NAMES BELOW<\/h3>\n\n### TEAM NUMBER:\n\n### TEAM MEMBERS:\n\n### A. Assignment Overview\nThis assignment provides the opportunity for you to practice with various skills in data pre-processing, feature analyses, and model tuning. \n\n### B. What to Hand In\n\t\nSumbit a completed this Jupyter notebook. \n\n### C. How to Hand In\n\nSubmit your Jupyter notebook file through the course website in the Blackboard Learn system.\n\n### D. When to Hand In\n\n1. Submit your assignment no later than 11:59 pm in the due date.\n2. There will be a 10% (absolute value) deduction for each day of lateness, to a maximum of 3 days; assignments will not be accepted beyond that point. Missing work will earn a zero grade.\n\n### E. Written Presentation Requirements (if applicable)\nImages must be clear and legible. Assignments will be judged on the basis of visual appearance, grammatical correctness, and quality of writing, as well as their contents. Please make sure that the text of your assignments is well-structured, using paragraphs, full sentences, and other features of well-written presentation.\n\n### F. Academic Honesty\n\nEach student is required to submit the Academic Honesty Form at the beginning of the term to cover all the deliverables (for example: assignments, projects, quizzes). Each piece of work must be original. That means, individual quizzes must be done individually without discussing and collaborating with anybody else. Team assignments must be written and programmed by your own team members. No team should copy any piece of work from other teams. The Drexel University Academic Honesty Rules and Procedures (as stated in the student handbook) will be adhered to strictly.  \n\n### G. Marking Schemes:\n\nMarking assignments will be based on several aspects: presentation, correctness and coding styles. \n\nFor programming questions, 10% of the mark will be judged on the coding style. \n\nThe following is a set of guidelines for the coding style in this course:\n1. Write a good comment.\n2. Use meaningful names for variables and functions.\n3. Build reuseable functions to achieve modularization\n4. Use efficient implementation in existing libraries\n\n### H. Answer the following questions: Your answer should be combined with code and brief text answer. Please ensure that your Jupyter notebook does not have too much spurious output. If you like, you can share your notebook in progress with me on Kaggle: yuanandrexel (ya45@drexel.edu)","98020ebd":"#### Quesetion 5 (Fine-tune Moldes) [15 points]:\nChoose and tune parameters of each of the models above. Discuss the selected parameters and tuning process.  Evaluate the performance of the best estimator of each type of model on the Test data. Does tuning imporve results? Discuss your results and findings."}}