{"cell_type":{"ebd0f8f8":"code","13db7ff6":"code","c8fa9351":"code","9c5d45e9":"code","3f7d9bb3":"code","6a1f0caa":"code","ae077f20":"code","4927229e":"code","a681bdfd":"code","fa18bf32":"code","228e9d30":"code","fc490ea0":"code","ccf9fb12":"code","5bb50d9c":"code","a7b6d947":"code","77366a29":"code","f1120f14":"code","67045cf2":"markdown","0ba78bda":"markdown","f1e6453c":"markdown","08c51ac3":"markdown","b2bf855d":"markdown","722110eb":"markdown","51fa7833":"markdown","3c3e6d9a":"markdown","6b288cf0":"markdown","a967b037":"markdown","72d20699":"markdown","d6b2f054":"markdown","5fa13b80":"markdown"},"source":{"ebd0f8f8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch ##for tensors\nimport torch.nn as nn ##for convolution\nimport torch.nn.functional as F #Might be not needed\nimport matplotlib.pyplot as plt #plotting for future\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col, GreedyAgent, random_agent","13db7ff6":"# get position in (row, column) format from index\ndef pos(index):\n    return row_col(index,11)\n\ndef state_from_observation(observation):\n    food_score = 3\n    this_head_score = 1\n    this_body_score = -2\n    this_tail_score = 0\n    other_body_score = -2\n    other_head_score = -4\n    \n    \n    geese = observation[\"geese\"] ##array of geese bodies\n    food = observation[\"food\"] ##array for food\n    index = observation[\"index\"] ##my index\n    this_goose = geese[index] #our controlled goose\n    other_goose = [geese[i] for i in range(len(geese)) if i != index]\n    \n    state = torch.zeros(7,11, dtype=torch.float) ##base state space\n    \n    #Set values for our goose\n    body_length = len(this_goose)\n    for i,body in enumerate(this_goose):\n        state[pos(body)] = this_body_score\n        if i == len(this_goose)-1:\n            state[pos(body)] = this_tail_score\n        if i == 0:\n            state[pos(body)] = this_head_score\n    \n    ##Set values for other geese\n    for goose in other_goose:\n        for i,body in enumerate(goose):\n            state[pos(body)] = other_body_score\n            if i == len(goose)-1:\n                state[pos(body)] = other_body_score\/2\n            if i==0:\n                state[pos(body)] = other_head_score\n                \n    ##Set values for food\n    for f in food:\n        state[pos(f)] = food_score\n    \n    return state","c8fa9351":"class TorusConv2d(nn.Module):\n    def __init__(self, input_shape, output_shape, kernel_size, bias=False):\n        super().__init__()\n        self.edge_size = (kernel_size[0]\/\/2, kernel_size[1]\/\/2) ##how much we have to pad, eg.: for 3x3 -> we need 1 extra layer\n        self.conv = nn.Conv2d(input_shape, output_shape, kernel_size, bias=False)\n    \n    def forward(self, x):\n        h = torch.cat([x[:, :, -self.edge_size[1]:], x, x[:, :, :self.edge_size[1]]], dim = 2) #adding the extra layer on the columns      \n        h = torch.cat([h[:, -self.edge_size[0]:], h, h[:, :self.edge_size[0]]], dim=1) #rows\n        h = self.conv(h)\n        return h","9c5d45e9":"def diffuse_state(state, n = 1):\n    #we expect the state to be a tensor of size (7,11)\n    #Pytorch needs format (N, C, Dim0, Dim1), where N=number of samples, C=number of channels (used for 3D state space), Dim0 and Dim1 is our 2D state space dimensions \n    x = state.view(1,1, state.size(0), state.size(1)).detach() #we have to make it dimensions (1, 1, 7, 11)\n    weight = torch.tensor([[0,1,0], [1,9,1], [0,1,0]], dtype=torch.float).view(1, 1, 3, 3).detach() ##custom weight for convolution \n    weight \/= weight.sum() ##for normalisation of kernel\n    conv = nn.Conv2d(1, 1, 3, padding=0, bias=False)\n    conv.weight = nn.Parameter(weight) ##setting the weight of the convolution\n    for i in range(n):\n        h = torch.cat([x[:, :, :, -1:], x, x[:, :, :, :1]], dim = 3) #adding the extra layer on the columns      \n        h = torch.cat([h[:, :, -1:], h, h[:, :, :1]], dim=2) #rows\n        y = conv(h).detach()\n        x = y\n    \n    return y.squeeze() ##we view y \"normally\" again, => size = (7,11)","3f7d9bb3":"%%writefile diffuse_agent_static.py\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\nActions = [\"NORTH\", \"SOUTH\", \"EAST\", \"WEST\"]\nAction_Map = {1:\"NORTH\", 3:\"WEST\", 5:\"EAST\", 7:\"SOUTH\"}\nOpposite_Action = {1:7, 7:1, 3:5, 5:3}\nenvironment_size = [7,11]\n\n# get position in (row, column) format from index\ndef pos(index):\n    return row_col(index,11)\n\ndef state_from_observation(observation):\n    food_score = 3\n    this_head_score = 1\n    this_body_score = -2\n    this_tail_score = 0\n    other_body_score = -2\n    other_head_score = -4\n    \n    \n    geese = observation[\"geese\"] ##array of geese bodies\n    food = observation[\"food\"] ##array for food\n    index = observation[\"index\"] ##my index\n    this_goose = geese[index] #our controlled goose\n    other_goose = [geese[i] for i in range(len(geese)) if i != index]\n    \n    state = torch.zeros(7,11, dtype=torch.float) ##base state space\n    \n    #Set values for our goose\n    body_length = len(this_goose)\n    for i,body in enumerate(this_goose):\n        state[pos(body)] = this_body_score\n        if i == len(this_goose)-1:\n            state[pos(body)] = this_tail_score\n        if i == 0:\n            state[pos(body)] = this_head_score\n    \n    ##Set values for other geese\n    for goose in other_goose:\n        for i,body in enumerate(goose):\n            state[pos(body)] = other_body_score\n            if i == len(goose)-1:\n                state[pos(body)] = other_body_score\/2\n            if i==0:\n                state[pos(body)] = other_head_score\n                \n    ##Set values for food\n    for f in food:\n        state[pos(f)] = food_score\n    \n    return state\n\n\"\"\"\nclass TorusConv2d(nn.Module):\n    #Input has size (N, channels_in, dim0, dim1)\n    def __init__(self, input_shape, output_shape, kernel_size, bias=False):\n        super().__init__()\n        self.edge_size = (kernel_size[0]\/\/2, kernel_size[1]\/\/2) ##how much we have to pad, eg.: for 3x3 -> we need 1 extra layer\n        self.conv = nn.Conv2d(input_shape, output_shape, kernel_size, padding=0, bias=False)\n    \n    def forward(self, x):\n        h = torch.cat([x[:, :, :, -self.edge_size[1]:], x, x[:, :, :, :self.edge_size[1]]], dim = 3) #adding the extra layer on the columns      \n        h = torch.cat([h[:, :, -self.edge_size[0]:], h, h[:, :, :self.edge_size[0]]], dim=2) #rows\n        h = self.conv(h)\n        return h\n\"\"\"\n\ndef diffuse_state(state, n = 1):\n    #we expect the state to be a tensor of size (7,11)\n    #Pytorch needs format (N, C, Dim0, Dim1), where N=number of samples, C=number of channels (used for 3D state space), Dim0 and Dim1 is our 2D state space dimensions \n    x = state.view(1,1, state.size(0), state.size(1)).detach() #we have to make it dimensions (1, 1, 7, 11)\n    weight = torch.tensor([[0,1,0], [1,9,1], [0,1,0]], dtype=torch.float).view(1, 1, 3, 3).detach() ##custom weight for convolution \n    weight \/= weight.sum() ##for normalisation of kernel\n    conv = nn.Conv2d(1, 1, 3, padding=0, bias=False)\n    conv.weight = nn.Parameter(weight) ##setting the weight of the convolution\n    for i in range(n):\n        h = torch.cat([x[:, :, :, -1:], x, x[:, :, :, :1]], dim = 3) #adding the extra layer on the columns      \n        h = torch.cat([h[:, :, -1:], h, h[:, :, :1]], dim=2) #rows\n        y = conv(h).detach()\n        x = y\n    \n    return y.squeeze() ##we view y \"normally\" again, => size = (7,11)\n\n##finding the local (3,3) box of the state around the point\ndef values_around_point(point, state):\n    points = [(point[0] + i) for i in range(-1, 2)]\n    for i  in range(len(points)):\n        if points[i] > 6:\n            points[i]-=7\n    \n    v = state[points] ##3 rows => current size=(3, 11)\n    \n    points = [(point[1] + i) for i in range(-1, 2)]\n    for i  in range(len(points)):\n        if points[i] > 10:\n            points[i]-=11\n    \n    v = v[:, points] ## 3 columns => size= (3,3)\n    \n    ##we give a large negative for the unreachable local points (could be made different with different sampling of states)\n    large = 1000\n    mask = torch.tensor([[0,1,0], [1,0,1], [0,1,0]], dtype=torch.float) ##reachable points on local \"box\"\n    inv_mask = (mask-torch.ones(3,3))*large ## the non-reachable points\n    v = (v*mask)+inv_mask\n    \n    return v\n\n\nprev_action = -1 #previous action taken (as index)\ndef agent(obs_dict, config_dict):\n    global prev_action\n    state = state_from_observation(obs_dict) #create current state matrix\n    \n    #the number of diffusions is a tuning parameter, same for kernel values\n    diff_state = diffuse_state(state, 5) #diffuse the state n times\n    my_index = obs_dict[\"index\"]\n    my_head = obs_dict[\"geese\"][my_index][0]\n    my_head_pos = pos(my_head)\n    \n    \n    around = values_around_point(my_head_pos, diff_state)  ##get values around our head\n    \n    # Apply Penalty to previous move, so we don't move in reverse\n    #create and apply mask for non-allowed moves eg.: reverse\n    mask_for_unallowed = torch.zeros(9)\n    if prev_action >=0:\n        a = Opposite_Action[prev_action]\n        mask_for_unallowed[a] = -1*1000000\n    mask_for_unallowed = mask_for_unallowed.view(3,3)\n    \n    \n    around = around + mask_for_unallowed ##apply mask\n    \n    #get direction by local reward for directions\n    direction = torch.argmax(around) ##can be 1, 3, 5, 7  ##selects the argmax based on column-major index\n    \n    #save previous action\n    prev_action = direction.item()\n    \n    #take action value and get the appropriate text\n    action = Action_Map[direction.item()]\n    #print(action)\n    \n    return action","6a1f0caa":"%%writefile diffuse_agent_variable.py\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\nActions = [\"NORTH\", \"SOUTH\", \"EAST\", \"WEST\"]\nAction_Map = {1:\"NORTH\", 3:\"WEST\", 5:\"EAST\", 7:\"SOUTH\"}\nOpposite_Action = {1:7, 7:1, 3:5, 5:3}\nenvironment_size = [7,11]\n\n# get position in (row, column) format from index\ndef pos(index):\n    return row_col(index,11)\n\ndef state_from_observation(observation):\n    food_score = 3\n    this_head_score = 1\n    this_body_score = -2\n    this_tail_score = 0\n    other_body_score = -2\n    other_head_score = -5\n    \n    \n    geese = observation[\"geese\"] ##array of geese bodies\n    food = observation[\"food\"] ##array for food\n    index = observation[\"index\"] ##my index\n    this_goose = geese[index] #our controlled goose\n    other_goose = [geese[i] for i in range(len(geese)) if i != index]\n    \n    state = torch.zeros(7,11, dtype=torch.float) ##base state space\n    \n    ## State based score increase\n    other_max_length = np.max([len(g) for g in other_goose])\n    relative_length = len(this_goose) - other_max_length\n    food_score -= max(0, relative_length) * 0.25 ##penalize the food score by 0.25 per length advantage\n    this_tail_score += max(0, relative_length) * 0.25 ##give bonus to tail (so we follow ourselves) => increases survival\n    \n    #Set values for our goose\n    body_length = len(this_goose)\n    for i,body in enumerate(this_goose):\n        state[pos(body)] = this_body_score\n        if i == len(this_goose)-1:\n            state[pos(body)] = this_tail_score\n        if i == 0:\n            state[pos(body)] = this_head_score\n    \n    ##Set values for other geese\n    for goose in other_goose:\n        for i,body in enumerate(goose):\n            state[pos(body)] = other_body_score\n            if i == len(goose)-1:\n                state[pos(body)] = other_body_score\/2\n            if i==0:\n                state[pos(body)] = other_head_score\n                \n    ##Set values for food\n    for f in food:\n        state[pos(f)] = food_score\n    \n    return state\n\n\"\"\"\nclass TorusConv2d(nn.Module):\n    #Input has size (N, channels_in, dim0, dim1)\n    def __init__(self, input_shape, output_shape, kernel_size, bias=False):\n        super().__init__()\n        self.edge_size = (kernel_size[0]\/\/2, kernel_size[1]\/\/2) ##how much we have to pad, eg.: for 3x3 -> we need 1 extra layer\n        self.conv = nn.Conv2d(input_shape, output_shape, kernel_size, padding=0, bias=False)\n    \n    def forward(self, x):\n        h = torch.cat([x[:, :, :, -self.edge_size[1]:], x, x[:, :, :, :self.edge_size[1]]], dim = 3) #adding the extra layer on the columns      \n        h = torch.cat([h[:, :, -self.edge_size[0]:], h, h[:, :, :self.edge_size[0]]], dim=2) #rows\n        h = self.conv(h)\n        return h\n\"\"\"\n\ndef diffuse_state(state, n = 1):\n    #we expect the state to be a tensor of size (7,11)\n    #Pytorch needs format (N, C, Dim0, Dim1), where N=number of samples, C=number of channels (used for 3D state space), Dim0 and Dim1 is our 2D state space dimensions \n    x = state.view(1,1, state.size(0), state.size(1)).detach() #we have to make it dimensions (1, 1, 7, 11)\n    weight = torch.tensor([[0,1,0], [1,9,1], [0,1,0]], dtype=torch.float).view(1, 1, 3, 3).detach() ##custom weight for convolution \n    weight \/= weight.sum() ##for normalisation of kernel\n    conv = nn.Conv2d(1, 1, 3, padding=0, bias=False)\n    conv.weight = nn.Parameter(weight) ##setting the weight of the convolution\n    for i in range(n):\n        h = torch.cat([x[:, :, :, -1:], x, x[:, :, :, :1]], dim = 3) #adding the extra layer on the columns      \n        h = torch.cat([h[:, :, -1:], h, h[:, :, :1]], dim=2) #rows\n        y = conv(h).detach()\n        x = y\n    \n    return y.squeeze() ##we view y \"normally\" again, => size = (7,11)\n\n##finding the local (3,3) box of the state around the point\ndef values_around_point(point, state):\n    points = [(point[0] + i) for i in range(-1, 2)]\n    for i  in range(len(points)):\n        if points[i] > 6:\n            points[i]-=7\n    \n    v = state[points] ##3 rows => current size=(3, 11)\n    \n    points = [(point[1] + i) for i in range(-1, 2)]\n    for i  in range(len(points)):\n        if points[i] > 10:\n            points[i]-=11\n    \n    v = v[:, points] ## 3 columns => size= (3,3)\n    \n    ##we give a large negative for the unreachable local points (could be made different with different sampling of states)\n    large = 1000\n    mask = torch.tensor([[0,1,0], [1,0,1], [0,1,0]], dtype=torch.float) ##reachable points on local \"box\"\n    inv_mask = (mask-torch.ones(3,3))*large ## the non-reachable points\n    v = (v*mask)+inv_mask\n    \n    return v\n\n\nprev_action = -1 #previous action taken (as index)\ndef agent(obs_dict, config_dict):\n    global prev_action\n    state = state_from_observation(obs_dict) #create current state matrix\n    \n    #the number of diffusions is a tuning parameter, same for kernel values\n    diff_state = diffuse_state(state, 5) #diffuse the state n times\n    my_index = obs_dict[\"index\"]\n    my_head = obs_dict[\"geese\"][my_index][0]\n    my_head_pos = pos(my_head)\n    \n    \n    around = values_around_point(my_head_pos, diff_state)  ##get values around our head\n    \n    # Apply Penalty to previous move, so we don't move in reverse\n    #create and apply mask for non-allowed moves eg.: reverse\n    mask_for_unallowed = torch.zeros(9)\n    if prev_action >=0:\n        a = Opposite_Action[prev_action]\n        mask_for_unallowed[a] = -1*1000000\n    mask_for_unallowed = mask_for_unallowed.view(3,3)\n    \n    \n    around = around + mask_for_unallowed ##apply mask\n    \n    #get direction by local reward for directions\n    direction = torch.argmax(around) ##can be 1, 3, 5, 7  ##selects the argmax based on column-major index\n    \n    #save previous action\n    prev_action = direction.item()\n    \n    #take action value and get the appropriate text\n    action = Action_Map[direction.item()]\n    #print(action)\n    \n    return action","ae077f20":"from kaggle_environments import make, evaluate\nenv = make(\"hungry_geese\", debug=True)\n\nobservations = env.run([\"diffuse_agent_variable.py\",\"diffuse_agent_static.py\", \"greedy\", \"greedy\"])\nenv.render(mode=\"ipython\", width=700, height=600)\n\nfig, ax = plt.subplots(1, 2)\nprint(observations[1][0][\"observation\"])\ns = state_from_observation(observations[1][0][\"observation\"])\nshow = ax[0].imshow(s, cmap = \"inferno\")\ndiff_s = diffuse_state(s, 5)\nax[1].imshow(diff_s, cmap = \"inferno\")\ncbar = fig.colorbar(show, ax=ax)","4927229e":"from kaggle_environments import make, evaluate\nenv = make(\"hungry_geese\", debug=True)\n\nenv.run([\"diffuse_agent_variable.py\",\"diffuse_agent_static.py\", \"greedy\", \"greedy\"])\n\nenv.render(mode=\"ipython\", width=700, height=600)","a681bdfd":"def setup_env(debug=False):\n    env = make(\n        \"hungry_geese\", \n        configuration={\n            \"episodeSteps\": 200,\n            \"actTimeout\": 1,\n        },\n        debug=debug\n    )\n    return env","fa18bf32":"def run_league(env, teams, nb_iter, debug=True):\n    # Run simulations\n    if debug:\n        team_names = [teams[0].split(\"\/\")[-1], teams[1].split(\"\/\")[-1], teams[2].split(\"\/\")[-1], teams[3].split(\"\/\")[-1]]\n        \n    current_score = evaluate(\n            \"hungry_geese\", \n            [\n                teams[0], \n                teams[1], \n                teams[2], \n                teams[3], \n            ],\n            num_episodes=nb_iter,\n        )\n    \n    # Retrieve results\n    episode_winners = np.argmax(current_score, axis=1)\n    print(episode_winners)\n    episode_winner_counts = collections.Counter(episode_winners)\n    print(episode_winner_counts)\n    if debug:\n        for i in range(4):\n            print(\"TEAM\", i, team_names[i], \"won\", episode_winner_counts.get(i, 0), \"times\")\n    \n    return env, episode_winner_counts","228e9d30":"LEAGUE_TEAMS = [\n    \"diffuse_agent_static.py\",\n    \"diffuse_agent_variable.py\",\n    \"greedy\",\n    \"greedy\"\n]","fc490ea0":"from time import time\nimport collections\n# League\nNB_ITER = 100 # number of times teams play each other\n\n# Set up the Environment.\nenv = setup_env(debug=True)\n\n# Run league\nprint(\"Start league...\")\nstart = time()\nenv, counts = run_league(env, LEAGUE_TEAMS, NB_ITER)\nprint(\"Runtime :\", np.round(time() - start, 2), \"seconds\")","ccf9fb12":"class Diffuse_Agent():\n    def __init__(self, food = 3, head = 1, body = -2, tail = 0, other_body = -2, other_head = -5, food_mult = 1, tail_mult = 1):\n        self.food_score = food\n        self.this_head_score = head\n        self.this_body_score = body\n        self.this_tail_score = tail\n        self.other_body_score = other_body\n        self.other_head_score = other_head\n        \n        self.food_multiplier = food_mult\n        self.tail_multiplier = tail_mult\n    \n        self.Actions = [\"NORTH\", \"SOUTH\", \"EAST\", \"WEST\"]\n        self.Action_Map = {1:\"NORTH\", 3:\"WEST\", 5:\"EAST\", 7:\"SOUTH\"}\n        self.Opposite_Action = {1:7, 7:1, 3:5, 5:3}\n        self.environment_size = [7,11]\n        \n        self.prev_action = -1 #previous action taken (as index)\n        return\n        \n    # get position in (row, column) format from index\n    def pos(index):\n        return row_col(index,11)\n\n    def state_from_observation(self, observation):\n        food_score = self.food_score\n        this_head_score = self.this_head_score\n        this_body_score = self.this_body_score\n        this_tail_score = self.this_tail_score\n        other_body_score = self.other_body_score\n        other_head_score = self.other_head_score\n        \n        geese = observation[\"geese\"] ##array of geese bodies\n        food = observation[\"food\"] ##array for food\n        index = observation[\"index\"] ##my index\n        this_goose = geese[index] #our controlled goose\n        other_goose = [geese[i] for i in range(len(geese)) if i != index]\n\n        state = torch.zeros(7,11, dtype=torch.float) ##base state space\n\n        ## State based score increase\n        other_max_length = np.max([len(g) for g in other_goose])\n        relative_length = len(this_goose) - other_max_length\n        food_score -= max(0, relative_length) * self.food_multiplier ##penalize the food score by 0.25 per length advantage\n        this_tail_score += max(0, relative_length) * self.tail_multiplier ##give bonus to tail (so we follow ourselves) => increases survival\n\n        #Set values for our goose\n        body_length = len(this_goose)\n        for i,body in enumerate(this_goose):\n            state[pos(body)] = this_body_score\n            if i == len(this_goose)-1:\n                state[pos(body)] = this_tail_score\n            if i == 0:\n                state[pos(body)] = this_head_score\n\n        ##Set values for other geese\n        for goose in other_goose:\n            for i,body in enumerate(goose):\n                state[pos(body)] = other_body_score\n                if i == len(goose)-1:\n                    state[pos(body)] = other_body_score\/2\n                if i==0:\n                    state[pos(body)] = other_head_score\n\n        ##Set values for food\n        for f in food:\n            state[pos(f)] = food_score\n\n        return state\n    \n    def diffuse_state(self, state, n = 1):\n        #we expect the state to be a tensor of size (7,11)\n        #Pytorch needs format (N, C, Dim0, Dim1), where N=number of samples, C=number of channels (used for 3D state space), Dim0 and Dim1 is our 2D state space dimensions \n        x = state.view(1,1, state.size(0), state.size(1)).detach() #we have to make it dimensions (1, 1, 7, 11)\n        weight = torch.tensor([[0,1,0], [1,9,1], [0,1,0]], dtype=torch.float).view(1, 1, 3, 3).detach() ##custom weight for convolution \n        weight \/= weight.sum() ##for normalisation of kernel\n        conv = nn.Conv2d(1, 1, 3, padding=0, bias=False)\n        conv.weight = nn.Parameter(weight) ##setting the weight of the convolution\n        for i in range(n):\n            h = torch.cat([x[:, :, :, -1:], x, x[:, :, :, :1]], dim = 3) #adding the extra layer on the columns      \n            h = torch.cat([h[:, :, -1:], h, h[:, :, :1]], dim=2) #rows\n            y = conv(h).detach()\n            x = y\n\n        return y.squeeze() ##we view y \"normally\" again, => size = (7,11)\n\n    ##finding the local (3,3) box of the state around the point\n    def values_around_point(self, point, state):\n        points = [(point[0] + i) for i in range(-1, 2)]\n        for i  in range(len(points)):\n            if points[i] > 6:\n                points[i]-=7\n\n        v = state[points] ##3 rows => current size=(3, 11)\n\n        points = [(point[1] + i) for i in range(-1, 2)]\n        for i  in range(len(points)):\n            if points[i] > 10:\n                points[i]-=11\n\n        v = v[:, points] ## 3 columns => size= (3,3)\n\n        ##we give a large negative for the unreachable local points (could be made different with different sampling of states)\n        large = 1000\n        mask = torch.tensor([[0,1,0], [1,0,1], [0,1,0]], dtype=torch.float) ##reachable points on local \"box\"\n        inv_mask = (mask-torch.ones(3,3))*large ## the non-reachable points\n        v = (v*mask)+inv_mask\n\n        return v\n    \n    def evaluate(self, obs):\n        state = self.state_from_observation(obs) #create current state matrix\n\n        #the number of diffusions is a tuning parameter, same for kernel values\n        diff_state = diffuse_state(state, 5) #diffuse the state n times\n        my_index = obs[\"index\"]\n        my_head = obs[\"geese\"][my_index][0]\n        my_head_pos = pos(my_head)\n\n\n        around = self.values_around_point(my_head_pos, diff_state)  ##get values around our head\n\n        # Apply Penalty to previous move, so we don't move in reverse\n        #create and apply mask for non-allowed moves eg.: reverse\n        mask_for_unallowed = torch.zeros(9)\n        if self.prev_action >=0:\n            a = self.Opposite_Action[self.prev_action]\n            mask_for_unallowed[a] = -1*1000000\n        mask_for_unallowed = mask_for_unallowed.view(3,3)\n\n\n        around = around + mask_for_unallowed ##apply mask\n\n        #get direction by local reward for directions\n        direction = torch.argmax(around) ##can be 1, 3, 5, 7  ##selects the argmax based on column-major index\n\n        #save previous action\n        self.prev_action = direction.item()\n\n        #take action value and get the appropriate text\n        action = self.Action_Map[direction.item()]\n        #print(action)\n\n        return action","5bb50d9c":"diffuse_agent = Diffuse_Agent()\ndef my_agent(obs_dict, config_dict):\n    return diffuse_agent.evaluate(obs_dict)","a7b6d947":"from time import time\nimport collections\n\n# League\nNB_ITER = 10 # number of times teams play each other\nteams = [my_agent, \"diffuse_agent_variable.py\", \"diffuse_agent_static.py\", \"greedy\"]\ncurrent_score = evaluate(\n        \"hungry_geese\", \n        [\n            teams[0], \n            teams[1], \n            teams[2], \n            teams[3], \n        ],\n        num_episodes=NB_ITER,\n    )\n# Retrieve results\nprint(current_score)\nepisode_winners = np.argmax(current_score, axis=1)\nepisode_winner_counts = collections.Counter(episode_winners)\nprint(episode_winner_counts)\nprint(episode_winner_counts.get(0, 0))","77366a29":"import optuna\nimport collections\n\ndef objective(trial):\n    food_score = trial.suggest_uniform(\"food score\", 0, 100)\n    this_head_score = trial.suggest_uniform(\"head score\", -25, 100)\n    this_body_score = trial.suggest_uniform(\"body score\", -100, 0)\n    this_tail_score = trial.suggest_uniform(\"tail score\", -100, 100)\n    other_body_score = trial.suggest_uniform(\"other body score\", -100, 0)\n    other_head_score = trial.suggest_uniform(\"other head score\", -100, 0)\n\n    food_multiplier = trial.suggest_uniform(\"food multiplier\", -10, 10)\n    tail_multiplier = trial.suggest_uniform(\"tail multiplier\", -10, 10)\n    \n    diffuse_agent = Diffuse_Agent(food_score, this_head_score, this_body_score, this_tail_score, \n                                  other_body_score, other_head_score, \n                                  food_multiplier, tail_multiplier)\n    \n    def new_agent(obs_dict, config_dict):\n        return diffuse_agent.evaluate(obs_dict)\n    \n    \n    \n    # League\n    NB_ITER = 100 # number of times teams play each other\n    teams = [new_agent, \"diffuse_agent_variable.py\", \"diffuse_agent_static.py\", \"greedy\"]\n    current_score = evaluate(\n            \"hungry_geese\", \n            [\n                teams[0], \n                teams[1], \n                teams[2], \n                teams[3], \n            ],\n            num_episodes=NB_ITER,\n        )\n    # Retrieve results\n    episode_winners = np.argmax(current_score, axis=1)\n    episode_winner_counts = collections.Counter(episode_winners)\n    return episode_winner_counts.get(0, 0)","f1120f14":"study = optuna.create_study(direction = \"maximize\")\nstudy.optimize(objective, n_trials = 100)","67045cf2":"## State dependent diffusive agent\n\nsimilar to previous, just gave a penalty and bonus to the food and tail score respectively depending on relative size of the agent","0ba78bda":"# The IDEA:","f1e6453c":"# Running Environment","08c51ac3":"## Diffusion Code","b2bf855d":"**The idea**: Create a state matrix from the current board and try to score certain points which we deem useful.\nThen to fill the empty matrix, we apply a simple convolution to diffuse the \"scores\" \/ rewards. This should result in the agent being able to \"plan ahead\" even with simply choosing the move that's the largest around its head.","722110eb":"1. We can create a simple score value for certain points on the board that are constants (independent of the board)\n2. We can create a bit more complex state dependent scoring (eg.: if we're the longest decrease score of food)\n\n\n\n\nTo find an slighly more optimal agent we could try to find\/tune the hyperparameters(scores) for each score value, and to what we want to score.\n\nThe score could also include different ways of changing it based on the state","51fa7833":"# League","3c3e6d9a":"# Hyperparameter tuning with Optuna","6b288cf0":"# Agents","a967b037":"### State independent diffusive agent","72d20699":"We can see that the empy scells are red, the bright white spots are food, and the yellow spot is our head, while the black spots are the enemy head","d6b2f054":"## Create Diffuse Agent Class","5fa13b80":"# Examples"}}