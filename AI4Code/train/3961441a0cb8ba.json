{"cell_type":{"ffb0633e":"code","791a10de":"code","708b5e48":"code","d2c5a247":"code","40ead224":"code","6ee4bcb3":"code","92712ac1":"code","64219683":"code","4722c516":"code","6340f675":"code","123e6f42":"code","6d6f80de":"markdown"},"source":{"ffb0633e":"!pip install lofo-importance","791a10de":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import *\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import *\nfrom sklearn.svm import SVR, LinearSVR, NuSVR\nfrom lofo import LOFOImportance, Dataset, plot_importance\n\nloading_df = pd.read_csv(\"..\/input\/trends-assessment-prediction\/loading.csv\")\nfnc_df = pd.read_csv(\"..\/input\/trends-assessment-prediction\/fnc.csv\")\n\nfnc_features, loading_features = list(fnc_df.columns[1:]), list(loading_df.columns[1:])\ndf = fnc_df.merge(loading_df, on=\"Id\")\n\n\nlabels_df = pd.read_csv(\"..\/input\/trends-assessment-prediction\/train_scores.csv\")\nlabels_df[\"is_train\"] = True\n\ndf = df.merge(labels_df, on=\"Id\", how=\"left\")\n\ntest_df = df[df[\"is_train\"] != True].copy()\ndf = df[df[\"is_train\"] == True].copy()\n\ndf.shape, test_df.shape","708b5e48":"def pca_trans(train, test):\n    pca = PCA().fit(train)\n    plt.figure(figsize=(10, 7))\n    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n    plt.xlabel('number of components')\n    plt.ylabel('cumulative explained variance')\n    plt.show()\n    \n    ss = StandardScaler()\n    train= ss.fit_transform(train)\n    test= ss.fit_transform(test)\n    \n    pca = PCA(n_components=350)\n    train = pca.fit_transform(train)\n    \n    test = pca.fit_transform(test)\n\n    train = pd.DataFrame(train)\n    test = pd.DataFrame(test)\n    print(train.shape, test.shape)\n    return train, test\n\ndef scale_data(train, test, convertor=StandardScaler()):\n    ss = convertor\n    train= ss.fit_transform(train)\n    test= ss.fit_transform(test)\n    print(train.shape, test.shape)\n    print(convertor)\n    train = pd.DataFrame(train)\n    test = pd.DataFrame(test)\n    return train, test","d2c5a247":"#df.isnull().sum()","40ead224":"\n#df , test_df = scale_data(df, test_df)","6ee4bcb3":"\n#from sklearn.isotonic import IsotonicRegression\n\ndef get_lofo_importance(target):\n    cv = KFold(n_splits=5, shuffle=True, random_state=0)\n\n    dataset = Dataset(df=df[df[target].notnull()], target=target, features=loading_features, \n                      feature_groups={\"fnc\": df[df[target].notnull()][fnc_features].values\/500})\n\n    model = NuSVR(nu=0.5, max_iter=-1,kernel='linear',  C=0.1, verbose=True)\n    lofo_imp = LOFOImportance(dataset, cv=cv, scoring=\"neg_mean_absolute_error\", model=model)\n\n    return lofo_imp.get_importance()","92712ac1":"#plot_importance(get_lofo_importance(target=\"age\"), figsize=(16, 8))","64219683":"#plot_importance(get_lofo_importance(target=\"domain1_var1\"), figsize=(16, 8))","4722c516":"#plot_importance(get_lofo_importance(target=\"domain1_var2\"), figsize=(16, 8))","6340f675":"plot_importance(get_lofo_importance(target=\"domain2_var1\"), figsize=(16, 8))","123e6f42":"plot_importance(get_lofo_importance(target=\"domain2_var2\"), figsize=(16, 8))","6d6f80de":"## [Leave One Feature Out Importance](https:\/\/github.com\/aerdem4\/lofo-importance)\n\nIt is difficult to calculate the feature importances with traditional ways in such high dimensional data. Thanks to LOFO, we can group the features and get one importance value for the whole group. In this notebook, while each loading feature is considered as separate feature, fnc features are considered as one group. Then we calculate the feature importances for each target using a ridge regression model within cross-validation.\n"}}