{"cell_type":{"52b6a252":"code","16b6e07a":"code","19b7049d":"code","8c110879":"code","22b4b77d":"code","fd9ec8ab":"code","511f9304":"code","e80eba2c":"code","3f2fe79f":"code","0b5a84bf":"code","099c3495":"code","a533bfe4":"code","1cbe7e88":"code","b5e0055b":"code","9716f8d3":"code","7e782f16":"code","d30d80d1":"code","016aaf36":"code","35d2659a":"code","e0dfff73":"code","c8e2108c":"code","cf59bf9e":"markdown","f77e0662":"markdown","34de04a6":"markdown"},"source":{"52b6a252":"!pip install ohmeow-blurr==0.0.22 datasets==1.3.0 fsspec==0.8.5 -qq","16b6e07a":"# turn off multithreading to avoid deadlock\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","19b7049d":"from transformers import *\nfrom fastai.text.all import *\n\nfrom blurr.data.all import *\nfrom blurr.modeling.all import *\n\nSEED = 42\nset_seed(SEED, True)","8c110879":"import json\n\nwith open('..\/input\/sclds2021preprocess\/wordlist.json', 'r') as f:\n    wordlist = json.load(f)","22b4b77d":"import ast\ndf_converters = {'tokens': ast.literal_eval, 'labels': ast.literal_eval}\n\nvalid_df = pd.read_csv('..\/input\/sclds2021preprocess\/valid.csv', converters=df_converters)","fd9ec8ab":"len(valid_df)","511f9304":"model_path = Path('..\/input\/sclds2021train')","e80eba2c":"# Re-define certain things for 'load_learner' to work\n\n@delegates()\nclass TokenCrossEntropyLossFlat(BaseLoss):\n    \"Same as `CrossEntropyLossFlat`, but for mutiple tokens output\"\n    y_int = True\n    @use_kwargs_dict(keep=True, weight=None, ignore_index=-100, reduction='mean')\n    def __init__(self, *args, axis=-1, **kwargs): super().__init__(nn.CrossEntropyLoss, *args, axis=axis, **kwargs)\n    def decodes(self, x):    return L([ i.argmax(dim=self.axis) for i in x ])\n    def activation(self, x): return L([ F.softmax(i, dim=self.axis) for i in x ])\n\ndef get_y(inp): return [(label, len(hf_tokenizer.tokenize(str(entity)))) for entity, label in zip(inp.tokens, inp.labels)]","3f2fe79f":"@patch\ndef blurr_predict(self:Learner, items, rm_type_tfms=None):\n    hf_before_batch_tfm = get_blurr_tfm(self.dls.before_batch)\n    is_split_str = hf_before_batch_tfm.is_split_into_words and isinstance(items[0], str)\n    is_df = isinstance(items, pd.DataFrame)\n    if (not is_df and (is_split_str or not is_listy(items))): items = [items]\n    dl = self.dls.test_dl(items, rm_type_tfms=rm_type_tfms, num_workers=0)\n    with self.no_bar(): probs, _, decoded_preds = self.get_preds(dl=dl, with_input=False, with_decoded=True)\n    trg_tfms = self.dls.tfms[self.dls.n_inp:]\n    outs = []\n    probs, decoded_preds = L(probs), L(decoded_preds)\n    for i in range(len(items)):\n        item_probs = [probs[i]]\n        item_dec_preds = [decoded_preds[i]]\n        item_dec_labels = tuplify([tfm.decode(item_dec_preds[tfm_idx]) for tfm_idx, tfm in enumerate(trg_tfms)])\n        outs.append((item_dec_labels, item_dec_preds, item_probs))\n    return outs","0b5a84bf":"from string import punctuation\n\ndef reconstruct(num, pred, raw_tokens, raw_address):\n    def complete_word(x):\n        y = x.strip().strip(punctuation)\n        if y != '' and y in wordlist:\n            x = x.replace(y, wordlist[y])\n        return x\n    \n    def normalize_bracket(x):\n        if '(' in x and ')' not in x:\n            x = x + ')'\n        elif ')' in x and '(' not in x:\n            x = '(' + x\n        return x\n    \n    ans = ['\/'] * num\n    for idx in range(num):\n        res = pred[idx]\n        start_poi, end_poi = -1, -1\n        start_str, end_str = -1, -1\n        for i in range(len(res[0])):\n            if 'POI' in res[1][i]:\n                if start_poi == -1: start_poi = i\n                end_poi = i\n            if 'STR' in res[1][i]:\n                if start_str == -1: start_str = i\n                end_str = i\n        \n        if start_poi != -1:\n            txt1 = raw_address[idx]\n            for i in range(start_poi):\n                txt1 = txt1[len(raw_tokens[idx][i]):].strip()\n            for i in range(len(raw_tokens[idx]) - 1, end_poi, -1):\n                txt1 = txt1[:-len(raw_tokens[idx][i])].strip()\n            \n            txt1_check = ''.join(raw_tokens[idx][start_poi:end_poi + 1]).replace(' ', '')\n            assert txt1.replace(' ', '') == txt1_check\n            \n            last = len(txt1)\n            for i in range(end_poi, start_poi - 1, -1):\n                while last > 0 and txt1[last - 1] == ' ':\n                    last -= 1\n                assert last >= len(raw_tokens[idx][i])\n                last -= len(raw_tokens[idx][i])\n                if 'SHORT' in res[1][i]:\n                    txt1 = txt1[:last] + complete_word(raw_tokens[idx][i]) + txt1[last + len(raw_tokens[idx][i]):]\n        else:\n            txt1 = ''\n        \n        if start_str != -1:\n            txt2 = raw_address[idx]\n            for i in range(start_str):\n                txt2 = txt2[len(raw_tokens[idx][i]):].strip()\n            for i in range(len(raw_tokens[idx]) - 1, end_str, -1):\n                txt2 = txt2[:-len(raw_tokens[idx][i])].strip()\n            \n            txt2_check = ''.join(raw_tokens[idx][start_str:end_str + 1]).replace(' ', '')\n            assert txt2.replace(' ', '') == txt2_check\n            \n            last = len(txt2)\n            for i in range(end_str, start_str - 1, -1):\n                while last > 0 and txt2[last - 1] == ' ':\n                    last -= 1\n                assert last >= len(raw_tokens[idx][i])\n                last -= len(raw_tokens[idx][i])\n                if 'SHORT' in res[1][i]:\n                    txt2 = txt2[:last] + complete_word(raw_tokens[idx][i]) + txt2[last + len(raw_tokens[idx][i]):]\n        else:\n            txt2 = ''\n        \n        txt1 = txt1.strip(punctuation)\n        txt2 = txt2.strip(punctuation)\n        txt1 = normalize_bracket(txt1)\n        txt2 = normalize_bracket(txt2)\n        \n        ans[idx] = (txt1 + '\/' + txt2)\n    \n    return ans\n\ndef reconstruct_ensemble(num, pred, raw_tokens, raw_address):\n    def complete_word(x):\n        y = x.strip().strip(punctuation)\n        if y != '' and y in wordlist:\n            x = x.replace(y, wordlist[y])\n        return x\n    \n    def normalize_bracket(x):\n        if '(' in x and ')' not in x:\n            x = x + ')'\n        elif ')' in x and '(' not in x:\n            x = '(' + x\n        return x\n    \n    ans = ['\/'] * num\n    for idx in range(num):\n        res = pred[idx]\n        start_poi, end_poi = -1, -1\n        start_str, end_str = -1, -1\n        for i in range(len(res)):\n            if 'POI' in res[i]:\n                if start_poi == -1: start_poi = i\n                end_poi = i\n            if 'STR' in res[i]:\n                if start_str == -1: start_str = i\n                end_str = i\n        \n        if start_poi != -1:\n            txt1 = raw_address[idx]\n            for i in range(start_poi):\n                txt1 = txt1[len(raw_tokens[idx][i]):].strip()\n            for i in range(len(raw_tokens[idx]) - 1, end_poi, -1):\n                txt1 = txt1[:-len(raw_tokens[idx][i])].strip()\n            \n            txt1_check = ''.join(raw_tokens[idx][start_poi:end_poi + 1]).replace(' ', '')\n            assert txt1.replace(' ', '') == txt1_check\n            \n            last = len(txt1)\n            for i in range(end_poi, start_poi - 1, -1):\n                while last > 0 and txt1[last - 1] == ' ':\n                    last -= 1\n                assert last >= len(raw_tokens[idx][i])\n                last -= len(raw_tokens[idx][i])\n                if 'SHORT' in res[i]:\n                    txt1 = txt1[:last] + complete_word(raw_tokens[idx][i]) + txt1[last + len(raw_tokens[idx][i]):]\n        else:\n            txt1 = ''\n        \n        if start_str != -1:\n            txt2 = raw_address[idx]\n            for i in range(start_str):\n                txt2 = txt2[len(raw_tokens[idx][i]):].strip()\n            for i in range(len(raw_tokens[idx]) - 1, end_str, -1):\n                txt2 = txt2[:-len(raw_tokens[idx][i])].strip()\n            \n            txt2_check = ''.join(raw_tokens[idx][start_str:end_str + 1]).replace(' ', '')\n            assert txt2.replace(' ', '') == txt2_check\n            \n            last = len(txt2)\n            for i in range(end_str, start_str - 1, -1):\n                while last > 0 and txt2[last - 1] == ' ':\n                    last -= 1\n                assert last >= len(raw_tokens[idx][i])\n                last -= len(raw_tokens[idx][i])\n                if 'SHORT' in res[i]:\n                    txt2 = txt2[:last] + complete_word(raw_tokens[idx][i]) + txt2[last + len(raw_tokens[idx][i]):]\n        else:\n            txt2 = ''\n        \n        txt1 = txt1.strip(punctuation)\n        txt2 = txt2.strip(punctuation)\n        txt1 = normalize_bracket(txt1)\n        txt2 = normalize_bracket(txt2)\n        \n        ans[idx] = (txt1 + '\/' + txt2)\n    \n    return ans","099c3495":"def calc_acc(df):\n    return df.loc[valid_df['pred'] == df['POI\/street'], 'id'].count() \/ len(df)","a533bfe4":"raw_tokens = list(valid_df['tokens'])\nraw_address = list(valid_df['raw_address'])","1cbe7e88":"raw_avg_pred = []\n\nfor model in model_path.ls():\n    learn = load_learner(model)\n    raw_pred = learn.blurr_predict_tokens(raw_tokens)\n    raw_avg_pred.append([raw[3] for raw in raw_pred])\n    pred = reconstruct(len(valid_df), raw_pred, raw_tokens, raw_address)\n    valid_df['pred'] = pred\n    score = calc_acc(valid_df)\n    print(f'{model.name} - {score:.5f}')","b5e0055b":"raw_ensemble_pred = [(sum(col))\/len(col) for col in zip(*raw_avg_pred)]\nraw_ensemble_pred = [pred.argmax(-1) for pred in raw_ensemble_pred]\nraw_ensemble_pred = learn.dls.vocab.map_ids(raw_ensemble_pred)\npred = reconstruct_ensemble(len(valid_df), raw_ensemble_pred, raw_tokens, raw_address)\nvalid_df['pred'] = pred\nscore = calc_acc(valid_df)\nprint(f'Ensemble - {score:.5f}')","9716f8d3":"import re\n\ndef clean(s):\n    res = re.sub(r'(\\w)(\\()(\\w)', '\\g<1> \\g<2>\\g<3>', s)\n    res = re.sub(r'(\\w)([),.:;]+)(\\w)', '\\g<1>\\g<2> \\g<3>', res)\n    res = re.sub(r'(\\w)(\\.\\()(\\w)', '\\g<1>. (\\g<3>', res)\n    res = re.sub(r'\\s+', ' ', res)\n    res = res.strip()\n    return res","7e782f16":"test_df = pd.read_csv('..\/input\/scl-2021-ds\/test.csv')\ntest_df['raw_address'] = test_df['raw_address'].apply(lambda x: x.strip())\ntest_df['tokens'] = test_df['raw_address'].apply(clean).str.split()\ntest_df.head()","d30d80d1":"raw_tokens = list(test_df['tokens'])\nraw_address = list(test_df['raw_address'])","016aaf36":"raw_avg_pred = []\n\nfor model in model_path.ls():\n    learn = load_learner(model)\n    raw_pred = learn.blurr_predict_tokens(raw_tokens)\n    raw_avg_pred.append([raw[3] for raw in raw_pred])","35d2659a":"raw_ensemble_pred = [(sum(col))\/len(col) for col in zip(*raw_avg_pred)]\nraw_ensemble_pred = [pred.argmax(-1) for pred in raw_ensemble_pred]\nraw_ensemble_pred = learn.dls.vocab.map_ids(raw_ensemble_pred)\npred = reconstruct_ensemble(len(test_df), raw_ensemble_pred, raw_tokens, raw_address)\ntest_df['POI\/street'] = pred","e0dfff73":"test_df.drop(columns=['raw_address', 'tokens'], inplace=True)\ntest_df.head()","c8e2108c":"test_df.to_csv('submission.csv', index=False)","cf59bf9e":"# Submission","f77e0662":"# Evaluation\n- This is only relevant during model selection and testing\n- For the final training, full dataset is used so the accuracy below doesn't really reflect the power of the model.","34de04a6":"# Training\n- Make inference using our finetuned models\n- Our 1st place solution used **ensembling of many models** by taking the **average of the prediction probabilities for each word** and **the entire dataset was used for training with no validation**\n\n## Steps:\n1. [Preprocessing](https:\/\/www.kaggle.com\/nguyncaoduy\/1-place-scl-ds-2021-voidandtwotsts-preprocess)\n2. [Training](https:\/\/www.kaggle.com\/nguyncaoduy\/1-place-scl-ds-2021-voidandtwotsts-train)\n3. [Ensembling](https:\/\/www.kaggle.com\/nguyncaoduy\/1-place-scl-ds-2021-voidandtwotsts-ensemble) - This Notebook"}}