{"cell_type":{"8f99bb68":"code","5c1195c5":"code","cdc62f3e":"code","739f44e6":"code","d05f2b2d":"code","980bd7e1":"code","a76943ee":"code","ac5b4608":"code","be31fabd":"code","06466350":"code","92f1321d":"code","1327821d":"code","7323ad0d":"code","fbebc909":"code","fb454689":"code","e94ca5ee":"code","3d151257":"code","0672351c":"code","9f2d8a98":"markdown","498d17c1":"markdown","95467a7d":"markdown","3bad4a54":"markdown"},"source":{"8f99bb68":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5c1195c5":"tokenizer=tf.keras.preprocessing.text.Tokenizer(oov_token=\"<OOV>\")","cdc62f3e":"files=os.listdir('..\/input\/bed-time-stories\/')\nfiles","739f44e6":"data=[]\nfor i in files:\n    file=open('..\/input\/bed-time-stories\/'+i,'r')\n    text=file.read().lower().split('\\n')\n    for lines in text:\n        data.append(lines)","d05f2b2d":"while('' in data):\n    data.remove('')\nwhile(' ' in data):\n    data.remove(' ')\ntokenizer.fit_on_texts(data)\ntokenizer","980bd7e1":"total_words=len(tokenizer.word_index)+1\ntotal_words","a76943ee":"word_index=tokenizer.word_index\nprint(word_index)","ac5b4608":"input_sequences=[]\nfor line in data:\n    token_list=tokenizer.texts_to_sequences([line])[0]\n    for i in range(1,len(token_list)):\n        n_gram_sequences=token_list[:i+1]\n        input_sequences.append(n_gram_sequences)","be31fabd":"max_len=max([len(x) for x in input_sequences])\nprint(max_len)","06466350":"input_sequences","92f1321d":"padded=tf.keras.preprocessing.sequence.pad_sequences(input_sequences,maxlen=max_len,padding='pre')\npadded","1327821d":"input_sequences=np.array(padded)\ninput_sequences","7323ad0d":"train=input_sequences[:,:-1]\nlabels=input_sequences[:,-1]\nlabels","fbebc909":"label_encoding=tf.keras.utils.to_categorical(labels,num_classes=total_words)\nlabel_encoding","fb454689":"model=tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Embedding(total_words, 100, input_length=max_len-1))\nmodel.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150, return_sequences = True)))\nmodel.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.LSTM(100))\nmodel.add(tf.keras.layers.Dense(total_words\/2, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\nmodel.add(tf.keras.layers.Dense(total_words, activation='softmax'))","e94ca5ee":"print(label_encoding.shape)\nprint(train.shape)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","3d151257":"history=model.fit(train,label_encoding,epochs=100)","0672351c":"seed_text = input('Enter a line related to story : ').lower()\nnext_words = int(input('Enter the number of words you need in the story : '))\n\nfor _ in range(next_words):\n    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n    token_list = tf.keras.preprocessing.sequence.pad_sequences([token_list], maxlen=max_len-1, padding='pre')\n    predicted = model.predict_classes(token_list, verbose=0)\n    output_word = \"\"\n    for word, index in tokenizer.word_index.items():\n        if index == predicted:\n            output_word = word\n            break\n    seed_text += ' '+output_word\nprint(seed_text)","9f2d8a98":"# Data Encoding","498d17c1":"# Model Architecture\nThe model consists of two LSTM layers and two dense layers.\n![LSTM](https:\/\/qph.fs.quoracdn.net\/main-qimg-a619fdcafcd6dbb2390d2d4c050e205a.webp)","95467a7d":"removing unwanted data and fit the other texts on the tokenizer","3bad4a54":"word index for each words in the tokenizer"}}