{"cell_type":{"280128a8":"code","e8ca3c15":"code","17724950":"code","bc0effd7":"code","8bcb735a":"code","4794ba65":"code","e48e38b9":"code","73058afe":"code","038ea503":"code","69d198b9":"code","d9c9e855":"code","df0ad19f":"code","64b76738":"code","3fff4bfb":"markdown","9f58bd1a":"markdown","85831c17":"markdown","dd7bfc44":"markdown","6f9843b7":"markdown","5b09d8be":"markdown","1a2f1003":"markdown","c06d675a":"markdown","5fca17fd":"markdown","80aa249d":"markdown"},"source":{"280128a8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e8ca3c15":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\ndf1=pd.read_csv(\"\/kaggle\/input\/real-or-fake-fake-jobposting-prediction\/fake_job_postings.csv\",index_col=\"job_id\")\ndf1[\"location\"]=df1[\"location\"].fillna(\"LOC\")\ndf1[\"department\"]=df1[\"department\"].fillna(\"DEPART\")\ndf1[\"salary_range\"]=df1[\"salary_range\"].fillna(\"0-0\")\ndf1[\"company_profile\"]=df1[\"company_profile\"].fillna(\"No Description\")\ndf1[\"description\"]=df1[\"description\"].fillna(\"No Description\")\ndf1[\"requirements\"]=df1[\"requirements\"].fillna(\"No Description\")\ndf1[\"benefits\"]=df1[\"benefits\"].fillna(\"Adequete benefits\")\ndf1[\"employment_type\"]=df1[\"employment_type\"].fillna(\"Other\")\ndf1[\"required_experience\"]=df1[\"required_experience\"].fillna(\"Not Applicable\")\ndf1[\"required_education\"]=df1[\"required_education\"].fillna(\"Bachelor's Degree\")\ndf1[\"industry\"]=df1[\"industry\"].fillna(\"None\")\ndf1[\"function\"]=df1[\"function\"].fillna(\"None\")\ndf1.head()\n# df1.industry.value_counts()","17724950":"df1.info()","bc0effd7":"from gensim.models import Doc2Vec\nmodel=Doc2Vec.load(\"\/kaggle\/input\/doc2vec-english-binary-file\/doc2vec.bin\")","8bcb735a":"#=========[CLEAN DATA]==============#\n# !pip install textcleaner==0.4.26\nimport string\n\n#=========[CLEAN PANDAS]==============#\n# employment_type\trequired_experience\trequired_education\tindustry\tfunction\nfrom sklearn.preprocessing import LabelEncoder\ndf1[\"location\"]=LabelEncoder().fit_transform(df1[\"location\"])\ndf1[\"department\"]=LabelEncoder().fit_transform(df1[\"department\"])\ndf1[\"salary_range\"]=LabelEncoder().fit_transform(df1[\"salary_range\"])\ndf1[\"employment_type\"]=LabelEncoder().fit_transform(df1[\"salary_range\"])\ndf1[\"required_experience\"]=LabelEncoder().fit_transform(df1[\"salary_range\"])\ndf1[\"required_education\"]=LabelEncoder().fit_transform(df1[\"salary_range\"])\ndf1[\"industry\"]=LabelEncoder().fit_transform(df1[\"salary_range\"])\ndf1[\"function\"]=LabelEncoder().fit_transform(df1[\"salary_range\"])\ndf1.head()","4794ba65":"!pip install clean-text\nfrom cleantext import clean\n\nprint(\"#==============[BEFORE]======================#\")\nprint(df1[\"company_profile\"].iloc[0])\nprint(\"#==============[AFTER]======================#\")\ntext=clean(df1[\"company_profile\"].iloc[0],no_punct=True)\nprint(text)","e48e38b9":"def convert_to_embeddings(text):\n    try:\n        text=clean(text,no_punct=True)\n    except:\n        text=\" \"\n    return model.infer_vector(text.split())\n\n\n\n#==========[IVED SAVED THIS PORTION IN .NPY FILE]=======================#\n# df1[\"title\"]=df1[\"title\"].apply(convert_to_embeddings)\n# df1[\"company_profile\"]=df1[\"company_profile\"].apply(convert_to_embeddings)\n# df1[\"description\"]=df1[\"description\"].apply(convert_to_embeddings)\n# df1[\"requirements\"]=df1[\"requirements\"].apply(convert_to_embeddings)\n# df1[\"benefits\"]=df1[\"benefits\"].apply(convert_to_embeddings)\n\n","73058afe":"from sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\n\nswag=np.load(\"\/kaggle\/input\/df1tonpy1\/data.npy\",allow_pickle=True)\n\ntraining_data_text=np.hstack([np.vstack(swag[:,0]),np.vstack(swag[:,4]),np.vstack(swag[:,5]),np.vstack(swag[:,6]),np.vstack(swag[:,7])])\ntraining_data_text.shape\n\ntraining_data=np.hstack([training_data_text,swag[:,1:3],swag[:,8:]])\n\n\ntraining_data=scaler.fit_transform(training_data)\n","038ea503":"X=training_data[:,:-1]\nY=training_data[:,-1]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=42)","69d198b9":"import tensorflow as tf\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  try:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n    print(e)\n    \nfrom tensorflow.keras.layers import Dense,Input,Dropout,BatchNormalization\nfrom tensorflow.keras.models import Sequential\n\nmodel2=Sequential()\nmodel2.add(Input(shape=(X.shape[1])))\nmodel2.add(BatchNormalization())\nmodel2.add(Dense(128,activation=tf.nn.selu))\nmodel2.add(Dropout(0.5))\nmodel2.add(Dense(64,activation=tf.nn.selu))\nmodel2.add(Dropout(0.2))\nmodel2.add(Dense(32,activation=tf.nn.selu))\nmodel2.add(Dense(1,activation=tf.nn.sigmoid))\n\n\nmodel2.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['accuracy'])\n\nmodel2.summary()","d9c9e855":"history=model2.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=80)","df0ad19f":"from sklearn.metrics import classification_report,confusion_matrix\n\npred=model2.predict(X_test)\npred=np.array([1 if row>=0.5 else 0 for row in pred])\nprint(classification_report(y_test,pred))\nsns.heatmap(confusion_matrix(y_test,pred),annot=True)\nplt.show()\n\npred=model2.predict(X_train)\npred=np.array([1 if row>=0.5 else 0 for row in pred])\nprint(classification_report(y_train,pred))\nsns.heatmap(confusion_matrix(y_train,pred),annot=True)","64b76738":"plt.plot(history.history[\"val_loss\"])\nplt.plot(history.history[\"loss\"])","3fff4bfb":"### <center> In this Notebook, the task is a binary classification of fraudulent behaviour by analysing text <\/center>\n\n#### Here is what ive used:\n#### 1. Doc2Vec\n#### 2. textclean\n#### 3. Keras","9f58bd1a":"### The training is slightly overfitted here, but as you can see, it achieves impressive results\n### -0.99 Accuraccy\n### -0.98 Val Accuaraccy","85831c17":"### Loss function confirms overfitting","dd7bfc44":"### Here we define the model\n- Noticed that ived add in dropout for regularization\n- the starting few codes are meant to reduce EOM errors on my gpu\n- BatchNorm is the most important layer here\n    - Batch Norm will cause the accuracy to go up significantly, \n    - You can test this by yourself\n    - This is because it reduces the covariate shift problem (Think of a classifier trained on black cats but the test set is on ginger cats)","6f9843b7":"### Ive converted the relavant columns to categorical here","5b09d8be":"### Import the data and fill NAs\n\n- Notice that ived replaced \"benefits\" with \"Adequete benefits\".\n- This is because doc2vec will intepret this as a vector. \n- This vector will contain information on its general context","1a2f1003":"### This cell converts the text to the word2doc embeddings\n- ived saved the dataframe in a .npy file as this cell will take awhile to run\n- you can uncomment the lines below to try for yourself","c06d675a":"### We text the text cleaner here\n- ive used this text cleaner because it does not remove stop words\n- all words are required in order for doc2vec to capture the relavant context as implied by the name\n- you can try removing stop words and see how it affects accuraccy (from my tests, it reduces accuracy if stopwords are taken out)","5fca17fd":"### Split the data into test and train set\n- 0.1 split was used","80aa249d":"### We then normalize the data"}}