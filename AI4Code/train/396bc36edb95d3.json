{"cell_type":{"54b7c347":"code","e2f23ad5":"code","fdd66fe3":"code","e0d61c12":"code","d6fd9e20":"code","54c48347":"code","10a10cc2":"code","a96ef2d2":"code","a6ae3743":"code","5be1a1c9":"code","87323411":"code","48fa9ba2":"code","e2d71a93":"code","9fe56728":"code","71f11ad7":"code","7f218d6b":"code","5b3bab9e":"code","ed2c79a8":"code","e78d7767":"code","baf8a8fb":"code","6e356149":"code","da01fd23":"code","e843bb64":"code","20fed65c":"code","f2afd365":"code","dbe1e3a1":"code","a8fb6db0":"code","7a6e21c0":"code","5a479458":"code","55cb232d":"code","de7d3869":"code","f32ec5be":"code","7ccc5d3e":"code","15ff7393":"code","35e0d06d":"code","50b04ae3":"code","efcd8885":"code","04464832":"code","1ef8e37e":"code","d0e39165":"code","ec3d9aee":"code","95a823e6":"code","7c0d8e65":"code","3ebf448b":"code","ee0fa117":"code","8bd331e5":"code","3b169e09":"code","66b63eca":"code","7b8e197d":"code","88ee4425":"code","5a902db9":"code","d941652f":"code","719a3db9":"code","a697b97f":"code","c9be400d":"code","60ef72be":"code","b6c78eaa":"code","0e92b36c":"code","f9f874cb":"code","b98a1ef2":"code","4a1d868c":"code","0a1d170d":"code","97ebce7d":"code","aff497a3":"code","3e22ead8":"code","8a66ea19":"code","dbdf641c":"code","d13fd5c0":"code","58b2d867":"code","8502d3df":"code","26d05e46":"code","6ed34d57":"code","428bd5ef":"code","1342449b":"code","354a00fb":"code","40491f36":"code","839b0d4f":"code","6ab3c0ff":"code","bb7034cb":"code","ed1d5f5c":"code","736fe41f":"code","d375d4d2":"code","1d2c9b9d":"code","30780b56":"code","8e05d0c8":"code","4dea673a":"code","30c83c6b":"code","8b2b14c2":"code","faba6501":"code","59cf64e8":"code","32856e5d":"code","4097c82e":"code","25aadbf7":"code","4cb231d8":"code","113e44bd":"code","7ef11937":"code","1afcf2db":"code","252a94d8":"code","94f019e6":"code","346ef46e":"code","3759cb52":"code","33028038":"code","80f01cdd":"code","924f5583":"code","7a4dc58d":"code","cbb0f31d":"code","8cff221d":"code","50c66d74":"code","4b7c8bb1":"code","7de38079":"code","fa2cc22b":"code","d01e411c":"code","5e8e26e3":"code","fe31c110":"code","1058ad5a":"code","4b24d571":"code","70f0b4b8":"code","1076a116":"code","0bee8679":"code","a8b336ac":"code","7255ae87":"code","5f42769c":"code","e3a9cd05":"markdown","bfd9a6ec":"markdown","29c82954":"markdown","a2a957a2":"markdown","2294c1f7":"markdown","fcacdacf":"markdown","a5c47aa5":"markdown","c7957b53":"markdown","7ea86a15":"markdown","b7e1f869":"markdown","808d6e28":"markdown","82ead489":"markdown","6c37ceed":"markdown","d29d987e":"markdown","fc4cb008":"markdown","2bfd92b1":"markdown","fdc235a9":"markdown","5211ce87":"markdown","ae038447":"markdown","add9273e":"markdown","5719da18":"markdown","a01ba04e":"markdown","bfc6a61e":"markdown","5dcb455d":"markdown","5b22816f":"markdown","24babe03":"markdown","835ae791":"markdown","23986ca1":"markdown","1ab9107e":"markdown","f87ce112":"markdown","cadfdcb2":"markdown","ad32d6a4":"markdown","81f2a856":"markdown","e94bc90d":"markdown","e86c9feb":"markdown","28135ea1":"markdown","0d7d9e8d":"markdown","5cc1e312":"markdown","4e4578df":"markdown","260274dc":"markdown","2fac95e7":"markdown","7bb35139":"markdown","4c5604b3":"markdown","cde467a2":"markdown","2c9d6070":"markdown","05ce6557":"markdown","4289b2b2":"markdown","26349869":"markdown","d8c97a43":"markdown","be28f620":"markdown","7aa7444d":"markdown","13e0f3e8":"markdown","9a54c36f":"markdown","ccc2b061":"markdown","74a055c5":"markdown","b71303e5":"markdown","86a65193":"markdown","cf27d341":"markdown"},"source":{"54b7c347":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e2f23ad5":"# Importing all libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","fdd66fe3":"# Loading the dataset\ndf = pd.read_csv('\/kaggle\/input\/data-mining-insurance-claim\/insurance_part2_data-2.csv')","e0d61c12":"df.head()","d6fd9e20":"# checking the rows and columns in the dataset\ndf.shape","54c48347":"# information on the dataset\ndf.info()","10a10cc2":"# checking for duplicates\nprint('There are',df.duplicated().sum(),'duplicates in this dataset')","a96ef2d2":"df.describe(percentiles=(0.10,0.25,0.50,0.75,0.95)).T","a6ae3743":"# Let us remove the duplicate rows\ndf.drop_duplicates(inplace=True)","5be1a1c9":"print('There are',df.duplicated().sum(),'duplicates in this dataset after removing the duplicates')","87323411":"df[df['Duration'] == -1]","48fa9ba2":"df['Duration'].mean()","e2d71a93":"df['Duration'] = df['Duration'].replace(to_replace=-1, value=df['Duration'].mean())","9fe56728":"df.loc[1508]","71f11ad7":"df_with_outliers = df","7f218d6b":"df_numerical = df[['Age', 'Duration', 'Commision', 'Sales']]\ndf_numerical.head()","5b3bab9e":"df_categorical = df[['Agency_Code', 'Type', 'Claimed', 'Channel', 'Product Name', 'Destination']]\ndf_categorical.head()","ed2c79a8":"df_categorical['Claimed'].value_counts()","e78d7767":"plt.figure(figsize = (15,8))\nfeature_list = df_numerical.columns\nfor i in range(len(feature_list)):\n    plt.subplot(2,2,i+1)\n    sns.boxplot(x=df_numerical[feature_list[i]], data=df_numerical, orient='h', color='r')\n    plt.title('Boxplot of {}'.format(feature_list[i]))\n    plt.tight_layout()","baf8a8fb":"from scipy.stats import norm","6e356149":"plt.figure(figsize = (15,8))\nfeature_list = df_numerical.columns\nfor i in range(len(feature_list)):\n    plt.subplot(2,2,i+1)\n    sns.distplot(x=df_numerical[feature_list[i]], color='b', fit=norm)\n    plt.title('Distribution Plot of {}'.format(feature_list[i]))\n    plt.tight_layout()","da01fd23":"for columns in df_numerical.columns:\n    print('Skewness of {} is'.format(columns), round(df_numerical[columns].skew(),2))\n    print('Kurtosis of {} is'.format(columns), round(df_numerical[columns].kurt(),2))","e843bb64":"plt.figure(figsize=(15,10))\n\nplt.subplot(2,2,1)\nsns.boxplot(y='Claimed', x='Age', data=df, palette='spring')\nplt.title('Boxplot of Age w.r.t Claimed')\n\nplt.subplot(2,2,2)\nsns.boxplot(y='Claimed', x='Duration', data=df, palette='spring')\nplt.title('Boxplot of Age w.r.t Duration')\n\nplt.subplot(2,2,3)\nsns.boxplot(y='Claimed', x='Commision', data=df, palette='spring')\nplt.title('Boxplot of Age w.r.t Commission')\n\nplt.subplot(2,2,4)\nsns.boxplot(y='Claimed', x='Sales', data=df, palette='spring')\nplt.title('Boxplot of Age w.r.t Sales');","20fed65c":"plt.figure(figsize=(15,10))\n\nplt.subplot(2,2,1)\nsns.boxplot(y='Age', x='Type', hue='Claimed', data=df, palette='afmhot')\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nplt.title('Boxplot of Age w.r.t Type')\n\nplt.subplot(2,2,2)\nsns.boxplot(y='Age', x='Channel', hue='Claimed', data=df, palette='afmhot')\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nplt.title('Boxplot of Age w.r.t Channel')\n\nplt.subplot(2,2,3)\nsns.boxplot(y='Age', x='Product Name', hue='Claimed', data=df, palette='afmhot')\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nplt.title('Boxplot of Age w.r.t Product Name');\n\nplt.subplot(2,2,4)\nsns.boxplot(y='Age', x='Destination', hue='Claimed', data=df, palette='afmhot')\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nplt.title('Boxplot of Age w.r.t Destination');","f2afd365":"plt.figure(figsize=(15,10))\n\nplt.subplot(2,2,1)\nsns.boxplot(y='Commision', x='Type', hue='Claimed', data=df, palette='autumn')\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nplt.title('Boxplot of Age w.r.t Type')\n\nplt.subplot(2,2,2)\nsns.boxplot(y='Commision', x='Channel', hue='Claimed', data=df, palette='autumn')\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nplt.title('Boxplot of Age w.r.t Channel')\n\nplt.subplot(2,2,3)\nsns.boxplot(y='Commision', x='Product Name', hue='Claimed', data=df, palette='autumn')\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nplt.title('Boxplot of Age w.r.t Product Name');\n\nplt.subplot(2,2,4)\nsns.boxplot(y='Commision', x='Destination', hue='Claimed', data=df, palette='autumn')\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nplt.title('Boxplot of Age w.r.t Destination');","dbe1e3a1":"plt.figure(figsize=(15,10))\n\nplt.subplot(2,2,1)\nsns.boxplot(y='Duration', x='Type', hue='Claimed', data=df, palette='winter')\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nplt.title('Boxplot of Duration w.r.t Type')\n\nplt.subplot(2,2,2)\nsns.boxplot(y='Duration', x='Channel', hue='Claimed', data=df, palette='winter')\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nplt.title('Boxplot of Duration w.r.t Channel')\n\nplt.subplot(2,2,3)\nsns.boxplot(y='Duration', x='Product Name', hue='Claimed', data=df, palette='winter')\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nplt.title('Boxplot of Duration w.r.t Product Name');\n\nplt.subplot(2,2,4)\nsns.boxplot(y='Duration', x='Destination', hue='Claimed', data=df, palette='winter')\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nplt.title('Boxplot of Duration w.r.t Destination');","a8fb6db0":"plt.figure(figsize=(15,10))\n\nplt.subplot(2,2,1)\nsns.boxplot(y='Sales', x='Type', hue='Claimed', data=df, palette='summer')\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nplt.title('Boxplot of Sales w.r.t Type')\n\nplt.subplot(2,2,2)\nsns.boxplot(y='Sales', x='Channel', hue='Claimed', data=df, palette='summer')\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nplt.title('Boxplot of Sales w.r.t Channel')\n\nplt.subplot(2,2,3)\nsns.boxplot(y='Sales', x='Product Name', hue='Claimed', data=df, palette='summer')\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nplt.title('Boxplot of Sales w.r.t Product Name');\n\nplt.subplot(2,2,4)\nsns.boxplot(y='Sales', x='Destination', hue='Claimed', data=df, palette='summer')\nsns.set_style('whitegrid')\nsns.set_palette('bright')\nplt.title('Boxplot of Sales w.r.t Destination');","7a6e21c0":"sns.pairplot(df, corner=True, hue='Claimed', palette='dark');","5a479458":"corr = df.corr()\ncorr","55cb232d":"plt.figure(figsize=(10,8))\nsns.heatmap(corr, annot=True);","de7d3869":"df['Agency_Code'].value_counts()","f32ec5be":"for column in df.columns:\n    if df[column].dtype == 'object':\n        df[column] = pd.Categorical(df[column]).codes","7ccc5d3e":"df.info()","15ff7393":"df.head()","35e0d06d":"df_categorical['Claimed'].value_counts()","50b04ae3":"# Let us check the proportion of our target variable\n# Claimed \"No\" = 0\n# Claimed \"Yes\" = 1\ndf['Claimed'].value_counts(normalize=True)","efcd8885":"X = df.drop('Claimed', axis=1)\n\ny = df.pop('Claimed')\n\nX.head()","04464832":"from sklearn.model_selection import train_test_split","1ef8e37e":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)","d0e39165":"df.shape","ec3d9aee":"print('X_train:', X_train.shape)\nprint('X_test:', X_test.shape)\nprint('y_train:', y_train.shape)\nprint('y_test:', y_test.shape)","95a823e6":"from sklearn.tree import DecisionTreeClassifier","7c0d8e65":"dtree_model = DecisionTreeClassifier(criterion='gini', random_state=0)","3ebf448b":"dtree_model.fit(X_train, y_train)","ee0fa117":"from sklearn import tree","8bd331e5":"train_char_label = ['No', 'Yes']\ntree_file = open('D:\\decision_tree.dot','w')\ndot_data = tree.export_graphviz(dtree_model, out_file=tree_file, feature_names=list(X_train), class_names=train_char_label)","3b169e09":"tree_file.close()","66b63eca":"from sklearn.model_selection import GridSearchCV","7b8e197d":"param_grid = {\n    'max_depth': [10,13,15],\n    'min_samples_leaf': [10,50,100],\n    'min_samples_split': [100,150,200]\n}","88ee4425":"reg_dtree = DecisionTreeClassifier()","5a902db9":"grid_search = GridSearchCV(estimator=reg_dtree, param_grid=param_grid, cv=3, scoring='recall')","d941652f":"grid_search.fit(X_train, y_train)","719a3db9":"grid_search.best_params_","a697b97f":"grid_search.best_estimator_","c9be400d":"best_grid = grid_search.best_estimator_","60ef72be":"best_grid.feature_importances_","b6c78eaa":"ytrain_predict = best_grid.predict(X_train)\nytest_predict = best_grid.predict(X_test)","0e92b36c":"print (pd.DataFrame(best_grid.feature_importances_, \n                    columns = [\"Importance\"], index = X_train.columns).sort_values('Importance',ascending=False))","f9f874cb":"from sklearn.metrics import roc_auc_score, roc_curve, classification_report, confusion_matrix","b98a1ef2":"# predict probabilities\nprobs = best_grid.predict_proba(X_train)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\ncart_train_auc = roc_auc_score(y_train, probs)\nprint('AUC: %.2f' % cart_train_auc)\n# calculate roc curve\ncart_train_fpr, cart_train_tpr, thresholds = roc_curve(y_train, probs)\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\n# plot the roc curve for the model\nplt.plot(cart_train_fpr, cart_train_tpr);","4a1d868c":"# predict probabilities\nprobs = best_grid.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\ncart_test_auc = roc_auc_score(y_test, probs)\nprint('AUC: %.2f' % cart_test_auc)\n# calculate roc curve\ncart_test_fpr, cart_test_tpr, thresholds = roc_curve(y_test, probs)\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\n# plot the roc curve for the model\nplt.plot(cart_test_fpr, cart_test_tpr);","0a1d170d":"print(confusion_matrix(y_train, ytrain_predict))","97ebce7d":"# Get the confusion matrix on the train data\nconfusion_matrix(y_train, ytrain_predict)\nsns.heatmap(confusion_matrix(y_train, ytrain_predict),annot=True, fmt='d',cbar=False, cmap='viridis')\nplt.xlabel('Predicted Label')\nplt.ylabel('Actual Label')\nplt.title('Confusion Matrix')\nplt.show()","aff497a3":"#Train Data Accuracy\ncart_train_accuracy = best_grid.score(X_train,y_train) \ncart_train_accuracy","3e22ead8":"print(classification_report(y_train, ytrain_predict))","8a66ea19":"print(confusion_matrix(y_test, ytest_predict))","dbdf641c":"# Get the confusion matrix on the test data\nconfusion_matrix(y_test, ytest_predict)\nsns.heatmap(confusion_matrix(y_test, ytest_predict),annot=True, fmt='d',cbar=False, cmap='viridis')\nplt.xlabel('Predicted Label')\nplt.ylabel('Actual Label')\nplt.title('Confusion Matrix')\nplt.show()","d13fd5c0":"# Test Data Accuracy\ncart_test_accuracy = best_grid.score(X_test, y_test) \ncart_test_accuracy","58b2d867":"print(classification_report(y_test, ytest_predict))","8502d3df":"cart_metrics = classification_report(y_train, ytrain_predict,output_dict=True)\ndfcart = pd.DataFrame(cart_metrics).transpose()\ncart_train_f1 = round(dfcart.loc[\"1\"][2],2)\ncart_train_recall = round(dfcart.loc[\"1\"][1],2)\ncart_train_precision = round(dfcart.loc[\"1\"][0],2)\nprint ('cart_train_precision ',cart_train_precision)\nprint ('cart_train_recall ',cart_train_recall)\nprint ('cart_train_f1 ',cart_train_f1)","26d05e46":"cart_metrics = classification_report(y_test, ytest_predict, output_dict=True)\ndfcart = pd.DataFrame(cart_metrics).transpose()\ncart_test_precision = round(dfcart.loc[\"1\"][0],2)\ncart_test_recall = round(dfcart.loc[\"1\"][1],2)\ncart_test_f1 = round(dfcart.loc[\"1\"][2],2)\nprint ('cart_test_precision ',cart_test_precision)\nprint ('cart_test_recall ',cart_test_recall)\nprint ('cart_test_f1 ',cart_test_f1)","6ed34d57":"from sklearn.ensemble import RandomForestClassifier","428bd5ef":"param_grid_rfcl = {\n    'max_depth': [10],## 20,30,40\n    'max_features': [5],## 7,8,9\n    'min_samples_leaf': [10],## 50,100\n    'min_samples_split': [100], ## 60,70\n    'n_estimators': [201] ## 100,200\n}\n\nrfcl = RandomForestClassifier(random_state=1)\n\ngrid_search = GridSearchCV(estimator = rfcl, param_grid = param_grid_rfcl, cv = 3, scoring='recall')","1342449b":"grid_search.fit(X_train, y_train)","354a00fb":"grid_search.best_params_","40491f36":"best_grid = grid_search.best_estimator_","839b0d4f":"best_grid","6ab3c0ff":"ytrain_predict = best_grid.predict(X_train)\nytest_predict = best_grid.predict(X_test)","bb7034cb":"print (pd.DataFrame(best_grid.feature_importances_, \n                    columns = [\"Importance\"], index = X_train.columns).sort_values('Importance',ascending=False))","ed1d5f5c":"# predict probabilities\nprobs = best_grid.predict_proba(X_train)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\nrf_train_auc = roc_auc_score(y_train, probs)\nprint('AUC: %.2f' % rf_train_auc)\n# calculate roc curve\nrf_train_fpr, rf_train_tpr, thresholds = roc_curve(y_train, probs)\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\n# plot the roc curve for the model\nplt.plot(rf_train_fpr, rf_train_tpr);","736fe41f":"# predict probabilities\nprobs = best_grid.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\nrf_test_auc = roc_auc_score(y_test, probs)\nprint('AUC: %.2f' % rf_test_auc)\n# calculate roc curve\nrf_test_fpr, rf_test_tpr, thresholds = roc_curve(y_test, probs)\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\n# plot the roc curve for the model\nplt.plot(rf_test_fpr, rf_test_tpr);","d375d4d2":"print(confusion_matrix(y_train, ytrain_predict))","1d2c9b9d":"# Get the confusion matrix on the train data\nconfusion_matrix(y_train, ytrain_predict)\nsns.heatmap(confusion_matrix(y_train, ytrain_predict),annot=True, fmt='d',cbar=False, cmap='viridis')\nplt.xlabel('Predicted Label')\nplt.ylabel('Actual Label')\nplt.title('Confusion Matrix')\nplt.show()","30780b56":"#Train Data Accuracy\nrf_train_accuracy = best_grid.score(X_train,y_train) \nrf_train_accuracy","8e05d0c8":"print(classification_report(y_train, ytrain_predict))","4dea673a":"print(confusion_matrix(y_test, ytest_predict))","30c83c6b":"# Get the confusion matrix on the test data\nconfusion_matrix(y_test, ytest_predict)\nsns.heatmap(confusion_matrix(y_test, ytest_predict),annot=True, fmt='d',cbar=False, cmap='viridis')\nplt.xlabel('Predicted Label')\nplt.ylabel('Actual Label')\nplt.title('Confusion Matrix')\nplt.show()","8b2b14c2":"# Test Data Accuracy\nrf_test_accuracy = best_grid.score(X_test, y_test) \nrf_test_accuracy","faba6501":"print(classification_report(y_test, ytest_predict))","59cf64e8":"rf_metrics = classification_report(y_train, ytrain_predict,output_dict=True)\ndfrf = pd.DataFrame(rf_metrics).transpose()\nrf_train_precision = round(dfrf.loc[\"1\"][0],2)\nrf_train_recall = round(dfrf.loc[\"1\"][1],2)\nrf_train_f1 = round(dfrf.loc[\"1\"][2],2)\nprint ('rf_train_precision ',rf_train_precision)\nprint ('rf_train_recall ',rf_train_recall)\nprint ('rf_train_f1 ',rf_train_f1)","32856e5d":"rf_metrics = classification_report(y_test, ytest_predict,output_dict=True)\ndfrf = pd.DataFrame(rf_metrics).transpose()\nrf_test_precision = round(dfrf.loc[\"1\"][0],2)\nrf_test_recall = round(dfrf.loc[\"1\"][1],2)\nrf_test_f1 = round(dfrf.loc[\"1\"][2],2)\nprint ('rf_test_precision ',rf_test_precision)\nprint ('rf_test_recall ',rf_test_recall)\nprint ('rf_test_f1 ',rf_test_f1)","4097c82e":"from sklearn.neural_network import MLPClassifier","25aadbf7":"from sklearn.preprocessing import StandardScaler","4cb231d8":"df1 = pd.read_csv('\/kaggle\/input\/data-mining-insurance-claim\/insurance_part2_data-2.csv')\ndf1.head()","113e44bd":"for column in df1.columns:\n    if df1[column].dtype == 'object':\n        df1[column] = pd.Categorical(df1[column]).codes","7ef11937":"df1.info()","1afcf2db":"X = df1.drop('Claimed', axis=1)\n\ny = df1.pop('Claimed')\n\nX.head()","252a94d8":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)","94f019e6":"sc = StandardScaler()","346ef46e":"sc.fit_transform(X_train)","3759cb52":"sc.transform(X_test)","33028038":"param_grid = {\n    'hidden_layer_sizes': [3,5,7], # 50, 200\n    'max_iter': [2500], #5000,2500\n    'solver': ['adam','sgd', 'lbfgs'], #sgd\n    'tol': [0.01, 0.001], \n}\n\nnncl = MLPClassifier(random_state=0)\n\ngrid_search = GridSearchCV(estimator = nncl, param_grid = param_grid, cv = 5, scoring='recall')","80f01cdd":"grid_search.fit(X_train, y_train)","924f5583":"grid_search.best_params_","7a4dc58d":"grid_search.best_estimator_","cbb0f31d":"best_grid = grid_search.best_estimator_","8cff221d":"ytrain_predict = best_grid.predict(X_train)\nytest_predict = best_grid.predict(X_test)","50c66d74":"print(confusion_matrix(y_train, ytrain_predict))","4b7c8bb1":"# Get the confusion matrix on the train data\nconfusion_matrix(y_train, ytrain_predict)\nsns.heatmap(confusion_matrix(y_train, ytrain_predict),annot=True, fmt='d',cbar=False, cmap='viridis')\nplt.xlabel('Predicted Label')\nplt.ylabel('Actual Label')\nplt.title('Confusion Matrix')\nplt.show()","7de38079":"#Train Data Accuracy\nnn_train_accuracy = best_grid.score(X_train,y_train) \nnn_train_accuracy","fa2cc22b":"print(classification_report(y_train, ytrain_predict))","d01e411c":"print(confusion_matrix(y_test, ytest_predict))","5e8e26e3":"# Get the confusion matrix on the test data\nconfusion_matrix(y_test, ytest_predict)\nsns.heatmap(confusion_matrix(y_test, ytest_predict),annot=True, fmt='d',cbar=False, cmap='viridis')\nplt.xlabel('Predicted Label')\nplt.ylabel('Actual Label')\nplt.title('Confusion Matrix')\nplt.show()","fe31c110":"#Test Data Accuracy\nnn_test_accuracy = best_grid.score(X_test,y_test) \nnn_test_accuracy","1058ad5a":"print(classification_report(y_test, ytest_predict))","4b24d571":"# predict probabilities\nprobs = best_grid.predict_proba(X_train)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\nnn_train_auc = roc_auc_score(y_train, probs)\nprint('AUC: %.2f' % nn_train_auc)\n# calculate roc curve\nnn_train_fpr, nn_train_tpr, thresholds = roc_curve(y_train, probs)\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\n# plot the roc curve for the model\nplt.plot(nn_train_fpr, nn_train_tpr);","70f0b4b8":"# predict probabilities\nprobs = best_grid.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\nnn_test_auc = roc_auc_score(y_test, probs)\nprint('AUC: %.2f' % nn_test_auc)\n# calculate roc curve\nnn_test_fpr, nn_test_tpr, thresholds = roc_curve(y_test, probs)\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\n# plot the roc curve for the model\nplt.plot(nn_test_fpr, nn_test_tpr);","1076a116":"nn_metrics = classification_report(y_train, ytrain_predict,output_dict=True)\ndfnn = pd.DataFrame(nn_metrics).transpose()\nnn_train_precision = round(dfnn.loc[\"1\"][0],2)\nnn_train_recall = round(dfnn.loc[\"1\"][1],2)\nnn_train_f1 = round(dfnn.loc[\"1\"][2],2)\nprint ('nn_train_precision ',nn_train_precision)\nprint ('nn_train_recall ',nn_train_recall)\nprint ('nn_train_f1 ',nn_train_f1)","0bee8679":"nn_metrics = classification_report(y_test, ytest_predict,output_dict=True)\ndfnn = pd.DataFrame(nn_metrics).transpose()\nnn_test_precision = round(dfnn.loc[\"1\"][0],2)\nnn_test_recall = round(dfnn.loc[\"1\"][1],2)\nnn_test_f1 = round(dfnn.loc[\"1\"][2],2)\nprint ('nn_test_precision ',nn_test_precision)\nprint ('nn_test_recall ',nn_test_recall)\nprint ('nn_test_f1 ',nn_test_f1)","a8b336ac":"index=['Accuracy', 'AUC', 'Recall','Precision','F1 Score']\ndata = pd.DataFrame({'CART Train':[cart_train_accuracy,cart_train_auc,cart_train_recall,cart_train_precision,cart_train_f1],\n        'CART Test':[cart_test_accuracy,cart_test_auc,cart_test_recall,cart_test_precision,cart_test_f1],\n       'Random Forest Train':[rf_train_accuracy,rf_train_auc,rf_train_recall,rf_train_precision,rf_train_f1],\n        'Random Forest Test':[rf_test_accuracy,rf_test_auc,rf_test_recall,rf_test_precision,rf_test_f1],\n       'Neural Network Train':[nn_train_accuracy,nn_train_auc,nn_train_recall,nn_train_precision,nn_train_f1],\n        'Neural Network Test':[nn_test_accuracy,nn_test_auc,nn_test_recall,nn_test_precision,nn_test_f1]},index=index)\nround(data,2)","7255ae87":"plt.plot([0, 1], [0, 1], linestyle='--')\nplt.plot(cart_train_fpr, cart_train_tpr,color='red',label=\"CART\")\nplt.plot(rf_train_fpr,rf_train_tpr,color='green',label=\"RF\")\nplt.plot(nn_train_fpr,nn_train_tpr,color='black',label=\"NN\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\nplt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower right');","5f42769c":"plt.plot([0, 1], [0, 1], linestyle='--')\nplt.plot(cart_test_fpr, cart_test_tpr,color='red',label=\"CART\")\nplt.plot(rf_test_fpr,rf_test_tpr,color='green',label=\"RF\")\nplt.plot(nn_test_fpr,nn_test_tpr,color='black',label=\"NN\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\nplt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower right');","e3a9cd05":"### Problem 2: CART-RF-ANN\n\nAn Insurance firm providing tour insurance is facing higher claim frequency. The management decides to collect data from the past few years. You are assigned the task to make a model which predicts the claim status and provide recommendations to management. Use CART, RF & ANN and compare the models' performances in train and test sets.","bfd9a6ec":"#### Confusion Matrix and Classification Report for Testing data","29c82954":"#### Confusion Matrix and Classification Report for Testing data - ANN","a2a957a2":"#### Plotting Commission with categorical variables w.r.t Claimed status","2294c1f7":"#### Plotting numerical variables w.r.t Claimed status","fcacdacf":"#### Predicting on Training and Test data","a5c47aa5":"### Univariate Analysis","c7957b53":"#### Confusion Matrix and Classification Report for Training data - ANN","7ea86a15":"### Model Evaluation","b7e1f869":"### Model Evaluation - CART","808d6e28":"#### AOC and ROC for Testing data - ANN","82ead489":"#### Splitting data into training and test set","6c37ceed":"### Bivariate Analysis","d29d987e":"### Building Random Forest Model","fc4cb008":"#### Checking the dimensions of the training and test data","2bfd92b1":"### Conclusion for Artificial Neural Network (ANN)","fdc235a9":"### Conclusion for CART Model","5211ce87":"#### AUC and ROC for Testing data - Random Forest","ae038447":"http:\/\/webgraphviz.com\/","add9273e":"#### Confusion Matrix and Classification Report for Training Data - CART","5719da18":"#### AUC and ROC for Training data - Random Forest","a01ba04e":"#### Data Dictionary\n\n1. Target: Claim Status (Claimed)\n2. Code of tour firm (Agency_Code)\n3. Type of tour insurance firms (Type)\n4. Distribution channel of tour insurance agencies (Channel)\n5. Name of the tour insurance products (Product)\n6. Duration of the tour (Duration in days)\n7. Destination of the tour (Destination)\n8. Amount worth of sales per customer in procuring tour insurance policies in rupees (in 100\u2019s)\n9. The commission received for tour insurance firm (Commission is in percentage of sales)\n10.Age of insured (Age)","bfc6a61e":"#### Extracting the target column into separate vectors for training set and test set","5dcb455d":"#### AUC and ROC for Testing data","5b22816f":"#### AOC and ROC for Training data - ANN","24babe03":"<b>Train Data:<\/b>  \n    AUC: 85%        \n    Accuracy: 80%             \n    Precision: 69%        \n    f1-Score: 64%       \n            \n<b>Test Data:<\/b>      \n    AUC: 80%      \n    Accuracy: 80%         \n    Precision: 73%       \n    f1-Score: 65%     \n  \nTraining and Test set results are almost similar, and with the overall measures high, the model is a good model.  \n  \nAgency_Code is the most important variable for predicting claim status. ","835ae791":"#### Boxplots","23986ca1":"-----HAPPY LEARNING-----","1ab9107e":"Out of the 3 models, Random Forest has slightly better performance than the CART and ANN model. \n\nOverall all the 3 models are reasonably stable enough to be used for making any future predictions. From CART and Random Forest Model, the variable Agency_Code is found to be the most useful feature amongst all other features for predicting if a customer has claimed the insurance or not. ","f87ce112":"<b>Train Data:<\/b>  \n    AUC: 81%        \n    Accuracy: 78%             \n    Precision: 65%        \n    f1-Score: 61%       \n            \n<b>Test Data:<\/b>      \n    AUC: 83%      \n    Accuracy: 77%         \n    Precision: 68%       \n    f1-Score: 61%     \n  \nThe ANN model seems to over-fit.   ","cadfdcb2":"#### Plotting Age with categorical variables w.r.t Claimed status","ad32d6a4":"### Conclusion for Random Forest Model","81f2a856":"### Building Neural Network Model","e94bc90d":"#### Feature Importance","e86c9feb":"<b>Train Data:<\/b>  \n    AUC: 83%        \n    Accuracy: 78%             \n    Precision: 66%        \n    f1-Score: 64%       \n            \n<b>Test Data:<\/b>      \n    AUC: 78%      \n    Accuracy: 75%         \n    Precision: 64%       \n    f1-Score: 60%     \n  \nTraining and Test set results are almost similar, and with the overall measures high, the model is a good model.  \n  \nAgency_Code is the most important variable for predicting claim status. Decision Tree root node split has been done based on Agency_Code. ","28135ea1":"#### Confusion Matrix and Classification Report for Training Data - Random Forest","0d7d9e8d":"### Building CART \/ Decision Tree Model","5cc1e312":"#### 2.3 Performance Metrics: Check the performance of Predictions on Train and Test sets using Accuracy, Confusion Matrix, Plot ROC curve and get ROC_AUC score for each model, Make classification reports for each model. Calculate Train and Test Accuracies for each model. Build confusion matrix for each model. Calculate roc_auc_score for each model. Build classification reports for each model. ","4e4578df":"#### 2.2 Data Split: Split the data into test and train to build classification model CART, Random Forest, Artificial Neural Network. Feature importance for each model.","260274dc":"From the above we can see that we have one observation in Duration column with value -1 which cannot be the case. Duration of a stay cannot be in negative numbers. Hence let us impute the bad data with mean value 72.14 days.","2fac95e7":"#### Feature Importance","7bb35139":"#### AUC and ROC for Training data","4c5604b3":"#### Skewness and Kurtosis","cde467a2":"#### Distribution Plots","2c9d6070":"#### Decision Tree Classifier Model","05ce6557":"### Model Evaluation - ANN","4289b2b2":"#### Comparison of the performance metrics from the 3 models","26349869":"#### Pairplot","d8c97a43":"#### 2.1 Read the data and do exploratory data analysis (4 pts). Describe the data briefly. Interpret the inferences for each (2 pts). Initial steps like head() .info(), Data Types, etc . Null value check. Distribution plots(histogram) or similar plots for the continuous columns. Box plots, Correlation plots. Appropriate plots for categorical variables. Inferences on each plot. Summary stats, Skewness, Outliers proportion should be discussed, and inferences from above used plots should be there. There is no restriction on how the learner wishes to implement this but the code should be able to represent the correct output and inferences should be logical and correct.","be28f620":"#### Plotting Sales with categorical variables w.r.t Claimed status","7aa7444d":"#### ROC Curve for the 3 models on the Training data","13e0f3e8":"#### ROC Curve for the 3 models on the Testing data","9a54c36f":"### Model Evaluation - Random Forest","ccc2b061":"Let us segregate the data into numerical and categorical dataframes","74a055c5":"#### Treating Bad Data","b71303e5":"#### Plotting Duration with categorical variables w.r.t Claimed status","86a65193":"#### Confusion Matrix and Classification Report for Testing data - Random Forest","cf27d341":"#### 2.4 Final Model - Compare all models on the basis of the performance metrics in a structured tabular manner (2.5 pts). Describe on which model is best\/optimized (1.5 pts ). A table containing all the values of accuracies, precision, recall, auc_roc_score, f1 score. Comparison between the different models(final) on the basis of above table values. After comparison which model suits the best for the problem in hand on the basis of different measures. Comment on the final model."}}