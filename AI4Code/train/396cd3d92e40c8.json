{"cell_type":{"6cc837b9":"code","8888ceef":"code","f7363a91":"code","19d293e9":"code","f4886ea3":"code","51568ab9":"code","7ecee03d":"code","d2406b7c":"code","53ddd282":"code","0d333fa4":"code","c275fe87":"code","e625bfd4":"code","6349730e":"code","56a5cb36":"code","3bb857d5":"code","f8df0a44":"code","4728bbe4":"code","7aacf683":"code","43bd3284":"code","3a891d88":"code","82f12e49":"code","9ae7eb15":"code","73a3e272":"code","86ac4133":"code","3a1a0d3b":"code","cbee642a":"code","5700b2f8":"code","fb8977c2":"code","7cc631d1":"code","ccd51351":"code","5237c135":"code","bbca11c9":"code","e5b5096a":"code","f61da2f3":"code","8933802a":"code","ba7103ae":"code","e2d31ad8":"code","9bd7109a":"code","a5e2b441":"code","fb64f3d9":"code","0b3d9620":"code","62e35c06":"code","9db6c82a":"code","40494d12":"code","62d74a6d":"code","5747de60":"code","2fe9d6f2":"code","6800f6f7":"code","2a658281":"code","1961bd8e":"code","81a716b4":"code","22b7c741":"code","766f5255":"code","89e54e93":"code","e1c51d97":"code","fff3dcbf":"code","f7e70868":"code","d7a3b7c4":"code","e29c5db9":"code","ec54f2ec":"code","ccd63b95":"code","821f3135":"code","0d9c14f2":"code","6b3c7160":"code","2c5882f5":"markdown","1911ba2a":"markdown","e071839f":"markdown","4c23f59c":"markdown","16ebff03":"markdown","00197a6f":"markdown","f2cf0bc3":"markdown","7d78bdec":"markdown","6b980c4b":"markdown","acf58e02":"markdown","29a1b54e":"markdown","a4244b72":"markdown","469f0f63":"markdown","e3e8e3fe":"markdown","983e91de":"markdown","0eec26d7":"markdown","129e69b8":"markdown","db1b3dda":"markdown","bc8c33bf":"markdown","d2fbaa6f":"markdown","99b0f3d1":"markdown","2b162dbc":"markdown","0a4b9b2d":"markdown","cef41277":"markdown","0ec8e678":"markdown","bb23150d":"markdown","dfe30040":"markdown","977ea02a":"markdown","8d94031f":"markdown","ec982fc9":"markdown","c4ea8a04":"markdown"},"source":{"6cc837b9":"from IPython.display import Image\nImage(\"..\/input\/type-text-summarization\/type_text_summarization.png\")","8888ceef":"Image(\"..\/input\/extractive-summarization\/extractive summarization.png\")","f7363a91":"Image(\"..\/input\/abstractive-type\/abstractive_sentence.png\")","19d293e9":"Image(\"..\/input\/encode-decode\/encode_decode.png\")","f4886ea3":"Image(\"..\/input\/encoder-diagaram\/encoder_diagram.png\")","51568ab9":"Image(\"..\/input\/decoder-diagram\/decoder_diagram.png\")","7ecee03d":"Image(\"..\/input\/inference\/inference.png\")","d2406b7c":"Image(\"..\/input\/timestep\/timestep1.png\")","53ddd282":"Image(\"..\/input\/timestep\/timestep2.png\")","0d333fa4":"Image(\"..\/input\/timestep\/timestep3.png\")","c275fe87":"Image(\"..\/input\/global-local\/global attention.jpg\")","e625bfd4":"Image(\"..\/input\/global-local\/local attention.jpg\")","6349730e":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport re, spacy\n\nfrom time import time \n\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer \nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, Concatenate, TimeDistributed, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n","56a5cb36":"data = pd.read_csv(\"..\/input\/bengali-text-summarization\/text_summarization.csv\")\ndata.head()","3bb857d5":"print(\"Shape of the dataset is :\", data.shape)","f8df0a44":"data.head(10)","4728bbe4":"data.info()","7aacf683":"for i in range(5):\n    print(\"Text is \\n\", data['text'][i], '\\n')\n    print(\"Summary is \\n\", data['summary'][i], '\\n\\n')","43bd3284":"#data = data.iloc[:5000, :]\n#data.head()","3a891d88":"#data.shape","82f12e49":"def clean_text_summary(columns):\n    for row in columns:\n        row = str(row) \n        row = row.replace('\\n', \" \")\n        row = row.replace('\\t', \" \")\n        row = row.replace('\\\\', \"\")\n        row = re.sub('[!\"#$%&\\'()*+,-.\/:;<=>?@[\\\\]^_`{|}~]', '', row)\n        row = re.sub(' +', ' ', row)\n        row = row.replace(\"\u0964\", \" \")\n        row = re.sub('[\u09e7\u09e8\u09e9\u09ea\u09eb\u09ec\u09ed\u09ee\u09ef\u09e6]', '', row)\n        row = re.sub('[1234567890]', '', row) \n        row = row.replace('\u2019', '')\n        row = row.replace('\u2018', '')\n        \n        \n        \n        row=re.sub(\"(\\\\t)\", ' ', row) #remove escape charecters\n        row=re.sub(\"(\\\\r)\", ' ', row)\n        row=re.sub(\"(\\\\n)\", ' ', row)\n        \n        row=re.sub(\"(__+)\", ' ', row)  #remove _ if it occors more than one time consecutively\n        row=re.sub(\"(--+)\", ' ', row)   #remove - if it occors more than one time consecutively\n        row=re.sub(\"(~~+)\", ' ', row) #remove ~ if it occors more than one time consecutively\n        row=re.sub(\"(\\+\\++)\", ' ', row)   #remove + if it occors more than one time consecutively\n        row=re.sub(\"(\\.\\.+)\", ' ', row)   #remove . if it occors more than one time consecutively\n        \n        row=re.sub(r\"[<>()|&\u00a9\u00f8\\[\\]\\'\\\",;?~*!]\", ' ', row) #remove <>()|&\u00a9\u00f8\"',;?~*!\n        \n        row=re.sub(\"(mailto:)\", ' ', row) #remove mailto:\n        row=re.sub(r\"(\\\\x9\\d)\", ' ', row) #remove \\x9* in text\n        row=re.sub(\"([iI][nN][cC]\\d+)\", 'INC_NUM', row) #replace INC nums to INC_NUM\n        row=re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", 'CM_NUM', row)#replace CM# and CHG# to CM_NUM\n        \n        row=re.sub(\"(\\.\\s+)\", ' ', row) #remove full stop at end of words(not between)\n        row=re.sub(\"(\\-\\s+)\", ' ', row) #remove - at end of words(not between)\n        row=re.sub(\"(\\:\\s+)\", ' ', row) #remove : at end of words(not between)\n        \n        row=re.sub(\"(\\s+.\\s+)\", ' ', row) #remove any single charecters hanging between 2 spaces\n        \n        #Replace any url as such https:\/\/abc.xyz.net\/browse\/sdf-5327 ====> abc.xyz.net\n        try:\n            url = re.search(r'((https*:\\\/*)([^\\\/\\s]+))(.[^\\s]+)', row)\n            repl_url = url.group(3)\n            row = re.sub(r'((https*:\\\/*)([^\\\/\\s]+))(.[^\\s]+)',repl_url, row)\n        except:\n            pass #there might be emails with no url in them\n        \n        row = re.sub(\"(\\s+)\",' ', row) #remove multiple spaces\n        \n        #Should always be last\n        row=re.sub(\"(\\s+.\\s+)\", ' ', row) #remove any single charecters hanging between 2 spaces\n\n        yield row \n        ","9ae7eb15":"text_cleaning = clean_text_summary(data['text'])\nsummary_cleaning = clean_text_summary(data['summary'])","73a3e272":"nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n\n#Taking advantage of spaCy .pipe() method to speed-up the cleaning process:\n#If data loss seems to be happening(i.e len(text) = 50 instead of 75 etc etc) in this cell , decrease the batch_size parametre \n\nt = time()\ntext = [str(doc) for doc in nlp.pipe(text_cleaning, batch_size=5000, n_threads=-1)]\n# Takes 7-8 mins\nprint(\"Time to clean up everything: {} mins\", format(round((time() - t)\/ 60, 2)))\n","86ac4133":"t = time()\n\n#Batch the data points into 5000 and run on all cores for faster preprocessing\nsummary = ['_START_ '+ str(doc) + ' _END_' for doc in nlp.pipe(summary_cleaning, batch_size=5000, n_threads=-1)]\n\n#Takes 7-8 mins\nprint('Time to clean up everything: {} mins'.format(round((time() - t) \/ 60, 2)))\n","3a1a0d3b":"for i in range(5):\n    print(\"Text is :\\n\", text[i], '\\n')\n    print(\"Summary is :\\n\", summary[i], '\\n\\n')","cbee642a":"pre = pd.DataFrame()\npre['cleaned_text'] = pd.Series(text)\npre['cleaned_summary'] = pd.Series(summary)\npre.head()","5700b2f8":"pre.shape","fb8977c2":"text_count = []\nsummary_count = []\n","7cc631d1":"for sent in pre['cleaned_text']:\n    text_count.append(len(sent.split()))\nfor sent in pre['cleaned_summary']:\n    summary_count.append(len(sent.split()))","ccd51351":"graph_df= pd.DataFrame()\ngraph_df['text']=text_count\ngraph_df['summary']=summary_count\ngraph_df.head()","5237c135":"graph_df.hist(bins = 5)\nplt.show()","bbca11c9":"#Check how much % of summary have 0-15 words\ncnt=0\nfor i in pre['cleaned_summary']:\n    if(len(i.split())<=15):\n        cnt=cnt+1\nprint(cnt\/len(pre['cleaned_summary']))","e5b5096a":"#Check how much % of text have 0-70 words\ncnt=0\nfor i in pre['cleaned_text']:\n    if(len(i.split())<=100):\n        cnt=cnt+1\nprint(cnt\/len(pre['cleaned_text']))","f61da2f3":"#Model to summarize the text between 0-15 words for Summary and 0-100 words for Text\nmax_text_len=100\nmax_summary_len=15","8933802a":"cleaned_text =np.array(pre['cleaned_text'])\ncleaned_summary=np.array(pre['cleaned_summary'])\n\nshort_text=[]\nshort_summary=[]\n\nfor i in range(len(cleaned_text)):\n    if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n        short_text.append(cleaned_text[i])\n        short_summary.append(cleaned_summary[i])\n        \npost_pre=pd.DataFrame({'text':short_text,'summary':short_summary})\npost_pre.head()\n","ba7103ae":"# Shape of post_pre \npost_pre.shape","e2d31ad8":"#Add sostok and eostok at \npost_pre['summary'] = post_pre['summary'].apply(lambda x : 'sostok '+ x + ' eostok')\n#post_pre.head()\n\nfor i in range(3):\n    print(post_pre['text'][i], '\\n')\n    print(post_pre['summary'][i], '\\n\\n') ","9bd7109a":"x_tr,x_val,y_tr,y_val = train_test_split(np.array(post_pre['text']), np.array(post_pre['summary']), test_size=0.1, random_state=0, shuffle=True)\n","a5e2b441":"#prepare a tokenizer for reviews on training data\nx_tokenizer = Tokenizer() \nx_tokenizer.fit_on_texts(list(x_tr))","fb64f3d9":"thresh=4\n\ncnt=0\ntot_cnt=0\nfreq=0\ntot_freq=0\n\nfor key,value in x_tokenizer.word_counts.items():\n    tot_cnt=tot_cnt+1\n    tot_freq=tot_freq+value\n    if(value<thresh):\n        cnt=cnt+1\n        freq=freq+value\n    \nprint(\"% of rare words in vocabulary:\",(cnt\/tot_cnt)*100)\nprint(\"Total Coverage of rare words:\",(freq\/tot_freq)*100)","0b3d9620":"len(x_tokenizer.index_word)","62e35c06":"#prepare a tokenizer for reviews on training data\nx_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \nx_tokenizer.fit_on_texts(list(x_tr))\n\n#convert text sequences into integer sequences (i.e one-hot encodeing all the words)\nx_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \nx_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n\n#padding zero upto maximum length\nx_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\nx_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n\n#size of vocabulary ( +1 for padding token)\nx_voc   =  x_tokenizer.num_words + 1\n\nprint(\"Size of vocabulary in X = {}\".format(x_voc))","9db6c82a":"#prepare a tokenizer for reviews on training data\ny_tokenizer = Tokenizer()   \ny_tokenizer.fit_on_texts(list(y_tr))","40494d12":"thresh=6\n\ncnt=0\ntot_cnt=0\nfreq=0\ntot_freq=0\n\nfor key,value in y_tokenizer.word_counts.items():\n    tot_cnt=tot_cnt+1\n    tot_freq=tot_freq+value\n    if(value<thresh):\n        cnt=cnt+1\n        freq=freq+value\n    \nprint(\"% of rare words in vocabulary:\",(cnt\/tot_cnt)*100)\nprint(\"Total Coverage of rare words:\",(freq\/tot_freq)*100)","62d74a6d":"#prepare a tokenizer for reviews on training data\ny_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \ny_tokenizer.fit_on_texts(list(y_tr))\n\n#convert text sequences into integer sequences (i.e one hot encode the text in Y)\ny_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \ny_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n\n#padding zero upto maximum length\ny_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\ny_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n\n#size of vocabulary\ny_voc  =   y_tokenizer.num_words +1\nprint(\"Size of vocabulary in Y = {}\".format(y_voc))\n","5747de60":"ind=[]\nfor i in range(len(y_tr)):\n    cnt=0\n    for j in y_tr[i]:\n        if j!=0:\n            cnt=cnt+1\n    if(cnt==2):\n        ind.append(i)\n\ny_tr=np.delete(y_tr,ind, axis=0)\nx_tr=np.delete(x_tr,ind, axis=0)","2fe9d6f2":"ind=[]\nfor i in range(len(y_val)):\n    cnt=0\n    for j in y_val[i]:\n        if j!=0:\n            cnt=cnt+1\n    if(cnt==2):\n        ind.append(i)\n\ny_val=np.delete(y_val,ind, axis=0)\nx_val=np.delete(x_val,ind, axis=0)","6800f6f7":"x_tr.shape","2a658281":"y_tr.shape\n","1961bd8e":"x_val.shape","81a716b4":"y_val.shape","22b7c741":"latent_dim = 300\nembedding_dim = 200","766f5255":"# Encoder\nencoder_inputs = Input(shape=(max_text_len,))\n\n#embedding layer\nenc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n\n#encoder lstm 1\nencoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\nencoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n\n#encoder lstm 2\nencoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\nencoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n\n#encoder lstm 3\nencoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\nencoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n\n\n\n# Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs = Input(shape=(None,))\n\n#embedding layer\ndec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\ndec_emb = dec_emb_layer(decoder_inputs)\n\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\ndecoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n\n#dense layer\ndecoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# Define the model \nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","89e54e93":"model.summary()","e1c51d97":"model.compile(optimizer='rmsprop', \n              loss='sparse_categorical_crossentropy')","fff3dcbf":"es = EarlyStopping(monitor='val_loss', \n                   mode='min', \n                   verbose=1,\n                   patience=2)","f7e70868":"history=model.fit([x_tr, y_tr[:,:-1]], \n                  y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:],\n                  epochs=50,\n                  callbacks=[es],\n                  batch_size=128, \n                  validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))\n","d7a3b7c4":"plt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show()","e29c5db9":"reverse_target_word_index=y_tokenizer.index_word\nreverse_source_word_index=x_tokenizer.index_word\ntarget_word_index=y_tokenizer.word_index","ec54f2ec":"# Encode the input sequence to get the feature vector\nencoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n\n# Decoder setup\n# Below tensors will hold the states of the previous time step\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n\n# Get the embeddings of the decoder sequence\ndec_emb2= dec_emb_layer(decoder_inputs) \n# To predict the next word in the sequence, set the initial states to the states from the previous time step\ndecoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n\n# A dense softmax layer to generate prob dist. over the target vocabulary\ndecoder_outputs2 = decoder_dense(decoder_outputs2) \n\n# Final decoder model\ndecoder_model = Model(\n    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n    [decoder_outputs2] + [state_h2, state_c2])","ccd63b95":"target_word_index","821f3135":"def decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    e_out, e_h, e_c = encoder_model.predict(input_seq)\n    \n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1,1))\n    \n    # Populate the first word of target sequence with the start word. \n    target_seq[0, 0] = target_word_index['sostok']\n\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n      \n        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_token = reverse_target_word_index[sampled_token_index]\n        \n        if(sampled_token!='eostok'):\n            decoded_sentence += ' '+sampled_token\n\n        # Exit condition: either hit max length or find stop word.\n        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1,1))\n        target_seq[0, 0] = sampled_token_index\n\n        # Update internal states\n        e_h, e_c = h, c\n\n    return decoded_sentence","0d9c14f2":"def seq2summary(input_seq):\n    newString=''\n    for i in input_seq:\n        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n            newString=newString+reverse_target_word_index[i]+' '\n    return newString\n\ndef seq2text(input_seq):\n    newString=''\n    for i in input_seq:\n        if(i!=0):\n            newString=newString+reverse_source_word_index[i]+' '\n    return newString\n","6b3c7160":"for i in range(0,100):\n    print(\"Review:\",seq2text(x_val[i]))\n    print(\"Original summary:\",seq2summary(y_val[i]))\n    print(\"Predicted summary:\",decode_sequence(x_val[i].reshape(1,max_text_len)))\n    print(\"\\n\")","2c5882f5":"## Training Phase\nIn the training phase, we will first set up the encoder and decoder. We will then train the model to predict the target sequence offset by one timestep. Let us see the details on how to set up the encoder and decoder. ","1911ba2a":"# Model","e071839f":"# There are two types of text summarization\n1. Abstractive Summarization\n2. Extractive Summarization ","4c23f59c":"# Finding word frequency for x_train and x_valid","16ebff03":"# Counting Words","00197a6f":"# Abstractive Summarization\nWe generate new sentences from the original text. This is in contrast to the extractive approach we saw earlier where we used only the sentences that were present. The sentences generated through abstractive summarization might not be present in the origina text. ","f2cf0bc3":"How much attention do we need to pay to every word in the input sequence for generating a word at timestep t? That's the key intuition behind this attention mechanism concept. \n\nLet's consider a simple exmple to understand how Attention Mechanism works. \n\n* Source sequence: \"Which sport do you like the most\"?\n* Target sequence: \"I love cricket\" \n\nThe first word \"I\" in the target sequence is connected to the fourth word \"you\" in the source sequence, right? Similarly, the second-word \"love\" in the target sequence is associated with the fifth word 'like' in the source sequence. \n\nSo, instead of looking at all the words in the source sequence, we can increase the importance o specific parts of the source sequence that result in the target sequence. This is the basic idea-behind attention mechanism. \n\nThere are two different classes of attention mechanism dependsing on the way the attention context vector is derived. \n\n1. Global Attention\n2. Local Attention \n\n","7d78bdec":"### How does the inference process work?\nHere are the steps to decode the test sequence:\n1. Encode the entire input sequence and initialize the decoder with internal states of the encoder\n2. Pass <start> token as an input to the decoder\n3. Run the decoder for one timestep with the internal states\n4. The output will be the probability for the next word. The word with the maximum probability will be selected\n5. Pass the sampled word as an input to the decoder in thenext timestep and update the internal states with the current time step. \n6. Repeat steps 3-5 until we generate <end> token or hit the maximum length of the target sequence\n","6b980c4b":"# Finding word frequency for y_train and y_valid","acf58e02":"# Inference Phase\nAfter training, the model is tested on new source sequence for which the target sequence is unknown. So, we need to setup the inference architecture to decode a test sequence. ","29a1b54e":"# Percent of the word","a4244b72":"### Encoder\nAn Encoder Long Short Term Memory model (LSTM) reads the entire input sequence wherein, at each timestep, one word is fed into the encoder. It then processes the information at every timestep and captures the contextual information present in the input sequence. \n\nI have put together the below diagram which illustrates this process. ","469f0f63":"# Selecting text and summary","e3e8e3fe":"# Tokenizer and pad sequence for y_train and y_valid","983e91de":"<start> and <end> are the special tokens which are added to the target sequence before feeding it into the decoder. The target sequenc is unknown while decoding the test sequence. So, we start predicting the target sequence by passing the first word into the decoder which would be always the <start> token. And the <end> token signals the end of the sentence. ","0eec26d7":"# Limitations of the Encoder \u2013 Decoder Architecture\n\nAs useful as this encoder-decoder architecture is, there are certain limitations that comes with it.\n\n1. The encoder converts the entire input sequence into a fixed length vector and then the decoder predicts the output sequence. This works only for short sequences since the decoder is looking at the entire input sequence for the prediction. \n2. Here comes the problem with long sequences. It is difficult for the encoder to memorize long sequences inot a fixed length vector. \n\nA potential issue with this encoder-decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence in a fixed length vector. This may take it difficult for the neural network to cope with long sentences. The performanc of a basic encoder-decoder deteriorates rapidly as the length of an input sentence increases. \n\n\nSo how do we overcome this problem of long sequence? This is where the concept of attention mechanism comes into to picture. It aims to predict a word by looking at a few specific parts of the sequence only, rather than the entire sequence. It really is as awesome as it sounds! \n","129e69b8":"# Extractive Summarization\n\nWe identify the important sentences of phrases from the original text and extract only those from the text. Thoe extracted sentence would be our summary. The below diagram illustrates extractive summarization. ","db1b3dda":"Let's take an example where the test sequence is given by [x1, x2, x3, x4]. How will the inference process work for this test sequence? I want you to think about it before you look at my thoughts below. \n\n1. Encode the test sequence into intial state vectors\n2. Observe how the decoder predicts the taget sequence at earch timestep. ","bc8c33bf":"# Tokenizer and pad sequence for x_train and x_valid","d2fbaa6f":"# Checking Empty word and removing","99b0f3d1":"Generally, variants of Recurrent Neural Network(RNN), i.e. Gated Recurrent Neural Network(GRU) or Long Short Term Memory(LSTM), are preffered as the encoder and decoder components. This is because they are capable of capturing long term dependencies by overcomming the problem of vanishing gradient. ","2b162dbc":"# How to increase test prediction\n1. Take more training data (I din't take all data because of the limitation of the GPU)  \n2. Use different LSTM for example, Bidirection LSTM and GRU\n3. Use the beam search strategy for decoding the test sequence instead of using the greedy approach (argmax)\n4. Remove stopword\n5. Evaluate the performance using the BLEU score(https:\/\/machinelearningmastery.com\/calculate-bleu-score-for-text-python\/)\n6. Implement pointer-generator networks and coverage mechanisms","0a4b9b2d":"# Splitting dataset","cef41277":"### Local Attention\nHere, the attention is places on only a few sorce positions. Only a few hidden states of the encoder are considered for deriving the attention context vector. ","0ec8e678":"### Globa Atteniton\nHere, the attention is places on all the source positions. In other words, all the hidden states of the encoder are considered for deriving the attention context vector. \n\n","bb23150d":"# Limitations of the Encoder \u2013 Decoder Architecture","dfe30040":"### Decoder\nThe decoder is also an LSTM network which reads the entire target sequence word-by-word and predicts the same sequence offset by one timestep. The decoder the trained to predict the next word in the sequence given the previous word","977ea02a":"# We can set up Encoder-Decoder in 2 phase\n1. Training Phase\n2. Inference Phase ","8d94031f":"# Data Cleaning","ec982fc9":"The hidden state (hi) and cell state (ci) of the last time step are used to initialize the decoder. Remember, this is because the encoder and decoder are two different sets of the LSTM architecture. ","c4ea8a04":"# Understanding the Encoder-Decoder Architecture\nThe Encoder-Decoder architecture is mainly used to solve the sequence-to-sequence problem where the input and output sequences are of different length. \n\nThe input is a long sequence of words and the output will be a short version of the input sequence. "}}