{"cell_type":{"f9838396":"code","ce18fc31":"code","9d7e71e9":"code","254bcb29":"code","64710abb":"code","9351c8ea":"code","28404863":"code","dc099e41":"code","a9c42304":"code","e29ea9ab":"code","9bb35717":"code","95cbf2d7":"code","60eada79":"code","4ef41760":"code","1b1858b5":"code","3b55f26c":"code","165a9949":"code","b4616117":"code","61d697d2":"code","66a726b1":"code","2b7fe670":"code","61de9414":"code","47da5f44":"code","f29b182a":"code","ec5dd802":"code","35480ece":"code","192d2305":"code","38e3118a":"code","f55df321":"code","5a8c1978":"code","8cdb3f98":"code","8eebe440":"code","5ca0483b":"code","505ebcb2":"markdown","89bf7eb4":"markdown","8705688f":"markdown","8f1354a6":"markdown","3f86b62e":"markdown","d67cb543":"markdown","0fd74d90":"markdown","04f70573":"markdown","459748f1":"markdown","59f1b2a7":"markdown","095feb7e":"markdown","0bfc401d":"markdown","f197c31d":"markdown","c20171d8":"markdown","9c477274":"markdown","57f4f3ea":"markdown","02170018":"markdown","ea7d4430":"markdown","fd22d944":"markdown","ca45b265":"markdown","fc65fc08":"markdown"},"source":{"f9838396":"from sklearn.utils import shuffle\n\nimport logging, os\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport os\nimport pandas as pd\nimport random \nimport tensorflow as tf\n\nlogging.disable(logging.WARNING)\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\nplt.rcParams.update({'font.size': 14})\n\n%matplotlib inline","ce18fc31":"tf.config.list_physical_devices('GPU')","9d7e71e9":"base_path = '\/kaggle\/input\/plant-pathology-fgvc78-640px\/train_images\/'","254bcb29":"df = pd.read_csv('\/kaggle\/input\/plant-pathology-fgvc78-640px\/train.csv')\ndf.head(5)","64710abb":"plt.figure(figsize=(18,6))\n\nfor i in range(3):\n    plt.subplot(1, 3, i+1)\n    idx = random.randint(0, len(df)-1)\n    img_path = os.path.join(base_path, df.iloc[idx]['image'])\n    img = mpimg.imread(img_path)\n    \n    plt.grid(False)\n    plt.imshow(img)\n    plt.xlabel(df.iloc[idx]['labels'])\n    \nplt.show()","9351c8ea":"labels = df['labels'].unique()\nfreqs = [ (df['labels'] == label).sum() for label in labels ]\n\nplt.figure(figsize=(16,4))\nplt.barh(range(len(labels)), freqs)\nplt.yticks(range(len(labels)), labels)\n\nplt.show()","28404863":"selected_labels = ['healthy', 'complex', 'frog_eye_leaf_spot', 'powdery_mildew', 'rust', 'scab']\ndf['labels'] = df['labels'].map(lambda x: x if x in selected_labels else 'multiple_diseases')","dc099e41":"labels = df['labels'].unique()\nfreqs = [ (df['labels'] == label).sum() for label in labels ]\n\nplt.figure(figsize=(16, 3))\nplt.barh(range(len(labels)), freqs)\nplt.yticks(range(len(labels)), labels)\n\nplt.show()","a9c42304":"one_hot_features = pd.get_dummies(df['labels'])\ndf = pd.concat((df, one_hot_features), axis=1)\ndf = df.drop(['labels'], axis=1)","e29ea9ab":"print (df.columns)","9bb35717":"df = df[['image', 'healthy', 'complex', 'frog_eye_leaf_spot', 'multiple_diseases', 'powdery_mildew', 'rust', 'scab']]\ndf.head(5)","95cbf2d7":"def create_dataset_from_df(df_x):\n    \n    filenames = df_x['image']\n    labels = df_x.drop('image', axis=1)\n    \n    # Approach #1\n    ds = tf.data.Dataset.from_tensor_slices((filenames, labels.to_numpy()))\n\n    # Approach #2\n#     filename_ds = tf.data.Dataset.from_tensor_slices(filenames)\n#     label_ds = tf.data.Dataset.from_tensor_slices(labels)\n#     ds = tf.data.Dataset.zip((filename_ds, label_ds))\n    \n    return ds","60eada79":"# Prepare and split dataframe\ndf = shuffle(df).reset_index(drop=True)\n\nnum_rows  = len(df)\nnum_train = int(num_rows * 0.8)\nnum_valid = int(num_rows * 0.1)\nnum_test  = num_rows - num_train - num_valid\n\ntrain_df = df.iloc[: num_train]\nvalid_df = df.iloc[num_train : num_train+num_valid]\ntest_df  = df.iloc[num_train+num_valid: ]","4ef41760":"# create dataset\ntrain_ds = create_dataset_from_df(train_df)\nvalid_ds = create_dataset_from_df(valid_df)\ntest_ds  = create_dataset_from_df(test_df)","1b1858b5":"img_height = 128\nimg_width = 128","3b55f26c":"def load_image(filename):\n    img = tf.io.read_file('\/kaggle\/input\/plant-pathology-fgvc78-640px\/train_images\/' + filename)\n    img = tf.io.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, [img_height, img_width])\n    img = tf.math.divide(img, 255.0)\n    \n    return img\n\ndef preprocess(filename, labels):\n    img = load_image(filename)\n    binary_label = tf.gather(labels, 0)\n    multi_label = tf.gather(labels, [1, 2, 3, 4, 5])\n    \n    return img, (binary_label, multi_label)","165a9949":"train_dataset = (train_ds\n                 .cache()          # when data is called, it is cached somewhere in case it is called again\n                 .shuffle(1000)    # Act as buffer. Shuffle when we take data from it. Then, replace buffer with new data.\n                 .map(preprocess)  # convert to feature and labels\n                 .batch(16)\n                 .prefetch(tf.data.AUTOTUNE))  # allow the later element to be ready while we compute the current ones.\n\nvalid_dataset = (valid_ds\n                 .cache()\n                 .map(preprocess)\n                 .batch(16)\n                 .prefetch(tf.data.AUTOTUNE))\n\ntest_dataset = (test_ds\n                 .cache()\n                 .map(preprocess)\n                 .batch(16)\n                 .prefetch(tf.data.AUTOTUNE))","b4616117":"def get_model():\n    inputs = tf.keras.layers.Input(shape=(128, 128, 3))\n\n    x = tf.keras.layers.Conv2D(64, 3, activation='relu')(inputs)\n    x = tf.keras.layers.MaxPool2D(pool_size=3)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    x = tf.keras.layers.Conv2D(16, 3, activation='relu')(inputs)\n    x = tf.keras.layers.MaxPool2D(pool_size=3)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    x = tf.keras.layers.Conv2D(16, 5, activation='relu')(inputs)\n    x = tf.keras.layers.MaxPool2D(pool_size=5)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    x = tf.keras.layers.Flatten()(x)\n    x = tf.keras.layers.Dense(100, activation='relu')(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    \n    output_1 = tf.keras.layers.Dense(1, name='output_1')(x)\n\n    x = tf.keras.layers.Dense(100, activation='relu')(x)\n    output_2 = tf.keras.layers.Dense(5, name='output_2')(x)\n\n    return tf.keras.Model(inputs=inputs, outputs=[output_1, output_2])","61d697d2":"model = get_model()\nmodel.summary()","66a726b1":"def binary_loss_fn(y_true, y_pred):\n    losses = tf.losses.binary_crossentropy(y_true, y_pred, from_logits=True)\n    return tf.reduce_mean(losses)","2b7fe670":"def multi_loss_fn(y_true, y_pred):\n    y_true = tf.cast(y_true, tf.float32)\n    \n    # Approach #1: softmax from scratch\n#     y_pred = tf.nn.softmax(y_pred, axis=1)\n#     losses = tf.negative(tf.math.log(y_pred + 1e-8)) \n#     losses = tf.reduce_sum(losses * y_true, axis=1)\n    \n    # Approach #2: Just use the built-in library \n    losses = tf.nn.softmax_cross_entropy_with_logits(y_true, y_pred)\n\n    # Ignore if the plant is healthy\n    mask = tf.reduce_max(y_true, axis=1)\n    losses = tf.math.multiply(losses, mask)\n\n    return tf.reduce_mean(losses)","61de9414":"def multi_acc_fn(y_true, y_pred):   \n    mask = tf.reduce_max(y_true, axis=1)\n    y_true = tf.argmax(y_true, axis=1)\n    \n    y_pred = tf.nn.softmax(y_pred, axis=1)\n    y_pred = tf.argmax(y_pred, axis=1)\n    \n    scores = tf.cast(y_pred == y_true, tf.float32)\n    scores = scores * mask\n    score = tf.reduce_sum(scores) \/ tf.reduce_sum(mask)\n    \n    return score  # assign less priority for this multi_acc_fn","47da5f44":"model.compile(\n    optimizer='adam', \n    loss={\n        \"output_1\": binary_loss_fn, \n        \"output_2\": multi_loss_fn\n    }, \n    metrics={\n        \"output_1\": \"accuracy\",\n        \"output_2\": multi_acc_fn\n    })","f29b182a":"checkpoint_filepath = '\/checkpoint\/'\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_loss',\n    mode='min',\n    save_best_only=True)","ec5dd802":"train_history = model.fit(\n    train_dataset, \n    validation_data=valid_dataset, \n    epochs=10,\n    callbacks=[model_checkpoint_callback, tf.keras.callbacks.EarlyStopping(patience=3)],\n    verbose=0\n)","35480ece":"train_history.history.keys()","192d2305":"plt.figure(figsize=(16, 4))\n\nplt.subplot(1, 3, 1)\nplt.plot(train_history.history['loss'], label='loss', linestyle='dashed')\nplt.plot(train_history.history['val_loss'], label='val_loss')\nplt.title('Total Loss')\nplt.legend()\n\nplt.subplot(1, 3, 2)\nplt.plot(train_history.history['output_1_loss'], label='output_1_loss', linestyle='dashed')\nplt.plot(train_history.history['val_output_1_loss'], label='val_output_1_loss')\nplt.title('Output_1 Loss')\nplt.legend()\n\nplt.subplot(1, 3, 3)\nplt.plot(train_history.history['output_2_loss'], label='output_2_loss', linestyle='dashed')\nplt.plot(train_history.history['val_output_2_loss'], label='val_output_2_loss')\nplt.title('Output_2 Loss')\nplt.legend()\n\nplt.show()","38e3118a":"model.load_weights(checkpoint_filepath)","f55df321":"model.evaluate(test_dataset)","5a8c1978":"for batch in test_dataset.take(1):\n    x, (y1, y2) = batch\n    y2 = tf.argmax(y2, axis=1)\n    pred = model(x)\n    \n    binary_pred, multi_pred = pred\n    binary_pred = tf.squeeze(tf.cast(tf.math.sigmoid(binary_pred) > 0.5, tf.int32))\n    multi_pred = tf.argmax(tf.math.softmax(multi_pred, axis=1), axis=1)","8cdb3f98":"disease_types = ['complex', 'frog_eye_leaf_spot', 'multiple_diseases', 'powdery_mildew', 'rust', 'scab']","8eebe440":"def display_prediction(i):\n    img = tf.cast(x[i] * 255, tf.int32)\n\n    text  = \"Ground-truth:\\n\"\n    text += \"Healthy\" if (y1[i].numpy().item() == 1) else \"Not Healthy (\" + str(disease_types[y2[i]]) + \")\"\n    text += \"\\n--------\\n\"\n    \n    text += \"Prediction: \\n\"\n    text += \"Healthy\" if (binary_pred[i].numpy().item() == 1) else \"Not Healthy (\" + str(disease_types[multi_pred[i]]) + \")\"\n\n    plt.imshow(img)\n    plt.title(text)","5ca0483b":"plt.figure(figsize=(21, 20))\n\nfor i in range(16):\n    plt.subplot(4, 6, i+1)\n    display_prediction(i)\n\nplt.show()","505ebcb2":"We have two loss functions. \n- One for binary classification class (healthy or not) \n- Another is for multi-label classification (if it's sick, which disease is it?).\n\nNote that when the plant is healthy, obviously there is also no disease. <br\/>\nThat would mean **we don't have to run the multi-label classfication when the plant is healthy**.","89bf7eb4":"## Compile and Train","8705688f":"## Training History","8f1354a6":"## Preprocess Data Labels","3f86b62e":"After the grouping, the distribution is not too bad as before. We will proceed with this data.","d67cb543":"## Metric Function\nWe want to consider the accuracy for the type of disease only if the plant is unhealthy.","0fd74d90":"Let's look at some examples.","04f70573":"# Evaluate","459748f1":"The labels data is in text. We will convert them to **one-hot-encoding**. \n\nIn other word, instead of having label=\"healthy\". \nWe will have label = [1,0,0,0,0,0,0] where 1st element denotes \"healthy\" and 2nd denotes \"complex\" and so on.<br\/>\nSince we have 1 in the 1st element, we can say the label is \"healthy\".\n","59f1b2a7":"The task is not to just identify whether the plant is \"healthy\", \"scrub\", or any other disease. <br\/>\nThere will be two tasks:\n- Is the plant healthy or not?\n- If it is unhealthy, which disease is it?\n\nIn other word, we can treat this as multi-task classification problem.","095feb7e":"# Data Analysis","0bfc401d":"Having multiple output is quite tricky. Apparently, it is not intuitive to zip two outputs together and feed into a single loss function. The loss function will be called one by one even if the argument to loss is `loss=[binary_loss_fn, multi_loss_fn]`.\n\nIn other words, it seems **Functional API** tends to assign **one output to one loss function**.\n\nIn the case we want flexible inputs and outputs of all shapes and size, we might want to look into custom training loop with **GradientTape**.","f197c31d":"# Architecture","c20171d8":"Let's run some bar plot to see the distribution for each class of the disease.","9c477274":"# Data Preprocessing","57f4f3ea":"# Plant Pathology - Disease Classification for Plants\n","02170018":"## Modeling\n\nWe use simple CNN follows by Dense layer. <br\/>\nNote that we don't use Sequential Modeling because such model allows only one output. <br\/>\nOur objective is multi-tasking, so `Functional API` is used.","ea7d4430":"We can load the data (numpy) directly to the model. <br\/>\nHowever, there could be some performance issue if the data is large as we would need to have in the memory.\n\nWe can use `tf.data` to get the data ready (load and pre-processing) in the background. <br\/>\nOnce the model is ready for more data, it can take fetch without having to wait for any preprocessing to be completed.","fd22d944":"## Data Splitting and Pipeline","ca45b265":"From the figure above, we can see that some labels are too few, e.g., \"rust complex\" or \"powdery_mildew complex\". Trying to predict them is difficult because there is not much data to train our model. If we observe closely, these labels are, in fact, a combinations of other diseases. \"rust complex\" = \"rust\" and \"complex\" together. \n\nDue to their low frequencies, we group them into another category, called **multiple_diseases**.","fc65fc08":"## Loss Function"}}