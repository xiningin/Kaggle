{"cell_type":{"5e493f1a":"code","4230ec6a":"code","81e6ad46":"code","2024974a":"code","32c69db6":"code","bbcdf211":"code","314c24e1":"code","005d32b1":"code","29810eb9":"code","79432197":"code","e928b890":"code","012d8ac1":"code","833649b3":"code","fe77a394":"code","f17793b3":"code","38d15993":"code","2637148b":"code","150f19b7":"code","4194f77e":"code","d2176e76":"code","ce441671":"code","c94f7ace":"code","c944895b":"code","20a1edef":"markdown","098ace4d":"markdown","4cfa621b":"markdown","ba825d62":"markdown","70c4b791":"markdown","636027f6":"markdown","286cab70":"markdown","467d1666":"markdown","1fcf66de":"markdown","df8143c2":"markdown","c47937c9":"markdown"},"source":{"5e493f1a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4230ec6a":"import pandas as pd\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom xgboost import XGBClassifier\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n\n\nfpath = \"\/kaggle\/input\/heart-failure-prediction\/heart.csv\"\ndf = pd.read_csv(fpath)\ndf.shape","81e6ad46":"df.head()","2024974a":"df.describe()","32c69db6":"le = LabelEncoder()\ndict_le = {}\ncats = [\"Sex\", \"ChestPainType\", \"RestingECG\", \"ExerciseAngina\", \"ST_Slope\"]\nfor cat in cats:\n    dict_le[cat] = LabelEncoder()\n    df[f\"{cat}_encoded\"] = dict_le[cat].fit_transform(df[cat])\n    \ndf = df.rename(columns={\"HeartDisease\": \"target\"})","bbcdf211":"df.head()","314c24e1":"df_processed = df.drop(cats, axis=1)\ntrain, test = train_test_split(df_processed, test_size=0.2)","005d32b1":"X_train = train.drop(\"target\", axis=1).values\ny_train = train.target.values\n\nX_test = test.drop(\"target\", axis=1).values\ny_test = test.target.values","29810eb9":"model = RandomForestClassifier()\nmodel.fit(X_train, y_train)","79432197":"y_pred = model.predict(X_train)\ny_pred","e928b890":"np.mean(y_train == y_pred)","012d8ac1":"y_pred = model.predict(X_test)\ny_pred","833649b3":"np.mean(y_test == y_pred)","fe77a394":"X_train","f17793b3":"y_train","38d15993":"model = SVC()\nmodel.fit(X_train, y_train)","2637148b":"y_pred = model.predict(X_train)\nnp.mean(y_train == y_pred)","150f19b7":"y_pred = model.predict(X_test)\nnp.mean(y_test == y_pred)","4194f77e":"\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_train)\nnp.mean(y_train == y_pred)","d2176e76":"y_pred = model.predict(X_test)\nnp.mean(y_test == y_pred)","ce441671":"# baseline model\ndef create_baseline():\n    model = Sequential()\n    model.add(Dense(60, input_dim=X_train.shape[1], activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","c94f7ace":"model = create_baseline()\nmodel.fit(X_train, y_train, epochs=100, validation_split=0.1)","c944895b":"y_pred = model.predict(X_test)\ny_pred = [1 if y > 0.5 else 0 for y in y_pred]\nnp.mean(y_test == y_pred)","20a1edef":"We are overfitting a bit (100% on train set vs 86.41% accuracy on test set)","098ace4d":"XGBoost is yielding to the best accuracy without optimizing any hyperparameters and with not much preprocessing","4cfa621b":"## SVM Classifier","ba825d62":"## Importation","70c4b791":"## Keras model","636027f6":"## XGBoost Classifier","286cab70":"Using these hyperparameters, keras model is not better than XGBoost on this dataset.","467d1666":"Trying crossvalidation","1fcf66de":"Binary classification task","df8143c2":"## Preparation","c47937c9":"## Random Forest Classifier"}}