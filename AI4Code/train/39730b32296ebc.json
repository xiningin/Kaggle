{"cell_type":{"7e7f2f25":"code","6236bbe6":"code","d853fafa":"code","465c689f":"code","181aaa8e":"code","59564190":"code","fb200d81":"code","67c40fca":"code","a3933862":"code","6897f277":"code","74c17711":"code","b2923a41":"code","eba495be":"code","62011aa3":"code","682d1a93":"code","5f01c074":"code","ea94f2e7":"code","9b250fba":"code","73da2ef2":"code","058e1dd7":"code","8b3c6012":"code","d3f35fed":"code","83de3d58":"code","ac4ed3c5":"code","af0ffcd9":"code","52a29a0a":"code","ff8a2045":"code","0f560d0f":"code","273b7a4e":"code","0d61110d":"code","a8f8ab10":"code","466e9be0":"code","ed303ee6":"code","4227d294":"code","9c48351a":"code","b7562949":"code","c6280c6c":"code","b36f552d":"code","e4028d45":"code","49f9eb1f":"code","264a69fd":"code","a5ab0b64":"code","d472448c":"code","682d609f":"code","55af1210":"code","7a9c1ce8":"code","d0e6a7bb":"code","101593fd":"code","d9bc43ad":"code","951a39f4":"code","89549b5a":"code","5f2e4d9e":"code","fd9dc4f1":"code","40cd5a26":"code","e5084cf3":"code","cb5869a1":"code","5cb49461":"code","42400342":"code","7885f015":"code","560cd905":"code","90ffcce3":"code","101cc7af":"code","497102c8":"code","d5836afa":"code","6054e677":"code","5fbe31c5":"code","7e675f7b":"code","bf2a4a0c":"code","58bc9dc5":"code","19fee5f6":"markdown","16b86fbe":"markdown","6ca4eb25":"markdown","a7f4c581":"markdown","84806f97":"markdown","f87f2a29":"markdown","bcafde71":"markdown","63692575":"markdown","46d778d5":"markdown","39353437":"markdown","095ddc28":"markdown","27babeee":"markdown","636ff390":"markdown","38ce0c8d":"markdown","81d3a882":"markdown","9d239911":"markdown","eb4ee74e":"markdown","12f60ae3":"markdown","e690ff6b":"markdown","ed3acd81":"markdown","bfbb1488":"markdown","8559cf1e":"markdown","08f9c388":"markdown","ef5e94fa":"markdown","ee65640b":"markdown","41ec260d":"markdown","0507e0e9":"markdown","d6c869eb":"markdown","5244a5f9":"markdown","52bc6f01":"markdown","a40dd657":"markdown","c94d589d":"markdown","f25d8b2c":"markdown","48bc2371":"markdown","b7bb8341":"markdown","215a280d":"markdown","5145f886":"markdown","3019830b":"markdown","d49c49cd":"markdown","55e91ff2":"markdown","87bca1a1":"markdown","8c31eaae":"markdown","05d08f59":"markdown"},"source":{"7e7f2f25":"import sys\nsys.path.append('..\/input\/iterativestratification')\n\nimport numpy as np\nimport random\nimport pandas as pd\nimport os\nimport copy\nimport gc\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.feature_selection import VarianceThreshold, SelectKBest\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport scipy.stats as stats\nfrom scipy.stats import kurtosis\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn.modules.loss import _WeightedLoss\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nos.listdir('..\/input\/lish-moa')\n\npd.set_option('max_columns', 2000)","6236bbe6":"# From optimal commit 9\nn_comp_GENES = 463\nn_comp_CELLS = 60\nVarianceThreshold_for_FS = 0.9\nDropout_Model = 0.25\n#QT_n_quantile_min=50, \n#QT_n_quantile_max=1000,\nprint('n_comp_GENES', n_comp_GENES, 'n_comp_CELLS', n_comp_CELLS, 'total', n_comp_GENES + n_comp_CELLS)","d853fafa":"commits_df = pd.DataFrame(columns = ['n_commit', 'n_comp_GENES', 'n_comp_CELLS', 'train_features','VarianceThreshold_for_FS', 'Dropout_Model', 'LB_score', 'CV_logloss'])","465c689f":"n=0\ncommits_df.loc[n, 'n_commit'] = 0                       # Number of commit\ncommits_df.loc[n, 'n_comp_GENES'] = 600                 # Number of output features for PCA for g-features\ncommits_df.loc[n, 'n_comp_CELLS'] = 50                  # Number of output features for PCA for c-features\ncommits_df.loc[n, 'VarianceThreshold_for_FS'] = 0.8     # Threshold for VarianceThreshold for feature selection\ncommits_df.loc[n, 'train_features'] = 1245              # Number features in the training dataframe after FE and before modeling\ncommits_df.loc[n, 'Dropout_Model'] = 0.2619422201258426 # Dropout in Model\ncommits_df.loc[n, 'CV_logloss'] = 0.01458269555140327   # Result CV logloss metrics\ncommits_df.loc[n, 'LB_score'] = 0.01839                 # LB score after submitting","181aaa8e":"n=1\ncommits_df.loc[n, 'n_commit'] = 4\ncommits_df.loc[n, 'n_comp_GENES'] = 610\ncommits_df.loc[n, 'n_comp_CELLS'] = 55\ncommits_df.loc[n, 'VarianceThreshold_for_FS'] = 0.82\ncommits_df.loc[n, 'train_features'] = 1240\ncommits_df.loc[n, 'Dropout_Model'] = 0.25\ncommits_df.loc[n, 'CV_logloss'] =  0.014584545081734047\ncommits_df.loc[n, 'LB_score'] = 0.01839","59564190":"n=2\ncommits_df.loc[n, 'n_commit'] = 5\ncommits_df.loc[n, 'n_comp_GENES'] = 670\ncommits_df.loc[n, 'n_comp_CELLS'] = 67\ncommits_df.loc[n, 'VarianceThreshold_for_FS'] = 0.67\ncommits_df.loc[n, 'train_features'] = 1298\ncommits_df.loc[n, 'Dropout_Model'] = 0.25\ncommits_df.loc[n, 'CV_logloss'] =  0.014588561242139069\ncommits_df.loc[n, 'LB_score'] = 0.01840","fb200d81":"n=3\ncommits_df.loc[n, 'n_commit'] = 6\ncommits_df.loc[n, 'n_comp_GENES'] = 450\ncommits_df.loc[n, 'n_comp_CELLS'] = 45\ncommits_df.loc[n, 'VarianceThreshold_for_FS'] = 0.67\ncommits_df.loc[n, 'train_features'] = 1297\ncommits_df.loc[n, 'Dropout_Model'] = 0.25\ncommits_df.loc[n, 'CV_logloss'] =  0.014586229676302227\ncommits_df.loc[n, 'LB_score'] = 0.01840","67c40fca":"n=4\ncommits_df.loc[n, 'n_commit'] = 9\ncommits_df.loc[n, 'n_comp_GENES'] = 463\ncommits_df.loc[n, 'n_comp_CELLS'] = 60\ncommits_df.loc[n, 'VarianceThreshold_for_FS'] = 0.9\ncommits_df.loc[n, 'train_features'] = 1219\ncommits_df.loc[n, 'Dropout_Model'] = 0.25\ncommits_df.loc[n, 'CV_logloss'] =  0.014572358066092783\ncommits_df.loc[n, 'LB_score'] = 0.01839","a3933862":"n=5\ncommits_df.loc[n, 'n_commit'] = 10\ncommits_df.loc[n, 'n_comp_GENES'] = 463\ncommits_df.loc[n, 'n_comp_CELLS'] = 80\ncommits_df.loc[n, 'VarianceThreshold_for_FS'] = 0.92\ncommits_df.loc[n, 'train_features'] = 1214\ncommits_df.loc[n, 'Dropout_Model'] = 0.25\ncommits_df.loc[n, 'CV_logloss'] =  0.014571552074579226\ncommits_df.loc[n, 'LB_score'] = 0.01841","6897f277":"n=6\ncommits_df.loc[n, 'n_commit'] = 12\ncommits_df.loc[n, 'n_comp_GENES'] = 450\ncommits_df.loc[n, 'n_comp_CELLS'] = 65\ncommits_df.loc[n, 'VarianceThreshold_for_FS'] = 0.9\ncommits_df.loc[n, 'train_features'] = 1219\ncommits_df.loc[n, 'Dropout_Model'] = 0.25\ncommits_df.loc[n, 'CV_logloss'] = 0.01458043214513875\ncommits_df.loc[n, 'LB_score'] = 0.01840","74c17711":"n=7\ncommits_df.loc[n, 'n_commit'] = 13\ncommits_df.loc[n, 'n_comp_GENES'] = 463\ncommits_df.loc[n, 'n_comp_CELLS'] = 60\ncommits_df.loc[n, 'VarianceThreshold_for_FS'] = 0.9\ncommits_df.loc[n, 'train_features'] = 1219\ncommits_df.loc[n, 'Dropout_Model'] = 0.4\ncommits_df.loc[n, 'CV_logloss'] = 0.014625250378417162\ncommits_df.loc[n, 'LB_score'] = 0.01844","b2923a41":"n=8\ncommits_df.loc[n, 'n_commit'] = 14\ncommits_df.loc[n, 'n_comp_GENES'] = 463\ncommits_df.loc[n, 'n_comp_CELLS'] = 60\ncommits_df.loc[n, 'VarianceThreshold_for_FS'] = 0.01\ncommits_df.loc[n, 'train_features'] = 1604\ncommits_df.loc[n, 'Dropout_Model'] = 0.25\ncommits_df.loc[n, 'CV_logloss'] = 0.014713482787703418\ncommits_df.loc[n, 'LB_score'] = 0.01849","eba495be":"n=9\ncommits_df.loc[n, 'n_commit'] = 18\ncommits_df.loc[n, 'n_comp_GENES'] = 363\ncommits_df.loc[n, 'n_comp_CELLS'] = 60\ncommits_df.loc[n, 'VarianceThreshold_for_FS'] = 0.9\ncommits_df.loc[n, 'train_features'] = 1219\ncommits_df.loc[n, 'Dropout_Model'] = 0.25\ncommits_df.loc[n, 'CV_logloss'] = 0.014568689235607534\ncommits_df.loc[n, 'LB_score'] = 0.01841","62011aa3":"n=10\ncommits_df.loc[n, 'n_commit'] = 19\ncommits_df.loc[n, 'n_comp_GENES'] = 550\ncommits_df.loc[n, 'n_comp_CELLS'] = 60\ncommits_df.loc[n, 'VarianceThreshold_for_FS'] = 0.91\ncommits_df.loc[n, 'train_features'] = 1218\ncommits_df.loc[n, 'Dropout_Model'] = 0.25\ncommits_df.loc[n, 'CV_logloss'] = 0.014577509066710863\ncommits_df.loc[n, 'LB_score'] = 0.01841","682d1a93":"n=11\ncommits_df.loc[n, 'n_commit'] = 20\ncommits_df.loc[n, 'n_comp_GENES'] = 463\ncommits_df.loc[n, 'n_comp_CELLS'] = 60\ncommits_df.loc[n, 'VarianceThreshold_for_FS'] = 0.9\ncommits_df.loc[n, 'train_features'] = 1218\ncommits_df.loc[n, 'Dropout_Model'] = 0.25\ncommits_df.loc[n, 'CV_logloss'] = 0.014572358066092783\ncommits_df.loc[n, 'LB_score'] = 0.01839","5f01c074":"# Commits 0-20\ncommits_df['QT_n_quantile_max'] = 100","ea94f2e7":"n=12\ncommits_df.loc[n, 'n_commit'] = 21\ncommits_df.loc[n, 'n_comp_GENES'] = 463\ncommits_df.loc[n, 'n_comp_CELLS'] = 60\ncommits_df.loc[n, 'VarianceThreshold_for_FS'] = 0.9\ncommits_df.loc[n, 'train_features'] = 1223\ncommits_df.loc[n, 'Dropout_Model'] = 0.25\ncommits_df.loc[n, 'QT_n_quantile_max'] = 200\ncommits_df.loc[n, 'CV_logloss'] = 0.014585887029392697\ncommits_df.loc[n, 'LB_score'] = 0.01841","9b250fba":"n=13\ncommits_df.loc[n, 'n_commit'] = 22\ncommits_df.loc[n, 'n_comp_GENES'] = 463\ncommits_df.loc[n, 'n_comp_CELLS'] = 60\ncommits_df.loc[n, 'VarianceThreshold_for_FS'] = 0.9\ncommits_df.loc[n, 'train_features'] = 1224\ncommits_df.loc[n, 'Dropout_Model'] = 0.25\ncommits_df.loc[n, 'QT_n_quantile_max'] = 500\ncommits_df.loc[n, 'CV_logloss'] = 0.014572447523411875\ncommits_df.loc[n, 'LB_score'] = 0.01840","73da2ef2":"n=14\ncommits_df.loc[n, 'n_commit'] = 23\ncommits_df.loc[n, 'n_comp_GENES'] = 463\ncommits_df.loc[n, 'n_comp_CELLS'] = 60\ncommits_df.loc[n, 'VarianceThreshold_for_FS'] = 0.9\ncommits_df.loc[n, 'train_features'] = 1212\ncommits_df.loc[n, 'Dropout_Model'] = 0.25\ncommits_df.loc[n, 'QT_n_quantile_max'] = 50\ncommits_df.loc[n, 'CV_logloss'] = 0.014581633680902033\ncommits_df.loc[n, 'LB_score'] = 0.01840","058e1dd7":"# Commits 0-23\ncommits_df['QT_n_quantile_min'] = commits_df['QT_n_quantile_max']","8b3c6012":"n=15\ncommits_df.loc[n, 'n_commit'] = 24\ncommits_df.loc[n, 'n_comp_GENES'] = 463\ncommits_df.loc[n, 'n_comp_CELLS'] = 60\ncommits_df.loc[n, 'VarianceThreshold_for_FS'] = 0.84\ncommits_df.loc[n, 'train_features'] = 1215\ncommits_df.loc[n, 'Dropout_Model'] = 0.25\ncommits_df.loc[n, 'QT_n_quantile_min'] = 10\ncommits_df.loc[n, 'QT_n_quantile_max'] = 200\ncommits_df.loc[n, 'CV_logloss'] = 0.014578913453054567\ncommits_df.loc[n, 'LB_score'] = 0.01840","d3f35fed":"n=15\ncommits_df.loc[n, 'n_commit'] = 24\ncommits_df.loc[n, 'n_comp_GENES'] = 463\ncommits_df.loc[n, 'n_comp_CELLS'] = 60\ncommits_df.loc[n, 'VarianceThreshold_for_FS'] = 0.9\ncommits_df.loc[n, 'train_features'] = 1223\ncommits_df.loc[n, 'Dropout_Model'] = 0.25\ncommits_df.loc[n, 'QT_n_quantile_min'] = 50\ncommits_df.loc[n, 'QT_n_quantile_max'] = 1000\ncommits_df.loc[n, 'CV_logloss'] = 0.014576369890002664\ncommits_df.loc[n, 'LB_score'] = 0.01842","83de3d58":"commits_df['n_comp_total'] = commits_df['n_comp_GENES'] + commits_df['n_comp_CELLS']\ncommits_df['seed'] = 42","ac4ed3c5":"commits_df['l_rate'] = 1e-3\ncommits_df.loc[11, 'l_rate'] = 5e-4","af0ffcd9":"# Find and mark minimun value of LB score\ncommits_df['LB_score'] = pd.to_numeric(commits_df['LB_score'])\ncommits_df = commits_df.sort_values(by=['LB_score', 'CV_logloss'], ascending = True).reset_index(drop=True)\ncommits_df['min'] = 0\ncommits_df.loc[0, 'min'] = 1\ncommits_df","52a29a0a":"commits_df.sort_values(by=['CV_logloss'], ascending = True)","ff8a2045":"# Interactive plot with results of parameters tuning\nfig = px.scatter_3d(commits_df, x='n_comp_GENES', y='n_comp_CELLS', z='LB_score', color = 'min', \n                    symbol = 'Dropout_Model',\n                    title='Parameters and LB score visualization of MoA solutions')\nfig.update(layout=dict(title=dict(x=0.1)))","0f560d0f":"# Interactive plot with results of parameters tuning\nfig = px.scatter_3d(commits_df, x='train_features', y='VarianceThreshold_for_FS', z='LB_score', color = 'min', \n                    symbol = 'l_rate',\n                    title='Parameters and LB score visualization of MoA solutions')\nfig.update(layout=dict(title=dict(x=0.1)))","273b7a4e":"# Interactive plot with results of parameters tuning\nfig = px.scatter_3d(commits_df, x='train_features', y='CV_logloss', z='LB_score', color = 'min', \n                    symbol = 'l_rate',\n                    title='Parameters and LB score visualization of MoA solutions')\nfig.update(layout=dict(title=dict(x=0.1)))","0d61110d":"# Interactive plot with results of parameters tuning\ncommits_df_1841 = commits_df[commits_df.LB_score <= 0.01841]\nfig = px.scatter_3d(commits_df_1841, x='train_features', y='CV_logloss', z='LB_score', color = 'min', \n                    symbol = 'l_rate',\n                    title='Parameters and LB score visualization of MoA solutions')\nfig.update(layout=dict(title=dict(x=0.1)))","a8f8ab10":"# Interactive plot with results of parameters tuning\ncommits_df_1840 = commits_df[commits_df.LB_score <= 0.01840]\nfig = px.scatter_3d(commits_df_1840, x='QT_n_quantile_max', y='train_features', z='LB_score', color = 'min', \n                    symbol = 'seed',\n                    title='Parameters and LB score visualization of MoA solutions')\nfig.update(layout=dict(title=dict(x=0.1)))","466e9be0":"# Interactive plot with results of parameters tuning\ncommits_df_1842 = commits_df[commits_df.LB_score <= 0.01842]\nfig = px.scatter_3d(commits_df_1842, x='QT_n_quantile_min', y='QT_n_quantile_max', z='LB_score', color = 'min',\n                    title='Parameters and LB score visualization of MoA solutions')\nfig.update(layout=dict(title=dict(x=0.1)))","ed303ee6":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","4227d294":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","9c48351a":"# Search for minimum and maximum values\n# df_kurt = pd.DataFrame(columns=['col','train', 'test'])\n# i = 0\n# for col in (GENES + CELLS):\n#     df_kurt.loc[i, 'col'] = col\n#     df_kurt.loc[i, 'train'] = kurtosis(train_features[col])\n#     df_kurt.loc[i, 'test'] = kurtosis(test_features[col])\n#     i += 1\n# print(df_kurt.min())\n# print(df_kurt.max())","b7562949":"def calc_QT_par_kurt(QT_n_quantile_min=10, QT_n_quantile_max=200):\n    # Calculation parameters of function: n_quantile(kurtosis) = k1*kurtosis + k0\n    # For Train & Test datasets (GENES + CELLS features): minimum kurtosis = 1.53655, maximum kurtosis = 30.4929\n    \n    a = np.array([[1.53655,1], [30.4929,1]])\n    b = np.array([QT_n_quantile_min, QT_n_quantile_max])\n    \n    return np.linalg.solve(a, b)","c6280c6c":"def n_quantile_for_kurt(kurt, calc_QT_par_kurt_transform):\n    # Calculation parameters of function: n_quantile(kurtosis) = calc_QT_par_kurt_transform[0]*kurtosis + calc_QT_par_kurt_transform[1]\n    return int(calc_QT_par_kurt_transform[0]*kurt + calc_QT_par_kurt_transform[1])","b36f552d":"# RankGauss - transform to Gauss\n\nfor col in (GENES + CELLS):\n\n    #kurt = max(kurtosis(train_features[col]), kurtosis(test_features[col]))\n    #QuantileTransformer_n_quantiles = n_quantile_for_kurt(kurt, calc_QT_par_kurt(QT_n_quantile_min, QT_n_quantile_max))\n    #transformer = QuantileTransformer(n_quantiles=QuantileTransformer_n_quantiles,random_state=0, output_distribution=\"normal\")\n    \n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")   # from optimal commit 9\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","e4028d45":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","49f9eb1f":"len(GENES)","264a69fd":"# GENES\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (PCA(n_components=n_comp_GENES, random_state=42).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp_GENES)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp_GENES)])\n\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","a5ab0b64":"len(CELLS)","d472448c":"# CELLS\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp_CELLS, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp_CELLS)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp_CELLS)])\n\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","682d609f":"train_features.shape","55af1210":"train_features.head(5)","7a9c1ce8":"data = train_features.append(test_features)\ndata","d0e6a7bb":"var_thresh = VarianceThreshold(VarianceThreshold_for_FS)\ndata = train_features.append(test_features)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\ntrain_features.shape","101593fd":"train_features.head(5)","d9bc43ad":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntarget = train[train_targets_scored.columns]","951a39f4":"train = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","89549b5a":"train.head(5)","5f2e4d9e":"target_cols = target.drop('sig_id', axis=1).columns.values.tolist()","fd9dc4f1":"folds = train.copy()\n\nmskf = MultilabelStratifiedKFold(n_splits=7)\n\nfor f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n    folds.loc[v_idx, 'kfold'] = int(f)\n\nfolds['kfold'] = folds['kfold'].astype(int)\nfolds","40cd5a26":"print(train.shape)\nprint(folds.shape)\nprint(test.shape)\nprint(target.shape)\nprint(sample_submission.shape)","e5084cf3":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct    ","cb5869a1":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss \/= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds","5cb49461":"class SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","42400342":"def process_data(data):\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data","7885f015":"feature_cols = [c for c in process_data(folds).columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\nlen(feature_cols)","560cd905":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 25\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 7\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=1500","90ffcce3":"class Model(nn.Module):\n    \n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(Dropout_Model)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(Dropout_Model)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x\n    \n    \nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing \/ (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))    ","101cc7af":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = process_data(folds)\n    test_ = process_data(test)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n    \n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n   \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    \n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[val_idx] = valid_preds\n            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n        \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n\n    )\n    \n    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions","497102c8":"def run_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training(fold, seed)\n        \n        predictions += pred_ \/ NFOLDS\n        oof += oof_\n        \n    return oof, predictions","d5836afa":"# Averaging on multiple SEEDS\n\nSEED = [0, 1, 2, 3, 4, 5, 6]\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    \n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ \/ len(SEED)\n    predictions += predictions_ \/ len(SEED)\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions\n","6054e677":"train_targets_scored","5fbe31c5":"len(target_cols)","7e675f7b":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ \/ target.shape[1]\n    \nprint(\"CV log_loss: \", score)    ","bf2a4a0c":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","58bc9dc5":"sub.shape","19fee5f6":"### It is recommended:\n* **n_comp_GENES** smaller, \n* **n_comp_CELLS** more,\n* **VarianceThreshold_for_FS** more, so that **train_features** is less.","16b86fbe":"# Acknowledgements\n\n* [MoA | Pytorch | 0.01859 | RankGauss | PCA | NN](https:\/\/www.kaggle.com\/kushal1506\/moa-pytorch-0-01859-rankgauss-pca-nn)\n* [[MoA] Pytorch NN+PCA+RankGauss](https:\/\/www.kaggle.com\/nayuts\/moa-pytorch-nn-pca-rankgauss)\n* [Pytorch CV|0.0145| LB| 0.01839 |](https:\/\/www.kaggle.com\/riadalmadani\/pytorch-cv-0-0145-lb-0-01839)\n* [[New Baseline] Pytorch | MoA](https:\/\/www.kaggle.com\/namanj27\/new-baseline-pytorch-moa)\n* [Deciding (n_components) in PCA](https:\/\/www.kaggle.com\/kushal1506\/deciding-n-components-in-pca)\n* [Titanic - Featuretools (automatic FE&FS)](https:\/\/www.kaggle.com\/vbmokin\/titanic-featuretools-automatic-fe-fs)\n* tuning and visualization from [Higher LB score by tuning mloss - upgrade & visual](https:\/\/www.kaggle.com\/vbmokin\/higher-lb-score-by-tuning-mloss-upgrade-visual)\n* [Data Science for tabular data: Advanced Techniques](https:\/\/www.kaggle.com\/vbmokin\/data-science-for-tabular-data-advanced-techniques)","6ca4eb25":"### 4.4 Feature selection<a class=\"anchor\" id=\"4.4\"><\/a>\n\n[Back to Table of Contents](#0.1)","a7f4c581":"### Commit 25","84806f97":"<a class=\"anchor\" id=\"0\"><\/a>\n# [Mechanisms of Action (MoA) Prediction](https:\/\/www.kaggle.com\/c\/lish-moa)","f87f2a29":"### 2.3 Parameters and LB score visualization <a class=\"anchor\" id=\"2.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","bcafde71":"## 6. Prediction & Submission <a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","63692575":"## 1. Import libraries<a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","46d778d5":"## 5. Modeling<a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","39353437":"### Commit 5","095ddc28":"## 3. Download data<a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","27babeee":"### 4.8 Preprocessing<a class=\"anchor\" id=\"4.8\"><\/a>\n\n[Back to Table of Contents](#0.1)","636ff390":"## 2. My upgrade <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","38ce0c8d":"### 4.5 CV folds<a class=\"anchor\" id=\"4.5\"><\/a>\n\n[Back to Table of Contents](#0.1)","81d3a882":"### Commit 23","9d239911":"### Commit 22","eb4ee74e":"### Commit 4","12f60ae3":"### 4.6 Dataset Classes<a class=\"anchor\" id=\"4.6\"><\/a>\n\n[Back to Table of Contents](#0.1)","e690ff6b":"### 2.1. Commit now <a class=\"anchor\" id=\"2.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","ed3acd81":"As already noted, **LEARNING_RATE** is adaptively adjusted. Therefore, it makes no sense to tune it - the result is the same.","bfbb1488":"### Commit 10","8559cf1e":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [My upgrade](#2)\n    -  [Commit now](#2.1)\n    -  [Previous commits](#2.2)\n    -  [Parameters and LB score visualization](#2.3)\n1. [Download data](#3)\n1. [FE & Data Preprocessing](#4)\n    - [RankGauss](#4.1)\n    - [Seed](#4.2)    \n    - [PCA features](#4.3)\n    - [Feature selection](#4.4)\n    - [CV folds](#4.5)\n    - [Dataset Classes](#4.6)\n    - [Smoothing](#4.7)\n    - [Preprocessing](#4.8)\n1. [Modeling](#5)\n1. [Prediction & Submission](#6)","08f9c388":"### Commit 14","ef5e94fa":"## My Conclusions in posts:\n\n* [FE for Pytorch-RankGauss-PCA-NN model with LB=0.01839](https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/194345)\n* [FE : VarianceThreshold - what else is there?](https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/194973)\n* [QuantileTransformer parameters tuning](https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/195788)","ee65640b":"### Commit 21","41ec260d":"### Commit 24","0507e0e9":"### Commit 20","d6c869eb":"### 4.1 RankGauss<a class=\"anchor\" id=\"4.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","5244a5f9":"### Commit 13","52bc6f01":"### Commit 9","a40dd657":"### 4.2 Seed<a class=\"anchor\" id=\"4.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","c94d589d":"## 4. FE & Data Preprocessing <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","f25d8b2c":"[Go to Top](#0)","48bc2371":"### Commit 19","b7bb8341":"### 4.3 PCA features<a class=\"anchor\" id=\"4.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","215a280d":"### Commit 12","5145f886":"## My upgrade:\n\n* PCA parameters\n* Feature Selection methods, including QuantileTransformer tuning\n* Dropout\n* Structuring of the notebook\n* Tuning visualization\n* Number of folds\n\nI used the code from sources (please see above). \n\nI have completed the improvement of this notebook. Commit 9 is optimal.\n\nI switch to improving my private notebook and other tasks.","3019830b":"### Commit 0 (parameters from https:\/\/www.kaggle.com\/riadalmadani\/pytorch-cv-0-0145-lb-0-01839, commit 8)","d49c49cd":"### 4.7 Smoothing<a class=\"anchor\" id=\"4.7\"><\/a>\n\n[Back to Table of Contents](#0.1)","55e91ff2":"### Commit 6","87bca1a1":"### 2.2 Previous commits <a class=\"anchor\" id=\"2.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","8c31eaae":"### Commit 18","05d08f59":"### I use the notebook [Pytorch CV|0.0145| LB| 0.01839 |](https:\/\/www.kaggle.com\/riadalmadani\/pytorch-cv-0-0145-lb-0-01839) from [riadalmadani](https:\/\/www.kaggle.com\/riadalmadani) as a basis and will try to tune its various parameters. "}}