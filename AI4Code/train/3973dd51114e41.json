{"cell_type":{"c4dab88f":"code","6161c14d":"code","f135cb2b":"code","4664c3ef":"code","7b11210d":"code","fdf832aa":"code","bc6a9bad":"code","2db22923":"code","c7982b11":"code","cac1c5e4":"code","500fef7d":"code","66b8e44f":"code","8d9be08c":"code","4d4bb587":"code","02658383":"code","d41c5f36":"code","33a6a444":"code","c54bc728":"code","a82b13fe":"code","a21d68c9":"code","b94d1519":"code","e2b6333d":"code","e94bf657":"code","47ae5b69":"code","3ec4065c":"code","caec5951":"code","d8ee7f00":"code","77b41082":"code","020a1394":"code","9a32808a":"code","62f2aa2d":"code","e6aef720":"code","147c530a":"code","adb1ac06":"code","b794a1e0":"code","376497f3":"code","c04df961":"code","aaa11c26":"code","dde32a95":"code","f76e0f3f":"code","d8b100f1":"code","8c083363":"code","f8ef70e2":"code","6fc576f3":"code","ba75d485":"code","60a768b9":"code","2ef48eb3":"code","5fde6314":"code","f3be6af6":"code","2f87dc9e":"code","17030bd9":"code","2e252a8b":"code","5306fb29":"code","49d49a54":"code","3c6366a9":"code","73c01dc7":"code","3ecca6ce":"code","0f8795ec":"code","704e3ca9":"code","b0ce7e0d":"code","c122e3a3":"code","64f6d385":"code","7780f77a":"code","1b31048f":"code","e4ddbda0":"code","fe4bdb1c":"markdown","a4a936db":"markdown","3718a865":"markdown","be40a0fb":"markdown","e8039cc3":"markdown","99711543":"markdown","1f332757":"markdown","d5e74788":"markdown","7b5e486a":"markdown","94c4b435":"markdown","9dd25c3e":"markdown","028b717b":"markdown","cc379459":"markdown","f239977f":"markdown","3b57af4f":"markdown","8907db94":"markdown","b0d43565":"markdown","964cd4c2":"markdown","e7c29f3d":"markdown","d4aae288":"markdown","8d93294c":"markdown","4acd5400":"markdown","fec203ab":"markdown","30658aae":"markdown","264ab2d5":"markdown","ec5bb78f":"markdown","ddff0953":"markdown","8dd40dd0":"markdown","d4ed86ad":"markdown","711d5127":"markdown","45e9a857":"markdown","b889ca5f":"markdown","af948456":"markdown","c3a4222e":"markdown","af3e56b3":"markdown","a1217b40":"markdown","0ce948b3":"markdown","6bffff14":"markdown","ae90d574":"markdown","186f7be3":"markdown","66967871":"markdown","1fb4a814":"markdown","bd6e4f29":"markdown","a24908b8":"markdown","94e73808":"markdown","2f9bfa09":"markdown","655d2885":"markdown","d8e75da5":"markdown","af1b95a3":"markdown","4b695f1c":"markdown","bdad92a2":"markdown","3189d5bc":"markdown","c496bee6":"markdown","83f4d6ee":"markdown","b67a7774":"markdown","41b9c958":"markdown","7e0e3dbe":"markdown","a80b11a5":"markdown","8a7f361b":"markdown","5c812a15":"markdown","73cc0b09":"markdown","1a1a3750":"markdown","7969b2a1":"markdown","dcb50668":"markdown"},"source":{"c4dab88f":"# importing libraires\n%matplotlib inline                 \n\nimport pandas as pd                # Implemennts milti-dimensional array and matrices\nimport numpy as np                 # For data manipulation and analysis\nimport matplotlib.pyplot as plt    # Plotting library for Python programming language and it's numerical mathematics extension NumPy\nimport seaborn as sns              # Provides a high level interface for drawing attractive and informative statistical graphics\n\n\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nlabel=LabelEncoder()\n\n\n\nfrom catboost import CatBoostClassifier   # import algorithms for model training ","6161c14d":"from sklearn.preprocessing import Imputer\n\ntrain_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\n\ntrain_df.head()","f135cb2b":"#As test has only one missing value so lets fill it..\ntest_df.Fare.fillna(test_df.Fare.mean(), inplace=True)\ndata_df = train_df.append(test_df) # The entire data: train + test.\npassenger_id=test_df['PassengerId']\n\n## We will drop PassengerID and Ticket since it will be useless for our data. \ntrain_df.drop(['PassengerId'], axis=1, inplace=True)\ntest_df.drop(['PassengerId'], axis=1, inplace=True)\ntest_df.shape","4664c3ef":"print (train_df.isnull().sum())\nprint (''.center(20, \"*\"))\nprint (test_df.isnull().sum())\nsns.boxplot(x='Survived',y='Fare',data=train_df)","7b11210d":"train_df=train_df[train_df['Fare']<400]","fdf832aa":"train_df['Sex'] = train_df.Sex.apply(lambda x: 0 if x == \"female\" else 1)\ntest_df['Sex'] = test_df.Sex.apply(lambda x: 0 if x == \"female\" else 1)","bc6a9bad":"train_df","2db22923":"pd.options.display.max_columns = 99\ntest_df['Fare'].fillna(test_df['Fare'].mean(),inplace=True)\ntrain_df.head()\n","c7982b11":"for name_string in data_df['Name']:\n    data_df['Title']=data_df['Name'].str.extract('([A-Za-z]+)\\.',expand=True)\n\n\nmapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\ndata_df.replace({'Title': mapping}, inplace=True)","cac1c5e4":"data_df.groupby('Title')['Age'].median()","500fef7d":"# imputing with the mean() strategy.\n\nfor name_string in data_df['Name']:\n    data_df['Title']=data_df['Name'].str.extract('([A-Za-z]+)\\.',expand=True)\n    \n#replacing the rare title with more common one.\n# here the mapping for different title is done through finding relation between two columns. (Sex,Title)\n\n#pd.crosstab(data_train['Title'],data_train['Sex']) (try this out)\n\nmapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\ndata_df.replace({'Title': mapping}, inplace=True)\n\ndata_df['Title'].value_counts()\ntrain_df['Title']=data_df['Title'][:891]\ntest_df['Title']=data_df['Title'][891:]\n\ntitles=['Mr','Miss','Mrs','Master','Rev','Dr']\nfor title in titles:\n    age_to_impute = data_df.groupby('Title')['Age'].mean()[titles.index(title)]\n    #print(age_to_impute)\n    data_df.loc[(data_df['Age'].isnull()) & (data_df['Title'] == title), 'Age'] = age_to_impute\ndata_df.isnull().sum()\n\n\n\ntrain_df['Age']=data_df['Age'][:891]\ntest_df['Age']=data_df['Age'][891:]\ntest_df.isnull().sum()","66b8e44f":"train_df.describe()","8d9be08c":"train_df.groupby('Survived').mean()","4d4bb587":"train_df.groupby('Sex').mean()","02658383":"train_df.corr()","d41c5f36":"plt.subplots(figsize = (15,8))\nsns.heatmap(train_df.corr(), annot=True,cmap=\"PiYG\")\nplt.title(\"Correlations Among Features\", fontsize = 18)","33a6a444":"plt.subplots(figsize = (15,8))\nsns.barplot(x = \"Sex\", y = \"Survived\", data=train_df, edgecolor=(0,0,0), linewidth=2)\nplt.title(\"Survived\/Non-Survived Passenger Gender Distribution\", fontsize = 25)\nlabels = ['Female', 'Male']\nplt.ylabel(\"% of passenger survived\", fontsize = 15)\nplt.xlabel(\"Gender\",fontsize = 15)\nplt.xticks(sorted(train_df.Sex.unique()), labels)\n\n# 1 is for male and 0 is for female.","c54bc728":"sns.set(style='darkgrid')\nplt.subplots(figsize = (15,8))\nax=sns.countplot(x='Sex',data=train_df,hue='Survived',edgecolor=(0,0,0),linewidth=2)\ntrain_df.shape\n## Fixing title, xlabel and ylabel\nplt.title('Passenger distribution of survived vs not-survived',fontsize=25)\nplt.xlabel('Gender',fontsize=15)\nplt.ylabel(\"# of Passenger Survived\", fontsize = 15)\nlabels = ['Female', 'Male']\n#Fixing xticks.\nplt.xticks(sorted(train_df.Survived.unique()),labels)\n## Fixing legends\nleg = ax.get_legend()\nleg.set_title('Survived')\nlegs=leg.texts\nlegs[0].set_text('No')\nlegs[1].set_text('Yes')\n","a82b13fe":"plt.subplots(figsize = (8,8))\nax=sns.countplot(x='Pclass',hue='Survived',data=train_df)\nplt.title(\"Passenger Class Distribution - Survived vs Non-Survived\", fontsize = 25)\nleg=ax.get_legend()\nleg.set_title('Survival')\nlegs=leg.texts\n\nlegs[0].set_text('No')\nlegs[1].set_text(\"yes\")","a21d68c9":"plt.subplots(figsize=(10,8))\nsns.kdeplot(train_df.loc[(train_df['Survived'] == 0),'Pclass'],shade=True,color='r',label='Not Survived')\nax=sns.kdeplot(train_df.loc[(train_df['Survived'] == 1),'Pclass'],shade=True,color='b',label='Survived' )\n\nlabels = ['First', 'Second', 'Third']\nplt.xticks(sorted(train_df.Pclass.unique()),labels)","b94d1519":"plt.subplots(figsize=(15,10))\n\nax=sns.kdeplot(train_df.loc[(train_df['Survived'] == 0),'Fare'],color='r',shade=True,label='Not Survived')\nax=sns.kdeplot(train_df.loc[(train_df['Survived'] == 1),'Fare'],color='b',shade=True,label='Survived' )\nplt.title('Fare Distribution Survived vs Non Survived',fontsize=25)\nplt.ylabel('Frequency of Passenger Survived',fontsize=20)\nplt.xlabel('Fare',fontsize=20)","e2b6333d":"train_df.head()","e94bf657":"#fig,axs=plt.subplots(nrows=2)\nfig,axs=plt.subplots(figsize=(10,8))\nsns.set_style(style='darkgrid')\nsns.kdeplot(train_df.loc[(train_df['Survived']==0),'Age'],color='r',shade=True,label='Not Survived')\nsns.kdeplot(train_df.loc[(train_df['Survived']==1),'Age'],color='b',shade=True,label='Survived')\n","47ae5b69":"train_df.head()","3ec4065c":"## Family_size seems like a good feature to create\ntrain_df['family_size'] = train_df.SibSp + train_df.Parch+1\ntest_df['family_size'] = test_df.SibSp + test_df.Parch+1\n","caec5951":"def family_group(size):\n    a = ''\n    if (size <= 1):\n        a = 'loner'\n    elif (size <= 4):\n        a = 'small'\n    else:\n        a = 'large'\n    return a\n\ntrain_df['family_group'] = train_df['family_size'].map(family_group)\ntest_df['family_group'] = test_df['family_size'].map(family_group)","d8ee7f00":"train_df['is_alone'] = [1 if i<2 else 0 for i in train_df.family_size]\ntest_df['is_alone'] = [1 if i<2 else 0 for i in test_df.family_size]","77b41082":"## We are going to create a new feature \"age\" from the Age feature. \ntrain_df['child'] = [1 if i<16 else 0 for i in train_df.Age]\ntest_df['child'] = [1 if i<16 else 0 for i in test_df.Age]\ntrain_df.child.value_counts()","020a1394":"train_df.head()\n#test_df.head()","9a32808a":"train_df['calculated_fare'] = train_df.Fare\/train_df.family_size\ntest_df['calculated_fare'] = test_df.Fare\/test_df.family_size\n","62f2aa2d":"train_df.calculated_fare.mean()","e6aef720":"train_df.calculated_fare.mode()","147c530a":"def fare_group(fare):\n    a= ''\n    if fare <= 4:\n        a = 'Very_low'\n    elif fare <= 10:\n        a = 'low'\n    elif fare <= 20:\n        a = 'mid'\n    elif fare <= 45:\n        a = 'high'\n    else:\n        a = \"very_high\"\n    return a\n","adb1ac06":"train_df['fare_group'] = train_df['calculated_fare'].map(fare_group)\ntest_df['fare_group'] = test_df['calculated_fare'].map(fare_group)","b794a1e0":"train_df = pd.get_dummies(train_df, columns=['Title',\"Pclass\",'Embarked', 'family_group', 'fare_group'], drop_first=True)\ntest_df = pd.get_dummies(test_df, columns=['Title',\"Pclass\",'Embarked', 'family_group', 'fare_group'], drop_first=True)\ntrain_df.drop(['Cabin', 'family_size','Ticket','Name', 'Fare'], axis=1, inplace=True)\ntest_df.drop(['Ticket','Name','family_size',\"Fare\",'Cabin'], axis=1, inplace=True)\n","376497f3":"pd.options.display.max_columns = 99\n","c04df961":"def age_group_fun(age):\n    a = ''\n    if age <= 1:\n        a = 'infant'\n    elif age <= 4: \n        a = 'toddler'\n    elif age <= 13:\n        a = 'child'\n    elif age <= 18:\n        a = 'teenager'\n    elif age <= 35:\n        a = 'Young_Adult'\n    elif age <= 45:\n        a = 'adult'\n    elif age <= 55:\n        a = 'middle_aged'\n    elif age <= 65:\n        a = 'senior_citizen'\n    else:\n        a = 'old'\n    return a\n        ","aaa11c26":"train_df['age_group'] = train_df['Age'].map(age_group_fun)\ntest_df['age_group'] = test_df['Age'].map(age_group_fun)","dde32a95":"train_df = pd.get_dummies(train_df,columns=['age_group'], drop_first=True)\ntest_df = pd.get_dummies(test_df,columns=['age_group'], drop_first=True)\n#Lets try all after dropping few of the column.\ntrain_df.drop(['Age','calculated_fare'],axis=1,inplace=True)\ntest_df.drop(['Age','calculated_fare'],axis=1,inplace=True)","f76e0f3f":"#age=pd.cut(data_df['Age'],4)\n#data_df['Age2']=label.fit_transform(age)\n#fare=pd.cut(data_df['Fare'],4)\n#data_df['Fare2']=label.fit_transform(fare)\n#train_df['Age']=data_df['Age2'][:891]\n#train_df['Fare']=data_df['Fare2'][:891]\n#test_df['Age']=data_df['Age2'][891:]\n#test_df['Fare']=data_df['Fare2'][891:]\n#train_df = pd.get_dummies(train_df,columns=['Age','Fare'], drop_first=True)\n#test_df = pd.get_dummies(test_df,columns=['Age','Fare'], drop_first=True)\n#print(test_df.shape)\n#print(train_df.shape)\ntrain_df.head()\n\ntrain_df.drop(['Title_Rev','age_group_old','age_group_teenager','age_group_senior_citizen','Embarked_Q'],axis=1,inplace=True)\ntest_df.drop(['Title_Rev','age_group_old','age_group_teenager','age_group_senior_citizen','Embarked_Q'],axis=1,inplace=True)","d8b100f1":"X = train_df.drop('Survived', 1)\ny = train_df['Survived']\n#testing = test_df.copy()\n#testing.shape","8c083363":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedShuffleSplit,train_test_split\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    svm.SVC(probability=True),\n    DecisionTreeClassifier(),\n    CatBoostClassifier(),\n    XGBClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    LogisticRegression()]\n    \n\n\nlog_cols = [\"Classifier\", \"Accuracy\"]\nlog= pd.DataFrame(columns=log_cols)\n","f8ef70e2":"from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split,StratifiedShuffleSplit\n\n\n# SSplit=StratifiedShuffleSplit(test_size=0.3,random_state=7)\n# acc_dict = {}\n\n# for train_index,test_index in SSplit.split(X,y):\n#     X_train,X_test=X.iloc[train_index],X.iloc[test_index]\n#     y_train,y_test=y.iloc[train_index],y.iloc[test_index]\n    \n#     for clf in classifiers:\n#         name = clf.__class__.__name__\n          \n#         clf.fit(X_train,y_train)\n#         predict=clf.predict(X_test)\n#         acc=accuracy_score(y_test,predict)\n#         if name in acc_dict:\n#             acc_dict[name]+=acc\n#         else:\n#             acc_dict[name]=acc\n","6fc576f3":"\n# log['Classifier']=acc_dict.keys()\n# log['Accuracy']=acc_dict.values()\n# #log.set_index([[0,1,2,3,4,5,6,7,8,9]])\n# %matplotlib inline\n# sns.set_color_codes(\"muted\")\n# ax=plt.subplots(figsize=(10,8))\n# ax=sns.barplot(y='Classifier',x='Accuracy',data=log,color='b')\n# ax.set_xlabel('Accuracy',fontsize=20)\n# plt.ylabel('Classifier',fontsize=20)\n# plt.grid(color='r', linestyle='-', linewidth=0.5)\n# plt.title('Classifier Accuracy',fontsize=20)\n","ba75d485":"## Necessary modules for creating models. \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.metrics import accuracy_score,classification_report, precision_recall_curve, confusion_matrix","60a768b9":"\nstd_scaler = StandardScaler()\nX = std_scaler.fit_transform(X)\ntestframe = std_scaler.fit_transform(test_df)\ntestframe.shape\n","2ef48eb3":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=1000)","5fde6314":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import precision_score,recall_score,confusion_matrix\nlogreg = LogisticRegression(solver='liblinear', penalty='l1')\nlogreg.fit(X_train,y_train)\npredict=logreg.predict(X_test)\nprint(accuracy_score(y_test,predict))\nprint(confusion_matrix(y_test,predict))\nprint(precision_score(y_test,predict))\nprint(recall_score(y_test,predict))","f3be6af6":"C_vals = [0.0001, 0.001, 0.01, 0.1,0.13,0.2, .15, .25, .275, .33, 0.5, .66, 0.75, 1.0, 2.5, 4.0,4.5,5.0,5.1,5.5,6.0, 10.0, 100.0, 1000.0]\npenalties = ['l1','l2']\n\nparam = {'penalty': penalties, 'C': C_vals, }\ngrid = GridSearchCV(logreg, param,verbose=False, cv = StratifiedKFold(n_splits=5,random_state=10,shuffle=True), n_jobs=1,scoring='accuracy')","2f87dc9e":"grid.fit(X_train,y_train)\nprint (grid.best_params_)\nprint (grid.best_score_)\nprint(grid.best_estimator_)","17030bd9":"#grid.best_estimator_.fit(X_train,y_train)\n#predict=grid.best_estimator_.predict(X_test)\n#print(accuracy_score(y_test,predict))\nlogreg_grid = LogisticRegression(penalty=grid.best_params_['penalty'], C=grid.best_params_['C'])\nlogreg_grid.fit(X_train,y_train)\ny_pred = logreg_grid.predict(X_test)\nlogreg_accy = round(accuracy_score(y_test, y_pred), 3)\nprint (logreg_accy)\nprint(confusion_matrix(y_test,y_pred))\nprint(precision_score(y_test,y_pred))\nprint(recall_score(y_test,y_pred))","2e252a8b":"ABC=AdaBoostClassifier()\n\nABC.fit(X_train,y_train)\npredict=ABC.predict(X_test)\nprint(accuracy_score(y_test,predict))\nprint(confusion_matrix(y_test,predict))\nprint(precision_score(y_test,predict))\n","5306fb29":"from sklearn.tree import DecisionTreeClassifier\nn_estimator=[50,60,100,150,200,300]\nlearning_rate=[0.001,0.01,0.1,0.2,]\nhyperparam={'n_estimators':n_estimator,'learning_rate':learning_rate}\ngridBoost=GridSearchCV(ABC,param_grid=hyperparam,verbose=False, cv = StratifiedKFold(n_splits=5,random_state=15,shuffle=True), n_jobs=1,scoring='accuracy')","49d49a54":"gridBoost.fit(X_train,y_train)\nprint(gridBoost.best_score_)\nprint(gridBoost.best_estimator_)","3c6366a9":"gridBoost.best_estimator_.fit(X_train,y_train)\npredict=gridBoost.best_estimator_.predict(X_test)\nprint(accuracy_score(y_test,predict))\n","73c01dc7":"xgb=XGBClassifier(max_depth=2, n_estimators=700, learning_rate=0.009,nthread=-1,subsample=1,colsample_bytree=0.8)\nxgb.fit(X_train,y_train)\npredict=xgb.predict(X_test)\nprint(accuracy_score(y_test,predict))\nprint(confusion_matrix(y_test,predict))\nprint(precision_score(y_test,predict))\nprint(recall_score(y_test,predict))","3ecca6ce":"lda=LinearDiscriminantAnalysis()\nlda.fit(X_train,y_train)\npredict=lda.predict(X_test)\nprint(accuracy_score(y_test,predict))\nprint(precision_score(y_test,predict))\nprint(recall_score(y_test,predict))","0f8795ec":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndectree = DecisionTreeClassifier( criterion=\"entropy\",\n                                 max_depth=5,\n                                class_weight = 'balanced',\n                                min_weight_fraction_leaf = 0.009,\n                                random_state=2000)\ndectree.fit(X_train, y_train)\ny_pred = dectree.predict(X_test)\ndectree_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(dectree_accy)\nprint(confusion_matrix(y_test,y_pred))\nprint(precision_score(y_test,y_pred))\nprint(recall_score(y_test,y_pred))\n","704e3ca9":"#from sklearn.ensemble import RandomForestClassifier\n#from sklearn.metrics import precision_score,recall_score,confusion_matrix\n#randomforest = RandomForestClassifier(n_estimators=100,max_depth=9,min_samples_split=6, min_samples_leaf=4)\n##randomforest = RandomForestClassifier(class_weight='balanced', n_jobs=-1)\n#randomforest.fit(X_train, y_train)\n#y_pred = randomforest.predict(X_test)\n#random_accy = round(accuracy_score(y_pred, y_test), 3)\n#print (random_accy)\n#print(confusion_matrix(y_test,y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nrandomforest = RandomForestClassifier(n_estimators=100,max_depth=5,min_samples_split=20,max_features=0.2, min_samples_leaf=8,random_state=20)\n#randomforest = RandomForestClassifier(class_weight='balanced', n_jobs=-1)\nrandomforest.fit(X_train, y_train)\ny_pred = randomforest.predict(X_test)\nrandom_accy = round(accuracy_score(y_pred, y_test), 3)\nprint (random_accy)\nprint(precision_score(y_test,y_pred))\nprint(recall_score(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\n","b0ce7e0d":"from sklearn.ensemble import BaggingClassifier\nBaggingClassifier = BaggingClassifier()\nBaggingClassifier.fit(X_train, y_train)\ny_pred = BaggingClassifier.predict(X_test)\nbagging_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(bagging_accy)","c122e3a3":"from sklearn.ensemble import VotingClassifier\n\nvoting_classifier = VotingClassifier(estimators=[\n    ('logreg',logreg), \n    ('random_forest', randomforest),\n    ('decision_tree',dectree), \n    ('XGB Classifier', xgb),\n    ('BaggingClassifier', BaggingClassifier)])\nvoting_classifier.fit(X_train,y_train)\ny_pred = voting_classifier.predict(X_test)\nvoting_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(voting_accy)","64f6d385":"#y_predict=randomforest.predict(testframe)","7780f77a":"# Prediction with catboost algorithm.\nfrom catboost import CatBoostClassifier\nmodel = CatBoostClassifier(verbose=False, one_hot_max_size=3)\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\nacc = round(accuracy_score(y_pred, y_test), 3)\nprint(acc)","1b31048f":"y_predict=model.predict(testframe)","e4ddbda0":"temp = pd.DataFrame(pd.DataFrame({\n        \"PassengerId\": passenger_id,\n        \"Survived\": y_predict\n    }))\n\n\ntemp.to_csv(\"..\/working\/submission3.csv\", index = False)","fe4bdb1c":"<a id=\"section302\"><\/a>\n#### 3.2  Why missing values treatment is required?\n\n__Missing data__ in the training data set can reduce the power \/ fit of a model or can lead to a biased model because we have not analysed the behavior and relationship with other variables correctly. It can lead to __wrong prediction__ or __classification__.\n","a4a936db":"<a id=\"section603\"><\/a>\n#### 6.3 AdaBoostClassifer","3718a865":"<a id=\"section501\"><\/a>\n#### 5.1 family_size feature","be40a0fb":"<a id=\"section4\"><\/a>\n### 4.Exploratory data analysis\n![](http:\/\/media.giphy.com\/media\/m3UHHYejQ4rug\/giphy.gif)\n\n**Exploratory data analysis (EDA)** is an approach to analyzing data sets to summarize their main characteristics, often with visual methods.","e8039cc3":"<a id=\"section305\"><\/a>\n#### 3.5 Treating Missing age","99711543":"<a id=\"section504\"><\/a>\n#### 5.4 fare feature","1f332757":"<a id=\"section402\"><\/a>\n#### 4.2 Gender and Survived\n","d5e74788":"<a id=\"section202\"><\/a>\n#### 2.2 Data Description","7b5e486a":"\nThere are a couple of points that should be noted from the statistical overview. They are..\n- About the survival rate, only 38% passenger survived during that tragedy.\n- About the survival rate for genders, 74% female passengers survived, while only 19% male passengers survived.","94c4b435":"<h1> History<\/h1>\n\n**Titanic** is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the **Titanic** sank after colliding with an iceberg, killing *1502* out of *2224* passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.<br><br>\n\n<h3>If you are using the kaggle first time<\/h3>\nThis github link is for \n<a href='https:\/\/github.com\/vikramvinay\/Titanic-Machine-Learning-from-Disaster'>Beginners  who try first time kaggle<\/a>\n\nIn this github link i  basically explains all the thing that i did when i begin first time with kaggle .So this one just gives you the kind of connect that you need when you begin with kaggle. It will also enhance your knowledge.\n\nTo use this github repository fork it and use it and try to gain most of it.","9dd25c3e":"**Grid Search on Logistic Regression**","028b717b":"This bar plot above shows the distribution of female and male survived. The x_label shows gender and the y_label shows % of passenger survived. This bar plot shows that 74% female passenger survived while only ~19% male passenger survived.","cc379459":"The data has been split into two groups:\n- training set (train.csv)\n- test set(test.csv)\n<br>\n\nThe training set includes passengers survival status(also know as the ground truth from the titanic tragedy) which along with other features like gender, class, fare and pclass is used to create machine learning model.\n<br><br>\nThe test set should be used to see how well my model performs on unseen data. The test set does not provide passengers survival status. We are going to use our model to predict passenger survival status.\n<br><br>\n\nLets describe whats the meaning of the features given the both train & test datasets.\n<h4>Variable Definition Key.<\/h4>\n- Survival\n - 0= No\n - 1= Yes\n- pclass (Ticket class)\n - 1=1st\n - 2=2nd\n - 3=3rd\n \n- sex\n<br>\n\n- age\n\n\n- sibsp (# of siblings \/ spouses aboard the Titanic)\n<br>\n- parch (# of parents \/ children aboard the Titanic)\n<br>\n- tickets\n<br>\n- fare\n<br>\n- cabin\n- embarked Port of Embarkation.\n - C = Cherbourg,\n - Q = Queenstown,\n - S = Southampton\n- pclass: A proxy for socio-economic status (SES)\n<br>\n<h4>This is important to remember and will come in handy for later analysis.<\/h4>\n - 1st = Upper\n - 2nd = Middle\n - 3rd = Lower\n","f239977f":"<a id=\"section1\"><\/a>\n### 1. Problem Statement\n\n\nIn this challenge, we need to complete the __analysis__ of what sorts of people were likely to __survive__. In particular,  we apply the tools of __machine learning__ to predict which passengers survived the tragedy\n\n- Predict whether passenger gonna __survive or not__.","3b57af4f":"<a id=\"section506\"><\/a>\n#### 5.6 Creating dummy variables","8907db94":"<a id=\"section604\"><\/a>\n#### 6.4 XGBClassifier","b0d43565":"<a id=\"section601\"><\/a>\n#### 6.1 Classifier Comparision\n\nBy Classifier Comparison we choose which model best for the given data.","964cd4c2":"**GridSearch on AdaBoostClassifer**","e7c29f3d":"From the above barplot, we can clearly see that the following classifiers are good-\n- LogisticRegression\n- XGBClassifier\n- AdaBoostClassifier\n- GradiendBoostingClassifier \n- LDA\n\n** Note**\n\nThe spliting of the test and train data is randomly done so possibility of getting different set of Best classifier that may be differ from mine one.\n\n\nBut While you running multiple time one a general scale. The given set of classifier used in this kernal are proved to be good one.\n## Edit: 4\/30\/2019\n- Recently i got comment regarding using __Cataboost algorithm__. As over in the barplot we can see catboost also doing great so this time we gonna use cataboost for final submission.","d4aae288":"> __Result after running the above code cell__\n\n![](https:\/\/raw.githubusercontent.com\/vikramvinay\/Titanic-Machine-Learning-from-Disaster\/master\/download.png)","8d93294c":"## Table of Contents\n\n1. [Problem Statement](#section1)<br>\n2. [Data Loading and Description](#section2)<br\/>\n    - 2.1  [Loading the data files](#section201)<br\/>\n    - 2.2 [Data Description](#section202)<br\/>\n3. [Cleaning the data](#section3)\n    - 3.1 [Understanding the Dataset](#section301)<br\/>\n    - 3.2 [Why missing values treatment is required?](#section302)<br\/>\n    - 3.3 [Dealing with Missing values](#section303)<br\/>\n    - 3.4 [Transforming Sex](#section304)<br\/>\n    - 3.5 [Treating Missing age](#section305)<br\/>\n4. [Exploratory data analysis](#section4)\n    - 4.1 [Correlation Matrix and Heatmap](#section401)<br\/>\n    - 4.2 [Gender and Survived](#section402)<br\/>\n    - 4.3 [Pclass and Survived](#section403)<br\/>\n    - 4.4 [Fare and Survived](#section404)<br\/>\n    - 4.5 [Age and Survived](#section405)<br\/>\n5. [Feature Engineering](#section5)<br\/>  \n    - 5.1 [family_size feature](#section501)<br\/>\n    - 5.2 [Is_alone feature](#section502)<br\/>\n    - 5.3 [Child feature](#section503)<br\/>\n    - 5.4 [fare feature](#section504)<br\/>\n    - 5.5 [Calculated_fare feature](#section505)<br\/>\n    - 5.6 [Creating dummy variables](#section506)<br\/>\n6. [Model Creation](#section6)<br\/>\n    - 6.1 [Classifier Comparision](#section601)<br\/>\n    - 6.2 [LogisticRegression](#section602)<br\/>\n    - 6.3 [AdaBoostClassifer](#section603)<br\/>\n    - 6.4 [XGBClassifier](#section604)<br\/>\n    - 6.5 [DecisionTree Classifier](#section605)<br\/>\n    - 6.6 [Random Forest Classifier](#section606)<br\/>\n    - 6.7 [Bagging Classifier](#section607)<br\/>\n    - 6.8 [Voting Classifier](#section608)<br\/>\n7. [Submit test predictions](#section7)<br\/>","4acd5400":"#### <font color=\"red\">Edit<\/font> \n- After ```Ken Lim``` __Suggestion__: Check in the __comment section__  ","fec203ab":"<a id=\"section505\"><\/a>\n#### 5.5 Calculated_fare feature","30658aae":"It looks like this dataset is quite organized, however, before using this dataset for analyzing and visualizing we need to deal with ..\n- Different variables\n- Null values\n\n__Different variables present in the datasets__\n - **There are four type of variables**\n  - **Numerical Features**: Age, Fare, SibSp and Parch\n  - **Categorical Features**: Sex, Embarked, Survived and Pclass\n  - **Alphanumeric Features**: Ticket and Cabin(Contains both alphabets and the numeric value)\n  - **Text Features**: Name\n\n- We really need to tweak these features so we get the desired form of input data","264ab2d5":"<a id=\"section404\"><\/a>\n#### 4.4 Fare and Survived","ec5bb78f":"**GridSearch**\n\nUsing the GridSearch ,Lets find out the most suitable parameter\/hyperparmeter which gives the best result.\n","ddff0953":"<a id=\"section401\"><\/a>\n#### 4.1 Correlation Matrix and Heatmap","8dd40dd0":"__Time to train every model with further hyperparameter tunning extension__","d4ed86ad":"<center><h3>From now onward we will follow below TOC points<\/h3><\/center>","711d5127":"<a id=\"section608\"><\/a>\n#### 6.8 Voting Classifier","45e9a857":"<a id=\"section2\"><\/a>\n### 2. Data Loading and Description","b889ca5f":"<a id=\"section502\"><\/a>\n#### 5.2 Is_alone feature","af948456":"<a id=\"section6\"><\/a>\n### 6. Model Creation","c3a4222e":"So it clearly seems that,The survival of the people belong to 3rd class is very least.\nIt looks like ...\n-  63% first class passenger survived titanic tragedy, while\n-  48% second class and\n-  only 24% third class passenger survived.","af3e56b3":"**Why scaling** : [Check my kernel on importance of scaling](https:\/\/www.kaggle.com\/vin1234\/why-feature-scaling-is-important)","a1217b40":"**This kernel is still under process for further improvement.**\n\nI will always incorporate new concepts of data science as I master them. This journey of learning is worth sharing as well as collaborating. So share your ideas and let's make this notebook best __to get the start for every beginner__... \n\n- Any comments about further improvements to the kernel would be genuinely appreciated.\n- Feel free to raise any doubt in the comment section regarding the kernel.","0ce948b3":"| Column Name                       | Description                                                                                        |\n| ----------------------------------|:--------------------------------------------------------------------------------------------------:|\n| PassengerId                       | Passenger Identity                                                                                                   | \n| Survived                          | Whether passenger survived or not                                                                  | \n| Pclass                            | Class of ticket                                                                                    | \n| Name                              | Name of passenger                                                                                  | \n| Sex                               | Sex of passenger                                                                                   |\n| Age                               | Age of passenger                                                                                   |\n| SibSp                             | Number of sibling and\/or spouse travelling with passenger                                          |\n| Parch                             | Number of parent and\/or children travelling with passenger                                         |\n| Ticket                            | Ticket number                                                                                      |\n| Fare                              | Price of ticket                                                                                    |\n| Cabin                             | Cabin number                                                                                       |","6bffff14":"<a id=\"section304\"><\/a>\n#### 3.4 Transforming Sex","ae90d574":"**Summary**\n\nFirst class passenger had the upper hand during the tragedy than second and third class passengers. You can probably agree with me more on this, when we look at the distribution of ticket fare and survived column.","186f7be3":"Remember the <font color='red'>Upvote button<\/font> is next to the fork button and if you wants further updates of my notebooks <font color='red'>Click Follow<\/font> , and it's free too! ;)","66967871":"**Negative Correlation Features:**\n- Fare and Pclass: -0.55\n - This relationship can be explained by saying that first class passenger(1) paid more for fare then second class passenger(2), similarly second class passenger paid more than the third class passenger(3). \n- Gender and Survived: -0.54\n - Basically is the info of whether the passenger was male or female.\n- Pclass and Survived: -0.34","1fb4a814":"<a id=\"section201\"><\/a>\n#### 2.1  Loading the data files ","bd6e4f29":"<a id=\"section3\"><\/a>\n### 3. Cleaning the data","a24908b8":"<a id=\"section606\"><\/a>\n#### 6.6 Random Forest Classifier\n","94e73808":"#### Important Note:\n\n- If you're facing an error to see the result of below code cell, It's because there is a lot of __calcualation__ is taking place as multiple algorithms are running so for that I suggest get this notebook to edit mode and try to run cell by cell.\n\n- I am commenting two of the below code cells as they are taking a lot of notebook space, but i want you guys try to run them from your side. ","2f9bfa09":"We see Age  and Cabin have a lot of missing value.So First we need to deal with all these NaN values.\n- As in Cabin column about 1\\3rd of the values are missing.So we get rid of this column. \n<br>\n","655d2885":"<a id=\"section503\"><\/a>\n\n#### 5.3 Child feature","d8e75da5":"<a id=\"section403\"><\/a>\n#### 4.3 Pclass and Survived","af1b95a3":"**Summary**\n- As we suspected, female passengers have survived at a much better rate than male passengers.\n- It seems about right since females and children were the priority.","4b695f1c":"<a id=\"section303\"><\/a>\n#### 3.3 Dealing with Missing values","bdad92a2":"This kde plot is pretty self explanatory with all the labels and colors. Something I have noticed that some readers might find questionable is that in, the plot; the third class passengers have survived more than second class passnegers. It is true since there were a lot more third class passengers than first and second.\n\n","3189d5bc":"<a id=\"section607\"><\/a>\n#### 6.7 Bagging Classifier\n","c496bee6":"This count plot shows the actual distribution of male and female passengers that survived and did not survive. It shows that among all the females ~ 230 survived and ~ 70 did not survive. While among male passengers ~110 survived and ~480 did not survive.","83f4d6ee":"<a id=\"section405\"><\/a>\n#### 4.5 Age and Survived","b67a7774":"**Hope you find it useful.** \n\n**If this notebook helped you in anyway, Then do <font color=\"red\">Follow <\/font> and  <font color=\"blue\">upvote!<\/font>**\n\n<img src=\"https:\/\/steemitimages.com\/640x0\/http:\/\/i0.kym-cdn.com\/photos\/images\/original\/000\/662\/100\/aff.jpg\" height=500 width=500\/>","41b9c958":"<a id=\"section605\"><\/a>\n#### 6.5 DecisionTree Classifier","7e0e3dbe":"**Positive Correlation Features:**\n- Fare and Survived: 0.26.\n\nThere is a positive correlation between Fare and Survived rated. This can be explained by saying that, the passenger who paid more money for their ticket were more likely to survive. ","a80b11a5":"<a id=\"section5\"><\/a>\n### 5. Feature Engineering","8a7f361b":"<a id=\"section7\"><\/a>\n### 7. Submit test predictions\n\nThe given parameter is used in the  model  is found through grid search which is not shown in the code .\n\nAs it took a lot long to run .","5c812a15":"# <center><h1><font color=\"red\">Titanic<\/font><\/h1><\/center>\n\n<center><img src=\"https:\/\/static3.thetravelimages.com\/wordpress\/wp-content\/uploads\/2018\/11\/titanic1-e1542497861799.jpg\" height=500 \/><\/center>","73cc0b09":"* I will train the data with the following models:\n- Logistic Regression\n- Gaussian Naive Bayes\n- Support Vector Machines\n- Decision Tree Classifier\n- K-Nearest Neighbors(KNN)\n -  and many other.....\n \n","1a1a3750":"There is nothing out of the ordinary of about this plot, except the very left part of the distribution. It shows that\n\nchildren and infants were the priority.","7969b2a1":"<a id=\"section602\"><\/a>\n#### 6.2 LogisticRegression","dcb50668":"**Scaling features**"}}