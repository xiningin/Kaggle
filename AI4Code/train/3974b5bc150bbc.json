{"cell_type":{"ef13bd1e":"code","5220c7d6":"code","d2a063ec":"code","f9ce0074":"code","e8615bc2":"code","3030bdbf":"code","d9da4108":"code","3c46703f":"code","9f1c132d":"code","1147c718":"code","0aef8b07":"code","1b11b948":"code","d323f811":"code","049da325":"code","1ab630d8":"code","d398351b":"code","34d87f21":"code","e4bee11a":"code","f1fb7174":"code","9b00ae26":"code","b22b7c8d":"code","51117379":"code","97c6b819":"code","f9d6fdf2":"code","ef99b270":"code","cd42e410":"code","8d59e9ea":"code","78414351":"code","53f920fa":"code","e961c9b0":"code","10076054":"code","51254513":"code","6474ab37":"code","31ed9a40":"code","70dc87a7":"code","6094022a":"code","4354c4bb":"code","f1b6c030":"code","da4e9853":"code","0549120a":"code","6b64b2f6":"code","78d18429":"code","83b75bb4":"code","7662eb5a":"code","199761a7":"code","8e16d00f":"code","d40c8d96":"code","a6668dea":"code","ee3105bd":"code","5f1ca9cf":"code","1220527a":"code","545608a9":"code","81c96788":"code","feff19e8":"code","6ae4ac90":"code","4a8b4343":"code","08a71542":"code","d512da12":"code","9daa1ca8":"code","82536698":"code","1337880a":"code","88d2e6b8":"code","0b0fd004":"code","b936bbac":"code","f209023a":"code","4012fe9d":"code","b5a9ce4b":"code","c225784a":"code","672347de":"code","f6670a51":"code","ae311103":"code","8da6d5b5":"code","f7e02d17":"code","a11644c6":"code","a6a53529":"code","85c64632":"code","839ed582":"code","25e2601d":"code","248a5eca":"code","19e8d1d2":"code","80b00b4b":"code","6e7c3292":"code","a336e6ed":"code","7f56c61a":"code","ad5047d0":"code","5cbb4478":"code","b2b5fbc2":"code","f90def54":"code","637c4ddd":"code","31e349f6":"code","169e7976":"code","23a63da5":"code","c38831bb":"code","3a80f06d":"code","5a77ea24":"code","866ef0c6":"code","1ecf8428":"code","2c60f3e2":"code","678fe41e":"code","cf664822":"code","81088d63":"code","228a508c":"code","e2643f62":"code","f07e6288":"code","be1b5735":"code","b594ab47":"code","cf962c14":"code","555475d8":"code","6d6c1312":"code","fb3640c9":"code","7c1a239e":"code","60407f04":"code","39317d5b":"code","89ed1c2e":"code","5e4e2efe":"code","85d81386":"code","170501a1":"code","55b01eaf":"code","4fcd3dc1":"code","17eea87d":"code","de9a2fc8":"code","224c1edb":"code","cdb57569":"code","56064b3a":"code","58c18223":"code","e424b9d1":"code","ff458ffb":"code","9c59ba5e":"code","0e114229":"code","513f67e8":"code","0f991aac":"code","4c69afb6":"code","aa97df8a":"code","9255c38f":"code","a8b796b2":"code","d54ccb0f":"code","c3dba81c":"code","9bd35871":"code","3d596843":"code","f43b4028":"code","593c7973":"code","4b473222":"code","34d2e18b":"code","37aff79e":"code","020996af":"code","80ea52d6":"code","fbc7fea6":"code","7614f303":"code","b96b7ab2":"code","1769dd26":"code","e1f81456":"code","498c732a":"code","4944a465":"markdown","53771089":"markdown","0e41aadd":"markdown","b22b59ee":"markdown"},"source":{"ef13bd1e":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport torch\nimport importlib\nimport cv2 \nimport pandas as pd\n\nfrom PIL import Image\nfrom IPython.display import display","5220c7d6":"%%time\n\n%cp -r \/kaggle\/input\/yolox-pet2 \/kaggle\/working\/\n%cd \/kaggle\/working\/yolox-pet2\/yolox-dep","d2a063ec":"%%time\n\n!pip install pip-21.3.1-py3-none-any.whl -f .\/ --no-index\n!pip install loguru-0.5.3-py3-none-any.whl -f .\/ --no-index\n!pip install ninja-1.10.2.3-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl -f .\/ --no-index\n!pip install onnx-1.8.1-cp37-cp37m-manylinux2010_x86_64.whl -f .\/ --no-index\n!pip install onnxruntime-1.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl -f .\/ --no-index\n!pip install onnxoptimizer-0.2.6-cp37-cp37m-manylinux2014_x86_64.whl -f .\/ --no-index\n!pip install thop-0.0.31.post2005241907-py3-none-any.whl -f .\/ --no-index\n!pip install tabulate-0.8.9-py3-none-any.whl -f .\/ --no-index","f9ce0074":"%%time\n\n%cd \/kaggle\/working\/yolox-pet2\/YOLOX\n!pip install -r requirements.txt\n!pip install -v -e . ","e8615bc2":"%%time\n\n# Install CocoAPI tool\n%cd \/kaggle\/working\/yolox-pet2\/yolox-dep\/cocoapi\/PythonAPI\n\n!make\n!make install\n!python setup.py install","3030bdbf":"import pycocotools","d9da4108":"import cv2\nimport matplotlib.pyplot as plt\nfrom glob import glob\nimport pandas as pd\nfrom pandarallel import pandarallel\npandarallel.initialize(progress_bar=True)\nimport torchvision.transforms as T\nfrom torchvision.io import ImageReadMode, read_image, write_jpeg\nimport gc","3c46703f":"%cd \/kaggle\/working\/","9f1c132d":"df = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/test.csv\")\n# df = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/train.csv\").sample(6800)","1147c718":"# df = pd.concat([df]*900, axis=\"index\").reset_index(drop=True)\n# df.shape","0aef8b07":"# df.head()","1b11b948":"# from tqdm import tqdm_notebook as tqdm\n# import shutil\n\n# !mkdir \/kaggle\/working\/copy_train\n# for file in tqdm(df[\"Id\"]):\n#     shutil.copyfile(f\"..\/input\/petfinder-pawpularity-score\/train\/{file}.jpg\", f\"\/kaggle\/working\/copy_train\/{file}.jpg\")","d323f811":"df[\"Id\"] = \"..\/input\/petfinder-pawpularity-score\/test\/\" + df[\"Id\"] + \".jpg\"\n# df[\"Id\"] = \"..\/input\/petfinder-pawpularity-score\/train\/\" + df[\"Id\"] + \".jpg\"\n# df[\"Id\"].head()","049da325":"!mkdir \/kaggle\/working\/test_center_crop\/","1ab630d8":"def center_crop(Id: str):\n    image = read_image(Id, mode=ImageReadMode.RGB)\n    write_jpeg(\n        T.CenterCrop(min(image.shape[1:]))(image),\n        f'\/kaggle\/working\/test_center_crop\/{Id.replace(\"..\/input\/petfinder-pawpularity-score\/test\/\", \"\")}',\n#         f'\/kaggle\/working\/test_center_crop\/{Id.replace(\"..\/input\/petfinder-pawpularity-score\/train\/\", \"\")}',\n        quality=100\n    )","d398351b":"%%time\ndf[\"Id\"].parallel_apply(center_crop)","34d87f21":"del df\ngc.collect()","e4bee11a":"%cd \/kaggle\/working\/yolox-pet2\/YOLOX","f1fb7174":"import argparse\nimport os\nimport time\n\nimport numpy as np\nfrom loguru import logger\n# import jpeg4py as jpeg\n\nimport cv2\n\nimport torch\n\nfrom yolox.data.data_augment import ValTransform\nfrom yolox.data.datasets import COCO_CLASSES\nfrom yolox.exp import get_exp\nfrom yolox.utils import fuse_model, get_model_info, postprocess, vis\n\nIMAGE_EXT = [\".jpg\", \".jpeg\", \".webp\", \".bmp\", \".png\"]","9b00ae26":"def make_parser():\n    parser = argparse.ArgumentParser(\"YOLOX Demo!\")\n    parser.add_argument(\n        \"demo\", default=\"image\", help=\"demo type, eg. image, video and webcam\"\n    )\n    parser.add_argument(\"-expn\", \"--experiment-name\", type=str, default=None)\n    parser.add_argument(\"-n\", \"--name\", type=str, default=None, help=\"model name\")\n\n    parser.add_argument(\n        \"--path\", default=\".\/assets\/dog.jpg\", help=\"path to images or video\"\n    )\n    parser.add_argument(\"--camid\", type=int, default=0, help=\"webcam demo camera id\")\n    parser.add_argument(\n        \"--save_result\",\n        action=\"store_true\",\n        help=\"whether to save the inference result of image\/video\",\n    )\n\n    # exp file\n    parser.add_argument(\n        \"-f\",\n        \"--exp_file\",\n        default=None,\n        type=str,\n        help=\"pls input your experiment description file\",\n    )\n    parser.add_argument(\"-c\", \"--ckpt\", default=None, type=str, help=\"ckpt for eval\")\n    parser.add_argument(\n        \"--device\",\n        default=\"cpu\",\n        type=str,\n        help=\"device to run our model, can either be cpu or gpu\",\n    )\n    parser.add_argument(\"--conf\", default=0.3, type=float, help=\"test conf\")\n    parser.add_argument(\"--nms\", default=0.3, type=float, help=\"test nms threshold\")\n    parser.add_argument(\"--tsize\", default=None, type=int, help=\"test img size\")\n    parser.add_argument(\n        \"--fp16\",\n        dest=\"fp16\",\n        default=False,\n        action=\"store_true\",\n        help=\"Adopting mix precision evaluating.\",\n    )\n    parser.add_argument(\n        \"--legacy\",\n        dest=\"legacy\",\n        default=False,\n        action=\"store_true\",\n        help=\"To be compatible with older versions\",\n    )\n    parser.add_argument(\n        \"--fuse\",\n        dest=\"fuse\",\n        default=False,\n        action=\"store_true\",\n        help=\"Fuse conv and bn for testing.\",\n    )\n    parser.add_argument(\n        \"--trt\",\n        dest=\"trt\",\n        default=False,\n        action=\"store_true\",\n        help=\"Using TensorRT model for testing.\",\n    )\n    return parser\n\n\ndef get_image_list(path):\n    image_names = []\n    for maindir, subdir, file_name_list in os.walk(path):\n        for filename in file_name_list:\n            apath = os.path.join(maindir, filename)\n            ext = os.path.splitext(apath)[1]\n            if ext in IMAGE_EXT:\n                image_names.append(apath)\n    return image_names\n\n\nclass Predictor(object):\n    def __init__(\n        self,\n        model,\n        exp,\n        cls_names=COCO_CLASSES,\n        trt_file=None,\n        decoder=None,\n        device=\"cpu\",\n        fp16=False,\n        legacy=False,\n    ):\n        self.model = model\n        self.cls_names = cls_names\n        self.decoder = decoder\n        self.num_classes = exp.num_classes\n        self.confthre = exp.test_conf\n        self.nmsthre = exp.nmsthre\n        self.test_size = exp.test_size\n        self.device = device\n        self.fp16 = fp16\n        self.preproc = ValTransform(legacy=legacy)\n        if trt_file is not None:\n            from torch2trt import TRTModule\n\n            model_trt = TRTModule()\n            model_trt.load_state_dict(torch.load(trt_file))\n\n            x = torch.ones(1, 3, exp.test_size[0], exp.test_size[1]).cuda()\n            self.model(x)\n            self.model = model_trt\n\n    def inference(self, img):\n        img_info = {\"id\": 0}\n        if isinstance(img, str):\n            img_info[\"file_name\"] = os.path.basename(img)\n            img = cv2.imread(img)\n#             img = jpeg.JPEG(img).decode()[:, :, ::-1]\n        else:\n            img_info[\"file_name\"] = None\n\n        height, width = img.shape[:2]\n        img_info[\"height\"] = height\n        img_info[\"width\"] = width\n        img_info[\"raw_img\"] = img\n\n        ratio = min(self.test_size[0] \/ img.shape[0], self.test_size[1] \/ img.shape[1])\n        img_info[\"ratio\"] = ratio\n\n        img, _ = self.preproc(img, None, self.test_size)\n        img = torch.from_numpy(img).unsqueeze(0)\n        img = img.float()\n        if self.device == \"gpu\":\n            img = img.cuda()\n            if self.fp16:\n                img = img.half()  # to FP16\n\n        with torch.no_grad():\n            t0 = time.time()\n            outputs = self.model(img)\n            if self.decoder is not None:\n                outputs = self.decoder(outputs, dtype=outputs.type())\n            outputs = postprocess(\n                outputs, self.num_classes, self.confthre,\n                self.nmsthre, class_agnostic=True\n            )\n            logger.info(\"Infer time: {:.4f}s\".format(time.time() - t0))\n        return outputs, img_info\n\n    def visual(self, output, img_info, cls_conf=0.35):\n        ratio = img_info[\"ratio\"]\n        img = img_info[\"raw_img\"]\n        if output is None:\n            return img\n        output = output.cpu()\n\n        bboxes = output[:, 0:4]\n\n        # preprocessing: resize\n        bboxes \/= ratio\n\n        cls = output[:, 6]\n        scores = output[:, 4] * output[:, 5]\n\n        vis_res = vis(img, bboxes, scores, cls, cls_conf, self.cls_names)\n        return vis_res\n\n    def crop_max_dog_or_cat(self, output, img_info):\n        ratio = img_info[\"ratio\"]\n        img = img_info[\"raw_img\"]\n        if output is None:\n            return img, None\n        output = output.cpu()\n\n        bboxes = output[:, 0:4]\n\n        # preprocessing: resize\n        bboxes \/= ratio\n        bboxes = bboxes.numpy()\n\n        cls = output[:, 6]\n        cls = cls.numpy().astype(int)\n        cls_names = [self.cls_names[clsid] for clsid in cls]\n        scores = (output[:, 4] * output[:, 5]).numpy()\n\n        img_bbox_infos = dict()\n        img_bbox_infos[\"file_name\"] = img_info[\"file_name\"]\n        img_bbox_infos[\"height\"] = img_info[\"height\"]\n        img_bbox_infos[\"width\"] = img_info[\"width\"]\n        img_bbox_infos[\"cls_names\"] = cls_names\n        img_bbox_infos[\"scores\"] = scores\n        img_bbox_infos[\"cls_ids\"] = cls\n        img_bbox_infos[\"bboxes\"] = bboxes\n\n        dog_or_cat_idx = [i for i, x in enumerate(cls_names) if x in [\"dog\", \"cat\"]]\n        if len(dog_or_cat_idx) < 1:\n            return img, img_bbox_infos\n\n        dog_or_cat_scores = scores[dog_or_cat_idx]\n        dog_or_cat_bboxes = bboxes[dog_or_cat_idx]\n\n        max_score_idx = np.argmax(dog_or_cat_scores)\n        max_score_bbox = dog_or_cat_bboxes[max_score_idx]\n        x0 = max(0, int(max_score_bbox[0]))\n        y0 = max(0, int(max_score_bbox[1]))\n        x1 = min(int(max_score_bbox[2]), img_info[\"width\"])\n        y1 = min(int(max_score_bbox[3]), img_info[\"height\"])\n\n        res_img = img[y0:y1, x0:x1]\n\n        return res_img, img_bbox_infos","b22b7c8d":"import pickle\nimport sys\n\ndef save_pickle(obj, file_path):\n    max_bytes = 2 ** 31 - 1\n    bytes_out = pickle.dumps(obj, protocol=pickle.HIGHEST_PROTOCOL)\n    n_bytes = sys.getsizeof(bytes_out)\n    with open(file_path, \"wb\") as f_out:\n        for idx in range(0, n_bytes, max_bytes):\n            f_out.write(bytes_out[idx : idx + max_bytes])\n\ndef load_pickle(file_path):\n    max_bytes = 2 ** 31 - 1\n    input_size = os.path.getsize(file_path)\n    bytes_in = bytearray(0)\n    with open(file_path, \"rb\") as f_in:\n        for _ in range(0, input_size, max_bytes):\n            bytes_in += f_in.read(max_bytes)\n    obj = pickle.loads(bytes_in)\n    return obj","51117379":"def image_crop(predictor, vis_folder, path, current_time, save_result):\n    if os.path.isdir(path):\n        files = get_image_list(path)\n    else:\n        files = [path]\n    files.sort()\n    for image_name in files:\n        outputs, img_info = predictor.inference(image_name)\n        result_image, bboxes_info = predictor.crop_max_dog_or_cat(outputs[0], img_info)\n        if save_result:\n            save_folder = os.path.join(\n                vis_folder, \"test_images\"\n            )\n            os.makedirs(save_folder, exist_ok=True)\n            save_file_name = os.path.join(save_folder, os.path.basename(image_name))\n            logger.info(\"Saving detection result in {}\".format(save_file_name))\n            cv2.imwrite(save_file_name, result_image)\n            if bboxes_info is not None:\n                save_pickle(bboxes_info, f\"{str(save_file_name)}.pkl\")\n","97c6b819":"!pwd","f9d6fdf2":"# args = make_parser().parse_args(args=[\"image_clop\", \"-n\", \"yolox-l\", \"-c\", \"weights\/yolox_l.pth\", \"--path\", \"\/kaggle\/working\/test_center_crop\/\", \"--conf\", \"0.25\", \"--nms\", \"0.45\", \"--tsize\", \"640\", \"--save_result\", \"--device\", \"gpu\"])\nargs = make_parser().parse_args(args=[\"image_clop\", \"-n\", \"yolox-x\", \"-c\", \"weights\/yolox_x.pth\", \"--path\", \"\/kaggle\/input\/petfinder-pawpularity-score\/test\/\", \"--conf\", \"0.01\", \"--nms\", \"0.4\", \"--tsize\", \"640\", \"--save_result\", \"--device\", \"gpu\"])\n# args = make_parser().parse_args(args=[\"image_clop\", \"-n\", \"yolox-x\", \"-c\", \"weights\/yolox_x.pth\", \"--path\", \"\/kaggle\/working\/copy_train\/\", \"--conf\", \"0.01\", \"--nms\", \"0.4\", \"--tsize\", \"640\", \"--save_result\", \"--device\", \"gpu\"])\nargs","ef99b270":"exp = get_exp(args.exp_file, args.name)\nexp.seed = 1031\nexp.output_dir = \"\/kaggle\/working\/\"\nexp.exp_name = \"yolox_x\/test_images\/\"\nexp","cd42e410":"if not args.experiment_name:\n    args.experiment_name = exp.exp_name\n\nfile_name = os.path.join(exp.output_dir, args.experiment_name)\nos.makedirs(file_name, exist_ok=True)\n\nvis_folder = None\nif args.save_result:\n    vis_folder = os.path.join(file_name, \"vis_res\")\n    os.makedirs(vis_folder, exist_ok=True)\n\nif args.trt:\n    args.device = \"gpu\"\n\nlogger.info(\"Args: {}\".format(args))\n\nif args.conf is not None:\n    exp.test_conf = args.conf\nif args.nms is not None:\n    exp.nmsthre = args.nms\nif args.tsize is not None:\n    exp.test_size = (args.tsize, args.tsize)\n\nmodel = exp.get_model()\nlogger.info(\"Model Summary: {}\".format(get_model_info(model, exp.test_size)))\n\nif args.device == \"gpu\":\n    model.cuda()\n    if args.fp16:\n        model.half()  # to FP16\nmodel.eval()\n\nif not args.trt:\n    if args.ckpt is None:\n        ckpt_file = os.path.join(file_name, \"best_ckpt.pth\")\n    else:\n        ckpt_file = args.ckpt\n    logger.info(\"loading checkpoint\")\n    ckpt = torch.load(ckpt_file, map_location=\"cpu\")\n    # load the model state dict\n    model.load_state_dict(ckpt[\"model\"])\n    logger.info(\"loaded checkpoint done.\")\n\nif args.fuse:\n    logger.info(\"\\tFusing model...\")\n    model = fuse_model(model)\n\nif args.trt:\n    assert not args.fuse, \"TensorRT model is not support model fusing!\"\n    trt_file = os.path.join(file_name, \"model_trt.pth\")\n    assert os.path.exists(\n        trt_file\n    ), \"TensorRT model is not found!\\n Run python3 tools\/trt.py first!\"\n    model.head.decode_in_inference = False\n    decoder = model.head.decode_outputs\n    logger.info(\"Using TensorRT to inference\")\nelse:\n    trt_file = None\n    decoder = None\n\npredictor = Predictor(\n    model, exp, COCO_CLASSES, trt_file, decoder,\n    args.device, args.fp16, args.legacy,\n)\ncurrent_time = time.localtime()\nimage_crop(predictor, vis_folder, args.path, current_time, args.save_result)","8d59e9ea":"from glob import glob\n\nbbox_df = pd.DataFrame().from_dict(\n    [\n        load_pickle(filepath) for filepath in glob(f\"\/kaggle\/working\/yolox_x\/test_images\/vis_res\/test_images\/*.jpg.pkl\")\n    ]\n)\nbbox_df[\"Id\"] = bbox_df[\"file_name\"].str.replace(\".jpg\", \"\")\nbbox_df[\"aspect_ratio\"] = bbox_df[\"height\"] \/ bbox_df[\"width\"]\nbbox_df[\"area\"] = bbox_df[\"height\"] * bbox_df[\"width\"]\nbbox_df[\"num_dog\"] = bbox_df[\"cls_names\"].astype(str).str.count(\"dog\")\nbbox_df[\"num_cat\"] = bbox_df[\"cls_names\"].astype(str).str.count(\"cat\")\nbbox_df[\"num_teddy_bear\"] = bbox_df[\"cls_names\"].astype(str).str.count(\"teddy bear\")\nbbox_df[\"num_person\"] = bbox_df[\"cls_names\"].astype(str).str.count(\"person\")\nbbox_df[\"num_dog_cat\"] = bbox_df[\"num_dog\"] + bbox_df[\"num_cat\"]\nbbox_df[\"num_dog_cat_teddy_bear\"] = bbox_df[\"num_dog\"] + bbox_df[\"num_cat\"] + bbox_df[\"num_teddy_bear\"]\nbbox_df","78414351":"import gc\n\nbbox_df.to_csv(\"\/kaggle\/working\/bbox_info.csv\", index=False)\ndel bbox_df\ngc.collect()","53f920fa":"!rm -rf \/kaggle\/working\/yolox_x\/test_images\/vis_res\/test_images\/*.jpg.pkl","e961c9b0":"%cd \/kaggle\/working\/","10076054":"%%time\n\nimport gc\nimport os\nimport sys\nimport warnings\nfrom pathlib import Path\nfrom pprint import pprint\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport torch\nimport torchvision.transforms as T\nfrom torchvision.io import ImageReadMode, read_image\nfrom pytorch_lightning import callbacks, seed_everything, LightningDataModule\nfrom torch.utils.data import DataLoader, Dataset\n\n!pip install ..\/input\/omegaconf\/omegaconf-2.0.5-py3-none-any.whl\nfrom omegaconf import OmegaConf\nsys.path.append('..\/input\/timm-3monthsold\/pytorch-image-models-master 2')\nimport timm\n\nwarnings.filterwarnings(\"ignore\")","51254513":"%%time\n\n!pip install ..\/input\/python-box\/python_box-5.4.1-py3-none-any.whl\nfrom box import Box","6474ab37":"GBDT_EXP_NUM = \"065\"","31ed9a40":"gbdt_config = OmegaConf.load(\n    f\"..\/input\/petfinder-pawpularity-score-xgb-exp-{GBDT_EXP_NUM}\/output.json\"\n)\n# pprint(gbdt_config)","70dc87a7":"gbdt_config[\"features\"]","6094022a":"PAW_EXP_NUM = gbdt_config[\"features\"][\"paw_embed\"].replace(\"exp_\", \"\")\nBIN_PAW_EXP_NUM = gbdt_config[\"features\"][\"bin_paw\"].replace(\"exp_\", \"\")\nAGE_EXP_NUM = gbdt_config[\"features\"][\"age\"].replace(\"exp_\", \"\")\nBREED_EXP_NUM = gbdt_config[\"features\"][\"breed\"].replace(\"exp_\", \"\")\nADOPTION_SPEED_EXP_NUM = gbdt_config[\"features\"][\"adoption_speed\"].replace(\"exp_\", \"\")\nGENDER_EXP_NUM = gbdt_config[\"features\"][\"gender\"].replace(\"exp_\", \"\")\nMATURITY_SIZE_EXP_NUM = gbdt_config[\"features\"][\"maturity_size\"].replace(\"exp_\", \"\")\nBIN_SWIN_EXP_NUM = \"109\"\nCENTER_CROP_SWIN_EXP_NUM = \"101\"","4354c4bb":"paw_config = OmegaConf.load(\n    f\"..\/input\/petfinder-pawpularity-score-exp-{PAW_EXP_NUM}\/exp_{PAW_EXP_NUM}.yaml\"\n)\npaw_config.val_loader.batch_size = 128\npprint(paw_config)","f1b6c030":"bin_paw_config = OmegaConf.load(\n    f\"..\/input\/petfinder-pawpularity-score-exp-{BIN_PAW_EXP_NUM}\/exp_{BIN_PAW_EXP_NUM}.yaml\"\n)\nbin_paw_config.val_loader.batch_size = 128\npprint(bin_paw_config)","da4e9853":"age_config = OmegaConf.load(\n    f\"..\/input\/petfinder-pawpularity-score-exp-{AGE_EXP_NUM}\/exp_{AGE_EXP_NUM}.yaml\"\n)\nage_config.val_loader.batch_size = 128\npprint(age_config)","0549120a":"breed_config = OmegaConf.load(\n    f\"..\/input\/petfinder-pawpularity-score-exp-{BREED_EXP_NUM}\/exp_{BREED_EXP_NUM}.yaml\"\n)\nbreed_config.val_loader.batch_size = 128\npprint(breed_config)","6b64b2f6":"adoption_speed_config = OmegaConf.load(\n    f\"..\/input\/petfinder-pawpularity-score-exp-{ADOPTION_SPEED_EXP_NUM}\/exp_{ADOPTION_SPEED_EXP_NUM}.yaml\"\n)\nadoption_speed_config.val_loader.batch_size = 128\npprint(adoption_speed_config)","78d18429":"gender_config = OmegaConf.load(\n    f\"..\/input\/petfinder-pawpularity-score-exp-{GENDER_EXP_NUM}\/exp_{GENDER_EXP_NUM}.yaml\"\n)\ngender_config.val_loader.batch_size = 128\npprint(gender_config)","83b75bb4":"maturity_size_config = OmegaConf.load(\n    f\"..\/input\/petfinder-pawpularity-score-exp-{MATURITY_SIZE_EXP_NUM}\/exp_{MATURITY_SIZE_EXP_NUM}.yaml\"\n)\nmaturity_size_config.val_loader.batch_size = 128\npprint(maturity_size_config)","7662eb5a":"bin_swin_config = OmegaConf.load(\n    f\"..\/input\/petfinder-pawpularity-score-exp-{BIN_SWIN_EXP_NUM}\/exp_{BIN_SWIN_EXP_NUM}.yaml\"\n)\nbin_swin_config.val_loader.batch_size = 128\npprint(bin_swin_config)","199761a7":"center_crop_swin_config = OmegaConf.load(\n    f\"..\/input\/petfinder-pawpularity-score-exp-{CENTER_CROP_SWIN_EXP_NUM}\/exp_{CENTER_CROP_SWIN_EXP_NUM}.yaml\"\n)\ncenter_crop_swin_config.val_loader.batch_size = 128\npprint(center_crop_swin_config)","8e16d00f":"seed_everything(paw_config.seed)\ntorch.autograd.set_detect_anomaly(True)","d40c8d96":"test_df = pd.read_csv(Path(\"..\/input\/petfinder-pawpularity-score\")\/ \"test.csv\")\n# test_df = pd.concat([test_df]*900, axis=\"index\").reset_index(drop=True)\ntest_df[\"org_Id\"] = test_df[\"Id\"].copy()\ntest_df[\"filepath\"] = (\"\/kaggle\/working\/yolox_x\/test_images\/vis_res\/test_images\/\" + test_df[\"Id\"] + \".jpg\")\ntest_df[\"Id\"] = (\"\/kaggle\/working\/yolox_x\/test_images\/vis_res\/test_images\/\" + test_df[\"Id\"] + \".jpg\")\ntest_df[\"center_Id\"] = (\"\/kaggle\/working\/test_center_crop\/\" + test_df[\"org_Id\"] + \".jpg\")\ndisplay(test_df.head())","a6668dea":"test_df.shape","ee3105bd":"# Pawpularity\nclass PetfinderDataset(Dataset):\n    def __init__(self, df, image_size=224, output_dim=1, over_100_or_not=False):\n        self._X = df[\"Id\"].values\n        self._y = None\n        if \"Pawpularity\" in df.keys():\n            pawpularity = df[\"Pawpularity\"].values\n\n            if output_dim == 100:\n                self._y = np.zeros((len(df), output_dim)).astype(np.float32)\n                for i in range(len(df)):\n                    self._y[i, : pawpularity[i]] = 1\n            else:\n                if over_100_or_not:\n                    self._y = (pawpularity == 100).astype(np.float32)\n                else:\n                    self._y = pawpularity\n\n        self._transform = T.Resize([image_size, image_size])\n\n    def __len__(self):\n        return len(self._X)\n\n    def __getitem__(self, idx):\n        image_path = self._X[idx]\n        image = read_image(image_path)\n        image = self._transform(image)\n        if self._y is not None:\n            label = self._y[idx]\n            return image, label\n        return image, 0\n\nclass PetfinderInferenceDataModule(LightningDataModule):\n    def __init__(\n        self,\n        test_df,\n        cfg,\n    ):\n        super().__init__()\n        self._test_df = test_df\n        self._cfg = cfg\n\n    def predict_dataloader(self):\n        dataset = PetfinderDataset(\n            self._test_df,\n            self._cfg.transform.image_size,\n            self._cfg.model.output_dim,\n            self._cfg.model.over_100_or_not,\n        )\n        return DataLoader(dataset, **self._cfg.val_loader)\n    \n# centercrop Pawpularity\nclass PetfinderCenterDataset(Dataset):\n    def __init__(self, df, image_size=224, output_dim=1, over_100_or_not=False):\n        self._X = df[\"center_Id\"].values\n        self._y = None\n        if \"Pawpularity\" in df.keys():\n            pawpularity = df[\"Pawpularity\"].values\n\n            if output_dim == 100:\n                self._y = np.zeros((len(df), output_dim)).astype(np.float32)\n                for i in range(len(df)):\n                    self._y[i, : pawpularity[i]] = 1\n            else:\n                if over_100_or_not:\n                    self._y = (pawpularity == 100).astype(np.float32)\n                else:\n                    self._y = pawpularity\n\n        self._transform = T.Resize([image_size, image_size])\n\n    def __len__(self):\n        return len(self._X)\n\n    def __getitem__(self, idx):\n        image_path = self._X[idx]\n        image = read_image(image_path)\n        image = self._transform(image)\n        if self._y is not None:\n            label = self._y[idx]\n            return image, label\n        return image, 0\n\nclass PetfinderCenterInferenceDataModule(LightningDataModule):\n    def __init__(\n        self,\n        test_df,\n        cfg,\n    ):\n        super().__init__()\n        self._test_df = test_df\n        self._cfg = cfg\n\n    def predict_dataloader(self):\n        dataset = PetfinderCenterDataset(\n            self._test_df,\n            self._cfg.transform.image_size,\n            self._cfg.model.output_dim,\n            self._cfg.model.over_100_or_not,\n        )\n        return DataLoader(dataset, **self._cfg.val_loader)\n\n\n# Age\nclass PetfinderAgeDataset(Dataset):\n    def __init__(self, df, image_size=224, output_dim=1):\n        self._X = df[\"filepath\"].values\n        self._y = None\n        if \"Age\" in df.keys():\n            self._y = df[\"Age\"].clip(0, 100).values\n\n        self._transform = T.Resize([image_size, image_size])\n\n    def __len__(self):\n        return len(self._X)\n\n    def __getitem__(self, idx):\n        image_path = self._X[idx]\n        image = read_image(image_path, mode=ImageReadMode.RGB)\n        image = self._transform(image)\n        if self._y is not None:\n            label = self._y[idx]\n            return image, label\n        return image, 0\n\nclass PetfinderAgeInferenceDataModule(LightningDataModule):\n    def __init__(\n        self,\n        test_df,\n        cfg,\n    ):\n        super().__init__()\n        self._test_df = test_df\n        self._cfg = cfg\n\n    def predict_dataloader(self):\n        dataset = PetfinderAgeDataset(\n            self._test_df,\n            self._cfg.transform.image_size,\n            self._cfg.model.output_dim,\n        )\n        return DataLoader(dataset, **self._cfg.val_loader)\n\n# Breed\n\nclass PetfinderBreedDataset(Dataset):\n    def __init__(self, df, image_size=224, output_dim=100):\n        self._X = df[\"filepath\"].values\n        self._y = None\n        if \"Breed1\" in df.keys():\n            self._y = np.identity(output_dim)[df[\"Breed1\"].values]\n\n        self._transform = T.Resize([image_size, image_size])\n\n    def __len__(self):\n        return len(self._X)\n\n    def __getitem__(self, idx):\n        image_path = self._X[idx]\n        image = read_image(image_path, mode=ImageReadMode.RGB)\n        image = self._transform(image)\n        if self._y is not None:\n            label = self._y[idx]\n            return image, label\n        return image, 0\n\nclass PetfinderBreedInferenceDataModule(LightningDataModule):\n    def __init__(\n        self,\n        test_df,\n        cfg,\n    ):\n        super().__init__()\n        self._test_df = test_df\n        self._cfg = cfg\n\n    def predict_dataloader(self):\n        dataset = PetfinderBreedDataset(\n            self._test_df,\n            self._cfg.transform.image_size,\n            self._cfg.model.output_dim,\n        )\n        return DataLoader(dataset, **self._cfg.val_loader)\n\n\n# Gender\nclass PetfinderGenderDataset(Dataset):\n    def __init__(self, df, image_size=224, output_dim=3):\n        self._X = df[\"filepath\"].values\n        self._y = None\n        if \"Gender\" in df.keys():\n            self._y = np.identity(output_dim)[df[\"Gender\"].values]\n\n        self._transform = T.Resize([image_size, image_size])\n\n    def __len__(self):\n        return len(self._X)\n\n    def __getitem__(self, idx):\n        image_path = self._X[idx]\n        image = read_image(image_path, mode=ImageReadMode.RGB)\n        image = self._transform(image)\n        if self._y is not None:\n            label = self._y[idx]\n            return image, label\n        return image, 0\n\n\nclass PetfinderGenderInferenceDataModule(LightningDataModule):\n    def __init__(\n        self,\n        test_df,\n        cfg,\n    ):\n        super().__init__()\n        self._test_df = test_df\n        self._cfg = cfg\n\n    def predict_dataloader(self):\n        dataset = PetfinderGenderDataset(self._test_df, self._cfg.transform.image_size)\n        return DataLoader(dataset, **self._cfg.val_loader)\n\n    \n# Adoption Speed\nclass PetfinderAdoptionSpeedDataset(Dataset):\n    def __init__(self, df, image_size=224):\n        self._X = df[\"filepath\"].values\n        self._y = None\n        if \"AdoptionSpeed\" in df.keys():\n            adoption_speed = df[\"AdoptionSpeed\"].values\n\n            self._y = np.zeros((len(df), 4)).astype(np.float32)\n            for i in range(len(df)):\n                self._y[i, : adoption_speed[i]] = 1\n\n        self._transform = T.Resize([image_size, image_size])\n\n    def __len__(self):\n        return len(self._X)\n\n    def __getitem__(self, idx):\n        image_path = self._X[idx]\n        image = read_image(image_path)\n        image = self._transform(image)\n        if self._y is not None:\n            label = self._y[idx]\n            return image, label\n        return image, 0\n\n\nclass PetfinderAdoptionSpeedInferenceDataModule(LightningDataModule):\n    def __init__(\n        self,\n        test_df,\n        cfg,\n    ):\n        super().__init__()\n        self._test_df = test_df\n        self._cfg = cfg\n\n    def predict_dataloader(self):\n        dataset = PetfinderAdoptionSpeedDataset(\n            self._test_df,\n            self._cfg.transform.image_size,\n        )\n        return DataLoader(dataset, **self._cfg.val_loader)\n\n# bin paw\nclass PetfinderBinPawpularityDataset(Dataset):\n    def __init__(self, df, image_size=224, num_bins=14):\n        self._X = df[\"filepath\"].values\n        self._y = None\n        if \"bins_paw\" in df.keys():\n            adoption_speed = df[\"bins_paw\"].values\n\n            self._y = np.zeros((len(df), num_bins)).astype(np.float32)\n            for i in range(len(df)):\n                self._y[i, : adoption_speed[i]] = 1\n\n        self._transform = T.Resize([image_size, image_size])\n\n    def __len__(self):\n        return len(self._X)\n\n    def __getitem__(self, idx):\n        image_path = self._X[idx]\n        image = read_image(image_path)\n        image = self._transform(image)\n        if self._y is not None:\n            label = self._y[idx]\n            return image, label\n        return image, 0\n\nclass PetfinderBinPawpularityInferenceDataModule(LightningDataModule):\n    def __init__(\n        self,\n        test_df,\n        cfg,\n    ):\n        super().__init__()\n        self._test_df = test_df\n        self._cfg = cfg\n\n    def predict_dataloader(self):\n        dataset = PetfinderBinPawpularityDataset(\n            self._test_df,\n            self._cfg.transform.image_size,\n        )\n        return DataLoader(dataset, **self._cfg.val_loader)\n    \n    \n# Maturity Size\nclass PetfinderMaturitySizeDataset(Dataset):\n    def __init__(self, df, image_size=224):\n        self._X = df[\"filepath\"].values\n        self._y = None\n        if \"MaturitySize\" in df.keys():\n            maturity_size = df[\"MaturitySize\"].values\n\n            self._y = np.zeros((len(df), 3)).astype(np.float32)\n            for i in range(len(df)):\n                self._y[i, : maturity_size[i]] = 1\n\n        self._transform = T.Resize([image_size, image_size])\n\n    def __len__(self):\n        return len(self._X)\n\n    def __getitem__(self, idx):\n        image_path = self._X[idx]\n        image = read_image(image_path)\n        image = self._transform(image)\n        if self._y is not None:\n            label = self._y[idx]\n            return image, label\n        return image, 0\n\nclass PetfinderMaturitySizeInferenceDataModule(LightningDataModule):\n    def __init__(\n        self,\n        test_df,\n        cfg,\n    ):\n        super().__init__()\n        self._test_df = test_df\n        self._cfg = cfg\n\n    def predict_dataloader(self):\n        dataset = PetfinderMaturitySizeDataset(\n            self._test_df,\n            self._cfg.transform.image_size,\n        )\n        return DataLoader(dataset, **self._cfg.val_loader)\n","5f1ca9cf":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.logits = logits\n        self.reduce = reduce\n\n    def forward(self, inputs, targets):\n        if self.logits:\n            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n        else:\n            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n        pt = torch.exp(-BCE_loss)\n        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n\n        if self.reduce:\n            return torch.mean(F_loss)\n        else:\n            return F_loss\n","1220527a":"IMAGENET_MEAN = [0.485, 0.456, 0.406]  # RGB\nIMAGENET_STD = [0.229, 0.224, 0.225]  # RGB\n\ndef get_default_transforms():\n    transform = {\n        \"train\": T.Compose(\n            [\n                T.RandomHorizontalFlip(),\n                T.RandomVerticalFlip(),\n                T.RandomAffine(15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n                T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n                T.ConvertImageDtype(torch.float),\n                T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n            ]\n        ),\n        \"val\": T.Compose(\n            [\n                T.ConvertImageDtype(torch.float),\n                T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n            ]\n        ),\n    }\n    return transform\n","545608a9":"def rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1.0 - lam)\n    cut_w = np.int(W * cut_rat)\n    cut_h = np.int(H * cut_rat)\n\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w \/\/ 2, 0, W)\n    bby1 = np.clip(cy - cut_h \/\/ 2, 0, H)\n    bbx2 = np.clip(cx + cut_w \/\/ 2, 0, W)\n    bby2 = np.clip(cy + cut_h \/\/ 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n\n\ndef rand_region(target_size, source_size):\n    t_h, t_w = target_size[2:]\n    s_h, s_w = source_size[2:]\n    cut_h = s_h \/\/ 2\n    cut_w = s_w \/\/ 2\n\n    cx = np.random.randint(cut_w, t_w - cut_w)\n    cy = np.random.randint(cut_h, t_h - cut_h)\n    x1 = cx - cut_w\n    x2 = x1 + s_w\n    y1 = cy - cut_h\n    y2 = y1 + s_h\n    return x1, y1, x2, y2\n\nimport numpy as np\nimport torch\nfrom torchvision.transforms import Resize\n\ndef cutmix(x, y, alpha):\n    assert alpha > 0, \"alpha should be larger than 0\"\n    lam = np.random.beta(alpha, alpha)\n    rand_index = torch.randperm(x.size()[0]).cuda()\n    target_a = y\n    target_b = y[rand_index]\n    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n    x[:, :, bbx1:bbx2, bby1:bby2] = x[rand_index, :, bbx1:bbx2, bby1:bby2]\n    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) \/ (x.size()[-1] * x.size()[-2]))\n    return x, target_a, target_b, lam\n\n\ndef mixup(x, y, alpha):\n    assert alpha > 0, \"alpha should be larger than 0\"\n\n    lam = np.random.beta(alpha, alpha)\n    rand_index = torch.randperm(x.size()[0]).cuda()\n    mixed_x = lam * x + (1 - lam) * x[rand_index, :]\n    target_a, target_b = y, y[rand_index]\n    return mixed_x, target_a, target_b, lam\n\n\ndef resizemix(x, y, alpha=0.1, beta=0.8):\n    assert alpha > 0, \"alpha should be larger than 0\"\n    assert beta < 1, \"beta should be smaller than 1\"\n\n    rand_index = torch.randperm(x.size()[0]).cuda()\n    tau = np.random.uniform(alpha, beta)\n    lam = tau ** 2\n\n    H, W = x.size()[2:]\n    resize_transform = Resize((int(H * tau), int(W * tau)))\n    resized_x = resize_transform(x[rand_index])\n\n    target_a = y[rand_index]\n    target_b = y\n    x1, y1, x2, y2 = rand_region(x.size(), resized_x.size())\n    x[:, :, y1:y2, x1:x2] = resized_x\n    return x, target_a, target_b, lam\n\n\ndef get_strong_transforms(cfg):\n    return eval(cfg.strong_transform.name)","81c96788":"# Copyright 2021 Sea Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nVision OutLOoker (VOLO) implementation\n\"\"\"\n\n# https:\/\/github.com\/sail-sg\/volo\/blob\/main\/models\/volo.py\n\nimport math\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.models.helpers import load_state_dict\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\nfrom timm.models.registry import register_model\n\n\ndef _cfg(url=\"\", **kwargs):\n    return {\n        \"url\": url,\n        \"num_classes\": 1000,\n        \"input_size\": (3, 224, 224),\n        \"pool_size\": None,\n        \"crop_pct\": 0.96,\n        \"interpolation\": \"bicubic\",\n        \"mean\": IMAGENET_DEFAULT_MEAN,\n        \"std\": IMAGENET_DEFAULT_STD,\n        \"first_conv\": \"patch_embed.proj\",\n        \"classifier\": \"head\",\n        **kwargs,\n    }\n\n\ndefault_cfgs = {\n    \"volo\": _cfg(crop_pct=0.96),\n    \"volo_large\": _cfg(crop_pct=1.15),\n}\n\n\nclass OutlookAttention(nn.Module):\n    \"\"\"\n    Implementation of outlook attention\n    --dim: hidden dim\n    --num_heads: number of heads\n    --kernel_size: kernel size in each window for outlook attention\n    return: token features after outlook attention\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        kernel_size=3,\n        padding=1,\n        stride=1,\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        proj_drop=0.0,\n    ):\n        super().__init__()\n        head_dim = dim \/\/ num_heads\n        self.num_heads = num_heads\n        self.kernel_size = kernel_size\n        self.padding = padding\n        self.stride = stride\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n        self.attn = nn.Linear(dim, kernel_size ** 4 * num_heads)\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        self.unfold = nn.Unfold(kernel_size=kernel_size, padding=padding, stride=stride)\n        self.pool = nn.AvgPool2d(kernel_size=stride, stride=stride, ceil_mode=True)\n\n    def forward(self, x):\n        B, H, W, C = x.shape\n\n        v = self.v(x).permute(0, 3, 1, 2)  # B, C, H, W\n\n        h, w = math.ceil(H \/ self.stride), math.ceil(W \/ self.stride)\n        v = (\n            self.unfold(v)\n            .reshape(\n                B,\n                self.num_heads,\n                C \/\/ self.num_heads,\n                self.kernel_size * self.kernel_size,\n                h * w,\n            )\n            .permute(0, 1, 4, 3, 2)\n        )  # B,H,N,kxk,C\/H\n\n        attn = self.pool(x.permute(0, 3, 1, 2)).permute(0, 2, 3, 1)\n        attn = (\n            self.attn(attn)\n            .reshape(\n                B,\n                h * w,\n                self.num_heads,\n                self.kernel_size * self.kernel_size,\n                self.kernel_size * self.kernel_size,\n            )\n            .permute(0, 2, 1, 3, 4)\n        )  # B,H,N,kxk,kxk\n        attn = attn * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (\n            (attn @ v)\n            .permute(0, 1, 4, 3, 2)\n            .reshape(B, C * self.kernel_size * self.kernel_size, h * w)\n        )\n        x = F.fold(\n            x,\n            output_size=(H, W),\n            kernel_size=self.kernel_size,\n            padding=self.padding,\n            stride=self.stride,\n        )\n\n        x = self.proj(x.permute(0, 2, 3, 1))\n        x = self.proj_drop(x)\n\n        return x\n\n\nclass Outlooker(nn.Module):\n    \"\"\"\n    Implementation of outlooker layer: which includes outlook attention + MLP\n    Outlooker is the first stage in our VOLO\n    --dim: hidden dim\n    --num_heads: number of heads\n    --mlp_ratio: mlp ratio\n    --kernel_size: kernel size in each window for outlook attention\n    return: outlooker layer\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        kernel_size,\n        padding,\n        stride=1,\n        num_heads=1,\n        mlp_ratio=3.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        qkv_bias=False,\n        qk_scale=None,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = OutlookAttention(\n            dim,\n            num_heads,\n            kernel_size=kernel_size,\n            padding=padding,\n            stride=stride,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            attn_drop=attn_drop,\n        )\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer\n        )\n\n    def forward(self, x):\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\n\nclass Mlp(nn.Module):\n    \"Implementation of MLP\"\n\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        drop=0.0,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    \"Implementation of self-attention\"\n\n    def __init__(\n        self,\n        dim,\n        num_heads=8,\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        proj_drop=0.0,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim \/\/ num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, H, W, C = x.shape\n\n        qkv = (\n            self.qkv(x)\n            .reshape(B, H * W, 3, self.num_heads, C \/\/ self.num_heads)\n            .permute(2, 0, 3, 1, 4)\n        )\n        q, k, v = (\n            qkv[0],\n            qkv[1],\n            qkv[2],\n        )  # make torchscript happy (cannot use tensor as tuple)\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, H, W, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n\n        return x\n\n\nclass Transformer(nn.Module):\n    \"\"\"\n    Implementation of Transformer,\n    Transformer is the second stage in our VOLO\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            attn_drop=attn_drop,\n        )\n\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer\n        )\n\n    def forward(self, x):\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\n\nclass ClassAttention(nn.Module):\n    \"\"\"\n    Class attention layer from CaiT, see details in CaiT\n    Class attention is the post stage in our VOLO, which is optional.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        num_heads=8,\n        head_dim=None,\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        proj_drop=0.0,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        if head_dim is not None:\n            self.head_dim = head_dim\n        else:\n            head_dim = dim \/\/ num_heads\n            self.head_dim = head_dim\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.kv = nn.Linear(dim, self.head_dim * self.num_heads * 2, bias=qkv_bias)\n        self.q = nn.Linear(dim, self.head_dim * self.num_heads, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(self.head_dim * self.num_heads, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n\n        kv = (\n            self.kv(x)\n            .reshape(B, N, 2, self.num_heads, self.head_dim)\n            .permute(2, 0, 3, 1, 4)\n        )\n        k, v = kv[0], kv[1]  # make torchscript happy (cannot use tensor as tuple)\n        q = self.q(x[:, :1, :]).reshape(B, self.num_heads, 1, self.head_dim)\n        attn = (q * self.scale) @ k.transpose(-2, -1)\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        cls_embed = (\n            (attn @ v).transpose(1, 2).reshape(B, 1, self.head_dim * self.num_heads)\n        )\n        cls_embed = self.proj(cls_embed)\n        cls_embed = self.proj_drop(cls_embed)\n        return cls_embed\n\n\nclass ClassBlock(nn.Module):\n    \"\"\"\n    Class attention block from CaiT, see details in CaiT\n    We use two-layers class attention in our VOLO, which is optional.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        head_dim=None,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = ClassAttention(\n            dim,\n            num_heads=num_heads,\n            head_dim=head_dim,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            attn_drop=attn_drop,\n            proj_drop=drop,\n        )\n        # NOTE: drop path for stochastic depth\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=drop,\n        )\n\n    def forward(self, x):\n        cls_embed = x[:, :1]\n        cls_embed = cls_embed + self.drop_path(self.attn(self.norm1(x)))\n        cls_embed = cls_embed + self.drop_path(self.mlp(self.norm2(cls_embed)))\n        return torch.cat([cls_embed, x[:, 1:]], dim=1)\n\n\ndef get_block(block_type, **kargs):\n    \"\"\"\n    get block by name, specifically for class attention block in here\n    \"\"\"\n    if block_type == \"ca\":\n        return ClassBlock(**kargs)\n\n\ndef rand_bbox(size, lam, scale=1):\n    \"\"\"\n    get bounding box as token labeling (https:\/\/github.com\/zihangJiang\/TokenLabeling)\n    return: bounding box\n    \"\"\"\n    W = size[1] \/\/ scale\n    H = size[2] \/\/ scale\n    cut_rat = np.sqrt(1.0 - lam)\n    cut_w = np.int(W * cut_rat)\n    cut_h = np.int(H * cut_rat)\n\n    # uniform\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w \/\/ 2, 0, W)\n    bby1 = np.clip(cy - cut_h \/\/ 2, 0, H)\n    bbx2 = np.clip(cx + cut_w \/\/ 2, 0, W)\n    bby2 = np.clip(cy + cut_h \/\/ 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\"\n    Image to Patch Embedding.\n    Different with ViT use 1 conv layer, we use 4 conv layers to do patch embedding\n    \"\"\"\n\n    def __init__(\n        self,\n        img_size=224,\n        stem_conv=False,\n        stem_stride=1,\n        patch_size=8,\n        in_chans=3,\n        hidden_dim=64,\n        embed_dim=384,\n    ):\n        super().__init__()\n        assert patch_size in [4, 8, 16]\n\n        self.stem_conv = stem_conv\n        if stem_conv:\n            self.conv = nn.Sequential(\n                nn.Conv2d(\n                    in_chans,\n                    hidden_dim,\n                    kernel_size=7,\n                    stride=stem_stride,\n                    padding=3,\n                    bias=False,\n                ),  # 112x112\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(\n                    hidden_dim,\n                    hidden_dim,\n                    kernel_size=3,\n                    stride=1,\n                    padding=1,\n                    bias=False,\n                ),  # 112x112\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(\n                    hidden_dim,\n                    hidden_dim,\n                    kernel_size=3,\n                    stride=1,\n                    padding=1,\n                    bias=False,\n                ),  # 112x112\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU(inplace=True),\n            )\n\n        self.proj = nn.Conv2d(\n            hidden_dim,\n            embed_dim,\n            kernel_size=patch_size \/\/ stem_stride,\n            stride=patch_size \/\/ stem_stride,\n        )\n        self.num_patches = (img_size \/\/ patch_size) * (img_size \/\/ patch_size)\n\n    def forward(self, x):\n        if self.stem_conv:\n            x = self.conv(x)\n        x = self.proj(x)  # B, C, H, W\n        return x\n\n\nclass Downsample(nn.Module):\n    \"\"\"\n    Image to Patch Embedding, downsampling between stage1 and stage2\n    \"\"\"\n\n    def __init__(self, in_embed_dim, out_embed_dim, patch_size):\n        super().__init__()\n        self.proj = nn.Conv2d(\n            in_embed_dim, out_embed_dim, kernel_size=patch_size, stride=patch_size\n        )\n\n    def forward(self, x):\n        x = x.permute(0, 3, 1, 2)\n        x = self.proj(x)  # B, C, H, W\n        x = x.permute(0, 2, 3, 1)\n        return x\n\n\ndef outlooker_blocks(\n    block_fn,\n    index,\n    dim,\n    layers,\n    num_heads=1,\n    kernel_size=3,\n    padding=1,\n    stride=1,\n    mlp_ratio=3.0,\n    qkv_bias=False,\n    qk_scale=None,\n    attn_drop=0,\n    drop_path_rate=0.0,\n    **kwargs,\n):\n    \"\"\"\n    generate outlooker layer in stage1\n    return: outlooker layers\n    \"\"\"\n    blocks = []\n    for block_idx in range(layers[index]):\n        block_dpr = (\n            drop_path_rate * (block_idx + sum(layers[:index])) \/ (sum(layers) - 1)\n        )\n        blocks.append(\n            block_fn(\n                dim,\n                kernel_size=kernel_size,\n                padding=padding,\n                stride=stride,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                attn_drop=attn_drop,\n                drop_path=block_dpr,\n            )\n        )\n\n    blocks = nn.Sequential(*blocks)\n\n    return blocks\n\n\ndef transformer_blocks(\n    block_fn,\n    index,\n    dim,\n    layers,\n    num_heads,\n    mlp_ratio=3.0,\n    qkv_bias=False,\n    qk_scale=None,\n    attn_drop=0,\n    drop_path_rate=0.0,\n    **kwargs,\n):\n    \"\"\"\n    generate transformer layers in stage2\n    return: transformer layers\n    \"\"\"\n    blocks = []\n    for block_idx in range(layers[index]):\n        block_dpr = (\n            drop_path_rate * (block_idx + sum(layers[:index])) \/ (sum(layers) - 1)\n        )\n        blocks.append(\n            block_fn(\n                dim,\n                num_heads,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                attn_drop=attn_drop,\n                drop_path=block_dpr,\n            )\n        )\n\n    blocks = nn.Sequential(*blocks)\n\n    return blocks\n\n\nclass VOLO(nn.Module):\n    \"\"\"\n    Vision Outlooker, the main class of our model\n    --layers: [x,x,x,x], four blocks in two stages, the first block is outlooker, the\n              other three are transformer, we set four blocks, which are easily\n              applied to downstream tasks\n    --img_size, --in_chans, --num_classes: these three are very easy to understand\n    --patch_size: patch_size in outlook attention\n    --stem_hidden_dim: hidden dim of patch embedding, d1-d4 is 64, d5 is 128\n    --embed_dims, --num_heads: embedding dim, number of heads in each block\n    --downsamples: flags to apply downsampling or not\n    --outlook_attention: flags to apply outlook attention or not\n    --mlp_ratios, --qkv_bias, --qk_scale, --drop_rate: easy to undertand\n    --attn_drop_rate, --drop_path_rate, --norm_layer: easy to undertand\n    --post_layers: post layers like two class attention layers using [ca, ca],\n                  if yes, return_mean=False\n    --return_mean: use mean of all feature tokens for classification, if yes, no class token\n    --return_dense: use token labeling, details are here:\n                    https:\/\/github.com\/zihangJiang\/TokenLabeling\n    --mix_token: mixing tokens as token labeling, details are here:\n                    https:\/\/github.com\/zihangJiang\/TokenLabeling\n    --pooling_scale: pooling_scale=2 means we downsample 2x\n    --out_kernel, --out_stride, --out_padding: kerner size,\n                                               stride, and padding for outlook attention\n    \"\"\"\n\n    def __init__(\n        self,\n        layers,\n        img_size=224,\n        in_chans=3,\n        num_classes=1000,\n        patch_size=8,\n        stem_hidden_dim=64,\n        embed_dims=None,\n        num_heads=None,\n        downsamples=None,\n        outlook_attention=None,\n        mlp_ratios=None,\n        qkv_bias=False,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n        norm_layer=nn.LayerNorm,\n        post_layers=None,\n        return_mean=False,\n        return_dense=True,\n        mix_token=False,\n        pooling_scale=2,\n        out_kernel=3,\n        out_stride=2,\n        out_padding=1,\n    ):\n\n        super().__init__()\n        self.num_classes = num_classes\n        self.num_features = embed_dims[-1]\n        self.patch_embed = PatchEmbed(\n            stem_conv=True,\n            stem_stride=2,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            hidden_dim=stem_hidden_dim,\n            embed_dim=embed_dims[0],\n        )\n\n        # inital positional encoding, we add positional encoding after outlooker blocks\n        self.pos_embed = nn.Parameter(\n            torch.zeros(\n                1,\n                img_size \/\/ patch_size \/\/ pooling_scale,\n                img_size \/\/ patch_size \/\/ pooling_scale,\n                embed_dims[-1],\n            )\n        )\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # set the main block in network\n        network = []\n        for i in range(len(layers)):\n            if outlook_attention[i]:\n                # stage 1\n                stage = outlooker_blocks(\n                    Outlooker,\n                    i,\n                    embed_dims[i],\n                    layers,\n                    downsample=downsamples[i],\n                    num_heads=num_heads[i],\n                    kernel_size=out_kernel,\n                    stride=out_stride,\n                    padding=out_padding,\n                    mlp_ratio=mlp_ratios[i],\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    attn_drop=attn_drop_rate,\n                    norm_layer=norm_layer,\n                )\n                network.append(stage)\n            else:\n                # stage 2\n                stage = transformer_blocks(\n                    Transformer,\n                    i,\n                    embed_dims[i],\n                    layers,\n                    num_heads[i],\n                    mlp_ratio=mlp_ratios[i],\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop_path_rate=drop_path_rate,\n                    attn_drop=attn_drop_rate,\n                    norm_layer=norm_layer,\n                )\n                network.append(stage)\n\n            if downsamples[i]:\n                # downsampling between two stages\n                network.append(Downsample(embed_dims[i], embed_dims[i + 1], 2))\n\n        self.network = nn.ModuleList(network)\n\n        # set post block, for example, class attention layers\n        self.post_network = None\n        if post_layers is not None:\n            self.post_network = nn.ModuleList(\n                [\n                    get_block(\n                        post_layers[i],\n                        dim=embed_dims[-1],\n                        num_heads=num_heads[-1],\n                        mlp_ratio=mlp_ratios[-1],\n                        qkv_bias=qkv_bias,\n                        qk_scale=qk_scale,\n                        attn_drop=attn_drop_rate,\n                        drop_path=0.0,\n                        norm_layer=norm_layer,\n                    )\n                    for i in range(len(post_layers))\n                ]\n            )\n            self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dims[-1]))\n            trunc_normal_(self.cls_token, std=0.02)\n\n        # set output type\n        self.return_mean = return_mean  # if yes, return mean, not use class token\n        self.return_dense = (\n            return_dense  # if yes, return class token and all feature tokens\n        )\n        if return_dense:\n            assert not return_mean, \"cannot return both mean and dense\"\n        self.mix_token = mix_token\n        self.pooling_scale = pooling_scale\n        if mix_token:  # enable token mixing, see token labeling for details.\n            self.beta = 1.0\n            assert return_dense, \"return all tokens if mix_token is enabled\"\n        if return_dense:\n            self.aux_head = (\n                nn.Linear(embed_dims[-1], num_classes)\n                if num_classes > 0\n                else nn.Identity()\n            )\n        self.norm = norm_layer(embed_dims[-1])\n\n        # Classifier head\n        self.head = (\n            nn.Linear(embed_dims[-1], num_classes) if num_classes > 0 else nn.Identity()\n        )\n\n        trunc_normal_(self.pos_embed, std=0.02)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=0.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"pos_embed\", \"cls_token\"}\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes):\n        self.num_classes = num_classes\n        self.head = (\n            nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n        )\n\n    def forward_embeddings(self, x):\n        # patch embedding\n        x = self.patch_embed(x)\n        # B,C,H,W-> B,H,W,C\n        x = x.permute(0, 2, 3, 1)\n        return x\n\n    def forward_tokens(self, x):\n        for idx, block in enumerate(self.network):\n            if idx == 2:  # add positional encoding after outlooker blocks\n                x = x + self.pos_embed\n                x = self.pos_drop(x)\n            x = block(x)\n\n        B, H, W, C = x.shape\n        x = x.reshape(B, -1, C)\n        return x\n\n    def forward_cls(self, x):\n        B, N, C = x.shape\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        for block in self.post_network:\n            x = block(x)\n        return x\n\n    def forward(self, x):\n        # step1: patch embedding\n        x = self.forward_embeddings(x)\n\n        # mix token, see token labeling for details.\n        if self.mix_token and self.training:\n            lam = np.random.beta(self.beta, self.beta)\n            patch_h, patch_w = (\n                x.shape[1] \/\/ self.pooling_scale,\n                x.shape[2] \/\/ self.pooling_scale,\n            )\n            bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam, scale=self.pooling_scale)\n            temp_x = x.clone()\n            sbbx1, sbby1, sbbx2, sbby2 = (\n                self.pooling_scale * bbx1,\n                self.pooling_scale * bby1,\n                self.pooling_scale * bbx2,\n                self.pooling_scale * bby2,\n            )\n            temp_x[:, sbbx1:sbbx2, sbby1:sbby2, :] = x.flip(0)[\n                :, sbbx1:sbbx2, sbby1:sbby2, :\n            ]\n            x = temp_x\n        else:\n            bbx1, bby1, bbx2, bby2 = 0, 0, 0, 0\n\n        # step2: tokens learning in the two stages\n        x = self.forward_tokens(x)\n\n        # step3: post network, apply class attention or not\n        if self.post_network is not None:\n            x = self.forward_cls(x)\n        x = self.norm(x)\n\n        if self.return_mean:  # if no class token, return mean\n            return self.head(x.mean(1))\n\n        x_cls = self.head(x[:, 0])\n        if not self.return_dense:\n            return x_cls\n\n        x_aux = self.aux_head(\n            x[:, 1:]\n        )  # generate classes in all feature tokens, see token labeling\n\n        if not self.training:\n            return x_cls + 0.5 * x_aux.max(1)[0]\n\n        # if (\n        #     self.mix_token and self.training\n        # ):  # reverse \"mix token\", see token labeling for details.\n        #     x_aux = x_aux.reshape(x_aux.shape[0], patch_h, patch_w, x_aux.shape[-1])\n        #\n        #     temp_x = x_aux.clone()\n        #     temp_x[:, bbx1:bbx2, bby1:bby2, :] = x_aux.flip(0)[\n        #         :, bbx1:bbx2, bby1:bby2, :\n        #     ]\n        #     x_aux = temp_x\n        #\n        #     x_aux = x_aux.reshape(x_aux.shape[0], patch_h * patch_w, x_aux.shape[-1])\n        #\n        # # return these: 1. class token, 2. classes from all feature tokens, 3. bounding box\n        # return x_cls, x_aux, (bbx1, bby1, bbx2, bby2)\n\n        return x_cls + 0.5 * x_aux.max(1)[0]\n\n\n@register_model\ndef volo_d1(pretrained=False, **kwargs):\n    \"\"\"\n    VOLO-D1 model, Params: 27M\n    --layers: [x,x,x,x], four blocks in two stages, the first stage(block) is outlooker,\n            the other three blocks are transformer, we set four blocks, which are easily\n             applied to downstream tasks\n    --embed_dims, --num_heads,: embedding dim, number of heads in each block\n    --downsamples: flags to apply downsampling or not in four blocks\n    --outlook_attention: flags to apply outlook attention or not\n    --mlp_ratios: mlp ratio in four blocks\n    --post_layers: post layers like two class attention layers using [ca, ca]\n    See detail for all args in the class VOLO()\n    \"\"\"\n    layers = [4, 4, 8, 2]  # num of layers in the four blocks\n    embed_dims = [192, 384, 384, 384]\n    num_heads = [6, 12, 12, 12]\n    mlp_ratios = [3, 3, 3, 3]\n    downsamples = [True, False, False, False]  # do downsampling after first block\n    outlook_attention = [True, False, False, False]\n    # first block is outlooker (stage1), the other three are transformer (stage2)\n    model = VOLO(\n        layers,\n        embed_dims=embed_dims,\n        num_heads=num_heads,\n        mlp_ratios=mlp_ratios,\n        downsamples=downsamples,\n        outlook_attention=outlook_attention,\n        post_layers=[\"ca\", \"ca\"],\n        **kwargs,\n    )\n    model.default_cfg = default_cfgs[\"volo\"]\n    return model\n\n\n@register_model\ndef volo_d2(pretrained=False, **kwargs):\n    \"\"\"\n    VOLO-D2 model, Params: 59M\n    \"\"\"\n    layers = [6, 4, 10, 4]\n    embed_dims = [256, 512, 512, 512]\n    num_heads = [8, 16, 16, 16]\n    mlp_ratios = [3, 3, 3, 3]\n    downsamples = [True, False, False, False]\n    outlook_attention = [True, False, False, False]\n    model = VOLO(\n        layers,\n        embed_dims=embed_dims,\n        num_heads=num_heads,\n        mlp_ratios=mlp_ratios,\n        downsamples=downsamples,\n        outlook_attention=outlook_attention,\n        post_layers=[\"ca\", \"ca\"],\n        **kwargs,\n    )\n    model.default_cfg = default_cfgs[\"volo\"]\n    return model\n\n\n@register_model\ndef volo_d3(pretrained=False, **kwargs):\n    \"\"\"\n    VOLO-D3 model, Params: 86M\n    \"\"\"\n    layers = [8, 8, 16, 4]\n    embed_dims = [256, 512, 512, 512]\n    num_heads = [8, 16, 16, 16]\n    mlp_ratios = [3, 3, 3, 3]\n    downsamples = [True, False, False, False]\n    outlook_attention = [True, False, False, False]\n    model = VOLO(\n        layers,\n        embed_dims=embed_dims,\n        num_heads=num_heads,\n        mlp_ratios=mlp_ratios,\n        downsamples=downsamples,\n        outlook_attention=outlook_attention,\n        post_layers=[\"ca\", \"ca\"],\n        **kwargs,\n    )\n    model.default_cfg = default_cfgs[\"volo\"]\n    return model\n\n\n@register_model\ndef volo_d4(pretrained=False, **kwargs):\n    \"\"\"\n    VOLO-D4 model, Params: 193M\n    \"\"\"\n    layers = [8, 8, 16, 4]\n    embed_dims = [384, 768, 768, 768]\n    num_heads = [12, 16, 16, 16]\n    mlp_ratios = [3, 3, 3, 3]\n    downsamples = [True, False, False, False]\n    outlook_attention = [True, False, False, False]\n    model = VOLO(\n        layers,\n        embed_dims=embed_dims,\n        num_heads=num_heads,\n        mlp_ratios=mlp_ratios,\n        downsamples=downsamples,\n        outlook_attention=outlook_attention,\n        post_layers=[\"ca\", \"ca\"],\n        **kwargs,\n    )\n    model.default_cfg = default_cfgs[\"volo_large\"]\n    return model\n\n\n@register_model\ndef volo_d5(pretrained=False, **kwargs):\n    \"\"\"\n    VOLO-D5 model, Params: 296M\n    stem_hidden_dim=128, the dim in patch embedding is 128 for VOLO-D5\n    \"\"\"\n    layers = [12, 12, 20, 4]\n    embed_dims = [384, 768, 768, 768]\n    num_heads = [12, 16, 16, 16]\n    mlp_ratios = [4, 4, 4, 4]\n    downsamples = [True, False, False, False]\n    outlook_attention = [True, False, False, False]\n    model = VOLO(\n        layers,\n        embed_dims=embed_dims,\n        num_heads=num_heads,\n        mlp_ratios=mlp_ratios,\n        downsamples=downsamples,\n        outlook_attention=outlook_attention,\n        post_layers=[\"ca\", \"ca\"],\n        stem_hidden_dim=128,\n        **kwargs,\n    )\n    model.default_cfg = default_cfgs[\"volo_large\"]\n    return model\n\n\ndef volo_load_pretrained_weights(\n    model, checkpoint_path, use_ema=False, strict=True, num_classes=1000\n):\n    \"\"\"load pretrained weight for VOLO models\"\"\"\n    state_dict = load_state_dict(checkpoint_path, model, use_ema, num_classes)\n    model.load_state_dict(state_dict, strict=strict)\n","feff19e8":"import torch\nimport torch.optim as optim\nfrom pytorch_lightning import LightningModule\nfrom timm import create_model\nfrom torch import nn\n\n\n\nclass Model(LightningModule):\n    def __init__(self, cfg, batch_size=32):\n        super().__init__()\n        self.cfg = cfg\n        self.batch_size = batch_size\n        self.__build_model()\n        self._criterion = eval(self.cfg.loss)()\n        self.transform = get_default_transforms()\n        self.strong_transform = get_strong_transforms(self.cfg)\n        self.save_hyperparameters(cfg)\n\n    def __build_model(self):\n        self.backbone = create_model(\n            self.cfg.model.name, pretrained=False, num_classes=0, in_chans=3\n        )\n        num_features = self.backbone.num_features\n        if self.cfg.model.name == \"CSWin_64_12211_tiny_224\":\n            self.backbone.load_state_dict(\n                torch.hub.load_state_dict_from_url(\n                    \"https:\/\/github.com\/microsoft\/CSWin-Transformer\/releases\/download\/v0.1.0\/cswin_tiny_224.pth\"\n                ),\n                strict=False,\n            )\n        elif self.cfg.model.name == \"volo_d1\":\n            self.backbone.load_state_dict(\n                torch.hub.load_state_dict_from_url(\n                    \"https:\/\/github.com\/sail-sg\/volo\/releases\/download\/volo_1\/d1_224_84.2.pth.tar\"\n                ),\n                strict=False,\n            )\n        elif self.cfg.model.name == \"volo_d5\":\n            self.backbone.load_state_dict(\n                torch.hub.load_state_dict_from_url(\n                    \"https:\/\/github.com\/sail-sg\/volo\/releases\/download\/volo_1\/d5_224_86.10.pth.tar\"\n                ),\n                strict=False,\n            )\n\n        self.fc = nn.Sequential(\n            nn.Dropout(0.5), nn.Linear(num_features, self.cfg.model.output_dim)\n        )\n\n    def forward(self, x):\n        f = self.backbone(x)\n        out = self.fc(f)\n        return out\n\n    def training_step(self, batch, batch_idx):\n        loss, pred, labels = self.__share_step(batch, \"train\")\n        return {\"loss\": loss, \"pred\": pred, \"labels\": labels}\n\n    def validation_step(self, batch, batch_idx):\n        loss, pred, labels = self.__share_step(batch, \"val\")\n        return {\"pred\": pred, \"labels\": labels}\n\n    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n        images, labels = batch\n        images = self.transform[\"val\"](images)\n        embed = self.backbone(images).detach().cpu()\n        out = self.forward(images).squeeze(1)\n        if self.cfg.model.output_dim == 1:\n            out = out.sigmoid().detach().cpu() * 100.0\n        else:\n            pred = out.sigmoid().detach().cpu().sum(axis=1)\n            out = (pred, out.sigmoid().detach().cpu())\n        return out, embed\n\n    def __share_step(self, batch, mode):\n        images, labels = batch\n        labels = labels.float()\n        if self.cfg.model.output_dim == 1:\n            labels \/= 100.0\n\n        images = self.transform[mode](images)\n\n        if torch.rand(1)[0] < 0.5 and mode == \"train\":\n            mix_images, target_a, target_b, lam = self.strong_transform(\n                images, labels, **self.cfg.strong_transform.params\n            )\n            logits = self.forward(mix_images).squeeze(1)\n            loss = self._criterion(logits, target_a) * lam + (\n                1 - lam\n            ) * self._criterion(logits, target_b)\n        else:\n            logits = self.forward(images).squeeze(1)\n            loss = self._criterion(logits, labels)\n\n        if self.cfg.model.output_dim == 1:\n            pred = logits.sigmoid().detach().cpu() * 100.0\n            labels = labels.detach().cpu() * 100.0\n        else:\n            pred = logits.sigmoid().detach().cpu().sum(axis=1)\n            labels = labels.detach().cpu().sum(axis=1)\n        return loss, pred, labels\n\n    def training_epoch_end(self, outputs):\n        self.__share_epoch_end(outputs, \"train\")\n\n    def validation_epoch_end(self, outputs):\n        self.__share_epoch_end(outputs, \"val\")\n\n    def __share_epoch_end(self, outputs, mode):\n        preds = []\n        labels = []\n        for out in outputs:\n            pred, label = out[\"pred\"], out[\"labels\"]\n            preds.append(pred)\n            labels.append(label)\n        preds = torch.cat(preds)\n        labels = torch.cat(labels)\n        metrics = torch.sqrt(((labels - preds) ** 2).mean())\n        self.log(f\"{mode}_loss\", metrics)\n\n    def check_gradcam(\n        self, dataloader, target_layer, target_category, reshape_transform=None\n    ):\n        cam = GradCAMPlusPlus(\n            model=self,\n            target_layer=target_layer,\n            use_cuda=self.cfg.trainer.gpus,\n            reshape_transform=reshape_transform,\n        )\n\n        org_images, labels = iter(dataloader).next()\n        cam.batch_size = len(org_images)\n        images = self.transform[\"val\"](org_images)\n        images = images.to(self.device)\n        logits = self.forward(images).squeeze(1)\n        pred = logits.sigmoid().detach().cpu().numpy() * 100\n        labels = labels.cpu().numpy()\n\n        grayscale_cam = cam(\n            input_tensor=images, target_category=target_category, eigen_smooth=True\n        )\n        org_images = org_images.detach().cpu().numpy().transpose(0, 2, 3, 1) \/ 255.0\n        return org_images, grayscale_cam, pred, labels\n\n    def configure_optimizers(self):\n        optimizer = eval(self.cfg.optimizer.name)(\n            self.parameters(), **self.cfg.optimizer.params\n        )\n        scheduler = eval(self.cfg.scheduler.name)(\n            optimizer, **self.cfg.scheduler.params\n        )\n        return [optimizer], [scheduler]\n\n    def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):\n        # https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/guides\/speed.html#set-grads-to-none\n        optimizer.zero_grad(set_to_none=False)\n\n\nclass ClassificationModel(LightningModule):\n    def __init__(self, cfg, batch_size=32):\n        super().__init__()\n        self.cfg = cfg\n        self.batch_size = batch_size\n        self.__build_model()\n        self._criterion = eval(self.cfg.loss)()\n        self.transform = get_default_transforms()\n        self.strong_transform = get_strong_transforms(self.cfg)\n        self.save_hyperparameters(cfg)\n\n    def __build_model(self):\n        self.backbone = create_model(\n            self.cfg.model.name, pretrained=False, num_classes=0, in_chans=3\n        )\n        num_features = self.backbone.num_features\n        self.fc = nn.Sequential(\n            nn.Dropout(0.5), nn.Linear(num_features, self.cfg.model.output_dim)\n        )\n\n    def forward(self, x):\n        f = self.backbone(x)\n        out = self.fc(f)\n        return out\n\n    def training_step(self, batch, batch_idx):\n        loss, pred, labels = self.__share_step(batch, \"train\")\n        return {\"loss\": loss, \"pred\": pred, \"labels\": labels}\n\n    def validation_step(self, batch, batch_idx):\n        loss, pred, labels = self.__share_step(batch, \"val\")\n        return {\"pred\": pred, \"labels\": labels}\n\n    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n        images, labels = batch\n        images = self.transform[\"val\"](images)\n        out = self.forward(images).squeeze(1)\n        out = out.sigmoid().detach().cpu()\n        return out\n\n    def __share_step(self, batch, mode):\n        images, labels = batch\n        labels = labels.float()\n\n        images = self.transform[mode](images)\n\n        if torch.rand(1)[0] < 0.5 and mode == \"train\":\n            mix_images, target_a, target_b, lam = self.strong_transform(\n                images, labels, **self.cfg.strong_transform.params\n            )\n            logits = self.forward(mix_images).squeeze(1)\n            loss = self._criterion(logits, target_a) * lam + (\n                1 - lam\n            ) * self._criterion(logits, target_b)\n        else:\n            logits = self.forward(images).squeeze(1)\n            loss = self._criterion(logits, labels)\n\n        pred = logits.sigmoid().detach().cpu()\n        labels = labels.detach().cpu()\n        return loss, pred, labels\n\n    def training_epoch_end(self, outputs):\n        self.__share_epoch_end(outputs, \"train\")\n\n    def validation_epoch_end(self, outputs):\n        self.__share_epoch_end(outputs, \"val\")\n\n    def __share_epoch_end(self, outputs, mode):\n        preds = []\n        labels = []\n        for out in outputs:\n            pred, label = out[\"pred\"], out[\"labels\"]\n            preds.append(pred)\n            labels.append(label)\n        preds = torch.cat(preds)\n        labels = torch.cat(labels)\n        metrics = torch.nn.BCEWithLogitsLoss()(\n            preds.to(torch.float32), labels.to(torch.float32)\n        )\n        self.log(f\"{mode}_loss\", metrics)\n\n    def check_gradcam(\n        self, dataloader, target_layer, target_category, reshape_transform=None\n    ):\n        cam = GradCAMPlusPlus(\n            model=self,\n            target_layer=target_layer,\n            use_cuda=self.cfg.trainer.gpus,\n            reshape_transform=reshape_transform,\n        )\n\n        org_images, labels = iter(dataloader).next()\n        cam.batch_size = len(org_images)\n        images = self.transform[\"val\"](org_images)\n        images = images.to(self.device)\n        logits = self.forward(images).squeeze(1)\n        pred = logits.sigmoid().detach().cpu().numpy() * 100\n        labels = labels.cpu().numpy()\n\n        grayscale_cam = cam(\n            input_tensor=images, target_category=target_category, eigen_smooth=True\n        )\n        org_images = org_images.detach().cpu().numpy().transpose(0, 2, 3, 1) \/ 255.0\n        return org_images, grayscale_cam, pred, labels\n\n    def configure_optimizers(self):\n        optimizer = eval(self.cfg.optimizer.name)(\n            self.parameters(), **self.cfg.optimizer.params\n        )\n        scheduler = eval(self.cfg.scheduler.name)(\n            optimizer, **self.cfg.scheduler.params\n        )\n        return [optimizer], [scheduler]\n\n    def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):\n        # https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/guides\/speed.html#set-grads-to-none\n        optimizer.zero_grad(set_to_none=False)\n\n","6ae4ac90":"configs = [\n    paw_config,\n    bin_swin_config,\n    center_crop_swin_config,\n    bin_paw_config, \n    age_config, \n    breed_config, \n    adoption_speed_config, \n    gender_config,\n    maturity_size_config\n]","4a8b4343":"paw_preds = []\nimg_embs = []\nbin_swin_paw_preds = []\ncenter_crop_swin_paw_preds = []\nbin_paw_preds = []\nbin_paw_preds_vecs = []\nage_preds = []\nbreed_preds = []\nadoption_speed_preds = []\nadoption_speed_preds_vecs = []\ngender_preds = []\nmaturity_size_preds = []\nmaturity_size_preds_vecs = []\nfor fold in range(max([config.n_splits for config in configs])):\n    print(\"*\"*20 + f\"fold: {fold}\" + \"*\"*20)\n    \n    # paw\n    if fold in paw_config.train_folds:\n        print(\"*\"*20 + f\"inference paw\" + \"*\"*20)\n        datamodule = PetfinderInferenceDataModule(test_df, paw_config)\n        model = Model(paw_config)\n        model.load_state_dict(\n            torch.load(f\"..\/input\/petfinder-pawpularity-score-exp-{paw_config.exp_num}\/fold_{fold}_best_loss.ckpt\")[\"state_dict\"] # FIX ME: pth \u304b ckpt\u304b\u7d71\u4e00\u3059\u308b\n        )\n        model = model.cuda().eval()\n\n        trainer = pl.Trainer(\n            logger=False,\n            **paw_config.trainer,\n        )\n        output = trainer.predict(\n            model=model,\n            dataloaders=datamodule.predict_dataloader(),\n            return_predictions=True,\n        )\n        oof_pred = []\n        oof_img_emb = []\n        for _output in output:\n            oof_pred.append(_output[0])\n            oof_img_emb.append(_output[1])\n        oof_pred = torch.cat(oof_pred).numpy()\n        oof_img_emb = torch.cat(oof_img_emb).numpy()\n        paw_preds.append(oof_pred)\n        img_embs.append(oof_img_emb)\n        \n        del datamodule, trainer, model, oof_pred, oof_img_emb, output\n        gc.collect()\n        torch.cuda.empty_cache()\n        pl.utilities.memory.garbage_collection_cuda()\n\n    # bin swin paw\n    if fold in bin_swin_config.train_folds:\n        print(\"*\"*20 + f\"inference bin swin paw\" + \"*\"*20)\n        datamodule = PetfinderInferenceDataModule(test_df, bin_swin_config)\n        model = Model(bin_swin_config)\n        model.load_state_dict(\n            torch.load(f\"..\/input\/petfinder-pawpularity-score-exp-{bin_swin_config.exp_num}\/fold_{fold}_best_loss.ckpt\")[\"state_dict\"]\n        )\n        model = model.cuda().eval()\n\n        trainer = pl.Trainer(\n            logger=False,\n            **bin_swin_config.trainer,\n        )\n        output = trainer.predict(\n            model=model,\n            dataloaders=datamodule.predict_dataloader(),\n            return_predictions=True,\n        )\n        oof_pred = []\n        for _output in output:\n            oof_pred.append(_output[0][0])\n        oof_pred = torch.cat(oof_pred).numpy()\n        bin_swin_paw_preds.append(oof_pred)\n        \n        del datamodule, trainer, model, oof_pred, output\n        gc.collect()\n        torch.cuda.empty_cache()\n        pl.utilities.memory.garbage_collection_cuda()\n        \n    # center_crop_swin paw\n    if fold in center_crop_swin_config.train_folds:\n        print(\"*\"*20 + f\"inference center_crop_swin paw\" + \"*\"*20)\n        datamodule = PetfinderCenterInferenceDataModule(test_df, center_crop_swin_config)\n        model = Model(center_crop_swin_config)\n        model.load_state_dict(\n            torch.load(f\"..\/input\/petfinder-pawpularity-score-exp-{center_crop_swin_config.exp_num}\/fold_{fold}_best_loss.ckpt\")[\"state_dict\"]\n        )\n        model = model.cuda().eval()\n\n        trainer = pl.Trainer(\n            logger=False,\n            **center_crop_swin_config.trainer,\n        )\n        output = trainer.predict(\n            model=model,\n            dataloaders=datamodule.predict_dataloader(),\n            return_predictions=True,\n        )\n        oof_pred = []\n        for _output in output:\n            oof_pred.append(_output[0])\n        oof_pred = torch.cat(oof_pred).numpy()\n        center_crop_swin_paw_preds.append(oof_pred)\n        \n        del datamodule, trainer, model, oof_pred, output\n        gc.collect()\n        torch.cuda.empty_cache()\n        pl.utilities.memory.garbage_collection_cuda()\n       \n    # bin_paw\n    if fold in bin_paw_config.train_folds:\n        print(\"*\"*20 + f\"inference bin_paw\" + \"*\"*20)\n\n        datamodule = PetfinderBinPawpularityInferenceDataModule(test_df, bin_paw_config)\n        model = Model(bin_paw_config)\n        model.load_state_dict(\n            torch.load(f\"..\/input\/petfinder-pawpularity-score-exp-{bin_paw_config.exp_num}\/fold_{fold}_best_loss.ckpt\")[\"state_dict\"]\n        )\n        model = model.cuda().eval()\n\n        trainer = pl.Trainer(\n            logger=False,\n            **bin_paw_config.trainer,\n        )\n\n        output = trainer.predict(\n            model=model,\n            dataloaders=datamodule.predict_dataloader(),\n            return_predictions=True,\n        )\n        oof_pred = []\n        oof_pred_vec = []\n        for _output in output:\n            oof_pred.append(_output[0][0])\n            oof_pred_vec.append(_output[0][1])\n        oof_pred = torch.cat(oof_pred).numpy()\n        bin_paw_preds.append(oof_pred)\n        oof_pred_vec = torch.cat(oof_pred_vec).numpy()\n        bin_paw_preds_vecs.append(oof_pred_vec)\n        \n        del datamodule, trainer, model, oof_pred, oof_pred_vec, output\n        gc.collect()\n        torch.cuda.empty_cache()\n        pl.utilities.memory.garbage_collection_cuda()\n        \n    # age\n    if fold in age_config.train_folds:\n        print(\"*\"*20 + f\"inference age\" + \"*\"*20)\n        datamodule = PetfinderAgeInferenceDataModule(test_df, age_config)\n        model = Model(age_config)\n        model.load_state_dict(\n            torch.load(f\"..\/input\/petfinder-pawpularity-score-exp-{age_config.exp_num}\/fold_{fold}_best_loss.pth\")\n        )\n        model = model.cuda().eval()\n\n        trainer = pl.Trainer(\n            logger=False,\n            **age_config.trainer,\n        )\n        output = trainer.predict(\n            model=model,\n            dataloaders=datamodule.predict_dataloader(),\n            return_predictions=True,\n        )\n        oof_pred = []\n        for _output in output:\n            oof_pred.append(_output[0])\n        oof_pred = torch.cat(oof_pred)\n        age_preds.append(oof_pred.numpy())\n        \n        del datamodule, trainer, model, oof_pred, output\n        gc.collect()\n        torch.cuda.empty_cache()\n        pl.utilities.memory.garbage_collection_cuda()\n\n    # breed\n    if fold in breed_config.train_folds:\n        print(\"*\"*20 + f\"inference breed\" + \"*\"*20)\n\n        datamodule = PetfinderBreedInferenceDataModule(test_df, breed_config)\n        model = ClassificationModel(breed_config)\n        model.load_state_dict(\n            torch.load(f\"..\/input\/petfinder-pawpularity-score-exp-{breed_config.exp_num}\/fold_{fold}_best_loss.pth\")\n        )\n        model = model.cuda().eval()\n\n        trainer = pl.Trainer(\n            logger=False,\n            **breed_config.trainer,\n        )\n\n        oof_pred = trainer.predict(\n            model=model,\n            dataloaders=datamodule.predict_dataloader(),\n            return_predictions=True,\n        )\n        breed_preds.append(torch.cat(oof_pred).numpy())\n        \n        del datamodule, trainer, model, oof_pred\n        gc.collect()\n        torch.cuda.empty_cache()\n        pl.utilities.memory.garbage_collection_cuda()\n\n    # adoption_speed\n    if fold in adoption_speed_config.train_folds:\n        print(\"*\"*20 + f\"inference adoption_speed\" + \"*\"*20)\n\n        datamodule = PetfinderAdoptionSpeedInferenceDataModule(test_df, adoption_speed_config)\n        model = Model(adoption_speed_config)\n        model.load_state_dict(\n            torch.load(f\"..\/input\/petfinder-pawpularity-score-exp-{adoption_speed_config.exp_num}\/fold_{fold}_best_loss.ckpt\")[\"state_dict\"] # FIX ME: pth \u304b ckpt\u304b\u7d71\u4e00\u3059\u308b\n        )\n        model = model.cuda().eval()\n\n        trainer = pl.Trainer(\n            logger=False,\n            **adoption_speed_config.trainer,\n        )\n\n        output = trainer.predict(\n            model=model,\n            dataloaders=datamodule.predict_dataloader(),\n            return_predictions=True,\n        )\n        oof_pred = []\n        oof_pred_vec = []\n        for _output in output:\n            oof_pred.append(_output[0][0])\n            oof_pred_vec.append(_output[0][1])\n        oof_pred = torch.cat(oof_pred).numpy()\n        adoption_speed_preds.append(oof_pred)\n        oof_pred_vec = torch.cat(oof_pred_vec).numpy()\n        adoption_speed_preds_vecs.append(oof_pred_vec)\n        \n        del datamodule, trainer, model, oof_pred, oof_pred_vec, output\n        gc.collect()\n        torch.cuda.empty_cache()\n        pl.utilities.memory.garbage_collection_cuda()\n\n    # gender\n    if fold in gender_config.train_folds:\n        print(\"*\"*20 + f\"inference gender\" + \"*\"*20)\n\n        datamodule = PetfinderGenderInferenceDataModule(test_df, gender_config)\n        model = ClassificationModel(gender_config)\n        model.load_state_dict(\n            torch.load(f\"..\/input\/petfinder-pawpularity-score-exp-{gender_config.exp_num}\/fold_{fold}_best_loss.pth\")\n        )\n        model = model.cuda().eval()\n\n        trainer = pl.Trainer(\n            logger=False,\n            **gender_config.trainer,\n        )\n\n        oof_pred = trainer.predict(\n            model=model,\n            dataloaders=datamodule.predict_dataloader(),\n            return_predictions=True,\n        )\n        gender_preds.append(torch.cat(oof_pred).numpy())\n        \n        del datamodule, trainer, model, oof_pred\n        gc.collect()\n        torch.cuda.empty_cache()\n        pl.utilities.memory.garbage_collection_cuda()\n\n\n    # Maturity Speed\n    if fold in maturity_size_config.train_folds:\n        print(\"*\"*20 + f\"inference MaturitySpeed\" + \"*\"*20)\n\n        datamodule = PetfinderMaturitySizeInferenceDataModule(test_df, maturity_size_config)\n        model = Model(maturity_size_config)\n        model.load_state_dict(\n            torch.load(f\"..\/input\/petfinder-pawpularity-score-exp-{maturity_size_config.exp_num}\/fold_{fold}_best_loss.ckpt\")[\"state_dict\"] \n        )\n        model = model.cuda().eval()\n\n        trainer = pl.Trainer(\n            logger=False,\n            **adoption_speed_config.trainer,\n        )\n\n        output = trainer.predict(\n            model=model,\n            dataloaders=datamodule.predict_dataloader(),\n            return_predictions=True,\n        )\n        oof_pred = []\n        oof_pred_vec = []\n        for _output in output:\n            oof_pred.append(_output[0][0])\n            oof_pred_vec.append(_output[0][1])\n        oof_pred = torch.cat(oof_pred).numpy()\n        maturity_size_preds.append(oof_pred)\n        oof_pred_vec = torch.cat(oof_pred_vec).numpy()\n        maturity_size_preds_vecs.append(oof_pred_vec)\n        \n        del datamodule, trainer, model, oof_pred, oof_pred_vec, output\n        gc.collect()\n        torch.cuda.empty_cache()\n        pl.utilities.memory.garbage_collection_cuda()\n","08a71542":"test_df[\"oof_pred\"] = np.mean(paw_preds, axis=0)\n\nemb_cols = [ f\"emb_{i}\" for i in range(len(img_embs[0][0]))]\ntest_df.loc[:, emb_cols] = np.mean(img_embs, axis=0)\n\ntest_df[\"bin_swin_oof_pred\"] = np.mean(bin_swin_paw_preds, axis=0)\n\ntest_df[\"center_crop_swin_oof_pred\"] = np.mean(center_crop_swin_paw_preds, axis=0)\n\n\ntest_df[\"bin_paw_oof_pred\"] = np.mean(bin_paw_preds, axis=0)\nbin_paw_pred_vec_cols = [f\"bin_paw_pred_vec_{i}\" for i in range(bin_paw_preds_vecs[0].shape[1])]\ntest_df.loc[:, bin_paw_pred_vec_cols] = np.mean(bin_paw_preds_vecs, axis=0)\ntest_df[\"bin_paw_oof_pred_bin\"] = pd.cut(\n                test_df[\"bin_paw_oof_pred\"],\n                3,\n                labels=False,\n            )\n\ntest_df[\"Age\"] = np.mean(age_preds, axis=0)\ntest_df[\"Age_bin\"] = pd.cut(\n    test_df[\"Age\"], [-np.inf, 7, 13, 48, np.inf], labels=False\n)\ntest_df[\"Age_year\"] = test_df[\"Age\"] \/\/ 12\n\nbreed_cols = [f\"breed_pred_{i}\" for i in range(len(breed_preds[0][0]))]\ntest_df.loc[:, breed_cols] = np.mean(breed_preds, axis=0)\ntest_df[\"breed\"] = np.argmax(np.mean(breed_preds, axis=0), axis=1)\n\npred_vec_cols = [f\"adoption_speed_pred_vec_{i}\" for i in range(4)]\ntest_df[\"AdoptionSpeed\"] = np.mean(adoption_speed_preds, axis=0)\ntest_df.loc[:, pred_vec_cols] = np.mean(adoption_speed_preds_vecs, axis=0)\n\ngender_cols = [f\"gender_pred_{i}\" for i in range(len(gender_preds[0][0]))]\ntest_df.loc[:, gender_cols] = np.mean(gender_preds, axis=0)\ntest_df[\"gender\"] = np.argmax(np.mean(gender_preds, axis=0), axis=1)\n\npred_vec_cols = [f\"maturity_size_pred_vec_{i}\" for i in range(3)]\ntest_df[\"MaturitySize\"] = np.mean(maturity_size_preds, axis=0)\ntest_df.loc[:, pred_vec_cols] = np.mean(maturity_size_preds_vecs, axis=0)\ntest_df[\"MaturitySize_bin\"] = pd.cut(\n    test_df[\"MaturitySize\"], 4, labels=False\n)","d512da12":"test_df.head().T","9daa1ca8":"bbox_info = pd.read_csv(\"\/kaggle\/working\/bbox_info.csv\")[\n                [\n                    \"Id\",\n                    \"height\",\n                    \"width\",\n                    \"aspect_ratio\",\n                    \"area\",\n                    \"num_dog\",\n                    \"num_cat\",\n                    \"num_teddy_bear\",\n                    \"num_person\",\n                    \"num_dog_cat\",\n                    \"num_dog_cat_teddy_bear\",\n                ]\n            ]\nbbox_info","82536698":"test_df[[col for col in test_df.columns if \"Id\" in col]]","1337880a":"test_df = pd.merge(test_df, bbox_info, how=\"left\", left_on=\"org_Id\", right_on=\"Id\")\ndel bbox_info\ngc.collect()","88d2e6b8":"test_df = test_df[[col for col in test_df.columns if col != \"Id_y\"]].rename(columns={\"Id_x\": \"Id\"})","0b0fd004":"test_df[[col for col in test_df.columns if \"Id\" in col]]","b936bbac":"\"\"\"types.\"\"\"\nfrom typing import Union\n\nimport numpy as np\nimport pandas as pd\n\n\ntry:\n    from cudf import DataFrame as CDataFrame\n    from cudf import Series as CSeries\n    from cupy import ndarray as CNDArray\nexcept ImportError:\n    CDataFrame = None\n    CSeries = None\n    CNDArray = None\n\nXDataFrame = Union[CDataFrame, pd.DataFrame]\nXSeries = Union[CSeries, np.ndarray]\nXSeriesB = Union[CSeries, pd.Series]\nXNDArray = Union[CNDArray, np.ndarray]\n\nimport importlib\nfrom typing import Any, List, Dict\n\nclass TransformerMixin:\n    \"\"\"Mixin class for `xfeat.encoder`.\"\"\"\n\n    def fit(self, input_df: XDataFrame) -> None:\n        \"\"\"Fit to data frame.\n\n        Args:\n            input_df (XDataFrame): Input data frame.\n        \"\"\"\n        raise NotImplementedError(\"Not implemented yet.\")\n\n    def transform(self, input_df: XDataFrame) -> XDataFrame:\n        \"\"\"Transform data frame.\n\n        Args:\n            input_df (XDataFrame): Input data frame.\n        Returns:\n            XDataFrame : Output data frame.\n        \"\"\"\n        raise NotImplementedError(\"Not implemented yet.\")\n\n    def fit_transform(self, input_df: XDataFrame) -> XDataFrame:\n        \"\"\"Fit to data frame, then transform it.\n\n        Args:\n            input_df (XDataFrame): Input data frame.\n        Returns:\n            XDataFrame : Output data frame.\n        \"\"\"\n        self.fit(input_df)\n        return self.transform(input_df)\n\n\nfrom itertools import combinations\nfrom typing import List, Optional\n\n\nclass ConcatCombination(TransformerMixin):\n    \"\"\"Generate combination of string columns.\n\n    Example:\n        ::\n\n            >>> import pandas as pd\n            >>> from xfeat import ConcatCombination\n            >>> df = pd.DataFrame({\n              \"col1\": [\"a\", \"b\"],\n              \"col2\": [\"@\", \"%\"],\n              \"col3\": [\"X\", \"Y\"]\n            })\n            >>> encoder = ConcatCombination()\n            >>> encoder.fit_transform(df)\n              col1 col2 col3 col1col2_combi col1col3_combi col2col3_combi\n            0    a    @    X             a@             aX             @X\n            1    b    %    Y             b%             bY             %Y\n\n            >>> encoder = ConcatCombination(output_suffix=\"\", drop_origin=True)\n            >>> encoder.fit_transform(df)\n              col1col2 col1col3 col2col3\n            0       a@       aX       @X\n            1       b%       bY       %Y\n\n            >>> encoder = ConcatCombination(output_suffix=\"\", drop_origin=True, r=3)\n            >>> encoder.fit_transform(df)\n              col1col2col3\n            0          a@X\n            1          b%Y\n\n    Args:\n        input_cols (Optional[List[str]]):\n            Input column names. The default uses all columns of the input data frame.\n\n        include_cols (Optional[List[str]]):\n            Columns of the input data frame that are passed on to the output data frame.\n            Defaults: None.\n\n        output_prefix (str):\n            Prefix of output column name. Defaults: `\"\"`.\n\n        output_suffix (str):\n            Suffix of output column name. Defaults: `\"_combi\"`.\n\n        drop_origin (bool):\n            Drop the original column names. Defaults: `False`.\n\n        fillna (str):\n            To concatenate the string columns, the missing values are replaced with the\n            string value `fillna`. Defaults: `\"_NaN_\"`.\n\n        r (int):\n            Length of combinations. Default: `2`.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_cols: Optional[List[str]] = None,\n        include_cols: Optional[List[str]] = None,\n        output_prefix: str = \"\",\n        output_suffix: str = \"_combi\",\n        drop_origin: bool = False,\n        fillna: str = \"_NaN_\",\n        r: int = 2,\n    ):\n        self._input_cols = input_cols or []\n        self._include_cols = include_cols or []\n        self._output_prefix = output_prefix\n        self._output_suffix = output_suffix\n        self._r = r\n        self._fillna = fillna\n        self._drop_origin = drop_origin\n\n    def fit_transform(self, input_df: XDataFrame) -> XDataFrame:\n        \"\"\"Fit to data frame, then transform it.\n\n        Args:\n            input_df (XDataFrame): Input data frame.\n\n        Returns:\n            XDataFrame : Output data frame.\n        \"\"\"\n        input_cols = self._input_cols\n\n        if not input_cols:\n            self._input_cols = [\n                col\n                for col in input_df.columns.tolist()\n                if col not in self._include_cols\n            ]\n\n        return self.transform(input_df)\n\n    def transform(self, input_df: XDataFrame) -> XDataFrame:\n        \"\"\"Transform data frame.\n\n        Args:\n            input_df (XDataFrame): Input data frame.\n\n        Returns:\n            XDataFrame : Output data frame.\n        \"\"\"\n        cols = []\n\n        n_fixed_cols = len(self._include_cols)\n        df = input_df.copy()\n\n        for cols_pairs in combinations(self._input_cols, r=self._r - n_fixed_cols):\n            fixed_cols_str = \"\".join(self._include_cols)\n            pairs_cols_str = \"\".join(cols_pairs)\n            new_col = (\n                self._output_prefix\n                + fixed_cols_str\n                + pairs_cols_str\n                + self._output_suffix\n            )\n            cols.append(new_col)\n\n            concat_cols = self._include_cols + list(cols_pairs)\n            new_ser = None\n            for col in concat_cols:\n                if new_ser is None:\n                    new_ser = df[col].fillna(self._fillna).copy()\n                else:\n                    new_ser = new_ser + df[col].fillna(self._fillna)\n\n            df[new_col] = new_ser\n\n        if self._drop_origin:\n            return df[cols]\n\n        return df\n","f209023a":"from sklearn.preprocessing import LabelEncoder\n\ngroupby_keys = [\n    \"MaturitySize_bin\",\n    \"Age_bin\",\n    \"breed\",\n    \"gender\",\n]\nnew_cat_df = pd.concat(\n    [\n        ConcatCombination(drop_origin=True, r=r).fit_transform(\n            test_df[groupby_keys].astype(str).fillna(\"none\")\n        )\n        for r in [\n            2,\n            3,\n            4,\n        ]\n    ],\n    axis=\"columns\",\n)\n\nfor col in new_cat_df.columns:\n    le = LabelEncoder()\n    new_cat_df[col] = le.fit_transform(new_cat_df[col])","4012fe9d":"new_cat_df","b5a9ce4b":"test_df = pd.concat(\n    [test_df, new_cat_df],\n    axis=\"columns\",\n)\ncombi_cat_cols = new_cat_df.columns.to_list()\ndel new_cat_df\ngc.collect()","c225784a":"groupby_dict = []\nnum_var_list = [\n    \"Age\",\n    \"MaturitySize\",\n    \"AdoptionSpeed\",\n    \"adoption_speed_pred_vec_0\",\n    \"adoption_speed_pred_vec_1\",\n    \"adoption_speed_pred_vec_2\",\n    \"adoption_speed_pred_vec_3\",\n    \"gender_pred_0\",\n    \"gender_pred_1\",\n    \"gender_pred_2\",\n] + [col for col in test_df.keys() if \"breed_pred_\" in col]\nnum_stats_list = [\n    \"mean\",\n    \"var\",\n    \"std\",\n    \"min\",\n    \"max\",\n    \"sum\",\n]\nfor key in groupby_keys + combi_cat_cols + [\"bin_paw_oof_pred_bin\"]:\n    groupby_dict.append(\n        {\n            \"key\": [key],\n            \"var\": num_var_list,\n            \"agg\": num_stats_list,\n        }\n    )","672347de":"from tqdm import tqdm_notebook as tqdm","f6670a51":"class GroupbyTransformer:\n    def __init__(self, param_dict=None):\n        self.param_dict = param_dict\n\n    def _get_params(self, p_dict):\n        key = p_dict[\"key\"]\n        if \"var\" in p_dict.keys():\n            var = p_dict[\"var\"]\n        else:\n            var = self.var\n        if \"agg\" in p_dict.keys():\n            agg = p_dict[\"agg\"]\n        else:\n            agg = self.agg\n        if \"on\" in p_dict.keys():\n            on = p_dict[\"on\"]\n        else:\n            on = key\n        return key, var, agg, on\n\n    def _aggregate(self, dataframe):\n        self.features = []\n        for param_dict in tqdm(self.param_dict):\n            key, var, agg, on = self._get_params(param_dict)\n            all_features = list(set(key + var))\n            new_features = self._get_feature_names(key, var, agg)\n            features = (\n                dataframe[all_features].groupby(key)[var].agg(agg).reset_index()\n            )\n            features.columns = key + new_features\n            self.features.append(features)\n        return self\n\n    def _merge(self, dataframe, merge=True):\n        for param_dict, features in tqdm(\n            zip(self.param_dict, self.features), total=len(self.features)\n        ):\n            key, var, agg, on = self._get_params(param_dict)\n            if merge:\n                dataframe = dataframe.merge(features, how=\"left\", on=on)\n            else:\n                new_features = self._get_feature_names(key, var, agg)\n                dataframe = pd.concat([dataframe, features[new_features]], axis=1)\n        return dataframe\n\n    def transform(self, dataframe):\n        self._aggregate(dataframe)\n        return self._merge(dataframe, merge=True)\n\n    def _get_feature_names(self, key, var, agg):\n        _agg = []\n        for a in agg:\n            if not isinstance(a, str):\n                _agg.append(a.__name__)\n            else:\n                _agg.append(a)\n        return [\"_\".join([a, v, \"groupby\"] + key) for v in var for a in _agg]\n\n    def get_feature_names(self):\n        self.feature_names = []\n        for param_dict in self.param_dict:\n            key, var, agg, on = self._get_params(param_dict)\n            self.feature_names += self._get_feature_names(key, var, agg)\n        return self.feature_names\n\n    def get_numerical_features(self):\n        return self.get_feature_names()\n\n\nclass DiffGroupbyTransformer(GroupbyTransformer):\n    def _aggregate(self):\n        raise NotImplementedError\n\n    def _merge(self):\n        raise NotImplementedError\n\n    def transform(self, dataframe):\n        for param_dict in tqdm(self.param_dict):\n            key, var, agg, on = self._get_params(param_dict)\n            for a in agg:\n                for v in var:\n                    if not isinstance(a, str):\n                        new_feature = \"_\".join([\"diff\", a.__name__, v, \"groupby\"] + key)\n                        base_feature = \"_\".join([a.__name__, v, \"groupby\"] + key)\n                    else:\n                        new_feature = \"_\".join([\"diff\", a, v, \"groupby\"] + key)\n                        base_feature = \"_\".join([a, v, \"groupby\"] + key)\n#                     print(new_feature)\n                    if str(dataframe[v].dtype) == \"category\":\n                        dataframe[new_feature] = dataframe[base_feature] - dataframe[\n                            v\n                        ].astype(int)\n                    else:\n                        dataframe[new_feature] = dataframe[base_feature] - dataframe[v]\n\n        return dataframe\n\n    def _get_feature_names(self, key, var, agg):\n        _agg = []\n        for a in agg:\n            if not isinstance(a, str):\n                _agg.append(a.__name__)\n            else:\n                _agg.append(a)\n        return [\"_\".join([\"diff\", a, v, \"groupby\"] + key) for v in var for a in _agg]\n\n\nclass RatioGroupbyTransformer(GroupbyTransformer):\n    def _aggregate(self):\n        raise NotImplementedError\n\n    def _merge(self):\n        raise NotImplementedError\n\n    def transform(self, dataframe):\n        for param_dict in tqdm(self.param_dict):\n            key, var, agg, on = self._get_params(param_dict)\n            for a in agg:\n                for v in var:\n                    if not isinstance(a, str):\n                        new_feature = \"_\".join(\n                            [\"ratio\", a.__name__, v, \"groupby\"] + key\n                        )\n                        base_feature = \"_\".join([a.__name__, v, \"groupby\"] + key)\n                    else:\n                        new_feature = \"_\".join([\"ratio\", a, v, \"groupby\"] + key)\n                        base_feature = \"_\".join([a, v, \"groupby\"] + key)\n#                     print(new_feature)\n\n                    if str(dataframe[v].dtype) == \"category\":\n                        dataframe[new_feature] = dataframe[base_feature] \/ dataframe[\n                            v\n                        ].astype(int)\n                    else:\n                        dataframe[new_feature] = dataframe[base_feature] \/ dataframe[v]\n        return dataframe\n\n    def _get_feature_names(self, key, var, agg):\n        _agg = []\n        for a in agg:\n            if not isinstance(a, str):\n                _agg.append(a.__name__)\n            else:\n                _agg.append(a)\n        return [\"_\".join([\"ratio\", a, v, \"groupby\"] + key) for v in var for a in _agg]\n","ae311103":"groupby = GroupbyTransformer(groupby_dict)\ntest_df = groupby.transform(test_df)\n\ngroupby = DiffGroupbyTransformer(groupby_dict)\ntest_df = groupby.transform(test_df)\n\ngroupby = RatioGroupbyTransformer(groupby_dict)\ntest_df = groupby.transform(test_df)","8da6d5b5":"print(\"end\")","f7e02d17":"oof_train = pd.read_csv(f\"..\/input\/petfinder-pawpularity-score-xgb-exp-{GBDT_EXP_NUM}\/oof_pred.csv\")\noof_train.head()","a11644c6":"gbdt_config.categorical_cols","a6a53529":"y_train = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/train.csv\")[\"Pawpularity\"].values\ny_train","85c64632":"if gbdt_config[\"pre_process\"][\"xentropy\"]:\n    y_train = y_train \/ 100\nelif gbdt_config[\"pre_process\"][\"tweedie\"]:\n    y_train = 100 - y_train","839ed582":"y_train","25e2601d":"cols = load_pickle(f\"..\/input\/petfinder-pawpularity-score-xgb-exp-{GBDT_EXP_NUM}\/cols.pkl\")","248a5eca":"# for col in cols:\n#     print(col)","19e8d1d2":"models = load_pickle(f\"..\/input\/petfinder-pawpularity-score-xgb-exp-{GBDT_EXP_NUM}\/model.pkl\")","80b00b4b":"models","6e7c3292":"from cuml.preprocessing.TargetEncoder import TargetEncoder\n\nif gbdt_config[\"target_encoding\"]:\n    cat_cols = gbdt_config[\"categorical_cols\"]\n    for cat_col in tqdm(cat_cols):\n        encoder = TargetEncoder(n_folds=4, smooth=0.3)\n        encoder.fit(oof_train[cat_col], y_train)\n        test_df[cat_col + \"_TE\"] = encoder.transform(test_df[cat_col])","a336e6ed":"set(cols) - set(test_df.columns)","7f56c61a":"set(test_df.columns) - set(cols)","ad5047d0":"import xgboost as xgb\nxgb.__version__","5cbb4478":"test_preds = np.zeros(len(test_df))\nfor fold in range(len(models)):\n    model = models[fold]\n\n    test_preds += (\n        model.predict(test_df[cols].values, ntree_limit=model.best_ntree_limit) \/ len(models)\n    )","b2b5fbc2":"test_df[\"xgb_oof_pred_xentropy_065\"] = test_preds * 100","f90def54":"GBDT_EXP_NUM = \"054\"","637c4ddd":"gbdt_config = OmegaConf.load(\n    f\"..\/input\/petfinder-pawpularity-score-xgb-exp-{GBDT_EXP_NUM}\/output.json\"\n)\n# pprint(gbdt_config)","31e349f6":"oof_train = pd.read_csv(f\"..\/input\/petfinder-pawpularity-score-xgb-exp-{GBDT_EXP_NUM}\/oof_pred.csv\")\noof_train.head()","169e7976":"gbdt_config.categorical_cols","23a63da5":"y_train = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/train.csv\")[\"Pawpularity\"].values\ny_train","c38831bb":"if gbdt_config [\"pre_process\"][\"xentropy\"]:\n    y_train = y_train \/ 100\nelif gbdt_config[\"pre_process\"][\"tweedie\"]:\n    y_train = 100 - y_train","3a80f06d":"y_train","5a77ea24":"cols = load_pickle(f\"..\/input\/petfinder-pawpularity-score-xgb-exp-{GBDT_EXP_NUM}\/cols.pkl\")","866ef0c6":"# for col in cols:\n#     print(col)","1ecf8428":"models = load_pickle(f\"..\/input\/petfinder-pawpularity-score-xgb-exp-{GBDT_EXP_NUM}\/model.pkl\")","2c60f3e2":"from cuml.preprocessing.TargetEncoder import TargetEncoder\n\nif gbdt_config[\"target_encoding\"]:\n    cat_cols = gbdt_config[\"categorical_cols\"]\n    for cat_col in tqdm(cat_cols):\n        encoder = TargetEncoder(n_folds=4, smooth=0.3)\n        encoder.fit(oof_train[cat_col], y_train)\n        test_df[cat_col + \"_TE\"] = encoder.transform(test_df[cat_col])","678fe41e":"set(cols) - set(test_df.columns)","cf664822":"set(test_df.columns) - set(cols)","81088d63":"test_preds = np.zeros(len(test_df))\nfor fold in range(len(models)):\n    model = models[fold]\n\n    test_preds += (\n        model.predict(test_df[cols].values, ntree_limit=model.best_ntree_limit) \/ len(models)\n    )","228a508c":"test_df[\"xgb_oof_pred_xentropy_054\"] = test_preds * 100","e2643f62":"GBDT_EXP_NUM = \"064\"","f07e6288":"gbdt_config = OmegaConf.load(\n    f\"..\/input\/petfinder-pawpularity-score-xgb-exp-{GBDT_EXP_NUM}\/output.json\"\n)\n# pprint(gbdt_config)","be1b5735":"oof_train = pd.read_csv(f\"..\/input\/petfinder-pawpularity-score-xgb-exp-{GBDT_EXP_NUM}\/oof_pred.csv\")\noof_train.head()","b594ab47":"gbdt_config.categorical_cols","cf962c14":"y_train = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/train.csv\")[\"Pawpularity\"].values\ny_train","555475d8":"gbdt_config[\"target\"]","6d6c1312":"gbdt_config[\"pre_process\"][\"xentropy\"]","fb3640c9":"gbdt_config[\"pre_process\"][\"tweedie\"]","7c1a239e":"if gbdt_config[\"target\"] == \"residual\":\n    y_train = y_train - (oof_train[\"ensemble_pred\"] - oof_train[\"xgb_oof_pred\"])\n    \nif gbdt_config[\"pre_process\"][\"xentropy\"]:\n    y_train = y_train \/ 100\nelif gbdt_config[\"pre_process\"][\"tweedie\"]:\n    y_train = 100 - y_train","60407f04":"y_train","39317d5b":"cols = load_pickle(f\"..\/input\/petfinder-pawpularity-score-xgb-exp-{GBDT_EXP_NUM}\/cols.pkl\")","89ed1c2e":"# for col in cols:\n#     print(col)","5e4e2efe":"models = load_pickle(f\"..\/input\/petfinder-pawpularity-score-xgb-exp-{GBDT_EXP_NUM}\/model.pkl\")","85d81386":"from cuml.preprocessing.TargetEncoder import TargetEncoder\n\nif gbdt_config[\"target_encoding\"]:\n    cat_cols = gbdt_config[\"categorical_cols\"]\n    for cat_col in tqdm(cat_cols):\n        encoder = TargetEncoder(n_folds=4, smooth=0.3)\n        encoder.fit(oof_train[cat_col], y_train)\n        test_df[cat_col + \"_TE\"] = encoder.transform(test_df[cat_col])","170501a1":"set(cols) - set(test_df.columns)","55b01eaf":"set(test_df.columns) - set(cols)","4fcd3dc1":"test_preds = np.zeros(len(test_df))\nfor fold in range(len(models)):\n    model = models[fold]\n\n    test_preds += (\n        model.predict(test_df[cols].values, ntree_limit=model.best_ntree_limit) \/ len(models)\n    )","17eea87d":"test_df[[col for col in test_df.columns if \"oof\" in col]]","de9a2fc8":"pd.Series(test_preds).describe()","224c1edb":"test_df[\"xgb_oof_pred_residual_064\"] = test_preds + test_df[\"oof_pred\"]","cdb57569":"test_df[[col for col in test_df.columns if (\"oof\" in col) & (\"groupby\" not in col) & (\"TE\" not in col)]]","56064b3a":"# test_df[[\"oof_pred\", \"xgb_oof_pred_xentropy\", \"tweedie_xgb_oof_pred_xentropy\"]]\ntest_df[[\"oof_pred\", \"bin_swin_oof_pred\", \"center_crop_swin_oof_pred\", \"xgb_oof_pred_xentropy_054\", \"xgb_oof_pred_residual_064\", \"xgb_oof_pred_xentropy_065\"]]","58c18223":"weights = np.array([-3.04675469,  1.37952536,  2.31896654,  0.26659276,  3.77603365,1.27495351])","e424b9d1":"test_df[\"Pawpularity\"] = np.dot(test_df[[\"oof_pred\", \"bin_swin_oof_pred\", \"center_crop_swin_oof_pred\", \"xgb_oof_pred_xentropy_054\", \"xgb_oof_pred_residual_064\", \"xgb_oof_pred_xentropy_065\"]].values \/ 6, weights)","ff458ffb":"# test_df[\"Pawpularity\"] = 0.5*test_df[\"oof_pred\"] + 0.5*test_df[\"xgb_oof_pred_xentropy\"]\n# test_df[\"Pawpularity\"] = test_df[\"oof_pred\"] ","9c59ba5e":"karunru = test_df[[\"org_Id\", \"Pawpularity\"]].rename(columns={\"org_Id\": \"Id\"})","0e114229":"import sys\nsys.path.append('..\/input\/timm-3monthsold\/pytorch-image-models-master 2')\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm.notebook import tqdm\nimport os, gc\nimport random\nfrom PIL import Image\nimport tifffile as tiff\nimport cv2\nimport zipfile\nimport collections\nfrom PIL import Image\nfrom sklearn import preprocessing\nfrom random import randint\nfrom glob import glob\nimport shutil\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torchvision\nfrom torchvision import transforms\nimport albumentations as A\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed = 2020\nseed_everything(seed)\nsz1 = 224\nsz2 = 384\nNFOLDS = 5\npet_num = 4\n\n#ImageNet\nmean = np.array([[[0.485, 0.456, 0.406]]])\nstd = np.array([[[0.229, 0.224, 0.225]]])","513f67e8":"test_df = pd.read_csv('\/kaggle\/input\/petfinder-pawpularity-score\/sample_submission.csv')\n#test_df = pd.concat([pd.read_csv('\/kaggle\/input\/petfinder-pawpularity-score\/sample_submission.csv')]*900, ignore_index=True)\ntest_df['class'] = -1\ntest_df['conf'] = -1\ntest_ids = test_df.Id.to_list()\n#test_ids = ['0013fd999caf9a3efe1352ca1b0d937e', '0009c66b9439883ba2750fb825e1d7db', '0007de18844b0dbbb5e1f607da0606e0']\ntest_dir = \"\/kaggle\/input\/petfinder-pawpularity-score\/test\/\"\n#test_dir = '\/kaggle\/input\/petfinder2-sample-images\/'\nshutil.copytree('\/kaggle\/input\/yolov5-official-v31-dataset\/yolov5', '\/kaggle\/working\/yolov5')\nos.chdir('\/kaggle\/working\/yolov5')","0f991aac":"tmp = pd.DataFrame(columns=test_df.columns)\ntmp","4c69afb6":"!python detect.py\\\n--weights \/kaggle\/input\/ultralyticsyolov5aweights\/yolov5x.pt\\\n--class 15 16\\\n--img 512\\\n--conf 0.3\\\n--iou 0.5\\\n--source $test_dir\\\n--name inference\\\n--save-txt --save-conf --exist-ok","aa97df8a":"os.chdir('\/kaggle\/working')\nsave_dir = f'\/kaggle\/working\/crop_images\/'\nos.makedirs(save_dir, exist_ok=True)\n\nfor n, image_id in tqdm(enumerate(test_ids)):\n    orig_image = cv2.imread(f'\/kaggle\/input\/petfinder-pawpularity-score\/test\/{image_id}.jpg')\n    orig_image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n    height = orig_image.shape[0]\n    width = orig_image.shape[1]\n    try:\n        file_path = f'\/kaggle\/working\/yolov5\/runs\/detect\/inference\/labels\/{image_id}.txt'\n        f = open(file_path, 'r')\n        data = np.array(f.read().replace('\\n', ' ').strip().split(' ')).astype(np.float32).reshape(-1, 6)\n        data = data[:, [0, 5, 1, 2, 3, 4]]\n        data = data[np.argsort(data[:, 1])[::-1]]  #sort by conf\n        for i, d in enumerate(data):\n            xmin = int((d[2]-d[4]\/2)*width)\n            ymin = int((d[3]-d[5]\/2)*height)\n            xmax = int((d[2]+d[4]\/2)*width)\n            ymax = int((d[3]+d[5]\/2)*height)\n            width_half = (xmax - xmin) \/\/ 2\n            height_half = (ymax - ymin) \/\/ 2\n            r = np.maximum(width_half, height_half)\n            xc = (xmin + xmax) \/\/ 2\n            yc = (ymin + ymax) \/\/ 2\n            final_xmin = np.maximum(xc-r, 0)\n            final_ymin = np.maximum(yc-r, 0)\n            final_xmax = np.minimum(xc+r, width)\n            final_ymax = np.minimum(yc+r, height)\n            crop_img = orig_image[final_ymin:final_ymax, final_xmin:final_xmax, :]\n            np.save(save_dir + f'{image_id}-{i}', crop_img.astype(np.uint8))\n            df = pd.DataFrame(columns=test_df.columns)\n            df.loc[0, 'Id'] = f'{image_id}-{i}'\n            df.loc[0, 'class'] = d[0]\n            df.loc[0, 'conf'] = d[1]\n            tmp = tmp.append(df, ignore_index=True)\n            \n    except:\n        np.save(save_dir + f'{image_id}-0', orig_image.astype(np.uint8))\n        df = pd.DataFrame(columns=test_df.columns)\n        df.loc[0, 'Id'] = f'{image_id}-0'\n        df.loc[0, 'class'] = 'NA'\n        df.loc[0, 'conf'] = 'NA'\n        tmp = tmp.append(df, ignore_index=True)","9255c38f":"tmp = tmp.rename(columns={'Id': 'Id2'})\ntmp['img_idx'] = tmp['Id2'].apply(lambda x: x.split('-')[1])\ntmp['Id'] = tmp['Id2'].apply(lambda x: x.split('-')[0])\ntmp","a8b796b2":"class Dataset(Dataset):\n    def __init__(self, df, size, transform=None):\n        self.df = df\n        self.size = size\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        image_id = self.df.loc[idx, 'Id2']\n        img = np.load(f'\/kaggle\/working\/crop_images\/{image_id}.npy')\n        img = cv2.resize(img, (self.size, self.size)).astype(np.float32)\n        \n        if self.transform:\n            sample = self.transform(image=img)\n            img = sample['image']\n        \n        img = (img\/255.0 - mean) \/ std\n        img = np.transpose(img, (2, 0, 1))\n        img = torch.from_numpy(img)\n\n        return img\n    \n    \nclass Dataset2(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        image_id = self.df.loc[idx, 'Id']\n        img = np.load(f'\/kaggle\/working\/crop_images\/{image_id}.npy').astype(np.float32)\n        \n        if self.transform:\n            sample = self.transform(image=img)\n            img = sample['image']\n        \n        img = (img\/255.0 - mean) \/ std\n        img = np.transpose(img, (2, 0, 1))\n        img = torch.from_numpy(img)\n\n        return img\n    \n\ndef inference_fn1(data_loader, model, device):\n    model.eval()    \n    val_preds = []\n    \n    for i, x in enumerate(data_loader):\n        img = x\n        img = img.to(device, dtype=torch.float)\n        \n        with torch.no_grad():\n            pred = model(img)\n            val_preds.append(nn.Softmax()(pred).detach().cpu().numpy())\n            \n    val_preds = np.concatenate(val_preds)\n                \n    return val_preds\n\n\ndef inference_fn2(data_loader, model, device):\n    model.eval()    \n    val_preds = []\n    \n    for i, x in enumerate(data_loader):\n        img = x\n        img = img.to(device, dtype=torch.float)\n        \n        with torch.no_grad():\n            pred = model(img)\n            val_preds.append(nn.Sigmoid()(pred).detach().cpu().numpy() * 100)\n            \n    val_preds = np.concatenate(val_preds)\n                \n    return val_preds\n\n\nclass Model(nn.Module):\n    def __init__(self, model_name=None, pretrained=False, num_classes=100):\n        super().__init__()\n        self.model = timm.create_model(\n            model_name=model_name, \n            in_chans=3, \n            pretrained=pretrained\n            )\n        n_features = self.model.head.in_features\n        self.model.head = nn.Linear(n_features, num_classes)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x","d54ccb0f":"#224x224\n#5folds\n#cropped imgs\ntest_ds = Dataset(df=tmp, size=sz1, transform=None)\ntest_dl = DataLoader(dataset=test_ds, batch_size=256, shuffle=False, num_workers=2)\ntarget_cols = np.arange(1, 101)\npredictions1 = 0\npredictions2 = 0\n\nfor fold in range(NFOLDS):\n    print(f'======== 224 yolo FOLD:{fold} inference ========')\n    \n    #SwinT type1: 100 outputs\n    model1 = Model(model_name='swin_large_patch4_window7_224_in22k', num_classes=100)\n    model1.load_state_dict(torch.load(f'\/kaggle\/input\/petfinder2-swint1-weight-016\/swint1_fold_{fold}.pth'))\n    model1.to(device)\n    prediction1 = inference_fn1(test_dl, model1, device)\n    predictions1 += (target_cols.reshape(-1, 100) * prediction1.reshape(-1, 100)).sum(axis=1) \/ NFOLDS\n    \n    #SwinT type2: 1 output\n    model2 = Model(model_name='swin_large_patch4_window7_224_in22k', num_classes=1)\n    model2.model.head = nn.Sequential(\n        nn.Linear(model2.model.head.in_features, 128),\n        nn.Dropout(0.0),\n        nn.Linear(128, 64),\n        nn.Linear(64, 1)\n    )\n    model2.load_state_dict(torch.load(f'\/kaggle\/input\/petfinder2-swint2-weight-011\/swint2_fold_{fold}.pth'))\n    model2.to(device)\n    prediction2 = inference_fn2(test_dl, model2, device).flatten()\n    predictions2 += prediction2 \/ NFOLDS\n    \n    del model1, model2, prediction1, prediction2\n    gc.collect()\n    \ntmp['pred1'] = predictions1\ntmp['pred2'] = predictions2\n\ndel test_ds, test_dl, predictions1, predictions2\ngc.collect()","c3dba81c":"#384x384\n#5folds\n#cropped imgs\ntest_ds = Dataset(df=tmp, size=sz2, transform=None)\ntest_dl = DataLoader(dataset=test_ds, batch_size=64, shuffle=False, num_workers=2)\ntarget_cols = np.arange(1, 101)\npredictions3 = 0\npredictions4 = 0\n\nfor fold in range(NFOLDS):\n    print(f'======== 384 yolo FOLD:{fold} inference ========')\n    \n    #SwinT type1: 100 outputs\n    model1 = Model(model_name='swin_large_patch4_window12_384_in22k', num_classes=100)\n    model1.load_state_dict(torch.load(f'\/kaggle\/input\/petfinder2-swint1-weight-068\/swint1_fold_{fold}.pth'))\n    model1.to(device)\n    prediction3 = inference_fn1(test_dl, model1, device)\n    predictions3 += (target_cols.reshape(-1, 100) * prediction3.reshape(-1, 100)).sum(axis=1) \/ NFOLDS\n    \n    #SwinT type2: 1 output\n    model2 = Model(model_name='swin_large_patch4_window12_384_in22k', num_classes=1)\n    model2.model.head = nn.Sequential(\n        nn.Linear(model2.model.head.in_features, 128),\n        nn.Dropout(0.0),\n        nn.Linear(128, 64),\n        nn.Linear(64, 1)\n    )\n    model2.load_state_dict(torch.load(f'\/kaggle\/input\/petfinder2-swint2-weight-028\/swint2_fold_{fold}.pth'))\n    model2.to(device)\n    prediction4 = inference_fn2(test_dl, model2, device).flatten()\n    predictions4 += prediction4 \/ NFOLDS\n    \n    del model1, model2, prediction3, prediction4\n    gc.collect()\n    \ntmp['pred3'] = predictions3\ntmp['pred4'] = predictions4\n\ndel test_ds, test_dl, predictions3, predictions4\ngc.collect()","9bd35871":"tmp","3d596843":"#postprocessing by multi-pets\ntest_df = tmp.groupby('Id').head(pet_num).groupby('Id').mean().reset_index()\ntest_df","f43b4028":"sub = pd.read_csv('..\/input\/petfinder-pawpularity-score\/sample_submission.csv')\ncoorabi = pd.concat(\n    [sub, \n     test_df[['pred1']], \n     test_df[['pred2']], \n     test_df[['pred3']], \n     test_df[['pred4']]\n    ], axis=1\n)\ncoorabi","593c7973":"!rm -r \/kaggle\/working\/yolov5\n!rm -r \/kaggle\/working\/crop_images","4b473222":"#fastai\n#224x224\n#5folds\n#original imgs\nimport fastai\nfrom fastai.vision.all import *\nfrom fastai.callback.all import *\nimport torchvision.models as torch_models\n\ndef petfinder_rmse(input, target):\n    return 100 * torch.sqrt(F.mse_loss(F.sigmoid(input.flatten()), target))\n\ntest_df = pd.read_csv('\/kaggle\/input\/petfinder-pawpularity-score\/sample_submission.csv')\ntest_df['Path'] = '..\/input\/petfinder-pawpularity-score\/test\/' + test_df['Id'] + '.jpg'\npredictions5 = 0\n\nfor fold in range(NFOLDS):\n    print(f'======== fastai FOLD:{fold} inference ========')\n    learn = load_learner(fname = Path(f'\/kaggle\/input\/petfinder2-swint3-weight-001\/swint3_fold_{fold}.pkl'), cpu=False)\n    test_dl = learn.dls.test_dl(test_df)\n    preds, _ = learn.get_preds(dl=test_dl)\n    predictions5 += preds * 100 \/ NFOLDS\n    del learn, test_dl, preds\n    gc.collect()\n    \ncoorabi['pred5'] = predictions5\n\ndel predictions5\ngc.collect()","34d2e18b":"#224x224\n#5folds\n#center crop\n#binary classifier\ntest_df = pd.read_csv('\/kaggle\/input\/petfinder-pawpularity-score\/sample_submission.csv')\ntest_ids = test_df.Id.to_list()\nos.chdir('\/kaggle\/working')\nsave_dir = f'\/kaggle\/working\/crop_images\/'\nos.makedirs(save_dir, exist_ok=True)\n\nfor n, image_id in tqdm(enumerate(test_ids)):\n    orig_image = cv2.imread(f'\/kaggle\/input\/petfinder-pawpularity-score\/test\/{image_id}.jpg')\n    orig_image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n    height = orig_image.shape[0]\n    width = orig_image.shape[1]\n    xc = width \/\/ 2\n    yc = height \/\/ 2\n    r = np.minimum(xc, yc)\n    xmin = np.maximum(xc-r, 0)\n    ymin = np.maximum(yc-r, 0)\n    xmax = np.minimum(xc+r, width)\n    ymax = np.minimum(yc+r, height)\n    crop_img = orig_image[ymin:ymax, xmin:xmax, :]\n    crop_img = cv2.resize(crop_img, (sz1, sz1)).astype(np.uint8)\n    np.save(save_dir + f'{image_id}', crop_img)\n\ntest_ds = Dataset2(df=coorabi, transform=None)\ntest_dl = DataLoader(dataset=test_ds, batch_size=256, shuffle=False, num_workers=2)\ntarget_cols = np.arange(1, 101)\npredictions6 = 0\npredictions7 = 0\nbinary = 0\n\nfor fold in range(NFOLDS):\n    print(f'======== 224 center FOLD:{fold} inference ========')\n    \n    #SwinT type1: 100 outputs\n    model1 = Model(model_name='swin_large_patch4_window7_224_in22k', num_classes=100)\n    model1.load_state_dict(torch.load(f'\/kaggle\/input\/petfinder2-swint1-weight-076\/swint1_fold_{fold}.pth'))\n    model1.to(device)\n    prediction6 = inference_fn1(test_dl, model1, device)\n    predictions6 += (target_cols.reshape(-1, 100) * prediction6.reshape(-1, 100)).sum(axis=1) \/ NFOLDS\n    \n    #SwinT type2: 1 output\n    model2 = Model(model_name='swin_large_patch4_window7_224_in22k', num_classes=1)\n    model2.model.head = nn.Sequential(\n        nn.Linear(model2.model.head.in_features, 128),\n        nn.Dropout(0.0),\n        nn.Linear(128, 64),\n        nn.Linear(64, 1)\n    )\n    model2.load_state_dict(torch.load(f'\/kaggle\/input\/petfinder2-swint2-weight-036\/swint2_fold_{fold}.pth'))\n    model2.to(device)\n    prediction7 = inference_fn2(test_dl, model2, device).flatten()\n    predictions7 += prediction7 \/ NFOLDS\n    \n    del model1, model2, prediction6, prediction7\n    gc.collect()\n    \n    print(f'======== 224 Binary FOLD:{fold} inference ========')\n    model = Model(model_name='swin_large_patch4_window7_224_in22k', num_classes=1)\n    model.model.head = nn.Sequential(\n        nn.Linear(model.model.head.in_features, 128),\n        nn.Dropout(0.0),\n        nn.Linear(128, 64),\n        nn.Linear(64, 1)\n    )\n    model.load_state_dict(torch.load(f'\/kaggle\/input\/petfinder2-binary-classifier-weight\/fold_{fold}.pth'))\n    model.to(device)\n    prediction = inference_fn2(test_dl, model, device).flatten()\n    binary += prediction \/ NFOLDS\n    \n    del model, prediction\n    gc.collect()\n\ncoorabi['pred6'] = predictions6\ncoorabi['pred7'] = predictions7\ncoorabi['binary1'] = binary\n\n!rm -r \/kaggle\/working\/crop_images\ndel test_ds, test_dl, predictions6, predictions7, binary\ngc.collect()","37aff79e":"#384x384\n#5folds\n#center crop\n#binary classifier\ntest_df = pd.read_csv('\/kaggle\/input\/petfinder-pawpularity-score\/sample_submission.csv')\ntest_ids = test_df.Id.to_list()\nos.chdir('\/kaggle\/working')\nsave_dir = f'\/kaggle\/working\/crop_images\/'\nos.makedirs(save_dir, exist_ok=True)\n\nfor n, image_id in tqdm(enumerate(test_ids)):\n    orig_image = cv2.imread(f'\/kaggle\/input\/petfinder-pawpularity-score\/test\/{image_id}.jpg')\n    orig_image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n    height = orig_image.shape[0]\n    width = orig_image.shape[1]\n    xc = width \/\/ 2\n    yc = height \/\/ 2\n    r = np.minimum(xc, yc)\n    xmin = np.maximum(xc-r, 0)\n    ymin = np.maximum(yc-r, 0)\n    xmax = np.minimum(xc+r, width)\n    ymax = np.minimum(yc+r, height)\n    crop_img = orig_image[ymin:ymax, xmin:xmax, :]\n    crop_img = cv2.resize(crop_img, (sz2, sz2)).astype(np.uint8)\n    np.save(save_dir + f'{image_id}', crop_img)\n\ntest_ds = Dataset2(df=coorabi, transform=None)\ntest_dl = DataLoader(dataset=test_ds, batch_size=64, shuffle=False, num_workers=2)\ntarget_cols = np.arange(1, 101)\npredictions8 = 0\npredictions9 = 0\nbinary = 0\n\nfor fold in range(NFOLDS):\n    print(f'======== 384 center FOLD:{fold} inference ========')\n    \n    #SwinT type1: 100 outputs\n    model1 = Model(model_name='swin_large_patch4_window12_384_in22k', num_classes=100)\n    model1.load_state_dict(torch.load(f'\/kaggle\/input\/petfinder2-swint1-weight-077\/swint1_fold_{fold}.pth'))\n    model1.to(device)\n    prediction8 = inference_fn1(test_dl, model1, device)\n    predictions8 += (target_cols.reshape(-1, 100) * prediction8.reshape(-1, 100)).sum(axis=1) \/ NFOLDS\n    \n    #SwinT type2: 1 output\n    model2 = Model(model_name='swin_large_patch4_window12_384_in22k', num_classes=1)\n    model2.model.head = nn.Sequential(\n        nn.Linear(model2.model.head.in_features, 128),\n        nn.Dropout(0.0),\n        nn.Linear(128, 64),\n        nn.Linear(64, 1)\n    )\n    model2.load_state_dict(torch.load(f'\/kaggle\/input\/petfinder2-swint2-weight-037\/swint2_fold_{fold}.pth'))\n    model2.to(device)\n    prediction9 = inference_fn2(test_dl, model2, device).flatten()\n    predictions9 += prediction9 \/ NFOLDS\n    \n    del model1, model2, prediction8, prediction9\n    gc.collect()\n    \n    print(f'======== 384 Binary FOLD:{fold} inference ========')\n    model = Model(model_name='swin_large_patch4_window12_384_in22k', num_classes=1)\n    model.model.head = nn.Sequential(\n        nn.Linear(model.model.head.in_features, 128),\n        nn.Dropout(0.0),\n        nn.Linear(128, 64),\n        nn.Linear(64, 1)\n    )\n    model.load_state_dict(torch.load(f'\/kaggle\/input\/petfinder2-binary-classifier-weight2\/fold_{fold}.pth'))\n    model.to(device)\n    prediction = inference_fn2(test_dl, model, device).flatten()\n    binary += prediction \/ NFOLDS\n    \n    del model, prediction\n    gc.collect()\n    \ncoorabi['pred8'] = predictions8\ncoorabi['pred9'] = predictions9\ncoorabi['binary2'] = binary\n\n!rm -r \/kaggle\/working\/crop_images\ndel test_ds, test_dl, predictions8, predictions9, binary\ngc.collect()","020996af":"coorabi['Pawpularity'] = ((coorabi['pred1'] * 0.275 + coorabi['pred2'] * 0.225 + coorabi['pred3'] * 0.275 + coorabi['pred4'] * 0.225) * 0.75 + coorabi['pred5'] * 0.25) * 0.6 + (coorabi['pred6'] * 0.22 + coorabi['pred7'] * 0.18 + coorabi['pred8'] * 0.33 + coorabi['pred9'] * 0.27) * 0.4\ncoorabi['binary'] = (coorabi['binary1'] + coorabi['binary2']) \/ 2","80ea52d6":"coorabi","fbc7fea6":"final = pd.read_csv('..\/input\/petfinder-pawpularity-score\/sample_submission.csv')\nfinal","7614f303":"final['Pawpularity'] = karunru['Pawpularity'].values * 0.5 + coorabi['Pawpularity'].values * 0.5\nfinal['binary'] = coorabi['binary'].values\nfinal","b96b7ab2":"idx = final[(final['binary'] > 51) & (final['Pawpularity'] < 49)].index","1769dd26":"final.loc[idx, 'Pawpularity'] = final.loc[idx, 'Pawpularity'].values * 0.1 + final.loc[idx, 'binary'].values * 0.9","e1f81456":"final","498c732a":"final[['Id', 'Pawpularity']].to_csv('\/kaggle\/working\/submission.csv', index=False)","4944a465":"# Ensemble","53771089":"# PostProcess","0e41aadd":"# \u3084\u307e\u3074\u30fc\u3055\u3093 part","b22b59ee":"# \u30af\u30fc\u30e9\u30d3 part"}}