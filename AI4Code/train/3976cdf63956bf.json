{"cell_type":{"fe838bb6":"code","08ceda2c":"code","e188e562":"code","aa3874b1":"code","e70f4f04":"code","2c0612fe":"code","35172fcc":"code","20354d02":"code","51e581c8":"code","86d04e0b":"code","78b4b7e8":"code","a852d3ae":"code","acef21c6":"code","65644d9f":"code","feec72fd":"code","e6955a83":"code","8fa98d75":"code","eb0023be":"code","6eb44514":"code","d4523457":"code","aa5bdca7":"code","764fa992":"code","53430a53":"code","405f272d":"code","82968850":"code","dcc5363c":"code","4d1b0d2d":"code","1426bf53":"code","92071ba0":"code","cd1378c1":"code","186ffb17":"code","78a5c65e":"code","5d45169a":"code","85751bd3":"code","881bc024":"code","617a0faa":"code","df12b67d":"code","f32473c8":"code","99422452":"code","230e025e":"code","cebe71d5":"code","5aa09810":"code","92fe345b":"code","cb8e903c":"code","7c70ae07":"code","3a71b692":"code","42d5bfed":"code","816a14d9":"code","08f639fb":"code","4f28a31a":"code","43dd6cd5":"code","250d14c3":"code","275c008c":"code","2cf3513c":"code","ff9569c0":"code","e76c5d0d":"code","e747159b":"code","1fc1775c":"code","68c838b1":"code","b204b676":"code","a4ca9291":"code","00c83533":"code","2de821e5":"code","a3fdf49f":"code","5130767f":"code","41c33ecd":"code","deab89fa":"code","0ce11600":"code","e8f9df9a":"code","5c028cfe":"code","57fa2150":"code","04a127c4":"code","99014c83":"code","e5df6352":"code","2d2a3188":"code","d0ac6201":"code","218be14f":"code","a6c13d8a":"code","86a49ee2":"code","7b025ba6":"code","8ef560b8":"code","c406b6cc":"code","eb373125":"code","3bc4582c":"code","4c7c4629":"code","9da5a1c3":"code","4dfffd98":"code","237b8339":"code","bcbc65c6":"code","8d60ae83":"code","302a8b9b":"code","94dabe56":"code","b577ad6a":"code","1acc9cd5":"code","049bad8f":"code","122089f1":"code","e942a652":"code","c0d2be72":"markdown","479f909d":"markdown","e9668238":"markdown","c9c7f9b9":"markdown","3e44d44d":"markdown","ac8fe405":"markdown","bbe3b856":"markdown","9f0bc49c":"markdown","d5703cc6":"markdown","c07a40e7":"markdown","cf020ac9":"markdown","bdc0b092":"markdown","fc6e1bc4":"markdown","919de65a":"markdown","6d464ad3":"markdown","a6b0814d":"markdown","901b16fe":"markdown","8f4619bc":"markdown","c9508616":"markdown","ab6cbe6a":"markdown","5fa7877b":"markdown","1bcdf7e3":"markdown","ce940fd5":"markdown","c462b3be":"markdown","7460f0d8":"markdown"},"source":{"fe838bb6":"# v2: 1.102507841834652\n# v9 : del area_floor\n# 10: remove 1099\n# 11: dayweek\n# 12 : del bil_median\n# 13 : leak data update\n# 14 : site-0 unit correction\n# sg filter\n\n#v3 : add diff2 (bug)\n#v4 : add diff2\n#v5 : black 10\n","08ceda2c":"black_day = 10\noutlier = False\nrescale = False\n\ndebug=False\nnum_rounds = 1000 #1000\n\nclip0=False # minus meter confirmed in test(site0 leak data)\n\nfolds = 3 # 3, 6, 12\n# 6: 1.1069822104487446\n# 3: 1.102507841834652\n# 12: 1.1074824417420517\n\nuse_ucf=False\nucf_clip=False\n\nucf_year = [2017, 2018] # ucf data year used in train \n\npredmode='all' # 'valid', train', 'all'","e188e562":"import gc\nimport os\nfrom pathlib import Path\nimport random\nimport sys\n\nfrom tqdm import tqdm_notebook as tqdm\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom IPython.core.display import display, HTML\n\n# --- plotly ---\nfrom plotly import tools, subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\n\n# --- models ---\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\n\nfrom sklearn.metrics import mean_squared_error","aa3874b1":"os.listdir('..\/input')","e70f4f04":"# Original code from https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage by @gemartin\n# Modified to support timestamp type, categorical type\n# Modified to add option to use float16 or not. feather format does not support float16.\nfrom pandas.api.types import is_datetime64_any_dtype as is_datetime\nfrom pandas.api.types import is_categorical_dtype\n\ndef reduce_mem_usage(df, use_float16=False):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n            # skip datetime type or categorical type\n            continue\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","2c0612fe":"\nzone_dict={0:4,1:0,2:7,3:4,4:7,5:0,6:4,7:4,8:4,9:5,10:7,11:4,12:0,13:5,14:4,15:4} \n\ndef set_local(df):\n    for sid, zone in zone_dict.items():\n        sids = df.site_id == sid\n        df.loc[sids, 'timestamp'] = df[sids].timestamp - pd.offsets.Hour(zone)","35172fcc":"!ls ..\/input","20354d02":"%%time\nroot = Path('..\/input\/ashrae-feather-format-for-fast-loading')\nroot_black = Path('..\/input\/ashrae-local-datatime-and-black-count')\ntrain_df = pd.read_feather(root\/'train.feather')\ntrain_df_black = pd.read_feather(root_black\/'train_black.feather')\nweather_train_df = pd.read_feather(root\/'weather_train.feather')\n#weather_test_df = pd.read_feather(root\/'weather_test.feather')\nbuilding_meta_df = pd.read_feather(root\/'building_metadata.feather')","51e581c8":"train_df = train_df [ train_df['building_id'] != 1099 ]","86d04e0b":"building_meta_df['floor_area'] = building_meta_df.square_feet \/ building_meta_df.floor_count","78b4b7e8":"import holidays\n\nen_holidays = holidays.England()\nir_holidays = holidays.Ireland()\nca_holidays = holidays.Canada()\nus_holidays = holidays.UnitedStates()\n\ndef add_holiyday(df_weather):\n    en_idx = df_weather.query('site_id == 1 or site_id == 5').index\n    ir_idx = df_weather.query('site_id == 12').index\n    ca_idx = df_weather.query('site_id == 7 or site_id == 11').index\n    us_idx = df_weather.query('site_id == 0 or site_id == 2 or site_id == 3 or site_id == 4 or site_id == 6 or site_id == 8 or site_id == 9 or site_id == 10 or site_id == 13 or site_id == 14 or site_id == 15').index\n\n    df_weather['IsHoliday'] = 0\n    df_weather.loc[en_idx, 'IsHoliday'] = df_weather.loc[en_idx, 'timestamp'].apply(lambda x: en_holidays.get(x, default=0))\n    df_weather.loc[ir_idx, 'IsHoliday'] = df_weather.loc[ir_idx, 'timestamp'].apply(lambda x: ir_holidays.get(x, default=0))\n    df_weather.loc[ca_idx, 'IsHoliday'] = df_weather.loc[ca_idx, 'timestamp'].apply(lambda x: ca_holidays.get(x, default=0))\n    df_weather.loc[us_idx, 'IsHoliday'] = df_weather.loc[us_idx, 'timestamp'].apply(lambda x: us_holidays.get(x, default=0))\n\n    holiday_idx = df_weather['IsHoliday'] != 0\n    df_weather.loc[holiday_idx, 'IsHoliday'] = 1\n    df_weather['IsHoliday'] = df_weather['IsHoliday'].astype(np.uint8)","a852d3ae":"set_local(weather_train_df)\nadd_holiyday(weather_train_df)","acef21c6":"weather_train_df.head()","65644d9f":"train_df_black[train_df_black.building_id == 954].black_count.plot()","feec72fd":"train_df.shape","e6955a83":"train_df = train_df[train_df_black.black_count < 24*black_day]","8fa98d75":"train_df.shape","eb0023be":"del train_df_black\ngc.collect()","6eb44514":"#building_meta_df[building_meta_df.site_id == 0]","d4523457":"train_df = train_df.query('not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-20\")')","aa5bdca7":"train_df[train_df.building_id == 954].meter_reading.plot()","764fa992":"(train_df[train_df.building_id == 954].meter_reading==0).astype(int).plot()","53430a53":"train_df[train_df.building_id == 1221].meter_reading.plot()","405f272d":"train_df = train_df.query('not (building_id == 954 & meter_reading == 0)')\ntrain_df = train_df.query('not (building_id == 1221 & meter_reading == 0)')","82968850":"train_df[train_df.building_id == 954].meter_reading.plot()","dcc5363c":"train_df[train_df.building_id == 1221].meter_reading.plot()","4d1b0d2d":"funny_bids = [993, 1168,  904,  954,  778, 1021]\n\nprint ('before', len(train_df))\n\nif outlier:\n    #993\n    # or delete\n    train_df.loc[(train_df.building_id == 993) & (train_df.meter == 0) & (train_df.meter_reading > 30000), 'meter_reading'] = 31921\n    train_df.loc[(train_df.building_id == 993) & (train_df.meter == 1) & (train_df.meter_reading > 90000), 'meter_reading'] =  96545.5\n\n    #1168\n    train_df = train_df[((train_df.building_id == 1168) & (train_df.meter == 0) & (train_df.meter_reading >10000)) == False]\n\n    #904\n    train_df.loc[(train_df.building_id == 904) & (train_df.meter == 0)& (train_df.meter_reading >10000), 'meter_reading'] = 11306\n\n    #954\n    train_df = train_df[((train_df.building_id == 954) & (train_df.meter_reading >10000))==False]\n\nif rescale:\n    #778 rescale ?\n    train_df.loc[(train_df.building_id == 778) & (train_df.meter == 1), 'meter_reading'] = train_df.loc[(train_df.building_id == 778) & (train_df.meter == 1), 'meter_reading']\/ 100\n    #\n    #1021 rescale ?\n    train_df.loc[(train_df.building_id == 1021) & (train_df.meter == 3), 'meter_reading'] = train_df.loc[(train_df.building_id == 1021) & (train_df.meter == 3), 'meter_reading']\/ 1000\n    #plt.plot(np.log1p(train_df.loc[(train_df.building_id == 1021) & (train_df.meter == 3), 'meter_reading'] ))\n\ntrain_df = train_df.reset_index()\n\n\nprint ('after', len(train_df))\ngc.collect()","1426bf53":"# for bid in funny_bids:\n#     plt.figure(figsize=[20,3])\n#     plt.subplot(141)\n#     plt.plot(train_df[(train_df.building_id == bid) & (train_df.meter == 0)].meter_reading)\n#     plt.subplot(142)\n#     plt.plot(train_df[(train_df.building_id == bid) & (train_df.meter == 1)].meter_reading)\n#     plt.subplot(143)\n#     plt.plot(train_df[(train_df.building_id == bid) & (train_df.meter == 2)].meter_reading)\n#     plt.subplot(144)\n#     plt.plot(train_df[(train_df.building_id == bid) & (train_df.meter == 3)].meter_reading)\n#     plt.title(bid)","92071ba0":"# https:\/\/www.kaggle.com\/c\/ashrae-energy-prediction\/discussion\/119261#latest-684102\nsite_0_bids = building_meta_df[building_meta_df.site_id == 0].building_id.unique()\nprint (len(site_0_bids), len(train_df[train_df.building_id.isin(site_0_bids)].building_id.unique()))\ntrain_df[train_df.building_id.isin(site_0_bids)].head()","cd1378c1":"train_df.loc[(train_df.building_id.isin(site_0_bids)) & (train_df.meter==0), 'meter_reading'] = train_df[(train_df.building_id.isin(site_0_bids)) & (train_df.meter==0) ]['meter_reading'] * 0.2931","186ffb17":"train_df[train_df.building_id.isin(site_0_bids)].head()","78a5c65e":"train_df['date'] = train_df['timestamp'].dt.date\ntrain_df['meter_reading_log1p'] = np.log1p(train_df['meter_reading'])","5d45169a":"def preprocess(df):\n    df[\"hour\"] = df[\"timestamp\"].dt.hour\n    df[\"day\"] = df[\"timestamp\"].dt.day\n    df[\"weekend\"] = df[\"timestamp\"].dt.weekday\n    df[\"month\"] = df[\"timestamp\"].dt.month\n    df[\"dayofweek\"] = df[\"timestamp\"].dt.dayofweek\n\n#     hour_rad = df[\"hour\"].values \/ 24. * 2 * np.pi\n#     df[\"hour_sin\"] = np.sin(hour_rad)\n#     df[\"hour_cos\"] = np.cos(hour_rad)","85751bd3":"preprocess(train_df)","881bc024":"if use_ucf:\n    train_df = train_df.sort_values('month')\n    train_df = train_df.reset_index()","617a0faa":"#df_group = train_df.groupby('building_id')['meter_reading_log1p']\n#building_median = df_group.median().astype(np.float16)\n#train_df['building_median'] = train_df['building_id'].map(building_median)","df12b67d":"weather_train_df.head()","f32473c8":"# weather_train_df.describe()","99422452":"weather_train_df.isna().sum()","230e025e":"weather_train_df.shape","cebe71d5":"weather_train_df.groupby('site_id').apply(lambda group: group.isna().sum())","5aa09810":"weather_train_df = weather_train_df.groupby('site_id').apply(lambda group: group.interpolate(limit_direction='both'))","92fe345b":"weather_train_df.groupby('site_id').apply(lambda group: group.isna().sum())","cb8e903c":"def add_lag_feature(weather_df, window=3):\n    group_df = weather_df.groupby('site_id')\n    cols = ['air_temperature', 'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction', 'wind_speed']\n    rolled = group_df[cols].rolling(window=window, min_periods=0)\n    lag_mean = rolled.mean().reset_index().astype(np.float16)\n    lag_max = rolled.max().reset_index().astype(np.float16)\n    lag_min = rolled.min().reset_index().astype(np.float16)\n    lag_std = rolled.std().reset_index().astype(np.float16)\n    for col in cols:\n        weather_df[f'{col}_mean_lag{window}'] = lag_mean[col]\n        weather_df[f'{col}_max_lag{window}'] = lag_max[col]\n        weather_df[f'{col}_min_lag{window}'] = lag_min[col]\n        weather_df[f'{col}_std_lag{window}'] = lag_std[col]","7c70ae07":"add_lag_feature(weather_train_df, window=3)\nadd_lag_feature(weather_train_df, window=72)","3a71b692":"weather_train_df.head()","42d5bfed":"weather_train_df.columns","816a14d9":"year_map = building_meta_df.year_built.value_counts()\nbuilding_meta_df['year_cnt'] = building_meta_df.year_built.map(year_map)\n\nbid_map = train_df.building_id.value_counts()\ntrain_df['bid_cnt'] = train_df.building_id.map(bid_map)","08f639fb":"# categorize primary_use column to reduce memory on merge...\n\nprimary_use_list = building_meta_df['primary_use'].unique()\nprimary_use_dict = {key: value for value, key in enumerate(primary_use_list)} \nprint('primary_use_dict: ', primary_use_dict)\nbuilding_meta_df['primary_use'] = building_meta_df['primary_use'].map(primary_use_dict)\n\ngc.collect()","4f28a31a":"train_df = reduce_mem_usage(train_df, use_float16=True)\nbuilding_meta_df = reduce_mem_usage(building_meta_df, use_float16=True)\nweather_train_df = reduce_mem_usage(weather_train_df, use_float16=True)","43dd6cd5":"building_meta_df.head()","250d14c3":"from scipy.signal import savgol_filter as sg\n\ndef add_sg(df):\n    w = 11\n    p = 2\n    for si in df.site_id.unique():\n        index = df.site_id == si\n        df.loc[index, 'air_smooth'] = sg(df[index].air_temperature, w, p)\n        df.loc[index, 'dew_smooth'] = sg(df[index].dew_temperature, w, p)\n        \n        df.loc[index, 'air_diff'] = sg(df[index].air_temperature, w, p, 1)\n        df.loc[index, 'dew_diff'] = sg(df[index].dew_temperature, w, p, 1)\n        \n        df.loc[index, 'air_diff2'] = sg(df[index].air_temperature, w, p, 2)\n        df.loc[index, 'dew_diff2'] = sg(df[index].dew_temperature, w, p, 2)","275c008c":"add_sg(weather_train_df)","2cf3513c":"weather_train_df[weather_train_df.site_id==1].air_temperature[:100].plot()\nweather_train_df[weather_train_df.site_id==1].air_smooth[:100].plot()","ff9569c0":"weather_train_df[weather_train_df.site_id==2].dew_temperature[:100].plot()\nweather_train_df[weather_train_df.site_id==2].dew_smooth[:100].plot()","e76c5d0d":"weather_train_df[weather_train_df.site_id==0].dew_diff[:100].plot()\nweather_train_df[weather_train_df.site_id==0].air_diff[:100].plot()","e747159b":"category_cols = ['building_id', 'site_id', 'primary_use', 'IsHoliday']  # , 'meter'\nfeature_cols = ['square_feet', 'year_built'] + [\n    'hour', 'weekend',\n    'day', # 'month' ,\n#    'dayofweek',\n#    'building_median'\n    ] + [\n    'air_temperature', 'cloud_coverage',\n    'dew_temperature', 'precip_depth_1_hr',\n    'sea_level_pressure',\n#'wind_direction', 'wind_speed',\n    'air_temperature_mean_lag72',\n    'air_temperature_max_lag72', 'air_temperature_min_lag72',\n    'air_temperature_std_lag72', 'cloud_coverage_mean_lag72',\n    'dew_temperature_mean_lag72', 'precip_depth_1_hr_mean_lag72',\n    'sea_level_pressure_mean_lag72',\n#'wind_direction_mean_lag72',\n    'wind_speed_mean_lag72', \n    'air_temperature_mean_lag3',\n    'air_temperature_max_lag3',\n    'air_temperature_min_lag3', 'cloud_coverage_mean_lag3',\n    'dew_temperature_mean_lag3',\n    'precip_depth_1_hr_mean_lag3',\n    'sea_level_pressure_mean_lag3',\n#    'wind_direction_mean_lag3', 'wind_speed_mean_lag3',\n#    'floor_area',\n    'year_cnt', 'bid_cnt',\n    'dew_smooth', 'air_smooth',\n    'dew_diff', 'air_diff',\n    'dew_diff2', 'air_diff2',\n]","1fc1775c":"def create_X_y(train_df, target_meter):\n    target_train_df = train_df[train_df['meter'] == target_meter]\n    target_train_df = target_train_df.merge(building_meta_df, on='building_id', how='left')\n    target_train_df = target_train_df.merge(weather_train_df, on=['site_id', 'timestamp'], how='left')\n    \n    X_train = target_train_df[feature_cols + category_cols]\n    y_train = target_train_df['meter_reading_log1p'].values\n    \n    del target_train_df\n    return X_train, y_train","68c838b1":"def fit_lgbm(train, val, devices=(-1,), seed=None, cat_features=None, num_rounds=1500, lr=0.1, bf=0.1):\n    \"\"\"Train Light GBM model\"\"\"\n    X_train, y_train = train\n    X_valid, y_valid = val\n    metric = 'l2'\n    params = {'num_leaves': 31,\n              'objective': 'regression',\n#               'max_depth': -1,\n              'learning_rate': lr,\n              \"boosting\": \"gbdt\",\n              \"bagging_freq\": 5,\n              \"bagging_fraction\": bf,\n              \"feature_fraction\": 0.9,\n              \"metric\": metric,\n#               \"verbosity\": -1,\n#               'reg_alpha': 0.1,\n#               'reg_lambda': 0.3\n              }\n    device = devices[0]\n    if device == -1:\n        # use cpu\n        pass\n    else:\n        # use gpu\n        print(f'using gpu device_id {device}...')\n        params.update({'device': 'gpu', 'gpu_device_id': device})\n\n    params['seed'] = seed\n\n    early_stop = 20\n    verbose_eval = 20\n\n    d_train = lgb.Dataset(X_train, label=y_train, categorical_feature=cat_features)\n    d_valid = lgb.Dataset(X_valid, label=y_valid, categorical_feature=cat_features)\n    watchlist = [d_train, d_valid]\n\n    print('training LGB:')\n    model = lgb.train(params,\n                      train_set=d_train,\n                      num_boost_round=num_rounds,\n                      valid_sets=watchlist,\n                      verbose_eval=verbose_eval,\n                      early_stopping_rounds=early_stop)\n\n    # predictions\n    y_pred_valid = model.predict(X_valid, num_iteration=model.best_iteration)\n    \n    print('best_score', model.best_score)\n    log = {'train\/mae': model.best_score['training']['l2'],\n           'valid\/mae': model.best_score['valid_1']['l2']}\n    return model, y_pred_valid, log","b204b676":"from sklearn.model_selection import GroupKFold\n\nseed = 666\nshuffle = False\n#kf = KFold(n_splits=folds, shuffle=shuffle, random_state=seed)\nkf = GroupKFold(n_splits=folds)","a4ca9291":"def get_groups(df, meter):\n    if folds == 12:\n        return df[df.meter==meter].month -1\n    elif folds == 6:\n        return (df[df.meter==meter].month -1) \/\/ 2\n    elif folds == 3:\n        return (df[df.meter==meter].month -1) \/\/ 4","00c83533":"oof_total = 0\n\ntarget_meter = 0\nX_train, y_train = create_X_y(train_df, target_meter=target_meter)\ny_valid_pred_total = np.zeros(X_train.shape[0])\ngc.collect()\nprint('target_meter', target_meter, X_train.shape)\n\ncat_features = [X_train.columns.get_loc(cat_col) for cat_col in category_cols]\nprint('cat_features', cat_features)\n\nmodels0 = []\n\n#for train_idx, valid_idx in kf.split(X_train, y_train):\n\nfor train_idx, valid_idx in kf.split(X_train, y_train, groups=get_groups(train_df, target_meter)):    \n    train_data = X_train.iloc[train_idx,:], y_train[train_idx]\n    valid_data = X_train.iloc[valid_idx,:], y_train[valid_idx]\n    \n    mindex = train_df[train_df.meter==target_meter].iloc[valid_idx,:].month.unique()\n    print (mindex)\n\n    print('train', len(train_idx), 'valid', len(valid_idx))\n#     model, y_pred_valid, log = fit_cb(train_data, valid_data, cat_features=cat_features, devices=[0,])\n    model, y_pred_valid, log = fit_lgbm(train_data, valid_data, cat_features=category_cols,\n                                        num_rounds=num_rounds, lr=0.05, bf=0.7)\n    y_valid_pred_total[valid_idx] = y_pred_valid\n    models0.append([mindex, model])\n    gc.collect()\n    if debug:\n        break\n\noof0 = mean_squared_error(y_train, y_valid_pred_total)\n\nsns.distplot(y_train)\nsns.distplot(y_valid_pred_total)\n\noof_total += oof0 * len(y_train)\n\ndel X_train, y_train\ngc.collect()","2de821e5":"def plot_feature_importance(model):\n    importance_df = pd.DataFrame(model[1].feature_importance(),\n                                 index=feature_cols + category_cols,\n                                 columns=['importance']).sort_values('importance')\n    fig, ax = plt.subplots(figsize=(8, 8))\n    importance_df.plot.barh(ax=ax)\n    fig.show()","a3fdf49f":"target_meter = 1\nX_train, y_train = create_X_y(train_df, target_meter=target_meter)\ny_valid_pred_total = np.zeros(X_train.shape[0])\ngc.collect()\nprint('target_meter', target_meter, X_train.shape)\n\ncat_features = [X_train.columns.get_loc(cat_col) for cat_col in category_cols]\nprint('cat_features', cat_features)\n\nmodels1 = []\n    \nfor train_idx, valid_idx in kf.split(X_train, y_train, groups=get_groups(train_df, target_meter)):    \n#for train_idx, valid_idx in kf.split(X_train, y_train):\n    train_data = X_train.iloc[train_idx,:], y_train[train_idx]\n    valid_data = X_train.iloc[valid_idx,:], y_train[valid_idx]\n\n    mindex = train_df[train_df.meter==target_meter].iloc[valid_idx,:].month.unique()\n    \n    print('train', len(train_idx), 'valid', len(valid_idx))\n#     model, y_pred_valid, log = fit_cb(train_data, valid_data, cat_features=cat_features, devices=[0,])\n    model, y_pred_valid, log = fit_lgbm(train_data, valid_data, cat_features=category_cols, num_rounds=num_rounds,\n                                       lr=0.05, bf=0.5)\n    y_valid_pred_total[valid_idx] = y_pred_valid\n    models1.append([mindex, model])\n    gc.collect()\n    if debug:\n        break\n\noof1 = mean_squared_error(y_train, y_valid_pred_total)\n\nsns.distplot(y_train)\nsns.distplot(y_valid_pred_total)\n\noof_total += oof1 * len(y_train)\n\ndel X_train, y_train\ngc.collect()","5130767f":"target_meter = 2\nX_train, y_train = create_X_y(train_df, target_meter=target_meter)\ny_valid_pred_total = np.zeros(X_train.shape[0])\n\ngc.collect()\nprint('target_meter', target_meter, X_train.shape)\n\ncat_features = [X_train.columns.get_loc(cat_col) for cat_col in category_cols]\nprint('cat_features', cat_features)\n\nmodels2 = []\n\n\nfor train_idx, valid_idx in kf.split(X_train, y_train, groups=get_groups(train_df, target_meter)):\n#for train_idx, valid_idx in kf.split(X_train, y_train):\n    train_data = X_train.iloc[train_idx,:], y_train[train_idx]\n    valid_data = X_train.iloc[valid_idx,:], y_train[valid_idx]\n\n    mindex = train_df[train_df.meter==target_meter].iloc[valid_idx,:].month.unique()\n    print('train', len(train_idx), 'valid', len(valid_idx))\n#     model, y_pred_valid, log = fit_cb(train_data, valid_data, cat_features=cat_features, devices=[0,])\n    model, y_pred_valid, log = fit_lgbm(train_data, valid_data, cat_features=category_cols,\n                                        num_rounds=num_rounds, lr=0.05, bf=0.8)\n    y_valid_pred_total[valid_idx] = y_pred_valid\n    models2.append([mindex, model])\n    gc.collect()\n    if debug:\n        break\n\noof2 = mean_squared_error(y_train, y_valid_pred_total)\n\noof_total += oof2 * len(y_train)\n\nsns.distplot(y_train)\nsns.distplot(y_valid_pred_total)\n\ndel X_train, y_train\ngc.collect()","41c33ecd":"target_meter = 3\nX_train, y_train = create_X_y(train_df, target_meter=target_meter)\ny_valid_pred_total = np.zeros(X_train.shape[0])\n\ngc.collect()\nprint('target_meter', target_meter, X_train.shape)\n\ncat_features = [X_train.columns.get_loc(cat_col) for cat_col in category_cols]\nprint('cat_features', cat_features)\n\nmodels3 = []\n\nfor train_idx, valid_idx in kf.split(X_train, y_train, groups=get_groups(train_df, target_meter)):    \n#for train_idx, valid_idx in kf.split(X_train, y_train):\n    train_data = X_train.iloc[train_idx,:], y_train[train_idx]\n    valid_data = X_train.iloc[valid_idx,:], y_train[valid_idx]\n\n    mindex = train_df[train_df.meter==target_meter].iloc[valid_idx,:].month.unique()\n    print('train', len(train_idx), 'valid', len(valid_idx))\n#     model, y_pred_valid, log = fit_cb(train_data, valid_data, cat_features=cat_features, devices=[0,])\n    model, y_pred_valid, log = fit_lgbm(train_data, valid_data, cat_features=category_cols, num_rounds=num_rounds,\n                                       lr=0.03, bf=0.9)\n    y_valid_pred_total[valid_idx] = y_pred_valid\n    models3.append([mindex, model])\n    gc.collect()\n    if debug:\n        break\n\noof3 = mean_squared_error(y_train, y_valid_pred_total)\n\noof_total += oof3 * len(y_train)\n\noof_total = oof_total \/ len(train_df)\n\nsns.distplot(y_train)\nsns.distplot(y_valid_pred_total)\n\ndel X_train, y_train\ngc.collect()","deab89fa":"print('oof0=', np.sqrt(oof0))\nprint('oof1=', np.sqrt(oof1))\nprint('oof2=', np.sqrt(oof2))\nprint('oof3=', np.sqrt(oof3))\nprint('total=', np.sqrt(oof_total))","0ce11600":"del train_df, weather_train_df #, building_meta_df \ngc.collect()","e8f9df9a":"print('loading...')\ntest_df = pd.read_feather(root\/'test.feather')\nweather_test_df = pd.read_feather(root\/'weather_test.feather')\n\nweather_test_df = weather_test_df.drop_duplicates(['timestamp', 'site_id'])\nset_local(weather_test_df)\nadd_holiyday(weather_test_df)\n\nprint('preprocessing building...')\ntest_df['date'] = test_df['timestamp'].dt.date\npreprocess(test_df)\n#test_df['building_median'] = test_df['building_id'].map(building_median)\n\nprint('preprocessing weather...')\nweather_test_df = weather_test_df.groupby('site_id').apply(lambda group: group.interpolate(limit_direction='both'))\nweather_test_df.groupby('site_id').apply(lambda group: group.isna().sum())\n\nadd_sg(weather_test_df)\n\nadd_lag_feature(weather_test_df, window=3)\nadd_lag_feature(weather_test_df, window=72)\n\n#bid_map = train_df.building_id.value_counts()\ntest_df['bid_cnt'] = test_df.building_id.map(bid_map)\n\nprint('reduce mem usage...')\ntest_df = reduce_mem_usage(test_df, use_float16=True)\nweather_test_df = reduce_mem_usage(weather_test_df, use_float16=True)\n\ngc.collect()\nprint (test_df.shape)","5c028cfe":"sample_submission = pd.read_feather(os.path.join(root, 'sample_submission.feather'))\nreduce_mem_usage(sample_submission)\n\nprint(sample_submission.shape)","57fa2150":"def create_X(test_df, target_meter):\n    target_test_df = test_df[test_df['meter'] == target_meter]\n    target_test_df = target_test_df.merge(building_meta_df, on='building_id', how='left')\n    target_test_df = target_test_df.merge(weather_test_df, on=['site_id', 'timestamp'], how='left')\n    X_test = target_test_df[feature_cols + category_cols + ['month']]\n    return X_test","04a127c4":"def pred_all(X_test, models, batch_size=1000000):\n    iterations = (X_test.shape[0] + batch_size -1) \/\/ batch_size\n    print('iterations', iterations)\n\n    y_test_pred_total = np.zeros(X_test.shape[0])\n    for i, (mindex, model) in enumerate(models):\n        print(f'predicting {i}-th model')\n        for k in tqdm(range(iterations)):\n            y_pred_test = model.predict(X_test[k*batch_size:(k+1)*batch_size], num_iteration=model.best_iteration)\n            y_test_pred_total[k*batch_size:(k+1)*batch_size] += y_pred_test\n\n    y_test_pred_total \/= len(models)\n    return y_test_pred_total\n\n\ndef pred(X_test, models, batch_size=1000000):\n    if predmode == 'valid':\n        print ('valid pred')\n        return pred_valid(X_test, models, batch_size=1000000)\n    elif predmode == 'train':\n        print ('train pred')\n        return pred_train(X_test, models, batch_size=1000000)\n    else:\n        print ('all pred')\n        return pred_all(X_test, models, batch_size=1000000)","99014c83":"#predmode='all'","e5df6352":"%%time\nX_test = create_X(test_df, target_meter=0)\ngc.collect()\n\ny_test0 = pred(X_test, models0)\n\nsns.distplot(y_test0)\n\nprint(X_test.shape, y_test0.shape)\n\ndel X_test\ngc.collect()","2d2a3188":"%%time\nX_test = create_X(test_df, target_meter=1)\ngc.collect()\n\ny_test1 = pred(X_test, models1)\nsns.distplot(y_test1)\n\nprint(X_test.shape, y_test1.shape)\n\ndel X_test\ngc.collect()","d0ac6201":"%%time\nX_test = create_X(test_df, target_meter=2)\ngc.collect()\n\ny_test2 = pred(X_test, models2)\nsns.distplot(y_test2)\n\nprint(X_test.shape, y_test2.shape)\ndel X_test\n\ngc.collect()","218be14f":"X_test = create_X(test_df, target_meter=3)\ngc.collect()\n\ny_test3 = pred(X_test, models3)\n\nsns.distplot(y_test3)\nprint(X_test.shape, y_test3.shape)\ndel X_test\ngc.collect()","a6c13d8a":"print(sample_submission.loc[test_df['meter'] == 0, 'meter_reading'].shape,np.expm1(y_test0).shape)\nprint(sample_submission.loc[test_df['meter'] == 1, 'meter_reading'].shape,np.expm1(y_test1).shape)\nprint(sample_submission.loc[test_df['meter'] == 2, 'meter_reading'].shape,np.expm1(y_test2).shape)\nprint(sample_submission.loc[test_df['meter'] == 3, 'meter_reading'].shape,np.expm1(y_test3).shape)\n\nsample_submission.loc[test_df['meter'] == 0, 'meter_reading'] = np.expm1(y_test0)\nsample_submission.loc[test_df['meter'] == 1, 'meter_reading'] = np.expm1(y_test1)\nsample_submission.loc[test_df['meter'] == 2, 'meter_reading'] = np.expm1(y_test2)\nsample_submission.loc[test_df['meter'] == 3, 'meter_reading'] = np.expm1(y_test3)","86a49ee2":"# https:\/\/www.kaggle.com\/c\/ashrae-energy-prediction\/discussion\/119261#latest-684102\nsample_submission.loc[(test_df.building_id.isin(site_0_bids)) & (test_df.meter==0), 'meter_reading'] = sample_submission[(test_df.building_id.isin(site_0_bids)) & (test_df.meter==0)]['meter_reading'] * 3.4118","7b025ba6":"if rescale:   \n    sample_submission.loc[(test_df.building_id == 778) & (test_df.meter == 1), 'meter_reading'] = 100 * sample_submission.loc[(test_df.building_id == 778) & (test_df.meter == 1), 'meter_reading']\n    sample_submission.loc[(test_df.building_id == 1021) & (test_df.meter == 3), 'meter_reading'] = 1000 * sample_submission.loc[(test_df.building_id == 1021) & (test_df.meter == 3), 'meter_reading']\n    \n    plt.figure()\n    plt.subplot(211)\n    sample_submission.loc[(test_df.building_id == 778) & (test_df.meter == 1), 'meter_reading'].plot()\n    plt.subplot(212)    \n    sample_submission.loc[(test_df.building_id == 1021) & (test_df.meter == 3), 'meter_reading'].plot()","8ef560b8":"if clip0:\n    sample_submission.loc[sample_submission.meter_reading < 0, 'meter_reading'] = 0","c406b6cc":"if not debug:\n    sample_submission.to_csv('submission.csv', index=False, float_format='%.4f')","eb373125":"np.log1p(sample_submission['meter_reading']).hist(bins=100)","3bc4582c":"# i'm now using my leak data station kernel to shortcut.\nleak_df = pd.read_feather('..\/input\/ashrae-leak-data-station\/leak.feather')\n\nprint(leak_df.duplicated().sum())\nprint(leak_df.meter.value_counts())\n\nleak_df.fillna(0, inplace=True)\nleak_df = leak_df[(leak_df.timestamp.dt.year > 2016) & (leak_df.timestamp.dt.year < 2019)]\nleak_df.loc[leak_df.meter_reading < 0, 'meter_reading'] = 0 # remove large negative values\nleak_df = leak_df[leak_df.building_id!=245]\n\nsample_submission.loc[sample_submission.meter_reading < 0, 'meter_reading'] = 0\n\ntest_df['pred'] = sample_submission.meter_reading\n\nleak_df = leak_df.merge(test_df[['building_id', 'meter', 'timestamp', 'pred', 'row_id']], left_on = ['building_id', 'meter', 'timestamp'], right_on = ['building_id', 'meter', 'timestamp'], how = \"left\")\nleak_df = leak_df.merge(building_meta_df[['building_id', 'site_id']], on='building_id', how='left')","4c7c4629":"leak_df.site_id.unique()","9da5a1c3":"leak_df['pred_l1p'] = np.log1p(leak_df.pred)\nleak_df['meter_reading_l1p'] = np.log1p(leak_df.meter_reading)\n\nsns.distplot(leak_df.pred_l1p)\nsns.distplot(leak_df.meter_reading_l1p)\n\nleak_score = np.sqrt(mean_squared_error(leak_df.pred_l1p, leak_df.meter_reading_l1p))","4dfffd98":"leak_df = leak_df[['meter_reading', 'row_id']].set_index('row_id').dropna()\nsample_submission.loc[leak_df.index, 'meter_reading'] = leak_df['meter_reading']","237b8339":"sample_submission.head()","bcbc65c6":"if not debug:\n    sample_submission.to_csv('submission_ucf_replaced.csv', index=False, float_format='%.4f')","8d60ae83":"sample_submission.tail()","302a8b9b":"np.log1p(sample_submission['meter_reading']).hist(bins=100)","94dabe56":"print ('oof score meter0 =', np.sqrt(oof0))\nprint ('oof score meter1 =', np.sqrt(oof1))\nprint ('oof score meter2 =', np.sqrt(oof2))\nprint ('oof score meter3 =', np.sqrt(oof3))\nprint ('oof score total  =', np.sqrt(oof_total))","b577ad6a":"print('LV score=', leak_score)","1acc9cd5":"plot_feature_importance(models0[0])","049bad8f":"plot_feature_importance(models1[0])","122089f1":"plot_feature_importance(models2[0])","e942a652":"plot_feature_importance(models3[0])","c0d2be72":"# Train model\n\nTo win in kaggle competition, how to evaluate your model is important.\nWhat kind of cross validation strategy is suitable for this competition? This is time series data, so it is better to consider time-splitting.\n\nHowever this notebook is for simple tutorial, so I will proceed with KFold splitting without shuffling, so that at least near-term data is not included in validation.","479f909d":"# Fill Nan value in weather dataframe by interpolation\n\n\nweather data has a lot of NaNs!!\n\n![](http:\/\/)I tried to fill these values by **interpolating** data.","e9668238":"# Prediction on test data","c9c7f9b9":"Seems number of nan has reduced by `interpolate` but some property has never appear in specific `site_id`, and nan remains for these features.","3e44d44d":"# sort train. i dont know it is best","ac8fe405":"# Data preprocessing\n\nNow, Let's try building GBDT (Gradient Boost Decision Tree) model to predict `meter_reading_log1p`. I will try using LightGBM in this notebook.","bbe3b856":"# References\n\nThese kernels inspired me to write this kernel, thank you for sharing!\n\n - https:\/\/www.kaggle.com\/rishabhiitbhu\/ashrae-simple-eda\n - https:\/\/www.kaggle.com\/isaienkov\/simple-lightgbm\n - https:\/\/www.kaggle.com\/ryches\/simple-lgbm-solution","9f0bc49c":"# OOf MSE","d5703cc6":"## Removing weired data on site_id 0\n\nAs you can see above, this data looks weired until May 20. It is reported in [this discussion](https:\/\/www.kaggle.com\/c\/ashrae-energy-prediction\/discussion\/113054#656588) by @barnwellguy that **All electricity meter is 0 until May 20 for site_id == 0**. I will remove these data from training data.\n\nIt corresponds to `building_id <= 104`.","c07a40e7":"# remove buildings","cf020ac9":"# Train model by each meter type","bdc0b092":"# Add time feature","fc6e1bc4":"# Site-0 Correction","919de65a":"## lags\n\nAdding some lag feature","6d464ad3":"# replace LEAK data","a6b0814d":"Some features introduced in https:\/\/www.kaggle.com\/ryches\/simple-lgbm-solution by @ryches\n\nFeatures that are likely predictive:\n\n#### Weather\n\n- time of day\n- holiday\n- weekend\n- cloud_coverage + lags\n- dew_temperature + lags\n- precip_depth + lags\n- sea_level_pressure + lags\n- wind_direction + lags\n- wind_speed + lags\n\n#### Train\n\n- max, mean, min, std of the specific building historically\n\n\n\nHowever we should be careful of putting time feature, since we have only 1 year data in training,\nincluding `date` makes overfiting to training data.\n\nHow about `month`? It may be better to check performance by cross validation.\nI go not using this data in this kernel for robust modeling.","901b16fe":"# SG Filter for Weather","8f4619bc":"# count encoding","c9508616":"# Scores","ab6cbe6a":"# Leak Data loading and concat","5fa7877b":"# Fast data loading\n\nThis kernel uses the preprocessed data from my previous kernel, [\nASHRAE: feather format for fast loading](https:\/\/www.kaggle.com\/corochann\/ashrae-feather-format-for-fast-loading), to accelerate data loading!","1bcdf7e3":"# Threshold By Black day","ce940fd5":"# site-0 correction ","c462b3be":"# Site specific holiday","7460f0d8":"# Delete Outliear"}}