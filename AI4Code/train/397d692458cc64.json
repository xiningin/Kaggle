{"cell_type":{"85167dcb":"code","33902849":"code","592db5ee":"code","0dc713c8":"code","1b49857e":"code","4dc914ef":"code","1c11a027":"code","eda81be3":"code","2c6b5abc":"code","dba53265":"code","887f827a":"code","db1995e4":"code","76bcef9d":"code","24ca23fc":"code","5ed433cf":"code","78ce64e2":"code","bc8311e9":"code","f04f6cee":"code","38072f97":"code","4a73181e":"code","9dc967f6":"code","740830e6":"code","09cade2e":"code","c340818b":"code","f83cf69d":"code","5ec1c027":"code","7f8ef284":"code","fd6a406f":"markdown","b9179def":"markdown","fac09d3b":"markdown","4c0a0030":"markdown","51461ec3":"markdown","a32efd81":"markdown","1f93e45d":"markdown","ce28c801":"markdown","f6a5e1d5":"markdown"},"source":{"85167dcb":"import pandas as pd\nimport numpy as np\nfrom imblearn import *\nimport pyspark\nfrom pyspark.ml.feature import ChiSqSelector\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.sql.types import * \nfrom pyspark.sql.functions import *\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import MinMaxScaler\nfrom pyspark.ml.classification import *\nfrom pyspark.ml.evaluation import *\nfrom pyspark.sql.functions import *\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.sql.functions import when, log, col\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt","33902849":"spark = SparkSession.builder.appName(\"fraud_detection\").getOrCreate()","592db5ee":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\ntrain = pd.read_csv('####\/datasets\/delloite_hackathon\/train.csv')\ntest = pd.read_csv('####\/datasets\/delloite_hackathon\/test.csv')","0dc713c8":"train.info()","1b49857e":"category_data = train.select_dtypes(include=['object']).copy()","4dc914ef":"category_data.head()","1c11a027":"sns.color_palette(\"rocket\", as_cmap=True)\nfor col in category_data:\n  sns.barplot(category_data[str(col)].value_counts().index, category_data[str(col)].value_counts().values)\n  plt.title('Frequency Distribution of' + str(col))\n  plt.ylabel('Number of Occurrences', fontsize=12)\n  plt.xlabel(str(col), fontsize=12)\n  plt.show()\n  #print(category_data[str(col)].value_counts())","eda81be3":"columns = ['Payment Plan', 'Loan Title', 'Application Type']\ntrain.drop(columns, axis = 1, inplace = True)\ntest.drop(columns, axis = 1, inplace = True)","2c6b5abc":"# convert category labels\nfor col in train.columns:\n  if (train[col].dtype == 'object'):\n    labels = train[col].astype('category').cat.categories.tolist()\n    replace_map_comp = {col : {k: v for k,v in zip(labels,list(range(1,len(labels)+1)))}}\n    train.replace(replace_map_comp, inplace=True)\n    test.replace(replace_map_comp, inplace=True)\n  elif (train[col].dtype == 'float64'):\n    train[col] = train[col].astype(np.int64)\n    test[col] = test[col].astype(np.int64)\ntest = test.iloc[:,:-1]","dba53265":"train.head()","887f827a":"test.head()","db1995e4":"train.info()","76bcef9d":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nbestk = SelectKBest(score_func=chi2, k=15)\nfit = bestk.fit(train.iloc[:,1:-1], train['Loan Status'])\nfeatures = fit.transform(train.iloc[:,1:-1])","24ca23fc":"bk = bestk.get_support().tolist()\nselected_features = [col for idx, col in enumerate(train.iloc[:,1:-1].columns) if bk[idx] == True]\nselected_features","5ed433cf":"train = train[selected_features+['Loan Status']]\ntest = test[selected_features]\n\ntrain.to_csv('training_data.csv')\ntest.to_csv('test_data.csv')","78ce64e2":"training_data = spark.read.csv('training_data.csv',inferSchema=True,header=True)\ntest_data = spark.read.csv('test_data.csv',inferSchema=True,header=True)","bc8311e9":"training_data.groupBy(\"Loan Status\").count().show(100)","f04f6cee":"input_columns = training_data.columns # Collect the column names as a list\ninput_columns = input_columns[1:-1] # keep only relevant columns: from column 1 to \ndependent_var = 'Loan Status'\n\nrenamed = training_data.withColumn(\"label_str\", training_data[dependent_var].cast(StringType())) #Rename and change to string type\nindexer = StringIndexer(inputCol=\"label_str\", outputCol=\"label\") #Pyspark is expecting the this naming convention \nindexed = indexer.fit(renamed).transform(renamed)","38072f97":"numeric_inputs = []\nstring_inputs = []\nfor column in input_columns:\n    if str(indexed.schema[column].dataType) == 'StringType':\n        indexer = StringIndexer(inputCol=column, outputCol=column+\"_num\") \n        indexed = indexer.fit(indexed).transform(indexed)\n        new_col_name = column+\"_num\"\n        # Add the new column name to the string inputs list\n        string_inputs.append(new_col_name)\n    else:\n        # If no change was needed, take no action \n        numeric_inputs.append(column)","4a73181e":"# Now create your final features list\nfeatures_list = numeric_inputs + string_inputs\n# Create your vector assembler object\nassembler = VectorAssembler(inputCols=features_list,outputCol='features')\n# And call on the vector assembler to transform your dataframe\noutput = assembler.transform(indexed).select('features','label')","9dc967f6":"# Create the mix max scaler object \n# This is what will correct for negative values\nscaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",min=0,max=1)\nprint(\"Features scaled to range: [%f, %f]\" % (scaler.getMin(), scaler.getMax()))\n\nscalerModel = scaler.fit(output)\nscaled_data = scalerModel.transform(output)\nfinal_data = scaled_data.select('scaledFeatures', 'label')\n\ntrain_data = final_data.withColumnRenamed(\"scaledFeatures\",\"features\")\ntrain_data.show()","740830e6":"from keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras import optimizers, regularizers","09cade2e":"input_dim = len(train_data.select('features').first()[0])","c340818b":"train_data['features']","f83cf69d":"model = Sequential()\nmodel.add(Dense(256, input_shape=(input_dim,), activity_regularizer=regularizers.l2(0.01)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(256, activity_regularizer=regularizers.l2(0.01)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(2))\nmodel.add(Activation('sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam')","5ec1c027":"from pyspark.ml.functions import vector_to_array\nx = train_data['features']#train_data.withColumn('features', vector_to_array('features'))\ny = train_data['label']#train_data.withColumn('label', vector_to_array('label'))","7f8ef284":"model.fit(x, y, epochs=10, batch_size=128, verbose=2, validation_split=0.3)","fd6a406f":"#### Feature Selection","b9179def":"### Data Prep","fac09d3b":"#### Format Data","4c0a0030":"### Train Model","51461ec3":"#### Categorical Data Frquency Distribution","a32efd81":"### EDA","1f93e45d":"#### How many classes do we have?","ce28c801":"### Persist Converted Dataframes","f6a5e1d5":"#### Map Object Data"}}