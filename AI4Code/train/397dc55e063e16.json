{"cell_type":{"b4933200":"code","dddd93b0":"code","1119cf3c":"code","296cdc67":"code","4d2a45b5":"code","9774d578":"code","cb0601a6":"code","a000cb55":"code","4561fd6e":"code","ff63b75e":"code","0b918468":"code","dfd8ec57":"code","8a20ced8":"code","16c92f54":"code","68dc08cd":"code","e5ec9627":"code","d8b36fa7":"code","e4935d22":"code","3583bc8c":"code","7d6c1d1a":"code","819552a4":"code","bed2b21d":"code","16c9ca31":"code","e8605b8d":"code","31d32fed":"code","f511efc3":"code","8333223e":"code","442d29dc":"code","59fd2b83":"code","72cf0f65":"code","f446fb5e":"code","0f959315":"code","594edd29":"markdown","212cb0c1":"markdown","10210a4f":"markdown","c77b0d12":"markdown","a0180eab":"markdown","5e4219dc":"markdown","4854fe4b":"markdown","21712a9f":"markdown","7bfe56a2":"markdown","4eb6799e":"markdown","c159e405":"markdown","c02c3837":"markdown","76b0fe4e":"markdown","838d989e":"markdown","348ed1e2":"markdown","54f60da1":"markdown","89540b6f":"markdown","585e65d3":"markdown","b72f22d4":"markdown","e57c1b22":"markdown","4eedf287":"markdown","3179b940":"markdown"},"source":{"b4933200":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nimport math","dddd93b0":"# Hide Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning) ","1119cf3c":"# MovieLens Dataset\nratings = pd.read_csv('..\/input\/movielens\/ml-latest-small\/ratings.csv')\nmovies = pd.read_csv('..\/input\/movielens\/ml-latest-small\/movies.csv')","296cdc67":"ratings.head()","4d2a45b5":"ratings.describe()","9774d578":"ratings['rating'].unique()","cb0601a6":"movies.head()","a000cb55":"movies.describe()","4561fd6e":"def get_mse(pred, actual):\n    pred = pred[actual.nonzero()].flatten()\n    actual = actual[actual.nonzero()].flatten()\n    return mean_squared_error(pred, actual)","ff63b75e":"class MatrixFactorization():\n    def __init__(self, \n                 ratings,\n                 n_latent_factors=50,\n                 reg=0.0):\n        \n        self.ratings = ratings\n        self.n_users, self.n_items = ratings.shape\n        self.n_latent_factors = n_latent_factors\n        self.reg = reg\n        \n        self.sample_row, self.sample_col = self.ratings.nonzero()\n        self.n_samples = len(self.sample_row)\n        \n\n    def train(self, n_iter=10, learning_rate=0.1):\n        self.learning_rate = learning_rate\n        self.user_bias = np.zeros(self.n_users)\n        self.item_bias = np.zeros(self.n_items)\n\n        # Global bias\n        self.global_bias = np.mean(self.ratings[np.where(self.ratings != 0)])\n        #print(\"Global bias: \", self.global_bias, \"\\n\")\n\n        # Set user bias\n        for u in range(0, self.n_users):\n            target_user = self.ratings[u, :]\n            #print(\"Target user: \", target_user)\n            pid = np.nonzero(target_user)\n            s = np.sum(target_user[pid])\n            a = s \/ np.asarray(pid).size\n            #print(\"Average Ratring: \", a)\n            u = self.global_bias - a\n            #print(\"User bias: \", u, \"\\n\")\n\n        # Set item bias\n        for i in range(0, self.n_items):\n            target_item = self.ratings[:, i]\n            #print(\"Target item: \", target_item)\n            pid = np.nonzero(target_item)\n            s = np.sum(target_item[pid])\n            a = s \/ np.asarray(pid).size\n            #print(\"Average Rating: \", a)\n            u = self.global_bias - a\n            #print(\"Item bias: \", u, \"\\n\")    \n\n        # Initialize Latent Factor Matrices for users and items        \n        self.user_factors = np.random.normal(scale=1.\/self.n_latent_factors, size=(self.n_users, self.n_latent_factors))\n        self.item_factors = np.random.normal(scale=1.\/self.n_latent_factors, size=(self.n_items, self.n_latent_factors))\n    \n        counter = 1\n        while counter <= n_iter:\n            self.training_indices = np.arange(self.n_samples)\n            np.random.shuffle(self.training_indices)\n\n            #cost = []\n            \n            # Stochastic Gradienrt Descent (SGD) \n            for index in self.training_indices:\n                u = self.sample_row[index]\n                i = self.sample_col[index]\n\n                # Prediction\n                bias = self.global_bias + self.user_bias[u] + self.item_bias[i]\n                prediction = self.user_factors[u, :].dot(self.item_factors[i, :].T) + bias\n\n                #print(\"Iteration \", counter, \"\\t[u,i] = \", u, \",\", i, \"\\t:\\tPrediction -> \", prediction, \"\\tbias ->\", bias)\n\n                # Error\n                e = (self.ratings[u,i] - prediction)\n                    \n                #cost.append(abs(e))    \n                #print(\" Error: \", e, \"\\n\\n\")\n                \n                # Update biases\n                self.user_bias[u] += self.learning_rate * (e - self.reg * self.user_bias[u])\n                self.item_bias[i] += self.learning_rate * (e - self.reg * self.item_bias[i])\n                \n                # Update latent factors\n                self.user_factors[u, :] += self.learning_rate * (e * self.item_factors[i, :] - self.reg * self.user_factors[u,:])\n                self.item_factors[i, :] += self.learning_rate * (e * self.user_factors[u, :] - self.reg * self.item_factors[i,:])\n\n           #print(\"Iteration => \", counter, \"\\tCost: \" ,sum(cost) \/ len(cost), \"\\n\")\n\n            counter += 1\n            \n\n    def predict(self):\n        predictions = np.zeros((self.user_factors.shape[0], self.item_factors.shape[0]))\n        \n        for u in range(self.user_factors.shape[0]):\n            for i in range(self.item_factors.shape[0]):\n                bias = self.global_bias + self.user_bias[u] + self.item_bias[i]\n                predictions[u, i] = self.user_factors[u, :].dot(self.item_factors[i, :].T) + bias\n                if np.any(np.isnan(predictions[u, i])):\n                    predictions[u, i] = 0\n        return predictions\n\n\n    def get_user_factors(self):\n        return self.user_factors\n\n\n    def get_item_factors(self):\n        return self.item_factors\n\n\n    def get_bias(self):    \n        return self.global_bias, self.user_bias, self.item_bias\n    \n    \n    def learning_curve(self, iter_arr, test_data, learning_rate=0.1):\n        iter_arr.sort()\n        self.train_mse =[]\n        self.test_mse = []\n        iter_diff = 0\n        \n        for (i, n_iter) in enumerate(iter_arr):\n            print('\\tTraining for ', n_iter, ' iterations')\n            if i == 0:\n                self.train(n_iter - iter_diff, learning_rate)\n            else:\n                self.train(n_iter - iter_diff)\n\n            predictions = self.predict()\n\n            self.train_mse += [get_mse(predictions, self.ratings)]\n            self.test_mse += [get_mse(predictions, test_data)]\n            \n            print ('\\tTrain MSE: \\t', str(self.train_mse[-1]))\n            print ('\\tTest MSE: \\t',str(self.test_mse[-1]), '\\n')\n            iter_diff = n_iter","0b918468":"ratings = ratings.sort_values(\"movieId\")\nn_list = []\n\ni = 0\np_id = 0\nm_id = 0 \nfor m in ratings.movieId:\n    if m == m_id:\n        n_list.append(p_id)\n    else:\n        m_id = m\n        p_id = p_id + 1\n        n_list.append(p_id) \n    i = i + 1\n\nratings['partial_movieId'] = n_list\nratings.head(300)    ","dfd8ec57":"ratings.describe()","8a20ced8":"# Training set ( 70% )\ntrain_data = ratings.sample(frac = 0.7)\n\n# Testing set ( 30%  )\ntest_data = ratings.drop(train_data.index)","16c92f54":"train_data.head()","68dc08cd":"train_data.describe()","e5ec9627":"test_data.describe()","d8b36fa7":"ratings = ratings.sort_values(\"userId\")","e4935d22":"ratings_matrix = ratings.pivot_table(index=\"userId\",columns=\"partial_movieId\",values=\"rating\") \nratings_matrix","3583bc8c":"ratings_arr = np.zeros((ratings.userId.max(), ratings.partial_movieId.max()))\ntrain_ratings_arr = np.zeros((ratings.userId.max(), ratings.partial_movieId.max()))\ntest_ratings_arr = np.zeros((ratings.userId.max(), ratings.partial_movieId.max()))\n\n# Training Set\nfor row in train_data.itertuples():\n    train_ratings_arr[row[1]-1, row[5]-1] = row[3]\n\n# Testing Set\nfor row in test_data.itertuples():\n    test_ratings_arr[row[1]-1, row[5]-1] = row[3]\n\n# Overall Data    \nfor row in ratings.itertuples():\n    ratings_arr[row[1]-1, row[5]-1] = row[3]\n    \nratings_arr","7d6c1d1a":"def plot_learning_curve(iter_array, model):\n    plt.plot(iter_array, model.train_mse, label='Training', linewidth=3)\n    plt.plot(iter_array, model.test_mse, label='Testing', linewidth=3)\n    \n    plt.title('Learning Curve', fontsize=30)\n    plt.xticks(fontsize=16)\n    plt.yticks(fontsize=16)\n    plt.xlabel('Iterations', fontsize=20)\n    plt.ylabel('MSE', fontsize=20)\n    plt.legend(loc='best', fontsize=20)","819552a4":"def get_recommendation(results_arr, user_id, n_top_movies=10):\n    \"\"\"\n    Returns a list of n movie ids (not rated yet) to be recommended to the user with given user_id\n    \"\"\"\n    # List of movies already rated by the target user\n    movies_already_rated = ratings[ratings['userId'] == user_id]['partial_movieId'].values.tolist()\n    n_rated = len(movies_already_rated) \n    \n    partial_movie_ids_to_be_recommended = []\n    actual_movie_ids_to_be_recommended = []\n\n    rowIndex = user_id - 1\n    \n    # Sort the target row\n    # argsort - returns the original indexes of the sorted array\n    arr_index = np.argsort(results_arr[rowIndex])\n        \n    if n_top_movies > 0:\n        n = n_top_movies\n    else:\n        n = 10\n        \n    for i in range(1, n + 1 + n_rated):\n        if (len(partial_movie_ids_to_be_recommended) < n):\n            if arr_index[-i]+1 in movies_already_rated:\n                # Skip if the movie has been already rated (watched) by the target user\n                continue\n            else:\n                partial_movie_ids_to_be_recommended.append(arr_index[-i]+1)\n    \n    for m in partial_movie_ids_to_be_recommended:\n        target_df = ratings.loc[ratings['partial_movieId'] == m]\n        actual = target_df['movieId'].iloc[0]\n        actual_movie_ids_to_be_recommended.append(actual)\n        \n    return actual_movie_ids_to_be_recommended","bed2b21d":"def get_title(movie_id):\n    \"\"\"\n    Returns the title of the movie\n    \"\"\"\n    target_df = movies.loc[movies['movieId'] == movie_id]\n    return target_df['title'].iloc[0]","16c9ca31":"iter_array = [10, 25, 50, 80, 100]\nlatent_factors = [20, 40, 60]\nregularizations = [0.01, 0.1, 0.25]\nregularizations.sort()\n\noptimal_params = {}\noptimal_params['num_factors'] = latent_factors[0]\noptimal_params['reg'] = regularizations[0]\noptimal_params['num_iter'] = 0\noptimal_params['train_mse'] = np.inf\noptimal_params['test_mse'] = np.inf\n\nfor fact in latent_factors:\n    for reg in regularizations:\n        print('===================================================')\n        print ('Factors: {}'.format(fact))\n        print ('Regularization: {}'.format(reg))\n        print('===================================================')\n        model = MatrixFactorization(train_ratings_arr, n_latent_factors=fact, reg=reg)\n        model.learning_curve(iter_array, test_data=test_ratings_arr, learning_rate=0.001)\n        \n        min_idx = np.argmin(model.test_mse)\n        if model.test_mse[min_idx] < optimal_params['test_mse']:\n            optimal_params['num_factors'] = fact\n            optimal_params['reg'] = reg\n            optimal_params['num_iter'] = iter_array[min_idx]\n            optimal_params['train_mse'] = model.train_mse[min_idx]\n            optimal_params['test_mse'] = model.test_mse[min_idx]\n            print('***************************************************')\n            print('New optimal hyperparameters')\n            print(pd.Series(optimal_params))\n            print('***************************************************')\n            print('\\n')\n","e8605b8d":"print ('Optimal regularization: {}'.format(optimal_params['reg']))\nprint ('Optimal latent factors: {}'.format(optimal_params['num_factors']))\nprint ('Optimal iterations: {}'.format(optimal_params['num_iter']))","31d32fed":"plot_learning_curve(iter_array, model)","f511efc3":"model = MatrixFactorization(ratings_arr, n_latent_factors=optimal_params['num_factors'], reg=optimal_params['reg'])\n\nmodel.train(n_iter=optimal_params['num_iter'], learning_rate=0.001)","8333223e":"result = model.predict()\nresult","442d29dc":"## Check for user with user_id 1\n\nrowIndex = 0\n# Sort the target row\nsortedArr = result[ :, result[rowIndex].argsort()][rowIndex]\narr_index = np.argsort(result[rowIndex])\n\nn_recomm = 20;\nprint('Top 10 recommendations for user with user_id 1\\n')\nfor i in range(1, n_recomm + 1):\n    print('rating: ', sortedArr[-i], '\\tpartial_movieId: ', arr_index[-i]+1)","59fd2b83":"movie_for_user_id_1 = get_recommendation(results_arr=result, user_id=1, n_top_movies=15)\n\nfor m in movie_for_user_id_1:\n    print(get_title(m)) ","72cf0f65":"movie_for_user_id_15 = get_recommendation(results_arr=result, user_id=15)\n\nfor m in movie_for_user_id_15:\n    print(get_title(m)) ","f446fb5e":"movie_for_user_id_100 = get_recommendation(results_arr=result, user_id=100, n_top_movies=5)\n\nfor m in movie_for_user_id_100:\n    print(get_title(m)) ","0f959315":"# Save the results as a CSV file\nnp.savetxt(\"results.csv\", result, delimiter=\",\")","594edd29":"# Read Data","212cb0c1":"### Convert the Dataframe into Numpy Array","10210a4f":"# Imports","c77b0d12":"### Top 15 movies to be recommended for user with user_id 1","a0180eab":"# Split the data into Training and Testing Set","5e4219dc":"# Matrix Factorization Model","4854fe4b":"### Optimal Hyperparametres (among our selections)","21712a9f":"# Data Preprocessing","7bfe56a2":"### Method to plot the learning curve","4eb6799e":"### Method to get the title of movie","c159e405":"### Add paritial_movieId columns","c02c3837":"### Helper method to get the MSE","76b0fe4e":"# Recommendation System\n\n## Using Model Based Collaborative Filtering\n## Matrix Factorization\n\nConsider a ratings matrix R where the entries $r_{ij}$ are ratings of user u for item i. \nThe matrix R is decomposed into two matrices X and Y such that:\n\\\n$$ R \u2248 X.Y^T $$\n\\\nHere, X is the user-feature matrix of order $k*u$ and Y is the item-feature matrix of order $k*i$. k is the number of latent (hidden) factors.\n\\\nThen our predicted rating of user u for item i is: \n\\\n$$ \\hat r_{ui} = x_u . y_i^T $$\n\\\nIf $b_{ui}$ be the bias involved in rating $r_{ui}$ such that:\n\\\n$$ b_{ui} = \\mu + b_u + b_i $$\nHere, \n$\\mu$ is the overall average rating,\n\\\n$b_i$ indicate the observed deviations of item i from the average and\n\\\n$b_u$ indicate the observed deviations of user u from the average.\n\\\n\\\nSo our predicted rating becomes:\n$$ r_{ui} = x_u . y_i^T + b_{ui} $$\n\\\n$$ r_{ui}= x_u . y_i^T + (\\mu + b_u + b_i) $$\n\\\nOur loss function is given as:\n$$ L = \\sum_{ui} (r_{ui}\u2212(\\mu + b_i + b_u + x_u . y_i^T))^2 $$\n\nAdding the regularization term, our loss function becomes:\n\\\n$$ L = \\sum_{ui} (r_{ui}\u2212(\\mu + b_i + b_u + x_u . y_i^T))^2 + \\lambda (\\sum_u ||b_u||^2 + \\sum_i ||b_i||^2 + \\sum_u ||x_u||^2 + \\sum_i ||y_i||^2) $$\nHere, $\\lambda$ is the regularization constant.\n\\\n\\\nIf $\\eta$ be our learning rate, then the update for user bias is given by:\n$$ b_u \\leftarrow b_u - \\eta \\frac {\\partial L}{\\partial b_u} $$\n\\\nTaking the derivative of L with respect to $b_u$, we get:\n$$ \\frac {\\partial L}{\\partial d_u} = 2(r_{ui} - (x_u.y_i^T + \\mu + b_i + b_u ))(-1) + 2 \\lambda b_u $$\n\\\n$$ \\frac {\\partial L}{\\partial d_u} = 2(e_{ui})(-1) + 2 \\lambda b_u $$\n\\\n$e_{ui}$ denotes the error in our prediction. We can drop the factor of 2, assuming it gets rolled up in the learning rate. Then,\n\n$$ \\frac {\\partial L}{\\partial d_u} = -(e_{ui}) + \\lambda b_u $$\n\\\nFor all of our features, the updates end up being:\n\n$$ b_u \\leftarrow b_u - \\eta (e_{ui} - \\lambda b_u) $$\n\\\n$$ b_i \\leftarrow b_i - \\eta (e_{ui} - \\lambda b_i) $$\n\\\n$$ x_u \\leftarrow x_u - \\eta (e_{ui} y_i - \\lambda x_u) $$\n\\\n$$ y_i \\leftarrow y_i - \\eta (e_{ui} x_u - \\lambda y_i) $$","838d989e":"# Save the Results","348ed1e2":"# Optimize Hyperparametres","54f60da1":"# Predictions","89540b6f":"# Utility Functions","585e65d3":"### Top 10 movies to be recommended for user with user_id 35","b72f22d4":"# Train the Matrix Factorization model","e57c1b22":"### Top 5 movies to be recommended for user with user_id 100","4eedf287":"### Method to get n recommendations for a target user","3179b940":"# Learning Curve"}}