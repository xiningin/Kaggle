{"cell_type":{"cdd4ddde":"code","34dedaf6":"code","89e8c2f3":"code","921175bc":"code","065a62c4":"code","9e6f7278":"code","c53fe36d":"code","15c962b9":"markdown"},"source":{"cdd4ddde":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","34dedaf6":"def univ_scatter(df, features, yname, n=4, writefolder=None):\n\n  for feature in features:\n\n    # tirando os nans\n    df_temp = df[~np.isnan(df[feature])][[feature, yname]]\n\n    bins_pos = np.percentile(df_temp[feature].values, np.linspace(0,100,n+1))\n    v_mean = list()\n    v_std = list()\n    \n    if bins_pos.size == np.unique(bins_pos).size: # variavel continua\n      hist, _ = np.histogram(df_temp[feature], bins_pos)\n      xtickslabel = list()\n      bin_pos_label = list()\n      for i in range(bins_pos.size-1): # vou pegar cada intervalo agora e calcular a media de y\n        v = df_temp[(df_temp[feature].values >= bins_pos[i]) & (df_temp[feature].values < bins_pos[i+1])][yname].values\n        if np.isnan(v.mean()) or np.isnan(v.std()) or abs(v.mean())==float('inf') or abs(v.std())==float('inf'):\n          continue\n        else:\n          xtickslabel.append('['+str('%.3f'%bins_pos[i])+'-'+str('%.3f'%bins_pos[i+1])+'[')\n          v_mean.append(v.mean())\n          v_std.append(v.std())\n          bin_pos_label.append((bins_pos[i]+bins_pos[i+1])\/2)\n\n      v_mean = np.array(v_mean)\n      v_std = np.array(v_std)\/np.sqrt(hist)\n      \n      fig, ax1 = plt.subplots()\n      ax1.set_xlabel(feature)\n      ax1.set_ylabel('mean ' + yname)\n      ax1.set_ylim([0, (v_mean+v_std).max()*1.05])\n      ax1.set_xticks(bin_pos_label)\n      #ax1.plot(bins_pos[:-1], v_mean, label='mean '+yname)\n      ax1.plot(bin_pos_label, v_mean, 'o-', label='mean '+yname)\n      ax1.set_xticklabels(xtickslabel, rotation=35)\n      #ax1.fill_between(bins_pos[:-1], v_mean + v_std, v_mean - v_std, alpha=0.1, color='b')\n      ax1.fill_between(bin_pos_label, v_mean + v_std, v_mean - v_std, alpha=0.1, color='b')\n      \n      color = 'tab:red'\n      ax2 = ax1.twinx()\n      #ax2.plot(bins_pos[:-1], hist, 'o-', label='bin count', color=color)\n      ax2.plot(bin_pos_label, hist, 'o--', label='bin count', color=color)\n      ax2.set_ylim([0, hist.max()*1.2])\n      ax2.set_ylabel('bin_count', color=color)\n      \n      if writefolder:\n        feature_ = feature.replace(' ', '_')\n        feature_ = feature_.replace('\/', '_')\n        plt.savefig(writefolder+'\/scatter_'+feature_+'.png')\n      else:      \n        plt.tight_layout()\n        plt.show()\n    else: # variavel categorica\n      bins_pos = np.unique(bins_pos)\n      hist = list()\n      for value in bins_pos:\n        hist.append((df_temp[feature].values==value).sum())\n      #hist, _ = np.histogram(df[feature], bins_pos)\n      for i in range(bins_pos.size): # vou pegar cada intervalo agora e calcular a media de y\n        v = df_temp[df_temp[feature].values == bins_pos[i]][yname].values\n        v_mean.append(v.mean())\n        v_std.append(v.std())\n\n      v_mean = np.array(v_mean)\n      v_std = np.array(v_std)\/np.sqrt(hist)\n      \n      fig, ax1 = plt.subplots()\n      ax1.set_xlabel(feature)\n      ax1.set_ylabel('mean '+yname)\n      ax1.set_ylim([0,(v_mean+v_std).max()*1.05])\n      ax1.set_xticks(bins_pos)\n      ax1.plot(bins_pos, v_mean, 'o-', label='mean '+yname)\n      ax1.fill_between(bins_pos, v_mean + v_std, v_mean - v_std, alpha=0.1, color='b')\n      \n      color = 'tab:red'\n      ax2 = ax1.twinx()\n      ax2.plot(bins_pos, hist, 'o--', label='bin count', color=color)\n      ax2.set_ylim([0, np.array(hist).max()*1.2])\n      ax2.set_ylabel('bin_count', color=color)\n      \n      if writefolder:\n        plt.savefig(writefolder+'\/scatter_'+feature+'.png')\n      else:      \n        plt.show()\n        \n","89e8c2f3":"input_file = '\/kaggle\/input\/transformed-covid19-dataset\/dataset.xlsx'\ndf = pd.read_excel(input_file)\n\n# column to model\ny_name = 'SARS-Cov-2 exam result'\n\nprint (df.columns)\n","921175bc":"features_to_remove = [\n    'Patient ID',\n    'Patient addmited to regular ward (1=yes. 0=no)',\n    'Patient addmited to semi-intensive unit (1=yes. 0=no)',\n    'Patient addmited to intensive care unit (1=yes. 0=no)',\n    'Urine - Aspect', # some categorical features which I dont want to deal right now\n    'Urine - Leukocytes',\n    'Urine - Crystals',\n    'Urine - Color',\n    'Relationship (Patient\/Normal)',\n    'Urine - Red blood cells', # this variable is .9 or 1 correlated to another (there is some 'inf' here)\n    'Vitamin B12', # needs proper cleaning\n    'Base excess (arterial blood gas analysis)', # needs proper cleaning\n    'Arteiral Fio2', # needs proper cleaning\n]\n\n\nlst_features = list(df.columns)\nlst_features.remove(y_name)\n\nfor feature in features_to_remove:\n    print ({feature})\n    lst_features.remove(feature)\n","065a62c4":"# Let's se some insightful figures\n\nfeature = 'Rhinovirus_Enterovirus' #good\n# This feature have a good amount of records in 0 and 1 cases and there is a clear correlation, records with\n# 'Rhinovirus_Enterovirus'==0 have an average of ~.10 frequency of covid and patients with 'Rhinovirus_Enterovirus'==1\n# have an average of 0.02.\nuniv_scatter(df, [feature], y_name, n=4, writefolder=None)\n\nfeature = 'Potassium' #bad\n# This is a bad feature, there is little to none information. The difference in the y-axis is little and statistical the same\nuniv_scatter(df, [feature], y_name, n=4, writefolder=None)\n\nfeature = 'Platelets' # good\n# Good stuff here\nuniv_scatter(df, [feature], y_name, n=4, writefolder=None)\n\nfeature = 'Parainfluenza 4' #bad\n# bad stuff here :\/, there is almost none records with 'Parainfluenza 4'==1, we cannot trust that -\n# - ONE EYE IN THE DATA AND THE ANOTHER ONE IN THE BIN COUNT :) \n\nuniv_scatter(df, [feature], y_name, n=4, writefolder=None)\n","9e6f7278":"# Now we are gonna see all the figures, take your time to decide what are the features you see relevant\n# Usually this step is done with a specialized professional, machine learning is not for the hacker geeks only folks! ;)\n# I'll put my rushed selection later, you can grab there\n\nuniv_scatter(df, lst_features, y_name, n=4, writefolder=None)","c53fe36d":"# This is my feature selection using the previous figures\n# You can generate a 1st seletion and a more rigorous 2st selection to compare\n# You must do other techniques like RFE, RF feature importances too! This is an initial approach.\n\nfeatures_sel1 = [\n  ##'Patient ID',\n  'Patient age quantile',\n  'SARS-Cov-2 exam result',\n  ##'Patient addmited to regular ward (1=yes. 0=no)',\n  ##'Patient addmited to semi-intensive unit (1=yes. 0=no)',\n  ##'Patient addmited to intensive care unit (1=yes. 0=no)',\n  'Hematocrit', #correlacao 1 com hemoglob\n  'Hemoglobin', #good\n  'Platelets', # good\n  'Mean platelet volume ', # good\n  #'Red blood Cells',\n  ##'Lymphocytes',\n  ##'Mean corpuscular hemoglobin concentration\u00a0(MCHC)',\n  'Leukocytes', # amazing!\n  'Basophils',\n  ##'Mean corpuscular hemoglobin (MCH)',\n  'Eosinophils', #good\n  ##'Mean corpuscular volume (MCV)', corr 0.9 com MCH\n  'Monocytes', #good\n  #'Red blood cell distribution width (RDW)',\n  ##'Serum Glucose',\n  ##'Respiratory Syncytial Virus',\n  ##'Influenza A',\n  ##'Influenza B',\n  ##'Parainfluenza 1',\n  ##'CoronavirusNL63',\n  'Rhinovirus_Enterovirus',\n  ##'Coronavirus HKU1',\n  ##'Parainfluenza 3',\n  #'Chlamydophila pneumoniae',\n  #'Adenovirus',\n  #'Parainfluenza 4',\n  ##'Coronavirus229E',\n  ##'CoronavirusOC43',\n  ##'Inf A H1N1 2009',\n  ##'Bordetella pertussis',\n  ###'Metapneumovirus',\n  ##'Neutrophils',\n  ##'Urea',\n  'Proteina C reativa mg_dL', #good\n  'Creatinine', # good\n  ##'Potassium',\n  ##'Sodium',\n  #'Influenza B. rapid test',\n  #'Influenza A. rapid test',\n  ##'Alanine transaminase',\n  'Aspartate transaminase',\n  'Gamma-glutamyltransferase\u00a0', #good\n  ##'Total Bilirubin',\n  ##'Direct Bilirubin',\n  ##'Indirect Bilirubin',\n  ##'Alkaline phosphatase',\n  ##'Ionized calcium\u00a0',\n  ##'Strepto A',\n  ##'Magnesium',\n  ##'pCO2 (venous blood gas analysis)',\n  ##'Hb saturation (venous blood gas analysis)',\n  ##'Base excess (venous blood gas analysis)',\n  #'pO2 (venous blood gas analysis)',\n  ##'Total CO2 (venous blood gas analysis)',\n  #'pH (venous blood gas analysis)',\n  ##'HCO3 (venous blood gas analysis)',\n  #'Rods #',\n  ##'Segmented',\n  #'Promyelocytes',\n  #'Metamyelocytes',\n  #'Myelocytes',\n  ##'Urine - Aspect',\n  ##'Urine - pH',\n  ##'Urine - Hemoglobin',\n  ##'Urine - Density',\n  ##'Urine - Leukocytes',\n  ##'Urine - Crystals',\n  ##'Urine - Red blood cells',\n  ##'Urine - Color',\n  ##'Relationship (Patient\/Normal)',\n  ##'International normalized ratio (INR)',\n  ##'Lactic Dehydrogenase',\n  ##'Vitamin B12',\n  ##'Creatine phosphokinase\u00a0(CPK)\u00a0',\n  ##'Ferritin',\n  ##'Arterial Lactic Acid',\n  ##'Lipase dosage',\n  ##'Albumin',\n  ##'Hb saturation (arterial blood gases)',\n  ##'pCO2 (arterial blood gas analysis)',\n  ##'Base excess (arterial blood gas analysis)',\n  ##'pH (arterial blood gas analysis)',\n  ##'Total CO2 (arterial blood gas analysis)',\n  ##'HCO3 (arterial blood gas analysis)',\n  ##'pO2 (arterial blood gas analysis)',\n  ##'Arteiral Fio2',\n  ##'Phosphor',\n  ##'ctO2 (arterial blood gas analysis)',\n]","15c962b9":"Selecting good features and rejecting bad ones is a task which there is several tools. Unfortunately for categorical features there less possiblities.\n\nI saw a very interesting work:\nhttps:\/\/github.com\/abhayspawar\/featexp\nThis work have some ideas to get the feature information. I made a reimplementation of this idea in the form of a function 'univ_scatter'.\n\nThe pro of a univariate feature expression is to see and decide in a graph if the feature is meaningfull. The con is the same as another univariate technique: you don't know if the features are redundant. You will have to check this with other techniques.\n\nI rushed to run this selection and I cleaned the original dataset.xlsx directly in libreoffice (shame on me). I'll use that file in this notebook. I saw after that a great work to clean the dataset here:\nhttps:\/\/www.kaggle.com\/ossamum\/exploratory-data-analysis-and-feature-importance\n\n"}}