{"cell_type":{"9e3866b5":"code","5965f35a":"code","b21f3609":"code","b1973e6f":"code","3ad9d3ca":"code","a4a7914e":"code","24a28948":"code","0253d1b5":"code","7d12f3b3":"code","192a799e":"code","2a142752":"code","38a15531":"code","e0622de8":"code","0ca42959":"code","84bda3f0":"code","8e0fc814":"code","875ac879":"code","6dcb92c1":"code","0c67fd2f":"code","3c7d7fc4":"markdown","06179325":"markdown","05d72d62":"markdown","3a8d76ba":"markdown","494b9a6c":"markdown","2d28e4b7":"markdown","7846f614":"markdown","2157bb12":"markdown","9ff85f24":"markdown","4facc69b":"markdown","93360918":"markdown","963d499c":"markdown","b3b243a9":"markdown","6e1a0518":"markdown","1db00152":"markdown","ceb43a59":"markdown","f8f62103":"markdown","35595a18":"markdown","d2b79eb4":"markdown","3bfe5ed1":"markdown","6ae50f5a":"markdown","aa376569":"markdown"},"source":{"9e3866b5":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","5965f35a":"import os\nfrom fastai import *\nfrom fastai.vision import *","b21f3609":"base_path = Path(os.path.join(\"..\", \"input\", \"intel-image-classification\"))\nprint(base_path.ls(), end=\"\\n\\n\")\n\ntrain_path = base_path\/'seg_train'\/'seg_train'\nprint(train_path.ls(), end=\"\\n\\n\")\nval_path = base_path\/'seg_test'\/'seg_test'\nprint(val_path.ls(),end=\"\\n\\n\")\npred_path = base_path\/'seg_pred'\/'seg_pred'\nprint(\"No. of test images : {}\".format(len(pred_path.ls())))","b1973e6f":"data = ImageDataBunch.from_folder(path = base_path,\n                                 train = 'seg_train',\n                                 valid = 'seg_test',\n                                 test = 'seg_pred',\n                                 seed = 42,\n                                 ds_tfms = get_transforms(),\n                                 bs = 32,\n                                 size = 224)\ndata.normalize(imagenet_stats)","3ad9d3ca":"data.show_batch(rows = 3, figsize = (12,10))","a4a7914e":"learner = cnn_learner(data, models.resnet34, metrics=[accuracy, error_rate])\nlearner.model_dir = \"\/kaggle\/working\/\"","24a28948":"learner.fit_one_cycle(5)","0253d1b5":"learner.save('resnet34-stage-1')","7d12f3b3":"interp = ClassificationInterpretation.from_learner(learner)","192a799e":"interp.plot_top_losses(5, figsize=(12,10), heatmap = True)","2a142752":"interp.plot_confusion_matrix(figsize=(12,10), dpi=60)","38a15531":"interp.most_confused(min_val = 5)","e0622de8":"learner.lr_find()\nlearner.recorder.plot(suggestion=True)","0ca42959":"learner.unfreeze()\nlearner.fit_one_cycle(5, max_lr = slice(1e-6,1e-4))","84bda3f0":"learner.save('resnet34-stage-2')","8e0fc814":"interp = ClassificationInterpretation.from_learner(learner)","875ac879":"interp.plot_confusion_matrix(figsize = (12,10), dpi = 60)","6dcb92c1":"interp.plot_top_losses(5, figsize = (15,11))","0c67fd2f":"interp.most_confused(5)","3c7d7fc4":"Let's plot the confusion matrix...","06179325":"Let's start this Jupyter notebook with these 3 lines.<br \/>\nAnything starting with % in Jupyter notebook is called a 'magic'. It's not Python code. It's a special Jupyter command.<br\/><br\/>\nThe code means the following :-\n*   If somebody changes underlying library code while I'm running this, please reload it automatically\n*   If somebody asks to plot something, then please plot it here in this Jupyter Notebook","05d72d62":"Now, we will be starting the data modelling process. It's quite easy when we use the fast-ai library.<br>\n\nThe ```cnn_learner``` is an API of the ```vision``` package that is used to build the learner object. The API takes in following parameters :-\n\n* data : The databunch object that will be used for training and validation purposes.\n* model : The architecture on which the data will be trained and evaluated. Here, we will be using the ResNet34 architecture for simplification reasons.\n* metrics : The metrics\/figures that depict the quality of the model predictions, It can ```accuracy```, ```error_rate```,etc.\n\nFast.ai consists of various model architectures already available. You just need to call the ```models``` API for that and pick any relevant architecture.\n\nThe other thing to be noted is that when the learner object is created, the trained model is saved in a directory called 'models' located wherever the image dataset is located.<br>\nNow, Kaggle kernels are built in such a way that there's a read-only permission applied to the dataset directory i.e ```\/input\/kaggle\/intel-image-classification```.<br>\nWe need to change this directory in order to save and load the trained models and avoid retraining the models again and again. So we write the following line after creating the ```learner``` object as follows :-\n\n```\nlearner.model_dir = \"\/kaggle\/working\"\n```\n\nWhen the learner object is being created, the fast-ai library downloads a pretrained model of the requested architecture from the PyTorch framework.","3a8d76ba":"Let's import the required packages :-\n\n1.   fastai : The deep learning library that sits on top of PyTorch.\nFor documentation, please visit <a href=\"docs.fast.ai\">docs.fast.ai<\/a>\n2.   fastai.vision : fastai consists of vision as its core application for working with computer vision tasks.\n3.   os : To deal with OS-related tasks like reading files, etc.\n\n","494b9a6c":"## CONCLUSION :\nWe've used fast-ai library to build a world class image classifier that can be used in computer vision challenges like this one.<br>\nIf you like this notebook, please upvote this. For questions, just comment and I'll respond to you guys.<br><br>\nI've also built other tutorials like this. Visit <a href=\"www.github.com\/umangjpatel\">my GitHub profile<\/a>.<br><br>\nAdios, I'll see you soon...","2d28e4b7":"Let's start with the images that were highly misclassified. To do that, we have the method called ```plot_top_losses```. The parameters to this method are :-\n\n* k : The number of images to plot.\n* figsize : The size of the figure for each top loss image.\n* heatmap : The visualization displaying which parts of the image activated the predicted class.\n\nThe output will be each image having the title of the format <b>prediction\/actual\/loss\/probability<\/b>. It means :-\n\n* prediction : The predicted class of the image.\n* actual : The actual class of the image.\n* loss : The value of the loss\/error.\n* probability : The probability of the actual class when the misclassification occured.","7846f614":"### Analysis conclusion:\nWe can clearly see that there is a label noise from our dataset. It's pretty bad. In the last figures regarding the ```plot_top_losses``` code, the forest class was predicted correctly as the image contained only forest image with a deer in it. But the actual label was glacier which is clearly not visible in the image.<br>\nTherefore, it can be concluded that these results are pretty much correct. If we remove the label noise, then would be achieve even more correct results.","2157bb12":"Next step after creating your data pipeline is to check how the images look before we train our model.\n\nTo do this, we use the ```show_batch``` method of ```ImageDataBunch``` object. The methods takes in 2 arguments as follows :-\n\n* rows : The no. of rows of images to be plotted.\n* figsize : The size of the figure for each image to will be plotted\n\nThe output also consists of the label of the respective image in the title of the figure.","9ff85f24":"Now ,what if we want to improve our model predictions ? What we need to do is train the model at good learning rates. Learning rates are the parameter that can significantly change the predictions of your model.<br>\nFast-ai provides the code to find a suitable learning rate for our model. We can even visualize the learning rate with the loss generated by our model.","4facc69b":"Confusion matrix is another vital piece during model analysis. It helps you give an idea of how many images were clearly identified as correct by the model and how many didn't classify as expected.\nThe fast-ai library provides a ```plot_confusion_matrix``` method for that purpose.","93360918":"Also check the most confused predictions of our model.","963d499c":"We see that the new accuracy is 93.63% which has increased by 1%. That's astonishing in itself. Let's quickly save the new model","b3b243a9":"Another crucial piece to identity misclassifications is ```most_confused```. ```most_confused``` is used to identify the possible classes between which the model was 'most-confused'.","6e1a0518":"We've got to choose the learning rates. One learning rate will be used to train the initial layers of our model while the other will be used to train the final layers of our model. The range of learning rates will be used by the layers between the initial and final layers.<br><br>\nSelecting good LR is challenging here. What we do is select the initial LR only. We select that LR after which the loss increases significantly.<br>\nFor final layers LR, we'll reduce the previous LR by 10 times. (By default, the LR is 3e-2) i.e we'll use 3e-4 as our LR for the final layers.<br><br>\nWe first unfreeze the model and retrain the model with the selected learning rates as follows.","1db00152":"Let's begin with creating the databunch object. DataBunch is an API in the fastai which helps practitioners build data pipelines for the models. Now, this API is subclassed into various specific databunch objects i.e ImageDataBunch works with data of the form of images.<br\/>\n\nThe ImageDataBunch consists of several factory methods via which you can create your data pipeline. They are listed in this link : <a href=\"https:\/\/docs.fast.ai\/vision.data.html#Factory-methods\">https:\/\/docs.fast.ai\/vision.data.html#Factory-methods<\/a>.<br\/>\nThese methods are defined according to the most common observed patterns via which the data is stored. For instance, if your images are stored in a single directory and the labels are stored in a CSV file, then the method used is :-\n\n```\ndatabunch = ImageDataBunch.from_csv(...)\n```\n\nIn this case, the images are stored in the 'ImageNet' style directory structure, i.e the labels of the images are the respective directory in which they are stored. That's why we use the following method :-\n\n```\ndatabunch = ImageDataBunch.from_folder(...)\n```\n\nThere are several parameters that we pass in the factory method. Let's talk about them now.\n\n* <u>path<\/u> : The path of the base directory i.e where all the image datasets are stored.\n* <u>train<\/u> : The name of the directory which will be the training set of our model.\n* <u>valid<\/u> : The name of the directory which will be the validation set of our model.\n* <u>test<\/u> : The name of the directory which will be the test set of our model.\n* <u>seed<\/u> : Any value which is used in order to obtain reproducible results\n* <u>ds_tfms<\/u> : It stands for data transforms. What it does is apply data augmentation techniques to the image like random zooming, center cropping, changing the contrast of the images, randomly rotating the images by certain degrees, etc. We've used the default values for all this techniques. It works fine for the fine-grained classification.\n* <u>bs<\/u> : It stands for batch size. It means how many images shall be there in each batch in order to be passed to the GPU for further computations.\n* <u>size<\/u> : The size of the images that will be resized into, in terms of pixels. For example, if size = 224, it means that the images will be resized into 224 x 224 pixels.\n\nThe other thing that we need to do is normalize the images. Normalizing the images with mean = 0 and standard deviation = 1, will not improve the predictions in general, but also speed up the training process to a great extent. To do this in fast-ai, we do the following :-\n\n```\ndatabunch.normalize(imagenet_stats)\n```","ceb43a59":"We again create our ```ClassificationInterpretation``` object for model analysis.","f8f62103":"We've got excellent results. Let's analyze them if anyone doesn't mind. My advice would be to see them as it'll get you a good sense of how predictions were made and how much correct they are.<br><br>\nWe'll be using the ```ClassificationInterpretation``` API in order to perform analysis. It's a single line of code that takes in a ```learner``` object and the output will be an ```interpretation``` object which we'll use for further analysis tasks.","35595a18":"To begin with any deep learning project, we need data. Without data, you can't do something. Kaggle provided an amazing dataset called \"Intel Image Classification\" hosted by Puneet Bansal. The link is : <a href=\"https:\/\/www.kaggle.com\/puneet6060\/intel-image-classification\">https:\/\/www.kaggle.com\/puneet6060\/intel-image-classification<\/a><br\/><br\/>\nThe dataset is divided into 3 subsets as follows :-\n\n*   seg_train : The training set\n*   seg_test : The validation set\n*   seg_pred : The test set\n\nHere, I'll be storing their respective paths in separate variables. These paths will be used when I'll create the databunch object during the modelling process.","d2b79eb4":"The step that you all are waiting for !!! Let's begin the training. All you need is write a single line of code. We'll be writing the following line :-\n\n```\nlearner.fit_one_cycle(5)\n```\n\nWhat we've done is use one-cycle learning approach of training large neural networks. The paper is located here : <a href=\"https:\/\/arxiv.org\/abs\/1506.01186\">https:\/\/arxiv.org\/abs\/1506.01186<\/a><br>\nThe implementation of this concept is already baked in the fast-ai library. With just 5 epochs (or less), you guys can easily create a world-class image classifier without any trouble of making a lot of decisions regarding the selection of hyperparameters and other configuration that you would have to deal with in Keras and TensorFlow.<br><br>\n\nOne thing to note is that the tabulated results during the training phase are of the validation set. Fast-ai makes it hard for practitioners to develop a model without any validation set. If we don't use this set, then the model is highly likely to overfit (memorize the images and not learn patters among them).","3bfe5ed1":"Let's check out the top losses from our model.","6ae50f5a":"# Intel Scenery Classification using Fast.ai library","aa376569":"We can see that we've obtained a whooping 92.6% accuracy !!! Whoa.... that is so cool. Now you don't want to lose all your efforts. Let' save it. It's quite simple."}}