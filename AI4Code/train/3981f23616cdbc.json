{"cell_type":{"75fbeb17":"code","56a6c83b":"code","b2326af9":"code","b932875a":"code","fe752cc6":"code","5ac3ff3e":"code","4910c5ea":"code","c35583fd":"code","4d21150f":"code","7b651789":"code","5fd688ce":"code","9c0ae986":"code","ebd4a6a5":"code","d36e3556":"code","04d1576e":"code","5fe21d55":"code","27c70585":"code","48cad905":"code","489752d4":"code","c3ecd82a":"code","e6bd6277":"code","c8d4abec":"code","7a4ac627":"code","05fb9472":"code","74eee39c":"code","528d0d48":"code","f156b5c1":"code","531a6324":"code","b8a5f42e":"code","74c17826":"code","602a4974":"code","defdad41":"code","8c912966":"code","e80cc0a3":"code","046842de":"code","6b0d1772":"code","cc029a68":"code","b16fa786":"code","d5ea6000":"code","b6ffa52d":"code","dfa9a705":"code","4f330b48":"code","0886ddfe":"code","2d4ab3c8":"code","6785c2f5":"code","0930e7a4":"code","a0d8a06e":"code","b24647f9":"code","c8f66b14":"code","aff4c680":"code","278b7942":"code","8b1439f1":"code","71d61650":"markdown","ff330cdd":"markdown","f818c31e":"markdown","4ba4345c":"markdown","9b564db2":"markdown","53f4e923":"markdown","b5450ea7":"markdown","2c79732d":"markdown","842fa349":"markdown","f1966295":"markdown","2a32bb99":"markdown","4941bda3":"markdown","7e10c280":"markdown","0400ddff":"markdown","e67a5fab":"markdown","cbc8d6e2":"markdown","ad8af1c0":"markdown","67da3b3c":"markdown","f420da32":"markdown","7bf9a225":"markdown","07fe5115":"markdown","143e9c83":"markdown","762f0866":"markdown","1983024c":"markdown","1c53a4ea":"markdown","2896f4bb":"markdown","af76e1e4":"markdown","79866319":"markdown","f500cbb3":"markdown","f6033c5a":"markdown","f88f2772":"markdown","b8eb51f9":"markdown","ee160bb5":"markdown","f5e50e01":"markdown","f64c078d":"markdown","e54f94f5":"markdown","e527b65d":"markdown","875495fc":"markdown","4a8a9115":"markdown","e7cd7ea3":"markdown","523153ab":"markdown","5dfa7a9e":"markdown","0f3759a7":"markdown","4fc33ccf":"markdown","028efb9f":"markdown","72d06277":"markdown"},"source":{"75fbeb17":"import numpy as np\nimport pandas as pd\n\n# for the plots\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\nimport seaborn as sns\n\nimport string\nimport pickle\n\n# for linear regressions\nfrom scipy import stats\n\n# for linear regressions with train and test data\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score","56a6c83b":"filename = '\/kaggle\/input\/harry-potter-and-the-philosophers-stone-script\/hp_script.csv'\nscript = pd.read_csv(filename, encoding='cp1252')\nscript.head()","b2326af9":"script.drop(columns='ID_number', inplace=True)\nscript.head()","b932875a":"script.loc[0, 'dialogue']","fe752cc6":"script.isna().sum()","5ac3ff3e":"lines = script['character_name'].value_counts()\nlines['Harry Potter']","4910c5ea":"character = pd.DataFrame(script['character_name'].unique(), columns=['name'])\ncharacter['lines'] = character['name'].apply(lambda name: lines[name])\ncharacter.sort_values(by='lines', ascending=False, inplace=True)\ncharacter.head()","c35583fd":"character['color'] = 'grey'\n\ncharacter.set_index('name', inplace=True)\n\ncharacter.loc['Harry Potter', 'color'] = 'green'\ncharacter.loc['Ron Weasley', 'color'] = 'red'\ncharacter.loc['Hermione Granger', 'color'] = 'brown'\ncharacter.head()","4d21150f":"top10 = character.head(10)\n\nplt.title('Top 10 number of lines of characters.')\nplt.barh(top10.index, top10['lines'], color=top10['color'])\nplt.gca().invert_yaxis()\nplt.xlabel('Number of lines')\nplt.ylabel('Character')\nplt.grid()\nplt.show()","7b651789":"script['words'] = script['dialogue'].apply(lambda x: len(x.split()))\nscript.head()","5fd688ce":"words = script[['character_name', 'words']].groupby('character_name').sum()\nwords.loc['Harry Potter', 'words']","9c0ae986":"character.reset_index(inplace=True)\ncharacter['words'] = character['name'].apply(lambda name: words.loc[name, 'words'])\ncharacter.set_index('name', inplace=True)\ncharacter.head()","ebd4a6a5":"top10lines = character.sort_values(by='lines', ascending=False).head(10)\ntop10words = character.sort_values(by='words', ascending=False).head(10)\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 5))\n\nax[0].set_title('Top 10 number of lines of characters.')\nax[0].barh(top10lines.index, top10lines['lines'], color=top10lines['color'])\nax[0].invert_yaxis()\nax[0].set_xlabel('Number of lines')\nax[0].set_ylabel('Character')\nax[0].grid()\n\nax[1].set_title('Top 10 number of words of characters.')\nax[1].barh(top10words.index, top10words['words'], color=top10words['color'])\nax[1].invert_yaxis()\nax[1].set_xlabel('Number of words')\nax[1].set_ylabel('Character')\nax[1].grid()\n\nfig.tight_layout()\n\nfig.show()","d36e3556":"character.describe()","04d1576e":"def line(slope, intercept, x):\n    return slope * x + intercept","5fe21d55":"def words_vs_lines(df, scale = 'lin'):\n    \n    copy = df.copy()\n    \n    if scale == 'log':\n        copy['lines'] = np.log(df['lines'])\n        copy['words'] = np.log(df['words'])\n    \n    x = copy['lines']\n    y = copy['words']\n    c = copy['color']\n    \n    # subset for text labels\n    mask = df['words'] > 400\n    t = copy[mask]\n\n    # LINEAR REGRESSION with H, R, H\n    s1, i1, r1, p1, e1 = stats.linregress(x, y)\n    if scale == 'log':\n        label1=f'HRH: log(words) = {s1.round(1)} * log(lines) + {i1.round(1)}, r2 = {r1.round(2)}'\n    else:\n        label1=f'HRH: words = {s1.round(1)} * lines + {i1.round(1)}, r2 = {r1.round(2)}'\n    \n    # LINEAR REGRESSION without H, R, H\n    main_characters = ['Harry Potter', 'Ron Weasley', 'Hermione Granger']\n    xx = x[~x.index.isin(main_characters)]\n    yy = y[~y.index.isin(main_characters)]\n    s2, i2, r2, p2, e2 = stats.linregress(xx, yy)\n    if scale == 'log':\n        label2=f'noHRH: log(words) = {s2.round(1)} * log(lines) + {i2.round(1)}, r2 = {r2.round(2)}'\n    else:\n        label2=f'noHRH: words = {s2.round(1)} * lines + {i2.round(1)}, r2 = {r2.round(2)}'\n    \n    # FIGURE\n    plt.title('Words versus lines.')\n    if scale == 'log':\n        plt.xlabel('Log of number of lines')\n        plt.ylabel('Log of number of words')\n    else:\n        plt.xlabel('Number of lines')\n        plt.ylabel('Number of words')\n\n    # scatter\n    plt.scatter(x, y, c='none', edgecolor=c)\n    t.apply(lambda row: plt.text(row['lines'], row['words'], row.name, c=row['color']), axis=1)\n\n    # lines\n    x_array = np.array([min(x), max(x)])\n    plt.plot(x_array, line(s1, i1, x_array), c='g', ls='--', lw=1, label=label1)\n    plt.plot(x_array, line(s2, i2, x_array), c='k', lw=1, label=label2)\n\n    # limits\n    margin = (max(y) - min(y)) \/ 10\n    plt.ylim([min(y) - margin, max(y) + margin])\n        \n    # legend\n    plt.legend()\n    \n    plt.show()","27c70585":"words_vs_lines(character)\nwords_vs_lines(character, 'log')","48cad905":"def linreg(X_train, X_test, y_train, y_test):\n\n    # 1. Linear Regression.\n    reg = LinearRegression()\n    reg.fit(X_train, y_train)\n\n    pred = reg.predict(X_test)\n\n    print(f'y = {reg.intercept_.round(2)}', end=' ')\n    i = 0\n    for c in reg.coef_.round(2):\n        i += 1\n        print(f'+ {c} * x{i}', end=' ')\n    \n    r2_test = r2_score(y_test, pred)\n    r2_train = r2_score(y_train, reg.predict(X_train))\n\n    up = max(max(pred), max(y_test))\n    down = min(min(pred), min(y_test))\n    margin = (up - down) \/ 20\n    \n    # 2. Plot.\n    plt.title('Linear regression.')\n    plt.scatter(pred, y_test, color='black', label=f'test, r2 = {r2_test.round(2)}')\n    plt.scatter(y_train, reg.predict(X_train), color='none', edgecolor='black', label=f'train, r2 = {r2_train.round(2)}')\n    plt.plot([down - margin, up + margin], [down - margin, up + margin], linewidth=1, color='blue')\n    plt.xlabel('Prediction')\n    plt.ylabel('Truth')\n    plt.xlim([down - margin, up + margin])\n    plt.ylim([down - margin, up + margin])\n    plt.legend()\n    plt.grid()\n\n    plt.show()","489752d4":"X = character[['lines']]\ny = character['words']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nlinreg(X_train, X_test, y_train, y_test)","c3ecd82a":"X_train = np.log(X_train)\nX_test = np.log(X_test)\ny_train = np.log(y_train)\ny_test = np.log(y_test)\n\nlinreg(X_train, X_test, y_train, y_test)","e6bd6277":"character['wpl'] = character.apply(lambda row: row.words \/ row.lines, axis=1)\ncharacter.head()","c8d4abec":"top30wpl = character.sort_values(by='wpl', ascending=False).head(30)\n\nplt.figure(figsize=(10,10))\n\nplt.title('Top 30 number of words per line of characters.')\nplt.barh(top30wpl.index, top30wpl['wpl'], color=top30wpl['color'])\nplt.gca().invert_yaxis()\nplt.xlabel('Words per line')\nplt.ylabel('Character')\nplt.grid()\nplt.show()","7a4ac627":"words = script[['character_name', 'dialogue']].copy()\nwords.columns = ['character', 'word']\nwords.head()","05fb9472":"words['word'] = words['word'].str.replace('[^\\w\\s]', '')\nwords['word'] = words['word'].str.lower()\nwords.head()","74eee39c":"words['word'] = words['word'].str.split()\nwords.head()","528d0d48":"words = words.explode('word').reset_index(drop=True)\nwords.head()","f156b5c1":"def say_my_name(name):\n    df = words[words['word'] == name]\n    df = df.groupby('character').count()\n    df = df.sort_values(by='word', ascending=False)\n    \n    top10 = df.copy().head(10)\n    \n    top10['color'] = 'grey'\n    top10.loc['Harry Potter', 'color'] = 'green'\n    top10.loc['Ron Weasley', 'color'] = 'red'\n    top10.loc['Hermione Granger', 'color'] = 'brown'\n    \n    plt.title(f'Who said {name}?')\n    plt.barh(top10.index, top10['word'], color=top10['color'])\n    plt.gca().invert_yaxis()\n    plt.xlabel(f'Number of times a character says {name}')\n    plt.ylabel('Character')\n    plt.grid()\n    plt.show()","531a6324":"say_my_name('voldemort')","b8a5f42e":"from nltk import word_tokenize\nfrom nltk.tag import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.corpus import stopwords","74c17826":"contractions_dict = {\"ain't\": 'am not', \"aren't\": 'are not', \"can't\": 'cannot', \"can't've\": 'cannot have', \"'cause\": 'because', \"could've\": 'could have', \"couldn't\": 'could not', \"couldn't've\": 'could not have', \"didn't\": 'did not', \"doesn't\": 'does not', \"don't\": 'do not', \"hadn't\": 'had not', \"hadn't've\": 'had not have', \"hasn't\": 'has not', \"haven't\": 'have not', \"he'd\": 'he would', \"he'd've\": 'he would have', \"he'll\": 'he will', \"he'll've\": 'he will have', \"he's\": 'he is', \"how'd\": 'how did', \"how'd'y\": 'how do you', \"how'll\": 'how will', \"how's\": 'how is', \"i'd\": 'i would', \"i'd've\": 'i would have', \"i'll\": 'i will', \"i'll've\": 'i will have', \"i'm\": 'i am', \"i've\": 'i have', \"isn't\": 'is not', \"it'd\": 'it had', \"it'd've\": 'it would have', \"it'll\": 'it will', \"it'll've\": 'it will have', \"it's\": 'it is', \"let's\": 'let us', \"ma'am\": 'madam', \"mayn't\": 'may not', \"might've\": 'might have', \"mightn't\": 'might not', \"mightn't've\": 'might not have', \"must've\": 'must have', \"mustn't\": 'must not', \"mustn't've\": 'must not have', \"needn't\": 'need not', \"needn't've\": 'need not have', \"o'clock\": 'of the clock', \"oughtn't\": 'ought not', \"oughtn't've\": 'ought not have', \"shan't\": 'shall not', \"sha'n't\": 'shall not', \"shan't've\": 'shall not have', \"she'd\": 'she would', \"she'd've\": 'she would have', \"she'll\": 'she will', \"she'll've\": 'she will have', \"she's\": 'she is', \"should've\": 'should have', \"shouldn't\": 'should not', \"shouldn't've\": 'should not have', \"so've\": 'so have', \"so's\": 'so is', \"that'd\": 'that would', \"that'd've\": 'that would have', \"that's\": 'that is', \"there'd\": 'there had', \"there'd've\": 'there would have', \"there's\": 'there is', \"they'd\": 'they would', \"they'd've\": 'they would have', \"they'll\": 'they will', \"they'll've\": 'they will have', \"they're\": 'they are', \"they've\": 'they have', \"to've\": 'to have', \"wasn't\": 'was not', \"we'd\": 'we had', \"we'd've\": 'we would have', \"we'll\": 'we will', \"we'll've\": 'we will have', \"we're\": 'we are', \"we've\": 'we have', \"weren't\": 'were not', \"what'll\": 'what will', \"what'll've\": 'what will have', \"what're\": 'what are', \"what's\": 'what is', \"what've\": 'what have', \"when's\": 'when is', \"when've\": 'when have', \"where'd\": 'where did', \"where's\": 'where is', \"where've\": 'where have', \"who'd\": 'who would', \"who'll\": 'who will', \"who'll've\": 'who will have', \"who's\": 'who is', \"who've\": 'who have', \"why's\": 'why is', \"why've\": 'why have', \"will've\": 'will have', \"won't\": 'will not', \"won't've\": 'will not have', \"would've\": 'would have', \"wouldn't\": 'would not', \"wouldn't've\": 'would not have', \"y'all\": 'you all', \"y'alls\": 'you alls', \"y'all'd\": 'you all would', \"y'all'd've\": 'you all would have', \"y'all're\": 'you all are', \"y'all've\": 'you all have', \"you'd\": 'you had', \"you'd've\": 'you would have', \"you'll\": 'you you will', \"you'll've\": 'you you will have', \"you're\": 'you are', \"you've\": 'you have'}","602a4974":"script['tokens'] = script['dialogue'].str.lower()\nscript.head()","defdad41":"def dict_replace(sentence):\n    for key in contractions_dict:\n        sentence = sentence.replace(key, contractions_dict[key])\n    return sentence","8c912966":"script['tokens'] = script['tokens'].apply(dict_replace)\nscript.head()","e80cc0a3":"script['tokens'] = script['tokens'].apply(word_tokenize)\nscript.head()","046842de":"stop_words = stopwords.words('english')\n\ndef clean_tokens(tokens_list):\n    cleaned_tokens_list = []\n    \n    # Identify Part Of Speech (POS)\n    for token, tag in pos_tag(tokens_list):\n        if tag == 'NN' or tag == 'NNS':\n            # Noun (non proper)\n            pos = 'n'\n        elif tag.startswith('VB'):\n            # Verb\n            pos = 'v'\n        elif tag.startswith('JJ'):\n            # Adjective\n            pos = 'a'\n        else:\n            continue\n        \n        # Lemmatize (for instance, cats -> cat, bringing -> bring, great -> good)\n        lemmatizer = WordNetLemmatizer()\n        token = lemmatizer.lemmatize(token, pos)\n        \n        # Filter out punctuation marks and stop_words\n        if token not in string.punctuation and token.lower() not in stop_words:\n            cleaned_tokens_list.append(token.lower())\n        \n    return cleaned_tokens_list","6b0d1772":"script['clean_tokens'] = script['tokens'].apply(clean_tokens)\nscript.head()","cc029a68":"filename = '\/kaggle\/input\/twitter-sentiment-analysis\/twitter_model.sav'\nmodel = pickle.load(open(filename, 'rb'))","b16fa786":"dist = model.prob_classify({'kill': True})\ndist.prob('pos')","d5ea6000":"script['dict'] = script.apply(lambda row: dict([token, True] for token in row['clean_tokens']), axis=1)\nscript.head()","b6ffa52d":"script['sentiment'] = script.apply(lambda row: model.prob_classify(row['dict']).prob('pos'), axis=1)\nscript.head()","dfa9a705":"words = ['good', 'evening', 'professor', 'dumbledore', 'rumour', 'true', 'hagrid', 'bring']\n\nprint('Probability that a tweet with some word is positive:\\n')\n\nfor word in words:\n    dist = model.prob_classify({word: True})\n    x = dist.prob('pos') * 100\n    category = 'POS' if x > 55 else ('NEG' if x < 45 else 'neutral')\n    print(f'p({word}) = {round(x, 2)} % ({category})')","4f330b48":"mask = script['character_name'] == 'Draco Malfoy'\ncolumns = ['dialogue', 'clean_tokens', 'sentiment']\nscript.loc[mask, columns].head(10)","0886ddfe":"df = script.copy()\ndf['sentiment'] = (2 * df['sentiment'] - 1).round(5)\ndf['sentiment'] = df.apply(lambda row: row['sentiment'] if len(row['dict']) > 0 else 0, axis=1)\ndf['sentiment_cat'] = df['sentiment'].apply(lambda x: 'POS' if x > 0.05 else ('NEG' if x < -0.05 else 'neutral'))\ndf.drop(columns=['tokens', 'dict'], inplace=True)\ndf.head()","2d4ab3c8":"np.mean(df['sentiment'])","6785c2f5":"plt.title('Sentiment distribution.')\nplt.hist(df['sentiment'], bins=11, color='b', edgecolor='k')\nplt.xlabel('Sentiment')\nplt.ylabel('Counts')\nplt.grid()\nplt.show()","0930e7a4":"total = df[['character_name', 'sentiment']].groupby('character_name').sum()\ntotal.loc['Harry Potter', 'sentiment']","a0d8a06e":"character.reset_index(inplace=True)\ncharacter['sentiment'] = character.apply(lambda row: total.loc[row['name'], 'sentiment'] \/ lines[row['name']], axis=1)\ncharacter['sentiment_cat'] = character['sentiment'].apply(lambda x: 'POS' if x > 0.05 else ('NEG' if x < -0.05 else 'neutral'))\ncharacter.set_index('name', inplace=True)\ncharacter.head()","b24647f9":"character.describe()","c8f66b14":"mask = character['words'] > 200\ncolumns = ['sentiment', 'sentiment_cat']\ncharacter.loc[mask, columns].sort_values(by='sentiment', ascending=False)","aff4c680":"mask = script['character_name'] == 'Petunia Dursley'\ncolumns = ['dialogue', 'clean_tokens', 'sentiment', 'sentiment_cat']\ndf.loc[mask, columns].head(10)","278b7942":"mask = script['character_name'] == 'Rubeus Hagrid'\ncolumns = ['dialogue', 'clean_tokens', 'sentiment', 'sentiment_cat']\ndf.loc[mask, columns].head(10)","8b1439f1":"tokens = ['wizard', 'harry']\n\nprint('Probability that a tweet with some word is positive:\\n')\n\nfor token in tokens:\n    dist = model.prob_classify({token: True})\n    x = dist.prob('pos') * 100\n    category = 'POS' if x > 55 else ('NEG' if x < 45 else 'neutral')\n    print(f'p({token}) = {round(x, 2)} % ({category})')","71d61650":"We create a new column to our dataframe *script*: *tokens*.","ff330cdd":"Given a dictionary of tokens, the model gives the probability that the tokens come from a positive message. For instance, because tweets containing the verb *kill* are typically negative, the probability that *kill* comes from a positive message is low, around 14%.","f818c31e":"The model works fine in most cases: Petunia talks very positively to Dudley, with positive tokens like *perfect*, *wonderful* or *darling*.\n\nIf we do the same with Hagrid, we observe quite the opposite: most lines are classified as negative.","4ba4345c":"## 1.3 Words per line.\n\nA ratio between words and lines comes to mind with this information. Of course, a *words per line* ratio (simplified as *wpl*) can be helpful in this analysis.","9b564db2":"And apply it to our dataframe. See how *i'm* has become *i am*.","53f4e923":"And apply the model, creating a new column: *sentiment*.","b5450ea7":"We make a function to replace contractions.","2c79732d":"The function we're looking for can be made:","842fa349":"Before filtering, we also have to analyse the number of words of each character. Indeed, what if a charachter has many lines but these lines are just single words or very short sentences? A character with less lines but more words is more important that a character with more lines but less words (at least in this analysis, as our purpose is to find sentiments in rather long sentences, not in short phrases like *Yes*, *No* or *Thanksss*.).\n\n## 1.2 Number of words.\n\nThe known functions *len* and *split* will do the job of counting how many words there are in each line of dialogue. Notice how contractions don't matter - for instance, *I'm* is considered a single word.","f1966295":"As expected, it is complete. Let's check for missing values now.","2a32bb99":"This is a very simple model that assigns a probability, between 0 and 1, given a collection of tokens. A human can't distinguish if the sentences *Good evening, Professor Dumbledore. Are the rumours true?* are positive or negative, but this model has assigned a negative value to the word *evening* because the sample of tweets with which it has been trained contains more negative tweets with the word *evening* than positive. See below.\n\nSimilarly, the sentence *Hagrid is bringing him.* is classified as positive because there are more positive tweets in the sample with the verb *bring* than negative.","4941bda3":"This has some valuable information: to start, there are 41 characters that have at least one line (and at least one word) in the movie. We have a mean 19 number of lines per character and 220 words in the whole movie. Also, the minimum of lines and words is 1, which means that there are characters that have just one line of one word (like the snake: *thanksss*). We already know the maximum: Harry Potter has 230 lines in the whole movie and 1609 words in total.\n\nIf we plot the pairs (lines, words), we observe a pretty linear relation.","7e10c280":"## 1.4 Who said what?\n\nA function that counts how many times each character has said a word can also be done. First, let's copy the *script* dataframe and rename its columns.","0400ddff":"Now we plot both graphs: number of lines and number of words. We observe how Rubeus Hagrid, who is 4th place in the number of lines ranking, becomes 2nd in the number of words ranking, meaning he has less lines than Ron or Hermione, but longer sentences.","e67a5fab":"We can apply this function to the values in a linear scale, but this results in a worse $r^2$ score: the characters with high numbers of lines and words deviate the prediction.","cbc8d6e2":"The first column can be dropped, as pandas.DataFrame has automatically generated a column of indices.","ad8af1c0":"See how a sentence like *And the boy?* becomes just *boy* after the filtering, as *and* and *the* are stop words. Also, we observe how *bringing* becomes *bring* after the lemmatizer has been applied.\n\n# 3. Sentiment analysis with Twitter model.\n\nWe use the twitter model we've developed in another notebook. See https:\/\/www.kaggle.com\/mlopez13\/twitter-sentiment-analysis","67da3b3c":"We can add up all scores for each character and then divide by the total number of lines that each character has to find the average sentiment.","f420da32":"Now remove punctuation marks and change uppercase letters to lowercase. See how contractions lose their apostrophe: *I'm* becomes *im*.","7bf9a225":"It's time to plot now. It's not surprising that Harry Potter has the greatest number of lines in a movie called *Harry Potter*.","07fe5115":"We define a function to filter and lemmatize the tokens. Check this link for Part Of Speech tags: https:\/\/www.ling.upenn.edu\/courses\/Fall_2003\/ling001\/penn_treebank_pos.html","143e9c83":"We split the strings into lists of words.","762f0866":"We can also use the method *.describe()* to check the principal features of the numerical values of *character*.","1983024c":"No missing values are found. Great!\n\n## 1.1 Number of lines.\n\nAs we want to apply statistical analysis to what the charachters say, we have to previously filter those characters with a minimum number of lines in the movie.\n\nLet's count how many lines has each character and store it in an array, *lines*. For instance, we see that Harry Potter has 230 lines within the whole movie.","1c53a4ea":"Before plotting these values, let's create a new column in the *character* dataframe: *color*. We'll just set specific colors for the three main characters.","2896f4bb":"# 2. Tokenize and lemmatize.","af76e1e4":"# 1. Exploratory Data Analsys.\n\nLet's open the file and read its first lines.","79866319":"We observe that the sentiment distribution is centered to -0.05, which is neutral. It's distribution is quite uniform, except for a peak of neutral lines at 0. This is due to the fact that many lines in this movie contain tokens, like proper nouns (*quidditch*, *weasley*, *voldemort*, *leviosa*...), that don't exist in the twitter model, hence are classified as neutral.","f500cbb3":"We prepare the tokens as dictionaries for the model.","f6033c5a":"We observe that even when no tokens are left for the model to be applied, it still classifies the empty list: the 7th line of Draco Malfoy, *No!*, is a stop word that is discarded. This should be set to 0.5 probability as a default.\n\nLet's copy this dataframe and rescale the value of *sentiment* to better analyse it: *sentiment > 0* if positive, else *sentiment < 0*. We set *sentiment = 0* if the line has no relevant tokens (if the dictionary is empty).","f88f2772":"And apply it.","b8eb51f9":"This seems contradictory, but it is in fact accurate. Petunia and Vernon appear mostly talking to their son, Dudley, and almost all these lines are classified as positive. Let's see it:","ee160bb5":"To add this information in the dataframe *character*, we reset the indices and then set *name* as the indices again. This is just to make the *apply* easier.","f5e50e01":"We apply the tokenizator of nltk.","f64c078d":"With this function, we can easily visualise how many times a character says a word, like 'Harry', 'Potter', 'Weasley', 'Magic', 'Quidditch', 'Football' and so. As can be seen below, only Harry, Hagrid and Hermione dare to pronounce the name of You Know Who.","e54f94f5":"We observe how our beloved Harry, Ron and Hermione fall to positions 28th, 25th and 21st in the *words per line* ranking. This means that, although having the top 3 number of lines, their sentences are short: under 10 words per line.\n\nOn the other hand, Ollivander the wand-maker is #1 in this ranking: over 40 words per line.","e527b65d":"# A philosopher's stone analysis.\n\nThe aim of this project is to make a simple sentiment analysis of *Harry Potter and the philosopher's stone* script. Special thanks to Erin Ward's dataset without which this notebook couldn't have been done.","875495fc":"We observe how the points corresponding to the main characters may deviate from the apparent line the rest of characters follow, specially on the linear scatter. Indeed, Harry, Ron and Hermione's points are further to the right than the rest, meaning that they have more lines, but that their sentences are shorter and so they don't follow the same ratio of words per line like the rest of the characters.\n\nWe observe how taking Harry, Ron and Hermione out of the set of characters, the linear regression is slightly better: the $r^2$ score is 0.95 against the 0.91 obtained when considering the main characters trio for the linear regression.\n\nUsing logarithms, we see an improvement on the 0.91 obtained before: now the $r^2$ score is 0.92. However, discarding Harry, Ron and Hermione makes no difference for the analysis.\n\nIf we had to test these linear models, we should separate the input data into two parts: train and test. We can make a function that, given a (train, test) pair, trains a model and makes a plot of predicted targets vs truth values.","4a8a9115":"Let's check if the *dialogue* column is complete, just in case.","e7cd7ea3":"The test $r^2$ depends on the test sample taken. For this analysis, there are only 41 points, so the deviation can be high. The train $r^2$ is the score obtained from the train sample selected. It can be observed that for this data, the log-log scale is preferable: the points are nicely gathered around the identity line, which makes both $r^2$ scores higher.\n\n","523153ab":"As observed above, maybe the model has some bias with the word *harry* being classified as negative.\n\n# 4. Further analysis.\n\nAnother model coming from different data should be used to better analyse the sentiments through this movie. Also, proper nouns should be discarded from the part of speech classification.","5dfa7a9e":"If we take the top 11 characters with more lines in the movie and sort them with their average sentiment, we observe that *mean* characters like Petunia or Vernon have positive average sentiments whilst *good* characters like Harry or Hagrid have negative average sentiments.","0f3759a7":"We can make another DataFrame, *character*, that contains this information: each character in the movie and how many lines this character has. The first column are the unique values of *script['character_name']* and the second can be taken from the array we've just created, *lines*. We see the top 3 are Harry Potter, Ron Weasley, Hermione Granger. Who would've told?","4fc33ccf":"And make a row for each word (with its corresponding character).","028efb9f":"It's not easy to work with contractions: for instance, what is *I'd*? Depending on the context, it can be *I had* or *I would*. This analysis doesn't take into account this depth of language, so we'll have enough with a simple dictionary of contractions.","72d06277":"As before, we store the total number of words in an array. This time, we call it *words*. We see that Harry's 230 lines add up to 1609 words in the movie."}}