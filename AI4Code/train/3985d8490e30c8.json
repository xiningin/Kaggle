{"cell_type":{"7a9cf6b8":"code","050dbd7f":"code","19c6857d":"code","24d1d210":"code","094efae2":"code","8f3369e4":"code","02ed7822":"code","8e1010d0":"code","eb0f3a9b":"code","2fd15f5a":"code","c82a59ce":"code","51db0758":"code","6cbca571":"code","164b6513":"code","b5356866":"code","17886aaa":"code","8ce2a839":"code","0de5e08d":"code","1620f0e0":"code","5f97129c":"code","af942c9c":"code","88b2d00a":"code","7fd181e7":"code","8e524f89":"code","d1fb3dbc":"code","8df4660e":"code","ccb6be34":"code","75082ee7":"code","df33deab":"code","e2e2e7f5":"code","db5d0dba":"code","e72e1aaf":"code","6e73c7b5":"code","0523495f":"code","8d92915a":"code","cba79ea9":"code","219b1cdf":"code","a12bdfc2":"code","51abe216":"code","ac379878":"code","0fa2b54a":"code","c83c9722":"code","734183a7":"code","0349ed6d":"code","55f562b0":"code","2059d399":"code","bbf238c1":"code","f1453f74":"code","a8d3e74e":"code","539235b1":"markdown","10d873f7":"markdown","e0858bc2":"markdown","5a13c1b9":"markdown","d990e980":"markdown","acfcab64":"markdown","435a489e":"markdown","be3f61f3":"markdown","10c17bef":"markdown","b198f79c":"markdown","bef24608":"markdown","8ed73c88":"markdown","576119ef":"markdown","16edd90b":"markdown","4536dbf7":"markdown","197a05c8":"markdown","6a3cf575":"markdown","77795023":"markdown","a37c824f":"markdown","8598423c":"markdown","3ca1c63b":"markdown","c6697ad6":"markdown","2c6d0900":"markdown","70e5345d":"markdown","6adc3208":"markdown","bcfb6591":"markdown","a0fb9b43":"markdown","dd6f2c2d":"markdown","9759baf5":"markdown","3fc1297a":"markdown","d0af2d83":"markdown","294a0cdb":"markdown","7e781513":"markdown","3dd81cd5":"markdown","7865663e":"markdown","47020ef9":"markdown","ccbb54a8":"markdown","a9f504b2":"markdown","370ca165":"markdown","2ff8e0bb":"markdown","11be0076":"markdown","52960591":"markdown","95751162":"markdown","19940330":"markdown","186b046f":"markdown","4d696fb3":"markdown","019d677f":"markdown","ded5292d":"markdown","3dfb963b":"markdown","38b9fb1e":"markdown","b1d49444":"markdown","0288e79b":"markdown","96ca27cf":"markdown","d55fa7e8":"markdown","6fb238ea":"markdown","f6177e64":"markdown","a0f96823":"markdown","d9fdc454":"markdown","61c35f48":"markdown","26319a70":"markdown","7794b6b1":"markdown","93d573ec":"markdown","99265f0c":"markdown","e2de4b3c":"markdown","9a71fb4d":"markdown","8066a2ad":"markdown","e0a34b6d":"markdown","38175d64":"markdown","0d67c3cf":"markdown","15b610bd":"markdown","e021537b":"markdown"},"source":{"7a9cf6b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","050dbd7f":"df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\nprint(df.head(10))","19c6857d":"df.info()","24d1d210":"Non_fraud = df[df['Class']==0]['Class'].count()\nFraud = df[df['Class']==1]['Class'].count()\nFraud_percent = Fraud\/(Fraud+Non_fraud)\nNon_Fraud_percent = Non_fraud\/(Fraud+Non_fraud)\nprint('Total Number of Non-Fraud Transactions:',Non_fraud)\nprint('Total Number of Fraud Transcations:',Fraud)\nprint('Percentage of Non-Fraud Transactions:',Non_Fraud_percent*100,'%')\nprint('Percentage of Fraud Transactions:',Fraud_percent*100,'%')","094efae2":"import seaborn as sns\n\nsns.countplot(df['Class'],data=df,color='green')","8f3369e4":"import matplotlib.pyplot as plt\n\nplt.figure(figsize = (15,8))\n\nplt.subplot(1,2,1)\n\nsns.distplot(df['Time'],color='blue')\nplt.title('Time Distribution')\nplt.xlim([min(df['Time']),max(df['Time'])])\n\nplt.subplot(1,2,2)\n\nsns.distplot(df['Amount'],color='red')\nplt.title('Amount Distribution')\nplt.xlim([min(df['Amount']),max(df['Amount'])])\n\nplt.show()","02ed7822":"from sklearn.preprocessing import StandardScaler\n\nscal = StandardScaler()\n\ndf['Scaled_Time'] = scal.fit_transform(df['Time'].values.reshape(-1,1))\ndf['Scaled_Amount'] = scal.fit_transform(df['Amount'].values.reshape(-1,1))\n\ndf.drop(['Time','Amount'],axis=1,inplace=True)\ndf.head()","8e1010d0":"plt.figure(figsize=(14,7))\nsns.heatmap(df.corr(),cmap='gnuplot_r')","eb0f3a9b":"print(df.corr()['Class'].sort_values(ascending = False))","2fd15f5a":"X = df.drop(['Class','V4','V2','V21','V19','V20','V8','V27','V28','Scaled_Amount','V26','V25','V22','V23','V15','V13','V24','Scaled_Time','V6','V5','V9','V1','V18'],axis=1)\ny = df['Class']","c82a59ce":"print(X.head())\nprint(y.head())","51db0758":"from imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler(random_state=0,replacement =True)\nrus.fit(X,y)\nX_under_sampled,y_under_sampled = rus.fit_resample(X,y)","6cbca571":"print(\"No. of Non-Fraud Transactions in under sampled data: \",y_under_sampled[y_under_sampled==0].value_counts())\nprint(\"No. of Fraud Transactions in under sampled data: \",y_under_sampled[y_under_sampled==1].value_counts())","164b6513":"from imblearn.under_sampling import NearMiss\n\nnear = NearMiss(sampling_strategy=\"not minority\")\nnear.fit(X,y)\nX_near_sampled,y_near_sampled = near.fit_resample(X,y)","b5356866":"print(\"No. of Non-Fraud Transactions in under sampled data: \",y_near_sampled[y_near_sampled==0].value_counts())\nprint(\"No. of Fraud Transactions in under sampled data: \",y_near_sampled[y_near_sampled==1].value_counts())","17886aaa":"from imblearn.under_sampling import TomekLinks\n\ntomek = TomekLinks(sampling_strategy='auto')\ntomek.fit(X,y)\nX_tomek_sampled,y_tomek_sampled = tomek.fit_resample(X,y)","8ce2a839":"print(\"No. of Non-Fraud Transactions in under sampled data: \",y_tomek_sampled[y_tomek_sampled==0].value_counts())\nprint(\"No. of Fraud Transactions in under sampled data: \",y_tomek_sampled[y_tomek_sampled==1].value_counts())","0de5e08d":"from imblearn.under_sampling import ClusterCentroids\n\nclusters = ClusterCentroids(sampling_strategy='auto',random_state = 1)\nclusters.fit(X,y)\nX_cluster_sampled,y_cluster_sampled = clusters.fit_resample(X,y)","1620f0e0":"print(\"No. of Non-Fraud Transactions in under sampled data: \",y_cluster_sampled[y_cluster_sampled==0].value_counts())\nprint(\"No. of Fraud Transactions in under sampled data: \",y_cluster_sampled[y_cluster_sampled==1].value_counts())","5f97129c":"from imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler(random_state=0)\nros.fit(X,y)\nX_over_sampled,y_over_sampled = ros.fit_resample(X,y)","af942c9c":"print(\"No. of Non-Fraud Transactions in under sampled data: \",y_over_sampled[y_over_sampled==0].value_counts())\nprint(\"No. of Fraud Transactions in under sampled data: \",y_over_sampled[y_over_sampled==1].value_counts())","88b2d00a":"from imblearn.over_sampling import SMOTE\n\nr_smote = SMOTE(random_state =0)\nr_smote.fit(X,y)\nX_smote,y_smote = r_smote.fit_resample(X,y)","7fd181e7":"print(\"No. of Non-Fraud Transactions in under sampled data: \",y_smote[y_smote==0].value_counts())\nprint(\"No. of Fraud Transactions in under sampled data: \",y_smote[y_smote==1].value_counts())","8e524f89":"from imblearn.over_sampling import ADASYN\n\nadasyn = ADASYN(random_state =0,sampling_strategy = 'auto')\nadasyn.fit(X,y)\nX_adasyn,y_adasyn = adasyn.fit_resample(X,y)","d1fb3dbc":"print(\"No. of Non-Fraud Transactions in under sampled data: \",y_adasyn[y_adasyn==0].value_counts())\nprint(\"No. of Fraud Transactions in under sampled data: \",y_adasyn[y_adasyn==1].value_counts())","8df4660e":"from imblearn.combine import SMOTETomek\n\nsmote_tomek = SMOTETomek(random_state =0,sampling_strategy = 'auto')\nsmote_tomek.fit(X,y)\nX_smote_tomek,y_smote_tomek = smote_tomek.fit_resample(X,y)","ccb6be34":"print(\"No. of Non-Fraud Transactions in under sampled data: \",y_smote_tomek[y_smote_tomek==0].value_counts())\nprint(\"No. of Fraud Transactions in under sampled data: \",y_smote_tomek[y_smote_tomek==1].value_counts())","75082ee7":"from sklearn.model_selection import train_test_split\n\nX_under_train,X_under_test,y_under_train,y_under_test = train_test_split(X_under_sampled,y_under_sampled,random_state=0,train_size=0.7)\nX_near_train,X_near_test,y_near_train,y_near_test = train_test_split(X_near_sampled,y_near_sampled,random_state=0,train_size=0.7)\nX_tomek_train,X_tomek_test,y_tomek_train,y_tomek_test = train_test_split(X_tomek_sampled,y_tomek_sampled,random_state=0,train_size=0.7)\nX_cluster_train,X_cluster_test,y_cluster_train,y_cluster_test = train_test_split(X_cluster_sampled,y_cluster_sampled,random_state=0,train_size=0.7)\n","df33deab":"X_over_train,X_over_test,y_over_train,y_over_test = train_test_split(X_over_sampled,y_over_sampled,random_state=0,train_size=0.7)\nX_SMOTE_train,X_SMOTE_test,y_SMOTE_train,y_SMOTE_test = train_test_split(X_smote,y_smote,random_state=0,train_size=0.7)\nX_adasyn_train,X_adasyn_test,y_adasyn_train,y_adasyn_test = train_test_split(X_adasyn,y_adasyn,random_state=0,train_size=0.7)\nX_SMOTETomek_train,X_SMOTETomek_test,y_SMOTETomek_train,y_SMOTETomek_test = train_test_split(X_smote_tomek,y_smote_tomek,random_state=0,train_size=0.7)","e2e2e7f5":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix,classification_report,mean_absolute_error,accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\nrfc = RandomForestClassifier(random_state = 1)","db5d0dba":"param_grid = { \n    'n_estimators': [100,200],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}","e72e1aaf":"CV_rfc_under = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 3)\nCV_rfc_under.fit(X_under_train, y_under_train)","6e73c7b5":"CV_rfc_under.best_params_","0523495f":"rfc_under = RandomForestClassifier(random_state=1, max_features='auto', n_estimators= 100, max_depth=5, criterion='entropy')\nrfc_under.fit(X_under_train, y_under_train)\npred_under = rfc_under.predict(X_under_test)\n\n\nconf_mat = confusion_matrix(y_true=y_under_test, y_pred=pred_under)\nprint('Confusion matrix:\\n', conf_mat)\nlabels = ['Non-Fraud', 'Fraud']\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(conf_mat, cmap=plt.cm.winter)\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","8d92915a":"print(classification_report(y_under_test,pred_under))","cba79ea9":"rfc_near_miss = RandomForestClassifier(random_state=1, max_features='auto', n_estimators= 100, max_depth=5, criterion='entropy')\nrfc_near_miss.fit(X_near_train, y_near_train)\npred_near_miss = rfc_near_miss.predict(X_near_test)\n\n\nconf_mat = confusion_matrix(y_true=y_near_test, y_pred=pred_near_miss)\nprint('Confusion matrix:\\n', conf_mat)\nlabels = ['Non-Fraud', 'Fraud']\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(conf_mat, cmap=plt.cm.winter)\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","219b1cdf":"print(classification_report(y_near_test,pred_near_miss))","a12bdfc2":"rfc_tomek = RandomForestClassifier(random_state=1, max_features='auto', n_estimators= 100, max_depth=5, criterion='entropy')\nrfc_tomek.fit(X_tomek_train, y_tomek_train)\npred_tomek = rfc_tomek.predict(X_tomek_test)\n\n\nconf_mat = confusion_matrix(y_true=y_tomek_test, y_pred=pred_tomek)\nprint('Confusion matrix:\\n', conf_mat)\nlabels = ['Non-Fraud', 'Fraud']\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(conf_mat, cmap=plt.cm.winter)\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","51abe216":"print(classification_report(y_tomek_test,pred_tomek))","ac379878":"rfc_cluster = RandomForestClassifier(random_state=1, max_features='auto', n_estimators= 100, max_depth=5, criterion='entropy')\nrfc_cluster.fit(X_cluster_train, y_cluster_train)\npred_cluster = rfc_cluster.predict(X_cluster_test)\n\n\nconf_mat = confusion_matrix(y_true=y_cluster_test, y_pred=pred_cluster)\nprint('Confusion matrix:\\n', conf_mat)\nlabels = ['Non-Fraud', 'Fraud']\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(conf_mat, cmap=plt.cm.winter)\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","0fa2b54a":"print(classification_report(y_cluster_test,pred_cluster))","c83c9722":"rfc_over = RandomForestClassifier(random_state=1, max_features='log2', n_estimators= 100, max_depth=8, criterion='entropy')\nrfc_over.fit(X_over_train, y_over_train)\npred_over = rfc_over.predict(X_over_test)\n\n\nconf_mat = confusion_matrix(y_true=y_over_test, y_pred=pred_over)\nprint('Confusion matrix:\\n', conf_mat)\nlabels = ['Non-Fraud', 'Fraud']\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(conf_mat, cmap=plt.cm.winter)\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","734183a7":"print(classification_report(y_over_test,pred_over))","0349ed6d":"rfc_smote = RandomForestClassifier(random_state=1, max_features='log2', n_estimators= 100, max_depth=8, criterion='entropy')\nrfc_smote.fit(X_SMOTE_train, y_SMOTE_train)\npred_smote = rfc_smote.predict(X_SMOTE_test)\n\n\nconf_mat = confusion_matrix(y_true=y_SMOTE_test, y_pred=pred_smote)\nprint('Confusion matrix:\\n', conf_mat)\nlabels = ['Non-Fraud', 'Fraud']\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(conf_mat, cmap=plt.cm.winter)\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","55f562b0":"print(classification_report(y_SMOTE_test,pred_smote))","2059d399":"rfc_adasyn = RandomForestClassifier(random_state=1, max_features='log2', n_estimators= 100, max_depth=8, criterion='entropy')\nrfc_adasyn.fit(X_adasyn_train, y_adasyn_train)\npred_adasyn = rfc_adasyn.predict(X_adasyn_test)\n\n\nconf_mat = confusion_matrix(y_true=y_adasyn_test, y_pred=pred_adasyn)\nprint('Confusion matrix:\\n', conf_mat)\nlabels = ['Non-Fraud', 'Fraud']\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(conf_mat, cmap=plt.cm.winter)\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","bbf238c1":"print(classification_report(y_adasyn_test,pred_adasyn))","f1453f74":"rfc_smotetomek = RandomForestClassifier(random_state=1, max_features='log2', n_estimators= 100, max_depth=8, criterion='entropy')\nrfc_smotetomek.fit(X_SMOTETomek_train, y_SMOTETomek_train)\npred_smotetomek = rfc_smotetomek.predict(X_SMOTETomek_test)\n\n\nconf_mat = confusion_matrix(y_true=y_SMOTETomek_test, y_pred=pred_smotetomek)\nprint('Confusion matrix:\\n', conf_mat)\nlabels = ['Non-Fraud', 'Fraud']\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(conf_mat, cmap=plt.cm.winter)\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","a8d3e74e":"print(classification_report(y_SMOTETomek_test,pred_smotetomek))","539235b1":"Near miss undersampling technique undersamples the majority class datapoints. The nearest majority class samples are sampled into dataset and eliminates the bias in the imbalanced dataset. Click the below link to dive deeper.\n\n[https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.under_sampling.NearMiss.html#imblearn.under_sampling.NearMiss](http:imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.under_sampling.NearMiss.html#imblearn.under_sampling.NearMiss)","10d873f7":"There are approximately equal proportion of data in both the classes,by oversampling the minority class.\n\n[https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.over_sampling.ADASYN.html#imblearn.over_sampling.ADASYN](https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.over_sampling.ADASYN.html#imblearn.over_sampling.ADASYN)","e0858bc2":"![](http:\/\/miro.medium.com\/max\/2760\/1*bSOwLuDleEEGiuw7PtooOQ.png)","5a13c1b9":"Print the set of hyperparameters which gives the best result.","d990e980":"# Modelling SMOTE + Tomek Links Combined sample data","acfcab64":"There are double the number of entries in the SMOTE data. SMOTE generates synthetic minority class datapoints, hence avoids the bias in the data (equal Proportion but 284315 entries of both the classes is maintained).","435a489e":"Let's analyse the data before building the model.","be3f61f3":"# SMOTE with Tomek Links (Combination of both Undersampling and Oversampling)","10c17bef":"# Comparison of Sampling Techniques in Credit Card Fraud Detection","b198f79c":"![](https:\/\/miro.medium.com\/max\/875\/1*pR35KsLpz7-_zvbvdm0frg.png)","bef24608":"Tomek links are pairs of very close instances that belong to different classes. They\u2019re samples near the borderline between classes. By removing the examples of the majority class of each pair, we increase the space between the two classes and move toward balancing the dataset by deleting those points.To know more about TomekLinks visit the link below.\n\n[https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.under_sampling.TomekLinks.html#imblearn.under_sampling.TomekLinks](http:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.under_sampling.TomekLinks.html#imblearn.under_sampling.TomekLinks)","8ed73c88":"One of the major issues that novice users fall into when dealing with imbalanced datasets relates to the metrics used to evaluate their model. Using simpler metrics like accuracy_score can be misleading. In a dataset with highly unbalanced classes, if the classifier always \"predicts\" the most common class without performing any analysis of the features, it will still have a high accuracy rate, obviously illusory.","576119ef":"Undersampling gets the ratio of classes equal, by limiting the number of majority classes. This process of choosing the majority class samples into resampled data is done randomly. Thus we get 50-50 ratio of classes, avoiding the bias in the results. To learn more about undersampling you can visit the link below.\n\n[http:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/under_sampling.html](http:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/under_sampling.html)","16edd90b":"I'm not using a GridSearchCV for over sampled data, running gridsearch on oversampled data which contains more than 500000 entries will take a lot of time for execution. So, I'm skipping the gridsearch process for over sampling models.","4536dbf7":"# Tomek Links Under Sampling","197a05c8":"# Near Miss Under Sampling","6a3cf575":"Comparison along with building Random Forest Models for all the sampled data discussed above using GridsearchCV for hyperparameter optimization.","77795023":"We have got an F1-score of **0.92** using random under sampling. \n\n**Note:** Here I used F1-score as performance metric, accuracy is misleading in imbalanced dataset. So, lookout for number of False positives in your model.","a37c824f":"# Random Forest Model for Random Under Sampling","8598423c":"We have got an F1-score of **0.96** using SMOTE + Tomek Links combined sampling.","3ca1c63b":"There are double the number of entries in the oversampled data. Over sampling repeats minority class data points, hence avoids the bias in the data. (Equal Proportion but 284315 entries of both the classes) ","c6697ad6":"# Cluster Centroids Under Sampling","2c6d0900":"# Modelling Near Miss Under sampled data","70e5345d":"# Random Under Sampling Non-Fraud Transactions (Majority Class)","6adc3208":"# Modelling Tomek Links Under sampled data","bcfb6591":"![](https:\/\/miro.medium.com\/max\/875\/1*1XlHmnc9hKn1oPz48lrn7Q.png)","a0fb9b43":"In SMOTE, we generate synthetic data points which is similar to our under sampled data (Fraud Transactions in this case) to overcome the bias. Thus we get 50-50 ratio of classes, avoiding the bias in the results. To learn more about SMOTE you can visit the link below.\n\n[https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/combine.html](http:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/combine.html)\n\n[https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.over_sampling.SMOTE.html#imblearn.over_sampling.SMOTE](http:imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.over_sampling.SMOTE.html#imblearn.over_sampling.SMOTE)","dd6f2c2d":"The main purpose of the notebook is to learn sampling techniques to handle imbalanced dataset. Many beginners have never sampled data of imbalanced dataset and got an accuracy of 98% or more (considering only accuracy as performance metric) and think it's perfect. But many fail to notice the False Positive rates they got as their result. In this notebook, we discuss about \n\ni)   Random Under Sampling.\n\nii)  Near Miss Under Sampling.\n\niii) Tomek Links Under Sampling.\n\niv)  Cluster Centroids Under Sampling.\n\nv)  Random Over Sampling.\n\nvi)   SMOTE. (Synthetic Minority OverSampling Technique)\n\nvii)  ADASYN. (Adaptive Synthetic Over Sampling)\n\nviii) SMOTE & Tomek Links. (Combination of Under sampling and Over sampling)\n\n\nA comparison of these sampling techniques and it's explanation using Credit Card Fraud Detection.\n","9759baf5":"# Sampling results comparison\nNow let's compare the results of these sampling techniques.\nSo, first splitting the data into training and testing set for the 8 different set of data samples.","3fc1297a":"We can see from the heatmap, that there is no solid evidence of features with high correlations. We drop the features with less correlations i.e) close to zero from either side. We will consider only highly correlated features both on the positive as well as negative side.","d0af2d83":"There are only 984 entries after undersampling. Let's move to Over sampling techniques and understand the difference between Under sampling techniques and Over sampling techniques.","294a0cdb":"We have got an F1-score of **0.96** using SMOTE over sampling.","7e781513":"**Note:** OverSampling repeats the original minority class data points to make the proportion of classes equal (increases sample size only by repetiton) whereas SMOTE generates new synthetic data points of minority class. ","3dd81cd5":"Building a random forest model.","7865663e":"There are only 984 entries in the under sampled data with equal proportion (492 entries of both the classes). So, undersampling helps to overcome the bias of the dataset.","47020ef9":"First, splitting undersampling data.","ccbb54a8":"Splitting the oversampling data.","a9f504b2":"We have got an F1-score of **0.98** using random over sampling.","370ca165":"Building a model with best set of hyperparameters using GridsearchCV.\n\nCheck out GridsearchCV documentation to dig deeper. Click the link below.\n\n[https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html](http:\/\/https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html)","2ff8e0bb":"Importing the dataset into Pandas Dataframe.","11be0076":"# Random Forest Model for Random Over Sampling","52960591":"We have got an F1-score of **1.00** using Tomek Links under sampling.","95751162":"\n[https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.combine.SMOTETomek.html#imblearn.combine.SMOTETomek](https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.combine.SMOTETomek.html#imblearn.combine.SMOTETomek)","19940330":"Now we have to list a set of hyperparameters for grid search to optimize hyperparamters.","186b046f":"In Over Sampling, we produce new data points of the minority class with replacement in the dataset. Thus we get 50-50 ratio of classes, avoiding the bias in the results. To learn more about oversampling you can visit the link below.\n\n[https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/over_sampling.html](https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/over_sampling.html)","4d696fb3":"We have got an F1-score of **0.93** using ADASYN over sampling.","019d677f":"We have got an F1-score of **0.92** using Cluster Centroid under sampling.","ded5292d":"\n![](https:\/\/miro.medium.com\/max\/2246\/1*gHW_PLz7kWrhdl5t1sJRRA.png)","3dfb963b":"Cluster centroids is similar to K-means clustering, it uses clusters to under sample majority class data as shown in the figure. You can see the documentation in the link.\n\n[https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.under_sampling.ClusterCentroids.html#imblearn.under_sampling.ClusterCentroids](https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.under_sampling.ClusterCentroids.html#imblearn.under_sampling.ClusterCentroids)","38b9fb1e":"There are no missing values in the dataset. There's no need for imputation. Next, we will explore the Class (Fraud or Not Fraud).","b1d49444":"To learn more about combination of undersampling and oversampling techniques, you can check imblearn.combine documentation for combination of sampling techniques.","0288e79b":"There are 31 features in the dataset. Except amount and time, all the others features are scaled using PCA (given in the dataset description).Checking for any missing values in the dataset for imputation.","96ca27cf":"ADASYN generates samples of the minority class. But here, because of their density distributions, this technique receives wide attention. It's purpose is to generate data for minority class samples that are harder to learn, as compared to those minority samples that are easier to learn.","d55fa7e8":"# Modelling ADASYN Over sampled data","6fb238ea":"We have got an F1-score of **0.95** using near miss under sampling.","f6177e64":"# ADASYN (Adaptive Synthetic Sampling)","a0f96823":"![](https:\/\/miro.medium.com\/max\/875\/1*8WM0gsh_naPEa9HTpE2c1A.png)","d9fdc454":"**Note:** Here I dropped Scaled_Amount and Scaled_Time after scaling time and amount which may seem useless, but scaling is done prior to analysis. If these features have a significant correlation, then these features would have been included in the model (require scaling before including in the model).","61c35f48":"# Modelling Cluster Centroid Under sampling data","26319a70":"# Summary","7794b6b1":"# Random Over Sampling Fraud Transactions (Minority Class)","93d573ec":"**Note:** I'm a newbie to kaggle. Any suggestions about the notebook is most welcome!!","99265f0c":"The time and Amount is not normally distributed. So before any analysis, we have to scale the data and transform. Scaling\/Transformation is only done prior to model building (before trail and error) to avoid errors while trying out Distance based algorithms.","e2de4b3c":"There are only 984 entries in the near miss under sampled data with equal proportion (492 entries of both the classes). So, near miss undersampling helps to overcome the bias of the dataset which is more efficient than random under sampling.","9a71fb4d":"You can able to notice that the classes are still biased by a big margin. This sampling fails for largely biased data. But still using it as a case for comparison.","8066a2ad":"Even a classifier with 99.83% accuracy,will wrongly classify the remaining 0.17% fraud transaction as Non-fraud Transaction simply because the model is trained with Non-fraud transactions than Fraud Transactions (High bias in the Class). So it's very crucial to classify Fraud transactions correctly. ","e0a34b6d":"We can combine under sampling and over sampling techniques to improve our results. A combination of SMOTE and Tomek Links gives better results than individual sampling techniques.","38175d64":"# SMOTE (Synthetic Minority OverSampling Technique)","0d67c3cf":"We have seen different types of resampling techniques used to eliminate bias in imbalanced dataset.\n\nI'm not going to state some sampling technique as the best using the results inferred above (also I have not used Gridsearch for over sampling and combined sampling model).\n\nEach model has it's own advantage on different dataset\/distributions.","15b610bd":"# Modelling SMOTE Over sampled data","e021537b":"![](https:\/\/miro.medium.com\/max\/2246\/1*o_KfyMzF7LITK2DlYm_wHw.png)"}}