{"cell_type":{"83655299":"code","ac331c39":"code","b0067a0d":"code","7ffa69fe":"code","4d640a10":"code","e27f9275":"code","1822da61":"code","088ffa7e":"code","d61553a2":"code","6f26fc32":"code","b55654c6":"code","26ce3836":"code","11957d80":"code","41cac9a2":"code","f679ac23":"code","b2327624":"code","c117620f":"code","45a41acb":"code","ab9bb4a3":"code","146989ed":"code","59ae3504":"code","8e3d3b0f":"markdown","cba211ec":"markdown","a31c1536":"markdown","f7ce44d7":"markdown","51e2a5a6":"markdown","1e8fa3ae":"markdown","a1b82069":"markdown"},"source":{"83655299":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport keras\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ac331c39":"data = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv(\"..\/input\/test.csv\")\ndata = data.sample(frac=1).reset_index(drop=True)\nY_train = data['label']\nX_train = data.drop(labels = [\"label\"],axis = 1) \n\nX_train = X_train.values.reshape(-1, 28, 28, 1)\nX_test = test.values.reshape(-1, 28, 28, 1)\n\n\nX_train, X_dev, Y_train, Y_dev = train_test_split(X_train, Y_train, test_size = 0.1)\n\nX_test = X_test \/ 255.0\nX_train = X_train \/ 255.0\nX_dev = X_dev \/ 255.0\n\nY_dev = keras.utils.to_categorical(Y_dev, num_classes = 10)\nY_train = keras.utils.to_categorical(Y_train, num_classes = 10)\n\ndel data\ndel test","b0067a0d":"train_gen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\ntrain_gen.fit(X_train)","7ffa69fe":"def plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize=(20,10))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","4d640a10":"def leNet5(X_in, filters=(32, 64), neurons=(256, 128)):\n    \n    F1, F2 = filters #6, 16\n    N1, N2 = neurons #120, 84\n    \n    X = keras.layers.Conv2D(filters=F2, kernel_size=(5,5), strides=(1,1), padding='valid', kernel_regularizer=keras.regularizers.l2(0.05))(X_in)\n    X = keras.layers.BatchNormalization(axis=3)(X)\n    X = keras.layers.Activation('relu')(X)\n    \n    X = keras.layers.MaxPool2D(pool_size=(2,2))(X)\n    \n    X = keras.layers.Conv2D(filters=F1, kernel_size=(5,5), strides=(1,1), padding='valid', kernel_regularizer=keras.regularizers.l2(0.03))(X)\n    X = keras.layers.BatchNormalization(axis=3)(X)\n    X = keras.layers.Activation('relu')(X)\n    \n    X = keras.layers.MaxPool2D(pool_size=(2,2))(X)\n    \n    X = keras.layers.Flatten()(X)\n    \n    X = keras.layers.Dense(N1, kernel_regularizer=keras.regularizers.l2(0.02))(X)\n    X = keras.layers.BatchNormalization(axis=1)(X)\n    X = keras.layers.Activation('relu')(X)\n    \n    X = keras.layers.Dense(N2, kernel_regularizer=keras.regularizers.l2(0.01))(X)\n    X = keras.layers.BatchNormalization(axis=1)(X)\n    X = keras.layers.Activation('relu')(X)\n    \n    model = keras.models.Model(inputs = X_in, outputs = X, name='LeNet-5-part')\n    \n    return model","e27f9275":"X_in = keras.layers.Input(X_train[0].shape)\n\nmodel_part = leNet5(X_in)\n\nX = keras.layers.Dense(10, activation='softmax')(model_part.outputs[0])\n\nmodel_full = keras.models.Model(inputs = X_in, outputs = X, name='LeNet-5')\n\nmodel_full.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","1822da61":"reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=2, min_lr=0.00001)\nhistory1 = model_full.fit_generator(train_gen.flow(X_train,Y_train, batch_size=128),\n                                epochs = 30, validation_data = (X_dev,Y_dev), \n                                steps_per_epoch=512, callbacks=[reduce_lr])","088ffa7e":"plt.figure(figsize=(20,10))\nplt.plot(history1.history['acc'])\nplt.plot(history1.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","d61553a2":"Y_pred = model_full.predict(X_dev)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(Y_dev,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10)) ","6f26fc32":"def inception(X_in, filters=(32, 32), neurons=(256, 128)):\n    \n    def create_branch(X, fs, size, strides):\n        X = keras.layers.Conv2D(filters=fs, kernel_size=size, strides=strides, padding='same', kernel_regularizer=keras.regularizers.l2(0.05))(X)\n        X = keras.layers.BatchNormalization(axis=3)(X)\n        X = keras.layers.Activation('relu')(X)\n        return X\n    \n    def create_layer(X, filters_1, filters_2):\n        X = keras.layers.concatenate([\n            create_branch(X, filters_1, (3,3), (1,1)),\n            create_branch(X, filters_1, (5,5), (1,1)),\n            create_branch(X, filters_1, (7,7), (1,1))\n        ])\n        X = keras.layers.Conv2D(filters=filters_2, kernel_size=(1,1), strides=(1,1), padding='same', kernel_regularizer=keras.regularizers.l2(0.03))(X)\n        X = keras.layers.BatchNormalization(axis=3)(X)\n        X = keras.layers.Activation('relu')(X)\n        return X\n    \n    F1, F2 = filters #8, 16\n    N1, N2 = neurons #120, 84\n    \n    X = create_layer(X_in, F1, F2)\n    X = create_layer(X, F1, F2)\n    X = create_layer(X, F1, F2)\n    X = keras.layers.MaxPool2D(pool_size=(2,2))(X)\n    \n    X = create_layer(X, F1, F2)\n    X = keras.layers.MaxPool2D(pool_size=(2,2))(X)\n    X = keras.layers.Conv2D(filters=1024, kernel_size=(7,7), strides=(1,1), padding='valid', kernel_regularizer=keras.regularizers.l2(0.01))(X)\n    X = keras.layers.Flatten()(X)\n    \n    X = keras.layers.Dense(N1, kernel_regularizer=keras.regularizers.l2(0.02))(X)\n    X = keras.layers.BatchNormalization(axis=1)(X)\n    X = keras.layers.Activation('relu')(X)\n    \n    X = keras.layers.Dense(N2, kernel_regularizer=keras.regularizers.l2(0.01))(X)\n    X = keras.layers.BatchNormalization(axis=1)(X)\n    X = keras.layers.Activation('relu')(X)\n    \n    model = keras.models.Model(inputs = X_in, outputs = X, name='Inception-part')\n    \n    return model","b55654c6":"X_in = keras.layers.Input(X_train[0].shape)\n\nmodel_2_part = inception(X_in)\n\nX = keras.layers.Dense(10, activation='softmax')(model_2_part.outputs[0])\n\nmodel_2_full = keras.models.Model(inputs = X_in, outputs = X, name='Inception')\n\nmodel_2_full.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","26ce3836":"history2 = model_2_full.fit_generator(train_gen.flow(X_train,Y_train, batch_size=128),\n                                epochs = 30, validation_data = (X_dev,Y_dev), \n                                steps_per_epoch=512, callbacks=[reduce_lr])","11957d80":"plt.figure(figsize=(20,10))\nplt.plot(history2.history['acc'])\nplt.plot(history2.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","41cac9a2":"Y_pred = model_2_full.predict(X_dev)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(Y_dev,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10)) ","f679ac23":"def custom(X_in, filters=(32, 64), neurons=(256, 128)):\n    \n    F1, F2 = filters #6, 16\n    N1, N2 = neurons #120, 84\n    \n    X = keras.layers.Conv2D(filters=F1, kernel_size=(5,5), strides=(1,1), padding='valid', kernel_regularizer=keras.regularizers.l2(0.01))(X_in)\n    X = keras.layers.BatchNormalization(axis=3)(X)\n    X = keras.layers.Activation('relu')(X)\n    \n    X = keras.layers.Conv2D(filters=F2, kernel_size=(5,5), strides=(1,1), padding='valid', kernel_regularizer=keras.regularizers.l2(0.01))(X)\n    X = keras.layers.BatchNormalization(axis=3)(X)\n    X = keras.layers.Activation('relu')(X)\n    \n    X = keras.layers.Conv2D(filters=F1, kernel_size=(5,5), strides=(1,1), padding='valid', kernel_regularizer=keras.regularizers.l2(0.01))(X_in)\n    X = keras.layers.BatchNormalization(axis=3)(X)\n    X = keras.layers.Activation('relu')(X)\n    \n    X = keras.layers.Conv2D(filters=F2, kernel_size=(3,3), strides=(1,1), padding='valid', kernel_regularizer=keras.regularizers.l2(0.01))(X)\n    X = keras.layers.BatchNormalization(axis=3)(X)\n    X = keras.layers.Activation('relu')(X)\n    \n    X = keras.layers.MaxPool2D(pool_size=(2,2), padding='same')(X)\n    \n    X = keras.layers.Conv2D(filters=1024, kernel_size=(7,7), strides=(1,1), padding='valid', kernel_regularizer=keras.regularizers.l2(0.01))(X)\n    X = keras.layers.BatchNormalization(axis=3)(X)\n    X = keras.layers.Activation('relu')(X)\n    \n    X = keras.layers.Flatten()(X)\n    \n    X = keras.layers.Dense(N1, kernel_regularizer=keras.regularizers.l2(0.01))(X)\n    X = keras.layers.BatchNormalization(axis=1)(X)\n    X = keras.layers.Activation('relu')(X)\n    \n    X = keras.layers.Dense(N2, kernel_regularizer=keras.regularizers.l2(0.01))(X)\n    X = keras.layers.BatchNormalization(axis=1)(X)\n    X = keras.layers.Activation('relu')(X)\n    \n    model = keras.models.Model(inputs = X_in, outputs = X, name='LeNet-5-part')\n    \n    return model","b2327624":"X_in = keras.layers.Input(X_train[0].shape)\n\nmodel_3_part = custom(X_in)\n\nX = keras.layers.Dense(10, activation='softmax')(model_3_part.outputs[0])\n\nmodel_3_full = keras.models.Model(inputs = X_in, outputs = X, name='Custom')\n\nmodel_3_full.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","c117620f":"history3 = model_3_full.fit_generator(train_gen.flow(X_train,Y_train, batch_size=128),\n                                epochs = 30, validation_data = (X_dev,Y_dev), \n                                steps_per_epoch=512, callbacks=[reduce_lr])","45a41acb":"Y_pred = model_3_full.predict(X_dev)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(Y_dev,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10)) ","ab9bb4a3":"plt.figure(figsize=(20,10))\nplt.plot(history3.history['acc'])\nplt.plot(history3.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","146989ed":"Y_pred = (model_2_full.predict(X_dev) + model_full.predict(X_dev) + model_3_full.predict(X_dev)) \/ 3\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(Y_dev,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10)) ","59ae3504":"def submit_results(model, file_name, test): \n    results = model.predict(test)\n    results = np.argmax(results, axis = 1)\n    results = pd.Series(results, name=\"Label\")\n    submission = pd.concat([pd.Series(range(1,28001), name = \"ImageId\"),results],axis = 1)\n    submission.to_csv(file_name, index=False)\n    \ndef ensamble_submission(model1, model2, model3, test): \n    results = (model1.predict(test) + model2.predict(test) + model3.predict(test)) \/ 2\n    results = np.argmax(results, axis = 1)\n    results = pd.Series(results, name=\"Label\")\n    submission = pd.concat([pd.Series(range(1,28001), name = \"ImageId\"),results],axis = 1)\n    submission.to_csv(\"ensamble_submission.csv\", index=False)\n    \nsubmit_results(model_full, \"lenet-5-submission.csv\", X_test)\nsubmit_results(model_2_full, \"inception-submission.csv\", X_test)\nensamble_submission(model_full, model_2_full, model_3_full, X_test)","8e3d3b0f":"## Loading and preprocessing train, dev and test data","cba211ec":"## Ensamble models","a31c1536":"## Custom CNN - Inception style CNN","f7ce44d7":"\n## Classic CNN - LeNet-5\nIs most classic CCN network for hand written digits recognition.\n\nOriginal LeNet-5 network as an input takes 32x32x1 images and consists of 3 types of building blocks:\n- Convolutional layer block (Conv2D) with filter size 5x5, stride 1x1, padding valid\n- Avg Pooling layer block (AvgPool) with window size 2x2, stride 2x2\n- Fully Connected layer block (FC)  \n\nNetwork looks as follows:\n\nInput -> Conv2D 6 (filters) -> AvgPool -> Conv2D 16 -> AvgPool -> FC 120 -> FC 84 -> Softmax 10\n\nBelow is implemetation of this model with one change, instead of AvgPool block I will use MaxPool block with the same parameters.\n","51e2a5a6":"## Submission","1e8fa3ae":"## Data augmentation","a1b82069":"## Custom CNN"}}