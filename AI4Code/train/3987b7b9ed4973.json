{"cell_type":{"41a6bb6d":"code","1b5a5566":"code","12911e86":"code","73cd27a5":"code","702e87f7":"code","7f4c25db":"code","78324a3f":"code","f8c94897":"code","5e08301e":"code","9ea84055":"code","041f7964":"code","30ff1924":"code","ee44cef9":"code","da207b13":"code","bf6d68fd":"code","a5d4e674":"code","c9c3c058":"code","9fb1232a":"code","0f37b8de":"code","e37d740e":"code","b2b5b506":"markdown"},"source":{"41a6bb6d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1b5a5566":"# loading data\ndata = pd.read_csv('\/kaggle\/input\/breastcancer.csv')\ndata.head()","12911e86":"# data information\ndata.info()","73cd27a5":"# checking for missing values\ndata.isna().sum()","702e87f7":"# to check number of attributes\ndata.columns","7f4c25db":"# drop unneccesary attributes\ndata.drop(columns= ['id','Unnamed: 32'], axis = 1 , inplace = True)\ndata.columns","78324a3f":"import matplotlib.pyplot as plt\nimport seaborn as sns\n# checking data distributions\ndata.select_dtypes(include= np.float).hist(figsize = (15,15))\nplt.tight_layout()\nplt.show()","f8c94897":"# pairplot for scatter and hists based on diagnosis\nsns.pairplot(data, hue = 'diagnosis')\nplt.tight_layout()\nplt.show()","5e08301e":"# checking for class balance with bar chart\ndata['diagnosis'].value_counts().plot(kind = 'bar')\nplt.show()","9ea84055":"# checking unique values in target class\ndata['diagnosis'].unique()","041f7964":"# converting target class into binary values \ndata['diagnosis'] = data['diagnosis'].apply(lambda x : 1 if x == 'M' else 0)\ndata['diagnosis'].unique()","30ff1924":"# split the data into X and y\nX = data.iloc[:,1:].values\ny = data.iloc[:, 0].values","ee44cef9":"# train and test split of data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 20)","da207b13":"# scaling the data\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","bf6d68fd":"# model building and necessary packages \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom xgboost import XGBRFClassifier","a5d4e674":"# calling all classification models\nclassication_lr = LogisticRegression()\nclassication_lr.fit(X_train,y_train)\n\nclassication_dt = DecisionTreeClassifier()\nclassication_dt.fit(X_train,y_train)\n\nclassication_rf = RandomForestClassifier()\nclassication_rf.fit(X_train,y_train)\n\nclassication_svc = SVC()\nclassication_svc.fit(X_train,y_train)\n\nclassication_knn = KNeighborsClassifier()\nclassication_knn.fit(X_train,y_train)\n\nclassication_xgb = XGBClassifier()\nclassication_xgb.fit(X_train,y_train)\n\nclassication_xgbrf = XGBRFClassifier()\nclassication_xgbrf.fit(X_train,y_train)","c9c3c058":"# each classification model score\nprint('training score : {}, test score : {}'.format(classication_lr.score(X_train, y_train), classication_lr.score(X_test, y_test)))\nprint('training score : {}, test score : {}'.format(classication_dt.score(X_train, y_train), classication_dt.score(X_test, y_test)))\nprint('training score : {}, test score : {}'.format(classication_rf.score(X_train, y_train), classication_rf.score(X_test, y_test)))\nprint('training score : {}, test score : {}'.format(classication_svc.score(X_train, y_train), classication_svc.score(X_test, y_test)))\nprint('training score : {}, test score : {}'.format(classication_knn.score(X_train, y_train), classication_knn.score(X_test, y_test)))\nprint('training score : {}, test score : {}'.format(classication_xgb.score(X_train, y_train), classication_xgb.score(X_test, y_test)))\nprint('training score : {}, test score : {}'.format(classication_xgbrf.score(X_train, y_train), classication_xgbrf.score(X_test, y_test)))\n","9fb1232a":"# getting predictions for all models \ny_pred_lr = classication_lr.predict(X_test)\ny_pred_dt = classication_dt.predict(X_test)\ny_pred_rf = classication_rf.predict(X_test)\ny_pred_svc = classication_svc.predict(X_test)\ny_pred_knn = classication_knn.predict(X_test)\ny_pred_xgb = classication_xgb.predict(X_test)\ny_pred_xgbrf = classication_xgbrf.predict(X_test)\n","0f37b8de":"# error metrics for classification problem\nfrom sklearn.metrics import confusion_matrix, classification_report\ncm_lr = confusion_matrix(y_test, y_pred_lr)\nprint('logistic reg: {}'.format(cm_lr), '\\n')\n\ncm_dt = confusion_matrix(y_test, y_pred_dt)\nprint('decison tree classif: {}'.format(cm_dt), '\\n')\n\ncm_rf = confusion_matrix(y_test, y_pred_rf)\nprint('randomforest classif: {}'.format(cm_rf), '\\n')\n\ncm_svc = confusion_matrix(y_test, y_pred_svc)\nprint('svc classif: {}'.format(cm_svc), '\\n')\n\ncm_knn = confusion_matrix(y_test, y_pred_knn)\nprint('knn classif: {}'.format(cm_knn), '\\n')\n\ncm_xgb = confusion_matrix(y_test, y_pred_xgb)\nprint('xgboost tree classif: {}'.format(cm_xgb), '\\n')\n\ncm_xgbrf = confusion_matrix(y_test, y_pred_xgbrf)\nprint('xgboost randomforest classif: {}'.format(cm_xgbrf))","e37d740e":"# classification report by each model\nprint('---------- logistic reg', '\\n')\nprint(classification_report(y_test, y_pred_lr),'\\n')\nprint('---------- decison tree', '\\n')\nprint(classification_report(y_test, y_pred_dt),'\\n')\nprint('---------- randomforest', '\\n')\nprint(classification_report(y_test, y_pred_rf),'\\n')\nprint('---------- svc', '\\n')\nprint(classification_report(y_test, y_pred_svc),'\\n')\nprint('---------- knn', '\\n')\nprint(classification_report(y_test, y_pred_knn),'\\n')\nprint('---------- xgboost', '\\n')\nprint(classification_report(y_test, y_pred_xgb),'\\n')\nprint('---------- xgboost with randomforest', '\\n')\nprint(classification_report(y_test, y_pred_xgbrf))","b2b5b506":"### Objective is to classify whether the tumor malignant or benign (M -malignantor : B - benign)\n- Dataset : https:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data"}}