{"cell_type":{"6886f773":"code","7b6f2107":"code","7e2b4125":"code","ad4ea98a":"code","60529117":"code","fff4cdcf":"code","9417e0ea":"code","3d44bcbe":"code","123d8de8":"code","3fb2b45d":"code","ea6216ec":"code","df17f594":"code","11384f85":"code","f2483a94":"code","9fc9ea45":"code","c84acedf":"code","66e4259b":"code","31121468":"code","bfdb7d71":"code","64f4ca80":"code","647cb90a":"code","d130402e":"code","66b91013":"code","1b77a32e":"code","42b3021b":"code","921292a2":"code","154bbe63":"code","29b6b596":"code","728938a6":"code","d0166fa7":"code","74479705":"code","aa7d44ed":"code","0b1eb657":"code","b0a25ee5":"code","a66720f1":"code","70d96bd3":"code","425cea45":"code","51ca9bac":"code","2127b5de":"code","c46516d6":"code","d2339b46":"code","905780f7":"code","7de6ef0b":"code","62a2fa12":"code","86108d95":"code","39fac0d0":"code","2f7ac8b8":"code","1f37b8c9":"code","0d572f7c":"code","ab903280":"code","4b32c38a":"code","381876f2":"code","30cdc01d":"code","2d09defe":"code","aad4d5c1":"code","7e2cc348":"code","28e3a3ff":"code","8ba94e23":"code","93e3babb":"code","4b52491f":"code","b621712c":"code","4b5da319":"code","6a23f88c":"code","5d922cea":"markdown","6a0b0864":"markdown","6564266c":"markdown","230ed414":"markdown","1466f851":"markdown","7f115e2b":"markdown","0c9bc8b2":"markdown","20df77a4":"markdown","1a07d2e7":"markdown","0da66e68":"markdown","afdf2291":"markdown","1ffdaaf8":"markdown","eae11cef":"markdown","73ff8ef0":"markdown","98b193c4":"markdown","f3472027":"markdown","4c0c6553":"markdown","12c736cc":"markdown","e08a6093":"markdown","4b0e957d":"markdown","caac48af":"markdown","59a6ed21":"markdown","1e0b5869":"markdown","7d8b5c3e":"markdown","3ca60a0b":"markdown","a3e1d6c7":"markdown"},"source":{"6886f773":"import numpy as np # linear algebra\nimport pandas as pd\nimport matplotlib.colors\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\nfrom matplotlib.patches import Rectangle\nimport seaborn as sns\nimport openslide\nimport skimage.io\nimport cv2\nimport fastai\nfrom fastai.vision import *\n\nfrom IPython.display import Image, display\n\n# Plotly for the interactive viewer (see last section)\nimport plotly.graph_objs as go\nimport os\nimport torch\nimport matplotlib.pyplot as plt\nfrom fastai.metrics import KappaScore\n%matplotlib inline \nimport warnings\nwarnings.filterwarnings('ignore')\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nbs = 2\nN = 12\nnworkers = 2\n","7b6f2107":"\nMODELS = [f'..\/input\/panda-starter-models\/RNXT50_{i}.pth' for i in range(4)]\n\nsz = 128","7e2b4125":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ad4ea98a":"BASE_PATH='..\/input\/prostate-cancer-grade-assessment\/'\nDATA = BASE_PATH+'test_images'\nSAMPLE = BASE_PATH+'sample_submission.csv'\nTRAIN=BASE_PATH+'train.csv'\nTEST = BASE_PATH+'test.csv'","60529117":"sys.path.insert( 0,'..\/input\/semisupervised-imagenet-models\/semi-supervised-ImageNet1K-models-master\/')\nfrom hubconf import *","fff4cdcf":"# image and mask directories\ndata_dir = f'{BASE_PATH}\/train_images'\nmask_dir = f'{BASE_PATH}\/train_label_masks'","9417e0ea":"# Location of training labels\ntrain = pd.read_csv(f'{BASE_PATH}\/train.csv')\ntest = pd.read_csv(f'{BASE_PATH}\/test.csv')\nsubmission = pd.read_csv(f'{BASE_PATH}\/sample_submission.csv')","3d44bcbe":"\ndisplay(train.head())\ndisplay(train.shape)","123d8de8":"train.info()","3fb2b45d":"train.data_provider.unique()","ea6216ec":"display(len(train.data_provider.unique()))","df17f594":"\ndisplay(test.head())\ndisplay(test.shape)\n                 ","11384f85":"test.info()","f2483a94":"train_image_list=os.listdir(os.path.join(BASE_PATH,'train_images'))\ntrain_label_masks_list=os.listdir(os.path.join(BASE_PATH,'train_label_masks'))","9fc9ea45":"train_image_list","c84acedf":"print(f\"train image_id list:{train.image_id.nunique()}\")","66e4259b":"\nprint(f\"train image list:{len(train_image_list)}\")\nprint(f\"train label masks list:{len(train_label_masks_list)}\")","31121468":"print(f\"sample of image_id list:{train.image_id.values[0:3]}\")\nprint(f\"sample of image list:{train_image_list[0:3]}\")\nprint(f\"sample of label masks list:{train_label_masks_list[0:3]}\")","bfdb7d71":"trimmed_image_list=[]\nfor img in train_image_list:\n    trimmed_image_list.append(img.split('.tiff')[0])","64f4ca80":"trimmed_label_masks_list=[]\nfor img in train_label_masks_list:\n    trimmed_label_masks_list.append(img.split('_mask.tiff')[0])","647cb90a":"intersect_i_m=(set(trimmed_image_list) & set(trimmed_label_masks_list))\nintersect_id_m=(set(train.image_id.unique()) & set(trimmed_label_masks_list))\nintersect_id_i=(set(train.image_id.unique()) & set(trimmed_image_list))\nprint(f\"image(tiff) & label masks:{len(intersect_i_m)}\")\nprint(f\"image_id(train) & label masks:{len(intersect_id_m)}\")\nprint(f\"image_id(train) & image(tiff):{len(intersect_id_i)}\")","d130402e":"missing_masks=np.setdiff1d(trimmed_image_list,trimmed_label_masks_list)\nprint(f'missing masks:{len(missing_masks)} images(press output button to see the list)')","66b91013":"print(list(missing_masks))","1b77a32e":"sub=pd.read_csv(SAMPLE)\nsub.head()","42b3021b":"masks=os.listdir(BASE_PATH+'train_label_masks\/')\nimages=os.listdir(BASE_PATH+'train_images\/')\ndf_masks=pd.Series(masks).to_frame()\ndf_masks.columns=['mask_file_name']\ndf_masks['image_id']=df_masks.mask_file_name.apply(lambda x:x.split('_')[0])\ndf_train=pd.merge(train,df_masks,on='image_id',how='outer')\ndel df_masks","921292a2":"import seaborn as sns\ndef plot_count(df,feature,title='',size=2):\n    f,ax=plt.subplots(1,1,figsize=(3*size,2*size))\n    total=float(len(df))\n    sns.countplot(df[feature],order=df[feature].value_counts().index,palette='Set3')\n    plt.title(title)\n    for p in ax.patches:\n        height=p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2,height+3,'{:1.2f}%'.format(100*height\/total),ha=\"center\")\n    plt.show()","154bbe63":"plot_count(train,'data_provider','Data provider-data count and percent')","29b6b596":"plot_count(train,'isup_grade','ISUP grade - data count and percent',size=3)","728938a6":"plot_count(train,'gleason_score','Gleason score -data count and percent',size=3)","d0166fa7":"fig,ax=plt.subplots(nrows=1,figsize=(12,6))\ntmp=train.groupby('isup_grade')['gleason_score'].value_counts()\ndf=pd.DataFrame(data={'Exams':tmp.values},index=tmp.index).reset_index()\nsns.barplot(ax=ax,x='isup_grade',y='Exams',hue='gleason_score',data=df,palette='Set1')\nplt.title(\"Number of examinations grouped on ISUP grade and Gleason Score\")\nplt.show()","74479705":"fig,ax=plt.subplots(nrows=1,figsize=(8,8))\nheatmap_data=pd.pivot_table(df,values='Exams',index=['isup_grade'],columns='gleason_score')\nsns.heatmap(heatmap_data,cmap='YlGnBu',linewidth=0.5,linecolor='Red')\nplt.title('Number of examination grouped on ISUP grade and gleason score')\nplt.show()","aa7d44ed":"from IPython.display import HTML, display\n\ndata = [[\"Gleason Score\", \"ISUP Grade\"],\n        [\"0+0\", \"0\"], [\"negative\", \"0\"],\n        [\"3+3\", \"1\"], [\"3+4\", \"2\"], [\"4+3\", \"3\"], \n        [\"4+4\", \"4\"], [\"3+5\", \"4\"], [\"5+3\", \"4\"],\n        [\"4+5\", \"5\"], [\"5+4\", \"5\"], [\"5+5\", \"5\"],\n        ]\n\ndisplay(HTML(\n   '<table><tr>{}<\/tr><\/table>'.format(\n       '<\/tr><tr>'.join(\n           '<td>{}<\/td>'.format('<\/td><td>'.join(str(_) for _ in row)) for row in data)\n      )\n))\n","0b1eb657":"fig, ax = plt.subplots(nrows=1,figsize=(12,6)) \ntmp = train.groupby('data_provider')['gleason_score'].value_counts() \ndf = pd.DataFrame(data={'Exams': tmp.values}, index=tmp.index).reset_index() \nsns.barplot(ax=ax,x = 'data_provider', y='Exams',hue='gleason_score',data=df, palette='Set1') \nplt.title(\"Number of examinations grouped on Data provider and Gleason score\") \nplt.show()","b0a25ee5":"df_train[(df_train.isup_grade==2)&(df_train.gleason_score !='3+4')]","a66720f1":"data_providers=df_train.data_provider.unique()\nfig=plt.figure(figsize=(6,4))\nax=sns.countplot(x=\"isup_grade\",hue=\"data_provider\",data=df_train)\nplt.title(\"ISUP Grade Count by Data Provider\",fontsize=14)\nplt.xlabel(\"ISUP Grade\",fontsize=14)\nplt.ylabel(\"Count\",Fontsize=14)\nplt.show()","70d96bd3":"data_providers=df_train.gleason_score.unique()\nfig=plt.figure(figsize=(6,4))\nax=sns.countplot(x=\"isup_grade\",hue=\"gleason_score\",data=df_train)\nplt.title(\"ISUP Grade Count by gleason_score\",fontsize=14)\nplt.xlabel(\"ISUP Grade\",fontsize=14)\nplt.ylabel(\"Count\",Fontsize=14)\nplt.show()","425cea45":"def plot_relative_distribution(df, feature, hue, title='', size=2):\n    f, ax = plt.subplots(1,1, figsize=(4*size,3*size))\n    total = float(len(df))\n    sns.countplot(x=feature, hue=hue, data=df, palette='Set2')\n    plt.title(title)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height\/total),\n                ha=\"center\") \n    plt.show()","51ca9bac":"plot_relative_distribution(df=train, feature='isup_grade', hue='data_provider', title = 'relative count plot of isup_grade with data_provider', size=2)\n","2127b5de":"plot_relative_distribution(df=train, feature='gleason_score', hue='data_provider', title = 'relative count plot of gleason_score with data_provider', size=3)","c46516d6":"plot_relative_distribution(df=train, feature='isup_grade', hue='gleason_score', title = 'relative count plot of isup_grade with gleason_score', size=3)","d2339b46":"def display_images(slides): \n    f, ax = plt.subplots(5,3, figsize=(18,22))\n    for i, slide in enumerate(slides):\n        image = openslide.OpenSlide(os.path.join(data_dir, f'{slide}.tiff'))\n        spacing = 1 \/ (float(image.properties['tiff.XResolution']) \/ 10000)\n        patch = image.read_region((1780,1950), 0, (256, 256))\n        ax[i\/\/3, i%3].imshow(patch) \n        image.close()       \n        ax[i\/\/3, i%3].axis('off')\n        \n        image_id = slide\n        data_provider = train.loc[slide, 'data_provider']\n        isup_grade = train.loc[slide, 'isup_grade']\n        gleason_score = train.loc[slide, 'gleason_score']\n        ax[i\/\/3, i%3].set_title(f\"ID: {image_id}\\nSource: {data_provider} ISUP: {isup_grade} Gleason: {gleason_score}\")\n\n    plt.show() \n","905780f7":"images = ['07a7ef0ba3bb0d6564a73f4f3e1c2293','037504061b9fba71ef6e24c48c6df44d','035b1edd3d1aeeffc77ce5d248a01a53','059cbf902c5e42972587c8d17d49efed','06a0cbd8fd6320ef1aa6f19342af2e68','06eda4a6faca84e84a781fee2d5f47e1','0a4b7a7499ed55c71033cefb0765e93d','0838c82917cd9af681df249264d2769c','046b35ae95374bfb48cdca8d7c83233f','074c3e01525681a275a42282cd21cbde',\n'05abe25c883d508ecc15b6e857e59f32','05f4e9415af9fdabc19109c980daf5ad','060121a06476ef401d8a21d6567dee6d','068b0e3be4c35ea983f77accf8351cc8','08f055372c7b8a7e1df97c6586542ac8'\n]\n\n","7de6ef0b":"display_images(images)","62a2fa12":"def display_masks(slides): \n    f, ax = plt.subplots(5,3, figsize=(18,22))\n    for i, slide in enumerate(slides):\n        \n        mask = openslide.OpenSlide(os.path.join(mask_dir))\n        mask_data = mask.read_region((0,0), mask.level_count - 1, mask.level_dimensions[-1])\n        cmap = matplotlib.colors.ListedColormap(['black', 'gray', 'green', 'yellow', 'orange', 'red'])\n\n        ax[i\/\/3, i%3].imshow(np.asarray(mask_data)[:,:,0], cmap=cmap, interpolation='nearest', vmin=0, vmax=5) \n        mask.close()       \n        ax[i\/\/3, i%3].axis('off')\n        \n        image_id = slide\n        data_provider = train.loc[slide, 'data_provider']\n        isup_grade = train.loc[slide, 'isup_grade']\n        gleason_score = train.loc[slide, 'gleason_score']\n        ax[i\/\/3, i%3].set_title(f\"ID: {image_id}\\nSource: {data_provider} ISUP: {isup_grade} Gleason: {gleason_score}\")\n        f.tight_layout()\n        \n    plt.show()\n","86108d95":"display_masks(data_sample)","39fac0d0":"data_providers = ['karolinska', 'radboud']\ntrain_df = pd.read_csv(f'{BASE_PATH}\/train.csv')\nmasks = os.listdir(mask_dir)\nmasks_df = pd.Series(masks).to_frame()\nmasks_df.columns = ['mask_file_name']\nmasks_df['image_id'] = masks_df.mask_file_name.apply(lambda x: x.split('_')[0])\ntrain_df = pd.merge(train_df, masks_df, on='image_id', how='outer')\ndel masks_df\nprint(f\"There are {len(train_df[train_df.mask_file_name.isna()])} images without a mask.\")\n\n## removing items where image mask is null\ntrain_df = train_df[~train_df.mask_file_name.isna()]","2f7ac8b8":"display_masks(data_sample)\n","1f37b8c9":"sample_images = list(train.loc[train.gleason_score==\"5+5\", \"image_id\"])\nsample_images = [ '08459aaedfda0679aab403ababbd6ece','0a848ccbbb065ef5ee59dd01710f8531', '0bbbb6734f721f4df4d2ba60ade0ed15', \n                 '0bd231c85b2695e2cf021299e67a6afc',  '0efdb66c93d6b474d93dfe41e40be6ca', '1364c10e1e7f1ad0457f649a44d74888', \n                 '1e644a98460e4f7ea50717720a001efd',  '1fb65315d7ded63d688194863a1b123e', '244d9617bd58fa1db73ab4c1f40d298e']\ndata_sample = train.loc[train.image_id.isin(sample_images)]\nshow_images(data_sample)\n","0d572f7c":"def load_and_resize_image(img_id):\n    \"\"\"\n    Edited from https:\/\/www.kaggle.com\/xhlulu\/panda-resize-and-save-train-data\n    \"\"\"\n    biopsy = skimage.io.MultiImage(os.path.join(data_dir, f'{img_id}.tiff'))\n    return cv2.resize(biopsy[-1], (512, 512))\n\ndef load_and_resize_mask(img_id):\n    \"\"\"\n    Edited from https:\/\/www.kaggle.com\/xhlulu\/panda-resize-and-save-train-data\n    \"\"\"\n    biopsy = skimage.io.MultiImage(os.path.join(mask_dir, f'{img_id}_mask.tiff'))\n    return cv2.resize(biopsy[-1], (512, 512))[:,:,0]","ab903280":"\nlabels = []\nfor grade in range(train.isup_grade.nunique()):\n    fig, ax = plt.subplots(nrows=4, ncols=4, figsize=(22, 22))\n\n    for i, row in enumerate(ax):\n        idx = i\/\/2\n        temp = train_df[(train_df.isup_grade == grade) & (train_df.data_provider == data_providers[idx])].image_id.head(4).reset_index(drop=True)\n        if i%2 < 1:\n            labels.append(f'{data_providers[idx]} (image)')\n            for j, col in enumerate(row):\n                col.imshow(load_and_resize_image(temp[j]))\n                col.set_title(f\"ID: {temp[j]}\")\n                \n        else:\n            labels.append(f'{data_providers[idx]} (mask)')\n            for j, col in enumerate(row):\n                if data_providers[idx] == 'radboud':\n                    col.imshow(load_and_resize_mask(temp[j]), \n                               cmap = matplotlib.colors.ListedColormap(['white', 'lightgrey', 'green', 'orange', 'red', 'darkred']), \n                               norm = matplotlib.colors.Normalize(vmin=0, vmax=5, clip=True))\n                else:\n                    col.imshow(load_and_resize_mask(temp[j]), \n                           cmap = matplotlib.colors.ListedColormap(['white', 'green', 'red']), \n                           norm = matplotlib.colors.Normalize(vmin=0, vmax=2, clip=True))\n                    \n                gleason_score = train.loc[temp[j], 'gleason_score']\n                col.set_title(f\"ID: {temp[j]}\")\n        \n    for row, r in zip(ax[:,0], labels):\n        row.set_ylabel(r, rotation=90, size='large', fontsize=14)\n\n    plt.suptitle(f'ISUP Grade {grade}', fontsize=20)\n    plt.show()","4b32c38a":"def overlay_mask_on_slide(images, center='radboud', alpha=0.8, max_size=(800, 800)):\n    \"\"\"Show a mask overlayed on a slide.\"\"\"\n    f, ax = plt.subplots(5,3, figsize=(18,22))\n    \n    \n    for i, image_id in enumerate(images):\n        slide = openslide.OpenSlide(os.path.join(data_dir, f'{image_id}.tiff'))\n        mask = openslide.OpenSlide(os.path.join(mask_dir, f'{image_id}_mask.tiff'))\n        slide_data = slide.read_region((0,0), slide.level_count - 1, slide.level_dimensions[-1])\n        mask_data = mask.read_region((0,0), mask.level_count - 1, mask.level_dimensions[-1])\n        mask_data = mask_data.split()[0]\n        \n        \n        # Create alpha mask\n        alpha_int = int(round(255*alpha))\n        if center == 'radboud':\n            alpha_content = np.less(mask_data.split()[0], 2).astype('uint8') * alpha_int + (255 - alpha_int)\n        elif center == 'karolinska':\n            alpha_content = np.less(mask_data.split()[0], 1).astype('uint8') * alpha_int + (255 - alpha_int)\n\n        alpha_content = PIL.Image.fromarray(alpha_content)\n        preview_palette = np.zeros(shape=768, dtype=int)\n\n        if center == 'radboud':\n            # Mapping: {0: background, 1: stroma, 2: benign epithelium, 3: Gleason 3, 4: Gleason 4, 5: Gleason 5}\n            preview_palette[0:18] = (np.array([0, 0, 0, 0.5, 0.5, 0.5, 0, 1, 0, 1, 1, 0.7, 1, 0.5, 0, 1, 0, 0]) * 255).astype(int)\n        elif center == 'karolinska':\n            # Mapping: {0: background, 1: benign, 2: cancer}\n            preview_palette[0:9] = (np.array([0, 0, 0, 0, 1, 0, 1, 0, 0]) * 255).astype(int)\n\n        mask_data.putpalette(data=preview_palette.tolist())\n        mask_rgb = mask_data.convert(mode='RGB')\n        overlayed_image = PIL.Image.composite(image1=slide_data, image2=mask_rgb, mask=alpha_content)\n        overlayed_image.thumbnail(size=max_size, resample=0)\n\n        \n        ax[i\/\/3, i%3].imshow(overlayed_image) \n        slide.close()\n        mask.close()       \n        ax[i\/\/3, i%3].axis('off')\n        \n        data_provider = train.loc[image_id, 'data_provider']\n        isup_grade = train.loc[image_id, 'isup_grade']\n        gleason_score = train.loc[image_id, 'gleason_score']\n        ax[i\/\/3, i%3].set_title(f\"ID: {image_id}\\nSource: {data_provider} ISUP: {isup_grade} Gleason: {gleason_score}\")\n","381876f2":"overlay_mask_on_slide(images)","30cdc01d":"overlay_mask_on_slide(images)","2d09defe":"pen_marked_images = [\n    \n    'ebb6a080d72e09f6481721ef9f88c472',\n    'ebb6d5ca45942536f78beb451ee43cc4',\n    'ea9d52d65500acc9b9d89eb6b82cdcdf',\n    'e726a8eac36c3d91c3c4f9edba8ba713',\n    'e90abe191f61b6fed6d6781c8305fe4b',\n    'fd0bb45eba479a7f7d953f41d574bf9f',\n    'ff10f937c3d52eff6ad4dd733f2bc3ac',\n    'feee2e895355a921f2b75b54debad328',\n    'feac91652a1c5accff08217d19116f1c',\n    'fb01a0a69517bb47d7f4699b6217f69d',\n    'f00ec753b5618cfb30519db0947fe724',\n    'e9a4f528b33479412ee019e155e1a197',\n    'f062f6c1128e0e9d51a76747d9018849',\n    'f39bf22d9a2f313425ee201932bac91a',\n]","aad4d5c1":"overlay_mask_on_slide(pen_marked_images)","7e2cc348":"import time\nstart_time = time.time()\nslide_dimensions, spacings, level_counts = [], [], []\n\nfor image_id in train.image_id:\n    image = str(image_id)+'.tiff'\n    image_path = os.path.join(PATH,\"train_images\",image)\n    slide = openslide.OpenSlide(image_path)\n    spacing = 1 \/ (float(slide.properties['tiff.XResolution']) \/ 10000)\n    slide_dimensions.append(slide.dimensions)\n    spacings.append(spacing)\n    level_counts.append(slide.level_count)\n    slide.close()\n    del slide\n\ntrain['width']  = [i[0] for i in slide_dimensions]\ntrain['height'] = [i[1] for i in slide_dimensions]\ntrain['spacing'] = spacings\ntrain['level_count'] = level_counts\n\nend_time = time.time()\nprint(f\"Total processing time: {round(end_time - start_time,2)} sec.\")","28e3a3ff":"\n\nclass MishFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return x * torch.tanh(F.softplus(x))   # x * tanh(ln(1 + exp(x)))\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x = ctx.saved_variables[0]\n        sigmoid = torch.sigmoid(x)\n        tanh_sp = torch.tanh(F.softplus(x)) \n        return grad_output * (tanh_sp + x * sigmoid * (1 - tanh_sp * tanh_sp))\n\nclass Mish(nn.Module):\n    def forward(self, x):\n        return MishFunction.apply(x)\n\ndef to_Mish(model):\n    for child_name, child in model.named_children():\n        if isinstance(child, nn.ReLU):\n            setattr(model, child_name, Mish())\n        else:\n            to_Mish(child)\n","8ba94e23":"def _resnext(url,block,layers,pretrained,progress,**kwargs):\n    model=ResNet(block,layers,**kwargs)\n    return model\nclass Model(nn.Module):\n    def __init__(self,arch='resnext50_32x4d',n=6,pre=True):\n        super().__init__()\n        m=_resnext(semi_supervised_model_urls[arch],Bottleneck,[3,4,6,3],False,progress=False,groups=32,width_per_group=4)\n        self.enc=nn.Sequential(*list(m.children())[:-2])\n        nc=list(m.children())[-1].in_features\n        self.head = nn.Sequential(AdaptiveConcatPool2d(),Flatten(),nn.Linear(2*nc,512),\n                Mish(),nn.BatchNorm1d(512),nn.Dropout(0.5),nn.Linear(512,n))\n        \n    def forward(self, x):\n        shape = x.shape\n        n = shape[1]\n        x = x.view(-1,shape[2],shape[3],shape[4])\n        x = self.enc(x)\n        shape = x.shape\n        x = x.view(-1,n,shape[1],shape[2],shape[3]).permute(0,2,1,3,4).contiguous()\\\n          .view(-1,shape[1],shape[2]*n,shape[3])\n        x = self.head(x)\n        return x\nmodels = []","93e3babb":"models = []\nfor path in MODELS:\n    state_dict = torch.load(path,map_location=torch.device('cpu'))\n    model = Model()\n    model.load_state_dict(state_dict)\n    model.float()\n    model.eval()\n    model.cuda()\n    models.append(model)","4b52491f":"del state_dict","b621712c":"def tile(img):\n    shape = img.shape\n    pad0,pad1 = (sz - shape[0]%sz)%sz, (sz - shape[1]%sz)%sz\n    img = np.pad(img,[[pad0\/\/2,pad0-pad0\/\/2],[pad1\/\/2,pad1-pad1\/\/2],[0,0]],\n                 constant_values=255)\n    img = img.reshape(img.shape[0]\/\/sz,sz,img.shape[1]\/\/sz,sz,3)\n    img = img.transpose(0,2,1,3,4).reshape(-1,sz,sz,3)\n    if len(img) < N:\n        img = np.pad(img,[[0,N-len(img)],[0,0],[0,0],[0,0]],constant_values=255)\n    idxs = np.argsort(img.reshape(img.shape[0],-1).sum(-1))[:N]\n    img = img[idxs]\n    return img\n\nmean = torch.tensor([1.0-0.90949707, 1.0-0.8188697, 1.0-0.87795304])\nstd = torch.tensor([0.36357649, 0.49984502, 0.40477625])\n\nclass PandaDataset(Dataset):\n    def __init__(self, path, test):\n        self.path = path\n        self.names = list(pd.read_csv(test).image_id)\n\n    def __len__(self):\n        return len(self.names)\n\n    def __getitem__(self, idx):\n        name = self.names[idx]\n        img = skimage.io.MultiImage(os.path.join(DATA,name+'.tiff'))[-1]\n        tiles = torch.Tensor(1.0 - tile(img)\/255.0)\n        tiles = (tiles - mean)\/std\n        return tiles.permute(0,3,1,2), name","4b5da319":"sub_df = pd.read_csv(SAMPLE)\nif os.path.exists(DATA):\n    ds = PandaDataset(DATA,TEST)\n    dl = DataLoader(ds, batch_size=bs, num_workers=nworkers, shuffle=False)\n    names,preds = [],[]\n\n    with torch.no_grad():\n        for x,y in tqdm(dl):\n            x = x.cuda()\n            #dihedral TTA\n            x = torch.stack([x,x.flip(-1),x.flip(-2),x.flip(-1,-2),\n              x.transpose(-1,-2),x.transpose(-1,-2).flip(-1),\n              x.transpose(-1,-2).flip(-2),x.transpose(-1,-2).flip(-1,-2)],1)\n            x = x.view(-1,N,3,sz,sz)\n            p = [model(x) for model in models]\n            p = torch.stack(p,1)\n            p = p.view(bs,8*len(models),-1).mean(1).argmax(-1).cpu()\n            names.append(y)\n            preds.append(p)\n    \n    names = np.concatenate(names)\n    preds = torch.cat(preds).numpy()\n    sub_df = pd.DataFrame({'image_id': names, 'isup_grade': preds})\n    \n","6a23f88c":"sub_df.to_csv(\"submission.csv\", index=False)\nsub_df.head(5)","5d922cea":"To detect and classify the severity of prostate cancer on images of prostate tissue samples.\n\nIn practice,tissue samples are examined and scored by pathologists according to the so-called Gleason grading system which is later converted to an ISUP grade.","6a0b0864":"# Gleason Score and ISUP Grade","6564266c":"# Differences Between Data Providers","230ed414":"# **Overlaying masks on the slides**","1466f851":"In the above dataframe it looks like one image might have been converted to wrong ISUP grade.","7f115e2b":"Note : In the example below you can also observe a few pen marking slide (dark green smudges).These markings are not part of the tissue but were made by the pathologists who originally checked this case.These pen markings are available on some slides in the training set.","0c9bc8b2":"# Visualizing Image and Mask Samples","20df77a4":"**One Mislabeled Image?**","1a07d2e7":"Parse all images for train data to extract image characteristics","0da66e68":"# Introduction","afdf2291":"# Model","1ffdaaf8":"few images and associated masks for samples with Gleason Score(5+5)","eae11cef":"#  Expolring images with pen markers","73ff8ef0":"![image.png](attachment:image.png)","98b193c4":"They used different scanners with slightly different maximu  microscope resolutions and worked with different pathologists for labeling their images.","f3472027":"This Kernel objective is to expolre the dataset for Prostate cANcer graDe Assessment(PANDA)challenge.\n\nProstate cancer begins when cells in the prostate gland start to grow out of control.The prostate is gland only in males.It makes some of the fluid that is part of semen.\n\nThe prostate is below the bladder and in front of the rectum.just behind the prostate are glands called seminal vesicles that make most of the fluid for semen.The urethra,which \n","4c0c6553":"It is mentioned that in training dataset,there are few images with pen markers on them.The organizers left us with a Note as described below.\nNote:that slightly different procedures were in place for the images used in the test set than training set.Some of the training set images have stray pen marks on them,but the test slides are free of pen marks.","12c736cc":"# Prediction","e08a6093":"Let's have a quick first look at the differences between the data providers in regards to the original images and the masks.\n\n**Radboud:**Prostate glands are individually labelled.Valid values are:\n* 0:background (non tissue) or unknown\n* 1:stroma(connective tissue,non-epithelium tissue)\n* 2.healthy (benign) epithelium\n* 3.cancerous epithelium (Gleason 3)\n* 4.Cancerous epithelium (Gleason 4)\n* 5.Cancerous epithelium (Gleason 5)\n\n**Karolinska**:Regions are labelled.Valid values are:\n* [0]:backgound (non tissue) or unknown\n* [1]:benign tissue(stroma and epithelium combined)\n* [2]:cancerous tissue (stroma and epithelium combined)","4b0e957d":"At first glance,we found 100 images without masks.For further analysis,we will drop the 100 images without a mask.\n\nit might be good idea to use these test cases for validation.All suspicious test cases found in this EDA are summarized in a .csv file ","caac48af":"# Overview","59a6ed21":"# EDA","1e0b5869":"The grading process consists of finding and classifying cancer tissue into so-called Gleaon patterns(3,4, or 5).After the biopsy is assigned a Gleason score,it is converted into an ISUP grade on 1-5 scale.However,the system suffers from significant inter-observer variability between pathologists,limiting its usefulness for individual patients.","7d8b5c3e":"# Objective","3ca60a0b":"All the Karolinska images in the training data is graded by the same pathologist.However,for the test set we used several pathologists who each labeled the images using ISUP(not Gleason) and derived a consensus label.The mislabeled image was one of the those images but was later moved to the training set. ","a3e1d6c7":"As the masks have the same dimension as the slides ,we can overlay the masks on the tissue to directly see which areas are cancerous .This overlay can help you identifying the different growth patterns.To do this,we load both the mask and the biopsy and merge them using PIL. "}}