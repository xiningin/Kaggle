{"cell_type":{"8427f144":"code","95c92c34":"code","576a1eac":"code","d0f81f3f":"code","6af708e8":"code","760059dc":"code","8408705b":"code","f326fdaf":"code","7ba15488":"code","a5b719a4":"code","fc933e0e":"code","19567fa4":"code","e4097f6c":"code","083f7512":"code","21a40bac":"code","127884e9":"code","3cb055de":"code","9590a6c6":"code","8fedd779":"code","83b90e06":"code","a4564788":"code","cd938ba5":"code","95223050":"code","ccc94974":"code","2529cba9":"code","a3cb126f":"code","05ba92d9":"code","7d10d1ff":"code","a38f7694":"code","30e74ef8":"code","04e808da":"code","700e0789":"code","71c08e86":"code","893b384e":"code","f9db9554":"code","b361c2cf":"code","690b8b33":"code","3518f47b":"code","bbd2edc6":"code","5c1cc60e":"code","faab9710":"code","3ff47ec8":"code","ce9a17ab":"code","9e30c538":"code","5212fc9e":"markdown","cd2a4396":"markdown","08915da4":"markdown","33d75cc9":"markdown","eae6be11":"markdown","d9cae26b":"markdown","69b1be50":"markdown","184ce1d4":"markdown","827ca3af":"markdown","ab7c39ab":"markdown","d12b118d":"markdown","26c19c5c":"markdown","a10be11f":"markdown","948b49c6":"markdown","b57db30d":"markdown","6c2fb83f":"markdown","fd3fa9fa":"markdown","09ae8d3d":"markdown","71d6c300":"markdown","e54c26c4":"markdown","cd8b5d81":"markdown","33783900":"markdown","3d5c9513":"markdown","60de1bc6":"markdown","701a8154":"markdown","37051926":"markdown","e866a164":"markdown","5db80e41":"markdown","81033399":"markdown","3ea4bab0":"markdown","3eda34f4":"markdown","c47159f1":"markdown","76801a91":"markdown","f5d5daff":"markdown","e0fe59f1":"markdown","6650c929":"markdown","c553d89b":"markdown","d20900b1":"markdown","7b3dc9dc":"markdown","2cf93bf8":"markdown","e0a29523":"markdown","d16ba3a8":"markdown","fbd1d27b":"markdown","790f0c79":"markdown","5e50c240":"markdown","3820cda4":"markdown","1066e559":"markdown","4456005d":"markdown","acce9cd3":"markdown","773d607f":"markdown","ac3ed25a":"markdown","9cd25ad7":"markdown","abaeb389":"markdown","29642f9c":"markdown","3f2d3a27":"markdown","1590cfdc":"markdown"},"source":{"8427f144":"import numpy as np # for linear algebra\nimport pandas as pd # data processing, CSV file I\/O, etc\nimport seaborn as sns # for plots\nimport plotly.graph_objects as go # for plots\nimport plotly.express as px #for plots\nimport matplotlib.pyplot as plt # for visualizations and plots\nimport missingno as msno # for plotting missing data\n\n# this eliminates the requirement to use plt.show() after every plot\n%matplotlib inline\n\n# changing the default figure sizes\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 15, 10\n\nimport random # random library\npallete = ['Accent_r', 'Blues', 'BrBG', 'BrBG_r', 'BuPu', 'CMRmap', 'CMRmap_r', 'Dark2', 'Dark2_r', 'GnBu', 'GnBu_r', 'OrRd', 'Oranges', 'Paired', 'PuBu', 'PuBuGn', 'PuRd', 'Purples', 'RdGy_r', 'RdPu', 'Reds', 'autumn', 'cool', 'coolwarm', 'flag', 'flare', 'gist_rainbow', 'hot', 'magma', 'mako', 'plasma', 'prism', 'rainbow', 'rocket', 'seismic', 'spring', 'summer', 'terrain', 'turbo', 'twilight']\n\nfrom sklearn.model_selection import train_test_split # spliting training and testing data\nfrom sklearn.preprocessing import MinMaxScaler # data normalization with sklearn\nfrom sklearn.preprocessing import StandardScaler # data standardization with  sklearn\nfrom sklearn.ensemble import RandomForestClassifier # model\nfrom sklearn.linear_model import LogisticRegression # model\nfrom sklearn.neighbors import KNeighborsClassifier # model\nfrom sklearn.metrics import classification_report, confusion_matrix # to evaluate the model\nfrom mlxtend.plotting import plot_confusion_matrix # plot confusion matrix\nfrom sklearn.model_selection import GridSearchCV # to finetune the model\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","95c92c34":"df = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndf.head() # displays the top 5 values in the dataset","576a1eac":"df.info()","d0f81f3f":"df.describe()","6af708e8":"df.isnull().sum()","760059dc":"df[\"Glucose\"] = df[\"Glucose\"].apply(lambda x: np.nan if x == 0 else x)\ndf[\"BloodPressure\"] = df[\"BloodPressure\"].apply(lambda x: np.nan if x == 0 else x)\ndf[\"SkinThickness\"] = df[\"SkinThickness\"].apply(lambda x: np.nan if x == 0 else x)\ndf[\"Insulin\"] = df[\"Insulin\"].apply(lambda x: np.nan if x == 0 else x)\ndf[\"BMI\"] = df[\"BMI\"].apply(lambda x: np.nan if x == 0 else x)","8408705b":"df.isnull().sum()","f326fdaf":"px.pie(df, names=\"Outcome\")","7ba15488":"sns.countplot(x=\"Outcome\", data=df, palette=random.choice(pallete))","a5b719a4":"sns.countplot(x=\"Pregnancies\", hue = \"Outcome\", data=df, palette=random.choice(pallete))","fc933e0e":"sns.histplot(x=\"Pregnancies\", hue=\"Outcome\", data=df, kde=True, palette=random.choice(pallete))","19567fa4":"sns.histplot(x=\"BloodPressure\", hue=\"Outcome\", data=df, kde=True, palette=random.choice(pallete))","e4097f6c":"sns.histplot(x=\"Glucose\", hue=\"Outcome\", data=df, kde=True, palette=random.choice(pallete))","083f7512":"sns.histplot(x=\"SkinThickness\", hue=\"Outcome\", data=df, kde=True, palette=random.choice(pallete))","21a40bac":"sns.histplot(x=\"Insulin\", hue=\"Outcome\", data=df, kde=True, palette=random.choice(pallete))","127884e9":"sns.histplot(x=\"Age\", hue=\"Outcome\", data=df, kde=True, palette=random.choice(pallete))","3cb055de":"sns.histplot(x=\"BMI\", hue=\"Outcome\", data=df, kde=True, palette=random.choice(pallete))","9590a6c6":"sns.histplot(x=\"DiabetesPedigreeFunction\", hue=\"Outcome\", data=df, kde=True, palette=random.choice(pallete))","8fedd779":"sns.pairplot(df, hue='Outcome',palette=random.choice(pallete))","83b90e06":"fig, axs = plt.subplots(4, 2, figsize=(20,20))\naxs = axs.flatten()\nfor i in range(len(df.columns)-1):\n    sns.boxplot(data=df, x=df.columns[i], ax=axs[i], palette=random.choice(pallete))","a4564788":"sns.heatmap(df.corr(), linewidths=0.1, vmax=1.0, square=True, cmap='coolwarm', linecolor='white', annot=True).set_title(\"Correlation Map\")","cd938ba5":"df.isnull().sum()","95223050":"msno.bar(df)","ccc94974":"msno.matrix(df, figsize=(20,35))","2529cba9":"msno.heatmap(df, cmap=random.choice(pallete))","a3cb126f":"msno.dendrogram(df)","05ba92d9":"df.isnull().sum()\/len(df)*100","7d10d1ff":"df.drop(columns=[\"Insulin\"], inplace=True)","a38f7694":"df.describe()","30e74ef8":"df.skew()","04e808da":"# Highly skewed\ndf[\"BMI\"].replace(to_replace=np.nan,value=df[\"BMI\"].median(), inplace=True)\ndf[\"Pregnancies\"].replace(to_replace=np.nan,value=df[\"Pregnancies\"].median(), inplace=True)\n\n# Normal\ndf[\"Glucose\"].replace(to_replace=np.nan,value=df[\"Glucose\"].mean(), inplace=True)\ndf[\"BloodPressure\"].replace(to_replace=np.nan,value=df[\"BloodPressure\"].mean(), inplace=True)\ndf[\"SkinThickness\"].replace(to_replace=np.nan,value=df[\"SkinThickness\"].mean(), inplace=True)","700e0789":"Q1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","71c08e86":"df_out = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]\nprint(f'Before: {df.shape}, After: {df_out.shape}')","893b384e":"for col in df.columns[:-1]:\n    up_out = df[col].quantile(0.90)\n    low_out = df[col].quantile(0.10)\n    med = df[col].median()\n#     print(col, up_out, low_out, med)\n    df[col] = np.where(df[col] > up_out, med, df[col])\n    df[col] = np.where(df[col] < low_out, med, df[col])","f9db9554":"df.describe()","b361c2cf":"X = df_out[df_out.columns[:-1]]\ny = df_out['Outcome']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)","690b8b33":"norm = MinMaxScaler().fit(X_train)\nX_train_norm = norm.transform(X_train)\nX_test_norm = norm.transform(X_test)","3518f47b":"log_params = {'C': [0.0001, 0.001, 0.01, 0.05, 0.1, 0.5, 1, 10, 100, 100]} \nlog_model = GridSearchCV(LogisticRegression(), log_params, cv=5)\nlog_model.fit(X_train_norm, y_train)\nlog_pred = log_model.predict(X_test_norm)","bbd2edc6":"rf_params = {'criterion' : ['gini', 'entropy'],\n             'n_estimators': list(range(60, 140, 20)),\n             'max_depth': list(range(3, 20, 2))}\nrf_model = GridSearchCV(RandomForestClassifier(), rf_params, cv=5)\nrf_model.fit(X_train_norm, y_train)\nrf_pred = rf_model.predict(X_test_norm)","5c1cc60e":"knn_params = {'n_neighbors': list(range(1,50))}\nknn_model = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5)\nknn_model.fit(X_train_norm, y_train)\nknn_pred = knn_model.predict(X_test_norm)","faab9710":"print(\"Logistic Regression: \\n\", classification_report(y_test, log_pred)) \nprint(\"\\nRandom Forest Classifier: \\n\", classification_report(y_test, rf_pred)) \nprint(\"\\nK Neighbors Classifier: \\n\", classification_report(y_test, knn_pred)) ","3ff47ec8":"labels = [\"Not Diabetic\", \"Diabetic\"]\ncm  = confusion_matrix(y_test, log_pred)\nplt.figure()\nplot_confusion_matrix(cm, hide_ticks=True, cmap=\"Reds\")\nplt.xticks(range(2), labels, fontsize=14)\nplt.yticks(range(2), labels, fontsize=14)\nplt.show()","ce9a17ab":"labels = [\"Not Diabetic\", \"Diabetic\"]\ncm  = confusion_matrix(y_test, rf_pred)\nplt.figure()\nplot_confusion_matrix(cm, hide_ticks=True, cmap=\"Blues\")\nplt.xticks(range(2), labels, fontsize=14)\nplt.yticks(range(2), labels, fontsize=14)\nplt.show()","9e30c538":"labels = [\"Not Diabetic\", \"Diabetic\"]\ncm  = confusion_matrix(y_test, knn_pred)\nplt.figure()\nplot_confusion_matrix(cm, hide_ticks=True, cmap=\"Greens\")\nplt.xticks(range(2), labels, fontsize=14)\nplt.yticks(range(2), labels, fontsize=14)\nplt.show()","5212fc9e":"### Matrix\n\n**How to read?**\n\nEach row in the matrix represents that row in the dataset. If any value in that row is NaN, then it is white else black(\/gray).\n\nThe graph on the right shows the number of missing values in each row. If a row has too many NaN values, we can remove that.","cd2a4396":"Here we can see 65.1% of the people in this dataset doesn't have Diabetes and 34.9% does.","08915da4":"# Read the dataset","33d75cc9":"Here we can see, *Random Forest Classifier* has better Precision and Recall (hence, better f1-score) compared to other models.","eae6be11":"### Insulin vs Outcome","d9cae26b":"### Correlation Matrix","69b1be50":"Wow, this dataset doesn't have any null values!!","184ce1d4":"# About the dataset\n\n## Context\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n## Content\nThe datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.","827ca3af":"## K Neighbors Classifier","ab7c39ab":"**Method 2**\n\n*Median Method*\n\nIn this method we will replace the outliers with *median*.","d12b118d":"### Skin Thickness vs Outcome","26c19c5c":"### Checking for NaN values","a10be11f":"### Outliers\nOutliers... more like **OUT**-liers!","948b49c6":"## K Neighbors Classifier","b57db30d":"## Normalize the data","6c2fb83f":"For highly skewed values we'll impute the column with **median** else **mean**.","fd3fa9fa":"## Random Forest Classifier","09ae8d3d":"**Method 1**\n\n*IQR Method*\n\nThis technique uses the IQR scores calculated earlier to remove outliers. The rule of thumb is that anything not in the range of $(Q1 - 1.5 IQR)$ and $(Q3 + 1.5 IQR)$ is an outlier, and can be removed.","71d6c300":"`Outcome` is highliy correlated with `Glucose`.","e54c26c4":"### Blood Pressure vs Outcome","cd8b5d81":"### Pregnencies vs Outcome","33783900":"We can see that the `Insulin` column has nearly 50% of NaN values. Therefore, it would be wise to drop the column entirely!","3d5c9513":"Using this method, we will be losing around 140 data points.","60de1bc6":"# Introduction\nIn this notebook I'll be exploring, cleaning and making a model on the *PIMA Diabetes* dataset.\n\n![Diabetes Picture](https:\/\/images.unsplash.com\/photo-1593491205049-7f032d28cf5c?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&dl=mykenzie-johnson-4qjxCUOc3iQ-unsplash.jpg)","701a8154":"Here we can see that the glucose levels of diabetic people is generally high.","37051926":"Here we can see diabetic people have a little more thick skin.","e866a164":"## Logistic Regression","5db80e41":"### Pairplot","81033399":"I hope you liked my notebook. Do not forget to upvote it.\n## Thank You","3ea4bab0":"### Barplot","3eda34f4":"### Age vs Outcome","c47159f1":"Diabetic people have higher BMI.","76801a91":"# Confusion Matrix","f5d5daff":"# Models\n## Logistic Regression","e0fe59f1":"### Heatmap\nThe heatmap is used to identify correlations of the nullity between each of the different columns.","6650c929":"### BMI vs Outcome","c553d89b":"# EDA\n### Distribution of the data","d20900b1":"We can see that old people are more diabetic.","7b3dc9dc":"## NaN Values Analysis\nLet's get rid of them NaNs.","2cf93bf8":"# Getting info about the dataset\n### General stats","e0a29523":"## Split the data","d16ba3a8":"### DiabetesPedigreeFunction vs Outcome","fbd1d27b":"# Setting up the environment","790f0c79":"# Evaluation\nFor the evaluation we will be mainly looking at `Precision` & `Recall` values. This is because in the dataset there are very less points for diabetic people, because of which even if a model predicts `0` for everyone, it can be very accurate!","5e50c240":"Here we can see that the BP levels of diabetic people is a little high.","3820cda4":"# Cleaning the dataset","1066e559":"### Percentages of NaNs","4456005d":"### Glucose vs Outcome","acce9cd3":"## Random Forest Classifier","773d607f":"# Data Cleaning\nIn the above stats we can see that there are people with 0 BP (dead person?), 0 skin thickness (skeleton?) and 0 Glucose (how do you even survive?).\n\nLet's convert those 0s to NaN.","ac3ed25a":"# Modeling","9cd25ad7":"Here we can see that Insulin and Skin Thickness are highly positively correlated with each other (nullity corr).","abaeb389":"### Dendrogram\nThe dendrogram plot provides a tree-like graph generated through hierarchical clustering and groups together columns that have strong correlations in nullity.\n\n\n**How to read?**\n\nIf a number of columns are grouped together at level zero, then the presence of nulls in one of those columns is directly related to the presence or absence of nulls in the others columns. The more separated the columns in the tree, the less likely the null values can be correlated between the columns.","29642f9c":"### Boxplots","3f2d3a27":"Here we can see diabetic people have a little more insulin.","1590cfdc":"We can conclude that **Random Forest Classifier** model works better in this case."}}