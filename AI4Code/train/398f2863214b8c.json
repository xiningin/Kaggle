{"cell_type":{"37fc88cd":"code","63161fc3":"code","131c7fee":"code","76e3538a":"code","6ae3276c":"code","22af0284":"code","3f4a79db":"code","4954822f":"code","e84891d7":"code","a05b228c":"code","42d47ba9":"code","c9e223ed":"code","59ca91f8":"code","c017b9ce":"code","ed082bf9":"code","d4914e5f":"code","99b47bb1":"code","558b4688":"code","4e252e16":"code","5df2c244":"code","a2e74916":"code","f5747dd0":"code","463d59ff":"code","59ad9c03":"code","83030948":"code","8dbb898e":"code","98a80e1b":"code","cdb9d436":"code","52f4e481":"code","f7e6655a":"code","57723dd7":"code","05a24aaa":"code","66a55259":"code","e069a09c":"code","8cf3de0f":"code","e3339ae6":"code","91b0a9d1":"code","6621e7ce":"markdown","5ddddc12":"markdown","71c33ca7":"markdown","07be7058":"markdown","f1fa8f52":"markdown"},"source":{"37fc88cd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","63161fc3":"#Load other modules\nimport matplotlib.pyplot as plt  #Graphics\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier  #Random Forest algorithm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV \nfrom sklearn.model_selection import cross_val_score\n#To show graphs within the notebook\n%matplotlib inline","131c7fee":"#load the datasets\ntrain = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")","76e3538a":"print(\"Training data shape : \" + str(train.shape))","6ae3276c":"train.head()","22af0284":"train.label.unique()","3f4a79db":"#Seperate the target and independant variables\ndf_x=train.iloc[:,1:]\ndf_y=train.iloc[:,0]","4954822f":"df_x","e84891d7":"def print_image(row, df):\n    temp=df.iloc[row,:].values\n    temp = temp.reshape(28,28).astype('uint8')\n    plt.imshow(temp)","a05b228c":"print_image(0, df_x)\ndf_y[0]","42d47ba9":"import warnings\nwarnings.filterwarnings('ignore')\nsns.countplot(df_y)","c9e223ed":"df_y.value_counts().sort_index()\n","59ca91f8":"#Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.2, random_state=0)\nprint(X_train.shape)\nprint(X_test.shape)","c017b9ce":"#training random Forest\nrf=RandomForestClassifier(n_estimators=100)\nrf.fit(X_train,y_train)","ed082bf9":"pred=rf.predict(X_test)\nprint (\"Classification Report\")\nprint(classification_report(y_test, pred))\nprint (\"Confusion Report\")\nprint(confusion_matrix(y_test, pred))","d4914e5f":"%%time\nfrom datetime import datetime\nstart=datetime.now()\nrf=RandomForestClassifier(n_estimators=100)\nrf.fit(X_train,y_train)\nend=datetime.now()\nprint(\"Time taken to run classifier : \" + str((end-start).total_seconds()) + \" secs\")","99b47bb1":"# Check output submission format\ndata = pd.read_csv('\/kaggle\/input\/digit-recognizer\/sample_submission.csv')\ndata.head()","558b4688":"pred = rf.predict(test)\npred = pd.Series(pred,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),pred],axis = 1)\nsubmission.to_csv(\"mnist_rf.csv\",index=False)","4e252e16":"submission.head()","5df2c244":"# Check prediction visually\nplt.figure(figsize=(12,10))\nfor i in range(0,5) :\n    plt.subplot(1, 5, i+1)\n    print_image(i, test)\n","a2e74916":"df_x.shape, test.shape","f5747dd0":"combined = pd.concat([df_x, test])\ncombined.shape","463d59ff":"from sklearn.decomposition import PCA\n# PCA automatically takes care of centering the data!\n#To better understand the problem, I want to start by plotting Explained Variance vs. Dimensions:\n\npca = PCA()\npca.fit(combined)\ncumsum = np.cumsum(pca.explained_variance_ratio_)\nd = np.argmax(cumsum >= 0.95) + 1\n\nprint(\"{} principal components account for 95 percent of the variability in the MNIST dataset.\".format(round(d)))\n\nplt.plot(cumsum, linewidth = 3)\nplt.axis([0,784,0,1])\nplt.xlabel(\"Dimensions\")\nplt.ylabel(\"Explained Variance\")\nplt.plot([0,784],[0.95,0.95],\"k:\")\nplt.plot([d,d],[0,0.95], \"k:\")\nplt.plot(d, 0.95, \"ko\")\nplt.grid(True)\nplt.title(\"95% Explained Variance vs. Dimensions for MNIST PCA\")\nplt.show()","59ad9c03":"pca = PCA(n_components = 154)\npca.fit(combined)","83030948":"len(pca.components_)","8dbb898e":"X_train_reduced = pca.fit_transform(X_train)\nX_test_reduced = pca.fit_transform(test)","98a80e1b":"y_train.shape, X_train_reduced.shape","cdb9d436":"%%time\nfrom datetime import datetime\nstart=datetime.now()\nrf=RandomForestClassifier(n_estimators=100)\nrf.fit(X_train_reduced,y_train)\nend=datetime.now()\nprint(\"Time taken to run classifier : \" + str((end-start).total_seconds()) + \" secs\")","52f4e481":"pred = rf.predict(X_test_reduced)\npred = pd.Series(pred,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),pred],axis = 1)\nsubmission.to_csv(\"mnist_pca_rf.csv\",index=False)","f7e6655a":"# Separate the training set from PCA \npca = PCA(n_components = 154)\npca.fit(X_train)","57723dd7":"X_train_reduced = pca.fit_transform(X_train)\nX_test_reduced = pca.fit_transform(test)","05a24aaa":"%%time\nfrom datetime import datetime\nstart=datetime.now()\nrf=RandomForestClassifier(n_estimators=100)\nrf.fit(X_train_reduced,y_train)\nend=datetime.now()\nprint(\"Time taken to run classifier : \" + str((end-start).total_seconds()) + \" secs\")","66a55259":"pred = rf.predict(X_test_reduced)\npred = pd.Series(pred,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),pred],axis = 1)\nsubmission.to_csv(\"mnist_pca_rf_fixed.csv\",index=False)","e069a09c":"#normalize data\nX = X_train.astype(float) \/ 255.","8cf3de0f":"from sklearn.cluster import MiniBatchKMeans\n\nn_digits = len(np.unique(y_test))\nprint(n_digits)\n# Initialize KMeans model\nkmeans = MiniBatchKMeans(n_clusters = n_digits)\n# Fit the model to the training data\nkmeans.fit(X)\nkmeans.labels_","e3339ae6":"def infer_cluster_labels(kmeans, actual_labels):\n    inferred_labels = {}\n    for i in range(kmeans.n_clusters):\n        # find index of points in cluster\n        labels = []\n        index = np.where(kmeans.labels_ == i)\n        # append actual labels for each point in cluster\n        labels.append(actual_labels[index])\n        # determine most common label\n        if len(labels[0]) == 1:\n            counts = np.bincount(labels[0])\n        else:\n            counts = np.bincount(np.squeeze(labels))\n        # assign the cluster to a value in the inferred_labels dictionary\n        if np.argmax(counts) in inferred_labels:\n            # append the new number to the existing array at this slot\n            inferred_labels[np.argmax(counts)].append(i)\n        else:\n            # create a new array in this slot\n            inferred_labels[np.argmax(counts)] = [i]\n     \n        #print(labels)\n        #print('Cluster: {}, label: {}'.format(i, np.argmax(counts)))\n         \n    return inferred_labels\n\ndef infer_data_labels(X_labels, cluster_labels):\n    # empty array of len(X)\n    predicted_labels = np.zeros(len(X_labels)).astype(np.uint8)\n    for i, cluster in enumerate(X_labels):\n        for key, value in cluster_labels.items():\n            if cluster in value:\n                predicted_labels[i] = key\n    return predicted_labels\n\n# test the infer_cluster_labels() and infer_data_labels() functions\n\ncluster_labels = infer_cluster_labels(kmeans, Y)\nX_clusters = kmeans.predict(X)\npredicted_labels = infer_data_labels(X_clusters, cluster_labels)\nprint(predicted_labels[:20])\nprint(Y[:20])","91b0a9d1":"# Initialize and fit KMeans algorithm\nkmeans = MiniBatchKMeans(n_clusters = 36)\nkmeans.fit(X)\n\n# record centroid values\ncentroids = kmeans.cluster_centers_\n\n# reshape centroids into images\nimages = centroids.reshape(36, 28, 28)\nimages *= 255\nimages = images.astype(np.uint8)\n\n# determine cluster labels\ncluster_labels = infer_cluster_labels(kmeans, Y)\n\n# create figure with subplots using matplotlib.pyplot\nfig, axs = plt.subplots(6, 6, figsize = (20, 20))\nplt.gray()\n\n# loop through subplots and add centroid images\nfor i, ax in enumerate(axs.flat):\n    \n    # determine inferred label using cluster_labels dictionary\n    for key, value in cluster_labels.items():\n        if i in value:\n            ax.set_title('Inferred Label: {}'.format(key))\n    \n    # add image to subplot\n    ax.matshow(images[i])\n    ax.axis('off')\n    \n# display the figure\nfig.show()","6621e7ce":"### Exploring the training data","5ddddc12":"### 5. Fixing the flaw","71c33ca7":"### 6. K-Means Clustering","07be7058":"## 2. PCA ","f1fa8f52":"### 3. Train with PCA"}}