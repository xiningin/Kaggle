{"cell_type":{"3854ea2b":"code","3054d264":"code","6be2e140":"code","756459bf":"code","5e62d172":"code","fafc7f1c":"code","7b99e75f":"code","d3a5d472":"code","8b832e35":"code","0be5d5ac":"code","9190abd7":"code","c80fd604":"code","44a8a290":"code","c0de6311":"code","2289f772":"code","596547e2":"code","028bdd49":"code","4c1842a4":"code","bb7235d1":"code","44328e74":"code","7d5924ab":"code","a58aa19e":"code","41006082":"markdown","6eaebabe":"markdown","330a34e5":"markdown","e288265f":"markdown","3656fb9d":"markdown","16bb13fe":"markdown","9336e041":"markdown","ce5a0493":"markdown","2645861a":"markdown"},"source":{"3854ea2b":"import joblib\nimport torch\ndevice = 'gpu' if torch.cuda.is_available() else 'cpu'\nimport numpy as np\nimport pandas as pd\npd.options.display.max_columns = 100\nimport matplotlib\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\nfrom sklearn.metrics import log_loss, accuracy_score\nfrom sklearn.preprocessing import QuantileTransformer, StandardScaler, PolynomialFeatures, LabelEncoder\nfrom sklearn.feature_selection import VarianceThreshold, SelectKBest\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport optuna\nimport tqdm\nimport gc\nimport os\nroot_path = '\/kaggle\/input\/tabular-playground-series-jun-2021\/'\n\ndef robust_pow(num_base, num_pow):\n    # numpy does not permit negative numbers to fractional power\n    # use this to perform the power algorithmic\n\n    return np.sign(num_base) * (np.abs(num_base)) ** (num_pow)\n\ndef focal_binary_object(pred, dtrain):\n    gamma_indct = 2.5\n    # retrieve data from dtrain matrix\n    label = dtrain.get_label()\n    # compute the prediction with sigmoid\n    sigmoid_pred = 1.0 \/ (1.0 + np.exp(-pred))\n    # gradient\n    # complex gradient with different parts\n    g1 = sigmoid_pred * (1 - sigmoid_pred)\n    g2 = label + ((-1) ** label) * sigmoid_pred\n    g3 = sigmoid_pred + label - 1\n    g4 = 1 - label - ((-1) ** label) * sigmoid_pred\n    g5 = label + ((-1) ** label) * sigmoid_pred\n    # combine the gradient\n    grad = gamma_indct * g3 * robust_pow(g2, gamma_indct) * np.log(g4 + 1e-9) + \\\n           ((-1) ** label) * robust_pow(g5, (gamma_indct + 1))\n    # combine the gradient parts to get hessian components\n    hess_1 = robust_pow(g2, gamma_indct) + \\\n             gamma_indct * ((-1) ** label) * g3 * robust_pow(g2, (gamma_indct - 1))\n    hess_2 = ((-1) ** label) * g3 * robust_pow(g2, gamma_indct) \/ g4\n    # get the final 2nd order derivative\n    hess = ((hess_1 * np.log(g4 + 1e-9) - hess_2) * gamma_indct +\n            (gamma_indct + 1) * robust_pow(g5, gamma_indct)) * g1\n\n    return grad, hess","3054d264":"#preprocessing\n\ntrain = pd.read_csv(os.path.join(root_path, 'train.csv'))\ntest = pd.read_csv(os.path.join(root_path, 'test.csv'))\nsample_submission = pd.read_csv(os.path.join(root_path, 'sample_submission.csv'))","6be2e140":"#label mapping\nunique_targets = train['target'].unique().tolist()\nlabel_mapping = dict(zip(unique_targets, [int(i[-1]) - 1 for i in unique_targets]))\n\ntrain['target'] = train['target'].map(label_mapping)\ndataset = pd.concat([train, test], axis = 0, ignore_index = True)\ntrain_len = len(train)\n\nfeatures = dataset.drop(['id', 'target'], axis=1).columns.tolist()\ncategorical_feature_columns = (dataset[features].apply(lambda x: x.nunique(), axis = 0)\n                               .rename('n_unique').to_frame()\n                               .query('n_unique < 10').index.tolist())\n\nlabel = LabelEncoder() \n#Not needed: in other challenges there were proper categorical string cols, so I did this and kept it here\n\nfor column in categorical_feature_columns:\n    label.fit(dataset[column])\n    dataset[column] = label.transform(dataset[column])\n        \ncategorical_features = list(range(len(categorical_feature_columns)))\n\ntrain_preprocessed = dataset[:train_len]\ntest_preprocessed = dataset[train_len:]\n\nassert train_preprocessed.shape[1] == test_preprocessed.shape[1]\n\ncat_indices = [features.index(i) for i in categorical_feature_columns]","756459bf":"train_target_counts = (train.target.value_counts().rename('count').to_frame().reset_index().rename({'index': 'class'}, axis = 1)\n                      .sort_values('class', ignore_index = True))\ncolors = sns.color_palette('rocket', 9)\nlevels = np.linspace(-1, 1, 9)\ncmap_plot, norm = matplotlib.colors.from_levels_and_colors(levels, colors, extend=\"max\")\n\nfig, ax = plt.subplots(1, 2, figsize = (18, 6), gridspec_kw={'width_ratios': [2, 1]})\n\nsns.barplot(data = train_target_counts, x = 'class', y = 'count', palette = 'rocket', ax = ax[0])\nplt.style.use('fivethirtyeight')\nplt.setp(ax[0].patches, linewidth=15)\n\nfor index, row in train_target_counts.iterrows():\n    value = row['count']\n    ax[0].text(index, value+1, value, color='black', ha=\"center\", \n               fontsize = 13, fontweight = 'bold')\n\nax[0].grid(True)\nax[0].legend(fontsize=18)\nax[0].set_title('Label Balance', fontsize = 18, fontweight = 'bold')\nax[0].tick_params(axis='both', which='major', labelsize=14)\nax[0].tick_params(axis='both', which='minor', labelsize=14)\nax[0].set_xlabel('')\nax[0].set_xticklabels(ax[0].get_xticklabels(), rotation = 35, fontsize = 13, color = 'black')\nax[0].set_ylabel('distinct_values', fontsize = 18, color ='black')\nplt.subplots_adjust(hspace = 0.3)\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.sans-serif'] = 'Helvetica'\n\nbbox=[-0.2, 0, 1.2, 0.9]\nax[1].axis('off')\nax[1].title.set_text('')\nccolors = plt.cm.BuPu(np.full(len(train_target_counts.columns), 0.1))\n\nmpl_table = ax[1].table(cellText = train_target_counts.values, bbox=bbox, colLabels=train_target_counts.columns, colColours=ccolors)\nmpl_table.auto_set_font_size(False)\nmpl_table.auto_set_column_width(col=list(range(len(train_target_counts.columns))))\nmpl_table.set_fontsize(18)","5e62d172":"unique_values = (train.drop(['id', 'target'], axis = 1).apply(lambda x: x.nunique(), axis = 0).rename('distinct_values').to_frame()\n.reset_index()\n.rename({'index': 'feature'}, axis = 1))\n\nplt.style.use('ggplot')\n\nfig, ax = plt.subplots(1, 1, figsize = (16, 6))#, gridspec_kw={'width_ratios': [1.5, 0.6]})\n\npercentiles_asked = [0.25, 0.5, 0.75, 0.9]\npercentiles = unique_values['distinct_values'].quantile(percentiles_asked).tolist()\n\nsns.histplot(data = unique_values, x = 'distinct_values', ax = ax, kde=False, bins = 20,\n             stat = 'density', \n             alpha = 0.5, \n             fill = True,\n             linewidth = 3,\n             edgecolor='black',\n             color = 'red',\n             #line_kws= {'linewidth': 5, 'color': 'red', 'alpha': 0.6}\n            )\n\nsns.kdeplot(data = unique_values, x = 'distinct_values', ax = ax, alpha = 0.01, fill = True, \n            linewidth = 5, color = 'blue')\n\nfor m, percentile in enumerate(percentiles):\n        ax.axvline(percentile, alpha = 0.35, ymin = 0, ymax = 1, linestyle = \":\", color = 'blue')\n        ax.text(percentile-0.16, 0.037, \"{}\".format(percentiles_asked[m]), size = 12, alpha = 1)\n        \nmean = np.round(unique_values.distinct_values.mean(), 2)\nmedian = np.round(unique_values.distinct_values.median(), 2)\nst_dev = np.round(unique_values.distinct_values.std(), 2)\n\nax.text(-10, 0.029, \"mean: {}\".format(mean), size = 12, alpha = 1)\nax.text(-10, 0.026, \"median: {}\".format(median), size = 12, alpha = 1)\nax.text(-10, 0.023, \"std deviation: {}\".format(st_dev), size = 12, alpha = 1)\n\n#https:\/\/stackoverflow.com\/questions\/49926147\/how-to-modify-edge-color-of-violinplot-using-seaborn\/55131881 \n#per cambiare colore linea esterna\n\nax.set_ylabel('Density', fontsize = 15)\nax.set_xlabel('distinct_values', fontsize = 15)\nax.set_title('hist-kde plot', fontsize = 16)\nax.set_ylim(0, 0.04)\n\nax.tick_params(axis='both', which='major', labelsize=14)\nax.tick_params(axis='both', which='minor', labelsize=14)\n#plt.subplots_adjust(hspace = 0.8)\nfig.suptitle('Distribution of Number of distinct values per feature (train)', fontsize = 20, fontweight = 'bold')\n","fafc7f1c":"unique_values = (test.drop(['id'], axis = 1).apply(lambda x: x.nunique(), axis = 0).rename('distinct_values').to_frame()\n.reset_index()\n.rename({'index': 'feature'}, axis = 1))\n\nplt.style.use('ggplot')\n\nfig, ax = plt.subplots(1, 1, figsize = (16, 6))#, gridspec_kw={'width_ratios': [1.5, 0.6]})\n\npercentiles_asked = [0.25, 0.5, 0.75, 0.9]\npercentiles = unique_values['distinct_values'].quantile(percentiles_asked).tolist()\n\nsns.histplot(data = unique_values, x = 'distinct_values', ax = ax, kde=False, bins = 20,\n             stat = 'density', \n             alpha = 0.5, \n             fill = True,\n             linewidth = 3,\n             edgecolor='black',\n             color = 'red',\n             #line_kws= {'linewidth': 5, 'color': 'red', 'alpha': 0.6}\n            )\n\nsns.kdeplot(data = unique_values, x = 'distinct_values', ax = ax, alpha = 0.01, fill = True, \n            linewidth = 5, color = 'blue')\n\nfor m, percentile in enumerate(percentiles):\n        ax.axvline(percentile, alpha = 0.35, ymin = 0, ymax = 1, linestyle = \":\", color = 'blue')\n        ax.text(percentile-0.16, 0.037, \"{}\".format(percentiles_asked[m]), size = 12, alpha = 1)\n        \nmean = np.round(unique_values.distinct_values.mean(), 2)\nmedian = np.round(unique_values.distinct_values.median(), 2)\nst_dev = np.round(unique_values.distinct_values.std(), 2)\n\nax.text(-10, 0.029, \"mean: {}\".format(mean), size = 12, alpha = 1)\nax.text(-10, 0.026, \"median: {}\".format(median), size = 12, alpha = 1)\nax.text(-10, 0.023, \"std deviation: {}\".format(st_dev), size = 12, alpha = 1)\n\n#https:\/\/stackoverflow.com\/questions\/49926147\/how-to-modify-edge-color-of-violinplot-using-seaborn\/55131881 \n#per cambiare colore linea esterna\n\nax.set_ylabel('Density', fontsize = 15)\nax.set_xlabel('distinct_values', fontsize = 15)\nax.set_title('hist-kde plot', fontsize = 16)\nax.set_ylim(0, 0.04)\n\nax.tick_params(axis='both', which='major', labelsize=14)\nax.tick_params(axis='both', which='minor', labelsize=14)\n#plt.subplots_adjust(hspace = 0.8)\nfig.suptitle('Distribution of Number of distinct values per feature (test)', fontsize = 20, fontweight = 'bold')\n","7b99e75f":"del train, test\ngc.collect()","d3a5d472":"#Set to False if you want to skip it\n\nOPTUNA_OPTIMIZATION = True\nN_SPLITS = 5 #Number of folds for validation\nN_TRIALS = 3 #Number of trials to find best hyperparameters\nTIME = 3600*5 #Time to run optimization (alternative to N_TRIALS)\nFOLD_RANDOM_SEED = 42\nREPEATED_FOLD = True #Whether to use RepeatedStratifiedKFold over StratifiedKFold\n\nFIXED_PARAMS = {\"random_state\": 42,\n                \"num_classes\": len(unique_targets),\n                \"categorical_feature\": cat_indices,\n                \"verbosity\": -1,\n                \"n_jobs\": -1}\n\nbest_params_v5 = {\"objective\": \"multiclass\",\n    \"boosting_type\": \"gbdt\",\n    \"n_estimators\": 100,\n    \"learning_rate\": 0.1,\n    \"num_leaves\": 22,\n    \"max_depth\": 10,\n    \"reg_alpha\": 15.457800377673841,\n    \"reg_lambda\": 8.958320766791369,\n    \"colsample_bytree\": 0.5508279412251145,\n    \"subsample\": 0.100790119987575,\n    \"cat_smooth\": 32.20219126721756}\n\nbest_params_v8 = {\n    \"objective\": \"multiclass\",\n    \"boosting_type\": \"gbdt\",\n    \"n_estimators\": 100,\n    \"learning_rate\": 0.1,\n    \"num_leaves\": 40,\n    \"max_depth\": 13,\n    \"reg_alpha\": 16.79089587101574,\n    \"reg_lambda\": 13.510484215533271,\n    \"colsample_bytree\": 0.3075172117556715,\n    \"subsample\": 0.08854097263254436,\n    \"cat_smooth\": 37.0724124612169}","8b832e35":"skfold = StratifiedKFold(N_SPLITS, shuffle = True, random_state = FOLD_RANDOM_SEED)\nif REPEATED_FOLD:\n    skfold = RepeatedStratifiedKFold(N_SPLITS, n_repeats=2, random_state=FOLD_RANDOM_SEED)\n\ndef objective(trial, cv=skfold):\n    \n    param_to_search_lgb = {\n        \"objective\": trial.suggest_categorical('objective', ['multiclass', 'multiclassova']),\n        \"boosting_type\": trial.suggest_categorical('boosting_type', ['gbdt', 'goss', 'dart']),\n        \"n_estimators\": trial.suggest_categorical('n_estimators', [100]),\n        \"learning_rate\": trial.suggest_categorical('learning_rate', [0.001, 0.005, 0.01, 0.05, 0.1]),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 1024),\n        'max_depth': trial.suggest_int('max_depth', -1, 16),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1E-16, 25),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1E-16, 25),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 1E-16, 0.7),\n        'subsample': trial.suggest_float('subsample', 1E-16, 0.3),\n        'cat_smooth': trial.suggest_float('cat_smooth', 1.0, 50.0)  \n    }\n    \n    param_lgb = param_to_search_lgb.copy()\n    param_lgb.update(FIXED_PARAMS)\n    \n    \n    val_losses = []\n    losses_1 = []\n    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, 'multi_logloss', valid_name='valid_1') \n    \n    for kfold, (train_idx, val_idx) in tqdm.tqdm(enumerate(cv.split(train_preprocessed[features].values, \n                                                                    train_preprocessed['target'].values))):\n        \n        X_train = train_preprocessed.loc[train_idx, features]\n        y_train = train_preprocessed.loc[train_idx, 'target']\n        \n        X_valid = train_preprocessed.loc[val_idx, features]\n        y_valid = train_preprocessed.loc[val_idx, 'target']\n        \n        d_train = lgb.Dataset(X_train, label=y_train)\n        d_valid = lgb.Dataset(X_valid, label=y_valid)\n        watchlist = [d_train, d_valid]\n        \n        model = lgb.train(param_lgb,\n                      train_set=d_train,\n                      valid_sets=watchlist,\n                      verbose_eval=0,\n                      early_stopping_rounds=100,\n                      callbacks=[pruning_callback])\n    \n        scores = model.predict(X_valid)\n        loss_1 = log_loss(y_valid, scores)\n        losses_1.append(loss_1)\n        \n    \n    return np.average(losses_1)","0be5d5ac":"if OPTUNA_OPTIMIZATION:\n    study = optuna.create_study(study_name = 'lgbm_parameter_opt', direction = 'minimize',\n                                pruner=optuna.pruners.MedianPruner(n_warmup_steps=25))\n    \n    study.enqueue_trial(best_params_v5)\n    study.enqueue_trial(best_params_v8)\n    #study.optimize(objective, n_trials=1, show_progress_bar=True)\n    study.optimize(objective, timeout=TIME, show_progress_bar=True) \n    \n    trial = study.best_trial\n    \n    print(\"  Value: {}\".format(trial.value))\n    \n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))\n    best_params = FIXED_PARAMS.copy()\n    best_params.update(trial.params)\n    best_params['n_estimators'] = 1000\n    \nelse:\n    trial = {\n            \"random_state\": 42,\n            \"metric\": \"auc\",\n            \"categorical_feature\": cat_indices,\n            \"verbosity\": -1,\n            \"n_estimators\": 20000,\n             'learning_rate': 0.1,\n             'num_leaves': 98,\n             'max_depth': 24,\n             'reg_alpha': 0.6013328384502188,\n             'reg_lambda': 8.864402629739141,\n             'colsample_bytree': 0.8295666531935949,\n             'subsample': 0.5621932264483348,\n             'cat_smooth': 31.788282544015413}\n    best_params=trial","9190abd7":"#if you wish to save the studya\nimport joblib\njoblib.dump(study, 'study.pkl')","c80fd604":"if OPTUNA_OPTIMIZATION:\n    display(optuna.visualization.plot_intermediate_values(study))","44a8a290":"if OPTUNA_OPTIMIZATION:\n    display()","c0de6311":"if OPTUNA_OPTIMIZATION:\n    display(optuna.visualization.plot_optimization_history(study, target_name = 'Average Validation LogLoss'))","2289f772":"if OPTUNA_OPTIMIZATION:\n    display(optuna.visualization.plot_slice(study, target_name = 'Average Validation LogLoss'))","596547e2":"if OPTUNA_OPTIMIZATION:\n    display(optuna.visualization.plot_parallel_coordinate(study, target_name = 'Average Validation LogLoss'))","028bdd49":"if OPTUNA_OPTIMIZATION:\n    display(study.trials_dataframe())","4c1842a4":"if OPTUNA_OPTIMIZATION:\n    final_model = LGBMClassifier(**best_params)\nelse:\n    final_model = LGBMClassifier(**trial)","bb7235d1":"test_preds = []\naccuracies = []\nloglosses = []\n\nfor kfold, (train_idx, val_idx) in enumerate(skfold.split(train_preprocessed[features].values, \n                                                          train_preprocessed['target'].values)):\n        \n        final_model.fit(train_preprocessed.loc[train_idx, features], \n                        train_preprocessed.loc[train_idx, 'target'])\n        print('Fitted {}'.format(type(final_model).__name__))\n        \n        val_true = train_preprocessed.loc[val_idx, 'target'].values\n        \n        preds = final_model.predict(train_preprocessed.loc[val_idx, features])\n        probs = final_model.predict_proba(train_preprocessed.loc[val_idx, features])\n        \n        accuracy = accuracy_score(val_true, preds)\n        accuracies.append(accuracy)\n        print('Fold: {}\\t Validation Accuracy: {}\\n'.format(kfold, accuracy))\n        \n        logloss = log_loss(val_true, probs)\n        loglosses.append(logloss)\n        print('Fold: {}\\t Validation logloss: {}\\n'.format(kfold, logloss))\n        \n        test_preds.append(final_model.predict_proba(test_preprocessed[features]))\n        \nprint(\"Best Parameters mean Accuracy: {}\".format(np.mean(accuracies)))\nprint(\"Best Parameters mean logloss: {}\".format(np.mean(loglosses)))","44328e74":"test_predictions = np.mean(test_preds, axis = 0)\nassert len(test_predictions) == len(test_preprocessed)","7d5924ab":"predictions_df = pd.DataFrame(test_predictions, columns = sorted(unique_targets))\npredictions_df['id'] = sample_submission['id']","a58aa19e":"predictions_df.to_csv(\"submission.csv\", index = False)","41006082":"_Objective_","6eaebabe":"<a id = \"optuna_objective\"><\/a>\n<h5> Define Objective <\/h5>\n\n1. Possibility to use [RepeatedStratifiedKFold](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RepeatedStratifiedKFold.html) for cross validation\n\n2. Define starting parameters for Optuna (read [here](https:\/\/github.com\/optuna\/optuna\/issues\/417)), using `enqueue_trial`\n\n3. Optimizing more than one loss: I've included [focal loss](https:\/\/paperswithcode.com\/method\/focal-loss), which in other contexts has done well with class imbalance.\n\n4. Saving your Optuna study","330a34e5":"<a id = \"optuna_plots\"><\/a>\n<h6> Check Optimization plots <\/h6>","e288265f":"<a id=\"optuna\"><\/a>\n\n### Optuna\n\nLook [here](https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/) for reference about Optuna library. \n\nLook [here](https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.LGBMClassifier.html) for a set of Lightgbm Classifier hyperparameters.","3656fb9d":"<a id = \"submission\"><\/a>\n\n### Submission","16bb13fe":"<a id = \"optuna_study\"><\/a>\n<h6> Start Optimization <\/h6>","9336e041":"<a id=\"loading\"><\/a>\n\n##### 0. Imports, Data Loading and Preprocessing","ce5a0493":"<a id = \"eda\"><\/a>\n<h4> Exploratory Data Analysis <\/h4>","2645861a":"## Tabular Playground Series June 2021\n\n<img src=\"https:\/\/i.imgur.com\/uHVJtv0.png\">\n<img src=\"https:\/\/lightgbm.readthedocs.io\/en\/latest\/_images\/LightGBM_logo_black_text.svg\">\n\n<br><br>\n\n### Notebook Contents:\n\n<div id=\"toc_container\" style=\"background: #f9f9f9; border: 1px solid #aaa; display: table; font-size: 95%;\n                               margin-bottom: 1em; padding: 20px; width: auto;\">\n<p class=\"toc_title\" style=\"font-weight: 700; text-align: center\">Notebook Contents<\/p>\n<ul class=\"toc_list\">\n  <li><a href=\"#loading\">0. Imports, Data Loading and Preprocessing<\/a>\n  <li><a href=\"#eda\">1. Exploratory Data Analysis<\/a>\n  <li><a href=\"#optuna\">2. Optuna Hyperparameter Optimization<\/a>\n      <br>\n      <ul>\n    <li><a href=\"#optuna_objective\">2.0 Define Objective<\/a><\/li>\n    <li><a href=\"#optuna_study\">2.1 Start Optimization<\/a><\/li>\n    <li><a href=\"#optuna_plots\">2.2 Check Optimization Plots<\/a><\/li>\n  <\/ul>\n<\/li>\n<li><a href=\"#submission\">3. Submission<\/a><\/li>\n<\/ul>\n<\/div>\n\n##### Props\n\nProps to [corochann](https:\/\/www.kaggle.com\/corochann\/optuna-tutorial-for-hyperparameter-optimization), I believe this notebook is the best you can find about Optuna.\n\n--- \n\n<h5> Disclaimer <\/h5>\n\nCode mainly taken from my [TPS May notebook](https:\/\/www.kaggle.com\/tomwarrens\/tps-may-2021-lightgbm-optuna)."}}