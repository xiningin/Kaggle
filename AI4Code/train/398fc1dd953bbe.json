{"cell_type":{"6e6db3c6":"code","1ab39457":"code","7ebffa57":"code","13034e64":"code","3f79d59e":"code","4c7cbd90":"code","fe2d3209":"code","f8231d81":"code","5a7f56c1":"code","54b7fdf2":"code","0d5c3a64":"code","ea736dda":"code","73ae6e20":"code","69b29a8a":"code","85fe1849":"code","d5340833":"code","08bacb73":"code","b2e7edfa":"markdown","745a3d44":"markdown"},"source":{"6e6db3c6":"  import numpy as np\n  from sklearn.linear_model import LogisticRegression\n  from sklearn import svm\n  from sklearn import datasets\t\t\n  from sklearn import svm    \t\t\t\n  import numpy as np\n  import matplotlib.pyplot as plt   \n  from sklearn.ensemble import RandomForestClassifier, VotingClassifier","1ab39457":"# import iris data to model Svm classifier\nI_D=datasets.load_iris()","7ebffa57":"I_D['data']","13034e64":"I_D['target']","3f79d59e":"#visualize_sepal_data taking first two features\nx=I_D.data[:,:2]  \ny=I_D.target\nplt.scatter(x[:,0],x[:,1], c=y)\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\nplt.title('Sepal Width & Length')\nplt.show()","4c7cbd90":"#visualize_petal_data taking last two features\nx=I_D.data[:,2:]  \ny=I_D.target\nplt.scatter(x[:,0],x[:,1], c=y)\nplt.xlabel('Petal length')\nplt.ylabel('Petal width')\nplt.title('Petal Width & Length')\nplt.show()","fe2d3209":"# Now we use a spilt function\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(I_D.data,I_D.target,test_size=0.2)","f8231d81":"#we apply a support vector m\/c\n\n#SVM model with linear kernel\nx_sepal=x_train[:,:2] \ny_sepal=y_train\n\nsvc=svm.SVC(kernel='linear', C=2.8).fit(x_sepal,y_sepal)","5a7f56c1":"# LinearSVC (linear kernel)\nlin_svc = svm.LinearSVC(C=2.8,max_iter=3000).fit(x_sepal, y_sepal)\n\n# SVC with RBF kernel\nrbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=2.8).fit(x_sepal, y_sepal)\n\n# SVC with polynomial (degree 3) kernel\npoly_svc = svm.SVC(kernel='poly', degree=3, C=2.8).fit(x_sepal, y_sepal)","54b7fdf2":"# Sepal test parformance\n\ny_pred_svc=svc.predict(x_test[:,:2])\ny_pred_lin_svc=lin_svc.predict(x_test[:,:2])\ny_pred_rbf_svc=rbf_svc.predict(x_test[:,:2])\ny_pred_poly_svc=poly_svc.predict(x_test[:,:2])\n\n#print Sepal test Praformance\n\nfrom sklearn.metrics import accuracy_score as ac\nprint(\"Accuracy Score:\")\nprint('SVM with kernel(linear):',ac(y_test,y_pred_svc)*100)\nprint('SVM with linear svc:',ac(y_test,y_pred_lin_svc)*100)\nprint('SVM with kernel(rbf):',ac(y_test,y_pred_rbf_svc)*100)\nprint('SVM with kernel(poly):',ac(y_test,y_pred_poly_svc)*100)","0d5c3a64":"# sepal training performance\n\ny_pred_svc=svc.predict(x_sepal)\ny_pred_lin_svc=lin_svc.predict(x_sepal)\ny_pred_rbf_svc=rbf_svc.predict(x_sepal)\ny_pred_poly_svc=poly_svc.predict(x_sepal)\n\n# print for sepal training performance\n\nfrom sklearn.metrics import accuracy_score as ac\nprint(\"Accuracy Score:\")\nprint('SVM with kernel(linear):',ac(y_sepal,y_pred_svc)*100)\nprint('SVM with linear svc:',ac(y_sepal,y_pred_lin_svc)*100)\nprint('SVM with kernel(rbf):',ac(y_sepal,y_pred_rbf_svc)*100)\nprint('SVM with kernel(poly):',ac(y_sepal,y_pred_poly_svc)*100)\n","ea736dda":"#SVM model with linear kernel\nx_petal=x_train[:,2:] \ny_petal=y_train\n\nsvc1=svm.SVC(kernel='linear', C=1.0).fit(x_petal,y_petal)","73ae6e20":"# LinearSVC (linear kernel)\nlin_svc1 = svm.LinearSVC(C=1.0,max_iter=3000).fit(x_petal, y_petal)\n\n# SVC with RBF kernel\nrbf_svc1 = svm.SVC(kernel='rbf', gamma=0.7, C=1.0).fit(x_petal, y_petal)\n\n# SVC with polynomial (degree 3) kernel\npoly_svc1 = svm.SVC(kernel='poly', degree=5, C=1.0).fit(x_petal, y_petal)","69b29a8a":"# Testing performance for Petal\ny_pred_svc1=svc.predict(x_test[:,2:])\ny_pred_lin_svc1=lin_svc.predict(x_test[:,2:])\ny_pred_rbf_svc1=rbf_svc.predict(x_test[:,2:])\ny_pred_poly_svc1=poly_svc.predict(x_test[:,2:])\n\n# print testing performance for Petal\n\nprint(\"Accuracy Score:\")\nprint('SVM with kernel(linear):',ac(y_test,y_pred_svc1)*100)\nprint('SVM with linear svc:',ac(y_test,y_pred_lin_svc1)*100)\nprint('SVM with kernel(rbf):',ac(y_test,y_pred_rbf_svc1)*100)\nprint('SVM with kernel(poly):',ac(y_test,y_pred_poly_svc1)*100)","85fe1849":"# Petal training performance\n\ny_pred_svc1=svc.predict(x_petal)\ny_pred_lin_svc1=lin_svc.predict(x_petal)\ny_pred_rbf_svc1=rbf_svc.predict(x_petal)\ny_pred_poly_svc1=poly_svc.predict(x_petal)\n\n# print for sepal training performance\n\nfrom sklearn.metrics import accuracy_score as ac\nprint(\"Accuracy Score:\")\nprint('SVM with kernel(linear):',ac(y_petal,y_pred_svc1)*100)\nprint('SVM with linear (svc):',ac(y_petal,y_pred_lin_svc1)*100)\nprint('SVM with kernel(rbf):',ac(y_petal,y_pred_rbf_svc1)*100)\nprint('SVM with kernel(poly):',ac(y_petal,y_pred_poly_svc1)*100)\n\n\n","d5340833":"  clf1 = LogisticRegression(multi_class='multinomial', random_state=1)\n  clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n  clf3 = svm.SVC()","08bacb73":"##VOTING CLASSIFIER\nfrom sklearn.ensemble import VotingClassifier\n\n#create a dictionary of our models\nestimators=[('log_reg', clf1), ('rf', clf2), ('svm', clf3)]\n\n#create our voting classifier, inputting our models\nensemble = VotingClassifier(estimators, voting='hard')\n\n#fit model to training data\nensemble.fit(x_sepal, y_sepal)\n\n#test our model on the test data\nensemble.score(x_sepal, y_sepal)","b2e7edfa":"2. Use iris flower dataset to create classification model. Your task is to predict the class to which \nthese plants belong. There are three classes in the dataset: Iris-setosa, Iris-versicolor and Iris\u0002virginica. Create the classification model using Random Forest classifier and evaluate the \nperformance of your classifier. Also you need to do the feature analysis and find out the important \nfeature in iris dataset\n","745a3d44":"# SVM Model for Petal"}}