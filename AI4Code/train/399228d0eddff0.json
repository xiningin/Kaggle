{"cell_type":{"e343a3e1":"code","d74fa959":"code","f1b95cb2":"code","0137991e":"code","91b7306b":"code","b202b094":"code","e87cc099":"code","49be0d9d":"code","a6d9485e":"code","d984e486":"code","644a3ffb":"code","d2f73577":"code","2c4fa99b":"code","839c7c2f":"code","6bdb93a5":"code","36d51a0a":"code","58e3055a":"code","adc8c49d":"code","750a9bc4":"code","764b1644":"code","72be9bb2":"code","2ba2d0ef":"code","c92b6eb0":"code","c8f99090":"code","1302d3b0":"code","f07d9bf6":"code","6a6e37ed":"code","1e56528b":"code","120eabd5":"code","c2bd0b28":"code","da1cd5cb":"code","d10f7b32":"code","52bcc9a1":"code","047f8a58":"code","30384bd8":"code","400d222f":"code","cfd2d7dd":"code","2a49d7a2":"code","c7b9ad86":"code","c87b1edd":"code","8dacd339":"code","74a28225":"code","41e54166":"code","238343a3":"code","afa870a5":"code","a9d9a384":"code","c2e3b86a":"code","9a501e36":"code","14c52218":"code","b2b5607c":"code","24a92786":"code","f27995d5":"code","7c4c964b":"code","557b8cfe":"code","905c04b6":"code","5f01184c":"code","3b1e285a":"code","f1498acc":"code","28ddb0d5":"code","e289172a":"code","cee6866b":"code","9aaa809d":"code","47c3453f":"code","bc45862b":"code","23a81793":"code","d51df391":"code","a67b215c":"code","af03558f":"code","7d3069a6":"code","6827bdae":"code","d5fc330e":"code","a981a38e":"code","bd9ce68f":"code","a164923e":"code","941f95e7":"code","a7cbb89b":"code","9aad675f":"code","425792f8":"code","876b5351":"code","5cf2a3b4":"code","495c5e99":"code","077e6655":"code","f6bc5076":"code","61a761d8":"code","1b798804":"code","a6debb0f":"code","03540232":"code","f54ed678":"code","1c97a283":"code","0b5131d9":"code","4ef5e145":"code","eec0a002":"code","8545d82a":"code","6189c0a0":"code","e78abbe3":"markdown","c48e15cf":"markdown","80d3c495":"markdown","1baaa38d":"markdown","7d533dd7":"markdown","fcb31bdc":"markdown","b45eae59":"markdown","35c13412":"markdown","558aff33":"markdown","bd5f5c41":"markdown","f6a47250":"markdown","1a942b58":"markdown","a8e39f3b":"markdown","b0692cec":"markdown","d3499197":"markdown","f02533b6":"markdown","980e4594":"markdown","ecb4c1d9":"markdown","4fe785fe":"markdown","eb1694c1":"markdown","591d8232":"markdown","05afe05f":"markdown","2e819ec0":"markdown","2baf4439":"markdown","e7dcc2a6":"markdown","60df1799":"markdown","0a10ded6":"markdown","dff80fae":"markdown","2cb33d1b":"markdown","581612ff":"markdown","56956818":"markdown","80a2f892":"markdown","eb61677d":"markdown","7f1d663d":"markdown","0e797df6":"markdown","91d42ea1":"markdown","cc0d2b55":"markdown","f5c3f7f7":"markdown","afa6518c":"markdown","738c8b70":"markdown","88c64e4a":"markdown","552ea5fa":"markdown","a478a2bc":"markdown","b4b7e315":"markdown","56bd12d7":"markdown","3902c8a2":"markdown","22e39ada":"markdown","6808dc51":"markdown","40fcf156":"markdown"},"source":{"e343a3e1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","d74fa959":"train = pd.read_csv('..\/input\/santander-value-prediction-challenge\/train.csv')","f1b95cb2":"train.shape","0137991e":"train.head()","91b7306b":"train.tail()","b202b094":"train.info()","e87cc099":"dtype_df = train.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df.groupby(\"Column Type\").aggregate('count').reset_index()","49be0d9d":"list(train.select_dtypes(['object']).columns)","a6d9485e":"int_f = list(train.select_dtypes(['int']).columns)\nfloat_f = list(train.select_dtypes(['float']).columns)\n\nlen(int_f), len(float_f)","d984e486":"for f in float_f:\n    train[f] = train[f].astype('float32')","644a3ffb":"dtype_df = train.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df.groupby(\"Column Type\").aggregate('count').reset_index()","d2f73577":"train.isnull().sum()","2c4fa99b":"missing_df = train.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df = missing_df[missing_df['missing_count']>0]\nmissing_df = missing_df.sort_values(by='missing_count')\nmissing_df","839c7c2f":"# randomly took one columns\ntrain['d5308d8bc'].nunique()","6bdb93a5":"unique_df = train.nunique().reset_index()\nunique_df.columns = [\"col_name\", \"unique_count\"]","36d51a0a":"unique_df","58e3055a":"constant_df = unique_df[unique_df[\"unique_count\"]==1]\nconstant_df","adc8c49d":"# train['9fc776466'].nunique()\ntrain['9fc776466'].unique()","750a9bc4":"# constant_df.col_name.tolist()\nprint('Original Shape of Train Dataset {}'.format(train.shape))\ntrain.drop(constant_df.col_name.tolist(), axis = 1, inplace = True)\nprint('Shape after dropping Constant Columns from Train Dataset {}'.format(train.shape))","764b1644":"train.head()","72be9bb2":"train_ids_df = train['ID']\ntrain_ids_df.head()","2ba2d0ef":"train.drop('ID', axis = 1, inplace = True)","c92b6eb0":"train.head()","c8f99090":"# k = 15 # Number of variables for heatmap.\n# target = 'target'\n\n# cols = train[int_f].corr().nlargest(k, target)[target].index\n\n# cm = train[cols].corr()\n\n# plt.figure(figsize = (10, 6))\n\n# sns.heatmap(cm, annot = True, cmap = 'viridis')","1302d3b0":"from sklearn.feature_selection import SelectKBest, chi2","f07d9bf6":"select_feature = SelectKBest(score_func=chi2, k = 5)","6a6e37ed":"X = train.drop('target', axis = 1)\ny = train['target']\n\nX.shape, y.shape","1e56528b":"# select_feature.fit_transform(X, y)","120eabd5":"train['target'].describe()","c2bd0b28":"plt.figure(figsize=(8,6))\nplt.scatter(range(train.shape[0]), np.sort(train['target'].values))\nplt.xlabel('Index --> ', fontsize=12)\nplt.ylabel('Target --> ', fontsize=12)\nplt.title(\"Target Distribution\", fontsize=14)\nplt.show()","da1cd5cb":"plt.figure(figsize=(12,8))\nsns.distplot(train[\"target\"].values, bins=50, kde=True)\nplt.xlabel('Target --> ', fontsize=12)\nplt.title(\"Target Histogram\", fontsize=14)\nplt.show()","d10f7b32":"# Taking log of target variable and re-check the same...\nplt.figure(figsize=(12,8))\nsns.distplot( np.log1p(train[\"target\"].values), bins=50, kde=True)\nplt.xlabel('Target --> ', fontsize=12)\nplt.title(\"Log of Target Histogram\", fontsize=14)\nplt.show()","52bcc9a1":"# from scipy.stats import spearmanr\n# import warnings\n# warnings.filterwarnings(\"ignore\")\n\n# labels = []\n# values = []\n# for col in train.columns:\n#     if col not in [\"ID\", \"target\"]:\n#         labels.append(col)\n#         values.append(spearmanr(train[col].values, train[\"target\"].values)[0])\n# corr_df = pd.DataFrame({'col_labels':labels, 'corr_values':values})\n# corr_df = corr_df.sort_values(by='corr_values')\n \n# corr_df = corr_df[(corr_df['corr_values']>0.1) | (corr_df['corr_values']<-0.1)]\n# ind = np.arange(corr_df.shape[0])\n# width = 0.9\n# fig, ax = plt.subplots(figsize=(12,30))\n# rects = ax.barh(ind, np.array(corr_df.corr_values.values), color='b')\n# ax.set_yticks(ind)\n# ax.set_yticklabels(corr_df.col_labels.values, rotation='horizontal')\n# ax.set_xlabel(\"Correlation coefficient\")\n# ax.set_title(\"Correlation coefficient of the variables\")\n# plt.show()","047f8a58":"# cols_to_use = corr_df[(corr_df['corr_values']>0.11) | (corr_df['corr_values']<-0.11)].col_labels.tolist()\n\n# temp_df = train[cols_to_use]\n# corrmat = temp_df.corr(method='spearman')\n# f, ax = plt.subplots(figsize=(20, 20))\n\n# # Draw the heatmap using seaborn\n# sns.heatmap(corrmat, vmax=1., square=True, cmap=\"YlGnBu\", annot=True)\n# plt.title(\"Important variables correlation map\", fontsize=15)\n# plt.show()","30384bd8":"# ### Get the X and y variables for building model ###\n# train_X = train.drop([\"ID\", \"target\"], axis=1)\n# train_y = np.log1p(train[\"target\"].values)","400d222f":"# from sklearn import ensemble\n# model = ensemble.ExtraTreesRegressor(n_estimators=200, max_depth=20, max_features=0.5, n_jobs=-1, random_state=0)\n# model.fit(train_X, train_y)\n\n# ## plot the importances ##\n# feat_names = train_X.columns.values\n# importances = model.feature_importances_\n# std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n# indices = np.argsort(importances)[::-1][:20]\n\n# plt.figure(figsize=(12,12))\n# plt.title(\"Feature importances\")\n# plt.bar(range(len(indices)), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\n# plt.xticks(range(len(indices)), feat_names[indices], rotation='vertical')\n# plt.xlim([-1, len(indices)])\n# plt.show()","cfd2d7dd":"from sklearn.model_selection import train_test_split \n  \nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = 0) ","2a49d7a2":"X.shape","c7b9ad86":"X_train.shape, X_valid.shape, y_train.shape, y_valid.shape","c87b1edd":"from sklearn.preprocessing import StandardScaler \nsc = StandardScaler() ","8dacd339":"X_train = sc.fit_transform(X_train) \nX_valid = sc.transform(X_valid)","74a28225":"from sklearn.decomposition import PCA \n  \npca = PCA(n_components = 10) ","41e54166":"X_train_pca = pca.fit_transform(X_train) \nX_valid_pca = pca.transform(X_valid) ","238343a3":"explained_variance = pca.explained_variance_ratio_ ","afa870a5":"explained_variance","a9d9a384":"X_train_pca.shape, X_valid_pca.shape","c2e3b86a":"len(pca.components_)","9a501e36":"# df_comp = pd.DataFrame(pca.components_, X.columns) \n  \n# plt.figure(figsize =(14, 6)) \n  \n# # plotting heatmap \n# sns.heatmap(df_comp) ","14c52218":"from xgboost import XGBRegressor","b2b5607c":"clf_xgb = XGBRegressor()\nclf_xgb.fit(X_train_pca, y_train)","24a92786":"y_pred = abs(clf_xgb.predict(X_valid_pca))","f27995d5":"# import math\n\n# #A function to calculate Root Mean Squared Logarithmic Error (RMSLE)\n# def rmsle(y, y_pred):\n#     assert len(y) == len(y_pred)\n#     terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n#     return (sum(terms_to_sum) * (1.0\/len(y))) ** 0.5","7c4c964b":"# rmsle_value = rmsle(y_valid, y_pred)\n# rmsle_value","557b8cfe":"from sklearn.metrics import mean_squared_log_error\nnp.sqrt(mean_squared_log_error( y_valid, y_pred ))","905c04b6":"y_valid.describe()","5f01184c":"min(y_pred), max(y_pred)","3b1e285a":"test_df = pd.read_csv(\"..\/input\/santander-value-prediction-challenge\/test.csv\")","f1498acc":"test_df.shape","28ddb0d5":"test_df.head()","e289172a":"float_f = list(test_df.select_dtypes(['float']).columns)","cee6866b":"for f in float_f:\n    test_df[f] = test_df[f].astype('float32')","9aaa809d":"dtype_test_df = train.dtypes.reset_index()\ndtype_test_df.columns = [\"Count\", \"Column Type\"]\ndtype_test_df.groupby(\"Column Type\").aggregate('count').reset_index()","47c3453f":"test_df.drop(constant_df.col_name.tolist(), axis = 1, inplace = True)\ntest_df.shape","bc45862b":"test_ids_df = test_df['ID']\ntest_ids_df.head()","23a81793":"test_df.drop('ID', axis = 1, inplace = True)\ntest_df.shape","d51df391":"test_df = sc.transform(test_df)","a67b215c":"test_df_pca = pca.transform(test_df) ","af03558f":"test_df_pca.shape","7d3069a6":"test_df_pca[:5]","6827bdae":"test_df_pca_5 = test_df_pca[:5]","d5fc330e":"pred_test_full = abs(clf_xgb.predict(test_df_pca_5))\npred_test_full","a981a38e":"pred_test_full = abs(clf_xgb.predict(test_df_pca))\n# pred_test_full","bd9ce68f":"len(pred_test_full)","a164923e":"# pred_test_full = []\n\n# pr = clf_xgb.predict(test_df_pca_5)\n# # print(len(pr))\n\n# pred_test_full.append(pr.tolist())\n\n# # pr\n# # print(len(pred_test_full))\n# # pred_test_full\n\n# pr = clf_xgb.predict(test_df_pca[5:10])\n# pred_test_full.append(pr.tolist())\n\n# # print(len(pred_test_full))\n\n# pred_test_full","941f95e7":"# flat_list = [item for sublist in pred_test_full for item in sublist]\n# flat_list","a7cbb89b":"# pr.tolist()","9aad675f":"# test_df_pca[0]\n# np.reshape(test_df_pca[0],(1, test_df_pca[0].size))","425792f8":"# pred_test_full = []\n\n# # for i in range(0,50) :\n# for i in range(0,test_df_pca.shape[0]) :\n#     pr = clf_xgb.predict(np.reshape(test_df_pca[i],(1, test_df_pca[i].size)))\n#     pred_test_full.append(pr.tolist())\n    \n# flat_list = [item for sublist in pred_test_full for item in sublist]\n# len(flat_list)","876b5351":"# l = train.shape[0]\n# c = l\n\n# pred_test_full = []\n\n# for i in range(0, len(l), 20):\n#     pred_test_full.append(test_df_pca[i:c])\n#     c += l","5cf2a3b4":"# flat_list = [item for sublist in pred_test_full for item in sublist]\n# len(flat_list)","495c5e99":"# import lightgbm as lgb\n# import xgboost as xgb\n# from catboost import CatBoostRegressor","077e6655":"def run_lgb(train_X, train_y, val_X, val_y, test_X):\n    params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\",\n        \"num_leaves\" : 40,\n        \"learning_rate\" : 0.004,\n        \"bagging_fraction\" : 0.6,\n        \"feature_fraction\" : 0.6,\n        \"bagging_frequency\" : 6,\n        \"bagging_seed\" : 42,\n        \"verbosity\" : -1,\n        \"seed\": 42\n    }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    evals_result = {}\n    model = lgb.train(params, lgtrain, 5000, \n                      valid_sets=[lgtrain, lgval], \n                      early_stopping_rounds=100, \n                      verbose_eval=150, \n                      evals_result=evals_result)\n    \n    pred_test_y = np.expm1(model.predict(test_X, num_iteration=model.best_iteration))\n    return pred_test_y, model, evals_result","f6bc5076":"# # Training LGB\n# pred_lgb, model, evals_result = run_lgb(X_train_pca, y_train, X_valid_pca, y_valid, test_df_pca)\n# print(\"LightGBM Training Completed...\")","61a761d8":"# # feature importance\n# print(\"Features Importance...\")\n# gain = model.feature_importance('gain')\n# featureimp = pd.DataFrame({'feature':model.feature_name(), \n#                    'split':model.feature_importance('split'), \n#                    'gain':100 * gain \/ gain.sum()}).sort_values('gain', ascending=False)\n# print(featureimp[:50])","1b798804":"def run_xgb(train_X, train_y, val_X, val_y, test_X):\n    params = {'objective': 'reg:linear', \n          'eval_metric': 'rmse',\n          'eta': 0.001,\n          'max_depth': 10, \n          'subsample': 0.6, \n          'colsample_bytree': 0.6,\n          'alpha':0.001,\n          'random_state': 42, \n          'silent': True}\n    \n    tr_data = xgb.DMatrix(train_X, train_y)\n    va_data = xgb.DMatrix(val_X, val_y)\n    \n    watchlist = [(tr_data, 'train'), (va_data, 'valid')]\n    \n    model_xgb = xgb.train(params, tr_data, 2000, watchlist, maximize=False, early_stopping_rounds = 100, verbose_eval=100)\n    \n    dtest = xgb.DMatrix(test_X)\n    xgb_pred_y = np.expm1(model_xgb.predict(dtest, ntree_limit=model_xgb.best_ntree_limit))\n    \n    return xgb_pred_y, model_xgb","a6debb0f":"# # Training XGB\n# pred_test_xgb, model_xgb = run_xgb(X_train_pca, y_train, X_valid_pca, y_valid, test_df_pca)\n# print(\"XGB Training Completed...\")","03540232":"# cb_model = CatBoostRegressor(iterations=500,\n#                              learning_rate=0.05,\n#                              depth=10,\n#                              eval_metric='RMSE',\n#                              random_seed = 42,\n#                              bagging_temperature = 0.2,\n#                              od_type='Iter',\n#                              metric_period = 50,\n#                              od_wait=20)","f54ed678":"# cb_model.fit(X_train_pca, y_train,\n#              eval_set=(X_valid_pca, y_valid),\n#              use_best_model=True,\n#              verbose=50)\n ","1c97a283":"# pred_test_cat = np.expm1(cb_model.predict(test_df_pca))","0b5131d9":"# from sklearn.metrics import mean_squared_log_error\n# np.sqrt(mean_squared_log_error( y_valid, y_pred ))","4ef5e145":"subm_sample = pd.read_csv('..\/input\/santander-value-prediction-challenge\/sample_submission.csv')\nsubm_sample.head()","eec0a002":"# test_ids_df.head()\ntest_ids_df[:]","8545d82a":"subm_df = pd.DataFrame({\"ID\":test_ids_df[:]})\nsubm_df[\"target\"] = pred_test_full\nsubm_df.to_csv(\"XGBReg_v1.csv\", index=False)","6189c0a0":"subm_df.head()","e78abbe3":"* Majority of the columns are of integer type and the rest are float type. \n* There is only one string column which is nothing but 'ID' column.","c48e15cf":"As column list is huge, lets take a DataFrame and capture the missing columns and their count. ","80d3c495":"From Histogram we could see that the majority of the data points are having low value.","1baaa38d":"Cool... no missing values in the dataset :)\n\nBut we have seen lots of zeros.. not sure if it represent somethig specific or its a missing data. Considering Zero represent some logical value.","7d533dd7":"# Feature Scaling\n\nDoing the pre-processing part on training and validation data set such as fitting the Standard scale. For testing dataset will do it later.","fcb31bdc":"# Applying PCA function\nApplying the PCA function into training and validation set for analysis.","b45eae59":"# XGB ","35c13412":"# Target Exploration","558aff33":"* So we have 4459 rows in train dataset. \n* We also have 4993 columns in total including the target and id column.\n* First time i am seeing where the number of columns are more than the data points (data rows). So need to be careful with feature selection \/ engineering.\n* We are provided with an anonymized dataset containing numeric feature variables, the numeric target column, and a string ID column. so we do not know what they mean.\n* The task is to predict the value of target column in the test set.\n* There are many zero values present in the data, this is just by looking top and last 5 records.","bd5f5c41":"Drop ID feature as well","f6a47250":"Due to huge count of columns, we could not use the info() method.\n\nLets have our own Data Frame to get the counf of different Data Types.","1a942b58":"Will try with going into loops.","a8e39f3b":"`ID` is of object type and we will not use it for our prediction, so no need to worry about it.","b0692cec":"<!-- Seems like none of the selected variables have spearman correlation more than 0.7 with each other.\n\nThe above plots helped us in identifying the important individual variables which are correlated with target. However we generally build many non-linear models in Kaggle competitions. So let us build some non-linear models and get variable importance from them.\n\nIn this notebook, we will build two models to get the feature importances - Extra trees and Light GBM. It could also help us to see if the important features coming out from both of them are consistent. Let us first start with ET model. -->","d3499197":"<!-- # Feature Importance - Extra trees model\n\nOur Evaluation metric for the competition is RMSLE. So let us use log of the target variable to build our models. Also please note that we are removing those variables with constant values (that we identified earlier). -->","f02533b6":"Using the SKLEarn","980e4594":"# Checking Data Type for each columns.","ecb4c1d9":"# Correlation\nAs the count of columns are huge.. unable to work on Correlation","4fe785fe":"> XGBClassifier is executed for 1+ hrs and yet did not resulted any predictions.. so i have forecfull stopped it. Will try other Algoriths.","eb1694c1":"Lets see the expected submission format.","591d8232":"Seems range is very high, and no visible outliers in the data.\n\nWe can now do a histogram plot of the target variable, lets see our findings.","05afe05f":"# Check for any Missing data","2e819ec0":"# Feature Selection\n\n## Using SelectKBest (Univariate Feature Selection)","2baf4439":"Transform PCA on to test_df","e7dcc2a6":"# Check for Uniqueness of data","60df1799":"# Splitting the dataset into the Training set and Validation set.\n","0a10ded6":"Scale the dataset","dff80fae":"# Working on Test DataSet","2cb33d1b":"# Import Libraries.","581612ff":"# RMSLE Function","56956818":"# Catboost","80a2f892":"<!-- # Correlation\nCopied the code from SRK (sudalairajkumar), thanks for the code. -->","eb61677d":"Seems for randomly selected one of the column 'd5308d8bc' has only single value in the entire data-set. For such we can consider them as a Constant and easily drop such columns as they do not contribute any thing in ML model.\n\nLets find out more of such.\n\nTo achieve this first we have to get the Unique value count for each column, for this let me create another data frame and have the list of all columns with their respective count of unique values.","7f1d663d":"# Converting the float64 to float32","0e797df6":"<!-- * 'f190486d6' seems to be the important variable followed by '58e2e02e6'. -->","91d42ea1":"Will drop the features.. which we dropped during the training phase.","cc0d2b55":"Getting some problem.. will visit this later.","f5c3f7f7":"https:\/\/stackoverflow.com\/questions\/64003981\/how-to-get-the-correlation-of-variables-with-thousands-of-feature-in-a-dataset","afa6518c":"<!-- There are quite a few variables with absolute correlation greater than 0.1 -->","738c8b70":"# Load DataSet \nFocusing only on train dataset for now.","88c64e4a":"Wow.. lot of constant columns. We should be good to drop these 256 while training the model.","552ea5fa":"# Submission","a478a2bc":"# LightGBM","b4b7e315":"# Evaluate - RMSLE ","56bd12d7":"# Modelling using LightGBM; XGB; CatBoost","3902c8a2":"Now we have the details.. lets check how may columns are there which has single value for entire data-set.","22e39ada":"# Modeling - using XGBClassifier","6808dc51":"# Santander Value Prediction Challenge \n\n## Understanding the Dataset.\n\nPlease refer to the link for more details https:\/\/www.kaggle.com\/c\/santander-value-prediction-challenge\/ \n","40fcf156":"We do not need ID field, so we will drop that as well.. but will have it in different DataFrame."}}