{"cell_type":{"3c1085bf":"code","f3aa476b":"code","1c468b0a":"code","4d9c16dd":"code","2fdc0f97":"code","e036058c":"code","4f331dff":"code","c79a4b51":"code","3e6225b0":"code","a450db00":"code","0a4743ef":"code","250e21b1":"code","da3da58a":"code","c982a3d2":"code","cf5d9ba3":"code","1f6c478d":"code","522a67c2":"code","3aeabeed":"code","ac32548a":"code","90a1223a":"code","4c25ba0f":"code","55dd19cb":"code","b0b6200e":"code","655ff914":"code","f03c091d":"code","852d64b2":"code","c5926179":"code","7690ab86":"code","d26b55db":"code","d5925d8d":"code","d9a2e852":"code","e5a7b421":"code","25d621e8":"code","a4d91849":"code","69949c88":"code","7b797bfd":"code","282c6957":"code","c0f3e798":"code","7023ddb1":"code","5d36ddc8":"code","91dc6926":"code","c8f8e1a5":"markdown","de9c3e71":"markdown","dc11ed32":"markdown","4270b9a1":"markdown","66974faa":"markdown","5fa35700":"markdown","908df151":"markdown","e1291ceb":"markdown","6c8abb5d":"markdown","b2a9a48e":"markdown","57171aef":"markdown","499c1798":"markdown","ffe85588":"markdown","87e61d5a":"markdown","265eb927":"markdown","5a3e08df":"markdown","a095bc9b":"markdown","46103ca2":"markdown","4953f023":"markdown","25719986":"markdown","6e5d0340":"markdown","601cb18f":"markdown","e24cf57e":"markdown","50a233c9":"markdown","a1bada8d":"markdown","9286ce65":"markdown","1269634c":"markdown"},"source":{"3c1085bf":"import warnings\nimport joblib\n#import pydotplus\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz, export_text\nfrom sklearn.metrics import classification_report, roc_auc_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_validate, validation_curve\n#from skompiler import skompile","f3aa476b":"#Reading the dataset\ndf = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")","1c468b0a":"# The first 5 observation units of the data set were accessed.\ndf.head()","4d9c16dd":"y = df[\"Outcome\"]\nX = df.drop([\"Outcome\"], axis=1)","2fdc0f97":"# Model\ncart_model = DecisionTreeClassifier(random_state=17).fit(X, y)","e036058c":"#y_pred for Confusion Matrix  :\ny_pred = cart_model.predict(X)","4f331dff":"#y_prob for AUC:\ny_prob = cart_model.predict_proba(X)[:, 1]","c79a4b51":"# Confusion matrix\nprint(classification_report(y, y_pred))","3e6225b0":"# AUC\nroc_auc_score(y, y_prob)","a450db00":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=85)","0a4743ef":"cart_model = DecisionTreeClassifier(random_state=17).fit(X_train, y_train)","250e21b1":"# Train Error\ny_pred = cart_model.predict(X_train)\ny_prob = cart_model.predict_proba(X_train)[:, 1]\nprint(classification_report(y_train, y_pred))","da3da58a":"roc_auc_score(y_train, y_prob)","c982a3d2":"# Test Error\ny_pred = cart_model.predict(X_test)\ny_prob = cart_model.predict_proba(X_test)[:, 1]\nprint(classification_report(y_test, y_pred))","cf5d9ba3":"roc_auc_score(y_test, y_prob)","1f6c478d":"cart_model = DecisionTreeClassifier(random_state=17).fit(X, y)","522a67c2":"cv_results = cross_validate(cart_model,\n                            X, y,\n                            cv=10,\n                            scoring=[\"accuracy\", \"f1\", \"roc_auc\"])","3aeabeed":"cv_results['test_accuracy'].mean()","ac32548a":"cv_results['test_f1'].mean()","90a1223a":"cv_results['test_roc_auc'].mean()","4c25ba0f":"cart_model.get_params()","55dd19cb":"# Hyperparameter set to search:\ncart_params = {'max_depth': range(1, 11),\n               \"min_samples_split\": range(2, 20)}","b0b6200e":"# GridSearchCV\ncart_best_grid = GridSearchCV(cart_model,\n                              cart_params,\n                              cv=5,\n                              n_jobs=-1,\n                              verbose=True).fit(X, y)","655ff914":"# Best hyper parameter values:\ncart_best_grid.best_params_","f03c091d":"# Best score:\ncart_best_grid.best_score_","852d64b2":"random = X.sample(1, random_state=45)\nprint(random)","c5926179":"cart_best_grid.predict(random)","7690ab86":"cart_final = DecisionTreeClassifier(**cart_best_grid.best_params_,\n                                    random_state=17).fit(X, y)","d26b55db":"cart_final.get_params()","d5925d8d":"# Another way to assign the best parameters to the model:\ncart_final = cart_model.set_params(**cart_best_grid.best_params_).fit(X, y)","d9a2e852":"# CV error of final model:\ncv_results = cross_validate(cart_final,\n                            X, y,\n                            cv=10,\n                            scoring=[\"accuracy\", \"f1\", \"roc_auc\"])","e5a7b421":"cv_results['test_accuracy'].mean()\n","25d621e8":"cv_results['test_f1'].mean()\n","a4d91849":"cv_results['test_roc_auc'].mean()\n","69949c88":"def plot_importance(model, features, num=len(X), save=False):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n    plt.figure(figsize=(10, 10))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n                                                                     ascending=False)[0:num])\n    plt.title('Features')\n    plt.tight_layout()\n    plt.show()\n    if save:\n        plt.savefig('importances.png')","7b797bfd":"plot_importance(cart_final, X, 15)","282c6957":"train_score, test_score = validation_curve(\n    cart_final, X=X, y=y,\n    param_name='max_depth',\n    param_range=range(1, 11),\n    scoring=\"roc_auc\",\n    cv=10)","c0f3e798":"mean_train_score = np.mean(train_score, axis=1)","7023ddb1":"mean_test_score = np.mean(test_score, axis=1)","5d36ddc8":"plt.plot(range(1, 11), mean_train_score,\n         label=\"Training Score\", color='b')\nplt.plot(range(1, 11), mean_test_score,\n         label=\"Validation Score\", color='g')\n\nplt.title(\"Validation Curve for CART\")\nplt.xlabel(\"Number of max_depth\")\nplt.ylabel(\"AUC\")\nplt.tight_layout()\nplt.legend(loc='best')\nplt.show()","91dc6926":"tree_rules = export_text(cart_model, feature_names=list(X.columns))\nprint(tree_rules)","c8f8e1a5":"# Modeling using CART","de9c3e71":"# 5. Final Model","dc11ed32":"![image.png](attachment:257ebf54-3e9d-442a-8dca-f6082ac8df98.png)","4270b9a1":"Classification tree is built in accordance with splitting rule - the rule that performs the splitting of learning sample into smaller parts. We already know that each time data have to be divided into two parts with maximum homogeneity:","66974faa":"[1] Timofeev, R. (2004). Classification and regression trees (CART) theory and applications. Humboldt University, Berlin, 1-40.\n\n[2] Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (2017). Classification and regression trees. Routledge.","5fa35700":"**Cost\/Loss\/Objective Function for Classification Problems**","908df151":"As can be seen in the leaf on the right, full homogeneity was achieved. There are 3 values belonging to the same class in the relevant leaf. The Gini value was calculated as 0 for that leaf.","e1291ceb":"    Both gini and entropy are measures of impurity of a node. A node having multiple classes is impure whereas a node having only one class is pure.  Entropy in statistics is analogous to entropy in thermodynamics where it signifies disorder. If there are multiple classes in a node, there is disorder in that node. Information gain is the entropy of parent node minus sum of weighted entropies of child nodes. \n    Weight of a child node is number of samples in the node\/total samples of all child nodes. Similarly information gain is calculated with gini score. \n","6c8abb5d":"The aim is to transform the complex structures in the data set into simple decision structures. Heterogeneous data sets are divided into homogeneous subgroups according to a specified target variable.","b2a9a48e":"![image.png](attachment:487f6360-13f1-4f93-b801-b8f53875c7ad.png)","57171aef":"# Hyperparameter Optimization with GridSearchCV","499c1798":"# 8. Extracting Decision Rules","ffe85588":"The feature at the top of the tree structure is the most important variable. Missing or outliers do not have much effect on the reliability of results in tree methods. Likewise, scaling operations do not have an effect on the result.","87e61d5a":"Entropy is a measure of information that indicates the disorder of the features with the target. Similar to the Gini Index, the optimum split is chosen by the feature with less entropy. It gets its maximum value when the probability of the two classes is the same and a node is pure when the entropy has its minimum value, which is 0","265eb927":"# REFERENCES","5a3e08df":"![image.png](attachment:80d98587-fba1-4194-aa30-577fe45740f0.png)","a095bc9b":"**Gini Calculation:**","46103ca2":"Here, we maintain the perspective of comparing actual values and predicted values, as we do when faced with a regression problem. We do a split, then look at the error. For example, we have taken into account the SSE value in Linear Problems. Gini index and entropy are the criteria for calculating information gain.\n\nThe gini impurity measures the frequency at which any element of the dataset will be mislabelled when it is randomly labeled.\n\nThe minimum value of the Gini Index is 0. This happens when the node is pure, this means that all the contained elements in the node are of one unique class. Therefore, this node will not be split again. Thus, the optimum split is chosen by the features with less Gini Index. Moreover, it gets the maximum value when the probability of the two classes are the same.","4953f023":"Classification and Regression Trees is a classification method which uses historical data to construct so-called decision trees. Decision trees are then used to classify new data. In order to use CART we need to know number of classes a priori.\nCART methodology was developed in 80s by Breiman, Freidman, Olshen, Stone in their paper \u201dClassification and Regression Trees\u201d (1984). For building decision trees, CART uses so-called learning sample - a set of historical data with pre-assigned classes for all observations. For example, learning sample for credit scoring system would be fundamental information about previous borrows (variables) matched with actual payoff results (classes).\nDecision trees are represented by a set of questions which splits the learning sample into smaller and smaller parts. CART asks only yes\/no questions. A possible question could be: \u201dIs age greater than 50?\u201d or \u201dIs sex male?\u201d. CART algorithm will search for all possible variables and all possible values in order to find the best split - the question that splits the data into two parts with maximum homogeneity. The process is then repeated for each of the resulting data fragments. [1]\n\n**CART methodology consists of tree parts:**\n\n1. Construction of maximum tree\n\n2. Choice of the right tree size\n\n3. Classification of new data using constructed tree","25719986":"# Evaluation of Success with the Holdout Method","6e5d0340":"![image.png](attachment:08e11bdb-27bf-4d07-b51b-03725202d6fd.png)","601cb18f":"**Cost\/Loss\/Objective Function for Regression Problems**","e24cf57e":"Gini for the first node can be calculated as (5\/11 x 6\/11) + (6\/11 x 5\/11) = 0\/496","50a233c9":"# 7. Analyzing Model Complexity with Learning Curves","a1bada8d":"![image.png](attachment:5645f2f7-8b1d-49d5-a68d-94a5e43021ca.png)","9286ce65":"# Evaluation of Success with Cross Validation","1269634c":"# 6. Feature Importance"}}