{"cell_type":{"80b6a203":"code","c5380c71":"code","1dc37605":"code","b88be916":"code","9d4ef469":"code","f5333606":"code","bd005cfb":"code","76427d6e":"code","d8406137":"markdown","8aa34b93":"markdown","1f979112":"markdown","ffa14f7e":"markdown","84d6d6b2":"markdown","fecdc20b":"markdown","57ecc151":"markdown","13e77412":"markdown","45854015":"markdown","7dee21ae":"markdown","74eef336":"markdown"},"source":{"80b6a203":"import copy\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nfrom sklearn.preprocessing import LabelEncoder, PolynomialFeatures\r\nfrom sklearn.linear_model import LinearRegression, RidgeCV\r\nfrom sklearn.metrics import mean_squared_error\r\nfrom sklearn.model_selection import train_test_split","c5380c71":"#The original data can never be added or deleted columns\r\noriginal_data = pd.read_csv(\"fish.csv\")\r\n#The data variable is used to make modifications on it\r\ndata = copy.deepcopy(original_data)\r\ndata.head()","1dc37605":"np.sum(data.isnull())","b88be916":"original_data[\"Species\"] = pd.DataFrame(original_data[\"Species\"]).apply(LabelEncoder().fit_transform)","9d4ef469":"sns.heatmap(data.corr(), annot=True)","f5333606":"corr = data.corr()[\"Weight\"].drop(\"Weight\")\r\nprint(corr)","bd005cfb":"#Variables for keeping track of errors are initialized\r\ne_train = []\r\ne_test = []\r\ne_train_hist = []\r\ne_test_hist = []\r\nalpha_hist = []\r\nalpha = []\r\n\r\n#Max degree of the regression\r\nmax_degree = 5\r\n\r\n#No. of training times\r\ntraining_times = 50\r\n\r\n#Iterate over the different degrees\r\nfor degree in range(1,max_degree):\r\n    poly = PolynomialFeatures(degree)\r\n    data = copy.deepcopy(original_data)\r\n    y = pd.DataFrame(data[\"Weight\"])\r\n    data = data.drop(\"Weight\", axis = 1)\r\n    x = poly.fit_transform(data)\r\n    for i in range(training_times):\r\n        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=np.random.randint(100))\r\n        model = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 2, 4, 6, 8, 16, 32, 40, 50, 80, 100, 150, 200, 250, 300, 350, 400])\r\n        model.fit(x_train, y_train)\r\n        #Training error is recorded\r\n        e = np.sqrt(mean_squared_error(y_train, model.predict(x_train)))\r\n        e_train.append(e)\r\n        #Test error is recorded\r\n        e = np.sqrt(mean_squared_error(y_test, model.predict(x_test)))\r\n        e_test.append(e)\r\n        #The alpha hyperparameter is recorded\r\n        alpha.append(model.alpha_)\r\n    #The records of the current degree are saved\r\n    e_train_hist.append(e_train)\r\n    e_train = []\r\n    e_test_hist.append(e_test)\r\n    e_test = []\r\n    alpha_hist.append(alpha)\r\n    alpha = []\r\n\r\n#The mean for each degree is calculated\r\ne_train = np.mean(np.array(e_train_hist),axis=1)\r\ne_test = np.mean(np.array(e_test_hist),axis=1)\r\nalpha = np.mean(np.array(alpha_hist),axis=1)\r\n\r\n#The errors and alpha record is plotted\r\nplt.plot(range(1,max_degree), e_train, 'o-', label = \"train\")\r\nplt.plot(range(1,max_degree), e_test, 'o-',label = \"test\")\r\nplt.legend()\r\nplt.figure()\r\nplt.plot(range(1,max_degree), alpha, 'o-',label = \"alpha\")\r\nplt.legend()\r\n","76427d6e":"#Variables for keeping track of errors are initialized\r\ne_train = []\r\ne_test = []\r\ne_train_hist = []\r\ne_test_hist = []\r\nalpha_hist = []\r\nalpha = []\r\n\r\n#Max degree of the regression\r\nmax_degree = 5\r\n\r\n#No. of training times\r\ntraining_times = 50\r\n\r\n#No. of training examples\r\nm = original_data.shape[0]\r\n\r\nstep = 1\r\n\r\ndegree = 2\r\n\r\n#For every iteration diferent amounts of data are selected\r\nfor n_data in range(20, m, step):\r\n    poly = PolynomialFeatures(degree)\r\n    # The model is trained several times with diferent data so as to get a non-random and more precise error. \r\n    for i in range(training_times):\r\n        data = copy.deepcopy(original_data)\r\n        data = data.iloc[np.random.permutation(np.arange(0,m)),:] #Data is shuffled\r\n        data = data.iloc[1:n_data,:]\r\n        y = pd.DataFrame(data[\"Weight\"])\r\n        data = data.drop(\"Weight\", axis = 1)\r\n        x = poly.fit_transform(data)\r\n        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=np.random.randint(100))\r\n        model = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 2, 4, 6, 8, 16, 32, 40, 50, 80, 100, 150, 200, 250, 300, 350, 400])\r\n        model.fit(x_train, y_train)\r\n        #Training error is recorded\r\n        e = np.sqrt(mean_squared_error(y_train, model.predict(x_train)))\r\n        e_train.append(e)\r\n        #Test error is recorded\r\n        e = np.sqrt(mean_squared_error(y_test, model.predict(x_test)))\r\n        e_test.append(e)\r\n        #The alpha hyperparameter is recorded\r\n        alpha.append(model.alpha_)\r\n    #The records of the current degree are saved\r\n    e_train_hist.append(e_train)\r\n    e_train = []\r\n    e_test_hist.append(e_test)\r\n    e_test = []\r\n    alpha_hist.append(alpha)\r\n    alpha = []\r\n\r\n#The mean for every training examples amount is calculated\r\ne_train = np.mean(np.array(e_train_hist),axis=1)\r\ne_test = np.mean(np.array(e_test_hist),axis=1)\r\nalpha = np.mean(np.array(alpha_hist),axis=1)\r\n\r\n#The errors and alpha record are plotted\r\nplt.plot(range(20, m, step), e_train, 'o-', label = \"train\")\r\nplt.plot(range(20, m, step), e_test, 'o-',label = \"test\")\r\nplt.legend()\r\nplt.figure()\r\nplt.plot(range(20, m, step), alpha, 'o-',label = \"alpha\")\r\nplt.legend()\r\n","d8406137":"## Check null data items","8aa34b93":"There aren't null items","1f979112":"The aim of this cell is to plot the learning curve of the model. <br>\r\nAs a result, it can be easily spotted that training a test error end up close one to each other. <br>\r\nIn addition, the hyperparameter alpha gets bigger and bigger because overfitting is decreasing for every dataset size iteration.","ffa14f7e":"## Error is studied according to amount of data","84d6d6b2":"## Let's see the linear correlation of the different features","fecdc20b":"## Data is loaded","57ecc151":"## Error is studied according to the number of degree of the regression","13e77412":"## Data is treated (Strings converted to numerical data)","45854015":"The aim of this cell is to choose the best degree for the regression. So as to achive this goal:\r\n- It iterates over the diferent degrees.\r\n- For each one, the model is trained several times (50 for example). For each training the training and test error is recorded. For each training, is randomly shuffled between training and test data. The purpose of this strategy is to getting a non random error, by averaging the errors.\r\nA conclusion can be drawn from the resulting plots:\r\n- 2 may be the most suitable degree for the regression because:\r\n    - It gets the lowest test error.\r\n    - It gets the closest test error to the training one.\r\n    - That's why a balance between bias and variance is found\r\n","7dee21ae":"The only non numerical column is the 'Species', so this one is encoded to an integer","74eef336":"# Predict the Weight of Fish\r\n"}}