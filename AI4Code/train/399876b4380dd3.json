{"cell_type":{"65fe1b9b":"code","ab87d88f":"code","1bbab83a":"code","ed45cfac":"code","7045448d":"code","f67e7975":"code","ccbfe039":"code","dd41d6a1":"code","27f76caf":"code","cece4a60":"code","00eba749":"code","1a784bb0":"code","c630d813":"code","7b1a204f":"code","fb76fe0e":"code","d0368204":"code","eadd80d9":"code","f97427cb":"code","fe8912e7":"code","4923b371":"code","5c7e2601":"code","db1f7573":"code","a7b85a33":"code","01a52298":"code","402b421d":"code","35e95c37":"code","d10ad5c8":"code","f27fe2ff":"code","54175bf0":"code","567218a4":"code","04df5b9e":"code","abd39136":"markdown","1339f4d4":"markdown","1ff6c3aa":"markdown","5d039557":"markdown","1d21a0cc":"markdown","98982e42":"markdown","1ee32fe6":"markdown","1d80e4a2":"markdown","8896687e":"markdown","87e8de25":"markdown","52007072":"markdown","54d1839f":"markdown","045ab5ae":"markdown","ccf8006e":"markdown","5129c836":"markdown","eb9101a2":"markdown","fa6c0b3d":"markdown","de6c23cd":"markdown","4c127eb8":"markdown","bc1b3151":"markdown","845639c4":"markdown","38e093b2":"markdown","0b0465ad":"markdown"},"source":{"65fe1b9b":"import pandas as pd\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MaxAbsScaler\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import average_precision_score\n\nfrom sklearn.linear_model import SGDClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom skopt import gp_minimize\nfrom skopt.plots import plot_convergence\n\nfrom imblearn.under_sampling import NearMiss\n\nfrom collections import Counter\nimport joblib","ab87d88f":"df = pd.read_csv(\"..\/input\/santander-customer-transaction-prediction\/train.csv\")\nprint(df.shape)\ndf.head()","1bbab83a":"print(df[\"target\"].value_counts()\/df.shape[0])\nsns.countplot(\"target\", data = df)","ed45cfac":"y = df[\"target\"]\nx = df.drop(columns=[\"target\", \"ID_code\"]).values","7045448d":"nm = NearMiss(version = 3, n_neighbors_ver3 = 3)\nnm_x, nm_y = nm.fit_resample(x, y)","f67e7975":"print(Counter(nm_y))\nsns.countplot(nm_y)","ccbfe039":"x_train, x_test_val, y_train, y_test_val = train_test_split(nm_x, nm_y, test_size = .3, \n                                                            stratify = nm_y, random_state = 42)\nx_test, x_val, y_test, y_val = train_test_split(x_test_val, y_test_val, test_size = .3, stratify = y_test_val,\n                                               random_state = 42)\n\n\nprint(\"Train size: %i\"%(len(y_train)))\nprint(\"Test size: %i\"%(len(y_test)))\nprint(\"Validation size: %i\"%(len(y_val)))","dd41d6a1":"baseline_sgd_pipeline = Pipeline([(\"scaler\", MaxAbsScaler()),\n                                  (\"sgd\", SGDClassifier(loss = \"log\", random_state = 42))\n                                 ])\nbaseline_sgd_pipeline.fit(x_train, y_train)\n\ny_pred = baseline_sgd_pipeline.predict(x_test)\nprint(classification_report(y_test, y_pred))\n\ny_pred_prob = baseline_sgd_pipeline.predict_proba(x_test)[:,1]\nprint(\"ROC AUC: %f\"%(roc_auc_score(y_test, y_pred_prob)))\nprint(\"AUPRC: %f\"%(average_precision_score(y_test, y_pred_prob)))","27f76caf":"def create_sgd(params):\n    penalty = params[0]\n    max_iter = params[1]\n    alpha = params[2]\n    \n    sgd_pipeline =  Pipeline([(\"scaler\", MaxAbsScaler()),\n                     (\"sgd\", SGDClassifier(loss = \"log\", penalty = penalty, \n                                           max_iter= max_iter, alpha = alpha, \n                                           random_state = 42))\n                    ])\n    \n    return sgd_pipeline\n\ndef train_sgd(params):\n    print(\"\\n\", params)\n    \n    sgd_pipeline = create_sgd(params)\n    \n    sgd_pipeline.fit(x_train, y_train)\n    \n    y_percs = sgd_pipeline.predict_proba(x_test)[:,1]\n    \n    return -roc_auc_score(y_test, y_percs)\n    \nparams = [([\"l1\", \"l2\"]), #penalty\n          (500, 10000), #max_iter\n          (1e-5, 1e-1, \"log-uniform\") #alpha\n         ]\n\ngp_sgd = gp_minimize(train_sgd, params, verbose = 1, n_calls = 50, n_random_starts = 10, random_state = 42)","cece4a60":"print(gp_sgd.x)\nplot_convergence(gp_sgd)","00eba749":"k_scores = []\nfor k in range(5, 201, 5):\n    selector = SelectKBest(f_classif, k = k)\n    \n    x_train_selected = selector.fit_transform(x_train, y_train)\n    x_test_selected = selector.transform(x_test)\n    \n    sgd_pipeline_selected = create_sgd(gp_sgd.x)\n    sgd_pipeline_selected.fit(x_train_selected, y_train)\n    \n    y_percs_selected = sgd_pipeline_selected.predict_proba(x_test_selected)[:,1]\n    \n    roc_score = roc_auc_score(y_test, y_percs_selected)\n    k_scores.append(roc_score)\n    \n    print(\"K = %i -> ROC AUC %f\"%(k, roc_score))\n\nind = k_scores.index(max(k_scores))\nopt_k = 5 + 5 * ind\nprint(\"Max score = %f; K = %i\"%(k_scores[ind], opt_k))\n\nsns.lineplot(x = range(2, 201, 5), y = k_scores)","1a784bb0":"selector_sgd = SelectKBest(f_classif, k = opt_k)\nx_train_sgd = selector_sgd.fit_transform(x_train, y_train)\n\ndf_sgd = df.drop(columns=[\"target\", \"ID_code\"]).iloc[:, selector_sgd.get_support()]\n\nprint(\"Removed cols ->\", set(df.columns) - set(df_sgd.columns))\ndf_sgd.head()","c630d813":"x_test_sgd = selector_sgd.transform(x_test)\nx_val_sgd = selector_sgd.transform(x_val)\n    \nopt_sgd_pipeline = create_sgd(gp_sgd.x)\nopt_sgd_pipeline.fit(x_train_sgd, y_train)","7b1a204f":"y_pred = opt_sgd_pipeline.predict(x_test_sgd)\nprint(classification_report(y_test, y_pred))\n\ny_pred_prob = opt_sgd_pipeline.predict_proba(x_test_sgd)[:,1]\nprint(\"ROC AUC: %f\"%(roc_auc_score(y_test, y_pred_prob)))\nprint(\"AUPRC: %f\"%(average_precision_score(y_test, y_pred_prob)))\n\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt=\"d\", cmap = \"BuGn\")","fb76fe0e":"y_pred = opt_sgd_pipeline.predict(x_val_sgd)\nprint(classification_report(y_val, y_pred))\n\ny_pred_prob = opt_sgd_pipeline.predict_proba(x_val_sgd)[:,1]\nprint(\"ROC AUC: %f\"%(roc_auc_score(y_val, y_pred_prob)))\nprint(\"AUPRC: %f\"%(average_precision_score(y_val, y_pred_prob)))\n\nsns.heatmap(confusion_matrix(y_val, y_pred), annot=True, fmt=\"d\", cmap = \"BuGn\")","d0368204":"baseline_xgb_pipeline = Pipeline([(\"scaler\", MaxAbsScaler()),\n                                  (\"gb\", XGBClassifier(random_state = 42))\n                                 ])\nbaseline_xgb_pipeline.fit(x_train, y_train)\n\ny_pred = baseline_xgb_pipeline.predict(x_test)\nprint(classification_report(y_test, y_pred))\n\ny_pred_prob = baseline_xgb_pipeline.predict_proba(x_test)[:,1]\nprint(\"ROC AUC: %f\"%(roc_auc_score(y_test, y_pred_prob)))\nprint(\"AUPRC: %f\"%(average_precision_score(y_test, y_pred_prob)))","eadd80d9":"def create_xgb(params):\n    learning_rate = params[0]\n    max_depth = params[1]\n    subsample = params[2]\n    colsample_bytree = params[3]\n    max_leaves = params[4]\n    min_child_weight = params[5]\n    \n    xgb_pipeline =  Pipeline([(\"scaler\", MaxAbsScaler()),\n                     (\"gb\", XGBClassifier(random_state = 42, \n                                          learning_rate = learning_rate, \n                                          max_depth = max_depth, \n                                          subsample = subsample,\n                                          colsample_bytree = colsample_bytree,\n                                          max_leaves = max_leaves, \n                                          min_child_weight = min_child_weight))\n                    ])\n    \n    return xgb_pipeline\n\ndef train_xgb(params):\n    print(\"\\n\", params)\n    \n    xgb_pipeline = create_xgb(params)\n    \n    xgb_pipeline.fit(x_train, y_train)\n    \n    y_percs = xgb_pipeline.predict_proba(x_test)[:,1]\n    \n    return -roc_auc_score(y_test, y_percs)\n    \nparams = [(1e-3, 1e-1, \"log-uniform\"), #learning_rate\n          (1, 50), #max_depth\n          (0.01, .95), #subsample\n          (0.01, .95), #colsample_bytree\n          (2, 512), #max_leaves\n          (1, 120) #min_child_weight\n         ]\n\ngp_xgb = gp_minimize(train_xgb, params, verbose = 1, n_calls = 65, n_random_starts = 15, random_state = 42)","f97427cb":"print(gp_xgb.x)\nplot_convergence(gp_xgb)","fe8912e7":"k_scores = []\nfor k in range(5, 201, 5):\n    selector = SelectKBest(f_classif, k = k)\n    \n    x_train_selected = selector.fit_transform(x_train, y_train)\n    x_test_selected = selector.transform(x_test)\n    \n    xgb_pipeline_selected = create_xgb(gp_xgb.x)\n    xgb_pipeline_selected.fit(x_train_selected, y_train)\n    \n    y_percs_selected = xgb_pipeline_selected.predict_proba(x_test_selected)[:,1]\n    \n    roc_score = roc_auc_score(y_test, y_percs_selected)\n    k_scores.append(roc_score)\n    \n    print(\"K = %i -> ROC AUC %f\"%(k, roc_score))\n\nind = k_scores.index(max(k_scores))\nopt_k = 5 + 5 * ind\nprint(\"Max score = %f; K = %i\"%(k_scores[ind], opt_k))\n\nsns.lineplot(x = range(2, 201, 5), y = k_scores)","4923b371":"selector_xgb = SelectKBest(f_classif, k = opt_k)\nx_train_xgb = selector_xgb.fit_transform(x_train, y_train)\n\ndf_xgb = df.drop(columns=[\"target\", \"ID_code\"]).iloc[:, selector_xgb.get_support()]\n\nprint(\"Removed cols ->\", set(df.columns) - set(df_xgb.columns))\ndf_xgb.head()","5c7e2601":"x_test_xgb = selector_xgb.transform(x_test)\nx_val_xgb = selector_xgb.transform(x_val)\n    \nopt_xgb_pipeline = create_xgb(gp_xgb.x)\nopt_xgb_pipeline.fit(x_train_xgb, y_train)","db1f7573":"y_pred = opt_xgb_pipeline.predict(x_test_xgb)\nprint(classification_report(y_test, y_pred))\n\ny_pred_prob = opt_xgb_pipeline.predict_proba(x_test_xgb)[:,1]\nprint(\"ROC AUC: %f\"%(roc_auc_score(y_test, y_pred_prob)))\nprint(\"AUPRC: %f\"%(average_precision_score(y_test, y_pred_prob)))\n\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt=\"d\", cmap = \"BuGn\")","a7b85a33":"y_pred = opt_xgb_pipeline.predict(x_val_xgb)\nprint(classification_report(y_val, y_pred))\n\ny_pred_prob = opt_xgb_pipeline.predict_proba(x_val_xgb)[:,1]\nprint(\"ROC AUC: %f\"%(roc_auc_score(y_val, y_pred_prob)))\nprint(\"AUPRC: %f\"%(average_precision_score(y_val, y_pred_prob)))\n\nsns.heatmap(confusion_matrix(y_val, y_pred), annot=True, fmt=\"d\", cmap = \"BuGn\")","01a52298":"baseline_lgbm_pipeline = Pipeline([(\"scaler\", MaxAbsScaler()),\n                                   (\"lgbm\", LGBMClassifier(random_state = 42))\n                                  ])\nbaseline_lgbm_pipeline.fit(x_train, y_train)\n\ny_pred = baseline_lgbm_pipeline.predict(x_test)\nprint(classification_report(y_test, y_pred))\n\ny_pred_prob = baseline_lgbm_pipeline.predict_proba(x_test)[:,1]\nprint(\"ROC AUC: %f\"%(roc_auc_score(y_test, y_pred_prob)))\nprint(\"AUPRC: %f\"%(average_precision_score(y_test, y_pred_prob)))","402b421d":"def create_lgbm(params):\n    learning_rate = params[0]\n    n_estimators = params[1]\n    num_leaves = params[2]\n    min_child_samples = params[3]\n    subsample = params[4]\n    colsample_bytree = params[5]\n    \n    lgbm_pipeline =  Pipeline([(\"scaler\", MaxAbsScaler()),\n                     (\"lgbm\", LGBMClassifier(random_state = 42, \n                                                     learning_rate = learning_rate,\n                                                     n_estimators = n_estimators, \n                                                     num_leaves = num_leaves, \n                                                     min_child_samples = min_child_samples, \n                                                     subsample = subsample, \n                                                     colsample_bytree = colsample_bytree))\n                    ])\n    \n    return lgbm_pipeline\n\ndef train_lgbm(params):\n    print(\"\\n\", params)\n    \n    lgbm_pipeline = create_lgbm(params)\n    \n    lgbm_pipeline.fit(x_train, y_train)\n    \n    y_percs = lgbm_pipeline.predict_proba(x_test)[:,1]\n    \n    return -roc_auc_score(y_test, y_percs)\n    \nparams = [(1e-3, 1e-1, 'log-uniform'), #learning_rate\n          (50, 2500), #n_estimators\n          (2, 256), #num_leaves\n          (2, 250), #min_child_samples\n          (0.05, 1.0), #subsample\n          (0.05, 1.0) #colsample_bytree\n         ]\n\ngp_lgbm = gp_minimize(train_lgbm, params, verbose = 1, n_calls = 40, n_random_starts = 10, random_state = 42)","35e95c37":"print(gp_lgbm.x)\nplot_convergence(gp_lgbm)","d10ad5c8":"k_scores = []\nfor k in range(5, 201, 5):\n    selector = SelectKBest(f_classif, k = k)\n    \n    x_train_selected = selector.fit_transform(x_train, y_train)\n    x_test_selected = selector.transform(x_test)\n    \n    lgbm_pipeline_selected = create_lgbm(gp_lgbm.x)\n    lgbm_pipeline_selected.fit(x_train_selected, y_train)\n    \n    y_percs_selected = lgbm_pipeline_selected.predict_proba(x_test_selected)[:,1]\n    \n    roc_score = roc_auc_score(y_test, y_percs_selected)\n    k_scores.append(roc_score)\n    \n    print(\"K = %i -> ROC AUC %f\"%(k, roc_score))\n\nind = k_scores.index(max(k_scores))\nopt_k = 5 + 5 * ind\nprint(\"Max score = %f; K = %i\"%(k_scores[ind], opt_k))\n\nsns.lineplot(x = range(2, 201, 5), y = k_scores)","f27fe2ff":"selector_lgbm = SelectKBest(f_classif, k = opt_k)\nx_train_lgbm = selector_lgbm.fit_transform(x_train, y_train)\n\ndf_lgbm = df.drop(columns=[\"target\", \"ID_code\"]).iloc[:, selector_lgbm.get_support()]\n\nprint(\"Removed cols ->\", set(df.columns) - set(df_lgbm.columns))\ndf_lgbm.head()","54175bf0":"x_test_lgbm = selector_lgbm.transform(x_test)\nx_val_lgbm = selector_lgbm.transform(x_val)\n    \nopt_lgbm_pipeline = create_lgbm(gp_lgbm.x)\nopt_lgbm_pipeline.fit(x_train_lgbm, y_train)","567218a4":"y_pred = opt_lgbm_pipeline.predict(x_test_lgbm)\nprint(classification_report(y_test, y_pred))\n\ny_pred_prob = opt_lgbm_pipeline.predict_proba(x_test_lgbm)[:,1]\nprint(\"ROC AUC: %f\"%(roc_auc_score(y_test, y_pred_prob)))\nprint(\"AUPRC: %f\"%(average_precision_score(y_test, y_pred_prob)))\n\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt=\"d\", cmap = \"BuGn\")","04df5b9e":"y_pred = opt_lgbm_pipeline.predict(x_val_lgbm)\nprint(classification_report(y_val, y_pred))\n\ny_pred_prob = opt_lgbm_pipeline.predict_proba(x_val_lgbm)[:,1]\nprint(\"ROC AUC: %f\"%(roc_auc_score(y_val, y_pred_prob)))\nprint(\"AUPRC: %f\"%(average_precision_score(y_val, y_pred_prob)))\n\nsns.heatmap(confusion_matrix(y_val, y_pred), annot=True, fmt=\"d\", cmap = \"BuGn\")","abd39136":"#### Select variables","1339f4d4":"# Models","1ff6c3aa":"## LGBM","5d039557":"#### Opt XGBoost model","1d21a0cc":"#### Opt LGBM model","98982e42":"## Undersample\n* As we saw, the classes are imbalanced.\n* So we will undersample the class 0 using the NearMiss approach.","1ee32fe6":"#### Testing model","1d80e4a2":"#### Baseline","8896687e":"#### Baseline","87e8de25":"#### Select variable","52007072":"#### Opt SGD model","54d1839f":"#### Validating model","045ab5ae":"#### Baseline","ccf8006e":"#### Validating model","5129c836":"## SGD","eb9101a2":"#### Testing model","fa6c0b3d":"#### Optimize","de6c23cd":"#### Optimize","4c127eb8":"#### Testing model","bc1b3151":"## XGBoost","845639c4":"#### Select variables","38e093b2":"#### Optimize","0b0465ad":"#### Validating model"}}