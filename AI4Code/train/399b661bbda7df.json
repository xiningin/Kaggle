{"cell_type":{"ce8edc33":"code","75237304":"code","4ee90098":"code","a1e659fa":"code","751ce9f3":"code","678df2d1":"code","9163da77":"code","0a39110f":"code","5301ec0d":"code","93121462":"code","cf5a6d29":"code","99409f70":"code","b324bbcc":"code","1f26a096":"code","9fba159f":"markdown","007b06b0":"markdown"},"source":{"ce8edc33":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","75237304":"import numpy as np\nimport pandas as pd\nimport plotly.express as px\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import r2_score\nfrom datetime import datetime\n\npd.set_option('display.max_rows', None)\n\ntrain = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-5\/train.csv\")\ntrain.columns = [\"Id\",\"Cty\",\"Prov\",\"Ctry\",\"Pop\",\"Weight\",\"Date\",\"Type\",\"Value\"]\ntest = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-5\/test.csv\")\ntest.columns = [\"Id\",\"Cty\",\"Prov\",\"Ctry\",\"Pop\",\"Weight\",\"Date\",\"Type\"]\ntest[\"Value\"]=0\ntrain[\"Date\"]= pd.to_datetime(train.Date,infer_datetime_format=True)\ntest[\"Date\"]= pd.to_datetime(test.Date,infer_datetime_format=True)\nworld = pd.read_csv(\"\/kaggle\/input\/wb0904\/WorldBankData.csv\")\nfor col in world.columns[1:]:\n    avgcol= world[col].mean()\n    world.loc[world[col]==0,col]=avgcol\n\nsample_sub = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-5\/submission.csv\")\nold_cols = sample_sub.columns\nsample_sub.columns=[\"Id\",\"Value\"]\n\nmysub=sample_sub.set_index('Id')\n\ntrain[\"Test\"]=0\ntest[\"Test\"]=1\n\nX_full = pd.concat((train[train.Date < \"2020-04-27\"], test[test.Date >= \"2020-04-27\"]),sort=True).reset_index(drop=True)\n\nX_full[\"Reg\"]=X_full[\"Ctry\"].astype(str)+X_full[\"Prov\"].astype(str)+X_full[\"Cty\"].astype(str)\n#pop[\"Reg\"]=pop[\"Ctry\"]+pop[\"Prov\"].fillna(\"None\")\n\nX_full= X_full.merge(world, on=[\"Ctry\"],how=\"left\")\n#X_full= X_full.merge(pop[[\"Pop\",\"Reg\"]], on=[\"Reg\"],how=\"left\")\n\nX_full.loc[:,\"GDPPerc\"]= X_full.GDPPerc.astype(\"float\")\nX_full.loc[:,\"GDPperCapita\"]= X_full.GDPperCapita.astype(\"float\")\nX_full.loc[X_full.GDPperCapita == 0,\"GDPperCapita\"]=10000\n    \nX_full.fillna(0,inplace=True)","4ee90098":"#X_full.drop(columns=[\"DollarPPP\",\"Physicians\",\"Nurses and midwives\",\"Specialists\"],inplace=True)\nX_full[\"Test\"]=pd.to_numeric(X_full[\"Test\"],downcast=\"integer\")\nX_full[\"Pop\"]=pd.to_numeric(X_full[\"Pop\"],downcast=\"integer\")\nX_full[\"Value\"]=pd.to_numeric(X_full[\"Value\"],downcast=\"integer\")\n\nX_full.info()\n","a1e659fa":"sample_sub.head()","751ce9f3":"pd.set_option('display.max_rows', None)\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\n\n\ndef model_fit(model,X,target_col,folds=3):\n    \n    kf = KFold(folds, shuffle=True, random_state=4991)\n    \n    drop_cols = set(X.columns).intersection({\"Test\",\"Id\",\"Value\",\"LogD\",\"Week\",\"Day\",\"Date\",\n                                             \"Prov\",\"FirstDate\",\"DayYear\",\"Sub\",\"Ctry\",\"Cty\",\"Reg\",\"Type\",\n                                             \"Pred\",\"Res\",\"Cluster\",\"Cluster1\",\"Pop\",\"TotalCases\",\"index\",\"Weight\",target_col})\n    \n    # create predictors for each region\n    \n    X_r=X\n    X[\"RegType\"]=X.Type+X.Reg\n    X[\"WDay\"] = (X.DayYear % 7).astype(str)\n    X_r =pd.get_dummies(X.copy(),columns=[\"RegType\",\"WDay\"])\n    \n    # add an indicator for countries\/states with a significant number of cases\n    \n    for col in X_r.columns:\n        if col[:8]== \"RegType_\":\n            regtype = col[:8]\n            if X.loc[(X.RegType==regtype) & (X.DayYear == (107)),\"TotalCases\"].mean() > 25000:\n                \n                X_r[col+\"1\"]=X_r[\"Week1\"]*X_r[col]\n                X_r[col+\"2\"]=X_r[\"Week2\"]*X_r[col]\n                X_r[col+\"3\"]=X_r[\"Week3\"]*X_r[col]\n                          \n    \n    # add interactions with health spending indicators \n    # the relationship is very weak but I have kept these features in the model for now\n    \n    inter_features ={\"GDPPerc\",\"Week1\",\"Week2\",\"Week3\",\"Age65Perc\",\"GDPperCapita\",\"LogPop\"}.difference(set(drop_cols)).intersection(set(X.columns)) \n    poly = PolynomialFeatures(degree=2,include_bias=False) \n    inter_cols = poly.fit_transform(X[inter_features])\n    X1= pd.DataFrame(inter_cols,columns= poly.get_feature_names(list(inter_features)),index=X.index)                            \n    X_r = pd.concat([X1,X_r.drop(columns=inter_features)],axis=1)\n    \n    X_train = X_r[X_r.Test==0].drop(columns=drop_cols).copy()\n    \n    y_train = X.loc[X_r.Test==0,target_col]\n    #print(X_train.columns)\n    model.fit(X_train,y_train)\n    X[\"Pred\"] = np.maximum(model.predict(X_r.drop(columns=drop_cols)),0)\n    X[\"Res\"]= X.Pred-X[target_col]\n    score = (-cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv = kf))**0.5\n    \n    return X, score\n\n\nmodel =Ridge(alpha=0.0065,random_state=35591,max_iter=10000,fit_intercept=True,normalize=True)\n#model =Ridge(alpha=0.01,random_state=35591,max_iter=10000,fit_intercept=True,normalize=True)\n\n\ngraph = []\n\n# create separate predictions for the public and private leaderboard (i.e. remove values from 1\/4 from public submission)\n\nlast_train = \"2020-05-10\"    \nX_full.loc[(X_full.Date > \"2020-04-26\") & (X_full.Date <= last_train),\"Value\"]=train.loc[\n    (train.Date > \"2020-04-26\") & (train.Date <= last_train),\"Value\"].values\n\nX_full[\"DayYear\"]=X_full.Date.dt.dayofyear\n######## Data adjustments for Ecuador\n\nX_full.loc[(X_full.Type==\"ConfirmedCases\") & (X_full.Ctry==\"Ecuador\") & X_full.DayYear.isin([115,116,117]),\"Value\"] = [2000,2000,2000]\nX_full.loc[(X_full.Type==\"Fatalities\") & (X_full.Ctry==\"Ecuador\") & X_full.DayYear.isin([115,116,117]),\"Value\"] = [100,100,100]\n\n# reset sample sub\n\nmysub=sample_sub.set_index('Id')\naddsub=None\n\nfor loop in [\"Public\",\"Private\"]:\n    \n    if loop == \"Public\":\n        start=116 # last day of train data \n        val_end=117+14 # end date for validation data (public submission only )\n        sub_start=118\n        sub_end=118+13\n        startint= 107 # fit more recent data only - this is a key parameter\n    \n        X_full.loc[:,\"Test\"] = (X_full.Date > \"2020-04-26\") *1\n        \n    else:\n        start=117+14\n        sub_start=118+14\n        sub_end=999\n        X_full.loc[:,\"Test\"] = (X_full.Date > \"2020-05-10\") *1 \n        val_end=117+14\n        startint= 107 # fit more recent data only - this is a key parameter\n    \n    \n    \n    \n    \n    for cl in set({\"New\"}): #set(X_full.Cluster):\n        X_all= X_full[X_full.Ctry!=\"US1\"].reset_index()\n        cum_cases_map= X_all[[\"Type\",\"Value\",\"Reg\",\"Date\"]].groupby([\"Type\",\"Reg\",\"Date\"]).sum().groupby(level=1).cumsum().to_dict()[\"Value\"]\n        X_all[\"TotalCases\"]= X_all.apply(lambda x: cum_cases_map[x[\"Type\"],x[\"Reg\"],x[\"Date\"]],axis=1)\n        X_reg= X_all[~((X_all.Test ==0) & (((X_all.TotalCases < np.maximum(X_all.Pop*0.00001,3)) & (X_all.Type==\"ConfirmedCases\")) | \n                       ((X_all.TotalCases <10) & (X_all.Type==\"Fatalities\"))))].copy()\n\n        X_reg[\"Date\"]= pd.to_datetime(X_reg.Date,infer_datetime_format=True)\n        first_p_map= X_reg.loc[X_reg.Type==\"ConfirmedCases\",[\"Reg\",\"Date\"]].groupby(\"Reg\").min().to_dict()[\"Date\"]\n        X_reg[\"FirstDate\"]=X_reg[\"Reg\"].map(first_p_map)\n\n        X_reg[\"Day\"]=(X_reg.Date-X_reg.FirstDate).dt.days\n        X_reg[\"Week\"]=X_reg.Day\/7\n        \n        X_reg.Value= np.maximum(X_reg.Value,0)\n\n        X_reg[\"LogVal\"]= np.log(X_reg.Value+1)\n        X_reg[\"LogPop\"]= np.log(X_reg.Pop+1)\n\n        X_reg[\"Week1\"]=np.tanh((X_reg.Week)\/10)\n        X_reg[\"Week2\"]=np.tanh((X_reg.Week)\/10*3)\n        X_reg[\"Week3\"]=np.tanh((X_reg.Week)\/10*5)\n        X_reg[\"Week4\"]=np.tanh((X_reg.Week-1)\/10)\n        X_reg[\"Week5\"]=np.tanh((X_reg.Week-3)\/10)\n        X_reg[\"Week6\"]=np.tanh((X_reg.Week-5)\/10)\n        X_reg[\"Week7\"]=np.tanh((X_reg.Week-7)\/10)\n        X_reg[\"Week9\"]=np.tanh((X_reg.Week-9)\/10)\n        X_reg[\"Sub\"]=loop\n\n        X= X_reg[X_reg.DayYear >= startint].copy()\n        if cl[:2] != \"US\":\n            print(\"\\nLast day of year for train\",start,\": \",X.loc[X.DayYear== start,\"Date\"].min())\n            print(\"First day of year for test\",start+1,\": \",X.loc[X.Test == 1,\"Date\"].min())\n        \n        # fit regressmodel to log of cases to align with evaluation metric\n        if len(X[(X.Test==0) & (X.Type == \"ConfirmedCases\")]) > 0:\n            X_res, score = model_fit(model,X.loc[(X.Type == \"ConfirmedCases\")].copy(),\"LogVal\")\n            X.loc[(X.Type == \"ConfirmedCases\") ,\"Pred\"]=np.exp(X_res.Pred)-1\n            print(\"ConfirmedCases complete\")\n            X_res, score = model_fit(model,X.loc[(X.Type == \"Fatalities\")].copy(),\"LogVal\")\n            X.loc[(X.Type == \"Fatalities\") ,\"Pred\"]=np.exp(X_res.Pred)-1\n        else:\n            X[\"Pred\"]=0\n\n        # average last two observation to scale data\n        scalemap =X.loc[(X.Test==0) & (X.DayYear >= start-2),[\"Type\",\"Value\",\"Reg\"]].groupby([\"Type\",\"Reg\"]).mean().to_dict()[\"Value\"]\n        scalepmap =X.loc[(X.Test==0) & (X.DayYear >= start-2),[\"Type\",\"Pred\",\"Reg\"]].groupby([\"Type\",\"Reg\"]).mean().to_dict()[\"Pred\"]\n        scalemap.setdefault(\"Reg\",)\n        X.loc[X.Ctry!=\"US\",\"Pred\"]= X.loc[X.Ctry!=\"US\",\"Pred\"]* np.maximum(X.apply(lambda x: scalemap[x[\"Type\"],x[\"Reg\"]]\n                    if (x[\"Type\"],x[\"Reg\"]) in scalemap.keys() else 0,axis=1).fillna(0),1)\/np.maximum(X.apply\n                    (lambda x: scalepmap[x[\"Type\"],x[\"Reg\"]] if (x[\"Type\"],x[\"Reg\"]) in scalepmap.keys() else 0,axis=1).fillna(0),1)\n        \n        print(\"Model complete\")\n        X[\"Res\"]=X.Value-X.Pred\n        X.loc[X.Test==1,\"Res\"]=0\n        # include first test day to have full data\n        stdresmap =X.loc[X.DayYear <=sub_start,[\"Type\",\"Res\",\"Reg\"]].groupby([\"Type\",\"Reg\"]).std().to_dict()[\"Res\"]\n        meanmap =X.loc[X.DayYear <=sub_start,[\"Type\",\"Value\",\"Reg\"]].groupby([\"Type\",\"Reg\"]).mean().to_dict()[\"Value\"]\n        X[\"Mean\"]= np.maximum(X.apply(lambda x: meanmap[x[\"Type\"],x[\"Reg\"]],axis=1),1)\n        stdmap =X.loc[X.DayYear <=sub_start,[\"Type\",\"Pred\",\"Reg\"]].groupby([\"Type\",\"Reg\"]).std().to_dict()[\"Pred\"]\n        X[\"Std\"]= np.maximum(X.apply(lambda x: stdmap[x[\"Type\"],x[\"Reg\"]],axis=1),1) \n        X[\"StdRes\"]= np.maximum(X.apply(lambda x: stdresmap[x[\"Type\"],x[\"Reg\"]],axis=1),1)\n          \n        X[\"Pred05\"]=np.maximum(X.Pred - (X.Std+X.StdRes)*2 \/np.sqrt(X.Mean)*np.sqrt(X.Pred),0)\n        X[\"Pred95\"]=np.maximum(X.Pred + (X.Std+X.StdRes)*2 \/np.sqrt(X.Mean)*np.sqrt(X.Pred),0)\n        \n        #print(\"\\nCV score: \",score,\"\\nMean: {:.4f} Std: {:.4f}\\n\".format(score.mean(), score.std()))\n\n        X.fillna(0,inplace=True)\n        \n        X[\"ResLV\"] = (X.Pred-X.Value)*X.Weight\n        X.loc[X.DayYear > val_end,\"ResLV\"]=0\n        \n        if loop == \"Public\": \n            #print(X.loc[(X.DayYear > start) & (X.DayYear <=val_end),[\"ResLV\",\"Pred\",\"Value\"]].describe())\n            print(cl,\": \",loop,\" submission - Score: \",np.sqrt(np.abs(X.loc[(X.DayYear >= sub_start) & (X.DayYear <=val_end),[\"ResLV\"]]).mean()))\n        X[\"Target\"]=X.Pred\n        X[\"Quantile\"]=0.5\n        X[\"IdSub\"]=X.Id.astype(str)+\"_0.5\"\n        addsub = pd.concat([addsub,X.loc[X.Test==1,[\"IdSub\",\"Target\",\"ResLV\",\"Quantile\",\"Type\",\"DayYear\",\"Ctry\",\"Prov\",\"Cty\",\"Sub\"]]])\n        X[\"IdSub\"]=X.Id.astype(str)+\"_0.05\"\n        X.Quantile=0.05\n        X.Target=X.Pred05\n        X.ResLV=(X.Pred05-X.Value)*X.Weight\n        addsub = pd.concat([addsub,X.loc[X.Test==1,[\"IdSub\",\"Target\",\"ResLV\",\"Quantile\",\"Type\",\"DayYear\",\"Ctry\",\"Prov\",\"Cty\",\"Sub\"]]])\n        X[\"IdSub\"]=X.Id.astype(str)+\"_0.95\"\n        X.Quantile=0.95\n        X.Target=X.Pred95\n        X.ResLV=(X.Pred95-X.Value)*X.Weight\n        addsub = pd.concat([addsub,X.loc[X.Test==1,[\"IdSub\",\"Target\",\"ResLV\",\"Quantile\",\"Type\",\"DayYear\",\"Ctry\",\"Prov\",\"Cty\",\"Sub\"]]])\n        addsub.fillna(0,inplace=True)\n                      \n    addsub[\"Err\"]= (addsub.ResLV > 0)*(1-addsub.Quantile)*(addsub.ResLV)+(addsub.ResLV <= 0)*(addsub.Quantile)*(-addsub.ResLV) \n    print (\"Final Score\",addsub.loc[addsub.DayYear <= val_end,\"Err\"].mean())\n    ","678df2d1":"fig = px.scatter(addsub[(addsub.Ctry==\"Austria\") & (addsub.Type == \"ConfirmedCases\")], x='DayYear', y='Target', color=\"Sub\")\nfig.show()\nfig = px.scatter(addsub[(addsub.Ctry==\"Austria\") & (addsub.Type == \"Fatalities\")], x='DayYear', y='Target', color=\"Sub\")\nfig.show()\nfig = px.scatter(addsub[(addsub.Ctry==\"United Kingdom\") & (addsub.Type == \"ConfirmedCases\")], x='DayYear', y='Target', color=\"Sub\")\nfig.show()\nfig = px.scatter(addsub[(addsub.Ctry==\"United Kingdom\") & (addsub.Type == \"Fatalities\")], x='DayYear', y='Target', color=\"Sub\")\nfig.show()\nfig = px.scatter(addsub[(addsub.Ctry==\"Russia\") & (addsub.Type == \"ConfirmedCases\")], x='DayYear', y='Target', color=\"Sub\")\nfig.show()\nfig = px.scatter(addsub[(addsub.Ctry==\"Russia\") & (addsub.Type == \"Fatalities\")], x='DayYear', y='Target', color=\"Sub\")\nfig.show()\n","9163da77":"addsub.loc[(addsub.DayYear > 117+14) & (addsub.Ctry == \"US\"),[\"Target\",\"Type\",\"Sub\",\"Quantile\",\"Prov\"]].groupby([\"Prov\",\"Type\",\"Quantile\",\"Sub\"]).sum()","0a39110f":"fig = px.scatter(addsub[(addsub.Cty==\"New York\") & (addsub.Type == \"ConfirmedCases\")], x='DayYear', y='Target', color=\"Sub\")\nfig.show()\nfig = px.scatter(addsub[(addsub.Cty==\"New York\") & (addsub.Type == \"Fatalities\")], x='DayYear', y='Target', color=\"Sub\")\nfig.show()","5301ec0d":"addsub[addsub.Sub==\"Private\"].describe()","93121462":"addsub[addsub.Sub==\"Public\"].describe()","cf5a6d29":"mysub=sample_sub.set_index('Id')\nfinalsub = addsub[((addsub.Sub==\"Public\") & (addsub.DayYear < 132)) | ((addsub.Sub==\"Private\") & (addsub.DayYear >= 132))]\nmysub= mysub.merge(finalsub[[\"IdSub\",\"Target\"]],left_index=True,right_on=\"IdSub\",how=\"left\")\nmysub.drop(columns=[\"Value\"],inplace=True)\nmysub.columns=[\"ForecastId_Quantile\",\"TargetValue\"]\nmysub.fillna(0,inplace=True)\nmysub.to_csv('submission.csv',index=False)","99409f70":"mysub.describe()","b324bbcc":"mysub.head(100)","1f26a096":"train.head(300)","9fba159f":"## Linear regression analysis for the COVID 19 Global Forecasting Challenge Week 5\u00b6\n\nUpdated model from Week 4 to adjust for revised training data and evaluation metric - not enough time to make further changes to obtain better fit to training data.\n\n### Data import\n\nThe external data for the submission has been derived from some of the World Development Indicators from the World Bank Open Data (Population, GDP and health spending) - with some adjustments and estimates for missing data items.\n\nYou can find the full dateset and licence here: https:\/\/www.kaggle.com\/theworldbank\/world-development-indicators","007b06b0":"### Create submission file"}}