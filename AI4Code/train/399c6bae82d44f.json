{"cell_type":{"d67dfa2e":"code","fa62a3b6":"code","5b5851fe":"code","6ae20224":"code","674c54f2":"code","d497ba0c":"code","720969b6":"code","ed18af5c":"code","b5b96456":"code","1c0885ed":"code","74d73f30":"code","62a06ba6":"code","d06ba7e5":"code","20078d13":"code","65ce4838":"code","5822a801":"code","b38a60b9":"code","4abb8a1a":"code","41a364df":"code","a67f3258":"code","2b80d2b0":"code","e5478ff1":"code","b50f601d":"code","3f5fdfb3":"code","e358d897":"markdown","1b248cfb":"markdown","92dad719":"markdown","30df94ea":"markdown","1c4145e0":"markdown","d30bd4de":"markdown","671b51e8":"markdown"},"source":{"d67dfa2e":"import numpy as np \nimport pandas as pd \nimport torch\nimport torch.nn as nn\nimport re\nimport fasttext.util\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import DataLoader\nfrom tqdm.notebook import tqdm","fa62a3b6":"INPUT_PATH = '\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv'\ndata = pd.read_csv(INPUT_PATH)[['v1', 'v2']]\ndata['target'] = (data['v1'] == 'spam').astype(int)\ndata['v2'] = data['v2'].apply(lambda w : w.lower())\ndata['v2'] = data['v2'].apply(lambda w : re.sub(r'[^a-z]+', ' ', w))","5b5851fe":"data.head(5)","6ae20224":"not_null_ind = data['v2'].apply(lambda w : len(w.strip()) > 0)\nsentences = data['v2'][not_null_ind].values\npre_target = data['target'][not_null_ind].values.astype(int)\nwords = map(lambda w : w.split(), sentences)\nwords = list(filter(lambda w : len(w) > 0, words))","674c54f2":"device = 'cuda'","d497ba0c":"%%time\nembeddings_dict = {}\nwith open(\"\/kaggle\/input\/glove-global-vectors-for-word-representation\/glove.6B.50d.txt\", 'r') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vector = np.asarray(values[1:], \"float32\")\n        embeddings_dict[word] = vector","720969b6":"embeddings = []\ntarget = []\nfor i, sentence in enumerate(words):\n    sent_emb = []\n    for word in sentence:\n        embedding_vector = embeddings_dict.get(word)\n        if embedding_vector is not None:\n            sent_emb.append([embedding_vector])\n    if len(sent_emb) > 0:\n        embeddings.append(torch.Tensor(sent_emb).to(device))\n        target.append(pre_target[i])","ed18af5c":"len(embeddings), len(target), embeddings[0].shape","b5b96456":"class CustomDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = torch.Tensor(y).to(device)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]","1c0885ed":"training_data = CustomDataset(embeddings, target)\ntrain_dataloader = DataLoader(training_data, batch_size=1, shuffle=True)","74d73f30":"class Head(nn.Module):\n    def __init__(self, input_size):\n        super(Head, self).__init__()\n        self.layer = nn.Linear(input_size, 1)\n        self.act = nn.Sigmoid()\n        \n    def forward(self, x):\n        return self.act(self.layer(x))","62a06ba6":"class RNN(nn.Module):\n    \n    def __init__(self, input_size, hidden_size, output_size):\n        super(RNN, self).__init__()\n        self.Lo = nn.Linear(input_size + hidden_size, output_size)\n        self.tanh = nn.Tanh()\n\n    def forward(self, x, hidden):\n        return self.tanh( self.Lo( torch.cat([x, hidden], dim=1) ) ) ","d06ba7e5":"hidden_size = 50\nepochs = 3\nrnn_model = RNN(embeddings[0][0][0].size(0), hidden_size, hidden_size).to(device)\nhead_model = Head(hidden_size).to(device)\noptimizer = torch.optim.Adam( list(rnn_model.parameters()) + list(head_model.parameters()) )\ncriterion = nn.BCEWithLogitsLoss()","20078d13":"for epoch in range(epochs):\n    \n    losses = []\n    i = 0\n    for X_in, y_in in train_dataloader:\n        \n        rnn_model.zero_grad()\n        head_model.zero_grad()\n\n        ht = torch.zeros(1, hidden_size).to(device)\n\n        for i, x_in in enumerate(X_in[0]):\n            ht = rnn_model(x_in, ht)\n\n        out = head_model(ht)\n        loss = criterion( out, y_in.unsqueeze(1) )\n        \n        loss.backward()\n        optimizer.step()\n        \n        if not np.isnan(loss.item()):\n            losses.append(loss.item())\n        \n    print(np.mean(losses))","65ce4838":"class LSTM(nn.Module):\n    \n    def __init__(self, input_dim, hidden_dim):\n        super(LSTM, self).__init__()\n        \n        self.forget_gate = nn.Sequential(\n            nn.Linear( input_dim + hidden_dim, hidden_dim ),\n            nn.Sigmoid()\n        )\n        self.input_gate = nn.Sequential(\n            nn.Linear( input_dim + hidden_dim, hidden_dim ),\n            nn.Sigmoid()\n        )\n        self.new_info = nn.Sequential(\n            nn.Linear( input_dim + hidden_dim, hidden_dim ),\n            nn.Tanh()\n        )\n        self.output_gate = nn.Sequential(\n            nn.Linear( input_dim + hidden_dim, hidden_dim ),\n            nn.Sigmoid()\n        )\n        self.new_hidden = nn.Sequential(\n            nn.Linear( hidden_dim, hidden_dim ),\n            nn.Tanh()\n        )\n        \n        self.head = nn.Sequential(\n            nn.Linear( hidden_dim, 1 ),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x, hidden, cell):\n        combined = torch.cat( [x, hidden], 1 )\n        old_info = self.forget_gate(combined) * cell\n        new_info = self.input_gate(combined) * self.new_info(combined)\n        new_cell_state = old_info + new_info\n        new_hidden = self.output_gate(combined) * self.new_hidden(new_cell_state)\n        output = self.head(new_hidden)\n        return new_hidden, new_cell_state, output","5822a801":"hidden_size = 50\nepochs = 3\nlstm_model = LSTM(embeddings[0][0][0].size(0), hidden_size).to(device)\noptimizer = torch.optim.Adam( lstm_model.parameters() )\ncriterion = nn.BCEWithLogitsLoss()","b38a60b9":"def train(model, optimizer, train_dataloader):\n    for epoch in range(epochs):\n        losses = []\n        for X_in, y_in in train_dataloader:\n\n            model.zero_grad()\n            ht = torch.zeros(1, hidden_size).to(device)\n            ct = torch.zeros(1, hidden_size).to(device)\n\n            for i, x_in in enumerate(X_in[0]):\n                ht, ct, output = model(x_in, ht, ct)\n\n            loss = criterion( output, y_in.unsqueeze(1) )\n\n            loss.backward()\n            optimizer.step()\n\n            if not np.isnan(loss.item()):\n                losses.append(loss.item())\n            else:\n                print('nan')\n\n        print(np.mean(losses))\n\n    return model, optimizer, losses","4abb8a1a":"_ = train(lstm_model, optimizer, train_dataloader)","41a364df":"class GRU(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(GRU, self).__init__()\n        self.reset_gate = nn.Sequential(\n            nn.Linear(input_dim + hidden_dim, hidden_dim),\n            nn.Sigmoid()\n        )\n        self.new_info = nn.Sequential(\n            nn.Linear(input_dim + hidden_dim, hidden_dim),\n            nn.Tanh()\n        )\n        self.update_gate = nn.Sequential(\n            nn.Linear(input_dim + hidden_dim, hidden_dim),\n            nn.Sigmoid()\n        )\n        \n        self.head = nn.Sequential(\n            nn.Linear(hidden_dim, 1),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x, hidden):\n        combined = torch.cat( [x, hidden], 1 )\n        forget_hidden = self.reset_gate(combined) * hidden\n        new_combined = torch.cat( [x, forget_hidden], 1 )\n        updated = self.update_gate(combined)\n        new_hidden = hidden * (1 - updated) + self.new_info(new_combined) * updated\n        output = self.head(new_hidden)\n        return new_hidden, output","a67f3258":"hidden_size = 50\nepochs = 3\ngru_model = GRU(embeddings[0][0][0].size(0), hidden_size).to(device)\noptimizer = torch.optim.Adam( gru_model.parameters() )\ncriterion = nn.BCEWithLogitsLoss()","2b80d2b0":"def train(model, optimizer, train_dataloader):\n    for epoch in range(epochs):\n        losses = []\n        for X_in, y_in in train_dataloader:\n\n            model.zero_grad()\n            ht = torch.zeros(1, hidden_size).to(device)\n\n            for i, x_in in enumerate(X_in[0]):\n                ht, output = model(x_in, ht)\n\n            loss = criterion( output, y_in.unsqueeze(1) )\n\n            loss.backward()\n            optimizer.step()\n\n            if not np.isnan(loss.item()):\n                losses.append(loss.item())\n            else:\n                print('nan')\n\n        print(np.mean(losses))\n\n    return model, optimizer, losses","e5478ff1":"_ = train(gru_model, optimizer, train_dataloader)","b50f601d":"def get_word_by_prob(probs):\n    pass\ndef get_embedding_by_word(word):\n    pass","3f5fdfb3":"class AttentionGRU(nn.Module):\n    def __init__(self, input_size, hidden_size, seq_len, vocab_size):\n        super(AttentionLSTM, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.seq_len = seq_len\n        self.vocab_size = vocab_size\n        \n        self.enc = GRU(input_size, hidden_size)\n        self.dec = GRU(input_size, hidden_size)\n        self.proj = nn.Sequential(\n            nn.Linear(hidden_size, seq_len),\n            nn.Softmax(seq_len)\n        )\n        self.vocab_proj = nn.Sequential(\n            nn.Linear(hidden_size, vocab_size),\n            nn.Softmax(vocab_size)\n        )\n        \n    def forward(self, seq_x, start_emb):\n        assert len(seq_x) == self.seq_len\n        hs = []\n        hidden = torch.zeros((1, self.hidden_size)).to(device)\n        \n        for x in seq_x:\n            hidden = self.enc(x, hidden)\n            hs.append(hidden)\n        \n        hs = torch.Tensor(hs).to(device)\n        \n        hidden = torch.zeros((1, self.hidden_size)).to(device)\n        prev = start_emb\n        outputs = []\n        for i in range(self.seq_len):\n            hidden = self.dec(prev, hidden)\n            input_mask = self.proj(hidden)\n            att_vector = (input_mask * hs).sum(dim=-1)\n            output = self.vocab_proj(att_vector)\n            \n            word = get_word_by_prob(output)\n            outputs.append(word)\n            \n            prev = get_embedding_by_word(word)","e358d897":"## Simple RNN","1b248cfb":"# Simple RNN, LSTM and GRU implementation. \n# Pipeline for spam detection task solving, using pretrained glove embedding.","92dad719":"## Data preparing","30df94ea":"## GRU","1c4145e0":"## LSTM","d30bd4de":"## Attention","671b51e8":"Just a scheme for attention in RNN implementation. You need to specify vocabulary, get_word_by_prob function, get_embedding_by_word function and reform train data to sequences of words."}}