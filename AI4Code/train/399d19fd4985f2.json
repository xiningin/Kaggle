{"cell_type":{"736e7bbd":"code","50f9e996":"code","35287d4c":"code","2bd9d71e":"code","756ade0f":"code","c49990e7":"code","c8e94ace":"code","2cebfc48":"code","f717bfda":"code","3d40b9c7":"code","d63580b7":"code","5578f27a":"code","490f1abb":"markdown","f758a9c5":"markdown","8dac5056":"markdown","8252f9f8":"markdown","38df7753":"markdown"},"source":{"736e7bbd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","50f9e996":"train_dir = \"..\/input\/train\/\"\ntest_dir = \"..\/input\/test\"\n\ndf_train = pd.read_csv('..\/input\/train_labels.csv')\ndf_train.head()","35287d4c":"# taking 10000 sample so our model run fast (experimentation)\n\nfrom sklearn.model_selection import train_test_split\n\n# df = df_train.sample(n=10000, random_state=2018)\ndf = df_train # using full dataset\ntrain, valid = train_test_split(df,test_size=0.2)","2bd9d71e":"# minimal preprocessing for experimentation. Will add more suitable preprocessing in the future\n\nfrom keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(preprocessing_function=lambda x:(x - x.mean()) \/ x.std() if x.std() > 0 else x,\n                                   horizontal_flip=True,\n                                   vertical_flip=True)\n\ntest_datagen = ImageDataGenerator(preprocessing_function=lambda x:(x - x.mean()) \/ x.std() if x.std() > 0 else x)","756ade0f":"# use flow_from_dataframe method to build train and valid generator\n# Only shuffle the train generator as we want valid generator to have the same structure as test\n\ntrain_generator = train_datagen.flow_from_dataframe(\n    dataframe = train,\n    directory='..\/input\/train\/',\n    x_col='id',\n    y_col='label',\n    has_ext=False,\n#     subset='training',\n    batch_size=32,\n    seed=2018,\n    shuffle=True,\n    class_mode='binary',\n    target_size=(96,96))\n\nvalid_generator = test_datagen.flow_from_dataframe(\n    dataframe = valid,\n    directory='..\/input\/train\/',\n    x_col='id',\n    y_col='label',\n    has_ext=False,\n#     subset='validation',\n    batch_size=32,\n    seed=2018,\n    shuffle=False,\n    class_mode='binary',\n    target_size=(96,96)\n)","c49990e7":"from keras.applications.resnet50 import ResNet50\nfrom keras.models import Sequential\nfrom keras import layers\n\nIMG_SIZE = (96, 96)\nIN_SHAPE = (*IMG_SIZE, 3)\n\ndropout_dense=0.5\n\nconv_base = ResNet50(\n    weights='imagenet',\n    include_top=False,\n    input_shape=IN_SHAPE\n)\n       \nmodel = Sequential()\nmodel.add(conv_base)\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(256, use_bias=False))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Activation(\"relu\"))\nmodel.add(layers.Dropout(dropout_dense))\nmodel.add(layers.Dense(1, activation = \"sigmoid\"))\n\nconv_base.summary()","c8e94ace":"# freeze layer. unfreeze start at layer 5. if freeze everything, val acc will be really bad\n\nconv_base.Trainable=True\n\nset_trainable=False\nfor layer in conv_base.layers:\n    if layer.name == 'res5a_branch2a':\n        set_trainable = True\n    if set_trainable:\n        layer.trainable = True\n    else:\n        layer.trainable = False\n    ","2cebfc48":"from keras import optimizers\n\n# conv_base.trainable = False\nmodel.compile(optimizers.Adam(0.01), loss = \"binary_crossentropy\", metrics=[\"accuracy\"])","f717bfda":"# from keras.models import Sequential\n# from keras import layers\n\n# kernel_size=(3,3)\n# pool_size=(2,2)\n# first_filter=32\n# second_filter=64\n# third_filter=128\n\n# dropout_conv=0.3\n\n# model = Sequential()\n# model.add(layers.Conv2D(first_filter, kernel_size, activation='relu', input_shape= (96,96,3)))\n# model.add(layers.Conv2D(first_filter, kernel_size, use_bias=False))\n# model.add(layers.BatchNormalization())\n# model.add(layers.Activation('relu'))\n# model.add(layers.MaxPool2D(pool_size=pool_size))\n# model.add(layers.Dropout(dropout_conv))\n\n# model.add(layers.Conv2D(second_filter, kernel_size, use_bias=False))\n# model.add(layers.BatchNormalization())\n# model.add(layers.Activation('relu'))\n# model.add(layers.Conv2D(second_filter, kernel_size, use_bias=False))\n# model.add(layers.BatchNormalization())\n# model.add(layers.Activation(\"relu\"))\n# model.add(layers.MaxPool2D(pool_size = pool_size))\n# model.add(layers.Dropout(dropout_conv))\n\n# model.add(layers.Conv2D(third_filter, kernel_size, use_bias=False))\n# model.add(layers.BatchNormalization())\n# model.add(layers.Activation(\"relu\"))\n# model.add(layers.Conv2D(third_filter, kernel_size, use_bias=False))\n# model.add(layers.BatchNormalization())\n# model.add(layers.Activation(\"relu\"))\n# model.add(layers.MaxPool2D(pool_size = pool_size))\n# model.add(layers.Dropout(dropout_conv))\n\n# #model.add(GlobalAveragePooling2D())\n# model.add(layers.Flatten())\n# model.add(layers.Dense(256, use_bias=False))\n# model.add(layers.BatchNormalization())\n# model.add(layers.Activation(\"relu\"))\n# model.add(layers.Dropout(dropout_dense))\n# model.add(layers.Dense(1, activation = \"sigmoid\"))\n\n# # Compile the model\n# model.compile(optimizers.Adam(0.01), loss = \"binary_crossentropy\", metrics=[\"accuracy\"])","3d40b9c7":"from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nSTEP_SIZE_TRAIN=train_generator.n\/\/train_generator.batch_size\nSTEP_SIZE_VALID=valid_generator.n\/\/valid_generator.batch_size\n\nearlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose=1, restore_best_weights=True)\nreducel = ReduceLROnPlateau(monitor='val_loss', patience=1, verbose=1, factor=0.1)\n\nhistory = model.fit_generator(train_generator, steps_per_epoch=STEP_SIZE_TRAIN, \n                    validation_data=valid_generator,\n                    validation_steps=STEP_SIZE_VALID,\n                    epochs=13,\n                   callbacks=[reducel, earlystopper])","d63580b7":"from glob import glob\nfrom skimage.io import imread\n\nbase_test_dir = '..\/input\/test\/'\ntest_files = glob(os.path.join(base_test_dir,'*.tif'))\nsubmission = pd.DataFrame()\nfile_batch = 5000\nmax_idx = len(test_files)\nfor idx in range(0, max_idx, file_batch):\n    print(\"Indexes: %i - %i\"%(idx, idx+file_batch))\n    test_df = pd.DataFrame({'path': test_files[idx:idx+file_batch]})\n    test_df['id'] = test_df.path.map(lambda x: x.split('\/')[3].split(\".\")[0])\n    test_df['image'] = test_df['path'].map(imread)\n    K_test = np.stack(test_df[\"image\"].values)\n    K_test = (K_test - K_test.mean()) \/ K_test.std()\n    predictions = model.predict(K_test)\n    test_df['label'] = predictions\n    submission = pd.concat([submission, test_df[[\"id\", \"label\"]]])\nsubmission.head()","5578f27a":"submission.to_csv(\"submission.csv\", index = False, header = True)","490f1abb":"## Predict the test and submission\n\nThanks to @fmarazzi for the elegent code for predicting test and submission. https:\/\/www.kaggle.com\/fmarazzi\/baseline-keras-cnn-roc-fast-10min-0-925-lb","f758a9c5":"## Training the model","8dac5056":"## Building the model\n\nUse pretrained ResNet50 model with Adam optimizer and binary cross entropy loss. Freeze Resnet50 layer until \"res5a_branch2a\". Freezing whole layer will give really bad results as the image is really different compared to imagenet dataset which Resnet50 model trained on.","8252f9f8":"## Use only 10k rows for experimentation and split up the dataset into train and test","38df7753":"## Loading data"}}