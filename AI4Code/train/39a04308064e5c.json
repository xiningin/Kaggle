{"cell_type":{"d082395f":"code","28d278d9":"code","43b596cf":"code","490db7cf":"code","5cfcf215":"code","ade0a388":"code","e49d2019":"code","5ae434c5":"code","e5a9a714":"code","e5885aca":"code","bd10d9e2":"code","2163399e":"code","516b3eaa":"code","26d38ee1":"code","43df8009":"code","f6f79e0a":"code","1dbabf2c":"code","636c63cd":"code","b58f57ad":"code","a6567569":"code","cc7aea3b":"code","6dd7cb25":"code","3da44dd7":"code","45d07c62":"code","71a4ad2d":"code","99bb0221":"code","f79f1aa0":"code","6ce868dc":"code","e7fa50d5":"code","697d4a6e":"code","f6857395":"code","d19981a1":"code","3649513e":"code","e6850785":"code","64aad5d5":"code","b40dbe76":"code","f004ba4b":"code","54cb2479":"code","f6c69b44":"markdown","d782e72a":"markdown","bd1d45de":"markdown","ae63218b":"markdown","2941dee7":"markdown","c22c694a":"markdown","baaf0f19":"markdown","d952d2c4":"markdown","6c08e8b6":"markdown","3335220a":"markdown","a622890d":"markdown","6514b7ae":"markdown","2fc498c2":"markdown","2f9d13f6":"markdown","0686e67a":"markdown","97906fc9":"markdown","3256f6b1":"markdown","a547e50c":"markdown","e003b83f":"markdown","1c35cafc":"markdown","c1272e98":"markdown","dfd1cd19":"markdown","68aac66e":"markdown","534b87f6":"markdown","ae30b6af":"markdown","4650de60":"markdown","831f03cb":"markdown","7b807df8":"markdown","1a0e60ba":"markdown","2474b50c":"markdown","9ab3af10":"markdown","e4a59ee9":"markdown","aa6c8e9e":"markdown","8c152e4c":"markdown","f6532336":"markdown","bd80b5d1":"markdown","c9378363":"markdown","d24bbd02":"markdown","7a74653e":"markdown","d09b22fa":"markdown","7b52afad":"markdown","10d1419c":"markdown","bce84319":"markdown","99127b82":"markdown","c6308b39":"markdown","8efd9674":"markdown","93ee0f2c":"markdown","51af26d2":"markdown","6576d61a":"markdown","8c3e4415":"markdown","e3c25ba9":"markdown","618bb830":"markdown","bd91933d":"markdown","841218e0":"markdown","7e398fff":"markdown","e38d896a":"markdown","bf7f218e":"markdown","b32d932f":"markdown","e6cbbd1e":"markdown","e56c2e0e":"markdown","2937ed7d":"markdown","db0649c4":"markdown"},"source":{"d082395f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","28d278d9":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\nimport math\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as ss\nimport cufflinks as cf\nfrom collections import Counter\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\nfrom nltk.tokenize import word_tokenize\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\n\nimport keras\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Embedding, LSTM, SpatialDropout1D, Bidirectional\nfrom keras.callbacks import ModelCheckpoint\nfrom sklearn.metrics import roc_auc_score\nfrom keras.preprocessing.text import Tokenizer\n","43b596cf":"data = pd.read_csv('..\/input\/Tweets.csv')\n","490db7cf":"data.head(100)\n","5cfcf215":"data.info()\n","ade0a388":"column_names = []\ncolumn_counts = []\nfor column in data:\n    column_names.append(column)\n    column_counts.append(data[column].count())","e49d2019":"#visualize columns count\nplt.subplots(figsize=(25,20))\nsns.barplot(x=column_names, y=column_counts)","5ae434c5":"#visualize columns names\nlist(data.columns)\n","e5a9a714":"cf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\ndata['airline_sentiment'].iplot(\n    kind='hist',\n    bins=50,\n    xTitle='polarity',\n    linecolor='black',\n    yTitle='count',\n    title='airline_sentiment Distribution')","e5885aca":"data['text'].iplot(\n    kind='hist',\n    bins=50,\n    xTitle='polarity',\n    linecolor='black',\n    yTitle='count',\n    title='text Distribution')","bd10d9e2":"data = data.drop_duplicates()","2163399e":"data['text_length'] = data['text'].apply(len)\nhist = data.hist(bins=4)","516b3eaa":"dp=data[ data['airline_sentiment'] == 'positive']\ndg=data[ data['airline_sentiment'] == 'negative']\ndn=data[ data['airline_sentiment'] == 'neutral']\npositive_retweet_mean=dp['retweet_count'].mean()\nnegative_retweet_mean=dg['retweet_count'].mean()\nneutral_retweet_mean=dn['retweet_count'].mean()\nprint(\"positive_retweet_mean\", positive_retweet_mean)\nprint(\"negative_retweet_mean\", negative_retweet_mean)\nprint(\"neutral_retweet_mean\",neutral_retweet_mean)","26d38ee1":"data.corr()\n","43df8009":"def correlation_ratio(categories, measurements):\n    fcat, _ = pd.factorize(categories)\n    cat_num = np.max(fcat)+1\n    y_avg_array = np.zeros(cat_num)\n    n_array = np.zeros(cat_num)\n    for i in range(0,cat_num):\n        cat_measures = measurements[np.argwhere(fcat == i).flatten()]\n        n_array[i] = len(cat_measures)\n        y_avg_array[i] = np.average(cat_measures)\n    y_total_avg = np.sum(np.multiply(y_avg_array,n_array))\/np.sum(n_array)\n    numerator = np.sum(np.multiply(n_array,np.power(np.subtract(y_avg_array,y_total_avg),2)))\n    denominator = np.sum(np.power(np.subtract(measurements,y_total_avg),2))\n    if numerator == 0:\n        eta = 0.0\n    else:\n        eta = np.sqrt(numerator\/denominator)\n    return eta","f6f79e0a":"#rank for the bad airline\nnegative_tweets = data[data['airline_sentiment'].str.contains(\"negative\")]\nbad_airline = negative_tweets[['airline','airline_sentiment_confidence','negativereason']]\nbad_airline_count = bad_airline.groupby('airline', as_index=False).count()\nbad_airline_count.sort_values('negativereason', ascending=False)","1dbabf2c":"#rank for the good airline\npositive_tweets = data[data['airline_sentiment'].str.contains(\"positive\")]\ngood_airline = positive_tweets[['airline','airline_sentiment_confidence']]\ngood_airline_count = good_airline.groupby('airline', as_index=False).count()\ngood_airline_count.sort_values('airline_sentiment_confidence', ascending=False)\n","636c63cd":"reason = negative_tweets[['airline','negativereason']]\nbad_flight_reason_count = reason.groupby('negativereason', as_index=False).count()\nbad_flight_reason_count.sort_values('negativereason', ascending=False)","b58f57ad":"data_new = pd.read_csv('..\/input\/Tweets.csv')\ndf = pd.DataFrame([])\ndf['airline_sentiment'] = data_new['airline_sentiment']\ndf['text'] = data_new['text']\ndf['negativereason'] = data_new['negativereason']","a6567569":"df.head()","cc7aea3b":"class Normalizer:\n    def __init__(self):\n        self.stop_words = stopwords.words('english')\n    \n    def lower(self, text):\n        return text.lower()\n    \n    def remove_punctuations(self, text):\n        return re.sub('[^\\w\\s]','', text)\n    \n    def remove_mentions(self, text):\n        return re.sub('@[a-zA-Z0-9-._]+', '', text)\n    \n    def remove_url(self, text):\n        url_ptrn = r'''(?i)\\b((?:[a-z][\\w-]+:(?:\/{1,3}|[a-z0-9%])|\n            www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}\/)(?:[^\\s()<>]+|\n            \\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|\n            (\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?\u00ab\u00bb\"\"'']))'''\n        return re.sub(url_ptrn, '', text)\n    \n    def remove_email(self, text):\n        return re.sub(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+',\n                             '', text)\n    def remove_stop_words(self, text):\n        return \" \".join(x for x in text.split() if x not in self.stop_words)\n    \n    def correct_spelling(self, text):\n        return str(TextBlob(text).correct())\n    \n    def tokenize(self, text):\n        return word_tokenize(text)\n    \n    def normalize(self, text):\n        text = self.lower(text)\n        text = self.remove_url(text)\n        text = self.remove_email(text)\n        text = self.remove_mentions(text)\n# #         text = self.correct_spelling(text)\n#         text = self.remove_stop_words(text)\n        text = self.remove_punctuations(text)\n        text = text.strip()\n        return text","6dd7cb25":"normalizer = Normalizer()\n","3da44dd7":"df.text = df.text.apply(normalizer.normalize)\n","45d07c62":"def word_cloud(text, Stop_words=None,path=None,height=3000, width=3000):\n    wc = WordCloud(background_color=\"white\", max_words=2000,\n                contour_width=3, contour_color='steelblue', stopwords=Stop_words,height=3000,width=3000).generate(text)\n    wc.to_file(path)\n    plt.imshow(wc, interpolation='bilinear', shape=(width, height ))\n    plt.axis(\"off\")\n    plt.figure()\n\nall_text = ''\nfor row in df.text:\n    all_text += ' ' + str(row)\nword_cloud(all_text, path='Cloud.png')\n","71a4ad2d":"vectorizer = TfidfVectorizer(ngram_range=(1,1))\nX = vectorizer.fit_transform(df.text)\ny = df.airline_sentiment","99bb0221":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n","f79f1aa0":"clf_mnb = MultinomialNB().fit(X_train, y_train)\n\n","6ce868dc":"clf_svm = svm.LinearSVC(penalty='l2',C=1).fit(X_train, y_train)\n","e7fa50d5":"print (\"MNB score for train\",clf_mnb.score(X_train, y_train))\nprint (\"MNB score for test\",clf_mnb.score(X_test, y_test))\nprint (\"SVM score for train\",clf_svm.score(X_train, y_train))\nprint (\"SVM score for test\",clf_svm.score(X_test, y_test))","697d4a6e":"max_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(df.text.values)\nX = tokenizer.texts_to_sequences(df.text.values)\n","f6857395":"X = pad_sequences(X)","d19981a1":"Y = pd.get_dummies(df['airline_sentiment']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 42, stratify=Y)\nX_train.shape","3649513e":"X_vald = X_train[:500]\nY_vald = Y_train[:500]\nx_train = X_train[500:]\ny_train = Y_train[500:]","e6850785":"embed_dim = 128\nlstm_out = 196\n\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\nmodel.add(Dropout(0.5))\nmodel.add(Bidirectional(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2)))\nmodel.add(Dense(3,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","64aad5d5":"batch_size = 512\nhistory = model.fit(x_train, \n                    y_train, \n                    epochs = 10, \n                    batch_size=batch_size, \n                    validation_data=(X_vald, Y_vald))","b40dbe76":"import matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","f004ba4b":"plt.clf()\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","54cb2479":"scores = model.evaluate(X_test, Y_test)\nscores","f6c69b44":"tried naive bayes, random forest and svm and select the two highest accuracy","d782e72a":"select rows in my data frame contains positive the choose the rows from the selected columns in the second line meets up to the selected rows, group it bu airline and sorted by negative reasons","bd1d45de":" ### split data into train and test","ae63218b":"We apply all the selected normalizer operations on the text column in our dataframe","2941dee7":"score() : In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true.\n\n","c22c694a":"## Evluate Ml model","baaf0f19":"## Evaluate the model","d952d2c4":"This method prints information about a DataFrame including the index dtype and column dtypes, non-null values and memory usage.\n","6c08e8b6":"we create a Word Cloud to visualize the most occuring words in our data","3335220a":"after visualizations and analysis, i selected the columns that seemed to be most important to create new data frame","a622890d":"Pandas head() method is used to return top 5 -by default- rows of a data frame or series.","6514b7ae":"## train the model","2fc498c2":"Pandas head() method is used to return top 100 rows of a data frame or series.","2f9d13f6":" ### split train data to train and validation set","0686e67a":"# Machine Learning Model","97906fc9":"## Data Analysis","3256f6b1":"The validation dataset provides an unbiased evaluation of a model fit on the training dataset while tuning the model's hyperparameters","a547e50c":"# Exploring Data\n","e003b83f":"We create a normalizer class that is responsible for doing preprocessing and cleaning functions like removing punctuations, mentions, urls, emails, stopwords","1c35cafc":"# Importing Libraries","c1272e98":" I used: Tensorflow and Keras to define the neural network\n \n Embedding: will learn an embedding for all of the words in the training datase\n \n Dropout: A Simple Way to Prevent Neural Networks from Overfitting\n \n \n LSTM :  allows you to specify the merge mode, that is how the forward and backward outputs should be combined before being passed on to the next layer\n \n \n Dense: recieves input from all the neurons in the previous layer, thus densely connected\n \n \n categorical_crossentropy: as our target to classify three labels","dfd1cd19":"found out that airline sentiment gold and negative reason gold and tweet coord have too many null data to be important","68aac66e":"Compute pairwise correlation of columns, excluding NA\/null values.\n\n","534b87f6":"### tokenizer","ae30b6af":"# Preprocessing Text Data","4650de60":"### MNB Classifier","831f03cb":"takes in a sequence of data-points gathered at equal intervals, along with time series parameters such as stride, length of history, etc., to produce batches for training\/validation.","7b807df8":"We initialize the Normalizer","1a0e60ba":"encoded the labels of the three classes to fit the model required format","2474b50c":"## preprocessing for DL","9ab3af10":"We create a tf-idf vectorizer to represent our data in format that would be acceptable for the ML models.","e4a59ee9":"# Deep Learning Model","aa6c8e9e":"Returns the loss value & metrics values for the model in test mode.","8c152e4c":"### SVM Classifier","f6532336":"## Create DL Model","bd80b5d1":"## apply histogram to see the distribution of data","c9378363":"### word cloud","d24bbd02":"## visualize the results","7a74653e":"we use stratify to equally split the three classes into train and test groups","d09b22fa":"### split the data into train and test","7b52afad":"A histogram is used to summarize discrete or continuous data. In other words, it provides a visual interpretation of numerical data by showing the number of data points that fall within a specified range of values (called \u201cbins\u201d). It is similar to a vertical bar graph","10d1419c":"## calc mean of retweet count for each category to decide if this feature is important","bce84319":"# Loading Data","99127b82":"## create ml model","c6308b39":"* ## airline_sentiment distribution","8efd9674":"## preprocessing for ML model","93ee0f2c":"visualize reasons based on the negative tweets of the selected columns","51af26d2":"Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\nLinearSVC is capable of performing multi-class classification on a dataset.","6576d61a":"## Utils","8c3e4415":"creating probability distributions for some columns of my data frame by distplot from seaborn library sns.distplot().","e3c25ba9":"select rows in my data frame contains negative the choose the rows from the selected columns in the second line meets up to the selected rows, group it bu airline and sorted by negative reasons","618bb830":"padding each vector to fit the dl model required format","bd91933d":"looping on the data columns to get the columns names and its count","841218e0":"## Create df with the important columns","7e398fff":"based on correlation: airline_sentiment,text and negativereason correlates more with the target","e38d896a":"remove duplicated rows to clean the data","bf7f218e":"converts Python text strings to streams of token objects, where each token object is a separate word, punctuation sign, number\/amount, date, e-mail, URL\/URI, etc.","b32d932f":"Naive Bayes classifier for multinomial models\n\nThe multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf works well.","e6cbbd1e":"### padding","e56c2e0e":"The batch size defines the number of samples that will be propagated through the network.","2937ed7d":"visualize the data columns names and counts","db0649c4":"## data correlation"}}