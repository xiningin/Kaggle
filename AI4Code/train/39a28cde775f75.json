{"cell_type":{"48a9100c":"code","aa6805db":"code","d7a689db":"code","204372f5":"code","e9c5af25":"code","1d5ca3fa":"code","eb2e8f55":"code","a8f3186b":"code","387eddb2":"code","0f9fee43":"code","85e1ea8e":"code","9327f8fc":"code","18e667b3":"code","04122e89":"code","2b850ae5":"code","af49b88b":"code","4c3b11c2":"code","d90cd3c8":"code","9784c6cb":"code","3a28dfdc":"markdown","e8526365":"markdown","13e660b9":"markdown","8b255132":"markdown","785de357":"markdown","4c07500e":"markdown","8e51a96f":"markdown","0e7638d8":"markdown","3eda2bf5":"markdown","86a60382":"markdown","5382bc3d":"markdown","49fed1d2":"markdown","f774c4d1":"markdown","50740b6c":"markdown"},"source":{"48a9100c":"from keras.datasets import mnist","aa6805db":"(train_images, train_labels), (test_images, test_labels) = mnist.load_data()","d7a689db":"import matplotlib.pyplot as plt\nplt.subplot(1,4,1)\nplt.imshow(train_images[0], cmap=plt.cm.binary)\n\nplt.subplot(1,4,2)\nplt.imshow(train_images[1], cmap=plt.cm.binary)\n\nplt.subplot(1,4,3)\nplt.imshow(train_images[2], cmap=plt.cm.binary)\n\nplt.subplot(1,4,4)\nplt.imshow(train_images[3], cmap=plt.cm.binary)\n\nplt.show()","204372f5":"train_images[1].shape","e9c5af25":"#Training data\nprint(\"Train Image Shape: \",train_images.shape)\nprint(\"Train dataset Length: \",len(train_labels))\nprint(\"Train Labels: \",train_labels)","1d5ca3fa":"#Testing data\nprint(\"ain TrImage Shape: \",test_images.shape)\nprint(\"Train dataset Length: \",len(test_labels))\nprint(\"Train Labels: \",test_labels)","eb2e8f55":"from keras import models\nfrom keras import layers","a8f3186b":"network = models.Sequential()\nnetwork.add( layers.Dense(512, activation='relu', input_shape=(28 * 28,) ) )\nnetwork.add( layers.Dense(10, activation='softmax') )","387eddb2":"network.compile( optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])","0f9fee43":"network.summary()","85e1ea8e":"train_images = train_images.reshape((60000, 28*28))\ntrain_images = train_images.astype('float32') \/ 255","9327f8fc":"test_images = test_images.reshape((10000, 28*28))\ntest_images = test_images.astype('float32') \/ 255","18e667b3":"train_labels[0]","04122e89":"from tensorflow.keras.utils import to_categorical","2b850ae5":"train_labels = to_categorical(train_labels)\ntest_labels = to_categorical(test_labels)","af49b88b":"train_labels[0]","4c3b11c2":"network.fit(train_images, train_labels, epochs=10, batch_size=128)","d90cd3c8":"test_loss, test_acc = network.evaluate(test_images, test_labels)","9784c6cb":"print(\"Test accuracy: \",test_acc)\nprint(\"Test loss: \", test_loss)","3a28dfdc":"**Layers**\n\n---\n\nThe core building block of the neural network is the ***layer***. It can be thought of as a data preprocessing module, which acts as a filter for the data.\n\n\nSome data goes in and it comes out as a more useful form. Specifically, layers extract ***representations*** out of the data fed into them. \nHopefully, representations that are more meaningful to the problem in hand. \n\n\nMost of deep learning consis of chaining together simple layers that will implement a a form of progressive ***data distillation***.\n\n\nA deep learning model is like a sieve for data preprocessing, made of succession of increasingly refined data filters - the layers\n\n\n\n\n**Network Architecture Layers**\n\n---\n\n**1.**   Here our network consist of two ***Dense layers***, which are densely connected (also called fully connected) neural layers.\n\n\n**2.**   The second (and last) layer is a **10-way softmax layer**, which means it will return an array of 10 probability scores (summing to 1). Each score will be the probability that the current digit image belongs to one of our 10 digit classes.\n\n***Compilation Step***\n\n---\n\nTo make the network ready for training we need to pick three more things as part of the compilation steps:\n\n\n**1.**   **A Loss Function** - How the network will be able to measure its performance on the training data, and thus how it willl be able to steer itself in the right direction.\n\n**2.**   **An optimizer** - The mechanism through whichthe ntwork wil update itself based on the data it sees and its loss function.\n\n**3.**   **Metrics to monitor during training and testing phase** - Here, we'll only care about accuracy (The fraction of the images that were correctly classified.)","e8526365":"The images are encoded as Numpy arrays, and the labels are and array of digits, ranging from 0 to 9.\n\nThe images and labels have a one-to-one correspondence.\n","13e660b9":"## **Steps**\n\n1) Importing the dataset\n\n2) Building the Deep Learning Architecture\n\n3) Complilation Step\n\n4) Preparing the image data (depondent Variable)\n\n5) Preparing the labels (target\/Independent variable)\n\n6) Training the Network\n\n7) Testing the Network","8b255132":"The **test accuracy** turned out to be **98.07%** - thats a bit lower than the training set accuracy.\n\nThis **gap between training and test accuracy** can be explained with the help of **overfitting**: The fact that machine learning models tend to  perform worse on new data than on their training data.\n\n","785de357":"### **5. Preparing the labels**\n\nWe now need to categorically encode the labels.\n","4c07500e":"Two quantities are displayed during training:\n\n\n\n1.   **The loss of the network over the training data**.\n\n2.   **The accuracy of the network over the training data**.\n\nwe quicklyreach an **accuracy of 98.85%** on the training data.\n","8e51a96f":"### **3. The Compilation Step**","0e7638d8":"## **1. Importing the Dataset**","3eda2bf5":"train_images and train_labels from the training set, the data the model will learn from.\n\nThe model will be tested on test_images and test_labels.\n","86a60382":"### **6. Training the network**\n\nnow we are ready to train the network, which is done by calling the **fit** method - where we fit the model to the training data.\n","5382bc3d":"# **Classify Handwritten Digits - MNIST Dataset**\n\nProblem we are trying to solve here is to classify a **grayscale images** of handwritten digits **(28 x 28 pixels)** into their **10 categories** (0 to 9).\n\nThe dataset consists of **60,000 training** images and **10,000 test** images.\nThe dataset was assembled by National Institiue of Standards and Technology (NIST) in 1980s.","49fed1d2":"**Workflow**\n\n1.   Feed The Neural network with training data - train_images, train_labels.\n2.   Ask the neural network to produce predictions for test_images.\n3.   Verify the predictions to match the labels from test_labels.\n\n\n\n### **2. The Network Architecture**","f774c4d1":"### **4. Preparing the image data**\n\nBefore training we will preprocess the data by reshaping it into theshape the network expects and scaling it so that all values are in the **[0,1] interval**.\n\nPreviously, our training images, for instance were stored in the array of shape **(6000, 28,28)** of **type uint8** with values int he range **[0,288]** interval. \n\nWe transform it into a **float32 array** of shape **(6000,28*28)** with values between **0 and 1.**","50740b6c":"### **7. Testing the network**\n\nNow let us check if the model performs well on the **test set**:The **test accuracy** turned out to be **98.07%** - thats a bit lower than the training set accuracy.\n\nThis **gap between training and test accuracy** can be explained with the help of **overfitting**: The fact that machine learning models tend to  perform worse on new data than on their training data.\n\n"}}