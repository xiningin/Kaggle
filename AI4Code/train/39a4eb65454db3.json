{"cell_type":{"d0c65fd3":"code","2b196087":"code","0759a0ab":"code","676a5f7f":"code","f3fa510e":"code","9109ca91":"code","9e969c17":"code","f4c2977f":"code","40ffb97c":"code","5db30aee":"code","403a8612":"code","3fe1f1e6":"code","7c130251":"code","aaef8cc8":"code","7b8122d7":"code","fc622aff":"code","27f2fbf0":"code","93cdb95c":"code","5019009c":"code","b8de5686":"code","2a79df16":"code","26a3effa":"code","eb33d7c8":"code","1dd50bd7":"code","637c1fdb":"markdown","28aa8f52":"markdown","6f0bfeba":"markdown","b9a6204c":"markdown","6a8201e8":"markdown","86e79989":"markdown","b9aeb6d7":"markdown","9a8f23d9":"markdown","bdb166ec":"markdown","f43ce5e2":"markdown","e7c1c99b":"markdown","d4eb748d":"markdown","8bb74b2a":"markdown","e080325d":"markdown","23d47004":"markdown","bf2092b6":"markdown","695a80fe":"markdown","15eb3762":"markdown","91971d9d":"markdown","cd30166c":"markdown","7835b8af":"markdown","cc0781b6":"markdown","82ff9e08":"markdown","87800439":"markdown","ccdadbdc":"markdown","cafd14dd":"markdown"},"source":{"d0c65fd3":"import numpy as np\nimport os\nimport sys\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nfrom PIL import Image\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\n\nimport keras\nfrom keras.preprocessing.image import load_img\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input, Dense, Dropout, Activation, BatchNormalization\nfrom keras.layers import Reshape, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.utils import to_categorical\nfrom keras.utils import np_utils\nfrom keras.regularizers import l1_l2\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\n%matplotlib inline\n%env JOBLIB_TEMP_FOLDER=\/tmp","2b196087":"dataset = 'notMNIST_small'\nDATA_PATH = '..\/input\/' + dataset + '\/' + dataset","0759a0ab":"max_images = 100\ngrid_width = 10\ngrid_height = int(max_images \/ grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nclasses = os.listdir(DATA_PATH)\nfor j, cls in enumerate(classes):\n    figs = os.listdir(DATA_PATH + '\/' + cls)\n    for i, fig in enumerate(figs[:grid_width]):\n        ax = axs[j, i]\n        ax.imshow(np.array(load_img(DATA_PATH + '\/' + cls + '\/' + fig)))\n        ax.set_yticklabels([])\n        ax.set_xticklabels([])","676a5f7f":"X = []\nlabels = []\n# for each folder (holding a different set of letters)\nfor directory in os.listdir(DATA_PATH):\n    # for each image\n    for image in os.listdir(DATA_PATH + '\/' + directory):\n        # open image and load array data\n        try:\n            file_path = DATA_PATH + '\/' + directory + '\/' + image\n            img = Image.open(file_path)\n            img.load()\n            img_data = np.asarray(img, dtype=np.int16)\n            # add image to dataset\n            X.append(img_data)\n            # add label to labels\n            labels.append(directory)\n        except:\n            None # do nothing if couldn't load file\nN = len(X) # number of images\nimg_size = len(X[0]) # width of image\nX = np.asarray(X).reshape(N, img_size, img_size,1) # add our single channel for processing purposes\nlabels_cat = to_categorical(list(map(lambda x: ord(x)-ord('A'), labels)), 10) # convert to one-hot\nlabels = np.asarray(list(map(lambda x: ord(x)-ord('A'), labels)))","f3fa510e":"cls_s = np.sum(labels,axis=0)\n\nfig, ax = plt.subplots()\nplt.bar(np.arange(10), cls_s)\nplt.ylabel('No of pics')\nplt.xticks(np.arange(10), np.sort(classes))\nplt.title('Checking balance for data set..')\nplt.show()","9109ca91":"from sklearn.cross_validation import train_test_split\nX_train,X_valid,y_train,y_valid=train_test_split(X,labels,test_size=0.2)\nX_train_cat,X_valid_cat,y_train_cat,y_valid_cat=train_test_split(X,labels_cat,test_size=0.2)\n\nprint('Training:', X_train.shape, y_train.shape)\nprint('Validation:', X_valid.shape, y_valid.shape)","9e969c17":"# def randomize(dataset, labels):\n#   permutation = np.random.permutation(labels.shape[0])\n#   shuffled_dataset = dataset[permutation,:,:]\n#   shuffled_labels = labels[permutation]\n#   return shuffled_dataset, shuffled_labels\n#   \n# X_train, y_train = randomize(X_train, y_train)\n# X_test, y_test = randomize(X_test, y_test)\n# X_train_cat, y_train_cat = randomize(X_train_cat, y_train_cat)\n# X_test_cat, y_test_cat = randomize(X_test_cat, y_test_cat)","f4c2977f":"fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor j in range(max_images):\n    ax = axs[int(j\/grid_width), j%grid_width]\n    ax.imshow(X_train[j,:,:,0])\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])","40ffb97c":"samples, width, height = np.squeeze(X_train).shape\nX_train_lr = np.reshape(X_train,(samples,width*height))\ny_train_lr = y_train\n\nsamples, width, height = np.squeeze(X_valid).shape\nX_valid_lr = np.reshape(X_valid,(samples,width*height))\ny_valid_lr = y_valid","5db30aee":"lr = LogisticRegression(multi_class='ovr', solver='saga', random_state=42, \n                        verbose=0, max_iter=5000, n_jobs=-1)\nlr.fit(X_train_lr, y_train_lr)\nlr.score(X_train_lr, y_train_lr)\ny_pred_lr = lr.predict(X_valid_lr)","403a8612":"from sklearn import metrics\nmetrics.accuracy_score(y_valid_lr, y_pred_lr)","3fe1f1e6":"rf = RandomForestClassifier(n_estimators=1000, criterion=\"gini\", max_depth=10, \n                            min_samples_split=2, min_samples_leaf=1, \n                            n_jobs=-1, random_state=42, verbose=0)\nrf.fit(X_train_lr, y_train_lr)\nrf.score(X_train_lr, y_train_lr)\ny_pred_rf = rf.predict(X_valid_lr)","7c130251":"metrics.accuracy_score(y_valid_lr, y_pred_rf)","aaef8cc8":"xg_train = xgb.DMatrix(X_train_lr, label=y_train_lr)\nxg_valid = xgb.DMatrix(X_valid_lr, label=y_valid_lr)\n# setup parameters for xgboost\nparam = {}\n# use softmax multi-class classification\nparam['objective'] = 'multi:softmax' # 'multi:softprob'\n# scale weight of positive examples\nparam['eta'] = 0.1\nparam['max_depth'] = 10\nparam['silent'] = 0\nparam['nthread'] = 4\nparam['num_class'] = 10\n\nwatchlist = [(xg_train, 'train'), (xg_valid, 'test')]\nnum_round = 20\nxg = xgb.train(param, xg_train, num_round, \n               watchlist, early_stopping_rounds=50, \n               maximize=True)\n\ny_pred_xg = xg.predict(xg_valid)","7b8122d7":"metrics.accuracy_score(y_valid_lr, y_pred_xg)","fc622aff":"def build_logistic_model(resolution, output_dim):\n    model = Sequential()\n    model.add(Flatten(input_shape=(resolution, resolution, 1)))\n    model.add(Dense(output_dim, activation='softmax'))\n    return model\n\ndef plot_training_curves(history):\n    \"\"\"\n    Plot accuracy and loss curves for training and validation sets.\n    Args:\n        history: a Keras History.history dictionary\n    Returns:\n        mpl figure.\n    \"\"\"\n    fig, (ax_acc, ax_loss) = plt.subplots(1, 2, figsize=(8,2))\n    if 'acc' in history:\n        ax_acc.plot(history['acc'], label='acc')\n        if 'val_acc' in history:\n            ax_acc.plot(history['val_acc'], label='Val acc')\n        ax_acc.set_xlabel('epoch')\n        ax_acc.set_ylabel('accuracy')\n        ax_acc.legend(loc='upper left')\n        ax_acc.set_title('Accuracy')\n\n    ax_loss.plot(history['loss'], label='loss')\n    if 'val_loss' in history:\n        ax_loss.plot(history['val_loss'], label='Val loss')\n    ax_loss.set_xlabel('epoch')\n    ax_loss.set_ylabel('loss')\n    ax_loss.legend(loc='upper right')\n    ax_loss.set_title('Loss')\n\n    sns.despine(fig)\n    return\n\nbatch_size = 128\nnb_classes = 10\nnb_epoch = 300\ninput_dim = 784\nresolution = 28\n\nmodel_ada = build_logistic_model(resolution, nb_classes)\n\nmodel_ada.summary()\n\n# compile the model\n# first with gradient descent optimizer\nmodel_ada.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model_ada.fit(X_train_cat, y_train_cat,\n                    batch_size=batch_size, epochs=nb_epoch,\n                    verbose=0, validation_data=(X_valid_cat, y_valid_cat))\nscore = model_ada.evaluate(X_valid_cat, y_valid_cat, verbose=0)\n\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n\nplot_training_curves(history.history)","27f2fbf0":"def build_logistic_model_reg(resolution, output_dim, reg):\n    model = Sequential()\n    model.add(Flatten(input_shape=(resolution, resolution, 1)))\n    model.add(Dense(output_dim, kernel_regularizer=reg, activation='softmax'))\n    return model\n\nreg = l1_l2(l1=0, l2=0.02)\nmodel_ada_reg = build_logistic_model_reg(resolution, nb_classes, reg)\n\nmodel_ada_reg.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model_ada_reg.fit(X_train_cat, y_train_cat,\n                    batch_size=batch_size, epochs=nb_epoch,\n                    verbose=0, validation_data=(X_valid_cat, y_valid_cat))\nscore = model_ada_reg.evaluate(X_valid_cat, y_valid_cat, verbose=0)\n\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n\nplot_training_curves(history.history)","93cdb95c":"# then with stochastic gradient descent optimizer\nmodel_sgd = build_logistic_model(resolution, nb_classes)\n\nmodel_sgd.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model_sgd.fit(X_train_cat, y_train_cat,\n                    batch_size=batch_size, epochs=nb_epoch,\n                    verbose=0, validation_data=(X_valid_cat, y_valid_cat))\nscore = model_sgd.evaluate(X_valid_cat, y_valid_cat, verbose=0)\n\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n\nplot_training_curves(history.history)","5019009c":"def build_logisticHidden_model(resolution, output_dim, hidden_dim):\n    model = Sequential()\n    model.add(Flatten(input_shape=(resolution, resolution, 1)))\n#     kernel_initializer='glorot_normal'\n    model.add(Dense(hidden_dim[0], activation='relu'))\n    model.add(Dense(hidden_dim[1], activation='relu'))\n    model.add(Dense(hidden_dim[2], activation='relu'))\n    model.add(Dense(output_dim, activation='softmax'))\n    return model\n\nhidden_dim = [1024, 300, 50]\n# sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nmodel_hid = build_logisticHidden_model(resolution, nb_classes, hidden_dim)\n\nmodel_hid.summary()\n\n# compile the model\nmodel_hid.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model_hid.fit(X_train_cat, y_train_cat,\n                    batch_size=batch_size, epochs=nb_epoch,\n                    verbose=0, validation_data=(X_valid_cat, y_valid_cat))\nscore = model_hid.evaluate(X_valid_cat, y_valid_cat, verbose=0)\n\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n\nplot_training_curves(history.history)","b8de5686":"def build_logisticHidden_model_reg(resolution, output_dim, hidden_dim, reg):\n    model = Sequential()\n    model.add(Flatten(input_shape=(resolution, resolution, 1)))\n    model.add(Dense(hidden_dim[0], kernel_regularizer=reg, activation='relu'))\n    model.add(Dense(hidden_dim[1], kernel_regularizer=reg, activation='relu'))\n    model.add(Dense(hidden_dim[2], kernel_regularizer=reg, activation='relu'))\n    model.add(Dense(output_dim, kernel_regularizer=reg, activation='softmax'))\n    return model\n\nmodel_hid_reg = build_logisticHidden_model_reg(resolution, nb_classes, hidden_dim, reg)\n\nmodel_hid_reg.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model_hid_reg.fit(X_train_cat, y_train_cat,\n                    batch_size=batch_size, epochs=nb_epoch,\n                    verbose=0, validation_data=(X_valid_cat, y_valid_cat))\nscore = model_hid_reg.evaluate(X_valid_cat, y_valid_cat, verbose=0)\n\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n\nplot_training_curves(history.history)","2a79df16":"def build_logisticHidden_model_dropOut(resolution, output_dim, hidden_dim):\n    model = Sequential()\n    model.add(Flatten(input_shape=(resolution, resolution, 1)))\n    model.add(Dense(hidden_dim[0], activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(hidden_dim[1], activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(hidden_dim[2], activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(output_dim, activation='softmax'))\n    return model\n\nmodel_hid_drop = build_logisticHidden_model_dropOut(resolution, nb_classes, hidden_dim)\n\nmodel_hid_drop.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model_hid_drop.fit(X_train_cat, y_train_cat,\n                    batch_size=batch_size, epochs=nb_epoch,\n                    verbose=0, validation_data=(X_valid_cat, y_valid_cat))\nscore = model_hid_drop.evaluate(X_valid_cat, y_valid_cat, verbose=0)\n\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n\nplot_training_curves(history.history)","26a3effa":"def build_logisticHidden_model_regDrop(resolution, output_dim, hidden_dim):\n    model = Sequential()\n    model.add(Flatten(input_shape=(resolution, resolution, 1)))\n    model.add(Dense(hidden_dim[0], kernel_regularizer=reg, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(hidden_dim[1], kernel_regularizer=reg, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(hidden_dim[2],kernel_regularizer=reg,  activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(output_dim, kernel_regularizer=reg, activation='softmax'))\n    return model\n\nmodel_hid_regDrop = build_logisticHidden_model_regDrop(resolution, nb_classes, hidden_dim)\n\nmodel_hid_regDrop.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model_hid_regDrop.fit(X_train_cat, y_train_cat,\n                    batch_size=batch_size, epochs=nb_epoch,\n                    verbose=0, validation_data=(X_valid_cat, y_valid_cat))\nscore = model_hid_regDrop.evaluate(X_valid_cat, y_valid_cat, verbose=0)\n\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n\nplot_training_curves(history.history)","eb33d7c8":"# input image dimensions\nimg_rows, img_cols = 28, 28\n# number of convolutional filters to use\nnb_filters = 64\n# size of pooling area for max pooling\npool_size = [2, 2]\n# convolution kernel size\nkernel_size = [3, 3]\n\ninput_shape = [img_rows, img_cols, 1]\n\n# model layers\ncnn = Sequential()\ncnn.add(Conv2D(nb_filters, kernel_size, padding='same', input_shape=input_shape))\ncnn.add(Activation('relu'))\ncnn.add(BatchNormalization())\ncnn.add(Conv2D(nb_filters, kernel_size, padding='same'))\ncnn.add(Activation('relu'))\ncnn.add(BatchNormalization())\ncnn.add(MaxPooling2D(pool_size=pool_size))\ncnn.add(Dropout(0.25))\ncnn.add(Flatten())\ncnn.add(Dense(128))\ncnn.add(Activation('relu'))\ncnn.add(BatchNormalization())\ncnn.add(Dropout(0.5))\ncnn.add(Dense(nb_classes))\ncnn.add(Activation('softmax'))\n\ncnn.summary()\n\ncnn.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n\nhistory = cnn.fit(X_train_cat, y_train_cat,\n                 batch_size=batch_size, epochs=nb_epoch,\n                 verbose=0, validation_data=(X_valid_cat, y_valid_cat))\nscore = cnn.evaluate(X_valid_cat, y_valid_cat, verbose=0)\n\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n\nplot_training_curves(history.history)","1dd50bd7":"# define path to save model\nmodel_path = '.\/cnn_notMNIST.h5'\n# prepare callbacks\ncallbacks = [\n    EarlyStopping(\n        monitor='val_acc', \n        patience=20,\n        mode='max',\n        verbose=1),\n    ModelCheckpoint(model_path,\n        monitor='val_acc', \n        save_best_only=True, \n        mode='max',\n        verbose=1),\n    ReduceLROnPlateau(\n        factor=0.1, \n        patience=5, \n        min_lr=0.00001, \n        verbose=1)\n]\n\n# model layers\ncnn_ad = Sequential()\ncnn_ad.add(Conv2D(nb_filters, kernel_size, padding='same', input_shape=input_shape))\ncnn_ad.add(Activation('relu'))\ncnn_ad.add(BatchNormalization())\ncnn_ad.add(Conv2D(nb_filters, kernel_size, padding='same'))\ncnn_ad.add(Activation('relu'))\ncnn_ad.add(BatchNormalization())\ncnn_ad.add(MaxPooling2D(pool_size=pool_size))\ncnn_ad.add(Dropout(0.25))\ncnn_ad.add(Flatten())\ncnn_ad.add(Dense(128))\ncnn_ad.add(Activation('relu'))\ncnn_ad.add(BatchNormalization())\ncnn_ad.add(Dropout(0.5))\ncnn_ad.add(Dense(nb_classes))\ncnn_ad.add(Activation('softmax'))\n\ncnn_ad.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n\nhistory = cnn_ad.fit(X_train_cat, y_train_cat,\n                 batch_size=batch_size, epochs=nb_epoch,\n                 verbose=0, validation_data=(X_valid_cat, y_valid_cat),\n                 shuffle=True, callbacks=callbacks)\nscore = cnn_ad.evaluate(X_valid_cat, y_valid_cat, verbose=0)\n\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n\nplot_training_curves(history.history)","637c1fdb":"Load images and make them ready for fitting a model.","28aa8f52":"Let's shuffle the data for a better conditioning of the sets. (useless?)","6f0bfeba":"Select which data to use.","b9a6204c":"Prepare the data for the Logistic Regression model.","6a8201e8":"Let's test different models.\n\nFirst, a Random Forest one.","86e79989":"So, with this notebook I've explored different architectures and strategies for classifying the notMNIST dataset, starting from the basics, till state-of-art architectures.\nThe best result is obtained by the CNN (accuracy ~95%) with further possible improvements related to hyper parameters tuning.\nIf you've any comment, question or advice, please do not hesitate to type it down in the comments.","b9aeb6d7":"And then adding some more advance features to make our training process smarter.","9a8f23d9":"Check balance of classes.","bdb166ec":"With regularization.","f43ce5e2":"Sanity check of the final dataset.","e7c1c99b":"Check some data from the training dataset","d4eb748d":"Let's try also SGD.","8bb74b2a":"With regularization and dropout.","e080325d":"Check accuracy of Random Forest model.","23d47004":"With dropout.","bf2092b6":"Divide data into train\/test datasets.","695a80fe":"With regularization.","15eb3762":"Let's switch to a more complicated architecture: CNN.","91971d9d":"The first task is to reproduce the Logistic Regressor.\n\nFirst attempt with \"Adam\" optimizer.","cd30166c":"Check accuracy of Logistic Regression model.","7835b8af":"Setup, train and predict with the Logistic Regression model.","cc0781b6":"Let's start using TensorFlow, specifically I'll use Keras as its wrapper.","82ff9e08":"What if introducing a hidden layer with ReLu activation?","87800439":"This notebook uses different techniques to create models for classification of the images of letters in the dataset.\n\nFirstly we import the libraries we'll need.","ccdadbdc":"Check its accuracy.","cafd14dd":"What about XGBoost?"}}