{"cell_type":{"70120d7d":"code","8ef1c72c":"code","636f89b9":"code","80f116c5":"code","e6f77071":"code","c474e5c7":"code","fb003373":"code","c475e25e":"code","e91e5e6c":"code","b793c0a5":"code","24b6e303":"code","504c6b6a":"code","3221a65b":"code","d6489c2a":"code","f6f19dc0":"code","d254a68d":"code","8d3eb431":"code","274e0c0a":"code","ba6fca92":"code","5271e22d":"code","d22fe085":"code","87d15a64":"code","3a7c3f9a":"code","235dc56d":"code","63a93bfc":"code","7ae0e895":"code","1e97226e":"code","19b1ff05":"code","30d6b880":"code","f787d06a":"code","ad2bc7ee":"code","c075b0b7":"code","57cf9547":"code","cf70d243":"code","87033cba":"code","4670470e":"code","bb4b9e42":"code","22b32c4a":"code","abfd6f07":"code","9bed6763":"code","278992f0":"code","0090cda2":"markdown","6763737d":"markdown","699529a5":"markdown","3d52fc4c":"markdown","6f1cb124":"markdown","49369f62":"markdown","caec2ff9":"markdown","2c275e48":"markdown","1e064dd6":"markdown"},"source":{"70120d7d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8ef1c72c":"df=pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')","636f89b9":"df.head()","80f116c5":"df.isnull().any()","e6f77071":"df.shape","c474e5c7":"df.describe()","fb003373":"df.info()","c475e25e":"cor=df.corr()","e91e5e6c":"cor","b793c0a5":"import seaborn as sns","24b6e303":"ax=sns.heatmap(cor)","504c6b6a":"t=abs(cor['target'])","3221a65b":"best_features=t[t>0.3]","d6489c2a":"best_features","f6f19dc0":"feature_cols=['cp','thalach','exang','oldpeak','slope','ca','thal']","d254a68d":"x=df[feature_cols]\ny=df.target","8d3eb431":"import matplotlib.pyplot as plt\n%matplotlib inline","274e0c0a":"sns.countplot(x='sex',data=df)","ba6fca92":"sns.scatterplot(x='age',y='target',data=df)\n","5271e22d":"sns.barplot(x='target',y='age',data=df)\n#visualizing the chance of disease with age","d22fe085":"sns.barplot(x='target',y='cp',data=df,color='r')\n#visualizing the chance of disease with cp","87d15a64":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,random_state=42,test_size=0.2)","3a7c3f9a":"from sklearn import svm","235dc56d":"sv=svm.SVC(gamma=0.01,C=1,kernel='linear')\nsv.fit(x_train,y_train)\npred=sv.predict(x_test)","63a93bfc":"from sklearn.metrics import classification_report","7ae0e895":"print(classification_report(y_test,pred))","1e97226e":"from sklearn.linear_model import LogisticRegression\nlr= LogisticRegression()","19b1ff05":"lr.fit(x_train,y_train)","30d6b880":"pre=lr.predict(x_test)","f787d06a":"print(classification_report(y_test,pre))","ad2bc7ee":"import xgboost","c075b0b7":"xbg=xgboost.XGBClassifier()","57cf9547":"xbg.fit(x_train,y_train)\npr=xbg.predict(x_test)","cf70d243":"print(classification_report(y_test,pr))","87033cba":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","4670470e":"tree=DecisionTreeClassifier()","bb4b9e42":"tree.fit(x_train,y_train)\np=tree.predict(x_test)","22b32c4a":"print(classification_report(y_test,p))","abfd6f07":"forest=RandomForestClassifier()","9bed6763":"forest.fit(x_train,y_train)\npp=forest.predict(x_test)","278992f0":"print(classification_report(y_test,pp))","0090cda2":"# building the model","6763737d":"# visualization","699529a5":"# from the above classification models we can conclude that 'support vector machine' gives us the highest accuracy","3d52fc4c":"# xgboost algorithm","6f1cb124":"# support vector machine","49369f62":"# data cleaning","caec2ff9":"# feature selection based on correlation matrix","2c275e48":"# decision tree and random forest","1e064dd6":"# logistic regression"}}