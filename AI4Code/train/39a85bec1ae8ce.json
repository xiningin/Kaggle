{"cell_type":{"dfd602e5":"code","759b74a5":"code","1b6348a9":"code","374acf79":"code","654ee927":"code","bee128f9":"code","712a643a":"code","7a95f2d2":"code","83fa75a0":"code","0238a317":"code","fec14b83":"code","f6ddf1c1":"code","ee14551a":"code","1636031b":"code","61f51a21":"code","9177d2db":"code","142b2859":"code","3b514b12":"code","0cf2f5bb":"code","8d875458":"code","1b26f1d7":"code","f3b2ccc8":"code","6bc3cd22":"code","91130c4a":"code","f662b674":"code","c5b9a8f1":"code","658eea44":"code","82ea9ec4":"code","69a9739f":"code","76badbd1":"code","4ef47381":"code","3d3dbe9b":"code","bd00fcb9":"code","17791bb6":"code","8a5b178a":"code","23e46ca2":"markdown","c8e4590d":"markdown","1e7c005a":"markdown","2853e371":"markdown","adcda21e":"markdown","481f9c83":"markdown","04110265":"markdown","a9e8dabe":"markdown","d74cb508":"markdown","ae19b749":"markdown","c73cf4f4":"markdown","004fb462":"markdown","1687c8cf":"markdown","a6d1b130":"markdown","2a776d33":"markdown","96658dcb":"markdown","a1ba69d1":"markdown"},"source":{"dfd602e5":"!pip install --upgrade seaborn","759b74a5":"import numpy as np, pandas as pd\nfrom glob import glob\nimport shutil, os\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GroupKFold\nfrom tqdm.notebook import tqdm\nimport seaborn as sns","1b6348a9":"dim = 512 #512, 256, 'original'\nfold = 4","374acf79":"train_df = pd.read_csv(f'..\/input\/vinbigdata-{dim}-image-dataset\/vinbigdata\/train.csv')\ntrain_df.head()","654ee927":"train_df['image_path'] = f'\/kaggle\/input\/vinbigdata-{dim}-image-dataset\/vinbigdata\/train\/'+train_df.image_id+('.png' if dim!='original' else '.jpg')\ntrain_df.head()","bee128f9":"train_df = train_df[train_df.class_id!=14].reset_index(drop = True)","712a643a":"!pip install ensemble_boxes","7a95f2d2":"gkf  = GroupKFold(n_splits = 5)\ntrain_df['fold'] = -1\nfor fold, (train_idx, val_idx) in enumerate(gkf.split(train_df, groups = train_df.image_id.tolist())):\n    train_df.loc[val_idx, 'fold'] = fold\ntrain_df.head()","83fa75a0":"train_files = []\nval_files   = []\nval_files += list(train_df[train_df.fold==fold].image_path.unique())\ntrain_files += list(train_df[train_df.fold!=fold].image_path.unique())\nlen(train_files), len(val_files)","0238a317":"# os.makedirs('\/kaggle\/working\/vinbigdata\/labels\/train', exist_ok = True)\n# os.makedirs('\/kaggle\/working\/vinbigdata\/labels\/val', exist_ok = True)\n# os.makedirs('\/kaggle\/working\/vinbigdata\/images\/train', exist_ok = True)\n# os.makedirs('\/kaggle\/working\/vinbigdata\/images\/val', exist_ok = True)\n# label_dir = '\/kaggle\/input\/vinbigdata-yolo-labels-dataset\/labels'","fec14b83":"import os.path as osp\nfrom path import Path\nfrom collections import Counter\nimport cv2\nfrom ensemble_boxes import *","f6ddf1c1":"imagepaths = train_df['image_path'].unique()\ntrain_annotations=train_df","ee14551a":"# img_array  = cv2.imread('\/kaggle\/input\/vinbigdata-512-image-dataset\/vinbigdata\/train\/d3637a1935a905b3c326af31389cb846.png')","1636031b":"def Create_nms_box_txt(desktop_path,name):\n    iou_thr = 0.5\n    skip_box_thr = 0.0001\n    viz_images = []\n#     image_basename = Path(path).stem\n    image_basename = name\n#     print(f\"(\\'{image_basename}\\', \\'{path}\\')\")\n    img_annotations = train_annotations[train_annotations.image_id==image_basename]\n\n    boxes_viz = img_annotations[['x_min', 'y_min', 'x_max', 'y_max']].to_numpy().tolist()\n    labels_viz = img_annotations['class_id'].to_numpy().tolist()\n\n    print(\"Bboxes before nms:\\n\", boxes_viz)\n    print(\"Labels before nms:\\n\", labels_viz)\n\n    boxes_list = []\n    scores_list = []\n    labels_list = []\n    weights = []\n\n    boxes_single = []\n    labels_single = []\n\n    cls_ids = img_annotations['class_id'].unique().tolist()\n    count_dict = Counter(img_annotations['class_id'].tolist())\n    print(count_dict)\n\n    for cid in cls_ids:       \n        ## Performing Fusing operation only for multiple bboxes with the same label\n        if count_dict[cid]==1:\n            labels_single.append(cid)\n            boxes_single.append(img_annotations[img_annotations.class_id==cid][['x_min', 'y_min', 'x_max', 'y_max']].to_numpy().squeeze().tolist())\n\n        else:\n            cls_list =img_annotations[img_annotations.class_id==cid]['class_id'].tolist()\n            labels_list.append(cls_list)\n            bbox = img_annotations[img_annotations.class_id==cid][['x_min', 'y_min', 'x_max', 'y_max']].to_numpy()\n\n            ## Normalizing Bbox by Image Width and Height\n            bbox = bbox\/(img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"], img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"])\n\n\n            bbox = np.clip(bbox, 0, 1)\n            boxes_list.append(bbox.tolist())\n\n            scores_list.append(np.ones(len(cls_list)).tolist())\n\n            weights.append(1)\n\n            \n    # Perform NMS\n    if len(boxes_list)==0:\n        boxes=boxes_single\n        box_labels=labels_single\n        print(\"Bboxes after nms:\\n\", boxes)\n        print(\"Labels after nms:\\n\", box_labels)\n        \n        count_dict = Counter(box_labels)\n        print(count_dict)\n\n        text_create(desktop_path,image_basename,boxes,box_labels,img_annotations.iloc[0][\"width\"],img_annotations.iloc[0][\"height\"])\n        \n    else:\n        boxes, scores, box_labels = nms(boxes_list, scores_list, labels_list, weights=weights,\n                                    iou_thr=iou_thr)\n\n\n        #img_array.shape[1]\u662f\u5bbd\u5ea6\n        boxes = boxes*(img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"], img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"])\n        boxes = boxes.round(1).tolist()\n        box_labels = box_labels.astype(int).tolist()\n\n        boxes.extend(boxes_single)\n        box_labels.extend(labels_single)\n\n        print(\"Bboxes after nms:\\n\", boxes)\n        print(\"Labels after nms:\\n\", box_labels)\n\n        count_dict = Counter(box_labels)\n        print(count_dict)\n\n        text_create(desktop_path,image_basename,boxes,box_labels,img_annotations.iloc[0][\"width\"],img_annotations.iloc[0][\"height\"])","61f51a21":"# iou_thr = 0.5\n# skip_box_thr = 0.0001\n# viz_images = []\n# for i, path in tqdm(enumerate(imagepaths[5:6])):\n#     image_basename = Path(path).stem\n#     print(f\"(\\'{image_basename}\\', \\'{path}\\')\")\n#     img_annotations = train_annotations[train_annotations.image_id==image_basename]\n\n#     boxes_viz = img_annotations[['x_min', 'y_min', 'x_max', 'y_max']].to_numpy().tolist()\n#     labels_viz = img_annotations['class_id'].to_numpy().tolist()\n\n#     print(\"Bboxes before nms:\\n\", boxes_viz)\n#     print(\"Labels before nms:\\n\", labels_viz)\n\n#     boxes_list = []\n#     scores_list = []\n#     labels_list = []\n#     weights = []\n\n#     boxes_single = []\n#     labels_single = []\n\n#     cls_ids = img_annotations['class_id'].unique().tolist()\n#     count_dict = Counter(img_annotations['class_id'].tolist())\n#     print(count_dict)\n\n#     for cid in cls_ids:       \n#         ## Performing Fusing operation only for multiple bboxes with the same label\n#         if count_dict[cid]==1:\n#             labels_single.append(cid)\n#             boxes_single.append(img_annotations[img_annotations.class_id==cid][['x_min', 'y_min', 'x_max', 'y_max']].to_numpy().squeeze().tolist())\n\n#         else:\n#             cls_list =img_annotations[img_annotations.class_id==cid]['class_id'].tolist()\n#             labels_list.append(cls_list)\n#             bbox = img_annotations[img_annotations.class_id==cid][['x_min', 'y_min', 'x_max', 'y_max']].to_numpy()\n\n#             ## Normalizing Bbox by Image Width and Height\n#             bbox = bbox\/(img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"], img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"])\n\n\n#             bbox = np.clip(bbox, 0, 1)\n#             boxes_list.append(bbox.tolist())\n\n#             scores_list.append(np.ones(len(cls_list)).tolist())\n\n#             weights.append(1)\n\n\n#     # Perform NMS\n#     boxes, scores, box_labels = nms(boxes_list, scores_list, labels_list, weights=weights,\n#                                     iou_thr=iou_thr)\n    \n#     print(\"Bboxes without multipy:\\n\", boxes)\n\n#     #img_array.shape[1]\u662f\u5bbd\u5ea6\n#     boxes = boxes*(img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"], img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"])\n#     boxes = boxes.round(1).tolist()\n#     box_labels = box_labels.astype(int).tolist()\n\n#     boxes.extend(boxes_single)\n#     box_labels.extend(labels_single)\n\n#     print(\"Bboxes after nms:\\n\", boxes)\n#     print(\"Labels after nms:\\n\", box_labels)\n\n#     count_dict = Counter(box_labels)\n#     print(count_dict)\n\n#     text_create('.\/',image_basename,boxes,box_labels,img_annotations.iloc[0][\"width\"],img_annotations.iloc[0][\"height\"])","9177d2db":"def Create_softnms_box_txt(desktop_path,name):\n    iou_thr = 0.5\n    skip_box_thr = 0.0001\n    viz_images = []\n    sigma = 0.1\n#     image_basename = Path(path).stem\n    image_basename = name\n#     print(f\"(\\'{image_basename}\\', \\'{path}\\')\")\n    img_annotations = train_annotations[train_annotations.image_id==image_basename]\n\n    boxes_viz = img_annotations[['x_min', 'y_min', 'x_max', 'y_max']].to_numpy().tolist()\n    labels_viz = img_annotations['class_id'].to_numpy().tolist()\n\n    print(\"Bboxes before nms:\\n\", boxes_viz)\n    print(\"Labels before nms:\\n\", labels_viz)\n\n    boxes_list = []\n    scores_list = []\n    labels_list = []\n    weights = []\n\n    boxes_single = []\n    labels_single = []\n\n    cls_ids = img_annotations['class_id'].unique().tolist()\n    count_dict = Counter(img_annotations['class_id'].tolist())\n    print(count_dict)\n\n    for cid in cls_ids:       \n        ## Performing Fusing operation only for multiple bboxes with the same label\n        if count_dict[cid]==1:\n            labels_single.append(cid)\n            boxes_single.append(img_annotations[img_annotations.class_id==cid][['x_min', 'y_min', 'x_max', 'y_max']].to_numpy().squeeze().tolist())\n\n        else:\n            cls_list =img_annotations[img_annotations.class_id==cid]['class_id'].tolist()\n            labels_list.append(cls_list)\n            bbox = img_annotations[img_annotations.class_id==cid][['x_min', 'y_min', 'x_max', 'y_max']].to_numpy()\n\n            ## Normalizing Bbox by Image Width and Height\n            bbox = bbox\/(img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"], img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"])\n\n\n            bbox = np.clip(bbox, 0, 1)\n            boxes_list.append(bbox.tolist())\n\n            scores_list.append(np.ones(len(cls_list)).tolist())\n\n            weights.append(1)\n\n            \n    # Perform NMS\n    if len(boxes_list)==0:\n        boxes=boxes_single\n        box_labels=labels_single\n        print(\"Bboxes after nms:\\n\", boxes)\n        print(\"Labels after nms:\\n\", box_labels)\n        \n        count_dict = Counter(box_labels)\n        print(count_dict)\n\n        text_create(desktop_path,image_basename,boxes,box_labels,img_annotations.iloc[0][\"width\"],img_annotations.iloc[0][\"height\"])\n        \n    else:\n        boxes, scores, box_labels = soft_nms(boxes_list, scores_list, labels_list, weights=weights,\n                                    iou_thr=iou_thr)\n\n\n        #img_array.shape[1]\u662f\u5bbd\u5ea6\n        boxes = boxes*(img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"], img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"])\n        boxes = boxes.round(1).tolist()\n        box_labels = box_labels.astype(int).tolist()\n\n        boxes.extend(boxes_single)\n        box_labels.extend(labels_single)\n\n        print(\"Bboxes after nms:\\n\", boxes)\n        print(\"Labels after nms:\\n\", box_labels)\n\n        count_dict = Counter(box_labels)\n        print(count_dict)\n\n        text_create(desktop_path,image_basename,boxes,box_labels,img_annotations.iloc[0][\"width\"],img_annotations.iloc[0][\"height\"])","142b2859":"def Create_non_maximum_weighted_box_txt(desktop_path,name):\n    iou_thr = 0.5\n    skip_box_thr = 0.0001\n    viz_images = []\n#     image_basename = Path(path).stem\n    image_basename = name\n#     print(f\"(\\'{image_basename}\\', \\'{path}\\')\")\n    img_annotations = train_annotations[train_annotations.image_id==image_basename]\n\n    boxes_viz = img_annotations[['x_min', 'y_min', 'x_max', 'y_max']].to_numpy().tolist()\n    labels_viz = img_annotations['class_id'].to_numpy().tolist()\n\n    print(\"Bboxes before nms:\\n\", boxes_viz)\n    print(\"Labels before nms:\\n\", labels_viz)\n\n    boxes_list = []\n    scores_list = []\n    labels_list = []\n    weights = []\n\n    boxes_single = []\n    labels_single = []\n\n    cls_ids = img_annotations['class_id'].unique().tolist()\n    count_dict = Counter(img_annotations['class_id'].tolist())\n    print(count_dict)\n\n    for cid in cls_ids:       \n        ## Performing Fusing operation only for multiple bboxes with the same label\n        if count_dict[cid]==1:\n            labels_single.append(cid)\n            boxes_single.append(img_annotations[img_annotations.class_id==cid][['x_min', 'y_min', 'x_max', 'y_max']].to_numpy().squeeze().tolist())\n\n        else:\n            cls_list =img_annotations[img_annotations.class_id==cid]['class_id'].tolist()\n            labels_list.append(cls_list)\n            bbox = img_annotations[img_annotations.class_id==cid][['x_min', 'y_min', 'x_max', 'y_max']].to_numpy()\n\n            ## Normalizing Bbox by Image Width and Height\n            bbox = bbox\/(img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"], img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"])\n\n\n            bbox = np.clip(bbox, 0, 1)\n            boxes_list.append(bbox.tolist())\n\n            scores_list.append(np.ones(len(cls_list)).tolist())\n\n            weights.append(1)\n\n            \n    # Perform NMS\n    if len(boxes_list)==0:\n        boxes=boxes_single\n        box_labels=labels_single\n        print(\"Bboxes after nms:\\n\", boxes)\n        print(\"Labels after nms:\\n\", box_labels)\n        \n        count_dict = Counter(box_labels)\n        print(count_dict)\n\n        text_create(desktop_path,image_basename,boxes,box_labels,img_annotations.iloc[0][\"width\"],img_annotations.iloc[0][\"height\"])\n        \n    else:\n        boxes, scores, box_labels = non_maximum_weighted(boxes_list, scores_list, labels_list, weights=weights,\n                                    iou_thr=iou_thr)\n\n\n        #img_array.shape[1]\u662f\u5bbd\u5ea6\n        boxes = boxes*(img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"], img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"])\n        boxes = boxes.round(1).tolist()\n        box_labels = box_labels.astype(int).tolist()\n\n        boxes.extend(boxes_single)\n        box_labels.extend(labels_single)\n\n        print(\"Bboxes after nms:\\n\", boxes)\n        print(\"Labels after nms:\\n\", box_labels)\n\n        count_dict = Counter(box_labels)\n        print(count_dict)\n\n        text_create(desktop_path,image_basename,boxes,box_labels,img_annotations.iloc[0][\"width\"],img_annotations.iloc[0][\"height\"])","3b514b12":"def Create_weighted_boxes_fusion_box_txt(desktop_path,name):\n    iou_thr = 0.5\n    skip_box_thr = 0.0001\n    viz_images = []\n#     image_basename = Path(path).stem\n    image_basename = name\n#     print(f\"(\\'{image_basename}\\', \\'{path}\\')\")\n    img_annotations = train_annotations[train_annotations.image_id==image_basename]\n\n    boxes_viz = img_annotations[['x_min', 'y_min', 'x_max', 'y_max']].to_numpy().tolist()\n    labels_viz = img_annotations['class_id'].to_numpy().tolist()\n\n    print(\"Bboxes before nms:\\n\", boxes_viz)\n    print(\"Labels before nms:\\n\", labels_viz)\n\n    boxes_list = []\n    scores_list = []\n    labels_list = []\n    weights = []\n\n    boxes_single = []\n    labels_single = []\n\n    cls_ids = img_annotations['class_id'].unique().tolist()\n    count_dict = Counter(img_annotations['class_id'].tolist())\n    print(count_dict)\n\n    for cid in cls_ids:       \n        ## Performing Fusing operation only for multiple bboxes with the same label\n        if count_dict[cid]==1:\n            labels_single.append(cid)\n            boxes_single.append(img_annotations[img_annotations.class_id==cid][['x_min', 'y_min', 'x_max', 'y_max']].to_numpy().squeeze().tolist())\n\n        else:\n            cls_list =img_annotations[img_annotations.class_id==cid]['class_id'].tolist()\n            labels_list.append(cls_list)\n            bbox = img_annotations[img_annotations.class_id==cid][['x_min', 'y_min', 'x_max', 'y_max']].to_numpy()\n\n            ## Normalizing Bbox by Image Width and Height\n            bbox = bbox\/(img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"], img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"])\n\n\n            bbox = np.clip(bbox, 0, 1)\n            boxes_list.append(bbox.tolist())\n\n            scores_list.append(np.ones(len(cls_list)).tolist())\n\n            weights.append(1)\n\n            \n    # Perform NMS\n    if len(boxes_list)==0:\n        boxes=boxes_single\n        box_labels=labels_single\n        print(\"Bboxes after nms:\\n\", boxes)\n        print(\"Labels after nms:\\n\", box_labels)\n        \n        count_dict = Counter(box_labels)\n        print(count_dict)\n\n        text_create(desktop_path,image_basename,boxes,box_labels,img_annotations.iloc[0][\"width\"],img_annotations.iloc[0][\"height\"])\n        \n    else:\n        boxes, scores, box_labels = weighted_boxes_fusion(boxes_list, scores_list, labels_list, weights=weights,\n                                    iou_thr=iou_thr)\n\n\n        #img_array.shape[1]\u662f\u5bbd\u5ea6\n        boxes = boxes*(img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"], img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"])\n        boxes = boxes.round(1).tolist()\n        box_labels = box_labels.astype(int).tolist()\n\n        boxes.extend(boxes_single)\n        box_labels.extend(labels_single)\n\n        print(\"Bboxes after nms:\\n\", boxes)\n        print(\"Labels after nms:\\n\", box_labels)\n\n        count_dict = Counter(box_labels)\n        print(count_dict)\n\n        text_create(desktop_path,image_basename,boxes,box_labels,img_annotations.iloc[0][\"width\"],img_annotations.iloc[0][\"height\"])","0cf2f5bb":"def text_create(desktop_path,name, boxes,box_labels,w,h):\n    \n    full_path = os.path.join(desktop_path, name+'.txt')  # \u4e5f\u53ef\u4ee5\u521b\u5efa\u4e00\u4e2a.doc\u7684word\u6587\u6863\n    print('full_path',full_path)\n    file = open(full_path, 'w')\n    \n    dw = 1. \/ (w)\n    dh = 1. \/ (h)\n    \n    for i in range(len(boxes)):\n\n        \n        x = (boxes[i][0] + boxes[i][2]) \/ 2.0\n        y = (boxes[i][1] + boxes[i][3]) \/ 2.0\n        w = boxes[i][2] - boxes[i][0]\n        h = boxes[i][3] - boxes[i][1]\n\n        \n        x = x * dw\n        w = w * dw\n        y = y * dh\n        h = h * dh\n        \n        \n        file.write(str(box_labels[i])+ ' '+ str(x)+ ' '+ str(y)+ ' '+ str(w)+ ' '+ str(h)+ ' '+ '\\n') \n        print(str(box_labels[i])+ ' '+ str(x)+ ' '+ str(y)+ ' '+ str(w)+ ' '+ str(h)+ ' ')\n    file.close()","8d875458":"os.makedirs('\/kaggle\/working\/vinbigdata\/labels\/train', exist_ok = True)\nos.makedirs('\/kaggle\/working\/vinbigdata\/labels\/val', exist_ok = True)\nos.makedirs('\/kaggle\/working\/vinbigdata\/images\/train', exist_ok = True)\nos.makedirs('\/kaggle\/working\/vinbigdata\/images\/val', exist_ok = True)\nlabel_dir = '\/kaggle\/input\/vinbigdata-yolo-labels-dataset\/labels'\nfor file in tqdm(train_files):\n    shutil.copy(file, '\/kaggle\/working\/vinbigdata\/images\/train')\n    filename = file.split('\/')[-1].split('.')[0]\n    \n    # nms stuff\n    print(filename)\n    Create_softnms_box_txt('\/kaggle\/working\/vinbigdata\/labels\/train',filename)\n    \n#     shutil.copy(os.path.join(label_dir, filename+'.txt'), '\/kaggle\/working\/vinbigdata\/labels\/train')\n    \nfor file in tqdm(val_files):\n    shutil.copy(file, '\/kaggle\/working\/vinbigdata\/images\/val')\n    filename = file.split('\/')[-1].split('.')[0]\n    \n    # nms stuff\n    Create_softnms_box_txt('\/kaggle\/working\/vinbigdata\/labels\/val',filename)\n#     shutil.copy(os.path.join(label_dir, filename+'.txt'), '\/kaggle\/working\/vinbigdata\/labels\/val')","1b26f1d7":"# import os\n# path = os.getcwd()#\u83b7\u53d6\u5f53\u524d\u8def\u5f84\n# print(path)\n\n# # all_files = [f for f in os.listdir('\/kaggle\/working\/vinbigdata\/labels\/val' )]#\u8f93\u51fa\u6839path\u4e0b\u7684\u6240\u6709\u6587\u4ef6\u540d\u5230\u4e00\u4e2a\u5217\u8868\u4e2d\n# all_files = [f for f in os.listdir(path )]#\u8f93\u51fa\u6839path\u4e0b\u7684\u6240\u6709\u6587\u4ef6\u540d\u5230\u4e00\u4e2a\u5217\u8868\u4e2d\n# #\u5bf9\u5404\u4e2a\u6587\u4ef6\u8fdb\u884c\u5904\u7406\n# print(all_files)","f3b2ccc8":"class_ids, class_names = list(zip(*set(zip(train_df.class_id, train_df.class_name))))\nclasses = list(np.array(class_names)[np.argsort(class_ids)])\nclasses = list(map(lambda x: str(x), classes))\nclasses","6bc3cd22":"from os import listdir\nfrom os.path import isfile, join\nimport yaml\n\ncwd = '\/kaggle\/working\/'\n\nwith open(join( cwd , 'train.txt'), 'w') as f:\n    for path in glob('\/kaggle\/working\/vinbigdata\/images\/train\/*'):\n        f.write(path+'\\n')\n            \nwith open(join( cwd , 'val.txt'), 'w') as f:\n    for path in glob('\/kaggle\/working\/vinbigdata\/images\/val\/*'):\n        f.write(path+'\\n')\n\ndata = dict(\n    train =  join( cwd , 'train.txt') ,\n    val   =  join( cwd , 'val.txt' ),\n    nc    = 14,\n    names = classes\n    )\n\nwith open(join( cwd , 'vinbigdata.yaml'), 'w') as outfile:\n    yaml.dump(data, outfile, default_flow_style=False)\n\nf = open(join( cwd , 'vinbigdata.yaml'), 'r')\nprint('\\nyaml:')\nprint(f.read())","91130c4a":"# https:\/\/www.kaggle.com\/ultralytics\/yolov5\n# !git clone https:\/\/github.com\/ultralytics\/yolov5  # clone repo\n# %cd yolov5\nshutil.copytree('\/kaggle\/input\/yolov5-official-v31-dataset\/yolov5', '\/kaggle\/working\/yolov5')\nos.chdir('\/kaggle\/working\/yolov5')\n# %pip install -qr requirements.txt # install dependencies\n\nimport torch\nfrom IPython.display import Image, clear_output  # to display images\n\nclear_output()\nprint('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))","f662b674":"!python detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source data\/images\/\nImage(filename='runs\/detect\/exp\/zidane.jpg', width=600)","c5b9a8f1":"# !WANDB_MODE=\"dryrun\" python train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt --nosave --cache \n!WANDB_MODE=\"dryrun\" python train.py --img 640 --batch 16 --epochs 30 --data \/kaggle\/working\/vinbigdata.yaml --weights yolov5s.pt --cache","658eea44":"plt.figure(figsize = (20,20))\nplt.axis('off')\nplt.imshow(plt.imread('runs\/train\/exp\/labels_correlogram.jpg'));","82ea9ec4":"plt.figure(figsize = (20,20))\nplt.axis('off')\nplt.imshow(plt.imread('runs\/train\/exp\/labels.jpg'));","69a9739f":"import matplotlib.pyplot as plt\nplt.figure(figsize = (15, 15))\nplt.imshow(plt.imread('runs\/train\/exp\/train_batch0.jpg'))\n\nplt.figure(figsize = (15, 15))\nplt.imshow(plt.imread('runs\/train\/exp\/train_batch1.jpg'))\n\nplt.figure(figsize = (15, 15))\nplt.imshow(plt.imread('runs\/train\/exp\/train_batch2.jpg'))","76badbd1":"fig, ax = plt.subplots(3, 2, figsize = (2*5,3*5), constrained_layout = True)\nfor row in range(3):\n    ax[row][0].imshow(plt.imread(f'runs\/train\/exp\/test_batch{row}_labels.jpg'))\n    ax[row][0].set_xticks([])\n    ax[row][0].set_yticks([])\n    ax[row][0].set_title(f'runs\/train\/exp\/test_batch{row}_labels.jpg', fontsize = 12)\n    \n    ax[row][1].imshow(plt.imread(f'runs\/train\/exp\/test_batch{row}_pred.jpg'))\n    ax[row][1].set_xticks([])\n    ax[row][1].set_yticks([])\n    ax[row][1].set_title(f'runs\/train\/exp\/test_batch{row}_pred.jpg', fontsize = 12)","4ef47381":"plt.figure(figsize=(30,15))\nplt.axis('off')\nplt.imshow(plt.imread('runs\/train\/exp\/results.png'));","3d3dbe9b":"plt.figure(figsize=(30,15))\nplt.axis('off')\nplt.imshow(plt.imread('runs\/train\/exp\/confusion_matrix.png'));","bd00fcb9":"!python detect.py --weights 'runs\/train\/exp\/weights\/best.pt'\\\n--img 640\\\n--conf 0.15\\\n--iou 0.5\\\n--source \/kaggle\/working\/vinbigdata\/images\/val\\\n--exist-ok","17791bb6":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nimport numpy as np\nimport random\nimport cv2\nfrom glob import glob\nfrom tqdm import tqdm\n\nfiles = glob('runs\/detect\/exp\/*')\nfor _ in range(3):\n    row = 4\n    col = 4\n    grid_files = random.sample(files, row*col)\n    images     = []\n    for image_path in tqdm(grid_files):\n        img          = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n        images.append(img)\n\n    fig = plt.figure(figsize=(col*5, row*5))\n    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n                     nrows_ncols=(col, row),  # creates 2x2 grid of axes\n                     axes_pad=0.05,  # pad between axes in inch.\n                     )\n\n    for ax, im in zip(grid, images):\n        # Iterating over the grid returns the Axes.\n        ax.imshow(im)\n        ax.set_xticks([])\n        ax.set_yticks([])\n    plt.show()","8a5b178a":"shutil.rmtree('\/kaggle\/working\/vinbigdata')\nshutil.rmtree('runs\/detect')\nfor file in (glob('runs\/train\/exp\/**\/*.png', recursive = True)+glob('runs\/train\/exp\/**\/*.jpg', recursive = True)):\n    os.remove(file)","23e46ca2":"# Split","c8e4590d":"# Version\n* `v13`: Fold4\n* `v12`: Fold3\n* `v10`: Fold2\n* `v09`: Fold1\n* `v03`: Fold0","1e7c005a":"# Class Distribution","2853e371":"# (Loss, Map) Vs Epoch","adcda21e":"# Inference","481f9c83":"# YOLOv5 Stuff","04110265":"# Get Class Name","a9e8dabe":"# Selecting Models\nIn this notebok I'm using `v5s`. To select your prefered model just replace `--cfg models\/yolov5s.yaml --weights yolov5s.pt` with the following command:\n* `v5s` : `--cfg models\/yolov5s.yaml --weights yolov5s.pt`\n* `v5m` : `--cfg models\/yolov5m.yaml --weights yolov5m.pt`\n* `v5l` : `--cfg models\/yolov5l.yaml --weights yolov5l.pt`\n* `v5x` : `--cfg models\/yolov5x.yaml --weights yolov5x.pt`","d74cb508":"# GT Vs Pred","ae19b749":"# Confusion Matrix","c73cf4f4":"# Batch Image","004fb462":"# [YOLOv5](https:\/\/github.com\/ultralytics\/yolov5)\n![](https:\/\/user-images.githubusercontent.com\/26833433\/98699617-a1595a00-2377-11eb-8145-fc674eb9b1a7.jpg)\n![](https:\/\/user-images.githubusercontent.com\/26833433\/90187293-6773ba00-dd6e-11ea-8f90-cd94afc0427f.png)","1687c8cf":"## Pretrained Checkpoints:\n\n| Model | AP<sup>val<\/sup> | AP<sup>test<\/sup> | AP<sub>50<\/sub> | Speed<sub>GPU<\/sub> | FPS<sub>GPU<\/sub> || params | FLOPS |\n|---------- |------ |------ |------ | -------- | ------| ------ |------  |  :------: |\n| [YOLOv5s](https:\/\/github.com\/ultralytics\/yolov5\/releases\/tag\/v3.0)    | 37.0     | 37.0     | 56.2     | **2.4ms** | **416** || 7.5M   | 13.2B\n| [YOLOv5m](https:\/\/github.com\/ultralytics\/yolov5\/releases\/tag\/v3.0)    | 44.3     | 44.3     | 63.2     | 3.4ms     | 294     || 21.8M  | 39.4B\n| [YOLOv5l](https:\/\/github.com\/ultralytics\/yolov5\/releases\/tag\/v3.0)    | 47.7     | 47.7     | 66.5     | 4.4ms     | 227     || 47.8M  | 88.1B\n| [YOLOv5x](https:\/\/github.com\/ultralytics\/yolov5\/releases\/tag\/v3.0)    | **49.2** | **49.2** | **67.7** | 6.9ms     | 145     || 89.0M  | 166.4B\n| | | | | | || |\n| [YOLOv5x](https:\/\/github.com\/ultralytics\/yolov5\/releases\/tag\/v3.0) + TTA|**50.8**| **50.8** | **68.9** | 25.5ms    | 39      || 89.0M  | 354.3B\n| | | | | | || |\n| [YOLOv3-SPP](https:\/\/github.com\/ultralytics\/yolov5\/releases\/tag\/v3.0) | 45.6     | 45.5     | 65.2     | 4.5ms     | 222     || 63.0M  | 118.0B","a6d1b130":"# Only 14 Class","2a776d33":"# Inference Plot","96658dcb":"# Copying Files","a1ba69d1":"# Train"}}