{"cell_type":{"e68df145":"code","15b0a33e":"code","3acc775c":"code","79df73c2":"code","c5fa0a0c":"code","a79ec02c":"code","1bed6f1e":"code","c5770d6d":"code","f59ee521":"code","0ec644e3":"code","f1325ad5":"code","c296adb3":"code","b42bcaff":"code","a4e9bb63":"code","8769709a":"code","d23a8ab8":"code","108bd041":"code","ee6a4a22":"code","b4a3251c":"code","971db771":"code","ddf2661b":"code","7534839e":"code","1fa5246c":"code","bb4352af":"code","a00b7128":"code","c367e0bf":"code","55cbb555":"code","389abc06":"code","0ecef428":"code","2acf1d1c":"code","c96a6e2c":"code","edaca78c":"code","8e76442b":"code","0a261743":"code","a91d3d84":"code","ca96deb6":"code","b6e8d9b7":"code","1a0a9e2e":"code","7e8ddbf5":"code","e35f8153":"code","420174ec":"code","b9c26e3b":"code","871917b0":"code","8a522a30":"code","d5bee15c":"code","3e19edf7":"code","15f4b7ad":"code","3892a8c8":"code","43567f5e":"code","47aa0a90":"code","04ee640d":"code","a59a26db":"code","bc55a5fa":"code","a38b3e27":"code","597a7847":"code","6bfd8702":"code","d3039b69":"code","cdcf2db5":"code","797563e5":"code","a74aa55d":"code","e63f8db1":"code","93bde692":"code","0dfc5b9c":"code","8e120b32":"code","67ee2f7c":"code","8cd641d5":"markdown","dae0385b":"markdown","5112db25":"markdown","ea4ce474":"markdown","6450c7f7":"markdown","8f30dd21":"markdown","30e58ed3":"markdown","5eb29390":"markdown","22562678":"markdown","68a019ca":"markdown","81ca2745":"markdown","a0c51d5e":"markdown","3e229325":"markdown","c6013285":"markdown","3a027bbd":"markdown","82e57fbc":"markdown","adc83ca0":"markdown","cc9df6fb":"markdown","bf1f1936":"markdown","b3a3598b":"markdown","c13a7b9b":"markdown","0e7751ae":"markdown","91fe8b8d":"markdown","9694459e":"markdown","49b95453":"markdown","277ff015":"markdown","59f59cdd":"markdown","1cf31919":"markdown","d1362771":"markdown","6640b4ea":"markdown","9269ba9f":"markdown","76d4ee64":"markdown","15515060":"markdown","c32ea128":"markdown","749eec3d":"markdown","6881e7c6":"markdown","2e69700d":"markdown"},"source":{"e68df145":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","15b0a33e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\npd.pandas.set_option('display.max_columns', None)","3acc775c":"#Importing datasets\ntrain_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","79df73c2":"train_df.head()","c5fa0a0c":"test_df.head()","a79ec02c":"# Basic feature details\ntrain_df.info()","1bed6f1e":"#Statistical details of quantative features\ntrain_df.describe().T","c5770d6d":"#making a df of column-wise missing values and their percentages\ndef missing_per(df):\n    total = df.isnull().sum()[df.isnull().sum() != 0].sort_values(ascending = False)\n    percent = np.round(total\/len(df) * 100 ,2)\n    return pd.concat([total,percent], axis = 'columns',keys = ['Total', 'Percent'])\n\nmissing_per(train_df)\n        ","f59ee521":"missing_per(test_df)","0ec644e3":"import seaborn as sns\nimport matplotlib.gridspec as gridspec\nfrom scipy import stats\nimport matplotlib.style as style\nstyle.use('fivethirtyeight')\n\ndef three_plots(df, feature):\n    fig = plt.figure(figsize = (13,8), layout = 'constrained')\n    grid = gridspec.GridSpec(nrows = 2, ncols = 3, figure = fig)\n\n    ax1 = fig.add_subplot(grid[0,:2])\n    ax1.set_title('Histogram')\n    sns.histplot(data = df[feature], kde = True)\n\n    ax2 = fig.add_subplot(grid[1,:2])\n    ax2.set_title('QQ Plot')\n    stats.probplot(df[feature], plot = ax2)\n\n    ax3 = fig.add_subplot(grid[:2,2])\n    ax3.set_title('Box Plot')\n    sns.boxplot(y = df[feature], ax= ax3)\n\nthree_plots(train_df, 'SalePrice')","f1325ad5":"print(f\"Skewness  : {train_df['SalePrice'].skew()}\")\nprint(f\"Kurotosis : {train_df['SalePrice'].kurt()}\")","c296adb3":"train_df.corr()['SalePrice'].sort_values(ascending = False)[1:]","b42bcaff":"numerical = [f for f in train_df.columns if train_df.dtypes[f] != 'object']\nnumerical.remove('SalePrice')\nnumerical.remove('Id')\ncategorical = [f for f in train_df.columns if train_df.dtypes[f] == 'object']\n\nlen(numerical), len(categorical)","a4e9bb63":"def scatter_plot(x,y):\n    style.use('fivethirtyeight')\n    plt.figure(figsize = (12,8))\n    sns.scatterplot(x=x, y=y)\n    \nscatter_plot(train_df['OverallQual'], train_df['SalePrice'])","8769709a":"scatter_plot(train_df['GrLivArea'], train_df['SalePrice'])","d23a8ab8":"# train_df.drop(train_df[train_df['GrLivArea']> 4000].index , axis = 0, inplace = True)","108bd041":"train_df.drop(train_df[(train_df['GrLivArea']>4000) & (train_df['SalePrice']<300000)].index, inplace = True)","ee6a4a22":"scatter_plot(train_df.GarageArea, train_df.SalePrice)","b4a3251c":"scatter_plot(train_df.TotalBsmtSF, train_df.SalePrice)","971db771":"scatter_plot(train_df['1stFlrSF'], train_df.SalePrice)","ddf2661b":"scatter_plot(train_df['MasVnrArea'], train_df['SalePrice'])","7534839e":"fig, (ax1, ax2) = plt.subplots(ncols = 2, figsize = (12,6))\n\nsns.regplot(data = train_df , x = 'GrLivArea', y = 'SalePrice', ax = ax1)\n\nsns.regplot(data = train_df, x = 'MasVnrArea', y = 'SalePrice', ax = ax2, color = 'c')","1fa5246c":"plt.figure(figsize = (12,8))\nsns.residplot(data= train_df, x = 'GrLivArea', y = 'SalePrice')","bb4352af":"three_plots(train_df, 'SalePrice')","a00b7128":"train_df_new = train_df.copy()\ntrain_df_new['SalePrice'] = np.log1p(train_df_new['SalePrice'])\nthree_plots(train_df_new, 'SalePrice')","c367e0bf":"print(f\"Skewness  : {train_df_new['SalePrice'].skew()}\")\nprint(f\"Kurotosis : {train_df_new['SalePrice'].kurt()}\")","55cbb555":"fig, (ax1, ax2) = plt.subplots(ncols = 2, figsize = (15,8))\nsns.residplot(x= train_df['GrLivArea'],y=train_df['SalePrice'], ax= ax1)\nsns.residplot(x= train_df_new['GrLivArea'],y=train_df_new['SalePrice'], ax= ax2, color = 'orange')","389abc06":"style.use('ggplot')\nsns.set_style('whitegrid')\nplt.figure(figsize =(30,20))\nmask = np.zeros_like(train_df.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(train_df.corr(), annot= True, mask=mask,cmap=sns.diverging_palette(20, 220, n=200))","0ecef428":"plt.figure(figsize = (30,8))\nsns.boxplot(x = 'Neighborhood', y ='LotFrontage', data = train_df)","2acf1d1c":"#concatenating training and test data\nall_data = pd.concat((train_df, test_df)).reset_index(drop=True)\nall_data.drop(['SalePrice','Id'], axis = 1,inplace = True)\nall_data","c96a6e2c":"missing_per(all_data)","edaca78c":"missing_val_col = [\"Alley\", \n                   \"PoolQC\", \n                   \"MiscFeature\",\n                   \"Fence\",\n                   \"FireplaceQu\",\n                   \"GarageType\",\n                   \"GarageFinish\",\n                   \"GarageQual\",\n                   \"GarageCond\",\n                   'BsmtQual',\n                   'BsmtCond',\n                   'BsmtExposure',\n                   'BsmtFinType1',\n                   'BsmtFinType2',\n                   'MasVnrType']\n\nfor i in missing_val_col:\n    all_data[i] = all_data[i].fillna('None')","8e76442b":"missing_per(all_data)","0a261743":"all_data['MSZoning'] = all_data.groupby('MSSubClass')['MSZoning'].transform(lambda x:x.fillna(x.mode()[0]))\nall_data['Functional'].fillna(all_data['Functional'].mode()[0], inplace =True)\nall_data['SaleType'].fillna(all_data['SaleType'].mode()[0], inplace = True)\nall_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0], inplace = True)\nall_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0], inplace = True)\nall_data['Electrical'].fillna(all_data['Electrical'].mode()[0], inplace = True)\nall_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0], inplace = True)\nall_data['Utilities'].fillna(all_data['Utilities'].mode()[0], inplace = True)\n","a91d3d84":"missing_val_col2 = ['BsmtFinSF1',\n                    'BsmtFinSF2',\n                    'BsmtUnfSF',\n                    'TotalBsmtSF',\n                    'BsmtFullBath', \n                    'BsmtHalfBath', \n                    'GarageArea',\n                    'GarageYrBlt', #no garage therefore no value for year built\n                    'GarageCars',\n                    'MasVnrArea']\n\nfor i in missing_val_col2:\n    all_data[i] = all_data[i].fillna(0)","ca96deb6":"all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x:x.fillna(x.median()))","b6e8d9b7":"missing_per(all_data)","1a0a9e2e":"#converting some columns from numerical to categorical\n\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","7e8ddbf5":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(all_data[c]) \n    all_data[c] = lbl.transform(all_data[c])\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","e35f8153":"# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","420174ec":"#Checking Skewness\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\nskewed_feats = all_data[numeric_feats].skew().sort_values(ascending=False)\nskewed_feats    ","b9c26e3b":"sns.histplot(all_data['1stFlrSF'], kde=True)","871917b0":"#Fixing Skewness\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n\nfor idx, feat in skewed_feats.iteritems():\n    if abs(feat) > 0.75:\n        all_data[idx] = boxcox1p(all_data[idx], .15)\n#         all_data[idx] = boxcox1p(all_data[idx], boxcox_normmax(all_data[idx]+1))","8a522a30":"sns.histplot(all_data['1stFlrSF'], kde=True)","d5bee15c":"all_data['Utilities'].value_counts()\n#doesnt seem like there is much information here. Can be dropped.","3e19edf7":"# all_data = all_data.drop(['Utilities', 'Street', 'PoolQC',], axis=1)\nall_data = all_data.drop(['Utilities'], axis=1)","15f4b7ad":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","3892a8c8":"all_data.head()","43567f5e":"train = all_data.iloc[:len(train_df_new),:]\ntest = all_data.iloc[len(train_df_new):,:]\ny_train = train_df_new['SalePrice'].values","47aa0a90":"from sklearn.linear_model import ElasticNet, Lasso, Ridge, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.ensemble import StackingRegressor, VotingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","04ee640d":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y_train, y_pred))","a59a26db":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=0))\n\nridge =  make_pipeline(RobustScaler(), Ridge(alpha =15, random_state=1, max_iter = 1e7))\n\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=1, max_iter = 1e7))\n\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =0)\n\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =0, nthread = -1,\n                             verbosity = 0)\n\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11,\n                              verbose_eval = -1)","bc55a5fa":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","a38b3e27":"score = rmsle_cv(ridge)\nprint(\"\\Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","597a7847":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","6bfd8702":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","d3039b69":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","cdcf2db5":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","797563e5":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","a74aa55d":"vr = VotingRegressor([('enet', ENet),('gb', GBoost),('lasso', lasso), ('KRR', KRR)])\n\nscore = rmsle_cv(vr)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","e63f8db1":"estimators = [('enet', ENet),('gb', GBoost), ('KRR', KRR)]\nsr = StackingRegressor(estimators,\n                      final_estimator = lasso)\n\nscore = rmsle_cv(sr)\nprint(\" Stacked models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","93bde692":"lasso_model = lasso.fit(train, y_train)\nridge_model = ridge.fit(train, y_train)\nEnet_model = ENet.fit(train, y_train)\nKRR_model = KRR.fit(train, y_train)\ngboost_model = GBoost.fit(train, y_train)\nxgboost_model = model_xgb.fit(train, y_train)\nlightgbm_model = model_lgb.fit(train, y_train)\n\nvr_model = vr.fit(train, y_train)\nsr_model = sr.fit(train, y_train)","0dfc5b9c":"print(\"lasso rmse : {:.4f}\".format(rmsle(y_train, lasso_model.predict(train))))\nprint(\"ridge rmse : {:.4f}\".format(rmsle(y_train, ridge_model.predict(train))))\nprint(\"Enet rmse : {:.4f}\".format(rmsle(y_train, Enet_model.predict(train))))\nprint(\"KRR rmse : {:.4f}\".format(rmsle(y_train, KRR_model.predict(train))))\nprint(\"gboost rmse : {:.4f}\".format(rmsle(y_train, GBoost.predict(train))))\nprint(\"xgboost rmse : {:.4f}\".format(rmsle(y_train, xgboost_model.predict(train))))\nprint(\"lgbm rmse : {:.4f}\".format(rmsle(y_train, lightgbm_model.predict(train))))\n\nprint(\"Voting Regressor rmse : {:.4f}\".format(rmsle(y_train, vr_model.predict(train))))\nprint(\"Stacking Regressor rmse : {:.4f}\".format(rmsle(y_train, sr_model.predict(train))))","8e120b32":"y_pred = ((0.70* np.expm1(sr_model.predict(test))) + \n          (0.15* np.expm1(xgboost_model.predict(test))) + \n          (0.15* np.expm1(lightgbm_model.predict(test))))\n                  ","67ee2f7c":"sub = pd.DataFrame()\nsub['Id'] = test_df['Id']\nsub['SalePrice'] = y_pred\nsub.to_csv('submission.csv',index=False)","8cd641d5":"As seen in the above resudal plot - Residuals are forming a funnel shape indicating high hetroscedasticity; which is a red flag for linear regression.\n","dae0385b":"As seen in above plot, 'SalePrice' is not normally distributed. We are going to do log transformation to make it close to Normal distribution.","5112db25":"The graph shows features are quite skewed.","ea4ce474":"We have filled all the missing values.","6450c7f7":"**Model Scores**","8f30dd21":"# Data Modelling","30e58ed3":"**Stacking Models**","5eb29390":"Residual Plot after doing the log transfomation is much more random as desired by linear regresssion. Heteroscedasticity of DV has been fixed.","22562678":"The above plot shows much more normality in data. Also the skewness and kurotosis have been fixed to an extent.","68a019ca":"**Credits**\n\nI have learnt a lot from following notebooks and this notebook is largely inspired from them -\n\n* [ A Detailed Regression Guide with House-pricing](http:\/\/https:\/\/www.kaggle.com\/masumrumi\/a-detailed-regression-guide-with-house-pricing\/notebook#Goals)\n* [Stacked Regressions : Top 4% on LeaderBoard](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\/notebook)\n\n\n","81ca2745":"There are a lot of null values in quite a few features. We have to come up with a way to cater for these.","a0c51d5e":"# Housing Price Prediction\n\nThe aim of this notebook is to apply regression techniques to the housing prices problem.","3e229325":"As we can see different neigborhoods have different distributions of lotfrontage. The mean\/median values will be useful in catering for the null values in the lotfrontage feature.","c6013285":"We can see at the bottom right two with extremely large GrLivArea that are of a low price. These values are huge oultliers.\n\nAs per the author of the [dataset](http:\/\/jse.amstat.org\/v19n3\/decock.pdf) all instances of'GrLivArea' > 4000 are outliers. \n\nFrom the above graph we can see two points in the extreme bottom right of the graph seem to be the outliers (considerably low saleprice w.r.t. the extreme area); hence we are dropping them.","3a027bbd":"Athough we can see that there is a linear relationship between IVs and DV, we still need to check if the data follows the assumptions of linear regression and if not we have to do some transormations in order to make it good for the linear regression algorithms.\n\n\nAssumptions for linear regression -\n\n* Linearity - **data is linear as confirmed by the regression plots**\n* Homoscedasticity ( Constant Variance )\n* Independence of Errors (no Autocorrelation) - **generally in time-series data like stock prices**\n* Multivariate Normality ( Normality of Errors)\n* No or Little multicollinearity","82e57fbc":"PoolQC : data description says NA means \"No Pool\". That make sense, given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general.\n\nMiscFeature : data description says NA means \"no misc feature\"\n\nAlley : data description says NA means \"no alley access\"\n\nFence : data description says NA means \"no fence\"\n\nFireplaceQu : data description says NA means \"no fireplace\"\n\nGarageType, GarageFinish, GarageQual and GarageCond: NA means no Garage \n\nBsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 : For all these categorical basement-related features, NaN means that there is no basement\n\nMasVnrType : NA most likely means no masonry veneer for these houses. \n\n**All of these are categorical features. Replacing missing data with None**","adc83ca0":"From the above graphs it is evedent that there is a linear relationship amongst the features w.r.t. SalePrice. Lets fit liner regression line using seaborn library to get the visual confirmation.","cc9df6fb":"Lets try to compute column-wise percentages of missing values.","bf1f1936":"**Doing some EDA with respect to 'SalePrice'**","b3a3598b":"Skewness seems to be fixed.","c13a7b9b":"We also need to see a relationship between lotfrontage with neighborhood. This will come handy later on.","0e7751ae":"**RMSE scores**","91fe8b8d":"**Findings so far -**\n\nThere are 1460 instances of training data and 1459 of test data. Total number of attributes equals 81, of which 36 is quantitative, 43 categorical + Id and SalePrice.\n\nQuantitative: 1stFlrSF, 2ndFlrSF, 3SsnPorch, BedroomAbvGr, BsmtFinSF1, BsmtFinSF2, BsmtFullBath, BsmtHalfBath, BsmtUnfSF, EnclosedPorch, Fireplaces, FullBath, GarageArea, GarageCars, GarageYrBlt, GrLivArea, HalfBath, KitchenAbvGr, LotArea, LotFrontage, LowQualFinSF, MSSubClass, MasVnrArea, MiscVal, MoSold, OpenPorchSF, OverallCond, OverallQual, PoolArea, ScreenPorch, TotRmsAbvGrd, TotalBsmtSF, WoodDeckSF, YearBuilt, YearRemodAdd, YrSold\n\nQualitative: Alley, BldgType, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtQual, CentralAir, Condition1, Condition2, Electrical, ExterCond, ExterQual, Exterior1st, Exterior2nd, Fence, FireplaceQu, Foundation, Functional, GarageCond, GarageFinish, GarageQual, GarageType, Heating, HeatingQC, HouseStyle, KitchenQual, LandContour, LandSlope, LotConfig, LotShape, MSZoning, MasVnrType, MiscFeature, Neighborhood, PavedDrive, PoolQC, RoofMatl, RoofStyle, SaleCondition, SaleType, Street, Utilities,","9694459e":"As we have seen above that different neighborhoods have different lotfrontage - it seems reasonable to exploit this relationship while filling the missing values.","49b95453":"**Models**","277ff015":"In the next cell, I am going to make Q-Q plot, histogram and box plot for the Independant Variable in order to find -\n* if the predictor (IV) is normal\n* outliers in the IV","59f59cdd":"# Imputing Missing Values","1cf31919":"**New training and test set**","d1362771":"**Averaging Base Models**","6640b4ea":"As it is evident from the plots and statistical metrics (skewness and kurtosis); SalePrice is not normal and there are quite a few outliers.\n\nWe need to normalize the data as it is one of the main assumptions of many regression algorithms including Linear Regression.\n\nLets also check the corelation of DV with respect to IV.","9269ba9f":"There is high correlation amongst certain features which is not suitable. However scikit learn algos like lasso will take care of these.","76d4ee64":"MSZoning, Functional, SaleType, Exterior1st, Exterior2nd, Electrical, KitchenQual, Utilities - **All of these are quantative features with upto 4 missing values at most. Imputing all of these with mode.**","15515060":"BsmtFinSF1, BsmtFinSF2, BsmtUnfSF,TotalBsmtSF, BsmtFullBath, BsmtHalfBath: NA means no basement\nGarageArea, GarageYrBlt, GarageCars: NA means no garage\nMasVnrArea: NA means no masonry veneer\n\n**All of these are quantative features therefore imputing with 0.**","c32ea128":"**Ensemble**","749eec3d":"# Feature Engineering","6881e7c6":"There are quite a few null values here.","2e69700d":"**Dummy Variables**"}}