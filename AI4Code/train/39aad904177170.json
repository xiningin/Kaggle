{"cell_type":{"6ffdcf67":"code","698bc0b2":"code","d33a207b":"code","e5040acb":"code","613c3cc6":"code","f08b8a2c":"code","4b36e12d":"code","42c0ced4":"code","8076a53e":"code","3ee91525":"code","7ea8fea1":"code","9785453b":"code","1a64ebdc":"code","326774b6":"code","f3e14429":"code","74d507c0":"code","7ce9eb71":"code","7d35b40a":"code","0bae11f0":"code","85e6a081":"code","17e36e2f":"code","3a92a0ac":"code","def1ff21":"code","cd2449a1":"code","1d18292f":"code","3fb81da7":"code","4ba053af":"code","985ce037":"code","1db51a6f":"code","a6fdd0b5":"code","419bf2e8":"code","ae2a5515":"code","8fbc0b59":"code","33a52448":"code","1b33bd56":"code","2431278d":"code","b35c8770":"code","b123cb46":"code","91a80fa6":"code","bfb4c7ba":"code","aab77eca":"code","61b00e25":"code","38d64117":"code","075a1f53":"code","073039e2":"code","92e66921":"code","f32f1b3d":"code","ed9dad10":"code","cf5d25dd":"code","2057c1a2":"code","fa109cfc":"code","f62444b7":"code","118597f1":"code","19f48a8f":"code","67e465df":"code","11b8c28e":"code","e99fc4ae":"code","32defd73":"code","00245f69":"code","eae9aeaa":"code","3647408c":"code","40143ed7":"code","f279ec1a":"code","2265df84":"code","24ac47ce":"code","5f52a5aa":"code","988a0e96":"code","f6c8c393":"code","496f2907":"code","291fd6fb":"code","88287cc2":"code","35e9a518":"code","3be6fb39":"markdown","096fccaa":"markdown","f9958b45":"markdown","a5a54279":"markdown","7f56b8dd":"markdown","63664fa6":"markdown","a659aeee":"markdown","300e7253":"markdown"},"source":{"6ffdcf67":"#import libraries\nimport numpy \nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn import metrics\nimport matplotlib.patches as mpatches\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nimport scipy.stats as st \n# import dtale \nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas_profiling as pdp\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import roc_curve","698bc0b2":"fd_dlvry=pd.read_csv('..\/input\/online-food-delivery-preferencesbangalore-region\/onlinedeliverydata.csv')","d33a207b":"fd_ori=fd_dlvry.copy(deep=True)\n#future uses one copy create","e5040acb":"fd_dlvry.shape","613c3cc6":"fd_dlvry.head()","f08b8a2c":"fd_dlvry.info()","4b36e12d":"#we can use datle library for EDA but \n# d = dtale.show(fd_dlvry)\n# d.open_browser()\n","42c0ced4":"# looking data with pdp.ProfileReport \ndata = pd.read_csv('..\/input\/online-food-delivery-preferencesbangalore-region\/onlinedeliverydata.csv')\n\n#Creating the Exploratory Data Analysis of our dataset\nreport = pdp.ProfileReport(data, title='Pandas Profiling Report')","8076a53e":"report","3ee91525":"pip install datacleaner","7ea8fea1":"#this library i found on google it's helpful for cleaning data,imputing value and lable encoding\nimport  datacleaner as dc # pip install datacleaner","9785453b":"dc.autoclean(fd_dlvry).head()","1a64ebdc":"fd_dlvry.head() ","326774b6":"fd_ori.head()","f3e14429":"l1=(fd_ori.columns[:])\nl1","74d507c0":"fd_dlvry.info()","7ce9eb71":"df=fd_ori.drop('Reviews',axis=1) #drop reviews because it's required  NLP in future i will try it\ndf.columns","7d35b40a":"%matplotlib inline","0bae11f0":"# we needs to back map for lable encoding \nlbl=dict()\nfor i in fd_ori.columns:\n    if i not in  ['latitude','longitude','Pin code','Output','Reviews']:\n                  lbl[i]=dict(zip(fd_ori[i].unique(), fd_dlvry[i].unique()))\n    ","85e6a081":"lbl","17e36e2f":"# # Optional practice chi-squared test \n# df=pd.crosstab(fd_ori['Meal(P1)'],fd_ori['Occupation'])\n# df\n\n# stat, p, dof, expected = st.chi2_contingency(df,correction=True)\n# stat, p, dof, expected\n\n\n","3a92a0ac":"#Cramer's V perform for find correlation between catgorical variable with respect to Output\ncrv={}\nl1=[]\nfor i in fd_ori.columns:\n    if i not in  ['latitude','longitude','Pin code','Output','Reviews']:\n        #we are not using lat and long because in EDA all location diffrent (reference dtale lib)\n        df=pd.crosstab(fd_ori['Output'],fd_ori[i])\n        stat, p, dof, expected = st.chi2_contingency(df,correction=True)\n        t=min(df.shape)-1\n        deno=sum(df.sum())*t\n        x=numpy.sqrt(stat \/ deno)\n        if(x>0.41): #thresold\n             l1.append((i,x))\n\n\ncrv['Output']=l1\n        \ncrv","def1ff21":"#Extract the feature from above dictionary \nbasic_f0=[]\nfor i in l1:\n    basic_f0.append(i[0])\n\nbasic_f0","cd2449a1":"X=fd_dlvry[basic_f0] # select X \ny=fd_dlvry['Output'] # select Y\n","1d18292f":"basic_f0","3fb81da7":"sns.countplot(fd_ori['Output']) # look target feature","4ba053af":"# train_test_split by sklearn \nX=fd_dlvry[basic_f0]\ny=fd_dlvry['Output']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33)","985ce037":"#select best knneighbour find \nplt.figure()\ntraining_accuracy = []\ntest_accuracy = []\n# try n_neighbors from 1 to 10\nneighbors_settings = range(1, 11)\nfor i in neighbors_settings:\n    clf = KNeighborsClassifier(n_neighbors=i)\n    clf.fit(X_train, y_train)\n    training_accuracy.append(clf.score(X_train, y_train))\n    # record generalization accuracy\n    test_accuracy.append(clf.score(X_test, y_test))\nplt.plot(neighbors_settings,training_accuracy,label=\"training accuracy\")\nplt.plot(neighbors_settings,test_accuracy,label=\"test accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"n_neighbors\")\nplt.legend()\n\nplt.show()","1db51a6f":"# appropriate test size find \nplt.figure()\ntraining_accuracy = []\ntest_accuracy = []\n# try n_neighbors from 1 to 10\ntest_size= numpy.arange(0.1, 0.9,0.05)\nknn = KNeighborsClassifier(n_neighbors = 5,p=1)\nfor i in test_size:\n    \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = i)\n        knn.fit(X_train, y_train)\n        training_accuracy.append(knn.score(X_test, y_test))\n        test_accuracy.append(knn.score(X_train, y_train))\n  \n\n    \nplt.plot(test_size,training_accuracy,label=\"training accuracy\")\nplt.plot(test_size,test_accuracy,label=\"test accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"test_size\")\nplt.legend()\n\nplt.show()","a6fdd0b5":"#split data\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.33)","419bf2e8":"# fit data and perform the knn algo\nneigh = KNeighborsClassifier(n_neighbors=5,p=1)\n\nknn=neigh.fit(X_train,y_train)\n\nknn.score(X_test,y_test)\n","ae2a5515":"#y_pred for confusion matrix\ny_pred=knn.predict(X_test)","8fbc0b59":"#confusion matrix \ncf_matrix_n=confusion_matrix(y_test, y_pred)\ncf_matrix_n","33a52448":"#classification report \nprint(classification_report(y_test,y_pred))","1b33bd56":"from sklearn.metrics import roc_curve\nfpr, tpr, thresholds=roc_curve(y_test, y_pred)\nAUC=round(metrics.auc(fpr, tpr),2)\n\nplt.plot([0,1], [0,1], linestyle='--', label='No Skill')\nplt.plot(fpr, tpr, marker='.', label=f'knn AUC={AUC}')\n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\n# show the plot\nplt.show()\n","2431278d":"'''Here weighted avg f1-score is good and AUC is greate but problems here Knn is not learning something\nknn just use more memory space for perform operation and mostly features are categorical so noise affect\non this algorithms\n'''","b35c8770":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2,mutual_info_classif","b123cb46":"def select_features(X_train, y_train, X_test):\n    fs = SelectKBest(score_func=chi2, k='all')\n    fs.fit(X_train, y_train)\n    X_train_fs = fs.transform(X_train)\n    X_test_fs = fs.transform(X_test)\n    return X_train_fs, X_test_fs, fs","91a80fa6":"#select feature \nl1=['latitude','longitude','Pin code','Output','Reviews']\nX=fd_dlvry[fd_dlvry.columns[~fd_dlvry.columns.isin(l1)]]\ny=fd_dlvry['Output']","bfb4c7ba":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)","aab77eca":"X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\nfor i in range(len(fs.scores_)):\n     print('Feature %d: %f' % (i, fs.scores_[i]))\n# plot the scores\nplt.bar([i for i in range(len(fs.scores_))], fs.scores_)\n\nplt.show()","61b00e25":"# Extract feature \nbasic_f1=[]\nfor i in range(len(fs.scores_)):\n    if(fs.scores_[i]>10): # set Thresold\n        print(X_test.columns[i])\n        basic_f1.append(X_test.columns[i])","38d64117":"X=fd_dlvry[basic_f1]\ny=fd_dlvry['Output']","075a1f53":"#neighbour find \nplt.figure()\ntraining_accuracy = []\ntest_accuracy = []\n# try n_neighbors from 1 to 10\nneighbors_settings = range(1, 11)\nfor i in neighbors_settings:\n    clf = KNeighborsClassifier(n_neighbors=i,p=1)\n    clf.fit(X_train, y_train)\n    training_accuracy.append(clf.score(X_train, y_train))\n    # record generalization accuracy\n    test_accuracy.append(clf.score(X_test, y_test))\nplt.plot(neighbors_settings,training_accuracy,label=\"training accuracy\")\nplt.plot(neighbors_settings,test_accuracy,label=\"test accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"n_neighbors\")\nplt.legend()\n\nplt.show()","073039e2":"# here some Extra work it's take long time to running\n# plt.figure()\n# training_accuracy = []\n# test_accuracy = []\n# # try n_neighbors from 1 to 10\n# test_size= numpy.arange(0.1, 0.9,0.05)\n# knn = KNeighborsClassifier(n_neighbors = 3)\n# for i in test_size:\n    \n    \n#     test_score = []\n#     train_score=[]\n#     for j in range(1,1000):\n#         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = i,random_state=6)\n#         knn.fit(X_train, y_train)\n#         test_score.append(knn.score(X_test, y_test))\n#         train_score.append(knn.score(X_train, y_train))\n  \n\n       \n#     training_accuracy.append(np.mean(train_score))\n#     # record generalization accuracy\n#     test_accuracy.append(np.mean(test_score))\n\n# plt.plot(test_size,training_accuracy,label=\"training accuracy\")\n# plt.plot(test_size,test_accuracy,label=\"test accuracy\")\n# plt.ylabel(\"Accuracy\")\n# plt.xlabel(\"test_size\")\n# plt.legend()\n\n# plt.show()","92e66921":"# appropriate test size find \nplt.figure()\ntraining_accuracy = []\ntest_accuracy = []\n# try n_neighbors from 1 to 10\ntest_size= numpy.arange(0.1, 0.9,0.05)\nknn = KNeighborsClassifier(n_neighbors = 5)\nfor i in test_size:\n    \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = i)\n        knn.fit(X_train, y_train)\n        training_accuracy.append(knn.score(X_train,y_train))\n        test_accuracy.append(knn.score(X_test,y_test))\n  \n\n    \nplt.plot(test_size,training_accuracy,label=\"training accuracy\")\nplt.plot(test_size,test_accuracy,label=\"test accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"test_size\")\nplt.legend()\n\nplt.show()","f32f1b3d":"#split data into test data and train data\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.4)\n\nneigh = KNeighborsClassifier(n_neighbors=5)\n\n\nknn=neigh.fit(X_train,y_train)\n\nknn.score(X_test,y_test)","ed9dad10":"y_pred=knn.predict(X_test)","cf5d25dd":"#confusion matrix \ncf_matrix_n=confusion_matrix(y_test, y_pred)\ncf_matrix_n","2057c1a2":"#classification report\nprint(classification_report(y_test,y_pred))","fa109cfc":"from sklearn.metrics import roc_curve\nfpr, tpr, thresholds=roc_curve(y_test, y_pred)\nAUC=round(metrics.auc(fpr, tpr),2)\n\nplt.plot([0,1], [0,1], linestyle='--', label='No Skill')\nplt.plot(fpr, tpr, marker='.', label=f'knn AUC={AUC}')\n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\n# show the plot\nplt.show()\n","f62444b7":"'''feature selection method affect our model,AUC decrease as compare to the first one'''","118597f1":"from sklearn.linear_model import LogisticRegression","19f48a8f":"# Here we are using  both method for feature selection  means we takes union both feature union","67e465df":"f_feature=list(set(basic_f0).union(set(basic_f1))) #final feature  just name","11b8c28e":"f_feature","e99fc4ae":"# Now we need to use some domain knowlegde for selecting feature \n'''Group 1: {'Ease and convenient','Easy Payment option'}\n   Group 2 :{'Order Time','Time saving','Delay of delivery person picking up food','Maximum wait time'}'''\nf_feature=['Ease and convenient','Self Cooking','Maximum wait time','More restaurant choices',\n 'Unaffordable','More Offers and Discount','Good Food quality','Good Tracking system','Age','Bad past experience']","32defd73":"#select X,y \nX=fd_dlvry[f_feature]\ny=fd_dlvry['Output']","00245f69":"f_feature","eae9aeaa":"Lg=LogisticRegression()","3647408c":"#\nplt.figure()\ntraining_accuracy = []\ntest_accuracy = []\ntest_size= numpy.arange(0.1, 0.9,0.05)\n\nfor i in test_size:\n        t_s=[]\n        tn_=[]\n        for j in range(0,50):\n            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = i)\n            Lg.fit(X_train, y_train)\n            \n            t_s.append(Lg.score(X_test,y_test))\n            tn_.append(Lg.score(X_train,y_train))\n        training_accuracy.append(np.mean(t_s))\n        test_accuracy.append(np.mean(tn_))\n\n\n    \nplt.plot(test_size,training_accuracy,label=\"training accuracy\")\nplt.plot(test_size,test_accuracy,label=\"test accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"test_size\")\nplt.legend()\n\nplt.show()\n\n\n","40143ed7":"#data split into train and test size \nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)\nLg.fit(X_train,y_train)\nLg.score(X_test,y_test)","f279ec1a":"y_pred=Lg.predict(X_test)","2265df84":"cf_matrix=confusion_matrix(y_test, y_pred)\ncf_matrix","24ac47ce":"print(classification_report(y_test,y_pred))","5f52a5aa":"thresolds = [0.2,0.3,0.4,0.5,0.55,0.6,0.62,0.65,0.7,0.8]\nplt.figure(figsize=(10,6))\nfor i in thresolds:\n    y_pred_proba = (Lg.predict_proba(X_test)[:,1]>=i).astype(int)\n    fpr, tpr, m= metrics.roc_curve(y_test,  y_pred_proba)\n    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n    plt.plot(fpr,tpr,label=f\"data 1, auc={auc} thresold={i}\")\n    plt.legend(loc=4)\n\n\n\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.show()","988a0e96":"'''In Knn weighted avg f1-score is good and AUC is great but problems in Knn is not learning something\nknn just use more memory space for perform operation and mostly features are categorical so noise affect\non this algorithms. Here we can see if we set thresold 0.65 in Logistics regression we get better model  and here algorithms \nsomething learn \n'''","f6c8c393":"#select X,y \nX=fd_dlvry[f_feature]\ny=fd_dlvry['Output']","496f2907":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.35)\nLg=LogisticRegression()\nLg.fit(X_train, y_train)\nprint(\"Training score=\",Lg.score(X_train,y_train))\nprint(\"Test score=\",Lg.score(X_test,y_test))","291fd6fb":"y_pred_proba = (Lg.predict_proba(X_test)[:,1]>=0.65).astype(int)","88287cc2":"cf_matrix=confusion_matrix(y_test, y_pred_proba)\ncf_matrix","35e9a518":"print(classification_report(y_test,y_pred_proba))","3be6fb39":"<b>method 2: feature selection using sklearn <\/b>","096fccaa":"<b> Conclusion: <\/b>","f9958b45":"<b> Final Model <\/b>","a5a54279":"<b> thank you :) <\/b> ","7f56b8dd":"<b> Conclusion:<\/b>","63664fa6":"<h1> KNN Algorithm <\/h1>","a659aeee":"<b><h1> Logistics Regression<\/h1> <\/b>\n","300e7253":"<b> method 1: Feature selection Cram\u00e9r's V <\/b>"}}