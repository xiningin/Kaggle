{"cell_type":{"6153b491":"code","bc1c1b4e":"code","1d29bd92":"code","eac927af":"code","e3b2fbb3":"code","2635c19b":"code","f0967fcb":"code","f3b952de":"code","a0f36e0a":"code","b3cb9da5":"code","21905475":"code","68d05631":"code","dab88d12":"code","3bd1091e":"code","752b7174":"code","fdbb7af5":"code","560fab0d":"code","23aa5147":"code","3f4ae336":"code","7ddd6b8e":"code","6eb2361d":"code","c0976620":"markdown","fdf6b70b":"markdown","decbf26f":"markdown","6dd56766":"markdown","fb8952bb":"markdown","c1bf3b47":"markdown","3ce2a4e4":"markdown","c1bb833c":"markdown","8b4a0fb3":"markdown","d695221b":"markdown","cd86410b":"markdown","f7e50ee4":"markdown","5884877a":"markdown","a5ae34a5":"markdown"},"source":{"6153b491":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom yellowbrick.cluster import KElbowVisualizer, silhouette_visualizer\nfrom scipy.cluster.hierarchy import fcluster, linkage, dendrogram\nfrom sklearn.cluster import DBSCAN\nfrom sklearn import metrics","bc1c1b4e":"df = pd.read_csv(\"..\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv\")","1d29bd92":"df.info()","eac927af":"df.describe()","e3b2fbb3":"df=df.drop(['CustomerID'], axis=1)","2635c19b":"sns.pairplot(df, hue=\"Gender\");","f0967fcb":"df=pd.get_dummies(df, drop_first=True)\ndfs=StandardScaler().fit_transform(df)","f3b952de":"distortions=[]\nfor i in range (1,15):\n    km=KMeans(n_clusters= i,\n              n_init=5,  # run 5 times with different random inicial centroids\n              max_iter=500,  # max iteration by run\n              random_state=1)\n    km.fit(dfs)\n    distortions.append(km.inertia_)  # inertia = within-cluster sum-of-squares ","a0f36e0a":"plt.plot(range(1,15), distortions,marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Distortion')\nplt.show()","b3cb9da5":"# using Yellowbricks - distortion metric = sum of squared distances from each point to its assigned center\nvis=KElbowVisualizer(km, k=(1, 15))\nvis.fit(dfs)\nvis.show();","21905475":"km= KMeans(n_clusters= 5,\n          n_init=5, \n          max_iter= 500,\n          random_state=1)","68d05631":"df['cluster kmeans']= km.fit_predict(dfs)\ndf.groupby('cluster kmeans').agg(Age_mean=('Age', 'mean'),\n                                 AnIncome_mean=('Annual Income (k$)', 'mean'),\n                                 SpenScore_mean=('Spending Score (1-100)', 'mean'),\n                                 Gender=('Gender_Male', 'mean'),\n                                 Count=('cluster kmeans', 'count'))\n","dab88d12":"metrics.silhouette_score(dfs, df['cluster kmeans'])","3bd1091e":"vis = silhouette_visualizer(km, dfs)\nvis","752b7174":"df=df.drop(['cluster kmeans'], axis=1)","fdbb7af5":"dist_matrix = linkage(dfs, method='complete', metric='euclidean')\ndn=dendrogram(dist_matrix)\nplt.show()","560fab0d":"df['cluster hier'] = fcluster(dist_matrix,4, criterion='maxclust')\ndf.groupby('cluster hier').agg(Age_mean=('Age', 'mean'),\n                                 AnIncome_mean=('Annual Income (k$)', 'mean'),\n                                 SpenScore_mean=('Spending Score (1-100)', 'mean'),\n                                 Gender=('Gender_Male', 'mean'),\n                                 Count=('cluster hier', 'count'))","23aa5147":"metrics.silhouette_score(dfs, df['cluster hier'])","3f4ae336":"df=df.drop(['cluster hier'], axis=1)","7ddd6b8e":"db=DBSCAN(eps=0.8,\n         min_samples=3,\n         metric='euclidean')\ndf['cluster dbscan'] = db.fit_predict(dfs)\n\ndf.groupby('cluster dbscan').agg(Age_mean=('Age', 'mean'),\n                                 AnIncome_mean=('Annual Income (k$)', 'mean'),\n                                 SpenScore_mean=('Spending Score (1-100)', 'mean'),\n                                 Gender=('Gender_Male', 'mean'),\n                                 Count=('cluster dbscan', 'count'))","6eb2361d":"metrics.silhouette_score(dfs, df['cluster dbscan'])","c0976620":"## 2.1 - K-Means\n\nHow it works? Basically:\n* 1- inicial clusters centers (centroids) are randomly chosen\n* 2- the observations are assigned to the closest centroid, based in some distance measure\n* 3- the centroids are recalculated with the means of the observations that makes part of each cluster.\n* 2 and 3 happen repeatedly, with the goal of minimize the total within cluster variation, until have no changes or reach some tolerance.   \n\nWith K-means we have to pre-specify the number of clusters. Here, we gonna use the elbow method, that is, we gonna try some numbers of clusters (n) and observe the within clusters variation related to each n. When the sum of a cluster does not mean a considerable reduction of the within clusters variation we have a possible good choice of n (elbow on the graph).  ","fdf6b70b":"References:\n\n* Practical Guide to Cluster Analysis in R: Unsupervised Machine Learning by Alboukadel Kassambara  \n* Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems from Aurelien Geron\n* Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow from Sebastian Raschka e Vahid Mirjalili\n* Machine Learning with R by Brett Lantz\n* Hands-On Machine Learning with R by Bradley Boehmke & Brandon Greenwell","decbf26f":"# 2- Clustering","6dd56766":"# Clustering: review with python","fb8952bb":"# 2 - Knowing the dataset","c1bf3b47":"# 1 - Knowing the problem\nAccording to the documentation, we have here:\n> \"... some basic data about your customers like Customer ID, age, gender, annual income and spending score. Spending Score is something you assign to the customer based on your defined parameters like customer behavior and purchasing data.\"\n\nHere I intend to find groups of consumers based on their similarities.","3ce2a4e4":"## 2.0 - Preprocessing\nHere, after take care of the categorical column, I'm going to bring variables into the same scale using standardization.","c1bb833c":"Side Note: PAM is similar to K-means, but makes the centroids using the median (medoids), so it seems to be more robust to outliers. CLARA is also similar to K-means and seems like a good option to larger datasets. The dataset is splitted, PAM is aplied to subsets to choose medoids and the complete dataset is assigned to some cluster.","8b4a0fb3":"So, we have (besides 11 outliers) 4 groups here:\n\nA group of men mean age of 39 years with medium income and medium score.  \nA group of women mean age of 38 years with medium income and medium score.   \nA group of men mean age of 61 years with low income and very low score.  \nA group of women mean age of 44 years with high income and low score. ","d695221b":"So, we have 5 groups here:\n\nA group of men\/women mean age of 38 years with high income and low score.  \nA group of women mean age of 28 years with medium income and relative high score.  \nA group of women mean age of 48 years with relative low income and low  score.  \nA group of men mean age of 56 years with relative low income and low score.  \nA group of men mean age of 28 years with medium income and relative high score.  \n\nHow to know if this is a good clustering? Let's try sillhouette analysis. The silhouette plot shows, basically, how similar observations from a cluster are to observations from a neighbor cluster. The coefficient goes from -1 to 1 (well clustered).","cd86410b":"## 2.2 - Hierarquical\n\nWith this method, we don't have to specify the number of clusters and we can plot a dendrogram. \nHere I'm going to use the agglomerative aproach where closest single observations will be combined until form a single one cluster. Divisive aproach is kind like the oposit, a single cluster will be splitted.\nBut how measure the similarity between clusters? Some of the options are the complete linkage and single linkage. While complete linkage considers the distance between the most dissimilar observations, the single linkage considers the distance between the most similar observations.\n\nHow it works? Basically in case of agglomerative with complete linkage:\n\n* 1- a distance (similarity) matrix is calculated.\n* 2- single samples (inicially considered as clusters) will be merged based on distance between the most dissimilar samples. \n* 3- the matrix is updated\n* 4- the steps are repeated until remains one cluster with all observations.\n","f7e50ee4":"Until now, kmeans seems to be the best method. Remember he gave us two groups mean age of 28 (one male and another female) with medium income and relative high score, a group of women mean age of 48 with relative low income and low score, group of men mean age of 56 years with relative low income and low score, but also a group with males and females  mean age of 38 with relative high income and low score (which seems like an oportunity).","5884877a":"So, we have 4 groups here:\n\nA group of men\/women mean age of 33 years with relative high income and high score.  \nA group of men\/women mean age of 40 years with relative high income and very low score.  \nA group of men\/women mean age of 55 years with relative low income and low score.  \nA group of men\/women mean age of 27 years with relative low income and better score.  ","a5ae34a5":"## 2.3 - DBSCAN\nThis method builds clusters based on density. MinPts and eps are parameters to be estimated. MinPoints is the minimum neighbors within eps radius of neighborhood.\n\nHow it works? Basically:\n\n* after distance of each point to others being calculated, neighborhood is defined and points are classified in core (neighbors>=MinPoints), border (neighbors < MinPoints but is neighbor of some core point) or outlier.\n* clusters based on core points continue to be building.\n* definition of noise\/outliers points.\n\nWhile Kmeans and Hierarquical are good for finding spherical\/convex clusters in datasets with low noise and outliers,\nDBSCAN has some advantages:  \n\n* doesn't need the number of clusters to be pre-specified.\n* it can find differents shapes of clusters\n* it can find outliers (not all points have to be assign to a cluster)"}}