{"cell_type":{"c1d316f9":"code","e4ddc5e8":"code","9f630c57":"code","af33434e":"code","42258c95":"code","9b02fb0e":"code","9a505904":"code","02682727":"code","22939d6c":"code","be95e38f":"code","71fe201c":"code","9c536cc1":"code","f27c1e7f":"code","39fd083d":"code","d8065c3e":"code","4ad74a88":"code","54fdf100":"code","709ae361":"code","637a3db7":"code","2d4bfdb6":"code","06ce9f0d":"code","8bd1c807":"code","04d88ec0":"code","7ce02602":"code","bb375a8b":"code","6f0f9e75":"code","dd15ab7f":"code","9edd1e7e":"code","2fc14dff":"code","8a1fbcdb":"code","0ad6b42f":"code","3c155562":"code","c0cfdd28":"code","752c2977":"code","5937c6f0":"code","679ff4e4":"code","174d3506":"code","b0c723a8":"code","a940dcd0":"code","a1cd2b1d":"code","478b58d5":"code","3cb0b271":"code","7a252c83":"code","78cef08f":"code","d864c42f":"code","0fc83cca":"code","ca641bb6":"code","20c9d625":"code","b059d08a":"markdown","de41494d":"markdown","bd7736b8":"markdown","a17b2470":"markdown","900d6ef7":"markdown","41282938":"markdown","b8dc6107":"markdown","0280d259":"markdown","e43b3689":"markdown","b61dd034":"markdown","ba52cf76":"markdown","a182c35b":"markdown","f5373e00":"markdown","c5adb2de":"markdown","efb67893":"markdown"},"source":{"c1d316f9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","e4ddc5e8":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","9f630c57":"train.corr()","af33434e":"sns.heatmap(train.corr(), annot=True)","42258c95":"print(train.isnull().sum())\nprint(test.isnull().sum())","9b02fb0e":"# fill 'Fare' and 'Embarked'\n\nprint(train['Embarked'].value_counts())\ntrain[train['Embarked'].isnull()]\n\n# We can see that we can fill two null values with 'S'","9a505904":"train['Embarked'].fillna('S', inplace=True)\nprint(train['Embarked'].value_counts())\nprint(train['Embarked'].isnull().sum())","02682727":"print(test['Fare'].value_counts())\ntest[test['Fare'].isnull()]\n\n# we can fill with Fare(mean) in Pclass = 3","22939d6c":"test['Fare'].fillna(test.loc[(test['Pclass'] == 3),'Fare'].mean(), inplace=True)\nprint([test['Fare'].isnull().sum()])","be95e38f":"train['Age'].describe()","71fe201c":"train['Age'].value_counts()","9c536cc1":"train.groupby(['Survived','Sex'])[['Age']].agg([np.mean,'std','count'])\n\n# I want to fill Age values randomly, we can see from table that females who survived ages between (mean-std) (mean+std) \n# so we can fill null values randomly between two numbers ","f27c1e7f":"# To learn how many null values in these groups I am doing this\n\ntrain[\"Age\"] = train[\"Age\"].fillna(-0.5)\n\ncut_points = [-1,0,5,12,18,35,60,100]\nlabel_names = [\"Missing\",\"Infant\",\"Child\",\"Teenager\",\"Young Adult\",\"Adult\",\"Senior\"]\n\ntrain[\"Age_categories\"] = pd.cut(train[\"Age\"],cut_points,labels=label_names)\n\ntrain.groupby(['Survived','Sex','Age_categories'])[['Age']].agg([np.mean,'count'])","39fd083d":"rand_age1 = np.random.randint(25.046875 - 13.618591, 25.046875 + 13.618591, size = 17)\nrand_age2 = np.random.randint(31.618056 - 14.056019, 31.618056 + 14.056019, size = 108)\nrand_age3 = np.random.randint(28.847716 - 14.175073, 28.847716 + 14.175073, size = 36)\nrand_age4 = np.random.randint(27.276022 - 16.504803, 27.276022 + 16.504803, size = 16)\n\nrand_age1 = list(rand_age1)\nrand_age2 = list(rand_age2)\nrand_age3 = list(rand_age3)\nrand_age4 = list(rand_age4)","d8065c3e":"train.loc[(train['Sex'] == 'female') & (train['Survived'] == 0) & (train['Age'] == -0.5),'Age'] = rand_age1\ntrain.loc[(train['Sex'] == 'male') & (train['Survived'] == 0) & (train['Age'] == -0.5), 'Age'] = rand_age2\ntrain.loc[(train['Sex'] == 'female') & (train['Survived'] == 1) & (train['Age'] == -0.5),'Age'] = rand_age3\ntrain.loc[(train['Sex'] == 'male') & (train['Survived'] == 1) & (train['Age'] == -0.5), 'Age'] = rand_age4","4ad74a88":"test.groupby(['Sex'])[['Age']].agg([np.mean,'std','count'])","54fdf100":"test[\"Age\"] = test[\"Age\"].fillna(-0.5)\ncut_points = [-1,0,5,12,18,35,60,100]\nlabel_names = [\"Missing\",\"Infant\",\"Child\",\"Teenager\",\"Young Adult\",\"Adult\",\"Senior\"]\ntest[\"Age_categories\"] = pd.cut(test[\"Age\"],cut_points,labels=label_names)\ntest.groupby(['Sex','Age_categories'])[['Age']].agg([np.mean,'count'])","709ae361":"rand_age1 = np.random.randint(30.272362 - 15.428613, 30.272362 + 15.428613, size = 25)\nrand_age2 = np.random.randint(30.272732 - 13.389528, 30.272732 + 13.389528, size = 61)\n\nrand_age1 = list(rand_age1)\nrand_age2 = list(rand_age2)\n\ntest.loc[(test['Sex'] == 'female') & (test['Age'] == -0.5), 'Age'] = rand_age1\ntest.loc[(test['Sex'] == 'male') & (test['Age'] == -0.5), 'Age'] = rand_age2","637a3db7":"# now we can drop age_categories\n\ntrain.drop('Age_categories', axis=1, inplace=True)\ntest.drop('Age_categories', axis=1, inplace=True)","2d4bfdb6":"train[\"Cabin_type\"] = train[\"Cabin\"].str[0]\ntrain[\"Cabin_type\"] = train[\"Cabin_type\"].fillna(\"Unknown\")\n\ntest[\"Cabin_type\"] = test[\"Cabin\"].str[0]\ntest[\"Cabin_type\"] = test[\"Cabin_type\"].fillna(\"Unknown\")","06ce9f0d":"sns.barplot(x = 'Cabin_type',y ='Survived',data=train)","8bd1c807":"train.drop(['Cabin','Cabin_type'], axis=1, inplace=True)\ntest.drop(['Cabin','Cabin_type'], axis=1, inplace=True)","04d88ec0":"# finally end of fill null values\n\nprint(train.isnull().sum())\nprint(test.isnull().sum())","7ce02602":"train.head()","bb375a8b":"# I think Ticket feature is not necessary for us so we can drop them.\n\ntrain.drop('Ticket', axis=1, inplace=True)\ntest.drop('Ticket', axis=1, inplace=True)","6f0f9e75":"train[\"Title\"] = train[\"Name\"].str.extract(' ([A-Za-z]+)\\.', expand=False)\ntest[\"Title\"] = test[\"Name\"].str.extract(' ([A-Za-z]+)\\.', expand=False)","dd15ab7f":"train['Title'] = train['Title'].replace(['Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\ntrain['Title'] = train['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\ntrain['Title'] = train['Title'].replace('Mlle', 'Miss')\ntrain['Title'] = train['Title'].replace('Ms', 'Miss')\ntrain['Title'] = train['Title'].replace('Mme', 'Mrs')","9edd1e7e":"test['Title'] = test['Title'].replace(['Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\ntest['Title'] = test['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\ntest['Title'] = test['Title'].replace('Mlle', 'Miss')\ntest['Title'] = test['Title'].replace('Ms', 'Miss')\ntest['Title'] = test['Title'].replace('Mme', 'Mrs')","2fc14dff":"# now we can drop NAME\n\ntrain.drop('Name', axis=1, inplace=True)\ntest.drop('Name', axis=1, inplace=True)","8a1fbcdb":"train.head()","0ad6b42f":"train['Family_Size'] = train['SibSp'] + train['Parch'] + 1\ntest['Family_Size'] = test['SibSp'] + test['Parch'] + 1","3c155562":"# Now we dont need SibSp and Parch and drop them\n\ntrain.drop(['SibSp','Parch'], axis=1, inplace=True)\ntest.drop(['SibSp','Parch'], axis=1, inplace=True)","c0cfdd28":"train.head()","752c2977":"# We can apply one hot encoding method to ['Pclass','Sex','Embarked','Title'] \n\ntrain = pd.get_dummies(train, columns = ['Pclass','Embarked','Title'],prefix=['Pc','',''])\ntest = pd.get_dummies(test, columns = ['Pclass','Embarked','Title'],prefix=['Pc','',''])","5937c6f0":"from sklearn import preprocessing\n\nlbe = preprocessing.LabelEncoder()\ntrain[\"Sex\"] = lbe.fit_transform(train[\"Sex\"])\ntest[\"Sex\"] = lbe.fit_transform(test[\"Sex\"])","679ff4e4":"train.head()","174d3506":"test.head()","b0c723a8":"# we can see there is no ROYAL column in test data so we have to add \n\ntest['_Royal'] = 0","a940dcd0":"test.head()","a1cd2b1d":"import warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import VotingClassifier","478b58d5":"def base_models(df):\n    \n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import accuracy_score\n    \n    X = df.drop(['Survived', 'PassengerId'], axis=1)\n    Y = df[\"Survived\"]\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, Y, \n                                                    test_size = 0.20, \n                                                    random_state = 42)\n    \n    #results = []\n    \n    names = [\"LogisticRegression\",\"GaussianNB\",\"KNN\",\"LinearSVC\",\"SVC\",\n             \"CART\",\"RF\",\"GBM\",\"XGBoost\",\"LightGBM\",\"CatBoost\"]\n    \n    \n    classifiers = [LogisticRegression(),GaussianNB(), KNeighborsClassifier(),LinearSVC(),SVC(),\n                  DecisionTreeClassifier(),RandomForestClassifier(), GradientBoostingClassifier(),\n                  XGBClassifier(), LGBMClassifier(), CatBoostClassifier(verbose = False)]\n    \n    \n    for name, clf in zip(names, classifiers):\n\n        model = clf.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        acc = accuracy_score(y_test, y_pred)\n        msg = \"%s: %f\" % (name, acc)\n        print(msg)","3cb0b271":"base_models(train)","7a252c83":"def base_models_cv(df):\n    \n    from sklearn.model_selection import cross_val_score, KFold \n    \n    X = df.drop(['Survived', 'PassengerId'], axis=1)\n    Y = df[\"Survived\"]\n    \n    results = []\n    A = []\n    \n    names = [\"LogisticRegression\",\"GaussianNB\",\"KNN\",\"LinearSVC\",\"SVC\",\n             \"CART\",\"RF\",\"GBM\",\"XGBoost\",\"LightGBM\",\"CatBoost\"]\n    \n    \n    classifiers = [LogisticRegression(),GaussianNB(), KNeighborsClassifier(),LinearSVC(),SVC(),\n                  DecisionTreeClassifier(),RandomForestClassifier(), GradientBoostingClassifier(),\n                  XGBClassifier(), LGBMClassifier(), CatBoostClassifier(verbose = False)]\n    \n    \n    for name, clf in zip(names, classifiers):\n        \n        kfold = KFold(n_splits=10, random_state=1001)\n        cv_results = cross_val_score(clf, X, Y, cv = kfold, scoring = \"accuracy\")\n        results.append(cv_results)\n        A.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean() , cv_results.std())\n        print(msg)","78cef08f":"base_models_cv(train)","d864c42f":"X = train.drop(['Survived', 'PassengerId'], axis=1)\ny = train[\"Survived\"]\n    \nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size = 0.20, \n                                                    random_state = 42)","0fc83cca":"models = [LogisticRegression(),\n          GaussianNB(), \n          KNeighborsClassifier(),\n          SVC(probability=True),\n          DecisionTreeClassifier(),\n          RandomForestClassifier(),\n          GradientBoostingClassifier(),\n          XGBClassifier(), \n          LGBMClassifier(),\n          CatBoostClassifier(verbose = False)]\n\nnames = [\"LogisticRegression\",\n         \"GaussianNB\",\n         \"KNeighborsClassifier\",\n         \"SVC\",\n         \"DecisionTreeClassifier\",\n         \"RandomForestClassifier\",\n         \"GradientBoostingClassifier\",\n         \"XGBClassifier\",\n         \"LGBMClassifier\",\n         \"CatBoostClassifier\"]\n    \nlogreg_params = {\"C\":np.logspace(-1, 1, 10),\n                 \"penalty\": [\"l1\",\"l2\"],\n                 \"solver\":['newton-cg','lbfgs','liblinear','sag','saga'],\n                 \"max_iter\":[1000]}\n    \nNB_params = {'var_smoothing': np.logspace(0,-9, num=100)}\n\nknn_params = {\"n_neighbors\":np.linspace(1,19,10, dtype = int).tolist(),\n                 \"weights\":[\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]}\n\nsvc_params = {\"kernel\":[\"rbf\"],\n                 \"gamma\": [0.001, 0.01, 0.1, 1, 5, 10 ,50 ,100],\n                 \"C\": [1,10,50,100,200,300,1000]}\n\ndtree_params = {\"min_samples_split\" : range(10,500,20),\n                    \"max_depth\": range(1,20,2)}\n\nrf_params = {\"max_features\": [\"log2\",\"Auto\",\"None\"],\n                 \"min_samples_split\":[2,3,5],\n                 \"min_samples_leaf\":[1,3,5],\n                 \"bootstrap\":[True,False],\n                 \"n_estimators\":[50,100,150],\n                 \"criterion\":[\"gini\",\"entropy\"]}\n\ngbm_params = {\"learning_rate\" : [0.001, 0.01, 0.1, 0.05],\n                  \"n_estimators\": [100,500,100],\n                  \"max_depth\": [3,5,10],\n                  \"min_samples_split\": [2,5,10]}\n\nxgb_params = {'n_estimators': [50, 100, 200],\n                 'subsample': [ 0.6, 0.8, 1.0],\n                 'max_depth': [1,2,3,4],\n                 'learning_rate': [0.1,0.2, 0.3, 0.4, 0.5],\n                 \"min_samples_split\": [1,2,4,6]}\n\nlgbm_params = {'n_estimators': [100, 500, 1000, 2000],\n               'subsample': [0.6, 0.8, 1.0],\n               'max_depth': [3, 4, 5,6],\n               'learning_rate': [0.1,0.01,0.02,0.05],\n               \"min_child_samples\": [5,10,20]}\n    \ncatb_params = {'depth':[2, 3, 4],\n                   'loss_function': ['Logloss', 'CrossEntropy'],\n                   'l2_leaf_reg':np.arange(2,31)}\n\nclassifier_params = [logreg_params, NB_params, knn_params, svc_params, dtree_params,\n                         rf_params, gbm_params, xgb_params, lgbm_params, catb_params]","ca641bb6":"cv_result = {}\nbest_estimators = {}\nbest_params = {}\n    \nfor name, model,classifier_param in zip(names, models,classifier_params):\n    clf = GridSearchCV(model, param_grid=classifier_param, cv =10, scoring = \"accuracy\", n_jobs = -1,verbose = False)\n    clf.fit(X_train,y_train)\n    cv_result[name]=clf.best_score_\n    best_estimators[name]=clf.best_estimator_\n    best_params[name]=clf.best_params_\n    print(name,'cross validation accuracy : %.3f'%cv_result[name])\n\naccuracies={}\n\nprint('Validation accuracies of the tuned models for the train data:', end = \"\\n\\n\")\n    \nfor name, model_tuned in zip(best_estimators.keys(),best_estimators.values()):\n    y_pred =  model_tuned.fit(X_train,y_train).predict(X_test)\n    accuracy=accuracy_score(y_pred, y_test)\n    print(name,':', \"%.3f\" %accuracy)\n    accuracies[name]=accuracy\nsorted_accuracies=sorted(accuracies, reverse=True, key= lambda k:accuracies[k])\n    \nprint(sorted_accuracies)\n    \nfor i in list(best_estimators):\n    if i == sorted_accuracies[0]:\n        model_tuned = best_estimators[sorted_accuracies[0]]\n        \npredictions = model_tuned.fit(X_train,y_train).predict(test.drop('PassengerId', axis=1))","20c9d625":"ids = test['PassengerId']\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions.astype(int)})\noutput.to_csv('submission.csv', index=False)","b059d08a":"### After looking plot I am thinking that 'Cabin' feature is not important to predict Survived I want to drop it","de41494d":"We can see that 'Age', 'Cabin', 'Fare' and 'Embarked' features have null values,\n- It is easy to fill Fare and Embarked because there are only 3 null values\n- I think that Age is important feature for predict so we have to fill it logically\n- I am not sure Cabin feature is important or not and there are many null values so we can explore it.","bd7736b8":"## Finally I select to use GradientBoostingClassifier()","a17b2470":"## This dataset is not big so to split train and validation set is not good solution, to aplly cross validation is necessary but to see difference I will do both of them","900d6ef7":"### 1) split data","41282938":"we can creat a new feature about family size because SibSp and Parch feature are about same meaning so if we merge them we can creat family size feature","b8dc6107":"## Now we have to focus categorical features because for ML algorithms we can not use it, Pclass, Sex, Embarked, Title\n\n## There are two options for categorical values to numerical, one hot encoding and label encoding, one hot is good for nominal variables and label is good for ordinal variables\n\n\n## All of them is nominal here so we can apply one hot encoding but sex has two categori so we have to apply label encoder for it","0280d259":"### 2) cross validation","e43b3689":"## After filling AGE values we can focus CABIN","b61dd034":"**MISSING VALUES**","ba52cf76":"We can see that 'Survived' has high correlation with 'Pclass'(0.34) and 'Fare'(0.26) categories...\n\nWe can concluded that;\n\n- the more money passengers pay, the higher probability of survival they have\n- the lower class passengers belongs to, the lowest probability of survival they have","a182c35b":"We can create new fature from NAME","f5373e00":"## Now we can check the other columns","c5adb2de":"# Now we can focus on null values in 'Age' feature","efb67893":"**MACHINE LEARNING**"}}