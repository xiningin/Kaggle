{"cell_type":{"4dfc928f":"code","1b15490a":"code","019d7471":"code","8d7d64f0":"code","1c13206a":"code","86eb5fbc":"code","9758be0f":"code","10ebff47":"code","f0cffd25":"code","d68d84be":"code","2705f905":"code","02b2a100":"code","de355710":"code","8603870d":"code","ec1c8d8d":"code","841e8812":"code","4cbe68d8":"code","d0245183":"code","a4e94496":"code","04f41a62":"code","6235656a":"code","ec578dcd":"code","ab3410e1":"code","386cf256":"code","a7fa50d8":"code","25dfc563":"code","c09ab63c":"code","82034222":"code","a54b3d68":"code","be5c0b36":"code","9510f1df":"code","3a682bca":"code","859e5fe1":"code","c93d02d8":"code","2db50f6c":"code","c1771747":"code","c424c6f7":"code","a7800f12":"code","d55c1d95":"code","fc39ccbb":"code","de69b7b1":"code","21661834":"code","33a86e84":"code","b46a9642":"code","e64a4219":"code","fe08e657":"code","1b5e1405":"code","2a1ccc4b":"markdown","c0c1590f":"markdown","af8e6240":"markdown","7fe449c2":"markdown","867f8dd3":"markdown","5ee5a756":"markdown","497c967d":"markdown","0fa00ca2":"markdown","a3fbb1fc":"markdown","c98d9ca6":"markdown","d87b682f":"markdown","14d3382b":"markdown","bcfacbae":"markdown","7c00a055":"markdown","bf73056d":"markdown","b3d4db1e":"markdown","5052571f":"markdown","4fde2b28":"markdown","bdca4440":"markdown","1f494963":"markdown","8f32c40c":"markdown","13721ccb":"markdown","b8952d9e":"markdown","dd3b1761":"markdown","43f9507e":"markdown","9db61388":"markdown","bbb6bda3":"markdown","94d86d45":"markdown","e3f86b5f":"markdown","4fbe3c79":"markdown","ff50683e":"markdown","891400e4":"markdown","630150aa":"markdown","52b95b9b":"markdown","467215dc":"markdown","82165a05":"markdown","8e9296c5":"markdown"},"source":{"4dfc928f":"%%capture\n!pip install datasets==1.4.1\n!pip install soundfile\n!pip install jiwer","1b15490a":"from datasets import load_dataset, load_metric\n\ntimit = load_dataset(\"timit_asr\")","019d7471":"timit","8d7d64f0":"timit = timit.remove_columns([\"phonetic_detail\", \"word_detail\", \"dialect_region\", \"id\", \"sentence_type\", \"speaker_id\"])","1c13206a":"from datasets import ClassLabel\nimport random\nimport pandas as pd\nfrom IPython.display import display, HTML\n\ndef show_random_elements(dataset, num_examples=10):\n    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n    picks = []\n    for _ in range(num_examples):\n        pick = random.randint(0, len(dataset)-1)\n        while pick in picks:\n            pick = random.randint(0, len(dataset)-1)\n        picks.append(pick)\n    \n    df = pd.DataFrame(dataset[picks])\n    display(HTML(df.to_html()))","86eb5fbc":"show_random_elements(timit[\"train\"].remove_columns([\"file\"]), num_examples=5)","9758be0f":"import re\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"]'\n\ndef remove_special_characters(batch):\n    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower() + \" \"\n    return batch","10ebff47":"timit = timit.map(remove_special_characters)","f0cffd25":"show_random_elements(timit[\"train\"].remove_columns([\"file\"]))","d68d84be":"def extract_all_chars(batch):\n  all_text = \" \".join(batch[\"text\"])\n  vocab = list(set(all_text))\n  return {\"vocab\": [vocab], \"all_text\": [all_text]}","2705f905":"vocabs = timit.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=timit.column_names[\"train\"])","02b2a100":"vocab_list = list(set(vocabs[\"train\"][\"vocab\"][0]) | set(vocabs[\"test\"][\"vocab\"][0]))","de355710":"vocab_dict = {v: k for k, v in enumerate(vocab_list)}\nvocab_dict","8603870d":"vocab_dict[\"|\"] = vocab_dict[\" \"]\ndel vocab_dict[\" \"]","ec1c8d8d":"vocab_dict[\"[UNK]\"] = len(vocab_dict)\nvocab_dict[\"[PAD]\"] = len(vocab_dict)\nlen(vocab_dict)","841e8812":"import json\nwith open('vocab.json', 'w') as vocab_file:\n    json.dump(vocab_dict, vocab_file)","4cbe68d8":"from transformers import Wav2Vec2CTCTokenizer\n\ntokenizer = Wav2Vec2CTCTokenizer(\".\/vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")","d0245183":"tokenizer.save_pretrained(\".\/wav2vec2-base-mine\/\")","a4e94496":"from transformers import Wav2Vec2FeatureExtractor\n\nfeature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)\n\nfeature_extractor.save_pretrained(\".\/wav2vec2-base-mine\/\")","04f41a62":"from transformers import Wav2Vec2Processor\n\nprocessor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n\nprocessor.save_pretrained(\".\/wav2vec2-base-mine\/\")\n","6235656a":"timit[\"train\"][0]","ec578dcd":"import soundfile as sf\n\ndef speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = sf.read(batch[\"file\"])\n    batch[\"speech\"] = speech_array\n    batch[\"sampling_rate\"] = sampling_rate\n    batch[\"target_text\"] = batch[\"text\"]\n    return batch","ab3410e1":"timit = timit.map(speech_file_to_array_fn, remove_columns=timit.column_names[\"train\"], num_proc=4)","386cf256":"import IPython.display as ipd\nimport numpy as np\nimport random\n\nrand_int = random.randint(0, len(timit[\"train\"]))\n\nipd.Audio(data=np.asarray(timit[\"train\"][rand_int][\"speech\"]), autoplay=True, rate=16000)","a7fa50d8":"rand_int = random.randint(0, len(timit[\"train\"]))\n\nprint(\"Target text:\", timit[\"train\"][rand_int][\"target_text\"])\nprint(\"Input array shape:\", np.asarray(timit[\"train\"][rand_int][\"speech\"]).shape)\nprint(\"Sampling rate:\", timit[\"train\"][rand_int][\"sampling_rate\"])","25dfc563":"def prepare_dataset(batch):\n    # check that all files have the correct sampling rate\n    assert (\n        len(set(batch[\"sampling_rate\"])) == 1\n    ), f\"Make sure all inputs have the same sampling rate of {processor.feature_extractor.sampling_rate}.\"\n\n    batch[\"input_values\"] = processor(batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"][0]).input_values\n    \n    with processor.as_target_processor():\n        batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\n    return batch","c09ab63c":"timit_prepared = timit.map(prepare_dataset, remove_columns=timit.column_names[\"train\"], batch_size=8, num_proc=4, batched=True)","82034222":"import torch\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional, Union\n\n@dataclass\nclass DataCollatorCTCWithPadding:\n    \"\"\"\n    Data collator that will dynamically pad the inputs received.\n    Args:\n        processor (:class:`~transformers.Wav2Vec2Processor`)\n            The processor used for proccessing the data.\n        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n            among:\n            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n              sequence if provided).\n            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n              maximum acceptable input length for the model if that argument is not provided.\n            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n              different lengths).\n        max_length (:obj:`int`, `optional`):\n            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n        max_length_labels (:obj:`int`, `optional`):\n            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n        pad_to_multiple_of (:obj:`int`, `optional`):\n            If set will pad the sequence to a multiple of the provided value.\n            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n            7.5 (Volta).\n    \"\"\"\n\n    processor: Wav2Vec2Processor\n    padding: Union[bool, str] = True\n    max_length: Optional[int] = None\n    max_length_labels: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    pad_to_multiple_of_labels: Optional[int] = None\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # split inputs and labels since they have to be of different lenghts and need\n        # different padding methods\n        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n\n        batch = self.processor.pad(\n            input_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=\"pt\",\n        )\n        with self.processor.as_target_processor():\n            labels_batch = self.processor.pad(\n                label_features,\n                padding=self.padding,\n                max_length=self.max_length_labels,\n                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n                return_tensors=\"pt\",\n            )\n\n        # replace padding with -100 to ignore loss correctly\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n\n        batch[\"labels\"] = labels\n\n        return batch","a54b3d68":"data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)","be5c0b36":"wer_metric = load_metric(\"wer\")","9510f1df":"def compute_metrics(pred):\n    pred_logits = pred.predictions\n    pred_ids = np.argmax(pred_logits, axis=-1)\n\n    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n\n    pred_str = processor.batch_decode(pred_ids)\n    # we do not want to group tokens when computing the metrics\n    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n\n    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n\n    return {\"wer\": wer}","3a682bca":"from transformers import Wav2Vec2ForCTC\n\nmodel = Wav2Vec2ForCTC.from_pretrained(\n    \"facebook\/wav2vec2-base\", \n    gradient_checkpointing=True, \n    ctc_loss_reduction=\"mean\", \n    pad_token_id=processor.tokenizer.pad_token_id,\n)","859e5fe1":"model.freeze_feature_extractor()","c93d02d8":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n  # output_dir=\"\/content\/gdrive\/MyDrive\/wav2vec2-base-timit-demo\",\n  output_dir=\".\/wav2vec2-base-mine\/\",\n  group_by_length=True,\n  per_device_train_batch_size=32,\n  evaluation_strategy=\"steps\",\n  num_train_epochs=2,\n  fp16=True,\n  save_steps=250,\n  eval_steps=500,\n  logging_steps=100,\n  learning_rate=1e-4,\n  weight_decay=0.005,\n  warmup_steps=100,\n  save_total_limit=2,\n)","2db50f6c":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    data_collator=data_collator,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=timit_prepared[\"train\"],\n    eval_dataset=timit_prepared[\"test\"],\n    tokenizer=processor.feature_extractor,\n)","c1771747":"trainer.train()","c424c6f7":"!ls \/kaggle\/working\/wav2vec2-base-mine","a7800f12":"!ls \/kaggle\/working\/wav2vec2-base-mine\/checkpoint-250","d55c1d95":"processor = Wav2Vec2Processor.from_pretrained(\".\/wav2vec2-base-mine\/\")","fc39ccbb":"model = Wav2Vec2ForCTC.from_pretrained(\".\/wav2vec2-base-mine\/checkpoint-250\")","de69b7b1":"def map_to_result(batch):\n  model.to(\"cuda\")\n  input_values = processor(\n      batch[\"speech\"], \n      sampling_rate=batch[\"sampling_rate\"], \n      return_tensors=\"pt\"\n  ).input_values.to(\"cuda\")\n\n  with torch.no_grad():\n    logits = model(input_values).logits\n\n  pred_ids = torch.argmax(logits, dim=-1)\n  batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n  \n  return batch","21661834":"results = timit[\"test\"].map(map_to_result)","33a86e84":"print(\"Test WER: {:.3f}\".format(wer_metric.compute(predictions=results[\"pred_str\"], references=results[\"target_text\"])))","b46a9642":"show_random_elements(results.remove_columns([\"speech\", \"sampling_rate\"]))","e64a4219":"model.to(\"cuda\")\ninput_values = processor(timit[\"test\"][0][\"speech\"], sampling_rate=timit[\"test\"][0][\"sampling_rate\"], return_tensors=\"pt\").input_values.to(\"cuda\")\n\nwith torch.no_grad():\n  logits = model(input_values).logits\n\npred_ids = torch.argmax(logits, dim=-1)\n\n# convert ids to tokens\n\" \".join(processor.tokenizer.convert_ids_to_tokens(pred_ids[0].tolist()))","fe08e657":"input_values = processor(timit[\"test\"][1][\"speech\"], sampling_rate=timit[\"test\"][1][\"sampling_rate\"], return_tensors=\"pt\").input_values.to(\"cuda\")\n\nwith torch.no_grad():\n  logits = model(input_values).logits\n\npred_ids = torch.argmax(logits, dim=-1)\n\n# convert ids to tokens\n\" \".join(processor.tokenizer.convert_ids_to_tokens(pred_ids[0].tolist()))","1b5e1405":"input_values = processor(timit[\"test\"][2][\"speech\"], sampling_rate=timit[\"test\"][2][\"sampling_rate\"], return_tensors=\"pt\").input_values.to(\"cuda\")\n\nwith torch.no_grad():\n  logits = model(input_values).logits\n\npred_ids = torch.argmax(logits, dim=-1)\n\n# convert ids to tokens\n\" \".join(processor.tokenizer.convert_ids_to_tokens(pred_ids[0].tolist()))","2a1ccc4b":"### Training","c0c1590f":"we will only consider the transcribed text for fine-tuning.\n\n","af8e6240":"It can be heard, that the speakers change along with their speaking rate, accent, etc. Overall, the recordings sound relatively clear though, which is to be expected from a read speech corpus.\n\nLet's do a final check that the data is correctly prepared, but printing the shape of the speech input, its transcription, and the corresponding sampling rate.\n\n**Note**: *You can click the following cell a couple of times to verify multiple samples.*","7fe449c2":"Let's start by loading the dataset and taking a look at its structure.","867f8dd3":"Next, we will create the feature extractor.","5ee5a756":"Training will take between 90 and 180 minutes depending on the GPU allocated to this notebook. While the trained model yields satisfying results on *Timit*'s test data, it is by no means an optimally fine-tuned model. The purpose of this notebook is to demonstrate how Wav2Vec2's [base](https:\/\/huggingface.co\/facebook\/wav2vec2-base), [large](https:\/\/huggingface.co\/facebook\/wav2vec2-large), and [large-lv60](https:\/\/huggingface.co\/facebook\/wav2vec2-large-lv60) checkpoints can be fine-tuned on any English dataset.\n\n","497c967d":"## Prepare Data, Tokenizer, Feature Extractor","0fa00ca2":"\n\nWe will give the fine-tuned model the name `\"wav2vec2-base-timit-demo\"`.","a3fbb1fc":"The model will return a sequence of logit vectors:\n$\\mathbf{y}_1, \\ldots, \\mathbf{y}_m$ with $\\mathbf{y}_1 = f_{\\theta}(x_1, \\ldots, x_n)[0]$ and $n >> m$.\n\nA logit vector $\\mathbf{y}_1$ contains the log-odds for each word in the vocabulary we defined earlier, thus $\\text{len}(\\mathbf{y}_i) =$ `config.vocab_size`. We are interested in the most likely prediction of the model and thus take the `argmax(...)` of the logits. Also, we transform the encoded labels back to the original string by replacing `-100` with the `pad_token_id` and decoding the ids while making sure that consecutive tokens are **not** grouped to the same token in CTC style ${}^1$.","c98d9ca6":"Let's write a short function to display some random samples of the dataset and run it a couple of times to get a feeling for the transcriptions.","d87b682f":"We can see that the transcriptions contain some special characters, such as `,.?!;:`. Without a language model, it is much harder to classify speech chunks to such special characters because they don't really correspond to a characteristic sound unit. *E.g.*, the letter `\"s\"` has a more or less clear sound, whereas the special character `\".\"` does not.\nAlso in order to understand the meaning of a speech signal, it is usually not necessary to include special characters in the transcription.\n\nIn addition, we normalize the text to only have lower case letters and append a word separator token at the end.","14d3382b":"\n\n---\n\n${}^1$ To allow models to become independent of the speaker rate, in CTC, consecutive tokens that are identical are simply grouped as a single token. However, the encoded labels should not be grouped when decoding since they don't correspond to the predicted tokens of the model, which is why the `group_tokens=False` parameter has to be passed. If we wouldn't pass this parameter a word like `\"hello\"` would incorrectly be encoded, and decoded as `\"helo\"`.\n\n${}^2$ The blank token allows the model to predict a word, such as `\"hello\"` by forcing it to insert the blank token between the two l's. A CTC-conform prediction of `\"hello\"` of our model would be `[PAD] [PAD] \"h\" \"e\" \"e\" \"l\" \"l\" [PAD] \"l\" \"o\" \"o\" [PAD]`.","bcfacbae":"Let's compute the overall WER now.","7c00a055":"Let's now save the vocabulary as a json file.","bf73056d":"## Training & Evaluation\n\n\n- Define a data collator. In contrast to most NLP models, Wav2Vec2 has a much larger input length than output length. *E.g.*, a sample of input length 50000 has an output length of no more than 100. Given the large input sizes, it is much more efficient to pad the training batches dynamically meaning that all training samples should only be padded to the longest sample in their batch and not the overall longest sample. Therefore, fine-tuning Wav2Vec2 requires a special padding data collator, which we will define below\n\n- Evaluation metric. During training, the model should be evaluated on the word error rate. We should define a `compute_metrics` function accordingly\n\n- Load a pretrained checkpoint. We need to load a pretrained checkpoint and configure it correctly for training.\n\n- Define the training configuration.\n\nAfter having fine-tuned the model, we will correctly evaluate it on the test data and verify that it has indeed learned to correctly transcribe speech.","b3d4db1e":"Finally, we can process the dataset to the format expected by the model for training. We will again make use of the `map(...)` function.\n\nFirst, we check that all data samples have the same sampling rate (of 16kHz).\nSecond, we extract the `input_values` from the loaded audio file. In our case, this includes only normalization, but for other speech models, this step could correspond to extracting, *e.g.* [Log-Mel features](https:\/\/en.wikipedia.org\/wiki\/Mel-frequency_cepstrum). \nThird, we encode the transcriptions to label ids.\n\n**Note**: This mapping function is a good example of how the `Wav2Vec2Processor` class should be used. In \"normal\" context, calling `processor(...)` is redirected to `Wav2Vec2FeatureExtractor`'s call method. When wrapping the processor into the `as_target_processor` context, however, the same method is redirected to `Wav2Vec2CTCTokenizer`'s call method.\nFor more information please check the [docs](https:\/\/huggingface.co\/transformers\/master\/model_doc\/wav2vec2.html#transformers.Wav2Vec2Processor.__call__).","5052571f":"# **video caption using WavetoVec 2.0**\n\nhttps:\/\/www.kaggle.com\/ajax0564\/video-caption-using-wavetovec-2-0","4fde2b28":"\n\n Take a look at [this](https:\/\/distill.pub\/2017\/ctc) very nice blog post to better understand CTC.","bdca4440":"Now, we can load the pretrained `Wav2Vec2` checkpoint. The tokenizer's `pad_token_id` must be to define the model's `pad_token_id` or in the case of `Wav2Vec2ForCTC` also CTC's *blank token* ${}^2$. To save GPU memory, we enable PyTorch's [gradient checkpointing](https:\/\/pytorch.org\/docs\/stable\/checkpoint.html) and also set the loss reduction to \"*mean*\".","1f494963":"Next, the evaluation metric is defined. As mentioned earlier, the \npredominant metric in ASR is the word error rate (WER), hence we will use it in this notebook as well.","8f32c40c":"In a final step, we define all parameters related to training. \nTo give more explanation on some of the parameters:\n- `group_by_length` makes training more efficient by grouping training samples of similar input length into one batch. This can significantly speed up training time by heavily reducing the overall number of useless padding tokens that are passed through the model\n- `learning_rate` and `weight_decay` were heuristically tuned until fine-tuning has become stable. Note that those parameters strongly depend on the Timit dataset and might be suboptimal for other speech datasets.\n\nFor more explanations on other parameters, one can take a look at the [docs](https:\/\/huggingface.co\/transformers\/master\/main_classes\/trainer.html?highlight=trainer#trainingarguments).\n\n**Note**: If one wants to save the trained models in his\/her google drive the commented-out `output_dir` can be used instead.","13721ccb":"Alright, the audio file is saved in the `.WAV` format. There are a couple of python-based libraries to read and process audio files, such as [librosa](https:\/\/github.com\/librosa\/librosa), [soundfile](https:\/\/github.com\/bastibe\/python-soundfile), and [audioread](https:\/\/github.com\/beetbox\/audioread). \n\n`librosa` seems to be the most active and prominent library, but since it depends on `soundfile` for loading of audio files, we will just use `soundfile` directly in this notebook.\n\nAn audio file usually stores both its values and the sampling rate with which the speech signal was digitalized. We want to store both in the dataset and write a `map(...)` function accordingly.","b8952d9e":"The first component of Wav2Vec2 consists of a stack of CNN layers that are used to extract acoustically meaningful - but contextually independent - features from the raw speech signal. This part of the model has already been sufficiently trained during pretrainind and as stated in the [paper](https:\/\/arxiv.org\/abs\/2006.11477) does not need to be fine-tuned anymore. \nThus, we can set the `requires_grad` to `False` for all parameters of the *feature extraction* part.","dd3b1761":"\n\nIn CTC, it is common to classify speech chunks into letters, so we will do the same here. \nLet's extract all distinct letters of the training and test data and build our vocabulary from this set of letters.\n\nWe write a mapping function that concatenates all transcriptions into one long transcription and then transforms the string into a set of chars. \nIt is important to pass the argument `batched=True` to the `map(...)` function so that the mapping function has access to all transcriptions at once.","43f9507e":"Alright! The transcriptions look very clean and the language seems to correspond more to written text than dialogue. This makes sense taking into account that [Timit](https:\/\/huggingface.co\/datasets\/timit_asr) is a read speech corpus.","9db61388":"Now, all instances can be passed to Trainer and we are ready to start training!","bbb6bda3":"The final WER should be below 0.3 which is reasonable given that state-of-the-art phoneme error rates (PER) are just below 0.1 (see [leaderboard](https:\/\/paperswithcode.com\/sota\/speech-recognition-on-timit)) and that WER is usually worse than PER.","94d86d45":"A Wav2Vec2 feature extractor object requires the following parameters to be instantiated:\n\n- `feature_size`: Speech models take a sequence of feature vectors as an input. While the length of this sequence obviously varies, the feature size should not. In the case of Wav2Vec2, the feature size is 1 because the model was trained on the raw speech signal ${}^2$.\n- `sampling_rate`: The sampling rate at which the model is trained on.\n- `padding_value`: For batched inference, shorter inputs need to be padded with a specific value\n- `do_normalize`: Whether the input should be *zero-mean-unit-variance* normalized or not. Usually, speech models perform better when normalizing the input\n- `return_attention_mask`: Whether the model should make use of an `attention_mask` for batched inference. In general, models should **always** make use of the `attention_mask` to mask padded tokens. However, due to a very specific design choice of `Wav2Vec2`'s \"base\" checkpoint, better results are achieved when using no `attention_mask`. This is **not** recommended for other speech models. For more information, one can take a look at [this](https:\/\/github.com\/pytorch\/fairseq\/issues\/3227) issue. **Important** If you want to use this notebook to fine-tune [large-lv60](https:\/\/huggingface.co\/facebook\/wav2vec2-large-lv60), this parameter should be set to `True`.","e3f86b5f":"In a final step, we use the json file to instantiate an object of the `Wav2Vec2CTCTokenizer` class.","4fbe3c79":"Great, Wav2Vec2's feature extraction pipeline is thereby fully defined!\n\nTo make the usage of Wav2Vec2 as user-friendly as possible, the feature extractor and tokenizer are *wrapped* into a single `Wav2Vec2Processor` class so that one only needs a `model` and `processor` object.","ff50683e":"Now, we create the union of all distinct letters in the training dataset and test dataset and convert the resulting list into an enumerated dictionary.","891400e4":"# ****For better result train the model atleast 50 epochs**","630150aa":"Cool, we see that all letters of the alphabet occur in the dataset (which is not really surprising) and we also extracted the special characters `\" \"` and `'`. Note that we did not exclude those special characters because: \n\n- The model has to learn to predict when a word finished or else the model prediction would always be a sequence of chars which would make it impossible to separate words from each other.\n- In English, we need to keep the `'` character to differentiate between words, *e.g.*, `\"it's\"` and `\"its\"` which have very different meanings.","52b95b9b":"Cool, now our vocabulary is complete and consists of 30 tokens, which means that the linear layer that we will add on top of the pretrained Wav2Vec2 checkpoint will have an output dimension of 30.","467215dc":"### Create Wav2Vec2 Feature Extractor","82165a05":"Now, we will make use of the `map(...)` function to predict the transcription of every test sample and to save the prediction in the dataset itself. We will call the resulting dictionary `\"results\"`. \n\n**Note**: we evaluate the test data set with `batch_size=1` on purpose due to this [issue](https:\/\/github.com\/pytorch\/fairseq\/issues\/3227). Since padded inputs don't yield the exact same output as non-padded inputs, a better WER can be achieved by not padding the input at all.","8e9296c5":"To make it clearer that `\" \"` has its own token class, we give it a more visible character `|`. In addition, we also add an \"unknown\" token so that the model can later deal with characters not encountered in Timit's training set. \n\nFinally, we also add a padding token that corresponds to CTC's \"*blank token*\". The \"blank token\" is a core component of the CTC algorithm. For more information, please take a look at the \"Alignment\" section [here](https:\/\/distill.pub\/2017\/ctc\/)."}}