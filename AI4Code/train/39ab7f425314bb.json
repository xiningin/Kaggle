{"cell_type":{"2a1ad79d":"code","9952aff6":"code","39b78870":"code","6556f651":"code","4c8f3e4d":"code","43628753":"code","c59fe3d7":"code","18c0f026":"code","1191ef15":"code","e5b79be3":"code","29111f56":"code","e85c1db4":"code","17728388":"code","fb1c6419":"code","e36bfab0":"code","5233d4d6":"code","2cede774":"code","fce3eb41":"code","1abfe0a0":"code","f26682f1":"code","eae067b5":"code","6cbf3146":"code","e7210ffe":"code","cae6374c":"code","f84edcb1":"code","a12b4ec1":"code","911840cf":"code","7c9dec54":"code","65d8292c":"code","e9502f74":"code","89dccc86":"code","6c560373":"code","5947ba24":"code","492d411f":"code","b2873953":"code","1aa55cb1":"code","6fbbd1f5":"code","6da637bb":"code","e2135140":"code","86e830b1":"code","7b970bfe":"code","37d39949":"code","c262b3e4":"code","9e6e15ba":"code","1c37b1ab":"code","218248b0":"code","0dc35024":"code","44528f00":"code","b3132933":"code","5f2115b2":"code","019cab47":"code","d898ea65":"code","4bbdd6b2":"code","e7ea9f58":"code","4531a73a":"code","dbbb7ead":"code","218e52ea":"code","d2b561b7":"code","b101d741":"markdown","5ebbea82":"markdown","cabd4c15":"markdown","0f95e69e":"markdown","21b87432":"markdown","15fc3034":"markdown","7a164900":"markdown","7581d599":"markdown","36ba7414":"markdown","3f706167":"markdown","7bf79637":"markdown","023e6154":"markdown","990c5e56":"markdown","4af8024c":"markdown","55cde45b":"markdown","14f16730":"markdown","7266f351":"markdown","e5252d9f":"markdown","635b2054":"markdown","c5cc6d75":"markdown","1358bbf2":"markdown","47bd7c6c":"markdown","b6ffeb6a":"markdown","c8b51354":"markdown","0c611098":"markdown","e040cffe":"markdown","3c420fb2":"markdown","0b12a30f":"markdown","ea6227c1":"markdown","77085a98":"markdown","1a8e224b":"markdown","aca4ec9a":"markdown","1e1e4e1c":"markdown","f1e8bb1b":"markdown","cb200d12":"markdown","33b6d9d7":"markdown","3027479d":"markdown","90731a19":"markdown","a29382ed":"markdown","222d7a34":"markdown","92e7f26d":"markdown","4fb79a29":"markdown","c30014d1":"markdown","f5b15a39":"markdown","919a1c7a":"markdown","e96327b8":"markdown"},"source":{"2a1ad79d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9952aff6":"# Import libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# preprocessing\nfrom scipy.stats import skew, kurtosis\nfrom scipy.stats import probplot\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom scipy.special import boxcox1p\n# model data\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\nimport optuna\n# Linear regressor\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, BayesianRidge\n# Kernel regressor\nfrom sklearn.svm import SVR\nfrom sklearn.kernel_ridge import KernelRidge\n# Neural Network\nfrom sklearn.neural_network import MLPRegressor\n# Tree based ensemble regressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier\nfrom xgboost.sklearn import XGBRegressor\nfrom lightgbm import LGBMRegressor\n# Stack regressor\nfrom sklearn.ensemble import StackingRegressor\n# Neural Network\nimport tensorflow as tf\nfrom tensorflow import keras","39b78870":"# load dataset\nraw_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv',index_col='Id')\nraw_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv',index_col='Id')\nraw_train.info()\nraw_test.info()","6556f651":"raw_train.head()","4c8f3e4d":"train = raw_train.copy()\ntest = raw_test.copy()\ntrain_test = pd.concat([train.drop(columns=['SalePrice']),test])","43628753":"# feature correlation with 'SalePrice'\ntrain.corr()['SalePrice'].sort_values(key=abs,ascending=False)","c59fe3d7":"sns.scatterplot(x='GrLivArea',y='SalePrice',data = train)","18c0f026":"train_test = train_test.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)","1191ef15":"# show number of missing values in each feature\ntrain.isnull().sum().sort_values(ascending=False).head(20)","e5b79be3":"test.isnull().sum().sort_values(ascending=False).head(30)","29111f56":"# NA literally means None\nfor col in ['PoolQC','MiscFeature','Alley','Fence','BsmtQual','BsmtCond','BsmtExposure',\n            'BsmtFinType1','BsmtFinType2','FireplaceQu','GarageType','GarageFinish',\n            'GarageQual','GarageCond','MasVnrType']:\n    train_test[col].fillna('None',inplace=True)\n# features with small num missing, use mode to cover\nfor col in ['Functional','Electrical','KitchenQual','Exterior1st','Exterior2nd','SaleType','MSZoning','Utilities']:\n    train_test[col].fillna(train_test[col].mode()[0],inplace=True)\nfor col in ['GarageYrBlt', 'GarageArea', 'GarageCars','MasVnrArea','BsmtFinSF1',\n            'BsmtUnfSF','BsmtFinSF2','BsmtHalfBath','BsmtHalfBath','BsmtFullBath','TotalBsmtSF']:\n    train_test[col].fillna(0,inplace=True)","e85c1db4":"train.corr()['LotFrontage'].sort_values(key=abs, ascending=False)","17728388":"# Use random forest regressor to predict 'LotFrontage'\nfrom sklearn.ensemble import RandomForestRegressor\n\n#choose related data to predict Lotfrontage on training set\nLF_df =train_test[['LotFrontage','1stFlrSF','LotArea','GrLivArea', 'TotalBsmtSF', 'MSSubClass', 'TotRmsAbvGrd','GarageArea','GarageCars']]\nLF_df_notnull = LF_df.loc[train_test['LotFrontage'].notnull()]\nLF_df_isnull = LF_df.loc[(train_test['LotFrontage'].isnull())]\nXtr_LF = pd.get_dummies(LF_df_notnull.drop(columns=['LotFrontage']))\nXte_LF = pd.get_dummies(LF_df_isnull.drop(columns=['LotFrontage']))\nY_LF = LF_df_notnull.LotFrontage\n# use RandomForestRegression to train data\nRFR = RandomForestRegressor(n_estimators=60, n_jobs=-1)\nRFR.fit(Xtr_LF,Y_LF)\npredict = RFR.predict(Xte_LF)\ntrain_test.loc[train_test['LotFrontage'].isnull(), ['LotFrontage']]= predict\nRFR.score(Xtr_LF,Y_LF)","fb1c6419":"# # Use the same model on test set to avoid data leakage\n# LF_df =test[['LotFrontage','1stFlrSF','LotArea','GrLivArea', 'TotalBsmtSF', 'MSSubClass', 'TotRmsAbvGrd','GarageArea','GarageCars']]\n# LF_df_notnull = LF_df.loc[test['LotFrontage'].notnull()]\n# LF_df_isnull = LF_df.loc[(test['LotFrontage'].isnull())]\n# Xte_LF = pd.get_dummies(LF_df_isnull.drop(columns=['LotFrontage']))\n# Y_LF = LF_df_notnull.LotFrontage\n# predict = RFR.predict(Xte_LF)\n# test.loc[test['LotFrontage'].isnull(), ['LotFrontage']]= predict","e36bfab0":"train_test.isnull().sum().sort_values(ascending=False).head(30)","5233d4d6":"train[['Neighborhood','SalePrice']].groupby('Neighborhood').mean().sort_values('SalePrice')","2cede774":"train_test[\"oNeighborhood\"] = train_test.Neighborhood.map({'MeadowV':1,\n                                                'IDOTRR':2, 'BrDale':2,\n                                               'OldTown':3, 'Edwards':3, 'BrkSide':3,\n                                               'Sawyer':4, 'Blueste':4, 'SWISU':4, 'NAmes':4,\n                                               'NPkVill':5, 'Mitchel':5,\n                                               'SawyerW':6, 'Gilbert':6, 'NWAmes':6,\n                                               'Blmngtn':7, 'CollgCr':7, 'ClearCr':7, 'Crawfor':7,\n                                               'Veenker':8, 'Somerst':8, 'Timber':8,\n                                               'StoneBr':9,\n                                               'NoRidge':10, 'NridgHt':10})","fce3eb41":"# create simple boolean features\ntrain_test['Has_pool'] = train_test['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ntrain_test['Has_2ndfloor'] = train_test['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ntrain_test['Has_garage'] = train_test['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ntrain_test['Has_bsmt'] = train_test['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ntrain_test['Has_fireplace'] = train_test['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\nbinary_features = ['Has_pool','Has_2ndfloor','Has_garage','Has_bsmt','Has_fireplace']","1abfe0a0":"# train_test['MSSubClass'] = train_test['MSSubClass'].astype('category')\n# train_test['OverallCond'] = train_test['OverallCond'].astype('category')\n# train_test['YrSold'] = train_test['YrSold'].astype('category')\n# train_test['MoSold'] = train_test['MoSold'].astype('category')","f26682f1":"ordinal_features = ['FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', \n                    'GarageCond', 'ExterQual', 'ExterCond','HeatingQC', \n                    'KitchenQual', 'BsmtFinType1', 'BsmtFinType2', \n                    'Functional', 'BsmtExposure', 'GarageFinish', \n                    'LandSlope','LotShape', 'PavedDrive', 'Street', \n                    'CentralAir']","eae067b5":"bin_map  = {'TA':2,'Gd':3, 'Fa':1,'Ex':4,'Po':1,'None':0,'Y':1,'N':0,'Reg':3,'IR1':2,'IR2':1,'IR3':0,\n            \"No\" : 2, \"Mn\" : 2, \"Av\": 3,\"Gd\" : 4,\"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3,\"BLQ\" : 4, \"ALQ\" : 5, \"GLQ\" : 6,\n            'Typ':7, 'Min1':6, \"Min2\":5, 'Mod':4, \"Maj1\":3, \"Maj2\":2, \"Sev\":1, \"Sal\":0,'Grvl':0,'Pave':1\n            }\ntrain_test['ExterQual'] = train_test['ExterQual'].map(bin_map)\ntrain_test['ExterCond'] = train_test['ExterCond'].map(bin_map)\ntrain_test['BsmtCond'] = train_test['BsmtCond'].map(bin_map)\ntrain_test['BsmtQual'] = train_test['BsmtQual'].map(bin_map)\ntrain_test['HeatingQC'] = train_test['HeatingQC'].map(bin_map)\ntrain_test['KitchenQual'] = train_test['KitchenQual'].map(bin_map)\ntrain_test['FireplaceQu'] = train_test['FireplaceQu'].map(bin_map)\ntrain_test['Functional'] = train_test['Functional'].map(bin_map)\ntrain_test['Street'] = train_test['Street'].map(bin_map)\ntrain_test['GarageQual'] = train_test['GarageQual'].map(bin_map)\ntrain_test['GarageCond'] = train_test['GarageCond'].map(bin_map)\ntrain_test['CentralAir'] = train_test['CentralAir'].map(bin_map)\ntrain_test['LotShape'] = train_test['LotShape'].map(bin_map)\ntrain_test['BsmtExposure'] = train_test['BsmtExposure'].map(bin_map)\ntrain_test['BsmtFinType1'] = train_test['BsmtFinType1'].map(bin_map)\ntrain_test['BsmtFinType2'] = train_test['BsmtFinType2'].map(bin_map)\n\nPavedDrive =   {\"N\" : 0, \"P\" : 1, \"Y\" : 2}\ntrain_test['PavedDrive'] = train_test['PavedDrive'].map(PavedDrive)\nGarageFinish = {'Fin':3, 'RFn':2, 'Unf':1, 'NA':0,'None':0}\ntrain_test['GarageFinish'] = train_test['GarageFinish'].map(GarageFinish)\nLandSlope = {'Gtl':2, 'Mod':1, 'Sev':0}\ntrain_test['LandSlope'] = train_test['LandSlope'].map(LandSlope)\ntrain_test[\"MSZoning\"] = train_test.MSZoning.map({'C (all)':1, 'RH':2, 'RM':2, 'RL':3, 'FV':4})\ntrain_test[\"oCondition1\"] = train_test.Condition1.map({'Artery':1,\n                                           'Feedr':2, 'RRAe':2,\n                                           'Norm':3, 'RRAn':3,\n                                           'PosN':4, 'RRNe':4,\n                                           'PosA':5 ,'RRNn':5})","6cbf3146":"# generalization features\ntrain_test['Total_SF']=train_test['TotalBsmtSF'] + train_test['1stFlrSF'] + train_test['2ndFlrSF']\n\ntrain_test['Total_sqr_footage'] = (train_test['BsmtFinSF1'] + train_test['BsmtFinSF2'] +\n                                 train_test['1stFlrSF'] + train_test['2ndFlrSF'])\n\ntrain_test['Total_Bathrooms'] = (train_test['FullBath'] + (0.5 * train_test['HalfBath']) +\n                               train_test['BsmtFullBath'] + (0.5 * train_test['BsmtHalfBath']))\n\ntrain_test['Total_porch_sf'] = (train_test['OpenPorchSF'] + train_test['3SsnPorch'] +\n                              train_test['EnclosedPorch'] + train_test['ScreenPorch'] +\n                              train_test['WoodDeckSF'])\n\ntrain_test[\"+_TotalHouse_OverallQual\"] = train_test[\"Total_SF\"] * train_test[\"OverallQual\"]\ntrain_test[\"+_GrLivArea_OverallQual\"] = train_test[\"GrLivArea\"] * train_test[\"OverallQual\"]\ntrain_test[\"+_oMSZoning_TotalHouse\"] = train_test[\"MSZoning\"] * train_test[\"Total_SF\"]\ntrain_test[\"+_oMSZoning_OverallQual\"] = train_test[\"MSZoning\"] + train_test[\"OverallQual\"]\ntrain_test[\"+_oNeighborhood_TotalHouse\"] = train_test[\"oNeighborhood\"] * train_test[\"Total_SF\"]\ntrain_test[\"+_oNeighborhood_OverallQual\"] = train_test[\"oNeighborhood\"] + train_test[\"OverallQual\"]\ntrain_test[\"+_oNeighborhood_YearBuilt\"] = train_test[\"oNeighborhood\"] + train_test[\"YearBuilt\"]\ntrain_test[\"+_BsmtFinSF1_OverallQual\"] = train_test[\"BsmtFinSF1\"] * train_test[\"OverallQual\"]\n\ntrain_test[\"-_oFunctional_TotalHouse\"] = train_test[\"Functional\"] * train_test[\"Total_SF\"]\ntrain_test[\"-_oFunctional_OverallQual\"] = train_test[\"Functional\"] + train_test[\"OverallQual\"]\ntrain_test[\"-_LotArea_OverallQual\"] = train_test[\"LotArea\"] * train_test[\"OverallQual\"]\ntrain_test[\"-_TotalHouse_LotArea\"] = train_test[\"Total_SF\"] + train_test[\"LotArea\"]\ntrain_test[\"-_oCondition1_TotalHouse\"] = train_test[\"oCondition1\"] * train_test[\"Total_SF\"]\ntrain_test[\"-_oCondition1_OverallQual\"] = train_test[\"oCondition1\"] + train_test[\"OverallQual\"]\n\n\ntrain_test[\"Bsmt\"] = train_test[\"BsmtFinSF1\"] + train_test[\"BsmtFinSF2\"] + train_test[\"BsmtUnfSF\"]\ntrain_test[\"Rooms\"] = train_test[\"FullBath\"]+train_test[\"TotRmsAbvGrd\"]\ntrain_test[\"TotalPlace\"] = train_test[\"TotalBsmtSF\"] + train_test[\"1stFlrSF\"] + train_test[\"2ndFlrSF\"] + train_test[\"GarageArea\"] +train_test[\"OpenPorchSF\"]+train_test[\"EnclosedPorch\"]+train_test[\"3SsnPorch\"]+train_test[\"ScreenPorch\"]","e7210ffe":"def plot_skew(feature):\n    \"\"\"\n    Function to plot distribution and probability(w.r.t quantiles of normal distribution)\n    \"\"\"\n    fig, axs = plt.subplots(figsize=(20,10),ncols=2)\n    sns.distplot(feature,kde=True,fit=norm,ax=axs[0])\n    # Generates a probability plot of sample data against the quantiles of a specified theoretical distribution (the normal distribution by default).\n    f=probplot(feature, plot=plt)\n    print('Skewness: {:f}'.format(feature.skew()))\n    print('Kurtosis: {:f}'.format(feature.kurtosis()))","cae6374c":"plot_skew(train.SalePrice)","f84edcb1":"train['SalePrice_log'] = np.log1p(train.SalePrice)\nplot_skew(train.SalePrice_log)","a12b4ec1":"# fix skew in all numeric data\ntrain_test_num = pd.DataFrame(train_test.select_dtypes(['float64','int32','int64']))\ntrain_test_num.drop(columns=ordinal_features,inplace=True)\ntrain_test_num.drop(columns=binary_features,inplace=True)","911840cf":"train_test_skewed = train_test_num.apply(lambda x:skew(x)).sort_values(key=abs,ascending=False)\ntrain_test_skewed_df = pd.DataFrame({'Skew':train_test_skewed})\ntrain_test_skewed_df","7c9dec54":"skewed_features = train_test_skewed_df[train_test_skewed_df.Skew>0.5].index\nlmbd = 0.15\nfor sk in skewed_features:\n    if sk not in  ['SalePrice','SalePrice_log']:\n        train_test[sk] = boxcox1p(train_test[sk], lmbd)","65d8292c":"train_test = pd.get_dummies(train_test)\ntrain_test.isna().sum().sort_values(ascending=False)","e9502f74":"lasso_model = Lasso(alpha=0.0001).fit(train_test[:1458],train.SalePrice_log)\nft_im_df = pd.DataFrame({\"Feature Importance\":lasso_model.coef_}, index=train_test.columns)\nft_im_df.sort_values('Feature Importance', key=abs,ascending=False).head(20)","89dccc86":"CV = KFold(n_splits=5,shuffle=True,random_state=42)\nrb_scaler = RobustScaler()","6c560373":"def rmse(model,x,y):\n    return -(cross_val_score(model, x, y,cv=CV,scoring='neg_root_mean_squared_error',n_jobs=-1))","5947ba24":"train_test = rb_scaler.fit_transform(train_test)\nX = train_test[:1458]\nX_test = train_test[1458:]\nY = train.SalePrice_log\nX.shape","492d411f":"pca = PCA(n_components=0.9999)\ntrain_test_pc = pca.fit_transform(train_test)\nX_PC = train_test_pc[:1458]\nX_test_PC = train_test_pc[1458:]\nX_PC.shape","b2873953":"n_trials = 500","1aa55cb1":"# # paramerter tuning using Optuna framework\n# def objective(trial):\n#     # define parameters' sample space and sample type\n#     alpha = trial.suggest_loguniform('alpha',1e-5,1e5)\n#     # define classifier\n#     reg = Ridge(alpha=alpha,random_state=42)\n#     # define evaluation matrix as objective to return\n#     score = rmse(reg,X,Y)\n#     return score.mean()\n# # create study\n# study = optuna.create_study(direction='minimize',sampler=optuna.samplers.TPESampler())\n# # run study to find best objective\n# study.optimize(objective,n_trials=n_trials,n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","6fbbd1f5":"# ridge_best_params = study.best_params\nridge_best_params = {'alpha': 36.31161037749121}\nridge_best_model = Ridge(**ridge_best_params)","6da637bb":"# # paramerter tuning using Optuna framework\n# def objective(trial):\n#     # define parameters' sample space and sample type\n#     alpha = trial.suggest_loguniform('alpha',1e-5,1e3)\n#     # define classifier\n#     reg = Lasso(alpha=alpha,random_state=42)\n#     # define evaluation matrix as objective to return\n#     score = rmse(reg,X_PC,Y)\n#     return score.mean()\n# # create study\n# study = optuna.create_study(direction='minimize',sampler=optuna.samplers.TPESampler())\n# # run study to find best objective\n# study.optimize(objective,n_trials=n_trials,n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","e2135140":"# lasso_best_params = study.best_params\nlasso_best_params = {'alpha': 0.0007350225758112973}\nlasso_best_model = Lasso(**lasso_best_params)","86e830b1":"# # paramerter tuning using Optuna framework\n# def objective(trial):\n#     # define parameters' sample space and sample type\n#     alpha = trial.suggest_loguniform('alpha',1e-5,1e3)\n#     l1_ratio = trial.suggest_discrete_uniform('l1_ratio',0.1,0.9,0.1)\n#     # define classifier\n#     reg = ElasticNet(alpha=alpha,l1_ratio=l1_ratio,random_state=42)\n#     # define evaluation matrix as objective to return\n#     score = rmse(reg,X_PC,Y)\n#     return score.mean()\n# # create study\n# study = optuna.create_study(direction='minimize',sampler=optuna.samplers.TPESampler())\n# # run study to find best objective\n# study.optimize(objective,n_trials=n_trials,n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","7b970bfe":"# elastic_best_params = study.best_params\nelastic_best_params = {'alpha': 0.004962477837834253, 'l1_ratio': 0.1}\nelastic_best_model = ElasticNet(**elastic_best_params)","37d39949":"# # paramerter tuning using Optuna framework\n# def objective(trial):\n#     # define parameters' sample space and sample type\n#     epsilon = trial.suggest_loguniform('epsilon',1e-4,1e2)# specifies the epsilon-tube within which no penalty\n#     kernel = trial.suggest_categorical('kernel',['poly','rbf'])\n#     gamma = trial.suggest_loguniform('gamma',1e-4,1e4) # kernel coifficient\n#     C = trial.suggest_loguniform('C',1e-4,1e4) # inversed regularization param\n#     # define classifier\n#     reg = SVR(epsilon =epsilon, C=C,kernel=kernel,gamma = gamma)\n#     # define evaluation matrix as objective to return\n#     score = rmse(reg,X_PC,Y)\n#     return score.mean()\n# # create study\n# study = optuna.create_study(direction='minimize',sampler=optuna.samplers.TPESampler())\n# # run study to find best objective\n# study.optimize(objective,n_trials=n_trials,n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","c262b3e4":"SVR_best_params = {'epsilon': 0.0007028077160999125, 'kernel': 'rbf', \n                   'gamma': 0.0001032023906441773, 'C': 20.673761351762828}     \n# SVR_best_params = study.best_params\nSVR_best_model = SVR(**SVR_best_params)","9e6e15ba":"# # paramerter tuning using Optuna framework\n# def objective(trial):\n#     # define parameters' sample space and sample type\n#     alpha = trial.suggest_loguniform('alpha',1e-4,1e3)\n#     kernel = trial.suggest_categorical('kernel',['polynomial','rbf'])\n#     gamma = trial.suggest_loguniform('gamma',1e-4,1e3)\n#     degree = trial.suggest_int('degree',3,5,1)\n#     # define classifier\n#     reg = KernelRidge(alpha=alpha,kernel=kernel,gamma = gamma,degree = degree)\n#     # define evaluation matrix as objective to return\n#     score = rmse(reg,X_PC,Y)\n#     return score.mean()\n# # create study\n# study = optuna.create_study(direction='minimize',sampler=optuna.samplers.TPESampler())\n# # run study to find best objective\n# study.optimize(objective,n_trials=n_trials,n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","1c37b1ab":"# KNR_best_params = study.best_params\nKNR_best_params = {'alpha': 0.017852963690058583, 'kernel': 'polynomial', \n                   'gamma': 0.00010010824404031089, 'degree': 4}   \nKNR_best_model = KernelRidge(**KNR_best_params)","218248b0":"# # paramerter tuning using Optuna framework\n# def objective(trial):\n#     # define parameters' sample space and sample type\n#     alpha_1 = trial.suggest_loguniform('alpha_1',1e-8,1e-5)\n#     alpha_2 = trial.suggest_loguniform('alpha_2',1e-8,1e-5)\n#     lambda_1 = trial.suggest_loguniform('lambda_1',1e-8,1e-5)\n#     lambda_2 = trial.suggest_loguniform('lambda_2',1e-8,1e-5)\n#     reg = BayesianRidge(alpha_1=alpha_1,alpha_2=alpha_1,lambda_1=lambda_1,lambda_2=lambda_2)\n#     # define evaluation matrix as objective to return\n#     score = rmse(reg,X_PC,Y)\n#     return score.mean()\n# # create study\n# study = optuna.create_study(direction='minimize',sampler=optuna.samplers.TPESampler())\n# # run study to find best objective\n# study.optimize(objective,n_trials=n_trials,n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","0dc35024":"# Bayesian_best_params = study.best_params\nBayesian_best_params = {'alpha_1': 9.974811163419825e-06, 'alpha_2': 5.050535825995536e-06,\n                        'lambda_1': 9.97006607898247e-06, 'lambda_2': 1.004807953151607e-08} \nBayesian_best_model = BayesianRidge(**Bayesian_best_params)","44528f00":"# # paramerter tuning using Optuna framework\n# def objective(trial):\n#     # define parameters' sample space and sample type\n#     n_layers = trial.suggest_int('n_layers', 1,1) # no. of hidden layers \n#     layers = []\n#     for i in range(n_layers):\n#         layers.append(trial.suggest_int('n_units_{}'.format(i+1), 100,300,20)) # no. of hidden unit\n#     activation=trial.suggest_categorical('activation',[ 'tanh', 'relu']) # activation function \n#     alpha=trial.suggest_loguniform('alpha',1e-4,100) #L2 penalty (regularization term) parameter.\n#     # define classifier\n#     reg =  MLPRegressor(random_state=42,\n#                         solver='adam',\n#                         activation=activation,\n#                         alpha=alpha,\n#                         hidden_layer_sizes=(layers),\n#                         max_iter=1000,\n#                         learning_rate='adaptive',\n#                         batch_size=64,\n#                         learning_rate_init=0.05,\n#                         early_stopping=True)\n#     # define evaluation matrix as objective to return\n#     score = rmse(reg,X_PC,Y)\n#     return score.mean()\n# # create study\n# study = optuna.create_study(direction='minimize',sampler=optuna.samplers.TPESampler())\n# # run study to find best objective\n# study.optimize(objective,n_trials = 1000,n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","b3132933":"# MLP_best_params = study.best_params\nMLP_best_model = MLPRegressor(random_state=42,\n                        solver='adam',\n                        activation='tanh',\n                        alpha= 2.9344049246725894,\n                        hidden_layer_sizes=(120),\n                        max_iter=1000,\n                        learning_rate='adaptive',\n                        batch_size=300,\n                        learning_rate_init=0.5,\n                        early_stopping=True)","5f2115b2":"# # paramerter tuning using Optuna framework\n# def objective(trial):\n#     # define parameters' sample space and sample type\n#     n_estimators = trial.suggest_int('n_estimators',50,500,50) \n#     learning_rate = trial.suggest_loguniform('learning_rate',1e-5,1e-3)\n#     max_depth = trial.suggest_int('max_depth',3,10,1)\n#     booster = trial.suggest_categorical('booster',['gbtree','gblinear','dart'])\n#     gamma = trial.suggest_loguniform('gamma', 1e-4,1e2)\n#     reg_alpha = trial.suggest_loguniform('reg_alpha',1e-3,1e2) # L1 regularization term on weights.\n#     reg_lambda = trial.suggest_loguniform('reg_lambda',1e-3,1e2) # L2 regularization term on weights.\n#     colsample_bytree = trial.suggest_discrete_uniform('colsample_bytree',0.4,1.0,0.2) # sub-features to use \n#     subsample = trial.suggest_discrete_uniform('subsample',0.8,1.0,0.1) # subsamples to use\n#     # define classifier\n#     reg = XGBRegressor(n_estimators=n_estimators, \n#                        objective='reg:pseudohubererror',\n#                        learning_rate=learning_rate,\n#                        max_depth=max_depth,\n#                        booster = booster,\n#                        gamma = gamma,\n#                        reg_alpha = reg_alpha,\n#                        reg_lambda = reg_lambda,\n#                        colsample_bytree = colsample_bytree,\n#                        subsample=subsample,\n#                        n_jobs=-1,\n#                        random_state=42)\n#     # define evaluation matrix as objective to return\n#     score = rmse(reg,X_PC,Y)\n#     return score.mean()\n# # create study\n# study = optuna.create_study(direction='minimize',sampler=optuna.samplers.TPESampler())\n# # run study to find best objective\n# study.optimize(objective,n_trials=1000,n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","019cab47":"# XGB_best_params = study.best_params\nXGB_best_params= {'n_estimators': 230, 'learning_rate': 0.05064350395549183, \n                  'max_depth': 2, 'booster': 'gblinear', 'gamma': 0.0005562582585578549,\n                  'reg_alpha': 0.0010013453661588325, 'reg_lambda': 0.0010065125775610678, \n                  'colsample_bytree': 0.6000000000000001, 'subsample': 1.0}\nXGB_best_model = XGBRegressor(**XGB_best_params,objective='reg:squaredlogerror',\n                              random_state=42,n_jobs=-1)","d898ea65":"# def objective(trial):\n#     # define parameters' sample space and sample type\n#     boosting_type  = trial.suggest_categorical('boosting_type',['gbdt','dart'])\n#     num_leaves = trial.suggest_int('num_leaves',10,30,5)\n#     learning_rate  = trial.suggest_loguniform('learning_rate',1e-5,1e-1)\n#     n_estimators = trial.suggest_int('n_estimators',10,500,10) \n#     max_depth = trial.suggest_int('max_depth',1,7,1) #-1 means no limit\n#     min_split_gain  = trial.suggest_loguniform('min_samples_split',1e-3,1e-1) #Minimum loss reduction required to make a further partition on a leaf node of the tree\n#     min_child_samples  = trial.suggest_int('min_samples_leaf',2,122,20) # minimum num of samples required to be a leaf node\n#     reg_alpha = trial.suggest_loguniform('reg_alpha',1e-3,1e2) # L1 regularization term on weights.\n#     reg_lambda = trial.suggest_loguniform('reg_lambda',1e-3,1e2) # L2 regularization term on weights.\n#     colsample_bytree = trial.suggest_discrete_uniform('colsample_bytree',0.4,1.0,0.2) # sub-features to use \n#     subsample = trial.suggest_discrete_uniform('subsample',0.8,1.0,0.1) # subsamples to use\n#     # define classifier\n#     reg = LGBMRegressor(n_estimators=n_estimators,\n#                                 boosting_type=boosting_type,\n#                                 num_leaves = num_leaves,\n#                                 max_depth=max_depth,\n#                                 learning_rate=learning_rate,\n#                                 min_split_gain = min_split_gain,\n#                                 min_child_samples=min_child_samples,\n#                                 reg_alpha=reg_alpha,\n#                                 reg_lambda=reg_lambda,\n#                                 n_jobs=-1,\n#                                 random_state=42)\n# #                                 device = 'gpu')\n#     # define evaluation matrix as objective to return\n#     score = rmse(reg,X_PC,Y)\n#     return score.mean()\n# # create study\n# study = optuna.create_study(direction='minimize',sampler=optuna.samplers.TPESampler())\n# # run study to find best objective\n# study.optimize(objective,n_trials=1000,n_jobs=-1)\n# print('Best model parameters:{} '.format(study.best_params))\n# print('Best score: {:.6f}'.format(study.best_value))","4bbdd6b2":"# LGBM_best_params = study.best_params\nLGBM_best_params={'boosting_type': 'gbdt', 'num_leaves': 10, \n                  'learning_rate': 0.0623344941102757, 'n_estimators': 350, \n                  'max_depth': 6, 'min_samples_split': 0.003721770553125834, \n                  'min_samples_leaf': 2, 'reg_alpha': 0.011733455705754578, \n                  'reg_lambda': 0.0692034942836032, 'colsample_bytree': 0.6000000000000001, \n                  'subsample': 0.9}\nLGBM_best_model = LGBMRegressor(**LGBM_best_params,random_state=42,n_jobs=-1)","e7ea9f58":"def model_eval():\n    rmse_mean = []\n    std = []\n    rmser = []\n    regressors=['Lasso','Ridge','Elastic_Net',\n                'Kernel_Ridge','SVR','Bayesian',\n                'MLP','XGB','LGBM']\n    models=[lasso_best_model,ridge_best_model,\n            elastic_best_model,KNR_best_model,\n            SVR_best_model,Bayesian_best_model,\n            MLP_best_model,XGB_best_model, \n            LGBM_best_model]\n    for model in models:\n        cv_result = rmse(model,X_PC,Y)\n        rmse_mean.append(cv_result.mean())\n        std.append(cv_result.std())\n        rmser.append(cv_result)\n    performance_df=pd.DataFrame({'CV_Mean':rmse_mean,'Std':std},index=regressors)\n    return performance_df\n\nperformance_df = model_eval()\nperformance_df.sort_values('CV_Mean')","4531a73a":"# ('SVR',SVR_best_model),('Kernel_ridge',KNR_best_model), ('ElasticNet',elastic_best_model), \n#                                          ('Ridge',ridge_best_model), \n#                                          ('Lasso', lasso_best_model),\n#                                          ('Bayesian',Bayesian_best_model)\nstacking = StackingRegressor(estimators=[('SVR',SVR_best_model), \n                                         ('Kernel_ridge',KNR_best_model),\n                                         ('ElasticNet',elastic_best_model), \n                                         ('Ridge',ridge_best_model), \n                                         ('Lasso', lasso_best_model),\n                                         ('Bayesian',Bayesian_best_model), \n                                         ('MLP',MLP_best_model), \n                                         ('LGBM',LGBM_best_model),\n                                         ('XGB',XGB_best_model)],\n                             final_estimator=KernelRidge(),\n                             cv=5,\n                             n_jobs=-1)\ncv_result = rmse(stacking,X_PC,Y)\nstk_acc = cv_result.mean()\nstk_std = cv_result.std()\nperformance_df.loc['Stacking'] = {'CV_Mean':stk_acc, 'Std':stk_std}\nperformance_df.sort_values(by=['CV_Mean'])","dbbb7ead":"stack_y = stacking.fit(X_PC,Y).predict(X_test_PC)\nelastic_y = elastic_best_model.fit(X_PC,Y).predict(X_test_PC)\nridge_y = ridge_best_model.fit(X_PC,Y).predict(X_test_PC)\nlasso_y = lasso_best_model.fit(X_PC,Y).predict(X_test_PC)\nSVR_y = SVR_best_model.fit(X_PC,Y).predict(X_test_PC)\nKNR_y = KNR_best_model.fit(X_PC,Y).predict(X_test_PC)\nBayesian_y = Bayesian_best_model.fit(X_PC,Y).predict(X_test_PC)\nXGB_y = XGB_best_model.fit(X_PC,Y).predict(X_test_PC)\n# 0.12393\n# blend = 1* stack_y + 0.0 * elastic_y + 0.0*ridge_y + 0.0*lasso_y+ 0*SVR_y\n# 0.12233\nblend = 0.0 * stack_y + 0.1 * elastic_y + 0.9*ridge_y + 0.0*lasso_y+ 0.0*SVR_y\n# 0.12579\n# blend = 0.7 * stack_y + 0.1 * elastic_y + 0.1*ridge_y + 0.1*lasso_y+ 0.0*SVR_y\n# 0.12489\n# blend = 0.0 * stack_y + 0.0 * elastic_y + 0.0*ridge_y + 0.0*lasso_y+ 0.0*SVR_y+0.0*KNR_y\n# 0.12312\n# blend = XGB_y\n# blend = ridge_y\n","218e52ea":"sub_pre = np.exp(blend)\nsub_pre = np.around(sub_pre,decimals=-1)\nsub_pre","d2b561b7":"\nsub_pd = pd.DataFrame({'Id':test.index,'SalePrice':sub_pre})\nsub_pd.to_csv('submit.csv' ,index=False)","b101d741":"# EDA","5ebbea82":"#### LightGBM","cabd4c15":"Kernel Ridge Regression","0f95e69e":"Best model parameters:{'n_estimators': 230, 'learning_rate': 0.05064350395549183, 'max_depth': 2, 'booster': 'gblinear', 'gamma': 0.0005562582585578549, 'reg_alpha': 0.0010013453661588325, 'reg_lambda': 0.0010065125775610678, 'colsample_bytree': 0.6000000000000001, 'subsample': 1.0}   \nBest score: 0.113198","21b87432":"## Model Data","15fc3034":"#### Bayesian Ridge Regression","7a164900":"Best model parameters:{'alpha_1': 9.974811163419825e-06, 'alpha_2': 5.050535825995536e-06, 'lambda_1': 9.97006607898247e-06, 'lambda_2': 1.004807953151607e-08}   \nBest score: 0.112688","7581d599":"Based on these information, we could go back to featuer engineering and build more features. And after adding new features, we can observe that many artifitial features contributes a lot in the model.","36ba7414":"#### XGBoost Regressor","3f706167":"# Load data","7bf79637":"Best model parameters:{'alpha': 0.004962477837834253, 'l1_ratio': 0.1}   \nBest score: 0.112756","023e6154":"Best model parameters:{'alpha': 36.31161037749121}   \nBest score: 0.112378 ","990c5e56":"Predict missing 'LotFrontage'.","4af8024c":"#### Ridge regression","55cde45b":"Most prices don't have single digits, so round the final result before submission.","14f16730":"### Categorical features","7266f351":"## Stacking Model","e5252d9f":"## Model Evaluation","635b2054":"Best model parameters:{'alpha': 0.0007350225758112973}   \nBest score: 0.112961","c5cc6d75":"Best model parameters:{'alpha': 0.017852963690058583, 'kernel': 'polynomial', 'gamma': 0.00010010824404031089, 'degree': 4}   \nBest score: 0.112657","1358bbf2":"Dimension reduction using PCA.","47bd7c6c":"Best model parameters:{'n_layers': 1, 'n_units_1': 120, 'activation': 'tanh', 'alpha': 2.9344049246725894}   \nBest score: 0.120864","b6ffeb6a":"Standerlize data using [Robust Scaler](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler).","c8b51354":"Trial 644 finished with value: 0.1248238412592273 and parameters: {'boosting_type': 'gbdt', 'num_leaves': 10, 'learning_rate': 0.0623344941102757, 'n_estimators': 350, 'max_depth': 6, 'min_samples_split': 0.003721770553125834, 'min_samples_leaf': 2, 'reg_alpha': 0.011733455705754578, 'reg_lambda': 0.0692034942836032, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.9}.  \nBest is trial 644 with value: 0.1248238412592273.","0c611098":"## Handle missing value","e040cffe":"### Handle missing values","3c420fb2":"## Feature Engineering","0b12a30f":"#### Elastic Net Regression","ea6227c1":"Drop outliers with large GrLivArea but low price in training set.","77085a98":"Basic rule for feature engineering is generate as many features as possible, encode features with corresponding schema and let algorithms to choose which features are important.","1a8e224b":"Use Random Forest model because it's scale rubust.","aca4ec9a":"## Reference\uff1a  \n* https:\/\/www.kaggle.com\/darkside92\/detailed-examination-for-house-price-top-10\n* https:\/\/www.kaggle.com\/masumrumi\/a-detailed-regression-guide-with-house-pricing\n* https:\/\/www.kaggle.com\/mabalogun\/using-xgboost-and-lightgbm-to-predict-house-price\n* https:\/\/www.kaggle.com\/zugariy\/regression-blending-and-stacking-v-02\/comments\n* https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n* https:\/\/www.kaggle.com\/massquantity\/all-you-need-is-pca-lb-0-11421-top-4#Content","1e1e4e1c":"## Outlier","f1e8bb1b":"## Submit","cb200d12":"### Numerical Features","33b6d9d7":"**Observation:**  \ntraining set: 1460 samples with 80 features(include 'Id').  \ntest set: 1459 samples with 79 features.","3027479d":"## Blending","90731a19":"### Feature importance","a29382ed":"Encode ordinal features.","222d7a34":"Normalize heavily skewed features.","92e7f26d":"Best model parameters:{'epsilon': 0.0007028077160999125, 'kernel': 'rbf', 'gamma': 0.0001032023906441773, 'C': 20.673761351762828}   \nBest score: 0.111166","4fb79a29":"#### Lasso Regression","c30014d1":"#### Neighborhood  \nAs we all know, neighborhood is a vatial important feature for house price but categorical string names can not really represent this feature very well. So I generate a new ordinal feature based on average SalePrice in each neighborhood.","f5b15a39":"#### Multi-layer Perceptron Regressor","919a1c7a":"### Dummy Encoding","e96327b8":"#### Epsilon-Support Vector Regression"}}