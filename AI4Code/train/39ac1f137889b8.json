{"cell_type":{"101f6c4a":"code","729c7c18":"code","79be40a9":"code","4051eea6":"code","90f4d37d":"code","6e9af162":"code","4fa13170":"code","f278d2d8":"code","1f9c4401":"code","95bbd26d":"code","54b5892c":"code","3e250a7f":"code","bcf55edd":"code","a39f01ea":"code","cd66ef16":"code","9ad336d2":"code","9688503d":"code","e41223c0":"code","2eb34e8b":"code","c84715bb":"code","b7c867bb":"code","60e86210":"code","4a8fd90d":"code","c1e71dd9":"code","48c37de9":"code","099312b2":"code","ebb351c8":"code","a2c82aab":"code","4c0a917e":"code","5ce5a0f4":"code","0593ef75":"code","76e322ec":"code","90213103":"code","45fcc243":"code","bbeae81d":"code","e8430f73":"code","ea2a11e6":"code","c0eb46e6":"code","74f55566":"code","9dec5f49":"code","78cd1036":"code","98f9d63b":"code","c7e39591":"code","d0e662a0":"code","68e6f53e":"markdown","40197a58":"markdown","d112dfd3":"markdown","1fcc5627":"markdown","fd080b81":"markdown","cfddc966":"markdown","06ed13e9":"markdown","0b3d3ade":"markdown","4bc1a0a3":"markdown","73a233b6":"markdown","7b5beede":"markdown","acf41228":"markdown","d85ada57":"markdown","050316f8":"markdown","9e4071c6":"markdown","31c15876":"markdown","0678a883":"markdown","5476f909":"markdown","3ed6e591":"markdown","2aef19ba":"markdown","889cfd67":"markdown","e8d1c01e":"markdown","53932e07":"markdown","f4c51809":"markdown","8b76479d":"markdown"},"source":{"101f6c4a":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n%matplotlib inline \nsns.set(color_codes=True)\n\nimport plotly.graph_objects as go\nfrom datetime import datetime\n\nimport warnings\nwarnings.filterwarnings('ignore')","729c7c18":"path = \"..\/input\/finance-accounting-courses-udemy-13k-course\/udemy_output_All_Finance__Accounting_p1_p626.csv\"\ndata = pd.read_csv(path)\ndf = data.copy()","79be40a9":"# to display the top 5 rows\ndata.head(5)","4051eea6":"# to display the bottom 5 rows\ndata.tail(5) ","90f4d37d":"data.dtypes","6e9af162":"data.describe().T","4fa13170":"data.info()","f278d2d8":"data.columns.to_list()","1f9c4401":"data.drop(['id', 'url', 'num_published_practice_tests', \"rating\",\n           \"discount_price__price_string\",\"price_detail__price_string\",\n          \"discount_price__currency\",\"price_detail__currency\"], axis=1, inplace=True)\ndata.head(5)\n\n\"\"\"\n  id, url: will not be useful for comparing data\n  num_published_practice_tests: Data is not consistent as it is used very little in general.\n  rating: avg_rating_recent will give us the final rating value\n  price_detail__price_string, discount_price__price_string: there are other columns holding the same values\n  Since all data uses the same currency, we remove the columns holding currency.\n\"\"\"","95bbd26d":"data = data.rename(columns = {\n    \"is_paid\": \"paid\",\n    \"num_subscribers\": \"subscribers\",\n    \"is_wishlisted\": \"widthlisted\",\n    \"num_published_lectures\": \"lectures\",\n    \"published_time\": \"publish\",\n    \"discount_price__amount\": \"dp_amount\",\n    \"price_detail__amount\": \"pd_amount\",\n})","54b5892c":"data.head(5)","3e250a7f":"# check NaN value for each columns\nfor col in data.columns.to_list():\n    print(\"Value: \", data[col].value_counts().index[0])\n    print(col, \": \", data[col].value_counts().sum())\n    print(\"Null: \", data[col].isnull().sum())\n    print(\"=\"*45)","bcf55edd":"data[data[\"dp_amount\"].isna()].head(5)","a39f01ea":"data[data[\"pd_amount\"].isna()].head(5)","cd66ef16":"def get_three_m(col): # mean median mod\n    print(\"========\",col,\"========\")\n    print(\"Mean   :\", np.mean(data[~data[col].isna()][col].to_list()))\n    print(\"Median :\", np.median(data[~data[col].isna()][col].to_list()))\n    print(\"Mod    :\", stats.mode(data[~data[col].isna()][col].to_list())[0][0])\n\n    plt.figure(figsize=(15,5))\n    ax = sns.countplot(x=col, data=data)\n    plt.xticks(rotation = 90)\n    plt.show()","9ad336d2":"get_three_m(\"dp_amount\")\nget_three_m(\"pd_amount\")","9688503d":"plt.style.use('fivethirtyeight')\nsns.distplot(data[~data[\"pd_amount\"].isna()][\"pd_amount\"].to_list(), color='green')\nplt.show()","e41223c0":"dp_amount_nan_indexes = data[data[\"dp_amount\"].isna()].index.to_list()\npd_amount_nan_indexes = data[data[\"pd_amount\"].isna()].index.to_list()\n\ndata.loc[dp_amount_nan_indexes] = 455.0\ndata.loc[pd_amount_nan_indexes] = 3200.0","2eb34e8b":"print(\"dp_amount nan count :\", data[\"dp_amount\"].isnull().sum())\nprint(\"pd_amount nan count :\", data[\"pd_amount\"].isnull().sum())","c84715bb":"plt.style.use('fivethirtyeight')\nsns.distplot(data[~data[\"pd_amount\"].isna()][\"pd_amount\"].to_list(), color='green')\nplt.show()","b7c867bb":"plt.figure(figsize=(15,5))\nsns.boxplot(x=data[\"subscribers\"])","60e86210":"for col in data.select_dtypes('float64').columns:\n    plt.figure(figsize=(15,5))\n    plt.title(col)\n    sns.boxplot(x=data[col])","4a8fd90d":"for col in data.select_dtypes('int64').columns:\n    plt.figure(figsize=(15,5))\n    plt.title(col)\n    sns.boxplot(x=data[col])","c1e71dd9":"print(\"for avg_rating: \", len(data[data['avg_rating'] > 5]))\nprint(\"for avg_rating_recent: \", len(data[data['avg_rating_recent'] > 5]))","48c37de9":"data[data['avg_rating'] > 5]","099312b2":"data[data['avg_rating_recent'] > 5]","ebb351c8":"data.drop(data[data['avg_rating'] > 5].index.to_list(), axis=0, inplace=True)\ndata.drop(data[data['avg_rating_recent'] > 5].index.to_list(), axis=0, inplace=True)","a2c82aab":"data","4c0a917e":"data['paid'].value_counts()","5ce5a0f4":"# data.drop(['paid'], axis=1, inplace=True)\n# data.drop(['widthlisted'], axis=1, inplace=True)","0593ef75":"data","76e322ec":"print(\"Max num_reviews : \", np.max(data['num_reviews'].to_list()))\nprint(\"Min num_reviews : \", np.min(data['num_reviews'].to_list()))\nprint(\"Mean num_reviews: \", np.mean(data['num_reviews'].to_list()))","90213103":"data['created']  = pd.to_datetime(data['created'].to_list()).strftime('%m\/%d\/%Y').values\ndata['publish']  = pd.to_datetime(data['publish'].to_list()).strftime('%m\/%d\/%Y').values","45fcc243":"data","bbeae81d":"# for pd_amount\nprint(\"Max pd_amount : \", np.max(data['pd_amount'].to_list()))\nprint(\"Min pd_amount : \", np.min(data['pd_amount'].to_list()))\nprint(\"Mean pd_amount: \", np.mean(data['pd_amount'].to_list()))","e8430f73":"# for dp_amount\nprint(\"Max dp_amount : \", np.max(data['dp_amount'].to_list()))\nprint(\"Min dp_amount : \", np.min(data['dp_amount'].to_list()))\nprint(\"Mean dp_amount: \", np.mean(data['dp_amount'].to_list()))","ea2a11e6":"# 1 Indian Rupee = 0.014 Dolar\ndata['pd_amount'] = round(data['pd_amount']*0.014,2).to_list()\ndata['dp_amount'] = round(data['dp_amount']*0.014,2).to_list()","c0eb46e6":"data","74f55566":"# Top 10 courses\ndata[['title', 'subscribers', 'avg_rating']] \\\n    .sort_values(by=\"subscribers\", ascending=False)[0:10].set_index('title') \\\n    .style.format(\"{:.2f}\", subset=['avg_rating']).background_gradient(cmap='Blues', subset = ['avg_rating']) \\\n    .set_caption('Most subscribed courses') \\\n    .set_properties(padding=\"15px\", border='2px solid white', width='150px')","9dec5f49":"data['rating_diff'] = data.avg_rating_recent - data.avg_rating","78cd1036":"data[data.subscribers > 10000][['title','subscribers', 'avg_rating', 'avg_rating_recent','rating_diff']] \\\n.sort_values(by = 'rating_diff')[:10] \\\n.set_index('title').style \\\n    .format(\"{:.4f}\", subset = ['avg_rating', 'avg_rating_recent','rating_diff']) \\\n    .background_gradient(cmap='Blues', subset = ['subscribers']) \\\n    .bar(align='mid', color=['#FCC0CB', '#90EE90'], subset = ['rating_diff']) \\\n    .set_caption('Nagative rating change') \\\n    .set_properties(padding=\"15px\", border='2px solid white', width='150px')","98f9d63b":"fig = go.Figure(go.Bar(\n            x=data.sort_values(by=\"subscribers\", ascending=False).subscribers[0:10],\n            y=data.sort_values(by=\"subscribers\", ascending=False).title[0:10],\n            orientation='h'))\nfig.update_layout(yaxis=dict(autorange=\"reversed\"), title='Top 10 most subscribed courses')\n\nfig.show()","c7e39591":"# lectures\ndf = data.sort_values(by=\"lectures\", ascending=False)\nfig = go.Figure(\n    data=[go.Bar(\n        x=df.lectures[0:10].to_list(), \n        y=df.title[0:10].to_list(), \n        orientation='h')],\n)\nfig.update_layout(yaxis=dict(autorange=\"reversed\"), title='Top 10 most lectures courses')\nfig.show()","d0e662a0":"data_date = data.groupby(['created']).size()[0:100]\nfig = px.line(data_date, \n              x=data_date.index, y=data_date, line_shape = 'linear', title='Created courses', labels={'y': 'Courses'})\nfig.update_layout(hovermode='x')\nfig.update_xaxes(\n    rangeslider_visible=True\n)\n\nfig.show()","68e6f53e":"The data content is not actually full as seen. So all the thoughts I just made, after seeing this, we saw that it didn't work. The fact that there are numbers in the title section is an indication that this data is completely ghost data. For this reason, we can easily delete these data.","40197a58":"When the graphs above are examined, we can see that this dataset is rich in outliers. <br>\nFor example, avg_rating and rating columns can take values between 0-5. But it is impossible to get values like 500,3000. We can delete them or we can actually check Udemy courses and update those with outlier this data.","d112dfd3":"## Checking the Types of Data <a id=\"6\"><\/a>\n\nHere we check for the datatypes because sometimes the MSRP or the price of the car would be stored as a string or object, if in that case, we have to convert that string to the integer data only then we can plot the data via a graph. Here, in this case, the data is already in integer format so nothing to worry.","1fcc5627":"When we look at the outlier graph above for num_reviews, we can see that some courses have been reviewed too much. This is very natural for the courses that are hit. Generally, the entire distribution is shifted in the 0 direction. This is because of the new and unfamiliar courses that are many times more than the number of hit courses.","fd080b81":"When the results and graphs are examined above, we can make the following comments:\n- For discounted course prices: It will be sufficient to enter the median value in the NaN incoming values.\n- For non-discounted course prices: There is no evenly distributed data in this column. 50 percent of the data is worth 1280 and 8640 coins. Here the mod is 8640. In this case, I care that it is as close to the mod as well as the median.","cfddc966":"## Renaming the Columns <a id=\"8\"><\/a>\n\nThe names of some columns are too long. We can optimize this.","06ed13e9":"## Dropping Irrelevant Columns <a id=\"7\"><\/a>","0b3d3ade":"# Introduction <a id=\"1\"><\/a>\n\nData analysis is a process used to analyze, clean, transform, and model data to discover useful information, draw conclusions, and support decision making. Data analysis has versatile and diverse approaches covering a variety of techniques under various names in different business, science and social science fields. <br>\nData integration is the pioneer of data analysis. Data analysis is closely related to data visualization and data distribution. The term data analysis is sometimes used as a synonym for data modeling.\n\n### Data Analysis Process\nData analysis is a process used to obtain raw data and turn them into useful information for users to make decisions. Data is collected first and then analyzed to answer questions, test hypotheses or reject theories. <br>\nData analysis has several stages. The stages are repetitive.\n\n#### \ud83d\udfe2 Data Requirements\nThe data required as input to the analysis is selected based on the requirements of the analyst or the customers who will use the result of the analysis. Data can be numerical or categorical.\n\n#### \ud83d\udfe2 Data Collecting\nData can be collected from a variety of sources. Data can be collected by surrounding sensors such as traffic cameras, satellites, recording devices. It is also possible to use interviews, downloads from online resources or documentation.\n\n#### \ud83d\udfe1 Data processing\nData initially obtained must be processed or edited for analysis. For example, these can be placed in rows and columns in a table format for further analysis such as a spreadsheet or statistical software.\n\n#### \ud83d\udd34 Data Cleaning\nData may be incomplete, duplicate, or contain errors. The need for data cleaning will result from problems with obtaining and storing data. Data cleansing is the process of preventing and correcting these errors. Data cleansing tasks include record matching, detecting data inaccuracy, overall quality of existing data, deduplication, and column segmentation. Such data problems can also be detected through a variety of analytical techniques. Unusual amounts above or below certain threshold values can be examined. Quantitative data methods for outlier detection can be used to remove possible incorrectly entered data.\n\n#### \ud83d\udd34 Exploratory Data Analysis (EDA)\nVarious mathematical formulas or models called algorithms can be applied to data to determine relationships between variables, such as correlation or causality. Inference statistics include techniques used to measure relationships between specific variables. <br>\nAnalysts can try to create models that describe the data to simplify the analysis and communicate results.","4bc1a0a3":"There is no problem in the data. But I want to make a conversion in this column as well. I will convert from Indian currency to Dollar currency which is more common in World.","73a233b6":"## Loading the Data Into the DataFrame <a id=\"5\"><\/a>","7b5beede":"As you can see, trying to fix them manually is very time consuming. Let's look at the content of the data.","acf41228":"Let's interpret our data by looking at the 3 outputs above. <br>\nThere is a lot of NaN value at discounted prices. If there is a large difference between the prices of discounted courses, it is difficult to predict. But if the discounted prices are more or less the same, we can fill in the blank values here by taking the average or the better median. This same process is valid for regular prices.","d85ada57":"## Importing the Required Libraries <a id=\"4\"><\/a>","050316f8":"# Data Visualization and EDA <a id=\"12\"><\/a>","9e4071c6":"# Content <a id=\"2\"><\/a>\n\nHere, I have extracted data related to 10k courses which come under the development category on Udemy's website.\nThe 17 columns in the dataset can be used to gain insights related to:\n\n- id : The course ID of that particular course.\n- title : Shows the unique names of the courses available under the development category on Udemy.\n- url: Gives the URL of the course.\n- is_paid : Returns a boolean value displaying true if the course is paid and false if otherwise.\n- num_subscribers : Shows the number of people who have subscribed that course.\n- avg_rating : Shows the average rating of the course.\n- avg rating recent : Reflects the recent changes in the average rating.\n- num_reviews : Gives us an idea related to the number of ratings that a course has received.\n- num_ published_lectures : Shows the number of lectures the course offers.\n- num_ published_ practice_tests : Gives an idea of the number of practice tests that a course offers.\n- created : The time of creation of the course.\n- published_time : Time of publishing the course.\n- discounted_ price_amount : The discounted price which a certain course is being offered at.\n- discounted_ price_currency : The currency corresponding to the discounted price which a certain course is being offered at.\n- price_ detail_amount : The original price of a particular course.\n- price_ detail_currency : The currency corresponding to the price detail amount for a course.","31c15876":"## Outlier Detection <a id=\"10\"><\/a>","0678a883":"I can begin to interpret according to the three outputs above.\n- id, url, num_published_practice_tests columns contain data that will not be useful for me when implementing EDA. That's why we're eliminating these columns.","5476f909":"The paid column holds a boolean value. Does this apply to all lines?","3ed6e591":"The creation and release dates of the courses are important to find out how long these courses were prepared by the course provider. But there is too much detail. The year, month and day will be sufficient. That's why we have to date conversion.","2aef19ba":"- [Introduction](#1) \n- [Content](#2)\n- [Data Processing](#3)\n    - [Importing the Required Libraries](#4)\n    - [Loading the Data Into the DataFrame](#5)\n    - [Checking the Types of Data](#6)\n    - [Renaming the Columns](#8)\n- [Data Cleaning](#11)\n    - [Dropping Irrelevant Columns](#7)\n    - [Missing Value](#9)\n    - [Outlier Detection](#10)\n- [Data Visualization and EDA](#12)","889cfd67":"## Missing Value <a id=\"9\"><\/a>","e8d1c01e":"# Data Processing <a id=\"3\"><\/a>","53932e07":"Let's examine the pd_amount and dp_amount columns.","f4c51809":"Sometimes it is necessary to remove some columns after thorough examination. One of them in this column. It will not work for us in data analysis as all courses have a True value. That's why we can remove it.","8b76479d":" # Data Cleaning <a id=\"11\"><a\/>"}}