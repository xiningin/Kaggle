{"cell_type":{"98fa841e":"code","ba2b5368":"code","bc597d55":"code","3bcf49c7":"code","ff6da733":"code","cd7675d1":"code","c52a442d":"code","823220b8":"code","853892d9":"code","95b00b74":"code","36504a31":"code","057bb56b":"code","4a425b60":"code","62429521":"code","86f6e3fd":"code","5593c3eb":"code","41a97403":"code","b2e185ca":"code","bf07146b":"code","cc1a696c":"code","5d23bf01":"code","bccf6476":"code","a0312a60":"code","31e60404":"code","3e09724a":"code","a8383385":"code","f7b45d6c":"code","b61215e5":"code","118eb07b":"code","c898eb47":"code","144d81c1":"code","c0d5420f":"code","cbdab32a":"markdown","440a23f8":"markdown","7b43a343":"markdown","3ac1e4ff":"markdown","d64c4835":"markdown","9cbe6a4b":"markdown","08cb4bdb":"markdown","1c763d58":"markdown","21474402":"markdown","effbbd96":"markdown","61937a8c":"markdown","a1ca1329":"markdown","1383abfa":"markdown","2b3783e5":"markdown","4c009ea7":"markdown","9ef39e41":"markdown","deee2d15":"markdown","f4d4f42c":"markdown","a881e23b":"markdown","8527c6f3":"markdown","f21af079":"markdown","43f85a79":"markdown","45898534":"markdown","10ec8595":"markdown","055d18fb":"markdown","b7bf7968":"markdown","c9180b7a":"markdown","9ab47697":"markdown","8518cf17":"markdown","49d64ea6":"markdown","b5cd3c7b":"markdown","83748516":"markdown","0962e2f7":"markdown","79ccdd90":"markdown","dfc4cd6b":"markdown","fa66c08e":"markdown","9feb5d63":"markdown","cb0dafab":"markdown","0ac06673":"markdown"},"source":{"98fa841e":"import pandas as pd\nimport numpy as np\nfrom pandas import Series,DataFrame\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","ba2b5368":"df = pd.read_csv('..\/input\/graduate-admissions\/Admission_Predict.csv',index_col='Serial No.')\n\ndf","bc597d55":"df.info()","3bcf49c7":"df.describe()","ff6da733":"def normalising(feature):\n    nmx = 10\n    nmn = 0\n    mx = feature.max()\n    mn = feature.min()\n    return ((nmx- nmn)\/(mx - mn)*(feature - mx) + nmx)","cd7675d1":"#values = df.columns\nnorm_df = normalising(df)\nnorm_df","c52a442d":"norm_df.describe()","823220b8":"figs,((ax1,ax2,ax3),(ax4,ax5,ax6)) = plt.subplots(2,3,figsize=(16,8))\nsns.scatterplot('GRE Score','Chance of Admit ',data=df,ax=ax1)\nsns.scatterplot('TOEFL Score','Chance of Admit ',data=df,ax=ax2)\nsns.barplot('University Rating','Chance of Admit ',data=df,ax=ax3)\nsns.barplot('SOP','Chance of Admit ',data=df,ax=ax4)\nsns.barplot('LOR ','Chance of Admit ',data=df,ax=ax5)\nsns.scatterplot('CGPA','Chance of Admit ',data=df,ax=ax6)","853892d9":"research_count = df.groupby('Research')['Research'].count()\nprint(research_count)\nlabels = ['0:Without','1:With']\nplt.pie(research_count,explode=[0,0.05],labels=labels)\nplt.legend(loc='lower right')","95b00b74":"df1=df.copy()\ndf1['Research'] = df['Research'].replace([1,0],['Yes','No'])\nsns.catplot(x='Chance of Admit ',y='Research',data=df1,kind='violin')","36504a31":"sns.kdeplot(norm_df['Chance of Admit '],shade=True)\nsns.kdeplot(norm_df['CGPA'])","057bb56b":"sns.heatmap(norm_df.corr(),annot=True)\nfig=plt.gcf()\nfig.set_size_inches(10,5)","4a425b60":"X = norm_df.drop('Chance of Admit ',axis=1)\nY = norm_df['Chance of Admit ']","62429521":"from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\n\nfrom sklearn.model_selection import cross_val_score\n\nMSEs = cross_val_score(lin_reg,X,Y,cv=5,scoring='neg_root_mean_squared_error')\nmean_MSE = np.mean(MSEs)\nprint(f' The Negative MSE which is to be maximised is {mean_MSE}')","86f6e3fd":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\n\nridge_reg = Ridge()\n\nparameters = {'alpha':[1e-15,1e-8,.002,0.1,0.5,1,5,10,20]}\n\nRMSEs = GridSearchCV(ridge_reg,parameters,scoring='neg_root_mean_squared_error',cv=5)\nRMSEs.fit(X,Y)\nprint(RMSEs.best_params_)\nprint(f' The Negative MSE which is to be maximised is {RMSEs.best_score_}')","5593c3eb":"from sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\n\nlasso_reg = Lasso()\n\nparameters = {'alpha':[.002,0.1,0.5,1,5,10,20]}\n\nLMSEs = GridSearchCV(lasso_reg,parameters,scoring='neg_root_mean_squared_error',cv=5)\nLMSEs.fit(X,Y)\nprint(LMSEs.best_params_)\nprint(f' The Negative MSE which is to be maximised is {LMSEs.best_score_}')","41a97403":"from sklearn.linear_model import LinearRegression","b2e185ca":"lreg = LinearRegression()","bf07146b":"lreg.fit(X,Y)","cc1a696c":"print(f'The intercept of the Linear Model \/ best fit line is {lreg.intercept_}')\nprint(f'Number of coefficents is {len(lreg.coef_)}')","5d23bf01":"coef_df = DataFrame(X.columns,columns=['Feature'])\ncoef_df['Coeff'] = lreg.coef_\ncoef_df","bccf6476":"sns.catplot(x='Feature',y='Coeff',data=coef_df,kind='point',height=8)","a0312a60":"lreg1 = LinearRegression()","31e60404":"norm_df.shape","3e09724a":"from sklearn.model_selection import train_test_split","a8383385":"x_train,x_test,y_train,y_test = train_test_split(X,Y)\nprint(x_train.shape,x_test.shape,y_train.shape,y_test.shape)","f7b45d6c":"lreg1.fit(x_train,y_train)","b61215e5":"y_pred = lreg1.predict(x_test)","118eb07b":"rms = np.mean((y_pred-y_test)*2)\nprint(f'The root-mean-square error of the predicted values from the actual values is {rms}')","c898eb47":"pred_val = DataFrame(y_pred,columns=['Predicted'])\npred_val['Actual'] = y_test.values\npred_val","144d81c1":"sns.lmplot(x='Predicted',y='Actual',data=pred_val)","c0d5420f":"sns.scatterplot(x=y_pred,y=(y_pred-y_test),data=pred_val)","cbdab32a":"# Data Visualisation","440a23f8":"Lasso Regression tuned with GridSearchCV","7b43a343":"So, data has been correctly split with 300 samples in training set and 100 samples to testing. All Set!!!","3ac1e4ff":"This is a residual plot. And randomly dispersed points around a horizontal line indicates, Linear Regression (our selected model) is appropriate for prediction.","d64c4835":"from the above graph it can be seen that, the predicted values and actual values are linear in relation. This implies, the prediction is somewhat similar to actual value.","9cbe6a4b":"Since the three regression has almost the same RMSE with very little variation, Ordinary Linear Regression can be chosen, for convienence and simplicity.","08cb4bdb":"Let's look at the intercept and coeffections of the features related to the best fit line.","1c763d58":"# Prediction","21474402":"The regression model has been formed with training set of data. Now, prediction can be done using the testing set.","effbbd96":"# Feature Engineering ( Normalising the data values )","61937a8c":"Let's take a look at the correlation matrix to see which factors have more impact on chance of admission.","a1ca1329":"There are 6 features as GRE Score, TOFEL Score, University Rating, SOP, LOR, CGPA and Research.\nThe task is to preapare a model that can effectively predict the Chance of Admit (WITH LOWEST RMSE)","1383abfa":"Obtaining variables and target data from dataset","2b3783e5":"How is the research experience impact chance of admission?????","4c009ea7":"So, more CGPA means more chance of admission. But the trailing chance of admission indicates a non-linear relation between the two. Some other factors are influencing!!","9ef39e41":"RESULT : A MODEL HAS BEEN CREATED WHERE THE PREDICTION IS DONE WITH MEAN SQUARE ERROR OF 0.096","deee2d15":"All the features are linearly related to the chance of admit. Nothing much to comprehend from this unless we look deep into each features.","f4d4f42c":"Important Note!!! : RMSE is simply the positive value of Negative MSE","a881e23b":"The mean of the dataset is made to come inline using Normalisation. This will greatly reduce error due to range and hence brings more flexibilty in data analysis and visualisation","8527c6f3":"The dataset is good, with no null objects. Out of the features, only Research Experience is a categorical value, as YES=1 or NO=0.\nAll other features are scaled, but on different scales. So might need to do normalisation to avoid skewness while analysing.","f21af079":"# Getting a general idea of data","43f85a79":"splitting the data into train and test data. It is mentioned in the task that, prediction should be done for the last 100 people. So, splitting shiuld be done in such way.","45898534":"Let's look at the distribution of Research Experience","10ec8595":"Ordinary Linear Regression valiated with cross-validation","055d18fb":"Also note that, students with good CGPA will have pretty good GRE Score and TOEFL Score. \nGRE Score and Research experience is related strongly compared to TOEFL Score, University Rating or SOP. ALso people with higher CGPA might have research experience.","b7bf7968":"The dataset has been normalised with max value at 10 and min value at 0.","c9180b7a":"Relation between Chance of Admission and Undergraduate CGPA","9ab47697":"Eventhough not much is the differnece, more people has research experience.","8518cf17":"Data Normalisation should be done before applying to a model!!!! The mean of each feature should come almost inline to form a good model.","49d64ea6":"It is evident that, people with research experience has a better chance of admission.","b5cd3c7b":"So, a model has been formed by fitting the features and target variables.","83748516":"All the features are positively correlated to Chance of Admit. The top three features highly influencing the chance are CGPA,GRE Score and TOEFL Score.","0962e2f7":"# Regression Model","79ccdd90":"# Model Selection","dfc4cd6b":"Ridge Regression tuned with GridSearchCV","fa66c08e":"Importing train_test_split from sklearn","9feb5d63":"Fitting the model with x_train and y_train","cb0dafab":"Total number of people is 400. So, 100 out of 400 is 25%.","0ac06673":"importing linear regression from sklearn"}}