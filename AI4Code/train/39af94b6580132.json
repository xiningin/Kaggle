{"cell_type":{"2c2824c1":"code","b3ce9c69":"code","b6565750":"code","6a1060cb":"code","69d5c5bb":"code","faf56603":"code","97911901":"code","3b868186":"code","b68c7cc3":"code","a1a8d843":"code","8a3da094":"code","524f9e40":"code","798320da":"code","3232c6a7":"code","9ce5447c":"code","f4e08d53":"code","5cd2fbf4":"code","eb62c412":"code","b6970a02":"code","a019fe95":"code","bbcd486c":"code","54ae90de":"code","2d6711cc":"code","30e021aa":"code","5db1296b":"code","32d46401":"code","5eca13bb":"code","dcf9b1a2":"code","4a2b2000":"code","ddafd4f1":"code","87967a5d":"code","2b5b18a3":"code","5067341b":"code","0822f0c7":"code","1eee3fe7":"code","90f34c81":"code","19188c86":"code","0b0ea0f6":"code","df12146e":"code","f1b06c22":"code","a73d2c5e":"code","d5bfc428":"code","a503509f":"code","233f8129":"code","e825bba5":"code","1fc13fc2":"code","eea0ffaf":"code","ea5675be":"code","78c670c0":"code","585e922f":"code","3e7bd333":"code","7144c22b":"code","1c3c88a9":"code","d0115f6b":"markdown","83bf3dc4":"markdown","162e0298":"markdown","cc16d21c":"markdown","7152efe3":"markdown","4dfb0ecf":"markdown","7b40dcd7":"markdown","123a857c":"markdown","aec22037":"markdown","be332788":"markdown","437990e7":"markdown","fe581bbd":"markdown","782836f9":"markdown","dc28affa":"markdown","10b3a5d6":"markdown","511489d0":"markdown","8646b024":"markdown","2ea864dc":"markdown","58d151db":"markdown","409344c8":"markdown","a3cf1c6c":"markdown","57c0a5a1":"markdown","b3b62eae":"markdown","c3587783":"markdown","aec79440":"markdown","538167e7":"markdown","2674a95c":"markdown","a55ebb93":"markdown","851902d4":"markdown","a7266673":"markdown","71abfa16":"markdown","121b4114":"markdown","1618f57a":"markdown","dc3c2ae1":"markdown","e13abc2f":"markdown","7cb73ebd":"markdown","25dde7ba":"markdown","4ef42f20":"markdown","8c20c0f7":"markdown","9a64f2fa":"markdown","07c2c838":"markdown","20ac9e6e":"markdown","8be5b59a":"markdown","f2ff9faa":"markdown","f5111282":"markdown","a9587f18":"markdown","a5af8869":"markdown","2b62ea98":"markdown","72a2c242":"markdown","8eafa2d7":"markdown","c6be5ad7":"markdown","7ed978c5":"markdown","7abbfdca":"markdown"},"source":{"2c2824c1":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport math\nfrom math import log,exp\nimport statsmodels.api as sm\n# Importing the dataset.\ndf = pd.read_csv(\"..\/input\/housedata\/data.csv\")","b3ce9c69":"# Dropping the display of Scientific Notations.\n# for pandas \npd.set_option('display.float_format', lambda x: '%.3f' % x)\n# for Numpys\nnp.set_printoptions(suppress=True,formatter={'float_kind':'{:16.3f}'.format}, linewidth=130)","b6565750":"# Information about the columns \nprint(df.info())\n# Null content about df\ndf.isnull().sum()","6a1060cb":"# Checking for Duplicate records.\nd = df.groupby([\"street\",\"date\",\"yr_built\",'price'])['street'].count().to_frame()\nd.reset_index(level= [1,2,3], inplace= True)\nd.index.name = None\nd = d.sort_values(['street'], ascending= False)\nprint(d.head(3) ) # we can conclude that there are no duplicate records for a house.","69d5c5bb":"# we would evaluating number of cases. which are valued as Zero.\ndef price_validation(x):\n  if(x <= 0 ):\n    x = 'Invalid'\n  else:\n    x = 'Valid'\n  return x\n\ndf[\"zero_price\"] = df['price'].apply(price_validation)\nzero_price = df.groupby(['zero_price'])[\"zero_price\"].count()\nzero_price_prct = (zero_price \/ zero_price.sum()) *100 \n\nprint(zero_price)\nprint(zero_price_prct)","faf56603":"#  Dropping cases which doesn't have Housing Prices.\ndf = df[(df.zero_price == 'Valid')]","97911901":"# import of visual library\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndata = df.price\n\nplt.subplots( figsize=(12, 5))\nsns.set_style('whitegrid')\n\nbinwidth = (data.max() - data.min()) \/ 100\n\nax = plt.hist(data, bins=np.arange(min(data), max(data) + binwidth, binwidth))\nplt.ticklabel_format(style='plain', axis='x')\nplt.ticklabel_format(style='plain', axis='y')\nplt.show()\n\n\n# Box Plot to check out on the distribution.\nsns.boxplot(y = 'price', data = df)\nplt.ticklabel_format(style='plain', axis='y')\nplt.show()","3b868186":"import math\n\n\ndef transform_log(x):\n  if(x <= 0):\n    x = 1\n  return x\n\n\ndef apply_log(x):\n  y = math.log(x)\n  return y\n\ndf['price'] = df.price.apply(transform_log)\ndf['log_price'] = df.price.apply(apply_log)\n\ndata = df['log_price']\n\nbinwidth = (data.max() - data.min()) \/ 100\n\nsns.set_style('whitegrid')\nax = plt.hist(data, bins=np.arange(min(data), max(data) + binwidth, binwidth))\nplt.ticklabel_format(style='plain', axis='x')\nplt.ticklabel_format(style='plain', axis='y')\nplt.show()\n\nsns.boxplot(y = 'log_price', data = df)\nplt.ticklabel_format(style='plain', axis='y')\nplt.show()","b68c7cc3":"# Evaluating Country presence.\n\ncountry  = df.groupby(['country'])['country'].count()\nprint(country)\n\nprint(\"Dropping Country  as it conatains only one value i.e. USA\")\ndf = df.drop('country', axis = 1)\n# print(df.columns)","a1a8d843":"#  removing Outliers from the Data at 99.7%  standard normality rate.\ndf_price_cap = np.mean(df['price']) + 3 * np.std(df['price'])\ndf_wo_outliers = df[(df['price'] <= df_price_cap)]\n\nhigh_quantile_90_quantile = df_wo_outliers['price'].quantile(.90)\nhigh_quantile_10_quantile = df_wo_outliers['price'].quantile(.10)\n\n\nplt.subplots( figsize=(20, 5))\nsns.set_style('whitegrid')\nax = sns.boxplot(x= 'city',y = 'price', \n                 data = df_wo_outliers,\n                 showmeans=True)\nax.axhline(high_quantile_90_quantile, ls='--', c= 'r')\nax.axhline(high_quantile_10_quantile, ls='--', c= 'b')\nplt.ticklabel_format(style='plain', axis='y')\nplt.setp(ax.get_xticklabels(), rotation=90) # to rotate city values by 90*\nplt.show()","8a3da094":"plt.subplots( figsize=(20, 5))\nsns.set_style('whitegrid')\nax = sns.countplot(x= 'city', data = df_wo_outliers)\nplt.ticklabel_format(style='plain', axis='y')\nplt.setp(ax.get_xticklabels(), rotation=90) # to rotate city values by 90*\nplt.show()","524f9e40":"#  filtering out only 5 cities.\ndf_city_OHC = pd.get_dummies(df['city'])\n\nselected_cities = ['Mercer Island', 'Clyde Hill', 'Yarrow Point', 'Medina', 'Seattle','Algona', 'Skykomish', 'SeaTac', 'Pacific']\ndf_city_OHC = df_city_OHC.loc[:,selected_cities]\ndf_city_OHC.columns = ['City_Mercer_Island', 'City_Clyde_Hill', 'City_Yarrow_Point', 'City_Medina', 'City_Seattle', 'City_Algona', 'City_Skykomish', 'City_SeaTac', 'City_Pacific']\n\n\ndf = pd.concat([df,df_city_OHC] , axis= 1)\nprint(df.head(2))","798320da":"zip_code = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/DSPOWER93\/Data\/main\/us-zip-code-latitude-and-longitude.csv\", sep= ';') \n\nstreet_long_lat = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/DSPOWER93\/Data\/main\/Washington_housing_data\/street_lat_long_file_WA_housing_data.csv\", \n                              index_col=0)\n\n\nprint(zip_code.head(2))\nprint(street_long_lat.head(2))\nprint(df.head(2))","3232c6a7":"# The Zip code file imported has zip-code values in numeric converting Dataframe Zip-codes only Numeric format removing State Initials from the data.\nimport re\ndef only_numeric(x):\n  x = int(re.search(r'\\d+', x)[0])\n  return x\ndf['zip'] = df.statezip.apply(only_numeric) \n\n#  Joining with Zip-code Lat-Long Data. \ndf = pd.merge(df, zip_code.loc[:, ['Zip','Latitude','Longitude']], how='inner', left_on='zip', right_on = 'Zip')\ndf = df.drop('Zip', axis = 1)\n\n#  creating a new column of complete address\nComplete_address = []\n\nfor i in range(df.shape[0]): \n  a = df.iloc[i,14]\n  b = df.iloc[i,15]\n  c = df.iloc[i,16]\n  d = a + ', ' + b + ', ' + c\n  Complete_address.append(d)\n\n#  Joining Complete adresss to the Data frame.\n\ndf['complete_address'] = Complete_address\n\n# Making sure that none of the duplicate\/repeated entires are  not joined. \n\nstreet_long_lat = street_long_lat.loc[:,['address','Lat_','long_']].drop_duplicates()\n\n#  Joining with Zip-code Lat-Long Data. \ndf = pd.merge(df,street_long_lat , how='left', left_on='complete_address', right_on = 'address').drop('address', axis = 1)\n\n# df = df.drop('Zip', axis = 1)\n\n# To evaluate the dataset >>\ndf.head(2)\ndf.isnull().sum()\n\n# repalcing the NAN values in Lat_ and Long_ with Latitude and longititude.\ndf.Lat_.fillna(df.Latitude, inplace=True)\ndf.long_.fillna(df.Longitude, inplace=True)\n\n#  Dropping the Non relvant columns.\ndf.drop(['Latitude', 'Longitude'], axis= 1, inplace= True)","9ce5447c":"import folium\nimport pandas as pd\nimport folium.plugins\n\nimport branca\nimport branca.colormap as cm\n\nx_start = (df['Lat_'].max() + df['Lat_'].min()) \/ 2\ny_start = (df['long_'].max() + df['long_'].min()) \/ 2\nstart_coord = (x_start, y_start)\n\ncolormap = cm.LinearColormap(colors=['red','black'],index=[7800,655000], vmin= 7800,vmax=655000)\n\nmap = folium.Map(location=start_coord, zoom_start=10)\n\nlat = list(df.Lat_)\nlon = list(df.long_)\npow = list(df.price)\n\nfor loc, p in zip(zip(lat, lon), pow):\n    folium.Circle(\n        location=loc,\n        radius=5,\n        fill=True,\n        color=colormap(p),\n        fill_opacity=0.7\n    ).add_to(map)\n\nmap.add_child(colormap)\n\nfolium.Circle(\n        location= [47.640295 , -122.258628],\n        radius=50,\n        fill=True,\n        color='yellow',\n        fill_opacity=2).add_to(map)\n\nfolium.Circle(\n        location= [47.640295 , -122.258628],\n        radius=50,\n        fill=True,\n        color='yellow',\n        fill_opacity=2).add_to(map)\n\n\n\nfolium.Circle(\n        location= [47.489137 , -122.215115],\n        radius=50,\n        fill=True,\n        color='green',\n        fill_opacity=2).add_to(map)\n\n\n\ndisplay(map)","f4e08d53":"# Evergreen Floating bridge\n# 47.640295 , -122.258628\n\nfrom math import acos,cos,radians,sin\n\n# 6371*acos(cos(radians(90-D7))*cos(radians(90-D8))+sin(radians(90-D7))*sin(radians(90-D8))*cos(radians(E7-E8)))\n\n# D7 IS LAT1, E7 IS LONG1, D8 IS LAT2, E8 IS LONG2 \n\n\ndef distance_evergreen_bridge(lat,long):\n  D7 =  47.640295 # LAT OF EVERGREEN BRIDGE.\n  E7 = -122.258628 # LONG OF EVERGREEN BRIDGE.\n  D8 = lat\n  E8 = long\n  distance = 6371*acos(cos(radians(90-D7))*cos(radians(90-D8))+sin(radians(90-D7))*sin(radians(90-D8))*cos(radians(E7-E8)))\n  return distance\n\n\ndf['distance_evergreen_bridge'] = df.apply(lambda x: distance_evergreen_bridge( x.Lat_ , x.long_), axis=1) \n\nprint(\"Log Price to Evergreen Bridge Distance Corr score: \",df['distance_evergreen_bridge'].corr(df['log_price'])) \n\n#  marking Houses below Renton Muncipal Airport\ndf['below_renton_airport'] = np.where(df.Lat_ < 47.489137,1, 0)\n\nprint(\"Log Price to below renton Airport : \", df['below_renton_airport'].corr(df['log_price']))\n","5cd2fbf4":"#  Checking the Columns in Data Which other columns needs to be preprocessed.\ndf.head(2)","eb62c412":"# House age\ndf['house_age'] =  2020 - df['yr_built']\n# Renovation age\ndf['renovation_age'] = np.where(df['yr_renovated'] <= 0 ,df['house_age'] , 2020 - df['yr_renovated'] )\n\nprint(df['house_age'].corr(df['renovation_age'])) # Not that bad to highly correlated to each other we can evaluate from VIF.","b6970a02":"#  checking for anomaly in the Data.\nprint(df[df['house_age'] < 0 ])\nprint(df[df['renovation_age'] < 0 ])\n\nprint(np.max(df['house_age'])) # to see outliers\nprint(np.max(df['renovation_age'])) # to see outliers\n\n#  Dropping Year Built and Year renovated column\ndf.drop(['yr_built', 'yr_renovated'], axis= 1, inplace= True)","a019fe95":"# Evaluating Dataframe and Dropping irrelevant Columns \ndf.head(2) \n\n# To Be Dropped\n# Price:  As we are considering Log price \n# Street: As it is categorical variable and we already treated it with adding Lat and Long Data.\n# StateZip: Already treated  by using Gegraphical cordinates.\n# City: already Treated above by selecting Five cities in One hot encoding.\n# zer_price : it is  dummy variable used to filter out data. \n# Lat Long Cordinates Data: We found on Plotting  regarding price correlation with location evergreen Bridge nearness ('distance_evergreen_bridge').\n# Complete_address:  for mapping purpose only.\n# Date: It is on  record Logged Date \n\ndf.drop([ 'date','price', 'street', 'city' , 'statezip', 'zero_price','zip', 'complete_address', 'Lat_', 'long_'], axis=1,\n        inplace = True)\ndf.head(2)","bbcd486c":"# Graph 1 Variables Left City_Yarrow_Point\n\nsns.pairplot(df.loc[:,:'City_Yarrow_Point'])","54ae90de":"# Graph 1 Variables right to Log price\n\nsns.pairplot( pd.concat([ df.log_price,df.loc[:,'City_Medina':] ] , axis = 1))","2d6711cc":"#  Spliting the data into train and test.\n\nX = df.drop('log_price', axis = 1)\nY = df.log_price\n\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.20,random_state=0)","30e021aa":"\n#  the purpose of including the Y variable as well, is because while dropping variable from infinite VIF score due to multi collinearity\n#  we can start from dropping least correlating variable.\n\nx_train_with_Y = pd.concat([x_train,y_train], axis= 1)\n\nVar_Corr = x_train_with_Y.corr()\n\n# plot the heatmap and annotation on it\nplt.subplots( figsize=(22, 5))\nsns.heatmap(Var_Corr, xticklabels=Var_Corr.columns, yticklabels=Var_Corr.columns, annot=True)\nplt.show()","5db1296b":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\n\nvif1 = [variance_inflation_factor( x_train.values, j) for j in range(x_train.shape[1])]\n\nfor vif_values in zip(x_train.columns , vif1):\n  print(vif_values)","32d46401":"#  as all of the sqft varaibles areas are correlating to each other we can start dropping  of the least correlated to y varaible.\n\n# Dropping ('sqft_basement', inf) being a redundant variable (adding no values) Also it is having least correlation among other three with log Price. \n\nx_train.drop('sqft_basement', axis = 1, inplace = True)\n\nvif1 = [variance_inflation_factor( x_train.values, j) for j in range(x_train.shape[1])]\n\nfor vif_values in zip(x_train.columns , vif1):\n  print(vif_values)\n\na = []\nfor i in (x_train.columns , vif1):\n  a.append(i)\nb = a[1]\nc= b.index(max(b))\nprint('Variable with Highest VIF score :',a[0][c])","5eca13bb":"# Dropping\n# ('sqft_living', 49.01240143028837)\nx_train.drop('sqft_living', axis = 1, inplace = True)\n\nvif1 = [variance_inflation_factor( x_train.values, j) for j in range(x_train.shape[1])]\n\nfor vif_values in zip(x_train.columns , vif1):\n  print(vif_values)\n\na = []\nfor i in (x_train.columns , vif1):\n  a.append(i)\nb = a[1]\nc= b.index(max(b))\nprint('Variable with Highest VIF score :',a[0][c])","dcf9b1a2":"\n# Dropping\n#  ('condition', 25.041700948458082)\n\nx_train.drop('condition', axis = 1, inplace = True)\n\nvif1 = [variance_inflation_factor( x_train.values, j) for j in range(x_train.shape[1])]\n\nfor vif_values in zip(x_train.columns , vif1):\n  print(vif_values)\n\na = []\nfor i in (x_train.columns , vif1):\n  a.append(i)\nb = a[1]\nc= b.index(max(b))\nprint('Variable with Highest VIF score :',a[0][c])","4a2b2000":"#  creating New Interactive variable Bedroom to Bathroom ratio. as both of them are having high multi-collinarity. \n\nx_train['bedroom_bathroom_ratio'] = x_train['bedrooms']\/ x_train['bathrooms']\nx_test['bedroom_bathroom_ratio'] =  x_test['bedrooms']\/ x_test['bathrooms']\n\nx_train['bedroom_bathroom_ratio'] = x_train['bedroom_bathroom_ratio'].replace(np.nan, 0)\nx_test['bedroom_bathroom_ratio'] =  x_test['bedroom_bathroom_ratio'].replace(np.nan, 0)\n\n\n\n#  dropping bedroom and bathroom.\n\nx_train.drop( [ 'bedrooms' ,'bathrooms'], axis = 1, inplace = True)\n\nvif1 = [variance_inflation_factor( x_train.values, j) for j in range(x_train.shape[1])]\n\nfor vif_values in zip(x_train.columns , vif1):\n  print(vif_values)\n\na = []\nfor i in (x_train.columns , vif1):\n  a.append(i)\nb = a[1]\nc= b.index(max(b))\nprint('Variable with Highest VIF score :',a[0][c])","ddafd4f1":"#  dropping bedroom and bathroom.\n\nx_train.drop( [ 'floors'], axis = 1, inplace = True)\n\nvif1 = [variance_inflation_factor( x_train.values, j) for j in range(x_train.shape[1])]\n\nfor vif_values in zip(x_train.columns , vif1):\n  print(vif_values)\n\na = []\nfor i in (x_train.columns , vif1):\n  a.append(i)\nb = a[1]\nc= b.index(max(b))\nprint('Variable with Highest VIF score :',a[0][c])","87967a5d":"fit1 = sm.OLS( list(y_train), sm.add_constant(x_train)).fit()\nprint(fit1.summary())","2b5b18a3":"x_train.drop( ['City_Pacific'], axis = 1, inplace = True)","5067341b":"fit1 = sm.OLS( list(y_train), sm.add_constant(x_train)).fit()\nprint(fit1.summary())","0822f0c7":"x_train.drop( ['City_Algona'], axis = 1, inplace = True)","1eee3fe7":"fit1 = sm.OLS( list(y_train), sm.add_constant(x_train)).fit()\nprint(fit1.summary())","90f34c81":"x_train.drop( ['City_Seattle'], axis = 1, inplace = True)\n\nfit1 = sm.OLS( list(y_train), sm.add_constant(x_train)).fit()\nprint(fit1.summary())","19188c86":"x_train.drop( ['City_Skykomish'], axis = 1, inplace = True)\n\nfit1 = sm.OLS( list(y_train), sm.add_constant(x_train)).fit()\nprint(fit1.summary())","0b0ea0f6":"x_train.drop( ['City_Yarrow_Point'], axis = 1, inplace = True)\n\nfit1 = sm.OLS( list(y_train), sm.add_constant(x_train)).fit()\nprint(fit1.summary())","df12146e":"#MAPE Value calculation\ndef mape(act,pred):\n    x = np.abs(1-pred\/act)*100\n    return(round(np.mean(x),2))","f1b06c22":"x_test = x_test.loc[:,x_train.columns]","a73d2c5e":"pred_train = fit1.predict()\n\ntrainmape = mape(np.exp(y_train),np.exp(pred_train))\n\npred_test=fit1.predict(sm.add_constant(x_test))\n\ntestmape=mape(np.exp(y_test),np.exp(pred_test))\n\nprint(\"Train MAPE is : \" + str(trainmape) + \"%\")\nprint(\"Test MAPE is : \" + str(testmape) + \"%\")","d5bfc428":"import scipy.stats as stats\nimport pylab\n\n\nresiduals = np.exp(y_train) - np.exp(pred_train)\n\nstats.probplot( residuals , dist=\"norm\", plot=pylab)\npylab.show()","a503509f":"# replotting Removing Outliers  \nresidual_wo_outliers_bool = (residuals >= (np.mean(residuals) - 2 * np.std(residuals))) & (residuals <= (np.mean(residuals) + 2 * np.std(residuals)))\n\nresidual_wo_outliers = residuals[residual_wo_outliers_bool]\n\nstats.probplot( residual_wo_outliers , dist=\"norm\", plot=pylab)\npylab.show()","233f8129":"plt.subplots( figsize=(20, 5))\nsns.set_style('whitegrid')\n\nplt.scatter(np.exp(y_train[residual_wo_outliers_bool]),residual_wo_outliers, s = 3)\nplt.ticklabel_format(style='plain', axis='x')\nplt.ticklabel_format(style='plain', axis='y')\nplt.show()","e825bba5":"\nfrom math import floor\n# Range of 100,000\n\ny_train_floor = np.hstack(np.floor( np.array(np.exp(y_train)) \/100000) *100000)\n\n\n# In case you would like represent the distribution in terms of range replace 'y_train_floor' with  'y_train_range'\ny_train_range = np.core.defchararray.add(y_train_floor.astype(str), np.repeat(' - ',len(y_train_floor)))\ny_train_range = np.core.defchararray.add(y_train_range,(100000+y_train_floor).astype(str)) \n\n\n\nresidual_distribution = pd.DataFrame({'y_train_floor' : np.round(y_train_floor,0).astype(int),\n                                      'y_train' : np.exp(y_train),\n                                      'Over_predicted' : np.where(residuals < 0, 1,0  ),\n                                      'Under_predicted': np.where(residuals >= 0, 1,0 ),\n                                      'Error_delta': np.abs(1 - np.exp(pred_train)\/np.exp(y_train))*100})\n\n","1fc13fc2":"residual_presentation = residual_distribution.groupby(['y_train_floor'])['Over_predicted','Under_predicted','Error_delta'].agg(['sum','mean'])\nresidual_presentation.columns = residual_presentation.columns.map('_'.join)\nresidual_presentation = residual_presentation.reset_index()","eea0ffaf":"plt.subplots( figsize=(20, 5))\nsns.set_style('whitegrid')\nax = sns.barplot('y_train_floor','Error_delta_mean', data = residual_presentation)\nplt.setp(ax.get_xticklabels(), rotation=90) # to rotate city values by 90*\nplt.xlabel('Price Range (Within 100K Range)')\nplt.ylabel('MAPE Score')\nplt.show()","ea5675be":"residual_presentation.plot(x = 'y_train_floor', y = ['Over_predicted_mean', 'Under_predicted_mean'], kind = 'bar',figsize=(20,5))\nplt.show()","78c670c0":"\nfrom math import floor\n# Range of 100,000\n\ny_test_floor = np.hstack(np.floor( np.array(np.exp(y_test)) \/100000) *100000)\n\n\n# In case you would like represent the distribution in terms of range replace 'y_train_floor' with  'y_train_range'\ny_test_range = np.core.defchararray.add(y_test_floor.astype(str), np.repeat(' - ',len(y_test_floor)))\ny_test_range = np.core.defchararray.add(y_test_range,(100000+y_test_floor).astype(str)) \n\n\nresiduals_test = np.exp(y_test) - np.exp(pred_test)\n\nresidual_distribution_test = pd.DataFrame({'y_train_floor' : np.round(y_test_floor,0).astype(int),\n                                            'y_train' : np.exp(y_test),\n                                            'Over_predicted' : np.where(residuals_test < 0, 1,0  ),\n                                            'Under_predicted': np.where(residuals_test >= 0, 1,0 ),\n                                            'Error_delta': np.abs(1 - np.exp(pred_test)\/np.exp(y_test))*100})\n\n","585e922f":"residual_presentation_test = residual_distribution_test.groupby(['y_train_floor'])['Over_predicted','Under_predicted','Error_delta'].agg(['sum','mean'])\nresidual_presentation_test.columns = residual_presentation_test.columns.map('_'.join)\nresidual_presentation_test = residual_presentation_test.reset_index()","3e7bd333":"plt.subplots( figsize=(20, 5))\nsns.set_style('whitegrid')\nax = sns.barplot('y_train_floor','Error_delta_mean', data = residual_presentation_test)\nplt.setp(ax.get_xticklabels(), rotation=90) # to rotate city values by 90*\nplt.xlabel('Price Range (Within 100K Range)')\nplt.ylabel('MAPE Score')\nplt.show()","7144c22b":"residual_presentation_test.plot(x = 'y_train_floor', y = ['Over_predicted_sum', 'Under_predicted_sum'], kind = 'bar',figsize=(20,5))\nplt.show()","1c3c88a9":"residual_presentation_test.plot(x = 'y_train_floor', y = ['Over_predicted_mean', 'Under_predicted_mean'], kind = 'bar',figsize=(20,5))\nplt.show()","d0115f6b":"### Visualizing  Location Data. \n\nAs we have got the Geo-points Data we will be plotting on map and visualize the scatter plots. in terms of pricing. ","83bf3dc4":"## Model Final Take Aways:\n\n*   Due to High Intercept Model is bad at predicting Lower valued Houses. Model is not recommended for predicting house below 300,000 USD.\n\n*  The model also misses at predicting highly values house prices. The High valued houses are under-predicted. The Data doesn't have strong variable predicting High Valued Houses prices. \n* There is strong  Negative correlation between House Age and Floors, Older the House lesser the floors.\n*  Lower priced City Variables weren't as impactful variables as presumed (having High P-values) so had to drop them from the calculation. \n\n### Suggestions to improve Model performance:\n\n* In plotting of house prices It can be seen that House Being away from water bodies are Cheaper in price, there is scope of feture Engineering for variable created to determine the distance from water bodies.\n\n* There could be a strong varaible added which can determine lower prices. As coefficients present in Data are not able to highly penalize the model to predict lower House prices.\n\n*   Major of its prediction accross the ranges is under-predicted, the values has to  be presented in ranges with Standard error, with Lower end to Upper end House price values. i.e Predicted Value \u00b1 Standard Error of range of 100,000s\n\n\n","162e0298":"**Inference**\nDue to Outliers  the chart has zoomed out, and residual distriution looks straight line to zero major of residual distribution is fitting on the line.\nWe can replot and see removing the Outliers (at \u00b13\u03c3)    ","cc16d21c":"### Data cleaning and Pre-processing\n\nLinear Regression can handle Numeric Values but we have to convert object\/ catagorical variables to Numerics before we proceed head.\n\n\n```\n 14  street         4600 non-null   object \n 15  city           4600 non-null   object \n 16  statezip       4600 non-null   object \n 17  country        4600 non-null   object\n\n```\n\n","7152efe3":"Dropping City_Skykomish Dues to  High P-value.","4dfb0ecf":"### City Names\n\nFrom Dataset City Name it is assured that the whole dataset represents city names located neighbouring to Seattle Washington.","7b40dcd7":"**Inference**\nThrough Histogram and Boxplot we can infer that the Data is highly skewed and needs to be Normalized.\n\n\n---","123a857c":"City Algona is having High P-value(0.482). This show-case the variable is  redundant and to be dropped.","aec22037":"The Model is predicting in Similar trend compared to Train data. ","be332788":"**Inference**\n\n\nFrom the Box Plot it's seen that other than top four cities with high median range most of the other cities house prices are not in distinct high price range. Other than 4 Cities considering 'Seattle' the most popular city for one hot encoding.\n","437990e7":"**Inference**\n\nFrom the above it can be seen that Sqft Variables are correlating amongselves. We have to treat it using VIF score.\n\nAlso there hasn't been pattern observed being Non-linear(exponential or quadratic etc..) in Nature. We can proceed with Linear regression on Dataset. ","fe581bbd":"For Train Data","782836f9":"### Tabulation of Residual Distribution on Train Data.\n\n**Objective:** To see the Residual distribution accross Different House Price Ranges.","dc28affa":"#### Under Prediction \/ Over Prediction accross House Price Range.","10b3a5d6":"**Inference** \n\nVaraible `City_Pacific` has high P-value, and Coefficient not adding addon Value to the model.\n\nWe would drop City_Pacific and re-run the model.","511489d0":"#### Adding Feature: \n* Distance from Evergreen Bridge (Feature Engineering)\n* Latitutude Below Renton Muncipal Airport.\n\nAs observed from above map that houses near to Evergreen bridge are very high in value creating Feature \"`Distance_Evergreen_bridge`\" using Radians method to calculate distance between bridge and houses.\n\nAlso observed that house prices below Latitude of Renton Muncipal airtport (marked green on Map) are low in values.","8646b024":"### Import of Libraries and Dataset","2ea864dc":"### EDA on Data set.","58d151db":"### Dropping In-valid Datapoints of target variables\n\nWe would inspect target variable in dataset  to see if any datapoint in target variable is holding Null or Zero Values. As Value of any House\/real-estate cannot be Zero.","409344c8":"#### Multi-Collinearity Check.\n\nEvaluating model performance using VIF score.","a3cf1c6c":"Dropping City_Yarrow_Point Due to  High P-value.","57c0a5a1":"### Zip-Codes and Street Address\n\nThis varible is most granular categorical variable, we can treat it by importing Gegraphical Cordinates (Long, Lat) data for given Street name for given Zip Code.","b3b62eae":"**1% of price datapoints is not having values, hence dropping these data points**","c3587783":"#### Under Prediction \/ Over Prediction accross House Price Range.","aec79440":"#### Import of Lat-Long Data for Street address.\n\nThere are two files imported holding geographical details one file contains the Gegraphical Cordinates Data on the basis of  street Address and other file contains cordinates on the basis of Zip-codes. If the Street Geographical data contains null values for cordinates we will replace it with Zip-code Cordinates for the given region.\n\nThe code used to extract Cordinate points are mentioned here. \n\n[link](https:\/\/github.com\/DSPOWER93\/Python_codes\/blob\/main\/Geo_cordinates\/Extract_Geographical_cordinates.ipynb)","538167e7":"**Inference**\n\nThe pattern of prediction is similar in Test data set. As due to High Intercept The model is unable to predict the lower prices houses, Which has impacted the overall accuracy highly.","2674a95c":"### Data Representation Settings","a55ebb93":"#### **Adding Feature:** \n\nHouse Age: Age of House to Present Year (2020)\nRenovation age: Age from last Renovation Done(If the house is not renovated then age of House and age of renovation will be same).\n\nWe will compare the correlation between both the variables and also eliminate variable through VIF if required.","851902d4":"#### Count plot most occured data points for city values. ","a7266673":"#### Under Prediction \/ Over Prediction accross House Price Range in %.","71abfa16":"**Inference**\nSeeing Usual Behaviour of Linear Regression Behaviour the Model is highly Overpredicting upto 500,000 and under predicting the value above 500,00 (just through Visual output) ","121b4114":"**Inference :** We applied cap line of 90th quantile (removing  at 3 Sigma from mean) to find out which of the city prices are above 90th Quantile, As prices of from  these cities will have very impact on the data. There are four Cities which are having Median Price above 90th quantile.  `['Mercer Island', 'Clyde Hill', 'Yarrow Point', 'Medina'] `.\nAlso We will include those cities whose prices are below 10th percentile as not including these variable will not penalize the intercept and model will over predict values for house in these cities. `['Algona', 'Skykomish', 'SeaTac', 'Pacific']`\n\nWe would treat these cities into One hot Encoding as price of these cities would highly impact the house prices.","1618f57a":"**Inference**\n\n\n\n*   Major values of the models are under-predicted.\n*   Values under 300,000 are highly Over-Predicted.\n*   Model Predications are extreme post House prices crossing 3.2 Mlns USD.\n\n","dc3c2ae1":"### Tabulation of Residual Distribution on Test Data.\n\n**Objective:** To see the Residual distribution accross Different House Price Ranges.","e13abc2f":"#### EDA Analysis on City to impact on Price.\n\nVisually evaluating if location on the bsis of City is having impact on the price of house. ","7cb73ebd":"**Inference**\n\nIt is Seen that Few of Price range are Highly Impacting the Mape score(accuracy in Predication) of the Model. Particaularly Ranges:\n1. 0 - 200,000\n2. 2.2 Mlns - 2.3 Mlns\n3. Above 12.8 Mlns\n\nThe Reason for High Mape score at lower values would be that the base Intercept of the model itself is around 400,312 which is bad to predict values for Lower house Price ranges.\n\nAlso above 12.8 Mlns the values are more of Outliers which Model fails to PRedict accurately.","25dde7ba":"# Synopsis\n\nThe following analysis is done to predict house price in State Washington cities Neighbouring Seattle, Washington.\n\n**Total Data Points used:** 4500\n\n**Train: Test ratio** 80:20\n\nPrediction Accuracy:\n* MAPE Train: 24.57%\n* MAPE Test: 22.0%\n\nRegression Approach: Linear Regression.\n\n\n\n","4ef42f20":"##### Through Box Plot","8c20c0f7":"**Inference** Form the above VIF run it was seen that Floors was Collinear to Sqft-above after dropping the variable floor the VIF of SQFT-bove has also dropped.","9a64f2fa":"**Inference**\n\nConsidering the limitation of of the Linear algorithm. the model has been efficient in predicting the in the Data quantile range from ( -2\u03c3 to +2\u03c3). however has not been efficient for  predicting outlier prices. ","07c2c838":"### Inspecting the Dataframe","20ac9e6e":"#### Normalizing the Distribution.\n\nTo Normalize Skewed distribution. We will be applying Log tranformation on Data. ","8be5b59a":"### Visualizing the distribution of Target variable.","f2ff9faa":"### Inpecting the dataset with Duplicate Records\n\nTo see if there are multiple records of same house present in Data, we would be inspecting the granularity of data by Grouping the Dataset count on  `[\"street\",\"date\",\"yr_built\",\"price\"]`. If Countof data has more than one count then we inspect further.","f5111282":"### Country","a9587f18":"#### Visualizing data patterns through Pairplot.","a5af8869":"#### Correlation Score","2b62ea98":"**Inference**: It can be observed that Houses near Evergreen Floating Bridge are very highly valued (Marked in Yellow on Map). Creating variables that measures distance between Floating Bridge and houses could help the model better in predicting house prices, being relative Negative coefficient to Data (Lesser the distance higher the cost).\n\nAlso it is seen that Houses below the latitude of Renton Muncipal Airport (Marked in Yellow on Map) are lower in prices We would create on Hot encodes variable which mark value below Renton Muncipal Airport.","72a2c242":"### Heteroscedasticity of residuals","8eafa2d7":"#### Iteration 1:","c6be5ad7":"**Inference** Negative Correlation Indicates Lesser the distance Higher the House prices","7ed978c5":"QQ Plot ","7abbfdca":"City Seattle is having High P-value(0.542). This show-case the variable is redundant and to be dropped."}}