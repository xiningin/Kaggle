{"cell_type":{"da06a94f":"code","8212337a":"code","61c8b3ec":"code","6deaacb8":"code","d295cdf2":"code","80a0c50a":"code","a7e7b564":"code","d2131a0a":"code","e7899319":"code","e65eb28f":"code","5c5a371f":"code","053d724a":"code","743a896a":"code","af982e23":"code","24f41387":"code","7ceeb9c3":"markdown"},"source":{"da06a94f":"import numpy as np\nimport tensorflow as tf\nimport keras\n\nfrom keras.models import Sequential,load_model\nfrom keras.layers import Dense, Conv2D ,LSTM, MaxPooling2D , Flatten , Dropout , BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping,ReduceLROnPlateau\n\nfrom tensorflow.keras import layers\n\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (15,5)\nimport seaborn as sns\nimport pandas as pd\nimport os\nimport random\nimport time\nimport os\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\nfrom scipy import signal\nfrom scipy.fft import fftshift\n","8212337a":"ACTIONS = [\"kiri\", \"maju\",\"idle\",\"kanan\"]\nreshape = (-1,8, 60)\n\n","61c8b3ec":"def create_data(starting_dir=\"..\/input\/eeg8chanel\/data8\"):\n    training_data = {}\n    for action in ACTIONS:\n        if action not in training_data:\n            training_data[action] = []\n        data_dir = os.path.join(starting_dir,action)\n        for item in os.listdir(data_dir):\n            data = np.load(os.path.join(data_dir, item))\n            for item in data:\n                training_data[action].append(item)\n\n    lengths = [len(training_data[action]) for action in ACTIONS]\n    print(lengths)\n\n    for action in ACTIONS:\n        np.random.shuffle(training_data[action])  \n        training_data[action] = training_data[action][:min(lengths)]\n\n    lengths = [len(training_data[action]) for action in ACTIONS]\n    print(lengths)\n    combined_data = []\n    for action in ACTIONS:\n        for data in training_data[action]:\n            if action == \"kiri\":\n                combined_data.append([data, [1, 0, 0,0]])\n            elif action == \"maju\":\n                combined_data.append([data, [0, 1, 0, 0]])\n            elif action == \"idle\":\n                combined_data.append([data, [0, 0, 1, 0]])\n            elif action == \"kanan\":\n                combined_data.append([data, [0, 0, 0, 1]])\n\n    np.random.shuffle(combined_data)\n    print(\"length:\",len(combined_data))\n    return combined_data\n","6deaacb8":"print(\"creating training data\")\ntraindata = create_data(starting_dir=\"..\/input\/eeg8chanel\/data8\")\ntrain_X = []\ntrain_y = []\n\nfor X, y in traindata:\n    train_X.append(X)\n    train_y.append(y)\n\ntrain_X = np.array(train_X).reshape(reshape)\ntrain_y = np.array(train_y)\n","d295cdf2":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 5, verbose=1,factor=0.5, min_lr=0.0001)","80a0c50a":"def lstm_model():\n    model = Sequential()\n\n    model.add(layers.GRU(64,input_shape = (train_X.shape[1:]), return_sequences=True))\n    model.add(Dropout(0.2))\n    model.add(BatchNormalization())\n\n    model.add(layers.GRU(64,input_shape = (train_X.shape[1:]), return_sequences=True))\n    model.add(Dropout(0.2))\n    model.add(BatchNormalization())\n\n    model.add(layers.GRU(64,input_shape = (train_X.shape[1:]), return_sequences=True))\n    model.add(Dropout(0.2))\n    model.add(BatchNormalization())\n \n    model.add(layers.GRU(64))\n    model.add(Flatten())\n    model.add(Dense(32, activation = \"relu\"))\n    model.add(BatchNormalization())\n    model.add(Dense(4, activation='softmax'))\n   \n  \n    model.compile(optimizer =\"adam\", loss = \"binary_crossentropy\", \n                  metrics = ['accuracy',tf.keras.metrics.AUC(),\n                             tf.keras.metrics.Precision(),\n                             tf.keras.metrics.PrecisionAtRecall(0.5),\n                             tf.keras.metrics.SpecificityAtSensitivity(0.5),\n                             tf.keras.metrics.SensitivityAtSpecificity(0.5)\n\n                             ])\n\n    return model","a7e7b564":"model = lstm_model()\nmodel.summary()","d2131a0a":"#set early stopping criteria\npat = 5\n#this is the number of epochs with no improvment after which the training will stop\nearly_stopping = EarlyStopping(monitor='loss', patience=pat, verbose=1)\n\n#define the model checkpoint callback -> this will keep on saving the model as a physical file\nmodel_checkpoint = ModelCheckpoint('subjek1LSTM.h5', verbose=1, save_best_only=True)\n\n#define a function to fit the model\ndef fit_and_evaluate(t_x, val_x, t_y, val_y, EPOCHS=100, BATCH_SIZE=32):\n    model = None\n    model = lstm_model() \n    results = model.fit(t_x, t_y, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[learning_rate_reduction,early_stopping, model_checkpoint], \n              verbose=1, validation_split=0.1)   \n    print(\"Val Score: \", model.evaluate(val_x, val_y))\n    return results\n    ","e7899319":"n_folds=5\nepochs=100\nbatch_size=32\n\n#save the model history in a list after fitting so that we can plot later\nmodel_history = [] \nfor i in range(n_folds):\n    print(\"Training on Fold: \",i+1)\n    t_x, val_x, t_y, val_y = train_test_split(train_X, train_y, test_size=0.2, \n                                               random_state = np.random.randint(1,1000, 1)[0])\n    model_history.append(fit_and_evaluate(t_x, val_x, t_y, val_y, epochs, batch_size))\n   \n    print(\"=======\"*12, end=\"\\n\\n\\n\")\n","e65eb28f":"\nprint(\"data train x\",t_x.shape)\nprint(\"data train y\",t_y.shape)\n\nprint(\"data tes x\",val_x.shape)\nprint(\"data tes y\",val_y.shape)\n\nmodel = load_model('subjek1LSTM.h5')\nb = model.evaluate(val_x, val_y)\n\nprint('loss',b[0])\nprint('Accuracy',b[1]*100)\nprint('AUC',b[2])\nprint('precision',b[3])\nprint('recall',b[4])\nprint('specificity_at_sensitivity',b[5])\nprint('sensitivity_at_specificity',b[6])","5c5a371f":"fig, (ax1, ax2) =  plt.subplots( ncols=2, sharex=True)\nax1.plot(model_history[0].history['accuracy'], label='Training Fold 1 accuration')\nax1.plot(model_history[1].history['accuracy'], label='Training Fold 2 accuration')\nax1.plot(model_history[2].history['accuracy'], label='Training Fold 3 accuration')\nax1.plot(model_history[3].history['accuracy'], label='Training Fold 4 accuration')\nax1.plot(model_history[4].history['accuracy'], label='Training Fold 5 accuration')\nax1.legend()\nax2.plot(model_history[1].history['loss'], label='Training Fold 1 loss')\nax2.plot(model_history[1].history['loss'], label='Training Fold 2 loss')\nax2.plot(model_history[2].history['loss'], label='Training Fold 3 loss')\nax2.plot(model_history[3].history['loss'], label='Training Fold 4 loss ')\nax2.plot(model_history[4].history['loss'], label='Training Fold 5 loss')\nax2.legend()\nplt.show()","053d724a":"\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[0].history['accuracy'], label='Train Accuracy Fold 1', color='black')\nplt1.plot(model_history[0].history['val_accuracy'], label='Val Accuracy Fold 1', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[0].history['loss'], label='Train Loss Fold 1', color='black')\nplt2.plot(model_history[0].history['val_loss'], label='Val Loss Fold 1', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()\n\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[1].history['accuracy'], label='Train Accuracy Fold 2', color='red')\nplt1.plot(model_history[1].history['val_accuracy'], label='Val Accuracy Fold 2', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[1].history['loss'], label='Train Loss Fold 2', color='red')\nplt2.plot(model_history[1].history['val_loss'], label='Val Loss Fold 2', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()\n\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[2].history['accuracy'], label='Train Accuracy Fold 3', color='green')\nplt1.plot(model_history[2].history['val_accuracy'], label='Val Accuracy Fold 3', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[2].history['loss'], label='Train Loss Fold 3', color='green')\nplt2.plot(model_history[2].history['val_loss'], label='Val Loss Fold 3', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()\n\n\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[3].history['accuracy'], label='Train Accuracy Fold 4', color='blue')\nplt1.plot(model_history[3].history['val_accuracy'], label='Val Accuracy Fold 4', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[3].history['loss'], label='Train Loss Fold 4', color='blue')\nplt2.plot(model_history[3].history['val_loss'], label='Val Loss Fold 4', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()\n\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[4].history['accuracy'], label='Train Accuracy Fold 5', color='purple')\nplt1.plot(model_history[4].history['val_accuracy'], label='Val Accuracy Fold 5', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[4].history['loss'], label='Train Loss Fold 4', color='purple')\nplt2.plot(model_history[4].history['val_loss'], label='Val Loss Fold 4', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()","743a896a":"\nMODEL_NAME ='subjek1LSTM.h5' \n\n#CLIP = True # if your model was trained with np.clip to clip  values\nCLIP = False\nCLIP_VAL = 8  # if above, what was the value +\/-\n\nmodel = tf.keras.models.load_model(MODEL_NAME)\n\nVALDIR = '..\/input\/eeg8chanel\/data8'\nACTIONS =  [\"idle\",\"maju\",\"kiri\", \"kanan\"]\nPRED_BATCH = 32\n\n\ndef get_val_data(valdir, action, batch_size):\n\n   # argmax_dict = {0: 0, 1: 0, 2: 0,3:0}\n    argmax_dict = {2: 0, 0: 0, 1: 0,3:0}\n    raw_pred_dict = {0: 0, 1: 0, 2: 0,3:0}\n\n    action_dir = os.path.join(valdir, action)\n    for session_file in os.listdir(action_dir):\n        filepath = os.path.join(action_dir,session_file)\n        if CLIP:\n            data = np.clip(np.load(filepath), -CLIP_VAL, CLIP_VAL) \/ CLIP_VAL\n            #print(data)\n        else:\n            data = np.load(filepath) \n        preds = model.predict([data.reshape(-1, 8, 60)], batch_size=batch_size)\n        \n        for pred in preds:\n            argmax = np.argmax(pred)\n            argmax_dict[argmax] += 1\n            for idx,value in enumerate(pred):\n                raw_pred_dict[idx] += value\n    \n    argmax_pct_dict = {}\n    for i in argmax_dict:\n        total = 0\n        correct = argmax_dict[i]\n        for ii in argmax_dict:\n            total += argmax_dict[ii]\n        argmax_pct_dict[i] = round(correct\/total, 4)\n    return argmax_dict, raw_pred_dict, argmax_pct_dict\n\n\ndef make_conf_mat(none,left,forward, right):\n    action_dict = {\"idle\":none,\"maju\": forward, \"kiri\": left, \"kanan\": right}\n    action_conf_mat = pd.DataFrame(action_dict)\n    actions = [i for i in action_dict]\n\n    fig = plt.figure(figsize=(15, 6))\n    ax = fig.add_subplot(111)\n    ax.matshow(action_conf_mat, cmap=plt.cm.RdYlGn)\n    ax.set_xticklabels([\"\"]+actions)\n    ax.set_yticklabels([\"\"]+actions)\n\n    for idx, i in enumerate(action_dict):\n        for idx2, ii in enumerate(action_dict[i]):\n            ax.text(idx, idx2, f\"{round(float(action_dict[i][ii]),2)}\", va='center', ha='center')\n    # Rotate the tick labels and set their alignment.\n    \n    plt.title(\"Matrik data\")\n    plt.ylabel(\"Predicted Action\")\n    fig.tight_layout()\n    plt.show()\n\n\n\nforward_argmax_dict, forward_raw_pred_dict, forward_argmax_pct_dict = get_val_data(VALDIR, \"maju\", PRED_BATCH)\nnone_argmax_dict, none_raw_pred_dict, none_argmax_pct_dict = get_val_data(VALDIR, \"idle\", PRED_BATCH)\nleft_argmax_dict, left_raw_pred_dict, left_argmax_pct_dict = get_val_data(VALDIR, \"kiri\", PRED_BATCH)\nright_argmax_dict, right_raw_pred_dict, right_argmax_pct_dict = get_val_data(VALDIR, \"kanan\", PRED_BATCH)\nmake_conf_mat(none_argmax_pct_dict,forward_argmax_pct_dict,left_argmax_pct_dict,  right_argmax_pct_dict)\n\n","af982e23":"x_train,x_test,y_train,y_test=train_test_split(train_X,train_y,test_size=0.2)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","24f41387":"from sklearn.metrics import roc_curve, auc\n\n#y_score = clf.decision_function(xval)\ny_score = model.predict(x_test)\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(4):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n    \n    \n    \nprint(\"roc_auc:\",sum(roc_auc.values())\/4)\nACTIONS =  [\"idle\",\"maju\",\"kiri\", \"kanan\"]\nfor i in range(0,4):\n    plt.figure()\n    fig, ax = plt.subplots(figsize=(10,10))\n    plt.plot(fpr[i], tpr[i],linewidth=3)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([-0.05, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])\n    plt.title('%s -ROC curve (area = %0.2f)' % (ACTIONS[i],roc_auc[i]))\n    \n    plt.legend(loc=\"lower right\")\n    plt.show()\n    ","7ceeb9c3":"# DATA"}}