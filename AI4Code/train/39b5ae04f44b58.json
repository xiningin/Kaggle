{"cell_type":{"61dcf95d":"code","52565da1":"code","ed41abeb":"code","b4e61f64":"code","7db9bef5":"code","6222a217":"code","4278eede":"code","991856a7":"code","93437351":"code","4614ee45":"code","db4627da":"code","76d38fde":"code","cc3bc4b6":"code","f09bc1fd":"code","eda08c7a":"code","71b53ee2":"code","4b452445":"code","668a3b7f":"code","3a316ebe":"code","6394f597":"code","ebd23d5f":"code","09106cc1":"code","ee8a0d45":"code","d1af5499":"code","786a8ff4":"code","3ee8a348":"code","334ec7b4":"code","a638e566":"code","333e41a9":"code","7731ed05":"code","01a35b05":"code","d26452fc":"code","fa849169":"code","a3ef2aaa":"code","e0935614":"code","5e058f66":"code","40cc6b7b":"markdown","6751b2f7":"markdown","202fcad9":"markdown","3aeb200f":"markdown","4e383dda":"markdown","abf12dea":"markdown","7999e74d":"markdown","416af645":"markdown","e73fa154":"markdown","6c42b2d0":"markdown","fa75298a":"markdown","a1f60791":"markdown","d6038c9e":"markdown","cdd613fe":"markdown","d1fe0d94":"markdown","9555114c":"markdown","7aef3411":"markdown"},"source":{"61dcf95d":"import os\nos.listdir('..\/input\/chest-xray-pneumonia\/chest_xray\/train\/')","52565da1":"os.listdir('..\/input\/chest-xray-pneumonia\/chest_xray\/val\/')","ed41abeb":"os.listdir('..\/input\/chest-xray-pneumonia\/chest_xray\/test\/')","b4e61f64":"train_dir='..\/input\/chest-xray-pneumonia\/chest_xray\/train\/'\ntest_dir='..\/input\/chest-xray-pneumonia\/chest_xray\/test\/'\nval_dir='..\/input\/chest-xray-pneumonia\/chest_xray\/val\/'","7db9bef5":"# train \nos.listdir(train_dir)\ntrain_n = train_dir+'NORMAL\/'\ntrain_p = train_dir+'PNEUMONIA\/'","6222a217":"#Normal pic \nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nprint(len(os.listdir(train_n)))\nrand_norm= np.random.randint(0,len(os.listdir(train_n)))\nnorm_pic = os.listdir(train_n)[rand_norm]\nprint('normal picture title: ',norm_pic)\n\nnorm_pic_address = train_n+norm_pic\n\n#Pneumonia\nrand_p = np.random.randint(0,len(os.listdir(train_p)))\n\nsic_pic =  os.listdir(train_p)[rand_norm]\nsic_address = train_p+sic_pic\nprint('pneumonia picture title:', sic_pic)\n\n# Load the images\nnorm_load = Image.open(norm_pic_address)\nsic_load = Image.open(sic_address)\n\n#Let's plt these images\nf = plt.figure(figsize= (10,6))\na1 = f.add_subplot(1,2,1)\nimg_plot = plt.imshow(norm_load)\na1.set_title('Normal')\n\na2 = f.add_subplot(1, 2, 2)\nimg_plot = plt.imshow(sic_load)\na2.set_title('Pneumonia')","4278eede":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","991856a7":"train_generator=ImageDataGenerator(rescale=1\/255,featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.2, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip = False,  # randomly flip images\n        vertical_flip=False)\ntraining_generator=train_generator.flow_from_directory(train_dir,target_size=(224,224),batch_size=4,class_mode='binary')","93437351":"val_generator=ImageDataGenerator(rescale=1\/255.0)\nvalidation_generator=val_generator.flow_from_directory(val_dir,target_size=(224,224),batch_size=12,class_mode='binary')","4614ee45":"test_generator=ImageDataGenerator(rescale=1\/255.0)\ntest_generator=test_generator.flow_from_directory(test_dir,target_size=(224,224,3),batch_size=12,class_mode='binary')","db4627da":"from tensorflow.keras.applications import ResNet50\nfrom tensorflow.python.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, ZeroPadding2D,BatchNormalization","76d38fde":"resnet=ResNet50(input_shape=[224,224,3],weights='imagenet',include_top=False)","cc3bc4b6":"resnet.summary()","f09bc1fd":"for layers in resnet.layers[:50]:\n    layers.trainable=False","eda08c7a":"from tensorflow.keras.layers import Input, Lambda, Dense, Flatten\nx = Flatten()(resnet.output)","71b53ee2":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense\nprediction = Dense(1, activation='sigmoid')(x)\n# create a model object\nmodel = Model(inputs=resnet.input, outputs=prediction)","4b452445":"model.summary()","668a3b7f":"pip install visualkeras","3a316ebe":"import visualkeras\nvisualkeras.layered_view(model, type_ignore=[ ZeroPadding2D,BatchNormalization,Flatten,Dropout])","6394f597":"model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['acc'])","ebd23d5f":"history = model.fit_generator(training_generator,validation_data = validation_generator,epochs = 20, verbose = 1)","09106cc1":"accuracy=history.history['acc']\nloss=history.history['loss']\nval_accuracy=history.history['val_acc']\nval_loss=history.history['val_loss']","ee8a0d45":"epochs = range(len(accuracy))\nepochs","d1af5499":"import matplotlib.pyplot as plt\nplt.plot(epochs,accuracy,'r',label='training_accuracy')\nplt.plot(epochs,val_accuracy,'g',label='val_accuracy')\nplt.legend()\nplt.show()","786a8ff4":"from keras.applications.resnet50 import preprocess_input","3ee8a348":"train_generator_2=ImageDataGenerator(preprocessing_function=preprocess_input,featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.2, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip = False,  # randomly flip images\n        vertical_flip=False)\ntraining_generator_2=train_generator_2.flow_from_directory(train_dir,target_size=(224,224),batch_size=4,class_mode='binary')","334ec7b4":"val_generator_2=ImageDataGenerator(preprocessing_function=preprocess_input)\nvalidation_generator_2=val_generator_2.flow_from_directory(val_dir,target_size=(224,224),batch_size=12,class_mode='binary')","a638e566":"from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau","333e41a9":"early_stopper = EarlyStopping( patience = 10)\n# checkpointer = ModelCheckpoint( monitor = 'val_loss', save_best_only = True, mode = 'auto')\nlearning_rate_reduction=ReduceLROnPlateau(monitor='val_acc',patience=2,verbose=1,factor=0.5,min_lr=0.001)","7731ed05":"hist=model.fit_generator(training_generator_2,validation_data = validation_generator_2,epochs = 20, verbose = 1)","01a35b05":"accuracy_2=hist.history['acc']\nloss_2=hist.history['loss']\nval_accuracy_2=hist.history['val_acc']\nval_loss_2=hist.history['val_loss']","d26452fc":"#plotting training values\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\n\n#accuracy plot\nplt.plot(epochs, acc, color='green', label='Training Accuracy')\nplt.plot(epochs, val_acc, color='blue', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\n\nplt.figure()\n#loss plot\nplt.plot(epochs, loss, color='pink', label='Training Loss')\nplt.plot(epochs, val_loss, color='red', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","fa849169":"import matplotlib.pyplot as plt\nplt.plot(epochs,accuracy_2,'r',label='training_accuracy')\nplt.plot(epochs,val_accuracy_2,'g',label='val_accuracy')\nplt.legend()\nplt.show()","a3ef2aaa":"class_dict = training_generator_2.class_indices\nprint(class_dict)","e0935614":"li = list(class_dict.keys())\nprint(li)","5e058f66":" #predicting an image\n\nfrom keras.preprocessing import image\nimport numpy as np\nimage_path = \"\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/test\/NORMAL\/person96_bacteria_464.jpeg\"\nnew_img = image.load_img(image_path, target_size=(224, 224))\nimg = image.img_to_array(new_img)\nimg = np.expand_dims(img, axis=0)\nimg = img\/255\n\nprint(\"Following is our prediction:\")\nprediction = model.predict(img)\n# decode the results into a list of tuples (class, description, probability)\n# (one such list for each sample in the batch)\nd = prediction.flatten()\nj = d.max()\nfor index,item in enumerate(d):\n    if item == j:\n        class_name = li[index]\nplt.figure(figsize = (4,4))\nplt.imshow(new_img)\nplt.axis('off')\nplt.title(class_name)\nplt.show()","40cc6b7b":"**Normally in all the previous architectures like vgg16 ,vgg19,Alexnet. We uses (rescale=1\/255 ) technique.So i am using that technique here also.**","6751b2f7":"# Image Preprocessing ","202fcad9":"**Look at the validation accuracy ,It Surprisingly increase to more than 90%**","3aeb200f":"**Here the validation accuracy increases to more than 90% i.e. good **","4e383dda":"Look at this Library (\"preprocess_input\") \n\n# Don't rescale your data 1\/255. But rather use the preprocess_input function in your generator.\n**The Keras ResNet pretrained weights are learned on a dataset with different preprocessing than Inception or VGG**","abf12dea":"**Fit the Model**","7999e74d":"Our weights are trained already,so (layers.trainable=False)","416af645":"We don't have as much images for training ,so we use data augmentation technique","e73fa154":"**Now I have Added our one and only dense layer i.e. also our output layer. Always remember in output layer for binary classification ,units=1 and activation='sigmoid'**","6c42b2d0":"# Import pretrained weighted ResNet Architecture of ImageNet","fa75298a":"2 directories in each of the segment","a1f60791":"# VisualKeras\n**Visualkeras is a Python package to help visualize Keras neural network architectures. It allows easy styling to fit most needs. As of now it only supports layered style architecture generation which is great for CNNs (Convolutional Neural Networks).**","d6038c9e":"**Look at the training accuracy and val_accuracy,there is a huge difference between them\nSometimes,it gets val_accuracy gets stuck at a paticular constant value.\nSo,to prevent your model from these problems ,follow the upcoming steps**","cdd613fe":"**Plots for Training Accuracy and validation Accuracy**","d1fe0d94":"**Import all the directories **","9555114c":"Add the Flatten layer","7aef3411":"# ResNet Architecture\n        ** At the ILSVRC 2015, the so-called Residual Neural Network (ResNet) by Kaiming He et al introduced anovel architecture with \u201cskip connections\u201d and features \n        heavy batch normalization. Such skip connections are also known as gated units or gated recurrent units and have a strong similarity to recent successful \n        elements applied in RNNs. Thanks to this technique they were able to train a NN with 152 layers while still having lower complexity than VGGNet. It achieves a \n        top-5 error rate of 3.57% which beats human-level performance on this dataset.**\n        "}}