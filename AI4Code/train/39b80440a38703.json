{"cell_type":{"a3759f46":"code","6f02d84d":"code","6a74cf09":"code","4c23c754":"code","00d021a5":"code","bf8d002d":"code","95acf2b7":"code","50dfb08a":"code","eddae63b":"code","25e06747":"code","044d4824":"code","c1efb131":"code","d67e23f5":"code","800089a3":"code","d615475f":"code","9d419bd3":"code","4a3613e8":"code","079ec76a":"code","5f4b7e16":"code","a5d8083c":"code","fb1c931e":"code","a8a878e0":"code","26163992":"code","75ce63fe":"code","85817112":"code","c816ba42":"code","8f944112":"code","d3a20ae8":"code","c01380e9":"code","f12ec627":"code","7183c223":"code","98f51934":"code","8af44b56":"code","6926d41f":"code","a9a60470":"code","7ed63fa5":"markdown","42394ec1":"markdown","6c4f8946":"markdown","5b5f685a":"markdown","239590e1":"markdown","1e56347d":"markdown","6867cab8":"markdown","0729ac7d":"markdown"},"source":{"a3759f46":"import numpy as np\nimport pandas as pd\nimport os\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom keras.utils import to_categorical\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, random_split, DataLoader\nimport torchvision.transforms as T\nfrom torchvision import models","6f02d84d":"lesion_type_dict = {\n    'nv': 'Melanocytic nevi',\n    'mel': 'dermatofibroma',\n    'bkl': 'Benign keratosis-like lesions ',\n    'bcc': 'Basal cell carcinoma',\n    'akiec': 'Actinic keratoses',\n    'vasc': 'Vascular lesions',\n    'df': 'Dermatofibroma'\n}\nlesion_categorical = {k:i for i, k in enumerate(lesion_type_dict)}","6a74cf09":"DATA_DIR = '..\/input\/skin-cancer-mnist-ham10000'\noriginal_df = pd.read_csv(os.path.join(DATA_DIR, 'HAM10000_metadata.csv'))","4c23c754":"original_df.head()","00d021a5":"original_df['localization'].unique()","bf8d002d":"localization_to_index = {name:i for i, name in enumerate(original_df['localization'].unique())}","95acf2b7":"original_df['lesion_id'].nunique()","50dfb08a":"# Drop dx_type since we don't care about it when predicting dx\ntemp_df = original_df.drop('dx_type', axis = 1)\n# Convert dx to categorical\ntemp_df['dx'] = temp_df['dx'].map(lesion_categorical)\n# Drop null values\ntemp_df = temp_df.dropna()\ntemp_df.head()","eddae63b":"freqs = temp_df['dx'].value_counts()\nfreqs","25e06747":"upsample_rate = [6705 \/\/ freq for freq in freqs]\n\nfor i in range(1, 7):\n    temp_df = temp_df.append([temp_df.loc[temp_df['dx'] == i,:]]*(upsample_rate[i]-1), ignore_index=True)\n    \ntemp_df['dx'].value_counts(normalize=True)","044d4824":"#duplicates = temp_df.groupby('lesion_id')['image_id'].count()\n#duplicates.head()\n#temp_df['has_duplicates'] = temp_df['lesion_id'].apply(\n#    lambda lesion_id: duplicates[lesion_id] > 1\n#)\n#temp_df['has_duplicates'].value_counts()\n#train_df = temp_df[temp_df['has_duplicates']]\n#mixed_df = temp_df[~temp_df['has_duplicates']]\n#train_df['dx'].value_counts(normalize=True)\n#val_df['dx'].value_counts(normalize=True)\n#mixed_df.shape[0], train_df.shape[0]\ntemp_df.shape[0]","c1efb131":"train_df, val_df = train_test_split(temp_df, test_size=0.2)\ntrain_df.shape[0], val_df.shape[0]","d67e23f5":"#train_df = pd.concat([train_df, train_temp], ignore_index=True)\n#train_df.shape[0], val_df.shape[0]","800089a3":"# Drop the has_duplicates column since we don't need it anymore\n#train_df = train_df.drop('has_duplicates', axis = 1)\n#val_df = val_df.drop('has_duplicates', axis = 1).reset_index()","d615475f":"train_df.reset_index(inplace=True, drop=True)\nval_df.reset_index(inplace=True, drop=True)","9d419bd3":"train_df.head()","4a3613e8":"val_df.head()","079ec76a":"# https:\/\/www.kaggle.com\/xinruizhuang\/skin-lesion-classification-acc-90-pytorch\nmean = [0.763038, 0.54564667, 0.57004464]\nstd = [0.14092727, 0.15261286, 0.1699712]","5f4b7e16":"class SkinCancerDataset(Dataset):\n    def __init__(self, df, transforms = None):\n        self.df = df\n        self.transforms = transforms\n        self.part1 = os.path.join(DATA_DIR, 'HAM10000_images_part_1')\n        self.part2 = os.path.join(DATA_DIR, 'HAM10000_images_part_2')\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.loc[idx]\n        image_id = row['image_id']\n        dx = row['dx']\n        features = list(row.loc['age':'localization'])\n        age, sex, localization = features[0], features[1], features[2]\n        sex = 0 if sex == 'male' else 1\n        localization = localization_to_index[localization]\n        sex = to_categorical(sex, num_classes = 2)\n        localization = to_categorical(localization, num_classes = len(localization_to_index))\n        \n        try:\n            img = Image.open(os.path.join(self.part1, image_id + '.jpg'))\n        except FileNotFoundError:\n            img = Image.open(os.path.join(self.part2, image_id + '.jpg'))\n            \n        if self.transforms != None:\n            img = self.transforms(img)\n\n        return  torch.tensor([age], dtype=torch.float32), torch.tensor(sex, dtype=torch.float32),\\\n                torch.tensor(localization, dtype=torch.float32), img, dx","a5d8083c":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","fb1c931e":"device = get_default_device()\ndevice","a8a878e0":"# feature_extract is a boolean that defines if we are finetuning or feature extracting. \n# If feature_extract = False, the model is finetuned and all model parameters are updated. \n# If feature_extract = True, only the last layer parameters are updated, the others remain fixed.\ndef set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False","26163992":"def initialize_model(model_name, num_classes, use_pretrained=True):\n    # Initialize these variables which will be set in this if statement. Each of these\n    #   variables is model specific.\n    \n    assert model_name in ['resnet', 'vgg', 'densenet', 'inception'], 'Invalid model name'\n\n    if model_name == \"resnet\":\n        \"\"\" Resnet18, resnet34, resnet50, resnet101 \"\"\"\n        model_ft = models.resnet50(pretrained=use_pretrained)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n\n\n    elif model_name == \"vgg\":\n        \"\"\" VGG11_bn \"\"\"\n        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n        num_ftrs = model_ft.classifier[6].in_features\n        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n        input_size = 224\n\n\n    elif model_name == \"densenet\":\n        \"\"\" Densenet121 \"\"\"\n        model_ft = models.densenet121(pretrained=use_pretrained)\n        num_ftrs = model_ft.classifier.in_features\n        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n\n    else:\n        \"\"\" Inception v3\n        Be careful, expects (299,299) sized images and has auxiliary output\n        \"\"\"\n        model_ft = models.inception_v3(pretrained=use_pretrained)\n        # Handle the auxilary net\n        num_ftrs = model_ft.AuxLogits.fc.in_features\n        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n        # Handle the primary net\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n        input_size = 299\n\n    return model_ft, input_size","75ce63fe":"model_name = 'densenet'\nmodel_ft, image_size = initialize_model(model_name, 20, use_pretrained=True)","85817112":"class ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        age, sex, localization, image, label = batch\n        out = self(image, age, sex, localization) \n        loss = F.cross_entropy(out, label)      \n        return loss\n    \n    def validation_step(self, batch):\n        age, sex, localization, image, label = batch\n        out = self(image, age, sex, localization)\n        loss = F.cross_entropy(out, label)\n        score = F_score(out, label)\n        return {'val_loss': loss.detach(), 'val_score': score.detach() }\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_scores = [x['val_score'] for x in outputs]\n        epoch_score = torch.stack(batch_scores).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_score': epoch_score.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], last_lr: {:.4f}, train_loss: {:.4f}, val_loss: {:.4f}, val_score: {:.4f}\".format(\n            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_score']))\n","c816ba42":"class MyModel(ImageClassificationBase):\n    def __init__(self, model_name):\n        super().__init__()\n        self.cnn = model_ft\n        self.fc1 = nn.Linear(20 + 1 + 2 + 15, 64)\n        self.fc2 = nn.Linear(64, 16)\n        self.fc3 = nn.Linear(16, 7)\n        \n    def forward(self, image, age, sex, localization):\n        x1 = self.cnn(image)\n        x = torch.cat([x1, age, sex, localization], dim=1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x","8f944112":"train_ds = SkinCancerDataset(train_df, transforms=T.Compose([\n                                                                T.RandomCrop(image_size, padding=8, padding_mode='reflect'),\n                                                                T.ToTensor(),\n                                                                T.Normalize(mean, std, inplace=True)\n                                                            ])\n                            )\nval_ds = SkinCancerDataset(val_df, transforms=T.Compose([\n                                                                T.RandomCrop(image_size, padding=8, padding_mode='reflect'),\n                                                                T.ToTensor(),\n                                                                T.Normalize(mean, std, inplace=True)\n                                                            ])\n                            )","d3a20ae8":"batch_size = 32\n\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True, \n                      num_workers=3, pin_memory=True)\nval_dl = DataLoader(val_ds, batch_size*2, \n                    num_workers=2, pin_memory=True)\n\ntrain_dl = DeviceDataLoader(train_dl, device)\nval_dl = DeviceDataLoader(val_dl, device)","c01380e9":"model = MyModel(model_name)\nmodel = model.to(device)","f12ec627":"batch_size = 1\nimage = torch.randn(batch_size, 3, 224, 224).to(device)\nage = torch.randn(batch_size, 1).to(device)\nsex = torch.randn(batch_size, 2).to(device)\nlocalization = torch.randn(batch_size, 15).to(device)","7183c223":"model(image, age, sex, localization)","98f51934":"def F_score(output, label, beta=1):\n    _, prob = output.max(dim=1)\n    TP = (prob & label).sum().float()\n    TN = ((~prob) & (~label)).sum().float()\n    FP = (prob & (~label)).sum().float()\n    FN = ((~prob) & label).sum().float()\n\n    precision = torch.mean(TP \/ (TP + FP + 1e-12))\n    recall = torch.mean(TP \/ (TP + FN + 1e-12))\n    F2 = (1 + beta**2) * precision * recall \/ (beta**2 * precision + recall + 1e-12)\n    return F2.mean(0)","8af44b56":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\ndef fit_one_cycle(epochs, max_lr, model, freeze, train_loader, val_loader, \n                  weight_decay=0, grad_clip=None, opt_func=optim.Adam):\n    model.train()\n    torch.cuda.empty_cache()\n    history = []\n    \n    # Set up cutom optimizer with weight decay\n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n    # Set up one-cycle learning rate scheduler\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n                                                steps_per_epoch=len(train_loader))\n    # Freeze or Unfreeze\n    set_parameter_requires_grad(model, freeze)\n    for epoch in range(epochs):\n        # Training Phase \n        \n        train_losses = []\n        lrs = []\n        for batch in tqdm(train_loader):\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            \n            # Gradient clipping\n            if grad_clip: \n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n            \n            optimizer.step()\n            optimizer.zero_grad()\n            \n            # Record & update learning rate\n            lrs.append(get_lr(optimizer))\n            sched.step()\n        \n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        result['lrs'] = lrs\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","6926d41f":"# Train all layers\nmodel = MyModel(model_name)\nmodel = model.to(device)\nepochs = 10\nmax_lr = 3e-3\nfreeze = False\nhistory = fit_one_cycle(epochs, \n                        max_lr,\n                        model,\n                        freeze, \n                        train_dl, \n                        val_dl)","a9a60470":"# Train all layers\n#history += fit_one_cycle(epochs,\n#                         max_lr\/10,\n#                         model,\n#                         freeze,\n#                         train_dl,\n#                         val_dl)","7ed63fa5":"for batch in train_dl:\n    age, sex, localization, image, label = batch\n    res = model(image, age, sex, localization)\n    break","42394ec1":"# Training Phase","6c4f8946":"The disease with index 0 has imbalanced the other diseases. Let's perform upsampling to solve this issue.","5b5f685a":"## Decide lengths of train and val dataframes","239590e1":"# Create Dataset","1e56347d":"## Perform upsampling\nThere is a huge class imbalance in the dataset. Let's first demonstrate it and then attempt to solve it by upsampling.","6867cab8":"# Note\n**This step is optional. I found that only class 0 has duplicates. So if you perform it, your validation set will only contain one class, which is way worse than the problem we're trying to avoid. Think of a way to solve it and tell me; thanks!**\n\n## Creating Train and Validation Sets\nWe know that there are duplicate lesion id's in the dataset. This is because each lesion id maps to more than 1 image id. So, if we split right away, duplicates might end up in the train and validation set. To avoid this, perform the following steps:\n1. Create a column that indicates whether a lesion id has duplicates or not\n2. Create training set with rows that have duplicates\n3. Perform a random split on rows without duplicates\n4. The result of the split will be the validation set and another training set that will be concatenated to the original\n","0729ac7d":"# Modeling with Pretrained Networks"}}