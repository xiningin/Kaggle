{"cell_type":{"605ae82a":"code","7dd0267d":"code","ececc292":"code","8d7ca363":"code","626507ae":"code","128c4d26":"code","2ab3e4e8":"code","1b3fcd25":"code","00fd76f6":"code","9fa1f127":"code","4975594e":"code","e2bc01d2":"code","2149b5e6":"code","ba108c8e":"code","aa46929a":"code","3225a685":"code","4252aec8":"code","684389e9":"code","3c4a7890":"code","07b51f2b":"code","40e40f99":"code","35cb7405":"code","46d91536":"code","7e225aaf":"code","0c66f044":"code","7ea9a841":"code","7fc02236":"code","ee1ffcd6":"code","55f9ae7d":"code","84da3d92":"code","ddaf32e0":"code","b5e676aa":"code","dbe7c98c":"code","9ba2a247":"code","a3725d34":"markdown","dfa8b2bf":"markdown","7054431c":"markdown","5395176a":"markdown","6af6b965":"markdown","50a4cac5":"markdown","94213ad5":"markdown","ba3f4741":"markdown","7bf733f7":"markdown","03bf6592":"markdown","1087efe6":"markdown","9a561a2c":"markdown","911f00a2":"markdown","2ad10e5e":"markdown","2eb79e25":"markdown","35ab3723":"markdown","c0c6dff6":"markdown","222523e1":"markdown","94d3628f":"markdown","b91c3877":"markdown","72c688db":"markdown","86934778":"markdown"},"source":{"605ae82a":"import pandas as pd\nimport time\nimport seaborn as sns\ndata_txt = pd.read_csv('..\/input\/income-data\/income_data.txt',sep=\",\",header=None)\ndata_txt.head()","7dd0267d":"data_txt.columns = [\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education.num\",\"marital.status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"capital.gain\",\"capital.loss\",\"hours.per.week\",\"native.country\",\"income\"]\ndata_txt.head()","ececc292":"categorical = [var for var in data_txt.columns if data_txt[var].dtype=='O']\nfor var in categorical: \n       print(data_txt[var].value_counts())","8d7ca363":"data_txt.isnull().sum()","626507ae":"missing_values = [\" ?\"]\ndata_txt.to_csv('a.csv',index=None)\ndata = pd.read_csv(\"a.csv\",na_values = missing_values)\ndata.head()","128c4d26":"data.isnull().sum()","2ab3e4e8":"data['workclass'].fillna(data['workclass'].mode()[0], inplace=True)\ndata['occupation'].fillna(data['occupation'].mode()[0], inplace=True)\ndata['native.country'].fillna(data['native.country'].mode()[0], inplace=True)","1b3fcd25":"data.isnull().sum()","00fd76f6":"from sklearn.preprocessing import LabelEncoder\nenc=LabelEncoder()\ndata['workclass']=enc.fit_transform(data['workclass'])\ndata['marital.status']=enc.fit_transform(data['marital.status'])\ndata['occupation']=enc.fit_transform(data['occupation'])\ndata['relationship']=enc.fit_transform(data['relationship'])\ndata['race']=enc.fit_transform(data['race'])\ndata['sex']=enc.fit_transform(data['sex'])\ndata['native.country']=enc.fit_transform(data['native.country'])\ndata['education']=enc.fit_transform(data['education'])\ndata['income']=enc.fit_transform(data['income'])","9fa1f127":"data.head()","4975594e":"x = data.iloc[:, 0:14]\ny = data.iloc[:, 14]","e2bc01d2":"from sklearn.model_selection import train_test_split\nxtrain, xtest, ytrain, ytest = train_test_split(x, y,train_size = 0.70, test_size = 0.30, random_state = 0) ","2149b5e6":"from sklearn.preprocessing import StandardScaler \nsc_x = StandardScaler() \nxtrain = sc_x.fit_transform(xtrain)  \nxtest = sc_x.transform(xtest) ","ba108c8e":"from sklearn.linear_model import LogisticRegression \nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import accuracy_score\nstart = time.time()\nclassifier = LogisticRegression(random_state = 0) \nclassifier.fit(xtrain, ytrain)\nend = time.time()\nt = end-start\ny_pred = classifier.predict(xtest) \nacc=accuracy_score(ytest, y_pred)\nprint (\"Accuracy : \", accuracy_score(ytest, y_pred))\nprint(\"Time elapsed: \",t)\ncm = confusion_matrix(ytest, y_pred) \nsns.heatmap(cm,annot=True)","aa46929a":"# predict probabilities\ny_pred = classifier.predict(xtest)\n#print(lr_probs)\n# keep probabilities for the positive outcome only\n\n#print(lr_probs)\nns_probs = [0 for _ in range(len(ytest))]\n#print(ns_probs)\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n# calculate scores\nns_auc = roc_auc_score(ytest, ns_probs)\nlr_auc = roc_auc_score(ytest, y_pred)\n# summarize scores\nprint('Random Prediction: ROC AUC=%.3f' % (ns_auc))\nprint('Logistic: ROC AUC=%.3f' % (lr_auc))","3225a685":"import matplotlib.pyplot as plt\nns_fpr, ns_tpr, _ = roc_curve(ytest, ns_probs)\nlr_fpr, lr_tpr, _ = roc_curve(ytest, y_pred)\n# plot the roc curve for the model\n\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='Random Prediction: ROC AUC=%.3f' % (ns_auc))\nplt.plot(lr_fpr, lr_tpr, linestyle='--',marker='*', label='Logistic: ROC AUC=%.3f' % (lr_auc))\n# axis labels\nplt.title('ROC CURVE')\nplt.xlabel('FALSE POSITIVE RATE')\nplt.ylabel('TRUE POSITIVE RATE')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","4252aec8":"from sklearn.naive_bayes import GaussianNB\nstart =time.time()\ngnb = GaussianNB()\ngnb.fit(xtrain, ytrain)\nend = time.time()\nt1=end-start\ny_pred1=gnb.predict(xtest)\nacc1=accuracy_score(ytest, y_pred1)\nprint (\"Accuracy : \", accuracy_score(ytest, y_pred1))\nprint(\"Time elapsed: \",t1)\ncm1 = confusion_matrix(ytest, y_pred1)\nsns.heatmap(cm1,annot=True)","684389e9":"# predict probabilities\ny_pred1 = gnb.predict(xtest)\n#print(lr_probs)\n# keep probabilities for the positive outcome only\n\n#print(lr_probs)\nns_probs = [0 for _ in range(len(ytest))]\n#print(ns_probs)\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n# calculate scores\nns_auc = roc_auc_score(ytest, ns_probs)\nnb_auc = roc_auc_score(ytest, y_pred1)\n# summarize scores\nprint('Random Prediction: ROC AUC=%.3f' % (ns_auc))\nprint('Naive Bayes: ROC AUC=%.3f' % (nb_auc))","3c4a7890":"import matplotlib.pyplot as plt\nns_fpr, ns_tpr, _ = roc_curve(ytest, ns_probs)\nnb_fpr, nb_tpr, _ = roc_curve(ytest, y_pred1)\n# plot the roc curve for the model\n\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='Random Prediction: ROC AUC=%.3f' % (ns_auc))\nplt.plot(lr_fpr, lr_tpr, linestyle='--',marker='*', label='Naive Bayes: ROC AUC=%.3f' % (nb_auc))\n# axis labels\nplt.title('ROC CURVE')\nplt.xlabel('FALSE POSITIVE RATE')\nplt.ylabel('TRUE POSITIVE RATE')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","07b51f2b":"from sklearn.neighbors import  KNeighborsClassifier\nstart = time.time()\nknn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\nknn.fit(xtrain, ytrain)\nend = time.time()\nt2 = end - start\ny_pred2 = knn.predict(xtest)\nacc2=accuracy_score(ytest, y_pred2)\nprint (\"Accuracy : \", accuracy_score(ytest, y_pred2))\nprint(\"Time elapsed: \",t2)\ncm2 = confusion_matrix(ytest, y_pred2) \nsns.heatmap(cm2,annot=True)","40e40f99":"# predict probabilities\ny_pred2 = knn.predict(xtest)\n#print(lr_probs)\n# keep probabilities for the positive outcome only\n\n#print(lr_probs)\nns_probs = [0 for _ in range(len(ytest))]\n#print(ns_probs)\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n# calculate scores\nns_auc = roc_auc_score(ytest, ns_probs)\nkn_auc = roc_auc_score(ytest, y_pred2)\n# summarize scores\nprint('Random Prediction: ROC AUC=%.3f' % (ns_auc))\nprint('KNN: ROC AUC=%.3f' % (kn_auc))","35cb7405":"import matplotlib.pyplot as plt\nns_fpr, ns_tpr, _ = roc_curve(ytest, ns_probs)\nkn_fpr, kn_tpr, _ = roc_curve(ytest, y_pred2)\n# plot the roc curve for the model\n\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='Random Prediction: ROC AUC=%.3f' % (ns_auc))\nplt.plot(lr_fpr, lr_tpr, linestyle='--',marker='*', label='KNN: ROC AUC=%.3f' % (kn_auc))\n# axis labels\nplt.title('ROC CURVE')\nplt.xlabel('FALSE POSITIVE RATE')\nplt.ylabel('TRUE POSITIVE RATE')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","46d91536":"from sklearn.tree import DecisionTreeClassifier\nstart = time.time()\ndecision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)\ndecision_tree = decision_tree.fit(xtrain, ytrain)\nend = time.time()\nt3 = end - start\ny_pred3 = decision_tree.predict(xtest)\nacc3=accuracy_score(ytest, y_pred3)\nprint (\"Accuracy : \", accuracy_score(ytest, y_pred3)) \nprint(\"Time elapsed: \",t3)\ncm3 = confusion_matrix(ytest, y_pred3) \nsns.heatmap(cm3,annot=True)","7e225aaf":"# predict probabilities\ny_pred3 = decision_tree.predict(xtest)\n#print(lr_probs)\n# keep probabilities for the positive outcome only\n\n#print(lr_probs)\nns_probs = [0 for _ in range(len(ytest))]\n#print(ns_probs)\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n# calculate scores\nns_auc = roc_auc_score(ytest, ns_probs)\ndt_auc = roc_auc_score(ytest, y_pred3)\n# summarize scores\nprint('Random Prediction: ROC AUC=%.3f' % (ns_auc))\nprint('Decision Tree: ROC AUC=%.3f' % (dt_auc))","0c66f044":"import matplotlib.pyplot as plt\nns_fpr, ns_tpr, _ = roc_curve(ytest, ns_probs)\ndt_fpr, dt_tpr, _ = roc_curve(ytest, y_pred3)\n# plot the roc curve for the model\n\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='Random Prediction: ROC AUC=%.3f' % (ns_auc))\nplt.plot(lr_fpr, lr_tpr, linestyle='--',marker='*', label='Decision Tree: ROC AUC=%.3f' % (dt_auc))\n# axis labels\nplt.title('ROC CURVE')\nplt.xlabel('FALSE POSITIVE RATE')\nplt.ylabel('TRUE POSITIVE RATE')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","7ea9a841":"from sklearn.ensemble import RandomForestClassifier\nstart = time.time()\nrfc = RandomForestClassifier()\nrfc.fit(xtrain,ytrain)\nend = time.time()\nt4 = end - start\ny_pred4 = rfc.predict(xtest)\nacc4=accuracy_score(ytest, y_pred4)\nprint (\"Accuracy : \", accuracy_score(ytest, y_pred4)) \nprint(\"Time elapsed: \",t4)\ncm4 = confusion_matrix(ytest, y_pred4) \nsns.heatmap(cm4,annot=True)","7fc02236":"# predict probabilities\ny_pred4 = rfc.predict(xtest)\n#print(lr_probs)\n# keep probabilities for the positive outcome only\n\n#print(lr_probs)\nns_probs = [0 for _ in range(len(ytest))]\n#print(ns_probs)\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n# calculate scores\nns_auc = roc_auc_score(ytest, ns_probs)\nrf_auc = roc_auc_score(ytest, y_pred4)\n# summarize scores\nprint('Random Prediction: ROC AUC=%.3f' % (ns_auc))\nprint('Random Forest: ROC AUC=%.3f' % (rf_auc))","ee1ffcd6":"import matplotlib.pyplot as plt\nns_fpr, ns_tpr, _ = roc_curve(ytest, ns_probs)\nrf_fpr, rf_tpr, _ = roc_curve(ytest, y_pred4)\n# plot the roc curve for the model\n\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='Random Prediction: ROC AUC=%.3f' % (ns_auc))\nplt.plot(lr_fpr, lr_tpr, linestyle='--',marker='*', label='Random Forest: ROC AUC=%.3f' % (rf_auc))\n# axis labels\nplt.title('ROC CURVE')\nplt.xlabel('FALSE POSITIVE RATE')\nplt.ylabel('TRUE POSITIVE RATE')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","55f9ae7d":"from sklearn.svm import SVC\nstart = time.time()\nsvm = SVC(kernel = 'rbf', random_state = 0)\nsvm.fit(xtrain, ytrain)\nend = time.time()\nt5 = end-start\ny_pred5 = svm.predict(xtest)\nacc5=accuracy_score(ytest, y_pred5)\nprint (\"Accuracy : \", accuracy_score(ytest, y_pred5))\nprint(\"Time elapsed: \",t5)\ncm5 = confusion_matrix(ytest, y_pred5) \nsns.heatmap(cm5,annot=True)","84da3d92":"# predict probabilities\ny_pred5 = svm.predict(xtest)\n#print(lr_probs)\n# keep probabilities for the positive outcome only\n\n#print(lr_probs)\nns_probs = [0 for _ in range(len(ytest))]\n#print(ns_probs)\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n# calculate scores\nns_auc = roc_auc_score(ytest, ns_probs)\nsv_auc = roc_auc_score(ytest, y_pred5)\n# summarize scores\nprint('Random Prediction: ROC AUC=%.3f' % (ns_auc))\nprint('SVC: ROC AUC=%.3f' % (sv_auc))","ddaf32e0":"import matplotlib.pyplot as plt\nns_fpr, ns_tpr, _ = roc_curve(ytest, ns_probs)\nsv_fpr, sv_tpr, _ = roc_curve(ytest, y_pred5)\n# plot the roc curve for the model\n\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='Random Prediction: ROC AUC=%.3f' % (ns_auc))\nplt.plot(lr_fpr, lr_tpr, linestyle='--',marker='*', label='SVC: ROC AUC=%.3f' % (sv_auc))\n# axis labels\nplt.title('ROC CURVE')\nplt.xlabel('FALSE POSITIVE RATE')\nplt.ylabel('TRUE POSITIVE RATE')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","b5e676aa":"plt.figure(figsize=(15,10))\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='Random Prediction: ROC AUC=%.3f' % (ns_auc))\nplt.plot(lr_fpr, lr_tpr, linestyle='--',marker='*', label='Logistic: ROC AUC=%.3f' % (lr_auc))\nplt.plot(dt_fpr, dt_tpr, linestyle='--',marker='*',label='Deciison Tree: ROC AUC=%.3f' % (dt_auc))\nplt.plot(nb_fpr, nb_tpr, linestyle='--',marker='*',label='Naive Bayes: ROC AUC=%.3f' % (nb_auc))\nplt.plot(rf_fpr, rf_tpr, linestyle='--',marker='*',label='Random Forest: ROC AUC=%.3f' % (rf_auc))\nplt.plot(sv_fpr, sv_tpr, linestyle='--',marker='*',label='Suppot Vector Machines: ROC AUC=%.3f' % (sv_auc))\nplt.plot(kn_fpr, kn_tpr, linestyle='--',marker='*',label='KNN: ROC AUC=%.3f' % (kn_auc))\n# axis labels\nplt.xlabel('FALSE POSITIVE RATE')\nplt.ylabel('TRUE POSITIVE RATE')\nplt.title('ROC CURVES')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","dbe7c98c":"ct = ['LR','NB','KNN','DT','RF','SVC']\nac = [acc,acc1,acc2,acc3,acc4,acc5]\nfig = plt.figure(figsize = (10, 5)) \nplt.bar(ct, ac) \nplt.xlabel(\"Classification Technique\") \nplt.ylabel(\"Accuracy Score\") \nplt.title(\"Accuracy Analysis\")\nplt.show() ","9ba2a247":"ct = ['LR','NB','KNN','DT','RF','SVC']\ntt = [t,t1,t2,t3,t4,t5]\nfig = plt.figure(figsize = (10, 10)) \nplt.bar(ct,tt) \nplt.xlabel(\"Classification Technique\") \nplt.ylabel(\"Time Elapsed in seconds\") \nplt.title(\"Time Elapsed Analysis\") \nplt.show() ","a3725d34":"> Splitting data into separate training and test set of 70% and 30% respectively ","dfa8b2bf":"# 6. Using Support Vector Classification","7054431c":"# Various Classification Analysis","5395176a":"> Exploring Data Categories","6af6b965":" # 5. Using Random Forest Classifier","50a4cac5":"# Data Preprocessing and Cleaning\n> Adding headers\/column labels\n","94213ad5":" # 4. Using Decision Tree Classifier","ba3f4741":"> # Encoding Categorical values using LabelEncoder","7bf733f7":"# Input DataSet","03bf6592":"# Conclusion","1087efe6":"> # Time Elapsed Analysis","9a561a2c":"**Hence,by using above obtained results and analysis, We can conclude that**\n\n* For higher Accuracy Random forest is  better than all. \n* For faster classification Naive Bayes is faster than all.","911f00a2":"> **Replacing null values with mode w.r.t. category**","2ad10e5e":"> # Accuracy Analysis","2eb79e25":" # 3. Using KNN Classifier","35ab3723":"> In this case the missing values are coded as '?'. Python fail to detect these as missing values because it do not consider '?' as missing values\n> So marking '?' as null value","c0c6dff6":"> ***Hence, No null\/missing values.***","222523e1":" # 2. Using Gaussian Naive Bayes Classifier","94d3628f":"> Declaring feature vector and target variable","b91c3877":"# Benchmark Analysis","72c688db":"> # ROC AUC Analysis","86934778":" # 1. Using Logistic Regression Classifier"}}