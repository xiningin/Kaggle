{"cell_type":{"19ba96de":"code","c4be68ea":"code","56f60cb6":"code","1480d098":"code","ea69b90a":"code","ad4561a7":"code","804fa6ff":"code","8d72c666":"code","1d895f7a":"code","60e0cd33":"code","75743fd1":"code","b36b27d1":"code","8f81b4ba":"code","8abde3f1":"code","a2572dd5":"code","be7a72f5":"code","67f0ab37":"code","f5b3716c":"code","5888fd1e":"code","41fee8f2":"code","5e70ce3d":"code","a9f63fe5":"code","1ab9991d":"code","83f61570":"code","9b623416":"code","f8feb5e9":"code","cf231cb3":"code","cadd3fef":"code","c3e33139":"code","15851a21":"code","feb93c0d":"code","30d008f2":"code","f607e088":"code","a8bb9db4":"code","9be5191f":"code","a623dba3":"code","636ef367":"code","de43acfd":"code","50ebf03d":"code","2146c0b9":"code","6860464f":"code","c4fd3a22":"code","226ad659":"code","eb5fa88f":"code","f62c7e20":"code","af2fe5b5":"code","72e1876c":"code","87e9fa36":"code","43c61c48":"code","8c8eaa34":"code","a2106f02":"code","58ba37d8":"code","c9d28c3b":"code","41b26cb3":"code","1c279d8f":"code","aea8abec":"code","7ec388f3":"code","2f867bf0":"code","670d25c7":"code","3a4f28f7":"code","8595e4bc":"code","469abb40":"code","eb028520":"code","9f07fe9d":"markdown","458a50a8":"markdown","41ff8b7a":"markdown","24fe176b":"markdown","1f585aa2":"markdown","7f8e7cda":"markdown","788bda28":"markdown","0a1e5fea":"markdown","50852852":"markdown","91cc0d92":"markdown","9fc24694":"markdown","6c9284c9":"markdown","b3cae87a":"markdown","225ddd2e":"markdown","c84087ae":"markdown","875e1099":"markdown","12a3f0c5":"markdown","42dd7f4b":"markdown","3e075a0e":"markdown","6e2c1e7a":"markdown","70ec1b17":"markdown","44b83c29":"markdown","d608eb70":"markdown","bcb74111":"markdown","750429b2":"markdown","17d6affc":"markdown","75682a45":"markdown","21d89cea":"markdown","20fc12e3":"markdown","7dc3a32d":"markdown","2abd9ae1":"markdown","472ef9bd":"markdown","23918e82":"markdown","1f1c0b42":"markdown","72045224":"markdown","06a648d4":"markdown"},"source":{"19ba96de":"!pip install emoji\n!pip install lazypredict\n!pip install keras\n!pip install sklearn","c4be68ea":"!conda install -c conda-forge hdbscan -y","56f60cb6":"##################################################\n# Imports\n##################################################\n\nimport numpy as np\nimport cv2\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport emoji\nfrom lazypredict.Supervised import LazyClassifier\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report, plot_confusion_matrix\nfrom tqdm import tqdm\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom tensorflow.keras import regularizers, initializers, optimizers, callbacks\nfrom keras.utils.np_utils import to_categorical\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model, Sequential\nfrom keras.initializers import Constant\nfrom keras.regularizers import l2\nfrom sklearn.model_selection import StratifiedKFold\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import load_model\nimport seaborn as sns\nimport hdbscan\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report\nimport pandas as pd\nimport seaborn as sns\nfrom itertools import product\n##################################################\n# Params\n##################################################\n\nDATA_BASE_FOLDER = '\/kaggle\/input\/emojify-challenge'\n\n\n##################################################\n# Utils\n##################################################\n\ndef label_to_emoji(label):\n    \"\"\"\n    Converts a label (int or string) into the corresponding emoji code (string) ready to be printed\n    \"\"\"\n    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)\n\n# Use this function to evaluate your model\ndef accuracy(y_pred, y_true):\n    '''\n    input y_pred: ndarray of shape (N,)\n    input y_true: ndarray of shape (N,)\n    '''\n    return (1.0 * (y_pred == y_true)).mean()","1480d098":"##################################################\n# Load dataset\n##################################################\n\ndf_train = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'train.csv'))\ny_train = df_train['class']\ndf_validation = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'validation.csv'))\ny_validation = df_validation['class']\nemoji_dictionary = {\n    '0': '\\u2764\\uFE0F',\n    '1': ':baseball:',\n    '2': ':smile:',\n    '3': ':disappointed:',\n    '4': ':fork_and_knife:'\n}\n\n# See some data examples\nprint('EXAMPLES:\\n####################')\nfor idx in range(10):\n    print(y_train[idx])\n    print(f'{df_train[\"phrase\"][idx]} -> {label_to_emoji(y_train[idx])}')","ea69b90a":"df_train.shape","ad4561a7":"# Load phrase representation\nx_train = np.load(\n    os.path.join(DATA_BASE_FOLDER, \n                 'train.npy')).reshape(len(df_train), -1)\nx_validation = np.load(\n    os.path.join(DATA_BASE_FOLDER, \n                 'validation.npy')).reshape(len(df_validation), -1)\nprint(f'Word embedding size: {x_train.shape[-1]}')\n\npd.DataFrame(x_train)","804fa6ff":"from scipy import stats\ntrain_df = pd.DataFrame(x_train)\n\nnon_normal_features = list()\n\nfor i in range(0, train_df.shape[1]):\n    shapiro_test = stats.shapiro(train_df.iloc[:, i]) # returns a tuple with two values: the test statistic and the pvalue\n    if shapiro_test[1] < 0.05:\n        non_normal_features.append(i)\n        \nprint(\"Number of non normalized features: \", len(non_normal_features))","8d72c666":"import random\n\nrandom_features = random.sample(non_normal_features, 10)\n\n\nfig, axs = plt.subplots(2, 5)\nrow = 0\ncol = 0\nfor _, i in enumerate(random_features):\n    \n    if col > 4:\n        row = 1\n        col = 0\n        \n    axs[row, col].hist(train_df.iloc[:, i], align='mid', alpha=0.6)\n    \n    col = col + 1\n    \nfor ax in axs.flat:\n    ax.set(xlabel='Values', ylabel='Count')\n    \nfig.savefig(\"random_non_normal_features.png\")","1d895f7a":"f = plt.figure(figsize=(15,5))\nf.add_subplot(1,2,1)\nsns.countplot(y_train).set_title('Frequency of classes on response variable (Training Set)')\nf.add_subplot(1,2,2)\nsns.countplot(y_validation).set_title('Frequency of classes on response variable (Validation Set)')","60e0cd33":"def separate_words(x,y):\n    x_new = x.ravel().reshape(-1,25)\n    y_new = y.repeat(10).reset_index(drop=True)\n    return (x_new, y_new)\n\nx, y = separate_words(x_train, y_train)\n\nwords_index=np.where(~np.all(np.isclose(x, 0), axis=1))\nclusterer = hdbscan.HDBSCAN(min_cluster_size=6)\nclusterer.fit(x[words_index])\nlabels = clusterer.labels_\ndf = pd.DataFrame({'cluster':labels, 'emoji':y.iloc[words_index]})\ndf['words']=1\nsummary = df.groupby(['cluster', 'emoji']).agg('count')\npd.pivot_table(df, index='cluster', columns='emoji', aggfunc=lambda x: len(x), margins=False).iloc[1:].plot.barh(stacked=True, figsize=(15,10), title='Clustering of words in training set')","75743fd1":"from collections import Counter\ndef separate_words(x,y):\n    x_new = x.ravel().reshape(-1,25)\n    y_new = y.repeat(10).reset_index(drop=True)\n    return (x_new, y_new)\n\nx, y = separate_words(x_train, y_train)\n\nwords_index=np.where(~np.all(np.isclose(x, 0), axis=1))\nwords_occurence = pd.DataFrame(Counter(map(tuple, x[words_index])).items(), columns=['Number of words', 'Occurrence']).groupby('Occurrence').count().reset_index()\nplt.figure(figsize=(12,6))\nsns.lineplot(data=words_occurence, x='Occurrence', y='Number of words').set_title('Words occurence distribution')","b36b27d1":"from nltk.tokenize import sent_tokenize\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\n\ntokenizer = RegexpTokenizer(r'\\w+')\n\n# Obtaining text\ntrain_text = '. '.join(df_train['phrase'])\nvalidation_text = '. '.join(df_validation['phrase'])\n\n# Obtaining the tokens aka every syntatical element in the sentences (numbers, stopwords, words..)\ntrain_words = tokenizer.tokenize(train_text)\nvalidation_words = tokenizer.tokenize(validation_text)\n\n# Removing commas and question mark and replacing them with a point in order to split the sentences\ntrain_sentences = train_text.replace(\",\", \".\").replace(\"?\", \".\").split(\".\")\nvalidation_sentences = validation_text.replace(\",\", \".\").replace(\"?\", \".\").split(\".\")\n\n# Obtaining the length of each sentence for both datasets\ntrain_sent_lenghs =[len(tokenizer.tokenize(sentence)) for sentence in train_sentences]\nvalidation_sent_lenghs =[len(tokenizer.tokenize(sentence)) for sentence in validation_sentences]\n\n# Storing the lengths previously obtained\ntrain_sent_len = [i for i in train_sent_lenghs if i!=0]\nvalidation_sent_len = [i for i in validation_sent_lenghs if i!=0]\n\n# Plotting an histogram of the lengths-\nplt.hist(train_sent_len, bins=range(min(train_sent_len), max(train_sent_len) + 1, 1),\n              alpha=0.4, color=\"blue\")\n\nplt.hist(validation_sent_len, bins=range(min(validation_sent_len), max(validation_sent_len) + 1, 1), \n              alpha=0.4, color=\"red\")\n\nlabels = ['Train',\"Validation\"]\nplt.legend(labels)\nplt.xlabel(\"Sentence Length\")\nplt.ylabel(\"Count\")\nplt.title(\"Comparing number of words per sentence distribution in Train and Validation sentences\")\n\nfig.savefig(\"train_validation_sentences_length.png\")","8f81b4ba":"from sklearn.linear_model import LogisticRegression\n\n# Report the accuracy in the train and validation sets.\nfor c in (0.8, 0.5, 0.1,0.05, 0.01,0.001):\n    log_class = LogisticRegression(fit_intercept=False, C=c)\n    log_class.fit(x_train, y_train)\n    y_pred = log_class.predict(x_validation)\n    print(f'c = {c} accuracy={accuracy(y_pred, y_validation)}')","8abde3f1":"log_class = LogisticRegression(fit_intercept=False, C=0.5)\nlog_class.fit(x_train, y_train)\ny_pred = log_class.predict(x_validation)\nprint(f'c={c} accuracy={accuracy(y_pred, y_validation)}')\nprint(classification_report(y_validation, y_pred, ))\nplot_confusion_matrix(log_class, x_validation, y_validation,)","a2572dd5":"from numpy.linalg import norm\n\nclass SmartChebyshev():\n    \n    def __repr__(self): return 'smart_chebyshev'\n    \n    def _extract_words(self, tweet):\n        all_words = tweet.reshape(-1,25)\n        return all_words[np.where(~np.all(np.isclose(all_words, 0), axis=1))]\n    \n    def _distance(self, w1, w2):\n        a = self._extract_words(w1)\n        b = self._extract_words(w2)\n        d = list()\n        for x, y in product(a, b):\n            d.append(norm(x-y))\n        return max(d)\n    \n    def __call__(self, w1, w2):\n        return self._distance(w1, w2)\n\nsc = SmartChebyshev()\n\ndef test_knn(k=5, metric='euclidean', verbose=False):\n    knn = KNeighborsClassifier(n_neighbors=k, metric=metric)\n    knn.fit(x_train, y_train)\n    \n    y_pred = knn.predict(x_validation)\n    \n    if verbose:\n        print(f'Accuracy {accuracy(y_pred, y_validation)}')\n        print(classification_report(y_validation, y_pred, ))\n        return plot_confusion_matrix(knn, x_validation, y_validation)\n    return accuracy(y_pred, y_validation)\n\nres = list()\n\nfor k, m in product(range(1,11), ('euclidean', 'manhattan', 'chebyshev', sc)):\n    res.append((k, m,  test_knn(k, metric=m)))\n    \nknn_tuning = pd.DataFrame(res, columns=['k', 'metric', 'accuracy'])\nprint(knn_tuning)\nplt.figure(figsize=(10,5))\nsns.lineplot(data=knn_tuning, x=\"k\", y=\"accuracy\", hue='metric')","be7a72f5":"test_knn(4,'euclidean', verbose=True)","67f0ab37":"x_test = np.load(os.path.join(DATA_BASE_FOLDER, 'test.npy'))\n\n# Reshaping feature datasets from 3D to 2D\nx_validation_reshaped = x_validation.reshape((-1, 250))\nx_test_reshaped = x_test.reshape((-1, 250))\n\n# Keras requires one hot encoded outputs, so we prepare the corresponding format\ncategorical_y_train = to_categorical(y_train)\ncategorical_y_validation = to_categorical(y_validation)","f5b3716c":"def print_training_metrics(model):\n    \"\"\"\n        Prints Accuracy and Validation Accuracy from the training procedure.\n    \"\"\"\n    \n    plt.plot(model.history['accuracy'])\n    plt.plot(model.history['val_accuracy'])\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    plt.show()\n\n    plt.plot(model.history['loss'])\n    plt.plot(model.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    plt.show()","5888fd1e":"from keras.utils.vis_utils import plot_model","41fee8f2":"naive_model = Sequential()\nnaive_model.add(Dense(64, activation = \"relu\", input_dim=250))\nnaive_model.add(Dropout(0.5))\nnaive_model.add(Dense(5, activation='softmax'))\nnaive_model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nnaive_model.summary()","5e70ce3d":"history = naive_model.fit(x_train, categorical_y_train, epochs=50)","a9f63fe5":"y_pred = naive_model.predict(x_validation_reshaped)\ny_pred = list(map(lambda x: np.argmax(x), y_pred))\naccuracy(y_pred, y_validation)","1ab9991d":"# Defining the number of folds\nK = 10\n\n# Defining the loss and accuracy for each fold\nK_fold_accuracy = list()\nK_fold_loss = list()\nK_fold_training_evolutions = [] # It will result in a list where each element has two nested lists, corresponding to the evolutions of both loss and accuracy during the training.","83f61570":"# We have to merge the train and validation dataset since sklearn's k-fold function will prepare the folds by itself.\nfeatures = np.concatenate((x_train, x_validation), axis = 0)\ntargets = np.concatenate((categorical_y_train, categorical_y_validation), axis = 0)\n\n# Creating the Stratifield KFold object with the number of splits equal to K\nskf = StratifiedKFold(n_splits = K, shuffle = True, random_state = 42) # 42 is our seed in order to provide reproducible results\n\nn_fold = 1\n\n\ncheckpoint = ModelCheckpoint(os.path.join(\".\/best_nn.h5\"), \n    monitor='val_accuracy', verbose=1, \n    save_best_only=True, mode='max')\ncallbacks_list = [checkpoint]\n    \nfor train, test in skf.split(features, targets.argmax(1)):\n    \n    naive_model = Sequential()\n    naive_model.add(Dense(64, activation = \"relu\", input_dim=250))\n    naive_model.add(Dropout(0.5))\n    naive_model.add(Dense(5, activation='softmax'))\n    naive_model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n\n    \n    print('------------------------------------------------------------------------')\n    print(f'Training for fold {n_fold} ...')  \n    history = naive_model.fit(features[train], targets[train],\n              batch_size=32,\n              epochs=50,\n              verbose=1,\n              callbacks = callbacks_list,\n              validation_data=(features[test], targets[test]))\n    \n    # Generate generalization metrics\n    scores = naive_model.evaluate(features[test], targets[test], verbose=0)\n    print(f'Score for fold {n_fold}: {naive_model.metrics_names[0]} of {scores[0]}; {naive_model.metrics_names[1]} of {scores[1]*100}%')\n    K_fold_accuracy.append(scores[1] * 100)\n    K_fold_loss.append(scores[0])\n    \n    K_fold_training_evolutions.append(history)\n    \n    # Increase fold number\n    n_fold = n_fold + 1","9b623416":"print('Average scores for all folds:')\nprint(f'> Accuracy: {np.mean(K_fold_accuracy)} (+- {np.std(K_fold_accuracy)})')\nprint(f'> Loss: {np.mean(K_fold_loss)}')\nprint('------------------------------------------------------------------------')","f8feb5e9":"# Plots Train and Validation accuracy for each fold\ndef show_k_fold_accuracy(history_list, K):\n    plt.figure(figsize=(15,8))\n    plt.title('Train and Validation Accuracy by Epochs')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    \n    palette = [\"#fff100\", \"#ff8c00\", \"#e81123\", \"#ec008c\", \"#68217a\", \"#00188f\", \"#00bcf2\", \"#00b294\", \"#009e49\", \"#bad80a\"]\n    \n    for i in range(0, K):\n        plt.plot(history_list[i].history['accuracy'], label = 'Train Accuracy Fold ' + str(i+1), color = palette[i])\n        plt.plot(history_list[i].history['val_accuracy'], label = 'Validation Accuracy Fold ' + str(i+1), color = palette[i], linestyle = \"dashdot\")\n\n        plt.legend()\n        \n# Plots Train and Validation accuracy for each fold\ndef show_k_fold_loss(history_list, K):\n    plt.figure(figsize=(15,8))\n    plt.title('Train and Validation Loss by Epochs')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    \n    palette = [\"#fff100\", \"#ff8c00\", \"#e81123\", \"#ec008c\", \"#68217a\", \"#00188f\", \"#00bcf2\", \"#00b294\", \"#009e49\", \"#bad80a\"]\n    \n    for i in range(0, K):\n        plt.plot(history_list[i].history['loss'], label = 'Train Accuracy Loss ' + str(i+1), color = palette[i])\n        plt.plot(history_list[i].history['val_loss'], label = 'Validation Accuracy Loss ' + str(i+1), color = palette[i], linestyle = \"dashdot\")\n\n        plt.legend()","cf231cb3":"show_k_fold_accuracy(K_fold_training_evolutions, K)\nshow_k_fold_loss(K_fold_training_evolutions, K)","cadd3fef":"naive_model = load_model('.\/best_nn.h5')\nnaive_model.summary()","c3e33139":"y_pred = naive_model.predict(x_validation_reshaped)\ny_pred = list(map(lambda x: np.argmax(x), y_pred))\naccuracy(y_pred, y_validation)","15851a21":"lambda_l = 0.02\n# Defining the loss and accuracy for each fold\nK_fold_accuracy = list()\nK_fold_loss = list()\nK_fold_training_evolutions = [] # It will result in a list where each element has two nested lists, corresponding to the evolutions of both loss and accuracy during the training.\n\n# We have to merge the train and validation dataset since sklearn's k-fold function will prepare the folds by itself.\nfeatures = np.concatenate((x_train, x_validation), axis = 0)\ntargets = np.concatenate((categorical_y_train, categorical_y_validation), axis = 0)\n\n# Creating the Stratifield KFold object with the number of splits equal to K\nskf = StratifiedKFold(n_splits = K, shuffle = True, random_state = 42) # 42 is our seed in order to provide reproducible results\n\nn_fold = 1\n\ncheckpoint = ModelCheckpoint(os.path.join(\".\/best_nn_reg.h5\"), \n    monitor='val_accuracy', verbose=1, \n    save_best_only=True, mode='max')\ncallbacks_list = [checkpoint]\n    \nfor train, test in skf.split(features, targets.argmax(1)):\n    \n    naive_model_regularized = Sequential()\n    naive_model_regularized.add(Dense(64, activation = \"relu\", input_dim=250, kernel_regularizer = l2(lambda_l)))\n    naive_model_regularized.add(Dropout(0.5))\n    naive_model_regularized.add(Dense(5, activation='softmax'))\n    naive_model_regularized.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n    naive_model_regularized.summary()\n\n\n    print('------------------------------------------------------------------------')\n    print(f'Training for fold {n_fold} ...')  \n    history = naive_model_regularized.fit(features[train], targets[train],\n              batch_size=32,\n              epochs=50,\n              verbose=1,\n              callbacks = callbacks_list,\n              validation_data=(features[test], targets[test]))\n    \n    # Generate generalization metrics\n    scores = naive_model_regularized.evaluate(features[test], targets[test], verbose=0)\n    print(f'Score for fold {n_fold}: {naive_model_regularized.metrics_names[0]} of {scores[0]}; {naive_model_regularized.metrics_names[1]} of {scores[1]*100}%')\n    K_fold_accuracy.append(scores[1] * 100)\n    K_fold_loss.append(scores[0])\n    \n    K_fold_training_evolutions.append(history)\n    \n    # Increase fold number\n    n_fold = n_fold + 1\n    \n","feb93c0d":"print('Average scores for all folds:')\nprint(f'> Accuracy: {np.mean(K_fold_accuracy)} (+- {np.std(K_fold_accuracy)})')\nprint(f'> Loss: {np.mean(K_fold_loss)}')\nprint('------------------------------------------------------------------------')","30d008f2":"show_k_fold_accuracy(K_fold_training_evolutions, K)\nshow_k_fold_loss(K_fold_training_evolutions, K)","f607e088":"naive_model_regularized = load_model('.\/best_nn_reg.h5')\nnaive_model_regularized.summary()","a8bb9db4":"# We have to merge the train and validation dataset since sklearn's k-fold function will prepare the folds by itself.\nfeatures = np.concatenate((x_train, x_validation), axis = 0)\ntargets = np.concatenate((categorical_y_train, categorical_y_validation), axis = 0)\n\nnaive_model_regularized = Sequential()\nnaive_model_regularized.add(Dense(64, activation = \"relu\", input_dim=250, kernel_regularizer = l2(lambda_l)))\nnaive_model_regularized.add(Dropout(0.5))\nnaive_model_regularized.add(Dense(5, activation='softmax'))\nnaive_model_regularized.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nhistory = naive_model_regularized.fit(features, targets,\n              batch_size=32,\n              epochs=50,\n              verbose=1)","9be5191f":"features.shape","a623dba3":"y_pred = naive_model_regularized.predict(x_validation_reshaped)\ny_pred = list(map(lambda x: np.argmax(x), y_pred))\naccuracy(y_pred, y_validation)","636ef367":"# NN submission\n\n# Create submission\nsubmission = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'sample_submission.csv'), dtype=str)\nsubmission['class'] = submission['class'].astype(\"str\")\nx_test = np.load(os.path.join(DATA_BASE_FOLDER, 'test.npy')).reshape(len(submission), -1)\n\ny_test_pred = naive_model_regularized.predict(x_test)\ny_test_pred = list(map(lambda x: np.argmax(x), y_test_pred))\ny_test_pred = list(map(str, y_test_pred))\n\nif y_test_pred is not None:\n    submission['class'] = y_test_pred\n\nsubmission['class'] = submission['class'].astype(\"str\")\nsubmission = submission[['Unnamed: 0', 'class']]\nsubmission = submission.rename(columns={'Unnamed: 0': 'id'})\n\nsubmission.to_csv('my_submission_nn.csv', index=False)","de43acfd":"model_dnn = Sequential()\nmodel_dnn.add(Dense(64, activation=\"relu\", input_dim=250))\nmodel_dnn.add(Dropout(0.5))\nmodel_dnn.add(Dense(32, activation=\"relu\"))\nmodel_dnn.add(Dense(5, activation=\"softmax\"))\nmodel_dnn.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nmodel_dnn.summary()\nplot_model(model_dnn, to_file='dnn_architecture.png', show_shapes=True, show_layer_names=True)","50ebf03d":"# Defining the loss and accuracy for each fold\nK_fold_accuracy = list()\nK_fold_loss = list()\nK_fold_training_evolutions = [] # It will result in a list where each element has two nested lists, corresponding to the evolutions of both loss and accuracy during the training.\n\n# Creating the Stratifield KFold object with the number of splits equal to K\nskf = StratifiedKFold(n_splits = K, shuffle = True, random_state = 42) # 42 is our seed in order to provide reproducible results\n\nn_fold = 1\n\ncheckpoint = ModelCheckpoint(os.path.join(\".\/best_dnn.h5\"), \n    monitor='val_accuracy', verbose=1, \n    save_best_only=True, mode='max')\ncallbacks_list = [checkpoint]\n\nfor train, test in skf.split(features, targets.argmax(1)):\n    \n    model_dnn = Sequential()\n    model_dnn.add(Dense(64, activation=\"relu\", input_dim=250))\n    model_dnn.add(Dropout(0.5))\n    model_dnn.add(Dense(32, activation=\"relu\"))\n    model_dnn.add(Dense(5, activation=\"softmax\"))\n    model_dnn.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n\n    print('------------------------------------------------------------------------')\n    print(f'Training for fold {n_fold}')  \n    history = model_dnn.fit(features[train], targets[train],\n              batch_size=32,\n              epochs=50,\n              verbose=1,\n              callbacks = callbacks_list,\n              validation_data=(features[test], targets[test]))\n    \n    # Generate generalization metrics\n    scores = model_dnn.evaluate(features[test], targets[test], verbose=0)\n    print(f'Score for fold {n_fold}: {model_dnn.metrics_names[0]} of {scores[0]}; {model_dnn.metrics_names[1]} of {scores[1]*100}%')\n    K_fold_accuracy.append(scores[1] * 100)\n    K_fold_loss.append(scores[0])\n    K_fold_training_evolutions.append(history)\n    print(history.history.keys())   \n    # Increase fold number\n    n_fold = n_fold + 1\n","2146c0b9":"print('Average scores for all folds:')\nprint(f'> Accuracy: {np.mean(K_fold_accuracy)} (+- {np.std(K_fold_accuracy)})')\nprint(f'> Loss: {np.mean(K_fold_loss)}')\nprint('------------------------------------------------------------------------')\n\nshow_k_fold_accuracy(K_fold_training_evolutions, K)\nshow_k_fold_loss(K_fold_training_evolutions, K)","6860464f":"model_dnn = load_model('.\/best_dnn.h5')\nmodel_dnn.summary()","c4fd3a22":"y_pred = model_dnn.predict(x_validation_reshaped)\ny_pred = list(map(lambda x: np.argmax(x), y_pred))\naccuracy(y_pred, y_validation)","226ad659":"model_dnn = Sequential()\nmodel_dnn.add(Dense(64, activation=\"relu\", input_dim=250))\nmodel_dnn.add(Dropout(0.5))\nmodel_dnn.add(Dense(32, activation=\"relu\"))\nmodel_dnn.add(Dense(5, activation=\"softmax\"))\nmodel_dnn.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nhistory = model_dnn.fit(features, targets,\n              batch_size=32,\n              epochs=50,\n              verbose=1)","eb5fa88f":"# DNN submission\n\n# Create submission\nsubmission = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'sample_submission.csv'))\nx_test = np.load(os.path.join(DATA_BASE_FOLDER, 'test.npy')).reshape(len(submission), -1)\n\ny_test_pred = model_dnn.predict(x_test)\ny_test_pred = list(map(lambda x: np.argmax(x), y_test_pred))\n\nif y_test_pred is not None:\n    submission['class'] = y_test_pred\n\nsubmission = submission[['Unnamed: 0', 'class']]\nsubmission = submission.rename(columns={'Unnamed: 0': 'id'})\nsubmission.to_csv('my_submission_dnn.csv', index=False)","f62c7e20":"#### Loading the GloVe word embedding.\n\n# Directory of the embedding file\nfilename = '..\/input\/glove-global-vectors-for-word-representation\/glove.twitter.27B.25d.txt'\n\n# Dictionary where the key is the word and the value its internal index\nindex_to_words = {}\n# Dictionary where the key is the index and the value its word \nwords_to_index = {}\n# Dictionary of word embeddings, where the key is the word and the value its embedding\nword_embeddings = {}\n\ni = 1\nwith open(filename, 'r') as f:\n    words = set() # To avoid any kind of duplicates\n    \n    # Storing words and their embeddings\n    for line in f:\n        line = line.strip().split()\n        current_word = line[0]\n        words.add(current_word)\n        word_embeddings[current_word] = np.array(line[1:], dtype=np.float64)\n\n    # Performing the mapping between word and index\n    for w in sorted(words):\n        words_to_index[w] = i\n        index_to_words[i] = w\n        i = i + 1\n        \n    ","af2fe5b5":"def convert_word_sentences_to_index_sentences(X, word_to_index, max_len):\n    \"\"\"\n        This function takes a matrix of sentences, the mapping between word and indices and the maximum sentence length, and returns\n        a matrix with the same shape as the one given in input but with the GloVe index in substition for each word.\n        \n        Input:\n            - X: sentence matrix\n            - word_to_index: dictionary of mappings between words and GloVe index\n            - max_len: maximum sentence length\n            \n        Returns:\n            - sentence_indices: matrix with the same shape as the one given as input, but with GloVe indices in substitution of the words.\n    \"\"\"\n    n_rows = X.shape[0]\n    sentence_indices = np.zeros((n_rows, max_len))\n    \n    # For each row, we perform the mapping by obtaining the words and, consecutively, their corresponding indices through mappings.\n    for i in range(n_rows):\n        sentence_words = X[i].lower().split()\n        j = 0\n        for w in sentence_words:\n            sentence_indices[i, j] = word_to_index[w]\n            j = j + 1\n            \n    return sentence_indices","72e1876c":"# Preparing the embedding matrix where each row corresponds to the index assigned during the GloVe mapping, \n# and the 25 columns will contain its embedding representation.\n\nnumber_of_words = len(words_to_index.keys())\nembedded_matrix = np.zeros((number_of_words, 25))\n    \nprint(embedded_matrix.shape)\n\nfor word, index in words_to_index.items():\n    try:\n        embedded_matrix[index, :] = word_embeddings[word]\n    except:\n        continue","87e9fa36":"features = np.concatenate((convert_word_sentences_to_index_sentences(df_train['phrase'].values, words_to_index, 10), convert_word_sentences_to_index_sentences(df_validation['phrase'].values, words_to_index, 10)), axis = 0)\ntargets = np.concatenate((to_categorical(y_train), to_categorical(y_validation)), axis = 0)","43c61c48":"embedding_layer = Embedding(number_of_words, 25, embeddings_initializer=Constant(embedded_matrix), input_length=10 , trainable=False)\nsequence_input = Input(shape=(10,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\n\nconvs = []\nfilter_sizes = [3,4,5]\n\nfor filter_size in filter_sizes:\n    l_conv = Conv1D(filters=32, \n                    kernel_size=filter_size, \n                    activation='relu')(embedded_sequences)\n    l_pool = GlobalMaxPooling1D()(l_conv)\n    convs.append(l_pool)\n\nconv_layers = concatenate(convs, axis=1)\nx = Dropout(0.5)(conv_layers)  \n\npredictions = Dense(5, activation='softmax')(x)\nmodel_cnn = Model(sequence_input, predictions)\nmodel_cnn.compile(loss='categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\nplot_model(model_cnn, to_file='cnn_architecture.png', show_shapes=True, show_layer_names=True)","8c8eaa34":"# Defining the loss and accuracy for each fold\nK_fold_accuracy = list()\nK_fold_loss = list()\nK_fold_training_evolutions = [] # It will result in a list where each element has two nested lists, corresponding to the evolutions of both loss and accuracy during the training.\n\n# Creating the Stratifield KFold object with the number of splits equal to K\nskf = StratifiedKFold(n_splits = K, shuffle = True, random_state = 42) # 42 is our seed in order to provide reproducible results\n\nn_fold = 1\n\ncheckpoint = ModelCheckpoint(os.path.join(\".\/best_cnn.h5\"), \n    monitor='val_accuracy', verbose=1, \n    save_best_only=True, mode='max')\ncallbacks_list = [checkpoint]\n\nfor train, test in skf.split(features, targets.argmax(1)):\n    embedding_layer = Embedding(number_of_words, 25, embeddings_initializer=Constant(embedded_matrix), input_length=10 , trainable=False)\n    sequence_input = Input(shape=(10,), dtype='int32')\n    embedded_sequences = embedding_layer(sequence_input)\n\n    convs = []\n    filter_sizes = [3,4,5]\n\n    for filter_size in filter_sizes:\n        l_conv = Conv1D(filters=32, \n                        kernel_size=filter_size, \n                        activation='relu')(embedded_sequences)\n        l_pool = GlobalMaxPooling1D()(l_conv)\n        convs.append(l_pool)\n\n    conv_layers = concatenate(convs, axis=1)\n    x = Dropout(0.5)(conv_layers)  \n\n    predictions = Dense(5, activation='softmax')(x)\n    model_cnn = Model(sequence_input, predictions)\n    model_cnn.compile(loss='categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    \n    print('------------------------------------------------------------------------')\n    print(f'Training for fold {n_fold} ...')  \n    history = model_cnn.fit(features[train], targets[train],\n              batch_size=32,\n              epochs=50,\n              verbose=1,\n              callbacks = callbacks_list,\n              validation_data=(features[test], targets[test]))\n    \n    # Generate generalization metrics\n    scores = model_cnn.evaluate(features[test], targets[test], verbose=0)\n    print(f'Score for fold {n_fold}: {model_cnn.metrics_names[0]} of {scores[0]}; {model_cnn.metrics_names[1]} of {scores[1]*100}%')\n    K_fold_accuracy.append(scores[1] * 100)\n    K_fold_loss.append(scores[0])\n    K_fold_training_evolutions.append(history)\n    # Increase fold number\n    n_fold = n_fold + 1","a2106f02":"print('Average scores for all folds:')\nprint(f'> Accuracy: {np.mean(K_fold_accuracy)} (+- {np.std(K_fold_accuracy)})')\nprint(f'> Loss: {np.mean(K_fold_loss)}')\nprint('------------------------------------------------------------------------')","58ba37d8":"show_k_fold_accuracy(K_fold_training_evolutions, K)\nshow_k_fold_loss(K_fold_training_evolutions, K)","c9d28c3b":"model_cnn = load_model('.\/best_cnn.h5')\nmodel_cnn.summary()","41b26cb3":"embedding_layer = Embedding(number_of_words, 25, embeddings_initializer=Constant(embedded_matrix), input_length=10 , trainable=False)\nsequence_input = Input(shape=(10,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\n\nconvs = []\nfilter_sizes = [3,4,5]\n\nfor filter_size in filter_sizes:\n    l_conv = Conv1D(filters=32, \n                    kernel_size=filter_size, \n                    activation='relu')(embedded_sequences)\n    l_pool = GlobalMaxPooling1D()(l_conv)\n    convs.append(l_pool)\n\nconv_layers = concatenate(convs, axis=1)\nx = Dropout(0.5)(conv_layers)  \n\npredictions = Dense(5, activation='softmax')(x)\nmodel_cnn = Model(sequence_input, predictions)\nmodel_cnn.compile(loss='categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\nhistory = model_cnn.fit(features, targets,\n          batch_size=32,\n          epochs=50,\n          verbose=1)","1c279d8f":"X = convert_word_sentences_to_index_sentences(df_validation['phrase'].values, words_to_index, 10)\ny = to_categorical(y_validation)\n\ny_pred = model_cnn.predict(X)\n\nloss, acc = model_cnn.evaluate(X, y)","aea8abec":"# CNN submission\n\nsubmission = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'sample_submission.csv'))\n\nsubmission['phrase'] = submission['phrase'].str.replace(\"\\t\", \"\")\nX = convert_word_sentences_to_index_sentences(submission['phrase'].values, words_to_index, 10)\n\ny_pred = model_cnn.predict(X)\ny_pred = list(map(lambda x: np.argmax(x), y_pred))\n\nif y_pred is not None:\n    submission['class'] = y_pred\n\nsubmission = submission[['Unnamed: 0', 'class']]\nsubmission = submission.rename(columns={'Unnamed: 0': 'id'})\nsubmission.to_csv('my_submission_cnn.csv', index=False)","7ec388f3":"model_lstm = Sequential() \nmodel_lstm.add(Embedding(number_of_words, 25,\n                           embeddings_initializer=Constant(embedded_matrix)))\nmodel_lstm.add(Bidirectional(LSTM(128)))\nmodel_lstm.add(Dropout(0.5))\nmodel_lstm.add(Dense(units=5, activation='softmax'))\nmodel_lstm.add(Activation('softmax'))\nmodel_lstm.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n\nplot_model(model_lstm, to_file='lstm_architecture.png', show_shapes=True, show_layer_names=True)","2f867bf0":"# Defining the loss and accuracy for each fold\nK_fold_accuracy = list()\nK_fold_loss = list()\nK_fold_training_evolutions = [] # It will result in a list where each element has two nested lists, corresponding to the evolutions of both loss and accuracy during the training.\n\n# Creating the Stratifield KFold object with the number of splits equal to K\nskf = StratifiedKFold(n_splits = K, shuffle = True, random_state = 42) # 42 is our seed in order to provide reproducible results\n\nn_fold = 1\n\ncheckpoint = ModelCheckpoint(os.path.join(\".\/best_lstm.h5\"), \n    monitor='val_accuracy', verbose=1, \n    save_best_only=True, mode='max')\ncallbacks_list = [checkpoint]\n\nfor train, test in skf.split(features, targets.argmax(1)):\n\n    model_lstm = Sequential() \n    model_lstm.add(Embedding(number_of_words, 25,\n                               embeddings_initializer=Constant(embedded_matrix)))\n    model_lstm.add(Bidirectional(LSTM(128)))\n    model_lstm.add(Dropout(0.5))\n    model_lstm.add(Dense(units=5, activation='softmax'))\n    model_lstm.add(Activation('softmax'))\n    model_lstm.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n    \n    \n    print('------------------------------------------------------------------------')\n    print(f'Training for fold {n_fold} ...')  \n    history = model_lstm.fit(features[train], targets[train],\n              batch_size=32,\n              epochs=50,\n              verbose=2,\n              callbacks = callbacks_list,\n              validation_data=(features[test], targets[test]))\n    \n    # Generate generalization metrics\n    scores = model_lstm.evaluate(features[test], targets[test], verbose=0)\n    print(f'Score for fold {n_fold}: {model_lstm.metrics_names[0]} of {scores[0]}; {model_lstm.metrics_names[1]} of {scores[1]*100}%')\n    K_fold_accuracy.append(scores[1] * 100)\n    K_fold_loss.append(scores[0])\n    K_fold_training_evolutions.append(history)\n    # Increase fold number\n    n_fold = n_fold + 1","670d25c7":"print('Average scores for all folds:')\nprint(f'> Accuracy: {np.mean(K_fold_accuracy)} (+- {np.std(K_fold_accuracy)})')\nprint(f'> Loss: {np.mean(K_fold_loss)}')\nprint('------------------------------------------------------------------------')\n\nshow_k_fold_accuracy(K_fold_training_evolutions, K)\nshow_k_fold_loss(K_fold_training_evolutions, K)","3a4f28f7":"model_lstm = load_model('.\/best_lstm.h5')\nmodel_lstm.summary()","8595e4bc":"model_lstm = Sequential() \nmodel_lstm.add(Embedding(number_of_words, 25,\n                           embeddings_initializer=Constant(embedded_matrix)))\nmodel_lstm.add(Bidirectional(LSTM(128)))\nmodel_lstm.add(Dropout(0.5))\nmodel_lstm.add(Dense(units=5, activation='softmax'))\nmodel_lstm.add(Activation('softmax'))\nmodel_lstm.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n\nhistory = model_lstm.fit(features, targets,\n          batch_size=32,\n          epochs=50,\n          verbose=2)","469abb40":"X = convert_word_sentences_to_index_sentences(df_validation['phrase'].values, words_to_index, 10)\ny = to_categorical(y_validation)\n\ny_pred = model_lstm.predict(X)\n\nloss, acc = model_lstm.evaluate(X, y)","eb028520":"# LSTM submission\n\nsubmission = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'sample_submission.csv'))\nx_test = np.load(os.path.join(DATA_BASE_FOLDER, 'test.npy')).reshape(len(submission), -1)\n\nsubmission['phrase'] = submission['phrase'].str.replace(\"\\t\", \"\")\n\nX = convert_word_sentences_to_index_sentences(submission['phrase'].values, words_to_index, 10)\n\ny_pred = model_lstm.predict(X)\ny_pred = list(map(lambda x: np.argmax(x), y_pred))\n\nif y_pred is not None:\n    submission['class'] = y_pred\n\nsubmission = submission[['Unnamed: 0', 'class']]\nsubmission = submission.rename(columns={'Unnamed: 0': 'id'})\nsubmission.to_csv('my_submission_lstm.csv', index=False)","9f07fe9d":"## Convolutional Neural Network (CNN)\n\nA Convolutional Neural Network (CNN) is a class of deep neural networks mainly used for computer vision. It is composed of different kind of layers such as _Convolution Layers, Pooling Layers_ and _Fully Connected Layers_. More specifically:\n- **Convolution Layer**: a convolution layer is the one that performs, as the name suggests, the operation of convolution, which is a an element-wise product between the input and a small matrix of numbers, called _kernel_. As the kernel slides across the input matrix or vector by sliding a number of positions ( _stride_ ). The result of the convolutions are then passed through a ReLu activation function, whose results will be stored into a second matrix called _Feature Map_.\n- **Pooling Layer**: it performs a dimensionality reduction operation on the feature map in order to reduce the number of trainable parameters and to perform a dimension shift. The typical pooling operation is the _max pooling_ operation, for which it returns the maximum value from the portion of the feature map analyzed.\n- **Fully Connected Layer**: the output from the pooling layer is typical 1D thanks to the dimensionality reduction previously introduced. As we have seen from the previous architectures, this layer connects the inputs obtained from the pooling with the output by a specific learnable weight.","458a50a8":"## LSTM\nThe last kind of network presented belongs to the field of _Recurrent Neural Network_, which were originally designed to capture insights from the feature from a sequence, which may be the time that passes, the sentence, and so on. To achieve this goal, they model the behavior of its layer at t+1 by giving as input the output of the layer at time t.\n\nIn our case, the LSTM network where the layers are substitued by cells which are structured with elements called _gates_ , which are:\n- **Input gate**: is a layer which receives the output from the hidden state layer and the previous time plus the current input. The nodes in this gate are reduced through the _tanh_ function present and they are activated by a sigmoid function whose goal is to discard the useless input elements\n- **Forget gate**: The result of the input gate is connected to this gate, which preserves the internal state of this cell. This variable state is obtained by adding the resulting input gate and the cell state at the previous time, and the forget gate helps to choose which elements maintain from this result\n- **Output gate**: it decides the values that are going to be given as an output of the current cell.\n\nThe hidden state layer that we mentioned is composed of a set of neural network nodes whose size is the one that we will define through the implementation of our architecture.","41ff8b7a":"#### Model Definition\n\nThe CNN architecture that we built was already introduced before. For this reason, we want to describe it more in detail, especially for explaining the 1D-convolution.\n- We define an _Embedding_ layer by giving as input the number of words with the embedding size, as well as the embedding size, the embedding matrix and the maximum number of words for each sentence. This ends in a 3D dataset of shape (number_of_words, 10, 25), where word will be the index for which the model access to its embedding vector\n- Since each sentence is composed by a matrix where each row is the embedding vector of the word, we can perform a 1D convolution for which we apply 32 filters of dimension 3,4 and 5, with stride equal to 1. This represents the idea of summarizing 3-grams, 4-grams and 5-grams to extract different meanings. For each feature map obtained we apply the ReLu activation, and a Max Pooling.\n- We end up with a final full connected layer\n- Dropout is applied at the end of the Convolutional layers\n","24fe176b":"# EDA\n\n","1f585aa2":"### Exploring the structure of the sentences\nSince we are dealing with an input which is encoded as a word embedding matrix, we want to understand how sentences are different to each other in terms of length. \nTo achieve this goal, we preprocessed the sentences from both train and validation set by _tokeninzing_ them, i.e., treating each word as an object, and then by removing everything that was not a word. Handling the words as tokens allowed us to easily count the length of each sentence, as reported in the plot at the end of this block","7f8e7cda":"### K-Fold Cross Validation\nAfter having defined the primal structure of our Neural Network, we now reintroduce all the training and regularization techniques adopted in the creation of the baseline classifiers, i.e., _K-Fold Cross Validation_ and _L2 Regularization_.\nWe then start extending our training model with a 5-Fold CV.","788bda28":"To reproduce the results from the paper, we had to download the same embedder used in the skeleton, which can be found as the file _glove.twitter.27B.25d.txt_.\nAfter having stored the mappings between words and GloVe internal indices and their corresponding embeddings, we will use it to represent our words as embedded vectors","0a1e5fea":"## Preliminary operations\n\nWe begin by performing operations in order to set up our datasets in the correct format for our networks and to define the utility functions that we will use later.","50852852":"## Initialization\n\nHere we import all the packages that we need to run the notebook. We also moved some functions originally provided in the skeleton for ","91cc0d92":"## KNN","9fc24694":"## Naive Neural Network\n\nThe first architechture is called _Naive Neural Network_ due to its simplicity. In fact, it is composed of a hidden layer of 64 neurons which uses _ReLu_ as activation functions and an output layer of 5 neurons, corresponding to the classes, which uses _softmax_ activation function. As shown in the summary of the model, this architecture produces a total of 16,389 trainable parameters.","6c9284c9":"In the training set, the response variable contains classes that are not equally distributed. Therefore, we have to account for the fact that our classifiers will have some difficulties to correctly classify sentences originally labeled with 4, 1 and 0 with respect to 3 and 2, which are the more predominant.\n\nIn the validation set, the proportion of the minor labels changes. In fact, label 1 is the rarest one, followed by 4 and 0. The unequity from both datasets may be fixed by rebalancing the class distributions by introducing augmented data or by reducing the observations from the most frequent classes. However, the first approach requires some domain expertise, because augmenting a sentence is something that cannot be achieved only by adding random words, but it has to maintain some linguistic features such as the semantic or the syntax. The second approach instead was considered, but the little observations in both training and validation sets - 100 and 32 respectively - do not allow any kind of reducement.","b3cae87a":"#### Preprocessing Functions","225ddd2e":"# Neural Networks\nIn this last part of the project, we describe the implementation of several Neural Network Architectures with the purpose of improving the accuracy of the previous classifiers.\nThe pipeline that we decided to follow is the following:\n- We start by building a Single Layer Neural Network which is composed of one input layer and a fully connected one to perform the classification\n- We extend the previous model by building a simple Deep Neural Network with just one hidden layer, to see if we can obtain any improvement\n- We end by applying some results from the scientific literature around this topic, especially by reimplementing the results presented in [Zhao et al.](https:\/\/web.stanford.edu\/class\/archive\/cs\/cs224n\/cs224n.1174\/reports\/2762064.pdf) in which they build a LSTM and a CNN to classify emojis from tweets.","c84087ae":"\n## Installing missing packages\n\nWe begin by installing the missing packages, useful for this project. ","875e1099":"Since we have a lot of non normalized features, we just some of their distributions, picked randomly.","12a3f0c5":"As we have seen in the EDA part, the distribution of the classes in our target variable is not equal. This means that the Cross Validation procedure cannot be done in a total randomic way, but instead our folds must maintain the proportion of the classes in the original datasets. ","42dd7f4b":"### Checking feature normalization\nWe start by checking the normality of of the distribution related to the embeddings. To accomplish this, we use _Shapiro-Wilk's_ test, where the null hyphothis express data comes from a normal distribution.","3e075a0e":"## Dataset\nWe load both .csv files referring to train and validation, checking if everything is correct by printing the first ten emojis","6e2c1e7a":"### L2 Regularization on a Neural Network\nApart from the dropout regularization previouslt described, the definition of the classifiers at the begin of this notebook made use of L2 regularization by default. This aspect is not maintained in the context of neural networks. Therefore, we take advantage of the simplicity of this Single Layer Neural Network to perform a regularization of the parameters to see if we can improve accuracy.","70ec1b17":"## Logistic Classifier","44b83c29":"# Emojify Project\nThe following project covers a Classification problem in the context of the sentiment analysis. In fact, the former dataset is composed of a series of tweets referring to a particular sentiment expressed by the emoji associated with them, from a pool of 5. These emojis were coded as integer, representing the classes we have to predict, ending in a multi-classification problem.\n\nThis notebook is structured as follows:\n* We begin by performing an Exploratory Data Analysis (EDA) from which we study some aspects regarding the distribution of the dependent variable and the structure of the given text\n* We study the classification problem in practice by building a pipeline of models which starts from by the application of classification models whose classifiers were the one we have seen during the course (Logistic, KNN). Then, we continue by introducing Neural Networks architectures with several variants (Single Layer NN, DNN, CNN, LSTM) by referring to scientific literature when it comes to justify the choices and assumptions that we made in order to obtain our results\n* We discuss the results obtained with their corresponding problems and possible improvements\n\nIn addition to this, the technical aspects of this project will be explained by including the theory behind them, in order to provide a comprehensive explanation for which the paper itself is not able to do.","d608eb70":"As we can see, we are dealing with a long tailed distribution, which means in this case that we will have to deal with singularity. In the context of machine learning, this may lead to not properly adjust the weights in order to better classify sentences with these words within, given the fact they do not appear frequently.","bcb74111":"The implementation of the model is relatively straightforward: we start from an input whose shape is the same as the one used for the CNN and we define a first Bidirectional LSTM layer with 128 hidden state nodes, followed by a dropout regularization and a fully connected layer.\nThe reason behind the usage of Bidirectional LSTM is that this kind of networks create two recurrent layers for which the first is the usual one that handles the input from its begin to the end, whilst the second reverses the input sequence starting from the back. Therefore, if vanilla LSTM preserves information from the past by using their hidden state, a Bidirectional LSTM allows to maintain information both from the past and the future which, in the context of NLP like ours, generally help to understand the context better.","750429b2":"## Deep Neural Network (DNN)\n\nWe extended the previous model by building a Deep Neural Network architecture. To achieve this goal, we tried many combinations of hidden layers by choosing their number and the neurons they contained. The best accuracy that we achieved includes the addition of only one new hidden layer containing 32 neurons, ending up with a total of 18.309 trainable parameters.","17d6affc":"### Checking the occurrence of different words through sentences\n\nWe want to account how many times each word occurred through the training set, so we computed how many words are associated with the number of times they appeared.","75682a45":"We now highlight some technical aspects of this solution since they will be widely used as well as in the upcoming architectures:\n\n- **The usage of ReLu activation function:** we choose to use this particular activation function due to three factors:\n    1. It is simple to compute, due to its structure. In fact, we to compute only the maximum of the input, if it is greater than 0, otherwise it returns 0. This non-linearity provides computational improvements with respect to _tanh_ or _sigmoid_ , speeding up the training procedure\n    2. The fact that, if the input is negative, the function returns 0 has the ability to enforce sparsity. This feature can speed up the training process and make the model more simpler.\n    3. The ReLu, due to its linear shape, produces values of the gradient which are constant as the input of the function increases. This is not true for example in sigmoids, where the gradient reduces its value as the _x_ values increase. In the end, this behavior reduce the probability of experiencing the vanishing gradient problem.\n- **The usage of Dropout regularization:** Dropout is a regularization technique originally proposed [here](https:\/\/jmlr.org\/papers\/volume15\/srivastava14a\/srivastava14a.pdf) in order to prevent overfitting. In this way, we assign a function for which is node may be not considered, i.e, _dropped out_, introducing some noise into the model that helps avoding overfitting.\n- **The usage of categorical crossentropy loss functions:** is due to the fact that we are dealing with a multi classification problem for which an instance can belong only to one class.\n- **The usage of Adam optimizer:** is related to its property for which the learning rate is not identical for alla parameters, but instead we have one for each of them, adapting them by making use of the exponential moving average of the gradient and its square. Gradient Descent instead makes no difference for its weight, and for this reason it was [demonstrated](https:\/\/arxiv.org\/pdf\/1412.6980.pdf) to achieve a lower cost function.\n  ","21d89cea":"Predictions on the validation set:","20fc12e3":"With the technical introduction of the K-fold CV for NNs, we implement two utility functions that allows us to plot accuracy and loss for each fold. ","7dc3a32d":"### Checking distribution of the classes in the response variable\nWe now want to characterize the distribution of the different levels of our response variable.","2abd9ae1":"### Clustering the word in the sentences based on their class \nWe then use HDBSCAN clustering method, with a number of clusters equal to 10, to check how well words from different classes can be aggregated.","472ef9bd":"As we can see, the length of the sentences follows a normal distribution, with a maximum of 10 which we will used as our upper bound for padding purposes. ","23918e82":"Despite CNNs are supposed to work with images, in the field of sentiment analysis they are used to extract some meaning from word embeddings, like [Zhao et al.](https:\/\/web.stanford.edu\/class\/archive\/cs\/cs224n\/cs224n.1174\/reports\/2762064.pdf) did. The idea is the following: representing each word with its corresponding GloVe embedding, such that each sentence becomes a matrix where the number of rows are the the number of tokens (after having padded to the maximum sentence length), and the number of columns corresponds the embedding dimension (25 in our case). Then, we use 64 filters with a kernel of size 3, 4 and 5 to reproduce the extraction of meaning from n-grams with 3 different sizes. ","1f1c0b42":"# Sentence Embedding\nAll the sentences are padded to the one with maximum length, i.e., 10. Each sentence was embedded by using GloVe, which works at word level, therefore performing word embedding. For each padded sentence, we concatenated the corresponding embeddings, obtaining with a matrix of dimension n_words x 250","72045224":"# Baseline Classifiers\n\nIn this section, we study different classifiers in order to choose the best one in terms of accuracy metrics. This classifier will be our baseline for the comparison between the Neural Network models.","06a648d4":"Best CV model performance"}}