{"cell_type":{"54eb521c":"code","d306e6c9":"code","0715f1a4":"code","84c09cfc":"code","77b481e6":"code","649d001f":"code","3c043be9":"code","e6d58d70":"code","e55603ca":"code","c81e63af":"code","cf35a8f3":"code","6d158ad6":"code","0302fdfc":"code","bfef3da5":"code","9347c858":"code","4a6af253":"code","c37e85d5":"code","7347ad75":"code","f3d2dd36":"code","b77c89bf":"code","e9e6ae30":"code","d0ec892d":"code","914d6418":"code","bf44daa3":"code","8b6b394f":"code","1ac599b4":"code","abce0cd6":"code","9f7f732e":"code","bc5ff58f":"code","47f6b4b5":"code","a18bf035":"code","7041a440":"code","3dc860f1":"code","65991be1":"code","e66813a6":"code","0e47241b":"code","cfd75a87":"code","d7483823":"code","3d23af33":"code","23cffa00":"code","543df955":"code","e318f484":"code","bb101fa8":"code","9556c95e":"code","a7aa08b7":"code","fa6675a7":"markdown","35e21667":"markdown","d72922c8":"markdown","c860049e":"markdown","82aa7c76":"markdown","449e8921":"markdown","851b3309":"markdown","2790eda6":"markdown","c32e94c0":"markdown","64a75791":"markdown","163e5e00":"markdown","a218c822":"markdown","f5831a96":"markdown","2b5f18a5":"markdown","b5ff9844":"markdown","25005d5a":"markdown","82f27d54":"markdown","7ade0ab5":"markdown","6978cf87":"markdown","45b4aaa3":"markdown","13da952d":"markdown","7b3f79cf":"markdown","f6dbec6d":"markdown","de4707fb":"markdown"},"source":{"54eb521c":"# General\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport datetime\nfrom time import time\nimport pandas as pd\nimport os\nimport numpy as np\nfrom tqdm import tqdm\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n%matplotlib inline\n\n# OpenCV\nimport cv2\n\n# ScikitLearn for Data Splitting\nfrom sklearn.model_selection import train_test_split, GroupKFold, StratifiedKFold\n\n# Albumentations\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n# Pytorch\nimport torch\nimport torchvision\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.ops import nms\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\n\n\n# Seed Everything for Reproducibility\nSEED = 42\n\ndef seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_everything(SEED)","d306e6c9":"DATA_PATH = '..\/input\/lisa-traffic-light-dataset'\nos.listdir(DATA_PATH)","0715f1a4":"DAY_TRAIN_PATH = '..\/input\/lisa-traffic-light-dataset\/Annotations\/Annotations\/dayTrain\/'\nNIGHT_TRAIN_PATH = '..\/input\/lisa-traffic-light-dataset\/Annotations\/Annotations\/nightTrain\/'","84c09cfc":"train_day = []\nfor clipName in tqdm(sorted(os.listdir(DAY_TRAIN_PATH))):\n    if 'dayClip' not in clipName:\n        continue\n    df = pd.read_csv(os.path.join(DAY_TRAIN_PATH,clipName,'frameAnnotationsBOX.csv'),sep=';')\n    train_day.append(df)\n    \ntrain_day_df = pd.concat(train_day,axis=0)\ntrain_day_df['isNight'] = 0\n    \ntrain_night = []\nfor clipName in tqdm(sorted(os.listdir(NIGHT_TRAIN_PATH))):\n    if 'nightClip' not in clipName:\n        continue\n    df = pd.read_csv(os.path.join(NIGHT_TRAIN_PATH,clipName,'frameAnnotationsBOX.csv'),sep=';')\n    train_night.append(df)\n\ntrain_night_df = pd.concat(train_night,axis=0)\ntrain_night_df['isNight'] = 1\n\ndf = pd.concat([train_day_df,train_night_df],axis=0)","77b481e6":"df.head()","649d001f":"# Duplicate Columns\nnp.all(df['Origin file'] == df['Origin track']), np.all(df['Origin frame number'] == df['Origin track frame number'])","3c043be9":"# Droppin duplicate columns & \"Origin file\" as we don't need it\ndf = df.drop(['Origin file','Origin track','Origin track frame number'],axis=1)","e6d58d70":"# Here Filename (Location of Image) is different -> Change it to appropriate name\n# Ex. dayTraining\/dayClip1--00000.jpg -> dayTrain\/dayTrain\/dayClip1\/frames\/dayClip1--00000.jpg\n\ndef changeFilename(x):\n    filename = x.Filename\n    isNight = x.isNight\n    \n    splitted = filename.split('\/')\n    clipName = splitted[-1].split('--')[0]\n    if isNight:\n        return os.path.join(DATA_PATH,f'nightTrain\/nightTrain\/{clipName}\/frames\/{splitted[-1]}')\n    else:\n        return os.path.join(DATA_PATH,f'dayTrain\/dayTrain\/{clipName}\/frames\/{splitted[-1]}')\n\ndf['Filename'] = df.apply(changeFilename,axis=1)","e55603ca":"df['Annotation tag'].unique()","c81e63af":"# We will change annotations to only -> stop (RED), go (GREEN) & warning (YELLOW)\nlabel_to_idx = {'go':1, 'warning':2, 'stop': 3}\nidx_to_label = {v:k for k,v in label_to_idx.items()}\n\ndef changeAnnotation(x):\n    if 'go' in x['Annotation tag']:\n        return label_to_idx['go']\n    elif 'warning' in x['Annotation tag']:\n        return label_to_idx['warning']\n    elif 'stop' in x['Annotation tag']:\n        return label_to_idx['stop']\n    \ndf['Annotation tag'] = df.apply(changeAnnotation,axis=1)\n\nannotation_tags = df['Annotation tag'].unique()\nannotation_tags","cf35a8f3":"df.head()","6d158ad6":"# Changing Column Names\ndf.columns = ['image_id','label','x_min','y_min','x_max','y_max','frame','isNight']","0302fdfc":"df.head()","bfef3da5":"print(\"Number of Unique Images: \",df.image_id.nunique(),'\/',df.shape[0])","9347c858":"fig, ax = plt.subplots(len(annotation_tags),1,figsize=(15,10*len(annotation_tags)))\n\nfor i, tag in enumerate(annotation_tags):\n    sample = df[df['label']==tag].sample(1)\n    bbox = sample[['x_min','y_min','x_max','y_max']].values[0]\n    \n    image = cv2.imread(sample.image_id.values[0])\n    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n    \n    cv2.rectangle(image,(bbox[0],bbox[1]),(bbox[2],bbox[3]),(220, 0, 0), 2)\n    \n    ax[i].set_title(idx_to_label[tag])\n    ax[i].set_axis_off()\n    ax[i].imshow(image)","4a6af253":"df['clipNames'] = df[['image_id']].applymap(lambda x: x.split('\/')[5])\ndf['clipNames'].unique()","c37e85d5":"def split(df,p=0.25):\n    clipNames = sorted(df['clipNames'].unique())\n\n    nightClips = [name for name in clipNames if 'night' in name]\n    dayClips = [name for name in clipNames if 'day' in name]\n\n    testNightClipNames = list(np.random.choice(nightClips,int(len(nightClips)*p)))\n    testDayClipNames = list(np.random.choice(dayClips,int(len(dayClips)*p)))\n    testClipNames = testNightClipNames + testDayClipNames\n\n    trainDayClipNames = list(set(dayClips) - set(testDayClipNames))\n    trainNightClipNames = list(set(nightClips) - set(testNightClipNames))\n    trainClipNames = trainNightClipNames + trainDayClipNames\n    \n    train_df = df[df.clipNames.isin(trainClipNames)]\n    test_df = df[df.clipNames.isin(testClipNames)]\n    \n    return train_df, test_df","7347ad75":"train_df, test_df = split(df)","f3d2dd36":"train_df.head()","b77c89bf":"test_df.head()","e9e6ae30":"print(\"Train shape: \",train_df.shape)\nprint(\"Test shape: \",test_df.shape)","d0ec892d":"train_df, val_df = split(train_df)\ntrain_df.head()","914d6418":"val_df.head()","bf44daa3":"print(\"Train shape: \",train_df.shape)\nprint(\"Validation shape: \",val_df.shape)","8b6b394f":"# train_df = train_df.sample(frac=0.1)\n# print(\"Train shape: \",train_df.shape)","1ac599b4":"EPOCHS = 3\nBATCH_SIZE = 16","abce0cd6":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(device)","9f7f732e":"class TrafficLightsDataset:\n    def __init__(self, df, transforms=None):\n        super().__init__()\n\n        # Image_ids will be the \"Filename\" here\n        self.image_ids = df.image_id.unique()\n        self.df = df\n        self.transforms = transforms\n        \n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n\n    def __getitem__(self, index: int):\n\n        image_id = self.image_ids[index]\n        records = self.df[self.df.image_id == image_id]\n\n        # Reading Image\n        image = cv2.imread(image_id)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n        \n        # Bounding Boxes\n        boxes = records[['x_min','y_min','x_max','y_max']].values\n        boxes = torch.as_tensor(boxes,dtype=torch.float32)\n        \n        # Area of the bounding boxes\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        area = torch.as_tensor(area, dtype=torch.float32)\n\n        # Labels of the object detected\n        labels = torch.as_tensor(records.label.values, dtype=torch.int64)\n        \n        iscrowd = torch.zeros_like(labels, dtype=torch.int64)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        target['image_id'] = torch.tensor([index])\n        target['area'] = area\n        target['iscrowd'] = iscrowd\n\n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n            # target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n            target['boxes'] = torch.as_tensor(sample['bboxes'],dtype=torch.float32)\n            target['labels'] = torch.as_tensor(sample['labels'])\n            \n        return image, target, image_id","bc5ff58f":"# Average loss -> (Total-Loss \/ Total-Iterations)\nclass LossAverager:\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","47f6b4b5":"# Custom Batching with no collate function your batch data would look like:\n# [(img_0, targets_0), (img_1, targets_1), ...]\n# but with the collate function it would be more like\n# [(img_0, img_1), (targets_0, targets_1), ...]\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))","a18bf035":"# Albumentations\n\n# For Train Data\ndef getTrainTransform():\n    return A.Compose([\n        A.Resize(height=512, width=512, p=1),\n        A.Flip(0.5),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\n# For Validation Data\ndef getValTransform():\n    return A.Compose([\n        A.Resize(height=512, width=512, p=1),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\n# For Test Data\ndef getTestTransform():\n    return A.Compose([\n        A.Resize(height=512, width=512, p=1),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})","7041a440":"trainDataset = TrafficLightsDataset(train_df,getTrainTransform())\nvalDataset = TrafficLightsDataset(val_df,getValTransform())\ntestDataset = TrafficLightsDataset(test_df,getTestTransform())","3dc860f1":"trainDataLoader = DataLoader(\n    trainDataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n\nvalDataLoader = DataLoader(\n    valDataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n\ntestDataLoader = DataLoader(\n    testDataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)","65991be1":"images, targets, image_ids = next(iter(trainDataLoader))\n\nboxes = targets[0]['boxes'].cpu().numpy().astype(np.int32)\nimage = images[0].permute(1,2,0).cpu().numpy()","e66813a6":"targets[0]","0e47241b":"def displayImage(image, boxes):\n    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\n    for box in boxes:\n        cv2.rectangle(image,\n                      (box[0], box[1]),\n                      (box[2], box[3]),\n                      (220, 0, 0), 3)\n\n    ax.set_axis_off()\n    ax.imshow(image)\n\n    plt.show()","cfd75a87":"displayImage(image,boxes)","d7483823":"model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\nN_CLASS = 4  # 3 classes (Stop, Warning, Go) + Background\n\n# Number of Input Features for the Classifier Head\nINP_FEATURES = model.roi_heads.box_predictor.cls_score.in_features\n\n# New Head for Classification\nmodel.roi_heads.box_predictor = FastRCNNPredictor(INP_FEATURES, N_CLASS)","3d23af33":"model.to(device)\n\nparams = [p for p in model.parameters() if p.requires_grad]\n# Optimizers\noptimizer = torch.optim.Adam(params)\n\n# LR Scheduler\nlr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)","23cffa00":"lossHist = LossAverager()\nvalLossHist = LossAverager()\n\nfor epoch in range(EPOCHS):\n    \n    start_time = time()\n    model.train()\n    lossHist.reset()\n    \n    for images, targets, image_ids in tqdm(trainDataLoader):\n        \n        images = torch.stack(images).to(device)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        \n        bs = images.shape[0]\n        \n        loss_dict = model(images, targets)\n        \n        totalLoss = sum(loss for loss in loss_dict.values())\n        lossValue = totalLoss.item()\n        \n        lossHist.update(lossValue,bs)\n\n        optimizer.zero_grad()\n        totalLoss.backward()\n        optimizer.step()\n    \n    # LR Update\n    if lr_scheduler is not None:\n        lr_scheduler.step(totalLoss)\n\n    print(f\"[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}]\")\n    print(f\"Epoch {epoch}\/{EPOCHS}\")\n    print(f\"Train loss: {lossHist.avg}\")\n    \n    torch.save(model.state_dict(), 'fasterrcnn_resnet50_fpn.pth')","543df955":"# model.load_state_dict(torch.load('..\/input\/trafficlightdetectionfasterrcnnweights\/fasterrcnn_resnet50_fpn.pth'))\nmodel.load_state_dict(torch.load('fasterrcnn_resnet50_fpn.pth'))","e318f484":"model.eval()\nimages, targets, image_ids = next(iter(testDataLoader))\nimages = torch.stack(images).to(device)\n\noutputs = model(images)","bb101fa8":"def filterBoxes(output,nms_th=0.3,score_threshold=0.5):\n    \n    boxes = output['boxes']\n    scores = output['scores']\n    labels = output['labels']\n    \n    # Non Max Supression\n    mask = nms(boxes,scores,nms_th)\n    \n    boxes = boxes[mask]\n    scores = scores[mask]\n    labels = labels[mask]\n    \n    boxes = boxes.data.cpu().numpy().astype(np.int32)\n    scores = scores.data.cpu().numpy()\n    labels = labels.data.cpu().numpy()\n    \n    mask = scores >= score_threshold\n    boxes = boxes[mask]\n    scores = scores[mask]\n    labels = labels[mask]\n    \n    return boxes, scores, labels","9556c95e":"def displayPredictions(image_id,output,nms_th=0.3,score_threshold=0.5):\n    \n    boxes,scores,labels = filterBoxes(output,nms_th,score_threshold)\n    \n    # Preprocessing\n    image = cv2.imread(image_id)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n    image = cv2.resize(image,(512,512))\n    image \/= 255.0\n    \n    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\n    colors = {1:(0,255,0), 2:(255,255,0), 3:(255,0,0)}\n    \n    for box,label in zip(boxes,labels):\n        image = cv2.rectangle(image,\n                      (box[0], box[1]),\n                      (box[2], box[3]),\n                      colors[label], 2)\n\n    ax.set_axis_off()\n    ax.imshow(image)\n\n    plt.show()","a7aa08b7":"displayPredictions(image_ids[2],outputs[2],0.2,0.4)","fa6675a7":"We will do the similar thing that we did for the train-test-split above","35e21667":"### Train & Test Set split","d72922c8":"## 7. Model","c860049e":"### Custom DataSet","82aa7c76":"## 4. Exploratory Data Analysis","449e8921":"## 9. Inference","851b3309":"### Checking DataPipeline","2790eda6":"### Data Loaders","c32e94c0":"### Train & Validation Split","64a75791":"### Average Loss","163e5e00":"## 8. Training","a218c822":"## 2. Load Data","f5831a96":"## 3. Data Preprocessing","2b5f18a5":"### Device","b5ff9844":"## 6. Utils","25005d5a":"## 1. Importing Necessary Libraries","82f27d54":"**\"isNight\"** feature will be used to split the data so that there will be balance between **Day & Night Clips** in **Train & Test sets**","7ade0ab5":"## \ud83d\udea6 Introduction - Traffic Light Detection using Pytorch | FasterRCNN\n\n![Traffic-Light](https:\/\/media.giphy.com\/media\/l1AsA1IZNWFyo6xlm\/giphy.gif)\n\n---\n\nWe will be using Object-Detection techniques to **Detect Traffic Lights**. For this we will use **Transfer-Learning** and the base architecture will be **FasterRCNN**. \n\n### \ud83e\udd14 What is FasterRCNN?\n\n![Faster-RCNN](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2018\/10\/Screenshot-from-2018-10-09-14-15-36.png)\n\nFaster RCNN is the modified version of **Fast RCNN**. The major difference between them is that **Fast RCNN uses selective search** for generating Regions of Interest, while **Faster RCNN uses \u201cRegion Proposal Network\u201d**, aka RPN. RPN takes image feature maps as an input and generates a set of **object proposals**, each with an **objectness score** as output.  \n\nThe  below steps are typically followed in a Faster RCNN approach:\n1. We take an image as input and pass it to the ConvNet which returns the feature map for that image.\n2. Region proposal network is applied on these feature maps. This returns the object proposals along with their objectness score.\n3. A RoI pooling layer is applied on these proposals to bring down all the proposals to the same size.\n4. Finally, the proposals are passed to a fully connected layer which has a softmax layer and a linear regression layer at its top, to classify and output the bounding boxes for objects.\n\n*For **Region Proposal Network (RPN)** & more on **Object Detection** look here* - [\ud83d\ude80](https:\/\/www.analyticsvidhya.com\/blog\/2018\/10\/a-step-by-step-introduction-to-the-basic-object-detection-algorithms-part-1\/)\n\n### \ud83d\udce6 Data Set:\n**LISA Traffic Light Dataset** - [\ud83d\ude80](https:\/\/www.kaggle.com\/mbornoe\/lisa-traffic-light-dataset)  \nIt has more than **44 minutes** of **annotated traffic light** data.","6978cf87":"### Merge all different Annotation Files into a single file","45b4aaa3":"## 5. Validation Scheme","13da952d":"### Configuration","7b3f79cf":"### Albumentations","f6dbec6d":"As we have data from multiple clips, we will have to ensure that all the images from a same clip would either be in the **train set** or in **test set**. This will ensure that, there will not be any **overlapping** between the train & the test data.","de4707fb":"## 10. Conclusion\n\n* This could be optimized using various **Augmentation** & **Ensembling** techniques. For various Augmentations look here - [\ud83d\ude80](https:\/\/albumentations.ai\/docs\/)\n* I don't know more about - **how to validate the model**. But I would be updating this notebook whenever I get to know about it. If anyone know about validating **FasterRCNN in Pytorch**, do comment below.\n\n### \ud83c\udf0a Resources:\n* [Inference on Video - Notebook](https:\/\/colab.research.google.com\/drive\/1Zr5ozHnN9bKi6NepYVvboQ6v20i8ZO9X?usp=sharing)\n* [Pytorch-Tutorial](https:\/\/pytorch.org\/tutorials\/intermediate\/torchvision_tutorial.html)\n* [A Step-by-Step Introduction to the Basic Object Detection Algorithms](https:\/\/www.analyticsvidhya.com\/blog\/2018\/10\/a-step-by-step-introduction-to-the-basic-object-detection-algorithms-part-1\/)\n* [Object-Detection-on-Video](https:\/\/www.dlology.com\/blog\/how-to-run-object-detection-and-segmentation-on-video-fast-for-free\/)\n* [Albumentations](https:\/\/albumentations.ai\/docs\/)\n* [a-PyTorch-Tutorial-to-Object-Detection](https:\/\/github.com\/sgrvinod\/a-PyTorch-Tutorial-to-Object-Detection)\n* [Guide to build Faster RCNN in PyTorch](https:\/\/medium.com\/@fractaldle\/guide-to-build-faster-rcnn-in-pytorch-95b10c273439)\n* [Object Detection for Dummies Part 3: R-CNN Family](https:\/\/lilianweng.github.io\/lil-log\/2017\/12\/31\/object-recognition-for-dummies-part-3.html)"}}