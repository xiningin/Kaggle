{"cell_type":{"0c3f321b":"code","e4669415":"code","61b5d4e8":"code","17ec9688":"code","97611043":"code","947c2c68":"code","864773e2":"code","453a13af":"code","b3bc326f":"code","6083b01e":"code","ef35ad9b":"code","ffa16131":"code","120b8b2e":"code","dae2f57e":"code","52734dca":"code","0142eb00":"code","efa8bc54":"code","0787def8":"code","6d4edd78":"code","af129735":"code","5f84fb6b":"markdown","6ef4f8dd":"markdown","7929caaa":"markdown","d7a7718f":"markdown","8bd721fb":"markdown","e2fdf040":"markdown","361636b3":"markdown","8213c0be":"markdown","09b2e8bb":"markdown","920c3caf":"markdown","63781d4f":"markdown","f0671921":"markdown"},"source":{"0c3f321b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e4669415":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom plotly.offline import init_notebook_mode,iplot\nfrom plotly.tools import FigureFactory as ff\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npd.set_option(\"display.float_format\",lambda x: \"%.5f\" % x)\npd.set_option(\"display.max_rows\",None)\npd.set_option(\"display.max_columns\",None)","61b5d4e8":"df = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")\nd=df.head(10)\ntable = ff.create_table(d)\nfor i in range(len(table.layout.annotations)):\n    table.layout.annotations[i].font.size = 9\niplot(table)","17ec9688":"# Let's distinguish between dependent and independent variables.\ny = df[[\"Outcome\"]]\nX = df.drop([\"Outcome\"], axis=1)","97611043":"tree = DecisionTreeClassifier(random_state=42).fit(X,y)\ny_pred = tree.predict(X)\nprint(classification_report(y, y_pred))","947c2c68":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","864773e2":"# train\nmodel = DecisionTreeClassifier(random_state=42).fit(X_train, y_train)\ny_pred = model.predict(X_train)\ny_prob = model.predict_proba(X_train)[:, 1]\nprint(classification_report(y_train, y_pred))","453a13af":"# test\ny_pred = model.predict(X_test)\ny_prob = model.predict_proba(X_test)[:, 1]\nprint(classification_report(y_test, y_pred))","b3bc326f":"pip install pydotplus","6083b01e":"from sklearn.tree import export_graphviz\nimport pydotplus","ef35ad9b":"def tree_graph_to_png(tree, feature_names):\n    tree_str = export_graphviz(tree, feature_names=feature_names, filled=True, out_file=None)\n    graph = pydotplus.graph_from_dot_data(tree_str)\n    ","ffa16131":"tree_str = export_graphviz(model, feature_names=X_train.columns, filled=True, out_file=None)\ngraph = pydotplus.graphviz.graph_from_dot_data(tree_str)\ngraph.write_png(\"cart2.png\")","120b8b2e":"from IPython.display import Image","dae2f57e":"Image(graph.create_png())","52734dca":"from sklearn.tree import export_text","0142eb00":"tree_rules = export_text(model, feature_names=list(X_train.columns))\nprint(tree_rules)","efa8bc54":"pip install skompiler","0787def8":"pip install astor","6d4edd78":"from skompiler import skompile\nimport astor","af129735":"print(skompile(model.predict).to('python\/code'))","5f84fb6b":"* Now let's create a prediction model with decision tree for pima indians diabetes dataset","6ef4f8dd":"# Data Description\n---\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage\n\n---\n\nSeveral constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n---\n\n* Pregnancies: Number of times pregnant\n* Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n* BloodPressure: Diastolic blood pressure (mm Hg)\n* SkinThickness: Triceps skin fold thickness (mm)\n* Insulin: 2-Hour serum insulin (mu U\/ml)\n* BMI: Body mass index (weight in kg\/(height in m)^2)\n* DiabetesPedigreeFunction: Diabetes pedigree function\n* Age: Age (years)\n* Outcome: Class variable (0 or 1) 268 of 768 are 1, the others are 0","7929caaa":"# Library","d7a7718f":"## Visualize the Decision Tree","8bd721fb":"* In the model we built, we both built and tested the model with the data we have. We need to find a way that will give us a more accurate result, namely model validation. I guessed but I need to verify this and for this I will use the Holdout method.\n\n---\n**Holdout Method:** The data set is divided into two groups as training and testing with the interval we will give. The model is built with the training set and the model success is tested with the test set.\nEvaluation can be very variable. Evaluation may depend on which data points enter the training set and which test set they convert to.\n\n---\n* The holdout method is more useful if the data set is large, as it makes a single training-test distinction. The commonly used method is cross validation, cross validation separates training and testing multiple times and gives us a better idea of how the model will perform on unseen data.","e2fdf040":"# Gini vs Entropy\n---\n* The main difference between Gini and Entropy is that Gini takes values between [0 - 0.5] while Entropy is between [0 - 1]. In the image below, gini has been multiplied by 2 for tangible difference.\n\n\n<center><img\nsrc=\"https:\/\/quantdare.com\/wp-content\/uploads\/2020\/11\/entropy_and_gini.png\" style=\"width:75%;height:75%;\">\n<\/center>\n\n* In terms of computation, Entropy is more complex as it uses logarithms, Gini computation will be faster and simpler. Scores with Entropy are slightly better, Gini is cheaper and faster. However, if the times are compared, it seems that the closeness of the scores to each other is not worth calculating Entropy, since the time spent on training is much more for Entropy.\n\n---\n\n# Information Gain\n\n---\n* Information gain is based on subtracting all entropy after dividing a data set over a feature. The smaller the entropy, the higher the importance of that feature (ID3). However, with Information Gain, the situation is the opposite. While constructing the decision tree, the feature with the highest Information Gain is selected.\n\n> The knowledge gain is why pollution is so important. We see how much information we have gained as we go down the tree and measure the impurity of the nodes.\n\n---\n* E.g; Let's have features such as Weather - Temperature - Wind - Humidity according to Football Playing status. Their entropies have been calculated, and now let's observe the information gain situation:\n- target:  plays football\n\n| Weather | Temperature | Moisture | Wind | Plays Football |\n| --- | --- | --- | --- | --- |\n| cloudy | hot weather | high | none | yes |\n| cloudy | cold weather | normal | yes | yes |\n| cloudy | warm weather | high | yes | yes |\n| cloudy | hot weather | normal | none | yes |\n\n\n* In this case, if it is Cloudy, Play Football is classified as Yes. After this stage, let's observe the other situation, namely the Sunny situation:\n\n\n| Weather | Temperature | Moisture | Wind | Plays Football |\n| --- | --- | --- | --- | --- |\n| sunny | warm weather | high | none | yes |\n| sunny | cold weather | normal | none | yes |\n| sunny | cold weather | normal | yes | no |\n| sunny | warm weather | normal | none | yes |\n| sunny | warm weather | high | yes | no |\n\n* Here, information gain is calculated again because there are Yes-Nos in the Sunny state. The wind condition becomes the deciding factor for us. The tree structure is labeled as Sunny \u2192 Wind \u2192 Yes if No If Yes No.\n---\n> Different decision trees use different methods for splitting. CART uses Gini, ID3 and C4.5 uses Entropy.\n---\n* When there are many features, the decision trees will be divided into many, and in such cases it will be a large complex tree. As a result, an overfit situation will occur. To prevent this, we can specify the minimum number of training inputs to be used in each leaf. For example, a minimum of 10 values can be used to decide the case of our target variable (1.0), and we can ignore leaves with less than 10 values. Another way is to determine the maximum depth, maximum depth refers to the length of the longest path from root to leaf.\n\n* We can increase the performance of a tree by pruning, we can increase model success by removing some low-significant variables from the model, and we can prevent overfit by reducing complexity. Pruning can start at the root or leaves, the simplest pruning is at the leaf and removes every node on that leaf with the most popular class, this change being preserved as long as it doesn't impair accuracy.\n\n---\n\n## Advantages of Decision Tree\n * It indirectly performs variable scanning or feature selection.\n * It can process both numeric and categorical data and find a solution to the multiple output problem.\n * Requires little effort in data preprocessing parts.\n * Unaffected by outliers and missing observations\n * Nonlinear relationships between parameters do not affect tree performance.\n \n## Disadvantages of Decision Tree\n* Decision trees can form complex trees whose data is not generalized, which introduces the overfit problem.\n* Decision trees can be unstable because small changes in the data can result in a completely different tree. This is called the variance that needs to be reduced by bagging and boosting methods.\n* If some classes are dominant in the data set, biased decision trees may occur. It would be good to balance the dataset before creating the decision tree.","361636b3":"* **We see the need for model validation**\n---\n* **We can say that it is a successful model of around 65%**","8213c0be":"* You don't have to deal with problems such as missing data and outliers in decision trees. Decision trees are not affected by these situations. The classification problem is unaffected, but if it were a regression problem, the outliers in the dependent variable might have a minor significance, but this would be negligible.\n\n---\n* To illustrate this situation, I will not cover any data preprocessing and feature engineering steps before building the model.","09b2e8bb":"# Decision Tree\n\n---\n\n<center><img\nsrc=\"https:\/\/i.pinimg.com\/originals\/ab\/8b\/39\/ab8b3911155c14eea07fd04c45cfab5a.jpg\" style=\"width:50%;height:50%;\">\n<\/center>\n\n---\n\n* A decision tree is a hierarchical data structure that implements the divide-and-conquer strategy. It is a non-parametric method used for classification and regression. A decision tree consists of internal decision nodes and terminal leaves. Each decision node implements a test function with different results that labels the branches. Given an input, a test is run at each node and depending on the result, one of the branches is taken. This process begins at the root and iteratively repeats until it reaches a leaf node.\n\n> In parametric estimation, we define a model over the entire input field and learn its parameters from all training data. In nonparametric estimation, we divide the input space into local regions defined by a distance measure such as the Euclidean norm, and for each input the corresponding local models calculated from the training data in that region are used.\n\n---\n### Decision trees basically learn the hierarchy of if-else questions\n\n---\n* Decision trees are a non-cyclical graph used for decision making.\n> Graph: It is a construct that specifies a set of objects to which pairs of objects are in some sense \"related\".\n* At each branching node of the graph, a certain property \"j\" of the feature vector is checked. A threshold is selected for the feature, and if the feature falls below this threshold, the left branch is followed, otherwise the right branch is followed. When the leaf node is reached, the class decision is made.\n* A decision tree starts with a single node, which is split into possible outcomes. Each of the allocated possible outcomes is split into different possible outcomes and additional nodes are created which form a tree view.\n---\n### Decision tree learns from data\n---\n> In the tree model, besides determining when the model should stop in the background, it is decided which features to choose and which conditions to use for splitting. Since the tree will grow haphazardly, it will need to be pruned.\n\n---\n<center><img\nsrc=\"https:\/\/annalyzin.files.wordpress.com\/2016\/07\/decision-tree-tutorial-animated3.gif\" style=\"width:50%;height:50%;\">\n<\/center>\n\n---\n**There are 3 types of nodes in the decision tree:**\n - Lucky Knot: indicated by a circle and shows the probabilities of certain outcomes.\n - Decision Node: indicated by a square and indicates a decision to be made.\n - Final Node: shows the final result of a decision path.\n<center><img\nsrc=\"https:\/\/cdn-cashy-static-assets.lucidchart.com\/lucidspark\/marketing\/blog\/2020Q4\/decision-tree\/Decision-tree.png\" style=\"width:75%;height:75%;\">\n<\/center>\n\n\n---\n\n> A decision tree is considered optimal when it represents the most data with the least number of questions. Algorithms designed to generate optimized decision trees include CART, ASSISTANT, CLS, and ID3-4-5.\n\n* The decision tree should identify the optimal point to split the data at each level. It does this with one of the techniques gini, information gain and variance reduction.\n\n# Gini\n---\nIt is a measure of how often a randomly selected item from the set will be mislabeled. Gini is calculated with the following formula:\n<center><img\nsrc=\"https:\/\/ichi.pro\/assets\/images\/max\/724\/1*otdoiyIwxJI-UV0ukkyutw.png\" style=\"width:25%;height:25%;\">\n<\/center>\n- Pj is the probability of class j.\n\nThe minimum value of the Gini index is 0. If the value is zero, it indicates that all data has a single label. In this case, the node is not split again and the optimum solution is chosen by features with less gini values. If there are equal amounts of two classes (0-1), the gini index takes the maximum value\n\n---\n\n### $$GINI_{min} = 1 - 1^2 = 0$$\n\n### $$GINI_{max} = 1 - (0.5^2 + 0.5^2) = 0.5$$\n\n---\n\n* E.g; we have 4 gummies and we want to estimate the red-blue label, let's calculate how often they will be measured incorrectly. Let's assume that the 4 gums we have are blue in color, where the gini is 0 because no gum can be miscalculated. If we choose to arbitrarily label the gums red, the gini will still be 0. Because we won't be able to guess any gum correctly. Assuming we have 2 blue and 2 red gums, the gini will be 0.5. We will guess half right and half wrong. For binary target variables, 0.5 is the least pure possible value. Tags are half scattered. If we divide the Gini scores by 0.5 it will be 0.5 \/ 0.5 = 1, so the grouping is not pure. 3 red and 1 blue gum;\n\n\n### $$GiniIndex = 1 - (0.75^2 + 0.25^2) = 0.375 $$\n\n- The Gini Index will be 0.375. If we divide this by 0.5, the probability of incorrect\/correct labeling will be 0.75.\n\n---\n\n# Entropy\n\n---\n\nWe calculate the entropy using the following formula:\n\n\n<center><img\nsrc=\"https:\/\/miro.medium.com\/max\/1008\/1*_Sj7YkkUJSOzDRv9_DATlQ.png\" style=\"width:25%;height:25%;\">\n<\/center>\n\n\n* Because Entropy uses logarithms, it is heavier to calculate. The basic idea, like Gini, is to measure the disorder of a group against the target variable. Again, like Gini, the optimum distribution is determined by the feature with less Entropy value, when the probability of the two classes is the same, the maximum Entropy is, and if the data consists of a label belonging to a single class, the node is pure, the Entropy minimum is 0.\n\n---\n\n### $$Entropy_{min} = -1.log_2(1) = 0$$\n### $$Entropy_{max} = -0.5.log_2(0.5)-0.5.log_2(0.5) = 1$$\n\n* If we use the chewing gum example above for entropy\n\n**For 4 blue 0 red gum:**\n\n$$Entropy = [(4\/4) . log_2(4\/4)] - [(0\/4) . log_2(0\/4)] = 0 $$\n\n**For 2 blue 2 red gums:**\n\n$$Entropy = [(2\/4) . log_2(2\/4)] - [(2\/4) . log_2(2\/4)] = 1 $$\n\n**For 3 blue 1 red gum:**\n\n$$Entropy = [(3\/4) . log_2(3\/4)] - [(1\/4) . log_2(1\/4)] = 0.811 $$\n\n\n\n#### Here, the false\/correct labeling ratio will be 0.811. Slightly worse than the Gini score.","920c3caf":"### Extracting Python Code of Decision Rules","63781d4f":"### Make Decision Rules","f0671921":"### Does our model look very successful ?\n ### Of course not, our model memorized the structure instead of learning it. There is no way we can predict 1 in real life, of course we will make mistakes.\n\n\n<center><img\nsrc=\"https:\/\/miro.medium.com\/max\/1400\/1*YqWguKepue2PlBX0LBqzbg.gif\" style=\"width:50%;height:50%;\">\n<\/center>\n\n"}}