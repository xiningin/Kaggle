{"cell_type":{"907e4286":"code","3b53e441":"code","d8bbb3c7":"code","7b0bf00b":"code","e247b7ee":"code","77f2d788":"code","d338055e":"code","3cccbb00":"code","39a59346":"code","10155d34":"code","433f88fa":"code","28ebe260":"code","f2957277":"code","118cc698":"code","8f60bdd2":"code","705d3cec":"code","87b94dc2":"code","fed63725":"code","0ad3068a":"code","ffa888ec":"code","03b49f42":"code","b816b1c4":"code","d37d0522":"code","fb629bdb":"code","aa99fa12":"code","ba69a25b":"code","dd3e6d9a":"code","9fe11c5c":"code","377a5b06":"code","0e939f55":"code","248ad6eb":"code","05844c17":"code","84ac28e6":"code","1bb81b79":"code","9a6f1eac":"code","ef1e579f":"code","d4bcfac5":"code","baab3015":"code","bab37a3d":"code","7ef6dc40":"code","f8eeeb1c":"code","3f8218e4":"code","002212ac":"code","472a30da":"code","1efea43c":"code","59f233a6":"code","9bf6f5d4":"code","6a39539d":"code","ce95158c":"code","2921fffa":"code","fbc5d65d":"code","2818cd89":"code","449bf826":"code","7e7e0ca7":"code","432f8d9e":"code","a7c14f0b":"code","c77e655d":"code","3f8e47a8":"code","a3217c25":"code","68205ba4":"code","a61a4c10":"code","cdd358df":"code","131f744f":"code","eff9b542":"code","4bed2bd8":"code","9f307f00":"code","1a772805":"code","3c0d0e6b":"code","e90a0a95":"code","d8466474":"code","2c196a06":"code","18a0f896":"code","f8bf65f8":"code","48980b51":"code","c2b4d66b":"code","339ca86d":"code","51e6b2f0":"markdown","a56f78bc":"markdown","738b861a":"markdown","8cbec373":"markdown","4ce5f386":"markdown","cb5d9d58":"markdown","c199c1aa":"markdown","cb1e6c9b":"markdown","1a249c0e":"markdown","d36237db":"markdown","f4a8670f":"markdown","c81ab91e":"markdown","7d9dc800":"markdown","bfde7272":"markdown","6978fb4a":"markdown"},"source":{"907e4286":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport xgboost as xgb\nimport csv\nimport os\n\nfrom itertools import zip_longest\nfrom sklearn import svm, model_selection, preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\n\n%matplotlib inline","3b53e441":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d8bbb3c7":"!unzip \/kaggle\/input\/sberbank-russian-housing-market\/train.csv.zip\n!unzip \/kaggle\/input\/sberbank-russian-housing-market\/test.csv.zip\n!unzip \/kaggle\/input\/sberbank-russian-housing-market\/macro.csv.zip\n!unzip \/kaggle\/input\/sberbank-russian-housing-market\/sample_submission.csv.zip","7b0bf00b":"!ls \/kaggle\/working","e247b7ee":"train = pd.read_csv(\"\/kaggle\/working\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/working\/test.csv\")","77f2d788":"pd.set_option('display.max_rows', None)\ntrain.head(5).transpose()","d338055e":"train.shape","3cccbb00":"pd.DataFrame(train.dtypes.value_counts())","39a59346":"missingValueColumns = train.columns[train.isnull().any()].tolist()\nmsno.bar(train[missingValueColumns], figsize=(20,8),color=(0.5, 0.5, 1),fontsize=12,labels=True,)","10155d34":"plt.hist(train['kremlin_km'], bins=100)","433f88fa":"plt.hist(train['nuclear_reactor_km'], bins=100)","28ebe260":"mean_price = np.mean(train['price_doc'])","f2957277":"plt.hist(train['price_doc'], bins=100)","118cc698":"plt.hist(np.log(train['price_doc']), bins=100)","8f60bdd2":"# Prepare dataset","705d3cec":"X_train = train.drop(['timestamp'], axis=1)\ny_train = train['price_doc']\nfeature_names = list(X_train.columns)","87b94dc2":"for c in X_train.columns:\n    if X_train[c].dtype == 'object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(X_train[c].values)) \n        X_train[c] = lbl.transform(list(X_train[c].values))\n\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\nimp = imp.fit(X_train)\nX_train = pd.DataFrame(imp.transform(X_train), columns=feature_names)","fed63725":"y_train.drop(X_train[X_train['full_sq'] == 0].index, inplace=True)\nX_train.drop(X_train[X_train['full_sq'] == 0].index, inplace=True)\nX_train['price_doc_per_sq'] = y_train \/ X_train['full_sq']\nX_train['average_price_doc_per_sq_near'] = X_train['price_doc_per_sq']","0ad3068a":"print(X_train.shape)\nprint(y_train.shape)","ffa888ec":"np.sum(np.isinf(X_train['price_doc_per_sq']))","03b49f42":"def get_candidates(row, X_train, eps, training=False):\n    less_kremlin = row['kremlin_km'] - eps < X_train['kremlin_km']\n    greater_kremlin = X_train['kremlin_km'] < row['kremlin_km'] + eps\n    less_nuclear = row['nuclear_reactor_km'] - eps < X_train['nuclear_reactor_km']\n    greater_nuclear = X_train['nuclear_reactor_km'] < row['nuclear_reactor_km'] + eps\n    same_district = X_train['sub_area'] == row['sub_area']\n    final = less_kremlin  \\\n        & greater_kremlin \\\n        & same_district   \\\n        & less_nuclear    \\\n        & greater_nuclear\n        \n    if training:\n        not_itself = X_train['id'] != row['id']\n        final = final & not_itself\n    return final","b816b1c4":"neighbors = []\nfor i, row in X_train.iterrows():\n    l_eps = 0 # km\n    r_eps = 2 #km\n\n    while r_eps - l_eps > 0.01: # TODO tune\n        m_eps = (l_eps + r_eps) \/ 2\n        candidates = get_candidates(row, X_train, m_eps, training=True)\n        if candidates.sum() <= 10:\n            l_eps = m_eps\n        elif candidates.sum() > 10:\n            r_eps = m_eps\n        else:\n            break\n    \n    X_train['average_price_doc_per_sq_near'][i] = np.mean(X_train['price_doc_per_sq'][candidates]) * X_train['full_sq'][i] if candidates.sum() != 0 else mean_price\n    neighbors.append(candidates.sum())\n    if i % 1000 == 0:\n        print(\"progress: {}\/{}\".format(i, X_train.shape[0]))","d37d0522":"arr = np.array(neighbors)\nprint(arr.min())\nprint(arr.max())\nprint(np.median(arr))\n(arr == 0).sum()","fb629bdb":"np.sum(np.isinf(X_train['average_price_doc_per_sq_near']))","aa99fa12":"plt.hist(X_train['average_price_doc_per_sq_near'])","ba69a25b":"# svm_model = svm.SVR(kernel='linear', epsilon=0.1, max_iter=10000).fit(X_train, y_train)","dd3e6d9a":"# imp = list(zip(abs(svm_model.coef_[0]), feature_names))\n# imp.sort(key=lambda x: x[0], reverse=True)\n# imp, names = list(zip(*imp))\n# imp = imp[:15]\n# names = names[:15]","9fe11c5c":"# names","377a5b06":"# plt.barh(range(len(imp)), imp, align='center')\n# plt.yticks(range(len(names)), names)\n# plt.show()","0e939f55":"\nxgb_params = {\n    'eta': 0.05,\n    'max_depth': 5,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmsle',\n    'silent': 1\n}\n\ndtrain = xgb.DMatrix(X_train.drop(['price_doc', 'price_doc_per_sq', 'id'], axis=1), y_train)\n\ncv_output = xgb.cv(xgb_params, dtrain, num_boost_round=1000, early_stopping_rounds=20,\n    verbose_eval=50, show_stdv=False)\ncv_output[['train-rmsle-mean', 'test-rmsle-mean']].plot()","248ad6eb":"num_boost_rounds = len(cv_output)\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)","05844c17":"featureImportance = model.get_fscore()\nfeatures = pd.DataFrame()\nfeatures['features'] = featureImportance.keys()\nfeatures['importance'] = featureImportance.values()\nfeatures.sort_values(by=['importance'],ascending=False,inplace=True)\nfig,ax= plt.subplots()\nfig.set_size_inches(20,10)\nplt.xticks(rotation=60)\nsns.barplot(data=features.head(30),x=\"features\",y=\"importance\",ax=ax,orient=\"v\")","84ac28e6":"features.head(30)['features']","1bb81b79":"topFeatures = features[\"features\"].tolist()[:30]\ntopFeatures.append(\"price_doc\")\ncorrMatt = X_train[topFeatures].corr()\nmask = np.array(corrMatt)\nmask[np.tril_indices_from(mask)] = False\nfig,ax= plt.subplots()\nfig.set_size_inches(30,30)\nsns.heatmap(corrMatt, mask=mask,vmax=.8, square=True,annot=True, center=0)","9a6f1eac":"bad_features = ['cafe_count_5000_price_high', 'cafe_count_2000', 'cafe_count_3000_price_2500', 'cafe_count_3000', 'cafe_count_2000_price_2500',\n               'swim_pool_km', 'ttk_km',\n               'cafe_count_5000_price_2500', 'sport_count_3000',\n               'university_km', 'theater_km',\n               'kindergarten_km',\n               'railroad_km',\n               'metro_min_avto',\n               'cafe_count_5000', 'cafe_count_3000_price_1500', 'cafe_count_5000_price_1500', 'cafe_count_5000_price_1500', 'cafe_count_3000_price_1000', 'cafe_count_5000_price_4000',\n               'nuclear_reactor_km', 'radiation_km',\n               'zd_vokzaly_avto_km', 'metro_km_avto',\n               'detention_facility_km', 'ice_rink_km',\n               'cafe_count_1000', 'cafe_count_2000_price_1000', 'cafe_sum_5000_min_price_avg', 'cafe_count_5000_price_1000',\n               'industrial_km', 'school_km', 'big_road1_km', 'park_km', 'trc_count_3000', 'exhibition_km',\n               'office_count_3000', 'office_count_5000']\n","ef1e579f":"# x_train = train.drop([\"id\", \"timestamp\", \"price_doc\"], axis=1)\n# x_train = x_train.drop(labels=bad_features, axis=1)\n\n\n# for c in x_train.columns:\n#     if x_train[c].dtype == 'object':\n#         lbl = preprocessing.LabelEncoder()\n#         lbl.fit(list(x_train[c].values)) \n#         x_train[c] = lbl.transform(list(x_train[c].values))\n        \n# dtrain = xgb.DMatrix(x_train, y_train)\n# cv_output = xgb.cv(xgb_params, dtrain, num_boost_round=100, early_stopping_rounds=20,\n#     verbose_eval=50, show_stdv=False)\n# cv_output[['train-rmsle-mean', 'test-rmsle-mean']].plot()\n# num_boost_rounds = len(cv_output)\n# model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round= num_boost_rounds)","d4bcfac5":"# featureImportance = model.get_fscore()\n# features = pd.DataFrame()\n# features['features'] = featureImportance.keys()\n# features['importance'] = featureImportance.values()\n# features.sort_values(by=['importance'],ascending=False,inplace=True)\n# topFeatures = features[\"features\"].tolist()[:30]\n# new_features = [x for x in topFeatures if x not in bad_features]\n# corrMatt = train[new_features].corr()\n# mask = np.array(corrMatt)\n# mask[np.tril_indices_from(mask)] = False\n# fig,ax= plt.subplots()\n# fig.set_size_inches(30,30)\n# sns.heatmap(corrMatt, mask=mask,vmax=.8, square=True,annot=True, center=0)","baab3015":"# final_features = ['full_sq',\n#  'life_sq',\n#  'kitch_sq',\n#  'num_room',\n#  'build_year',\n#  'max_floor',\n#  'floor',\n#  'state',\n#  'sadovoe_km',\n#  'metro_min_walk',\n#  'public_healthcare_km',\n#  'office_sqm_5000',\n#  'public_transport_station_km',\n#  'church_count_5000',\n#  'material',\n#  'sub_area',\n#  'ID_metro',\n#  'indust_part']","bab37a3d":"final_features = ['full_sq',\n                  'life_sq',\n                  'build_year',\n                  'num_room',\n                  'kitch_sq',\n                  'floor',\n                  'state',\n                  'max_floor',\n                  'kremlin_km',\n                  'nuclear_reactor_km',\n                  'metro_min_walk',\n                  'public_healthcare_km',\n                  'office_sqm_5000',\n                  'public_transport_station_km']","7ef6dc40":"optional_features = ['average_price_doc_per_sq_near']","f8eeeb1c":"len(final_features)","3f8218e4":"corrMatt = X_train[final_features + ['average_price_doc_per_sq_near', 'price_doc']].corr()\nmask = np.array(corrMatt)\nmask[np.tril_indices_from(mask)] = False\nfig,ax= plt.subplots()\nfig.set_size_inches(30,30)\nsns.heatmap(corrMatt, mask=mask,vmax=.8, square=True,annot=True, center=0)","002212ac":"# y_train = train[\"price_doc\"]\n# x_train = train[final_features]\n# x_test = test[final_features]\n\n# for c in x_train.columns:\n#     if x_train[c].dtype == 'object':\n#         lbl = preprocessing.LabelEncoder()\n#         lbl.fit(list(x_train[c].values)) \n#         x_train[c] = lbl.transform(list(x_train[c].values))\n        \n# for c in x_test.columns:\n#     if x_test[c].dtype == 'object':\n#         lbl = preprocessing.LabelEncoder()\n#         lbl.fit(list(x_test[c].values)) \n#         x_test[c] = lbl.transform(list(x_test[c].values))\n\n# xgb_params = {\n#     'eta': 0.05,\n#     'max_depth': 5,\n#     'subsample': 0.7,\n#     'colsample_bytree': 0.7,\n#     'objective': 'reg:linear',\n#     'eval_metric': 'rmsle',\n#     'silent': 1\n# }\n\n# dtrain = xgb.DMatrix(x_train, y_train)\n# dtest = xgb.DMatrix(x_test)\n\n# cv_output = xgb.cv(xgb_params, dtrain, num_boost_round=1000, early_stopping_rounds=100,\n#     verbose_eval=50, show_stdv=False)\n# cv_output[['train-rmsle-mean', 'test-rmsle-mean']].plot()","472a30da":"knn_dtrain = xgb.DMatrix(X_train[final_features + optional_features], y_train)","1efea43c":"cv_output = xgb.cv(xgb_params, knn_dtrain, num_boost_round=1000, early_stopping_rounds=900,\n    verbose_eval=50, show_stdv=False)\ncv_output[['train-rmsle-mean', 'test-rmsle-mean']].plot()","59f233a6":"knn_model = xgb.train(dict(xgb_params, silent=0), knn_dtrain, num_boost_round=1000)","9bf6f5d4":"X_train[final_features + optional_features].head()","6a39539d":"# average_price_doc_per_sq_near","ce95158c":"# x_test['kremlin_km'] = test['kremlin_km']\n# x_test['nuclear_reactor_km'] = test['nuclear_reactor_km']\n# x_test['average_price_doc_per_sq_near'] = x_test['full_sq']","2921fffa":"X_test = test[final_features]\nX_test['average_price_doc_per_sq_near'] = X_test['full_sq']","fbc5d65d":"imp = SimpleImputer(missing_values=np.nan, strategy='mean')\nimp = imp.fit(X_test)\nX_test = pd.DataFrame(imp.transform(X_test), columns=final_features + ['average_price_doc_per_sq_near'])","2818cd89":"X_test['sub_area'] = test['sub_area']","449bf826":"X_test.head()","7e7e0ca7":"train.drop(train[train['full_sq'] == 0].index, inplace=True)","432f8d9e":"X_train['sub_area'] = train['sub_area']","a7c14f0b":"test_neighbors = []\nfor i, row in X_test.iterrows():\n    l_eps = 0 # km\n    r_eps = 2 # km\n\n    while r_eps - l_eps > 0.01: #TODO tune\n        m_eps = (l_eps + r_eps) \/ 2\n        candidates = get_candidates(row, X_train, m_eps, training=False)\n        if candidates.sum() <= 10:\n            l_eps = m_eps\n        elif candidates.sum() > 10:\n            r_eps = m_eps\n        else:\n            break\n    \n    X_test['average_price_doc_per_sq_near'][i] = np.mean(X_train['price_doc_per_sq'][candidates]) * X_test['full_sq'][i] if candidates.sum() != 0 else mean_price\n    test_neighbors.append(candidates.sum())\n    if i % 1000 == 0:\n        print(i)","c77e655d":"X_test.drop(['sub_area'], axis=1, inplace=True)","3f8e47a8":"X_test.head()","a3217c25":"dtest = xgb.DMatrix(X_test)","68205ba4":"test_neighbors_np = np.array(test_neighbors)\nprint(test_neighbors_np.min())\nprint(test_neighbors_np.max())\nprint(np.median(test_neighbors_np))\nprint(np.sum(test_neighbors_np == 0))","a61a4c10":"ans[list(np.where(test_neighbors_np == 0)[0])]","cdd358df":"# candidates = get_candidates(row, X_train, 0)\n# candidates.sum()","131f744f":"# plt.hist(x_test['kremlin_km'], bins=100)","eff9b542":"# from scipy import stats\n# stats.mode(x_test['average_price_doc_per_sq_near'])","4bed2bd8":"num_boost_rounds = len(cv_output)\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round= num_boost_rounds)","9f307f00":"# x_knn_train = X_train[final_features + ['average_price_doc_per_sq_near']]\n# d_knn_train = xgb.DMatrix(x_knn_train, y_train)\n# knn_model = xgb.train(dict(xgb_params, silent=0), d_knn_train, num_boost_round= num_boost_rounds)","1a772805":"ans = knn_model.predict(dtest)","3c0d0e6b":"plt.hist(ans, bins=100)","e90a0a95":"any(ans < 0)","d8466474":"np.any(np.isnan(ans))","2c196a06":"np.any(np.isinf(ans))","18a0f896":"# for i, val in enumerate(ans):\n#     if val < 0: \n#         print(i, val)","f8bf65f8":"# ans[6289] = train['price_doc'].mean()","48980b51":"subm = np.array(list(zip(test['id'], ans)))\ndf = pd.DataFrame(subm, columns=['id', 'price_doc'])\ndf['id'] = df['id'].astype('int64')","c2b4d66b":"df.head()","339ca86d":"df.to_csv('\/kaggle\/working\/output-ver3.csv', index=False)","51e6b2f0":"## Here we carefully exclude features that are highly correlated with others.\n## There is few expertise beyound the scene. For example, cafes are located in office premises and we don't need to keep both features.\n## Also, distance from metro in kilometers and in minutes means the same.\n## After several iterations we exclude following features.","a56f78bc":"# Final private score: RMSLE = 0.32382\n# ~ 1682th rank on the private leaderboard","738b861a":"## Let's investigate data types and missing values","8cbec373":"## Now we can discover features that xgboost considers to be important.","4ce5f386":"### We will use neighborhood in order to estimate price_doc_per_sq_meter\n\n### So, we need anchors. One of them may be heart of Moscow.\n\n### Then, there is a few nuclear reactors in Moscow. One one then in kurchatov institute.","cb5d9d58":"## Seems like SVM with linear kernel and tremendous number of features would not approximate prices well. ","c199c1aa":"## Plan B: use xgboost.","cb1e6c9b":"## Dealing with multicollinearity\n\n### Unfortunantly, most of this features are hightly correlated:","1a249c0e":"## Required packages","d36237db":"## After another one selection we choose following features:","f4a8670f":"## it might be little confusing to remove universities, schools and kindergartens from feature space, but it is too highly correlated with publics healthcare which is more important.","c81ab91e":"# And now we can fit xgboost in this feature space","7d9dc800":"## price_doc distribution looks like lognormal","bfde7272":"## Suddenly there is one negative prediction in testing. Let's just replace it with the sample mean","6978fb4a":"## Here we try to obtain important features from  svm.SVR coefficients"}}