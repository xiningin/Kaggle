{"cell_type":{"5fd6568b":"code","0863db37":"code","2951ad19":"code","27edb133":"code","592bac83":"code","c3ffc3ca":"code","f5eb799d":"code","b9eb6cd2":"code","9733aaed":"code","e0f10e23":"code","11363a4f":"code","06fa6c9a":"code","ad9eac0a":"code","45ab60df":"code","b4010567":"code","55927e55":"code","b9f110f3":"code","23fdfa22":"code","69f1ef85":"code","82d63246":"code","05f75d0a":"code","be25ebd0":"code","6899e2eb":"code","4a05a889":"code","191e76ab":"code","3cfa5cd8":"code","91d3e671":"code","5c226579":"code","ab85234d":"code","dec75e17":"code","f0730eed":"code","dd470dee":"code","57c5e4c9":"code","526d2e74":"code","49b07c7a":"code","1d23d068":"code","3ada9cdf":"code","5e5e68aa":"code","9c9d6e35":"code","ebb2b429":"code","3cce8550":"code","7fea3335":"code","e073b3f9":"code","43aa64ba":"code","ce32f941":"code","9bfc4e19":"code","222ca103":"code","4bd12fa6":"code","b698b6ae":"code","feecd019":"code","bfafcbab":"code","9df40e10":"code","675030fc":"code","97c1a67a":"code","9b81711f":"code","097e82a0":"code","3c0c0b39":"code","7d7215b9":"markdown","7eba59c1":"markdown","c5229a3c":"markdown","aedd6f4c":"markdown","799a8097":"markdown","a1e92e01":"markdown","997ba89a":"markdown","c7e4bf1e":"markdown","e544ac1c":"markdown","6375deeb":"markdown","dccfcfd9":"markdown","aef39786":"markdown","4241b9ed":"markdown","92da9d3d":"markdown","7d683c96":"markdown","073a8aa5":"markdown","53c65a8a":"markdown","84b8cb48":"markdown","1ac71b91":"markdown","a9b63b30":"markdown","a72835cc":"markdown","c2654d88":"markdown","1aceec51":"markdown","a25cdfa1":"markdown","dcb3f671":"markdown","c58e240f":"markdown","bc67a398":"markdown","ce324f93":"markdown","461bbeee":"markdown","e70c4584":"markdown","faa4c61f":"markdown","e974f7b5":"markdown","f0379f56":"markdown","e5c74d80":"markdown","cec99830":"markdown","a94ad561":"markdown","60c1f84e":"markdown","47712481":"markdown","e11267b7":"markdown","5af6b875":"markdown","e268badd":"markdown","18013d18":"markdown","d1075093":"markdown","354fdc71":"markdown","a3b4d3f1":"markdown","236a7049":"markdown","d822b725":"markdown","ec41a7bb":"markdown","265842a0":"markdown","8f32802f":"markdown","6d4fce35":"markdown","1da4b338":"markdown","168fb3cc":"markdown","4c3ee12e":"markdown","92ebfc96":"markdown","badec6d1":"markdown","c69596b4":"markdown","3068ddc9":"markdown","767c15f8":"markdown","4bff7df2":"markdown","d57c0101":"markdown"},"source":{"5fd6568b":"!pip install ftfy\n!pip install gensim","0863db37":"!pip install scikit-optimize","2951ad19":"!pip install skorch\n!pip install scikit-learn==0.23.0\n\nfrom numpy.ma import MaskedArray\nimport sklearn.utils.fixes\n\nsklearn.utils.fixes.MaskedArray = MaskedArray","27edb133":"# Geral\nimport numpy as np\nimport pandas as pd\n\n# Visualiza\u00e7\u00e3o de dados\nimport seaborn as sns\n\n# Processamento de textos \nfrom ftfy import fix_text\nimport string\nimport re\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\n\n# Modelos\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.neural_network import MLPClassifier\n\nimport torch\nimport skorch\nfrom torch import optim\nfrom skorch import NeuralNetClassifier\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Treino e compara\u00e7\u00e3o\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import loguniform as sp_loguniform\nfrom sklearn.metrics import roc_auc_score\n\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Integer\n","592bac83":"train_ds = pd.read_csv(\"..\/input\/sentiment-analysis-pmr3508\/data_train.csv\") ","c3ffc3ca":"train_ds.head()","f5eb799d":"train_ds.shape","b9eb6cd2":"sns.catplot(x=\"positive\", kind=\"count\", data=train_ds)","9733aaed":"train_ds = train_ds.drop_duplicates(keep='first')","e0f10e23":"train_ds.shape","11363a4f":"print(\"Dados faltantes na base de treino: \", train_ds.isna().sum().sum())","06fa6c9a":"def preptext(text):\n    txt=text.replace(\"<br \/>\",\" \") #retirando tags\n    txt=fix_text(txt) #consertando Mojibakes (Ver https:\/\/pypi.org\/project\/ftfy\/)\n    txt=txt.lower() #passando tudo para min\u00fasculo\n    txt=txt.translate(str.maketrans('', '', string.punctuation)) #retirando toda pontua\u00e7\u00e3o\n    txt=txt.replace(\" \u2014 \", \" \") #retirando h\u00edfens\n    txt=re.sub(\"\\d+\", ' <number> ', txt) #colocando um token especial para os n\u00fameros\n    txt=re.sub(' +', ' ', txt) #deletando espa\u00e7os extras\n    return txt","ad9eac0a":"train_X = train_ds['review']\ntrain_Y = train_ds['positive']","45ab60df":"train_X.head()","b4010567":"train_Y.head()","55927e55":"%%time\n\n\ntrain_X = train_X.apply(preptext)","b9f110f3":"train_X.head()","23fdfa22":"%%time\n\n\ntrain_X = train_X.apply(lambda x: x.split())","69f1ef85":"train_X.head()","82d63246":"d2v = Doc2Vec.load(\"..\/input\/sentiment-analysis-pmr3508\/doc2vec\")","05f75d0a":"def emb(txt, model, normalize=False): \n    model.random.seed(42)\n    x=model.infer_vector(txt, steps=20)\n    \n    if normalize: return(x\/np.sqrt(x@x))\n    else: return(x)","be25ebd0":"%%time\n\n\ntrain_X = [emb(x, d2v) for x in train_X] \ntrain_X = np.array(train_X)","6899e2eb":"train_X","4a05a889":"train_X.shape","191e76ab":"test_ds = pd.read_csv(\"..\/input\/sentiment-analysis-pmr3508\/data_test1.csv\") \nsub_ds = pd.read_csv(\"..\/input\/sentiment-analysis-pmr3508\/data_test2_X.csv\") ","3cfa5cd8":"test_ds.head()","91d3e671":"test_ds.shape","5c226579":"sub_ds.head()","ab85234d":"sub_ds.shape","dec75e17":"test_X = test_ds['review']\ntest_Y = test_ds['positive']\n\nsub_X = sub_ds['review']","f0730eed":"%%time\n\n\ntest_X = [preptext(x).split() for x in test_X]\ntest_X = [emb(x, d2v) for x in test_X] \ntest_X = np.array(test_X)\n\nsub_X = [preptext(x).split() for x in sub_X]\nsub_X = [emb(x, d2v) for x in sub_X] \nsub_X = np.array(sub_X)","dd470dee":"test_X","57c5e4c9":"sub_X","526d2e74":"test_X.shape","49b07c7a":"sub_X.shape","1d23d068":"%%time\n\n\n# Modelo de k Vizinhos mais Pr\u00f3ximos\nknn = KNeighborsClassifier()\n\n# Hiperpar\u00e2metros a serem otimizados: 'n_neighbors'\nhyperparams = {'n_neighbors':[(i) for i in np.arange(1, 50)]}\n\n\n# Busca de Hiperpar\u00e2metros\nknn_clf = RandomizedSearchCV(knn, hyperparams, scoring='roc_auc', n_iter=50, cv=2, n_jobs=-1, random_state=0, verbose=2)\nknn_search = knn_clf.fit(train_X, train_Y)","3ada9cdf":"knn_search.best_params_, knn_search.best_score_","5e5e68aa":"%%time\n\n\n# Modelo de Regress\u00e3o Log\u00edstica\nlogreg = LogisticRegression(solver='liblinear',random_state=42)\n\n# Hiperpar\u00e2metros a serem otimizados: 'C' e tipo de regulariza\u00e7\u00e3o\nhyperparams = dict(C=np.linspace(0,10,100), \n                   penalty=['l2', 'l1'])\n\n# Busca de Hiperpar\u00e2metros\nlogreg_clf = RandomizedSearchCV(logreg, hyperparams, scoring='roc_auc', n_iter=50, cv=2, n_jobs=-1, random_state=0, verbose=2)\nlogreg_search = logreg_clf.fit(train_X, train_Y)","9c9d6e35":"logreg_search.best_params_, logreg_search.best_score_","ebb2b429":"%%time\n\n\n# Modelo de Rede Neural de 1 camada escondida\nmlp_1l = MLPClassifier(random_state=10, early_stopping=True)\n\n# Hiperpar\u00e2metros a serem otimizados\nhyperparams = {'hidden_layer_sizes': [(2 ** i) for i in np.arange(6, 12)],\n               'alpha': sp_loguniform(0.000001, 0.1),\n               'learning_rate': ['constant','adaptive']}\n\n# Busca de Hiperpar\u00e2metros\nmlp_clf_1l = RandomizedSearchCV(mlp_1l, hyperparams, scoring='roc_auc', n_iter=30, cv=2, n_jobs=-1, random_state=0, verbose=2)\nsearch_mlp = mlp_clf_1l.fit(train_X, train_Y)","3cce8550":"search_mlp.best_params_, search_mlp.best_score_","7fea3335":"%%time\n\n\n# Modelo de Rede Neural de 2 camadas escondidas\nmlp_2l = MLPClassifier(random_state=42, early_stopping=True)\n\n# Hiperpar\u00e2metros a serem otimizados\nhyperparams = {'hidden_layer_sizes': [(2 ** i, 2 ** j) for j in np.arange(6, 10) for i in np.arange(6, 10)],\n               'alpha': sp_loguniform(0.000001, 0.1),\n               'learning_rate': ['constant','adaptive']}\n\n# Busca de Hiperpar\u00e2metros\nmlp_clf_2l = RandomizedSearchCV(mlp_2l, hyperparams, scoring='roc_auc', n_iter=25, cv=2, n_jobs=-1, random_state=0, verbose=2)\nsearch_mlp = mlp_clf_2l.fit(train_X, train_Y)","e073b3f9":"search_mlp.best_params_, search_mlp.best_score_","43aa64ba":"# Nossa rede neural\nclass MLPNet(nn.Module):\n    def __init__(self, hidden1_dim=512, hidden2_dim=64, p=0.25):\n        super().__init__()\n        self.fc1 = nn.Linear(50, hidden1_dim)           \n        self.fc2 = nn.Linear(hidden1_dim, hidden2_dim)  \n        self.fc3 = nn.Linear(hidden2_dim, 2)           \n\n        self.dropout = nn.Dropout(p)                 \n        \n    def forward(self, X, **kwargs):\n        fc_out = F.relu(self.fc1(X))                    \n        fc_out = self.dropout(fc_out)                  \n        \n        fc_out = F.relu(self.fc2(fc_out))             \n        fc_out = self.dropout(fc_out)                   \n        \n        fc_out = self.fc3(fc_out)                      \n        soft_out = F.softmax(fc_out, dim=-1)            \n        \n        return soft_out","ce32f941":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nmlp_net = MLPNet().to(device)","9bfc4e19":"skorch_net = NeuralNetClassifier(mlp_net,\n                                 max_epochs=20,\n                                 lr=1e-4,\n                                 optimizer=optim.Adam,\n                                 optimizer__weight_decay=1e-4,\n                                 train_split=False,\n                                 verbose=0,\n                                 iterator_train__shuffle=True,\n                                 )","222ca103":"%%time\n\n\n# Hiperpar\u00e2metros a serem otimizados\nhyperparams = {'module__hidden1_dim': Integer(256, 2048),\n               'module__hidden2_dim': Integer(64, 1024),\n               'module__p': Real(0.1, 0.75, prior='uniform'),\n               'optimizer__weight_decay': Real(1e-10, 1e-2, prior='log-uniform')}\n\n# Busca de Hiperpar\u00e2metros\nskorch_clf = BayesSearchCV(skorch_net, hyperparams, scoring='roc_auc', n_iter=30, cv=2, n_jobs=-1, random_state=42, verbose=0)\nsearch_skorch = skorch_clf.fit(train_X.astype(np.float32), train_Y.astype(np.int64))","4bd12fa6":"search_skorch.best_params_, search_skorch.best_score_","b698b6ae":"# Calculado a AUC do k-vizinhos mais pr\u00f3ximos\nknn_roc_auc = roc_auc_score(test_Y, knn_clf.predict_proba(test_X)[:,1])\n\nprint('AUCs --- kNN: {:.4f}'.format(knn_roc_auc))","feecd019":"# Calculando a AUC da regress\u00e3o log\u00edstica\nlogreg_roc_auc = roc_auc_score(test_Y, logreg_clf.predict_proba(test_X)[:,1])\n\nprint('AUCs --- Log. Reg.: {:.4f}'.format(logreg_roc_auc))","bfafcbab":"# Calculando a AUC da rede neural de 1 camada do sklearn\nmlp_1l_roc_auc = roc_auc_score(test_Y, mlp_clf_1l.predict_proba(test_X)[:,1])\n\nprint('AUCs --- MLP (1 camada): {:.4f}'.format(mlp_1l_roc_auc))","9df40e10":"# Calculando a AUC da rede neural de 2 camadas do sklearn\nmlp_2l_roc_auc = roc_auc_score(test_Y, mlp_clf_2l.predict_proba(test_X)[:,1])\n\nprint('AUCs --- MLP (2 camadas): {:.4f}'.format(mlp_2l_roc_auc))","675030fc":"# Calculando a AUC da rede neural do Skorch\nskorch_roc_auc = roc_auc_score(test_Y, search_skorch.predict_proba(test_X)[:,1])\n\nprint('AUCs --- MLP (Skorch): {:.4f}'.format(skorch_roc_auc))","97c1a67a":"prediction = search_skorch.predict_proba(sub_X)[:,1]","9b81711f":"submission = {'positive': prediction}\nsubmission = pd.DataFrame(submission)","097e82a0":"submission.head(10)","3c0c0b39":"submission.to_csv(\"submission.csv\", index = True, index_label = 'Id')","7d7215b9":"Como essa fun\u00e7\u00e3o ser\u00e1 aplicada apenas \u00e0 coluna da esquerda da base de dados, ou seja, apenas aos reviews\/textos, iremos separar os dados da base de treino de seus r\u00f3tulos. Assim, teremos:","7eba59c1":"### 4.1. kNN","c5229a3c":"Vejamos como ficou nosso *dataset* rec\u00e9m montado:","aedd6f4c":"Iremos utilizar o scikit-learn novamente para a otimiza\u00e7\u00e3o de hiperpar\u00e2metros, pois essa biblioteca disp\u00f5e de uma s\u00e9rie de ferramentas \u00fateis para isto. Para fazermos isso, utilizaremos a biblioteca skorch, que consegue adaptar o PyTorch para usar recursos do scikit-learn.","799a8097":"## 5. Submiss\u00e3o\n\nFinalmente chegamos \u00e0 se\u00e7\u00e3o final de nosso projeto, onde faremos a classifica\u00e7\u00e3o da base de dados n\u00e3o rotulados para submiss\u00e3o. Como j\u00e1 pr\u00e9-processamos a base de dados de submiss\u00e3o, vamos j\u00e1 realizar sua predi\u00e7\u00e3o:","a1e92e01":"Dividindo os datasets:","997ba89a":"### 3.2. Regress\u00e3o log\u00edstica\n\nUtilizando ainda um algoritmo relativamente simples comparado aos subesquentes, utilizemos uma regress\u00e3o log\u00edstica, utilizando o RandomizedSearchCV, como citado anteriormente:","c7e4bf1e":"Podemos ver que nosso modelo em PyTorch apresentou uma melhora em rela\u00e7\u00e3o \u00e0queles constru\u00eddos por meio do MLPClassifier so scikit-learn.\n\n## 4. Compara\u00e7\u00e3o dos classificadores\n\nPor mais que j\u00e1 tenhamos obtido a melhor AUC, na se\u00e7\u00e3o anterior, por meio da rede neural de PyTorch, ainda n\u00e3o temos certeza que esse \u00e9 o melhor classificador. Ou seja, testemos agora os modelos constru\u00eddos na se\u00e7\u00e3o anterior com os dados de teste, para evitar selecionarmos um modelo com problema de *overfitting*, por exemplo:","e544ac1c":"Vejamos como ficou nossa base de treino:","6375deeb":"Se compararmos com a dimens\u00e3o da base original (que obtivemos durante a fase de importa\u00e7\u00e3o), vemos que retiramos cerca de 100 dados duplicados.\n\n### 2.2. Dados faltantes\n\nVejamos se nossa base de dados de teste possui dados faltantes:\n\nEm seguida, iremos utilizar a fun\u00e7\u00e3o dispon\u00edvel no moodle da disciplina. A fun\u00e7\u00e3o consiste em preparar nossos textos (dados), de modo que possam ser analisados por algoritmos do ML, sendo essa etapa do pr\u00e9-processamento fundamental em aplica\u00e7\u00f5es de NLP. Sendo assim, a fun\u00e7\u00e3o consiste em realizar a tokeniza\u00e7\u00e3o das palavras dos textos: removendo eventuais pontua\u00e7\u00f5es, transformando todas as letras para min\u00fasculo, etc. ","dccfcfd9":"Bom, parece que importamos as bibliotecas e arquivos necess\u00e1rios para o projeto corretamente. Partamos ent\u00e3o para o pr\u00e9-processamento dos dados.\n\n##2. Pr\u00e9-Processamento\n\nNessa etapa iremos limpar nossa base de teste e pr\u00e9-processar os textos, para que possamos, ap\u00f3s este processo, treinar o nosso modelo.\n\nEntretanto, antes de iniciarmos o pr\u00e9-processamento do dados em si, \u00e9 interessante visualizarmos brevemente a disitribui\u00e7\u00e3o dos dados em nossa base de treio:\n\n","aef39786":"Ou seja, criando um modelo a partir dos dados de treino usando uma rede neural com uma camada escondida, otimizando o tipo de aprendizado para \"adaptativo\", o n\u00famero de neur\u00f4nios na camada escondida para 128 e o coeficiente alpha para 0.003077201812975572, obtivemos uma AUC de 0.8888334541909316. Realizando o mesmo processo de otimiza\u00e7\u00e3o e a mesma base de treino, iremos obter a AUC para um modelo usando uma rede neural com duas camadas esconidadas:\n\n### 3.4. Rede neural com duas camadas ocultas\n\nUtilizando tamb\u00e9m o MLPClassifier da biblioteca scikit-learn, iremos criar uma rede neural com 2 camadas escondidas. Os par\u00e2metros que ser\u00e3o otimizados pelo RandomizedSearchCV ser\u00e3o neste caso, a quantidade de neur\u00f4nios na *hidden layer*, o tipo da taxa de aprendizado do modelo e o cofiente de regulariza\u00e7\u00e3o.","4241b9ed":"Vejamos como ficam as dimens\u00f5es de nossa base de treinos ap\u00f3s a retirada dos dados duplicados:","92da9d3d":"Otimizando os hiperpar\u00e2metros do modelo:","7d683c96":"Visualizamos como ficaram nossos dados vetorizados:","073a8aa5":"### 3.1. kNN\n\nO primeiro modelo ser\u00e1 construido usando de um algoritmo com complexidade menro em rela\u00e7\u00e3o aos posteriores, que \u00e9 o k-nearest neighbors, um dos mais simples algoritmos de ML.","53c65a8a":"Ou seja, o Doc2Vec transformou nossos vetores de strings em vetores num\u00e9ricos, em que cada vetor possui 50 colunas.","84b8cb48":"Vejamos o resultado da aplica\u00e7\u00e3o da fun\u00e7\u00e3o:","1ac71b91":"Concluimos, ap\u00f3s essa an\u00e1lise comparativa que, de fato, o melhor modelo para a classifica\u00e7\u00e3o nesse problema \u00e9 a rede neural de duas camadas constru\u00edda a partir do PyTorch. Dessa forma, ser\u00e1 esse classificador que utilizaremos para classificar os dados n\u00e3o rotulados que submeteremos no Kaggle.","a9b63b30":"### 1.2. Importa\u00e7\u00e3o dos dados de treino\n\nImportando a base de dados atrav\u00e9s do pandas:","a72835cc":"Vejamos como \u00e9 nossa base de teste:","c2654d88":"E a dimens\u00e3o da base de treino:","1aceec51":"Realizaremos aqui as mesmas transforma\u00e7\u00f5es realizadas na base de treino \u00e0 base de teste de tamb\u00e9m de submiss\u00e3o (sem r\u00f3tulos).\n\nImportando os dados:","a25cdfa1":"A nossa base de treino ser\u00e1 composta por duas colunas: a da esquerda, em que cada linha corresponde a um *review* do filme; a da direita, em que cada linha corresponde ao r\u00f3tulo associado \u00e0quela review (se o valor for 1, a review \u00e9 positiva, se o valor for 0, a review \u00e9 negativa).\n\nVejamos ent\u00e3o nossa base de treino:","dcb3f671":"### 2.6. Pr\u00e9-processamento da base de teste e de submiss\u00e3o\n\n","c58e240f":"### 2.5. Embedding\n\nUtilizaremos o modelo Doc2Vec para realizar o *word embedding*, que consiste na transforma\u00e7\u00e3o dos vetores de strings, que obtivemos na etapa anterior por meio de transfomra\u00e7\u00f5es dos dados da nossa base de treino, em vetores num\u00e9ricos, sendo assim poss\u00edvel a utiliza\u00e7\u00e3o de algoritmos de ML e DeepLearning aos nossos dados de treino. Utilizemos ent\u00e3o o Doc2Vec, carregando o modelo pr\u00e9-treinado disponibilizado:","bc67a398":"Os textos de ambas as bases de dados est\u00e3o realmente no mesmo formato, com cada texto sendo representado por um vetor num\u00e9rico de 50 colunas.\n\nEstando os dados das tr\u00eas bases devidamente processados, estamos prontos para treinarmos diferntes modelo usando diferentes algoritmos, para ent\u00e3o escolhermos o modelo com melhor AUC.\n\n## 3. Cria\u00e7\u00e3o dos modelos\n\nIremos, nesta etapa, treinar diferntes modelos com o resultado do Doc2Vec da nossa base de treinos. Em seguida iremos testar nosso modelo com a base de teste, usando o RandomizedSearchCV do scikit-learn, para otimizar os hiperpar\u00e2metros do modelo, a fim de se produzir um modelo com a melhor AUC poss\u00edvel.\n\n","ce324f93":"Com o *dataset* de submiss\u00e3o pronto, salvemos o nosso arquivo para podermos enviar no Kaggle:","461bbeee":"Vejamos como ficaram os textos de teste e de submiss\u00e3o:","e70c4584":"Vamos, ent\u00e3o, utilizar o word2vet, o formato de vetoriza\u00e7\u00e3o escolhido.","faa4c61f":"Iremos tamb\u00e9m importar um pacote para nos auxiliar na integra\u00e7\u00e3o do modelo em PyTorch que criaremos posteriormente com o scikit-learn:","e974f7b5":"# PMR3508 - Aprendizado de M\u00e1quina e Reconhecimento de Padr\u00f5es.\n \nAn\u00e1lise de sentimentos em avalia\u00e7\u00f5es de filmes atrav\u00e9s do uso de redes neurais.\n\nAutor: Paulino Fonseca Veloso Junior\n\n","f0379f56":"## 1 Inicializa\u00e7\u00e3o\n\n### 1.1. Importa\u00e7\u00e3o das bibliotecas\n\nNessa primeira etapa do projeto, iremos importar bibliotecas importantes para o tratamento da nossa base de dados. Entretanto, antes de importar as bibliotecas, precisamos instalar alguns pacotes em nosso ambiente de desenvolvimento. Primeiramente, iremos importar pacotes espec\u00edficos voltados ao processamento de linguagem natural (NLP). O pacote ftfy ser\u00e1 usada para corrigir os textos, enquanto que o gensim ser\u00e1 usada para utiliza\u00e7\u00e3o do modelo Doc2Vec de representa\u00e7\u00e3o vetorial dos textos:","e5c74d80":"### 4.2. Regress\u00e3o log\u00edstica","cec99830":"Instaciando a rede:","a94ad561":"Vejamos como \u00e9 nossa base de submiss\u00e3o:","60c1f84e":"### 3.5. Rede neural com o PyTorch\n\nAp\u00f3s a cria\u00e7\u00e3o de duas redes neurais usando do MLPClassifier da biblioteca scikit-learn, iremos utilizar um diferente recurso para a cria\u00e7\u00e3o da rede neural deste modelo. Utilizaremos, juntamente com o TensorFlow, o PyTorch, que \u00e9 um framework de Deep Learning extremamente \u00fatil e vers\u00e1til.\n\nIremos criar uma rede neural de duas camadas escondidas no PyTorch, com o n\u00famero de neur\u00f4nios em cada camada usados sendo aqueles que obtivemos na otimiza\u00e7\u00e3o da rede anterior. Ent\u00e3o, teremos: ","47712481":"Otimizando os hiperpar\u00e2metros do modelo:","e11267b7":"Montando o *dataset* de submiss\u00e3o:","5af6b875":"Vejamos como ficaram as dimens\u00f5es de train_X:","e268badd":"### 4.5. Rede neural com o PyTorch","18013d18":"Os textos em quest\u00e3o aparecem no mesmo formato que os da base de treino! Vejamos suas dimens\u00f5es:","d1075093":"Podemos ver que a pontu\u00e7\u00e3o foi retirada dos textos, todas as letras foram convertidas para min\u00fasculo, entre outras altera\u00e7\u00f5es. \n\n### 2.4. Vetoriza\u00e7\u00e3o\n\nDevemos, ent\u00e3o, transformar os nossos textos de strings para vetores, uma vez que, durante o word embedding, atrav\u00e9s do Doc2Vec, iremos transformar vetores de palavras em vetores num\u00e9ricos.","354fdc71":"Otimizando os hiperpar\u00e2metros do modelo:","a3b4d3f1":"### 4.3. Rede neural com uma camada oculta","236a7049":"Ap\u00f3s esse processo, podemos otimizar a rede em skorch por meio de ferramentas do scikit-learn. Neste caso, utilizaremos o BayesSearchCV para esse processo:","d822b725":"Com a mesma otimiza\u00e7\u00e3o de par\u00e2metros que a rede anterior (mudando apenas o fato que agora temos dois hiperpar\u00e2metros para o n\u00famero de neur\u00f4nios, uma para cada camada escondida), obtivemos uma AUC de 0.8905417989867219, melhor que a obtida com a rede neural de apenas uma *hidden layer*, o que era esperado.\n","ec41a7bb":"Utilizando a fun\u00e7\u00e3o definida pelo Felipe Maia Polo no github: ","265842a0":"N\u00e3o temos dados faltantes em nosso dataset de treino, portanto n\u00e3o precisamos nos preocupar com isso tamb\u00e9m.","8f32802f":"Otimizando os hiperpar\u00e2metros do modelo:","6d4fce35":"Ou seja, criando um modelo a partir dos dados de treino usando o algoritmo de regress\u00e3o log\u00edstica, otimizando os valores de *C* e do tipo de regulariza\u00e7\u00e3o, obtivemos uma AUC de 0.8826236103075686. Realizando o mesmo processo de otimiza\u00e7\u00e3o e a mesma base de treino, iremos obter a AUC para modelos usando redes neurais:\n\n### 3.3. Rede neural com uma camada oculta\n\nPor meio do MLPClassifier da biblioteca scikit-learn, iremos criar umarede neural com apenas 1 camada escondida. Os par\u00e2metros a serem otimizados pelo RandomizedSearchCV ser\u00e3o os mesmos que os da rede anterior.\n","1da4b338":"### 4.4. Rede neural com duas camadas ocultas","168fb3cc":"Por fim, iremos tamb\u00e9m instalar o pacote skorch, para podermos utilizar o PyTorch em conjunto com o scikit-learn. Ademais, iremos atualizar a vers\u00e3o que iremos usar do scikit-learn, para que fique compat\u00edvel com a biblioteca skorch:","4c3ee12e":"Aplicando essa fun\u00e7\u00e3o de embedding, definida pelo Felipe Maia Polo, aos textos vetorizados da nossa base de treino:","92ebfc96":"Como pode ser visto, temos praticamente o mesmo n\u00famero de reviews rotuladas como positivas e negativas. Desse modo, n\u00e3o precisaremos nos preocupar com dados desbalanceados.\n\n### 2.1. Limpeza dos dados\n\nAssim, comecemos o pr\u00e9-processamento: limpemos os dados duplicados de nossa base de treinos","badec6d1":"Ent\u00e3o, tendo instalado os pacotes necess\u00e1rios, iremos importar as bibliotecas cujos recursos ser\u00e3o usados ao longo do projeto:","c69596b4":"Otimizando os hiperpar\u00e2metros do modelo:","3068ddc9":"### 2.3. Limpeza dos textos\n\nEm seguida, iremos utilizar a fun\u00e7\u00e3o dispon\u00edvel no moodle da disciplina. A fun\u00e7\u00e3o consiste em preparar nossos textos (dados), de modo que possam ser analisados por algoritmos do ML, sendo essa etapa do pr\u00e9-processamento fundamental em aplica\u00e7\u00f5es de NLP. Sendo assim, a fun\u00e7\u00e3o consiste em realizar a tokeniza\u00e7\u00e3o das palavras dos textos: removendo eventuais pontua\u00e7\u00f5es, transformando todas as letras para min\u00fasculo, etc.","767c15f8":"Agora, aplicando a fun\u00e7\u00e3o de prepara\u00e7\u00e3o dos textos:","4bff7df2":"Fazendo a limpeza dos textos, vetoriza\u00e7\u00e3o e embedding:","d57c0101":"Ent\u00e3o, para nosso primeiro modelo, obtivemos que o hiperpar\u00e2metro que optimiza a AUC (dentro o nosso universo de procura, que foi de 1 a 50 vizinhos) \u00e9 o 49 vizinhos. O modelo com esse hiperpar\u00e2metro produz uma AUC de 0.8558126666933973."}}