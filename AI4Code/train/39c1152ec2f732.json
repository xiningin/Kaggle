{"cell_type":{"148fc60a":"code","9df36074":"code","36b4dd0f":"code","6bc7285f":"code","2eec4148":"code","6441d9c6":"code","c8878c1f":"code","76c93d28":"code","8e957f6b":"code","adb93fca":"code","fd948699":"code","c197fc4c":"code","ebec31b3":"code","e28a2354":"code","2ece2bb2":"code","672d9657":"code","f9019cf2":"code","41c3c491":"code","78f47bb5":"code","ff2cecc5":"code","6336cf30":"code","895a5dfa":"code","8230955e":"code","6a39ec20":"code","f11cde1c":"code","dec2a677":"code","56fcc060":"code","8080f22f":"code","a7da198e":"code","d13f71e2":"code","6880830c":"code","759e819b":"code","db0a48f4":"code","11c1851d":"code","b9b8c96b":"markdown","8573934c":"markdown","0eeb5294":"markdown","27df635d":"markdown","c36a657d":"markdown","088182de":"markdown","e768fd29":"markdown","bd6abb44":"markdown","d137bc41":"markdown","f727cd9e":"markdown","f844e66e":"markdown","1ef48de2":"markdown","06eff187":"markdown","01d035d1":"markdown","389b5a97":"markdown","944d7f13":"markdown"},"source":{"148fc60a":"import pandas as pd\nimport numpy as np \nimport graphviz\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.preprocessing import StandardScaler\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","9df36074":"# Download data and preparing to prediction\ntraindf = pd.read_csv('..\/input\/titanic\/train.csv').set_index('PassengerId')\ntestdf = pd.read_csv('..\/input\/titanic\/test.csv').set_index('PassengerId')\ndf = pd.concat([traindf, testdf], axis=0, sort=False)","36b4dd0f":"df.isna().sum()","6bc7285f":"newtitles={\n    \"Capt\":       \"Officer\",\n    \"Col\":        \"Officer\",\n    \"Major\":      \"Officer\",\n    \"Jonkheer\":   \"Royalty\",\n    \"Don\":        \"Royalty\",\n    \"Sir\" :       \"Royalty\",\n    \"Dr\":         \"Officer\",\n    \"Rev\":        \"Officer\",\n    \"the Countess\":\"Royalty\",\n    \"Dona\":       \"Royalty\",\n    \"Mme\":        \"Mrs\",\n    \"Mlle\":       \"Miss\",\n    \"Ms\":         \"Mrs\",\n    \"Mr\" :        \"Mr\",\n    \"Mrs\" :       \"Mrs\",\n    \"Miss\" :      \"Miss\",\n    \"Master\" :    \"Master\",\n    \"Lady\" :      \"Royalty\"}\n\n# Extract title from name column\ndf['Title']=df.Name.apply(lambda x: x.split('.')[0].split(',')[1].strip())\n# Mapping new title\ndf['Title']=df.Title.map(newtitles)\ndf['Title'].unique()","2eec4148":"pd.crosstab(df['Title'],df['Survived']).T.style.background_gradient(cmap='summer_r')","6441d9c6":"# Created a new column : \"Gender\", to seperate male, femail and kid\n# Note : If create \"Gender\" column after \"Age\", it can only get LB=0.82057, because \"Age\" will fill missing value.\n\ndef newgender (cols):\n    Sex=cols[0]\n    Age=cols[1]\n    if Age < 15:\n        return 'kid'\n    else:\n        return Sex\n    \ndf['Gender'] = df[['Sex','Age']].apply(newgender, axis=1)","c8878c1f":"df.groupby(['Title','Sex']).Age.mean()","76c93d28":"def newage (cols):\n    title=cols[0]\n    Sex=cols[1]\n    Age=cols[2]\n    if pd.isnull(Age):\n        if title=='Master' and Sex==\"male\":\n            return 5.48\n        elif title=='Miss' and Sex=='female':\n            return 21.79\n        elif title=='Mr' and Sex=='male': \n            return 32.25\n        elif title=='Mrs' and Sex=='female':\n            return 36.86\n        elif title=='Officer' and Sex=='female':\n            return 49\n        elif title=='Officer' and Sex=='male':\n            return 46.12\n        elif title=='Royalty' and Sex=='female':\n            return 40\n        else:\n            return 42.33\n    else:\n        return Age\n    \ndf['Age']=df[['Title','Sex','Age']].apply(newage, axis=1)","8e957f6b":"df['IsWomanOrBoy'] = ((df.Age < 15) | (df.Sex == 'female'))\ndf['LastName'] = df.Name.str.split(',').str[0]","adb93fca":"# Calculate each family's female and kids survival rate by family's last name\nfamily = df.groupby(df.LastName).Survived\ndf['F_TotalCount'] = family.transform(lambda s: s.count())\ndf['F_WomanOrBoyCount'] = family.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).count())\ndf['F_WomanOrBoyCount'] = df.mask(df.IsWomanOrBoy, df.F_WomanOrBoyCount - 1, axis=0)\ndf['F_FamilySurvivedCount'] = family.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).sum())\ndf['F_FamilySurvivedCount'] = df.mask(df.IsWomanOrBoy, df.F_FamilySurvivedCount - df.Survived.fillna(0), axis=0)\ndf['F_WomanOrBoySurvived'] = df.F_FamilySurvivedCount \/ df.F_WomanOrBoyCount.replace(0, np.nan)\ndf['F_WomanOrBoySurvived'] = df.F_WomanOrBoySurvived.fillna(0)\ndf['F_Alone'] = (df.F_WomanOrBoyCount == 0)","fd948699":"# Calculate each family's female and kids survival rate by ticket number\nticket = df.groupby(df.Ticket).Survived\ndf['T_TotalCount'] = ticket.transform(lambda s: s.count())\ndf['T_WomanOrBoyCount'] = ticket.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).count())\ndf['T_WomanOrBoyCount'] = df.mask(df.IsWomanOrBoy, df.T_WomanOrBoyCount - 1, axis=0)\ndf['T_FamilySurvivedCount'] = ticket.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).sum())\ndf['T_FamilySurvivedCount'] = df.mask(df.IsWomanOrBoy, df.T_FamilySurvivedCount - df.Survived.fillna(0), axis=0)\ndf['T_WomanOrBoySurvived'] = df.T_FamilySurvivedCount \/ df.T_WomanOrBoyCount.replace(0, np.nan)\ndf['T_WomanOrBoySurvived'] = df.T_WomanOrBoySurvived.fillna(0)\ndf['T_Alone'] = (df.T_WomanOrBoyCount == 0)","c197fc4c":"df['WomanOrBoySurvived'] = df[['F_WomanOrBoySurvived','T_WomanOrBoySurvived']].max(axis=1)\ndf['Alone'] = ((df.F_Alone) & (df.T_Alone))","ebec31b3":"med_fare = df.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\n# Filling the missing value in Fare with the median Fare of 3rd class alone passenger\ndf['Fare'] = df['Fare'].fillna(med_fare)","e28a2354":"# Calculate fare for single person and assume fare free for age under 1\nfare = df.groupby(df.Ticket).Fare\ndf['FareOne'] = fare.transform(lambda x: x.mean() \/ x[df.Age > 1].count())\ndf.sort_values(by = 'FareOne', ascending = False)['FareOne'].head(5)","2ece2bb2":"df[df['LastName'] == \"Sandstrom\"]","672d9657":"auxfare = pd.cut(df['FareOne'],5)\nfig, axs = plt.subplots(figsize=(15, 5))\nsns.countplot(x=auxfare, hue='Survived', data=df).set_title(\"Fare One Bins\",fontdict= { 'fontsize': 20, 'fontweight':'bold'});\nsns.despine()","f9019cf2":"auxage = pd.cut(df['Age'], 4)\nfig, axs = plt.subplots(figsize=(15, 5))\nsns.countplot(x=auxage, hue='Survived', data=df).set_title(\"Age Bins\",fontdict= { 'fontsize': 20, 'fontweight':'bold'});\nsns.despine()","41c3c491":"df['Gender_Code'] = df.Gender.replace({'male': 0, 'female': 1, 'kid': 2})\ndf['FareOne_Code'] = LabelEncoder().fit_transform(auxfare) \ndf['Age_Code'] = LabelEncoder().fit_transform(auxage)","78f47bb5":"df.columns","ff2cecc5":"df2 = df[['Survived','WomanOrBoySurvived','Alone','Gender_Code','FareOne_Code','Age_Code']]\n\ntrain, test = df2.loc[traindf.index], df2.loc[testdf.index]\n\ntrain_x = train.drop(['Survived'], axis = 1)\ntrain_y = train[\"Survived\"]\ntrain_names = train_x.columns\n\ntest_x = test.drop(['Survived'], axis = 1)\ntrain_x.sort_values(by = 'WomanOrBoySurvived', ascending = True).head(10)","6336cf30":"# Tuning the DecisionTreeClassifier by the GridSearchCV\nparameters = {'max_depth' : np.arange(2, 9, dtype=int),\n              'min_samples_leaf' :  np.arange(1, 3, dtype=int)}\nclassifier = DecisionTreeClassifier(random_state=1000)\nmodel = GridSearchCV(estimator=classifier, param_grid=parameters, scoring='accuracy', cv=10, n_jobs=-1)\nmodel.fit(train_x, train_y)\nbest_parameters = model.best_params_\nprint(best_parameters)","895a5dfa":"model=DecisionTreeClassifier(max_depth = best_parameters['max_depth'], \n                             min_samples_leaf = best_parameters['min_samples_leaf'], \n                             random_state = 1118)\nmodel.fit(train_x, train_y)","8230955e":"# plot tree\nimport os\nos.environ[\"PATH\"] += os.pathsep + 'C:\/Program Files (x86)\/Graphviz2.38\/bin\/'\ndot_data = export_graphviz(model, out_file=None, feature_names=train_names, class_names=['0', '1'], \n                           filled=True, rounded=False,special_characters=True) \ngraph = graphviz.Source(dot_data)\ngraph ","6a39ec20":"# Prediction by the DecisionTreeClassifier\ny_pred = model.predict(test_x).astype(int)\nprint(\"Accuracy of the model: \",round(model.score(train_x, train_y) * 100, 2))","f11cde1c":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nimportances = pd.DataFrame(model.feature_importances_, index = train_names)\nimportances.sort_values(by = 0, inplace=True, ascending = False)\nimportances = importances.iloc[0:6,:] \n\nplt.figure(figsize=(8, 5)) \nsns.barplot(x=0, y=importances.index, data=importances,palette=\"deep\").set_title(\"Feature Importances\",\n                                                                                 fontdict= { 'fontsize': 20,\n                                                                                            'fontweight':'bold'});\nsns.despine()","dec2a677":"df5 = pd.get_dummies(df, columns=['Gender_Code','FareOne_Code'])\ndf5 = df5[['WomanOrBoySurvived', 'Alone', 'Age_Code', \\\n           'Gender_Code_0', 'Gender_Code_1', 'Gender_Code_2', \\\n#            'FareOne_Code_0', 'FareOne_Code_1', 'FareOne_Code_2', 'FareOne_Code_3', 'FareOne_Code_4', \\\n           'FareOne_Code_1', 'FareOne_Code_2', 'FareOne_Code_3', 'FareOne_Code_4', \\\n           'Survived']]\ndf5.rename(columns = {'Age_Code':'Age'}, inplace = True)\ndf5.rename(columns = {'Gender_Code_0':'Gender_0','Gender_Code_1':'Gender_1', 'Gender_Code_2':'Gender_2'}, inplace = True)\ndf5.rename(columns = {'FareOne_Code_1':'Fare_1', 'FareOne_Code_2':'Fare_2'}, inplace = True)\ndf5.rename(columns = {'FareOne_Code_3':'Fare_3', 'FareOne_Code_4':'Fare_4'}, inplace = True)\n# df5.rename(columns = {'FareOne_Code_0':'Fare_0'}, inplace = True)\ndf5.columns","56fcc060":"train, test = df5.loc[traindf.index], df5.loc[testdf.index]\n\ntrain_x = train.drop(['Survived'], axis = 1)\ntrain_y = train[\"Survived\"]\ntrain_names = train_x.columns\n\ntest_x = test.drop(['Survived'], axis = 1)\ntrain.head(5)","8080f22f":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ntrain_x = scaler.fit_transform(train_x)\ntest_x = scaler.transform(test_x)","a7da198e":"#Common Model Algorithms\nfrom sklearn import tree, ensemble\n\n#why choose one model, when you can pick them all with voting classifier\n#http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.VotingClassifier.html\n#removed models w\/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\nvote_est = [\n    #Ensemble Methods: http:\/\/scikit-learn.org\/stable\/modules\/ensemble.html\n    ('etc',ensemble.ExtraTreesClassifier()),\n    ('rfc', ensemble.RandomForestClassifier()),\n\n    #Decision Tree: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html\n    ('dtc', tree.DecisionTreeClassifier())\n]","d13f71e2":"from sklearn import model_selection\n\n#split dataset in cross-validation with this splitter class: \n#http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n#note: this is an alternative to train_test_split\n# run model 10x with 60\/30 split intentionally leaving out 10%\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) \n\n#Hard Vote or majority rules\nvote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\nvote_hard_cv = model_selection.cross_validate(vote_hard, train_x, train_y, cv=cv_split, return_train_score=True)\nvote_hard.fit(train_x, train_y)\n\nprint(\"Hard Voting Training w\/bin score mean: {:.2f}\". format(vote_hard_cv['train_score'].mean()*100)) \nprint(\"Hard Voting Test w\/bin score mean: {:.2f}\". format(vote_hard_cv['test_score'].mean()*100))\nprint(\"Hard Voting Test w\/bin score 3*std: +\/- {:.2f}\". format(vote_hard_cv['test_score'].std()*100*3))\nprint('-'*10)\n\n#Soft Vote or weighted probabilities\nvote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\nvote_soft_cv = model_selection.cross_validate(vote_soft, train_x, train_y, cv=cv_split, return_train_score=True)\nvote_soft.fit(train_x, train_y)\n\nprint(\"Soft Voting Training w\/bin score mean: {:.2f}\". format(vote_soft_cv['train_score'].mean()*100)) \nprint(\"Soft Voting Test w\/bin score mean: {:.2f}\". format(vote_soft_cv['test_score'].mean()*100))\nprint(\"Soft Voting Test w\/bin score 3*std: +\/- {:.2f}\". format(vote_soft_cv['test_score'].std()*100*3))\nprint('-'*10)","6880830c":"import time\n\n#WARNING: Running is very computational intensive and time expensive.\n#Code is written for experimental\/developmental purposes and not production ready!\n\n#Hyperparameter Tune with GridSearchCV: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html\ngrid_seed = [42]\n\ngrid_param = [      \n    [{\n        #ExtraTreesClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n        'n_estimators': [10], #default=10\n        'criterion': ['gini'], #default=\u201dgini\u201d\n        'max_depth': [9], #default=None\n        'random_state': grid_seed\n    }],\n\n    [{\n        #RandomForestClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n        'n_estimators': [100], #default=10\n        'criterion': ['gini'], #default=\u201dgini\u201d\n        'max_depth': [12], #default=None\n        'random_state': grid_seed\n    }],\n    \n    [{\n        #DecisionTreeClassifier: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html\n        'max_depth': [4], #default=None\n        'min_samples_leaf': [1],\n        'random_state': grid_seed\n    }] \n]\n\nstart_total = time.perf_counter() #https:\/\/docs.python.org\/3\/library\/time.html#time.perf_counter\nfor clf, param in zip (vote_est, grid_param): #https:\/\/docs.python.org\/3\/library\/functions.html#zip\n    \n    start = time.perf_counter()        \n    best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc')\n    best_search.fit(train_x, train_y)\n    run = time.perf_counter() - start\n\n    best_param = best_search.best_params_\n    print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run))\n    clf[1].set_params(**best_param) \n\nrun_total = time.perf_counter() - start_total\nprint('Total optimization time was {:.2f} minutes.'.format(run_total\/60))\n\nprint('-'*10)","759e819b":"#Hard Vote or majority rules w\/Tuned Hyperparameters\ngrid_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\ngrid_hard_cv = model_selection.cross_validate(grid_hard, train_x, train_y, cv  = cv_split, return_train_score=True)\ngrid_hard.fit(train_x, train_y)\n\nprint(\"Hard Voting w\/Tuned Hyperparameters Training w\/bin score mean: {:.2f}\". format(grid_hard_cv['train_score'].mean()*100)) \nprint(\"Hard Voting w\/Tuned Hyperparameters Test w\/bin score mean: {:.2f}\". format(grid_hard_cv['test_score'].mean()*100))\nprint(\"Hard Voting w\/Tuned Hyperparameters Test w\/bin score 3*std: +\/- {:.2f}\". format(grid_hard_cv['test_score'].std()*100*3))\nprint('-'*10)\n\n#Soft Vote or weighted probabilities w\/Tuned Hyperparameters\ngrid_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\ngrid_soft_cv = model_selection.cross_validate(grid_soft, train_x, train_y, cv  = cv_split, return_train_score=True)\ngrid_soft.fit(train_x, train_y)\n\nprint(\"Soft Voting w\/Tuned Hyperparameters Training w\/bin score mean: {:.2f}\". format(grid_soft_cv['train_score'].mean()*100)) \nprint(\"Soft Voting w\/Tuned Hyperparameters Test w\/bin score mean: {:.2f}\". format(grid_soft_cv['test_score'].mean()*100))\nprint(\"Soft Voting w\/Tuned Hyperparameters Test w\/bin score 3*std: +\/- {:.2f}\". format(grid_soft_cv['test_score'].std()*100*3))\nprint('-'*10)","db0a48f4":"# Prediction by the VotingClassifier\ny_pred = grid_hard.predict(test_x).astype(int)\nprint(\"Accuracy of the model: \",round(grid_hard.score(train_x, train_y) * 100, 2))","11c1851d":"# Saving the result\npd.DataFrame({'Survived': y_pred}, index=testdf.index).reset_index().to_csv('submission_SimonWCC.csv', index=False)","b9b8c96b":"## 2.8 Fare","8573934c":"# 4. Voting Classifier\nThis voting model is original from : LD Freeman \nhttps:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy. ","0eeb5294":"## 2.2 Gender ","27df635d":"## 2.4 IsWomanOrBoy & LastName","c36a657d":"# 1. Data Preparation","088182de":"## 2.7 WomanOrBoySurvived & Alone","e768fd29":"## 2.6 WomanOrBoySurvived By Ticket","bd6abb44":"## 2.10 Encoding : Gender, FareOne, Age","d137bc41":"## 2.3 Age","f727cd9e":"# Titanic - Machine Learning from Disaster\n## LB=0.82535 (Top 3%) \n\nThis model is original from : Vitalii Mokin \nhttps:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models. \n\nBasen on original model and put some imporvment what I discovered to get LB=0.82296\n\nUpdate\n2021.07.01 : Using voting model from : LD Freeman \nhttps:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy. \nTo get LB=0.82535","f844e66e":"# 3. Prepare for modeling","1ef48de2":"### Each of these three model(ExtraTreesClassifier, RandomForestClassifier, DecisionTreeClassifier) can get LB=0.82296","06eff187":"## 2.1 Title","01d035d1":"## 2.9 FareOne","389b5a97":"# 2. Features Engineering (FE)","944d7f13":"## 2.5 WomanOrBoySurvived By Last Name"}}