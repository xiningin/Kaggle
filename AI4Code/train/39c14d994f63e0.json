{"cell_type":{"7e5830de":"code","b1c9ef71":"code","b82ef9e4":"code","b0899258":"code","8763056e":"code","4c71bc11":"code","015e1987":"code","206c15aa":"code","141fb7cb":"code","0d9c3056":"code","0dc921f3":"code","7ea7d260":"code","72e1d1f4":"code","3af60652":"code","e1326c14":"code","1597523c":"code","53646906":"code","63cdb48d":"code","4cc18cb0":"code","a5877741":"code","1e0e9d1c":"code","d20b252a":"code","f680349f":"code","d2f855bf":"code","8aa1b729":"code","09883563":"code","5164f9a7":"code","328f5a80":"code","20f8f392":"code","9c41a0c5":"code","357ff596":"code","60909444":"code","5874ad54":"code","1f1e25ed":"code","5d88b6e9":"code","37b15163":"code","e1b2e353":"code","b781238b":"code","78996b06":"code","45cfb339":"code","a89d17b0":"code","5d89d938":"code","b36a284b":"code","320f3187":"code","199a0281":"code","52c07126":"code","7fcf1c70":"code","e798de61":"code","49fea20f":"code","f1b97759":"code","ccde0bed":"code","96cb8a87":"markdown","6e6e1c8d":"markdown","9afab953":"markdown","878717a5":"markdown","82e91c93":"markdown","bf3d3628":"markdown","ac005a98":"markdown","20afb536":"markdown","62952752":"markdown","cfa685fb":"markdown","d1733f99":"markdown","de1b14ea":"markdown","e46ce0cb":"markdown","390322f1":"markdown","2a27edf4":"markdown","3eb3ad6e":"markdown","8689993a":"markdown","e44bd567":"markdown","d4600aad":"markdown","d5dfbb05":"markdown","5225941d":"markdown","11e52bf9":"markdown","2aaee71a":"markdown","19e57f01":"markdown","423c34b2":"markdown","cdb6da63":"markdown","a4055959":"markdown","d018a334":"markdown","8a7d067a":"markdown","c848051e":"markdown","0f6741c1":"markdown","216015c7":"markdown","d0f59787":"markdown","23bd7385":"markdown","c9ef3766":"markdown","fc2faca9":"markdown","293d3c02":"markdown","a8092f3e":"markdown","7027ccf5":"markdown","c23f123b":"markdown","8b480041":"markdown","235b42e3":"markdown","0c45c1b9":"markdown","e1308666":"markdown","41b8b340":"markdown","1776850e":"markdown","d786a421":"markdown","bc32c406":"markdown","0404a4ce":"markdown","a4ed128b":"markdown","23e398ba":"markdown"},"source":{"7e5830de":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b1c9ef71":"housing = pd.read_csv(\"\/kaggle\/input\/california-housing-prices\/housing.csv\")","b82ef9e4":"from sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport matplotlib\n%matplotlib inline","b0899258":"housing.info()","8763056e":"housing['ocean_proximity'].value_counts()","4c71bc11":"housing.head()","015e1987":"housing.hist(bins=50,figsize = (20,20))","206c15aa":"housing['income_cat'] = pd.cut(housing['median_income'],\n                              bins = [0.,1.5,3.0,4.5,6.,np.inf],\n                              labels = [1,2,3,4,5])\n","141fb7cb":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits = 1 , test_size = 0.2 , random_state = 42)","0d9c3056":"for train_index , test_index in split.split(housing , housing['income_cat']):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]","0dc921f3":"strat_train_set.drop(\"income_cat\",axis = 1 ,inplace = True)\nstrat_test_set.drop(\"income_cat\",axis = 1 ,inplace = True)","7ea7d260":"strat_train_set.columns","72e1d1f4":"strat_test_set.columns","3af60652":"housing_train = strat_train_set.copy()","e1326c14":"housing_train.plot(kind = \"scatter\" , x = 'longitude' , y = 'latitude')","1597523c":"housing_train.plot(kind = \"scatter\" , x = 'longitude',y = 'latitude',alpha = 0.1)","53646906":"housing_train.plot(kind = \"scatter\" , x = 'longitude',y = 'latitude',alpha = 0.4,\n                  s = housing_train['population']\/100 , label = 'population' , figsize = (10,7),\n                  c = 'median_house_value' , cmap = plt.get_cmap(\"jet\"),colorbar = True)\nplt.legend()","63cdb48d":"corr_mat = housing_train.corr()\ncorr_mat","4cc18cb0":"corr_mat['median_house_value'].sort_values(ascending = False)","a5877741":"from pandas.plotting import scatter_matrix\n\nattr = ['median_house_value' , 'median_income' , 'total_rooms','housing_median_age']\nscatter_matrix(housing_train[attr],figsize=(15,10))","1e0e9d1c":"housing_train['rooms_per_household'] = housing_train['total_rooms'] \/ housing_train['households']\nhousing_train['bedrooms_per_household'] = housing_train['total_bedrooms'] \/ housing_train['total_rooms']\nhousing_train['population_per_household'] = housing_train['population'] \/ housing_train['households']","d20b252a":"corr_mat = housing_train.corr()","f680349f":"corr_mat['median_house_value'].sort_values(ascending = False)","d2f855bf":"housing = strat_train_set.drop(\"median_house_value\", axis=1)\nhousing_labels = strat_train_set[\"median_house_value\"].copy()","8aa1b729":"housing_num = housing.drop('ocean_proximity',axis = 1)","09883563":"from sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy = 'median')\n\nX = imputer.fit_transform(housing_num)","5164f9a7":"imputer.statistics_","328f5a80":"X","20f8f392":"housing_tr = pd.DataFrame(X, columns=housing_num.columns,index=housing_num.index)\nhousing_tr","9c41a0c5":"housing_train","357ff596":"housing_cat = pd.DataFrame(housing_train['ocean_proximity'])\nhousing_cat","60909444":"from sklearn.preprocessing import OneHotEncoder\n\nonehotencoder = OneHotEncoder()\nhousing_cat_1hot = onehotencoder.fit_transform(housing_cat)\nhousing_cat_1hot","5874ad54":"housing_cat_1hot.toarray()","1f1e25ed":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n ('imputer', SimpleImputer(strategy=\"median\")),\n ('std_scaler', StandardScaler()),\n ])\nhousing_num_tr = num_pipeline.fit_transform(housing_num)","5d88b6e9":"from sklearn.compose import ColumnTransformer\nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\nfull_pipeline = ColumnTransformer([\n (\"num\", num_pipeline, num_attribs),\n (\"cat\", OneHotEncoder(), cat_attribs),\n ])\nhousing_prepared = full_pipeline.fit_transform(housing)","37b15163":"from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)","e1b2e353":"from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","b781238b":"from sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(housing_prepared, housing_labels)","78996b06":"housing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","45cfb339":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels,\nscoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)\n\ntree_rmse_scores","a89d17b0":"tree_rmse_scores.mean()","5d89d938":"lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\nscoring=\"neg_mean_squared_error\", cv=10)\n\nlin_rmse_scores = np.sqrt(-lin_scores)\nlin_rmse_scores\n","b36a284b":"lin_rmse_scores.mean()","320f3187":"from sklearn.ensemble import RandomForestRegressor\nforest_reg = RandomForestRegressor()\nforest_reg.fit(housing_prepared, housing_labels)\n\nforest_score = cross_val_score(forest_reg,housing_prepared,housing_labels,scoring = 'neg_mean_squared_error',cv=10)\nforest_rmse_score = np.sqrt(-forest_score)\nforest_rmse_score","199a0281":"forest_rmse_score.mean()","52c07126":"from sklearn.model_selection import GridSearchCV\nparam_grid = [\n {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n ]\nforest_reg = RandomForestRegressor()\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n scoring='neg_mean_squared_error',\nreturn_train_score=True)\ngrid_search.fit(housing_prepared, housing_labels)","7fcf1c70":"grid_search.best_params_","e798de61":"grid_search.best_estimator_","49fea20f":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n     print(np.sqrt(-mean_score), params)","f1b97759":"final_model = grid_search.best_estimator_\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse) \n","ccde0bed":"final_rmse","96cb8a87":"As linear regression rmse is not very fine we will define a DecisionTreeRegressor and fit the model with training data and predict it.","6e6e1c8d":"Plotting a histogram to visualize and understand more about the data\n","9afab953":"Here we will create a Linear Regression model and fit with the training data.","878717a5":"# Creating pipeline","82e91c93":"Reading the csv file using pandas read_csv() function and storing it in a housing variable","bf3d3628":"Importing all the modules which are necessary","ac005a98":"This can be done by splitting the median_income into different bins of values and then labelling it.","20afb536":"Now finding the correlation for the dataframe with new added columns.","62952752":"Viewing the score for all combination of parameters that were used for testing the model.","cfa685fb":"Making a copy of the stratified training set to the housing_train","d1733f99":"Here the size of the circle represents the size the population in that area and the color scale represents the median_house_value in that area with blue as the lowest and the red as the highest median_house_value.","de1b14ea":"# Prediction","e46ce0cb":"head() returns the first 5 values in the dataframe.","390322f1":"Then using StratifiedShuffleSplit from sklearn.model_selection we will split the data into training and test sets based on the test_size.","2a27edf4":"# Fitting the model","3eb3ad6e":"Storing the categorical variable in the housing_cat.","8689993a":"Hurray! RandomForestRegressor models rmse value is lower than the all other models but it also not the best score.","e44bd567":"Since both the models rmse values are too high we will try it with the RandomForestRegressor.","d4600aad":"# GridSearchCV","d5dfbb05":"Converting the matrix which is returned from the imputer into the dataframe using the below code.","5225941d":"Now the coorelation between the rooms_per_household and the median_house_value is high positive correlated and the bedrooms_per_household is high negative correlated which means that the feature created using the already existing ones make more sense.","11e52bf9":"# Feature Engineering","2aaee71a":"Creating new features based on the features that are already present.","19e57f01":"The ocean proximity contains categorical values so we are viewing the data using value_counts() to know the occurence of each value in the dataset.","423c34b2":"Using simpleImputer from the sklearn.impute we will fill all the numerical missing values with the median of that value.","cdb6da63":"predicting the value for the test sets in the linear regression model and using the error function root mean squared error.","a4055959":"Since the correlation between the median_income is the important feature in predicting the house value we will ensure that while splitting the data into training and testing sets we will split them with the equal values of bins in the both the training and testing set.","d018a334":"Predicting the median_house_value using the best estimators in the gridsearch.","8a7d067a":"# Data visualisation\n","c848051e":"OneHotEncoder returns the result as a sparse matrix, we are converting that into an array using pandas toarray() method.","0f6741c1":"Using OneHotEncoder from the sklearn.preprocessing to encode all the categorical variables into numerical ones.","216015c7":"To overcome the overfitting in the training set we will use cross_validation for the decision tree regressor. ","d0f59787":"Finding the correlation between the mediana_house_value with all other features.\nmedain_income is high positively correlated with in median_house_value and the latitude is high negative correlated with the median_house_value.","23bd7385":"After splitting the data the income_cat column is dropped since there is no further use of it.","c9ef3766":"# Handling missing values","fc2faca9":"Now we will get all the numerical columns from the dataframe and store it seperately.","293d3c02":"Using the GridSearchCV for finding the best hyperparameter that gives the minimum rmse score.","a8092f3e":"Plotting the graph based on the latitude and longitude values.","7027ccf5":"In the below code we will get the index for the train and test set splitting based on the income_cat column produced before.","c23f123b":"Finding the correlation between the different features.","8b480041":"The model has the rmse value of 0. which says that it will predict with 100% accuracy,but the model overfits the data so that the model only works well on the training data and predicts worst with the unknown values. ","235b42e3":"First we will use GridSearchCV for the RandomForestRegressor with the following list of hyperparameters.","0c45c1b9":"Dropping the label value from the training set.","e1308666":"# Getting the data","41b8b340":"The final rmse score for the test set is 47572.","1776850e":"The best parameters are","d786a421":"Using the cross validation for the linear regressor.","bc32c406":"Scatter matrix is an another type to view the correlation between different columns in the graph visualization.","0404a4ce":"# Handling categorical variables","a4ed128b":"Now the rmse score is much worse than the linear regressor but its not overfitting the data.","23e398ba":"Creating a pipeline to preprocess the numerical data."}}