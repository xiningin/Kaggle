{"cell_type":{"f36dc4c7":"code","a3c47d64":"code","f132e6bc":"code","500aa013":"code","5b15e245":"markdown","a0277368":"markdown","6262f3be":"markdown","e58234aa":"markdown"},"source":{"f36dc4c7":"import xgboost as xgb\n\nmodel_xgb = xgb.XGBClassifier(max_depth=9, learning_rate=0.01, n_estimators=500, reg_alpah=1.1,\n                             colsample_bytree=0.9, subsample=0.9, n_jobs=5)\nmodel_xgb.fit(X_tr, y_tr, eval_set=[(X_vld, y_vld)], verbose=False, early_stopping_rounds=50)\npred_xgb = model_xgb.predict(X_vld)\nscore_xgb = metrics.accuracy_score(pred_xgb, y_vld)\nprint(\"XGBoost Test score: \", score_xgb)","a3c47d64":"import lightgbm as lgbm\n\nmodel_lgbm = lgbm.LGBMClassifier(max_depth=9, lambda_l1=0.1, lambda_l2=0.01, learning_rate=0.01,\n                               n_estimators=500, reg_alpha=1.1, colsample_bytree=0.9, subsample=0.9, n_jobs=5)\nmodel_lgbm.fit(X_tr, y_tr, eval_set=[(X_vld, y_vld)], verbose=False, early_stopping_rounds=50,\n              eval_metric=\"accuracy\")\npred_lgbm = model_lgbm.predict(X_vld)\nscore_lgbm = metrics.accuracy_score(pred_lgbm, y_vld)\nprint(\"LightGBM Test Score: \", score_lgbm)","f132e6bc":"import catboost as cboost\n\nmodel_cboost = cboost.CatBoostClassifier(depth=9, reg_lambda=0.1, learning_rate=0.01, iterations=500)\nmodel_cboost.fit(X_tr, y_tr, eval_set=[(X_vld, y_vld)], verbose=False, early_stopping_rounds=50)\npred_cboost = model_cboost.predict(X_vld)\nscore_cboost = metrics.accuracy_score(pred_cboost, y_vld)\nprint(\"CatBoost Test Score: \", score_cboost)\n","500aa013":"\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization, Activation\nfrom keras.optimizers import Adam\nfrom keras import backend as K\n\nmodel_mlp = Sequential()\nmodel_mlp.add(Dense(45 ,activation='linear', input_dim=13))\nmodel_mlp.add(BatchNormalization())\n\nmodel_mlp.add(Dense(9,activation='linear'))\nmodel_mlp.add(BatchNormalization())\nmodel_mlp.add(Dropout(0.4))\n\nmodel_mlp.add(Dense(5,activation='linear'))\nmodel_mlp.add(BatchNormalization())\nmodel_mlp.add(Dropout(0.2))\n\nmodel_mlp.add(Dense(1,activation='relu', ))\n\noptimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0)\nmodel_mlp.compile(optimizer=optimizer, \n            loss='binary_crossentropy', \n            metrics=['accuracy'])\n\nhist = model_mlp.fit(X_tr, y_tr, epochs=500, batch_size=30, validation_data=(X_vld,y_vld), verbose=False)\n\npred_mlp = model_mlp.predict_classes(X_vld)[:,0]\nscore_mlp = metrics.accuracy_score(pred_mlp, y_vld)\nprint(\"MLP Test Score: \", score_mlp)","5b15e245":"# XGBoost","a0277368":"# MLP (MultiLayer Perceptron)","6262f3be":"# LightGBM","e58234aa":"# CatBoost"}}