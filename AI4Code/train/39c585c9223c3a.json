{"cell_type":{"64253e94":"code","15a3d4b6":"code","20ee18fd":"code","0cab2403":"code","b51676a9":"code","46092dd4":"code","c2068bac":"markdown","064178f8":"markdown","2c6a1d1f":"markdown","5a053bc3":"markdown"},"source":{"64253e94":"import numpy as np\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom scipy.cluster import hierarchy\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nplt.style.use('ggplot')","15a3d4b6":"X,y = make_classification(100,n_features=2,n_redundant=0)","20ee18fd":"class HC:\n    def __init__(self, verbose=False, linkage_type='complete'):\n        self.verbose= verbose\n        self.linkage_type = linkage_type\n        \n    def argmin(self, D):\n        \"\"\"\n        Given a 2D array, returns the minimum value that is not in the main diagonal, i.e x==y\n        and (x,y) index of that value.\n        \"\"\"\n        minx, miny = (0,0)\n        min_val = 10e5\n        for i in range(D.shape[0]):\n            for j in range(D.shape[0]):\n                if j==i:\n                    continue\n                else:\n                    if D[i,j] < min_val:\n                        min_val = D[i,j]\n                        minx = i\n                        miny = j\n                        \n        return min_val, minx, miny\n        \n    def cluster_distance(self, cluster_members):\n        \"\"\"\n        Calculates the cluster euclidean distances. \n        \n        Params\n        ------\n        cluster_members: dict.\n            stores the cluster members in format: {key: [item1, item2 ..]}. \n            if key is less than X.shape[0] then, it only has itself in the cluster. \n        \n        Returns\n        -------\n        Distance: 2D array. \n            Contains distances between each cluster. \n        \"\"\"\n        nClusters = len(cluster_members)\n        keys = list(cluster_members.keys())\n        Distance = np.zeros((nClusters, nClusters))\n        for i in range(nClusters):\n            ith_elems = cluster_members[keys[i]]\n            for j in range(nClusters):\n                jth_elems = cluster_members[keys[j]]\n                d_in_clusters = euclidean_distances(X[ith_elems], X[jth_elems])\n                if self.linkage_type == 'complete':\n                    dij = np.max(d_in_clusters)\n                elif self.linkage_type == 'single':\n                    dij = np.min(d_in_clusters)\n                Distance[i,j] = dij\n\n        # so Distance has minimum distance between clusters \n        # row and col of Distance denotes element in `keys`\n        return Distance\n    \n    def fit(self, X):\n        \"\"\"\n        Generates the dendrogram.\n        \n        Params\n        ------\n        X: Dataset, shape (nSamples, nFeatures)\n        \n        Returns\n        -------\n        Z: 2D array. shape (nSamples-1, 4). \n            Linkage matrix. Stores the merge information at each iteration.\n        \"\"\"\n        self.nSamples = X.shape[0]\n\n        cluster_members = dict([(i,[i]) for i in range(self.nSamples)])\n        Z = np.zeros((self.nSamples-1,4)) # c1, c2, d, count\n        \n        for i in range(0, self.nSamples-1):\n            if self.verbose:\n                print(f'\\n-------\\nDebug Line at, i={i}\\n--------')\n            \n            nClusters = len(cluster_members)\n            keys = list(cluster_members.keys())\n            # caculate the distance between existing clusters\n            D = self.cluster_distance(cluster_members)\n            _, tmpx, tmpy = self.argmin(D)\n#             tmpx, tmpy = np.unravel_index(np.argmin(D), shape=D.shape)\n            if self.verbose:\n                print(f'Z:\\n{Z}, \\nCluster Members: {cluster_members}, D: \\n {D}')\n            \n            x = keys[tmpx]\n            y = keys[tmpy]\n            # update Z\n            Z[i,0] = x\n            Z[i,1] = y\n            Z[i,2] = D[tmpx, tmpy] # that's where the min value is\n            Z[i,3] = len(cluster_members[x]) + len(cluster_members[y])\n            \n            # new cluster created\n            cluster_members[i+self.nSamples] = cluster_members[x] + cluster_members[y]\n            # remove merged from clusters pool, else they'll be recalculated\n            del cluster_members[x]\n            del cluster_members[y]\n            \n        self.Z = Z\n        return self.Z\n    \n    def predict(self, n_cluster=3):\n        \"\"\"\n        Get cluster label for specific cluster size.\n        \n        Params\n        ------\n        n_cluster: int. \n            Number of clusters to keep. Can not be > nSamples\n        \n        Returns\n        -------\n        labels: list.\n            Cluster labels for each sample.\n        \"\"\"\n        labels = np.zeros((self.nSamples))\n        cluster_members = dict([(i,[i]) for i in range(self.nSamples)])\n        for i in range(self.nSamples - n_cluster):\n            x,y = (Z[i,0], Z[i,1])\n            cluster_members[self.nSamples + i] = cluster_members[x] + cluster_members[y]\n            del cluster_members[x]\n            del cluster_members[y]\n            \n        keys = list(cluster_members.keys())\n        \n        for i in range(len(keys)):\n            samples_in_cluster = cluster_members[keys[i]]\n            labels[samples_in_cluster] = i\n            \n        return labels","0cab2403":"%%time\nhc = HC(linkage_type='complete')\nZ = hc.fit(X)\nmyLabel = hc.predict(n_cluster=3)","b51676a9":"%%time\nclustering = AgglomerativeClustering(n_clusters=3,linkage='complete').fit(X)\nskLabel = clustering.labels_","46092dd4":"fig, ax = plt.subplots(2,2,facecolor='white',figsize=(15,5*2),dpi=120)\n\n# Cluster\nfor i in range(3):\n    myIndices = myLabel==i\n    skIndices = skLabel==i\n    ax[0,0].scatter(x=X[myIndices,0], y=X[myIndices,1],label=i)\n    ax[0,1].scatter(x=X[skIndices,0], y=X[skIndices,1],label=i)\n    \nax[0,0].set_title('Custom | Cluster')\nax[0,1].set_title('Sklearn | Cluster')\nax[0,0].legend()\nax[0,1].legend()\n\n# Dendrogram\nz = hierarchy.linkage(X, 'complete') # scipy agglomerative cluster\nhierarchy.dendrogram(Z, ax=ax[1,0]) # plotting mine with their function\nhierarchy.dendrogram(z, ax=ax[1,1]) # plotting their with their function\n\nax[1,0].set_title('Custom | Dendrogram')\nax[1,1].set_title('Sklearn | Dendrogram')\nplt.show()","c2068bac":"# HC Class\nI have tried to comment as much as I could, if you still need clarification, comment please. This implementation is not efficient at all. This is what I call \"Visible Implementation\". I should be able to know what a code does just by looking at it and reading comments (not documentation) if it is a visible implementation.","064178f8":"# Hierarchical Clustering\nHierarchical clustering does not need predefined cluster size, K. The resulting cluster is a tree-based representation called *dendogram* (scroll to last to see a dendrogram). There are mainly two ways to build a dendogram: 1) Agglomerative clustering, aka bottom-up, 2) Divisive Analysis, aka top-down. I will only discuss the bottom-up approach. \n\n## Agglomerative Clustering\nSuppose we have $n$ samples with $p$ features. Before clustering, we need to decide on a dissimilarity measure. It will be discussed further later, but for now let us use Euclidean Distance. So, at first each sample will be a cluster it self. On each iteration, we will merge clusters that are least dissimilar(most similar) to each other. We are using the word dissimilar because dissimilarity will denote the tree depth. And tree depth will be interpreted as how similar two samples\/clusters are. \n\n### Algorithm\n1. Take n samples, and a measure pairwise dissimilarity of all $n(n-1)\/2$ possible pairs. \n2. For 1 = n, n-1, n-2,.., 2:\n    1. Examine all pairwise inter-cluster\/inter-sample dissimilarities among the i clusters\/samples and identify the pair of clusters\/samples that are least dissimilar. Merge these clusters\/samples together. Their height in the tree is denoted by their dissimilarity.\n    2. Compute the new pairwise inter-cluster dissimilarities among the remaining clusters.\n\n\n## Dissimilarity Calculation\nWe can calculate measures like Euclidean distance between two samples, but how to calculate the distance between two clusters, or a cluster and a sample? This is solved by *linkage*. A linkage tells us how to get a single valued measure from multiple values. There are mainly 4 types available. \n\n* **Complete Linkage**. Calculates all pairwise measure for observations in clusters and returns the largest of those values. \n* **Single Linkage**. Minimum distance is recorded from all pairwise measure.\n* **Average Linkage**. Records the average of all pairwise distances.\n* **Ward Linkage**. \n\nAll of them are briefly explained [here](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage)","2c6a1d1f":"The whole implementation is only done using list, dict and for loops. While scipy's take `1.51ms`, mine took `1m 5s`. But I have learnt something while writing this. I hope you do too. Let me know if you find any error.","5a053bc3":"sanity check."}}