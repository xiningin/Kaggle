{"cell_type":{"ea8ad622":"code","4b0f1d33":"code","cfb474cf":"code","9c3f08f4":"code","e787bbda":"code","1db761de":"code","b826cd2f":"code","68ec21ac":"code","a1698bc9":"code","4f454668":"code","7130da52":"code","c1701171":"code","3a4cedae":"code","c90a126c":"code","8551d828":"code","4b7a00dd":"code","216ccd62":"code","2db79e55":"code","d4fca693":"code","00844ea5":"code","9ccd120d":"code","7b7bad45":"code","0df9ba3f":"code","a26997c5":"code","c35b99a6":"code","4bdab467":"code","4d536e9b":"code","1c6e6f95":"code","383426f2":"code","9c055a6f":"code","2fc4fd56":"code","8bcee398":"code","75cdafe3":"code","80876c6a":"code","bc0d052c":"code","247a21b2":"code","3bbda9f0":"code","7a5697f0":"code","29e36534":"code","e1f09d7c":"code","4a5593d1":"code","1e5f6952":"code","e21496da":"code","fca0aa08":"code","42de12e2":"code","3fa76b67":"code","ab7fdbcd":"code","a51877fa":"code","e2848695":"code","8e6887b8":"code","d0a89690":"code","f1a74faa":"code","43640940":"code","524092a8":"code","c413fb4f":"code","24648183":"code","d633959d":"code","c524456f":"code","79a924cc":"code","1e4e7976":"code","877664d4":"code","d72655e5":"code","3718119e":"markdown","f55a33f6":"markdown","c31d614e":"markdown","ea9f20f8":"markdown","5be19675":"markdown","1ce09fde":"markdown","3f22d9c8":"markdown","fdacb830":"markdown","1d135f53":"markdown","4c1d3c85":"markdown","f771924a":"markdown","183185a4":"markdown","2ad2e03b":"markdown","93c6ab33":"markdown","78b16009":"markdown"},"source":{"ea8ad622":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")","4b0f1d33":"df=pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf.head(3)","cfb474cf":"df.shape","9c3f08f4":"# Type of datas\ndf.info()","e787bbda":"# To see all rows;\npd.options.display.max_rows\npd.set_option(\"display.max_rows\",None)\n\n\n# Any missing values in dataset?\ndf.isnull().sum().sort_values(ascending=False)","1db761de":"# Now, let's look at the percentage of null values to decide dropping them\n\nprint(\"Percentage of PoolQC: % {:.4f}\".format((df[\"PoolQC\"].isnull().sum())\/len(df)*100))\nprint(\"Percentage of MiscFeature: % {:.4f}\".format((df[\"MiscFeature\"].isnull().sum())\/len(df)*100))\nprint(\"Percentage of Alley: % {:.4f}\".format((df[\"Alley\"].isnull().sum())\/len(df)*100))\nprint(\"Percentage of Fence: % {:.4f}\".format((df[\"Fence\"].isnull().sum())\/len(df)*100))\nprint(\"Percentage of FireplaceQu: % {:.4f}\".format((df[\"FireplaceQu\"].isnull().sum())\/len(df)*100))\nprint(\"Percentage of LotFrontage: % {:.4f}\".format((df[\"LotFrontage\"].isnull().sum())\/len(df)*100))","b826cd2f":"# Let'S visualize, how much are there missing values in this dataset?\n\nmiss=df.isnull().sum().sort_values(ascending=False).head(20)\nplt.figure(figsize=(8,5),dpi=100)\nmiss_val=pd.Series(miss, miss.index)\nmiss_val.plot(kind=\"barh\");","68ec21ac":"# We can drop first 5 columns and \"id\" column\n\ndf.drop([\"PoolQC\",\"MiscFeature\",\"Alley\",\"Fence\",\"FireplaceQu\",\"Id\"], axis=1, inplace=True)\n\ndf.head(2)","a1698bc9":"# Fill with mean() function the numerical variables and fill with mode() function the categorical variables\n\ndf[\"LotFrontage\"].fillna(df[\"LotFrontage\"].mean(), inplace=True)\ndf[\"GarageType\"].fillna(df[\"GarageType\"].mode()[0], inplace=True)\ndf[\"GarageYrBlt\"].fillna(df[\"GarageYrBlt\"].mode()[0], inplace=True)\ndf[\"GarageFinish\"].fillna(df[\"GarageFinish\"].mode()[0], inplace=True)\ndf[\"GarageCond\"].fillna(df[\"GarageCond\"].mode()[0], inplace=True)\ndf[\"GarageQual\"].fillna(df[\"GarageQual\"].mode()[0], inplace=True)\ndf[\"BsmtExposure\"].fillna(df[\"BsmtExposure\"].mode()[0], inplace=True)\ndf[\"BsmtFinType2\"].fillna(df[\"BsmtFinType2\"].mode()[0], inplace=True)\ndf[\"BsmtFinType1\"].fillna(df[\"BsmtFinType1\"].mode()[0], inplace=True)\ndf[\"BsmtCond\"].fillna(df[\"BsmtCond\"].mode()[0], inplace=True)\ndf[\"BsmtQual\"].fillna(df[\"BsmtQual\"].mode()[0], inplace=True)\ndf[\"MasVnrType\"].fillna(df[\"MasVnrType\"].mode()[0], inplace=True)\ndf[\"MasVnrArea\"].fillna(df[\"MasVnrArea\"].mode()[0], inplace=True)\ndf[\"Electrical\"].fillna(df[\"Electrical\"].mode()[0], inplace=True)\n\nprint(\"Are there any missing values in all dataset: \", (df.isnull().sum()>0).any())","4f454668":"# We should seperate categorical and continuous variable:\n\ndf_cat=df.select_dtypes(include=\"O\")\ndf_num=df.select_dtypes(exclude=\"O\")\n\ndisplay(df_cat.head(2))\ndisplay(df_num.head(2))","7130da52":"df_cat_col=df.select_dtypes(exclude=\"O\").columns\ndf_year=pd.DataFrame()\n\n# \"YrSold\" column must be higher than other columns in df_year dataframe, so we should check it:\n\nfor k in range(len(df_cat_col)):\n    if (\"Year\" in df_cat_col[k]) or (\"Yr\" in df_cat_col[k]):\n        df_year=df_year.append(df[df_cat_col[k]]) \n        \ndf_year=df_year.T        \ndisplay(df_year.head()) \n\nprint(\"YearBuilt column is higher than YrSold column\", np.where(df_year[\"YearBuilt\"]>df_year[\"YrSold\"]),\"\\n\")\nprint(\"YearRemodAdd column is higher than YrSold column\", np.where(df_year[\"YearRemodAdd\"]>df_year[\"YrSold\"]),\"\\n\")\nprint(\"GarageYrBlt column is higher than YrSold column\", np.where(df_year[\"GarageYrBlt\"]>df_year[\"YrSold\"]))\n\n# YearRemodAdd column has a wrong value, so we must change it","c1701171":"for index, row in df_year.iterrows():\n    if row[\"YearRemodAdd\"]>row[\"YrSold\"]:\n        print(index, row[\"YearRemodAdd\"], row[\"YrSold\"])\n\n# At 523. row, YearRemodAdd is higher than YrSold, now i will replace \"2008.0 \"","3a4cedae":"df_num[\"YearRemodAdd\"][523]=2007\n# We changed the specific row with 2007","c90a126c":"df_num.hist(layout=(10,8), figsize=(28,25))\n\nplt.show()\n\n# Like we see in the graphs, most of numerical variables is not the normal distributions, to predict target value, we should normalize them","8551d828":"# OUTLIERS:\n\ndf_num_col = list(df_num.iloc[:,:36].columns)\n\nplt.figure(figsize=(28,25),dpi=100)\nfor i in range(len(df_num_col)):\n    plt.subplot(6,6,i+1)\n    plt.title(df_num_col[i])\n    plt.boxplot(df_num[df_num_col[i]]);","4b7a00dd":"from scipy import stats\n\n# First, look at \"LotFrontage\" column to understand zscore\n\nz_lot=stats.zscore(df_num.LotFrontage)\n\nfor i in range(1,7):\n    print(\"Threshold value: \", i)\n    print(\"Number of outliers:\", len(np.where(z_lot>i)[0]),\"\\n\")\n\nprint(len(np.where(z_lot>3)[0]))","216ccd62":"# If it is applied log transformation to \"LotFrontage\" column, number of outliers:\n\ndf_num[\"LotFrontage_log\"] = np.log(df_num[\"LotFrontage\"])\nzscore_log=stats.zscore(df_num[\"LotFrontage_log\"])\nprint(len(np.where(zscore_log>2.5)[0]))\nplt.boxplot(zscore_log, whis=2.5);\n\n# Then drop it\ndf_num.drop([\"LotFrontage_log\"], axis=1, inplace=True)","2db79e55":"# I have chosen in below \"outlier\" list because, they are continuous variables\n\noutlier=[\"LotFrontage\",\"LotArea\",\"MasVnrArea\",\"BsmtFinSF1\",\"BsmtFinSF2\",\"BsmtUnfSF\",\"TotalBsmtSF\",\"1stFlrSF\",\"LowQualFinSF\",\n         \"GrLivArea\",\"GarageArea\",\"WoodDeckSF\",\"OpenPorchSF\",\"EnclosedPorch\",\"3SsnPorch\",\"ScreenPorch\",\"PoolArea\",\"MiscVal\"]\n\n\nfor i in range(len(outlier)):\n    print(\"Variable: \", outlier[i])\n    zscore_outlier=stats.zscore(df_num[outlier[i]])\n    print(\"Number of outliers higher than threshold 2.5 : \", len(np.where(zscore_outlier>2.5)[0]))\n    print(\"*\"*40)\n","d4fca693":"# Let's analyze using percentile method for \"LotFrontage\" column as an example\n\nQ1 = np.percentile(df_num[\"LotFrontage\"],25)\nQ3 = np.percentile(df_num[\"LotFrontage\"],75)\n\nIQR= Q3-Q1\nLower=Q1 - (1.5*IQR)\nUpper=Q3 + (1.5*IQR)\n\nprint(\"Number of outliers lower than Lower\",len(df_num[df_num[\"LotFrontage\"]<Lower]))\nprint(\"Number of outliers higher than Higher\", len(df_num[df_num[\"LotFrontage\"]>Upper]))","00844ea5":"from scipy.stats.mstats import winsorize\n\n# Now, firstly i will get log transformation continuous variable in \"outlier\" list and i'll winsorize \"outliers_wins\" list; then i will cut outliers if threshold is higher than 2.5:\nfrom scipy import stats\n\noutlier=[\"LotFrontage\",\"LotArea\",\"BsmtUnfSF\",\"TotalBsmtSF\",\"1stFlrSF\",\"GrLivArea\",\"GarageArea\"]\n\nfor i in range(len(outlier)):\n    df_num[outlier[i]]=np.log(df_num[outlier[i]]+1)\n    \nprint(\"*\"*40,\"Continuous Variables\",\"*\"*40)    \nfor j in range(len(outlier)):\n    print(\"Variable: \", outlier[j])\n    zscore_outlier=stats.zscore(df_num[outlier[j]])\n    print(\"Number of outliers higher than threshold 2 : \", len(np.where(zscore_outlier>2)[0]))\n    print(\"*\"*40,\"\\n\")    \n\n \n    \n    \noutlier_wins=[\"MasVnrArea\",\"BsmtFinSF1\",\"BsmtFinSF2\",\"LowQualFinSF\",\"WoodDeckSF\",\"OpenPorchSF\",\"EnclosedPorch\",\n              \"3SsnPorch\",\"ScreenPorch\",\"PoolArea\",\"MiscVal\"]\n\nfor k in range(len(outlier_wins)):\n    df_num[outlier_wins[k]]=stats.mstats.winsorize(df_num[outlier_wins[k]], limits=0.05)    \n\n    \nprint(\"*\"*40,\"Categorical Numeric Variables\",\"*\"*40)\nfor j in range(len(outlier_wins)):\n    \n    print(\"Variable: \", outlier_wins[j])\n    zscore_outlier_wins=stats.zscore(df_num[outlier_wins[j]])\n    print(\"Number of outliers higher than threshold 2 : \", len(np.where(zscore_outlier_wins>2)[0]))\n    print(\"*\"*40)  \n    \n    \n# After the log transformation, we get rid of most of outliers.","9ccd120d":"# I have seperated categorical variable and numerical variable so i will firstly analyze relationship between numerical variables\nfocus_col=[\"SalePrice\"]\n\ndf_num.corr().filter(focus_col).drop(focus_col).abs().sort_values(by=\"SalePrice\", ascending=False).plot(kind=\"barh\", figsize=(15,10));\n\n# According to graph, the most correlated value is \"OverallQual\".","7b7bad45":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler","0df9ba3f":"X=df_num.drop([\"SalePrice\"], axis=1)\nY=df_num[\"SalePrice\"]","a26997c5":"X_scaled=StandardScaler().fit_transform(X)\n\n\npca=PCA(n_components=35)\n\nX_scaled=pca.fit_transform(X_scaled)\n\nexp_var=pca.explained_variance_ratio_\ncumsum_exp=np.cumsum(exp_var)\n\nplt.plot(cumsum_exp)\nplt.grid();\n\n# According to graph, first 15 columns are important to predict ","c35b99a6":"X_scaled=pca.fit_transform(X_scaled)\n\npca_new=PCA(n_components=16)\n\nX_new=pca_new.fit_transform(X_scaled)\n\nexp_var_new=pca_new.explained_variance_ratio_\ncumsum_exp_new=np.cumsum(exp_var_new)\n\nplt.plot(cumsum_exp_new)\nplt.grid();\n\nX_new_df=pd.DataFrame(X_new)\nX_new_df.head(3)","4bdab467":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error,accuracy_score\nfrom sklearn.model_selection import train_test_split","4d536e9b":"dummy=X_new_df.copy()\ndummy[\"target\"]=Y\n\n\nX1=dummy.iloc[:,:16]\nY1=dummy[\"target\"]","1c6e6f95":"X1_train, X1_test, Y1_train, Y1_test = train_test_split(X1, Y1, test_size=0.20, random_state=42)\n\nY1_train = Y1_train.values.reshape(-1,1)\nY1_test=Y1_test.values.reshape(-1,1)\n\nX1_train=StandardScaler().fit_transform(X1_train)\nX1_test=StandardScaler().fit_transform(X1_test)\nY1_train=StandardScaler().fit_transform(Y1_train)\nY1_test=StandardScaler().fit_transform(Y1_test)\n\n\nforest=RandomForestRegressor(n_estimators=25,\n                            random_state=42)\n\nforest.fit(X1_train, Y1_train)\n\ny_test_pred=forest.predict(X1_test)\n\nprint(\"MSE:\", mean_squared_error(Y1_test,y_test_pred))\nprint(\"RMSE:\", np.sqrt(mean_squared_error(Y1_test,y_test_pred)))","383426f2":"from sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm","9c055a6f":"X5=dummy.iloc[:,:16]\nY5=dummy[\"target\"]","2fc4fd56":"X5_train, X5_test, Y5_train, Y5_test = train_test_split(X5,Y5, test_size=0.20, random_state=42)\n\nY5_train = Y5_train.values.reshape(-1,1)\nY5_test=Y5_test.values.reshape(-1,1)\n\n\nX5_train=StandardScaler().fit_transform(X5_train)\nX5_test=StandardScaler().fit_transform(X5_test)\nY5_train=StandardScaler().fit_transform(Y5_train)\nY5_test=StandardScaler().fit_transform(Y5_test)\n\nlrm=LinearRegression()\n\nlrm.fit(X5_train,Y5_train)\n\nlrm_y_test_pred=lrm.predict(X5_test)\n\nplt.figure(figsize=(8,3),dpi=100)\nplt.subplot(1,2,1)\nplt.scatter(Y5_test,lrm_y_test_pred)\nplt.plot(Y5_test,Y5_test,c=\"r\");\n\n# Feature importance with linear regression:\nplt.subplot(1,2,2)\nplt.bar([x for x in range(len(lrm.coef_[0]))], lrm.coef_[0]);","8bcee398":"# According to the Gauss Markov's Assumption, errors shouldn't be correlation each other.\n\nerror=Y5_test-lrm_y_test_pred\nplt.plot(error);","75cdafe3":"X5_train=sm.add_constant(X5_train)\nX5_test=sm.add_constant(X5_test)\n\npredict=sm.OLS(Y5_train,X5_train).fit()\npredict.summary()","80876c6a":"X2=df_num.drop([\"SalePrice\"], axis=1)\nY2=df_num[\"SalePrice\"]\n\n# All continuous variable is defined by X2 variable.","bc0d052c":"X2_train, X2_test, Y2_train, Y2_test = train_test_split(X2,Y2,test_size=0.20, random_state=42)\n\nY2_train=Y2_train.values.reshape(-1,1)\nY2_test=Y2_test.values.reshape(-1,1)\n\nX2_train=StandardScaler().fit_transform(X2_train)\nX2_test=StandardScaler().fit_transform(X2_test)\nY2_train=StandardScaler().fit_transform(Y2_train)\nY2_test=StandardScaler().fit_transform(Y2_test)","247a21b2":"X2_train=sm.add_constant(X2_train)\nX2_test=sm.add_constant(X2_test)\n\npred2=sm.OLS(Y2_train,X2_train).fit()\npred2.summary()","3bbda9f0":"X3=df_num[[\"LotArea\",\"OverallQual\",\"FullBath\",\"OverallCond\", \"MasVnrArea\", \"BsmtFinSF1\",\"TotalBsmtSF\",\"1stFlrSF\",\"2ndFlrSF\",\n           \"LowQualFinSF\",\"GrLivArea\",\"LotFrontage\",\"GarageCars\",\"GarageArea\",\"WoodDeckSF\",\"YearBuilt\",\"Fireplaces\",\"OpenPorchSF\"]]\nY3=df_num[\"SalePrice\"]\n\n# These features are meaningful because of p_value<0.05","7a5697f0":"X3_train,X3_test, Y3_train,Y3_test = train_test_split(X3,Y3, test_size=0.20, random_state=42)\n\nX3_train=sm.add_constant(X3_train)\nX3_test=sm.add_constant(X3_test)\n\npredict3=sm.OLS(Y3_train,X3_train).fit()\npredict3.summary()","29e36534":"# I will choose below features for continuous variables so we shall keep them.\n\nX8=df_num[[\"LotArea\",\"OverallQual\", \"OverallCond\",\"BsmtFinSF1\",\"1stFlrSF\",\"2ndFlrSF\",\"LowQualFinSF\",\"GrLivArea\",\n          \"GarageCars\",\"GarageArea\",\"WoodDeckSF\",\"YearBuilt\",\"Fireplaces\",\"OpenPorchSF\"]]\nY8=df_num[\"SalePrice\"]","e1f09d7c":"# Now, I will analyze the categorical variables\ndf_cat.head(2)","4a5593d1":"# First, visualize them:\n\ndf_cat_col=list(df_cat.columns)\nplt.figure(figsize=(28,25),dpi=100)\nfor i in range(len(df_cat_col)):\n    plt.subplot(8,5,i+1)\n    sns.countplot(df_cat[df_cat_col[i]]);","1e5f6952":"# Machine learning algorithm uses only numerical variable, so we should transform them using variety methods; one of them label encoder:\n\nfrom sklearn.preprocessing import LabelEncoder\n\ndf_cat_col=list(df_cat.columns)\nfor i in range(len(df_cat_col)):\n    df_cat[df_cat_col[i]] = LabelEncoder().fit_transform(df_cat[df_cat_col[i]])\n    ","e21496da":"# For prediction, we should add target variable in this dataset:\ndf_cat[\"SalePrice\"]=df_num[\"SalePrice\"]\ndf_cat.head(3)","fca0aa08":"# I have seperated categorical variable and numerical variable so i will firstly analyze relationship between numerical variables\nfocus_cols=[\"SalePrice\"]\n\ndf_cat.corr().filter(focus_cols).drop(focus_cols).abs().sort_values(by=\"SalePrice\", ascending=False).plot(kind=\"barh\", figsize=(15,10));","42de12e2":"X4=df_cat.drop([\"SalePrice\"], axis=1)\nY4=df_cat[\"SalePrice\"]\n\nX4_train,X4_test,Y4_train,Y4_test= train_test_split(X4,Y4,test_size=0.20, random_state=42)","3fa76b67":"Y4_train=Y4_train.values.reshape(-1,1)\nY4_test=Y4_test.values.reshape(-1,1)\n\nX4_train=StandardScaler().fit_transform(X4_train)\nX4_test=StandardScaler().fit_transform(X4_test)\nY4_train=StandardScaler().fit_transform(Y4_train)\nY4_test=StandardScaler().fit_transform(Y4_test)\n\nX4_train=sm.add_constant(X4_train)\nX4_test=sm.add_constant(X4_test)\n\npredict4=sm.OLS(Y4_train,X4_train).fit()\npredict4.summary()","ab7fdbcd":"from sklearn.ensemble import RandomForestRegressor\n\nX_cat=df_cat.drop([\"SalePrice\"], axis=1)\nY_cat=df_cat[\"SalePrice\"]\n\nX_cat_train, X_cat_test, Y_cat_train, Y_cat_test = train_test_split(X_cat, Y_cat, test_size=0.20, random_state=42)\n\nforest_cat=RandomForestRegressor()\nforest_cat.fit(X_cat_train,Y_cat_train)\n\nfeature_import=pd.Series(data=forest_cat.feature_importances_, index=X_cat_train.columns)\nfeature_import=feature_import.sort_values(ascending=False)\nplt.figure(figsize=(10,8),dpi=100)\nfeature_import.plot(kind=\"barh\");\n\n# It looks like correlation matrix","a51877fa":"feature_import.nlargest(15)\n\n# I want to choose the first 15 features.","e2848695":"# Let's concat both categorical variable columns and numerical variable columns\n\ndf_cat=df_cat[['ExterQual', 'BsmtQual', 'Neighborhood', 'KitchenQual', 'GarageFinish', 'Exterior2nd', 'HouseStyle', 'Exterior1st',\n       \"BldgType\",\"SaleCondition\",\"GarageType\",'BsmtExposure',\"SalePrice\",\"LotShape\",\"CentralAir\"]]\n\ndf_num=df_num[[\"LotArea\",\"OverallQual\", \"OverallCond\",\"BsmtFinSF1\",\"1stFlrSF\",\"2ndFlrSF\",\"LowQualFinSF\",\"GrLivArea\",\n          \"GarageCars\",\"GarageArea\",\"WoodDeckSF\",\"YearBuilt\",\"Fireplaces\",\"OpenPorchSF\"]]\n\ndf_end=pd.concat([df_cat,df_num], axis=1)\ndf_end.head(3)","8e6887b8":"# Target variable should be at the end\n\ncols=[col for col in df_end if col!=\"SalePrice\" ] + [\"SalePrice\"]\ndf_end = df_end[cols]\n\ndf_end.head(3)","d0a89690":"X_end=df_end.drop([\"SalePrice\"], axis=1)\nY_end=df_end[\"SalePrice\"]","f1a74faa":"from sklearn.preprocessing import StandardScaler\n\nX_end_train, X_end_test, Y_end_train, Y_end_test = train_test_split(X_end,Y_end, test_size=0.30, random_state=42)\n\nprint(len(X_end_train))\nprint(len(X_end_test))","43640940":"X_end_train=StandardScaler().fit_transform(X_end_train)\nX_end_test=StandardScaler().fit_transform(X_end_test)\n\nY_end_train=Y_end_train.values.reshape(-1,1)\nY_end_test=Y_end_test.values.reshape(-1,1)\n\nY_end_train=StandardScaler().fit_transform(Y_end_train)\nY_end_test=StandardScaler().fit_transform(Y_end_test)","524092a8":"lrm_end=LinearRegression(normalize=True)\nlrm_end.fit(X_end_train,Y_end_train)\n\npredict_end_test=lrm_end.predict(X_end_test)\npredict_end_train=lrm_end.predict(X_end_train)\n\nplt.figure(figsize=(10,4),dpi=100)\nplt.subplot(1,2,1)\nplt.title(\"Test set prediction\")\nplt.scatter(Y_end_test,predict_end_test)\nplt.plot(Y_end_test,Y_end_test, c=\"r\")\n\nplt.subplot(1,2,2)\nplt.title(\"Train set prediction\")\nplt.scatter(Y_end_train,predict_end_train)\nplt.plot(Y_end_train,Y_end_train, c=\"r\");","c413fb4f":"import statsmodels.api as sm\n\nresult_end=sm.OLS(Y_end_train,X_end_train).fit()\nresult_end.summary()","24648183":"forest_end=RandomForestRegressor(n_estimators=20,\n                                max_depth=7,\n                                criterion=\"mae\")\n\nforest_end.fit(X_end_train, Y_end_train)\n\nforest_test_pred=forest_end.predict(X_end_test)\nforest_train_pred=forest_end.predict(X_end_train)\n\nprint(\"MSE:\", mean_squared_error(Y_end_test,forest_test_pred))\nprint(\"RMSE:\", np.sqrt(mean_squared_error(Y_end_test,forest_test_pred)))\n\nplt.figure(figsize=(7,5),dpi=100)\nimp_for=pd.Series(data=forest_end.feature_importances_, index=X_end.columns).sort_values(ascending=False)\n\nimp_for.plot(kind=\"barh\");","d633959d":"from sklearn.model_selection import GridSearchCV","c524456f":"param_forest={\"n_estimators\":np.arange(10,35,5),\n             \"criterion\":[\"mse\",\"mae\"],\n             \"max_depth\":np.arange(3,10,1)\n             }\n\ngrid_forest=GridSearchCV(estimator=forest_end,\n                        param_grid=param_forest,\n                        cv=10\n                        )\n\ngrid_forest.fit(X_end_train,Y_end_train)\n\nprint(\"Best params:\", grid_forest.best_params_)\nprint(\"Best score:\", grid_forest.best_score_)","79a924cc":"plt.figure(figsize=(5,4), dpi=100)\nplt.title(\"Random Forest Prediction\")\nplt.scatter(forest_test_pred,Y_end_test)\nplt.plot(Y_end_test,Y_end_test, c=\"r\");","1e4e7976":"grid_forest=pd.DataFrame(grid_forest.cv_results_)\n\ngrid_forest[[\"param_criterion\",\"param_max_depth\",\"param_n_estimators\",\"mean_test_score\"]].sort_values(by=[\"mean_test_score\"], ascending=False).head(6)\n\n# According to grid search, param estimator is 20, max_depth is 7, criterion is mae. That's why, we can run again with this parameters","877664d4":"from xgboost import XGBRegressor\n\n\nxgboost=XGBRegressor(objective='reg:linear',\n                    max_depth =7,\n                    n_estimators =15,\n                    learning_rate=0.07,\n                    seed=42,\n                    )\n\nxgboost.fit(X_end_train, Y_end_train)\n\nxgboost_test_pred=forest_end.predict(X_end_test)\nxgboost_train_pred=forest_end.predict(X_end_train)\n\nprint(\"MSE:\", mean_squared_error(Y_end_test,xgboost_test_pred))\nprint(\"RMSE:\", np.sqrt(mean_squared_error(Y_end_test,xgboost_test_pred)))","d72655e5":"plt.figure(figsize=(5,4),dpi=100)\nplt.title(\"XGBOOST Prediction\")\nplt.scatter(xgboost_test_pred,Y_end_test)\nplt.plot(Y_end_test,Y_end_test, c=\"r\");","3718119e":"## Handling Missing Values","f55a33f6":"# PCA for Feature Selection","c31d614e":"### I have applied 3 different machine learning algorithm for prediction SalePrice. The best score is %84.2 with Random Forest algorithm. Linear regression's R-squared is about %83; so features explain %83 of the target value's variance","ea9f20f8":"# After categorical and numerical features were selected, we can continue for prediction","5be19675":"## Linear Regression with PCA","1ce09fde":"### Without PCA, linear regression model's accuracy is higher with 35 features, but some features is not meaningful because of p_value>0.05, so we should use only meaningful features","3f22d9c8":"### * Linear Regression","fdacb830":"# Feature Selection for Categorical Variables","1d135f53":"### What is the linear regression model's accuracy only continuous variable without PCA?","4c1d3c85":"### * Random Forest","f771924a":"### With PCA, linear regression R-squared values is %77.7 ","183185a4":"## Random Forest with PCA","2ad2e03b":"I am new at data science and this is my first competition so; for beginners i will try to analyze and predict clearly. My goal is to make this work a roadmap for people like me at the beginner level.","93c6ab33":"# Conclusion","78b16009":"## For feature selection of categorical variable, apply random forest algorithm"}}