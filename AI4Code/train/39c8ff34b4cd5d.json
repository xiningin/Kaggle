{"cell_type":{"343c3540":"code","27541c76":"code","fd0b86b4":"code","dea4b37d":"code","78e372cd":"code","7046d77b":"code","8024f2a6":"code","20320d82":"code","a4ff7c18":"code","e47eea7f":"code","5fc32829":"code","41796404":"code","7d9ea872":"code","49501ad9":"code","4bef9c50":"code","cf89ec8e":"code","c271ea20":"code","ce19d633":"code","803f03e8":"code","224f11ac":"code","e75eac5b":"code","a2366272":"code","339c919b":"code","ba25cb67":"code","feea19b9":"code","0557b4f6":"code","0bc6957e":"code","cbd5083e":"code","f9b38594":"code","df2d10fa":"code","44564a75":"code","e1400ece":"code","a92e0ab0":"code","18a2a531":"code","524fd88a":"code","34eea7a9":"code","17b9f126":"code","e8bfde10":"code","587940e1":"code","1fb2e62e":"code","e1ea2642":"code","b20f06fb":"code","f1d48bbd":"code","cfacf2b8":"code","87e595f6":"code","d9df1ed7":"code","1adb9697":"code","0290dcf5":"code","6b377a17":"code","87d8a882":"code","1f300c34":"code","4c8bde0f":"code","bb322a16":"code","29fc4887":"code","8f781110":"code","a428b02e":"code","c14c6d88":"code","737719b8":"code","40c24aa5":"code","be136cf0":"code","7fe6e508":"code","5254ff82":"code","349eb03a":"code","7c2190fa":"code","37e9017a":"code","b2c57649":"code","7c11c2bd":"code","7cb39ccc":"code","d6f56f3a":"code","b844b7eb":"code","5f92c3f1":"code","e5c6fb1a":"code","7737227a":"code","f08164af":"code","28b69e7c":"code","6f1db86b":"code","43517da6":"code","cf6c6dc9":"code","68c3d4c4":"markdown","8bdb8e84":"markdown","e1865b17":"markdown","63f26f26":"markdown","0e994b56":"markdown","973bb50d":"markdown","37c0a1a3":"markdown","0345c540":"markdown","612de967":"markdown","8d55a7b3":"markdown","47ec4eb7":"markdown","a769fdf1":"markdown","e6cadf3d":"markdown","aa74fe0f":"markdown","c8292b48":"markdown","4d14a1e7":"markdown","bbdfdfa4":"markdown","4e987557":"markdown","1ccf1daf":"markdown","45cbce20":"markdown","28994857":"markdown","733ca2ad":"markdown","80121ffd":"markdown","d25aee2e":"markdown","8f88acc8":"markdown","b46f35a3":"markdown","cbdd424e":"markdown","fa7ed8b4":"markdown","ff591f3a":"markdown","54402acf":"markdown","c4fe6fd9":"markdown","3e065c8d":"markdown","7823e465":"markdown","9bc1c56c":"markdown","532537d7":"markdown","19062298":"markdown","abd3df24":"markdown"},"source":{"343c3540":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pandas_profiling import ProfileReport\nfrom pathlib import Path\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n%matplotlib inline\n        \ninput_path = Path('\/kaggle\/input\/tabular-playground-series-mar-2021\/')","27541c76":"train = pd.read_csv(input_path \/ 'train.csv', index_col='id')\ndisplay(train.head())","fd0b86b4":"test = pd.read_csv(input_path \/ 'test.csv', index_col='id')\ndisplay(test.head())","dea4b37d":"submission = pd.read_csv(input_path \/ 'sample_submission.csv', index_col='id')\ndisplay(submission.head())","78e372cd":"train.info()","7046d77b":"train.shape","8024f2a6":"test.info()","20320d82":"# All columns\ncols = train.columns\n# Find numerical columns\nnum_cols = train._get_numeric_data().columns\nlen(num_cols)","a4ff7c18":"# Categorical Columns\ncategorical = list(set(cols) - set(num_cols))\nlen(categorical)","e47eea7f":"train.describe()","5fc32829":"profile = ProfileReport(train)\nprofile","41796404":"train['target'].value_counts()","7d9ea872":"# as we can see here, only 16.07% of customers who have churned.\nsns.countplot('target', data=train)","49501ad9":"# Plot categorical columns\ndef pltCountplot(cat, df):\n    \n    fig, axis = plt.subplots((len(cat) \/\/ 4)+1, 4, figsize=(30,16))  \n\n    index = 0\n    sns.set()\n    for i in range((len(cat) \/\/ 4)+1):\n            \n        for j in range(4):\n            \n            if index == len(cat):\n                break\n\n            ax = sns.countplot(cat[index], data=df, ax=axis[i][j]);\n        \n            #for item in ax.get_xticklabels():\n            #    item.set_rotation(15)\n\n            for p in ax.patches:\n                height = p.get_height()\n                ax.text(p.get_x()+p.get_width(), height + 3, '{:1.2f}%'.format(height\/len(df)*100), ha=\"center\", fontsize=14) \n            \n            index += 1\n        \n    plt.subplots_adjust(wspace=0.3, hspace=0.4)","4bef9c50":"# Plot categorical columns\npltCountplot(categorical, train);","cf89ec8e":"# Plot categorical columns with different y using matplotlib library\ndef pltCrosstab(cat, df):\n    \n    fig, axis = plt.subplots((len(cat) \/\/ 4)+1, 4, figsize=(30,16))  \n    fig.tight_layout()\n\n    index = 0\n    #sns.set()\n    for i in range((len(cat) \/\/ 4)+1):\n            \n        for j in range(4):\n            \n            # Since we have 11 numerical columns, some plots will be empty\n            if index == len(cat):\n                break\n            \n            ax = pd.crosstab(df[cat[index]], df['target']).plot(kind='bar', ax=axis[i][j])\n        \n            for item in ax.get_xticklabels():\n                item.set_rotation(90)\n\n            for p in ax.patches:\n                height = p.get_height()\n                ax.text(p.get_x()+p.get_width(), height + 3, '{:1.2f}%'.format(height\/len(df)*100), ha=\"center\", fontsize=8) \n            \n            index += 1\n            \n    plt.subplots_adjust(wspace=0.1, hspace=0.4)","c271ea20":"# Plot categorical columns with different Attrition_Flag\npltCrosstab(categorical, train);","ce19d633":"from plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\n# Plot categorical columns by y with Plotly bar plots\ndef pltStackedBarsPlots(train0, train1):\n\n    num_rows, num_cols = 5,4\n    fig = make_subplots(rows=num_rows, cols=num_cols)\n\n    for index, column in enumerate(train[categorical].columns):\n        i,j = ((index \/\/ num_cols)+1, (index % num_cols)+1)\n        data = train_0.groupby(column)[column].count().sort_values(ascending=False)\n        data = data \n        fig.add_trace(go.Bar(\n            x = data.index,\n            y = data.values,\n            name='Label: 0',\n        ), row=i, col=j)\n\n        data = train_1.groupby(column)[column].count().sort_values(ascending=False)\n        data = data \n        fig.add_trace(go.Bar(\n            x = data.index,\n            y = data.values,\n            name='Label: 1'\n        ), row=i, col=j)\n\n        fig.update_xaxes(title=column, row=i, col=j)\n        fig.update_layout(barmode='stack')\n\n    fig.update_layout(\n        autosize=False,\n        width=1400,\n        height=1400,\n        showlegend=False,\n    )\n    fig.show()\n","803f03e8":"train_0 = train.loc[train['target'] == 0]\ntrain_1 = train.loc[train['target'] == 1]\n    \n# Plot stacked categorical columns by y with Plotly\npltStackedBarsPlots(train_0, train_1)","224f11ac":"def countPlot(column, target, df):\n    ax = pd.crosstab(df[column], df[target]).plot(kind='bar')\n\n    #for item in ax.get_xticklabels():\n    #    item.set_rotation(90)\n\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width(), height + 3, '{:1.2f}%'.format(height\/len(df)*100), ha=\"center\", fontsize=8) \n    \n    fig = plt.gcf()\n    fig.set_size_inches(20, 8)\n    plt.tight_layout()\n    plt.show()","e75eac5b":"countPlot('cat1', 'target', train)","a2366272":"countPlot('cat2', 'target', train)","339c919b":"countPlot('cat3', 'target', train)","ba25cb67":"countPlot('cat4', 'target', train)","feea19b9":"countPlot('cat6', 'target', train)","0557b4f6":"countPlot('cat9', 'target', train)","0bc6957e":"countPlot('cat7', 'target', train)","cbd5083e":"countPlot('cat8', 'target', train)","f9b38594":"countPlot('cat5', 'target', train)","df2d10fa":"countPlot('cat10', 'target', train)","44564a75":"fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(30, 16))\nfig.suptitle('Distribution of Features by Target', fontsize=16)\n\nfor index, column in enumerate(num_cols):\n    i,j = (index \/\/ 3, index % 3)\n    sns.kdeplot(data=train, x=column, hue=\"target\", multiple=\"stack\",ax=axes[i,j])\n    #sns.kdeplot(train.loc[train['target'] == 0, column], color=\"m\", shade=True, ax=axes[i,j])\n    #sns.kdeplot(train.loc[train['target'] == 1, column], color=\"b\", shade=True, ax=axes[i,j])\n\nfig.delaxes(axes[3, 2])\nplt.tight_layout()\nplt.show()","e1400ece":"corr = train[num_cols].corr().abs()\n\nfig, ax = plt.subplots(figsize=(20, 12))\n\n# plot heatmap\nsns.heatmap(corr, mask=np.triu(np.ones_like(corr, dtype=np.bool)), annot=True, fmt=\".2f\", cmap='coolwarm',\n            cbar_kws={\"shrink\": .8}, vmin=0, vmax=1)\n# yticks\nplt.yticks(rotation=0) #Don't rotate the features\nplt.show()","a92e0ab0":"# Check corralated features with eachother\ndef get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.select_dtypes(include=[np.number]).columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_abs_correlations(df, n=5):\n    corr = df.select_dtypes(include=[np.number]).corr()\n    au_corr = corr.abs().unstack()\n    labels_to_drop = get_redundant_pairs(df.select_dtypes(include=[np.number]))\n    #From corrolation table, drop the diagonals(which gives 1 corr)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]\n\nprint(\"Top Absolute Correlations\")\nprint(get_top_abs_correlations(train, 10))","18a2a531":"# Examine the correlations between the features and the target.\nprint(\"Top Correlated Features with Target\")\ncorr = train.select_dtypes(include=[np.number]).corr()\nprint (corr['target'].sort_values(ascending=False)[:5], '\\n')\nprint (corr['target'].sort_values(ascending=False)[-5:])","524fd88a":"from category_encoders import LeaveOneOutEncoder\n\ndef loo_encode(train_df, test_df, column):\n    loo = LeaveOneOutEncoder()\n    new_feature = \"{}_loo\".format(column)\n    loo.fit(train_df[column], train_df[\"target\"])\n    train_df[new_feature] = loo.transform(train_df[column])\n    test_df[new_feature] = loo.transform(test_df[column])\n    return new_feature\n\nloo_features = []\nfor feature in categorical:\n    loo_features.append(loo_encode(train, test, feature))","34eea7a9":"numerical = num_cols.drop('target').tolist()\nfeatures = numerical + loo_features","17b9f126":"# Check features that are in the train DF but not in test DF\nfeatures_not_in_test = set(train.drop('target', axis=1).columns) - set(test.columns)\nfeatures_not_in_test","e8bfde10":"# Check features that are in the train DF but not in test DF\nfeatures_not_in_train = set(test.columns) - set(train.drop('target', axis=1).columns)\nfeatures_not_in_train","587940e1":"print(\"Top Absolute Correlations\")\nprint(get_top_abs_correlations(train, 10))","1fb2e62e":"# Examine the correlations between the features and the target.\nprint(\"Top Correlated Features with Target\")\ncorr = train.select_dtypes(include=[np.number]).corr()\nprint (corr['target'].sort_values(ascending=False)[:5], '\\n')\nprint (corr['target'].sort_values(ascending=False)[-5:])","e1ea2642":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\n\n# set x and y\nX = train[features]\ny = train['target']\n\n# train test split\n# stratify=y preserve the proportion of target as in orginal dataset in the train and test datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99, stratify=y) #train_size=0.60\n\n# scale the data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# set the model\nlogreg = LogisticRegression()\n\n# fit model\nlogreg.fit(X_train, y_train)","b20f06fb":"# Baseline accuracy = proportion of the majority class\nprint('Baseline Accuracy: ',1. - y_train.mean())\nprint('Train Accuracy :',logreg.score(X_train, y_train))\nprint('Test Accuracy: ',logreg.score(X_test, y_test))","f1d48bbd":"print('ROC AUC Score = {}'.format(roc_auc_score(y_test, logreg.predict_proba(X_test)[:, 1])))","cfacf2b8":"from sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nauc = []\nfor n in range(5,10):\n    \n    pipe = make_pipeline(StandardScaler(), LogisticRegression())\n\n    # Note the results will vary each run since we take a different subset of the data each time (since shuffle=True)\n    scores = np.mean(cross_val_score(pipe, X, y, cv=StratifiedKFold(n, random_state=10, shuffle=True), scoring='roc_auc'))\n    auc.append(scores)\n    print(str(n), ' folds: ', 'ROC AUC Score: ', scores)\n\n# plot to see clearly\nplt.plot(range(5,10), auc)\nplt.xlabel('n split')\nplt.ylabel('Mean ROC AUC Score for all folds')\nplt.show();","87e595f6":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\npipe = Pipeline([('classifier' , LogisticRegression())])\n\n# Create param grid.\nparam_grid = [\n    {'classifier' : [LogisticRegression()],\n     'classifier__penalty' : ['l1', 'l2'],\n    'classifier__C' : np.logspace(-4, 4, 20),\n    'classifier__solver' : ['liblinear']}]\n\nclf = GridSearchCV(pipe, \n                   param_grid = param_grid, \n                   cv = StratifiedKFold(8, random_state=10, shuffle=True), \n                   scoring='roc_auc',\n                   return_train_score=True,\n                   n_jobs=-1,\n                   verbose=True)\nclf.fit(X, y)  \nclf.best_estimator_","d9df1ed7":"logreg = LogisticRegression()\nlogreg.fit(X, y)\nsubmission['target'] = logreg.predict_proba(test[features])[:, 1]\nsubmission.to_csv('logistic_regression.csv')","1adb9697":"# RidgeClassifier\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.preprocessing import StandardScaler\n\n# set x and y\nX = train[features]\ny = train['target']\n\n# train test split\n# stratify=y preserve the proportion of target as in orginal dataset in the train and test datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99, stratify=y) #train_size=0.60\n\n# scale the data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# set the model\nridge = RidgeClassifier()\n\n# fit model\nridge.fit(X_train, y_train)","0290dcf5":"# Baseline accuracy = proportion of the majority class\nprint('Baseline Accuracy: ',1. - y_train.mean())\nprint('Train Accuracy :',ridge.score(X_train, y_train))\nprint('Test Accuracy: ',ridge.score(X_test, y_test))","6b377a17":"#print('ROC AUC Score = {}'.format(roc_auc_score(y_test, ridge.predict_proba(X_test)[:, 1])))","87d8a882":"from sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nauc = []\nfor n in range(5,10):\n    \n    pipe = make_pipeline(StandardScaler(), RidgeClassifier())\n\n    # Note the results will vary each run since we take a different subset of the data each time (since shuffle=True)\n    scores = np.mean(cross_val_score(pipe, X, y, cv=StratifiedKFold(n, random_state=10, shuffle=True), scoring='roc_auc'))\n    auc.append(scores)\n    print(str(n), ' folds: ', 'ROC AUC Score: ', scores)\n\n# plot to see clearly\nplt.plot(range(5,10), auc)\nplt.xlabel('n split')\nplt.ylabel('Mean ROC AUC Score for all folds')\nplt.show();","1f300c34":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import StratifiedKFold\n\npipe=make_pipeline(StandardScaler(),RidgeClassifier()) \n\ngrid_search = GridSearchCV(\n    pipe, \n    {'ridgeclassifier__alpha': [0, 0.001, 0.01, 0.1, 0.5, 1, 1.5, 100]},  # Tried [0, 0.001, 0.01, 0.1, 0.5, 1, 1.5, 100] as well range(0,100)\n    cv=StratifiedKFold(8, random_state=10, shuffle=True),\n    scoring='roc_auc',\n    return_train_score=True,\n    n_jobs=-1,\n    verbose=3)\n\n#X_sc = scaler.transform(X)  # can use this if I don't use pipe\ngrid_search.fit(X, y)\ngrid_search.best_estimator_","4c8bde0f":"#ridge = RidgeClassifier()\n#ridge.fit(X, y)\n#submission['target'] = ridge.predict_proba(test[features])[:, 1]\n#submission.to_csv('ridge_classifier.csv')","bb322a16":"ridge = RidgeClassifier()\nridge.fit(X, y)\npredictions = ridge.predict(test[features])\npredictions=pd.DataFrame(predictions,index=test.index)\npredictions.rename(columns={predictions.columns[0]:'target'}, inplace=True)\npredictions.to_csv('ridge_classifier.csv')","29fc4887":"# RidgeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# set x and y\nX = train[features]\ny = train['target']\n\n# train test split\n# stratify=y preserve the proportion of target as in orginal dataset in the train and test datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99, stratify=y) #train_size=0.60\n\n# set the model\nclf = RandomForestClassifier(n_estimators=200, max_depth=7, n_jobs=-1)\n\n# fit model\nclf.fit(X_train, y_train)","8f781110":"# Baseline accuracy = proportion of the majority class\nprint('Baseline Accuracy: ',1. - y_train.mean())\nprint('Train Accuracy :',clf.score(X_train, y_train))\nprint('Test Accuracy: ',clf.score(X_test, y_test))","a428b02e":"y_pred = clf.predict_proba(X_test)[:, 1] # This grabs the positive class prediction\n# score = roc_auc_score(y_test, y_pred)\n# print(f'{score:0.5f}')\nprint('ROC AUC Score = {}'.format(roc_auc_score(y_test, y_pred)))","c14c6d88":"plt.figure(figsize=(8,4))\nplt.hist(y_pred[np.where(y_test == 0)], bins=100, alpha=0.75, label='neg class')\nplt.hist(y_pred[np.where(y_test == 1)], bins=100, alpha=0.75, label='pos class')\nplt.legend()\nplt.show()","737719b8":"clf = RandomForestClassifier(n_estimators=200, max_depth=7, n_jobs=-1)\nclf.fit(X, y)\nsubmission['target'] = clf.predict_proba(test[features])[:, 1]\nsubmission.to_csv('random_forest.csv')","40c24aa5":"import xgboost as xgb\n\n# set x and y\nX = train[features]\ny = train['target']\n\n# train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99, stratify=y)\n\n# Create xgboost model\nm_xgb = xgb.XGBClassifier(n_estimators=200, max_depth=2, random_state=42)\n\n#Train the model using the training sets\nm_xgb.fit(X_train,y_train)","be136cf0":"# Baseline accuracy = proportion of the majority class\nprint('Baseline Accuracy: ',1. - y_train.mean())\nprint('Train Accuracy :',m_xgb.score(X_train, y_train))\nprint('Test Accuracy: ',m_xgb.score(X_test, y_test))","7fe6e508":"y_pred = m_xgb.predict_proba(X_test)[:, 1] # This grabs the positive class prediction\n# score = roc_auc_score(y_test, y_pred)\n# print(f'{score:0.5f}')\nprint('ROC AUC Score = {}'.format(roc_auc_score(y_test, y_pred)))","5254ff82":"plt.figure(figsize=(8,4))\nplt.hist(y_pred[np.where(y_test == 0)], bins=100, alpha=0.75, label='neg class')\nplt.hist(y_pred[np.where(y_test == 1)], bins=100, alpha=0.75, label='pos class')\nplt.legend()\nplt.show()","349eb03a":"# set x and y\nX = train[features]\ny = train['target']\n\n# train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99, stratify=y)\n\n# Create xgboost model\nm_xgb = xgb.XGBClassifier(seed=2021,\n                        n_estimators=10000,\n                        verbosity=1,\n                        eval_metric=\"auc\",\n                        alpha=7.105038963844129,\n                        colsample_bytree=0.25505629740052566,\n                        gamma=0.4999381950212869,\n                        reg_lambda=1.7256912198205319,\n                        learning_rate=0.011823142071967673,\n                        max_bin=338,\n                        max_depth=8,\n                        min_child_weight=2.286836198630466,\n                        subsample=0.618417952155855,\n                        use_label_encoder=False)\n\n#Train the model using the training sets\nm_xgb.fit(X_train,y_train)","7c2190fa":"y_pred = m_xgb.predict_proba(X_test)[:, 1] # This grabs the positive class prediction\n# score = roc_auc_score(y_test, y_pred)\n# print(f'{score:0.5f}')\nprint('ROC AUC Score = {}'.format(roc_auc_score(y_test, y_pred)))","37e9017a":"m_xgb = xgb.XGBClassifier(seed=2021,\n                        n_estimators=10000,\n                        verbosity=1,\n                        eval_metric=\"auc\",\n                        alpha=7.105038963844129,\n                        colsample_bytree=0.25505629740052566,\n                        gamma=0.4999381950212869,\n                        reg_lambda=1.7256912198205319,\n                        learning_rate=0.011823142071967673,\n                        max_bin=338,\n                        max_depth=8,\n                        min_child_weight=2.286836198630466,\n                        subsample=0.618417952155855,\n                        use_label_encoder=False)\nm_xgb.fit(X, y)\nsubmission['target'] = m_xgb.predict_proba(test[features])[:, 1]\nsubmission.to_csv('tuned_xgboost.csv')","b2c57649":"import lightgbm as lbm\n\n# set x and y\nX = train[features]\ny = train['target']\n\n# train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99, stratify=y) #train_size=0.60\n\n# Create LightGBM model\nlgbm = lbm.LGBMClassifier()\n\n#Train the model using the training sets\nlgbm.fit(X_train,y_train)","7c11c2bd":"# Baseline accuracy = proportion of the majority class\nprint('Baseline Accuracy: ',1. - y_train.mean())\nprint('Train Accuracy :',lgbm.score(X_train, y_train))\nprint('Test Accuracy: ',lgbm.score(X_test, y_test))","7cb39ccc":"y_pred = lgbm.predict_proba(X_test)[:, 1] # This grabs the positive class prediction\n# score = roc_auc_score(y_test, y_pred)\n# print(f'{score:0.5f}')\nprint('ROC AUC Score = {}'.format(roc_auc_score(y_test, y_pred)))","d6f56f3a":"plt.figure(figsize=(8,4))\nplt.hist(y_pred[np.where(y_test == 0)], bins=100, alpha=0.75, label='neg class')\nplt.hist(y_pred[np.where(y_test == 1)], bins=100, alpha=0.75, label='pos class')\nplt.legend()\nplt.show()","b844b7eb":"import optuna\nfrom optuna import Trial, visualization\nimport lightgbm as lbm\ndef objective(trial, X=train[features], y=train['target']):\n\n    X_train,X_test,y_train,y_test=train_test_split(X, y, random_state=99, stratify=y)\n\n\n    lgb_params={\n        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2),\n        'max_depth': trial.suggest_int('max_depth', 6, 200),\n        'num_leaves': trial.suggest_int('num_leaves', 31, 120),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0),\n        'random_state': 2021,\n        'metric': 'auc',\n        'n_estimators': trial.suggest_int('n_estimators', 6, 300000),\n        'n_jobs': 12,\n        'cat_feature': [x for x in range(len(categorical))],\n        'bagging_seed': 2021,\n        'feature_fraction_seed': 2021,\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 0.9),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 500),\n        'subsample_freq': trial.suggest_int('subsample_freq', 1, 10),\n        'subsample': trial.suggest_float('subsample', 0.3, 0.9),\n        'max_bin': trial.suggest_int('max_bin', 128, 1024),\n        'min_data_per_group': trial.suggest_int('min_data_per_group', 50, 350),\n        'cat_smooth': trial.suggest_int('cat_smooth', 10, 250),\n        'cat_l2': trial.suggest_int('cat_l2', 1, 20)\n    }\n\n    lgb = lbm.LGBMClassifier(\n        **lgb_params\n    )\n    lgb.fit(\n        X_train,\n        y_train,\n        eval_set=(X_test,y_test),\n        eval_metric='auc',\n        early_stopping_rounds=100,\n        verbose=False\n    )\n    predictions=lgb.predict_proba(X_test)[:,1]\n\n    return roc_auc_score(y_test,predictions)","5f92c3f1":"opt = optuna.create_study(direction='maximize')\nopt.optimize(objective, timeout=3600*7, n_trials=15)","e5c6fb1a":"opt.best_params","7737227a":"# set x and y\nX = train[features]\ny = train['target']\n\n# train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99, stratify=y) #train_size=0.60\n\n# Create LightGBM model\nlgbm = lbm.LGBMClassifier(\n    learning_rate= 0.00630790267395036,\n     max_depth= 197,\n     num_leaves= 55,\n     reg_alpha= 3.1353823119798956,\n     reg_lambda= 6.232626693293953,\n     n_estimators= 297391,\n     colsample_bytree= 0.6074065972411667,\n     min_child_samples= 299,\n     subsample_freq= 4,\n     subsample= 0.8950482914463721,\n     max_bin= 137,\n     min_data_per_group= 51,\n     cat_smooth= 111,\n     cat_l2= 20)\n\n#Train the model using the training sets\nlgbm.fit(X_train,y_train)","f08164af":"y_pred = lgbm.predict_proba(X_test)[:, 1] # This grabs the positive class prediction\n# score = roc_auc_score(y_test, y_pred)\n# print(f'{score:0.5f}')\nprint('ROC AUC Score = {}'.format(roc_auc_score(y_test, y_pred)))","28b69e7c":"lgbm = lbm.LGBMClassifier(\n    learning_rate= 0.00630790267395036,\n     max_depth= 197,\n     num_leaves= 55,\n     reg_alpha= 3.1353823119798956,\n     reg_lambda= 6.232626693293953,\n     n_estimators= 297391,\n     colsample_bytree= 0.6074065972411667,\n     min_child_samples= 299,\n     subsample_freq= 4,\n     subsample= 0.8950482914463721,\n     max_bin= 137,\n     min_data_per_group== 51,\n     cat_smooth= 111,\n     cat_l2= 20)\nlgbm.fit(X, y)\nsubmission['target'] = lgbm.predict_proba(test[features])[:, 1]\nsubmission.to_csv('tuned2_LightGBM.csv')","6f1db86b":"# set x and y\nX = train[features]\ny = train['target']\n\n# train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99, stratify=y) #train_size=0.60\n\n# Create LightGBM model\nlgbm = lbm.LGBMClassifier(learning_rate= 0.00605886703283976,\n    max_depth= 42,\n    num_leaves= 108,\n    reg_alpha= 0.9140720355379223,\n    reg_lambda= 9.97396811596188,\n    colsample_bytree= 0.2629101393563821,\n    min_child_samples= 61,\n    subsample_freq= 2,\n    subsample= 0.8329687190743886,\n    max_bin= 899,\n    min_data_per_group= 73,\n    cat_smooth= 21,\n    cat_l2= 11,\n    random_state= 2021,\n    metric= 'auc',\n    n_estimators= 20000,\n    n_jobs= -1,\n    bagging_seed= 2021,\n    feature_fraction_seed= 2021)\n\n#Train the model using the training sets\nlgbm.fit(X_train,y_train)","43517da6":"y_pred = lgbm.predict_proba(X_test)[:, 1] # This grabs the positive class prediction\n# score = roc_auc_score(y_test, y_pred)\n# print(f'{score:0.5f}')\nprint('ROC AUC Score = {}'.format(roc_auc_score(y_test, y_pred)))","cf6c6dc9":"lgbm = lbm.LGBMClassifier(learning_rate= 0.00605886703283976,\n    max_depth= 42,\n    num_leaves= 108,\n    reg_alpha= 0.9140720355379223,\n    reg_lambda= 9.97396811596188,\n    colsample_bytree= 0.2629101393563821,\n    min_child_samples= 61,\n    subsample_freq= 2,\n    subsample= 0.8329687190743886,\n    max_bin= 899,\n    min_data_per_group= 73,\n    cat_smooth= 21,\n    cat_l2= 11,\n    random_state= 2021,\n    metric= 'auc',\n    n_estimators= 20000,\n    n_jobs= -1,\n    bagging_seed= 2021,\n    feature_fraction_seed= 2021)\nlgbm.fit(X, y)\nsubmission['target'] = lgbm.predict_proba(test[features])[:, 1]\nsubmission.to_csv('tuned_LightGBM.csv')","68c3d4c4":"<a id='2'><\/a>\n# <p style=\"background-color: #ea4335; color: #FFFFFF ;font-family:newtimeroman; font-size:120%\">2. Data Exploration<\/p>","8bdb8e84":"## Tuned LightGBM Model","e1865b17":"<a id='2.3'><\/a>\n## <p style=\"background-color: #ea4335; color: #FFFFFF ; font-family:newtimeroman; font-size:120%\">2.3 Numerical Columns <\/p>","63f26f26":"We checked that there is no null values either in train or test data.\n\n**Train data:** 300k rows, 31 columns\n* 12 numerical columns\n* 19 categorical columns                \n\n**Test data:** 200k rows, 30 columns\n * 11 numerical columns (since target not included)\n * 19 categorical columns   ","0e994b56":"Seems like there is no highly correlated numeric feature with target.","973bb50d":"## Optuna","37c0a1a3":"* **cont1** has the highest correlation with **cont2** with a correlation of 0.861623.\n* **cont10** has a high correlation of 0.807896 and 0.775453 with **cont0** and **cont7**.\n* Continuous features that have a correlation > 0.7:\n **cont0** and **cont7**, \n **cont8** with **cont1**, \n **cont8** with **cont2**","0345c540":"[This source](https:\/\/towardsdatascience.com\/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159 ) provides good explanation of types to encode categorical data and which encoding technique should be used when. \n\nSince **we have features with high cordinality**(high number of dictinct values) such as **cat5**(84 distinct categories), **cat7**(51 distinct categories), **cat8**(61 distinct categories) and **cat10**(299 distinct categories), It would be great if we use Bayesian encoders.  \n\nThe Bayesian encoders use information from the dependent variable in their encodings. They output one column and can work well with high cardinality data.","612de967":"Some features seems correlated with eachother.","8d55a7b3":"## Let's take a look at how the model predicted the various classes\n\nThe graph below shows that the model does well with most of the negative observations, but struggles with many of the positive observations.","47ec4eb7":"<a id='4.3'><\/a>\n## <p style=\"background-color: #34a853; color: #FFFFFF ; font-family:newtimeroman; font-size:120%\">4.3 Random Forest <\/p>","a769fdf1":"## Optuna","e6cadf3d":"<a id='4'><\/a>\n# <p style=\"background-color: #34a853; color: #FFFFFF ;font-family:newtimeroman; font-size:120%\">4. Model Building<\/p>","aa74fe0f":"<a id='2.1'><\/a>\n## <p style=\"background-color: #ea4335; color: #FFFFFF ; font-family:newtimeroman; font-size:120%\">2.1 Target Variable<\/p>","c8292b48":"Here we can see that our data is unbalanced. Only 26.5% of out target = 1, rest of them target = 0.","4d14a1e7":"<a id='4.4'><\/a>\n## <p style=\"background-color: #34a853; color: #FFFFFF ; font-family:newtimeroman; font-size:120%\">4.4 XGBoost <\/p>","bbdfdfa4":"## Tuned LightGBM Model","4e987557":"#### Top Correlated Features with Target","1ccf1daf":"## Submission","45cbce20":"<a id='4.1'><\/a>\n## <p style=\"background-color: #34a853; color: #FFFFFF ; font-family:newtimeroman; font-size:120%\">4.1 Logistic Regression <\/p>","28994857":"<a id='2.2'><\/a>\n## <p style=\"background-color: #ea4335; color: #FFFFFF ; font-family:newtimeroman; font-size:120%\">2.2 Categorical Columns<\/p>","733ca2ad":"#### Top Absolute Correlations","80121ffd":"<a id='3'><\/a>\n# <p style=\"background-color: #fbbc05; color: #FFFFFF ;font-family:newtimeroman; font-size:120%\">3. Encode Features<\/p>","d25aee2e":"<a id='2.4'><\/a>\n## <p style=\"background-color: #ea4335; color: #FFFFFF ; font-family:newtimeroman; font-size:120%\">2.4 Correlation Check <\/p>","8f88acc8":"## <p style=\"background-color:GreenYellow; font-family:newtimeroman; font-size:120%; text-align:center\">Table of Content<\/p>\n\n* [1. Read the Data Files](#1)\n* [2. Data Exploration](#2)\n    * [2.1 Target Variable](#2.1)\n    * [2.2 Categorical Columns](#2.2)\n    * [2.3 Numerical Columns](#2.3)\n    * [2.4 Correlation Check](#2.4)\n* [3. Encode Features](#3)\n* [4. Model Building](#4)\n    * [4.1 Logistic Regression](#4.1)\n    * [4.2 Ridge Classifier](#4.2)\n    * [4.3 Random Forest](#4.3)\n    * [4.4 XGBoost](#4.4)\n    * [4.5 LightGBM](#4.5)","b46f35a3":"Here is another way to compare categorical features by it's target values:\n\nUsing stacked bar plots, we can compare target 1 and 0 ratio within the category easily. And plotly library give us more dynamic plots.","cbdd424e":"## Submission","fa7ed8b4":"<a id='4.5'><\/a>\n## <p style=\"background-color: #34a853; color: #FFFFFF ; font-family:newtimeroman; font-size:120%\">4.5 LightGBM <\/p>","ff591f3a":"<a id='4.2'><\/a>\n## <p style=\"background-color: #34a853; color: #FFFFFF ; font-family:newtimeroman; font-size:120%\">4.2 Ridge Classifier <\/p>","54402acf":"## Submission","c4fe6fd9":"## Pull out the target, and make a validation split","3e065c8d":"## Tuned XGBoost Model","7823e465":"## Submission","9bc1c56c":"cat1, cat2, cat3, cat4, cat5, cat6, cat7, cat8, cat9 and cat10 have a lot of categories that it wasn't showing pretty when we print all the categorical columns' categories. \n\nThe more categories you have, the more columns you get when using one-hot encoding, which can create huge tables but it's more difficult to handle and look through the data. Some tree-based algorithms also use subsets of columns to prevent overfitting, which can cause problems when using a lot of one-hot-encoded columns.\n\nLet's check those features seperately.","532537d7":"<a id='1'><\/a>\n# <p style=\"background-color: #4285f4; color: #FFFFFF ;font-family:newtimeroman; font-size:120%\">1. Read the Data Files<\/p>","19062298":"## Submission","abd3df24":"cat5, cat7, cat8 and cat10 have high cardinality(high number of dictinct values):\n* cat5 has 84 distinct categories\n* cat7 has 51 distinct categories\n* cat8 has 61 distinct categories\n* cat10 hast 299 distinct categories"}}