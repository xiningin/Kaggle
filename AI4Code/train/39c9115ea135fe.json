{"cell_type":{"30d077d7":"code","4cdeb7ac":"code","534d60f2":"code","f1bfee39":"code","d48efeff":"code","0e1ca212":"code","40ab7141":"code","7fd6b2d6":"code","92f8e3a5":"code","defed09c":"code","d5509e47":"markdown","6caad54f":"markdown","63cf1a19":"markdown","ab8dff41":"markdown","463a1caf":"markdown","266e86eb":"markdown","f2f77308":"markdown","a57d9af8":"markdown"},"source":{"30d077d7":"import numpy as np\nimport math \nimport pandas as pd\nimport matplotlib.pyplot as plt","4cdeb7ac":"#nltk - Natural language ToolKit\nfrom collections import Counter\nfrom nltk import word_tokenize\nfrom nltk.util import ngrams\n\n#textblob - library for NLP\nfrom textblob import TextBlob","534d60f2":"# reading dataset\nreddit_df = pd.read_pickle('..\/input\/reddit-comments-from-subreddits-humour-and-news\/redditDataset.pkl')\nreddit_df.info()\n\n# check missing values\nmissing = reddit_df.isnull().sum().sum()\nif missing > 0:\n    reddit_df.dropna(inplace=True)\n    \n# removing duplicates \nduplicates = reddit_df.duplicated().sum() \nif duplicates > 0:\n    reddit_df.drop_duplicates(inplace=True)\n\n# general statistics of data \nreddit_df.describe()","f1bfee39":"# Taking Category 1 from dataframe\nL_text = reddit_df.text.loc[reddit_df.subreddit ==\"humor\"].tolist()\n\npolarity = [] # Polarity(float) - lies b\/w [-1,1], where \"+1\" indicates positive sentiments and \"-1\" indicates negative sentiments\nsubjectivity = [] # Subjectivity(float) - lies b\/w [0,1]. It refers to an opinion\/emotion etc.\n\nfor i in range(len(L_text)):\n    Sentiment = TextBlob(L_text[i]).sentiment\n    polarity.append(Sentiment.polarity)\n    subjectivity.append(Sentiment.subjectivity)\n\nprint(\"Category 1 - where (x-axis :  polarity\/subjectivity , y-axis :  number of comments)\")\n\nmean_polarity = sum(polarity)\/len(polarity)\nsumm = 0\nfor i in range(len(polarity)):\n    summ = summ + pow(polarity[i]- mean_polarity,2)\n\nstd_polarity = math.sqrt(summ\/len(polarity))\n\nprint(\"mean_polarity: \" + str(mean_polarity))\nprint(\"std_polarity: \" + str(std_polarity)+\"\\n\")\n\nmean_subjectivity = sum(subjectivity)\/len(subjectivity)\nsumm = 0\nfor i in range(len(subjectivity)):\n    summ = summ + pow(subjectivity[i] - mean_subjectivity,2)\n\nstd_subjectivity = math.sqrt(summ\/len(subjectivity))\n\nprint(\"mean_subjectivity: \"+ str(mean_subjectivity))\nprint(\"std_subjectivity: \" + str(std_subjectivity))\n\n#plot taking uniform buckets to club the polarity values \nplt.title(\"Sentiments Analysis - Polarity\") \nplt.hist(polarity,bins = np.arange(-1,1,0.1),ec='black') #to show separate bins \nplt.ylabel('No. of comments #')\nplt.xlabel('Polarity')\nplt.show()\n\n#plot taking uniform buckets to club the polarity values \nplt.title(\"Sentiments Analysis - Subjectivity\") \nplt.hist(subjectivity,bins = np.arange(0,1,0.1),ec='black') #to show separate bins \nplt.ylabel('No. of comments #')\nplt.xlabel('Subjectivity')\nplt.show()\n","d48efeff":"# Similarly taking Category 2 from dataframe\n\nL_text2 = reddit_df.text.loc[reddit_df.subreddit ==\"news\"].tolist()\n\npolarity2 = []\nsubjectivity2 = []\nfor i in range(len(L_text2)):\n    Sentiment = TextBlob(L_text2[i]).sentiment\n    polarity2.append(Sentiment.polarity)\n    subjectivity2.append(Sentiment.subjectivity)\n\nprint(\"Category 2 - where (x-axis :  polarity\/subjectivity , y-axis :  number of comments)\")\n\nmean_polarity2 = sum(polarity2)\/len(polarity2)\nsumm = 0\nfor i in range(len(polarity2)):\n    summ = summ + pow(polarity2[i] - mean_polarity2,2)\n\nstd_polarity2 = math.sqrt(summ\/len(polarity2))\n\nprint(\"mean_polarity: \" + str(mean_polarity2))\nprint(\"std_polarity: \" + str(std_polarity2)+\"\\n\")\n\nmean_subjectivity2 = sum(subjectivity2 )\/len(subjectivity2)\nsumm = 0\nfor i in range(len(subjectivity2)):\n    summ = summ + pow(subjectivity2[i]- mean_subjectivity2,2)\n\nstd_subjectivity2 = math.sqrt(summ\/len(subjectivity2))\n\nprint(\"mean_subjectivity: \"+ str(mean_subjectivity2))\nprint(\"std_subjectivity: \" + str(std_subjectivity2))\n\n# taking uniform buckets to club the polarity values \nplt.title(\"Sentiments Analysis - Polarity\") \nplt.hist(polarity2,bins = np.arange(-1,1,0.1),ec='black') #to show separate bins \nplt.ylabel('No. of comments #')\nplt.xlabel('Polarity')\nplt.show()\n\n# taking uniform buckets to club the polarity values \nplt.title(\"Sentiments Analysis - Subjectivity\") \nplt.hist(subjectivity2,bins = np.arange(0,1,0.1),ec='black') #to show separate bins \nplt.ylabel('No. of comments #')\nplt.xlabel('Subjectivity')\nplt.show()","0e1ca212":"# Category - humour\nprint(\"Category - 1\")\n\ncount_unigram = Counter()\ncount_bigram = Counter()\nfor i in range(len(L_text)):\n    token = word_tokenize(L_text[i])\n    unigram = ngrams(token,1)\n    count_unigram += Counter(unigram)\n    bigram = ngrams(token,2)\n    count_bigram += Counter(bigram)\n    \nA = count_unigram.most_common(10)  #tuple having text and it's count\nB = count_bigram.most_common(10)\n\nA_values = []\nA_text = []\nB_values = []\nB_text = []\n\nfor i in range(len(A)):\n    A_values.append((A[i][1]))\n    B_values.append((B[i][1]))\n    A_text.append(str(A[i][0])[2:-3]) #since counter function gives a tuple I'm str slicing from to get string\n    B_text.append(str(B[i][0]).replace(\"(\",'').replace(\")\",'').replace(\"'\",'').replace(\" \",''))\n\n# taking uniform buckets to club the polarity values \nplt.title(\"Top 10 most occuring - unigram\") \n# plt.hist(A_values,bins = np.arange(0,10,0.5),ec='black') #to show separate bins \nplt.bar(A_text,A_values)\nplt.ylabel('Occurrence ')\nplt.xlabel('Unigram')\nplt.show()\n\n# taking uniform buckets to club the polarity values \nplt.title(\"Top 10 most occuring - bigram\") \n# plt.hist(A_values,bins = np.arange(0,10,0.5),ec='black') #to show separate bins \nplt.bar(B_text,B_values)\nplt.ylabel('Occurrence ')\nplt.xlabel('Bigram')\nplt.show()","40ab7141":"print(\"Category - 2\")\ncount_unigram = Counter()\ncount_bigram = Counter()\nfor i in range(len(L_text2)):\n    token = word_tokenize(L_text2[i])\n    unigram = ngrams(token,1)\n    count_unigram += Counter(unigram)\n    bigram = ngrams(token,2)\n    count_bigram += Counter(bigram)\n    \nA = count_unigram.most_common(10)  #tuple having text and it's count\nB = count_bigram.most_common(10)\n\nA_values = []\nA_text = []\nB_values = []\nB_text = []\n\nfor i in range(len(A)):\n    A_values.append((A[i][1]))\n    B_values.append((B[i][1]))\n    A_text.append(str(A[i][0])[2:-3]) #since counter function gives a tuple I'm str slicing from to get string\n    B_text.append(str(B[i][0]).replace(\"(\",'').replace(\")\",'').replace(\"'\",'').replace(\" \",''))\n\n# plot taking uniform buckets to club the polarity values \nplt.title(\"Top 10 most occuring - unigram\") \n# plt.hist(A_values,bins = np.arange(0,10,0.5),ec='black') #to show separate bins \nplt.bar(A_text,A_values)\nplt.ylabel('Occurrence ')\nplt.xlabel('Unigram')\nplt.show()\n\n# plot taking uniform buckets to club the polarity values \nplt.title(\"Top 10 most occuring - bigram\") \n# plt.hist(A_values,bins = np.arange(0,10,0.5),ec='black') #to show separate bins \nplt.bar(B_text,B_values)\nplt.ylabel('Occurrence ')\nplt.xlabel('Bigram')\nplt.show()","7fd6b2d6":"# preprocessing the text corpus \n# lowering case and removing stopwords\n# Ignoring punctuation & special character\n# Reducing words to their stem (e.g. \u201cplay\u201d from \u201cplaying\u201d) (Optional)\n\nL_text = [i.lower() for i in reddit_df.text.loc[reddit_df.subreddit ==\"humor\"].tolist()]\nL_text = [i.replace('[^\\w\\s]','') for  i in L_text]\n\n# Building the Bag of Words model - represent text for feature extraction\n# 1. Tokenize each sentence to words, take all the words & count them to find frequent words\n# 2. Label them to Create binary vector of length L(unique words)\n# 3. Take Single token (I'm taking unigrams as we already having unigram for all comments)\n\nA = count_unigram.most_common(10)  #changing counter object to tuple (containing word and token count)\n\nA_values = []\nA_text = []\nfor i in range(len(A)):\n    A_values.append((A[i][1]))\n    A_text.append(A[i][0])\n\n    \ndef makebow(L_text):\n    BOW = []  #list of binary list(vectors)\n    for i in range(len(L_text)):\n        row = L_text[i]\n        sent_vec = []\n        eachrow_tokens = word_tokenize(row)\n        for word in A_text:\n            word = str(word)[2:-3]\n            if word in eachrow_tokens:\n                sent_vec.append(1)\n            else:\n                sent_vec.append(0)\n        BOW.append(sent_vec)\n    return BOW\n\nbow = []\nbow = makebow(L_text)\n# uncomment below line to view\n# bow  ","92f8e3a5":"# Using sklearn to trained a decent machine learning model to classify the comments\n\n# libraries - Trying different Models By checking accuracies and modifying other parameters\nfrom sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# libraries - labelling of data & analysis metrics\nfrom sklearn.preprocessing import LabelEncoder \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics","defed09c":"# since there are more than 60k duplicate comments in overall 100k \n# It's highly likely that model will give 100% accuracy in that case therfore removing duplicates\n\ndataset = reddit_df.drop_duplicates().copy()     #copy of original dataset\n#Since scikit built-in model uses float not string directly we are using label encoder to label binary numbers out to category\ndataset.text = LabelEncoder().fit_transform(dataset.text) \ndataset.subreddit = LabelEncoder().fit_transform(dataset.subreddit) \n\n# splitting the dataset 70% for training and 30% for testing \ntext_train, text_test, subreddit_train, subreddit_test = train_test_split(dataset, dataset.subreddit, test_size = 0.3)\n\n# model = LogisticRegression()\n# model = RandomForestRegressor()\n# model = RandomForestClassifier()\n# model = SVC()\nmodel = KNeighborsClassifier()\n\n# fitting the model \nmodel.fit(text_train, subreddit_train)\n\n# evaluating the model for predictions\npredicted = model.predict(text_test)\n\n# accuracy of predictions by matching predicted and test \nacc = accuracy_score(subreddit_test, predicted)\nprint('Accuracy: %.3f' % (acc*100))\n\n# metrics showing -  precision, accuracy, recall and F1-score. \nprint(metrics.classification_report(subreddit_test, predicted))\n\n#assigning text label to display predicted category\nlabel_predicted = []\nfor i in range(len(predicted)):\n    if predicted[i] == 0:\n        label_predicted.append(\"humor\")\n    elif predicted[i] == 1:\n        label_predicted.append(\"news\")\n        \nconf_matrix = confusion_matrix(subreddit_test, predicted,labels=[0,1])\nprint(conf_matrix)\n\n# Plotting confusion matrix to demonstrate our model\u2019s performance.\nplt.matshow(conf_matrix)\nplt.title('Confusion Matrix\\n')\nplt.xlabel('Predicted')\nplt.ylabel('Correct')\n\nplt.colorbar()\nplt.show()\nprint(\"0 represents  - Humour \\n1 represents  - News\")\nprint(\"\\nNote - Model performance could be improved But I left that part for you :) \")\nlabel_predicted","d5509e47":"\n# 2. Plotting distributions of the top 10 most occurring unigrams and bi-grams \n---\n## Category 1 - Humour ","6caad54f":"---\n## Category 2 - News \n","63cf1a19":"\n## Category 2 - News ","ab8dff41":"# 1. Plotting distribution of polarity and subjectivity for both categories \n\n\n---\n## Category 1 - Humour \n","463a1caf":"# Pre-processing Data","266e86eb":"# Reddit + NLP\/ML \n\n1.  Finding polarity and Subjectivity of the comments\n2.  Finding top 10 most occurring unigrams and bi-grams\n3.  Representing the Comments as vectors using Bag of words(BOW) approach\n4.  Building ML model to classify the comments as \u201cnews\u201dand \u201chumour\u201d and reporting their accuracies\n---","f2f77308":"# 4. Machine learning Classifier to classify the comments as \u201cnews\u201dand \u201chumour\u201d\n---\n* Reporting the performance of classification model ","a57d9af8":"# 3. Representing the Comments as vectors using Bag of words(BOW) approach "}}