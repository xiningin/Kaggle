{"cell_type":{"cbc033c5":"code","ea3a9251":"code","971b2791":"code","93829112":"code","069e8073":"code","5f0a4efc":"code","7a531874":"code","88652d8b":"code","f2c0f57e":"code","198a5be2":"code","58c0519f":"code","801f1c41":"code","5ac2a26e":"code","d21c1fd0":"code","25e6526e":"code","a4692ce2":"code","95ccca53":"code","9f1a8608":"code","ccf7ef17":"code","3ec3e523":"code","5db6e072":"code","8b0d0255":"code","5d2e1f31":"code","7a2b4c1d":"code","f450ef75":"code","b3c87503":"code","cfbf667b":"code","380163f0":"code","0527251b":"code","f6388f8f":"code","eab895e4":"code","2b351ade":"code","9e50f148":"code","54ae4c85":"code","5ecb1739":"code","4f89c37b":"code","6d035d43":"code","08628fc5":"code","de084f36":"code","622ab778":"code","645e9136":"code","5171c319":"code","2c4baf9a":"code","88f7e2d6":"code","8ccd862d":"code","aa54832c":"code","2dc6a30b":"code","8d508fa7":"code","033fb1d4":"code","ab403685":"code","d96549dc":"markdown","7da9d293":"markdown","2831c052":"markdown","664a94bb":"markdown","b124c7f4":"markdown","83fc73f4":"markdown","f631a7fc":"markdown","7489a8d4":"markdown","199c87e4":"markdown","f3da7f93":"markdown","decc1294":"markdown","7abb7a57":"markdown","eef82ab9":"markdown","3ca1d505":"markdown","1589455f":"markdown","b62987d9":"markdown","4e73901d":"markdown","81a0ce82":"markdown","a0804a2f":"markdown","a0ecc94e":"markdown","c0e24ed7":"markdown","d19e70c2":"markdown","ff60442e":"markdown","a77148e4":"markdown","961f7309":"markdown","a8215d4a":"markdown","cdd4be61":"markdown","b12528b6":"markdown","b81626ed":"markdown","dc9a644e":"markdown","f930cd9e":"markdown","03b83561":"markdown","df0cd65d":"markdown","2c3b300d":"markdown","d94a0c5a":"markdown","5b4b1158":"markdown","ddef8908":"markdown","40063f09":"markdown","8055710c":"markdown","e63da446":"markdown","ca2d7aa5":"markdown","3ae9df90":"markdown","8d5f1d38":"markdown","651780d4":"markdown","8e207177":"markdown"},"source":{"cbc033c5":"# Basic libraries\nimport pandas as pd\nimport numpy as np\nimport time\nimport datetime\nimport gc\n\n# Data preprocessing\nimport category_encoders as ce\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use(\"fivethirtyeight\")\n\n# Time series analysis\nfrom statsmodels.graphics.tsaplots import plot_acf\nimport statsmodels.api as sm\n\n# Normality test\nfrom scipy import stats\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ea3a9251":"# Dataloading\npath = \"\/kaggle\/input\/m5-forecasting-accuracy\/\"\n\ncalendar = pd.read_csv(os.path.join(path,\"calendar.csv\"))\ntrain = pd.read_csv(os.path.join(path, \"sales_train_validation.csv\"))\nprice = pd.read_csv(os.path.join(path, \"sell_prices.csv\"))","971b2791":"# Change dtype to light\n# Calendar data\ndef dtype_ch_calendar(df):\n    # Columns name\n    int16_col = [\"wday\", \"month\", \"snap_CA\", \"snap_TX\", \"snap_WI\",\"wm_yr_wk\", \"year\"]\n\n    # dtype change\n    df[\"date\"] = pd.to_datetime(df[\"date\"])\n    df[int16_col] = df[int16_col].astype(\"int16\")\n\n    return df\n\n# price data\ndef dtype_ch_price(df):\n    # Columns name\n    int16_col = [\"wm_yr_wk\"]\n    float16_col = [\"sell_price\"]\n\n    # dtype change\n    df[int16_col] = df[int16_col].astype(\"int16\")\n    df[float16_col] = df[float16_col].astype(\"float16\")\n\n    return df\n\n# train data\ndef dtype_ch_train(df):\n    # Columns name\n    int16_col = df.loc[:,\"d_1\":].columns\n    # dtype change\n    df[int16_col] = df[int16_col].astype(\"int16\")\n\n    return df","93829112":"def create_features_calendar(df):\n    # Change dtype to light\n    df = dtype_ch_calendar(df)\n\n    # day of month variable\n    df[\"mday\"] = df[\"date\"].dt.day.astype(\"int16\")\n\n    # event object to numerical, ordinal encoder\n    list_col = [\"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n    for i in list_col:\n        ce_oe = ce.OrdinalEncoder(cols=i, handle_unknown='impute') # Create instance of OrdinalEncoder\n        df = ce_oe.fit_transform(df)\n        df[i] = df[i].astype(\"int16\") # change to light dtype\n        \n    return df","069e8073":"# dtype change to light\ncalendar = create_features_calendar(dtype_ch_calendar(calendar))\nprice = dtype_ch_price(price)\ntrain = dtype_ch_train(train)","5f0a4efc":"# Data merge\ndef data_merge_3df(train, calendar, price):\n    df = pd.DataFrame({})\n    id_col = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n    df = train.melt(id_vars=id_col, var_name=\"d\", value_name=\"volume\")\n    df.drop([\"id\", \"cat_id\", \"state_id\"], axis=1, inplace=True)\n\n    # calendar data merge\n    df = pd.merge(df, calendar, left_on=\"d\", right_on=\"d\", how=\"left\")\n    # price data merge\n    df = pd.merge(df, price, left_on=[\"store_id\", \"item_id\", \"wm_yr_wk\"], right_on=[\"store_id\", \"item_id\", \"wm_yr_wk\"], how='left')\n    \n    df.drop(\"wm_yr_wk\", axis=1, inplace=True)\n\n    gc.collect()\n\n    return df","7a531874":"gc.collect()","88652d8b":"# Create merged dataframe\nmaster = data_merge_3df(train, calendar, price)","f2c0f57e":"del calendar\ngc.collect()","198a5be2":"# Distribution plot function\ndef distribution_plot(df, col_name=\"item_id\", target_value=\"d_1913\"):\n    value = df.groupby(col_name)[target_value].mean().values\n\n    # Visualization\n    plt.figure(figsize=(10,6))\n    sns.distplot(value)\n    plt.xlabel(\"Volume\")\n    plt.ylabel(\"Frequency\")\n    plt.title(\"Volume distribution by each item_id at {}\".format(target_value))\n    \n# Box plot function\ndef box_plot(x, y, df, size=(20,6), y_label=\"Volume\", stripplot=True):\n    fig, ax = plt.subplots(1, 2, figsize=size)\n\n    # Including outliers\n    if stripplot == True:\n        sns.boxplot(x=x, y=y, data=df, showfliers=False, ax=ax[0])\n        sns.stripplot(x=x, y=y, data=df, jitter=True, ax=ax[0])\n    else:\n        sns.boxplot(x=x, y=y, data=df, ax=ax[0])\n    ax[0].set_ylabel(y_label)\n    ax[0].set_title(\"box plot at {} with outliers\".format(y))\n    ax[0].tick_params(axis='x', labelrotation=45)\n    \n    # Not including outliers\n    sns.boxplot(x=x, y=y, data=df, ax=ax[1], sym=\"\")\n    ax[1].set_ylabel(y_label)\n    ax[1].set_title(\"box plot at {} without outliers\".format(y))\n    ax[1].tick_params(axis='x', labelrotation=45)\n\n# 2 params bubble plot function\ndef bubble_plot(x=\"store_id\", y=\"dept_id\", s=\"d_1913\", df=train, size=(20,10)):\n    # mean value\n    data_ave = df.groupby([x,y])[s].mean().reset_index()\n    x_ave = data_ave[x]\n    y_ave = data_ave[y]\n    s_ave = data_ave[s]\n\n    # max values\n    data_max = df.groupby([x,y])[s].max().reset_index()\n    x_max = data_max[x]\n    y_max = data_max[y]\n    s_max = data_max[s]\n\n    # visualization\n    fig, ax = plt.subplots(1, 2, figsize=size)\n\n    ax[0].scatter(x_ave, y_ave, s=s_ave*100, alpha=0.5, color=\"blue\")\n    ax[0].set_xlabel(x)\n    ax[0].set_ylabel(y)\n    ax[0].set_title(\"Bubble chart of average volume on {}\".format(s))\n\n    ax[1].scatter(x_max, y_max, s=s_max*100, alpha=0.5, color=\"green\")\n    ax[1].set_xlabel(x)\n    ax[1].set_ylabel(y)\n    ax[1].set_title(\"Bubble chart of average volume on {}\".format(s))\n\n# correlation plot\ndef correlation_plot(x=\"sell_price\", y=\"volume\", df=master, sample_size=3000, size=(8,8)):\n    samp = df.sample(sample_size)\n    x_data = samp[x]\n    y_data = samp[y]\n    \n    plt.figure(figsize=size)\n    plt.scatter(x_data,y_data)\n    plt.xlabel(x)\n    plt.ylabel(y)\n    plt.title(x+\"vs\"+y + \", Sampling {}\".format(sample_size))","58c0519f":"# Time series\ndef time_series_plot(data, freq=28, size=(20,12), title=\"\"):\n    index = data.index\n    col = data.columns\n    \n    fig, ax = plt.subplots(2,1, figsize=size)\n    \n    # Raw data\n    for i in col:\n        ax[0].plot(index, data[i], label=i, linewidth=1)\n        ax[0].legend()\n        ax[0].set_title(\"{} : Time series plot of raw data\".format(title))\n    \n    # Rolling mean data\n    for i in col:\n        ax[1].plot(index, data[i].rolling(freq).mean(), label=i, linewidth=1)\n        ax[1].legend()\n        ax[1].set_title(\"{} : Time series plot of rolling {} data\".format(title, freq))\n\n# R coefficient plot \ndef r_coef_plot(df, max_lag=72, size=(20,6)):\n    col = df.loc[:, \"d_1\":].columns\n    \n    r_corr = []\n    lag = range(len(col))\n    \n    for i in range(len(col)):\n        x = df.iloc[:,-1]\n        y = df[col[-i-1]]\n        r = np.corrcoef(x,y)[0,1]\n        r_corr.append(r)\n        \n    fig, ax = plt.subplots(1,2, figsize=size)\n    \n    ax[0].plot(lag, r_corr)\n    ax[0].set_xlabel(\"lag\")\n    ax[0].set_ylabel(\"R coefficient\")\n    ax[0].set_ylim([0,1])\n    ax[0].set_title(\"Lag volume R coefficient\")\n    \n    ax[1].plot(lag[:max_lag], r_corr[:max_lag])\n    ax[1].set_xlabel(\"lag\")\n    ax[1].set_ylabel(\"R coefficient\")\n    ax[1].set_ylim([0,1])\n    ax[1].set_title(\"Lag volume (Max lag range {}) R coefficient\".format(max_lag))\n\n# Auto correlation plot\ndef autocorrelation_plot(data, lags=28):\n    col = data.columns\n    fig, ax = plt.subplots(len(col), 2, figsize=(20, 6*len(col)))\n    \n    for c in range(len(col)):                 \n        # autocorrelation\n        plot_acf(data[col[c]], lags=lags, ax=ax[c,0])\n        ax[c,0].set_title(\"Auto correlation of {}\".format(col[c]))\n        ax[c,0].set_xlabel(\"lag\")\n        ax[c,0].set_ylabel(\"auto correlation\")\n        # time series\n        ax[c,1].plot(data[col[c]][-365:].index, data[col[c]][-365:], linewidth=1)\n        ax[c,1].set_title(\"Time series data of {}\".format(col[c]))\n        ax[c,1].set_xlabel(\"day\")\n        ax[c,1].set_ylabel(\"volume\")\n        \n    plt.show()\n\n# Resid normality test\ndef normality_test(df, freq=7):\n    col_name = df.columns\n    resid_df = pd.DataFrame({})\n    \n    for c in col_name:\n        res = sm.tsa.seasonal_decompose(df[c], period=freq)\n        resid_df[\"Resid_{}\".format(c)] = res.resid\n        \n    resid_df.dropna(inplace=True)\n        \n    fig, ax = plt.subplots(resid_df.shape[1], 2, figsize=(20, 6*resid_df.shape[1]))\n    plt.subplots_adjust(hspace=0.4)\n    col_name = resid_df.columns\n    for i in range(len(resid_df.columns)):\n        # Shapiro wilk test\n        WS, p = stats.shapiro(resid_df[col_name[i]])\n        # distribution plot\n        sns.distplot(resid_df[col_name[i]], ax=ax[i, 0])\n        ax[i, 0].set_xlabel(\"resid\")\n        ax[i, 0].set_title(\"Distribution of resid : {} \\n p-value of Shapiro Wilk test : {:.3f}\".format(col_name[i], p))\n        # probability\n        stats.probplot(resid_df[col_name[i]], plot=ax[i,1])\n        ax[i, 1].set_title(\"Probability plot\")","801f1c41":"distribution_plot(train, col_name=\"item_id\", target_value=\"d_1913\")","5ac2a26e":"# dept_id : N=7 separated from cat_id, volume distribution on latest day with boxplot\nbox_plot(x=\"dept_id\", y=\"d_1913\", df=train, size=(20,6), y_label=\"Volume\", stripplot=True)","d21c1fd0":"box_plot(x=\"store_id\", y=\"d_1913\", df=train, size=(20,6), y_label=\"Volume\", stripplot=True)","25e6526e":"bubble_plot(x=\"store_id\", y=\"dept_id\", s=\"d_1913\", df=train, size=(20,6))","a4692ce2":"# for keeping memory\ngc.collect()","95ccca53":"# price distribution with boxplot on latest day\nbox_plot(x=\"store_id\", y=\"sell_price\", df=price, size=(20,6), y_label=\"Price\", stripplot=False)","9f1a8608":"# create dept_id from item_id\nprice_copy = price.copy()\nprice_copy[\"dept_id\"] = [s.rsplit(\"_\",1)[0] for s in price_copy[\"item_id\"]]\n\nbox_plot(x=\"dept_id\", y=\"sell_price\", df=price_copy, size=(20,6), y_label=\"Price\", stripplot=False)\n\ndel price_copy\ngc.collect()","ccf7ef17":"del price\ngc.collect()","3ec3e523":"# Correlation with volume\ncorrelation_plot(x=\"sell_price\", y=\"volume\", df=master, sample_size=1500, size=(8,8))","5db6e072":"# year\nbox_plot(x=\"year\", y=\"volume\", df=master, size=(20,6), y_label=\"Volume\", stripplot=False)","8b0d0255":"# month\nbox_plot(x=\"month\", y=\"volume\", df=master[master[\"year\"]==2015], size=(20,6), y_label=\"Volume\", stripplot=False)","5d2e1f31":"# weekday\nbox_plot(x=\"weekday\", y=\"volume\", df=master[master[\"year\"]==2015], size=(20,6), y_label=\"Volume\", stripplot=False)","7a2b4c1d":"master[\"day_of_month\"] = pd.to_datetime(master[\"date\"]).dt.day\n\n# day of month\nbox_plot(x=\"day_of_month\", y=\"volume\", df=master[master[\"year\"]==2015], size=(20,6), y_label=\"Volume\", stripplot=False)","f450ef75":"gc.collect()","b3c87503":"# Omitted due to memory over on kaggle\n# snap\n# master[\"snap_flag\"] = master[\"snap_CA\"] + master[\"snap_TX\"] + master[\"snap_WI\"]\n\n# box_plot(x=\"snap_flag\", y=\"volume\", df=master[master[\"year\"]==2015], size=(20,6), y_label=\"Volume\", stripplot=False)","cfbf667b":"# Omitted due to memory over on kaggle\n# event_name\n# master[[\"event_name_1\", \"event_name_2\"]] = master[[\"event_name_1\", \"event_name_2\"]].fillna(\"no\")\n# master[\"event_flag\"] = master[\"event_name_1\"] + str(\"+\") + master[\"event_name_2\"]","380163f0":"# Omitted due to memory over on kaggle\n# box_plot(x=\"event_flag\", y=\"volume\", df=master[master[\"year\"]==2015], size=(20,6), y_label=\"Volume\", stripplot=False)","0527251b":"gc.collect()","f6388f8f":"r_coef_plot(train, max_lag=72, size=(20,6))","eab895e4":"# Sampling items creation\nsample_num = [1, 100, 1000, 10000]\nsample_id = []\nfor i in sample_num:\n    id_name = train[\"item_id\"].values[i]\n    sample_id.append(id_name)\n\nsample_df = master[(master[\"item_id\"] == sample_id[0]) | (master[\"item_id\"] == sample_id[1]) | (master[\"item_id\"] == sample_id[2])| (master[\"item_id\"] == sample_id[3])]\nsample_df = pd.pivot_table(sample_df, index=\"date\", columns=\"item_id\", values=\"volume\", aggfunc=\"mean\")","2b351ade":"time_series_plot(sample_df, freq=7, size=(20,12), title=\"item_id\")","9e50f148":"# Sample id\nautocorrelation_plot(data=sample_df, lags=56)","54ae4c85":"# Normality test of sample \nnormality_test(sample_df, freq=7)","5ecb1739":"normality_test(sample_df, freq=28)","4f89c37b":"del sample_df\ngc.collect()","6d035d43":"# dept_id\ndept_df = pd.pivot_table(master, index=\"date\", columns=\"dept_id\", values=\"volume\", aggfunc=\"mean\")","08628fc5":"time_series_plot(dept_df, freq=7, size=(20,12), title=\"dept_id\")","de084f36":"time_series_plot(dept_df, freq=28, size=(20,12), title=\"dept_id\")","622ab778":"# dept_id\nautocorrelation_plot(data=dept_df, lags=56)","645e9136":"# dept_df\nnormality_test(dept_df, freq=7);","5171c319":"# dept_df\nnormality_test(dept_df, freq=28);","2c4baf9a":"del dept_df\ngc.collect()","88f7e2d6":"# store_id\nstore_df = pd.pivot_table(master, index=\"date\", columns=\"store_id\", values=\"volume\", aggfunc=\"mean\")","8ccd862d":"time_series_plot(store_df, freq=28, size=(20,12), title=\"store_id\")","aa54832c":"time_series_plot(store_df, freq=28, size=(20,12), title=\"store_id\")","2dc6a30b":"# store_id\nautocorrelation_plot(data=store_df, lags=56)","8d508fa7":"# store_df\nnormality_test(store_df, freq=7)","033fb1d4":"normality_test(store_df, freq=28)","ab403685":"del store_df\ngc.collect()","d96549dc":"### I'm having a hard time building the model myself and the accuracy is not so increasing, but I hope this notebook will be helpful to someone.","7da9d293":"My Model training plan <br>\n*Creating model is not including this kernel.\n\n### Target data : volume value <br>\n### Train data : Using only before 28days information from target data date. <br>\n\nAnalysis of target data relation ship <br>\n\n1) category features : \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\" <br>\n\n- item_id : N=3,049, volume(item_id average) distribution on latest day.\n\n- dept_id : N=7 separated from cat_id, volume distribution on latest day with boxplot<br>\n\n- cat_id : N=3 \u2191<br>\n\n- store_id : N=10 separated from state_id, volume distribution on latest day with boxplot<br>\n\n- state_id : N=3 \u2191<br>\n\n- dept_id and state_id and volume(average) of latest day with bubble plot\n\n2) price features : \"sell_price\"\n- sell_price : price distribution with boxplot on latest day & scatter plot vs volume\n \n3-1) day features : \"year\", \"month\", \"day of month\", \"weekday\" <br>\n\n- year : 2011 ~ 2016 vs volume(Average) with boxplot<br>\n\n- month : 1 ~ 12 vs volume(Average) with boxplot <br>\n\n- day of month : 1 ~ vs volume(Average) with boxplot<br>\n\n- weekday : 1 ~ 7 (Saturday ~ Friday ) vs volume(Average) with boxplot <br>\n\n3-2) Time series\n\n- Target time series analysis, autocorrelation plot\n\n- Resid of time series volume, check by Normality test","2831c052":"## price\nprice distribution with boxplot on latest day & scatter plot vs volume","664a94bb":"## Sample item (item_id)","b124c7f4":"- By dept_id, we can see that the price range will change significantly. In particular, HOUSEHOLD_2 has a wide price range, and it can be seen that large outliers also belong to this category.\n- we can see that HOBBIES_1 is expensive on average.","83fc73f4":"We can see that sales are increasing at specific events. event_flag is likely to be an important feature amount in sales forecast.","f631a7fc":"## day features : \"year\", \"month\", \"day of month\", \"weekday\", \"event\", \"snap\"<br>\n\n- year : 2011 ~ 2016 vs volume(Average) <br>\n\n- month : 1 ~ 12 vs volume(Average) <br>\n\n- day of month : 1 ~ vs volume(Average)<br>\n\n- weekday : 1 ~ 7 (Saturday ~ Friday ) vs volume(Average) <br>\n\n- snap and event flag vs volume","7489a8d4":"This figure visualizes the average and maximum sales volume by combining store_id and dept_id.\n\n- Each average is large in CA_1,2,3 of HOUSEHOLD_1 and FOODS_3. FOODS_3 has a large difference between the average and the maximum value.\n- We can see that the difference between the average and the maximum value is not constant, but changes depending on the combination of dept_id and store_id.\n\nI will create a feature that combines the two.","199c87e4":"The average value by dept_id is plotted.\n\n- We can see that there are big depressions throughout the year.\n\n- The periodicity seems to be able to grasp the weekly cycle by removing noise with a 7-day moving average. Also, by taking a moving average on a 28-day cycle, it seems that overall sales fluctuations can be captured.\n\n- Looking at the data for the last year, We can see a big drop just before 2016-1. It is speculated that this is a Christmas holiday.\n\n- Many of the residuals from the moving average graph have a long left tail. It is speculated that this is due to the drop in particular days. Even if a moving average is taken, it is necessary to be careful in handling data on a specific day when such a large drop occurs when creating a feature amount.","f3da7f93":"## dept_id","decc1294":"## Time series analysis","7abb7a57":"Several samples were taken to visualize the time series.\n\n- Looking at the results, it can be seen that the overall sales fluctuate like noise. It is necessary to understand the cycle.\n\n- Also, the tendency is different for each one. In some cases, it can be said that sales have dropped significantly during a certain period.\n\n- It can be seen that the cycle of increase and decrease in sales is mostly a 7-day cycle. As for the lag feature, it is considered better to adopt a 7-day cycle.\n\n- Checking the residual with the moving average, we can see that the normality is high around 0, but the tail of the distribution is large, and the normality of the outliers is low. As can be seen from the results obtained so far, it is not enough to obtain information about the lag feature amount that is the moving average, and it is necessary to include information such as other prices and specific dates.","eef82ab9":"- Looking at the time-series graphs for each store, in addition to the big drop in Christmas as before, we were able to confirm the drop in specific stores. There may be a specific day like a store holiday.\n\n- The autocorrelation coefficient also has a strong 7-day cycle. Only in WI_2 and WI_3, the strength of the 28-day cycle is noticeable.\n\n- Similar to dept_id, there are many graphs with long left tails as the residuals from the moving average graph.","3ca1d505":"This tends to differ in sales volume by store_id.\n- CA_1,2,3 and WI_1,2 are sold a lot. On the other hand, CA_4, TX_1,2,3 and WI_3 are few in sales.\n- Although there are outliers in all stores, CA_3 is particularly large and conversely CA_4 is small.\n\nIt seems that the feature quantity of dept_id is also required.","1589455f":"Since the volume of sales volume may vary from year to year, we analyzed the 2015 data for the subsequent time variables.","b62987d9":"- The sales volume for each item has a wide range. There are a wide range of items, especially those with 10 or more, but the frequency is very low. These are considered to be difficult to predict due to their very small sample and low data density. <br>\n- First, it is necessary to understand what kind of characteristics the data is located in this skirt.","4e73901d":"## item_id \nN=3,049, volume(item_id average) distribution on latest day","81a0ce82":"In order to improve the estimation accuracy of the model, it is important to understand what characteristics the data feature has. This time, we analyzed the relationship between each data and the target sales. <br>\n\nNext under stury <br>\nBased on this data, we consider the preprocessing of the data and the selection and creation of important features, and build the model.","a0804a2f":"## dept_id and state_id\ndept_id and state_id and volume(average and max) of latest day with bubble plot","a0ecc94e":"## month","c0e24ed7":"## dept_id\ndept_id : N=7 separated from cat_id, volume distribution on latest day with boxplot","d19e70c2":"Regarding snap_flag, there is no difference in sales volume for each flag.","ff60442e":"Looking at the date of the month, there is no characteristic that it is often found at the beginning or end of the month.","a77148e4":"## store_id\nstore_id : N=10 separated from state_id, volume distribution on latest day with by boxplot","961f7309":"# Features and EDA","a8215d4a":"# Data loading","cdd4be61":"## day of month","b12528b6":"This figure is a box plot of store_id and price.\n\n-We can see that the average price does not change much at any store. However, some stores have large outliers, and the size is very large.","b81626ed":"## weekday","dc9a644e":"This is a boxplot of sales volume of final data by dept_id.\n- First, most of the sales are 0 to 2. On the other hand, many outliers are also observed, and the number is wide, ranging from 10 to a maximum of 120.\n\n- It can also be seen that quite a lot of products have 0 sales. Predicting that it is 0 is a very important factor.\n\nIt seems that the feature quantity of dept_id is necessary.","f930cd9e":"# Exploratory data analysis","03b83561":"First, I confirmed the correlation between the latest date sales and the previous sales data.<br>\n- As a result, it can be seen that the correlation coefficient decreases as the dates depart. It was confirmed that the information for the forecast data is closer to the data that is closer to the date and time as possible.\n- At a certain point, there is a point at which the correlation coefficient decreases periodically. It is speculated that this is due to a specific day of the year (see below).","df0cd65d":"- Most of the forecast sales volume is 0 or 1 or 2. However, the range above that is very wide. Therefore, the larger the value, the lower the density of the training model, so some improvement is required to improve the estimation accuracy.\n\n- Sales volume varies in specific categories. However, many of the differences are small, and 75% of the data are between 0 and 3. There is a category that tends to take a large value, and it is necessary to include data that can separate its characteristics.\n\n- Price range and category are related. Price and sales volume are also related, but not constant. Cheaper products may sell more, but not necessarily.\n\n- From the time series data, it is better to select the lag feature amount that is closer in time than the yearly period. The cycle is strong for 7 days and 28 days.\n\n- Raw data if the 7-day cycle is captured. 7-day moving average data may be included if the 28-day cycle is captured. The overall trend is likely to be captured using 28-day moving average data.\n\n- A big drop in sales on certain days like vacation is a strong noise. Strong noise days such as Christmas are not included in the prediction range, but caution should be exercised when creating a lag feature and building a model.","2c3b300d":"This figure is a plot of the relationship between price and sales volume.\n\n- It can be seen that the price of a product with a large sales volume is low. It can be seen that the number of sales does not increase significantly as the price goes up.","d94a0c5a":"# Summery","5b4b1158":"Plotted box plots by year.\n\n- Most of the sales volume is 0 to 2, but there are large outliers in each year.\n\n- Although the period may be short in 2016, the number of large sales is getting smaller as the years go by.","ddef8908":"Next) <br>\nData cleaning and feature quantities are created to build a prediction model, and the results are evaluated.","40063f09":"It can be seen that there are large sales volumes on Saturday and Sunday. However, outliers such as the maximum number do not mean that there are many Saturdays and Sundays.","8055710c":"Looking at the overall average, it is not that there are many specific months.","e63da446":"## year","ca2d7aa5":"## store_id","3ae9df90":"## snap_flg","8d5f1d38":"## event_flg","651780d4":"# Create EDA Functions","8e207177":"### Correlation coefficient between objective variable and lag sales"}}