{"cell_type":{"dc9525f0":"code","9bab3615":"code","43012989":"code","32d537b3":"code","0e9afed4":"code","4a5e5242":"code","4335a173":"code","b48c7be7":"code","aeb53520":"code","f6c38dd9":"code","fd5ea962":"code","a74c312a":"code","e3bb2baa":"code","e3577355":"code","1e0a95c9":"code","52acc91a":"code","b1cafd38":"code","1645ff92":"code","a974952b":"code","1e5cb8e1":"code","754f5050":"code","46abbd19":"code","d138aede":"code","99d55d39":"code","784b07e7":"code","eecb35bd":"code","8393b875":"code","2344c265":"code","29ad427b":"code","5ce9ca47":"code","aeafd5d1":"code","09302fb0":"code","2b1f1885":"code","de3558ca":"code","80413968":"code","6c22ed63":"code","c34179d0":"code","bafe38de":"code","b568c7a7":"code","9fadee64":"code","12f2eccc":"code","bea94618":"code","b8ef0f86":"code","ebd4cfca":"code","0656b05a":"code","d345889d":"markdown","a221e311":"markdown","b27a76d6":"markdown","fce4597a":"markdown","a411b4a7":"markdown","ff7217cd":"markdown","a46b32dc":"markdown","9711ff5a":"markdown","b6943d69":"markdown","b8e65a13":"markdown","805be3f9":"markdown","a3f302d5":"markdown","5be95a7d":"markdown","57333a34":"markdown","1dd01672":"markdown","3280a45d":"markdown","35a218ee":"markdown","1aeab4b3":"markdown","c6ab18f8":"markdown","6dc4b767":"markdown","8f658dba":"markdown","6380cc0b":"markdown","43ee0f2a":"markdown","09ff3806":"markdown","a1ba5c03":"markdown","b3f57086":"markdown","64748763":"markdown"},"source":{"dc9525f0":"!mkdir fastai_lib;cd fastai_lib;git clone https:\/\/github.com\/rpauli\/fastai.git;ln -s fastai_lib\/fastai\/old\/fastai\/ ..\/","9bab3615":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport ast\nimport json\n\nimport numpy as np\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.utils import shuffle\n\nfrom fastai.imports import *\n\nfrom fastai.transforms import *\nfrom fastai.conv_learner import *\nfrom fastai.model import *\nfrom fastai.dataset import *\nfrom fastai.sgdr import *\nfrom fastai.plots import *\n\n%load_ext autoreload\n%autoreload","43012989":"import warnings\nwarnings.filterwarnings(\"ignore\")","32d537b3":"\ndef f2cat(filename: str) -> str:\n    return filename.split('.')[0]\n\ndef list_all_categories():\n    files = os.listdir(os.path.join(path_base, path_train))\n    return sorted([f2cat(f) for f in files], key=str.lower)","0e9afed4":"def calc_apk(pred,actual,k=3):\n    \"\"\"\n    pred 2D array, \n    axis 0 is predicted items\n    axis 1 is probabilities per label\n    \"\"\"\n    pred=to_np(pred)\n    actual=to_np(actual)\n    top_idxs=top_k_preds(pred,k)\n    idx_0=(np.where(actual==1)[1][:,None]-top_idxs)\n    idx_correct=idx_0==0\n    weighted_scores=1\/np.arange(1,k+1)\n    score_per_data=weighted_scores[np.argmax(idx_correct,axis=1)]*np.max(idx_correct,axis=1)\n    mean_score=np.mean(score_per_data)\n    return mean_score\n\ndef top_k_preds(pred,k):\n    top_idxs=np.argsort(-to_np(pred),axis=1)[:,:k]\n    return top_idxs\n\ndef acc_metric(preds,actual):\n    return accuracy_score(np.where(actual==1)[1],np.argmax(to_np(preds),axis=1))","4a5e5242":"class FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=1, reduce=True):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduce = reduce\n\n    def forward(self, inputs, targets):\n        \n        BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n        pt = torch.exp(-BCE_loss)\n        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n\n        if self.reduce:\n            return torch.mean(F_loss)\n        else:\n            return F_loss","4335a173":"def draw_cv2(raw_strokes, size=256, lw=6, time_color=True):\n    img = np.zeros((BASE_SIZE, BASE_SIZE,3), np.float32)\n    t_max=max(1,len(raw_strokes))\n    \n    velos=[np.sqrt(np.sum(np.square(np.diff(stroke)),axis=0)).astype(int) for stroke in raw_strokes]\n    velo_min=np.min([np.min(velo) for velo in velos])\n    velo_max=max(1,np.max([np.max(velo) for velo in velos]))\n    for t, stroke in enumerate(raw_strokes):\n        velo=velos[t]\n        velo=(velo)*0.9\/(velo_max)+0.1\n        velo*=255\n        for i in range(len(stroke[0]) - 1):\n            try:\n                color = (255 - int(t*255.\/t_max), int(t*255.\/t_max), int(velo[i])) if time_color else 255    \n            except:\n                print(t,t_max,t*255.\/t_max,int(t*255.\/t_max),int(velo[i]))\n            _ = cv2.line(img, (stroke[0][i], stroke[1][i]),\n                         (stroke[0][i + 1], stroke[1][i + 1]), color, lw)\n    img\/=255\n    if size != BASE_SIZE:\n        return cv2.resize(img, (size, size))\n    else:\n        return img","b48c7be7":"class DoodleDataset(Dataset):\n    \"\"\"Adapted the fastai Dataset class for my needs, mainly the get_x function\"\"\"\n    def __init__(self, x, y, transform=None,sz=64):\n        self.x = x\n        self.y = y\n        self.transform = transform\n        self.n = self.get_n()\n        self.c = self.get_c()\n        self.sz = sz\n    @abstractmethod\n    def get_x(self, i):\n        return draw_cv2(json.loads(self.x[i]),self.sz)\n    @abstractmethod\n    def get_y(self, i):\n        y= np.zeros(self.c,dtype=np.float)\n        y[int(self.y[i])]=1\n        return y\n    @property\n    def is_multi(self): return True\n    @property\n    def is_reg(self):return False\n    #this flag is set to remove the output sigmoid that allows log(sigmoid) optimization\n    #of the numerical stability of the loss function\n    \n    def get1item(self, idx):\n        x,y = self.get_x(idx),self.get_y(idx)\n        return self.get(self.transform, x, y)\n\n\n    def __getitem__(self, idx):\n        if isinstance(idx,slice):\n            xs,ys = zip(*[self.get1item(i) for i in range(*idx.indices(self.n))])\n            return np.stack(xs),ys\n        return self.get1item(idx)\n\n    def __len__(self): return self.n\n\n    def get(self, tfm, x, y):     \n        return (x,y) if tfm is None else tfm(x,y)    \n    \n    @abstractmethod\n    def get_n(self):\n        return len(self.y)\n\n    @abstractmethod\n    def get_c(self):\n        return len(np.unique(self.y))\n\n    @abstractmethod\n    def get_sz(self):\n        return self.sz\n\n\n    @property\n    def is_multi(self):\n        \"\"\"Returns true if this data set contains multiple labels per sample.\"\"\"\n        return True\n\n\n\nclass DoodleImageData(ImageData):\n\n    def get_dl(self, ds, shuffle):\n        if ds is None: return None\n        return DataLoader(ds, batch_size=self.bs, shuffle=shuffle,\n            num_workers=self.num_workers, pin_memory=False)\n\n","aeb53520":"def show_img(ims, idx, figsize=(5,5), normed=True, ax=None):\n    if ax is None: fig,ax = plt.subplots(figsize=figsize)\n    if normed: ims = np.rollaxis(to_np(ims),1,4);ims*=0.07367;ims+=0.09587;\n    else:      ims = np.rollaxis(to_np(ims),1,4)\n    ax.imshow(np.clip(ims,0,1)[idx,:,:,:])\n    ax.axis('off')","f6c38dd9":"class DoodleConvnetBuilder(ConvnetBuilder):\n    def __init__(self, f, c, is_multi, is_reg, ps=None, xtra_fc=None, xtra_cut=0, custom_head=None, pretrained=True):\n        self.f,self.c,self.is_multi,self.is_reg,self.xtra_cut = f,c,is_multi,is_reg,xtra_cut\n        if xtra_fc is None: xtra_fc = [512]\n        if ps is None: ps = [0.25]*len(xtra_fc) + [0.5]\n        self.ps,self.xtra_fc = ps,xtra_fc\n\n        if f in model_meta: cut,self.lr_cut = model_meta[f]\n        else: cut,self.lr_cut = 0,0\n        cut-=xtra_cut\n        layers = cut_model(f(pretrained), cut)\n        \n        layers[0] = nn.Conv2d(1,64,kernel_size=(7,7),stride=(2,2),padding=(3, 3), bias=False)\n        \n        self.nf = model_features[f] if f in model_features else (num_features(layers)*2)\n        if not custom_head: layers += [AdaptiveConcatPool2d(), Flatten()]\n        self.top_model = nn.Sequential(*layers)\n\n        n_fc = len(self.xtra_fc)+1\n        if not isinstance(self.ps, list): self.ps = [self.ps]*n_fc\n\n        if custom_head: fc_layers = [custom_head]\n        else: fc_layers = self.get_fc_layers()\n        self.n_fc = len(fc_layers)\n        self.fc_model = to_gpu(nn.Sequential(*fc_layers))\n        if not custom_head: apply_init(self.fc_model, kaiming_normal)\n        self.model = to_gpu(nn.Sequential(*(layers+fc_layers)))\n        \n\n        \nclass DoodleConvLearner(ConvLearner):\n    @classmethod\n    def pretrained(cls, f, data, ps=None, xtra_fc=None, xtra_cut=0, custom_head=None, precompute=False,\n                   pretrained=True, **kwargs):\n        models = DoodleConvnetBuilder(f, data.c, data.is_multi, data.is_reg,\n            ps=ps, xtra_fc=xtra_fc, xtra_cut=xtra_cut, custom_head=custom_head, pretrained=pretrained)\n        return cls(data, models, precompute, **kwargs)\n\n    @classmethod\n    def lsuv_learner(cls, f, data, ps=None, xtra_fc=None, xtra_cut=0, custom_head=None, precompute=False,\n                  needed_std=1.0, std_tol=0.1, max_attempts=10, do_orthonorm=False, **kwargs):\n        models = DoodleConvnetBuilder(f, data.c, data.is_multi, data.is_reg,\n            ps=ps, xtra_fc=xtra_fc, xtra_cut=xtra_cut, custom_head=custom_head, pretrained=False)\n        convlearn=cls(data, models, precompute, **kwargs)\n        convlearn.lsuv_init()\n        return convlearn","fd5ea962":"def get_stats(X_trn,X_val,y_trn,y_val,N=100):\n    bs=340\n    num_workers=4 # Apprently 2 cpus per kaggle node\n    sz=64\n    aug_tfms = None\n    #dummy transformations, i subtract mean 0 and devide by 1 so it shouldnt change the stats\n    tfms = tfms_from_stats((np.array([0.,0.,0.]), np.array([1.,1.,1.])), sz=sz, aug_tfms=aug_tfms,tfm_y=TfmType.NO)\n    datasets=DoodleImageData.get_ds(DoodleDataset,(X_trn,y_trn),(X_val, y_val),tfms=tfms,sz=sz)\n    doodleimageloader=DoodleImageData('.',datasets,bs,num_workers,enc.classes_)\n    mn,var=[],[]\n    for i,(x,y) in tqdm(enumerate(iter(doodleimageloader.trn_dl))):\n        batch=to_np(x).swapaxes(1,3).reshape([-1,3])\n        mn.append(np.mean(batch,axis=0))\n        var.append(np.var(batch,axis=0))\n        if i>N:\n            break\n    channel_mean,channel_var=np.mean(mn,axis=0),np.mean(var,axis=0)\n    return channel_mean,channel_var","a74c312a":"def get_set(N_cat,N_rows,N_skip):\n    if N_cats < len(list_all_categories()):\n        df = pd.concat([pd.read_csv(f'{path_base}\/{path_train}\/{cat}.csv',usecols=['drawing','word','key_id','recognized'],nrows=N_rows,skiprows=np.arange(1,N_skip+1)) for cat in  tqdm(np.random.choice(list_all_categories(),N_cats,replace=False))])\n    elif  N_cats == len(list_all_categories()):\n        df = pd.concat([pd.read_csv(f'{path_base}\/{path_train}\/{cat}.csv',usecols=['drawing','word','key_id','recognized'],nrows=N_rows,skiprows=np.arange(1,N_skip+1)) for cat in  tqdm(list_all_categories())])\n    else:\n        print('Somethings wrong')\n    df_cleaned=df[df.recognized==True].drop(['recognized'],axis=1)\n    #df_cleaned=df.drop(['recognized'],axis=1)\n    df_cleaned.word=df_cleaned.word.str.replace(' ','_')\n    return df","e3bb2baa":"path_base='..\/input'\npath_train='train_simplified'\n# path_base='.'\n# path_train='train'","e3577355":"np.random.seed(0)\n#subsample for now\nN_cats=len(list_all_categories())\nN_rows_train=10000 #I can currently do about 8k samples which is not nearly enough\nN_rows_val=100 # This is a common value among kagglers 100 items per category to validate\nN_cats","1e0a95c9":"df_val=get_set(N_cats,N_rows_val,0)\ndf_trn=get_set(N_cats,N_rows_train,N_rows_val)\ndf_val=df_val.set_index('key_id')\ndf_trn=df_trn.set_index('key_id')\ndf_val.index.intersection(df_trn.index)","52acc91a":"test_file='test_simplified.csv'\ndf_test=pd.read_csv(f'{path_base}\/{test_file}')\ndf_test.info()","b1cafd38":"enc=LabelEncoder()\nenc.fit(df_trn.word)\ndf_trn=shuffle(df_trn)\ndf_val=shuffle(df_val)\ny_trn=enc.transform(df_trn.word)\ny_val=enc.transform(df_val.word)\nX_trn=df_trn.drawing.values\nX_val=df_val.drawing.values","1645ff92":"bs=256\nnum_workers=4 # Apprently 2 cpus per kaggle node\nBASE_SIZE=256\nsz=128\narch=resnet18","a974952b":"channel_mean,channel_var=get_stats(X_trn,X_val,y_trn,y_val,50)","1e5cb8e1":"channel_mean,channel_var","754f5050":"aug_tfms = transforms_side_on+[RandomRotate(10)]\ntfms = tfms_from_stats((channel_mean, channel_var), sz=sz, aug_tfms=aug_tfms)","46abbd19":"datasets=DoodleImageData.get_ds(DoodleDataset,(X_trn,y_trn),(X_val, y_val),tfms=tfms,sz=sz,test=df_test.drawing.values)\ndatasets_warmup=DoodleImageData.get_ds(DoodleDataset,(X_trn[:200*bs],y_trn[:200*bs]),(X_val, y_val),tfms=tfms,sz=sz,test=df_test.drawing.values)","d138aede":"doodleplot=DoodleImageData('.',datasets_warmup,8,num_workers,enc.classes_)\ndoodle_warmup=DoodleImageData('.',datasets_warmup,bs,num_workers,enc.classes_)\ndoodleimageloader=DoodleImageData('.',datasets,bs,num_workers,enc.classes_)","99d55d39":"idx=0\nbatches = [next(iter(doodleplot.trn_dl)) for i in range(8)]\nfig, axes = plt.subplots(1,8, figsize=(18,9))\nfor i,(x,y) in enumerate(batches):\n    show_img(x,idx, ax=axes.flat[i])\n    axes.flat[i].set_title(enc.inverse_transform(np.where(y==1)[1][idx]))","784b07e7":"idx=2\nbatches = [next(iter(doodleplot.aug_dl)) for i in range(8)]\nfig, axes = plt.subplots(1,8, figsize=(18,9))\nfor i,(x,y) in enumerate(batches):\n    show_img(x,idx, ax=axes.flat[i])\n    axes.flat[i].set_title(enc.inverse_transform(np.where(y==1)[1][idx]))","eecb35bd":"learn=ConvLearner.pretrained(arch,doodle_warmup,metrics=[acc_metric,calc_apk])\nlearn.crit=FocalLoss()","8393b875":"learn.lr_find(end_lr=1000,start_lr=0.0001)","2344c265":"learn.sched.plot(n_skip_end=1)","29ad427b":"lr=10\nlearn.fit(lr,1,cycle_len=1,use_clr_beta = (10,10,0.95,0.85))","5ce9ca47":"learn.sched.plot_loss()\nlearn.sched.plot_lr()","aeafd5d1":"learn.save('freeze')","09302fb0":"learn.load('freeze')","2b1f1885":"learn.unfreeze()","de3558ca":"learn.lr_find(end_lr=10000,start_lr=0.0001)\nlearn.sched.plot(n_skip_end=1)","80413968":"lr=100\nlrs=[lr\/9,lr\/3,lr]\nlearn.set_data(doodleimageloader)\nlearn.load('freeze')\nlearn.unfreeze()\nlearn.fit(lrs,1,cycle_len=1,use_clr_beta = (10,10,0.95,0.85))","6c22ed63":"learn.sched.plot_loss()\n\nlearn.sched.plot_lr()","c34179d0":"learn.save('unfreeze')\n","bafe38de":"learn.load('unfreeze')","b568c7a7":"log_preds_test,y_test=learn.TTA(is_test=True,n_aug=16)","9fadee64":"mean_test_preds=np.mean(np.exp(log_preds_test),0)\nmax_test_preds=np.max(np.exp(log_preds_test),0)\n","12f2eccc":"def prepare_submission(preds,fname):\n    preds_transformed=enc.inverse_transform(top_k_preds(preds,3))\n    preds_joined=[' '.join(words) for words in preds_transformed]\n    df_test['word']=preds_joined\n    df_test.loc[:,['key_id','word']].to_csv(fname,index=None)","bea94618":"prepare_submission(mean_test_preds,f'submission_mean.csv')\nprepare_submission(max_test_preds,f'submission_max.csv')\n","b8ef0f86":"!ls","ebd4cfca":"!rm fastai","0656b05a":"!rm -r fastai_lib\/","d345889d":"# Load Datasets in fastai format","a221e311":"## Read in image from raw stroke [(again thanks @Beluga)](https:\/\/www.kaggle.com\/gaborfodor\/greyscale-mobilenet-lb-0-892\/notebook) \nThis version color codes the strokes from red to green in order, and the blue channel is the stroke velocity.","b27a76d6":"## Define Dataset and Dataloader using the fastai library data format","fce4597a":"### Predit the validation data using TTA\nHere for every image we want to predict on, n_augs images are augmented form the original image.\nWe can then compare the predictions on for example the image and the image flipped \/ roated \/ slightly different crop\/ lighting\/stretched etc. \nFor now only the diherdral and rotations are used. THis gives a nice extra percent or two when compared to the auc above after training where not TTA is used. \nI also test if mean or max is better to use on the image and its augments but it can't conclude anything yet.\n","a411b4a7":"### load in test data","ff7217cd":"## For now the layers are frozen except the last one to not unlearn all the tuned weights\n","a46b32dc":"## Logarithmic sweep to check for a good learning rate","9711ff5a":"The learning rate from the layers is the optimal learning rate from the find_lr method. We also decrease the learning rate for earlier layers so all the learned features from the resnet18 are not unlearned.","b6943d69":"# Preprocessing","b8e65a13":"This is the learner object, it has all the fancy learning rate finding and cyclicle learning rate with momentum methods.","805be3f9":"## Unfreeze network and check again if lr is appropriate","a3f302d5":"#### Some images,time information is encoded in the brightness\nSome of these drawings are absolutely hilarious","5be95a7d":"#### An image and some augmentations,","57333a34":"I created this kernel to get to know the fastai dataformats a bit better.\nI currently get 0.87 score, which is not too high but Ithink it should be easy to improve my approach.\n\nI had to fix a tiny bug in the lr_find2 so I'll import my hotfix of fastai. The problem was that some augmentations changed the shape of an image if it only has a single channel.\nFor now the runtime of 9hrs on kernels is the main bottleneck and im not sure how to fix this,\nI can currently train 12 cycles with 5k images per category. If anyone has Ideas for improvements go ahead.\n\nMy next Ideas would be trying some different augmentations and image sizes.\nAlso an Idea would be tuning the learning hyperparameters for the 1 cycle learning policy as described here:\n    \n1. [blog post by Sylvain Gugger summarizing the following papers](https:\/\/sgugger.github.io\/the-1cycle-policy.html#the-1cycle-policy)\n2. [original papers by leslie smith on hyperparameter tuning](https:\/\/arxiv.org\/pdf\/1803.09820.pdf) \n3. [ and Superconvergence, the 1 cycle policy learning](https:\/\/arxiv.org\/pdf\/1708.07120.pdf)\n","1dd01672":"# Imports and definition of necessary functions","3280a45d":"### Read in subset of row and categories","35a218ee":"## Defining the learn object based on the CNN objects from the fastai library","1aeab4b3":"## I add the score to the name of the file so I can later plot the leaderboard score versus my validation score\nIn the fastai course Jeremy mentions that if you have a monotonic relation between validation and LB score the way you set up your validation set matches what the test set consists of.","c6ab18f8":"An Appropriate learning rate should be on a slope but before the minimum.\n[(Great Explanation)](https:\/\/towardsdatascience.com\/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10)\nAn important thing to know is, in the original paper, the minimum is mentioned as the best value.\nTo smooth the curve a running average is used in the fastai library, so we have to take an earlier value. Something like minimum\/10 works well for me.","6dc4b767":"## Define apk Metric and FocalLoss","8f658dba":"I can currently only run about 8k samples with the 9h time limit on kaggle, a good size for the validation set seems to be 100 samples per category from what a read of other kernels and the discussion. I will also disregard any unregognized and later see if it makes a difference. In some cases there were class impurities where the doodle was missclassified, I hope those would drop out for regognized=True","6380cc0b":"\n## Ordinal encoding of labels and splitting into train and validation set\nAlso shuffle samples, apparently sklearn.utils.shuffle is the fastest method [ apparently sklearn.utils.shuffle is the fastest method](https:\/\/stackoverflow.com\/questions\/29576430\/shuffle-dataframe-rows)","43ee0f2a":"## Learning rate for earlier layers is supposed to be lower","09ff3806":"# Fastai conform dataset with resnet18","a1ba5c03":"# Start learning stuff!","b3f57086":"## Fill datasets and corresponding dataloader and model architecture (Resnet18)\nThe Image statistics of the doodle will be wildly different from imagenet, so I load in N batches of doodles and calculate channel mean and variance for proper normalization","64748763":"### Thanks to  [Belugas Kernel](https:\/\/www.kaggle.com\/gaborfodor\/greyscale-mobilenet-lb-0-892\/notebook)"}}