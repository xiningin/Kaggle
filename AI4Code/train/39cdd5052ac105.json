{"cell_type":{"17ecea83":"code","d398f450":"code","7437472f":"code","2e195fa9":"code","cc240c92":"code","a570e3df":"code","53a9a4d4":"code","c8ebeace":"code","74cdc384":"code","3e63876b":"code","b8e612f9":"code","6ea8dbca":"code","f5d90842":"code","a694c040":"code","127e1df1":"code","48095dc6":"code","f5a9e4eb":"code","584f7399":"code","4f9edc60":"code","3ea46b8f":"code","7adf2ff9":"code","d412cec4":"code","d2c06d72":"code","1e29af49":"code","358324db":"code","bc23d3ab":"markdown","108096b2":"markdown","8003d9ae":"markdown","49717ee9":"markdown","e23186fc":"markdown","95047900":"markdown","e8bc0182":"markdown","4f282d1f":"markdown","01e89f5e":"markdown"},"source":{"17ecea83":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf","d398f450":"classes = 10\nrows, cols, channel = 28, 28, 1\ninput_shape = (rows, cols, channel)","7437472f":"(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nx_train, x_test = x_train\/255.0, x_test\/255.0\n\nx_train = x_train.reshape(x_train.shape[0], *input_shape)\nx_test = x_test.reshape(x_test.shape[0], *input_shape)","2e195fa9":"from tensorflow.keras import Model\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\nfrom tensorflow.keras.models import Sequential","cc240c92":"class LeNet(Model):\n    def __init__(self, classes):\n        super(LeNet, self).__init__()\n        self.conv1 = Conv2D(6, kernel_size = (5, 5), padding = 'SAME', activation='relu')\n        self.conv2 = Conv2D(16, kernel_size = (5,5), activation = 'relu')\n        self.pool = MaxPooling2D(pool_size = (2,2))\n        self.flatten = Flatten()\n        self.dense1 = Dense(120, activation = 'relu')\n        self.dense2 = Dense(84, activation = 'relu')\n        self.dense3 = Dense(classes, activation='softmax')\n    \n    def call(self, inputs):\n        x = self.pool(self.conv1(inputs))\n        x = self.pool(self.conv2(x))\n        x = self.flatten(x)\n        x = self.dense3(self.dense2(self.dense1(x)))\n        return x","a570e3df":"model = LeNet(classes)\nmodel.compile(optimizer = 'sgd', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])","53a9a4d4":"\n\n# We can call `model.summary()` only if the model was built before. \n# It is normally done automatically at the first use of the network,\n# inferring the input shapes from the samples the network is given.\n# For instance, the command below would build the network (then use it for prediction):\n_ = model.predict(x_test[:10])\n\n# Method to visualize the architecture of the network:\nmodel.summary()","c8ebeace":"callbacks = [\n    # Callback to interrupt the training if the validation loss (`val_loss`) stops improving for over 3 epochs:\n    tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_loss'),\n    # Callback to log the graph, losses and metrics into TensorBoard (saving log files in `.\/logs` directory):\n    tf.keras.callbacks.TensorBoard(log_dir='.\/logs', histogram_freq=1, write_graph=True)]","74cdc384":"history = model.fit(x_train, y_train,\n                    batch_size=32, epochs=80, validation_data=(x_test, y_test), \n                    verbose=1,\n                    callbacks=callbacks)","3e63876b":"import matplotlib.pyplot as plt","b8e612f9":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('MODEL ACCURACY')\nplt.ylabel('Accuracy')\nplt.xlabel('No. of epochs')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","6ea8dbca":"def lenet(name = 'lenet'):\n    model = Sequential(name = name)\n    model.add(Conv2D(6, kernel_size = (5,5), padding = 'same', activation = 'relu', input_shape = input_shape))\n    model.add(MaxPooling2D(pool_size = (2,2)))\n    model.add(Conv2D(16, kernel_size = (5,5), activation = 'relu'))\n    model.add(MaxPooling2D(pool_size = (2,2)))\n    \n    model.add(Flatten())\n    model.add(Dense(120, activation = 'relu'))\n    model.add(Dense(84, activation = 'relu'))\n    model.add(Dense(classes, activation = 'softmax'))\n    return model","f5d90842":"from tensorflow.keras import optimizers\n\n# Setting some variables to format the logs:\nlog_begin_red, log_begin_blue, log_begin_green = '\\033[91m', '\\n\\033[94m', '\\033[92m'\nlog_begin_bold, log_begin_underline = '\\033[1m', '\\033[4m'\nlog_end_format = '\\033[0m'","a694c040":"optimizers_list = {\n    'sgd' : optimizers.SGD(),\n    'momentum' : optimizers.SGD(momentum=0.9),\n    'nag': optimizers.SGD(momentum=0.9, nesterov=True),\n    'adagrad': optimizers.Adagrad(),\n    'adadelta': optimizers.Adadelta(),\n    'rmsprop': optimizers.RMSprop(),\n    'adam': optimizers.Adam()\n}","127e1df1":"history_per_opti = dict()","48095dc6":"print(\"Experiment: {0}start{1} (training logs = off)\".format(log_begin_red, log_end_format))\nfor opti in optimizers_list:\n    tf.random.set_seed (42)\n    np.random.seed(42)\n    \n    model = lenet('lenet_{}'.format(opti))\n    optimizer = optimizers_list[opti]\n    model.compile(optimizer = optimizer, loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    print(\"\\t> Training with {0}: {1}start{2}\".format(\n        opti, log_begin_red, log_end_format))\n    history = model.fit(x_train, y_train, batch_size = 32, \n                       epochs = 20, validation_data = (x_test, y_test),\n                       verbose = 1)\n    history_per_opti[opti] = history\n    print('\\t> Training with {0}: {1}done{2}.'.format(\n        opti, log_begin_green, log_end_format))\nprint(\"Experiment: {0}done{1}\".format(log_begin_green, log_end_format))","f5a9e4eb":"import matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport seaborn as sns\n","584f7399":"fig, ax = plt.subplots(2, 2, figsize=(10,10), sharex='col')\nax[0, 0].set_title(\"loss\")\nax[0, 1].set_title(\"val-loss\")\nax[1, 0].set_title(\"accuracy\")\nax[1, 1].set_title(\"val-accuracy\")\n\nlines, labels = [], []\nfor optimizer_name in history_per_opti:\n    history = history_per_opti[optimizer_name]\n    ax[0, 0].plot(history.history['loss'])\n    ax[0, 1].plot(history.history['val_loss'])\n    ax[1, 0].plot(history.history['accuracy'])\n    line = ax[1, 1].plot(history.history['val_accuracy'])\n    lines.append(line[0])\n    labels.append(optimizer_name)\n\nfig.legend(lines,labels, loc='center right', borderaxespad=0.1)\nplt.subplots_adjust(right=0.85)","4f9edc60":"x_train, y_train = x_train[:200], y_train[:200]\nprint('Training data: {}'.format(x_train.shape))\nprint('Testing data: {}'.format(x_test.shape))","3ea46b8f":"from tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import (Input, Activation, Dense, Flatten, Conv2D, \n                                     MaxPooling2D, Dropout, BatchNormalization)","7adf2ff9":"def lenet5_reg(name='lenet', input_shape = input_shape, use_dropout = False,\n              use_batchnorm = False, regularizer = None):\n    layers = []\n    layers += [Conv2D(6, kernel_size = (5,5), padding = 'same', input_shape = \n                     input_shape, kernel_regularizer = regularizer)]\n    if use_batchnorm:\n        layers +=[BatchNormalization()]\n    layers+=[Activation('relu'),\n            MaxPooling2D(pool_size = (2,2))]\n    if use_dropout:\n        layers +=[Dropout(0.25)]\n        \n    layers += [Conv2D(16, kernel_size = (5,5),\n                      kernel_regularizer = regularizer)]\n    if use_batchnorm:\n        layers +=[BatchNormalization()]\n    layers+=[Activation('relu'),\n            MaxPooling2D(pool_size = (2,2))]\n    if use_dropout:\n        layers +=[Dropout(0.25)]\n    \n    layers +=[Flatten()]\n    \n    layers +=[Dense(120, kernel_regularizer = regularizer)]\n    if use_batchnorm:\n        layers +=[BatchNormalization()]\n    layers+=[Activation('relu')]\n    if use_dropout:\n        layers +=[Dropout(0.25)]\n    \n    layers += [Dense(84, kernel_regularizer = regularizer)]\n    layers +=[Activation('relu')]\n    layers += [Dense(classes, activation = 'softmax')]\n    model = Sequential(layers, name = name)\n    return model\n    \n        \n    \n        ","d412cec4":"configurations = {\n    'none':         {'use_dropout': False, 'use_batchnorm': False, 'regularizer': None},\n    'l1':           {'use_dropout': False, 'use_batchnorm': False, 'regularizer': tf.keras.regularizers.l1(0.01)},\n    'l2':           {'use_dropout': False, 'use_batchnorm': False, 'regularizer': tf.keras.regularizers.l2(0.01)},\n    'dropout':      {'use_dropout': True,  'use_batchnorm': False, 'regularizer': None},\n    'bn':           {'use_dropout': False, 'use_batchnorm': True,  'regularizer': None}\n}","d2c06d72":"history_per_instance = dict()\nrandom_seed = 42\nprint(\"Experiment: {0}start{1} (training logs = off)\".format(log_begin_red, log_end_format))\nfor config_name in configurations:\n    tf.random.set_seed(random_seed)\n    np.random.seed(random_seed)\n    \n    model = lenet5_reg('lenet_{}'.format(config_name), **configurations[config_name])\n    model.compile(optimizer = 'sgd', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n    print('\\t> Training with {0}: {1}start{2}'.format(\n    config_name, log_begin_red, log_end_format))\n    history = model.fit(x_train, y_train, batch_size = 32, epochs = 150, \n                       validation_data = (x_test, y_test), verbose = 0)\n    history_per_instance[config_name] = history\n    print('\\t> Training with {0}: {1}done{2}.'.format(\n        config_name, log_begin_green, log_end_format))\nprint(\"Experiment: {0}done{1}\".format(log_begin_green, log_end_format))","1e29af49":"fig, ax = plt.subplots(2, 2, figsize=(10,10), sharex='col') # add parameter `sharey='row'` for a more direct comparison\nax[0, 0].set_title(\"loss\")\nax[0, 1].set_title(\"val-loss\")\nax[1, 0].set_title(\"accuracy\")\nax[1, 1].set_title(\"val-accuracy\")\n\nlines, labels = [], []\nfor config_name in history_per_instance:\n    history = history_per_instance[config_name]\n    ax[0, 0].plot(history.history['loss'])\n    ax[0, 1].plot(history.history['val_loss'])\n    ax[1, 0].plot(history.history['accuracy'])\n    line = ax[1, 1].plot(history.history['val_accuracy'])\n    lines.append(line[0])\n    labels.append(config_name)\n\nfig.legend(lines,labels, loc='center right', borderaxespad=0.1)\nplt.subplots_adjust(right=0.84)","358324db":"for config_name in history_per_instance:\n    best_val_acc = max(history_per_instance[config_name].history['val_accuracy']) * 100\n    print('Max val-accuracy for model \"{}\": {:2.2f}%'.format(config_name, best_val_acc))","bc23d3ab":"# Intoduction:\n\nThis notebook is a starter for LENET model along with the trials with different optimizers. We will be using MNIST digit dataset for the test.","108096b2":"# Results with different Opimizers.","8003d9ae":"# Conclusion:\n\nBy the notebook we can thus conclude that even with limited data we can get a good amount of accuracy by using different regulaizaiton techniques. We need to prevent data from overfitting and thus imporve the accuracy by trying with different optimizers and regulaization techniques.\n\nDo check my other notebooks. Give some valuable feedback and comments.","49717ee9":"# Optimizers:\n\n1. SGD : Simple Gradient Descent can be summarized with single equation as:\n![image.png](attachment:image.png)\nWith its value, *epsilon* , the learning rate hyperparameter accentuates or attenuates how the network's parameters are updated with regard to the gradient of the loss at every training iteration.\n\n2. Momentum: First suggested by Boris Polyak (in Some methods of speeding up the convergence of iteration methods, Elsevier, 1964), the Momentum algorithm is based on SGD and inspired by the physics notion of momentum\u2014 as long as an object is moving downhill, its speed will increase each step.\n\n3. nag : The Nesterov accelerated gradient (NAG or Nesterov Momentum, a related course is Introductory Lectures on Convex Programming Volume I: Basic course by Yurii Nesterov, Springer Science and Business Media). Back in the 1980s, Yurii Nesterov's idea was to give the optimizer the possibility to have a look at the slope ahead, so that it knows it should slow down if the slope starts going up.\n\n4. ADA family: Adagrad, Adadelta, Adam, and so on are several iterations and variations around the idea of adapting the learning rate depending on the sensitivity and\/ or activation frequency of each neuron. Developed first by John Duchi and others (in Adaptive Subgradient Methods for Online Learning and Stochastic Optimization, Journal of Machine Learning Research, 2011), the Adagrad optimizer (for adaptive gradients) uses a neat formula (which we won't expand here, though we invite you to search for it), to automatically decrease the learning rate faster for parameters linked to commonly found features, and slower for infrequent ones.\n\n5. RMSPROP : rmsprop algorithm, that\u2019s used for full-batch optimization. Rprop tries to resolve the problem that gradients may vary widely in magnitudes. Some gradients may be tiny and others may be huge, which result in very difficult problem \u2014 trying to find a single global learning rate for the algorithm. If we use full-batch learning we can cope with this problem by only using the sign of the gradient.","e23186fc":"# LENET Model\n\nIt consist of two CNN layers along with MaxPool layer with each CNN. The output is then passed through a Flatten layer and then through 3 Fully connected Layers.","95047900":"# Regularization\n\nWhile creating a network the aim is not only to minimize the losses but also look that the model doesn't overfit the training data and generalises well with different variations and inputs. We want that our model should neither be too shallow to underfit nor too complex and overfit. Regularization is the process of refining the optimization phase to avoid overfitting.\n\n\n","e8bc0182":"**Results**","4f282d1f":"We will take only 200 entries thus limited data and see advantages of regularization.","01e89f5e":"# Types of Regulatization Techniques:\n\n1. L1 and L2 Regularization: In this we modify the loss function and add a parameter and force the network to not only optimize the task objective but also contrain the values its parametes can take\n\nL1: It minimses the sum of its absolute parameter value. In the larger values are not penalized by squaring thus it shrinks the parameters linked to less important features to 0 and prevent overfitting.\n\nL2: It minimises the squared parametes values and thus encourages to keep parameter values low and prevents from developing large values in predictions.\n\n2. Dropout: It consists of randomly dropping some features or disconnecting some neuron of target layers at every training iteration. It makes the network to figure out the significant features in order to reach the exact prediction.\n\n3. Batch Normalization: This operation takes batched results of preceding layers and normalizes them. Data will never be normalized in the same way as before thus making the network learn how to deal with these fluctuations and making it more robust and generic."}}