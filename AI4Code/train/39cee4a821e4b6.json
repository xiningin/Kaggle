{"cell_type":{"3ff0a384":"code","a2603040":"code","9714f8a8":"code","1990f8ec":"code","3ea7b974":"code","b9a9408f":"code","33d4f923":"code","48500759":"code","7cb0b2fd":"code","90e8921a":"code","29a6e46e":"code","55965d16":"code","7e6b7fcf":"code","54398e61":"code","8c05a1ae":"code","b1cf0c7b":"code","931e31a6":"code","7af69357":"code","abeea603":"code","f5cc1dd9":"code","42812fbc":"markdown","3713cb1c":"markdown","a9c83f34":"markdown","a640497f":"markdown","d4e159c7":"markdown","8e058f47":"markdown","1c2e8cd6":"markdown","6046be8a":"markdown","72aa3bb2":"markdown","011c0507":"markdown","0e58e5ce":"markdown","fb8a7053":"markdown","169b9ab6":"markdown","f323f993":"markdown","ca1b128b":"markdown","c5631130":"markdown","ce928232":"markdown","fcf06c88":"markdown","f743715b":"markdown","cf16f80f":"markdown","d826cb3b":"markdown"},"source":{"3ff0a384":"import numpy as np\nimport pandas as pd \n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(font = 'Serif', style = 'white', rc = {'axes.facecolor':'#f1f1f1', 'figure.facecolor':'#f1f1f1'})\n\nimport wandb\nfrom wandb.keras import WandbCallback\n\nimport warnings\nwarnings.filterwarnings('ignore')","a2603040":"# login into wandb\nwandb.login()","9714f8a8":"# Reading the data\ndf = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\ndf.head()","1990f8ec":"# Number of images\ndf.shape","3ea7b974":"dictionary = {\n    0 :'T-shirt\/top',\n    1 : 'Trouser',\n    2 : 'Pullover',\n    3 : 'Dress',\n    4 : 'Coat',\n    5 : 'Sandal',\n    6 : 'Shirt',\n    7 : 'Sneaker',\n    8 : 'Bag',\n    9 : 'Ankle boot'\n}\n\n# Getting the values from the dictionary\nclass_names = list(dictionary.values())","b9a9408f":"# Plotting  first 30 images\nplt.style.use('seaborn-dark')\nfig, ax = plt.subplots(3,10, figsize = (20,8), constrained_layout = True)\n\nfor i, axis in zip(range(0,30), ax.ravel()):\n    img = np.reshape(df.iloc[i, 1:].values, (28,28))\n    axis.imshow(img)\n    title = dictionary[df.iloc[i,0]]\n    axis.set_title(title, fontsize = 16)\n    axis.set_xticklabels(\"\")\n    axis.set_yticklabels(\"\")\n    \nfig.suptitle('First 30 images of Fashion MNIST', size = 20, y=1.05)\nfig.show()","33d4f923":"# Number of images in each class\nplt.style.use('seaborn')\nfig, ax = plt.subplots(1,1, figsize = (12,6))\nsns.countplot(x='label', data = df, ax = ax, color = 'skyblue')\n\nfor i, counter in zip(ax.patches, range(0,10)):\n        ax.text(x = i.get_x() + i.get_width()\/2, y = -600,\n                s = f\"{class_names[counter]}\",\n                ha = 'center', size = 14, rotation = 0, color = 'black')\n\nax.set_title('Label count', fontsize = 18, y = 1.05)\nax.set_xticklabels(list(range(0,10)), fontsize = 14)\nax.set_xlabel(None)\nax.set_ylabel('Count', fontsize = 14);","48500759":"# Splitting the data into training and validation sets\nX = df.iloc[:,1:].values.reshape(-1,28,28,1)\ny = to_categorical(df['label'], num_classes=10)\n\n# stratify = y, ensures that the proportion of the classes is same in train and validation split\nX_train, X_val, y_train, y_val = train_test_split(X, y, train_size = 0.8, stratify = y)","7cb0b2fd":"sweep_config = {\n    'method': 'bayes'\n}","90e8921a":"metric = {\n      'goal': 'maximize',\n      'name': 'accuracy'\n  }\n    \nsweep_config['metric'] = metric\n\nparameter_dict = { 'batch_size': {\n                        'values' : [32, 64, 128, 256]},\n                  'learning_rate': { \n                      'distribution': 'uniform',\n                      'max': 0.01,\n                      'min': 0.0001},\n                  'epochs': {\n                      'distribution': 'int_uniform',\n                      'max': 30,\n                      'min': 5 },\n                  'optimizer': {\n                      'values':['adam', 'sgd']},\n                  'fc_layer_size': {\n                      'values' : [128, 256, 512]},\n                  'dropout': {\n                      'values' : [0.3, 0.4, 0.5]}\n    }\nsweep_config['parameters'] = parameter_dict","29a6e46e":"sweep_id = wandb.sweep(sweep_config, project = 'Fashion MNIST')","55965d16":"# Defining the train function for iterations\ndef train():\n    # Initializing the wandb\n    run = wandb.init(project = 'Fashion MNIST',\n                     config = {'epochs': 10,\n                               'learning_rate': 0.001,\n                               'batch_size': 64,\n                               'optimizer':'adam',\n                               'fc_layer_size':128,\n                               'dropout':0.3\n                             })\n    config = wandb.config\n    \n    model = create_model(config.fc_layer_size, config.dropout)\n    optimizer = create_optimizer(config.optimizer, config.learning_rate)\n    \n    # Compiling the model\n    model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    # Model training\n    _ = model.fit(X_train, y_train,\n             epochs = config.epochs,\n             batch_size = config.batch_size,\n             validation_data = (X_val, y_val),\n             callbacks = [WandbCallback(data_type = 'image',\n                                       training_data = (X_val, y_val),\n                                       labels = class_names)])","7e6b7fcf":"def create_model(fc_layer_size, dropout):\n    \n    # Creating the CNN model\n    model = Sequential()\n\n    model.add(Conv2D(32, (3, 3), input_shape=(28,28,1), activation = 'relu', padding='same'))\n    model.add(MaxPool2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation = 'relu'))\n    model.add(MaxPool2D(pool_size=(2, 2)))\n    model.add(Conv2D(128, (3, 3), activation='relu'))\n    model.add(MaxPool2D(pool_size=(2, 2)))\n    \n    model.add(Dropout(dropout))\n    \n    model.add(Flatten())\n\n    model.add(Dense(fc_layer_size, activation = 'relu'))\n    model.add(Dense(10, activation = 'softmax'))\n    \n    return model","54398e61":"def create_optimizer(optimizer, learning_rate):\n    if optimizer == 'sgd':\n        optimizer = keras.optimizers.SGD(learning_rate = learning_rate)        \n    else:\n        optimizer = keras.optimizers.Adam(learning_rate = learning_rate)\n        \n    return optimizer","8c05a1ae":"wandb.agent(sweep_id = sweep_id, function = train, count = 20)","b1cf0c7b":"# Creating CNN model with the optimal hyperparameters\n\noptimal_param = {'optimizer' : 'sgd',\n                'learning_rate' : 0.00622,\n                'batch_size' : 32,\n                'dropout' : 0.3,\n                'epochs' : 30,\n                'fc_layer_size' : 256}\n\n# Initializing the wandb\nrun = wandb.init(project = 'Fashion MNIST',\n                 config = optimal_param)\n\nconfig = wandb.config\n\nmodel = create_model(config.fc_layer_size, config.dropout)\noptimizer = create_optimizer(config.optimizer, config.learning_rate)\n\n# Compiling the model\nmodel.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n\n# Model training\n_ = model.fit(X_train, y_train,\n         epochs = config.epochs,\n         batch_size = config.batch_size,\n         validation_data = (X_val, y_val),\n         callbacks = [WandbCallback(data_type = 'image',\n                                   training_data = (X_val, y_val),\n                                   labels = class_names)])","931e31a6":"hist = _.history\nacc = hist['accuracy']\nval_acc = hist['val_accuracy']\nloss = hist['loss']\nval_loss = hist['val_loss']\nepochs = list(range(1,len(acc)+1))\n\nplt.style.use('seaborn')\nfig, (ax1,ax2) = plt.subplots(1,2, figsize = (10,6), sharex = True)\n\nax1 = sns.lineplot(epochs,acc, ax = ax1)\nax1 = sns.lineplot(epochs,val_acc, ax = ax1)\nax2 = sns.lineplot(epochs,loss, ax = ax2)\nax2 = sns.lineplot(epochs,val_loss, ax = ax2)\n\nax1.set_title('Accuracy', size = 14)\n# ax1.set_title('Validation accuracy', size = 14)\nax2.set_title('Loss', size = 14)\n# ax2.set_title('Validation loss', size = 14)\n\nax1.set_xlabel('epochs', size = 12)\nax2.set_xlabel('epochs', size = 12)\nax2.legend(['Train', 'Validation'])\n\nfig.suptitle('Performance Evaluation', size = 16)\n\nplt.tight_layout()","7af69357":"# Loading the test data\ndf_test = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')\nX_test = df_test.iloc[:,1:].values.reshape(-1,28,28,1)\ny_test = to_categorical(df_test['label'], num_classes=10)","abeea603":"# Visualizing few predictions\nplt.style.use('seaborn-dark')\nfig, ax = plt.subplots(6,5, figsize = (18,14), constrained_layout = True)\n\nfor i, axis in zip(range(0,30), ax.ravel()):\n    img = np.reshape(df_test.iloc[i, 1:].values, (28,28))\n    axis.imshow(img)\n    \n    img_reshape = np.reshape(df_test.iloc[i, 1:].values, (1,28,28,1))\n    prediction = model.predict_classes(img_reshape)[0]\n    \n    actual_title = dictionary[df_test.iloc[i,0]]\n    predicted_title = dictionary[prediction]\n    \n    axis.set_title(f\"True:{actual_title}| Pred:{predicted_title}\", fontsize = 12)\n    axis.set_xticklabels(\"\")\n    axis.set_yticklabels(\"\")\n    \nfig.suptitle('Testing first 30 images of Fashion MNIST', size = 20, y=1.05)\nfig.show()","f5cc1dd9":"# Confusion matrix\ny_true = df_test.iloc[:,0]\n\nimgs = np.reshape(df_test.iloc[:, 1:].values, (-1,28,28,1))\ny_predict = model.predict_classes(imgs)\n\nconf_matrix = confusion_matrix(y_true, y_predict)\n\nplt.style.use('seaborn-dark')\nfig, axis = plt.subplots(1,1, figsize=(18,10), constrained_layout = True)\naxis = sns.heatmap(conf_matrix, annot=True, fmt = '.0f', cbar=False, cmap='Blues',\n                    linewidths=3, square=True, ax = axis, annot_kws={\"fontsize\":15})\naxis.set_title(f\"Confusion Matrics\", fontsize=20, y=1.05);\naxis.set_xlabel('Predicted', fontsize=16)\naxis.set_ylabel('Actual', fontsize=16)\naxis.set_xticklabels(class_names, fontsize=12)\naxis.set_yticklabels(class_names, fontsize=12, rotation=0);","42812fbc":"### Project link is [here](https:\/\/wandb.ai\/prathameshdinkar19\/Fashion%20MNIST\/sweeps\/5zrt4jyh?workspace=user-prathameshdinkar19)","3713cb1c":"<h2><center>Data reading and Visualization<\/center><\/h2>","a9c83f34":"> I am using Weights and Biases (W&B) for this notebook. Wandb is a developer tool for companies turn deep learning research projects into deployed software by helping teams track their models, visualize model performance and easily automate training and improving models. \n<center><img src =\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQNzY8UF90EGDfGRP50YPPdb02LEreE-I0hsYeu_s8MCzjHjtW9ox63vBveACxaGWgTHA&usqp=CAU\"><\/center>\n\n> To create a free account, you can click [here](https:\/\/wandb.ai\/site).","a640497f":"<img src =\"https:\/\/i.imgur.com\/ifKRnZK.png\">\n\n<img src = \"https:\/\/i.imgur.com\/zhueHcd.png\">\n\n<img src = \"https:\/\/i.imgur.com\/aVRCqeS.png\">","d4e159c7":"#### Observations:\n> `dropout` and `batch_size` are negetively proportional to accuracy, as they increase the accuracy drops\n> `fc_layer_size`, `epochs` and `learning_rate` are positively proportional to accuracy, as they increase the accuracy increases","8e058f47":"<h2><center>Creating training and validation data<\/center><\/h2>","1c2e8cd6":"<h2><center>Performance Evaluation<\/center><\/h2>","6046be8a":"<h2><center>If you like it, don't forget to upvote!<\/center><\/h2> ","72aa3bb2":"<h2><center>2. Initialize a sweep<\/center><\/h2>\n\n> Now, we initialize the sweep usind `wandb.sweep`, which has the information about the sweep configuration, project name, etc. It launches the sweep server. W&B hosts this central controller and coordinate between the agents that execute the sweep.","011c0507":"#### Defining the `train` function, on which the hyperparameter tuning will happen","0e58e5ce":"> There are 785 columns, 1st column is for `label` which has values ranging from 0 to 9 and remaining 784 columns have pixel values for 28x28 image (28x28=784).\n\n> There are 10 different classes of images, as following:\n\n>- 0: T-shirt\/top\n>- 1: Trouser\n>- 2: Pullover\n>- 3: Dress\n>- 4: Coat\n>- 5: Sandal\n>- 6: Shirt\n>- 7: Sneaker\n>- 8: Bag\n>- 9: Ankle boot\n\n> We have 60K of such labeled images.","fb8a7053":"`grid` : Grid search iterates over all possible combinations of parameter values.\n> Slow but ensures the optimal hyperparameters\n\n`random` : Random search chooses a random set of values on each iteration.\n> Fast but does not ensures optimal hyperparameters\n\n`bayes` : Bayesian hyperparameter search method uses a Gaussian Process to model the relationship between the parameters and the model metric and chooses parameters to optimize the probability of improvement. This strategy requires the `metric` key to be specified.\n> Faster than `grid` and ensures the optimal hyperparameters as per our strategy","169b9ab6":"<h2><center>3. Launch agent(s)<\/center><\/h2>\n\n>Run a single-line command on each machine you'd like to use to train models in the sweep. The agents ask the central sweep server what hyperparameters to try next, and then they execute the runs.","f323f993":"**Observations**:\n> The model is quite good in identifying Trousers, Sandals and Bags\n\n>The model is getting confused between Shirts and T-shirts\/top, which is fine because we also will not be able to distinguish between them just looking at the 28x28 image as both look silimar.\n\n>The next confusing item for the model is pullover and coat as both have long arms","ca1b128b":"<h2><center>Importing the libraries<\/center><\/h2>","c5631130":"<h1><center>Introduction<\/center><\/h1>\n\n> Hyperparameter tuning is choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a model argument whose value is set before the learning process begins. The key to machine learning algorithms is hyperparameter tuning.\n\n> There are many hypreparameter and setting them to the optimal value is quite difficult. So, tuning them is the last thing you want try to improve your model performance.\n\n<center><img src = \"https:\/\/78.media.tumblr.com\/tumblr_m8f78uL6uv1rrlrnjo1_500.gif\"><\/center>\n\n> Searching through high dimensional hyperparameter spaces to find the most performant model can get unwieldy very fast. W&B's hyperparameter sweeps provide an organized and efficient way to conduct a battle royale of models and pick the most \ud83c\udfaf accurate model. They enable this by automatically searching through combinations of hyperparameter values (e.g. learning rate, batch size, number of hidden layers, optimizer type) to find the most optimal values.\n\n>If you want, you can also use [keras hyperparameter tuner](https:\/\/www.tensorflow.org\/tutorials\/keras\/keras_tuner) for hyperparamer tuning, but for this notebook, I will be using the W&B sweep as it has following merits:\n\n> 1. Quick setup: Get going with just a few lines of code. You can launch a sweep across dozens of machines, and it's just as easy as starting a sweep on your laptop.\n> 2. Transparent: All the algorithms used are cited, and the code is open source.\n> 3. Powerful: Sweeps are completely customizable and configurable. ","ce928232":"### Project link is [here](https:\/\/wandb.ai\/prathameshdinkar19\/Fashion%20MNIST\/sweeps\/5zrt4jyh?workspace=user-prathameshdinkar19)","fcf06c88":"<h2><center>Model Prediction<\/center><\/h2>","f743715b":"> Each label has 6K samples and so there is no class imbalance!\n\n> Now let's go for the hyperparameter tuning with sweep.","cf16f80f":"### Optimal Hyperparameters:\n\n- Optimizer : SGD\n- Learning rate : 0.00622\n- Batch_size : 32\n- Dropout : 0.3\n- Epochs : 30\n- fc_layer_size : 256\n\n#### Train accuracy : 94.81%, Validation accuracy : 90.71%\n\n#### Parameters correlation with accuracy:\n\n<img src=\"https:\/\/i.imgur.com\/V3vFhWR.png\">","d826cb3b":"<h2><center>1. Configure your sweep<\/center><\/h2>\n\n> We have first create the sweep configuration, which has information about the hyperparameters to tune and the method for tuning them.\n\n> **Structure of the Sweep Configuration** : Sweep configurations are nested: keys can have, as their values, further keys. The top-level keys are listed and briefly described below, and then detailed in the following section, for more details you can read [this](https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration)."}}