{"cell_type":{"7d6e913e":"code","a9e275b9":"code","4b58e15b":"code","89060fef":"code","3f343f6f":"code","b92e601d":"code","6084e85f":"code","17183ed0":"code","82b69e51":"code","b90d5528":"code","720eceac":"code","f813afa9":"code","70b12eb8":"code","03c779e6":"code","cacc9943":"code","4e110534":"code","8ba2d3d0":"code","c1b0dfc0":"code","e836d825":"code","42dc4deb":"code","b53b0cf2":"code","8f0fe7d5":"code","a8edb8e5":"code","c07bddfc":"code","bd4892f4":"code","294825fc":"code","fcf17180":"code","a94721f8":"code","f0a738c3":"code","e5d44915":"code","a6c3eb9c":"code","44246639":"code","9557179c":"code","b87d2070":"code","43e394e3":"code","dcb14f43":"code","7d43a3af":"code","27a2e010":"code","a61d9956":"code","993a50a4":"code","425dbf27":"code","b39f8a9d":"markdown","e895113a":"markdown","ade34dc6":"markdown","b19e3872":"markdown","36d2c61f":"markdown","7bf9fee9":"markdown","b6373acb":"markdown","080efefa":"markdown","94503416":"markdown","dcecbed6":"markdown","2f353932":"markdown","9d401a64":"markdown","a75992a3":"markdown","c7835e7c":"markdown","36fac6d6":"markdown","f18a5dfc":"markdown","16242f77":"markdown","464f6047":"markdown","d0f1d47e":"markdown","c2b842f6":"markdown","dd7645a5":"markdown","66ba623a":"markdown","a42b8950":"markdown","59639ec5":"markdown","53e9b284":"markdown","6ea08dcb":"markdown","5549b438":"markdown","57b07bf1":"markdown","4e4d062f":"markdown","9dfb2792":"markdown","64e360e0":"markdown","fb14c772":"markdown","deb059ab":"markdown"},"source":{"7d6e913e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy.stats import norm\nfrom scipy import stats\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\n\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom math import sqrt\nfrom sklearn.metrics import r2_score","a9e275b9":"nyc_data = pd.read_csv('..\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv')","4b58e15b":"nyc_data.info()","89060fef":"nyc_data.head(10)","3f343f6f":"nyc_data.isnull().sum()","b92e601d":"plt.figure(figsize=(15,12))\nsns.scatterplot(x='room_type', y='price', data=nyc_data)\n\nplt.xlabel(\"Room Type\", size=13)\nplt.ylabel(\"Price\", size=13)\nplt.title(\"Room Type vs Price\",size=15, weight='bold')","6084e85f":"plt.figure(figsize=(20,15))\nsns.scatterplot(x=\"room_type\", y=\"price\",\n            hue=\"neighbourhood_group\", size=\"neighbourhood_group\",\n            sizes=(50, 200), palette=\"Dark2\", data=nyc_data)\n\nplt.xlabel(\"Room Type\", size=13)\nplt.ylabel(\"Price\", size=13)\nplt.title(\"Room Type vs Price vs Neighbourhood Group\",size=15, weight='bold')","17183ed0":"plt.figure(figsize=(20,15))\nsns.set_palette(\"Set1\")\n\nsns.lineplot(x='price', y='number_of_reviews', \n             data=nyc_data[nyc_data['neighbourhood_group']=='Brooklyn'],\n             label='Brooklyn')\nsns.lineplot(x='price', y='number_of_reviews', \n             data=nyc_data[nyc_data['neighbourhood_group']=='Manhattan'],\n             label='Manhattan')\nsns.lineplot(x='price', y='number_of_reviews', \n             data=nyc_data[nyc_data['neighbourhood_group']=='Queens'],\n             label='Queens')\nsns.lineplot(x='price', y='number_of_reviews', \n             data=nyc_data[nyc_data['neighbourhood_group']=='Staten Island'],\n             label='Staten Island')\nsns.lineplot(x='price', y='number_of_reviews', \n             data=nyc_data[nyc_data['neighbourhood_group']=='Bronx'],\n             label='Bronx')\nplt.xlabel(\"Price\", size=13)\nplt.ylabel(\"Number of Reviews\", size=13)\nplt.title(\"Price vs Number of Reviews vs Neighbourhood Group\",size=15, weight='bold')","82b69e51":"nyc_data['neighbourhood_group']= nyc_data['neighbourhood_group'].astype(\"category\").cat.codes\nnyc_data['neighbourhood'] = nyc_data['neighbourhood'].astype(\"category\").cat.codes\nnyc_data['room_type'] = nyc_data['room_type'].astype(\"category\").cat.codes\nnyc_data.info()","b90d5528":"plt.figure(figsize=(10,10))\nsns.distplot(nyc_data['price'], fit=norm)\nplt.title(\"Price Distribution Plot\",size=15, weight='bold')","720eceac":"nyc_data['price_log'] = np.log(nyc_data.price+1)","f813afa9":"plt.figure(figsize=(12,10))\nsns.distplot(nyc_data['price_log'], fit=norm)\nplt.title(\"Log-Price Distribution Plot\",size=15, weight='bold')","70b12eb8":"plt.figure(figsize=(7,7))\nstats.probplot(nyc_data['price_log'], plot=plt)\nplt.show()","03c779e6":"nyc_model = nyc_data.drop(columns=['name','id' ,'host_id','host_name', \n                                   'last_review','price'])\nnyc_model.isnull().sum()","cacc9943":"mean = nyc_model['reviews_per_month'].mean()\nnyc_model['reviews_per_month'].fillna(mean, inplace=True)\nnyc_model.isnull().sum()","4e110534":"plt.figure(figsize=(15,12))\npalette = sns.diverging_palette(20, 220, n=256)\ncorr=nyc_model.corr(method='pearson')\nsns.heatmap(corr, annot=True, fmt=\".2f\", cmap=palette, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}).set(ylim=(11, 0))\nplt.title(\"Correlation Matrix\",size=15, weight='bold')","8ba2d3d0":"nyc_model_x, nyc_model_y = nyc_model.iloc[:,:-1], nyc_model.iloc[:,-1]","c1b0dfc0":"f, axes = plt.subplots(5, 2, figsize=(15, 20))\nsns.residplot(nyc_model_x.iloc[:,0],nyc_model_y, lowess=True, ax=axes[0, 0], \n                          scatter_kws={'alpha': 0.5}, \n                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nsns.residplot(nyc_model_x.iloc[:,1],nyc_model_y, lowess=True, ax=axes[0, 1],\n                          scatter_kws={'alpha': 0.5}, \n                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nsns.residplot(nyc_model_x.iloc[:,2],nyc_model_y, lowess=True, ax=axes[1, 0], \n                          scatter_kws={'alpha': 0.5}, \n                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nsns.residplot(nyc_model_x.iloc[:,3],nyc_model_y, lowess=True, ax=axes[1, 1], \n                          scatter_kws={'alpha': 0.5}, \n                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nsns.residplot(nyc_model_x.iloc[:,4],nyc_model_y, lowess=True, ax=axes[2, 0], \n                          scatter_kws={'alpha': 0.5}, \n                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nsns.residplot(nyc_model_x.iloc[:,5],nyc_model_y, lowess=True, ax=axes[2, 1], \n                          scatter_kws={'alpha': 0.5}, \n                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nsns.residplot(nyc_model_x.iloc[:,6],nyc_model_y, lowess=True, ax=axes[3, 0], \n                          scatter_kws={'alpha': 0.5}, \n                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nsns.residplot(nyc_model_x.iloc[:,7],nyc_model_y, lowess=True, ax=axes[3, 1], \n                          scatter_kws={'alpha': 0.5}, \n                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nsns.residplot(nyc_model_x.iloc[:,8],nyc_model_y, lowess=True, ax=axes[4, 0], \n                          scatter_kws={'alpha': 0.5}, \n                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nsns.residplot(nyc_model_x.iloc[:,9],nyc_model_y, lowess=True, ax=axes[4, 1], \n                          scatter_kws={'alpha': 0.5}, \n                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nplt.setp(axes, yticks=[])\nplt.tight_layout()","e836d825":"#Eigen vector of a correlation matrix.\nmulticollinearity, V=np.linalg.eig(corr)\nmulticollinearity","42dc4deb":"scaler = StandardScaler()\nnyc_model_x = scaler.fit_transform(nyc_model_x)","b53b0cf2":"X_train, X_test, y_train, y_test = train_test_split(nyc_model_x, nyc_model_y, test_size=0.3,random_state=42)","8f0fe7d5":"lab_enc = preprocessing.LabelEncoder()\n\nfeature_model = ExtraTreesClassifier(n_estimators=50)\nfeature_model.fit(X_train,lab_enc.fit_transform(y_train))\n\nplt.figure(figsize=(7,7))\nfeat_importances = pd.Series(feature_model.feature_importances_, index=nyc_model.iloc[:,:-1].columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","a8edb8e5":"### Linear Regression ###\n\ndef linear_reg(input_x, input_y, cv=5):\n    ## Defining parameters\n    model_LR= LinearRegression()\n\n    parameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\n\n    ## Building Grid Search algorithm with cross-validation and Mean Squared Error score.\n\n    grid_search_LR = GridSearchCV(estimator=model_LR,  \n                         param_grid=parameters,\n                         scoring='neg_mean_squared_error',\n                         cv=cv,\n                         n_jobs=-1)\n\n    ## Lastly, finding the best parameters.\n\n    grid_search_LR.fit(input_x, input_y)\n    best_parameters_LR = grid_search_LR.best_params_  \n    best_score_LR = grid_search_LR.best_score_ \n    print(best_parameters_LR)\n    print(best_score_LR)\n\n\n# linear_reg(nyc_model_x, nyc_model_y)","c07bddfc":"### Ridge Regression ###\n\ndef ridge_reg(input_x, input_y, cv=5):\n    ## Defining parameters\n    model_Ridge= Ridge()\n\n    # prepare a range of alpha values to test\n    alphas = np.array([1,0.1,0.01,0.001,0.0001,0])\n    normalizes= ([True,False])\n\n    ## Building Grid Search algorithm with cross-validation and Mean Squared Error score.\n\n    grid_search_Ridge = GridSearchCV(estimator=model_Ridge,  \n                         param_grid=(dict(alpha=alphas, normalize= normalizes)),\n                         scoring='neg_mean_squared_error',\n                         cv=cv,\n                         n_jobs=-1)\n\n    ## Lastly, finding the best parameters.\n\n    grid_search_Ridge.fit(input_x, input_y)\n    best_parameters_Ridge = grid_search_Ridge.best_params_  \n    best_score_Ridge = grid_search_Ridge.best_score_ \n    print(best_parameters_Ridge)\n    print(best_score_Ridge)\n    \n# ridge_reg(nyc_model_x, nyc_model_y)","bd4892f4":"### Lasso Regression ###\n\ndef lasso_reg(input_x, input_y, cv=5):\n    ## Defining parameters\n    model_Lasso= Lasso()\n\n    # prepare a range of alpha values to test\n    alphas = np.array([1,0.1,0.01,0.001,0.0001,0])\n    normalizes= ([True,False])\n\n    ## Building Grid Search algorithm with cross-validation and Mean Squared Error score.\n\n    grid_search_lasso = GridSearchCV(estimator=model_Lasso,  \n                         param_grid=(dict(alpha=alphas, normalize= normalizes)),\n                         scoring='neg_mean_squared_error',\n                         cv=cv,\n                         n_jobs=-1)\n\n    ## Lastly, finding the best parameters.\n\n    grid_search_lasso.fit(input_x, input_y)\n    best_parameters_lasso = grid_search_lasso.best_params_  \n    best_score_lasso = grid_search_lasso.best_score_ \n    print(best_parameters_lasso)\n    print(best_score_lasso)\n\n# lasso_reg(nyc_model_x, nyc_model_y)","294825fc":"### ElasticNet Regression ###\n\ndef elastic_reg(input_x, input_y,cv=5):\n    ## Defining parameters\n    model_grid_Elastic= ElasticNet()\n\n    # prepare a range of alpha values to test\n    alphas = np.array([1,0.1,0.01,0.001,0.0001,0])\n    normalizes= ([True,False])\n\n    ## Building Grid Search algorithm with cross-validation and Mean Squared Error score.\n\n    grid_search_elastic = GridSearchCV(estimator=model_grid_Elastic,  \n                         param_grid=(dict(alpha=alphas, normalize= normalizes)),\n                         scoring='neg_mean_squared_error',\n                         cv=cv,\n                         n_jobs=-1)\n\n    ## Lastly, finding the best parameters.\n\n    grid_search_elastic.fit(input_x, input_y)\n    best_parameters_elastic = grid_search_elastic.best_params_  \n    best_score_elastic = grid_search_elastic.best_score_ \n    print(best_parameters_elastic)\n    print(best_score_elastic)\n\n# elastic_reg(nyc_model_x, nyc_model_y)","fcf17180":"kfold_cv=KFold(n_splits=5, random_state=42, shuffle=False)\nfor train_index, test_index in kfold_cv.split(nyc_model_x,nyc_model_y):\n    X_train, X_test = nyc_model_x[train_index], nyc_model_x[test_index]\n    y_train, y_test = nyc_model_y[train_index], nyc_model_y[test_index]","a94721f8":"Poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\nX_train = Poly.fit_transform(X_train)\nX_test = Poly.fit_transform(X_test)","f0a738c3":"##Linear Regression\nlr = LinearRegression(copy_X= True, fit_intercept = True, normalize = True)\nlr.fit(X_train, y_train)\nlr_pred= lr.predict(X_test)\n\n#Ridge Model\nridge_model = Ridge(alpha = 0.01, normalize = True)\nridge_model.fit(X_train, y_train)             \npred_ridge = ridge_model.predict(X_test) \n\n#Lasso Model\nLasso_model = Lasso(alpha = 0.001, normalize =False)\nLasso_model.fit(X_train, y_train)\npred_Lasso = Lasso_model.predict(X_test) \n\n#ElasticNet Model\nmodel_enet = ElasticNet(alpha = 0.01, normalize=False)\nmodel_enet.fit(X_train, y_train) \npred_test_enet= model_enet.predict(X_test)\n","e5d44915":"nyc_model_xx= nyc_model.drop(columns=['neighbourhood_group', 'room_type'])","a6c3eb9c":"nyc_model_xx, nyc_model_yx = nyc_model_xx.iloc[:,:-1], nyc_model_xx.iloc[:,-1]\nX_train_x, X_test_x, y_train_x, y_test_x = train_test_split(nyc_model_xx, nyc_model_yx, test_size=0.3,random_state=42)","44246639":"scaler = StandardScaler()\nnyc_model_xx = scaler.fit_transform(nyc_model_xx)","9557179c":"### Linear Regression ###\n# linear_reg(nyc_model_xx, nyc_model_yx, cv=4)","b87d2070":"### Ridge Regression ###\n# ridge_reg(nyc_model_xx, nyc_model_yx, cv=4)","43e394e3":"### Lasso Regression ###\n# lasso_reg(nyc_model_xx, nyc_model_yx, cv=4)","dcb14f43":"### ElasticNet Regression ###\n# elastic_reg(nyc_model_xx, nyc_model_yx, cv=4)","7d43a3af":"kfold_cv=KFold(n_splits=4, random_state=42, shuffle=False)\nfor train_index, test_index in kfold_cv.split(nyc_model_xx,nyc_model_yx):\n    X_train_x, X_test_x = nyc_model_xx[train_index], nyc_model_xx[test_index]\n    y_train_x, y_test_x = nyc_model_yx[train_index], nyc_model_yx[test_index]","27a2e010":"Poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\nX_train_x = Poly.fit_transform(X_train_x)\nX_test_x = Poly.fit_transform(X_test_x)","a61d9956":"###Linear Regression\nlr_x = LinearRegression(copy_X= True, fit_intercept = True, normalize = True)\nlr_x.fit(X_train_x, y_train_x)\nlr_pred_x= lr_x.predict(X_test_x)\n\n###Ridge\nridge_x = Ridge(alpha = 0.01, normalize = True)\nridge_x.fit(X_train_x, y_train_x)           \npred_ridge_x = ridge_x.predict(X_test_x) \n\n###Lasso\nLasso_x = Lasso(alpha = 0.001, normalize =False)\nLasso_x.fit(X_train_x, y_train_x)\npred_Lasso_x = Lasso_x.predict(X_test_x) \n\n##ElasticNet\nmodel_enet_x = ElasticNet(alpha = 0.01, normalize=False)\nmodel_enet_x.fit(X_train_x, y_train_x) \npred_train_enet_x= model_enet_x.predict(X_train_x)\npred_test_enet_x= model_enet_x.predict(X_test_x)\n","993a50a4":"print('-------------Lineer Regression-----------')\n\nprint('--Phase-1--')\nprint('MAE: %f'% mean_absolute_error(y_test, lr_pred))\nprint('RMSE: %f'% np.sqrt(mean_squared_error(y_test, lr_pred)))   \nprint('R2 %f' % r2_score(y_test, lr_pred))\n\nprint('--Phase-2--')\nprint('MAE: %f'% mean_absolute_error(y_test_x, lr_pred_x))\nprint('RMSE: %f'% np.sqrt(mean_squared_error(y_test_x, lr_pred_x)))   \nprint('R2 %f' % r2_score(y_test_x, lr_pred_x))\n\nprint('---------------Ridge ---------------------')\n\nprint('--Phase-1--')\nprint('MAE: %f'% mean_absolute_error(y_test, pred_ridge))\nprint('RMSE: %f'% np.sqrt(mean_squared_error(y_test, pred_ridge)))   \nprint('R2 %f' % r2_score(y_test, pred_ridge))\n\nprint('--Phase-2--')\nprint('MAE: %f'% mean_absolute_error(y_test_x, pred_ridge_x))\nprint('RMSE: %f'% np.sqrt(mean_squared_error(y_test_x, pred_ridge_x)))   \nprint('R2 %f' % r2_score(y_test_x, pred_ridge_x))\n\nprint('---------------Lasso-----------------------')\n\nprint('--Phase-1--')\nprint('MAE: %f' % mean_absolute_error(y_test, pred_Lasso))\nprint('RMSE: %f' % np.sqrt(mean_squared_error(y_test, pred_Lasso)))\nprint('R2 %f' % r2_score(y_test, pred_Lasso))\n\nprint('--Phase-2--')\nprint('MAE: %f' % mean_absolute_error(y_test_x, pred_Lasso_x))\nprint('RMSE: %f' % np.sqrt(mean_squared_error(y_test_x, pred_Lasso_x)))\nprint('R2 %f' % r2_score(y_test_x, pred_Lasso_x))\n\nprint('---------------ElasticNet-------------------')\n\nprint('--Phase-1 --')\nprint('MAE: %f' % mean_absolute_error(y_test,pred_test_enet)) #RMSE\nprint('RMSE: %f' % np.sqrt(mean_squared_error(y_test,pred_test_enet))) #RMSE\nprint('R2 %f' % r2_score(y_test, pred_test_enet))\n\nprint('--Phase-2--')\nprint('MAE: %f' % mean_absolute_error(y_test_x,pred_test_enet_x)) #RMSE\nprint('RMSE: %f' % np.sqrt(mean_squared_error(y_test_x,pred_test_enet_x))) #RMSE\nprint('R2 %f' % r2_score(y_test_x, pred_test_enet_x))","425dbf27":"fig, ((ax1, ax2, ax3, ax4), (ax5, ax6, ax7, ax8)) = plt.subplots(2, 4, figsize=(30, 20))\nfig.suptitle('True Values vs Predictions')\n\nax1.scatter(y_test, lr_pred)\nax1.set_title('Linear Regression - Phase-1')\n\nax2.scatter(y_test, pred_ridge)\nax2.set_title('Ridge - Phase-1')\n\nax3.scatter(y_test, pred_Lasso)\nax3.set_title('Lasso - Phase-1')\n\nax4.scatter(y_test, pred_test_enet)\nax4.set_title('ElasticNet - Phase-1')\n\nax5.scatter(y_test_x, lr_pred_x)\nax5.set_title('Linear Regression - Phase-2')\n\nax6.scatter(y_test_x, pred_ridge_x)\nax6.set_title('Ridge - Phase-2')\n\nax7.scatter(y_test_x, pred_Lasso_x)\nax7.set_title('Lasso - Phase-2')\n\nax8.scatter(y_test_x, pred_test_enet_x)\nax8.set_title('ElasticNet - Phase-2')\n\nfor ax in fig.get_axes():\n    ax.set(xlabel='True Values', ylabel='Predictions')\n","b39f8a9d":"Above table shows that, there are some missing data for some features. They will be detailed later. ","e895113a":"First, data content will be examined. ","ade34dc6":"Before examining ``price`` feature, categorical variables will be mapped with help of ``cat.code``. This will assist to make easier and comprehensible data analysis. ","b19e3872":"## Multicollinearity\n\nMulticollinearity will help to measure the relationship between explanatory variables in multiple regression. If there is multicollinearity occurs, these highly related input variables should be eliminated from the model.\n\nIn this kernel, multicollinearity will be control with ``Eigen vector values`` results. ","36d2c61f":"With help of log transformation, now, price feature have normal distribution. ","7bf9fee9":"## Phase 2 - Without All Features\n\nAll steps from Phase 1, will be repeated in this Phase. The difference is, ``neighbourhood_group`` and ``room_type`` features will be eliminated.","b6373acb":"The last graph is about the difference between True Values vs Prediction for Phase 1 and Phase 2. The great difference between the two phases has been seen in 'Linear Regression' and 'ElasticNet Regression' models. ","080efefa":"In below graph, the good fit indicates that normality is a reasonable approximation.","94503416":"Now it is time to prepare data for modeling. First, non-nominal data and old ``price`` feature will be eliminated.","dcecbed6":"### Polynomial Transformation","2f353932":"### Polynomial Transformation\nThe polynomial transformation will be made with a second degree which adding the square of each feature.","9d401a64":"# 3. Model Building\n\n## Phase 1 - With All Features\n\nCorrelation matrix, Residual Plots and Multicollinearity results show that underfitting occurs on the model and there is no multicollinearity on the independent variables. Avoiding underfitting will be made with ``Polynomial Transformation`` since no new features can not be added or replaced with the existing ones.  \n\nIn model building section, `Linear Regression`, `Ridge Regression`, `Lasso Regression`, and `ElasticNet Regression` models will be built. These models will be used to avoiding plain ``Linear Regression`` and show the results with a little of regularization. \n\nFirst, `GridSearchCV` algorithm will be used to find the best parameters and tuning hyperparameters for each model. In this algorithm ``5-Fold Cross Validation`` and ``Mean Squared Error Regression Loss`` metrics will be used. ","a75992a3":"Secondly, data will be split in a 70\u201330 ratio","c7835e7c":"### Model Prediction","36fac6d6":"The above distribution graph shows that there is a right-skewed distribution on ``price``. This means there is a positive skewness. Log transformation will be used to make this feature less skewed. This will help to make easier interpretation and better statistical analysis\n\nSince division by zero is a problem, ``log+1`` transformation would be better.","f18a5dfc":"None one of the eigenvalues of the correlation matrix is close to zero. It means that there is no multicollinearity exists in the data.","16242f77":"The above graph shows the feature importance of dataset. According to that, ``neighborhood group`` and ``room type`` have the lowest importance on the model. Under this result, the model building will be made in 2 phases. In the first phase, models will be built within all features and in the second phase, models will be built without ``neighborhood group`` and ``room type`` features.  ","464f6047":"### K-Fold Cross Validation\n\nBefore model building, 5-Fold Cross Validation will be implemented for validation.","d0f1d47e":"``Number of reviews`` feature has some missing data. For this feature, missing data will be replaced with mean. Since the data is more symmetric, mean replacement would be better. ","c2b842f6":"## Residual Plots\n\nResidual Plot is strong method to detect outliers, non-linear data and detecting data for regression models. The below charts show the residual plots for each feature with the ``price``. \n\nAn ideal Residual Plot, the red line would be horizontal. Based on the below charts, most features are non-linear. On the other hand, there are not many outliers in each feature. This result led to underfitting. Underfitting can occur when input features do not have a strong relationship to target variables or over-regularized. For avoiding underfitting new data features can be added or regularization weight could be reduced.\n\nIn this kernel, since the input feature data could not be increased, Regularized Linear Models will be used for regularization and polynomial transformation will be made to avoid underfitting. ","dd7645a5":"# 2. Data Exploratory Analysis\n\nThe first graph is about the relationship between ``price`` and ``room type``. The ``Shared room`` price is always lower than 2000 dollars. On the other hand, the ``private room`` and the ``entire home`` have the highest price in some. ","66ba623a":"Now it is time to build a ``feature importance`` graph. For this ``Extra Trees Classifier`` method will be used. In the below code, ``lowess=True`` makes sure the lowest regression line is drawn.","a42b8950":"## Feature Selection and GridSearch\n\nFirst, ``Standard Scaler`` technique will be used to normalize the data set. Thus, each feature has 0 mean and 1 standard deviation. ","59639ec5":"### Model Prediction","53e9b284":"The results show that all models have similar prediction results. Phase 1 and 2 have a great difference for each metric. All metric values are increased in Phase 2 it means, the prediction error value is higher in that Phase and model explainability are very low the variability of the response data around mean.\n\n* The MAE value of 0 indicates no error on the model. In other words, there is a perfect prediction. The above results show that all predictions have great error especially in phase 2. \n* RMSE gives an idea of how much error the system typically makes in its predictions. The above results show that all models with each phase have significant errors.\n* R2 represents the proportion of the variance for a dependent variable that's explained by an independent variable. The above results show that, in phase 1, 52% of data fit the regression model while in phase 2, 20% of data fit the regression model. ","6ea08dcb":"# 5. Conclusion\n\nIn this kernel, I try to make predictions with different Regression models and comparing the importance metric results. I hope it would help other people. \n\nAny comments and feedback are welcome.","5549b438":"Another graph is about ``price`` vs ``number of reviews`` based on ``neighborhood group``. It shows us the lowest prices have higher reviews than the higher prices. It shows negative correlation between ``price`` and ``number of reviews``. Also ``Manhattan``, ``Brooklyn`` and ``Queens`` areas have higher reviews than others.","57b07bf1":"Below graph shows details about ``price`` and ``room type`` based on ``neighborhood group``. The highest price of ``Private Room`` and ``Entire Home\/Apt``is in the same area which is ``Manhattan``. Also, Brooklyn has very-high prices both in ``Private Room`` and ``Entire Home\/Apt``. On the other hand,  ``shared room``'s highest price is in the Queens area.","4e4d062f":"The correlation table shows that there is no strong relationship between price and other features. This indicates no feature needed to be taken out of data. This relationship will be detailed with Residual Plots and Multicollinearity.","9dfb2792":"# 1. Introduction\n\nAirbnb is an online marketplace for arranging or offering lodging, primarily homestays, or tourism experiences since 2008. NYC is the most populous city in the United States and also one of the most popular tourism and business place in the world. \n\nAirbnb NYC 2019 data contains listing activity and metrics. In this kernel, I would like to choose the best prediction model for price. Meanwhile, price feature's relationship examines with others and some data exploratory analysis will be made. ","64e360e0":"### K-Fold Cross Validation","fb14c772":"# 4. Model Comparison\n\nIn this part, 3 metrics will be calculated for evaluating predictions.\n\n* ``Mean Absolute Error (MAE)``    shows the difference between predictions and actual values.\n\n* ``Root Mean Square Error (RMSE)`` shows how accurately the model predicts the response.\n\n*                   ``R^2``  will be calculated to find the goodness of fit measure.","deb059ab":"Now it is time to make more details about data. A correlation table will be created and the Pearson method will be used."}}