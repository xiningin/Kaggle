{"cell_type":{"737e9024":"code","86489d9a":"code","6305444f":"code","5059af14":"code","6f286073":"code","d6161f06":"code","68e7052a":"code","22d1b33e":"code","07ddaac7":"code","162adf76":"code","70afe5cb":"code","2897784c":"code","725c5a26":"code","5bc118d5":"code","2ee574f1":"code","3b4464d6":"code","54551a9f":"code","f18eb1a8":"code","97d89c56":"code","b29a53da":"code","703cac18":"code","524ba3b3":"markdown","7889a4c8":"markdown","578a0844":"markdown","82ce647e":"markdown","6537da8c":"markdown","4e9c79af":"markdown","b6dd6e07":"markdown","734f2034":"markdown","edc2ab46":"markdown","c70ed186":"markdown","8985cff5":"markdown","83cdb40f":"markdown","91538c72":"markdown"},"source":{"737e9024":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import KFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn import metrics\n\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree","86489d9a":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6305444f":"input_dir = '\/kaggle\/input\/santander-customer-transaction-prediction\/'\n\ndf_train = pd.read_csv(input_dir + 'train.csv')\ndf_train","5059af14":"df_train.groupby('target').size()","6f286073":"var_columns = [c for c in df_train.columns if c not in ['ID_code','target']]\nX = df_train.loc[:,var_columns]\ny = df_train.loc[:,'target']\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train.shape, X_valid.shape, y_train.shape, y_valid.shape","d6161f06":"model_tree = DecisionTreeClassifier(max_leaf_nodes=8, class_weight='balanced')\nmodel_tree.fit(X_train, y_train)","68e7052a":"#Create the figure\nplt.figure(figsize=(20,10))\n\n#Create the tree plot\nplot_tree(model_tree,\n           feature_names = var_columns, #Feature names\n           class_names = [\"0\",\"1\"], #Class names\n           rounded = True,\n           filled = True)\n\nplt.show()","22d1b33e":"y_train_pred = model_tree.predict(X_train)\ny_valid_pred = model_tree.predict(X_valid)","07ddaac7":"auc_train = metrics.roc_auc_score(y_train, y_train_pred)\nauc_valid = metrics.roc_auc_score(y_valid, y_valid_pred)\n\nprint(\"AUC Train = {}\\nAUC Valid = {}\".format(round(auc_train,4), round(auc_valid,4)))","162adf76":"def tree_training(max_leaf_nodes, X_train, y_train, X_valid, y_valid):\n    model_tree = DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes, class_weight='balanced')\n    model_tree.fit(X_train, y_train)\n    \n    y_train_pred = model_tree.predict(X_train)\n    y_valid_pred = model_tree.predict(X_valid)\n    \n    auc_train = metrics.roc_auc_score(y_train, y_train_pred)\n    auc_valid = metrics.roc_auc_score(y_valid, y_valid_pred)\n    \n    print(\"Nodes:{}, Train:{:.4f}, Valid:{:.4f}, Diff:{:.4f}\".format(max_leaf_nodes,\n                                                                     auc_train,\n                                                                     auc_valid,\n                                                                     auc_train-auc_valid))\n          \n\n# Run few iterations to find which max_tree_nodes works best\nfor i in range(2, 20):\n    tree_training(i, X_train, y_train, X_valid, y_valid)","70afe5cb":"kfold = KFold(5, shuffle=True, random_state=1)\n\nfor idx_train, idx_valid in kfold.split(df_train):\n    X_train = df_train.loc[idx_train, var_columns]\n    y_train = df_train.loc[idx_train, 'target']\n    \n    X_valid = df_train.loc[idx_valid, var_columns]\n    y_valid = df_train.loc[idx_valid, 'target']\n    \n    # Try 10 leaf nodes, we saw lot of leaf nodes don't increase performance\n    print(\"Iteration Starts\")\n    for i in range(2, 16):\n        tree_training(i, X_train, y_train, X_valid, y_valid)\n    \n    print(\"Iteration Ends\\n-----------------------\")","2897784c":"# CV function requires a scorer of this form\ndef cv_roc_auc_scorer(model, X, y): return metrics.roc_auc_score(y, model.predict(X))\n\n# Loop through multiple values of max_leaf_nodes to find best parameter\nfor num_leaf_node in range(2,16):\n    model_tree = DecisionTreeClassifier(max_leaf_nodes=num_leaf_node, class_weight='balanced')\n    kfold_scores = cross_validate(model_tree,\n                                  X,\n                                  y,\n                                  cv=5,\n                                  scoring=cv_roc_auc_scorer,\n                                  return_train_score=True)\n\n    # Find average train and test score\n    train_auc_avg = np.mean(kfold_scores['train_score'])\n    test_auc_avg = np.mean(kfold_scores['test_score'])\n\n    print(\"Nodes:{}, Train:{:.4f}, Valid:{:.4f}, Diff:{:.4f}\".format(num_leaf_node,\n                                                                     train_auc_avg,\n                                                                     test_auc_avg,\n                                                                     train_auc_avg-test_auc_avg))","725c5a26":"model_tree = DecisionTreeClassifier(max_leaf_nodes=8, class_weight='balanced')\nmodel_tree.fit(X_train_full, y_train_full)","5bc118d5":"plt.figure(figsize=(20,10))\n\nplot_tree(model_tree,\n          feature_names=var_columns,\n          class_names = ['0','1'],\n          rounded=True,\n          filled=True)\n\nplt.show()","2ee574f1":"y_train_full_pred = model_tree.predict(X_train_full)\n\nfpr, tpr, threshold = metrics.roc_curve(y_train_full, y_train_full_pred)\nmetrics.auc(fpr, tpr)","3b4464d6":"zeros_probs = [0 for _ in range(len(y_valid))]\n\nfpr_zeros, tpr_zeros, _ = metrics.roc_curve(y_valid, zeros_probs)\n\n# Plot the roc curve for the model\nplt.plot(fpr_zeros, tpr_zeros, linestyle='--', label='No Model')\nplt.plot(fpr, tpr, marker='.', label='Model')\n\n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n\n# Add legend\nplt.legend()\n\nplt.show()","54551a9f":"df_test = pd.read_csv(input_dir + 'test.csv')\ndf_test","f18eb1a8":"X_test = df_test.loc[:, var_columns]\ny_test_pred  = model_tree.predict(X_test)","97d89c56":"df_sample_subm = pd.read_csv(input_dir + 'sample_submission.csv')\ndf_sample_subm","b29a53da":"df_sample_subm['target'] = y_test_pred\ndf_sample_subm","703cac18":"output_dir = '\/kaggle\/working\/'\ndf_sample_subm.to_csv(output_dir + '\/01_tree_scores.csv', index=False)","524ba3b3":"## Step3: Simple decision tree\n\nLet us try to use a simple decision tree to predict the target variable.  \nAlso plot the tree to make sure it looks fine.","7889a4c8":"The performance on validation data peaks with less number of nodes. It appears that we don't need very high number of leaf nodes.  \n\nAt `6 leaf nodes`, we are getting the highest validation AUC. Performance of the model on train and validation is virtually the same.\n\n## Step5: k-fold cross validation\n\nLooking at the result, I felt the need to perform a k-fold cross validation. Let us try `5-fold cross validation`","578a0844":"## Step7: Find Predictions for Test Data and store as final excel","82ce647e":"We already know the profile of data based on the overview provide by Kaggle.  \nLet us confirm the event rate for the training data. Event rate is approx 10%","6537da8c":"Print the final tree","4e9c79af":"# Santander Customer Transaction Prediction - Decision Tree\n\nIn the Kaggle competition, the objective is to identify which customer will make a transaction in the future.\n\n**Link to the competition**: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/  \n**Type of Problem**: Classification  \n**Metric for evalution**: AOC (Area Under Curve)\n\nThis Python 3 environment comes with many helpful analytics libraries installed\nIt is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python","b6dd6e07":"## Step1: Read the datasets","734f2034":"## Step4: Iterate over number of leaf nodes\nLet us iterate through the steps to find the appropriate level of tree depth (max leaf nodes)  \nFor that, we will write all steps as a function and call that function in loop","edc2ab46":"Let us find the final AUC value on training data  \nAnd also plot the AUC curve","c70ed186":"A better way to perform 5-fold cross validation is using the sklearn function `cross_val_score`  \n\nI will iterate over the number of nodes and take average AUC for each iteration","8985cff5":"The best performance on validation set (with minimum number of trees) is for 8 nodes.  \n\n## Step6: Final Model using Trees\nThe find model has `8 leaf nodes`. Let us create that model with entire training data and look at the output.","83cdb40f":"## Step2: Split the data into training and validation data\n20% of data would be kept for validation","91538c72":"Let us look at the training and validation performance"}}