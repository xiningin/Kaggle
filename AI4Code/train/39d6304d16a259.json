{"cell_type":{"5e490565":"code","e37a2c75":"code","1396a871":"code","7ed8d074":"code","b328e173":"code","966d793b":"code","876e6a6c":"code","cc62c48e":"code","fc05b2bb":"code","95f9034a":"code","2b8fce86":"code","cb67d100":"code","1bad1c42":"code","1658fdf4":"code","08ca5087":"code","759fae80":"code","76cdb771":"code","eb7f8b03":"code","38247d04":"code","f2e105c8":"code","c4ab092e":"code","354be4f1":"code","0fd14f30":"code","120244b0":"code","fcab22d9":"code","c76d786c":"code","18200b41":"code","0d9e1e87":"markdown"},"source":{"5e490565":"import pandas as pd\nimport numpy as np\nimport nltk\nimport seaborn  as sns\nimport re\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nprint(tf.__version__)\nimport warnings\nimport tqdm\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dropout, LSTM, Dense, Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom nltk.stem import PorterStemmer\nimport matplotlib.pyplot as plt","e37a2c75":"warnings.filterwarnings('ignore')","1396a871":"train=pd.read_csv('..\/input\/fake-news\/train.csv')\ntest=pd.read_csv('..\/input\/fake-news\/test.csv')\nprint(train.shape,test.shape)","7ed8d074":"train.head()","b328e173":"test.head()","966d793b":"print(train.isnull().sum())\ntrain.dropna(inplace=True)\nprint(train.isnull().sum())\ntest.dropna(inplace=True)\nprint(test.isnull().sum())","876e6a6c":"sns.countplot(train['label'])","cc62c48e":"y_train=train['label']\nX_train=train.drop('label', axis=1)\ny_train.shape  ,X_train.shape\nX_train.reset_index(inplace=True)\n#data=pd.concat([X_train,test])","fc05b2bb":"X_train.shape","95f9034a":"#remove stopwords and apply stemming with stemming process\ncorpus=[]\ntest_corpus=[]\ndef remove_stop_words(X):\n    corpus=[]\n    ps=PorterStemmer()   \n    for i in range(len(X.index)):\n        #print(X['title'].i)\n        sent= re.sub('[^a-zA-Z]', ' ', X['title'][i])\n        sent= sent.lower()\n        sent= sent.split()\n        sent= [ps.stem(word) for word in sent if word not in set(stopwords.words('english'))]\n        sent= ' '.join(sent)\n        corpus.append(sent)\n    return corpus\n\ncorpus=remove_stop_words(X_train)\n","2b8fce86":"corpus[0:3]\nlen(corpus)","cb67d100":"#data tokenization \nvoc_size=10000\nonehot=[one_hot(word, voc_size) for word in corpus]\n#word embadding\nmax_seq_size=25\nembadding=pad_sequences(onehot,maxlen=max_seq_size,padding='pre')\nembadding\n","1bad1c42":"# to array\nX_train=np.array(embadding)\ny_train=np.array(y_train)\n#split into train test\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n\n","1658fdf4":"#Model \ndef create_model():\n    output_dim=45\n    model=Sequential()\n    model.add(Embedding(voc_size,output_dim, input_length=max_seq_size))\n    model.add(Dropout(0.25))\n    model.add(LSTM(200))\n    #model.add(LSTM(100))\n    model.add(Dense(1,activation='sigmoid'))\n    return model","08ca5087":"model=create_model()","759fae80":"model.compile(optimizer=tf.keras.optimizers.Adam(\n    learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n    name='Adam'), loss=tf.keras.losses.BinaryCrossentropy(),metrics=['accuracy'])\nmodel.summary()","76cdb771":"history=model.fit(X_train,y_train, epochs=50, validation_data=(X_test, y_test),batch_size=16)","eb7f8b03":"model.save('fake_news_model.h5')","38247d04":"#plot learning curves\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model error')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n    \n    ","f2e105c8":"pred=model.predict_classes(y_test)\npred","c4ab092e":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nprint(f\"Accuracy \\n {accuracy_score(pred,y_test)}\")\nprint(f\"Confusion matrix = \\n {confusion_matrix(pred,y_test)}\")\nprint(f\"Accuracy\\n {classification_report(pred,y_test)}\")","354be4f1":"from keras.models import load_model\nclassifier=load_model(\".\/fake_news_model.h5\")","0fd14f30":"def test_model(dataset):\n    corpus1=[]\n    \n    #corpus1=remove_stop_words(dataset)\n    ps=PorterStemmer()   \n    for i in range(len(dataset.index)):\n        #print(len(dataset.index))\n        sent= re.sub('[^a-zA-Z]', ' ', dataset[i])\n        sent= sent.lower()\n        sent= sent.split()\n        sent= [ps.stem(word) for word in sent if word not in set(stopwords.words('english'))]\n        sent= ' '.join(sent)\n        corpus1.append(sent)\n    voc_size1=10000\n    onehot1=[one_hot(word, voc_size1) for word in corpus1]\n    #word embadding\n    max_seq_size=25\n    embadding=pad_sequences(onehot1,maxlen=max_seq_size,padding='pre')\n    embadding1\n    X_test=np.array(embadding1)\n    pred1=classifier.predict_classes(X_test)\n    return pred1\n","120244b0":"pred1=test_model(test)\npred1","fcab22d9":"range(len(test.index))","c76d786c":"def test_model(dataset):\n    corpus1=[]\n    \n    #corpus1=remove_stop_words(dataset)\n    ps=PorterStemmer()   \n    for i in range(len(dataset.index)):\n        #print(len(dataset.index))\n        sent= re.sub('[^a-zA-Z]', ' ', dataset[i])\n        sent= sent.lower()\n        sent= sent.split()\n        sent= [ps.stem(word) for word in sent if word not in set(stopwords.words('english'))]\n        sent= ' '.join(sent)\n        corpus1.append(sent)\n    voc_size1=10000\n    onehot1=[one_hot(word, voc_size1) for word in corpus1]\n    #word embadding\n    max_seq_size=25\n    embadding=pad_sequences(onehot1,maxlen=max_seq_size,padding='pre')\n    embadding1\n    X_test=np.array(embadding1)\n    pred1=classifier.predict_classes(X_test)\n    if pred1==1:\n        return \"Message is real\"\n          \n    return \"Message is fake\"","18200b41":"pred1=test_model1(test['title'])\npred1","0d9e1e87":"# #EDA:"}}