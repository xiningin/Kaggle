{"cell_type":{"acd54d3d":"code","5702ca39":"code","0776a6f2":"code","330e77f1":"code","2acdbcae":"code","0d1cbc6a":"code","2932d877":"code","55f6f178":"code","fb3c5575":"code","383950f6":"code","511ce680":"code","812a121e":"code","bfc34865":"code","239e466f":"code","74d5176e":"code","c0cd305e":"code","4c37f58e":"markdown","fca57de2":"markdown","66def48f":"markdown","5c43b1f6":"markdown","0884f192":"markdown","6fa38da8":"markdown","41b8c709":"markdown","5233ecba":"markdown","b1a74c65":"markdown","9081b727":"markdown","479e4c16":"markdown","f99174a1":"markdown","67e11f42":"markdown","55c78b85":"markdown","c08395a4":"markdown"},"source":{"acd54d3d":"import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_boston\nimport plotly.express as px","5702ca39":"# Load the data and convert it to a DataFrame\ndata = load_boston()\ndf = pd.DataFrame(data['data'],columns=data['feature_names'])\ndf.insert(13,'Y',data['target'])\ndf.head(5)","0776a6f2":"X = df[df.columns[:-1]]\nY = df['Y']","330e77f1":"X_norm = (X - X.min()) \/ (X.max() - X.min())\nX = X_norm","2acdbcae":"theta = np.zeros(X.shape[1])","0d1cbc6a":"def cost_function(X,Y,theta):\n    m = len(Y)\n    prediction = np.dot(X,theta.T)\n    return (1\/(m)) * np.sum((prediction - Y) ** 2)","2932d877":"# To test that our cost function is working correctly\nfrom sklearn.metrics import mean_squared_error","55f6f178":"cost_function(X,Y,theta)","fb3c5575":"mean_squared_error(np.dot(X,theta.T),Y)","383950f6":"def batch_gradient_descent(X,Y,theta,alpha,iters):\n    cost_history = [0] * iters\n    #initalize our cost history list to store the cost function on every iteration\n    for i in range(iters):\n        prediction = np.dot(X,theta.T)\n        \n        theta = theta - (alpha\/len(Y)) * np.dot(prediction - Y,X)\n        cost_history[i] = cost_function(X,Y,theta)\n    return theta,cost_history","511ce680":"%%time\nbatch_theta,batch_history = batch_gradient_descent(X,Y,theta,0.05,5000)","812a121e":"cost_function(X,Y,batch_theta)","bfc34865":"fig = px.line(batch_history,x=range(5000),y=batch_history,labels={'x':'no. of iterations','y':'cost function'})\nfig.show()","239e466f":"def mini_batch_gradient_descent(X,Y,theta,alpha,iters,batch_size=10):\n    for i in range(iters):\n        cost_history = [0] * iters\n        for j in range(0,len(Y),batch_size):\n            theta = theta - (alpha\/batch_size) * (np.dot(np.dot(X,theta.T) - Y,X))\n            cost_history[i] = cost_function(X,Y,theta)\n        return theta,cost_history","74d5176e":"%%time\nmini_batch_theta,mini_batch_history = mini_batch_gradient_descent(X,Y,theta,0.05,3000)","c0cd305e":"cost_function(X,Y,mini_batch_theta)","4c37f58e":"Ok so now that we have scaled our features,we can start by initialising our coefficients. These are the parameters of the model and the actual values we are trying to optimize. For gradient descent, the coefficients are usually randomly initialized. As this is just an example, I am going to set them all to zero.","fca57de2":"## Mini Batch Gradient Descent.\n\nNow, batch gradient descent is great, but like I said before, it can be awfully slow on large datasets. That is where mini batch gradient descent comes in! Instead of taking **All** the examples into account, it takes it the examples in small bathes,hence the name. This variable, `batch_size` is a downside of mini batch gradient descent as the user has to find the optimal batch size. However, the speed is incredible and most of the time it tends to do better than gradient descent on large datasets. Lets see the pseudo-code\/algorithm for mini batch gradient descent:\n![](https:\/\/i.stack.imgur.com\/h407Z.jpg)","66def48f":"As you can see, that is a huge reduction in our loss function.","5c43b1f6":"## Cost Function + Linear Regression\n\nA cost function, or a loss function, bascially calculates the 'cost' or loss of your model. There are many out there, but a common one is Mean Squared Error, or MSE. We also need a hypothesis to generate predictions. I will be using Linear Regression for this. The formulas of these can be seen below:\n![](https:\/\/humanunsupervised.github.io\/humanunsupervised.com\/topics\/images\/lesson1\/21.png)\n","0884f192":"## Batch Gradient Descent\n\nThis is probably the first optimization algorithm you will encounter and is a very powerful one. One of the problems with it is that it takes into account **ALL** the training examples, so it can be somewhat slow. Nevertheless, it is a very useful algorithm to know.\nJust to clear some things up about terminology:\n\n`alpha`:  refers to the learning rate of the algorithm. In other words, if we imagine gradient descent as an algorithm that iteratively takes steps in the best possible direction to the optima, this would control the size of the steps.\n\n`iters`:  refers to the amount of iterations that you want gradient descent to run for.","6fa38da8":"## Gradient Descent\n\nOk great, so now that we know our cost function is working properly, let us move to the big guns .. Gradient Descent!\nBut wait.. What is Gradient Descent? Good question, amigo. Gradient Descent is an optimization algorithm to find the minimum of a function(in our case, that being the cost function). It operates by iterativelty taking small steps it the negative direction of the slope. The algorithm can be given as the following:\n![](https:\/\/i.stack.imgur.com\/3MhPr.png)\n\nIf you want to get a mathematical and in-depth understanding of exactly what it does, you should definitely check out Andrew Ng's Coursera course on Machine Learning. He is amazing at explaining complex topics in a simple way.\n\nNow, there are types of Gradient Descent:\n\n1. Batch Gradient Descent\n2. Mini Batch Gradient Descent\n3. Stochastic Gradient Descent\n\nI will be going over the first two. So, let us begin!","41b8c709":"# Feature Scaling\n\nin order for gradient descent and pca to work, feature scaling is required. What is feature scaling? It is a preprocessing technique used to normalize the range of independent variables. In other words, it scales all the features to a similiar range. There are many different scaling techniques, but the one I will be using is Min Max Scaling. The algorithm looks like the following:\n\n![](https:\/\/miro.medium.com\/max\/506\/1*ii46F2WDo9mvFvdxzUvGbQ.png)","5233ecba":"### Some basic imports","b1a74c65":"Wow look at the speed up! Now let's see how well it reduced our cost function","9081b727":"## Don't forget to upvote if you enjoyed it!!","479e4c16":"I have always found it useful to programmaticaly code alogrithms from scratch in order to fully understand what one is doing. \nKeeping that in mind, I would like to show how one can code PCA and gradient descent from scratch! Enjoy.","f99174a1":"# Welcome to my kernel! ","67e11f42":"We will be using the boston house price dataset, so\u00a0we can conveniently import them from `sklearn`","55c78b85":"### Thanks for reading my kernel! If you enjoyed, please upvote!","c08395a4":"So definitely a big improvement! We can visualize this by using a line plot:"}}