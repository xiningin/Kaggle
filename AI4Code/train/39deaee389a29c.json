{"cell_type":{"2cec1ff3":"code","bf6e2d49":"code","fa639fb1":"code","1f43bc5c":"code","310487fa":"code","8c82ea48":"code","23bc171b":"code","c6a07b75":"code","6a7dfead":"code","1c7e82f3":"code","02acb7d5":"code","45c8f0fd":"code","3e29277c":"code","807ff0ae":"code","5b9b6d69":"code","4071fcf7":"code","ebb570d5":"code","f50aad6b":"code","aa51c71d":"code","07828580":"code","ae04c428":"code","af4d93d6":"code","9fcc62fc":"code","cea58040":"code","3eb07652":"code","cb6d0b7c":"code","9b0edbf4":"code","d0bb2511":"code","804b6975":"code","95868d18":"code","80e6ae03":"code","0d44591e":"code","a0fe2895":"code","d47ca27a":"code","e76c4ec2":"code","998d062a":"code","b116c4c7":"markdown","6dd0b9ef":"markdown","57503f28":"markdown","7db8a4b9":"markdown","3afac702":"markdown","18bc3586":"markdown","e8c267cd":"markdown","2c2f213a":"markdown","74d9ff71":"markdown","2708a1fc":"markdown","c96955ed":"markdown","31f04081":"markdown","52b7b146":"markdown","d8931eaa":"markdown","320946eb":"markdown","1fb33cca":"markdown","90984feb":"markdown","0fcc637d":"markdown","306632de":"markdown","be7f4595":"markdown","3ce34d39":"markdown","8b285bce":"markdown","7712704c":"markdown","c66b2839":"markdown","847dd59d":"markdown","0fc35da8":"markdown","dcdc202a":"markdown"},"source":{"2cec1ff3":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n        import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy\nimport seaborn as sns\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import GridSearchCV\n\nsns.set()","bf6e2d49":"data = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')","fa639fb1":"data.describe()","1f43bc5c":"data.drop(['id'], inplace=True, axis=1)","310487fa":"fig, (ax1, ax2,ax3) = plt.subplots(1,3, figsize=(15,4))\n\nsns.histplot(data['bmi'], bins=np.arange(data['bmi'].min(), data['bmi'].max()+1), ax = ax1)\nax1.set(xlabel='BMI')\nsns.histplot( data['avg_glucose_level'], bins=np.arange(data['avg_glucose_level'].min(), data['avg_glucose_level'].max()+1), ax = ax2)\nax2.set(xlabel='Avg Glucose Level')\nsns.histplot(data['age'], bins=np.arange(data['age'].min(), data['age'].max()+1), ax = ax3)\nax3.set(xlabel='Age')\nplt.show()","8c82ea48":"def outlier_detection(data, category):\n    mean = np.mean(data[category])\n    sd = np.std(data[category])\n    threshold = 2\n    outliers = []\n    for k, i in enumerate(data[category]): \n        z = (i-mean)\/sd \n        if abs(z) > threshold: \n            outliers.append(k) \n    return data.drop(data.index[outliers])\n\ndata = outlier_detection(data, 'bmi')\ndata = outlier_detection(data, 'avg_glucose_level')\n\nprint(data['stroke'].value_counts())\nprint('--------------------------------')\nprint(data.shape)\n","23bc171b":"# Visualising the data now:\nfig, (ax1, ax2,ax3) = plt.subplots(1,3, figsize=(15,4))\n\nsns.histplot(data['bmi'], bins=np.arange(data['bmi'].min(), data['bmi'].max()+1), ax = ax1)\nax1.set(xlabel='BMI')\nsns.histplot( data['avg_glucose_level'], bins=np.arange(data['avg_glucose_level'].min(), data['avg_glucose_level'].max()+1), ax = ax2)\nax2.set(xlabel='Avg Glucose Level')\nsns.histplot(data['age'], bins=np.arange(data['age'].min(), data['age'].max()+1), ax = ax3)\nax3.set(xlabel='Age')\nplt.show()\n","c6a07b75":"\n\ndisplay(data[data['work_type'] == 'children'].describe())\ndisplay(data[data['age'] < 3].describe())","6a7dfead":"fig, ((ax2,ax3), (ax1, ax4)) = plt.subplots(2,2, figsize=(15,12))\n\nsns.violinplot(x=data['stroke'], y=data['age'], ax = ax2)\nax2.set_title('Stroke Proportions by Age')\nax2.set_xticklabels( ['No Stroke', 'Stroke'])\n\nsns.histplot(data = data, x = 'age', hue='stroke', ax = ax3)\nax3.set_title('Age Proportions of Healthy and Individuals with Stroke')\nax3.legend(loc='upper left', labels=['No Stroke', 'Stroke'])\n\nminy_lim, y_lim = plt.ylim()\nax3.axvline(data[data['stroke']==1]['age'].mean(), linestyle='--', color='r')\nax3.axvline(data[data['stroke']==0]['age'].mean(), linestyle='--')\nax3.text(s = f\"Mean Age (Stroke) : \\n {data[data['stroke']==1]['age'].mean():.2f}\",\n         y = y_lim * 0.75, x =data[data['stroke']==1]['age'].mean() )\nax3.text(s = f\"Mean Age : \\n {data[data['stroke']==0]['age'].mean():.2f}\",\n         y = y_lim * 0.9, x =data[data['stroke']==0]['age'].mean()+3 )\n\n\nedaData = data[data['stroke']==1]\nsns.histplot(data = data, x = 'bmi', ax = ax1, alpha=0.5 )\nax10 = ax1.twinx()\nsns.histplot(data = edaData, x = 'bmi', ax = ax10, color='g', alpha=0.3)\n\nsns.histplot(data = data, x = 'avg_glucose_level', ax = ax4, alpha=0.5)\nax11 = ax4.twinx()\nax11.grid(False)\nax10.grid(False)\nax1.grid(False)\nax4.grid(False)\nsns.histplot(data = edaData, x = 'avg_glucose_level', ax = ax11, color='g', alpha=0.3)\nax10.legend(loc='upper right', labels=['positive stroke values'])\nax10.set_ylabel('positive stroke values')\n\nax11.legend(loc='upper right', labels=['positive stroke values'])\nax11.set_ylabel('positive stroke values')\n\nplt.show()\n","1c7e82f3":"# Note: The BMI index works as a range in combination with height and weight. Since we don't have these parameters, we will be avergaing the range and applying the BMI value.\n# 0 - Underweight; 1 - Healthy; 2 - Overweight; 3 - Obese\ndata['obesity'] = 0\ndata['obesity'].loc[ data['bmi'] > 18 ] = 1\ndata['obesity'].loc[ data['bmi'] > 25 ] = 2\ndata['obesity'].loc[ data['bmi'] > 30 ] = 3\n\n# 0 - Low; 1 - Normal; 2 - Diabetic\ndata['diabetic'] = 0\ndata['diabetic'].loc[ data['avg_glucose_level'] > 60 ] = 1\ndata['diabetic'].loc[ data['avg_glucose_level'] > 125 ] = 2\n\ndata['married'] = 0\ndata['married'].loc[ data['ever_married'] ==  'Yes' ] = 1\n\ndata['gender'] = data['gender'].replace({'Male':0,'Female':1,'Other':-1}).astype(np.uint8)\ndata['Residence_type'] = data['Residence_type'].replace({'Rural':0,'Urban':1}).astype(np.uint8)\ndata['smoking_status'] = data['smoking_status'].replace({'never smoked': 0, 'smokes': 1, 'formerly smoked': 2, 'Unknown': 3}).astype(np.uint8)\ndata['work_type'] = data['work_type'].replace({'Private':0,'Self-employed':1,'Govt_job':2,'children':3,'Never_worked':4}).astype(np.uint8)","02acb7d5":"data.drop(['bmi'], inplace=True, axis=1)\ndata.drop(['avg_glucose_level'], inplace=True, axis=1)\ndata.drop(['ever_married'], inplace=True, axis=1)","45c8f0fd":"data_encoded = pd.get_dummies(data)","3e29277c":"data_encoded","807ff0ae":"from sklearn.model_selection import train_test_split\n\nX = data_encoded.drop('stroke', axis=1)\ny = data_encoded['stroke']\n\nX_train, X_test, y_train, y_test = train_test_split( X, y, train_size=0.7, random_state=42)","5b9b6d69":"# oversample = SMOTE(random_state=42)\n# X_train_smote, y_train_smote = oversample.fit_resample(X_train, y_train)\n# X_test_smote, y_test_smote = oversample.fit_resample(X_test, y_test)","4071fcf7":"from imblearn.combine import SMOTETomek \n\noversample = SMOTETomek(random_state=42)\nX_train_smote, y_train_smote = oversample.fit_resample(X_train, y_train)\nX_test_smote, y_test_smote = oversample.fit_resample(X_test, y_test)","ebb570d5":"print('original train', X_train.shape, y_train.shape)\nprint('smote train', X_train_smote.shape, y_train_smote.shape)\n\nprint('original test', X_test.shape, y_test.shape)\nprint('smote test', X_test_smote.shape, y_test_smote.shape)\n","f50aad6b":"# models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom collections import Counter\nfrom xgboost import XGBClassifier\n\n# metrics and pickle\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support, classification_report, confusion_matrix\nimport pickle","aa51c71d":"import warnings\nwarnings.filterwarnings('ignore')\n\n# Data without SMOTE\n\nmodel_dict = {\n    'Logistic Reg': LogisticRegression(random_state=0, max_iter=350, solver='lbfgs'),\n    'Naive Bayes': GaussianNB(), \n    'Stochastic Grad Descent': SGDClassifier(random_state=0), \n    'Random Forest Classifier': RandomForestClassifier(random_state=0),\n    'Gradient Boosting Classifier': GradientBoostingClassifier(random_state=0),\n    'Support Vector Machine': SVC(random_state=0),\n    'K Nearest Classifier': KNeighborsClassifier(),\n    'Decison Tree': DecisionTreeClassifier(random_state=0)\n            }\nmodel_list = []\ntrain_acc_list = []\ntest_acc_list = []\ncounter_list = []\n\nfor model, clf in model_dict.items():\n    clf.fit(X_train, y_train)\n    predictions = clf.predict(X_test)\n    acc = accuracy_score(y_test, predictions)\n    train_pred =  clf.predict(X_train)\n    train_acc = accuracy_score(y_train, train_pred)\n    counter = Counter(predictions)\n    print(model, 'Model')\n    print(classification_report(y_test, predictions))\n    print(confusion_matrix(y_test,predictions))\n    print('--------------------------------')\n    \n    model_list.append(model)\n    train_acc_list.append(train_acc)\n    test_acc_list.append(acc)   \n    counter_list.append(counter)\n    \n\nresults = pd.DataFrame({\"model\": model_list, \"train_accuracy\": train_acc_list, \"test_acc\": test_acc_list, 'counter': counter_list})\n","07828580":"display(results)\nCounter(y_test)","ae04c428":"y_test_analys = y_test.reset_index().drop('index', axis=1)\nindex = y_test_analys.index[y_test_analys['stroke'] == 1]\nprint('predictions by Dec Tree model for all positive true values:  ', Counter(predictions[index]) )\nprint('All predictions by Dec Tree model:  ', Counter(predictions) )","af4d93d6":"# Let's take a look at results using SMOTE\n\nmodel_dict = {\n    'Logistic Reg': LogisticRegression(random_state=0, max_iter=550, solver='lbfgs'),\n    'Naive Bayes': GaussianNB(), \n    'Stochastic Grad Descent': SGDClassifier(random_state=0), \n    'Random Forest Classifier': RandomForestClassifier(random_state=0),\n    'Gradient Boosting Classifier': GradientBoostingClassifier(random_state=0),\n    'Support Vector Machine': SVC(random_state=0),\n    'K Nearest Classifier': KNeighborsClassifier(),\n    'Decison Tree': DecisionTreeClassifier(random_state=0),\n    # 'XGBClassifier': XGBClassifier(learning_rate=0.1,objective='binary:logistic',random_state=0,eval_metric='mlogloss')\n            }\nmodel_list = []\ntrain_acc_list = []\ntest_acc_list = []\ncounter_list = []\nprediction_list = []\nmetric_scores_list = []\n\nfor model, clf in model_dict.items():\n    clf.fit(X_train_smote, y_train_smote)\n    predictions = clf.predict(X_test_smote)\n    acc = accuracy_score(y_test_smote, predictions)\n    train_pred =  clf.predict(X_train_smote)\n    train_acc = accuracy_score(y_train_smote, train_pred)\n    counter = Counter(predictions)\n    report = precision_recall_fscore_support(y_test_smote, predictions, average='binary')\n    report_values =  (\"precision = %0.2f, recall = %0.2f, F1 = %0.2f, accuracy = %0.2f\" % \\\n           (report[0], report[1], report[2], acc))\n\n#     path = '.\/models\/' + model\n#     with open(path, 'wb') as f:\n#         pickle.dump(clf, f)\n\n\n    print(model, 'Model')\n    print(classification_report(y_test_smote, predictions))\n    print(confusion_matrix(y_test_smote,predictions))\n    print('--------------------------------')\n    \n    model_list.append(model)\n    train_acc_list.append(train_acc)\n    test_acc_list.append(acc)   \n    counter_list.append(counter)\n    prediction_list.append(predictions)\n    metric_scores_list.append(report_values)\n    \n\nresults = pd.DataFrame({\"model\": model_list, \"train_accuracy\": train_acc_list, \"test_acc\": test_acc_list,'metric':metric_scores_list, 'counter': counter_list, 'predictions': prediction_list})\n","9fcc62fc":"display(results) \nCounter(y_test_smote)","cea58040":"y_test_analys_smote = y_test_smote.reset_index().drop('index', axis=1)\nindex = y_test_analys_smote.index[y_test_analys_smote['stroke'] == 1]\nprint('predictions by GBC for all true values being positive:  ', Counter(prediction_list[4][index]) )\nprint('all predictions by GBC:  ', Counter(prediction_list[4]) )","3eb07652":"# from sklearn.model_selection import RepeatedStratifiedKFold\n# from sklearn.model_selection import GridSearchCV\n\n# model = LogisticRegression()\n# solvers = ['newton-cg', 'lbfgs', 'liblinear']\n# penalty = ['l2']\n# c_values = [100, 10, 1.0, 0.1, 0.01]\n\n# # define grid search\n# grid = dict(solver=solvers,penalty=penalty,C=c_values)\n# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n# grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n# grid_result = grid_search.fit(X_train_smote, y_train_smote)\n\n# # summarize results\n# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# means = grid_result.cv_results_['mean_test_score']\n# stds = grid_result.cv_results_['std_test_score']\n# params = grid_result.cv_results_['params']\n# for mean, stdev, param in zip(means, stds, params):\n#     print(\"%f (%f) with: %r\" % (mean, stdev, param))","cb6d0b7c":"# model = LogisticRegression(C=100,penalty='l2',random_state=0, solver='lbfgs')\n# model.fit(X_train_smote, y_train_smote)\n\n# logreg_tuned_pred = model.predict(X_test)\n\n\n# print(classification_report(y_test,logreg_tuned_pred))\n\n# print('Accuracy Score: ',accuracy_score(y_test,logreg_tuned_pred))","9b0edbf4":"# p_test3 = {'learning_rate':[0.15,0.1,0.05,0.01,0.005,0.001], 'n_estimators':[100,250,500,750,1000,1250,1500,1750], 'max_depth': [3,5,7]}\np_test3 = {'learning_rate':[0.1], 'n_estimators':[1500], 'max_depth': [5]}\n\ntuning = GridSearchCV(estimator=GradientBoostingClassifier(min_samples_split=2, min_samples_leaf=1, subsample=1,max_features='sqrt', random_state=10), \n            param_grid = p_test3, scoring='accuracy',n_jobs=4, cv=3, verbose=20)\ntuning.fit(X_train_smote,y_train_smote)","d0bb2511":"tuning.best_params_, tuning.best_score_\n\n\npreds = tuning.predict(X_test_smote)\nacc = accuracy_score(y_test_smote, preds)\nprint(acc)","804b6975":"rf = RandomForestClassifier(random_state=0)\n\nprint('Parameters currently in use:\\n')\nprint(rf.get_params())","95868d18":"from sklearn.model_selection import RandomizedSearchCV\n\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 10, stop = 100, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt', 'log2',1,2,3]\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)\n","80e6ae03":"# Use the random grid to search for best hyperparameters\n\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)","0d44591e":"# rf_random.fit(X_train_smote, y_train_smote)\n# rf_random.best_params_","a0fe2895":"rf = RandomForestClassifier(bootstrap= False, max_depth=100, max_features= 'log2', min_samples_leaf = 1, \n                            min_samples_split=5, n_estimators= 50)\n\nrf.fit(X_train_smote, y_train_smote)\nfinal_pred = rf.predict(X_test_smote)\nacc = accuracy_score(y_test_smote, final_pred)\nreport = precision_recall_fscore_support(y_test_smote, predictions, average='binary')\nreport_values =  (\"precision = %0.2f, recall = %0.2f, F1 = %0.2f, accuracy = %0.2f\" % \\\n           (report[0], report[1], report[2], acc))\n\nprint(report_values)\n","d47ca27a":"from sklearn.metrics import classification_report\n\nreport = classification_report(y_test_smote, predictions)\nprint('Accuracy Score: ',accuracy_score(y_test_smote,predictions))\nprint(report)","e76c4ec2":"rf.feature_importances_","998d062a":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_recall_curve\n\n\nns_probs = [0 for _ in range(len(y_test_smote))]\nlr_probs = rf.predict_proba(X_test_smote)\nlr_probs = lr_probs[:, 1]\nns_auc = roc_auc_score(y_test_smote, ns_probs)\nlr_auc = roc_auc_score(y_test_smote, lr_probs)\n\nns_fpr, ns_tpr, _ = roc_curve(y_test_smote, ns_probs)\nlr_fpr, lr_tpr, _ = roc_curve(y_test_smote, lr_probs)\n\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\nplt.plot(lr_fpr, lr_tpr, marker='.', label='Random Forest')\n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()\n# y_scores = rf.predict_proba(X_train_smote)[:,1]\n# precisions, recalls, thresholds = precision_recall_curve(y_train_smote, y_scores)\nprint(lr_auc)\n","b116c4c7":"### Gradient Boosting Classifier ","6dd0b9ef":"Now, let's move on to the age. Firstly, the age at 0 have an abnormally high number of entries. \nMoreover, the ages are in decimal. We need to clean these values as well","57503f28":"BMI and Glucose Levels seem to be Normally Distributed so we will use Z-Score to find outliers. \nHowever, for Age, we will use IQR.\n","7db8a4b9":"# HyperParameter Tuning","3afac702":"# Class Imbalance Techniques\n\nNow that we're done cleaning and exploring the data, let's dive into our problem of class imbalances. As mentioned earlier, less than 5% of our data is positive for strokes and that can pose an issue since the model would overfit and not generalise well.","18bc3586":"## SMOTE","e8c267cd":"Now, we start seeing results resemble the true value number more. Let's see how accurate they are for the gradient boosting classifier:","2c2f213a":"Finally, we start to see much better results! Using the GBC, our positive values match correctly for over 85% of the values.\n\nWe can start Hyperparameter Tuning on some of the models to improve the performance and finally select one. Let's start with the GBC owing to its high 82% test accuracy along with a precision of 86% and recall of 77%.\n\nNext, we will also look at the Random Forest model and the Logistic Regression model.","74d9ff71":"# Data Exploration and Visualisation\n\nNow, we can start creating new variables and define bmi and glucose levels as categories instead which will make it easy for the model to read.\nAlso, let's take a look at some possible correlations that could exist.","2708a1fc":"### Logistic Regression Tuning","c96955ed":"There are several ways to remove outliers. For this, I am considering using the Z-Score method if the variable is normally distributed and IQR if they're not. So, lets start by checking if age, bmi and glucose are guassian or not.","31f04081":"Looking closer at the data, we find that there are over 140 entries with children aged below 3 and this was also the reason for our extremely low BMI values. The values in decimals are the months of the children who are less than 1 year old. Thus, we won't be changing anything in this column","52b7b146":"# Building the model","d8931eaa":"# Data Cleaning","320946eb":"As shown above, the true values of y_test show 60 positive detections of stroke and most of our models are overfitting and prediciting negative results(0). Decision Tree seems to be the only ones having higher values of positive predictions (1) but we need to see if these predictions are actually accurate. ","1fb33cca":"## Random Forest Tuning","90984feb":"# Plan Going Forward\n\n- Look at the outliers and why some of the values are abnormal.\n- Look at the bmi levels in the data for all 'work types' and plot against stroke variable.\n- Check for correlation between variables\n- Create categories for underweight and obese using bmi. Also diabetic categories for glucose levels\n- Analyse relation between hypyertension, heart disease as compared to stroke detection and create histograms in different work types.","0fcc637d":"## SMOTETomek","306632de":"### Initial Model Thoughts\nEven our most promising model, the Decision Tree, predicted only 8 correct values. It had 67 positive predictions out which 89% of the predictions were incorrect. Similar results are seen for other models as well. \n\nThus we cannot use these models at the moment. We will see the results after customising the models and testing the SMOTE data. ","be7f4595":"## Insights\n\nFrom the plots above, we can clearly see that age plays a huge role in predicting stroke. The normal BMI and Glucose Levels are in line with the values for positive cases. Alternatively, the positive cases tend to happen for ages 40+. \n\nThis does not mean bmi and glucose levels don't play a role. They have an effect but its not clear yet. ","3ce34d39":"## Class Imbalance\n\nIn this dataset, we encounter imbalanced data with only 5% of the data being positive for stroke. We will try different methods to counteract this problem.\n\nBefore we begin, let's understand why this is a problem in the first place. When we work with machine learning algorithm, the model learns over time coursing through the data. It looks at the data and starts to find patterns and distinguishes between them to make predictions. We track a model's effectiveness from the accuracy of the predictions. Now, if only 5% of the data is positive, the model can learn to start predicting negative values in all cases. Thus, while the model has a 95% accuracy, it cannot catch a patient having a stroke.\n\nWe need to b able to distinguish between the false negative (when the model should predict stroke but does not) and true negative values. It is okay if the models has a few false positive values (determining stroke when the patient doesn't have one) but false negative values should be avoided completely.","8b285bce":"### Final Model Selection","7712704c":"# Stroke Prediction (Class Imbalances)\n\nHey there, for this project, I am taking up the dataset from Kaggle on stroke prediction. The data provided has several features (such as age, hypertension, smoking status, BMI, glucose, etc. \n\nUsing these, I will attempt to perform extensive Data Visualisation, Exploratory Data Analysis, Feature Engineering and finally fit an ML model to make predictions on said features. Some other techniques we'll try out here are fixing class imbalances with oversampling techinques, Hyperparameter Tuning of the ML model, dealing with outliers and missing data, etc. \n\nI will try my best to explain each step as we go and help you understand.\n\n## Background \nThis is my first stab at a dataset with Class Imbalances and from my inital research, there are several ways to deal with them namely:\n- Random Over Sampling\n- Random Under Sampling\n- TomekLinks\n- SMOTE\n- ADASYN \nand many others.\n\nSince our data is small to begin with (being only around 5000 entries), I am avoiding undersampling techniques. Instead, I am going to be testing techniques like SMOTE, ADASYN, NearMiss to try and balance out our classes.","c66b2839":"Let's start with some classification models that would do well with this data and narrow down from there. These are the next steps:\n- Out of these, which model to finally choose. Why?\n- Hyperparameter Tuning for the chosen model to imporve performance\n- Check predictions. Are they learning to only predict 0 and getting a high accuracy? Compare results with oversampled data.\n- How else can we improve accuracy? Maybe try further Data Cleaning and Encoding?","847dd59d":"Now that all our data is categorical, let's create dummies. Of course, we are increasing the dimensionality of the data by doing this, but it will help us analyse the model easier and make predictions more efficiently.","0fc35da8":"## Imports","dcdc202a":"From the data above, there are a few things to note - hypertension, heart_disease and stroke are categorical variables. We will need to analyse those separately.\n\n### Possible Outliers:\n- There seems to be something wrong with the age column since we have a value of 0.08. \n- The BMI column should not have values of 10\n- Similarly, glucose levels of 55 and 270 are concerning and might need to be adjusted.\nWe will look at these more closely soon.\n"}}