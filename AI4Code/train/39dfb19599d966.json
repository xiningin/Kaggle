{"cell_type":{"0349ab0b":"code","6e3e590a":"code","4318bdfe":"code","069b9e7a":"code","414478b3":"code","0f5cd921":"code","73d530da":"code","9e48af4f":"code","70a27165":"code","5405cc9a":"code","adb306bf":"code","d855bad4":"code","acd77849":"code","eaec5f19":"code","fca7fe45":"code","9e990d5a":"code","2059a872":"code","93aef4ac":"code","0252d8b3":"code","a625424a":"code","0b7713d3":"code","0c362569":"code","2e9e6beb":"code","09511684":"code","77260893":"code","34d00d3e":"code","8769a50e":"code","ab31c0b8":"code","fc25b7b9":"code","6211c808":"code","4c7312b0":"code","e941e197":"code","b60aa884":"code","62c5316b":"code","01569095":"code","3758a333":"code","6ac98c02":"code","f3fe8e09":"code","0de5d4d8":"code","f4fe6483":"code","500ddc73":"code","4c62788c":"code","46ba4bb1":"code","2272c7d8":"code","e744a9b8":"code","1332fc7c":"code","c3fefd09":"code","dcd07782":"code","5d8c224d":"code","8624bc46":"code","46541aac":"code","c9c48559":"code","54eb1001":"code","d66ed998":"code","0e791156":"code","e6cb54aa":"code","c47429c1":"code","05e7d943":"code","899173c8":"code","d83af4c0":"code","43e1bf27":"code","b68c243e":"code","ade4ec46":"code","fa0a692f":"code","3828f532":"code","aad50b2c":"code","061e94bc":"code","54fc221c":"code","9c975a3a":"code","4ed2995b":"code","de1063b2":"code","bcac2614":"code","22a0fb8b":"code","e7da4dca":"code","45f4b63b":"code","a2114a0c":"markdown","00f5d5dc":"markdown","dc9d26ff":"markdown","40277254":"markdown","a88d6f8f":"markdown","afe3a067":"markdown","4c9020b3":"markdown","8e0b206f":"markdown","53fc5e4b":"markdown","973b88d3":"markdown","e66bb7c8":"markdown","2e5d45b3":"markdown","dbe418ab":"markdown","932b6f68":"markdown","c47a2d73":"markdown","a33e806f":"markdown","74c48d8b":"markdown","c76038a2":"markdown","38e56521":"markdown","6f257acc":"markdown","f7af9e4b":"markdown","716c3e81":"markdown","233b9273":"markdown","835d02ec":"markdown","02c50351":"markdown","eb7b2463":"markdown","6be8270f":"markdown","c7ad442b":"markdown","2fcaf441":"markdown","ad88f297":"markdown","d5d329b7":"markdown","2ec96bed":"markdown","9fdebbad":"markdown","c35be358":"markdown","5863b79a":"markdown","d3ad249b":"markdown","ba8ac1d0":"markdown","460f11ce":"markdown","67b2b351":"markdown","b013b778":"markdown","394447ed":"markdown","17df59fc":"markdown","82ac89df":"markdown","fed50361":"markdown","e2e298bc":"markdown","ffda7a17":"markdown","2e7ad9fe":"markdown","65f4d7a5":"markdown","0cc19a97":"markdown","6de4ca63":"markdown","d459a302":"markdown","1152c7df":"markdown","28e1d09a":"markdown","02db6e0c":"markdown","f6d3c583":"markdown","9408e1bc":"markdown","cb0b549d":"markdown","83a50cdd":"markdown","a7498725":"markdown","82fbd241":"markdown","cabd107e":"markdown","191cdb05":"markdown","f3fce041":"markdown","e442c142":"markdown","f47cc99f":"markdown","3756bb0a":"markdown","63b64199":"markdown","257e4a25":"markdown","14865b94":"markdown","d7a9ad9b":"markdown","2da2bdba":"markdown","f982fa71":"markdown","b76ca2da":"markdown","afeefda4":"markdown","87a69570":"markdown","9ab35f26":"markdown","778c8ef8":"markdown","bfc07359":"markdown","2c0afcec":"markdown","df4c01ec":"markdown","88572604":"markdown","b9c435d6":"markdown","5b216fcc":"markdown","975b90f3":"markdown","d1cb55dd":"markdown","faf07807":"markdown","bd7e2f35":"markdown","cb50dfe1":"markdown","ad03f203":"markdown","ea9d605e":"markdown","cbd0aa26":"markdown","842c9891":"markdown","4e92cb02":"markdown","5dba98f7":"markdown","245c8e59":"markdown","a4ccb344":"markdown","0354028d":"markdown","b5a0b026":"markdown","dd2e3410":"markdown","c9fe4b14":"markdown","7a959106":"markdown","df6d5b7a":"markdown","be34e227":"markdown","bb88e726":"markdown","167f0fc2":"markdown","e85d2a12":"markdown","a4d73f86":"markdown","c1c19e09":"markdown","7ece4d3e":"markdown","7e43b05c":"markdown","fe76c80b":"markdown","c5e76fa0":"markdown","ce2ade42":"markdown","d8213508":"markdown"},"source":{"0349ab0b":"#Importing the usual python libraries\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport statsmodels.api as sm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n%matplotlib inline\n\n#Importing libraries to hide warnings as well as to time code execution duration\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom timeit import default_timer as timer\n\n#Importing libraries for machine learning algorithms\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, roc_curve, auc, recall_score\nfrom sklearn.model_selection import GridSearchCV","6e3e590a":"empdf = pd.read_csv('..\/input\/hr-analytics-case-study\/general_data.csv', dtype={'EmployeeID': object})\nempsurvey = pd.read_csv('..\/input\/hr-analytics-case-study\/employee_survey_data.csv',dtype={'EmployeeID': object})\nmgrsurvey = pd.read_csv('..\/input\/hr-analytics-case-study\/manager_survey_data.csv',dtype={'EmployeeID': object})","4318bdfe":"empdf.head()","069b9e7a":"empsurvey.head()","414478b3":"empsurvey.describe()","0f5cd921":"mgrsurvey.head()","73d530da":"mgrsurvey.describe()","9e48af4f":"df = pd.merge(pd.merge(empdf, empsurvey, on = 'EmployeeID'), mgrsurvey, on = 'EmployeeID')","70a27165":"df.head()","5405cc9a":"df.shape","adb306bf":"df.info()","d855bad4":"df.describe()","acd77849":"dupcheck = df['EmployeeID'].nunique() - df['EmployeeID'].count()\n\nprint('No. of duplicated Employee records :')\nprint(dupcheck)","eaec5f19":"nullcount = df.isnull().sum()\nnullcheck = nullcount[nullcount >0]\n\nprint('These are the columns that contain null values: ' + '\\n')\nif nullcheck.empty == False:\n    print(nullcheck)\nelse:\n    print('*No more columns with null values*')","fca7fe45":"df.fillna(df.median(), inplace=True)","9e990d5a":"nullcount = df.isnull().sum()\nnullcheck = nullcount[nullcount >0]\n\nprint('These are the columns that contain null values: ' + '\\n')\nif nullcheck.empty == False:\n    print(nullcheck)\nelse:\n    print('*No more columns with null values*')","2059a872":"uniquecount = df.nunique()\nuniquecheck = uniquecount[uniquecount == 1]\n\nprint('These are the columns with just 1 unique value: ' + '\\n')\nprint(uniquecheck)","93aef4ac":"df = df.drop(columns = ['EmployeeCount', 'Over18','StandardHours','EmployeeID'])","0252d8b3":"df['AvgEmpScore'] = round(df[['EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance']].mean(axis=1),1)\ndf['AvgMgrScore'] = round(df[['JobInvolvement', 'PerformanceRating']].mean(axis=1),1)\ndf.head()","a625424a":"sns.set(context=\"paper\", font_scale=1.5)\n\nplt.figure(figsize=(20,5))\n\nplt.subplot(1,5,1)\nsns.countplot(df['Gender'])\nplt.xticks(rotation=90)\nplt.ylim((0,3000))\nplt.title('Distribution by Gender')\n\nplt.subplot(1,5,2)\nsns.countplot(df['MaritalStatus'])\nplt.xticks(rotation=90)\nplt.yticks([])\nplt.ylim((0,3000))\nplt.ylabel('')\nplt.title('Distribution by Marital Status')\n\nplt.subplot(1,5,3)\nsns.countplot(df['Department'])\nplt.xticks(rotation=90)\nplt.yticks([])\nplt.ylim((0,3000))\nplt.ylabel('')\nplt.title('Distribution by Department')\n\nplt.subplot(1,5,4)\nsns.countplot(df['JobLevel'])\nplt.xticks(rotation=0)\nplt.yticks([])\nplt.ylim((0,3000))\nplt.ylabel('')\nplt.title('Distribution by JobLevel')\n\nplt.subplot(1,5,5)\nsns.countplot(df['BusinessTravel'])\nplt.xticks(rotation=90)\nplt.yticks([])\nplt.ylim((0,3000))\nplt.ylabel('')\nplt.title('Distribution by BusinessTravel')\n\nplt.show()","0b7713d3":"sns.set(context=\"paper\", font_scale=1.5)\nplt.figure(figsize=(20,5))\n\nplt.subplot(1,3,1)\nsns.kdeplot(data= df['Age'][df.Attrition == 'Yes'], color='red', shade=True)\nsns.kdeplot(data= df['Age'][df.Attrition == 'No'], color='green', shade=True)\nplt.axvline(df['Age'].median(), color='k', linestyle='dashed', linewidth=1)\nplt.legend(['Attrition = Yes' , 'Attrition = No', 'Median Age'],prop={'size': 10})\nplt.yticks([])\nplt.title('Distribution by Age')\n\nplt.subplot(1,3,2)\nsns.kdeplot(data= df['DistanceFromHome'][df.Attrition == 'Yes'], color='red', shade=True)\nsns.kdeplot(data= df['DistanceFromHome'][df.Attrition == 'No'], color='green', shade=True)\nplt.axvline(df['DistanceFromHome'].median(), color='k', linestyle='dashed', linewidth=1)\nplt.legend(['Attrition = Yes' , 'Attrition = No', 'Median Distance'],prop={'size': 10})\nplt.yticks([])\nplt.title('Distribution by DistanceFromHome')\n\nplt.subplot(1,3,3)\nsns.kdeplot(data= df['AvgEmpScore'][df.Attrition == 'Yes'], color='red', shade=True)\nsns.kdeplot(data= df['AvgEmpScore'][df.Attrition == 'No'], color='green', shade=True)\nplt.axvline(df['AvgEmpScore'].median(), color='k', linestyle='dashed', linewidth=1)\nplt.legend(['Attrition = Yes' , 'Attrition = No', 'Median EmpScore'],prop={'size': 10})\nplt.yticks([])\nplt.title('Distribution by AvgEmpScore')\n\n\nplt.show()","0c362569":"sns.set(context=\"paper\", font_scale=1.5)\nplt.figure(figsize=(20,5))\n\nplt.subplot(1,3,1)\nsns.kdeplot(data= df['YearsAtCompany'][df.Attrition == 'Yes'], color='red', shade=True)\nsns.kdeplot(data= df['YearsAtCompany'][df.Attrition == 'No'], color='green', shade=True)\nplt.axvline(df['YearsAtCompany'].median(), color='k', linestyle='dashed', linewidth=1)\nplt.legend(['Attrition = Yes' , 'Attrition = No', 'Median Years'],prop={'size': 10})\nplt.yticks([])\nplt.title('Distribution by YearsAtCompany')\n\nplt.subplot(1,3,2)\nsns.kdeplot(data= df['YearsSinceLastPromotion'][df.Attrition == 'Yes'], color='red', shade=True)\nsns.kdeplot(data= df['YearsSinceLastPromotion'][df.Attrition == 'No'], color='green', shade=True)\nplt.axvline(df['YearsSinceLastPromotion'].median(), color='k', linestyle='dashed', linewidth=1)\nplt.legend(['Attrition = Yes' , 'Attrition = No', 'Median Years'],prop={'size': 10})\nplt.yticks([])\nplt.title('Distribution of YearsSinceLastPromotion')\n\nplt.subplot(1,3,3)\nsns.kdeplot(data= df['YearsWithCurrManager'][df.Attrition == 'Yes'], color='red', shade=True)\nsns.kdeplot(data= df['YearsWithCurrManager'][df.Attrition == 'No'], color='green', shade=True)\nplt.axvline(df['YearsWithCurrManager'].median(), color='k', linestyle='dashed', linewidth=1)\nplt.legend(['Attrition = Yes' , 'Attrition = No', 'Median Years'],prop={'size': 10})\nplt.yticks([])\nplt.title('Distribution of YearsWithCurrManager')\n\nplt.show()","2e9e6beb":"sns.set(context=\"paper\", font_scale=1.5)\nplt.figure(figsize=(20,5))\n\nplt.subplot(1,2,1)\nsns.kdeplot(data= df['MonthlyIncome'][df.Attrition == 'Yes'], color='red', shade=True)\nsns.kdeplot(data= df['MonthlyIncome'][df.Attrition == 'No'], color='green', shade=True)\nplt.axvline(df['MonthlyIncome'].median(), color='k', linestyle='dashed', linewidth=1)\nplt.legend(['Attrition = Yes' , 'Attrition = No', 'Median Income'],prop={'size': 10})\nplt.yticks([])\nplt.title('Distribution of MonthlyIncome')\n\nplt.show()\n\n# plt.subplot(1,2,2)\n# sns.kdeplot(x=\"PercentSalaryHike\", y=\"MonthlyIncome\", hue=\"Attrition\", palette=['g','r'], data=df)\n# #sns.despine()\n# plt.title('MonthlyIncome vs. PercentSalaryHike')","09511684":"sns.set(context=\"paper\", font_scale=1.5)\nplt.figure(figsize=(20,10))\n\nplt.subplot(1,2,1)\nsns.kdeplot(data=df['MonthlyIncome'][df.Attrition == 'No'], data2=df['DistanceFromHome'][df.Attrition == 'No'], cmap=\"Greens\", shade=True, shade_lowest=False)\nplt.title('MonthlyIncome vs. DistanceFromHome (of Attrition = No)')\n\nplt.subplot(1,2,2)\nsns.kdeplot(data=df['MonthlyIncome'][df.Attrition == 'Yes'], data2=df['DistanceFromHome'][df.Attrition == 'Yes'], cmap=\"Reds\", shade=True, shade_lowest=False)\nplt.title('Distribution of MonthlyIncome')\nplt.title('MonthlyIncome vs. DistanceFromHome (of Attrition = Yes)')\n\nplt.show()","77260893":"sns.set(context=\"paper\", font_scale=1.5)\nplt.figure(figsize=(20,10))\n\nplt.subplot(1,2,1)\nsns.boxplot(x=\"AvgEmpScore\", y=\"MonthlyIncome\", hue=\"Attrition\", palette=['g','r'], data=df)\nplt.legend(loc=1,title='Attrition')\nsns.despine()\nplt.title('MonthlyIncome vs. AvgEmpScore')\n\nplt.subplot(1,2,2)\nsns.boxplot(x=\"Education\", y=\"MonthlyIncome\", hue=\"Attrition\", palette=['g','r'], data=df)\nplt.legend(loc=1, title='Attrition')\nplt.yticks([])\nplt.ylabel('')\nsns.despine()\nplt.title('MonthlyIncome vs. Education')\n\nplt.show()","34d00d3e":"sns.set(context=\"paper\", font_scale=1.5)\n\nAttr = df.groupby(['Attrition']).size()\n\nfig, (ax1) = plt.subplots(1,1,figsize=(5,5))\n\nax1.pie(Attr, autopct = '%.0f%%', radius= 1, startangle = 0,labels = ('Attrition: No','Attrition: Yes'),labeldistance = 1.1, colors = ('gray','r'),explode=(0,0.1))\nax1.set_title('Overall Attrition Rate')\n\nplt.show()","8769a50e":"sns.set(context=\"paper\", font_scale=1.5)\n\nMale = df[df['Gender'] == 'Male'].groupby(['Attrition']).size()\nFemale = df[df['Gender'] == 'Female'].groupby(['Attrition']).size()\n\nfig, (ax1,ax2) = plt.subplots(1,2,figsize=(10,10))\n\nax1.pie(Male, autopct = '%.0f%%', radius= 1, startangle = 0,labels = ('Attrition: No','Attrition: Yes'),labeldistance = 1.1,colors = ('gray','r'),explode=(0,0.1))\nax1.set_title('Male Attrition Rate')\n\n\nax2.pie(Female, autopct = '%.0f%%', radius= 1, startangle = 0, labels =('Attrition: No','Attrition: Yes'), labeldistance = 1.1,colors = ('gray','r'),explode=(0,0.1))\nax2.set_title('Female Attrition Rate')\n\nplt.show()","ab31c0b8":"sns.set(context=\"paper\", font_scale=1.5)\n\nMarried = df[df['MaritalStatus'] == 'Married'].groupby(['Attrition']).size()\nSingle = df[df['MaritalStatus'] == 'Single'].groupby(['Attrition']).size()\nDivorced = df[df['MaritalStatus'] == 'Divorced'].groupby(['Attrition']).size()\n\nfig, (ax1,ax2, ax3) = plt.subplots(1,3,figsize=(15,15))\n\nax1.pie(Married, autopct = '%.0f%%', radius= 1, startangle = 0,labels = ('Attrition: No','Attrition: Yes'),labeldistance = 1.1,colors = ('gray','r'), explode=(0,0.1))\nax1.set_title('Married Attrition Rate')\n\nax2.pie(Single, autopct = '%.0f%%', radius= 1, startangle = 0, labels = ('Attrition: No','Attrition: Yes'), labeldistance = 1.1,colors = ('gray','r'),explode=(0,0.1))\nax2.set_title('Single Attrition Rate')\n\nax3.pie(Divorced, autopct = '%.0f%%', radius= 1, startangle = 0, labels =('Attrition: No','Attrition: Yes'), labeldistance = 1.1,colors = ('gray','r'),explode=(0,0.1))\nax3.set_title('Divorced Attrition Rate')\n\nplt.show()","fc25b7b9":"sns.set(context=\"paper\", font_scale=1.5)\n\nRD = df[df['Department'] == 'Research & Development'].groupby(['Attrition']).size()\nSales = df[df['Department'] == 'Sales'].groupby(['Attrition']).size()\nHR = df[df['Department'] == 'Human Resources'].groupby(['Attrition']).size()\n\nfig, (ax1,ax2, ax3) = plt.subplots(1,3,figsize=(15,15))\n\nax1.pie(RD, autopct = '%.0f%%', radius= 1, startangle = 0,labels = ('Attrition: No','Attrition: Yes'),labeldistance = 1.1,colors = ('gray','r'), explode=(0,0.1))\nax1.set_title('R&D Dept Attrition Rate')\n\nax2.pie(Sales, autopct = '%.0f%%', radius= 1, startangle = 0, labels =('Attrition: No','Attrition: Yes'), labeldistance = 1.1,colors = ('gray','r'), explode=(0,0.1))\nax2.set_title('Sales Dept Attrition Rate')\n\nax3.pie(HR, autopct = '%.0f%%', radius= 1, startangle = 0, labels =('Attrition: No','Attrition: Yes'), labeldistance = 1.1,colors = ('gray','r'), explode=(0,0.1))\nax3.set_title('HR Dept Attrition Rate')\n\nplt.show()","6211c808":"sns.set(context=\"paper\", font_scale=1.5)\n\nRare = df[df['BusinessTravel'] == 'Travel_Rarely'].groupby(['Attrition']).size()\nFreq = df[df['BusinessTravel'] == 'Travel_Frequently'].groupby(['Attrition']).size()\nNon = df[df['BusinessTravel'] == 'Non-Travel'].groupby(['Attrition']).size()\n\nfig, (ax1,ax2, ax3) = plt.subplots(1,3,figsize=(15,15))\n\nax1.pie(Rare, autopct = '%.0f%%', radius= 1, startangle = 0,labels = ('Attrition: No','Attrition: Yes'),labeldistance = 1.1,colors = ('gray','r'), explode=(0,0.1))\nax1.set_title('Rare Traveller Attrition Rate')\n\nax2.pie(Freq, autopct = '%.0f%%', radius= 1, startangle = 0, labels = ('Attrition: No','Attrition: Yes'), labeldistance = 1.1,colors = ('gray','r'), explode=(0,0.1))\nax2.set_title('Frequent Traveller Attrition Rate')\n\nax3.pie(Non, autopct = '%.0f%%', radius= 1, startangle = 0, labels =('Attrition: No','Attrition: Yes'), labeldistance = 1.1,colors = ('gray','r'), explode=(0,0.1))\nax3.set_title('Non Traveller Attrition Rate')\n\nplt.show()","4c7312b0":"sns.set(context=\"paper\", font_scale=1.5)\nsns.pairplot(df[['Age','MonthlyIncome','DistanceFromHome','AvgEmpScore','YearsWithCurrManager','Attrition']],hue = 'Attrition', height = 5, kind=\"reg\")\nplt.show()","e941e197":"#maintaining the integrity of the original dataframe, transform categorical data into numerical data in new dataframe\n\ndf_num = df.copy()\ndf_num.head()","b60aa884":"catcoldf = df.select_dtypes(include='object')\ncatcolname = list(catcoldf.columns.values)\ncatcolname","62c5316b":"df_num = pd.get_dummies(df_num, columns = catcolname)\ndf_num.head()","01569095":"sns.set(style=\"white\")\ncorr = df_num.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(30, 30))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\ncm = sns.heatmap(corr, cmap=cmap, vmax=.3, center=0,annot=True, fmt=\".0%\", square=True, linewidths=.1, cbar_kws={\"shrink\": .5}, annot_kws={\"size\": 15})\nbottom, top = cm.get_ylim()\ncm.set_ylim(bottom + 0.5, top - 0.5)\n\nplt.show()","3758a333":"highcorrel = set()\ncorrelmatrix = df_num.corr()\n\nfor x in range(len(correlmatrix.columns)):\n    for y in range(x):\n        if abs(correlmatrix.iloc[x, y]) > 0.7:\n            colname = correlmatrix.columns[y]\n            highcorrel.add(colname)\n\nhighcorrel","6ac98c02":"df_num = df_num.drop(columns=highcorrel)","f3fe8e09":"sns.set(style=\"white\")\ncorr = df_num.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(30, 30))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\ncm = sns.heatmap(corr, cmap=cmap, vmax=.3, center=0,annot=True, fmt=\".0%\", square=True, linewidths=.1, cbar_kws={\"shrink\": .5}, annot_kws={\"size\": 15})\nbottom, top = cm.get_ylim()\ncm.set_ylim(bottom + 0.5, top - 0.5)\n\nsns.set_context(\"paper\", font_scale=1.5)\n\nplt.show()","0de5d4d8":"X = df_num.drop('Attrition_Yes', axis =1)\ny = df_num['Attrition_Yes']\n\nX_train,X_test, y_train, y_test = train_test_split(X,y, test_size = 0.30, random_state =1)\n\nss = preprocessing.StandardScaler()\nX_train = ss.fit_transform(X_train)\nX_test = ss.transform(X_test)","f4fe6483":"#TimerStart\nlrstart = timer()\n\nlr = LogisticRegression(random_state=1)\n\nlr.fit(X_train, y_train)\n\ny_pred = lr.predict(X_test)","500ddc73":"lraccuracy = accuracy_score(y_test, y_pred)\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nlrroc_auc = auc(fpr, tpr)\n\nlrf1_score = f1_score(y_test, y_pred)\n\nlrrecall = recall_score(y_test, y_pred)\n\nprint('Model Accuracy: ', lraccuracy)\nprint('ROC_AUC Score: ', lrroc_auc)\nprint('F1 Score: ', lrf1_score)\nprint('Recall Score: ', lrrecall)","4c62788c":"sns.set(context=\"paper\", font_scale=1.5)\nlrcm = confusion_matrix(y_test, y_pred)\n\nax = heatmap = sns.heatmap(lrcm, cmap=\"Blues\", annot= True,fmt=\".0f\")\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\nplt.title('Confusion Matrix: Logistic Regression (*Tuned*)')\nplt.ylabel('Real Outcome')\nplt.xlabel('Predicted Outcome')\nplt.yticks(rotation=0)\nplt.show()\n\n#TimerEnd\nlrtime = (timer() - lrstart)","46ba4bb1":"X = df_num.drop('Attrition_Yes', axis =1)\ny = df_num['Attrition_Yes']\n\nX_train,X_test, y_train, y_test = train_test_split(X,y, test_size = 0.30, random_state =1)\n\nss = preprocessing.StandardScaler()\nX_train = ss.fit_transform(X_train)\nX_test = ss.transform(X_test)","2272c7d8":"#TimerStart\nlrtunedstart = timer()\n\nlrtuned = LogisticRegression(random_state=1, class_weight=\"balanced\")\n\nlrtuned.fit(X_train, y_train)\n\ny_pred = lrtuned.predict(X_test)","e744a9b8":"lrtunedaccuracy = accuracy_score(y_test, y_pred)\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nlrtunedroc_auc = auc(fpr, tpr)\n\nlrtunedf1_score = f1_score(y_test, y_pred)\n\nlrtunedrecall = recall_score(y_test, y_pred)\n\nprint('Model Accuracy: ', lrtunedaccuracy)\nprint('ROC_AUC Score: ', lrtunedroc_auc)\nprint('F1 Score: ', lrtunedf1_score)\nprint('Recall Score: ', lrtunedrecall)","1332fc7c":"sns.set(context=\"paper\", font_scale=1.5)\nlrtunedcm = confusion_matrix(y_test, y_pred)\n\nax = heatmap = sns.heatmap(lrtunedcm, cmap=\"Blues\", annot= True,fmt=\".0f\")\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\nplt.title('Confusion Matrix: Logistic Regression')\nplt.ylabel('Real Outcome')\nplt.xlabel('Predicted Outcome')\nplt.yticks(rotation=0)\nplt.show()\n\n#TimerEnd\nlrtunedtime = (timer() - lrtunedstart)","c3fefd09":"lrtunedcoef = []\n\nfor i in range(len(lrtuned.coef_)):\n    for j in range(len(lrtuned.coef_[0])):\n            featcoef = (lrtuned.coef_[0][j])\n            lrtunedcoef.append(featcoef)\n\ncolname = X.columns.values\n\nlrtunedtopfeat = pd.DataFrame( data = lrtunedcoef , index = colname, columns = ['Coefficient'])\nlrtunedtopfeat = lrtunedtopfeat.sort_values(by = ['Coefficient'] ,ascending = False)\nlrtunedtopfeat['Positive'] = lrtunedtopfeat['Coefficient']>0","dcd07782":"sns.set(context=\"paper\", font_scale=1.2)\nplt.figure(figsize=(15,10))\nsns.set(font_scale=1.2)\nlrtunedtopfeat['Coefficient'].plot(kind='barh', color=lrtunedtopfeat.Positive.map({True: 'g', False: 'r'}))\ngreen = mpatches.Patch(color='g', label = 'Positive Coefficient')\nred = mpatches.Patch(color='r', label = 'Negative Coefficient')\nplt.gca().invert_yaxis()\nplt.legend(handles = [green, red], loc='lower right')\nplt.title('Coefficients of ALL Features')\nplt.show()","5d8c224d":"lrtunedtopfeat['AbsCoef'] = lrtunedtopfeat['Coefficient'].abs()\nlrtunedtopfeat = lrtunedtopfeat.sort_values(by = 'AbsCoef', ascending = False)[0:5]\n\nsns.set(context=\"paper\", font_scale=1.5)\nplt.figure(figsize=(15,10))\nlrtunedtopfeat['AbsCoef'].plot(kind = 'barh', color=lrtunedtopfeat.Positive.map({True: 'g', False: 'r'}))\nplt.gca().invert_yaxis()\nplt.legend(handles = [green, red], loc='lower right')\nplt.title('(Absolute) Coefficients of Top 5 Features')\nplt.show()","8624bc46":"X = df_num.drop('Attrition_Yes', axis =1)\ny = df_num['Attrition_Yes']\n\nX_train,X_test, y_train, y_test= train_test_split(X,y, test_size = 0.30, random_state=1)","46541aac":"#TimerStart\nrfstart = timer()\n\nrf = RandomForestClassifier(n_estimators = 100, max_depth = 10, max_features = None, random_state=1, class_weight = \"balanced\")\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)","c9c48559":"rfaccuracy = accuracy_score(y_test, y_pred)\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nrfroc_auc = auc(fpr, tpr)\n\nrff1_score = f1_score(y_test, y_pred)\n\nrfrecall = recall_score(y_test, y_pred)\n\nprint('Model Accuracy: ', rfaccuracy)\nprint('ROC_AUC Score: ', rfroc_auc)\nprint('F1 Score: ', rff1_score)\nprint('Recall Score: ', rfrecall)","54eb1001":"sns.set(context=\"paper\", font_scale=1.5)\n\nrfcm = confusion_matrix(y_test, y_pred)\n\nax = heatmap = sns.heatmap(rfcm, cmap=\"Blues\", annot= True,fmt=\".0f\") \nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\n\nplt.title('Confusion Matrix: Random Forest')\nplt.ylabel('Real Outcome')\nplt.xlabel('Predicted Outcome')\nplt.yticks(rotation=0)\nplt.show()\n\n#TimerEnd\nrftime = (timer() - rfstart)","d66ed998":"#TimerStart\nrftunedstart = timer()\n\nn_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]\ntrainf1 = []\ntestf1 = []\n\nfor i in n_estimators:\n    rf = RandomForestClassifier(n_estimators=i, random_state=1, class_weight = \"balanced\")\n    rf.fit(X_train, y_train)\n    \n    train_pred = rf.predict(X_train)\n    \n    f1train = f1_score(y_train, train_pred)\n    trainf1.append(f1train)\n    \n    y_pred = rf.predict(X_test)\n    \n    f1test = f1_score(y_test, y_pred)\n    testf1.append(f1test)\n    \n\nsns.set(context=\"paper\", font_scale=1.5)\nplt.figure(figsize=(15,10))\nplt.plot(n_estimators, trainf1, 'b', label= 'Train F1 Score')\nplt.plot(n_estimators, testf1, 'g', label= 'Test F1 Score')\n\nplt.legend()\nplt.ylabel('F1 Score')\nplt.xlabel('n_estimators')\nplt.show()","0e791156":"a = list(zip(n_estimators, testf1))\nb = pd.DataFrame( data = a , columns = ('NTrees','Accuracy'))\nbestntree = int(b.iloc[ b['Accuracy'].idxmax(axis = 0) , 0])\nprint('Optimum No. of Trees: ' , bestntree)","e6cb54aa":"max_depth = np.arange(1,21,1)\ntrainf1 = []\ntestf1 = []\n\nfor i in max_depth:\n    rf = RandomForestClassifier(n_estimators = bestntree, max_depth=i, random_state=1,class_weight = \"balanced\")\n    rf.fit(X_train, y_train)\n    \n    train_pred = rf.predict(X_train)\n    \n    f1train = f1_score(y_train, train_pred)\n    trainf1.append(f1train)\n    \n    y_pred = rf.predict(X_test)\n    \n    f1test = f1_score(y_test, y_pred)\n    testf1.append(f1test)\n\nsns.set(context=\"paper\", font_scale=1.5)\nplt.figure(figsize=(15,10))\nplt.plot(max_depth, trainf1, 'b', label= 'Train F1 Score')\nplt.plot(max_depth, testf1, 'g', label= 'Test F1 Score')\n\nplt.legend()\nplt.ylabel('F1 Score')\nplt.xlabel('max_depth')\nplt.show()","c47429c1":"c = list(zip(max_depth, testf1))\nd = pd.DataFrame( data = c , columns = ('NDepth','Accuracy'))\nbestndepth = int(d.iloc[ d['Accuracy'].idxmax(axis = 0) , 0])\nprint('Optimum Max Depth: ' , bestndepth) ","05e7d943":"max_features = list(range(1,X_train.shape[1]))\ntrainf1 = []\ntestf1 = []\nfor i in max_features:\n    rf = RandomForestClassifier(n_estimators = bestntree, max_depth=bestndepth, max_features=i, random_state=1,class_weight = \"balanced\")\n    rf.fit(X_train, y_train)\n    \n    train_pred = rf.predict(X_train)\n    \n    f1train = f1_score(y_train, train_pred)\n    trainf1.append(f1train)\n    \n    y_pred = rf.predict(X_test)\n    \n    f1test = f1_score(y_test, y_pred)\n    testf1.append(f1test)\n\nsns.set(context=\"paper\", font_scale=1.5)\nplt.figure(figsize=(15,10))\nplt.plot(max_features, trainf1, 'b', label= 'Train F1 Score')\nplt.plot(max_features, testf1, 'g', label= 'Test F1 Score')\n\nplt.legend()\nplt.ylabel('F1 Score')\nplt.xlabel('max_features')\nplt.show()","899173c8":"e = list(zip(max_features, testf1))\nf = pd.DataFrame( data = e , columns = ('NFeat','Accuracy'))\nbestnfeat = int(f.iloc[ f['Accuracy'].idxmax(axis = 0) , 0])\nprint('Optimum Max Feature: ' , bestnfeat)","d83af4c0":"X = df_num.drop('Attrition_Yes', axis =1)\ny = df_num['Attrition_Yes']\n\nX_train,X_test, y_train, y_test= train_test_split(X,y, test_size = 0.30, random_state=1)\n\nrftuned = RandomForestClassifier(n_estimators = bestntree, max_depth = bestndepth , max_features = bestnfeat, random_state=1, class_weight=\"balanced\")\nrftuned.fit(X_train, y_train)\ny_pred = rftuned.predict(X_test)","43e1bf27":"rftunedaccuracy = accuracy_score(y_test, y_pred)\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nrftunedroc_auc = auc(fpr, tpr)\n\nrftunedf1_score = f1_score(y_test, y_pred)\n\nrftunedrecall = recall_score(y_test, y_pred)\n\nprint('Model Accuracy: ', rftunedaccuracy)\nprint('ROC_AUC Score: ', rftunedroc_auc)\nprint('F1 Score: ', rftunedf1_score)\nprint('Recall: ', rftunedrecall)","b68c243e":"sns.set(context=\"paper\", font_scale=1.5)\n\nrftcm = confusion_matrix(y_test,y_pred)\n\nax = heatmap = sns.heatmap(rftcm, cmap=\"Blues\", annot= True,fmt=\".0f\") \nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\nplt.title('Confusion Matrix: Random Forest (*Tuned*)')\nplt.ylabel('Real Outcome')\nplt.xlabel('Predicted Outcome')\nplt.yticks(rotation=0)\nplt.show()\n\n#TimerEnd\nrftunedtime = (timer() - rftunedstart)","ade4ec46":"feature_importances = pd.DataFrame(rftuned.feature_importances_, index = X.columns, columns=['importance']).sort_values('importance', ascending=False)\n\nplt.figure(figsize=(15,10))\nsns.set(context=\"paper\", font_scale=1.2)\nfeature_importances['importance'].plot(kind='barh')\nplt.title('Relative Importance of ALL Features')\nplt.gca().invert_yaxis()\nplt.show()","fa0a692f":"feature_importances = pd.DataFrame(rftuned.feature_importances_, index = X.columns, columns=['importance']).sort_values('importance', ascending=False)[0:5]\n\nsns.set(context=\"paper\", font_scale=1.5)\nplt.figure(figsize=(15,10))\nfeature_importances['importance'].plot(kind='barh')\nplt.title('Top 5 Features')\nplt.gca().invert_yaxis()\nplt.show()","3828f532":"X = df_num.drop('Attrition_Yes', axis =1)\ny = df_num['Attrition_Yes']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)\n\nss = preprocessing.StandardScaler()\nX_train = ss.fit_transform(X_train)\nX_test = ss.transform(X_test)","aad50b2c":"#TimerStart\nsvmstart = timer()\n\nsvm = SVC(random_state=1, class_weight = 'balanced')\nsvm.fit(X_train,y_train)\ny_pred = svm.predict(X_test)","061e94bc":"svmaccuracy = accuracy_score(y_test, y_pred)\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nsvmroc_auc = auc(fpr, tpr)\n\nsvmf1_score = f1_score(y_test, y_pred)\n\nsvmrecall = recall_score(y_test, y_pred)\n\nprint('Model Accuracy: ', svmaccuracy)\nprint('ROC_AUC Score: ', svmroc_auc)\nprint('F1 Score: ', svmf1_score)\nprint('Recall: ', svmrecall)","54fc221c":"sns.set(context=\"paper\", font_scale=1.5)\n\nsvmcm = confusion_matrix(y_test,y_pred)\n\nax = heatmap = sns.heatmap(svmcm, cmap=\"Blues\", annot= True,fmt=\".0f\") \nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\n\nplt.title('Confusion Matrix: SVM')\nplt.ylabel('Real Outcome')\nplt.xlabel('Predicted Outcome')\nplt.yticks(rotation=0)\nplt.show()\n\n#TimerEnd\nsvmtime = (timer() - svmstart)","9c975a3a":"#TimerStart\nsvmtunedstart = timer()\n\nparam_grid = {'C': [0.1,0.5,0.6,0.7,0.8,0.9, 1 , 10 , 20],  \n              'gamma': ['auto','scale']} \n  \ngrid = GridSearchCV(SVC(class_weight = 'balanced'), param_grid, refit = True, verbose = 1)\n\ngrid.fit(X_train,y_train)","4ed2995b":"print ('Best Combination of Hyperparameters: ')\ngrid.best_estimator_","de1063b2":"y_pred = grid.best_estimator_.predict(X_test)\n\nsvmtaccuracy = accuracy_score(y_test, y_pred)\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nsvmtroc_auc = auc(fpr, tpr)\n\nsvmtf1_score = f1_score(y_test, y_pred)\n\nsvmtrecall = recall_score(y_test, y_pred)\n\nprint('Model Accuracy: ', svmtaccuracy)\nprint('ROC_AUC Score: ', svmtroc_auc)\nprint('F1 Score: ', svmtf1_score)\nprint('Recall: ', svmtrecall)","bcac2614":"sns.set(context=\"paper\", font_scale=1.5)\n\nsvmtcm = confusion_matrix(y_test,y_pred)\n\nax = heatmap = sns.heatmap(svmtcm, cmap=\"Blues\", annot= True,fmt=\".0f\") \nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\nplt.title('Confusion Matrix: SVM (*tuned*)')\nplt.ylabel('Real Outcome')\nplt.xlabel('Predicted Outcome')\nplt.yticks(rotation=0)\nplt.show()\n\n#TimerEnd\nsvmtunedtime = (timer()- svmtunedstart)","22a0fb8b":"LogReg = ( lraccuracy, lrroc_auc , lrf1_score , lrrecall )\nLogRegT = ( lrtunedaccuracy, lrtunedroc_auc , lrtunedf1_score , lrrecall )\nRF = ( rfaccuracy , rfroc_auc , rff1_score , rfrecall )\nRFT = ( rftunedaccuracy , rftunedroc_auc , rftunedf1_score , rftunedrecall )\nSVC = ( svmaccuracy , svmroc_auc , svmf1_score , svmrecall )\nSVCT = ( svmtaccuracy , svmtroc_auc , svmtf1_score , svmtrecall )","e7da4dca":"modelsummary = pd.DataFrame(data = (LogReg, LogRegT, RF, RFT, SVC, SVCT), \n                         columns = ('Accuracy', 'ROC_AUC Score', 'F1 Score','Recall Score')).mul(100).round(1).astype(str).add('%')\n\nmodelsummary.rename(index={0 : 'Logistic Regression (*Baseline*)' , \n                        1 : 'Logistic Regression (*Tuned*)' , \n                        2 : 'Random Forest (*Baseline*)' , \n                        3 : 'Random Forest (*Tuned*)' , \n                        4 : 'Support Vector Machine (*Baseline*)' , \n                        5 : 'Support Vector Machine (*Tuned*)' },\n                 inplace = True)\n\nmodelsummary['Run Time'] = (lrtime, lrtunedtime, rftime, rftunedtime, svmtime, svmtunedtime)","45f4b63b":"modelsummary","a2114a0c":"<b>Plotting Confusion Matrix\n    <\/b>","00f5d5dc":"### <font color=darkred>6.3.1 Baseline Hyperparameters<\/font>","dc9d26ff":"  <b>Key Observation of Charts Above:<\/b> \n  \n \nMale employees have marginally higher attrition rate compared to Female employees.\n  ","40277254":"### 7.2 Proposal to Management of Company XYZ","a88d6f8f":"## 7. Model Evaluation and Recommendation","afe3a067":"The key objectives of this project are to: \n\n    a. Provide the Management (of Company XYZ) with the Top Factors that have a major influence on why their employees leave the organisation\n    b. Build a Machine Learning model to predict and identify employees with high attrition risks for early Management (of Company XYZ) intervention\n  \nAs such, this is a <b>Classification<\/b> problem with a binary\/categorical predictive outcome (i.e. Attrition:Yes or Attrition:No). The key is to be able to identify top features that can be used to accurately predict employees who are likely to leave the company (i.e. target variable value = 1). Therefore, <b>F1 Score<\/b> should be used as the main metric to evaluate the performance of the model.\n\nModel explainability, or the ability to articulate to Management of Company XYZ the top reasons that the model used for prediction, is a fundamental requirement. A 'black-box' model, even if it is extremely accurate, is an unacceptable outcome. For this reason, I choose not to apply Principal Component Analysis (PCA) as a dimensionality reduction technique as PCA would heavily transform known features into unrecognisable features.\n\nI will be implementing and optimising the following Classification algorithms to compare and evaluate their performance, before recommending the final model.\n\n    1. Logistic Regression\n    2. Random Forest\n    3. Support Vector Machine\n    \n","4c9020b3":"### 6.3 Support Vector Machine","8e0b206f":"### <font color=darkred>6.1.2 Tuned Hyperparameters<\/font>","53fc5e4b":"<b>Plotting Confusion Matrix\n    <\/b>","973b88d3":"<b>Model Performance Scores\n    <\/b>","e66bb7c8":"### <font color=darkred>6.2.1 Baseline Hyperparameters (with Max Depth randomly set at 10)<\/font>","2e5d45b3":"  <b>Conclusion<\/b> \n  \n \nWhile Model Accuracy is decent at 84%, F1 Score is unacceptable at 26%. We should either reject this model or optimise the hyperparameters to seek a higher F1 Score.","dbe418ab":"  <b>Key Observations of Charts Above:<\/b> \n  \n \n  a. Employees who scored 1.0 in the Employee Survey (AvgEmpScore) have higher risk of attrition regardless of MonthlyIncome.\n  \n  b. Employees with Education Level '5' (i.e. Doctorate) have higher risk of attrition, especially if their MonthlyIncome is no low (at overall Median level or lower).\n  ","932b6f68":"<b>Top 5 Features (absolute value of coefficients)\n    <\/b>","c47a2d73":"  <b>Key Observations of Charts Above:<\/b> \n  \n  a. Median Age of employee is 36 years old. Younger employees seem to have a higher risk of attrition.\n  \n  b. Median Distance from office is 7 km. Employees living further from office seem to have a higher risk of attrition.\n  \n  c. Median AvgEmpScore is about 2.7. Employees who scored the Employee Survey with lower scores seem to have higher risk of attrition.","a33e806f":"<font color=black><b>1. general_data.csv<\/b>\n\n\n\n  - <b>Age:<\/b> Age of the employee\n  - <b>Attrition:<\/b> Whether the employee has left the organisation\n  - <b>BusinessTravel:<\/b> How frequent the employee travelled for business in the last year\n  - <b>Department:<\/b> Employee's department \n  - <b>DistanceFromHome:<\/b> Distance between Office and Employee's home (in km)\n  - <b>Education:<\/b> Employee's level of education (1: 'Below College' , 2: 'College' , 3: 'Bachelor's Degree' , 4 : 'Masters Degree' , 5. 'Doctorate')\n  - <b>EducationField:<\/b> Employee's field of education\n  - <b>EmployeeCount:<\/b> Employee count\n  - <b>EmployeeID:<\/b> Unique Employee ID\n  - <b>Gender:<\/b> Employee's gender\n  - <b>JobLevel:<\/b> Employee's job level on a scale of 1 to 5\n  - <b>JobRole:<\/b> Employee's role title\n  - <b>MaritalStatus:<\/b> Employee's marital status\n  - <b>MonthlyIncome:<\/b> Employee's monthly income (in Rupees per month)\n  - <b>NumCompaniesWorked:<\/b> Total number of companies the employee has worked for\n  - <b>Over18:<\/b> Whether the employee is above 18 years of age\n  - <b>PercentSalaryHike:<\/b> Employee's salary hike last year (in percentage points)\n  - <b>StandardHours:<\/b> Employee's standard working hours (duration)\n  - <b>StockOptionLevel:<\/b> Employee's stock option level\n  - <b>TotalWorkingYears:<\/b> Employee's total number of working years (entire life)\n  - <b>TrainingTimesLastYear:<\/b> Number of times employee attended training last year\n  - <b>YearsAtCompany:<\/b> Employee's total number of working years (in the company)\n  - <b>YearsSinceLastPromotion:<\/b> Employee's number of years since last promotion\n  - <b>YearsWithCurrManager:<\/b> Employee's number of years working under current manager\n\n\n<b>2. employee_survey_data.csv<\/b>\n \n- <b>EmployeeID:<\/b> Unique Employee ID\n- <b>EnvironmentSatisfaction:<\/b> Employee's Work Environment Satisfaction Level (1: 'Low' , 2: 'Medium' , 3: 'High' , 4 : 'Very High')\n- <b>JobSatisfaction:<\/b> Employee's Job Satisfaction Level (1: 'Low' , 2: 'Medium' , 3: 'High' , 4 : 'Very High') \n- <b>WorkLifeBalance:<\/b> Employee's Work Life Balance Rating Level (1: 'Low' , 2: 'Medium' , 3: 'High' , 4 : 'Very High') \n\n\n<b>3. manager_survey_data.csv<\/b>\n\n- <b>EmployeeID:<\/b> Unique Employee ID\n- <b>JobInvolvement:<\/b> Employee's Job Involvement Level (1: 'Low' , 2: 'Medium' , 3: 'High' , 4 : 'Very High')  \n- <b>PerformanceRating:<\/b> Employee's performance rating last year\n  \n  ","74c48d8b":"### <b>2.4 Checking the merged dataframe for data attributes and completeness<\/b>","c76038a2":"## <font color=darkblue>3. Data Pre-processing<\/font>","38e56521":"<b>Plotting Confusion Matrix\n    <\/b>","6f257acc":"<b>Retrieving and visualising the Feature Importance\n    <\/b>","f7af9e4b":"<b>Model Performance Scores\n    <\/b>","716c3e81":"<b>Re-creating a new and tuned Random Forest model with these identified input hyperparameters\n    <\/b>","233b9273":"### By Kevin Chan Yongda","835d02ec":"<b>Datasets<\/b>\n\n   There are 3 data files that I will be using for this case study in total.  \n  \n    \n   ","02c50351":"<b>Feature Engineering: Deriving new features relating to Employee Survey Scores and Manager Survey Scores\n    <\/b>","eb7b2463":"<b>Model Performance Evaluation\n    <\/b>","6be8270f":"<b>Performing Train-Test Split and Data Scaling required for Logistic Regression\n    <\/b>","c7ad442b":"<b>Creating Model Object, Training the Model and Creating Predictions from the Model (using default model hyperparameters)\n    <\/b>","2fcaf441":"# <font color=darkblue>Demystifying Employee Attrition with Machine Learning<\/font>","ad88f297":"The <b>Top 5<\/b> Features are in the order of:\n\n    1. YearsWithCurrManager\n    2. YearsSinceLastPromotion\n    3. BusinessTravel_Non-Travel\n    4. TotalWorkingYears\n    5. BusinessTravel_Travel_Rarely\n  ","d5d329b7":"  <b>Conclusion<\/b> \n  \n  By optimising hyperparameter C, the F1 Score improved from 75% to 91%! However, an important downside of using SVM is the fact that the feature importance cannot be retrieved if we use RBF kernal. This goes against our analysis objective of preserving model explainability.\n \n Furthermore, the F1 Score is slightly less superior compared to an optimised Random Forest model (seen earlier).","2ec96bed":"### 6.2 Random Forest Classification","9fdebbad":"### <b>2.3 Merging the 3 dataframes into 1 dataframe based on EmployeeID key<\/b>","c35be358":"<b>Fill all null values with median of the column. I chose median instead of mean because most of these identified columns are ordinal data with categorical underlyings. Using mean may introduce decimal values which may not make sense to these columns.\n    <\/b>","5863b79a":"<b>Check if there are any duplicated employee entries in the dataframe\n    <\/b>","d3ad249b":"  <b>Key Observation of Charts Above:<\/b> \n  \n \n  Attrition rate for Single employees is more than twice that of Married and Divorced employees.\n  ","ba8ac1d0":"  <b>Key Observation of Chart Above:<\/b> \n  \n \nMedian MonthlyIncome is around 50,000 Rupees. Employees who are paid lower than 80,000 Rupees seem to have a slightly higher risk of attrition. However, the observed difference is not as visually significant compared to features explored earlier.","460f11ce":"<b>Creating Model Object, Training the Model and Creating Predictions from the Model (using default model hyperparameters)\n    <\/b>","67b2b351":"<b>Top 5 Most Important Features of Random Forest Model\n    <\/b>","b013b778":"<b>Re-plotting Correlation Matrix to check again after feature selection process above\n    <\/b>","394447ed":"<b>Performing Train-Test Split and Scaling of data for SVM\n    <\/b>","17df59fc":"### 7.1 Summarising scores from all 6 models","82ac89df":"### <b>4.2 Visualising the distribution of key numerical data and its impact on Attrition<\/b>","fed50361":"<b>Performing Train-Test Split\n    <\/b>","e2e298bc":"  <b>Key Observation of Charts Above:<\/b> \n  \n \n  Attrition rate of HR Department is twice that of Sales or R&D Department.\n  ","ffda7a17":"<b>Check if there are any columns with just 1 unique value (not useful)\n    <\/b>","2e7ad9fe":"  <b>Conclusion<\/b> \n  \n  Model accuracy is good at 91%, but F1 Score is only at 75%. Since F1 Score is the key metric, I will proceed to tune the hyperparameter C to see if I am able to obtain a better F1 Score.\n ","65f4d7a5":"  <b>Key Observations of Charts Above:<\/b> \n  \n \n  a. Median YearsAtCompany is 5 years. Newer employees (who have spent lesser years in company) seem to have a higher risk of attrition.\n  \n  b. Median YearsSinceLastPromotion is 1 year. Employees who were recently promoted seem to have higher risk of attrition.\n  \n  c. Median YearsWithCurrManager is 3 years. Employees who have spent lesser years with current manager seem to have higher risk of attrition.","0cc19a97":"## <font color=darkblue>4. Exploratory Data Analysis<\/font>","6de4ca63":"<b>Model Performance Evaluation\n    <\/b>","d459a302":"- n_estimators = 32 (bestntree)\n- max_depth = 16 (bestndepth)\n- max_features = 12 (bestnfeat)","1152c7df":"## <font color=darkblue> 1. Introduction","28e1d09a":"## <font color=darkblue>6. Implementing Machine Learning Algorithms<\/font>","02db6e0c":"<b>Identifying all features that are categorical in nature\n    <\/b>","f6d3c583":"<b>Extracting the best n_estimator value and storing it as a variable for subsequent optimisation\n    <\/b>","9408e1bc":"I would like to propose for the adoption and implementation of my <b>Random Forest (Tuned)<\/b> model. With this model, the Management would be able to effectively predict and identify the list of employees with high attrition risk.\n\nThe adoption of my Random Forest (Tuned) model with a <b>95%<\/b> F1 Score would imply the following:\n\n1. For every <b>100<\/b> employees who truly want to leave the organisation, the model will be able to identify up to <b>91<\/b> of them on a timely basis.\n2. For every <b>100<\/b> employees identified by the model to have high attrition risk, <b>all 100<\/b> of them indeed want to leave the company.\n    \nThe <b>Top 5<\/b> factors that the model is basing the prediction on are in the following order:\n\n1. <b>Age:<\/b> The younger the employee, the more likely he\/she would want to leave the organisation.\n2. <b>TotalWorkingYears:<\/b> The lesser the number of working years the employee has clocked in his\/her life, the more likely he\/she will leave the organisation.\n3. <b>MonthlyIncome:<\/b> The lower the monthly income, the more likely the employee will leave the organisation.\n4. <b>YearsWithCurrManager:<\/b> The lesser the number of years spent with the current manager, the more likely the employee will leave the organisation.\n5. <b>DistanceFromHome:<\/b> The further the distance from office the employee's home is located, the more likely he\/she will leave the organisation.","cb0b549d":"### <font color=darkred>6.1.1 Baseline Hyperparameters<\/font>","83a50cdd":"With a for-loop on 9 n_estimator values , I am seeking to find the most optimum number of trees by running multiple Random Forest iterations to pick n_estimator value with the highest F1 Score.","a7498725":"<b>Key Observation:<\/b> There are some columns with blank\/missing data and will require further pre-processing.","82fbd241":"<b>Retrieving and plotting the coefficient all features in the model to identify Top Features\n    <\/b>","cabd107e":"  <b>Key Observations of Charts Above:<\/b> \n  \n \n  a. Most of the employees who lived further away from office (more than 15 km) are paid around Median MonthlyIncome of 50,000 Rupees. They also exhibit higher risk of attrition.\n  \n  b. Employees who are paid 3x more (i.e. more than 150,000 Rupees) than Median are also exhibiting higher risk of attrition if they live beyond 5 km of the office.\n  ","191cdb05":"### <b>4.1 Visualising the distribution of key categorical data<\/b>","f3fce041":"<b>Creating Model Object, Training the Model and Creating Predictions from the Model (using default model hyperparameters)\n    <\/b>","e442c142":"<b>Creating Model Object, Training the Model and Creating Predictions from the Model (using default model hyperparameters)\n    <\/b>","f47cc99f":"<b>Plotting Confusion Matrix\n    <\/b>","3756bb0a":"<b>Model Performance Evaluation\n    <\/b>","63b64199":"With a for-loop on 20 max_depth values (from 1 to 20), I am seeking to find the most optimum max_depth by running multiple Random Forest iterations to pick max_depth value that will yield the highest F1 Score. \n\nThe variable created earlier containing the most optimum number of trees (bestntree) is introduced into this for-loop.","257e4a25":"### <font color=darkred>6.2.2 Tuned Hyperparameters<\/font>","14865b94":"  <b>Key Observation of Chart Above:<\/b> \n  \n Overall attrition rate of Company XYZ is 16%.\n  ","d7a9ad9b":"<b>Link:<\/b> [HR Analytics Case Study - Kaggle](https:\/\/www.kaggle.com\/vjchoudhary7\/hr-analytics-case-study)","2da2bdba":"<b>Talent Management Strategy<\/b> is dominating the boardroom agenda of many organisations across the globe. Increasingly, it have evolved into a core Business Strategy where organisations strive to retain existing talents and compete to attract more top talents in order to have a competitive edge over their closest business rivals. \n\nThis is especially prevalent in organisations with businesses that are heavily-reliant on human capital for a differentiating edge, such as Financial Institutions. In such organisations, Staff Cost is a substantial driver of Total Cost, and hence Profitability. For mid-size Financial Institutions, Staff Cost represents as high as about <b>60%-70%<\/b> of their Total Cost. In larger Financial Institutions, Staff Cost as a percentage of Total Cost remains sizable at about <b>40%-50%<\/b>.\n\nWhen employees (especially top talents) leave the organisation, they bring about disruptions to client experience, team's morale and project delivery timelines. They will leave and also bring along with them the experience, training and development exposures that the organisation have previously invested in them. To the organisation, this means additional costs will have to be spent again in the acquisiton and subsequent development of replacement hires.\n\nAs such, <b>Employee Attrition<\/b> is a real strategic issue that organisations have to manage in order to protect the overall profitability of the business.  Organisations that master the effective planning and execution of Talent Management Strategy will emerge a winner in this competitive landscape, especially amidst the challenging and peculiar macro-economic setting that we are in today.\n\n\nTherefore, my analysis hereinafter hopes to unravel <b>key insights<\/b> on Employee Attrition Risks (through Company XYZ dataset), with the 2 following objectives:\n\n            a. Provide the Management (of Company XYZ) with the Top Factors that have a major influence on why their employees leave the organisation\n            b. Build a Machine Learning model to predict and identify employees with high attrition risks for early Management (of Company XYZ) intervention","f982fa71":"### 1.3 How is my approach different from the rest?","b76ca2da":"<b>Case Study Extract (Source: Kaggle)<\/b>\n\nA large company named XYZ, employs, at any given point of time, around 4000 employees. However, every year, around 15% of its employees leave the company and need to be replaced with the talent pool available in the job market. ","afeefda4":"<b>Plotting Confusion Matrix\n    <\/b>","87a69570":"## <font color=darkblue>5. Preparing DataFrame for Machine Learning<\/font>","9ab35f26":"### 1.1 Problem Statement","778c8ef8":"<b>Optimising the max depth used in the model (max_depth)\n    <\/b>","bfc07359":"<b>Extracting the best max_depth value and storing it as a variable for subsequent optimisation\n    <\/b>","2c0afcec":"  <b>Conclusion<\/b> \n  \n The model accuracy and F1 Score of Random Forest classification model improved dramatically over the Logistic Regression model built earler. In particular, both F1 Score and Recall Score (more relevant to our analysis) have Scores of 86%. \n \n With these Scores, this model is good enough for use. However. I will attempt to see if I can further increase these Scores by optimising the Hyperparameters of Random Forest algorithm.\n","df4c01ec":"<b>Identify all the columns in the dataframe with null values\n    <\/b>","88572604":"### 1.2 Understanding the Dataset","b9c435d6":"  <b>Conclusion<\/b> \n  \n With the 3 optimum variables derived as hyperparameter inputs, the F1 Score improved from 86% to 95%! Recall Score is also considerably high at 91%. This model is definitely good enough for use.\n","5b216fcc":"<b>Using a for-loop to cycle through the Correlation Matrix to identify 1 leg of feature-pairs with correlation of >0.7\n    <\/b>","975b90f3":"<b>Duplicating the dataframe to keep a copy of the original\n    <\/b>","d1cb55dd":"I will introduce 1 change to the model training by including a definition on class_weight. From the Exploratory Data Analysis, I know that the dataset is not balanced, with only 16% of the records belonging to Class 1 (our subject of interest) and 84% belonging to Class 0.\n\nTherefore, setting a 'balanced' class_weight attempts to correct the model training to shift more weight to Class 1 relative to the default hyperparameter.","faf07807":"### 6.1 Logistic Regression Model","bd7e2f35":"Thus far, I have effectively built 6 models applying 3 different Classification Machine Learning algorithms. Let me now consolidate and summarise the scores of all 6 models in a table to facilitate evaluation and selection of the final model.\n\nAs explained earlier, though I have computed 4 metrics (Accuracy, ROC_AUC, F1 and Recall) for all 6 models, <b>F1 Score<\/b> remains the chosen evaluation metric given the objective of the analysis.","cb50dfe1":"  <b>Key Observation of Charts Above:<\/b> \n  \n \n  Attrition rate of employees who are Frequent Business Travellers is significantly higher than employees who are Rare or Non Business Travellers.\n  ","ad03f203":"## <font color=darkblue>2. Importing Dataset and Libraries<\/font>","ea9d605e":"  <b>Key Observation of Chart Above:<\/b> \n  \n \nThere are more features with negative coefficients compared to positive coefficients. Those features with negative cofficients (e.g. YearsWithCurrManger) have an inverse relationship with Attrition Risk (target variable value = 1). \n\nUsing YearsWithCurrManager as an example, this means that the lesser the years the employee spent with the current manager, the more likely the employee is to leave the organisation. This coincides with the observation made in Exploratory Data Analysis section.\n  ","cbd0aa26":"![employee-retention.jpg](attachment:employee-retention.jpg)","842c9891":"### <b>2.1 Importing the libraries<\/b>","4e92cb02":"  <b>Conclusion<\/b> \n  \nBased on its F1 Score of <b>95.1%<\/b>, <b>Random Forest (Tuned)<\/b> model performs the best among the 6 models.","5dba98f7":"<b>Model Evaluation Scores\n    <\/b>","245c8e59":"### <font color=darkred>6.3.2 Tuned Hyperparameters<\/font>","a4ccb344":"  <b>Conclusion<\/b> \n  \n \nModel Accuracy is dropped from 84% to 69%, but F1 Score improved significantly from 26% to 43% and Recall Score is now at 70% vs. 16% from before. This is still not good enough a model because this means that the model will be falsely predicting too many employees leaving when they did not in reality.","0354028d":"The <b>Top 5<\/b> Features are in the order of:\n\n    1. Age\n    2. TotalWorkingYears\n    3. MonthlyIncome\n    4. YearsWithCurrManager\n    5. DistanceFromHome\n  ","b5a0b026":"<b>Extracting the best max_features value and storing it as a variable for subsequent optimisation\n    <\/b>","dd2e3410":"<b>Plotting Confusion Matrix\n    <\/b>","c9fe4b14":"  <b>Key Observations:<\/b> \n  \n \n  a. There are more Male than Female employees.\n  \n  b. There are more Married employees than Single or Divorced employees.\n  \n  c. Most of the employees are in Research and Development Department. \n  \n  d. Details of the Job Level are not given, but based on data inspection, it is probable that JobLevel increases with seniority (i.e. 5 being the most senior) assuming normal organisational hierarchy.\n  \n  e. Most of the employees do low frequncy business travels. Only a small proportion of employees do no travel for business at all.","7a959106":"<b>Plotting a Correlation Matrix to identify feature-pairs with high correlation with each other\n    <\/b>","df6d5b7a":"  <b>Key Observation of Charts Above:<\/b> \n  \n \nThere are some features that are correlated with each other (e.g. Age vs. YearsWithCurrManager). I will be further analysing collinearity of the features using Correlation Matrix in subsequent segments below.\n  ","be34e227":"<b>1. Holistic Consideration of Classification Modelling Techniques<\/b>\n\nOne of the objectives of this analysis is to predict and identify employees with high attrition risks. As such, this is clearly a Classification problem with a binary\/categorical predictive outcome (i.e. Attrition:Yes or Attrition:No).\n\nMany of those who have attempted this case study focused on applying only Logistic Regression technique in building their machine learning model, perhaps because the case study specifically prescribed for this technique to be applied.\n\nAs part of my learning journey, I have decided to go beyond the use of Logistic Regression technique to compare and evaluate the efficacies of a few Classification techniques before recommending the machine learning model to be used. \n\nI will be using the following Classification Modelling Techniques:\n- Logistic Regression\n- Random Forest Classifier\n- Support Vector Machine\n\n\n<b>2. Using F1 Score as the Model Evaluation Metric<\/b>\n\nMy area of interest is clearly on employees with high attrition risks. Employees who are deemed to have no attrition risks are not the key focus of this analysis. As such, the model evaluation metric should not be centered on the Accuracy Score in the Confusion Matrix. Many of those who have attempted this case study stopped at just evaluating the Accuracy Score.\n\nInstead, I will be using <b>F1 Score<\/b>, which focuses on Class 1 of the Classification outcome, as the key model evaluation metric when I am comparing across the 3 Classification techniques.\n\n\n<b>3. Performing Hyperparameter Tuning<\/b>\n\nRather than just leaving it to the default hyperparameters defined in the algorithm, I will also be performing hyperparameter tuning to optimise the performance of the model.","bb88e726":"<b>Transforming the categorical features into numerical features using get_dummies function in Pandas in order for these features to be used by machine learning algorithms\n    <\/b>","167f0fc2":"<b>Check if there are still any null values\n    <\/b>","e85d2a12":"<b>Performing Train-Test Split and Data Scaling required for Logistic Regression\n    <\/b>","a4d73f86":"### <b>2.2 Importing and performing high-level checks on the 3 datasets<\/b>","c1c19e09":"<b>Optimising the max features used in the model (max_depth)\n    <\/b>","7ece4d3e":"<b>Dropping the features identified above to reduce multi-collinearity\n    <\/b>","7e43b05c":"<b>Optimising number of trees used in the model (n_estimator)\n    <\/b>","fe76c80b":"<b>Optimising C (regularisation parameter) with the use of GridSearchCV\n    <\/b>","c5e76fa0":"With a for-loop on all the input feature counts, I am seeking to find the most optimum max_features by running multiple Random Forest iterations to pick max_feature value that will yield the highest F1 Score. \n\nThe variables created earlier containing the most optimum number of trees (bestntree) and most optimum max_depth (bestdepth) are introduced into this for-loop.","ce2ade42":"<b>Remove columns identified above (i.e. with just 1 unique value)\n    <\/b>","d8213508":"<b>DataFrame is now ready for Machine Learning implementations\n    <\/b>"}}