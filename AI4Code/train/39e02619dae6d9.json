{"cell_type":{"be5ef618":"code","7aeeb62f":"code","76b80e59":"code","5beb3c1e":"code","5ca70b09":"code","c48cf17e":"code","aa1704ff":"code","4aaff1cd":"code","584c9480":"code","013a8bd4":"code","b26cb5b8":"code","cee8cc53":"code","5902e77f":"code","1bb2cb06":"code","3a5ba817":"code","ba20ede2":"code","e1718a82":"code","78c90a8a":"code","dd02780b":"code","51fe9d90":"markdown","75880f2e":"markdown","28c3498f":"markdown","11d23140":"markdown","12aebda3":"markdown","a6179623":"markdown","a4587876":"markdown","2ad59eb6":"markdown","0ed10f15":"markdown","cf4722ea":"markdown","e0d16423":"markdown","a2645dc9":"markdown"},"source":{"be5ef618":"training = 'gpu'\nprofile =  'high'  # for this dtata set to use tpu set these var to low","7aeeb62f":"if training =='tpu':\n    !curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n    !python pytorch-xla-env-setup.py --version 1.7 --apt-packages libomp5 libopenblas-dev","76b80e59":"! pip install pytorch-lightning  #==1.1.5\n","5beb3c1e":"!pip install timm","5ca70b09":"\n\nimport torch\nfrom torch.nn import functional as F\nfrom torch import nn\nfrom pytorch_lightning.core import LightningModule\nfrom sklearn.metrics import label_ranking_average_precision_score\n\nprint(torch.__version__)","c48cf17e":"import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom torchvision import datasets, models, transforms\nfrom torch import nn, optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nfrom torchvision import datasets, models, transforms  \nfrom torch.utils.data.sampler import SubsetRandomSampler  \nfrom torch.utils.data import Dataset, DataLoader\n\nimport pandas as pd\nimport torch.nn.functional as F\n\nimport time\nimport os\nimport time\nimport random\nfrom datetime import datetime\n\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom sklearn import model_selection, metrics\n\nimport timm\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint , EarlyStopping\n","aa1704ff":"def seed_everything(seed):\n    \"\"\"\n    Seeds basic parameters for reproductibility of results\n    \n    Arguments:\n        seed {int} -- Number of the seed\n    \"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if training == 'gpu':\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\nseed_everything(1001)","4aaff1cd":"DATA_PATH = \"..\/input\/ranzcr-clip-catheter-line-classification\"\nTRAIN_PATH = \"..\/input\/ranzcr-clip-catheter-line-classification\/train\/\"\nTEST_PATH = \"..\/input\/ranzcr-clip-catheter-line-classification\/test\/\"\nMODEL_PATH = (\n    \"..\/input\/vit-base-models-pretrained-pytorch\/jx_vit_base_p16_224-80ecf9dd.pth\"\n)\n\n\n\nIMG_SIZE = 384\nif profile =='low':\n    BATCH_SIZE = 8 \n    val_BATCH_SIZE = 2\n    valchecking = 3\n    epoch = 2\n    monit = 'train_loss'\nelse:\n    BATCH_SIZE = 16\n    val_BATCH_SIZE = 16\n    valchecking = 1\n    epoch = 20\n    monit = 'val_loss'","584c9480":"df = pd.read_csv(os.path.join(DATA_PATH, \"train.csv\"))\ndf.head()","013a8bd4":"train_df, valid_df = model_selection.train_test_split(\n    df, test_size=0.2, random_state=42\n)","b26cb5b8":"\n    \nclass TaskDataset(torch.utils.data.Dataset):\n    \"\"\"\n    Helper Class to create the pytorch dataset\n    \"\"\"\n\n    def __init__(self, df, data_path=DATA_PATH, mode=\"train\", transforms=None):\n        super().__init__()\n        self.df_data = df\n        self.data_path = data_path\n        self.transforms = transforms\n        self.mode = mode\n        self.data_dir = TRAIN_PATH if mode == \"train\" else TEST_PATH\n\n    def __len__(self):\n        return len(self.df_data)\n\n    def __getitem__(self, index):\n        img_name = self.df_data.StudyInstanceUID.values[index]\n        label = self.df_data[self.df_data.StudyInstanceUID == img_name].values.tolist()[0][1:-1]\n        img_path = os.path.join(self.data_dir, img_name + \".jpg\" )\n        img =   Image.open(img_path).convert(\"RGB\")\n        label = torch.tensor(label,dtype= torch.float32) \n        if self.transforms is not None:\n            image = self.transforms(img)\n\n        return image, label","cee8cc53":"# Imagenet means and stds\nmean = [0.485, 0.456, 0.406]\nstd  = [0.229, 0.224, 0.225]\n        \ntransforms_train = transforms.Compose([\n                            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n                         transforms.RandomRotation(45),\n                            transforms.RandomHorizontalFlip(),\n                            transforms.RandomVerticalFlip(),\n                            transforms.RandomResizedCrop(IMG_SIZE),\n                            transforms.ToTensor(),\n                            transforms.Normalize(mean=mean, std=std),\n                           # transforms.Grayscale(num_output_channels=1)\n                        ]\n                    )\n\ntransforms_valid = transforms.Compose(\n                        [\n                            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n                            transforms.ToTensor(),\n                            transforms.Normalize(mean=mean, std=std),\n                            #transforms.Grayscale(num_output_channels=1)\n                        ]\n                    )        \n\n","5902e77f":"train_dataset = TaskDataset(train_df, transforms=transforms_train)\nvalid_dataset = TaskDataset(valid_df, transforms=transforms_valid)","1bb2cb06":"del df\ndel train_df\ndel valid_df","3a5ba817":"train_loader =  DataLoader( dataset=train_dataset,\n        batch_size=BATCH_SIZE, \n      \n        drop_last=True,\n        num_workers=8,shuffle=True)\nvalid_loader =    DataLoader(  dataset=valid_dataset,\n        batch_size=val_BATCH_SIZE ,\n        \n        drop_last=True,\n        num_workers=8,shuffle=False)\n","ba20ede2":"inputs, classes = next(iter(train_loader))  \nprint(classes)","e1718a82":"class Mamonmodel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        i = 0\n        self.lr=1e-3\n        self.wd = 1e-6\n        self.schd = 'CosineAnnealingWarmRestarts'\n        #self.best_model_wts = copy.deepcopy(model.state_dict())\n        self.best_loss = np.Inf\n\n        basemodel = torch.hub.load('facebookresearch\/deit:main', 'deit_base_patch16_384', pretrained=True)\n        for child in basemodel.blocks.children():\n            if i < 3:\n                for param in child.parameters():\n                    param.requires_grad = False\n            else:\n                for param in child.parameters():\n                    param.requires_grad = True\n            i +=1\n        basemodel.head  =  nn.Sequential( \n                                nn.Dropout(0.25), \n             nn.Linear(768, 11)\n\n                    )\n        self.model = basemodel\n        \n    def forward(self,x):\n        x = self.model(x)\n        return x\n    def training_step(self,batch,batch_idx):\n        x , y = batch\n        y_hat = self.model(x)\n        loss = F.binary_cross_entropy_with_logits(y_hat , y)\n        #scor = label_ranking_average_precision_score(y.cpu(), torch.sigmoid(y_hat).cpu())\n        self.log('train_loss', loss)\n        #self.log('train_score', scor)\n        return loss  \n    def validation_step(self,batch,batch_idx):\n        x , y = batch\n        y_hat = self.model(x)\n        loss = F.binary_cross_entropy_with_logits(y_hat , y)\n        #scor = label_ranking_average_precision_score(y.cpu(), torch.sigmoid(y_hat).cpu())\n        #self.log('val_score', scor)\n        self.log('val_loss', loss) \n        if self.best_loss > loss:\n            self.best_loss = loss\n            torch.save(self.model.state_dict(), 'Deit-bestwigh-ep-'+str(self.current_epoch)+'.pth')\n        return loss    \n    def configure_optimizers(self):\n        optimizer = optim.AdamW(\n            self.model.parameters(), lr=self.lr, weight_decay=self.wd\n        )\n        if self.schd == 'ReduceLROnPlateau':\n            scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=3,\n                                          verbose=True)\n        elif self.schd == 'CosineAnnealingLR':\n            scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n        elif self.schd == 'CosineAnnealingWarmRestarts':\n            scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=0.001, last_epoch=-1) \n\n        return {'optimizer': optimizer, 'lr_scheduler': scheduler  ,  'monitor':monit }\n        ","78c90a8a":"# init model\nmemomodel = Mamonmodel()","dd02780b":"'''\n{val_loss:.2f}\n'''\nearly_stop_callback = EarlyStopping(\n   monitor='val_loss',\n    min_delta=0.00,\n   patience=3,\n    verbose=False,\n   mode='min'\n )\nif training == 'gpu':\n    checkpoint_callback = ModelCheckpoint(filename='{epoch}-gpumodel.pth', \n    verbose=True,\n    monitor='val_loss',\n    mode='min')\n    trainer = pl.Trainer(min_epochs=1 , max_epochs=epoch, check_val_every_n_epoch=valchecking,\n            \n                        gpus=-1,callbacks=[checkpoint_callback , early_stop_callback]\n    ) \n    \nelif training == 'tpu':\n    print('now we n tpu')\n    checkpoint_callback = ModelCheckpoint(filename='{epoch}-tpumodel.pth',  \n    verbose=True,\n    monitor=monit,\n    mode='min')\n    trainer = pl.Trainer(tpu_cores=8,min_epochs=1,num_sanity_val_steps=0 , check_val_every_n_epoch=valchecking , max_epochs=epoch,  callbacks=[checkpoint_callback])\nelse:\n    checkpoint_callback = ModelCheckpoint(filename='{epoch}-cpumodel.pth',  \n    verbose=True,\n    monitor='val_loss',\n    mode='min')\n    trainer = pl.Trainer(min_epochs=1 , max_epochs=epoch,check_val_every_n_epoch=valchecking,  callbacks=[checkpoint_callback , early_stop_callback])\n\n    \n\n\ntrainer.fit(memomodel, train_loader , valid_loader)","51fe9d90":"# seeding everthing","75880f2e":"# library  needed by lightining","28c3498f":"# Data set class for getting data from path","11d23140":"# in these code we defin pytorchlightning model\n\n\n","12aebda3":"# split data","a6179623":"# configartions","a4587876":"#  make datasets and data loaders","2ad59eb6":"# setting kernel type\n\nto use tpu  set training = 'tpu'   for gpu  training = 'gpu'   for cpu training = 'cpu'\n\n# **Note for TPU\n**while these notebook is ok to work in tpu  ,  the xla for pytorch As per they have announced, its still under development and so for operations not supported they automatically switch to CPU so it might get even slower than GPU.\n\nso for some reson in these notebook when using this data sets where  we have  large size of images when  image resize and transformations  it will use cpu for that transformation and do training in tpu and these will cost memory to work around with these either you save images after transformation and read them diractly into the model or  i just skip val phase and also  make batches very small in tpu \nto  use tpu in these dataset\njust set profile='low'  other  can set profile=''high'","0ed10f15":"# install lighting and xla for tpu","cf4722ea":"# read csv file with pandas","e0d16423":"# other libraarys","a2645dc9":"# **  transforming data doing augmention **"}}