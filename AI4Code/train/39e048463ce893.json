{"cell_type":{"26c06468":"code","caeb3da2":"code","4675d34a":"code","ba7cdd98":"code","bdb34690":"code","fcd1460e":"code","9dab344a":"code","7cf8e804":"code","e10ba498":"code","2390753e":"code","2cfb3d2d":"code","509b844d":"code","15206adf":"code","01a28f2c":"code","39ccb819":"code","2e8b9040":"code","30f0ce66":"code","fb735dfc":"code","39f2d7fc":"code","20ebec52":"code","cb207f47":"code","a53b3093":"code","28d83b5a":"code","f6625553":"code","2bbe76b0":"code","2d0b7c56":"code","a8945cff":"code","2b622fb4":"code","8b3ccc60":"code","3845c9ba":"code","05b2219d":"code","b3f03300":"code","0eb29822":"code","d2fc3466":"code","1ebde5b9":"code","430dbaba":"code","993356c8":"code","3a1e381b":"code","67f5cb34":"code","8568b865":"code","f173f6e2":"code","0dfdca03":"code","43c55282":"code","f8f229c1":"code","e8db1482":"code","898a7e8e":"code","624faf9e":"code","86808931":"code","48cb6914":"code","86c4bdfc":"code","4289848b":"code","1faec649":"code","658b4da3":"code","0cf69883":"code","09a766e7":"code","02f1af16":"code","fd9de91a":"code","adbf318e":"code","fd583ab9":"code","73d50a4b":"code","64acfb38":"code","7bc5c3c7":"code","d07f52fa":"code","ed54cdfd":"code","294bcec6":"code","0ff8a90a":"code","8641e4ab":"code","1a1f05d3":"code","f735ed32":"code","9f11b883":"code","f21add93":"code","738b6147":"code","0f7cbcc1":"code","e2110ae9":"code","710a9354":"markdown","4d18367e":"markdown","6d72e85d":"markdown","c60e472a":"markdown","b8406a98":"markdown","84f42552":"markdown","8acef93e":"markdown","86031c71":"markdown","9d826328":"markdown","ef6a7b84":"markdown","28cb6c23":"markdown","a1591418":"markdown","32bd9332":"markdown","82a0b1d5":"markdown","5a635ed8":"markdown","70a2d0fd":"markdown","731da305":"markdown","3fd82062":"markdown","8310001a":"markdown","d1e74176":"markdown","1920b367":"markdown","8415a373":"markdown","a79c5521":"markdown","2c135fef":"markdown","95df00e4":"markdown","047335f9":"markdown","8e4a6795":"markdown","ef52d7f1":"markdown","8075055e":"markdown","38b0f91f":"markdown","568a56b2":"markdown","87e3acc9":"markdown","00b6e894":"markdown","ad2b44a0":"markdown","c293bee6":"markdown","0cf95dfd":"markdown","24abdfe7":"markdown","9e30edd4":"markdown","baf277b0":"markdown","e5e72a2c":"markdown","cb9c8f10":"markdown","2ced9ae7":"markdown","5eefc53c":"markdown","3ae43e77":"markdown","fd947137":"markdown","8984511e":"markdown","c63b3237":"markdown","86bee89d":"markdown","c5f339ff":"markdown","db55acbc":"markdown","944dbc97":"markdown","3fde4709":"markdown","ae75c151":"markdown","d92a584e":"markdown","683ae8d7":"markdown","d8940fee":"markdown","e9c74cc0":"markdown"},"source":{"26c06468":"!pip install swifter","caeb3da2":"# import shutil\n# shutil.rmtree(\"\/kaggle\/working\")","4675d34a":"import requests, zipfile, io\nimport pandas as pd\nimport numpy as np\nimport glob\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport swifter\nfrom IPython.display import display\n\n%matplotlib inline\n\nsns.set(style=\"whitegrid\")\npd.set_option('float_format', '{:f}'.format)\n","ba7cdd98":"files = [\n        {\"year\": \"2018\",\n        \"months\": ['01','02','03','04','05','06','07','08', '09', '10','11','12'],\n        \"template_url\" : 'https:\/\/s3.amazonaws.com\/baywheels-data\/%s%s-fordgobike-tripdata.csv.zip',\n        \"filetype\":'zip'},\n        {\"year\": \"2019\",\n        \"months\": ['01','02','03','04'],\n        \"template_url\" : 'https:\/\/s3.amazonaws.com\/baywheels-data\/%s%s-fordgobike-tripdata.csv.zip',\n        \"filetype\":'zip'},\n        {\"year\": \"2019\",\n        \"months\": ['05','06','07','08', '09', '10'],\n        \"template_url\" : 'https:\/\/s3.amazonaws.com\/baywheels-data\/%s%s-baywheels-tripdata.csv.zip',\n        \"filetype\":'zip'},\n    {\"year\":\"2017\",\n        \"months\": [''], \n        \"template_url\" : 'https:\/\/s3.amazonaws.com\/baywheels-data\/%s%s-fordgobike-tripdata.csv',\n         \"filetype\":'csv'},\n]\n\n# looping on the files based on the dictionary, read the stream in the memory, and extract it into data folder.\n\nfor file in files:\n    for month in file[\"months\"]:\n        url = file[\"template_url\"] % (file[\"year\"], month)\n        response = requests.get(url)\n        if file[\"filetype\"] == 'zip':\n            z = zipfile.ZipFile(io.BytesIO(response.content))\n            z.extractall('source')\n        else:\n            with open(os.path.join('source', \"%s%s-fordgobike-tripdata.csv\" % (file[\"year\"], month)), mode='wb') as file:\n                file.write(response.content)\n        ","bdb34690":"# data_files = []\n# for (dirpath, dirnames, filenames) in os.walk('\/kaggle\/working\/source'):\n#     data_files.extend(filenames)\n#     break\n# for file in data_files:\n#     file_structure = file.split('-')\n#     old = os.path.join(\"\/kaggle\/working\/source\", file)\n#     print(file_structure)\n#     new = os.path.join(\"\/kaggle\/working\/source\", file_structure[0]+'_'+file_structure[2])\n#     os.rename(old,new)","fcd1460e":"path = r'\/kaggle\/working\/source' # use your path\nall_files = glob.glob(path + \"\/*.csv\")\n\nli = []\n\nfor filename in all_files:\n    if filename == '\/kaggle\/working\/source\/201907-fordgobike-tripdata.csv':\n        df = pd.read_csv(filename, index_col=None, sep=\";\",low_memory=False)\n    else:\n        df = pd.read_csv(filename, index_col=None,low_memory=False)\n    li.append(df)","9dab344a":"li_columns = ['duration_sec',\n           'start_time','start_station_id','start_station_name', 'start_station_latitude','start_station_longitude',\n           'end_time'  ,'end_station_id'  ,'end_station_name'  ,'end_station_latitude'   ,'end_station_longitude'  ,\n           'bike_id','user_type','member_birth_year','member_gender','bike_share_for_all_trip','rental_access_method'\n          ]","7cf8e804":"for i in li:\n    deltacolumns = list(set(li_columns) - set(i.columns.tolist()))\n    for column in deltacolumns:\n        i[column] = np.nan","e10ba498":"df = pd.concat(li, axis=0, ignore_index=True, sort=True)","2390753e":"df = df[li_columns].sort_values(by=['start_time', 'end_time'])","2cfb3d2d":"df.reset_index(inplace=True)","509b844d":"df[li_columns].to_csv('all_tripdata.csv')","15206adf":"df = pd.read_csv('all_tripdata.csv',low_memory=False)[li_columns]","01a28f2c":"df['start_station_id'] = df.start_station_id.astype('Int64').astype(str)\ndf['end_station_id'] = df.end_station_id.astype('Int64').astype(str)\ndf.loc[df['start_station_id'] == 'nan',['start_station_id']] = np.nan\ndf.loc[df['end_station_id'] == 'nan',['end_station_id']]  = np.nan","39ccb819":"df['start_time'] = pd.to_datetime(df['start_time'], format=\"%Y-%m-%d %H:%M:%S.%f\")\ndf['end_time'] = pd.to_datetime(df['end_time'], format=\"%Y-%m-%d %H:%M:%S.%f\")","2e8b9040":"df['bike_id'] = df.bike_id.astype(str)\ndf['member_birth_year'] = df.member_birth_year.astype('Int64').astype(str)","30f0ce66":"display(df.head(10))\ndisplay(df.info())\ndisplay((df.isna().mean() * df.shape[0]).astype(int))\ndisplay(df.describe())","fb735dfc":"display(df.query('start_station_latitude == 0 or start_station_longitude == 0').start_station_id.unique())\ndisplay(df.query('end_station_latitude == 0 or end_station_longitude == 0').end_station_id.unique())","39f2d7fc":"start_stations_locations = df.query(\"start_station_id not in ('420', '449')\")[['start_station_id', 'start_station_latitude',\"start_station_longitude\"]]\nend_stations_locations = df.query(\"end_station_id not in ('420', '449')\")[['end_station_id', 'end_station_latitude',\"end_station_longitude\"]]\nstart_stations_locations.rename(columns={\"start_station_id\": \"station_id\", \"start_station_latitude\": \"station_latitude\", \"start_station_longitude\":\"station_longitude\"},inplace=True)\nend_stations_locations.rename(columns={\"end_station_id\": \"station_id\", \"end_station_latitude\": \"station_latitude\", \"end_station_longitude\":\"station_longitude\"},inplace=True)\nstations_locations = pd.concat([start_stations_locations,end_stations_locations])\nstations_locations_max = stations_locations.groupby('station_id').max()\nstations_locations_min = stations_locations.groupby('station_id').min()\nstations_locations_max_min = stations_locations_max.join(stations_locations_min, lsuffix='_max', rsuffix='_min')\nstations_locations_max_min_wo_nan = stations_locations_max_min.query('station_id != \"nan\"')\nstations_locations_max_min_wo_nan = stations_locations_max_min_wo_nan.round(4)\nstations_locations_max_min_wo_nan['matched_latitude'] = stations_locations_max_min_wo_nan.station_latitude_max == stations_locations_max_min_wo_nan.station_latitude_min\nstations_locations_max_min_wo_nan['matched_longitude'] = stations_locations_max_min_wo_nan.station_longitude_max == stations_locations_max_min_wo_nan.station_longitude_min\nstations_locations_max_min_wo_nan_unmatched = stations_locations_max_min_wo_nan.query('matched_latitude == False or matched_longitude == False')\nstations_locations_max_min_wo_nan_unmatched.plot.scatter('station_longitude_max','station_latitude_max',figsize=(10,5), c=\"blue\")\nstations_locations_max_min_wo_nan_unmatched.plot.scatter('station_longitude_min','station_latitude_min',figsize=(10,5), c=\"red\")","20ebec52":"stations_locations_max_min_wo_nan_unmatched.query('station_latitude_max > 44')","cb207f47":"stations_locations_max_min_wo_nan_unmatched_filtered = stations_locations_max_min_wo_nan_unmatched.query('station_latitude_max < 44')\nstations_locations_max_min_wo_nan_unmatched_filtered.plot.scatter('station_longitude_max','station_latitude_max',figsize=(10,5),c=\"blue\")\nstations_locations_max_min_wo_nan_unmatched_filtered.plot.scatter('station_longitude_min','station_latitude_min',figsize=(10,5),c=\"red\")","a53b3093":"actual_duration_second = df[['duration_sec', 'start_time', 'end_time']].copy()\nactual_duration_second['actual_duration_sec'] = (actual_duration_second['end_time'] - actual_duration_second['start_time']).dt.seconds\nactual_duration_second.query('duration_sec != actual_duration_sec').shape[0]","28d83b5a":"display(\n    df.member_gender.unique(),\n    df.bike_share_for_all_trip.unique(),\n    df.rental_access_method.unique())\n","f6625553":"df[[\"member_birth_year\"]].astype(float).boxplot(column=\"member_birth_year\")","2bbe76b0":"df[[\"member_birth_year\"]].astype(float).query('member_birth_year > 1960').boxplot(column=\"member_birth_year\")","2d0b7c56":"display(df[[\"member_birth_year\"]].astype(float).query('member_birth_year <= 1960 and member_birth_year > 1940').boxplot(column=\"member_birth_year\"))","a8945cff":"display(df[[\"member_birth_year\"]].astype(float).query('member_birth_year <= 1940').boxplot(column=\"member_birth_year\"))","2b622fb4":"df_clean = df.copy()","8b3ccc60":"display(df_clean.query('start_station_id == \"420\"'),df_clean.query('end_station_id == \"420\"'))","3845c9ba":"df_clean = df_clean.query('start_station_id != \"420\" and end_station_id != \"420\"')","05b2219d":"display(df_clean.query('start_station_id == \"449\"'),df_clean.query('end_station_id == \"449\"'))","b3f03300":"df_clean = df_clean.query('start_station_id != \"449\" and end_station_id != \"449\"')","0eb29822":"display(df_clean.query('start_station_id == \"408\"'),df_clean.query('end_station_id == \"408\"'))","d2fc3466":"df_clean.loc[df_clean[\"end_station_id\"] == \"408\", \n             [\"start_station_latitude\",\"start_station_longitude\",\n              \"end_station_latitude\",\"end_station_longitude\"]] = [37.718513,-122.388320,37.718513, -122.388320 ]","1ebde5b9":"start_stations_locations = df_clean[['start_station_id', 'start_station_latitude',\"start_station_longitude\"]].copy()\nend_stations_locations = df_clean[['end_station_id', 'end_station_latitude',\"end_station_longitude\"]].copy()\nstart_stations_locations.rename(columns={\"start_station_id\": \"station_id\", \"start_station_latitude\": \"station_latitude\", \"start_station_longitude\":\"station_longitude\"},inplace=True)\nend_stations_locations.rename(columns={\"end_station_id\": \"station_id\", \"end_station_latitude\": \"station_latitude\", \"end_station_longitude\":\"station_longitude\"},inplace=True)\nstations_locations = pd.concat([start_stations_locations,end_stations_locations])\nstations_locations_median = stations_locations.groupby('station_id').median()","430dbaba":"start_nan_stations = df_clean.query('start_station_id != start_station_id').reset_index()[['index', 'start_station_latitude', 'start_station_longitude']]\nend_nan_stations = df_clean.query('end_station_id != end_station_id').reset_index()[['index', 'end_station_latitude', 'end_station_longitude']]","993356c8":"def start_distance_calculation(row):\n    delta_lat = (stations_locations_median['station_latitude'] - row['start_station_latitude']) ** 2\n    delta_long = (stations_locations_median['station_longitude'] - row['start_station_longitude']) ** 2\n    distance = np.sqrt(delta_lat + delta_long)\n    new_cols = pd.Series(data=[distance.min(), distance.idxmin()], index=['start_min_distance_value', 'start_min_distance_station']) \n    result = pd.concat([row, new_cols])\n    return result\n\n\nstart_nan_stations_nearest = start_nan_stations.swifter.apply(start_distance_calculation, axis=1)\nstart_nan_stations_nearest[\"index\"] = start_nan_stations_nearest[\"index\"].astype(int)","3a1e381b":"def end_distance_calculation(row):\n    delta_lat = (stations_locations_median['station_latitude'] - row['end_station_latitude']) ** 2\n    delta_long = (stations_locations_median['station_longitude'] - row['end_station_longitude']) ** 2\n    distance = np.sqrt(delta_lat + delta_long)\n    new_cols = pd.Series(data=[distance.min(), distance.idxmin()], index=['end_min_distance_value', 'end_min_distance_station']) \n    result = pd.concat([row, new_cols])\n    return result\n\n\nend_nan_stations_nearest = end_nan_stations.swifter.apply(end_distance_calculation, axis=1)\nend_nan_stations_nearest[\"index\"] = end_nan_stations_nearest[\"index\"].astype(int)","67f5cb34":"start_nan_stations_accepted = start_nan_stations_nearest.query('start_min_distance_value < 0.01').set_index(\"index\")[[\"start_min_distance_value\", \"start_min_distance_station\"]]\nend_nan_stations_accepted = end_nan_stations_nearest.query('end_min_distance_value < 0.01').set_index(\"index\")[[\"end_min_distance_value\", \"end_min_distance_station\"]]","8568b865":"df_clean.index.name = \"index\"","f173f6e2":"df_clean = df_clean.join(start_nan_stations_accepted)\ndf_clean.loc[df_clean[\"start_min_distance_value\"] == df_clean[\"start_min_distance_value\"], [\"start_station_id\"]]= df_clean[\"start_min_distance_station\"]\ndf_clean.drop(columns=['start_min_distance_value', 'start_min_distance_station'],inplace=True)","0dfdca03":"df_clean = df_clean.join(end_nan_stations_accepted)\ndf_clean.loc[df_clean[\"end_min_distance_value\"] == df_clean[\"end_min_distance_value\"], [\"end_station_id\"]]= df_clean[\"end_min_distance_station\"]\ndf_clean.drop(columns=['end_min_distance_value', 'end_min_distance_station'],inplace=True)","43c55282":"df_clean = df_clean.query('start_station_id == start_station_id')\ndf_clean = df_clean.query('end_station_id == end_station_id')","f8f229c1":"def fix_station_lat_long(row):\n    df_clean.loc[df_clean[\"start_station_id\"] == row[\"station_id\"], [\"start_station_latitude\",\"start_station_longitude\"]]  =  [row.station_latitude,row.station_longitude]\n    df_clean.loc[df_clean[\"end_station_id\"]   == row[\"station_id\"], [\"end_station_latitude\",\"end_station_longitude\"]]    =  [row.station_latitude, row.station_longitude]\n\nstations_locations_median_rounded_4 = stations_locations_median.reset_index().round(4)    \n_ignore = stations_locations_median_rounded_4.swifter.apply(fix_station_lat_long, axis=1)","e8db1482":"start_stations_locations_clean = df_clean[['start_station_id', 'start_station_latitude',\"start_station_longitude\"]].copy()\nend_stations_locations_clean = df_clean[['end_station_id', 'end_station_latitude',\"end_station_longitude\"]].copy()\nstart_stations_locations_clean.rename(columns={\"start_station_id\": \"station_id\", \"start_station_latitude\": \"station_latitude\", \"start_station_longitude\":\"station_longitude\"},inplace=True)\nend_stations_locations_clean.rename(columns={\"end_station_id\": \"station_id\", \"end_station_latitude\": \"station_latitude\", \"end_station_longitude\":\"station_longitude\"},inplace=True)\nstations_locations_clean = pd.concat([start_stations_locations_clean,end_stations_locations_clean])\nstations_locations_max_clean = stations_locations_clean.groupby('station_id').max()\nstations_locations_min_clean = stations_locations_clean.groupby('station_id').min()\nstations_locations_max_min_clean = stations_locations_max_clean.join(stations_locations_min_clean, lsuffix='_max', rsuffix='_min')\nstations_locations_max_min_wo_nan_clean = stations_locations_max_min_clean.query('station_id != \"nan\"').round(4).copy()\nstations_locations_max_min_wo_nan_clean['matched_latitude'] = stations_locations_max_min_wo_nan_clean.station_latitude_max == stations_locations_max_min_wo_nan_clean.station_latitude_min\nstations_locations_max_min_wo_nan_clean['matched_longitude'] = stations_locations_max_min_wo_nan_clean.station_longitude_max == stations_locations_max_min_wo_nan_clean.station_longitude_min\nstations_locations_max_min_wo_nan_unmatched_clean = stations_locations_max_min_wo_nan_clean.query('matched_latitude == False or matched_longitude == False')\nstations_locations_max_min_wo_nan_unmatched_clean","898a7e8e":"main_map_edges = (-122.5,-121.8,37.2,37.9)\nmain_map_image = plt.imread('..\/input\/map-images\/map.png')\nfig, ax = plt.subplots(figsize = (10,10))\nax.scatter(stations_locations_max_min_clean.station_longitude_max,stations_locations_max_min_clean.station_latitude_max,zorder=1, alpha= 0.8, c='b', s=20)\nax.set_xlim(main_map_edges[0],main_map_edges[1])\nax.set_ylim(main_map_edges[2],main_map_edges[3])\nax.imshow(main_map_image, zorder=0, extent = main_map_edges, aspect= 'equal')","624faf9e":"df_clean[\"start_station_city\"] = np.nan\ndf_clean[\"end_station_city\"] = np.nan\ndf_clean.loc[df_clean['start_station_longitude'] < -122.35, \"start_station_city\"] = \"SAN FRANSISCO\"\ndf_clean.loc[(df_clean['start_station_longitude'] > -122.35) & (df_clean['start_station_longitude'] < -122.1 ), \"start_station_city\"] = \"OAKLAND\"\ndf_clean.loc[df_clean['start_station_longitude'] > -122.1 , \"start_station_city\"] = \"SAN JOSE\"\ndf_clean.loc[df_clean['end_station_longitude'] < -122.35, \"end_station_city\"] = \"SAN FRANSISCO\"\ndf_clean.loc[(df_clean['end_station_longitude'] > -122.35) & (df_clean['end_station_longitude'] < -122.1 ), \"end_station_city\"] = \"OAKLAND\"\ndf_clean.loc[df_clean['end_station_longitude'] > -122.1 , \"end_station_city\"] = \"SAN JOSE\"","86808931":"df_clean.query('start_station_city != end_station_city').start_station_city.count()","48cb6914":"start_stations_names = df_clean[['start_station_id', 'start_station_name']].copy()\nend_stations_names = df_clean[['end_station_id', 'end_station_name']].copy()\nstart_stations_names.rename(columns={\"start_station_id\": \"station_id\", \"start_station_name\": \"station_name\"},inplace=True)\nend_stations_names.rename(columns={\"end_station_id\": \"station_id\", \"end_station_name\": \"station_name\"},inplace=True)\nstations_names = pd.concat([start_stations_names,end_stations_names])\nstations_names_wo_duplicates = stations_names.drop_duplicates()\nstations_many_names = stations_names_wo_duplicates.groupby('station_id').count().query('station_name > 1')","86c4bdfc":"stations_many_names.shape[0]","4289848b":"stations_names_requiers_offline_fix = stations_names_wo_duplicates.query(f'station_id in {stations_many_names.index.tolist()}').sort_values('station_id').set_index('station_id').query('station_name == station_name')\nstations_names_requiers_offline_fix.to_csv('stations_names_error.csv')","1faec649":"station_names_fixed = pd.read_csv(\"..\/input\/visual-assessment\/stations_names_fixed.csv\")","658b4da3":"station_names_fixed[\"station_id\"] = station_names_fixed.station_id.astype(str)","0cf69883":"stations_names_wo_duplicates_fixed = stations_names_wo_duplicates.query('station_name == station_name').set_index('station_id').join(station_names_fixed.set_index('station_id'),rsuffix=('_fixed')).reset_index()\nstations_names_wo_duplicates_fixed.loc[stations_names_wo_duplicates_fixed['station_name_fixed'] == stations_names_wo_duplicates_fixed['station_name_fixed'], \"station_name\"] = stations_names_wo_duplicates_fixed['station_name_fixed'] \nstations_names_wo_duplicates_fixed = stations_names_wo_duplicates_fixed.drop_duplicates().set_index(\"station_id\")[[\"station_name\"]]","09a766e7":"def fix_station_names(row):\n    df_clean.loc[df_clean[\"start_station_id\"] == row[\"station_id\"], [\"start_station_name\"]]  =  [row.station_name]\n    df_clean.loc[df_clean[\"end_station_id\"]   == row[\"station_id\"], [\"end_station_name\"]]    =  [row.station_name]\n\n_ignore = stations_names_wo_duplicates_fixed.reset_index().swifter.apply(fix_station_names, axis=1)","02f1af16":"df_clean.loc[:,['duration_sec']] = (df_clean['end_time'] - df_clean['start_time']).dt.seconds","fd9de91a":"actual_duration_second = df_clean[['duration_sec', 'start_time', 'end_time']].copy()\nactual_duration_second['actual_duration_sec'] = (actual_duration_second['end_time'] - actual_duration_second['start_time']).dt.seconds\nactual_duration_second.query('duration_sec != actual_duration_sec').shape[0]","adbf318e":"df_clean.duration_sec.describe()","fd583ab9":"df_clean.member_gender.replace('M','Male', inplace=True)\ndf_clean.member_gender.replace('F','Female', inplace=True)\ndf_clean.member_gender.replace('O','Other', inplace=True)\ndf_clean.member_gender.replace('?','Other', inplace=True)\ndf_clean.member_gender.fillna(\"Other\", inplace=True)","73d50a4b":"df_clean.member_gender.unique()","64acfb38":"df_clean.member_birth_year = df_clean.member_birth_year.astype(float)\ndf_clean.boxplot(column=\"member_birth_year\")","7bc5c3c7":"df_clean.query('member_birth_year < 1960').member_birth_year.hist(bins=30)","d07f52fa":"df_clean.query('member_birth_year < 1960 and member_birth_year > 1945').boxplot(column=\"member_birth_year\")","ed54cdfd":"df_clean.query('member_birth_year <= 1945').boxplot(column=\"member_birth_year\")","294bcec6":"df_clean.query('member_birth_year < 1945').member_birth_year.hist(bins=20)","0ff8a90a":"df_clean.query('member_birth_year < 1945').shape[0]","8641e4ab":"df_clean.loc[df_clean[\"member_birth_year\"] < 1945, \"member_birth_year\" ] = np.nan\ndf_clean.query('member_birth_year < 1945').shape[0]","1a1f05d3":"df_clean.boxplot(column=\"member_birth_year\")","f735ed32":"df_clean[\"member_age_group\"] = np.nan","9f11b883":"df_clean.loc[(df_clean[\"member_birth_year\"] <= 2001) & (df_clean[\"member_birth_year\"] > 1992), \"member_age_group\"] = \"18-26\"\ndf_clean.loc[(df_clean[\"member_birth_year\"] <= 1992) & (df_clean[\"member_birth_year\"] > 1979), \"member_age_group\"] = \"27-39\"\ndf_clean.loc[(df_clean[\"member_birth_year\"] <= 1979) & (df_clean[\"member_birth_year\"] > 1962), \"member_age_group\"] = \"40-57\"\ndf_clean.loc[(df_clean[\"member_birth_year\"] <= 1862), \"member_age_group\"] = \"older than 57\"","f21add93":"df_clean.head(10)","738b6147":"(df_clean.isna().mean() * df_clean.shape[0]).astype(int)","0f7cbcc1":"df_clean.to_csv('all_tripdata_cleaned.csv', index=False)","e2110ae9":"from subprocess import call\ncall(['python', '-m', 'nbconvert', 'Project5-Data Visualization _ Wrangling.ipynb'])","710a9354":"station 408 requires some attention. let us exclude it and plot the locations again.","4d18367e":"from above, you can clearly notice that still we can consider `member_birth_year` who are between 1960 and 1945, let us view the distribution of it","6d72e85d":"station 420 is a test station, I am going to exclude it from the data","c60e472a":"station 490 is an actual station, but I couldnot recover station location, I am going to exclude it","b8406a98":"#### Data Cleansing\n\nFrom the assessments above i have found the following quality issues and data tidenss required to be applied on the data\n\n**Quality**\n\n* Fix lat\/long for station id 420 => testing station, dropped\n* Fix lat\/long for station id 449 => unrecoverable, dropped\n* Fix lat\/long for station id 408 => assigned correct lat\/long for this station\n* Trips with no station id should be filled with nearest station per lat\/long => solved as possible, dropped the rest\n* Assign median (rounded upto 4 digits) lat\/long for every station id\n* Assign station names which has null values and fix stations that has multible stations names => require visual assessment and fixed the naming in csv file and imported. \n* Fix 45942 rows with wrong trip duration => cleaned trup duration by calculating the differnce between end and start time\n* Fix member genders column => fixed the genders to be Female, Male and Others where Others are any null values or O, or Other\n* Fix weird members birth years => reassign members birth years which are less 1945 to nulls, and kept the rest.\n* Fix null values for bike_share_for_all_trip => could not fix null values, kept it to try extract some insights\n* Fix null values for rental_access_method => could not fix null values, kept it to try extract some insights\n\n**Tideness**\n* Classify start_stations_id and end_station_id into cities (SAN FRANSISCO, SAN JOSE, OAKLAND)\n* Classify member birth year into age groups (18-26, 27-39,40,57,older than 57)\n","84f42552":"After the dataframes match in term of the colomns, and all of them contains the timestamp of the trip, i did not find any any column to indicate each dataframe originally came from which source. Therfore i have merged all the data frames into one.","8acef93e":"#### Data Assessment:\n\nI have imported the csv file `all_tripdata.csv` which contains bike trips for all months, and i have performed some type changes as the following:\n* converted start and end station ids into an `Int64` to accept null values and to remove deimal digits, then I have converted them into a `string` again. \n* converted start and end stations with string as `\"nan\"` into numpy nan value `np.nan`\n* converted trips start and end time into `datetime` format.\n* converted `bike_id` into string\n* converted `member_birth_year` into `Int64` then into `string`.","86031c71":"exploring `member_birth_year` between 1960 adn 1940","9d826328":"Good result, Mostly each station has very close locations registered. ","ef6a7b84":"I have used the function `fix_stations_names` to update all trips with correct `start_station_name` and `end_station_name` based on the fixed stations.","28cb6c23":"***Cleaning 4*** : Fill trips start and end stations with nearest station based on locations","a1591418":"***Assessment 1:*** print our station ids which has 0 longitude or latitude for start and end stations.","32bd9332":"evaluate the distribution of `member_birth_year` column, it has a lot of wide range of outlier values which is `member_birth_year` is less than 1960","82a0b1d5":"***Assessment 4:*** print out unique values of `member_gender`, `bike_share_for_all_trip` and `rental_access_method`","5a635ed8":"From above results below are list of assessments I would perform on the data:\n\n* check stations with 0 lat\/long\n* check each station has unique location\n* check trip duration in second that matches the start and end of the trip\n* check null values for any member information\n* check members birth year values\n* check null values for bike_id bike_share_for_all_trip\n* check null value for rental_access_method","70a2d0fd":"I will apply on each subset the corrsponding function `start_distance_calculation` and `end_distance_calculation` which will assign return the nearest station id and its distance of every trip","731da305":"***Cleaning 3***: clean station 408","3fd82062":"***Cleaning 1***: clean station 420","8310001a":"#### Data Storing\n\nCheck null values again, and then store them into `all_tripdata_cleaned.csv`","d1e74176":"From above, as the records which have `member_birth_year` less than 1945 only 7856 records, and their not well distributed, and by looking at at least 1200 records has default `member_birth_year` equals to 1900. then i have decided to clean this data by assigning `np.nan` value to them","1920b367":"exploring the `member_birth_year` yonger then 1960","8415a373":"from above, i have noticed that some trips are actually started and ended in two different cities. I am going to explore this area in exploration section.","a79c5521":"i have only 31 stations, I am going to perform a visual assessment on them, I have exported them into a csv file `stations_name_error.csv`","2c135fef":"***Cleaning 10***: cleaning `member_birth_year`","95df00e4":"***Assessment 2:*** excluding stations from `Assessment 1`, I will perform the following:\n* combine each station with existing latitude and longitude recorded in start or end station\n* record the maximum and the minimum latitude and longitude recorded on each station.\n* compare the result of the maximum latitude and longitude with minimum latitude and longitude","047335f9":"Let us view the distribution for `member_birth_year` who are below 1945","8e4a6795":"Now the range of `member_birth_year` is between 2001 and 1945, with an outliers values between 1960 and 1945, which is quite resonable.","ef52d7f1":"validate if any stations has different locations by calculating the minumum and maximum locations (latitude, longitude)","8075055e":"station 408 has multiple locations, location of -122.388320, 37.18513 is closer to the truth, I am going to assign this location for station 408","38b0f91f":"I have looped on all the files under data folder and loaded them into an array of dataframes","568a56b2":"I will have two subsets of trips with missing starting stations (`start_nan_staitons`) and ending stations (`end_nan_stations`)","87e3acc9":"I have sorted the dataframe based on `start_time` and `end_time` then i have reset the index column and stored the dataframe which contains the gathered data into one big csv and called it `all_tripdata.csv`","00b6e894":"using `stations_locations_median`, I will assign each station with corresponding median location (latitude, longitude)","ad2b44a0":"I will drop any observation which remains their `start_station_id` or `end_station_id` as nulls","c293bee6":"I have imported a map image using the following technique (<a href=\"https:\/\/towardsdatascience.com\/easy-steps-to-plot-geographic-data-on-a-map-python-11217859a2db\">click here<\/a>) to represent in which cities each station are in","0cf95dfd":"all stations are located in 3 different cities:\n* SAN FRANSISCO\n* SAN JOSE\n* OAKLAND\n\nI am going to classify every trip with `start_station_city` and `end_station_city` by using start and end station longitude","24abdfe7":"# Project 5 - (Data Visualization)\n## Ford GoBike\n### Phase 1: Data Wrangling\n\n<a href=\"#Introduction:\">Introduction<\/a><br\/>\n<a href=\"#Data-Gathering:\">Data Gathering<\/a><br\/>\n<a href=\"#Data-Assessment:\">Data Assessment<\/a><br\/>\n<a href=\"#Data-Cleansing\">Data Cleansing<\/a><br\/>\n<a href=\"#Data-Storing\">Data Stroing<\/a><br\/>\n<a href=\"\">Project 5 - (Data Visualization) - Ford GoBike - Phase 2: Data Exploratory<\/a>\n\n#### Introduction:\n\nFord GoBike has shared puplic bikes trips in main 3 cities in Florida. the data is between the period of June 2017 to October 2019. the data are distributed in multible csv files based on the month and the year. The data has more than 4 millions observations and 15 to 17 features. _check readme.md file to know more about the data_\n\n","9e30edd4":"***Cleaning 2***: clean station 449","baf277b0":"***Cleaning 7***: cleaning stations which has multiple names","e5e72a2c":"Now, I will assign every missing stations Id based on the accepted nearest stations calculated from above.","cb9c8f10":"I have performed a visual assessment by fixing some spelling mistakes, or descriping the same place with some abbriviations. but some of the stations mentioning complete different names, for those i had to look it up on the map and select the correct name. I have done the fixing of the 31 stations in `stations_names_fixed.csv` then i have imported it to fix the stations names.","2ced9ae7":"One of the locations above is very far from the rest of locations, let me evalute it","5eefc53c":"***Assessment 5:*** evaluate `member_birth_year` distribution. ","3ae43e77":"I have renamed the file to a better naming","fd947137":"> ***Cleaning 6:*** Class[](http:\/\/)ify stations by cities","8984511e":"***Assessment 3*** validate `duration_sec` equals to the delta between trip `start_time` and `end_time` ","c63b3237":"***Cleaning 9:*** clean `member_gender` with proper values","86bee89d":"By have a quick glance on all the data frames columns, the data has the bellow columns only, some dataframes has some columns are missing. Hence, i had to loop on all the data frames and find the missing columns and concat the missing columns and fill it with empty values.","c5f339ff":"***Cleaning 8***: clean `duration_sec` by finding delta between `end_time` and `start_time`","db55acbc":"To achieve this, first i will calculate the median locations for each station and store it in `stations_locations_median`","944dbc97":"***Cleaning 5***: assign stations locations with their medians locations","3fde4709":"validate the fix `duration_sec` is fixed","ae75c151":"I have stored station_ids for any station which have multiple station names into `stations_many_names`","d92a584e":"#### Data Gathering:\n\nThe data are spreaded on miltible files on the cloud. For 2017, it contains all months, but starting 2018 and 2019, it has a file for every month. some of these files has different naming conventions. so i had to make a dictionary that it generate the file url. All files are zipped csv file except 2017 is csv.\n","683ae8d7":"I will only accept a station to be considered the nearest if the distance is less than 0.01 KM, and I will store the results in `start_nan_stations_accepted` and `end_nan_stations_accepted`","d8940fee":"exploring `member_birth_year` older than 1940","e9c74cc0":"Now i want to have a quick glance on the data.\n* I have printed the 10 first records\n* checked features datatypes\n* printed out features with null values\n* printed out description of countative features. "}}