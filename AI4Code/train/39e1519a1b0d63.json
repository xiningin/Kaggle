{"cell_type":{"ef49df5f":"code","8069fe92":"code","777d068b":"code","0097c6cb":"code","cfce2f2e":"code","68dddf77":"code","71fc1ffe":"code","aceb70f3":"code","b3fb9a05":"code","268eab57":"code","70aad057":"code","478a9eb4":"code","d485dcb1":"code","f86a9d6a":"code","9a7d7939":"code","a6a0a572":"code","5febec93":"code","e322312d":"code","6d0e1390":"code","712af0ee":"code","5dd5d300":"code","fb6a3a2a":"code","1542ac4c":"code","9c157c98":"code","890bffbd":"code","0cd4e9de":"code","6f40a14d":"code","bec192bc":"code","535f9abe":"code","9708825c":"code","93a0b589":"code","386f1759":"code","75e7e76d":"code","95d4cefb":"code","a1134db5":"code","28c52c64":"code","b20a869d":"code","70c97e99":"code","dcf0d3f5":"code","96a26c04":"code","e1f44719":"code","87bdbd4c":"code","2b64c86b":"code","6840c7d5":"code","72bfd067":"code","2d994602":"code","83e69325":"code","299771ea":"code","0d036fbf":"code","3ea95da7":"code","2b970477":"markdown","3ad6d263":"markdown","d56b9f2b":"markdown","1899724b":"markdown","5ac44692":"markdown","1bcb1589":"markdown","3792bcdb":"markdown","e6edf6b2":"markdown","9e97b919":"markdown","736e2163":"markdown","ff5de548":"markdown","12e1df2c":"markdown","7323238d":"markdown","6630e038":"markdown","8fc7c7b9":"markdown","35f38c97":"markdown","003df0ff":"markdown","84c29094":"markdown","590a183c":"markdown","3e01e732":"markdown","28fb592e":"markdown","18d093a2":"markdown","271a5196":"markdown","1496bb9d":"markdown","0c3052ac":"markdown","516530c8":"markdown","c2b562ba":"markdown","e6105959":"markdown","75f1ed0d":"markdown","33c2e143":"markdown","26697edd":"markdown","e7d88691":"markdown","9448c5b0":"markdown","9b6cbdbe":"markdown","99244bbe":"markdown","59d6ec06":"markdown","1e66e7b4":"markdown","99c78655":"markdown","fb94789e":"markdown","1bf111f0":"markdown","06300360":"markdown","2e5b80b4":"markdown","d66a1224":"markdown","54b1c60b":"markdown","12492981":"markdown"},"source":{"ef49df5f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport seaborn as sns\nimport plotly.express as px\n%matplotlib inline\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score,confusion_matrix, classification_report, roc_curve,auc,roc_auc_score\nfrom sklearn import tree\n\n# Make Plotly work in your Jupyter Notebook\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot \ninit_notebook_mode(connected = True)\n\nimport cufflinks as cf\n\n# Use Plotly locally\ncf.go_offline()","8069fe92":"df_detect = pd.read_csv(\"\/kaggle\/input\/electrical-fault-detection-and-classification\/detect_dataset.csv\")\ndf_detect.head()","777d068b":"df_detect.sample(5)","0097c6cb":"df_detect.info()","cfce2f2e":"df_detect = df_detect.drop(columns=[\"Unnamed: 7\", \"Unnamed: 8\"])","68dddf77":"df_detect[\"Output (S)\"].value_counts()","71fc1ffe":"fig = px.line(df_detect, x = \"Ib\", y = \"Vb\", facet_col=\"Output (S)\", color = \"Output (S)\")\n\n\nfor annotation in fig.layout.annotations:\n    annotation.text = \"\"\n\nfor axis in fig.layout:\n    if type(fig.layout[axis]) == go.layout.YAxis:\n        fig.layout[axis].title.text = ''\n    if type(fig.layout[axis]) == go.layout.XAxis:\n        fig.layout[axis].title.text = ''\n        \n\n\nfig.update_layout(\n    plot_bgcolor = \"#ECECEC\",\n    title = \"<b>Current and Voltage in line b<\/b>\",\n    title_font_size = 16,\n    title_font_color = \"black\",\n    title_pad_t = 5,\n    title_pad_l = 20,\n    yaxis = dict(\n        title_text = \"<b> Voltage <\/b>\",\n        titlefont = dict(size = 12)\n    ),\n    \n    hoverlabel=dict(\n        bgcolor=\"white\",\n        font_size=16,\n        font_family=\"Rockwell\"\n    )\n)\n\nfig.update_yaxes(showticklabels = True, showline = True, linewidth = 2, linecolor = \"black\")\nfig.update_xaxes(showticklabels = True, showline = True, linewidth = 2, linecolor = \"black\",\n                title_text = \"<b>Current<\/b>\")\n\n\n\nfig.show()","aceb70f3":"x_a = df_detect[df_detect[\"Output (S)\"] == 0][\"Ia\"]\ny_a = df_detect[df_detect[\"Output (S)\"] == 0][\"Va\"]\n\nx_b = df_detect[df_detect[\"Output (S)\"] == 0][\"Ib\"]\ny_b = df_detect[df_detect[\"Output (S)\"] == 0][\"Vb\"]\n\nx_c = df_detect[df_detect[\"Output (S)\"] == 0][\"Ic\"]\ny_c = df_detect[df_detect[\"Output (S)\"] == 0][\"Vc\"]\n\n\nx_a1 = df_detect[df_detect[\"Output (S)\"] == 1][\"Ia\"]\ny_a1 = df_detect[df_detect[\"Output (S)\"] == 1][\"Va\"]\n\nx_b1 = df_detect[df_detect[\"Output (S)\"] == 1][\"Ib\"]\ny_b1 = df_detect[df_detect[\"Output (S)\"] == 1][\"Vb\"]\n\nx_c1 = df_detect[df_detect[\"Output (S)\"] == 1][\"Ic\"]\ny_c1 = df_detect[df_detect[\"Output (S)\"] == 1][\"Vc\"]\n\nfig = go.Figure()\n\nfig = make_subplots(rows=3, cols=2,\n                    subplot_titles=(\"No Fault\", \"Fault\"))\n\nfig.add_trace(go.Scatter(x = x_a, y = y_a, mode = \"lines+markers\", legendgroup = 'A',name = \"A line\",\n                        line=dict(color='#EC2781', width=2)), row = 1, col = 1)\nfig.add_trace(go.Scatter(x = x_b, y = y_b, mode = \"lines+markers\", legendgroup = 'B',name = \"B line\",\n                        line=dict(color='#3C8DD6', width=2)), row = 2, col = 1)\nfig.add_trace(go.Scatter(x = x_c, y = y_c, mode = \"lines+markers\", legendgroup = 'C',name = \"C line\",\n                        line=dict(color='black', width=2)), row = 3, col = 1)\nfig.add_trace(go.Scatter(x = x_a1, y = y_a1, mode = \"lines+markers\", legendgroup = 'A', showlegend = False,\n                        line=dict(color='#EC2781', width=2)), row = 1, col = 2)\nfig.add_trace(go.Scatter(x = x_b1, y = y_b1, mode = \"lines+markers\", legendgroup = 'B', showlegend = False,\n                        line=dict(color='#3C8DD6', width=2)), row = 2, col = 2)\nfig.add_trace(go.Scatter(x = x_c1, y = y_c1, mode = \"lines+markers\", legendgroup = 'C', showlegend = False,\n                        line=dict(color='black', width=2)), row = 3, col = 2)\n\n\n\nfig.update_layout(\n    height=800, width=900,\n    plot_bgcolor = \"#ECECEC\",\n    title = \"<b>Current and Voltage in line a, b, c under no fault condition<\/b>\",\n    title_font_size = 16,\n    title_font_color = \"black\",\n    title_pad_t = 5,\n    title_pad_l = 20,\n    \n    hoverlabel=dict(\n        bgcolor=\"white\",\n        font_size=16,\n        font_family=\"Rockwell\"\n    ),\n    annotations = list(fig.layout.annotations) + \n    [go.layout.Annotation(\n            x=-0.07,\n            y=0.5,\n            font=dict(\n                size=12\n            ),\n            text=\"<b> Votage <\/b>\",\n            textangle=-90,\n            xref=\"paper\",\n            yref=\"paper\"\n        )\n    ] +\n    [go.layout.Annotation(\n            x=0.5,\n            y=-0.08,\n            font=dict(\n                size=12, color = 'black'\n            ),\n            showarrow=False,\n            text=\"<b> Current <\/b>\",\n            textangle=-0,\n            xref=\"paper\",\n            yref=\"paper\"\n        )\n    ]\n    \n)\n\nfig.update_yaxes(showticklabels = True, showline = True, linewidth = 2, linecolor = \"black\")\nfig.update_xaxes(showticklabels = True, showline = True, linewidth = 2, linecolor = \"black\")\n\n\n\n\nfig.show()","b3fb9a05":"df_detect_corr = df_detect[['Ia', 'Ib', 'Ic', 'Va', 'Vb', 'Vc']].corr()\ndf_corr_round = df_detect_corr.round(3)\nfig = ff.create_annotated_heatmap(\n            z=df_detect_corr.to_numpy(),\n            x=df_detect_corr.columns.tolist(),\n            y=df_detect_corr.index.tolist(),\n            zmax=1, zmin=-1,\n            showscale=True,\n            hoverongaps=True,\n            colorscale='rdpu', \n            annotation_text=df_corr_round.to_numpy()\n            )\n\nfig.update_layout(title_text='<b>Correlation Heatmap<\/b>')\nfig.show()","268eab57":"features = ['Ia', 'Ib','Ic','Va','Vb','Vc']\n\ndetection_data_X = df_detect[features]\ndetection_data_Y = df_detect['Output (S)']","70aad057":"X_train, X_test, y_train, y_test = train_test_split(detection_data_X, detection_data_Y, test_size=0.3, \n                                                    random_state=101)","478a9eb4":"clf = DecisionTreeClassifier(random_state=101)\nclf.fit(X_train,y_train)\ny_train_pred = clf.predict(X_train)\ny_test_pred = clf.predict(X_test)","d485dcb1":"y_train_acc = accuracy_score(y_train,y_train_pred)\ny_test_acc = accuracy_score(y_test,y_test_pred)","f86a9d6a":"print(\"Train Accuracy: \", y_train_acc)\nprint(\"Test Accuracy: \", y_test_acc)","9a7d7939":"def plot_feature_importance(model, prune):\n    fig = px.bar(x = df_detect.columns[1:], y = model.feature_importances_)\n    fig.update_layout(\n        plot_bgcolor = \"#ECECEC\",\n        title=f'<b>Feature Importance {prune} <\/b> ' ,\n        #title = \"<b>Feature Importance <\/b>\",\n        title_font_size = 16,\n        title_font_color = \"black\",\n        title_pad_t = 5,\n        title_pad_l = 20,\n        yaxis = dict(\n            title_text = \"<b> Feature importance <\/b>\",\n            titlefont = dict(size = 12)\n        ),\n        xaxis = dict(\n            title_text = \"<b> Feature <\/b>\",\n            titlefont = dict(size = 12)\n        )\n    )\n    fig.show()","a6a0a572":"plot_feature_importance(clf,  prune = \"(No pruning)\")","5febec93":"def tree_plot(model_name):\n    plt.figure(figsize=(20,20))\n    features = df_detect.columns\n    classes = ['Not Fault','Fault']\n    tree.plot_tree(model_name,feature_names=features,class_names=classes,filled=True)\n    plt.show()","e322312d":"tree_plot(clf)","6d0e1390":"print(classification_report(y_test, y_test_pred))","712af0ee":"def plot_confusionmatrix(y_train_pred,y_train,dom):\n    print(f'{dom} Confusion matrix')\n    cf = confusion_matrix(y_train_pred,y_train)\n    x = [\"Not Fault\", \"Fault\"]\n    y = [\"Not Fault\", \"Fault\"]\n    font_colors = ['black', 'white']\n    fig = ff.create_annotated_heatmap(cf, x = x, y = y, colorscale='rdpu', font_colors=font_colors)\n    fig.update_layout(\n    xaxis = dict(\n        title_text = \"<b> Predicted <\/b>\",\n        titlefont = dict(size = 12)\n    ),\n    yaxis = dict(\n        title_text = \"<b> Actual <\/b>\",\n        titlefont = dict(size = 12)\n    )\n    )\n    fig.show()","5dd5d300":"plot_confusionmatrix(y_train_pred,y_train,dom='Train')","fb6a3a2a":"plot_confusionmatrix(y_test_pred,y_test,dom='Test')","1542ac4c":"dt_probs = clf.predict_proba(X_test)[:,1]\nfpr_dt, tpr_dt, thresholds_dt = roc_curve(y_test,dt_probs)\nauc_score_dt = auc(fpr_dt,tpr_dt)\nauc_score_dt","9c157c98":"def roc_plot(fpr, tpr, params):\n    fig = px.line(x = fpr, y = tpr)\n    fig.add_shape(type='line', line=dict(color=\"black\",width=3, dash = \"dash\"),x0=0, x1=1, y0=0, y1=1)\n    fig.update_layout(\n    plot_bgcolor = \"#ECECEC\",\n    title=f'ROC Curve {params} (AUC={auc(fpr, tpr):.4f})' ,\n    title_font_size = 16,\n    title_font_color = \"black\",\n    title_pad_t = 5,\n    title_pad_l = 20,\n    yaxis = dict(\n        title_text = \"<b> True Positive Rate <\/b>\",\n        titlefont = dict(size = 12)\n    ),\n    xaxis = dict(\n        title_text = \"<b> False Positive Rate <\/b>\",\n        titlefont = dict(size = 12)))\n\n    fig.show()","890bffbd":"roc_plot(fpr_dt, tpr_dt, params = \"(No Pruning)\")","0cd4e9de":"from sklearn.model_selection import  GridSearchCV\nparams = {'max_depth': [2,4,6,8,10,12],\n         'min_samples_split': [2,3,4],\n         'min_samples_leaf': [1,2]}\n\nclf = DecisionTreeClassifier(random_state=101)\ngcv = GridSearchCV(estimator=clf,param_grid=params)\ngrid_cv = gcv.fit(X_train,y_train)","6f40a14d":"print(\"The best parameter selected are: \", grid_cv.best_estimator_)","bec192bc":"print(\"The best parameters selected are: \",grid_cv.best_params_)","535f9abe":"model_pre = gcv.best_estimator_\nmodel_pre.fit(X_train,y_train)\ny_train_pred = model_pre.predict(X_train)\ny_test_pred = model_pre.predict(X_test)","9708825c":"print(f'Train score {accuracy_score(y_train_pred,y_train)}')\nprint(f'Test score {accuracy_score(y_test_pred,y_test)}')","93a0b589":"plot_feature_importance(model_pre,   prune = \"(Pre pruning)\")","386f1759":"tree_plot(model_pre)","75e7e76d":"print(classification_report(y_test, y_test_pred))","95d4cefb":"plot_confusionmatrix(y_train_pred,y_train,dom='Train')\nplot_confusionmatrix(y_test_pred,y_test,dom='Test')","a1134db5":"pre_dt_probs = model_pre.predict_proba(X_test)[:,1]\nfpr_pre_dt, tpr_pre_dt, thresholds_pre_dt = roc_curve(y_test,pre_dt_probs)\nauc_score_pre_dt = auc(fpr_pre_dt,tpr_pre_dt)\nauc_score_pre_dt\n","28c52c64":"roc_plot(fpr_pre_dt, tpr_pre_dt, params = \"(Pre Pruning)\")","b20a869d":"path = clf.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities\nprint(ccp_alphas)","70c97e99":"fig, ax = plt.subplots()\nax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\nax.set_xlabel(\"effective alpha\")\nax.set_ylabel(\"total impurity of leaves\")\nax.set_title(\"Total Impurity vs effective alpha for training set\");","dcf0d3f5":"# For each alpha we will append our model to a list\nclfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    clf.fit(X_train, y_train)\n    clfs.append(clf)","96a26c04":"print(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n      clfs[-1].tree_.node_count, ccp_alphas[-1]))","e1f44719":"clfs = clfs[:-1]\nccp_alphas = ccp_alphas[:-1]\n\nnode_counts = [clf.tree_.node_count for clf in clfs]\ndepth = [clf.tree_.max_depth for clf in clfs]\nfig, ax = plt.subplots(2, 1,figsize=(10,8))\nax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle=\"steps-post\")\nax[0].set_xlabel(\"alpha\")\nax[0].set_ylabel(\"number of nodes\")\nax[0].set_title(\"Number of nodes vs alpha\")\nax[1].plot(ccp_alphas, depth, marker='o', drawstyle=\"steps-post\")\nax[1].set_xlabel(\"alpha\")\nax[1].set_ylabel(\"depth of tree\")\nax[1].set_title(\"Depth vs alpha\")\nfig.tight_layout()","87bdbd4c":"train_scores = [clf.score(X_train, y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, y_test) for clf in clfs]\n\nfig, ax = plt.subplots(figsize=(10,5))\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n        drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n        drawstyle=\"steps-post\")\nax.legend()\nplt.show()","2b64c86b":"model_post = DecisionTreeClassifier(random_state=101,ccp_alpha=0.01)\nmodel_post.fit(X_train,y_train)","6840c7d5":"y_train_pred = model_post.predict(X_train)\ny_test_pred = model_post.predict(X_test)\n\nprint(f'Train score {accuracy_score(y_train_pred,y_train)}')\nprint(f'Test score {accuracy_score(y_test_pred,y_test)}')","72bfd067":"plot_feature_importance(model_post,  prune = \"(Post pruning)\")","2d994602":"tree_plot(model_post)","83e69325":"print(classification_report(y_test, y_test_pred))","299771ea":"plot_confusionmatrix(y_train_pred,y_train,dom='Train')\nplot_confusionmatrix(y_test_pred,y_test,dom='Test')","0d036fbf":"post_dt_probs = model_post.predict_proba(X_test)[:,1]\nfpr_post_dt, tpr_post_dt, thresholds_post_dt = roc_curve(y_test,post_dt_probs)\nauc_score_post_dt = auc(fpr_post_dt,tpr_post_dt)\nauc_score_post_dt","3ea95da7":"roc_plot(fpr_post_dt, tpr_post_dt, params = \"(Post Pruning)\")","2b970477":"### Variation of nodes and depth of tree with alpha","3ad6d263":"The classification report is about key metrics in a classification problem.<br>\n\nYou'll have precision, recall, f1-score and support for each class you're trying to find.<br>\n\n1. The recall means \"how many of this class you find over the whole number of element of this class\".<br>\n\n2. The precision will be \"how many are correctly classified among that class\".<br>\n\n3. The f1-score is the harmonic mean between precision & recall.<br>\n\n4. The support is the number of occurence of the given class in your dataset.<br>","d56b9f2b":"# Model Design","1899724b":"## Confusion Matrix","5ac44692":"## ROC Curve","1bcb1589":"# Import libraries","3792bcdb":"## Splitting data","e6edf6b2":"## Feature Importance","9e97b919":"## Information about the dataset columns","736e2163":"We will remove the last element in clfs and ccp_alphas, because it is the trivial tree with only one node.\n\n","ff5de548":"From above diagram we can see that the features doest not have positive corelation trend.","12e1df2c":"### Total Impurity vs effective alpha for training set","7323238d":"## Defining model","6630e038":"## Feature Importance","8fc7c7b9":"## Hyper Parameter Tuning","35f38c97":"Above graphs shows us the relationship between current and voltage in different line A, B and C under faulty and non-faulty condition. ","003df0ff":"As the value of alphs is increased, more of the tree is pruned. This results in increase in impurity at leaf node. As a result the accuracy decreases.","84c29094":"Electrical powers system is growing in size and complexity in all sectors such as generation, transmission, distribution and load systems. Types of faults like short circuit condition in power system network results in severe economic losses and reduces the reliability of the electrical system.<br>\nElectrical fault is an abnormal condition, caused by equipment failures such as transformers and rotating machines, human errors and environmental conditions. Theses faults cause interruption to electric flows, equipment damages and even cause death of humans, birds and animals.<br>\n<b>Types of Faults<\/b><br>\nElectrical fault is the deviation of voltages and currents from nominal values or states. Under normal operating conditions, power system equipment or lines carry normal voltages and currents which results in a safer operation of the system.<br>\n\nBut when fault occurs, it causes excessively high currents to flow which causes the damage to equipments and devices. Fault detection and analysis is necessary to select or design suitable switchgear equipments, electromechanical relays, circuit breakers and other protection devices.<br>\n\nThere are mainly two types of faults in the electrical power system:\n1. Symmetrical Faults\n2. Unsymmetrical Faults.\n\n<b>Symmetrical faults<\/b><br>\nThese are very severe faults and occur infrequently in the power systems. These are also called as balanced faults and are of two types namely line to line to line to ground <b>(L-L-L-G)<\/b> and line to line to line <b>(L-L-L)<\/b>.<br>\nOnly 2-5 percent of system faults are symmetrical faults. If these faults occur, system remains balanced but results in severe damage to the electrical power system equipments. Analysis of these faults is easy and usually carried by per phase basis. Three phase fault analysis or information is required for selecting set-phase relays, rupturing capacity of the circuit breakers and rating of the protective switchgear.\n\n<b>Unsymmetrical faults<\/b><br>\nThese are very common and less severe than symmetrical faults. There are mainly three types namely line to ground <b>(L-G)<\/b>, line to line <b>(L-L)<\/b> and double line to ground <b>(LL-G)<\/b> faults.<br>\nLine to ground fault (L-G) is most common fault and 65-70 percent of faults are of this type.<br>\nIt causes the conductor to make contact with earth or ground. 15 to 20 percent of faults are double line to ground(LL-G) and causes the two conductors to make contact with ground. Line to line faults (L-L) occur when two conductors make contact with each other mainly while swinging of lines due to winds and 5- 10 percent of the faults are of this type.<br>\nThese are also called unbalanced faults since their occurrence causes unbalance in the system. Unbalance of the system means that that impedance values are different in each phase causing unbalance current to flow in the phases. These are more difficult to analyze and are carried by per phase basis similar to three phase balanced faults.<br>","590a183c":"## Fitting the model","3e01e732":"# Post pruning techniques\n<b>Cost Complexity Pruning<\/b>\nDecision trees can easily overfit. One way to avoid it is to limit the growth of trees by setting constrains. We can limit parameters like max_depth , min_samples etc. But a most effective way is to use post pruning methods like cost complexity pruning. This helps to improve test accuracy and get a better model.<br>\n\nCost complexity pruning is all about finding the right parameter for alpha.We will get the alpha values for this tree and will check the accuracy with the pruned trees.<br>","28fb592e":"## Fitting best parameter","18d093a2":"# Visualization","271a5196":"### Accuracy of training and test data with alpha variation","1496bb9d":"## Tree Visualization","0c3052ac":"## Cost complexity parameter","516530c8":"## Drop the null columns","c2b562ba":"## ROC Curve","e6105959":"## Calculating model accuracy","75f1ed0d":"## ROC Curve","33c2e143":"## Visualizing tree after pre pruning","26697edd":"## Feature Importance","e7d88691":"From above we can see that the problem is about binary classification.<br>\n<b>Inputs<\/b> - Ia,Ib,Ic,Va,Vb,Vc<br>\n<b>Outputs<\/b> - 0 (No-fault) or 1(Fault is present)","9448c5b0":"## Confusion Matrix","9b6cbdbe":"## Confusion Matrix","99244bbe":"## Classification Report","59d6ec06":"## Feature and label seperation","1e66e7b4":"## Classification Report","99c78655":"# Pre pruning techniques\n\nPre pruning is nothing but stoping the growth of decision tree on an early stage. For that we can limit the growth of trees by setting constrains. We can limit parameters like max_depth , min_samples etc.<br>\n\nAn effective way to do is that we can grid search those parameters and choose the optimum values that gives better performace on test data.<br>\n\nAs of now we will control these parameters<br>\n\nmax_depth: maximum depth of decision tree<br>\nmin_sample_split: The minimum number of samples required to split an internal node:<br>\nmin_samples_leaf: The minimum number of samples required to be at a leaf node.<br>","fb94789e":"Train accuracy is 100% thus we see there is overfitting and it needs hyperparameter tuning.","1bf111f0":"## Classification Report","06300360":"# Building classifier without tuning data","2e5b80b4":"## Visualizing Tree","d66a1224":"The above plot shows the relationship between the current and voltage under no fault (0) and faulty (1) conditions. We can see that in first graph there is nearly a smooth relationship between current and voltage but that is not the case in second graph. The variation between the two is completely incoherent.","54b1c60b":"## Output ","12492981":"# Read dataset"}}