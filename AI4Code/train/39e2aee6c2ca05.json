{"cell_type":{"ce872e6a":"code","8248d033":"code","4da5a990":"code","a8d3a75f":"code","6f8ae098":"code","78778051":"code","72f62dd2":"code","a91dcc98":"code","7578f687":"code","a223dc61":"code","d7be0b57":"code","15f62f58":"code","cc850838":"code","712e1ee9":"code","a35c08d5":"code","3a841c8f":"code","2edb7256":"code","91d7e302":"code","2ea1f792":"code","2e8c1443":"code","37a57bd9":"code","0e876f08":"code","ab140629":"code","d7be3d2c":"code","f099e6c3":"code","bf72bc4f":"markdown","02fc6674":"markdown","d33311b9":"markdown","eb8b227e":"markdown","b4e3bb38":"markdown","a6403142":"markdown","1a4a9cc6":"markdown","3ac0b615":"markdown","36711223":"markdown","88cf20f2":"markdown","80b0cb19":"markdown","3a2e3e1d":"markdown","c7e786e8":"markdown","9f195396":"markdown","78bb0e5d":"markdown","35c737cf":"markdown","cf81509e":"markdown","3d607057":"markdown","4f260941":"markdown","a2997391":"markdown","5db0879e":"markdown","21b89cee":"markdown","386f12c6":"markdown","31725c9b":"markdown","b1a64471":"markdown","47e46e3f":"markdown","b1c06d02":"markdown","fcef968e":"markdown","d85f8248":"markdown"},"source":{"ce872e6a":"import pandas as pd\nimport numpy as np","8248d033":"data = pd.DataFrame({\n    'X1': [1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3],\n    'X2': ['S', 'M', 'M', 'S', 'S', 'S', 'M', 'M', 'L', 'L', 'L', 'M', 'M', 'L', 'L'],\n    'y': [-1, -1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1]\n})","4da5a990":"data.y.value_counts()","a8d3a75f":"data[data.y == 1].X1.value_counts()","6f8ae098":"data[data.y == 1].X2.value_counts()","78778051":"data[data.y == -1].X1.value_counts()","72f62dd2":"data[data.y == -1].X2.value_counts()","a91dcc98":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom scipy.stats import norm","7578f687":"x = np.linspace(-5, 5)\ny = norm.pdf(x)\nplt.plot(x, y)\nplt.vlines(ymin=0, ymax=0.4, x=1, colors=['red'])","a223dc61":"train = pd.read_csv('..\/input\/train.csv', index_col=0)\ntest = pd.read_csv('..\/input\/test.csv', index_col=0)\ntarget = train.target.values\ntrain.drop('target', axis=1, inplace=True)\ntrain.shape, target.shape, test.shape","d7be0b57":"pos_idx = (target == 1)\nneg_idx = (target == 0)\nstats = []\nfor col in train.columns:\n    stats.append([\n        train.loc[pos_idx, col].mean(),\n        train.loc[pos_idx, col].std(),\n        train.loc[neg_idx, col].mean(),\n        train.loc[neg_idx, col].std()\n    ])\n    \nstats_df = pd.DataFrame(stats, columns=['pos_mean', 'pos_sd', 'neg_mean', 'neg_sd'])\nstats_df.head()","15f62f58":"# priori probability\nppos = pos_idx.sum() \/ len(pos_idx)\npneg = neg_idx.sum() \/ len(neg_idx)\n\ndef get_proba(x):\n    # we use odds P(target=1|X=x)\/P(target=0|X=x)\n    return (ppos * norm.pdf(x, loc=stats_df.pos_mean, scale=stats_df.pos_sd).prod()) \/\\\n           (pneg * norm.pdf(x, loc=stats_df.neg_mean, scale=stats_df.neg_sd).prod())","cc850838":"tr_pred = train.apply(get_proba, axis=1)","712e1ee9":"from sklearn.metrics import roc_auc_score\nroc_auc_score(target, tr_pred)","a35c08d5":"plt.figure(figsize=(20, 10))\nplt.subplot(1, 2, 1)\nsns.distplot(train.loc[pos_idx, 'var_0'])\nplt.plot(np.linspace(0, 20), norm.pdf(np.linspace(0, 20), loc=stats_df.loc[0, 'pos_mean'], scale=stats_df.loc[0, 'pos_sd']))\nplt.title('target==1')\nplt.subplot(1, 2, 2)\nsns.distplot(train.loc[neg_idx, 'var_0'])\nplt.plot(np.linspace(0, 20), norm.pdf(np.linspace(0, 20), loc=stats_df.loc[0, 'neg_mean'], scale=stats_df.loc[0, 'neg_sd']))\nplt.title('target==0')","3a841c8f":"from scipy.stats.kde import gaussian_kde","2edb7256":"plt.figure(figsize=(20, 10))\nplt.subplot(1, 2, 1)\nsns.distplot(train.loc[pos_idx, 'var_0'])\nkde = gaussian_kde(train.loc[pos_idx, 'var_0'].values)\nplt.plot(np.linspace(0, 20), kde(np.linspace(0, 20)))\nplt.title('target==1')\nplt.subplot(1, 2, 2)\nsns.distplot(train.loc[neg_idx, 'var_0'])\nkde = gaussian_kde(train.loc[neg_idx, 'var_0'].values)\nplt.plot(np.linspace(0, 20), kde(np.linspace(0, 20)))\nplt.title('target==0')","91d7e302":"stats_df['pos_kde'] = None\nstats_df['neg_kde'] = None\nfor i, col in enumerate(train.columns):\n    stats_df.loc[i, 'pos_kde'] = gaussian_kde(train.loc[pos_idx, col].values)\n    stats_df.loc[i, 'neg_kde'] = gaussian_kde(train.loc[neg_idx, col].values)","2ea1f792":"def get_proba2(x):\n    proba = ppos \/ pneg\n    for i in range(200):\n        proba *= stats_df.loc[i, 'pos_kde'](x[i]) \/ stats_df.loc[i, 'neg_kde'](x[i])\n    return proba","2e8c1443":"%%time\nget_proba2(train.iloc[0].values)","37a57bd9":"def get_col_prob(df, coli, bin_num=100):\n    bins = pd.cut(df.iloc[:, coli].values, bins=bin_num)\n    uniq = bins.unique()\n    uniq_mid = uniq.map(lambda x: (x.left + x.right) \/ 2)\n    dense = pd.DataFrame({\n        'pos': stats_df.loc[coli, 'pos_kde'](uniq_mid),\n        'neg': stats_df.loc[coli, 'neg_kde'](uniq_mid)\n    }, index=uniq)\n    return bins.map(dense.pos).astype(float) \/ bins.map(dense.neg).astype(float)","0e876f08":"tr_pred = ppos \/ pneg\nfor i in range(200):\n    tr_pred *= get_col_prob(train, i)","ab140629":"roc_auc_score(target, tr_pred)","d7be3d2c":"te_pred = ppos \/ pneg\nfor i in range(200):\n    te_pred *= get_col_prob(test, i)","f099e6c3":"pd.DataFrame({\n    'ID_code': test.index,\n    'target': te_pred\n}).to_csv('sub.csv', index=False)","bf72bc4f":"### Bayes' theorem\n\nBayes' theorem can be deduced by conditional probability:\n\n$$\nP(A|B) = \\frac {P(A \\bigcap B)}{P(B)}\n$$\n\n$$\nP(B|A) = \\frac {P(A \\bigcap B)}{P(A)}\n$$\n\n$$\n\\Rightarrow P(A|B) = \\frac {P(B|A) \\cdot P(A)} {P(B)}\n$$","02fc6674":"In the kernel [Modified Naive Bayes scores 0.899 LB - Santander](https:\/\/www.kaggle.com\/cdeotte\/modified-naive-bayes-santander-0-899), [Chris Deotte](https:\/\/www.kaggle.com\/cdeotte) has demonstrated that Naive Bayes can be a simple but powful method when there is little or no interaction between the features. I think it's a good time to study Naive Bayes Method.\n\nIn this kernel, I'll try to introduce Naive Bayes method step by step.","d33311b9":"Calculate conditional probability","eb8b227e":"We can see that the data is very different from normal distribution, we need use more accurate probability density function to estimate, this can be done by kernel function estimation. Let's use `scipy.stats.kde.gaussian_kde` ","b4e3bb38":"### Remove the Gaussian constrain","a6403142":"How can we calculate $P(y=+1|X=x_0)$ ? We can use Bayes's theorem:\n$$\nP(y=+1|X=x_0) = \\frac {P(X=x_0 | y=+1) \\cdot P(y=+1)} {P(X=x_0)}\n$$\n\nBecause $P(X=X_0)$ is the probability(or frequency) of a sample in test set, it's the same for every sample, so we can simply write the formular as:\n\n$$\nP(y=+1|X=x_0) = P(X=x_0 | y=+1) \\cdot P(y=+1)\n$$\n\n$P(y=+1\/-1)$ is called priori probability and $P(X=x_0 | y=+1)$ is called conditional probability. Training process is to estimate this two kind of probability.\n","1a4a9cc6":"### Discrete variable\n\nWe will use a sample example to demostrate the training and predicting process of Naive Bayes for discrete variable.","3ac0b615":"Using normal distribution to estimate each feature","36711223":"### Conclusion\n\nIn this kernel we demonstrate how Naive bayes works, we build Gaussian Naive Bayes, which gives us 0.890 AUC. By remove Gaussian constrain and choosing more accurate kernel function, we can get better performance.\n\nHolp this can help, thanks!","88cf20f2":"### Naive Bayes\n\nIf we have many features, then:\n$$\nP(X=x_0 | y=+1) = P(X^{(1)}=x_0^{(1)}, X^{(2)}=x_0^{(2)}, ..., X^{(n)}=x_0^{(n)} | y=+1)\n$$\n\nTo simplify this problem, we assume the features as independent(this is the Naive means):\n$$\nP(X=x_0 | y=+1) = P(X^{(1)}=x_0^{(1)} | y=+1) \\cdot P(X^{(2)}=x_0^{(2)} | y=+1) \\cdot, ..., \\cdot P(X^{(n)}=x_0^{(n)} | y=+1)\n$$","80b0cb19":"$$\nP(X1=1 | y=-1) = \\frac{3}{6}, P(X1=2 | y=-1) = \\frac{2}{6}, P(X1=3 | y=-1) = \\frac{1}{6}\n$$","3a2e3e1d":"It's too slow, we can speed up by binize the variable values.","c7e786e8":"$$\nP(X2=S | y=1) = \\frac{1}{9}, P(X2=M | y=1) = \\frac{4}{9}, P(X2=L | y=1) = \\frac{4}{9}\n$$","9f195396":"When dealing with continuous variable, how do you calculate the probabilty for a given value(e.g. x=1)? The probability should be zero. So we'd better calculate the probability of a interval(e.g. $1-\\Delta < x < 1+\\Delta$).","78bb0e5d":"Infact our data is not normal distributed, we can achive better score with Gaussian constran removed. let's take `var_0` as an example.","35c737cf":"Calculate priori probability $P(y=1)$ and $P(y=-1)$","cf81509e":"$$\nP(X1=1 | y=1) = \\frac{2}{9}, P(X1=2 | y=1) = \\frac{3}{9}, P(X1=3 | y=1) = \\frac{4}{9}\n$$","3d607057":"Kernel funtion can fit the data better, which will give us better accuracy.","4f260941":"If the interval is small enough(i.e. $\\Delta \\rightarrow 0$), the probability of a given value(e.g. x=1) can be represented by probability density(pdf) value. How can we know the probability function of a variable? The convenient way is to estimate using normal distribution. This is the **Gaussian Naive Bayes**.  ","a2997391":"Try to use data to training a Naive Bayes classifier, and classify the sample $X_0=(2, S)$.","5db0879e":"Using more accurate kernel function, we can achieve 0.909 AUC(maybe a little overfit, since we fit train's data, but it's not too much). Let's use this model to predict the test data.","21b89cee":"Calculate mean\/sd of train data for each each feature.","386f12c6":"### Gaussian Naive Bayes","31725c9b":"$$\nP(y=1) = \\frac {9}{15}, P(y=-1) = \\frac {6}{15}\n$$","b1a64471":"For the given sample $X_0=(2, S)$:\n$$\nP(y=1 | X=X_0) = P(y=1) \\cdot P(X1=2|y=1) \\cdot P(X2=S|y=1) = \\frac{9}{15} \\cdot \\frac{3}{9} \\cdot \\frac{1}{9} = \\frac{1}{45}\n$$\n\n$$\nP(y=-1 | X=X_0) = P(y=-1) \\cdot P(X1=2|y=-1) \\cdot P(X2=S|y=-1) = \\frac{6}{15} \\cdot \\frac{2}{6} \\cdot \\frac{3}{6} = \\frac{1}{15}\n$$\n\nBecause $P(y=-1|X=X_0) > P(y=1|X=X_0)$, so $y=-1$\n\nThis is Naive Bayes for discrete variable, pretty simple.","47e46e3f":"Let's apply Gaussian Naive Bayes to our Santander data.","b1c06d02":"Gaussian Naive Bayes can give us 0.890 AUC, which is quite good!","fcef968e":"Suppose we have features $X \\in R^n$, target $y \\in \\{+1, -1\\}$, for a given $x_0$ our goal is to predict $y=+1$ or $-1$.\n\nwe can achive this by calculate\n\n$$P(y=+1 | X=x_0)$$ \nand\n$$P(y=-1 |  X=x_0)$$\n\nthen choose the one with bigger probability.","d85f8248":"$$\nP(X2=S | y=1) = \\frac{3}{6}, P(X2=M | y=1) = \\frac{2}{6}, P(X2=L | y=1) = \\frac{1}{6}\n$$"}}