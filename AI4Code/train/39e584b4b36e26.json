{"cell_type":{"f7d9650f":"code","44965c6f":"code","d4ec18d8":"code","096af67c":"code","a8fffd2d":"code","6662ec06":"code","857a8d41":"code","b4bee373":"code","5c589481":"code","4e78ed15":"code","614dc309":"code","21488147":"code","501bb731":"code","4a0e78dc":"code","3dd7bd59":"code","3e8b075c":"markdown","a7d27390":"markdown","c512e0aa":"markdown","0908516a":"markdown","c488b796":"markdown","acd6e247":"markdown","811a3b84":"markdown","646b8795":"markdown","1acfb46a":"markdown"},"source":{"f7d9650f":"import numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport cv2\nimport tqdm\nfrom socket import socket","44965c6f":"\nCATEGORIES = ['covid', 'healthy']\nDATADIR = '..\/input\/covidistesgp\/CovidDataset\/train'\nfor category in CATEGORIES:\n    path = os.path.join(DATADIR,category)\n    for img in os.listdir(path):\n        img_arr = cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE)   \n        plt.imshow(img_arr, cmap='gray')\n        plt.xlabel(category)\n        plt.show()\n        break","d4ec18d8":"IMG_SIZE=50\ntrain_data=[]\ntest_data=[]\n\ndef create_data(data_dir):\n    for category in CATEGORIES:\n        path=os.path.join(data_dir, category)\n        class_num=CATEGORIES.index(category)\n        \n        for img in (os.listdir(path)):                                             ## We use os to iterate over all our files in the directory \n            try:\n                img_arr=cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE)   ## GRAYSCALING\n                img_arr=cv2.resize(img_arr, (IMG_SIZE,IMG_SIZE))                   ## RESIZING\n                if(data_dir=='..\/input\/covidistesgp\/CovidDataset\/train'):\n                    train_data.append([img_arr,class_num])\n                else:\n                    test_data.append([img_arr,class_num])\n            except exception as e:\n                pass","096af67c":"create_data('..\/input\/covidistesgp\/CovidDataset\/train')\ncreate_data('..\/input\/covidistesgp\/CovidDataset\/validation')\n\nprint(len(train_data))\nprint(len(test_data))","a8fffd2d":"for sample in train_data[:10]:\n    print(sample[1])","6662ec06":"import random\n\nrandom.shuffle(train_data)              ## Shuffling the dataset\n\nfor sample in train_data[:10]:\n    print(sample[1])","857a8d41":"x_train=[]\ny_train=[]\nx_test=[]\ny_test=[]\n\nfor features,label in train_data:\n    x_train.append(features)\n    y_train.append(label)\n    \nfor features,label in test_data:\n    x_test.append(features)\n    y_test.append(label)\n\nx_train = np.array(x_train).reshape(-1, IMG_SIZE, IMG_SIZE, 1)   ## reshaping the dataset to (length, 50, 50, 1)\nx_test = np.array(x_test).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n\nprint(len(x_train))\nprint(len(x_test))","b4bee373":"import pickle\n\npickle_out_x_train = open(\"x_train.pickle\",\"wb\")          # open\/create a file called x_train.pickle, and write into it         \npickle.dump(x_train, pickle_out_x_train)                  # dump the contents of the np array\npickle_out_x_train.close()                                # close the file\n\npickle_out_y_train = open(\"y_train.pickle\",\"wb\")\npickle.dump(y_train, pickle_out_y_train)\npickle_out_y_train.close()\n\npickle_out_x_test = open(\"x_test.pickle\",\"wb\")\npickle.dump(x_test, pickle_out_x_test)\npickle_out_x_test.close()\n\npickle_out_y_test = open(\"y_test.pickle\",\"wb\")\npickle.dump(y_test, pickle_out_y_test)\npickle_out_y_test.close()\n\nprint(len(x_train))\nprint(len(x_test))","5c589481":"pickle_in_x_train = open(\"x_train.pickle\",\"rb\")           # open the file\ntrainX = pickle.load(pickle_in_x_train)                   # load its contents into a python varriable\npickle_in_x_train.close()                                 # close the file\n\npickle_in_y_train = open(\"y_train.pickle\",\"rb\")\ntrainY = pickle.load(pickle_in_y_train)\npickle_in_y_train.close()\n\npickle_in_x_test = open(\"x_test.pickle\",\"rb\")\ntestX = pickle.load(pickle_in_x_test)\npickle_in_x_test.close()\n\npickle_in_y_test = open(\"y_test.pickle\",\"rb\")\ntestY = pickle.load(pickle_in_y_test)\npickle_in_y_test.close()\n\nprint(str(len(trainX)) + ', ' + str(len(testX)))","4e78ed15":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n\nprint(len(x_test))","614dc309":"### NORMALIZE the data (trainX, trainY) from 0-255 to 0-1, and convert trainX,trainY,testX,testY to np arrays\n### approx. (1 x 4) lines of code\n\ntrainX = trainX\/255\ntestX = testX\/255\ntrainX = np.array(trainX)\ntestX = np.array(testX)\ntrainY = np.array(trainY)\ntestY = np.array(testY)\n\nprint(trainX.shape)\nprint(testX.shape)\n\n\n","21488147":"model = Sequential()\n\n### Use model.add to add layers (example: conv2D layers, then Maxpooling2D layers, Dense)\n### Experiment with tf keras documentation to complete the model\n### approx 5-12 lines of code, feel free to experiment with different model structures\n\nmodel.add(Conv2D(activation=\"tanh\",filters=32,kernel_size=(5,5)))\nmodel.add(MaxPooling2D(pool_size = (2,2)))\nmodel.add(Conv2D(activation=\"tanh\",filters=64,kernel_size=(3,3)))\nmodel.add(MaxPooling2D(pool_size = (2, 2)))\nmodel.add(Conv2D(activation=\"tanh\",filters=128,kernel_size=(1,1)))\nmodel.add(MaxPooling2D(pool_size = (2,2)))\nmodel.add(Flatten())\nmodel.add(Dense(200,activation=\"tanh\"))\nmodel.add(Dense(100,activation=\"tanh\"))\nmodel.add(Dense(50,activation=\"tanh\"))\nmodel.add(Dense(10,activation=\"tanh\"))\n\n\n\n\n\n\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam', metrics=[tf.keras.metrics.AUC()])\n\n","501bb731":"model.fit(trainX, trainY, batch_size=32, epochs=5, validation_split=0.3)","4a0e78dc":"score = model.evaluate(trainX, trainY, verbose = 1) \n\nprint('Train loss:', score[0]) \nprint('Train accuracy:', score[1])","3dd7bd59":"score = model.evaluate(testX, testY, verbose = 0) \n\nprint('Test loss:', score[0]) \nprint('Test accuracy:', score[1])","3e8b075c":"# Deep Learning SGP WEEK 3 Convolutional Neural Networks","a7d27390":"### Now we use a python library called openCV to read and perform some operations on the input data, such as GRAYSCALING and RESIZING","c512e0aa":"### Our Dataset is visible on the top right of the screen. We have two categories for images (covid, healthy) for both 'train' and 'validation' folders","0908516a":"### Now fill out the code in the below 2 cells following the instructions","c488b796":"#### Expected Test Accuracy 70-80%","acd6e247":"#### Expected training accuracy 90-98% ","811a3b84":"Expected: 2000, 200\n","646b8795":"### Its pretty important that we randomise our images rather than having all covid images together and all healthy images together","1acfb46a":"### After we've done this preprocessing work, its handy to store our final array instead of repeating this everytime we want to use these values\n### For this, we use the python library called pickle to store all the values and load them in directly later"}}