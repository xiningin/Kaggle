{"cell_type":{"23321832":"code","8c98f151":"code","065e26c9":"code","12e9d794":"code","a056bd35":"code","d9b068ab":"code","c2bc1537":"code","8c935db4":"code","1da1eae3":"code","4848045c":"code","1f7fea0f":"code","92f7f8fd":"code","e5f24158":"code","feb52b02":"code","d0ae7bed":"code","bb52feee":"code","148170c0":"code","5d3dfd4c":"code","efd42244":"code","9b7eaa3a":"code","86c457d2":"code","aa4c4758":"code","bb4a0e78":"code","ced60732":"code","b79e6fc0":"code","5dfc8f91":"code","e4c927c8":"code","39cb8e7e":"code","7ac33e24":"code","2a29a8ea":"code","28e3ff41":"code","c05fd1de":"code","66e01b3d":"code","223125ab":"code","377ebd92":"code","5433916e":"code","2dfa3dc6":"code","875edf50":"code","31ed9a1d":"code","67de48b1":"code","0dac9e48":"code","d3b648bb":"code","d98f474a":"code","20bc294a":"code","21fec49c":"code","909576f8":"code","3f1464c0":"code","314f002a":"code","bb9338c5":"code","7af59415":"code","4c2388f5":"code","976bab5e":"code","5353c3c4":"code","5e8555e4":"code","af782ce6":"code","180b684d":"code","d42e667b":"code","2ba6b637":"code","e2137b91":"code","25bfd7be":"code","0f1b1331":"code","632d1d00":"code","3c9f11cf":"code","2afb326d":"code","cd340eeb":"code","b5f7998a":"code","0995b5a1":"code","b0d8e5e7":"code","e3d7416a":"code","57d46424":"code","ee60ac27":"markdown","01f3b6f7":"markdown","7aa4cfcf":"markdown","1015fb6b":"markdown","22c357c2":"markdown","3f3136be":"markdown","90069bac":"markdown","5c30400a":"markdown","7c38aca6":"markdown","9dd101f6":"markdown","cef347d8":"markdown","28c5589f":"markdown","0f66f5dc":"markdown","4ad9175c":"markdown","f1db98ee":"markdown","7e435d34":"markdown","fac95a78":"markdown","4e027fab":"markdown","21053cf0":"markdown","ee4017e6":"markdown","ba6a61f5":"markdown","e956f75b":"markdown","19a3f468":"markdown","0e35a2ab":"markdown","67c4babc":"markdown","a62bdfae":"markdown","dd9eda7f":"markdown","7d22166a":"markdown","d4d7bccb":"markdown","e5a40651":"markdown","c739c8ec":"markdown","21b6b27c":"markdown","4f4910a0":"markdown","924a938e":"markdown","7e087ce9":"markdown","60606134":"markdown","12e6384e":"markdown","98671af4":"markdown","ab11bff2":"markdown","bfd354e3":"markdown","68f49681":"markdown","48246e3e":"markdown","5359af02":"markdown","7195cbd4":"markdown"},"source":{"23321832":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings # Ignores any warning\nwarnings.filterwarnings(\"ignore\")","8c98f151":"!ls ..\/input\/big-mart-sales\ntrainfile = \"..\/input\/big-mart-sales\/Train.csv\"\ntestfile = \"..\/input\/big-mart-sales\/Test.csv\"\ntrain = pd.read_csv(trainfile) \ntest = pd.read_csv(testfile) ","065e26c9":"train.head()\ntrain.info()\ntrain.describe()","12e9d794":"#Check for duplicates\nidsUnique = len(set(train.Item_Identifier))\nidsTotal = train.shape[0]\nidsDupli = idsTotal - idsUnique\nprint(\"There are \" + str(idsDupli) + \" duplicate IDs for \" + str(idsTotal) + \" total entries\")","a056bd35":"plt.style.use('fivethirtyeight')\nplt.figure(figsize=(12,7))\nsns.distplot(train.Item_Outlet_Sales, bins = 25)\nplt.ticklabel_format(style='plain', axis='x', scilimits=(0,1))\nplt.xlabel(\"Item_Outlet_Sales\")\nplt.ylabel(\"Number of Sales\")\nplt.title(\"Item_Outlet_Sales Distribution\")\n","d9b068ab":"print (\"Skew is:\", train.Item_Outlet_Sales.skew())\nprint(\"Kurtosis: %f\" % train.Item_Outlet_Sales.kurt())","c2bc1537":"numeric_features = train.select_dtypes(include=[np.number])\nnumeric_features.dtypes","8c935db4":"numeric_features.corr()","1da1eae3":"corr = numeric_features.corr()\n\nprint (corr['Item_Outlet_Sales'].sort_values(ascending=False))","4848045c":"#correlation matrix\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corr, vmax=.8, square=True);","1f7fea0f":"train.Item_Fat_Content.value_counts()","92f7f8fd":"sns.countplot(train.Item_Fat_Content)\n","e5f24158":"train.Item_Type.value_counts()","feb52b02":"sns.countplot(train.Item_Type)\nplt.xticks(rotation=90)","d0ae7bed":"train.Outlet_Size.value_counts()","bb52feee":"sns.countplot(train.Outlet_Size)","148170c0":"train.Outlet_Location_Type.value_counts()","5d3dfd4c":"sns.countplot(train.Outlet_Location_Type)","efd42244":"train.Outlet_Type.value_counts()","9b7eaa3a":"sns.countplot(train.Outlet_Type)\nplt.xticks(rotation=90)","86c457d2":"plt.figure(figsize=(12,7))\nplt.xlabel(\"Item_Weight\")\nplt.ylabel(\"Item_Outlet_Sales\")\nplt.title(\"Item_Weight and Item_Outlet_Sales Analysis\")\nplt.plot(train.Item_Weight, train[\"Item_Outlet_Sales\"],'.', alpha = 0.3)","aa4c4758":"plt.figure(figsize=(12,7))\nplt.xlabel(\"Item_Visibility\")\nplt.ylabel(\"Item_Outlet_Sales\")\nplt.title(\"Item_Visibility and Item_Outlet_Sales Analysis\")\nplt.plot(train.Item_Visibility, train[\"Item_Outlet_Sales\"],'.', alpha = 0.3)","bb4a0e78":"plt.figure(figsize=(12,7))\nplt.xlabel(\"Item_MRP\")\nplt.ylabel(\"Item_Outlet_Sales\")\nplt.title(\"Item_MRP and Item_Outlet_Sales Analysis\")\nplt.plot(train.Item_MRP, train[\"Item_Outlet_Sales\"],'.', alpha = 0.3)","ced60732":"Outlet_Establishment_Year_pivot = \\\ntrain.pivot_table(index='Outlet_Establishment_Year', values=\"Item_Outlet_Sales\", aggfunc=np.median)\n\nOutlet_Establishment_Year_pivot.plot(kind='bar', color='blue',figsize=(12,7))\nplt.xlabel(\"Outlet_Establishment_Year\")\nplt.ylabel(\"Item_Outlet_Sales\")\nplt.title(\"Impact of Outlet_Establishment_Year on Item_Outlet_Sales\")\nplt.xticks(rotation=0)\nplt.show()","b79e6fc0":"numeric_features = train.select_dtypes(include=[np.object])\nnumeric_features.dtypes","5dfc8f91":"plt.figure(figsize=(12,7))\nplt.xlabel(\"Item_Identifier\")\nplt.ylabel(\"SQRT Item_Outlet_Sales\")\nplt.title(\"Item_Identifier  and Item_Outlet_Sales Analysis\")\nplt.plot(train.Item_Identifier , train[\"Item_Outlet_Sales\"],'.', alpha = 0.3)","e4c927c8":"Item_Fat_Content_pivot = \\\ntrain.pivot_table(index='Item_Fat_Content', values=\"Item_Outlet_Sales\", aggfunc=np.median)\n\nItem_Fat_Content_pivot.plot(kind='bar', color='blue',figsize=(12,7))\nplt.xlabel(\"Item_Fat_Content\")\nplt.ylabel(\"Item_Outlet_Sales\")\nplt.title(\"Impact of Item_Fat_Content on Item_Outlet_Sales\")\nplt.xticks(rotation=0)\nplt.show()","39cb8e7e":"Outlet_Identifier_pivot = \\\ntrain.pivot_table(index='Outlet_Identifier', values=\"Item_Outlet_Sales\", aggfunc=np.median)\n\nOutlet_Identifier_pivot.plot(kind='bar', color='blue',figsize=(12,7))\nplt.xlabel(\"Outlet_Identifier \")\nplt.ylabel(\"Item_Outlet_Sales\")\nplt.title(\"Impact of Outlet_Identifier on Item_Outlet_Sales\")\nplt.xticks(rotation=0)\nplt.show()","7ac33e24":"train.pivot_table(values='Outlet_Type', columns='Outlet_Identifier',aggfunc=lambda x:x.mode())","2a29a8ea":"train.pivot_table(values='Outlet_Type', columns='Outlet_Size',aggfunc=lambda x:x.mode())","28e3ff41":"Outlet_Size_pivot = \\\ntrain.pivot_table(index='Outlet_Size', values=\"Item_Outlet_Sales\", aggfunc=np.median)\n\nOutlet_Size_pivot.plot(kind='bar', color='blue',figsize=(12,7))\nplt.xlabel(\"Outlet_Size \")\nplt.ylabel(\"Item_Outlet_Sales\")\nplt.title(\"Impact of Outlet_Size on Item_Outlet_Sales\")\nplt.xticks(rotation=0)\nplt.show()","c05fd1de":"Outlet_Location_Type_pivot = \\\ntrain.pivot_table(index='Outlet_Location_Type', values=\"Item_Outlet_Sales\", aggfunc=np.median)\n\nOutlet_Location_Type_pivot.plot(kind='bar', color='blue',figsize=(12,7))\nplt.xlabel(\"Outlet_Location_Type \")\nplt.ylabel(\"Item_Outlet_Sales\")\nplt.title(\"Impact of Outlet_Location_Type on Item_Outlet_Sales\")\nplt.xticks(rotation=0)\nplt.show()","66e01b3d":"train.pivot_table(values='Outlet_Location_Type', columns='Outlet_Type',aggfunc=lambda x:x.mode())","223125ab":"Outlet_Type_pivot = \\\ntrain.pivot_table(index='Outlet_Type', values=\"Item_Outlet_Sales\", aggfunc=np.median)\n\nOutlet_Type_pivot.plot(kind='bar', color='blue',figsize=(12,7))\nplt.xlabel(\"Outlet_Type \")\nplt.ylabel(\"Item_Outlet_Sales\")\nplt.title(\"Impact of Outlet_Type on Item_Outlet_Sales\")\nplt.xticks(rotation=0)\nplt.show()","377ebd92":"pivoTable = \\\ntrain.pivot_table(index='Item_Type', values=\"Item_Outlet_Sales\", aggfunc=np.mean)\n\npivoTable.plot(kind='bar', color='blue',figsize=(12,7))\nplt.xlabel(\"Item_Type \")\nplt.ylabel(\"Item_Outlet_Sales\")\nplt.title(\"Impact of Item_Type on Item_Outlet_Sales\")\nplt.xticks(rotation=90)\nplt.show()","5433916e":"pivoTable = \\\ntrain.pivot_table(index='Item_Type', values=\"Item_Visibility\", aggfunc=np.mean)\n\npivoTable.plot(kind='bar', color='blue',figsize=(12,7))\nplt.xlabel(\"Item_Type \")\nplt.ylabel(\"Item_Visibility\")\nplt.title(\"Item_Type vs Item_Visibility\")\nplt.xticks(rotation=90)\nplt.show()","2dfa3dc6":"# Join Train and Test Dataset\ntrain['source']='train'\ntest['source']='test'\n\ndata = pd.concat([train,test], ignore_index = True)\ndata.to_csv(\"data.csv\",index=False)\nprint(train.shape, test.shape, data.shape)","875edf50":" #aggfunc is mean by default! Ignores NA by default\nitem_avg_weight = data.pivot_table(values='Item_Weight', index='Item_Identifier', aggfunc=lambda x:x.mode())\nprint(item_avg_weight)","31ed9a1d":"def impute_weight(cols):\n    Weight = cols[0]\n    Identifier = cols[1]\n    \n    if pd.isnull(Weight):\n        return item_avg_weight['Item_Weight'][item_avg_weight.index == Identifier]\n    else:\n        return Weight","67de48b1":"print ('Orignal #missing: %d'%sum(data['Item_Weight'].isnull()))\ndata['Item_Weight'] = data[['Item_Weight','Item_Identifier']].apply(impute_weight,axis=1).astype(float)\nprint ('Final #missing: %d'%sum(data['Item_Weight'].isnull()))","0dac9e48":"#Import mode function:\nfrom scipy.stats import mode\n\n#Determing the mode for each\noutlet_size_mode = data.pivot_table(values='Outlet_Size', columns='Outlet_Type',aggfunc=lambda x:x.mode())\noutlet_size_mode","d3b648bb":"def impute_size_mode(cols):\n    Size = cols[0]\n    Type = cols[1]\n    if pd.isnull(Size):\n        return outlet_size_mode.loc['Outlet_Size'][outlet_size_mode.columns == Type][0]\n    else:\n        return Size\n\nprint ('Orignal #missing: %d'%sum(data['Outlet_Size'].isnull()))\ndata['Outlet_Size'] = data[['Outlet_Size','Outlet_Type']].apply(impute_size_mode,axis=1)\nprint ('Final #missing: %d'%sum(data['Outlet_Size'].isnull()))","d98f474a":"#Creates pivot table with Outlet_Type and the mean of Item_Outlet_Sales. Agg function is by default mean()\ndata.pivot_table(values='Item_Outlet_Sales', columns='Outlet_Type')","20bc294a":"#Get all Item_Visibility mean values for respective Item_Identifier\nvisibility_item_avg = data.pivot_table(values='Item_Visibility',index='Item_Identifier')","21fec49c":"def impute_visibility_mean(cols):\n    visibility = cols[0]\n    item = cols[1]\n    if visibility == 0:\n        return visibility_item_avg['Item_Visibility'][visibility_item_avg.index == item]\n    else:\n        return visibility\n\nprint ('Original #zeros: %d'%sum(data['Item_Visibility'] == 0))\ndata['Item_Visibility'] = data[['Item_Visibility','Item_Identifier']].apply(impute_visibility_mean,axis=1).astype(float)\nprint ('Final #zeros: %d'%sum(data['Item_Visibility'] == 0))","909576f8":"#Years:\ndata['Outlet_Years'] = 2013 - data['Outlet_Establishment_Year']\ndata['Outlet_Years'].describe()","3f1464c0":"#Get the first two characters of ID:\ndata['Item_Type_Combined'] = data['Item_Identifier'].apply(lambda x: x[0:2])\n#Rename them to more intuitive categories:\ndata['Item_Type_Combined'] = data['Item_Type_Combined'].map({'FD':'Food',\n                                                             'NC':'Non-Consumable',\n                                                             'DR':'Drinks'})\ndata['Item_Type_Combined'].value_counts()","314f002a":"#Change categories of low fat:\nprint('Original Categories:')\nprint(data['Item_Fat_Content'].value_counts())\n\nprint('\\nModified Categories:')\ndata['Item_Fat_Content'] = data['Item_Fat_Content'].replace({'LF':'Low Fat',\n                                                             'reg':'Regular',\n                                                             'low fat':'Low Fat'})\n\nprint(data['Item_Fat_Content'].value_counts())","bb9338c5":"#Mark non-consumables as separate category in low_fat:\ndata.loc[data['Item_Type_Combined']==\"Non-Consumable\",'Item_Fat_Content'] = \"Non-Edible\"\ndata['Item_Fat_Content'].value_counts()","7af59415":"func = lambda x: x['Item_Visibility']\/visibility_item_avg['Item_Visibility'][visibility_item_avg.index == x['Item_Identifier']][0]\ndata['Item_Visibility_MeanRatio'] = data.apply(func,axis=1).astype(float)\ndata['Item_Visibility_MeanRatio'].describe()","4c2388f5":"#Import library:\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\n#New variable for outlet\ndata['Outlet'] = le.fit_transform(data['Outlet_Identifier'])\nvar_mod = ['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','Item_Type_Combined','Outlet_Type','Outlet']\nle = LabelEncoder()\nfor i in var_mod:\n    data[i] = le.fit_transform(data[i])","976bab5e":"#Dummy Variables:\ndata = pd.get_dummies(data, columns=['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','Outlet_Type',\n                              'Item_Type_Combined','Outlet'])\n\ndata.dtypes","5353c3c4":"#Drop the columns which have been converted to different types:\ndata.drop(['Item_Type','Outlet_Establishment_Year'],axis=1,inplace=True)\n\n#Divide into test and train:\ntrain = data.loc[data['source']==\"train\"]\ntest = data.loc[data['source']==\"test\"]\n\n#Drop unnecessary columns:\ntest.drop(['Item_Outlet_Sales','source'],axis=1,inplace=True)\ntrain.drop(['source'],axis=1,inplace=True)\n\n#Export files as modified versions:\ntrain.to_csv(\"train_modified.csv\",index=False)\ntest.to_csv(\"test_modified.csv\",index=False)","5e8555e4":"train_df = pd.read_csv('train_modified.csv')\ntest_df = pd.read_csv('test_modified.csv')\n\ntarget = 'Item_Outlet_Sales'\nIDcol = ['Item_Identifier','Outlet_Identifier']\n# predictors = train_df.columns.drop(['Item_Outlet_Sales','Item_Identifier','Outlet_Identifier', 'Item_Fat_Content_1', 'Item_Type_Combined_0', 'Outlet_Location_Type_1', 'Item_Fat_Content_2', 'Outlet_Location_Type_2', 'Outlet_0', 'Outlet_Location_Type_0', 'Outlet_9', 'Outlet_3', 'Outlet_1', 'Outlet_Size_2', 'Outlet_4', 'Item_Type_Combined_2'])\npredictors = train_df.columns.drop(['Item_Outlet_Sales','Item_Identifier','Outlet_Identifier'])","af782ce6":"from sklearn import metrics\nfrom sklearn.model_selection import cross_validate\n\n#Export submission file:\ndef subsave(IDcol, target, dtest):\n    IDcol.append(target)\n    submission = pd.DataFrame({ x: dtest[x] for x in IDcol})\n    return submission\n\ndef modelfit(alg, dtrain, dtest, predictors, target, IDcol, filename):\n    #Fit the algorithm on the data\n    alg.fit(dtrain[predictors], dtrain[target])\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain[predictors])\n\n    #Perform cross-validation:\n    cv_score = cross_validate(alg, dtrain[predictors],(dtrain[target]) , cv=20, scoring='neg_mean_squared_error')\n    cv_score = np.sqrt(np.abs(cv_score['test_score']))\n    \n    #Print model report:\n    print(\"\\nModel Report\")\n    print(\"RMSE : %.4g\" % np.sqrt(metrics.mean_squared_error((dtrain[target]).values, dtrain_predictions)))\n    print(\"CV Score : Mean - %.4g | Std - %.4g | Min - %.4g | Max - %.4g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n    \n    #Predict on testing data:\n    dtest[target] = alg.predict(dtest[predictors])\n    \n    return subsave(IDcol, target, dtest)","180b684d":"from sklearn.linear_model import LinearRegression\nLR = LinearRegression(normalize=True)\n\npredictions = modelfit(LR, train_df, test_df, predictors, target, IDcol, 'LR.csv')\n\ncoef1 = pd.Series(LR.coef_, predictors).sort_values()\ncoef1.plot(kind='bar', title='Model Coefficients')","d42e667b":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(train_df, test_size=.2, random_state=42)","2ba6b637":"from xgboost import XGBRegressor\nfrom sklearn.ensemble import BaggingRegressor\n\n# xgb = XGBRegressor(n_estimators=1000, learning_rate=.1, colsample_bytree=.8, max_depth=4, reg_alpha=.45, subsample=0.8, tree_method='gpu_hist', objective='reg:squarederror', n_jobs=16, random_state=42, importance_type='total_gain')\n# xgb.fit(train[predictors], train[target], early_stopping_rounds=400, \n#              eval_set=[(test[predictors], test[target])], verbose=True)\n\n# # bxgb = BaggingRegressor(xgb, n_estimators=50, random_state=42)\n# # bxgb.fit(train_df[predictors], train_df[target])\n","e2137b91":"# #Predict training set:\n# train_df_predictions = xgb.predict(train_df[predictors])\n\n# # make predictions\n# predictions = xgb.predict(test_df[predictors])\n\n# from xgboost import plot_tree\n# from matplotlib import pyplot as plt\n\n# from sklearn.metrics import mean_absolute_error\n# print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_df[target])))\n# print(\"RMSE : %.4g\" % np.sqrt(metrics.mean_squared_error((train_df[target]).values, train_df_predictions)))\n\n# coef = pd.Series(xgb.feature_importances_, predictors).sort_values(ascending=False)\n# print(coef.index.tolist())\n# coef.plot(kind='bar', title='Feature Importances')\n\n# # plot_tree(xgb)\n# # plt.show()\n\n# test_df[target] = predictions\n# submission = subsave(IDcol, target, test_df)","25bfd7be":"# from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor\n# from sklearn.tree import ExtraTreeRegressor\n\n# RF = RandomForestRegressor(n_estimators=400, max_depth=8, min_samples_leaf=100, min_samples_split=12, oob_score=True, n_jobs=-1, random_state=42)\n\n# ET = ExtraTreeRegressor(splitter='best', max_depth=8, min_samples_leaf=75, min_samples_split=16, random_state=42) # 1061\n\n# EST = ExtraTreesRegressor(n_estimators=25, max_depth=10, min_samples_leaf=100, min_samples_split=12, n_jobs=16, bootstrap=True, oob_score=True, random_state=42)\n\n# BET = BaggingRegressor(base_estimator=ET, n_estimators=50, max_samples=.9, bootstrap_features=True, oob_score=True, n_jobs=8, random_state=42)\n\n# AET = AdaBoostRegressor(EST, n_estimators=20, learning_rate=.01, loss='square', random_state=42) # 1043\n\n# GB = GradientBoostingRegressor(loss='huber', learning_rate=.01, n_estimators=250, min_samples_split=10, min_samples_leaf=70, max_depth=7, subsample=.8, random_state=42)\n\n# model = GB\n\n# # submission = modelfit(GB, train_df, test_df, predictors, target, IDcol, 'RF.csv')\n# # submission = modelfit(AET, train_df, test_df, predictors, target, IDcol, 'RF.csv')\n# # test_df[target] = (GB.predict(test_df[predictors]) + AET.predict(test_df[predictors])) \/ 2\n# # submission = subsave(IDcol, target, test_df)\n\n# submission = modelfit(model, train_df, test_df, predictors, target, IDcol, 'RF.csv')","0f1b1331":"# coef = pd.Series(model.feature_importances_, predictors).sort_values(ascending=False)\n# print(coef.index.tolist())\n# coef.plot(kind='bar', title='Feature Importances')","632d1d00":"# from lightgbm import LGBMRegressor, plot_importance\n\n# lgb = LGBMRegressor(max_depth=5, learning_rate=.0005, n_estimators=5000, subsample=.85, subsample_freq=1, colsample_bytree=.8, reg_alpha=.4, reg_lambda=1, random_state=42)\n\n# lgb.fit(train[predictors], train[target], early_stopping_rounds=250, \n#              eval_set=[(test[predictors], test[target])], verbose=False)\n\n\n# train_df_predictions = lgb.predict(train_df[predictors])\n\n# # make predictions\n# predictions = lgb.predict(test_df[predictors])","3c9f11cf":"# from sklearn.metrics import mean_absolute_error\n# print(\"RMSE : %.4g\" % np.sqrt(metrics.mean_squared_error((train_df[target]).values, train_df_predictions)))\n\n# plot_importance(lgb, figsize=(20,10))\n\n# test_df[target] = predictions\n# submission = subsave(IDcol, target, test_df)","2afb326d":"# from keras.models import Sequential\n# from keras.layers.core import Dense, Activation, Dropout\n# from keras.callbacks import EarlyStopping","cd340eeb":"# from sklearn.preprocessing import StandardScaler\n\n# normalized = train_df.columns.drop(['Item_Outlet_Sales','Item_Identifier','Outlet_Identifier', 'Item_MRP'])\n\n# scaler = StandardScaler(with_std=True, with_mean=False)\n# scaler.fit(train[normalized])\n\n# train_keras = train.copy()\n# test_keras = test.copy()\n# result = test_df.copy()\n\n# # train_keras[normalized] = pd.DataFrame(scaler.transform(train[normalized]))\n# # train_keras[target] = train[target]\n# # test_keras[normalized] = pd.DataFrame(scaler.transform(test[normalized]))\n# # test_keras[target] = test[target]\n# # result[normalized] = pd.DataFrame(scaler.transform(test_df[normalized]))","b5f7998a":"# train_keras.head()","0995b5a1":"# model = Sequential()\n# model.add(Dense(128, input_dim=len(predictors)))\n# model.add(Activation('relu'))\n# model.add(Dense(256))\n# model.add(Activation('relu'))\n# # model.add(Dropout(0.25))\n# model.add(Dense(256))\n# model.add(Activation('elu'))\n# model.add(Dense(256))\n# model.add(Activation('elu'))\n# # model.add(Dropout(0.5))\n# model.add(Dense(64))\n# model.add(Activation('elu'))\n# model.add(Dense(1))\n# model.add(Activation('relu'))","b0d8e5e7":"# num_epochs=200\n\n# # we'll use categorical xent for the loss, and RMSprop as the optimizer\n# model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n# criterion = EarlyStopping(patience=int(num_epochs * .4))\n\n# print(\"Training...\")\n# model.fit(train_keras[predictors], train_keras[target], epochs=num_epochs, batch_size=8, validation_split=.25, callbacks = [criterion], verbose=1)\n\n# print(\"Generating test predictions...\")\n# test_predictions = model.predict(test[predictors], verbose=0)","e3d7416a":"# from sklearn.metrics import mean_absolute_error\n# print(\"RMSE : %.4g\" % np.sqrt(metrics.mean_squared_error((test_keras[target]).values, test_predictions)))\n\n# result[target] = model.predict(result[predictors])\n# submission = subsave(IDcol, target, result)","57d46424":"# import the modules we'll need\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"submission.csv\"):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create_download_link(submission)","ee60ac27":"**Linear Regression**","01f3b6f7":"<h5>1.1.3.3. Distribution of the Outlet_Size<\/h5>","7aa4cfcf":"<h2>2. Data Pre-Processing<\/h2>\n<h3> 2.1. Looking for missing\u00a0values<\/h3>","1015fb6b":"<h3>1.2. Bivariate Distribution<\/h3>\n<h4>1.2.1. Numerical Variables<\/h4>\n<h5>1.2.1.1. Item_Weight and Item_Outlet_Sales Analysis<\/h5>","22c357c2":"<h5>1.2.2.5. Impact of Outlet_Location_Type on Item_Outlet_Sales<\/h5>","3f3136be":"## Project #1: Bigmart Sale Prediction","90069bac":"<h5>1.2.2.3. Impact of Outlet_Identifier on Item_Outlet_Sales<\/h5>","5c30400a":"<h4>1.2.2. Categorial Variables<\/h4>","7c38aca6":"<h5>1.1.3.2. Distribution of the Item_Type<\/h5>","9dd101f6":"<h5>1.2.2.2. Impact of Item_Fat_Content on Item_Outlet_Sales<\/h5>","cef347d8":"<h5>1.2.1.3. Item_MRP and Item_Outlet_Sales Analysis<\/h5>","28c5589f":"<h2>3. Feature Engineering<\/h2>","0f66f5dc":"**Keras**","4ad9175c":"<h4> 4.2. Numerical and Categorical Variables \u2013 Dummy variables<\/h4>","f1db98ee":"<h5>1.1.3.4. Distribution of the Outlet_Location_Type<\/h5>","7e435d34":"<h4> 3.4. Create a broad category of Type of Item<\/h4>","fac95a78":"<h4>1.1.3. Categorical Variables<\/h4>\n<h5>1.1.3.1. Distribution of the Item_Fat_Content<\/h5>","4e027fab":"<h4 style = \"text-align:justify;font-family:Verdana;font-size:14px\">\n**XGBoost**\n<\/h4>","21053cf0":"<h4> 4.3. Exporting Data<\/h4>\n","ee4017e6":"<h4> 3.2. Item_Visibility minimum value 0<\/h4>","ba6a61f5":"<h3> 2.2. Imputing Missing Values <\/h3>","e956f75b":"<h2>5. Model, predict and solve the problem<\/h2>","19a3f468":"<h5>1.2.2.7. Impact of Item_Type on Item_Outlet_Sales<\/h5>","0e35a2ab":"<h2>4. Feature Transformations<\/h2>","67c4babc":"<h5>1.2.2.4. Impact of Outlet_Size on Item_Outlet_Sales<\/h5>","a62bdfae":"<h5>1.2.1.2. Item_Visibility and Item_Outlet_Sales Analysis<\/h5>","dd9eda7f":"<h3> 2.3. Imputing Outlet_size with the mode<\/h3>\n","7d22166a":"<h4> 4.1. Creating variable Item_Visibility_MeanRatio<\/h4>","d4d7bccb":"<h4> 3.1. Should we combine Outlet_Type?<\/h4>\n","e5a40651":"<a id = \"#initialization\"><\/a>\n<h2>Initializing Packages and Importing Data<\/h2>","c739c8ec":"**Tree-Stuffs**","21b6b27c":"## **6. Export**","4f4910a0":"<h4>1.1.2. Numerical Variables<\/h4>\n","924a938e":"**LightGBM**","7e087ce9":"<h2>1. Exploratory Data Analysis (EDA)<\/h2>\n<h3>1.1. Univariate Distribution<\/h3>\n<h4>1.1.1.  Distribution of the target variable\u00a0: Item_Outlet_Sales<\/h4>","60606134":"<h4> 3.5. Modify categories of Item_Fat_Content<\/h4>\n","12e6384e":"<h4> 3.3. Determine the years of operation of a store<\/h4>\n","98671af4":"<h5>1.2.2.1. Impact of Item_Identifier on Item_Outlet_Sales<\/h5>","ab11bff2":"<h5>1.2.2.8. Impact of Item_Type vs Item_Visibility<\/h5>","bfd354e3":"#### LR: 1440\n#### SVR: 1218\n\n#### DT: 1235\n#### ET: 1158\n\n#### RF: 1152\n#### EST: 1159\n#### BET: 1245\n#### BEST: 1168\n\n#### AET: 1159\n#### AEST: 1157\n#### GB: 1155\n\n#### LGBM: 1153\n#### (AEST + GB) \/ 2: 1152\n#### XGB: 1150\n\n#### KERAS: 1153","68f49681":"<a id = \"#viewdata\"><\/a>\n<h2>Taking a peak at our data<\/h2>","48246e3e":"<h5>1.2.1.4. Outlet_Establishment_Year and Item_Outlet_Sales Analysis<\/h5>","5359af02":"<h5>1.1.3.5. Distribution of the Outlet_Type<\/h5>","7195cbd4":"<h5>1.2.2.6. Impact of Outlet_Type on Item_Outlet_Sales<\/h5>"}}