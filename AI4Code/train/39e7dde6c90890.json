{"cell_type":{"5a2a09e6":"code","9979e5e1":"code","e30d66cd":"code","1fec0563":"code","f04be0eb":"code","2b623e0e":"code","513d367c":"code","8a021e98":"markdown","3aa0973d":"markdown","d094e6c0":"markdown","8aec0fcf":"markdown","a4575aba":"markdown","97763a36":"markdown"},"source":{"5a2a09e6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\nfrom sklearn.metrics import log_loss\nimport matplotlib.pyplot as plt\n% matplotlib inline","9979e5e1":"emb_size = 768\n\ndef parse_json(embeddings):\n\t'''\n\tParses the embeddigns given by BERT, and suitably formats them to be passed to the MLP model\n\n\tInput: embeddings, a DataFrame containing contextual embeddings from BERT, as well as the labels for the classification problem\n\tcolumns: \"emb_A\": contextual embedding for the word A\n\t         \"emb_B\": contextual embedding for the word B\n\t         \"emb_P\": contextual embedding for the pronoun\n\t         \"label\": the answer to the coreference problem: \"A\", \"B\" or \"NEITHER\"\n\n\tOutput: X, a numpy array containing, for each line in the GAP file, the concatenation of the embeddings of the target words\n\t        Y, a numpy array containing, for each line in the GAP file, the one-hot encoded answer to the coreference problem\n\t'''\n\tembeddings.sort_index(inplace = True) # Sorting the DataFrame, because reading from the json file messed with the order\n\tX = np.zeros((len(embeddings),3*emb_size))\n\tY = np.zeros((len(embeddings), 3))\n\n\t# Concatenate features\n\tfor i in range(len(embeddings)):\n\t\tA = np.array(embeddings.loc[i,\"emb_A\"])\n\t\tB = np.array(embeddings.loc[i,\"emb_B\"])\n\t\tP = np.array(embeddings.loc[i,\"emb_P\"])\n\t\tX[i] = np.concatenate((A,B,P))\n\n\t# One-hot encoding for labels\n\tfor i in range(len(embeddings)):\n\t\tlabel = embeddings.loc[i,\"label\"]\n\t\tif label == \"A\":\n\t\t\tY[i,0] = 1\n\t\telif label == \"B\":\n\t\t\tY[i,1] = 1\n\t\telse:\n\t\t\tY[i,2] = 1\n\n\treturn X, Y","e30d66cd":"development = pd.read_json(\"..\/input\/taming-the-bert-a-baseline\/contextual_embeddings_gap_development.json\")\nX_development, Y_development = parse_json(development)\n\n# There may be a few NaN values, where the offset of a target word is greater than the max_seq_length of BERT.\n# They are very few, so rather than dealing with the problem, I'm replacing those rows with random values.\nremove_development = [row for row in range(len(X_development)) if np.sum(np.isnan(X_development[row]))]\nX_development[remove_development] = np.random.randn(3*emb_size)","1fec0563":"n_rows = len(X_development)\nindex = list(range(n_rows))\ndistance = pd.DataFrame(index = index, columns = [\"d_PA\", \"d_PB\", \"d_AB\"])\nfor i in index:\n\tdistance.loc[i,\"d_PA\"] = np.linalg.norm(X_development[i,2*emb_size:] - X_development[i,:emb_size], ord = 2)  \n\tdistance.loc[i,\"d_PB\"] = np.linalg.norm(X_development[i,emb_size:2*emb_size] - X_development[i,2*emb_size:], ord = 2) \n\tdistance.loc[i,\"d_AB\"] = np.linalg.norm(X_development[i,:emb_size] - X_development[i,emb_size:2*emb_size], ord = 2)","f04be0eb":"plt.scatter(distance[\"d_PA\"] \/ distance[\"d_PB\"], \n            distance[\"d_AB\"] \/ (distance[\"d_PB\"]+ distance[\"d_PA\"]), \n            c = np.argmax(Y_development[:n_rows], axis = 1), \n            alpha = 0.5)\nplt.xlabel(\"d(A,Pronoun) \/ d(B,Pronoun)\")\nplt.ylabel(\"d(A,B) \/ d(A,Pronoun) + d(B,Pronoun)\")\nplt.colorbar()\nplt.show()","2b623e0e":"threshold = -1\n\ndef softmax(v):\n\texp = np.exp(v)\n\treturn exp \/ np.sum(exp, axis = 1, keepdims = True)\n\ndistance[\"A\/B\"] = 1- distance[\"d_PA\"] \/ distance[\"d_PB\"] \ndistance[\"B\/A\"] = 1- distance[\"d_PB\"] \/ distance[\"d_PA\"]  \ndistance[\"N\/A+B\"] = threshold - distance[\"d_AB\"] \/ (distance[\"d_PB\"]+ distance[\"d_PA\"])\nvalues = distance[[\"A\/B\", \"B\/A\", \"N\/A+B\"]].values.astype(float)\nprediction = softmax(values)\nprint(\"The score is :\", log_loss(Y_development, prediction))","513d367c":"submission = pd.read_csv(\"..\/input\/gendered-pronoun-resolution\/sample_submission_stage_1.csv\", index_col = \"ID\")\nsubmission[\"A\"] = prediction[:,0]\nsubmission[\"B\"] = prediction[:,1]\nsubmission[\"NEITHER\"] = prediction[:,2]\nsubmission.to_csv(\"submission_bert_more_data.csv\")","8a021e98":"I'm reading the contextual embeddings obtained from BERT in my previous kernel, and saved in a json file. I'm also reading true labels, which are used in the plot below, and to evaluate the performance at the end.","3aa0973d":"I've been wondering how far one can get in this competition using an unsupervised learning approach. So I decided to play with the contextual embeddings I obtained from BERT in my previous [kernel](https:\/\/www.kaggle.com\/mateiionita\/taming-the-bert-a-baseline), and try to make predictions without having a training set with ground truth labels.\n\nSpoiler alert: I didn't get very far, the best score I could get was around 0.93. Still, I think it's instructive to visualize some of the output of BERT. Specifically, I'm looking at:\n* The Euclidean distance d(A,P) between the embeddings of the Pronoun and the target word A.\n* The Euclidean distance d(B,P).\n* The Euclidean distance d(A,B) between the two target words.\n\nThis is low-dimensional information that you can plot, and then use to make predictions for the coreference resolution problem. The simplest approach would be:\n* If d(A,B) is large compared to d(A,P) and d(B,P), in a sense that will be made precise later, classify the data as \"Neither\".\n* Otherwise, if d(A,P) < d(B,P), classify the data as \"A\". If d(A,P) > d(B,P), classify the data as \"B\".\n\nThis is more or less what I do below. To obtain class probabilities, I pass the distances through a softmax function.","d094e6c0":"Compute the Euclidean distances between contextual embeddings.","8aec0fcf":"Plot the relative sizes of the distances computed above. The purple points, which have true label \"A\", tend to appear in the left half of the plot. The blue points, which have true label \"B\", tend to appear in the right. Finally, the yellow points, which have class \"Neither\", are more likely to appear towards the bottom.","a4575aba":"For prediction, I want to compare d(A,P)\/d(B,P) to 1, and d(A,B) \/  (d(B,P) + d(A,P)) to some threshold value, then pass these through a softmax function to obtain probabilities. This threshold is the only parameter in my model. Finding an appropriate value for it is where I'm cheating and using information about the true labels. Intuitively, there are much fewer \"Neither\" data points than either \"A\" or \"B\". Choosing a negative threshold biases the probability for class \"Neither\" to be very low","97763a36":"The score is nothing to write home about, but I had my fun."}}