{"cell_type":{"ed17ab9f":"code","5fc7a2fe":"code","92ed6a6e":"code","08dfc666":"code","71ff5164":"code","630fc0d4":"code","92f35ff0":"code","9eb5da3b":"code","8a2d77c7":"code","5d31760f":"code","841d94a4":"code","a7baca2b":"code","4768b6e2":"code","fc19bfc7":"code","77f9cb54":"code","c43b65b7":"code","5edf38bf":"code","5aad37ce":"code","1ae16b15":"code","a79fabfe":"code","89118013":"markdown","4fb9a2e0":"markdown","59a85702":"markdown"},"source":{"ed17ab9f":"\"Importing Packages\" \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels as smd\nfrom astropy.table import Table, Column\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.svm import SVC # \"Support Vector Classifier\" \nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import svm\nfrom sklearn.preprocessing import Imputer","5fc7a2fe":"\"Reading data into Python\" \nbank_Train = pd.read_csv(\"..\/input\/bank-train.csv\")\nbank_Test = pd.read_csv(\"..\/input\/bank-test.csv\")\nbank_Train = bank_Train.drop(columns=['duration'])\nbank_Test = bank_Test.drop(columns=['duration'])\nbank_Test.shape","92ed6a6e":"\"exploring data\" \nbank_Train.head()\nbank_Train.columns\nbank_Train.dtypes\nbank_Train['marital']\nbank_Train['y']\nbank_Train.describe()\nbank_Train.shape\nbank_Test.shape","08dfc666":"\"splitting data into numerical and categorical variables\"\nbank_Train_Numerical = bank_Train.select_dtypes(exclude=['object'])\nbank_Train_Numerical.columns\nbank_Train_Numerical.dtypes\nbank_Train_Numerical['pdays']\nbank_Train_Numerical['previous']\nbank_Train_Numerical['emp.var.rate']","71ff5164":"bank_Train_Categorical = bank_Train.select_dtypes(include=['object'])\nbank_Train_Y = bank_Train['y']\nbank_Train_Categorical = pd.concat([bank_Train_Categorical, bank_Train_Y], axis=1)\nbank_Train_Categorical.columns\nbank_Train_Categorical.dtypes","630fc0d4":"bank_Test_Numerical = bank_Test.select_dtypes(exclude=['object'])\nbank_Test_Numerical.columns\nbank_Test_Numerical.dtypes\nbank_Test_Numerical['pdays']\nbank_Test_Numerical['previous']\nbank_Test_Numerical['emp.var.rate']","92f35ff0":"bank_Test_Categorical = bank_Test.select_dtypes(include=['object'])\nbank_Test_Categorical.columns\nbank_Test_Categorical.dtypes","9eb5da3b":"\"exploring relationships in numerical variables\"\n\"exploring distribution\"\nageDistributionPlot = sns.distplot(bank_Train_Numerical['age'], hist=True, kde=True, color = 'red', hist_kws={'edgecolor':'black'})\ncampaignDistributionPlot = sns.distplot(bank_Train_Numerical['campaign'], hist=True, kde=False, color = 'red', hist_kws={'edgecolor':'black'})\npdaysDistributionPlot = sns.distplot(bank_Train_Numerical['pdays'], hist=True, kde=False, color = 'red', hist_kws={'edgecolor':'black'})\npreviousDistributionPlot = sns.distplot(bank_Train_Numerical['previous'], hist=True, kde=False, color = 'red', hist_kws={'edgecolor':'black'})\nempvarrateDistributionPlot = sns.distplot(bank_Train_Numerical['emp.var.rate'], hist=True, kde=False, color = 'red', hist_kws={'edgecolor':'black'})\nconspriceidxDistributionPlot = sns.distplot(bank_Train_Numerical['cons.price.idx'], hist=True, kde=False, color = 'red', hist_kws={'edgecolor':'black'})\nconsconfidxDistributionPlot = sns.distplot(bank_Train_Numerical['cons.conf.idx'], hist=True, kde=True, color = 'red', hist_kws={'edgecolor':'black'})\neuribor3mDistributionPlot = sns.distplot(bank_Train_Numerical['euribor3m'], hist=True, kde=True, color = 'red', hist_kws={'edgecolor':'black'})\nnremployedDistributionPlot = sns.distplot(bank_Train_Numerical['nr.employed'], hist=True, kde=True, color = 'red', hist_kws={'edgecolor':'black'})\nyDistributionPlot = sns.distplot(bank_Train_Numerical['y'], hist=True, kde=False, color = 'red', hist_kws={'edgecolor':'black'})","8a2d77c7":"\"exploring relationships between variables\"\nvariableCorrelogram = sns.pairplot(bank_Train_Numerical)\nplt.show()\n   \n\"summarizing relationship dynamic between variables\" \nvarCorrelation = bank_Train_Numerical.corr()\nfig, ax = plt.subplots(figsize=(10, 10))\ncorrelationHeatmap = sns.heatmap(varCorrelation, annot=True, fmt=\".3f\")","5d31760f":"\"exploring relationships in categorical variables\" \nfig, ax = plt.subplots(figsize=(15, 10))\njobPlot = sns.countplot(x=\"job\", hue=\"y\", data=bank_Train_Categorical)\nmaritalPlot = sns.countplot(x='marital', hue='y', data=bank_Train_Categorical)\nfig, ax = plt.subplots(figsize=(20, 10))\neducationPlot = sns.countplot(x='education', hue='y', data=bank_Train_Categorical)\ndefaultPlot = sns.countplot(x='default', hue='y', data=bank_Train_Categorical)\nhousingPlot = sns.countplot(x='housing', hue='y', data=bank_Train_Categorical)\nloanPlot = sns.countplot(x='loan', hue='y', data=bank_Train_Categorical)\ncontactPlot = sns.countplot(x='contact', hue='y', data=bank_Train_Categorical)\nmonthPlot = sns.countplot(x='month', hue='y', data=bank_Train_Categorical)\ndayPlot = sns.countplot(x='day_of_week', hue='y', data=bank_Train_Categorical)\npoutcomePlot = sns.countplot(x='poutcome', hue='y', data=bank_Train_Categorical)","841d94a4":"\"Checking for NULL Values\" \nbank_Train_Numerical.isna().sum() #no null values in any numerical columns\nbank_Train_Categorical.isna().sum() #no null values in any categorical columns ALL CLEAR","a7baca2b":"\"Making 'Other' column for low frequency character values\" \ndef replace_low_frequency_wOther(columnName):\n    series = pd.value_counts(bank_Train_Categorical[columnName])\n    mask = (series\/series.sum() * 100).lt(2)\n    bank_Train_Categorical[columnName] = np.where(bank_Train_Categorical[columnName].isin(series[mask].index),'Other',bank_Train_Categorical[columnName])\n\ndef replace_low_frequency_wOther2(columnName):\n    series = pd.value_counts(bank_Test_Categorical[columnName])\n    mask = (series\/series.sum() * 100).lt(2)\n    bank_Test_Categorical[columnName] = np.where(bank_Test_Categorical[columnName].isin(series[mask].index),'Other',bank_Test_Categorical[columnName])","4768b6e2":"replace_low_frequency_wOther('job')\nreplace_low_frequency_wOther('marital')\nreplace_low_frequency_wOther('education')\nreplace_low_frequency_wOther('default')\nreplace_low_frequency_wOther('housing')\nreplace_low_frequency_wOther('loan')\nreplace_low_frequency_wOther('contact')\nreplace_low_frequency_wOther('month')\nreplace_low_frequency_wOther('day_of_week')\nreplace_low_frequency_wOther('poutcome')\n    \nreplace_low_frequency_wOther2('job')\nreplace_low_frequency_wOther2('marital')\nreplace_low_frequency_wOther2('education')\nreplace_low_frequency_wOther2('default')\nreplace_low_frequency_wOther2('housing')\nreplace_low_frequency_wOther2('loan')\nreplace_low_frequency_wOther2('contact')\nreplace_low_frequency_wOther2('month')\nreplace_low_frequency_wOther2('day_of_week')\nreplace_low_frequency_wOther2('poutcome')","fc19bfc7":"\"Calculating Variance Inflation Factor\" \n#drop when VIF is greater than 2.5 or if R^2 is greater than 0.6 approximately\nX = add_constant(bank_Train_Numerical) \nmulticollTable = pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)\nmulticollTable\n\"removing columns with VIF > 2.5\" \nbank_Train_Numerical = bank_Train_Numerical.drop(columns=['emp.var.rate', 'cons.price.idx', 'euribor3m', 'nr.employed'])\nbank_Train_Numerical.dtypes","77f9cb54":"\"setting dummy variables for categorical data\" \nbank_Train_Categorical_Dummy = pd.get_dummies(bank_Train_Categorical)\nbank_Train_Categorical_Dummy.dtypes\nbank_Test_Categorical_Dummy = pd.get_dummies(bank_Test_Categorical)\nbank_Test_Categorical_Dummy.dtypes","c43b65b7":"\"Making Test and Training Datasets\" \nframes = [bank_Train_Numerical, bank_Train_Categorical_Dummy]    \nallTestData = pd.concat(frames, axis=1)\nallTestData.dtypes\nallTestData_X = allTestData.drop(columns='y')\nallTestData_Y = allTestData['y']\nallTestData_Y.columns = [\"y\", \"y2\"]\nallTestData_Y = allTestData_Y.drop(columns=['y2'])\nX_train, X_test, y_train, y_test = train_test_split(allTestData_X, allTestData_Y, test_size=0.25)\nX_test = X_test.drop(columns=['default_Other', 'job_student'])\nX_train = X_train.drop(columns=['default_Other', 'job_student'])\n    \nframes2 = [bank_Test_Numerical, bank_Test_Categorical_Dummy]    \nallTestData2 = pd.concat(frames2, axis=1)\nallTestData2 = allTestData2.drop(columns=['emp.var.rate', 'cons.price.idx', 'euribor3m', 'nr.employed'])\nallTestData2","5edf38bf":"\"Obtaining Results using Logistic Regression\" \nlogreg2 = LogisticRegression()\nlogreg2.fit(X_train, y_train)\ny_pred2 = logreg2.predict(allTestData2)\nprint('Accuracy of logistic regression classifier on test set: {:.5f}'.format(logreg2.score(X_test, y_test)))\nprint('F1 Score of logistic regression on test set: {:.5f}'.format(f1_score(y_test, y_pred2)))\nprint(classification_report(y_test, y_pred2))","5aad37ce":"\"Obtaining Results using Support Vector Machines\"\nclf = svm.SVC(gamma='scale') \nytrain = y_train.values\nytrain = ytrain.ravel()\nclf.fit(X_train, ytrain) \ny_pred_SVM = clf.predict(allTestData2)","1ae16b15":"\"confusion matrix for Logistic regression\" \nconfusionMatrix = confusion_matrix(y_test, y_pred2)\nconfusionMatrix","a79fabfe":"t = Table(names=('Logistic Regression', 'Support Vector Machine', 'K-nearest Neighbors', 'Random Forest'))\nt.add_row((0.88709, 0.88628, 0.88142, 0.88466))\nt","89118013":"**HYPOTHESIS 3 - LOGISTIC REGRESSION**\n\nHere we try different models to fit the data and utilize the one that yields the highest F1 score. We hypothesized that logistic regression would best predict the test data because we believed that the logistic regression's algorithm was best designed to deal with the binary classification in the response variable. \n\nIn the table at the end, you will see that our hypothesis held true. Logistic regression ended up being the most accurate model.","4fb9a2e0":"**HYPOTHESIS 1 - MULTICOLLINEARITY**\n\nHere we used multicollinearity to identify exploratory variables that are correlated with each other. We recognized 4 variables that were correlated and proced a VIF above 2.5. We hypothesized that if we removed these columns our F1 score would increase. The variables that we dropped were employee variance rate, consumer price index, euribor rate, and number of employees.\n\nOur hypothesis proved to be true. Using mulitcollinearity improved the F1 score by about 0.012.","59a85702":"**HYPOTHESIS 2 - IMPUTATION**\n\nThe code below shows an attempt at imputing the unknown data values in the categorical columns. The reason that we attempted imputation was that we believed that unknown data points negatively impacted the accuracy F1 scores. Furthermore, most of these categorical columnns contained a class that *dominated* the resepective column. For instance, most of the jobs in the dataset were \"admin.\" and most people didn't default on loans. Therefore, we replaced all the unknown values with the most frequent values in the column.\n\nHowever, imputation reduced the F1 score by about 0.01. We believed this happened due to an increase in bias and overfitting. So, our hypothesis did not hold true and we decided to not use imputation as one of our model optimization techniques.\n\n\n\"imputing variables in categorical dataset\"\nimp = SimpleImputer(missing_values=\"unknown\", strategy='most_frequent')\n\nbank_Train_Categorical = pd.DataFrame(imp.fit_transform(bank_Train_Categorical))\nbank_Test_Categorical = pd.DataFrame(imp.fit_transform(bank_Test_Categorical))\n\ntrain = pd.concat([bank_Train_Categorical, bank_Train_Numerical], axis=1, sort=True)\nbank_Train_Numerical = train.select_dtypes(exclude=['object'])\nbank_Train_Categorical = train.select_dtypes(include=['object'])\n\nbank_Train_Categorical.columns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome', 'y']\nbank_Test_Categorical.columns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome']"}}