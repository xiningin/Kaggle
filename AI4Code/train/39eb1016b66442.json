{"cell_type":{"59426347":"code","3b85d781":"code","d6411e7e":"code","5a141e99":"code","abf74f60":"code","4fea00e6":"code","023628ce":"code","3446cacb":"code","e5d6ce07":"code","1b626cd7":"code","423cf73c":"code","41225d77":"code","93f4f32b":"code","574cbc55":"code","477ac141":"code","f6f08289":"markdown","40c6223d":"markdown","03125c84":"markdown","77dff1e8":"markdown","44ee0e54":"markdown","52234a15":"markdown","5266ad55":"markdown","74a2fc3b":"markdown","55a169c1":"markdown"},"source":{"59426347":"# import libraries  \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom nltk import pos_tag_sents, word_tokenize\nfrom nltk.tokenize import wordpunct_tokenize\nfrom pprint import pprint\nimport re, random, os\nimport string\n\n# spacy for basic preprocessing, optional, can use nltk as well (lemmatisation etc.)\nimport spacy\n\n# gensim for LDA \nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\n# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.gensim  # don't skip this\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)","3b85d781":"# stop words\n# string.punctuation (from the 'string' module) contains a list of punctuations\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english') + list(string.punctuation)","d6411e7e":"df = pd.read_csv('..\/input\/amazondataset\/7817_1.csv')","5a141e99":"df.head()","abf74f60":"# filter for product id = amazon echo\ndf = df[df['asins']==\"B01BH83OOM\"]\ndf.head()","4fea00e6":"# tokenize words and clean \ndef sent_to_words(sentences, deacc=True): # deacc=True removes punctuations\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence)))  \n\n\n# convert to list\ndata = df['reviews.text'].values.tolist()\ndata_words = list(sent_to_words(data))\n\nprint(data_words[3])","023628ce":"# Define functions for stopwords and lemmatization\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https:\/\/spacy.io\/api\/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out","3446cacb":"# call functions\n\n# remove Stop Words\ndata_words_nostops = remove_stopwords(data_words)\n\n# initialize spacy 'en' model, keeping only tagger component (for efficiency)\n# python3 -m spacy download en\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\n# Do lemmatization keeping only noun, adj, vb, adv\ndata_lemmatized = lemmatization(data_words_nostops, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\nprint(data_lemmatized[3])","e5d6ce07":"# compare the nostop, lemmatised version with the original one\nprint(data_words[3])","1b626cd7":"# create dictionary and corpus\n# Create Dictionary\nid2word = corpora.Dictionary(data_lemmatized)\n\n# Create Corpus\ntexts = data_lemmatized\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n# View\nprint(corpus[2])","423cf73c":"# Human readable format of corpus (term-frequency)\n[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]","41225d77":"# Build LDA model\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=10, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)","93f4f32b":"# Print the Keyword in the 10 topics\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","574cbc55":"# Compute Perplexity\nprint('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n\n# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","477ac141":"# Visualize the topics\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\nvis","f6f08289":"For building topic models, let's experiment with the Amazon product reviews dataset. We have a list of","40c6223d":"# LDA Demonstration\n\n\nIn this notebook, we will build LDA models on various datasets. We'll use the gensim implementation of LDA, though sklearn also comes with one. \n\nWe will also use the library spacy for preprocessing (specifically lemmatisation). Though you can also perform lemmatisation in NLTK, it is slightly more convenient and less verbose in spacy. For visualising the topics and the word-topic distributions (interactively!), we'll use the 'pyLDAvis' module.","03125c84":"**We will be using LDA technique**\nData set used in this Amazon review: https:\/\/www.kaggle.com\/kamalnaithani\/amazondataset\n","77dff1e8":"### Building the Topic Model","44ee0e54":"<hr>\n\n**Some problems in defining topics in single term are :**\n\n1. Polysemy: If a document has words having same meaning such as(lunch, food, cuisine, meal etc.), the model would choose only one word(say food) as topic and will ignore all the others.\n2. Word sense disambiguation : Words with multiple meanings such as \u2018stars\u2019 would be incorrectly inferred as representing one topic, though the document could have both topics(movie star and astronomical star)\n\n**As per above mentioned points we need more complex definition of topic to solve the problem of Polysemy and Word sense disambiguation**\n\n**To summarize, there are multiple advantages of defining a topic as a distribution over terms.**\n\n1. Consider two topics \u2013 \u2018magic\u2018 and \u2018science\u2018. The term \u2018magic\u2019 would have a very high weight in the topic \u2018magic\u2019 and a very low weight in the topic \u2018science\u2019. That is, a word can now have different weights in different topics. You can also represent more complex topics which are hard to define via a single term.\n\n**There are multiple models through which you can model the topics in this manner. You will study two techniques in the following lectures \u2013 Probabilistic Latent Semantic Analysis (PLSA) and Latent Dirichlet Allocation (LDA).**","52234a15":"<hr> \n\n**Topic Modelling**\n\n1. Topic Modelling is an unsupervised Machine Learning Technique used for identifying multiple topics in text or you can say identifying abstract or \u201ctopics\u201d that are talked in multiple documents. \n2. Let us say I Phone or Galaxy Note series have launched newer version of Phone and they want to understand about the the features which customers are talking in n number of reviews( each review will be considered as one document). Let us say 50 % of customers are talking about hardware, 20 % are talking about camera quality and features , 10 % are talking about music quality and 20 % are talking about packaging of product.\n3. Similarly you can say you have large corpus of scientific documents and you want to build a search engine for this corpus. Imagine you have n number of documents which talk about diseases such as heart, lungs, diabetes, so applying topic modelling on top of this document will lead to identify important analysis as per document and Key terms which are most talked about or are responsible for this diseases to occur.\n4. We will covering the understanding of Topic Modelling with practical demonstration with LDA(Latent Dirichlet Allocation)\n5. Defining a Topic: As with other semantic analytics technique we are aware that topic is distribution over terms, i.e. each term has a certain \u201cweights\u201d in each topic, term here can be referred as k number of words in n number of documents. But is this the only way to define topic ? What are the other way in which topics can be defined ?\n\n**There are two major task in Topic Modelling :**\n1. Estimating Topic Term Distribution : In this case we define each topic as single term ( which will be changes as per LDA further)\n2. Estimating the coverage of Topics in Document, i.e. the document \u2013 topic distribution : Coverage= the frequency of topic j in document i \/ \u03a3j( the frequency of topic j in document i)","5266ad55":"**The code below creates a list of stop words. The 'string' module in python comes with a list of punctuation character, which we'll append to the builtin stopwords of NLTK.**","74a2fc3b":"**So as u can see, we get different cluters as per differet words in particular topic.**","55a169c1":"**Important Note:** Models are not sutomatically downloaded with Spacy, so you may need to do a ```python -m spacy download en``` to use its preprocessing methods."}}