{"cell_type":{"6df6047c":"code","0d906c6e":"code","4fb2f0f9":"code","8099d7d5":"code","d2cc93ff":"code","71d3c8d8":"code","e02b07a8":"code","05568555":"code","d663d62b":"markdown","a293607d":"markdown","92b02a74":"markdown","a24eec90":"markdown","172c820e":"markdown","e2a7b384":"markdown"},"source":{"6df6047c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0d906c6e":"df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv', index_col='id')\ndf_test = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv', index_col='id')","4fb2f0f9":"y = df.target\nX = df.drop(columns=['target'])\nX_test = df_test.copy()","8099d7d5":"from sklearn.preprocessing import OrdinalEncoder\n\nobj_cols=[name for name in X.columns if X[name].dtypes == 'object']\n\nencoder = OrdinalEncoder()\nX[obj_cols] = encoder.fit_transform(X[obj_cols])\nX_test[obj_cols] = encoder.transform(X_test[obj_cols])\n\nX.head()\nX_test.head()","d2cc93ff":"from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import RandomizedSearchCV\n\ngbm_param_grid = {\n    'colsample_bytree': np.arange(0, 1, 0.1),\n    'n_estimators': np.arange(1000, 10000, 1000),\n    'max_depth': [2, 3, 4, 5, 6],\n    'learning_rate': [0.05, 0.1, 0.2, 0.3],\n    'gamma': np.arange(0.5, 0.9, 0.1),\n    'reg_alpha': np.arange(10, 80, 10),\n    'reg_lambda': np.arange(0, 10, 1),\n    'subsample': np.arange(0, 1, 0.1),\n    'colsample_bytree': np.arange(0, 1, 0.1)\n}\n\ngbm = XGBRegressor(tree_method='gpu_hist', n_jobs=-1)\n\nrandomized_mse = RandomizedSearchCV(param_distributions=gbm_param_grid, \n                                    estimator=gbm, \n                                    scoring='neg_mean_squared_error', \n                                    n_iter=150, \n                                    cv=4, \n                                    verbose=1,\n                                    random_state=0)\n\nrandomized_mse.fit(X,y)","71d3c8d8":"print(randomized_mse.best_params_, np.sqrt(np.abs(randomized_mse.best_score_)))\nprint(randomized_mse.best_estimator_)","e02b07a8":"pred_test = randomized_mse.predict(X_test)","05568555":"output = pd.DataFrame({'id':X_test.index, 'target':pred_test})\noutput.to_csv('submission.csv', index=False)","d663d62b":"Use ordinal encoder to have a quick solution for the categorial columns","a293607d":"Import XGBRegressor, RandomizedSearchCV and parameter space for testing","92b02a74":"This notebook shows hyperparameter tuning for XGBoost with RandomizedSearchCV. Please note the script runs approx. two hours on GPU and scores 0.71944.","a24eec90":"# First Test of Hyperparameter Tuning with RandomizedSearchCV\n","172c820e":"Load the data","e2a7b384":"Check the best parameters found"}}