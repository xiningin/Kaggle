{"cell_type":{"32a1835f":"code","ce6c185c":"code","f75f4894":"code","4094adaf":"code","4851520a":"code","379d5c86":"code","13db50a8":"code","c6eecbb1":"code","311fcfd2":"code","a4146785":"code","229bc1b9":"code","96ba3a09":"code","fdc8d8c9":"markdown","d4732c41":"markdown","b8c37a3f":"markdown","bc94e7ee":"markdown","02ad9678":"markdown","8517d486":"markdown","8cb127d8":"markdown","44f208c2":"markdown","79aa61bf":"markdown","64e197fe":"markdown"},"source":{"32a1835f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ce6c185c":"import seaborn as sns \nimport matplotlib.pyplot as plt \nplt.style.use('ggplot')\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (12, 8)","f75f4894":"data = pd.read_csv('\/kaggle\/input\/salary-data-simple-linear-regression\/Salary_Data.csv')\ndata.head()\n\n","4094adaf":"data.info()","4851520a":"ax = sns.scatterplot(x='YearsExperience', y='Salary', data=data)\nax.set_title(\"Salary according to YearsExperience\");","379d5c86":"def cost_function(x, y, theta ) : \n    m = len(y)\n    y_pred = x.dot(theta)\n    \n    fn =  ( y_pred - y )**2\n    \n    cost = 1 \/ ( (2*m)* np.sum(fn))\n    \n    return cost\n\nm = data.YearsExperience.values.size \nx = np.append(np.ones((m,1)), data.YearsExperience.values.reshape(m, 1), axis=1) \ny = data.Salary.values.reshape(m,1)\ntheta = np.zeros((2,1))\n\ncost_function(x,y,theta)","13db50a8":"def gradient_descent(x, y, theta, alpha, iters) :\n    m = len(y)\n    # tracking the history of all Costs in costs list\n    costs = []\n    for i in range(iters) :\n        y_pred = x.dot(theta)\n        fn = np.dot(x.T, (y_pred - y))\n        theta -= alpha*1\/m*np.sum(fn)\n        costs.append(cost_function(x,y,theta))\n    return theta,costs\n\ntheta, costs = gradient_descent(x, y, theta, alpha=0.01, iters=1000)\n\nprint(\"h(x) = {} + {}x1\".format(str(round(theta[0, 0], 2)),\n                                str(round(theta[1, 0], 2))))\n","c6eecbb1":"from mpl_toolkits.mplot3d import Axes3D\n\ntheta_0 = np.linspace(-20,20,1000)\ntheta_1 = np.linspace(-1,4,1000)\n\ncost_values = np.zeros((len(theta_0), len(theta_1)))\n\nfor i in range(len(theta_0)):\n    for j in range(len(theta_1)):\n        t = np.array([theta_0[i], theta_1[j]])\n        cost_values[i, j] = cost_function(x, y, t)\n        \nfig = plt.figure(figsize = (12, 8))\nax = fig.gca(projection = '3d')\n\nsurf = ax.plot_surface(theta_0, theta_1, cost_values, cmap = \"viridis\", linewidth = 0.2)\nfig.colorbar(surf, shrink=0.5, aspect=5)\n\nplt.xlabel(\"$\\Theta_0$\")\nplt.ylabel(\"$\\Theta_1$\")\nax.set_zlabel(\"$J(\\Theta)$\")\nax.set_title(\"Cost Surface\")\nax.view_init(30,330)\n\nplt.show()","311fcfd2":"plt.plot(costs)\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"$J(\\Theta)$\")\nplt.title(\"Values of Cost Function over iterations of Gradient Descent\");","a4146785":"theta = np.squeeze(theta)\nsns.scatterplot(x = \"YearsExperience\", y= \"Salary\", data = data)\n\nx_value=[x for x in range(1, 12)]\ny_value=[(x * theta[1] + theta[0]) for x in x_value]\nsns.lineplot(x_value,y_value)\n\nplt.xlabel(\"Years Experience\")\nplt.ylabel(\"Salary\")\nplt.title(\"Linear Regression Fit\");","229bc1b9":"def predict(x, theta):\n    y_pred = np.dot(theta.transpose(), x)\n    return y_pred","96ba3a09":"x1 = 8.7\ny_pred_1 = predict(np.array([1, x1]),theta) \n\nprint(\"Salary [ for experience {x1} ] = \".format(x1=x1) + str(round(y_pred_1, 5)))","fdc8d8c9":"# Task 5 : Plotting Convergence","d4732c41":"# Task 3 : Computing The Batch Gradient Descent :\n\nMinimize the cost function $J(\\theta)$ by updating the below equation and repeat unitil convergence\n        \n$\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}$ (simultaneously update $\\theta_j$ for all $j$).\n\n### Defining the gradient descent function ","b8c37a3f":"# Task 1 : Load Data\n","bc94e7ee":"The objective of linear regression is to minimize the cost function\n\n$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)} )^2$$\n\nwhere $h_{\\theta}(x)$ is the hypothesis and given by the linear model\n\n$$h_{\\theta}(x) = \\theta^Tx = \\theta_0 + \\theta_1x_1$$\n\n## Computing : ","02ad9678":"# Task 0 : Load libraries","8517d486":"# Task 3 : Compute Cost function $J(\\theta)$","8cb127d8":"# Task 4 : Visualizing cost function","44f208c2":"# Task 7 : Final Prediction\n\n$h_\\theta(x) = \\theta^Tx$","79aa61bf":"# Task 2 : Visualize Salary Data ","64e197fe":"# Task 6: Training Data with Linear Regression Fit"}}