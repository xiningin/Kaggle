{"cell_type":{"edc185ee":"code","dee53b39":"code","67995c0d":"code","660f7ca1":"code","fe3117b5":"code","0239382c":"code","698b5e95":"code","02b717ea":"code","ca39e55a":"code","e4756b9e":"code","dac0ebce":"code","125d4608":"code","8aa4a534":"code","1c26a3fa":"code","31125358":"code","6b00f4ed":"markdown"},"source":{"edc185ee":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\nimport os","dee53b39":"MJTCP = 32292 #Michael Jordan total career points\ndef assign_rnd_integer(dataset, number_of_times = 5, seed = MJTCP):\n    new_dataset = pd.DataFrame()\n    np.random.seed(seed)\n    for c in dataset.columns:\n        for i in range(number_of_times):\n            col_name = c+\"_\"+str(i)\n            unique_vals = dataset[c].unique()\n            labels = np.array(list(range(len(unique_vals))))\n            np.random.shuffle(labels)\n            mapping = pd.DataFrame({c: unique_vals, col_name: labels})\n            new_dataset[col_name] = (dataset[[c]]\n                                     .merge(mapping, on = c, how = 'left')[col_name]\n                                    ).values\n    return new_dataset\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\ndef extract_col_interaction(dataset, col1, col2, tfidf = True):\n    data = dataset.groupby([col1])[col2].agg(lambda x: \" \".join(list([str(y) for y in x])))\n    if tfidf:\n        vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split(\" \"))\n    else:\n        vectorizer = CountVectorizer(tokenizer=lambda x: x.split(\" \"))\n    \n    data_X = vectorizer.fit_transform(data)\n    dim_red = TruncatedSVD(n_components=1, random_state = 5511)\n    data_X = dim_red.fit_transform(data_X)\n    \n    result = pd.DataFrame()\n    result[col1] = data.index.values\n    result[col1+\"_{}_svd\".format(col2)] = data_X.ravel()\n    return result\n\nimport itertools\ndef get_col_interactions_svd(dataset, tfidf = True):\n    new_dataset = pd.DataFrame()\n    for col1,col2 in itertools.permutations(dataset.columns, 2):\n        data = extract_col_interaction(dataset, col1,col2, tfidf)\n        col_name = [x for x in data.columns if \"svd\" in x][0]\n        new_dataset[col_name] = (dataset[[col1]]\n                                 .merge(data, on = col1, how = 'left')\n                                )[col_name]\n    return new_dataset\n\ndef get_freq_encoding(dataset):\n    new_dataset = pd.DataFrame()\n    for c in dataset.columns:\n        data = dataset.groupby([c]).size().reset_index()\n        new_dataset[c+\"_freq\"] = dataset[[c]].merge(data, on = c, how = \"left\")[0]\n    return new_dataset\n\ndef transform_dataset(train, test, func, func_params = {}):\n    dataset = pd.concat([train, test], ignore_index = True)\n    dataset = func(dataset, **func_params)\n    if isinstance(dataset, pd.DataFrame):\n        new_train = dataset.iloc[:train.shape[0],:].reset_index(drop = True)\n        new_test =  dataset.iloc[train.shape[0]:,:].reset_index(drop = True)\n    else:\n        new_train = dataset[:train.shape[0]]\n        new_test =  dataset[train.shape[0]:]\n    return new_train, new_test\n\nimport lightgbm as lgbm\n\ndef get_model(params):\n    return lgbm.LGBMClassifier(\n        n_estimators=250,\n        metric='auc',\n        objective='binary', \n        n_jobs=3,\n        random_state = 42,\n        **params\n    )\n\nclass TargetEncoding():\n    def __init__(self, columns_names ):\n        self.columns_names = columns_names\n        self.learned_values = {}\n        self.dataset_mean = np.nan\n    def fit(self, X, y, **fit_params):\n        X_ = X.copy()\n        self.learned_values = {}\n        X_[\"__target__\"] = y\n        for c in [x for x in X_.columns if x in self.columns_names]:\n            self.learned_values[c] = (X_[[c,\"__target__\"]]\n                                      .groupby(c)[\"__target__\"].mean()\n                                      .reset_index())\n\n        self.dataset_mean = np.mean(y)\n        return self\n    def transform(self, X, **fit_params):\n        transformed_X = X[self.columns_names].copy()\n        for c in transformed_X.columns:\n            transformed_X[c] = (transformed_X[[c]]\n                                .merge(self.learned_values[c], on = c, how = 'left')\n                               )[\"__target__\"]\n        transformed_X = transformed_X.fillna(self.dataset_mean)\n        return transformed_X\n    def fit_transform(self, X, y, **fit_params):\n        self.fit(X,y)\n        return self.transform(X)\nfrom sklearn.model_selection import StratifiedKFold\ndef get_CV_target_encoding(data, y, encoder, cv = 5):\n    skfTE = StratifiedKFold(n_splits=cv, random_state = 545167, shuffle = True)\n    result = []\n    for train_indexTE, test_indexTE in skfTE.split(data, y):\n        encoder.fit(data.iloc[train_indexTE,:].reset_index(drop = True), y[train_indexTE])\n        tmp =  encoder.transform(data.iloc[test_indexTE,:].reset_index(drop = True))\n        tmp[\"index\"] = test_indexTE\n        result.append(tmp)\n    result = pd.concat(result, ignore_index = True)\n    result = result.sort_values('index').reset_index(drop = True).drop('index', axis = 1)\n    return result","67995c0d":"# Loading data directly from CatBoost\nfrom catboost.datasets import amazon\ntrain, test = amazon()\n\ntarget = \"ACTION\"\ncol4train = [x for x in train.columns if x not in [target, \"ROLE_TITLE\"]]\ny = train[target].values\n\ntrain[col4train] = train[col4train].values.astype(str)\ntest[col4train] = test[col4train].values.astype(str)\n\nfrom itertools import combinations\nnew_col4train = col4train\nfor c1,c2 in combinations(col4train, 2):\n    name = \"{}_{}\".format(c1,c2)\n    new_col4train.append(name)\n    train[name] = train[c1] + \"_\" + train[c2]\n    test[name] = test[c1] + \"_\" + test[c2]","660f7ca1":"#dataset #1\ncols_svd = ['MGR_ID_ROLE_CODE','MGR_ID_ROLE_DEPTNAME','MGR_ID_ROLE_FAMILY', \n            'RESOURCE_MGR_ID','RESOURCE_ROLE_CODE', 'RESOURCE_ROLE_FAMILY',\n            'RESOURCE_ROLE_ROLLUP_1','RESOURCE_ROLE_ROLLUP_2','RESOURCE',\n            'ROLE_DEPTNAME_ROLE_CODE','ROLE_DEPTNAME_ROLE_FAMILY',\n            'ROLE_FAMILY_DESC_ROLE_FAMILY','ROLE_FAMILY_ROLE_CODE',\n            'ROLE_FAMILY','ROLE_ROLLUP_1_ROLE_DEPTNAME',\n            'ROLE_ROLLUP_1_ROLE_FAMILY_DESC', 'ROLE_ROLLUP_1_ROLE_FAMILY',\n            'ROLE_ROLLUP_1','ROLE_ROLLUP_2']\n\ncols_rnd = ['MGR_ID_ROLE_DEPTNAME','MGR_ID_ROLE_FAMILY','MGR_ID_ROLE_ROLLUP_1',\n 'MGR_ID_ROLE_ROLLUP_2','MGR_ID','RESOURCE_MGR_ID','RESOURCE_ROLE_CODE',\n 'RESOURCE_ROLE_FAMILY_DESC','RESOURCE_ROLE_FAMILY','RESOURCE_ROLE_ROLLUP_1',\n 'RESOURCE_ROLE_ROLLUP_2','ROLE_DEPTNAME_ROLE_FAMILY_DESC','ROLE_FAMILY_DESC_ROLE_CODE',\n 'ROLE_FAMILY_DESC_ROLE_FAMILY','ROLE_FAMILY','ROLE_ROLLUP_1_ROLE_CODE',\n 'ROLE_ROLLUP_1_ROLE_DEPTNAME','ROLE_ROLLUP_1_ROLE_FAMILY_DESC','ROLE_ROLLUP_2_ROLE_FAMILY']\n\ncols_freq = ['MGR_ID_ROLE_DEPTNAME','RESOURCE_MGR_ID','RESOURCE_ROLE_CODE',\n 'RESOURCE_ROLE_DEPTNAME','RESOURCE_ROLE_FAMILY_DESC','RESOURCE_ROLE_FAMILY',\n 'RESOURCE_ROLE_ROLLUP_1','ROLE_DEPTNAME_ROLE_FAMILY_DESC','ROLE_DEPTNAME_ROLE_FAMILY',\n 'ROLE_DEPTNAME','ROLE_FAMILY_DESC_ROLE_CODE','ROLE_FAMILY_DESC_ROLE_FAMILY',\n 'ROLE_ROLLUP_1_ROLE_CODE','ROLE_ROLLUP_2_ROLE_DEPTNAME']\n\ndata_svd = transform_dataset(train[cols_svd], test[cols_svd], get_col_interactions_svd)\ndata_rnd = transform_dataset(train[cols_rnd], test[cols_rnd], \n                             assign_rnd_integer, {\"number_of_times\":5})\ndata_freq = transform_dataset(train[cols_freq], test[cols_freq], get_freq_encoding)","fe3117b5":"data_train = pd.concat([x[0] for x in [data_svd, data_rnd, data_freq]], axis = 1)\ndata_test = pd.concat([x[1] for x in [data_svd, data_rnd, data_freq]], axis = 1)","0239382c":"print(\"Dataset shape, Train: {}, Test: {}\".format(data_train.shape, data_test.shape))","698b5e95":"del([data_svd, data_rnd, data_freq])\ngc.collect()\ndata_train = data_train.values\ndata_test = data_test.values\ngc.collect()","02b717ea":"params = {'colsample_bytree': 0.312002398119274,\n 'lambda_l1': 1.919962415701389,\n 'learning_rate': 0.03363113877976891,\n 'max_bin': 484,\n 'max_depth': 10,\n 'min_child_weight': 0.035307873174480586,\n 'num_leaves': 220}\n\nmodel = get_model(params)\nmodel.fit(data_train, y)\npredictions_1 = model.predict_proba(data_test)[:,1]","ca39e55a":"del([data_train, data_test, model])\ngc.collect()","e4756b9e":"#dataset #2\ncols_svd = ['MGR_ID','RESOURCE_MGR_ID','RESOURCE_ROLE_CODE',\n 'RESOURCE_ROLE_DEPTNAME','RESOURCE_ROLE_FAMILY_DESC','RESOURCE_ROLE_FAMILY',\n 'RESOURCE_ROLE_ROLLUP_1','RESOURCE','ROLE_CODE',\n 'ROLE_DEPTNAME_ROLE_CODE','ROLE_DEPTNAME_ROLE_FAMILY','ROLE_FAMILY_DESC_ROLE_CODE',\n 'ROLE_FAMILY_DESC_ROLE_FAMILY','ROLE_FAMILY_DESC','ROLE_ROLLUP_1_ROLE_DEPTNAME',\n 'ROLE_ROLLUP_1_ROLE_FAMILY','ROLE_ROLLUP_1_ROLE_ROLLUP_2','ROLE_ROLLUP_2_ROLE_FAMILY_DESC',\n 'ROLE_ROLLUP_2_ROLE_FAMILY','ROLE_ROLLUP_2']\n\ncols_rnd = ['MGR_ID_ROLE_CODE','MGR_ID_ROLE_DEPTNAME','MGR_ID_ROLE_FAMILY_DESC',\n 'MGR_ID_ROLE_ROLLUP_1','MGR_ID','RESOURCE_ROLE_DEPTNAME',\n 'RESOURCE_ROLE_FAMILY','RESOURCE_ROLE_ROLLUP_1','ROLE_CODE',\n 'ROLE_DEPTNAME_ROLE_FAMILY_DESC','ROLE_FAMILY_DESC_ROLE_CODE',\n 'ROLE_FAMILY_DESC_ROLE_FAMILY','ROLE_FAMILY_ROLE_CODE',\n 'ROLE_ROLLUP_1_ROLE_CODE','ROLE_ROLLUP_1_ROLE_FAMILY_DESC',\n 'ROLE_ROLLUP_1_ROLE_ROLLUP_2']\n\ncols_freq = ['MGR_ID_ROLE_CODE','MGR_ID_ROLE_DEPTNAME','MGR_ID_ROLE_ROLLUP_1',\n 'MGR_ID_ROLE_ROLLUP_2','MGR_ID','RESOURCE_MGR_ID',\n 'RESOURCE_ROLE_DEPTNAME','RESOURCE_ROLE_FAMILY_DESC','RESOURCE_ROLE_ROLLUP_2',\n 'RESOURCE','ROLE_DEPTNAME_ROLE_FAMILY_DESC','ROLE_DEPTNAME',\n 'ROLE_FAMILY_DESC','ROLE_FAMILY','ROLE_ROLLUP_1_ROLE_FAMILY_DESC',\n 'ROLE_ROLLUP_1_ROLE_FAMILY','ROLE_ROLLUP_1_ROLE_ROLLUP_2',\n 'ROLE_ROLLUP_1','ROLE_ROLLUP_2_ROLE_CODE','ROLE_ROLLUP_2']\n\ncols_te = ['MGR_ID','RESOURCE_MGR_ID','RESOURCE_ROLE_CODE',\n 'RESOURCE_ROLE_DEPTNAME','RESOURCE_ROLE_ROLLUP_2','RESOURCE',\n 'ROLE_CODE','ROLE_DEPTNAME_ROLE_FAMILY_DESC','ROLE_DEPTNAME_ROLE_FAMILY',\n 'ROLE_FAMILY_DESC_ROLE_CODE','ROLE_FAMILY_DESC','ROLE_FAMILY_ROLE_CODE',\n 'ROLE_ROLLUP_1_ROLE_FAMILY','ROLE_ROLLUP_2_ROLE_FAMILY','ROLE_ROLLUP_2']\n\ndata_svd = transform_dataset(train[cols_svd], test[cols_svd], get_col_interactions_svd)\ndata_rnd = transform_dataset(train[cols_rnd], test[cols_rnd], \n                             assign_rnd_integer, {\"number_of_times\":5})\ndata_freq = transform_dataset(train[cols_freq], test[cols_freq], get_freq_encoding)","dac0ebce":"te = TargetEncoding(columns_names = cols_te)\ndata_te_tr = get_CV_target_encoding(train[cols_te], y, te, 5)\nte.fit(train[cols_te], y)\ndata_te_te = te.transform(test[cols_te])","125d4608":"data_train = pd.concat([x[0] for x in [data_svd, data_rnd, data_freq]], axis = 1)\ndata_test = pd.concat([x[1] for x in [data_svd, data_rnd, data_freq]], axis = 1)\ndata_train = pd.concat([data_train, data_te_tr], axis = 1)\ndata_test = pd.concat([data_test, data_te_te], axis = 1)\nprint(\"Dataset shape, Train: {}, Test: {}\".format(data_train.shape, data_test.shape))\ndel([data_svd, data_rnd, data_freq, data_te_tr, data_te_te])\ngc.collect()\ndata_train = data_train.values\ndata_test = data_test.values\ngc.collect()","8aa4a534":"params = {'colsample_bytree': 0.5280533549534434,\n 'lambda_l1': 0.1267270702844549,\n 'learning_rate': 0.012220447574715732,\n 'max_bin': 131,\n 'max_depth': 18,\n 'min_child_weight': 1.1518716916679328,\n 'num_leaves': 184}\n\nmodel = get_model(params)\nmodel.fit(data_train, y)\npredictions_2 = model.predict_proba(data_test)[:,1]\n\ndel([data_train, data_test, model])\ngc.collect()","1c26a3fa":"submission = pd.DataFrame()\nsubmission[\"Id\"] = test[\"id\"]\nsubmission[\"ACTION\"] = (predictions_1 + predictions_2) \/ 2","31125358":"submission.to_csv(\"submission.csv\", index = False)","6b00f4ed":"Let's define all things we need. And we will need: Frequency Encoding, Label Encoding, SVD encoding and target encoding."}}