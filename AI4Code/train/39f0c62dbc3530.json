{"cell_type":{"10021cf8":"code","cd00ff14":"code","ab824b27":"code","48bd3b80":"code","f9c818d5":"code","0cb2820f":"code","1051dca2":"code","6a7ab238":"code","b39413fa":"code","df7cbb4f":"code","0077a68a":"code","6b9fee4a":"code","0990f365":"code","356de114":"code","270ab3bb":"code","0ea4216f":"code","2297a08a":"code","19a7c6af":"code","49ee81bb":"code","f8ee06e2":"code","93f23b60":"code","9767495a":"code","2796bd51":"code","78db9127":"code","ed68e947":"code","52cf04b5":"code","1f17b169":"code","d82dab7f":"code","2333dd70":"code","1f8c7140":"code","5fba5c2a":"code","24a8e1f2":"code","41cbb0cb":"code","83f6c7a7":"code","1394bc6e":"code","55fad1f3":"code","1297afcc":"code","1f2a1d71":"code","e9889cc7":"code","acd2c127":"code","91ea6475":"code","3287761d":"code","f30258fa":"code","eedc43e7":"code","7d9179cb":"code","c95aa184":"code","5e0a35ec":"code","bf24a5aa":"code","fecd0b90":"code","bece2a7e":"code","1b8f18b6":"code","509bbad2":"code","d7995762":"code","bebe2c5c":"code","e29c742f":"code","01d4bddc":"code","f908a7a8":"code","abe961f1":"code","41ab519e":"code","58ec895b":"code","e3eab381":"code","3e700577":"code","657084f9":"code","af9f1db0":"code","e55a554a":"code","876ed948":"code","b7aa5dac":"code","7b3672cf":"code","97471b4a":"code","c0c05ceb":"code","08066429":"code","5561eb9b":"code","d7bb460d":"markdown","a3f62ef8":"markdown","e96605df":"markdown","c1fc8d2a":"markdown","fa48105d":"markdown","ce3c25f7":"markdown","2c8875e1":"markdown","0c9598fe":"markdown","9d744fbd":"markdown","fec53672":"markdown","ac8b04fe":"markdown","2757d9b5":"markdown","38a9d029":"markdown","177de0c2":"markdown","489e4cbc":"markdown","ad776e5f":"markdown","542799d3":"markdown","07020d3d":"markdown"},"source":{"10021cf8":"# import library\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport math\n\n# import data\ndata = pd.read_csv(\"..\/input\/linearregressiondataset3\/linear-regression-dataset.csv\")\nprint(data.info())\nprint(data.head())\n#print(data.describe())","cd00ff14":"# plot data\nplt.scatter(data.deneyim,data.maas)\nplt.xlabel(\"deneyim\")\nplt.ylabel(\"maas\")\nplt.show()","ab824b27":"#%% linear regression\n\n# sklearn library\nfrom sklearn.linear_model import LinearRegression\n# linear regression model\nlinear_reg = LinearRegression()\n\nx = data.deneyim.values.reshape(-1,1)\ny = data.maas.values.reshape(-1,1)\n\nlinear_reg.fit(x,y)\n\nprint('R sq: ', linear_reg.score(x, y))\nprint('Correlation: ', math.sqrt(linear_reg.score(x, y)))","48bd3b80":"#%% prediction\nimport numpy as np\n\nprint(\"Coefficient for X: \", linear_reg.coef_)\nprint(\"Intercept for X: \", linear_reg.intercept_)\nprint(\"Regression line is: y = \" + str(linear_reg.intercept_[0]) + \" + (x * \" + str(linear_reg.coef_[0][0]) + \")\")\n\n# maas = 1663 + 1138*deneyim \nmaas_yeni = 1663 + 1138*11\nprint(maas_yeni)\n\narray = np.array([11]).reshape(-1,1)\nprint(linear_reg.predict(array))","f9c818d5":"# visualize line\narray = np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]).reshape(-1,1)  # deneyim\n\nplt.scatter(x,y)\n#plt.show()\ny_head = linear_reg.predict(array)  # maas\nplt.plot(array, y_head,color = \"red\")\narray = np.array([100]).reshape(-1,1)\nlinear_reg.predict(array)","0cb2820f":"y_head = linear_reg.predict(x)  # maas\nfrom sklearn.metrics import r2_score\nprint(\"r_square score: \", r2_score(y,y_head))","1051dca2":"import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndata = pd.read_csv(\"..\/input\/multiplelinearregressiondataset\/multiple-linear-regression-dataset.csv\")\nprint(data.info())\nprint(data.head())\n#print(data.describe())","6a7ab238":"x = data.iloc[:,[0,2]].values\ny = data.maas.values.reshape(-1,1)\n\nmultiple_linear_regression = LinearRegression()\nmultiple_linear_regression.fit(x,y)\n\nprint(\"b0: \",multiple_linear_regression.intercept_)\nprint(\"b1: \", multiple_linear_regression.coef_)\n\n#predict\nx_ = np.array([[10,35],[5,35]])\nmultiple_linear_regression.predict(x_)\n\ny_head = multiple_linear_regression.predict(x) \nfrom sklearn.metrics import r2_score\nprint(\"r_square score: \", r2_score(y,y_head))","b39413fa":"import pandas as pd\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\"..\/input\/polynomialregressioncsv\/polynomial-regression.csv\")\nprint(data.info())\nprint(data.head())\n#print(data.describe())","df7cbb4f":"x = data.araba_fiyat.values.reshape(-1,1)\ny = data.araba_max_hiz.values.reshape(-1,1)\n\nplt.scatter(x,y)\nplt.xlabel(\"araba_max_hiz\")\nplt.ylabel(\"araba_fiyat\")\nplt.show()","0077a68a":"# polynomial regression =  y = b0 + b1*x +b2*x^2 + b3*x^3 + ... + bn*x^n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\npolynominal_regression = PolynomialFeatures(degree=4)\nx_polynomial = polynominal_regression.fit_transform(x,y)\n\n# %% fit\nlinear_regression = LinearRegression()\nlinear_regression.fit(x_polynomial,y)\n# %%\ny_head2 = linear_regression.predict(x_polynomial)\n\nplt.plot(x,y_head2,color= \"green\",label = \"poly\")\nplt.legend()\nplt.scatter(x,y)\nplt.xlabel(\"araba_max_hiz\")\nplt.ylabel(\"araba_fiyat\")\nplt.show()\n\nfrom sklearn.metrics import r2_score\nprint(\"r_square score: \", r2_score(y,y_head2))\n\n","6b9fee4a":"import pandas as pd\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\"..\/input\/support-vector-regression\/maaslar.csv\")\nprint(data.info())\nprint(data.head())\n#print(data.describe())","0990f365":"x = data.iloc[:,1:2].values\ny = data.iloc[:,2:].values\n\nplt.scatter(x,y)\nplt.xlabel(\"araba_max_hiz\")\nplt.ylabel(\"araba_fiyat\")\nplt.show()","356de114":"#verilerin olceklenmesi\nfrom sklearn.preprocessing import StandardScaler\nsc1 = StandardScaler()\nx_olcekli = sc1.fit_transform(x)\nsc2 = StandardScaler()\ny_olcekli = sc2.fit_transform(y)\n\n#%% SVR\nfrom sklearn.svm import SVR\n\nsvr_reg = SVR(kernel = 'rbf')\nsvr_reg.fit(x_olcekli,y_olcekli)\n\ny_head = svr_reg.predict(x_olcekli)\n\n# visualize line\nplt.plot(x_olcekli,y_head,color= \"green\",label = \"SVR\")\nplt.legend()\nplt.scatter(x_olcekli,y_olcekli,color='red')\nplt.show()\n\nprint('R sq: ', svr_reg.score(x_olcekli, y_olcekli))","270ab3bb":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = pd.read_csv(\"..\/input\/decisiontreeregressiondataset\/decision-tree-regression-dataset.csv\", header=None)\nprint(data.info())\nprint(data.head())\n#print(data.describe())\n","0ea4216f":"x = data.iloc[:,[0]].values.reshape(-1,1)\ny = data.iloc[:,[1]].values.reshape(-1,1)","2297a08a":"#%%  decision tree regression\nfrom sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(x,y)\n\nprint(tree_reg.predict(np.array([5.5]).reshape(-1,1)))","19a7c6af":"x_ = np.arange(min(x),max(x),0.01).reshape(-1,1)\n#print(x)\ny_head = tree_reg.predict(x_)\n#print(y_head)\n\n# %% visualize\nplt.scatter(x,y,color=\"red\")\nplt.plot(x_,y_head,color = \"green\")\nplt.xlabel(\"tribun level\")\nplt.ylabel(\"ucret\")\nplt.show()\n\ny_head = tree_reg.predict(x)\n#from sklearn.metrics import r2_score\nprint(\"r_square score: \", r2_score(y,y_head))","49ee81bb":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = pd.read_csv(\"..\/input\/randomforestregressiondataset\/random-forest-regression-dataset.csv\", header=None)\nprint(data.info())\nprint(data.head())\n#print(data.describe())","f8ee06e2":"x = data.iloc[:,0].values.reshape(-1,1)\ny = data.iloc[:,1].values.reshape(-1,1)","93f23b60":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators = 100, random_state= 42) \nrf.fit(x,y)\n\nprint(\"7.8 seviyesinde fiyat\u0131n ne kadar oldu\u011fu: \",rf.predict(np.array([7.8]).reshape(-1,1)))\n\nx_ = np.arange(min(x),max(x),0.01).reshape(-1,1)\ny_head = rf.predict(x_)","9767495a":"# visualize\nplt.scatter(x,y,color=\"red\")\nplt.plot(x_,y_head,color=\"green\")\nplt.xlabel(\"tribun level\")\nplt.ylabel(\"ucret\")\nplt.show()","2796bd51":"y_head = rf.predict(x)\nfrom sklearn.metrics import r2_score\nprint(\"r_score: \", r2_score(y,y_head))","78db9127":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = pd.read_csv(\"..\/input\/classification\/data.csv\")\n#print(data.info())\n#print(data.head())\n#print(data.describe())\n# %%\ndata.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)\ndata.tail()\n# malignant = M  kotu huylu tumor\n# benign = B     iyi huylu tumor","ed68e947":"# %%\nM = data[data.diagnosis == \"M\"]\nB = data[data.diagnosis == \"B\"]\n# scatter plot\nplt.scatter(M.radius_mean,M.texture_mean,color=\"red\",label=\"kotu\",alpha= 0.3)\nplt.scatter(B.radius_mean,B.texture_mean,color=\"green\",label=\"iyi\",alpha= 0.3)\nplt.xlabel(\"radius_mean\")\nplt.ylabel(\"texture_mean\")\nplt.legend()\nplt.show()","52cf04b5":"# %%\ndata.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\ny = data.diagnosis.values\nx_data = data.drop([\"diagnosis\"],axis=1)\n# %%\n# normalization \nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))","1f17b169":"#%%\n# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state=1)\n\n# %%\n# knn model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3) # n_neighbors = k\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\nprint(\" {} nn score: {} \".format(3,knn.score(x_test,y_test)))","d82dab7f":"# %%\n# find k value\nscore_list = []\nfor each in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n    \nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","2333dd70":"# %%\n# knn model\nknn = KNeighborsClassifier(n_neighbors = 8) # n_neighbors = k\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\nprint(\" {} nn score: {} \".format(3,knn.score(x_test,y_test)))","1f8c7140":"#%% confusion matrix\ny_pred = knn.predict(x_test)\ny_true = y_test\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\n# %% cm visualization\nimport seaborn as sns\n\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","5fba5c2a":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = pd.read_csv(\"..\/input\/classification\/data.csv\")\n#print(data.info())\n#print(data.head())\n#print(data.describe())\n# %%\ndata.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)\ndata.tail()\n# malignant = M  kotu huylu tumor\n# benign = B     iyi huylu tumor","24a8e1f2":"# %%\nM = data[data.diagnosis == \"M\"]\nB = data[data.diagnosis == \"B\"]\n# scatter plot\nplt.scatter(M.radius_mean,M.texture_mean,color=\"red\",label=\"kotu\",alpha= 0.3)\nplt.scatter(B.radius_mean,B.texture_mean,color=\"green\",label=\"iyi\",alpha= 0.3)\nplt.xlabel(\"radius_mean\")\nplt.ylabel(\"texture_mean\")\nplt.legend()\nplt.show()","41cbb0cb":"# %%\ndata.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\ny = data.diagnosis.values\nx_data = data.drop([\"diagnosis\"],axis=1)\n# %%\n# normalization \nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))","83f6c7a7":"#%%\n# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state=1)\n\n# %% SVM\nfrom sklearn.svm import SVC\n \nsvm = SVC(random_state = 1)\nsvm.fit(x_train,y_train)\n# %% test\nprint(\"print accuracy of svm algo: \",svm.score(x_test,y_test))\n\n","1394bc6e":"#%% confusion matrix\ny_pred = svm.predict(x_test)\ny_true = y_test\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\n# %% cm visualization\nimport seaborn as sns\n\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","55fad1f3":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = pd.read_csv(\"..\/input\/classification\/data.csv\")\n#print(data.info())\n#print(data.head())\n#print(data.describe())\n# %%\ndata.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)\ndata.tail()\n# malignant = M  kotu huylu tumor\n# benign = B     iyi huylu tumor","1297afcc":"# %%\nM = data[data.diagnosis == \"M\"]\nB = data[data.diagnosis == \"B\"]\n# scatter plot\nplt.scatter(M.radius_mean,M.texture_mean,color=\"red\",label=\"kotu\",alpha= 0.3)\nplt.scatter(B.radius_mean,B.texture_mean,color=\"green\",label=\"iyi\",alpha= 0.3)\nplt.xlabel(\"radius_mean\")\nplt.ylabel(\"texture_mean\")\nplt.legend()\nplt.show()","1f2a1d71":"# %%\ndata.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\ny = data.diagnosis.values\nx_data = data.drop([\"diagnosis\"],axis=1)\n\n# %%\n# normalization \nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))","e9889cc7":"#%%\n# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state=1)\n\n# %% Naive bayes \nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n\nnb.score(x_test,y_test)\n # %% test\nprint(\"print accuracy of naive bayes algo: \",nb.score(x_test,y_test))","acd2c127":"#%% confusion matrix\ny_pred = nb.predict(x_test)\ny_true = y_test\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\n# %% cm visualization\nimport seaborn as sns\n\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","91ea6475":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = pd.read_csv(\"..\/input\/classification\/data.csv\")\n#print(data.info())\n#print(data.head())\n#print(data.describe())\n# %%\ndata.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)\ndata.tail()\n# malignant = M  kotu huylu tumor\n# benign = B     iyi huylu tumor","3287761d":"# %%\nM = data[data.diagnosis == \"M\"]\nB = data[data.diagnosis == \"B\"]\n# scatter plot\nplt.scatter(M.radius_mean,M.texture_mean,color=\"red\",label=\"kotu\",alpha= 0.3)\nplt.scatter(B.radius_mean,B.texture_mean,color=\"green\",label=\"iyi\",alpha= 0.3)\nplt.xlabel(\"radius_mean\")\nplt.ylabel(\"texture_mean\")\nplt.legend()\nplt.show()","f30258fa":"# %%\ndata.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\ny = data.diagnosis.values\nx_data = data.drop([\"diagnosis\"],axis=1)\n\n# %%\n# normalization \nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))","eedc43e7":"# %% train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test,y_train, y_test = train_test_split(x,y,test_size = 0.15,random_state = 42)\n\n#%%\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\n\nprint(\"score: \", dt.score(x_test,y_test))","7d9179cb":"#%% confusion matrix\ny_pred = dt.predict(x_test)\ny_true = y_test\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\n# %% cm visualization\nimport seaborn as sns\n\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","c95aa184":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = pd.read_csv(\"..\/input\/classification\/data.csv\")\n#print(data.info())\n#print(data.head())\n#print(data.describe())\n# %%\ndata.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)\ndata.tail()\n# malignant = M  kotu huylu tumor\n# benign = B     iyi huylu tumor","5e0a35ec":"# %%\nM = data[data.diagnosis == \"M\"]\nB = data[data.diagnosis == \"B\"]\n# scatter plot\nplt.scatter(M.radius_mean,M.texture_mean,color=\"red\",label=\"kotu\",alpha= 0.3)\nplt.scatter(B.radius_mean,B.texture_mean,color=\"green\",label=\"iyi\",alpha= 0.3)\nplt.xlabel(\"radius_mean\")\nplt.ylabel(\"texture_mean\")\nplt.legend()\nplt.show()","bf24a5aa":"# %%\ndata.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\ny = data.diagnosis.values\nx_data = data.drop([\"diagnosis\"],axis=1)\n\n# %%\n# normalization \nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))","fecd0b90":"# %% train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test,y_train, y_test = train_test_split(x,y,test_size = 0.15,random_state = 42)\n\n#%%  random forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators = 100,random_state = 1)\nrf.fit(x_train,y_train)\nprint(\"random forest algo result: \",rf.score(x_test,y_test))","bece2a7e":"#%% confusion matrix\ny_pred = rf.predict(x_test)\ny_true = y_test\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\n# %% cm visualization\nimport seaborn as sns\n\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","1b8f18b6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# %% create dataset\n# class1\nx1 = np.random.normal(25,5,1000)\ny1 = np.random.normal(25,5,1000)\n\n# class2\nx2 = np.random.normal(55,5,1000)\ny2 = np.random.normal(60,5,1000)\n\n# class3\nx3 = np.random.normal(55,5,1000)\ny3 = np.random.normal(15,5,1000)\n\nx = np.concatenate((x1,x2,x3),axis = 0)\ny = np.concatenate((y1,y2,y3),axis = 0)\n\ndictionary = {\"x\":x,\"y\":y}\ndata = pd.DataFrame(dictionary)\n\nplt.scatter(x1,y1)\nplt.scatter(x2,y2)\nplt.scatter(x3,y3)\nplt.show()","509bbad2":"# %% KMEANS\n\nfrom sklearn.cluster import KMeans\nwcss = []\n\nfor k in range(1,15):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(data)\n    wcss.append(kmeans.inertia_)\n    \nplt.plot(range(1,15),wcss)\nplt.xlabel(\"number of k (cluster) value\")\nplt.ylabel(\"wcss\")\nplt.show()","d7995762":"#%% k = 3 icin modelim\nkmeans2 = KMeans(n_clusters=3)\nclusters = kmeans2.fit_predict(data)\n\ndata[\"label\"] = clusters\n\nplt.scatter(data.x[data.label == 0 ],data.y[data.label == 0],color = \"red\")\nplt.scatter(data.x[data.label == 1 ],data.y[data.label == 1],color = \"green\")\nplt.scatter(data.x[data.label == 2 ],data.y[data.label == 2],color = \"blue\")\nplt.scatter(kmeans2.cluster_centers_[:,0],kmeans2.cluster_centers_[:,1],color = \"yellow\")\nplt.show()","bebe2c5c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# %% create dataset\n# class1\nx1 = np.random.normal(25,5,100)\ny1 = np.random.normal(25,5,100)\n\n# class2\nx2 = np.random.normal(55,5,100)\ny2 = np.random.normal(60,5,100)\n\n# class3\nx3 = np.random.normal(55,5,100)\ny3 = np.random.normal(15,5,100)\n\nx = np.concatenate((x1,x2,x3),axis = 0)\ny = np.concatenate((y1,y2,y3),axis = 0)\n\ndictionary = {\"x\":x,\"y\":y}\n\ndata = pd.DataFrame(dictionary)\n\nplt.scatter(x1,y1,color=\"black\")\nplt.scatter(x2,y2,color=\"black\")\nplt.scatter(x3,y3,color=\"black\")\nplt.show()","e29c742f":"# %% dendogram\nfrom scipy.cluster.hierarchy import linkage, dendrogram\n\nmerg = linkage(data,method=\"ward\")\ndendrogram(merg,leaf_rotation = 90)\nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidean distance\")\nplt.show()","01d4bddc":"# %% HC\nfrom sklearn.cluster import AgglomerativeClustering\n\nhiyerartical_cluster = AgglomerativeClustering(n_clusters = 3,affinity= \"euclidean\",linkage = \"ward\")\ncluster = hiyerartical_cluster.fit_predict(data)\n\ndata[\"label\"] = cluster\n\nplt.scatter(data.x[data.label == 0 ],data.y[data.label == 0],color = \"red\")\nplt.scatter(data.x[data.label == 1 ],data.y[data.label == 1],color = \"green\")\nplt.scatter(data.x[data.label == 2 ],data.y[data.label == 2],color = \"blue\")\n#plt.scatter(data.x[data.label == 3 ],data.y[data.label == 3],color = \"black\")\nplt.show()","f908a7a8":"import pandas as pd\n# %% import twitter data\ndata = pd.read_csv(\"..\/input\/natural-language-process-nlp\/gender-classifier.csv\",encoding = \"latin1\")\ndata = pd.concat([data.gender,data.description],axis=1)\ndata.dropna(axis = 0,inplace = True)\ndata.gender = [1 if each == \"female\" else 0 for each in data.gender]\nprint(data.info())\nprint(data.head())\n#print(data.describe())","abe961f1":"import nltk # natural language tool kit\n#nltk.download(\"stopwords\")      # corpus diye bir kalsore indiriliyor\nfrom nltk.corpus import stopwords  # sonra ben corpus klasorunden import ediyorum\nimport re\ndescription_list = []\nfor description in data.description:\n    description = re.sub(\"[^a-zA-Z]\",\" \",description) # regular expression RE mesela \"[^a-zA-Z]\"\n    description = description.lower()   # buyuk harftan kucuk harfe cevirme\n    description = nltk.word_tokenize(description)# split kullan\u0131rsak \"shouldn't \" gibi kelimeler \"should\" ve \"not\" diye ikiye ayr\u0131lmaz ama word_tokenize() kullanirsak ayrilir\n    description = [ word for word in description if not word in set(stopwords.words(\"english\"))] # greksiz kelimeleri cikar\n    lemma = nltk.WordNetLemmatizer() # lemmatazation loved => love   gitmeyecegim = > git\n    description = [ lemma.lemmatize(word) for word in description]\n    description = \" \".join(description)\n    description_list.append(description)\n#print(description_list)","41ab519e":"# %% bag of words\nfrom sklearn.feature_extraction.text import CountVectorizer # bag of words yaratmak icin kullandigim metot\nmax_features = 5000\n\ncount_vectorizer = CountVectorizer(max_features=max_features,stop_words = \"english\")\nsparce_matrix = count_vectorizer.fit_transform(description_list).toarray()  # x\n\n#print(\"en sik kullanilan {} kelimeler: {}\".format(max_features,count_vectorizer.get_feature_names()))\n","58ec895b":"# %%\ny = data.iloc[:,0].values   # male or female classes\nx = sparce_matrix\n# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.1, random_state = 42)","e3eab381":"# %% naive bayes\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n#%% prediction\ny_pred = nb.predict(x_test)\nprint(\"accuracy: \",nb.score(y_pred.reshape(-1,1),y_test))","3e700577":"from sklearn.datasets import load_iris\nimport pandas as pd\n# %%\niris = load_iris()\n\nfeature_names = iris.feature_names\ny = iris.target\n\ndata = pd.DataFrame(iris.data,columns = feature_names)\ndata[\"sinif\"] = y\n\nx = iris.data\nprint(data.info())\nprint(data.head())\n#print(data.describe())","657084f9":"#%% PCA\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 2, whiten= True )  # whitten = normalize\npca.fit(x)\n\nx_pca = pca.transform(x)\n\nprint(\"variance ratio: \", pca.explained_variance_ratio_)\nprint(\"sum: \",sum(pca.explained_variance_ratio_))","af9f1db0":"#%% 2D\ndata[\"p1\"] = x_pca[:,0]\ndata[\"p2\"] = x_pca[:,1]\n\ncolor = [\"red\",\"green\",\"blue\"]\n\nimport matplotlib.pyplot as plt\nfor each in range(3):\n    plt.scatter(data.p1[data.sinif == each],data.p2[data.sinif == each],color = color[each],label = iris.target_names[each])\n    \nplt.legend()\nplt.xlabel(\"p1\")\nplt.ylabel(\"p2\")\nplt.show()","e55a554a":"from sklearn.datasets import load_iris\nimport pandas as pd\nimport numpy as np\n#%%\niris = load_iris()\nx = iris.data\ny = iris.target\n\ndata = pd.DataFrame(iris.data,columns = feature_names)\ndata[\"sinif\"] = y\n\nprint(data.info())\nprint(data.head())\n#print(data.describe())\n\n# %% normalization\nx = (x-np.min(x))\/(np.max(x)-np.min(x))","876ed948":"# %% train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3)\n\n# knn model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 13) # n_neighbors = k\n\n# %% K fold CV K = 10\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = knn, X = x_train, y= y_train, cv = 10)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))\n\nknn.fit(x_train,y_train)\nprint(\"test accuracy: \",knn.score(x_test,y_test))","b7aa5dac":"#Model Selection  grid search cross validation for knn\nfrom sklearn.model_selection import GridSearchCV\n\ngrid = {\"n_neighbors\":np.arange(1,50)}\nknn= KNeighborsClassifier()\n\nknn_cv = GridSearchCV(knn, grid, cv = 10)  # GridSearchCV\nknn_cv.fit(x,y)\n\n#%% print hyperparameter KNN algoritmasindaki K degeri\nprint(\"tuned hyperparameter K: \",knn_cv.best_params_)\nprint(\"tuned parametreye gore en iyi accuracy (best score): \",knn_cv.best_score_)","7b3672cf":"#Model Selection Grid search CV with logistic regression\nx = x[:100,:]\ny = y[:100] \n\nfrom sklearn.linear_model import LogisticRegression\ngrid = {\"C\":np.logspace(-3,3,7),\"penalty\":[\"l1\",\"l2\"]}  # l1 = lasso ve l2 = ridge\n\nlogreg = LogisticRegression()\nlogreg_cv = GridSearchCV(logreg,grid,cv = 10)\nlogreg_cv.fit(x,y)\n\nprint(\"tuned hyperparameters: (best parameters): \",logreg_cv.best_params_)\nprint(\"accuracy: \",logreg_cv.best_score_)","97471b4a":"import pandas as pd\nimport os\nprint(os.listdir(\"..\/input\/movielens-20m-dataset\/\"))\n# import movie data set and look at columns\nmovie = pd.read_csv(\"..\/input\/movielens-20m-dataset\/movie.csv\")\nprint(movie.columns)\nmovie = movie.loc[:,[\"movieId\",\"title\"]]\nmovie.head(10)","c0c05ceb":"# import rating data and look at columsn\nrating = pd.read_csv(\"..\/input\/movielens-20m-dataset\/rating.csv\")\nprint(rating.columns)\n# what we need is that user id, movie id and rating\nrating = rating.loc[:,[\"userId\",\"movieId\",\"rating\"]]\nrating.head(10)","08066429":"# then merge movie and rating data\ndata = pd.merge(movie,rating)\n# now lets look at our data \ndata.head(10)\nprint(data.shape)\ndata = data.iloc[:1000000,:]\n# lets make a pivot table in order to make rows are users and columns are movies. And values are rating\npivot_table = data.pivot_table(index = [\"userId\"],columns = [\"title\"],values = \"rating\")\npivot_table.head(10)","5561eb9b":"movie_watched = pivot_table[\"Bad Boys (1995)\"]\nsimilarity_with_other_movies = pivot_table.corrwith(movie_watched)  # find correlation between \"Bad Boys (1995)\" and other movies\nsimilarity_with_other_movies = similarity_with_other_movies.sort_values(ascending=False)\nsimilarity_with_other_movies.head()","d7bb460d":"# Other Content\n<a class=\"anchor\" id=\"14.\"><\/a> \n# Natural Language Process (NLP)","a3f62ef8":"\n<a class=\"anchor\" id=\"2.\"><\/a> \n# Multiple Linear Regression","e96605df":"\n<a class=\"anchor\" id=\"13.\"><\/a> \n# Hierarchical Clustering","c1fc8d2a":"\n<a class=\"anchor\" id=\"8.\"><\/a> \n# Support Vector Machine (SVM) Classification","fa48105d":"# Clustering\n<a class=\"anchor\" id=\"12.\"><\/a> \n# K-Means Clustering","ce3c25f7":"\n<a class=\"anchor\" id=\"16.\"><\/a> \n# Model Selection","2c8875e1":"\n<a class=\"anchor\" id=\"3.\"><\/a> \n# Polynomial Linear Regression","0c9598fe":"\n<a class=\"anchor\" id=\"6.\"><\/a> \n# Random Forest Regression","9d744fbd":"\n<a class=\"anchor\" id=\"5.\"><\/a> \n# Decision Tree Regression","fec53672":"# Classification\n<a class=\"anchor\" id=\"7.\"><\/a> \n# K-Nearest Neighbour (KNN) Classification","ac8b04fe":"\n\n<a class=\"anchor\" id=\"17.\"><\/a> \n# Recommendation Systems","2757d9b5":"\n<a class=\"anchor\" id=\"4.\"><\/a> \n# Support Vector Regression","38a9d029":"# **Content**\n\n## Regression\n\n* [Linear Regression](#1.)\n* [Multiple Linear Regression](#2.)\n* [Polynomial Linear Regression](#3.)\n* [Support Vector Regression](#4.)\n* [Decision Tree Regression](#5.)\n* [Random Forest Regression](#6.)\n\n![](https:\/\/iili.io\/J1bpse.md.png)\n\n\n\n\n## Classification\n\n* [K-Nearest Neighbour (KNN) Classification](#7.)\n* [Support Vector Machine (SVM) Classification](#8.)\n* [Naive Bayes Classification](#9.)\n* [Decision Tree Classification](#10.)\n* [Random Forest Classification](#11.)\n\n![](https:\/\/iili.io\/J1bmX9.png)\n\n\n\n\n\n## Clustering\n\n* [K-Means Clustering](#12.)\n* [Hierarchical Clustering](#13.)\n\n\n![](https:\/\/iili.io\/J1m9qu.png)\n\n\n\n\n## Other Content\n\n* [Natural Language Process (NLP)](#14.)\n* [Principal Component Analysis (PCA)](#15.)\n* [Model Selection](#16.)\n* [Recommendation Systems](#17.)\n\n\n","177de0c2":"\n<a class=\"anchor\" id=\"15.\"><\/a> \n# Principal Component Analysis (PCA)","489e4cbc":"\n<a class=\"anchor\" id=\"10.\"><\/a> \n# Decision Tree Classification","ad776e5f":"\n<a class=\"anchor\" id=\"9.\"><\/a> \n# Naive Bayes Classification","542799d3":"# Regression\n<a class=\"anchor\" id=\"1.\"><\/a> \n# Linear Regression\n\n","07020d3d":"\n<a class=\"anchor\" id=\"11.\"><\/a> \n# Random Forest Classification"}}