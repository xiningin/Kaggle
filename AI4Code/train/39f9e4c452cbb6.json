{"cell_type":{"de7a99f2":"code","3ba325b1":"code","94c12780":"code","15712b10":"code","3aa96b71":"code","6e8f9d1c":"code","354478e4":"code","50d9807e":"code","115a74e6":"code","d36d6ad3":"code","bff3c59f":"code","decd1f26":"code","23e1248c":"code","6cefeb80":"code","c6f1dd0c":"code","ba5d638b":"code","cb498a76":"code","8e21180f":"code","0362dc7b":"code","5764c79a":"code","c7c57476":"code","2ca64140":"code","4d7b2ddd":"code","7a80f291":"code","744e72ad":"code","bd2e574e":"code","92ea8452":"code","4ccdfd8a":"code","05b8942b":"code","3a4fe0f4":"markdown","0cc6cf65":"markdown","82f575f7":"markdown","6a6a1e99":"markdown","1defd17d":"markdown","07074da6":"markdown","89c73915":"markdown","e48bc193":"markdown","606a609d":"markdown","135ae3d2":"markdown","ca9ce1a5":"markdown","ba68ec2c":"markdown","62fe2221":"markdown","98f311aa":"markdown","2101d3d4":"markdown","1d8dc516":"markdown","5341dac2":"markdown","52afb5a6":"markdown","8417720e":"markdown","fd2f05c7":"markdown","a11f9a85":"markdown","40d2f39e":"markdown","e033f27b":"markdown","cb3ddf92":"markdown","0e1174cb":"markdown","60970878":"markdown","94b8b701":"markdown"},"source":{"de7a99f2":"# Import Packages\nimport os\nimport re\nimport nltk\nimport json\nimport torch\nimport nltk.corpus  \nimport pandas as pd\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom copy import deepcopy\nfrom fuzzywuzzy import fuzz \nfrom nltk.stem import PorterStemmer\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n!pip install transformers\nfrom transformers import BertForQuestionAnswering\nfrom transformers import BertTokenizer\nnltk.download('punkt')\nnltk.download('stopwords')","3ba325b1":"# Text Preprocessing `clean_sent()`----------------------------------------------------------------------\nporter_stemmer = PorterStemmer()\ndef clean_sent(sentence):\n    \"\"\"\n    Clean the sentence\n    :param sentence: text to to be cleaned\n    :return: text that has been cleaned\n    \"\"\"\n    #nltk.FreqDist(words).most_common(10)\n    stopwords = set(nltk.corpus.stopwords.words('english'))\n    words = sentence.split()\n    # Lowercase all words (default_stopwords are lowercase too)\n    words = [word.lower() for word in words]\n    #words = sentence\n    words = [word for word in words if len(word) > 1]\n    # Remove numbers\n    words = [word for word in words if not word.isnumeric()]\n    # Remove punctuation\n    words = [word for word in words if word.isalpha()]\n    # Remove stopwords\n    words = [word for word in words if word not in stopwords]\n    # Porter\n    words = [porter_stemmer.stem(word) for word in words]\n    #fdist = nltk.FreqDist(words_lc)   \n    return \" \".join(words)\n\n\n## Data Load----------------------------------------------------------------------\ndef format_name(author):\n    middle_name = \" \".join(author['middle'])\n    if author['middle']:\n        return \" \".join([author['first'], middle_name, author['last']])\n    else:\n        return \" \".join([author['first'], author['last']])\n\ndef format_affiliation(affiliation):\n    text = []\n    location = affiliation.get('location')\n    if location:\n        text.extend(list(affiliation['location'].values()))\n\n    institution = affiliation.get('institution')\n    if institution:\n        text = [institution] + text\n    return \", \".join(text)\n\ndef format_authors(authors, with_affiliation=False):\n    name_ls = []\n\n    for author in authors:\n        name = format_name(author)\n        if with_affiliation:\n            affiliation = format_affiliation(author['affiliation'])\n            if affiliation:\n                name_ls.append(f\"{name} ({affiliation})\")\n            else:\n                name_ls.append(name)\n        else:\n            name_ls.append(name)\n\n    return \", \".join(name_ls)\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n\n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n\n    return body\n\ndef format_bib(bibs):\n    if type(bibs) == dict:\n        bibs = list(bibs.values())\n    bibs = deepcopy(bibs)\n    formatted = []\n\n    for bib in bibs:\n        bib['authors'] = format_authors(\n            bib['authors'],\n            with_affiliation=False\n        )\n        formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n        formatted.append(\", \".join(formatted_ls))\n\n    return \"; \".join(formatted)\n\ndef load_files(dirname):\n    filenames = os.listdir(dirname)\n    raw_files = []\n\n    for filename in tqdm(filenames):\n        filename = dirname + filename\n        file = json.load(open(filename, 'rb'))\n        raw_files.append(file)\n\n    return raw_files\n\ndef clean_pdf_files(file_list, keyword_list):\n    nth_paper=0\n    cleaned_files=[]\n    for file in file_list:\n        with open(file) as f:\n            file=json.load(f)\n        features = [\n            file['paper_id'],\n            file['metadata']['title'],\n            format_authors(file['metadata']['authors']),\n            format_authors(file['metadata']['authors'],\n                           with_affiliation=True),\n            format_body(file['abstract']),\n            format_body(file['body_text']),\n            format_bib(file['bib_entries']),\n            file['metadata']['authors'],\n            file['bib_entries']\n        ]\n        if(nth_paper%1000)==0:\n            print(nth_paper)\n        nth_paper=nth_paper+1\n\n        has_keyword = False\n        for keyword in keyword_list:\n            if keyword in features[5]:\n                has_keyword = True\n                break\n        if has_keyword == True:\n            cleaned_files.append(features)\n    col_names = ['paper_id', 'title', 'authors',\n                 'affiliations', 'abstract', 'text',\n                 'bibliography','raw_authors','raw_bibliography']\n    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n    return clean_df\n\n\n\n# BERT----------------------------------------------------------------------\nmodel = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\ndef answer_question(question, answer_text):\n    '''\n    Takes a `question` string and an `answer_text` string (which contains the\n    answer), and identifies the words within the `answer_text` that are the\n    answer. Prints them out.\n    '''\n    # ======== Tokenize ========\n    # Apply the tokenizer to the input text, treating them as a text-pair.\n    input_ids = tokenizer.encode(question, answer_text,max_length=500\n                                )\n\n    # Report how long the input sequence is.\n    #print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n\n    # ======== Set Segment IDs ========\n    # Search the input_ids for the first instance of the `[SEP]` token.\n    sep_index = input_ids.index(tokenizer.sep_token_id)\n\n    # The number of segment A tokens includes the [SEP] token istelf.\n    num_seg_a = sep_index + 1\n\n    # The remainder are segment B.\n    num_seg_b = len(input_ids) - num_seg_a\n\n    # Construct the list of 0s and 1s.\n    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n\n    # There should be a segment_id for every input token.\n    assert len(segment_ids) == len(input_ids)\n\n    # ======== Evaluate ========\n    # Run our example question through the model.\n    start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n                                    token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text\n\n    # ======== Reconstruct Answer ========\n    # Find the tokens with the highest `start` and `end` scores.\n    answer_start = torch.argmax(start_scores)\n    answer_end = torch.argmax(end_scores)\n    \n    \n    # Get the string versions of the input tokens.\n    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n\n    # Start with the first token.\n    answer = tokens[answer_start]\n\n    # Select the remaining answer tokens and join them with whitespace.\n    for i in range(answer_start + 1, answer_end + 1):\n        \n        # If it's a subword token, then recombine it with the previous token.\n        if tokens[i][0:2] == '##':\n            answer += tokens[i][2:]\n        \n        # Otherwise, add a space then the token.\n        else:\n            answer += ' ' + tokens[i]\n            \n    s_scores = start_scores.detach().numpy().flatten()\n    e_scores = end_scores.detach().numpy().flatten()\n\n    return answer\n\n\n# Similarity ----------------------------------------------------------------------\ndef calc_simlarity_score(question_list, text_list,threshold=None, top=None):\n    if (threshold==None)  and  (top==None):\n        raise ValueError(\"Parameter `threshold` and `top` cannot both be None\")\n    dic = {}\n    tfidf = TfidfVectorizer()\n    corpus_tfidf_matrix = tfidf.fit_transform(text_list)\n    ques_tfidf_matrix = tfidf.transform(question_list)\n    sim_matrix = cosine_similarity(corpus_tfidf_matrix, ques_tfidf_matrix)\n    for ques_idx in range(sim_matrix.shape[1]):\n        dic[ques_idx] = []\n        if threshold != None:\n            if (threshold>1) or (threshold <0):\n                raise ValueError(\"Please enter a value from 0 to 1 for parameter `threshold`\")\n            for paper_idx in range(sim_matrix.shape[0]):\n                score = sim_matrix[paper_idx, ques_idx]\n                if score >= threshold:\n                    dic[ques_idx].append((paper_idx, score))\n            dic[ques_idx]=sorted(dic[ques_idx], key=lambda i: i[1], reverse=True)\n        elif top != None:\n            top_paper_idx_list = sorted(range(len(sim_matrix[:, ques_idx])), key=lambda i: sim_matrix[:,0][i], reverse=True)[:top]\n            dic[ques_idx] = [(top_idx, sim_matrix[top_idx, ques_idx]) for top_idx in top_paper_idx_list]\n    return dic, sim_matrix\n\n# Retrieve relevant paper----------------------------------------------------------------------\ndef retrieve_paper(df, dic):\n    df_dic={}\n    for ques_idx in dic:\n        new_df = df.iloc[[item[0] for item in dic[ques_idx]], :]\n        new_df['score'] = [item[1] for item in dic[ques_idx]]\n        new_df['question'] = questions[ques_idx]\n        df_dic[ques_idx]=new_df.copy()\n    return df_dic\n\n# Determine if a string has a value----------------------------------------------------------------------\ndef hasNumbers(inputString):\n     return any(char.isdigit() for char in inputString)\n","94c12780":"# Assign the path of the paper collections provided by Kaggle\npath = '\/kaggle\/input\/CORD-19-research-challenge\/document_parses\/pdf_json'\nfile_list = [os.path.join(r, file)  for r, _, f in os.walk(path)  for file in f] # Convert the collections to a list","15712b10":"# Set keywords to filter the papers relevant to COVID-19\nkeyword_list = ['novel coronavirus', 'novel-coronavirus', 'coronavirus-2019', \n                'sars-cov-2', 'sarscov2', 'covid-19', 'covid19',\n                '2019ncov', '2019-ncov', 'wuhan']\n\n# Clean papers\uff08This takes ~15 min\uff09\nclean_pdf_df = clean_pdf_files(file_list, keyword_list)","3aa96b71":"# Append additional info from metadata to main df\nmetadata = pd.read_csv(\"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\")\nclean_pdf_df = clean_pdf_df.merge(metadata[['sha', 'title', 'authors', 'abstract', 'doi', 'publish_time', 'journal']], \n                                  how ='left', left_on='paper_id', right_on='sha')\n\n# Clean columns\nclean_pdf_df['title_x'] = clean_pdf_df['title_x'].fillna(clean_pdf_df['title_y'])\nclean_pdf_df['authors_x'] = clean_pdf_df['authors_x'].fillna(clean_pdf_df['authors_y'])\nclean_pdf_df['abstract_x'] = clean_pdf_df['abstract_x'].fillna(clean_pdf_df['abstract_y'])\nclean_pdf_df = clean_pdf_df.drop(['sha', 'title_y', 'authors_y', 'abstract_y'], axis=1)\nclean_pdf_df = clean_pdf_df.rename(columns={'title_x': 'title', 'authors_x': 'authors', 'abstract_x': 'abstract'})","6e8f9d1c":"clean_pdf_df['text_cleaned'] = clean_pdf_df.apply(lambda row: clean_sent(row['text']), axis=1)","354478e4":"#clean_pdf_df.to_pickle((\".\/clean_pdf_df.pkl\"))\n#clean_pdf_df=pd.read_pickle(\"\/kaggle\/input\/clean-pdf-folder\/clean_pdf_df.pkl\")","50d9807e":"clean_pdf_df.shape","115a74e6":"clean_pdf_df.head()","d36d6ad3":"text_cleaned = clean_pdf_df['text_cleaned']\npath = '\/kaggle\/input\/CORD-19-research-challenge\/Kaggle\/target_tables\/2_relevant_factors\/'\n\nfile_list = sorted(list(Path(path).glob('*.csv')))\nquestions = [file.name.split(\".csv\")[0].strip('_') for file in file_list]\nquestions_cleaned = [clean_sent(ques) for ques in questions]\nfor i,q in enumerate(questions):\n    print(\"Question\" ,i + 1,\":\",q)","bff3c59f":"table_cols_dic={}\ntable_dic={}\ntarget_table_dic={}\nfor i,file in enumerate(file_list):\n    df=pd.read_csv(file)\n    cols=list(df.columns)\n    table_cols_dic[i]=cols[1:]\n    table_dic[i]=df\n    target_table_dic[i]=pd.DataFrame(columns=cols)\n[print(table_cols_dic[key]) for key in table_cols_dic.keys()]","decd1f26":"# Select relevant paper to \ndic, sim_matrix = calc_simlarity_score(questions_cleaned, text_cleaned, threshold=0.15)\nrelevant_paper_dic = retrieve_paper(clean_pdf_df, dic)","23e1248c":"relevant_paper_dic[4].head()","6cefeb80":"for key in target_table_dic.keys():\n    target_table_dic[key][['Date', 'Study', 'Journal']]=relevant_paper_dic[key][['publish_time', 'title', 'journal']]\n    target_table_dic[key]['Study Link'] = \"https:\/\/doi.org\/\" + relevant_paper_dic[key]['doi']\n    relevant_paper_dic[key]=relevant_paper_dic[key].reset_index(drop=True)\n    target_table_dic[key]=target_table_dic[key].reset_index(drop=True)\n    target_table_dic[key]['Added on'] = \"10-Jun-2020\"","c6f1dd0c":"for k,v in target_table_dic.items():\n    print(k, ':', v.shape[0])","ba5d638b":"target_table_dic[1].head()","cb498a76":"# question index\nindex=4\nquestions[index]","8e21180f":"# use key word to locate all key sentences and then use BERT to find out the question answers\nquestion = \"Effectiveness of school distancing?\"\nexcerpt_list=[]\nquestion_factor = 'what are the factors of school distancing?'\nfactor_list = []\nmeasure_evidnc_list = []\nmeasure_country_list = []\nmeasure_timeline_list = []\nstudy_type_list = []\nquestion_study = \"What is the study type or article research type like Systematic review, meta-analysis, Prospective observational study, Retrospective observational study, Observational study, Cross-sectional study, Case series, Expert review, Editorial, Simulation, or not found?\"\nquestion_measure_country = 'which country take measure on school distancing?'\nquestion_measure_timeline = 'what timeline did they take measure on school distancing'\n\n\nfor i, row in relevant_paper_dic[index].iterrows():    \n    # Divide to sentences\n    sent_list = row['text'].split('. ')\n    \n    # Find relevant sentences\n    relevant_sent_list = [sent for sent in sent_list if ( ('school' in sent.lower()) & ('distance' in sent.lower()) ) \n                          or ('effective' in sent.lower()) or ('factors' in sent.lower()) \n                          or ('timeline' in sent.lower()) or (('countries' in sent.lower()) or ('country' in sent.lower())) \n                          or ('measure' in sent.lower())\n                          or ( ('study' in sent.lower()) & ('type' in sent.lower()) )]\n    \n    # Join sentences to form excerpt\n    excerpt_string = '; \\n'.join([sent for sent in relevant_sent_list])\n    excerpt_list.append(excerpt_string)\n    \n    # run BERT on joined excerpt to get answer\n    if excerpt_string != '':\n        factor_ans = answer_question(question_factor, excerpt_string)\n        measure_country_ans = answer_question(question_measure_country, excerpt_string)\n        measure_timeline_ans = answer_question(question_measure_timeline, excerpt_string)\n        study_ans = answer_question(question_study, excerpt_string)\n        if not hasNumbers(measure_timeline_ans):\n            measure_country_ans=''\n        \n        for a in [factor_ans, measure_country_ans,measure_timeline_ans,study_ans]:\n            if (\"[CLS]\" in a) or ('[SEP]' in a):\n                a = \"\"\n            \n    else:\n        factor_ans = ''\n        measure_country_ans = ''\n        measure_timeline_ans = ''\n        study_ans = ''\n        \n    # Save measure_factor to list\n    factor_list.append(factor_ans)\n    measure_country_list.append(measure_country_ans) # answer for measure of countries\n    measure_timeline_list.append(measure_timeline_ans) # answer for measure of timeline\n    study_type_list.append(study_ans)                          \n    \ntarget_table_dic[index][\"Excerpt\"]=excerpt_list\ntarget_table_dic[index]['Factors'] = factor_list\n\nfor i,j in zip(measure_country_list,measure_timeline_list):\n    measure_evidnc_list.append((i,j))\n    \ntarget_table_dic[index]['Measure of Evidence'] = measure_evidnc_list\n","0362dc7b":"target_table_dic[index]","5764c79a":"df1 = target_table_dic[index]\ndf1.head()","c7c57476":"df1.to_csv('.\/target_table_2_q1.csv')","2ca64140":"##new method to select study type and measure of evidence: select top 5 most related sentence for a specific question, and then find out the answer in the top 5 sentences\ntarget_table_dic_2 = target_table_dic\ndef col_fill(col_questions,col_names,col_for_excerpt,may_not_have_num,index):\n    \"\"\"\n    Get answers for multiple columns and append them back to target table (target_table_dic[index])\n    :param col_questions: a list of string - questions, each corresponds to a specific column (excluding excerpt)\n    :param col_names: a list of string - columns names in target table to which col_questions correspond\n    :param col_for_excerpt: string - the most important question in col_questions, on which excerpt based\n    :param may_not_have_num: a list of string - column names in col_names, to indicate columns that may not have digits (columns that not put in this list must have digits in its answer)\n    :param index: question number in this task\n    :return: target_table_dic[index] with content filled in designated columns \n    \"\"\"\n    col_questions_cleaned = [clean_sent(ques) for ques in col_questions]\n    # go through papers\n    for i in tqdm(range(relevant_paper_dic[index].shape[0])):\n        #get sentences of each paper, preprocess it\n        paper_sent=relevant_paper_dic[index].text[i].split(\". \")\n        cleaned_paper_sent = [clean_sent(t) for t in paper_sent]\n        for num,q in enumerate(col_questions_cleaned):\n            # define questions\n            col_name=col_names[num]\n            full_question= col_questions[num]\n            # extract top 3 sentences\/paper for a specific questions, join them together as a string\n            lis=[]\n            for sent in cleaned_paper_sent:\n                lis.append(fuzz.ratio(q,sent) )\n            top_3_idx = [item[0] for item in sorted(enumerate(lis), key=lambda x: x[1],reverse=True)[0:5]]\n            #max_idx=max(enumerate(lis), key=lambda x: x[1])[0]\n            string ='; \\n'.join([paper_sent[idx] for idx in top_3_idx])\n            # Get answers, delete those without a number \n            answer=answer_question(full_question,string)        \n            if (not hasNumbers(answer)) and (col_name not in may_not_have_num):\n                answer=\"\" \n            if \"[CLS]\" in answer:\n                answer = \"\"\n            if \"[SEP]\" in answer:\n                answer = \"\"\n            target_table_dic_2[index].loc[i,col_name]=answer\n            #  Exerpt answer extract\n            if col_name == col_for_excerpt:\n                excerpt_ans=string\n                for idx in top_3_idx:\n                    if (answer in paper_sent[idx]) and (answer!=\"\"):\n                        excerpt_ans=paper_sent[idx]\n                target_table_dic_2[index].loc[i,\"Excerpt\"]=excerpt_ans\n    return target_table_dic_2[index]","4d7b2ddd":"col_questions = ['what are the factors of school distancing?',\n                 'which country take measure on school distancing?',\n                 'what timeline did they take measure on school distancing',\n                 \"What is the study type or article research type like Systematic review, meta-analysis, Prospective observational study, Retrospective observational study, Observational study, Cross-sectional study, Case series, Expert review, Editorial, Simulation, or not found?\"]\n\ncol_names=['Factors','Measure_country',\"Measure_timeline\",\"Study Type\"]\ncol_for_excerpt='Factors'\nmay_not_have_num=['Factors','Measure_country',\"Study Type\"]\n\ntarget_table_dic_2[index]=col_fill(col_questions,col_names,col_for_excerpt,may_not_have_num,index)","7a80f291":"target_table_dic_2[index]['Measure of Evidence'] = target_table_dic_2[index].apply(lambda x: (x['Measure_country'], x['Measure_timeline']), axis = 1)","744e72ad":"target_table_dic_2[index]","bd2e574e":"\ndf2 = target_table_dic_2[index].rename(columns = {'Factors':'Factors_2','Study Type':'Study Type_2','Measure of Evidence':'Measure of Evidence_2'})\ndf2.to_csv('.\/target_table_2_q1_2.csv')","92ea8452":"tb = pd.read_csv('..\/input\/output\/target_table_2_q1_combined.csv')\ntb.shape","4ccdfd8a":"tb","05b8942b":"tb.to_csv('.\/target_table_2_q1_final_result.csv')","3a4fe0f4":"# Data Preparation","0cc6cf65":"## Functions\n### Data Loading\n- Function `format_name(author)`, `format_affiliation(affiliation)`, `format_authors(authors, with_affiliation=False)`, `format_body(body_text)`, and `format_bib(bibs)` each converts the paper-related information to standard format.\n- Function `load_files(dirname)` loads the papers from json and saves them to a list.\n- Function `clean_pdf_files(file_list, keyword_list)` takes a list of file and converts it to a dataframe with cleaned columns.\n\n> The script for loading papers is based on: https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv","82f575f7":"# Initial Setup","6a6a1e99":"Check for the number of relevant papers for each questions","1defd17d":"## Load Libraries","07074da6":"![image.png](attachment:image.png)","89c73915":"## 2. Combine selected papers with the metadata","e48bc193":"# Introduction\nWith the COVID-19 keeps ongoing, the number of research publications on COVID-19 is growing fast, making it increasingly difficult for researchers to spot the most relevant findings. This project aims to help the medical community find the information they seek by building data mining and search tools, which can ultimately support the ongoing fight against this pandemic. This notebook contains a basic pipeline of using NLP models to develop answers to high priority scientific questions regarding the COVID-19.\n\nThe pipeline in this notebook includes the following steps:\n1. Initial Setup\n2. Functions\n3. Data Preparation\n4. Paper Selection\n4. Extract Answers for Summary Tables","606a609d":"## Question: \"Effectiveness of school distancing\" `index: 4`","135ae3d2":"## 4. Add common columns to all summary tables","ca9ce1a5":"## 1. Load data","ba68ec2c":"### Load relevant papers\n* Load the papers and related information to the data structure.\n* Filter the papers to only articles related to COVID-19.","62fe2221":"# Contact\n* Kaitan Sun, kaitan9095@gmail.com\n* Jiangxue Han, jxhan0317@gmail.com\n* Richard Luo, ruize.luo@outlook.com\n* Hanying Gan, hanying.gan@outlook.com\n","98f311aa":"# Conclusion\nHere we tackled the question about the effectiveness of school distancing. Two methods are invovled in extracting answers for specific columns: factors, influencial, measure of evidence, study type.\n\nThe first method is to use key word to locate key sentences, and then apply BERT model on the key sentences to find out specific answers. The second method is similar to the first one, it is to extract top 3 sentences\/paper for a specific questions, join them together as a string, and then apply BERT model to find out answers. \n\nThe first method performs well on the extraction of factors and measure of evidence, while the second model has better performs on the extraction of study type, and some of the results for measure of evidence can be a complement to the first method result. So the final result 'tb' is a combination of method 1 and method 2 with certain human adjustments. \n\nFrom the table we can see that most of the school distancing methods involves school closure, from care center to universities, and has a influencial impact on reducing the virus transmission, especially when combined with other social distancing methods.\n","2101d3d4":"# TF-IDF Cosine Simlarity & Key Words \/BERT [Task 2]\n\nThis notebook is for task 2 only","1d8dc516":"### Extract Answers for Summary Tables\n- Function `hasNumbers(inputString)` determines if a string contains any digit, which is used to filter answers to certain questions that expect the answer to be a digit. \n- Function `col_fill(col_questions,col_names,col_for_excerpt,may_not_have_num,index)` fills the summary table with the existing columns from the paper collection(if available).\n- Function `answer_question(question, answer_text)` extracts the information with given question.\n\n> The code for Bert is based on: https:\/\/colab.research.google.com\/drive\/1uSlWtJdZmLrI3FCNIlUHFxwAJiSu2J0-","5341dac2":"### Paper Selection\n- Function `calc_simlarity_score(question_list, text_list,threshold=None, top=None)` calculates the similarity score between the TF-IDF matrix of the article and that of questions.\n- Function `retrieve_paper(df, dic)` retrieves the information of relevant papers for each question in the specific topic.\n\n> The code for TF-IDF cosine similarity is based on: https:\/\/github.com\/mbulusu\/Duplicate-Document-Detection-Meetup-Presentation","52afb5a6":"## 2. Build table structures with given target tables","8417720e":"## 3. Data preprocessing\n* Use lowercase for all content.\n* Remove numbers, punctuations, stopwords.\n* Remove the commoner morphological and inflexional endings from words in English.\n","fd2f05c7":"### Data Cleaning\n- Function `clean_sent(sentence)` cleans the text by lowercasing all words, removing numbers, punctuations, and stopwords, and stemming all words.","a11f9a85":"## 3. Select relevant papers by TF-IDF cosine similarity","40d2f39e":"# Extract Answers for Summary Tables","e033f27b":"# Reference\n- The code for paper loading functions is based on: https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv\n- The code for TF-IDF cosine similarity is based on: https:\/\/github.com\/mbulusu\/Duplicate-Document-Detection-Meetup-Presentation\n- The code for Bert is based on: https:\/\/colab.research.google.com\/drive\/1uSlWtJdZmLrI3FCNIlUHFxwAJiSu2J0-","cb3ddf92":"## 1. Set cleaned text and topic","0e1174cb":"### 2.4 (Optional) Save dataframe","60970878":"### Set directory","94b8b701":"# Paper selection\nThe relevant papers for each question are selected by calculating the cosine similarity score based on the TF-IDF pair of articles and questions. \n\nTF-IDF is a statistical measure that evaluates how important a word is to a document when compared against the other document in the corpus. In this project, both articles and questions are converted to TF-IDF matrix so that the information can be captured with fewer words and can be used for calculation.\n\nCosine similarity is a commonly used metric to measure the similarity between two vectors by taking the dot product divided by the product of two vector\u2019s length. In this project, cosine similarity is used to measure the similarity between the TF-IDF matrix pair of documents and questions. In this way, the level of relevance can be quantified, which ultimately helps us to identify the most relevant papers for each question against more than 6,000 articles.\n"}}