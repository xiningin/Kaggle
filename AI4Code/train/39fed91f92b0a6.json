{"cell_type":{"393c8999":"code","61a7858d":"code","466f15c8":"code","ce5a8be7":"code","06a95703":"code","b53a559a":"code","ca74162a":"code","ce14d78d":"code","06566581":"markdown","8826a77f":"markdown","b2bcf28a":"markdown","bc119c98":"markdown"},"source":{"393c8999":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.preprocessing import LabelEncoder\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","61a7858d":"train_df = pd.read_json('..\/input\/train.json')\ntest_df = pd.read_json('..\/input\/test.json')\ntrain_df.head()","466f15c8":"def generate_text(data):\n\ttext_data = [\" \".join(doc).lower() for doc in data.ingredients]\n\treturn text_data \n\ntrain_text = generate_text(train_df)\ntest_text = generate_text(test_df)\ntarget = [doc for doc in train_df.cuisine]","ce5a8be7":"# Feature Engineering \nprint (\"TF-IDF on text data ... \")\ntfidf = TfidfVectorizer(binary=True)\ndef tfidf_features(txt, flag):\n    if flag == \"train\":\n    \tx = tfidf.fit_transform(txt)\n    else:\n\t    x = tfidf.transform(txt)\n    x = x.astype('float16')\n    return x ","06a95703":"X = tfidf_features(train_text, flag=\"train\")\nX_test = tfidf_features(test_text, flag=\"test\")\n\n# Label Encoding - Target \nprint (\"Label Encode the Target Variable ... \")\nlb = LabelEncoder()\ny = lb.fit_transform(target)","b53a559a":"from sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\nfrom sklearn.svm import SVC\nclassifier = SVC(C=100, # penalty parameter, setting it to a larger value \n                 kernel='rbf', # kernel type, rbf working fine here\n\t \t\t\t degree=3, # default value, not tuned yet\n\t \t\t\t gamma=1, # kernel coefficient, not tuned yet\n\t \t\t\t coef0=1, # change to 1 from default value of 0.0\n\t \t\t\t shrinking=True, # using shrinking heuristics\n\t \t\t\t tol=0.001, # stopping criterion tolerance \n\t      \t\t probability=False, # no need to enable probability estimates\n\t      \t\t cache_size=200, # 200 MB cache size\n\t      \t\t class_weight=None, # all classes are treated equally \n\t      \t\t verbose=False, # print the logs \n\t      \t\t max_iter=-1, # no limit, let it run\n          \t\t decision_function_shape=None, # will use one vs rest explicitly \n          \t\t random_state=None)\n\nmodel = OneVsRestClassifier(classifier, n_jobs=4)\nmodel.fit(X, y)\n# model = xgb.XGBClassifier(max_depth=6, n_estimators=1000, learning_rate=0.1\n#                          ,min_child_weight=5,\n#                          gamma=1,\n#                          subsample=0.8,\n#                          colsample_bytree=0.8,\n#                          nthread=4,\n#                          scale_pos_weight=1,\n#                          )\n# model.fit(X,y)","ca74162a":"\n# Predictions \nprint (\"Predict on test data ... \")\ny_test = model.predict(X_test)\ny_pred = lb.inverse_transform(y_test)\n\n# Submission\nprint (\"Generate Submission File ... \")\ntest_id = [doc for doc in test_df.id]\nsub = pd.DataFrame({'id': test_id, 'cuisine': y_pred}, columns=['id', 'cuisine'])\nsub.to_csv('svm_output.csv', index=False)","ce14d78d":"sub.head(5)","06566581":"****Some Feature Engineering****","8826a77f":"This is the notebook to the What's Cooking competition. \nI have used SVM. ","b2bcf28a":"**Prediction on Test data**","bc119c98":"**Fitting the Model using SVM**"}}