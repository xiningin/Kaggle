{"cell_type":{"158c30a2":"code","ba3403b8":"code","78891c52":"code","0fff4842":"code","33254e47":"code","16dbca80":"code","7b3b3091":"code","0a249a64":"code","cbd85b13":"code","bd8d5947":"code","abe41dd1":"code","c4007ff8":"code","f519e080":"code","335e0387":"code","a4e36968":"code","4c7ccf83":"markdown","623b7b45":"markdown","162675e4":"markdown","596aa5c3":"markdown","873ac4dd":"markdown","76af0734":"markdown","9f72f72a":"markdown"},"source":{"158c30a2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom datetime import datetime\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nfrom os.path import join as pjoin\n\ndata_root = '..\/input\/make-data-ready'\nprint(os.listdir(data_root))\n\n# Any results you write to the current directory are saved as output.","ba3403b8":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestRegressor as RFF\nimport xgboost as xgb\n\nfrom pprint import pprint\nimport math\n\nfrom scipy.stats import kurtosis, skew\n","78891c52":"\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport shap\nplt.rcParams['figure.figsize'] = (12,6)","0fff4842":"def load_data(data='train',n=2):\n    df = pd.DataFrame()\n    for i in range(n) :\n        if data=='train':\n            if i > 8 :\n                break\n            dfpart = pd.read_pickle(pjoin(data_root,f'train_{i}.pkl'))\n        elif data=='test':\n            if i > 2 :\n                break\n            dfpart = pd.read_pickle(pjoin(data_root,f'test_{i}.pkl'))\n        df = pd.concat([df,dfpart])\n        del dfpart\n    return df\n        ","33254e47":"df_train = load_data(n=9)\ndf_test = load_data('test',n=4)","16dbca80":"print(f'# of columns has na value: {(df_test.isnull().sum().sort_values(ascending=False) > 0).sum()}')","7b3b3091":"def rmse(y_true, y_pred):\n    return round(np.sqrt(mean_squared_error(y_true, y_pred)), 5)\n\ndef split_data(df=df_train,rate=.8):\n    # sort the date first\n    df = df.sort_values('date').copy()\n    \n    df.drop(['fullVisitorId','visitId','visitStartTime'],axis=1,inplace=True)\n    df['Revenue'] = np.log1p(df['Revenue'])\n    \n    global X_train,X_valid,y_train,y_valid\n    \n    n_train = int(len(df)*rate)\n    X_train = df.drop(['Revenue','date'],axis=1).iloc[:n_train]\n    X_valid = df.drop(['Revenue','date'],axis=1).iloc[n_train:]\n    \n    y_train = df['Revenue'].iloc[:n_train]\n    y_valid = df['Revenue'].iloc[n_train:]\n    \n    print(X_train.shape,X_valid.shape)\n    \n    \n\ndef encode_data(verbose=False):\n    global df_train_encoded,df_test_encoded\n    df_train_encoded = df_train.copy()\n    df_test_encoded = df_test.copy()\n    for col in df_train.columns:\n        if df_train_encoded[col].dtype == 'object' and col not in ['fullVisitorId','visitId','visitStartTime','date']:\n            if verbose:\n                print(col)\n            lb = LabelEncoder()\n            lb.fit( list(df_train_encoded[col].unique()) + list(df_test_encoded[col].unique()))\n            df_train_encoded[col] = lb.transform(df_train_encoded[col])\n            df_test_encoded[col] = lb.transform(df_test_encoded[col])\n        \ndef run_xgb():\n    params = {\n        'objective':'reg:linear',\n        'eval_metric':'rmse',\n        'learning_rate':.01,\n        'eta': 0.1, # Step size shrinkage used in update to prevents overfitting\n        'max_depth': 10,\n        'subsample': 0.6, # sample of rows\n        'colsample_bytree': 0.6, # sample of features\n#         'alpha':0.001, \n        'lambda':1, # l2 regu\n        'random_state': 42,\n        'silent':True\n        \n    }\n    xgb_train_data = xgb.DMatrix(X_train, y_train)\n    xgb_val_data = xgb.DMatrix(X_valid, y_valid)\n    \n    model = xgb.train(params, xgb_train_data,\n          num_boost_round=1000,\n          evals= [(xgb_train_data, 'train'), (xgb_val_data, 'valid')],\n          early_stopping_rounds=10, \n          verbose_eval=20\n         )\n    return model\n\ndef submit():\n    test_matrix = xgb.DMatrix(X_test)\n    y_pred = clf.predict(test_matrix,ntree_limit=clf.best_ntree_limit)\n    df_test['PredictedLogRevenue'] = y_pred\n    submit = df_test[['PredictedLogRevenue','fullVisitorId']].groupby('fullVisitorId').PredictedLogRevenue.sum().reset_index()\n    submit.to_csv('submit.csv',index=False)\n    \n    test(y_pred)\n\n    \ndef test(predict):\n    y_test = df_test['totals_transactionRevenue']\n    print(rmse(y_test,predict))\n","0a249a64":"def prepare_data(df_train,df_test,\n                 del_col=['fullVisitorId','visitId','visitStartTime','date']):\n    df_train = df_train.sort_values('date').copy()\n    \n    df_train = df_train.drop(del_col,axis=1).copy()\n    df_test = df_test.drop(del_col,axis=1).copy()\n    \n    # totals_transactionRevenue\n    df_train['totals_transactionRevenue'] = np.log1p(df_train['totals_transactionRevenue'])\n    df_test['totals_transactionRevenue'] = np.log1p(df_test['totals_transactionRevenue'])\n    \n    global X_train,X_valid,y_train,y_valid,X_test,y_test\n    # 80\/20 : train\/valid\n    n_train = int(len(df_train)*.8)\n    \n    # split\n    X_train = df_train.drop(['totals_transactionRevenue'],axis=1).iloc[:n_train]\n    X_valid = df_train.drop(['totals_transactionRevenue'],axis=1).iloc[n_train:]\n    \n    y_train = df_train['totals_transactionRevenue'].iloc[:n_train]\n    y_valid = df_train['totals_transactionRevenue'].iloc[n_train:]\n    \n    X_test = df_test.drop(['totals_transactionRevenue'],axis=1)\n    y_test = df_test['totals_transactionRevenue']\n    \n    ","cbd85b13":"encode_data(verbose=True)\nprepare_data(df_train_encoded,df_test_encoded,del_col=['fullVisitorId','visitId',\n            'visitStartTime','date','totals_totalTransactionRevenue','totals_transactions'])\n","bd8d5947":"clf = run_xgb()","abe41dd1":"# try to find a good validation set\n# Why our score so different with the leader board?\n# check with the target in dataset first","c4007ff8":"xgb.plot_importance(clf,importance_type='gain')\nplt.title('Gain Feature important')","f519e080":"xgb.plot_importance(clf,importance_type='cover')\nplt.title('Cover Feature important')","335e0387":"xgb.plot_importance(clf,importance_type='weight')\nplt.title('Weight Feature important')","a4e36968":"submit()","4c7ccf83":"# Import and load data","623b7b45":"# Base model","162675e4":"\"weight\" is the number of times a feature appears in a tree","596aa5c3":"\"cover\" is the average coverage of splits which use the feature where coverage is defined as the number of samples affected by the split","873ac4dd":"# Submit","76af0734":"\"gain\" is the average gain of splits which use the feature","9f72f72a":"# Feature important"}}