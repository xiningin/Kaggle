{"cell_type":{"80a2db66":"code","51bd65d7":"code","6155086f":"code","5d334229":"code","39cac4d6":"code","adb5e9cd":"code","e5ef7274":"code","458651b1":"code","0a1fc813":"code","15062bbe":"code","640e76af":"code","d8439975":"code","487578e2":"code","8ac860d5":"code","4f341849":"code","f604256f":"code","73a7559a":"code","638fdfb1":"code","cf9ad093":"code","b98c1c20":"markdown","b2d5292e":"markdown","b232648b":"markdown","f4b8d25f":"markdown","d62ebbfb":"markdown","ca625acd":"markdown","e4c20016":"markdown","ff036f3f":"markdown","c8443d7b":"markdown","90910685":"markdown","c3abd771":"markdown","b1b2996c":"markdown","a71df9fe":"markdown","f42dd11c":"markdown","ae619468":"markdown","60241485":"markdown"},"source":{"80a2db66":"%matplotlib inline\n\nimport os\nimport numpy as np\nimport tensorflow as tf\n\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\n\nprint('Using TensorFlow', tf.__version__)","51bd65d7":"generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    rotation_range=40\n)","6155086f":"image_path = '..\/input\/data-aug\/images\/train\/cat\/cat.jpg'\n\nplt.imshow(plt.imread(image_path));","5d334229":"x, y = next(generator.flow_from_directory('..\/input\/data-aug\/images', batch_size=1))\nplt.imshow(x[0].astype('uint8'));","39cac4d6":"generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    width_shift_range=[-40, -20, 0, 20, 40],\n    height_shift_range=[-50,50]\n)","adb5e9cd":"x, y = next(generator.flow_from_directory('..\/input\/data-aug\/images', batch_size=1))\nplt.imshow(x[0].astype('uint8'));","e5ef7274":"generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    brightness_range=(0., 2.)\n)\n\nx, y = next(generator.flow_from_directory('..\/input\/data-aug\/images', batch_size=1))\nplt.imshow(x[0].astype('uint8'));","458651b1":"generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    shear_range=45\n)\n\nx, y = next(generator.flow_from_directory('..\/input\/data-aug\/images', batch_size=1))\nplt.imshow(x[0].astype('uint8'));","0a1fc813":"generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    zoom_range=0.5\n)\n\nx, y = next(generator.flow_from_directory('..\/input\/data-aug\/images', batch_size=1))\nplt.imshow(x[0].astype('uint8'));","15062bbe":"generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    channel_shift_range=100\n)\n\nx, y = next(generator.flow_from_directory('..\/input\/data-aug\/images', batch_size=1))\nplt.imshow(x[0].astype('uint8'));","640e76af":"generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    horizontal_flip=True,\n    vertical_flip=True\n)\n\nx, y = next(generator.flow_from_directory('..\/input\/data-aug\/images', batch_size=1))\nplt.imshow(x[0].astype('uint8'));","d8439975":"(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n\ngenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n    featurewise_center=True,\n    featurewise_std_normalization=True\n)\n\ngenerator.fit(x_train)","487578e2":"x, y = next(generator.flow(x_train, y_train, batch_size=1))\nprint(x.mean(), x.std(), y)\nprint(x_train.mean())","8ac860d5":"generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    samplewise_center=True,\n    samplewise_std_normalization=True\n)\n\nx, y = next(generator.flow(x_train, y_train, batch_size=1))\nprint(x.mean(), x.std(), y)","4f341849":"generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n    rescale=1.\n)","f604256f":"x, y = next(generator.flow(x_train, y_train, batch_size=1))","73a7559a":"print(x.mean(), x.std(), y)","638fdfb1":"model = tf.keras.models.Sequential([\n    tf.keras.applications.mobilenet_v2.MobileNetV2(include_top=False, input_shape=(32, 32, 3), pooling='avg'),\n    tf.keras.layers.Dense(10, activation='softmax')\n])\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","cf9ad093":"_ = model.fit(\n    generator.flow(x_train, y_train, batch_size=32),\n    steps_per_epoch=10, epochs=1\n)","b98c1c20":"# Task 10: Rescale and Preprocessing Function","b2d5292e":"# Task 5: Shear Transformation\n\n\n#### Shear transformation slants the shape of the image. This is different from rotation in the sense that in shear transformation, we fix one axis and stretch the image at a certain angle known as the shear angle. This creates a sort of \u2018stretch\u2019 in the image, which is not seen in rotation. shear_range specifies the angle of the slant in degrees.","b232648b":"### Why do we need Image Augmentation??\n\nImage data augmentation is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset. ... Image data augmentation is used to expand the training dataset in order to improve the performance and ability of the model to generalize.","f4b8d25f":"# Task 11: Using in Model Training\n\n\n\n##### Ignore the warning messages!!","d62ebbfb":"# Task 6: Zoom\n\n#### A random zoom is obtained by the zoom_range argument. A zoom less than 1.0 magnifies the image, while a zoom greater than 1.0 zooms out of the image.","ca625acd":"# Image Data Augmentation with Keras\n\n![Horizontal Flip](..\/input\/\/data-aug\/assets\/dataaug.jpeg)","e4c20016":"* This Notebook contains the tutorial on all the important parameters of ImageDataflowgenerator class of keras preprocessing.\n* \n* I've covered \n* 1. Rotation\n* 2.Width and height shifts\n* 3.Brightness\n* 4.Shear Transformation\n* 5.Zoom\n* 6.Channel Shift\n* 7. Flips\n* 8.Normalization\n*   8.1 Samplewise \n*   8.2 featurewise\n* 9.Rescale and Preprocessing","ff036f3f":"### Samplewise","c8443d7b":"# Task 4: Brightness\n\n#### brightness_range: Tuple or list of two floats. Range for picking a brightness shift value from.","90910685":"#### Ignore the accuracy of the model. As the idea was just to demonstrate image augmentation while training the model and moreover we've just used the very basic shallow neural network.","c3abd771":"# Task 1: Import Libraries","b1b2996c":"# Task 8: Flips\n\n\n#### The generator will generate images, which on a random basis, will be horizontally flipped.\n#### Instead of flipping horizontally, we can also apply a vertical flip.","a71df9fe":"# Task 7: Channel Shift\n\n#### Channel shift randomly shifts the channel values by a random value chosen from the range specified by `channel_shift_range`.","f42dd11c":"# Task 3: Width and Height Shifts\n\n\n#### width_shift_range: Float, 1-D array-like or int\n#### float: fraction of total width, if < 1, or pixels if >= 1.\n#### 1-D array-like: random elements from the array.\n#### int: integer number of pixels from interval (-width_shift_range, +width_shift_range)\n#### With width_shift_range=2 possible values are integers [-1, 0, +1], same as with width_shift_range=[-1, 0, +1], while with #### width_shift_range=1.0 possible values are floats in the interval [-1.0, +1.0).","ae619468":"# Task 9: Normalization\n\n#### The pixel standardization is supported at two levels: either per-image (called sample-wise) or per-dataset (called feature-wise). Specifically, the mean and\/or mean and standard deviation statistics required to standardize pixel values can be calculated from the pixel values in each image only (sample-wise) or across the entire training dataset (feature-wise).","60241485":"# Task 2: Rotation\n\n#### rotation_range: Int. Degree range for random rotations."}}