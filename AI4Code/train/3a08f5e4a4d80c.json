{"cell_type":{"6ca860a3":"code","14bc2f0a":"code","c007b143":"code","baa43cab":"code","30689132":"code","d5198a46":"code","a95e8dfe":"code","289c879b":"code","c0eeeac5":"code","c97ec38c":"code","e8bc86ad":"code","b2cc89bb":"code","3178b423":"code","c494559d":"code","a57dbc35":"code","00fe5f3d":"code","846df1ec":"code","3d87ecc0":"code","60696788":"code","4eacd5a5":"code","ee5636cb":"code","43a62fe9":"code","d9a9ef99":"code","72367feb":"code","66284fef":"code","042ebfd5":"code","a3a56788":"code","34f1675a":"code","25e7fcb8":"code","950d00ce":"code","a61e165d":"code","b64ba6fd":"code","2f8ca72a":"code","ae8fe88c":"code","d6edbce8":"code","04ea2b2a":"code","65399b9a":"code","457738f0":"code","ef8e0247":"code","6a54891a":"code","6b1c8543":"code","eda8a9d5":"code","19d10a51":"code","1d1e1709":"code","4bb3f78f":"code","05fcefa7":"code","e2ba4b95":"code","a7fb636d":"code","30494380":"code","327ce9f7":"code","7848d1cc":"code","ff81412d":"markdown","682dcda1":"markdown","7f25bd65":"markdown","d0012b67":"markdown","dfefcb0b":"markdown","ea2b2bf2":"markdown","b9849a33":"markdown","d0bfaf12":"markdown","9eb1df57":"markdown","e880ccd2":"markdown","abc7d8cd":"markdown","539ddd63":"markdown","9c2edba0":"markdown","dd1547be":"markdown"},"source":{"6ca860a3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\n\npd.options.display.max_colwidth = 200\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames[:5]:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","14bc2f0a":"!pip install kaggle ","c007b143":"!pip install https:\/\/med7.s3.eu-west-2.amazonaws.com\/en_core_med7_lg.tar.gz","baa43cab":"# We read all the medical notes from the directory\ndir = '\/kaggle\/input\/nlp-specialization-data\/Medical_Notes\/Medical_Notes'\nprint (\"Total {} files in directory\".format(len(os.listdir(dir))))","30689132":"labels = pd.read_csv(\"\/kaggle\/input\/nlp-specialization-data\/Labels_Medical_Notes.csv\",header=None)\nlabels.columns = ['file','label']","d5198a46":"labels.head(5)","a95e8dfe":"labels.shape[0]","289c879b":"labels['label'].value_counts()","c0eeeac5":"\"-\".join([\"Rahul\",\"Aggarwal\",\"HERE\"])","c97ec38c":"# read each medical notes and the corresponding label (disease category)\ntexts = []\nclasses = []\n\nfor i in tqdm(range(labels.shape[0])):\n    filename = os.path.join(dir,labels.iloc[i]['file'])\n    text = \" \".join(open(filename,'r',errors='ignore').readlines())\n    texts.append(text)\n    classes.append(labels.iloc[i]['label'])\n    \ndata = pd.DataFrame()\ndata['text'] = texts\ndata['label'] = classes","e8bc86ad":"print (data.shape)","b2cc89bb":"data.head(5)","3178b423":"data.sample(10)","c494559d":"sample_text = data.text.iloc[3]\nprint (sample_text)","a57dbc35":"import re","00fe5f3d":"def remove_html(text):\n    text = text.replace(\"\\n\",\" \")\n    pattern = re.compile('<.*?>') #all the HTML tags\n    return pattern.sub(r'', text)","846df1ec":"sample_text_processed = remove_html(sample_text)\nprint (sample_text_processed)","3d87ecc0":"def remove_headings(text):\n    pattern = re.compile('\\w+:')\n    return pattern.sub(r'', text)","60696788":"sample_text_processed = remove_headings(sample_text_processed)\nprint (sample_text_processed)","4eacd5a5":"def replace_mult_spaces(text):\n    text = text.replace(\"&quot\",\"\")\n    pattern = re.compile(' +')\n    text = pattern.sub(r' ', text)\n    text = text.strip()\n    return text","ee5636cb":"replace_mult_spaces(\"Rahul     Aggarwal\")","43a62fe9":"string=\" Rahul \"\nstring","d9a9ef99":"string.strip()","72367feb":"sample_text_processed = replace_mult_spaces(sample_text_processed)\nprint (sample_text_processed)","66284fef":"# read.csv(r\"download\/txct.csv\")","042ebfd5":"def replace_other_chars(text):\n    pattern = re.compile(r'[()!@&;]')\n    text = pattern.sub(r'', text)\n    return text","a3a56788":"sample_text_processed = replace_other_chars(sample_text_processed)\nprint (sample_text_processed)","34f1675a":"def clean_text(text):\n    text = remove_html(text)\n    text = remove_headings(text)\n    text = replace_mult_spaces(text)\n    text = replace_other_chars(text)\n    text = text.lower()\n    return text","25e7fcb8":"data","950d00ce":"data['clean_text'] = data.text.apply(clean_text)\ndata","a61e165d":"import matplotlib.pyplot as plt\n\ndata.clean_text.apply(len).plot.hist()\ndata.text.apply(len).plot.hist()\nplt.title(\"Distribution of total number of characters in the clinical notes\")\nplt.legend([\"before cleaning\",\"after cleaning\"])\nplt.show()","b64ba6fd":"data.clean_text.apply(lambda x: len(x.split())).plot.hist()\ndata.text.apply(lambda x: len(x.split())).plot.hist()\nplt.title(\"Distribution of total number of words in the clinical notes\")\nplt.legend([\"before cleaning\",\"after cleaning\"])\nplt.show()","2f8ca72a":"sample_text = data.clean_text.iloc[1]\nprint (sample_text)","ae8fe88c":"import nltk\n\ndef simple_stemmer(text):\n    ps = nltk.stem.SnowballStemmer('english')\n    text = ' '.join([ps.stem(word) for word in text.split()])\n    return text","d6edbce8":"stemmed_text = simple_stemmer(sample_text)\nprint (stemmed_text)","04ea2b2a":"\" \".join([\"Rahul\",\"Aggarwal\",\"is\",\"a\",\"trainer\"])","65399b9a":"import spacy\nimport en_core_med7_lg #en_core_web_sm\n\nnlp = en_core_med7_lg.load()\n\ndef simple_lemmatizer(text):\n    text = nlp(text)\n    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n    return text","457738f0":"test_text=nlp(\"preoper ganglion of the left wrist. postop ganglion of the left wrist. excis of ganglion. general. estim blood less than 5 ml\")\ntest_text","ef8e0247":"for word in test_text:\n    print(word.lemma_)","6a54891a":"sample_text","6b1c8543":"lemmatized_text = simple_lemmatizer(sample_text)\nprint (lemmatized_text)","eda8a9d5":"sample_text = data.clean_text.iloc[1]\ndoc = nlp(sample_text)\nfor token in doc:\n    print(token.text, token.pos_)","19d10a51":"pd.Series(\" \".join(data.clean_text.values).split()).value_counts().head(20)","1d1e1709":"stopword_list = nltk.corpus.stopwords.words('english')\n\nprint (stopword_list[:10])","4bb3f78f":"def lemmatize_and_remove_stopwords(text):\n    doc = nlp(text)\n    tokens = [word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in doc]\n    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text","05fcefa7":"sample_text","e2ba4b95":"sample_text_processed = lemmatize_and_remove_stopwords(sample_text)\nprint (sample_text_processed)","a7fb636d":"data.clean_text = data.clean_text.apply(lemmatize_and_remove_stopwords)","30494380":"data.clean_text.apply(lambda x: len(x.split())).plot.hist()\nimport matplotlib.pyplot as plt\nplt.title(\"Distribution of total number of words in the texts\")\nplt.show()","327ce9f7":"data","7848d1cc":"#data.to_csv(\"clinical_notes_cleaned.csv\",index=False)","ff81412d":"<a id='datasetreading'><\/a>\n\n### Read the dataset\n\nThe dataset is collected from https:\/\/www.kaggle.com\/c\/medical-notes\/data. It contains 800 anonymised transcribed medical reports with the disease category (specialty). For more information browse original source - https:\/\/www.mtsamples.com\/","682dcda1":"<a id='regex'><\/a>\n\n### Basic data cleaning\n\nNatural language in its pure form can bring lot of noise. We need to clean the data in order to use any statistical\/machine learning model. Below are the few techniques for cleaning the text data.\n\n* Using RegEx (regular expressions) to identify the irrelevant text sections for removal\n* Standardizing\/normalizing texts like - abbreviations, spelling mistakes\n* For social media data - remove smileys, email ids if these information are not relevant for downstream analysis\n","7f25bd65":"Remove &quot marks and other characters. Replace multiple spaces with single space","d0012b67":"<a id='tokenization'><\/a>\n\n### Tokenization\n\nTokenization splits a text into tokens or, words. Typically, words are splitted based on blank spaces. But tokenizations can also split words joined by other characters.","dfefcb0b":"<a id='stopword'><\/a>\n\n### Stop word removal\n\nLet us first see the most frequent words in the dataset","ea2b2bf2":"<a id='stemming'><\/a>\n\n### Stemming and Lemmatization\n\nStemming changes word into its root stem. \n\n<img src = https:\/\/miro.medium.com\/max\/359\/1*l65c30sY9fQsWPKIckqmCQ.png>\n\nHowever, the root stem may not be lexicographically a correct word. Lemmatization on the other hand standardizes a word into its root word. Lemmatization deals with higher level of abstraction.\n\n<img src = https:\/\/devopedia.org\/images\/article\/227\/6785.1570815200.png>\n","b9849a33":"## Contents\n\n* [Read the dataset](#datasetreading)\n* [Data Cleaning](#regex)\n* [Stemming & Lemmatization](#stemming)\n* [Tokenization](#tokenization)\n* [Stop word removal](#stopword)","d0bfaf12":"Putting everything together in a function and apply the cleaning on all the texts. Further, convert everything into lower case.","9eb1df57":"Remove all the headings from text","e880ccd2":"<a id='eda'><\/a>\n\n### Basic descriptive analysis on the texts","abc7d8cd":"### References for further reading\n\n<strong> NLP overview - <\/strong> https:\/\/towardsdatascience.com\/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\n\n<strong> Regular Expressions - <\/strong> https:\/\/regex101.com\/ \n\n<strong> Spacy - <\/strong> https:\/\/spacy.io\/usage\/spacy-101\n\n<strong> NLTK - <\/strong> https:\/\/www.nltk.org\/book\/\n\n","539ddd63":"remove multiple consecutive spaces and replace with single space","9c2edba0":"We remove all the special characters like - \"\\n\", HTML tags from the texts","dd1547be":"Top 10 words based on frequency are english words like - articles, conjuctions, prepositions etc. These words often do not play in significant roles in the downstream applications. We need to remove these words to reduce the model complexity."}}