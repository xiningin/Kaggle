{"cell_type":{"2be39a19":"code","a0113eda":"code","898f05ec":"code","b5bee82c":"code","2dcbffe7":"code","f148e348":"code","9376ea3f":"code","4c247ebb":"code","cd5b368e":"code","a67b81ab":"code","86b6dde4":"code","a0ed659c":"code","71fbf6c6":"code","6aa81336":"code","b48ed8e1":"code","274709aa":"code","b29baf3e":"code","336032d1":"code","27914303":"code","512e2b22":"code","351b3ddb":"code","761c573e":"code","fb9ec286":"code","a0c6d92e":"code","99770539":"code","eb739411":"code","60612595":"code","8e950d94":"code","839eaeb7":"code","febe341e":"code","e20068c8":"code","c5d1fc8e":"code","62b7895b":"code","e028b85e":"code","107678e1":"code","5b8c6663":"code","b248583a":"code","71280626":"code","fadd8e15":"code","d54d9c88":"code","8f7c0ee9":"code","4e9d3afc":"code","4be44d60":"code","186a4c7c":"code","ed3872e1":"code","54b88868":"code","0eb6dbd6":"code","b3d091c1":"code","fa7ac011":"code","661ed135":"code","db12527d":"code","5c0587d5":"code","c7798dfd":"code","dab9a11a":"code","207b37d2":"code","b410fc52":"code","5831cf4f":"code","5a0afd46":"code","5be90796":"code","de33f702":"code","88fbc4a3":"code","0b4ca346":"code","bbc102eb":"code","be86a976":"code","23ee461b":"code","4df46821":"code","c09ac828":"code","718fe475":"code","9bb16c75":"code","ba798859":"code","0e86c689":"code","6d34695d":"code","96927f28":"code","83a4d53f":"code","be0bc37e":"code","8afaa52d":"code","7e20b0cd":"code","4850524a":"code","2bb0de25":"code","dbd30d4b":"code","22ab4b54":"markdown","4cd6074a":"markdown"},"source":{"2be39a19":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))","a0113eda":"import warnings\nwarnings.filterwarnings('ignore')\n\nfrom nltk.tokenize import sent_tokenize, word_tokenize, WhitespaceTokenizer\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.probability import FreqDist\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk import pos_tag\nimport string\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom nltk.tokenize import RegexpTokenizer\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# add sentiment anaylsis columns\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n# create doc2vec vector columns\nfrom gensim.test.utils import common_texts\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument","898f05ec":"df_train=pd.read_csv('..\/input\/train.csv')\ndf_test=pd.read_csv('..\/input\/test.csv')\ndf_subm=pd.read_csv('..\/input\/sample_submission.csv')","b5bee82c":"df_train.info()","2dcbffe7":"df_train.sample(5)","f148e348":"df_train.shape, df_test.shape","9376ea3f":"# for i in range(len(df_train.columns)):\n#     if i in [0,2,3,6,7,8,9]:\n#         pass\n#     else:\n#         print(df_train.iloc[:,i].value_counts())","4c247ebb":"df_train.dtypes","cd5b368e":"df_train.isnull().sum()","a67b81ab":"col = ['score_1', 'score_2', 'score_3', 'score_4', 'score_5']\nfor c in col:\n    df_train[c].fillna(df_train[c].dropna().median(), inplace=True)\n    df_test[c].fillna(df_train[c].dropna().median(), inplace=True)\n\ncol1 = ['negatives', 'summary', 'advice_to_mgmt']\nfor c in col1:\n    df_train[c].fillna('', inplace=True)\n    df_test[c].fillna('', inplace=True)","86b6dde4":"df_train.shape, df_test.shape","a0ed659c":"df_train.isnull().sum().any(), df_test.isnull().sum().any()\n","71fbf6c6":"# df_test.isnull().sum()\n# location column has lots of Nan values. lets drop it.","6aa81336":"drop_col = ['ID', 'location', 'date']\ndf_train.drop(columns=drop_col, inplace=True)\ndf_test.drop(columns=drop_col, inplace=True)","b48ed8e1":"df_train.shape, df_test.shape","274709aa":"df_train['Place'].value_counts()","b29baf3e":"OEncoder  = OrdinalEncoder()\nEnc_train = OEncoder.fit_transform(df_train[['Place', 'status']])\nEnc_test  = OEncoder.transform(df_test[['Place', 'status']])","336032d1":"Enc_train[:5]","27914303":"Enc_train.shape, df_train.shape","512e2b22":"# Enc_mapped = map(lambda x: x[0], Enc_train.tolist())\n# print(list(Enc_mapped))","351b3ddb":"def Create_ENC(df, Enc):\n#   Create empty arrays with random elements with dimensions of the encoded column\n    Place_enc = np.empty((len(Enc),))  \n    Status_enc = np.empty((len(Enc),))\n    for i in range(len(Enc)):\n        Place_enc[i] = Enc[i][0]\n        Status_enc[i] = Enc[i][1]\n    df['place_enc'] = Place_enc\n    df['status_enc'] = Status_enc","761c573e":"Create_ENC(df_train, Enc_train)\nCreate_ENC(df_test,  Enc_test)","fb9ec286":"df_train.isnull().sum().any(), df_test.isnull().sum().any()","a0c6d92e":"df_train.sample(5)","99770539":"df_train.groupby('overall').Place.count()","eb739411":"df_train.groupby('Place').overall.count()","60612595":"# df_train.groupby('job_title').overall.count()\n# no information from this","8e950d94":"def Review_len(df):\n    df['len_pos'] = df['positives'].str.len()\n    df['len_neg'] = df['negatives'].str.len()","839eaeb7":"Review_len(df_train)\nReview_len(df_test)","febe341e":"df_train.sample(3)","e20068c8":"def ChangeToInt(df,col):\n    df[col]=df[col].astype('int')","c5d1fc8e":"label='overall'\nChangeToInt(df_train,label)","62b7895b":"def show_wordcloud(data, title = None):\n    V_wordcloud = WordCloud(\n        background_color = 'white',\n        max_words = 200,\n        max_font_size = 40, \n        scale = 3,\n        random_state = 7\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize = (20, 20))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize = 20)\n        fig.subplots_adjust(top = 2.3)\n\n    plt.imshow(V_wordcloud)\n    plt.show()","e028b85e":"# print positive wordcloud\nshow_wordcloud(df_train[\"positives\"])","107678e1":"# print negatives wordcloud\nshow_wordcloud(df_train[\"negatives\"])","5b8c6663":"# print summary wordcloud\nshow_wordcloud(df_train[\"summary\"])","b248583a":"# Get the lemmas of words from wordnet corpus reader\ndef get_wordnet_pos(pos_tag):\n    if pos_tag.startswith('J'):\n        return wordnet.ADJ\n    elif pos_tag.startswith('V'):\n        return wordnet.VERB\n    elif pos_tag.startswith('N'):\n        return wordnet.NOUN\n    elif pos_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN","71280626":"def clean_text(text):\n    # lower text\n    text = text.lower()\n    # tokenize text and remove puncutation\n    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n    # remove words that contain numbers\n    text = [word for word in text if not any(c.isdigit() for c in word)]\n    # remove stop words\n    stop = stopwords.words('english')\n    text = [x for x in text if x not in stop]\n    # remove empty and less than 3 length tokens\n    text = [t for t in text if len(t) >= 3]\n    # pos tag text\n    pos_tags = pos_tag(text)\n    # lemmatize text\n    text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n    # remove words with less than 3 letters\n    text = [t for t in text if len(t) >= 3]\n    # join all\n    text = \" \".join(text)\n    return(text)","fadd8e15":"df_train[\"Clean_positives\"] = df_train[\"positives\"].apply(lambda x: clean_text(x))\ndf_train[\"Clean_negatives\"] = df_train[\"negatives\"].apply(lambda x: clean_text(x))","d54d9c88":"df_test[\"Clean_positives\"] = df_test[\"positives\"].apply(lambda x: clean_text(x))\ndf_test[\"Clean_negatives\"] = df_test[\"negatives\"].apply(lambda x: clean_text(x))","8f7c0ee9":"df_train.shape, df_test.shape","4e9d3afc":"df_train.isnull().sum().any(), df_test.isnull().sum().any()","4be44d60":"df_test.sample(2)","186a4c7c":"df_train[\"Clean_reviews\"] = df_train[\"Clean_positives\"]+' '+df_train[\"Clean_negatives\"]\ndf_test[\"Clean_reviews\"] = df_test[\"Clean_positives\"]+' '+df_test[\"Clean_negatives\"]","ed3872e1":"df_train.head(2)","54b88868":"df_train.isnull().sum().any(), df_test.isnull().sum().any()","0eb6dbd6":"# scores = ['score_1', 'score_2', 'score_3', 'score_4', 'score_5', 'score_6']\n# for col in scores:\n#     print(df_train[col].value_counts())\n# # score_6 column doesn't have uniform realistic values. Ignore this column in analysis","b3d091c1":"def num_words(df):\n    df['num_words_pos'] = df['positives'].apply(lambda x: len(x.split()))\n    df['num_words_neg'] = df['negatives'].apply(lambda x: len(x.split()))","fa7ac011":"num_words(df_train)\nnum_words(df_test)","661ed135":"df_train.columns","db12527d":"def SIAscores(df):\n    SIA = SentimentIntensityAnalyzer()\n    df[\"sentiments\"] = df[\"Clean_reviews\"].apply(lambda x: SIA.polarity_scores(x))\n    return pd.concat([df.drop(['sentiments'], axis=1), df['sentiments'].apply(pd.Series)], axis=1)","5c0587d5":"# SIA.polarity_scores returns a dictionary of 4 scores for each sentence.\n# {'neg': 0.0, 'neu': 0.404, 'pos': 0.596, 'comp': 0.7096}","c7798dfd":"df_train = SIAscores(df_train)\ndf_test = SIAscores(df_test)","dab9a11a":"df_train.shape, df_test.shape","207b37d2":"df_train.columns","b410fc52":"df_test.head(3)","5831cf4f":"dummy_cols=['place_enc','status_enc']\ntrain_d = pd.get_dummies(data=df_train, columns=dummy_cols)\ntest_d  = pd.get_dummies(data=df_test, columns=dummy_cols)","5a0afd46":"len(train_d.columns), len(test_d.columns)","5be90796":"def drop_D2V(df):\n    D2V = [col for col in df.columns if 'D2V_' in col]\n    df.drop(columns=D2V, inplace=True)","de33f702":"drop_D2V(train_d)\ndrop_D2V(test_d)","88fbc4a3":"# create doc2vec vector columns\ndef Create_Doc2Vec(df):\n    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(df[\"Clean_reviews\"].apply(lambda x: x.split(\" \")))]\n    # train a Doc2Vec model with our text data\n    model = Doc2Vec(documents, size=10, window=2, min_count=2, workers=4)\n    # transform each document into a vector data\n    doc2vec_df = df[\"Clean_reviews\"].apply(lambda x: model.infer_vector(x.split(\" \"))).apply(pd.Series)\n    doc2vec_df.columns = [\"D2V_\" + str(x) for x in doc2vec_df.columns]\n    return pd.concat([df, doc2vec_df], axis=1)","0b4ca346":"train_d = Create_Doc2Vec(train_d)\ntest_d  = Create_Doc2Vec(test_d)","bbc102eb":"# tfidf = TfidfVectorizer(min_df = 3)\n# tfidf_train = tfidf.fit_transform(df_train[\"Clean_reviews\"]).toarray()\n# tfidf_test  = tfidf.transform(df_test[\"Clean_reviews\"]).toarray()\n\n# def Create_TFIDF(df, tfidf_result):\n#     tfidf_df = pd.DataFrame(tfidf_result, columns = tfidf.get_feature_names())\n#     tfidf_df.columns = [\"word_\" + str(x) for x in tfidf_df.columns]\n#     tfidf_df.index = df.index\n#     return pd.concat([df, tfidf_df], axis=1)","be86a976":"# df_train = Create_TFIDF(df_train, tfidf_train)\n# df_test  = Create_TFIDF(df_test,  tfidf_test)","23ee461b":"train_d.columns","4df46821":"len(train_d.columns)","c09ac828":"# feature selection\nlabel = \"overall\"\nignore_cols = [label, 'Place', 'status', 'job_title', 'summary', 'positives', 'negatives', 'advice_to_mgmt', \n               'score_6', 'Clean_positives', 'Clean_negatives', 'Clean_reviews']\nfeatures = [c for c in train_d.columns if c not in ignore_cols]","718fe475":"len(features), train_d.shape, test_d.shape","9bb16c75":"X = train_d[features]\ny = train_d[label]\nX_test = test_d[features]","ba798859":"X.shape, y.shape, X_test.shape","0e86c689":"# del df_train, df_test\n# # , tfidf, tfidf_test, tfidf_train\n# # deleted to save RAM\n# import gc\n# gc.collect()","6d34695d":"# who_ls","96927f28":"def evaluate(model, train_x, val_x, train_y, val_y):\n    model.fit(train_x,train_y)\n    pred_y = model.predict(val_x)\n    train_acc = model.score(train_x, train_y)\n    test_acc = accuracy_score(val_y, pred_y)\n    f1_sc = f1_score(val_y, pred_y, average='weighted')\n    return train_acc, test_acc, f1_sc","83a4d53f":"# StratifiedKFold and Model Training & Evaluation\nkfold = 6\nskf = StratifiedKFold(n_splits=kfold,shuffle=True,random_state=7)\nmodels = [\n          LogisticRegression(n_jobs=-1, random_state=6), \n          XGBClassifier(random_state=5, n_jobs=-1),\n          ExtraTreesClassifier(random_state=97, n_estimators=100, n_jobs=-1),\n          LGBMClassifier(objective='multiclass', random_state=5)\n         ]\nscores_df = pd.DataFrame(index=range(kfold * len(models)))\ndf_row = []\nfor i, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n    X_train, X_val = X.loc[train_idx], X.loc[test_idx]\n    y_train, y_val = y.loc[train_idx], y.loc[test_idx]\n    print('[Fold: {}\/{}]'.format(i + 1, kfold))\n    for model in models:\n        model_name = model.__class__.__name__\n        trn, acc, f1 = evaluate(model, X_train, X_val, y_train, y_val)\n        df_row.append((model_name, i, f1, acc, trn))\n        \nprint('Training Done!')\n\n#SVM model\n#     model_SVC = LinearSVC(random_state=7)\n# worse metric evaluation for SVM. Do not use it. \n    ","be0bc37e":"scores_df = pd.DataFrame(df_row, columns=['model_name', 'fold_idx', 'F1_score', 'Test_acc', 'Train_acc'])\nscores_df.sort_values(by=['model_name', 'fold_idx'], inplace=True)\nscores_df.reset_index(drop=True, inplace=True)","8afaa52d":"scores_df","7e20b0cd":"scores_df.groupby(['model_name'])['F1_score'].mean()","4850524a":"models = [ LogisticRegression(n_jobs=-1, random_state=0), XGBClassifier(random_state=5, n_jobs=-1) ]\nCV = 5\ncv_df = pd.DataFrame(index=range(CV * len(models)))\nentries = []\nfor model in models:\n    model_name = model.__class__.__name__\n    f1_scores = cross_val_score(model, X, y, scoring='f1_weighted', cv=CV)\n    for cv_idx, f1 in enumerate(f1_scores):\n        entries.append((model_name, cv_idx, f1))\n\ncv_df = pd.DataFrame(entries, columns=['model_name', 'cv_idx', 'F1_score'])","2bb0de25":"cv_df","dbd30d4b":"cv_df.groupby(['model_name'])['F1_score'].mean()","22ab4b54":"> References: \n> > https:\/\/towardsdatascience.com\/detecting-bad-customer-reviews-with-nlp-d8b36134dc7e\n> > https:\/\/medium.com\/@mishra.thedeepak\/doc2vec-simple-implementation-example-df2afbbfbad5\n> > https:\/\/towardsdatascience.com\/mapping-word-embeddings-with-word2vec-99a799dc9695\n> > https:\/\/medium.com\/explore-artificial-intelligence\/word2vec-a-baby-step-in-deep-learning-but-a-giant-leap-towards-natural-language-processing-40fe4e8602ba\n> > https:\/\/blog.insightdatascience.com\/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e\n> > https:\/\/blog.insightdatascience.com\/using-nlp-to-gain-insights-from-employee-review-data-da15687f311a","4cd6074a":"Default parameters LogReg:\n>     Avg_CV_score: 0.4040081841492168, Avg_F1_score: 0.3763991552269393\n\nDefault parameters XGB:\n>     Avg_CV_score: 0.41343607919726366, Avg_F1_score: 0.39661000929134393\n\nDefault parameters DTC:\n>     Avg_CV_score: 0.3374539112115183, Avg_F1_score: 0.3376255378164512"}}