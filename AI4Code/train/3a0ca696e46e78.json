{"cell_type":{"6fa158f6":"code","0862eab4":"code","b9896e9c":"code","ca4b704b":"code","1fe9180b":"code","5078e33d":"code","52f416cd":"code","eca8f040":"code","35094d72":"code","e7cee882":"code","3ce3cae5":"code","ed50e65a":"code","c75dccc8":"code","65a1f27f":"code","26c8ae87":"code","cb0b570c":"code","e862dec0":"code","65d75590":"code","9afdb655":"code","7f0f058c":"markdown","539cb35f":"markdown","4eb68908":"markdown","bf80e5f4":"markdown","45e67099":"markdown","e9f3874b":"markdown","77cb3b70":"markdown","d8fc295e":"markdown","7e4182ff":"markdown","1ef22aec":"markdown","cf937c60":"markdown","74892dd1":"markdown"},"source":{"6fa158f6":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor","0862eab4":"# Read the data\nX_full = pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col='id')\nX_test_full = pd.read_csv('..\/input\/30-days-of-ml\/test.csv', index_col='id')","b9896e9c":"X_full.shape","ca4b704b":"X_full.columns","1fe9180b":"X_full.info()","5078e33d":"X_full.describe()","52f416cd":"X_full.head(4)","eca8f040":"X_full.isnull().values.any()","35094d72":"X_full.select_dtypes(['object']).nunique()","e7cee882":"# since it is good practice to choose column with low cardinality\n# we will drop columns with cardianlity greater than 10\nX_full.drop(['cat9'], axis=1, inplace=True)\n","3ce3cae5":"# also drop duplicates \nX_full.drop_duplicates(inplace=True)","ed50e65a":"#removes the rows with missing target\nX_full.dropna(axis=0, subset=['target'], inplace=True)\n\n#remove the target column and seprate the dataset into training and validation\ny = X_full.target\nX_full.drop(['target'], axis=1, inplace=True)","c75dccc8":"# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, \n                                                                train_size=0.8, test_size=0.2,\n                                                                random_state=0)","65a1f27f":"# Select categorical columns\ncat_cols = [cname for cname in X_train_full.columns if \n           X_train_full[cname].dtype == 'object']\n\n# Select numerical columns\nnum_cols = [cname for cname in X_train_full.columns if \n           X_train_full[cname].dtype == 'float64']\n\n# keep selected columns only\nmy_cols = cat_cols + num_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()","26c8ae87":"X_train.head()","cb0b570c":"# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='mean')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, num_cols),\n        ('cat', categorical_transformer, cat_cols)\n    ])\n","e862dec0":"model = XGBRegressor(n_estimators = 300, learning_rate = 0.1)","65d75590":"# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\n\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = my_pipeline.predict(X_valid)\n\n# Evaluate the model\nscore = mean_absolute_error(y_valid, preds)\nprint('MAE:', score)","9afdb655":"# Preprocessing of test data, fit model\npredictions_test = my_pipeline.predict(X_test) \n\n# Save test predictions to file\noutput = pd.DataFrame({'id': X_test.index,\n                       'target': predictions_test})\noutput.to_csv('submission.csv', index=False)","7f0f058c":"## Importing libraries","539cb35f":"**1. first checking for null values in dataset**","4eb68908":"## Preparing the pipeline\n**Step 1. Define the preprocessing steps**\n\nWe use the ColumnTransformer class to bundle together different preprocessing steps. The code below:\n\n* imputes missing values in numerical data\n* imputes missing values and applies a ordinal encoding to categorical data.","bf80e5f4":"# XGBoost 30 days ml\nThis notebook contains my working on the **30DaysMLChallenge dataset**. The dataset consists of total 500000 rows (train = 300000, test = 200000) and 26 columns. The first column is the 'id' columns, the next 10 columns (cat0 - cat9) are categorical columns, which is followed by 14 numerical columns (cont0 - cont13), and the last column is the target column. The goal is to predict the continous target column.\n\nI will be applying the techniques which I have learnt from Kaggle courses.\n\nIf you like my work please give an upvote. Thanks","45e67099":"## Loading Data","e9f3874b":"**Step 3. Create and Evaluate the pipeline**\n\nFinally, we use the Pipeline class to define a pipeline that bundles the preprocessing and modeling steps.","77cb3b70":"## Preparing the Data for Model building and pipeline","d8fc295e":"## Generate test predictions\nNow, we will use our trained model to generate predictions with the test data","7e4182ff":"## Analyzing Data","1ef22aec":"## Data Preprossing Analysis\nIn order for applying preprossing we need to do some preprocessing analysis","cf937c60":"**2. Checking for cardinality in catecorical columns**","74892dd1":"**Step 2. Define the model**\n\nWe are using XGBoost Regressor to train our model. The values for the model are given on trial and error bases.  The optimal hyperparameters can be find using SKlearn GridSearchCV."}}