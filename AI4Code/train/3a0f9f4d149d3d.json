{"cell_type":{"6385e26b":"code","71500d54":"code","57c490f9":"code","9b4be274":"code","4a0cab35":"code","90e95891":"code","34cf3905":"code","8a6851c4":"code","f47bf623":"code","8c440744":"code","b5f40578":"code","5229ab94":"code","0c2bc34b":"code","4a91dfec":"code","89fb2904":"code","c3c31bbb":"code","7feea868":"code","18c93838":"code","77558125":"code","05f302f2":"code","43994c3f":"code","0f7c2758":"code","808a5f83":"code","055876d2":"code","bfea1030":"code","e4696404":"markdown","e3744bb5":"markdown","964ddb59":"markdown","f1f39efa":"markdown"},"source":{"6385e26b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV\nfrom xgboost import XGBRegressor\nfrom scipy.stats import randint, uniform","71500d54":"# Read the data\npath = '..\/input\/house-prices-advanced-regression-techniques\/'\ntrain_data = pd.read_csv(path + 'train.csv', index_col='Id')\ntest_data = pd.read_csv(path + 'test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\ntrain_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = np.log(train_data.SalePrice) # evaluation is done on log of SalePrice              \ntrain_data.drop(['SalePrice'], axis=1, inplace=True)\n\n# Use numerical columns only\nnumeric_cols = [col for col in train_data.columns if train_data[col].dtype in ['int64', 'float64']]\nX = train_data[numeric_cols].copy()\nX_test = test_data[numeric_cols].copy()\n\nprint(X.shape)\nprint(X_test.shape)","57c490f9":"SEED = 1 #to reproduce results","9b4be274":"# Select scoring metrics in line with competitions evaluation metrics\n# Since we already took log of prices above we can use simple mean squared error metrics\nscorer = 'neg_mean_squared_error'\nEVAL_METRIC = 'rmse'","4a0cab35":"# Before tuning we calc the cross-val RMSE for the untuned model\n# This serves as our baseline for comparison with the tuned models\nscores = -1 * cross_val_score(XGBRegressor(random_state = SEED), X, y,\n                              cv=5,\n                              scoring=scorer)\n\nprint(\"Untuned-model 5-fold cross-validation average RMSE score:\", np.sqrt(scores.mean()))","90e95891":"###  Parameter Tuning\n# Let's set up a dictionary to keep track of our tuned parameters\ntuned_params = {}\n# Set up train and validation set which is needed for early stopping \nfrom sklearn.model_selection import train_test_split\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state = SEED)","34cf3905":"# Step 1 - pre tune n_estimators and learning rate","8a6851c4":"# We use a large number for n_estimators and try a number of learning_rates to pick an initial rate\n# This can be done quickly by using early_stopping_rounds\nN_ESTIMATORS = 2000\nlearning_rates = [0.01,0.05,0.1,0.2,0.3]\n\nbest_learning_rate = learning_rates[0]\nbest_score = 1000 #just need something large\nfor lr in learning_rates:\n    model_lr = XGBRegressor(n_jobs = -1, n_estimators=N_ESTIMATORS, learning_rate=lr, random_state = SEED)\n    model_lr.fit(train_X, train_y, early_stopping_rounds=20, eval_metric = [EVAL_METRIC],\n             eval_set = [(train_X, train_y), (val_X, val_y)], verbose=False)\n    n_estimators_best = model_lr.best_ntree_limit\n    if model_lr.best_score < best_score:\n        best_score = model_lr.best_score\n        best_learning_rate = lr\n    print('For learning rate ' + str(lr)+ ' the optimal n_estimators is ' + str(n_estimators_best) \n          + ' with simple cross-validation set RMSE score ' + str(model_lr.best_score))\n\nprint('Select initial learning rate ' + str(best_learning_rate))\ntuned_params['learning_rate'] = best_learning_rate","f47bf623":"# We create a helper function to plot early_stopping results\ndef plot_test_train(model):\n    eval_results = model.evals_result()\n    epochs = len(eval_results['validation_0'][EVAL_METRIC])\n    x_axis = range(0, epochs)\n    # plot log loss\n    fig, ax = plt.subplots(figsize=(10,5))\n    ax.plot(x_axis, eval_results['validation_0'][EVAL_METRIC], label='Train')\n    ax.plot(x_axis, eval_results['validation_1'][EVAL_METRIC], label='Test')\n    ax.legend()\n    plt.ylabel('rmse')\n    plt.title('XGBoost rmse')\n    plt.show()\n    n_estimators_best = model.best_ntree_limit\n    print('Optimal n_estimators ' + str(n_estimators_best))\n    print('Simple cross-validation set RMSE score for optimal n_estimators ' + str(model.best_score))","8c440744":"# Let's run the model with our selected initial learning rate again and plot the results of cross-valuation\n# Define and fit the model\nmy_model = XGBRegressor(n_estimators=N_ESTIMATORS,\n                        learning_rate=tuned_params['learning_rate'], random_state = SEED)\nmy_model.fit(train_X, train_y, early_stopping_rounds=20, eval_metric = [EVAL_METRIC],\n             eval_set = [(train_X, train_y), (val_X, val_y)], verbose=False)\n\n# Plot cross-val results and save tuned parameter to our dictionary\nplot_test_train(my_model)\ntuned_params['n_estimators'] = my_model.best_ntree_limit\n\n# Let's also calculate 5-fold cross validation RMSE score \n# (this will be a bit more precise than the simple cross-validation score from early_stopping and better for comparison of the tuned model at different stages)\nscores = -1 * cross_val_score(XGBRegressor(n_estimators = tuned_params['n_estimators'],\n                                           learning_rate = tuned_params['learning_rate'],\n                                           random_state = SEED), X, y,\n                              cv=5,\n                              scoring=scorer)\n\nprint(\"5-fold cross-validation average RMSE score:\", np.sqrt(scores.mean()))","b5f40578":"# This is already an improvement on our un-tuned model base score","5229ab94":"# Step 2 - Use GridSearch to tune tree parameters max_depth and min_child_weight ","0c2bc34b":"# Helper function to display results of Gridsearch\ndef grid_heatmap(gridsearch, param1, param2):\n    grid_df = pd.DataFrame(gridsearch.cv_results_)\n    pivot = grid_df.pivot(index='param_'+param1, columns='param_'+ param2, values='mean_test_score')\n    sns.set(rc={'figure.figsize':(10,5)})\n    sns.heatmap(pivot * -1, linewidths = 0.5)\n    # optimal parameters\n    print('optimal ' + str(param1) + ' '+ str(gridsearch.best_params_[param1]))\n    print('optimal ' + str(param2) + ' ' + str(gridsearch.best_params_[param2]))\n    print(\"5-fold cross-validation average RMSE score:\",np.sqrt( gridsearch.best_score_ * -1))","4a91dfec":"params = {\n    'max_depth':range(2,10,1),\n    'min_child_weight':range(1,6,1)\n}\n# estimator = XGBRegressor(n_estimators = n_estimators_best , learning_rate = LEARNING_RATE, random_state = SEED)\nestimator = XGBRegressor(n_estimators = tuned_params['n_estimators'] ,\n                         learning_rate = tuned_params['learning_rate'],\n                         random_state = SEED)\n\n\ngridsearch = GridSearchCV(estimator,\n                          param_grid = params,\n                          scoring=scorer,\n                          cv=5)\ngridsearch.fit(X,y)\n# display results in heatmap\ngrid_heatmap(gridsearch,'max_depth','min_child_weight')\n# Save optimal parameters and estimator\ntuned_params['max_depth'] = gridsearch.best_params_['max_depth']\ntuned_params['min_child_weight'] = gridsearch.best_params_['min_child_weight']\nbest_estimator_2 = gridsearch.best_estimator_","89fb2904":"# The optimal combination is in the middle of our grid. \n# If you find it on the border you might want to shift\/extend your grid to ensure it covers the optimum","c3c31bbb":"# Step 3 - Tune Gamma","7feea868":"params_gamma = {\n 'gamma':[i\/10.0 for i in range(0,5)]\n}\n\ngridsearch_gamma = GridSearchCV(best_estimator_2,\n                          param_grid = params_gamma,\n                          scoring=scorer,\n                          cv=5)\ngridsearch_gamma.fit(X,y)\n\n# Display result\nsns.set(rc={'figure.figsize':(10,1)})\nsns.heatmap(pd.DataFrame(np.sqrt(gridsearch_gamma.cv_results_['mean_test_score'] * -1), index = params_gamma['gamma']).transpose(),\n           cbar=False, linewidths = 0.5, annot = True, fmt=\".3%\")\nprint('optimal gamma ' + str(gridsearch_gamma.best_params_['gamma']))\nprint(\"5-fold cross-validation average RMSE score:\", np.sqrt(gridsearch_gamma.best_score_ * -1))\n# Save optimal gamma to dictionary\ntuned_params['gamma'] = gridsearch_gamma.best_params_['gamma']","18c93838":"# Step 4: Tune subsample and colsample_bytree","77558125":"params_4 = {\n    'subsample':[i\/10.0 for i in range(5,11)],\n    'colsample_bytree':[i\/10.0 for i in range(3,8)]\n}\n\nestimator_4 = XGBRegressor(n_estimators=tuned_params['n_estimators'],\n                          learning_rate=tuned_params['learning_rate'],\n                          max_depth = tuned_params['max_depth'],\n                          min_child_weight = tuned_params['min_child_weight'],\n                          gamma = tuned_params['gamma'],\n                          random_state = SEED)\n\ngridsearch_4 = GridSearchCV(estimator_4,\n                          param_grid = params_4,\n                          scoring=scorer,\n                          cv=5)\ngridsearch_4.fit(X,y)\n\n# Display results of Gridsearch\ngrid_heatmap(gridsearch_4,'subsample','colsample_bytree')\n# Save optimal parameters and estimator\ntuned_params['subsample'] = gridsearch_4.best_params_['subsample']\ntuned_params['colsample_bytree'] = gridsearch_4.best_params_['colsample_bytree']","05f302f2":"# Step 5- Tune regularisation parameter lambda","43994c3f":"params_lambda = {\n    'lambda':[i\/10.0 for i in range(7,16)],\n}\n\nestimator_lambda = XGBRegressor(n_estimators=tuned_params['n_estimators'],\n                                learning_rate=tuned_params['learning_rate'],\n                                max_depth = tuned_params['max_depth'],\n                                min_child_weight = tuned_params['min_child_weight'],\n                                gamma = tuned_params['gamma'],\n                                subsample = tuned_params['subsample'],\n                                colsample_bytree = tuned_params['colsample_bytree'],\n                                random_state = SEED)\n\ngridsearch_lambda = GridSearchCV(estimator_lambda,\n                          param_grid = params_lambda,\n                          scoring=scorer,\n                          cv=5)\ngridsearch_lambda.fit(X,y)\n\n# Display result\nsns.set(rc={'figure.figsize':(10,1)})\nsns.heatmap(pd.DataFrame(np.sqrt(gridsearch_lambda.cv_results_['mean_test_score'] * -1), index = params_lambda['lambda']).transpose(),\n           cbar=False, linewidths = 0.5, annot = True, fmt=\".3%\")\nprint('Optimal lambda ' + str(gridsearch_lambda.best_params_['lambda']))\nprint(\"5-fold cross-validation average RMSE score:\", np.sqrt(gridsearch_lambda.best_score_ * -1))\n# Save optimal lambda parameter\ntuned_params['lambda'] = gridsearch_lambda.best_params_['lambda']","0f7c2758":"# At this point you can check if re-tuning learning_rate and n_estimators improves performance by repeating step 1 for the tuned paramters.\n# I found that it didnt improve performance here so I skipped it","808a5f83":"# This is our final model:\nn_estimators_best_final = n_estimators_best\nmy_model_best = XGBRegressor(n_estimators=tuned_params['n_estimators'],\n                              learning_rate= tuned_params['learning_rate'],\n                              max_depth = tuned_params['max_depth'],\n                              min_child_weight = tuned_params['min_child_weight'],\n                              gamma = tuned_params['gamma'],\n                              subsample = tuned_params['subsample'],\n                              colsample_bytree = tuned_params['colsample_bytree'],\n                              reg_lambda = tuned_params['lambda'],\n                              random_state = SEED)\n\nmy_model_best.fit(X, y, verbose=False)\nscores_final = -1 * cross_val_score(my_model_best, X, y,\n                              cv=5,\n                              scoring=scorer)\n\nprint(\"Final model 5-fold cross-validation average MSE score:\", np.sqrt(scores_final.mean()))","055876d2":"# Now we compare our results to RandomizedSearchCV\n# For RandomizedSearchCV we provide a distributon for each tuning parameter\n# RandomizedSearchCV samples values from these distributions for each fit\n#estimator = xgb.XGBRegressor()\n\nparam_dist = {'n_estimators': randint(100, 1000),\n              'learning_rate': uniform(0.01,0.3), \n              'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n              'min_child_weight': [1, 2, 3, 4, 5, 6],\n              'gamma' : uniform(0.0,0.4),\n              'subsample': uniform(0.5, 1),\n              'colsample_bytree': uniform(0.5,1),\n              'reg_lambda' : uniform(0.7,1.5)\n             }\n\n\nclf = RandomizedSearchCV(XGBRegressor(tree_method='gpu_hist'), \n                         param_distributions = param_dist,\n                         cv = KFold(),  \n                         n_iter = 120, \n                         scoring = scorer, \n                         verbose = 3, \n                         random_state = SEED,\n                         n_jobs = -1)\nclf.fit(X,y)\n\nprint(\"Final model 5-fold cross-validation average MSE score:\", np.sqrt(clf.best_score_ *-1 )) \nclf.best_params_","bfea1030":"# Compare to iterative results\ntuned_params","e4696404":"For simplicity, we only use numerical columns. There's of course a lot of information in the categorical features and a consdiderable amount of feature engineering to do, tuning alone won't do the job :)","e3744bb5":"# The Dark Art of Parameter Tuning\n## XGBoost for house price prediction  \n### \"Parameter tuning is a dark art in machine learning, the optimal parameters of a model can depend on many scenarios. So it is impossible to create a comprehensive guide for doing so.\" \n  \nI came across this quote when reading the **XGBoost** documentation. While this might not be the most encouring statement I decided to give it a shot anyway.\n\nLet's begin by looking at some of the tuning parameters. You can find the full list of parameters here: https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html#general-parameters  \n\n-  **n_estimators** [default = 100, alias: num_round]  Number of gradient boosted trees. Equivalent to number of boosting rounds\n\n- **learning_rate** [default=0.3, alias: eta]: Step size shrinkage used in update, prevents overfitting. After each boosting step, we can directly get the weights of new features, and learning_rate shrinks the feature weights to make the boosting process more conservative. range: [0,1]\n\nParameters **max_depth**, **min_child_weight** and **gamma** control model complexity and can prevent overfitting:\n- **max_depth** [default=6]: Maximum depth of each base learner tree. Increasing this value will make the model more complex and more likely to overfit. range: (0,\u221e]\n\n- **min_child_weight** [default=1]: If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression task, this simply corresponds to minimum number of instances needed to be in each node. The larger min_child_weight is, the more conservative the algorithm will be. range: [0,\u221e]\n\n- **gamma** [default=0, alias: min_split_loss]: Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be. range: [0,\u221e]\n\nParameters like **subsample** and **colsample_bytree** control randomness and make training more robust to noise:\n- **subsample** [default=1]: Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees, and this will prevent overfitting. range: (0,1]\n\n- **colsample_bytree** [default=1] : colsample_bytree is the subsample ratio of columns when constructing each tree. range: (0,1]\n\nParameters **lambda** and **alpha** are  model regularisation parameters, equivalent to Ridge and Lasso for linear regression:\n- **lambda** [default=1, alias: reg_lambda]: L2 regularization term on weights. Increasing this value will make model more conservative.\n\n- **alpha** [default=0, alias: reg_alpha]:  L1 regularization term on weights. Increasing this value will make model more conservative.","964ddb59":"XGBoost works with **GridSearchCV**. Thus, one way to calibrate parameters is to run a massive grid search with all possible combinations of parameters. Depending on your problem this could take a very long time. Instead we will run an iterative process where we tune a small set of parameters at a time. We begin with the ones we think have the largest impact and then fine tune further. This also has the advantage that we get a better understanding of how parameters interact. We can run small grid searches and zone in on the optimal parameter combinations.\n\nOur approach is as follows:\n- Step 1: Tune **n_estimators\/num_round** for a given learning rate. This can be done quickly by using early_stopping_rounds\n- Step 2: Run GridSearchCV for **max_depth**, **min_child_weight** for model complexity\n- Step 3: Run GridSearchCV for **gamma**\n- Step 4: Increase robustness by tuning **subsample** and **colsample_bytree** with GridSearchCV\n- Step 5: Tune the regularisation parameter **lambda** \n\nWe compare this with the results from **RandomizedSearchCV**. We see that in this case the iterative approach and the randomised search lead to similar Cross-Val scores (with the iterative process performing slightly better) which should make us fairly confident that we are on the right path.","f1f39efa":"We see that the headline cross-val score for both, our iterative process and the randomized cross-valuation, is a substantial improvement upon the base case (untuned model). It's interesting that the randomized search finds parameters with a much larger number of estimators (505 vs 207) which seems to be compensated by less complexity for the base learner trees (max_depth = 2 vs 4) and more model regularisation (lamba = 1.6 vs 1.0)."}}