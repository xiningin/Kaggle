{"cell_type":{"8bc010b1":"code","bb14d696":"code","cb861965":"code","b1ce4542":"code","64254e37":"code","79f571f8":"code","19c0bf77":"code","cb13e78f":"code","01f2fdda":"code","f82dccb0":"code","29b5a882":"code","3a4e9392":"code","8d7b3f0d":"code","e5e517d7":"code","ae47522f":"code","bb9f2796":"code","1eb75e0e":"code","2911962f":"code","825d2f1b":"code","fdf8f002":"code","d4e3c393":"code","31c32879":"code","fb555ff5":"code","0ce0c128":"code","55e4f82d":"code","caddfdd2":"code","c1ed1264":"code","3b3e06b4":"code","64c15bda":"code","605dccfc":"code","e9a89a22":"code","cd1181cb":"code","d23df289":"code","cb4b7bb8":"markdown"},"source":{"8bc010b1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # \nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bb14d696":"fname = os.path.join(dirname, filename)\n# Load dataframe\ndf = pd.read_csv(fname)","cb861965":"df.head()","b1ce4542":"df.duplicated().sum()\ndf.drop_duplicates(inplace=True)","64254e37":"len(df)","79f571f8":"df['Outcome'].value_counts()","19c0bf77":"# train, validate, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n# Split the data\nX = df.drop('Outcome', axis=1)\ny = df['Outcome']\n\n# Setup random seed\nnp.random.seed(42)\n\n# Train and test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Instantiate classifier\nclf = RandomForestClassifier(n_jobs=1)\n\n# Fit the data\nclf.fit(X_train, y_train)\n\n# Score the data\nclf_score1 = clf.score(X_test, y_test)\nprint(f\"The score is: {clf_score1:.6f}%\")","cb13e78f":"np.random.seed(42)\n\n# Cross_val_score mean\ncr_val_score = np.mean(cross_val_score(clf, X_test, y_test))\ncr_val_score","01f2fdda":"y_preds_proba = clf.predict_proba(X_test)","f82dccb0":"proba_df = pd.DataFrame(y_preds_proba,\n                       columns=['0', '1'])\nproba_df","29b5a882":"y_test[:5]","3a4e9392":"grid = {\n       'n_estimators': list(range(530,571,10)),\n       'criterion': ['gini', 'entropy'],\n       #'max_depth': [None, 3, 10, 20],\n       'min_samples_split': [3],\n       #'min_samples_leaf': [1, 4, 8],\n       #'max_features': ['auto', 'sqrt'],\n       }\n\nclf_2 = GridSearchCV(clf, grid)\nclf_2.fit(X_train, y_train)","8d7b3f0d":"gs_mean_scores = clf_2.cv_results_['mean_test_score']\ngs_mean_scores","e5e517d7":"gs_params = clf_2.cv_results_['params']\nparam_df = pd.DataFrame(gs_params)\nparam_df['Mean Score'] = gs_mean_scores","ae47522f":"param_df","bb9f2796":"from pandas.plotting import parallel_coordinates\n\nplt.style.use(\"seaborn\")\nfig = plt.figure(figsize=(10, 6))\ntitle = fig.suptitle(\"Parallel Coordinates\", fontsize=18)\nfig.subplots_adjust(top=0.93, wspace=0)\n\npc = parallel_coordinates(param_df, 'Mean Score', color=('skyblue', 'firebrick'))","1eb75e0e":"clf = RandomForestClassifier(n_estimators=500, n_jobs=1)\n\nclf.fit(X_train, y_train)\n\nclf_score = clf.score(X_test, y_test)\nprint(f\"The best score possible is:  {clf_score:.6f}%\")","2911962f":"#plt.style.available\nprint(param_df.columns)\n","825d2f1b":"import matplotlib.pyplot as plt\nplt.style.use('classic')\n%matplotlib inline\n\n#sns.kdeplot(param_df)\nsns.set()\n\n# for col in ['min_samples_split', 'n_estimators']:\n#     #sns.kdeplot(param_df[col], shade=True)\n#     plt.plot(param_df[col],param_df['Mean Score'] )\n    \n# plt.legend('ABCDEF', ncol=2, loc='upper left');\n\n\n\n#sns.kdeplot(param_df);\n\n# with sns.axes_style('white'):\n#     sns.jointplot(\"Mean Score\", param_df, kind='hex')\n\nprint(param_df.head())\n#mappy = sns.load_dataset('flights')\nmappy = param_df.copy()\nmappy = mappy.drop('min_samples_split', axis=1)\nprint(mappy.head())\nmappy = mappy.pivot(\"criterion\", \"n_estimators\", \"Mean Score\")\n#sns.heatmap(param_df)\nax = sns.heatmap(mappy, cmap='Blues', annot=True)","fdf8f002":"sns.palplot(sns.light_palette(\"navy\", reverse=True))","d4e3c393":"# Testing RandomForestClassifiers over a range of n_estimators\n\nmax = 0\nfor est in range(100,700,100):\n    clf = RandomForestClassifier(n_estimators=est, n_jobs=1)\n\n    clf = RandomForestClassifier(\n                      # min_samples_leaf=2,\n                      min_samples_split = 4,        \n                      n_estimators=est,\n                      bootstrap=True,\n                      n_jobs=1,\n                      random_state=42,\n                      max_features='auto')\n    \n    clf.fit(X_train, y_train)\n\n    clf_score = clf.score(X_test, y_test)\n    if clf_score > max:\n        max = clf_score\n        print(f\"New max with estimators={est}, the best score possible is:  {clf_score:.6f}%\")\n    else:\n        print('.', end=\"\")\nprint(f'\\nFinished.')\n        ","31c32879":"# Testing XGBoost over a range of n_estimators\n\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\n\nlowest = 100\nfor est in range(100, 700, 100):\n    xg_reg = xgb.XGBClassifier(max_depth=10, learning_rate=0.15, n_estimators=est, verbosity=0,\n                              silent=True, objective='binary:logistic', booster='gbtree')\n\n    xg_reg.fit(X_train,y_train)\n    preds = xg_reg.predict(X_test)\n    rmse = np.sqrt(mean_squared_error(y_test, preds))\n    \n    if rmse < lowest:\n        lowest = rmse\n        print(f\"\\nNew low with estimators={est}, rmse:  {rmse:.6f}%\")\n        \n        preds[preds>0.5] = \"1\"\n        pred_y = preds==y_test\n        print(f'Percent Correct: {100*pred_y.mean()}%')        \n        \n    else:\n        print('.', end=\"\")\nprint(f'\\nFinished.')    \n    ","fb555ff5":"df.head()","0ce0c128":"df[df['Pregnancies'] == 3].count()","55e4f82d":"g = sns.distplot(df['Pregnancies'], color='purple', bins=30)\ng.set(xlim=(0, 17));","caddfdd2":"# Split the data into neg and pos diabetes\ndiabetes_neg = df[df['Outcome'] == 0]\ndiabetes_pos = df[df['Outcome'] == 1]","c1ed1264":"# Plot the neg and pos diabetes in a stacked bar chart\nfig, ax = plt.subplots()\n\nax.bar(diabetes_neg['Pregnancies'], diabetes_neg['Outcome'], label='Negative', width=0.35)\n#ax.bar(diabetes_pos['Outcome'], diabetes_pos['Pregnancies'], label='Positive')\nax.legend()\nplt.show()","3b3e06b4":"df[:5]","64c15bda":"df['Outcome'].value_counts()","605dccfc":"491 \/ (491 + 253)","e9a89a22":"for i, col in enumerate(df.columns):\n    if col == 'Outcome':\n        continue\n    plt.figure(i)\n    positives = df[col][df['Outcome']==1]\n    negatives = df[col][df['Outcome']==0]\n    sns.distplot(positives, hist = False, color=\"red\", label=\"Positive\")\n    sns.distplot(negatives, hist = False, color=\"skyblue\", label=\"Negative\")","cd1181cb":"## All in one cell\n\nnp.random.seed(42)\n\n# Split data\nX = df.drop('Outcome', axis=1)\ny = df['Outcome']\n\n# Train and test splits\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Instantiate the model\nclf = RandomForestClassifier(n_jobs=-1)\n\n# Grid for RandomSearchCV\n\ngrid = {\n       'n_estimators': list(range(100, 801, 100)),\n       'criterion': ['gini', 'entropy'],\n       'max_depth': [None, 3, 10, 20],\n       'min_samples_split': [3],\n       'min_samples_leaf': [1, 4, 8],\n       'max_features': ['auto', 'sqrt'],\n       }\n\nrs_clf = GridSearchCV(clf, grid)\nrs_clf.fit(X_train, y_train)\nrs_clf.score(X_test, y_test)","d23df289":"rs_clf.best_params_","cb4b7bb8":"**Objective**:\nCreate model with minimum error on test set ( competition or local)\n\n**Method**:\n\nA. Test & Compare non-Tensor model\n* Load Data\n* Split Data\n* Do initial run with XGBoost classifier. Record Results\n* Graph data with Seaborn\/scipltlib\n* Engineer features\n* Retest\n\nB. Create Tensor & Compare."}}