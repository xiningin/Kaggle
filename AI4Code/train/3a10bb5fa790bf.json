{"cell_type":{"cd383a98":"code","3d6d4b48":"code","e8d3c7e4":"code","a726ce95":"code","daba1b48":"code","27e7246c":"code","295639ee":"code","233fb084":"code","ec972fc2":"code","746d3195":"code","fc3e5b94":"code","b99e8837":"code","f891e7ae":"code","5faa7b2e":"code","fa44ac0d":"code","f2991879":"code","629e146c":"code","791bc940":"markdown","51914de3":"markdown","e0d34894":"markdown","8a63086f":"markdown"},"source":{"cd383a98":"\nimport numpy as np \nimport pandas as pd \nimport os\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import MiniBatchKMeans\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nfrom tqdm.notebook import tqdm\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","3d6d4b48":"torch.cuda.empty_cache()","e8d3c7e4":"if torch.cuda.is_available():\n    print(\"GPU available\")\n    device = \"cuda:0\"\nelse:\n    print(\"CPU available\")\n    device = \"cpu\"\n    \ndevice = torch.device(device)\nprint(device)\n\n\n","a726ce95":"data_dir = os.path.join(\"\/kaggle\", \"input\", \"cityscapes-image-pairs\", \"cityscapes_data\")\ntrain_dir = os.path.join(data_dir, \"train\") \nval_dir = os.path.join(data_dir, \"val\")\ntrain_fns = os.listdir(train_dir)\nval_fns = os.listdir(val_dir)\n\nprint(\"train patch size:\", len(train_fns))\nprint(\"validation patch size:\", len(val_fns))","daba1b48":"sample_image_fp = os.path.join(train_dir, train_fns[0])\nsample_image = Image.open(sample_image_fp).convert(\"RGB\")\nplt.imshow(sample_image)\n\ndef split_image(image):\n    image = np.array(image)\n    cityscape = image[:, :256, :] \n    label = image[:, 256:, :]\n    return cityscape, label\n\nsample_image = np.array(sample_image)\nprint(\"sample image shape: \",sample_image.shape)\n\ncityscape, label = split_image(sample_image)\nprint(\"image shape:\" , cityscape.shape)\nprint(\"label shape:\" , label.shape)\n\ncityscape = Image.fromarray(cityscape) \nlabel = Image.fromarray(label)\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\naxes[1].imshow(cityscape)\naxes[0].imshow(label)","27e7246c":"num_items = 1000\ncolor_array = np.random.choice(range(256), 3*num_items).reshape(-1, 3)\nprint(color_array.shape)\nprint(color_array[:5, :])\n\nnum_classes = 15\nlabel_model = MiniBatchKMeans(n_clusters=num_classes)\nlabel_model.fit(color_array)\n\nlabel_model.predict(color_array[:5, :])\n\ncityscape, label = split_image(sample_image)\nlabel_class = label_model.predict(label.reshape(-1, 3)).reshape(256, 256)\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\naxes[0].imshow(cityscape)\naxes[1].imshow(label)\naxes[2].imshow(label_class)\nprint(label_class)","295639ee":"class CityscapeDataset(Dataset):\n    \n    def __init__(self, image_dir, label_model):\n        self.image_dir = image_dir\n        self.image_fns = os.listdir(image_dir)\n        self.label_model = label_model\n        \n    def __len__(self):\n        return len(self.image_fns)\n    \n    def __getitem__(self, index):\n        image_fn = self.image_fns[index]\n        image_fp = os.path.join(self.image_dir, image_fn)\n        image = Image.open(image_fp).convert('RGB')\n        image = np.array(image)\n        cityscape, label = self.split_image(image)\n        label_class = self.label_model.predict(label.reshape(-1, 3)).reshape(256, 256)\n        cityscape = self.transform(cityscape)\n        label_class = torch.Tensor(label_class).long()\n        return cityscape, label_class\n    \n    def split_image(self, image):\n        image = np.array(image)\n        cityscape, label = image[:, :256, :], image[:, 256:, :]\n        return cityscape, label\n    \n    def transform(self, image):\n        transform_ops = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n        ])\n        return transform_ops(image)","233fb084":"class UNet(nn.Module):\n    \n    def __init__(self, num_classes):\n        super(UNet, self).__init__()\n        self.num_classes = num_classes\n        self.contracting_11 = self.conv_block(in_channels=3, out_channels=64)\n        self.contracting_12 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.contracting_21 = self.conv_block(in_channels=64, out_channels=128)\n        self.contracting_22 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.contracting_31 = self.conv_block(in_channels=128, out_channels=256)\n        self.contracting_32 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.contracting_41 = self.conv_block(in_channels=256, out_channels=512)\n        self.contracting_42 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.middle = self.conv_block(in_channels=512, out_channels=1024)\n        self.expansive_11 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_12 = self.conv_block(in_channels=1024, out_channels=512)\n        self.expansive_21 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_22 = self.conv_block(in_channels=512, out_channels=256)\n        self.expansive_31 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_32 = self.conv_block(in_channels=256, out_channels=128)\n        self.expansive_41 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_42 = self.conv_block(in_channels=128, out_channels=64)\n        self.output = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=3, stride=1, padding=1)\n        \n    def conv_block(self, in_channels, out_channels):\n        block = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n                                    nn.ReLU(),\n                                    nn.BatchNorm2d(num_features=out_channels),\n                                    nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n                                    nn.ReLU(),\n                                    nn.BatchNorm2d(num_features=out_channels))\n        return block\n    \n    def forward(self, X):\n        contracting_11_out = self.contracting_11(X) \n        contracting_12_out = self.contracting_12(contracting_11_out) \n        contracting_21_out = self.contracting_21(contracting_12_out) \n        contracting_22_out = self.contracting_22(contracting_21_out) \n        contracting_31_out = self.contracting_31(contracting_22_out) \n        contracting_32_out = self.contracting_32(contracting_31_out) \n        contracting_41_out = self.contracting_41(contracting_32_out) \n        contracting_42_out = self.contracting_42(contracting_41_out) \n        middle_out = self.middle(contracting_42_out) \n        expansive_11_out = self.expansive_11(middle_out) \n        expansive_12_out = self.expansive_12(torch.cat((expansive_11_out, contracting_41_out), dim=1))\n        expansive_21_out = self.expansive_21(expansive_12_out) \n        expansive_22_out = self.expansive_22(torch.cat((expansive_21_out, contracting_31_out), dim=1))\n        expansive_31_out = self.expansive_31(expansive_22_out) \n        expansive_32_out = self.expansive_32(torch.cat((expansive_31_out, contracting_21_out), dim=1))\n        expansive_41_out = self.expansive_41(expansive_32_out) \n        expansive_42_out = self.expansive_42(torch.cat((expansive_41_out, contracting_11_out), dim=1)) \n        output_out = self.output(expansive_42_out) \n        return output_out","ec972fc2":"batch_size = 16\nepochs = 20\nlr = 0.001","746d3195":"dataset = CityscapeDataset(train_dir, label_model)\ndata_loader = DataLoader(dataset, batch_size=batch_size)\n\nmodel = UNet(num_classes=num_classes).to(device)\nprint(model)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=lr)","fc3e5b94":"step_losses = []\nepoch_losses = []\nfor epoch in tqdm(range(epochs)):\n    epoch_loss = 0\n    for X, Y in tqdm(data_loader, total=len(data_loader), leave=False):\n        X, Y = X.to(device), Y.to(device)\n        optimizer.zero_grad()\n        Y_pred = model(X)\n        loss = criterion(Y_pred, Y)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        step_losses.append(loss.item())\n    epoch_losses.append(epoch_loss\/len(data_loader))","b99e8837":"fig, axes = plt.subplots(1, 2, figsize=(10, 5))\naxes[0].plot(step_losses)\naxes[1].plot(epoch_losses)","f891e7ae":"model_name = \"U-Net.pth\"\ntorch.save(model.state_dict(), model_name)\nmodel_path = \"\/kaggle\/working\/U-Net.pth\"\nmodel_ = UNet(num_classes=num_classes).to(device)\nmodel_.load_state_dict(torch.load(model_path))","5faa7b2e":"test_batch_size = 8\ndataset = CityscapeDataset(val_dir, label_model)\n\ndata_loader = DataLoader(dataset, batch_size=test_batch_size)\n\nX, Y = next(iter(data_loader))\nX, Y = X.to(device), Y.to(device)\nprint(X.shape)\nY_pred = model_(X)\nprint(Y_pred.shape)\nY_pred = torch.argmax(Y_pred, dim=1)\nprint(Y_pred.shape)\n\ninverse_transform = transforms.Compose([\n    transforms.Normalize((-0.485\/0.229, -0.456\/0.224, -0.406\/0.225), (1\/0.229, 1\/0.224, 1\/0.225))\n])","fa44ac0d":"fig, axes =  plt.subplots(test_batch_size, 3, figsize=(3*5, test_batch_size*5))\n\niou_scores = []\n\nfor i in range(test_batch_size):\n    \n    landscape = inverse_transform(X[i]).permute(1, 2, 0).cpu().detach().numpy()\n    label_class = Y[i].cpu().detach().numpy()\n    label_class_predicted = Y_pred[i].cpu().detach().numpy()\n    \n    # IOU score\n    intersection = np.logical_and(label_class, label_class_predicted)\n    union = np.logical_or(label_class, label_class_predicted)\n    iou_score = np.sum(intersection) \/ np.sum(union)\n    iou_scores.append(iou_score)\n    \n    \n    axes[i,0].imshow(landscape)\n    axes[i,0].set_title(\"Landscape\")\n    axes[i,1].imshow(label_class)\n    axes[i,1].set_title(\"Label Class\")\n    axes[i,2].imshow(label_class_predicted)\n    axes[i,2].set_title(\"Label Class - Predicted\")\n\nprint(\"Mean IOU Score : \"+str(sum(iou_scores) \/ len(iou_scores)))\n","f2991879":"import cv2\nimport numpy as np\nfrom PIL import Image\nfrom matplotlib import cm\ncap = cv2.VideoCapture('..\/input\/videocheck\/1.mp4')\nframeCount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nframeWidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframeHeight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nbuf = np.empty((frameCount, frameHeight, frameWidth, 3), np.dtype('uint8'))\n\nfc = 0\nret = True\ntest_batch_size = 1\ndef transform(image):\n            transform_ops = transforms.Compose([\n            transforms.RandomHorizontalFlip(),\n            transforms.Resize(256),\n            transforms.CenterCrop(256),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n            ])\n            return transform_ops(image)\n        \ninverse_transform = transforms.Compose([\n    \n    transforms.Normalize((-0.485\/0.229, -0.456\/0.224, -0.406\/0.225), (1\/0.229, 1\/0.224, 1\/0.225))\n])\ntrans = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomHorizontalFlip(),\n    #transforms.Resize(256),\n    #transforms.CenterCrop(256),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n])\n#while (fc < frameCount  and ret):\nret, img = cap.read()\n\nres = cv2.resize(img,(256,256) , interpolation = cv2.INTER_AREA)\n\n\n#print(type(img))\n#print(res)\n#im = Image.fromarray(np.uint8(cm.gist_earth(img)*255))\ninp = trans(res)\nx=inp.view(1,3,256,256)\n#print(x)\n#im_rgb = cv2.cvtColor(buf, cv2.COLOR_BGR2RGB)\n#res = cv2.resize(im_rgb,(384,512) , interpolation = cv2.INTER_AREA)\n\n#ten = transform(res)\n#ten = ten.unsqueeze(0)\n#x = ten.reshape(3,3,256,256)\nx = x.to(device)\nY_pred = model_(x)\n#print(Y_pred)\nY_pred = torch.argmax(Y_pred, dim=1)\n#print(Y_pred.shape)\n#print(Y_pred)\n#Y_pred = Y_pred.view(256,256,3)\n#print(Y_pred.shape)\nlandscape = inverse_transform(x[0]).permute(1, 2, 0).cpu().detach().numpy()\nprint(landscape.shape)\nlabel_class_predicted = Y_pred[0].cpu().detach().numpy()\n#fig, axi =  plt.subplots(1)\nax = plt.imshow(landscape)\n\n\n\n    \n    #fc += 1\n    \n\n#cityscape = transform(buf)","629e146c":"axi = plt.imshow(label_class_predicted)","791bc940":"we will show original image, label and label class in this section. Fistly we indicate color array as randomly. we will use Kmeans algorithm to perict label model. classes number is 10, iteration numbers is 1000. This is hyperparameter. you can change these two variables. ","51914de3":"REFERENCES\n* GokulKarthik","e0d34894":"in this section, we will create Dataset class and we will use to initialize and data loader parts. CityscapeDataset class include five functions. Initialize, len, getitem, split image and transform. \n","8a63086f":"we visualize losses lists."}}