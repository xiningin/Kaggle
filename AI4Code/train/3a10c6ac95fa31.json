{"cell_type":{"f4b4e3f4":"code","608a2181":"code","53354cad":"code","2549a062":"code","65424f5e":"code","4d500b41":"code","850d1533":"code","789e21cb":"code","b6e53116":"code","08c78ff4":"code","2885ba7f":"code","a52f047a":"code","dd5f835e":"code","cadff93d":"code","8591bc69":"code","bde7f5d8":"code","95660733":"code","040e71ed":"code","40100f0b":"code","c3f9e100":"code","b9ede98c":"code","b62ddb05":"code","45b7479e":"code","293dca10":"code","1bb9b133":"code","546cb874":"code","49f9c92c":"code","3f95910b":"code","22eabc43":"code","b3132554":"code","e1f0acaf":"code","f3b53e82":"code","a1d1e1e5":"code","b6e78826":"code","9a292261":"code","7a8102b7":"code","009a5d94":"code","a79ba2af":"code","a0a8dcb3":"code","10020ce1":"code","c770d33e":"markdown","4c07ae30":"markdown","69c782d4":"markdown","9d36ed79":"markdown","ad46e61a":"markdown","c9dc20e6":"markdown","6afd10a7":"markdown","b077c7b2":"markdown","212f2022":"markdown","d558246a":"markdown","ac1d92b7":"markdown","03a34ed0":"markdown","27426867":"markdown","4f8b4b06":"markdown","db9ef8f0":"markdown","9a65f343":"markdown","0050481f":"markdown","554316b9":"markdown","03260870":"markdown"},"source":{"f4b4e3f4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom datetime import datetime, timedelta\n\n\nprint('numpy version : ',np.__version__)\nprint('pandas version : ',pd.__version__)\nprint('seaborn version : ',sns.__version__)\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\npd.set_option('display.max_columns', None)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","608a2181":"sns.set(rc={'figure.figsize':(20.7,8.27)})\nsns.set_style(\"whitegrid\")\nsns.color_palette(\"dark\")\nplt.style.use(\"fivethirtyeight\")","53354cad":"#read dataset\ndf=pd.read_csv('\/kaggle\/input\/bank-marketing\/bank-marketing.csv')\n\n# developer-friendly column names and format\ndf.columns = [x.lower().replace('-', '_').replace(' ', '_') for x in df.columns.to_list()]\n\ndf.sample(5, random_state=42)","2549a062":"# Check For Missing Value (Null, NaN, or None)\ndf.isna().sum()","65424f5e":"# Check For Invalid Data By Column's Unique Value\nfor x in df.columns.to_list():\n    print(x + ' :')\n    print(df[x].sort_values().unique())\n    print('')","4d500b41":"non_target_columns = df.columns.to_list()\nnon_target_columns.remove('y')\nnon_target_columns.remove('response')\n\ndf.duplicated(non_target_columns).sum()","850d1533":"#drop y because its redundant with the target column\nif 'y' in df.columns.to_list():\n  df = df.drop(['y'], axis = 1)\n\ndf.sample(5)","789e21cb":"#change pdays -1 to 0\ndf.pdays = df.pdays.apply(lambda x: 0 if (x == -1) else x)\ndf.sample(5, random_state=42)","b6e53116":"#separate column that use categorical and numerical\ncats= list(df.select_dtypes(include=['object','datetime64[ns]','bool']) )\nnums= list(df.select_dtypes(include='int64') )\nprint(nums)\nprint(cats)","08c78ff4":"#Detect outlier\nfor i in range(0, len(nums)):\n    plt.subplot(2, (len(nums)\/2)+1, i+1)\n    sns.boxplot(y=df[nums[i]], color='gray', orient='v')\n    plt.tight_layout()","2885ba7f":"#zscore to take out outlier\n\nfrom scipy import stats\nprint(f'Jumlah baris sebelum memfilter outlier: {len(df)}')\n\nfiltered_entries = np.array([True] * len(df))\n\nfor col in ['balance', 'duration', 'campaign']:\n    zscore = abs(stats.zscore(df[col])) # hitung absolute z-scorenya\n    filtered_entries = (zscore < 3) & filtered_entries # keep yang kurang dari 3 absolute z-scorenya\n    \ndf = df[filtered_entries] # filter, cuma ambil yang z-scorenya dibawah 3\n\nprint(f'Jumlah baris setelah memfilter outlier: {len(df)}')","a52f047a":"#correlation check\ndf_new=df.copy()\nplt.figure(figsize=(15, 8))\nsns.heatmap(df_new.corr(), cmap='Blues', annot=True, fmt='.2f')","dd5f835e":"features= ['duration','pdays','previous']\nplt.figure(figsize=(15, 15))\nsns.pairplot(df_new[features], diag_kind='kde')","cadff93d":"plt.figure(figsize=(15, 5))\nax = sns.barplot(x='campaign', y='age', data=df_new.groupby(['campaign']).agg('count').reset_index()[['campaign', 'age']])\nax.set_ylabel('count')","8591bc69":"plt.figure(figsize=(20, 23))\nsns.displot(x='pdays', hue='response', data=df_new)","bde7f5d8":"plt.figure(figsize=(15, 5))\nsns.displot(x='duration', hue='response', data=df_new)","95660733":"plt.figure(figsize=(15, 5))\nsns.barplot(x='poutcome', y='duration' ,hue='response', data=df_new)","040e71ed":"plt.figure(figsize=(15, 5))\nsns.barplot(x='poutcome', y='pdays' ,hue='response', data=df_new)","40100f0b":"plt.figure(figsize=(15, 5))\nsns.barplot(x='housing',y='response',data=df_new)","c3f9e100":"plt.figure(figsize=(15, 5))\nsns.boxplot(x='contact',y='duration', hue='response',data=df_new)","b9ede98c":"#chechk distribution of all feature that we use\nplt.figure(figsize=(12, 15))\nfor i in range(0, len(nums)):\n    plt.subplot(5, (len(nums)\/3)+1, i+1)\n    sns.distplot(df_new[nums[i]], color='gray')\n    plt.tight_layout()","b62ddb05":"### Label Encoding\nif 'contacted_date' in df_new.columns.to_list():\n    df_new.drop(['contacted_date'], axis=1, inplace=True)\nif 'pcontacted_date' in df_new.columns.to_list():\n    df_new.drop(['pcontacted_date'], axis=1, inplace=True)\n\n\nfor f in df_new.select_dtypes(include='object').columns.to_list():\n    df_new[f] = df_new[f].astype('category').cat.codes\n\ndf_new","45b7479e":"plt.figure(figsize=(20, 20))\nsns.heatmap(df_new.corr(), cmap='Blues', annot=True, fmt='.2f')","293dca10":"df_new.describe()","1bb9b133":"#check imbalance\ndf_new['response'].value_counts()\nplt.figure(figsize=(5, 5))\nsns.countplot(x ='response', data = df_new)","546cb874":"x = df_new.drop(['response'], axis=1)\ny = df_new[['response']]\n\nx.head()\ny.head()","49f9c92c":"np.unique","3f95910b":"#do smote to balancing our target column\nfrom imblearn import under_sampling, over_sampling\nx_over, y_over = over_sampling.SMOTE(0.5).fit_resample(x, y)","22eabc43":"print('SMOTE')\nprint((y_over).value_counts())\n","b3132554":"#plotting target column after do smote\nplt.figure(figsize=(5, 5))\nsns.countplot(x ='response', data = y_over)","e1f0acaf":"from sklearn.model_selection import train_test_split \nx_train, x_test, y_train, y_test = train_test_split(x_over, y_over, test_size = 0.3, random_state = 42)","f3b53e82":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_curve, auc\n\ndef eval_classification(model, pred, xtrain, ytrain, xtest, ytest):\n    print(\"\\nAccuracy (Test Set): %.2f\" % accuracy_score(ytest, pred))\n    print(\"Precision (Test Set): %.2f\" % precision_score(ytest, pred))\n    print(\"Recall (Test Set): %.2f\" % recall_score(ytest, pred))\n    print(\"F1-Score (Test Set): %.2f\" % f1_score(ytest, pred))\n    \n    fpr, tpr, thresholds = roc_curve(ytest, pred, pos_label=1) # pos_label: label yang kita anggap positive\n    print(\"AUC: %.2f\" % auc(fpr, tpr))\n\ndef show_feature_importance(model):\n    feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n    ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))\n    ax.invert_yaxis()\n\n    plt.xlabel('score')\n    plt.ylabel('feature')\n    plt.title('feature importance score')\n\ndef show_best_hyperparameter(model, hyperparameters):\n    for key, value in hyperparameters.items() :\n        print('Best '+key+':', model.get_params()[key])","a1d1e1e5":"#model Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier(random_state=42)\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_test)\neval_classification(model, y_pred, x_train, y_train, x_test, y_test)\nprint(model.feature_importances_)\n\nfeat_importances = pd.Series(model.feature_importances_, index=x.columns)\n# feat_importances = feat_importances.sort_values(ascending=True)\nax = feat_importances.nlargest(10).plot(kind='barh')\nax.invert_yaxis()\n\n# ax.figure(figsize=())","b6e78826":"#logistic regression\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(random_state=42, solver='liblinear')\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_test)\neval_classification(model, y_pred, x_train, y_train, x_test, y_test)","9a292261":"#extra tree classifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier(random_state=42)\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_test)\neval_classification(model, y_pred, x_train, y_train, x_test, y_test)\nprint(model.feature_importances_)\n\nfeat_importances = pd.Series(model.feature_importances_, index=x.columns)\n# plt.figure(figsize=(12, 12))\nplt.rcParams['ytick.labelsize'] = '20'\nax = feat_importances.nlargest(4).plot(kind='barh')\nax.invert_yaxis()\nplt.show()","7a8102b7":"\nypred1 = model.predict(x)\nnp.unique(ypred1, return_counts=True)","009a5d94":"#knn\nfrom sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier(n_neighbors=5)\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_test)\neval_classification(model, y_pred, x_train, y_train, x_test, y_test)","a79ba2af":"#decision tree\nmodel = DecisionTreeClassifier(random_state=42)\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_test)\neval_classification(model, y_pred, x_train, y_train, x_test, y_test)","a0a8dcb3":"#xg boost\n\nfrom xgboost import XGBClassifier\nxg = XGBClassifier(random_state=42)\nxg.fit(x_train, y_train)\n\ny_pred = xg.predict(x_test)\neval_classification(xg, y_pred, x_train, y_train, x_test, y_test)","10020ce1":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\nimport numpy as np\n\n# List of hyperparameter\nmax_features = ['auto', 'sqrt']\n\nhyperparameters = dict(max_features=max_features)\n\n# Inisialisasi Model\nmodel = ExtraTreesClassifier(random_state=42)\nmodel = RandomizedSearchCV(model, hyperparameters, cv=5, random_state=42, scoring='f1')\nmodel.fit(x_train, y_train)\n# Predict & Evaluation\ny_pred = model.predict(x_test)\neval_classification(model, y_pred, x_train, y_train, x_test, y_test)","c770d33e":"outlier at balance,duration,campaign need to be take out","4c07ae30":"**Bussiness Recomendation**","69c782d4":"we're chechking the highest correlation column to our target column and we get column duration,pdays, and previous","9d36ed79":"![image.png](attachment:dade96c7-8bf6-4b57-873c-8f03170d33f5.png)","ad46e61a":"we now get insight that the higher pdays the response for taking the product is going to down","c9dc20e6":"**EDA**","6afd10a7":"we get to know that the campaign isn't impactful because the graph is going down for the next campaign","b077c7b2":"we get insight customer that dont have house are have higher probability to buy our product","212f2022":"**MODELLING**","d558246a":"we get insight that if we want to contact customer that we already offer previous campaign, the maximum days to contact was 150 days from the last time we contact them","ac1d92b7":"fom extra tree classifier we get 4 important parameter which is \n![image.png](attachment:8efc498c-27cc-466d-a852-935670246496.png)\n","03a34ed0":"![image.png](attachment:e870ee62-a410-4548-9661-fdd404021121.png)","27426867":"we get to know that if the longer duration to call customer then the chances that customer take our product is also increasing, and we also know that the minimum duration for making calls is 200s or 3 minutes","4f8b4b06":"after modelling with 5 method we choose extra tree classifier because its have the best accuracy \nwe choose accuracy to be our primary parameter because we dont wanna miss our target customer","db9ef8f0":"![image.png](attachment:652d1dcc-5b7e-49c0-827d-4f7b93a46454.png)","9a65f343":"![image.png](attachment:920df944-09b3-4c0d-a106-3ff687b47fcd.png)","0050481f":"this parameter will be the most important parameter to sorting our target customer\n","554316b9":"This is My Final Project as i take a course at rakamin academy\n\n- dataset : https:\/\/www.kaggle.com\/dhirajnirne\/bank-marketing\n- target : response\n\nfrom this dataset i make a hypotesis that marketing to get customer still a blind marketing(proven by response is imbalance), which i assume we still dont know which is feature that highly make our customer to buy our product. the purpose of this project is to know which of the feature is the most important and to reduce cost marketing.","03260870":"**Insight**"}}