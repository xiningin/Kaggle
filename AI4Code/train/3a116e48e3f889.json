{"cell_type":{"1131cc51":"code","6668659c":"code","b26fbf47":"code","8fe9af10":"code","d620d7c7":"code","8fd16431":"code","a3170f3b":"code","1c5d0f51":"code","18399ea7":"code","67ce117e":"markdown","ff7138b3":"markdown","2cda4c8a":"markdown"},"source":{"1131cc51":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nimport pickle","6668659c":"import gresearch_crypto\nimport datetime as dt","b26fbf47":"!pip install --no-index --no-deps ..\/input\/prophet\/wheelhouse\/*.whl","8fe9af10":"!pip uninstall -y pystan ","d620d7c7":"!pip install --no-index --no-deps ..\/input\/pystan\/wheelhouse\/*.whl","8fd16431":"from fbprophet import Prophet","a3170f3b":"\ntrain = pd.read_csv(\"..\/input\/g-research-crypto-forecasting\/train.csv\")\ntrain['timestamp'] = pd.to_datetime(train['timestamp'],unit='s')\ntrain = train.rename(columns={'timestamp': 'ds',\"Target\":\"y\"})\ntrain = train[train[\"y\"].isnull()==0]\ntrain = train[train['ds'] < dt.datetime(2021,6,13)]\n\ndef upper_shadow(df):\n    return df['High'] - np.maximum(df['Close'], df['Open'])\n\ndef lower_shadow(df):\n    return np.minimum(df['Close'], df['Open']) - df['Low']\n\ndef candle_body(df):\n    return np.abs(df.Close-df.Open)\n\ndef get_features(df):\n    df_feat = df[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP',\"y\",\"Asset_ID\",\"ds\"]].copy()\n    df_feat['Upper_Shadow'] = upper_shadow(df_feat)\n    df_feat['Lower_Shadow'] = lower_shadow(df_feat)\n    df_feat[\"body_len\"] = candle_body(df_feat)\n    return df_feat\n\ntrain = get_features(train)\n\n\n\nm0 = Prophet()\nm1 = Prophet()\nm2 = Prophet()\nm3 = Prophet()\nm4 = Prophet()\nm5 = Prophet()\nm6 = Prophet()\nm7 = Prophet()\nm8 = Prophet()\nm9 = Prophet()\nm10 = Prophet()\nm11 = Prophet()\nm12 = Prophet()\nm13 = Prophet()\n\nmodels = {0: m0,\n 1: m1,\n 2: m2,\n 3: m3,\n 4: m4,\n 5: m5,\n 6: m6,\n 7: m7,\n 8: m8,\n 9: m9,\n 10: m10,\n 11: m11,\n 12: m12,\n 13: m13}\n\nfor i in range(14):\n    models[i].fit(train[train[\"Asset_ID\"]==i])\n    pkl_model_path = '.\/prophet-model{0}.pkl'.format(i)\n    with open(pkl_model_path, 'wb') as f:\n        pickle.dump(models[i], f)","1c5d0f51":"models = {0: m0,\n 1: m1,\n 2: m2,\n 3: m3,\n 4: m4,\n 5: m5,\n 6: m6,\n 7: m7,\n 8: m8,\n 9: m9,\n 10: m10,\n 11: m11,\n 12: m12,\n 13: m13}","18399ea7":"feature_cols = ['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP',\"Asset_ID\",\"ds\",'Upper_Shadow','Lower_Shadow',\"body_len\"]\ndef get_features(df):\n    df_feat = df[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP',\"Asset_ID\",\"ds\"]].copy()\n    df_feat['Upper_Shadow'] = upper_shadow(df_feat)\n    df_feat['Lower_Shadow'] = lower_shadow(df_feat)\n    df_feat[\"body_len\"] = candle_body(df_feat)\n    return df_feat\nenv = gresearch_crypto.make_env()\niter_test = env.iter_test()\n\nfor i, (df_test, df_pred) in enumerate(iter_test):\n    df_test['timestamp'] = pd.to_datetime(df_test['timestamp'],unit='s')\n    df_test = df_test.rename(columns={'timestamp': 'ds'})\n    df_test = get_features(df_test)\n    for j , row in df_test.iterrows():\n        model = models[row['Asset_ID']]\n        x_test = pd.DataFrame(row).T\n        y_pred = model.predict(x_test[feature_cols])[\"yhat\"][0]\n\n        df_pred.loc[df_pred['row_id'] == row['row_id'], 'Target'] = y_pred\n        \n        \n        # Print just one sample row to get a feeling of what it looks like\n\n\n    # Display the first prediction dataframe\n\n        \n\n    # Send submissions\n    env.predict(df_pred)","67ce117e":"\"\"\"\nwith open('..\/input\/fbp-model\/prophet-model0.pkl', 'rb') as f:\n    m0 = pickle.load(f)\nwith open('..\/input\/fbp-model\/prophet-model1.pkl', 'rb') as f:\n    m1 = pickle.load(f)\nwith open('..\/input\/fbp-model\/prophet-model2.pkl', 'rb') as f:\n    m2 = pickle.load(f)\nwith open('..\/input\/fbp-model\/prophet-model3.pkl', 'rb') as f:\n    m3 = pickle.load(f)\nwith open('..\/input\/fbp-model\/prophet-model4.pkl', 'rb') as f:\n    m4 = pickle.load(f)\nwith open('..\/input\/fbp-model\/prophet-model5.pkl', 'rb') as f:\n    m5 = pickle.load(f)\nwith open('..\/input\/fbp-model\/prophet-model6.pkl', 'rb') as f:\n    m6 = pickle.load(f)\nwith open('..\/input\/fbp-model\/prophet-model7.pkl', 'rb') as f:\n    m7 = pickle.load(f)\nwith open('..\/input\/fbp-model\/prophet-model8.pkl', 'rb') as f:\n    m8 = pickle.load(f)\nwith open('..\/input\/fbp-model\/prophet-model9.pkl', 'rb') as f:\n    m9 = pickle.load(f)\nwith open('..\/input\/fbp-model\/prophet-model10.pkl', 'rb') as f:\n    m10 = pickle.load(f)\nwith open('..\/input\/fbp-model\/prophet-model11.pkl', 'rb') as f:\n    m11 = pickle.load(f)\nwith open('..\/input\/fbp-model\/prophet-model12.pkl', 'rb') as f:\n    m12 = pickle.load(f)\nwith open('..\/input\/fbp-model\/prophet-model13.pkl', 'rb') as f:\n    m13 = pickle.load(f)\u201d\u201d\u201d","ff7138b3":"# **Reference**\n","2cda4c8a":"*  https:\/\/www.kaggle.com\/axzhang\/s-baseline-lgb-reproduce-the-lb-score\n* https:\/\/www.kaggle.com\/jiny333\/creating-whl-files-to-install-external-libraries"}}