{"cell_type":{"864d6f5d":"code","11f42335":"code","8f36b7fd":"code","50bb5bbe":"code","14ac3020":"code","1a64f9bd":"code","90502112":"code","13d7094f":"code","0c9551cd":"code","98561102":"code","5e26d551":"code","70c1bcdc":"code","b630b913":"code","b1143584":"code","3af5a014":"code","c0166642":"code","3e871622":"code","08dc9252":"code","4f7db289":"code","7d4d15c2":"code","6a367c0a":"code","483856d7":"markdown","bd9ba39a":"markdown","3c82dd8c":"markdown","6ac8fa13":"markdown","a2f6ec07":"markdown","61d521c4":"markdown","54393a74":"markdown"},"source":{"864d6f5d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/from-coursera\"))\n\n# Any results you write to the current directory are saved as output.","11f42335":"import math\nimport pandas as pd\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split","8f36b7fd":"df = pd.read_csv('..\/input\/from-coursera\/gbm-data.csv')\ndf.tail()","50bb5bbe":"df_arr = df.values","14ac3020":"y = df_arr[:,0]","1a64f9bd":"X = df_arr[:,1:]","90502112":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=241)","13d7094f":"learning_rates = [1, 0.5, 0.3, 0.2, 0.1]","0c9551cd":"def sigmoid(y_pred):\n    return 1.0 \/ (1.0 + np.exp(-y_pred))","98561102":"# import machine learning algorithms\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import log_loss\nimport matplotlib.pyplot as plt","5e26d551":"\n\nglobal_learning_rate = 0\nglobal_iter_num = 0\nglobal_min_log = 1000\n\nfor learning_rate in learning_rates:\n    gb = GradientBoostingClassifier(n_estimators=250, learning_rate = learning_rate, verbose=True, random_state = 241)\n    gb.fit(X_train, y_train)\n    print(\"Learning rate: \", learning_rate)\n    print(\"Accuracy score (training): {0:.3f}\".format(gb.score(X_train, y_train)))\n    print(\"Accuracy score (validation): {0:.3f}\".format(gb.score(X_test, y_test)))\n    # compute test set deviance\n    result = []\n    iter_num = 0\n    min_log = 0\n    \n    for i, y_pred in enumerate(gb.staged_decision_function(X_test)):\n        # clf.loss_ assumes that y_test[i] in {0, 1}\n        tmp = log_loss(y_test, sigmoid(y_pred))\n        if i == 1:\n            min_log = tmp\n            iter_num = 1\n        if tmp < min_log:\n            min_log = tmp\n            iter_num = i\n        result.append(tmp)\n    if global_min_log > min_log:\n        global_min_log = min_log\n        global_learning_rate = learning_rate\n        global_iter_num = iter_num\n    plt.plot(result)\n    plt.show()\n    ","70c1bcdc":"for learning_rate in learning_rates:\n    gb = GradientBoostingClassifier(n_estimators=250, learning_rate = learning_rate, verbose=True, random_state = 241)\n    gb.fit(X_train, y_train)\n    print(\"Learning rate: \", learning_rate)\n    print(\"Accuracy score (training): {0:.3f}\".format(gb.score(X_train, y_train)))\n    print(\"Accuracy score (validation): {0:.3f}\".format(gb.score(X_test, y_test)))\n    # compute test set deviance\n    train_loss = []\n    test_loss = []\n    \n    for i, y_pred in enumerate(gb.staged_decision_function(X_test)):\n        # clf.loss_ assumes that y_test[i] in {0, 1}\n        tmp = log_loss(y_test, sigmoid(y_pred))\n        test_loss.append(tmp)\n    \n    for i, y_pred in enumerate(gb.staged_decision_function(X_train)):\n        # clf.loss_ assumes that y_test[i] in {0, 1}\n        tmp = log_loss(y_train, sigmoid(y_pred))\n        train_loss.append(tmp)\n    plt.figure()\n    plt.plot(test_loss, 'r', linewidth=2)\n    plt.plot(train_loss, 'g', linewidth=2)\n    plt.legend(['test', 'train'])\n    plt.show()","b630b913":"print(global_learning_rate, global_iter_num, global_min_log)","b1143584":"print(1, 'overfitting')","3af5a014":"gb = GradientBoostingClassifier(n_estimators=250, learning_rate = 0.2, verbose=True, random_state = 241)\ngb.fit(X_train, y_train)\nprint(\"Learning rate: \", learning_rate)\nprint(\"Accuracy score (training): {0:.3f}\".format(gb.score(X_train, y_train)))\nprint(\"Accuracy score (validation): {0:.3f}\".format(gb.score(X_test, y_test)))\n# compute test set deviance\niter_num = 0\nmin_log = 0\n    \nfor i, y_pred in enumerate(gb.staged_decision_function(X_test)):\n    # clf.loss_ assumes that y_test[i] in {0, 1}\n    tmp = log_loss(y_test, sigmoid(y_pred))\n    if i == 1:\n        min_log = tmp\n        iter_num = 1\n    if tmp < min_log:\n        min_log = tmp\n        iter_num = i","c0166642":"print(iter_num, min_log)","3e871622":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification","08dc9252":"clf = RandomForestClassifier(n_estimators=51, random_state=241)\nclf.fit(X_train, y_train)\ny_pred = clf.predict_proba(X_test)\ntmp = log_loss(y_test, y_pred)","4f7db289":"print(tmp)","7d4d15c2":"clf = RandomForestClassifier(n_estimators=36, random_state=241)\nclf.fit(X_train, y_train)\ny_pred = clf.predict_proba(X_test)\ntmp = log_loss(y_test, y_pred)","6a367c0a":"print(tmp)","483856d7":"How can we characterize the quality graph on the test sample, starting with some iteration: retraining (overfitting) or underfitting (underfitting)? In response, specify one of the words overfitting or underfitting.","bd9ba39a":"This is my first notebook and first English article. I apologize to native English speakers. This is my solution in the course on Cursera. Hope it might be useful here.","3c82dd8c":"Give the minimum low-loss value on the test sample and the iteration number at which it is reached, with learning_rate = 0.2.","6ac8fa13":"## Predicting a Biological Response.","a2f6ec07":"You must Load a selection from the game-data file.csv using pandas and convert it to a numpy array (the parameter values at dataframe). In the first column of the data file it is written whether there was a reaction or not. All other columns (d1 - d1776) contain a variety of molecular characteristics such as size, shape, etc. Split the sample into training and test using the train_test_split function with parameters test_size = 0.8 and random_state = 241.","61d521c4":"Train the Gradient Boosting Classifier with the parameters n_estimators=250, verbose=True, random_state=241 and for each value learning_rate from the list [1, 0.5, 0.3, 0.2, 0.1] do the following:\n\n* Use the staged_decision_function method to predict quality on the training and test samples at each iteration.\n* Convert the resulting prediction using the sigmoid function using the formula 1 \/ (1 + e^{\u2212y_pred}), where y_pred is the predicted value.\n* Calculate and plot log-loss values (which can be calculated using the sklearn.metrics.log_loss function) on the training and test samples, and find the minimum metric value and the iteration number at which it is achieved.","54393a74":"On the same data, train RandomForestClassifier with the number of trees equal to the number of iterations on which the best quality is achieved for gradient boosting from the previous point, random_state=241 and other default parameters. What is the value of logloss on the test obtained from this random forest? (Do not forget that the predictions must be obtained using the predict_proba function. In this case, taking the sigmoid of estimation of probability of class is not necessary)"}}