{"cell_type":{"747b138b":"code","265807d8":"code","5cbcf60f":"code","df5b7c9f":"code","e7cb4740":"code","ef8a75af":"code","85af83c3":"code","827132d6":"code","bd82c23b":"code","fbe2e919":"code","e5d13973":"code","432d05bb":"code","2b06399a":"code","dfd084d7":"code","cd33d826":"code","820e8e6b":"code","4b2d3172":"code","d43a3b6e":"code","c3a078db":"code","11ed6edd":"code","ca4f0342":"code","b88503f6":"markdown","46dc649d":"markdown","a851e45c":"markdown","620d9d78":"markdown","cc8c00f3":"markdown","568a8d6e":"markdown","7d872071":"markdown","9de8cb03":"markdown","7e1054c0":"markdown","20cdfc36":"markdown","29a7f449":"markdown","e5df9be4":"markdown","b042011c":"markdown","ac0ef637":"markdown"},"source":{"747b138b":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.models import Sequential\nfrom tensorflow.keras.optimizers import RMSprop, Adam\nfrom sklearn.metrics import confusion_matrix\nfrom tensorflow.keras import callbacks\nfrom keras.callbacks import ReduceLROnPlateau\nimport tensorflow as tf\nfrom keras.preprocessing.image import ImageDataGenerator\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","265807d8":"# Load the data\ntrain = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\n\n# Create the X_train and y_train variables \nX_train = train.drop('label', axis=1)\ny_train = train.label\nprint(f'X_train = {X_train.shape}')\nprint(f'test = {test.shape}')\n\n# Set the plots size\nsns.set(rc={'figure.figsize':(10,7)})\nsns.countplot(y_train)","5cbcf60f":"# Normalize the data \nX_train = X_train \/ 255.0\ntest = test \/ 255.0\n\n# Reshape the data\nX_train = X_train.values.reshape(-1, 28, 28, 1)\ntest = test.values.reshape(-1, 28, 28, 1)\n\n# label encoding \ny_train = to_categorical(y_train, num_classes = 10)\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=5)\n\nprint(f'X_train = {X_train.shape}')\nprint(f'test = {test.shape}')","df5b7c9f":"# Take a look at the data\nrows = 5 # defining no. of rows in figure\ncols = 6 # defining no. of colums in figure\n\nf = plt.figure(figsize=(2*cols,2*rows)) # defining a figure \n\nfor i in range(rows*cols): \n    f.add_subplot(rows,cols,i+1) # adding sub plot to figure on each iteration\n    plt.imshow(X_train[i].reshape([28,28]), cmap=\"Blues\") \n    plt.axis(\"off\")","e7cb4740":"# CNN\n\nmodel = Sequential([\n    layers.Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation ='relu', input_shape = (28,28,1)),\n    layers.Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation ='relu'),\n    layers.MaxPool2D(pool_size=(2,2)),\n    layers.Dropout(0.1),\n    \n    layers.Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation ='relu'),\n    layers.Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation ='relu'),\n    layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n    layers.Dropout(0.1),\n    \n    layers.Flatten(),\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(0.1),\n    layers.Dense(10, activation='softmax'),\n])","ef8a75af":"# optimizer \n\noptimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n#optimizer = RMSprop(learning_rate=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n\n# Compile the model\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","85af83c3":"# Set a learning rate annealer\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3,  \n                                            factor=0.5, \n                                            min_lr=0.00001)","827132d6":"# Data Augmentation\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # dimesion reduction\n        rotation_range=5,  # randomly rotate images in the range 5 degrees\n        zoom_range = 0.1, # Randomly zoom image 10%\n        width_shift_range=0.1,  # randomly shift images horizontally 10%\n        height_shift_range=0.1,  # randomly shift images vertically 10%\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\ndatagen.fit(X_train)","bd82c23b":"# Fit the model\n\nepochs = 35\nbatch_size = 90\n\nhistory = model.fit(datagen.flow(X_train,y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (X_test,y_test),\n                              verbose = 1, steps_per_epoch=X_train.shape[0] \/\/ batch_size,\n                              callbacks=[learning_rate_reduction])","fbe2e919":"# Plot the loss and val_loss along with accuracy ans val_accuracy\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot()","e5d13973":"# confusion matrix\n\n# Predict the values from the validation dataset\nY_pred = model.predict(X_test)\n\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n\n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_test,axis = 1) \n\n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\n\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","432d05bb":"# Display some error results \n\n# Errors are difference between predicted labels and true labels\nerrors = (Y_pred_classes - Y_true != 0)\n\nY_pred_classes_errors = Y_pred_classes[errors]\nY_pred_errors = Y_pred[errors]\nY_true_errors = Y_true[errors]\nX_val_errors = X_test[errors]\n\ndef display_errors(errors_index,img_errors,pred_errors, obs_errors):\n    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n    n = 0\n    nrows = 2\n    ncols = 3\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n            n += 1\n\n# Probabilities of the wrong predicted numbers\nY_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n\n# Predicted probabilities of the true values in the error set\ntrue_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n\n# Difference between the probability of the predicted label and the true label\ndelta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n\n# Sorted list of the delta prob errors\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\n\n# Top 6 errors \nmost_important_errors = sorted_dela_errors[-6:]\n\n# Show the top 6 errors\ndisplay_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)","2b06399a":"mnist = tf.keras.datasets.mnist\n(X_train_mnist, y_train_mnist), (X_val_mnist, y_val_mnist) = mnist.load_data()","dfd084d7":"y_train_mnist = np.array(pd.get_dummies(pd.Series(y_train_mnist)))\ny_test_mnist = np.array(pd.get_dummies(pd.Series(y_val_mnist)))","cd33d826":"X_train_mnist = X_train_mnist.reshape(-1, 28, 28, 1)\nX_test_mnist = X_val_mnist.reshape(-1, 28, 28, 1)\nX_train_mnist = X_train_mnist \/ 255\nX_test_mnist = X_test_mnist \/255","820e8e6b":"X_train_ext = np.concatenate((X_train, X_train_mnist), axis = 0)\nX_test_ext = np.concatenate((X_test, X_test_mnist), axis = 0)\ny_train_ext = np.concatenate((y_train, y_train_mnist), axis = 0)\ny_test_ext = np.concatenate((y_test, y_test_mnist), axis = 0)","4b2d3172":"epochs = 100\nbatch_size = 90\n\nhistory = model.fit(datagen.flow(X_train_ext,y_train_ext, batch_size=batch_size),\n                              epochs = epochs, validation_data = (X_test,y_test),\n                              verbose = 1, steps_per_epoch=X_train.shape[0] \/\/ batch_size,\n                              callbacks=[learning_rate_reduction])","d43a3b6e":"# Plot the loss and val_loss along with accuracy ans val_accuracy\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot()","c3a078db":"# confusion matrix\n\n# Predict the values from the validation dataset\nY_pred = model.predict(X_test)\n\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n\n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_test,axis = 1) \n\n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\n\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","11ed6edd":"# Display some error results \n\n# Errors are difference between predicted labels and true labels\nerrors = (Y_pred_classes - Y_true != 0)\n\nY_pred_classes_errors = Y_pred_classes[errors]\nY_pred_errors = Y_pred[errors]\nY_true_errors = Y_true[errors]\nX_val_errors = X_test[errors]\n\ndef display_errors(errors_index,img_errors,pred_errors, obs_errors):\n    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n    n = 0\n    nrows = 2\n    ncols = 3\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n            n += 1\n\n# Probabilities of the wrong predicted numbers\nY_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n\n# Predicted probabilities of the true values in the error set\ntrue_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n\n# Difference between the probability of the predicted label and the true label\ndelta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n\n# Sorted list of the delta prob errors\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\n\n# Top 6 errors \nmost_important_errors = sorted_dela_errors[-6:]\n\n# Show the top 6 errors\ndisplay_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)","ca4f0342":"pred = model.predict(test)\n\npred = np.argmax(pred, axis=1)\n\npred = pd.Series(pred, name='Label')\n\nsubmission = pd.concat([pd.Series(range(1, 28001), name='ImageID'), pred], axis=1)\nsubmission.to_csv('predictions11.csv', index=False)\n\nsubmission","b88503f6":"## 2 - Normalize, reshape, encode the output and split the data into train and test variables","46dc649d":"## Summary \n\nAs we can see, extending the data was an exelent choice, our predictions improved greatly","a851e45c":"## Submit the results","620d9d78":"## 6 - Augment the data\n\nTo avoid the overfitting problem, it's necessary to expand artificially our handwritten digit dataset. By altering the training data with small transformations we are able to reproduce the variations occuring when someone is writing a digit.\n\nMethods like the one described above that alter the training data in ways that change the array representation while keeping the label the same are known as \"data augmentation\" techniques. Some popular augmentations used are: grayscales, horizontal flips, vertical flips, random crops, color jitters, translations, rotations, and more.","cc8c00f3":"# Steps to create a CNN","568a8d6e":"## Aknowledgements\n\nThese four notebooks below taught me a lot about CNNs, they were, in my opinion, the most complete kernels about the topic in this competition.\n\n* https:\/\/www.kaggle.com\/yassineghouzam\/introduction-to-cnn-keras-0-997-top-6\n* https:\/\/www.kaggle.com\/cdeotte\/25-million-images-0-99757-mnist\n* https:\/\/www.kaggle.com\/cdeotte\/how-to-choose-cnn-architecture-mnist\n* https:\/\/www.kaggle.com\/kanncaa1\/convolutional-neural-network-cnn-tutorial","7d872071":"## 1 - Load the data and define the variables ","9de8cb03":"## 5 - Create an annealer\n\nIn order to make the olgorithm converges to the global minimum of the loss fuction, we will define an annealing method to the learning rate, meaning that the LR will decrease during trainning reaching the global minimum instead of falling into the local minima.\n\nWith the ReduceLROnPlateau function from Keras.callbacks, the LR will be reduced by half if the accuracy is not improved after 3 epochs.","7e1054c0":"## 3 - Create the CNN model\n\n* The kernel size does not need to be 3 by 3 matrix. It can be 5 by 5 or 7 by 7.\n* kernels detects features like edges or convex shapes. Example, if out input is dog, feature detector can detect features like ear or tail of the dog.\n* feature map = conv(input image, feature detector). Element wise multiplication of matrices. feature map = convolved feature\n* We reduce the size of image. This is important bc code runs faster. However, we lost information.\n* We create multiple feature maps bc we use multiple feature detectors(filters).","20cdfc36":"## 4 - Define the optimizer\n\nThe ADAM optimizer will be used, we are gonna change the learning rate, after that, compile the model.","29a7f449":"## 6 - Fit the model \n\nSay you have a dataset of 10 examples (or samples). You have a batch size of 2, and you've specified you want the algorithm to run for 3 epochs. Therefore, in each epoch, you have 5 batches (10\/2 = 5). Each batch gets passed through the algorithm, therefore you have 5 iterations per epoch.\n\nreference: https:\/\/stackoverflow.com\/questions\/4752626\/epoch-vs-iteration-when-training-neural-networks","e5df9be4":"![I-want-results--meme-8314.jpg](attachment:91b0169c-3078-4252-832b-ca9a3d02a730.jpg)\n\n## Let's plot the results and take a look at the confusion matrix","b042011c":"## Summary\n\nit seems that our CNN has some troubles with the 4 digits, they are misclassified as 9. Sometimes it is very difficult to catch the difference between 4 and 9 when the person has a terrible handwriting. Our algorithm goes through the same problems we do.\n\nLet's take a look at some errors we got","ac0ef637":"![better-results-1-e1532015332817.jpg](attachment:2f192882-7163-4bfa-91f3-3551ffda65c3.jpg)\n\n## Extend the data\n\nLet's extend the data, maybe our results are going to improve."}}