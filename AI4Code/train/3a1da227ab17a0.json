{"cell_type":{"b3d5a091":"code","d89c7187":"code","de674ec9":"code","ba25695b":"code","ad1e8085":"code","bd55eecd":"code","5d579ab9":"code","d9671acd":"code","fb5f054e":"code","a3339320":"code","edcd64d9":"code","ddf088fb":"code","a3261b23":"code","0e0b4f91":"code","4a7a0628":"code","713070ec":"code","e404d303":"code","bfe92f0a":"code","12d7b84f":"code","db0ac454":"code","9fd8da37":"code","fa398056":"code","e88ea066":"code","a0b547b0":"code","80b0777d":"code","ebdddda2":"code","8fdafcae":"code","5ef6d382":"code","ae84d245":"code","12884fd6":"code","f7e5dc48":"code","0011fb5a":"code","592fbcd5":"code","151eea9b":"code","3474bd03":"code","f6803753":"code","4d4aaa9b":"markdown","af7b9fec":"markdown","3d8a9698":"markdown","59951d62":"markdown","eb84176a":"markdown","42f4811c":"markdown","9028eb11":"markdown","2c0880ff":"markdown","3a8a4ca8":"markdown","614dc8f2":"markdown","1da4faa9":"markdown","6fff37a8":"markdown","c207296f":"markdown","d669e716":"markdown","1a5bab86":"markdown","7a10688f":"markdown","01abdbfd":"markdown","582a9f97":"markdown","b618f3b9":"markdown","fa2fa8bb":"markdown","2c9bd198":"markdown","00d0f4ce":"markdown","d51aa36b":"markdown","84614b76":"markdown","b595af7e":"markdown","c27fbe87":"markdown","0b7466cb":"markdown","49042970":"markdown","4ffe931b":"markdown","82452007":"markdown","08d43462":"markdown","4f2d9386":"markdown","923b1add":"markdown","785e5832":"markdown","aa8f985f":"markdown","6b613b78":"markdown","89a2e07e":"markdown","f5b52908":"markdown","6b5f26e5":"markdown","5cd88a0a":"markdown","76a865b3":"markdown","8267b12b":"markdown"},"source":{"b3d5a091":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nimport string\nimport re\nimport os\nimport random\nfrom tqdm import tqdm\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\n\nimport torch\nfrom torch import nn\nfrom torch import optim\n\n%matplotlib inline","d89c7187":"# load dataset\ntrain_df = pd.read_csv('..\/input\/quora-insincere-questions-classification\/train.csv')\ntest_df = pd.read_csv('..\/input\/quora-insincere-questions-classification\/test.csv')\nprint(\"Train shape: \", train_df.shape)\nprint(\"Test shape: \", test_df.shape)","de674ec9":"train_df.head(5)","ba25695b":"train_target = train_df['target'].values\nnp.unique(train_target)","ad1e8085":"print(\"Check for missing values in train set\")\nnull_check=train_df.isnull().sum()\nprint(null_check)\nprint(\"Check for missing values in test set\")\nnull_check=test_df.isnull().sum()\nprint(null_check)","bd55eecd":"target_0 = train_df.loc[train_df['target'] == 0].count()[0]\ntarget_1 = train_df.loc[train_df['target'] == 1].count()[0]\ncolors = ['r', 'b']\n\nplt.figure(figsize=(10,7))\nplt.subplot(1, 2, 1)\nplt.title('Question count')\nbar = plt.bar(['Sincere', 'Insincere'], [target_0, target_1])\n\nplt.subplot(1, 2, 2)\nplt.title('Question percentage')\npie = plt.pie([target_0, target_1], labels=['Sincere', 'Insincere'], autopct='%.2f %%')\nplt.show()","5d579ab9":"# Get stopwords from nltk lib\neng_stopwords = set(stopwords.words('english'))","d9671acd":"# Number of words in the text\ntrain_df[\"num_words\"] = train_df[\"question_text\"].apply(lambda x: len(str(x).split()))\n\n# Number of unique words in the text \ntrain_df[\"num_unique_words\"] = train_df[\"question_text\"].apply(lambda x: len(set(str(x).split())))\n\n# Number of characters in the text\ntrain_df[\"num_chars\"] = train_df[\"question_text\"].apply(lambda x: len(str(x)))\n\n# Number of stopwords in the text \ntrain_df[\"num_stopwords\"] = train_df[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n\n# Number of punctuations in the text\ntrain_df[\"num_punctuations\"] = train_df['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n# Number of upper case words in the text \ntrain_df[\"num_words_upper\"] = train_df[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n# Number of numbers in the text \ntrain_df['num_numbers'] = train_df['question_text'].apply(lambda x: sum(c.isdigit() for c in x))","fb5f054e":"train_df.describe().apply(lambda x: x.apply('{:.2f}'.format))","a3339320":"#truncation for better visuals\ntrain_df['num_words'].loc[train_df['num_words']>60] = 60 \ntrain_df['num_punctuations'].loc[train_df['num_punctuations']>10] = 10\ntrain_df['num_chars'].loc[train_df['num_chars']>350] = 350 \n\nf, axes = plt.subplots(2, 3, figsize=(20,10))\nsns.boxplot(x='target', y='num_words', data=train_df, ax=axes[0][0])\naxes[0][0].set_title(\"Number of words\")\n\nsns.boxplot(x='target', y='num_unique_words', data=train_df, ax=axes[0][1])\naxes[0][1].set_title(\"Number of unique words\")\n\nsns.boxplot(x='target', y='num_chars', data=train_df, ax=axes[0][2])\naxes[0][2].set_title(\"Number of characters\")\n\nsns.boxplot(x='target', y='num_stopwords', data=train_df, ax=axes[1][0])\naxes[1][0].set_title(\"Number of stopwords\")\n\nsns.boxplot(x='target', y='num_punctuations', data=train_df, ax=axes[1][1])\naxes[1][1].set_title(\"Number of punctuations\")\n\nsns.boxplot(x='target', y='num_words_upper', data=train_df, ax=axes[1][2])\naxes[1][2].set_title(\"Number of upper case words\")\nplt.show()","edcd64d9":"# split into Insincere and Sincere classes\ntrain1_df = train_df[train_df[\"target\"]==1]\ntrain0_df = train_df[train_df[\"target\"]==0]\n\n# n-grams count\ndef get_top_ngram(text, n=1):\n    # make word vector from corpus\n    vec = CountVectorizer(ngram_range=(n, n), stop_words='english').fit(text)\n    # corpus as input of CountVectorizer\n    bag_of_words = vec.transform(text)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:50]","ddf088fb":"f, axes = plt.subplots(1, 2, figsize=(20, 15))\nplt.suptitle(\"Unigram frequency\", fontsize=20)\n\n# uni-grams in Sincere class\ntop_uni_grams = get_top_ngram(train0_df['question_text'],n=1)\nx,y = map(list,zip(*top_uni_grams))\nsns.barplot(x=y,y=x,ax=axes[0])\naxes[0].set_title(\"Sincere\")\n\n# uni-grams in Insincere class\ntop_uni_grams = get_top_ngram(train1_df['question_text'],n=1)\nx,y = map(list,zip(*top_uni_grams))\nsns.barplot(x=y,y=x,ax=axes[1])\naxes[1].set_title(\"Insincere\")\nplt.show()","a3261b23":"f, axes = plt.subplots(1, 2, figsize=(20, 15))\nplt.suptitle(\"Bigram frequency\", fontsize=20)\n\n# bi-grams in Sincere class\ntop_bi_grams = get_top_ngram(train0_df['question_text'],n=2)\nx,y = map(list,zip(*top_bi_grams))\nsns.barplot(x=y,y=x,ax=axes[0])\naxes[0].set_title(\"Sincere\")\n\n# bi-grams in Insincere class\ntop_bi_grams = get_top_ngram(train1_df['question_text'],n=2)\nx,y = map(list,zip(*top_bi_grams))\nsns.barplot(x=y,y=x,ax=axes[1])\naxes[1].set_title(\"Insincere\")\nplt.show()","0e0b4f91":"from wordcloud import WordCloud, STOPWORDS\n\ndef plot_wordcloud(text, mask=None, max_words=100, max_font_size=100, figsize=(10.0,7.0), \n                   title=None, title_size=25):\n    stopwords = set(STOPWORDS)\n    \n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    plt.figure(figsize=figsize)\n    plt.imshow(wordcloud);\n    plt.title(title, fontdict={'size': title_size, 'color': 'black'})\n    plt.axis('off')\n    plt.show()\n    \nplot_wordcloud(train0_df['question_text'], title='Word Cloud of Sincere Questions')\nplot_wordcloud(train1_df['question_text'], title='Word Cloud of Insincere Questions')","4a7a0628":"mispell_dict = {\n    'grey': 'gray',\n    'litre': 'liter',\n    'labour': 'labor',\n    'favour': 'favor',\n    'colour': 'color',\n    'centre': 'center',\n    'honours': 'honor',\n    'theatre': 'theater',\n    'realise': 'realize',\n    'defence': 'defense',\n    'licence': 'license',\n    'analyse': 'analyze',\n    'practise': 'practice',\n    'behaviour': 'behavior',\n    'neighbour': 'neighbor',\n    'recognise': 'recognize',\n    'organisation':'organization',\n    'Qoura': 'Quora',\n    'quora': 'Quora',\n    'Quorans': 'Quoran',\n    'infty': 'infinity',\n    'judgement': 'judge',\n    'learnt': 'learn',\n    'modelling': 'model',\n    'cancelled': 'cancel',\n    'travelled': 'travel',\n    'travelling': 'travel',\n    'aluminium': 'alumini',\n    'counselling':'counseling',\n    'cheque': 'bill',\n    'upvote': 'agree',\n    'upvotes': 'agree',\n    'vape': 'cigarette',\n    'jewellery': 'jewell',\n    'Fiverr': 'freelance',\n    'programd': 'program',\n    'programme': 'program',\n    'programr': 'programer',\n    'programrs': 'programer',\n    'WeChat': 'socialmedia',\n    'Snapchat': 'socialmedia',\n    'Redmi': 'cellphone',\n    'Xiaomi': 'cellphone',\n    'OnePlus': 'cellphone',\n    'cryptos': 'crypto',\n    'bitcoin': 'crypto',\n    'Coinbase': 'crypto',\n    'bitcoins': 'crypto',\n    'ethereum': 'crypto',\n    'Ethereum': 'crypto',\n    'Blockchain': 'crypto',\n    'blockchain': 'crypto',\n    'cryptocurrency': 'crypto',\n    'cryptocurrencies': 'crypto',\n    '\u20b9': 'rupee',\n    'Brexit': 'Britain exit',\n    'Paytm': 'Pay Through Mobile',\n    'KVPY': 'Kishore Vaigyanik Protsahan Yojana',\n    'GDPR': 'General Data Protection Regulation',\n    'INTJ': 'Introversion Intuition Thinking Judgment',\n    \"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"'cause\": \"because\",\n    \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\", \"doesn't\": \"does not\",\n    \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n    \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"i'd\": \"i would\",\n    \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"I will have\", \"i'm\": \"i am\",\n    \"i've\": \"I have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\",\n    \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\n    \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\",\n    \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n    \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\",\n    \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n    \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n    \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\", \"so's\": \"so as\", \"this's\": \"this is\", \"that'd\": \"that would\",\n    \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n    \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n    \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n    \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\",\n    \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\",\n    \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n    \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\",\n    \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n    \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\n    \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\",\n    \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n    \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\",\n    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n    \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color',\n    'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling',\n    'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor',\n    'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize',\n    'youtu ': 'youtube ', 'qoura': 'quora', 'sallary': 'salary', 'whta': 'what',\n    'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can',\n    'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doi': 'do I',\n    'thebest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation',\n    'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis',\n    'etherium': 'ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017',\n    '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess',\n    \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization',\n    'demonitization': 'demonetization', 'demonetisation': 'demonetization'\n}\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\nmispellings, mispellings_re = _get_mispell(mispell_dict)\n\ndef clean_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)","713070ec":"# correct mispelling\ntrain_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_misspell(x))\ntest_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_misspell(x))","e404d303":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3',\n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', '\\xa0', '\\t',\n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', '\\u3000', '\\u202f',\n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', '\u00ab',\n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\n\n# replace puncts\ndef clean_puncts(text):\n    text = str(text)\n    for punct in \"\/-'\":\n        text = text.replace(punct, ' ')\n    for punct in puncts:\n        text = text.replace(punct, f' {punct} ')\n    return text","bfe92f0a":"train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_puncts(x))\ntest_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_puncts(x))","12d7b84f":"# replace numbers with \"#\"\ndef clean_numbers(text):\n    text = re.sub('[0-9]{5,}', '#####', text)\n    text = re.sub('[0-9]{4}', '####', text)\n    text = re.sub('[0-9]{3}', '###', text)\n    text = re.sub('[0-9]{2}', '##', text)\n    return text","db0ac454":"train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_numbers(x))\ntest_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_numbers(x))","9fd8da37":"\"\"\" # Save dataset\ntrain_df.to_csv('.\/train_fix.csv', index=False)\ntest_df.to_csv('.\/test_fix.csv', index=False)\n\"\"\"","fa398056":"\"\"\" # Load dataset\ntrain_df = pd.read_csv('.\/train_fix.csv')\ntest_df = pd.read_csv('.\/test_fix.csv')\nprint(\"Train shape: \", train_df.shape)\nprint(\"Test shape: \", test_df.shape)\n\"\"\"","e88ea066":"embed_size = 300 # how big is each word vector\nmax_features = 120000 # how many unique words to use\nmaxlen = 72 # max number of words in a question to use\nhidden_size = 64  # models hidden_size\nn_splits = 5   # number of folds in k-fold cv\nN_REPEATS = 3  # number of repeats\n\nbatch_size = 512\ntrain_epochs = 5\n\nSEED = 1029\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","a0b547b0":"train_X = train_df[\"question_text\"].values\ntest_X = test_df[\"question_text\"].values\n\n# Tokenize the corpus of both train and test set\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X))\ntrain_X = tokenizer.texts_to_sequences(train_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n# Pad the sentences of both train and test set\ntrain_X = pad_sequences(train_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)\n\n## Get the target values\ntrain_y = train_df['target'].values","80b0777d":"from zipfile import ZipFile\n\nzip_file = ZipFile('..\/input\/quora-insincere-questions-classification\/embeddings.zip', 'r')\nzip_file.namelist()","ebdddda2":"zip_file.extractall()\nglove_path = '.\/glove.840B.300d\/glove.840B.300d.txt'\nparagram_path =  '.\/paragram_300_sl999\/paragram_300_sl999.txt'\nwiki_news_path = '.\/wiki-news-300d-1M\/wiki-news-300d-1M.vec'","8fdafcae":"# load embedding functions taken from https:\/\/www.kaggle.com\/gmhost\/gru-capsule\ndef load_glove(word_index):\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(glove_path))\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    nb_words = min(max_features, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n            \n    return embedding_matrix \n\ndef load_fasttext(word_index):\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(wiki_news_path) if len(o)>100)\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    nb_words = min(max_features, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n\n    return embedding_matrix\n\ndef load_para(word_index):\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(paragram_path, encoding=\"utf8\", errors='ignore') if len(o)>100)\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    nb_words = min(max_features, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n    \n    return embedding_matrix","5ef6d382":"# load pre-trained embeddings and blend them\nglove_embedding = load_glove(tokenizer.word_index)\nfasttext_embedding = load_fasttext(tokenizer.word_index)\npara_embedding = load_para(tokenizer.word_index)\nembedding_matrix = np.mean([glove_embedding, fasttext_embedding, para_embedding], axis = 0)","ae84d245":"kfold = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED).split(train_X, train_y))","12884fd6":"# solution to ensure determinism in the results. From https:\/\/www.kaggle.com\/hengzheng\/pytorch-starter\ndef seed_torch(seed=1029):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","f7e5dc48":"class BiLSTM(nn.Module):\n    def __init__(self, num_layer=2, output_size=1):\n        super(BiLSTM, self).__init__()\n        self.hidden_size = hidden_size\n        \n        # Freeze embedding layer\n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = nn.Dropout2d(0.2)\n        # Modules\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layer, batch_first=True, bidirectional=True)\n        \n        self.linear = nn.Linear(hidden_size*2, 32)\n        self.batchnorm = nn.BatchNorm1d(num_features=32)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.2)\n        self.fc = nn.Linear(32, output_size)\n        # Skip sigmoid() because using BCEWithLogitsLoss() function\n\n    def forward(self, input):\n        # word embedding\n        x = self.embedding(input)   # batch_size x sentence_len x embed_size\n        x = torch.squeeze(self.embedding_dropout(torch.unsqueeze(x, 0)))\n        # LSTM\n        h, _ = self.lstm(x)       # batch_size x sentence_len x hidden_size*2\n        # take the last hidden state only\n        out = self.linear(h[:, -1, :])  # batch_size x sentence_len x 1\n        out = self.batchnorm(out)\n        out = self.relu(out)\n        out = self.dropout(out)\n        out = self.fc(out)\n        return out","0011fb5a":"x_test_cuda = torch.tensor(test_X, dtype=torch.long).to(device)\ntest = torch.utils.data.TensorDataset(x_test_cuda)\ntest_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n\ntest_preds_models = []\ntrain_preds_models = []\ntrain_losses_models = []\nval_losses_models = []\n\nfor rep_idx in range(N_REPEATS):\n    # TODO: loop for each repeat\n    print(f'Repeat {rep_idx + 1}')\n    # change seed after each repeat\n    seed_torch(SEED + rep_idx)\n    train_losses = []\n    val_losses = []\n    train_preds = np.zeros((len(train_X)))\n    test_preds = np.zeros((len(test_X)))\n    for i_fold, (train_idx, valid_idx) in enumerate(kfold):\n        # TODO: loop for each fold\n        x_train_fold = torch.tensor(train_X[train_idx], dtype=torch.long).to(device)\n        y_train_fold = torch.tensor(train_y[train_idx, np.newaxis], dtype=torch.float32).to(device)\n        x_val_fold = torch.tensor(train_X[valid_idx], dtype=torch.long).to(device)\n        y_val_fold = torch.tensor(train_y[valid_idx, np.newaxis], dtype=torch.float32).to(device)\n        # model initialize\n        model = BiLSTM().to(device)\n        # loss func and optimizer define\n        loss_fn = torch.nn.BCEWithLogitsLoss(reduction=\"sum\")\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n        # dataloader\n        train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n        valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n\n        print(f'Fold {i_fold + 1}')\n\n        for epoch in range(train_epochs):\n            # TODO: loop for each epoch\n            ## TRAIN\n            model.train()\n            avg_loss = 0.\n            for x_batch, y_batch in tqdm(train_loader, disable=True):\n                # forward\n                y_pred = model(x_batch)\n                # compute BCE loss\n                loss = loss_fn(y_pred, y_batch)\n                # zero the gradient \n                optimizer.zero_grad()\n                # backward and update params\n                loss.backward()\n                optimizer.step()\n                avg_loss += loss.item() \/ len(train_loader)\n            \n            ## VALID\n            model.eval()\n            valid_preds_fold = np.zeros((x_val_fold.size(0)))\n            test_preds_fold = np.zeros(len(test_X))\n            avg_val_loss = 0.\n            for i, (x_batch, y_batch) in enumerate(valid_loader):\n                y_pred = model(x_batch).detach()\n                avg_val_loss += loss_fn(y_pred, y_batch).item() \/ len(valid_loader)\n                valid_preds_fold[i * batch_size:(i+1) * batch_size] = torch.sigmoid(y_pred.cpu())[:, 0]\n\n            print('Epoch {}\/{} \\t loss={:.4f} \\t val_loss={:.4f}'.format(\n                epoch + 1, train_epochs, avg_loss, avg_val_loss))\n            train_losses.append(avg_loss)\n            val_losses.append(avg_val_loss)\n        \n        # predictions for test set\n        for i, (x_batch,) in enumerate(test_loader):\n            y_pred = model(x_batch).detach()\n            test_preds_fold[i * batch_size:(i+1) * batch_size] = torch.sigmoid(y_pred.cpu())[:, 0]\n\n        train_preds[valid_idx] = valid_preds_fold\n        test_preds += test_preds_fold \/ len(kfold)\n        train_losses_models.append(train_losses)\n        val_losses_models.append(val_losses)\n    test_preds_models.append(test_preds)\n    train_preds_models.append(train_preds)","592fbcd5":"train_loss_arr = []\nval_loss_arr = []\ntrain_losses = np.array(train_losses_models)\nval_losses = np.array(val_losses_models)\n# Average loss among repeats\ntrain_losses = np.mean(np.column_stack(train_losses), axis = 1)\nval_losses = np.mean(np.column_stack(val_losses), axis = 1)\n\n# The error would be the average loss of each epoch over K-folds\nfor i in range(train_epochs):\n    train_loss_arr.append(train_losses[i::train_epochs].mean(axis=0))\n    val_loss_arr.append(val_losses[i::train_epochs].mean(axis=0))\n\n# Models loss plot\ntrain_loss_arr = np.array(train_loss_arr)\nval_loss_arr = np.array(val_loss_arr)\nplt.plot(train_loss_arr, label='train loss')\nplt.plot(val_loss_arr, label='val loss')\nplt.title('Model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend()\nplt.show()","151eea9b":"def threshold_search(y_true, y_proba):\n    best_threshold = 0\n    best_score = 0\n    for threshold in tqdm([i * 0.01 for i in range(100)]):\n        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n        if score > best_score:\n            best_threshold = threshold\n            best_score = score\n    search_result = {'threshold': best_threshold, 'f1': best_score}\n    return search_result","3474bd03":"# average of repeats on train set\ntrain_predictions = np.mean(np.column_stack(train_preds_models), axis = 1)\n# search for best thresh hold - highest f1 score on train set\nsearch_result = threshold_search(train_y, train_predictions)\nsearch_result","f6803753":"# average of repeats\ntest_predictions = np.mean(np.column_stack(test_preds_models), axis = 1)\n# submission\nsub = pd.read_csv('..\/input\/quora-insincere-questions-classification\/sample_submission.csv')\nsub.prediction = test_predictions > search_result['threshold']\nsub.to_csv(\"submission.csv\", index=False)","4d4aaa9b":"T\u01b0\u01a1ng t\u1ef1, ta l\u1ea5y trung b\u00ecnh c\u1ee7a c\u00e1c d\u1ef1 \u0111o\u00e1n gi\u1eefa nh\u1eefng l\u1ea7n l\u1eb7p l\u1ea1i \u0111\u1ec3 l\u00e0m k\u1ebft qu\u1ea3 d\u1ef1 \u0111o\u00e1n tr\u00ean t\u1eadp test. Sau \u0111\u00f3 n\u1ed9p b\u00e0i.","af7b9fec":"Ways to improve the score:\n- Use some Statistical features for training model\n- Find better threshold search strategy\n- Fine-tuning pre-trained models: BERT, ELECTRA, etc. (but it violates the rule of competition: no using external data)\n- Data augment","3d8a9698":"B\u00e2y gi\u1edd, ch\u00fang ta s\u1ebd ph\u00e2n t\u00edch m\u1ed9t s\u1ed1 th\u00f4ng s\u1ed1 \u0111\u1eb7c tr\u01b0ng c\u1ee7a b\u1ed9 d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n:\n\n* S\u1ed1 l\u01b0\u1ee3ng t\u1eeb trong c\u00e2u\n* S\u1ed1 l\u01b0\u1ee3ng t\u1eeb ch\u1ec9 xu\u1ea5t hi\u1ec7n 1 l\u1ea7n trong c\u00e2u (unique words)\n* S\u1ed1 l\u01b0\u1ee3ng k\u00fd t\u1ef1 trong c\u00e2u\n* S\u1ed1 l\u01b0\u1ee3ng stopwords trong c\u00e2u (l\u00e0 c\u00e1c ch\u1eef nh\u01b0 \"a, an, the,...\")\n* S\u1ed1 l\u01b0\u1ee3ng d\u1ea5u c\u00e2u trong c\u00e2u (\"?, ., !,...\")\n* S\u1ed1 l\u01b0\u1ee3ng t\u1eeb \u0111\u01b0\u1ee3c vi\u1ebft hoa","59951d62":"## Gi\u1edbi thi\u1ec7u b\u00e0i to\u00e1n\n\nQuora l\u00e0 m\u1ed9t m\u1ea1ng x\u00e3 h\u1ed9i tr\u1ef1c tuy\u1ebfn n\u01a1i t\u1ea5t c\u1ea3 m\u1ecdi ng\u01b0\u1eddi \u0111\u01b0\u1ee3c ph\u00e9p \u0111\u0103ng t\u1ea3i c\u00e2u h\u1ecfi v\u00e0 nh\u1eadn c\u00e2u tr\u1ea3 l\u1eddi t\u01b0\u01a1ng \u1ee9ng, l\u00e0 n\u01a1i \u0111\u1ec3 m\u1ecdi ng\u01b0\u1eddi c\u00f3 th\u1ec3 c\u00f9ng trao \u0111\u1ed5i v\u00e0 chia s\u1ebb ki\u1ebfn th\u1ee9c.\n\nV\u00ec Quora l\u00e0 m\u1ed9t n\u01a1i d\u00e0nh cho t\u1ea5t c\u1ea3 m\u1ecdi ng\u01b0\u1eddi n\u00ean kh\u00f4ng th\u1ec3 tr\u00e1nh kh\u1ecfi c\u00e1c\u00a0c\u00e2u h\u1ecfi kh\u00f4ng c\u00f3 ch\u1ea5t l\u01b0\u1ee3ng hay th\u1eadm ch\u00ed l\u00e0 mang t\u00ednh ti\u00eau c\u1ef1c, kh\u00f4ng t\u1ed1t \u0111\u1eb9p. Nh\u1eefng c\u00e2u h\u1ecfi \u0111\u00f3 \u0111\u01b0\u1ee3c cho l\u00e0 kh\u00f4ng ch\u00e2n th\u00e0nh (Insincere). Ch\u00fang c\u00f3 \u0111\u1eb7c \u0111i\u1ec3m l\u00e0 ch\u1ec9 \u0111\u01b0a ra nh\u1eefng \u0111\u1ecbnh ki\u1ebfn c\u1ee7a b\u1ea3n th\u00e2n m\u00e0 kh\u00f4ng c\u00f3 m\u1ee5c \u0111\u00edch l\u00e0 \u0111i t\u00ecm c\u00e2u tr\u1ea3 l\u1eddi.\n\nB\u00e0i to\u00e1n \u0111\u1eb7t ra l\u00e0 ta c\u1ea7n ph\u00e2n lo\u1ea1i \u0111\u01b0\u1ee3c \u0111\u00e2u l\u00e0 nh\u1eefng c\u00e2u h\u1ecfi ch\u00e2n th\u00e0nh (Sincere) v\u00e0 kh\u00f4ng ch\u00e2n th\u00e0nh (Insincere).","eb84176a":"### D\u1eef li\u1ec7u \u0111\u01b0\u1ee3c cung c\u1ea5p\n\n![image.png](attachment:67be173c-b553-4b0f-be9d-d32aaf6010f8.png)","42f4811c":"### \u0110\u00e1nh gi\u00e1: F1 Score\n\nF1 Score d\u00f9ng l\u00e0m ti\u00eau ch\u00ed \u0111\u00e1nh gi\u00e1 cho b\u00e0i to\u00e1n. F1 Score c\u00f3 th\u1ec3 \u0111o s\u1ef1 c\u00e2n b\u1eb1ng gi\u1eefa precision v\u00e0 recall.\n\n$$ F1 = \\frac {2 * \\text{precision} * \\text{recall}} {\\text{precision} + \\text{recall}} $$","9028eb11":"## M\u00f4 h\u00ecnh\n\nV\u00ec d\u1eef li\u1ec7u l\u00e0 d\u1ea1ng chu\u1ed7i, n\u00ean ta s\u1ebd s\u1eed d\u1ee5ng **m\u1ea1ng n\u01a1-ron h\u1ed3i quy (Recurrent Neural Network - RNN)**.\n\nM\u1ea1ng RNN l\u00e0 m\u1ed9t lo\u1ea1i m\u1ea1ng trong \u0111\u00f3 hidden state t\u1eeb b\u01b0\u1edbc tr\u01b0\u1edbc (m\u00f4 t\u1ea3 ng\u1eef c\u1ea3nh c\u1ee7a chu\u1ed7i \u0111\u1ebfn th\u1eddi \u0111i\u1ec3m \u0111\u00f3) l\u00e0m \u0111\u1ea7u v\u00e0o cho b\u01b0\u1edbc hi\u1ec7n t\u1ea1i. Nh\u01b0ng m\u1ea1ng RNN c\u00f3 \u0111i\u1ec3m y\u1ebfu l\u00e0 kh\u00f4ng m\u00f4 t\u1ea3 h\u1ecdc \u0111\u01b0\u1ee3c chu\u1ed7i qu\u00e1 d\u00e0i do hi\u1ec7n t\u01b0\u1ee3ng vanishing gradient. M\u1ea1ng LSTM kh\u1eafc ph\u1ee5c ph\u1ea7n n\u00e0o nh\u01b0\u1ee3c \u0111i\u1ec3m n\u00e0y b\u1eb1ng c\u00e1ch cho ph\u00e9p th\u00f4ng tin lan truy\u1ec1n tr\u1ef1c ti\u1ebfp h\u01a1n th\u00f4ng qua m\u1ed9t bi\u1ebfn tr\u1ea1ng th\u00e1i \u00f4 (cell state). \n\nV\u00e0 \u0111\u1ec3 h\u1ecdc \u0111\u01b0\u1ee3c ng\u1eef c\u1ea3nh 2 chi\u1ec1u trong c\u00e2u, t\u1ee9c l\u00e0 nh\u00ecn v\u00e0o c\u1ea3 qu\u00e1 kh\u1ee9 v\u00e0 t\u01b0\u01a1ng lai c\u1ee7a t\u1eeb hi\u1ec7n t\u1ea1i, ta s\u1eed d\u1ee5ng m\u1ea1ng **Bidirectional LSTM**. Ngo\u00e0i ra, \u0111\u1ec3 h\u1ecdc \u0111\u01b0\u1ee3c c\u00e1c bi\u1ec3u di\u1ec5n tr\u1eebu t\u01b0\u1ee3ng h\u01a1n, ta tr\u1ed3ng c\u00e1c t\u1ea7ng BiLSTM l\u00ean nhau.","2c0880ff":"Thay v\u00ec ch\u1ec9 s\u1eed d\u1ee5ng m\u1ed9t b\u1ed9 embedding, ta tr\u1ed9n c\u00e1c b\u1ed9 embedding l\u1ea1i v\u1edbi nhau (l\u1ea5y gi\u00e1 tr\u1ecb trung b\u00ecnh c\u1ee7a c\u00e1c embedding). Em \u0111\u00e3 th\u1eed nghi\u1ec7m v\u00e0 nh\u1eadn th\u1ea5y khi tr\u1ed9n c\u00e1c b\u1ed9 embedding th\u00ec k\u1ebft qu\u1ea3 t\u1ed1t h\u01a1n l\u00e0 ch\u1ec9 s\u1eed d\u1ee5ng ri\u00eang m\u1ed9t b\u1ed9 embedding.","3a8a4ca8":"![image.png](attachment:61a55560-04b8-4657-abf1-8fe13f97404f.png)","614dc8f2":"Load b\u1ed9 embeddings t\u1eeb file zip","1da4faa9":"S\u1ed1 l\u01b0\u1ee3ng c\u00e2u h\u1ecfi kh\u00f4ng ch\u00e2n th\u00e0nh (Insincere) ch\u1ec9 chi\u1ebfm h\u01a1n 6% trong t\u1ed5ng s\u1ed1 c\u00e2u h\u1ecfi. Nh\u01b0 v\u1eady d\u1eef li\u1ec7u b\u1ecb l\u1ec7ch r\u1ea5t nhi\u1ec1u, v\u00ec th\u1ebf b\u00e0i to\u00e1n \u0111\u01b0\u1ee3c \u0111\u00e1nh gi\u00e1 d\u1ef1a tr\u00ean F1 Score. T\u1eadp d\u1eef li\u1ec7u kh\u00f4ng c\u00e2n b\u1eb1ng s\u1ebd kh\u00f4ng \u1ea3nh h\u01b0\u1edfng \u0111\u1ebfn \u0111i\u1ec3m s\u1ed1.","6fff37a8":"### Tokenizer and padding","c207296f":"Tr\u01b0\u1edbc ti\u00ean, ta s\u1eed d\u1ee5ng pandas \u0111\u1ec3 load file csv","d669e716":"## Hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh\n\n**Chi\u1ebfn thu\u1eadt hu\u1ea5n luy\u1ec7n**\n* **Stratified K-Fold Cross Validation**\n* S\u1eed d\u1ee5ng **DataLoader** c\u1ee7a pytorch \u0111\u1ec3 load d\u1eef li\u1ec7u v\u00e0o theo c\u00e1c batch\n* **Loss function: BCEWithLogitsLoss()** - c\u00e0i \u0111\u1eb7t c\u1ea3 sigmoid v\u00e0 h\u00e0m BCE loss\n* **Optimizer: Adam** v\u1edbi learning rate l\u00e0 0.001.\n* **Averaging over multiple seeds**: L\u1eb7p l\u1ea1i nhi\u1ec1u l\u1ea7n hu\u1ea5n luy\u1ec7n v\u1edbi c\u00e1c SEED kh\u00e1c nhau. Sau \u0111\u00f3 l\u1ea5y gi\u00e1 tr\u1ecb trung b\u00ecnh c\u1ee7a c\u00e1c k\u1ebft qu\u1ea3 d\u1ef1 \u0111o\u00e1n \u0111\u1ec3 l\u00e0m k\u1ebft qu\u1ea3 d\u1ef1 \u0111o\u00e1n cu\u1ed1i c\u00f9ng. Vi\u1ec7c n\u00e0y gi\u00fap cho model tr\u00e1nh b\u1ecb overfit v\u00e0o d\u1eef li\u1ec7u trong 1 l\u1ea7n train.","1a5bab86":"Nh\u1eadn x\u00e9t:\n\n* C\u00f3 nh\u1eefng t\u1eeb ph\u1ed5 bi\u1ebfn \u1edf c\u1ea3 2 class nh\u01b0: 'people', 'like', 'think', 'year old', 'united states'... $\\rightarrow$ Nh\u1eefng t\u1eeb n\u00e0y mang t\u00ednh trung l\u1eadp, kh\u00f4ng c\u00f3 m\u1ea5y t\u00e1c d\u1ee5ng \u0111\u1ec3 ph\u00e2n bi\u1ec7t\n* Nh\u1eefng t\u1eeb ch\u1ec9 ph\u1ed5 bi\u1ebfn \u1edf class Sincere: 'does', 'best', 'good',...\n* Nh\u1eefng t\u1eeb ch\u1ec9 ph\u1ed5 bi\u1ebfn \u1edf class Insincere: 'donald trump', 'women', 'men', 'white people', 'black people'... $\\rightarrow$ Li\u00ean quan \u0111\u1ebfn ch\u00ednh tr\u1ecb, ph\u00e2n bi\u1ec7t gi\u1edbi t\u00ednh, ph\u00e2n bi\u1ec7t ch\u1ee7ng t\u1ed9c,...","7a10688f":"### Stratified K-fold cross-validation cho t\u1eadp d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n\n\n**K-fold cross-validation**: Th\u1ef1c hi\u1ec7n vi\u1ec7c tr\u1ed9n dataset v\u00e0 chia b\u1ed9 dataset th\u00e0nh K nh\u00f3m (fold), sau \u0111\u00f3 th\u1ef1c hi\u1ec7n l\u1eb7p K l\u1ea7n. \u1ede m\u1ed7i v\u00f2ng l\u1eb7p, ta ch\u1ecdn kh\u00f4ng ho\u00e0n l\u1ea1i 1 nh\u00f3m \u0111\u1ec3 l\u00e0m t\u1eadp valid v\u00e0 (K-1) nh\u00f3m c\u00f2n l\u1ea1i \u0111\u1ec3 l\u00e0m t\u1eadp train. Model \u0111\u01b0\u1ee3c kh\u1edfi t\u1ea1o l\u1ea1i \u0111\u1ea7u m\u1ed7i v\u00f2ng l\u1eb7p. Ta gi\u1eef l\u1ea1i k\u1ebft qu\u1ea3 d\u1ef1 \u0111o\u00e1n c\u1ee7a model \u1edf t\u1eebng fold v\u00e0 cu\u1ed1i c\u00f9ng t\u00ednh trung b\u00ecnh d\u1ef1 \u0111o\u00e1n \u1edf c\u00e1c fold.\n\n**Stratified K-fold cross-validation**: Khi chia b\u1ed9 dataset, m\u1ed7i fold s\u1ebd c\u00f3 t\u1ec9 l\u1ec7 gi\u1eefa c\u00e1c class gi\u1ed1ng v\u1edbi t\u1ec9 l\u1ec7 \u1edf dataset ban \u0111\u1ea7u. \u0110\u01b0\u1ee3c s\u1eed d\u1ee5ng khi t\u1ec9 l\u1ec7 gi\u1eefa c\u00e1c class qu\u00e1 l\u1ec7ch trong b\u1ed9 dataset.","01abdbfd":"## L\u00e0m s\u1ea1ch d\u1eef li\u1ec7u\n\nT\u1eeb nh\u1eefng ph\u00e2n t\u00edch tr\u00ean, ta c\u1ea7n l\u00e0m s\u1ea1ch c\u00e1c k\u00fd t\u1ef1 nh\u01b0 d\u1ea5u c\u00e2u, s\u1ed1 \u0111\u1ebfm.\n\nB\u00ean c\u1ea1nh \u0111\u00f3, ta c\u1ea7n s\u1eeda l\u1ea1i nh\u1eefng t\u1eeb hay b\u1ecb sai ch\u00ednh t\u1ea3, nh\u1eefng contraction (v\u00ed d\u1ee5 nh\u01b0 \"aren't\" l\u00e0 \"are not\").\n\nTa s\u1ebd kh\u00f4ng s\u1eed d\u1ee5ng stemming (\u0111\u01b0a t\u1eeb v\u1ec1 d\u1ea1ng g\u1ed1c), kh\u00f4ng x\u00f3a c\u00e1c stopwords v\u00ec s\u1ebd s\u1eed d\u1ee5ng pre-trained embedding.","582a9f97":"![image.png](attachment:95cf7a39-05aa-474d-b1e8-0f1fe6d674e4.png)","b618f3b9":"Bi\u1ec3u \u0111\u1ed3 loss trong qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n","fa2fa8bb":"Mispelling","2c9bd198":"### Pre-trained Embeddings\n\nThay v\u00ec ph\u1ea3i h\u1ecdc c\u00e1ch m\u00e3 h\u00f3a c\u00e1c word id sang c\u00e1c vector t\u1eeb \u0111\u1ea7u, ta t\u1eadn d\u1ee5ng c\u00e1c word embedding \u0111\u00e3 \u0111\u01b0\u1ee3c train s\u1eb5n m\u00e0 \u0111\u1ec1 b\u00e0i cung c\u1ea5p. \u0110i\u1ec1u n\u00e0y c\u0169ng gi\u00fap c\u1ea3i thi\u1ec7n \u0111\u01b0\u1ee3c performance v\u00e0 gi\u1ea3m \u0111\u01b0\u1ee3c th\u1eddi gian training c\u1ee7a model.","00d0f4ce":"\u0110\u1ec1 b\u00e0i cung c\u1ea5p c\u00e1c b\u1ed9 pre-trained embedding: glove, paragram, fasttext (wiki-news) v\u00e0 word2vec (GoogleNews). \u1ede \u0111\u00e2y ta s\u1ebd s\u1eed d\u1ee5ng 3 b\u1ed9: glove, fasttext v\u00e0 paragram.","d51aa36b":"Hyperparams","84614b76":"## Chu\u1ea9n b\u1ecb d\u1eef li\u1ec7u","b595af7e":"**Ki\u1ebfn tr\u00fac c\u1ee7a m\u00f4 h\u00ecnh**\n\n* **Embedding layer** l\u00e0 b\u1ed9 pre-trained embedding \u0111\u01b0\u1ee3c \u0111\u00f3ng b\u0103ng (kh\u00f4ng \u0111\u01b0\u1ee3c c\u1eadp nh\u1eadt). \u0110\u1ea7u ra c\u00f3 s\u1eed d\u1ee5ng Dropout \u0111\u1ec3 tr\u00e1nh overfit.\n* **BiLSTM layer** v\u1edbi 2 t\u1ea7ng v\u00e0 m\u1ed7i t\u1ea7ng 128 units.\n* **M\u1ea1ng MLP** v\u1edbi h\u00e0m k\u00edch ho\u1ea1t l\u00e0 ReLU, c\u00f3 s\u1eed d\u1ee5ng Batchnorm v\u00e0 Dropout.\n\nTa skip h\u00e0m sigmoid \u1edf l\u1edbp cu\u1ed1i c\u00f9ng v\u00ec s\u1ebd s\u1eed d\u1ee5ng h\u00e0m loss l\u00e0 BCEWithLogitsLoss(), h\u00e0m n\u00e0y \u0111\u00e3 c\u00e0i \u0111\u1eb7t sigmoid s\u1eb5n sau \u0111\u00f3 t\u00ednh BCE, t\u1eadn d\u1ee5ng \u0111\u01b0\u1ee3c trick c\u00e0i \u0111\u1eb7t v\u1edbi s\u1ed1 m\u0169 $e$ n\u00ean k\u1ebft qu\u1ea3 s\u1ebd \u1ed5n \u0111\u1ecbnh h\u01a1n.","c27fbe87":"## Ph\u00e2n t\u00edch d\u1eef li\u1ec7u","0b7466cb":"Ta s\u1eed d\u1ee5ng th\u00eam m\u1ed9t s\u1ed1 bi\u1ec3u \u0111\u1ed3 \u0111\u1ec3 tr\u1ef1c quan h\u01a1n c\u00e1c \u0111\u1eb7c tr\u01b0ng","49042970":"Kh\u00f4ng c\u00f3 d\u1eef li\u1ec7u b\u1ecb thi\u1ebfu \u1edf c\u1ea3 t\u1eadp train v\u00e0 t\u1eadp test, t\u1ee9c l\u00e0 kh\u00f4ng c\u00f3 \u00f4 n\u00e0o trong b\u1ea3ng b\u1ecb tr\u1ed1ng c\u1ea3. Ta kh\u00f4ng c\u1ea7n x\u1eed l\u00fd nh\u1eefng d\u1eef li\u1ec7u thi\u1ebfu.\n\nTa ki\u1ec3m tra ph\u00e2n b\u1ed1 d\u1eef li\u1ec7u c\u1ee7a b\u1ed9 d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n","4ffe931b":"![image.png](attachment:0a2de1e2-9d9e-4863-9713-c3f81d114ed3.png)","82452007":"Nh\u1eadn x\u00e9t:\n* C\u00e2u h\u1ecfi kh\u00f4ng ch\u00e2n th\u00e0nh c\u00f3 nhi\u1ec1u ch\u1eef c\u00e1i, k\u00fd t\u1ef1 h\u01a1n.\n* C\u00f3 nhi\u1ec1u d\u1ea5u c\u00e2u, s\u1ed1 v\u00e0 ch\u1eef vi\u1ebft hoa l\u00e0m lo\u00e3ng d\u1eef li\u1ec7u, kh\u00f4ng gi\u00fap \u00edch cho model \u0111\u1ec3 ph\u00e2n lo\u1ea1i.","08d43462":"Trong \u0111\u00f3, gi\u00e1 tr\u1ecb loss l\u00e0 trung b\u00ecnh loss c\u1ee7a c\u00e1c l\u1ea7n l\u1eb7p.","4f2d9386":"Punctuations","923b1add":"\u0110\u1ec3 c\u00f3 th\u1ec3 \u0111\u01b0a d\u1eef li\u1ec7u v\u00e0o m\u00f4 h\u00ecnh, ta c\u1ea7n m\u1ed9t s\u1ed1 b\u01b0\u1edbc:\n* **tokenization**: T\u00e1ch c\u00e1c c\u00e2u trong corpus th\u00e0nh c\u00e1c token v\u00e0 g\u00e1n id (m\u1ed9t s\u1ed1 t\u1ef1 nhi\u00ean) cho t\u1eebng token. M\u1ed9t token \u1edf \u0111\u00e2y l\u00e0 m\u1ed9t t\u1eeb. Nh\u01b0 v\u1eady, t\u1eeb m\u1ed9t c\u00e2u v\u0103n s\u1ebd \u0111\u01b0\u1ee3c d\u1ecbch th\u00e0nh m\u1ed9t d\u00e3y c\u00e1c s\u1ed1 t\u1ef1 nhi\u00ean - l\u00e0 d\u00e3y c\u00e1c id c\u1ee7a t\u1eeb c\u00f3 m\u1eb7t trong c\u00e2u.\n* **padding**: Th\u00eam c\u00e1c \u00f4 tr\u1ed1ng v\u00e0o \u0111\u1ea7u c\u00e2u \u0111\u1ec3 c\u00e1c c\u00e2u trong corpus c\u00f3 c\u00f9ng \u0111\u1ed9 d\u00e0i.\n* **word embedding**: M\u00e3 h\u00f3a c\u00e1c word th\u00e0nh m\u1ed9t vector s\u1ed1 th\u1ef1c \u0111\u1ec3 model c\u00f3 th\u1ec3 x\u1eed l\u00fd \u0111\u01b0\u1ee3c. Gi\u1ed1ng nh\u01b0 m\u1ed9t cu\u1ed1n t\u1eeb \u0111i\u1ec3n c\u1ee7a model, d\u1ecbch t\u1eeb c\u00e1c word id sang c\u00e1c vector s\u1ed1 th\u1ef1c m\u00e0 model c\u00f3 th\u1ec3 hi\u1ec3u. Sau qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n, l\u1edbp embedding c\u00f3 th\u1ec3 h\u1ecdc \u0111\u01b0\u1ee3c c\u00e1c ng\u1eef ngh\u0129a c\u1ee7a t\u1eeb, c\u00e1c t\u1eeb g\u1ea7n ngh\u0129a nhau s\u1ebd c\u00f3 vector gi\u00e1 tr\u1ecb g\u1ea7n gi\u1ed1ng nhau.","785e5832":"Sau khi m\u00f4 h\u00ecnh \u0111\u00e3 \u0111\u01b0\u1ee3c hu\u1ea5n luy\u1ec7n, ta t\u00ecm threshold m\u00e0 model c\u00f3 F1 score t\u1ed1t nh\u1ea5t (\u0111\u1ed1i v\u1edbi t\u1eadp train).","aa8f985f":"Numbers","6b613b78":"V\u1ec1 t\u1ed5ng quan, b\u1ed9 d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n g\u1ed3m 3 c\u1ed9t:\n* **qid**: ID c\u1ee7a c\u00e2u h\u1ecfi\n* **question_text**: d\u1eef li\u1ec7u d\u1ea1ng chu\u1ed7i, l\u00e0 n\u1ed9i dung c\u00e2u h\u1ecfi c\u1ea7n ph\u00e2n lo\u1ea1i\n* **target**: ch\u1ec9 g\u1ed3m 2 gi\u00e1 tr\u1ecb l\u00e0 0 v\u00e0 1, l\u00e0 k\u1ebft qu\u1ea3 cho th\u1ea5y c\u00e2u h\u1ecfi kh\u00f4ng ch\u00e2n th\u00e0nh (1) ho\u1eb7c ch\u00e2n th\u00e0nh (0)\n\nTrong \u0111\u00f3, ta quan t\u00e2m \u0111\u1ebfn 2 c\u1ed9t ch\u00ednh l\u00e0 \"question_text\" v\u00e0 \"target\"\n\nT\u1eadp train g\u1ed3m h\u01a1n 1,3 tri\u1ec7u c\u00e2u h\u1ecfi v\u00e0 t\u1eadp test g\u1ed3m h\u01a1n 375 ngh\u00ecn c\u00e2u h\u1ecfi.","89a2e07e":"## B\u00c0I T\u1eacP L\u1edaN M\u00d4N H\u1eccC M\u00c1Y\n\n**Quora Insincere Question Classification**\n\nL\u1edbp: INT3405E_20\n\nH\u1ecd v\u00e0 t\u00ean: Nguy\u1ec5n \u0110\u1ee9c Huy\n\nMSSV: 19021299","f5b52908":"S\u1eed d\u1ee5ng wordcloud \u0111\u1ec3 d\u1eef li\u1ec7u th\u00eam tr\u1ef1c quan","6b5f26e5":"L\u1ea5y trung b\u00ecnh c\u1ee7a c\u00e1c d\u1ef1 \u0111o\u00e1n gi\u1eefa nh\u1eefng l\u1ea7n l\u1eb7p \u0111\u1ec3 l\u00e0m k\u1ebft qu\u1ea3 d\u1ef1 \u0111o\u00e1n.","5cd88a0a":"![image.png](attachment:200e63e2-19da-4f1b-a674-4edb32099a0a.png)","76a865b3":"B\u00e0i to\u00e1n l\u00e0 binary classification n\u00ean ta s\u1ebd s\u1eed d\u1ee5ng m\u1ed9t output n\u01a1-ron \u1edf l\u1edbp cu\u1ed1i c\u00f9ng \u0111\u1ec3 l\u1ea5y k\u1ebft qu\u1ea3 t\u1eeb LSTM v\u00e0 \u0111\u01b0a ra m\u1ed9t gi\u00e1 tr\u1ecb logits, sau \u0111\u00f3 \u0111\u01b0a qua h\u00e0m sigmoid \u0111\u1ec3 nh\u1eadn \u0111\u01b0\u1ee3c k\u1ebft qu\u1ea3 ph\u00e2n l\u1edbp cu\u1ed1i c\u00f9ng. \u1ede \u0111\u00e2y ch\u1ec9 l\u1ea5y hidden state cu\u1ed1i c\u1ee7a LSTM \u0111\u1ec3 l\u00e0m k\u1ebft qu\u1ea3 \u0111\u1ec3 ph\u00e2n l\u1edbp.\n\nTa th\u00eam m\u1ed9t l\u1edbp MLP tr\u01b0\u1edbc khi \u0111\u01b0a k\u1ebft qu\u1ea3 v\u00e0o l\u1edbp output.","8267b12b":"B\u00e2y gi\u1edd, ta s\u1ebd ph\u00e2n t\u00edch t\u1ea7n su\u1ea5t c\u1ee7a c\u00e1c n-grams xu\u1ea5t hi\u1ec7n nhi\u1ec1u trong b\u1ed9 d\u1eef li\u1ec7u. (n-grams l\u00e0 m\u1ed9t c\u1ee5m t\u1eeb c\u1ea5u t\u1ea1o b\u1edfi n t\u1eeb)\n\n\u1ede \u0111\u00e2y ta s\u1ebd s\u1eed d\u1ee5ng CountVectorizer c\u1ee7a th\u01b0 vi\u1ec7n sklearn. CountVectorizer c\u00f3 nhi\u1ec7m v\u1ee5 l\u00e0 vector h\u00f3a corpus (g\u00e1n id cho c\u00e1c t\u1eeb c\u00f3 m\u1eb7t trong corpus) v\u00e0 bi\u1ebfn \u0111\u1ed5i m\u1ed9t chu\u1ed7i \u0111\u1ea7u v\u00e0o th\u00e0nh ma tr\u1eadn s\u1ed1 \u0111\u1ebfm d\u1ef1a tr\u00ean c\u00e1c t\u1eeb c\u00f3 trong corpus. Nh\u01b0 v\u1eady ta c\u00f3 th\u1ec3 t\u1eadn d\u1ee5ng CountVectorizer \u0111\u1ec3 \u0111\u1ebfm s\u1ed1 n-grams b\u1eb1ng c\u00e1ch d\u00f9ng corpus \u0111\u1ec3 v\u1eeba l\u00e0m word vector v\u00e0 v\u1eeba l\u00e0m chu\u1ed7i \u0111\u1ea7u v\u00e0o cho CountVectorizer."}}