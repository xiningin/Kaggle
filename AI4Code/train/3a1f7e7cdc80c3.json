{"cell_type":{"4730ef72":"code","7f70eb00":"code","27265c49":"code","4ffa7f87":"code","1f849d58":"code","645460f5":"code","91c93b79":"code","16143aed":"code","94557134":"code","a3daebb5":"code","26510eae":"code","091860f7":"code","883504c9":"code","30350d71":"code","cfd5ec67":"code","502e3c93":"code","f06910a7":"code","986ee301":"code","b33ff498":"code","c15e118f":"code","66289134":"code","c1d6406a":"code","8830c38b":"code","e13fcc3b":"code","9e3ad66f":"code","3bd05cd5":"code","bcc18c77":"code","b4290ac7":"code","816eaeb0":"code","73a1a40d":"code","e3ced7f7":"code","f372cb7e":"code","31f03f1a":"code","7069cf39":"code","fd3550c6":"code","70fb6d9f":"code","4b5f548e":"code","4d77ae8f":"code","035420f7":"code","f8d69f15":"code","08db0331":"code","e9c2dbc8":"code","fa7ea7b2":"code","c3887495":"markdown","f5c86247":"markdown","775a664d":"markdown","7df25269":"markdown","958bd9a5":"markdown","75379f76":"markdown","9b244061":"markdown","93ed9c25":"markdown","6f5ed86d":"markdown","24a75d7c":"markdown","62836ff6":"markdown","566bc89a":"markdown","26c7adff":"markdown","5c0c693d":"markdown","d687a4b0":"markdown"},"source":{"4730ef72":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import confusion_matrix, cohen_kappa_score, classification_report\nfrom sklearn.metrics import r2_score, roc_auc_score\nfrom sklearn.metrics import average_precision_score, auc, roc_curve, precision_recall_curve\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nfrom imblearn.over_sampling import SMOTE\n\nimport featuretools as ft\nimport gc\n\n%matplotlib inline\nsns.set(style='whitegrid', palette='inferno', font_scale=1.5)\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")","7f70eb00":"# load the data\ndata = pd.read_csv(\"..\/input\/creditcard.csv\")","27265c49":"# get column names\ncolNames = data.columns.values\ncolNames","4ffa7f87":"# get dataframe dimensions\nprint (\"Dimension of dataset:\", data.shape)","1f849d58":"# get attribute summaries\nprint(data.describe())","645460f5":"# get class distribution\nprint (\"Normal transaction:\", data['Class'][data['Class']==0].count()) #class = 0\nprint (\"Fraudulent transaction:\", data['Class'][data['Class']==1].count()) #class = 1","91c93b79":"sns.countplot(data['Class'])","16143aed":"# separate classes into different datasets\nnormal_class = data.query('Class == 0')\nfraudulent_class = data.query('Class == 1')\n\n# randomize the datasets\nnormal_class = normal_class.sample(frac=1,random_state=1210)\nfraudulent_class = fraudulent_class.sample(frac=1,random_state=1210)","94557134":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15,9))\nf.suptitle('Time of transaction vs Amount by class')\n\nax1.scatter(fraudulent_class.Time, fraudulent_class.Amount)\nax1.set_title('Fraud')\n\nax2.scatter(normal_class.Time, normal_class.Amount)\nax2.set_title('Normal')\n\nplt.xlabel('Time (in Seconds)')\nplt.ylabel('Amount')\nplt.show()","a3daebb5":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15,9))\nf.suptitle('Amount per transaction by class')\n\nbins = 50\n\nax1.hist(fraudulent_class.Amount, bins = bins)\nax1.set_title('Fraud')\n\nax2.hist(normal_class.Amount, bins = bins)\nax2.set_title('Normal')\n\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.xlim((0, 20000))\nplt.yscale('log')\nplt.show();","26510eae":"data = data.drop(['Time'], axis=1)\ndata['Amount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))","091860f7":"# separate classes into different datasets\nnormal_class = data.query('Class == 0')\nfraudulent_class = data.query('Class == 1')\n\n# randomize the datasets\nnormal_class = normal_class.sample(frac=1,random_state=1210)\nfraudulent_class = fraudulent_class.sample(frac=1,random_state=1210)","883504c9":"X = data.drop(['Class'], axis = 1)\n\ny = data['Class']","30350d71":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1210)","cfd5ec67":"X_train.head()","502e3c93":"gc.collect()","f06910a7":"def score(model, test = X_test, y_true = y_test):\n    \n    pred = model.predict(test)\n\n    print('Average precision-recall score RF:\\t', round(average_precision_score(y_true, pred),4)*100)\n    print()\n    print(\"Cohen's Kappa Score:\\t\",round(cohen_kappa_score(y_true,pred),4)*100)\n    print()\n    print(\"R-Squared Score:\\t\",round(r2_score(y_true,pred),4)*100)\n    print()\n    print(\"Area Under ROC Curve:\\t\",round(roc_auc_score(y_true,pred),4)*100)\n    print()\n    print(classification_report(y_true,pred))\n    \n    \n    precision, recall, _ = precision_recall_curve(y_true, pred)\n\n    plt.step(recall, precision, color='b', alpha=0.2, where='post')\n    \n    plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.ylim([0.0, 1.05])\n    plt.xlim([0.0, 1.0])\n    plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(average_precision_score(y_true, pred)))\n    \n    \n    \n    fpr_rf, tpr_rf, _ = roc_curve(y_true, pred)\n    roc_auc_rf = auc(fpr_rf, tpr_rf)\n    plt.figure(figsize=(8,8))\n    plt.xlim([-0.01, 1.00])\n    plt.ylim([-0.01, 1.01])\n    plt.step(fpr_rf, tpr_rf, lw=1, label='{} curve (AUC = {:0.2f})'.format('RF',roc_auc_rf))\n    #plt.fill_between(fpr_rf, tpr_rf, step='post', alpha=0.2, color='b')\n\n\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.title('ROC curve', fontsize=16)\n    plt.legend(loc='lower right', fontsize=13)\n    plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n    plt.axes().set_aspect('equal')\n    plt.show()\n    ","986ee301":"def plot_2d_space(X, y, label='Classes'):   \n    colors = ['#1F77B4', '#FF7F0E']\n    markers = ['o', 's']\n    plt.figure(figsize=(12, 9), dpi=80)\n    for l, c, m in zip(np.unique(y), colors, markers):\n        plt.scatter(X[y==l, 0], X[y==l, 1], c=c, label=l, marker=m)\n    plt.title(label)\n    plt.legend(loc='upper right')\n    plt.show()","b33ff498":"#np.unique(y_train, return_counts= True)\nsns.countplot(y_train)","c15e118f":"plot_2d_space(np.array(X), np.array(y), 'Before SMOTE over-sampling')","66289134":"smote = SMOTE(ratio='minority', random_state=1210)\nX_sm, y_sm = smote.fit_sample(X_train, y_train)\n\n#np.unique(y_sm, return_counts= True)\nsns.countplot(y_sm)","c1d6406a":"plot_2d_space(X_sm, y_sm, 'After SMOTE over-sampling')","8830c38b":"# See category counts for test data\ncategory, records = np.unique(y_test, return_counts= True)\ncat_counts = dict(zip(category,records))\n\nprint(cat_counts)\nsns.countplot(y_test)","e13fcc3b":"rf_model = RandomForestClassifier(n_estimators=500,verbose=1,n_jobs=8)","9e3ad66f":"rf_model.fit(X_sm,y_sm)","3bd05cd5":"score(rf_model)","bcc18c77":"xgb_model = XGBClassifier(n_estimators=500, n_jobs=8)\n\nxgb_model.fit(X_sm,y_sm)","b4290ac7":"score(xgb_model, test= np.array(X_test))","816eaeb0":"lr_model = LogisticRegression(max_iter=1000)\n\nlr_model.fit(X_sm,y_sm)","73a1a40d":"score(lr_model)","e3ced7f7":"import lightgbm","f372cb7e":"lgbm = lightgbm.LGBMClassifier(n_estimators=1000, verbose=1)","31f03f1a":"lgbm.fit(X_sm, y_sm)","7069cf39":"score(lgbm)","fd3550c6":"normal_class.head(3)","70fb6d9f":"fraudulent_class.head(3)","4b5f548e":"resampled = normal_class.sample(n=int(len(fraudulent_class)*3), random_state=1210)","4d77ae8f":"len(resampled)","035420f7":"data = pd.concat([fraudulent_class,resampled])","f8d69f15":"X_tr, X_te, y_tr, y_te = train_test_split(data.drop('Class',axis=1), data['Class'], test_size=0.2, random_state=1210)","08db0331":"sns.countplot(data['Class'])","e9c2dbc8":"score(RandomForestClassifier(n_estimators=500,random_state=1210).fit(X_tr,y_tr),test=X_te, y_true=y_te)","fa7ea7b2":"score(lightgbm.LGBMClassifier(n_estimators=5000, random_state=1210).fit(X_tr,y_tr),test=X_te, y_true=y_te)","c3887495":"# Importing required Libraries","f5c86247":"### Random Forest Classifier","775a664d":"# Context\nIt is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n\n### Content\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n\n### Acknowledgements\nThe dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http:\/\/mlg.ulb.ac.be) of ULB (Universit\u00e9 Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on http:\/\/mlg.ulb.ac.be\/BruFence and http:\/\/mlg.ulb.ac.be\/ARTML\n\nPlease cite: Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015","7df25269":"# Oversampling to deal with class imbalance\n\nThe examples of the majority class, in this case the normal transactions, drastically outnumber the \nincidences of fraudulent transactions in our dataset. One of the strategies employed in the data science community is \nto generate synthetic data points for under-represented class to improve the learning function.","958bd9a5":"### The best results are 92% Recall, 97% Precision with Area Under Precision-Recall Curve = 91.64%","75379f76":"# Exploratory Data Analysis","9b244061":"# Time to train and test the performance of various models","93ed9c25":"# Time to try Random Under-Sampling","6f5ed86d":"We can see that SMOTE doesn't give us very good results no matter which algorithm we try.\nI believe this is because we don't have enough **actual** fraudulent samples and the patterns just get lost in between so many non-fraudulent transaction samples.","24a75d7c":"### The above graph shows that **Time** is irrelevent for detecting fraudulent transactions","62836ff6":"### Logistic Regression","566bc89a":"### The above graph shows that most of the fraudulent transactions are of very low amount","26c7adff":"# Results","5c0c693d":"## Light GBM","d687a4b0":"### XGBoost Classifier"}}