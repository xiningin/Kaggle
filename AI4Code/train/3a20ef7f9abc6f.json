{"cell_type":{"a13ecc4c":"code","55565023":"code","1019b061":"code","ebdda760":"code","da0d6d7f":"code","78e88e7a":"code","2423258c":"code","4dc364e2":"code","c9ad59c0":"code","ee5022be":"code","4f0ff7cc":"code","7cbd48d2":"code","3220ac53":"code","f32e82fb":"code","9bd4948f":"code","81afb253":"code","4450854a":"code","f55dcf98":"code","6f3a328c":"code","29795137":"code","6a031c09":"code","a8a0c6f1":"code","26bfd06c":"code","93fa44cf":"code","6d2fe19f":"code","b60c9864":"code","fd80b712":"code","9be0bdc2":"code","c73ad941":"code","a0e22067":"code","257a4178":"code","d4b17da8":"code","69d18c07":"code","55a0f956":"markdown","13f46b88":"markdown","0686a80a":"markdown","b99f1c1d":"markdown","302b991a":"markdown","b1eb0462":"markdown","efb165a6":"markdown","6844f0c7":"markdown","7f28d4e7":"markdown","080dd7dc":"markdown","4306df8e":"markdown","7a273b90":"markdown"},"source":{"a13ecc4c":"import numpy as np, pandas as pd\nimport matplotlib.pyplot as plt, seaborn as sns\n%matplotlib inline","55565023":"df=pd.read_csv(\"..\/input\/mall-customers-kmeans-pcacsv\/Mall_Customers_Kmeans_PCA.csv\")","1019b061":"df.head()","ebdda760":"df.info()","da0d6d7f":"## For convinence rename the attributes Annual INcome(k$) and Spending Score (1-100)\n\ndf.rename({'Annual Income (k$)':'Income',  \\\n          'Spending Score (1-100)':'Spend_Score'}, axis=1,inplace=True)","78e88e7a":"df.head()","2423258c":"## To get a high-level understanding on the numerical fields and print out the descriptive summary, lets use describe method()\n\ndf.describe()","4dc364e2":"sns.histplot(data=df, x=\"Income\")","c9ad59c0":"## Creating three clusters based on the above understanding\n\ndf['Cluster']=np.where(df.Income >= 90, 'High_Earners',\n                      np.where(df.Income < 50, 'Low_Income', 'Moderate_Income'))","ee5022be":"df.groupby('Cluster')['Income'].describe()","4f0ff7cc":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()","7cbd48d2":"columns_to_scale = ['Age','Income','Spend_Score']\ndata_scaled=df.copy()","3220ac53":"data_scaled[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])","f32e82fb":"data_scaled[columns_to_scale].describe()","9bd4948f":"sel_cols=['Income','Spend_Score']\ncust3=data_scaled[sel_cols].head(3)\ncust3","81afb253":"from scipy.spatial.distance import cdist\n","4450854a":"cdist(cust3, cust3, metric='euclidean')","f55dcf98":"# verify the Eucliean distance between Income and Spend_Score\n\nnp.sqrt((-1.739+1.739)**2 + (-0.4348-1.1957)**2)","6f3a328c":"cluster_cols=['Income','Spend_Score']\ncust4=data_scaled[cluster_cols].head(3)\ncust4","29795137":"data_scaled.plot.scatter(x='Income', y='Spend_Score', \\\n                         color='maroon')\n\nprint(\"From the plot, there are 5 natural clusters in the data. This tells us that we need to specify 5 as the number of clusters for the k-means algorithm\")","6a031c09":"from sklearn.cluster import KMeans\nmodel=KMeans(n_clusters=5, random_state=42)\n\nprint(\"The model instance is created. Note that no clustering has been performed on the data yet. n_clusters specifies the number of clusters to create as 5. Specifying a random state ensures that we get the same results on repeated executions since the k-means algorithm is not deterministic. 42 is an arbitrary choice\")","a8a0c6f1":"model.fit(data_scaled[cluster_cols])\ndata_scaled['Clusters']=model.predict(data_scaled[cluster_cols])\nprint(\"fit the model on the data using the columns in cluster_cols for the purpose. Using the predict method of the k-means model, assign the cluster for each customer to the 'Cluster' variable. Print the first three records of the data_scaled dataset\")","26bfd06c":"data_scaled.head(3)","93fa44cf":"markers = ['x', '*', '.','|','_']\n\nfor clust in range(5):\n    temp = data_scaled[data_scaled.Clusters == clust]\n    plt.scatter(temp.Income, temp.Spend_Score, \\\n                marker=markers[clust], \\\n                color = 'gray',\\\n                label=\"Cluster \"+str(clust))\nplt.xlabel('Income')\nplt.ylabel('Spend_score')\nplt.legend()\nplt.show()","6d2fe19f":"df['Clusters']=data_scaled.Clusters\ndf.groupby('Clusters')[['Income','Spend_Score']].mean().plot.bar(color=['gray','black'])\nplt.show()","b60c9864":"cluster_cols = ['Age','Income','Spend_Score'] \ndata_scaled[cluster_cols].head(3)  ## Remember we already scaled these attributes","fd80b712":"model = KMeans(n_clusters=4, random_state=42)\nmodel.fit(data_scaled[cluster_cols])\n\ndata_scaled['Clusters_4'] = model.predict(data_scaled[cluster_cols])","9be0bdc2":"data_scaled","c73ad941":"from sklearn import decomposition\n\npca=decomposition.PCA(n_components=2)\n\npca_res=pca.fit_transform(data_scaled[cluster_cols])\n\ndata_scaled['pc1'] = pca_res[:,0]\ndata_scaled['pc2'] = pca_res[:,1]\n","a0e22067":"markers = ['x', '*', 'o','|']\n\nfor clust in range(4):\n    temp = data_scaled[data_scaled.Clusters_4 == clust]\n    plt.scatter(temp.pc1, temp.pc2, marker=markers[clust], \\\n                label=\"Cluster \"+str(clust), \\\n                color='maroon')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.show()","257a4178":"data_scaled","d4b17da8":"df['Clusters_4']=data_scaled.Clusters_4\ndf.groupby('Clusters_4')[['Age','Income','Spend_Score']].mean()","69d18c07":"df.groupby('Clusters_4')[['Age','Income','Spend_Score']].mean().plot.bar(color=['lightgray','darkgray','black'])\nplt.show()","55a0f956":"One way to describe the clusters is as follows: \n\nCluster 0: Middle-aged penny pinchers (high income, low spend)\n\nCluster 1: Young high rollers (younger age, high income, high spend)\n\nCluster 2: Young aspirers (low income, high spend)\n\nCluster 3: Old average Joes (average income, average spend","13f46b88":"#### Standardising the numerical attributes","0686a80a":"K-means clustering is a very common unsupervised learning technique with a wide \nrange of applications. It is powerful because it is conceptually relatively simple, scales \nto very large datasets, and tends to work well in practice. \n\nIn this section, you will learn \n1) the conceptual foundations of k-means clustering,\n\n2) how to apply k-means clustering to data\n\n3) how to deal with high-dimensional data (that is, data with many different variables) in the context of clustering","b99f1c1d":"K-means clustering is an algorithm that tries to find the best way of grouping data \npoints into k different groups, where k is a parameter given to the algorithm. For \nnow, we will choose k arbitrarily. We will revisit how to choose k in practice in the next \nchapter. The algorithm then works iteratively to try to find the best grouping. There \nare two steps to this algorithm:\n1. The algorithm begins by randomly selecting k points in space to be the centroids of the clusters. Each data point is then assigned to the centroid that is closest to it.\n2. The centroids are updated to be the mean of all of the data points assigned to them. The data points are then reassigned to the centroid closest to them.\n\nStep 2 is repeated until none of the data points change the centroid they are assigned \nto after the centroid is updated.\n\nOne point to note here is that this algorithm is not deterministic, that is, the outcome \nof the algorithm depends on the starting locations of the centroids. Therefore, it is \nnot always guaranteed to find the best grouping. However, in practice, it tends to find \ngood groupings while still being computationally inexpensive even for large datasets. \nK-means clustering is fast and easily scalable and is, therefore, the most common \nclustering algorithm used.","302b991a":"## Understanding Data","b1eb0462":"## Dimensionality Reduction\n\nIn the previous mall segementation, we had used Income and Spend Scores attributes for the segementation, what if we add Age, to the segmentation.\nSo far, we performed the Euclidean distance for the two dimensional attributes and Euclidean distance is very well handled for the multi dimensions, once the distance is defined, the usual machine learning algorithms for clustering, say K-means, can be employed.\n\nMotivation for Dimensional Reduction:\n    \n Since, we have introduced Age as the third dimension, however we need to first reduce the data to two dimensions.\n\n\nDimensionality reduction techniques are commonly employed for this. The idea of dimensionality reduction is that multi-dimensional data is reduced, usually to \ntwo dimensions, for visualization purposes, in a manner that preserves the distance between the points. A simple and intuitive way to understand dimensionality \nreduction is to consider that pictures and videos we watch on two-dimensional screens are in fact representations of a three-dimensional world, reduced to two \ndimensions so that they can be visualized on the two-dimensional screen. While we do lose a dimension (depth) in the images, the loss is minimal and does not \nruin the viewing experience. Dimensionality reduction, when applied to our multi\u0002dimensional dataset, would condense most of the information into two dimensions to enable visualization\n\n\n\n\n## Principal Component Analysis\n\nThe techniques for dimensionality reduction are many. Will look and apply the most popular approach by far \u2013 Principal Component Analysis (PCA). PCA is a method of transforming data. It takes the original features\/dimensions and creates \nnew features\/dimensions that capture the most variance in the data. In other words, it creates dimensions that contain the most amount of information about the data, so that when you take the first two Principal Components (PCs), that is, dimensions, \nyou are left with most of the information about the data, but reduced to only two dimensions.\n\n\n","efb165a6":" using the PCA module from the scikit-learn package. Usage of the package is similar to the usage of the KMeans module. \n Similar to the KMeans module, the PCA module has fit and transform methods. \n Fitting would mean calculating the PCs (Principal Components) from the data and transforming would mean \n representing the data in these new PCs. The fit_transform method combines the two methods.\n n_components=N, where \"N\" is the number of PCs to calculate, cols is a list of columns to calculate the PCAs from\n and data is the dataset containing these columns. pca_res would contain the transformed data.\n\n\nfrom sklearn import decomposition\n\npca = decomposition.PCA(n_components=N)\n\npca_res = pca.fit_transform(data[cols])\n\nTo summarize the discussion on PCA, when performing clustering with high-dimensional data, the cluster formation approach remains the same. It is just the \nvisualization that now needs an additional step \u2013 reduction to two dimensions using \na dimensionality reduction technique such as PCA. In the exercise that follows, \nyou will try this firsthand by clustering the mall customers using age, income, and \nspend score.\n\n","6844f0c7":"Each cluster can be described as follows:\n\n\u2022 Cluster 0: Low-income high spenders\n\n\u2022 Cluster 1: Moderate-income moderate spenders\n\n\u2022 Cluster 2: High-income low spenders\n\n\u2022 Cluster 3: Low-income low spenders\n\n\u2022 Cluster 4: High-income high spender\n","7f28d4e7":"## Traditional Segmentation","080dd7dc":"## K-means Clustering Algorithm","4306df8e":"Beyond 90k, the frequency in the bins falls sharply and it seems that these \ncustomers can naturally be considered a separate group representing high-income customers. A good proportion of customers seems to lie in the 50k-90k \nrange. These can be considered moderate-income customers. Customers \nearning less than 40k would be low-income customers. ","7a273b90":"## Euclidean Distance or Euclidean Measure"}}