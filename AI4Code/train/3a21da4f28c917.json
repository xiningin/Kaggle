{"cell_type":{"2bf7ea62":"code","a32de02c":"code","7f19a967":"code","406d85c6":"code","d33501af":"code","0dbd3053":"code","32c81855":"code","41df1196":"code","d59c5590":"code","aaa49f88":"code","cd4615bf":"code","a5a1fe8e":"code","5ba8826d":"code","4bd7fbc9":"code","0cebcdad":"code","64dcfe79":"code","c0eecf2e":"code","6f68dd74":"code","bce5364d":"code","9e77ba2e":"code","07372b0c":"code","94241131":"code","e200eb69":"code","1eb2288b":"markdown","a45f5b26":"markdown"},"source":{"2bf7ea62":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a32de02c":"import sklearn\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt","7f19a967":"np.random.seed(42)\ntf.random.set_seed(42)\nn_steps = 5\ndataset = tf.data.Dataset.from_tensor_slices(tf.range(13))\ndataset = dataset.window(n_steps, shift=1, drop_remainder=True)\ndataset = dataset.flat_map(lambda window: window.batch(n_steps))\ndataset = dataset.shuffle(10).map(lambda window: (window[:-1], window[4:]))\ndataset = dataset.batch(3).prefetch(1)\nfor index, (Src_batch, Tgt_batch) in enumerate(dataset):\n    print(\"_\" * 20, \"Batch\", index, \"\\nSrc_batch\")\n    print(X_batch.numpy())\n    print(\"=\" * 5, \"\\nTgt_batch\")\n    print(Y_batch.numpy())","406d85c6":"#shakespeare_url = \"https:\/\/homl.info\/shakespeare\" # shortcut URL\n#filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n#with open(filepath) as f:\n#    shakespeare_text = f.read()\nshakespeare_text  = open(\"\/kaggle\/input\/shakespeare.txt\").read().lower()\nprint(shakespeare_text[:100])","d33501af":"shakespeare_text=shakespeare_text[:20000]","0dbd3053":"tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\ntokenizer.fit_on_texts([shakespeare_text])","32c81855":"seq=tokenizer.texts_to_sequences([shakespeare_text])\nlen(seq[0])","41df1196":"max_id = len(tokenizer.word_index) # number of distinct characters\ndataset_size = len(seq[0]) # total number of characters","d59c5590":"[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\ntrain_size = dataset_size * 90 \/\/ 100\ndataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])","aaa49f88":"n_steps = 100\nwindow_length = n_steps + 1 # target = input shifted 1 character ahead\ndataset = dataset.window(window_length, shift=1, drop_remainder=True)","cd4615bf":"dataset = dataset.flat_map(lambda window: window.batch(window_length))","a5a1fe8e":"np.random.seed(42)\ntf.random.set_seed(42)","5ba8826d":"batch_size = 32\ndataset = dataset.shuffle(10000).batch(batch_size)\ndataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))","4bd7fbc9":"max_id","0cebcdad":"dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id),Y_batch))","64dcfe79":"dataset = dataset.prefetch(1)","c0eecf2e":"for index, (X_batch, Y_batch) in enumerate(dataset):\n    print(\"_\" * 20, \"Batch\", index, \"\\nX_batch\")\n    print(X_batch.numpy())\n    print(\"=\" * 5, \"\\nY_batch\")\n    print(Y_batch.numpy())","6f68dd74":"model = keras.models.Sequential([\nkeras.layers.GRU(128, return_sequences=True, input_shape=[None,max_id],dropout=0.2, recurrent_dropout=0.2),\nkeras.layers.GRU(128, return_sequences=True,dropout=0.2, recurrent_dropout=0.2),\nkeras.layers.TimeDistributed(keras.layers.Dense(max_id,\nactivation=\"softmax\"))\n])","bce5364d":"from keras.utils.vis_utils import plot_model\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","9e77ba2e":"model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\nhistory = model.fit(dataset, epochs=5)","07372b0c":"import pandas as pd\nimport matplotlib.pyplot as plt\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(1, 5) # set the vertical range to [0-1]\nplt.show()","94241131":"def preprocess(texts):\n    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n    return tf.one_hot(X, max_id)","e200eb69":"X_new = preprocess([\"what is the time no\"])\nY_pred = model.predict_classes(X_new)\ntokenizer.sequences_to_texts(Y_pred + 1)[0][-1]","1eb2288b":"### Shuffling and Slicing","a45f5b26":"### Importing Libraries"}}