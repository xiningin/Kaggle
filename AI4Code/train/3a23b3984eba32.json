{"cell_type":{"53892c93":"code","18cbcb82":"code","45202abc":"code","d45e467d":"code","3873e2ee":"code","082fadf7":"code","5e2264b1":"code","f52aea58":"code","63cba747":"code","671d2df3":"code","a299df0b":"code","3f0cc647":"code","774d9ae0":"code","093f2f73":"code","f1089060":"code","bf70db7b":"code","5219f45a":"code","64e11c97":"code","17e01e2a":"code","694be4b9":"code","3e23116e":"code","f74816e6":"code","7705f9b3":"code","e49af3c9":"code","dbe70e34":"code","0d3a97df":"code","5d94b14f":"code","d7b89bb4":"code","98c64adf":"markdown"},"source":{"53892c93":"# Import libraries\n\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, Activation,Dropout\nfrom keras.models import Model,load_model\nfrom tensorflow.keras.layers import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.core import Flatten, Dense\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nimport tensorflow.python.keras.engine\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import backend as K\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.datasets import load_files\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport cv2\nimport matplotlib.pyplot as plt\nimport itertools\n%matplotlib inline","18cbcb82":"train_dir = '\/kaggle\/input\/waste-classification-data\/dataset\/DATASET\/TRAIN'\ntest_dir = '\/kaggle\/input\/waste-classification-data\/dataset\/DATASET\/TEST'\n\ndef load_dataset(path):\n    data = load_files(path) #load all files from the path\n    files = np.array(data['filenames']) #get the file  \n    targets = np.array(data['target'])#get the the classification labels as integer index\n    target_labels = np.array(data['target_names'])#get the the classification labels \n    return files,targets,target_labels\n    \nx_train, y_train,target_labels = load_dataset(train_dir)\nx_test, y_test,_ = load_dataset(test_dir)\n\nprint('Training set size : ' , x_train.shape[0])\nprint('Testing set size : ', x_test.shape[0]) ","45202abc":"x_train,x_validate,y_train,y_validate = train_test_split(x_train,y_train,test_size = 0.2,random_state = 1)\n","d45e467d":"print (\"x_train shape: \" + str(x_train.shape))\nprint (\"x_train shape: \" + str(y_train.shape))\nprint (\"x_validate shape: \" + str(x_validate.shape))\nprint (\"y_validate shape: \" + str(y_validate.shape))\nprint (\"x_test shape: \" + str(x_test.shape))\nprint (\"y_test shape: \" + str(y_test.shape))","3873e2ee":"# Convert jpg file to numpy array to feed to the CNN.\n#By using Opencv .\n\ndef convert_image_to_array(files):\n    width, height, channels = 100, 100, 3\n    images_as_array = np.empty((files.shape[0], width, height, channels), dtype=np.uint8) #define train and test data shape\n    for idx,file in enumerate(files):\n        img = cv2.imread(file) \n        res = cv2.resize(img, dsize=(width, height), interpolation=cv2.INTER_CUBIC) #As images have different size, resizing all images to have same shape of image array\n        images_as_array[idx] = res\n    return images_as_array\n\nx_train = np.array(convert_image_to_array(x_train))\nprint('Training set shape : ',x_train.shape)\n\nx_valid = np.array(convert_image_to_array(x_validate))\nprint('Validation set shape : ',x_valid.shape)\n\nx_test = np.array(convert_image_to_array(x_test))\nprint('Test set shape : ',x_test.shape)","082fadf7":"x_train = x_train.astype('float32')\/255\nx_valid = x_valid.astype('float32')\/255\nx_test = x_test.astype('float32')\/255\ny_train = y_train.reshape(y_train.shape[0],1)\ny_test = y_test.reshape(y_test.shape[0],1)\ny_validate = y_validate.reshape(y_validate.shape[0],1)","5e2264b1":"plt.figure(figsize=(20,20))\nclasses = ['O','R']\nfor i in range(1,26):\n    index = np.random.randint(x_train.shape[0])\n    plt.subplot(5, 5, i)\n    plt.imshow(np.squeeze(x_train[index]), cmap='cool')\n    plt.title(classes[int(y_train[index])])\n    plt.tight_layout()\nplt.show() ","f52aea58":"from glob import glob \n\nclassName = glob(train_dir + '\/*' )\nnumberOfClass = len(className)\nprint(\"Number Of Class: \",numberOfClass)\n","63cba747":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\ndatagen.fit(x_train) ","671d2df3":"train_datagen = ImageDataGenerator(rescale= 1.\/255) \ntest_datagen = ImageDataGenerator(rescale= 1.\/255) ","a299df0b":"train_generator = train_datagen.flow_from_directory(\n        train_dir, \n        target_size= (224,224),\n        batch_size = 128,\n        color_mode= \"rgb\",\n        class_mode= \"categorical\")\n\ntest_generator = test_datagen.flow_from_directory(\n        test_dir, \n        target_size= (224,224),\n        batch_size = 128,\n        color_mode= \"rgb\",\n        class_mode= \"categorical\")","3f0cc647":"'''model = Sequential()\nmodel.add(Conv2D(32,(3,3),input_shape = (224,224,3)))\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D())\n\nmodel.add(Conv2D(64,(3,3)))  \nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D())\n\nmodel.add(Flatten())\nmodel.add(Conv2D(128,(3,3)))\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D())\nmodel.add(Dense(numberOfClass)) # output\nmodel.add(Activation(\"sigmoid\"))\n\nmodel.compile(loss = \"binary_crossentropy\",\n              optimizer = \"adam\",\n              metrics = [\"accuracy\"]) \nbatch_size = 128 '''\n\nmodel = Sequential()\nmodel.add(Conv2D(32,kernel_size=(3, 3),activation='relu',input_shape=(100,100,3)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(32, kernel_size=(3, 3),activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(2, activation='sigmoid'))\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\",optimizer='adam',metrics=['accuracy']) ","774d9ae0":"earlystop = EarlyStopping(monitor = 'val_loss', # value being monitored for improvement\n                          min_delta = 0, #Abs value and is the min change required before we stop\n                          patience = 15, #Number of epochs we wait before stopping \n                          verbose = 1,\n                          restore_best_weights = True) #keeps the best weigths once stopped","093f2f73":"ReduceLR = ReduceLROnPlateau(patience=3, verbose=1)","f1089060":"callbacks = [earlystop, ReduceLR]","bf70db7b":"history = model.fit_generator(datagen.flow(x_train, y_train, batch_size= 32), epochs = 10, verbose=1, validation_data=(x_valid,y_validate)) ","5219f45a":"import matplotlib.pyplot as plt\n\n# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validate'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validate'], loc='upper left')\nplt.show() ","64e11c97":"model = Sequential()\nmodel.add(Conv2D(64,(3,3),input_shape = (100,100,3)))\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D())\n\nmodel.add(Conv2D(128,(3,3)))  \nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D())\n\nmodel.add(Conv2D(256,(3,3)))\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D())\n\n#model.add(Flatten())\nmodel.add(Dense(256))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64))\nmodel.add(Activation(\"relu\"))\n\nmodel.add(Dropout(0.5))\nmodel.add(Flatten()) \nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\n\nmodel.compile(loss = \"sparse_categorical_crossentropy\",\n              optimizer = \"adam\",\n              metrics = [\"accuracy\"])  ","17e01e2a":"earlystop = EarlyStopping(monitor = 'val_loss', # value being monitored for improvement\n                          min_delta = 0, #Abs value and is the min change required before we stop\n                          patience = 15, #Number of epochs we wait before stopping \n                          verbose = 1,\n                          restore_best_weights = True) #keeps the best weigths once stopped","694be4b9":"ReduceLR = ReduceLROnPlateau(patience=3, verbose=1)","3e23116e":"callbacks = [earlystop, ReduceLR]","f74816e6":"history = model.fit_generator(datagen.flow(x_train, y_train, batch_size= 256), epochs = 10, verbose=1, validation_data=(x_valid,y_validate)) ","7705f9b3":"model_3 = Sequential()\nmodel_3.add(Conv2D(32,(3,3),input_shape = (224,224,3)))\nmodel_3.add(Activation(\"relu\"))\nmodel_3.add(MaxPooling2D())\nmodel_3.add(Conv2D(64,(3,3)))\nmodel_3.add(Activation(\"relu\"))\nmodel_3.add(MaxPooling2D())\nmodel_3.add(Conv2D(128,(3,3)))\nmodel_3.add(Activation(\"relu\"))\nmodel_3.add(MaxPooling2D())\nmodel_3.add(Flatten())\nmodel_3.add(Dense(256))\nmodel_3.add(Activation(\"relu\"))\nmodel_3.add(Dropout(0.5))\nmodel_3.add(Dense(64))\nmodel_3.add(Activation(\"relu\"))\nmodel_3.add(Dropout(0.5))\nmodel_3.add(Dense(numberOfClass)) # output\nmodel_3.add(Activation(\"sigmoid\"))\nmodel_3.compile(loss = \"binary_crossentropy\",\noptimizer = \"adam\",\nmetrics = [\"accuracy\"])\nbatch_size = 256","e49af3c9":"train_datagen1 = ImageDataGenerator(rescale= 1.\/255)\ntest_datagen1 = ImageDataGenerator(rescale= 1.\/255)","dbe70e34":"train_generator1 = train_datagen1.flow_from_directory(\ntrain_dir,\ntarget_size= (224,224),\nbatch_size = batch_size,\ncolor_mode= \"rgb\",\nclass_mode= \"categorical\")\ntest_generator1 = test_datagen1.flow_from_directory(\ntest_dir,\ntarget_size= (224,224),\nbatch_size = batch_size,\ncolor_mode= \"rgb\",\nclass_mode= \"categorical\")","0d3a97df":"hist = model_3.fit_generator(\ngenerator = train_generator1,\nepochs=10,\nvalidation_data = test_generator1)","5d94b14f":"plt.figure(figsize=[10,6])\nplt.plot(hist.history[\"accuracy\"], label = \"Train acc\")\nplt.plot(hist.history[\"val_accuracy\"], label = \"Validation acc\")\nplt.legend()\nplt.show()","d7b89bb4":"plt.figure(figsize=(10,6))\nplt.plot(hist.history['loss'], label = \"Train loss\")\nplt.plot(hist.history['val_loss'], label = \"Validation loss\")\nplt.legend()\nplt.show()","98c64adf":"# Convolutional Neural Network - CNN \n\n'''model = Sequential()\nmodel.add(Conv2D(32,(3,3),input_shape = (224,224,3)))\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D())\n\nmodel.add(Conv2D(64,(3,3)))  \nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D())\n\nmodel.add(Flatten())\nmodel.add(Conv2D(128,(3,3)))\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D())\nmodel.add(Dense(numberOfClass)) # output\nmodel.add(Activation(\"sigmoid\"))\n\nmodel.compile(loss = \"binary_crossentropy\",\n              optimizer = \"adam\",\n              metrics = [\"accuracy\"]) \nbatch_size = 128 '''\n\nmodel = Sequential()\nmodel.add(Conv2D(16,kernel_size=(3, 3),activation='relu',input_shape=(224,224,3)))\nmodel.add(MaxPooling2D())\n\nmodel.add(Conv2D(32, kernel_size=(3, 3),activation='relu'))\nmodel.add(MaxPooling2D())\n\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(2, activation='sigmoid'))\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\",optimizer='adam',metrics=['accuracy']) \n"}}