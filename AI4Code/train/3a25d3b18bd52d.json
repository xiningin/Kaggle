{"cell_type":{"4e11b8a9":"code","3ceb8480":"code","d289fe36":"code","18030369":"code","14d77630":"code","f4831c6a":"code","6c1e8f2d":"code","7d86bf42":"code","2221b0d2":"code","d326c73f":"code","c6db2658":"code","c1e4c2cd":"code","b89df011":"code","974854d6":"code","1cbc074a":"code","e47f44ab":"code","58dcae45":"code","3dc722f4":"code","b848766b":"code","6b69b2b6":"code","49250d61":"code","4bd1b40b":"code","5cdd64b7":"code","14acd3f0":"code","78728c68":"code","da6d5599":"code","65410b42":"code","f2aa2657":"code","097ba773":"code","9ad245ef":"code","584d725f":"code","ffc3ca22":"code","0a3742ec":"code","32d7b75f":"code","ce62a69a":"code","fc3b7df4":"code","1b039251":"code","e85afea4":"code","add5a1ad":"code","62b7108a":"code","0ae03f62":"code","3c17204b":"code","60312170":"code","38a7181e":"code","895af4e2":"code","49e2c4f8":"code","e8f9106e":"code","a2faeb94":"code","5516c8e1":"code","3b42c8bb":"code","61adb630":"code","c3bdb38b":"code","15b50d0b":"markdown","d0013f27":"markdown","ee474369":"markdown","3c45d995":"markdown","16c0a227":"markdown","2f7fe6f7":"markdown","0ad49abc":"markdown","0f3b256d":"markdown","13d3624b":"markdown","60dd5d7e":"markdown","93070ed5":"markdown","bfa3d82e":"markdown","aab27218":"markdown","0f7a9569":"markdown","d7ac387f":"markdown","e2b80438":"markdown","34fe9b6f":"markdown","262a390b":"markdown","71d462ee":"markdown","7eae79c7":"markdown","b81570b7":"markdown","53bd182a":"markdown","8c523533":"markdown","c2bbd923":"markdown","3c10eadd":"markdown","7f86b0fc":"markdown"},"source":{"4e11b8a9":"# importing all the necessary frame works\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import confusion_matrix , classification_report , precision_score,recall_score,f1_score,plot_roc_curve\n\nfrom sklearn.model_selection import train_test_split , cross_val_score , RandomizedSearchCV , GridSearchCV\n\nimport warnings\nwarnings.filterwarnings('ignore')","3ceb8480":"df = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')","d289fe36":"df.head()","18030369":"df.target.value_counts()","14d77630":"df['target'].value_counts().plot(kind='bar');","f4831c6a":"df.info()","6c1e8f2d":"df.isna().sum()","7d86bf42":"df.describe()","2221b0d2":"df.sex.value_counts()","d326c73f":" pd.crosstab(df.target,df.sex)","c6db2658":"pd.crosstab(df.target,df.sex).plot(kind = \"bar\",color = ['salmon','lightblue'],figsize=(10,6));\n    \nplt.title(\"Heart diesase with gender\")\nplt.legend(['female','male'])\nplt.xlabel(\"0 = No heart disease  1 = heart disease\")","c1e4c2cd":"plt.figure(figsize=(10,7))\n\nplt.scatter(df.age[df.target==1],\n            df.thalach[df.target==1],\n            c = 'darkred')\n\nplt.scatter(df.age[df.target==0],\n            df.thalach[df.target==0],\n            c = 'salmon')\n\nplt.title(\"Heart disease in function of age and max heart rate\")\nplt.xlabel('Age')\nplt.ylabel('Max heart rate (thalach)')\nplt.legend(['Disease','No disease'])","b89df011":"## check the distribution of the age column with hist\nplt.hist(df.age)","974854d6":"pd.crosstab(df.cp,df.target)","1cbc074a":"pd.crosstab(df.cp,df.target).plot(kind='bar',color=['salmon','darkred'])\n\n\nplt.title(\"Heart disease in function chest pain with target\")\nplt.xlabel('Chest pain')\nplt.ylabel('Amount')\nplt.legend(['NO disease','Disease'])","e47f44ab":"df.corr()","58dcae45":"corr_mat = df.corr()\n\nplt.figure(figsize=(15,10))\nsns.heatmap(corr_mat,annot=True,linewidths=0.5)","3dc722f4":"# split data into X and y\n\nX = df.drop('target',axis=1)\ny = df['target']","b848766b":"# train test spliting\n\nnp.random.seed(42) #To reproduced the randomized data again\n\nX_train , X_test , y_train , y_test = train_test_split(X,y,test_size = 0.2)\n\nX_train.shape , X_test.shape","6b69b2b6":"# Time to build a machine learning model\n\nmodels = {'logistic regression':LogisticRegression(),\n          'Random forest classifier':RandomForestClassifier(),\n          'KNearest neighbor':KNeighborsClassifier()\n         }\n\nnp.random.seed(42)\n\ndef fit_and_score(models,X_train,X_test,y_train,y_test):\n    \"\"\"\n    Fits and evaluatest different machine learning models.\n    models : a dict of different sciit learn machine learning models.\n    X_train : training data (no labels)\n    X_test : Testing data (no labels)\n    y_train : training labels\n    y_test : testing labels    \n    \"\"\"\n    scores = {}\n    \n    for name , model in models.items():\n        model.fit(X_train,y_train)\n        scores[name] = model.score(X_test,y_test)\n        \n    return scores\n        ","49250d61":"model_scores = fit_and_score(models=models,X_train=X_train,X_test=X_test,\n                             y_train=y_train,y_test=y_test)\n\nmodel_scores","4bd1b40b":"model_scores_df = pd.DataFrame(model_scores,index=['accuracy'])\nmodel_scores_df.T.plot.bar();\n","5cdd64b7":"#Tuning knn\n\ntrain_scores = []\ntest_scores = []\n\nneighbors = range(1,21)\n\nknn = KNeighborsClassifier()\n\nfor i in neighbors:\n    knn.set_params(n_neighbors = i)\n    \n    knn.fit(X_train,y_train)\n    train_scores.append(knn.score(X_train,y_train))\n    test_scores.append(knn.score(X_test,y_test))","14acd3f0":"plt.plot(neighbors,train_scores,label = 'train scores')\nplt.plot(neighbors,test_scores,label = 'train scores')\nplt.legend()\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.xticks(np.arange(1,21,1));\nprint(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\")\n","78728c68":"#logistic regression grid\nlog_reg_grid = {'C': np.logspace(-4,4,20),\n                'solver': ['liblinear']}\n\n#random forest grid\nrf_grid = {'n_estimators':np.arange(10,1000,50),\n           'max_depth': [None,3,5,10],\n           'min_samples_split':np.arange(2,20,2),\n           'min_samples_leaf':np.arange(1,20,2)}\n","da6d5599":"np.random.seed(42)\n\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                                param_distributions=log_reg_grid,\n                                cv=5,\n                                n_iter=20,\n                                verbose=True)\n\nrs_log_reg.fit(X_train,y_train)","65410b42":"rs_log_reg.best_params_","f2aa2657":"rs_log_reg.score(X_test,y_test)","097ba773":"# Setup random seed\nnp.random.seed(42)\n\n# Setup random hyperparameter search for RandomForestClassifier\nrs_rf = RandomizedSearchCV(RandomForestClassifier(), \n                           param_distributions=rf_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose=True)\n\n# Fit random hyperparameter search model for RandomForestClassifier()\nrs_rf.fit(X_train, y_train)","9ad245ef":"rs_rf.best_params_","584d725f":"rs_rf.score(X_test,y_test)","ffc3ca22":"#Different hyperprameters for our logistic regression model\n\nlog_reg_grid = {'C': np.logspace(-4, 4, 30),\n                \"solver\": [\"liblinear\"]}\n\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                         param_grid=log_reg_grid,\n                         cv=5,\n                         verbose=True)\n\ngs_log_reg.fit(X_train,y_train);","0a3742ec":"gs_log_reg.best_params_\n","32d7b75f":"gs_log_reg.score(X_test,y_test)","ce62a69a":"y_preds = gs_log_reg.predict(X_test)","fc3b7df4":"y_preds","1b039251":"plot_roc_curve(gs_log_reg, X_test, y_test)","e85afea4":"sns.set(font_scale=1.5)\n\nsns.heatmap(confusion_matrix(y_test,y_preds),\n            annot=True,\n            cbar=False)","add5a1ad":"print(classification_report(y_test,y_preds))","62b7108a":"# Check best hyperparameters\ngs_log_reg.best_params_","0ae03f62":"# Create a new classifier with best parameters\nclf = LogisticRegression(C=0.20433597178569418,\n                         solver=\"liblinear\")","3c17204b":"# Cross-validated accuracy\ncv_acc = cross_val_score(clf,\n                         X,\n                         y,\n                         cv=5,\n                         scoring=\"accuracy\")\ncv_acc = cv_acc.mean()\nprint(f\"Cross Validated accuracy {cv_acc}\")","60312170":"# Cross-validated precision\ncv_precision = cross_val_score(clf,\n                         X,\n                         y,\n                         cv=5,\n                         scoring=\"precision\")\ncv_precision=np.mean(cv_precision)\nprint(f\"Cross Validated Precision {cv_precision}\")","38a7181e":"# Cross-validated recall\ncv_recall = cross_val_score(clf,\n                         X,\n                         y,\n                         cv=5,\n                         scoring=\"recall\")\ncv_recall=np.mean(cv_recall)\nprint(f\"Cross Validated Recall {cv_recall}\")","895af4e2":"# Cross-validated f1_score\ncv_f1 = cross_val_score(clf,\n                         X,\n                         y,\n                         cv=5,\n                         scoring=\"f1\")\ncv_f1=np.mean(cv_f1)\nprint(f\"Cross Validated F1_score {cv_f1}\")","49e2c4f8":"# Visualizing cross-validated metrics\ncv_metrics = pd.DataFrame({\"Accuracy\": cv_acc,\n                           \"Precision\": cv_precision,\n                           \"Recall\": cv_recall,\n                           \"F1\": cv_f1},\n                          index=[0])\n\ncv_metrics.T.plot.bar(title=\"Cross-validated classification metrics\",\n                      legend=False);","e8f9106e":"\n# Fit an instance of LogisticRegression\nclf = LogisticRegression(C=0.20433597178569418,\n                         solver=\"liblinear\")\n\nclf.fit(X_train, y_train);","a2faeb94":"# Check coef_\nclf.coef_","5516c8e1":"# Match coef's of features to columns\nfeature_dict = dict(zip(df.columns, list(clf.coef_[0])))\nfeature_dict","3b42c8bb":"# Visualize feature importance\nfeature_df = pd.DataFrame(feature_dict, index=[0])\nfeature_df.T.plot.bar(title=\"Feature Importance\", legend=False);","61adb630":"pd.crosstab(df[\"sex\"], df[\"target\"])","c3bdb38b":"pd.crosstab(df[\"slope\"], df[\"target\"])","15b50d0b":"**Models to build**\n\n1. RandomForestClassifier\n2. Logistic Regression\n3. KNearest neighbor classifier","d0013f27":"For logistic regression coef_ is used to find the feature importances of the model","ee474369":"<a id = \"correlation\"><\/a>\n### Correlation matrix","3c45d995":" In this,\n\n    total female = 96\n    affected female = 72 , one third of the female are affected\n    \n    total male = 207\n    affected male = 93 half of the male are affected ","16c0a227":"Sex is highly negative correlated with the target variable lets look at it..","2f7fe6f7":"<a id = \"exploration_by_pain\"><\/a>\n### Heart disease frequency for chest pain","0ad49abc":"<a id = \"random_tuning\"><\/a>\n### Hyperparameter tuning with Randomized search CV\n\nWe're going to tune , \n\n - LogisticRegression()\n - RandomForestClassifier()","0f3b256d":"<a id = \"import\"><\/a>\n# Importing Libraries","13d3624b":"<a id = \"cross_validation\"><\/a>\n### Different evaluation metrics using cross-validation\n\nWe're going to calculate accuracy, precision, recall and f1-score of our model using cross-validation and to do so we'll be using cross_val_score().","60dd5d7e":"<a id = \"grid_tuning\"><\/a>\n### Hyperparameter tuning with GridsearchCV","93070ed5":"<a id = \"exploration_by_sex\"><\/a>\n### Heart disease frequency according to sex","bfa3d82e":"<a id = \"hyperparameter_tuning\"><\/a>\n## Hyper Parameter Tuning\n","aab27218":"Now we've got a baseline model... and we know a model's first predictions aren't always what we should based our next steps off. What should we do?\n\nLet's look at the following:\n\n    - Hypyterparameter tuning\n    - Feature importance\n    - Confusion matrix\n    - Cross-validation\n    - Precision\n    - Recall\n    - F1 score\n    - Classification report\n    - ROC curve\n    - Area under the curve (AUC)","0f7a9569":"<a id = \"feature_importance\"><\/a>\n## Feature Importance\n\nFeature importance is another as asking, \"which features contributed most to the outcomes of the model and how did they contribute?\"\n\nFinding feature importance is different for each machine learning model. One way to find feature importance is to search for \"(MODEL NAME) feature importance\".\n\nLet's find the feature importance for our LogisticRegression model...","d7ac387f":"<a id = \"evaluation\"><\/a>\n### Evaluting our tuned machine learning classifier, beyond accuracy\n\n- ROC curve and AUC score\n- Confusion matrix\n- Classification report\n- Precision\n- Recall\n- F1-score","e2b80438":"Now we've got a ROC curve, an AUC metric and a confusion matrix, let's get a classification report as well as cross-validated precision, recall and f1-score.","34fe9b6f":"<a id = \"manual_tuning\"><\/a>\n### Manual HyperParameter Tuning","262a390b":"<a id = \"model_building\"><\/a>\n## Model Building","71d462ee":"1. [Problem Definition](#problem_definition)\n\n2. [Importing Libraries](#import)\n\n3. [Data Dictionary](#data_dictionary)\n\n4. [Data Exploration](#data_exploration)\n\n    4.1 [Heart Disease frequency according to Sex](#exploration_by_sex)\n\n    4.2 [Heart Disease frequency for chest pain](#exploration_by_pain)\n\n    4.3 [Correlation matrix](#correlation)\n    \n5. [Model Building](#model_building)\n\n6. [HyperParameter Tuning](#hyperparameter_tuning)\n    \n    6.1 [Manual Hyperparameter Tuning](#manual_tuning)\n    \n    6.2 [RandomizedSearchCV](#random_tuning)\n    \n    6.3 [GridSearchCV](#grid_tuning)\n\n7. [Evaluation beyond accuracy](#evaluation)\n\n    7.1. [Different Evaluation metrics using Cross-Validation](#cross_validation)\n\n9. [Feature Importance](#feature_importance)","7eae79c7":"slope - the slope of the peak exercise ST segment\n\n- 0: Upsloping: better heart rate with excercise (uncommon)\n- 1: Flatsloping: minimal change (typical healthy heart)\n- 2: Downslopins: signs of unhealthy heart","b81570b7":"# Please do Upvote if you find it useful","53bd182a":"Sex class is highly imbalanced with lower female and high male values and also we can see that ratio of the female is 3 : 1 and for the male it is more or less 1:2 hence as the sex is increasing the target value is decreasing","8c523533":"<a id = \"data_exploration\"><\/a>\n## Data Exploration\n\n1. What questions are you trying to solve \n2. What kind of data do we have and how we handle it\n3. What is missing and how you are going to handle it\n4. Where are the outliers and why should we care about them\n5. How can you add, change or remove features to get more out of your data\n","c2bbd923":"<a id = \"data_dictionary\"><\/a>\n# Data Dictionary\n\n1. `age` - age in years\n2. `sex` - (1 = male; 0 = female)\n3. `cp` - chest pain \n        type 0: Typical angina: chest pain related decrease blood supply to the heart \n        type 1: Atypical angina: chest pain not related to heart \n        type 2: Non-anginal pain: typically esophageal spasms (non heart related) \n        type 3: Asymptomatic: chest pain not showing signs of disease\n4. `trestbps` - resting blood pressure (in mm Hg on admission to the hospital) anything above 130-140 is typically cause for concern\n5. `chol` - serum cholestoral in mg\/dl\n            * serum = LDL + HDL + .2 * triglycerides\n            * above 200 is cause for concern\n6. `fbs` - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false) \n            * '>126' mg\/dL signals diabetes\n7. `restecg` - resting electrocardiographic results \n        * 0: Nothing to note \n        * 1: ST-T Wave abnormality\n            - can range from mild symptoms to severe problems\n            - signals non-normal heart beat\n        * 2: Possible or definite left ventricular hypertrophy\n            - Enlarged heart's main pumping chamber\n8. `thalach` - maximum heart rate achieved\n9. `exang` - exercise induced angina (1 = yes; 0 = no)\n10. `oldpeak` - ST depression induced by exercise relative to rest looks at stress of heart during excercise unhealthy heart will stress more\n11. `slope` - the slope of the peak exercise ST segment \n        * 0: Upsloping: better heart rate with excercise (uncommon) \n        * 1: Flatsloping: minimal change (typical healthy heart) \n        * 2: Downslopins: signs of unhealthy heart\n12. `ca` - number of major vessels (0-3) colored by flourosopy \n        * colored vessel means the doctor can see the blood passing through\n        * the more blood movement the better (no clots)\n13. `thal` - thalium stress result\n        * 1,3: normal\n        * 6: fixed defect: used to be defect but ok now\n        * 7: reversable defect: no proper blood movement when excercising\n14. `target` - have disease or not (1=yes, 0=no) (the predicted attribute)","3c10eadd":"<a id = \"problem_definition\"><\/a>\n# Problem Definition\n\nGiven clinical parameters about a patient, can we predict whether or not they have heart disease?","7f86b0fc":"# Predicting heart disease using machine learning\n\nThis notebook looks into using various Python-based machine learning and data science libraries in an attempt to build a machine learning model capable of predicting whether or not someone has heart disease based on their medical attributes.\n\n![heartimage](https:\/\/2rdnmg1qbg403gumla1v9i2h-wpengine.netdna-ssl.com\/wp-content\/uploads\/sites\/3\/2019\/10\/cardiacDocs-1125401691-770x553-650x428.jpg)"}}