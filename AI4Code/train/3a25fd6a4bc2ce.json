{"cell_type":{"3c599be8":"code","ad429b75":"code","07cc7ee9":"code","1937c0c8":"code","4229db1e":"code","349367c7":"code","5baf33b7":"code","e830ebe9":"code","ac25d888":"code","579e419d":"code","9958864b":"code","3be9da71":"code","d44954ba":"code","a12562e5":"code","4771ecdf":"code","ebaa440b":"code","11f4c66c":"code","634036cf":"code","ea143ce1":"code","9211843c":"code","d992578a":"code","00586cd4":"code","01bc600f":"code","64bad6b1":"code","9cb4573c":"code","f5d95ca4":"code","659d48b3":"code","eab0fbd6":"code","62c61af7":"code","0d5832d0":"code","e2610499":"code","66bbbd9e":"code","f5b5832c":"code","37b64f2d":"code","7972e92b":"code","e8346899":"markdown","047b29b5":"markdown","2c6a3dfa":"markdown","3d3f8d73":"markdown","7344b9a2":"markdown","56b29506":"markdown","22b92b96":"markdown","94bdc2e2":"markdown","4dfcf11f":"markdown","390e218a":"markdown","0208727f":"markdown","998b98f6":"markdown","8571c10b":"markdown","7f3aee73":"markdown","1044a3c4":"markdown","d1d94e1f":"markdown","773de189":"markdown","883e3269":"markdown","6d5197dc":"markdown","81dc5f6f":"markdown"},"source":{"3c599be8":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\nfrom scipy import stats\nstats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)","ad429b75":"data_train = pd.read_csv('train.csv',index_col = 'Id')\ndata_test = pd.read_csv('test.csv',index_col = 'Id')\n\ndata_train.head()","07cc7ee9":"pd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', 100)","1937c0c8":"data_train.describe()","4229db1e":"#Check percent of null values.\n\nfor i in data_train.columns:\n    print(i,'have null values',(data_train[i].isnull().sum())\/len(data_train[i]*100),'%')","349367c7":"data_train = data_train.drop([ 'Alley', 'PoolQC', 'Fence', 'MiscFeature'], axis=1)\ndata_test = data_test.drop([ 'Alley', 'PoolQC', 'Fence', 'MiscFeature'], axis=1)\ndata_train.head()","5baf33b7":"numerical_cols = [cname for cname in data_train.columns if \n                data_train[cname].dtype in ['int64', 'float64']]\n\n#Numerical columns\ncategorical_cols = [cname for cname in data_train.columns if\n                    data_train[cname].dtype == \"object\"]","e830ebe9":"\nfor i in data_train[categorical_cols]:\n    print(i,'has values',data_train[i].unique(),'amount is',data_train[i].nunique())","ac25d888":"data_train = data_train.drop(['Exterior1st'], axis=1)\ndata_test = data_test.drop(['Exterior1st'], axis=1)\ndata_train.head()","579e419d":"data_train[numerical_cols].head()","9958864b":"data_train[numerical_cols].describe()","3be9da71":"data_train.isnull().sum()","d44954ba":"X = data_train.drop(['SalePrice'], axis=1)\ny = data_train['SalePrice']","a12562e5":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\n\nnum_imputer = SimpleImputer(strategy='median')\ncat_imputer = SimpleImputer(strategy='most_frequent')\n\n\n\n# Preprocessing for numerical data\nfor cname in X.columns:\n    if X[cname].dtype in ['int64', 'float64'] :\n        X[cname] = num_imputer.fit_transform(X[[cname]])\nfor cname in data_test.columns:\n    if data_test[cname].dtype in ['int64', 'float64'] :\n        data_test[cname] = num_imputer.fit_transform(data_test[[cname]]) \n            \n# Preprocessing for categorical data\nfor cname in X.columns:\n     if X[cname].dtype == \"object\":\n            X[cname] = cat_imputer.fit_transform(X[[cname]])\n        \nfor cname in data_test.columns:\n    if data_test[cname].dtype == \"object\":\n        data_test[cname] = cat_imputer.fit_transform(data_test[[cname]])","4771ecdf":"#Rearrange columns\nnumerical_cols = [cname for cname in X.columns if \n                data_train[cname].dtype in ['int64', 'float64']]\ncategorical_cols = [cname for cname in X.columns if\n                    X[cname].dtype == \"object\"]\nX = X[numerical_cols + categorical_cols]\nX.head()","ebaa440b":"#Rearrange columns for test data\n\ndata_test = data_test[numerical_cols + categorical_cols]\ndata_test.head()","11f4c66c":"X = pd.get_dummies(X, drop_first = True)\nX.head()","634036cf":"data_test = pd.get_dummies(data_test, drop_first = True)\ndata_test.head()","ea143ce1":"real_col = []\n\nfor i in X.columns.values:\n    for j in data_test.columns.values:\n        if i == j:\n            real_col.append(i)\n\nreal_col","9211843c":"len(real_col)","d992578a":"X_adjusted = X[real_col]\ndata_test_adjusted = data_test[real_col]","00586cd4":"colll = pd.DataFrame({'X_col':[i for i in X_adjusted.columns.values],\n                     'test_col':[i for i in data_test_adjusted.columns.values]})\ncolll","01bc600f":"print(X_adjusted.shape)\nprint(data_test_adjusted.shape)","64bad6b1":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler \n\nscaler = StandardScaler() \nX_scaled = scaler.fit_transform(X_adjusted)\ndata_test_scaled = scaler.fit_transform(data_test_adjusted)","9cb4573c":"pca = PCA(n_components=X_adjusted.shape[1])\npca.fit_transform(X_scaled)\n","f5d95ca4":"explain_ratio = pca.explained_variance_ratio_\nexplain_ratio_cum = np.cumsum(pca.explained_variance_ratio_)\n\nplt.figure(figsize = (12, 8))\nax = plt.axes()\nax.set_facecolor('#dfe3e6')\nplt.grid(color = 'w')\nplt.xlabel('Number of components')\nplt.ylabel('variance explained')\nplt.title('Scree plot')\n\n#Plotting\nplt.plot(range(1, explain_ratio.shape[0] + 1), \n         explain_ratio, c = 'royalblue', marker = 'o', linewidth = 1, label = 'Individual')\nplt.plot(range(1, explain_ratio.shape[0] + 1),\n         explain_ratio_cum, c = 'firebrick', marker = 'o', linestyle = '--', label = 'Cumulative')\n\n#Adding values to plot\nfor x, ex_ratio, ex_ratio_cum in zip(range(1, explain_ratio.shape[0] + 1),\n                                     explain_ratio,\n                                     explain_ratio_cum):\n  ex_ratio_label = f'{ex_ratio * 100:.2f}%'\n  plt.annotate(ex_ratio_label, (x, ex_ratio), textcoords = 'offset points',\n               xytext = (5, 5), ha = 'center')\n  ex_ratio_cum_label = f'{ex_ratio_cum * 100:.2f}%'\n  plt.annotate(ex_ratio_cum_label, (x, ex_ratio_cum), textcoords = 'offset points',\n               xytext = (5, 5), ha = 'center')\n\nplt.show()","659d48b3":"pca_data = pd.DataFrame({'n_components':[i for i in range(1, explain_ratio.shape[0] + 1)],\n                        'PCA' : explain_ratio_cum })","eab0fbd6":"pca_data","62c61af7":"new_pca = PCA(n_components=144)\nX_pca = new_pca.fit_transform(X_scaled)\ndata_test_pca = new_pca.transform(data_test_scaled) ","0d5832d0":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_pca,y, test_size = 0.25, random_state = 0)","e2610499":"print(X_train.shape)\nprint(y_train.shape)","66bbbd9e":"from catboost import CatBoostRegressor\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\nmodel_1 = CatBoostRegressor()\nmodel_2 = ElasticNet() \ndef find_best_model(X, y):\n    for model in [model_1, model_2]:\n        cvs = -1*cross_val_score(model, X, y, cv=5,  scoring='neg_mean_absolute_error')\n        print('{0} MAE is {1}'.format(model,cvs.mean()))\n  \n    \nfind_best_model(X_train, y_train)","f5b5832c":"best_model = model_2 \nbest_model.fit(X_train, y_train)\ny_test_pred = best_model.predict(X_test)\nprint('Test MAE :', mean_absolute_error(y_test,y_test_pred))\nprint('Test MSE :', mean_squared_error(y_test,y_test_pred))\nprint('RMSE Test :', np.sqrt(mean_squared_error(y_test,y_test_pred)))\nprint('R2 Test :', r2_score(y_test,y_test_pred))\n\n    \n","37b64f2d":"predictions = best_model.predict(data_test_pca)","7972e92b":"data_for_sub = pd.read_csv('test.csv')\nID = data_for_sub['Id']\nsubmission  = pd.DataFrame({'ID': ID,'SalePrice':predictions})\nsubmission.to_csv('submission.csv',index=False)","e8346899":"# Load data","047b29b5":"# Get Dummies Variables","2c6a3dfa":"# Import Train Test Split","3d3f8d73":"Check NaN values, in this notebook I will show percent of NaN values.","7344b9a2":"# Intro explore the data","56b29506":"# PCA ","22b92b96":"# Cleaning data and Proprocessing","94bdc2e2":"Spit data to numerical and categorical columns","4dfcf11f":"# Import Libralies","390e218a":"From above, I found 'Alley', 'PoolQC', 'Fence' and 'MiscFeature' \nhave more than 80% null values. Then I drop these 2 columns","0208727f":"Before using PCA, I have standardize data to same scale.","998b98f6":"# Import model ","8571c10b":"Before Cleaning data, We should declare it.","7f3aee73":"Explore numerical columns","1044a3c4":"From above, 'Exterior1st' has values more than 10 values.Then columns 'Exterior1st' will be dropped.","d1d94e1f":"Set Screen to see all output","773de189":"According from data frame, using n_components = 144 can explanin varaince = 95 %, then I selected n_components = 144","883e3269":"Explore the categorical columns","6d5197dc":"It's hard to find appropriate n_components from graph. I will create dataframe which collected cum explain_ratio.","81dc5f6f":"Check values in catagorical columns "}}