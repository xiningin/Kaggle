{"cell_type":{"08340f95":"code","a3fe1365":"code","21ea49ff":"code","f94956c2":"code","c3e6c498":"code","da6a1291":"code","bf8e7436":"code","73d1ea95":"code","85128067":"code","0a8d4960":"code","c794c7da":"code","a7d5eded":"code","2ffd1338":"code","9a4ea4d5":"code","99471cfb":"code","3527a263":"code","e3981e23":"markdown","ab398246":"markdown","d652bf20":"markdown","afce1f49":"markdown","8038a40a":"markdown","ccd2ab02":"markdown","f511faa3":"markdown","7aff9466":"markdown","18895bc6":"markdown","7893b677":"markdown","0eb97967":"markdown","55107bd6":"markdown","e34138d0":"markdown","fce491cd":"markdown","0a780086":"markdown","4197baa7":"markdown","26e6a963":"markdown","efb881f0":"markdown","cfd9b4fd":"markdown","653cfeb6":"markdown","7f66d665":"markdown","50949a0e":"markdown","6e9d1a99":"markdown","f3741996":"markdown","a36bcd37":"markdown","b33093b8":"markdown","1af0309f":"markdown","3b94b232":"markdown"},"source":{"08340f95":"import pandas as pd\n\niris_data = pd.read_csv(\"..\/input\/iris-data\/iris.csv\", index_col  = \"id\")","a3fe1365":"iris_data.shape","21ea49ff":"iris_data.head()","f94956c2":"iris_data.corr()","c3e6c498":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.title(\"SepalLengthCm & SepalWidthCm Scatter Plots by 'Species' - Correlation : - 0.109369\")\nsns.scatterplot(x = \"SepalLengthCm\", y = \"SepalWidthCm\", hue = \"Species\", data = iris_data);","da6a1291":"iris_model_data = iris_data.loc[:,[\"SepalLengthCm\", \"SepalWidthCm\", \"Species\"]]\niris_model_data.head()","bf8e7436":"independent = iris_data.drop(\"Species\", axis = 1)\ndependent = iris_data[\"Species\"]","73d1ea95":"from sklearn.model_selection import train_test_split\n\nindependent_train, independent_test, dependent_train, dependent_test = train_test_split(\n    independent, \n    dependent, \n    test_size = 0.20, \n    random_state = 50)","85128067":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn_model = knn.fit(independent_train, dependent_train)\n\ndependent_pred = knn_model.predict(independent_test)","0a8d4960":"prediction_df = pd.DataFrame({\n    \"Dependent_Test\": dependent_test,\n    \"Dependent_Predicted\": dependent_pred\n})\n\nprediction_df","c794c7da":"from sklearn.metrics import confusion_matrix\n\ndependent_head_knn = knn.predict(independent_test)\ncm_knn = confusion_matrix(dependent_test,dependent_head_knn)\n\nplt.figure(figsize=(20,10))\n\nplt.subplot(2,3,2)\nplt.title(\"K Nearest Neighbors Confusion Matrix\")\nsns.heatmap(cm_knn,annot=True,cmap = \"Blues\",fmt=\"d\",cbar = True, annot_kws={\"size\": 10})","a7d5eded":"from sklearn import metrics\n\nprint(\"Accuracy:\",metrics.accuracy_score(dependent_test, dependent_pred))","2ffd1338":"import numpy as np\nfrom sklearn.model_selection import GridSearchCV\n\nknn_params = {\"n_neighbors\": np.arange(1,20)}\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, knn_params, cv = 10)\nknn_cv.fit(independent_train, dependent_train)\n\nprint(\"Best Score:\" + str(knn_cv.best_score_))\nprint(\"Best Parameter:\" + str(knn_cv.best_params_))\n","9a4ea4d5":"knn = KNeighborsClassifier(n_neighbors = 8)\nknn_model = knn.fit(independent_train, dependent_train)\n\ndependent_pred = knn_model.predict(independent_test)","99471cfb":"dependent_head_knn = knn.predict(independent_test)\ncm_knn = confusion_matrix(dependent_test,dependent_head_knn)\n\nplt.figure(figsize=(20,10))\n\nplt.subplot(2,3,2)\nplt.title(\"K Nearest Neighbors Confusion Matrix\")\nsns.heatmap(cm_knn,annot=True,cmap = \"Blues\",fmt=\"d\",cbar = True, annot_kws={\"size\": 10})","3527a263":"from matplotlib import pyplot as plt\nfrom sklearn import neighbors, datasets\nfrom matplotlib.colors import ListedColormap\n\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n\niris = datasets.load_iris()\nX = iris.data[:, :2] \ny = iris.target\n\nknn = neighbors.KNeighborsClassifier(n_neighbors = 8)\nknn.fit(X, y)\n\nx_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\ny_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n                        np.linspace(y_min, y_max, 100))\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.figure()\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\nplt.xlabel('sepal length (cm)')\nplt.ylabel('sepal width (cm)')\nplt.axis('tight')","e3981e23":"Since my data size is small, I set a value of 0.20 as test_size.","ab398246":"## Parameter Optimization","d652bf20":"The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems. The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other. \nKNN captures the idea of similarity (sometimes called distance, proximity, or closeness) with some mathematics we might have learned in our childhood\u2014 calculating the distance between points on a graph.\nThere are other ways of calculating distance, and one way might be preferable depending on the problem we are solving. However, the straight-line distance (also called the Euclidean distance) is a popular and familiar choice.\n\nChoosing the right value for K:\nTo select the K that\u2019s right for your data, we run the KNN algorithm several times with different values of K and choose the K that reduces the number of errors we encounter while maintaining the algorithm\u2019s ability to accurately make predictions.\n\n**Advantages:**\n\n-\tThe algorithm is simple and easy to implement.\n-\tThere\u2019s no need to build a model, tune several parameters, or make additional assumptions.\n-\tThe algorithm is versatile. It can be used for classification, regression, and search.\n\n**Disadvantages:**\n\n-\tThe algorithm gets significantly slower as the number of examples and\/or predictors\/independent variables increase.\n","afce1f49":"As we will see in the confusion matrix, there were 3 errors in total in the estimation we made with the model we built. It is not surprising that errors occur between **\"iris_versicolor\"** and **\"irisvirginica\"**. At the beginning of the study, I mentioned that the possibility of error between these two species is possible. In the model , 3 **iris_versicolor** types were estimated as **iris_virginica**.","8038a40a":"## Model","ccd2ab02":"When we use all dependent variables with the KNN algorithm, overfitting occurs. So I'll look at the relationship between variables. The Independent Variable is a (explanatory) variable that is not affected by another variable but is effect of dependent variable or thought to affect it. **I select the variables \"SepalLengthCm\" and \"SepalLengthCm\", with a very small correlation \"-0.109369\" between them.**","f511faa3":"As can be seen, while the number of errors was 3, we reduced it to 2 with parameter optimization. (The reduction in error is satisfactory as the amount of old errors is small.) There is no 100% realization in statistics anyway.","7aff9466":"![ml.PNG](attachment:ml.PNG)","18895bc6":"In the study, I preferred to use the frequently preferred **\"iris\"** data for classification. The data set consists of 50 samples from each of three species of Iris (**Iris setosa**, **Iris virginica** and **Iris versicolor**). Four features were measured from each sample: the **length** and the **width** of the **sepals** and **petals**, in centimeters. ","7893b677":"As dependent variable \"Species\" variable will be used.","0eb97967":"Classification Accuracy is what we usually mean, when we use the term accuracy. It is the ratio of number of correct predictions to the total number of input samples.","55107bd6":"# K - NEAREST NE\u0130GHBOURS","e34138d0":"[Theoretical Reference 1](https:\/\/en.wikipedia.org\/wiki\/Machine_learning)\n\n[Theoretical Reference 2](https:\/\/www.guru99.com\/supervised-vs-unsupervised-learning.html)\n\n[Theoretical Reference 3](https:\/\/en.wikipedia.org\/wiki\/Reinforcement_learning)\n\n[Theoretical Reference 4](https:\/\/en.wikipedia.org\/wiki\/Statistical_classification)\n\n[Theoretical Reference 5](https:\/\/en.wikipedia.org\/wiki\/Regression_analysis)\n\n[Theoretical Reference 6](https:\/\/towardsdatascience.com\/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761)\n\n[Theoretical Reference 7](https:\/\/en.wikipedia.org\/wiki\/Iris_flower_data_set)\n\n[Theoretical Reference 8](https:\/\/en.wikipedia.org\/wiki\/Hyperparameter_optimization)\n","fce491cd":"In the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). ","0a780086":"![](https:\/\/miro.medium.com\/max\/2550\/0*QHogxF9l4hy0Xxub.png)","4197baa7":"In machine learning, parameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned.\n\nThe same kind of machine learning model can require different constraints, weights or learning rates to generalize different data patterns. These measures are called hyperparameters, and have to be tuned so that the model can optimally solve the machine learning problem. Hyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given independent data. The objective function takes a tuple of hyperparameters and returns the associated loss.  Cross-validation is often used to estimate this generalization performance.\n\nWith parameter optimization, my goal is to determine the optimum number of **neighbors(k)**.","26e6a963":"Machine learning (ML) is the study of computer algorithms that improve automatically through experience. It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to do so.\n\nMachine learning approaches are traditionally divided into three broad categories:\n\n-\t**Supervised Machine Learning:** In Supervised learning, you train the machine using data which is well \"labeled.\" It means some data is already tagged with the correct answer. A supervised learning algorithm learns from labeled training data, helps you to predict outcomes for unforeseen data.\n-\t**Unsupervised Machine Learning:** Unsupervised learning is a machine learning technique, where you do not need to supervise the model. Instead, you need to allow the model to work on its own to discover information. It mainly deals with the unlabelled data.\n-\t**Reinforcement learning (RL)** is an area of machine learning concerned with how software agents ought to take actions in an environment in order to maximize the notion of cumulative reward.\n","efb881f0":"## Data","cfd9b4fd":"## Train & Test Split","653cfeb6":"In statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known. Examples are assigning a given email to the \"spam\" or \"non-spam\" class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.)","7f66d665":"# Application","50949a0e":"# Machine Learning","6e9d1a99":"![](https:\/\/miro.medium.com\/max\/1064\/1*5XuZ_86Rfce3qyLt7XMlhw.png)","f3741996":"# CLASSIFICATION","a36bcd37":"Our aim in this study is to determine the iris type by using the **\"K neirest neighbors\"** algorithm, one of the classification algorithms.","b33093b8":"When we look at the distribution of variables that we will apply KNN according to the dependent variable, we can make an instinctive prediction about the success of the model. **Since the distribution of \"iris_setosa\" type does not intersect with \"iris_versicolor\" and \"irisvirginica\", it will be the most successful type in the KNN algorithm.** I'll go back to this chapter in the confusion matrix.","1af0309f":"Some of the Machine Learning Classification Algorithms:\n-\tLogistic Regression\n-\tNaive Bayes \n-\tDecision Tree\n-\t**K-Nearest Neighbours**\n-\tSupport Vector Machine\n-\tRandom Forest\n","3b94b232":"At the beginning of the study, I mentioned that the possibility of error between these two species is possible. (This value is quite high.)"}}