{"cell_type":{"22e18fa9":"code","ddab0125":"code","a25e3e20":"code","0e618df7":"code","73bb7549":"code","d0f95aaa":"code","dba5fcf7":"code","51665877":"code","73b39400":"code","d49393b3":"code","6ad16e9d":"code","0fc89afb":"code","cf3b23e7":"code","1679b896":"code","69ce6318":"code","42d78d6c":"code","87a27782":"code","79eb0469":"code","06b01fcf":"code","e1b66299":"code","75def100":"code","41ef0d3b":"code","cea0ebe1":"code","b0f7f9e6":"code","8548c478":"code","1baf4dfe":"code","26c439c7":"code","dc2eb0ec":"code","91029483":"code","7dc3578c":"code","4d1d3dad":"code","3ad4fa3c":"code","75a09018":"code","b098caab":"code","36aa37df":"code","409ae7a9":"code","7413c087":"code","b3f0de88":"code","82cdb936":"code","18ea8984":"code","9d154d80":"code","eeb29a75":"code","8822dd6b":"code","8c9ec45c":"code","96e2519b":"code","c24b92a8":"code","422870eb":"code","3a103054":"code","4b689532":"code","4ed4f763":"code","3c51d783":"code","c348d7e4":"markdown","4c7a4499":"markdown","abd78042":"markdown","e7fa88e8":"markdown","ab5ac761":"markdown","f813d93a":"markdown","6880ca69":"markdown","cbc8cadf":"markdown","4c3878b8":"markdown","3100dbd2":"markdown","1413f45b":"markdown","2bef1170":"markdown","67c00541":"markdown","9af9befc":"markdown","bad5f5bf":"markdown","9c0edee7":"markdown","6b427726":"markdown","3b91cf00":"markdown","c76395d6":"markdown","c8899010":"markdown","e4e60553":"markdown","fc8d65eb":"markdown","00ae2d2c":"markdown","dad92c29":"markdown","e694b2f8":"markdown","f526c041":"markdown","a60d7b1a":"markdown","0adcaacd":"markdown","1c522466":"markdown","8842a74e":"markdown"},"source":{"22e18fa9":"!pip install -q git+https:\/\/github.com\/rwightman\/pytorch-image-models.git\n!pip install -q torchsummary\n!pip install -q -U git+https:\/\/github.com\/albu\/albumentations --no-cache-dir\n!pip install -q neptune-client ","ddab0125":"import math\nimport os\nimport random\nimport warnings\nfrom typing import *\n\nimport albumentations\nimport albumentations as A\nimport cv2\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport seaborn as sn\nimport seaborn as sns\nimport timm\nimport gc\nimport torch\nimport torch.nn.functional as F\nimport torchvision\nimport cuml\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom IPython.display import clear_output\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import (CosineAnnealingLR,\n                                      CosineAnnealingWarmRestarts,\n                                      ReduceLROnPlateau, _LRScheduler)\nfrom torch.optim.optimizer import Optimizer\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchsummary import summary\nfrom torchvision import models\nfrom tqdm import tqdm\nfrom tqdm.notebook import tqdm\n\nwarnings.filterwarnings(\"ignore\")\nclear_output()\n","a25e3e20":"CONFIG = {\n    \"COMPETITION_NAME\": \"SETI Breakthrough Listen - E.T. Signal Search\",\n    \"MODEL\": {\"MODEL_FACTORY\": \"timm\", \"MODEL_NAME\": \"efficientnet_b3\"},\n    \"WORKSPACE\": \"KAGGLE\",\n    \"DATA\": {\n        \"TARGET_COL_NAME\": \"target\",\n        \"IMAGE_COL_NAME\": \"id\",\n        \"NUM_CLASSES\": 1,\n        \"CLASS_LIST\": [0, 1],\n        \"IMAGE_SIZE\": 512,\n        \"CHANNEL_MODE\": \"spatial_3ch\",\n        \"IS_TRANSPOSE\": False,\n        \"USE_MIXUP\": True\n    },\n    \"CROSS_VALIDATION\": {\"SCHEMA\": 'StratifiedKFold', \"NUM_FOLDS\": 5},\n    \"TRAIN\": {\n        \"DATALOADER\": {\n            \"batch_size\": 32,\n            \"shuffle\": True,  # using random sampler\n            \"num_workers\": 4,\n            \"drop_last\": False,\n        },\n        \"SETTINGS\": {\n            \"IMAGE_SIZE\": 512,\n            \"NUM_EPOCHS\": 20,\n            \"USE_AMP\": True,\n            \"USE_GRAD_ACCUM\": False,\n            \"ACCUMULATION_STEP\": 1,\n            \"DEBUG\": False,\n            \"VERBOSE\": True,\n            \"VERBOSE_STEP\": 10,\n        },\n    },\n    \"VALIDATION\": {\n        \"DATALOADER\": {\n            \"batch_size\": 32,\n            \"shuffle\": False,\n            \"num_workers\": 4,\n            \"drop_last\": False,\n        }\n    },\n    \"TEST\": {\n        \"DATALOADER\": {\n            \"batch_size\": 32,\n            \"shuffle\": False,\n            \"num_workers\": 4,\n            \"drop_last\": False,\n        }\n    },\n    \"OPTIMIZER\": {\n        \"NAME\": \"AdamW\",\n        # if use big model like nfnet change lr to 1e-5\n        \"OPTIMIZER_PARAMS\": {\"lr\": 1e-4, \"eps\": 1.0e-8, \"weight_decay\": 1.0e-3},\n    },\n    \"SCHEDULER\": {\n        \"NAME\": \"CosineAnnealingWarmRestarts\",\n        \"SCHEDULER_PARAMS\": {\n            \"T_0\": 19,\n            \"T_mult\": 1,\n            \"eta_min\": 1.0e-7,\n            \"last_epoch\": -1,\n            \"verbose\": True,\n            # \"NAME\": \"CosineAnnealingLR\",\n            # \"SCHEDULER_PARAMS\": {\n            #     \"T_max\": 16,\n            #     \"eta_min\": 1.0e-7,\n            #     \"last_epoch\": -1,\n            #     \"verbose\": True,\n        },\n        \"CUSTOM\": \"GradualWarmupSchedulerV2\",\n        \"CUSTOM_PARAMS\": {\"multiplier\": 10, \"total_epoch\": 1},\n        \"VAL_STEP\": False,\n    },\n    \"CRITERION_TRAIN\": {\n        \"NAME\": \"BCEWithLogitsLoss\",\n        \"LOSS_PARAMS\": {\n            \"weight\": None,\n            \"size_average\": None,\n            \"reduce\": None,\n            \"reduction\": \"mean\",\n            \"pos_weight\": None\n        },\n    },\n    \"CRITERION_VALIDATION\": {\n        \"NAME\": \"BCEWithLogitsLoss\",\n        \"LOSS_PARAMS\": {\n            \"weight\": None,\n            \"size_average\": None,\n            \"reduce\": None,\n            \"reduction\": \"mean\",\n            \"pos_weight\": None\n        },\n    },\n    \"TRAIN_TRANSFORMS\": {\n        # \"RandomResizedCrop\": {\"height\": 384, \"width\": 384, \"scale\": [0.9, 1.0], \"p\": 1},\n        \"VerticalFlip\": {\"p\": 0.4},\n        \"HorizontalFlip\": {\"p\": 0.4},\n        \"ShiftScaleRotate\": {\"rotate_limit\": 10, \"p\": 0.4},\n        \"Resize\": {\"height\": 512, \"width\": 512, \"p\": 1},\n        # \"Normalize\": {\"mean\": (0.485, 0.456, 0.406), \"std\": (0.229, 0.224, 0.225)},\n\n    },\n    \"VALID_TRANSFORMS\": {\n        \"Resize\": {\"height\": 512, \"width\": 512, \"p\": 1},\n        # \"Normalize\": {\"mean\": (0.485, 0.456, 0.406), \"std\": (0.229, 0.224, 0.225)},\n    },\n    \"TEST_TRANSFORMS\": {\n        \"Resize\": {\"height\": 512, \"width\": 512, \"p\": 1},\n        # \"Normalize\": {\"mean\": (0.485, 0.456, 0.406), \"std\": (0.229, 0.224, 0.225)},\n    },\n    \"PATH\": {\n        \"ROOT_DIR\": \"..\/input\/seti-breakthrough-listen\",\n        \"TRAIN_CSV\": \"..\/input\/seti-breakthrough-listen\/train_labels.csv\",\n        \"TRAIN_PATH\": \"..\/input\/seti-breakthrough-listen\/train\",\n        \"TEST_CSV\": \"..\/input\/seti-breakthrough-listen\/sample_submission.csv\",\n        \"TEST_PATH\": \"..\/input\/seti-breakthrough-listen\/test\",\n        \"WEIGHTS_PATH\": \"..\/input\/et-alien-weights\",\n        \"OOF_PATH\": \"\",\n        \"LOG_PATH\": \".\/log.txt\"\n    },\n    \"SEED\": 19921930,\n    \"DEVICE\": \"cuda\",\n    \"GPU\": \"P100\",\n}","0e618df7":"config = CONFIG","73bb7549":"df_train = pd.read_csv(config[\"PATH\"][\"TRAIN_CSV\"])\ndf_test  = pd.read_csv(config[\"PATH\"][\"TEST_CSV\"])","d0f95aaa":"px.histogram(df_train, y=\"target\", color=\"target\", title='Target Distribution')","dba5fcf7":"def get_train_filename_by_id(image_id: str) -> str:\n    \"\"\"This function takes in an filename id and returns the path of this file.\n\n    Args:\n        image_id (str): [description]\n\n    Returns:\n        str: [description]\n    \"\"\"\n    return f\"..\/input\/seti-breakthrough-listen\/train\/{image_id[0]}\/{image_id}.npy\"\n\n\ndef get_test_filename_by_id(image_id: str) -> str:\n    \"\"\"This function takes in an filename id and returns the path of this file.\n\n    Args:\n        image_id (str): An example of an image_id is cc9526e839463b1. \n                        Note that image_id[0] = c gives you the subfolder containing images that starts with c.\n                        You can see the usage in the return statement.\n\n    Returns:\n        str: [description]\n    \"\"\"\n    return f\"..\/input\/seti-breakthrough-listen\/test\/{image_id[0]}\/{image_id}.npy\"","51665877":"# Seems need to find easy\/hard images as the data refreshes\n\neasy_image_1 = get_train_filename_by_id(image_id = 'cd73ff1954feeb9')\neasy_image_2 = get_train_filename_by_id(image_id = '0e55f80554f8d36')\nmedium_image_1 = get_train_filename_by_id(image_id = '886f7aa765d6282')\nmedium_image_2 = get_train_filename_by_id(image_id = '56fe32cddc6d17e')\n# This hard_image_1 is difficult because all channels seem to have signal but they are not.\nhard_image_1 = get_train_filename_by_id(image_id = 'cc9526e839463b1')\nhard_image_2 = get_train_filename_by_id(image_id = 'a5abd8d2eafd618')\ntest_image = get_test_filename_by_id(image_id = '10013eb3e11e199')\n\n\nimage_list : List[str] = [easy_image_1, easy_image_2,\n                           medium_image_1, medium_image_2,\n                           hard_image_1, hard_image_2]","73b39400":"df_train['file_path'] = df_train['id'].apply(get_train_filename_by_id)\ndf_test['file_path']  = df_test['id'].apply(get_test_filename_by_id)\ndisplay(df_train.head())","d49393b3":"def show_cadence_channels(filename: str, label: int, show_text: bool = True) -> None:\n    \"\"\"Plot and show cadence as images. Note on channel = odd channels\n       This plot shows all 6 cadences with ON and OFF channels. The host says extraterrestrial signals\n       should appear in ON channels.\n\n    Args:\n        filename (str): [description]\n        label (int): [description]\n    \"\"\"\n    plt.figure(figsize=(16, 10))\n    # load .npy files into np.array\n    image_arr = np.load(filename)\n\n    for i in range(6):\n        plt.subplot(6, 1, i + 1)\n        if i == 0:\n            plt.title(\n                f\"ID: {os.path.basename(filename)} TARGET: {label}\", fontsize=18)\n        plt.tight_layout()\n        plt.imshow(image_arr[i].astype(float),\n                   interpolation='nearest', aspect='auto')\n        if show_text:\n            plt.text(5, 100, [\"ON\", \"OFF\"][i % 2], bbox={'facecolor': 'white'})\n        plt.xticks(np.arange(0, 255, step=25))\n        plt.yticks(np.arange(0, 275, step=100))\n        plt.ylabel(ylabel='Time Axis')\n    plt.show()\n","6ad16e9d":"df_tmp = df_train[df_train[\"target\"] == 0].sample(1)\nfor ind, row in df_tmp.iterrows():\n    show_cadence_channels(get_train_filename_by_id(row[\"id\"]), row[\"target\"], show_text=True)\n\ndf_tmp = df_train[df_train[\"target\"] == 1].sample(1)\nfor ind, row in df_tmp.iterrows():\n    show_cadence_channels(get_train_filename_by_id(row[\"id\"]), row[\"target\"], show_text=True)","0fc89afb":"def show_cadence_spatial(filename: str, label: int, show_text: bool = True, spatial_mode: str = 'spatial_3ch', transpose: bool = True) -> None:\n    \"\"\"Plot and show cadence as images. Note on channel = odd channels\n       This plot shows all 6 cadences with ON and OFF channels. The host says extraterrestrial signals\n       should appear in ON channels.\n\n    Args:\n        filename (str): [description]\n        label (int): [description]\n    \"\"\"\n    plt.figure(figsize=(16, 10))\n    # load .npy files into np.array\n    image_arr = np.load(filename)\n    assert image_arr.shape == (6, 273, 256)\n\n    if spatial_mode == 'spatial_3ch':\n        image_arr_3ch = image_arr[::2, :, :].astype(np.float32)\n        assert image_arr_3ch.shape == (3, 273, 256)\n        # print(np.array_equal(image_arr_3ch, image[::2].astype(np.float32), equal_nan=False))\n        if transpose is True:\n            image_arr_3ch = np.vstack(image_arr_3ch).transpose((1, 0))\n            assert image_arr_3ch.shape == (256, 819)\n        else:\n            image_arr_3ch = np.vstack(image_arr_3ch)\n            assert image_arr_3ch.shape == (819, 256)\n\n        plt.title(\n            f\"ID: {os.path.basename(filename)} TARGET: {label}\", fontsize=18)\n        #plt.subplot(5, 2, i + 1)\n        plt.tight_layout()\n        plt.imshow(image_arr_3ch)\n\n    elif spatial_mode == 'spatial_6ch':\n        # note [:, :, :] produces a verbatim copy of the array\n        image_arr_6ch = image_arr[:, :, :].astype(np.float32)\n        assert image_arr_6ch.shape == (3, 273, 256)\n\n        if transpose is True:\n            image_arr_6ch = np.vstack(image_arr_6ch).transpose((1, 0))\n            assert image_arr_6ch.shape == (256, 1638)\n        else:\n            image_arr_6ch = np.vstack(image_arr_6ch)\n            assert image_arr_6ch.shape == (1638, 256)\n\n        #plt.subplot(5, 2, i + 1)\n        plt.tight_layout()\n        plt.imshow(image_arr_6ch)\n\n    plt.show()","cf3b23e7":"df_tmp = df_train[df_train[\"target\"] == 0].sample(1)\nfor ind, row in df_tmp.iterrows():\n    show_cadence_spatial(get_train_filename_by_id(row[\"id\"]), row[\"target\"], show_text=True)\n\ndf_tmp = df_train[df_train[\"target\"] == 1].sample(1)\nfor ind, row in df_tmp.iterrows():\n    show_cadence_spatial(get_train_filename_by_id(row[\"id\"]), row[\"target\"], show_text=True)","1679b896":"# for images in image_list:\n#     show_cadence_channels(filename=images, label = 1)","69ce6318":"show_cadence_spatial(filename=easy_image_1, label = 1, transpose=True)","42d78d6c":"show_cadence_spatial(filename=easy_image_1, label = 1, transpose=False)","87a27782":"class Transform:\n    # The variant here uses `ToTensorV2` so you do not need to transpose in Dataset as Albumentations does it for you they can detect if you using channels first, else will throw error.\n    def __init__(self, aug_kwargs: Dict):\n\n        albu_augs = [getattr(A, name)(**kwargs)\n                     for name, kwargs in aug_kwargs.items()]\n        albu_augs.append(ToTensorV2(p=1))\n\n        self.transform = A.Compose(albu_augs)\n\n    def __call__(self, image: Union[np.ndarray, torch.tensor]):\n        image = self.transform(image=image)[\"image\"]\n        return image\n\n","79eb0469":"def seed_all(seed: int = 1930) -> None:\n    \"\"\"Seeds all random number generators.\n\n    Args:\n        seed (int, optional): [description]. Defaults to 1930.\n    \"\"\"\n\n    print(\"Using Seed Number {}\".format(seed))\n\n    os.environ[\"PYTHONHASHSEED\"] = str(\n        seed\n    )  # set PYTHONHASHSEED env var at fixed value\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.cuda.manual_seed(seed)  # pytorch (both CPU and CUDA)\n    np.random.seed(seed)  # for numpy pseudo-random generator\n\n    # set fixed value for python built-in pseudo-random generator\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.enabled = False\n\n\ndef seed_worker(_worker_id: int) -> None:\n    \"\"\"Seed a worker with the given ID. For Torch users.\n\n    Args:\n        _worker_id (int): [description]\n    \"\"\"\n    worker_seed = torch.initial_seed() % 2 ** 32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)","06b01fcf":"seed_all(config['SEED'])","e1b66299":"class AlienTrainDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, config: CONFIG, transform: Transform = None, mode: str = 'train'):\n        self.df = df\n        self.config = config\n        self.transform = transform\n        self.mode = mode\n\n        self.file_names: Union[List[str],\n                               np.ndarray[str]] = df['file_path'].values\n\n        self.labels: Union[List[int], np.ndarray[int]\n                           ] = df[config['DATA']['TARGET_COL_NAME']].values\n\n    def __len__(self):\n        \"\"\"Method - Len of the dataset\n\n        Returns:\n            [type]: [description]\n        \"\"\"\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        image_arr = np.load(self.file_names[idx]).astype(np.float32)\n        assert image_arr.shape == (6, 273, 256)\n\n        if self.config['DATA']['CHANNEL_MODE'] == 'spatial_6ch':\n            \n            if self.config['DATA']['IS_TRANSPOSE']:\n                image_arr = np.vstack(image_arr).transpose((1, 0))\n                assert image_arr.shape == (256, 1638)\n            else:\n                image_arr = np.vstack(image_arr)\n                assert image_arr.shape == (1638, 256)\n\n        elif self.config['DATA']['CHANNEL_MODE'] == 'spatial_3ch':\n            image_arr = image_arr[::2, :, :].astype(np.float32)\n            assert image_arr.shape == (3, 273, 256)\n            # print(np.array_equal(image_arr_3ch, image[::2].astype(np.float32), equal_nan=False))\n            if self.config['DATA']['IS_TRANSPOSE']:\n                image_arr = np.vstack(image_arr).transpose((1, 0))\n                assert image_arr.shape == (256, 819)\n            else:\n                image_arr = np.vstack(image_arr)\n                assert image_arr.shape == (819, 256)\n\n        elif self.config['DATA']['CHANNEL_MODE'] == '6_channel':\n            image_arr = image_arr.astype(np.float32)\n            image_arr = np.transpose(image_arr, (1, 2, 0))\n\n        elif self.config['DATA']['CHANNEL_MODE'] == '3_channel':\n            image_arr = image_arr[::2].astype(np.float32)\n            image_arr = np.transpose(image_arr, (1, 2, 0))\n\n        if self.transform:\n            image_arr = self.transform(image_arr)\n        else:\n            image_arr = torch.from_numpy(image_arr).float()\n\n        if self.mode == 'test':\n            return {\"image\": image_arr}\n        else:\n            label = torch.tensor(self.labels[idx]).float()\n            return {\"image\": image_arr, \"target\": label}\n","75def100":"train_dataset = AlienTrainDataset(df=df_train, config=config,\n                                  transform=Transform(config[\"TRAIN_TRANSFORMS\"]),\n                                  mode='train')\n\nfor i in range(2):\n    image, label = train_dataset[i]['image'], train_dataset[i]['target']\n    plt.imshow(image[0])\n    plt.title(f'label: {label}')\n    plt.show()\nimage.shape","41ef0d3b":"sigmoid = torch.nn.Sigmoid()\n\n\nclass Swish(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, i):\n        result = i * sigmoid(i)\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n        sigmoid_i = sigmoid(i)\n        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n\n\nclass Swish_Module(torch.nn.Module):\n    def forward(self, x):\n        return Swish.apply(x)","cea0ebe1":"class AlienSingleHead(torch.nn.Module):\n    \"\"\"A custom model.\"\"\"\n\n    def __init__(\n        self,\n        config: type,\n        pretrained: bool = True,\n    ):\n        \"\"\"Construct a custom model.\"\"\"\n        super().__init__()\n        self.config = config\n        self.pretrained = pretrained\n        print(\"Pretrained is {}\".format(self.pretrained))\n        # self.activation = Swish_Module()\n        self.activation = Swish_Module()\n        self.architecture = {\n            \"backbone\": None,\n            \"bottleneck\": None,\n            \"classifier_head\": None,\n        }\n\n        def __setattr__(self, name, value):\n            self.model.__setattr__(self, name, value)\n\n        _model_factory = (\n            timm.create_model\n            if self.config[\"MODEL\"][\"MODEL_FACTORY\"] == \"timm\"\n            else geffnet.create_model\n        )\n        if config['DATA']['CHANNEL_MODE'] == 'spatial_6ch' or config['DATA']['CHANNEL_MODE'] == 'spatial_3ch':\n\n            self.model = _model_factory(\n                model_name=self.config[\"MODEL\"][\"MODEL_NAME\"],\n                pretrained=self.pretrained, in_chans=1) # set channel = 1 since we using spatial\n\n        else:\n            self.model = _model_factory(\n                            model_name=self.config[\"MODEL\"][\"MODEL_NAME\"],\n                            pretrained=self.pretrained, in_chans=3) # set channel = 1 since we using spatial\n\n        # reset head\n        self.model.reset_classifier(num_classes=0, global_pool=\"avg\")\n        # after resetting, there is no longer any classifier head, therefore it is the backbone now.\n        self.architecture[\"backbone\"] = self.model\n        # get out features of the last cnn layer from backbone, which is also the in features of the next layer\n\n        self.in_features = self.architecture[\"backbone\"].num_features\n        print(self.in_features)\n\n        # self.single_head_fc = torch.nn.Sequential(\n        #     torch.nn.Linear(self.in_features, self.config[\"DATA\"][\"NUM_CLASSES\"])\n        # )\n        self.single_head_fc = torch.nn.Sequential(\n            torch.nn.Linear(self.in_features, self.in_features),\n            self.activation,\n            torch.nn.Dropout(p=0.5),\n            torch.nn.Linear(self.in_features, self.config[\"DATA\"][\"NUM_CLASSES\"]),\n        )\n        self.architecture[\"classifier_head\"] = self.single_head_fc\n\n\n    # feature map after cnn layer\n    def extract_features(self, x):\n        feature_logits = self.architecture[\"backbone\"](x)\n        assert feature_logits.shape[1] == self.in_features , \"feature_logits is the output logits right after the CNN extraction layer, in other words, it is the output to be fed in to the head layer. Thus the shape must match.\\\n                                                                as an example, if batch_size is 4, then at this stage the shape should be [4, in_features].\"\n        \n        # TODO: caution, if you use forward_features, then you need reshape. See test.py\n        return feature_logits\n\n    def forward(self, x):\n        feature_logits = self.extract_features(x)\n        # print(self.architecture[\"classifier_head\"][3])\n        classifier_logits = self.architecture[\"classifier_head\"](feature_logits)\n        return classifier_logits\n","b0f7f9e6":"model = AlienSingleHead(config,pretrained=False)\ntrain_dataset = AlienTrainDataset(df_train, config, transform=Transform(config[\"TRAIN_TRANSFORMS\"]))\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True,\n                          num_workers=4, pin_memory=True, drop_last=True)\n\nfor data in train_loader:\n    image, label = data['image'], data['target']\n    output = model(image)\n    print(output)\n    break","8548c478":"dataset_train = AlienTrainDataset(\n    config=config,\n    df=df_train,\n    mode=\"train\",\n    transform=Transform(config[\"VALID_TRANSFORMS\"]), # use valid transforms no augmentation\n)\ndataset_test = AlienTrainDataset(\n    config=config,\n    df=df_test,\n    mode=\"test\",\n    transform=Transform(config[\"TEST_TRANSFORMS\"]),\n)\n\ntrain_loader = torch.utils.data.DataLoader(\n    dataset_train,\n    # sampler=RandomSampler(dataset_train),\n    **config[\"VALIDATION\"][\"DATALOADER\"], # DO not use train dataloader, else will cause issue cause shuffle\n)\ntest_loader = torch.utils.data.DataLoader(\n    dataset_test, **config[\"TEST\"][\"DATALOADER\"]\n)\n\n","1baf4dfe":"efficientnet_b3 = AlienSingleHead(config, pretrained=False)","26c439c7":"model_path = \"..\/input\/et-alien-weights\/efficientnet_b3_fold_1_epoch_18.pt\"\nefficientnet_b3.load_state_dict(torch.load(model_path, map_location=config['DEVICE'])[\"model_state_dict\"])","dc2eb0ec":"def extract_features(model, loader):\n    \"\"\"When you plot the feature embeddings, there should be a clear boundary between classes. See ArcFace MNIST Notebook.\n\n    Args:\n        model ([type]): [description]\n        loader ([type]): [description]\n        device ([type]): [description]\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    model.to(config['DEVICE'])\n    model.eval()\n    emb_list = []\n    pred_list = []\n    with torch.no_grad():\n        for batch in tqdm(loader):\n            x = batch['image'].to(config['DEVICE'])\n            embeddings = model.extract_features(x) \n            # embeddings = model.architecture[\"backbone\"](x) \n            y_logits   = model.architecture[\"classifier_head\"](embeddings) \n\n            emb_list.append(embeddings.detach().cpu().numpy())\n            pred_list.append(y_logits.detach().cpu().numpy())\n\n        emb_arr = np.concatenate(emb_list)\n        pred_arr = np.concatenate(pred_list)\n        del emb_list\n        del pred_list\n    return emb_arr, pred_arr","91029483":"train_emb, train_pred = extract_features(efficientnet_b3, train_loader)","7dc3578c":"test_emb, test_pred = extract_features(efficientnet_b3, test_loader)","4d1d3dad":"print(train_emb.shape, train_pred.shape)\nprint(test_emb.shape, test_pred.shape)","3ad4fa3c":"del efficientnet_b3, train_loader, test_loader\ntorch.cuda.empty_cache()\ngc.collect()","75a09018":"all_emb = np.concatenate([train_emb, test_emb], axis=0)\nall_pred = np.concatenate([train_pred, test_pred], axis=0)\nprint(all_emb.shape, all_pred.shape)","b098caab":"all_df = pd.concat([df_train, df_test], axis=0, ignore_index=True)\nall_df[\"target\"].value_counts()","36aa37df":"all_df[\"data_type\"] = \"\"\nall_df.loc[all_df.target == 1.0, \"data_type\"] = \"train_pos\"\nall_df.loc[all_df.target == 0.0, \"data_type\"] = \"train_neg\"\nall_df.loc[all_df.target == 0.5, \"data_type\"] = \"test\"\nall_df[\"data_type\"].value_counts()","409ae7a9":"tsne = cuml.TSNE(n_components=2, perplexity=10.0)\nall_emb_2d = tsne.fit_transform(all_emb)\n\nneg_emb_2d = all_emb_2d[all_df.query(\"data_type == 'train_neg'\").index.values]\npos_emb_2d = all_emb_2d[all_df.query(\"data_type == 'train_pos'\").index.values]\ntest_emb_2d = all_emb_2d[all_df.query(\"data_type == 'test'\").index.values]","7413c087":"fig = plt.figure(figsize=(20,20))\nax_neg = fig.add_subplot(2,2,1)\nax_pos = fig.add_subplot(2,2,2)\nax_posneg = fig.add_subplot(2,2,3)\n\nax_neg.scatter(neg_emb_2d[:, 0],neg_emb_2d[:, 1],color='red',s=10,label='train_non-needles', alpha=0.3)\nax_neg.legend(fontsize=13)\nax_neg.set_title('non-\"needles\" in Train', fontsize=18)\nax_pos.scatter(pos_emb_2d[:, 0],pos_emb_2d[:, 1],color='blue',s=10,label='train_needles', alpha=0.3)\nax_pos.legend(fontsize=13)\nax_pos.set_title('\"needles\" in Train', fontsize=18)\n\nax_posneg.scatter(neg_emb_2d[:, 0],neg_emb_2d[:, 1],color='red',s=10,label='train_non-needles', alpha=0.3)\nax_posneg.scatter(pos_emb_2d[:, 0],pos_emb_2d[:, 1],color='blue',s=10,label='train_needles', alpha=0.3)\nax_posneg.legend(fontsize=13)\nax_posneg.set_title('\"needles\" v.s. non-\"needles\" in Train', fontsize=18)","b3f0de88":"fig = plt.figure(figsize=(20,25))\n\nax_posneg = fig.add_subplot(3,2,1)\nax_test = fig.add_subplot(3,2,2)\nax_negtest = fig.add_subplot(3,2,3)\nax_postest = fig.add_subplot(3,2,4)\nax_all = fig.add_subplot(3,2,5)\n\nax_posneg.scatter(neg_emb_2d[:, 0],neg_emb_2d[:, 1],color='red',s=10, label='train_non-needles', alpha=0.3)\nax_posneg.scatter(pos_emb_2d[:, 0],pos_emb_2d[:, 1],color='blue',s=10, label='train_needles', alpha=0.3)\nax_posneg.legend(fontsize=13)\nax_posneg.set_title('\"needles\" v.s. non-\"needles\" in Train', fontsize=18)\n\nax_test.scatter(test_emb_2d[:, 0],test_emb_2d[:, 1],color='limegreen',s=10, label='test_examples', alpha=0.3)\nax_test.legend(fontsize=13)\nax_test.set_title('examples in Test', fontsize=18)\n\nax_negtest.scatter(test_emb_2d[:, 0],test_emb_2d[:, 1],color='limegreen',s=10, label='test_examples', alpha=0.3)\nax_negtest.scatter(neg_emb_2d[:, 0],neg_emb_2d[:, 1],color='red',s=10, label='train_non-needles', alpha=0.3)\nax_negtest.legend(fontsize=13)\nax_negtest.set_title('non-\"needles\" in Train  v.s. examples in Test', fontsize=18)\n\nax_postest.scatter(test_emb_2d[:, 0],test_emb_2d[:, 1],color='limegreen',s=10, label='test_examples', alpha=0.3)\nax_postest.scatter(pos_emb_2d[:, 0],pos_emb_2d[:, 1],color='blue',s=10, label='train_needles', alpha=0.3)\nax_postest.legend(fontsize=13)\nax_postest.set_title('\"needles\" in Train  v.s. examples in Test', fontsize=18)\n\nax_all.scatter(test_emb_2d[:, 0],test_emb_2d[:, 1],color='limegreen',s=10, label='test_examples', alpha=0.3)\nax_all.scatter(neg_emb_2d[:, 0],neg_emb_2d[:, 1],color='red',s=10, label='train_non-needles', alpha=0.3)\nax_all.scatter(pos_emb_2d[:, 0],pos_emb_2d[:, 1],color='blue',s=10, label='train_needles', alpha=0.3)\nax_all.legend(fontsize=13)\nax_all.set_title('Train v.s. Test', fontsize=18)","82cdb936":"##### AUGMENTATIONS\n# A.Resize(height  = image_size, \n#                            width   = image_size),\n# note ToTensorV2 expands grayscale to one more dim.\naugs = A.Compose([\n                  A.transforms.Normalize(mean=(-0.0001,), std=(0.9055,), max_pixel_value=255.0, always_apply=False, p=1.0),\n                  ToTensorV2(p=1.0),\n                  \n                  ])#","18ea8984":"# ###### DATASET & DATALOADER\n\n# # dataset\n# image_dataset = ImageData(df        = df, \n#                           transform = augs)\n\n# # data loader\n# image_loader = DataLoader(image_dataset, \n#                           batch_size  = batch_size, \n#                           shuffle     = False, \n#                           num_workers = num_workers)","9d154d80":"from typing import *","eeb29a75":"filenames: List = [_id.split(\"\/\")[-1] for _id in image_dataset.file_names]","8822dd6b":"filenames.index(hard_image_1.split(\"\/\")[-1])","8c9ec45c":"image_dataset[47987]","96e2519b":"image_dataset[47987].shape","c24b92a8":"plt.imshow(image_dataset[47987][0], cmap='gray')","422870eb":"from sklearn.metrics import roc_auc_score, roc_curve, auc","3a103054":"list_y_true = [\n    [1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n    [1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n    [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.], #  IMBALANCE\n]\nlist_y_pred = [\n    [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n    [0.9, 0.9, 0.9, 0.9, 0.1, 0.9, 0.9, 0.1, 0.9, 0.1, 0.1, 0.5],\n    [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], #  IMBALANCE\n]\n\nfor y_true, y_pred in zip(list_y_true, list_y_pred):\n    fpr, tpr, _ = roc_curve(y_true, y_pred)\n    roc_auc = auc(fpr, tpr)\n\n    plt.figure(figsize=(5, 5))\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([-0.01, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()","4b689532":"df_submission = pd.read_csv(\"..\/input\/seti-breakthrough-listen\/sample_submission.csv\")\ndf_submission","4ed4f763":"df_submission[\"target\"] = 0.51\ndf_submission.to_csv(\"submission.csv\", index=False)","3c51d783":"df_prepared = pd.read_csv(\"..\/input\/signal-search-submissions\/submission_2021-05-13_20-00-00.csv\", index_col=0)\ndf_prepared.to_csv(\"submission_2021-05-13_20-00-00.csv\")\ndf_prepared = pd.read_csv(\"..\/input\/signal-search-submissions\/submission_2021-05-13_21-00-00.csv\", index_col=0)\ndf_prepared.to_csv(\"submission_2021-05-13_21-00-00.csv\")","c348d7e4":"<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:black; border:0' role=\"tab\" aria-controls=\"home\"><center>Quick Navigation<\/center><\/h3>\n\n* [Overview](#1)\n* [Visualizations](#2)\n* [Targets](#3)\n    \n    \n\n* [Competition Metric](#10)\n* [Sample Submission](#20)\n* [Prepared Submission](#30)","4c7a4499":"Parameters\nin_features \u2013 size of each input sample\n\nout_features \u2013 size of each output sample\n\nbias \u2013 If set to False, the layer will not learn an additive bias. Default: True\n\n---\n\n\nApplies a linear transformation to the incoming data: y = xW^T + b \nAnd our incoming data is [4, in_features] in this case which is x. And if our head is the native `torch.nn.Linear(self.in_features, self.config[\"DATA\"][\"NUM_CLASSES\"])`, then it is clear that our output which is feature logits must be of compatible shape - because we know for a fact that","abd78042":"<a id=\"1\"><\/a>\n<h2 style='background:black; border:0; color:white'><center>Overview<center><h2>","e7fa88e8":"## Model","ab5ac761":"<a id=\"20\"><\/a>\n<h2 style='background:black; border:0; color:white'><center>Sample Submission<center><h2>","f813d93a":"Targets: \n\n- 54000 Positive\n- 6000  Negatives","6880ca69":"## CNN Embeddings","cbc8cadf":"## Config","4c3878b8":"# SETI Breakthrough Listen - E.T. Signal Search - Exploratory Data Analysis\n\nQuick Exploratory Data Analysis for [SETI Breakthrough Listen - E.T. Signal Search](https:\/\/www.kaggle.com\/c\/seti-breakthrough-listen\/) challenge    \n\n**\u201cAre we alone in the Universe?\u201d**\n\n\nIn this competition, use your data science skills to help identify anomalous signals in scans of Breakthrough Listen targets. Because there are no confirmed examples of alien signals to use to train machine learning algorithms, the team included some simulated signals (that they call \u201cneedles\u201d) in the haystack of data from the telescope. They have identified some of the hidden needles so that you can train your model to find more. The data consist of two-dimensional arrays, so there may be approaches from computer vision that are promising, as well as digital signal processing, anomaly detection, and more. The algorithm that\u2019s successful at identifying the most needles will win a cash prize, but also has the potential to help answer one of the biggest questions in science.","3100dbd2":"<a id=\"10\"><\/a>\n<h2 style='background:black; border:0; color:white'><center>Competition Metric<center><h2>","1413f45b":"![Image](https:\/\/i.ibb.co\/JFQ44tB\/channel-vs-spatial.png)","2bef1170":"#### Easy to find\n![](https:\/\/i.imgur.com\/5ohQpvE.png)\n\n#### Medium\n![](https:\/\/i.imgur.com\/Pz6YdoV.png)\n![](https:\/\/i.imgur.com\/81jL2N7.png)\n\n#### Hard\n![](https:\/\/i.imgur.com\/Sgu0k7n.png)","67c00541":"<a id=\"30\"><\/a>\n<h2 style='background:black; border:0; color:white'><center>Prepared Submission<center><h2>","9af9befc":"<a id=\"2\"><\/a>\n<h2 style='background:black; border:0; color:white'><center>Visualizations<center><h2>","bad5f5bf":"![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/23652\/logos\/header.png)","9c0edee7":"**train\/** - a training set of cadence snippet files stored in numpy float16 format (v1.20.1), one file per cadence snippet id, with corresponding labels found in the train_labels.csv file. Each file has dimension (6, 273, 256), with the 1st dimension representing the 6 positions of the cadence, and the 2nd and 3rd dimensions representing the 2D spectrogram.  \n**test\/** - the test set cadence snippet files; you must predict whether or not the cadence contains a \"needle\", which is the target for this competition  \n**sample_submission.csv** - a sample submission file in the correct format  \n**train_labels** - targets corresponding (by id) to the cadence snippet files found in the train\/ folder","6b427726":"In this competition you are tasked with looking for technosignature signals in cadence snippets taken from the Green Bank Telescope (GBT)","3b91cf00":"I experiments with these two excellent kernels, try to retrain and ensemble them:   \n[SETI \/ NFNet_l0 starter [inference]](https:\/\/www.kaggle.com\/yasufuminakama\/seti-nfnet-l0-starter-inference)   \n[SETI-BL: TF Starter TPU \ud83d\ude80](https:\/\/www.kaggle.com\/awsaf49\/seti-bl-tf-starter-tpu)","c76395d6":"## WORK IN PROGRESS...","c8899010":"## Dataset","e4e60553":"## Basic Plotting","fc8d65eb":"Note that `plt.imshow()` is CxHxW and so the H is the vertical axis which is time axis 273, while W is the horizontal axis which is frequency axis.","00ae2d2c":"## Augmentations","dad92c29":"Submissions are evaluated on [area under the ROC curve](https:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic) between the predicted probability and the observed target.","e694b2f8":"[REFERENCE: signal-search-exploratory-data-analysis](https:\/\/www.kaggle.com\/ihelon\/signal-search-exploratory-data-analysis)\n\n[REFERENCE: signal-search-exploratory-data-analysis](https:\/\/www.kaggle.com\/evilpsycho42\/eda-baseline-boolart)\n","f526c041":"[REFERENCE: eda-seti-e-t-train-v-s-test-by-cnn-embedding](https:\/\/www.kaggle.com\/ttahara\/eda-seti-e-t-train-v-s-test-by-cnn-embeddings)","a60d7b1a":"Note that `plt.imshow()` is CxHxW and so the H is the vertical axis which is time axis 273, while W is the horizontal axis which is frequency axis.\n\nWe have a function called `show_cadence_spatial` where we do the following:\n\n1. There are 6 channels in the images of shape (6, 273, 256) where 273 is the time axis, 256 is the frequency axis and 6 is the 6 cadence snippets. Note that `plt.imshow()` is CxHxW and so the H is the vertical axis which is time axis 273, while W is the horizontal axis which is frequency axis.\n> Each file has dimension (6, 273, 256), with the 1st dimension representing the 6 positions of the cadence, and the 2nd and 3rd dimensions representing the 2D spectrogram.\n\n2. Since the following were made clear by the host, we have quite a few options. \n    - spatial 6 channels - shape = (273 * 6, 256) or (256, 273 * 6) if you transpose it.\n        - We **concatenate** all 6 channels **vertically along the time axis (axis=0)**.\n        \n    - spatial 3 channels - shape = (273 * 3, 256) or (256, 273 * 3) if you transpose it.\n        - We **concatenate** channels 1, 3 and 5 **vertically along the time axis (axis=0)**. Note to me it does not make sense to concatenate channels 1, 3 and 5 along the frequency axis horizontally. As one cadence consists of 6 snippets of different time, therefore, in order to use spatial, one can concateneate along time axis to have a \"full overview\" of the sequence of events. One can also consider using RNN or time series to model here but I won't use it.\n    \n    > Not all of the \u201cneedle\u201d signals look like diagonal lines, and they may not be present for the entirety of all three \u201cA\u201d observations, but what they do have in common is that they are only present in some or all of the \u201cA\u201d observations (panels 1, 3, and 5 in the cadence snippets).\n\n    - channel wise: We won't be using it here as empirically speaking, it does not perform as well as spatial wise.\n","0adcaacd":"Apply the two functions on train and test csv to add an additional column `file_path` that indicates the filepath of the images. This is useful as we can directly query the paths during training later.","1c522466":"<a id=\"3\"><\/a>\n<h2 style='background:black; border:0; color:white'><center>Targets<center><h2>","8842a74e":"[REFERENCE: search-for-effective-data-augmentation](https:\/\/www.kaggle.com\/shionhonda\/search-for-effective-data-augmentation)"}}