{"cell_type":{"7199016e":"code","6e61cb0d":"code","b9c50509":"code","7d6c079b":"code","c618f9d5":"code","d17e1fc3":"code","44462ffd":"code","dd474fd8":"code","13d0ce8f":"code","c27144c1":"code","cc22949f":"code","6f9e5ce4":"code","63c5d75f":"code","b3257599":"code","2c92e4f7":"code","7b63eae7":"code","da7e7842":"code","4440ec72":"code","a5cf947f":"code","2a131929":"code","e873ba7c":"code","bf293908":"code","4d0b37c1":"code","e3d88603":"code","55f6cfdd":"code","0a2125b8":"markdown","4892026c":"markdown","7970758a":"markdown","90af08cb":"markdown","0ac2141c":"markdown","759e0c43":"markdown","28af6620":"markdown","ba08e47c":"markdown","41a321c2":"markdown","948addc1":"markdown","83f89a59":"markdown","d167f5f7":"markdown","2f10e812":"markdown","a58d83e7":"markdown","174d0114":"markdown","d903e579":"markdown","70d0a87b":"markdown","5b82d8bf":"markdown","dbe025bb":"markdown","0f047a66":"markdown","5be1782e":"markdown","cf0fe885":"markdown","9b8739b1":"markdown","44ba4e06":"markdown","b1b1b607":"markdown","f11c9196":"markdown","f2de754d":"markdown","4b743132":"markdown","ece10473":"markdown","6a76dc3c":"markdown","2a342038":"markdown"},"source":{"7199016e":"from IPython.display import Image\nImage(\"..\/input\/presentationimages\/CNN_Arch\/CNN_block.jpeg\",width = 1000, height = 600)","6e61cb0d":"import os\nimport cv2 #opencv image processing\nimport time # Runtime calculation\nimport shutil \nimport itertools\nimport tensorflow # Deep Learning Frameworks\nimport numpy as np  # Mathematical and Array Operations\nimport pandas as pd# Data Frame Operations\nfrom PIL import Image # Python Imaging Library\nimport seaborn as sns # Visualization library\nimport matplotlib.pyplot as plt  # Visualization library\n# Deep Learning API functionalities\nfrom tensorflow.keras.models import Model \nfrom sklearn.metrics import confusion_matrix # Confusion matrix for model evaluation \nfrom tensorflow.keras.models import Sequential # Sequential model initialization\nfrom sklearn.model_selection import train_test_split  # function to split sample data to train and test\nfrom tensorflow.keras.optimizers import Adam, Nadam, Ftrl # Optimizer algorithms\nfrom tensorflow.keras.metrics import categorical_crossentropy  # Loss functions\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator # For data pipeline building\n# Neural network layers\nfrom tensorflow.keras.layers import Dense,BatchNormalization,Dropout \nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten,AveragePooling2D\n# To prevent Over-fitting\nfrom tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint","b9c50509":"class_names = os.listdir('..\/input\/skin-cancer-malignant-vs-benign\/train')\nclass_types = len(os.listdir('..\/input\/skin-cancer-malignant-vs-benign\/train'))\nprint('Number of classes for Classification: ',class_types)\nprint(f'The class names are {class_names[0]} and {class_names[1]}')\nprint('--> Count of Train Images <--')\nfor i in class_names:\n    print(i + ':' + str(len(os.listdir('..\/input\/skin-cancer-malignant-vs-benign\/train\/'+i))))\nprint('--> Count of Test Images <--')\nfor i in class_names:\n    print(i + ':' + str(len(os.listdir('..\/input\/skin-cancer-malignant-vs-benign\/test\/'+i))))","7d6c079b":"train_datagen = ImageDataGenerator(rescale=1.\/255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n        '..\/input\/skin-cancer-malignant-vs-benign\/train',\n        target_size=(224,224),\n        batch_size=16,\n        class_mode='binary')\nvalidation_generator = test_datagen.flow_from_directory(\n        '..\/input\/skin-cancer-malignant-vs-benign\/test',\n        target_size=(224,224),\n        batch_size=8,\n        class_mode='binary')\n#model.fit(train_generator,epochs=10,validation_data=validation_generator)","c618f9d5":"# Build a 11 layer Convolutional Neural Network from scratch\ndef define_model():\n    model = Sequential()\n    model.add(Conv2D(32,(3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(224,224,3)))\n    model.add(Conv2D(32,(3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(6, activation='softmax'))\n    opt = tensorflow.keras.optimizers.Adam(learning_rate=0.01)\n    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n# Build a 23 layer CNN from scratch\ndef define_model1():\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(224,224,3)))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Dropout(0.2))\n    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Dropout(0.3))\n    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Dropout(0.4))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n    model.add(Dense(6,activation='softmax'))\n    # compile model\n    opt = tensorflow.keras.optimizers.Adam()\n    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","d17e1fc3":"from IPython.display import Image\nImage('..\/input\/presentationimages\/CNN_Arch\/deep learning architecture.png')","44462ffd":"from tensorflow.keras.applications.vgg16 import VGG16\nbase_model = VGG16(input_shape = (224, 224, 3),include_top = False,weights = 'imagenet')\n# we don\u2019t have to train all the layers, we make them non_trainable\nfor layer in base_model.layers:\n    layer.trainable = False\n# Flatten the output layer to 1 dimension\nx = Flatten()(base_model.output)\n# Add a fully connected layer with 512 hidden units and ReLU activation\nx = Dense(512, activation='relu')(x)\n# Add a dropout rate of 0.5 or any acceptable values as per your need\/complexity\nx = Dropout(0.5)(x)\n# Add a final sigmoid layer for classification\nx = Dense(1,activation='sigmoid')(x) # Use Softmax if class more than 2 classes\nmodel = tensorflow.keras.models.Model(base_model.input,x)","dd474fd8":"base_model = tensorflow.keras.applications.ResNet50(include_top=False,weights=\"imagenet\",input_shape=(224,224,3),classes=6)","13d0ce8f":"tailModel = base_model.output\ntailModel = AveragePooling2D(pool_size=(7, 7))(tailModel)\ntailModel = Flatten(name=\"flatten\")(tailModel)\ntailModel = Dense(256, activation=\"relu\")(tailModel)\ntailModel = Dropout(0.2)(tailModel)\ntailModel = Dense(1, activation=\"softmax\")(tailModel)\n# place the head FC model on top of the base model (this will become\n# the actual model we will train)\nresmodel = Model(inputs=base_model.input, outputs=tailModel)\n# loop over all layers in the base model and freeze them so they will\n# layers not freezed will be updated during the training process\nfor layer in base_model.layers:\n    layer.trainable = False","c27144c1":"mobile = tensorflow.keras.applications.mobilenet.MobileNet()\nx = mobile.layers[-6].output\nx = Dropout(0.20)(x)\npredictions = Dense(1, activation='sigmoid')(x)\n# Combining pre-trained MobileNet model with pedictions layer to help it to connect input and output layers for out purpose\nmob_model = Model(inputs=mobile.input, outputs=predictions)","cc22949f":"# Number of layers in pre-trained model and tuned model\nprint('Number of layers in MobileNet_v1 is ' + str(len(mobile.layers)))\n# We have removed 5 layers from the original imagenet model and appended 2 layers on top of it. LET's check if it is 92 - 5 + 2 = 89\nprint('Number of layers in Tuned MobileNet model is '+ str(len(mob_model.layers)))","6f9e5ce4":"from tensorflow.keras.applications.inception_v3 import InceptionV3\nbase_model = InceptionV3(input_shape = (150, 150, 3), include_top = False, weights = 'imagenet')","63c5d75f":"for layer in base_model.layers:\n    layer.trainable = False\n\nx = Flatten()(base_model.output)\nx = Dense(1024, activation='relu')(x)\nx = Dropout(0.2)(x)\nx = Dense(1, activation='sigmoid')(x)\ninception_model = tensorflow.keras.models.Model(base_model.input, x)","b3257599":"from IPython.display import Image\nImage('..\/input\/presentationimages\/CNN_Arch\/efficientnetvsall.jpeg')","2c92e4f7":"# Install EfficientNet libraries with tensorflow-keras support\n!pip install -U efficientnet","7b63eae7":"import efficientnet.keras as efn","da7e7842":"# Initialize the model\nbase_model = efn.EfficientNetB0(input_shape = (224, 224, 3), include_top = False, weights = 'imagenet') \nfor layer in base_model.layers:\n    layer.trainable = False","4440ec72":"x = base_model.output\nx = Flatten()(x)\nx = Dense(1024, activation=\"relu\")(x)\nx = Dropout(0.5)(x)\npredictions = Dense(1, activation=\"sigmoid\")(x)\nmodel_final = Model(base_model.input,predictions)","a5cf947f":"from tensorflow.keras.applications.nasnet import NASNetMobile, preprocess_input, decode_predictions\nmodel_imagenet = NASNetMobile(weights='imagenet', include_top=True)","2a131929":"logits_layer = model_imagenet.get_layer('predictions')\nmodel_logits = tensorflow.keras.Model(inputs=model_imagenet.input,outputs=logits_layer.output)","e873ba7c":"file_location = \"model.h5\"\n# A feature of CallBack API that keeps track of performance of the model and stores the best performaces at different time steps.\ncheckpoint = ModelCheckpoint(file_location,monitor='val_accuracy',verbose=1,save_best_only=True, mode='max')\n# Used for reducing learning rate when a metric has stopped improving.\nreduce_lr = ReduceLROnPlateau(monitor='val_accuracy',patience=2,verbose=1,mode='max',min_lr=0.00001)\n# List of callbacks used from CallBack APIs                                                        \ncallbacks_list = [checkpoint, reduce_lr]","bf293908":"# Setup Optimizer Algorithm --> Adam (Adaptive Moment Estimation)\nopt_algo = tensorflow.keras.optimizers.Adam()\nloss_type = tensorflow.keras.losses.BinaryCrossentropy()\n#model.compile(optimizer=opt_algo, loss=loss_type,metrics=['accuracy'])","4d0b37c1":"mob_model.compile(optimizer=opt_algo, loss=loss_type,metrics=['accuracy'])","e3d88603":"start = time.time()\nhistory = mob_model.fit(train_generator,epochs=5,validation_data=validation_generator,verbose=1,callbacks=callbacks_list)\nend = time.time()\nprint(f'The time taken to execute is {round(end-start,2)} seconds.')","55f6cfdd":"val_loss,val_acc = mob_model.evaluate(validation_generator) #Picking our best model","0a2125b8":"<h3> Performance of Various Transfer Learning Algorithms vs EfficientNets Versions <\/h3>","4892026c":"# Optimizer | Loss Function | Metric ","7970758a":"# 3) ResNet50 Model\n\n* Research Paper: https:\/\/arxiv.org\/pdf\/1512.03385.pdf\n* Developed by  : <font color=\"Blue\">Microsoft Research <\/font>[Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun]","90af08cb":"# 7) NASNet","0ac2141c":"**VGG16 Implementation using Tensorflow --> Python**","759e0c43":"* Google published both a very exciting paper and source code for a newly designed Convolutional neural network called EfficientNet, that set new records for both accuracy and computational efficiency.\n* Currently there are various version of EfficientNets from EfficientNetB0 to B7 ,differing on size of input images that are to be given to the model for training.\n* The invention of EfficientNet was not a minor improvement but rather an accuracy improvement of up to 6% while on the order of 5 \u2013 10x more efficient than most current CNN\u2019s architectures. Their underlying findings should serve as solid guides for those looking to architect better CNN\u2019s in the future developments.\n* EfficientNet currently tops the 'State of the Art' CNN algorithms both in accuracy and in computational performance efficiency.\n* It is mainly developed by learning the mistakes of previous architctures and its shortcommings.\n* It uses a scaling method that uniformly scales all dimensions of depth\/width\/resolution using a compound coefficient.\n* The huge reduction in parameters and computations required with EfficientNet may open up new opportunities for CNN\u2019s to be used on mobile platforms and represent a big leap forward for Mobile native developments in AI.\n* Learn more @ https:\/\/lessw.medium.com\/efficientnet-from-google-optimally-scaling-cnn-model-architectures-with-compound-scaling-e094d84d19d4","28af6620":"# Installing all Requirements","ba08e47c":"Initialize a base model with imagenet weights and required input size of images","41a321c2":"With Additional Hyper-parameter tuning and Image processing, one can develop a highly competant Deep Learning Diagnostic Model that generalizes better !!","948addc1":"# Compilation Phase","83f89a59":"define Tail model to tune the output to our requirements","d167f5f7":"# 5) Inception or GoogleNet\n\n* Research Paper: https:\/\/static.googleusercontent.com\/media\/research.google.com\/en\/\/pubs\/archive\/43022.pdf\n* Developed by  : Google Inc., University of North Carolina, Chapel Hill, University of Michigan, Ann Arbor and Magic Leap Inc.","2f10e812":"# 1) Custom CNN Models\n\n* <font color = \"red\"> **Convolution Layer** <\/font>(Conv2D used here) : In a convolutional layer, a neuron is only connected to a local area of input neurons instead of full-connection so that the number of parameters to be learned is reduced significantly and a network can grow deeper with fewer parameters. This layer creates a convolution kernel that is wind with layers input which helps produce a tensor of outputs.\n* <font color = \"red\"> **DropOut Layer** <\/font>: Dropout is a technique used to prevent a model from overfitting. The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting and thus making the model generalize better.\n* <font color = \"red\"> **Optimizers** <\/font>: They are algorithms or methods used to change the attributes of your neural network such as 'weights' and 'learning rate' in order to reduce the losses. These algorithms are responsible for reducing the loss,cost function to provide the most accurate results possible. \n* <font color = \"red\"> Types of Optimizers <\/font>: Gradient Descent, Stochastic Gradient Descent, RMSProp, Adam, AdaDelta, Adagrad, Adamax, Nadam, and Ftrl.\n* <font color = \"red\"> Activation Functions <\/font> : Activation function decides, whether a neuron should be activated or not by calculating weighted sum and further adding bias with it. The purpose of the activation function is to introduce non-linearity into the output of a neuron. Examples: ReLU, Leaky ReLU, Parametric ReLU, Softmax,tanh, Swish, Sigmoid, Mish and much more to be innovated !! ","a58d83e7":"# 6) EfficientNet\n\n* Research Paper: https:\/\/arxiv.org\/pdf\/1905.11946.pdf\n* Developed by: Google [ Mingxing Tan, and Quoc V. Le ]","174d0114":"# Fittting our MobileNet model on Skin Lesion Dataset\n* MobileNetv1 Implemenatation for Classifying Malignant and Benign skin lesions\n* With No Feature Engineering or Image Processing done, we still could get 88% Training Accuracy and 86% Validation Accuracy in just 5 Epochs\n\n","d903e579":"# 4) MobileNet\n\n* Research Paper: https:\/\/arxiv.org\/pdf\/1704.04861.pdf\n* Developed by  : Google Inc.","70d0a87b":"### To Note:\n* The problem with the above custom developed CNNs are that their performance decreases with increase in complex data and number of epochs, and tend to overfit at higher epochs and also underfit in some severe highly complex usecases where feature extraction plays a major role in classifiation.\n* These Custom Models fail to perform well on lower data. In real world, most important problem that every data scientist face is 'Data Insufficiency' and 'Class Imbalance' after that. \n* The above mentioned problem is solved by a special technique called '**Transfer Learning**'.","5b82d8bf":"Hope all liked and and understood this kernel on CNN Architectures !! If any mistakes do comment it below !!\n\n# UPVOTE the Notebook, if you Liked it !!","dbe025bb":"# 2) VGGNet16\n* Research Paper:  https:\/\/arxiv.org\/abs\/1409.1556\n* Developed by  : Karen Simonyan, Andrew Zisserman","0f047a66":"# Training Phase","5be1782e":"<h4> What is Computer Vision ? <\/h4>\n<p> Computer vision is a field of artificial intelligence that trains computers to interpret and understand the visual world. Using digital images from cameras and videos and deep learning models, machines can accurately identify and classify objects \u2014 and then react to what they \u201csee.\u201d <\/p>\n<br>\n<h4> What is Convolution Neural Networks and Why is it used ? <\/h4>\n* In machine learning, <font color = \"red\">Convolutional Neural Networks (CNN or ConvNet) <\/font> are complex feed forward neural networks. CNNs are used for image classification and recognition because of its high accuracy. It was proposed by computer scientist Yann LeCun in the late 90s, when he was inspired from the human visual perception of recognizing things. The CNN follows a hierarchical model which works on building a network, like a funnel, and finally gives out a fully-connected layer where all the neurons are connected to each other and the output is processed.\n* The benefit of using CNNs is their ability to develop an internal representation of a two-dimensional image. This allows the model to learn position and scale in variant structures in the data, which is important when working with images.\n\n* Use CNNs For: Image data, Classification prediction problems, and Regression prediction problems.\n* More generally, CNNs work well with data that has a <font color = \"green\">spatial relationship<\/font>.\n\n* The CNN input is traditionally two-dimensional, a field or matrix, but can also be changed to be one-dimensional, allowing it to develop an internal representation of a one-dimensional sequence.\n\n* This allows the CNN to be used more generally on other types of data that has a spatial relationship. For example, there is an order relationship between words in a document of text. There is an ordered relationship in the time steps of a time series.\n\n* Although not specifically developed for non-image data, CNNs achieve state-of-the-art results on problems such as document classification used in sentiment analysis and related problems.","cf0fe885":"There are many other models like Xception, NASNet and combination of existing transfer learning models, increasing performance with decrease in parameters and training time. One of them is NASNet, whose implementation is shown below:-","9b8739b1":"# Fetching the Data using 'flow_from_directory' function ","44ba4e06":"<h3> How to achieve Best performance using limited data and limited hardwares in Image Classification <\/h3>\n<p>The Answer is \"<font color=\"orange\">Transfer Learning<\/font>\" !! <br>\nTransfer Learning is a research problem in Machine Learning that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. Keeping it simple, we say transfer learning as usage of <b>\" Model trained on one task is represented on a second related task.\" <\/b> <\/p>\n<br>\n<p>There are two types of Transfer learning techniques:\n1. <font color=\"blue\">Develop Model Approaches <\/font>\n    2. <font color=\"green\"> Pre-trained Model Approaches <\/font> <\/p>\n<br>\n\n<h3> 1) Develop Model Approaches : <\/h3>\nSelect Data Source -----> Develop Source Model -----> Reuse model -----> Tune model\n<br>\n<p>\n<h3> 2) Pre-trained Model Approaches : <\/h3>\nSelect Source Model -----> Reuse Model -----> Tune model -----> One Generally Freeze the layers of pre-trained Neural Networks models, except output layer.\n<\/p>","b1b1b607":"* When AlexNet was published, it easily won the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) and proved itself to be one of the most capable models for object-detection out there. Its key features include using ReLU instead of the tanh function, optimization for multiple GPUs, and overlapping pooling. It addressed overfitting by using data augmentation and dropout.\n* While previous derivatives of AlexNet focused on smaller window sizes and strides in the first convolutional layer, VGG addresses another very important aspect of CNNs: depth. Let\u2019s go over the architecture of VGG:\n1. <font color=\"red\">Input <\/font>: VGG takes in a 224x224 pixel RGB image. For the ImageNet competition, the authors cropped out the center 224x224 patch in each image to keep the input image size consistent.\n2. <font color=\"red\"> Convolutional Layers<\/font>: The convolutional layers in VGG use a very small receptive field (3x3, the smallest possible size that still captures left\/right and up\/down). There are also 1x1 convolution filters which act as a linear transformation of the input, which is followed by a ReLU unit. The convolution stride is fixed to 1 pixel so that the spatial resolution is preserved after convolution.\n3. <font color=\"red\"> Fully-Connected Layers<\/font>: VGG has three fully-connected layers: the first two have 4096 channels each and the third has 1000 channels, 1 for each class.\n4. <font color=\"red\"> Hidden Layers<\/font>: All of VGG\u2019s hidden layers use ReLU (a huge innovation from AlexNet that cut down training time). Also,VGGNet does not generally use Local Response Normalization (LRN), as LRN increases memory consumption and training time with no particular increase in accuracy.\n* Learn more @ https:\/\/towardsdatascience.com\/vgg-neural-networks-the-next-step-after-alexnet-3f91fa9ffe2c","f11c9196":"# World of Knowledge Sharing -- If Humans can, then Neural Networks too. \n<br>\nHere are some of the Architectural designs of best Transfer Learning techniques that are widely and most popularly used for any problem statement in computer vision field. ","f2de754d":"* Inception Neural network was once considered a state-of-the-art deep learning architecture for solving image recognition and detection problems.\n* Inception Modules are used in Convolutional Neural Networks to allow for more efficient computation and deeper Networks through a dimensionality reduction with stacked 1\u00d71 convolutions. The modules were designed to solve the problem of computational expense, as well as overfitting, among other issues. The solution, in short, is to take multiple kernel filter sizes within the CNN, and rather than stacking them sequentially, ordering them to operate on the same level. \n* Inception Modules are incorporated into convolutional neural networks (CNNs) as a way of reducing computational expense. As a neural net deals with a vast array of images, with wide variation in the featured image content, also known as the salient parts, they need to be designed appropriately. \n* The most simplified version of an inception module works by performing a convolution on an input with not one, but three different sizes of filters (1x1, 3x3, 5x5). Also, max pooling is performed. Then, the resulting outputs are concatenated and sent to the next layer. By structuring the CNN to perform its convolutions on the same level, the network gets progressively wider, not deeper. \n* To make the process even less computationally expensive, the neural network can be designed to add an extra 1x1 convolution before the 3x3 ad 5x5 layers. By doing so, the number of input channels is limited and 1x1 convolutions are far cheaper than 5x5 convolutions. It is important to note, however, that the 1x1 convolution is added after the max-pooling layer, rather than before. \n* The design of this initial Inception Module is known commonly as GoogLeNet, or Inception v1. Additional variations to the inception module have been designed, reducing issues such as the vanishing gradient problem and overfitting.\n* To go Deeper refer: https:\/\/www.analyticsvidhya.com\/blog\/2018\/10\/understanding-inception-network-from-scratch\/","4b743132":"# Model Checkpoints to track optimal weights and performance","ece10473":"* It is based on the concept of 'Residual Learning'. Residual Learning is based on using residual blocks and stacking them   gives neural networks to perform better.\n* Resnet is short name for Residual Network that supports Residual Learning. The 50 indicates the number of layers that it   has. So Resnet50 stands for Residual Network with 50 layers.\n* Simpler Neural netoworks do not perform well on training data due to inability to extract important features from images.    \n  Thus, one will automatically increase the number of layers to increase performance metrics like accuracy and AUROC. But as we   go deeper with the neural networks by adding more layers, the accuracy starts saturating and then also degrades. Residual  \n  training tries to solve this problem.\n*  In residual learning, instead of trying to learn some features, try to learn some residual. Residual can be simply understood as subtraction of feature learned from input of that layer. ResNet does this using shortcut connections (directly connecting input of nth layer to some (n+x)th layer. It has proved that training this form of networks is easier than training simple deep convolutional neural networks and also the problem of degrading accuracy is resolved.\n* With residual blocks, inputs can forward propagate faster through the residual connections across layers and more specially it does use Batch Normalization layers to speed up runtime and reduce overfitting\n* ResNets also tries to solve the Vanishing Gradients problem. It avoids the gradients to decrease exponentially quickly to zero. In ResNet, a \"Short-cut\" or a Skip Connection allows a gradient to be directly back-propagate to earlier layers.\n* For deeper tutorials, click here: https:\/\/cv-tricks.com\/keras\/understand-implement-resnets\/. Probably one of the best tutorials. ","6a76dc3c":"* MobileNets are a family of mobile-first computer vision models for TensorFlow, designed to effectively maximize accuracy while being mindful of the restricted resources for an on-device or embedded application.\n* Also, play a very handly role when there is limited hardware setup like smaller GPUs and CPUs for training your Neural networks on larger datasets.\n* MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases.   They can be built upon for classification, detection, embeddings and segmentation tasks.\n* MobileNets uses 'Depthwise Separable Convolutions'. It significantly reduces the number of parameters when compared to the network with regular convolutions with the same depth in the nets. This results in a light weight Deep Neural Networks.\n* Explore more @ https:\/\/cutt.ly\/tjhmbKl","2a342038":"# Custom CNNs\n\nIt is always better to start form scratch or create baseline CNN models to get a gist of project and set targets and limitations on top of it."}}