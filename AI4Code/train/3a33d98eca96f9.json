{"cell_type":{"2f42b076":"code","7d10f74e":"code","f9271f7c":"code","59fc5115":"code","9a190fcd":"code","681cbcc0":"code","ae00368b":"code","b1bbcf09":"code","ea68aef3":"code","447fa487":"code","7a133a6f":"code","78c26f7f":"code","5e57fe56":"code","2074a95a":"markdown","4a09d25d":"markdown","ddb2e74a":"markdown","3dc71b6c":"markdown","664a1b84":"markdown"},"source":{"2f42b076":"import numpy as np\nimport pandas as pd\nimport sys\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy \nimport sklearn","7d10f74e":"data = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\ndata.columns","f9271f7c":"data.shape","59fc5115":"# random_state helps assure that you always get the same output when you split the data\n# this helps create reproducible results and it does not actually matter what the number is\n# frac is percentage of the data that will be returned\ndata = data.sample(frac = 0.2, random_state = 1)\nprint(data.shape)","9a190fcd":"# plot the histogram of each parameter\ndata.hist(figsize=(20,20))\nplt.show()","681cbcc0":"# determine the number of fraud cases\nfraud = data[data['Class'] == 1]\nvalid = data[data['Class'] == 0]\n \noutlier_frac = len(fraud)\/float(len(valid))\nprint(\"outliar frac: \", outlier_frac)\n\nprint(\"Number of fraud transactions: \", len(fraud))\nprint(\"Number of valid transactions: \", len(valid))","ae00368b":"# Correlaton matrix\ncorr = data.corr()\nfig = plt.figure(figsize=(10,8))\n\nsns.heatmap(corr, cmap='viridis')\nplt.show()","b1bbcf09":"# get the columns from the dataframe\ncolumns = data.columns.tolist()\n\n# filter the columns to remove the data we do not want\ncolumns = [c for c in columns if c not in [\"Class\"]]\n\n# store the variable we will be predicting on which is class\ntarget = \"Class\"\n\n# X includes everything except our class column\nX = data[columns]\n\n# y includes all the class labels for each sample\n# this is also one-dimensional\nY = data[target]\n\n# print the shapes of X and Y\nprint(X.shape)\nprint(Y.shape)","ea68aef3":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor","447fa487":"# define a random state\nstate = 1\n\n# define the outlier detection methods\nclassifiers = {\n    # contamination is the number of outliers we think there are\n    'Isolation Forest': IsolationForest(max_samples = len(X),\n                                       contamination = outlier_frac,\n                                       random_state = state),\n    # number of neighbors to consider, the higher the percentage of outliers the higher you want to make this number\n    'Local Outlier Factor': LocalOutlierFactor(\n    n_neighbors = 20,\n    contamination = outlier_frac)\n}","7a133a6f":"n_outliers = len(fraud)\n\nfor i, (clf_name, clf) in enumerate(classifiers.items()):\n    \n    # fit the data and tag outliers\n    if clf_name == 'Local Outlier Factor':\n        y_pred = clf.fit_predict(X)\n        scores_pred = clf.negative_outlier_factor_\n    else:\n        clf.fit(X)\n        scores_pred = clf.decision_function(X)\n        y_pred = clf.predict(X)\n        \n    # reshape the prediction values to 0 for valid and 1 for fraud\n    y_pred[y_pred == 1] = 0\n    y_pred[y_pred == -1] = 1\n\n    # calculate the number of errors\n    n_errors = (y_pred != Y).sum()\n    \n    # classification matrix\n    print('{}: {}'.format(clf_name, n_errors))\n    print(accuracy_score(Y, y_pred))\n    print(classification_report(Y, y_pred))","78c26f7f":"from sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\n\nX = data[columns]\ny = data[target]\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.4, random_state=100)\nmodel = SVC()\n\nmodel.fit(train_X, train_y)\n\nprediction = model.predict(test_X)","5e57fe56":"print(confusion_matrix(test_y, prediction))\nprint(classification_report(test_y, prediction))\nprint('\\n')\nprint(accuracy_score(test_y, prediction))","2074a95a":"# Applying Algorithms","4a09d25d":"You can see a lot of the values are close to 0 . Most of them are fairly unrelated. The lighter squares signify a stronger correlation.","ddb2e74a":"# Predicton with SVM","3dc71b6c":"# Organizing the data","664a1b84":"You can see most of the V's are clustered around 0 with some or no outliers. Notice we have very few fraudulent cases over valid cases in our class histogram."}}