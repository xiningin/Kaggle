{"cell_type":{"bac5e8c5":"code","2acfce65":"code","fdc71f36":"code","e0fe6c54":"code","e870cdd5":"code","eca85279":"code","b34e6a8a":"code","ae3e0ee8":"code","436121c5":"code","268673f9":"code","f4bab3ae":"code","ab88f789":"code","38255bce":"code","cb4e7976":"code","735cfff9":"code","172bb217":"code","4160ed1d":"code","57154d6c":"code","09803ffe":"code","f65ed1d5":"code","8c2b5b5a":"code","c87ca0a7":"code","b4c84d60":"code","87f9b0dc":"code","dbdd5a85":"code","09307699":"code","bfb2a640":"code","3bceaa87":"code","b753a12e":"code","e913d88c":"code","b2a26fe5":"code","7831c670":"code","1a5be1a4":"code","09b131b4":"code","947d738d":"code","df4e2a58":"code","49366dc3":"code","63a2a831":"code","3e261ad3":"code","e6d9a39a":"code","93e9cdcf":"code","232c5d85":"code","324048b9":"code","2fed2107":"code","824bc4a7":"code","8254a886":"code","c6a80984":"markdown","8a14ecfb":"markdown","0de7b312":"markdown","df419bea":"markdown","b380b20e":"markdown","5fb72a5c":"markdown","9dcea53d":"markdown","8c24822b":"markdown","4b05aed6":"markdown","a3190269":"markdown","e61bb002":"markdown","bb2f716c":"markdown","93ea3cb8":"markdown","2ea091db":"markdown","35ab3752":"markdown","a583cabe":"markdown","44b9e381":"markdown","ba543fe5":"markdown","3219b8a4":"markdown","ea7e1e98":"markdown","5c3f95d6":"markdown","a066bf3f":"markdown","43a34f5c":"markdown","5b8ed324":"markdown","9da4564b":"markdown","314ae189":"markdown","35589d35":"markdown","1e40a060":"markdown","2b57a604":"markdown","66c907ab":"markdown","556929d3":"markdown","7e144f08":"markdown","9246c940":"markdown","fd43c160":"markdown","b0f75dd9":"markdown","07f1cc54":"markdown","b035ffa9":"markdown"},"source":{"bac5e8c5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split,KFold,GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score,confusion_matrix,precision_recall_curve,auc,roc_auc_score,\\\nroc_curve,recall_score,classification_report ,f1_score,precision_score\nfrom sklearn.svm import SVC\n\nplt.style.use('fivethirtyeight')\nimport itertools\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2acfce65":"df=pd.read_csv(\"..\/input\/data.csv\")","fdc71f36":"df.shape","e0fe6c54":"df.head()","e870cdd5":"df.columns","eca85279":"missing_cols=(df.isnull().sum()\/df.shape[0])*100\nmissing_cols= missing_cols[missing_cols>0]\nmissing_cols.plot(kind='bar')\nplt.xlabel('Features')\nplt.ylabel('Missing Percentage %')\nplt.title('Missing Data')\nplt.show()","b34e6a8a":"df.shape","ae3e0ee8":"df.id.unique().shape[0]","436121c5":"df.columns","268673f9":"cols_to_drop=['id','Unnamed: 32']\ndf.drop(cols_to_drop,axis=1,inplace=True)\ndf.shape","f4bab3ae":"f,ax=plt.subplots(1,2,figsize=(12,3))\ndf['diagnosis'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('diagnosis')\nax[0].set_ylabel('')\nsns.countplot('diagnosis',data=df,ax=ax[1])\nax[1].set_title('classLabel')\nplt.show()","ab88f789":"df.dtypes","38255bce":"df['diagnosis'] = df['diagnosis'].map({'M':1,'B':0})\ndf.head()","cb4e7976":"df.describe()","735cfff9":"columns = df.iloc[:,1:11].columns\nlength  = len(columns)\ncolors  = [\"r\",\"g\",\"b\",\"m\",\"y\",\"c\",\"k\",\"orange\",'pink','brown'] \n\nplt.figure(figsize=(15,20))\nfor i,j,k in itertools.zip_longest(columns,range(length),colors):\n    #plt.subplot(length\/3,length\/2,j+1)\n    plt.subplot(5,3,j+1)\n    #print(length\/2,length\/3,j+1)\n    sns.distplot(df.iloc[:,1:11][i],color=k)\n    #sns.boxplot(df_num[i+1],color=k)\n    plt.title(i)\n    plt.subplots_adjust(hspace = .3)\n    plt.axvline(df.iloc[:,1:11][i].mean(),color = \"k\",linestyle=\"dashed\",label=\"MEAN\")\n    plt.axvline(df.iloc[:,1:11][i].std(),color = \"b\",linestyle=\"dotted\",label=\"STANDARD DEVIATION\")\n    plt.legend(loc=\"upper right\")\n    ","172bb217":"columns = df.iloc[:,1:11].columns\nlength  = len(columns)\ncolors  = [\"r\",\"g\",\"b\",\"m\",\"y\",\"c\",\"k\",\"orange\",'pink','brown'] \n\nplt.figure(figsize=(15,20))\nfor i,j,k in itertools.zip_longest(columns,range(length),colors):\n    plt.subplot(length\/2,length\/3,j+1)\n    sns.boxplot(df.iloc[:,1:11][i],color=k)\n    plt.title(i)\n    plt.subplots_adjust(hspace = .4)\n    #plt.axvline(df_num[i].mean(),color = \"k\",linestyle=\"dashed\",label=\"MEAN\")\n    #plt.axvline(df_num[i].std(),color = \"b\",linestyle=\"dotted\",label=\"STANDARD DEVIATION\")\n    plt.legend(loc=\"upper right\")\n    ","4160ed1d":"columns = df.iloc[:,11:21].columns\nlength  = len(columns)\ncolors  = [\"r\",\"g\",\"b\",\"m\",\"y\",\"c\",\"k\",\"orange\",'pink','brown'] \n\nplt.figure(figsize=(15,20))\nfor i,j,k in itertools.zip_longest(columns,range(length),colors):\n    #plt.subplot(length\/3,length\/2,j+1)\n    plt.subplot(5,3,j+1)\n    #print(length\/2,length\/3,j+1)\n    sns.distplot(df.iloc[:,11:21][i],color=k)\n    #sns.boxplot(df_num[i+1],color=k)\n    plt.title(i)\n    plt.subplots_adjust(hspace = .3)\n    plt.axvline(df.iloc[:,11:21][i].mean(),color = \"k\",linestyle=\"dashed\",label=\"MEAN\")\n    plt.axvline(df.iloc[:,11:21][i].std(),color = \"b\",linestyle=\"dotted\",label=\"STANDARD DEVIATION\")\n    plt.legend(loc=\"upper right\")\n    ","57154d6c":"columns = df.iloc[:,11:21].columns\nlength  = len(columns)\ncolors  = [\"r\",\"g\",\"b\",\"m\",\"y\",\"c\",\"k\",\"orange\",'pink','brown'] \n\nplt.figure(figsize=(15,20))\nfor i,j,k in itertools.zip_longest(columns,range(length),colors):\n    plt.subplot(length\/2,length\/3,j+1)\n    sns.boxplot(df.iloc[:,11:21][i],color=k)\n    plt.title(i)\n    plt.subplots_adjust(hspace = .4)\n    #plt.axvline(df_num[i].mean(),color = \"k\",linestyle=\"dashed\",label=\"MEAN\")\n    #plt.axvline(df_num[i].std(),color = \"b\",linestyle=\"dotted\",label=\"STANDARD DEVIATION\")\n    plt.legend(loc=\"upper right\")\n    ","09803ffe":"columns = df.iloc[:,21:].columns\nlength  = len(columns)\ncolors  = [\"r\",\"g\",\"b\",\"m\",\"y\",\"c\",\"k\",\"orange\",'pink','brown'] \n\nplt.figure(figsize=(15,20))\nfor i,j,k in itertools.zip_longest(columns,range(length),colors):\n    #plt.subplot(length\/3,length\/2,j+1)\n    plt.subplot(5,3,j+1)\n    #print(length\/2,length\/3,j+1)\n    sns.distplot(df.iloc[:,21:][i],color=k)\n    #sns.boxplot(df_num[i+1],color=k)\n    plt.title(i)\n    plt.subplots_adjust(hspace = .3)\n    plt.axvline(df.iloc[:,21:][i].mean(),color = \"k\",linestyle=\"dashed\",label=\"MEAN\")\n    plt.axvline(df.iloc[:,21:][i].std(),color = \"b\",linestyle=\"dotted\",label=\"STANDARD DEVIATION\")\n    plt.legend(loc=\"upper right\")\n    ","f65ed1d5":"columns = df.iloc[:,21:].columns\nlength  = len(columns)\ncolors  = [\"r\",\"g\",\"b\",\"m\",\"y\",\"c\",\"k\",\"orange\",'pink','brown'] \n\nplt.figure(figsize=(15,20))\nfor i,j,k in itertools.zip_longest(columns,range(length),colors):\n    plt.subplot(length\/2,length\/3,j+1)\n    sns.boxplot(df.iloc[:,21:][i],color=k)\n    plt.title(i)\n    plt.subplots_adjust(hspace = .4)\n    #plt.axvline(df_num[i].mean(),color = \"k\",linestyle=\"dashed\",label=\"MEAN\")\n    #plt.axvline(df_num[i].std(),color = \"b\",linestyle=\"dotted\",label=\"STANDARD DEVIATION\")\n    plt.legend(loc=\"upper right\")\n    ","8c2b5b5a":"columns = df.iloc[:,1:11].columns\nlength  = len(columns)\n#colors  = [\"r\",\"g\",\"b\",\"m\",\"y\",\"c\",\"k\",\"orange\"] \n\nplt.figure(figsize=(15,20))\nfor i,j in itertools.zip_longest(columns,range(length)):\n    plt.subplot(length\/2,length\/3,j+1)\n    plt.hist(x = [df[df['diagnosis']==1][i], df[df['diagnosis']==0][i]], \n         stacked=True, color = ['brown','green'],label = ['M','B'])\n    plt.title(i)\n    plt.subplots_adjust(hspace = .3)\n    #plt.axvline(df_num[i].mean(),color = \"k\",linestyle=\"dashed\",label=\"MEAN\")\n    #plt.axvline(df_num[i].std(),color = \"b\",linestyle=\"dotted\",label=\"STANDARD DEVIATION\")\n    plt.legend(loc=\"upper right\")\n    ","c87ca0a7":"columns = df.iloc[:,11:21].columns\nlength  = len(columns)\n#colors  = [\"r\",\"g\",\"b\",\"m\",\"y\",\"c\",\"k\",\"orange\"] \n\nplt.figure(figsize=(15,20))\nfor i,j in itertools.zip_longest(columns,range(length)):\n    plt.subplot(length\/2,length\/3,j+1)\n    plt.hist(x = [df[df['diagnosis']==1][i], df[df['diagnosis']==0][i]], \n         stacked=True, color = ['brown','green'],label = ['M','B'])\n    plt.title(i)\n    plt.subplots_adjust(hspace = .3)\n    #plt.axvline(df_num[i].mean(),color = \"k\",linestyle=\"dashed\",label=\"MEAN\")\n    #plt.axvline(df_num[i].std(),color = \"b\",linestyle=\"dotted\",label=\"STANDARD DEVIATION\")\n    plt.legend(loc=\"upper right\")\n    ","b4c84d60":"columns = df.iloc[:,21:].columns\nlength  = len(columns)\n#colors  = [\"r\",\"g\",\"b\",\"m\",\"y\",\"c\",\"k\",\"orange\"] \n\nplt.figure(figsize=(15,20))\nfor i,j in itertools.zip_longest(columns,range(length)):\n    plt.subplot(length\/2,length\/3,j+1)\n    plt.hist(x = [df[df['diagnosis']==1][i], df[df['diagnosis']==0][i]], \n         stacked=True, color = ['brown','green'],label = ['M','B'])\n    plt.title(i)\n    plt.subplots_adjust(hspace = .3)\n    #plt.axvline(df_num[i].mean(),color = \"k\",linestyle=\"dashed\",label=\"MEAN\")\n    #plt.axvline(df_num[i].std(),color = \"b\",linestyle=\"dotted\",label=\"STANDARD DEVIATION\")\n    plt.legend(loc=\"upper right\")\n    ","87f9b0dc":"df.columns","dbdd5a85":"cols_to_use=['diagnosis', 'radius_mean',  'perimeter_mean',\\\n       'area_mean', 'compactness_mean', \\\n       'concave points_mean','concave points_se','radius_worst','perimeter_worst', 'area_worst','compactness_worst','concave points_worst']\ndf1=df[cols_to_use]\ndf1.shape","09307699":"df1.head()","bfb2a640":"correlation = df1.corr()\nplt.figure(figsize=(9,7))\nsns.heatmap(correlation,annot=True,edgecolor=\"k\",cmap=sns.color_palette(\"magma\"))\nplt.title(\"CORRELATION BETWEEN VARIABLES\")\nplt.show()","3bceaa87":"#plt.scatter(x=df1['radius_mean'],y=df['radius_worst'],)\nsns.scatterplot(x=df1['radius_mean'], y=df1['radius_worst'], hue=df.diagnosis)\nplt.title('Scatter radius_mean vs radius_wrost')\nplt.show()","b753a12e":"#plt.scatter(x=df1['radius_mean'],y=df['radius_worst'],)\nsns.scatterplot(x=df1['radius_mean'], y=df1['perimeter_worst'], hue=df.diagnosis)\nplt.title('Scatter radius_mean vs perimeter_worst')\nplt.show()","e913d88c":"#plt.scatter(x=df1['radius_mean'],y=df['radius_worst'],)\nsns.scatterplot(x=df1['radius_mean'], y=df1['area_worst'], hue=df.diagnosis)\nplt.title('Scatter radius_mean vs area_worst')\nplt.show()","b2a26fe5":"#plt.scatter(x=df1['radius_mean'],y=df['radius_worst'],)\nsns.scatterplot(x=df1['concave points_mean'], y=df1['concave points_worst'], hue=df.diagnosis)\nplt.title('Scatter concave points_mean vs concave points_worst')\nplt.show()","7831c670":"#plt.scatter(x=df1['radius_mean'],y=df['radius_worst'],)\nsns.scatterplot(x=df1['radius_mean'], y=df1['perimeter_mean'], hue=df.diagnosis)\nplt.title('Scatter radius_mean vs perimeter_mean')\nplt.show()","1a5be1a4":"#plt.scatter(x=df1['radius_mean'],y=df['radius_worst'],)\nsns.scatterplot(x=df1['radius_mean'], y=df1['area_mean'], hue=df.diagnosis)\nplt.title('Scatter radius_mean vs area_mean')\nplt.show()","09b131b4":"cols_to_use=['diagnosis', 'radius_mean',  \\\n       'compactness_mean', \\\n       'concave points_se']\ndf2=df[cols_to_use]\ndf2.shape","947d738d":"correlation = df2.corr()\nplt.figure(figsize=(9,7))\nsns.heatmap(correlation,annot=True,edgecolor=\"k\",cmap=sns.color_palette(\"magma\"))\nplt.title(\"CORRELATION BETWEEN VARIABLES\")\nplt.show()","df4e2a58":"y=df2.diagnosis\nX=df[['radius_mean','compactness_mean','concave points_se']]\nX.shape,y.shape","49366dc3":"# Normalization\nscaler = StandardScaler()\nX = scaler.fit_transform(X)","63a2a831":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)","3e261ad3":"model = LogisticRegression()\nmodel.fit(X_train,y_train)\nprediction1=model.predict(X_test)\nprint(\"__\"*50,\"\\n\")\nprint('The accuracy of the Logistic Regression is',accuracy_score(y_test,prediction1))\nprint(\"__\"*50,\"\\n\")\nprint(classification_report(y_test,prediction1))\nprint(\"__\"*50)\nsns.heatmap(confusion_matrix(y_test,prediction1),annot=True,fmt=\"d\")\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()","e6d9a39a":"model = SVC(kernel='linear')\nmodel.fit(X_train,y_train)\nprediction1=model.predict(X_test)\nprint(\"__\"*50,\"\\n\")\nprint('The accuracy of the Linear SVM is',accuracy_score(y_test,prediction1))\nprint(\"__\"*50,\"\\n\")\nprint(classification_report(y_test,prediction1))\nprint(\"__\"*50)\nsns.heatmap(confusion_matrix(y_test,prediction1),annot=True,fmt=\"d\")\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()","93e9cdcf":"model = SVC(kernel='poly',degree=3)\nmodel.fit(X_train,y_train)\nprediction1=model.predict(X_test)\nprint(\"__\"*50,\"\\n\")\nprint('The accuracy of the Polynomial SVM is',accuracy_score(y_test,prediction1))\nprint(\"__\"*50,\"\\n\")\nprint(classification_report(y_test,prediction1))\nprint(\"__\"*50)\nsns.heatmap(confusion_matrix(y_test,prediction1),annot=True,fmt=\"d\")\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()","232c5d85":"model = SVC(kernel='poly',degree=4)\nmodel.fit(X_train,y_train)\nprediction1=model.predict(X_test)\nprint(\"__\"*50,\"\\n\")\nprint('The accuracy of the Polynomial SVM is',accuracy_score(y_test,prediction1))\nprint(\"__\"*50,\"\\n\")\nprint(classification_report(y_test,prediction1))\nprint(\"__\"*50)\nsns.heatmap(confusion_matrix(y_test,prediction1),annot=True,fmt=\"d\")\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()","324048b9":"model = SVC(kernel='rbf')\nmodel.fit(X_train,y_train)\nprediction1=model.predict(X_test)\nprint(\"__\"*50,\"\\n\")\nprint('The accuracy of the SVM is',accuracy_score(y_test,prediction1))\nprint(\"__\"*50,\"\\n\")\nprint(classification_report(y_test,prediction1))\nprint(\"__\"*50)\nsns.heatmap(confusion_matrix(y_test,prediction1),annot=True,fmt=\"d\")\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()","2fed2107":"%%time\nparams_dict=\\\n{'C':[0.001,0.01,0.1,1,10,100],\n 'gamma':[0.001,0.01,0.1,1,10,100],\n 'kernel':['linear','rbf']}\nmodel1=GridSearchCV(estimator=SVC(),param_grid=params_dict,scoring='accuracy',cv=10)\nmodel1.fit(X_train,y_train)","824bc4a7":"#Best parameters for our svc model\nmodel1.best_params_","8254a886":"#Let's run our SVC again with the best parameters.\nmodel_final = SVC(C = 0.01, gamma =  0.001, kernel= 'linear')\nmodel_final.fit(X_train,y_train)\nprediction1=model_final.predict(X_test)\nprint(\"__\"*50,\"\\n\")\nprint('The accuracy of the SVM  is',accuracy_score(y_test,prediction1))\nprint(\"__\"*50,\"\\n\")\nprint(classification_report(y_test,prediction1))\nprint(\"__\"*50)\nsns.heatmap(confusion_matrix(y_test,prediction1),annot=True,fmt=\"d\")\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()","c6a80984":"- All features are in different scale , we need to make them comparable","8a14ecfb":"## Logistic Regresion","0de7b312":"## Support Vector Machine","df419bea":"* **Polynomial Kernel of degree 3**","b380b20e":"#### Mean Values distribution w.r.t Target Varaible","5fb72a5c":"Observations\n- wrost values of radius, perimeter, area, compactness and concave points can be used in classification of the cancer. Larger values of these parameters tends to show a correlation with malignant tumors.\n- Other does not show a particular preference of one diagnosis over the other. Overlapping Distributions.","9dcea53d":"Inference :\n    - All features are normally distributed.\n    - No significant outliers.","8c24822b":"**Splitting Data into training & Testing**","4b05aed6":"### Boxplot Wrost Features","a3190269":"#### Hyperparameter Tuning","e61bb002":"Observation:\n    - balanced dataset\n    - Class distribution: 357 benign, 212 malignant","bb2f716c":"### Univariate Analysis\n- Histograms\n- Boxplots\n\nhighlight missing and outlier values","93ea3cb8":"**RBF Kernel**","2ea091db":"Observations\n- mean values of radius, perimeter, area, compactness and concave points can be used in classification of the cancer. Larger values of these parameters tends to show a correlation with malignant tumors.\n- Other does not show a particular preference of one diagnosis over the other. Overlapping Distributions.","35ab3752":"#### Boxplots Mean Features","a583cabe":"#### Boxplot SE Features","44b9e381":"#### Starting with mean  features","ba543fe5":"#### Target Feature\n","3219b8a4":"**Observation Summary**\n- Mean features to be used radius, perimeter, area, compactness and concave points ( 5 features)\n- SE features to be used concave points ( 1 features)\n- Wrost features to be used radius, perimeter, area, compactness and concave points. (5 features)\n- 11 features ","ea7e1e98":"#### Largest Value features","5c3f95d6":"## Modelling","a066bf3f":"Some random column \" Unamed : 32 \" got added , dropping as everything is NaN","43a34f5c":"#### Wrost type features w.r.t target variable \n","5b8ed324":"**Dropping  correlated features**\n- Dropping radius_worst,perimeter_worst,area_worst,compactness_worst,concave points_worst","9da4564b":"**Improved accuracy from 88% to 89% by tuning hyperparameters**","314ae189":"* **Polynomial Kernel of degree 4**","35589d35":"#### Missing Data","1e40a060":"**Model Accuracy**\n1. Logistic Regression : 88 %\n2. Linear SVM : 88 %\n3. Polynomial Kernel of degree 3 : 87 %\n4. Polynomial Kernel of degree 4 : 74 %\n4. RBF Kernel : 88 %","2b57a604":"Obsertvations :\n    \n- se values of Concave points can also be used in classification of the cancer.Smaller values of these parameters tends to show a correlation with benign tumors.\n- Other does not show a particular preference of one diagnosis over the other. Overlapping Distributions.","66c907ab":"#### LabelEncoding target variable","556929d3":"#### Variable Identification\n- Variable DataType : Numerical or Categorical","7e144f08":"**Linear Kernel**","9246c940":"Observation \n- High correlation between mean features & wrost features\n- High correlation within mean features","fd43c160":"Observation :\n\n- Id Column has all unique values , no pattern\n- Unamed : 32 column has all NaN \n- Dropping these columns","b0f75dd9":"## Data Exploration & Prepration\n","07f1cc54":"#### Looking at distribution of SE variables","b035ffa9":"#### SE features w.r.t target variable"}}