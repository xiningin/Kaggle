{"cell_type":{"56f43b96":"code","5cfcfb75":"code","eb69f1db":"code","93741863":"code","5ce083a6":"code","287bd2d0":"code","642fe6e2":"code","ac8cad7a":"code","68e03123":"code","0c8498a1":"code","7b1c9f9e":"code","06deea9f":"code","1ecfb206":"code","2268941f":"code","e293f98d":"code","168630c5":"code","c1461bb2":"code","121cb3fc":"code","99e875e5":"code","fc009a76":"code","e50af3ef":"code","f2b900ff":"code","d0883736":"code","db2648c9":"code","0b15bb85":"code","0d7b67b1":"code","7343b876":"code","cb0c0899":"code","3d5ea1bf":"code","3f9c70d1":"code","12aced8b":"code","addf4e04":"code","37f853ca":"code","5f799541":"code","e968eb0c":"code","20471282":"code","26a344c9":"code","78a8c94a":"code","5fc409f7":"code","e1e078e6":"code","827a9907":"code","6604203e":"code","967e4deb":"code","1411b44d":"code","13357b24":"code","505e9f9d":"code","d014cbe7":"code","3a4b5a0d":"code","00d3e942":"code","982054d4":"code","faf97107":"code","854f61a0":"code","903de574":"code","841372b0":"code","ac2dd59a":"code","c95e1613":"code","28ae6782":"code","46fd4e33":"code","2b887d12":"code","30466893":"code","4a968bc2":"code","b271f23d":"code","81632c55":"code","4d9464e3":"code","05fa3617":"code","8e2c8ed2":"code","298d01fc":"code","52a623be":"code","a63c36ba":"code","28ae0023":"code","854813e2":"code","f65f72a2":"code","103c199e":"code","9c7de1f3":"code","df09f42f":"code","9fdebcc3":"code","0fe49ef1":"code","1cb2029d":"code","c0ba6c5b":"code","eade9ae8":"code","78e20d87":"code","1b568456":"markdown","90efd52d":"markdown","805f5375":"markdown","95c0e2ad":"markdown","267ffeb3":"markdown","7737597e":"markdown","c27a2e0c":"markdown","631bafca":"markdown","b088b41f":"markdown","761b416a":"markdown","37000f99":"markdown","af3cfc47":"markdown","a553e8a9":"markdown","3784452e":"markdown","276ce9b6":"markdown","9feec435":"markdown","b6b29a5d":"markdown","9a801d98":"markdown","938cab6e":"markdown","ae84d98b":"markdown","0820f5de":"markdown","b48dc4e3":"markdown","78f9a755":"markdown","f677509c":"markdown","d2538689":"markdown","802d1e8c":"markdown","0367be47":"markdown","f6b3f51d":"markdown","2c5b65aa":"markdown"},"source":{"56f43b96":"import matplotlib as mpl\n# General plot parameters\nmpl.rcParams['font.family'] = 'Avenir'\nmpl.rcParams['font.size'] = 28\nmpl.rcParams['axes.linewidth'] = 4\nmpl.rcParams['axes.spines.top'] = False\nmpl.rcParams['axes.spines.right'] = False\nmpl.rcParams['xtick.major.size'] = 10\nmpl.rcParams['xtick.major.width'] = 2\nmpl.rcParams['ytick.major.size'] = 10\nmpl.rcParams['ytick.major.width'] = 4\nmpl.rcParams['lines.linewidth']= 3","5cfcfb75":"import numpy as np\n# El tiempo --\nt = np.linspace(0, 10, 11)\nprint(t)\n# La fuerza\nF=5\n# la masa\nm=6\n# La velocidad inicial\nv0=0.\n# Las velocidades:\nv = [vi for vi in (F\/m*t)]\nprint(v)","eb69f1db":"#To start, using the set style allows us to easily elevate the level of our visualization\nimport matplotlib.style as style\nstyle.available\n## Escogamos un tema \nstyle.use('fivethirtyeight')","93741863":"import matplotlib.pyplot as plt\n# --\nplt.plot(t, v, \n         color = 'blue', marker = \"o\", markersize=17)\nplt.title(\"Predicci\u00f3n de la velocidad --\")\nplt.xlabel(\"t\")\nplt.ylabel(\"v(t)\")\n# The signature bar\nplt.text(x = -1.7, y = -2,\n    s = ' \u00a9DronesPeiskos                                  Source: 2da Ley de Newton ',\n              fontsize = 14, color = '#f0f0f0', backgroundcolor = 'grey');","5ce083a6":"# Python program showing Graphical\n# representation of tanh() function\nimport numpy as np\nimport matplotlib.pyplot as plt\n  \nin_array = np.linspace(-np.pi, np.pi, 12)\nout_array = np.tanh(in_array)\n  \nprint(\"in_array : \", in_array)\nprint(\"\\nout_array : \", out_array)\n  \n# red for numpy.tanh()\nplt.plot(in_array, out_array, \n         color = 'blue', marker = \"o\", markersize=17)\nplt.title(\"A simple tanh(x) function --\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.show()","287bd2d0":"A=np.array([1,2,3]);\nB=np.array([4,5,6]);\nA+B","642fe6e2":"np.array([[1,2,3],[11,22,33]])","ac8cad7a":"A=np.array([[1,2,3],[11,22,33]]);\nB=np.array([[4,5,6],[44,55,66]]);\nA+B","68e03123":"A=np.array([[1,2,3],[11,22,33]]);\nB=np.array([[4,5,6],[44,55,66],[11,22,33]]);\n## Producto punto\nprint('El producto punto: \\n', \n      np.dot(A,B))\n##","0c8498a1":"A=np.array([[1,2,3],[11,22,33]]);\nB=np.array([[4,5,6],[44,55,66]]);\n## Un producto diferente que se llama element-wise product.\nA*B","7b1c9f9e":"# Definici\u00f3n de Softmax --\ndef softmax(x):\n    \"\"\"Compute softmax values for each sets of y.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x \/ e_x.sum(axis=0) #","06deea9f":"softmax([0.4, 1.2])","1ecfb206":"## -- Una funci\u00f3n: Explicar!\ndef rnn_cell(xt, a_prev, parameters):\n    \n    # Retira los parametros de \"parameters\"\n    Wax = parameters[\"Wax\"]\n    Waa = parameters[\"Waa\"]\n    Wya = parameters[\"Wya\"]\n    ba = parameters[\"ba\"]\n    by = parameters[\"by\"]\n    \n    # Como dice arriba --\n    a_next1 = np.tanh(np.dot(Waa,a_prev)+np.dot(Wax,xt)+ba)\n    # Esto es lo predicho --\n    yt_pred1 = softmax(np.dot(Wya,a_next1)+by)\n    \n    # Almacena valores para usar luego --\n    cache1 = (a_next1, a_prev, xt, parameters)\n    \n    return a_next1, yt_pred1, cache1","2268941f":"np.random.seed(1)\nxt = np.random.randn(3,10)\na_prev = np.random.randn(5,10)\n## -- Creaci\u00f3n de un diccionario: Explicar!\nparameters = {}\n# -----------------------------------------\nparameters['Waa'] = np.random.randn(5,5)\nparameters['Wax'] = np.random.randn(5,3)\nparameters['Wya'] = np.random.randn(2,5)\nparameters['ba'] = np.random.randn(5,1)\nparameters['by'] = np.random.randn(2,1)","e293f98d":"parameters['Waa']","168630c5":"parameters","c1461bb2":"a_next, yt_pred, cache = rnn_cell(xt, a_prev, parameters)\nprint(\"a_next[4] = \\n\", a_next[4])\n#print(\"a_next.shape = \\n\", a_next.shape)\n#print(\"yt_pred[1] =\\n\", yt_pred[1])\n#print(\"yt_pred.shape = \\n\", yt_pred.shape)","121cb3fc":"!pip install Keras","99e875e5":"import tensorflow as tf","fc009a76":"print(tf.__version__)","e50af3ef":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.optimizers import SGD","f2b900ff":"opt = SGD","d0883736":"import numpy as np","db2648c9":"n=3\ny_0_Test = np.random.randint(2, size=(n, 1))\ny_0_Test","0b15bb85":"switch_Test = np.random.random(size=(n, 1)) <= 0.9\nswitch_Test","0d7b67b1":"y_1_Test = ~y_0_Test  & switch_Test\ny_1_Test","7343b876":"y_2_Test = ~y_0_Test  & ~switch_Test\ny_2_Test","cb0c0899":"np.concatenate([y_0_Test, y_1_Test, y_2_Test], axis=1)","3d5ea1bf":"np.random.normal(size=n)","3f9c70d1":"y_0_Test + np.random.normal(size=n) # \/ 5  #[np.newaxis].T","12aced8b":"## -- Una funci\u00f3n: Explicar un poco.\ndef sample_Classes(nClasses, n, ratio=0.8):\n    np.random.seed(42)\n    y_0 = np.random.randint(2, size=(n, 1))\n    switch = (np.random.random(size=(n, 1)) <= ratio)\n    ## Posiblemente explicar estos operadores logicos!\n    ## & = AND    -     ~ = NOT \n    y_1 = ~y_0 & switch\n    y_2 = ~y_0 & ~switch\n    y = np.concatenate([y_0, y_1, y_2], axis=1)\n    \n    X = y_0 + (np.random.normal(size=n) \/ 5)[np.newaxis].T\n    return (X, y)\n\n\n## Generar la data sintetica\nnClasses = 3\nX_train, y_train = sample_Classes(nClasses, 1000)\nX_test, y_test = sample_Classes(nClasses, 100)","addf4e04":"X_train[:,0].shape, y_train[:,0].shape","37f853ca":"X_train[:10]","5f799541":"import matplotlib.pyplot as plt\nplt.scatter(X_train[:, 0], y_train[:, 0], marker='x', c=y_train)","e968eb0c":"clf = Sequential()\nclf.add(Dense(3, activation='linear', input_shape=(1,), name='hidden'))\nclf.add(Dense(nClasses, activation='softmax', name='out'))\nclf.compile(loss='categorical_crossentropy', optimizer=opt(), metrics=['accuracy'])","20471282":"dot_img_file = '.\/model_1.png'\ntf.keras.utils.plot_model(clf, to_file=dot_img_file, show_shapes=True)","26a344c9":"clf.fit(X_train, y_train, epochs=10, batch_size=16)","78a8c94a":"## Punto a predecir \nnpre=767\nprint('Esta es la predicci\u00f3n: {}. \\\nY este el valor real{}'.format(clf.predict(X_train[npre]), y_train[npre]))","5fc409f7":"def logistic_func(x): \n    return np.e**x\/(np.e**x + 1)","e1e078e6":"import matplotlib.pyplot as plt\n%matplotlib inline\ninpX = np.arange(-7, 7, 0.6)\nplt.plot(inpX, \n         [logistic_func(x) for x in inpX]\n        , color = 'orange', marker = \"o\", markersize=17)","827a9907":"## Generar la data sintetica\nnClases = 3\nX_train, y_train = sample_Classes(nClases, 1000)\nX_test, y_test = sample_Classes(nClases, 100)","6604203e":"clf = Sequential()\nclf.add(Dense(3, activation='sigmoid', input_shape=(1,), name='hidden'))\nclf.add(Dense(3, activation='softmax', name='out'))\nclf.compile(loss='categorical_crossentropy', optimizer=opt(), metrics=['accuracy'])\n\nclf.fit(X_train, y_train, epochs=20, batch_size=16, verbose=1)","967e4deb":"fig = plt.figure(figsize=(9, 6))\nplt.plot(range(len(clf.history.history['accuracy'])), \n         clf.history.history['accuracy'], \n         linewidth=4, marker = \"o\", markersize=17)\n#import seaborn as sns; sns.despine()\nplt.title(\"Sigmoid Activation Accuracy Per Epoch\", fontsize=20)\npass","1411b44d":"y_train.shape","13357b24":"clf = Sequential()\nclf.add(Dense(3, activation='tanh', input_shape=(1,), name='hidden'))\nclf.add(Dense(nClasses, activation='softmax', name='out'))\nclf.compile(loss='categorical_crossentropy', \n            optimizer=opt(), \n            metrics=['accuracy'])\nclf.summary()","505e9f9d":"import matplotlib.pyplot as plt\n%matplotlib inline\ninpX = np.arange(-7, 7, 0.8)\nplt.plot(inpX, \n         [np.tanh(x) for x in inpX]\n        , color = 'orange', marker = \"o\", markersize=17)","d014cbe7":"clf.fit(X_train, y_train, epochs=10, batch_size=4, verbose=1)","3a4b5a0d":"fig = plt.figure(figsize=(9, 6))\nplt.plot(range(len(clf.history.history['accuracy'])),\n         clf.history.history['accuracy'], \n         linewidth=4, marker = \"o\", markersize=17)\nimport seaborn as sns; sns.despine()\nplt.title(\"Tanh Activation Accuracy Per Epoch\", fontsize=20)\npass","00d3e942":"#!pip install sci-kit-learn","982054d4":"from sklearn.metrics import classification_report\n\n## Hablar un poco, de que el modelo lo que\n## predice no es la clase en si, si no las\n## probabilidades. Asi que convirtamos en clases.\n\ny_pred = clf.predict(X_test)\ny_pred = (y_pred > 0.5).astype(int)\n\nprint(classification_report(y_test, y_pred))","faf97107":"def relu(x):\n    return 0 if x <= 0 else x\n\nplt.plot(np.arange(-5, 5, 1), \n         [relu(x) for x in np.arange(-5, 5, 1)], \n         linewidth=4, marker = \"o\", markersize=17)\npass","854f61a0":"## Preguntar espontaneamente si corremos el modelo\n## con una funci\u00f3n de activaci\u00f3n RELU?","903de574":"## -- Revisar que tienes el archivo en este directorio de trabajo\n## https:\/\/arxiv.org\/help\/bulk_data\n!ls \/kaggle\/input\/arxiv","841372b0":"## -- Levantar y transformar la data\nimport json\nimport re\nimport string\nimport itertools\nimport numpy as np\nimport tqdm","ac2dd59a":"## Vamos a probar un modelo basado en palabras y no en carateres.\n## Al vocabulario sacado del corpus, le vamos a adicionar una lista\n## muy com\u00fan de caracteres adicionale.\nvocab_pre = [n for n in string.printable]","c95e1613":"## Una peque\u00f1a prueba\ndata_file = '\/kaggle\/input\/arxiv\/arxiv-metadata-oai-snapshot.json'\n\ngen_json = \\\n(json.loads(line) for line in \\\n itertools.islice(open(data_file,'r'),10))\n\n\ntemp = []\nfor line in gen_json:\n    temp.append(line['abstract'])","28ae6782":"temp[0]","46fd4e33":"## Tomar una muestra mas grande ...\nnum_examples = 150000\n\n# we will consider below 3 categories for training \npaper_categories = [\"cs.AI\", # Artificial Intelligence\n                    \"cs.CV\", # Computer Vision and Pattern Recognition\n                    \"cs.LG\"] # Machine Learning\ncategories=paper_categories\n\n## Reading the json per se --\ngen_json = \\\n(json.loads(line) for line in \\\n itertools.islice(open(data_file,'r'),\n                  num_examples) if json.loads(line).get('categories') in categories)\n\n\nabs_list = []\nfor line in gen_json:\n    abs = line['abstract'].lower()\n    abs = re.sub(r'(\\S)\\s+(\\S)',r'\\1 \\2',abs).replace('.\\n','.\\n\\n')\n    abs = abs.replace('\u00e2\\x80\\x99',\"'\")\n    abs = abs.replace('\\x7f',\"\")\n    abs = abs.replace('\u00e2\\x88\\x9e',\"'\")\n    abs = abs.replace('\u00e2\\x89\u00a4',\"'\")\n    abs = abs.replace('\u00e2\\x80\\x94',\"'\")\n    abs = abs.replace('\u00e2\\x80\\x93',\"-\")\n  ## Anything custom here! --\n    abs = abs.replace('\u00e2\\x80\\x9c',\"<\")\n    abs = abs.replace('\u00e2\\x80\\x9d',\">\")\n    abs = abs.replace('.',\" .\")\n    abs = abs.replace(',',\" ,\")\n    abs = abs.replace('\\n',\" \\n \")\n    abs = abs.replace(')',\" ) \")\n    abs = abs.replace('(',\" ( \")\n    abs = abs.replace(']',\" ] \")\n    abs = abs.replace('[',\" [ \")\n    abs = abs.replace('{',\" } \")\n  ## -- For the word- based model --\n    abs_list.append(abs)\n\nabs_list = np.array(abs_list)","2b887d12":"## Cantidad de abstracts que leiste!\nabs_list.shape","30466893":"abs_list[1].strip()","4a968bc2":"result1 = []\nfor p in abs_list:\n    result1.append(p.strip())","b271f23d":"result1[0].strip().split(' ')[:10]","81632c55":"words = [w.strip().split(' ') for w in abs_list]","4d9464e3":"## En formato requerido por el entrenaminto\nwords1=[]\nfor p in words:\n    words1+=p","05fa3617":"words1[40:47]","8e2c8ed2":"vocab = words1+vocab_pre\n\nvocab = set(vocab)\nvocab_size = len(vocab)\nchar2idx = {v:idx for idx,v in enumerate(vocab)}\nidx2char= np.array(vocab)\nint_to_char = dict((i, c) for i, c in enumerate(vocab))\n## imprime\nprint(\"El tama\u00f1o de este vocabulario (con palabras unicas) es: \", vocab_size)\nprint('\\n')","298d01fc":"## Hemos convertido palabras en numeros y viceversa\nlist(char2idx.items())[:4]","52a623be":"import os\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, GRU, Dense","a63c36ba":"text_as_int = np.array([char2idx[w] for w in words1], dtype=np.int32)\nprint ('{}\\n Mapped to integers:\\n {}'.format(repr(words1[:10]), text_as_int[:10]))","28ae0023":"seq_len = 20\nbatch_size = 48\ndataset = tf.data.Dataset.from_tensor_slices(text_as_int)\ndataset = dataset.batch(seq_len+1,drop_remainder=True)\n## x = 'hola como estas' ; y = 'como estas tu?' \ndataset = dataset.map(lambda x: (x[:-1],  x[1:]))\ndataset = dataset.shuffle(30).batch(batch_size,drop_remainder=True)","854813e2":"print([int_to_char[w] for w in text_as_int[:20-1]])\nprint(\"\\n\")\nprint([int_to_char[w] for w in text_as_int[1:20]])","f65f72a2":"## Finamente la funci\u00f3n que crea el modelo\ndef make_model(vocabulary_size,embedding_dimension,rnn_units,batch_size,stateful):\n    model = Sequential()\n    ## ---- Capas (2 LSTM seguidas)\n    model.add(Embedding(vocabulary_size,embedding_dimension,\n                      batch_input_shape=[batch_size,None]))\n    \n    ## -- Las dos capas van aqui!--------------------------------------\n    model.add(LSTM(rnn_units,return_sequences=True,stateful=stateful))\n    model.add(LSTM(rnn_units,return_sequences=True,stateful=stateful))\n    ## -----------------------------------------------------------------\n    \n    model.add(Dense(vocabulary_size))\n    ## ----\n    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                optimizer='adam',metrics=['accuracy'])\n    model.summary()\n    return model","103c199e":"emb_dim = 12\nrnn_units = 128\nmodel_1 = make_model(vocab_size,emb_dim,rnn_units,batch_size,False)","9c7de1f3":"#- localmente , y para testeos iniciales --\ncheckpoint_dir = '.\/checkpoints_Curso_v1'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\n##---\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)","df09f42f":"history1 = model_1.fit(dataset, \n                    epochs=5, batch_size=batch_size,\n                    callbacks=[checkpoint_callback])","9fdebcc3":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(history1.history['loss'], 'g',color = 'blue', marker = \"o\", markersize=17)\nplt.plot(history1.history['accuracy'], 'rx')\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train'], loc='upper right')\nplt.legend(['Train', 'Validation'], loc='upper right')\nplt.show()","0fe49ef1":"# -- si ha estado dificil -- guardar --\nfilename1='model_LSTM_Words1_param.hdf5'\nmodel_1.save('.\/'+filename1)","1cb2029d":"model_1 = make_model(vocab_size,emb_dim,rnn_units,1,True)\nmodel_1.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\nmodel_1.build(tf.TensorShape([1,None]))","c0ba6c5b":"def generate_text(model,seed,num_characters,remember1):\n    seed_text = tf.expand_dims([char2idx[k] for k in seed.split(' ')],0)\n    generated_text = []\n    ## -- If you want to remember, then commented. Otherwise UN-coment\n    if remember1: model.reset_states()\n    for n in range(num_characters+1):\n        result = tf.random.categorical(model(seed_text)[0,-1:],num_samples=1)\n        result = result[0,0].numpy()\n        generated_text.append(result)\n        seed_text = tf.expand_dims([result],0)\n    return ' '.join([int_to_char[k] for k in generated_text])","eade9ae8":"## -- If you want to remember = False\nremember1a=True\n## -- The number of words to generate per line --\nchar2Gen=10\n\n## Ex. 3 ----\nnchar=36\nnexte=6*char2Gen #\nseed = str(result1[0][:nchar]).strip()\nseedN = str(result1[0][nchar:nchar+nexte]).strip()\n\nprint('Seed --: '+seed+' <--> \\n')\n##---\nfor k in range(1):\n    temp = generate_text(model_1,seed,char2Gen,remember1a)\n    print('Pred.--: '+temp)\n\nprint('True --: '+seedN+' <--> \\n')","78e20d87":"## -- If you want to remember = False\nremember1a=False\n## -- The number of words to generate per line --\nchar2Gen=10\n\n## Ex. 1 ----\nnchar=36\nnexte=30\nseed = str(result1[0][:nchar]).strip()\nseedN = str(result1[0][nchar+1:nchar+1+nexte]).strip()\n\nprint('--: '+seed+' <--> \\n')\n##---\n#print('** This is the generated text! **:')\nfor k in range(7):\n    temp = generate_text(model_1,seed,char2Gen,remember1a)\n    print(temp)\nprint('-'*80)","1b568456":"# Generar una linea de texto nuevo!","90efd52d":"En este curso hemos visto los fundamentos para la generaci\u00f3n de texto.\nHasta la proxima aventura.","805f5375":"<div class=\"alert alert-block alert-info\" style=\"font-size:44px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp; \n\nElement-wise product -\n    \n$$ C = A \\circ B $$\n    \n<\/div>","95c0e2ad":"## Tanh(x)","267ffeb3":"# $$ y(x)^{SOFTMAX} = \\frac{ \\exp(x-x_{max})} { \\sum{\\exp(x)} }$$","7737597e":"# Generar varias lineas! ","c27a2e0c":"<div class=\"alert alert-block alert-info\" style=\"font-size:24px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp; \n\n$$ f_t = \\sigma_g(W_f x_t + U_f h_{(t-1)}+b_f) $$\n$$ i_t = \\sigma_g(W_i x_t + U_i h_{(t-1)}+b_i) $$\n$$ o_t = \\sigma_g(W_o x_t + U_o h_{(t-1)}+b_o) $$\n$$ \\tilde{c}_t = \\sigma_g(W_c x_t + U_c h_{(t-1)}+b_c) $$\n$$ c_{t} = f_{t}\\circ c_{t-1}+i_{t}\\circ {\\tilde {c}}_{t} $$\n$$ h_{t} = o_{t}\\circ \\sigma _{h}(c_{t}) $$\n    \n<\/div>","631bafca":"# Generando texto nuevo!","b088b41f":"# TensorFlow: una herramienta util para producir Machine Learning rapidamente.","761b416a":"# Creaci\u00f3n de una minuscula Red Neuronal \n# tres perceptrones, con funci\u00f3n de activaci\u00f3n Lineal (mx+b)","37000f99":"# Operaciones con vectores y matrices:","af3cfc47":"# Una celda RNN real con TensorFlow, y ver como actua una funci\u00f3n de activaci\u00f3n Relu(x):","a553e8a9":"## multiplicando dos vectores o matrices\n\n<div class=\"alert alert-block alert-info\" style=\"font-size:24px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp; \n\n <b>A<\/b> * <b>B<\/b> \n    \n <\/div>","3784452e":"# Una funci\u00f3n logistica (sigmoid):\n<div class=\"alert alert-block alert-info\" style=\"font-size:24px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp; \n\n$$ \\sigma(x) = \\frac{e^x}{e^x+1} $$\n\n<\/div>","276ce9b6":"## Generaci\u00f3n de datos de pruebas (siempre!)","9feec435":"# Comentarios finales","b6b29a5d":"# Analog\u00eda con la predicci\u00f3n de la velocidad de un objeto","9a801d98":"## Sumando dos vectores o matrices\n\n<div class=\"alert alert-block alert-info\" style=\"font-size:24px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp; \n\n <b>A<\/b> + <b>B<\/b> \n    \n <\/div>","938cab6e":"# \u00bfC\u00f3mo queda una RNN en Python?","ae84d98b":"# Entrenando el modelo","0820f5de":"# Ecuaciones fundamentales de la celda LSTM:","b48dc4e3":"<div class=\"alert alert-block alert-info\" style=\"font-size:24px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp; \n    \n$$ h_t = \\sigma_h(W_h x_t + U_h h_{(t-1)}+b_h) $$\n$$ y_t = SOFTMAX_y(W_y h_t + b_y) $$\n    \n<\/div>","78f9a755":"# Algunos parametros para graficar (no prestar mucha atenci\u00f3n)","f677509c":"...","d2538689":"# Text Generation: introductory course (v1.0):\n![Logo_Labs_v3.png](attachment:dcfec14c-6c93-491d-bbd3-133a10feffa0.png)","802d1e8c":"# Creando el modelo","0367be47":"<div class=\"alert alert-block alert-info\" style=\"font-size:24px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp; \n    \n### Matrices\n <b>A<\/b> + <b>B<\/b> \n    \n <\/div>","f6b3f51d":"# Una celda RNN real con TensorFlow, y ver como actua una funci\u00f3n de activaci\u00f3n Tanh(x):","2c5b65aa":"# Proyecto final de curso: un c\u00f3digo real para generar texto a partir de alg\u00fan \"corpus\" de texto que consigas"}}