{"cell_type":{"c83699bb":"code","7489d20b":"code","5a99515e":"code","689eb9c4":"code","90ad9d0c":"code","7aeebf8b":"code","81029ea8":"code","93ff2186":"code","ae81f9a0":"code","184bb527":"code","8ea035c8":"code","7075102d":"code","1082907c":"code","ea419085":"code","0bf9fbbe":"code","9f8e2b86":"markdown","a5001cb6":"markdown","3c97c327":"markdown","2454d158":"markdown","f3e30aad":"markdown","d450c500":"markdown","e453ea35":"markdown","5bc3b608":"markdown","499b6e7f":"markdown","bf8ac871":"markdown","c7efaf1a":"markdown","25d435f2":"markdown"},"source":{"c83699bb":"# Global variables for testing changes to this notebook quickly\nRANDOM_SEED = 0\nNUM_FOLDS = 3\nMAX_TREES = 20000\nEARLY_STOP = 150\nNUM_TRIALS = 50","7489d20b":"# General imports\nimport numpy as np\nimport pandas as pd\nimport datatable as dt\nimport time\nimport gc\n\n# Model and evaluation\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\n\n# Optuna\nimport optuna\nfrom optuna.visualization import plot_param_importances, plot_parallel_coordinate\nfrom optuna.pruners import PercentilePruner\n\n# Hide warnings (makes optuna output easier to parse)\nimport warnings\nwarnings.filterwarnings('ignore')","5a99515e":"# Helper function for downcasting \ndef reduce_memory_usage(df, verbose=True):\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col, dtype in df.dtypes.iteritems():\n        if dtype.name.startswith('int'):\n            df[col] = pd.to_numeric(df[col], downcast ='integer')\n        elif dtype.name == 'bool':\n            df[col] = df[col].astype('int8')\n        elif dtype.name.startswith('float'):\n            df[col] = pd.to_numeric(df[col], downcast ='float')\n        \n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df","689eb9c4":"%%time\n\n# Load training data\ntrain = dt.fread(r'..\/input\/tabular-playground-series-oct-2021\/train.csv').to_pandas()\ntrain = reduce_memory_usage(train)\n\n# Holdout set for testing our models\ntrain, holdout = train_test_split(\n    train,\n    test_size = 0.5,\n    shuffle = True,\n    stratify = train['target'],\n    random_state = RANDOM_SEED,\n)\n\ntrain.reset_index(drop = True, inplace = True)\nholdout.reset_index(drop = True, inplace = True)\n\n# Get features\nfeatures = [x for x in train.columns if x not in ['id','target']]","90ad9d0c":"# Default XGBoost params, used for ALL models considered\ndefault_params = dict(            \n    random_state = RANDOM_SEED,\n    n_estimators = MAX_TREES,\n    tree_method = 'gpu_hist',\n    predictor = \"gpu_predictor\",\n)","7aeebf8b":"def score_xgboost(trial = None, model_params = {}, fit_params = {}):\n    \n    # Store the holdout predictions\n    holdout_preds = np.zeros((holdout.shape[0],))\n    \n    # Stratified k-fold cross-validation\n    skf = StratifiedKFold(n_splits = NUM_FOLDS, shuffle = True, random_state = RANDOM_SEED)\n    for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train['target'])):\n        \n        # Training and Validation Sets\n        start = time.time()\n        X_train, y_train = train[features].iloc[train_idx], train['target'].iloc[train_idx]\n        X_valid, y_valid = train[features].iloc[valid_idx], train['target'].iloc[valid_idx]\n        \n        # Define Model\n        model = XGBClassifier(**default_params, **model_params)\n        gc.collect()\n        \n        model.fit(\n            X_train, y_train,\n            verbose = False,\n            eval_set = [(X_valid, y_valid)],\n            eval_metric = \"logloss\",\n            early_stopping_rounds = EARLY_STOP,\n            **fit_params\n        )\n        \n        # validation\/holdout predictions\n        valid_preds = model.predict_proba(X_valid)[:, 1]\n        holdout_preds += model.predict_proba(holdout[features])[:, 1] \/ NUM_FOLDS\n        valid_auc = roc_auc_score(y_valid, valid_preds)\n        end = time.time()\n        \n        print(f'Fold {fold} AUC: {round(valid_auc, 6)} in {round((end-start) \/ 60, 2)} minutes.')\n        \n        time.sleep(0.5)\n        if trial:\n            # Use pruning on fold AUC\n            trial.report(\n                value = valid_auc,\n                step = fold\n            )\n            # prune slow trials and bad fold AUCs\n            if trial.should_prune():\n                raise optuna.TrialPruned()\n        \n        \n    return roc_auc_score(holdout['target'], holdout_preds)","81029ea8":"# Percentile Pruner settings\npruner = PercentilePruner(\n    percentile = 66,\n    n_startup_trials = 5,\n    n_warmup_steps = 0,\n    interval_steps = 1,\n    n_min_trials = 5,\n)","93ff2186":"def parameter_search(trials):\n    \n    # Optuna objective function\n    def objective(trial):\n        model_params = dict( \n            # default 6\n            max_depth = trial.suggest_int(\n                \"max_depth\", 2, 12\n            ), \n            # default 0.3\n            learning_rate = trial.suggest_loguniform(\n                \"learning_rate\", 0.01, 0.3\n            ),\n            # default 0\n            gamma = trial.suggest_loguniform(\n                \"gamma\", 1e-10, 100\n            ), \n            # default 1\n            min_child_weight = trial.suggest_loguniform(\n                \"min_child_weight\", 1e-2, 1e2\n            ),\n            # default 1\n            subsample = trial.suggest_discrete_uniform(\n                \"subsample\", 0.2, 1.0, 0.01\n            ),\n            # default 1\n            colsample_bytree = trial.suggest_discrete_uniform(\n                \"colsample_bytree\",  0.2, 1.0, 0.01\n            ),\n            # default 1\n            colsample_bylevel = trial.suggest_discrete_uniform(\n                \"colsample_bylevel\",  0.2, 1.0, 0.01\n            ),\n            # default 1\n            reg_lambda = trial.suggest_loguniform(\n                \"reg_lambda\", 1e-10, 100\n            ),\n            # default 0\n            reg_alpha = trial.suggest_loguniform(\n                \"reg_alpha\", 1e-10, 100\n            ),\n        )\n        \n        return score_xgboost(trial, model_params)\n    \n    \n    optuna.logging.set_verbosity(optuna.logging.DEBUG)\n    study = optuna.create_study(pruner = pruner,direction = \"maximize\")\n    \n    # (nearly) defaults\n    study.enqueue_trial({\n        \"max_depth\": 6,\n        'learning_rate': 0.3, \n        'gamma': 1e-10, \n        'min_child_weight': 1.0, \n        'subsample': 1.0,\n        'colsample_bytree': 1.0,\n        'colsample_bylevel': 1.0,\n        'reg_alpha': 1e-10,\n        'reg_lambda': 1.0,\n    })\n    # high auc from previous run\n    study.enqueue_trial({\n        'max_depth': 4, \n        'learning_rate': 0.010283092300598066, \n        'gamma': 0.03506917176837801,\n        'min_child_weight': 0.3878531236460043, \n        'subsample': 0.8900000000000001, \n        'colsample_bytree': 0.69, \n        'colsample_bylevel': 0.24000000000000002, \n        'reg_lambda': 5.051637651463356e-07,\n        'reg_alpha': 30.170712609605435\n    })\n    study.optimize(objective, n_trials=trials)\n    return study","ae81f9a0":"# Hide output\nstudy = parameter_search(NUM_TRIALS)","184bb527":"print(\"Best Parameters:\", study.best_params)","8ea035c8":"plot_param_importances(study)","7075102d":"# Likely broken on GitHub, view on Kaggle for interactive version\nplot_parallel_coordinate(study)","1082907c":"%%time\ntrain = dt.fread(r'..\/input\/tabular-playground-series-oct-2021\/train.csv').to_pandas()\ntest = dt.fread(r'..\/input\/tabular-playground-series-oct-2021\/test.csv').to_pandas()\nsubmission = dt.fread(r'..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv').to_pandas()\n\ntrain = reduce_memory_usage(train)\ntest = reduce_memory_usage(test)\ngc.collect()","ea419085":"# Similar to scoring function but trains on full data and predicts on test\ndef train_xgboost(folds, model_params = {}, fit_params = {}):\n    \n    # Store the holdout predictions\n    test_preds = np.zeros((test.shape[0],))\n    print('')\n    \n    # Stratified k-fold cross-validation\n    skf = StratifiedKFold(n_splits = folds, shuffle = True, random_state = RANDOM_SEED)\n    for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train['target'])):\n        \n        # Training and Validation Sets\n        start = time.time()\n        X_train, y_train = train[features].iloc[train_idx], train['target'].iloc[train_idx]\n        X_valid, y_valid = train[features].iloc[valid_idx], train['target'].iloc[valid_idx]\n        \n        # Define Model\n        model = XGBClassifier(**default_params, **model_params)\n        gc.collect()\n        \n        model.fit(\n            X_train, y_train,\n            verbose = False,\n            eval_set = [(X_valid, y_valid)],\n            eval_metric = \"logloss\",\n            early_stopping_rounds = EARLY_STOP,\n            **fit_params\n        )\n        \n        # validation and test predictions\n        valid_preds = model.predict_proba(X_valid)[:, 1]\n        test_preds += model.predict_proba(test[features])[:, 1] \/ folds\n        \n        # fold auc score\n        fold_auc = roc_auc_score(y_valid, valid_preds)\n        end = time.time()\n        print(f'Fold {fold} AUC: {round(fold_auc, 6)} in {round((end-start) \/ 60, 2)} minutes.')\n\n        \n    return test_preds","0bf9fbbe":"# Make submission\nsubmission['target'] = train_xgboost(6, model_params = study.best_params)\nsubmission.to_csv('xgboost_submission.csv', index=False)","9f8e2b86":"# Evaluation","a5001cb6":"## 2. Parameter Importances","3c97c327":"## 3. Parallel Coordinate Plot\n\nClick on the vertical axes to see how certain parameter ranges affected the scores","2454d158":"# Preparing the Data\n\n1. Load data with `datatable` and convert to `pandas`\n2. Reduce memory usage by downcasting datatypes\n3. Get holdout set from training data using a stratified scheme","f3e30aad":"# XGBoost\n\nWe create a function to train an XGBoost model and return the holdout AUC.","d450c500":"## 1. Best Parameters","e453ea35":"## 2. Scoring Function\n\n* `model_params` - parameters passed to `XGBClassifier`\n* `fit_params` - parameters passed to the `fit` method","5bc3b608":"# Make Submission","499b6e7f":"# XGBoost Hyperparameter Search\n\nIn this notebook we optimize an XGBoost model using the optuna library along with a [pruner](https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/pruners.html). For each set of parameters, we perform k-fold cross-validation and our pruner references past models trained on the same data and ends unpromising trials early (i.e. if the AUC on a given fold is too low).","bf8ac871":"## 1. Default Parameters","c7efaf1a":"Hope you found this notebook useful, feel free to fork it and adapt it to your own uses.","25d435f2":"# Hyperparameter Search\n\nTo tweak the pruner consider adding\/adjusting the following keyword arguments:\n\n* `percentile` - prunes trial if in lower percentile of trials at a given step\n* `n_startup_trials` - number of trials (models trained) before pruning starts\n* `n_warmup_steps` - number of iterations before pruning checks\n* `interval_steps` - number of iterations between pruning checks\n* `n_min_trials` - skip pruning check if too few trials"}}