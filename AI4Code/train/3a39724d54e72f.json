{"cell_type":{"1d9ab540":"code","6131e1f1":"code","c471840e":"code","9b7517aa":"code","3a3d4212":"code","92f691bf":"code","49eacf74":"code","afd03dce":"code","fa647968":"code","dc2869b9":"code","389e5e79":"code","122dc40e":"code","d5e0fd41":"code","3bdac9d3":"code","a4dfe9a7":"code","2d626e6a":"code","ce3945fc":"code","8c8fea22":"code","8afd4bfb":"code","c787d41d":"code","377f67c7":"code","c12f5ddd":"code","cf20ab09":"code","3610fc74":"code","4c5efccf":"code","ae27071a":"code","0fef8c89":"code","f88700a4":"code","3aa860bf":"code","cc5930e1":"code","960a674d":"code","7ca8adaa":"code","b5ac2797":"code","2ac80e65":"code","b6c9dba9":"code","f0f95662":"code","3dc91acd":"code","21279a0d":"code","c6022c35":"code","5e3c4acf":"code","02027345":"code","248bd745":"code","338d16e8":"code","76661c27":"code","20cb6dff":"code","796eb09e":"code","7ea37e49":"code","96e9e160":"code","b3eed237":"code","ca24d294":"code","a36a28f3":"code","b24ae9d1":"code","651a6e83":"code","a74a4f6e":"code","0da20e48":"code","32d60f37":"code","46320c9f":"markdown","f70a8214":"markdown","5713d7c6":"markdown","13a47582":"markdown","83dd9594":"markdown","fa3fef66":"markdown","c9a3cc7c":"markdown","9d65d3a0":"markdown","944926df":"markdown","d6a54090":"markdown","a7e6c564":"markdown","eedfefd6":"markdown","42a89b60":"markdown","0fd8fa3a":"markdown","ab0ce385":"markdown","80fa4648":"markdown","e68f8b55":"markdown","d3ac02c3":"markdown","8575ef21":"markdown","57be267d":"markdown","02ac4a8e":"markdown","0237bbe1":"markdown","6c1715b4":"markdown","359cd154":"markdown","33424af9":"markdown","efea3389":"markdown","c406f739":"markdown","ddf33aa3":"markdown","4b8bc33b":"markdown","63b1b766":"markdown"},"source":{"1d9ab540":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom ast import literal_eval\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel, cosine_similarity\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom surprise import Reader, Dataset, SVD\nfrom sklearn import svm\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split\nimport datetime\nimport string\n\n\nimport warnings; warnings.simplefilter('ignore')\nTHRESHOLD_PREDICTION = 1","6131e1f1":"def clean_sentence(s, concat=None):\n    s = s.translate(str.maketrans('', '', string.punctuation))\n    s = s.lower()\n    if concat:\n        s = concat.join(s.split())\n    return s\n\ndef _make_in_format(df):\n    y = np.array(df['rating'])\n    temp_x = df.drop('rating', axis=1)  \n#     print(temp_x)\n    #min-max normalization\n#     temp_x = (temp_x-temp_x.mean())\/(temp_x.max()-temp_x.min())\n    X = np.array(temp_x)\n\n    return X,y\n\ndef accuracy_score(y_test,predictions):\n        correct = []\n        for i in range(len(y_test)):\n            if predictions[i]>=y_test[i]-THRESHOLD_PREDICTION and predictions[i]<=y_test[i]+THRESHOLD_PREDICTION:\n                correct.append(1)\n            else:\n                correct.append(0)\n\n        accuracy = sum(map(int,correct))*1.0\/len(correct)\n        return accuracy","c471840e":"md = pd.read_csv('..\/input\/the-movies-dataset\/movies_metadata.csv')\nmd.head()","9b7517aa":"credits = pd.read_csv('..\/input\/the-movies-dataset\/credits.csv')\nkeywords = pd.read_csv('..\/input\/the-movies-dataset\/keywords.csv')\nratings = pd.read_csv('..\/input\/the-movies-dataset\/ratings_small.csv')","3a3d4212":"credits.head(5)","92f691bf":"keywords.head(5)","49eacf74":"ratings.head(5)","afd03dce":"keywords_len = len(keywords)\nkeywords_dict = {}\nkeywords_id_dict = {}\n\nfor it in range(keywords_len):\n    keywords_arr = keywords.iloc[it]['keywords']\n    keywords_arr = eval(keywords_arr)\n    keywords_id_dict[keywords.iloc[it]['id']]=\"\"\n    for iit in range(len(keywords_arr)):\n        keywords_id_dict[keywords.iloc[it]['id']] = keywords_id_dict[keywords.iloc[it]['id']] + clean_sentence(keywords_arr[iit]['name']) + \" \"\n        if keywords_dict.get(keywords_arr[iit]['name']):\n            keywords_dict[keywords_arr[iit]['name']] = keywords_dict[keywords_arr[iit]['name']]+1\n        else:\n            keywords_dict[keywords_arr[iit]['name']]=1","fa647968":"# sort in ascending order of occurence\nkeyword_occurences = []\nfor k,v in keywords_dict.items():\n    keyword_occurences.append([k,v])\nkeyword_occurences.sort(key = lambda x:x[1], reverse = True)","dc2869b9":"# HISTOGRAMS\nfig = plt.figure(1, figsize=(18,13))\nax = fig.add_subplot(1,1,1)\ntrunc_occurences = keyword_occurences[0:50]\ny_axis = [i[1] for i in trunc_occurences]\nx_axis = [k for k,i in enumerate(trunc_occurences)]\nx_label = [i[0] for i in trunc_occurences]\nplt.xticks(rotation=85, fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.xticks(x_axis, x_label)\nplt.ylabel(\"Nb. of occurences\", fontsize = 18, labelpad = 10)\nax.bar(x_axis, y_axis, align = 'center', color='g')\n#_______________________\nplt.title(\"Keywords popularity\",bbox={'facecolor':'k', 'pad':5},color='w',fontsize = 25)\nplt.show()","389e5e79":"def missing_factor(p_df):\n    missing_df = p_df.isnull().sum(axis=0).reset_index()\n    missing_df.columns = ['column_name', 'missing_count']\n    missing_df['filling_factor'] = (p_df.shape[0] \n                                - missing_df['missing_count']) \/ p_df.shape[0] * 100\n    \n    return missing_df","122dc40e":"meta_missing = missing_factor(md)\nmeta_missing.sort_values('filling_factor').reset_index(drop = True)","d5e0fd41":"keywords_missing = missing_factor(keywords)\nkeywords_missing.sort_values('filling_factor').reset_index(drop = True)","3bdac9d3":"credits_missing = missing_factor(credits)\ncredits_missing.sort_values('filling_factor').reset_index(drop = True)","a4dfe9a7":"ratings_missing = missing_factor(ratings)\nratings_missing.sort_values('filling_factor').reset_index(drop = True)","2d626e6a":"md.replace(r'^\\s*$', np.NaN, regex=True)\nmd = md.dropna(subset=['overview'])\nmd = md.dropna(subset=['vote_count'])\nmd = md.dropna(subset=['release_date'])\nmd = md.dropna(subset=['revenue'])\nmd = md.dropna(subset=['title'])\nmd = md.dropna(subset=['vote_average'])","ce3945fc":"md.shape","8c8fea22":"movie_id_dict = {}\nmovies_data_len = md.shape[0]\ntrain_dataset = pd.DataFrame()\navg_popularity = 0\navg_vote_count = 0\navg_vote_average = 0\navg_revenue = 0\n\ntotally_filled_data_count = 1\n\npopularity_total = 0\nvote_count_total = 0\nvote_average_total = 0\nrevenue_total = 0\n\nflag = False\n\nfor it in range(movies_data_len):\n    if md.iloc[it]['popularity'] and isinstance(md.iloc[it]['popularity'], float) and np.isnan(md.iloc[it]['popularity'])==False:\n        if md.iloc[it]['vote_count'] and isinstance(md.iloc[it]['vote_count'], float) and np.isnan(md.iloc[it]['vote_count'])==False:\n            if md.iloc[it]['vote_average']  and isinstance(md.iloc[it]['vote_average'], float) and np.isnan(md.iloc[it]['vote_average'])==False:\n                if md.iloc[it]['revenue'] and isinstance(md.iloc[it]['revenue'], float) and np.isnan(md.iloc[it]['revenue'])==False:\n                    popularity_total += md.iloc[it]['popularity']\n                    vote_count_total += md.iloc[it]['vote_count']\n                    vote_average_total += md.iloc[it]['vote_average']\n                    revenue_total += md.iloc[it]['revenue']\n                    totally_filled_data_count += 1\n                    \n    cur_genres = eval(md.iloc[it]['genres'])\n    concated_genres = \"\" \n    for git in range(len(cur_genres)):\n        item = cur_genres[git]['name']\n        concated_genres += clean_sentence(item, \"_\")+' '\n        \n    \n    movie_id_dict[int(md.iloc[it]['id'])] = {'popularity': md.iloc[it]['popularity'], \n                                        'vote_count': md.iloc[it]['vote_count'], \n                                        'vote_average': md.iloc[it]['vote_average'],\n                                        'revenue': md.iloc[it]['revenue'],\n                                        'genres': concated_genres,\n                                        'overview': clean_sentence(md.iloc[it]['overview']),\n                                        'title': clean_sentence(md.iloc[it]['title'])}\n    \navg_popularity = popularity_total\/totally_filled_data_count\navg_vote_count = vote_count_total\/totally_filled_data_count\navg_vote_average = vote_average_total\/totally_filled_data_count\navg_revenue = revenue_total\/totally_filled_data_count","8afd4bfb":"movie_id_dict[15602]","c787d41d":"md['genres'] = md['genres'].fillna('[]').apply(literal_eval).apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])","377f67c7":"vote_counts = md[md['vote_count'].notnull()]['vote_count'].astype('int')\nvote_averages = md[md['vote_average'].notnull()]['vote_average'].astype('int')\nC = vote_averages.mean()\nC","c12f5ddd":"m = vote_counts.quantile(0.90)\nm ","cf20ab09":"md['year'] = pd.to_datetime(md['release_date'], errors='coerce').apply(lambda x: str(x).split('-')[0] if x != np.nan else np.nan)","3610fc74":"qualified = md[(md['vote_count'] >= m) & (md['vote_count'].notnull()) & (md['vote_average'].notnull())][['title', 'year', 'vote_count', 'vote_average', 'popularity', 'genres']]\nqualified['vote_count'] = qualified['vote_count'].astype('int')\nqualified['vote_average'] = qualified['vote_average'].astype('int')\nqualified.shape","4c5efccf":"def weighted_rating(x):\n    v = x['vote_count']\n    R = x['vote_average']\n    return (v\/(v+m) * R) + (m\/(m+v) * C)","ae27071a":"qualified['wr'] = qualified.apply(weighted_rating, axis=1)","0fef8c89":"qualified = qualified.sort_values('wr', ascending=False).head(250)","f88700a4":"qualified.head(10)","3aa860bf":"s = md.apply(lambda x: pd.Series(x['genres']),axis=1).stack().reset_index(level=1, drop=True)\ns.name = 'genre'\ngen_md = md.drop('genres', axis=1).join(s)","cc5930e1":"def build_chart(genre, percentile=0.85):\n    df = gen_md[gen_md['genre'] == genre]\n    vote_counts = df[df['vote_count'].notnull()]['vote_count'].astype('int')\n    vote_averages = df[df['vote_average'].notnull()]['vote_average'].astype('int')\n    C = vote_averages.mean()\n    m = vote_counts.quantile(percentile)\n    \n    qualified = df[(df['vote_count'] >= m) & (df['vote_count'].notnull()) & (df['vote_average'].notnull())][['title', 'year', 'vote_count', 'vote_average', 'popularity']]\n    qualified['vote_count'] = qualified['vote_count'].astype('int')\n    qualified['vote_average'] = qualified['vote_average'].astype('int')\n    \n    qualified['wr'] = qualified.apply(lambda x: (x['vote_count']\/(x['vote_count']+m) * x['vote_average']) + (m\/(m+x['vote_count']) * C), axis=1)\n    qualified = qualified.sort_values('wr', ascending=False).head(250)\n    \n    return qualified","960a674d":"build_chart('Romance').head(15)","7ca8adaa":"links_small = pd.read_csv('..\/input\/the-movies-dataset\/links_small.csv')\nlinks_small = links_small[links_small['tmdbId'].notnull()]['tmdbId'].astype('int')\n# md.shape","b5ac2797":"md['id'] = md['id'].astype('int')","2ac80e65":"smd = md[md['id'].isin(links_small)]\nsmd.shape","b6c9dba9":"#Check EDA Notebook for how and why I got these indices.\nsmd['id'] = smd['id'].astype('int')","f0f95662":"credits_id_dict = {}\n\nfor cit in range(len(credits)):\n    cast_item = eval(credits.iloc[cit]['cast'])\n    crew_item = eval(credits.iloc[cit]['crew'])\n    concat_cast_crew = \"\"\n    \n    for cast_it in range(len(cast_item)):\n        concat_cast_crew += clean_sentence(cast_item[cast_it]['name'], \"_\")+\" \"\n        \n    for crew_it in range(len(crew_item)):\n        concat_cast_crew += clean_sentence(crew_item[crew_it]['name'], \"_\")+\" \"\n        \n    credits_id_dict[credits.iloc[cit]['id']] = concat_cast_crew","3dc91acd":"smd['description'] = smd['overview']\n\nfor mvit in range(len(smd)):\n    mid = smd.iloc[mvit]['id']\n    smd.iloc[mvit]['description'] = \"\"\n    \n    if movie_id_dict.get(mid):\n        smd.iloc[mvit]['description'] += movie_id_dict[mid]['genres'] + \" \" + movie_id_dict[mid]['overview']\n        \n    if keywords_id_dict.get(mid):\n        smd.iloc[mvit]['description'] += keywords_id_dict[mid] + \" \"\n        \n    if credits_id_dict.get(mid):\n        smd.iloc[mvit]['description'] += credits_id_dict[mid] + \" \"","21279a0d":"smd.shape","c6022c35":"tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\ntfidf_matrix = tf.fit_transform(smd['description'])","5e3c4acf":"# tfidf_matrix.shape\ntfidf_matrix","02027345":"cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)","248bd745":"cosine_sim.shape","338d16e8":"smd = smd.reset_index()\ntitles = smd['title']\nindices = pd.Series(smd.index, index=smd['title'])","76661c27":"def get_recommendations(title):\n    idx = indices[title]\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:31]\n    movie_indices = [i[0] for i in sim_scores]\n    return titles.iloc[movie_indices]","20cb6dff":"get_recommendations('The Godfather').head(20)","796eb09e":"get_recommendations('The Dark Knight').head(10)","7ea37e49":"reader = Reader()","96e9e160":"from numpy import nan","b3eed237":"rating_data_len = ratings.shape[0]\n\nmodified_trainset = pd.DataFrame(index=range(rating_data_len), \n                                columns=['userId', 'movieId','popularity',\n                                        'vote_count','vote_average','revenue', \n                                        'rating'])\npopularityArr = [0]*rating_data_len\nvoteCountArr = [0]*rating_data_len\nvoteAverageArr = [0]*rating_data_len\nrevenueArr = [0]*rating_data_len\n\n\nfor it in range(rating_data_len):\n    movie_id = int(ratings.iloc[it]['movieId'])\n    movie_metadata = movie_id_dict.get(movie_id)\n    \n    if movie_metadata:\n        temp = movie_metadata['popularity']\n        if isinstance(temp, str):\n            temp = float(temp)\n            \n        if np.isnan(temp):\n            popularityArr[it] = avg_popularity\n        else:\n            popularityArr[it] = temp\n\n            \n            \n        temp = movie_metadata['vote_count']\n        if isinstance(temp, str):\n            temp = float(temp)\n            \n        if np.isnan(temp):\n            voteCountArr[it] =  avg_vote_count\n        else:\n            voteCountArr[it] = temp\n\n            \n            \n        temp = movie_metadata['vote_average']\n        if isinstance(temp, str):\n            temp = float(temp)\n            \n        if np.isnan(temp):\n            voteAverageArr[it] = avg_vote_average\n        else:\n            voteAverageArr[it] = temp\n\n        \n        \n        temp = movie_metadata['revenue']\n        if isinstance(temp, str):\n            temp = float(temp)\n            \n        if np.isnan(temp):\n            revenueArr[it] =  avg_revenue\n        else:\n            revenueArr[it] = temp\n    else:\n        popularityArr[it] = avg_popularity\n        voteCountArr[it] =  avg_vote_count\n        voteAverageArr[it] = avg_vote_average\n        revenueArr[it] =  avg_revenue","ca24d294":"modified_trainset['userId'] = ratings['userId']*100\nmodified_trainset['movieId'] = ratings['movieId']\nmodified_trainset['popularity'] = popularityArr\nmodified_trainset['vote_count'] = voteCountArr\nmodified_trainset['vote_average'] = voteAverageArr\nmodified_trainset['revenue'] = revenueArr\nmodified_trainset['rating'] = ratings['rating']","a36a28f3":"modified_trainset.head(12)","b24ae9d1":"X,y = _make_in_format(modified_trainset)\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=1)","651a6e83":"svr_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\nsvr_system = svr_rbf.fit(X_train, y_train)","a74a4f6e":"y_pred_svr = svr_system.predict(X_test)\naccuracy_score(y_test, y_pred_svr)","0da20e48":"def make_prediction_svr(userId, movieId):\n    x_test = modified_trainset.loc[modified_trainset['movieId'] == movieId].head(1)\n    if len(x_test)==0:\n        print('No movie found')\n    else:\n        y_prediction = svr_system.predict([[userId, movieId, x_test['popularity'], x_test['vote_count'], x_test['vote_average'], x_test['revenue']]])\n        print(y_prediction)","32d60f37":"make_prediction_svr(100, 1029)","46320c9f":"# Movie Recommendation System & Rating Predictor","f70a8214":"## Exploration\n---\nLet's import necessary modules first","5713d7c6":"We see that for **The Dark Knight**, our system is able to identify it as a Batman film and subsequently recommend other Batman films as its top recommendations. But unfortunately, that is all this system can do at the moment.","13a47582":"#### 3.1.2 Top movies","83dd9594":"We will now take a look at the frequency of keywords. This will help us to get the popular keywords.","fa3fef66":"The top romance movie according to our metrics is Bollywood's **Dilwale Dulhania Le Jayenge**.","c9a3cc7c":"### 1.2 Filling factor: missing values\n\nThe dataset consists in 45466 films or TV series which are described by 24 variables. As in every analysis, at some point, we will have to deal with the missing values and as a first step, I determine the amount of data which is missing in every variable:","9d65d3a0":"### 1.1 Keywords\n\nWe will first load the movies metadata and try to explore it.","944926df":"Now, we will create a movie dictionary with movie id as key and another dictionary with important feature as value","d6a54090":"### Collaborative Filtering\n\nOur content based engine suffers from some severe limitations. It is only capable of suggesting movies which are *close* to a certain movie. That is, it is not capable of capturing tastes and providing recommendations across genres.\n\nAlso, the engine that we built is not really personal in that it doesn't capture the personal tastes and biases of a user. Anyone querying our engine for recommendations based on a movie will receive the same recommendations for that movie, regardless of who s\/he is.\n\nTherefore, in this section, we will use a technique called **Collaborative Filtering** to make recommendations to Movie Watchers. Collaborative Filtering is based on the idea that users similar to a me can be used to predict how much I will like a particular product or service those users have used\/experienced but I have not.\n\nWe will not be implementing Collaborative Filtering from scratch. Instead, we will use the **sklearn's svr**.","a7e6c564":"We can see that, there are 24 columns in each row. But in this notebook, I will only consider the following columns in my calculation-\n\n- Overview\n- Cast-crew\n- Director\n- Genre\n- Vote count\n- Release date\n- Revenue\n- Vote average\n\nNow load the **credits**,**keywords** and **ratings** dataset-","eedfefd6":"Some utility functions","42a89b60":"## Recommendation Engine\n---\nNow we will build some recommendation engine.","0fd8fa3a":"### 3.1 Simple Recommender\n\nThe Simple Recommender offers generalized recommnendations to every user based on movie popularity and (sometimes) genre. The basic idea behind this recommender is that movies that are more popular and more critically acclaimed will have a higher probability of being liked by the average audience. This model does not give personalized recommendations based on the user.","ab0ce385":"In this notebook, I will attempt at implementing a few recommendation algorithms (content based, popularity based and collaborative filtering) and try to build an ensemble of these models to come up with our final recommendation system. With us, we have two MovieLens datasets.\n\n* **The Full Dataset:** Consists of 26,000,000 ratings and 750,000 tag applications applied to 45,000 movies by 270,000 users. Includes tag genome data with 12 million relevance scores across 1,100 tags.\n* **The Small Dataset:** Comprises of 100,000 ratings and 1,300 tag applications applied to 9,000 movies by 700 users.\n\nI will build a Simple Recommender using movies from the *Full Dataset* whereas all personalised recommender systems will make use of the small dataset (due to the computing power I possess being very limited). As a first step, I will build my simple recommender system.\n\n**NB:** This is the modified version of Rounak Banik's implementation.\n\n---\nThis notebook is organized as follows:\n\n**1. Exploration**\n- 1.1 Keywords\n- 1.2 Filling factor: missing values\n\n**2. Data Cleaning**\n- 2.1 Drop missing valued rows\n    \n**3. Recommendation Engine**\n- 3.1 Simple Recommender\n - 3.1.1 Weight sorting\n - 3.1.2 Top movies\n- 3.2 Content Based Recommender\n - 3.2.1 Movie metadata based filtering\n- 3.3 Collaborative Filtering\n\n**4. Conclusion: possible improvements and points to adress**","80fa4648":"## Conclusion\n\nIn this notebook, I have built 4 different recommendation engines based on different ideas and algorithms. They are as follows:\n\n1. **Simple Recommender:** This system used overall TMDB Vote Count and Vote Averages to build Top Movies Charts, in general and for a specific genre. The IMDB Weighted Rating System was used to calculate ratings on which the sorting was finally performed.\n2. **Content Based Recommender:** We built a content based engine; that took movie overview, cast, crew, genre and keywords to come up with predictions.\n3. **Collaborative Filtering:** We used the powerful sklearn library to build a collaborative filter based on support vector machine. The engine gave estimated ratings for a given user and movie.","e68f8b55":"Let us now construct our function that builds charts for particular genres. For this, we will use relax our default conditions to the **85th** percentile instead of 90. ","d3ac02c3":"#### 3.2.1 Movie metadata based filtering\n\nLet us first try to build a recommender using movie metadata(overview, ). We do not have a quantitative metric to judge our machine's performance so this will have to be done qualitatively.","8575ef21":"Let us see our method in action by displaying the Top **15 Romance Movies** (Romance almost didn't feature at all in our Generic Top Chart despite  being one of the most popular movie genres).","57be267d":"## Data Cleaning\n\nWe've saw that the main data columns in the dataset is well filled. So, we've decided to drop the row which are not well filled in the concerned column.\n\n### 2.1 Drop missing valued rows\n\nOnly the metadata set have some missing valued rows in the following columns of concern:\n\n- Overview\n- Vote count\n- Release date\n- Revenue\n- Title\n- Vote average\n\nSo, we will drop the rows which have missing values in the specified columns","02ac4a8e":"We have **9087** movies avaiable in our small movies metadata dataset which is 5 times smaller than our original dataset of movies.","0237bbe1":"\n\nWe can see that most of the variables are well filled since only 3 of them have a filling factor below 97%. Later we will see that those 3 columns are not used in any calculation.\n\n\n\n","6c1715b4":"#### Cosine Similarity\n\nWe will be using the Cosine Similarity to calculate a numeric quantity that denotes the similarity between two movies. Mathematically, it is defined as follows:\n\n$cosine(x,y) = \\frac{x. y^\\intercal}{||x||.||y||} $\n\nSince we have used the TF-IDF Vectorizer, calculating the Dot Product will directly give us the Cosine Similarity Score. Therefore, we will use sklearn's **linear_kernel** instead of cosine_similarities since it is much faster.","359cd154":"We now have a pairwise cosine similarity matrix for all the movies in our dataset. The next step is to write a function that returns the 30 most similar movies based on the cosine similarity score.","33424af9":"Therefore, to qualify to be considered for the chart, a movie has to have at least **434 votes**. We also see that the average rating for a movie on is **5.244** on a scale of 10. **4462** Movies qualify to be on our chart.","efea3389":"**SVR**","c406f739":"We're all set. Let us now try and get the top recommendations for a few movies and see how good the recommendations are.","ddf33aa3":"#### 3.1.1 Weight sorting\n\nThe implementation of this model is extremely trivial. All we have to do is sort our movies based on ratings and popularity and display the top movies of our list. As an added step, we can pass in a genre argument to get the top movies of a particular genre. ","4b8bc33b":"### 3.2 Content Based Recommender\n\nTo personalise our recommendations more, we are going to build an engine that computes similarity between movies based on certain metrics and suggests movies that are most similar to a particular movie that a user liked. Since we will be using movie metadata (or content) to build this engine, this also known as **Content Based Filtering.** We will use the TMDB movies for this purpose because of computational complexity.\n","63b1b766":"We will use IMDB's *weighted rating* formula to construct my chart. Mathematically, it is represented as follows:\n\nWeighted Rating (WR) = $(\\frac{v}{v + m} . R) + (\\frac{m}{v + m} . C)$\n\nwhere,\n* *v* is the number of votes for the movie\n* *m* is the minimum votes required to be listed in the chart\n* *R* is the average rating of the movie\n* *C* is the mean vote across the whole report\n\nThe next step is to determine an appropriate value for *m*, the minimum votes required to be listed in the chart. We will use **90th percentile** as our cutoff. In other words, for a movie to feature in the charts, it must have more votes than at least 90% of the movies in the list.\n\nWe will build our overall Top 250 Chart and will define a function to build charts for a particular genre. Let's begin!"}}