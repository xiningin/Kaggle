{"cell_type":{"2a89708c":"code","ec5e660c":"code","f8a19f10":"code","0315a88d":"code","c35e3b78":"code","dc2b90f8":"code","45e2782e":"code","6a5e1c6c":"code","4477a16d":"code","4fe99237":"code","8ec7c0e2":"code","7d95bcbe":"code","e2044d2c":"code","e04e5f97":"code","b1ffb70c":"code","a8acc97e":"code","731f1538":"code","15ddae90":"code","23fa9298":"markdown","5b7fff79":"markdown","a617ac81":"markdown","dd08b1b0":"markdown","8d30a757":"markdown","d9455834":"markdown","e4f657ac":"markdown"},"source":{"2a89708c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport numpy as np\nimport pandas as pd\nimport numpy as np\nimport random as rnd\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.gridspec as gridspec\nfrom sklearn.preprocessing import StandardScaler\nfrom numpy import genfromtxt\nfrom scipy.stats import multivariate_normal\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score , average_precision_score\nfrom sklearn.metrics import precision_score, precision_recall_curve\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n\n%matplotlib inline\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ec5e660c":"df = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\nprint(df.columns.values)","f8a19f10":"df.head(5)","0315a88d":"ax = sns.countplot(x=\"Class\", data=df)","c35e3b78":"def estimateGaussian(dataset):\n    mu = np.mean(dataset, axis=0)\n    sigma = np.cov(dataset.T)\n    return mu, sigma\n\ndef multivariateGaussian(dataset,mu,sigma):\n    p = multivariate_normal(mean=mu, cov=sigma)\n    return p.pdf(dataset)","dc2b90f8":"def selectThresholdByCV(probs,gt):\n    best_epsilon = 0\n    best_f1 = 0\n    f = 0\n    farray = []\n    Recallarray = []\n    Precisionarray = []\n    epsilons = (0.0000e+00, 1.0527717316e-70, 1.0527717316e-50, 1.0527717316e-24)\n    #epsilons = np.asarray(epsilons)\n    for epsilon in epsilons:\n        predictions = (p_cv < epsilon)\n        f = f1_score(train_cv_y, predictions, average = \"binary\")\n        Recall = recall_score(train_cv_y, predictions, average = \"binary\")\n        Precision = precision_score(train_cv_y, predictions, average = \"binary\")\n        farray.append(f)\n        Recallarray.append(Recall)\n        Precisionarray.append(Precision)\n        print ('For below Epsilon')\n        print(epsilon)\n        print ('F1 score , Recall and Precision are as below')\n        print ('Best F1 Score %f' %f)\n        print ('Best Recall Score %f' %Recall)\n        print ('Best Precision Score %f' %Precision)\n        print ('-'*40)\n        if f > best_f1:\n            best_f1 = f\n            best_recall = Recall\n            best_precision = Precision\n            best_epsilon = epsilon    \n    fig = plt.figure()\n    ax = fig.add_axes([0.1, 0.5, 0.7, 0.3])\n    #plt.subplot(3,1,1)\n    plt.plot(farray ,\"ro\")\n    plt.plot(farray)\n    ax.set_xticks(range(5))\n    ax.set_xticklabels(epsilons,rotation = 60 ,fontsize = 'medium' )\n    ax.set_ylim((0,1.0))\n    ax.set_title('F1 score vs Epsilon value')\n    ax.annotate('Best F1 Score', xy=(best_epsilon,best_f1), xytext=(best_epsilon,best_f1))\n    plt.xlabel(\"Epsilon value\") \n    plt.ylabel(\"F1 Score\") \n    plt.show()\n    fig = plt.figure()\n    ax = fig.add_axes([0.1, 0.5, 0.9, 0.3])\n    #plt.subplot(3,1,2)\n    plt.plot(Recallarray ,\"ro\")\n    plt.plot(Recallarray)\n    ax.set_xticks(range(5))\n    ax.set_xticklabels(epsilons,rotation = 60 ,fontsize = 'medium' )\n    ax.set_ylim((0,1.0))\n    ax.set_title('Recall vs Epsilon value')\n    ax.annotate('Best Recall Score', xy=(best_epsilon,best_recall), xytext=(best_epsilon,best_recall))\n    plt.xlabel(\"Epsilon value\") \n    plt.ylabel(\"Recall Score\") \n    plt.show()\n    fig = plt.figure()\n    ax = fig.add_axes([0.1, 0.5, 0.9, 0.3])\n    #plt.subplot(3,1,3)\n    plt.plot(Precisionarray ,\"ro\")\n    plt.plot(Precisionarray)\n    ax.set_xticks(range(5))\n    ax.set_xticklabels(epsilons,rotation = 60 ,fontsize = 'medium' )\n    ax.set_ylim((0,1.0))\n    ax.set_title('Precision vs Epsilon value')\n    ax.annotate('Best Precision Score', xy=(best_epsilon,best_precision), xytext=(best_epsilon,best_precision))\n    plt.xlabel(\"Epsilon value\") \n    plt.ylabel(\"Precision Score\") \n    plt.show()\n    return best_f1, best_epsilon","45e2782e":"v_features = df.iloc[:,1:29].columns","6a5e1c6c":"plt.figure(figsize=(12,8*4))\ngs = gridspec.GridSpec(7, 4)\nfor i, cn in enumerate(df[v_features]):\n    ax = plt.subplot(gs[i])\n    sns.distplot(df[cn][df.Class == 1], bins=50)\n    sns.distplot(df[cn][df.Class == 0], bins=50)\n    ax.set_xlabel('')\n    ax.set_title('feature: ' + str(cn))\nplt.show()","4477a16d":"rnd_clf = RandomForestClassifier(n_estimators = 100 , criterion = 'entropy',random_state = 0)\nrnd_clf.fit(df.iloc[:,1:29],df.iloc[:,30]);","4fe99237":"for name, importance in zip(df.iloc[:,1:29].columns, rnd_clf.feature_importances_):\n    if importance > 0.020 :\n        print('\"' + name + '\"'+',')","8ec7c0e2":"df.drop(labels = [\"Amount\",\"Time\"], axis = 1, inplace = True)","7d95bcbe":"train_strip_v1 = df[df[\"Class\"] == 1]\ntrain_strip_v0 = df[df[\"Class\"] == 0]","e2044d2c":"Normal_len = len (train_strip_v0)\nAnomolous_len = len (train_strip_v1)\n\nstart_mid = Anomolous_len \/\/ 2\nstart_midway = start_mid + 1\n\ntrain_cv_v1  = train_strip_v1 [: start_mid]\ntrain_test_v1 = train_strip_v1 [start_midway:Anomolous_len]\n\nstart_mid = (Normal_len * 60) \/\/ 100\nstart_midway = start_mid + 1\n\ncv_mid = (Normal_len * 80) \/\/ 100\ncv_midway = cv_mid + 1\n\ntrain_fraud = train_strip_v0 [:start_mid]\ntrain_cv    = train_strip_v0 [start_midway:cv_mid]\ntrain_test  = train_strip_v0 [cv_midway:Normal_len]\n\ntrain_cv = pd.concat([train_cv,train_cv_v1],axis=0)\ntrain_test = pd.concat([train_test,train_test_v1],axis=0)\n\n\nprint(train_fraud.columns.values)\nprint(train_cv.columns.values)\nprint(train_test.columns.values)\n\ntrain_cv_y = train_cv[\"Class\"]\ntrain_test_y = train_test[\"Class\"]\n\ntrain_cv.drop(labels = [\"Class\"], axis = 1, inplace = True)\ntrain_fraud.drop(labels = [\"Class\"], axis = 1, inplace = True)\ntrain_test.drop(labels = [\"Class\"], axis = 1, inplace = True)","e04e5f97":"mu, sigma = estimateGaussian(train_fraud)\np = multivariateGaussian(train_fraud,mu,sigma)\np_cv = multivariateGaussian(train_cv,mu,sigma)\np_test = multivariateGaussian(train_test,mu,sigma)","b1ffb70c":"fscore, ep= selectThresholdByCV(p_cv,train_cv_y)","a8acc97e":"predictions = (p_test < ep)\nRecall = recall_score(train_test_y, predictions, average = \"binary\")    \nPrecision = precision_score(train_test_y, predictions, average = \"binary\")\nF1score = f1_score(train_test_y, predictions, average = \"binary\")    \nprint ('F1 score , Recall and Precision for Test dataset')\nprint ('Best F1 Score %f' %F1score)\nprint ('Best Recall Score %f' %Recall)\nprint ('Best Precision Score %f' %Precision)","731f1538":"fig, ax = plt.subplots(figsize=(10, 10))\nax.scatter(train_test['V14'],train_test['V11'],marker=\"o\", color=\"lightBlue\")\nax.set_title('Anomalies(in red) vs Predicted Anomalies(in Green)')\nfor i, txt in enumerate(train_test['V14'].index):\n       if train_test_y.loc[txt] == 1 :\n            ax.annotate('*', (train_test['V14'].loc[txt],train_test['V11'].loc[txt]),fontsize=13,color='Red')\n       if predictions[i] == True :\n            ax.annotate('o', (train_test['V14'].loc[txt],train_test['V11'].loc[txt]),fontsize=15,color='Green')","15ddae90":"predictions = (p_cv < ep)\nRecall = recall_score(train_cv_y, predictions, average = \"binary\")    \nPrecision = precision_score(train_cv_y, predictions, average = \"binary\")\nF1score = f1_score(train_cv_y, predictions, average = \"binary\")    \nprint ('F1 score , Recall and Precision for Cross Validation dataset')\nprint ('Best F1 Score %f' %F1score)\nprint ('Best Recall Score %f' %Recall)\nprint ('Best Precision Score %f' %Precision)","23fa9298":"References:\n* https:\/\/www.kaggle.com\/john77eipe\/anomaly-detection-basics\/\n* https:\/\/www.kaggle.com\/shelars1985\/anomaly-detection-using-gaussian-distribution\n* https:\/\/en.wikipedia.org\/wiki\/Multivariate_normal_distribution#Joint_normality","5b7fff79":"Epsilon value = 1.0527717316e-70 is selected as threshold to identify Anomalous transactions\n\nnow time to Predict and calculate F1 , Recall and Precision score for our Test Dataset","a617ac81":"### Feature selection","dd08b1b0":"### Choosing Epsilon Values\n\nI calculated probability value for all the rows present in Normal Transaction and found the minimum probability value by using below command","8d30a757":"In the overview of the data it has been mentioned that \"Class\" field in this dataframe consists binary data in 1 and zeros\n1 for fraudulent transactions, 0 otherwise","d9455834":"Epsilon value is the threshold value below which we will mark transaction as Anomalous.","e4f657ac":"Fraudulent transactions provided here contributes mere 0.17% which indicates we have a highly imbalanced data to work on."}}