{"cell_type":{"c62aa894":"code","d80a12ee":"code","964a31e7":"code","16360409":"code","c655876c":"code","a76222cd":"code","86a67f85":"code","da1535fe":"code","93f069cc":"code","0bdeed07":"code","f8b2aa61":"code","ba153cbe":"code","2050d006":"code","4dc77e79":"code","53143858":"code","3157ec4d":"code","00f5a264":"code","e306457b":"code","557a5bcb":"code","f56a4ab7":"code","a4ca9333":"code","70f0509d":"code","d6f48aea":"code","742bdde3":"code","97dd1a5a":"code","4730446e":"code","c5140262":"code","4ae0563d":"markdown","7d29045b":"markdown","48bf1ef4":"markdown","8e0369bb":"markdown","03fcd340":"markdown","a016459f":"markdown","0d806cc7":"markdown","27f7e780":"markdown","823d6b6b":"markdown","b2546801":"markdown","f74969cc":"markdown","a85813ba":"markdown","e432eba1":"markdown","51418d82":"markdown","85ca2116":"markdown","ce4a6c5d":"markdown","c6b19611":"markdown"},"source":{"c62aa894":"import numpy as np\nimport pandas as pd\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import precision_score, recall_score , f1_score\n\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, ReduceLROnPlateau \nfrom tensorflow.keras.models import load_model","d80a12ee":"# have a look at dataset\ndf = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ndf.head()","964a31e7":"df.info()","16360409":"# Read from pretrained Embeddings\ndef read_from_glove(filename):\n    word_to_index = {}\n    index_to_word = {}\n    word_to_vec   = {}\n    words         = [] \n    with open(filename,\"r\") as f:\n        for i,line in enumerate(f,1):\n            line = line.strip().split()\n            word = line[0]\n            vector = np.array(line[1:],dtype=np.float32)\n            word_to_vec[word] = vector\n            word_to_index[word] = i\n            index_to_word[i] = word\n            words.append(word)\n    return word_to_index, index_to_word, word_to_vec, words \n\nword_to_index, index_to_word, word_to_vec, words = read_from_glove(\"\/kaggle\/input\/glove6b50dtxt\/glove.6B.50d.txt\")","c655876c":"len(df) - df[\"text\"].count()\n# No null values.","a76222cd":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(df[\"text\"])\n\nsequences = tokenizer.texts_to_sequences(df[\"text\"])\nprint(sequences[0])\nprint(len(sequences[2]))","86a67f85":"# Configrations\n\nMAXLEN         = max([len(e) for e in sequences])\nVOCAB_SIZE     = len(word_to_vec)+1\nEMBEDDING_DIM  = 50\nEPOCHS         = 50\nBUFFER_SIZE    = 1000\nBATCH_SIZE     = 16","da1535fe":"def create_emb_matrix(vocab_len,emb_dim):\n    emb_matrix = np.zeros((vocab_len,emb_dim))\n    for i,vec in enumerate(word_to_vec.values()):\n        emb_vec = vec\n        if emb_vec is not None:\n            emb_matrix[i] = emb_vec\n    return emb_matrix\n\nembedding_matrix = create_emb_matrix(VOCAB_SIZE,EMBEDDING_DIM)","93f069cc":"#Padding\npadded_sequences = pad_sequences(sequences, maxlen=MAXLEN, padding=\"post\", truncating=\"pre\")\n\n# Extracting Labels\nlabels=df[\"target\"].values\n\n#Splitting training and Test examples.\nx_train,x_test,y_train,y_test = train_test_split(padded_sequences,labels,stratify=labels,random_state=42,test_size=0.3)\nx_train.shape","0bdeed07":"np.bincount(y_train), np.bincount(y_test)","f8b2aa61":"def create_dataset(padded_sequence,labels,batch_size):\n    dataset = tf.data.Dataset.from_tensor_slices((padded_sequence,labels))\n    dataset = dataset.shuffle(BUFFER_SIZE)\n    dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n    return dataset","ba153cbe":"train_dataset = create_dataset(x_train,y_train,BATCH_SIZE)\ntest_dataset  = create_dataset(x_test,y_test,BATCH_SIZE)","2050d006":"#Building Model\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(VOCAB_SIZE,\n                              EMBEDDING_DIM,\n                              weights = [embedding_matrix],\n                              trainable=True,\n                              input_length=MAXLEN),\n    \n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16,return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(64,activation=\"tanh\"),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(64,activation=\"tanh\"),\n    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n])\n\nmodel.compile(loss=\"binary_crossentropy\",optimizer=tf.keras.optimizers.RMSprop(lr=0.0001,momentum=0.3),metrics=[\"accuracy\"])","4dc77e79":"#model.load_weights(\"\/kaggle\/working\/disaster_v1.h5\",by_name=True)\nmodel.summary()","53143858":"def CB():\n        callbacks = []\n        checkpoint = ModelCheckpoint(\"\/kaggle\/working\/disaster_v1.h5\",\n                                    monitor=\"val_loss\",\n                                    save_best_only=True,\n                                    mode=\"min\",verbose=1,\n                                    )\n\n        reducelr   = ReduceLROnPlateau(monitor=\"val_loss\",\n                                      factor=0.3,patience=5,\n                                      verbose=1, mode=\"min\", min_lr=1e-09)\n\n        log        = CSVLogger(\"\/kaggle\/working\/disaster_v1.csv\")\n\n        callbacks.append(checkpoint)\n        callbacks.append(reducelr)\n        callbacks.append(log)\n        return callbacks\n\ncallbacks = CB()","3157ec4d":"np.bincount(labels)\n# Clear Sign of Data Imbalance\npatches, texts, autotexts = plt.pie(np.bincount(labels),explode=[0.1,0.1],labels=list(np.unique(labels)),radius=1.5,shadow=True,autopct='%1.1f%%')\ntexts[0].set_fontsize(20)\ntexts[1].set_fontsize(20)\nplt.show()","00f5a264":"# Fitting the model\nmodel.fit(train_dataset,epochs=EPOCHS,\n             validation_data=test_dataset,verbose=1,\n             callbacks=[callbacks])","e306457b":"history = pd.read_csv(\"\/kaggle\/working\/disaster_v1.csv\")\nplt.plot(history[\"epoch\"],history[\"loss\"],label=\"Training Loss\",c=\"red\")\nplt.plot(history[\"epoch\"],history[\"val_loss\"],label=\"Validation loss\",c=\"green\")\nplt.legend()\nplt.show()","557a5bcb":"def eval(y_true,y_pred):\n    print(\"Accuracy Score \\t:\",round(accuracy_score(y_true,y_pred),4))\n    print(\"R-square Coefficient \\t:\",round(r2_score(y_true,y_pred),4))\n    # Model's performance is worse than horizontal line\n    print(\"Precision Score \\t:\",round(precision_score(y_true,y_pred),4))\n    print(\"Recall Score \\t:\",round(recall_score(y_true,y_pred),4))\n    print(\"F1 Score \\t:\",round(f1_score(y_true,y_pred),4))\n    \ny_pred = model.predict(x_test)\ncat_out = np.round(y_pred)\ncat_out = cat_out.ravel()\neval(y_test,cat_out)","f56a4ab7":"model.save(\"\/kaggle\/working\/disaster_v1_model.h5\")","a4ca9333":"def create_submission_data(data):\n    sub_data = tf.data.Dataset.from_tensor_slices(data)\n    return sub_data","70f0509d":"#prepairing the submission_data\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\n\nsequences = tokenizer.texts_to_sequences(test_df[\"text\"])\npadded_sequences = pad_sequences(sequences, maxlen=MAXLEN, padding=\"post\", truncating=\"pre\")\n\nsub_data = create_submission_data(padded_sequences)","d6f48aea":"sub_predictions = model.predict(sub_data)\nsub_predictions = sub_predictions.ravel()","742bdde3":"cat_sub_out = np.round(sub_predictions)\n#cat_sub_out = cat_sub_out.ravel()\ncat_sub_out = cat_sub_out.astype(\"int64\")","97dd1a5a":"#Submission\ndef predict(id,data,filename=\"submission.csv\"):\n    with open(filename,\"w\") as file:\n        file.write(\"id,target\\n\")\n        for idx,e in zip(id,data):\n            file.write(f\"{idx},{e}\\n\")\npredict(test_df[\"id\"],cat_sub_out)","4730446e":"sub = pd.read_csv(\"\/kaggle\/working\/submission.csv\")","c5140262":"sub","4ae0563d":"## Submitting the predictions","7d29045b":"### Evaluating the model by calculating <br> -  Accuracy-Score<br>- R2-Score <br> - Precision-Score <br> - Recall-Score <br>- F1-Score","48bf1ef4":"### Looking for NULL Entry in the tweets","8e0369bb":"### Visualizing Class Imbalance","03fcd340":"### Start training","a016459f":"# Import Libraries","0d806cc7":"### Configrations for Model","27f7e780":"### Splitting the train and validation data","823d6b6b":"### Utility function for Defining Callbacks","b2546801":"### Utility function to create Embedding Matrix","f74969cc":"### Utility function to create dataset","a85813ba":"### Checking for Class Imbalance","e432eba1":"# Building the Model","51418d82":"# Prepare the Dataset","85ca2116":"# Using Pretrained Glove Embeddings as weights to Embedding layer ","ce4a6c5d":"### Tokenizing Data","c6b19611":"Got a good accuracy but still can do much better"}}