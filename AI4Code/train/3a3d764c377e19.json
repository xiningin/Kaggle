{"cell_type":{"f2c88b1e":"code","85194bd3":"code","e5b012f7":"code","93ad2427":"code","300b993b":"code","2296a9d4":"code","85ba7960":"code","ea1f9b2e":"code","2da7dcb9":"code","1f8ee72c":"code","4797450c":"code","2a513953":"code","b7c43445":"code","ac4866ed":"code","77554d46":"code","4cf05fad":"markdown","0c184556":"markdown"},"source":{"f2c88b1e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))","85194bd3":"cols_keep = ['Accident_Severity', 'Date','Time', 'Latitude','Longitude',\n             'Local_Authority_(District)', 'Local_Authority_(Highway)',\n            'LSOA_of_Accident_Location', 'Number_of_Casualties', \"1st_Road_Number\",\"2nd_Road_Number\"]","e5b012f7":"df = pd.read_csv('..\/input\/Accident_Information.csv',usecols=cols_keep, #nrows=12345,\n                 parse_dates=[['Date', 'Time']],keep_date_col=True)\ndf.shape","93ad2427":"df[\"Date_Time\"] = pd.to_datetime(df[\"Date_Time\"],infer_datetime_format=True,errors=\"coerce\")","300b993b":"# we see that some cases lack a time of events - creating a bad date format. we'll fix these\n\ndf.loc[df['Date_Time'].isna(), 'Date_Time'] = df[\"Date\"]\ndf.loc[df[\"Date_Time\"].isna()]","2296a9d4":"df.drop([\"Date\",\"Time\"],axis=1,inplace=True)\ndf.set_index(\"Date_Time\",inplace=True)\ndf.index = pd.to_datetime(df.index)","85ba7960":"df[\"serious_accident\"] = df.Accident_Severity != \"Slight\"","ea1f9b2e":"df.nunique()","2da7dcb9":"df.columns","1f8ee72c":"df.describe()","4797450c":"df.index.dtype","2a513953":"df.head()","b7c43445":"# Identifying the worst districts to travel.\n### https:\/\/stackoverflow.com\/questions\/19384532\/how-to-count-number-of-rows-per-group-and-other-statistics-in-pandas-group-by\n### https:\/\/stackoverflow.com\/questions\/32012012\/pandas-resample-timeseries-with-groupby\/39186403#39186403\n\nlsoa_wise = df.groupby( 'LSOA_of_Accident_Location').resample(\"M\").agg({\"Number_of_Casualties\":\"sum\",\"serious_accident\":\"sum\",\n                                                                        \"Accident_Severity\":\"count\",\n                                                                       \n#                                                                         \"Latitude\":scipy.stats.mode,\"Longitude\":scipy.stats.mode\n#                                                                         \"Latitude\":\"mean\",\"Longitude\":\"mean\" # we get missing latLong when no accidents occured, and their locations can change unless we use mode! \n                                                                       })\nlsoa_wise.rename(columns={\"Accident_Severity\":\"Accident_counts\"},inplace=True)\nlsoa_wise[\"percent_seriousAccidents\"] = 100*lsoa_wise[\"serious_accident\"]\/lsoa_wise[\"Accident_counts\"].round(2)\nlsoa_wise.loc[lsoa_wise['percent_seriousAccidents'].isna(), 'percent_seriousAccidents'] = 0\nprint(lsoa_wise.shape)\nlsoa_wise.head()","ac4866ed":"lsoa_wise.describe()","77554d46":"lsoa_wise.to_csv(\"uk_accidents_lsoa_monthly.csv.gz\",compression=\"gzip\")","4cf05fad":"### Get the amount of accidents per LSOA , so we can analyze dangerous regions\n* Could aggregate at more granular level - per junction\/area (e.g. based on LatLong rounding), or by road number !\n\n* For a similar project, see our Anyway\/Public knowledge \/ Datahack hackathon project:\n    * https:\/\/github.com\/hasadna\/anyway\n    * https:\/\/github.com\/ddofer?tab=repositories\n    \n    \n* We will ignore \"slight\" ('fender bender') accidents for now, but any model would benefit from them , and they could still be of interest. ","0c184556":"## Targets\n* based on : https:\/\/www.kaggle.com\/yesterdog\/eda-of-1-6-mil-traffic-accidents-in-london\n* Accidents by LSOA (region), by road, by latlong (rounded)..."}}