{"cell_type":{"c93a0baf":"code","7deaf6f3":"code","1ece8833":"code","f80b3abc":"code","cceaf1b2":"code","85208685":"code","a71e4659":"code","84ba68d1":"code","27d0d3a7":"code","e27a6847":"code","3084a01e":"code","c11b6e0e":"code","329b6d03":"code","f3306832":"code","622ebc79":"code","eb02273b":"code","62d86736":"code","354b1b85":"code","c1ce9993":"code","504d1cfd":"markdown","bd71aa67":"markdown","b3e77de7":"markdown","84fe6e83":"markdown","68823141":"markdown","cdc21b13":"markdown","712ab37f":"markdown","5cdcd7a5":"markdown","00939e83":"markdown","35dbbc10":"markdown","57aa2e18":"markdown","52fe075e":"markdown","cc50a3b7":"markdown","e59d040e":"markdown","41a7f198":"markdown"},"source":{"c93a0baf":"import pandas as pd\n\ndf=pd.read_csv('\/kaggle\/input\/laptop-price\/laptop_price.csv',encoding='latin-1')\ndf.head()","7deaf6f3":"df.drop(['laptop_ID'],inplace=True,axis=1)\ndf.dtypes","1ece8833":"df.isna().sum()","f80b3abc":"df.Ram.value_counts()","cceaf1b2":"df.Ram = df.Ram.map(lambda x: x.rstrip('GB'))\ndf['Ram']=df['Ram'].astype('int32')\ndf.Weight = df.Weight.map(lambda x: x.rstrip('kg'))\ndf['Weight']=df['Weight'].astype('float64')","85208685":"df.dtypes","a71e4659":"df.Cpu.value_counts()","84ba68d1":"df.Memory.value_counts()","27d0d3a7":"df.ScreenResolution.value_counts()","e27a6847":"df[\"CpuSpeed\"] = df.Cpu.map(lambda x: x[-6:])\ndf[\"CpuSpeed\"] = df.CpuSpeed.map(lambda x: x.rstrip('GHz').lstrip('P6U0'))\ndf['CpuSpeed']=df['CpuSpeed'].astype('float64')","3084a01e":"corr_matrix = df.corr()\ncorr_matrix['Price_euros'].sort_values(ascending=False)","c11b6e0e":"from pandas.plotting import scatter_matrix\natt=['Price_euros','Ram','CpuSpeed','Weight','Inches']\nscatter_matrix(df[att],figsize=(12,8))","329b6d03":"df.drop(['Company','Product','TypeName','ScreenResolution','Cpu','Memory','Gpu','OpSys'],axis=1,inplace=True)","f3306832":"from sklearn.model_selection import train_test_split\nimport numpy as np\ntrain_set,test_set = train_test_split(df,test_size=0.2)\ntrain_y=train_set[\"Price_euros\"]\ntrain_x=train_set.drop([\"Price_euros\"], axis=1)\ntest_y=test_set[\"Price_euros\"]\ntest_x=test_set.drop([\"Price_euros\"], axis=1)\ntrain_set_num=train_x.select_dtypes(include=[np.number])\ntrain_set_ob=train_x.select_dtypes(include=[object])\nnum_column=list(train_set_num.columns)\nob_column=list(train_set_ob.columns)\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\n\nnum_pipeline = Pipeline([\n        ('std_scaler', StandardScaler()),\n    ])\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_column),\n        (\"cat\", OrdinalEncoder(), ob_column),\n    ])\n\ntrain_x = full_pipeline.fit_transform(train_x)\ntest_x = full_pipeline.fit_transform(test_x)","622ebc79":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, Lasso, ElasticNet\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport xgboost as xgb\n\ndef eval(model,test_x,test_y):\n    ymul=model.predict(test_x)\n    print(\"Model score: %.4f\" % model.score(test_x,test_y))  # This is just shorthand for the R2 score\n    print(\"Mean absolute error: %.4f\" % mean_squared_error(ymul,test_y))\n    print(\"R2-score: %.4f\" % r2_score(test_y , ymul) )\n    score=np.sqrt(-cross_val_score(model, train_x, train_y, scoring=\"neg_mean_squared_error\", cv = 10))\n    print(\"RMSE score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","eb02273b":"ridge_reg=Ridge(alpha=0.9,solver=\"cholesky\")\nridge_reg.fit(train_x,train_y)\neval(ridge_reg,test_x,test_y)","62d86736":"from sklearn.ensemble import RandomForestRegressor\n\nrmdtree_reg = RandomForestRegressor(max_depth=5)\nrmdtree_reg.fit(train_x,train_y)\neval(rmdtree_reg,test_x,test_y)","354b1b85":"from sklearn.ensemble import AdaBoostRegressor\n\nregr = AdaBoostRegressor(random_state=0, n_estimators=100)\nregr.fit(train_x,train_y)\neval(regr,test_x,test_y)","c1ce9993":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4, gamma=0.05, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.78, n_estimators=2200,\n                             reg_alpha=0.46, reg_lambda=0.85,\n                             subsample=0.52, verbosity=0,\n                             random_state =7, nthread = -1)\nmodel_xgb.fit(train_x,train_y)\neval(model_xgb,test_x,test_y)","504d1cfd":"# Further work\n* Transforming the data in a more sutible way\n* Feature engineering\n* Look more into stacking models","bd71aa67":"This shows we only have to remove GB not covert between units.","b3e77de7":"We do expect the lines to exist, this is because there is only certian values for each ram, cpu speed etc","84fe6e83":"### Random Forest\nA random forest algorthim is a ensemble of decesion trees trained using the bagging method. This involves using the same algorithm and training them on a subets of the training data, once this is done the ensemble makes predictions by agregating the result (most frequent for classification and average for regression).               \nA decision tree is normally used for classification but can also but done for regression. A decision tree works by asking a intial question at the root node for instance in this case is the sale condition normal and then depending on the answer you go down one of the nodes to another question if needed (for simple datasets it may not be needed to be able to divide the data up). You measure the impurity of the node called gini, if the node is complety pure then it would =0. In skearn it uses the CART algorithm, this works by splitting the data set into 2 subsets using a single feature and a threshold, it then continues to do this until the max depth is reached or a split will not reduce the impurity. To use this for regression it is very similar, however instead of predicting a class to assign to it predicts a value. You have to be careful though as not setting a maximum depth would normally result in overfitting.","68823141":"### Evaluating the model\nThere are a number of evaluation tools for regression. These are\n* Mean Absolute Error: It is the mean of the absolute value of the errors. This is the easiest of the metrics to understand since it\u2019s just average error.\n* Mean Squared Error (MSE): Mean Squared Error (MSE) is the mean of the squared error. It\u2019s more popular than Mean Absolute Error because the focus is geared more towards large errors. This is due to the squared term exponentially increasing larger errors in comparison to smaller ones.\n* Root Mean Squared Error (RMSE). RMSD is the square root of the average of squared errors. The effect of each error on RMSD is proportional to the size of the squared error; thus larger errors have a disproportionately large effect on RMSD. Consequently, RMSD is sensitive to outliers\n* R-squared is not an error, but rather a popular metric to measure the performance of your regression model. It represents how close the data points are to the fitted regression line. The higher the R-squared value, the better the model fits your data. The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse).","cdc21b13":"The speed of the processor can be extracted and used. The screen resolution can also be extracted. However the storage size will be harder to use, this is because of the many different types of storage will effect the price in a different way.","712ab37f":"### Linear regression\nWhen trying to fit a LinearRegression model you are trying to minise the cost function which is the mean square error. There are 2 main ways of doing this mathmatically. These are the normal equation and Singular Value Decompostion, with the implementation in sklearn using SVD. These however have the problem that they scale porely with large data sets.            \nWhen the data set is large you can train the model instead using gradient decent. The basic idea of this is to tweak the feature of the cost function iteratively to minimize the cost function. The anology that is often given is if you are on a mountian and trying to find the way down, you could look at the steepness of the slope and go where the steepness is greatest. The most important parameter of this method is the learning rate, this is because a small value will take a long time to converge to the correct value. While a large one the model would bounce around the optimal soluation making the model diverge. Another problem with this method is if the cost function is not a nice regular bowl. This could lead to the soluation actually being a local minumum rather than a global minumum. However this isnt a problem with the MSE cost function as it happens to be a convex function. This means that if you pick 2 point on the curve the line segment joining them would never cross the curve.             \nIf you use the whole training set for this then it is known as batch gradient decent, this however has the problem of taking a long time with large data sets. To try and solve this problem we can use Stochastic gradient descent. In this you pick a random instance from training set and calulate the gradiants based only on that instance. This increases the speed of the calculation as it only does it on a small amount of data. However due to the randomness of this procress the cost function will bounce around and will never settle at the global miniumum, this however can sometimes help if the cost function has local minimums it can bounce out of it. The method is impletmented in SGDRegressor.               \nYou can also have a mini-batch gradient decent, this computes of the cost function on a small random subset of the overall data.       \nSometimes when we make a model it will overfit the data. A good way to reduce this is to regularize the model (constrain it). For a linear model the normal way is to constrain the weights of the model. The first one I will try in this is ridge regression, in this a term is added to the cost function to keep the model weights as small as possible. They hyperparameters alpha controles how much you want this model to regularize, =0 is just linear regression and =1 means all the weights will be close to 0.              ","5cdcd7a5":"## Modeling the data\nAs there is not a huge amount of data and we have already selected the variables that we are interested in we will start by using ridge regression.","00939e83":"\nThe first thing to do is clean the data set to make it useable","35dbbc10":"### XGBoost and gradient boosting\nThis works in a very similar was as adaboost but trys to fit to the residual erros made by the previous predictor.\nA optimerized version of this algorithm exists in xgboost.","57aa2e18":"First was to remove the ID number, its not needed the pandas ID works the same. The check if there is any missing values before checking what type each column is. The weight and RAM needs to be changed to be a number.","52fe075e":"These are the easiest to convert, now I will look at some more of the categorical variable. For instance there is different types of memory which changes the Speed.","cc50a3b7":"### Adaboost\nUsing adaboost is when the next training instance pays more atention to previous training instances where it underfitted. This will keep going focussing on the harder to fit cases. For example when the first algorithm trains it will have some misslassified training instances, on the next instance it will increase the weight of the missclassified instances and then trains again. This will produces the final model.","e59d040e":"# Predicting the laptop price","41a7f198":"## Preparing the data for modeling\nOne of the first things to do is check for which values have a correlation."}}