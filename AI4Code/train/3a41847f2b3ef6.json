{"cell_type":{"532f2947":"code","59b65251":"code","d80e16e1":"code","598b5a58":"code","e60b36b8":"code","ad8217bd":"code","bf7f75ae":"code","2c7175a5":"markdown","7efce331":"markdown","3679f440":"markdown","7f269de5":"markdown","0cb7f273":"markdown","64163ed4":"markdown","eff437e4":"markdown","0b58955f":"markdown"},"source":{"532f2947":"# Preparing the data\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom imp import reload\nimport utility\n\nu = utility.Utility('\/kaggle\/input\/jane-street-market-prediction\/')\ntrain = pd.read_csv(u.filepath_train(), nrows=int(1e6))\ntrain = u.add_intraday_ts(train)\ntrain_na = u.build_train_na(train)","59b65251":"train_na = train_na[train_na['date'] == 12]\n\nquery = 'tag_6' \ngroups_def = {\n    'group_0': u.get_features(query + ' & tag_0'),\n    'group_1': u.get_features(query + ' & tag_1'),\n    'group_2': u.get_features(query + ' & tag_2'),\n    'group_3': u.get_features(query + ' & tag_3'),\n    'group_4': u.get_features(query + ' & tag_4'),\n}\nfor g in groups_def.keys(): \n    train_na[g] = train_na[groups_def[g]].mean(axis=1)\n\nfig, axs = plt.subplots(5, 1, sharex=True, sharey=True, figsize=(10, 5))\naxs[0].set_title('NA Clustering tag_6 + {tag_0, tag_1, tag_2, tag_3, tag_4}')\nfor i, g in enumerate(groups_def.keys()): \n    axs[i].plot(train_na['intraday_ts'], train_na[g], label=g)\n    axs[i].set_ylabel(g)\nplt.xlabel('Intraday Time')\nplt.show()\n\n\nquery = 'tag_23' \ngroups_def = {\n    'group_0': u.get_features(query + ' & tag_0'),\n    'group_1': u.get_features(query + ' & tag_1'),\n    'group_2': u.get_features(query + ' & tag_2'),\n    'group_3': u.get_features(query + ' & tag_3'),\n    'group_4': u.get_features(query + ' & tag_4'),\n}\nfor g in groups_def.keys(): \n    train_na[g] = train_na[groups_def[g]].mean(axis=1)\n\nfig, axs = plt.subplots(5, 1, sharex=True, sharey=True, figsize=(10, 5))\naxs[0].set_title('NA Clustering tag_23 + {tag_0, tag_1, tag_2, tag_3, tag_4}')\nfor i, g in enumerate(groups_def.keys()): \n    axs[i].plot(train_na['intraday_ts'], train_na[g], label=g)\n    axs[i].set_ylabel(g)\nplt.xlabel('Intraday Time')\nplt.show()","d80e16e1":"u.select_tags('tag_23 & tag_15 & tag_26')","598b5a58":"train_na = u.build_train_na(train)\nquery = 'tag_23' \ngroups = ['group_0', 'group_1', 'group_2', 'group_3', 'group_4']\ngroups_def = {\n    'group_0': u.get_features(query + ' & tag_0'),\n    'group_1': u.get_features(query + ' & tag_1'),\n    'group_2': u.get_features(query + ' & tag_2'),\n    'group_3': u.get_features(query + ' & tag_3'),\n    'group_4': u.get_features(query + ' & tag_4'),\n}\nfor g in groups_def.keys(): \n    train_na[g] = train_na[groups_def[g]].mean(axis=1)\n\ntrain_na = train_na[train_na['intraday_ts'] > 0.4][['date', 'intraday_ts'] + groups]\ndef bound_ts_nan(df, g): \n    x =  df[df[g] == 1]\n    return x['intraday_ts'].min(), x['intraday_ts'].max()\n\nna_summary = []\nfor date in train_na['date'].unique():\n    train_na_date = train_na[(train_na['date'] == date)]\n    this_date_data = {'date': date}\n    for g in groups: \n        bounds = bound_ts_nan(train_na_date, g)\n        this_date_data[f'{g}_min'] = bounds[0]\n        this_date_data[f'{g}_max'] = bounds[1]\n    na_summary.append(this_date_data)\nna_summary = pd.DataFrame(na_summary)\nfor g in groups: \n    na_summary[f'{g}_size'] = na_summary[f'{g}_max'] - na_summary[f'{g}_min'] \n    \nplt.figure(figsize=(10, 5))\nfor g in groups: \n    plt.plot(na_summary['date'], na_summary[f'{g}_size'] * 100, label=g)\nplt.legend()\nplt.ylabel('% Time of day')\nplt.xlabel('Date')\nplt.title('Lag Size')\nplt.show()","e60b36b8":"for g in groups: \n    mean_size = (na_summary[f'{g}_size'] * 100).mean()\n    print(f'{g} \\t % of day lag: {mean_size: .2f}')","ad8217bd":"train_date = train[train['date'] == 12]\nstocks = u.add_stock_id(train_date)\n\n# Pick the most frequent stock\nstock_id = stocks.groupby('stock_id').agg({'ts_id': 'count'}).sort_values('ts_id', ascending=False).index[0]\nstock = stocks[stocks['stock_id'] == stock_id]\n\n# Features to plot: \nfeatures_to_plot = u.get_features('tag_23 & tag_15 & tag_26')[:5]\ntags = ['tag_4', 'tag_0', 'tag_3', 'tag_2', 'tag_1']\nplt.figure(figsize=(10, 5))\nfor f, t in zip(features_to_plot, tags): \n    plt.scatter(\n        stock['intraday_ts'], \n        stock[f], \n        label=f'{t}: {f}', \n        marker='.',    )\nplt.legend()\nplt.xlabel('Time of day')\nplt.show()","bf7f75ae":"train_date = train[train['date'] == 12]\nstocks = u.add_stock_id(train_date)\n\n# Pick the most frequent stock\nstock_id = stocks.groupby('stock_id').agg({'ts_id': 'count'}).sort_values('ts_id', ascending=False).index[0]\nstock = stocks[stocks['stock_id'] == stock_id]\n\n# Features to plot: \nf = u.get_features('tag_23 & tag_15 & tag_26')[5]\nt = 'tag_5'\nplt.figure(figsize=(10, 5))\nplt.scatter(\n    stock['intraday_ts'], \n    stock[f], \n    label=f'{t}: {f}', \n    marker='.',    )\nplt.legend()\nplt.xlabel('Time of day')\nplt.show()","2c7175a5":"# Conclusion\n\nI hope there is enough evidence to show that the five first tags refer to time aggreation methods, with different characteristic times. \n\nIf the conclusions of this notebook are agreed by others, I will show more de-anonymization in another notebook. My current hypotheses are: \n- `tag_14` is a stock embedding, so is `tag_18`\n- `tag_6` is a price\n- `tag_23` is a volume \/ number of trades \/ quantity\n- `tag_20` is a spread\n\n\nLet me know your thoughts ! ","7efce331":"## Clusters and Intraday distributions of NaNs\n\nSome notebooks already saw that the NaN value had an intraday pattern. [NaN values depending on time of day](https:\/\/www.kaggle.com\/tomwarrens\/nan-values-depending-on-time-of-day). Other saw that they clustered with features (i.e. some features are NaNs together) [NaN Cluster](https:\/\/www.kaggle.com\/samir95\/features-nan-clusters-and-pca). \n\n\nI dug into those ideas to find the meaning of the first five tags. Here are the steps I took: \n- Cluster the features according to their \"NaN similarity\". (not shown here, just SVD + KMeans + Manual data viz) \n- For each cluster, plot throughout a day, the NaN indicator. \n- Make sense of the clusters of features with the `features.csv` file. \n\n\nThe cluster are governed by the 5 first tags of the `features.csv` file. To prove this, I grouped the features into 5 different groups: \n- `tag_23` and `tag_0` true \n- `tag_23` and `tag_1` true\n- `tag_23` and `tag_2` true\n- `tag_23` and `tag_3` true\n- `tag_23` and `tag_4` true\n\n`tag_23` is used as a filter to have underlying hidden features of the same category. \n`tag_6` could work as well. \nOr even no other filtering that `tag_{0, 1, 2, 3, 4}`\n\n\nBelow are the results for a date chosen randomly (12): ","3679f440":"By averaging over those dates, we find: ","7f269de5":"## More evidence \n\nTo further prove my statement, I am displaying such an underlying variable with the different time aggregation for a given stock, for a given day. \nYou can see the smoothing effect of the tags 0->4. \n\nSee my code to understand how I am able to isolate one financial instrument. I will try to post a notebook on this later. ","0cb7f273":"## And tag_5 ??\n\n`tag_5` appears for some underlying features, but not all. In short appears for features related to `tag_23`. \nAnd it seems to play a similar role as tags 0->4. Yet, it's slightly different. For me, `tag_5` means a cumsum of an underlying variable.\n\nIt could be also written again as a convolution, with $w$ being non-zero everywhere. \n\n$$ f(t) = \\sum_{s<t} w(t-s) u(s)$$\n\nOn the same example as before: ","64163ed4":"##### What do we observe?\nFrom those graphs, we can see that there are NaN twice a day. At the open, and around \"mid-day\". Furthermore, we can see that the \"duration\" of the NaN window depends on the group. \n\nThose results are true for all the dates. \n\n##### How can we explan this? \nFinancial markets are scheduled. Some events occur at pre-arranged times: open and close of course, but others throughout the day. Some markets have mid-day auctions, some markets even have a lunch break. \n\nIt seems that the \"open event\" and the \"lunch event\" causes an underlying metric to become NaN for a short period of time. For example if the underlying metric is a price, there is no price during auction. This NaN is then persited in our dataset features if the feature construction uses a rolling window. \n\nIf we call $f$ our feature (from the dataset) and $u$ an underlying metric, non shown in our dataset, we have the convolution with a weight $w$:\n$$ f(t) = \\sum_{s > t - \\tau} w(t-s) u(s)$$\nIf there is a NaN in $u(s)$ between $t-\\tau$ and $t$, there will be a NaN in $f(t)$. \n\nIt explains the time distribution of NaN and the features being clustered with NaN. \nThe time window of a NaN period will be the value $\\tau$. \n\n\n##### What is $w$, $\\tau$ and $u$ ? \n\n$w$ can be anything, the constant value 1 will give you plain rolling average, an exponential function will give you a exponential moving average, etc... \n$\\tau$ is the characteristic time of the rolling mechanism. \n\n$u$ is an underlying metric not shown in the dataset. It can be for example the price of the stock, or the number of trades. \n\nA feature (in our dataset) is defined by this rolling mechanism on an underlying metric. The underlying metric will be defined by other tags of the features. \n\nSee the table below for an example. The underlying (hidden) metric is defined by \"tag_23 and tag_15 and tag_26\". \nFor the sake of the example, one could imagine the meaning:  \n- \"tag_23\" -> number of trades\n- \"tag_15\" -> buyer\n- \"tag_26\" -> trade quantity > 100\n\nThen to complete the definition of the feature, you add one of the tags (0, 1, 2, 3, 4, 5). \n\nTherefore, we would have: \n- `feature_108` is rolling sum over the last 30 min of the number of trades made by a buyer on this exchange with size larger than 100. \n- `feature_109` is rolling sum over the last 20 sec of the number of trades made by a buyer on this exchange with size larger than 100. \n- etc\n\nOf course, those values are then re-normalized. ","eff437e4":"## Characteristic Time $\\tau$ of the rolling metrics\n\nNow that we know that the tags refer to those rolling window, we can try to quantify this characteristic $\\tau$ for each tag (0, 1, 2, 3, 4). \n\nWe keep the grouping of features. Then, for each day, we look only a the mid-day NaN period. We compute the \"length\" of the period (in terms of % of the trading day). ","0b58955f":"# De-anonymization: Time Aggregation Tags\n\nAfter some very poor submssions and a lot of timeouts, I decided to focus on de-anonymizing the data. \n\nHere, I show the meaning of: `tag_{0, 1, 2, 3, 4, 5}`.\n\n## Results\n\n\n| Tag      | Aggregation Type | Characteristic Time (% of day) | Characteristic Time in min\n| ----------- | ----------- | ----------- | ----------- |\n| `tag_0`      | Rolling Sum\/Average       | 0.03 | ~ 20 sec\n| `tag_1`      | Rolling Sum\/Average       | 0.27 | ~ 3 min\n| `tag_2`      | Rolling Sum\/Average       | 0.59 | ~ 5 min\n| `tag_3`      | Rolling Sum\/Average       | 1.33 | ~ 10 min\n| `tag_4`      | Rolling Sum\/Average       | 4.55 | ~ 30 min\n| `tag_5`      | Cumulative throughout the day        | | \n\n\n## Pre-requisite \n\nThis notebook will use other knowledge about the tags without showing how I got those results. What you need to know: \n- `tag_6` and `tag_23` are key tags that group features in 2 different groups. \n  - `tag_6` means a price feature\n  - `tag_23` means an \"additive\" feature (either a number of trades or volume trade, still not sure). \n- For a given day, grouping trades made on the same instrument is possible. One way to do this is to look at `feature_{41, 42, 43, 44, 45}`. They are constant for a day \/ stock combination. \n  - `41, 42, 43` are probably an embedding (PCA on returns?) of the different stocks. \n  - `44, 45` might be related to volumes? "}}