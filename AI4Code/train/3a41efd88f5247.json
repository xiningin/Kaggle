{"cell_type":{"8a6d8565":"code","15335fce":"code","32fb9669":"code","edafe89f":"code","fae29571":"code","9cab7628":"code","3a743519":"code","0852ccb5":"code","8880ee63":"code","b457dde3":"code","97a8490e":"code","b455af0e":"code","3eb6349b":"code","45d9982c":"code","729752b6":"code","c1fc5ef4":"code","9a7a89ef":"code","c631c134":"code","ebfd0c43":"code","e9d19af2":"code","44384dd4":"code","b55b36dc":"code","a3e2e542":"code","8d21691b":"code","3a86130b":"code","efbabba0":"code","9c648cf7":"code","4072d305":"code","ba0e29ec":"code","d078602b":"code","a9bd6c4f":"markdown","de6f67c9":"markdown","11272118":"markdown"},"source":{"8a6d8565":"import os\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom keras.utils import np_utils, plot_model\nimport datetime\nimport sklearn\nimport scikitplot\nfrom keras.callbacks import ModelCheckpoint, CSVLogger, TensorBoard, EarlyStopping, ReduceLROnPlateau","15335fce":"TRAIN_PATH= '..\/input\/fer2013\/train\/'\nTEST_PATH = '..\/input\/fer2013\/test\/'","32fb9669":"one_image = cv2.imread('..\/input\/fer2013\/train\/angry\/Training_10118481.jpg')\none_image.shape","edafe89f":"\n\ndef count_exp(path, set_):\n    dict_ = {}\n    for expression in os.listdir(path):\n        dir_ = path + expression\n        dict_[expression] = len(os.listdir(dir_))\n    df = pd.DataFrame(dict_, index=[set_])\n    return df\n            \n            \n            \ntrain_count = count_exp(TRAIN_PATH, 'train')\ntest_count = count_exp(TEST_PATH, 'test')\nprint(train_count)\nprint(test_count)\n    ","fae29571":"def count_image(data_path):\n    total_images = 0\n    for directory in os.listdir(data_path):\n        count =0\n        for f in os.listdir(data_path + directory + '\/'):\n            count +=1\n            total_images +=1\n            \n        print(f\"{directory} has {count} number images\")\n        \n    print(f\"Total number images is {total_images}\")\n    \n            ","9cab7628":"count_image(TRAIN_PATH)","3a743519":"count_image(TEST_PATH)","0852ccb5":"EMOTIONS = ['surprise', 'fear', 'angry', 'neutral', 'sad', 'disgust', 'happy']\n\ntrain_total_image  = 28709\ntest_total_image = 7178","8880ee63":"def image_to_array(total_image, data_path):\n    image_array = np.empty(shape=(total_image, 48, 48, 3))\n    image_label = np.empty(shape=(total_image))\n    label_to_text= {}\n    \n    \n    i =0\n    e= 0\n    \n    for directory in os.listdir(data_path):\n        if directory in EMOTIONS:\n            label_to_text[e] = directory\n            \n            for f in os.listdir(data_path + directory + \"\/\"):\n                image_array[i] = cv2.imread(data_path + directory + \"\/\" +f)\n                image_label[i]= e\n                i +=1\n                \n            print(f\"All images converted to array of {directory} directory\")\n            \n            e +=1\n            \n    return image_array, image_label, label_to_text\n            ","b457dde3":"train_image_array, train_image_label, train_label_to_text = image_to_array(train_total_image, TRAIN_PATH)","97a8490e":"print(train_image_array)\nprint(\"\\n\", train_image_label)\nprint(\"\\n\", train_label_to_text)","b455af0e":"test_image_array, test_image_label, test_label_to_text = image_to_array(test_total_image, TEST_PATH)\n\nprint(test_image_array)\nprint(\"\\n\", test_image_label)\nprint(\"\\n\", test_label_to_text)","3eb6349b":"def show_some_image(label_to_text, image_array, image_label):\n    fig = plt.figure(1,(14, 14))\n    \n    idx = 0\n    \n    for k in label_to_text:\n        indices = np.random.choice(np.where(image_label==k)[0], size=4, replace=False)\n        \n        sample_images = image_array[indices]\n        \n        for img in sample_images:\n            idx += 1\n            ax = plt.subplot(7,7,idx)\n            ax.imshow(img[:,:,0], cmap='gray')\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_title(label_to_text[k])\n            plt.tight_layout()","45d9982c":"show_some_image(train_label_to_text, train_image_array, train_image_label)","729752b6":"show_some_image(test_label_to_text, test_image_array, test_image_label)","c1fc5ef4":"train_image_label_cat = np_utils.to_categorical(train_image_label)\ntrain_image_label_cat","9a7a89ef":"test_image_label_cat = np_utils.to_categorical(test_image_label)\ntest_image_label_cat","c631c134":"train_image_array_resize = train_image_array\/ 255.0\ntest_image_array_resize = test_image_array\/ 255.0","ebfd0c43":"from sklearn.model_selection import train_test_split\nimport keras","e9d19af2":"x_train, x_val, y_train, y_val = train_test_split(train_image_array_resize, train_image_label_cat, stratify = train_image_label_cat, shuffle =True, test_size=0.2, random_state= 42)","44384dd4":"NUM_CLASSES = 7\n\n\ndef xception_model():\n    \n    model=  tf.keras.Sequential()\n    \n    model.add(tf.keras.applications.Xception(input_shape=(71, 71,3),\n                                             include_top = False,\n                                            weights=\"imagenet\", classes=  NUM_CLASSES))\n    model.add(tf.keras.layers.GlobalAveragePooling2D())\n    model.add(tf.keras.layers.Dense(7, activation='softmax'))\n    opt = tf.keras.optimizers.Adam(lr=0.0001 , decay=1e-6)\n    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model\n    ","b55b36dc":"xception = xception_model()\nxception.summary()","a3e2e542":"plot_model(xception, to_file='model_1.png', show_shapes=True, show_layer_names=True)","8d21691b":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\ntrain_datagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.15,\n    height_shift_range=0.15,\n    shear_range=0.15,\n    zoom_range=0.15,\n    horizontal_flip=True,\n    zca_whitening=False,\n)\ntrain_datagen.fit(x_train)","3a86130b":"chk_path = 'model_1.h5'\nlog_dir = \"checkpoint\/logs\/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\ncheckpoint = ModelCheckpoint(filepath=chk_path,\n                             save_best_only=True,\n                             verbose=1,\n                             mode='min',\n                             moniter='val_loss')\n\nearlystop = EarlyStopping(monitor='val_loss', \n                          min_delta=0, \n                          patience=3, \n                          verbose=1, \n                          restore_best_weights=True)\n                        \nreduce_lr = ReduceLROnPlateau(monitor='val_loss', \n                              factor=0.2, \n                              patience=6, \n                              verbose=1, \n                              min_delta=0.0001)\n\n\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\ncsv_logger = CSVLogger('training.log')\n\ncallbacks = [checkpoint, reduce_lr, csv_logger]","efbabba0":"batch_size =256\nepochs=35\nhistory = xception.fit_generator(\n    train_datagen.flow(x_train, y_train, batch_size=batch_size),\n    validation_data=(x_val, y_val),\n    steps_per_epoch=len(x_train) \/ batch_size,\n    epochs=epochs,\n    callbacks=callbacks,\n    use_multiprocessing=False\n)","9c648cf7":"sns.set()\nfig = plt.figure(0, (12, 4))\n\nax = plt.subplot(1, 2, 1)\nsns.lineplot(history.epoch, history.history['accuracy'], label='train')\nsns.lineplot(history.epoch, history.history['val_accuracy'], label='valid')\nplt.title('Accuracy')\nplt.tight_layout()\n\nax = plt.subplot(1, 2, 2)\nsns.lineplot(history.epoch, history.history['loss'], label='train')\nsns.lineplot(history.epoch, history.history['val_loss'], label='valid')\nplt.title('Loss')\nplt.tight_layout()\n\nplt.savefig('epoch_history_mobilenet.png')\nplt.show()","4072d305":"train_txt_to_label = dict((v,k) for k,v in train_label_to_text.items())\ntest_txt_to_label = dict((v,k) for k,v in test_label_to_text.items())","ba0e29ec":"yhat_val = np.argmax(xception.predict(x_val), axis=1)\nyval = np.argmax(y_val, axis=1)\n\nscikitplot.metrics.plot_confusion_matrix(yval, yhat_val, figsize=(7,7))\nplt.savefig(\"confusion_matrix_mobilenet.png\")\n\ntest_accu = np.sum(yval == yhat_val) \/ len(yval) * 100\nprint(f\"test accuracy: {round(test_accu, 4)} %\\n\\n\")\n\nprint(sklearn.metrics.classification_report(yval, yhat_val))","d078602b":"final_predict = xception.predict(test_image_array_resize)\nfinal_predict","a9bd6c4f":"# ***Import necessary Libraries***","de6f67c9":"# **Convert image to array**","11272118":"# **Convert Image label to Categorical**"}}