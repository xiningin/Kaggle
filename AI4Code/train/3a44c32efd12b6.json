{"cell_type":{"a8b1b7e7":"code","0081ad2e":"code","d0c11eca":"code","ae1c69d9":"code","e6be29a5":"code","8aeaf7fe":"code","ee8e029f":"code","730a6079":"code","313ef959":"code","a0c47550":"code","e663d8d1":"code","88c8a3cb":"code","cbaf89d9":"code","f230c8d9":"code","2cc4393d":"code","a7e8f932":"code","6ceb699e":"code","760b4f6e":"code","d88cb3d9":"code","867fbb10":"code","19c7016e":"code","ca6d9b1f":"code","aa0ecac4":"code","288f808f":"code","94f0787c":"code","bd6820ab":"code","c4551e7d":"code","7db84277":"markdown","00750996":"markdown","387c17dc":"markdown","4ce7788b":"markdown","a03d77f5":"markdown","6a6647bf":"markdown","b81331b3":"markdown","09c08de9":"markdown","6331d6d6":"markdown","37ee8fbb":"markdown","29d9eab4":"markdown"},"source":{"a8b1b7e7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0081ad2e":"import warnings\nimport seaborn as sns\nimport matplotlib.pylab as plt\nimport PIL\nfrom keras import backend as K\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications import Xception\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom keras import layers, models, optimizers\n\nwarnings.filterwarnings('ignore')\n\nimage_size = 299\napplication = Xception\nbatch_size = 32","d0c11eca":"DATA_PATH = '..\/input'\nTRAIN_IMG_PATH = os.path.join(DATA_PATH, 'train')\nTEST_IMG_PATH = os.path.join(DATA_PATH, 'test')\n\ndf_train = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\ndf_test = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\ndf_class = pd.read_csv(os.path.join(DATA_PATH, 'class.csv'))","ae1c69d9":"df_train.head()","e6be29a5":"plt.figure(figsize=(15,6))\nsns.countplot('class', data=df_train)","8aeaf7fe":"df_train['class'].value_counts()","ee8e029f":"df_train['class'].value_counts().mean()","730a6079":"df_train['class'].value_counts().describe()","313ef959":"def crop_boxing_img(img_name, margin=16, size=(image_size, image_size)):\n    if img_name.split('_')[0] == 'train':\n        PATH = TRAIN_IMG_PATH\n        data = df_train\n    else:\n        PATH = TEST_IMG_PATH\n        data = df_test\n\n    img = PIL.Image.open(os.path.join(PATH, img_name))\n    pos = data.loc[data[\"img_file\"] == img_name, ['bbox_x1', 'bbox_y1', 'bbox_x2', 'bbox_y2']].values.reshape(-1)\n\n    width, height = img.size\n    x1 = max(0, pos[0] - margin)\n    y1 = max(0, pos[1] - margin)\n    x2 = min(pos[2] + margin, width)\n    y2 = min(pos[3] + margin, height)\n\n    return img.crop((x1, y1, x2, y2)).resize(size)","a0c47550":"nb_train_sample = df_train.shape[0] * 0.7\nnb_validation_sample = df_train.shape[0] - nb_train_sample\nnb_test_sample = df_test.shape[0]","e663d8d1":"TRAIN_CROPPED_PATH = '..\/cropped_train'\nVALID_CROPPED_PATH = '..\/cropped_valid'\nTEST_CROPPED_PATH = '..\/cropped_test'","88c8a3cb":"if (os.path.isdir(TRAIN_CROPPED_PATH) == False):\n    os.mkdir(TRAIN_CROPPED_PATH)\n\nif (os.path.isdir(VALID_CROPPED_PATH) == False):\n    os.mkdir(VALID_CROPPED_PATH)\n\nif (os.path.isdir(TEST_CROPPED_PATH) == False):\n    os.mkdir(TEST_CROPPED_PATH)","cbaf89d9":"df_train['class'] = df_train['class'].astype('str')","f230c8d9":"for i, row in df_train.iterrows():\n    cropped = crop_boxing_img(row['img_file'])\n    \n    if ( i < nb_train_sample):\n        class_path = os.path.join(TRAIN_CROPPED_PATH, df_train['class'][i])\n        if(os.path.isdir(class_path) == False):\n            os.mkdir(class_path)\n\n        cropped.save(os.path.join(class_path, row['img_file']))\n    else:\n        class_path = os.path.join(VALID_CROPPED_PATH, df_train['class'][i])\n        if(os.path.isdir(class_path) == False):\n            os.mkdir(class_path)\n\n        cropped.save(os.path.join(class_path, row['img_file']))\n\nfor i, row in df_test.iterrows():\n    cropped = crop_boxing_img(row['img_file'])\n    cropped.save(os.path.join(TEST_CROPPED_PATH, row['img_file']))","2cc4393d":"train_datagen = ImageDataGenerator(\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True,\n    vertical_flip=False,\n    zoom_range=0.1,\n    fill_mode='nearest'\n    )\n\nvalid_datagen = ImageDataGenerator()\ntest_datagen = ImageDataGenerator()","a7e8f932":"train_generator = train_datagen.flow_from_directory(\n    TRAIN_CROPPED_PATH,\n    target_size=(image_size, image_size),\n    batch_size=batch_size,\n    class_mode='categorical',\n    seed=2019,\n    color_mode='rgb'\n)","6ceb699e":"validation_generator = valid_datagen.flow_from_directory(\n    VALID_CROPPED_PATH,\n    target_size=(image_size,image_size),\n    batch_size=batch_size,\n    class_mode='categorical',\n    seed=2019,\n    color_mode='rgb'\n)","760b4f6e":"test_generator = test_datagen.flow_from_dataframe(\n    dataframe=df_test,\n    directory=TEST_CROPPED_PATH,\n    x_col='img_file',\n    y_col=None,\n    target_size= (image_size,image_size),\n    color_mode='rgb',\n    class_mode=None,\n    batch_size=batch_size,\n    shuffle=False\n)","d88cb3d9":"def get_model():\n    base_model = application(weights='imagenet', input_shape=(image_size,image_size,3), include_top=False)\n    #base_model.trainable = False\n\n    model = models.Sequential()\n    model.add(base_model)\n    model.add(layers.GlobalAveragePooling2D())\n    model.add(layers.Dense(1024, activation='relu'))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(196, activation='softmax'))\n    model.summary()\n\n    #optimizer = optimizers.SGD(lr=1e-4, decay=1e-6, momentum=0.9, nesterov=True)\n    optimizer = optimizers.RMSprop(lr=0.0001)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n\n    return model","867fbb10":"model = get_model()","19c7016e":"model_path = '..\/model\/'\nif not os.path.exists(model_path):\n    os.mkdir(model_path)\n    \nmodel_path = model_path + 'best_model.hdf5'","ca6d9b1f":"patient = 2\ncallbacks1 = [\n    EarlyStopping(monitor='val_loss', patience=patient, mode='min', verbose=1),\n    ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, patience = patient \/ 2, min_lr=0.00001, verbose=1, mode='min'),\n    ModelCheckpoint(filepath=model_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min'),\n    ]","aa0ecac4":"def get_steps(num_samples, batch_size):\n    if (num_samples % batch_size) > 0:\n        return (num_samples \/\/ batch_size) + 1\n    else:\n        return num_samples \/\/ batch_size","288f808f":"history = model.fit_generator(\n    train_generator,\n    steps_per_epoch=get_steps(nb_train_sample, batch_size),\n    epochs=100,\n    validation_data=validation_generator,\n    validation_steps=get_steps(nb_validation_sample, batch_size),\n    verbose=1,\n    callbacks = callbacks1\n)","94f0787c":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training Acc')\nplt.plot(epochs, val_acc, 'b', label='Validation Acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(epochs, loss, 'bo', label='Traing loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Trainging and validation loss')\nplt.legend()\nplt.show()","bd6820ab":"model.load_weights(model_path)\ntest_generator.reset()\n\nprediction = model.predict_generator(\n    generator=test_generator,\n    steps = get_steps(nb_test_sample, batch_size),\n    verbose=1\n)","c4551e7d":"\npredicted_class_indices=np.argmax(prediction, axis=1)\n\n# Generator class dictionary mapping\nlabels = (train_generator.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npredictions = [labels[k] for k in predicted_class_indices]\n\nsubmission = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'))\nsubmission[\"class\"] = predictions\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()","7db84277":"\uc798\ub77c\ub0b8 \uc0ac\uc9c4\ub4e4\uc744 \ub514\ub809\ud1a0\ub9ac\uc5d0 \uac01 \ud074\ub798\uc2a4\ubcc4\ub85c \uc800\uc7a5\uc744 \ud569\ub2c8\ub2e4.\n\uc800\uc7a5\ud560\ub54c \ud6c8\ub828\ub370\uc774\ud130 \ub514\ub809\ud1a0\ub9ac\uc640 \uac80\uc99d \ub514\ub809\ud1a0\ub9ac, \ud14c\uc2a4\ud2b8\uc6a9 \ub514\ub809\ud130\ub9ac \ubd84\ub958\ud558\uc5ec \uc800\uc7a5\ud569\ub2c8\ub2e4.\n\ud6c8\ub828\uc6a9\uacfc \uac80\uc99d\uc6a9 \uac1c\uc218\ub294 7:3 \uc785\ub2c8\ub2e4.\n\n\ub514\ub809\ud1a0\ub9ac\uc5d0 \uc800\uc7a5\ud55c \uc774\uc720\ub294 \uc774\ubbf8\uc9c0 \ud30c\uc77c\uc744 PC\uc5d0\uc11c \ubc14\ub85c \uc5f4\uc5b4\ubcf4\uae30 \ud3b8\ud558\uae30 \ub54c\ubb38\uc774\uace0, \ub2e4\ub978 \ub73b \uc740 \uc5c6\uc2b5\ub2c8\ub2e4.","00750996":"\ud074\ub798\uc2a4\ubcc4 \ub514\ub809\ud1a0\ub9ac\ub97c \ub9cc\ub4e4\uae30 \uc704\ud574\uc11c class \ud615\uc744 \ubb38\uc790\uc5f4\ub85c \ubcc0\uacbd","387c17dc":"\ud6c8\ub828\ud558\uba74\uc11c \uac80\uc99d\ub370\uc774\ud130\uc5d0 \ub300\ud574\uc11c val_loss\uac00 \uac00\uc7a5 \uc791\uc740 \uacbd\uc6b0\uc5d0 \ub300\ud574\uc11c weight \uc800\uc7a5.\n\uc774 \ud30c\uc77c\uba85\uc744 \uc798 \uad00\ub9ac\ud574 \ub450\uc5c8\ub2e4\uac00 \ub098\uc911\uc5d0 \uc559\uc0c1\ube14\uc5d0 \uc774\uc6a9\ud560 \uc608\uc815\uc784.\nbest_model.hd5\ub85c \uc800\uc7a5\uc744 \ud558\ub2e4\uac00 \ud6c8\ub828\uc774 \ub05d\ub098\uba74 best_model_(\uc0ac\uc804\ud6c8\ub828\ucee4\ube0c\ub137).hd5\ub85c \uc800\uc7a5.","4ce7788b":"keras\uc758 ImageDataGenerator\ub97c \uc774\uc6a9\ud558\uc5ec \ub370\uc774\ud130 \uc99d\uc2dd\uc744 \ud569\ub2c8\ub2e4.\n - rotation_range : \ub79c\ub364\ud558\uac8c \uc0ac\uc9c4\uc744 \ud68c\uc804\uc2dc\ud0ac \uac01\ub3c4 \ubc94\uc704\n - width_shift_range : \uc0ac\uc9c4 \uc218\ud3c9\uc774\ub3d9 \ube44\uc728\n - height_shift_range : \uc0ac\uc9c4 \uc218\uc9c1\uc774\ub3d9 \ube44\uc728\n - horizontal_flip : \uc0ac\uc9c4 \uc218\ud3c9\uc73c\ub85c \ub4a4\uc9d1\uae30\n - zoom_range : \ub79c\ub364\ud558\uac8c \uc0ac\uc9c4 \ud655\ub300\n - fill_mode : \uc0ac\uc9c4\uc744 \ud68c\uc804\/\uc774\ub3d9\ud55c\ud6c4\uc5d0 \uc0c8\ub86d\uac8c \uc0dd\uc131\ud574\uc57c\ud560 \ud53d\uc140\uc744 \ucc44\uc6b8 \ubc29\ubc95","a03d77f5":"119\ubc88 \uc720\ud615\uc774 84\uac1c\ub85c \uac00\uc7a5 \ub9ce\uace0, 136\ubc88 \uc720\ud615\uc774 30\uac1c\ub85c \uac00\uc7a5 \uc801\uc74c.","6a6647bf":"\uc544\ub798 \ubd80\ubd84\uc740 \ub2e4\ub978 \ubd84\ub4e4\uc758 \ucee4\ub110\uc744 \ucc38\uace0\ud558\uc5ec\uc11c \ubc88\uc601 \ud558\uc600\uc2b5\ub2c8\ub2e4.\n\uae40\ud0dc\uc9c4\ub2d8, \ud5c8\ud0dc\uba85\ub2d8, Daehun Gwak\ub2d8\uc758 \ucee4\ub110\uc744 \ucc38\uace0 \ud588\uc2b5\ub2c8\ub2e4.\n\n[Daehun Gwak\ub2d8 \ucee4\ub110](https:\/\/www.kaggle.com\/daehungwak\/keras-how-to-use-pretrained-model)\n[\ud5c8\ud0dc\uba85\ub2d8 \ucee4\ub110](https:\/\/www.kaggle.com\/tmheo74\/3rd-ml-month-car-image-cropping)\n[\uae40\ud0dc\uc9c4\ub2d8 \ucee4\ub110](https:\/\/www.kaggle.com\/fulrose\/3rd-ml-month-car-model-classification-baseline)","b81331b3":"\uad50\ucc28\uac80\uc99d\uc740.... PC \uc790\uc6d0\uc758 \ubb38\uc81c\ub85c \uc5c4\ub450\ub97c \ubabb\ub0b4\uace0 \uc788\ub124\uc694. ","09c08de9":"\ud6c8\ub828 \uc911 val_loss\uac00 2\ud68c \uc5f0\uc18d \uac1c\uc120\uc774 \ub418\uc9c0 \uc54a\ub294\ub2e4\uba74 \uba48\ucd94\uae30 \uc704\ud574\uc11c.","6331d6d6":"\uba3c\uc800 \ub3c4\uc6c0\uc744 \uc8fc\uc2e0 \ud5c8\ud0dc\uba85\ub2d8, \uae40\ud0dc\uc9c4\ub2d8, Daehun Gwak\ub2d8 \uac10\uc0ac\ud569\ub2c8\ub2e4.\n\uc62c\ub824\uc8fc\uc2e0 \ucee4\ub110\uc744 \ucc38\uace0\ud558\uc5ec \uc190\uc27d\uac8c \uc811\uadfc\ud560 \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4.\n\n\uac10\uc0ac\ud569\ub2c8\ub2e4.","37ee8fbb":"\uac01 \ucc28\uc885\uc758 \ud3c9\uade0 \uac1c\uc218\ub294 51\uac1c\uc784.","29d9eab4":"\ud6c8\ub828\ub514\ub809\ud1a0\ub9ac \/ \uac80\uc99d \ub514\ub809\ud1a0\ub9ac \/ \ud14c\uc2a4\ud2b8 \ub514\ub809\ud1a0\ub9ac \uc0dd\uc131"}}