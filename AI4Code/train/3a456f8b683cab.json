{"cell_type":{"1f097450":"code","d64f29ff":"code","0646ce8c":"code","c4d7e1bd":"code","ea4fc3fa":"code","75917525":"code","51d25f57":"code","b9684fb6":"code","90b6ddb8":"code","b2e2c2e6":"code","8b4be589":"code","2eeef573":"code","4f5ee1e9":"code","2b020191":"code","38af499a":"code","95f22bce":"code","cf85f733":"code","a1e2362a":"code","3a388f96":"code","591d6711":"code","083a499b":"code","c50c090b":"code","04c4fa4b":"code","b577e953":"code","60a028f8":"code","80d31739":"code","d52295ba":"code","9d56da40":"code","13374e44":"code","7f84f118":"code","62a1dd32":"code","18120f39":"code","d6032ec5":"code","1583d31c":"code","4b812137":"code","b7159fd8":"code","856fe84d":"code","0477b926":"code","7f938cce":"code","05de8248":"code","f2f76464":"code","4f23e5fc":"code","975944fe":"markdown","66652c98":"markdown","5dc1929d":"markdown","99690383":"markdown","c9f01dc2":"markdown","26e555c2":"markdown","167d368e":"markdown","783c7828":"markdown","ce124416":"markdown","399efa8b":"markdown","3196f2c0":"markdown","15784756":"markdown","7bca01b8":"markdown","ccb9b516":"markdown","99793a8d":"markdown","ea8f9873":"markdown","f61cbe0a":"markdown","d8518205":"markdown","b33e5c82":"markdown","75378887":"markdown","d04c87a8":"markdown"},"source":{"1f097450":"import numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nfrom scipy.io import loadmat\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.ensemble import VotingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nimport warnings\nwarnings.filterwarnings(\"ignore\") ","d64f29ff":"## Setting base path\npath = \"..\/input\/utd-multimodal-human-action-dataset\"\nfiles = os.listdir(path)","0646ce8c":"## Function to get frame length\ndef findFrames(file):\n    action_dict = loadmat(\"..\/input\/utd-multimodal-human-action-dataset\/\"+file)\n    action = action_dict[\"d_skel\"]\n    return action.shape[2]","c4d7e1bd":"## Function to get the data array\ndef findData(file):\n     action_dict = loadmat(\"..\/input\/utd-multimodal-human-action-dataset\/\"+file)\n     action = action_dict[\"d_skel\"]\n     return action","ea4fc3fa":"## Finding min and max length of frames in all videos\nframes = []\nfor file in files:\n    if(file.endswith(\".mat\")) :\n        frame = findFrames(file)\n        frames.append(frame)\n    \nmin_frame_count = min(frames)\nmin_frame_file = files[frames.index(min_frame_count)]\n\nmax_frame_count = max(frames)\nmax_frame_file = files[frames.index(max_frame_count)]\n\nprint(\"Maximum Frame Count:\",max_frame_count)\nprint(\"Minimum Frame Count:\",min_frame_count)\n","75917525":"# S1,S3,S5,S7 - Training\n# S2,S4,S6,S8 - Testing\ntrainData=[]\ntestData=[]\nfor file in files:\n    if file.endswith('.mat'):\n        if \"s1\" in file or \"s3\" in file or \"s5\" in file or \"s7\" in file:\n            #Odd files for train set\n            trainData.append(file)\n        else:\n            #Even files for test set\n            testData.append(file) ","51d25f57":"# Getting train data and appending them in a list\ntrain_features=[]\nfor tr in trainData:\n     if(tr.endswith(\".mat\")) :\n        frame = findData(tr)\n        train_features.append(frame)\n    ","b9684fb6":"# Getting test data and appending them in a list\ntest_features=[]\nfor tr in testData:\n     if(tr.endswith(\".mat\")) :\n        frame = findData(tr)\n        test_features.append(frame)","90b6ddb8":"train_features","b2e2c2e6":"# Length of Action videos\nprint(\"Length of Train Videos    : \",len(train_features))\n# Shape of one Action Video \nprint(\"Shape of One Action Video : \",train_features[0].shape)","8b4be589":"# Slicing First Action Video\ntrain_features[0]","2eeef573":"# Slicing First Joint in Head Action Video\ntrain_features[0][0]","4f5ee1e9":"# Slicing X Coordinates in Head Joint in First Action Video\ntrain_features[0][0][0]","2b020191":"# Slicing First Frame X Coordinates in Head Joint in First Action Video\ntrain_features[0][0][0][0]","38af499a":"# This function will take first 40 frames and find temporal euclidean distance between them\ndef get_features_top_40_frames(features):\n    list1 = []\n    list2 = []\n    squared = 0\n    euclidean_distance = []\n\n    for video in range(len(features)):\n        for joint in range(features[0].shape[0]):\n            for frame in range(40):\n                for coordinate in features[video][joint]:\n                    list1.append(coordinate[frame])\n                    list2.append(coordinate[frame+1])\n\n    for i in range(0,len(list1),3):\n        squared += (list1[i]-list2[i])**2+(list1[i+1]-list2[i+1])**2+(list1[i+2]-list2[i+2])**2\n        euclidean_distance.append(np.sqrt(squared))\n    return euclidean_distance","95f22bce":"# Extracting Euclidean distance for all Features from test and train\ntrain_list = get_features_top_40_frames(train_features)\ntest_list = get_features_top_40_frames(test_features)","cf85f733":"# Reshaping feature array - Video as rows, temporal euclidean distance for all joints in 40 frames as columns\ntrain_array_reshaped= np.reshape(train_list,(431,800))\ntest_array_reshaped= np.reshape(test_list,(430,800))","a1e2362a":"# Converting array to a Dataframe\ndf_train_top_40_frames = pd.DataFrame(train_array_reshaped)\ndf_test_top_40_frames = pd.DataFrame(test_array_reshaped)","3a388f96":"# Extracting action label for each video using filenames\ndef send_actions(filenames):\n    action = []\n    for i in filenames:\n        action.append(i.split('_')[0])\n    return action","591d6711":"# Adding dependent variable in our data for modelling\ndf_train_top_40_frames[\"action\"] = send_actions(trainData)\ndf_test_top_40_frames[\"action\"] = send_actions(testData)","083a499b":"# Writing data into CSV files\ndf_train_top_40_frames.to_csv(\"df_train_top_40_frames.csv\",index=False)\ndf_test_top_40_frames.to_csv(\"df_test_top_40_frames.csv\",index=False)","c50c090b":"# This function will take create synthetic frames with equal ratio \ndef trim(video,minvalue):\n    ratio = (video.shape[2]\/(minvalue))\n    array = []\n    temp=0+ratio\n    round_ratio=int(temp)\n    while round_ratio+1 < video.shape[2]:\n        array.append(video[:,:,round_ratio] + ((video[:,:,round_ratio] - video[:,:,round_ratio+1])*(temp-round_ratio)))\n        temp+=ratio\n        round_ratio=int(temp)-1\n    return np.array(array)[:40].transpose(1,2,0)","04c4fa4b":"# Extracting synthetic Features from test and train\ntrain_synthetic_features = []\nfor i in range(len(train_features)):\n    train_synthetic_features.append(trim(train_features[i],41))\n\ntest_synthetic_features = []\nfor i in range(len(test_features)):\n    test_synthetic_features.append(trim(test_features[i],41)) ","b577e953":"# This function will use the synthetic frames and find temporal euclidean distance between them\ndef get_features_synthetic_frames(features):\n    list1 = []\n    list2 = []\n    squared = 0\n    euclidean_distance = []\n    for video in range(len(features)):\n        for joint in range(features[0].shape[0]):\n            for frame in range(features[0].shape[2]-1):\n                for coordinate in features[video][joint]:\n                    list1.append(coordinate[frame])\n                    list2.append(coordinate[frame+1])\n\n    for i in range(0,len(list1),3):\n        squared += (list1[i]-list2[i])**2+(list1[i+1]-list2[i+1])**2+(list1[i+2]-list2[i+2])**2\n        euclidean_distance.append(np.sqrt(squared))\n    return euclidean_distance","60a028f8":"# Extracting Euclidean Distances from Synthetic Features for test and train\ntrain_list = get_features_synthetic_frames(train_synthetic_features)\ntest_list  = get_features_synthetic_frames(test_synthetic_features)","80d31739":"# Reshaping feature array - Video as rows, temporal euclidean distance for all joints in 40 frames as columns\ntrain_array_reshaped= np.reshape(train_list,(431,780))\ntest_array_reshaped= np.reshape(test_list,(430,780))","d52295ba":"# Converting array to a Dataframe\ndf_train_synthetic_frames = pd.DataFrame(train_array_reshaped)\ndf_test_synthetic_frames = pd.DataFrame(test_array_reshaped)","9d56da40":"# Adding dependent variable in our data for modelling\ndf_train_synthetic_frames[\"action\"] = send_actions(trainData)\ndf_test_synthetic_frames[\"action\"] = send_actions(testData)","13374e44":"# Writing data into CSV files\ndf_train_synthetic_frames.to_csv(\"df_train_synthetic_frames.csv\",index=False)\ndf_test_synthetic_frames.to_csv(\"df_test_synthetic_frames.csv\",index=False)","7f84f118":"df_train_top_40_frames = pd.read_csv(\"..\/input\/utd-multimodal-human-action-dataset\/df_train_top_40_frames.csv\")\ndf_test_top_40_frames = pd.read_csv(\"..\/input\/utd-multimodal-human-action-dataset\/df_test_top_40_frames.csv\")\ndf_train_synthetic_frames = pd.read_csv(\"..\/input\/utd-multimodal-human-action-dataset\/df_train_synthetic_frames.csv\")\ndf_test_synthetic_frames = pd.read_csv(\"..\/input\/utd-multimodal-human-action-dataset\/df_test_synthetic_frames.csv\")","62a1dd32":"# Splitting dependent and independent variables in train and test\ndef split_XY(train,test):\n    X_train = train.drop(\"action\",axis = 1)\n    y_train = train[\"action\"]\n    X_test = test.drop(\"action\",axis = 1)\n    y_test = test[\"action\"]\n    return X_train,y_train,X_test,y_test","18120f39":"# X,Y Split \nX_train_top_40_frames,y_train_top_40_frames,X_test_top_40_frames,y_test_top_40_frames = split_XY(df_train_top_40_frames,df_test_top_40_frames)\nX_train_synthetic_frames,y_train_synthetic_frames,X_test_synthetic_frames,y_test_synthetic_frames = split_XY(df_train_synthetic_frames,df_test_synthetic_frames)","d6032ec5":"# General \ndef Data_Modelling(model,X_train,y_train,X_test,y_test):\n    classifier = model\n    classifier.fit(X_train,y_train)\n    label_predicted = classifier.predict(X_test)\n    print(\"Accuarcy Score : \",accuracy_score(y_test,label_predicted))\n    plt.figure(figsize = (14,8))\n    return sns.heatmap(confusion_matrix(y_test,label_predicted))","1583d31c":"# Logistic Regression for top 40 frames\nData_Modelling(LogisticRegression(solver=\"newton-cg\"),X_train_top_40_frames,y_train_top_40_frames,X_test_top_40_frames,y_test_top_40_frames)","4b812137":"# Logistic Regression for synthetic frames\nData_Modelling(LogisticRegression(solver=\"newton-cg\"),X_train_synthetic_frames,y_train_synthetic_frames,X_test_synthetic_frames,y_test_synthetic_frames)","b7159fd8":"# XGBoost for top 40 frames\nData_Modelling(XGBClassifier(),X_train_top_40_frames,y_train_top_40_frames,X_test_top_40_frames,y_test_top_40_frames)","856fe84d":"# XGBoost Classifier for synthetic frames\nData_Modelling(XGBClassifier(),X_train_synthetic_frames,y_train_synthetic_frames,X_test_synthetic_frames,y_test_synthetic_frames)","0477b926":"# SVM for top 40 frames\nData_Modelling(SVC(),X_train_top_40_frames,y_train_top_40_frames,X_test_top_40_frames,y_test_top_40_frames)","7f938cce":"# SVM for synthetic frames\nData_Modelling(SVC(),X_train_synthetic_frames,y_train_synthetic_frames,X_test_synthetic_frames,y_test_synthetic_frames)","05de8248":"# Voting Based Models\ndef ensemble_model(X_train,y_train,X_test,y_test):\n    model1 = LogisticRegression(random_state=1,solver='newton-cg')\n    model2 = tree.DecisionTreeClassifier(random_state=1)\n    model = VotingClassifier(estimators=[('lr', model1), ('dt', model2)], voting='hard')\n    model.fit(X_train,y_train)\n    label_predicted = model.predict(X_test)\n    print(\"Accuarcy Score : \",accuracy_score(y_test,label_predicted))\n    plt.figure(figsize = (14,8))\n    return sns.heatmap(confusion_matrix(y_test,label_predicted))","f2f76464":"ensemble_model(X_train_top_40_frames,y_train_top_40_frames,X_test_top_40_frames,y_test_top_40_frames)","4f23e5fc":"ensemble_model(X_train_synthetic_frames,y_train_synthetic_frames,X_test_synthetic_frames,y_test_synthetic_frames)","975944fe":"## Logistic Regression","66652c98":"<div style=\"background:#c72e57;color:#fff;padding:1em 2em 1.5em 2em;border-radius: 4px;font-weight:bold\">\n    <strong>\n        <h4 style = \"color:#fff\"><font size = 4>Problem Statement<\/font><\/h4>\n    <\/strong>\n<\/div><br>\n<font size = 3>Develop and Evaluate a machine learning model for 3D skeleton-based action recognition using UTD MHAD dataset. <\/font>  ","5dc1929d":"<div style=\"background:#c72e57;color:#fff;padding:1em 2em 1.5em 2em;border-radius: 5px;font-weight:bold\">\n    <strong>\n        <h4 style = \"color:#fff\"><font size = 4>Approach 2<\/font><\/h4>\n        <h4>Creating synthetic frames with equal intervals for Handling Variable-length Strategy in all videos <\/h4>\n    <\/strong>\n<\/div>","99690383":"## Ensemble Model","c9f01dc2":"<div style=\"background:#c72e57;color:#fff;padding:1em 2em 1.5em 2em;border-radius: 5px;font-weight:bold\">\n    <strong>\n        <h4 style = \"color:#fff\"><font size = 4>Train Test Split<\/font><\/h4>\n    <\/strong>\n<\/div>`","26e555c2":"<center><h1 class=\"list-group-item list-group-item-success\"> 3D Skeleton Action Recognition <\/center>","167d368e":"<div style=\"background:#c72e57;color:#fff;padding:1em 2em 1.5em 2em;border-radius: 5px;font-weight:bold\">\n    <strong>\n        <h4 style = \"color:#fff\"><font size = 4>Data Splitting<\/font><\/h4>\n    <\/strong>\n<\/div>","783c7828":"<img src = \"https:\/\/sinestesia.co\/wp\/wp-content\/uploads\/2020\/03\/euclidean_distance3d.png\">\n\n<img src = \"https:\/\/res.cloudinary.com\/qna\/image\/upload\/v1633065597\/photo_2021-10-01_10-49-45_voj65v.jpg\" width = 500 >\n","ce124416":"## SVM Classifier","399efa8b":"<img src= \"https:\/\/res.cloudinary.com\/qna\/image\/upload\/v1633066385\/39214_wezmee.png\" width = 700>","3196f2c0":"<div style=\"background:#c72e57;color:#fff;padding:1em 2em 1.5em 2em;border-radius: 5px;font-weight:bold\">\n    <strong>\n        <h4 style = \"color:#fff\"><font size = 4>Approach 1<\/font><\/h4>\n        <h4>Extracting top 40 frames for Handling Variable-length Strategy in all videos <\/h4>\n    <\/strong>\n<\/div>","15784756":"<div style=\"background:#c72e57;color:#fff;padding:1em 2em 1.5em 2em;border-radius: 5px;font-weight:bold\">\n    <strong>\n        <h4 style = \"color:#fff\"><font size = 4>Data Modelling<\/font><\/h4>\n    <\/strong>\n<\/div>\n\n","7bca01b8":"<div style=\"background:#c72e57;color:#fff;padding:1em 2em 1.5em 2em;border-radius: 5px;font-weight:bold\">\n    <strong>\n        <h4 style = \"color:#fff\"><font size = 4>Data Overview<\/font><\/h4>\n    <\/strong>\n<\/div>","ccb9b516":"<div style=\"background:#c72e57;color:#fff;padding:1em 2em 1.5em 2em;border-radius: 5px;font-weight:bold\">\n    <strong>\n        <h4 style = \"color:#fff\"><font size = 4>Importing Data<\/font><\/h4>\n    <\/strong>\n<\/div>`","99793a8d":"<div style=\"background:#c72e57;color:#fff;padding:1em 2em 1.5em 2em;border-radius: 5px;font-weight:bold\">\n    <strong>\n        <h4 style = \"color:#fff\"><font size = 4>Features Overview<\/font><\/h4>\n    <\/strong>\n<\/div>\n<br><br>\n<img src = \"https:\/\/res.cloudinary.com\/qna\/image\/upload\/v1633065597\/photo_2021-10-01_10-49-45_voj65v.jpg\" width = 500 >\n\n<font size = 3><br>\nThe skeleton joint order in UTD-MAD dataset:\n<br>\n1. head,<br>\n2. shoulder_center,<br>\n3. spine,<br>\n4. hip_center,<br>\n5. left_shoulder,<br>\n6. left_elbow,<br>\n7. left_wrist,<br>\n8. left_hand,<br>\n9. right_shoulder,<br>\n10. right_elbow,<br>\n11. right_wrist,<br>\n12. right_hand,<br>\n13. left_hip,<br>\n14. left_knee,<br>\n15. left_ankle,<br>\n16. left_foot,<br>\n17. right_hip,<br>\n18. right_knee,<br>\n19. right_ankle,<br>\n20. right_foot,<br>\n<br>\n\nEach skeleton data is a 20 x 3 x num_frame matrix. Each row of a skeleton frame corresponds to three spatial coordinates of a joint.<br><br>\n    \n<div style=\"background:#c72e57;color:#fff;padding:1em 2em 1.5em 2em;border-radius: 5px;font-weight:bold\">\n    <strong>\n        <h4 style = \"color:#fff\"><font size = 4>Different Actions<\/font><\/h4>\n    <\/strong>\n<\/div>\n    \n<br>\nUTD-MHAD dataset consists of 27 different actions: <br><br>\n    1  right arm swipe to the left, <br>\n    2  right arm swipe to the right, <br>\n    3  right hand wave, <br>\n    4  two hand front clap, <br>\n    5  right arm throw, <br>\n    6  cross arms in the chest,<br> \n    7  basketball shoot, <br>\n    8  right hand draw x, <br>\n    9  right hand draw circle (clockwise), <br>\n    10 right hand draw circle (counter clockwise), <br>\n    11 draw triangle, <br>\n    12 bowling (right hand), <br>\n    13 front boxing, <br>\n    14 baseball swing from right, <br>\n    15 tennis right hand forehand swing, <br>\n    16 arm curl (two arms), <br>\n    17 tennis serve, <br>\n    18 two hand push, <br>\n    19 right hand knock on door, <br>\n    20 right hand catch an object, <br>\n    21 right hand pick up and throw, <br>\n    22 jogging in place, <br>\n    23 walking in place, <br>\n    24 sit to stand, <br>\n    25 stand to sit, <br>\n    26 forward lunge (left foot forward), <br>\n    27 squat (two arms stretch out)<br>\n<\/font>","ea8f9873":"<div style=\"background:#c72e57;color:#fff;padding:1em 2em 1.5em 2em;border-radius: 5px;font-weight:bold\">\n    <strong>\n        <h4 style = \"color:#fff\"><font size = 4>Feature Extraction<\/font><\/h4>\n        <h4>Euclidean distance for all joints in consecutive frames<\/h4>\n    <\/strong>\n<\/div>","f61cbe0a":"<center><h1 class=\"list-group-item list-group-item-success\"> THANK YOU <\/center>","d8518205":"<div style=\"background:#c72e57;color:#fff;padding:1em 2em 1.5em 2em;border-radius: 5px;font-weight:bold\">\n    <strong>\n        <h4 style = \"color:#fff\"><font size = 4>Train\/Test Split Criteria<\/font><\/h4>\n    <\/strong>\n<\/div>\n<br>\n<font size = 3>Odd subjects (S1, S3, S5, S7) for Training and the rest (S2, S4, S6, S8) for testing.<\/font>  ","b33e5c82":"<div style=\"background:#c72e57;color:#fff;padding:1em 2em 1.5em 2em;border-radius: 5px;font-weight:bold\">\n    <strong>\n        <h4 style = \"color:#fff\"><font size = 4>Conclusion<\/font><\/h4>\n    <\/strong>\n<\/div><br><br>\n<font size = 3.5><ul><li>In this notebook, we have implemented <strong>3D skeleton based action recognition<\/strong> using Euclidean Distance between joints in consecutive frames as a feature extraction technique and we have trained many classification models and tried with some ensemble methods to improvise accuracy.<\/li><br>\n<li>We used two approaches for handling variable length frames. Comparing them with accuracy we get 5% increase in accuracy results by taking top 40 frames than creating synthetic frames.\n    <\/li><br>\n    <li>We observe that ensemble learning gives 10% increase in accuracy compared to Discrete Learners like XGboost, LogisticRegression, SVM<\/li><br>\n<ul>","75378887":"## XGBoost Classifier","d04c87a8":"<div style=\"background:#c72e57;color:#fff;padding:1em 2em 1.5em 2em;border-radius: 5px;font-weight:bold\">\n    <strong>\n        <h4 style = \"color:#fff\"><font size = 4>Importing Packages<\/font><\/h4>\n    <\/strong>\n<\/div>"}}