{"cell_type":{"05f924e9":"code","5f477a24":"code","8fee5847":"code","2fea2f91":"code","991ea576":"code","9ac6b26d":"code","c607c2b9":"code","d0a87438":"code","7e512865":"code","ab50d0f8":"code","25a47f1a":"code","2023dfc6":"code","cc6df279":"code","48813c12":"code","9d00a6e1":"code","3705ce48":"code","a203c6dc":"markdown"},"source":{"05f924e9":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport tensorflow as tf\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow import keras\nimport pandas_profiling as pp\n\n\n\nsc = MinMaxScaler(feature_range = (0, 1))\n\nrock_dataset = pd.read_csv(\"..\/input\/0.csv\", header=None) # class = 0\nscissors_dataset = pd.read_csv(\"..\/input\/1.csv\", header=None) # class = 1\npaper_dataset = pd.read_csv(\"..\/input\/2.csv\", header=None) # class = 2\nok_dataset = pd.read_csv(\"..\/input\/3.csv\", header=None) # class = 3\n\nprint(\"Rock Shape: \",rock_dataset.shape,\n      \"\\nScissor Shape: \",scissors_dataset.shape,\n      \"\\nPaper Shape: \",paper_dataset.shape,\n      \"\\nOK Shape: \",ok_dataset.shape)\n\n\nrock_dataset.head(3)","5f477a24":"scissors_dataset.head(3)","8fee5847":"paper_dataset.head(3)","2fea2f91":"ok_dataset.head(3)","991ea576":"colors=[\"forestgreen\",\"teal\",\"crimson\",\"chocolate\",\"darkred\",\"lightseagreen\",\"orangered\",\"chartreuse\"]\ntime_rock=rock_dataset.iloc[:,0:8]\ntime_rock.index=pd.to_datetime(time_rock.index)\ntime_rock.iloc[:170,:].plot(subplots=True,figsize=(10,10),colors=colors);","9ac6b26d":"time_scis=scissors_dataset.iloc[:,0:8]\ntime_scis.index=pd.to_datetime(time_scis.index)\ntime_scis.iloc[:170,:].plot(subplots=True,figsize=(10,10),colors=colors);","c607c2b9":"time_paper=paper_dataset.iloc[:,0:8]\ntime_paper.index=pd.to_datetime(time_paper.index)\ntime_paper.iloc[:170,:].plot(subplots=True,figsize=(10,10),colors=colors);","d0a87438":"time_ok=ok_dataset.iloc[:,0:8]\ntime_ok.index=pd.to_datetime(time_ok.index)\ntime_ok.iloc[:170,:].plot(subplots=True,figsize=(10,10),colors=colors);","7e512865":"frames = [rock_dataset, scissors_dataset, paper_dataset, ok_dataset]\ndataset = pd.concat(frames)\n\ndataset_train = dataset.iloc[np.random.permutation(len(dataset))]\ndataset_train.reset_index(drop=True)\n\nX_train = []\ny_train = []\n\nfor i in range(0, dataset_train.shape[0]):\n    row = np.array(dataset_train.iloc[i:1+i, 0:64].values)\n    X_train.append(np.reshape(row, (64, 1)))\n    y_train.append(np.array(dataset_train.iloc[i:1+i, -1:])[0][0])\n\nX_train = np.array(X_train)\ny_train = np.array(y_train)\n\n# Reshape to one flatten vector\nX_train = X_train.reshape(X_train.shape[0]*X_train.shape[1], 1)\nX_train = sc.fit_transform(X_train)\n\n# Reshape again after normalization to (-1, 8, 8)\nX_train = X_train.reshape((-1, 8, 8))\n\n# Convert to one hot\ny_train = np.eye(np.max(y_train) + 1)[y_train]\n\n\nprint(\"All Data size X and y\")\nprint(X_train.shape)\nprint(y_train.shape)\n# Splitting Train\/Test\nX_test = X_train[7700:]\ny_test = y_train[7700:]\nprint(\"Test Data size X and y\")\nprint(X_test.shape)\nprint(y_test.shape)\n\nX_train = X_train[0:7700]\ny_train = y_train[0:7700]\nprint(\"Train Data size X and y\")\nprint(X_train.shape)\nprint(y_train.shape)\n\n# Creating the model\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dropout\n\nclassifier = Sequential()\n\nclassifier.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 8)))\nclassifier.add(Dropout(0.2))\n\nclassifier.add(LSTM(units = 50, return_sequences = True))\nclassifier.add(Dropout(0.2))\n\nclassifier.add(LSTM(units = 50, return_sequences = True))\nclassifier.add(Dropout(0.2))\n\nclassifier.add(LSTM(units = 50))\nclassifier.add(Dropout(0.2))\n\nclassifier.add(Dense(units = 64))\nclassifier.add(Dense(units = 128))\n\nclassifier.add(Dense(units = 4, activation=\"softmax\"))\n\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy')\n\nclassifier.fit(X_train, y_train, epochs = 250, batch_size = 32, verbose=2)\n\n# Save\nclassifier.save(\"model_cross_splited_data.h5\")\nprint(\"Saved model to disk\")\n\n###############################################\n\nfrom tensorflow import keras\n\n# # Load Model\n# model = keras.models.load_model('model_cross_splited_data.h5')\n# model.summary()\n\ndef evaluateModel(prediction, y):\n    good = 0\n    for i in range(len(y)):\n        if (prediction[i] == np.argmax(y[i])):\n            good = good +1\n    return (good\/len(y)) * 100.0\n\nresult_test = classifier.predict_classes(X_test)\nprint(\"Correct classification rate on test data\")\nprint(evaluateModel(result_test, y_test))\n\nresult_train = classifier.predict_classes(X_train)\nprint(\"Correct classification rate on train data\")\nprint(evaluateModel(result_train, y_train))\n","ab50d0f8":"col_names = list()\nfor i in range(0,65):\n    if i == 64:\n        col_names.append(\"class\")\n    else:\n        col_names.append(\"sensor\"+str(i+1))\n        \nrock_dataset.columns = col_names\nscissors_dataset.columns = col_names\npaper_dataset.columns = col_names\nok_dataset.columns = col_names\n\ndataset_2 = pd.concat([rock_dataset,scissors_dataset,paper_dataset,ok_dataset],ignore_index=True)\nprint(dataset_2.tail())\nprint(dataset_2.shape)","25a47f1a":"from sklearn.neighbors import KNeighborsClassifier\n\ny = dataset_2[\"class\"].values.reshape(-1,1)\nx = dataset_2.drop([\"class\"],axis = 1).values\n\ntotal = dataset_2.isnull().sum().sort_values(ascending = False)\npercentage = (dataset_2.isnull().sum()\/dataset_2.isnull().count()).sort_values(ascending = False)\nmissing_data = pd.concat([total,percentage],axis = 1, keys = [\"Total\",\"Percentage\"])\nmissing_data","2023dfc6":"dataset_2.describe()","cc6df279":"#from pandas_profiling import \n#ProfileReportprofile = ProfileReport(dataset_2, title='Pandas Profiling Report')\n#pp.ProfileReport(dataset_2)","48813c12":"sns.boxplot(x = dataset_2['sensor32'])","9d00a6e1":"#next step is to standardize our data - using MinMaxScaler\ny = dataset_2[\"class\"]\nx_data1 = dataset_2.drop([\"class\"],axis = 1)\n\nfrom sklearn import preprocessing\n\nscaler = preprocessing.MinMaxScaler()\nscaler.fit(x_data1)\n\nx_data1 = pd.DataFrame(scaler.transform(x_data1), index=x_data1.index, columns=x_data1.columns)\npp.ProfileReport(x_data1)","3705ce48":"y = dataset_2[\"class\"].values.reshape(-1,1)\nx = dataset_2.drop([\"class\"],axis = 1).values\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3, random_state = 42)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)  #n_neighbor = k\nknn.fit(x_train,y_train)\n\n#Find the best K value\n\nk_value = []\naccuracy = []\n\nfor i in range(1,22):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train,np.ravel(y_train,order='C'))\n    \n    score = knn.score(x_test,y_test)\n    k_value.append(i)\n    accuracy.append(score)\nfor i,j in zip(k_value,accuracy):\n    print(i,j)\n\n#Find K value for Max accuracy\nplt.plot(range(1,22),accuracy,color = \"blue\")\nplt.xlabel(\"K value\")\nplt.ylabel(\"Accuracy\")\nplt.show()","a203c6dc":"Thanks to [Vitalii Mokin](https:\/\/www.kaggle.com\/vbmokin) for his kernel:\n* [BOD prediction in river - 15 regression models](https:\/\/www.kaggle.com\/vbmokin\/code-starter-ammonium-prediction-in-river)\n\nThanks to [Wathek LOUED](https:\/\/www.kaggle.com\/wathek) for his kernel:\n* [lstm](https:\/\/www.kaggle.com\/wathek\/lstm-model)\n\nThanks to [Furkan KASIM](https:\/\/www.kaggle.com\/fkasim) for his kernel:\n* [Bitirme Proje](https:\/\/www.kaggle.com\/fkasim\/bitirme-proje)"}}