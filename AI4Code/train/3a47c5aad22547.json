{"cell_type":{"eed53c3d":"code","6b6d5258":"code","b3dafc24":"code","ca40ec07":"code","2e4cffa0":"code","2bc999b6":"code","eed5ae52":"code","60f682ea":"code","9d70ee52":"code","28b69be8":"code","937df142":"code","39f7ddc4":"code","58a171d2":"code","1f0693cf":"code","7839a5bb":"code","2aa635f6":"code","9cb53844":"code","fbf9a8bb":"code","c3084c02":"code","28b942aa":"code","54c371c5":"code","7eb5433b":"code","40144e92":"code","01893c62":"code","e17d747f":"code","11153d03":"code","7709bc55":"code","5cc71f0a":"code","4ee42a15":"code","552047b3":"code","725d21a3":"code","a1610044":"code","065e376c":"code","758c7a7c":"code","adc0d17c":"code","f22f4e0e":"code","0934cfd7":"code","d1d20c9a":"code","c5d14e01":"code","2368de8f":"code","8eeb0575":"code","9f18e0aa":"code","61e9cc4a":"code","e97cc845":"code","001a78b1":"code","8186d86c":"code","9e47d470":"code","26c16c09":"code","96f51907":"code","595c2818":"code","b58e4814":"code","b83261e5":"code","1f9408a3":"code","bb3e7d50":"code","b2d11ff1":"code","cd2cd2fd":"code","8ac27cde":"code","fedfe4c7":"code","6ffb2893":"code","6b94008d":"code","14176bed":"code","e073cf43":"code","91da5676":"code","3681ab84":"code","22715352":"code","41f6ca6e":"code","84a0f122":"code","84aabc97":"code","e1984aa4":"code","6c4921a9":"code","1d029d30":"code","31d22a18":"code","3c318fbb":"code","266607f1":"code","5fa5783a":"code","2a58861b":"code","30dbd252":"code","d91f3b2b":"code","39202b1a":"code","c47b8ca0":"code","e37d8a84":"code","592d3806":"code","6dbda296":"code","d29550e0":"code","56b48527":"code","00f7880a":"code","79b9b83b":"code","4e5cafff":"code","19a88465":"code","ea0bbc72":"code","807a943d":"code","31f311d8":"code","b17fa05f":"code","b8064262":"code","82b70c70":"code","11615f21":"markdown","26004519":"markdown","03de630e":"markdown","895ba4ba":"markdown","655f7b74":"markdown","f9455a1d":"markdown","690b0408":"markdown","6bdb2d5e":"markdown","7201a986":"markdown","e2a710a8":"markdown","1eb7d230":"markdown","0b9d6ed5":"markdown","d6cf7a24":"markdown","27ff30fe":"markdown","423805e9":"markdown","16d44a2a":"markdown","c5f5debb":"markdown","fb228937":"markdown","ab4e3a84":"markdown","41fe52c5":"markdown","a1262016":"markdown","3bd3cd6e":"markdown","c98ecd6d":"markdown","446e5af3":"markdown","e30f4d38":"markdown","add5cac1":"markdown","31b3edb3":"markdown","5ae81352":"markdown","313cb102":"markdown","57d2f2e9":"markdown","baa52ab2":"markdown","ccb2f056":"markdown","99289970":"markdown","ba070c3d":"markdown","071d673f":"markdown","be8adcb0":"markdown","8280862a":"markdown","68e76786":"markdown","de00ea29":"markdown","408a4ee5":"markdown","4941ad3f":"markdown","8b22fdb7":"markdown","a9d4a12c":"markdown","ffb3dd55":"markdown","e5328277":"markdown","48f269a6":"markdown","26a6b1bd":"markdown"},"source":{"eed53c3d":"# data analysis libraries:\nimport numpy as np\nimport pandas as pd\n\n# data visualization libraries:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# to ignore warnings:\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# to display all columns:\npd.set_option('display.max_columns', None)\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\n","6b6d5258":"#import train and test CSV files\n\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","b3dafc24":"# copy data in order to avoid any change in the original:\n\ntrain = train_data.copy()\ntest = test_data.copy()","ca40ec07":"#take a look at the training data\n\ntrain.describe(include=\"all\")","2e4cffa0":"#get information about the dataset\n\ntrain.info()","2bc999b6":"#get a list of the features within the dataset\n\nprint(train.columns)\n","eed5ae52":"#head \n\ntrain.head()\n","60f682ea":"#head \n\ntest.head()\n","9d70ee52":"#tail\n\ntrain.tail()","28b69be8":"#see a sample of the dataset to get an idea of the variables\n\ntrain.sample(5)\n\n","937df142":"#check for any other unusable values\n\nprint(pd.isnull(train).sum())\n","39f7ddc4":"#see a summary of the training dataset\n\ntrain.describe().T\n","58a171d2":"100*train.isnull().sum()\/len(train)","1f0693cf":"train['Pclass'].value_counts()","7839a5bb":"train['Sex'].value_counts()","2aa635f6":"train['SibSp'].value_counts()","9cb53844":"train['Parch'].value_counts()","fbf9a8bb":"train['Ticket'].value_counts()","c3084c02":"train['Cabin'].value_counts()","28b942aa":"train['Embarked'].value_counts()","54c371c5":"#draw a bar plot of survival by sex\n\nsns.barplot(x=\"Sex\", y=\"Survived\", data=train)\n\n#print percentages of females vs. males that survive\n\nprint(\"Percentage of females who survived:\", train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of males who survived:\", train[\"Survived\"][train[\"Sex\"] == 'male'].value_counts(normalize = True)[1]*100)\n","7eb5433b":"#draw a bar plot of survival by Pclass\n\nsns.barplot(x=\"Pclass\", y=\"Survived\", data=train)\n\n#print percentage of people by Pclass that survived\n\nprint(\"Percentage of Pclass = 1 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of Pclass = 2 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 2].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of Pclass = 3 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 3].value_counts(normalize = True)[1]*100)","40144e92":"#draw a bar plot for SibSp vs. survival\n\nsns.barplot(x=\"SibSp\", y=\"Survived\", data=train)\n\n#I won't be printing individual percent values for all of these.\n\nprint(\"Percentage of SibSp = 0 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 0].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of SibSp = 1 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of SibSp = 2 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 2].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of SibSp = 3 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 3].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of SibSp = 4 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 4].value_counts(normalize = True)[1]*100)\n","01893c62":"#draw a bar plot for Parch vs. survival\n\nsns.barplot(x=\"Parch\", y=\"Survived\", data=train)\nplt.show()\n","e17d747f":"\ntrain[\"CabinBool\"] = (train[\"Cabin\"].notnull().astype('int'))\ntest[\"CabinBool\"] = (test[\"Cabin\"].notnull().astype('int'))\n\n#calculate percentages of CabinBool vs. survived\n\nprint(\"Percentage of CabinBool = 1 who survived:\", train[\"Survived\"][train[\"CabinBool\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of CabinBool = 0 who survived:\", train[\"Survived\"][train[\"CabinBool\"] == 0].value_counts(normalize = True)[1]*100)\n\n#draw a bar plot of CabinBool vs. survival\n\nsns.barplot(x=\"CabinBool\", y=\"Survived\", data=train)\n\nplt.show()\n","11153d03":"test.describe().T","7709bc55":"# Create CabinBool variable which states if someone has a Cabin data or not:\n\ntrain = train.drop(['Cabin'], axis = 1)\ntest = test.drop(['Cabin'], axis = 1)\n\ntrain.head()\n","5cc71f0a":"train.isnull().sum()","4ee42a15":"test.isnull().sum()","552047b3":"print(pd.isnull(train.CabinBool).sum())\n","725d21a3":"# We can drop the Ticket feature since it is unlikely to have useful information\n\ntrain = train.drop(['Ticket'], axis = 1)\ntest = test.drop(['Ticket'], axis = 1)\n\ntrain.head()","a1610044":"train.describe().T\n","065e376c":"# It looks like there is a problem in Fare max data. Visualize with boxplot.\n\nsns.boxplot(x = train['Fare']);","758c7a7c":"Q1 = train['Fare'].quantile(0.05)\nQ3 = train['Fare'].quantile(0.95)\nIQR = Q3 - Q1\n\nlower_limit = Q1- 1.5*IQR\nlower_limit\n\nupper_limit = Q3 + 1.5*IQR\nupper_limit","adc0d17c":"# observations with Fare data higher than the upper limit:\n\ntrain['Fare'] > (upper_limit)","f22f4e0e":"train.sort_values(\"Fare\", ascending=False).head()","0934cfd7":"# In boxplot, there are too many data higher than upper limit; we can not change all. Just repress the highest value -512- \n\ntrain['Fare'] = train['Fare'].replace(512.3292, 270)","d1d20c9a":"train.sort_values(\"Fare\", ascending=False).head()","c5d14e01":"train.sort_values(\"Fare\", ascending=False)","2368de8f":"test.sort_values(\"Fare\", ascending=False)","8eeb0575":"test['Fare'] = test['Fare'].replace(512.3292, 270)","9f18e0aa":"test.sort_values(\"Fare\", ascending=False)","61e9cc4a":"#drop the name feature since it contains no more useful information.\n\ntrain = train.drop(['Name'], axis = 1)\ntest = test.drop(['Name'], axis = 1)","e97cc845":"train.describe().T","001a78b1":"train.isnull().sum()","8186d86c":"train[\"Age\"] = train[\"Age\"].fillna(train[\"Age\"].median())","9e47d470":"test[\"Age\"] = test[\"Age\"].fillna(test[\"Age\"].median())","26c16c09":"train.isnull().sum()","96f51907":"test.isnull().sum()","595c2818":"train.describe().T","b58e4814":"train.isnull().sum()","b83261e5":"test.isnull().sum()","1f9408a3":"#now we need to fill in the missing values in the Embarked feature\n\nprint(\"Number of people embarking in Southampton (S):\")\nsouthampton = train[train[\"Embarked\"] == \"S\"].shape[0]\nprint(southampton)\n\nprint(\"Number of people embarking in Cherbourg (C):\")\ncherbourg = train[train[\"Embarked\"] == \"C\"].shape[0]\nprint(cherbourg)\n\nprint(\"Number of people embarking in Queenstown (Q):\")\nqueenstown = train[train[\"Embarked\"] == \"Q\"].shape[0]\nprint(queenstown)\n","bb3e7d50":"train[\"Embarked\"].value_counts()","b2d11ff1":"#replacing the missing values in the Embarked feature with S\n\ntrain = train.fillna({\"Embarked\": \"S\"})","cd2cd2fd":"test = test.fillna({\"Embarked\": \"S\"})","8ac27cde":"train.Embarked","fedfe4c7":"train.isnull().sum()","6ffb2893":"test.isnull().sum()","6b94008d":"print(pd.isnull(train.Embarked).sum())\n","14176bed":"test[test[\"Fare\"].isnull()]","e073cf43":"test[[\"Pclass\",\"Fare\"]].groupby(\"Pclass\").mean()","91da5676":"test[\"Fare\"] = test[\"Fare\"].fillna(12)","3681ab84":"test[\"Fare\"].isnull().sum()","22715352":"#check train data\n\ntrain.head()","41f6ca6e":"#check test data\n\ntest.head()","84a0f122":"#map each Sex value to a numerical value\n\nsex_mapping = {\"male\": 0, \"female\": 1}\ntrain['Sex'] = train['Sex'].map(sex_mapping)\ntest['Sex'] = test['Sex'].map(sex_mapping)\n\ntrain.head()","84aabc97":"#map each Embarked value to a numerical value\n\nfrom sklearn import preprocessing\n\nlbe = preprocessing.LabelEncoder()\ntrain[\"Embarked\"] = lbe.fit_transform(train[\"Embarked\"])\ntest[\"Embarked\"] = lbe.fit_transform(test[\"Embarked\"])\ntrain.head()\n","e1984aa4":"bins = [0, 5, 12, 18, 24, 35, 60, np.inf]\nmylabels = ['Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ntrain['AgeGroup'] = pd.cut(train[\"Age\"], bins, labels = mylabels)\ntest['AgeGroup'] = pd.cut(test[\"Age\"], bins, labels = mylabels)","6c4921a9":"# Map each Age value to a numerical value:\nage_mapping = {'Baby': 1, 'Child': 2, 'Teenager': 3, 'Student': 4, 'Young Adult': 5, 'Adult': 6, 'Senior': 7}\ntrain['AgeGroup'] = train['AgeGroup'].map(age_mapping)\ntest['AgeGroup'] = test['AgeGroup'].map(age_mapping)","1d029d30":"train.head()","31d22a18":"#dropping the Age feature for now, might change:\ntrain = train.drop(['Age'], axis = 1)\ntest = test.drop(['Age'], axis = 1)","3c318fbb":"train.head()","266607f1":"# Map Fare values into groups of numerical values:\ntrain['FareBand'] = pd.qcut(train['Fare'], 4, labels = [1, 2, 3, 4])\ntest['FareBand'] = pd.qcut(test['Fare'], 4, labels = [1, 2, 3, 4])","5fa5783a":"# Drop Fare values:\ntrain = train.drop(['Fare'], axis = 1)\ntest = test.drop(['Fare'], axis = 1)","2a58861b":"train.head()","30dbd252":"train.head()","d91f3b2b":"train[\"FamilySize\"] = train_data[\"SibSp\"] + train_data[\"Parch\"] + 1","39202b1a":"test[\"FamilySize\"] = test_data[\"SibSp\"] + test_data[\"Parch\"] + 1","c47b8ca0":"# Create new feature of family size:\n\ntrain['Single'] = train['FamilySize'].map(lambda s: 1 if s == 1 else 0)\ntrain['SmallFam'] = train['FamilySize'].map(lambda s: 1 if  s == 2  else 0)\ntrain['MedFam'] = train['FamilySize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ntrain['LargeFam'] = train['FamilySize'].map(lambda s: 1 if s >= 5 else 0)","e37d8a84":"train.head()","592d3806":"# Create new feature of family size:\n\ntest['Single'] = test['FamilySize'].map(lambda s: 1 if s == 1 else 0)\ntest['SmallFam'] = test['FamilySize'].map(lambda s: 1 if  s == 2  else 0)\ntest['MedFam'] = test['FamilySize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ntest['LargeFam'] = test['FamilySize'].map(lambda s: 1 if s >= 5 else 0)","6dbda296":"test.head()\n","d29550e0":"# Convert Title and Embarked into dummy variables:\n\ntrain = pd.get_dummies(train, columns = [\"Embarked\"], prefix=\"Em\")","56b48527":"train.head()","00f7880a":"\ntest = pd.get_dummies(test, columns = [\"Embarked\"], prefix=\"Em\")\n","79b9b83b":"test.head()","4e5cafff":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\npredictors = train.drop(['Survived', 'PassengerId'], axis=1)\ntarget = train[\"Survived\"]\nx_train, x_test, y_train, y_test = train_test_split(predictors, target, test_size = 0.20, random_state = 0)\n","19a88465":"x_train.shape","ea0bbc72":"x_test.shape","807a943d":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_test)\nacc_logreg = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_logreg)\n","31f311d8":"from sklearn.ensemble import RandomForestClassifier\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_test)\nacc_randomforest = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_randomforest)","b17fa05f":"from sklearn.ensemble import GradientBoostingClassifier\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train, y_train)\ny_pred = gbk.predict(x_test)\nacc_gbk = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_gbk)\n\n","b8064262":"xgb_params = {\n        'n_estimators': [200, 500],\n        'subsample': [0.6, 1.0],\n        'max_depth': [2,5,8],\n        'learning_rate': [0.1,0.01,0.02],\n        \"min_samples_split\": [2,5,10]}\n\nxgb = GradientBoostingClassifier()\n\nxgb_cv_model = GridSearchCV(xgb, xgb_params, cv = 10, n_jobs = -1, verbose = 2)\n\nxgb_cv_model.fit(x_train, y_train)\n\nxgb_cv_model.best_params_\n\nxgb = GradientBoostingClassifier(learning_rate = xgb_cv_model.best_params_[\"learning_rate\"], \n                    max_depth = xgb_cv_model.best_params_[\"max_depth\"],\n                    min_samples_split = xgb_cv_model.best_params_[\"min_samples_split\"],\n                    n_estimators = xgb_cv_model.best_params_[\"n_estimators\"],\n                    subsample = xgb_cv_model.best_params_[\"subsample\"])\nxgb_tuned =  xgb.fit(x_train,y_train)\n\ny_pred = xgb_tuned.predict(x_test)\nacc_gbk = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_gbk)\n","82b70c70":"test\n\n#set ids as PassengerId and predict survival \nids = test['PassengerId']\npredictions = xgb_tuned.predict(test.drop('PassengerId', axis=1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)\n\noutput.head()\n","11615f21":"#### Name Feature","26004519":"#### Gradient Boosting Classifier","03de630e":"##### Sex Feature","895ba4ba":"### Classes of some categorical variables","655f7b74":"##### SibSp Feature","f9455a1d":"#### Embarked & Title","690b0408":"People with less than four parents or children aboard are more likely to survive than those with four or more.\n\nAgain, people traveling alone are less likely to survive than those with 1-3 parents or children.","6bdb2d5e":"## Deployment","7201a986":"#### Cabin Feature\n","e2a710a8":"# Titanic Survival Prediction:\n\nUse machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n","1eb7d230":"## 2) Loading Data\nIt's time to read in our training and testing data using pd.read_csv, and take a first look at the training data using the describe() function. We will take train and test copies.\n","0b9d6ed5":"#### Ticket Feature","d6cf7a24":"## 5) Cleaning Data","27ff30fe":"## 3) Data Analysis\nWe're going to consider the features in the dataset and how complete they are.\n","423805e9":"#### Sex Feature","16d44a2a":"## 1) Import Necessary Libraries\nFirst off, we need to import several Python libraries such as numpy, pandas, matplotlib and seaborn.","c5f5debb":"#### Embarked Feature","fb228937":"It's time separate the fare values into some logical groups as well as filling in the single missing value in the test dataset.","ab4e3a84":"# 4) Data Visualization\nIt's time to visualize our data so we can see whether our predictions were accurate! \nIn general, barplot is used for categorical variables while histogram, density and boxplot are used for numerical data.\n\n","41fe52c5":"As predicted, females have a much higher chance of survival than males. The Sex feature is essential in our predictions.","a1262016":"As predicted, people with higher socioeconomic class had a higher rate of survival. (62.9% vs. 47.3% vs. 24.2%)\n","3bd3cd6e":"#### AgeGroup","c98ecd6d":"\nWe'll fill in the missing values in the Age feature.\n","446e5af3":"#### Fare Feature","e30f4d38":"People with a recorded Cabin number are, in fact, more likely to survive. (66.6% vs 29.9%)","add5cac1":"##### Parch Feature","31b3edb3":"### Variable Transformation\n","5ae81352":"#### Random Forest","313cb102":"### Missing Value Treatment\n","57d2f2e9":"I think the idea here is that people with recorded cabin numbers are of higher socioeconomic class, and thus more likely to survive. \n\n","baa52ab2":"We can see that except for the above mentioned missing values, not NaN values exist.","ccb2f056":"## 6) Feature Engineering\n","99289970":"We can drop the name feature now that we've extracted the titles.","ba070c3d":"#### Embarked Feature","071d673f":"Some Observations:\n\nThere are a total of 891 passengers in our training set.\nThe Age feature is missing approximately 19.8% of its values. I'm guessing that the Age feature is pretty important to survival, so we should probably attempt to fill these gaps.\nThe Cabin feature is missing approximately 77.1% of its values. Since so much of the feature is missing, it would be hard to fill in the missing values. We'll probably drop these values from our dataset.\nThe Embarked feature is missing 0.22% of its values, which should be relatively harmless.","be8adcb0":"##### Pclass Feature","8280862a":"Time to clean our data to account for missing values and unnecessary information!\n\nLooking at the Test Data\n\nLet's see how our test data looks!\n","68e76786":"## 7) Modeling","de00ea29":"### Variables and Their Types:\n\nSurvival: Survival -> 0 = No, 1 = Yes\n\nPclass: Ticket class -> 1 = 1st, 2 = 2nd, 3 = 3rd\n\nSex: Sex\n\nAge: Age in years\n\nSibSp: # of siblings \/ spouses aboard the Titanic\n\nParch: # of parents \/ children aboard the Titanic\n\nTicket: Ticket number\n\nFare: Passenger fare\n\nCabin: Cabin number\n\nEmbarked: Port of Embarkation -> C = Cherbourg, Q = Queenstown, S = Southampton\n\n#### Types\n\nNumerical Features: Age , Fare , SibSp , Parch \n\n    Age: float\n    Fare: float\n    SibSp: int\n    Parch: int   \n    \nCategorical Features: Survived, Sex, Embarked, Pclass\n\n    Survived: int\n    Sex: string\n    Embarked: string\n    Pclass: int\n    \nAlphanumeric Features: Ticket, Cabin\n\n    Ticket: string\n    Cabin: string\n","408a4ee5":"##### Cabin Feature\n","4941ad3f":"#### Logistic Regression","8b22fdb7":"##### Family Size","a9d4a12c":"### Variable Notes:\n\nPclass: A proxy for socio-economic status (SES)\n\n    1st = Upper\n    2nd = Middle\n    3rd = Lower\n\nAge: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nSibSp: The dataset defines family relations in this way...\n\n    Sibling = brother, sister, stepbrother, stepsister\n    Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nParch: The dataset defines family relations in this way...\n\n    Parent = mother, father\n    Child = daughter, son, stepdaughter, stepson Some children travelled only with a nanny, therefore parch=0 for them.\n#### Now  we have an idea of what kinds of features we're working with.\n","ffb3dd55":"#### Fare","e5328277":"#### Age Feature","48f269a6":"#### Spliting the train data\n","26a6b1bd":"In general, it's clear that people with more siblings or spouses aboard were less likely to survive.\n\nHowever, contrary to expectations, people with no siblings or spouses were less to likely to survive than those with one or two. (34.5% vs 53.4% vs. 46.4%)\n"}}