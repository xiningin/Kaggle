{"cell_type":{"745f21d5":"code","e9fe3cf7":"code","d09d3e55":"code","99ef3b73":"code","1a6003cf":"code","8217fabc":"code","7b5640c3":"code","152c278d":"code","87f68454":"code","a004e22d":"code","384dae14":"code","ea265777":"code","8a465a60":"code","c0ca587b":"code","b7183f02":"code","a37c9819":"code","4cba8e68":"code","ec78f305":"code","4ce5db59":"code","1000371e":"code","afce2bfe":"code","7bc518d5":"code","f4f0cfee":"code","8b02a996":"code","8cbe1fcf":"code","0e90a477":"code","ecb91028":"code","cfd8e62c":"code","72641fc9":"code","8f97ffe8":"code","8d384f43":"code","c6ad9a24":"code","4a9dc4d7":"code","d1b6de91":"code","63af0751":"code","01e9d84b":"code","ed0418e0":"code","a2912183":"code","0af5cd30":"code","32ddfe80":"code","c78ee46e":"code","c49ab4e6":"code","315cc107":"code","f7b3a01e":"code","5a66c125":"code","7a577b5a":"code","028720b1":"code","b20b4caa":"code","bccfddd8":"code","cc3e3bd8":"code","5638cd39":"code","4224c588":"code","1e6f9e3d":"code","8b9a490f":"code","954d0d01":"code","56668c20":"code","397b17fe":"markdown","e942691b":"markdown","5e6de093":"markdown","0b6b7cfe":"markdown","d565142f":"markdown","09a98b6b":"markdown","f4968888":"markdown","32ec04b4":"markdown","07fec224":"markdown","bd1aa032":"markdown","28586e7b":"markdown","7c60d4d4":"markdown","d67539b7":"markdown","9bce9f91":"markdown","90ea96d3":"markdown","bb5e9a1a":"markdown","a9b13fd5":"markdown","41c6a072":"markdown","6e37dd12":"markdown","41419241":"markdown","10665453":"markdown","5c62a954":"markdown","b6ddce14":"markdown","e6094194":"markdown","b60efc10":"markdown","e3b9a109":"markdown","7edd94df":"markdown","72701e68":"markdown","1ba78375":"markdown","f86d0267":"markdown","71885400":"markdown","addc5752":"markdown","522a8641":"markdown","c8971039":"markdown","f69476d0":"markdown","bd5d1c99":"markdown","f4219350":"markdown","c03fabeb":"markdown","458197d4":"markdown","70442ea3":"markdown","06c2b19d":"markdown","5cc1dadb":"markdown","1b31db7d":"markdown","f6a986f2":"markdown","530dec16":"markdown","e68033b2":"markdown","54816a4f":"markdown","01bdf11e":"markdown","0d95644d":"markdown","fdcead73":"markdown","eaee0a87":"markdown","7e28a964":"markdown","eaf189b3":"markdown","c3ca335f":"markdown"},"source":{"745f21d5":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\n%matplotlib inline\nsns.set_style(\"whitegrid\")\n","e9fe3cf7":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d09d3e55":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col=0)\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col=0)","99ef3b73":"print(\"Train shape: \", train.shape)\nprint(\"Test shape: \", test.shape)","1a6003cf":"X = pd.concat([train.drop(\"SalePrice\", axis=1),test], axis=0)\ny = train[['SalePrice']]","8217fabc":"X.describe()","7b5640c3":"X.head()","152c278d":"X.tail()","87f68454":"numerical_features = X.select_dtypes(exclude=['object']).drop(['MSSubClass'], axis=1).copy()\nnumerical_features.columns","a004e22d":"cat_num_var = ['OverallQual','OverallCond','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath',\n                'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'MoSold', 'YrSold', 'YearBuilt', 'YearRemodAdd', 'GarageYrBlt']\nnum_var = [feature for feature in numerical_features if feature not in cat_num_var]   \nnum_var             ","384dae14":"cat_features = X.select_dtypes(include=[np.object]).copy()\ncat_features['MSSubClass'] = X['MSSubClass'] \n\ncat_features.columns","ea265777":"y.describe()","8a465a60":"sns.distplot(y)","c0ca587b":"print(\"Skewness \", y.skew())\nprint(\"Kurtosis \", y.kurt())","b7183f02":"for feature in num_var:\n    data = pd.concat([y,X[feature]], axis = 1)\n    data.plot.scatter(x = feature, y = 'SalePrice', ylim = (0, 800000))","a37c9819":"fig = plt.figure(figsize=(20,15))\nfor feature in cat_num_var:\n    data = pd.concat([y,X[feature]], axis = 1)\n    data.plot.scatter(x = feature, y = 'SalePrice', ylim = (0, 800000))","4cba8e68":"for feature in cat_features:\n    data = pd.concat([y,X[feature]], axis = 1)\n    f, ax = plt.subplots(figsize = (8, 6))\n    fig = sns.boxplot(x = feature, y = 'SalePrice', data = data)\n    fig.axis(ymin=0, ymax=800000)","ec78f305":"fig = plt.figure(figsize=(10,6))\nfor index,col in enumerate(num_var):\n    plt.subplot(6,4,index+1)\n    sns.distplot(numerical_features.loc[:,col].dropna(), kde=False)\nfig.tight_layout(pad=1.0)\n","4ce5db59":"fig = plt.figure(figsize=(20,15))\nfor index,col in enumerate(cat_num_var):\n    plt.subplot(9,5,index+1)\n    sns.countplot(x=col, data=numerical_features.dropna())\nfig.tight_layout(pad=1.0)","1000371e":"fig = plt.figure(figsize=(20,15))\nfor index in range(len(cat_features.columns)):\n    plt.subplot(9,5,index+1)\n    sns.countplot(x=cat_features.iloc[:,index], data=cat_features.dropna())\nfig.tight_layout(pad=1.0)","afce2bfe":"corr_matrix = X.corr()\nf, ax = plt.subplots(figsize=(20,15))\nsns.heatmap(corr_matrix,  linewidth=0.5, cmap='Greys')","7bc518d5":"corr_matrix = train.corr()\nf, ax = plt.subplots(figsize=(10,8))\nnum_of_var = 15 \ncols = corr_matrix.nlargest(num_of_var, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1)\nheatmap = sns.heatmap(cm,vmin = -1, vmax = 1, center = 0, annot = True,cmap = 'Accent', yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","f4f0cfee":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt','GarageArea','1stFlrSF', 'TotRmsAbvGrd','YearRemodAdd']\nsns.pairplot(train[cols], size = 2.5)\nplt.show()","8b02a996":"total = X.isnull().sum().sort_values(ascending=False)\npercent = (X.isnull().sum()\/X.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","8cbe1fcf":"X.drop(['PoolQC','MiscFeature','Alley'], axis=1, inplace=True)","0e90a477":"X.drop(['GarageYrBlt','GrLivArea','TotalBsmtSF','GarageCars'], axis=1, inplace=True)","ecb91028":"corr_matrix[['SalePrice']].sort_values(['SalePrice'], ascending=False).tail(10)\n\nX.drop(['MoSold','YrSold'], axis=1, inplace=True)","cfd8e62c":"cat_col = X.select_dtypes(include=['object']).columns\noverfit_cat = []\nfor i in cat_col:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(X) * 100 > 90:\n        overfit_cat.append(i)\n\noverfit_cat = list(overfit_cat)\nX = X.drop(overfit_cat, axis=1)","72641fc9":"num_col = X.select_dtypes(exclude=['object']).drop(['MSSubClass'], axis=1).columns\noverfit_num = []\nfor i in num_col:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(X) * 100 > 90:\n        overfit_num.append(i)\n\noverfit_num = list(overfit_num)\nX = X.drop(overfit_num, axis=1)","8f97ffe8":"pd.DataFrame(X.isnull().sum(), columns=['sum']).sort_values(by=['sum'],ascending=False).head(20)","8d384f43":"cat = ['GarageType','GarageFinish','BsmtFinType2','BsmtExposure','BsmtFinType1', \n       'GarageQual','BsmtCond','BsmtQual','FireplaceQu','Fence',\"KitchenQual\",\n       \"HeatingQC\",'ExterQual','ExterCond']\n\nX[cat] = X[cat].fillna(\"NA\")","c6ad9a24":"cols = [\"MasVnrType\", \"MSZoning\", \"Exterior1st\", \"Exterior2nd\", \"SaleType\"]\nX[cols] = X.groupby(\"Neighborhood\")[cols].transform(lambda x: x.fillna(x.mode()[0]))","4a9dc4d7":"print(\"Mean of LotFrontage: \", X['LotFrontage'].mean())\nprint(\"Mean of GarageArea: \", X['GarageArea'].mean())\n","d1b6de91":"neigh_lot = X.groupby('Neighborhood')['LotFrontage'].mean().reset_index(name='LotFrontage_mean')\nneigh_garage = X.groupby('Neighborhood')['GarageArea'].mean().reset_index(name='GarageArea_mean')\n\nfig, axes = plt.subplots(1,2,figsize=(22,8))\naxes[0].tick_params(axis='x', rotation=90)\nsns.barplot(x='Neighborhood', y='LotFrontage_mean', data=neigh_lot, ax=axes[0])\naxes[1].tick_params(axis='x', rotation=90)\nsns.barplot(x='Neighborhood', y='GarageArea_mean', data=neigh_garage, ax=axes[1])\n","63af0751":"\nX['LotFrontage'] = X.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.mean()))\nX['GarageArea'] = X.groupby('Neighborhood')['GarageArea'].transform(lambda x: x.fillna(x.mean()))\nX['MSZoning'] = X.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\n\ncont = [ \"BsmtFullBath\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\",  \"MasVnrArea\"]\nX[cont] = X[cont] = X[cont].fillna(X[cont].mean())","01e9d84b":"X['MSSubClass'] = X['MSSubClass'].apply(str)","ed0418e0":"X.isnull().sum().max()","a2912183":"ordinal_map = {'Ex': 5,'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA':0}\nfintype_map = {'GLQ': 6,'ALQ': 5,'BLQ': 4,'Rec': 3,'LwQ': 2,'Unf': 1, 'NA': 0}\nexpose_map = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0}\nfence_map = {'GdPrv': 4,'MnPrv': 3,'GdWo': 2, 'MnWw': 1,'NA': 0}","0af5cd30":"ord_col = ['ExterQual','ExterCond','BsmtQual', 'BsmtCond','HeatingQC','KitchenQual','GarageQual', 'FireplaceQu']\nfor col in ord_col:\n    X[col] = X[col].map(ordinal_map)\n    \nfin_col = ['BsmtFinType1','BsmtFinType2']\nfor col in fin_col:\n    X[col] = X[col].map(fintype_map)\n\nX['BsmtExposure'] = X['BsmtExposure'].map(expose_map)\nX['Fence'] = X['Fence'].map(fence_map)","32ddfe80":"out_col = ['LotFrontage','LotArea','BsmtFinSF1']\nfig = plt.figure(figsize=(20,5))\nfor index,col in enumerate(out_col):\n    plt.subplot(1,5,index+1)\n    sns.boxplot(y=col, data=X)\nfig.tight_layout(pad=1.5)","c78ee46e":"train = train.drop(train[train['LotFrontage'] > 200].index)\ntrain = train.drop(train[train['LotArea'] > 100000].index)\ntrain = train.drop(train[train['BsmtFinSF1'] > 4000].index)\ntrain = train.drop(train[train['TotalBsmtSF'] > 5000].index)\ntrain = train.drop(train[train['GrLivArea'] > 4000].index)\n","c49ab4e6":"X['TotalLot'] = X['LotFrontage'] + X['LotArea']\nX['TotalBsmtFin'] = X['BsmtFinSF1'] + X['BsmtFinSF2']\nX['TotalBath'] = X['FullBath'] + X['HalfBath']\nX['TotalPorch'] = X['OpenPorchSF'] + X['EnclosedPorch']","315cc107":"columns = ['MasVnrArea','TotalBsmtFin','2ndFlrSF','WoodDeckSF','TotalPorch']\n\nfor col in columns:\n    col_name = col+'_bin'\n    X[col_name] = X[col].apply(lambda x: 1 if x > 0 else 0)","f7b3a01e":"X = pd.get_dummies(X)","5a66c125":"from sklearn.preprocessing import RobustScaler\n\ncols = X.select_dtypes(np.number).columns\nX[cols] = RobustScaler().fit_transform(X[cols])","7a577b5a":"sns.distplot(train['SalePrice'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)","028720b1":"y[\"SalePrice\"] = np.log(y['SalePrice'])","b20b4caa":"x = X.loc[train.index]\ny = y.loc[train.index]\ntest = X.loc[test.index]","bccfddd8":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=1)","cc3e3bd8":"from xgboost import XGBRegressor\nmy_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\nmy_model.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_val, y_val)])","5638cd39":"print (\"R^2 is: \\n\", my_model.score(X_val, y_val))","4224c588":"predictions = my_model.predict(X_val)","1e6f9e3d":"from sklearn.metrics import mean_squared_error\nprint ('RMSE is: \\n', mean_squared_error(y_val, predictions))","8b9a490f":"final_predictions = my_model.predict(test)\nfinal_predictions = np.exp(final_predictions)","954d0d01":"print (\"Original predictions are: \\n\", predictions[:5], \"\\n\")\nprint (\"Final predictions are: \\n\", final_predictions[:5])","56668c20":"submission = pd.DataFrame({'Id': test.index,\n                           'SalePrice': final_predictions})\n\nsubmission.to_csv(\"submission1.csv\", index=False)","397b17fe":"### SalePrice Correlation Matrix:","e942691b":"From the analysis earlier we found some features with outliers, after deleting the missing values only 3 features remain which we will deal with now.","5e6de093":"## Missing Data:","0b6b7cfe":"If you have made it this far then thanks for taking the time and reading this! Also any feedback on how to possibly improve it is very appreciated!\n\nResources:\n\nhttps:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python#Conclusion\n\nhttps:\/\/www.kaggle.com\/angqx95\/data-science-workflow-v2-top-2\/notebook#Table-of-Contents\n\nhttps:\/\/www.dataquest.io\/blog\/kaggle-getting-started\/","d565142f":"# 2. Handling The Data:","09a98b6b":"From this heatmap, these are the correlated features we found:\n\n-With 'SalePrice' we have 'OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea', 'TotalBsmtSF', '1stFlrSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt' and 'YearRemodAdd'\n\n-'YearBuilt' and 'GarageYrBlt'\n\n-'GarageArea' and 'GarageCars'\n\n-'1stFlrSF' and 'TotalBsmtSF'\n\n-'TotRmsAbvGrd' and 'GrLivArea'","f4968888":"Now we will replace the NA in the previous ordinal features.","32ec04b4":"Finally converting categorical features to one hot features.","07fec224":"## Transforming Some Graphs Using Log:","bd1aa032":"### Relations With Categorical Features:","28586e7b":"### Numerical Features:","7c60d4d4":"Twin features(>=0.8):\n\n-'YearBuilt' and 'GarageYrBlt'\n\n-'GarageArea' and 'GarageCars'\n\n-'1stFlrSF' and 'TotalBsmtSF'\n\n-'TotRmsAbvGrd' and 'GrLivArea'\n","d67539b7":"Some categorical features mostly have one value which won't help us in our predictions and we will drop them later on.","9bce9f91":"### Relations With Numerical Features:","90ea96d3":"We will add MSSubClass into the categorical features.","bb5e9a1a":"### Scatterplot between 'SalePrice' and the highly correlated features with it.","a9b13fd5":"Some numerical values are like categorical ones but with numbers so we will split our numerical features into categorical ones and normal ones.","41c6a072":"### Features With A Lot Of Missing Values","6e37dd12":"Earlier we noticed some twin features and that's not good for our model therefore we will be dropping one feature from each twin.","41419241":"We will be filling the ordinal features with 'NA' and then replace them later with encoding.","10665453":"Now it's time to handle the missing data.","5c62a954":"We will create binary columns.","b6ddce14":"## Analysing SalePrice:","e6094194":"Now we will split the data into numerical and categorical features.","b60efc10":"## Scaling the data:","e3b9a109":"Now after analysing the missing data and deciding on its importance I decided to delete all features with missing values apart from the ones with just 1 missing value for which I will h=just delete that particular feature.","7edd94df":"We will create new features like getting the number of all baths in the house.","72701e68":"We will fill the categorical features with their most occuring values.","1ba78375":"### Correlation Matrix:","f86d0267":"# 3. Feature Engineering:\n\n\n\n","71885400":"I will be using the xgboost model to get our predictions.","addc5752":"## Correlation:","522a8641":"### Mapping Some Features:","c8971039":"Dropping the features that mostly have 1 value in other words one value is repeated more than 90% in that feature.","f69476d0":"These are features that don't have any clear relation with our target and won't help us in anyway.","bd5d1c99":"### Features With Mostly 1 Value:","f4219350":"## Removing Some Features:","c03fabeb":"# 4. Modeling:","458197d4":"### Categorical Features:","70442ea3":"We can see a lot of columns with mostly 0 values which will just mess up our model's predictions instead of helping it therefore we will remove them later on.","06c2b19d":"# 1. Understanding The Data:","5cc1dadb":"### High Correlated Features:","1b31db7d":"From that we can see that house prices range from 34900 to 755000","f6a986f2":"## Next is just the scoring system and submitting.","530dec16":"We will fill them by their mean apart from LotFrontage and GarageArea which which take mean values across Neighborhood","e68033b2":"### Ordinal Features:","54816a4f":"### Useless Features:","01bdf11e":"Since it's like a categorical feature we change its type to str.","0d95644d":"## Analysing Categorical Features:","fdcead73":"## Outliers:","eaee0a87":"## Splitting the data into train-validation","7e28a964":"From the first and last 5 rows we can see that our data is filled with a lot of missing values.","eaf189b3":"Since just filling them won't help our predictions we will be dropping the features with more than 90% of missing data.","c3ca335f":"## Analysing Numerical Features:"}}