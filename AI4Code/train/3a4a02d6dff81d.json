{"cell_type":{"edf109f9":"code","83e4ab77":"code","8630f21a":"code","c1d31e56":"code","73fbf8ce":"code","0d9a50c0":"code","278987c1":"code","c06cb23e":"code","1149277c":"code","f86f2f75":"code","056e24f1":"code","edfc62da":"code","b66ba657":"code","5f362fdc":"code","4963faa0":"code","99420b48":"markdown","846eedf1":"markdown","a4c53ee1":"markdown","dbfac7a5":"markdown","1254e2e6":"markdown","e872abc7":"markdown","e6d1ddf5":"markdown","d0a916db":"markdown","c629e123":"markdown","88605d3b":"markdown","9453f747":"markdown","543ecad6":"markdown"},"source":{"edf109f9":"#import all the necessary packages.\nimport os\nimport re\nimport nltk\nimport time\nimport warnings\nimport numpy as np\nimport pandas as pd \nfrom PIL import Image\nimport seaborn as sns\nfrom matplotlib import gridspec\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer \nwarnings.filterwarnings(\"ignore\")","83e4ab77":"#Load data\ndata = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')","8630f21a":"print('Number of data points : ', data.shape[0])\nprint('Number of features\/variables:', data.shape[1])\nprint('Feature names : ', data.columns.tolist())","c1d31e56":"print(data['title'].describe())","73fbf8ce":"# find the 10 most frequent titles.\ntitle_count = Counter(list(data['title']))\ntitle_count.most_common(10)","0d9a50c0":"# find number of products that have duplicate titles.\nprint(sum(data.duplicated('title')))\n#we have 1133 products which have same title.","278987c1":"# Remove All products with very few words in title\ndata_sorted = data[data['title'].apply(lambda x: len(x.split())>4)]\nprint(\"After removal of products with short description:\", data_sorted.shape[0])","c06cb23e":"# Sort the whole data based on title (alphabetical order of title) \ndata_sorted.sort_values('title',inplace=True, ascending=False)\ndata_sorted.head()","1149277c":"# we use the list of stop words that are downloaded from nltk lib.\nstop_words = set(stopwords.words('english'))\n\ndef nlp_preprocessing(total_text, index, column):\n    if type(total_text) is not int:\n        string = \"\"\n        for words in total_text.split():\n            # remove the special chars in review like '\"#$@!%^&*()_+-~?>< etc.\n            word = (\"\".join(e for e in words if e.isalnum()))\n            # Conver all letters to lower-case\n            word = word.lower()\n            # stop-word removal\n            if not word in stop_words:\n                string += word + \" \"\n        data[column][index] = string","f86f2f75":"start_time = time.clock()\n# we take each title and we text-preprocess it.\nfor index, row in data.iterrows():\n    nlp_preprocessing(row['title'], index, 'title')\n# we print the time it took to preprocess whole titles \nprint(time.clock() - start_time, \"seconds\")","056e24f1":"# Utility Functions which we will use through the rest of this notebook\n#Display an image\ndef display_img(img_id,ax,fig):\n    images_path = '..\/input\/shopee-product-matching\/train_images\/'\n    img = Image.open(images_path + img_id)\n    plt.figure(figsize=(3,3))\n    # we will display it in notebook \n    plt.imshow(img, interpolation='nearest',aspect='auto')\n    \n#plotting code to understand the algorithm's decision.\ndef plot_heatmap(keys, values, labels, img_id, text):\n        gs = gridspec.GridSpec(2, 2, width_ratios=[8,1], height_ratios=[8,1]) \n        fig = plt.figure(figsize=(20,3))\n        #1st plotting image\n        ax = plt.subplot(gs[0])\n        ax = sns.heatmap(np.array([values]), annot=np.array([labels]))\n        ax.set_xticklabels(keys) # set that axis labels as the words of title\n        ax.set_title(text) # product title\n        \n        # 2nd, plotting image \n        ax = plt.subplot(gs[1])\n        # we don't want any grid lines for image and no labels on x-axis and y-axis\n        ax.grid(False)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        \n        # we call dispaly_img based with parameter\n        display_img(img_id,ax, fig)\n        \n        # displays combine figure ( heat map and image together)\n        plt.show()\n    \ndef plot_heatmap_image(doc_id, vec1, vec2, img_id, text, model):\n\n    # doc_id : index of the title1\n    # vec1 : input product's vector, it is of a dict type {word:count}\n    # vec2 : recommended product's vector, it is of a dict type {word:count}\n    # img_id : image path\n    # text: title of recomonded producr (used to keep title of image)\n    # model, it can be any of the models, \n        # 1. bag_of_words\n        # 2. tfidf\n        \n\n    intersection = set(vec1.keys()) & set(vec2.keys()) \n    for i in vec2:\n        if i not in intersection:\n            vec2[i]=0\n\n    # for labeling heatmap, keys contains list of all words in title2\n    keys = list(vec2.keys())\n    #  if ith word in intersection(lis of words of title1 and list of words of title2): values(i)=count of that word in title2 else values(i)=0 \n    values = [vec2[x] for x in vec2.keys()]\n    \n    # labels: len(labels) == len(keys), the values of labels depends on the model we are using\n        # if model == 'bag of words': labels(i) = values(i)\n        # if model == 'tfidf weighted bag of words':labels(i) = tfidf(keys(i))\n        # if model == 'idf weighted bag of words':labels(i) = idf(keys(i))\n\n    if model == 'bag_of_words':\n        labels = values\n    elif model == 'tfidf':\n        labels = []\n        for x in vec2.keys():\n            # tfidf_title_vectorizer.vocabulary_ it contains all the words in the corpus\n            # tfidf_title_features[doc_id, index_of_word_in_corpus] will give the tfidf value of word in given document (doc_id)\n            if x in  tfidf_title_vectorizer.vocabulary_:\n                labels.append(tfidf_title_features[doc_id, tfidf_title_vectorizer.vocabulary_[x]])\n            else:\n                labels.append(0)\n    elif model == 'idf':\n        labels = []\n        for x in vec2.keys():\n            # idf_title_vectorizer.vocabulary_ it contains all the words in the corpus\n            # idf_title_features[doc_id, index_of_word_in_corpus] will give the idf value of word in given document (doc_id)\n            if x in  idf_title_vectorizer.vocabulary_:\n                labels.append(idf_title_features[doc_id, idf_title_vectorizer.vocabulary_[x]])\n            else:\n                labels.append(0)\n\n    plot_heatmap(keys, values, labels, img_id, text)\n\n\n# this function gets a list of wrods along with the frequency of each \n# word given \"text\"\ndef text_to_vector(text):\n    word = re.compile(r'\\w+')\n    words = word.findall(text)\n    # words stores list of all words in given string, you can try 'words = text.split()' this will also gives same result\n    return Counter(words) # Counter counts the occurence of each word in list, it returns dict type object {word1:count}\n\n\n\ndef get_result(doc_id, content_a, content_b, img_id, model):\n    text1 = content_a\n    text2 = content_b\n    \n    # vector1 = dict{word11:#count, word12:#count, etc.}\n    vector1 = text_to_vector(text1)\n\n    # vector1 = dict{word21:#count, word22:#count, etc.}\n    vector2 = text_to_vector(text2)\n\n    plot_heatmap_image(doc_id, vector1, vector2, img_id, text2, model)","edfc62da":"title_vectorizer = CountVectorizer()\ntitle_features   = title_vectorizer.fit_transform(data['title'])\ntitle_features.get_shape() ","b66ba657":"def bag_of_words_model(doc_id, num_results):\n    # doc_id: product id in given corpus\n    \n    # pairwise_dist will store the distance from given input product to all remaining products\n    # the metric we used here is cosine, the coside distance is mesured as K(X, Y) = <X, Y> \/ (||X||*||Y||)\n    # http:\/\/scikit-learn.org\/stable\/modules\/metrics.html#cosine-similarity\n    pairwise_dist = pairwise_distances(title_features,title_features[doc_id])\n    \n    # np.argsort will return indices of the smallest distances\n    indices = np.argsort(pairwise_dist.flatten())[0:num_results]\n    #pdists will store the smallest distances\n    pdists  = np.sort(pairwise_dist.flatten())[0:num_results]\n\n    #data frame indices of the 9 smallest distace's\n    df_indices = list(data.index[indices])\n    \n    for i in range(0,len(indices)):\n        # we will pass 1. product_title, 2. title1, 3. title2, img_id, model\n        get_result(indices[i],data['title'].loc[df_indices[0]], data['title'].loc[df_indices[i]],  data['image'].loc[df_indices[i]], 'bag_of_words')\n        print ('Euclidean similarity with the query image :', pdists[i])\n        print('='*60)\n        \n\n#call the bag-of-words model for a product to get similar products.\nbag_of_words_model(7, 20) # change the index if you want to.\n# In the output heat map each value represents the count value \n# of the label word, the color represents the intersection \n# with inputs title.\n#you can try differnt product_id's to finid similar products:)","5f362fdc":"tfidf_title_vectorizer = TfidfVectorizer(min_df = 0)\ntfidf_title_features = tfidf_title_vectorizer.fit_transform(data['title'])","4963faa0":"def tfidf_model(doc_id, num_results):\n    # doc_id: product's id in given corpus\n    \n    # pairwise_dist will store the distance from given input product to all remaining products\n    # the metric we used here is cosine, the coside distance is mesured as K(X, Y) = <X, Y> \/ (||X||*||Y||)\n    # http:\/\/scikit-learn.org\/stable\/modules\/metrics.html#cosine-similarity\n    pairwise_dist = pairwise_distances(tfidf_title_features,tfidf_title_features[doc_id])\n\n    # np.argsort will return indices of 9 smallest distances\n    indices = np.argsort(pairwise_dist.flatten())[0:num_results]\n    #pdists will store the 9 smallest distances\n    pdists  = np.sort(pairwise_dist.flatten())[0:num_results]\n\n    #data frame indices of the 9 smallest distace's\n    df_indices = list(data.index[indices])\n\n    for i in range(0,len(indices)):\n        # we will pass 1. product_title, 2. title1, 3. title2, img_id, model\n        get_result(indices[i], data['title'].loc[df_indices[0]], data['title'].loc[df_indices[i]], data['image'].loc[df_indices[i]], 'tfidf')\n        print ('Eucliden distance from the given image :', pdists[i])\n        print('='*60)\ntfidf_model(7, 20)\n# in the output heat map each value represents the tfidf values of the label word, the color represents the intersection with inputs title","99420b48":"## TF-IDF Observations:\n* **we picked the same query product title which was used earlier for BoW and applied TF-IDF algorithm and the results were improved compared to Bow model(expected).**\n* **we can see few new recommendations in TF-IDF and the ranking has been improved for some of the items.**\n* **Few of the duplicated items are repeated in the top recommendation which we will handle them later on**\n","846eedf1":"  ## Thanks for reading, will keep posting more visualizations with diferent techniques.\n  ## Stay Tuned and upvote if you like this notebook","a4c53ee1":"## BoW Observations:\n*  **A simple BoW model was able to give similar products for the query product title, Interesting observation was the most of the recommended similar products(Fashion wear) has the same pattern style wear**\n* **All the recommended items are related to fashion for women and was able to rightly pick clothing**","dbfac7a5":"# 4.Text Preprocessing","1254e2e6":"# 2.Libraries and Load Dataset\n* All the necessary libraries are imported and we are working with train dataset only as our objective is to visualize the text modelling outputs.","e872abc7":"# Basic EDA Stats for the feature : title\n* our focus will be on title column as we are going to apply techniques like BoW,TF-IDF....etc in this notebook","e6d1ddf5":"# 6. TF-IDF on Product titles","d0a916db":"# 5.Bag of Words (BoW) on product titles.","c629e123":"### Understanding Utility Functions \n* we will divide the whole figure into two parts.\n* 1st, plotting heat map that represents the count of commonly ocurred words in title2.\n* It displays a cell in white color if the word is intersection(list of words of title1 and list of words of title2), in black if not.\n* 2nd, we plot the corresponding image\n*  we find the common words in both titles, because these only words contribute to the distance between two title vec's.\n* we set the values of non intersecting words to zero, this is just to show the difference in heatmap.","88605d3b":"![Shopee-Logo-Branding-in-Asia.jpg](attachment:Shopee-Logo-Branding-in-Asia.jpg)","9453f747":"# 1.Data Description\n\nFinding near-duplicates in large datasets is an important problem for many online businesses. In Shopee's case, everyday users can upload their own images and write their own product descriptions, adding an extra layer of challenge. Your task is to identify which products have been posted repeatedly. The differences between related products may be subtle while photos of identical products may be wildly different!\n\n[train\/test].csv - the training set metadata. Each row contains the data for a single posting. Multiple postings might have the exact same image ID, but with different titles or vice versa.\n\n    posting_id - the ID code for the posting.\n\n    image - the image id\/md5sum.\n\n    image_phash - a perceptual hash of the image.\n\n    title - the product description for the posting.\n\n    label_group - ID code for all postings that map to the same product. Not provided for the test set.\n","543ecad6":"# Content of this Notebook:\n*     This Notebook focuses mostly on visual representation of the text modelling output.\n*     This notebook has preliminary visual outputs for Bag of Words(BoW),TF-IDF to find similarity in product titles and it will be updated frequently so do check regulary for new developments.\n* Any feedback or feature requests are welcome. \n\n\n**Table of Contents:**\n1. Data Description\n2. Libraries and load Dataset\n3. Basic EDA\n4. Text Preprocessing\n5. Bag Of Words(BoW)\n6. TF-IDF\n"}}