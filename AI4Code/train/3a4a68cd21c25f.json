{"cell_type":{"54c413a1":"code","0cccca41":"code","50ed6642":"code","88da704b":"code","f2fdd626":"code","932b5a96":"code","96f949f6":"code","94115f41":"code","251909a1":"code","8bbb88ee":"code","ac22378f":"code","64af82ae":"code","fd171663":"code","437bdc5a":"code","698e56ba":"code","9550f4b1":"code","e4c0f3b5":"code","40e3ca48":"code","55ccdb47":"code","570a5fb6":"code","e7983e54":"code","421c85ef":"code","7dfc9a7d":"code","6da4baf3":"code","2d1a95c4":"code","32c892e3":"code","16a51224":"code","73d2da5b":"code","ec0407b4":"code","fb2394cf":"code","00547012":"code","391ea12a":"code","34128d86":"code","a0a5a575":"code","36659230":"code","dc412d49":"code","e6b2e5c1":"code","3a56f9d6":"code","92372bf0":"code","8c93951a":"code","d42e48b9":"code","9c355398":"code","b05ecc59":"code","ca5c9b3a":"markdown","3920e234":"markdown","bbc8eed1":"markdown","eb0a99a6":"markdown","540a3fbc":"markdown","7a2cbdd5":"markdown","5e986044":"markdown","f9a836f6":"markdown","5e4e2e32":"markdown","3d59e83c":"markdown","9b57063c":"markdown","2763406f":"markdown","ce2b0173":"markdown","16a99326":"markdown","4e8fe371":"markdown","4a5ee881":"markdown","e5bc2601":"markdown","24f69349":"markdown","4760828f":"markdown","6aab2094":"markdown","3ce21542":"markdown","6363d66e":"markdown","d0b9d978":"markdown","0542a0ca":"markdown","610e9780":"markdown"},"source":{"54c413a1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0cccca41":"!pip install pyunpack\n!pip install patool\n!pip install py7zr","50ed6642":"from py7zr import unpack_7zarchive\nimport shutil\n\nshutil.register_unpack_format('7zip', ['.7z'], unpack_7zarchive)\nshutil.unpack_archive('\/kaggle\/input\/tensorflow-speech-recognition-challenge\/train.7z', '\/kaggle\/working\/tensorflow-speech-recognition-challenge\/train\/')","88da704b":"# from pyunpack import Archive\n# import shutil\n# if not os.path.exists('\/kaggle\/working\/tensorflow-speech-recognition-challenge\/train\/'):\n#     os.makedirs('\/kaggle\/working\/tensorflow-speech-recognition-challenge\/train\/')\n# Archive('\/kaggle\/input\/tensorflow-speech-recognition-challenge\/train.7z').extractall('\/kaggle\/working\/tensorflow-speech-recognition-challenge\/train\/')\n\nfor dirname, _, filenames in os.walk('\/kaggle\/working\/tensorflow-speech-recognition-challenge\/train\/train\/audio'):\n    for filename in filename[:5]:\n        print(os.path.join(dirname, filename))","f2fdd626":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport librosa\nimport IPython.display as ipd\nfrom scipy.io import wavfile","932b5a96":"train_audio_path = '\/kaggle\/working\/tensorflow-speech-recognition-challenge\/train\/train\/audio\/'","96f949f6":"samples, sample_rate = librosa.load(train_audio_path+'yes\/0a7c2a8d_nohash_0.wav', sr = 16000)\nfig = plt.figure(figsize=(14, 8))\nax1 = fig.add_subplot(211)\nax1.set_title('Raw wave of ' + '..\/input\/train\/audio\/yes\/0a7c2a8d_nohash_0.wav')\nax1.set_xlabel('time')\nax1.set_ylabel('Amplitude')\nax1.plot(np.linspace(0, sample_rate\/len(samples), sample_rate), samples)","94115f41":"samples, sample_rate = librosa.load(train_audio_path+'yes\/0a7c2a8d_nohash_0.wav', sr = 100)\nfig = plt.figure(figsize=(14, 8))\nax1 = fig.add_subplot(211)\nax1.set_title('Raw wave of ' + '..\/input\/train\/audio\/yes\/0a7c2a8d_nohash_0.wav')\nax1.set_xlabel('time')\nax1.set_ylabel('Amplitude')\nax1.plot(np.linspace(0, sample_rate\/len(samples), sample_rate), samples)","251909a1":"sample_rate=40000\nsamples, sample_rate = librosa.load(train_audio_path+'yes\/0a7c2a8d_nohash_0.wav', sr = sample_rate)\nipd.Audio(samples, rate=sample_rate)","8bbb88ee":"sample_rate=16000\nsamples, sample_rate = librosa.load(train_audio_path+'yes\/0a7c2a8d_nohash_0.wav', sr = sample_rate)\nipd.Audio(samples, rate=sample_rate)","ac22378f":"sample_rate=6000\nsamples, sample_rate = librosa.load(train_audio_path+'yes\/0a7c2a8d_nohash_0.wav', sr = sample_rate)\nipd.Audio(samples, rate=sample_rate)","64af82ae":"sample_rate=3000\nsamples, sample_rate = librosa.load(train_audio_path+'yes\/0a7c2a8d_nohash_0.wav', sr = sample_rate)\nipd.Audio(samples, rate=sample_rate)","fd171663":"samples = librosa.resample(samples, sample_rate, 8000)\nipd.Audio(samples, rate=8000)","437bdc5a":"labels=os.listdir(train_audio_path)","698e56ba":"#find count of each label and plot bar graph\nno_of_recordings=[]\nfor label in labels:\n    waves = [f for f in os.listdir(train_audio_path + '\/'+ label) if f.endswith('.wav')]\n    no_of_recordings.append(len(waves))\n    \n#plot\nplt.figure(figsize=(30,5))\nindex = np.arange(len(labels))\nplt.bar(index, no_of_recordings)\nplt.xlabel('Commands', fontsize=12)\nplt.ylabel('No of recordings', fontsize=12)\nplt.xticks(index, labels, fontsize=15, rotation=60)\nplt.title('No. of recordings for each command')\nplt.show()","9550f4b1":"# labels=[\"yes\", \"no\",\"stop\", \"go\"]\nlabels=[\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\"]","e4c0f3b5":"duration_of_recordings=[]\nfor label in labels:\n    waves = [f for f in os.listdir(train_audio_path + '\/'+ label) if f.endswith('.wav')]\n    for wav in waves:\n        sample_rate, samples = wavfile.read(train_audio_path + '\/' + label + '\/' + wav)\n        duration_of_recordings.append(float(len(samples)\/sample_rate))\n    \nplt.hist(np.array(duration_of_recordings))","40e3ca48":"all_wave = []\nall_label = []\nfor label in labels:\n    print(label)\n    waves = [f for f in os.listdir(train_audio_path + '\/'+ label) if f.endswith('.wav')]\n    for wav in waves:\n        samples, sample_rate = librosa.load(train_audio_path + '\/' + label + '\/' + wav, sr = 16000)\n        samples = librosa.resample(samples, sample_rate, 8000)\n        if(len(samples)== 8000) : \n            all_wave.append(samples)\n            all_label.append(label)","55ccdb47":"print(type(all_wave))\nprint(len(all_wave))","570a5fb6":"print(len(all_wave))\nprint(all_wave[1].shape)\nall_wave[1]","e7983e54":"len(set(all_label))","421c85ef":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny=le.fit_transform(all_label)\nclasses= list(le.classes_)","7dfc9a7d":"classes","6da4baf3":"from keras.utils import np_utils\ny=np_utils.to_categorical(y, num_classes=len(labels))","2d1a95c4":"y","32c892e3":"all_wave = np.array(all_wave).reshape(-1,8000,1)","16a51224":"all_wave.shape","73d2da5b":"from sklearn.model_selection import train_test_split\nx_tr, x_val, y_tr, y_val = train_test_split(np.array(all_wave),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)","ec0407b4":"from keras.layers import Dense, Dropout, Flatten, Conv1D, Input, MaxPooling1D\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import backend as K\nK.clear_session()","fb2394cf":"# ?Conv1D","00547012":"inputs = Input(shape=(8000,1))\n\n#First Conv1D layer\nconv = Conv1D(filters=8,kernel_size=13, padding='valid', activation='relu', strides=1)(inputs)\nconv = MaxPooling1D(3)(conv)\nconv = Dropout(0.3)(conv)\n\n#Second Conv1D layer\nconv = Conv1D(16, 11, padding='valid', activation='relu', strides=1)(conv)\nconv = MaxPooling1D(3)(conv)\nconv = Dropout(0.3)(conv)\n\n#Third Conv1D layer\nconv = Conv1D(32, 9, padding='valid', activation='relu', strides=1)(conv)\nconv = MaxPooling1D(3)(conv)\nconv = Dropout(0.3)(conv)\n\n#Fourth Conv1D layer\nconv = Conv1D(64, 7, padding='valid', activation='relu', strides=1)(conv)\nconv = MaxPooling1D(3)(conv)\nconv = Dropout(0.3)(conv)\n\n#Flatten layer\nconv = Flatten()(conv)\n\n#Dense Layer 1\nconv = Dense(256, activation='relu')(conv)\nconv = Dropout(0.3)(conv)\n\n#Dense Layer 2\nconv = Dense(128, activation='relu')(conv)\nconv = Dropout(0.3)(conv)\n\noutputs = Dense(len(labels), activation='softmax')(conv)\n\nmodel = Model(inputs, outputs)\nmodel.summary()","391ea12a":"model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])","34128d86":"es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, min_delta=0.0001) \nmc = ModelCheckpoint('best_model.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')","a0a5a575":"history=model.fit(x_tr, y_tr ,epochs=100, callbacks=[es,mc], batch_size=32, validation_data=(x_val,y_val))","36659230":"from matplotlib import pyplot\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()","dc412d49":"from keras.models import load_model\nmodel.save(\"SpeechRecogModel.h5\")\n#model=load_model('\/kaggle\/working\/best_model.hdf5')","e6b2e5c1":"def predict(audio):\n    prob=model.predict(audio.reshape(1,8000,1))\n    index=np.argmax(prob[0])\n    return classes[index]","3a56f9d6":"import random\nindex=random.randint(0,len(x_val)-1)\nsamples=x_val[index].ravel()\nprint(\"Audio:\",classes[np.argmax(y_val[index])])\nipd.Audio(samples, rate=8000)","92372bf0":"print(\"Text:\",predict(samples))","8c93951a":"# # ! pip install sounddevice==0.2.1\n\n# import sounddevice as sd\n# import soundfile as sf","d42e48b9":"# samplerate = 16000  \n# duration = 1 # seconds\n# filename = 'yes.wav'\n# print(\"start\")\n# mydata = sd.rec(int(samplerate * duration), samplerate=samplerate,channels=1, blocking=True)\n# print(\"end\")\n# sd.wait()\n# sf.write(filename, mydata, samplerate)","9c355398":"# #reading the voice commands\n# samples, sample_rate = librosa.load(filepath + '\/' + 'stop.wav', sr = 16000)\n# samples = librosa.resample(samples, sample_rate, 8000)\n# ipd.Audio(samples,rate=8000)              ","b05ecc59":"# #converting voice commands to text\n# predict(samples)","ca5c9b3a":"**The best part is yet to come! Here is a script that prompts a user to record voice commands. Record your own voice commands and test it on the model:**","3920e234":"**Resampling**\n\nFrom the above, we can understand that the sampling rate of the signal is 16000 hz. Let us resample it to 8000 hz since most of the speech related frequencies are present in 8000z ","bbc8eed1":"Let us now read the saved voice command and convert it to text:","eb0a99a6":"**Sampling rate**\n\nLet us now look at the sampling rate of the audio signals","540a3fbc":"**Preprocessing the audio waves**\n\nIn the data exploration part earlier, we have seen that the duration of a few recordings is less than 1 second and the sampling rate is too high. So, let us read the audio waves and use the below-preprocessing steps to deal with this.\n\nHere are the two steps we\u2019ll follow:\n\n* Resampling\n* Removing shorter commands of less than 1 second\n\nLet us define these preprocessing steps in the below code snippet:","7a2cbdd5":"Reshape the 2D array to 3D since the input to the conv1d must be a 3D array:","5e986044":"Prediction time! Make predictions on the validation data:","f9a836f6":"**Split into train and validation set**\n\nNext, we will train the model on 80% of the data and validate on the remaining 20%:\n","5e4e2e32":"**Duration of recordings**\n\nWhat\u2019s next? A look at the distribution of the duration of recordings:","3d59e83c":"**Model building**\n\nLet us implement the model using Keras functional API.","9b57063c":"Define the loss function to be categorical cross-entropy since it is a multi-classification problem:","2763406f":"Now, let\u2019s understand the number of recordings for each voice command:","ce2b0173":"Convert the output labels to integer encoded:","16a99326":"**Diagnostic plot**\n\nI\u2019m going to lean on visualization again to understand the performance of the model over a period of time:","4e8fe371":"Early stopping and model checkpoints are the callbacks to stop training the neural network at the right time and to save the best model after every epoch:","4a5ee881":"# <center> Implementing the Speech-to-Text Model in Python\n**Understanding the Problem Statement for our Speech-to-Text Project**\n\nLet\u2019s understand the problem statement of our project before we move into the implementation part.\n\nWe might be on the verge of having too many screens around us. It seems like every day, new versions of common objects are \u201cre-invented\u201d with built-in wifi and bright touchscreens. A promising antidote to our screen addiction is voice interfaces. \n\n__You can download the dataset from__ [here](https:\/\/www.kaggle.com\/c\/tensorflow-speech-recognition-challenge).\n    \nTensorFlow recently released the Speech Commands Datasets. It includes 65,000 one-second long utterances of 30 short words, by thousands of different people. We\u2019ll build a speech recognition system that understands simple spoken commands. <br>    \n__Reference Aticle__ [Click here](https:\/\/www.analyticsvidhya.com\/blog\/2019\/07\/learn-build-first-speech-to-text-model-python\/) to understand the basics of signal processing prior implementing the speech to text.","e5bc2601":"### Conv1D, Conv2D and Conv3D : https:\/\/xzz201920.medium.com\/conv1d-conv2d-and-conv3d-8a59182c4d6","24f69349":"**Loading the best model**","4760828f":"**Model Architecture for this problem**\n\nWe will build the speech-to-text model using conv1d. Conv1d is a convolutional neural network which performs the convolution along only one dimension. ","6aab2094":"Let us train the model on a batch size of 32 and evaluate the performance on the holdout set:","3ce21542":"**Import the libraries**\n\nFirst, import all the necessary libraries into our notebook. LibROSA and SciPy are the Python libraries used for processing audio signals.","6363d66e":"Define the function that predicts text for the given audio:","d0b9d978":"Now, convert the integer encoded labels to a one-hot vector since it is a multi-classification problem:","0542a0ca":"**Congratulations! You have just built your very own speech-to-text model!**","610e9780":"**Data Exploration and Visualization**\n\nData Exploration and Visualization helps us to understand the data as well as pre-processing steps in a better way. \n\n**Visualization of Audio signal in time series domain**\n\nNow, we\u2019ll visualize the audio signal in the time series domain:"}}