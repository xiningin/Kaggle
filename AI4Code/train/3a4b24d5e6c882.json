{"cell_type":{"c3685f4e":"code","74183031":"code","6c384731":"code","73558d32":"code","addbf759":"code","7b6012d7":"code","debbaf91":"code","ac705164":"code","a3222cf4":"code","6ce1a858":"code","957ee2fb":"code","6dd558c4":"code","c9b2c27d":"code","06e58834":"markdown","9d3da006":"markdown","fd3f4f8c":"markdown","80bc63b4":"markdown"},"source":{"c3685f4e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","74183031":"#Let's import the data into this Kaggle workspace first.\nExasensData = \"\/kaggle\/input\/Exasens.csv\"\nExasensDF = pd.read_csv(\"..\/input\/exasens-data-set\/Exasens.csv\")\nExasensDF.head(30)","6c384731":"# i) Let's just quickly check the data types of the different columns in the Dataframe after we drop the first two rows. Help source: \n# https:\/\/hackersandslackers.com\/pandas-dataframe-drop\/\nExasensDF = ExasensDF.drop([0,1], axis = 0)\nExasensDF.head(5)","73558d32":"# ii) Let's try casting the real and imaginary (permittivity reading) part columns into float64 data to avoid issues with the .abs() function\n#later (error trying to convert string types into their absolute values). To do this, let's use pandas'.to_numeric() function, as demonstrated\n#in this thread: https:\/\/stackoverflow.com\/questions\/42719749\/pandas-convert-string-to-int.\nExasensDF['Imaginary Part'] = ExasensDF['Imaginary Part'].astype(int)\nExasensDF['Real Part'] = ExasensDF['Real Part'].astype(int)\n\nExasensDF.head(5)","addbf759":"ExasensDF.dtypes","7b6012d7":"# iii) Now, let's convert these two columns' elements into their absolute values for ease of scatterplot interpretation: \nExasensDF['Real Part']=ExasensDF['Real Part'].abs()\nExasensDF['Imaginary Part']=ExasensDF['Imaginary Part'].abs()\nExasensDF.head(5)","debbaf91":"# iv) Now, let's clean the data by removing any participants with NaN values from the dataframe. More specifically, let's go ahead and \n#drop any participants (rows from the Dataframe) with 'Real Part', 'Imaginary Part', or 'Diagnosis' column values of NaN.\n#Let's also completely drop the \"Unnamed: 9\" and \"Unnamed: 10\" columns, as it doesn't \n#seem like the researchers utilized these for anything. **Good Pandas Data Wrangling Reference: https:\/\/chrisalbon.com\/python\/data_wrangling\/pandas_dropping_column_and_rows\/**\n\nExasensDF = ExasensDF[ExasensDF['Imaginary Part'].notna()]\nExasensDF = ExasensDF[ExasensDF['Real Part'].notna()]\nExasensDF = ExasensDF[ExasensDF['Diagnosis'].notna()]\nExasensDF.head(5)","ac705164":"import matplotlib.pyplot as plt\n\nExasensScatters = plt.figure(figsize=(40,10))\n\nExasensScatterplot = ExasensScatters.add_subplot(1,2,1)\n\n#Since we are going to analyze the relationship between two continuous parameters and a multiclass target variable, let's use the \n#following StackOverflow thread on how to use Pandas to scatterplot this scenario: https:\/\/stackoverflow.com\/questions\/26139423\/plot-different-color-for-different-categorical-levels-using-matplotlib.\n#This method utilizes a dictionary of color-to-class codes and a lambda function within a \"c\" parameter in the pyplot .scatter() function.\n\ncolors = {'COPD':'red', 'HC':'green', 'Asthma':'blue', 'Infected':'black'}\nExasensScatter = ExasensScatterplot.scatter(ExasensDF['Real Part'], ExasensDF['Imaginary Part'], c=ExasensDF['Diagnosis'].apply(lambda x: colors[x]))\nplt.title(\"Real and Imaginary Part Saliva Permittivity Readings vs. Respiratory Illness Diagnoses: Exasens Saliva Biosensor Study\",fontsize=20)\nplt.xlabel(\"Permittivity Real Part\",fontsize=20)\nplt.ylabel(\"Permittivity Imaginary Part\",fontsize=20)\n\n#7\/18\/2020: NOTE: Let's scale these axes to better fit their ranges and make a more compressed plot. This modification must occur on the\n#subplot object itself. Let's also add a plot legend for readability.\nExasensScatterplot.set_xlim([425,560])\nhandles, labels = ExasensScatter.legend_elements(prop = 'colors')\nlabels = ['COPD','HC','Asthma','Infected']\nplt.legend(handles, labels, title = 'Diagnoses By Color')\n\n#Note: After all of the cleaning that took place beforehand, we are left with a dataset of ~59 study participants\n#7\/18\/2020: Still need to get the legend outputting correctly. ","a3222cf4":"# 2i) First, let's convert the Pandas dataframe above into two numpy arrays for more ease of use with scikit learn functions (train\/test splitting, etc.): \nExasensFeatures = np.asarray(ExasensDF[['Real Part','Imaginary Part']])\nExasensTarget = np.asarray(ExasensDF['Diagnosis'])\n\n#Next, let's split the whole dataset into training and testing sets for higher validity: \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(ExasensFeatures, ExasensTarget, test_size=0.2, random_state=4)","6ce1a858":"# 2ii) Now, I'll go ahead and build the KNN Model with Scikit Learn and evaluate it, plotting the accuracy results: \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics \n\nExasensKNNScores = []\nExasensNeighborsAccuracyArray = np.zeros(19)\nkTestIterationList = range(1,20)\nfor k in kTestIterationList:\n\n    ExasensNeighbors = KNeighborsClassifier(n_neighbors=k).fit(X_train,y_train)\n    DiagnosesPredictions=ExasensNeighbors.predict(X_test)\n    ExasensNeighborsAccuracyArray[k-1]=metrics.accuracy_score(y_test,DiagnosesPredictions)\n    ExasensKNNScores.append(metrics.accuracy_score(y_test, DiagnosesPredictions))\n    \nplt.plot(kTestIterationList, ExasensKNNScores)\nplt.xlabel('Value of K for Respiratory Illness Diagnosis KNN')\nplt.ylabel('KNN Testing Accuracy')","957ee2fb":"# 3i) Implementing a Scikit Learn Logistic Model on the Dataset: \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nExasensLogistic = LogisticRegression(C=0.01, solver='newton-cg', multi_class='multinomial').fit(X_train,y_train)\n\n#Let's make a few predictions using this model and the test set, as well as the probability of each of the class targets:\nExasensLogisticScorePreds = ExasensLogistic.predict(X_test) \nExasensLogisticScorePreds","6dd558c4":"# 3ii) Let's quickly use a Jaccard Score to evaluate the performance of the multiclass LR model: \nfrom sklearn.metrics import jaccard_score\njaccard_score(y_test, ExasensLogisticScorePreds, average = 'weighted')\n#Using the \"weighted\" parameter returns the higest Jaccard score, which is only ~0.12894.","c9b2c27d":"# 3iii) I'm going to quickly double check the probability of these LR classifications: \nLogisticExasensScoreProbas = ExasensLogistic.predict_proba(X_test)\nLogisticExasensScoreProbas\n#So in addition to the MLR model's low accuracy, the probabilities of its predictions' accuracies were low. Let's try incorporating\n#a few more parameters to increase the accuracies of one of\/both of these models. ","06e58834":"Now that we have a decent scatterplot going and can see a few outliers and two main clusters in the upper half of the plot, let's start making a K-Nearest Neighbors model to predict diagnoses labels based on saliva permittivity readings. ","9d3da006":"Huh. The testing accuracy for a KNN Model was quite low. Let's try multiclass Logistic Regression next to see if we can make better predictions. If that doesn't work, let's see if incorporating a greater number of dataset parameters makes KNN accuracy higher. ","fd3f4f8c":"Let's go ahead and spend some time cleaning the data before its initial plot (Code cells i) through iv) accomplish this).","80bc63b4":"From the background paper for this dataset (https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC6473814\/), it seems that \nboth the real and imaginary components of the selective permittivity of the saliva samples measured (continuous values)\npredicted categorical respiratory health outcomes for study participants: \"COPD\", \"Healthy\", and so on. \nIn the meantime, let's go on and create an initial scatterplot of the data to see whether there are any initially apparent clusters\nor other relationships between these paremeters and class target variables."}}