{"cell_type":{"5dc9948b":"code","f0293a56":"code","d1ea421b":"code","867917eb":"code","3402a4cc":"code","5a66818f":"code","b0b637c6":"code","c24ebb0f":"code","bcbf14e6":"code","19fb8edb":"code","925b02ae":"code","83cf1a86":"code","41027f70":"code","7a7445fd":"code","0abda8cc":"code","5bf42027":"code","1d8397d5":"code","3bcbeb11":"code","be8d8c0a":"code","41190348":"code","909d31b9":"code","056bde4d":"code","88f303eb":"code","7f3169c5":"code","9b35b7c3":"markdown","873e35e1":"markdown","ba91724b":"markdown","a3adb3f0":"markdown","93c0e6ce":"markdown","3ecce965":"markdown","1661ed79":"markdown","a6717c80":"markdown"},"source":{"5dc9948b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas_profiling\nimport re","f0293a56":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\",header = [0])\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\",header = [0])\ntrain1 = train.drop('Survived',axis=1)\ndf = pd.concat([train1,test])","d1ea421b":"profile = df.profile_report(title='Report - Titanic',minimal=True)\nprofile","867917eb":"# Avalia\u00e7\u00e3o da vari\u00e1vel da idade\ndf['Age'] = df['Age'].fillna(method='ffill')\ndf['Age_bin'] = pd.qcut(df['Age'], 10, labels=False)\ndf['Age_bin']= df['Age_bin'].round(0).astype(str)","3402a4cc":"# Avalia\u00e7\u00e3o de vari\u00e1vel de Fare\ndf['Fare'] = df['Fare'].fillna(method = 'ffill')\ndf['Fare_bin'] = pd.qcut(df['Fare'], 10, labels=False)\ndf['Fare_bin']= df['Fare_bin'].round(0).astype(str)","5a66818f":"# Avalia\u00e7\u00e3o da vari\u00e1vel Nome (T\u00edtulo)\ndf['Title'] = [nameStr[1].strip().split('.')[0] for nameStr in df['Name'].str.split(',')]","b0b637c6":"# Tamanho da familia\ndf['FamilySize'] =  df['SibSp'] + df['Parch'] + 1","c24ebb0f":"# Avalia\u00e7\u00e3o da vari\u00e1vel Cabine\ndf['IsCabinDataEmpty'] = 0\ndf.loc[df['Cabin'].isnull(),'IsCabinDataEmpty'] = 1","bcbf14e6":"# Avalia\u00e7\u00e3o da vari\u00e1vel PClass\ndf['Pclass'] = df['Pclass'].astype(str)","19fb8edb":"# Sele\u00e7\u00e3o das vari\u00e1veis\ndf2 = df.filter(['Pclass', 'Sex', 'Embarked', 'Age_bin', 'Fare_bin', 'Title', 'FamilySize', 'IsCabinDataEmpty'])","925b02ae":"# Transforma\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas para n\u00famericas\ndf3 = pd.get_dummies(df2)","83cf1a86":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\nfrom sklearn.model_selection import KFold, cross_val_score, GridSearchCV, train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import (confusion_matrix, precision_recall_curve, \n                             auc,roc_curve,recall_score, classification_report,\n                             f1_score, precision_recall_fscore_support)","41027f70":"treino = df3.iloc[0:891]\nteste = df3.iloc[891:1309]","7a7445fd":"xtr, xval, ytr, yval = train_test_split(treino,\n                                        train['Survived'],\n                                        test_size=0.2,\n                                        random_state=67)","0abda8cc":"%%time\nsel = SelectKBest(mutual_info_classif, k=15).fit(xtr,ytr)\nselecao = list(xtr.columns[sel.get_support()])\nprint(selecao)","5bf42027":"xtr = xtr.filter(selecao)\nxval = xval.filter(selecao)","1d8397d5":"# Random Search Cross Validation\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","3bcbeb11":"%%time\n# Use the random grid to search for best hyperparameters\nrf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = rf,param_distributions = random_grid,n_iter = 100,cv = 5,verbose=2,random_state=42,n_jobs = -1)\nrf_random.fit(xtr, ytr)","be8d8c0a":"baseline = RandomForestClassifier(**rf_random.best_params_)\nbaseline.fit(xtr,ytr)","41190348":"p = baseline.predict(xval)","909d31b9":"# Calculo das probabilidade das classes e as m\u00e9tricas para curva ROC.\ny_pred_prob = baseline.predict_proba(xval)[:,1]\nfpr,tpr,thresholds = roc_curve(yval,y_pred_prob)\nroc_auc = auc(fpr, tpr)\nprint(\"Area under the ROC curve : %f\" % roc_auc)\n\n# Curva ROC.\nplt.plot(fpr, tpr)\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('Curva ROC - Survived in Titanic')\nplt.xlabel('False Positive Rate (1 \u2014 Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)\nplt.show()\n\n# Calculo de metricas e thresholds.\ni = np.arange(len(tpr)) # index for df\nroc = pd.DataFrame({'fpr' : pd.Series(fpr, index=i),\n                    'tpr' : pd.Series(tpr, index = i), \n                    '1-fpr' : pd.Series(1-fpr, index = i), \n                    'tf' : pd.Series(tpr - (1-fpr), index = i), \n                    'threshold' : pd.Series(thresholds, index = i)})\ntab_metricas = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\nprint(tab_metricas)\n\n# Threshold: O corte ideal seria onde tpr \u00e9 alto e fpr \n# \u00e9 baixo tpr - (1-fpr) \u00e9 zero ou quase zero \u00e9 o ponto de corte ideal.\nt = tab_metricas.iloc[0].values[4]\ny_pred = [1 if e > t else 0 for e in y_pred_prob]\n\n# Constru\u00e7\u00e3o do plot da matriz de confus\u00e3o\nLABELS = ['not_survived', 'survived']\nconf_matrix = confusion_matrix(yval, p)\nplt.figure(figsize=(8,6))\nsns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\nplt.title(\"Matriz de confus\u00e3o\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.grid(False)\nplt.show()\n\nprint(classification_report(yval, p))","056bde4d":"sns.set(rc={'figure.figsize':(8, 8)})\nfeatures = xtr.columns\nimportances = baseline.feature_importances_\nindices = np.argsort(importances)\n\nplt.title('Importancia das vari\u00e1veis')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Importancia relativa')\nplt.show()","88f303eb":"y_pred = baseline.predict(teste.filter(selecao))","7f3169c5":"submission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\nsubmission_df['PassengerId'] = test['PassengerId']\nsubmission_df['Survived'] = y_pred\nsubmission_df.to_csv('submissions.csv', header=True, index=False)","9b35b7c3":"## Feature Engineering","873e35e1":"## Modelagem de aprendizado de m\u00e1quina Random Forest\n\nBibliotecas e pacotes:","ba91724b":"Entrada de dados","a3adb3f0":"## Titanic - Feature engineering and Random Forest\n\nA competi\u00e7\u00e3o \u00e9 simples: use o aprendizado de m\u00e1quina para criar um modelo que prev\u00ea quais passageiros sobreviveram ao naufr\u00e1gio do Titanic. Neste notebook vai ter analise explor\u00e1toria de dados, engenharia de recursos, modelagem de aprendizado de m\u00e1quina com algoritmo Random Forest com otimiza\u00e7\u00e3o de hiperpar\u00e2metros via pesquisa aleat\u00f3ria e a cria\u00e7\u00e3o do arquivo de submiss\u00e3o.\n\n1. Analise Explor\u00e1toria de dados.\n2. Engenharia de recursos.\n3. Modelagem de aprendizagem de m\u00e1quina - Random Forest.\n4. Otimiza\u00e7\u00e3o de hiperpar\u00e2metros via pesquisa aleat\u00f3ria.\n5. Arquivo de submiss\u00e3o.","93c0e6ce":"Sele\u00e7\u00e3o de Vari\u00e1veis","3ecce965":"Bibliotecas para analise de dados e feature enginnering","1661ed79":"Divis\u00e3o do dataset","a6717c80":"## Analise Exploratoria de dados"}}