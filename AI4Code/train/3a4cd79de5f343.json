{"cell_type":{"d58888f4":"code","4a0cdba5":"code","cf1261a3":"code","c1a58730":"code","b4eca2e3":"code","cffbb430":"code","dee602bc":"code","cbb750c3":"code","d04d3f5c":"markdown","4f6e4b68":"markdown","716069cf":"markdown","b9177637":"markdown","578c327c":"markdown","e035ed00":"markdown","5d3d625b":"markdown","bc1b125b":"markdown","8ee534ea":"markdown","81f18c07":"markdown"},"source":{"d58888f4":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\nfrom sklearn.metrics import log_loss\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.datasets import make_hastie_10_2\nfrom sklearn.model_selection import train_test_split\n\ndef sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))","4a0cdba5":"X_all = np.random.randn(5000, 1)\ny_all = (X_all[:, 0] > 0)*2 - 1\n\nX_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.5, random_state=42)","cf1261a3":"clf = DecisionTreeClassifier(max_depth=1)\nclf.fit(X_train, y_train)\n\nprint ('Accuracy for a single decision stump: {}'.format(clf.score(X_test, y_test)))","c1a58730":"# For convenience we will use sklearn's GBM, the situation will be similar with XGBoost and others\nclf = GradientBoostingClassifier(n_estimators=5000, learning_rate=0.01, max_depth=3, random_state=0)\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict_proba(X_test)[:, 1]\nprint(\"Test logloss: {}\".format(log_loss(y_test, y_pred)))","b4eca2e3":"def compute_loss(y_true, scores_pred):\n    '''\n        Since we use raw scores we will wrap log_loss \n        and apply sigmoid to our predictions before computing log_loss itself\n    '''\n    return log_loss(y_true, sigmoid(scores_pred))\n    \n\n'''\n    Get cummulative sum of *decision function* for trees. i-th element is a sum of trees 0...i-1.\n    We cannot use staged_predict_proba, since we want to maniputate raw scores\n    (not probabilities). And only in the end convert the scores to probabilities using sigmoid\n'''\ncum_preds = np.array([x for x in clf.staged_decision_function(X_test)])[:, :, 0] \n\nprint (\"Logloss using all trees:           {}\".format(compute_loss(y_test, cum_preds[-1, :])))\nprint (\"Logloss using all trees but last:  {}\".format(compute_loss(y_test, cum_preds[-2, :])))\nprint (\"Logloss using all trees but first: {}\".format(compute_loss(y_test, cum_preds[-1, :] - cum_preds[0, :])))","cffbb430":"# Pick an object of class 1 for visualisation\nplt.plot(cum_preds[:, y_test == 1][:, 0])\n\nplt.xlabel('n_trees')\nplt.ylabel('Cumulative decision score');","dee602bc":"clf = GradientBoostingClassifier(n_estimators=5000, learning_rate=8, max_depth=3, random_state=0)\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict_proba(X_test)[:, 1]\nprint(\"Test logloss: {}\".format(log_loss(y_test, y_pred)))","cbb750c3":"cum_preds = np.array([x for x in clf.staged_decision_function(X_test)])[:, :, 0] \n\nprint (\"Logloss using all trees:           {}\".format(compute_loss(y_test, cum_preds[-1, :])))\nprint (\"Logloss using all trees but last:  {}\".format(compute_loss(y_test, cum_preds[-2, :])))\nprint (\"Logloss using all trees but first: {}\".format(compute_loss(y_test, cum_preds[-1, :] - cum_preds[0, :])))","d04d3f5c":"Even though the dataset is synthetic, the similar intuition will work with the real data, except GBM can diverge with high learning rates for a more complex dataset. If you want to play with a little bit more realistic dataset, you can generate it in this notebook with the following code:\n\n`X_all, y_all = make_hastie_10_2(random_state=0)` \n\nand run the code starting from \"Learn GBM\".","4f6e4b68":"# Learn GBM","716069cf":"See, the decision function improves almost linearly untill about 800 iteration and then stops. And the slope of this line is connected with the learning rate, that we have set in GBM! \n\nIf you remember the main formula of boosting, you can write something like:\n    $$ F(x) = const + \\sum\\limits_{i=1}^{n}\\gamma_i h_i(x) $$\n\nIn our case, $\\gamma_i$ are constant and equal to learning rate $\\eta = 0.01$. And look, it takes about $800$ iterations to get the score $8$, which means at every iteration score goes up for about $0.01$. It means that first 800 terms are approximately equal to $0.01$, and the following are almost $0$. \n\nWe see, that if we drop the last tree, we lower $F(x)$ by $0$ and if we drop the first tree we lower $F(x)$ by $0.01$, which results in a very very little performance drop.  \n\nSo, even in the case of simple dataset which can be solved with single decision stump, in GBM we need to sum a lot of trees (roughly $\\frac{1}{\\eta}$) to approximate this golden single decision stump.","b9177637":"But we will need 800 trees in GBM to classify it correctly.","578c327c":"The datast is really simple and can be solved with a single decision stump.","e035ed00":"**To prove the point**, let's try a larger learning rate of $8$.","5d3d625b":"You can see that there is a difference, but not as huge as one could expect! Moreover, if we get rid of the first tree \u2014 overall model still works! \n\nIf this is supprising for you \u2014 take a look at the plot of cummulative decision function depending on the number of trees.","bc1b125b":"# Make dataset\nWe will use a very simple dataset: objects will come from 1D normal distribution, we will need to predict class $1$ if the object is positive and 0 otherwise.","8ee534ea":"That is it! Now we see, that it is crucial to have the first tree in the ensemble!","81f18c07":"Hi! In this notebook we will do a little \"how *Gradient Boosting* works\" and find out answer for the question:\n## \"Will performance of GBDT model drop dramatically if we remove the first tree?\""}}