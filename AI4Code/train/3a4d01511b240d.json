{"cell_type":{"9425ee2c":"code","dbf5d3df":"code","cda71ce3":"code","14c75f35":"code","5fdf5d5d":"code","4956b4bb":"code","f75ce54b":"code","829004c0":"code","a0cac59d":"code","52bb8f6e":"code","5745ae26":"code","ecf88f6a":"code","21cd1d93":"code","5594a2b2":"code","2cd363fa":"code","3389bb03":"code","0f4333e1":"code","24a2d8db":"markdown","fd258ee4":"markdown","df7405f8":"markdown","bdc0591e":"markdown","5bcfa3a1":"markdown"},"source":{"9425ee2c":"# We import the libraries\nimport numpy as np\nimport pandas as pd\npd.options.display.precision = 15\nimport seaborn as sns\nimport lightgbm as lgb\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import LeaveOneOut, train_test_split\nimport warnings\nwarnings.filterwarnings(\"ignore\")","dbf5d3df":"# We load the train.csv\nPATH=\"..\/input\/\"\ntrain_df = pd.read_csv(PATH + 'train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})\ntrain_df.head()","cda71ce3":"print(\"There are {} rows in train_df\".format(len(train_df)))","14c75f35":"# We define the number of rows in each segment as the same number of rows in the real test segments (150000 rows)\nrows = 150000\nsegments = int(np.floor(train_df.shape[0] \/ rows))\nprint(\"Number of segments: \", segments)","5fdf5d5d":"train_X = pd.DataFrame(index=range(segments), dtype=np.float64)\ntrain_y = pd.DataFrame(index=range(segments), dtype=np.float64, columns=['time_to_failure'])","4956b4bb":"train_X.head(3)","f75ce54b":"train_y.head(3)","829004c0":"def create_features(seg_id, xc, X):\n    \n    x_roll_std_50 = xc.rolling(50).std().dropna().values\n    x_roll_mean_50 = xc.rolling(50).mean().dropna().values\n    x_roll_std_1000 = xc.rolling(1000).std().dropna().values\n    x_roll_mean_1000 = xc.rolling(1000).mean().dropna().values\n    x_roll_std_5000 = xc.rolling(5000).std().dropna().values\n    x_roll_mean_5000 = xc.rolling(5000).mean().dropna().values\n    \n    X.loc[seg_id, 'q05_roll_std_50'] = np.quantile(x_roll_std_50, 0.05)\n    X.loc[seg_id, 'q90_roll_mean_50'] = np.quantile(x_roll_mean_50, 0.90)\n    X.loc[seg_id, 'q05_roll_std_1000'] = np.quantile(x_roll_std_1000, 0.05)\n    X.loc[seg_id, 'q90_roll_mean_1000'] = np.quantile(x_roll_mean_1000, 0.90)\n    X.loc[seg_id, 'q05_roll_std_5000'] = np.quantile(x_roll_std_5000, 0.05)\n    X.loc[seg_id, 'q90_roll_mean_5000'] = np.quantile(x_roll_mean_5000, 0.90)\n    \n    X.loc[seg_id, 'abs_max'] = np.abs(xc).max()\n    X.loc[seg_id, 'abs_q95'] = np.quantile(np.abs(xc), 0.95)\n    X.loc[seg_id, 'abs_q75'] = np.quantile(np.abs(xc), 0.75)\n    X.loc[seg_id, 'abs_q25'] = np.quantile(np.abs(xc), 0.25)\n    X.loc[seg_id, 'abs_q05'] = np.quantile(np.abs(xc), 0.05)","a0cac59d":"# iterate over all segments\nfor seg_id in tqdm_notebook(range(segments)):\n    seg = train_df.iloc[seg_id*rows:seg_id*rows + rows]\n    create_features(seg_id, seg['acoustic_data'], train_X)\n    train_y.loc[seg_id, 'time_to_failure'] = seg['time_to_failure'].values[-1]","52bb8f6e":"train_X.shape, train_y.shape","5745ae26":"train_X.head(3)","ecf88f6a":"train_y.head(3)","21cd1d93":"X_train, X_test, y_train, y_test = train_test_split(train_X, train_y, test_size=0.25, random_state=42)","5594a2b2":"seed = 42\n    \nparams_lgb = {'bagging_fraction': 0.9,\n            'bagging_freq': 5,\n            'early_stopping_rounds': 10,\n            'feature_fraction': 0.7,\n            'learning_rate': 0.02,\n            'num_boost_round': 100,\n            'seed': seed,\n            'feature_fraction_seed': seed,\n            'bagging_seed': seed,\n            'drop_seed': seed,\n            'data_random_seed': seed,\n            'objective': 'regression_l1',\n            'boosting': 'gbdt',\n            'verbosity': -1,\n            'metric': 'mae',\n            'num_threads': 8}","2cd363fa":"# Leave One Out cross validation\nloo = LeaveOneOut()\nn_splits = loo.get_n_splits(X_train)\nt = tqdm_notebook(total=n_splits)\noof = np.zeros(len(X_train))\npredictions = np.zeros(len(X_test))\n\nfor trn_idx, val_idx in loo.split(X_train):\n    t.update(1)\n    trn_data = lgb.Dataset(X_train.iloc[trn_idx], label=y_train.iloc[trn_idx])\n    val_data = lgb.Dataset(X_train.iloc[val_idx], label=y_train.iloc[val_idx])\n    \n    clf = lgb.train(params_lgb, trn_data, valid_sets = [trn_data, val_data], verbose_eval=False)\n    oof[val_idx] = clf.predict(X_train.iloc[val_idx], num_iteration=clf.best_iteration)\n    predictions += clf.predict(X_test, num_iteration=clf.best_iteration) \/ n_splits\n    \nprint('CV MAE: {}'.format(mean_absolute_error(y_train.values, oof)))\nprint('Test MAE: {}'.format(mean_absolute_error(y_test.values, predictions)))","3389bb03":"# Scatter plot the real time to failure vs predicted (Leave One Out Cross Validation)\nplt.figure(figsize=(6, 6))\nplt.scatter(y_train.values.flatten(), oof)\nplt.xlim(0, 20)\nplt.ylim(0, 20)\nplt.title('Leave One Out Cross Validation')\nplt.xlabel('time to failure', fontsize=12)\nplt.ylabel('predicted', fontsize=12)\nplt.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)])","0f4333e1":"# Scatter plot the real time to failure vs predicted (Testing Set)\nplt.figure(figsize=(6, 6))\nplt.scatter(y_test.values.flatten(), predictions)\nplt.xlim(0, 20)\nplt.ylim(0, 20)\nplt.title('Testing Set')\nplt.xlabel('time to failure', fontsize=12)\nplt.ylabel('predicted', fontsize=12)\nplt.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)])","24a2d8db":"## We create a testing set","fd258ee4":"\n## We create the LGB model","df7405f8":"## Conclusion\n#### As we can see, the MAE of the Leave One Out Cross Validation is really good (MAE 1.57) but the MAE of the test set is not good (MAE 2.20). We conclude that LOOCV is not a good choice for this problem because it clearly leads to overfitting.","bdc0591e":"### This kernel was created to the <a target=\"_blank\" href=\"https:\/\/www.kaggle.com\/c\/LANL-Earthquake-Prediction\">LANL Earthquake Prediction<\/a> competition to show how to use the Leave One Out Cross Validation.\n#### Leave One Out Cross Validation is a K-fold cross validation with K equal to N, the number of samples in the data set. It means that for each point the model is trained on all the data except for that point and a prediction is made for that point. As we'll see this technique will lead to overfitting in this case.","5bcfa3a1":"## We process the train file"}}