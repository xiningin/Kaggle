{"cell_type":{"2e4558c0":"code","7a9c5665":"code","935f7b7b":"code","cfc50624":"code","a611c0a3":"code","35eef4a8":"code","f13b45cc":"code","42ff2d0c":"code","4ca95503":"code","f05d329c":"code","320a0bed":"code","fbed6b15":"code","eaba5ca6":"code","8d94362a":"code","8e81e32b":"code","fd92653e":"code","7c0009ec":"code","8e304675":"code","7797e5ed":"code","7ca44d9a":"code","9f63c41b":"code","38541b4a":"code","fa9ee262":"code","fa9dd2c2":"code","c1777ca7":"code","3aab93e3":"code","12ef543f":"code","ce809be4":"code","1b4c2e71":"code","3936b779":"code","1414eef8":"code","dda341da":"code","898acc94":"code","a7699064":"code","4f27e789":"code","84db404e":"code","61bdd60e":"code","8b24c44c":"code","f8ebb6a0":"code","f85a0db0":"code","2565d33d":"code","ee2503ba":"code","da4ef40b":"code","f053d974":"code","0287e8b1":"code","c34153d0":"code","2584726f":"code","8f631d53":"code","90ddb04e":"code","54baeb15":"code","a0f1f0f0":"code","6e0ce53e":"code","70f18497":"code","66c185dd":"code","7d52574a":"code","543ac02d":"code","4c97df8d":"code","55864c11":"code","d1601258":"code","cf7127d7":"code","a049ec0a":"code","6335bb27":"code","42c90c47":"code","dc5a14d8":"code","d1c37908":"code","e22144ec":"markdown","3e007ad0":"markdown","9af10fa4":"markdown","0f0c5889":"markdown","9ca98563":"markdown","4e3e67e5":"markdown","42a69dbe":"markdown","b8672e42":"markdown","72c14849":"markdown","1385a267":"markdown","377e59e0":"markdown","c703bf79":"markdown","b1df1704":"markdown","82a28d7a":"markdown","b3980238":"markdown","880532b2":"markdown","2688ba88":"markdown","b5f42411":"markdown","09e5cf19":"markdown","642f5502":"markdown","4ceb1e07":"markdown","64fa84d6":"markdown","068a8c44":"markdown","97112244":"markdown","e8d69747":"markdown","4dab7dc2":"markdown","cf0c893b":"markdown","938af142":"markdown","3277217a":"markdown","7488e2bf":"markdown","1a4b13e7":"markdown","baebe0f5":"markdown","7e3a2cf2":"markdown","160db3e5":"markdown","879ca515":"markdown","26418c77":"markdown","c1ff48f8":"markdown","57be2fe8":"markdown","5a1852d2":"markdown","01ea3000":"markdown","a67e55bb":"markdown","2fcbc499":"markdown","9b83bc5e":"markdown","4640abf3":"markdown","45059548":"markdown","805c9d6e":"markdown","f8fbea03":"markdown","2eea5d74":"markdown","b270290f":"markdown","60652744":"markdown"},"source":{"2e4558c0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7a9c5665":"#ref: https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering\nclass FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            # Abstract\n            for entry in content['abstract']:\n                self.abstract.append(entry['text'])\n            # Body text\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'","935f7b7b":"#ref: https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering\nimport glob\nimport json\n\nmetadata_path = \"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\"\nmetadata_df = pd.read_csv(metadata_path)\n\nall_json = glob.glob(\"\/kaggle\/input\/CORD-19-research-challenge\/**\/*.json\", recursive=True)\n\ndict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) \/\/ 10) == 0:\n        print(f'Processing index: {idx} of {len(all_json)}')\n    content = FileReader(entry)\n    \n    # get metadata information\n    meta_data = metadata_df.loc[metadata_df['sha'] == content.paper_id]\n    # no metadata, skip this paper\n    if len(meta_data) == 0:\n        continue\n    \n    dict_['paper_id'].append(content.paper_id)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\n    \n    # also create a column for the summary of abstract to be used in a plot\n    if len(content.abstract) == 0: \n        # no abstract provided\n        dict_['abstract_summary'].append(\"Not provided.\")\n    else:\n        # abstract is short enough\n        summary = content.abstract\n        dict_['abstract_summary'].append(summary)\n        \n    # get metadata information\n    meta_data = metadata_df.loc[metadata_df['sha'] == content.paper_id]\n    \n    try:\n        # if more than one author\n        authors = meta_data['authors'].values[0].split(';')\n        if len(authors) > 2:\n            # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n            dict_['authors'].append(\". \".join(authors[:2]) + \"...\")\n        else: # modifying from the above mentioned kernel, since we are not interested in similar plotting\n            # authors will fit in plot\n            dict_['authors'].append(\". \".join(authors))\n    except Exception as e:\n        # if only one author - or Null valie\n        dict_['authors'].append(meta_data['authors'].values[0])\n    \n    # add the title information, add breaks when needed\n    try:\n        title = meta_data['title'].values[0]\n        dict_['title'].append(title)\n    # if title was not provided\n    except Exception as e:\n        dict_['title'].append(meta_data['title'].values[0])\n    \n    # add the journal information\n    dict_['journal'].append(meta_data['journal'].values[0])\n    \ndf_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])\ndf_covid.head()","cfc50624":"df_covid.isna().sum()","a611c0a3":"df_covid.loc[df_covid['paper_id']=='f318f417880d9beb2ce5c8444f3597a8808eae30', ['abstract']]['abstract'].values[0]","35eef4a8":"#nltk.download('stopwords')\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords \nimport pandas\n\ndef get_important_words(doc, threshold=3):\n    df = pd.DataFrame(columns=['word', 'count'])\n    stop_words = set(stopwords.words('english'))\n    tokens = word_tokenize(doc)\n    #get rid of specical characters\n    words = [word.lower() for word in tokens if word.isalpha()]\n    filtered_sentence = [] \n    #get rid of stop words\n    filtered_sentence = [w for w in words if not w in stop_words] \n    if len(words) <= 0: return df;\n    for w in words: \n        if w not in stop_words: \n            filtered_sentence.append(w) \n\n    fdist = nltk.FreqDist(filtered_sentence)\n   \n    #find frequency distribution of words\n    for key in fdist:\n        word = key\n        cnt = fdist[key]\n        d = {'word': word, 'count': cnt}\n        df = df.append(d, ignore_index=True)\n    #find words that are used 'way' more than others\n    arr = df.sort_values(by=['count'], ascending=False)['count'].values\n    outliers = []\n    mean_1 = np.mean(arr)\n    std_1 = np.std(arr)\n    for y in arr:\n        z_score= (y - mean_1)\/std_1 \n        if np.abs(z_score) > threshold:\n            outliers.append(y)  \n    if(len(outliers)>0):\n        df['outlier'] = df['count'] >= outliers[-1]\n    else:\n        df['outlier'] = False\n    return_df = df.loc[df['outlier'] == True, ['word']]\n    return return_df","f13b45cc":"doc = df_covid.loc[df_covid['paper_id']=='f318f417880d9beb2ce5c8444f3597a8808eae30', ['abstract']]['abstract'].values[0]\nget_important_words(doc, 3)","42ff2d0c":"doc = df_covid.loc[df_covid['paper_id']=='f318f417880d9beb2ce5c8444f3597a8808eae30', ['abstract']]['abstract'].values[0]\nget_important_words(doc, 2)","4ca95503":"import numpy as np\nimport random\ndef get_important_words_per_document(df_all, num_samples):\n    random.seed(1234)\n    idx = np.arange(df_all.shape[0])\n    random.shuffle(idx)\n    df = pd.DataFrame(columns=['p_id','word'])\n    for indx, (p_id, abstr) in df_covid.loc[idx[:num_samples]][['paper_id', 'abstract']].iterrows():\n        df_temp = get_important_words(abstr, 4)#increase the threshold\n        df_temp['p_id'] = p_id\n        df = df.append(df_temp[['p_id','word']])\n    return df","f05d329c":"df_imp_words_all_docs = get_important_words_per_document(df_covid, 500)","320a0bed":"df_imp_words_all_docs.groupby(['word']).count().reset_index().sort_values(by=['p_id'], ascending=False).head(15)","fbed6b15":"df_imp_words_all_docs[df_imp_words_all_docs['word'] == 'sars']['p_id']","eaba5ca6":"df_covid.loc[df_covid['paper_id']=='ce708dd37870908f94d2e5845c963cfadaa38b0d', 'abstract'].values[0]","8d94362a":"df_covid.loc[df_covid['paper_id']=='25cc93bafacf163c6e315809b41ef6d814c15b15', 'abstract'].values[0]","8e81e32b":"data = df_imp_words_all_docs.groupby(['word']).count().reset_index().sort_values(by=['p_id'], ascending=False)[:50]\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfig, ax = plt.subplots(1,1, figsize=(10,10))\nax.barh(data['word'], data['p_id'])\nax.invert_yaxis()  # labels read top-to-bottom\nax.set_xlabel('Ocurrence')\nax.set_title('Important (top 50) abstract words that occur frequently across corpus')","fd92653e":"#https:\/\/www.programcreek.com\/python\/example\/91606\/nltk.corpus.wordnet.wup_similarity\nfrom nltk.corpus import wordnet\nimport itertools\ndef similarity(word1, word2):\n    allsyns1 = set(ss for ss in wordnet.synsets(word1))\n    allsyns2 = set(ss for ss in wordnet.synsets(word2))\n    try:\n        best = max((wordnet.wup_similarity(s1, s2) or 0) for s1, s2 in \n                itertools.product(allsyns1, allsyns2))\n    except: \n        best = 0\n    return best","7c0009ec":"print(similarity('infection','disease'))\nprint(similarity('one','four'))\nprint(similarity('dna','rna'))\nprint(similarity('pig','bat'))\nprint(similarity('influenza','virus'))","8e304675":"def find_closest_words(wrd, wordlist):\n    df = pd.DataFrame(columns=['word','similarity'])\n    for word in wordlist:\n        sim = similarity(wrd, word)\n        d = {'word': word, 'similarity': sim}\n        df = df.append(d, ignore_index=True)\n    return df;","7797e5ed":"df_closest = find_closest_words('antiviral', df_imp_words_all_docs['word'].unique())\ndf_closest.sort_values(by=['similarity'], ascending=False).head(10)","7ca44d9a":"df_closest.sort_values(by=['similarity'], ascending=False)['similarity'].hist(bins=50)","9f63c41b":"def find_closest_words_with_cutoff(wrd, wordlist, cut_off):\n    df = pd.DataFrame(columns=['word','similarity'])\n    for word in wordlist:\n        sim = similarity(wrd, word)\n        d = {'word': word, 'similarity': sim}\n        df = df.append(d, ignore_index=True)\n    if(df.shape[0] <=0): return df\n    vals = df['similarity'].values\n    cut = np.quantile(vals, cut_off)\n    #print(df.sort_values(by=['similarity'], ascending=False).quantile(cut_off))\n    #cut_off = df.sort_values(by=['similarity'], ascending=False).quantile(cut_off).values[0]\n    #df = df.sort_values(by=['similarity'], ascending=False)\n    return df.loc[df['similarity'] >= cut, ];","38541b4a":"find_closest_words_with_cutoff('virus', df_imp_words_all_docs['word'].unique(), .98)","fa9ee262":"def find_closest_word_and_document(p_id, word, unique_words, all_docs):\n    df_temp = pd.DataFrame(columns=['p_id','word','sim_word','ref_p_id'])\n    df_closest = find_closest_words_with_cutoff(word, unique_words, .98)\n    if(df_closest.shape[0] <= 0): return df_temp\n    if(df_closest['similarity'].max() < .7): return df_temp\n    for wrd in df_closest['word']:\n        d = pd.DataFrame(columns=['p_id','word','sim_word','ref_p_id'])\n        d['ref_p_id'] = all_docs.loc[all_docs['word'] == wrd,['p_id']]['p_id']\n        d['sim_word'] = wrd\n        #todo: store the similarity values too, this will help with comparison and possible distance plots\n        df_temp = df_temp.append(d)\n    df_temp['p_id'] = p_id\n    df_temp['word'] = word\n    return df_temp","fa9dd2c2":"unique_words = df_imp_words_all_docs['word'].unique()\nfind_closest_word_and_document('01c47a7e53b4cf4783d55125936061e2ca0d9817', 'transmission', unique_words, df_imp_words_all_docs)","c1777ca7":"find_closest_word_and_document('01c47a7e53b4cf4783d55125936061e2ca0d9817', 'incubation', unique_words, df_imp_words_all_docs)","3aab93e3":"#enhanced similar word search supporting all imp. words in a document\ndef find_closest_word_and_document_v2(x, unique_words, all_docs):\n    #p_id = x.reset_index()['p_id'][0]\n    #df = pd.DataFrame(columns=['p_id','word','sim_word','ref_p_id'])\n    df = pd.DataFrame(columns=['word','sim_word','ref_p_id'])\n    for w in x['word']:\n        #df_temp = pd.DataFrame(columns=['p_id','word','sim_word','ref_p_id'])\n        df_temp = pd.DataFrame(columns=['word','sim_word','ref_p_id'])\n        df_closest = find_closest_words_with_cutoff(w, unique_words, .98)\n        if(df_closest.shape[0] <= 0): return df_temp\n        #for very low similarities, skip further processing\n        if(df_closest['similarity'].min() < .7): return df_temp\n        for wrd in df_closest['word']:\n            #d = pd.DataFrame(columns=['p_id','word','sim_word','ref_p_id'])\n            d = pd.DataFrame(columns=['word','sim_word','ref_p_id'])\n            d['ref_p_id'] = all_docs.loc[all_docs['word'] == wrd,['p_id']]['p_id']\n            d['sim_word'] = wrd\n            d['word'] = w\n            #d['p_id'] = p_id      \n            df_temp = df_temp.append(d)  \n        #df_temp['word'] = w\n        #df_temp['p_id'] = p_id\n        #display(df_temp)\n        df = df.append(df_temp)\n    #df['p_id'] = p_id\n    return df_temp","12ef543f":"#build the dataframe with all similar abstracts wrt important words\n#df_imp_words_all_docs contains information about 500 randomly chosen abstracts\ndf_wo_index = df_imp_words_all_docs.reset_index(drop=True)\nunique_words = df_imp_words_all_docs['word'].unique()\ndf_sim_matrix = df_wo_index[0:500].groupby('p_id').apply(lambda x: find_closest_word_and_document_v2(x,unique_words,df_imp_words_all_docs)).reset_index()","ce809be4":"#imp_word = 'incubation'\ndata = df_sim_matrix.groupby('word')\\\n    .apply(lambda x: len(np.unique(x['ref_p_id']))).reset_index().sort_values(by=[0], ascending=False)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfig, ax = plt.subplots(1,1, figsize=(10,25))\nax.barh(data['word'], data[0])\nax.invert_yaxis()  # labels read top-to-bottom\nax.set_xlabel('Ocurrence')\nax.set_title('Important words \/ similar words that occur frequently across abstracts')","1b4c2e71":"df_sim_matrix[df_sim_matrix['word']=='transmission']","3936b779":"df_covid.loc[df_covid['paper_id']=='95d070f39f49f5d56d1330a2056f2e953d37af0f', 'abstract'].values[0]","1414eef8":"df_covid.loc[df_covid['paper_id']=='d04d63e56673f57ed326ebf2314e5b8192266a79', 'abstract'].values[0]","dda341da":"df_covid.loc[df_covid['paper_id']=='473c721f42096f1b8450c669b607486841a5f72a', 'abstract'].values[0]","898acc94":"#use all available data to build an extensive similarity matrix\ndf_wo_index_full = df_imp_words_all_docs.reset_index(drop=True)\nunique_words_full = df_imp_words_all_docs['word'].unique()\ndf_sim_matrix_full = df_wo_index_full.groupby('p_id').apply(lambda x: find_closest_word_and_document_v2(x,unique_words_full,df_imp_words_all_docs)).reset_index()","a7699064":"df_sim_matrix_full.shape","4f27e789":"#imp_word = 'incubation'\ndata = df_sim_matrix_full.groupby('word')\\\n    .apply(lambda x: len(np.unique(x['ref_p_id']))).reset_index().sort_values(by=[0], ascending=False).head(50)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfig, ax = plt.subplots(1,1, figsize=(10,25))\nax.barh(data['word'], data[0])\nax.invert_yaxis()  # labels read top-to-bottom\nax.set_xlabel('Ocurrence')\nax.set_title('Important words \/ similar words that occur frequently across abstracts')","84db404e":"'incubation' in unique_words_full","61bdd60e":"sim = []\nfor wrd in unique_words_full:\n    sim.append(similarity('incubation',wrd))\ndf_wrd_sim = pd.DataFrame(columns=['word', 'sim'])\ndf_wrd_sim['word'] = unique_words_full\ndf_wrd_sim['sim'] = sim\nvals = df_wrd_sim.sort_values(by=['sim'], ascending = False)['sim'].values\ncut_off = np.quantile(vals, .98)\nrelevant_words = df_wrd_sim.loc[df_wrd_sim['sim']>cut_off, ]['word']","8b24c44c":"df_abstract_word = pd.DataFrame(columns = ['p_id', 'word'])\nfor word in relevant_words:\n    df_select_columns = df_sim_matrix.loc[df_sim_matrix['word']==word, ]\n    d = pd.DataFrame(columns=['p_id', 'word'])\n    #d = {'p_id': df_select_columns['p_id'].unique(), 'word': word}\n    d['p_id'] = df_select_columns['ref_p_id']\n    d['word'] = df_select_columns['sim_word']\n    df_abstract_word = df_abstract_word.append(d)\n    #docs.extend(df_select_columns['p_id'].unique())\n    #docs.extend(df_select_columns['ref_p_id'].tolist())\nprint(df_abstract_word)","f8ebb6a0":"sim = []\nfor wrd in unique_words_full:\n    sim.append(similarity('incubation',wrd))\ndf_wrd_sim = pd.DataFrame(columns=['word', 'sim'])\ndf_wrd_sim['word'] = unique_words_full\ndf_wrd_sim['sim'] = sim\nvals = df_wrd_sim.sort_values(by=['sim'], ascending = False)['sim'].values\ncut_off = np.quantile(vals, .995) #changing this value to a higher number\nrelevant_words = df_wrd_sim.loc[df_wrd_sim['sim']>cut_off, ]['word']","f85a0db0":"df_abstract_word = pd.DataFrame(columns = ['p_id', 'word'])\nfor word in relevant_words:\n    df_select_columns = df_sim_matrix.loc[df_sim_matrix['word']==word, ]\n    d = pd.DataFrame(columns=['p_id', 'word'])\n    #d = {'p_id': df_select_columns['p_id'].unique(), 'word': word}\n    d['p_id'] = df_select_columns['ref_p_id']\n    d['word'] = df_select_columns['sim_word']\n    df_abstract_word = df_abstract_word.append(d)\n    #docs.extend(df_select_columns['p_id'].unique())\n    #docs.extend(df_select_columns['ref_p_id'].tolist())\nprint(df_abstract_word)","2565d33d":"df_covid.loc[df_covid['paper_id']=='5eb34e4b386106962c368bb7c32db8995190e5c6', 'abstract'].values[0]","ee2503ba":"def find_abstracts_discussing_specific_terms(term):\n    sim = []\n    for wrd in unique_words_full:\n        sim.append(similarity(term,wrd))\n    df_wrd_sim = pd.DataFrame(columns=['word', 'sim'])\n    df_wrd_sim['word'] = unique_words_full\n    df_wrd_sim['sim'] = sim\n    vals = df_wrd_sim.sort_values(by=['sim'], ascending = False)['sim'].values\n    cut_off = np.quantile(vals, .995) #changing this value to a higher number\n    relevant_words = df_wrd_sim.loc[df_wrd_sim['sim']>cut_off, ]['word']\n    df_abstract_word = pd.DataFrame(columns = ['p_id', 'word'])\n    for word in relevant_words:\n        df_select_columns = df_sim_matrix.loc[df_sim_matrix['word']==word, ]\n        d = pd.DataFrame(columns=['p_id', 'word'])\n        #d = {'p_id': df_select_columns['p_id'].unique(), 'word': word}\n        d['p_id'] = df_select_columns['ref_p_id']\n        d['word'] = df_select_columns['sim_word']\n        df_abstract_word = df_abstract_word.append(d)\n        #docs.extend(df_select_columns['p_id'].unique())\n        #docs.extend(df_select_columns['ref_p_id'].tolist())\n    return df_abstract_word","da4ef40b":"df = find_abstracts_discussing_specific_terms('transmission')\nprint(df['word'].unique())","f053d974":"df_covid.loc[df_covid['paper_id'] == '0015023cc06b5362d332b3baf348d11567ca2fbb', ['abstract']].values[0][0]","0287e8b1":"import nltk\nfrom nltk import word_tokenize\ndoc = df_covid.loc[df_covid['paper_id'] == '0015023cc06b5362d332b3baf348d11567ca2fbb', ['abstract']].values[0][0]\ntokens = word_tokenize(doc)\ntagged_tokens = nltk.pos_tag(tokens)\nfdist = nltk.FreqDist(tagged_tokens)\ndf = pd.DataFrame(columns=['word', 'pos'])\nfor key in fdist:\n    word = key[0]\n    pos = key[1]\n    cnt = fdist[key]\n    d = {'word': word, 'pos': pos}\n    df = df.append(d, ignore_index=True)\nprint(\"Unique POS =\", df['pos'].nunique())","c34153d0":"import matplotlib.pyplot as plt\n%matplotlib inline\nfig, ax = plt.subplots(1,1, figsize=(6,5))\nplot_data = df.groupby('pos').count().reset_index().sort_values(by = ['word'], ascending=False)\nax.invert_yaxis()\nax.set_xlabel(\"Ocurrence of POS tags\")\nax.set_title(\"Words by POS tags\")\nax.barh(plot_data['pos'], plot_data['word'])","2584726f":"df[df['pos']=='NNP'].head()","8f631d53":"df[df['pos'] == 'NNP'].groupby('word').count().reset_index().sort_values(by=['pos'], ascending=False).head()","90ddb04e":"import pandas\ndef freq_dist_pos_by_sents(doc): #todo: handle a regex for the POS\n    sentences = nltk.sent_tokenize(doc)\n    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n    df = pd.DataFrame(columns=['word', 'pos'])\n    for sent in sentences:\n        tagged_tokens = nltk.pos_tag(sent)\n        fdist = nltk.FreqDist(tagged_tokens)\n        for key in fdist:\n            word = key[0]\n            pos = key[1]\n            cnt = fdist[key]\n            d = {'word': word, 'pos': pos}\n            df = df.append(d, ignore_index=True)\n    return df;","54baeb15":"def find_important_pos(doc, pos, threshold=2):\n    # grammar = \"CHUNK: {<NN|NNP|NNS|NP><NN|NNP|NNS|NP>}  # Chunk two consecutive nouns\"\n    df = freq_dist_pos_by_sents(doc)\n    df = df.loc[df['pos'] == pos,]\n    df_ordered = df.groupby(['word']).count().reset_index().sort_values(['pos'], ascending=False)\n    df_word_weight = df_ordered\\\n        .apply(lambda x: pd.Series([x['word'], x['pos']\/df_ordered['word'].shape[0],x['pos']], \\\n                                    index=['word','weight','pos']), axis = 1)\n    outliers = []\n    if ((df_word_weight.shape[0] > 0) & ('weight' in df_word_weight.columns.tolist())):\n        arr = df_word_weight['weight'].values\n        #find outliers\n        mean_1 = np.mean(arr)\n        std_1 = np.std(arr)\n        for y in arr:\n            z_score= (y - mean_1)\/std_1 \n            if np.abs(z_score) > threshold:\n                outliers.append(y)  \n        if(len(outliers)>0):\n            df_word_weight['outlier'] = df_word_weight['weight'] >= outliers[-1]\n        else:\n            df_word_weight['outlier'] = False\n        return_df = df_word_weight.loc[df_word_weight['outlier'] == True, ['word']]\n    else:\n        df_word_weight['word'] = None\n        return_df = df_word_weight.loc[:, ['word']]\n    return return_df","a0f1f0f0":"doc = df_covid.loc[df_covid['paper_id'] == '0015023cc06b5362d332b3baf348d11567ca2fbb', ['abstract']].values[0][0]\nfind_important_pos(doc, 'NNP', 2)","6e0ce53e":"import random\nrandom.seed(1234)\nidx = np.arange(df_covid.shape[0])\nrandom.shuffle(idx)\ndf_imp_pos = pd.DataFrame(columns=['p_id','word','pos'])\nfor i_d, (abstr, p_id) in df_covid.loc[idx[0:1000],['abstract','paper_id']].iterrows():\n    df = find_important_pos(abstr, 'NNP', 2)\n    df['p_id'] = p_id\n    df['pos'] = 'NNP'\n    df_imp_pos = df_imp_pos.append(df, sort=False)","70f18497":"data_plot = df_imp_pos.groupby('word')\\\n    .p_id.nunique().reset_index()\\\n    .sort_values(by=['p_id'], ascending=False).head(20)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfig, ax = plt.subplots(1,1, figsize=(8,6))\nax.barh(data_plot['word'], data_plot['p_id'])\nax.invert_yaxis()\nax.set_xlabel(\"Ocurrence of sci. proper nouns\")\nax.set_title(\"Ocurrence of top 20 sci. terms used as proper nouns across a sample of 1000 abstracts\")","66c185dd":"def get_usage_context_pos(doc, grammer):\n    sentences = nltk.sent_tokenize(doc)\n    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n    #tagged_tokens = nltk.pos_tag(tokens)\n    #use simple adjective and noun, and noun -> proper noun\n    #grammar = \"CHUNK: {<VB*>?<JJ*>?<NN|NP|NNP|NNS|NNPS>?<NNP>+}\"\n    df = pd.DataFrame(columns=['phrase', 'pos'])\n    for sent in sentences:\n        tokens = nltk.FreqDist(sent)\n        tagged_tokens = nltk.pos_tag(tokens)\n        #use simple adjective and noun, and noun -> proper noun\n        #grammar = \"CHUNK: {<DT>?<JJ><NN|NP|NNP|NNS|NNPS>?<NN|NP|NNP|NNS|NNPS>}\"\n        #grammar = \"CHUNK: {<PRP><VBP>?<IN|DT>*<NN|NP|NNP|NNS|NNPS>?<NN|NP|NNP|NNS|NNPS>}\"\n        cp = nltk.RegexpParser(grammar)\n        tree = cp.parse(tagged_tokens)\n\n        for subtree in tree.subtrees():\n            if subtree.label() == 'CHUNK':\n                #join words into one n-gram (depending upon how many matched the regex)\n                word = ''\n                for leaf in subtree.leaves():\n                    word += leaf[0] + \" \"  \n                d = {'phrase': word.strip(), 'pos': 'CHUNK'}\n                df = df.append(d, ignore_index=True)\n    return \", \".join(df.loc[:,['phrase']]['phrase'].values.tolist())","7d52574a":"grammar = \"CHUNK: {<VB*>?<JJ*>?<NN|NP|NNP|NNS|NNPS>?<NNP>+}\"\n#looking for abstracts that primarily discuss RNA, but looking at the proper noun used in conjunction with an adjective or other nouns\nfor p_id in df_imp_pos.loc[df_imp_pos['word']=='RNA', ['p_id']].values:\n    print(\"_________________\")\n    doc = df_covid.loc[df_covid['paper_id'] == p_id[0], ['abstract']].values[0][0]\n    print(get_usage_context_pos(doc, grammar))","543ac02d":"#complex phrases when scanned by sentences\nimport pandas\ndef freq_dist_pos_complex_by_sent(doc, grammer, threshold): #todo: handle a regex for the POS\n    sentences = nltk.sent_tokenize(doc)\n    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n    #use simple adjective and noun, and noun -> proper noun\n    #grammar = \"CHUNK: {<DT>?<JJ><NN|NP|NNP|NNS|NNPS>?<NN|NP|NNP|NNS|NNPS>}\"\n    df = pd.DataFrame(columns=['phrase', 'pos'])\n    for sent in sentences:\n        #tokens = nltk.FreqDist(sent)\n        tagged_tokens = nltk.pos_tag(sent)\n        #use simple adjective and noun, and noun -> proper noun\n        #grammar = \"CHUNK: {<DT>?<JJ><NN|NP|NNP|NNS|NNPS>?<NN|NP|NNP|NNS|NNPS>}\"\n        #grammar = \"CHUNK: {<PRP><VBP>?<IN|DT>*<NN|NP|NNP|NNS|NNPS>?<NN|NP|NNP|NNS|NNPS>}\"\n        cp = nltk.RegexpParser(grammar)\n        tree = cp.parse(tagged_tokens)\n    \n        for subtree in tree.subtrees():\n            if subtree.label() == 'CHUNK':\n                #join words into one n-gram (depending upon how many matched the regex)\n                word = ''\n                leaf_count = len(subtree.leaves())\n                for leaf in subtree.leaves():\n                    word += leaf[0] + \" \"  \n                d = {'phrase': word.strip(), 'pos': 'CHUNK', 'leaf_count': leaf_count}\n                df = df.append(d, ignore_index=True)\n    return df","4c97df8d":"grammar = \"CHUNK: {<VB*>?<JJ*>?<NN|NP|NNP|NNS|NNPS>?<NNP>+}\"\nimport random\nrandom.seed(1234)\nidx = np.arange(df_covid.shape[0])\nrandom.shuffle(idx)\ndf_imp_pos = pd.DataFrame(columns=['p_id','pos','phrase','leaf_count'])\nfor i_d, (abstr, p_id) in df_covid.loc[idx[0:1000],['abstract','paper_id']].iterrows():\n    df = freq_dist_pos_complex_by_sent(abstr, grammar, 2)\n    df['p_id'] = p_id\n    df_imp_pos = df_imp_pos.append(df, sort=False)\ndf_imp_pos.head()","55864c11":"data_plot = df_imp_pos.loc[df_imp_pos['leaf_count']>=2, ].groupby('phrase')\\\n    .p_id.nunique().reset_index()\\\n    .sort_values(by=['p_id'], ascending=False).head(30)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfig, ax = plt.subplots(1,1, figsize=(8,6))\nax.barh(data_plot['phrase'], data_plot['p_id'])\nax.invert_yaxis()\nax.set_xlabel(\"Ocurrence of sci. proper nouns\")\nax.set_title(\"Ocurrence of top 20 sci. phrases used as proper nouns across a sample of 1000 abstracts\")","d1601258":"grammar = \"CHUNK: {<VBD|VBP>?<JJ*>?<NN|NP|NNP|NNS|NNPS>?<NNP>+}\"\nphrase_to_match = \"real-time RT-PCR\"\nfor p_id in np.unique(df_imp_pos.loc[df_imp_pos['phrase']==phrase_to_match, ['p_id']].values):\n    print(\"_________________\")\n    doc = df_covid.loc[df_covid['paper_id'] == p_id, ['abstract']].values[0][0]\n    sentences = nltk.sent_tokenize(doc)\n    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n    for sent in sentences:\n        tagged_tokens = nltk.pos_tag(sent)\n        cp = nltk.RegexpParser(grammar)\n        tree = cp.parse(tagged_tokens)\n        for subtree in tree.subtrees():\n            if subtree.label() == 'CHUNK':\n                #join words into one n-gram (depending upon how many matched the regex)\n                word = ''\n                leaf_count = len(subtree.leaves())\n                for leaf in subtree.leaves():\n                    word += leaf[0] + \" \"  \n                if(phrase_to_match.strip() == word.strip()):\n                    print(\" \".join(sent))","cf7127d7":"def find_claims_in_abstracts(phrase_to_match, pivot_word=\"reveal\"):\n    grammar = \"CHUNK: {<VBD|VBP>+}\"\n    for p_id in np.unique(df_imp_pos.loc[df_imp_pos['phrase']==phrase_to_match, ['p_id']].values):\n        print(\"_________________\")\n        doc = df_covid.loc[df_covid['paper_id'] == p_id, ['abstract']].values[0][0]\n        sentences = nltk.sent_tokenize(doc)\n        sentences = [nltk.word_tokenize(sent) for sent in sentences]\n        for sent in sentences:\n            tagged_tokens = nltk.pos_tag(sent)\n            cp = nltk.RegexpParser(grammar)\n            tree = cp.parse(tagged_tokens)\n            for subtree in tree.subtrees():\n                if subtree.label() == 'CHUNK':\n                    #join words into one n-gram (depending upon how many matched the regex)\n                    word = ''\n                    match = False\n                    leaf_count = len(subtree.leaves())\n                    for leaf in subtree.leaves():\n                        sim = similarity(leaf[0], pivot_word)\n                        if(sim >= .8):\n                            match = True\n                            break;\n                    if (match == True):\n                        print(\" \".join(sent))","a049ec0a":"find_claims_in_abstracts(\"real-time RT-PCR\", \"reveal\")","6335bb27":"find_claims_in_abstracts(\"real-time RT-PCR\", \"discover\")","42c90c47":"find_claims_in_abstracts(\"viral RNA\", \"reveal\")","dc5a14d8":"find_claims_in_abstracts(\"viral RNA\", \"detect\")","d1c37908":"find_claims_in_abstracts(\"viral RNA\", \"discover\")","e22144ec":"Clearly, this absract does talk about studies conducted to determine the nature of human to human contact that can promote transmission.\nNow that we have a fairly good approach to extracting abstracts that are primarily referring to specific important words or similar words in similar context. Let's run this at scale and see what we can find about our specific research questions.","3e007ad0":"We can tell from the above that in the sub-sample we are examining, RNA, SARS, MERS-CoV, FIPV etc. have been extensively discussed. We can easily identify the abstracts that primarily discuss these dieases and help a researcher locate useful information from these. Let's see if we can take a 2nd pass at these abstracts to identify the context in which these terms are used.","9af10fa4":"As seen from the above abstract, it does primarily talk about spreading and transmission. Another example is from the word 'contacts', let's examine it below.","0f0c5889":"The word 'incubation' is not in unique important words list, so we will find words that are similar to 'incubation' in the unique words list","9ca98563":"There quite a few important matches here where RNA is used in conjunction with some adjectives that provide a little more context to the terms such as \"positive-strand RNA\" (which is mentioned more than once in the sub-sample of the abstracts), \"single-stranded RNA\" etc. We could optionally repeat the above process of POS tag matching to include an adjective and a noun that would potentially match for such bigrams and perhaps be more precise in information extraction.","4e3e67e5":"Get a list of unique words in the corpus of the limited (randomly sampled 500 abstracts) dataset and find documents that are using some specific words or similar words. Let's try with the word 'transmission'.","42a69dbe":"# We will focus on discovering insights about the abstracts, specifically what they are primarily talking about and use that information to match \/ group abstracts that are similar \n\n# We will also attempt to demonstrate how we can automatically extract information about experimental findings and claims made by researchers\n\n\nThe challenge here is dealing with scientific terms. We do not know specifically what to look for that makes sense to us non-virologists or non-epidemiologists. E.g. searching for incubation or virus will return several matches used in different contexts that may not be useful to an expert looking at our analysis.\n\nInstead of looking for specific words such as virus, vaccine or incubation, given the lack of domain expertise here, I would instead prefer to determine important words or phrases from the text.","b8672e42":"We have even better insights when using the word 'discover'.","72c14849":"We are now able to identify that the abstract is about HBV and MK886 (PPAR\u03b1) and also about 'expressions' (gene?). We would have ideally preferred identify the context in which the word expression is used. We will get to that later in the notebook. For now, we will focus on single words only.\n### Random sample some abstracts from the corpus and see if such words seen across abstracts provide us any useful information about  the important words used frequently in this type of research","1385a267":"There are largely nouns ('NN') and a very few personal pronouns (PRP). **The PRP's might used to refer to claims by researchers**, so we may want to take a closer look at them. The number of proper nouns (NNP) are far less when compates to the NN's. Let's look at those.","377e59e0":"Let's examine the result for the word 'reveal'","c703bf79":"A few important observations:\n* Scientific terms are used extensively\n* Short form of phrases are used after being defined once, e.g. HBV, PTT22-vector etc.\n* Some findings is also reported e.g. *\"Therefore, \u03b1-mannosidase I may be a novel drug target...\"*\n\nTypically, we approach the problem by extracting words from the documents and using those words to perform several tasks such as\n* finding most frequently used words\n* findings documents that are similar to each other based on the words used\n\nWe will use a slightly different approach here. If you read the abstract carefully, you will notice that it repeatedly refers to 'bone repair' or 'bone regeneration'. If we build a feature extractor that extracts these important keywords that provide us an insight about what the abstract is 'in general' referring to, we can limit our feature set to a manageable size and also remove a lot of noise.\n\nWe will start with a a simple approach to find words that are frequently used in an abstract","b1df1704":"Let's see how we did on these randomly sampled 500 abstracts","82a28d7a":"There are 25 unique POS that were found in the text. Let's look at their distribution","b3980238":"None of the abstract or paper_id fields are null. So we are good to proceed with our EDA. Let's begin by looking at the first abstract we have here.","880532b2":"For a threshold of 3, it was able to identify the short notation for **hepatitis B virus (HBV)**\nIf we relax the threshold, can we do a little better?","2688ba88":"We will use the NLTK package to extract some of the important POS (Part-Of-Speech) from the text here.","b5f42411":"Find **important proper nouns** across abstracts and see how they are distributed ","09e5cf19":"Let's see how this routine does","642f5502":"Let's see how our routine did on the first abstract","4ceb1e07":"We have about several matches that were identified as ones primarily discussing specific terms. This is way too many for a researcher to scan manually. We will adjust the threshold value in the similarity match to reduce this list to a manageable number.","64fa84d6":"Let's see if we can figure out what words there are in our *limited* dataset that are similar to the word 'antiviral'","068a8c44":"The above shows the different types of findings using the RT-PCR (reverse transcription polymerase chain reaction) method. The above methods can be easily used to provide a **graphical interface** to researchers so as to make the navigation from important scientific phrases to the usage of them in actual sentences. We shall attempt to do that in a subsequent notebook. \nOne interesting observation we make is the use of certain phrases to convey the fact that there were some findings from these experiments and procedures. \nIt would be helpful to be able to extract these automatically from the abstracts. Let's take the same example above and try to grab such claims from the text.\n\nFor this expriment, we will need to pivot on a word. We will choose 'reveal' as the word that will most commonly refer to findings or claims.\nWe will use the POS tags to locate a a verb (past or present tense) since 'reveal' is one such POS.","97112244":"We will now use some similarity detection techniques so that we can find documents that not only refer to the identical word, but similar words so that we can broaden our match.\nI will use a similarity computation routine that is available here https:\/\/www.programcreek.com\/python\/example\/91606\/nltk.corpus.wordnet.wup_similarity","e8d69747":"Again, the information seems to provide some clear insights about the findings from various experiments conducted. Use of different pivot words seems to provide slightly different but useful sets of information.","4dab7dc2":"There are several values that are close to zero or 'small' while only a few that are 'large'. This indicates that we might need a mechanism to choose a cut-off for picking a similarity threshold value. Let's write a routine to find similar words based on a cut_off or threshold (set to 98 percentile value of the similarity measure)","cf0c893b":"We seem to have done well. We have 'lozenge', 'vaccines', 'assay' etc. that seem relevant here. \nWe do seem some issues such as 'cat' that may be a noise and we can investigate those anomalies to improve on our similarity detection task.\n\nLet's look at the distribution of the similarity measures with respect to the word 'antiviral'","938af142":"Let's now look at how the important words are distributed among the 500 randomly selected documents","3277217a":"We did find several matches for incubation, however not many look very relevant. Even though 'binding' is a word used in the same context as 'incubation', the rest don't look that useful. \nThe above routine uses only one word and finds words similar to it across the abstracts in the sub-sample. We will enhance the routine to use all the important words in an abstract and then compare other abstract that use words similar to any of the important words used in the abstract.","7488e2bf":"Apparently, we have an abstract here that is discussing the contact patterns that determine transmission which is remotely related to incubation.\n\nLet's put this together in a routine","1a4b13e7":"Now that we have processed the documents, we will now start the EDA that our kernel focusses on. I will focus on the abstract and leave the body alone for now. Let' see if any data clean up is necessary.","baebe0f5":"We find several words that are imporant in the context of a single abstract that are also imporant in the context of other abstracts. The intuition here is that these abstracts identified by the p_id in the dataset above refer extensively to the words mentioned against it in the dataset.","7e3a2cf2":"We will look for these relevant words in the similarity matrix","160db3e5":"#### Find abstracts that are similar to each other with respect to the important words used, using word similarity measures \nAn abstract may have more then one important word, and we plan to use all of their similarity measures to compute the similarity between abstracts.","879ca515":"### Analyzing sentences and the contexts\nWe will now attempt to extract some context and if possible meaningful phrases from sentences used in the abstracts.\nFirst, let's go back to our sample abstract and figure out what type of analysis we would like to perform.","26418c77":"We can only look at NNPs and look at the ones that are used more often in the abstract. However, the count might still be unmanageable as we cover the larger corpus so we may need to apply a cut off to select only a few NNPs per abstract. We use our previous method of selecting impotant terms to selecting important NNPs.","c1ff48f8":"At the outset, the first few words such as 'patients' and 'infection' may not seem very useful, but if you look carefully, there are quite a few important words such as 'pigs', 'respiratory', 'amplification' etc. that may we very useful for researchers if they wish to refer to abstracts that extensively mention those words.","57be2fe8":"Let's check for the term 'transmission'","5a1852d2":"Let's look the abstract that refers to 'contact'","01ea3000":"The routine seems to be able to correctly identify that infection and disease are used in similar contexts. Again examples such as 'one' and 'four', 'dna' and 'rna' further confirm the usage of the routine. However 'influzenza' and 'virus' have low similarity because even though they may be used together in several literatures, they are never used in the same context.\n\nWe will go back to the randomly sampled 500 abstracts *df_imp_words_all_docs* and try this routine on the unique important words seen in that dataset.\nFor this we need to write a routine to find the closest word (based on the *similarity* function) for any given word.","a67e55bb":"'transmission' and other words used in similar context has been used extensively in 13 of these 500 abstracts. Let's look at one of them, the one corresponding to 'spreading'","2fcbc499":"We seem to have done a fairly good job in finding abstracts that discuss topics similar to transmissions.","9b83bc5e":"Interestingly, these NNPs are very useful in extracting the scientific notations or short forms of specific scientific terms being discussed in the abstract.","4640abf3":"Above, we see the distribution of all important words across the sub-sample of the randomly chosen 500 abstracts. \nNote: This is not a simple distribution of words across the corpus. In fact, the words corresponing to the larger 'bar' indicate the number of abstracts that extensively discuss those terms. We will look at an example:","45059548":"From the distribution we have been able to extract some useful phrases such as 'viral RNA', 'genomic RNA', 'real-time RT-PCR', 'quantitative PCR' etc. (Close to 1% of the abstracts in our sub-sample refer to the RT-PCR\/PCR process aparently). Some of these my refer to nameds of specific scientific procedures and it might be useful to look at what these abstracts are commonly referring to.","805c9d6e":"We have correctly identified the claims \/ statements referring to findings for abstracts that primarily talk about the \"real-time RT-PCR\" procedure.","f8fbea03":"All these documents (in the randomly sampled 500 abstracts) talk extensively abour 'sars'. Let's verify that.","2eea5d74":"But first, lets get the documents processed. I am going to reuse the preprocessing code from a kernel in this challenge.\nUsing the preprocessor from the kernel https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering for processing the dataset","b270290f":"We do find several abstracts that refer to transmission. Note that this is not the entire corpus of 29K abstracts. This is only a 2% sample of the corpus, so the match is not exhaustive. Despite the fact, we still manage to find several abstracts that refer to terms similar to 'transmission'.","60652744":"We have done quite well here as seen from the above results. We may need to further take a second pass on the similarity values if a word is quite unique in a given abstract and has no similar matches elsewhere in the corpus, in which case, even with a cut-off, the similarity measure may be poor.\n\n#### Lets get back to our important words document list and determine other documents that use similar important words\nWe write a new routine that will first find words in the dataset across abstracts that are similar and then using the information about the abstracts that use those words will find abstracts that are similar with respect to those words."}}