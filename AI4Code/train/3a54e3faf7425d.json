{"cell_type":{"22c38d43":"code","a864772e":"code","279635ae":"code","66349436":"code","779a7c70":"code","ce02e002":"code","23343990":"code","41a58ec3":"code","446c634e":"code","369b309f":"code","9c26ac5b":"code","340f8e63":"code","9ae6132e":"code","d701c066":"code","b87a61e0":"code","bda40e2e":"code","cede0156":"code","1dba2b76":"code","731b9743":"code","c5098393":"code","fcded789":"code","98a8841a":"code","83e3cee3":"code","106a425f":"code","cee5c59c":"markdown","351abaac":"markdown","a4769b20":"markdown","8fd04be2":"markdown","710d11d6":"markdown","0d3f3f5b":"markdown","a634cbeb":"markdown","96a6dc17":"markdown","bc43c6b1":"markdown","bfba7ed3":"markdown","42da3aab":"markdown","dbc67a0b":"markdown","b593f325":"markdown","b219760c":"markdown","ce835742":"markdown","8b5a8793":"markdown","beaad9b4":"markdown","d62ce1a2":"markdown","e53da1b1":"markdown","718e7fae":"markdown"},"source":{"22c38d43":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a864772e":"import numpy as np\nimport pandas as pd\nimport matplotlib as plt\nimport seaborn as sns\nimport sklearn.utils as skutils\nimport math\nfrom sklearn.model_selection import train_test_split","279635ae":"ploan_data = pd.read_csv(\"\/kaggle\/input\/bank-personal-loan\/Bank_Personal_Loan_Modelling.csv\")\nprint (\"Rows:\",ploan_data.shape[0] ,\" Columns:\",ploan_data.shape[1] )\nploan_data.head(10)","66349436":"ploan_data.info()\n#This clearly shows that all the data types are numerical and there are no objects.\n","779a7c70":"ploan_data.isna().sum()\n#The below results show that there are no null data in the dataset","ce02e002":"ploan_data.describe().T\n#Since there are many categorical columns , we see weird values for mean and median in the data description.\n#Columns  \"Personal Loan\",\"Securities Account\",\"CD Account\",\"Online\",\"Credit Card\" are categorical columns.\n","23343990":"#Transforming the data type of categorical columns\nploan_data[\"Personal Loan\"] = ploan_data[\"Personal Loan\"].astype(\"category\")\nploan_data[\"Securities Account\"] = ploan_data[\"Securities Account\"].astype(\"category\")\nploan_data[\"CD Account\"] = ploan_data[\"CD Account\"].astype(\"category\")\nploan_data[\"Online\"] = ploan_data[\"Online\"].astype(\"category\")\nploan_data[\"CreditCard\"] = ploan_data[\"CreditCard\"].astype(\"category\")\nploan_data[\"ZIP Code\"] = ploan_data[\"ZIP Code\"].astype(\"category\")\n\nprint(ploan_data.info())\nploan_data.describe().T\n","41a58ec3":"#Data distribution of every attribute \nsns.pairplot(ploan_data)\n\n","446c634e":"\nsns.heatmap(ploan_data.corr(), annot = True, cmap= 'coolwarm')","369b309f":"ploan_cleaned = ploan_data.drop([\"Experience\",\"ID\",\"ZIP Code\"],axis=1)\n\nploan_cleaned.head()","9c26ac5b":"#Target Column Distribution\n#Since the target column is a categorical column, we use the plot of counts.\n\nvalue_counts = ploan_cleaned[\"Personal Loan\"].value_counts()\nprint(value_counts)\nsns.barplot(ploan_cleaned[\"Personal Loan\"].unique(),value_counts)\n\n\n# From the barplot and value_counts , we see a heavy imbalance in classes. \n# Class imbalance can cause the model to behave wrongly on the test data since the model would not have seen much of \n# data in the minority class. \n","340f8e63":"# So we try to resample the data to manage the class imbalance problem\nploan_cleaned_min = ploan_cleaned[ploan_cleaned[\"Personal Loan\"] == 1] \nploan_cleaned_maj = ploan_cleaned[ploan_cleaned[\"Personal Loan\"] == 0]\nploan_cleaned_min_upsampled = skutils.resample(ploan_cleaned_min,n_samples=4520,random_state=1);\n\nploan_upsampled = pd.concat([ploan_cleaned_maj,ploan_cleaned_min_upsampled])\n\nploan_upsampled[\"Personal Loan\"].value_counts()\n                                     ","9ae6132e":"# Splitting the data into independent and dependent variables\n\ny = ploan_upsampled[\"Personal Loan\"]\nX = ploan_upsampled.drop([\"Personal Loan\"],axis=1)\n\n#Splitting the data into training and test set\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=5)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\n\nprint(y_test.shape)","d701c066":"from sklearn.linear_model import LogisticRegression as logisticRegressor\nimport sklearn.metrics as skmetrics\nfrom  sklearn.neighbors import KNeighborsClassifier as knnClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.preprocessing import StandardScaler","b87a61e0":"def get_accuracy(algorithm,X_train,y_train):\n    \n     #Doing 10 fold cross validation to find the average accuracy on the training data\n     cross_val_scores = cross_val_score(algorithm,X_train,y_train,cv=10);\n     print(cross_val_scores)\n     print(\"Average cross validation accuracy on training data:\", cross_val_scores.mean()*100,\"%\")\n     \n    ","bda40e2e":"def fit_and_predict(algorithm,hyperparameter, X_train,y_train,X_test,y_test):\n     \n     if (hyperparameter != None):\n      model = GridSearchCV(algorithm,hyperparameter).fit(X_train,y_train)\n     else:\n      model = algorithm.fit(X_train,y_train)\n     y_pred = model.predict(X_test)\n     test_score = skmetrics.accuracy_score(y_test,y_pred)\n        \n        \n     print(\"Accuracy score on test data:\",test_score*100,\"%\")\n     return model,y_pred","cede0156":"algorithm = logisticRegressor(max_iter=10000,random_state=1);\n\nhyperparameter = {'solver' : ['newton-cg', 'lbfgs','liblinear', 'sag', 'saga']}\n\nget_accuracy(algorithm,X_train,y_train)\nlogisticModel, y_pred_log = fit_and_predict(algorithm,hyperparameter, X_train,y_train,X_test,y_test)\n\n\nprint(logisticModel.best_estimator_)\n","1dba2b76":"scaler = StandardScaler()\nscaledX_train = scaler.fit_transform(X_train)\nscaledX_test  = scaler.fit_transform(X_test)\n\n\nmodel = logisticRegressor(random_state=1)\nmodel.fit(scaledX_train,y_train)\ny_pred = model.predict(scaledX_test)\ntest_score = skmetrics.accuracy_score(y_test,y_pred)\nprint(\"Accuracy score on test data:\",test_score*100,\"%\")","731b9743":"algorithm = knnClassifier();\n\nhyperparameter = {'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n                   'n_neighbors': np.arange(5 , 20 , 2),\n                   'metric': ['euclidean','manhattan','minkowski']}\n\nget_accuracy(algorithm,X_train,y_train)\n\nknnModel ,y_pred_knn = fit_and_predict(algorithm,hyperparameter,X_train,y_train,X_test,y_test)\n\nprint(knnModel.best_estimator_)\n","c5098393":"#Gaussian Naive Bayes is chosen here as the features are partially continuous and partially categorical\n\nget_accuracy(algorithm,X_train,y_train)\nNBModel,y_pred_NB = fit_and_predict(GaussianNB(),None,X_train,y_train,X_test,y_test)","fcded789":"def confusion_matrix(y_test,y_pred):\n    \n        data = {'y_Actual':y_test,\n                'y_Predicted': y_pred\n                }\n\n        cm_df = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\n   \n       \n        cm_crosstab = pd.crosstab(cm_df['y_Actual'], cm_df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n       \n        sns.heatmap(cm_crosstab, annot=True,fmt='d')\n    ","98a8841a":"\nconfusion_matrix(y_test,y_pred_log)\n\nreport = skmetrics.classification_report(y_test,y_pred_log)\nprint(report)\n\n","83e3cee3":"confusion_matrix(y_test,y_pred_knn)\n\nreport = skmetrics.classification_report(y_test,y_pred_knn)\nprint(report)\n\n","106a425f":"confusion_matrix(y_test,y_pred_NB)\n\nreport = skmetrics.classification_report(y_test,y_pred_NB)\nprint(report)\n","cee5c59c":"From the above data \"Experience\" column has min value of -3 which is not correct.","351abaac":"The get_accuracy method is used to do a K-Fold cross validation on the dataset to identify whether the accuracy in each fold of validation is nearly similar. Similar accuracy values show that the data is treated properly and is a good candidate for model creation","a4769b20":" \"Experience\" column has min value of -3 which is not correct.\n\nMoreover we see a strong linear relationship between age and experience.\nThis could potentially create a multicollinearity problem. So we need to drop any one of the column.\n\nWe also see a relation between mortgage , cc average and income columns.\nIt is very clear that as the income increases , the Mortgage and CC Average also increases.\nThis can be re-verified using a correlation heatmap as below\n\nThe heatmap clearly shows the correlation between \"Age\" and \"Experience\" \nAlso it shows the correlation between mortgage , cc average and income columns","8fd04be2":"####  K-NN Model","710d11d6":"###### Inference from Classification Report - Naive Bayes\n\nClass 1 indicates \"Getting a personal loan\" and analysis is done using class 1 as \"Positive\".\n\nThe f1 score is 81% which indicates that the model is not performing well compared to the earlier models.\n\nWe see a good difference in the values of precision and recall. \n\nThe trade-off decision has to be made considering the business case.","0d3f3f5b":"To avoid multi-collinearity we drop the experience column.\nExperience column is chosen to be dropped as it already has impure data (negative values).\nWe also drop the ID and zip code columns","a634cbeb":"#### Confusion Martix - Logistic Regression","96a6dc17":"#### Naive Bayes Model","bc43c6b1":"We see that the accuracy has not improved greatly by scaling the data.","bfba7ed3":"#### Logistic Regression","42da3aab":"#### Trying to improve the accuracy of the model by scaling the feature vectors","dbc67a0b":"###### Inference from Classification Report - Logistic Regression\n\nClass 1 indicates \"Getting a personal loan\" and analysis is done using class 1 as \"Positive\".\n\nThe f1 score is 90% which indicates that the model is not performing poor.\n\nSince the precision and recall are exactly same ,this model seems to have a decent trade-off and should perform well in most of the cases","b593f325":"The fit_and_predict method is used to train the model, tune its hyperparameters and get the accuracy score on the test data\n","b219760c":"# Supervised Learning - Bank Personal Loan Modelling","ce835742":"###### Exploratory Data Analysis","8b5a8793":"Analysis of Confusion matrix is done by considering 1 as positive and 0 as negative. \n1 implies getting a personal loan and\n0 implies not getting a personal loan","beaad9b4":"#### Confusion Martix  K-NN","d62ce1a2":"###### Inference from Classification Report - K-NN\n\nClass 1 indicates \"Getting a personal loan\" and analysis is done using class 1 as \"Positive\".\n\nThe f1 score is 96% which indicates that the model performing very well.\n\nWe see a good difference in the values of precision and recall.\n\nSo trade-off decision has to be made considering the business case\n","e53da1b1":"#### Confusion Martix - Naive Bayes","718e7fae":"On comparing the above models, we see that K-NN performs better that Naive Bayes and Logistic Regression in this case.\n\n### Reasoning (Analysis is done using \"Getting a personal loan\" as positive and denoted by class 1)\n\n#### Logistic Regression \nPrecision :90%\nRecall :90%\nAccuracy :90%\n\nPrecision 90% means if the model predicts that a customer will get personal loan, it is 90% correct.\nRecall 90% means (100 - 90 = 10 )% of customers who will potentially take a personal loan are wrongly classified as they will not take personal loan.  \nSince 10% of  potential customers are missed out due to wrong prediction ,this model might not be suitable for this business case.\n\n\n#### K-NN \n\nPrecision :92%\nRecall :100%\nAccuracy :96%\n\nPrecision 92% means if the model predicts that a customer will get personal loan, it is 92% correct.\nRecall 100% means (100 - 100 =0 )% of customers who will potentially take a personal loan are wrongly classified as they will not take personal loan.  \nSince no potential customers are missed out due to wrong prediction ,this model performs the best for this business case\n\n#### Naive Bayes\n\nPrecision :87%\nRecall :76%\nAccuracy :81%\n\nPrecision 87% says that if the model predicts that a customer will get personal loan, it is 87% correct.\nRecall 76% says that (100-76 = 24)% of customers who will potentially take a personal loan are wrongly classified as they will not take personal loan.\n\nThis is risky because we might end up missing 24% of  potential customers who will take personal loan. Hence this model cannot be considered for prediction\n\n#### Why K-NN peforms better than Logistic Regression and Naive Bayes ?\n\nSince there are multiple features to be considered for prediction, the sigmoid function drawn by Logistic Regression might have errors in fitting. \nMoreover, the feature set is partially categorical and partially continuous. \nThis might induce certain errors in prediction. \n\nNaive-Bayes has the fundamental assumption that all the features are independent and have an indpendent contribution towards the predicition probability of the dependent variable. This is not true in our case and therefore has a poor performance compared to the other algorithms. \n\n\nWhereas K-NN is non-paramteric and does not take into account any of the assumptions on the data. \nSince decision is taken predominantly by the influence of neighbours alone\n,KNN has a better perdiction accuracy in this business case."}}