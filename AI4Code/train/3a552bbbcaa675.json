{"cell_type":{"e459b406":"code","bdae1641":"code","8056e30f":"code","7eb74143":"code","2203c0e1":"code","e96bd91c":"code","7dfe1a76":"code","b32f81c4":"code","b207fd45":"code","c2afecf7":"code","50c7aa7c":"code","a62036b2":"code","765a4767":"code","866caddd":"code","f4f292d0":"code","f2d1b82a":"code","37590c81":"code","67ede0da":"code","e6ac6200":"code","c122f6af":"code","4ee7342b":"code","4325c281":"code","52fc5e4c":"code","a10c22e4":"code","ffda3c7a":"code","52b36eee":"code","136eade5":"code","9fa78c40":"code","46561f4c":"code","f23a1d26":"code","43059fbb":"code","c52be5fa":"code","619e2315":"code","5b5f2fe5":"code","7590df79":"code","2ce39c42":"code","d3472d3c":"code","4d81c8c0":"code","f388bcc8":"code","e42c1751":"code","3c3f5b79":"code","8cc2c59b":"code","40e9c3cf":"code","3c466956":"code","be754571":"code","39a2e616":"code","63fa43b8":"code","bf4b46b6":"code","f1767379":"code","124aaf70":"code","37609e85":"code","11eb6df0":"code","85262358":"code","36b80d16":"code","d97e932a":"code","e65427c4":"code","055c5c27":"code","71cf7830":"code","93c3802f":"code","8f8e5e92":"code","b6f83792":"code","81670470":"code","49e692ef":"code","683ac515":"code","59476e30":"code","ec61a9ae":"code","cdb0b41f":"code","7e9766c1":"code","c27b7749":"code","42ac6693":"code","48d8ecc0":"code","6d717615":"code","ad94d027":"code","300edfb5":"code","6b13a0ee":"code","dc838907":"code","d38db16c":"code","5c10aae3":"code","6b8fde05":"code","4a3a220d":"code","c543942f":"code","a0bff0e4":"code","261c9b26":"code","898587c1":"code","b0ddab1b":"code","71fd5851":"code","998990bf":"code","ccf72ac5":"code","ee3eeedc":"code","bb86679d":"code","fb697976":"code","c69f350f":"code","8f30fcac":"code","65fd9dd9":"code","859edbbc":"code","35e1f2e8":"code","801108d4":"code","e3ed90e0":"markdown","d5d2cde3":"markdown","6fa12054":"markdown","dfd26edd":"markdown","8b12377e":"markdown","02968c8b":"markdown","1113a73a":"markdown","22b57b4a":"markdown","b175d92c":"markdown","2a7dfd14":"markdown","75af5e45":"markdown","9d04a580":"markdown","af974fc5":"markdown","d27b29b5":"markdown","7f06f6cb":"markdown","b14565e1":"markdown","9ebcec00":"markdown","047de4b3":"markdown","3b020788":"markdown","5a78b5ca":"markdown","76b470d7":"markdown","2d27e8be":"markdown","df967533":"markdown","816f8122":"markdown","4b7a436d":"markdown","60d9b188":"markdown","6110247c":"markdown","13f8d411":"markdown","77bc7c2b":"markdown","7d784fcb":"markdown","1762c434":"markdown","26b92630":"markdown","92714068":"markdown","8c3b3ca5":"markdown","22e23d3f":"markdown","a1f1951f":"markdown","3e1228a1":"markdown","a3634cb6":"markdown","a303105e":"markdown","3b6358c5":"markdown","73b0f4ac":"markdown","6d49c10a":"markdown","800c32bf":"markdown","d8f4f4e2":"markdown","8ca2cf4c":"markdown","c2a07149":"markdown","0cbde67e":"markdown","2db4247d":"markdown","69812fe1":"markdown","511bdca1":"markdown","1f2e0622":"markdown","707c035b":"markdown","5285762b":"markdown","bf8a0da0":"markdown","4f1f67b7":"markdown","089bf8f7":"markdown","cf78066b":"markdown","2eb8cca9":"markdown","d03ecdc7":"markdown","1da7d705":"markdown","6b062365":"markdown","a384c25a":"markdown","c9d172ee":"markdown","36fce1ab":"markdown","b7120d06":"markdown","e593cee5":"markdown","98b75e45":"markdown","59edb0b8":"markdown","e6edf8ec":"markdown","bea5de0b":"markdown","4119dddc":"markdown","b8f68d02":"markdown","d0c8e442":"markdown","7c18c564":"markdown","f59bbf65":"markdown","dfe7300a":"markdown","fd12d6df":"markdown","b95eb95e":"markdown","c3bd2e0a":"markdown","773235c4":"markdown","c086d581":"markdown","859780b5":"markdown","da251961":"markdown"},"source":{"e459b406":"#Import Libraries\nimport numpy as np \nimport random  \nimport pandas as pd \nimport os\nimport glob\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport string\nimport itertools\nfrom nltk import word_tokenize, FreqDist\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import scale\n\nimport gensim\nfrom gensim.models.word2vec import Word2Vec \nfrom tqdm import tqdm\n\nimport bokeh.plotting as bp\nfrom bokeh.models import HoverTool, BoxSelectTool\nfrom bokeh.plotting import figure, show, output_notebook\n\nimport keras\nfrom keras.preprocessing import text, sequence\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model, Sequential\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.layers import Input, Dense, LSTM, Embedding, Dropout","bdae1641":"#List of csv files with twitter data\nPATH = \"..\/input\/russian-troll-tweets\"\nprint(os.listdir(PATH))","8056e30f":"#Create filenames\nfilenames = glob.glob(os.path.join(PATH, \"*.csv\"))\nprint(filenames)","7eb74143":"#Combine csv files\ndf_raw = pd.concat((pd.read_csv(f) for f in filenames))\nprint(df_raw.shape)\ndf_raw.tail()","2203c0e1":"#number of unique authors\nlen(df_raw.author.unique())","e96bd91c":"#Look at types of data included in each field.  \ndf_raw.info()","7dfe1a76":"#Are there any missing values?  \ndf_raw.isna().sum().sort_values(ascending = False)","b32f81c4":"#Remove 1 content N\/A row\ndf_raw.dropna(subset = ['content'], inplace = True)\ndf_raw.isna().sum().sort_values(ascending = False)","b207fd45":"#Looking at data in more detail\ndf_raw.describe(include=\"all\")","c2afecf7":"#dropping columns that won't be used\ndf = df_raw.drop(['external_author_id', 'harvested_date'], axis=1)\ndf.head()","50c7aa7c":"#review languages of tweets\ndf.language.value_counts(normalize=True).head()","a62036b2":"#update data to only contain english tweets\ndf = df.loc[df.language == 'English']\nprint(df.shape)\ndf.drop(['language'], axis = 1, inplace = True)\ndf.head()","765a4767":"#review missing data again\ndf.isna().sum().sort_values(ascending = False)","866caddd":"#Look at region in more detail\ndf.region.unique()","f4f292d0":"#value counts of region\ndf.region.value_counts(normalize=True)","f2d1b82a":"#rename region nan values to unknown\ndf['region'].fillna(value='Unknown', inplace = True)\ndf.region.unique()","37590c81":"#recheck regions\ndf.region.value_counts(normalize=True)","67ede0da":"#update data to only contain United States tweets\ndf = df.loc[df.region == 'United States']\nprint(df.shape)\ndf.drop(['region'], axis = 1, inplace = True)\ndf.head()","e6ac6200":"#review nan values again\ndf.isna().sum().sort_values(ascending = False)","c122f6af":"#look at account_type in more detail\ndf.account_type.unique()","4ee7342b":"#update account_type\ndf['account_type'].fillna(value='Unknown', inplace = True)\ndf['account_type'].replace({'?': 'Unknown', 'right': 'Right', \n                            'left': 'Left', 'news': 'News', \n                           'local': 'Local', 'ZAPOROSHIA': 'Zaporoshia'}, \n                           inplace = True)\ndf.account_type.unique()","4325c281":"#revisit missing data\ndf.isna().sum().sort_values(ascending = False)","52fc5e4c":"#look further at post_type\ndf.post_type.unique()","a10c22e4":"#Check retweet column vs post_type\ndf_post_type_nan = df.loc[df.post_type.isna()]\ndf_post_type_nan.retweet.unique()","ffda3c7a":"#Check retweet column vs post_type\ndf_post_type_notnan = df.loc[df.post_type.notnull()]\ndf_post_type_notnan.retweet.unique()","52b36eee":"#update post_type\ndf['post_type'].fillna(value='NOT_RETWEET', inplace = True)\ndf.post_type.unique()","136eade5":"#confirm there is no more missing data\ndf.isna().sum().sort_values(ascending = False)","9fa78c40":"#look at info\ndf.info()","46561f4c":"# convert publish_date to datetime format\ndf['publish_date'] = pd.to_datetime(df['publish_date']).dt.date\ndf.head()","f23a1d26":"#check publish_date\ndf.info()","43059fbb":"#review updated data\ndf.describe(include=\"all\")","c52be5fa":"#review data first 5 rows\ndf.head()","619e2315":"#resample data for eda purposes\neda_sample = df.take(np.random.permutation(len(df))[:50000])\nprint(len(eda_sample))\neda_sample.head()","5b5f2fe5":"#Check original distribution\ndf.hist(figsize = (16,8))","7590df79":"#Check eda distribution\neda_sample.hist(figsize = (16,8))","2ce39c42":"#look at original categorical features\nfig, ax =plt.subplots(3,1)\nsns.countplot(df['post_type'], ax=ax[0], color='#95a5a6', order=df['post_type'].value_counts().index)\nsns.countplot(df['account_type'], ax=ax[1], color='#2ecc71', order=df['account_type'].value_counts().index)\nsns.countplot(df['account_category'], ax=ax[2], color='#3498db', order=df['account_category'].value_counts().index)\nax[0].set_xlabel('Post Type')\nax[1].set_xlabel('Account Type')\nax[2].set_xlabel('Account Category')\nax[0].set_ylabel('Tweet Count')\nax[1].set_ylabel('Tweet Count')\nax[2].set_ylabel('Tweet Count')\nfig.set_size_inches(15, 20)\nfig.show()","d3472d3c":"#look at eda sample categorical features\nfig, ax =plt.subplots(3,1)\nsns.countplot(eda_sample['post_type'], ax=ax[0], color='#95a5a6', order=eda_sample['post_type'].value_counts().index)\nsns.countplot(eda_sample['account_type'], ax=ax[1], color='#2ecc71', order=eda_sample['account_type'].value_counts().index)\nsns.countplot(eda_sample['account_category'], ax=ax[2], color='#3498db',order=eda_sample['account_category'].value_counts().index)\nax[0].set_xlabel('Post Type')\nax[1].set_xlabel('Account Type')\nax[2].set_xlabel('Account Category')\nax[0].set_ylabel('Tweet Count')\nax[1].set_ylabel('Tweet Count')\nax[2].set_ylabel('Tweet Count')\nfig.set_size_inches(15, 20)\nfig.show()","4d81c8c0":"#account_category\nplt.figure(figsize=(16,8))\nax = sns.scatterplot(x=\"followers\", y=\"following\", hue=\"account_category\",data=eda_sample)\nplt.title('Follower and Following Counts by Account Category', fontsize = 15)\nplt.xlabel('Number of Followers', fontsize = 13)\nplt.ylabel('Number of Following', fontsize = 13)\nplt.legend(loc = 'best')","f388bcc8":"#account_type\nplt.figure(figsize=(16,8))\nax = sns.scatterplot(x=\"followers\", y=\"following\", hue=\"account_type\",data=eda_sample)\nplt.title('Follower and Following Counts by Account Type', fontsize = 15)\nplt.xlabel('Number of Followers', fontsize = 13)\nplt.ylabel('Number of Following', fontsize = 13)","e42c1751":"#Look at the NonEnglish account category\nnon_engl_acc_cat = eda_sample.loc[eda_sample.account_category == 'NonEnglish']\nnon_engl_acc_cat.account_type.value_counts(normalize=True)","3c3f5b79":"#post_type\nplt.figure(figsize=(16,8))\nax = sns.scatterplot(x=\"followers\", y=\"following\", hue=\"post_type\",data=eda_sample)\nplt.title('Follower and Following Counts by Post Type', fontsize = 15)\nplt.xlabel('Number of Followers', fontsize = 13)\nplt.ylabel('Number of Following', fontsize = 13)","8cc2c59b":"# Count the number of times a date appears and convert to dataframe\ntweet_trend = pd.DataFrame(df['publish_date'].value_counts())\n\n# index is date, columns indicate tweet count on that day\ntweet_trend.columns = ['tweet_count']\n\n# sort the dataframe by the dates to have them in order\ntweet_trend.sort_index(ascending = True, inplace = True)\n\n# plot\ntweet_trend['tweet_count'].plot(linestyle = \"-\", figsize = (16,8), color = 'blue')\nplt.title('Tweet Count by Date', fontsize = 15)\nplt.xlabel('Date', fontsize = 13)\nplt.ylabel('Tweet Count', fontsize = 13)","40e9c3cf":"#Important dates from Trumps campaign\n#https:\/\/www.reuters.com\/article\/us-usa-election-timeline-factbox\/timeline-pivotal-moments-in-trumps-presidential-campaign-idUSKBN1341FJ\ndates_list = ['2015-06-16', '2015-12-07', '2016-02-01',\n              '2016-03-01', '2016-03-03', '2016-03-11',\n              '2016-05-03', '2016-05-26', '2016-06-20', \n              '2016-07-15', '2016-07-21', '2016-08-17',\n              '2016-09-01', '2016-10-07', '2016-11-08']\n\n# create a series of these dates.\nimportant_dates = pd.Series(pd.to_datetime(dates_list))\n\n#add columns to identify important events, and mark a 0 or 1.\ntweet_trend['Important Events'] = False\ntweet_trend.loc[important_dates, 'Important Events'] = True\ntweet_trend.head()\ntweet_trend['values'] = 0\ntweet_trend.loc[important_dates, 'values'] = 1","3c466956":"# mark important events\ntweet_trend['tweet_count'].plot(linestyle = \"-\", figsize = (12,8), rot = 45, color = 'blue', label = 'Tweet Counts')\n\n# plot dots for where values in the tweet_trend df are 1\nplt.plot(tweet_trend[tweet_trend['Important Events'] == True].index.values,\n         tweet_trend.loc[tweet_trend['Important Events'] == True, 'values'],\n         marker = 'o', color = 'red', linestyle = 'none', label = \"Important Dates in Trump's Campaign\")\n\n# Adding a 30 day moving average on top to view the trend\nplt.plot(tweet_trend['tweet_count'].rolling(window = 30, min_periods = 10).mean(), \n         color = 'red', label = '30 Day Moving Avg # of Tweets')\n\nplt.title('Tweet Count by Date', fontsize = 15)\nplt.xlabel('Date', fontsize = 13)\nplt.ylabel('Tweet Count', fontsize = 13)\nplt.legend(loc = 'best')","be754571":"#Look further at the huge spike in 2016 that had 17,500+ tweets\ndf.publish_date.value_counts()[:20]","39a2e616":"#look at account categorys of those who tweeted on the most tweeted day 10\/6\/2016\ndf_2016_10_06 = df.loc[df['publish_date'] == pd.to_datetime('2016-10-06')]\nprint(len(df_2016_10_06))\n\nsns.countplot(df_2016_10_06['account_category'], color='#3498db', order=df_2016_10_06['account_category'].value_counts().index)\nplt.xlabel(\"Account Category\", fontsize = 13)\nplt.ylabel(\"Tweet Count\", fontsize = 13)\nplt.title(\"Tweet Count on October 6, 2016\", fontsize = 15)\nfig.set_size_inches(15, 20)\nfig.show()","63fa43b8":"#look at account categorys of those who tweeted on the most tweeted day 10\/7\/2016\ndf_2016_10_07 = df.loc[df['publish_date'] == pd.to_datetime('2016-10-07')]\nprint(len(df_2016_10_07))\n\nsns.countplot(df_2016_10_07['account_category'], color='#2ecc71', order=df_2016_10_07['account_category'].value_counts().index)\nplt.xlabel(\"Account Category\", fontsize = 13)\nplt.ylabel(\"Tweet Count\", fontsize = 13)\nplt.title(\"Tweet Count on October 7, 2016\", fontsize = 15)\nfig.set_size_inches(15, 20)\nfig.show()","bf4b46b6":"# Look at data again\nprint(len(df))\ndf.head()","f1767379":"# Cleaning up the tweets column in our dataframe\ndef standardize_text(df, content_field):\n    df[content_field] = df[content_field].str.replace(r\"http\\S+\", \"\")\n    df[content_field] = df[content_field].str.replace(r\"http\", \"\")\n    df[content_field] = df[content_field].str.replace(r\"@\\S+\", \"\")\n    df[content_field] = df[content_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n    df[content_field] = df[content_field].str.replace(r\"@\", \"at\")\n    df[content_field] = df[content_field].str.lower()\n    return df\n\ndf = standardize_text(df, \"content\")","124aaf70":"#review dataframe\ndf.head()","37609e85":"#Additional cleaning with stopwords\nstopwords_list = stopwords.words('english') + list(string.punctuation)\nstopwords_list += [\"''\", '\"\"', '...', '``']","11eb6df0":"#Tokenizing and removing stopwords\ndef process_content(data):\n    tokens = word_tokenize(data)\n    stopwords_removed = [token for token in tokens if token not in stopwords_list]\n    return stopwords_removed   \n\ndf['tokens'] = df['content'].apply(process_content)\ndf['text'] = df['tokens'].apply(' '.join)","85262358":"#Further clean\/check\ndf = df.reset_index()\ndf.drop(['content'], axis=1, inplace=True)\ndf.head()","36b80d16":"## review tokens in more detail\nword_tot = [word for tokens in df['tokens'] for word in tokens]\nword_unique = set(word_tot)\ntweet_len = [len(tokens) for tokens in df['tokens']]\n\nprint('{} total words with a vocabulary size of {}'.format(len(word_tot), len(word_unique)))\nprint('Maximum sentence length is {}'.format(max(tweet_len)))","d97e932a":"#Look at histogram of tweet lengths\nfig = plt.figure(figsize=(10, 10)) \nplt.xlabel('Tweet Length')\nplt.ylabel('Number of Tweets')\nplt.hist(tweet_len)\nplt.show()","e65427c4":"# join tweets to a single string\nall_words = ' '.join(df['text'])\n\nwordcloud = WordCloud(stopwords=STOPWORDS,width=800,height=500,\n                      max_font_size=110, collocations=False).generate(all_words)\n\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","055c5c27":"# join tweets to a single string - Right Trolls\ndf_right = df.loc[df.account_category == \"RightTroll\"]\nright_words = ' '.join(df_right['text'])\n\nwordcloud = WordCloud(stopwords=STOPWORDS,width=800,height=500,\n                      max_font_size=110, collocations=False).generate(right_words)\n\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","71cf7830":"# join tweets to a single string - Left Trolls\ndf_left = df.loc[df.account_category == \"LeftTroll\"]\nleft_words = ' '.join(df_left['text'])\n\nwordcloud = WordCloud(stopwords=STOPWORDS,width=800,height=500,\n                      max_font_size=110, collocations=False).generate(left_words)\n\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","93c3802f":"# join tweets to a single string - News Feed\ndf_news = df.loc[df.account_category == \"NewsFeed\"]\nnews_words = ' '.join(df_news['text'])\n\nwordcloud = WordCloud(stopwords=STOPWORDS,width=800,height=500,\n                      max_font_size=110, collocations=False).generate(news_words)\n\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","8f8e5e92":"# join tweets to a single string - 10\/6\/2016 large spike in tweets\ndf_2016_10_06 = df.loc[df['publish_date'] == pd.to_datetime('2016-10-06')]\ndf_100616_words = ' '.join(df_2016_10_06['text'])\n\nwordcloud = WordCloud(stopwords=STOPWORDS,width=800,height=500,\n                      max_font_size=110, collocations=False).generate(df_100616_words)\n\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","b6f83792":"#Update dataframe for modeling\nclean_data = df.drop(['index','author', 'publish_date', \n                      'following', 'followers', 'updates', 'account_type', \n                      'new_june_2018', 'retweet', 'post_type'], axis=1)\nclean_data.head()","81670470":"print(len(clean_data))","49e692ef":"clean_data.account_category.value_counts(normalize=True)","683ac515":"#renaming account_categories\nclean_data['account_category'].replace({'HashtagGamer': 'Other','NonEnglish': 'Other', 'Unknown': 'Other', \n                            'Fearmonger': 'Other', 'Commercial': 'Other'}, inplace = True)\nprint(clean_data.shape)\nclean_data.account_category.value_counts(normalize=True)","59476e30":"#Define X and y\nX = np.array(clean_data.tokens)\ny = np.array(clean_data.account_category)\n\n#Create Train\/Validate\/Test data splits\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=0.8, random_state=40)","ec61a9ae":"#labelize tweets\nLabeledSentence = gensim.models.doc2vec.LabeledSentence\ntqdm.pandas(desc=\"progress-bar\") #estimate time to completion\n\n#return labelized tweets\ndef labelizeTweets(tweets, label_type):\n    labelized = []\n    for i,v in tqdm(enumerate(tweets)):\n        label = '%s_%s'%(label_type,i)\n        labelized.append(LabeledSentence(v, [label]))\n    return labelized\n\n#split labelized tweets by train\/test\/split data\nX_train = labelizeTweets(X_train, 'TRAIN')\nX_val = labelizeTweets(X_val, 'VALIDATE')\nX_test = labelizeTweets(X_test, 'TEST')","cdb0b41f":"#Check the first element of the labelized trained data to make sure it worked\nX_train[0]","7e9766c1":"#Build the Word2Vec Model\ntweet_w2v = Word2Vec(size=200, window = 5, min_count=10, workers=4) #initialize model\ntweet_w2v.build_vocab([x.words for x in tqdm(X_train)]) #create vocabulary\ntweet_w2v.train([x.words for x in tqdm(X_train)], total_examples=tweet_w2v.corpus_count, epochs=2) #train model","c27b7749":"#Check that the Word2Vec code worked correctly \ntweet_w2v['happy']","42ac6693":"#example producing the most similar words\ntweet_w2v.most_similar('happy')","48d8ecc0":"# defining the chart\noutput_notebook()\nplot_tfidf = bp.figure(plot_width=700, plot_height=600, title=\"A map of 10000 word vectors\",\n    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n    x_axis_type=None, y_axis_type=None, min_border=1)\n\n# getting a list of word vectors. limit to 10,000. each is of 200 dimensions\nword_vectors = [tweet_w2v[w] for w in list(tweet_w2v.wv.vocab.keys())[:5000]]\n\n# dimensionality reduction. converting the vectors to 2d vectors\ntsne_model = TSNE(n_components=2, verbose=1, random_state=0)\ntsne_w2v = tsne_model.fit_transform(word_vectors)\n\n# putting everything in a dataframe\ntsne_df = pd.DataFrame(tsne_w2v, columns=['x', 'y'])\ntsne_df['words'] = list(tweet_w2v.wv.vocab.keys())[:5000]\n\n# plotting the corresponding word appears when you hover on the data point.\nplot_tfidf.scatter(x='x', y='y', source=tsne_df)\nhover = plot_tfidf.select(dict(type=HoverTool))\nhover.tooltips={\"word\": \"@words\"}\nshow(plot_tfidf)\n","6d717615":"# Create tf-idf matrix\nvectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\nmatrix = vectorizer.fit_transform([x.words for x in X_train])\ntfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\nprint('vocab size :', len(tfidf))","ad94d027":"#Build the vector producing averaged tweets\ndef buildWordVector(tokens, size):\n    vec = np.zeros(size).reshape((1, size))\n    count = 0.\n    for word in tokens:\n        try:\n            vec += tweet_w2v[word].reshape((1, size)) * tfidf[word]\n            count += 1.\n        except KeyError: # handling the case where the token is not\n                         # in the corpus. useful for testing.\n            continue\n    if count != 0:\n        vec \/= count\n    return vec","300edfb5":"#Convert into vector and scale\ntrain_vecs_w2v = np.concatenate([buildWordVector(z, 200) for z in tqdm(map(lambda x: x.words, X_train))])\ntrain_vecs_w2v = scale(train_vecs_w2v)\n\nval_vecs_w2v = np.concatenate([buildWordVector(z, 200) for z in tqdm(map(lambda x: x.words, X_val))])\nval_vecs_w2v = scale(val_vecs_w2v)\n\ntest_vecs_w2v = np.concatenate([buildWordVector(z, 200) for z in tqdm(map(lambda x: x.words, X_test))])\ntest_vecs_w2v = scale(test_vecs_w2v)","6b13a0ee":"#Using a Random Forest Classifier model\nrfc =  RandomForestClassifier(n_estimators=100, verbose=True)","dc838907":"#Fitting a Random Forest Classifier\nrfc.fit(train_vecs_w2v, y_train)\ny_pred_rf = rfc.predict(val_vecs_w2v)","d38db16c":"#Define a function to get metrics\ndef get_metrics(y_val, y_pred):  \n    # true positives \/ (true positives+false positives)\n    precision = precision_score(y_val, y_pred, pos_label=None, average='weighted')             \n    # true positives \/ (true positives + false negatives)\n    recall = recall_score(y_val, y_pred, pos_label=None,average='weighted')\n    # harmonic mean of precision and recall\n    f1 = f1_score(y_val, y_pred, pos_label=None, average='weighted')\n    # true positives + true negatives\/ total\n    accuracy = accuracy_score(y_val, y_pred)\n    return accuracy, precision, recall, f1","5c10aae3":"#View metrics for random forest classifier\naccuracy, precision, recall, f1 = get_metrics(y_val, y_pred_rf)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, \n                                                                       recall, f1))","6b8fde05":"#Plot Confusion Matrix\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.winter):\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=30)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, fontsize=20)\n    plt.yticks(tick_marks, classes, fontsize=20)\n    \n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", \n                 color=\"white\" if cm[i, j] < thresh else \"black\", fontsize=40)\n    \n    plt.tight_layout()\n    plt.ylabel('True label', fontsize=30)\n    plt.xlabel('Predicted label', fontsize=30)\n\n    return plt","4a3a220d":"#Plot confusion matrix for random forest model\nclass_names = list(set(y))\ncm = confusion_matrix(y_val, y_pred_rf)\nfig = plt.figure(figsize=(15, 15))\nplot = plot_confusion_matrix(cm, classes=class_names, normalize=True, title='Confusion matrix')\nplt.show()\nprint(cm)","c543942f":"# Convert labels to categorical one-hot encoding\nohe_y_train = pd.get_dummies(y_train)\nohe_y_val = pd.get_dummies(y_val)\nohe_y_test = pd.get_dummies(y_test)","a0bff0e4":"#For a single-input model using RMSprop optimizer\nrandom.seed(123)\nmodel = Sequential()\nmodel.add(Dense(32, activation='relu', input_shape=(200,)))\nmodel.add(Dense(4, activation='softmax'))\nmodel.compile(optimizer=optimizers.RMSprop(learning_rate=0.001, rho=0.9),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model, iterating on the data in batches of 32 samples\nhistory = model.fit(train_vecs_w2v, ohe_y_train, epochs=2, batch_size=32,\n                    validation_data=(val_vecs_w2v, ohe_y_val))","261c9b26":"#view loss and accuracy dictionary\nhistory.history","898587c1":"#LSTM\n#model = Sequential()\n#model.add(Embedding(len(tfidf), 200))\n#model.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2))\n#model.add(Dense(4, activation='softmax'))\n#model.compile(optimizer='rmsprop',\n#              loss='binary_crossentropy',\n#              metrics=['accuracy'])\n\n# Train the model, iterating on the data in batches of 32 samples\n#history = model.fit(train_vecs_w2v, ohe_y_train, epochs=2, \n#                    batch_size=32,validation_data=(val_vecs_w2v, ohe_y_val))","b0ddab1b":"#using adam optimization\nrandom.seed(123)\nmodel = Sequential()\nmodel.add(Dense(32, activation='relu', input_shape=(200,)))\nmodel.add(Dense(4, activation='softmax'))\nmodel.compile(optimizer=optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model, iterating on the data in batches of 32 samples\nhistory = model.fit(train_vecs_w2v, ohe_y_train, epochs=2, batch_size=32, \n                   validation_data=(val_vecs_w2v, ohe_y_val))","71fd5851":"#Increase layers in this model using RMSprop optimizer and an increased batch size\nrandom.seed(123)\nmodel = Sequential()\nmodel.add(Dense(50, activation='relu', input_shape=(200,)))\nmodel.add(Dense(25, activation='relu'))\nmodel.add(Dense(4, activation='softmax'))\nmodel.compile(optimizer=optimizers.RMSprop(learning_rate=0.001, rho=0.9),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model, iterating on the data in batches of 256 samples\nhistory = model.fit(train_vecs_w2v, ohe_y_train, epochs=2, batch_size=256,\n                   validation_data=(val_vecs_w2v, ohe_y_val))","998990bf":"#L2 Regularizer\nrandom.seed(123)\nmodel = Sequential()\nmodel.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.005), input_shape=(200,)))\nmodel.add(Dense(25, activation='relu', kernel_regularizer=regularizers.l2(0.005)))\nmodel.add(Dense(4, activation='softmax'))\nmodel.compile(optimizer=optimizers.RMSprop(learning_rate=0.001, rho=0.9),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model, iterating on the data in batches of 256 samples\nhistory = model.fit(train_vecs_w2v, ohe_y_train, epochs=2, batch_size=256, \n         validation_data=(val_vecs_w2v, ohe_y_val))","ccf72ac5":"#L1 Regularizer\nrandom.seed(123)\nmodel = Sequential()\nmodel.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l1(0.005), input_shape=(200,)))\nmodel.add(Dense(25, activation='relu', kernel_regularizer=regularizers.l1(0.005)))\nmodel.add(Dense(4, activation='softmax'))\nmodel.compile(optimizer=optimizers.RMSprop(learning_rate=0.001, rho=0.9),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model, iterating on the data in batches of 256 samples\nhistory = model.fit(train_vecs_w2v, ohe_y_train, epochs=2, batch_size=256,\n                   validation_data=(val_vecs_w2v, ohe_y_val))","ee3eeedc":"#Dropout Regularizer \nrandom.seed(123)\nmodel = Sequential()\nmodel.add(Dropout(0.3, input_shape=(200,)))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(layers.Dropout(0.3))\nmodel.add(Dense(25, activation='relu'))\nmodel.add(layers.Dropout(0.3))\nmodel.add(Dense(4, activation='softmax'))\nmodel.compile(optimizer=optimizers.RMSprop(learning_rate=0.001, rho=0.9),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model, iterating on the data in batches of 256 samples\nhistory = model.fit(train_vecs_w2v, ohe_y_train, epochs=2, batch_size=256,\n                   validation_data=(val_vecs_w2v, ohe_y_val))","bb86679d":"#Increase epochs \nrandom.seed(123)\nmodel = Sequential()\nmodel.add(Dense(50, activation='relu', input_shape=(200,)))\nmodel.add(Dense(25, activation='relu'))\nmodel.add(Dense(4, activation='softmax'))\nmodel.compile(optimizer=optimizers.RMSprop(learning_rate=0.001, rho=0.9),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model, iterating on the data in batches of 256 samples with 100 epochs\nhistory = model.fit(train_vecs_w2v, ohe_y_train, epochs=400, batch_size=256,\n                   validation_data=(val_vecs_w2v, ohe_y_val))","fb697976":"#plot to see how many epochs makes sense\nhistory_dict = history.history\ntrain_loss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\n\nepochs = range(1, len(train_loss_values) + 1)\nplt.plot(epochs, train_loss_values, 'g', label='Training loss')\nplt.plot(epochs, val_loss_values, 'r', label='Validation loss')\n\nplt.title('Training loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","c69f350f":"#determine predictions\ny_pred_nn = model.predict_classes(val_vecs_w2v)\ny_pred_nn","8f30fcac":"#reformat y_val for confusion matrix and metrics\nohe_y_val_arr = np.array(ohe_y_val)\ny_val_nn = np.argmax(ohe_y_val_arr, axis=1)\ny_val_nn","65fd9dd9":"#View metrics for neural network\naccuracy, precision, recall, f1 = get_metrics(y_val_nn, y_pred_nn)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, \n                                                                       recall, f1))","859edbbc":"#Plot confusion matrix for neural network\nclass_names =list(set(y))\ncm = confusion_matrix(y_val_nn, y_pred_nn)\nfig = plt.figure(figsize=(15, 15))\nplot = plot_confusion_matrix(cm, classes=class_names, normalize=True, title='Confusion matrix')\nplt.show()\nprint(cm)","35e1f2e8":"#View metrics for random forest classifier on test data\ny_pred_test = rfc.predict(test_vecs_w2v)\naccuracy, precision, recall, f1 = get_metrics(y_test, y_pred_test)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, \n                                                                       recall, f1))","801108d4":"#Plot confusion matrix for test data using random forest model\nclass_names = list(set(y))\ncm = confusion_matrix(y_test, y_pred_test)\nfig = plt.figure(figsize=(15, 15))\nplot = plot_confusion_matrix(cm, classes=class_names, normalize=True, title='Confusion matrix')\nplt.show()\nprint(cm)","e3ed90e0":"I'll now look at region in more detail (there are 1,145 missing values) that are all English speaking","d5d2cde3":"Now that I have my train, validate and test sets properly vectorized, I can look at models to classify my data","6fa12054":"It appears that the nans are not retweets (quote_tweets are a type of retweet where you can comment on it).  Therefore, since this isn't actually missing data I'll replace the NaN values under post_type with NOT_RETWEET","dfd26edd":"Since most of the regions are United States at 87%, still followed by Unknown at 12% I'll actually only look at United States Region for analysis purposed and remove the other regions.","8b12377e":"# Business Understanding <a id=\"1\"><\/a>","02968c8b":"There are almost 3M records and 15 fields to the combined dataframe.  I will now look at how many authors, or twitter handles, are involved in the data set.","1113a73a":"Revisit missing data","22b57b4a":"View original data here to determine the tweet counts by date.  \n- Code reference for charts from: https:\/\/www.kaggle.com\/chadalee\/text-analytics-on-russian-troll-tweets-part-1","b175d92c":"There are 2,848 authors or twitter handles in total in the dataset.","2a7dfd14":"- It looks like 20% of the tweets are duplicates since there are only 2,365,942 unique tweets.  However, since these can be retweets of the same information and could then validate the extra weight it would give to them, I'll keep them all in for modeling purposes later\n- It also looks like there are some NaN values for \"external_author_id\", which is the author account ID from Twitter.  Since I have the Twitter Handle as the \"author\" for each tweet I won't be using the \"external_author_id\" in my analysis and will remove this field from the dataset\n- I'll also be removing the \"harvested_date\" since I don't need it for my analysis","75af5e45":"# Russian Troll Tweets","9d04a580":"Since it took a while to run the Random Forest model using simple default parameters, I will not over complicate the model due to the cost of additional time it may take to run.  Additionally, the model is producing around 80% accuracy, which is pretty strong!  ","af974fc5":"Lastly will look at post_type in more detail, the last item with missing values","d27b29b5":"There was a huge spike in tweets on October 6, 2016, they day before October 7, 2016.  On October 7, 2016, 30 minutes after the Access Hollywood tape was first published where Trump spoke crudely about groping women, WikiLeaks began publishing thousands of emails from Podesta's Gmail account.  The most popular tweets from trolls on this day used the words below to target users:\n- \"Trump\", \"Hillary\", \"Black\", \"New\", \"People\" and \"n't\"\n- There are  additional tweets on Hurricane Matthew as well","7f06f6cb":"## Word2Vec <a id=\"9\"><\/a>","b14565e1":"Comparing both the original data with our resampled data I have similar distributions across the features.  Now I can comfortably explore the smaller resampled data if needed.","9ebcec00":"I will use Word2Vec as my vectorization strategy here because of the large size of my data.  Word2vec is a model that was pre-trained on a very large corpus, and provides embeddings that map words that are similar or close to each other.","047de4b3":"## Publish Date Exploration <a id=\"6\"><\/a>","3b020788":"## Cleaning and Tokenizing Tweets + Additional Exploration <a id=\"7\"><\/a>","5a78b5ca":"# Data Cleaning <a id=\"3\"><\/a>","76b470d7":"- The Right Trolls have many more followers and following more accounts as well.  \n- The Left Trolls are pretty equal in the number of followers compared to those they are following\n- NewsFeed does have some more followers then those they are following \n- Non English account categories have more followers then those they are following","2d27e8be":"## Following\/Follower Exploration <a id=\"5\"><\/a>","df967533":"I see a slight decrease in loss with a larger batch size and more layers.  I'll look at regularizers now to see if they help out.","816f8122":"# Conclusion <a id=\"14\"><\/a>","4b7a436d":"It looks like once I hit 100 epochs things start to flatten out for a while (I'm not yet overfitting though).  I'll look at the results of my model.","60d9b188":"In the data cleaning process I will look at the data types of each field.  I will determine if any data is missing, and handle it appropriately.","6110247c":"I'll now feed my word2vec vectors into a neural network.  I'll start with a very simple 2 layer architecture and run only 2 epochs to keep it easy and quick at first.","13f8d411":"It doesn't look as though adding in regularizers improves anything so I'll go back to the model before they were added and increase the epochs to see if I get a lower loss associated with my model.","77bc7c2b":"Right Trolls mostly used the following words to target users:\n- \u201cTrump\u201d, \u201cHillary\u201d, \u201cObama\u201d, \u201cNew\u201d, \"People\" and \u201cn\u2019t\u201d\n\nIt appears that right trolls focused on Obama and Hillary in their tweets more than Left Trolls did (see below)","7d784fcb":"Success! Data is finalized for the exploratory phase!","1762c434":"It looks like the Random Forest Model is a better model for classifying my tweets after plotting the confusion matrix and looking at metrics","26b92630":"Looking at all the data the russian trolls mostly used the following words to target users:\n- \u201cTrump\u201d,\u201cNew\u201d, and \u201cn\u2019t\u201d\n\nI will look at this split by account categories and publish date in more detail below","92714068":"Review dataset post initial cleaning","8c3b3ca5":"## Table of Contents\n* [Business Understanding](#1)\n* [Data Import](#2)\n* [Data Cleaning](#3)\n* [Data Exploration](#4)\n    * [Following\/Follower Exploration](#5)\n    * [Publish Date Exploration](#6)\n    * [Cleaning & Tokenizing Tweets + Additional Exploration](#7)\n* [Feature Engineering & Modeling](#8)\n    * [Word2Vec](#9)\n    * [Train\/Validate\/Split Data for Modeling](#10)    \n    * [Random Forest Classifier](#11)      \n    * [Neural Network Classifier](#12)  \n    * [Final Model](#13)   \n* [Conclusion](#14)","22e23d3f":"The code should have produced a vector that is 200 in size (typical size that you want is 100-300 so this is nicely in the middle), which looks to be true above!  Now I'll look to see what the most similar words are to the word 'happy' as a test below:","a1f1951f":"Awesome! My very simple random forest classifier predicted the four Account Categories with almost 80% accuracy.  I'll look closer at the results below with a confusion matrix:","3e1228a1":"This initial model provided a final loss associated with the model of 0.31. I'll continue tuning my neural network.","a3634cb6":"The first model I'll try before diving into Neural Networks is to see how a simple random forest classifier will work on my data","a303105e":"Convert X_train, X_val and X_test into list of vectors.  Scale each column to have zero mean and unit standard deviation.","3b6358c5":"Compare retweet column to post_type","73b0f4ac":"\nReview that missing values no longer exist","6d49c10a":"Convert publish_date to a datetime format for analysis purposes","800c32bf":"It looks like the top left category had a bit of trouble, but the others were predicted pretty strongly with the random forest classifier!  I'll now see how a simple Neural Network Classifier will perform on my data better.","d8f4f4e2":"View the number of followers and those following for, account category, account type and post type","8ca2cf4c":"Running the code above using LSTM was very time costly and did not perform as well as my initial model.  I stopped it partway in fact.  My loss was worse and I was getting an accuracy of around 0.81 after the first epoch.  Therefore, I'll go back to my initial model and tweak it to see if I can do better.","c2a07149":"News Feed mostly used the following words to target users:\n- \u201cSport\u201d, \u201cPolitic\u201d, \u201cPolice\u201d, \u201cNew\u201d, \"World\", and \u201cMan\u201d\n\nThese are common words we hear on the news, which makes sense the News Feed category uses them to target audiences","0cbde67e":"# Data Exploration <a id=\"4\"><\/a>","2db4247d":"Next I'll look at the language of tweets - are there a lot of tweets that are non-english?","69812fe1":"Code reference from: https:\/\/www.ahmedbesbes.com\/blog\/sentiment-analysis-with-keras-and-word-2-vec","511bdca1":"After modeling the dataset I\u2019ve uncovered the following findings and recommendation for determining future Russian Trolls:\n- 72% of all tweets were in English followed by 21% in Russian.  It appears that most handlers use the English language when they tweet.  \n- 87% of all tweets were classified as being from the region of the United States.  They could have been using a VPN or tweeting within the United States.  \n- Most tweets are split between Retweets and Non-Retweets and from the following 3 account categories: \n    - News Feed\n    - Right Troll\n    - Left Troll\n- Right Trolls have the largest number of followers and have more followers than those they are following compared to Left Trolls and News Feed account categories.  There is a small subsection of Left Trolls and News Feed account categories that do have more followers than those they are following; however, for the most part they are balanced. \n    - The most tweeted words used by Right Trolls are \u201cTrump\u201d, \u201cHillary\u201d, \u201cObama\u201d, \u201cNew\u201d, \"People\" and \u201cn\u2019t\u201d to target audiences.  \n    - The most tweeted words used by Left Trolls are \u201cTrump\u201d, \u201cBlack\u201d, \u201cWhite\u201d, \u201cNew\u201d, \"People\" and \u201cn\u2019t\u201d to target audiences.  \n    - The differences between Left Trolls and Right Trolls are the focus on Democrats for Right Trolls, with the words \u201cHillary\u201d and \u201cObama\u201d used often, over race, with Left Trolls using the words \u201cBlack\u201d and \u201cWhite\u201d more.\n    - The most tweeted words used by News Feeds are \u201cSport\u201d, \u201cPolitic\u201d, \u201cPolice\u201d, \u201cNew\u201d, \"World\" and \u201cMan\u201d, which makes sense as these are common words used by the news\n- There was a huge spike in tweets on 10\/6\/2016 with 17,984 tweets.  This is followed by the second most tweeted day on 10\/7\/2016 with 9,620 tweets.  Most of these tweets came from Left Trolls.  \n    - When looking at the language used on 10\/6\/2016 the words \"Trump\", \"Hillary\", \"Black\", \"New\", \"People\" and \"n't\" were the most tweeted words to target audiences.\n    - 10\/6\/2016, the day before 10\/7\/2016, when 30 minutes after the Access Hollywood tape was first published where Trump spoke crudely about groping women, WikiLeaks began publishing thousands of emails from John Podesta\u2019s Gmail account.  Hurricane Matthew was also taking place in early\/mid October.  These events probably caused the huge rush of tweets on that day.\n\nAfter looking at a Random Forest Classifier and various Neural Network Classifiers to predict account category based on tweet, a Random Forest Classifier with the simple parameters above is the best model I found, with 80% accuracy!\n\n\nFuture work could include performing a grid search to see if there could be any farther improvement on the Random Forest model.","1f2e0622":"The first step is to import all the libraries I'll need for my analysis.","707c035b":"Produce averaged tweet vector","5285762b":"It looks like on the 2 most tweeted days of 10\/6\/2016 and 10\/7\/2016, most tweets came from Left Trolls with Right Trolls and News Feed trailing.","bf8a0da0":"Left Trolls mostly used the following words to target users:\n- \u201cTrump\u201d, \u201cBlack\u201d, \u201cWhite\u201d, \u201cNew\u201d, \"People\" and \u201cn\u2019t\u201d\n\nIt appears that race played a large role for them with the hashtage #blacklivesmatter also largely used","4f1f67b7":"There's not much difference using the Adam optimizer over RMSprop.  I'll go back to RMSprop since it's more simple.  I'll increase the layers and batch size though to see if there's improvement.","089bf8f7":"Here I'll explore the data in more detail.  To begin, I'll resample the dataset so it's easier to create some of the charts and graphs for the exploration phase.","cf78066b":"In order to classify the tweets, I'll need to turn them into vectors.  I know the vector representation of each word within a tweet so I'll have to combine the vectors to get a new vector that represents the whole tweet.  I've looked at different methods and it appears the best solution is to compute a weighted average, the weight being the tf-idf score.  The weight will provide the importance of the word with respect to the entire corpus. ","2eb8cca9":"Need to clean up account_type a bit.  Will update:\n- 'right' to \"Right'\n- 'left' to 'Left'\n- 'news' to 'News'\n- 'ZAPOROSHIA' to 'Zaporoshia'\n- 'local' to 'Local'\n- '?' to 'Unknown' ","d03ecdc7":"The next step is to import the data from Kaggle, and combine the 7 separate csv files the data is stored in.","1da7d705":"# Feature Engineering & Modeling <a id=\"8\"><\/a>","6b062365":"- The account type lines up with the account category when split by followers and following, as expected.  \n- It looks like the non english speaking account category is mostly russian speaking account types, also as expected.","a384c25a":"This chart doesn't show too much differentiation between post type and the number of followers and those following ","c9d172ee":"72% of all tweets are in english.  For ease of research I'll only keep the tweets that are english language.","36fce1ab":"## Random Forest Classifier <a id=\"11\"><\/a>","b7120d06":"The points above are words similar\/close in context.  The chart shows that the words are pretty spread out actually with a few closer clusters","e593cee5":"Look at dataset in more detail","98b75e45":"Revisit missing data items","59edb0b8":"## Neural Network Classifier <a id=\"12\"><\/a>","e6edf8ec":"Look at account_type in more detail","bea5de0b":"## Train\/Validate\/Split Data for Modeling <a id=\"10\"><\/a>","4119dddc":"Most tweets are around 10 words in length","b8f68d02":"## Final Model - Random Forest","d0c8e442":"How cool! For the word 'happy' the model is producing similar words within the same context.  These words have a probability to be closer to the word 'happy' in most tweet. Now let's visualize results using the bokeh library for interactive visualization and tsne for converting the vectors to 2D vectors","7c18c564":"87% of region values are \"United States\" and 12% of region values are \"Unknown\".  Since all the missing values here do speak English, I'll replace the Nan with Unknown","f59bbf65":"The purpose of this project is to determine if the account categories, as coded by Linvill and Warren, in the 3 Million Russian Troll Tweets shared by FiveThirtyEight can be classified by the tweets themselves.  The Twitter handles within the dataset are connected to the Internet Research Agency, which is a Russian \u201ctroll factory\u201d and a defendant in an indictment as part of Robert Mueller\u2019s Russia investigation.  The tweets in the dataset are between February 2012 and May 2018.  \n\n### Methodology:\n- Initially the data is contained in 7 separate csv files.  These data files will be combined to analyze all 3 Million tweets.  \n- The data files will be cleaned to remove, keep or replace any missing or limited values for analysis purposes.  \n- After cleaning begins the exploratory phase.  This will include:\n    - Producing follower\/following graphs based on account category;\n    - Producing tweet frequency by publish date; and\n    - Producing wordclouds split by popular account categories to view language patterns.\n- Once the data has been explored feature engineering and classification modeling of the tweets is next:\n    - A combination of Word2Vec and TF-IDF Vectorization will be used for modeling\n    - Random Forest Classifiers and Neural Network Classifiers will be examined to determine the best fit model.\n\nThis analysis is to help determine how Russian Trolls use language in their Tweets to target audiences in future endeavors, such as influencing future elections. \n\n### The initial data files have the following columns: \n\nHeader | Definition\n---|---------\n`external_author_id` | An author account ID from Twitter \n`author` | The handle sending the tweet\n`content` | The text of the tweet\n`region` | A region classification, as [determined by Social Studio](https:\/\/help.salesforce.com\/articleView?   id=000199367&type=1)\n`language` | The language of the tweet\n`publish_date` | The date and time the tweet was sent\n`harvested_date` | The date and time the tweet was collected by Social Studio\n`following` | The number of accounts the handle was following at the time of the tweet\n`followers` | The number of followers the handle had at the time of the tweet\n`updates` | The number of \u201cupdate actions\u201d on the account that authored the tweet, including tweets, retweets and likes\n`post_type` | Indicates if the tweet was a retweet or a quote-tweet\n`account_type` | Specific account theme, as coded by Linvill and Warren\n`retweet` | A binary indicator of whether or not the tweet is a retweet\n`account_category` | General account theme, as coded by Linvill and Warren\n`new_june_2018` | A binary indicator of whether the handle was newly listed in June 2018\n\n  ","dfe7300a":"When modeling the data I've decided to combine HashtagGamer, Commercial, Fearmonger, Unknown and NonEnglish into 'Other' since they haven't been prominant in the exploratory phase when looking at the number of followers\/following or the largest tweet dates (publish_dates).  ","fd12d6df":"Before feeding my lists of tokens into the word2vec model, I need to  turn them into LabeledSentence objects below","b95eb95e":"First I'll develop my train\/validate\/test data sets for modeling","c3bd2e0a":"# Data Import <a id=\"2\"><\/a>","773235c4":"There are a lot of regions included. I'll look at the percentage of each region to determine if I should keep them all for analysis purposes.","c086d581":"Now that I have the final data for analysis, on to feature engineering and modeling!","859780b5":"Now each element is an object with two attributes: a list of tokens and a label.  The next step is to build and train my Word2Vec model","da251961":"Since there is only one missing 'content' value, and 'content' will be our feature during modeling, I'll remove this one row"}}