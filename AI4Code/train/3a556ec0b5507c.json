{"cell_type":{"900cefb4":"code","5f135357":"code","346211cd":"code","53e7a6f2":"code","8f13ebc6":"code","7c963ec2":"code","14cc1426":"code","f3cac968":"code","0618d053":"code","391477d1":"code","7c10fb81":"code","68b2d655":"code","89b6b50d":"code","238472a7":"code","3a609e5d":"code","fafa7a74":"code","f797183e":"code","e92f592a":"code","869518cd":"code","e17f6874":"code","7e3c578f":"code","7d5fd77f":"code","bfe360ea":"code","a6d07b95":"code","b39cf04b":"code","7c184244":"code","a3d6ebb7":"code","5edd84ca":"code","d623627b":"code","3e2ed31a":"code","230c3cca":"code","1ee21b53":"code","4a239d16":"code","0a09b4dc":"code","61ac298c":"code","4d3beb36":"code","ecdcca31":"code","b543099c":"code","77c78009":"code","0cac3766":"code","b57608c8":"code","7cfd75e4":"code","59987fcd":"code","ce097f76":"code","6255820c":"code","e8578f10":"code","49219136":"code","431e07a0":"code","01749682":"code","fdc43b46":"code","80798463":"code","63210d07":"code","ed46abd0":"code","9b421b7d":"code","d4ede211":"code","573a065b":"code","03e801bc":"code","c6306c70":"code","4c3df96b":"code","6945ced8":"code","bd609054":"code","14923477":"code","9b2fc1c3":"code","9ddb09a7":"code","8d6e1f29":"code","ad3e126f":"code","56eab36e":"code","c1cd0aa5":"code","10acfdd3":"code","1d8f380e":"code","9c99ecdf":"code","7062d757":"code","ae00c54a":"code","fa3033da":"code","ebade26a":"code","853fc20d":"code","1fca3a91":"code","d7100973":"code","9423dd19":"code","5906f11f":"code","abff88aa":"code","68e634c7":"code","100e8ea1":"code","838477f5":"code","5ac6f08c":"code","cd49e102":"code","bb7dcb32":"code","e78f4de7":"code","7ef7e533":"code","93d39814":"code","560bfeaa":"code","975eb3c4":"code","ac0ca02a":"code","36aff84e":"code","7f13ccd6":"markdown","4012b907":"markdown","82e1c4dd":"markdown","14f9a46e":"markdown","b5f55174":"markdown","8603ebba":"markdown","5f543dcf":"markdown","335c9e7d":"markdown","1cda8378":"markdown","e4847e40":"markdown","9b91f5f2":"markdown","f6859e06":"markdown","6aeeef8f":"markdown","a25a3896":"markdown","6fed12a6":"markdown","3d814e83":"markdown","1d28c891":"markdown","d73949e2":"markdown","a27b31c9":"markdown"},"source":{"900cefb4":"import copy\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport matplotlib.style as style\nfrom mpl_toolkits import mplot3d\nfrom scipy import stats\nimport seaborn as sns\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode,iplot\n%matplotlib inline\n\n# preprocessing\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\nimport pandas_profiling as pp\nfrom sklearn.linear_model import LinearRegression,LogisticRegression, SGDRegressor, RidgeCV\n\n# models\nfrom sklearn.linear_model import LinearRegression, SGDRegressor, RidgeCV\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor \nfrom sklearn.ensemble import BaggingRegressor, AdaBoostRegressor, VotingRegressor \nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nimport sklearn.model_selection\nfrom sklearn.model_selection import cross_val_predict as cvp\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn import preprocessing\nimport xgboost as xgb\nimport lightgbm as lgb\n\n# model tuning\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe, space_eval\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5f135357":"valid_part = 0.3\npd.set_option('max_columns',100)","346211cd":"train0 = pd.read_csv('\/kaggle\/input\/craigslist-carstrucks-data\/craigslistVehicles.csv')\ntrain0.head(5)","53e7a6f2":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\ndrop_columns = ['url', 'city', 'city_url', 'make', 'title_status', 'VIN', 'size', 'image_url', 'desc', 'lat','long']\ntrain0 = train0.drop(columns = drop_columns)","8f13ebc6":"train0.info()","7c963ec2":"train0 = train0.dropna()\ntrain0.head(5)","14cc1426":"# Clone data for FE \n_train = copy.deepcopy(train0)","f3cac968":"train0 = train0.loc[train0['fuel'] == 'diesel']\ntrain0.head(5)","0618d053":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/automatic-selection-from-20-classifier-models\n# Determination categorical features\nnumerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_columns = []\nfeatures = train0.columns.values.tolist()\nfor col in features:\n    if train0[col].dtype in numerics: continue\n    categorical_columns.append(col)\n# Encoding categorical features\nfor col in categorical_columns:\n    if col in train0.columns:\n        le = LabelEncoder()\n        le.fit(list(train0[col].astype(str).values))\n        train0[col] = le.transform(list(train0[col].astype(str).values))","391477d1":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\ntrain0['year'] = (train0['year']-1900).astype(int)\ntrain0['odometer'] = train0['odometer'].astype(int)","7c10fb81":"train0.head(10)","68b2d655":"train0.info()","89b6b50d":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d-abnormals-filter\n#Thanks to https:\/\/www.kaggle.com\/masumrumi\/a-detailed-regression-guide-with-house-pricing\ndef plotting_3_chart(df, feature):\n    ## Importing seaborn, matplotlab and scipy modules. \n    style.use('fivethirtyeight')\n\n    ## Creating a customized chart. and giving in figsize and everything. \n    fig = plt.figure(constrained_layout=True, figsize=(15,10))\n    ## creating a grid of 3 cols and 3 rows. \n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n    \n    ## Customizing the histogram grid. \n    ax1 = fig.add_subplot(grid[0, :2])\n    ## Set the title. \n    ax1.set_title('Histogram')\n    ## plot the histogram. \n    sns.distplot(df.loc[:,feature], norm_hist=True, ax = ax1)\n\n    # customizing the QQ_plot. \n    ax2 = fig.add_subplot(grid[1, :2])\n    ## Set the title. \n    ax2.set_title('QQ_plot')\n    ## Plotting the QQ_Plot. \n    stats.probplot(df.loc[:,feature], plot = ax2)\n\n    ## Customizing the Box Plot. \n    ax3 = fig.add_subplot(grid[:, 2])\n    ## Set title. \n    ax3.set_title('Box Plot')\n    ## Plotting the box plot. \n    sns.boxplot(df.loc[:,feature], orient='v', ax = ax3 );","238472a7":"train0['price'].value_counts()","3a609e5d":"train0.describe(percentiles=[.01, .05, .1, .5, .9, .92, .93, .94, .96, .97, .99])","fafa7a74":"train0 = train0[train0['price'] > 1000]\ntrain0 = train0[train0['price'] < 58566]\n\ntrain0['odometer'] = train0['odometer'] \/\/ 5000\ntrain0 = train0[train0['year'] > 100]","f797183e":"train0.info()","e92f592a":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d-abnormals-filter\nplotting_3_chart(train0, 'price')","869518cd":"# Thanks to: https:\/\/www.kaggle.com\/dzvlfi\/craiglist-eda-dzulfiqar-ridha\n#create correlation with heatmap\ncorr = train0.corr(method = 'pearson')\n\n#convert correlation to numpy array\nmask = np.array(corr)\n\n#to mask the repetitive value for each pair\nmask[np.tril_indices_from(mask)] = False\nfig, ax = plt.subplots(figsize = (15,12))\nfig.set_size_inches(20,20)\nsns.heatmap(corr, mask = mask, vmax = 0.9, square = True, annot = True)","e17f6874":"#Thanks to https:\/\/towardsdatascience.com\/an-easy-introduction-to-3d-plotting-with-matplotlib-801561999725\n\nfig = plt.figure(figsize=(10,10))\nax = plt.axes(projection=\"3d\")\n\nz_points = train0['price']\nx_points = train0['odometer']\ny_points = train0['year']\nax.scatter3D(x_points, y_points, z_points, c=z_points, cmap='hsv');\n\nax.set_xlabel('odometer')\nax.set_ylabel('year')\nax.set_zlabel('price')\n\nplt.show()","7e3c578f":"# Thanks to: https:\/\/www.kaggle.com\/darkcore\/house-sales-visualization\n#Create Cylinders Frame\ncylindersframe = pd.DataFrame({\"Cylinders\":_train.cylinders.value_counts().index,\"Car_cylinders\":_train.cylinders.value_counts().values})\ncylindersframe[\"Cylinders\"] = cylindersframe[\"Cylinders\"].apply(lambda x : \"\" + str(x))\ncylindersframe.set_index(\"Cylinders\",inplace=True)","7d5fd77f":"# Thanks to: https:\/\/www.kaggle.com\/darkcore\/house-sales-visualization\np1 = [go.Pie(labels = cylindersframe.index,values = cylindersframe.Car_cylinders,hoverinfo=\"percent+label+value\",hole=0.1,marker=dict(line=dict(color=\"#000000\",width=2)))]\nlayout4 = go.Layout(title=\"Cylinders Pie Chart\")\nfig4 = go.Figure(data=p1,layout=layout4)\niplot(fig4)","bfe360ea":"# Thanks to: https:\/\/www.kaggle.com\/darkcore\/house-sales-visualization\n#Create Condition Frame\nconditionframe = pd.DataFrame({\"Condition\":_train.condition.value_counts().index,\"Car_conditions\":_train.condition.value_counts().values})\nconditionframe[\"Condition\"] = conditionframe[\"Condition\"].apply(lambda x : \"\" + str(x))\nconditionframe.set_index(\"Condition\",inplace=True)","a6d07b95":"# Thanks to: https:\/\/www.kaggle.com\/darkcore\/house-sales-visualization\np1 = [go.Pie(labels = conditionframe.index,values = conditionframe.Car_conditions,hoverinfo=\"percent+label+value\",hole=0.1,marker=dict(line=dict(color=\"#000000\",width=2)))]\nlayout4 = go.Layout(title=\"Condition Pie Chart\")\nfig4 = go.Figure(data=p1,layout=layout4)\niplot(fig4)","b39cf04b":"# Thanks to: https:\/\/www.kaggle.com\/darkcore\/house-sales-visualization\n#Create Manufacturer Frame\nmanufacturerframe = pd.DataFrame({\"Manufacturer\":_train.manufacturer.value_counts().index,\"Car_manufacturer\":_train.manufacturer.value_counts().values})\nmanufacturerframe[\"Manufacturer\"] = manufacturerframe[\"Manufacturer\"].apply(lambda x : \"\" + str(x))\nmanufacturerframe.set_index(\"Manufacturer\",inplace=True)","7c184244":"# Thanks to: https:\/\/www.kaggle.com\/darkcore\/house-sales-visualization\np1 = [go.Pie(labels = manufacturerframe.index,values = manufacturerframe.Car_manufacturer,hoverinfo=\"percent+label+value\",hole=0.1,marker=dict(line=dict(color=\"#000000\",width=2)))]\nlayout4 = go.Layout(title=\"Manufacturer Pie Chart\")\nfig4 = go.Figure(data=p1,layout=layout4)\niplot(fig4)","a3d6ebb7":"# Thanks to: https:\/\/www.kaggle.com\/darkcore\/house-sales-visualization\n#Create Fuel Frame\nfuelframe = pd.DataFrame({\"Fuel\":_train.fuel.value_counts().index,\"Car_fuel\":_train.fuel.value_counts().values})\nfuelframe[\"Fuel\"] = fuelframe[\"Fuel\"].apply(lambda x : \"\" + str(x))\nfuelframe.set_index(\"Fuel\",inplace=True)","5edd84ca":"# Thanks to: https:\/\/www.kaggle.com\/darkcore\/house-sales-visualization\np1 = [go.Pie(labels = fuelframe.index,values = fuelframe.Car_fuel,hoverinfo=\"percent+label+value\",hole=0.1,marker=dict(line=dict(color=\"#000000\",width=2)))]\nlayout4 = go.Layout(title=\"Fuel Pie Chart\")\nfig4 = go.Figure(data=p1,layout=layout4)\niplot(fig4)","d623627b":"# Thanks to: https:\/\/www.kaggle.com\/darkcore\/house-sales-visualization\n#Create transmission Frame\ntransmissionframe = pd.DataFrame({\"Transmission\":_train.transmission.value_counts().index,\"Car_transmission\":_train.transmission.value_counts().values})\ntransmissionframe[\"Transmission\"] = transmissionframe[\"Transmission\"].apply(lambda x : \"\" + str(x))\ntransmissionframe.set_index(\"Transmission\",inplace=True)","3e2ed31a":"# Thanks to: https:\/\/www.kaggle.com\/darkcore\/house-sales-visualization\np1 = [go.Pie(labels = transmissionframe.index,values = transmissionframe.Car_transmission,hoverinfo=\"percent+label+value\",hole=0.1,marker=dict(line=dict(color=\"#000000\",width=2)))]\nlayout4 = go.Layout(title=\"Transmission Pie Chart\")\nfig4 = go.Figure(data=p1,layout=layout4)\niplot(fig4)","230c3cca":"# Thanks to: https:\/\/www.kaggle.com\/darkcore\/house-sales-visualization\n#Create type Frame\ntypeframe = pd.DataFrame({\"Type\":_train.type.value_counts().index,\"Car_type\":_train.type.value_counts().values})\ntypeframe[\"Type\"] = typeframe[\"Type\"].apply(lambda x : \"\" + str(x))\ntypeframe.set_index(\"Type\",inplace=True)","1ee21b53":"# Thanks to: https:\/\/www.kaggle.com\/darkcore\/house-sales-visualization\np1 = [go.Pie(labels = typeframe.index,values = typeframe.Car_type,hoverinfo=\"percent+label+value\",hole=0.1,marker=dict(line=dict(color=\"#000000\",width=2)))]\nlayout4 = go.Layout(title=\"Type Pie Chart\")\nfig4 = go.Figure(data=p1,layout=layout4)\niplot(fig4)","4a239d16":"# Thanks to: https:\/\/www.kaggle.com\/darkcore\/house-sales-visualization\n#Create paint_color Frame\npaint_colorframe = pd.DataFrame({\"Paint_color\":_train.paint_color.value_counts().index,\"Car_paint_color\":_train.paint_color.value_counts().values})\npaint_colorframe[\"Paint_color\"] = paint_colorframe[\"Paint_color\"].apply(lambda x : \"\" + str(x))\npaint_colorframe.set_index(\"Paint_color\",inplace=True)","0a09b4dc":"# Thanks to: https:\/\/www.kaggle.com\/darkcore\/house-sales-visualization\np1 = [go.Pie(labels = paint_colorframe.index,values = paint_colorframe.Car_paint_color,hoverinfo=\"percent+label+value\",hole=0.1,marker=dict(line=dict(color=\"#000000\",width=2)))]\nlayout4 = go.Layout(title=\"Paint color Pie Chart\")\nfig4 = go.Figure(data=p1,layout=layout4)\niplot(fig4)","61ac298c":"# Thanks to: https:\/\/www.kaggle.com\/darkcore\/house-sales-visualization\n#Create drive Frame\ndriveframe = pd.DataFrame({\"Drive\":_train.drive.value_counts().index,\"Car_drive\":_train.drive.value_counts().values})\ndriveframe[\"Drive\"] = driveframe[\"Drive\"].apply(lambda x : \"\" + str(x))\ndriveframe.set_index(\"Drive\",inplace=True)","4d3beb36":"# Thanks to: https:\/\/www.kaggle.com\/darkcore\/house-sales-visualization\np1 = [go.Pie(labels = driveframe.index,values = driveframe.Car_drive,hoverinfo=\"percent+label+value\",hole=0.1,marker=dict(line=dict(color=\"#000000\",width=2)))]\nlayout4 = go.Layout(title=\"Drive Pie Chart\")\nfig4 = go.Figure(data=p1,layout=layout4)\niplot(fig4)","ecdcca31":"train0.info()","b543099c":"train0.corr()","77c78009":"train0.describe()","0cac3766":"pp.ProfileReport(train0)","b57608c8":"# Clone data for FE \ntrain_fe = copy.deepcopy(train0)\ntarget_fe = train_fe['price']\ndel train_fe['price']","7cfd75e4":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\nX = train_fe\nz = target_fe","59987fcd":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\n#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(X, z, test_size=0.2, random_state=0)\ntrain_set = lgb.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgb.Dataset(Xval, Zval, silent=False)","ce097f76":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\nparams = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 5000 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'zero_as_missing': True,\n        'seed':0,        \n    }\n\nmodelL = lgb.train(params, train_set = train_set, num_boost_round=1000,\n                   early_stopping_rounds=50,verbose_eval=10, valid_sets=valid_set)","6255820c":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nlgb.plot_importance(modelL,ax = axes,height = 0.5)\nplt.show();plt.close()","e8578f10":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\nfeature_score = pd.DataFrame(train_fe.columns, columns = ['feature']) \nfeature_score['score_lgb'] = modelL.feature_importance()","49219136":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\n#%% split training set to validation set \ndata_tr  = xgb.DMatrix(Xtrain, label=Ztrain)\ndata_cv  = xgb.DMatrix(Xval   , label=Zval)\nevallist = [(data_tr, 'train'), (data_cv, 'valid')]","431e07a0":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\nparms = {'max_depth':8, #maximum depth of a tree\n         'objective':'reg:squarederror',\n         'eta'      :0.3,\n         'subsample':0.8,#SGD will use this percentage of data\n         'lambda '  :4, #L2 regularization term,>1 more conservative \n         'colsample_bytree ':0.9,\n         'colsample_bylevel':1,\n         'min_child_weight': 10}\nmodelx = xgb.train(parms, data_tr, num_boost_round=200, evals = evallist,\n                  early_stopping_rounds=30, maximize=False, \n                  verbose_eval=10)\n\nprint('score = %1.5f, n_boost_round =%d.'%(modelx.best_score,modelx.best_iteration))","01749682":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nxgb.plot_importance(modelx,ax = axes,height = 0.5)\nplt.show();plt.close()","fdc43b46":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\nfeature_score['score_xgb'] = feature_score['feature'].map(modelx.get_score(importance_type='weight'))\nfeature_score","80798463":"# Standardization for regression model\ntrain_fe = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(train_fe),\n    columns=train_fe.columns,\n    index=train_fe.index\n)","63210d07":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\n# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(train_fe, target_fe)\ncoeff_logreg = pd.DataFrame(train_fe.columns.delete(0))\ncoeff_logreg.columns = ['feature']\ncoeff_logreg[\"score_logreg\"] = pd.Series(logreg.coef_[0])\ncoeff_logreg.sort_values(by='score_logreg', ascending=False)","ed46abd0":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\n# the level of importance of features is not associated with the sign\ncoeff_logreg[\"score_logreg\"] = coeff_logreg[\"score_logreg\"].abs()\nfeature_score = pd.merge(feature_score, coeff_logreg, on='feature')","9b421b7d":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\n# Linear Regression\n\nlinreg = LinearRegression()\nlinreg.fit(train_fe, target_fe)\ncoeff_linreg = pd.DataFrame(train_fe.columns.delete(0))\ncoeff_linreg.columns = ['feature']\ncoeff_linreg[\"score_linreg\"] = pd.Series(linreg.coef_)\ncoeff_linreg.sort_values(by='score_linreg', ascending=False)","d4ede211":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\ncoeff_linreg[\"score_linreg\"] = coeff_linreg[\"score_linreg\"].abs()\nfeature_score = pd.merge(feature_score, coeff_linreg, on='feature')\nfeature_score = feature_score.fillna(0)\nfeature_score = feature_score.set_index('feature')\nfeature_score","573a065b":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# Thanks to: https:\/\/www.kaggle.com\/nanomathias\/feature-engineering-importance-testing\n# MinMax scale all importances\nfeature_score = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(feature_score),\n    columns=feature_score.columns,\n    index=feature_score.index\n)\n\n# Create mean column\nfeature_score['mean'] = feature_score.mean(axis=1)\n\n# Plot the feature importances\nfeature_score.sort_values('mean', ascending=False).plot(kind='bar', figsize=(20, 10))","03e801bc":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nfeature_score.sort_values('mean', ascending=False)","c6306c70":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# Create total column with different weights\nfeature_score['total'] = 0.5*feature_score['score_lgb'] + 0.3*feature_score['score_xgb'] \\\n                       + 0.1*feature_score['score_logreg'] + 0.1*feature_score['score_linreg']\n\n# Plot the feature importances\nfeature_score.sort_values('total', ascending=False).plot(kind='bar', figsize=(20, 10))","4c3df96b":"feature_score.sort_values('total', ascending=False)","6945ced8":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\ntarget_name = 'price'\ntrain_target0 = train0[target_name]\ntrain0 = train0.drop([target_name], axis=1)","bd609054":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n# Synthesis test0 from train0\ntrain0, test0, train_target0, test_target0 = train_test_split(train0, train_target0, test_size=0.2, random_state=0)","14923477":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n# For boosting model\ntrain0b = train0\ntrain_target0b = train_target0\n# Synthesis valid as test for selection models\ntrainb, testb, targetb, target_testb = train_test_split(train0b, train_target0b, test_size=valid_part, random_state=0)","9b2fc1c3":"#For models from Sklearn\nscaler = StandardScaler()\ntrain0 = pd.DataFrame(scaler.fit_transform(train0), columns = train0.columns)","9ddb09a7":"train0.head(3)","8d6e1f29":"len(train0)","ad3e126f":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n# Synthesis valid as test for selection models\ntrain, test, target, target_test = train_test_split(train0, train_target0, test_size=valid_part, random_state=0)","56eab36e":"train.head(3)","c1cd0aa5":"test.head(3)","10acfdd3":"train.info()","1d8f380e":"test.info()","9c99ecdf":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\nacc_train_r2 = []\nacc_test_r2 = []\nacc_train_d = []\nacc_test_d = []\nacc_train_rmse = []\nacc_test_rmse = []","7062d757":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\ndef acc_d(y_meas, y_pred):\n    # Relative error between predicted y_pred and measured y_meas values\n    return mean_absolute_error(y_meas, y_pred)*len(y_meas)\/sum(abs(y_meas))\n\ndef acc_rmse(y_meas, y_pred):\n    # RMSE between predicted y_pred and measured y_meas values\n    return (mean_squared_error(y_meas, y_pred))**0.5","ae00c54a":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\ndef acc_boosting_model(num,model,train,test,num_iteration=0):\n    # Calculation of accuracy of boosting model by different metrics\n    \n    global acc_train_r2, acc_test_r2, acc_train_d, acc_test_d, acc_train_rmse, acc_test_rmse\n    \n    if num_iteration > 0:\n        ytrain = model.predict(train, num_iteration = num_iteration)  \n        ytest = model.predict(test, num_iteration = num_iteration)\n    else:\n        ytrain = model.predict(train)  \n        ytest = model.predict(test)\n\n    print('target = ', targetb[:5].values)\n    print('ytrain = ', ytrain[:5])\n\n    acc_train_r2_num = round(r2_score(targetb, ytrain) * 100, 2)\n    print('acc(r2_score) for train =', acc_train_r2_num)   \n    acc_train_r2.insert(num, acc_train_r2_num)\n\n    acc_train_d_num = round(acc_d(targetb, ytrain) * 100, 2)\n    print('acc(relative error) for train =', acc_train_d_num)   \n    acc_train_d.insert(num, acc_train_d_num)\n\n    acc_train_rmse_num = round(acc_rmse(targetb, ytrain) * 100, 2)\n    print('acc(rmse) for train =', acc_train_rmse_num)   \n    acc_train_rmse.insert(num, acc_train_rmse_num)\n\n    print('target_test =', target_testb[:5].values)\n    print('ytest =', ytest[:5])\n    \n    acc_test_r2_num = round(r2_score(target_testb, ytest) * 100, 2)\n    print('acc(r2_score) for test =', acc_test_r2_num)\n    acc_test_r2.insert(num, acc_test_r2_num)\n    \n    acc_test_d_num = round(acc_d(target_testb, ytest) * 100, 2)\n    print('acc(relative error) for test =', acc_test_d_num)\n    acc_test_d.insert(num, acc_test_d_num)\n    \n    acc_test_rmse_num = round(acc_rmse(target_testb, ytest) * 100, 2)\n    print('acc(rmse) for test =', acc_test_rmse_num)\n    acc_test_rmse.insert(num, acc_test_rmse_num)","fa3033da":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\ndef acc_model(num,model,train,test):\n    # Calculation of accuracy of model \u0430\u043a\u0449\u044c Sklearn by different metrics   \n  \n    global acc_train_r2, acc_test_r2, acc_train_d, acc_test_d, acc_train_rmse, acc_test_rmse\n    \n    ytrain = model.predict(train)  \n    ytest = model.predict(test)\n\n    print('target = ', target[:5].values)\n    print('ytrain = ', ytrain[:5])\n\n    acc_train_r2_num = round(r2_score(target, ytrain) * 100, 2)\n    print('acc(r2_score) for train =', acc_train_r2_num)   \n    acc_train_r2.insert(num, acc_train_r2_num)\n\n    acc_train_d_num = round(acc_d(target, ytrain) * 100, 2)\n    print('acc(relative error) for train =', acc_train_d_num)   \n    acc_train_d.insert(num, acc_train_d_num)\n\n    acc_train_rmse_num = round(acc_rmse(target, ytrain) * 100, 2)\n    print('acc(rmse) for train =', acc_train_rmse_num)   \n    acc_train_rmse.insert(num, acc_train_rmse_num)\n\n    print('target_test =', target_test[:5].values)\n    print('ytest =', ytest[:5])\n    \n    acc_test_r2_num = round(r2_score(target_test, ytest) * 100, 2)\n    print('acc(r2_score) for test =', acc_test_r2_num)\n    acc_test_r2.insert(num, acc_test_r2_num)\n    \n    acc_test_d_num = round(acc_d(target_test, ytest) * 100, 2)\n    print('acc(relative error) for test =', acc_test_d_num)\n    acc_test_d.insert(num, acc_test_d_num)\n    \n    acc_test_rmse_num = round(acc_rmse(target_test, ytest) * 100, 2)\n    print('acc(rmse) for test =', acc_test_rmse_num)\n    acc_test_rmse.insert(num, acc_test_rmse_num)","ebade26a":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(trainb, targetb, test_size=0.2, random_state=0)\ntrain_set = lgb.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgb.Dataset(Xval, Zval, silent=False)","853fc20d":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\nparams = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 31,\n        'learning_rate': 0.01,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 5000 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'zero_as_missing': False,\n        'seed':0,        \n    }\nmodelL = lgb.train(params, train_set = train_set, num_boost_round=10000,\n                   early_stopping_rounds=8000,verbose_eval=500, valid_sets=valid_set)","1fca3a91":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\nacc_boosting_model(1,modelL,trainb,testb,modelL.best_iteration)","d7100973":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\nfig =  plt.figure(figsize = (5,5))\naxes = fig.add_subplot(111)\nlgb.plot_importance(modelL,ax = axes,height = 0.5)\nplt.show();\nplt.close()","9423dd19":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\nxgb_clf = xgb.XGBRegressor({'objective': 'reg:squarederror'}) \nparameters = {'n_estimators': [60, 100, 120, 140], \n              'learning_rate': [0.01, 0.1],\n              'max_depth': [5, 7],\n              'reg_lambda': [0.5]}\nxgb_reg = GridSearchCV(estimator=xgb_clf, param_grid=parameters, cv=5, n_jobs=-1).fit(trainb, targetb)\nprint(\"Best score: %0.3f\" % xgb_reg.best_score_)\nprint(\"Best parameters set:\", xgb_reg.best_params_)\nacc_boosting_model(2,xgb_reg,trainb,testb)","5906f11f":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n# Decision Tree Regression\n\ndecision_tree = DecisionTreeRegressor()\ndecision_tree.fit(train, target)\nacc_model(3,decision_tree,train,test)","abff88aa":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\nmodels = pd.DataFrame({\n    'Model': ['LGBM', 'XGB', 'Decision Tree Regressor'],\n    \n    'r2_train': acc_train_r2,\n    'r2_test': acc_test_r2,\n    'd_train': acc_train_d,\n    'd_test': acc_test_d,\n    'rmse_train': acc_train_rmse,\n    'rmse_test': acc_test_rmse\n                     })","68e634c7":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\npd.options.display.float_format = '{:,.2f}'.format","100e8ea1":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\nprint('Prediction accuracy for models by R2 criterion - r2_test')\nmodels.sort_values(by=['r2_test', 'r2_train'], ascending=False)","838477f5":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\nprint('Prediction accuracy for models by relative error - d_test')\nmodels.sort_values(by=['d_test', 'd_train'], ascending=True)","5ac6f08c":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\nprint('Prediction accuracy for models by RMSE - rmse_test')\nmodels.sort_values(by=['rmse_test', 'rmse_train'], ascending=True)","cd49e102":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n# Plot\nplt.figure(figsize=[25,6])\nxx = models['Model']\nplt.tick_params(labelsize=14)\nplt.plot(xx, models['r2_train'], label = 'r2_train')\nplt.plot(xx, models['r2_test'], label = 'r2_test')\nplt.legend()\nplt.title('R2-criterion for 15 popular models for train and test datasets')\nplt.xlabel('Models')\nplt.ylabel('R2-criterion, %')\nplt.xticks(xx, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()","bb7dcb32":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n# Plot\nplt.figure(figsize=[25,6])\nxx = models['Model']\nplt.tick_params(labelsize=14)\nplt.plot(xx, models['d_train'], label = 'd_train')\nplt.plot(xx, models['d_test'], label = 'd_test')\nplt.legend()\nplt.title('Relative errors for 15 popular models for train and test datasets')\nplt.xlabel('Models')\nplt.ylabel('Relative error, %')\nplt.xticks(xx, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()","e78f4de7":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n# Plot\nplt.figure(figsize=[25,6])\nxx = models['Model']\nplt.tick_params(labelsize=14)\nplt.plot(xx, models['rmse_train'], label = 'rmse_train')\nplt.plot(xx, models['rmse_test'], label = 'rmse_test')\nplt.legend()\nplt.title('RMSE for 15 popular models for train and test datasets')\nplt.xlabel('Models')\nplt.ylabel('RMSE, %')\nplt.xticks(xx, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()","7ef7e533":"test0.info()","93d39814":"test0.head(3)","560bfeaa":"# LGB Regression model for basic train\nlgb_predict = modelL.predict(test0)\nlgb_predict[:3]","975eb3c4":"# XGB Regression model for basic train\nxgb_reg.fit(train0, train_target0)\nxgb_predict = xgb_reg.predict(test0)\nxgb_predict[:3]","ac0ca02a":"# Decision Tree Regression for basic train\ndecision_tree.fit(train0, train_target0)\ndecision_trees_predict = decision_tree.predict(test0)\ndecision_trees_predict[:3]","36aff84e":"# Thanks to: https:\/\/www.kaggle.com\/dnzcihan\/house-sales-prediction-and-eda\nfinal_df = test_target0.values\nfinal_df = pd.DataFrame(final_df,columns=['Real_price'])\nfinal_df['predicted_prices'] = lgb_predict.astype(int)\nfinal_df['difference'] = abs(final_df['Real_price'] - final_df['predicted_prices']).astype(int)\nfinal_df.head(20)","7f13ccd6":"### 7.3 Decision Tree Regressor <a class=\"anchor\" id=\"7.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","4012b907":"<a class=\"anchor\" id=\"4.4\"><\/a>\n### 4.1 Linear Regression\n##### [Back to Table of Contents](#0.1)","82e1c4dd":"## 6. Preparing to modeling <a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","14f9a46e":"## 8. Models comparison <a class=\"anchor\" id=\"8\"><\/a>\n\n[Back to Table of Contents](#0.1)","b5f55174":"<a class=\"anchor\" id=\"4.2\"><\/a>\n### 4.3 XGB\n##### [Back to Table of Contents](#0.1)","8603ebba":"<a class=\"anchor\" id=\"4.3\"><\/a>\n### 4.3 Logistic Regression\n##### [Back to Table of Contents](#0.1)","5f543dcf":"<a class=\"anchor\" id=\"4.1\"><\/a>\n### 4.3 LGBM\n##### [Back to Table of Contents](#0.1)","335c9e7d":"### 7.1 LGBM <a class=\"anchor\" id=\"7.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","1cda8378":"## 9. Prediction <a class=\"anchor\" id=\"9\"><\/a>\n\n[Back to Table of Contents](#0.1)","e4847e40":"## 3. EDA & Visualization <a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","9b91f5f2":"<a class=\"anchor\" id=\"0\"><\/a>\n\n# Estimation of the price of selling diesel cars with 3 models + EDA","f6859e06":"### 7.2 XGB<a class=\"anchor\" id=\"7.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","6aeeef8f":"## **Acknowledgements**\n#### This kernel uses such good kernels:\n   - https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n   - https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d-abnormals-filter\n   - https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n   - https:\/\/www.kaggle.com\/darkcore\/house-sales-visualization","a25a3896":"## 1. Import libraries <a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","6fed12a6":"## 5. Comparison of the all feature importance diagrams <a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","3d814e83":"## 2. Download datasets <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","1d28c891":"## 7. Tuning models and test for all features <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)","d73949e2":"<a class=\"anchor\" id=\"4\"><\/a>\n## 4. FE: building the feature importance diagrams\n##### [Back to Table of Contents](#0.1)","a27b31c9":"<a class=\"anchor\" id=\"0.1\"><\/a>\n\n## Table of Contents\n\n1. [Import libraries](#1)\n2. [Download datasets](#2)\n3. [EDA & Visualization](#3)\n4. [FE: building the feature importance diagrams](#4)\n  - [LGBM](4.1)\n  - [XGB](4.2)\n  - [Logistic Regression](4.3)\n  - [Linear Regression](4.3)\n5. [Comparison of the all feature importance diagrams](#5)\n6. [Preparing to modeling](#6)\n7. [Tuning models and test for all features](#7)\n  - [LGBM](7.1)\n  - [XGB](7.2)\n  - [Decision Tree Regressor](7.3)\n8. [Models comparison](#8)\n9. [Prediction](#9)\n\n    \n"}}