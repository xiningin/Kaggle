{"cell_type":{"b91725f5":"code","6d242b6a":"code","0a14f90c":"code","a924094d":"code","757c260e":"code","e12c8d76":"code","36afb0c3":"code","a2d4d45b":"code","391bdba1":"code","bead2e1d":"code","920166c1":"code","c3d78f57":"code","cef98531":"code","7cb27c87":"code","e6764e32":"code","9ccbf17c":"code","954dd969":"code","0af614e1":"code","d250bd03":"code","f227733c":"code","c2682ee6":"code","97a33bd8":"code","361ea146":"code","1b60be47":"code","f5c09f12":"code","8902252e":"code","17a70095":"code","c1d0142f":"code","591cd46c":"code","e4ed4484":"code","0ce5fda5":"code","0f13a054":"code","25de7b15":"markdown","3dee6f2d":"markdown","5ea64012":"markdown","e1bb4592":"markdown","8d912af1":"markdown","b30dd605":"markdown","163b3daa":"markdown","952b0ed7":"markdown","6e3c4007":"markdown","2d4ad5cf":"markdown","9ffc9bbf":"markdown","6a9d3be2":"markdown","2550d803":"markdown","5edbf042":"markdown","6d4c80b2":"markdown","97ea3fce":"markdown","b58d520b":"markdown","cf0b3409":"markdown","cca805b5":"markdown","0e4a5228":"markdown","ac1fe665":"markdown","b8cce641":"markdown","cee31a1b":"markdown","bdf43e0c":"markdown","26a7f2e3":"markdown","36e977bf":"markdown","df6da7dc":"markdown","5cfbbd9e":"markdown","587357cd":"markdown","9eaa3234":"markdown"},"source":{"b91725f5":"import numpy as np\nimport itertools\nimport os\nimport pandas as pd\nimport seaborn as sea\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.feature_selection import RFE\n\nimport xgboost as xgb\nfrom xgboost import plot_importance\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec","6d242b6a":"sea.set_style(\"darkgrid\")","0a14f90c":"data = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/\"\n                   \"diabetes.csv\")\n\ndata.head(10).style.set_precision(2). \\\n                    set_properties(**{\"min-width\": \"60px\"}). \\\n                    set_properties(**{\"color\": \"#111111\"}). \\\n                    set_properties(**{\"text-align\": \"center\"}). \\\n                    set_table_styles([\n                          {\"selector\": \"th\",\n                           \"props\": [(\"font-weight\", \"bold\"),\n                                     (\"font-size\", \"12px\"),\n                                     (\"text-align\", \"center\")]},\n                          {\"selector\": \"tr:nth-child(even)\",\n                           \"props\": [(\"background-color\", \"#f2f2f2\")]},\n                          {\"selector\": \"tr:nth-child(odd)\",\n                           \"props\": [(\"background-color\", \"#fdfdfd\")]},\n                          {\"selector\": \"tr:hover\",\n                           \"props\": [(\"background-color\", \"#bcbcbc\")]}])","a924094d":"# disable SettingWithCopyWarning messages\npd.options.mode.chained_assignment = None\n\ndata_X = data.loc[:, data.columns != \"Outcome\"]\ndata_Y = data[[\"Outcome\"]]\n\nprint(\"data_X info:\\n\")\ndata_X.info()\nprint(\"\\ndata_Y info:\\n\")\ndata_Y.info()","757c260e":"data_Y[\"Outcome\"].value_counts()","e12c8d76":"train_X, test_X, train_Y, test_Y = train_test_split(data_X, data_Y,\n                                                    test_size=0.2,\n                                                    stratify=data_Y,\n                                                    random_state=0)\n\ntrain_X.reset_index(drop=True, inplace=True);\ntest_X.reset_index(drop=True, inplace=True);\ntrain_Y.reset_index(drop=True, inplace=True);\ntest_Y.reset_index(drop=True, inplace=True);","36afb0c3":"def plots(feature):\n    fig = plt.figure(constrained_layout = True, figsize=(10,3))\n    gs = gridspec.GridSpec(nrows=1, ncols=4, figure=fig)\n\n    ax1 = fig.add_subplot(gs[0,:3])    \n    sea.distplot(train_X.loc[train_Y[\"Outcome\"]==0,feature],\n                 kde = False, color = \"#004a4d\", norm_hist=False,\n                 hist_kws = dict(alpha=0.8), bins=40,\n                 label=\"Not Diabetes\", ax=ax1);\n    sea.distplot(train_X.loc[train_Y[\"Outcome\"]==1,feature],\n                 kde = False, color = \"#7d0101\", norm_hist=False,\n                 hist_kws = dict(alpha=0.6), bins=40,\n                 label=\"Diabetes\", ax=ax1);\n    ax2 = fig.add_subplot(gs[0,3])    \n    sea.boxplot(train_X[feature], orient=\"v\", color = \"#989100\",\n                width = 0.2, ax=ax2);\n    \n    ax1.legend(loc=\"upper right\");","a2d4d45b":"plots(\"Pregnancies\")","391bdba1":"Q1 = train_X[\"Pregnancies\"].quantile(0.25)\nQ3 = train_X[\"Pregnancies\"].quantile(0.75)\nq95th = train_X[\"Pregnancies\"].quantile(0.95)\nIQR = Q3 - Q1\nUW = Q3 + 1.5*IQR\n\ntrain_X[\"Pregnancies\"] = np.where(train_X[\"Pregnancies\"] > UW,\n                                  q95th, train_X[\"Pregnancies\"])","bead2e1d":"plots(\"Glucose\")","920166c1":"med = train_X[\"Glucose\"].median()\ntrain_X[\"Glucose\"] = np.where(train_X[\"Glucose\"] == 0, med, train_X[\"Glucose\"])","c3d78f57":"plots(\"BloodPressure\")","cef98531":"med = train_X[\"BloodPressure\"].median()\nq5th = train_X[\"BloodPressure\"].quantile(0.05)\nq95th = train_X[\"BloodPressure\"].quantile(0.95)\nQ1 = train_X[\"BloodPressure\"].quantile(0.25)\nQ3 = train_X[\"BloodPressure\"].quantile(0.75)\nIQR = Q3 - Q1\nLW = Q1 - 1.5*IQR\nUW = Q3 + 1.5*IQR\n\ntrain_X[\"BloodPressure\"] = np.where(train_X[\"BloodPressure\"] == 0,\n                                    med, train_X[\"BloodPressure\"])\ntrain_X[\"BloodPressure\"] = np.where(train_X[\"BloodPressure\"] < LW,\n                                    q5th, train_X[\"BloodPressure\"])\ntrain_X[\"BloodPressure\"] = np.where(train_X[\"BloodPressure\"] > UW,\n                                    q95th, train_X[\"BloodPressure\"])","7cb27c87":"plots(\"SkinThickness\")","e6764e32":"med = train_X[\"SkinThickness\"].median()\nq95th = train_X[\"SkinThickness\"].quantile(0.95)\nQ1 = train_X[\"SkinThickness\"].quantile(0.25)\nQ3 = train_X[\"SkinThickness\"].quantile(0.75)\nIQR = Q3 - Q1\nUW = Q3 + 1.5*IQR\n\ntrain_X[\"SkinThickness\"] = np.where(train_X[\"SkinThickness\"] == 0,\n                                    med, train_X[\"SkinThickness\"])\ntrain_X[\"SkinThickness\"] = np.where(train_X[\"SkinThickness\"] > UW,\n                                    q95th, train_X[\"SkinThickness\"])","9ccbf17c":"plots(\"Insulin\")","954dd969":"q60th = train_X[\"Insulin\"].quantile(0.60)\nq95th = train_X[\"Insulin\"].quantile(0.95)\nQ1 = train_X[\"Insulin\"].quantile(0.25)\nQ3 = train_X[\"Insulin\"].quantile(0.75)\nIQR = Q3 - Q1\nUW = Q3 + 1.5*IQR\n\ntrain_X[\"Insulin\"] = np.where(train_X[\"Insulin\"] == 0,\n                              q60th, train_X[\"Insulin\"])\ntrain_X[\"Insulin\"] = np.where(train_X[\"Insulin\"] > UW,\n                              q95th, train_X[\"Insulin\"])","0af614e1":"plots(\"BMI\")","d250bd03":"med = train_X[\"BMI\"].median()\nq95th = train_X[\"BMI\"].quantile(0.95)\nQ1 = train_X[\"BMI\"].quantile(0.25)\nQ3 = train_X[\"BMI\"].quantile(0.75)\nIQR = Q3 - Q1\nUW = Q3 + 1.5*IQR\n\ntrain_X[\"BMI\"] = np.where(train_X[\"BMI\"] == 0,\n                          med, train_X[\"BMI\"])\ntrain_X[\"BMI\"] = np.where(train_X[\"BMI\"] > UW,\n                          q95th, train_X[\"BMI\"])","f227733c":"plots(\"DiabetesPedigreeFunction\")","c2682ee6":"q95th = train_X[\"DiabetesPedigreeFunction\"].quantile(0.95)\nQ1 = train_X[\"DiabetesPedigreeFunction\"].quantile(0.25)\nQ3 = train_X[\"DiabetesPedigreeFunction\"].quantile(0.75)\nIQR = Q3 - Q1\nUW = Q3 + 1.5*IQR\n\ntrain_X[\"DiabetesPedigreeFunction\"] = np.where(\n                        train_X[\"DiabetesPedigreeFunction\"] > UW,\n                        q95th, train_X[\"DiabetesPedigreeFunction\"])","97a33bd8":"plots(\"Age\")","361ea146":"q95th = train_X[\"Age\"].quantile(0.95)\nQ1 = train_X[\"Age\"].quantile(0.25)\nQ3 = train_X[\"Age\"].quantile(0.75)\nIQR = Q3 - Q1\nUW = Q3 + 1.5*IQR\n\ntrain_X[\"Age\"] = np.where(train_X[\"Age\"] > UW,\n                          q95th, train_X[\"Age\"])","1b60be47":"feature_names = train_X.columns\n\nscaler = StandardScaler()\n\n# fit to train_X\nscaler.fit(train_X)\n\n# transform train_X\ntrain_X = scaler.transform(train_X)\ntrain_X = pd.DataFrame(train_X, columns = feature_names)\n\n# transform test_X\ntest_X = scaler.transform(test_X)\ntest_X = pd.DataFrame(test_X, columns = feature_names)","f5c09f12":"corr_matrix = pd.concat([train_X, train_Y], axis=1).corr()\nmask = np.triu(np.ones_like(corr_matrix, dtype=np.bool))\n\nplt.figure(figsize=(10,8))\nsea.heatmap(corr_matrix,annot=True, fmt=\".4f\",\n            vmin=-1, vmax=1, linewidth = 1,\n            center=0, mask=mask,cmap=\"RdBu_r\");","8902252e":"# logistic regression with l1 penalty - embedded method\nlr1 = LogisticRegression(random_state=0, penalty=\"l1\",\n            class_weight=\"balanced\", solver=\"saga\", max_iter=5000)\nlr1.fit(train_X, train_Y.values.ravel())\n\nlr1.coef_","17a70095":"# logistic regression with l2 penalty - embedded method\nlr2 = LogisticRegression(random_state=0, penalty=\"l2\",\n            class_weight=\"balanced\", solver=\"saga\", max_iter=5000)\nlr2.fit(train_X, train_Y.values.ravel())\n\nlr2.coef_","c1d0142f":"# xgboost - embedded method\ngb = xgb.XGBClassifier(booster=\"gbtree\",\n                      learning_rate=0.15,\n                      max_depth=4,\n                      n_estimators=20,\n                      random_state=0)\ngb.fit(train_X, train_Y.values.ravel())\n\nplot_importance(gb);","591cd46c":"# recursive feature elimination with logistic regression - wrapper method\nlr3 = LogisticRegression(random_state=0, penalty=\"l2\",\n            class_weight=\"balanced\", solver=\"saga\", max_iter=5000)\nrfe_lr3 = RFE(lr3, n_features_to_select=7)\nrfe_lr3.fit(train_X, train_Y.values.ravel())\n\nprint(rfe_lr3.support_)","e4ed4484":"train_X.drop(\"SkinThickness\", axis=1, inplace=True)\ntest_X.drop(\"SkinThickness\", axis=1, inplace=True)","0ce5fda5":"clf = MLPClassifier(solver=\"adam\", max_iter=5000, activation = \"relu\",\n                    hidden_layer_sizes = (12),                      \n                    alpha = 0.01,\n                    batch_size = 64,\n                    learning_rate_init = 0.001,\n                    random_state=2)\n\nclf.fit(train_X, train_Y.values.ravel());","0f13a054":"print(classification_report(test_Y, clf.predict(test_X),\n                            digits = 4,\n                            target_names=[\"Not Diabetes\",\n                                          \"Diabetes\"]))","25de7b15":"First three analysis mark BloodPressure, SkinThickness and Insulin as the least important features. Note that these features have very low correlation with Outcome. RFE marks SkinThickness as the least important.","3dee6f2d":"### Feature 4 - Insulin","5ea64012":"There are 8 features and Outcome is the target variable.\n\n* Pregnancies\n* Glucose\n* BloodPressure\n* SkinThickness\n* Insulin\n* BMI (Body mass index)\n* DiabetesPedigreeFunction\n* Age\n\nFeatures are assigned to data_X and corresponding labels to data_Y. Pandas info shows column (feature) data types and number of non-null values.","e1bb4592":"There are some 0 values for **Glucose**. We can deem 0 values as placeholder for missing data. So we replace them with median.","8d912af1":"## Split Data\n\nDataset is divided into train and test sets. We use stratify parameter of train_test_split function to get the same class distribution across train and test sets.","b30dd605":"### Feature 2 - BloodPressure","163b3daa":"There are some 0 values for **SkinThickness** which is unlikely. So we replace them with median. Also, we replace values greater than UW with 95th quantile.","952b0ed7":"Dataset has 768 rows (training samples). All features are numeric. And it seems there aren't any missing values. Here, one point requires extra attention. In your dataset, missing values may be encoded other than **NaN** or **NA**. For example, if you go and change some cells in dataset to **Na** or **NAN** and load it again and check info, **Pandas** won't recognize them as null values. So, **Pandas** may not detect these cells as missing. In our case, dataset is small and a short visual inspection reveals no missing values. But there is another point. Some cells may be encoded with an unlikely input. For example, 0 may indicate a missing value for a numeric feature. We will return this issue later.\n\nNumber of classes and their distribution is inspected in below cell.","6e3c4007":"MLP (Multi Layer Perceptron) Classifier is the feedforward artificial neural network implementation of Scikit-Learn library.\n\nMLPClassifier is used to predict whether a patient has diabetes based on a set of diagnostics.\n\nOutline of the work is as follows:\n\n* Load Data\n* Split Data\n* Visualization and Outlier Check\n* Standardization\n* Correlation Analysis\n* Feature Importance\n* Train MLP\n* Test MLP","2d4ad5cf":"## Feature Importance\n\nFeature importance analysis methods can be divided into 3 broad categories:\n\n* Filter method: Calculating a metric like correlation coefficient between each feature and output separately as we did above. In this method all features are evaluated independently.\n\n* Embedded methods: Methods like logistic regression or linear regression learn the coefficients that multiply each feature. The magnitude of the coefficients are associated with the importance of the features. Also, tree based methods like random forests or gradient tree boosting learn feature importances during training process. In embedded methods, all features are evaluated jointly.\n\n* Wrapper methods: Basically you have an estimator and you train this estimator with the subsets of features. The subset giving the best score is selected and other features are eliminated.","9ffc9bbf":"## Visualization and Outlier Check\n\nOutliers degrade the learning performance. Outlier analysis is performed for each feature one-by-one. We use quartile analysis for outlier detection. For each feature, there are two plots below. Distribution of feature is on the left. Box plot of the same feature is on the right. Both of them are analyzed together to get an idea about the outliers. From this point on, lower whisker of the boxplot is denoted as LW and upper whisker is denoted as UW.\n\nIn order to draw a boxplot, feature data is divided into four. Three cut points are needed. These points are lower quartile (or first quartile), median (or second quartile) and upper quartile (or third quartile). First quartile is the median of the data lower than second quartile. Third quartile is the median of the data greater than the second quartile. Interquartile range (IQR) is found subtracting lower quartile from upper quartile. Outliers are determined using lower, upper quartiles and IQR.","6a9d3be2":"Correlation matrix shows that there are mild correlations between **SkinThickness-BMI** and **Age-Pregnancies**. **Outcome** has the highest linear correlation with **Glucose**.","2550d803":"### Feature 0 - Pregnancies","5edbf042":"## Train MLP\n\nMLPClassifier with single hidden layer is used for diabetes prediction.","6d4c80b2":"We replace values greater than UW with 95th quantile.","97ea3fce":"## Test MLP","b58d520b":"There are some 0 values for **BMI**. We replace them with median. Also, we replace values greater than UW with q95th.","cf0b3409":"## Load Data\n\nPima Indians Diabetes dataset is used.","cca805b5":"### Feature 6 - DiabetesPedigreeFunction","0e4a5228":"## Correlation Analysis\n\nLinear correlations between features and also between features and output are computed. **Pandas corr** function is used to compute correlation matrix and **Seaborn heatmap** is used for plotting.","ac1fe665":"There are two classes as expected, a patient has diabetes or doesn't have.","b8cce641":"### Feature 5 - BMI","cee31a1b":"### Feature 3 - SkinThickness","bdf43e0c":"There are some 0 values for **BloodPressure** which is unlikely. So we replace them with median. Also, we replace values lower than LW (except zeros) with 5th quantile and replace values greater than UW with 95th quantile.","26a7f2e3":"For Pregnancies feature, there are some measurements above upper whisker. These are rare events. We replace them with 95th quantile.","36e977bf":"There are some 0 values for **Insulin** which is unlikely. So we replace them with 60th quantile becuse median is 0. Also, we replace values greater than UW with 95th quantile.","df6da7dc":"## Standardization\n\nTo increase the learning performance, input features are standardized. Mean and standard deviation of the feature are computed. Then, mean is subtracted from each sample of the feature and result is divided by standard deviation. The aim is to transform the feature to have mean of 0 and standard deviation of 1. **StandardScaler** of **scikit-learn** is used. A **StandardScaler** is fit to the feature in **train_X**, then this scaler transforms the same feature in **train_X** and **test_X**.","5cfbbd9e":"### Feature 7 - Age","587357cd":"### Feature 1 - Glucose","9eaa3234":"There are some measurements above UW due to rare events. We replace them with 95th quantile."}}