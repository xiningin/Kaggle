{"cell_type":{"beeb766a":"code","07e5d1d1":"code","99e6e3a9":"code","4e75d425":"code","c9d47eae":"code","bbf72aca":"code","b3c8a094":"code","695f718c":"code","f79f6f14":"markdown","51649af8":"markdown","8e424bea":"markdown","93629d70":"markdown"},"source":{"beeb766a":"import os\n\nimport librosa\nimport numpy as np\nimport torch","07e5d1d1":"RAW_AUDIO = \"..\/input\/english-multispeaker-corpus-for-voice-cloning\/VCTK-Corpus\"\nRAW_AUDIO = \"\/kaggle\/input\/english-multispeaker-corpus-for-voice-cloning\/VCTK-Corpus\/VCTK-Corpus\/wav48\/\"\ntoal_num_speakers = len(os.listdir(RAW_AUDIO))\ntoal_num_speakers","99e6e3a9":"i = 0\nj = 0\nfor dirname, _, filenames in os.walk(RAW_AUDIO):\n    i = 0\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        i += 1\n        \n        if i > 10:\n            print('')\n            break\n    \n    if j > 2:\n        break\n    \n    j += 1\n","4e75d425":"def get_hyper_parameters():\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    print(f\"Device type available {device}\")\n    hparam_dict = {\n\n        # genereal parameters\n        \"is_training_mode\": True,\n        \"device\": device,\n\n        # data:\n        \"raw_data\": \"\/kaggle\/input\/english-multispeaker-corpus-for-voice-cloning\/VCTK-Corpus\/VCTK-Corpus\/wav48\/\",\n        \"train_spectrogram_path\": \"spectrograms\/train\",\n        \"test_spectrogram_path\": \"spectrograms\/test\",\n        \n        \"is_data_preprocessed\": True,\n        \"sr\": 16000,\n\n        # For mel spectrogram preprocess\n        \"nfft\": 512,\n        \"window\": 0.025,  # (s)\n        \"hop\": 0.01,  # (s)\n        \"n_mels\": 40,  # Number of mel energies\n        \"tisv_frame\": 180,  # Max number of time steps in input after preprocess\n\n        # small error\n        \"small_err\": 1e-6,\n\n    }\n    return hparam_dict\n\n\nclass DictWithDotNotation(dict):\n    \"\"\"\n    a dictionary that supports dot notation \n    as well as dictionary access notation \n    usage: d = DotDict() or d = DotDict({'val1':'first'})\n    set attributes: d.val2 = 'second' or d['val2'] = 'second'\n    get attributes: d.val2 or d['val2']\n    \"\"\"\n    __getattr__ = dict.__getitem__\n    __setattr__ = dict.__setitem__\n    __delattr__ = dict.__delitem__\n\n    def __init__(self, dct=None):\n        dct = dict() if not dct else dct\n        for key, value in dct.items():\n            if hasattr(value, 'keys'):\n                value = Dict_with_dot_notation(value)\n            self[key] = value\n\n\nclass HyperParameters(DictWithDotNotation):\n\n    def __init__(self, hp_dict=None):\n        super(DictWithDotNotation, self).__init__()\n\n        if hp_dict is None:\n            hp_dict = get_hyper_parameters()\n\n        hp_dotdict = DictWithDotNotation(hp_dict)\n        for k, v in hp_dotdict.items():\n            setattr(self, k, v)\n\n    __getattr__ = DictWithDotNotation.__getitem__\n    __setattr__ = DictWithDotNotation.__setitem__\n    __delattr__ = DictWithDotNotation.__delitem__\n\n\nhp = HyperParameters()\nhp","c9d47eae":"def save_spectrogram_tisv(train_cnt, speaker_break_cnt=30, speaker_utter_cnt=100):\n    \"\"\" Full preprocess of text independent utterance. The log-mel-spectrogram is saved as numpy file.\n        Each partial utterance is split by voice detection using DB\n        and the first and the last 180 frames from each partial utterance are saved.\n        Need : utterance data set (VTCK)\n    \"\"\"\n    print(\"Text independent speaker verification (TISV) utterance feature extraction started..\")\n\n    # Create folder if does not exist, if exist then ignore\n    os.makedirs(hp.train_spectrogram_path, exist_ok=True)\n    os.makedirs(hp.test_spectrogram_path, exist_ok=True)\n\n    # lower bound of utterance length\n    utter_min_len = (hp.tisv_frame * hp.hop + hp.window) * hp.sr\n\n    lst_folders = os.listdir(hp.raw_data)\n    total_speaker_num = len(lst_folders)\n    \n    # split total data 80% train and 20% test\n    train_speaker_num = (total_speaker_num \/\/ 10) * 8  \n    \n    # normalizing train and test splits based on the inputs received\n    if total_speaker_num < speaker_break_cnt:\n        speaker_break_cnt = total_speaker_num \n    \n    if train_speaker_num < train_cnt:\n        train_cnt = train_speaker_num\n    \n    print(f\"Total speakers to be saved {speaker_break_cnt}\")\n    print(f\"Saving '{train_cnt}' training data and '{speaker_break_cnt - train_cnt}' test data\")\n    \n    print(\"\\n\\n####################\\n\\n\")\n    \n    # this is list has 109 folders\/items\/speakers\n    lst_all_speaker_folders = os.listdir(hp.raw_data)\n    \n    for i, folder in enumerate(lst_all_speaker_folders):\n        # path of each speaker\n        per_speaker_folder = os.path.join(hp.raw_data, folder)\n        per_speaker_wavs = os.listdir(per_speaker_folder)\n\n\n        print(f\"Processing speaker no. {i + 1} with '{len(per_speaker_wavs)}'' audio files\")\n        utterances_spec = []\n        k = 0\n        for utter_wav_file in per_speaker_wavs:\n            try:\n                utter_wav_file_path = os.path.join(per_speaker_folder, utter_wav_file)  # path of each utterance\n                utter, sr = librosa.core.load(utter_wav_file_path, hp.sr)  # load utterance audio\n                intervals = librosa.effects.split(utter, top_db=20)  # voice activity detection\n\n            except:\n                print(f\"Error occured, skipped speaker {folder}\")\n                continue\n\n            # looping through all the instance in the audios where utterance was loud enough\n            for interval in intervals:\n                if (interval[1] - interval[0]) >= utter_min_len:  # If partial utterance is sufficient long,\n                    utter_part = utter[interval[0]: interval[1]]  # save first and last 180 frames of spectrogram.\n\n                    dict_data = {\n                        \"y\": utter_part,\n                        \"n_fft\": hp.nfft,\n                        \"win_length\": int(hp.window * sr),\n                        \"hop_length\": int(hp.hop * sr)\n                    }\n                    # this return S as complex number with magnitude and direction\n                    S = librosa.core.stft(**dict_data)\n\n                    # we take ABS of complex number, we get the magnitude and we loose the direction info\n                    S = np.abs(S) ** 2\n                    mel_basis = librosa.filters.mel(sr=hp.sr, n_fft=hp.nfft, n_mels=hp.n_mels)\n                    S = np.log10(np.dot(mel_basis, S) + hp.small_err)  # log mel spectrogram of utterances\n\n                    # first 180 frames of partial utterance\n                    audio_extract = S[:, :hp.tisv_frame]\n                    utterances_spec.append(audio_extract)\n\n                    # last 180 frames of partial utterance\n                    if (interval[1] - interval[0]) > utter_min_len:\n                        audio_extract = S[:, -hp.tisv_frame:]\n                        utterances_spec.append(audio_extract)\n\n            # checking the length of utterance spec per wav file\n            print(\".\", end=\"\")\n\n            # if utter len already more than requested len, then no need to check other audio files\n            if len(utterances_spec) >= speaker_utter_cnt:\n                print(\"\")\n                break\n\n        # collecting all the utterances across all the wav files into np array\n        utterances_spec = np.array(utterances_spec)\n        # Checking if speaker's utterance qualifies to be used. i.e. a min utterance length is available in the audio\n        if utterances_spec.shape[0] >= speaker_utter_cnt:\n            print(f\"Size saved = {utterances_spec.shape}\")\n            if i < train_cnt:  # save spectrogram as numpy file\n                file_full_path = os.path.join(hp.train_spectrogram_path, f\"speaker_{folder}_{i + 1}.npy\")\n            else:\n                file_full_path = os.path.join(hp.test_spectrogram_path, f\"speaker_{folder}_{i + 1}.npy\")\n\n            np.save(file_full_path, utterances_spec)\n\n        if (i + 1) >= speaker_break_cnt:\n            break\n                \n    print(\"Spectrograms saved!!\")","bbf72aca":"train_cnt = 30 # number of speakers for training\ntest_cnt = 8 # number of speakers for testing\nspeaker_break_cnt = train_cnt + test_cnt\nsave_spectrogram_tisv(train_cnt, speaker_break_cnt, speaker_utter_cnt=100)","b3c8a094":"# checking files in train folder\ni = 0\nj = 0\nfor dirname, _, filenames in os.walk(\".\/spectrograms\/train\"):\n    i = 0\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        i += 1\n        \n        if i > 10:\n            print('')\n            break\n    \n    if j > 2:\n        break\n    \n    j += 1","695f718c":"# checking files in train folder\ni = 0\nj = 0\nfor dirname, _, filenames in os.walk(\".\/spectrograms\/test\"):\n    i = 0\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        i += 1\n        \n        if i > 10:\n            print('')\n            break\n    \n    if j > 2:\n        break\n    \n    j += 1","f79f6f14":"# Defining hyper parameters\n\n**You should change parameters here (if required)**","51649af8":"# Please read the data source used to understand the data","8e424bea":"# This is S1 of a series of 3 notebooks\n* S1 - data prepration\n* S2 - building and training the model\n* S3 - testing the model\n\n\nIn this notebook we will read through all the speakers and save a mel-spectrogram as a np file. I take following steps.\n1. Use 'train_cnt' to decide how many training sample I need.\n2. Use 'speaker_break_cnt' which say how many total sample I need\n3. Therefore test samples = speaker_break_cnt-train_cnt\n4. Since there are 109 speakers with so many audio files. I read each file and pick up utterances> certain dB level and put it into a list. If this list contains more than 'speaker_utter_cnt' I stop collecting sample for that person and move on to next person.\n\n**Hence, at the end I have a list that contains 'train_cnt' number of files and each file is of shape (speaker_utter_cnt x n_mel x utter_length)**","93629d70":"# Looking at how data is structured"}}