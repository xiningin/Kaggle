{"cell_type":{"ec675a99":"code","8ee4aef7":"code","d23ae0d4":"code","925c6dab":"code","3716baed":"code","94bb9484":"code","9b04c358":"code","a65bedb4":"code","c7f2c899":"code","ec8979cc":"code","b35ffc39":"code","31231f81":"code","854dbfb3":"code","ffbc4416":"code","5acd1eec":"code","455baff6":"code","b06f7087":"code","4ea1a256":"code","4fe9641d":"code","74eb0e55":"code","a25de777":"code","e6f08650":"code","70e59e20":"code","4d444f8f":"code","dbd46654":"code","a27a7c61":"markdown","1ae34e45":"markdown","f7bef91b":"markdown","69b58345":"markdown","04aae6cf":"markdown","09a1d1a7":"markdown","896b8559":"markdown","39f034fa":"markdown","867c51ff":"markdown","3ab320be":"markdown","6a74793d":"markdown","2c61383a":"markdown"},"source":{"ec675a99":"import numpy as np \nimport pandas as pd","8ee4aef7":"from sklearn.decomposition import PCA, SparsePCA","d23ae0d4":"train_org = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\nprint(train_org.shape)\n\ntest_org = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')\nprint(test_org.shape)\n\ndf_dae = pd.read_csv('..\/input\/dae-te-le-tps0521\/df_dae.csv')\nprint(df_dae.shape)\n\ndf_le = pd.read_csv(\"..\/input\/dae-te-le-tps0521\/df_le.csv\")\nprint(df_le.shape)\n\ndf_te = pd.read_csv(\"..\/input\/dae-te-le-tps0521\/df_te.csv\")\nprint(df_te.shape)\n\ny = pd.read_csv(\"..\/input\/dae-te-le-tps0521\/y.csv\")\nprint(y.shape)","925c6dab":"X = pd.concat([df_dae, df_le, df_te], axis = 1)[:100000]\nprint(X.shape)\n\nX_tst = pd.concat([df_dae, df_le, df_te], axis = 1)[100000:]\nprint(X_tst.shape)","3716baed":"pca = PCA(n_components = 40)\n\nX_pca = pca.fit_transform(X)\n\nX_tst_pca = pca.transform(X_tst)\n\nprint(sum(pca.explained_variance_ratio_))","94bb9484":"print(X_pca.shape)\n\nprint(X_tst_pca.shape)\n\n\nnp.save('X_pca.npy', X_pca)\nnp.save('X_tst_pca.npy', X_tst_pca)\n\n#X_pca.to_csv(\"X_pca.csv\", index = False)\n#X_tst_pca.to_csv(\"X_tst_pca.csv\", index = False)","9b04c358":"import matplotlib.pyplot as plt","a65bedb4":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(random_state=1, max_depth=10)\n#df=pd.get_dummies(df)\nmodel.fit(X,y)\n\n\nfeatures = X.columns\nimportances = model.feature_importances_\nindices = np.argsort(importances)[-19:]  # top 10 features\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()\n\nfrom sklearn.feature_selection import SelectFromModel\nfeature = SelectFromModel(model)\nFit = feature.fit_transform(X,y)","c7f2c899":"feature.threshold_","ec8979cc":"X.shape","b35ffc39":"Fit.shape","31231f81":"np.save('RF_features.npy', Fit)","854dbfb3":"from sklearn.feature_selection import RFE\nfrom sklearn.svm import SVC","ffbc4416":"# selector = RFE(estimator = SVC(), n_features_to_select=40, step=1)\n# Fit_rfe = selector.fit_transform(X, y)","5acd1eec":"# Fit_rfe.shape","455baff6":"# np.save('RFE_features.npy', Fit_rfe)","b06f7087":"!pip install imbalanced-learn -q","4ea1a256":"import imblearn \n\nprint(imblearn.__version__)","4fe9641d":"from collections import Counter \nfrom imblearn.over_sampling import SMOTE\n\ncounter = Counter(np.array(y['target']))\nprint(counter)\n# transform the dataset\noversample = SMOTE()\nX_smote, y_smote = oversample.fit_resample(X, y)\n# summarize the new class distribution\n","74eb0e55":"counter = Counter(np.array(y_smote['target']))\nprint(counter)","a25de777":"np.save('X_smote.npy', X_smote)\nnp.save('y_smote.npy', y_smote)","e6f08650":"#spca = SparsePCA()","70e59e20":"#!pip install git+https:\/\/github.com\/niitsuma\/delayedsparse \n","4d444f8f":"#Later\n\n# delayed_pca=delayedsparse.PCA(n_components=3)\n\n# delayed_pca.fit(X)\n\n# delayed_pca.explained_variance_ratio_","dbd46654":"# Feature Reduction methods\n# https:\/\/thenewstack.io\/3-new-techniques-for-data-dimensionality-reduction-in-machine-learning\/\n# https:\/\/www.kdnuggets.com\/2015\/05\/7-methods-data-dimensionality-reduction.html\n# https:\/\/machinelearningmastery.com\/dimensionality-reduction-for-machine-learning\/\n# https:\/\/machinelearningmastery.com\/calculate-feature-importance-with-python\/\n# https:\/\/machinelearningmastery.com\/feature-selection-with-real-and-categorical-data\/\n# https:\/\/machinelearningmastery.com\/rfe-feature-selection-in-python\/\n    \n\n# https:\/\/scikit-learn.org\/0.18\/auto_examples\/plot_compare_reduction.html\n\n# UMAP \n# https:\/\/umap-learn.readthedocs.io\/en\/latest\/sparse.html","a27a7c61":"Delayed Sparse Matrix","1ae34e45":"## Collection of various Feature Reduction Methods ","f7bef91b":"Random Forest Feature Selection","69b58345":"Highly Computationally Expensive","04aae6cf":"Non-Negative Matrix Factorization","09a1d1a7":"TruncatedSVD","896b8559":"SparsePCA","39f034fa":"PCA Standard","867c51ff":"Forward Selection","3ab320be":"IncrementalSVD","6a74793d":"RBE ","2c61383a":"SMOTE Upsampled Features"}}