{"cell_type":{"218d7ef9":"code","422f08e8":"code","67ec793c":"code","5a99ba6d":"code","83413069":"code","2f7dcd21":"code","cc80bdd3":"code","c5de3cdd":"code","51e0a022":"code","517ffe44":"code","b2a32644":"code","62654621":"code","552d3d2c":"code","e34834c7":"code","8a7cb83f":"markdown","3e5566ff":"markdown","7945fe8f":"markdown","1f4b4017":"markdown"},"source":{"218d7ef9":"#@title Install dependencies\n!pip install pyknon\n!pip install pretty_midi\n!pip install pypianoroll\n!pip install mir_eval\n!apt install fluidsynth #Pip does not work for some reason. Only apt works\n!pip install midi2audio\n!cp \/usr\/share\/sounds\/sf2\/FluidR3_GM.sf2 \/content\/font.sf2","422f08e8":"#@title Load all modules, check the available devices (GPU\/CPU), and setup MIDI parameters\nimport numpy as np\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\n\nimport keras\nfrom keras.utils import to_categorical\n\nimport time\n\nimport pretty_midi\nfrom midi2audio import FluidSynth\nfrom google.colab import output\nfrom IPython.display import display, Javascript, HTML, Audio\n\ndtype = torch.float\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# Assume that we are on a CUDA machine, then this should print a CUDA device:\nprint('Available Device:', device)\n\n!mkdir \/content\/midis\n\nsample_freq_variable = 12 #@param {type:\"number\"}\nnote_range_variable = 62 #@param {type:\"number\"}\nnote_offset_variable = 33 #@param {type:\"number\"}\nnumber_of_instruments = 2 #@param {type:\"number\"}\nchamber_option = True #@param {type:\"boolean\"}","67ec793c":"#@title (OPTION 1) Convert your own MIDIs to Notewise TXT DataSet (before running this cell, upload your MIDI DataSet to \/content\/midis folder)\nimport tqdm.auto\nimport argparse\nimport random\nimport os\nimport numpy as np\nfrom math import floor\nfrom pyknon.genmidi import Midi\nfrom pyknon.music import NoteSeq, Note\nimport music21\nfrom music21 import instrument, volume\nfrom music21 import midi as midiModule\nfrom pathlib import Path\nimport glob, sys\nfrom music21 import converter, instrument\n%cd \/content\nnotes=[]\nInstrumentID=0\nfolder = '\/content\/midis\/*mid'\nfor file in tqdm.auto.tqdm(glob.glob(folder)):\n    filename = file[-53:]\n    print(filename)\n\n    # fname = \"..\/midi-files\/mozart\/sonat-3.mid\"\n    fname = filename\n\n    mf=music21.midi.MidiFile()\n    mf.open(fname)\n    mf.read()\n    mf.close()\n    midi_stream=music21.midi.translate.midiFileToStream(mf)\n    midi_stream\n\n\n\n    sample_freq=sample_freq_variable\n    note_range=note_range_variable\n    note_offset=note_offset_variable\n    chamber=chamber_option\n    numInstruments=number_of_instruments\n\n    s = midi_stream\n    #print(s.duration.quarterLength)\n\n    s[0].elements\n\n\n    maxTimeStep = floor(s.duration.quarterLength * sample_freq)+1\n    score_arr = np.zeros((maxTimeStep, numInstruments, note_range))\n\n    #print(maxTimeStep, \"\\n\", score_arr.shape)\n\n    # define two types of filters because notes and chords have different structures for storing their data\n    # chord have an extra layer because it consist of multiple notes\n\n    noteFilter=music21.stream.filters.ClassFilter('Note')\n    chordFilter=music21.stream.filters.ClassFilter('Chord')\n      \n\n    # pitch.midi-note_offset: pitch is the numerical representation of a note. \n    #                         note_offset is the the pitch relative to a zero mark. eg. B-=25, C=27, A=24\n\n    # n.offset: the timestamps of each note, relative to the start of the score\n    #           by multiplying with the sample_freq, you make all the timestamps integers\n\n    # n.duration.quarterLength: the duration of that note as a float eg. quarter note = 0.25, half note = 0.5\n    #                           multiply by sample_freq to represent duration in terms of timesteps\n\n    notes = []\n    instrumentID = 0\n    parts = instrument.partitionByInstrument(s)\n    for i in range(len(parts.parts)): \n      instru = parts.parts[i].getInstrument()\n      \n\n    for n in s.recurse().addFilter(noteFilter):\n        if chamber:\n          # assign_instrument where 0 means piano-like and 1 means violin-like, and -1 means neither\n          if instru.instrumentName == 'Violin':\n            notes.append((n.pitch.midi-note_offset, floor(n.offset*sample_freq), \n              floor(n.duration.quarterLength*sample_freq), 1))\n            \n        notes.append((n.pitch.midi-note_offset, floor(n.offset*sample_freq), \n              floor(n.duration.quarterLength*sample_freq), 0))\n        \n    #print(len(notes))\n    notes[-5:]\n\n    # do the same using a chord filter\n\n    for c in s.recurse().addFilter(chordFilter):\n        # unlike the noteFilter, this line of code is necessary as there are multiple notes in each chord\n        # pitchesInChord is a list of notes at each chord eg. (<music21.pitch.Pitch D5>, <music21.pitch.Pitch F5>)\n        pitchesInChord=c.pitches\n        \n        # do same as noteFilter and append all notes to the notes list\n        for p in pitchesInChord:\n            notes.append((p.midi-note_offset, floor(c.offset*sample_freq), \n                          floor(c.duration.quarterLength*sample_freq), 1))\n\n        # do same as noteFilter and append all notes to the notes list\n        for p in pitchesInChord:\n            notes.append((p.midi-note_offset, floor(c.offset*sample_freq), \n                          floor(c.duration.quarterLength*sample_freq), 0))\n    #print(len(notes))\n    notes[-5:]\n\n    # the variable\/list \"notes\" is a collection of all the notes in the song, not ordered in any significant way\n\n    for n in notes:\n        \n        # pitch is the first variable in n, previously obtained by n.midi-note_offset\n        pitch=n[0]\n        \n        # do some calibration for notes that fall our of note range\n        # i.e. less than 0 or more than note_range\n        while pitch<0:\n            pitch+=12\n        while pitch>=note_range:\n            pitch-=12\n            \n        # 3rd element refers to instrument type => if instrument is violin, use different pitch calibration\n        if n[3]==1:      #Violin lowest note is v22\n            while pitch<22:\n                pitch+=12\n\n        # start building the 3D-tensor of shape: (796, 1, 38)\n        # score_arr[0] = timestep\n        # score_arr[1] = type of instrument\n        # score_arr[2] = pitch\/note out of the range of note eg. 38\n        \n        # n[0] = pitch\n        # n[1] = timestep\n        # n[2] = duration\n        # n[3] = instrument\n        #print(n[3])\n        score_arr[n[1], n[3], pitch]=1                  # Strike note\n        score_arr[n[1]+1:n[1]+n[2], n[3], pitch]=2      # Continue holding note\n\n    #print(score_arr.shape)\n    # print first 5 timesteps\n    score_arr[:5,0,]\n\n\n    for timestep in score_arr:\n        #print(list(reversed(range(len(timestep)))))\n        break\n\n    instr={}\n    instr[0]=\"p\"\n    instr[1]=\"v\"\n\n    score_string_arr=[]\n\n    # loop through all timesteps\n    for timestep in score_arr:\n        \n        # selecting the instruments: i=0 means piano and i=1 means violin\n        for i in list(reversed(range(len(timestep)))):   # List violin note first, then piano note\n            \n            # \n            score_string_arr.append(instr[i]+''.join([str(int(note)) for note in timestep[i]]))\n\n    #print(type(score_string_arr), len(score_string_arr))\n    score_string_arr[:5]\n\n    modulated=[]\n    # get the note range from the array\n    note_range=len(score_string_arr[0])-1\n\n    for i in range(0,12):\n        for chord in score_string_arr:\n            \n            # minus the instrument letter eg. 'p'\n            # add 6 zeros on each side of the string\n            padded='000000'+chord[1:]+'000000'\n            \n            # add back the instrument letter eg. 'p'\n            # append window of len=note_range back into \n            # eg. if we have \"00012345000\"\n            # iteratively, we want to get \"p00012\", \"p00123\", \"p01234\", \"p12345\", \"p23450\", \"p34500\", \"p45000\",\n            modulated.append(chord[0]+padded[i:i+note_range])\n\n    # 796 * 12\n    #print(len(modulated))\n    modulated[:5]\n\n    # input of this function is a modulated string\n    long_string = modulated\n\n    translated_list=[]\n\n    # for every timestep of the string\n    for j in range(len(long_string)):\n        \n        # chord at timestep j eg. 'p00000000000000000000000000000000000100'\n        chord=long_string[j]\n        next_chord=\"\"\n        \n        # range is from next_timestep to max_timestep\n        for k in range(j+1, len(long_string)):\n            \n            # checking if instrument of next chord is same as current chord\n            if long_string[k][0]==chord[0]:\n                \n                # if same, set next chord as next element in modulation\n                # otherwise, keep going until you find a chord with the same instrument\n                # when you do, set it as the next chord\n                next_chord=long_string[k]\n                break\n        \n        # set prefix as the instrument\n        # set chord and next_chord to be without the instrument prefix\n        # next_chord is necessary to check when notes end\n        prefix=chord[0]\n        chord=chord[1:]\n        next_chord=next_chord[1:]\n        \n        # checking for non-zero notes at one particular timestep\n        # i is an integer indicating the index of each note the chord\n        for i in range(len(chord)):\n            \n            if chord[i]==\"0\":\n                continue\n            \n            # set note as 2 elements: instrument and index of note\n            # examples: p22, p16, p4\n            #p = music21.pitch.Pitch()\n            #nt = music21.note.Note(p)\n            #n.volume.velocity = 20\n            #nt.volume.client == nt\n            #V = nt.volume.velocity\n            #print(V)\n            #note=prefix+str(i)+' V' + str(V)\n            note=prefix+str(i)                \n            \n            # if note in chord is 1, then append the note eg. p22 to the list\n            if chord[i]==\"1\":\n                translated_list.append(note)\n            \n            # If chord[i]==\"2\" do nothing - we're continuing to hold the note\n            \n            # unless next_chord[i] is back to \"0\" and it's time to end the note.\n            if next_chord==\"\" or next_chord[i]==\"0\":      \n                translated_list.append(\"end\"+note)\n\n        # wait indicates end of every timestep\n        if prefix==\"p\":\n            translated_list.append(\"wait\")\n\n    #print(len(translated_list))\n    translated_list[:10]\n\n    # this section transforms the list of notes into a string of notes\n\n    # initialize i as zero and empty string\n    i=0\n    translated_string=\"\"\n\n\n    while i<len(translated_list):\n        \n        # stack all the repeated waits together using an integer to indicate the no. of waits\n        # eg. \"wait wait\" => \"wait2\"\n        wait_count=1\n        if translated_list[i]=='wait':\n            while wait_count<=sample_freq*2 and i+wait_count<len(translated_list) and translated_list[i+wait_count]=='wait':\n                wait_count+=1\n            translated_list[i]='wait'+str(wait_count)\n            \n        # add next note\n        translated_string+=translated_list[i]+\" \"\n        i+=wait_count\n\n    translated_string[:100]\n    len(translated_string)\n\n    #print(\"chordwise encoding type and length:\", type(modulated), len(modulated))\n    #print(\"notewise encoding type and length:\", type(translated_string), len(translated_string))\n\n    # default settings: sample_freq=12, note_range=62\n\n    chordwise_folder = \"..\/\"\n    notewise_folder = \"..\/\"\n\n    # export chordwise encoding\n#    f=open(chordwise_folder+fname+\"_chordwise\"+\".txt\",\"w+\")\n#    f.write(\" \".join(modulated))\n#    f.close()\n\n    # export notewise encoding\n    f=open(notewise_folder+fname+\"_notewise\"+\".txt\",\"w+\")\n    f.write(translated_string)\n    f.close()\n\nfolder = '\/content\/midis\/*notewise.txt'\n\n\nfilenames = glob.glob('\/content')\nwith open('notewise_custom_dataset.txt', 'w') as outfile:\n    for fname in glob.glob(folder)[-53:]:\n        with open(fname) as infile:\n            for line in infile:\n                outfile.write(line)\n\n#folder = '\/content\/midis\/*chordwise.txt'\n\n#filenames = glob.glob('\/content')\n#with open('chordwise_custom_dataset.txt', 'w') as outfile:\n#    for fname in glob.glob(folder)[-53:]:\n#        with open(fname) as infile:\n#            for line in infile:\n#                outfile.write(line)","5a99ba6d":"#@title (OPTION 2) Download ready-to-use Piano and Chamber Notewise DataSets\n%cd \/content\/\n!wget 'https:\/\/github.com\/asigalov61\/SuperPiano\/raw\/master\/Super%20Chamber%20Piano%20Violin%20Notewise%20DataSet.zip'\n!unzip '\/content\/Super Chamber Piano Violin Notewise DataSet.zip'\n!rm '\/content\/Super Chamber Piano Violin Notewise DataSet.zip'\n\n!wget 'https:\/\/github.com\/asigalov61\/SuperPiano\/raw\/master\/Super%20Chamber%20Piano%20Only%20Notewise%20DataSet.zip'\n!unzip '\/content\/Super Chamber Piano Only Notewise DataSet.zip'\n!rm '\/content\/Super Chamber Piano Only Notewise DataSet.zip'","83413069":"#@title Load and Encode TXT Notes DataSet\nselect_training_dataset_file = \"\/content\/notewise_custom_dataset.txt\" #@param {type:\"string\"}\n\n# replace with any text file containing full set of data\nMIDI_data = select_training_dataset_file\n\nwith open(MIDI_data, 'r') as file:\n    text = file.read()\n\n# get vocabulary set\nwords = sorted(tuple(set(text.split())))\nn = len(words)\n\n# create word-integer encoder\/decoder\nword2int = dict(zip(words, list(range(n))))\nint2word = dict(zip(list(range(n)), words))\n\n# encode all words in dataset into integers\nencoded = np.array([word2int[word] for word in text.split()])","2f7dcd21":"#@title Define all functions\n# define model using the pytorch nn module\nclass WordLSTM(nn.ModuleList):\n    \n    def __init__(self, sequence_len, vocab_size, hidden_dim, batch_size):\n        super(WordLSTM, self).__init__()\n        \n        # init the hyperparameters\n        self.vocab_size = vocab_size\n        self.sequence_len = sequence_len\n        self.batch_size = batch_size\n        self.hidden_dim = hidden_dim\n        \n        # first layer lstm cell\n        self.lstm_1 = nn.LSTMCell(input_size=vocab_size, hidden_size=hidden_dim)\n        \n        # second layer lstm cell\n        self.lstm_2 = nn.LSTMCell(input_size=hidden_dim, hidden_size=hidden_dim)\n\n        # third layer lstm cell\n        #self.lstm_3 = nn.LSTMCell(input_size=hidden_dim, hidden_size=hidden_dim)\n\n        # dropout layer\n        self.dropout = nn.Dropout(p=0.35)\n        \n        # fully connected layer\n        self.fc = nn.Linear(in_features=hidden_dim, out_features=vocab_size)\n        \n    # forward pass in training   \n    def forward(self, x, hc):\n        \"\"\"\n            accepts 2 arguments: \n            1. x: input of each batch \n                - shape 128*149 (batch_size*vocab_size)\n            2. hc: tuple of init hidden, cell states \n                - each of shape 128*512 (batch_size*hidden_dim)\n        \"\"\"\n        \n        # create empty output seq\n        output_seq = torch.empty((self.sequence_len,\n                                  self.batch_size,\n                                  self.vocab_size))\n        # if using gpu        \n        output_seq = output_seq.to(device)\n        \n        # init hidden, cell states for lstm layers\n        hc_1, hc_2, hc_3 = hc, hc, hc\n        \n        # for t-th word in every sequence \n        for t in range(self.sequence_len):\n            \n            # layer 1 lstm\n            hc_1 = self.lstm_1(x[t], hc_1)\n            h_1, c_1 = hc_1\n            \n            # layer 2 lstm\n            hc_2 = self.lstm_2(h_1, hc_2)\n            h_2, c_2 = hc_2\n\n            # layer 3 lstm\n            #hc_3 = self.lstm_3(h_2, hc_3)\n            #h_3, c_3 = hc_3\n            \n            # dropout and fully connected layer\n            output_seq[t] = self.fc(self.dropout(h_2))\n            \n        return output_seq.view((self.sequence_len * self.batch_size, -1))\n          \n    def init_hidden(self):\n        \n        # initialize hidden, cell states for training\n        # if using gpu\n        return (torch.zeros(self.batch_size, self.hidden_dim).to(device),\n                torch.zeros(self.batch_size, self.hidden_dim).to(device))\n    \n    def init_hidden_generator(self):\n        \n        # initialize hidden, cell states for prediction of 1 sequence\n        # if using gpu\n        return (torch.zeros(1, self.hidden_dim).to('cpu'),\n                torch.zeros(1, self.hidden_dim).to('cpu'))\n    \n    def predict(self, seed_seq, top_k=5, pred_len=128):\n        \"\"\"\n            accepts 3 arguments: \n            1. seed_seq: seed string sequence for prediction (prompt)\n            2. top_k: top k words to sample prediction from\n            3. pred_len: number of words to generate after the seed seq\n        \"\"\"\n        \n        # set evaluation mode\n        self.eval()\n        \n        # split string into list of words\n        seed_seq = seed_seq.split()\n        \n        # get seed sequence length\n        seed_len = len(seed_seq)\n        \n        # create output sequence\n        out_seq = np.empty(seed_len+pred_len)\n        \n        # append input seq to output seq\n        out_seq[:seed_len] = np.array([word2int[word] for word in seed_seq])\n \n        # init hidden, cell states for generation\n        hc = self.init_hidden_generator()\n        hc_1, hc_2, hc_3 = hc, hc, hc\n        \n        # feed seed string into lstm\n        # get the hidden state set up\n        for word in seed_seq[:-1]:\n            \n            # encode starting word to one-hot encoding\n            word = to_categorical(word2int[word], num_classes=self.vocab_size)\n\n            # add batch dimension\n            word = torch.from_numpy(word).unsqueeze(0)\n            # if using gpu\n            word = word.to('cpu') \n            \n            # layer 1 lstm\n            hc_1 = self.lstm_1(word, hc_1)\n            h_1, c_1 = hc_1\n            \n            # layer 2 lstm\n            hc_2 = self.lstm_2(h_1, hc_2)\n            h_2, c_2 = hc_2\n\n            # layer 3 lstm\n            #hc_3 = self.lstm_3(h_2, hc_3)\n            #h_3, c_3 = hc_3            \n\n        word = seed_seq[-1]\n        \n        # encode starting word to one-hot encoding\n        word = to_categorical(word2int[word], num_classes=self.vocab_size)\n\n        # add batch dimension\n        word = torch.from_numpy(word).unsqueeze(0)\n        # if using gpu\n        word = word.to('cpu') \n\n        # forward pass\n        for t in range(pred_len):\n            \n            # layer 1 lstm\n            hc_1 = self.lstm_1(word, hc_1)\n            h_1, c_1 = hc_1\n            \n            # layer 2 lstm\n            hc_2 = self.lstm_2(h_1, hc_2)\n            h_2, c_2 = hc_2\n\n            # layer 3 lstm\n            #hc_3 = self.lstm_3(h_2, hc_3)\n            #h_3, c_3 = hc_3\n            \n            # fully connected layer without dropout (no need)\n            output = self.fc(h_2)\n            \n            # software to get probabilities of output options\n            output = F.softmax(output, dim=1)\n            \n            # get top k words and corresponding probabilities\n            p, top_word = output.topk(top_k)\n            # if using gpu           \n            p = p.cpu()\n            \n            # sample from top k words to get next word\n            p = p.detach().squeeze().numpy()\n            top_word = torch.squeeze(top_word)\n            \n            word = np.random.choice(top_word, p = p\/p.sum())\n            \n            # add word to sequence\n            out_seq[seed_len+t] = word\n            \n            # encode predicted word to one-hot encoding for next step\n            word = to_categorical(word, num_classes=self.vocab_size)\n            word = torch.from_numpy(word).unsqueeze(0)\n            # if using gpu\n            word = word.to('cpu')\n            \n        return out_seq\n\n\ndef get_batches(arr, n_seqs, n_words):\n    \"\"\"\n        create generator object that returns batches of input (x) and target (y).\n        x of each batch has shape 128*128*149 (batch_size*seq_len*vocab_size).\n        \n        accepts 3 arguments:\n        1. arr: array of words from text data\n        2. n_seq: number of sequence in each batch (aka batch_size)\n        3. n_word: number of words in each sequence\n    \"\"\"\n    \n    # compute total elements \/ dimension of each batch\n    batch_total = n_seqs * n_words\n    \n    # compute total number of complete batches\n    n_batches = arr.size\/\/batch_total\n    \n    # chop array at the last full batch\n    arr = arr[: n_batches* batch_total]\n    \n    # reshape array to matrix with rows = no. of seq in one batch\n    arr = arr.reshape((n_seqs, -1))\n    \n    # for each n_words in every row of the dataset\n    for n in range(0, arr.shape[1], n_words):\n        \n        # chop it vertically, to get the input sequences\n        x = arr[:, n:n+n_words]\n        \n        # init y - target with shape same as x\n        y = np.zeros_like(x)\n        \n        # targets obtained by shifting by one\n        try:\n            y[:, :-1], y[:, -1] = x[:, 1:], x[:, n+n_words]\n        except IndexError:\n            y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n        \n        # yield function is like return, but creates a generator object\n        yield x, y   ","cc80bdd3":"#@title Compile the Model\ntraining_batch_size = 1024 #@param {type:\"slider\", min:0, max:1024, step:16}\nattention_span_in_tokens = 256 #@param {type:\"slider\", min:0, max:512, step:64}\nhidden_dimension_size = 256 #@param {type:\"slider\", min:0, max:512, step:64}\ntest_validation_ratio = 0.1 #@param {type:\"slider\", min:0, max:1, step:0.1}\nlearning_rate = 0.001 #@param {type:\"number\"}\n\n\n# compile the network - sequence_len, vocab_size, hidden_dim, batch_size\nnet = WordLSTM(sequence_len=attention_span_in_tokens, vocab_size=len(word2int), hidden_dim=hidden_dimension_size, batch_size=training_batch_size)\n# if using gpu\nnet.to(device)\n\n# define the loss and the optimizer\noptimizer = optim.Adam(net.parameters(), lr=learning_rate)\ncriterion = nn.CrossEntropyLoss()\n\n# split dataset into 90% train and 10% using index\nval_idx = int(len(encoded) * (1 - test_validation_ratio))\ntrain_data, val_data = encoded[:val_idx], encoded[val_idx:]\n\n# empty list for the validation losses\nval_losses = list()\n\n# empty list for the samples\nsamples = list()","c5de3cdd":"#@title (OPTION 1) Train the Model\nnumber_of_training_epochs = 300 #@param {type:\"slider\", min:1, max:300, step:1}\n\nimport tqdm\n\n# track time\nstart_time = time.time()\n\n# declare seed sequence\n#seed_string = \"p47 p50 wait8 endp47 endp50 wait4 p47 p50 wait8 endp47 endp50\"\n\n# finally train the model\nfor epoch in tqdm.auto.tqdm(range(number_of_training_epochs)):\n    \n    # init the hidden and cell states to zero\n    hc = net.init_hidden()\n    \n    # (x, y) refers to one batch with index i, where x is input, y is target\n    for i, (x, y) in enumerate(get_batches(train_data, training_batch_size, hidden_dimension_size)):\n        \n        # get the torch tensors from the one-hot of training data\n        # also transpose the axis for the training set and the targets\n        x_train = torch.from_numpy(to_categorical(x, num_classes=net.vocab_size).transpose([1, 0, 2]))\n        targets = torch.from_numpy(y.T).type(torch.LongTensor)  # tensor of the target\n        \n        # if using gpu\n        x_train = x_train.to(device)\n        targets = targets.to(device)\n        \n        # zero out the gradients\n        optimizer.zero_grad()\n        \n        # get the output sequence from the input and the initial hidden and cell states\n        # calls forward function\n        output = net(x_train, hc)\n    \n        # calculate the loss\n        # we need to calculate the loss across all batches, so we have to flat the targets tensor\n        loss = criterion(output, targets.contiguous().view(training_batch_size*hidden_dimension_size))\n        \n        # calculate the gradients\n        loss.backward()\n        \n        # update the parameters of the model\n        optimizer.step()\n        \n        # track time\n    \n        # feedback every 100 batches\n        if i % 100 == 0:\n            \n            # initialize the validation hidden state and cell state\n            val_h, val_c = net.init_hidden()\n            \n            for val_x, val_y in get_batches(val_data, training_batch_size, hidden_dimension_size):\n        \n                # prepare the validation inputs and targets\n                val_x = torch.from_numpy(to_categorical(val_x).transpose([1, 0, 2]))\n                val_y = torch.from_numpy(val_y.T).type(torch.LongTensor).contiguous().view(training_batch_size*hidden_dimension_size)\n  \n                # if using gpu\n                val_x = val_x.to(device)\n                val_y = val_y.to(device)\n            \n                # get the validation output\n                #val_output = net(val_x, (val_h, val_c))\n                \n                # get the validation loss\n                #val_loss = criterion(val_output, val_y)\n                \n                # append the validation loss\n                #val_losses.append(val_loss.item())\n                 \n                # samples.append(''.join([int2char[int_] for int_ in net.predict(\"p33\", seq_len=1024)]))\n                \n#            with open(\"..\/content\" + str(epoch) + \"_batch\" + str(i) + \".txt\", \"w\") as loss_file:\n#                loss_file.write(\"Epoch: {}, Batch: {}, Train Loss: {:.6f}, Validation Loss: {:.6f}\".format(epoch, i, loss.item(), val_loss.item()))\n\n#            with open(\"..\/content\" + str(epoch) + \"_batch\" + str(i) + \".txt\", \"w\") as outfile:\n#                outfile.write(' '.join([int2word[int_] for int_ in net.predict(seed_seq=seed_string, pred_len=512)]))\n        \n            # track time\n            duration = round(time.time() - start_time, 1)\n            start_time = time.time()\n    \n            print(\"Epoch: {}, Batch: {}, Duration: {} sec, Test Loss: {}\".format(epoch, i, duration, loss.item()))\n","51e0a022":"#@title (OPTION 1) Save the trained Model from memory\ntorch.save(net, '\/content\/trained_model.h5')","517ffe44":"#@title (OPTION 2) Download Super Chamber Piano Pre-Trained Chamber Model\n%cd \/content\/\n!wget 'https:\/\/github.com\/asigalov61\/SuperPiano\/raw\/master\/trained_model.h5'","b2a32644":"#@title (OPTION 2) Load existing\/pre-trained Model checkpoint\nmodel = torch.load('..\/content\/trained_model.h5', map_location='cpu')\nmodel.eval()","62654621":"#@title Generate TXT and MIDI file\nseed_prompt = \"p24 v24\" #@param {type:\"string\"}\ntokens_to_generate = 512 #@param {type:\"slider\", min:0, max:1024, step:16}\ntime_coefficient = 1.25 #@param {type:\"number\"}\ntop_k_coefficient =  4#@param {type:\"integer\"}\n\nwith open(\"..\/content\/output.txt\", \"w\") as outfile:\n    outfile.write(' '.join([int2word[int_] for int_ in model.predict(seed_seq=seed_prompt, pred_len=tokens_to_generate, top_k=top_k_coefficient)]))\nimport tqdm\nimport os\nimport dill as pickle\nfrom pathlib import Path\nimport random\nimport numpy as np\nimport pandas as pd\nfrom math import floor\nfrom pyknon.genmidi import Midi\nfrom pyknon.music import NoteSeq, Note\nimport music21\nimport random\nimport os, argparse\n\n# default settings: sample_freq=12, note_range=62\n\ndef decoder(filename):\n    \n    filedir = '\/content\/'\n\n    notetxt = filedir + filename\n\n    with open(notetxt, 'r') as file:\n        notestring=file.read()\n\n    score_note = notestring.split(\" \")\n\n    # define some parameters (from encoding script)\n    sample_freq=sample_freq_variable\n    note_range=note_range_variable\n    note_offset=note_offset_variable\n    chamber=chamber_option\n    numInstruments=number_of_instruments\n\n    # define variables and lists needed for chord decoding\n    speed=time_coefficient\/sample_freq\n    piano_notes=[]\n    violin_notes=[]\n    time_offset=0\n\n    # start decoding here\n    score = score_note\n\n    i=0\n\n    # for outlier cases, not seen in sonat-1.txt\n    # not exactly sure what scores would have \"p_octave_\" or \"eoc\" (end of chord?)\n    # it seems to insert new notes to the score whenever these conditions are met\n    while i<len(score):\n        if score[i][:9]==\"p_octave_\":\n            add_wait=\"\"\n            if score[i][-3:]==\"eoc\":\n                add_wait=\"eoc\"\n                score[i]=score[i][:-3]\n            this_note=score[i][9:]\n            score[i]=\"p\"+this_note\n            score.insert(i+1, \"p\"+str(int(this_note)+12)+add_wait)\n            i+=1\n        i+=1\n\n\n    # loop through every event in the score\n    for i in tqdm.auto.tqdm(range(len(score))):\n\n        # if the event is a blank, space, \"eos\" or unknown, skip and go to next event\n        if score[i] in [\"\", \" \", \"<eos>\", \"<unk>\"]:\n            continue\n\n        # if the event starts with 'end' indicating an end of note\n        elif score[i][:3]==\"end\":\n\n            # if the event additionally ends with eoc, increare the time offset by 1\n            if score[i][-3:]==\"eoc\":\n                time_offset+=1\n            continue\n\n        # if the event is wait, increase the timestamp by the number after the \"wait\"\n        elif score[i][:4]==\"wait\":\n            time_offset+=int(score[i][4:])\n            continue\n\n        # in this block, we are looking for notes   \n        else:\n            # Look ahead to see if an end<noteid> was generated\n            # soon after.  \n            duration=1\n            has_end=False\n            note_string_len = len(score[i])\n            for j in range(1,200):\n                if i+j==len(score):\n                    break\n                if score[i+j][:4]==\"wait\":\n                    duration+=int(score[i+j][4:])\n                if score[i+j][:3+note_string_len]==\"end\"+score[i] or score[i+j][:note_string_len]==score[i]:\n                    has_end=True\n                    break\n                if score[i+j][-3:]==\"eoc\":\n                    duration+=1\n\n            if not has_end:\n                duration=12\n\n            add_wait = 0\n            if score[i][-3:]==\"eoc\":\n                score[i]=score[i][:-3]\n                add_wait = 1\n\n            try: \n                new_note=music21.note.Note(int(score[i][1:])+note_offset)    \n                new_note.duration = music21.duration.Duration(duration*speed)\n                new_note.offset=time_offset*speed\n                if score[i][0]==\"v\":\n                    violin_notes.append(new_note)\n                else:\n                    piano_notes.append(new_note)                \n            except:\n                print(\"Unknown note: \" + score[i])\n\n\n\n\n            time_offset+=add_wait\n\n    # list of all notes for each instrument should be ready at this stage\n\n    # creating music21 instrument objects      \n    \n    piano=music21.instrument.fromString(\"Piano\")\n    violin=music21.instrument.fromString(\"Violin\")\n\n    # insert instrument object to start (0 index) of notes list\n    \n    piano_notes.insert(0, piano)\n    violin_notes.insert(0, violin)\n    # create music21 stream object for individual instruments\n    \n    piano_stream=music21.stream.Stream(piano_notes)\n    violin_stream=music21.stream.Stream(violin_notes)\n    # merge both stream objects into a single stream of 2 instruments\n    note_stream = music21.stream.Stream([piano_stream, violin_stream])\n\n    \n    note_stream.write('midi', fp=\"\/content\/\"+filename[:-4]+\".mid\")\n    print(\"Done! Decoded midi file saved to 'content\/'\")\n\n    \ndecoder('output.txt')\nfrom google.colab import files\nfiles.download('\/content\/output.mid')","552d3d2c":"#@title Plot, Graph, and Listen to the Output :)\ngraphs_length_inches = 18 #@param {type:\"slider\", min:0, max:20, step:1}\nnotes_graph_height = 6 #@param {type:\"slider\", min:0, max:20, step:1}\nhighest_displayed_pitch = 92 #@param {type:\"slider\", min:1, max:128, step:1}\nlowest_displayed_pitch = 24 #@param {type:\"slider\", min:1, max:128, step:1}\n\nimport librosa\nimport numpy as np\nimport pretty_midi\nimport pypianoroll\nfrom pypianoroll import Multitrack, Track\nimport matplotlib\nimport matplotlib.pyplot as plt\n#matplotlib.use('SVG')\n# For plotting\nimport mir_eval.display\nimport librosa.display\n%matplotlib inline\n\n\nmidi_data = pretty_midi.PrettyMIDI('\/content\/output.mid')\n\ndef plot_piano_roll(pm, start_pitch, end_pitch, fs=100):\n    # Use librosa's specshow function for displaying the piano roll\n    librosa.display.specshow(pm.get_piano_roll(fs)[start_pitch:end_pitch],\n                             hop_length=1, sr=fs, x_axis='time', y_axis='cqt_note',\n                             fmin=pretty_midi.note_number_to_hz(start_pitch))\n\n\n\nroll = np.zeros([int(graphs_length_inches), 128])\n# Plot the output\n\ntrack = Multitrack('\/content\/output.mid', name='track')\nplt.figure(figsize=[graphs_length_inches, notes_graph_height])\nfig, ax = track.plot()\nfig.set_size_inches(graphs_length_inches, notes_graph_height)\nplt.figure(figsize=[graphs_length_inches, notes_graph_height])\nax2 = plot_piano_roll(midi_data, int(lowest_displayed_pitch), int(highest_displayed_pitch))\nplt.show(block=False)\n\n\nFluidSynth(\"\/content\/font.sf2\", 16000).midi_to_audio('\/content\/output.mid', '\/content\/output.wav')\nAudio('\/content\/output.wav', rate=16000)","e34834c7":"from google.colab import drive\ndrive.mount('\/content\/drive')","8a7cb83f":"# Super Chamber Piano\n## A Mini Musical Neural Net","3e5566ff":"<a href=\"https:\/\/colab.research.google.com\/github\/asigalov61\/SuperPiano\/blob\/master\/Super_Chamber_Piano.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","7945fe8f":"# Model Specs and Default Parameters\n\n## 2 Layers LSTM\n\n### Hyperparameters\n\n1. sequence_len \/ n_word = 512 (previous: 128)\n2. batch_size \/ n_seq = 128\n3. hidden_dim = 512\n4. top_k words = 3 (previous: 5)\n5. predict seq_len = 512 (previous: 1024)\n6. epoch = 300\n\n***","1f4b4017":"### All thanks and credit for this beautiful colab go out to edtky of GitHub on whose repo and code it is based: https:\/\/github.com\/edtky\/mini-musical-neural-net\n\n---\n\n"}}