{"cell_type":{"ba9c78ab":"code","661e55ef":"code","d4af6dc1":"code","4823937a":"code","fa2104b1":"code","93b80a17":"code","05ecb949":"code","dd348bc0":"code","bdad3487":"code","741be051":"code","ee502a36":"code","d026ce00":"code","fd3d64af":"code","65f9842c":"code","154522f9":"code","b5a201d4":"code","4e48e195":"code","81e653ec":"code","c3971812":"code","319c9ef3":"code","6242dc38":"code","0b3281f8":"code","313e2b5e":"code","ca770fd6":"code","35c6c404":"code","65c34217":"code","e5e65995":"code","1156a9fd":"code","74f99b71":"code","61b98d35":"code","05718c35":"code","df1726fb":"code","f339c04b":"code","9aa5ee40":"code","98713c11":"code","d10251d1":"code","f4fea8bf":"code","d9693c6e":"code","c91f3918":"code","0dbeaed5":"code","1d15270b":"code","df51c442":"code","cac304de":"code","806a1869":"code","209a6b26":"code","02218f4d":"code","72a2a5d5":"code","045e4e06":"code","2b592a50":"code","8b122621":"code","55e8e49f":"code","bb17cb0d":"markdown","d2bd1122":"markdown","bd9dbb9f":"markdown","101ab8fd":"markdown","e9c13034":"markdown","19f88864":"markdown","e28b094e":"markdown","dd8b57d8":"markdown","3b64e12a":"markdown","4ce68b9a":"markdown","3df8f09b":"markdown","12a8f251":"markdown","657696f5":"markdown","032cdba8":"markdown","c8faeaab":"markdown","51f9f881":"markdown","a7f5f44f":"markdown","d6c1c444":"markdown","9b39b553":"markdown","1d959270":"markdown","0f20af54":"markdown","e907f1f2":"markdown","875ace1a":"markdown","f2705afa":"markdown","1ddb2250":"markdown","219d1f91":"markdown","d537a729":"markdown","1c08c089":"markdown","a1984f72":"markdown","9994fa00":"markdown","a2f41e29":"markdown","c9b53988":"markdown","15156fd1":"markdown","edd78905":"markdown","144c7d5b":"markdown","6e97a647":"markdown","cb9fe271":"markdown","3d011959":"markdown","a6694dc4":"markdown"},"source":{"ba9c78ab":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport csv\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport os\nimport tempfile\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n\nfrom imblearn.over_sampling import RandomOverSampler,SMOTE\nfrom imblearn.under_sampling  import RandomUnderSampler\n\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go","661e55ef":"mpl.rcParams['figure.figsize'] = (12, 10)\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']","d4af6dc1":"df_train=pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/train.csv')\ndf_test=pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/test.csv')\ndf_train.head()  ","4823937a":"df_train.isnull().sum()","fa2104b1":"neg, pos = np.bincount(df_train['Response'])\nfig = make_subplots(rows=1, cols=2)\n\ntraces = [\n    go.Bar(\n        x=['Yes', 'No'], \n        y=[\n            len(df_train[df_train['Response']==1]),\n            len(df_train[df_train['Response']==0])\n        ], \n        name='Train Response'\n    ),\n]\n\n\nfor i in range(len(traces)):\n    fig.append_trace(traces[i], (i \/\/ 2) + 1, (i % 2)  +1)\n\nfig.update_layout(\n    title_text='Train Response distribution',\n    height=400,\n    width=400\n)\nfig.show()\n\n","93b80a17":"\ndf_train.loc[df_train['Gender'] == 'Male', 'Gender'] = 1\ndf_train.loc[df_train['Gender'] == 'Female', 'Gender'] = 2\ndf_train['Gender'] = df_train['Gender'].astype(int)\ndf_test.loc[df_test['Gender'] == 'Male', 'Gender'] = 1\ndf_test.loc[df_test['Gender'] == 'Female', 'Gender'] = 2\ndf_test['Gender'] = df_test['Gender'].astype(int)\n\n\ndf_train.loc[df_train['Vehicle_Age'] == '> 2 Years', 'Vehicle_Age'] = 2\ndf_train.loc[df_train['Vehicle_Age'] == '1-2 Year', 'Vehicle_Age'] = 1\ndf_train.loc[df_train['Vehicle_Age'] == '< 1 Year', 'Vehicle_Age'] = 0\ndf_train['Vehicle_Age'] = df_train['Vehicle_Age'].astype(int)\ndf_test.loc[df_test['Vehicle_Age'] == '> 2 Years', 'Vehicle_Age'] = 2\ndf_test.loc[df_test['Vehicle_Age'] == '1-2 Year', 'Vehicle_Age'] = 1\ndf_test.loc[df_test['Vehicle_Age'] == '< 1 Year', 'Vehicle_Age'] = 0\ndf_test['Vehicle_Age'] = df_test['Vehicle_Age'].astype(int)\n\n\ndf_train.loc[df_train['Vehicle_Damage'] == 'Yes', 'Vehicle_Damage'] = 1\ndf_train.loc[df_train['Vehicle_Damage'] == 'No', 'Vehicle_Damage'] = 0\ndf_train['Vehicle_Damage'] = df_train['Vehicle_Damage'].astype(int)\ndf_test.loc[df_test['Vehicle_Damage'] == 'Yes', 'Vehicle_Damage'] = 1\ndf_test.loc[df_test['Vehicle_Damage'] == 'No', 'Vehicle_Damage'] = 0\ndf_test['Vehicle_Damage'] = df_test['Vehicle_Damage'].astype(int)\n","05ecb949":"df_train.head()","dd348bc0":"f = plt.figure(figsize=(11, 13))\nplt.matshow(df_train.corr(), fignum=f.number)\nplt.xticks(range(df_train.shape[1]), df_train.columns, fontsize=14, rotation=75)\nplt.yticks(range(df_train.shape[1]), df_train.columns, fontsize=14)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)","bdad3487":"fig = make_subplots(rows=1, cols=2)\n\ntraces = [\n    go.Bar(\n        x=['Male', 'Female'], \n        y=[\n            len(df_train[df_train['Gender']==1]),\n            len(df_train[df_train['Gender']==2])\n        ], \n        name='Train Gender',\n        text = [\n            str(round(100 * len(df_train[df_train['Gender']==1]) \/ len(df_train), 2)) + '%',\n            str(round(100 * len(df_train[df_train['Gender']==2]) \/ len(df_train), 2)) + '%'\n        ],\n        textposition='auto'\n    ),\n    go.Bar(\n        x=['Male', 'Female'], \n        y=[\n            len(df_test[df_test['Gender']==1]),\n            len(df_test[df_test['Gender']==2])\n        ], \n        name='Test Gender',\n        text=[\n            str(round(100 * len(df_test[df_test['Gender']==1]) \/ len(df_test), 2)) + '%',\n            str(round(100 * len(df_test[df_test['Gender']==2]) \/ len(df_test), 2)) + '%'\n        ],\n        textposition='auto'\n    ),\n\n]\n\nfor i in range(len(traces)):\n    fig.append_trace(traces[i], (i \/\/ 2) + 1, (i % 2)  +1)\n\nfig.update_layout(\n    title_text='Train\/test gender column',\n    height=400,\n    width=700\n)\nfig.show()","741be051":"fig = make_subplots(rows=1, cols=2)\n\ntraces = [\n    go.Bar(\n        x=['Yes', 'No'], \n        y=[\n            len(df_train[df_train['Driving_License']==1]),\n            len(df_train[df_train['Driving_License']==0])\n        ], \n        name='Train Driving_License',\n        text = [\n            str(round(100 * len(df_train[df_train['Driving_License']==1]) \/ len(df_train), 2)) + '%',\n            str(round(100 * len(df_train[df_train['Driving_License']==0]) \/ len(df_train), 2)) + '%'\n        ],\n        textposition='auto'\n    ),\n    go.Bar(\n        x=['Yes', 'No'], \n        y=[\n            len(df_test[df_test['Driving_License']==1]),\n            len(df_test[df_test['Driving_License']==0])\n        ], \n        name='Test Driving_License',\n        text=[\n            str(round(100 * len(df_test[df_test['Driving_License']==1]) \/ len(df_test), 2)) + '%',\n            str(round(100 * len(df_test[df_test['Driving_License']==0]) \/ len(df_test), 2)) + '%'\n        ],\n        textposition='auto'\n    ),\n\n]\n\nfor i in range(len(traces)):\n    fig.append_trace(traces[i], (i \/\/ 2) + 1, (i % 2)  +1)\n\nfig.update_layout(\n    title_text='Train\/test Driving_License column',\n    title_x=0.5,\n    height=400,\n    width=700\n)\nfig.show()","ee502a36":"fig = make_subplots(rows=1, cols=2)\n\ntraces = [\n    go.Bar(\n        x=['Yes', 'No'], \n        y=[\n            len(df_train[df_train['Previously_Insured']==1]),\n            len(df_train[df_train['Previously_Insured']==0])\n        ], \n        name='Train Previously_Insured',\n        text = [\n            str(round(100 * len(df_train[df_train['Previously_Insured']==1]) \/ len(df_train), 2)) + '%',\n            str(round(100 * len(df_train[df_train['Previously_Insured']==0]) \/ len(df_train), 2)) + '%'\n        ],\n        textposition='auto'\n    ),\n    go.Bar(\n        x=['Yes', 'No'], \n        y=[\n            len(df_test[df_test['Previously_Insured']==1]),\n            len(df_test[df_test['Previously_Insured']==0])\n        ], \n        name='Test Previously_Insured',\n        text = [\n            str(round(100 * len(df_test[df_test['Previously_Insured']==1]) \/ len(df_test), 2)) + '%',\n            str(round(100 * len(df_test[df_test['Previously_Insured']==0]) \/ len(df_test), 2)) + '%'\n        ],\n        textposition='auto'\n    ),\n\n]\n\nfor i in range(len(traces)):\n    fig.append_trace(traces[i], 1, (i % 2)  +1)\n\nfig.update_layout(\n    title_text='Train\/test Previously_Insured column',\n    title_x=0.5,\n    height=400,\n    width=700\n)\nfig.show()","d026ce00":"fig = make_subplots(rows=1, cols=2)\n\ntraces = [\n    go.Bar(\n        x=['Yes', 'No'], \n        y=[\n            len(df_train[df_train['Vehicle_Damage']==1]),\n            len(df_train[df_train['Vehicle_Damage']==0])\n        ], \n        name='Train Vehicle_Damage',\n        text = [\n            str(round(100 * len(df_train[df_train['Vehicle_Damage']==1]) \/ len(df_train), 2)) + '%',\n            str(round(100 * len(df_train[df_train['Vehicle_Damage']==0]) \/ len(df_train), 2)) + '%'\n        ],\n        textposition='auto'\n    ),\n    go.Bar(\n        x=['Yes', 'No'], \n        y=[\n            len(df_test[df_test['Vehicle_Damage']==1]),\n            len(df_test[df_test['Vehicle_Damage']==0])\n        ], \n        name='Test Vehicle_Damage',\n        text = [\n            str(round(100 * len(df_test[df_test['Vehicle_Damage']==1]) \/ len(df_test), 2)) + '%',\n            str(round(100 * len(df_test[df_test['Vehicle_Damage']==0]) \/ len(df_test), 2)) + '%'\n        ],\n        textposition='auto'\n    ),\n\n]\n\nfor i in range(len(traces)):\n    fig.append_trace(traces[i], 1, (i % 2)  +1)\n\nfig.update_layout(\n    title_text='Train\/test Vehicle_Damage column',\n    title_x=0.5,\n    height=400,\n    width=700\n)\nfig.show()","fd3d64af":"fig = make_subplots(rows=1, cols=2)\n\ntraces = [\n    go.Bar(\n        x=['> 2 Years', '1-2 Year', '< 1 Year'], \n        y=[\n            len(df_train[df_train['Vehicle_Age']==2]),\n            len(df_train[df_train['Vehicle_Age']==1]),\n            len(df_train[df_train['Vehicle_Age']==0])\n        ], \n        name='Train Vehicle_Age',\n        text = [\n            str(round(100 * len(df_train[df_train['Vehicle_Age']==2]) \/ len(df_train), 2)) + '%',\n            str(round(100 * len(df_train[df_train['Vehicle_Age']==1]) \/ len(df_train), 2)) + '%',\n            str(round(100 * len(df_train[df_train['Vehicle_Age']==0]) \/ len(df_train), 2)) + '%'\n        ],\n        textposition='auto'\n    ),\n    go.Bar(\n        x=['> 2 Years', '1-2 Year', '< 1 Year'], \n        y=[\n            len(df_test[df_test['Vehicle_Age']==2]),\n            len(df_test[df_test['Vehicle_Age']==1]),\n            len(df_test[df_test['Vehicle_Age']==0])\n        ], \n        name='Test Vehicle_Age',\n        text = [\n            str(round(100 * len(df_test[df_test['Vehicle_Age']==2]) \/ len(df_test), 2)) + '%',\n            str(round(100 * len(df_test[df_test['Vehicle_Age']==1]) \/ len(df_test), 2)) + '%',\n            str(round(100 * len(df_test[df_test['Vehicle_Age']==0]) \/ len(df_test), 2)) + '%'\n        ],\n        textposition='auto'\n    ),\n\n]\n\nfor i in range(len(traces)):\n    fig.append_trace(traces[i], 1, (i % 2)  +1)\n\nfig.update_layout(\n    title_text='Train\/test Vehicle_Age column',\n    title_x=0.5,\n    height=400,\n    width=700\n)\nfig.show()","65f9842c":"fig = make_subplots(rows=1, cols=2)\n\ntraces = [\n    go.Histogram(\n        x=df_train['Age'], \n        name='Train Age'\n    ),\n    go.Histogram(\n        x=df_test['Age'], \n        name='Test Age'\n    ),\n\n]\n\nfor i in range(len(traces)):\n    fig.append_trace(traces[i], (i \/\/ 2) + 1, (i % 2)  +1)\n\nfig.update_layout(\n    title_text='Train\/test Age column distribution',\n    title_x=0.5,\n    height=500,\n    width=900\n)\nfig.show()","154522f9":"train_arr=df_train.values.tolist()\ndata=[x[:-1] for x in train_arr]\nresponse=[x[-1] for x in train_arr]\ndata = np.array(data, dtype='float')\nresponse = np.array(response, dtype='float')\n","b5a201d4":"# split into 40% for training and 60% for testing\ndata_training, data_testing, response_training, response_testing = train_test_split(data, response, test_size=0.4, random_state=42)\nbool_response_training = response_training != 0\n","4e48e195":"scaler = StandardScaler()\ndata_training = scaler.fit_transform(data_training)\ndata_testing = scaler.transform(data_testing)\n\ndata_training = np.clip(data_training, -5, 5)\ndata_testing = np.clip(data_testing, -5, 5)\n\n\nprint('Training labels shape:', response_training.shape)\nprint('Test labels shape:', response_testing.shape)\n\nprint('Training features shape:', data_training.shape)\nprint('Test features shape:', data_testing.shape)","81e653ec":"METRICS = [\n      tf.keras.metrics.TruePositives(name='tp'),\n      tf.keras.metrics.FalsePositives(name='fp'),\n      tf.keras.metrics.TrueNegatives(name='tn'),\n      tf.keras.metrics.FalseNegatives(name='fn'), \n      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n      tf.keras.metrics.Precision(name='precision'),\n      tf.keras.metrics.Recall(name='recall'),\n      tf.keras.metrics.AUC(name='auc'),\n]\n\ndef make_model(metrics = METRICS, output_bias=None):\n  if output_bias is not None:\n    output_bias = tf.keras.initializers.Constant(output_bias)\n  model = tf.keras.Sequential([\n      tf.keras.layers.Dense(16, activation='relu'),\n      tf.keras.layers.Dense(1, activation='sigmoid',\n                         bias_initializer=output_bias),\n  ])\n\n  model.compile(\n      optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n      loss=tf.keras.losses.BinaryCrossentropy(),\n      metrics=metrics)\n\n  return model\n","c3971812":"EPOCHS = 100\nBATCH_SIZE = 2000\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_auc', \n    verbose=1,\n    patience=10,\n    mode='max',\n    restore_best_weights=True)","319c9ef3":"model = make_model(output_bias  = np.log([pos\/neg]))","6242dc38":"model.predict(data_training[:10])","0b3281f8":"initial_weights = os.path.join(tempfile.mkdtemp(),'initial_weights')\nmodel.save_weights(initial_weights)","313e2b5e":"history = model.fit(\n    data_training,\n    response_training,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks = [early_stopping],\n    validation_data=(data_testing, response_testing))","ca770fd6":"def plot_metrics(history):\n  metrics =  ['loss', 'auc', 'precision', 'recall']\n  for n, metric in enumerate(metrics):\n    name = metric.replace(\"_\",\" \").capitalize()\n    plt.subplot(2,2,n+1)\n    plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n    plt.plot(history.epoch, history.history['val_'+metric],\n             color=colors[0], linestyle=\"--\", label='Val')\n    plt.xlabel('Epoch')\n    plt.ylabel(name)\n    if metric == 'loss':\n      plt.ylim([0, plt.ylim()[1]])\n    elif metric == 'auc':\n      plt.ylim([0.8,1])\n    else:\n      plt.ylim([0,1])\n\n    plt.legend()\n","35c6c404":"plot_metrics(history)","65c34217":"predict_train = model.predict_classes(data_training)\npredict_test = model.predict_classes(data_testing)","e5e65995":"cm = confusion_matrix(response_testing, predict_test)\n\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax, fmt='g')\n\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\n\nunique, counts = np.unique(response_testing, return_counts=True)\nprint(dict(zip(unique, counts)))\n\nunique, counts = np.unique(predict_test, return_counts=True)\nprint(dict(zip(unique, counts)))\n","1156a9fd":"def plot_roc(name, labels, predictions, **kwargs):\n  fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n\n  plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n  plt.xlabel('False positives [%]')\n  plt.ylabel('True positives [%]')\n  plt.grid(True)\n  ax = plt.gca()\n  ax.set_aspect('equal')","74f99b71":"plot_roc(\"Train Baseline\", response_training, predict_train, color=colors[0])\nplot_roc(\"Test Baseline\", response_testing, predict_test, color=colors[0], linestyle='--')\nplt.legend(loc='lower right')","61b98d35":"# Scaling by total\/2 helps keep the loss to a similar magnitude.\n# The sum of the weights of all examples stays the same.\n\nweight_for_0 = (1 \/ neg)*(neg+pos)\/2.0 \nweight_for_1 = (1 \/ pos)*(neg+pos)\/2.0\n\nclass_weight = {0: weight_for_0, 1: weight_for_1}\n\nprint('Weight for class 0: {:.2f}'.format(weight_for_0))\nprint('Weight for class 1: {:.2f}'.format(weight_for_1))","05718c35":"weighted_model = make_model()\nweighted_model.load_weights(initial_weights)\n\nweighted_history = weighted_model.fit(\n    data_training,\n    response_training,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks = [early_stopping],\n    validation_data=(data_testing, response_testing),\n    # The class weights go here\n    class_weight=class_weight) ","df1726fb":"def plot_metrics(weighted_history):\n  metrics =  ['loss', 'auc', 'precision', 'recall']\n  for n, metric in enumerate(metrics):\n    name = metric.replace(\"_\",\" \").capitalize()\n    plt.subplot(2,2,n+1)\n    plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n    plt.plot(history.epoch, history.history['val_'+metric],\n             color=colors[0], linestyle=\"--\", label='Val')\n    plt.xlabel('Epoch')\n    plt.ylabel(name)\n    if metric == 'loss':\n      plt.ylim([0, plt.ylim()[1]])\n    elif metric == 'auc':\n      plt.ylim([0.8,1])\n    else:\n      plt.ylim([0,1])\n\n    plt.legend()","f339c04b":"plot_metrics(weighted_history)","9aa5ee40":"predict_weighted_train = weighted_model.predict_classes(data_training)\npredict_weighted_test = weighted_model.predict_classes(data_testing)","98713c11":"cm = confusion_matrix(response_testing, predict_weighted_test)\n\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax, fmt='g')\n\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\n\nunique, counts = np.unique(response_testing, return_counts=True)\nprint(dict(zip(unique, counts)))\n\nunique, counts = np.unique(predict_weighted_test, return_counts=True)\nprint(dict(zip(unique, counts)))","d10251d1":"plot_roc(\"Train Baseline\", response_training, predict_train, color=colors[0])\nplot_roc(\"Test Baseline\", response_testing, predict_test, color=colors[0], linestyle='--')\n\nplot_roc(\"Train Weighted\", response_training, predict_weighted_train, color=colors[1])\nplot_roc(\"Test Weighted\", response_testing, predict_weighted_test, color=colors[1], linestyle='--')\n\n\nplt.legend(loc='lower right')","f4fea8bf":"pos_features = data_training[bool_response_training]\nneg_features = data_training[~bool_response_training]\n\npos_labels = response_training[bool_response_training]\nneg_labels = response_training[~bool_response_training]","d9693c6e":"ids = np.arange(len(pos_features))\nchoices = np.random.choice(ids, len(neg_features))\n\nres_pos_features = pos_features[choices]\nres_pos_labels = pos_labels[choices]\n\nprint(res_pos_features.shape)\nprint(pos_features.shape)","c91f3918":"resampled_features = np.concatenate([res_pos_features, neg_features], axis=0)\nresampled_labels = np.concatenate([res_pos_labels, neg_labels], axis=0)\n\norder = np.arange(len(resampled_labels))\nnp.random.shuffle(order)\nresampled_features = resampled_features[order]\nresampled_labels = resampled_labels[order]\n\nresampled_features.shape","0dbeaed5":"BUFFER_SIZE = 100000\n\ndef make_ds(features, labels):\n  ds = tf.data.Dataset.from_tensor_slices((features, labels))#.cache()\n  ds = ds.shuffle(BUFFER_SIZE).repeat()\n  return ds\n\npos_ds = make_ds(pos_features, pos_labels)\nneg_ds = make_ds(neg_features, neg_labels)","1d15270b":"for features, label in pos_ds.take(1):\n  print(\"Features:\\n\", features.numpy())\n  print()\n  print(\"Label: \", label.numpy())","df51c442":"resampled_ds = tf.data.experimental.sample_from_datasets([pos_ds, neg_ds], weights=[0.5, 0.5])\nresampled_ds = resampled_ds.batch(BATCH_SIZE).prefetch(2)","cac304de":"for features, label in resampled_ds.take(1):\n  print(label.numpy().mean())","806a1869":"resampled_steps_per_epoch = np.ceil(2.0*neg\/BATCH_SIZE)\nresampled_steps_per_epoch","209a6b26":"resampled_model = make_model()\nresampled_model.load_weights(initial_weights)\n# Reset the bias to zero, since this dataset is balanced.\nresampled_model = make_model(output_bias  = 0)\n\noutput_layer = resampled_model.layers[-1] \n\nval_ds = tf.data.Dataset.from_tensor_slices((data_testing, response_testing)).cache()\nval_ds = val_ds.batch(BATCH_SIZE).prefetch(2) \n\nresampled_history = resampled_model.fit(\n    resampled_ds,\n    epochs=EPOCHS,\n    steps_per_epoch=resampled_steps_per_epoch,\n    callbacks = [early_stopping],\n    validation_data=val_ds)","02218f4d":"plot_metrics(resampled_history )","72a2a5d5":"train_predictions_resampled = resampled_model.predict(data_training, batch_size=BATCH_SIZE)\ntest_predictions_resampled = resampled_model.predict(data_testing, batch_size=BATCH_SIZE)","045e4e06":"plot_roc(\"Train Baseline\", response_training, predict_train, color=colors[0])\nplot_roc(\"Test Baseline\", response_testing, predict_test, color=colors[0], linestyle='--')\n\nplot_roc(\"Train Weighted\", response_training, predict_weighted_train, color=colors[1])\nplot_roc(\"Test Weighted\", response_testing, predict_weighted_test, color=colors[1], linestyle='--')\n\nplot_roc(\"Train Resampled\", response_training, train_predictions_resampled,  color=colors[2])\nplot_roc(\"Test Resampled\", response_testing, test_predictions_resampled,  color=colors[2], linestyle='--')\nplt.legend(loc='lower right')","2b592a50":"data_test=df_test.values.tolist()\ndata_test = np.array(data_test, dtype='float')","8b122621":"prediction = resampled_model.predict_classes(data_test)\nid=[]\nfor i in data_test:\n    id.append(i[0])\n\nid=np.array(id, dtype='int')\nresult = prediction[:, 0]\ncombined=np.vstack((id, result)).T","55e8e49f":"pd.DataFrame(combined).to_csv('Submission .csv')","bb17cb0d":"First, we need to convert our text data ","d2bd1122":"<a id=\"2_1\"><\/a>\n<h1 style='border:0; color:black'>2.1. Setup and examine the train dataset<h1>","bd9dbb9f":"Using tf.data\nIf you're using tf.data the easiest way to produce balanced examples is to start with a positive and a negative dataset, and merge them. See the tf.data guide for more examples.","101ab8fd":"Oversample the minority class\nA related approach would be to resample the dataset by oversampling the minority class.","e9c13034":"The correct bias to set can be derived from:\n\n$$ p_0 = pos\/(pos + neg) = 1\/(1+e^{-b_0}) $$\n$$ b_0 = -log_e(1\/p_0 - 1) $$\n$$ b_0 = log_e(pos\/neg)$$","19f88864":"Calculate class weights\n\nThe goal is to identify the customers interested in auto insurance, but you don't have very many of those positive samples to work with, so you would want to have the classifier heavily weight the few examples that are available. You can do this by passing Keras weights for each class through a parameter. These will cause the model to \"pay more attention\" to examples from an under-represented class.","e28b094e":"<a id=\"2_3\"><\/a>\n<h1 style='border:0; color:black'>2.3. Compare training and testing sets<h1>","dd8b57d8":"# Plot the ROC","3b64e12a":"<a id=\"4\"><\/a>\n# Output","4ce68b9a":"If the model had predicted everything perfectly, this would be a diagonal matrix where values off the main diagonal, indicating incorrect predictions, would be zero. In this case, we can accept few false positives (89 cases in the matrix) meaning that we might ask a customer to subscribe to auto insurance even if he doesn't accept. \nHowever, we need to decrease as much as possible the false negative responses (18 783 cases) because they refer to the customers that want an auto subscription while the model flagged them as customers that don't want the auto insurance.","3df8f09b":"<a id=\"3_4\"><\/a>\n# Oversampling","12a8f251":"![](http:\/\/)<a id=\"2\"><\/a>\n<h2>Data processing and exploration<\/center><h2>","657696f5":"[](http:\/\/)![](http:\/\/)We download the train and test datasets :","032cdba8":"# Setup\n \nWe  will first import the libraries needed.\n\n*RandomOverSampler and SMOTE(Synthetic Minority Oversampling Technique) are used to treat imbalanced datasets (which is the case here as we can see in the first figure).*\n\n*RandomOverSampler duplicates the minority class data until minority class data reaches specified proportion of majority class data.*\n\n*SMOTE generates synthetic data of minority classes and ensures that the data doesn't overfit*","c8faeaab":"**There's no missing Data.**","51f9f881":"<a id=\"1\"><\/a>\n<h2> Introduction<h2>","a7f5f44f":"Check training history :","d6c1c444":"Merge the two together using experimental.sample_from_datasets:","9b39b553":"# Task Details\n****\nYour client is an Insurance company that has provided Health Insurance to its customers now they need your help in building a model to predict whether the policyholders (customers) from past year will also be interested in Vehicle Insurance provided by the company.\n\nFor example, you may pay a premium of Rs. 5000 each year for a health insurance cover of Rs. 200,000\/- so that if, God forbid, you fall ill and need to be hospitalised in that year, the insurance provider company will bear the cost of hospitalisation etc. for upto Rs. 200,000. Now if you are wondering how can company bear such high hospitalisation cost when it charges a premium of only Rs. 5000\/-, that is where the concept of probabilities comes in picture. For example, like you, there may be 100 customers who would be paying a premium of Rs. 5000 every year, but only a few of them (say 2-3) would get hospitalised that year and not everyone. This way everyone shares the risk of everyone else.\n\nJust like medical insurance, there is vehicle insurance where every year customer needs to pay a premium of certain amount to insurance provider company so that in case of unfortunate accident by the vehicle, the insurance provider company will provide a compensation (called \u2018sum assured\u2019) to the customer.\n\nBuilding a model to predict whether a customer would be interested in Vehicle Insurance is extremely helpful for the company because it can then accordingly plan its communication strategy to reach out to those customers and optimise its business model and revenue.\n\nNow, in order to predict, whether the customer would be interested in Vehicle insurance, you have information about demographics (gender, age, region code type), Vehicles (Vehicle Age, Damage), Policy (Premium, sourcing channel) etc.\n\n# Evaluation Metric\n****\nThe evaluation metric for this hackathon is ROC_AUC score.","1d959270":"Normalize the input features using the sklearn StandardScaler. This will set the mean to 0 and standard deviation to 1.\n\nNote: The StandardScaler is only fit using the train_features to be sure the model is not peeking at the validation or test sets.","0f20af54":"<IMG align=\"center\" src=\"https:\/\/www.netclipart.com\/pp\/m\/84-843864_traffic-clipart-car-collision-car-accident-cartoon-insurance.png\" alt=\"car accident img\">\n","e907f1f2":"Now plot the ROC. This plot is useful because it shows, at a glance, the range of performance the model can reach just by tuning the output threshold.","875ace1a":"<a id=\"3_3\"><\/a>\n# Class weights","f2705afa":"Train on the oversampled data\nNow try training the model with the resampled data set instead of using class weights to see how these methods compare.","1ddb2250":"### Understanding useful metrics\n\nNotice that there are a few metrics defined above that can be computed by the model that will be helpful when evaluating the performance.\n\n\n\n*   **False** negatives and **false** positives are samples that were **incorrectly** classified\n*   **True** negatives and **true** positives are samples that were **correctly** classified\n*   **Accuracy** is the percentage of examples correctly classified\n>   $\\frac{\\text{true samples}}{\\text{total samples}}$\n*   **Precision** is the percentage of **predicted** positives that were correctly classified\n>   $\\frac{\\text{true positives}}{\\text{true positives + false positives}}$\n*   **Recall** is the percentage of **actual** positives that were correctly classified\n>   $\\frac{\\text{true positives}}{\\text{true positives + false negatives}}$\n*   **AUC** refers to the Area Under the Curve of a Receiver Operating Characteristic curve (ROC-AUC). This metric is equal to the probability that a classifier will rank a random positive sample higher than a random negative sample.\n\nNote: Accuracy is not a helpful metric for this task. You can 99.8%+ accuracy on this task by predicting False all the time.  \n\nRead more:\n*  [True vs. False and Positive vs. Negative](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/true-false-positive-negative)\n*  [Accuracy](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/accuracy)\n*   [Precision and Recall](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/precision-and-recall)\n*   [ROC-AUC](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/roc-and-auc)","219d1f91":"Using NumPy\nYou can balance the dataset manually by choosing the right number of random indices from the positive examples:","d537a729":"Checkpoint the initial weights\n\nTo make the various training runs more comparable, keep this initial model's weights in a checkpoint file, and load them into each model before training.","1c08c089":"<a id=\"3_1\"><\/a>\n\n# Define the model and the metrics","a1984f72":"The dataset is imbalanced. Set the output layer's bias to reflect that (See: [A Recipe for Training Neural Networks: \"init well\"](http:\/\/karpathy.github.io\/2019\/04\/25\/recipe\/#2-set-up-the-end-to-end-trainingevaluation-skeleton--get-dumb-baselines)). This can help with initial convergence.","9994fa00":"We now check if theres is some data missing :","a2f41e29":"# Examine the class label imbalance","c9b53988":"\n\n# Context\n\n****\nOur client is an Insurance company that has provided Health Insurance to its customers now they need your help in building a model to predict whether the policyholders (cust\nomers) from past year will also be interested in Vehicle Insurance provided by the company.\n\nAn insurance policy is an arrangement by which a company undertakes to provide a guarantee of compensation for specified loss, damage, illness, or death in return for the payment of a specified premium. A premium is a sum of money that the customer needs to pay regularly to an insurance company for this guarantee.\n\nFor example, you may pay a premium of Rs. 5000 each year for a health insurance cover of Rs. 200,000\/- so that if, God forbid, you fall ill and need to be hospitalised in that year, the insurance provider company will bear the cost of hospitalisation etc. for upto Rs. 200,000. Now if you are wondering how can company bear such high hospitalisation cost when it charges a premium of only Rs. 5000\/-, that is where the concept of probabilities comes in picture. For example, like you, there may be 100 customers who would be paying a premium of Rs. 5000 every year, but only a few of them (say 2-3) would get hospitalised that year and not everyone. This way everyone shares the risk of everyone else.\n\nJust like medical insurance, there is vehicle insurance where every year customer needs to pay a premium of certain amount to insurance provider company so that in case of unfortunate accident by the vehicle, the insurance provider company will provide a compensation (called \u2018sum assured\u2019) to the customer.\n\nBuilding a model to predict whether a customer would be interested in Vehicle Insurance is extremely helpful for the company because it can then accordingly plan its communication strategy to reach out to those customers and optimise its business model and revenue.\n\nNow, in order to predict, whether the customer would be interested in Vehicle insurance, you have information about demographics (gender, age, region code type), Vehicles (Vehicle Age, Damage), Policy (Premium, sourcing channel) etc.","15156fd1":"<a id=\"3\"><\/a>\n<h2>3. Modeling<h2>","edd78905":"The figure below shows the correlation between different labels :","144c7d5b":"<a id=\"2_2\"><\/a>\n<h1 style='border:0; color:black'>2.2. Training data and response correlation<h1>","6e97a647":"To use this dataset, you'll need the number of steps per epoch.\n\nThe definition of \"epoch\" in this case is less clear. Say it's the number of batches required to see each negative example once:","cb9fe271":"[](http:\/\/)<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2>Quick navigation <\/h2>\n\n[1. Introduction](#1)\n    \n[2. Data processing and exploration](#2) \n    \n&nbsp;&nbsp;&nbsp;&nbsp;[2.1. Setup and examine the train dataset](#2_1)    \n&nbsp;&nbsp;&nbsp;&nbsp;[2.2. Training data and response correlation](#2_2)   \n&nbsp;&nbsp;&nbsp;&nbsp;[2.3. Compare training and testing sets](#2_3)   \n    \n[3. Modeling (In progress)](#3)\n    \n&nbsp;&nbsp;&nbsp;&nbsp;[3.1. Define the model and the metrics](#3_1)    \n&nbsp;&nbsp;&nbsp;&nbsp;[3.2. Set the correct initial bias](#3_2)   \n&nbsp;&nbsp;&nbsp;&nbsp;[3.3. Class weight](#3_3)  \n&nbsp;&nbsp;&nbsp;&nbsp;[3.4. Over sampling](#3_4)      \n    \n[4. Output](#4)    ","3d011959":"<a id=\"3_2\"><\/a>\n\n# Set the correct initial bias","a6694dc4":"the datasets is quiet imbalanced."}}