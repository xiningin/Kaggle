{"cell_type":{"5638e2c1":"code","41fde739":"code","dfc2b7ad":"code","bb8fcdc1":"code","47cbfad6":"code","2a3d098d":"code","83d7f8a6":"code","5182802c":"code","9dbd3d09":"code","559c1e29":"code","4060c0cf":"code","7cb9b9d5":"code","d02a48c6":"code","c897b251":"code","efc291e2":"code","98c9560c":"code","065d6731":"code","665042ab":"code","1d435ba5":"code","a3dca746":"code","110c3250":"code","fc5da2c0":"code","a8e096b7":"code","821c483a":"code","b19cb609":"code","1bce89dd":"code","fe7c8a5f":"markdown","ce394988":"markdown","f6fcbc3d":"markdown","7478a8ed":"markdown","ef30c4a9":"markdown","8fd15065":"markdown","881c4913":"markdown","eca3780d":"markdown"},"source":{"5638e2c1":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","41fde739":"df = pd.read_csv('..\/input\/credit-card-customers\/BankChurners.csv')\ndf.drop(df.columns[[21, 22]], axis = 1, inplace = True) # drop naives_bayes columns\ndf.shape","dfc2b7ad":"df.describe()","bb8fcdc1":"# Less text on income bins\ndf['Income_Category'] = df['Income_Category'].replace('Less than $40K', '< 40K')\ndf['Income_Category'] = df['Income_Category'].replace('$40K - $60K', '40K - 60K')\ndf['Income_Category'] = df['Income_Category'].replace('$60K - $80K', '60K - 80K')\ndf['Income_Category'] = df['Income_Category'].replace('$80K - $120K', '80K - 120K')\ndf['Income_Category'] = df['Income_Category'].replace('$120K +', '>120K')\n\n# Setup target variable\ndf['churn'] = df['Attrition_Flag'].replace('Existing Customer',0).replace('Attrited Customer',1)\ndf.drop('Attrition_Flag',axis=1,inplace=True)\n\n# Check for Nulls\ndf.isna().any()","47cbfad6":"df.info()\n","2a3d098d":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\n# create a label encoder for columns with <2 unique values\nle = LabelEncoder()\ncount = 0\nfor col in df.columns[1:]:\n    if df[col].dtype == 'object':\n        if len(list(df[col].unique())) <= 2:\n            le.fit(df[col])\n            df[col] = le.transform(df[col])","83d7f8a6":"EDA_df = df[['Customer_Age',\n             'Gender',\n             'Dependent_count',\n             'Months_on_book',\n             'Total_Revolving_Bal',\n             'Total_Amt_Chng_Q4_Q1',\n             'Avg_Utilization_Ratio',\n             'Contacts_Count_12_mon',\n            'Total_Relationship_Count']]\n\n    \nfig = plt.figure(figsize=(10, 10))\nplt.suptitle('Histograms of Numerical Columns\\n',\n             horizontalalignment=\"center\",\n             fontstyle = \"normal\",\n             fontsize = 24,\n             fontfamily = \"sans-serif\")\n\nfor i in range(EDA_df.shape[1]):\n    plt.subplot(3, 3, i + 1)\n    f = plt.gca()\n    f.set_title(EDA_df.columns.values[i])\n    vals = np.size(EDA_df.iloc[:, i].unique())\n    if vals >= 100:\n        vals = 100\n    plt.hist(EDA_df.iloc[:, i], \n         bins=vals,\n         color = '#AEC3B0')\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])","5182802c":"categories = ['Education_Level',\n              'Marital_Status',\n              'Income_Category',\n              'Card_Category']\n\nfig, ax = plt.subplots(4, figsize=(14, 7))\n\ni = 0\nfor cat in categories:\n    ax[i].hist(df[cat],color = '#AEC3B0')\n    ax[i].set_title(cat)\n    i += 1\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])","9dbd3d09":"# Visualise churn rate by categories\ncategories = ['Education_Level','Marital_Status','Income_Category','Card_Category']\n\nfor cat in categories:\n    temp_churn = df.groupby([cat,'churn']).size().unstack()\n    temp_churn.rename(columns={0:'No', 1:'Yes'}, inplace=True)\n    colors  = ['#598392','#AEC3B0']\n\n    ax = (temp_churn.T*100.0 \/ temp_churn.T.sum()).T.plot(\n        kind='bar',\n        figsize = (12,6),\n        width = 0.5,\n        stacked = True,\n        color = colors)\n\n    plt.ylabel('% of Customers')\n    plt.xlabel(cat)\n    plt.title(cat + ' Churn Rate')\n\n    plt.legend(loc='right', fontsize = \"medium\")\n    plt.xticks(rotation=0, horizontalalignment=\"center\")\n    plt.yticks(rotation=0, horizontalalignment=\"right\")\n    ax.yaxis.set_major_formatter(mtick.PercentFormatter())\n    for p in ax.patches:\n        width, height = p.get_width(), p.get_height()\n        x, y = p.get_xy() \n        ax.text(x+width\/2, \n                y+height\/2, \n                '{:.1f}%'.format(height), \n                horizontalalignment='center', \n                verticalalignment='center')\n    ax.autoscale(enable=False, axis='both', tight=False)","559c1e29":"# Create a bivariate correlation plot\n\nimport seaborn as sn\ncorr = df.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(18, 15))\ncmap = sn.diverging_palette(220, 10, as_cmap=True)\nsn.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","4060c0cf":"#from statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef calc_vif(X):\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) \n    for i in range(X.shape[1])]\n    return(vif)\n\n#dataset2 = df.drop(['Customer_Age','Education_Level','Marital_Status','Income_Category','Card_Category','CLIENTNUM','Avg_Open_To_Buy','Total_Revolving_Bal','Credit_Limit'],axis=1)\n#temp = calc_vif(dataset2)\n#temp = temp[ temp['VIF'] > 10]\n#temp.sort_values('VIF',ascending=False)","7cb9b9d5":"# One-hot encode the data\nID = df[\"CLIENTNUM\"] #extract as we do not want to encode this unique identifier\ndf = df.drop(columns=\"CLIENTNUM\")\ndf = pd.get_dummies(df)\ndf = pd.concat([df, ID], axis = 1)\n\n# Split for training and test data\nresponse = df[\"churn\"]\ndf = df.drop(columns=\"churn\")\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df, response, stratify=response, test_size = 0.2)","d02a48c6":"train_identity = X_train['CLIENTNUM']\nX_train = X_train.drop(columns = ['CLIENTNUM'])\ntest_identity = X_test['CLIENTNUM']\nX_test = X_test.drop(columns = ['CLIENTNUM'])","c897b251":"from sklearn.preprocessing import StandardScaler\n\nsc_X = StandardScaler()\nX_train2 = pd.DataFrame(sc_X.fit_transform(X_train))\nX_train2.columns = X_train.columns.values\nX_train2.index = X_train.index.values\nX_train = X_train2\nX_test2 = pd.DataFrame(sc_X.transform(X_test))\nX_test2.columns = X_test.columns.values\nX_test2.index = X_test.index.values\nX_test = X_test2","efc291e2":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import model_selection\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score","98c9560c":"models = []\nmodels.append(('Logistic Regression', LogisticRegression(solver='liblinear', random_state = 0,class_weight='balanced')))\nmodels.append(('SVC', SVC(kernel = 'linear', random_state = 0)))\nmodels.append(('Kernel SVM', SVC(kernel = 'rbf', random_state = 0)))\nmodels.append(('KNN', KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)))\nmodels.append(('Gaussian NB', GaussianNB()))\nmodels.append(('Decision Tree Classifier',DecisionTreeClassifier(criterion = 'entropy', random_state = 0)))\nmodels.append(('Random Forest', RandomForestClassifier(n_estimators=100, criterion = 'entropy', random_state = 0)))\n\nacc_results = []\nauc_results = []\nprecision = []\nrecall = []\nnames = []\ncol = ['Algorithm', 'ROC AUC Mean', 'ROC AUC STD', 'Accuracy Mean', 'Accuracy STD','Recall','Precision']\nmodel_results = pd.DataFrame(columns=col)\ni = 0\n# Evaluate each model using k-fold cross-validation:\nfor name, model in models:\n    kfold = model_selection.KFold(\n        n_splits=10)\n    # accuracy scoring:\n    cv_acc_results = model_selection.cross_val_score(  \n    model, X_train, y_train, cv=kfold, scoring='accuracy')\n    # roc_auc scoring:\n    cv_auc_results = model_selection.cross_val_score(  \n    model, X_train, y_train, cv=kfold, scoring='roc_auc')\n    # Precision & Recall\n    model.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    precision.append(precision_score(y_test, pred))\n    recall.append(recall_score(y_test, pred))\n    # Storing Results\n    acc_results.append(cv_acc_results)\n    auc_results.append(cv_auc_results)\n    names.append(name)\n    model_results.loc[i] = [name,\n                        round(cv_auc_results.mean()*100, 2),\n                        round(cv_auc_results.std()*100, 2),\n                        round(cv_acc_results.mean()*100, 2),\n                        round(cv_acc_results.std()*100, 2),\n                        round(recall[i]*100, 2),\n                        round(precision[i]*100, 2)]\n    i += 1","065d6731":"recall","665042ab":"from sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\n\nmodel = ExtraTreesClassifier()\nmodel.fit(X_train,y_train)\nprint(model.feature_importances_) \nplt.style.use('ggplot')\nplt.figure(figsize=(6,6))\nfeat_importances = pd.Series(model.feature_importances_, index=X_train.columns)\nfeat_importances.nlargest(50).plot(kind='barh')\nplt.show()","1d435ba5":"test = ['CLIENTNUM',\n        'Total_Trans_Ct',\n        'Total_Trans_Amt',\n        'Total_Ct_Chng_Q4_Q1',\n        'Total_Relationship_Count',\n       'Avg_Utilization_Ratio',\n       'Contacts_Count_12_mon',\n       'Total_Amt_Chng_Q4_Q1']","a3dca746":"# Split for training and test data\ndf = df[test]\n\nX_train, X_test, y_train, y_test = train_test_split(df, response, stratify=response, test_size = 0.2)\n\ntrain_identity = X_train['CLIENTNUM']\nX_train = X_train.drop(columns = ['CLIENTNUM'])\ntest_identity = X_test['CLIENTNUM']\nX_test = X_test.drop(columns = ['CLIENTNUM'])","110c3250":"from sklearn.preprocessing import StandardScaler\n\nsc_X = StandardScaler()\nX_train2 = pd.DataFrame(sc_X.fit_transform(X_train))\nX_train2.columns = X_train.columns.values\nX_train2.index = X_train.index.values\nX_train = X_train2\nX_test2 = pd.DataFrame(sc_X.transform(X_test))\nX_test2.columns = X_test.columns.values\nX_test2.index = X_test.index.values\nX_test = X_test2","fc5da2c0":"models = []\nmodels.append(('Logistic Regression', LogisticRegression(solver='liblinear', random_state = 0,class_weight='balanced')))\nmodels.append(('SVC', SVC(kernel = 'linear', random_state = 0)))\nmodels.append(('Kernel SVM', SVC(kernel = 'rbf', random_state = 0)))\nmodels.append(('KNN', KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)))\nmodels.append(('Gaussian NB', GaussianNB()))\nmodels.append(('Decision Tree Classifier',DecisionTreeClassifier(criterion = 'entropy', random_state = 0)))\nmodels.append(('Random Forest', RandomForestClassifier(n_estimators=100, criterion = 'entropy', random_state = 0)))\n\nacc_results = []\nauc_results = []\nprecision = []\nrecall = []\nnames = []\ncol = ['Algorithm', 'ROC AUC Mean', 'ROC AUC STD', 'Accuracy Mean', 'Accuracy STD','Recall','Precision']\nmodel_results2 = pd.DataFrame(columns=col)\ni = 0\n# Evaluate each model using k-fold cross-validation:\nfor name, model in models:\n    kfold = model_selection.KFold(\n        n_splits=10)\n    # accuracy scoring:\n    cv_acc_results = model_selection.cross_val_score(  \n    model, X_train, y_train, cv=kfold, scoring='accuracy')\n    # roc_auc scoring:\n    cv_auc_results = model_selection.cross_val_score(  \n    model, X_train, y_train, cv=kfold, scoring='roc_auc')\n    # Precision & Recall\n    model.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    precision.append(precision_score(y_test, pred))\n    recall.append(recall_score(y_test, pred))\n    # Storing Results\n    acc_results.append(cv_acc_results)\n    auc_results.append(cv_auc_results)\n    names.append(name)\n    model_results2.loc[i] = [name,\n                        round(cv_auc_results.mean()*100, 2),\n                        round(cv_auc_results.std()*100, 2),\n                        round(cv_acc_results.mean()*100, 2),\n                        round(cv_acc_results.std()*100, 2),\n                        round(recall[i]*100, 2),\n                        round(precision[i]*100, 2)]\n    i += 1","a8e096b7":"model_results.sort_values(by=['Recall'], ascending=False)","821c483a":"model_results2.sort_values(by=['Recall'], ascending=False)","b19cb609":"models = []\nmodels.append(('KNN_mink', KNeighborsClassifier(n_neighbors = 3, metric = 'minkowski', p = 2)))\nmodels.append(('KNN_cheb', KNeighborsClassifier(n_neighbors = 3, metric = 'chebyshev')))\nmodels.append(('KNN_cheb', KNeighborsClassifier(n_neighbors = 3, metric = 'manhattan')))\nmodels.append(('Logistic Regression', LogisticRegression(solver='lbfgs', penalty='l2',random_state = 0, class_weight='balanced')))\nmodels.append(('SVC', SVC(kernel = 'linear', random_state = 0)))\nmodels.append(('Kernel SVM', SVC(kernel = 'rbf', random_state = 0)))\nmodels.append(('Decision Tree Classifier',DecisionTreeClassifier(criterion = 'entropy', random_state = 0)))\nmodels.append(('Random Forest', RandomForestClassifier(n_estimators=1000, criterion = 'entropy', random_state = 0)))\n\nacc_results = []\nauc_results = []\nprecision = []\nrecall = []\nnames = []\ncol = ['Algorithm', 'ROC AUC Mean', 'ROC AUC STD', 'Accuracy Mean', 'Accuracy STD','Recall','Precision']\nmodel_results3 = pd.DataFrame(columns=col)\ni = 0\n# Evaluate each model using k-fold cross-validation:\nfor name, model in models:\n    kfold = model_selection.KFold(\n        n_splits=10)\n    # accuracy scoring:\n    cv_acc_results = model_selection.cross_val_score(  \n    model, X_train, y_train, cv=kfold, scoring='accuracy')\n    # roc_auc scoring:\n    cv_auc_results = model_selection.cross_val_score(  \n    model, X_train, y_train, cv=kfold, scoring='roc_auc')\n    # Precision & Recall\n    model.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    precision.append(precision_score(y_test, pred))\n    recall.append(recall_score(y_test, pred))\n    # Storing Results\n    acc_results.append(cv_acc_results)\n    auc_results.append(cv_auc_results)\n    names.append(name)\n    model_results3.loc[i] = [name,\n                        round(cv_auc_results.mean()*100, 2),\n                        round(cv_auc_results.std()*100, 2),\n                        round(cv_acc_results.mean()*100, 2),\n                        round(cv_acc_results.std()*100, 2),\n                        round(recall[i]*100, 2),\n                        round(precision[i]*100, 2)]\n    i += 1","1bce89dd":"model_results3.sort_values(by=['Recall'], ascending=False)","fe7c8a5f":"# Model Selection","ce394988":"#### After removing categorical variables, and variables which returned and \"infinite\" VIF, the values above a 10 (good threshold) drop considerably when you also remove Age. Total Transaction went from a 75 VIF to 25.","f6fcbc3d":"This data was pre-processed so a lot of the cleaning has already been done, but at this point you would do any cleaning (checking for nulls, data types, etc)","7478a8ed":"* The higher the customers education the more likely they are to churn\n* Married customers are less likely to churn\n* Those under 40K or over 120K have the highest chrun rate\n* Platinum card holders have a very high churn rate","ef30c4a9":"Still not sure why the education graph has oddly spaced data, if you know I'd like to hear how to fix it!\n\n* Most credit card holders are Graduates\n* Divorcees are least likely to own a credit card\n* The most credit cards belong to people with an income under $40,000\n* I'd imagine that Blue is the standard credit card category, in which case it doesn't appear that other categories are being utilised correctly. Maybe due to high turn-over at the 3 year mark?","8fd15065":"Using describe on the dataset we can see the average time on books is almost 3 years with a utilization ratio of 27%","881c4913":"# Results\nUsing the feature importance graph, I took a few of the best performing features (this could probably be extended) and scored the model using all features against the model with the highest performing features","eca3780d":"* We can see a spike in ages at ~25, presumably you cannot get a credit card with this bank before then or perhaps they don't record lower than that. There is a second spike at ~50.\n\n* For Gender, there is a fairly even distribution.\n\n* Dependents are mostly 2\/3 while the least are 0 or 5\/greater than 5. \n\n* Months on book has a fairly even distribtion but a huge spike at ~36. Presumably there might be some common contract for 3 years\n\n* Total revolving balance appears to be commonly low with a lot of customers at 0, and at 2500 (maybe a limit?), otherwise and even distribution.\n\n* Total amount changed between quarter 4 and q 1 is commonly between 0.5 and 1, with some outliers going past that\n\n* Average Utilization Ratio is (apart from the 0 counts) right-skewed with a subtle spike around 0.05\/0.1"}}