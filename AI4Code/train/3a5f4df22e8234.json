{"cell_type":{"77ad7f47":"code","e0e34283":"code","623e85c4":"code","6ca13030":"code","1058f3d1":"code","b336f4ac":"code","7a71cdc8":"code","b7130878":"code","ed01a9e5":"code","4405e037":"code","e9b74f4e":"code","a3caa6c3":"code","847a5b64":"code","09737a6f":"code","44cccbff":"code","2e9305a5":"code","bd0d676b":"code","fc571cbe":"code","3d497956":"code","2e95acc8":"code","fa774cf0":"code","35816fb7":"code","42ebf4ca":"code","f10fc516":"code","c8bc2329":"code","028f7b37":"code","4e6183ac":"code","57db389b":"code","052d27d1":"code","efae7d81":"code","0a1a6857":"code","991883ff":"markdown","a8006b50":"markdown","9f1b0c45":"markdown","eb27bb0c":"markdown","d6c60321":"markdown","97ecf4ae":"markdown"},"source":{"77ad7f47":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical","e0e34283":"import tensorflow as tf","623e85c4":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Activation, add\nfrom tensorflow.keras.utils import plot_model","6ca13030":"label_map = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\nnames=['emotion','pixels','usage']","1058f3d1":"df=pd.read_csv('..\/input\/facial-expression\/fer2013\/fer2013.csv',names=names, na_filter=False)\ndf = df.iloc[1:]\ndf.head()","b336f4ac":"def get_train_data(df):\n    x = []\n\n    train = df['pixels'].to_numpy()\n\n    for i in range(len(train)):\n        x.append(train[i].split(' '))\n\n    x = np.array(x)\n    x = x.astype('float32').reshape(len(train), 48, 48, 1)\n\n    return x","7a71cdc8":"train = get_train_data(df)\nlabels = df['emotion'].to_numpy().astype('int')","b7130878":"plt.imshow(train[0].reshape(48, 48))","ed01a9e5":"labels[0]","4405e037":"X_train, X_test, y_train, y_test = train_test_split(train, labels, test_size=0.33, random_state=42)","e9b74f4e":"y_train = to_categorical(y_train)\ny_test = to_categorical(y_test)","a3caa6c3":"X_train.shape","847a5b64":"import time\n\nclass TimeHistory(tf.keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.times = []\n\n    def on_epoch_begin(self, epoch, logs={}):\n        self.epoch_time_start = time.time()\n\n    def on_epoch_end(self, epoch, logs={}):\n        self.times.append(time.time() - self.epoch_time_start)","09737a6f":"time_callback_vgg = TimeHistory()\ntime_callback_incep = TimeHistory()\ntime_callback_resid = TimeHistory()","44cccbff":"def vgg_block(layer_in, n_filters, n_conv):\n    # add convolutional layers\n    for _ in range(n_conv):\n        layer_in = Conv2D(n_filters, (3,3), padding='same', activation='relu')(layer_in)\n    # add max pooling layer\n    layer_in = MaxPooling2D((2,2), strides=(2,2))(layer_in)\n    return layer_in","2e9305a5":"# define model input\nvisible = Input(shape=(48, 48, 1))\n# add vgg module\nlayer = vgg_block(visible, 64, 2)\n# add vgg module\nlayer = vgg_block(layer, 128, 2)\n# add vgg module\nlayer = vgg_block(layer, 256, 4)\n\nlayer = Flatten()(layer)\nlayer = Dense(7, activation='softmax')(layer)\n\nmodel_vgg = Model(inputs=visible, outputs=layer)\nmodel_vgg.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","bd0d676b":"model_vgg.summary()\nplot_model(model_vgg, show_shapes=True, to_file='vgg_block.png')","fc571cbe":"history_vgg = model_vgg.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), callbacks=[time_callback_vgg])","3d497956":"def inception_module(layer_in, f1, f2_in, f2_out, f3_in, f3_out, f4_out):\n    # 1x1 conv\n    conv1 = Conv2D(f1, (1,1), padding='same', activation='relu')(layer_in)\n    # 3x3 conv\n    conv3 = Conv2D(f2_in, (1,1), padding='same', activation='relu')(layer_in)\n    conv3 = Conv2D(f2_out, (3,3), padding='same', activation='relu')(conv3)\n    # 5x5 conv\n    conv5 = Conv2D(f3_in, (1,1), padding='same', activation='relu')(layer_in)\n    conv5 = Conv2D(f3_out, (5,5), padding='same', activation='relu')(conv5)\n    # 3x3 max pooling\n    pool = MaxPooling2D((3,3), strides=(1,1), padding='same')(layer_in)\n    pool = Conv2D(f4_out, (1,1), padding='same', activation='relu')(pool)\n    # concatenate filters, assumes filters\/channels last\n    layer_out = tf.keras.layers.concatenate([conv1, conv3, conv5, pool], axis=-1)\n    return layer_out","2e95acc8":"# define model input\nvisible = Input(shape=(48, 48, 1))\n# add inception block 1\nlayer = inception_module(visible, 64, 96, 128, 16, 32, 32)\n# add inception block 1\nlayer = inception_module(layer, 128, 128, 192, 32, 96, 64)\n\nlayer = Flatten()(layer)\nlayer = Dense(7, activation='softmax')(layer)\n\nmodel_inception = Model(inputs=visible, outputs=layer)\nmodel_inception.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","fa774cf0":"model_inception.summary()\nplot_model(model_inception, show_shapes=True, to_file='inception_block.png')","35816fb7":"history_inception = model_inception.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), callbacks=[time_callback_incep])","42ebf4ca":"def residual_module(layer_in, n_filters):\n    merge_input = layer_in\n    # check if the number of filters needs to be increase, assumes channels last format\n    if layer_in.shape[-1] != n_filters:\n        merge_input = Conv2D(n_filters, (1,1), padding='same', activation='relu', kernel_initializer='he_normal')(layer_in)\n    # conv1\n    conv1 = Conv2D(n_filters, (3,3), padding='same', activation='relu', kernel_initializer='he_normal')(layer_in)\n    # conv2\n    conv2 = Conv2D(n_filters, (3,3), padding='same', activation='linear', kernel_initializer='he_normal')(conv1)\n    # add filters, assumes filters\/channels last\n    layer_out = add([conv2, merge_input])\n    # activation function\n    layer_out = Activation('relu')(layer_out)\n    return layer_out","f10fc516":"# define model input\nvisible = Input(shape=(48, 48, 1))\n# add vgg module\nlayer = residual_module(visible, 64)\n# create model\n\nlayer = Flatten()(layer)\nlayer = Dense(7, activation='softmax')(layer)\n\nmodel_residual = Model(inputs=visible, outputs=layer)\nmodel_residual.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","c8bc2329":"model_residual.summary()\nplot_model(model_residual, show_shapes=True, to_file='residual_block.png')","028f7b37":"history_residual = model_residual.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), callbacks=[time_callback_resid])","4e6183ac":"plt.title(\"Accuracy\")\nplt.plot(history_residual.history['accuracy'], 'b')\nplt.plot(history_inception.history['accuracy'], 'g')\nplt.plot(history_vgg.history['accuracy'], 'r')","57db389b":"plt.title(\"Loss\")\nplt.plot(history_residual.history['loss'], 'b')\nplt.plot(history_inception.history['loss'], 'g')\nplt.plot(history_vgg.history['loss'], 'r')","052d27d1":"plt.title(\"Value Loss\")\nplt.plot(history_residual.history['val_loss'], 'b')\nplt.plot(history_inception.history['val_loss'], 'g')\nplt.plot(history_vgg.history['val_loss'], 'r')","efae7d81":"plt.title(\"Value Accuracy\")\nplt.plot(history_residual.history['val_accuracy'], 'b')\nplt.plot(history_inception.history['val_accuracy'], 'g')\nplt.plot(history_vgg.history['val_accuracy'], 'r')","0a1a6857":"plt.title(\"Time to train per epoch (seconds)\")\nplt.plot(time_callback_resid.times, 'b')\nplt.plot(time_callback_incep.times, 'g')\nplt.plot(time_callback_vgg.times, 'r')","991883ff":"# Goal\n\nTry facial expresion with VGG, Inception and Resnet to see which one is more efficient.","a8006b50":"## Facial Expression VGG vs Inception vs ResNet","9f1b0c45":"# VGG","eb27bb0c":"# Residual Network","d6c60321":"# Graphs","97ecf4ae":"# Inception"}}