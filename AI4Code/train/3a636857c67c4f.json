{"cell_type":{"e070c22d":"code","07ac4cc3":"code","eac459e4":"code","fbafca16":"code","335751f3":"code","87f1c01f":"code","58a843a8":"code","610a1057":"code","3cf9bf12":"code","7d071481":"code","ea3bdeda":"code","d1449656":"code","9adbcee1":"code","b14c117f":"code","34b516ca":"code","47509cd6":"code","c0dca683":"code","a8ff04f7":"code","3c10934b":"code","b8cfd7c1":"code","bcff7b75":"code","7ab1ebb5":"code","4d19da41":"code","3070e331":"code","cfde52d1":"code","0a8de79c":"code","bb8851aa":"code","714a3a9f":"code","bd79a11d":"code","a13c85db":"code","4c1e110a":"markdown"},"source":{"e070c22d":"import numpy as np\n# pylab for seeing visualizations\nimport pylab as plt\n# networkx for creating and manipulating complex networks\nimport networkx as nx\nimport pandas as pd","07ac4cc3":"edge_list = [(0,2),(0,1),(0,3),(2,4),(5,6),(7,4),(0,6),(5,3),(3,7),(0,8)]","eac459e4":"goal = 7","fbafca16":"G = nx.Graph()\nG.add_edges_from(edge_list)","335751f3":"position = nx.spring_layout(G)","87f1c01f":"nx.draw_networkx_nodes(G, position)\nnx.draw_networkx_edges(G, position)\nnx.draw_networkx_labels(G, position)\nplt.show()","58a843a8":"SIZE_MATRIX = 9","610a1057":"R = np.matrix(np.ones(shape=(SIZE_MATRIX,SIZE_MATRIX)))\nR *= -1","3cf9bf12":"R","7d071481":"for edge in edge_list:\n    print(edge)\n    if edge[1] == goal:\n        R[edge] = 100\n    else:\n        R[edge] = 0\n    if edge[0] == goal:\n        R[edge[::-1]] = 100\n    else:\n        R[edge[::-1]] = 0","ea3bdeda":"R[goal,goal] = 100","d1449656":"R","9adbcee1":"gamma = 0.8","b14c117f":"Q = np.matrix(np.zeros(shape=(SIZE_MATRIX,SIZE_MATRIX)))","34b516ca":"pd.DataFrame(Q)","47509cd6":"def get_available_actions(state):\n    current_state_row = R[state,]\n    available_actions = np.where(current_state_row >= 0)[1]\n    \n    return available_actions","c0dca683":"def sample_next_action(available_actions):\n    next_action = int(np.random.choice(available_actions, size=1))\n    return next_action","a8ff04f7":"def update(current_state, action, gamma):\n    max_index = np.where(Q[action,] == np.max(Q[action,]))[1]\n    print('max_index', max_index.shape)\n    \n    if max_index.shape[0] > 1:\n        max_index = int(np.random.choice(max_index, size=1))\n    else:\n        max_index = int(max_index)\n    \n    max_value = Q[action, max_index]\n    \n    Q[current_state, action] = R[current_state, action] + gamma*max_value\n    \n    print('max_value', R[current_state, action] + gamma*max_value)","3c10934b":"initial_state = 0","b8cfd7c1":"available_actions = get_available_actions(initial_state)","bcff7b75":"print(available_actions)","7ab1ebb5":"action = sample_next_action(available_actions)","4d19da41":"print(action)","3070e331":"update(initial_state, action, gamma)","cfde52d1":"for i in range(700):\n    current_state = np.random.randint(0, int(Q.shape[0]))\n    \n    available_action = get_available_actions(current_state)\n    action = sample_next_action(available_action)\n    \n    update(current_state, action, gamma)","0a8de79c":"print('Trained Q matrix:')\npd.DataFrame(Q)","bb8851aa":"print('Normalized Q matrix:')\npd.DataFrame(Q\/np.max(Q)*100)","714a3a9f":"current_state = 0\nsteps = [current_state]","bd79a11d":"while current_state != goal:\n    next_step_index = np.where(Q[current_state,] == np.max(Q[current_state,]))[1] \n    \n    if next_step_index.shape[0] > 1:\n        next_step_index = int(np.random.choice(next_step_index, size=1))\n    else:\n        next_step_index = int(next_step_index)\n    \n    steps.append(next_step_index)\n    current_state = next_step_index","a13c85db":"print('Most efficient path:')\nprint(steps)","4c1e110a":"Purpose: Demonstrate Q-Learning algorithms using the temporal difference method in Reinforcement Learning\n\nGoal: Use rewards to find the shortest path in a graph from source to destination out of a defined state space\n\nMethod: \nUsing the mathematical formula for temporal difference\nEstimate the future reward based on the best currently known action\nEvaluate the max reward from the next state that it can get to all possible other states"}}