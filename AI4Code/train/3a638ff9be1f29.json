{"cell_type":{"7ddb314f":"code","47b9a61b":"markdown","b6c61351":"markdown"},"source":{"7ddb314f":"from IPython.display import YouTubeVideo\nYouTubeVideo('kQmHaI5Jw1c', width=800, height=450)","47b9a61b":"# Keep Going\nNow you are ready to **[train your own models from scratch](https:\/\/www.kaggle.com\/dansbecker\/deep-learning-from-scratch).**\n\n---\n**Links Mentioned**\n\n[ReLU activation function](https:\/\/www.kaggle.com\/dansbecker\/rectified-linear-units-relu-in-deep-learning)","b6c61351":"# Introduction\n\nAt the end of this lesson, you will understand how stochastic gradient descent and back-propagation are used to set the weights in a deep learning model. These topics are complex, but many experts view them as the most important ideas in deep learning.\n\n# Lesson\n"}}