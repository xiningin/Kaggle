{"cell_type":{"c41ec4db":"code","8c52a1ef":"code","01831477":"code","77bfb984":"code","4e08bfcd":"code","50ba95d2":"code","6dfbd753":"code","bcd5bcdb":"code","362417f5":"code","010b3f63":"code","407f7aba":"code","5dbe192c":"code","436487e9":"code","718a224e":"code","06d5e974":"code","802a50fd":"code","64a5e3f9":"code","ba3452de":"code","6e003b11":"code","512a134e":"code","82978107":"code","1d394aab":"code","4f6b729f":"code","168914e1":"code","c67cc900":"code","07ae554e":"code","94576702":"code","a9bf0bf9":"code","ff4baea2":"code","1d911914":"code","9b8cd7b0":"code","10d7fa77":"code","84b81ebc":"code","4b16d43f":"code","25b44532":"code","d0579049":"code","b8e60c5e":"code","a5165043":"code","9240e0ba":"code","9f150219":"code","8b714cd2":"code","e4bedde6":"code","94521f8d":"code","08a2353b":"code","dc7a9f83":"code","4275e72d":"code","eed64d1c":"code","7aa12fd1":"code","f585dae8":"code","2c7c1291":"code","f2b6fb13":"code","e4631b82":"code","b0c1f386":"code","41d63464":"code","cc1c3e5c":"code","51357889":"code","f416d781":"code","a6381f22":"code","6530c006":"code","9341c801":"code","4f01c5bf":"code","e8656ea5":"code","ab38627f":"code","1e836bc9":"code","5a2ca307":"code","eb03b148":"code","7a923f54":"code","0f209b74":"code","73edb50d":"code","205912ab":"code","8b12e21f":"code","22cacad6":"code","356cd282":"code","d4b7be11":"code","4e993962":"code","4e8e19fd":"code","11ce4628":"code","62922244":"code","d6681f1a":"code","21428d21":"code","69070e05":"code","518cb92d":"code","0d6db2ec":"code","fa5fd60e":"code","0aee6343":"code","609b5955":"code","da63a991":"code","599c12d0":"code","993276dd":"code","0e178976":"code","9bfb7709":"code","f1f6165f":"code","6db48488":"code","a9a89f16":"code","be4d3ddf":"code","512e41a4":"code","4ee2d57e":"code","9e4b4746":"code","2be9ae5c":"markdown","c447bcab":"markdown","e05438f9":"markdown","6053d5d4":"markdown","e2986e9e":"markdown","47fd4953":"markdown","5a6f2779":"markdown","cc700ec6":"markdown","5ff72da6":"markdown","7c8d743e":"markdown","980bb24e":"markdown","bbc75d37":"markdown","ccb06664":"markdown","76335d4f":"markdown","0bcaaf8c":"markdown","286374fa":"markdown","1eabd67b":"markdown","1e3ead72":"markdown","054b1786":"markdown","c7333c56":"markdown","bebbaaee":"markdown","47a1dd3d":"markdown","78220f98":"markdown","ad330d9c":"markdown","c79dfb7a":"markdown","67d82707":"markdown","c8c8dfae":"markdown","98e1533c":"markdown","429dc652":"markdown"},"source":{"c41ec4db":"# load libraries we will be using\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import cross_val_score, KFold, GridSearchCV, RandomizedSearchCV, train_test_split\nfrom sklearn.feature_selection import SelectFromModel, RFECV\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor,VotingRegressor,StackingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression,LogisticRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNetCV, SGDRegressor, ARDRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\nimport xgboost\nfrom sklearn.metrics import mean_squared_error,mean_squared_log_error\n\n\n%matplotlib inline\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = 500","8c52a1ef":"# load our training dataset\ndf_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf_submission = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\n\ndf = pd.concat([df_train, df_test], axis=0)\ndf.head()","01831477":"df_test.head()","77bfb984":"df.tail()","4e08bfcd":"# what is size of our data?\nprint('Dataset has {} rows and {} columns'.format(df.shape[0], df.shape[1]))","50ba95d2":"# checking data types, number of rows, missing rows\ndf.info()","6dfbd753":"# check summary statistics\ndf.describe()","bcd5bcdb":"# drop ID as it's not beneficial\n#df.drop('Id', axis=1, inplace=True)","362417f5":"null_counts = df.isnull().sum()\nfull_counts = df.isnull().count()\n\nmc = null_counts[null_counts > 0]\nnmc = null_counts[null_counts > 0]\/full_counts[null_counts > 0]\n\nndf = pd.DataFrame([mc, (nmc*100).round(1)], index=['Null Count', 'Null %']).T.sort_values('Null Count', ascending=False)\nndf.style.background_gradient(cmap='PuBu_r', vmin=19, vmax=20, subset=['Null %'])","010b3f63":"col_mis_high = ['FireplaceQu', 'Fence', 'Alley', 'MiscFeature', 'PoolQC']\ncol_mis_low  = ['LotFrontage', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtExposure', 'BsmtFinType2', 'BsmtFinType1', 'BsmtCond', 'BsmtQual', 'MasVnrArea', 'MasVnrType', 'Electrical','MSZoning', 'Utilities', 'Exterior1st', 'Exterior2nd', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'KitchenQual', 'Functional', 'GarageCars', 'GarageArea', 'SaleType']","407f7aba":"# look on value counts for each field\nfor c in col_mis_high:\n    print('Unique values for column {}:'.format(c.upper()))\n    print(df[c].value_counts())\n    print('----------\\n')","5dbe192c":"# impute missing\nfor c in col_mis_high:\n    df[c].fillna(value='None', inplace=True)","436487e9":"plt.figure(figsize=(18, 5))\nfor i,c in enumerate(col_mis_high):\n    plt.subplot(1,len(col_mis_high),i+1)\n    sns.barplot(x=c, y='SalePrice', data=df)\n    plt.yticks([])","718a224e":"# split data for numerical and categorical, impute and then push back to original dataset\ndf_missing = df[col_mis_low].copy()\ndf_missing_cat = df_missing.select_dtypes(include='object')\ndf_missing_num = df_missing.select_dtypes(include='number')\n\nfor c in df_missing_cat.columns:\n    df[c] = df_missing_cat[c].fillna(df_missing_cat[c].mode()[0])\n\nfor c in df_missing_num.columns:\n    df[c] = df_missing_num.fillna(df_missing_num[c].mean())","06d5e974":"# final check, do we have any missing left?\ndf.isnull().sum()[df.isnull().sum() > 0]","802a50fd":"# take a look on co\ncorrmat = df.corr().style.background_gradient()","64a5e3f9":"# Try seaborn heatmap for more condensed view\nplt.figure(figsize=(12, 12))\nsns.heatmap(df.corr(), vmin = -0.8, vmax=0.8, annot=False, square=True, cmap='seismic_r')","ba3452de":"plt.figure(figsize=(15, 10))\nplt.subplot(321)\nsns.scatterplot(df['MiscVal'], df['SalePrice'])\nplt.subplot(322)\nsns.scatterplot(df['GrLivArea'], df['SalePrice'])\nplt.subplot(323)\nsns.scatterplot(df['EnclosedPorch'], df['SalePrice'])\nplt.subplot(324)\nsns.scatterplot(df['BsmtFinSF1'], df['SalePrice'])\nplt.subplot(325)\nsns.scatterplot(df['LotFrontage'], df['SalePrice'])\nplt.subplot(326)\nsns.scatterplot(df['LotArea'], df['SalePrice'])","6e003b11":"i1 = np.array(df[df['MiscVal']>3000]['Id'])\ni2 = np.array(df[df['GrLivArea']>4000]['Id'])\ni3 = np.array(df[df['EnclosedPorch']>350]['Id'])\ni4 = np.array(df[df['BsmtFinSF1']>3000]['Id'])\ni5 = np.array(df[df['LotArea']>70000]['Id'])\ni6 = np.array(df[df['LotFrontage']>200]['Id'])\n\noutlier_idx = np.concatenate((i1,i2,i3,i4,i5,i6))\noutlier_idx = [x for x in outlier_idx if x not in df_test['Id'].tolist()]\nprint('We have found {} outliers!'.format(len(outlier_idx)))\nprint(outlier_idx)\n","512a134e":"df = df[~df['Id'].isin(outlier_idx)]\ndf_train = df_train[~df_train['Id'].isin(outlier_idx)]","82978107":"# some of our numerical variables are category IDs (like overall quality etc)\n\ndf_cat = df.select_dtypes(exclude='number').copy()\ndf_int = df[['YearBuilt', 'YearRemodAdd', 'MSSubClass', 'OverallQual', 'OverallCond', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'MoSold', 'YrSold']].copy()\ndf_num = df[['LotFrontage', 'LotArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']].copy()\n\nprint('Columns in categorical set: {}'.format(df_cat.shape))\nprint('Columns in ID set: {}'.format(df_int.shape))\nprint('Columns in numerical set: {}'.format(df_num.shape))","1d394aab":"# check correlations to SalePrice ... we need 20 charts\nfig = plt.figure(figsize=(15, 15))\n\nfor i,c in enumerate(df_num):\n    plt.subplot(5, 4, i+1)\n    plt.xticks([])\n    plt.yticks([])\n    sns.regplot(x=df_num[c], y=df['SalePrice'])","4f6b729f":"# check distributions\nfig = plt.figure(figsize=(15, 15))\n\nfor i,c in enumerate(df_num):\n    plt.subplot(5, 4, i+1)\n    plt.xticks([])\n    plt.yticks([])\n    sns.distplot(df_num[c], kde=False, rug=True)    # kde disabled as there was issue to calculate it for some fields","168914e1":"# check skewness of our fields\n# screen only those that have higher or lower skew than 1, -1\nskews = df_num.skew().sort_values()\nskew_index = skews[abs(skews) > 0.5].index\nskews[abs(skews) > 0.5]","c67cc900":"# take a look on distributions of these fields separately\nfig = plt.figure(figsize=(15, 10))\n\nfor i,c in enumerate(df_num[skew_index]):\n    plt.subplot(4, 5, i+1)\n    plt.xticks([])\n    plt.yticks([])\n    sns.distplot(df_num[c], kde=False, rug=True)    # kde disabled as there was issue to calculate it for some fields\n","07ae554e":"# transform fields using log1p\ndf_num[skew_index] = df_num[skew_index].apply(np.log1p)\ndf['SalePrice'] = df['SalePrice'].apply(np.log1p)\n#df['SalePrice'] = RobustScaler().fit_transform(df['SalePrice'].values.reshape(-1,1))","94576702":"# take a look on distributions of these fields after transformation\n# LotArea is definitely better\nfig = plt.figure(figsize=(15, 10))\n\nfor i,c in enumerate(df_num[skew_index]):\n    plt.subplot(4, 5, i+1)\n    plt.xticks([])\n    plt.yticks([])\n    sns.distplot(df_num[c], kde=False, rug=True)    # kde disabled as there was issue to calculate it for some fields\n","a9bf0bf9":"# check skewness of our problematic fields\n# notice these fields had skewness higher than 0.5\nskews = df_num[skew_index].skew().sort_values()\nskews","ff4baea2":"# basement finished percentage\ndf_num['BsmtFinSF_P'] = (df_num['BsmtFinSF1'] + df_num['BsmtFinSF2'])\/(df['TotalBsmtSF'] + 0.01)\n\n# fllor total size & low quality percentage\ndf_num['TotalFlrSF'] = (df_num['1stFlrSF'] + df_num['2ndFlrSF'])\ndf_num['FlrSF_P'] = df_num['LowQualFinSF']\/(df_num['TotalFlrSF']+0.01)\n\n# porch\ndf_num['Porch'] = df_num['OpenPorchSF'] + df_num['EnclosedPorch'] + df_num['3SsnPorch'] + df_num['ScreenPorch']\n\ndf_num.drop(['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','1stFlrSF','2ndFlrSF','LowQualFinSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch'], axis=1, inplace=True)","1d911914":"# bathrooms\ndf_int['BsmtBath'] = (df_int['BsmtFullBath'] + 0.5*df_int['BsmtHalfBath'])\ndf_int['Bath'] = (df_int['FullBath'] + 0.5*df_int['HalfBath'])\ndf_int['BsmtBath_P'] = df_int['Bath']\/(df_int['BsmtBath'] + 0.01)\n\ndf_int.drop(['BsmtFullBath','BsmtHalfBath','FullBath','HalfBath'], axis=1, inplace=True)","9b8cd7b0":"rs = RobustScaler()\nfor c in df_num.columns:\n    df_num[c] = rs.fit_transform(df_num[c].values.reshape(-1,1))\n\n#df['SalePrice'] = rs.fit_transform(df['SalePrice'].values.reshape(-1,1))\ndf_num.head()","10d7fa77":"# original data shape\ndf_num.shape","84b81ebc":"# Explain 0.95 of variance\npca = PCA(0.95)\ndf_num_pca = pca.fit_transform(df_num)\ndf_num_pca.shape","4b16d43f":"df_int.shape","25b44532":"# Do we have some clear correlation? Yes, slightly\nsns.regplot(df_int['YearBuilt'], df['SalePrice'],color='red')\nsns.regplot(df_int['YearRemodAdd'], df['SalePrice'],color='blue')","d0579049":"df_int['YearBuilt'] = (2010 - df_int['YearBuilt'])\/\/10*10\ndf_int['YearRemodAdd'] = (2010 - df_int['YearRemodAdd'])\/\/10*10\ndf_int['YrSold'] = (2010 - df_int['YrSold'])\/\/10*10","b8e60c5e":"# yes it's much better!\nsns.regplot(df_int['YearBuilt'], df['SalePrice'],color='red')\nsns.regplot(df_int['YearRemodAdd'], df['SalePrice'],color='blue')","a5165043":"df_int['YearBuiltRemod'] = df_int['YearRemodAdd'] + df_int['YearBuilt']\ndf_int.drop(['YearRemodAdd','YearBuilt'], axis=1, inplace=True)","9240e0ba":"# for me it was hard to understand what GarageYrBlt\ndef garage_year(x):\n    y = 1900 + x\n    \n    if y > 2020:\n        y = y-100\n        \n    return round(y)\n\nhelp = df_int['GarageYrBlt'].map(garage_year)\ndf_int['GarageYrBlt'] = (2010 - df_int['GarageYrBlt'])\/\/10*10","9f150219":"sns.regplot(df_int['GarageYrBlt'], df['SalePrice'])","8b714cd2":"sns.regplot(df_int['MoSold'], df['SalePrice'])","e4bedde6":"def month_sold(x):\n    if x == 7:\n        return 1\n    else:\n        return 0","94521f8d":"df_int['SoldSeason'] = df_int['MoSold'].map(month_sold)\ndf_int.drop(['MoSold'], axis=1, inplace=True)","08a2353b":"sns.regplot(df_int['SoldSeason'], df['SalePrice'])","dc7a9f83":"for c in df_int.columns:\n    df_int[c] = StandardScaler().fit_transform(df_int[c].values.reshape(-1,1))","4275e72d":"df_int.head()","eed64d1c":"df_cat.head()","7aa12fd1":"# Checking now unique values we have for each column, we will have to split our fields to candidates for one hot encoding and ordinal values\ndf_cat.describe(include='all')","f585dae8":"ordinal = ['LandSlope', 'BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','KitchenQual', 'HeatingQC','FireplaceQu','GarageFinish','GarageQual','GarageCond','PoolQC','Fence']\nonehot = df_cat.columns.tolist()\nonehot = [x for x in onehot if x not in ordinal]","2c7c1291":"df_cat_oe = OneHotEncoder(handle_unknown='ignore', sparse=False).fit_transform(df_cat[onehot])\ndf_cat_oe.shape","f2b6fb13":"df_cat_le = df_cat[ordinal]\ndf_cat_le.head()","e4631b82":"df_cat_le.describe(include='all')","b0c1f386":"df_cat_le['LandSlope'] = df_cat_le['LandSlope'].map({'Gtl':0, 'Mod':1, 'Sev':2})","41d63464":"df_cat_le['BsmtQual'] = df_cat_le['BsmtQual'].map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})","cc1c3e5c":"df_cat_le['BsmtCond'] = df_cat_le['BsmtCond'].map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})","51357889":"df_cat_le['BsmtExposure'] = df_cat_le['BsmtExposure'].map({'NA':0, 'No':1, 'Mn':2, 'Av':3, 'Gd':4})","f416d781":"df_cat_le['BsmtFinType1'] = df_cat_le['BsmtFinType1'].map({'NA':0, 'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6})","a6381f22":"df_cat_le['BsmtFinType2'] = df_cat_le['BsmtFinType2'].map({'NA':0, 'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6})","6530c006":"df_cat_le['BsmtFinType'] = df_cat_le['BsmtFinType1'] + df_cat_le['BsmtFinType2']\ndf_cat_le.drop(['BsmtFinType1','BsmtFinType2'], axis=1, inplace=True)","9341c801":"df_cat_le['KitchenQual'] = df_cat_le['KitchenQual'].map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})","4f01c5bf":"df_cat_le['HeatingQC'] = df_cat_le['HeatingQC'].map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})","e8656ea5":"df_cat_le['FireplaceQu'] = df_cat_le['FireplaceQu'].map({'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})","ab38627f":"df_cat_le['GarageFinish'] = df_cat_le['GarageFinish'].map({'NA':0, 'Unf':1, 'RFn':2, 'Fin':3})","1e836bc9":"df_cat_le['GarageQual'] = df_cat_le['GarageQual'].map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})","5a2ca307":"df_cat_le['GarageCond'] = df_cat_le['GarageCond'].map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})","eb03b148":"df_cat_le['PoolQC'] = df_cat_le['PoolQC'].map({'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})","7a923f54":"df_cat_le['Fence'] = df_cat_le['Fence'].map({'None':0, 'MnWw':1, 'GdWo':2, 'MnPrv':3, 'GdPrv':4})","0f209b74":"for c in df_cat_le.columns:\n    df_cat_le[c] = StandardScaler().fit_transform(df_cat_le[c].values.reshape(-1,1))","73edb50d":"df_cat_le.head()","205912ab":"print(df_num_pca.shape)\nprint(df_int.shape)\nprint(df_cat_oe.shape)\nprint(df_cat_le.shape)","8b12e21f":"df_final = np.concatenate((df_num_pca, df_int, df_cat_oe, df_cat_le.values), axis=1)\ndf_final.shape","22cacad6":"max_id = df_train.shape[0]\n\ndf_final_t = df_final[:max_id]\nprint('Training shape: ', df_final_t.shape)\ny = df['SalePrice'].iloc[:max_id].values.reshape(-1,1)\nprint('Target shape: ', y.shape)\n\n# # prepare feature values\ndf_predict = df_final[max_id:,]\nprint('Features shape: ', df_predict.shape)\n","356cd282":"# split data into train & test size\nX_train, X_test, y_train, y_test = train_test_split(df_final_t, y, test_size=0.25, random_state=123)","d4b7be11":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression\nselector = SelectFromModel(estimator=LassoCV(n_jobs=-1, random_state=123)).fit(X_train, y_train)\ncoef = np.abs(selector.estimator_.coef_)\ncoef = coef>0.0001\n\nX_train = X_train[:, coef]\nX_test = X_test[:, coef]\ndf_predict = df_predict[:,coef]","4e993962":"# define function to get negative root mean squared error from model\ndef rmse_cv_train(model):\n    kf = KFold(5, random_state=123)\n    rmse = np.sqrt(-cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=kf))\n    return rmse.mean()\n\ndef rmse_cv_test(model):\n    kf = KFold(5, random_state=123)\n    rmse = np.sqrt(-cross_val_score(model, X_test, y_test, scoring=\"neg_mean_squared_error\", cv=kf))\n    return rmse.mean()","4e8e19fd":"rg = RidgeCV(alphas=np.linspace(1, 20, 60), cv=KFold(5,random_state=123)).fit(X_train, y_train)\nprint('Score on train set:', rmse_cv_train(rg))\nprint('Score on test set:', rmse_cv_test(rg))","11ce4628":"ls = LassoCV(n_alphas=200,random_state=123, cv=KFold(5,random_state=123)).fit(X_train, y_train)\nprint('Score on train set:', rmse_cv_train(ls))\nprint('Score on test set:', rmse_cv_test(ls))","62922244":"el = ElasticNetCV(n_alphas=200,random_state=123, cv=KFold(5,random_state=123)).fit(X_train, y_train)\nprint('Score on train set:', rmse_cv_train(el))\nprint('Score on test set:', rmse_cv_test(el))","d6681f1a":"gb = GradientBoostingRegressor().fit(X_train, y_train)\nprint('Score on train set:', rmse_cv_train(gb))\nprint('Score on test set:', rmse_cv_test(gb))","21428d21":"rf = RandomForestRegressor(n_jobs=-1).fit(X_train, y_train)\nprint('Score on train set:', rmse_cv_train(rf))\nprint('Score on test set:', rmse_cv_test(rf))","69070e05":"xg = XGBRegressor(objective='reg:squarederror', n_jobs=-1).fit(X_train, y_train)\nprint('Score on train set:', rmse_cv_train(xg))\nprint('Score on test set:', rmse_cv_test(xg))","518cb92d":"sv = SVR().fit(X_train, y_train)\nprint('Score on train set:', rmse_cv_train(sv))\nprint('Score on test set:', rmse_cv_test(sv))","0d6db2ec":"# merge results\ny_test_hat_rg = rg.predict(X_test)\ny_test_hat_ls = ls.predict(X_test)\ny_test_hat_el = el.predict(X_test)\ny_test_hat_gb = gb.predict(X_test)\ny_test_hat_rf = rf.predict(X_test)\ny_test_hat_xg = xg.predict(X_test)\n\ny_test_hat = 0.3*y_test_hat_rg.ravel() + 0.2*y_test_hat_ls.ravel() + 0.2*y_test_hat_el.ravel() + 0.1*y_test_hat_gb.ravel() +  0.1*y_test_hat_rf.ravel() + 0.1*y_test_hat_rg.ravel()\n\nrmse = np.sqrt(mean_squared_error(y_test_hat,  y_test))\nprint('RMSE: ', rmse)","fa5fd60e":"# do the same using voting\nvreg = VotingRegressor([\n    ('rg', rg), \n    ('ls', ls),\n    ('el', el),\n    ('gb', gb),\n    ('rf', rf),\n    ('xg', xg)\n])\n\nvr = vreg.fit(X_train, y_train)\nprint('Score on train set:', rmse_cv_train(vr))\nprint('Score on test set:', rmse_cv_test(vr))\n\ny_test_hat_vr = vr.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test_hat_vr,  y_test))\nprint('RMSE: ', rmse)","0aee6343":"# support vector machines\n# actually we've ended up not using SVM as it looked it's score is not so good as from the others\nparams = {\n    'degree':[1,2,3,4],\n    'C':[0.01,0.1,0.5,1,2,5,10],\n    'epsilon':[0.001,0.01,0.1,1,5]\n}\n\ngssvr = GridSearchCV(SVR(), param_grid=params, n_jobs=-1, scoring='neg_mean_squared_error').fit(X_train, y_train)\nprint('SVR best params: ', gssvr.best_params_)\nprint('SVR best score: ', gssvr.best_score_)","609b5955":"# gradient boosting\nparams = {\n    'random_state': [123],\n    'max_depth': [1,2],\n    'max_features': [8,10],\n    'min_samples_leaf': [3,5],\n    'min_samples_split': [1,2,3],\n    'n_estimators': [1000,1200]\n}\n\ngsgb = GridSearchCV(GradientBoostingRegressor(), param_grid=params, n_jobs=-1, scoring='neg_mean_squared_error').fit(X_train, y_train)\nprint('Gradient Boosting best params: ', gsgb.best_params_)\nprint('Gradient Boosting best score: ', gsgb.best_score_)","da63a991":"# random forest\nparams = {\n    'random_state': [123],\n    'n_jobs': [-1],\n    'bootstrap': [True],\n    'max_depth': [200, 300],\n    'max_features': [30,40],\n    'min_samples_leaf': [1,2],\n    'min_samples_split': [2,3],\n    'n_estimators': [1200]\n}\n\ngsrf = GridSearchCV(RandomForestRegressor(), param_grid=params, n_jobs=-1, scoring='neg_mean_squared_error').fit(X_train, y_train)\nprint('Random Forest best params: ', gsrf.best_params_)\nprint('Random Forest best score: ', gsrf.best_score_)","599c12d0":"# xgb regressor\nparams = {\n    'nthread':[-1],\n    'objective':['reg:squarederror'],\n    'learning_rate': [0.02,0.03],\n    'max_depth': [2,3],\n    'min_child_weight': [1,2],\n    'subsample': [0.7],\n    'colsample_bytree': [0.5,0.6],\n    'n_estimators': [1500,2000]\n}\n\ngsxb = GridSearchCV(XGBRegressor(), param_grid=params, n_jobs=-1, scoring='neg_mean_squared_error').fit(X_train, y_train)\nprint('XGB Regressor best params: ', gsxb.best_params_)\nprint('XGB Regressor best score: ', gsxb.best_score_)","993276dd":"#gbT = GradientBoostingRegressor(max_depth=2, max_features=10, min_samples_leaf=5,min_samples_split=2, n_estimators=500, random_state=123).fit(X_train, y_train)\ngbT = gsgb.best_estimator_.fit(X_train, y_train)\nprint('Score on train set:', rmse_cv_train(gbT))\nprint('Score on test set:', rmse_cv_test(gbT))","0e178976":"#rfT = RandomForestRegressor(bootstrap=True, max_depth=200, max_features=30, min_samples_leaf=1, min_samples_split=3, n_estimators=1200, n_jobs=-1, random_state=123).fit(X_train, y_train)\nrfT = gsrf.best_estimator_.fit(X_train, y_train)\nprint('Score on train set:', rmse_cv_train(rfT))\nprint('Score on test set:', rmse_cv_test(rfT))","9bfb7709":"#xgT = XGBRegressor(colsample_bytree=0.5, learning_rate=0.03, max_depth=3, min_child_weight=1, n_estimators=1200, nthread=-1, objective='reg:squarederror', silent=None, subsample=0.7).fit(X_train, y_train)\nxgT = gsxb.best_estimator_.fit(X_train, y_train)\nprint('Score on train set:', rmse_cv_train(xgT))\nprint('Score on test set:', rmse_cv_test(xgT))","f1f6165f":"# merge results\ny_test_hat_rg = rg.predict(X_test)\ny_test_hat_ls = ls.predict(X_test)\ny_test_hat_el = el.predict(X_test)\ny_test_hat_gb = gbT.predict(X_test)\ny_test_hat_rf = rfT.predict(X_test)\ny_test_hat_xg = xgT.predict(X_test)\n\ny_test_hat = 0.2*y_test_hat_rg.ravel() + 0.2*y_test_hat_ls.ravel() + 0.2*y_test_hat_el.ravel() + 0.1*y_test_hat_gb.ravel() +  0.1*y_test_hat_rf.ravel() + 0.1*y_test_hat_rg.ravel()\n\nrmse = np.sqrt(mean_squared_error(y_test_hat,  y_test))\nprint('RMSE: ', rmse)\n\nrmsle = np.sqrt(mean_squared_log_error(y_test_hat,  y_test))\nprint('RMSLE: ', rmsle)","6db48488":"# Predicted vs acutals\nsns.scatterplot(x=y_test.ravel(), y=y_test.ravel(), color='blue')\nsns.scatterplot(x=y_test.ravel(), y=y_test_hat.ravel(), color='red')","a9a89f16":"# Predicted vs acutals\nsns.scatterplot(x=y_test.ravel(), y=y_test.ravel(), color='blue')\nsns.scatterplot(x=y_test.ravel(), y=y_test_hat_vr.ravel(), color='red')","be4d3ddf":"# predict price\ny_pred_rg = rg.predict(df_predict)\ny_pred_ls = ls.predict(df_predict)\ny_pred_el = el.predict(df_predict)\ny_pred_gb = gbT.predict(df_predict)\ny_pred_rf = rfT.predict(df_predict)\ny_pred_xg = xgT.predict(df_predict)\n\ny_pred = 0.2*y_pred_rg.ravel() + 0.2*y_pred_ls.ravel() + 0.2*y_pred_el.ravel() + 0.2*y_pred_gb.ravel() +  0.1*y_pred_rf.ravel() + 0.1*y_pred_rg.ravel()\n","512e41a4":"# revert log transformation\ny_pred = np.expm1(y_pred)","4ee2d57e":"# save to csv\ndf_submission['SalePrice'] = y_pred\ndf_submission.to_csv('Submission.csv', index = False)\nprint('Submission saved!')","9e4b4746":"df_submission.head()","2be9ae5c":"**Good!** We have fixed skewness at least a bit, it's time to scale our numerical features","c447bcab":"Some of features has strong correlation with low p-value, some of them has strong correlation with high p-value and some of them has low correlation.\nNotice also outliers having in numerous dimensions, might be handled, but we will use RobustScaler later on that is pretty good agains outliers","e05438f9":"Sligtly better? Maybe just a little bit","6053d5d4":"## Exploratory Data Analysis (EDA)\nLet's play with data now, explore some interesting facts and relations that could be really useful for predictions","e2986e9e":"#### Now exclude our testing dataset we wants to predict","47fd4953":"### Handle missing values\nBased on analysis above we may see some fields have missing values. To make it clear, we will screen just features those has some missing values and how much it does","5a6f2779":"**Outcome?** It's obvious that significant part of our features are correlated, thus may not be independed. Also there is pretty strong correlation to target variable (last row), some of them are strongly positive, few have negative correlation","cc700ec6":"#### Integer \/ ID values\nWe are now done with purely numerical values, let's go to integer values\/ids","5ff72da6":"#### One hot encoding","7c8d743e":"**Years**? Do we need years? maybe better would be to transform to something different?","980bb24e":"### Distributions\nWe have a lot of variables, it would take some time to draw pairplot on all of them, let's check just relation to SalePrice\nTo do this, we also split our variables to numerical and categorical and will continue checking them and doing analyses separately","bbc75d37":"checking descriptions for all these fields, missing value simply means that property does not have that feature. Dropping these fields could be mistake, as it can have impact on price predicted. Let's impute these values with None and check how it is correlated with SalePrice","ccb06664":"## Modelling\nWe will use Lasso and Ridge","76335d4f":"#### PCA\nSome of our features has higher correlation, let's try PCA and check how much fields is needed to explain 0.95 of variance","0bcaaf8c":"# House Prices Prediction - Standard regression problem\nOur task today is to predict prices of houses based on numerous features. Let's try!","286374fa":"#### Categorical features\nNow work with categorical values and encode them","1eabd67b":"### Correlation\nIt's time to look how our features correlates to each other as well how they correlate to target variable. Our features should be independed, meaning correlation between features itself should be close to 0","1e3ead72":"#### Numerical \/ float values\nFirst work with float values","054b1786":"#### Features engineering","c7333c56":"#### Hyperparameter tuning\nWe got nice scores from different models, let's try to tune them first before merging them together. Note that Ridge, Lasso and ElasticNet are already tuned. I will do several iterrations in each step to get best parameters. Alternatively I could use random grid search to speedup process.","bebbaaee":"#### Hypertuning outcome\nWe have run parameter hypertuning on multiple models, we will now retrain our models to use best parameters and compare score of our prediction once again","47a1dd3d":"#### Label encoder\nMost of our fields are ordinal, would be mistake to simply encode them, let's do dirty job and encode them manually","78220f98":"That's pretty good! we've reduced our data by 5 columns and got 0.95 of variance","ad330d9c":"**Conclussion?** We've highlighted fields those has missing percentage under 20%. Generally everything under 15% should be dropped, let's check what values we have in fields with high missing values.\nRemember, missing value may not always mean it's missing ;)","c79dfb7a":"Those are pretty good scores! Will use manual merge later on after optimizatoin","67d82707":"Take a look on impact of month sold, it seems there is almost no correlation.. let's try to reshape month to winter, summer, ...","c8c8dfae":"Looks good, it seems some fields has impact on price, now impute fields with missing values less than 20%, will use mode for categorical features and mean for numerical features","98e1533c":"## Predict our test data\nWe are done with modelling now, it's time to predict SalePrice for test set","429dc652":"### Find outliers\nWe wants to check following fields: MiscVal, GrLivArea, EnclosedPorch, BsmtFinSF1"}}