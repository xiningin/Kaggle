{"cell_type":{"e52fd668":"code","e3fa9191":"code","618b5599":"code","b4a03841":"code","e38fbfe0":"code","fb3d2c2f":"code","bfd57fba":"code","736fbfd6":"code","d5959a3c":"code","ce22f0c0":"code","88352cd1":"code","d05b4dba":"code","2939b55a":"code","ca0c9491":"code","dc5d4964":"code","4b5b28b3":"code","b050db29":"code","3eaaee09":"code","623e02bf":"code","a88b00bc":"code","5f5ec49d":"code","92f68d0c":"code","0d7822e4":"code","c31d18b5":"code","f4fe741e":"code","28c2e5af":"code","19c1eb10":"code","3c505d50":"code","21a536c4":"code","a93fbb83":"code","e79227bd":"code","9cfb750d":"code","ff165a29":"code","114d6127":"code","9903f7ea":"code","1d613cde":"code","623238a9":"code","df2161ea":"markdown","441de6fe":"markdown","58631e49":"markdown","d1b25e3f":"markdown","12742d9e":"markdown","7c13f6e5":"markdown","a033b80a":"markdown","e7a270cf":"markdown","cb2cc9c3":"markdown","69f1eea0":"markdown","7cdf0229":"markdown"},"source":{"e52fd668":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e3fa9191":"dataset = pd.read_csv(\"..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\")","618b5599":"dataset.head(20)","b4a03841":"import pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers.core import Activation, Dropout, Dense\nfrom keras.layers import Flatten\nfrom keras.layers import GlobalMaxPooling1D,Conv1D,LSTM\nfrom keras.layers.embeddings import Embedding\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer","e38fbfe0":"dataset.isnull().values.any()","fb3d2c2f":"dataset.shape","bfd57fba":"dataset['review'][3]","736fbfd6":"import seaborn as sns","d5959a3c":"sns.countplot(x='sentiment', data=dataset)","ce22f0c0":"TAG_RE = re.compile(r'<[^>]+>')\ndef remove_tags(text):\n    return TAG_RE.sub('', text)\n\ndef preprocessing_text(text):\n    # Remove html tag\n    sentence = remove_tags(text)\n    # Remove link\n    sentence = re.sub(r'https:\\\/\\\/[a-zA-Z]*\\.com',' ',sentence)\n    # Remove number\n    sentence = re.sub(r'\\d+',' ',sentence)\n    # Remove white space\n    sentence = re.sub(r'\\s+',' ',sentence)\n    # Remove single character\n    sentence = re.sub(r\"\\b[a-zA-Z]\\b\", ' ', sentence)\n    # Remove bracket\n    sentence = re.sub(r'\\W+',' ',sentence)\n    # Make sentence lowercase\n    sentence = sentence.lower()\n    return sentence\n\n\n    ","88352cd1":"pre_proces_sen = []\nsentences = list(dataset['review'])\nfor sen in sentences:\n    pre_proces_sen.append(preprocessing_text(sen))","d05b4dba":"print(pre_proces_sen[2])","2939b55a":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize","ca0c9491":"stop = ['has', 'its', \"needn't\", 'm', \"wouldn't\", 'but', 'he', \"mustn't\", 'his', 'there', 'or', \"won't\", 'can', 'd', \"hadn't\", 'how', 'hasn', 'very', 'wouldn', 'own', \"doesn't\", 'their', \"isn't\", 'an', \"haven't\", \"wasn't\", 'those', 'once', \"shan't\", 'when', \"aren't\", 've', 'it', \"it's\", 'of', \"don't\", 'and', 'down', 'yours', 'to', 'over', \"she's\", 'we', 'they', 'haven', 'having', 'ain', 'no', 'her', 'you', 'then', 'just', 'didn', 'into', 'before', 'shouldn', 'here', 'yourselves', 's', 'will', 'which', 'are', 'who', 'with', \"you'd\", 'this', 'me', 'themselves', \"you've\", 'hadn', 'mightn', 'she', 'o', 'more', 'whom', 'for', 'him', 'again', 'below', 'few', 'most', 'been', 'such', 'shan', 'is', 'ourselves', 'y', 'by', 'being', 'in', 'mustn', \"you'll\", 'herself', 'yourself', 'ours', 'between', 'had', 'other', \"should've\", 't', 'isn', 'them', 'himself', 're', 'doing', 'only', 'where', 'your', 'after', 'so', 'll', 'against', 'the', 'about', 'each', 'aren', 'wasn', \"couldn't\", 'have', 'ma', 'i', 'my', \"mightn't\", 'as', 'from', 'itself', 'under', 'same', 'why', 'any', 'our', 'be', 'off', \"hasn't\", 'through', \"you're\", 'was', 'did', \"shouldn't\", 'myself', 'some', 'theirs', 'hers', 'further', 'do', 'now', 'than', 'too', 'during', 'at', 'because', 'doesn', 'needn', \"weren't\", 'don', \"didn't\", 'couldn', 'what', 'does', 'if', 'up', 'on', 'these', 'should', 'all', \"that'll\", 'above', 'weren', 'that', 'a', 'while', 'both', 'until', 'were', 'am']","dc5d4964":"for i in range(len(pre_proces_sen)):\n    x = pre_proces_sen[i]\n    x = word_tokenize(x)\n    new_x_list = [word for word in x if word not in stop]\n    pre_proces_sen[i] = ' '.join(new_x_list)\n    if i% 2000 == 0:\n        print(i,end=\" \")\n","4b5b28b3":"print(pre_proces_sen[2])","b050db29":"y  = dataset['sentiment']\ny = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, y)))","3eaaee09":"X = pre_proces_sen","623e02bf":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","a88b00bc":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)","5f5ec49d":"vocab_size = len(tokenizer.word_index) + 1\nprint(vocab_size)\nmaxlen = 100\nX_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_test = pad_sequences(X_test, padding='post', maxlen=maxlen)","92f68d0c":"import gensim\nWORD2VEC_MODEL = \"..\/input\/one-lac-bin\/model_1_lac.bin\"\n#load word2vec model\nword2vec = gensim.models.KeyedVectors.load_word2vec_format(WORD2VEC_MODEL, binary=True, limit=100000)","0d7822e4":"embedding_weights = np.zeros((vocab_size, 300))\nfor word, index in tokenizer.word_index.items():\n    #embedding_vector = word2vec.get(word)\n    try:\n        embedding_weights[index] = word2vec[word]\n    except:\n        pass ","c31d18b5":"print(word2vec['not'][:40])","f4fe741e":"model = Sequential()\n\nembedding_layer = Embedding(vocab_size, 300, weights=[embedding_weights], input_length=maxlen , trainable=False)\nmodel.add(embedding_layer)\n\nmodel.add(Conv1D(128, 5, activation='relu'))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])","28c2e5af":"print(model.summary())","19c1eb10":"history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)\n\nscore = model.evaluate(X_test, y_test, verbose=1)","3c505d50":"print(\"Test Score:\", score[0])\nprint(\"Test Accuracy:\", score[1])","21a536c4":"import matplotlib.pyplot as plt\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\n\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train','test'], loc='upper left')\nplt.show()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\n\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train','test'], loc='upper left')\nplt.show()","a93fbb83":"model = Sequential()\nembedding_layer = Embedding(vocab_size, 300, weights=[embedding_weights], input_length=maxlen , trainable=False)\nmodel.add(embedding_layer)\nmodel.add(LSTM(128))\n\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])","e79227bd":"print(model.summary())","9cfb750d":"history =  model.fit(X_train, y_train,  batch_size=128, epochs=8, validation_split=0.2,verbose=1)","ff165a29":"score = model.evaluate(X_test, y_test, verbose=1)","114d6127":"print(\"Test Score:\", score[0])\nprint(\"Test Accuracy:\", score[1])","9903f7ea":"import matplotlib.pyplot as plt\n\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\n\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train','test'], loc='upper left')\nplt.show()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\n\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train','test'], loc='upper left')\nplt.show()","1d613cde":"text = ['I laughed all the way through this rotten movie It so unbelievable woman leaves her husband after many years of marriage has breakdown in front of real estate office What happens The office manager comes outside and offers her job Hilarious Next thing you know the two women are going at it Yep they re lesbians Nothing rings true in this Lifetime for Women with nothing better to do movie Clunky dialogue like don want to spend the rest of my life feeling like had chance to be happy and didn take it doesn help There a wealthy distant mother who disapproves of her daughter new relationship sassy black maid unbelievable that in the year film gets made in which there a sassy black maid Hattie McDaniel must be turning in her grave The woman has husband who freaks out and wants custody of the snotty teenage kids Sheesh No cliche is left unturned']","623238a9":"pre = text\npre_sequences = tokenizer.texts_to_sequences(pre)\npre_padded = pad_sequences(pre_sequences,maxlen=maxlen, padding='post')\nprediction = model.predict(pre_padded)\nprediction","df2161ea":"# Embedding_Matrics","441de6fe":"# Removing Stopwords","58631e49":"# Train_Test_split","d1b25e3f":"### Testing on some Data ","12742d9e":"# Plotting Accuracy and Validation accuracy","7c13f6e5":"# Tokenizing","a033b80a":"# Evaluating on Test data","e7a270cf":"# Transfer Learning","cb2cc9c3":"# PreProcessing Reviews","69f1eea0":"# Convolutional Neural Network","7cdf0229":"# LSTM Model ###Final Model"}}