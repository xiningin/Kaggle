{"cell_type":{"6dfb3a3a":"code","09f21dc9":"code","faaf9d34":"code","68822017":"code","8195cab0":"code","f404918b":"code","2ec39c52":"code","1213bd8a":"code","9ab79c71":"code","a0fea393":"code","849e7dd5":"code","c0b9ae6d":"code","8a1112ba":"code","8dcec467":"code","1aac5439":"code","966ab9c6":"markdown","31263a10":"markdown","4c8f175d":"markdown","67e3ef19":"markdown","9890e51e":"markdown"},"source":{"6dfb3a3a":"from keras.datasets import imdb\n","09f21dc9":"vocabulary_size = 5000\n(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\nprint('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))","faaf9d34":"print('---review---')\nprint(X_train[6])\nprint('---label---')\nprint(y_train[6])","68822017":"word2id = imdb.get_word_index()\nid2word = {i: word for word, i in word2id.items()}\nprint('---review with words---')\nprint([id2word.get(i, ' ') for i in X_train[6]])\nprint('---label---')\nprint(y_train[6])","8195cab0":"print('Maximum review length: {}'.format(\nlen(max((X_train + X_test), key=len))))","f404918b":"print('Minimum review length: {}'.format(\nlen(min((X_test + X_test), key=len))))","2ec39c52":"from keras.preprocessing import sequence\nmax_words = 500\nX_train = sequence.pad_sequences(X_train, maxlen=max_words)\nX_test = sequence.pad_sequences(X_test, maxlen=max_words)","1213bd8a":"from keras import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, Dropout\nembedding_size=32\nmodel=Sequential()\nmodel.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\nmodel.add(LSTM(100))\nmodel.add(Dense(1, activation='sigmoid'))\nprint(model.summary())","9ab79c71":"model.compile(loss='binary_crossentropy', \n             optimizer='adam', \n             metrics=['accuracy'])","a0fea393":"batch_size = 64\nnum_epochs = 3\nX_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\nX_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]\nmodel.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)","849e7dd5":"scores = model.evaluate(X_test, y_test, verbose=0)\nprint('Test accuracy:', scores[1])","c0b9ae6d":"predict=model.predict_classes(X_test)\npredict_classes=predict.reshape(len(X_test))","8a1112ba":"def get_original_text(i):\n    word_to_id = imdb.get_word_index()\n    word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n    word_to_id[\"<PAD>\"] = 0\n    word_to_id[\"<START>\"] = 1\n    word_to_id[\"<UNK>\"] = 2\n\n    id_to_word = {value:key for key,value in word_to_id.items()}\n    return ' '.join(id_to_word[id] for id in X_test[i])","8dcec467":"SentimentDict={1:'positive', 0:'negative'}\ndef display_test_sentiment(i):\n    print(get_original_text(i))\n    print('label: ', SentimentDict[y_test[i]], ', prediction: ', SentimentDict[predict_classes[i]])","1aac5439":"display_test_sentiment(4)","966ab9c6":"# TRAINING THE MODEL","31263a10":"# BUILDING MODEL","4c8f175d":"# DATA PREPROCESSING","67e3ef19":"# DOWNLOADING THE DATASET","9890e51e":"# DATA EXPLORATION"}}