{"cell_type":{"1468ea9e":"code","7421667a":"code","33f6be49":"code","8f502405":"code","aac235b1":"code","e6525f33":"code","92fafa65":"code","de22f965":"code","fdba90b0":"code","cae3ea6b":"code","8288e179":"code","c0500bcd":"code","8488b98c":"code","0c16931f":"code","bfa43f7f":"code","451e2b05":"code","3d8ddd5c":"code","29aeb6ef":"code","b13c7555":"code","95d37b8d":"code","be2cd0f7":"code","8143e589":"code","43cc8578":"code","3ca49e1e":"code","1691043c":"code","13572c0a":"code","d691b22d":"code","e64e3ff6":"code","7581ff06":"code","e817961c":"code","bbe942bd":"code","c3b4b5fb":"code","8fd6fd51":"code","2b7661b0":"code","03c86291":"code","1858bef3":"code","82262918":"code","d47e3dc4":"code","def95d98":"code","40100571":"code","0b021619":"code","e30128bc":"code","724ac3ec":"code","633438a0":"code","c31690d6":"markdown","726aa5f1":"markdown","598aa14d":"markdown","28544766":"markdown","4278884d":"markdown","ca9afc28":"markdown","4170db06":"markdown","fd0d7e5f":"markdown","947eeaa6":"markdown","b8dc2600":"markdown","f238332e":"markdown","5a137cbe":"markdown","98e25547":"markdown","8a68b423":"markdown","3c3afe96":"markdown","888dbbaa":"markdown","39863af9":"markdown","c3f9d07b":"markdown","cca4ffdc":"markdown","37baf04d":"markdown","19a4bccb":"markdown","c9927303":"markdown","cd977d74":"markdown","55ee1f5e":"markdown","fe872c2a":"markdown","e39c82cc":"markdown","7c215f4b":"markdown","c5e8f285":"markdown","274ba6e7":"markdown","7b595e9e":"markdown","b58f056f":"markdown","9fd21040":"markdown","8183c623":"markdown","31bd8b76":"markdown","609557ae":"markdown","11e10111":"markdown","cea3eb2c":"markdown","9551f698":"markdown","981cc3aa":"markdown","cd3fbf33":"markdown","d139c853":"markdown","0a5ac854":"markdown","4f839a9c":"markdown","e80a8e46":"markdown","e8473ca3":"markdown"},"source":{"1468ea9e":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\n\n# models that i will use\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')","7421667a":"train_set = pd.read_csv('..\/input\/titanic\/train.csv')\nsubmission_set = pd.read_csv('..\/input\/titanic\/test.csv')","33f6be49":"train_set.head()","8f502405":"print(\"Train shape:\" , train_set.shape)\nprint(\"Submission shape:\" , submission_set.shape)","aac235b1":"print(train_set.info())\nprint(\"*\"*40)\nprint(submission_set.info())","e6525f33":"train_set.describe()","92fafa65":"train_set['Embarked'].unique()","de22f965":"train_set.drop(labels = ['PassengerId' , 'Ticket' , 'Cabin'] , axis = 1 , inplace = True)\n\nsubmission_pId = submission_set['PassengerId']\nsubmission_set.drop(labels = ['PassengerId' , 'Ticket' , 'Cabin'] , axis = 1 , inplace = True)","fdba90b0":"train_set['Title'] = train_set['Name'].str.extract(r'(\\w+)\\.')\nsubmission_set['Title'] = submission_set['Name'].str.extract(r'(\\w+)\\.')\n\ndf_title = pd.DataFrame()\ndf_title['Title'] = train_set['Title'].unique()\n\ntotal_arr = []\nsurvived_arr = []\nsurvival_rate_arr = []\n\nfor title in df_title['Title'].unique():\n    total_arr.append(len(train_set[train_set['Title'] == title]))\n    survived_arr.append(len(train_set[(train_set['Title'] == title) & (train_set['Survived'] == 1)]))\n    survival_rate_arr.append(survived_arr[-1]\/total_arr[-1])\n\n    \ndf_title['Total'] = total_arr\ndf_title['Survived'] = survived_arr\ndf_title['Survival_rate'] = survival_rate_arr\n\ndf_title","cae3ea6b":"plt.figure(figsize = (15,5))\nplt_font = {'family':'serif' , 'size':16}\nplt.bar(x = df_title['Title'] , height = df_title['Total'])\nplt.bar(x = df_title['Title'] , height = df_title['Survived'])\nplt.title('Name titles' , fontdict =  plt_font)\nplt.ylabel('Total number' , fontdict = plt_font)\nplt.text(5,300,'orange bar is showing survived count' , fontdict = plt_font)","8288e179":"print(train_set.Title.unique())\nprint(\"-\"*70)\nprint(submission_set.Title.unique())","c0500bcd":"submission_set[submission_set.Title == 'Dona']","8488b98c":"train_set['Survival_rate'] = np.NaN\n\nfor i in range(len(train_set)):\n    train_set['Survival_rate'][i] = df_title[df_title['Title'] == train_set['Title'][i]]['Survival_rate']\n    \n# lets do the same thing for submission_set\nsubmission_set['Survival_rate'] = np.NaN\nfor i in range(len(submission_set)):\n    if i == 414:\n        submission_set['Survival_rate'][i] = 0.697802\n    else:\n        submission_set['Survival_rate'][i] = df_title[df_title['Title']==submission_set['Title'][i]]['Survival_rate']","0c16931f":"train_set.drop(labels = ['Name' , 'Title'] , axis = 1 , inplace = True)\nsubmission_set.drop(labels = ['Name' , 'Title'] , axis = 1 , inplace = True)","bfa43f7f":"family_size_train = []\nis_alone_train = []\n\nfor i in range(len(train_set)):\n    family_size_train.append(train_set['Parch'][i]+train_set['SibSp'][i])\n    if family_size_train[-1] == 0:\n        is_alone_train.append(1)\n    else:\n        is_alone_train.append(0)\n        \n        \n# family_size_subm = family size submission set\nfamily_size_subm = []\nis_alone_subm = []\n\nfor i in range(len(submission_set)):\n    family_size_subm.append(submission_set['Parch'][i]+submission_set['SibSp'][i])\n    if family_size_subm[-1] == 0:\n        is_alone_subm.append(1)\n    else:\n        is_alone_subm.append(0)\n        \ntrain_set['Family_size'] = family_size_train\ntrain_set['Is_alone'] = is_alone_train\n\nsubmission_set['Family_size'] = family_size_subm\nsubmission_set['Is_alone'] = is_alone_subm","451e2b05":"print(train_set.info())\nprint(\"-\"*30)\nprint(submission_set.info())","3d8ddd5c":"print(train_set.isnull().sum())\nprint(\"-\"*20)\nprint(submission_set.isnull().sum())","29aeb6ef":"train_set['Embarked'].replace({np.nan:train_set['Embarked'].mode()[0]} , inplace = True)","b13c7555":"print(\"percentage of missing ages :\" , train_set['Age'].isnull().sum()\/len(train_set))","95d37b8d":"sns.displot(data = train_set[train_set['Age'].notnull()] , x = 'Age' , kind = 'kde')\nplt.title(\"Age distribution before imputing\" , fontdict = plt_font)","be2cd0f7":"train_set['Age'].replace({np.nan:train_set['Age'].mean()} , inplace = True)","8143e589":"sns.displot(data = train_set[train_set['Age'].notnull()] , x = 'Age' , kind = 'kde')\nplt.title(\"Age distribution after imputing\" , fontdict = plt_font)","43cc8578":"submission_set['Age'].replace({np.nan:train_set['Age'].mean()} , inplace = True)","3ca49e1e":"submission_set.iloc[submission_set[submission_set.Fare.isnull()].index,:]","1691043c":"submission_set['Fare'][152] = train_set[(train_set.Pclass == 3) & (train_set.Embarked == 'S')]['Fare'].mean()","13572c0a":"train_set['Sex'].replace({'male':1 , 'female':0} , inplace = True)\nsubmission_set['Sex'].replace({'male':1 , 'female':0} , inplace = True)\n\ntrain_set = pd.concat([train_set , pd.get_dummies(train_set['Embarked'])] , axis = 1)\ntrain_set.drop('Embarked' , axis = 1 , inplace = True)\n\nsubmission_set = pd.concat([submission_set , pd.get_dummies(submission_set['Embarked'])] , axis = 1)\nsubmission_set.drop('Embarked' , axis = 1 , inplace = True)","d691b22d":"print(train_set.info())\nprint(\"-\"*30)\nprint(submission_set.info())","e64e3ff6":"corr = train_set.corr()\nplt.figure(figsize = (14,7))\nsns.heatmap(corr , cmap = 'coolwarm' , annot = True , fmt = '.2f')","7581ff06":"plt.subplots_adjust(left = 1 , bottom = 1 , right = 3 , top = 3 , wspace = 0.5 , hspace = None)\nplt.subplot(1,3,1)\nplt.pie(x = [len(train_set[train_set.Survived == 1]) , len(train_set.Survived == 0)]\n        , labels = ['survived' , 'dead'] , shadow = True , explode = [0.2 , 0] , startangle = -40 , \\\n       colors = ['tab:green' , 'tab:red'] , autopct = '%.1f%%')\nplt.title('Survived VS Dead' , fontdict = {'size':16})\n\n\nplt.subplot(1,3,2)\nplt.pie(x = [len(train_set[(train_set.Survived == 1)&(train_set.Sex == 1)]) ,\n             len(train_set[(train_set.Survived == 1)&(train_set.Sex == 0)])] , labels = ['Male' , 'Female']\\\n       , shadow = True , explode = [0.2 , 0] , startangle = -50 , \\\n       colors = ['tab:blue' , 'tab:pink'] , autopct = '%.1f%%')\nplt.title('Survived people' , fontdict = {'color':'tab:green' , 'size':16})\n\nplt.subplot(1,3,3)\nplt.pie(x = [len(train_set[(train_set.Survived == 0)&(train_set.Sex == 1)]) ,\n             len(train_set[(train_set.Survived == 0)&(train_set.Sex == 0)])] , labels = ['Male' , 'Female']\\\n       , shadow = True , explode = [0.2 , 0] , startangle = 30 , \\\n       colors = ['tab:blue' , 'tab:pink'] , autopct = '%.1f%%')\nplt.title('Dead people' , fontdict = {'color':'tab:red' , 'size':16})\n\n\nplt.show()","e817961c":"# n_c1 = number of people with ticket class 1\nn_c1 = len(train_set[train_set.Pclass == 1])\nn_c2 = len(train_set[train_set.Pclass == 2])\nn_c3 = len(train_set[train_set.Pclass == 3])\n\n# s_p_c1 = survival probability class 1\ns_p_c1 = len(train_set[(train_set.Pclass == 1) & (train_set.Survived == 1)]) \/ n_c1\ns_p_c2 = len(train_set[(train_set.Pclass == 2) & (train_set.Survived == 1)]) \/ n_c2\ns_p_c3 = len(train_set[(train_set.Pclass == 3) & (train_set.Survived == 1)]) \/ n_c3\n\nplt.subplots_adjust(left = 1 , bottom = 1 , right = 2.5 , top = 2 , wspace = 0.5 , hspace = None)\nplt.subplot(1,2,1)\nplt.plot([\"ticket class 1\",\"ticket class 2\",\"ticket class 3\"] , [s_p_c1 , s_p_c2 , s_p_c3] , linestyle = '--' , marker = 'D'\n        , color = 'tab:red')\nplt.title(\"Survival Probability per ticket class\" , fontdict = plt_font)\nplt.ylabel(\"Survival Probability\")\n\n\n# fm_c1 = fare mean ticket class 1\nfm_c1 = train_set[train_set.Pclass == 1]['Fare'].mean()\nfm_c2 = train_set[train_set.Pclass == 2]['Fare'].mean()\nfm_c3 = train_set[train_set.Pclass == 3]['Fare'].mean()\n\n\nplt.subplot(1,2,2)\nplt.plot([\"ticket class 1\",\"ticket class 2\",\"ticket class 3\"] , [fm_c1 , fm_c2 , fm_c3] , linestyle = '--' , marker = 'D'\n        , color = 'tab:blue')\nplt.title(\"Fare mean per ticket class\" , fontdict = plt_font)\nplt.ylabel(\"Fare mean\")\nplt.show()","bbe942bd":"fig, axes = plt.subplots(1 , 2)\nplt.subplots_adjust(left = 1 , bottom = 1 , right = 2.5 , top = 2 , wspace = 0.5 , hspace = None)\nsns.pointplot(data = train_set , x = 'Survived' , y = 'Fare' , color = 'tab:red' , ax = axes[0])\nsns.kdeplot(data = train_set , x = 'Fare' , hue = 'Survived'  , fill = True , ax = axes[1])","c3b4b5fb":"num_alone = len(train_set[train_set.Is_alone == 1])\nnum_family = len(train_set[train_set.Is_alone == 0])\n\nplt.bar(x = ['Alone' , 'With Family'] , height = [num_alone , num_family])\n\nnum_alone_survived = len(train_set[(train_set['Is_alone'] == 1) & train_set['Survived'] == 1])\nnum_family_survived = len(train_set[(train_set['Is_alone'] == 0) & train_set['Survived'] == 1])\n\nplt.bar(x = ['Alone' , 'With Family'] , height = [num_alone_survived , num_family_survived])\n\nplt.text(-0.4,160,'-'*84)\nplt.show()\nprint(\"Alone person chance to survive =\",num_alone_survived\/num_alone)\nprint(\"Person with family, chance to survive =\",num_family_survived\/num_family)","8fd6fd51":"scaler = StandardScaler()\n\n# for train_set first split to x and y\nx_set = train_set.iloc[:,1:]\ny_set = train_set.iloc[:,0]\n\nx_set = scaler.fit_transform(x_set)\n\n\n# for submission_set\nsubmission_set = scaler.transform(submission_set)\n\n\nprint(submission_set.shape)\nprint(x_set.shape)\n# these two should be same in columns","2b7661b0":"lr = LogisticRegression(random_state = 1)\nlr.fit(x_set , y_set)\nlr_accuracies = cross_val_score(estimator = lr , X = x_set , y = y_set , cv = 10 , scoring = 'accuracy')\nlr_acc = lr_accuracies.mean()\nlr_f1 = cross_val_score(estimator = lr , X = x_set , y = y_set , cv = 10 , scoring = 'f1')\nlr_f1 = lr_f1.mean()\nprint(\"accuracy =\",lr_acc)\nprint(\"f1 =\",lr_f1)","03c86291":"knc = KNeighborsClassifier(n_neighbors = 5)\nknc.fit(x_set , y_set)\nknc_accuracies = cross_val_score(estimator = knc , X = x_set , y = y_set , cv = 10 , scoring = 'accuracy' )\nknc_acc = knc_accuracies.mean()\nknc_f1 = cross_val_score(estimator = knc , X = x_set , y = y_set , cv = 10 , scoring = 'f1')\nknc_f1 = knc_f1.mean()\nprint(\"accuracy = \" , knc_acc)\nprint(\"f1 = \" , knc_f1)","1858bef3":"gnb = GaussianNB()\ngnb.fit(x_set , y_set)\ngnb_accuracies = cross_val_score(estimator = gnb , X = x_set  , y = y_set , cv = 10 , scoring = 'accuracy')\ngnb_acc = gnb_accuracies.mean()\ngnb_f1 = cross_val_score(estimator = gnb , X = x_set  , y = y_set , cv = 10 , scoring = 'f1')\ngnb_f1 = gnb_f1.mean()\nprint(\"accuracy =\" , gnb_acc)\nprint(\"f1 =\" , gnb_f1)","82262918":"\ncv_acc_arr = []\ntrain_acc = []\nbest_acc = 0\nbest_k = 0\n\nfor k in range(1,91):\n    ct = DecisionTreeClassifier(min_samples_leaf = k)\n    ct.fit(x_set , y_set)\n    acc = cross_val_score(estimator = ct , X = x_set , y = y_set , cv = 10 , scoring = 'accuracy')\n    acc = acc.mean()\n    cv_acc_arr.append(acc)\n    train_acc.append(sklearn.metrics.accuracy_score(y_set , ct.predict(x_set)))\n    if acc > best_acc :\n        best_acc = acc\n        best_k = k\n        \nprint(\"best accuracy = %d -- %f\" %(best_k , best_acc))\nplt.plot(np.arange(1,91),cv_acc_arr , color = 'tab:green')\nplt.plot(np.arange(1,91),train_acc , color = 'tab:red')","d47e3dc4":"ct = DecisionTreeClassifier(min_samples_leaf = 43)\nct.fit(x_set , y_set)\nct_accuracies = cross_val_score(estimator = ct , X = x_set , y = y_set , cv = 10 , scoring = 'accuracy')\nct_acc = ct_accuracies.mean()\nct_f1 = cross_val_score(estimator = ct , X = x_set , y = y_set , cv = 10 , scoring = 'f1')\nct_f1 = ct_f1.mean()\nprint(\"accuracy =\" , ct_acc)\nprint(\"f1 =\" , ct_f1)","def95d98":"svc = SVC(random_state = 1 , C = 0.5 , gamma = 0.1 , kernel = 'rbf')\nsvc.fit(x_set , y_set)\n\nsvc_accuracies = cross_val_score(estimator = svc , X = x_set , y = y_set , cv = 10 , scoring = 'accuracy')\nsvc_acc = svc_accuracies.mean()\nsvc_f1 = cross_val_score(estimator = svc , X = x_set , y = y_set , cv = 10 , scoring = 'f1')\nsvc_f1 = svc_f1.mean()\n\nprint(\"accuracy =\" , svc_acc)\nprint(\"f1 =\" , svc_f1)","40100571":"rfc = RandomForestClassifier(random_state = 1 , max_depth = 7 , max_features='auto' , n_estimators = 900)\nrfc.fit(x_set , y_set)\nrfc_accuracies = cross_val_score(estimator = rfc , X = x_set , y = y_set , cv = 10 , scoring = 'accuracy')\nrfc_acc = rfc_accuracies.mean()\nrfc_f1 = cross_val_score(estimator = rfc , X = x_set , y = y_set , cv = 10 , scoring = 'f1')\nrfc_f1 = rfc_f1.mean()\nprint(\"accuracy = \" , rfc_acc)\nprint(\"f1 =\" , rfc_f1)","0b021619":"\nada_boost = AdaBoostClassifier(random_state = 1 , learning_rate = 0.01 , n_estimators = 300)\nada_boost.fit(x_set , y_set)\nada_boost_accuracies = cross_val_score(estimator = ada_boost , X = x_set , y = y_set , cv = 10 ,scoring = 'accuracy')\nada_boost_acc = ada_boost_accuracies.mean()\nada_boost_f1 = cross_val_score(estimator = ada_boost , X = x_set , y = y_set , cv = 10 ,scoring = 'f1')\nada_boost_f1 = ada_boost_f1.mean()\nprint(\"accuracy =\" , ada_boost_acc)\nprint(\"f1 =\" , ada_boost_f1)","e30128bc":"xgb = XGBClassifier(gamma=0.3 , learning_rate=0.005 , max_depth=5 , n_estimators=1800 , subsample=1.)\nxgb.fit(x_set , y_set)\nxgb_accuracies = cross_val_score(estimator = xgb , X = x_set , y = y_set , cv = 10 , scoring = 'accuracy')\nxgb_acc = xgb_accuracies.mean()\nxgb_f1 = cross_val_score(estimator = xgb , X = x_set , y = y_set , cv = 10 , scoring = 'f1')\nxgb_f1 = xgb_f1.mean()\nprint(\"accuracy =\" , xgb_acc)\nprint(\"f1 =\" , xgb_f1)","724ac3ec":"x = [lr_acc,knc_acc,gnb_acc,ct_acc,svc_acc,rfc_acc,ada_boost_acc,xgb_acc]\ny = [lr_f1,knc_f1,gnb_f1,ct_f1,svc_f1,rfc_f1,ada_boost_f1,xgb_f1]\nlabels = ['lr','knc','gnb','ct','svc','rfc','ada','xgb']\n\nplt.xlim([0.78,0.86])\nplt.ylim([0.72,0.8])\n\nfor i in range(len(labels)):\n    plt.annotate(labels[i], (x[i], y[i]))\n\nplt.title(\"Model comparison\" , fontdict = plt_font)\nplt.xlabel(\"Accuracy\" , fontdict = plt_font)\nplt.ylabel(\"F1 score\" , fontdict = plt_font)","633438a0":"final_pred = rfc.predict(submission_set)\n\nfinal_sub = pd.DataFrame()\nfinal_sub['PassengerId'] = submission_pId\nfinal_sub['Survived'] = final_pred\nfinal_sub.to_csv('.\/submission.csv' , index = False)","c31690d6":"<a id=\"comparison\"><\/a>\n<h2 style=\"color:tomato\">7. Models comparison<\/h2>","726aa5f1":"<h2 style=\"color:#005b96\">5-5. Bar plot results :<\/h2>\n\n- <h3>Alone people are more than people with their family in this ship!<\/h3>\n- <h3>If you are alone you have 0.3 chance to survive, but with family you have 0.51 chance to survive.<\/h3>","598aa14d":"<a id=\"IL\"><\/a>\n## 1. Importing the libraries","28544766":"- <h2 style=\"color:#009b96\">Every thing seems to be good.<\/h2>\n- <h2 style=\"color:#009b96\">I will do scaling in part 4-7, after the visualiaztion.<\/h2>","4278884d":"<h2 style=\"color:#005b96\">For more robust evaluation we will use 10_fold cross validation :<\/h2>","ca9afc28":"<meta charset=\"UTF-8\">\n<h2 style=\"color:#005b96\">5-3. Line plot results :<\/h2>\n\n| | First class      | Second class  | Third class\n|:-|:-:|:-:|:-:\n| Total          |216    |184   |491    \n| Survived       |136    |87    |119\n| Survival probability  |0.63   |0.47  |0.24\n\n- <h2 style=\"color:tomato\">Money Talks ?... \ud83d\udcb8\ud83d\udcb8\ud83d\udcb8<\/h2>\n- <h3>People with third class ticket have the lowest survival probability on the other side people in first class have the highest survival probability!<\/h3>\n- <h3>In other words people in first class have 63% chance to survive,but people in third class have only 24% chance to survive!<\/h3>\n<p>Titanic was a luxurious ship and tickets were expensive. A third class ticket cost around \u00a37 in 1912 which is nearly \u00a3800 in today's money. A second class ticket cost around \u00a313 or nearly \u00a31500 today and a first class ticket would have set you back a minimum of \u00a330 or more than \u00a33300 today.<\/p>\n<b><a href=\"https:\/\/www.bbc.co.uk\/bitesize\/topics\/z8mpfg8\/articles\/zng8jty\">view source<\/a><\/b>","4170db06":"- <h2 style = 'color:#005b96'>As i mentioned the evaluation results show that XGB performance is the best. But random forest gets a better score on the unseen data.Is there a more robust way to evaluate our models? i appreciate if we can discuss about it in the comments. or if you know a similar discussion on kaggle i would also appreciate to give me the link.<\/h2>\n- <h2 style = 'color:#005b96'>Hope to be helpful<\/h2>\n- <h2 style = 'color:#005b96'>Thanks for reading<\/h2>","fd0d7e5f":"<h3 style=\"color:#005b96\">What about null Ages ?<\/h3>\n\n- <h2> train_set :<\/h2>","947eeaa6":"<h2 style=\"color:#005b96\">4-2. Extracting Survival_rate from Name :<\/h2>","b8dc2600":"<a id=\"RF\"><\/a>\n- <h2 style=\"color:tomato\">6-6. Random Forest :<\/h2>","f238332e":"<h1 style=\"color:tomato;\">I should be careful !!!<\/h1>\n\n-  As you see in above, we dont have Dona in the train_set.\n-  So we dont have any survival_rate for this person.\n-  If you google her name you will find out that she survived,<b> but i dont like to cheat.<\/b>\n-  By looking at the above cell, we find out that this person is female and alone(Sib + Parch = 0)\n-  As she is travelling alone maybe we could say she is a Miss.\n-  So we put 0.697802 for her Survival_rate.","5a137cbe":"<a id=\"LD\"><\/a>\n## 2. Load and Prepare Data","98e25547":"<h1 style = 'color:tomato'>But after several submissions, i find out that random forest classifier has a better result on the unseen data!<\/h1>","8a68b423":"<a id=\"Ada\"><\/a>\n- <h2 style=\"color:tomato\">6-7. Ada boost :<\/h2>","3c3afe96":"<h2 style=\"color:#009b96\">4-7. Scaling the input values :<\/h2>","888dbbaa":"<h2 style=\"color:#005b96\">Untill now :<\/h2>","39863af9":"## 6. Train the model (Classification)","c3f9d07b":"<h2 style=\"color:#005b96\">Lets drop Name and Title columns :<\/h2>","cca4ffdc":"<h2 style=\"color:#005b96\">4-6. Encoding :<\/h2>","37baf04d":"- <h2 style=\"color:#00fb96\">In the above plot each point is representing a model.<\/h2>\n- <h2 style=\"color:#00fb96\">x-axis is showing the model accuracy and y-axis is showing the model f1 score.<\/h2>\n- <h2 style=\"color:#00fb96\">As the model(each point) goes to the top right corner of the plot, its a better model with respect of accuracy and f1.<\/h2>\n- <h2 style=\"color:#00fb96\">So here the plot is showing that XGB is the best model.<\/h2>","19a4bccb":"<a id=\"LR\"><\/a>\n- <h2 style=\"color:tomato\">6-1. Logistic regression :<\/h2>","c9927303":"<h2 style=\"color:#005b96\">5-2. Pie plot results <\/h2>\n<h2 style=\"color:#005b96\">lets describe it easily :<\/h2>\n\n\n- <h3>Unfortunately out of every 18 people, 13 are dead and 5 people survived.<\/h3>\n- <h3>Out of every 25 people who survived, 17 are female and 8 are male.<\/h3>\n- <h3>Out of every 27 people who died, 23 are male and 4 are female<\/h3> ","cd977d74":"- <h3 style=\"color:#005b96\">Replace null Embarked values in train_set with Embarked most frequent value :<\/h3>","55ee1f5e":"<h1 style=\"color:tomato;\">What will i do in this notebook?<\/h1>\n\n    \n* [1.Importing libraries](#IL)\n\n* [2.Loading the data](#LD)\n\n* [3.Abstract EDA](#EDA)\n\n* [4.Preprocessing](#Preprocessing)\n\n* [5.Story telling and visualization](#Strorytelling-Visualization)\n\n* [6-1.Logistic regression](#LR)\n\n* [6-2.K-nearest neighbors](#KNN)\n\n* [6-3.Gaussian naive bayes](#GNB)\n\n* [6-4.Classification Tree](#CT)\n\n* [6-5.Support vector Classifier](#SVC)\n\n* [6-6.Random forest](#RF)\n\n* [6-7.AdaBoost](#Ada)\n\n* [6-8.XGBoost](#XGB)\n\n* [7.Models comparison](#comparison)","fe872c2a":"<h2 style=\"color:#005b96\">5-4. point-plot and kde results :<\/h2>\n\n- <h3>More fare, more chance to survive!<\/h3>\n- <h3>Right plot is showing that by increasing the fare, number of survived is more than dead people.<\/h3>\n- <h3>As you see in the kde plot by going to the right, the orange line is on the top of the blue line.<\/h3>","e39c82cc":"<a id=\"Strorytelling-Visualization\"><\/a>\n## 5. Strorytelling - Visualization","7c215f4b":"<h3 style=\"color:#005b96\">4-5. One fare missing in submission_set :<\/h3>","c5e8f285":"<h2 style=\"color:#005b96\">4-3. Adding Family_size and Is_alone features :<\/h2>","274ba6e7":"<h2 style=\"color:#005b96\">4-4. Missing values :<\/h2>","7b595e9e":"<h2 style=\"color:#005b96\">Lets check the results :<\/h2>","b58f056f":"- This passenger is in third class and the Embarked is S.\n- So i fill his Fare with Fare mean value in train_set with above features.","9fd21040":"<a id=\"CT\"><\/a>\n- <h2 style=\"color:tomato\">6-4. Classification tree :<\/h2>","8183c623":"- <h3 style='color:tomato'>By doing this Imputation we moved the kernel from 24 to 29.699(the mean value)<\/h3>\n- <h2 style=\"color:#50fb56\">And because we replaced the missing ages with the mean age value, so the new mean is equal to the previous one.<\/h2>","31bd8b76":"<a id=\"SVC\"><\/a>\n- <h2 style=\"color:tomato\">6-5. Support vector classifier :<\/h2>","609557ae":"<a id=\"XGB\"><\/a>\n- <h2 style=\"color:tomato\">6-8. XGboost :<\/h2>","11e10111":"- <h2 style=\"color:#005b96\">As you see when we put small values for min_samples_leaf, we face to overfitting.<\/h2>\n- <h2 style=\"color:#005b96\">So best min_samples_leaf = 43<\/h2>","cea3eb2c":"<h1 style=\"color:tomato;\">4-1. Note :<\/h1>\n\n-  I will not use PassengerId, Ticket and Cabin\n-  So lets drop them both from train_set and submission_set\n-  But i will store submission_set passengerId, because we will need it later.","9551f698":"<a id=\"GNB\"><\/a>\n- <h2 style=\"color:tomato\">6-3. Gaussian Naive Bayes :<\/h2>","981cc3aa":"- <h3> As you see if we drop all rows with null ages, we lose 20% of the data.<\/h3>\n- <h3> So i will replace them with mean of the ages.<\/h3>","cd3fbf33":"<a id=\"EDA\"><\/a>\n## 3. EDA","d139c853":"- <h2> submission_set :<\/h2>\n- I will do the same thing on submission_set\n- I replace missing ages <u>with the train_set ages mean<\/u> (however there is not a big difference between train and submission in ages mean)!","0a5ac854":"<a id=\"Preprocessing\"><\/a>\n## 4. Data Preprocessing","4f839a9c":"<h2 style=\"color:#005b96\">5-1. Heat map results :<\/h2>\n\n- <h3><u>Survival_rate(with respect of name title)<\/u>, <u>Gender<\/u>, <u>ticket class<\/u>,<u>passanger fare<\/u> and <u>to be alone or not<\/u> are the most effective features on Survival !<\/h3>","e80a8e46":"<h2 style=\"color:#005b96\">Now its time to assign survival_rate to each person in train_set and submission_set :<\/h2>","e8473ca3":"<a id=\"KNN\"><\/a>\n- <h2 style=\"color:tomato\">6-2. KNN classifier :<\/h2>"}}