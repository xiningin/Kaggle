{"cell_type":{"b3df47e6":"code","bede0f19":"code","3d1278f1":"code","200ba311":"code","f48f2067":"code","0196f1c5":"code","b235d7e5":"code","df719192":"code","2682b792":"code","af5b6688":"code","2ab64418":"code","45e96cba":"code","849c702f":"code","171c9beb":"code","6b6cb688":"code","b392bcb4":"code","81514132":"code","208bd2d3":"code","e9886abb":"code","ae5d2630":"markdown","844dc22f":"markdown","109c6e37":"markdown","54651ff0":"markdown","e667d814":"markdown","dba230e0":"markdown"},"source":{"b3df47e6":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","bede0f19":"dataset_columns = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\ndataset_encode = \"ISO-8859-1\"\ntweet_df = pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv',\n                      encoding=dataset_encode , names=dataset_columns)\n\ntweet_df.head()","3d1278f1":"tweet_df['sentiment'].value_counts()","200ba311":"new_df = tweet_df[['sentiment', 'text']]\nnew_df.head()","f48f2067":"# Separating positive and negative rows\n\ndf_pos = tweet_df[tweet_df['sentiment'] == 4]\ndf_neg = tweet_df[tweet_df['sentiment'] == 0]\nprint(len(df_pos), len(df_neg))","0196f1c5":"# Concatinating both positive and negative groups and storing them back into a single dataframe\n\nnew_df = pd.concat([df_pos, df_neg])\nlen(new_df)","b235d7e5":"from nltk.tokenize import TweetTokenizer\nfrom time import time","df719192":"# The reduce_len parameter will allow a maximum of 3 consecutive repeating characters, while trimming the rest\n# For example, it will tranform the word: 'Helloooooooooo' to: 'Hellooo'\nstart_time = time()\ntoken = TweetTokenizer(reduce_len=True)\n\ndata = []\n\n# Separating our features (text) and our labels into two lists to smoothen our work\nX = new_df['text'].tolist()\nY = new_df['sentiment'].tolist()\n\n# Building data list, that is a list of tuples, where each tuple is a pair of the tokenized text\n# and its corresponding label\nX = new_df['text'].tolist()\nY = new_df['sentiment'].tolist()\n\nfor x, y in zip(X, Y):\n    if y == 4:\n        data.append((token.tokenize(x), 1))\n    else:\n        data.append((token.tokenize(x), 0))\n\nprint('CPU Time:', time() - start_time)\ndata[:5]","2682b792":"import nltk\nnltk.download('all')","af5b6688":"from nltk.tag import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\n# Previewing the pos_tag() output\nprint(pos_tag(data[0][0]))","2ab64418":"def lemmatize_sentence(tokens):\n    lemmatizer = WordNetLemmatizer()\n    lemmatized_sentence = []\n    for word, tag in pos_tag(tokens):\n        # First, we will convert the pos_tag output tags to a tag format that the WordNetLemmatizer can interpret\n        # In general, if a tag starts with NN, the word is a noun and if it stars with VB, the word is a verb.\n        if tag.startswith('NN'):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n    return lemmatized_sentence\n\n# Previewing the WordNetLemmatizer() output\nprint(lemmatize_sentence(data[0][0]))","45e96cba":"import re, string\n\n# Stopwords are frequently-used words (such as \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cin\u201d) that do not hold any meaning useful to extract sentiment.\n# If it's your first time ever using nltk, you can download nltk's stopwords using: nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nSTOP_WORDS = stopwords.words('english')\n\n# A custom function defined in order to fine-tune the cleaning of the input text. This function is highly dependent on each usecase.\n# Note: Only include misspelling or abbreviations of commonly used words. Including many minimally present cases would negatively impact the performance. \ndef cleaned(token):\n    if token == 'u':\n        return 'you'\n    if token == 'r':\n        return 'are'\n    if token == 'some1':\n        return 'someone'\n    if token == 'yrs':\n        return 'years'\n    if token == 'hrs':\n        return 'hours'\n    if token == 'mins':\n        return 'minutes'\n    if token == 'secs':\n        return 'seconds'\n    if token == 'pls' or token == 'plz':\n        return 'please'\n    if token == '2morow':\n        return 'tomorrow'\n    if token == '2day':\n        return 'today'\n    if token == '4got' or token == '4gotten':\n        return 'forget'\n    if token == 'amp' or token == 'quot' or token == 'lt' or token == 'gt' or token == '\u00bd25':\n        return ''\n    return token","849c702f":"# This function will be our all-in-one noise removal function\ndef remove_noise(tweet_tokens):\n\n    cleaned_tokens = []\n\n    for token, tag in pos_tag(tweet_tokens):\n        # Eliminating the token if it is a link\n        token = re.sub('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n        # Eliminating the token if it is a mention\n        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n\n        if tag.startswith(\"NN\"):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n\n        lemmatizer = WordNetLemmatizer()\n        token = lemmatizer.lemmatize(token, pos)\n\n        cleaned_token = cleaned(token.lower())\n        \n        # Eliminating the token if its length is less than 3, if it is a punctuation or if it is a stopword\n        if cleaned_token not in string.punctuation and len(cleaned_token) > 2 and cleaned_token not in STOP_WORDS:\n            cleaned_tokens.append(cleaned_token)\n            \n    return cleaned_tokens","171c9beb":"# Prevewing the remove_noise() output\nprint(remove_noise(data[0][0]))","6b6cb688":"start_time = time()\n\n# As the Naive Bayesian classifier accepts inputs in a dict-like structure,\n# we have to define a function that transforms our data into the required input structure\ndef list_to_dict(cleaned_tokens):\n    return dict([token, True] for token in cleaned_tokens)\n\ncleaned_tokens_list = []\n\n# Removing noise from all the data\nfor tokens, label in data:\n    cleaned_tokens_list.append((remove_noise(tokens), label))\n\nprint('Removed Noise, CPU Time:', time() - start_time)\nstart_time = time()\n\nfinal_data = []\n\n# Transforming the data to fit the input structure of the Naive Bayesian classifier\nfor tokens, label in cleaned_tokens_list:\n    final_data.append((list_to_dict(tokens), label))\n    \nprint('Data Prepared for model, CPU Time:', time() - start_time)\n\n# Previewing our final (tokenized, cleaned and lemmatized) data list\nfinal_data[:5]","b392bcb4":"import random\n\n# As data is currently ordered by label, we have to shuffle it before splitting it\n# .Random(140) randomizes our data with seed = 140. This guarantees the same shuffling for every execution of our code\nrandom.Random(140).shuffle(final_data)\n\n# 90% train data and 10% test data\ntrim_index = int(len(final_data) * 0.9)\n\ntrain_data = final_data[:trim_index]\ntest_data = final_data[trim_index:]","81514132":"# Training the model\n\nstart_time = time()\n\nfrom nltk import classify\nfrom nltk import NaiveBayesClassifier\nclassifier = NaiveBayesClassifier.train(train_data)\n\nprint('Accuracy on train data:', classify.accuracy(classifier, train_data))\nprint('Accuracy on test data:', classify.accuracy(classifier, test_data))\n\nprint(classifier.show_most_informative_features(20))\n\nprint('\\nCPU Time:', time() - start_time)","208bd2d3":"custom_tweet = \"I ordered just once from Veg Treat, they screwed up, never used the app again.\"\n\ncustom_tokens = remove_noise(token.tokenize(custom_tweet))\n\nprint(classifier.classify(dict([token, True] for token in custom_tokens)))","e9886abb":"custom_tweet = \"I loved the show today! It was amazing.\"\n\ncustom_tokens = remove_noise(token.tokenize(custom_tweet))\n\nprint(classifier.classify(dict([token, True] for token in custom_tokens)))","ae5d2630":"# **Twitter Sentiment Analysis**","844dc22f":"**3. Cleaning the Data**","109c6e37":"# **Cleaning and Processing the Data** \n\n**1. Tokenization**","54651ff0":"**2. Lemmatization**","e667d814":"**Testing the Model** ","dba230e0":"# **Prediction**\n\n**For this I have used naive bayes algorithm**"}}