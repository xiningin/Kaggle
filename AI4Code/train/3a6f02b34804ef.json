{"cell_type":{"67a8da27":"code","072937a1":"code","74cc0bda":"code","85ba0ae2":"code","243ec616":"code","8280c436":"code","21510682":"code","6752c042":"code","44d3e9d8":"code","c4f2e5d6":"code","d92b362d":"code","4d23a28c":"code","77357082":"code","0172d072":"code","53489498":"code","9c1e0d63":"code","750d03d9":"code","fe2b255a":"code","563b177a":"code","06a41936":"code","af6d9cbd":"code","e2b57a96":"code","c972b51b":"code","8e490aab":"code","b04a6f31":"code","cb2eda88":"code","ef2c2828":"code","700648d1":"code","aa8785d7":"code","adb760c7":"code","d0637b8e":"markdown","a1dcdf47":"markdown","0eaa7bfc":"markdown","8f80c57c":"markdown"},"source":{"67a8da27":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","072937a1":"# Import Libraries\nnp.random.seed(0) \nimport random\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","74cc0bda":"# Prepare Dataset\n# load data\ntrain = pd.read_csv(r\"..\/input\/fashionmnist\/fashion-mnist_train.csv\",dtype = np.float32)\ntest =  pd.read_csv(r\"..\/input\/fashionmnist\/fashion-mnist_test.csv\",dtype = np.float32)","85ba0ae2":"test.describe()","243ec616":"train.describe()","8280c436":"# split data into features(pixels) and labels(numbers from 0 to 9)\ny_train = train['label']\nX_train = train.drop(labels = ['label'], axis = 1)\/255 # normalization\nX_test = test.drop(labels = ['label'], axis = 1)\/255","21510682":"g = sns.countplot(y_train)\n\ny_train.value_counts()","6752c042":"print(X_train.shape)\nprint(y_train.shape)","44d3e9d8":"X_train_plot = X_train.values.reshape(-1,28,28)","c4f2e5d6":"fashion_map = { 0 :'T-shirt\/top',1 :'Trouser', 2 :'Pullover', 3 :'Dress',\n                4 : 'Coat', 5 : 'Sandal', 6 : 'Shirt', 7 : 'Sneaker',\n                8 : 'Bag', 9 : 'Ankle boot'}","d92b362d":"def Show_example_fashions(mono = 'gray'):\n    fig = plt.figure(figsize = (16, 16))\n    for idx in range(15):\n        plt.subplot(5, 5,idx+1)\n        plt.imshow(X_train_plot[idx], cmap = mono)\n        plt.title(\"Number {}\".format(fashion_map[y_train[idx]]))\n        \n    plt.tight_layout()\n    \nShow_example_fashions()","4d23a28c":"\n# Function return digit in grayscale\ndef plot_fashion(digit, dem = 28, font_size = 12):\n    max_ax = font_size * dem\n    \n    fig = plt.figure(figsize=(13, 13))\n    plt.xlim([0, max_ax])\n    plt.ylim([0, max_ax])\n    plt.axis('off')\n    black = '#000000'\n    \n    for idx in range(dem):\n        for jdx in range(dem):\n\n            t = plt.text(idx * font_size, max_ax - jdx*font_size, digit[jdx][idx], fontsize = font_size, color = black)\n            c = digit[jdx][idx] \/ 255.\n            t.set_bbox(dict(facecolor=(c, c, c), alpha = 0.5, edgecolor = 'black'))\n            \n    plt.show()","77357082":"rand_number = random.randint(0, len(y_train))\nprint(fashion_map[y_train[rand_number]])\nplot_fashion(X_train_plot[rand_number])","0172d072":"# train test split. Size of train data is 80% and size of test data is 20%. \nfeatures_train, features_test, targets_train, targets_test = train_test_split(X_train,\n                                                                             y_train,\n                                                                             test_size = 0.2,\n                                                                             random_state = 42) \n\n# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\nfeaturesTrain = torch.from_numpy(features_train.values)\ntargetsTrain = torch.from_numpy(targets_train.values).type(torch.LongTensor) # data type is long\n\n# create feature and targets tensor for test set.\nfeaturesTest = torch.from_numpy(features_test.values)\ntargetsTest = torch.from_numpy(targets_test.values).type(torch.LongTensor) # data type is long","53489498":"X_test.shape","9c1e0d63":"X_Test = torch.from_numpy(X_test.values)","750d03d9":"print(len(features_train))\nprint(features_train.shape)","fe2b255a":"# batch_size, epoch and iteration\nbatch_size = 100\nn_iters = 14000\nnum_epochs = n_iters \/ (len(features_train) \/ batch_size)\nnum_epochs = int(num_epochs)\nprint(num_epochs)","563b177a":"# Pytorch train and test sets\ntrain = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\ntest = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n\n# data loader\ntrain_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\ntest_loader = DataLoader(test, batch_size = batch_size, shuffle = False)","06a41936":"# Create Logistic Regression Model\nclass LogisticRegressionModel(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LogisticRegressionModel, self).__init__()\n        # Linear part\n        self.linear = nn.Linear(input_dim, output_dim)\n        # There should be logistic function right?\n        # However logistic function in pytorch is in loss function\n        # So actually we do not forget to put it, it is only at next parts\n    \n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\n# Instantiate Model Class\ninput_dim = 28*28 # size of image px*px\noutput_dim = 10  # labels 0,1,2,3,4,5,6,7,8,9\n\n# create logistic regression model\nmodel = LogisticRegressionModel(input_dim, output_dim)\n\n# Cross Entropy Loss  \nerror = nn.CrossEntropyLoss()\n\n# SGD Optimizer \nlearning_rate = 0.001\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","af6d9cbd":"# Traning the Model\ncount = 0\nloss_list = []\niteration_list = []\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        \n        # Define variables\n        train = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)\n        \n        # Calculate softmax and cross entropy loss\n        loss = error(outputs, labels)  #loss\u7684\u7ed3\u679c\u4f1a\u5b58\u5230train\u7684Variable\u4e2d\n        \n        # Calculate gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        count += 1\n        \n        # Prediction\n        if count % 50 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Predict test dataset\n            for images, labels in test_loader: \n                test = Variable(images.view(-1, 28*28))\n                \n                # Forward propagation\n                outputs = model(test)\n                \n                # Get predictions from the maximum value\n                predicted = torch.max(outputs.data, 1)[1]\n                \n                # Total number of labels\n                total += len(labels)\n                \n                # Total correct predictions\n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct \/ float(total)\n            \n            # store loss and iteration\n            loss_list.append(loss.data)\n            iteration_list.append(count)\n        if count % 500 == 0:\n            # Print Loss\n            print('Iteration: {}  Loss: {}  Accuracy: {}%'.format(count, loss.data, accuracy))","e2b57a96":"# visualization\nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Logistic Regression: Loss vs Number of iteration\")\nplt.show()","c972b51b":"featuresTrain.shape","8e490aab":"test = Variable(X_Test.view(-1,28*28))","b04a6f31":"output = model(test)\n\npredicted = torch.max(outputs.data, 1)[1]\n\npredicted_numpy = predicted.numpy()\n\noutput = pd.Series(predicted_numpy, name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,10001), name = \"ImageId\"), output], axis = 1)\n\nsubmission.to_csv(\"logstic_submission.csv\", index=False)","cb2eda88":"class ANNModel(nn.Module):\n    \n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(ANNModel, self).__init__()\n        \n        # Linear function 1: 784 --> 150\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity 1\n        self.relu1 = nn.ReLU()\n        \n        # Linear function 2: 150 --> 150\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        # Non-linearity 2\n        self.tanh2 = nn.Tanh()\n        \n        # Linear function 3: 150 --> 150\n        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n        # Non-linearity 3\n        self.elu3 = nn.ELU()\n        \n        # Linear function 4 (readout): 150 --> 10\n        self.fc4 = nn.Linear(hidden_dim, output_dim)  \n    \n    def forward(self, x):\n        # Linear function 1\n        out = self.fc1(x)\n        # Non-linearity 1\n        out = self.relu1(out)\n        \n        # Linear function 2\n        out = self.fc2(out)\n        # Non-linearity 2\n        out = self.tanh2(out)\n        \n        # Linear function 2\n        out = self.fc3(out)\n        # Non-linearity 2\n        out = self.elu3(out)\n        \n        # Linear function 4 (readout)\n        out = self.fc4(out)\n        return out\n\n# instantiate ANN\ninput_dim = 28*28\nhidden_dim = 150 #hidden layer dim is one of the hyper parameter and it should be chosen and tuned. For now I only say 150 there is no reason.\noutput_dim = 10\n\n# Create ANN\nmodel = ANNModel(input_dim, hidden_dim, output_dim)\n\n# Cross Entropy Loss \nerror = nn.CrossEntropyLoss()\n\n# SGD Optimizer\nlearning_rate = 0.02\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","ef2c2828":"# ANN model training\ncount = 0\nloss_list = []\niteration_list = []\naccuracy_list = []\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n\n        train = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)\n        \n        # Calculate softmax and ross entropy loss\n        loss = error(outputs, labels)\n        \n        # Calculating gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        count += 1\n        \n        if count % 50 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Predict test dataset\n            for images, labels in test_loader:\n\n                test = Variable(images.view(-1, 28*28))\n                \n                # Forward propagation\n                outputs = model(test)\n                \n                # Get predictions from the maximum value\n                predicted = torch.max(outputs.data, 1)[1]\n                \n                # Total number of labels\n                total += len(labels)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct \/ float(total)\n            \n            # store loss and iteration\n            loss_list.append(loss.data)\n            iteration_list.append(count)\n            accuracy_list.append(accuracy)\n        if count % 500 == 0:\n            # Print Loss\n            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))","700648d1":"output = model(test)\n\npredicted = torch.max(outputs.data, 1)[1]\n\npredicted_numpy = predicted.numpy()\n\noutput = pd.Series(predicted_numpy, name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,10001), name = \"ImageId\"), output], axis = 1)\n\nsubmission.to_csv(\"ANN_submission.csv\", index=False)","aa8785d7":"hidden_dim = 28*28*2 + 1\nmodel = ANNModel(input_dim, hidden_dim, output_dim)\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n# ANN model training\ncount = 0\nloss_list = []\niteration_list = []\naccuracy_list = []\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n\n        train = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)\n        \n        # Calculate softmax and ross entropy loss\n        loss = error(outputs, labels)\n        \n        # Calculating gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        count += 1\n        \n        if count % 50 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Predict test dataset\n            for images, labels in test_loader:\n\n                test = Variable(images.view(-1, 28*28))\n                \n                # Forward propagation\n                outputs = model(test)\n                \n                # Get predictions from the maximum value\n                predicted = torch.max(outputs.data, 1)[1]\n                \n                # Total number of labels\n                total += len(labels)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct \/ float(total)\n            \n            # store loss and iteration\n            loss_list.append(loss.data)\n            iteration_list.append(count)\n            accuracy_list.append(accuracy)\n        if count % 500 == 0:\n            # Print Loss\n            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))","adb760c7":"output = model(test)\n\npredicted = torch.max(outputs.data, 1)[1]\n\npredicted_numpy = predicted.numpy()\n\noutput = pd.Series(predicted_numpy, name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,10001), name = \"ImageId\"), output], axis = 1)\n\nsubmission.to_csv(\"ANN_submission_hyperparameters.csv\", index=False)","d0637b8e":"\u5f53\u8bad\u7ec3\u96c6\u786e\u5b9a\u4e4b\u540e\uff0c\u8f93\u5165\u5c42\u7ed3\u70b9\u6570\u548c\u8f93\u51fa\u5c42\u7ed3\u70b9\u6570\u968f\u4e4b\u800c\u786e\u5b9a\uff0c\u9996\u5148\u9047\u5230\u7684\u4e00\u4e2a\u5341\u5206\u91cd\u8981\u800c\u53c8\u56f0\u96be\u7684\u95ee\u9898\u662f\u5982\u4f55\u4f18\u5316\u9690\u5c42\u7ed3\u70b9\u6570\u548c\u9690\u5c42\u6570\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5982\u679c\u9690\u5c42\u7ed3\u70b9\u6570\u8fc7\u5c11\uff0c\u7f51\u7edc\u4e0d\u80fd\u5177\u6709\u5fc5\u8981\u7684\u5b66\u4e60\u80fd\u529b\u548c\u4fe1\u606f\u5904\u7406\u80fd\u529b\u3002\u53cd\u4e4b\uff0c\u82e5\u8fc7\u591a\uff0c\u4e0d\u4ec5\u4f1a\u5927\u5927\u589e\u52a0\u7f51\u7edc\u7ed3\u6784\u7684\u590d\u6742\u6027\uff08\u8fd9\u4e00\u70b9\u5bf9\u786c\u4ef6\u5b9e\u73b0\u7684\u7f51\u7edc\u5c24\u5176\u91cd\u8981\uff09\uff0c\u7f51\u7edc\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u66f4\u6613\u9677\u5165\u5c40\u90e8\u6781\u5c0f\u70b9\uff0c\u800c\u4e14\u4f1a\u4f7f\u7f51\u7edc\u7684\u5b66\u4e60\u901f\u5ea6\u53d8\u5f97\u5f88\u6162\u3002\u9690\u5c42\u7ed3\u70b9\u6570\u7684\u9009\u62e9\u95ee\u9898\u4e00\u76f4\u53d7\u5230\u795e\u7ecf\u7f51\u7edc\u7814\u7a76\u5de5\u4f5c\u8005\u7684\u9ad8\u5ea6\u91cd\u89c6\u3002\n\n\u65b9\u6cd51\uff1a \nfangfaGorman\u6307\u51fa\u9690\u5c42\u7ed3\u70b9\u6570s\u4e0e\u6a21\u5f0f\u6570N\u7684\u5173\u7cfb\u662f\uff1as\uff1dlog2N\uff1b\n\n\u65b9\u6cd5\u4e8c\uff1a \nKolmogorov\u5b9a\u7406\u8868\u660e\uff0c\u9690\u5c42\u7ed3\u70b9\u6570s\uff1d2n\uff0b1\uff08n\u4e3a\u8f93\u5165\u5c42\u7ed3\u70b9\u6570\uff09\uff1b\n\n\u65b9\u6cd5\u4e09\uff1a \ns\uff1dsqrt\uff080.43mn\uff0b0.12nn\uff0b2.54m\uff0b0.77n\uff0b0.35\uff09\uff0b0.51 \n\uff08m\u662f\u8f93\u5165\u5c42\u7684\u4e2a\u6570\uff0cn\u662f\u8f93\u51fa\u5c42\u7684\u4e2a\u6570\uff09\u3002","a1dcdf47":"# Logistic Regression\n1. Import Libraries\n    1. Prepare Dataset\n        - We use FashionMNIST dataset.\n        - There are 28*28 images and 10 labels from 0 to 9\n        - Data is not normalized so we divide each image to 255 that is basic normalization for images.\n        - In order to split data, we use train_test_split method from sklearn library\n        - Size of train data is 80% and size of test data is 20%.\n        - Create feature and target tensors. At the next parts we create variable from these tensors. As you remember we need to define variable for accumulation of gradients.\n        - batch_size = batch size means is that for example we have data and it includes 1000 sample. We can train 1000 sample in a same time or we can divide it 10 groups which include 100 sample and train 10 groups in order. Batch size is the group size. For example, I choose batch_size = 100, that means in order to train all data only once we have 480 groups. We train each groups(480) that have batch_size(quota) 100. Finally we train 48000 sample one time.\n        - epoch: 1 epoch means training all samples one time.\n        - In our example: we have 48000 sample to train and we decide our batch_size is 100. Also we decide epoch is 29(accuracy achieves almost highest value when epoch is 29). Data is trained 29 times. Question is that how many iteration do I need? Lets calculate: \n            - training data 1 times = training 48000 sample (because data includes 48000 sample) \n            - But we split our data 480 groups(group_size = batch_size = 100) our data \n            - Therefore, 1 epoch(training data only once) takes 480 iteration\n            - We have 29 epoch, so total iterarion is 13920(that is almost 14000 which I used)\n        - TensorDataset(): Data set wrapping tensors. Each sample is retrieved by indexing tensors along the first dimension.\n        - DataLoader(): It combines dataset and sample. It also provides multi process iterators over the dataset. \u672c\u8d28\u662f\u4e00\u4e2a\u8fed\u4ee3\u5668\u3002\n    1. Create Logistic Regression Model\n        - Same with linear regression.\n        - However as you expect, there should be logistic function in model right?\n        - In pytorch, logistic function is in the loss function where we will use at next parts.\n    1. Instantiate Model\n        - input_dim = 28*28 # size of image px*px\n        - output_dim = 10  # labels 0,1,2,3,4,5,6,7,8,9\n        - create model\n    1. Instantiate Loss \n        - Cross entropy loss\n        - It calculates loss that is not surprise :)\n        - It also has softmax(logistic function) in it.\n    1. Instantiate Optimizer \n        - SGD Optimizer\n    1. Traning the Model\n    1. Prediction\n- As a result, as you can see from plot, while loss decreasing, accuracy(almost 85%) is increasing and our model is learning(training).    ","0eaa7bfc":"# Artificial Neural Network (ANN)\n- Logistic regression is good at classification but when complexity(non linearity) increases, the accuracy of model decreases.\n\naccurancy rate for MNIST\n<img src=\"https:\/\/i.ibb.co\/M7c27Zx\/MNIST.png\" style=\"zoom:50%;\" \/>\n\naccurancy rate for Fashion-MNIST\n\n<img src=\"https:\/\/i.ibb.co\/3rkKzQ5\/Fashion-MNIST.png\" style=\"zoom:50%;\" \/>\n\n- What we expect from artificial neural network is that when complexity increases, we use more hidden layers and our model can adapt better. As a result accuracy increase.\n- **Steps of ANN:**\n    1. Prepare Dataset\n        - Totally same with previous part(logistic regression).\n        - We use same dataset so we only need train_loader and test_loader. \n        - We use same batch size, epoch and iteration numbers.\n    1. Create ANN Model\n        - We add 3 hidden layers.\n        - We use ReLU, Tanh and ELU activation functions for diversity.\n    1. Instantiate Model Class\n        - input_dim = 28*28 # size of image px*px\n        - output_dim = 10  # labels 0,1,2,3,4,5,6,7,8,9\n        - Hidden layer dimension is 150. I only choose it as 150 there is no reason. Actually hidden layer dimension is hyperparameter and it should be **chosen and tuned**. You can try different values for hidden layer dimension and observe the results.\n<img src=\"https:\/\/i.ibb.co\/LPmBJ6B\/image.png\" style=\"zoom:50%;\" >\n        - create model\n    1. Instantiate Loss\n        - Cross entropy loss\n        - It also has softmax(logistic function) in it.\n    1. Instantiate Optimizer\n        - SGD Optimizer\n    1. Traning the Model\n    1. Prediction\n- As a result, as you can see from plot, while loss decreasing, accuracy is increasing and our model is learning(training). \n- Thanks to hidden layers model learnt better and accuracy(almost 95%) is better than accuracy of logistic regression model.","8f80c57c":"# Explore the dataset"}}