{"cell_type":{"a9653b67":"code","6200833b":"code","4f8eea68":"code","7d9c3dca":"code","f55193d8":"code","1192902a":"code","8fc1a718":"code","c6eaac44":"code","66304546":"code","f1a4835c":"code","874a9d19":"code","22c3bc3d":"code","1275b50d":"code","3216a7cb":"code","a098a820":"code","447de8cf":"code","7f1a4b8b":"code","3d0a34a8":"code","ac4ad6e5":"code","a8fc99e7":"code","f07b7c3a":"code","f710e189":"code","2f7a9c5a":"code","f4c5d636":"code","7d2306c6":"code","f641dc3b":"code","0effc918":"code","abad32fd":"code","453604a5":"code","9ee05cfd":"code","50989dff":"code","f305ad30":"code","87b3120b":"code","2356ffee":"code","adf97900":"code","01e679ba":"code","63041480":"code","e249d010":"markdown","57ac246d":"markdown","4687548b":"markdown","217f04a6":"markdown","38c4b68f":"markdown","8104ae13":"markdown","744eb891":"markdown","d27b1185":"markdown","6eb3ad19":"markdown","9bf07d4f":"markdown","f59ccaed":"markdown","188113a5":"markdown","d5c8d7c5":"markdown","5c3955b7":"markdown","c0b0a266":"markdown","5e44d102":"markdown","66e166be":"markdown","6c53330f":"markdown","3431ff8e":"markdown","67e66ed3":"markdown","36488be4":"markdown"},"source":{"a9653b67":"!pip install bs4","6200833b":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.util import ngrams\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\n\nimport pickle\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.svm import SVC","4f8eea68":"# read dataset\ndf = pd.read_csv('..\/input\/jigsaw-toxic-comment-train-and-test\/train.csv')\n\n# first few rows\ndf.head()","7d9c3dca":"# drop columns\ndf.drop(['id','severe_toxic','obscene','threat','insult','identity_hate'],axis=1,inplace=True)","f55193d8":"# shape of the dataset\ndf.shape","1192902a":"sns.countplot(df['toxic'])","8fc1a718":"df['toxic'].value_counts()","c6eaac44":"df['Number_of_words'] = df['comment_text'].apply(lambda x:len(str(x).split()))\ndf.head()","66304546":"df.describe()","f1a4835c":"print('Number of sentences having one word are',len(df[df['Number_of_words']==1]))","874a9d19":"df[df['Number_of_words']==1]['comment_text']","22c3bc3d":"plt.style.use('ggplot')\nplt.figure(figsize=(12,6))\nsns.distplot(df['Number_of_words'],kde = False,color=\"red\",bins=200)\nplt.title(\"Frequency distribution of number of words for each text extracted\", size=20)","1275b50d":"# toxic comments\ntoxic_comments = df[df['toxic'] ==1]['comment_text']\ntoxic_comments.reset_index(inplace=True,drop=True)\nfor i in range(5):\n    print(toxic_comments[i])","3216a7cb":"# non toxic comments\nnon_toxic_comments = df[df['toxic'] ==0]['comment_text']\nnon_toxic_comments.reset_index(inplace=True,drop=True)\nfor i in range(5):\n    print(non_toxic_comments[i])","a098a820":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))\ntext_len=df[df['toxic']==1]['comment_text'].str.len()\nax1.hist(text_len,color='orange')\nax1.set_title('Toxic Comment')\ntext_len=df[df['toxic']==0]['comment_text'].str.len()\nax2.hist(text_len,color='yellow')\nax2.set_title('Non-Toxic Commet')\nfig.suptitle('Characters in Sentence')\nplt.show()","447de8cf":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))\ntext_len=df[df['toxic']==1]['comment_text'].str.split().map(lambda x: len(x))\nax1.hist(text_len,color='red')\nax1.set_title('Toxic Comments')\ntext_len=df[df['toxic']==0]['comment_text'].str.split().map(lambda x: len(x))\nax2.hist(text_len,color='b')\nax2.set_title('Non-Toxic Comment')\nfig.suptitle('Words in Sentence')\nplt.show()","7f1a4b8b":"# toxic\ntoxic_text = ' '.join(df.loc[df.toxic == 1, 'comment_text'].values)\ntoxic_text_trigrams = [i for i in ngrams(toxic_text.split(), 3)]\nCounter(toxic_text_trigrams).most_common(30)","3d0a34a8":"# non-toxic\nnon_toxic_text = ' '.join(df.loc[df.toxic == 0, 'comment_text'].values)\nnon_toxic_text_trigrams = [i for i in ngrams(non_toxic_text.split(), 3)]\nCounter(non_toxic_text_trigrams).most_common(30)","ac4ad6e5":"# word cloud of toxic and non-toxic comment\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[20, 5])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(toxic_comments))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Toxic Comments',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(non_toxic_comments))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Non Toxic Comments',fontsize=40);","a8fc99e7":"stop = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstop.update(punctuation)","f07b7c3a":"def strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\n#Removing the square brackets\ndef remove_between_square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n# Removing URL's\ndef remove_between_square_brackets(text):\n    return re.sub(r'http\\S+', '', text)\n#Removing the stopwords from text\ndef remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop and i.strip().lower().isalpha():\n            final_text.append(i.strip().lower())\n    return \" \".join(final_text)\n#Removing the noisy text\ndef denoise_text(text):\n    text = strip_html(text)\n    text = remove_between_square_brackets(text)\n    text = remove_stopwords(text)\n    return text\n#Apply function on review column\ndf['comment_text']=df['comment_text'].apply(denoise_text)","f710e189":"print('ORIGINAL SENTENCE :',non_toxic_comments[0])\nprint('-'*100)\nprint('CLEANED SENTENCE :',df['comment_text'][0])","2f7a9c5a":"# dependent and independent variable\nX = df['comment_text']\ny = df['toxic']","f4c5d636":"# countvectorizer\ncv = CountVectorizer()\nX = cv.fit_transform(X)","7d2306c6":"smote = SMOTE(random_state = 402)\nX_smote, Y_smote = smote.fit_resample(X,y)\n\n\nsns.countplot(Y_smote)","f641dc3b":"# train-test split\nX_train, X_test, y_train, y_test = train_test_split(X_smote, Y_smote, test_size = 0.20, random_state = 0)","0effc918":"lr = LogisticRegression()\n#Fitting the model \nlr.fit(X_train,y_train)","abad32fd":"# Predicting the Test set results\ny_pred_lr = lr.predict(X_test)","453604a5":"# Accuracy, Precision,f1 and Recall\nscore1 = accuracy_score(y_test,y_pred_lr)\nscore2 = precision_score(y_test,y_pred_lr)\nscore3= recall_score(y_test,y_pred_lr)\nscore4 = f1_score(y_test,y_pred_lr)\nprint(\"---- Scores ----\")\nprint(\"Accuracy score is: {}%\".format(round(score1*100,2)))\nprint(\"Precision score is: {}\".format(round(score2,2)))\nprint(\"Recall score is: {}\".format(round(score3,2)))\nprint(\"F1 Score score is: {}\".format(round(score4,2)))","9ee05cfd":"# Fitting Naive Bayes to the Training set\nclassifier = MultinomialNB()\nclassifier.fit(X_train, y_train)","50989dff":"# Predicting the Test set results\ny_pred_nb = classifier.predict(X_test)","f305ad30":"# Accuracy, Precision,f1 and Recall\nscore1 = accuracy_score(y_test,y_pred_nb)\nscore2 = precision_score(y_test,y_pred_nb)\nscore3 = recall_score(y_test,y_pred_nb)\nscore4 = f1_score(y_test,y_pred_nb)\nprint(\"---- Scores ----\")\nprint(\"Accuracy score is: {}%\".format(round(score1*100,2)))\nprint(\"Precision score is: {}\".format(round(score2,2)))\nprint(\"Recall score is: {}\".format(round(score3,2)))\nprint(\"F1 Score score is: {}\".format(round(score4,2)))","87b3120b":"# xgbClassifier\nclf = XGBClassifier()\nclf.fit(X_train, y_train)","2356ffee":"# Predicting the Test set results\ny_pred_xg = classifier.predict(X_test)","adf97900":"# Accuracy, Precision,f1 and Recall\nscore1 = accuracy_score(y_test,y_pred_xg)\nscore2 = precision_score(y_test,y_pred_xg)\nscore3= recall_score(y_test,y_pred_xg)\nscore4 = f1_score(y_test,y_pred_nb)\nprint(\"---- Scores ----\")\nprint(\"Accuracy score is: {}%\".format(round(score1*100,2)))\nprint(\"Precision score is: {}\".format(round(score2,2)))\nprint(\"Recall score is: {}\".format(round(score3,2)))\nprint(\"F1 Score score is: {}\".format(round(score4,2)))","01e679ba":"# open a file, where you want to store the data\nfile = open('toxic_comments.pkl', 'wb')\n\n# dump information to that file\npickle.dump(clf, file)","63041480":"pickle.dump(cv, open('transform.pkl', 'wb'))","e249d010":"So in this notebook we are going to focus on weather a comment is toxic or not.We only need toxic and commment column so we will going to others drop","57ac246d":"### Number of characters in sentence","4687548b":"# Data Visualization","217f04a6":"## XgbClassifier","38c4b68f":"## Naive Bayes","8104ae13":"# Save Model","744eb891":"It's time to clean our dataset","d27b1185":"## Logistic Regression","6eb3ad19":"# Data Cleaning","9bf07d4f":"### Number of words in each text","f59ccaed":"### Flask application\nSo basically I have also made a flask application for this problem by writing whole code in colab.If you want to know how to run a flask application in colab,then click on this link: https:\/\/www.kaggle.com\/dikshabhati2002\/run-flask-in-colab?scriptVersionId=55081927","188113a5":"From above we can see that we have maximum 1411 words in our sentence and average length is 67.The minimum words in sentence is 1,let's see what are those sentences and how many sentences are there","d5c8d7c5":"Let's see our cleaned data","5c3955b7":"# Model","c0b0a266":"From above we can clearly see that our dataset is imbalanced dataset.We will later handle with it till then we will do data visualization after all first we need to understand our data","5e44d102":"# WordCloud","66e166be":"# Import Libraries","6c53330f":"So basically there are link,random words and numbers, so there are no one word sentence having some meaning","3431ff8e":"## Tri-gram","67e66ed3":"Let's see weather our dataset is balanced or imbalanced","36488be4":"Now let's see toxic and non-toxic comments"}}