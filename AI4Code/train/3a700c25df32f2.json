{"cell_type":{"79cb8d0b":"code","2688baf4":"code","63e8856e":"code","c8abff12":"code","ca07a92f":"code","fb8ec452":"markdown","a83e207c":"markdown","0252918b":"markdown"},"source":{"79cb8d0b":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.neighbors import LocalOutlierFactor\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\"\/kaggle\/input\/stellar-classification-dataset-sdss17\/star_classification.csv\")\nlabels = data[\"class\"].to_numpy()\nlabEncoder = LabelEncoder()\nlabels = labEncoder.fit_transform(labels)\n\ndata = data.drop([\"obj_ID\", \"class\", \"spec_obj_ID\", \"fiber_ID\", \"run_ID\", \"rerun_ID\", \"field_ID\"], axis=1)\nheaders = list(data)\ndata = data.to_numpy()\ndata = MinMaxScaler().fit(data).transform(data)\n\nisOutlier = LocalOutlierFactor().fit_predict(data)\ncleanData = data[np.where(isOutlier == 1)]\ncleanData = MinMaxScaler().fit(cleanData).transform(cleanData)\ncleanLabels = labels[np.where(isOutlier == 1)]\n\nprint(f\"Percentage of data considered outliers: {((data.shape[0]-cleanData.shape[0])\/data.shape[0])*100}%\")","2688baf4":"colors = [\"red\", \"blue\", \"green\"]\n\nfor x in range(len(headers)):\n\n    for y in range(x+1, len(headers)):\n\n        for i in range(np.unique(cleanLabels).shape[0]):\n            ind = np.where(cleanLabels == i)\n            label=str(labEncoder.inverse_transform([i]))\n            plt.scatter(cleanData[ind, x], cleanData[ind, y], s=1, color=colors[i], alpha=0.8)\n        plt.ylabel(headers[y])\n        plt.xlabel(headers[x])\n        #plt.legend()\n        plt.show()","63e8856e":"dfData = pd.DataFrame(cleanData)\n\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(dfData.corr())\nfig.colorbar(cax)\n\nxaxis = np.arange(len(headers))\nax.set_xticks(xaxis)\nax.set_yticks(xaxis)\nax.set_xticklabels(headers, rotation=45)\nax.set_yticklabels(headers)","c8abff12":"results = []\n\nkf = KFold(n_splits=10)\ni = 0\nfor trainIndex, testIndex in kf.split(cleanData):\n    xTrain, xTest = cleanData[trainIndex], cleanData[testIndex]\n    yTrain, yTest = cleanLabels[trainIndex], cleanLabels[testIndex]\n    \n    model = RandomForestClassifier(n_estimators=25).fit(xTrain, yTrain)\n    preds = model.predict(xTest)\n\n    results.append(accuracy_score(yTest, preds))\n    print(f\"Iteration: {i}   (Accuracy: {results[-1]})\")\n    i += 1\n    \nprint(f\"Average accuracy: {np.mean(results)}\")","ca07a92f":"plt.barh(headers, model.feature_importances_)","fb8ec452":"# Results\n## Method\n### Preprocessing\n- Encode string labels to integers.\n- Remove identifier features from data.\n- Normalize features.\n- Identify potential outliers using the comparitive local density of features (LOF).\n- Create a copy of the data with samples containing outliers removed.\n- Re-normalize the features of the outlier free data.\n\n### Modelling\n- 10-Fold Cross validation.\n- Random Forest Classifier using 25 estimators.\n- Record accuracy in each fold.\n- Average fold-wise accuracy.\n\n\n## Result\n### All Data\n- Average accuracy: 0.9773\n\n### With Potential Outliers Removed (5.239% of Data Removed)\n- Average accuracy: 0.9787570710061058","a83e207c":"# Modelling","0252918b":"# Exploratory Analysis Plots\n## Independent Variable Relationships"}}