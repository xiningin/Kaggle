{"cell_type":{"d8552d67":"code","1b680f78":"code","b87eccef":"code","0292e34f":"code","29a54bc1":"code","72b7fe1b":"code","8f28090e":"code","26655aa0":"code","9139c6ec":"code","e9ac9507":"code","713949b0":"code","61bb82b3":"code","1faabecd":"code","1cf72186":"code","8ce893a3":"code","7337358a":"code","762f751e":"code","75490b86":"code","2b0fb34b":"code","570087aa":"code","de212471":"code","4468ef4e":"code","2c9d52c7":"code","69b6ed6b":"code","c38650fe":"code","32c1ef17":"code","9188f053":"code","8fad27f9":"code","1f0899ab":"code","843bc5d4":"code","dd77e297":"code","f2267862":"code","b323aba6":"code","998fcee5":"code","cdd313be":"code","35557248":"code","db0f057f":"code","8797bfcf":"code","266a96e1":"code","8d6e37d3":"code","1456929f":"code","ef188fac":"code","52402a20":"code","ea89c701":"code","7b1c6cfe":"code","9c4fe175":"code","63a2f540":"code","d8a66fc0":"markdown","de5776a2":"markdown","43c932b4":"markdown","77a9e1d0":"markdown","4e90d46e":"markdown","868f6280":"markdown","6ee21d3e":"markdown","779ca5c1":"markdown","e6c5505d":"markdown","88c63dab":"markdown","f9561160":"markdown","d565a410":"markdown","f4a3086c":"markdown","da6f80f8":"markdown"},"source":{"d8552d67":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1b680f78":"# Importing  datasets\ntrain=pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain.head()","b87eccef":"#to see the rows and columns\ntrain.shape","0292e34f":"#to see the info \ntrain.info()","29a54bc1":"#to see the \ntrain.describe()","72b7fe1b":"train = train.drop('Cabin', axis=1)","8f28090e":"train.isnull().sum()","26655aa0":"#taking only not null values from 'Age' and 'Embarked' since they have not null values\ntrain=train[train['Age'].notnull()]\ntrain=train[train['Embarked'].notnull()]","9139c6ec":"train.isnull().sum()","e9ac9507":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndef plotcountgraph(var):\n    \n    sns.set(font_scale=1.25)\n    plt.figure(figsize=(10, 5))\n    ax = sns.countplot(x=var, hue='Survived', data=train,  palette=\"viridis\")\n    plt.setp(ax.get_xticklabels(), rotation=0)","713949b0":"var=['Sex','Pclass','Embarked','SibSp','Parch']\nfor i in var:\n    plotcountgraph(i)","61bb82b3":"#Finding the correlation between the numeric variables\nplt.figure(figsize = (20,10))\nsns.heatmap(train.corr(),annot = True)\nplt.show()","1faabecd":"#drpping the Parch as it is having low correlation with survived(target variable)\ntrain = train.drop('Parch', axis=1)\n#dropping ticket and Parch as it is not required for further analysis\ntrain = train.drop('Ticket', axis=1)\ntrain = train.drop('Name', axis=1)","1cf72186":"#importing the test data set and checking\ntest=pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest.head()","8ce893a3":"#converting  the categorical variables to numeric as required for model building\n\ndummy1 = pd.get_dummies(train[['Sex','Embarked']], drop_first=True)\n# Adding the results to the master dataframe\ntrain = pd.concat([train, dummy1], axis=1)\n\ndummy1 = pd.get_dummies(test[['Sex','Embarked']], drop_first=True)\n# Adding the results to the master dataframe\ntest = pd.concat([test, dummy1], axis=1)\n","7337358a":"#since the dummies are already present\ntrain=train.drop(['Sex','Embarked'],1)\ntest=test.drop(['Sex','Embarked'],1)\n","762f751e":"train.head()","75490b86":"#dropping the target variable for training the model\nX_train=train.drop('Survived', axis=1)\n","2b0fb34b":"#dataset for the target variable for training the model\ny_train=train['Survived']","570087aa":"#Creating the test data set X_test \nX_test=test\n","de212471":"#Running Your First Training Model\nimport statsmodels.api as sm\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","4468ef4e":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()","2c9d52c7":"from sklearn.feature_selection import RFE\nrfe = RFE(logreg,6)             # running RFE with 13 variables as output\nrfe = rfe.fit(X_train, y_train)","69b6ed6b":"rfe.support_","c38650fe":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","32c1ef17":"col = X_train.columns[rfe.support_]\nX_train.columns[~rfe.support_]","9188f053":"X_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","8fad27f9":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","1f0899ab":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","843bc5d4":"y_train_pred_final = pd.DataFrame({'Survived':y_train.values, 'Survived_prob':y_train_pred})\ny_train_pred_final['PassengerId'] = y_train.index\ny_train_pred_final.head()","dd77e297":"y_train_pred_final['predicted'] = y_train_pred_final.Survived_prob.map(lambda x: 1 if x > 0.54 else 0)\n\n# Let's see the head\ny_train_pred_final","f2267862":"from sklearn import metrics\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Survived, y_train_pred_final.predicted )\nprint(confusion)","b323aba6":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Survived, y_train_pred_final.predicted))","998fcee5":"X_test = X_test[col]\nX_test.head()","cdd313be":"X_test_sm = sm.add_constant(X_test)","35557248":"#Making predictions on the test set\ny_test_pred = res.predict(X_test_sm)","db0f057f":"y_test_pred[:10]","8797bfcf":"## Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)\ny_pred_1.head()","266a96e1":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\n#y_test_df.reset_index(drop=True, inplace=True)","8d6e37d3":"# creating a final \ny_pred_final = pd.concat([ y_pred_1],axis=1)","1456929f":"y_pred_final.head()","ef188fac":"# Renaming and Reaaranging the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Survived_Prob'})","52402a20":"# Let's see the head of y_pred_final\ny_pred_final.head()","ea89c701":"y_pred_final['Survived'] = y_pred_final.Survived_Prob.map(lambda x: 1 if x > 0.54 else 0)","7b1c6cfe":"y_pred_final.head()","9c4fe175":"#creating the submission.csv for the submission \n\noutput = pd.DataFrame({'PassengerId': test.PassengerId , 'Survived': y_pred_final.Survived})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","63a2f540":"output = pd.DataFrame({'PassengerId': test.PassengerId , 'Survived': y_pred_final.Survived_Prob})\noutput.to_csv('my_submission_Prob.csv', index=False)","d8a66fc0":"<h1 style=\"background-color:Skyblue;font-family:Georgia;font-size:175%;text-align:left\">Step 5: Creating the Test and Train Data properly<\/h1>","de5776a2":"<h1 style=\"background-color:Skyblue;font-family:Georgia;font-size:175%;text-align:left\">Step 2: Inspecting the Dataframe <\/h1>","43c932b4":"<h1 style=\"background-color:Skyblue;font-family:Georgia;font-size:175%;text-align:left\">Step 8: Making predictions on the test set<\/h1>","77a9e1d0":"<h1 style=\"background-color:Skyblue;font-family:Georgia;font-size:175%;text-align:left\"> Step 1: Importing  Data<\/h1>","4e90d46e":"<h1 style=\"background-color:Skyblue;font-family:Georgia;font-size:175%;text-align:left\">Step 3: Data Cleaning and Preparation<\/h1>","868f6280":"<h1 style=\"background-color:Skyblue;font-family:Georgia;font-size:175%;text-align:left\">Step 7: Feature Selection Using RFE<\/h1>","6ee21d3e":" Creating a dataframe with the actual churn flag and the predicted probabilities","779ca5c1":"<h1 style=\"background-color:Skyblue;font-family:Georgia;font-size:175%;text-align:left\">Step 6: Model Building<\/h1>","e6c5505d":"<h1 style=\"background-color:Skyblue;font-family:Georgia;font-size:350%;text-align:center\"> Titanic Data EDA and Logestic Regression <\/h1>\n\n\n<img src=\"https:\/\/data.whicdn.com\/images\/172028045\/original.gif\">\n\n\n\n\n\n<h1 style=\"background-color:Skyblue;font-family:Georgia;font-size:250%;text-align:center\">Analysis of the different features of the Titanic Data Set and Classification <\/h1>\n\n<h1 style=\"background-color:BLUE;font-family:Georgia;font-size:100%;text-align:center\">Please Upvote and comment if you like the EDA and classification<\/h1>\n","88c63dab":"### The accuracy is pretty Good!","f9561160":"<h1 style=\"background-color:Skyblue;font-family:Georgia;font-size:175%;text-align:left\">Step 4: Data Visualization<\/h1>","d565a410":"Assessing the model with StatsModels","f4a3086c":"Creating new column 'predicted' with 1 if Survived_Prob > 0.5 else 0","da6f80f8":"<h1 style=\"background-color:Skyblue;font-family:Georgia;font-size:175%;text-align:left\"> Inference from the above graphs\n   \n    \n    *Number of females survived is more when compared to male\n    *Lower the Pclass higher is the rate of survival\n    *Survival rate of people embarked from Southampton is higher \n    *Survival rate of people having no siblings is higher\n    *Survival rate of people having no parents \/ children aboard the Titanic is higher\n"}}