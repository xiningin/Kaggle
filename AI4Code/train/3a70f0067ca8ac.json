{"cell_type":{"f9ee8ba6":"code","ad1f9f30":"code","778a8465":"code","01a3ffa7":"code","bdb47a59":"code","6c27dbd3":"code","4d9e3005":"code","7a6b4591":"code","beb901d4":"code","26931372":"code","3c89c7de":"code","80c41c3f":"code","64e1ed1f":"code","40c70ec5":"code","3b14470a":"code","f70ae025":"code","4070d259":"code","d1e59a0a":"code","e4b7f734":"code","478e8855":"code","1d784438":"code","8af45fc8":"code","d0829614":"code","aa550fcb":"code","bbf2f867":"code","05fa6202":"code","6254f425":"code","2a51c906":"markdown","b0b766d6":"markdown","00bd4f30":"markdown","068f0d61":"markdown","cbb8a8d7":"markdown"},"source":{"f9ee8ba6":"!pip install transformers","ad1f9f30":"import os\nimport math\nimport datetime\n\nfrom tqdm import tqdm\n\nimport pandas as pd\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\n\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nfrom matplotlib import rc\n\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\nsns.set(style='whitegrid', palette='muted', font_scale=1.2)\n\nHAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n\nsns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n\nrcParams['figure.figsize'] = 12, 8\n\nRANDOM_SEED = 42\n\nnp.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)\nfrom transformers import BertTokenizer, TFBertForSequenceClassification","778a8465":"def regular_encode(texts, tokenizer, maxlen=256):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_masks=False, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids' ])","01a3ffa7":"try:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","bdb47a59":"AUTO = tf.data.experimental.AUTOTUNE\n\n# Data access\n#GCS_DS_PATH = KaggleDatasets().get_gcs_path()\n\n# Configuration\nEPOCHS = 1\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync\nMAX_LEN = 256\n","6c27dbd3":"train = pd.read_csv(\"\/kaggle\/input\/data\/train.csv\")\nvalid = pd.read_csv(\"\/kaggle\/input\/data\/valid.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/data\/test.csv\")","4d9e3005":"train.head()","7a6b4591":"train=train.iloc[:,1:]","beb901d4":"test=test.iloc[:,1:]\n\nvalid=valid.iloc[:,1:]\n","26931372":"train_y=train['intent'].astype('category').cat.codes\n","3c89c7de":"valid_y=valid['intent'].astype('category').cat.codes\ntest_y=test['intent'].astype('category').cat.codes","80c41c3f":"train.drop(columns=['intent'],inplace=True)","64e1ed1f":"valid.drop(columns=['intent'],inplace=True)\ntest.drop(columns=['intent'],inplace=True)\n","40c70ec5":"train_y=train_y.astype('int64')","3b14470a":"test_y=test_y.astype('int64')\nvalid_y=valid_y.astype('int64')","f70ae025":"def build_model(transformer, max_len=100):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output\n    logits = keras.layers.Dense(units=768, activation=\"tanh\")(cls_token)\n    logits = keras.layers.Dropout(0.5)(logits)\n    logits = keras.layers.Dense(units=7, activation=\"softmax\")(logits)\n\n    model = Model(inputs=input_word_ids, outputs=logits)\n    model.compile(optimizer=keras.optimizers.Adam(1e-5),loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")]\n)\n    \n\n    \n    \n    return model","4070d259":"tokenizer = BertTokenizer.from_pretrained('bert-base-cased')","d1e59a0a":"x_train = regular_encode(train.text\t.values, tokenizer, maxlen=MAX_LEN)\nx_valid = regular_encode(valid.text\t.values, tokenizer, maxlen=MAX_LEN)\nx_test = regular_encode(test.text.values, tokenizer, maxlen=MAX_LEN)\n","e4b7f734":"\n \ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, train_y.values))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, valid_y.values))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_test,test_y.values))\n    .batch(BATCH_SIZE)\n)\n","478e8855":"from tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.models import Model","1d784438":"\n%%time\nwith strategy.scope():\n    model = TFBertForSequenceClassification.from_pretrained('bert-base-cased')\n    model = build_model(model, max_len=256)\nmodel.summary()","8af45fc8":"EPOCHS = 5\nn_steps = x_train.shape[0] \/\/ 16\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS\n    \n)","d0829614":"y_pred = model.predict(x_test).argmax(axis=-1)","aa550fcb":" from sklearn.metrics import accuracy_score\n accuracy_score(y_pred,test_y)","bbf2f867":"y_pred","05fa6202":"from sklearn.metrics import classification_report\nprint(classification_report(test_y, y_pred))","6254f425":"predictions = model.predict(x_test).argmax(axis=-1)\n\n","2a51c906":"# Intent Recognition with BERT using Keras and TensorFlow 2","b0b766d6":"# Intent Recognition with BERT","00bd4f30":"<font face = \"Verdana\" size =\"5\">intent classification is  very important process  in developing dialog system in nlp it is  core process  of all voice assistant developed using nlp.earlier spacy ,svm ,naive bayes ,lstm ,cnnseq2seq models  were used for creating intent classification model .transformer models  have created benchmark in nlp process ,lets  see how it works  in intent classification\n \n   <font face = \"Verdana\" size =\"4\">\n   <br>Feel free to provide me with feedbacks. \n   \n    \n","068f0d61":"# References\n\n- https:\/\/mccormickml.com\/2019\/07\/22\/BERT-fine-tuning\/\n- https:\/\/github.com\/snipsco\/nlu-benchmark\/tree\/master\/2017-06-custom-intent-engines\n- https:\/\/jalammar.github.io\/illustrated-bert\/\n- https:\/\/towardsdatascience.com\/bert-for-dummies-step-by-step-tutorial-fb90890ffe03\n- https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/ao23cp\/p_how_to_use_bert_in_kaggle_competitions_a\/","cbb8a8d7":"## Training"}}