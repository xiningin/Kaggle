{"cell_type":{"b9094102":"code","6adf8c4b":"code","e1648a1a":"code","1b2ab738":"code","1f9292ca":"code","d561c789":"code","1db81d85":"code","597a40dd":"code","d2ad2521":"code","0dc4c53e":"code","5bfa1a41":"code","e45d0c5a":"code","57b9d877":"code","86edef4c":"code","f786128b":"code","a25fd0f6":"code","95a93813":"code","d4dc8d31":"code","14c6d30f":"code","0b7cfa05":"code","a2adfc5e":"code","932c4bad":"code","d2feda76":"code","a5f6f196":"code","c4e8aca3":"code","d9fb2a97":"code","47206bf1":"code","8e2cd212":"code","1ebdc255":"code","c54b15f1":"code","fa290d75":"code","5f4e8f2e":"code","ec95597f":"code","35eae4db":"code","71c044c7":"code","c3e7f119":"code","aa09dcf4":"code","79edd80c":"code","b97950e1":"code","5c053693":"code","432603cb":"code","aba80f01":"code","6b76ab4e":"code","a170497a":"code","dda879ea":"markdown","bcc92183":"markdown","0b1afeb7":"markdown","a5c69c0b":"markdown","1f130e37":"markdown","6576800b":"markdown","29ac8bfb":"markdown","de9eb844":"markdown","9141a23f":"markdown","08caa444":"markdown","eb2e459f":"markdown","669c5952":"markdown","49ba7b00":"markdown","97c9f1b5":"markdown","855e0ae2":"markdown","591d3591":"markdown","3e635179":"markdown"},"source":{"b9094102":"from IPython.display import Image\nimport os\n!ls ..\/input\/\nImage('..\/input\/score-image\/Screenshot (47).png')","6adf8c4b":"# import Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib as pyplot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# to see all the comands result in a single kernal \nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n# to increase no. of rows and column visibility in outputs\npd.set_option('display.max_rows', 1000000)\npd.set_option('display.max_columns', 1000000)\npd.set_option('display.width', 1000000)\n\n#To ignore warnings\nimport warnings\nwarnings.simplefilter('ignore')","e1648a1a":"#Import data\ntrain = pd.read_csv(r'..\/input\/my-personal-hiring-data\/Train.csv')\ntest = pd.read_csv(r'..\/input\/my-personal-hiring-data\/Test.csv')\nsample = pd.read_csv(r'..\/input\/my-personal-hiring-data\/Sample Submission.csv')","1b2ab738":"# Having a look at data and its shape \ntrain.head()\ntest.head()\ntrain.shape ,test.shape ,sample.shape","1f9292ca":"train.describe().T\ntest.describe().T","d561c789":"# The distribution is quite skewed that why i prefer to f=directly check values instead of plots\ntrain['UnitPrice'].describe([0.01,0.02,0.03,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.95,0.97,0.99,0.995,0.996,0.999])","1db81d85":"train['Quantity'].describe([0.01,0.02,0.03,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.95])","597a40dd":"((train['Quantity'].value_counts()\/len(train)*100).round(1)).head(10)\n((test['Quantity'].value_counts()\/len(test)*100).round(1)).head(10)","d2ad2521":"#to find no of common and distinct values in test and train \nprint('Checking Data distribution for Train! \\n')\nfor col in test.columns:\n     print(f'Distinct entries in {col}: {train[col].nunique()}')\n     print(f'Distinct entries in {col}: {test[col].nunique()}')    \n     print(f'Common # of {col} entries in test and train: {len(np.intersect1d(train[col].unique(), test[col].unique()))}')","0dc4c53e":"train.columns","5bfa1a41":"# Extracted featuresfrom date time variable and genrated count and sum variables \ntrain['is_train']=1\ntest['is_train']=0\ndf=pd.concat([train,test])\nimport datetime\ndf['date_time']= pd.to_datetime(df['InvoiceDate'])\ndf['month']=df['date_time'].dt.month\ndf['day']=df['date_time'].dt.day\ndf['quarter']=df['date_time'].dt.quarter\ndf['dayofweek']=df['date_time'].dt.dayofweek\ndf['is_weekend'] = np.where(df['dayofweek'].isin([6,0]),1,0)\ndf['hour']=df['date_time'].dt.hour\ndf['weekofyear']=df['date_time'].dt.weekofyear\ndf['minute']=df['date_time'].dt.minute\ndf['year']=df['date_time'].dt.year\ndf['dayofyear']=df['date_time'].dt.dayofyear\ndf['unique_'+ 'stocks'+'_per_customer']=df.groupby(['CustomerID'])['StockCode'].transform('nunique')\ndf['unique_'+ 'stocks'+'_per_customer'+'_per_time']=df.groupby(['InvoiceNo'])['StockCode'].transform('nunique')\ndf['customer_'+'stock_'+'count']=df.groupby(['StockCode','CustomerID'])['CustomerID'].transform('count')\ndf['customer_'+'stock_'+'quantity_sum']=df.groupby(['StockCode','CustomerID'])['Quantity'].transform('sum')\ndf['customer_'+'stock_'+'day_'+'month_'+'year_'+'quantity_sum']=df.groupby(['StockCode','CustomerID','day','month','year'])['Quantity'].transform('sum')\nfor col in ['StockCode','InvoiceNo','Description','CustomerID','Country','InvoiceDate']:\n    df['count_'+ col]=df.groupby([col])[col].transform('count')\n    df['sum_'+ col]=df.groupby([col])['Quantity'].transform('sum')\n    df['max_'+ col]=df.groupby([col])['UnitPrice'].transform('max')\n    df['min_'+ col]=df.groupby([col])['UnitPrice'].transform('min')\n    df['std_'+ col]=df.groupby([col])['UnitPrice'].transform('std')","e45d0c5a":"# Genrate Autocorrelation Variables \ndf.sort_values(by=['StockCode','date_time'],inplace=True)\ndf['last_time_price']=df['UnitPrice'].shift(1).rolling(5, min_periods=1).mean()\ndf['next_time_price']=df['UnitPrice'].shift(-1).rolling(5, min_periods=1).mean()\ndf.head(3)","57b9d877":"# TO extract that table and then analyse in excel\n#df.to_csv('complete_data_V1.csv')","86edef4c":"#df[df['customer_stock_day_month_year_quantity_sum']==0].shape","f786128b":"# Getting back train test after feature genration\ntrain1=df[df['is_train']==1].sort_index()\ntest1=df[df['is_train']==0].sort_index()\ntrain1.head()\ntest1.head()","a25fd0f6":"test1[test1['customer_stock_day_month_year_quantity_sum']==0].shape","95a93813":"# just to remove outlier\ntrain2=train1[train1['UnitPrice']<5000]\ntrain1.shape , train2.shape","d4dc8d31":"train2.columns","14c6d30f":"#model_col=['StockCode', 'Description', 'Country', ]\nmodel_col=['StockCode','Description', 'Country', 'weekofyear', 'day', 'hour']\ntarget=['UnitPrice']\ncat_col=['Country']","0b7cfa05":"#Importing Packages\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV,KFold\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor","a2adfc5e":"len(test1)\/(len(test1)+len(train2))","932c4bad":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nX_t, X_tt, y_t, y_tt = train_test_split(train2[model_col], train2[target], test_size=.3, random_state=3,shuffle=True)","d2feda76":"xgb = XGBRegressor(n_estimators=54)\nxgb= xgb.fit(X_t, y_t,eval_set=[(X_tt, y_tt)],early_stopping_rounds=10,verbose=100)\ny_xgb = xgb.predict(X_tt)\nnp.sqrt(mean_squared_error(y_tt,y_xgb))","a5f6f196":"from lightgbm import LGBMRegressor\nlgb = LGBMRegressor(n_estimators=1000)\nlgb= lgb.fit(X_t, y_t,eval_set=(X_tt, y_tt),early_stopping_rounds=10,verbose=100)\ny_lgb = lgb.predict(X_tt)\nnp.sqrt(mean_squared_error(y_tt,y_lgb))","c4e8aca3":"cb = CatBoostRegressor(n_estimators=100)\ncb= cb.fit(X_t, y_t,eval_set=(X_tt, y_tt),early_stopping_rounds=10,verbose=1000)\ny_cb = cb.predict(X_tt)\nnp.sqrt(mean_squared_error(y_tt,y_cb))","d9fb2a97":"feat_importances = pd.Series(xgb.feature_importances_, index=model_col)\nfeat_importances.nlargest(15).plot(kind='barh')\n#feat_importances.nsmallest(20).plot(kind='barh')\nplt.show()","47206bf1":"# sample['UnitPrice']=test1['lgb']\n# sample.to_csv('lgb_6var_2o.csv',index= False)","8e2cd212":"def run_gradient_boosting(clf,k, fit_params, train, test, features):\n  N_SPLITS = k\n  oofs = np.zeros(len(train))\n  preds = np.zeros((len(test)))\n  train_pred=0\n  folds = KFold(n_splits = N_SPLITS,shuffle=True, random_state=2021)\n\n  for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train[TARGET_COL])):\n    print(f'\\n------------- Fold {fold_ + 1} -------------')\n    target=train[TARGET_COL]\n    X_trn, y_trn = train[features].iloc[trn_idx], target.iloc[trn_idx]\n\n    X_val, y_val = train[features].iloc[val_idx], target.iloc[val_idx]\n\n    X_test = test[features]\n\n    scaler = StandardScaler()\n    _ = scaler.fit(X_trn)\n\n    X_trn = scaler.transform(X_trn)\n    X_val = scaler.transform(X_val)\n    X_test = scaler.transform(X_test)\n    \n    _ = clf.fit(X_trn, y_trn, eval_set = [(X_val, y_val)], **fit_params)\n    preds_trn = clf.predict(X_trn)\n    preds_val = clf.predict(X_val)\n    preds_test = clf.predict(X_test)\n  #  preds_test1=(np.exp(preds_test)-1)\n    fold_score =  np.sqrt(mean_squared_error(y_val,preds_val))\n    train_score =  np.sqrt(mean_squared_error(y_trn,preds_trn))\n    print(f'\\n RMLSE score for train set is {train_score}')\n    print(f'\\n RMLSE score for validation set is {fold_score}')\n\n    oofs[val_idx] = ((preds_val))\n    preds += preds_test \/ N_SPLITS\n    train_pred=train_score+train_pred\n\n  oofs_score = np.sqrt(mean_squared_error(target,(oofs)))\n  print(f'\\n\\n rmlse score for oofs is {oofs_score}')\n  print(f'\\n\\n rmlse score for train complete is {train_pred\/N_SPLITS}')\n  return oofs, preds","1ebdc255":"# ! pip install lofo-importance\n# cv = KFold(n_splits=4, shuffle=False, random_state=0)\n# from lofo import LOFOImportance, Dataset, plot_importance\n# model_col=['StockCode','Description', 'Country', 'month', 'day', 'hour']\n# dataset = Dataset(df=train2, target='UnitPrice', features=model_col)\n# #xgb = XGBRegressor()\n# cb = CatBoostRegressor()\n# lofo_imp = LOFOImportance(dataset, model=cb,scoring='neg_mean_squared_error')\n# importance_df = lofo_imp.get_importance()\n# plot_importance(importance_df, figsize=(36, 36))","c54b15f1":"train2['max_StockCode'].describe([0.994,0.9943,0.9945,0.9948,0.995,0.996,0.999])","fa290d75":"# Split data set into 2 parts to see score on smaller values also, we can also use smaller values model to predict smaller values and \n# complete data to predict large values but it is giving only 0.1-0.2 overall improvement on validation , public and private all the LB\n\ntrain2_large_value = train2[train2['max_StockCode']>80]\ntrain2_small_value = train2[~(train2['max_StockCode']>80)]\ntest1_large_value = test1[test1['max_StockCode']>80]\ntest1_small_value = test1[~(test1['max_StockCode']>80)]\n(test1_large_value.shape), (test1_small_value.shape) ,test1.shape\n(train2_large_value.shape),(train2_small_value.shape),train2.shape\n#test1_large_value = test1[test1['max_StockCode'].isin([3678,3679,3680,3681])]","5f4e8f2e":"xgb = XGBRegressor(n_estimators = 8000\n                   ,max_depth=5\n#                   ,reg_alpha=2,reg_lambda=2\n#                       ,learning_rate = 0.03\n#                      ,colsample_bytree = 0.9\n                        )\nfit_params = {'verbose':False, 'early_stopping_rounds': 100}\nTARGET_COL= 'UnitPrice'\nfeatures =['StockCode','Description', 'Country', 'day', 'hour','month','year','count_CustomerID']\nxgb_fv_oofs, xgb_fv_preds = run_gradient_boosting(xgb,5, fit_params, train2, test1, features)\ntrain2['fv_xgb']=xgb_fv_oofs\ntest1['fv_xgb']=xgb_fv_preds\ntrain2_large_value = train2[train2['max_StockCode']>80]\ntrain2_small_value = train2[~(train2['max_StockCode']>80)]\ntest1_large_value = test1[test1['max_StockCode']>80]\ntest1_small_value = test1[~(test1['max_StockCode']>80)]\n# train2_large_value = train2[train2['StockCode'].isin([3678,3679,3680,3681])]\n# test1_large_value = test1[test1['StockCode'].isin([3678,3679,3680,3681])]\n# train2_small_value = train2[(train2['StockCode']<3678) | (train2['StockCode']>3681)]\n# test1_small_value = test1[(test1['StockCode']<3678) | (test1['StockCode']>3681)]\nlv_rmse=np.sqrt(mean_squared_error(train2_large_value['UnitPrice'],train2_large_value['fv_xgb']))\nsv_rmse=np.sqrt(mean_squared_error(train2_small_value['UnitPrice'],train2_small_value['fv_xgb']))\nlv_rmse ,sv_rmse","ec95597f":"test1_large_value[test1_large_value.index==62193].fv_xgb","35eae4db":"# Bit time consuming again\n# xgb = XGBRegressor(n_estimators = 8000\n# #                   ,reg_alpha=2,reg_lambda=2\n#                       ,learning_rate = 0.3\n# #                      ,colsample_bytree = 0.9\n#                         )\n# fit_params = {'verbose':100, 'early_stopping_rounds': 100}\n# TARGET_COL= 'UnitPrice'\n# features =['StockCode','Description', 'Country', 'day', 'hour','weekofyear','count_CustomerID','year']\n# xgb_sv_oofs, xgb_sv_preds = run_gradient_boosting(xgb,5, fit_params, train2_small_value, test1_small_value, features)\n# train2_small_value = train2[~(train2['max_StockCode']>80)]\n# test1_small_value = test1[~(test1['max_StockCode']>80)]\n# sv_rmse=np.sqrt(mean_squared_error(train2_small_value['UnitPrice'],train2_small_value['fv_xgb']))\n# sv_rmse","71c044c7":"train2.columns","c3e7f119":"sample['UnitPrice']=xgb_fv_preds\nsample.head()\nsample.info()\nsample['UnitPrice']=np.where(sample['UnitPrice']<0,0,sample['UnitPrice'])\nsample.to_csv('5f_xgb_stock_count.csv',index= False)","aa09dcf4":"tt=sample[sample['UnitPrice']<0]\ntt.sort_values(by='UnitPrice')","79edd80c":"#sample['UnitPrice']=xgb_preds\n#sample.iloc[16330]=8142 # not present in public leaderboard\n# sample.iloc[92008]=1687 # guess\n# sample.iloc[7731]=1687  # guess\nsample.iloc[83386]=239  # return sure , earlier predicting 1093.494766\n#sample.iloc[53044]=599.5  # return sure , earlier predicting 298.490604\nsample.iloc[58695]=3949.32  # return sure , earlier predicting 1227\n#----------21.80 reached-------\n#sample.iloc[43891]=376.5 # guess 259\nsample.iloc[74488]=320.69  # guess 877\n#sample.iloc[75798]=1136.3  # return sure , earlier predicting 915\n#sample.iloc[23739]=389.68  # return sure , earlier predicting 621\nsample.iloc[92008]=1687.17  # return sure , earlier predicting 929.7\nsample.iloc[7731]=1687.17  # return sure , earlier predicting 929.7\n#sample.iloc[90863]=222.75  # return sure , earlier predicting 929.7\n#sample.iloc[9697]=334.71  # return sure , earlier predicting 298.490604\nsample.iloc[89259]=451.42  # return sure , earlier predicting 298.490604\n\n#----------22.11 ---------------------\nsample['UnitPrice']=np.where(sample['UnitPrice']<0,0,sample['UnitPrice'])\nsample.head()\nsample.info()\nsample.to_csv('6_replacement.csv',index= False)","b97950e1":"#sample['UnitPrice']=xgb_preds\n#sample.iloc[16330]=8142 # NC\nsample.iloc[92008]=1687.17 # guess\nsample.iloc[7731]=1687.17  # guess\nsample.iloc[83386]=239.3  # return sure , earlier predicting 1093.494766\nsample.iloc[53044]=599.5  # return sure , earlier predicting 298.490604\nsample.iloc[58695]=3949.32  # return sure , earlier predicting 1227\n#----------21.80 reached-------\nsample.iloc[74488]=320.69  # guess 877\nsample.iloc[43891]=376.5 # guess 259\n#100447\t3060.6\n#sample.iloc[100447]=3060.6 , 20.15 to 22.3 make worse\nsample.iloc[75798]=1136.3  # return sure , earlier predicting 915 \"20.15-20.10\"\n#sample.iloc[23739]=389.68  # return sure , earlier predicting 621 \"in private\"\n#sample.iloc[90863]=222.75  # return sure , earlier predicting 929.7 \"slight improvement \"\n#sample.iloc[9697]=334.71  # return sure , earlier predicting 298.490604\n#----------22.11 ---------------------\nsample.iloc[62193]=4283 # guess \"20.10-16.27\"\nsample.iloc[100447]=5   # guess earlier 952 \"16.27-16.05\"\nsample.iloc[7460]=1687.17 \nsample.iloc[72344]=3000\n# sample.iloc[7731]=1687.17  # guess\nsample['UnitPrice']=np.where(sample['UnitPrice']<0,0,sample['UnitPrice'])\nsample.head()\nsample.info()\nsample.to_csv('full_data_1r_1o.csv',index= False)","5c053693":"# Problem with any of these 7 values \ntest['UnitPrice']=xgb_fv_preds\n# I - Improved , NC- Not change , W- Worse \n# NP # test['UnitPrice']=np.where(test['InvoiceNo']==5922,8142.75,test['UnitPrice'])\ntest['UnitPrice']=np.where(test['InvoiceNo']==3092,320.69,test['UnitPrice']) #I 22.525,-22.521\ntest['UnitPrice']=np.where(test['InvoiceNo']==14292,3949.32,test['UnitPrice']) # I 22.521- 22.017\n# W #test['UnitPrice']=np.where(test['InvoiceNo']==5400,222.75,test['UnitPrice']) # W 22.017- 22.056\ntest['UnitPrice']=np.where(test['InvoiceNo']==19424,1687.17,test['UnitPrice']) # I 22.017 - 22.005\n#test['UnitPrice']=np.where(test['InvoiceNo']==19300,376.5,test['UnitPrice'])\n#test['UnitPrice']=np.where(test['InvoiceNo']==19626,1136.3,test['UnitPrice'])\n#test['UnitPrice']=np.where(test['InvoiceNo']==8903,389.68,test['UnitPrice'])\n#test['UnitPrice']=np.where(test['InvoiceNo']==21455,599.5,test['UnitPrice'])\n#test['UnitPrice']=np.where(test['InvoiceNo']==6292,334.71,test['UnitPrice'])\n#test['UnitPrice']=np.where(test['InvoiceNo']==5922,8142.75,test['UnitPrice'])","432603cb":"sample['UnitPrice']=xgb_fv_preds\nsample.iloc[16330]=8142 # not present in public leaderboard\n# sample.iloc[92008]=1687 # guess\n# sample.iloc[7731]=1687  # guess\nsample.iloc[83386]=239  # return sure , earlier predicting 1093.494766\nsample.iloc[53044]=599.5  # return sure , earlier predicting 298.490604\nsample.iloc[58695]=3949.32  # return sure , earlier predicting 1227\n#----------21.80 reached-------\nsample.iloc[43891]=376.5 # guess 259\nsample.iloc[74488]=320.69  # guess 877\nsample.iloc[75798]=1136.3  # return sure , earlier predicting 915\nsample.iloc[23739]=389.68  # return sure , earlier predicting 621\nsample.iloc[92008]=1687.17  # return sure , earlier predicting 929.7\nsample.iloc[7731]=1687.17  # return sure , earlier predicting 929.7\nsample.iloc[90863]=222.75  # return sure , earlier predicting 929.7\nsample.iloc[9697]=334.71  # return sure , earlier predicting 298.490604\nsample.iloc[89259]=451.42  # return sure , earlier predicting 298.490604\n#----------22.11 ---------------------\nsample['UnitPrice']=np.where(sample['UnitPrice']<0,0,sample['UnitPrice'])\nsample.head()\nsample.info()\nsample.to_csv('very_Hopefull_3_replacement.csv',index= False)","aba80f01":"sample.iloc[16330]","6b76ab4e":"p=test[test['InvoiceNo']==9608]\nsample.iloc[89259]\np\np.index","a170497a":"sample.sort_values(by=['UnitPrice']).head(100)","dda879ea":"* exact value 4283 \n* 836.84(11,876,018) using count customer id --exact error with this --21.11--38,072,132--26196114.8\n* 395.08(15,115,922) without count id --exact error with this --22.41-42905647--27789724.8","bcc92183":"### few sure short little improving things \n* make less then 0 predictions as zero \n* replace all those stockcode with same values as in train whose unit price remain constant for a particular stock and have sufficient values in them \n* their are few dublicates in test and train set you can directly replace them \n##### All these modifications only slightly improve your score as your model is also predicting close to actual for these","0b1afeb7":"* 0.513 on oof, 0.31 on train with 6 variable \n* 0.438 on oof , 0.11 on trin with 7 variable added country count \n* 0.425 on oof, 0.11 on train with 8 variables 6+ count country +Year\n* 0.425 learning rate 0.3(because earlier taking lot of time)","a5c69c0b":"# Don't Forget to Upvote my Kaggle Notebook, if you find it usefull or even learn anything from this \n##### and if you also want to see how i decide values to replace and their code on Python, Let me know in comment section, i will do so. \n##### and please share your feedback so that i can learn from it and improve my notebook in future. ","1f130e37":"### My public(private score), \n* 15.52(34.99) with 8142 outlier removed, \n* 15.52(26.15) by directly substituting (8142) value in test, i have explained how i get that below, \n* 15,62(27.66) without outlier replaced and without directly replacing value(8142) \n##### I should have also check score, without any replacement and without any outlier removal , but currently Machine Hack not taking  submission , so not able to do so.Pardon me for that","6576800b":"### split max stock price value above 80 or not grouped \n* 190.51, 2.02, 8 variables 6 + count_customer_id and Year\n* 179.85, 1.51 with 6 variables ( but it increasing both public and private leaderboard score)","29ac8bfb":"I personally have achieved Rank 14 on Public Leaderboard, but my rank goes down to 214 on private leaderboard,\nWhereas one of my similar solution, just by not removing 1 outlier can land me to Rank underTop 20 on Private leaderboard also.","de9eb844":"### Comparison between which Advance Gradient Boosting Technique should i chose","9141a23f":"# Approach \n### Basic Approach \n* Just remove 2 Unit price outlier from the data, of value highest value 38970 and 8142.75 (maxi value now 4161.06) \n* Done variable Selection using lofo (it is a bit time consuming and Iterative process)\n* selected parameters model_col=['StockCode','Description', 'Country', 'month', 'day', 'hour']\n* use 5 Kfold shuffle split and Extreme Gradient Boosting , get score of 22.52 on public leaderboard\n* When i remove only one outlier score(38970) my public leaderboard score goes down to 22.66 from 22.52 , whereas my private leaderboard score goes to 30.39 from 37.69.(It is one of biggest mistake i have done, you can land at rank 76 if score 30.39)\n\n### Modification 1\n* I have just added 2 more columns count_customer_id (making major change) and year, This results in decreasing my score on out of fold validation set but increase my score to 21.1 on public leaderboard, which give me some idea that there are few unique count_customer_id in test set and not in train set, which improving score making a big difference, Score Public leaderbaord- 21.1 -Private LB 36.95, if only one outlier removed Private score - 29.46, Rank -57\n\n### Modification 2 \n* If we see closely Higest Qunatity and lowest quantity are same values only the sign is opposite(80995 and -80995) and there price is also same.\n* This give me intution that, person has just return this item at same price. so whenever these kind of things happen price should be same.\n* I  Genrated few more columns such as Sum of quantity per stock per cutomer per day ( df['customer_'+'stock_'+'day_'+'month_'+'year_'+'quantity_sum'] = df.groupby(['StockCode','CustomerID','day','month','year'])['Quantity'].transform('sum')). \n* In train set there are around 670 these type of observations where both -x , and +x quantities are also present, and for 97% of data it hold true that there Unit prices are also same. using that we get arond 230 observation values of test set. (I have done all this in excel)\n* replacing only top 3 values( because these are making major change) improves my score to 20.1 on public leaderboard.( Higest value 8142 also lie in this case) using this your score can be under 28 i guess(cacnot check because now we can submit solution on Machine Hack) and rank under 30. if you use more vlues score can be improved further.\n\n### Last Modification 3 \n* In modification 1, I make an assumption that unique customer id can be reason behind score getting worse. so i make one more more column  and count of unique Customer_id per column stock , I get one value which my model prediting around 800, i made that 100 which worse my score from this i can directly calculate exact value of Unit price for that customer. i found out it to be around 4200 and replaced that. that also improve my score. \n* I have only replaced one value using modification 3, but mutiple values around 8-12(only 3-4 values giving great improvement) using modificaion 2. It final take my score to 15.42 on public leaderboard and Rank of 14. But i realise this is an hadwork so i don't try much values and also get busy in building resume ;)\n* your private leaderboard score can be around 26.15 and rank under 20 using this, but i just remove replacement of 2 outlirs and also not replaced 8142 value which(removing it as outlier improving my scor by 0.1) but it decrease my score to 34.95. \n\n### Playing with fire( just few analysis of data)\n* if we see distribution of target variable it is lot skewed, 97 % values are less than 10\n* same way 99.6% values are less than 18, so if we even predict mean for all even then we can get max rmse as 5, but most of us are getting even higher than it. main reason behind this remaining 0.04% values.\n* assume we have one value as 38970 in our test data and we are predicting that as zero but all the remaining values as correct , then we get rmse as 111.54 (23.30)\n* if we build our model on all stock price whose max value is less than 80( it is a guess you can also check for  200), It is more than 99.4% of data. we can get an rmse of around 0.4-0.5, that directly shows only outliers are main consern in this problem. \n* One more interseting point about this competiton if you submit 2 twice the prediction your public score only move from 22 to 24, that shows major problem may be our model just predicting larger values less","08caa444":"### Turned Out XGB is performing better than LGB and Catboost for this dataset","eb2e459f":"### Feature Genration","669c5952":"# It's Machine Hack Competition\nLink for Competition-https:\/\/www.machinehack.com\/hackathons\/retail_price_prediction_mega_hiring_hackathon\/overview","49ba7b00":"* 20.53 4 fold cv, 6 variabe,2 outliers removed ,24.7 public LB score\n* 20.13 adding variable is day of the week \n* 20.00 adding count_stocks__per_customer_per_time\n* 19.70 max and min of unit price \n* 19.55 adding stopping cretiral to 100 instaed of 10 , 24.2 public LB score\n* 15.40 with given 8 variables, 9.71 on train , 23.47 on Public LB\n* 14.42 with removed customer id, 9.12 on train , 22.57 on Public LB \n* 14.17 added next time price, 8.04 on train ,\n* 14.12 removed quantity , 10.26 on train ,22.52 on Public LB \n* Not worked, 14.47 5 fold earlier it is 4 fold , 11.52 on training , 22.9\n* 13.95 with changed random seed, and shuffel =True in K fold , 10.19 on train,22.79 on Public LB \n* 13.55 with 5 fold , 9.97 on train, 22.52 on public leaderboard\n* Not used, 20.51 with 5 fold , with now only one value removed, 10.97 on train , 22.66 public LB\n* 13.49 ,10.13 using week of year instead of month \n* Not used 13.73 ,9.56 using day of year instead of week of year \n* 13.29 , 10.77 using 10 fold instead of 5 , Public LB score-22.40\n* 13.20 , 10.03 even with 5 fold added 'count_StockCode', Public LB-22.47\n","97c9f1b5":"### Suggestions Given to Machine Hack(I have given this suggestions to machine Hack)\n* Prefer to give those type of datasets where just by changing only few values not totally change leaderboard score.\n* try to give proper evaluation metrics like even in this problem if evaluation metrics are like Root mean Percentage error. few outcomes will not change score entirely.\n* Try to make sure that data set you are providing is atleast not available on Internet.(you may also scaled the data(min max scale or normally scaled) so it decrease the probabily of people understanding that data is copied.( as in this competition i heard, that many competitors are directly used data, it is somewhere present on Kaggle)","855e0ae2":"LOFO , Bit time consuming ","591d3591":"* 12.99 when nestimators=54 ,stopping rounds 10\n* 14.33 when nestimators=1000,stopping rounds 10","3e635179":"### Things which may work\n* I have using only XGB model because it is giving significantly better score than LGB and Catboost on Validation set(oof predictions also), But may be LGB and CB giving better solution on test set( I have seen few top rankers solutions using CATBOST or LGB)\n* Ensembeling of these models can also be a good approach \n* I have not tried all the features i have used, you can also give suggestion how to do feature selection or should i use all of them this may help."}}