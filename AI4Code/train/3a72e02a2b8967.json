{"cell_type":{"f6412fc6":"code","af14b781":"code","21e82d32":"code","c2c7f83e":"code","7a2f4d1f":"code","7cc280d5":"code","89ab62e6":"code","048f3b16":"code","a4996585":"code","0a52ca5e":"code","b6b553c2":"code","96fade31":"code","7ed67822":"code","83d1234b":"code","b6c19422":"code","6a2ee246":"code","e54295aa":"code","17af2d66":"code","f1dc7854":"code","cb1252c2":"code","0f68d391":"code","44ec31e9":"code","df195574":"code","2664e2e0":"code","e634beac":"code","6fb75fb0":"code","325713fa":"code","7ca05f4e":"code","f6ca2e50":"code","f7ed89d8":"code","1dafd5d2":"code","e369a736":"code","677e821f":"code","9caa7f5f":"code","a15c013f":"code","eeb5ff79":"code","33bf6892":"code","9e2a62af":"code","1ce96350":"code","275a5579":"code","98c63448":"code","a3d9f282":"code","0ed07c8a":"code","26073c05":"code","529552e6":"code","ed790d5b":"code","e0c4df22":"code","78a85b66":"code","2eef30ad":"code","73cfe017":"code","0ad592d7":"code","585a400a":"code","ed1e76ad":"code","02b35007":"code","6ad547f4":"code","36e2fbea":"code","c5b056f3":"code","a2c4c5e0":"code","299ac818":"code","df3acf14":"code","b94fa863":"code","a3fdae80":"code","8ba06d75":"code","5ec74aac":"code","58b4c146":"code","acac4f14":"code","ff2feeb4":"code","852f1d3a":"code","64454aaa":"code","ea46fe1e":"code","96ff6365":"code","908187bc":"code","69e59ea0":"code","a08b7ac5":"code","bd4c2808":"code","b881a042":"markdown","fe830cf3":"markdown","8ff00108":"markdown","77d94eeb":"markdown","7bddc71c":"markdown","cacab3ae":"markdown","efef0ee0":"markdown","9ddfa191":"markdown","20546c71":"markdown","3d81185e":"markdown","66f212b6":"markdown","bad5af88":"markdown","efa9d5fa":"markdown","d574e090":"markdown","14a9ce04":"markdown","110564e2":"markdown","644150b4":"markdown","f7bf7eaf":"markdown","74c80914":"markdown","fea4f137":"markdown","d565acc7":"markdown","f87e013c":"markdown","111b9783":"markdown","3297d077":"markdown","0fbefe73":"markdown","6ffe835a":"markdown","2b3f4255":"markdown","fd808ce1":"markdown","f6f54000":"markdown","5d0d0a8d":"markdown","24d136ce":"markdown","717f377c":"markdown","eb973c48":"markdown","4914817c":"markdown","53b1a889":"markdown","4f4d5b57":"markdown","9757d980":"markdown","4eb48d5d":"markdown","276fe8af":"markdown","396ef725":"markdown","55bc6e75":"markdown","43c7c40e":"markdown","d7d21509":"markdown","c4bc8406":"markdown","1cb4a51a":"markdown","49e81f06":"markdown","3834f573":"markdown","0e6fccbe":"markdown","be0ab20d":"markdown","801c7e04":"markdown","55b84040":"markdown","eee274ef":"markdown","496d2ae2":"markdown","f1db0589":"markdown","0e125aa3":"markdown","7be81c29":"markdown","e03590f0":"markdown","d2279dfd":"markdown","114345df":"markdown","7178c812":"markdown","c2d7af7a":"markdown","33b42520":"markdown","5f2c897e":"markdown","d42fd345":"markdown","1670e5ab":"markdown","bdfaf3b5":"markdown","dd154e7a":"markdown","fc10c3b3":"markdown","83379ddc":"markdown","43cb515d":"markdown","c1ffdef9":"markdown","a7b12aa9":"markdown","fa585adc":"markdown","cc86ede1":"markdown","2fd389d9":"markdown","a898df4d":"markdown","b11802bb":"markdown"},"source":{"f6412fc6":"#import some necessary librairies\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\/house-prices-advanced-regression-techniques\/\"]).decode(\"utf8\")) #check the files available in the directory","af14b781":"#Now let's import and put the train and test datasets in  pandas dataframe\n\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","21e82d32":"##display the first five rows of the train dataset.\ntrain.head(5)","c2c7f83e":"##display the first five rows of the test dataset.\ntest.head(5)","7a2f4d1f":"#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","7cc280d5":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","89ab62e6":"#Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","048f3b16":"sns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","a4996585":"from scipy.special import boxcox1p\nlam = 0.0\ntrain[\"SalePrice\"] = boxcox1p(train[\"SalePrice\"], lam)","0a52ca5e":"### If we use the numpy fuction log1p which  applies log(1+x) to all elements of the column, then:\n### train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","b6b553c2":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","96fade31":"all_data","7ed67822":"len(all_data)","83d1234b":"all_data.isnull()","b6c19422":"all_data.isnull().sum()","6a2ee246":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]   #drop all columns (features) that have no missing values (and keep only the top 30 in terms of missing values for the plot below)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","e54295aa":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","17af2d66":"#Correlation map to see how features are correlated with SalePrice\ncorrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","f1dc7854":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")","cb1252c2":"all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")","0f68d391":"all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")","44ec31e9":"all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")","df195574":"all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")","2664e2e0":"#Group by neighborhood and fill in missing value by the median LotFrontage of each corresponding neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","e634beac":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')","6fb75fb0":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)","325713fa":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)","7ca05f4e":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')","f6ca2e50":"all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)","f7ed89d8":"all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])","1dafd5d2":"all_data = all_data.drop(['Utilities'], axis=1)","e369a736":"all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")","677e821f":"all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])","9caa7f5f":"all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])","a15c013f":"all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","eeb5ff79":"all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","33bf6892":"all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")","9e2a62af":"#Check remaining missing values, if any \nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","1ce96350":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","275a5579":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","98c63448":"# For example, note how Street, Alley, Fence ... are encoded (compare with (pre-label encoding) table at the top of the notebook)\nall_data","a3d9f282":"# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","0ed07c8a":"all_data.shape","26073c05":"all_data.dtypes","529552e6":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","ed790d5b":"skewness.shape","e0c4df22":"skewness = skewness[abs(skewness) > 0.75]   \nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)\n    \n#all_data[skewed_features] = np.log1p(all_data[skewed_features])","78a85b66":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","2eef30ad":"all_data   # it is interesting to compare this table below with its previous version above","73cfe017":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","0ad592d7":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","585a400a":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","ed1e76ad":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","02b35007":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","6ad547f4":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","36e2fbea":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","c5b056f3":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","a2c4c5e0":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","299ac818":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","df3acf14":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","b94fa863":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","a3fdae80":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","8ba06d75":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","5ec74aac":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","58b4c146":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","acac4f14":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","ff2feeb4":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","852f1d3a":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","64454aaa":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n","ea46fe1e":"stacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","96ff6365":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","908187bc":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","69e59ea0":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","a08b7ac5":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15\n","bd4c2808":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)","b881a042":"* **Alley** : data description says NA means \"no alley access\".","fe830cf3":"* **Fence** : data description says NA means \"no fence\".","8ff00108":"Notice that in the train data we now have 2 rows less than initially (1460+1459=2919) because above we have deleted the 2 outlier observations.  \n","77d94eeb":"* **LightGBM** :","7bddc71c":"# >>> Imputing missing values  \n\nWe impute them by proceeding sequentially through features with missing values determined above:  \n\n* **PoolQC** : data description says NA means \"No Pool\". Therefore, given the huge ratio of missing value (+99%) this is consistent with the fact that the majority of houses have no Pool.\n","cacab3ae":"Let's see how these base models perform on the data by evaluating the cross-validation rmsle error.","efef0ee0":"# >>> Box-Cox-Transformation of (highly) skewed features\n\nWe use the scipy function boxcox1p which computes the Box-Cox transformation of  1+x .\n\nNote that setting  \u03bb=0  is equivalent to log1p used above for the target variable.\n\nThese are links for more details on [Box Cox Transformation](http:\/\/onlinestatbook.com\/2\/transformations\/box-cox.html) and the [scipy function](https:\/\/docs.scipy.org\/doc\/scipy-0.19.0\/reference\/generated\/scipy.special.boxcox1p.html).","9ddfa191":"# *Stacked Regressions | House Prices*  \n**David Rivas, Ph.D.**  \n\n","20546c71":"* **FireplaceQu** : data description says NA means \"no fireplace\".","3d81185e":"Notes for further research:  \nThe (highly >0.75) skewed range below does not seem to make too much difference because below we end up tranforming ALL (59) the skew (numeric) features. Also,a good exercise would be to plot the distributions of the above numeric features to visually verify the relative differences of skewness (shown in the above table).  ","66f212b6":"* **Utilities** : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . Since the house with 'NoSewa' is in the training set, **this feature won't help in predictive modelling**. We can then safely remove it.","bad5af88":"# >> Defining a cross validation strategy\n\nWe use the [cross_val_score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_val_score.html) function of Sklearn. However, this function does not have a shuffle attribute, so in order to shuffle the dataset we use [KFold](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.KFold.html) prior to cross-validation.  \nWe will be training our models in log space (the train and test data were transformed to this space above so as to comply with the normality assumption). Therefore, we now define our validation function RMSLE - Root Mean Squared Log Error, which we'll use to get a good idea of the improvements in the accuracy of our models in log space.","efa9d5fa":"# >> Skewed features","d574e090":"* **GarageType, GarageFinish, GarageQual and GarageCond** : Replacing missing data with None.","14a9ce04":"* **Exterior1st and Exterior2nd** : Again, both Exterior 1 & 2 have only one missing value. We will just substitute it with the most common string.","110564e2":"It is important to note that even the simplest stacking approach improves the score . This encourages us to go further and explore a less simple stacking approch.","644150b4":"* **Kernel Ridge Regression** :","f7bf7eaf":"**XGBoost**:","74c80914":"**LightGBM**:","fea4f137":"# >> Box-Cox-transformation of the target variable","d565acc7":"# >> Less simple Stacking : Adding a Meta-model  \n![](http:\/\/i.imgur.com\/QBuDOjs.jpg)\n(Image taken from [Faron](https:\/\/www.kaggle.com\/getting-started\/18153#post103381))  \n\n**Base Models** = Model 1, Model 2, Model 3, Model 4, and Model 5  (in each base model the **holdout** is the fold (shown in brown) used to predict by using that base model, which was trained by using the other folds).   \n**Out-of-folds predictions** = **New Feature** predictions (corresponding to the holdouts above).   \n**Meta Model** = Model 6, is a higher learner trained by using the Out-of-folds predictions (input) and the correct\/real target variable (output).   \nFor the final prediction, we calculate the **Average** prediction (**Variant A**) and used it as **meta-features** (input) on which the final prediction (output) is done by using the meta-model.","f87e013c":"# >>> Data Correlations","111b9783":"* **MSZoning (The general zoning classification)** : 'RL' is by far the most common value. So we can fill in missing values with 'RL'.","3297d077":"We get again a better score by adding a meta learner.","0fbefe73":"We will have to invert the Box-Cox transform above to be able to submit the predictions from the test data","6ffe835a":"* **KitchenQual**: Has only one NA value, and thus like with Electrical, we set the missing value in KitchenQual to 'TA' (which is the most frequent value).\n","2b3f4255":"* **Elastic Net Regression** :  \nLet us also make it robust to outliers.","fd808ce1":"Let us verify that no missing values remain:","f6f54000":"* **Electrical** : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.","5d0d0a8d":"* **SaleType** : Fill in again with most frequent which is \"WD\".","24d136ce":"**Ensemble prediction**:","717f377c":"* **Gradient Boosting Regression** :  \nWith **huber** loss that makes it robust to outliers.","eb973c48":"# Reading the data","4914817c":"# >> Simplest Stacking approach : Averaging base models\nWe begin with this simple approach of averaging base models. We build a new class to extend scikit-learn for this purpose by leveraging encapsulation and inheritance.  \n\n**Averaged base models class**","53b1a889":"### Note:\nOutliers removal is not always safe. We decided to delete these two as they are huge and outstandingly bad ( extremely large areas at very low prices).\n\nThere are probably others outliers in the training data. However, removing all of them may affect badly our models if there ever were also outliers in the test data. **That's why , instead of removing them all, we will just manage to make some of our models robust on them. We will see this below in the modelling section of this notebook.**","4f4d5b57":"* **LotFrontage** : Since houses in the same neighborhood are usually similar to one another, we can fill in this **missing value by the median LotFrontage of the neighborhood.**    ","9757d980":"# >> Adding one more important feature - TotalSF  \nSince area related features are very important to determine house prices, we add one more feature which is the total area of basement, first and second floor areas of each house.","4eb48d5d":"**StackedRegressor**:","276fe8af":"We now determine the skewness of the distributions of the numerical features (int64 and float64).","396ef725":"# >> Missing Data  ","55bc6e75":"# Abstract  \nWe start with a rather quick features engineering:  \n* Imputing missing values by proceeding sequentially through the data.\n* Transforming some numerical variables that really seem categorical.\n* Label Encoding some categorical variables that may contain information in their ordering set.\n* [Box Cox Transformation](http:\/\/onlinestatbook.com\/2\/transformations\/box-cox.html) of skewed features (instead of log-transformation) : This gave a slightly better result both on accuracy (leaderboard) and cross-validation.\n* Getting dummy variables for categorical features.\n\nThen, we choose many base models (mostly sklearn based models + sklearn API of DMLC's XGBoost and Microsoft's LightGBM), cross-validate them on the data before stacking\/ensembling them. The key here is to make the (linear) models robust to outliers. This improved the result both in accuracy (leaderboard) and cross-validation. We build two stacking classes (the simplest approach and a less simple one).  \n\nThis work is based on the references and will be further extended in the future. In particular, more exploratory data analysis, including more feature engineering will be performed. ","43c7c40e":"**Stacking Averaged models Score**\n\nTo make the two approaches comparable (by using the same number of models) , we just average **Enet KRR and Gboost**, then we add **lasso as meta-model**.","d7d21509":"The skew seems now corrected and the data appears more normally distributed.  \n\n# > Features engineering  \n#### Concatenating the train and test data in the same dataframe and deleting the SalePrice column","c4bc8406":"# >> Ensembling StackedRegressor, XGBoost and LightGBM   \nWe add **XGBoost** and **LightGBM** to the **StackedRegressor** defined previously.\n\nWe first define a rmsle evaluation function:","1cb4a51a":"* **BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 **: For all these categorical basement-related features, NaN means that there is no basement.","49e81f06":"* **GarageYrBlt, GarageArea and GarageCars **: Replacing missing data with 0 (Since No garage = no cars in such garage).","3834f573":"We analyse how the target (dependent) variable SalePrice correlates with the features (independent variables) ","0e6fccbe":"Our [Explanatory Data Analysis |House Prices](https:\/\/www.kaggle.com\/davidrivasphd\/explanatory-data-analysis-house-prices) analyses the significant variables, missing values and outliers in this data so it is useful to consider it as background information for this secion.  \n# > Outliers","be0ab20d":"**Averaged base models score**  \n\nWe just average four models here **ENet, GBoost, KRR and lasso**. Of course we could easily add more models in the mix.","801c7e04":"Notice that in the above plot by features we mean the columns of the data that have null values (missing data).","55b84040":"# >> Base models scores","eee274ef":"# >> Label Encoding some categorical variables that may contain information in their ordering set","496d2ae2":"# > More feature engineering  \n# >> Transforming some numerical variables that are actually categorical","f1db0589":"# >> Plot of significant variable GrLivArea with its outliers","0e125aa3":"* **LASSO Regression** :  \nThis model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's **Robustscaler()** method on pipeline.","7be81c29":"* **Functional** : data description says NA means typical.","e03590f0":"Indeed, no missing values remain, we have successfully imputed them above.","d2279dfd":"# References used  \n[Stacked Regressions : Top 4% on LeaderBoard](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard) by Serigne  \n[Explanatory Data Analysis |House Prices](https:\/\/www.kaggle.com\/davidrivasphd\/explanatory-data-analysis-house-prices) by David Rivas, Ph.D.  \n[COMPREHENSIVE DATA EXPLORATION WITH PYTHON](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) by Pedro Marcelino   \n[A study on Regression applied to the Ames dataset](https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset) by Julien Cohen-Solal  \n[Regularized Linear Models](https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models) by Alexandru Papiu  \n[XGBoost + Lasso](https:\/\/www.kaggle.com\/humananalog\/xgboost-lasso) by Human Analog  ","114345df":"* **MSSubClass** : Na most likely means No building class. We can replace missing values with None.","7178c812":"#### Import librairies","c2d7af7a":"* **MasVnrArea and MasVnrType** : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type.","33b42520":"# > Stacking models","5f2c897e":"Notes for further research:  \n**lam in the target variable transformation above**  \nThe target variable transformation above is very sensitive to lam (in Ref1 kernel lam = 0.00 for the target variable transformation above - as opposed to lam = 0.15 in the features transformations below).  It may be verified that increasing the lam in the target variable transformation above to slightly positive values decreases the accuracy significantly. On the other hand, using lam = -0.05 or more negative lam values in the target variable transformation above improves the accuracy significantly relative to the Ref1 kernel accuracies, with the training data (but is this due to overfiting?).  \n**lam in the skewness transformation below**  \nIt may be verified that varying lam in the range from 0.0 to 0.2 (in the skewness transformation below) does NOT affect the models results\/accuracy significantly (in Ref1 kernel lam = 0.15 was used).   \nHowever, the question is: should not the target variable transformation above use the exact same lam value as the skewness transformation below? (or is it because of some statistical iterations\/optimizations that we can have such small differences between both lam s?)  \n","d42fd345":"* **MiscFeature** : data description says NA means \"no misc feature\".  ","1670e5ab":"* **XGBoost **:","bdfaf3b5":"# > Target(dependent) Variable SalePrice","dd154e7a":"Therefore, the new train and test data sets are:","fc10c3b3":"The outliers in this case are the two far lower RHS observations.  \n# >> Plot of significant variable GrLivArea without its outliers  \n","83379ddc":"# >> Getting dummy categorical features","43cb515d":"* **BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath** : missing values are likely to be zero for having no basement.","c1ffdef9":"#### Final Training and Prediction","a7b12aa9":"# > Base models","fa585adc":"# Data Processing   ","cc86ede1":"**Stacking averaged Models Class**","2fd389d9":"# Modeling","a898df4d":"**Submission**","b11802bb":"The target variable is right skewed. As (linear) models assume normally distributed data; therefore, we need to transform this variable and make it more normally distributed."}}