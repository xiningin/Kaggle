{"cell_type":{"67136516":"code","fcb166c7":"code","fca5546f":"code","1d9fa763":"code","a14473a8":"code","c2097426":"code","0afb69fc":"code","cea0fb2c":"code","630e05ad":"code","35ed4873":"code","f5d4fc23":"code","daa0904a":"code","e8861b71":"code","58164244":"code","afd4a7bd":"code","03e9646e":"code","fc4a5bd3":"code","7e24c317":"code","a336d001":"code","d1f51e54":"code","b2fef6ad":"code","49a15f39":"code","bfaa19ea":"code","cb705d7a":"code","51448563":"code","a26a8844":"code","03836e9d":"code","32aad899":"code","379158a5":"code","8178e574":"code","ceaa04ca":"code","7b295b1c":"code","cb703cc8":"code","6c947881":"code","b59f4454":"code","fd3793a7":"code","5f696e24":"code","97ac0868":"code","e2835b8a":"code","827d0245":"code","93205f7c":"code","a8fabe07":"code","7568a20d":"code","6566bb14":"code","edad3ce4":"code","74ab1c17":"code","f781b105":"code","cc23aa54":"code","20fd8b14":"code","b8470a92":"code","d0fb522f":"code","4fcd9335":"code","5c047dd4":"code","2de30a5c":"code","252be0a4":"code","e294cc30":"markdown","f4cf65fc":"markdown","cf45d9e0":"markdown","aec7661f":"markdown","432855e3":"markdown","2c7f5c9a":"markdown","620b8a11":"markdown","4e3eb025":"markdown","2a5ab5e9":"markdown","798732f0":"markdown","dbd1352f":"markdown","fb4a8de7":"markdown","60f25b4a":"markdown","049eeb66":"markdown","c7c54bf2":"markdown","7470621c":"markdown","b06c47b2":"markdown","3a9ab489":"markdown","c5226f4a":"markdown","d7218781":"markdown"},"source":{"67136516":"import pandas as pd\nimport numpy as np\nimport re\nimport math\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom pandas.api.types import is_numeric_dtype\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","fcb166c7":"def add_datepart(df, fldname, drop=True, time=False):\n    \"Helper function that adds columns relevant to a date in the column `fldname` of `df`.\"\n    fld = df[fldname]\n    fld_dtype = fld.dtype\n    \n    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtypeType):\n        fld_dtype = np.datetime64\n        \n    if not np.issubdtype(fld_dtype, np.datetime64):\n        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n         \n    prefix = re.sub('[Dd]ate$', '', fldname)\n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n    if time: \n        attr = attr + ['Hour', 'Minute', 'Second']\n    for n in attr: \n        df[prefix + n] = getattr(fld.dt, n.lower())\n    df[prefix + 'Elapsed'] = fld.astype(np.int64) \/\/ 10 ** 9\n        \n    if drop: df.drop(fldname, axis=1, inplace=True)","fca5546f":"def fix_missing(df, na_dict):\n    \"\"\" Fill missing data in a column of df with the median, and add a {name}_na column\n    which specifies if the data was missing.\"\"\"\n    for name,col in df.items():\n        if is_numeric_dtype(col):\n            if pd.isnull(col).sum():\n                df[name+'_na'] = pd.isnull(col)\n                filler = na_dict[name] if name in na_dict else col.median()\n                df[name] = col.fillna(filler)\n                na_dict[name] = filler\n    return na_dict","1d9fa763":"def numericalize(df, max_cat):\n    \"\"\" Changes the column col from a categorical type to it's integer codes.\"\"\"\n    for name, col in df.items():\n        if hasattr(col, 'cat') and (max_cat is None or len(col.cat.categories)>max_cat):\n            df[name] = col.cat.codes+1\n            \ndef get_sample(df,n):\n    \"\"\" Gets a random sample of n rows from df, without replacement.\"\"\"\n#     idxs = sorted(np.random.permutation(len(df))[:n])\n    return df.iloc[-n:].copy()","a14473a8":"def process_df(df_raw,y_fld=None, subset=None, na_dict={}, max_cat=None,):\n    if subset: df = get_sample(df_raw,subset)\n    else: df = df_raw.copy()\n    if y_fld is None: y = None\n    else:\n        if not is_numeric_dtype(df[y_fld]): df[y_fld] = df[y_fld].cat.codes\n        y = df[y_fld].values\n    df.drop(y_fld, axis=1, inplace=True)\n    \n    # Missing continuous values\n    na_dict = fix_missing(df, na_dict)\n    \n    # Normalizing continuous variables\n    means, stds = {}, {}\n    for name,col in df.items():\n        if is_numeric_dtype(col) and col.dtype not in ['bool', 'object']:\n            means[name], stds[name] = col.mean(), col.std()\n            df[name] = (col-means[name])\/stds[name] \n    \n    # categorical variables\n    categorical = []\n    for col in df.columns:\n        if df[col].dtype == 'object' : categorical.append(col)  # pandas treat \"str\" as \"object\"\n    for col in categorical: \n        df[col] = df[col].astype(\"category\").cat.as_ordered()\n        \n    # converting categorical variables to integer codes.\n    numericalize(df, max_cat) # features with cardinality more than \"max_cat\".\n    \n    df = pd.get_dummies(df, dummy_na=True) # one-hot encoding for features with cardinality lower than \"max_cat\".\n    \n    return df, y#, na_dict, means, stds","c2097426":"df_org = pd.read_csv('..\/input\/train\/Train.csv', low_memory=False); df_org.head()","0afb69fc":"df_org.SalePrice = np.log(df_org.SalePrice)","cea0fb2c":"df = df_org.copy()","630e05ad":"add_datepart(df, 'saledate')","35ed4873":"df, y = process_df(df, 'SalePrice'); df.head().T","f5d4fc23":"def split_vals(a,n): return a[:n].copy(), a[n:].copy()\n\nn_valid = 12000  # same as Kaggle's test set size\nn_trn = len(df)-n_valid\nX_train, X_valid = split_vals(df, n_trn)\ny_train, y_valid = split_vals(y, n_trn)\n\nX_train.shape, y_train.shape, X_valid.shape","daa0904a":"def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","e8861b71":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","58164244":"def rf_feature_importance(m,df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)","afd4a7bd":"fi = rf_feature_importance(m, X_train); fi[:10]","03e9646e":"fi.plot('cols', 'imp', figsize=(10,6), legend=False);","fc4a5bd3":"def plot_fi(fi): return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)","7e24c317":"plot_fi(fi[:30]);","a336d001":"to_keep = fi[fi.imp>0.005].cols; len(to_keep) # taking only the important features","d1f51e54":"df_keep = df[to_keep].copy()\nX_train, X_valid = split_vals(df_keep, n_trn)","b2fef6ad":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m) # removing less important features not just saved the computation but also gave better scores.","49a15f39":"fi = rf_feature_importance(m, df_keep)\nplot_fi(fi);","bfaa19ea":"from scipy.cluster import hierarchy as hc\nimport scipy","cb705d7a":"corr = np.round(scipy.stats.spearmanr(df_keep).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(16,10))\ndendrogram = hc.dendrogram(z, labels=df_keep.columns, orientation='left', leaf_font_size=16)\nplt.show()","51448563":"def get_oob(df):\n    m = RandomForestRegressor(n_estimators=30, min_samples_leaf=5, max_features=0.6, n_jobs=-1, oob_score=True)\n    x, _ = split_vals(df, n_trn)\n    m.fit(x, y_train)\n    return m.oob_score_","a26a8844":"get_oob(df_keep)","03836e9d":"for c in ('saleYear', 'saleElapsed', 'fiModelDesc', 'fiBaseModel', 'Grouser_Tracks', 'Coupler_System'):\n    print(c, get_oob(df_keep.drop(c, axis=1)))","32aad899":"to_drop = ['saleYear', 'fiBaseModel', 'Grouser_Tracks']\nget_oob(df_keep.drop(to_drop, axis=1))","379158a5":"df_keep.drop(to_drop, axis=1, inplace=True)\nX_train, X_valid = split_vals(df_keep, n_trn)","8178e574":"keep_cols = df_keep.columns\ndf_keep = df[keep_cols]","ceaa04ca":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","7b295b1c":"df_ext = df_keep.copy()\ndf_ext['is_valid'] = 1\ndf_ext.is_valid[:n_trn] = 0\nx, y = process_df(df_ext, 'is_valid')","cb703cc8":"m = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(x, y);\nm.oob_score_","6c947881":"fi = rf_feature_importance(m, x); fi[:10]","b59f4454":"feats=['SalesID', 'saleElapsed', 'MachineID']","fd3793a7":"(X_train[feats]\/1000).describe()","5f696e24":"(X_valid[feats]\/1000).describe()","97ac0868":"x.drop(feats, axis=1, inplace=True)","e2835b8a":"m = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(x, y);\nm.oob_score_","827d0245":"fi = rf_feature_importance(m, x); fi[:10]","93205f7c":"feats=['SalesID', 'saleElapsed', 'MachineID', 'state', 'saleDay', 'saleDayofyear']","a8fabe07":"X_train, X_valid = split_vals(df_keep, n_trn)\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","7568a20d":"for f in feats:\n    df_subs = df_keep.drop(f, axis=1)\n    X_train, X_valid = split_vals(df_subs, n_trn)\n    m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n    m.fit(X_train, y_train)\n    print(f)\n    print_score(m)","6566bb14":"df_subs = df_keep.drop(['SalesID', 'MachineID'], axis=1)\nX_train, X_valid = split_vals(df_subs, n_trn)\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","edad3ce4":"plot_fi(rf_feature_importance(m, X_train));","74ab1c17":"# np.save('tmp\/subs_cols.npy', np.array(df_subs.columns)) # save the final columsn list\n","f781b105":"from treeinterpreter import treeinterpreter as ti","cc23aa54":"df_train, df_valid = split_vals(df_raw[df_keep.columns], n_trn)","20fd8b14":"row = X_valid.values[None,0]; row","b8470a92":"prediction, bias, contributions = ti.predict(m, row)","d0fb522f":"prediction[0], bias[0]","4fcd9335":"idxs = np.argsort(contributions[0])","5c047dd4":"[o for o in zip(df_keep.columns[idxs], df_valid.iloc[0][idxs], contributions[0][idxs])]","2de30a5c":"contributions[0].sum()","252be0a4":"m = RandomForestRegressor(n_estimators=160, max_features=0.5, n_jobs=-1, oob_score=True)\n%time m.fit(X_train, y_train)\nprint_score(m)","e294cc30":"Now we try removing each variable one at a time.","f4cf65fc":"# Removing redundant features","cf45d9e0":"okay! it means you can classify training and validation samples with 100% accuracy. Clearly, There are some very prominent features which makes it so easy and obvious that something is a training sample or a validation sample.","aec7661f":"Looking good! Let's use this dataframe from here. We'll save the list of columns so we can reuse it later.","432855e3":"# Our final model!","2c7f5c9a":"### Feature Importance\nIt's not normally enough to just to know that a model can make accurate predictions - we also want to know how it's making predictions. The most important way to see this is with feature importance.","620b8a11":"And since we are not using any randomness to split our data, the differences between **mean** and **std** of [SalesID, saleElapsed, MachineID] in validation and train set is huge because they are from completely different time stamps. These differences make sure that we easily distinguish between trainning and validation sample, hence the high oob_score.\n\nIf we get rid of these features i.e we are getting rid of features that differentiate training and validation sample. Removing such features makes training and validation samples less distinct. Making sure that the model will generalize better by learning **actually** important features rather than learning from features like [SalesID, saleElapsed, MachineID] which introduced temporal information (make it difficult for the model to extrapolate).\n\nTree model find it very difficult to extraplolate. Intuitionaly, random forests are taking average of the nearest neighbors to make predictions. If we have temporal features or information in out training data; the model will leverage this temporal information, giving you better scores in validation set. But during the test set, the new data point belongs to different time stamp (completely new, something that model has not seen before), now the model dont have any neighbors to compare, to take average. The model tries it's best, ends up making a false prediction, giving you lower test scores. \n\nSo, removing temporal features can help random forests to extrapolate (to some extend).","4e3eb025":"It looks like we can try one from each group for removal. Let's see what that does.","2a5ab5e9":"# Tree interpreter\nInterpretation of the model is a very rare practice, I don't know why? It gives you a very clear idea of why your tree is making certain prediction, how the values change internally and how much is each feature contributing to the final prediction. If you have some domain knowledge you can better interprete the why or how is certain thing making an impact. \n> Machine Learing + Domanin Knowledge = Great Results","798732f0":"Until this point, we worked with the df_ext and RF classifier.\n___","dbd1352f":"And let's see how this model looks on the full dataset.","fb4a8de7":"Everything until this point in the notebook was covered in the [previous notebook](https:\/\/www.kaggle.com\/ankursingh12\/lessons-learnt-from-fast-ai-lectures-part-1). Again, if you have not read the [Part-1](https:\/\/www.kaggle.com\/ankursingh12\/lessons-learnt-from-fast-ai-lectures-part-1), please go through it before moving any further.","60f25b4a":"Here's our baseline.","049eeb66":"Our score of 0.21229 is as good as the competition winner. Its not directly comparable because we are using different validation set but you can expect similar results. This  is so amazing, with no knowledge of the domain, using only our ML skills, we can easily win this kaggle competition. I highly recommend reading what the winner's did, their explanation and their code. You can find all of it in the [discussion tab](https:\/\/www.kaggle.com\/c\/bluebook-for-bulldozers\/discussion).","c7c54bf2":"## Summary\n\nIn this notebook we covered:\n- **Feature Importance** : How taking only the important features not only saved us from large computation but also gave us better scores\n- **Removing redundant features** : We use scipy package and created a **dendrogram**, then removed the features which represented the same imformation\n- **Extrapolation** : We remove temporal features to help the model generalize better for new data, by learning the features which are actually important\n- **Tree Interpretation** : Tree interpretation can help you answer question that was not possible before. Also opens new doors for domain experts to analysis\n\nIt was a great learning experience for me. Writing these notebooks was also a part of learning. I am really glad that I did it. I am planning to write a medium article as well.\n\nHope these [[Part-1](https:\/\/www.kaggle.com\/ankursingh12\/lessons-learnt-from-fast-ai-lectures-part-1) & [Part-2](https:\/\/www.kaggle.com\/ankursingh12\/lessons-learnt-from-fast-ai-lectures-part-2)] notebooks will help you to learn something new. Please comment to me know the things I need to work on.","7470621c":"# Extrapolation\n\nRemoving temporal features to better generalize the model for unseen data points from different timestamp.","b06c47b2":"One thing that makes this harder to interpret is that there seem to be some variables with very similar meanings. Let's try to remove redundent features.","3a9ab489":"This notebooks is inspired by 2nd Lecture of Fast.ai's ML course. In this notebook, we will be talking about some of the best practices (of removing features) which can save you alot of computation and at the same time increase your score. In particular:\n- Features importance\n- Removing redundant features\n- Removing temporal features\n\nFor all of these topics we will discuss the theory, code and interpretation as well. Hope you all will enjoy it. This is my second kernel, so please comment and let me know the things that you liked and things that you want me to improve. I would love you hear from you.\n\n> **Note:** I highly recommend going through the [Part-1](https:\/\/www.kaggle.com\/ankursingh12\/lessons-learnt-from-fast-ai-lectures-part-1) as this notebook is basically the continuation of it. Alot of code written here is directly taken from Part-1.","c5226f4a":"Lets make a classifier which will classify training and validation sample","d7218781":"Let's try removing some of these related features to see if the model can be simplified without impacting the accuracy."}}