{"cell_type":{"b1dcd9f3":"code","41990d5d":"code","4e0795df":"code","cf33f46d":"code","8c2a9dc2":"code","7a476dce":"code","2eb7d9ee":"code","3e90e038":"code","45645628":"code","37013ef2":"code","fa993536":"code","0d44c5bd":"code","e93ca06f":"code","8a478f42":"code","aa61b9c5":"code","8ea1cab5":"code","9ee4bf0d":"code","44ac3cd3":"code","7aee95c1":"code","083a72fb":"code","9b0053f0":"code","4fed0fdd":"code","42de0276":"code","1a73c477":"code","e78436cb":"code","24cb146f":"code","4823f099":"code","80e10f07":"code","6ab2587f":"code","dd2e4d1d":"code","c6e5bd38":"code","b87f8fc5":"markdown","0c454587":"markdown","65fd399e":"markdown","ee947df3":"markdown","c34e1bfc":"markdown","a35e7739":"markdown","519199e9":"markdown","d766bdd3":"markdown","e385609b":"markdown","6cc0f567":"markdown","e817175e":"markdown","71710702":"markdown","4da7cfbd":"markdown","f8aa2e4c":"markdown","63128f03":"markdown","a791b570":"markdown","cebf632a":"markdown","d8e435ff":"markdown","ec5f260a":"markdown","f65bf769":"markdown","a9b86075":"markdown","92a027b2":"markdown","42243d44":"markdown","27df04f8":"markdown","ba7cb3a0":"markdown","64e60812":"markdown","3c58b515":"markdown","fa91569b":"markdown","44f880dc":"markdown","ade3f70e":"markdown","92728c56":"markdown","72ba992c":"markdown","a14db706":"markdown","1ef32414":"markdown","cd437398":"markdown","40aa2269":"markdown","4afa0204":"markdown"},"source":{"b1dcd9f3":"# Python libraries\n# Classic,data manipulation and linear algebra\nfrom datetime import datetime\nfrom scipy import interp\nimport pandas as pd\nimport numpy as np\nimport itertools\n\n# Plots\nimport shap\nshap.initjs()\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\n\n# Data processing, metrics and modeling\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_curve, precision_recall_curve\nfrom sklearn.metrics import accuracy_score, roc_auc_score, f1_score, auc\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb\nimport catboost as cb\n\n# Filter werning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', 500)\n\n#Timer\ndef timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('Time taken for Modeling: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))","41990d5d":"#Dataset\ndata = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')","4e0795df":"display(data.head(), data.describe(), data.shape)","cf33f46d":"colors = ['darkturquoise', 'darkorange']\nplt.style.use('dark_background')\nplt.rcParams['figure.figsize']=(15,8)\n\nax = sns.countplot(x='target', data=data, palette=colors, alpha=0.9, edgecolor=('white'), linewidth=2)\nax.set_ylabel('count', fontsize=12)\nax.set_xlabel('target', fontsize=12)\nax.grid(b=True, which='major', color='grey', linewidth=0.2)\nplt.title('Target count', fontsize=18)\nplt.show()\n\ntarget_0 = len(data[data.target == 0])\ntarget_1 = len(data[data.target == 1])\nprint(\"Percentage Haven't Heart Disease: {:.2f}%\".format((target_0 \/ (len(data.target))*100)))\nprint(\"Percentage Have Heart Disease: {:.2f}%\".format((target_1 \/ (len(data.target))*100)))","8c2a9dc2":"# Correlation matrix \nf, (ax1, ax2) = plt.subplots(1,2,figsize =(15, 8))\ncorr = data.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nheatmapkws = dict(linewidths=0.1) \nsns.heatmap((data[data['target'] ==1]).corr(), vmax = .8, square=True, ax = ax2, cmap = 'YlGnBu', mask=mask, **heatmapkws);\nax1.set_title('Disease', fontsize=18)\nsns.heatmap((data[data['target'] ==0]).corr(), vmax = .8, square=True, ax = ax1, cmap = 'afmhot', mask=mask,**heatmapkws);\nax2.set_title('Healthy', fontsize=18)\nplt.show()","7a476dce":"f,ax=plt.subplots(3,2,figsize=(12,12))\nf.delaxes(ax[2,1])\n\nfor i,feature in enumerate(['age','thalach','chol','trestbps','oldpeak']):\n    sns.distplot(data[feature], ax=ax[i\/\/2,i%2], kde_kws={\"color\":\"white\"}, hist=False )\n\n    # Get the two lines from the ax[i\/\/2,i%2]es to generate shading\n    l1 = ax[i\/\/2,i%2].lines[0]\n\n    # Get the xy data from the lines so that we can shade\n    x1 = l1.get_xydata()[:,0]\n    y1 = l1.get_xydata()[:,1]\n    ax[i\/\/2,i%2].fill_between(x1,y1, color=\"goldenrod\", alpha=0.8)\n\n    #grid\n    ax[i\/\/2,i%2].grid(b=True, which='major', color='grey', linewidth=0.3)\n    \n    ax[i\/\/2,i%2].set_title('Distribution of {}'.format(feature), fontsize=18)\n    ax[i\/\/2,i%2].set_ylabel('count', fontsize=12)\n    ax[i\/\/2,i%2].set_xlabel('Modality', fontsize=12)\n\n    ax[i\/\/2,i%2].set_ylabel(\"frequency\", fontsize=12)\n    ax[i\/\/2,i%2].set_xlabel(str(feature), fontsize=12)\n    \nplt.tight_layout()\nplt.show()","2eb7d9ee":"f,ax=plt.subplots(4,2,figsize=(12,12))\n\nfor i,feature in enumerate(['sex','cp','fbs','restecg','exang','slope','ca','thal']):\n    colors = ['darkturquoise']\n    sns.countplot(x=feature,data=data,ax=ax[i\/\/2,i%2], palette = colors, alpha=0.8, edgecolor=('white'), linewidth=2)\n    ax[i\/\/2,i%2].grid(b=True, which='major', color='grey', linewidth=0.2)\n    ax[i\/\/2,i%2].set_title('Count of {}'.format(feature), fontsize=18)\n    ax[i\/\/2,i%2].set_ylabel('count', fontsize=12)\n    ax[i\/\/2,i%2].set_xlabel('modality', fontsize=12)\n\nplt.tight_layout()\nplt.show()","3e90e038":"f,ax=plt.subplots(3,2,figsize=(12,12))\nf.delaxes(ax[2,1])\n\nfor i,feature in enumerate(['age','thalach','chol','trestbps','oldpeak','age']):\n    sns.distplot(data[data['target']==0][(feature)], ax=ax[i\/\/2,i%2], kde_kws={\"color\":\"white\"}, hist=False )\n    sns.distplot(data[data['target']==1][(feature)], ax=ax[i\/\/2,i%2], kde_kws={\"color\":\"white\"}, hist=False )\n\n    # Get the two lines from the ax[i\/\/2,i%2]es to generate shading\n    l1 = ax[i\/\/2,i%2].lines[0]\n    l2 = ax[i\/\/2,i%2].lines[1]\n\n    # Get the xy data from the lines so that we can shade\n    x1 = l1.get_xydata()[:,0]\n    y1 = l1.get_xydata()[:,1]\n    x2 = l2.get_xydata()[:,0]\n    y2 = l2.get_xydata()[:,1]\n    ax[i\/\/2,i%2].fill_between(x2,y2, color=\"darkorange\", alpha=0.6)\n    ax[i\/\/2,i%2].fill_between(x1,y1, color=\"darkturquoise\", alpha=0.6)\n\n    #grid\n    ax[i\/\/2,i%2].grid(b=True, which='major', color='grey', linewidth=0.3)\n    \n    ax[i\/\/2,i%2].set_title('{} vs target'.format(feature), fontsize=18)\n    ax[i\/\/2,i%2].set_ylabel('count', fontsize=12)\n    ax[i\/\/2,i%2].set_xlabel('Modality', fontsize=12)\n\n    #sns.despine(ax[i\/\/2,i%2]=ax[i\/\/2,i%2], left=True)\n    ax[i\/\/2,i%2].set_ylabel(\"frequency\", fontsize=12)\n    ax[i\/\/2,i%2].set_xlabel(str(feature), fontsize=12)\n\nplt.tight_layout()\nplt.show()","45645628":"f,ax=plt.subplots(3,2,figsize=(12,12))\nf.delaxes(ax[2,1])\ncolors=['darkturquoise','darkorange']\nfor i,feature in enumerate(['age','thalach','chol','trestbps','oldpeak']):\n    sns.boxplot(x='target', y=feature, data=data , ax=ax[i\/\/2,i%2], palette=colors, boxprops=dict(alpha=0.8))\n    sns.stripplot(y=feature, x='target', \n                          data=data,\n                          ax=ax[i\/\/2,i%2],\n                          jitter=True, marker='o',\n                          alpha=0.6, \n                          color=\"springgreen\")\n\n    ax[i\/\/2,i%2].grid(b=True, which='major', color='grey', linewidth=0.2)\n    ax[i\/\/2,i%2].set_title('Count of {}'.format(feature), fontsize=18)\n    ax[i\/\/2,i%2].set_ylabel('count', fontsize=12)\n    ax[i\/\/2,i%2].set_xlabel('Modality', fontsize=12)\n    \n    sns.despine()\n    ax[i\/\/2,i%2].grid(b=True, which='major', color='grey', linewidth=0.4)\n\n    ax[i\/\/2,i%2].set_title(str(feature)+' '+'vs'+' '+'target', fontsize=18)\n    ax[i\/\/2,i%2].set_ylabel(\"count\", fontsize=12)\n    ax[i\/\/2,i%2].set_xlabel(('target'), fontsize=12)\n\n    plt.setp(ax[i\/\/2,i%2].artists, edgecolor = 'white')\n    plt.setp(ax[i\/\/2,i%2].lines, color='white')\n\nplt.tight_layout()\nplt.show()","37013ef2":"f,ax=plt.subplots(4,2,figsize=(12,12))\n\nfor i,feature in enumerate(['sex','cp','fbs','restecg','exang','slope','ca','thal']):\n    colors = ['darkturquoise', 'darkorange']\n    sns.countplot(x=feature,data=data,hue='target',ax=ax[i\/\/2,i%2], palette = colors, alpha=0.7, edgecolor=('white'), linewidth=2)\n    ax[i\/\/2,i%2].grid(b=True, which='major', color='grey', linewidth=0.4)\n    ax[i\/\/2,i%2].set_title('Count of {} vs target'.format(feature), fontsize=18)\n    ax[i\/\/2,i%2].legend(loc='best')\n    ax[i\/\/2,i%2].set_ylabel('count', fontsize=12)\n    ax[i\/\/2,i%2].set_xlabel('modality', fontsize=12)\n\nplt.tight_layout()\nplt.show()","fa993536":"def scatterplot(var1,var2,var3,var4):\n    f,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n    #f.delaxes(ax[2,1])\n    \n    colors = ['darkturquoise','darkorange']\n    ax1 = sns.scatterplot(x = data[var1], y = data[var2], hue = \"target\",\n                        data = data,  ax=ax1, palette=colors, alpha=0.8, edgecolor=\"white\",linewidth=0.1)\n    ax1.grid(b=True, which='major', color='lightgrey', linewidth=0.2)\n    ax1.set_title(str(var1)+' '+'vs'+' '+str(var2)+' '+'vs target', fontsize=18)\n    ax1.set_xlabel(str(var1), fontsize=12)\n    ax1.set_ylabel(str(var2), fontsize=12)\n\n    ax2 = sns.scatterplot(x = data[var3], y = data[var4], hue = \"target\",\n                        data = data,  ax=ax2, palette=colors, alpha=0.8, edgecolor=\"white\",linewidth=0.1)\n    ax2.grid(b=True, which='major', color='lightgrey', linewidth=0.2)\n    ax2.set_title(str(var3)+' '+'vs'+' '+str(var4)+' '+'vs target', fontsize=18)\n    ax2.set_xlabel(str(var1), fontsize=12)\n    ax2.set_ylabel(str(var2), fontsize=12)","0d44c5bd":"scatterplot('age','thalach','age', 'chol')\nscatterplot('age','trestbps','age', 'oldpeak')\nscatterplot('thalach','trestbps','thalach', 'chol')\nscatterplot('thalach','oldpeak','chol', 'trestbps')\nscatterplot('chol','oldpeak','trestbps', 'oldpeak')","e93ca06f":"def multivariate_count(var):\n    for x in [\n        #'sex',\n        'cp',\n        'fbs',\n        #'restecg',\n        #'exang',\n        #'slope',\n        #'ca',\n        'thal']:\n        ax = sns.catplot(x=x, hue=\"target\", col=var, \n               data=data, kind=\"count\", palette = ['darkturquoise', 'darkorange'], alpha=0.7, edgecolor=('white'), linewidth=2)\n        ax.fig.suptitle(str(var)+' vs '+str(x)+' vs target', fontsize=18) \n    \n        plt.subplots_adjust(top=0.8)\n        plt.show()  ","8a478f42":"multivariate_count('exang')\n#multivariate_count('fbs')\n#multivariate_count('cp')\n#multivariate_count('ca')\n#multivariate_count('restecg')\n#multivariate_count('slope')\n#multivariate_count('thal')\nmultivariate_count('sex')","aa61b9c5":"def multivariate_swarn(col,x,y):\n    g = sns.FacetGrid(data, col=col, hue='target', palette = ['darkturquoise', 'darkorange'], height=5)\n    ax = g.map(sns.swarmplot, x, y, alpha=0.7, edgecolor=('white'), linewidth=0.1)\n    ax.fig.suptitle(str(x)+' vs '+str(y)+' by '+str(col)+' vs target', fontsize=18) \n    plt.subplots_adjust(top=0.8)\n    plt.show()","8ea1cab5":"multivariate_swarn('fbs','exang','oldpeak')\n#multivariate_swarn('exang','thal','thalach')\n#multivariate_swarn('sex','thal','chol')\n#multivariate_swarn('slope','ca','age')\n\nmultivariate_swarn('fbs','ca','oldpeak')\n#multivariate_swarn('exang','cp','thalach')\n#multivariate_swarn('sex','restecg','chol')\n#multivariate_swarn('slope','thal','age')\n\nmultivariate_swarn('fbs','ca','age')\n#multivariate_swarn('exang','cp','oldpeak')\n#multivariate_swarn('sex','restecg','thalach')\n#multivariate_swarn('slope','thal','chol')\n\nmultivariate_swarn('exang','cp','age')\n#multivariate_swarn('restecg','ca','oldpeak')\n#multivariate_swarn('ca','sex','thalach')\n#multivariate_swarn('cp','slope','chol')","9ee4bf0d":"data['oldpeak_x_age'] = data['oldpeak'] * data['age']\ndata['oldpeak_div_age'] = data['oldpeak'] \/ data['age']\ndata['ca_div_age'] = data['ca'] \/ data['age']\ndata['oldpeak_square'] = data['oldpeak'] * data['oldpeak']\n\ndata = data.fillna(0)\n\nfeatures = list(data)\nfeatures.remove('target')\n\nidx = features \nfor df in [data]:\n    df['sum'] = df[idx].sum(axis=1)  \n    df['count'] = df[idx].count(axis=1)  \n    df['min'] = df[idx].min(axis=1)\n    df['max'] = df[idx].max(axis=1)\n    df['mean'] = df[idx].mean(axis=1)\n    df['std'] = df[idx].std(axis=1)\n    df['skew'] = df[idx].skew(axis=1)\n    df['kurt'] = df[idx].kurtosis(axis=1)\n    df['med'] = df[idx].median(axis=1)","44ac3cd3":"def preprocessing(dataset, y):\n    target_col = [y]\n    cat_cols   = dataset.nunique()[dataset.nunique() < 5].keys().tolist()\n    cat_cols   = [x for x in cat_cols ]\n    #numerical columns\n    num_cols   = [x for x in dataset.columns if x not in cat_cols + target_col]\n    #Binary columns with 2 values\n    bin_cols   = dataset.nunique()[dataset.nunique() == 2].keys().tolist()\n    #Columns more than 2 values\n    multi_cols = [i for i in cat_cols if i not in bin_cols]\n\n    #Label encoding Binary columns\n    le = LabelEncoder()\n    for i in bin_cols :\n        dataset[i] = le.fit_transform(dataset[i])\n\n    #Duplicating columns for multi value columns\n    dataset = pd.get_dummies(data = dataset,columns = multi_cols )\n\n    #Scaling Numerical columns\n    std = StandardScaler()\n    scaled = std.fit_transform(dataset[num_cols])\n    scaled = pd.DataFrame(scaled,columns=num_cols)\n\n    #dropping original values merging scaled values for numerical columns\n    df_dataset_og = dataset.copy()\n    dataset = dataset.drop(columns = num_cols,axis = 1)\n    dataset = dataset.merge(scaled,left_index=True,right_index=True,how = \"left\")\n    return dataset","7aee95c1":"data = preprocessing(data,'target')","083a72fb":"train_df = data\nfeatures = list(train_df)\nfeatures.remove('target')\ntarget = train_df['target']","9b0053f0":"# Confusion matrix \ndef plot_confusion_matrix(cm, classes,\n                          normalize = False,\n                          title = 'Confusion matrix\"',\n                          cmap = plt.cm.Blues) :\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title, fontsize=12)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 0)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment = 'center',\n                 color = 'white' if cm[i, j] > thresh else 'black')\n \n    plt.tight_layout()\n    plt.ylabel('True label', fontsize=12)\n    plt.xlabel('Predicted label', fontsize=12)","4fed0fdd":"model = lgb.LGBMClassifier(**{\n                'learning_rate': 0.06,\n                'feature_fraction': 0.7,\n                'bagging_freq': 6,\n                'scale_pos_weight': 1,         \n                'bagging_fraction': 0.3,\n                'max_depth':-1,\n                'objective': 'binary',\n                'n_jobs': -1,\n                'n_estimators':5000,\n                'metric':'auc',\n                'save_binary': True,\n                'feature_fraction_seed': 42,\n                'bagging_seed': 42,\n                'boosting_type': 'gbdt',\n                'verbose': 1,\n                'is_unbalance': False,\n                'boost_from_average': True\n})","42de0276":"plt.rcParams['figure.figsize']=(6,4)\n\nprint('LGBM modeling...')\nstart_time = timer(None)\n\ncms= []\ntprs = []\naucs = []\ny_real = []\ny_proba = []\nrecalls = []\nroc_aucs = []\nf1_scores = []\naccuracies = []\nprecisions = []\n\noof = np.zeros(len(train_df))\nmean_fpr = np.linspace(0,1,100)\nfeature_importance_df = pd.DataFrame()\ni = 1\n\nfolds = StratifiedKFold(n_splits=5, shuffle=False, random_state=42)\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n    print('Fold:', fold_ )\n    model = model.fit(train_df.iloc[trn_idx][features], target.iloc[trn_idx],\n                      eval_set = (train_df.iloc[val_idx][features], target.iloc[val_idx]),\n                      verbose = 100,\n                      eval_metric = 'auc',\n                      early_stopping_rounds = 100)\n    \n    oof[val_idx] =  model.predict_proba(train_df.iloc[val_idx][features])[:,1]\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = model.feature_importances_\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    # Roc curve by fold\n    f = plt.figure(1)\n    fpr, tpr, t = roc_curve(train_df.target[val_idx], oof[val_idx])\n    tprs.append(interp(mean_fpr, fpr, tpr))\n    roc_auc = auc(fpr, tpr)\n    aucs.append(roc_auc)\n    plt.plot(fpr, tpr, lw=2, alpha=0.5, label='ROC fold %d (AUC = %0.4f)' % (i,roc_auc))\n    \n    # Precion recall by folds\n    g = plt.figure(2)\n    precision, recall, _ = precision_recall_curve(train_df.target[val_idx], oof[val_idx])\n    y_real.append(train_df.target[val_idx])\n    y_proba.append(oof[val_idx])\n    plt.plot(recall, precision, lw=2, alpha=0.3, label='P|R fold %d' % (i))  \n    \n    i= i+1\n    \n    # Shap values\n    explainer = shap.TreeExplainer(model)\n    shap_values = shap.TreeExplainer(model).shap_values(train_df.iloc[val_idx][features])\n    \n    # Scores \n    roc_aucs.append(roc_auc_score(train_df.target[val_idx], oof[val_idx]))\n    accuracies.append(accuracy_score(train_df.target[val_idx], oof[val_idx].round()))\n    recalls.append(recall_score(train_df.target[val_idx], oof[val_idx].round()))\n    precisions.append(precision_score(train_df.target[val_idx], oof[val_idx].round()))\n    f1_scores.append(f1_score(train_df.target[val_idx], oof[val_idx].round()))\n    \n    # Confusion matrix by folds\n    cms.append(confusion_matrix(train_df.target[val_idx], oof[val_idx].round()))\n    \n# Metrics\nprint(\n        '\\nCV roc score        : {0:.4f}, std: {1:.4f}.'.format(np.mean(roc_aucs), np.std(roc_aucs)),\n        '\\nCV accuracy score   : {0:.4f}, std: {1:.4f}.'.format(np.mean(accuracies), np.std(accuracies)),\n        '\\nCV recall score     : {0:.4f}, std: {1:.4f}.'.format(np.mean(recalls), np.std(recalls)),\n        '\\nCV precision score  : {0:.4f}, std: {1:.4f}.'.format(np.mean(precisions), np.std(precisions)),\n        '\\nCV f1 score         : {0:.4f}, std: {1:.4f}.'.format(np.mean(f1_scores), np.std(f1_scores))\n)\n\n# Timer end    \ntimer(start_time)\n\n# Roc curve\nf = plt.figure(1)\nplt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'grey')\nmean_tpr = np.mean(tprs, axis=0)\nmean_auc = auc(mean_fpr, mean_tpr)\nplt.plot(mean_fpr, mean_tpr, color='blue',\n         label=r'Mean ROC (AUC = %0.4f)' % ((mean_auc)),lw=2, alpha=1)\nplt.grid(b=True, which='major', color='grey', linewidth=0.4)\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate', fontsize=12)\nplt.title('LGBM - ROC by folds', fontsize=18)\nplt.legend(loc=\"lower right\")\n\n# PR plt\ng = plt.figure(2)\nplt.plot([0,1],[1,0],linestyle = '--',lw = 2,color = 'grey')\ny_real = np.concatenate(y_real)\ny_proba = np.concatenate(y_proba)\nprecision, recall, _ = precision_recall_curve(y_real, y_proba)\nplt.plot(recall, precision, color='blue',\n         label=r'Mean P|R')\nplt.grid(b=True, which='major', color='grey', linewidth=0.4)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('LGBM P|R curve by folds', fontsize=18)\nplt.legend(loc=\"lower left\")\n\n# Confusion matrix \nplt.rcParams[\"axes.grid\"] = False\ncm = np.average(cms, axis=0)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title='LGBM Confusion matrix [averaged\/folds]')\nplt.show()","1a73c477":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:37].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(10,10))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False),\n            edgecolor=('white'), linewidth=2, palette=\"rocket\")\nplt.title('LGBM Features importance (averaged\/folds)', fontsize=18)\nplt.tight_layout()","e78436cb":"display(\nshap.force_plot(explainer.expected_value, shap_values[0,:], train_df.iloc[val_idx][features].iloc[0,:],figsize=(10, 5)),\nshap.force_plot(explainer.expected_value, shap_values[1,:], train_df.iloc[val_idx][features].iloc[1,:],figsize=(10, 5)),\nshap.force_plot(explainer.expected_value, shap_values[2,:], train_df.iloc[val_idx][features].iloc[2,:],figsize=(10, 5)),\nshap.force_plot(explainer.expected_value, shap_values[3,:], train_df.iloc[val_idx][features].iloc[3,:],figsize=(10, 5)),\nshap.force_plot(explainer.expected_value, shap_values[4,:], train_df.iloc[val_idx][features].iloc[4,:],figsize=(10, 5)),\nshap.force_plot(explainer.expected_value, shap_values[5,:], train_df.iloc[val_idx][features].iloc[5,:],figsize=(10, 5)))","24cb146f":"shap.force_plot(explainer.expected_value, shap_values, train_df.iloc[val_idx][features],figsize=(10, 5), plot_cmap='RdBu')","4823f099":"model = lgb.LGBMClassifier(**{\n                'learning_rate': 0.06,\n                'feature_fraction': 0.7,\n                'bagging_freq': 6,\n                'scale_pos_weight': 1.75,         \n                'bagging_fraction': 0.3,\n                'max_depth':-1,\n                'objective': 'binary',\n                'n_jobs': -1,\n                'n_estimators':5000,\n                'metric':'auc',\n                'save_binary': True,\n                'feature_fraction_seed': 42,\n                'bagging_seed': 42,\n                'boosting_type': 'gbdt',\n                'verbose': 1,\n                'is_unbalance': False,\n                'boost_from_average': True\n})","80e10f07":"plt.rcParams['figure.figsize']=(6,4)\n\nprint('LGBM modeling...')\nstart_time = timer(None)\n\ncms= []\ntprs = []\naucs = []\ny_real = []\ny_proba = []\nrecalls = []\nroc_aucs = []\nf1_scores = []\naccuracies = []\nprecisions = []\n\noof = np.zeros(len(train_df))\nmean_fpr = np.linspace(0,1,100)\nfeature_importance_df = pd.DataFrame()\ni = 1\n\nfolds = StratifiedKFold(n_splits=5, shuffle=False, random_state=42)\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n    print('Fold:', fold_ )\n    model = model.fit(train_df.iloc[trn_idx][features], target.iloc[trn_idx],\n                      eval_set = (train_df.iloc[val_idx][features], target.iloc[val_idx]),\n                      verbose = 100,\n                      eval_metric = 'auc',\n                      early_stopping_rounds = 100)\n    \n    oof[val_idx] =  model.predict_proba(train_df.iloc[val_idx][features])[:,1]\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = model.feature_importances_\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    # Roc curve by fold\n    f = plt.figure(1)\n    fpr, tpr, t = roc_curve(train_df.target[val_idx], oof[val_idx])\n    tprs.append(interp(mean_fpr, fpr, tpr))\n    roc_auc = auc(fpr, tpr)\n    aucs.append(roc_auc)\n    plt.plot(fpr, tpr, lw=2, alpha=0.5, label='ROC fold %d (AUC = %0.4f)' % (i,roc_auc))\n    \n    # Precion recall by folds\n    g = plt.figure(2)\n    precision, recall, _ = precision_recall_curve(train_df.target[val_idx], oof[val_idx])\n    y_real.append(train_df.target[val_idx])\n    y_proba.append(oof[val_idx])\n    plt.plot(recall, precision, lw=2, alpha=0.3, label='P|R fold %d' % (i))  \n    \n    i= i+1\n    \n    # Shap values\n    explainer = shap.TreeExplainer(model)\n    shap_values = shap.TreeExplainer(model).shap_values(train_df.iloc[val_idx][features])\n    \n    # Scores \n    roc_aucs.append(roc_auc_score(train_df.target[val_idx], oof[val_idx]))\n    accuracies.append(accuracy_score(train_df.target[val_idx], oof[val_idx].round()))\n    recalls.append(recall_score(train_df.target[val_idx], oof[val_idx].round()))\n    precisions.append(precision_score(train_df.target[val_idx], oof[val_idx].round()))\n    f1_scores.append(f1_score(train_df.target[val_idx], oof[val_idx].round()))\n    \n    # Confusion matrix by folds\n    cms.append(confusion_matrix(train_df.target[val_idx], oof[val_idx].round()))\n    \n# Metrics\nprint(\n        '\\nCV roc score        : {0:.4f}, std: {1:.4f}.'.format(np.mean(roc_aucs), np.std(roc_aucs)),\n        '\\nCV accuracy score   : {0:.4f}, std: {1:.4f}.'.format(np.mean(accuracies), np.std(accuracies)),\n        '\\nCV recall score     : {0:.4f}, std: {1:.4f}.'.format(np.mean(recalls), np.std(recalls)),\n        '\\nCV precision score  : {0:.4f}, std: {1:.4f}.'.format(np.mean(precisions), np.std(precisions)),\n        '\\nCV f1 score         : {0:.4f}, std: {1:.4f}.'.format(np.mean(f1_scores), np.std(f1_scores))\n)\n\n# Timer end    \ntimer(start_time)\n\n# Roc curve\nf = plt.figure(1)\nplt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'grey')\nmean_tpr = np.mean(tprs, axis=0)\nmean_auc = auc(mean_fpr, mean_tpr)\nplt.plot(mean_fpr, mean_tpr, color='blue',\n         label=r'Mean ROC (AUC = %0.4f)' % ((mean_auc)),lw=2, alpha=1)\nplt.grid(b=True, which='major', color='grey', linewidth=0.4)\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate', fontsize=12)\nplt.title('LGBM - ROC by folds', fontsize=18)\nplt.legend(loc=\"lower right\")\n\n# PR plt\ng = plt.figure(2)\nplt.plot([0,1],[1,0],linestyle = '--',lw = 2,color = 'grey')\ny_real = np.concatenate(y_real)\ny_proba = np.concatenate(y_proba)\nprecision, recall, _ = precision_recall_curve(y_real, y_proba)\nplt.plot(recall, precision, color='blue',\n         label=r'Mean P|R')\nplt.grid(b=True, which='major', color='grey', linewidth=0.4)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('LGBM P|R curve by folds', fontsize=18)\nplt.legend(loc=\"lower left\")\n\n# Confusion matrix \nplt.rcParams[\"axes.grid\"] = False\ncm = np.average(cms, axis=0)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title='LGBM Confusion matrix [averaged\/folds]')\nplt.show()","6ab2587f":"model = cb.CatBoostClassifier(**{\n                                'learning_rate':0.05,\n                                'max_depth':2,\n                                'n_estimators':100,\n                                'eval_metric': 'AUC',\n                                'bootstrap_type': 'Bayesian',\n                                'use_best_model':True,\n                                'bagging_temperature': 1,\n                                'objective': 'Logloss',\n                                'od_type': 'Iter',\n                                'l2_leaf_reg': 2,\n                                'allow_writing_files': False})","dd2e4d1d":"plt.rcParams['figure.figsize']=(6,4)\n\nprint('CatBoost modeling...')\nstart_time = timer(None)\n\ncms= []\ntprs = []\naucs = []\ny_real = []\ny_proba = []\nrecalls = []\nroc_aucs = []\nf1_scores = []\naccuracies = []\nprecisions = []\n\noof = np.zeros(len(train_df))\nmean_fpr = np.linspace(0,1,100)\nfeature_importance_df = pd.DataFrame()\ni = 1\n\nfolds = StratifiedKFold(n_splits=5, shuffle=False, random_state=42)\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n    print('Fold:', fold_ )\n    model = model.fit(train_df.iloc[trn_idx][features], target.iloc[trn_idx],\n                      eval_set = (train_df.iloc[val_idx][features], target.iloc[val_idx]),\n                      verbose = 100,\n                      early_stopping_rounds=100)\n    \n    oof[val_idx] =  model.predict_proba(train_df.iloc[val_idx][features])[:,1]\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = model.feature_importances_\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    # Roc curve by fold\n    f = plt.figure(1)\n    fpr, tpr, t = roc_curve(train_df.target[val_idx], oof[val_idx])\n    tprs.append(interp(mean_fpr, fpr, tpr))\n    roc_auc = auc(fpr, tpr)\n    aucs.append(roc_auc)\n    plt.plot(fpr, tpr, lw=2, alpha=0.5, label='ROC fold %d (AUC = %0.4f)' % (i,roc_auc))\n    \n    # Precion recall by folds\n    g = plt.figure(2)\n    precision, recall, _ = precision_recall_curve(train_df.target[val_idx], oof[val_idx])\n    y_real.append(train_df.target[val_idx])\n    y_proba.append(oof[val_idx])\n    plt.plot(recall, precision, lw=2, alpha=0.3, label='P|R fold %d' % (i))  \n    \n    i= i+1\n    \n    # Scores \n    roc_aucs.append(roc_auc_score(train_df.target[val_idx], oof[val_idx]))\n    accuracies.append(accuracy_score(train_df.target[val_idx], oof[val_idx].round()))\n    recalls.append(recall_score(train_df.target[val_idx], oof[val_idx].round()))\n    precisions.append(precision_score(train_df.target[val_idx], oof[val_idx].round()))\n    f1_scores.append(f1_score(train_df.target[val_idx], oof[val_idx].round()))\n    \n    # Confusion matrix by folds\n    cms.append(confusion_matrix(train_df.target[val_idx], oof[val_idx].round()))\n    \n# Metrics\nprint(\n        '\\nCV roc score        : {0:.4f}, std: {1:.4f}.'.format(np.mean(roc_aucs), np.std(roc_aucs)),\n        '\\nCV accuracy score   : {0:.4f}, std: {1:.4f}.'.format(np.mean(accuracies), np.std(accuracies)),\n        '\\nCV recall score     : {0:.4f}, std: {1:.4f}.'.format(np.mean(recalls), np.std(recalls)),\n        '\\nCV precision score  : {0:.4f}, std: {1:.4f}.'.format(np.mean(precisions), np.std(precisions)),\n        '\\nCV f1 score         : {0:.4f}, std: {1:.4f}.'.format(np.mean(f1_scores), np.std(f1_scores))\n)\n\n# Timer end    \ntimer(start_time)\n\n# Roc curve\nf = plt.figure(1)\nplt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'grey')\nmean_tpr = np.mean(tprs, axis=0)\nmean_auc = auc(mean_fpr, mean_tpr)\nplt.plot(mean_fpr, mean_tpr, color='blue',\n         label=r'Mean ROC (AUC = %0.4f)' % ((mean_auc)),lw=2, alpha=1)\nplt.grid(b=True, which='major', color='grey', linewidth=0.4)\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate', fontsize=12)\nplt.title('LGBM - ROC by folds', fontsize=18)\nplt.legend(loc=\"lower right\")\n\n# PR plt\ng = plt.figure(2)\nplt.plot([0,1],[1,0],linestyle = '--',lw = 2,color = 'grey')\ny_real = np.concatenate(y_real)\ny_proba = np.concatenate(y_proba)\nprecision, recall, _ = precision_recall_curve(y_real, y_proba)\nplt.plot(recall, precision, color='blue',\n         label=r'Mean P|R')\nplt.grid(b=True, which='major', color='grey', linewidth=0.4)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('CB P|R curve by folds', fontsize=18)\nplt.legend(loc=\"lower left\")\n\n# Confusion matrix \nplt.rcParams[\"axes.grid\"] = False\ncm = np.average(cms, axis=0)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title='CB Confusion matrix [averaged\/folds]')\nplt.show()","c6e5bd38":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:37].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(10,10))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False),\n            edgecolor=('white'), linewidth=2, palette=\"rocket\")\nplt.title('CB - Features importance (averaged\/folds)', fontsize=18)\nplt.tight_layout()","b87f8fc5":"## <a id='3.1'>3.1. Feature engineering<\/a> ","0c454587":"## <a id='4.4'>4.4. Light GBM - Features importance <\/a> ","65fd399e":"![](http:\/\/)","ee947df3":"## <a id='1.1'>1.1. Loading libraries<\/a> ","c34e1bfc":"## <a id='2.5'>2.5. Bivariate analysis (vs target)<\/a> ","a35e7739":"# <a id='5'>5. SHAP<\/a> ","519199e9":"## <a id='2.6'>2.6. Multivariate analysis<\/a> ","d766bdd3":"## <a id='4.1'>4.1. Confusion Matrix function<\/a> ","e385609b":"----------\n**Heart Disease Analysis and Prediction**\n=====================================\n\n***LGB : CV - Auc = .948 Recall = .915 Precision = .867 Accuracy = .871***\n\n***CB  : CV - Auc = .916 Recall = .897 Precision = .843 Accuracy = .851***\n\n***Vincent Lugat***\n\n*July 2019*\n\n----------","6cc0f567":"### <a id='2.5.1.'>2.5.1. Numerical data<\/a> ","e817175e":"![](https:\/\/image.noelshack.com\/fichiers\/2019\/27\/6\/1562430213-logo-heart-care.jpg)","71710702":"## <a id='1.2'>1.2. Reading data<\/a> ","4da7cfbd":"## <a id='2.3'>2.3. Correlation matrix<\/a> ","f8aa2e4c":"## <a id='2.2'>2.2. Target count<\/a> ","63128f03":"### <a id='2.6.3.'>2.6.3. Numerical & categorical data<\/a> ","a791b570":"## <a id='4.2'>4.2. Light GBM - Hyperparameters<\/a> ","cebf632a":"# <a id='2'>2. Exploratory Data Analysis (EDA)<\/a>  ","d8e435ff":"## <a id='6.1'>6.1. Light GBM : Scale_pos_weight augmentation <\/a> ","ec5f260a":"# <a id='7'>7. CatBoost <\/a> ","f65bf769":"## <a id='7.1'>7.1. CatBoost - Hyperparameters<\/a> ","a9b86075":"- <a href='#1'>1. Libraries and Data<\/a>  \n    - <a href='#1.1'>1.1. Loading libraries<\/a> \n    - <a href='#1.2'>1.2. Reading data<\/a> \n- <a href='#2'>2. Exploratory Data Analysis (EDA)<\/a> \n    - <a href='#2.1'>2.1. Head, describe and shape<\/a> \n    - <a href='#2.2'>2.2. Target count<\/a> \n    - <a href='#2.3'>2.3. Correlation Matrix<\/a> \n    - <a href='#2.4'>2.4. Univariate analysis<\/a> \n        - <a href='#2.4.1'>2.4.1. Numerical data<\/a> \n        - <a href='#2.4.2'>2.4.2. Categorical data<\/a> \n    - <a href='#2.5'>2.5. Bivariate analysis (vs target)<\/a> \n        - <a href='#2.5.1'>2.5.1. Numerical data<\/a> \n        - <a href='#2.5.2'>2.5.2. Categorical data<\/a> \n    - <a href='#2.6'>2.6. Multivariate analysis<\/a> \n        - <a href='#2.6.1'>2.6.1. Numerical data<\/a> \n        - <a href='#2.6.2'>2.6.2. Categorical data<\/a> \n        - <a href='#2.6.3'>2.6.3. Numerical & categorical data<\/a> \n- <a href='#3'>3. Feature engineering and preprossesing<\/a>\n    - <a href='#3.1'>3.1. Feature engineering<\/a> \n    - <a href='#3.2'>3.2. Preprocessing<\/a> \n- <a href='#4'>4. Light GBM<\/a>\n    - <a href='#4.1'>4.1. Confusion matrix function <\/a> \n    - <a href='#4.2'>4.2. Light GBM - Hyperparameters <\/a> \n    - <a href='#4.3'>4.3. Light GBM - 5 folds <\/a>\n    - <a href='#4.4'>4.4. Light GBM - Features importance<\/a>\n- <a href='#5'>5. SHAP<\/a>\n- <a href='#6'>6. Recall augmentation <\/a> \n    - <a href='#6.1'>6.1. Light GBM : Scale_pos_weight augmentation <\/a> \n- <a href='#7'>7. CatBoost<\/a>\n    - <a href='#7.1'>7.1. CatBoost - Hyperparameters <\/a> \n    - <a href='#7.2'>7.2. CatBoost - 5 folds <\/a>\n    - <a href='#7.2'>7.2. CatBoost - Features importance <\/a>","92a027b2":"## <a id='7.2'>7.2. CatBoost - 5 folds<\/a> ","42243d44":"## <a id='2.1'>2.1. Head, describe and shape<\/a> ","27df04f8":"# <a id='1'>1. Librairies and data<\/a> ","ba7cb3a0":"# <a id='6'>6. Recall augmentation <\/a> ","64e60812":"## <a id='4.3'>4.3. Light GBM - 5 folds <\/a> ","3c58b515":"# <a id='3'>3. Feature engineering and Preprocessing<\/a>  ","fa91569b":"## <a id='3.1'>3.1. Preprocessing<\/a> ","44f880dc":"### <a id='2.4.2.'>2.4.2. Categorical data<\/a> ","ade3f70e":"### <a id='2.5.2.'>2.5.2. Categorical data<\/a> ","92728c56":"1. age : age in years\n2. sex : (1 = male; 0 = female)\n3. cp : chest pain type (4 values) \n4. trestbps : resting blood pressure (in mm Hg on admission to the hospital)\n5. chol : serum cholestoral in mg\/dl \n6. fbs : fasting blood sugar > 120 mg\/dl : (1 = true; 0 = false)\n7. restecg : resting electrocardiographic results (values 0,1,2)\n8. thalach : maximum heart rate achieved \n9. exang : exercise induced angina \n10. oldpeak : ST depression induced by exercise relative to rest \n11. the slope of the peak exercise ST segment \n12. ca : number of major vessels (0-3) colored by flourosopy \n13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect","72ba992c":"## <a id='7.3'>7.3. CatBoost - Features importance <\/a> ","a14db706":"# <a id='4'>4. Light GBM<\/a>  ","1ef32414":"### <a id='2.4.1.'>2.4.1. Numerical data<\/a> ","cd437398":"### <a id='2.6.2.'>2.6.2. Categorical data<\/a> ","40aa2269":"### <a id='2.6.1.'>2.6.1. Numerical data<\/a> ","4afa0204":"## <a id='2.4'>2.4. Univariate analysis<\/a> "}}