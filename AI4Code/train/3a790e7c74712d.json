{"cell_type":{"16d5cc32":"code","2c9cc150":"code","e8c53ff5":"code","53a38ccc":"code","efd6bfab":"code","0d8b7be1":"code","e33c6c9b":"code","f0b21ac4":"code","ec3f1630":"code","82662c10":"code","3d690826":"code","97b2e609":"code","1d3cfde3":"code","bc1208ae":"code","801d0df0":"code","7591b1ee":"code","95685be9":"code","8c69d1ea":"code","6c0b42cc":"code","4c11a01d":"code","0a690480":"code","6c469720":"code","1fab5598":"code","49cd1896":"code","7bccaa01":"code","41955469":"code","131fcb53":"code","8c61171f":"code","675df892":"code","e1af6e48":"code","de6d7236":"code","340affb8":"code","86cd5373":"code","dce741b1":"code","8b96b540":"code","664c3984":"markdown","0e314b22":"markdown","c4e31631":"markdown","f4d58429":"markdown","df7d3f18":"markdown","c30ff96c":"markdown","eb938429":"markdown","5f8bc970":"markdown","053556e8":"markdown","0cd6512c":"markdown","eb8435ea":"markdown","a4e6c648":"markdown","365a55c0":"markdown"},"source":{"16d5cc32":"# load data libraries\nimport numpy as np # linear algebra library\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport zipfile # to read zip files\nfrom sklearn.model_selection import train_test_split\n\n\n# data understanding libraries\nimport matplotlib.pyplot as plt # ploting library\n%matplotlib inline\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom collections import Counter\nimport seaborn as sns\n\n\n\n# data preparation\nimport re\nfrom nltk.stem import PorterStemmer\n\n\n# ADS Creation\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.preprocessing import StandardScaler\n# Modeling\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Evaluation and Model Selection\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import GridSearchCV","2c9cc150":"pd.set_option('display.max_rows', 10000)\npd.set_option('display.max_columns', 500)\npd.set_option('display.precision',150)\npd.options.display.float_format = '{:,.3f}'.format","e8c53ff5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","53a38ccc":"#unzip the files\narchive_train = zipfile.ZipFile('..\/input\/sentiment-analysis-on-movie-reviews\/train.tsv.zip')\n\n#read training tsv file \ntrain = pd.read_csv('..\/input\/sentiment-analysis-on-movie-reviews\/train.tsv.zip', delimiter='\\t')\ntrain.head(20)","efd6bfab":"#split data into train and test data\n#then split the test data into test and validation \ntrain_data, test_data = train_test_split(train, test_size=0.4, random_state=1)\nval_data, test_data = train_test_split(test_data, test_size=0.5, random_state=1)\n\n#resets index after splitting data\ntrain_data = train_data.reset_index(drop=True)\nval_data = val_data.reset_index(drop=True)\ntest_data = test_data.reset_index(drop=True)\n\nprint(\"Train set size is \",len(train_data))\nprint(\"Val set size is \",len(val_data))\nprint(\"Test set size is \",len(test_data))","0d8b7be1":"stopwords = set(['Laura', 'Pauline', 'BMW', 'Carol', 'Kane', 'Evelyn', 'Cahill',\n 'Adam',\n 'Garcia',\n 'X-Men',\n 'Glover',\n 'Disney',\n 'Antwone',\n 'Fisher',\n 'Gayton',\n 'Lyne',\n 'African',\n 'American',\n 'Godard',\n 'Dickens',\n 'Woody',\n 'Martin',\n 'Scorsese',\n 'New',\n 'York',\n 'Decasia',\n 'Limit',\n 'Hollywood',\n 'Michael',\n 'Moore',\n 'Robin',\n 'Williams',\n 'Rorschach',\n 'Watts',\n 'Broomfield',\n 'Washington',\n 'Argentine',\n 'Fabian',\n 'Bielinsky',\n 'Tunis',\n 'Bebe',\n 'Neuwirth',\n 'John',\n 'C.',\n 'Walsh',\n 'Ms.',\n 'Fontaine',\n 'Ritchie',\n 'Copmovieland',\n 'Egoyan',\n 'France',\n 'Carlen',\n 'Campanella',\n 'Norma',\n 'Rae',\n 'Bartlett',\n 'Chicago',\n 'Gere',\n 'Peter',\n 'Mattei',\n 'Famuyiwa',\n 'Tara',\n 'Reid',\n 'Hawaiian',\n 'Sendak',\n 'David',\n 'Cronenberg',\n 'Anne',\n 'Busby',\n 'Malkovich',\n 'Statham',\n 'Freudian',\n 'Cattaneo',\n 'Tim',\n 'Apted',\n 'Nicholas',\n 'Kazan',\n 'Kilmer',\n 'One',\n 'Jaglom',\n 'Twohy',\n 'Chinese',\n 'Jerry',\n 'Bruckheimer',\n 'Joel',\n 'Schumacher',\n 'Chris',\n 'Smith',\n 'Hawk',\n 'Katherine',\n 'Blake',\n 'Wilde',\n 'Clint',\n 'Eastwood',\n 'Miramax',\n 'Reno',\n 'Louiso',\n 'Ivory',\n 'Vietnam',\n 'Vietnamese',\n 'Shakespeare',\n 'America',\n 'Del',\n 'Toro',\n 'Hip-Hop',\n 'Scooby-Doo',\n 'Aan',\n 'Steve',\n 'Irwin',\n 'Taylor',\n 'Le',\n 'Jean-Luc',\n 'Pamela',\n 'Taymor',\n 'George',\n 'W.',\n 'Bush',\n 'Henry',\n 'Kissinger',\n 'Larry',\n 'King',\n 'Holly',\n 'Marina',\n 'Stanley',\n 'Kwan',\n 'Joseph',\n 'Heller',\n 'Kurt', \n 'Vonnegut',\n 'Titus',\n 'Minnie',\n 'Allen',\n 'Britney',\n 'Spears',\n 'Farrelly',\n 'Tsai',\n 'Rob',\n 'Schneider',\n 'Greenfingers',\n 'Harmon',\n 'Zhuangzhuang',\n 'Eric',\n 'Rohmer',\n 'Blanchett',\n 'Ribisi',\n 'Jacquot',\n 'Zaidan',\n 'Wiseman',\n 'Ian',\n 'Holm',\n 'Vincent',\n 'R.',\n 'Nebrida',\n 'Laurice',\n 'Guillen',\n 'De',\n 'Niro',\n 'Martha',\n 'Margaret',\n 'Thatcher',\n 'Assayas',\n 'Sara',\n 'Abdul',\n 'Amy',\n 'Neil',\n 'Dana',\n 'Sinise',\n 'Hartley',\n 'Bruce',\n 'Willis',\n 'Dean',\n 'Zelda',\n 'Tony',\n 'Jackal',\n 'Nelson',\n 'Harlem',\n 'Mick',\n 'Graham',\n 'Myer',\n 'Koepp',\n 'Wimmer',\n 'Robert',\n 'Travis',\n 'Bickle',\n 'Cockettes',\n 'Jez',\n 'Steven',\n 'Shainberg',\n 'Mary',\n 'Gaitskill',\n 'Reyes',\n 'Roberto',\n 'Benigni',\n 'Kaufman',\n 'Jonze',\n 'Ralph',\n 'Waltz',\n 'Roger',\n 'Kumble',\n 'Damon\\\\\/Bourne',\n 'Charlie',\n 'Donald',\n 'Madonna'])\nporter = PorterStemmer()\n\ndef ret_words(Phrase):\n    Phrase = Phrase.lower()\n    Phrase = Phrase.replace('-', '')\n    Phrase = Phrase.replace(',', ' ')\n    Phrase = Phrase.replace(';', ' ')\n    Phrase = Phrase.replace('\\'', '')\n    Phrase = Phrase.replace('\\\\', '')\n    Phrase = Phrase.replace('\/', '')\n    Phrase = Phrase.replace('.', '')\n    Phrase = Phrase.replace('...', '')\n    words = []\n    for word in Phrase.split():\n        if re.findall('[0-9]', word): continue\n        if word in stopwords: continue\n        if re.findall('[^a-zA-Z]',re.sub(r'[^\\w\\s]','',word)): continue\n        if len(word) > 0: words.append(porter.stem(re.sub(r'[^\\w\\s]','',word)))\n    return ' '.join(words)\n\ndef preprocess(df,flag):\n    # add column\n    df[\"words_num\"]=df['Phrase'].str.split().str.len()\n     # Remove recipes with only one Ingredient\n    if flag == 0 :\n        df = df.drop(df[df[\"words_num\"]<=1].index)\n    \n    # Convert list of ingredients to string\n    df['Phrase'] = df[\"Phrase\"].apply(ret_words)\n    \n    return df","e33c6c9b":"train_preprocessed = preprocess(train_data,0)\ntest_preprocessed = preprocess(test_data,1)\nval_preprocessed = preprocess(val_data,1)","f0b21ac4":"train_preprocessed.head(10)","ec3f1630":"id_train, X_train, y_train = train_preprocessed['PhraseId'], train_preprocessed['Phrase'], train_preprocessed['Sentiment']\nid_test, X_test, y_test = test_preprocessed['PhraseId'], test_preprocessed['Phrase'], test_preprocessed['Sentiment']\nid_val, X_val, y_val = val_preprocessed['PhraseId'], val_preprocessed['Phrase'], val_preprocessed['Sentiment']","82662c10":"LR_clf_counts = Pipeline([\n    ('vect', CountVectorizer()),\n    ('clf', LogisticRegression(random_state=0, max_iter=2000))\n])\nLR_clf_counts.fit(X_train, y_train)\nLR_cnt_pred_tr = LR_clf_counts.predict(X_train)\n\nprint(accuracy_score(y_train, LR_cnt_pred_tr))\nprint(precision_score(y_train, LR_cnt_pred_tr, average='weighted'))","3d690826":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(LR_clf_counts, \n                                                        X_train, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=3,\n                                                        # Evaluation metric\n                                                        scoring='precision_weighted',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nfig, ax = plt.subplots(figsize=(15,10))\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Weighted Precision Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()\n","97b2e609":"SVM_clf_counts = Pipeline([\n    ('vect', CountVectorizer()),\n    ('clf', LinearSVC(max_iter=3000))\n])\nSVM_clf_counts.fit(X_train, y_train)\nSVM_cnt_pred_tr = SVM_clf_counts.predict(X_train)\n\nprint(accuracy_score(y_train, SVM_cnt_pred_tr))\nprint(precision_score(y_train, SVM_cnt_pred_tr, average='weighted'))","1d3cfde3":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(SVM_clf_counts, \n                                                        X_train, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=3,\n                                                        # Evaluation metric\n                                                        scoring='precision_weighted',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nfig, ax = plt.subplots(figsize=(15,10))\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Weighted Precision Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","bc1208ae":"NB_clf_counts = Pipeline([\n    ('vect', CountVectorizer()),\n    ('clf', MultinomialNB())\n])\nNB_clf_counts.fit(X_train, y_train)\nNB_cnt_pred_tr = NB_clf_counts.predict(X_train)\n\nprint(accuracy_score(y_train, NB_cnt_pred_tr))\nprint(precision_score(y_train, NB_cnt_pred_tr, average='weighted'))","801d0df0":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(NB_clf_counts, \n                                                        X_train, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=3,\n                                                        # Evaluation metric\n                                                        scoring='precision_weighted',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nfig, ax = plt.subplots(figsize=(15,10))\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Weighted Precision Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","7591b1ee":"LR_clf_tfidf = Pipeline([\n    ('tfidf', TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.25, norm='l2', encoding='latin-1',ngram_range=(1, 2), stop_words='english')),\n    ('clf', LogisticRegression(random_state=0, max_iter=2000))\n])\nLR_clf_tfidf.fit(X_train, y_train)\nLR_tfidf_pred_tr = LR_clf_tfidf.predict(X_train)\n\nprint(accuracy_score(y_train, LR_tfidf_pred_tr))\nprint(precision_score(y_train, LR_tfidf_pred_tr, average='weighted'))","95685be9":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(LR_clf_tfidf, \n                                                        X_train, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=3,\n                                                        # Evaluation metric\n                                                        scoring='precision_weighted',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nfig, ax = plt.subplots(figsize=(15,10))\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Weighted Precision Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","8c69d1ea":"SVM_clf_tfidf = Pipeline([\n    ('tfidf', TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.25, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')),\n    ('clf', LinearSVC( max_iter=2000))\n])\nSVM_clf_tfidf.fit(X_train, y_train)\nSVM_tfidf_pred_tr = SVM_clf_tfidf.predict(X_train)\n\nprint(accuracy_score(y_train, SVM_tfidf_pred_tr))\nprint(precision_score(y_train, SVM_tfidf_pred_tr, average='weighted'))","6c0b42cc":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(SVM_clf_tfidf, \n                                                        X_train, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=3,\n                                                        # Evaluation metric\n                                                        scoring='precision_weighted',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nfig, ax = plt.subplots(figsize=(15,10))\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Weighted Precision Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","4c11a01d":"NB_clf_tfidf = Pipeline([\n    ('tfidf', TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.25, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')),\n    ('clf', MultinomialNB())\n])\nNB_clf_tfidf.fit(X_train, y_train)\nNB_tfidf_pred_tr = NB_clf_tfidf.predict(X_train)\n\nprint(accuracy_score(y_train, NB_tfidf_pred_tr))\nprint(precision_score(y_train, NB_tfidf_pred_tr, average='weighted'))","0a690480":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(NB_clf_tfidf, \n                                                        X_train, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=3,\n                                                        # Evaluation metric\n                                                        scoring='precision_weighted',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nfig, ax = plt.subplots(figsize=(15,10))\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Weighted Precision Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","6c469720":"vect=  CountVectorizer()\nX_train_cnt = vect.fit_transform(X_train)","1fab5598":"def LR_param_selection(X, y, nfolds):\n    Cs = [0.01, 0.1, 1, 10]\n    param_grid = {'C': Cs}\n    grid_search = GridSearchCV(LogisticRegression(random_state=0,max_iter=2000), param_grid, cv=nfolds)\n    grid_search.fit(X, y)\n    grid_search.best_params_\n    return grid_search.best_params_","49cd1896":"LR_param_selection( X_train_cnt,y_train,2)","7bccaa01":"LR_clf_counts = Pipeline([('vect', CountVectorizer()),\n                   ('clf', LogisticRegression(C=1,random_state=0, max_iter=2000)),\n                  ])\nLR_clf_counts.fit(X_train, y_train)\nLR_cnt_pred_tr = LR_clf_counts.predict(X_train)\nLR_cnt_pred_val = LR_clf_counts.predict(X_val)\n\nprint(\"accuracy on training: \",accuracy_score(y_train, LR_cnt_pred_tr))\nprint(\"precision on training: \",precision_score(y_train, LR_cnt_pred_tr, average='micro'))\nprint(\"precision on validation: \",precision_score(y_val, LR_cnt_pred_val, average='micro'))","41955469":"LR_clf_counts = Pipeline([('vect', CountVectorizer()),\n                   ('clf', LogisticRegression(C=1,random_state=0, max_iter=2000)),\n                  ])\nLR_clf_counts.fit(X_train, y_train)\nLR_cnt_pred_tr = LR_clf_counts.predict(X_train)\nLR_cnt_pred_val = LR_clf_counts.predict(X_val)\nLR_cnt_pred_tst = LR_clf_counts.predict(X_test)\n\n\nprint(\"precision on training: \",precision_score(y_train, LR_cnt_pred_tr, average='micro'))\nprint(\"precision on validation: \",precision_score(y_val, LR_cnt_pred_val, average='micro'))\nprint(\"precision on testing: \",precision_score(y_test, LR_cnt_pred_tst, average='micro'))","131fcb53":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(LR_clf_counts, \n                                                        X_train, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=3,\n                                                        # Evaluation metric\n                                                        scoring='precision_weighted',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nfig, ax = plt.subplots(figsize=(15,10))\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Weighted Precision Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","8c61171f":"labels = train['Sentiment'].unique()","675df892":"conf_mat = pd.DataFrame(confusion_matrix(y_val, LR_cnt_pred_val, labels=labels))\nconf_mat_n = conf_mat.divide(conf_mat.sum(axis=1), axis=0)\nfig, ax = plt.subplots(figsize=(20,12))\nsns.heatmap(conf_mat_n, annot=True,xticklabels=labels,yticklabels=labels)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","e1af6e48":"Predicted_vals = pd.DataFrame({'id' : id_val, 'Sentiment': X_val, 'Actual': y_val, 'Preds': LR_cnt_pred_val})","de6d7236":"#read\narchive_train = zipfile.ZipFile('..\/input\/sentiment-analysis-on-movie-reviews\/train.tsv.zip')\narchive_test = zipfile.ZipFile('..\/input\/sentiment-analysis-on-movie-reviews\/test.tsv.zip')\n\n\n#read training tsv file \nfinal_train = pd.read_csv('..\/input\/sentiment-analysis-on-movie-reviews\/train.tsv.zip', delimiter='\\t')\nfinal_test = pd.read_csv('..\/input\/sentiment-analysis-on-movie-reviews\/test.tsv.zip', delimiter='\\t')","340affb8":"#prepare\nftrain_preprocessed = preprocess(final_train,0)\nftest_preprocessed = preprocess(final_test,1)","86cd5373":"id_train, X_train, y_train = ftrain_preprocessed['PhraseId'], ftrain_preprocessed['Phrase'], ftrain_preprocessed['Sentiment']\nid_test, X_test, = ftest_preprocessed['PhraseId'], ftest_preprocessed['Phrase']","dce741b1":"#LR\nLR_clf = Pipeline([('vect', CountVectorizer()),\n                   ('clf', LogisticRegression(C=1,random_state=0, max_iter=2000)),\n                  ])\nLR_clf.fit(X_train, y_train)\npred_test = LR_clf_counts.predict(X_test)","8b96b540":"output=pd.DataFrame({'PhraseId':id_test, 'Sentiment':pred_test})\noutput.to_csv('Sentiment_preds_LR.csv',index= False)","664c3984":"<h3>3. Data Modeling<\/h3>","0e314b22":"<h2>Model Run:<\/h2>","c4e31631":"<h4>3.2 BOW<\/h4>","f4d58429":"<h2>2. Load Data <\/h2>","df7d3f18":"<h3>Logistic Regression<\/h3>","c30ff96c":"<h4>There are a total of 4 columns<\/h4>\n\n<h5>&emsp;PhraseId<\/h5>\n<h5>&emsp;SentenceId<\/h5>\n<h5>&emsp;Phrase<\/h5>\n\n<h5>&emsp;Sentiment:<\/h5> \n<h6>&emsp;&emsp;&emsp;&emsp;0 - negative<br>\n&emsp;&emsp;&emsp;&emsp;1 - somewhat negative<br>\n&emsp;&emsp;&emsp;&emsp;2 - neutral<br>\n&emsp;&emsp;&emsp;&emsp;3 - somewhat positive<br>\n&emsp;&emsp;&emsp;&emsp;4 - positive<\/h6>","eb938429":"<h4>Hyperparameter tuning<\/h4>","5f8bc970":"<h3>2. Data Preparation<\/h3>","053556e8":"<h4>3.1 BOW<\/h4>","0cd6512c":"<h3>Final Model<\/h3>","eb8435ea":"<h1>Movie Reviews Analysis<\/h1>\n\n<h3> Important Q&A<\/h3>\n\n<h4>1. What is the business question?<\/h4>\n<h5>What is the movie review phrases on a scale of five values: negative, somewhat negative, neutral, somewhat positive, positive?<\/h5>\n\n<h4>2. What does each row represent?<\/h4>\n<h5>Each row repersents a phrase<\/h5>\n\n<h4>3. What is the evaluation method?<\/h4>\n<h5>What is the movie review phrases on a scale of five values: negative, somewhat negative, neutral, somewhat positive, positive?<\/h5>\n","a4e6c648":"<h2>1. Imports<\/h2>","365a55c0":"<h4>Logistic Regression<\/h4>"}}