{"cell_type":{"c66dd960":"code","55bff8e0":"code","1b7def7d":"code","ac056a58":"code","87c115f3":"code","e5b52aca":"code","5816171f":"code","0b4f441e":"code","754fc1e7":"code","b8e7f966":"code","3e8d4358":"code","ad924c0f":"code","146a196d":"code","86e270a6":"code","7256bdf9":"code","1e6d2bc3":"code","5b3cad4f":"code","27c4914c":"code","18cb910a":"code","f44865c8":"code","da5e6d2c":"code","e46f356a":"code","0fa7b09d":"code","e929d752":"code","a6a87509":"code","87c149f9":"code","801596fd":"code","6b68d3e0":"code","bd1f973b":"code","b2ee7d37":"code","95d2bf27":"code","d4c96e0d":"code","551e9863":"code","90533cb7":"code","2f4ca434":"code","c7924146":"code","5cc3e34f":"code","9f25c35b":"code","a0762896":"code","25b34c6f":"code","f84fb768":"code","c68f3d86":"code","f3e0ac0d":"code","824e8772":"code","48b30012":"code","ed920d40":"code","81dc0b52":"code","dcfcb3ba":"code","2f6612b5":"code","5a287e94":"code","ef5ac77e":"code","acd2476c":"code","34585766":"code","f675f6af":"code","734992d9":"code","0e9f7e51":"code","bb468492":"code","99b9aeac":"code","2e9f438d":"code","f2b987f7":"code","807d6e55":"code","b1af6a09":"code","01ab4cdb":"code","f4d63703":"code","d875580c":"code","31deb99c":"code","3f0577f9":"code","d6747181":"code","d7e2599f":"code","bcb8d002":"code","de8f5e74":"code","8bba9c6f":"code","a9cd241f":"code","a540f604":"code","81b56658":"code","d9be51b1":"code","297c35a5":"code","fc486a2e":"code","a4b26951":"code","e7305e47":"code","fa938dd6":"code","cccf8162":"code","8418acff":"code","d5e84ce2":"code","7ebef89b":"code","d4d2b4e9":"code","1fdfd7b8":"code","e2a8f5e9":"code","d3c685c2":"markdown","8e91b7c3":"markdown","1e93ad22":"markdown","88402c6f":"markdown","c326e1f4":"markdown","b4762ff1":"markdown","ef7a3d48":"markdown","3dfe34f9":"markdown","df3b0182":"markdown","f6a9f9a4":"markdown","57610cc2":"markdown","9e18e5f6":"markdown","17380f98":"markdown","2c771962":"markdown","fb4e7cf0":"markdown","d2e8d86c":"markdown","563a994e":"markdown","f388426c":"markdown","02a65f6e":"markdown","f989a20c":"markdown","d632369d":"markdown","e45ddb75":"markdown"},"source":{"c66dd960":"import numpy as np","55bff8e0":"# So easy to create a numpy array:\nmyarray = np.array([0, 1, 2, 3])\nprint(myarray)","1b7def7d":"# Wanna sum the values, 0 + 1 + 2 + 3 = 6? \n# go:\nprint(myarray.sum())","ac056a58":"# Want to find the maximum or the minimum, nothing has changed ..\nprint(f'Max: {myarray.max()}, Min: {myarray.min()}')","87c115f3":"# I know, you want to know the average values\n# Very difficult..\nprint (myarray.mean ())","e5b52aca":"# Let's create an array like the previous one, [0, 1, 2, 3]\nmyarray2 = np.arange (4)\nprint (myarray2)","5816171f":"# No,\n# I want to create an array, with 4 positions but only with the value 1\nprint (np.ones (4))","0b4f441e":"# Or with a value of 0\nprint (np.zeros (4))","754fc1e7":"# Or with random (random) values\nprint (np.random.random (4))","b8e7f966":"# but Italo, the demand requires random numbers, however, integers\nprint (np.random.randint (4))","3e8d4358":"# ooops .. it only returned a random number from 0 to 3 ..\n# but our demand requires it to be an array with 4 positions and\n# with integer numbers. Complicated? I don't think so\u2026\n# With numpy, we will use the randint function (start, end, qty)\nprint (np.random.randint (0, 10, 4))","ad924c0f":"# And just to finish these cool commands, remember the arange\nprint (np.arange (4))","146a196d":"# So, another widely used numpy command is linspace.\nprint (np.linspace (0, 3, 4))","86e270a6":"# It is very similar to the linspace function, it returns spaced values\n# at a break, but evenly. So why not use the\n# arange? Well, linspace is used a lot when we are plotting\n# data, both when viewing the values \u200b\u200bor when we are assembling\n# the axes of this plot\nprint (np.linspace (0, 1, 4))","7256bdf9":"# let's import the pandas library into python\nimport pandas as pd","1e6d2bc3":"# and create the Week Series, I make it simple like this:\nweek = pd.Series (['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday'])\nprint (week)","5b3cad4f":"# or I could create like this\ndata = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\nweek = pd.Series (data)\nprint (week)","27c4914c":"# some ways to print df data or attributes\n# if you want to print only the values\nprint (week.values)\n\n# if you want to print only the indexes\nprint (week.index)\n\n# if you want\u2026 aff, it's already repetitive\u2026 but if you want to print Monday,\nprint (week.iloc [1])\n\n# and remember, the Series is an array, so I could print like that too\nprint (week [1])","18cb910a":"# Now we will create a DataFrame\ndf = pd.DataFrame ()\ndf ['column 1'] = [1, 2, 3, 4]\ndf ['column 2'] = ['a', 'b', 'c', 'd']\n\n# let's print our dataframe\nprint(df)","f44865c8":"# see, we have indexes ranging from 0 to 3, but we can also name the indexes. Like? So for example:\ndf.index = ['line0', 'line1', 'line2', 'line3']\nprint(df)","da5e6d2c":"# a very common way would be to create this dataframe using a 'dict' dictionary\ndata = {\n'column 1': [1, 2, 3, 4],\n'column 2': ['a', 'b', 'c', 'd']\n}\nindex = ['row0', 'row1', 'row2', 'row3']\ndf = pd.DataFrame (data = data, index = index)\n\nprint(df)","e46f356a":"# importing data from a csv file\ndfCSV = pd.read_csv('\/kaggle\/input\/minidatasetsexemplos\/meuArquivo.csv')\ndfCSV","0fa7b09d":"# importing data from a json file\ndfJSON = pd.read_json('\/kaggle\/input\/minidatasetsexemplos\/meuArquivo.json')\ndfJSON","e929d752":"# receiving data from an excel file\ndfXLSX = pd.read_excel('\/kaggle\/input\/minidatasetsexemplos\/meuArquivo.xlsx')\ndfXLSX","a6a87509":"# Let's go back to our simple df\nprint(df, '\\n\\n')\n# and explore our data a little more\n# if I want to print column 1,\ndf['column 1']","87c149f9":"# if I want to print column 2,\nprint(df['column 2'])","801596fd":"# if i want row 1\nprint (df.loc['row1'])","6b68d3e0":"# but you guys remember that before renaming our indexes,\n# Did they have your number string?\n# So, in this case, just use iloc\nprint(df.iloc[1])","bd1f973b":"# e to print only the value of [column 1] x [row1] .. then\nprint (df['column 1']['row1'])","b2ee7d37":"# let's import my dataset meuArquivo.csv\ndf = pd.read_csv('\/kaggle\/input\/minidatasetsexemplos\/meuArquivo.csv')","95d2bf27":"# or I could make a copy of the dfCSV that we already imported\ndf = dfCSV.copy()","d4c96e0d":"# let's view the first 5 records of our dataset\ndf.head()","551e9863":"# or the last 5 records\ndf.tail()","90533cb7":"# Italo, can you increase it to 10 records? Yes, just add the number you want to view, on the head or tail\ndf.head(10)","2f4ca434":"# And if you want to print or randomly pick some lines\n# of your dataset, the sample function will help you\n# and make it a lot easier ...\n# So if I want 8 random lines from the df dataset, that's enough.\n\ndf.sample(8)\n# run more than once so you can see that the values really change","c7924146":"# we also have the info function that gives us basic information about the structure of the dataset and its data\ndf.info()","5cc3e34f":"# another widely used function is the shape, which returns only the number of columns and rows in the dataset\nprint(df.shape)","9f25c35b":"# Continuing the analysis, a widely used function is the isnull function\ndf.isnull()","a0762896":"df.isnull().sum()","25b34c6f":"# What if there are null values? Well, here comes the analysis.\n# just to do a test, let's create a copy of the df\ntest = df.copy ()\n\n# let's show the shape\ntest.shape","f84fb768":"# now, I will use a feature to add data to the dataframe\n# and in the test dataset, I'm going to add all the data from the df dataset again,\n# ie I will duplicate values\ntest = test.append(df)\n\ntest.shape","c68f3d86":"# now yes, another rich function: remove duplicate data\ntest = test.drop_duplicates ()\n\ntest.shape","f3e0ac0d":"# so we can do the same thing for null data\n# let's review this information\ntest.isnull().sum ()","824e8772":"# see, there is no column with a null value ...\n# so let's generate a value, just for analysis\ntest.head()","48b30012":"# we will display the first five products on the test dataframe\ntest['PRODUTO'].head()","ed920d40":"# I will change the value of the first product to null (None)\ntest['PRODUTO'][0] = None\ntest['PRODUTO'].head ()","81dc0b52":"# ulalaaa, our first product is worthless, so let's analyze the info from our test dataset\ntest.isnull().sum()","dcfcb3ba":"# and now we see that the PRODUCT variable (or column) has a null value\n# let's print the shape of this dataset\nprint(test.shape)","2f6612b5":"test.dropna()\nprint(test.shape) ","5a287e94":"test = test.dropna()","ef5ac77e":"# let's see the summary of null records\nprint(test.isnull().sum ())\n# and see the shape\nprint(test.shape)","acd2476c":"# another tip, imagining that I want to have a variable, all the products that were sold\n\nproducts = test['PRODUTO']\nprint(products)","34585766":"products = test['PRODUTO'].unique()\nprint(products)","f675f6af":"# just for curiosity: what is the type of the products variable?\n#?\n#?\ntype(products)\n# numpy array\u2026. what a wonderful World..","734992d9":"# of entire dataset\ntest.describe()","0e9f7e51":"# or one only numeric feature\nprint(test['QTDE'].describe())","bb468492":"# or categorical\ntest['LOJA'].describe()","99b9aeac":"# summing up values\nprint(test['QTDE'].sum())","2e9f438d":"# count the number of records\nprint(test['QTDE'].count())","f2b987f7":"# return the minimum or maximum value\nprint(test['QTDE'].min())\nprint(test['QTDE'].max())","807d6e55":"# want the average\nprint(test.mean())","b1af6a09":"# or the median\nprint(test.median())","01ab4cdb":"# want to know which are the 10 best selling products,\n# we will use the value_counts function combined with the head function.\n# That's it buddy ..\nprint(test['PRODUTO'].value_counts().head(10))","f4d63703":"# if I want to make a slice, that is, a slicing, in the dataset\n# I simply assign the desired columns (variables)\n\nsales_shop = test [['LOJA', 'VENDEDOR']]","d875580c":"# let's see the result of this\nprint (sales_shop)","31deb99c":"# but I want only the stores called GFFF\ncondition = (sales_shop['LOJA'] == 'GFFF')\nprint(condition)\n# That is, it returned the variable (column) SHOP, and where the value is equal to GFFF,\n# it returns True, if not false","3f0577f9":"# To display only GFFF stores, we will use this technique\nprint (sales_shop[condition])","d6747181":"# we created the second condition\ncondition2 = (sales_shop ['VENDEDOR'] == 'DEEDE')","d7e2599f":"# and now we will display the data with both conditions\n# I want the condition AND condition2\nprint(sales_shop [condition & condition2])","bcb8d002":"# I just want to print a dataframe with all \u00b4A\u00b4 products\ntest.query ('PRODUTO == \"A\"')","de8f5e74":"# But only products sold in the GGFF store\ntest.query ('PRODUTO == \"A\" and LOJA == \"GGFG\"')","8bba9c6f":"# example, print all sales for product category BB\ntest.query ('`CATEGORIA PRODUTO` == \"BB\"')","a9cd241f":"# For example, I want to know by product category,\n# how many products and how much in value\n# If I only put the 3 variables involved in this proposal\n# see the result:\ntest[['CATEGORIA PRODUTO', 'QTDE', 'VALOR']].sum()","a540f604":"# This is where groupby fits like a glove\n# first, I grouped by the variable to be grouped, PRODUCT CATEGORY\n# second, I put my target variables, that is, that I will have the result\n# third, the operation\ntest.groupby('CATEGORIA PRODUTO')[['QTDE', 'VALOR']].sum()","81b56658":"# but from this list, I just want the top 5 and the 3 worst\n# how Pandas is the buddy\nprint ('Top 5')\ntest.groupby('CATEGORIA PRODUTO')[['QTDE', 'VALOR']].sum().head()","d9be51b1":"print('3 Piores')\ntest.groupby('CATEGORIA PRODUTO')[['QTDE', 'VALOR']].sum().tail(3)","297c35a5":"test.corr()","fc486a2e":"# First,\n# let's remember our variables in the test dataset\ntest.columns","a4b26951":"# or\ntest.info()","e7305e47":"# Objective: Display the total sales value month by month\n# how I want the total month to month, so I will have to group\ntotalSales = test.groupby('MES') ['TOTAL'].sum ()\nprint (totalSales)","fa938dd6":"# NOTICE, it is not importing a variable to receive the value\n# we could plot directly, as in the line commented below\n# test.groupby('MES')['TOTAL'].sum()\n# do the test..\n\n# But I will display the graph through the variable totalSales.\ntotalSales.plot()","cccf8162":"# but what if we want horizontal bars\ntotalSales.plot(kind = 'barh', title = 'Bar Graph')","8418acff":"# or vertical bars only\ntotalSales.plot(kind = 'bar', title = 'Bar Chart')","d5e84ce2":"# or in pie\ntotalSales.plot (kind = 'pie', title = 'Pie Chart')","7ebef89b":"# or boxplot\ntotalSales.plot (kind = 'box', title = 'Boxplot Chart')","d4d2b4e9":"# and lastly, let's make a chart that shows month to month\n# how TOTAL sales behaved, and with possible sales without discount\ntotalXbruto = test.groupby('MES')[['QTDE','VALOR','DESCONTO','TOTAL']].sum ()\nprint (totalXbruto)","1fdfd7b8":"# to understand, the total variable, brings the result\n# qty * amount - discount applied\n# QTDE * VALOR - DESCONTO variables\n# then to have the gross, we could use only the QTDE * VALOR\n# or add the total discount\n# let's go for the easiest, I will create a column in our dataset and do the calculation\ntotalXbruto['GROSS VALUE'] = (totalXbruto['QTDE'] * totalXbruto['VALOR'])\nprint(totalXbruto)","e2a8f5e9":"# done, our data is ready, now we just have to show\ntotalXbruto.plot (kind = 'line', title = 'Line Graph', y = ['TOTAL', 'GROSS VALUE'])","d3c685c2":"Every scientist or data analyst has the main task of analyzing their databases and this, in the language of the datascience ghetto, is known as Exploratory Data Analysis, because only then will we be able to extract information that has supported us to better understand what this data is telling us and, up ahead, being able to create some hypotheses and \/ or predictions, based on what the information is offering us.\n\nAs the goal here is to share basic knowledge, so don't worry if you've never heard of Pandas\u2026 this is our job. We will continue in a very simple and practical language so that you understand what is happening.","8e91b7c3":"But don't forget, Pandas is the guy, so if you want statistical data, he will have it for you:","1e93ad22":"what if our data set to be analyzed is external? The pandas offers a lot of resources for this. let's go to the most common ones ..\n\nIn my datasets, I left 3 examples of mini datasets in CSV, JSON and XLSX formats, which contain the same data. Follow the link https:\/\/www.kaggle.com\/italomarcelo\/minidatasetsexemplos\n\nTo import, Pandas offers several functions. Here are some","88402c6f":"If you want to add new conditions, we will need to use the & (for and) and | for or\n\nWe already have a condition, but in addition to that, I also want to add another condition: VENDEDOR has to be DEEDE","c326e1f4":"See, how much easier it is when filtering information, it acts as a SQL select.\n\nJust to eliminate errors or doubts, in the case of variables with compound names, such as the variable PRODUCT CATEGORY, for example, we have to mark the with the character \"`\"","b4762ff1":"Well, we've already seen how to perform various operations on our Numpy array. But on a daily basis, we need to create arrays to simulate data, in short, various situations, and as I said, numpy is a fantastic library for that\u2026 so, let's go to play\u2026","ef7a3d48":"Another very useful feature is groupby. It is widely used when we need to return values or apply mathematical operation on a given variable","3dfe34f9":"Another feature widely used by data analysts at Pandas is the correlation matrix. This function returns the correlation between the numeric variables in your dataframe or only in a subset.","df3b0182":"But back to our exploratory analysis, another widely used function is describe, which returns a statistical summary with the number of records (count), the mean (mean), the minimum, the maximum, the standard deviation (std) and the quartiles (25%, 50% and 75%)","f6a9f9a4":"**CHALLENGE**\n\nItalo, is it possible for me to create a variable with the 15 best sold products, but also with the VALOR and QTDE columns (value and qty)? How about developing ...\n\nIf you can't or give up, send a message on my linkedin https:\/\/www.linkedin.com\/in\/italomarcelo\/\n\nIt will be a pleasure to create a professional connection and exchange knowledge and opportunities","57610cc2":"It will return True, if the data is null or nan and here is a tip: pandas offers several mathematical functions and one of them is sum (). What I'm going to do: instead of displaying the entire dataset, I'm going to add up how many null attributes there are in each variable (column)","9e18e5f6":"The values will vary from -1 (negative correlation) to 1 (Positive correlation). See that the correlation between the same variables are all equal to 1.\n\nAnd here comes a comment: When we work with Machine Learning, especially when we work with regressor algorithms, which work very well with numeric variables, when it comes to training and testing your algorithm, one of the tasks is to say which are the best features variables (columns ) of your dateset correlates better with your target variable (the expected result), and then the corr matrix can help you at that moment\n\nAnd to conclude this long tutorial, let's talk a little bit about the plotting features (graphics) that Pandas contains.\n\nSo, let's make graphics ...","17380f98":"Now I want to know more about the data structure, so I'm going to use the info function that gives us some details about how the dataset is composed, number of rows and columns (and their types), if they have null values, etc.","2c771962":"Pandas returned 99 products to us, that is, the complete PRODUCT column. But I want a variable with all products, however, I don't need to repeat them. And again, Pandas being Pandas:","fb4e7cf0":"Guys, giving continuity in general results according to imposed conditions, another way to do this is using the query function of Pandas. For those who are SQL programmers, they will tie themselves in this function. So, let's program !!","d2e8d86c":"# Pandas\n\nNow the game will get even cooler. When we are working with Pandas, we are largely talking about Series and DataFrame. Series is a column or array of 1 dimension and the DataFrame, in a very simple way to explain, is a multidimensional table with a collection of Series. For those who are starting in Pandas, one thing is certain: being Serie or being DataFrame, both have their index, very used to reference the rows.\n\nBut come on, it's time to play with data ...","563a994e":"# Numpy\n\nIt is very common for python programmers to import the numpy package by calling it np, then follow the command:","f388426c":"Oooww ... didn't you delete it? Of course not, we only display our dataset without null records. To really exclude, we have to assign:","02a65f6e":"That the python language is one of the most used languages \u200b\u200bin the world is nothing new. So I decided to share with you some commands, perhaps the most used for manipulating and analyzing data. And for that I will address only 2 packages, Numpy and Pandas and, of course the combination of them.\n\nThe Numpy package, widely used in calculations with arrays, provides several functions and operations that facilitate when we need to perform numerical calculations and, with ease, be they simple arrays like this: [0, 1] or as a multidimensional array, like this: [ [[0, 1], [0, 0]], [[0, 1], [1, 0]]] With numpy, we can facilitate calculations between arrays such as addition, multiplication, as well as manipulation and \/ or transposition of Dice\n\nPanda is one of the more used packages in Python when it comes to analyze data. And that owes a lot for all its functions that other packages do not offer and, above all, the ease of applying them.\n\nTalking about Numpy and Pandas would be something that would take a lot of time, so I'm going to bring you some commands that I think are super important when our goal is to analyze data. Let's go, baby\u2026","f989a20c":"# Python - Numpy and Pandas\n\n# Basic commands for data analysis","d632369d":"Knowing that an EDA or Exploratory Data Analysis is much more than that and, reinforcing, we will often use a large volume of data.\n\nLet the games begin","e45ddb75":"and let's assume that we don't want records with null values in our dataset\u2026 what do we do? we will delete these null records in our dataset:"}}