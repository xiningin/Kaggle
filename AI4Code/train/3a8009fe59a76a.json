{"cell_type":{"270bde0f":"code","e62eee59":"code","ae5b097d":"code","63868fb6":"code","3abf9d54":"code","8bfb3bcf":"code","c1a752ae":"code","d4be4c0a":"code","f23f0c1a":"code","4f8f7fe0":"code","400d10ff":"code","d9e49bbe":"code","e39b2f54":"code","c496acce":"code","1f8f5d13":"code","af213c6b":"code","cb2fd3c9":"code","64ddded9":"code","cb170d0b":"code","4534e5a0":"code","a43f1110":"code","91b68158":"code","974f285d":"code","a4bfd2fb":"code","df1b4137":"code","75f27711":"code","2bdfeed0":"code","b19a3601":"code","262f0fe6":"code","83b8a197":"code","fcbc03cf":"code","4158a732":"code","79b7336f":"code","21cd9793":"code","0146125d":"code","6c0fdee9":"code","707d9482":"markdown","5526813b":"markdown","86baaeda":"markdown","77e88a7d":"markdown","a401fd8a":"markdown","f2a2f1cb":"markdown","25118ed4":"markdown","ab82eef1":"markdown","d1e886b1":"markdown","2aeacfe8":"markdown","5ac6710b":"markdown","eeff9ec5":"markdown","11acc9b7":"markdown","1d4cb010":"markdown","d9e8cc1a":"markdown","8609db0f":"markdown","08c980d0":"markdown","7b946621":"markdown","668b1037":"markdown","5b5a8146":"markdown","ea757c1b":"markdown","5bf3a5bd":"markdown","6952a7ab":"markdown","f18e9ef5":"markdown","88527d02":"markdown","3fc2d7e2":"markdown"},"source":{"270bde0f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow as tf\nimport time \nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom IPython.core.display import display, HTML\nimport plotly.graph_objects as go\nimport re\n# Natural Language Tool Kit \nimport nltk  \nnltk.download('stopwords') \nfrom nltk.corpus import stopwords \nfrom nltk.stem.porter import PorterStemmer ","e62eee59":"sns.set()","ae5b097d":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\", index_col= 'id')\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\", index_col= 'id')\nsubmission =  pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\", index_col= 'id')","63868fb6":"train.shape","3abf9d54":"test.shape","8bfb3bcf":"train.info()","c1a752ae":"train.head()","d4be4c0a":"train.isnull().sum()","f23f0c1a":"test.isnull().sum()","4f8f7fe0":"train.location.unique()[-10:-1]","400d10ff":"train['location_has_hash'] = train.location.apply(lambda x: 1 if '#'in str(x) else 0 )\ntest['location_has_hash'] = test.location.apply(lambda x: 1 if '#'in str(x) else 0 )","d9e49bbe":"train['location_treated'] = train.location.str.lower().str.replace(r\"[^A-Z|a-z|0-9]\",\" \").str.strip()\ntest['location_treated'] = test.location.str.lower().str.replace(r\"[^A-Z|a-z|0-9]\",\" \").str.strip()","e39b2f54":"train.keyword.unique()[-10:-1]","c496acce":"train['keyword_has_hash'] = train.keyword.apply(lambda x: 1 if '%20'in str(x) else 0 )\ntrain['keyword'] = train.keyword.str.replace(r\"%20\",\" \")\n\ntest['keyword_has_hash'] = test.keyword.apply(lambda x: 1 if '%20'in str(x) else 0 )\ntest['keyword'] = test.keyword.str.replace(r\"%20\",\" \")","1f8f5d13":"train['keyword_treated'] = train.keyword.str.lower().str.replace(r\"[^A-Z|a-z|0-9]\",\" \").str.strip()\ntest['keyword_treated'] = test.keyword.str.lower().str.replace(r\"[^A-Z|a-z|0-9]\",\" \").str.strip()","af213c6b":"train['text'] = train.text.str.lower().str.strip()\ntest['text'] = test.text.str.lower().str.strip()\n\ntrain['text_has_mentions'] = train.text.apply(lambda x: 1 if '@'in str(x) else 0 )\ntest['text_has_mentions'] = test.text.apply(lambda x: 1 if '@'in str(x) else 0 )\n\ntrain['text_mentions_count'] = train.text.apply(lambda x: str(x).count(\"@\"))\ntest['text_mentions_count'] = test.text.apply(lambda x: str(x).count(\"@\"))","cb2fd3c9":"train.head()","64ddded9":"ax = sns.countplot(x = 'target', data = train )\n\nfor p in ax.patches:\n    ax.annotate(f'{p.get_height():.0f}\\n({p.get_height() \/ (train.target.count()) * 100:.1f}%)', \n                xy=(p.get_x() + p.get_width()\/2., p.get_height()), ha='center', xytext=(0,5), textcoords='offset points')\nax.set_ylim(0, 2*train.target.sum())\n_ = plt.title('Target Analysis')","cb170d0b":"train['target_mean'] = train.groupby('keyword_treated')['target'].transform('mean')\n\nfig = plt.figure(figsize=(8, 72), dpi=100)\n\nsns.countplot(y=train.sort_values(by='target_mean', ascending=False)['keyword_treated'],\n              hue=train.sort_values(by='target_mean', ascending=False)['target'])\n\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=12)\nplt.legend(loc=1)\nplt.title('Target Distribution in Keywords')\n\nplt.show()\n\ntrain.drop(columns=['target_mean'], inplace=True)","4534e5a0":"# helper refer https:\/\/www.kaggle.com\/marcovasquez\/basic-nlp-with-tensorflow-and-wordcloud\n\nSTOPWORDS.add('https')  # remove htps to the world Cloud\n\ndef plot_world(text, bg_color):\n    \n    comment_words = ' '\n    stopwords = set(STOPWORDS) \n    \n    for val in text: \n\n        # typecaste each val to string \n        val = str(val) \n\n        # split the value \n        tokens = val.split() \n\n#         # Converts each token into lowercase \n#         for i in range(len(tokens)): \n#             tokens[i] = tokens[i].lower() \n\n        for words in tokens: \n            comment_words = comment_words + words + ' '\n\n\n    wordcloud = WordCloud(width = 5000, height = 4000, \n                    background_color =bg_color, \n                    stopwords = stopwords, \n                    min_font_size = 10).generate(comment_words) \n\n    # plot the WordCloud image                        \n    plt.figure(figsize = (12, 12), facecolor = 'k', edgecolor = 'k' ) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n\n    plt.show() ","a43f1110":"text_1 = train[train.target==1].text.values\nplot_world(text_1, 'green')","91b68158":"text_0 = train[train.target==0].text.values\nplot_world(text_0, 'red')","974f285d":"text_1 = train[train.target==1].keyword_treated.fillna(\"\").values\nplot_world(text_1, 'green')","a4bfd2fb":"text_0 = train[train.target==0].keyword_treated.fillna(\"\").values\nplot_world(text_0, 'red')","df1b4137":"train['text_length'] = train['text'].fillna(\"\").apply(len)\ntest['text_length'] = test['text'].fillna(\"\").apply(len)\n\ntrain['keyword_length'] = train['keyword_treated'].fillna(\"\").apply(str).apply(len)\ntest['keyword_length'] = test['keyword_treated'].fillna(\"\").apply(str).apply(len)\n\ntrain['location_length'] = train['location'].fillna(\"\").apply(str).apply(len)\ntest['location_length'] = test['location'].fillna(\"\").apply(str).apply(len)","75f27711":"_ = sns.factorplot(y = 'text_length', x = 'target', data = train, kind = 'box')","2bdfeed0":"_ = sns.factorplot(x = 'text_length', y = None, data = train, kind = 'count', aspect = 2.5)","b19a3601":"_ = sns.factorplot(x = 'text_length', y = None, data = test, kind = 'count', aspect = 2.5)","262f0fe6":"_ = sns.factorplot(y = 'keyword_length', x = 'target', data = train, kind = 'box')","83b8a197":"_ = sns.factorplot(x = 'keyword_length', y = None, data = train, kind = 'count', aspect = 2.5)","fcbc03cf":"_ = sns.factorplot(x = 'keyword_length', y = None, data = test, kind = 'count', aspect = 2.5)","4158a732":"_ = sns.factorplot(y = 'location_length', x = 'target', data = train, kind = 'box')","79b7336f":"_ = sns.factorplot(x = 'location_length', y = None, data = train, kind = 'count', aspect = 2.5)","21cd9793":"_ = sns.factorplot(x = 'location_length', y = None, data = test, kind = 'count', aspect = 2.5)","0146125d":"print(str(100*test.location_treated.isin(train.location_treated).sum()\/test.location_treated.isin(train.location_treated).count())+\" % of unique test locations are from train\")","6c0fdee9":"print(str(test.keyword_treated.isin(train.keyword_treated).sum()*100\/test.keyword_treated.isin(train.keyword_treated).count())+\" % of unique test keywords are from train\") ","707d9482":"# What is this competition about?\n\n#### We are asked to build a machine learning model that predicts which Tweets are about real disasters and which one\u2019s aren\u2019t. For support we have access to a dataset of 10,000 tweets that were hand classified.","5526813b":"# Keyword Data Analysis","86baaeda":"# Location Length Analysis","77e88a7d":"# Length of Text, Keyword and Location","a401fd8a":"# Keyword Length Analysis","f2a2f1cb":"# Target Analysis","25118ed4":"# Target based Text Column WordCloud","ab82eef1":"# Text Column","d1e886b1":"# Conclusion 1","2aeacfe8":"# Text Length Analysis","5ac6710b":"# The Evaluation\n\nSubmissions are evaluated using F1 between the predicted and expected answers.\n\nHere's more about that and the submission format [Link](https:\/\/www.kaggle.com\/c\/nlp-getting-started\/overview\/evaluation)","eeff9ec5":"# Conclusion 2","11acc9b7":"# Explorers Out!","1d4cb010":"## The Location data has many unique values and the location data is not very structured as one can manually type the address too.","d9e8cc1a":"# Check if all test keywords are there in train","8609db0f":"# Location Column","08c980d0":"# Keyword Column","7b946621":"In the Location column:-\n\n1. Remove spaces from start and end of words.\n2. Make a flag for the words that contain '#' in it.\n3. Replace all special characters with space in the words.\n4. Make all characters lowercase.","668b1037":"In the Keyword column:-\n\n1. Remove spaces from start and end of words.\n2. Make a flag for the words that contain '%20' in it.\n3. Replace all special characters with space in the words.\n4. Make all characters lowercase.","5b5a8146":"# Conclusion - The number of unique keywords and locations seem high enough to not one hot encode them all to get high dimensionality already before touching the text column.","ea757c1b":"# WordCloud with target 0","5bf3a5bd":"# Check if all test locations are there in train","6952a7ab":"# Target based Keyword Column WordCloud","f18e9ef5":"# Updates Coming Soon","88527d02":"#### Location data was pretty rough already. Now there's unseen too.","3fc2d7e2":"# WordCloud with target 1"}}