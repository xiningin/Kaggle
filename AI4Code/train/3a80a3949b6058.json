{"cell_type":{"c60d8495":"code","8ae8035b":"code","06d8125e":"code","15344be4":"code","05d0d0c7":"code","09bc6a2e":"code","612bdce8":"code","c07ba54e":"code","73bd1579":"code","e8c3734d":"code","8d1e325b":"code","a35def0a":"code","61ed5fd6":"code","3f786d1b":"code","b4db0a53":"code","0502ecea":"code","6ed70af4":"code","fc4d549e":"code","473cb210":"code","d6057e6b":"code","8b44f908":"code","b5299301":"code","42118051":"code","7dc0d0f4":"code","abb851a5":"code","1331e356":"code","e4219125":"code","0bf6d591":"code","acf96e45":"code","c25bbb42":"code","297180a0":"code","e5feaf3f":"code","816d6b69":"code","f86cde7b":"code","7aec5d71":"code","9942200c":"code","295b96cc":"code","10985af8":"code","040779f4":"code","7d4c9ff8":"code","d41859ab":"code","72eaa903":"code","48781f63":"code","e676ea4e":"code","33e76bfe":"code","c6309a0b":"code","de7b7b0e":"code","e9493803":"code","89894568":"code","a3b11956":"code","2ceb0e4c":"code","8ce605bd":"code","091082e6":"code","7501e3e5":"code","0c8c0a02":"code","967e24ab":"code","5580a17b":"code","bd93c372":"code","c0786856":"code","6f29776f":"code","22e9e5d1":"code","c9134a68":"code","abf4b8a2":"code","b94f3cc1":"code","861848ea":"code","a3995099":"code","a4e11382":"code","e399e76a":"code","7c3bde89":"code","5e72e0d7":"code","997532da":"code","741fc6f5":"code","2b5eb386":"code","13f84d14":"code","7c3d16bd":"code","55cb07cf":"code","d1438acb":"code","5a69acc7":"code","99bda7f8":"code","61f4b8f2":"code","193778e0":"code","f53627ae":"code","6f27688c":"code","fb25e927":"code","95cc4dc7":"code","88b1f401":"code","125da434":"code","ed07ea4f":"code","8c23f052":"code","410de0e6":"code","5950033c":"code","e7a9fcf6":"code","b3d898e6":"code","520069cf":"code","2bde008c":"code","2f0d7e52":"code","56403cca":"code","17e4ce62":"code","28d9d780":"code","738e0543":"code","bdf6f45f":"code","c22d0d64":"code","db856103":"code","6e74b3e6":"code","82a490a7":"code","ef975457":"code","ed425aa6":"code","3ba9da89":"code","6ee36103":"code","6899c956":"code","22a38939":"code","8368ee09":"code","7a36b2d7":"code","8974ae83":"code","1a56bea0":"code","bbd77ba3":"code","8eba296d":"code","d2a64690":"code","33747234":"code","43b066bf":"code","8a03dbfa":"code","c16682b1":"code","ec156b9c":"code","ee55e22c":"code","e8bb514d":"code","b63737aa":"code","4048defc":"code","4cb7536c":"code","9f272d65":"code","cb97f55c":"code","032d52bf":"code","b42bc030":"code","478dced3":"code","e46fc8ae":"code","99af314d":"code","2f5a8242":"code","3e586c9d":"code","e3559dde":"code","cac54405":"code","5eb70367":"code","93eed4cd":"code","c2373e66":"code","2087cab0":"code","703a5722":"code","e8198cde":"code","93767576":"code","55e98401":"code","41d207ae":"code","d49b75c3":"code","89372792":"code","3c3a7e4a":"code","9080c146":"code","b5f2c25c":"markdown","cced351d":"markdown","58838988":"markdown","cc0b3fa6":"markdown","37e213b2":"markdown","efa1ea5f":"markdown","9b127d5d":"markdown","c8cfb3ac":"markdown","2db07d4b":"markdown","12491573":"markdown","2418808c":"markdown","3119105c":"markdown","f19d9485":"markdown","9f3a1198":"markdown","f926bffa":"markdown","79ede00b":"markdown","452182bc":"markdown","0d1868dd":"markdown","4fe6fc08":"markdown","78955e37":"markdown","b85be885":"markdown","64651f5b":"markdown","6ec46285":"markdown","978ee001":"markdown","8e10ea6f":"markdown","8884b54a":"markdown","1381c3c0":"markdown","b7842d3e":"markdown","215dfa96":"markdown","e07422ad":"markdown","b236751f":"markdown","984244c7":"markdown","b3414724":"markdown","1bb0ac5d":"markdown","44e7d3c3":"markdown","ac8d0a61":"markdown","8e34f760":"markdown","664a9b27":"markdown"},"source":{"c60d8495":"pip install imbalanced-learn","8ae8035b":"!pip install pycaret","06d8125e":"#Import Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import date","15344be4":"#Load data\ninitial= pd.read_csv('https:\/\/ckan0.cf.opendata.inter.prod-toronto.ca\/download_resource\/e5bf35bc-e681-43da-b2ce-0242d00922ad?format=csv')","05d0d0c7":"#Check total number of records, initial data types and number of missing values\n#Most recent data set has 15540 records, 3 features have null values\ninitial.info()","09bc6a2e":"#Most features are categorical and boolean\n#There are two date features\ninitial.head(5)","612bdce8":"#According to the data dictionary, the column \"_id\" is a unique identifier value, not useful for analysis\nboolean = initial['_id'].duplicated().any() \nboolean","c07ba54e":"#The column \"Assigned_ID\" is also a unique value used for tracking purposes and resolving duplicate cases, and not useful for analysis\nboolean = initial['Assigned_ID'].duplicated().any() \nboolean","73bd1579":"#The column \"Outcome\" is our target variable\n#ACTIVE represents an existing infection where the patient has neither died (FATAL) nor tested negative (RESOLVED) yet\ninitial['Outcome'].value_counts()","e8c3734d":"# Records of fatal outcome\ntotal = len(initial)\nfatal = initial.loc[initial['Outcome'] == \"FATAL\"]\ntotal_fatal = len(fatal)\npercentagefatal = total_fatal\/total*100\npercentagefatal","8d1e325b":"#Vizualize fatalities against other outcomes\nsns.countplot(x=\"Outcome\", data=initial)","a35def0a":"#Need to combine RESOLVED+ACTIVE so that target variable can be a boolean\ninitial['Outcome'] = initial['Outcome'].replace(['RESOLVED'],0)\ninitial['Outcome'] = initial['Outcome'].replace(['ACTIVE'],0)\ninitial['Outcome'] = initial['Outcome'].replace(['FATAL'],1)","61ed5fd6":"#The column \"Outbreak Associated\" has two potential values, this is how the infection has been categorized\ninitial['Outbreak Associated'].value_counts()","3f786d1b":"#The label is confusing as it is the same string one of the two possible values\n#Will rename to \"Infection Category\" as it is a more accurate representation\ninitial.rename(columns={'Outbreak Associated':'Infection Category'}, inplace=True)","b4db0a53":"#Can see that among fatal outcomes, the infection is more likely to be associated with an outbreak (proportionally)\nsns.catplot(x=\"Infection Category\", col=\"Outcome\",\n                data=initial, kind=\"count\",\n                height=4, aspect=.7);","0502ecea":"#Although there are no null values, can see that there are two values which \n#represent data not currently present (Unknown\/Missing and Pending)\n#Later on, these will be combined to be the value \"Currently Unknown\"\ninitial['Source of Infection'].value_counts()","6ed70af4":"#Visualizing, can see that among fatal outcomes they are primarily associated with Outbreaks\n#For some reason, \"Close contact\" occurs very often among non-fatal but not for fatal outcomes\n#Can also see that instances where the source is unknown are primarily associated with non-fatal outcomes\nsns.catplot(x=\"Source of Infection\", col=\"Outcome\",\n                data=initial, kind=\"count\",\n                height=8, aspect=1.7);","fc4d549e":"#This feature represents whether the case was confirmed or not\n#Link to definitions: http:\/\/www.health.gov.on.ca\/en\/pro\/programs\/publichealth\/coronavirus\/docs\/2019_case_definition.pdf\n#PROBABLE either means there is evidence to believe a person was exposed to the virus but lacking a laboratory test\n#or it means the person exhibits symptoms of COVID but laboratory tests are inconclusive\n\ninitial['Classification'].value_counts()","473cb210":"#Can see that among fatal outcomes, very few are classified as probable\nsns.catplot(x=\"Classification\", col=\"Outcome\",\n\n                data=initial, kind=\"count\",\n\n                height=6, aspect=1.0);","d6057e6b":"#Create a new dataframe containing only PROBABLE cases\nprobable = initial.loc[initial['Classification'] == \"PROBABLE\"]","8b44f908":"#Viualize\nsns.catplot(x='Outcome',\n\n                data=probable, kind=\"count\",\n\n                height=6, aspect=1.0);","b5299301":"#Can see that amongst cases categorized as PROBABLE\n#Fatality rate is lower than the 7.5% rate in the entire dataset\nprobable['Outcome'].value_counts()","42118051":"total_probable = len(probable)\nfatal_probable = probable.loc[probable['Outcome'] == 1]\ntotal_fatal_probable = len(fatal_probable)\n","7dc0d0f4":"percentage_fatal_probable = total_fatal_probable\/total_probable*100\npercentage_fatal_probable","abb851a5":"#239 cases where gender is UNKNOWN, does this mean not collected?  It could also mean information is pending\n#as we saw this number decrease over time\ninitial['Client Gender'].value_counts()","1331e356":"#There are zero fatal outcomes amongst the patients who are OTHER or TRANSGENDER\n#Later on, we will combine the values OTHER and TRANSGENDER to become NON-BINARY\nsns.catplot(x=\"Client Gender\", col=\"Outcome\",\n\n                data=initial, kind=\"count\",\n\n                height=8, aspect=.6);","e4219125":"#Need to change values to be more readable for visualization purposes\ninitial['Age Group'] = initial['Age Group'].replace(['19 and younger'],\"<19\")\ninitial['Age Group'] = initial['Age Group'].replace(['20 to 29 Years'],\"20-29\")\ninitial['Age Group'] = initial['Age Group'].replace(['30 to 39 Years'],\"30-39\")\ninitial['Age Group'] = initial['Age Group'].replace(['40 to 49 Years'],\"40-49\")\ninitial['Age Group'] = initial['Age Group'].replace(['50 to 59 Years'],\"50-59\")\ninitial['Age Group'] = initial['Age Group'].replace(['60 to 69 Years'],\"60-69\")\ninitial['Age Group'] = initial['Age Group'].replace(['70 to 79 Years'],\"70-79\")\ninitial['Age Group'] = initial['Age Group'].replace(['80 to 89 Years'],\"80-89\")\ninitial['Age Group'] = initial['Age Group'].replace(['90 and older'],\">90\")","0bf6d591":"#Look at counts\ninitial['Age Group'].value_counts()","acf96e45":"#Create a separate dataframe to look at fatal outcomes, to see age groups\nagedf = initial.loc[initial['Outcome'] == 1]","c25bbb42":"#Visualize the ages of fatal outcomes and order the labels in increasing order\n#Can see that fatalies are common in the higher age groups\n#Although the highest age group is not the highest volume of fatalities, \n#there are fewer people in general who reach this age (90+) as life expectancy is lower than 90: https:\/\/www150.statcan.gc.ca\/t1\/tbl1\/en\/tv.action?pid=1310040901\ncategory_order = [\"<19\", \"20-29\", \"30-39\", \"40-49\", \"50-59\", \"60-69\", \"70-79\", \"80-89\", \">90\"]\n\nsns.catplot(x=\"Age Group\", \n\n                data=agedf, kind=\"count\", order=category_order,\n\n                height=8, aspect=1.2);","297180a0":"#There are 27 rows with no value for age\ninitial['Age Group'].isnull().sum()","e5feaf3f":"#Temporarily put value Unknown when there is no value\ninitial['Age Group'] = initial['Age Group'].fillna(\"Age Unknown\")","816d6b69":"#Create new dataframe for anytime Age is not available\nunknownage = initial.loc[initial['Age Group']==\"Age Unknown\"]","f86cde7b":"#Can see that for all instances where Age Group is unknown, there was no fatality nor hospitalization\n#Additionally, the majority are \"PROBABLE\" infections\n#These may represent that there was an issue in the data collection process that was not corrected \n#The missing age values represent a small percentage of the total data set and may be removed later on\nunknownage","7aec5d71":"#We can see that less than 0.5% of patients in the data set are hospitalized\ninitial['Currently Hospitalized'].value_counts()\/total*100","9942200c":"#Check to see the ages of those currently in hospital\nhospitalized = initial.loc[initial['Currently Hospitalized']==\"Yes\"]","295b96cc":"#Those hospitalized are mostly older than 49, but the data is not nearly as scewed than those cases where the outcome was fatal\nsns.catplot(x=\"Age Group\", \n\n                data=hospitalized, kind=\"count\", order=category_order,\n\n                height=8, aspect=1.2);","10985af8":"#We can see that only ~0.1% of patients in the data set are in the ICU\ninitial['Currently in ICU'].value_counts()\/total*100","040779f4":"#Vizualise the spread for Age Group, can see no one younger than 30 is in the ICU \nICU = initial.loc[initial['Currently in ICU']==\"Yes\"]\n\nsns.catplot(x=\"Age Group\", \n\n                data=ICU, kind=\"count\", order=category_order,\n\n                height=8, aspect=1.2);\n","7d4c9ff8":"#We can see that less than %0.1 of patients in the data set are intubated\ninitial['Currently Intubated'].value_counts()\/total*100","d41859ab":"#Similar to those in the ICU\n#Seems like most of the patients in the ICU are Inutubated\nintubated = initial.loc[initial['Currently Intubated']==\"Yes\"]\n\nsns.catplot(x=\"Age Group\", \n\n                data=intubated, kind=\"count\", order=category_order,\n\n                height=8, aspect=1.2);","72eaa903":"#We can see that ~12% of patients in the data set have ever been hospitalized\ninitial['Ever Hospitalized'].value_counts()\/total*100","48781f63":"#Can see that patients who were ever hospitalized had higher likelihood (~6 times) of fatal outcome\nsns.barplot(x = 'Ever Hospitalized', y = 'Outcome', data = initial, \n            palette = 'hls',\n             \n            capsize = 0.05,             \n            saturation = 8,             \n            errcolor = 'gray',   \n             \n            )\nplt.show()","e676ea4e":"#We can see that about 2.6% of patients in the data set have ever been in the ICU\ninitial['Ever in ICU'].value_counts()\/total*100","33e76bfe":"#Can see that patients who were ever in the ICU had higher likelihood (~5 times) of fatal outcome\nsns.barplot(x = 'Ever in ICU', y = 'Outcome', data = initial, \n            palette = 'hls',\n             \n            capsize = 0.05,             \n            saturation = 8,             \n            errcolor = 'gray',   \n             \n            )\nplt.show()","c6309a0b":"#We can see that about %1.9 of patients in the data set have ever been hospitalized\ninitial['Ever Intubated'].value_counts()\/total*100","de7b7b0e":"#Can see that patients who were ever intubated had higher likelihood (~5 times) of fatal outcome\nsns.barplot(x = 'Ever Intubated', y = 'Outcome', data = initial, \n            palette = 'hls',\n             \n            capsize = 0.05,             \n            saturation = 8,             \n            errcolor = 'gray',   \n             \n            )\nplt.show()","e9493803":"#According to the data dictionary, this feature represents when the data was reported to Toronto Public Health\nreporteddate = sns.catplot(x=\"Reported Date\",  col=\"Outcome\",\n data=initial, kind=\"count\",\n          height=8, aspect=1.0);","89894568":"#Create vizualization for Episode Date, which is the best estimate of when the infection occured (derived via other means)\nreporteddate = sns.catplot(x=\"Episode Date\",  col=\"Outcome\",\n data=initial, kind=\"count\",\n          height=8, aspect=1.0);","a3b11956":"#This feature represents a geographical area associated with the first 3 digits of a postal code and has some null values\ninitial['FSA'].value_counts()","2ceb0e4c":"#Temporarily fill with a default value and plot\ninitial['FSA'] = initial['FSA'].fillna(\"Unknown_FSA\")\nsns.catplot(x=\"FSA\",  \n data=initial, kind=\"count\",\n          height=10, aspect=6.0);","8ce605bd":"#This is another geographic feature that has null values\ninitial['Neighbourhood Name'].value_counts()","091082e6":"#Fill missing values with \"Unknown\" temporarily\ninitial['Neighbourhood Name'] = initial['Neighbourhood Name'].fillna(\"Unknown\")\nsns.catplot(x=\"Neighbourhood Name\",  \n data=initial, kind=\"count\",\n          height=10, aspect=6.0);","7501e3e5":"#Combine Forward Sortation Area (FSA) into boroughs https:\/\/www.postalcodesincanada.com\/province-ontario\/\ninitial['FSA'] = initial['FSA'].replace(['M1B'],\"Scarborough\")\ninitial['FSA'] = initial['FSA'].replace(['M1C'],\"Scarborough\")\ninitial['FSA'] = initial['FSA'].replace(['M1E'],\"Scarborough\")\ninitial['FSA'] = initial['FSA'].replace(['M1G'],\"Scarborough\")\ninitial['FSA'] = initial['FSA'].replace(['M1H'],\"Scarborough\")\ninitial['FSA'] = initial['FSA'].replace(['M1J'],\"Scarborough\")\ninitial['FSA'] = initial['FSA'].replace(['M1K'],\"Scarborough\")\ninitial['FSA'] = initial['FSA'].replace(['M1L'],\"Scarborough\")\ninitial['FSA'] = initial['FSA'].replace(['M1M'],\"Scarborough\")\ninitial['FSA'] = initial['FSA'].replace(['M1N'],\"Scarborough\")\ninitial['FSA'] = initial['FSA'].replace(['M1P'],\"Scarborough\")\ninitial['FSA'] = initial['FSA'].replace(['M1R'],\"Scarborough\")\ninitial['FSA'] = initial['FSA'].replace(['M1S'],\"Scarborough\")\ninitial['FSA'] = initial['FSA'].replace(['M1T'],\"Scarborough\")\ninitial['FSA'] = initial['FSA'].replace(['M1V'],\"Scarborough\")\ninitial['FSA'] = initial['FSA'].replace(['M1W'],\"Scarborough\")\ninitial['FSA'] = initial['FSA'].replace(['M1X'],\"Scarborough\")\ninitial['FSA'] = initial['FSA'].replace(['M2H'],\"North York\")\ninitial['FSA'] = initial['FSA'].replace(['M2J'],\"North York\")\ninitial['FSA'] = initial['FSA'].replace(['M2K'],\"North York\")\ninitial['FSA'] = initial['FSA'].replace(['M2L'],\"North York\")\ninitial['FSA'] = initial['FSA'].replace(['M2M'],\"North York\")\ninitial['FSA'] = initial['FSA'].replace(['M2N'],\"North York\")\ninitial['FSA'] = initial['FSA'].replace(['M2P'],\"North York\")\ninitial['FSA'] = initial['FSA'].replace(['M2R'],\"North York\")\ninitial['FSA'] = initial['FSA'].replace(['M3A'],\"North York\")\ninitial['FSA'] = initial['FSA'].replace(['M3B'],\"North York\")\ninitial['FSA'] = initial['FSA'].replace(['M3C'],\"North York\")\ninitial['FSA'] = initial['FSA'].replace(['M3H'],\"North York\")\ninitial['FSA'] = initial['FSA'].replace(['M3J'],\"North York\")\ninitial['FSA'] = initial['FSA'].replace(['M3K'],\"North York\")\ninitial['FSA'] = initial['FSA'].replace(['M3L'],\"North York\")\ninitial['FSA'] = initial['FSA'].replace(['M3M'],\"North York\")\ninitial['FSA'] = initial['FSA'].replace(['M3N'],\"North York\")\ninitial['FSA'] = initial['FSA'].replace(['M4A'],\"North York\")\ninitial['FSA'] = initial['FSA'].replace(['M4B'],\"East York\")\ninitial['FSA'] = initial['FSA'].replace(['M4C'],\"East York\")\ninitial['FSA'] = initial['FSA'].replace(['M4E'],\"East Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M4G'],\"East York\")\ninitial['FSA'] = initial['FSA'].replace(['M4H'],\"East York\")\ninitial['FSA'] = initial['FSA'].replace(['M4J'],\"East York\")\ninitial['FSA'] = initial['FSA'].replace(['M4K'],\"East Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M4L'],\"East Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M4M'],\"East Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M4N'],\"Central Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M4P'],\"Central Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M4R'],\"Central Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M4S'],\"Central Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M4T'],\"Central Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M4V'],\"Central Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M4W'],\"Downtown Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M4X'],\"Downtown Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M4Y'],\"Downtown Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M5A'],\"Downtown Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M5B'],\"Downtown Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M5C'],\"Downtown Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M5E'],\"Downtown Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M5G'],\"Downtown Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M5H'],\"Downtown Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M5J'],\"Downtown Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M5M'],\"North York\")\ninitial['FSA'] = initial['FSA'].replace(['M5N'],\"Central Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M5P'],\"Central Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M5R'],\"Central Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M5S'],\"Central Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M5T'],\"Downtown Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M5V'],\"Downtown Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M6A'],\"North York\")\ninitial['FSA'] = initial['FSA'].replace(['M6B'],\"North York\")\ninitial['FSA'] = initial['FSA'].replace(['M6C'],\"York\")\ninitial['FSA'] = initial['FSA'].replace(['M6E'],\"York\")\ninitial['FSA'] = initial['FSA'].replace(['M6G'],\"Downtown Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M6H'],\"West Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M6J'],\"West Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M6K'],\"West Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M6L'],\"North York\")\ninitial['FSA'] = initial['FSA'].replace(['M6M'],\"York\")\ninitial['FSA'] = initial['FSA'].replace(['M6N'],\"York\")\ninitial['FSA'] = initial['FSA'].replace(['M6P'],\"West Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M6R'],\"West Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M6S'],\"West Toronto\")\ninitial['FSA'] = initial['FSA'].replace(['M8V'],\"Etobicoke\")\ninitial['FSA'] = initial['FSA'].replace(['M8W'],\"Etobicoke\")\ninitial['FSA'] = initial['FSA'].replace(['M8X'],\"Etobicoke\")\ninitial['FSA'] = initial['FSA'].replace(['M8Y'],\"Etobicoke\")\ninitial['FSA'] = initial['FSA'].replace(['M8Z'],\"Etobicoke\")\ninitial['FSA'] = initial['FSA'].replace(['M9A'],\"Etobicoke\")\ninitial['FSA'] = initial['FSA'].replace(['M9B'],\"Etobicoke\")\ninitial['FSA'] = initial['FSA'].replace(['M9C'],\"Etobicoke\")\ninitial['FSA'] = initial['FSA'].replace(['M9L'],\"North York\")\ninitial['FSA'] = initial['FSA'].replace(['M9M'],\"North York\")\ninitial['FSA'] = initial['FSA'].replace(['M9N'],\"York\")\ninitial['FSA'] = initial['FSA'].replace(['M9P'],\"Etobicoke\")\ninitial['FSA'] = initial['FSA'].replace(['M9R'],\"Etobicoke\")\ninitial['FSA'] = initial['FSA'].replace(['M9V'],\"Etobicoke\")\ninitial['FSA'] = initial['FSA'].replace(['M9W'],\"Etobicoke\")\n\n#For null values, populate with GTA to generalize\ninitial['FSA'] = initial['FSA'].fillna(\"Unknown_FSA\")\ninitial['FSA'] = initial['FSA'].replace(['Unknown_FSA'],\"GTA\")\n\ninitial['FSA'].value_counts()","0c8c0a02":"#Can see some boroughs in Toronto have a higher chance of fatality\nsns.barplot(x = 'FSA', y = 'Outcome', data = initial, \n            palette = 'hls',\n             \n            capsize = 0.05,             \n            saturation = 8,             \n            errcolor = 'gray', \n            )\nplt.show()","967e24ab":"#First need to adjust data type and determine day of the week the data corresponds to\ninitial['Episode Date'] = pd.to_datetime(initial['Episode Date'].str.strip())\ninitial['day_of_week'] = initial['Episode Date'].dt.day_name()\n\n#Now that we have created a new day of the week feature, can vizualise\n#Can see that higher volume of infections are reported on weekdays vs. weekends\n\ndayofweekorder = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n\nsns.catplot(x=\"day_of_week\",  col=\"Outcome\", order=dayofweekorder,\n data=initial, kind=\"count\",\n          height=8, aspect=1.0);","5580a17b":"#Will create new feature, weekday(0) or weekend(1)\ninitial['day_of_week'] = initial['day_of_week'].replace(['Monday'],0)\ninitial['day_of_week'] = initial['day_of_week'].replace(['Tuesday'],0)\ninitial['day_of_week'] = initial['day_of_week'].replace(['Wednesday'],0)\ninitial['day_of_week'] = initial['day_of_week'].replace(['Thursday'],0)\ninitial['day_of_week'] = initial['day_of_week'].replace(['Friday'],0)\ninitial['day_of_week'] = initial['day_of_week'].replace(['Saturday'],1)\ninitial['day_of_week'] = initial['day_of_week'].replace(['Sunday'],1)","bd93c372":"#Reported Date needs to be adjusted first\ninitial['Reported Date'] = pd.to_datetime(initial['Reported Date'].str.strip())","c0786856":"#Calculate date difference by subtracting Episode Date from Reported Date\ninitial['date_diff'] = (initial['Reported Date']-initial['Episode Date']).dt.days","6f29776f":"#Now have a continuous variable\ninitial['date_diff'].describe()","22e9e5d1":"#Calculate difference between todays date and Episode Date\ncurrent = pd.to_datetime('today')\ninitial['curr_date_diff'] = (current-initial['Episode Date']).dt.days","c9134a68":"#Now have another continuous variable\ninitial['curr_date_diff'].describe()","abf4b8a2":"initial","b94f3cc1":"#For \"Source of Infection\", need to combine values and rename for better readability\ninitial.rename(columns={'Source of Infection':'Source'}, inplace=True)\n\ninitial['Source'] = initial['Source'].replace(['Unknown\/Missing'],'Currently Unknown')\ninitial['Source'] = initial['Source'].replace(['Pending'],'Currently Unknown')\n\ninitial['Source'] = initial['Source'].replace(['N\/A - Outbreak associated'],'Outbreak')\n\ninitial['Source'].value_counts()","861848ea":"#For \"Classification\", convert to integer \ninitial['Classification'] = initial['Classification'].replace(['CONFIRMED'],0)\ninitial['Classification'] = initial['Classification'].replace(['PROBABLE'],1)","a3995099":"#For \"Client Gender\" need to combine meaning of values and rename for better reability\ninitial.rename(columns={'Client Gender':'Gender'}, inplace=True)\ninitial['Gender'] = initial['Gender'].replace(['TRANSGENDER'],\"NON-BINARY\")\ninitial['Gender'] = initial['Gender'].replace(['OTHER'],\"NON-BINARY\")\n\ninitial['Gender'].value_counts()","a4e11382":"#For \"Age Group\" need to convert values to ordinal integers\n#Due to the small amount of records containing no value for Age, will drop columns\n\ninitial['Age Group'] = initial['Age Group'].replace(['<19'],\"1\")\ninitial['Age Group'] = initial['Age Group'].replace(['20-29'],\"2\")\ninitial['Age Group'] = initial['Age Group'].replace(['30-39'],\"3\")\ninitial['Age Group'] = initial['Age Group'].replace(['40-49'],\"4\")\ninitial['Age Group'] = initial['Age Group'].replace(['50-59'],\"5\")\ninitial['Age Group'] = initial['Age Group'].replace(['60-69'],\"6\")\ninitial['Age Group'] = initial['Age Group'].replace(['70-79'],\"7\")\ninitial['Age Group'] = initial['Age Group'].replace(['80-89'],\"8\")\ninitial['Age Group'] = initial['Age Group'].replace(['>90'],\"9\")\n\ninitial.replace (\"Age Unknown\", np.nan, inplace = True)\ninitial['Age Group'].value_counts()","e399e76a":"initial.dropna(subset=['Age Group'],inplace= True)","7c3bde89":"#For \"Infection Category\", need to convert to integers\ninitial['Infection Category'] = initial['Infection Category'].replace(['Sporadic'],0)\ninitial['Infection Category'] = initial['Infection Category'].replace(['Outbreak Associated'],1)","5e72e0d7":"#For \"Ever Hospitalized\", need to convert to integers\ninitial['Ever Hospitalized'] = initial['Ever Hospitalized'].replace(['No'],0)\ninitial['Ever Hospitalized'] = initial['Ever Hospitalized'].replace(['Yes'],1)","997532da":"#Will drop \"_id\" and \"Assigned_ID\" as these are ID values and not used for analysis\n#Will drop \"Neighbourhood Name\" as this is a geographic feature which overlaps with FSA and FSA has been flattened into larger geographic areas\n#Will drop \"Currently Hospitalized\", \"Currently in ICU\", \"Currently Intubated\" as these features are only true at a specific instance\n#in time (patient is in hospital) and we do not have metadata knowing when patient was admitted or discharged. Also, majority of\n#patients not admitted to hospital\n#Will drop \"Ever in ICU\" and \"Ever Intubated\" as the feature \"Ever Hospitalized\" includes all those instance (avoiding counting more than once)\n#Will drop \"Episode Date\" and \"Reported Date\" as these have been used to create new features\ndropcolumns = ['_id','Assigned_ID','Neighbourhood Name','Currently Hospitalized','Currently in ICU',\n               'Currently Intubated','Ever in ICU','Ever Intubated','Episode Date','Reported Date']\n\n\ninitial.drop(columns=dropcolumns, axis=1, inplace=True)\n","741fc6f5":"initial","2b5eb386":"#Convert FSA to onehot encoding\nfsa_dummies = pd.get_dummies(initial.FSA, drop_first=False)\ninitial = pd.concat([initial, fsa_dummies], axis=1)\ninitial = pd.get_dummies(initial, columns = [\"FSA\"])","13f84d14":"#Convert Gender to onehot encoding\ngender_dummies = pd.get_dummies(initial.Gender, drop_first=False)\ninitial = pd.concat([initial, gender_dummies], axis=1)\ninitial = pd.get_dummies(initial, columns = [\"Gender\"])","7c3d16bd":"#Convert Source (of Infection) to onehot encoding\nsource_dummies = pd.get_dummies(initial.Source, drop_first=False)\ninitial = pd.concat([initial, source_dummies], axis=1)\ninitial = pd.get_dummies(initial, columns = [\"Source\"])","55cb07cf":"#Drop additional redundant columns\nmorecolumns = ['Central Toronto','Downtown Toronto','East Toronto','East York','Etobicoke','GTA',\n               'North York','Scarborough','West Toronto','York','NON-BINARY','FEMALE','MALE',\n               'UNKNOWN','Close contact','Community','Currently Unknown','Healthcare',\n               'Institutional','Outbreak','Travel']\n\n\ninitial.drop(columns=morecolumns, axis=1, inplace=True)","d1438acb":"#Make a copy\nscaled_features = initial.copy()","5a69acc7":"#Scale the new features\nfrom sklearn.preprocessing import StandardScaler\ncol_names = ['date_diff', 'curr_date_diff']\nfeatures = scaled_features[col_names]\nscaler = StandardScaler().fit(features.values)\nfeatures = scaler.transform(features.values)","99bda7f8":"\nscaled_features[col_names] = features\nprint(scaled_features)","61f4b8f2":"#Dataframe \"df1\" is the resulting dataset from which we will perform further analysis\ndf1=scaled_features\ndf1.head()","193778e0":"df1.isnull().sum()","f53627ae":"# Importing Sklearn libraries\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import RandomizedSearchCV\n","6f27688c":"##### Defining Target (y) and Data (X) for model development","fb25e927":"target = df1.iloc[:,3]\ntarget.value_counts()","95cc4dc7":"## Noticeable here is that among the target instances, only about 7.5% are in 0 category and 92.5% are in 1 category. So, we are dealing with imbalanced data. ","88b1f401":"data = df1.loc[:, df1.columns != 'Outcome']\nX, y = data, target\nprint(X.shape)\nprint(y.shape)","125da434":"# Train Test Split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size= 0.3, random_state = 42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","ed07ea4f":"# Dummy Classification\n\ndummy_clf = DummyClassifier(strategy=\"most_frequent\")\ndummy_clf.fit(X_train, y_train)","8c23f052":"y_pred_dummy = dummy_clf.predict(X_test)\nconfusion_matrix(y_test, y_pred_dummy)","410de0e6":"print(\"Precision: {}\".format(precision_score(y_test, y_pred_dummy)))\nprint(\"Recall: {}\".format(recall_score(y_test, y_pred_dummy)))\nprint(\"f1_score: {}\".format(f1_score(y_test, y_pred_dummy)))\nprint(\"Accuracy: {}\".format(accuracy_score(y_test, y_pred_dummy)))","5950033c":"# Modeling the data AS IS for Logistic Regression\n\n# Train model\n\nlr_clf = LogisticRegression(max_iter=1000, tol=1e-3, random_state=42)\nlr_clf.fit(X_train, y_train)","e7a9fcf6":"## Cross validation\ncross_val_score(lr_clf, X_train, y_train, cv=5, scoring=\"accuracy\")","b3d898e6":"# Predict on training set\n\ny_pred_lr = lr_clf.predict(X_test)","520069cf":"## Confusion Matrix\nconfusion_matrix(y_test, y_pred_lr)","2bde008c":"#Checking Accuracy, Percision, Recall and f1 Score\n\nprint(\"Precision: {}\".format(precision_score(y_test, y_pred_lr)))\nprint(\"Recall: {}\".format(recall_score(y_test, y_pred_lr)))\nprint(\"f1_score: {}\".format(f1_score(y_test, y_pred_lr)))\nprint(\"Accuracy: {}\".format(accuracy_score(y_test, y_pred_lr)))","2f0d7e52":"lr_auc = roc_auc_score(y_test, y_pred_lr)\nlr_auc","56403cca":"# Although accuracy is 92.75%, the recall is extremely low and a very low F1 score. We need to get a higher recall in this model. In other words, we would like to have the minimum number of false negative predictions.\n# This is important in case of our data, as we do not want to report the cases are not fatal, when they actuallt are.\n\n# As descibed above, the data is a highly imbalance data set with a very high proportion leaning towards Recovery class \n\n# 0    14350\n# 1     1163\n\n# To overcome this we need to balance the dataset by resampling. The method that would be employed is \"Undersampling\"","17e4ce62":"# Shuffle the Dataset.\n\nshuffled_df = df1.sample(frac=1,random_state=42)","28d9d780":"# Put all the fatal outcomes in a separate dataset.\n\nfatal_df = shuffled_df.loc[shuffled_df['Outcome'] == 1]","738e0543":"#Randomly select 1000 observations from the resolved (majority class)\n\nresolved_df = shuffled_df.loc[shuffled_df['Outcome'] == 0].sample(n=1200,random_state=42)","bdf6f45f":"# Concatenate both dataframes again\n\nnormalized_df = pd.concat([fatal_df, resolved_df])\nnormalized_df[\"Outcome\"].value_counts()","c22d0d64":"#plot the dataset before undersampling\nplt.figure(figsize=(8, 8))\nsns.countplot('Outcome', data=df1)\nplt.title('Initial')\nplt.show()\n\n#plot the dataset after undersampling\nplt.figure(figsize=(8, 8))\nsns.countplot('Outcome', data=normalized_df)\nplt.title('Balanced Classes')\nplt.show()","db856103":"## The other way to do deal with imbalanced data is using the SMOTH method:\n\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE \nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\n\nover = SMOTE(sampling_strategy = 0.1)      # We can get different number of 0 and 1 instances by changing the sampling strategy values\nunder = RandomUnderSampler(sampling_strategy = 0.8)    \nsteps = [('o', over) , ('u', under)]\npipeline = Pipeline(steps =  steps) \n\nX_res,y_res = pipeline.fit_resample(X,y)\ncounter = Counter(y_res)","6e74b3e6":"counter","82a490a7":"## We can either work with normalized df or with x-res and y_res","ef975457":"target_bal = normalized_df.iloc[:,3]\ntarget_bal.value_counts()","ed425aa6":"#define the new X and y\n\nnormalized_data = normalized_df.loc[:, df1.columns != 'Outcome']\nX, y = normalized_data, target_bal\nprint(X.shape)\nprint(y.shape)","3ba9da89":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size= 0.3, random_state = 42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","6ee36103":"lrn_clf = LogisticRegression(max_iter=1000, tol=1e-3, random_state=42).fit(X_train, y_train)","6899c956":"y_lrn_pred = lrn_clf.predict(X_test)","22a38939":"confusion_matrix(y_test, y_lrn_pred)","8368ee09":"print(\"Precision: {}\".format(precision_score(y_test, y_lrn_pred)))\nprint(\"Recall: {}\".format(recall_score(y_test, y_lrn_pred)))\nprint(\"f1_score: {}\".format(f1_score(y_test, y_lrn_pred)))\nprint(\"Accuracy: {}\".format(accuracy_score(y_test, y_lrn_pred)))","7a36b2d7":"# Although the accuracy of the resampled model has reduced slightly, the recall, precision and f1 score have increased significantly.","8974ae83":"lrn_auc = roc_auc_score(y_test, y_lrn_pred)\nlrn_auc","1a56bea0":"## Grid search on Logisitic Regression to Hyperparameter Tuning","bbd77ba3":"lrn_clf= LogisticRegression(max_iter=1000, tol=1e-3, random_state=42)\ngrid_values = {'penalty': ['l1', 'l2'],'C':[0.001,.009,0.01,.09,1,5,10,25], 'class_weight' : ['None', 'balanced']}\nlrn_clf_grid = GridSearchCV(lrn_clf, param_grid = grid_values,scoring = 'recall')\nlrn_clf_grid.fit(X_train, y_train)","8eba296d":"y_lrn_pred_grid = lrn_clf_grid.predict(X_test)","d2a64690":"print(\"Precision: {}\".format(precision_score(y_test, y_lrn_pred_grid)))\nprint(\"Recall: {}\".format(recall_score(y_test, y_lrn_pred_grid)))\nprint(\"f1_score: {}\".format(f1_score(y_test,y_lrn_pred_grid)))\nprint(\"Accuracy: {}\".format(accuracy_score(y_test, y_lrn_pred_grid)))","33747234":"## We can see that with grid search we obtain slight improvment in all the above parameters.","43b066bf":"lrn_clf_grid.best_params_","8a03dbfa":"rf_clf = RandomForestClassifier(random_state = 42)","c16682b1":"rf_clf.fit(X_train, y_train)","ec156b9c":"y_rf_pred = rf_clf.predict(X_test)","ee55e22c":"confusion_matrix(y_test, y_rf_pred)","e8bb514d":"print(\"Precision: {}\".format(precision_score(y_test, y_rf_pred)))\nprint(\"Recall: {}\".format(recall_score(y_test, y_rf_pred)))\nprint(\"f1_score: {}\".format(f1_score(y_test, y_rf_pred)))\nprint(\"Accuracy: {}\".format(accuracy_score(y_test, y_rf_pred)))","b63737aa":"# define evaluation procedure\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)","4048defc":"# evaluate model\nscores = cross_val_score(rf_clf, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)","4cb7536c":"# summarize performance\nfrom numpy import mean\n\nprint('Mean ROC AUC: %.3f' % mean(scores))","9f272d65":" pip install imbalanced-learn","cb97f55c":"from imblearn.ensemble import EasyEnsembleClassifier\nmodel = EasyEnsembleClassifier(n_estimators=10)","032d52bf":"model.fit(X_train, y_train)","b42bc030":"cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)","478dced3":"scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)","e46fc8ae":"print('Mean ROC AUC: %.3f' % mean(scores))","99af314d":"rf_clf = RandomForestClassifier(random_state = 42)\nprint(rf_clf.get_params)","2f5a8242":"## Create the random grid\n\nparam_grid = {\n   'bootstrap': [True],\n   'max_depth': [80, 90, 100, 110],\n   'max_features': [2, 3],\n   'min_samples_leaf': [3, 4, 5],\n   'min_samples_split': [8, 10, 12],\n   'n_estimators': [100, 200, 300, 1000]    }   \n\nrf_clf_rdn = RandomizedSearchCV(rf_clf, param_distributions= param_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\nrf_clf_rdn.fit(X_train, y_train)","3e586c9d":"y_rf_rdn_pred = rf_clf_rdn.predict(X_test)","e3559dde":"print(\"Precision: {}\".format(precision_score(y_test, y_rf_rdn_pred)))\nprint(\"Recall: {}\".format(recall_score(y_test, y_rf_rdn_pred)))\nprint(\"f1_score: {}\".format(f1_score(y_test, y_rf_rdn_pred)))\nprint(\"Accuracy: {}\".format(accuracy_score(y_test, y_rf_rdn_pred)))","cac54405":"confusion_matrix(y_test, y_rf_rdn_pred)","5eb70367":"## The best Recall, f1_score and acuuracy so far!","93eed4cd":"knn_clf = KNeighborsClassifier(n_neighbors=5)\nknn_clf.fit(X_train, y_train)","c2373e66":"y_knn_pred= knn_clf.predict(X_test)","2087cab0":"confusion_matrix(y_test, y_knn_pred)","703a5722":"print(\"Precision: {}\".format(precision_score(y_test, y_knn_pred)))\nprint(\"Recall: {}\".format(recall_score(y_test, y_knn_pred)))\nprint(\"f1_score: {}\".format(f1_score(y_test, y_knn_pred)))\nprint(\"Accuracy: {}\".format(accuracy_score(y_test, y_knn_pred)))","e8198cde":"knn_clf.get_params","93767576":"# Lets see if we can improve the model by grid search","55e98401":"k_range = list(range(1, 15))\nparam_grid_knn = dict(n_neighbors = k_range)\nknn_clf_grid = GridSearchCV(knn_clf, param_grid_knn, cv=10, scoring='accuracy')\nknn_clf_grid.fit(X_train, y_train)","41d207ae":"y_knn_clf_grid = knn_clf_grid.predict(X_test)","d49b75c3":"confusion_matrix(y_test,y_knn_clf_grid)","89372792":"print(\"Precision: {}\".format(precision_score(y_test, y_knn_clf_grid)))\nprint(\"Recall: {}\".format(recall_score(y_test, y_knn_clf_grid)))\nprint(\"f1_score: {}\".format(f1_score(y_test, y_knn_clf_grid)))\nprint(\"Accuracy: {}\".format(accuracy_score(y_test, y_knn_clf_grid)))","3c3a7e4a":"knn_clf_grid.best_params_","9080c146":"## GridSeacrh Improved the results , but the random forest model with Randomized Search is still the best model!","b5f2c25c":"## Randomized Search for Hyperparameter Tuning of Random Forest Model","cced351d":"# Data Exploration","58838988":"# **Data Transformation**","cc0b3fa6":"**Feature: Ever Intubated**","37e213b2":"**Feature: Neighbourhood Name**","efa1ea5f":"### KNN Algorithm","9b127d5d":"# **Models: Hyper Paremeter Tuning \/ Performance \/ Results**","c8cfb3ac":"**Feature: FSA**","2db07d4b":"**Feature: Classification**","12491573":"# Background:\n\n* Determine the case demographics and other public health characteristics within the data which may contribute to a fatal outcome of a COVID-19 infection in Toronto, Canada.\n* Engineer new features using the existing data set which allows more granular insight to be unlocked\n* Develop models capable of predicting health outcomes, which can be optimized and maintained throughout the pandemic","2418808c":"**Feature: Client Gender**","3119105c":"**Feature Engineering: FSA**","f19d9485":"**Feature: Ever Hospitalized**","9f3a1198":"**Feature: Source of Infection**","f926bffa":"**Feature: Currently Hospitalized**","79ede00b":"# Metadata Columns","452182bc":"**Dropping Columns**","0d1868dd":"### Logistic Regression on the balanced dataset","4fe6fc08":"**Feature: Episode Date**","78955e37":"**Feature: Currently in ICU**","b85be885":"### Resampling Method - Undersampling","64651f5b":"**Feature: Ever in ICU**","6ec46285":"**Feature Engineering: Episode Date**","978ee001":"**Feature: Reported Date**","8e10ea6f":"# **Data Preparation**","8884b54a":"### Easy Ensemble","1381c3c0":"**Feature: Currently Intubated**","b7842d3e":"# Target Variable: Outcome","215dfa96":"**One Hot Encoding**","e07422ad":"# Dataset:\n\nThe data set originates from Toronto Public Health and is refreshed weekly on Wednesdays. It contains records for each reported COVID-19 infection within Toronto since the beginning of the pandemic, consisting of case demographics, level of care, case outcome and date information. https:\/\/open.toronto.ca\/dataset\/covid-19-cases-in-toronto\/","b236751f":"# Methodology\n\n# Data Cleaning\n* \u201cOutcome\u201d was selected as the target variable and had certain values combined together to separate into Fatal vs. Non-fatal to create boolean result\n* Geographic categorical variable FSA had values combined to represent larger geographic areas and associated null values converted to represent a larger geographic region\n* Ordinal variable Age Group contained a small number (< 0.2%) of null values where the majority of cases were unconfirmed and where zero fatalities occurred; these values were dropped\n\n# Feature Engineering and Transformation\n* Generated new features using the date of reported infection and date of presumed infection\n* Used StandardScaler for engineered features and One Hot Encoding for categorical features","984244c7":"**Feature Engineering: date_diff**","b3414724":"### Random Forest Classifier","1bb0ac5d":"# Feature: Outbreak Associated","44e7d3c3":"**Scaling**","ac8d0a61":"**Feature Engineering: curr_date_diff**","8e34f760":"**Cleaning and Combining of values**","664a9b27":"**Feature: Age Group**"}}