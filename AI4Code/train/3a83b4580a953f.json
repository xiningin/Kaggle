{"cell_type":{"f10e3f9e":"code","28383def":"code","10228734":"code","77ca3813":"code","2db7f9c7":"code","82f6c563":"code","6f87fe7d":"code","84be7990":"code","b91a838a":"code","96e5f90e":"code","c6aa4d2f":"code","e1b360ed":"code","e549d683":"code","25d4013f":"code","88293997":"code","11b0e382":"code","53e1e637":"code","66ce966a":"code","d09ab7a9":"markdown","459b3b3e":"markdown","472c4cdb":"markdown","6bcaeae0":"markdown","9e3dc9c5":"markdown","65d19b53":"markdown","0370b70c":"markdown","0bb02e6d":"markdown"},"source":{"f10e3f9e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom keras.utils.np_utils import to_categorical\nimport matplotlib.pyplot as plt\nfrom pandas import read_csv, DataFrame\nimport math\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout,Flatten, Reshape, Activation\nfrom keras.layers import LSTM\nfrom keras.layers import noise\nfrom keras.models import load_model\nfrom keras import optimizers\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import f1_score, accuracy_score\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","28383def":"#df_1=pd.read_csv('..\/input\/2chan10dbnoise\/output3.csv',header=None,nrows=1000)\ndf=pd.read_csv('..\/input\/liverpool-ion-switching\/train.csv')\ndataset=df.values\nprint(dataset[0:20,:])","10228734":"categorical_labels = to_categorical(dataset[:,2], num_classes= 11)\nprint(categorical_labels.shape)","77ca3813":"#does this help?\nscaler = MinMaxScaler(feature_range=(0, 1))\ndataset[:,0:2] = scaler.fit_transform(dataset[:,0:2])\nprint(dataset[:10,:])","2db7f9c7":"from keras import backend as K\n\ndef recall_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\ndef precision_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    \ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","82f6c563":"batch_size=100\nmodel = Sequential()\ntimestep=1\ninput_dim=1\nmodel.add(LSTM(64, batch_input_shape=(batch_size, timestep, input_dim), stateful=True, return_sequences=True))\nmodel.add(Flatten())\nmodel.add(Dense(11))\nmodel.add(Activation('softmax'))\n#binary sinks like a stone! b\/c not binary @@\n#model.compile(loss='binary_crossentropy', optimizer=optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True), metrics=['accuracy'])\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True), metrics=[f1_m])\n\nprint(model.summary())","6f87fe7d":"import math\ntrain_size = math.floor(len(dataset) * 0.80\/100)\ntrain_size = int (train_size*100)\ntest_size = math.floor((len(dataset) - train_size)\/100)\ntest_size = int(test_size*100)\nprint ('training set= ',train_size)\nprint('test set =', test_size)\nprint ('total length', test_size+train_size)\nprint ('Dataset= ', len(dataset))","84be7990":"in_train, in_test = dataset[0:train_size,1], dataset[train_size:train_size+test_size,1]\ntarget_train, target_test = categorical_labels[0:train_size,:], categorical_labels[train_size:train_size+test_size,:]\n\nin_train = in_train.reshape(len(in_train),1,1)\nin_test = in_test.reshape(len(in_test), 1,1)\n\nprint('in_train Shape',in_train.shape)\nprint(in_train[0:2,:])\nprint('target_train Shape',target_train.shape)\nstate=np.argmax(target_train,axis=-1)\nprint(state)\n\nprint('in_test Shape',in_test.shape)\nprint(in_test[0:2,:])\nprint('target_test Shape',in_test.shape)\nstate=np.argmax(target_test,axis=-1)\nprint(state)","b91a838a":"epochers=3\nhistory=model.fit(x=in_train,y=target_train, initial_epoch=0, epochs=epochers, batch_size=batch_size, verbose=2, shuffle=False)","96e5f90e":"plt.plot(history.history['f1_m'])","c6aa4d2f":"predict = model.predict(in_train, batch_size=batch_size)\nprint(predict.shape)\nprint(predict[:5,:])","e1b360ed":"state_train=np.argmax(target_train,axis=-1)\nclass_predict_train=np.argmax(predict,axis=-1)\nprint(state_train[:20])\nprint(class_predict_train[:20])","e549d683":"print('F1_macro = ',f1_score(state_train,class_predict_train, average='macro'))","25d4013f":"plotlen=test_size\nstarting_point = 15000\nlenny=1000\n#target_test = dataset[train_size:len(dataset),3]\n#target_test = target_test.reshape(plotlen, 1)\nplt.figure(figsize=(30,6))\nplt.subplot(2,1,1)\n#temp=scaler.inverse_transform(dataset)\n#plt.plot (temp[train_size:len(dataset),1], color='blue', label=\"some raw data\")\nplt.plot (dataset[starting_point:starting_point+lenny,1], color='blue', label=\"some raw data\")\nplt.title(\"The raw test\")\ndf=DataFrame(dataset[starting_point:starting_point+lenny,1])\nplt.subplot(2,1,2)\n#plt.plot(target_test.reshape(plotlen,1)*maxchannels, color='black', label=\"the actual idealisation\")\nplt.plot(state_train[starting_point:starting_point+lenny], color='black', label=\"the actual idealisation\")\n#plt.plot(spredict, color='red', label=\"predicted idealisation\")\nline,=plt.plot(class_predict_train[starting_point:starting_point+lenny], color='red', label=\"predicted idealisation\")\nplt.setp(line, linestyle='--')\nplt.xlabel('timepoint')\nplt.ylabel('current')\n#plt.savefig(name)\nplt.legend()\nplt.show()","88293997":"predict_test = model.predict(in_test, batch_size=batch_size)\nprint(predict_test.shape)\nprint(predict_test[:5,:])","11b0e382":"state_test=np.argmax(target_test,axis=-1)\nclass_predict=np.argmax(predict_test,axis=-1)\nprint(state[:20])\nprint(class_predict[:20])","53e1e637":"print('F1_macro = ',f1_score(state,class_predict, average='macro'))","66ce966a":"plotlen=test_size\nlenny=1000\n#target_test = dataset[train_size:len(dataset),3]\n#target_test = target_test.reshape(plotlen, 1)\nplt.figure(figsize=(30,6))\nplt.subplot(2,1,1)\n#temp=scaler.inverse_transform(dataset)\n#plt.plot (temp[train_size:len(dataset),1], color='blue', label=\"some raw data\")\nplt.plot (dataset[train_size:train_size+lenny,1], color='blue', label=\"some raw data\")\nplt.title(\"The raw test\")\ndf=DataFrame(dataset[train_size:train_size+lenny,1])\nplt.subplot(2,1,2)\n#plt.plot(target_test.reshape(plotlen,1)*maxchannels, color='black', label=\"the actual idealisation\")\nplt.plot(state[0:lenny], color='black', label=\"the actual idealisation\")\n#plt.plot(spredict, color='red', label=\"predicted idealisation\")\nline,=plt.plot(class_predict[:lenny], color='red', label=\"predicted idealisation\")\nplt.setp(line, linestyle='--')\nplt.xlabel('timepoint')\nplt.ylabel('current')\n#plt.savefig(name)\nplt.legend()\nplt.show()","d09ab7a9":"in_train, in_test = dataset[0:train_size,1], dataset[train_size:len(dataset),1]\ntarget_train, target_test = categorical_labels[0:train_size,:], categorical_labels[train_size:len(dataset),:]\nin_train = in_train.reshape(len(in_train),1,1)\nin_test = in_test.reshape(len(in_test), 1,1)\nb=np.zeros([len(in_train),1,3])\nb[:,0,0]=in_train[:,0,0]\nin_train=b\nprint('in_train Shape',in_train.shape)\nprint('target train shape',target_train.shape)\nb=np.zeros([len(in_test),1,3])\nb[:,0,0]=in_test[:,0,0]\nin_test=b\nprint('in_test Shape',in_test.shape)\nprint('target_test Shape',in_test.shape)","459b3b3e":"Second job turn the current channel labels (which are the actual channel amplitudes in pA) into simpler classifier labels 0, 1 or 2.","472c4cdb":"# Channel LSTM Model\n\n### I have taken the kernel provided by [richardbj](https:\/\/www.kaggle.com\/richardbj) and modified it to use the current competition dataset. Original kernel [here](https:\/\/www.kaggle.com\/richardbj\/channel-lstm-101)","6bcaeae0":"### Training Data (First 80% of train)","9e3dc9c5":"First job; load the data","65d19b53":"### Validation Hold Out Data (Last 20% of train)","0370b70c":"# Validation","0bb02e6d":"#### Observations from Validation:\n- The Model does not do very well in terms of responding to signal noise. Example, We get a lot of misclassifications when the true target is 0 for long stretches in time, yet our prediction bounces back and forth between 0 and 1.\n- A better Validation scheme should be pursued instead of simply cutting off the last chunk of the training data to use for validation. There is a lot of data in the last part of the training data that the model hasn't seen yet, therefore it responds poorly to the validation set. "}}