{"cell_type":{"1d173885":"code","e06147ae":"code","1e95d993":"code","d7bd24fe":"code","aa8f4381":"code","c04e8b31":"code","685858e7":"code","19c620dc":"code","890c108c":"code","d98695e8":"code","c6013bc0":"code","a5cce5b2":"code","bf0ef8e0":"code","bf2f3a67":"code","f09ba30b":"markdown"},"source":{"1d173885":"import pandas as pd\ndf=pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndf.head()","e06147ae":"import numpy as np\ndf['Glucose']=np.where(df['Glucose']==0,df['Glucose'].median(),df['Glucose'])\ndf.head()","1e95d993":"X=df.drop('Outcome',axis=1)\ny=df['Outcome']","d7bd24fe":"pd.DataFrame(X,columns=df.columns[:-1])","aa8f4381":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=0)","c04e8b31":"from hyperopt import hp,fmin,tpe,STATUS_OK,Trials","685858e7":"space = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\n        'max_depth': hp.quniform('max_depth', 10, 1200, 10),\n        'max_features': hp.choice('max_features', ['auto', 'sqrt','log2', None]),\n        'min_samples_leaf': hp.uniform('min_samples_leaf', 0, 0.5),\n        'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),\n        'n_estimators' : hp.choice('n_estimators', [10, 50, 300, 750, 1200,1300,1500])\n    }","19c620dc":"space","890c108c":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\ndef objective(space):\n    model = RandomForestClassifier(criterion = space['criterion'], max_depth = space['max_depth'],\n                                 max_features = space['max_features'],\n                                 min_samples_leaf = space['min_samples_leaf'],\n                                 min_samples_split = space['min_samples_split'],\n                                 n_estimators = space['n_estimators'], \n                                 )\n    \n    accuracy = cross_val_score(model, X_train, y_train, cv = 5).mean()\n\n    # We aim to maximize accuracy, therefore we return it as a negative value\n    return {'loss': -accuracy, 'status': STATUS_OK }","d98695e8":"from sklearn.model_selection import cross_val_score\ntrials = Trials()\nbest = fmin(fn= objective,\n            space= space,\n            algo= tpe.suggest,\n            max_evals = 80,\n            trials= trials)\nbest","c6013bc0":"\ncrit = {0: 'entropy', 1: 'gini'}\nfeat = {0: 'auto', 1: 'sqrt', 2: 'log2', 3: None}\nest = {0: 10, 1: 50, 2: 300, 3: 750, 4: 1200,5:1300,6:1500}\n\n\nprint(crit[best['criterion']])\nprint(feat[best['max_features']])\nprint(est[best['n_estimators']])","a5cce5b2":"best['min_samples_leaf']","bf0ef8e0":"from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\ntrainedforest = RandomForestClassifier(criterion = crit[best['criterion']], max_depth = best['max_depth'], \n                                       max_features = feat[best['max_features']], \n                                       min_samples_leaf = best['min_samples_leaf'], \n                                       min_samples_split = best['min_samples_split'], \n                                       n_estimators = est[best['n_estimators']]).fit(X_train,y_train)\npredictionforest = trainedforest.predict(X_test)\nprint(confusion_matrix(y_test,predictionforest))\nprint(accuracy_score(y_test,predictionforest))\nprint(classification_report(y_test,predictionforest))\nacc5 = accuracy_score(y_test,predictionforest)","bf2f3a67":"import matplotlib.pyplot as plt\nplt.figure(figsize = (10,8))\nplt.xlabel('Log  Bins')\nplt.ylabel('Frequency')\nplt.title('Histogram of Hyperopt Solution Scores')\nplt.hist(y, 30, density=True, facecolor='r', alpha=0.75)\nplt.show","f09ba30b":"# Bayesian Optimization"}}