{"cell_type":{"383438b5":"code","6257b73d":"code","c758ed53":"code","ddb9050e":"code","94803bae":"code","1db4af0c":"code","1477ee2f":"code","a78e21c2":"code","699f9398":"code","88d1e475":"code","8db53f5c":"code","e928ea9b":"code","eee8a342":"code","6198d37d":"code","2d7bf72f":"code","e511d7fa":"code","040dd159":"code","814bdcc3":"code","f9e10a96":"code","1086e22a":"code","c8b1f6a3":"code","76af278c":"code","ff1cd6b0":"code","2e89d9eb":"code","678a760d":"code","df1384c5":"code","e0f455ae":"code","d5f0b87a":"code","d3958f2e":"code","3b3e6a8b":"code","94b10e4b":"code","470fd51f":"code","15c89dc1":"code","2cac5b3f":"code","021a905d":"code","5cf6de6f":"code","c480e477":"code","c84dfd29":"code","b6c8cdd6":"code","1478e698":"code","d91c7157":"code","630b19a8":"code","8071388f":"code","f16f1c57":"code","13a19850":"code","e089a6c9":"code","b0725f24":"code","d5b40757":"code","6cc16102":"code","a6eabb6b":"code","7137bbf5":"code","200b43ae":"code","b6aa95d6":"code","72b296c3":"code","4284d066":"code","ef95b8d9":"code","6e53ca58":"code","ddcf27d0":"code","a056b1a9":"code","e8598c84":"code","61ae31a1":"code","4c99234c":"code","20fd4dda":"code","8aa8990e":"code","457dff53":"code","8a96e075":"code","b5accc3c":"code","309c8aaf":"code","a2054bd3":"code","c910da60":"code","3ed99ccb":"code","93453393":"code","491afe3b":"code","74b3cfab":"code","881c89f8":"markdown","da2530bd":"markdown","c160bcb5":"markdown","6c7b02e7":"markdown","496acd4e":"markdown","6cd7185e":"markdown","590009d1":"markdown","804b71e5":"markdown","805ada3a":"markdown","3b2f6783":"markdown","716005c7":"markdown","9a2fefb2":"markdown","2669add2":"markdown","888d7acf":"markdown","5b216c85":"markdown","be0d6b9f":"markdown","b195d068":"markdown","cf6b35f3":"markdown","4d57cddf":"markdown","8ede14e9":"markdown","76c6349c":"markdown","0ad38378":"markdown","bcffe027":"markdown","c5aada3e":"markdown","d0f53d58":"markdown","9beb643f":"markdown","ad87f0ab":"markdown","76a34ce5":"markdown","01e8b568":"markdown","4dc17b53":"markdown","b27b6349":"markdown","3d0e70d0":"markdown","de997afe":"markdown","4d592605":"markdown","0217d8bb":"markdown","544e167a":"markdown","723006bc":"markdown","ef3465a7":"markdown","f35629aa":"markdown","f0b849b0":"markdown","4f9d4738":"markdown","63b8a2cf":"markdown","7f30f4d6":"markdown","6c5cd769":"markdown","b64d9360":"markdown","a17091d4":"markdown","62700a8e":"markdown","5a3a7ad0":"markdown","ec46765b":"markdown","5b8a4363":"markdown","4f6b40cd":"markdown","1cf57821":"markdown","3dc26f07":"markdown","fe491a75":"markdown","aeef794e":"markdown","7c0e5127":"markdown","8e7800eb":"markdown","e493a041":"markdown","13b88f40":"markdown","1a7e0ec8":"markdown","48f2fa30":"markdown","f3dd8565":"markdown","7db9963e":"markdown","1e8db7ff":"markdown","ab4ab4dd":"markdown","5a4651e5":"markdown","be4df571":"markdown","4427facf":"markdown"},"source":{"383438b5":"#whyempterminated.show()\npy.offline.iplot(whyempterminated)","6257b73d":"#org_position_proportion.show()\npy.offline.iplot(org_position_proportion)","c758ed53":"# import main libs\nimport os \nimport pandas as pd\nimport numpy as np\nfrom pandas import DataFrame as DFM\nimport warnings\nwarnings.filterwarnings('ignore')","ddb9050e":"# import viz libs\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport matplotlib.style\nimport matplotlib as mpl\nmpl.style.use('bmh')\nmpl.rcParams['figure.figsize'] = [12.0, 6.0]\nmpl.rcParams['figure.dpi'] = 96\nplotCOLOR = 'slategrey'\nmpl.rcParams['text.color'] = plotCOLOR\nmpl.rcParams['axes.labelcolor'] = plotCOLOR\nmpl.rcParams['xtick.color'] = plotCOLOR\nmpl.rcParams['ytick.color'] = plotCOLOR\nmpl.rcParams['axes.labelsize'] = 15","94803bae":"from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly as py\nimport plotly.express as px\nimport plotly.graph_objects as go\n\ncolor1 = px.colors.qualitative.Prism[0]\ncolor2 = px.colors.qualitative.Prism[7]\ncolor3 = px.colors.qualitative.Prism[10]\ncolor4 = px.colors.qualitative.Set1[8]\n\ninit_notebook_mode(connected=True) ","1db4af0c":"# enlarge the notebook\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:100% !important; } <\/style>\"))\n#------------------------------------------------------------------------","1477ee2f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a78e21c2":"# import file into a dataframe\nfiledir = '\/kaggle\/input\/hr-data-set-based-on-human-resources-data-set\/HR DATA.txt'\nrawdata = pd.read_csv(filedir, sep='\\t', parse_dates=True)","699f9398":"def dispdfm(x):\n    x = display(pd.DataFrame(x))\n    return x","88d1e475":"def plotol(x):\n    py.offline.iplot(x)","8db53f5c":"# print initial rows and cols of dataset\ndispdfm(rawdata.sample(3))","e928ea9b":"# Review basic information on the dataset\nrawdata.info()","eee8a342":"# WORKOUT NULL PERCENTAGES\ncols = ['Attr', 'Len', 'Nullcnt', 'Nullpct']\ndata = []\nfor i in rawdata.columns:\n    attr        = i\n    length      = int(len(rawdata[i]))\n    attrnulls   = rawdata[i].isna().sum()\n    attrnullpct = round((rawdata[i].isna().sum())\/len(rawdata[i])*100,2)\n    values = [\n      attr, \n      length,\n      attrnulls,\n      attrnullpct\n    ]\n    zipped = zip(cols, values)\n    nulldict = dict(zipped)\n    data.append(nulldict)\n\n#     print(values)\n#     nulldf.append(pd.Series(data), ignore_index=True)\n\nnulldf = pd.DataFrame(columns=cols).append(data)\ndisplay(nulldf[nulldf['Nullpct'] != 0].sort_values('Nullpct', ascending=False))","6198d37d":"# Graph missing values\nplotdf = nulldf[nulldf['Nullpct'] != 0].sort_values('Nullpct', ascending=False)\nfig = px.histogram(\n    plotdf,\n    y='Nullpct',\n    x='Attr',\n    color='Nullpct',\n    color_discrete_sequence=px.colors.qualitative.Prism,\n    title='Null Values'\n)\n\nplotol(fig)","2d7bf72f":"# Reasons for leaving based on the employmentstatus column\nplotdf = DFM(rawdata[(rawdata['EmploymentStatus'] == 'Terminated for Cause') | \n        (rawdata['EmploymentStatus'] == 'Voluntarily Terminated')]['TermReason'].value_counts())\nfig, axs = plt.subplots(figsize=(2, 6))\n#------------------------------------------------------------------------\nfig = sns.heatmap(\n    plotdf,\n    cbar=True,\n    #square= True,\n    fmt='.0f',\n    annot=True,\n    annot_kws={\n        'size': 10,\n        'rotation': 0\n    },\n    cmap='Purples',\n    ax=axs)\nwhyemployeesleave = fig\n#------------------------------------------------------------------------\naxs.set(ylabel=\"Reasons \",xlabel=\"Count\")\nplt.title('Chart 1 - Reasons for Leaving \\n')\nplt.tight_layout()","e511d7fa":"# Reasons for leaving based on the termd attribute\nplotdf = DFM(rawdata[rawdata['Termd'] == 1]['TermReason'].value_counts())\nfig, axs = plt.subplots(figsize=(2, 6))\n#------------------------------------------------------------------------\nfig = sns.heatmap(\n    plotdf,\n    cbar=True,\n    #square= True,\n    fmt='.0f',\n    annot=True,\n    annot_kws={\n        'size': 10,\n        'rotation': 0\n    },\n    cmap='Purples',\n    ax=axs)\n#------------------------------------------------------------------------\naxs.set(ylabel=\"Reasons \",xlabel=\"Count\")\nplt.title('Chart 2 - Reasons for Leaving \\n')\nplt.tight_layout()","040dd159":"termcolrev = rawdata[(rawdata['Termd'] == 1)\n        & ~(rawdata['TermReason'] == 'N\/A - still employed')\n        & (rawdata['DateofTermination'].isnull()\n           )][[#'TermReason','DateofTermination',\n               'Termd', 'EmploymentStatus']]#['EmploymentStatus'].value_counts()\n\ndispdfm(termcolrev.tail(5))","814bdcc3":"from datetime import datetime\nnow = datetime.now()","f9e10a96":"# Setting a list of categorial features\/attributes\ncatcols = []\nfor x in rawdata.columns:\n    if rawdata[x].dtype == 'object':\n        #print(\"%s %s %s \" % (x,'\\t', rawdata[x].dtype))\n        rawdata[x] = rawdata[x].astype('category')\n        #print(\"%s %s %s \" % (x,'\\t', rawdata[x].dtype))\n        #display(pd.DataFrame(rawdata[x].value_counts()))\n        #print('\\n'*2)\n        catcols.append(x)\nprint(\"Each attribute has the following amount of unique values:  \")\nfor i, v in enumerate(catcols):\n    print('  ',str(i).ljust(2) , v.ljust(28),\"Values:\", len(set(rawdata[v])))\n#print(catcols, sep='\\t')","1086e22a":"# Replacing Null Values\nrawdata['TermReason'] = rawdata['TermReason'].cat.add_categories('Unknown')\nrawdata['TermReason'].fillna('Unknown', inplace=True)\nrawdata['ManagerName'] = rawdata['ManagerName'].cat.add_categories('Unknown')\nrawdata['ManagerName'].fillna('Unknown', inplace=True)","c8b1f6a3":"# Transform dates\ndates = ['DOB', 'DateofHire', 'DateofTermination', 'LastPerformanceReview_Date']\ndisplay(rawdata[dates].sample(3),\n        rawdata[dates].dtypes)","76af278c":"# Transform date types\nfor i in dates:\n    rawdata[i] = pd.to_datetime(rawdata[i], \n                                errors = 'coerce',\n                                infer_datetime_format=False,\n                                format='%d-%m-%y'\n                               )","ff1cd6b0":"# setting dob attribute\ncurrentyear = now.year\ncurrentyear\nrawdata['doby'] = ''\n#------------------------------------------------------------------------\nfor idx, date in enumerate(rawdata['DOB']):\n#     print(type(date))\n    if date.year >= currentyear:\n        rawdata['doby'][idx] = rawdata['DOB'][idx] - pd.DateOffset(years=100)\n        #print('greater',idx, date)\n    else:\n        rawdata['doby'][idx] = rawdata['DOB'][idx]\n        #print('normal',idx, date)\n#------------------------------------------------------------------------\nprint('Before:', rawdata['DOB'][138])        \nrawdata['DOB'] = pd.to_datetime(rawdata['doby'], format='%y-%m-%d')\nprint('After:', rawdata['DOB'][138])\n#------------------------------------------------------------------------","2e89d9eb":"dupnames = ['Warner, Larissa', 'Young, Darien']","678a760d":"dispdfm(rawdata[rawdata['Employee_Name'].isin(dupnames)][['Employee_Name', 'EmpID','DOB', ]])","df1384c5":"# Creating a binary churn column\nrawdata['Churn'] = np.where(rawdata['DateofTermination'].notna() == True,1,0)\n#------------------------------------------------------------------------\n# Cross checking churn against Employment Status\npd.crosstab(rawdata['Churn'], rawdata['EmploymentStatus'])\n#------------------------------------------------------------------------\n# Creating an AGE column\nrawdata['Age'] = (now.year - rawdata['DOB'].dt.year)","e0f455ae":"dispdfm(rawdata[['Churn', 'Age']].sample(3))","d5f0b87a":"rawdata['Churn-Yes\/No'] = rawdata['Churn'].astype('category')\nrawdata['Churn-Yes\/No'].replace({1:'Yes', 0:'No'}, inplace=True)","d3958f2e":"# Additional dropped columns\nrawdata.drop(['Employee_Name', 'EmpID',], inplace=True, axis=1)\nrawdata.drop(['Termd'], inplace=True, axis=1)","3b3e6a8b":"# Dropping certain columns with high null count\nrawdata.drop(['doby'], inplace=True, axis=1)\nrawdata.drop(['Zip', 'LastPerformanceReview_Date','DaysLateLast30'], inplace=True, axis=1)","94b10e4b":"# Cross checking churn against Employment Status - BEFORE\ndispdfm(pd.crosstab(rawdata['Churn'], rawdata['EmploymentStatus']))\n#------------------------------------------------------------------------\n# Dropping employees that have not started working yet\nrawdata.drop(rawdata[rawdata['EmploymentStatus']=='Future Start'].index, axis=0, inplace=True)","470fd51f":"#\nrawdata_copy1 = rawdata.copy()\ndropcols = ['DateofTermination','ManagerID', 'DOB']\nrawdata_copy1['Churn-Yes\/No'] = rawdata_copy1['Churn-Yes\/No'].astype('category')\nrawdata_copy1.drop(dropcols,\n              axis=1, inplace=True)\nfirst_col = rawdata_copy1.pop('Churn')\nrawdata_copy1.insert(len(rawdata_copy1.columns), 'Churn', first_col)","15c89dc1":"# Imbalance correction - Synthetic Minority Oversampling \nfrom imblearn.over_sampling import SMOTE, SMOTENC\nfrom sklearn.model_selection import train_test_split\n# Resample the minority class. You can change the strategy to 'auto' if you are not sure.\ntest = DFM([rawdata_copy1.dtypes == 'category']).T.reset_index()\ncatlistsmotenc = list(test[test[0]==True]['index'])\nsmnc = SMOTENC(categorical_features=[rawdata_copy1.dtypes == 'category'],\n               sampling_strategy='minority',\n               random_state=10)\n' - >  -  -  -  -  -  -  -  -  -  -  -  -  < - '\nX, y = rawdata_copy1.drop('Churn', axis=1), rawdata_copy1['Churn']\n' - >  -  -  -  -  -  -  -  -  -  -  -  -  < - '\n# run the train\/test split\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.2,\n                                                    stratify=y)\n' - >  -  -  -  -  -  -  -  -  -  -  -  -  < - '\n# Fit the model to generate the data.\noversampled_trainX, oversampled_trainY = smnc.fit_sample(X_train, y_train)\n' - >  -  -  -  -  -  -  -  -  -  -  -  -  < - '\nos_rawdata = pd.concat(\n    [pd.DataFrame(oversampled_trainX),\n    pd.DataFrame(oversampled_trainY)],\n    axis=1)\n' - >  -  -  -  -  -  -  -  -  -  -  -  -  < - '\n#os_modeldata.columns = modeldata.columns\nos_rawdata.columns = rawdata_copy1.copy().columns","2cac5b3f":"# create modeldata dataframe\nmodeldata = os_rawdata.copy()\n#------------------------------------------------------------------------\n# drop na values\ndropNaValCols = [\n    'ManagerID'\n]\n#------------------------------------------------------------------------\nfor col in dropNaValCols:\n    if col in modeldata.columns:\n        print('Dropped NA Values:', col)\n        modeldata.dropna(subset=dropNaValCols, inplace=True)","021a905d":"# any additional cols that need to be dropped\nmodelcolstodrop = [\n    'EmpStatusID',\n    'EmploymentStatus',\n    'TermReason',\n    'ManagerID',\n    'PositionID',\n    'ManagerID',\n    'DOB',\n    'DateofHire',\n    'DateofTermination',\n    'DeptID',\n    'PerfScoreID',\n    'Churn-Yes\/No',\n    'GenderID',\n]\n\nfor col in modelcolstodrop:\n    if col in modeldata.columns:\n        print('Dropped Column:', col)\n        modeldata.drop(col, axis=1, inplace=True)\n#print(modeldata.columns)","5cf6de6f":"# Setting a list of categorial features\/attributes\nmodelcatcols = []\nfor x in modeldata.columns:\n    if str(modeldata[x].dtype) == 'category':\n        #modeldata[x].astype('category')\n        modelcatcols.append(x)\nfor i, v in enumerate(modelcatcols):\n    print(i, v)","c480e477":"# Apply label encoder to each column with categorical data\nfrom sklearn.preprocessing import LabelEncoder\n# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\n#print(' - - Encoded Values - - ')\nfor col in modelcatcols:\n    print(\"Encoded: \", col)\n    modeldata[col] = label_encoder.fit_transform(modeldata[col])","c84dfd29":"# Review Correlation of covariates\/explanatory variables\ncordf = modeldata\ncordf = cordf.corr()\n#------------------------------------------------------------------------\nfig, axs = plt.subplots(figsize=(10,7))\ncordf_subset = pd.DataFrame(cordf).drop('Churn', axis=0)\ncordf_subset = cordf_subset.drop('Churn', axis=1)\nmask = np.triu(np.ones_like(cordf_subset, dtype=np.bool))\n#------------------------------------------------------------------------\nfig = sns.heatmap(\n    cordf_subset,\n    cbar=False,\n    square= True,\n    fmt='.2f',\n    annot=True,\n        annot_kws={\n        'size': 5},\n    cmap='Purples',\n    mask=mask,\n    #vmin=-1, vmax=1, center= 0,\n    ax=axs)\n\nplt.title('Correlations - Final Set of Model Data \\n',\n         size=20)\nplt.tight_layout()","b6c8cdd6":"# Review Correlation of covariates\/explanatory variables\ncordf = modeldata\ncordf = cordf.corr()\ncordf_subset = pd.DataFrame(cordf.loc[:, 'Churn']).sort_values('Churn')\ncordf_subset.drop('Churn', axis=0, inplace=True)\n#------------------------------------------------------------------------\nfig, axs = plt.subplots(figsize=(1, 6))\nfig = sns.heatmap(\n    cordf_subset,\n    cbar=True,\n    #square= True,\n    fmt='.2f',\n    annot=True,\n    annot_kws={\n        'size': 8,\n        'rotation': 0,\n        #'color': 'slategrey'\n    },\n    cmap='Purples',\n    ax=axs)\nplt.title('Correlations - Explanitory v Response \\n')\nplt.tight_layout()","1478e698":"# \nplotdata = pd.pivot_table(\n\trawdata,\n    values = ['PayRate'],\n    index = 'Position',\n    aggfunc = np.median\n).sort_values('PayRate')\n\nfig, axs = plt.subplots(figsize=(1, 12))\nfig = sns.heatmap(\n    plotdata,\n    cbar=True,\n    #square= True,\n    fmt='.2f',\n    annot=True,\n    annot_kws={\n        'size': 8,\n        'rotation': 0,\n        #'color': 'slategrey'\n    },\n    cmap='Purples',\n    ax=axs)\n\nplt.title('Payrate (Median) - by Position \\n')\nplt.tight_layout()","d91c7157":"#\nplotdata = pd.pivot_table(rawdata,\n                          values=['PayRate'],\n                          index='PerformanceScore',\n                          aggfunc=np.median)\n\nfig, (axs1) = plt.subplots(1, figsize=(1, 6), dpi=96)\ng1 = sns.heatmap(\n    plotdata,\n    cbar=True,\n    #square= True,\n    fmt='.2f',\n    annot=True,\n    annot_kws={\n        'size': 8,\n        'rotation': 0,\n        #'color': 'slategrey'\n    },\n    cmap='Purples',\n    ax=axs1)\nplt.title('Payrate (Median) - by PerformanceScore \\n')\nplt.tight_layout()","630b19a8":"#\nplotdata = pd.pivot_table(rawdata,\n                          values=['PayRate'],\n                          index='Department',\n                          aggfunc=np.median)\n\nfig, axs1 = plt.subplots(figsize=(1, 6),dpi=96)\ng2 = sns.heatmap(\n    plotdata,\n    cbar=True,\n    #square= True,\n    fmt='.2f',\n    annot=True,\n    annot_kws={\n        'size': 8,\n        'rotation': 0,\n        #'color': 'slategrey'\n    },\n    cmap='Purples',\n    ax=axs1)\n\nplt.title('Payrate (Median) - by Department \\n')\n\nplt.tight_layout()","8071388f":"# correlation matrix of initial numerical values\ncols = [\n    'FromDiversityJobFairID', 'PayRate', 'EmpSatisfaction',\n    'SpecialProjectsCount', 'Age', 'Churn'\n]\n#------------------------------------------------------------------------\ncordf = rawdata[cols]\ncordf = cordf.corr()\n#------------------------------------------------------------------------\nmask = np.triu(np.ones_like(cordf, dtype=np.bool))\n#------------------------------------------------------------------------\nplt.figure(figsize=(8, 8))\nsns.heatmap(cordf,\n            cbar=True,\n            square=True,\n            mask=mask,\n            fmt='.1f',\n            annot=True,\n            annot_kws={'size': 15},\n            cmap='Purples')\nplt.title('Correlation of Initial Numerical Values')\nplt.tight_layout()","f16f1c57":"# \nprint(\n    'Meta Stats:')\ndisplay(rawdata.describe().T)","13a19850":"dispdfm(rawdata.var().sort_values())","e089a6c9":"# why do employees get terminated\ntermreason = rawdata[rawdata['TermReason'] != 'N\/A - still employed' ]\ntermreason = termreason.groupby('TermReason')['Churn'].count().reset_index()\ntermreason.sort_values('Churn', ascending=False, inplace=True)\n\nwhyempterminated = px.histogram(\n    termreason.iloc[:10],\n    x='TermReason',\n    y='Churn',\n    color='TermReason',\n    opacity=0.8,\n    color_discrete_sequence=px.colors.qualitative.Prism,\n    #color_discrete_sequence=px.colors.diverging.PuOr,\n    #color_discrete_sequence=px.colors.sequential.Plasma,\n    title='Why do employees Churn? (Top 10)',\n)  #.update_xaxes(        categoryorder='total descending')\n\nplotol(whyempterminated)\n","b0725f24":"# Plotting currently 'active' employee position numbers - top 5 by percent of staff\nplotdata = rawdata[rawdata['EmploymentStatus'] == 'Active']['Position'].value_counts(ascending=False)\nplotdata = plotdata.reset_index().rename(columns={'index':'Position', 'Position': 'Position Count'})\n#plotdata = rawdata[rawdata['EmploymentStatus'] == 'Active']['Position'].value_counts(ascending=False).reset_index()\nfig = px.bar(plotdata,\n             x='Position',\n             y='Position Count',\n             color='Position',\n             opacity=0.8,\n             #histfunc='sum',\n             title='What is the company makeup by position?',\n             color_discrete_sequence=px.colors.qualitative.Prism\n            )#.update_xaxes(        categoryorder='total descending')\n\nplotol(fig)","d5b40757":"# Proportion of Organization by Positions Filled\nplotdata = rawdata['Position'].value_counts().reset_index()\nplotdata['Pct'] = ((plotdata['Position'] \/ plotdata['Position'].sum()) *\n                   100).round(2)\nfig = px.treemap(\n    plotdata,\n    values='Position',\n    path=['Pct', 'index'],\n    color_discrete_sequence=px.colors.qualitative.Prism,\n    title='What Percent of Organization by Position Groups'\n)\n\norg_position_proportion = fig\n\n#fig.data[0].textinfo = 'label+text+value+current path'\nplotol(fig)","6cc16102":"# Department\nplotdata = rawdata[rawdata['EmploymentStatus'] == 'Active']['Department'].value_counts(ascending=False).reset_index()\npx.pie(plotdata.loc[:4], values='Department', names='index', opacity=0.8,\n       title='Proportion of Top 5 by Department',\n       color_discrete_sequence=px.colors.qualitative.Prism, hole=.5)","a6eabb6b":"# plot the state \nplotdata = rawdata[rawdata['State']!='MA']['State'].value_counts()\nplotdata = plotdata.iloc[:9]\npx.bar(plotdata, color=plotdata.index, \n       color_discrete_sequence=px.colors.qualitative.Prism, \n       opacity=0.8,\n       title='Top 10 Hiring States (Not Including MA)')\n","7137bbf5":"#\nplotdata = rawdata['State'].value_counts().reset_index()\nplotdata['Pct'] = ((plotdata['State'] \/ plotdata['State'].sum()) *\n                   100).round(2)\n\npx.treemap(plotdata,\n           path=['index', 'Pct'],\n           values='State',\n           title='Top 10 Hiring States (Does Include MA)')","200b43ae":"plotdata = rawdata.groupby('ManagerName')['Position'].count().reset_index()\nplotdata.sort_values('Position', ascending=False, inplace=True)\npx.histogram(plotdata.iloc[0:20], x='ManagerName', y='Position', color='ManagerName',\n       color_discrete_sequence=px.colors.qualitative.Prism,\n       opacity=0.8,\n       title='Top 20 Managers by Number of People Managed')\n#fig.data[0].textinfo = 'label+text+value+current path'","b6aa95d6":"# Percentages\ndispdfm((rawdata['Churn-Yes\/No'].value_counts(normalize=True)*100).round(2))","72b296c3":"#\nplotdata = rawdata.groupby('Churn-Yes\/No')['Churn'].count()\npx.bar(\n    plotdata.reset_index(),\n    x='Churn-Yes\/No',\n    y='Churn',\n    title='Number of Churn v No Churn',\n    color='Churn-Yes\/No',\n    color_discrete_sequence=[color4, color1],\n    opacity=0.8\n)\n\n","4284d066":"#\nplotdata = os_rawdata.groupby('Churn-Yes\/No')['Churn'].count()\npx.bar(\n    plotdata.reset_index(),\n    x='Churn-Yes\/No',\n    y='Churn',\n    title='Number of Churn v No Churn - Oversampled',\n    color='Churn-Yes\/No',\n    color_discrete_sequence=[color4, color1],\n    opacity=0.8\n)\n\n","ef95b8d9":"# \nplotdata = os_rawdata[os_rawdata['State'] != 'MA']\n\npx.bar(plotdata,\n             x='State',\n             color='Churn-Yes\/No',\n             color_discrete_sequence=[color4, color1],\n             barmode='stack',\n             title='Churn by State (Not in MA)',\n             opacity=0.8).update_xaxes(categoryorder='total descending')","6e53ca58":"#\nplotdata = os_rawdata.copy()\n\nbin_labels = ['0-24 ', '25-29', '30-34', '35-39', '40-44','45-49', '50-54', '55+']\nplotdata['Ages'] = pd.cut(plotdata['Age'],\n                              bins=[0, 24,30, 34,40, 44,50, 54, 100],\n                              labels=bin_labels)\n\npx.histogram(plotdata,\n             y=\"Ages\",\n             color='Churn-Yes\/No',\n             title='Churn by Age Group',\n             barnorm  ='percent', \n    color_discrete_sequence=[color4, color1],\n    opacity=0.8)\\\n    .update_xaxes(title='Percent')\\\n    #.update_yaxes(categoryorder='total descending')","ddcf27d0":"#\npx.histogram(os_rawdata,\n             y='Department',\n             color='Churn-Yes\/No',\n             title='Churn by Department',\n             barmode='stack' ,\n             barnorm  ='percent',\n             color_discrete_sequence=[color4, color1],\n             opacity=0.8)\\\n    .update_xaxes(title='Percent')\\\n    .update_yaxes(categoryorder='total descending')","a056b1a9":"#\npx.histogram(os_rawdata,\n             y='Position',\n             color='Churn-Yes\/No',\n             title='Churn by Position',\n             barnorm  ='percent',\n             height=750,\n             color_discrete_sequence=[color4, color1],\n             opacity=0.8)\\\n    .update_xaxes(title='Percent')\\\n    .update_yaxes(categoryorder='total descending')","e8598c84":"px.histogram(os_rawdata,\n             y='PayRate',\n             color='Churn-Yes\/No',\n             title='Churn by PayRate (Ungrouped)',\n             barmode='stack',\n             barnorm='percent',\n             height=750,\n             color_discrete_sequence=[color4, color1],\n             opacity=0.8)\\\n    .update_xaxes(title='Percent')","61ae31a1":"#\nlisted = []\nfor idx, val in enumerate(range(15, 85, 5)):\n    #for idx2, val2 in enumerate(range(15,90,5)):\n    #print(str(val)+\",\"+str(val+4))\n    listed.append(str(val) + \"-\" + str(val + 4))\n\n    bin_labels = [\n        '15-19', '0-24', '25-29', '30-34', '35-39', '40-44', '45-49', '50-54',\n        '55-59', '60-64', '65-69', '70-74', '75-79', '80+'\n    ]\n\n    bin_values = [15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 100]\nplotdata = rawdata.copy()\nplotdata['Pay Range'] = pd.cut(plotdata['PayRate'],\n                               bins=bin_values,\n                               labels=bin_labels)\n\npx.histogram(plotdata,\n             y='Pay Range',\n             color='Churn-Yes\/No',\n             title='Churn by PayRate (Grouped)',\n             barmode='stack',\n             barnorm='percent',\n             color_discrete_sequence=[color4, color1],\n             opacity=0.8)\\\n    .update_xaxes(title='Percent')\\\n    .update_yaxes(categoryorder='max descending')","4c99234c":"#\npx.histogram(os_rawdata,\n             y='ManagerName',\n             color='Churn-Yes\/No',\n             title='Churn by Manager',\n             \n             barnorm='percent',\n             color_discrete_sequence=[color4, color1],\n             opacity=0.8)\\\n    .update_xaxes(title='Percent')\\\n    .update_yaxes(categoryorder='total descending')","20fd4dda":"#### Model Libs\nfrom sklearn.preprocessing   import StandardScaler\nfrom sklearn.linear_model    import LogisticRegression\nfrom sklearn.pipeline        import Pipeline\nfrom lightgbm                import LGBMClassifier\n# IMPORTING MODEL METRICS\nfrom sklearn.metrics         import confusion_matrix, roc_auc_score","8aa8990e":"print(\"Reviewing summary info: \\n\")\nmodeldata.info()","457dff53":"print(\"Reviewing null values:\")\nmodeldata.isna().sum()","8a96e075":"logpipe = Pipeline([\n    ('normalizer', StandardScaler()),  #Step1 - normalize data\n    ('clf', LogisticRegression(solver='sag'))  #step2 - classifier\n])\n#------------------------------------------------------------------------\nlgbmpipe = Pipeline([\n    ('normalizer', StandardScaler()),  #Step1 - normalize data\n    ('clf', LGBMClassifier(eval_metric='auc'))  #step2 - classifier\n])","b5accc3c":"# Set train test function and initiate train and test data\nfrom sklearn.model_selection import train_test_split\n# setting x and y dataset\nX, y = modeldata.drop('Churn', axis=1), modeldata['Churn']\n\n#------------------------------------------------------------------------\n# run the train\/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n#------------------------------------------------------------------------\nlenx_train = len(X_train)\nleny_train = len(y_train)\nlenx_test = len(X_test)\nleny_test = len(y_test)\nprint('Sanity check on train\/test:')\nprint('\\n Train x length: ', lenx_train,\n      '\\n Train y length: ',leny_train,\n      '\\n Test x  length: ',lenx_test,\n      '\\n Test y  length: ',leny_test, '\\n',\n      '\\n Split pct     : ',round((lenx_train \/ (lenx_test + lenx_train))*100, 2))\n#print(X_train.columns, sep='\\n')","309c8aaf":"# Fitting pipeline and predictions\nlogfitted = logpipe.fit(X_train, y_train)\nlgbmfitted = lgbmpipe.fit(X_train, y_train)\n#------------------------------------------------------------------------\ny_pred_vals_log = logfitted.predict(X_test)\ny_pred_probs_log = logfitted.predict_proba(X_test)[:, 1]\n#------------------------------------------------------------------------\ny_pred_vals_lgbm = lgbmfitted.predict(X_test)\ny_pred_probs_lgbm = lgbmfitted.predict_proba(X_test)[:, 1]","a2054bd3":"# collecting the auc score\nlog_roc_auc      = roc_auc_score(y_test, y_pred_probs_log)\nlog_roc_auc_pct  = (log_roc_auc*100).round(1)\nlgbm_roc_auc     = roc_auc_score(y_test, y_pred_probs_lgbm)\nlgbm_roc_auc_pct = (lgbm_roc_auc*100).round(1)\n#------------------------------------------------------------------------","c910da60":"# creating confusion matrices for each model\nlog_cnfmtx = confusion_matrix(y_test, y_pred_vals_log)\n#------------------------------------------------------------------------\nlgbm_cnfmtx = confusion_matrix(y_test, y_pred_vals_lgbm)","3ed99ccb":"fig, (axs1, axs2) = plt.subplots(1,2, figsize=(10, 5))\ng1 = sns.heatmap(\n    log_cnfmtx,\n    cbar=False,\n    square= True,\n    fmt='.0f',\n    annot=True,\n    #annot_kws={'size': 15},\n    cmap='Purples', \n    ax=axs1)\ng1.set_title('Confusion Matrix - LogReg \\n AUC:{}% \\n'.format(log_roc_auc_pct))\n\ng2 = sns.heatmap(\n    lgbm_cnfmtx,\n    cbar=False,\n    \n    square= True,\n    fmt='.0f',\n    annot=True,\n    #annot_kws={'size': 15},\n    cmap='Purples',\n    ax=axs2)\ng2.set_title('Confusion Matrix - Light GBM \\n AUC:{}% \\n'.format(lgbm_roc_auc_pct))\n\nplt.show()","93453393":"import shap\nshap.initjs()","491afe3b":"# lgbm features\nprint('Light GBM Model - Attributes by Importance')\nshap_values_lgbm = shap.TreeExplainer(lgbmfitted[1]).shap_values(X_test)\nshap.summary_plot(shap_values_lgbm[1], X_test, plot_type=\"bar\")","74b3cfab":"# logreg features\nprint('Log Reg Model - Attributes by Importance')\nshap_values_log = shap.LinearExplainer(logfitted[1], X_test).shap_values(X_test)\nshap.summary_plot(shap_values_log, X_test, plot_type=\"bar\")\n","881c89f8":"&emsp;  Looking at churn by age groups, we see that proportions are fairly close. We do see that 45-49's have the most churn, with 50-54's having the least. \n\n&emsp; The same is not true for each department. Looking at the churns by department, we see that 58% of empoyees from production and 52% of employees from Software Engineering are noted as leaving. \n\n&emsp; There are no churns numbers in the Executure office and a lowly 3% churn within the Admin Offices. ","da2530bd":"#### ENCODING","c160bcb5":"For this analysis I decided to create an additional 'Churn' and 'Age' Column. This can be seen below.","6c7b02e7":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#IMPORTS\" data-toc-modified-id=\"IMPORTS-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>IMPORTS<\/a><\/span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#IMPORT---LIBS\" data-toc-modified-id=\"IMPORT---LIBS-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;<\/span>IMPORT - LIBS<\/a><\/span><\/li><li><span><a href=\"#IMPORT---DATA\" data-toc-modified-id=\"IMPORT---DATA-1.0.2\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;<\/span>IMPORT - DATA<\/a><\/span><\/li><li><span><a href=\"#IMPORT---SET-FUNCS\" data-toc-modified-id=\"IMPORT---SET-FUNCS-1.0.3\"><span class=\"toc-item-num\">1.0.3&nbsp;&nbsp;<\/span>IMPORT - SET FUNCS<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#ROUGH-REVIEW\" data-toc-modified-id=\"ROUGH-REVIEW-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>ROUGH REVIEW<\/a><\/span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#SAMPLING-AND-INFO\" data-toc-modified-id=\"SAMPLING-AND-INFO-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;<\/span>SAMPLING AND INFO<\/a><\/span><\/li><li><span><a href=\"#NULL-VALS\" data-toc-modified-id=\"NULL-VALS-2.0.2\"><span class=\"toc-item-num\">2.0.2&nbsp;&nbsp;<\/span>NULL VALS<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#DATA-MANIPULATION\" data-toc-modified-id=\"DATA-MANIPULATION-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>DATA MANIPULATION<\/a><\/span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#CATEGORICAL-FEATURES\" data-toc-modified-id=\"CATEGORICAL-FEATURES-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;<\/span>CATEGORICAL FEATURES<\/a><\/span><\/li><li><span><a href=\"#REPLACE-NULL-VALUES\" data-toc-modified-id=\"REPLACE-NULL-VALUES-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;<\/span>REPLACE NULL VALUES<\/a><\/span><\/li><li><span><a href=\"#CORRECTING-THE-DATE-COLUMNS\" data-toc-modified-id=\"CORRECTING-THE-DATE-COLUMNS-3.0.3\"><span class=\"toc-item-num\">3.0.3&nbsp;&nbsp;<\/span>CORRECTING THE DATE COLUMNS<\/a><\/span><\/li><li><span><a href=\"#REVIEW-DUPLICATES\" data-toc-modified-id=\"REVIEW-DUPLICATES-3.0.4\"><span class=\"toc-item-num\">3.0.4&nbsp;&nbsp;<\/span>REVIEW DUPLICATES<\/a><\/span><\/li><li><span><a href=\"#CREATING-NEW-ATTRIBUTES\" data-toc-modified-id=\"CREATING-NEW-ATTRIBUTES-3.0.5\"><span class=\"toc-item-num\">3.0.5&nbsp;&nbsp;<\/span>CREATING NEW ATTRIBUTES<\/a><\/span><\/li><li><span><a href=\"#DROPPING-COLUMNS\" data-toc-modified-id=\"DROPPING-COLUMNS-3.0.6\"><span class=\"toc-item-num\">3.0.6&nbsp;&nbsp;<\/span>DROPPING COLUMNS<\/a><\/span><\/li><li><span><a href=\"#OVERSAMPLING\" data-toc-modified-id=\"OVERSAMPLING-3.0.7\"><span class=\"toc-item-num\">3.0.7&nbsp;&nbsp;<\/span>OVERSAMPLING<\/a><\/span><\/li><li><span><a href=\"#ENCODING\" data-toc-modified-id=\"ENCODING-3.0.8\"><span class=\"toc-item-num\">3.0.8&nbsp;&nbsp;<\/span>ENCODING<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#EDA\" data-toc-modified-id=\"EDA-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>EDA<\/a><\/span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#NUMERCIAL\" data-toc-modified-id=\"NUMERCIAL-4.0.1\"><span class=\"toc-item-num\">4.0.1&nbsp;&nbsp;<\/span>NUMERCIAL<\/a><\/span><\/li><li><span><a href=\"#VIZ---UNIVARAITE\" data-toc-modified-id=\"VIZ---UNIVARAITE-4.0.2\"><span class=\"toc-item-num\">4.0.2&nbsp;&nbsp;<\/span>VIZ - UNIVARAITE<\/a><\/span><\/li><li><span><a href=\"#VIZ---BIVARIATE\" data-toc-modified-id=\"VIZ---BIVARIATE-4.0.3\"><span class=\"toc-item-num\">4.0.3&nbsp;&nbsp;<\/span>VIZ - BIVARIATE<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#MODEL---DATA-PREPROESSING\" data-toc-modified-id=\"MODEL---DATA-PREPROESSING-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>MODEL - DATA PREPROESSING<\/a><\/span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#MODEL---PIPELINE-SCALE-AND-LOAD-MODEL\" data-toc-modified-id=\"MODEL---PIPELINE-SCALE-AND-LOAD-MODEL-5.0.1\"><span class=\"toc-item-num\">5.0.1&nbsp;&nbsp;<\/span>MODEL - PIPELINE SCALE AND LOAD MODEL<\/a><\/span><\/li><li><span><a href=\"#TRAIN---TEST\" data-toc-modified-id=\"TRAIN---TEST-5.0.2\"><span class=\"toc-item-num\">5.0.2&nbsp;&nbsp;<\/span>TRAIN - TEST<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#FIT-MODEL\" data-toc-modified-id=\"FIT-MODEL-5.0.2.1\"><span class=\"toc-item-num\">5.0.2.1&nbsp;&nbsp;<\/span>FIT MODEL<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#MODEL---METRICS\" data-toc-modified-id=\"MODEL---METRICS-5.0.3\"><span class=\"toc-item-num\">5.0.3&nbsp;&nbsp;<\/span>MODEL - METRICS<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#POST-MODEL-REVIEW\" data-toc-modified-id=\"POST-MODEL-REVIEW-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>POST MODEL REVIEW<\/a><\/span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#ATTRIBUTE-IMPORTANCE\" data-toc-modified-id=\"ATTRIBUTE-IMPORTANCE-6.0.1\"><span class=\"toc-item-num\">6.0.1&nbsp;&nbsp;<\/span>ATTRIBUTE IMPORTANCE<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#CONCLUSIONARY-REMARKS\" data-toc-modified-id=\"CONCLUSIONARY-REMARKS-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>CONCLUSIONARY REMARKS<\/a><\/span><\/li><\/ul><\/div>","496acd4e":"&emsp; We can see that churn is not equally distributed between states. Fore instance, every employee from Pennsylvania has left the company. Most of those from Texas are Churner's. ","6cd7185e":"##### FIT MODEL","590009d1":"<p><img src=\"https:\/\/geoffportfolioii.files.wordpress.com\/2020\/08\/hr-analytics-image.png\" style=\"float: center;\" alt=\"A pictogram of a blood bag with blood donation written in it\" width=\"500\">\n<figcaption><center><i>HR Analytics. Image Credit: analyticsinhr.com<\/i><\/center><\/figcaption>    \n<\/p>","804b71e5":"#### CORRECTING THE DATE COLUMNS","805ada3a":"#### NULL VALS","3b2f6783":"#### VIZ - UNIVARAITE","716005c7":">&emsp; Further more, when looking at the dataframe to the left, we see that there are employees marked as being terminated (Termd=1), yet there is no termination date or the employment status remains Active.\n\n>&emsp; Because of this ambiguity, when classifying latter in this analysis, it is decided that we will derive the 'Churn' attribute using the logic noted above and to the left.","9a2fefb2":"> Below are the meta statistics for the numeric data. We can see the mean age of 41 years old, with the youngest at 28 and the oldest at 69 years.\nWe can see that people generally like the company, with a mean employement satisfaction score of 3.8 out of 5.\nWe see that the mean payrate is 31, with a median of 24, meaning that the pay rate distribition is skewed and more employees are paid less that the average.\n","2669add2":"#### OVERSAMPLING","888d7acf":"## ROUGH REVIEW","5b216c85":"#### IMPORT - SET FUNCS","be0d6b9f":"By looking at the initial data, we see that there are two name duplicates. Upon reviewing the attributes associated with these duplicates we can see that the they are not exact copies; for instance, the date of birth values are different. Becuase of this, we decide not to drop these duplicates. ","b195d068":"> It looks as though some managers have a higher associative percent of employee churn than others. For instance, D. Houlihan, W. Butler, and Amy Dunn have the highest managed employee churn rate, while B. LeBlanc, L. Daneault, and E. Dougall have no associate churn rate.","cf6b35f3":"#### MODEL - METRICS ","4d57cddf":"## CONCLUSIONARY REMARKS","8ede14e9":"#### NUMERCIAL","76c6349c":"After looking at the null values in certain cols it is possible to replace certain values with a baseline value. This is carried out in the code below.","0ad38378":"&emsp; Regarding the bivariate analysis, since our dataset has roughly a 70\/30 data imbalance for the negative\/positive class, we will be running the bivaraite analysis on the oversampled dataset. Directly below we can see this imbalance. The plot on the left shows the raw data 'as is', while the one on the right shows the oversampled data.","bcffe027":"#### REVIEW DUPLICATES ","c5aada3e":"#### REPLACE NULL VALUES","d0f53d58":"## DATA MANIPULATION","9beb643f":">The correlation plot below shows that attributes like payrate and project count are positively correlated. We also see that there is a weak negative correlation between churn and payrate and then again between churn and special project count.","ad87f0ab":"For data visualizations there are several visualization techniques available within the python environment, however, for this analysis plotly seemed to be the best vis technique.\n","76a34ce5":"&emsp; Looking further at the null values, we see that there are some attributes with a very high null value count. Attributes like zip code, performance review date, and the days late. Furthermore, the percent of missing values in these columns is 90% and above. Null values are common and can be dealt with through blanket imputation, column similarity, or the consultation of a subject matter expert (SME).\n\n&emsp; One of the high null  value columns is the 'DateofTermination' column. This possibly indicates that only 71% of employees in this dataset of still 'Active'. We will further investigate this.","01e8b568":"#### TRAIN - TEST ","4dc17b53":"<H1><center>Human Resources Data Set - Analysis<\/center><\/H1>","b27b6349":" Does it pay to perform better? ...below we can see the median payrate per performance score. \n \n We can also see below how the median payrate for Sales, exceeds Software Engineers, which even exceeds Executive Office position holders.\n \n We see that the lowest rates are in technician positions, early accountants, and even data analysts, however, senior management positions and IT management carry the highest payrate. We see IT Manager - Support at the highest, which could be a consultant?","3d0e70d0":"In this section we are simply converting date and time attributes. \n\nWe can see that after setting the category data types that we now need to further convert to datetime data types.","de997afe":"## POST MODEL REVIEW","4d592605":"#### ATTRIBUTE IMPORTANCE","0217d8bb":"Looking at the correlations below, we can see that PayRate and SpecialProjectsCount are the most negatively correlate attributes. This means that as pay or project count decrease, there is a slight increase in churn. The opposite is true for Position and FromdiversityJobFair. As the representative numbers for these values increases, there is a slight ","544e167a":"&emsp; Below, we look at the reported reasoning for why employees leave or are terminated. \nThe most notable are:\n    1. Another Position\n    2. Unhappy\n    3. More Money \n    \n&emsp; Given this information, it would be possible for the company to place counter measures that combat the highest reasons for leaving. this could include higher pay, training, or the ability for employees to take on some work that better utilizes their core skill sets.\n","723006bc":"# Introduction","ef3465a7":" In this section, I am simply reviewing the data types and the null value counts. We need to impute of remove null values and encode any categorical attributes.","f35629aa":"&emsp; In starting the analysis process, we see that on the bottom left there are 3310 observations or employees in this dataset. There are also 36 columns\/attributes. We see employee names, marriage status, date of hire, and date of termination.\n\n&emsp; Given the information on the left we can also see the null value count and the data types of each attribute.","f0b849b0":"&emsp; To initiate the analysis we import the data. The filenames within the data directory are visibale below. \n\n&emsp; Also, as with an excel document, the shape (rows & columns) and the column\/attribute names are also visible. There are a total of 36 initial attributes within this dataset.","4f9d4738":"&emsp; \nWhile reviewing the available termination values, it became apparent that there are some data consistency issues. Firstly, we can see above that there are two illustrations counting reasons for why employees leave. The one on the left is derived from counts that exclude currently 'Active' or 'Leave of Absence' employees. It's clear that this is different from the illustration on the right, which is derived when setting the 'Termd' column equal to 1. There must be a set reason for this inconsistency, but since we don't have someone to directly clarify this situation, we need to figure out which is 'right' to use in our analysis.\n","63b8a2cf":"Before we can pass the data into the model, there are several preprocessing steps that are neccesary, like encoding, scaling, and the imputation\/removal of null values. We will carry out these steps quickly before we move into the modeling.\n\nSince this analysis is mainly concerned with an initial dataset review, we will only run two basic models. These are Logistic for a baseline and then a follow up with LightGBM. ","7f30f4d6":"#### VIZ - BIVARIATE","6c5cd769":"#### MODEL - PIPELINE SCALE AND LOAD MODEL","b64d9360":"> Looking at the Light GBM model, we see that position, payrate, managername are the top three attributes.","a17091d4":"We use the power of python to help with the tran\/test split. The train and test data lengths and the percentage split is visible below.","62700a8e":"#### IMPORT - DATA","5a3a7ad0":"## MODEL - DATA PREPROESSING","ec46765b":"#### DROPPING COLUMNS","5b8a4363":"There are serveral columns that were dropped, inclding Zip, employeename, and ID. \n\nWe also drop the few rows indicating that an employee had not yet started working with the company.","4f6b40cd":"#### CATEGORICAL FEATURES","1cf57821":"## EDA","3dc26f07":"## IMPORTS","fe491a75":"&emsp; Data types are important to know when running an analysis, since each data type needs to be handled in a different way. For instance we cannot run statistics off of text values and we cannot perform time series analysis if our date values are in an unfamiliar format. Through this analysis we will run through the necessary data conversions for analysis. \n\n&emsp; Below, we can see the number of unique values per categorical type attribute. ","aeef794e":"&emsp; Within most programming oriented analyses, it is often much easier to declare certain functions for both ease of use and consitency of use. While in this analysis I only used one function, which is visible below, usually there would be several functions in use.","7c0e5127":"#### SAMPLING AND INFO","8e7800eb":"#### IMPORT - LIBS","e493a041":"We can also see that after converting the datatimes, that there was an issue. Since time travel has not yet been invented, this has also been corrected.","13b88f40":"Below are the two confusion matrices per each model. It was decided not to use precision, accuracy, or recall since there is considerable data imbalance, but I have included the Confusion Matrix as a basic overview of performance. Also the AUC is included.\n\nWe can see that the LighGBM model is performing better by a massive 25% margin in AUC over the baseline logistic regression model.","1a7e0ec8":"The top indicators of people leaving include (looking at the LightGBM model):\n* Position\n* Payrate\n* Manager\n* Recruitment Sources\n* State\n\nWhen looking at both models, we see that Position, Pay, Manager, and Recruitment source are the top attributes associated with Churn for this company.\n\nKnowing this, HR can further investigate positions where pay is not sufficient for certain employees. They can also investigate management issues. This can be done through surveys or during general reviews. \n\nQuestions we will answer include:\n* What states do most of our employees reside?\n *  The company seems to reside in MA, since 90% of employees are noted as residing there too.\n\n\n* What is the organizational composition by position?\n * 45% of company is staffed by 'Production Technician I's' , then 17% Production Technician 2's, and 9% Area Sales Manager\n\n\n* What are the top reasons employees leave?\n * The top reasons employees leave are for another position, unhappiness, and for more money.\n * There is also a high churn rate under the Production department.\n\n\n* Are some Employees more or less likely to churn under some managers?\n * Under some managers like D Houlihan, W. Butler, and A. Dunn, there is an alarming churn rate. Under normal circumstances, this would certainly need to be further investigated.\n\n\n* Which positions have the highest associated churn?\n * There is high churn for - data architects, production technicians, CIO's, BI Developers, and DB admins.\n\n","48f2fa30":"#### CREATING NEW ATTRIBUTES","f3dd8565":" We use the help of pipeline objects to run both models and the scaling. While we don't take full advantage of the pipeline, if we needed to expound on this analysis later then the essential structures are already in place. Additionally, the aim of this analysis is not to run predictions, so the classifiers are left mostly in their baseline state. ","7db9963e":"> Feel free to skip ahead to the VIZ section to get a quick review and then back into the weeded details (Link's in the TOC click: EDA-> VIZ). Here's a brief glimpse of what you'd find...","1e8db7ff":"In starting this analysis, the main libraries that we use are pandas, numpy, and plotly. There is also a simple function used specifically for jupyter. ","ab4ab4dd":"&emsp; The are several categorical type attributes in this dataset. In order to pass these into the model, we first need to encode the categorical values. There are several methods to do this, like simple label encoding and one-hot encoding. For this analysis we will use label encoding. The columns for encoding are listed below.","5a4651e5":"> Below we see Churn by Position and Churn by Pay.\nWe see that that data architects, CIO's, and DB Admins with the highest proportion of churner's.\n\n>For pay, firstly, there is a huge payrate gap between the high 60's and 80, but then we see that a payrate of 40-44 and 25-29 has the highest proportion of churner's, with 30-34 and 70-74 with the lowest.","be4df571":"\n&ensp; This analysis is focused on Human Resources Analytics. The main intent is to review the historical dataset and find insights through modern programming and visualization techniques. I do also include a rudimentary predictive model that can be used in analyzing important attributes and predicting churn cases.\n\n>Questions we will answer include:\n* What states do most of our employees reside?\n* What is the organizational composition by position?\n* What are the top reasons employees leave?\n* Are some Employees more or less likely to churn under some managers?\n* Which positions have the highest associated churn?\n\nThe dataset used for this analysis is available online; the link for this is included below.\n\nSource:https:\/\/www.kaggle.com\/davidepolizzi\/hr-data-set-based-on-human-resources-data-set\n\nDict: https:\/\/rpubs.com\/rhuebner\/HRCodebook-13","4427facf":"> Looking at the logreg model, we see that PayRate, Position, and Age are the top three attributes."}}