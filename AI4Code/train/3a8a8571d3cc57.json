{"cell_type":{"0c0da3be":"code","d7fdc878":"code","a6bdd2d5":"code","96b73637":"code","858e4798":"code","1a75f32b":"code","b1336291":"code","08f74c8d":"code","da7403fb":"code","49a354e5":"code","7e5de90c":"code","e33bda7c":"code","dd17cbe0":"markdown","e9f2936e":"markdown","0da4341a":"markdown","89ef8b3b":"markdown"},"source":{"0c0da3be":"%matplotlib inline\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom torch import autograd\nfrom torch.autograd import Variable\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt","d7fdc878":"class FashionMNIST(Dataset):\n    def __init__(self, transform=None):\n        self.transform = transform\n        fashion_df = pd.read_csv('..\/input\/fashion-mnist_train.csv')\n        self.labels = fashion_df.label.values\n        self.images = fashion_df.iloc[:, 1:].values.astype('uint8').reshape(-1, 28, 28)\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        label = self.labels[idx]\n        img = Image.fromarray(self.images[idx])\n        \n        if self.transform:\n            img = self.transform(img)\n\n        return img, label","a6bdd2d5":"dataset = FashionMNIST()\ndataset[0][0]","96b73637":"transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n])\ndataset = FashionMNIST(transform=transform)\ndata_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)","858e4798":"class Discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.label_emb = nn.Embedding(10, 10)\n        \n        self.model = nn.Sequential(\n            nn.Linear(794, 1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout(0.3),\n            nn.Linear(1024, 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout(0.3),\n            nn.Linear(256, 1),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x, labels):\n        x = x.view(x.size(0), 784)\n        c = self.label_emb(labels)\n        x = torch.cat([x, c], 1)\n        out = self.model(x)\n        return out.squeeze()","1a75f32b":"class Generator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.label_emb = nn.Embedding(10, 10)\n        \n        self.model = nn.Sequential(\n            nn.Linear(110, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(1024, 784),\n            nn.Tanh()\n        )\n    \n    def forward(self, z, labels):\n        z = z.view(z.size(0), 100)\n        c = self.label_emb(labels)\n        x = torch.cat([z, c], 1)\n        out = self.model(x)\n        return out.view(x.size(0), 28, 28)","b1336291":"generator = Generator().cuda()\ndiscriminator = Discriminator().cuda()","08f74c8d":"criterion = nn.BCELoss()\nd_optimizer = torch.optim.Adam(discriminator.parameters(), lr=1e-4)\ng_optimizer = torch.optim.Adam(generator.parameters(), lr=1e-4)","da7403fb":"def generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion):\n    g_optimizer.zero_grad()\n    z = Variable(torch.randn(batch_size, 100)).cuda()\n    fake_labels = Variable(torch.LongTensor(np.random.randint(0, 10, batch_size))).cuda()\n    fake_images = generator(z, fake_labels)\n    validity = discriminator(fake_images, fake_labels)\n    g_loss = criterion(validity, Variable(torch.ones(batch_size)).cuda())\n    g_loss.backward()\n    g_optimizer.step()\n    return g_loss.data[0]","49a354e5":"def discriminator_train_step(batch_size, discriminator, generator, d_optimizer, criterion, real_images, labels):\n    d_optimizer.zero_grad()\n\n    # train with real images\n    real_validity = discriminator(real_images, labels)\n    real_loss = criterion(real_validity, Variable(torch.ones(batch_size)).cuda())\n    \n    # train with fake images\n    z = Variable(torch.randn(batch_size, 100)).cuda()\n    fake_labels = Variable(torch.LongTensor(np.random.randint(0, 10, batch_size))).cuda()\n    fake_images = generator(z, fake_labels)\n    fake_validity = discriminator(fake_images, fake_labels)\n    fake_loss = criterion(fake_validity, Variable(torch.zeros(batch_size)).cuda())\n    \n    d_loss = real_loss + fake_loss\n    d_loss.backward()\n    d_optimizer.step()\n    return d_loss.data[0]","7e5de90c":"num_epochs = 30\nn_critic = 5\ndisplay_step = 300\nfor epoch in range(num_epochs):\n    print('Starting epoch {}...'.format(epoch))\n    for i, (images, labels) in enumerate(data_loader):\n        real_images = Variable(images).cuda()\n        labels = Variable(labels).cuda()\n        generator.train()\n        batch_size = real_images.size(0)\n        d_loss = discriminator_train_step(len(real_images), discriminator,\n                                          generator, d_optimizer, criterion,\n                                          real_images, labels)\n        \n\n        g_loss = generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion)\n\n    generator.eval()\n    print('g_loss: {}, d_loss: {}'.format(g_loss, d_loss))\n    z = Variable(torch.randn(9, 100)).cuda()\n    labels = Variable(torch.LongTensor(np.arange(9))).cuda()\n    sample_images = generator(z, labels).unsqueeze(1).data.cpu()\n    grid = make_grid(sample_images, nrow=3, normalize=True).permute(1,2,0).numpy()\n    plt.imshow(grid)\n    plt.show()","e33bda7c":"z = Variable(torch.randn(100, 100)).cuda()\nlabels = Variable(torch.LongTensor([i for _ in range(10) for i in range(10)])).cuda()\nsample_images = generator(z, labels).unsqueeze(1).data.cpu()\ngrid = make_grid(sample_images, nrow=10, normalize=True).permute(1,2,0).numpy()\nfig, ax = plt.subplots(figsize=(15,15))\nax.imshow(grid)\n_ = plt.yticks([])\n_ = plt.xticks(np.arange(15, 300, 30), ['T-Shirt', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], rotation=45, fontsize=20)\n","dd17cbe0":"## PyTorch Conditional GAN\n\nThis kernel is a PyTorch implementation of [Conditional GAN](https:\/\/arxiv.org\/abs\/1411.1784), which is a GAN that allows you to choose the label of the generated image. The generator and the discriminator are going to be simple feedforward networks, so I guess the images won't be as good as in this [nice kernel](https:\/\/www.kaggle.com\/sgamez\/fashion-ac-gan-with-keras) by [Sergio G\u00e1mez](https:\/\/www.kaggle.com\/sgamez). I used [this implementation](https:\/\/github.com\/eriklindernoren\/PyTorch-GAN\/blob\/master\/implementations\/cgan\/cgan.py) by [eriklindernoren](https:\/\/github.com\/eriklindernoren) as inspiration.","e9f2936e":"## Results","0da4341a":" Let's start by defining a Dataset class:\n* [Data Loading and Processing Tutorial on PyTorch's documentation](https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html)\n* [torchvision](https:\/\/github.com\/pytorch\/vision) has a [built-in class for Fashion MNIST](https:\/\/pytorch.org\/docs\/stable\/torchvision\/datasets.html#fashion-mnist)","89ef8b3b":"Now let's define the generator and the discriminator, which are simple MLPs. I'm going to use an embedding layer for the label:"}}