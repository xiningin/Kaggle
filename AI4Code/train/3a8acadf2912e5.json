{"cell_type":{"6d5efa65":"code","c4094547":"code","44376aa2":"code","3ca0eb95":"code","0a1c3017":"code","ced8f0bc":"code","c84841ae":"code","f8fbf8ea":"code","bbbb0071":"code","01a33b64":"code","5bc7fa5b":"code","6eee8066":"code","0c935f6e":"code","0a9e6379":"code","6ff8c1d3":"code","464acce7":"code","2b0645ae":"code","4e959784":"code","1cca9336":"code","055a9eff":"code","9bfdf65b":"code","ca46ec69":"code","51b69219":"code","92629a21":"code","6f514b98":"code","51c56c4a":"code","13c3cacf":"code","13d9eaed":"code","9e8440bb":"code","66f18a3d":"markdown","bdfbf822":"markdown","ce05ae22":"markdown","14a34ef9":"markdown","2ef0b83e":"markdown","2d7f2267":"markdown","d2a771e5":"markdown","e2e0077e":"markdown","9c5892fd":"markdown","519f0e74":"markdown","6d0ddfd9":"markdown","c26e4956":"markdown","72c310df":"markdown","c381b939":"markdown","19c2433e":"markdown","cf860da4":"markdown","0856e7a1":"markdown","1a27825e":"markdown","06e41fb3":"markdown","48b9f9e3":"markdown","141f167c":"markdown","e60e7dc1":"markdown","ecbb2b37":"markdown","8751d69c":"markdown","0472a97d":"markdown","ddba0038":"markdown","b882b0cb":"markdown","74e9650e":"markdown","a1f1fbb2":"markdown","e071c2d9":"markdown","a6582025":"markdown","3e89b74d":"markdown"},"source":{"6d5efa65":"import pandas as pd                 #Using to read the csv files\nimport matplotlib.pyplot as plt     #For data plotting\nimport numpy as np                  #Building mathematical calculations\nimport glob                         #Retrieve the file\/path name\nimport re                           #Regular-Expression to fetch data from text\nimport json                         #Reading the JSON Documents\nimport string                       #For doing string operations\n\n#Essential ML Libraries\n\nfrom sklearn.feature_extraction.text import HashingVectorizer      #Performing Hashing Vectorization\nfrom sklearn.feature_extraction.text import TfidfVectorizer        #Tfidf vector for text analysis\nfrom sklearn.model_selection import train_test_split               #For train-test-split for the dataset\nfrom sklearn.ensemble import RandomForestClassifier                #Random-forest-classifier\nfrom sklearn.neighbors import KNeighborsClassifier                 #KNN Classifier\nfrom sklearn import svm                                            #Support Vector Machine classifier\nfrom sklearn import metrics                                        #Module to check the model accuracy\n\n#Essential Neural Network Libraries\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout\nfrom keras.optimizers import SGD\n\n#Other essential libraries\nimport plotly.express as px                                        #Plotly for plotting the COVID-19 Spread.\nimport plotly.offline as py                                        \n\nprint(\"Mentioned Libraries Successfully Imported\")","c4094547":"\"\"\"literature_data = pd.read_csv('..\/input\/CORD-19-research-challenge\/metadata.csv')\nliterature_data.head(2)\"\"\"","44376aa2":"\"\"\"literature_data['doi'].astype('str') #Changing datatype of the column doi to string\nliterature_data.dtypes\"\"\"","3ca0eb95":"\"\"\"#Getting the filepaths of JSON Documents with GLOB\nfilepaths_json = glob.glob('..\/input\/CORD-19-research-challenge\/\/**\/*.json', recursive=True)  #Setting Recursive Feature as True.\n\n#printing the datatype for the filepaths_json variable\ntype(filepaths_json)\n\n#Printing the first two elements from the list\nfilepaths_json[0:2]\"\"\"","0a1c3017":"\"\"\"class FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n        \n            self.body_text = []\n            # Code to read the abstract of JSON File\n            for entry in content['abstract']:\n                self.abstract.append(entry['text'])\n            # Code to read the body of the JSON File\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\nfirst_row = FileReader(filepaths_json[0])\nprint(first_row)\n\ndef get_breaks(content, length):\n    data = \"\"\n    words = content.split(' ')\n    total_chars = 0\n\n    # add break every length characters\n    for i in range(len(words)):\n        total_chars += len(words[i])\n        if total_chars > length:\n            data = data + \"<br>\" + words[i]\n            total_chars = 0\n        else:\n            data = data + \" \" + words[i]\n    return data\n\ndict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\nfor idx, entry in enumerate(filepaths_json):\n    if idx % (len(filepaths_json) \/\/ 10) == 0:\n        print(f'Processing index: {idx} of {len(filepaths_json)}')\n    content = FileReader(entry)\n    \n    # get the information from the JSON File\n    meta_data = literature_data.loc[literature_data['sha'] == content.paper_id]\n    # If no information is found for the provided paper, the paper is skipped.\n    if len(meta_data) == 0:\n        continue\n    \n    dict_['paper_id'].append(content.paper_id)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\n    \n    # also create a column for the summary of abstract to be used in a plot\n    if len(content.abstract) == 0: \n        # no abstract provided\n        dict_['abstract_summary'].append(\"Not provided.\")\n    elif len(content.abstract.split(' ')) > 100:\n        # abstract provided is too long for plot, take first 300 words append with ...\n        info = content.abstract.split(' ')[:100]\n        summary = get_breaks(' '.join(info), 40)\n        dict_['abstract_summary'].append(summary + \"...\")\n    else:\n        # abstract is short enough\n        summary = get_breaks(content.abstract, 40)\n        dict_['abstract_summary'].append(summary)\n        \n    # get metadata information\n    meta_data = literature_data.loc[literature_data['sha'] == content.paper_id]\n    \n    try:\n        # if more than one author\n        authors = meta_data['authors'].values[0].split(';')\n        if len(authors) > 2:\n            # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n            dict_['authors'].append(\". \".join(authors[:2]) + \"...\")\n        else:\n            # authors will fit in plot\n            dict_['authors'].append(\". \".join(authors))\n    except Exception as e:\n        # if only one author - or Null valie\n        dict_['authors'].append(meta_data['authors'].values[0])\n    \n    # add the title information, add breaks when needed\n    try:\n        title = get_breaks(meta_data['title'].values[0], 40)\n        dict_['title'].append(title)\n    # if title was not provided\n    except Exception as e:\n        dict_['title'].append(meta_data['title'].values[0])\n    \n    # add the journal information\n    dict_['journal'].append(meta_data['journal'].values[0])\n    \nliterature_data = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])\nliterature_data.head()\"\"\"","ced8f0bc":"\"\"\"#Checking for the null values\n\nliterature_data.isnull().sum()\"\"\"","c84841ae":"\"\"\"#Dropping the duplicate values.\nliterature_data.drop_duplicates(['abstract', 'body_text'], inplace=True)\n\n#Checking the count of null values again.\nliterature_data.isnull().sum()\n\n#Dropping the null values.\nliterature_data.dropna(inplace=True)\n\n#Removing Punctuations\nliterature_data['body_text'] = literature_data['body_text'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\n\n#Converting to Lower case\ndef lower_case(input_str):\n    input_str = input_str.lower()\n    return input_str\n\nliterature_data['body_text'] = literature_data['body_text'].apply(lambda x: lower_case(x))\"\"\"","f8fbf8ea":"literature_data = pd.read_csv('..\/input\/covid19-literary-analysis-dataset-covlad\/COVID-Literature-Analysis.csv')\nliterature_data.head(2)","bbbb0071":"#Dropping off the unecessary rows.\nliterature_data.drop(['Unnamed: 0','Unnamed: 0.1'],axis=1, inplace=True)\n\n#Genrating a column total that reads the word count of the column body_text\nliterature_data['Word_Count'] = literature_data['body_text'].apply(lambda x: len(x.strip().split()))\n\n#Viewing the dataset\nliterature_data.head(2)","01a33b64":"X = literature_data['body_text']  #Selecting the independent variable for classification\n\n#Building a Hashing Vectorizer to convert the text to vectorized numbers via a sparse matrix\nfrom sklearn.feature_extraction.text import HashingVectorizer\n\n# hash vectorizer instance\nvectors = HashingVectorizer(lowercase=False, analyzer=lambda l:l, n_features=2**12)\n\n# features matrix X\nX = vectors.fit_transform(X)","5bc7fa5b":"#Selecting the column for labels as the independent variable (Labels from K-Means Clustering)\n\nY = literature_data['Labels']","6eee8066":"#Implementing a train test split over the dataset\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.25, random_state=4)","0c935f6e":"#Implementing a Random Forest Classifier for the dataset.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrnd = RandomForestClassifier()\nrnd.fit(x_train,y_train)\nyhat = rnd.predict(x_test)","0a9e6379":"\n#Searching for the model accuracy\n\nfrom sklearn import metrics\nmetrics.accuracy_score(y_test,yhat)","6ff8c1d3":"#Implementing a SVM Classifier\n\nfrom sklearn import svm\nclf = svm.SVC(kernel='rbf', probability=True)\nclf.fit(x_train,y_train)\n\nyhat = clf.predict(x_test)","464acce7":"#Searching for the model accuracy\n\nfrom sklearn import metrics\nmetrics.accuracy_score(y_test,yhat)","2b0645ae":"#Implementing a  for KNN Classifier to the dataset.\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nk = 9 #Initializing the value of K for classification\n\nneigh = KNeighborsClassifier(n_neighbors=k, metric='minkowski',p=2).fit(x_train,y_train)\nyhat = neigh.predict(x_test)","4e959784":"#Searching for the model accuracy\n\nfrom sklearn import metrics\nmetrics.accuracy_score(y_test,yhat)","1cca9336":"#Importing the essential libraries for neural network development\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout\nfrom keras.optimizers import SGD\n\n#Initiating the model\nmodel = Sequential()\n\n#Adding layers to the model\nmodel.add(Dense(2000, activation='sigmoid', input_shape=(4096,)))\nmodel.add(Dense(750, activation='sigmoid')) \nmodel.add(Dense(17, activation='softmax'))\n\n#Model Compilation and fitting the model\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.fit(x_train,y_train, batch_size=512, epochs=200, validation_data=(x_test,y_test))\n\n#Neural Network Evaluation\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(score[0])\nprint(score[1])","055a9eff":"def prediction_text():\n    print(\"Enter the text to search for\")  #Input statement to enter the text to search for.\n    \n    #Basic Editing for the input text\n    \n    text_search = str(input())\n    text_search = text_search.lower()                            #Converts to lower case.\n    text_search = re.sub('[^a-zA-z0-9\\s]','',text_search)        #Removes punctuations\n    text_search = [text_search]   \n    \n    \n    #Using Hash Vectorizer to analyze the input text and convert it into a sparse matrix.\n    hvec = HashingVectorizer(lowercase=False, analyzer=lambda l:l, n_features=2**12)   \n    test_sample = hvec.fit_transform(text_search)\n    \n    #Predicting the results\n    pred = rnd.predict(test_sample)\n    pred = pred[0]\n    \n    print(\"The matched Label for the input query is = {}\".format(pred))\n    print(\"\\nReturning the dataset (named - search_results) for the label\")\n    \n    label_dataset = literature_data['Labels'] == pred        #Returning the dataset that matches with the prediction label\n    search_results = literature_data[label_dataset]\n    \n    print(search_results.head(4))","9bfdf65b":"#Using the above function\nprediction_text()","ca46ec69":"covid_data = pd.read_csv('..\/input\/novel-corona-virus-2019-dataset\/covid_19_data.csv')\ncovid_data.head()","51b69219":"covid_data = pd.read_csv('..\/input\/novel-corona-virus-2019-dataset\/covid_19_data.csv')\ncovid_data.head()","92629a21":"#Dropping of the column Last Update\ncovid_data.drop('Last Update', axis=1, inplace=True)\n\n#Resetting the index to SNo Column\ncovid_data.set_index(['SNo'], inplace=True)\n\n#Viewing the dataset\ncovid_data.head()\n\n#Replacing NaN Values in Province\/State with a string \"Not Reported\"\ncovid_data['Province\/State'].replace(np.nan, \"Not Reported\", inplace=True)\n\n#Printing the dataset\ncovid_data.head()","6f514b98":"#Creating the interactive map\npy.init_notebook_mode(connected=True)\n\n#GroupingBy the dataset for the map\nformated_gdf = covid_data.groupby(['ObservationDate', 'Country\/Region'])['Confirmed', 'Deaths', 'Recovered'].max()\nformated_gdf = formated_gdf.reset_index()\nformated_gdf['Date'] = pd.to_datetime(formated_gdf['ObservationDate'])\nformated_gdf['Date'] = formated_gdf['Date'].dt.strftime('%m\/%d\/%Y')\nformated_gdf['size'] = formated_gdf['Confirmed'].pow(0.3)\n\n#Plotting the figure\nfig = px.scatter_geo(formated_gdf, locations=\"Country\/Region\", locationmode='country names', \n                     color=\"Confirmed\", size='size', hover_name=\"Country\/Region\", \n                     range_color= [0, max(formated_gdf['Confirmed'])+2], \n                     projection=\"natural earth\", animation_frame=\"Date\", \n                     title='Spread of COVID-19 Virus')\n\n#Showing the figure\nfig.update(layout_coloraxis_showscale=False)\npy.offline.iplot(fig)","51c56c4a":"#Groping the same cities and countries together along with their successive dates.\n\ncountry_list = covid_data['Country\/Region'].unique()\n\ncountry_grouped_covid = covid_data[0:1]\n\nfor country in country_list:\n    test_data = covid_data['Country\/Region'] == country   \n    test_data = covid_data[test_data]\n    country_grouped_covid = pd.concat([country_grouped_covid, test_data], axis=0)\n    \ncountry_grouped_covid.reset_index(drop=True)\ncountry_grouped_covid.head()","13c3cacf":"def plot_case_graph():\n    \n    print(\"Enter the city to be searched\\n\")\n    search_city = str(input())\n\n    #Draws the plot for the searched city\n\n    search_data = country_grouped_covid['Province\/State'] == search_city       #Selecting the city\n    search_data = country_grouped_covid[search_data]                           #Filtering the dataset\n\n    x = search_data['ObservationDate']\n    y = search_data['Confirmed']\n    b = search_data['Confirmed'].values\n    \n    \n    a = b.shape   \n    a = a[0]\n    growth_rate = []    \n    \n    for i in range(1,a):                                       #Loop to calculate the daily growth rate of cases\n        daily_growth_rate = ((b[i]\/b[i-1])-1)*100\n        growth_rate.append(daily_growth_rate)                                      \n\n    growth_rate.append(daily_growth_rate)\n        \n    data = {'Growth' : growth_rate}\n    b = pd.DataFrame(data)\n    \n    #Plotting the chart for confirmed cases vs date     \n        \n    plt.figure(figsize=(15,5))\n    plt.bar(x,y,color=\"#9ACD32\")                              \n    plt.xticks(rotation=90)\n    \n    plt.title('Confirmed Cases Over time in {}'.format(search_city))\n    plt.xlabel('Time')\n    plt.ylabel('Confirmed Cases')\n\n    plt.tight_layout()\n    plt.show()\n    \n    #Plotting the chart daily growth rate in confirmed COVID-19 Cases.\n    \n    plt.figure(figsize=(15,5))\n    plt.plot(x,b,color='red', marker='o', linestyle='dashed',linewidth=2, markersize=8,label=\"Daily Growth Rate of New Confirmed Cases\")\n    plt.xticks(rotation=90)\n    \n    plt.title('Confirmed Cases Over time in {}'.format(search_city))\n    plt.xlabel('Time')\n    plt.ylabel('Percentage Daily Increase')\n\n    plt.tight_layout()\n    plt.show()\n    \nplot_case_graph()","13d9eaed":"#Viewing the generated graphs for the reported cities in China\n\n%pylab inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nplt.figure(figsize=(15,5))\nimg=mpimg.imread('..\/input\/china-covid19-data\/Anhui.png')\nimgplot = plt.imshow(img)\nplt.show()\n\nplt.figure(figsize=(15,5))\nimg=mpimg.imread('..\/input\/china-covid19-data\/Beijing.png')\nimgplot = plt.imshow(img)\nplt.show()\n\nfor i in range(1,17):\n    plt.figure(figsize=(15,5))\n    img=mpimg.imread('..\/input\/china-covid19-data\/Screenshot ({}).png'.format(303+i))\n    imgplot = plt.imshow(img)\n    plt.show()","9e8440bb":"#Searching for the relevant articles\nprediction_text()","66f18a3d":"Note: The function call prediction_text() is used to envoke the above function. The more detail is the input the better is the model accuracy for prediction the corect label.","bdfbf822":"Now the dataset generated above is Wrangled and is ready for observations. We check once to ensure the graphs of confirmed cases are plotted.\n\nFor this case we select to plot the graph for Hubei Province of China, to see total confirmed cases over time. The following is done via the MatPlotLib Library.\n\nThe underlying code below prints the confirmed cases vs observation date for the city that is provided as the input","ce05ae22":"<H2> Fetching data from the JSON Document <\/H2>\n\n(Special Thanks to maksimeren for the code to fetch the details.)\n(maksimeren Notebook available at - https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering)\n","14a34ef9":"<h2> Prediction with the Random Forest Classifier Model <\/h2>\n\n1. We develop a function which takes input as the text and predicts the label. \n2. The more is the detail for the text inputed, the higher is the chances for a better prediction.\n3. The code returns a label which then can be used to find the related articles from the main COVID Literature dataset.","2ef0b83e":"# Covid(RAN) - COVID-19 Research & Analytics Notebook\n\n<h2> COVID-19 : Transmission, Incubation, and Environmental Stability <\/h1>\n<h2> Making conclusions form data and validating with Literary References <\/h2>","2d7f2267":"<h2> Basic Data Wrangling for the generated Dataset <\/h2>","d2a771e5":"<h1> Implementing a Classification Code over the labeled dataset<\/h1>","e2e0077e":"<h1> Task 1 - Seasonality of COVID-19 Transmission <\/h1>","9c5892fd":"<h2> Hunting for a neural network for higher accuracy <\/h2>\n\nWe develop a neural network to check if the accuracy goes up for the classification for the model","519f0e74":"<h3> #Selecting the Body_Text form the dataset and storing the value in the variable X. <\/h3>\n\nX = literature_covid['body_text']\n\n\n<h3> #Implementing the Tf-IDF-Vectorizer to vectorize the dataset. <\/h3>\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(max_features=2**12)\n\nX = vectorizer.fit_transform(df_covid['body_text'])","6d0ddfd9":"<h1> Manipulations with the JSON Documents <\/h1>","c26e4956":"<h2> Loading the Literary Clustered Dataset <\/h2>\n\nThe followind dataset is generated from the code written in  markdown above.","72c310df":"We first observe the COVID-19 Spreading Trend Patterns in all Provinces of China. For this the graph generated above was plotted for all the provinces. The Graph was made in MS-Excel and the results are fetched underneath.","c381b939":"<h2> Creating an interactive Map for COVID-19 Cases <\/h2>","19c2433e":"<h3> #Implementing K-Means Clustering Algorithm to the generated Sparse Matrix. <\/h3>\n\nfrom sklearn.cluster import KMeans\n\nk = 17\n\nkmeans = MiniBatchKMeans(n_clusters=k)\n\ny_pred = kmeans.fit_predict(X)","cf860da4":"<h1> Importing the COVID-19 Global Cases Dataset <\/h1>","0856e7a1":"<h1> Please Note <\/h1>\n\nThis notebook is still in it's development Phase. We would add more details to the same with the later versions. Feel free to share much queries\/information.\n\nDevelopers:\n\n1. Aman Kumar - [linkedin.com\/in\/amankumar01\/](http:\/\/)\n2. Keertikesh Rajkiran","1a27825e":"<H1> Analyzing the Graph Generated <\/H1>\n\nFor all of the Graphs of Provicnes Generated above, the Graph is divided into 3 sections. \n\n1. The vertical bars represent the total confimed COVID-19 Cases (Tick Labels on LHS of Y axis inside the figure).\n2. The X axis for all the above graphs denoted the dates on which the cases were reported.\n3. The (RHS) Y axis denoted the % daily growth rate of newer confirmed COVID-19 Cases.\n\nThe stacked bars, mentioned in chart above shows the confirmed cases of COVID-19 and it's progression with time. The red line chart shows the growth rate of newer cases reported daily.\n\nThe newer cases have been calculated here by the formula :\n\nGrowth Rate = ((Newer Cases \/ Older cases) - 1) * 100\n\n4. The four translucent rectangle boxes in the chart highlights the 4 weeks since the first outbreak of COVID-19 was reported.\n\n\n<h1> Trends Observed in the Underlying Graphs for China <\/h1>\n\n* For all of the provinces displayed above, the frequency of COVID-19 Cases reduced drastically after Week 4 of disease outbreak. The daily growth rate of cases declined.\n\n<h1> Hunting for the possible cases for decline in growth rate of new COVID-19 Cases in China post week 4 <\/h1>\n\nWe search for the text in literature to find the best papers regarding the same. This is done by using prediction_text()","06e41fb3":"<h1> Importing the Literature Dataset <\/h1>","48b9f9e3":"<h2> Support Vector Machine Classifier <\/h2>","141f167c":"<h2> Notebook details <\/h2>\n\n1. The dataset mentioned underneath is uploaded from the kaggle COVID-19 Dataset. (https:\/\/www.kaggle.com\/sudalairajkumar\/novel-corona-virus-2019-dataset).\n\n2. Apart Python Notebooks, visualizations are also imported from Tableau, PowerBI and analysis is also done on Excel and SAS.\n\n3. The details for the same is added to this python notebook wherever applicable.\n\n4. The Literary references for this notebook was fetched from the https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge dataset.\n\n5. The above mentioned data was wrangled to serve its best purpose.\n\n6. Due to the complexity of the notebook some lines of code are under comments. They can be uncommented.\n\n<h2> Dataset Description <\/h2>\n\n'In response to the COVID-19 pandemic, the White House and a coalition of leading research groups have prepared the COVID-19 Open Research Dataset (CORD-19). CORD-19 is a resource of over 29,000 scholarly articles, including over 13,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. This freely available dataset is provided to the global research community to apply recent advances in natural language processing and other AI techniques to generate new insights in support of the ongoing fight against this infectious disease. There is a growing urgency for these approaches because of the rapid acceleration in new coronavirus literature, making it difficult for the medical research community to keep up.' - Kaggle\n\n<h2> Importing the Essential Libraries <\/h2>","e60e7dc1":"<h2> Grouping the countries together <\/h2>","ecbb2b37":"<h2> Understanding the dataset Above <\/h2>\n\nThe dataset above highlights the respective paperid along with the abstract, body text, the title of the paper, its authors and a short summary of the abstract containing the first 300 chracters from the abstract column\n\n<h2> Wrangling the Generated dataset <\/h2>","8751d69c":"<h2> Implementing Machine Learning on the previous dataset <\/h2>\n\nThe steps involved in this procedure is as under:\n\n1. The column body_text is taken as the main value to form the segmentation algorithm. (Taken as X)\n2. Using Tfidf Vectorization method the text was vectorized.\n3. The body_text using the fit method was then fitted to the vectorization instance.\n4. Using mini-batch kmeans the data was fitted and labels was added at the end of the dataset.","0472a97d":"<h2> Implementing KNN Classifier to the dataset <\/h2>","ddba0038":"<h2> Wrangling the above dataset <\/h2>","b882b0cb":"<h2> Analyzing the classification model <\/h2>\n\n1. Since the neural network didin't provided a substantial increase in the overall accuracy for the model, the basic ML Models are taken into considerations for the prediction purposes.\n\n2. The Random Forest Classifier which possessed the higher accuracy, is taken as a initiator to help with the classification example. \n\n3. Further, the neural network can also be developed to attain a much higher accuracy.","74e9650e":"<h1> Basic Data Wrangling <\/h1>\n\n1. Deleting the not required column - (Last update)\n2. Resetting the index with the column SNo.\n3. Checking for missing and null values","a1f1fbb2":"<h2> Implementing the ML Algorithms <\/h2>\n\n<h2> Random Forest Regressor <\/h2>","e071c2d9":"<h2> Please Note <\/h2>\n\nThe following code runs for 26,000+ rows in the dataset hence the computatiion time is higher. It could take anywhere between a couple of hours to run the underlying code below hence the code has been marked as a markdown. The code could safely run and give the same values. \n\nAn already executed version of this code, executed on Rapids.ai Tesla P100 GPU has been added as a public dataset on Kaggle by me. The same is used here to do more analysis. The dataset is made available publically on kaggle under the url - https:\/\/www.kaggle.com\/aestheteaman01\/covid19-literary-analysis-dataset-covlad. The same is imported in this notebook.","a6582025":"<h2> The code for vectorization and K-Means is under <\/h2>\n\nRuntime for this code is higher as already discussed.","3e89b74d":"<h2> Findings from the Null value operation <\/h2>\n\n1. We find 0 null values for the columns paper_id, abstract, body_text.\n2. However some null values are present in the author and title column which has to be taken care of.\n\nWe first drop the duplicate values in the prior column and then check the null value again."}}