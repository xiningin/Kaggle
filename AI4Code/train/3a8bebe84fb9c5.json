{"cell_type":{"ce62fe48":"code","b2c160af":"code","52000514":"code","c7dbecbf":"code","04511350":"code","28517be0":"code","579f37f3":"code","3a4343c9":"code","8e159180":"code","bc740893":"code","b320bf9f":"code","4459235e":"code","b403c7bd":"code","477dfb9f":"markdown","a36a2b0b":"markdown","2d7e6c7d":"markdown","84cd2159":"markdown","322a5962":"markdown","1dfac56a":"markdown","a6d6df7b":"markdown","c858e141":"markdown","fdb75d87":"markdown","3a3b6bea":"markdown","022c73a7":"markdown","ca1ee04a":"markdown","3957ec8a":"markdown"},"source":{"ce62fe48":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import Imputer, StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import mean_squared_error\n\nfrom __future__ import print_function\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","b2c160af":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","52000514":"train = train[train.GrLivArea < 4000]\ntrain.shape","c7dbecbf":"missing = train.isnull().sum().sort_values(ascending = False)\nmissing_pct = missing \/len(train)\nfeatures_to_discard = list(missing_pct[missing_pct > 0.8].index)\n\nprint('features discared:', features_to_discard)\ntrain.drop(features_to_discard, axis=1, inplace = True)","04511350":"nominal_features = ['MSSubClass', 'MSZoning', 'LandContour', 'LotConfig', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', \n                   'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation', 'Heating',\n                   'GarageType', 'SaleType', 'SaleCondition']\nordinal_features = ['Street', 'CentralAir', 'LotShape', 'Utilities', 'LandSlope', 'OverallQual', 'OverallCond', 'ExterCond', 'ExterQual', \n                   'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'Electrical', 'KitchenQual',\n                   'Functional', 'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive']\n#the propotion of bedroom, bathroom, kitchen compare to total rooms maybe a good feature\ndiscrete_features = ['YearBuilt', 'YearRemodAdd', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr',\n                    'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'YrSold', ]\n#the proportion of living area compare to total area maybe a good feature\ncontinuous_features = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n                      'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch',\n                      'PoolArea', 'MiscVal', ]\n\n\nmisc_features = ['MoSold', ]","28517be0":"# Building Block for the full pipeline\n\noridnal_map = {\n    #qual, cond ect.\n    'No':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5, \n    #yes or no\n    'N':0, 'Y': 2, \n    # fence feature\n    'MnWw':1, 'GdWo':2, 'MnPrv':3, 'GdPrv':4,\n    #LotShape\n    'IR3':1, 'IR2':2, 'IR1':3, 'Reg':4,\n    #Utilities\n    'ELO':1, 'NoSeWa':2, 'NoSewr':3, 'AllPub':4,\n    #Land Slope\n    'Sev':1, 'Mod':2, 'Gtl':3,\n    #BsmtFin Type 1\/2\n    'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6,\n    #Electrical\n    'Mix':1, 'FuseP':2, 'FuseF':3, 'FuseA':4, 'SBrkr':5,\n    #Functional\n    'Sal':1, 'Sev':2, 'Maj2':3, 'Maj1':4, 'Mod':5, 'Min2':6, 'Min1':7, 'Typ':8,\n    #Garage Finish\n    'Unf':1, 'RFn':2, 'Fin':3,\n    #Paved Drive\n    'N':0, 'P':1, 'Y':2,\n    #Street\n    'Grvl':1, 'Pave':2,\n    #Basement exposure\n    'Mn':2, 'Av':3, 'Gd':4\n}\n\n\nclass OrdImputer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y = None):\n        return self\n    def transform(self, X, y = None):\n        df_x = pd.DataFrame(X, columns=ordinal_features)\n        df_x = df_x.fillna('No')\n        df_x =  df_x.applymap(lambda x:oridnal_map.get(x,x))\n        result = df_x.values\n        return result\n    \nclass CatImputerEncoder(BaseEstimator, TransformerMixin):\n    \"\"\" Impute categorical features, using most frequent strategy \n        and Encode categorical features using OneHot Encoder\n    \"\"\"\n    def __init__(self, features):\n        self.features = features\n        \n    def fit(self, X, y=None):\n        self.fill = pd.Series([X[c].value_counts().index[0]\n            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n            index=X.columns)\n        tmp = X.fillna(self.fill).values\n        self.encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n        self.encoder.fit(tmp)\n        self.categories_ = [feat + '_' + str(it) for feat, items in zip(self.features, self.encoder.categories_) for it in items]\n        return self\n\n    def transform(self, X, y=None):\n        return self.encoder.transform(X)\n\n\nclass FeatureEngneer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        \"\"\"\n        Feature Engneering here!!\n        \"\"\"\n        return\n    def fit(self, X, y = None):\n        return self\n    def transform(self, X, y = None):\n        return X\n\nclass Normalizer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y = None):\n        from scipy.stats import skew\n        tmp = pd.DataFrame(X)\n        n_uniq_values = tmp.apply(lambda x:len(x.unique()))\n        n_uniq_values_gt_100 = n_uniq_values[n_uniq_values > 100].index\n        sk = skew(tmp[n_uniq_values_gt_100])\n        self.index2normalize_ = n_uniq_values_gt_100[np.abs(sk) > 0.5]\n        return self\n    def transform(self, X, y = None):\n        X[:, self.index2normalize_] = np.log1p(X[:, self.index2normalize_])\n        return X\n\nclass FeatureEngneer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y = None):\n        return self\n    def transform(self, X, y = None):\n        return None\n\nclass Wrapper(BaseEstimator, TransformerMixin):\n        def __init__(self):\n            \"\"\"\n            For ordinal features, impute with 'No'\n            For discrete and nominal features, impute with most frequent values\n            For continuous features, impute with mean\/median, making it a hyperparameter for later search\n            \"\"\"\n            self.imputer =  ColumnTransformer([\n                ('ordinal', OrdImputer(), ordinal_features),\n                ('discrete', Imputer(strategy = 'most_frequent'), discrete_features),\n                ('continuous', Imputer(strategy='mean'), continuous_features),\n                 ('nominal', CatImputerEncoder(features = nominal_features), nominal_features),\n            ])\n            self.feature_engneer = FeatureEngneer()\n            \n        def fit(self, X, y = None):\n            self.imputer.fit(X)\n            self.feature_engneer.fit(X)\n            return self\n        \n        def transform(self, X, y = None):\n            transformed = self.imputer.transform(X)\n            fe = self.feature_engneer.transform(transformed)\n            if fe:\n                return np.c_[transformed, fe]\n            else:\n                return transformed\n","579f37f3":"from sklearn.preprocessing import RobustScaler\n\nnorm = Normalizer()\nstd_scaler = RobustScaler()\n\nfull_pipeline = Pipeline([\n    \n    # Including purifying the data and possibly some feature engneering.\n    # This is ugly, but I dont have a better solution right now.\n    ('expansion', Wrapper()),  \n    ('normalization', norm),\n    ('standardization', std_scaler),\n])","3a4343c9":"from sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(train, test_size = 0.2, shuffle= True, random_state = 1)\n\nprint('train set:', train_set.shape)\nprint('test set:', test_set.shape)","8e159180":"from sklearn.linear_model import Lasso, LassoCV\nfrom sklearn.metrics import mean_squared_error, make_scorer\n\nscorer = make_scorer(mean_squared_error, greater_is_better=False)\n\nlasso_reg = Lasso()\n\nprepare_predict_pipeline = Pipeline([\n    ('prepare', full_pipeline),\n    ('predict', lasso_reg),\n])\n\nparams = [\n    {\n     'predict__alpha':[0.0005]\n    }\n]\n\ngrid_search = GridSearchCV(prepare_predict_pipeline, param_grid=params, scoring=scorer, cv = 5, verbose=1)\ngrid_search.fit(train_set, np.log(train_set.SalePrice.copy()))","bc740893":"cvres = grid_search.cv_results_\nsorted([(np.sqrt(-score), para) for score, para in zip(cvres['mean_test_score'], cvres['params'])], reverse=False)[:10]","b320bf9f":"pred = grid_search.best_estimator_.predict(test_set)\nprint('result on test set:' ,np.sqrt(mean_squared_error(np.log(test_set.SalePrice), pred)))","4459235e":"y = np.log(train.SalePrice.copy())\ngrid_search.best_estimator_.fit(train, y)","b403c7bd":"pred = grid_search.best_estimator_.predict(test)\n\nresult = pd.DataFrame()\nresult['Id'] = test.Id\nresult['SalePrice'] = np.exp(pred)\nresult.to_csv('it2_result.csv', index=False)","477dfb9f":"## prepare a test set","a36a2b0b":"## Remove features with large proportion of missing values","2d7e6c7d":"# Prepare a data preparation pipeline","84cd2159":"No fancy feature engneering, no EDA, no ensembling many models, just a single pipeline with lasso regression.<br>\nI figure this could be a good start point for begginers.","322a5962":"In order to apply diffent types of impuation, preprocessing ect. Also, during the process, you may abtain a good sense about all the features, possibly some ideas about feature engneering.","1dfac56a":"## Retrain on all training data","a6d6df7b":"# Train and fine tune a model","c858e141":"# Make Predictions","fdb75d87":"* Discard outliers\n* Remove features which are missing k(defualt 80) percent of values\n* For ordinal features, fill na with 'No'\n* Map ordinal feature values to int\n* For discrete features, fill na with 'most frequent'\n* For continuous features, fill na with 'median\/ mean'\n* Log transform skewed continuous features\n* Standardize log-transformed continuouse features\n* For nominal features, fill na with 'most frequent'\n* One-Hot encode nominal features","3a3b6bea":"* Purifying the data. <br>\n    1.Missing value imputation; <br>\n    2.Convert all the features to numerical<br>\n* Feature engneering.<br>\n* Normalization, for skewed features<br>\n* Stadardization<br>","022c73a7":"## Workflow of the data preprocessing pipeline","ca1ee04a":"## Seperate different types of features","3957ec8a":"## Discard outliers"}}