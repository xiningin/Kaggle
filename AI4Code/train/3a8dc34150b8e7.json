{"cell_type":{"85d8e105":"code","a7c02d73":"code","5fb8eb12":"code","3cdcae65":"code","3527d2b0":"code","6b278231":"code","75ad5e08":"code","aca21580":"code","70738c3a":"code","62f801b7":"code","2948f84a":"code","a6c31b76":"markdown","174b8dbf":"markdown"},"source":{"85d8e105":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a7c02d73":"#Importing everything i need\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport warnings\nimport time\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor","5fb8eb12":"# Read the data\nX = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id')\nX_test_full = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice              \n\n#I decide to drop columns with NaN and not fill them so i can have less features but with more values\ncols_with_missing = [col for col in X.columns if X[col].isnull().any()]\nX.drop(cols_with_missing, axis=1, inplace=True)\n\nprint(X.info())","3cdcae65":"#I Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\n\ndef make_mi_scores(X, y):\n    '''This function will help me do some basic feature engineering \n    by calculating mutual information of the features i want\n    '''\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n\n\n#Here i plot some randomly chosen features to observe which one has high Mutual Information and which is not\nsns.set\nfeatures = [\"YearBuilt\", \"Utilities\", \"GarageArea\", \"2ndFlrSF\", \"GarageCars\", \"Heating\"]\nsns.relplot(\n   x=\"value\", y=\"SalePrice\", col=\"variable\", data=X.melt(id_vars=\"SalePrice\", value_vars=features), facet_kws=dict(sharex=False),\n);","3527d2b0":"X = X.copy()\n#I don't drop SalePrice here out of curiosity, so i can see the Mutual Information score it has with itself. I will drop later of course to avoid Data Leakage\nmi_scores = make_mi_scores(X, y)\nprint(mi_scores.head(50))\nplt.figure(dpi=100, figsize=(8, 5))\n\n#Here i plot the 20 features with the lowest Mutual Information score to see if any are equal to 0.\nplot_mi_scores(mi_scores.tail(20))","6b278231":"#I make a list of features that have mutual information higher or equal to 0 and lower or equal to 0.02 (boundaries included)\n#Features with MI=0 are irrelevant with the SalePrice and the others are weakly related to SalePrice, so i will try to find some #relation between them.\n#If relation is unimportant, all these features will be dropped\n#Boundaries 0 and 0.08 are randomly chosen and i will toggle them later see how the mean absolute error will react.\ncol_del = mi_scores[mi_scores.between(0, 0.08, inclusive=True)].index.tolist()\nprint(col_del)","75ad5e08":"#Now i am going to use PCA to see if any of these irrelevant features have any kind of relation between them(if they do i wont #discard them. But to use PCA i first have to Label Encode the categorical features and leave others as is.)\nlabel_encoder = LabelEncoder()\nfeatures = col_del\nX_label = X.loc[:, features]\n\nfor c in features:\n    if X_label[c].dtype ==\"object\":\n        X_label[c] = label_encoder.fit_transform(X_label[c])\n\n\n#I Standardize the data \nX_scaled = (X_label - X_label.mean(axis=0)) \/ X_label.std(axis=0)\n\n# Create principal components\npca = PCA()\nX_pca = pca.fit_transform(X_scaled)\n\n# Convert to dataframe\ncomponent_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\nX_pca = pd.DataFrame(X_pca, columns=component_names)\n\nloadings = pd.DataFrame(\n    pca.components_.T,  # transpose the matrix of loadings\n    columns=component_names,  # so the columns are the principal components\n    index=X_label.columns,  # and the rows are the original features\n)\nloadings","aca21580":"#Here i include the explained variance ratio of each Principal Component in a list as integers so i can iterate it\nexplained_variance =list(pca.explained_variance_ratio_)\n\ndef pca_result(expl_var_list, min_pct=0.15, c=0):\n    '''\n    This function iterates over the afforementioned list, \n    and shows whether all Principal Components are unimportant or not. \n    If all of them are unimportant, it deletes them all,\n    but if even one is important, the function suggests further examination of these features.\n    Principal components that are lesser or equal to min_pct are considered unimportant\n    because it shows very little correlation between features. \n    Also, its value is randomly chosen by me (but will always be close to 0) and i will toggle it\n    later to see how Mean Absolute Error reacts\n    '''\n\n    for z in expl_var_list:\n        if z <= min_pct:\n            c = c + 1\n    if c == len(loadings.index):\n        X.drop(col_del, axis=1, inplace=True)\n        print(\"All Principal Components appear to be unimportant so all features from col_del are deleted\")     \n    else:\n        print(\"Some Principal Components may not be useless, need further examination\") \n\npca_result(explained_variance)","70738c3a":"#Here i check what features are left\nprint(X.info())","62f801b7":"#Now that Feature Engineering is over,i will separate the target from the features and then \n#i will LabelEncode X dataframe because xgboost only deals with numeric columns\n\nX.drop(['SalePrice'], axis=1, inplace=True)\n\nfinal_features = X.columns.to_list()\nprint(type(final_features))\n\nfor i in final_features:\n    if X[i].dtype == \"object\":\n        X[i] = label_encoder.fit_transform(X[i]) \n\n# Check \nprint(X.dtypes)\n\n#Now i can safely break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n\nmy_model = XGBRegressor() #You can try here the tree_method='gpu_hist' argument if you have \n                          #a GPU with CUDA cores for faster execution\n\nparameters = {\n              'eval_metric':['mae'],\n              'learning_rate':  [0.1, 0.12 ], \n              'max_depth': [5, 6, 7],\n              'min_child_weight': [1, 2, 4],\n              'subsample': [0.7, 1],\n              'n_estimators': [500, 1000],\n              }\n\nxgb_grid = GridSearchCV(my_model,\n                        parameters,\n                        cv = 2,\n                        verbose=False,\n                        #n_jobs=1   Uncomment if you want to use the GPU\n                        )\n\n#Here i filter UserWarnings because currently XGBoost devs suggest so, in order to avoid a bug              \nwarnings.filterwarnings(action='ignore', category=UserWarning)\n\n# I fit the model with the grid, and also set up a simple minute counter to see how execution time reacts \n#when i execute XGBoost in CPU or GPU, and then keep the faster option out of those two   \nstart_time = time.time()\nxgb_grid.fit(X_train, y_train)\nduration = (time.time() - start_time)\/60\nprint(\"XGBOOST HyperParameter Tuning  %s minutes ---\" % + duration)\n\n#This prints the Grid's parameters that achieve the lowest Mean Absolute Error\nprint(xgb_grid.best_params_)\n\npredictions = xgb_grid.predict(X_valid)\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))","2948f84a":"# Preprocessing of test data, fit model\nX_test_full = X_test_full[final_features]\n\nfor i in final_features:\n    if X_test_full[i].dtype == \"object\":\n        X_test_full[i] = label_encoder.fit_transform(X_test_full[i]) \n\npreds_test = xgb_grid.predict(X_test_full) \n\n#The output is a DataFrame, ready to get submited\noutput = pd.DataFrame({'Id': X_test_full.index,\n                       'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False)","a6c31b76":"Feature engineering","174b8dbf":"In some of these plots, there is a visible correlation betweem the feature and SalePrice (*for example there is a moderately positive correlation between \"YearBuilt\" and \"SalePrice\", which totally makes sense)*, whereas other features appear to be almost totally irrelevant with SalePrice *(for instance \"Utilities\" and \"SalePrice\" seem to be fully unrelated which also makes some sense).*A matrix with every feature's Mutual Information though, would be more helpful, as the plots only allow us to take a glimpse.\n\n\n**NOTE:** I chose to use **Mutual Information** instead of Simple Correlation to examine the relationship between features and Target, because the former can capture every possible relation between them, while the latter can only capture correlation patterns, which is not enough for our analysis"}}