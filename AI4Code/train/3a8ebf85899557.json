{"cell_type":{"0a843578":"code","b859928f":"code","929084e3":"markdown","db0527b7":"markdown","20ee8051":"markdown","c5b138d4":"markdown","2b13cfb9":"markdown","e1048202":"markdown"},"source":{"0a843578":"import numpy as np\nimport pandas as pd","b859928f":"class LRGD:\n    def __init__(self):\n        self.theta=0\n        self.cost=0\n        \n    #hypothesis function\n    def hypothesis(self,X, n):  #n is the no. of features\n        h = np.ones((X.shape[0],1))\n        self.theta = theta.reshape(1,n+1)\n        for i in range(0,X.shape[0]):\n            h[i] = float(np.matmul(self.theta, X[i]))\n        h = h.reshape(X.shape[0])\n        return h\n    \n    #batch gradient descent\n    def BGD(self, alpha, num_iters, h, X, y, n):    #alpha is the learning rate and num_iters is the no. of iterations.\n        self.cost = np.ones(num_iters)\n        for i in range(0,num_iters):\n            self.theta[0] = self.theta[0] - (alpha\/X.shape[0]) * sum(h - y)\n            for j in range(1,n+1):\n                self.theta[j] = self.theta[j] - (alpha\/X.shape[0]) * sum((h-y) * X.transpose()[j])\n            h = hypothesis(X, n)\n            self.cost[i] = (1\/X.shape[0]) * 0.5 * sum(np.square(h - y))\n        self.theta = self.theta.reshape(1,n+1)\n        return self\n\n    #the main function in which we will use the above created function\n    def fit(self,X, y, alpha=0.0001, num_iters=10000):\n        n = X.shape[1]\n        one_column = np.ones((X.shape[0],1))\n        X = np.concatenate((one_column, X), axis = 1)\n        # the parameter vector\n        self.theta = np.zeros(n+1)\n        # hypothesis calculation\n        h = hypothesis(self, X, n)\n        # returning the optimized parameters by Gradient Descent\n        self.theta, self.cost = BGD(self,alpha,num_iters,h,X,y,n)\n        return self\n    \n    #predictions\n    def predit(self,X):\n        X = np.concatenate((np.ones((X.shape[0],1)), X),axis = 1)\n        predictions = hypothesis(self, X, X.shape[1] - 1)\n        return predictions","929084e3":"**For multi-variate data, only the linear term of each feature will be taken in the framing of the hypothesis.** \n\n*Let, x_1, x_2, \u2026 x_n, be the features on which the Target Outcome depends upon.* \n\nThen, the hypothesis for Multi-Variate Linear Regression:\n\n![image1.png](attachment:image.png)\n\nwhere theta_0, theta_1, theta_2, theta_3,\u2026., theta_n are the parameters\nand  h(x1,x2,x3,.......xn) is called the **hypothesis function** and is represented by h(x).","db0527b7":"*Here, vanilla means pure \/ without any adulteration. Its main feature is that we take small steps in the direction of the minima by taking gradient of the cost function. So BGD is also known as Vanilla.*","20ee8051":"h(x) in vector form can be represented by:\n![image2.png](attachment:image.png)","c5b138d4":"*We will put this optimization technique in Linear Regression problem*\n\n**In linear Regression cost function is defined as:**\n\n![image3.png](attachment:image.png)","2b13cfb9":"**The cost function is dependent on the hypothesis function.**\n\nAnd we need to find the minimum value of this cost function.\n\nSo, these parameters, theta_0, theta_1, theta_2, \u2026, theta_n have to assume such values for which the cost function (or simply cost) reaches to its minimum value possible. \n\nIn other words, the minima of the Cost Function have to be found out.","e1048202":"*here m is the no. of features*\n\nThe Batch Gradient descent algorithm is defined as :\n![image4.png](attachment:image.png)"}}