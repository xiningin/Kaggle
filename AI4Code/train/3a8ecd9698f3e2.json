{"cell_type":{"a9be26b4":"code","2bf66340":"code","2fcd0525":"code","d3e82c76":"code","202fea9a":"code","da45705e":"code","94ea7293":"code","783c021e":"code","5b35cca5":"code","934b5ca3":"code","bce76f52":"code","5a0c6b5e":"code","6e837415":"code","2f4f9eba":"code","cf9bed40":"code","f7c8e7fd":"markdown"},"source":{"a9be26b4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2bf66340":"from sklearn import svm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mlt\n# \u524d\u51e6\u7406\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# XGBoost\nimport xgboost as xgb\n\n# LightGBM\nimport lightgbm as lgb\n\n# CatBoost\nimport catboost as  cb\nfrom catboost import CatBoost, Pool\n\n\n# \u8a55\u4fa1\u6307\u6a19\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error","2fcd0525":"train = pd.read_csv(\"\/kaggle\/input\/ventilator-pressure-prediction\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/ventilator-pressure-prediction\/test.csv\")","d3e82c76":"train.info()","202fea9a":"train","da45705e":"features = [\"id\",\"breath_id\",\"R\",\"C\",\"time_step\",\"u_in\",\"u_out\"]\nx_train=train[features]\ny_train=train[\"pressure\"]","94ea7293":"from sklearn.model_selection import KFold\nfold = KFold(n_splits=5, shuffle=True, random_state=71)\ncv = list(fold.split(x_train,y_train))#\u3082\u3068\u3082\u3068\u304c generator \u306a\u305f\u3081\u660e\u793a\u7684\u306blist\u306b\u5909\u63db\u3059\u308b","783c021e":"cv","5b35cca5":"params = {\n    'objective': \"regression\",\n    'metric': \"mae\",\n    \"verbosity\" :-1,\n}","934b5ca3":"from sklearn.metrics import mean_absolute_error\nimport optuna.integration.lightgbm as lgb_o\n\ndef fit_lgbm(X, \n             y, \n             cv, \n             params: dict=None, \n             ):\n    \"\"\"lightGBM \u3092 CrossValidation \u306e\u67a0\u7d44\u307f\u3067\u5b66\u7fd2\u3092\u884c\u306a\u3046 function\"\"\"\n\n    # \u30d1\u30e9\u30e1\u30fc\u30bf\u304c\u306a\u3044\u3068\u304d\u306f\u3001\u7a7a\u306e dict \u3067\u7f6e\u304d\u63db\u3048\u308b\n    if params is None:\n        params = {}\n    \n    scores = {}\n    models = []\n    # training data \u306e target \u3068\u540c\u3058\u3060\u3051\u306e\u30bc\u30ed\u914d\u5217\u3092\u7528\u610f\n    oof_pred = np.zeros_like(y, dtype=np.float)\n\n    evaluation_results = []\n    #cv\u3057\u305f\u5206\u306e\u30e2\u30c7\u30eb\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u4fdd\u5b58 \n    #all_best_params=[]\n    for i, (idx_train, idx_valid) in enumerate(cv): #cv\u306b\u306ftrain\u3068test\u306eindex\u756a\u53f7\u304cidx_train\u3068idx_valid\u306b\u5165\u308b\n        # \u3053\u306e\u90e8\u5206\u304c\u4ea4\u5dee\u691c\u8a3c\u306e\u3068\u3053\u308d\u3067\u3059\u3002\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092 cv instance \u306b\u3088\u3063\u3066\u5206\u5272\u3057\u307e\u3059\n        # training data \u3092 trian\/valid \u306b\u5206\u5272\n        #array\u578b\u304b\u3089\u5404cv\u3067\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u756a\u53f7\u3092\u6307\u5b9a\u3057\u3066train\u3068test\u3092\u4f5c\u308b\n        x_train, y_train = X[idx_train], y[idx_train]#x\u306farray\u578b\u3067dataframe\u578b\u3067\u306f\u306a\u3044\n        x_valid, y_valid = X[idx_valid], y[idx_valid]\n\n        #lgb\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u4f5c\u6210\n        lgb_train=lgb.Dataset(x_train,y_train)\n        lgb_eval=lgb.Dataset(x_valid,y_valid)#val\u30c7\u30fc\u30bf\n        \n        #\u8a55\u4fa1\u95a2\u6570\u3092\u4fdd\u5b58\u3059\u308b\n        evaluation_results_i = {} \n        #oputna\u3067\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u4fdd\u5b58\u3059\u308b\n        #best_params = {}\n\n        #\u5b66\u7fd2\n        gbm = lgb.train(\n            params,\n            lgb_train,\n            num_boost_round=100,\n            valid_sets=[lgb_train,lgb_eval],#mse\u306e\u63a8\u79fb\u3092\u4fdd\u5b58\u3059\u308b\n            evals_result=evaluation_results_i,\n            valid_names=['train', 'valid'],\n            early_stopping_rounds=10,\n            )\n\n\n        #best_params = gbm.params\n        #all_best_params.append(best_params)\n    \n        #val\u30c7\u30fc\u30bf\u306b\u5f53\u3066\u306f\u3081\u3066\u63a8\u8ad6\n        #pred_i\u306f\u3069\u3093\u306a\u578b??\n        pred_i = gbm.predict(x_valid,num_iteration=gbm.best_iteration)#\u63a8\u8ad6\n        \n        oof_pred[idx_valid] = pred_i#oof_pred(\u30bc\u30ed\u914d\u5217)\u306ecv\u3057\u305f\u6642\u306ecv\u3057\u305f\u6642\u306e\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306eindex\u756a\u53f7\u306b\u4e88\u6e2c\u5024\u3092\u5165\u308c\u308b\n        models.append(gbm)#\u30e2\u30c7\u30eb\u3092model\u914d\u5217\u306b\u8ffd\u52a0\n        evaluation_results.append(evaluation_results_i)\n        print(f'Fold {i} MAE: {mean_absolute_error(y_valid, pred_i) ** .5:.4f}')\n\n    scores = np.sqrt(mean_absolute_error(y,oof_pred))\n        \n    #score = mean_squared_error(y, oof_pred) ** .5\n    print('-' * 50)\n    print('FINISHED | Whole MAE: {:.4f}'.format(scores))\n    return oof_pred, models ,evaluation_results","bce76f52":"oof, models ,evaluation_results= fit_lgbm(x_train.values,y_train.values,cv, params=params)","5a0c6b5e":"def visualize_importance(models, feat_train_df):\n    \"\"\"lightGBM \u306e model \u914d\u5217\u306e feature importance \u3092 plot \u3059\u308b\n    CV\u3054\u3068\u306e\u30d6\u30ec\u3092 boxen plot \u3068\u3057\u3066\u8868\u73fe\u3057\u307e\u3059.\n\n    args:\n        models:\n            List of lightGBM models\n        feat_train_df:\n            \u5b66\u7fd2\u6642\u306b\u4f7f\u3063\u305f DataFrame\n    \"\"\"\n    feature_importance_df = pd.DataFrame()\n    for i, model in enumerate(models):\n        _df = pd.DataFrame()\n        _df['feature_importance'] = model.feature_importance()\n        _df['column'] = feat_train_df.columns\n        _df['fold'] = i + 1\n        feature_importance_df = pd.concat([feature_importance_df, _df], \n                                          axis=0, ignore_index=True)\n\n    order = feature_importance_df.groupby('column')\\\n        .sum()[['feature_importance']]\\\n        .sort_values('feature_importance', ascending=False).index[:50]#50\u884c\u76ee\u307e\u3067\u62bd\u51fa\n\n    fig, ax = plt.subplots(figsize=(8, max(6, len(order) * .25)))\n    sns.boxenplot(data=feature_importance_df, \n                  x='feature_importance', \n                  y='column', \n                  order=order, \n                  ax=ax,\n                  palette='viridis', \n                  orient='h')\n    ax.tick_params(axis='x', rotation=90)\n    ax.set_title('Importance')\n    ax.grid()\n    fig.tight_layout()\n    return fig, ax\n\nfig, ax = visualize_importance(models, x_train)","6e837415":"pred = 0\nfor i in range(5):\n    pred += (models[0].predict(test))\/5","2f4f9eba":"submission = pd.DataFrame({\"id\":test[\"id\"],\"pressure\":pred})","cf9bed40":"submission.to_csv(\"submission.csv\",index=False)","f7c8e7fd":"### \u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u8a2d\u5b9a\n- \u8a55\u4fa1\u6307\u6a19\u306f\u5e73\u5747\u7d76\u5bfe\u8aa4\u5dee"}}