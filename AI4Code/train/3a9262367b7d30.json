{"cell_type":{"63db34c3":"code","8c4b0e26":"code","720b4e6b":"code","26b8bb45":"code","c07d5b08":"code","f747a4e1":"code","36985f31":"code","4f837c89":"code","e7411a76":"code","fcebe6e8":"code","e435917f":"code","a4ead588":"code","9cf0ba67":"code","6eea2ea4":"code","e130c959":"code","845c6fef":"code","edf23eca":"code","3f6b7e9d":"code","3256da91":"code","3224d72f":"code","b8539a3b":"code","99df4b4f":"code","a5f6d627":"code","87f34a7b":"code","7d9ac624":"code","db8a487c":"code","77302754":"code","e021df86":"code","2e248cd5":"code","933a1d5a":"code","94d97688":"code","b4e5c083":"code","21a74fd8":"code","0b1cd9f6":"code","7a835d04":"code","b8b481ab":"code","9607795c":"markdown","b35c9052":"markdown","9aaea90b":"markdown","23d82238":"markdown","2584f8aa":"markdown","5bd42f91":"markdown","a633cdc2":"markdown","f852e9b4":"markdown","c13648ba":"markdown","bfeec693":"markdown","6ee55e1a":"markdown","944b814d":"markdown","0c42affc":"markdown","c1ecc24b":"markdown","2350375d":"markdown","f7f40400":"markdown","9557f662":"markdown","300151f1":"markdown","4e63a2b7":"markdown","d8908af7":"markdown","d78a44c8":"markdown","052912e0":"markdown","7606cd9f":"markdown","248214ca":"markdown","9f1d0913":"markdown","ef30c57a":"markdown","838bc2f5":"markdown"},"source":{"63db34c3":"import numpy as np\nimport pandas as pd\nimport gc\nimport json\nimport subprocess\n\nfrom tqdm import tqdm_notebook as tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns","8c4b0e26":"DIR = '..\/input\/tensorflow2-question-answering\/'\nPATH_TRAIN = DIR + 'simplified-nq-train.jsonl'\nPATH_TEST = DIR + 'simplified-nq-test.jsonl'","720b4e6b":"N_TRAIN_bytes = subprocess.check_output('wc -l {}'.format(PATH_TRAIN), shell=True)\nN_TEST_bytes = subprocess.check_output('wc -l {}'.format(PATH_TEST), shell=True)\n\nN_TRAIN = int(N_TRAIN_bytes.split()[0])\nN_TEST = int(N_TEST_bytes.split()[0])","26b8bb45":"print(N_TRAIN)\nprint(N_TEST)","c07d5b08":"df_test = pd.read_json(PATH_TEST, orient='records', lines=True, dtype={'example_id':np.dtype('object')})","f747a4e1":"df_test","36985f31":"json_train_head = []\nN_HEAD = 10\n\nwith open(PATH_TRAIN, 'rt') as f:\n    for i in range(N_HEAD):\n        json_train_head.append(json.loads(f.readline()))","4f837c89":"df_train_head = pd.DataFrame(json_train_head)","e7411a76":"df_train_head","fcebe6e8":"df_train_head.iloc[0,:].loc['long_answer_candidates']","e435917f":"df_train_head.iloc[0,:].loc['annotations']","a4ead588":"del df_train_head, df_test\ngc.collect()","9cf0ba67":"n_long_candidates_train = np.zeros(N_TRAIN)\nt_long_train = np.zeros((N_TRAIN,2))\nt_yesno_train = []","6eea2ea4":"with open(PATH_TRAIN, 'rt') as f:\n    for i in tqdm(range(N_TRAIN)):\n        dic = json.loads(f.readline())\n        n_long_candidates_train[i] = len(dic['long_answer_candidates'])\n        t_long_train[i,0] = dic['annotations'][0]['long_answer']['start_token']\n        t_long_train[i,1] = dic['annotations'][0]['long_answer']['end_token']\n        t_yesno_train.append(dic['annotations'][0]['yes_no_answer'])","e130c959":"n_long_candidates_test = np.zeros(N_TEST)","845c6fef":"with open(PATH_TEST, 'rt') as f:\n    for i in tqdm(range(N_TEST)):\n        dic = json.loads(f.readline())\n        n_long_candidates_test[i] = len(dic['long_answer_candidates'])","edf23eca":"plt.style.use('seaborn-darkgrid')\nplt.style.use('seaborn-poster')","3f6b7e9d":"pd.Series(n_long_candidates_train).describe()","3256da91":"pd.Series(n_long_candidates_test).describe()","3224d72f":"plt.hist(n_long_candidates_train, bins=64, alpha=0.5, color='c', label='train')\nplt.xlabel('long answer candidates')\nplt.ylabel('samples')\nplt.legend()","b8539a3b":"plt.hist(n_long_candidates_train[n_long_candidates_train < np.max(n_long_candidates_test)], density=True, bins=64, alpha=0.5, color='c', label='train')\nplt.hist(n_long_candidates_test, density=True, bins=64, alpha=0.5, color='orange', label='test')\nplt.xlabel('long answer candidates')\nplt.ylabel('sample proportion')\nplt.legend()","99df4b4f":"plt.hist(t_yesno_train, bins=[0,1,2,3], align='left', density=True, rwidth=0.6, color='lightseagreen', label='train')\nplt.xlabel('yes-no answer')\nplt.ylabel('sample proportion')\nplt.legend()","a5f6d627":"pd.Series(t_long_train[:,0]).describe()","87f34a7b":"pd.Series(t_long_train[:,1]).describe()","7d9ac624":"print('{0:.1f}% of start tokens are -1.'.format(np.sum(t_long_train[:,0] < 0) \/ N_TRAIN * 100))\nprint('{0:.1f}% of end tokens are -1.'.format(np.sum(t_long_train[:,1] < 0) \/ N_TRAIN * 100))","db8a487c":"np.sum(t_long_train[:,0] * t_long_train[:,1] < 0)","77302754":"# no_answer_state[1,:] is the number of train data whose start token and end token are -1\n# no_answer_state[:,1] is the number of train data whose yes-no answer is 'NONE'\n\nno_answer_state = np.zeros((2,2))\nno_answer_state[1,1] = np.sum((t_long_train[:,0]==-1) * (np.array([ 1 if t=='NONE' else 0 for t in t_yesno_train ])))\nno_answer_state[1,0] = np.sum((t_long_train[:,0]==-1) * (np.array([ 0 if t=='NONE' else 1 for t in t_yesno_train ])))\nno_answer_state[0,1] = np.sum((t_long_train[:,0]>=0) * (np.array([ 1 if t=='NONE' else 0 for t in t_yesno_train ])))\nno_answer_state[0,0] = np.sum((t_long_train[:,0]>=0) * (np.array([ 0 if t=='NONE' else 1 for t in t_yesno_train ])))                             ","e021df86":"no_answer_state","2e248cd5":"sns.heatmap(no_answer_state \/ N_TRAIN, annot=True, annot_kws={\"size\": 25}, fmt='.3f', vmin=0, vmax=1, cmap='Blues_r')","933a1d5a":"del n_long_candidates_train, n_long_candidates_test, t_long_train, t_yesno_train, no_answer_state\ngc.collect()","94d97688":"q_lens_train = np.zeros(N_TRAIN)\nd_lens_train = np.zeros(N_TRAIN)","b4e5c083":"with open(PATH_TRAIN, 'rt') as f:\n    for i in tqdm(range(N_TRAIN)):\n        dic = json.loads(f.readline())\n        q_lens_train[i] = len(dic['question_text'].split())\n        d_lens_train[i] = len(dic['document_text'].split())","21a74fd8":"q_lens_test = np.zeros(N_TEST)\nd_lens_test = np.zeros(N_TEST)","0b1cd9f6":"with open(PATH_TEST, 'rt') as f:\n    for i in tqdm(range(N_TEST)):\n        dic = json.loads(f.readline())\n        q_lens_test[i] = len(dic['question_text'].split())\n        d_lens_test[i] = len(dic['document_text'].split())","7a835d04":"plt.hist(q_lens_train, density=True, bins=8, alpha=0.5, color='c', label='train')\nplt.hist(q_lens_test, density=True, bins=8, alpha=0.5, color='orange', label='test')\nplt.xlabel('question length')\nplt.ylabel('sample proportion')\nplt.legend()","b8b481ab":"plt.hist(d_lens_train, density=True, bins=64, alpha=0.5, color='c', label='train')\nplt.hist(d_lens_test, density=True, bins=64, alpha=0.5, color='orange', label='test')\nplt.xlabel('document length')\nplt.ylabel('sample proportion')\nplt.legend()","9607795c":"We can see significant class imbalance in yes-no answer labels.","b35c9052":"Some of data for long answers are swamped with a lot of candidates (**7946 in maximum!**):","9aaea90b":"Let us look into word counts of question texts & document texts.","23d82238":"### 3-2. Visualization","2584f8aa":"#### 3-2-1. Word counts of question text","5bd42f91":"### 2-2-2. Yes-no answer labels","a633cdc2":"## 2-1. Obtain data","f852e9b4":"# 2. Data Visualization","c13648ba":"We can see below that nearly half of the long answers have start\/end token -1.  \nIn other words, there are a considerable number of '**NO ANSWERS**' in long answer labels, not only in yes-no labels:","bfeec693":"### 3-1. Obtain data","6ee55e1a":"As we know, one of the most common way to convert .jsonl file into pd.DataFrame is  `pd.read_json(FILENAME, orient='records', lines=True)`:","944b814d":"Desciption of end token labels:","0c42affc":"## 2-2. Visualization","c1ecc24b":"Description of start token labels:","2350375d":"### 2-2-1. Number of long answer candidates","f7f40400":"# 1. Load .jsonl file iteratively","9557f662":"#### 3-2-2. Word counts of document text","300151f1":"The heatmap below tells us that:\n- when the start token and\/or the end token are -1, yes-no answer is 'NONE'\n- yes-no answer 'NONE' does not always mean that the start token and\/or the end token are -1","4e63a2b7":"However, since we have a **HUGE train dataset** for this competition, Kaggle Notebook RAM cannot afford this method.\nInstead, we probablly have to load the train dataset iteratively:","d8908af7":"## 3. Text Word Counts","d78a44c8":"We must be cautious that **\"short answer\" for this competition corresponds to \"yes-no answer\" in the original dataset**.  ","052912e0":"Let us have fun!  \nComments and recommendations will be welcomed ;)","7606cd9f":"# TL;DR\nThe dataset is **TOO LARGE** for the Kaggle Notebook RAM to load at once.","248214ca":"If the start token is -1, the corresponding end token is also -1:","9f1d0913":"# 0. Preparation","ef30c57a":"### 2-2-3. Long answer labels","838bc2f5":"### 0-1. Number of samples in train & test dataset"}}