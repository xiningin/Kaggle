{"cell_type":{"fa436083":"code","bf5d288e":"code","2ddac645":"code","76175c04":"code","37286a6b":"code","27094c1b":"code","d1fc20da":"code","a36d3ab2":"code","25184c94":"code","60e6c94b":"code","93d18bc7":"code","ca065e07":"code","2038a8c8":"code","ceba6dc3":"code","e3da0323":"code","ab22df1e":"code","aa0933da":"code","bffe0c33":"code","19a14622":"code","dadb4b60":"code","a0160a32":"code","cddb9659":"code","2f505991":"code","fd5210d2":"code","d0c9035c":"code","aebb3447":"code","5b0976de":"code","e0465de8":"code","64542ae7":"code","58aa12e2":"code","92aaa70e":"code","7f2c8831":"code","c643bafe":"code","f1a5da22":"code","ffd6104b":"code","14e36953":"code","8b148c46":"code","1675da27":"code","a8d79e6f":"code","b96fff7d":"code","df4ab39b":"code","603a3d28":"code","8983bdc8":"code","ceaa30b7":"code","21434fce":"code","157eaf1b":"code","9ac934ad":"code","978ee519":"code","9dd5734c":"code","3908263e":"code","5998e696":"code","66bbe70b":"code","6aaf10a5":"code","a97b1a21":"code","f2d58169":"code","87b92e7d":"code","5c392b2c":"code","ad87d227":"code","46528403":"code","2fee412b":"code","58895977":"code","c142800b":"code","0076cb6f":"code","31b71334":"code","afafcebc":"code","6a406275":"code","1acab391":"code","1097375c":"code","ded20821":"code","ca6eac3a":"code","b62767d6":"code","1dcc246b":"code","cb9b1954":"code","cc975c42":"code","3a0aa54c":"code","624325e0":"code","67a9125b":"code","6ce84fb0":"code","585bb4d9":"code","8e9fbbcd":"code","4cf28d15":"code","8376876c":"code","df36beac":"code","04d00dc2":"code","20f5f123":"code","c86d1ec7":"code","5e174b9f":"code","a457344e":"code","98a69531":"code","1d1a08df":"code","b72c32c5":"code","f029b31a":"code","c7df59e9":"code","dc686262":"code","27c5f614":"code","8d2cdb5d":"code","b8fcd9d2":"code","f81b1614":"code","77303453":"code","32744e33":"code","703e620a":"code","a3d7b524":"code","b513e79f":"code","f1a66c92":"code","7d24a0c0":"code","ca5c416d":"code","0654e687":"markdown","3dcffea1":"markdown","cf5a0a04":"markdown","cf63035e":"markdown","5977b92f":"markdown","5fb17aab":"markdown","12ced204":"markdown","02f232db":"markdown","98cb6c9a":"markdown","7fede5b4":"markdown","92c2e985":"markdown","35229e81":"markdown","8a4ac76a":"markdown","acf13f94":"markdown","429a74f1":"markdown","700e272d":"markdown","252ce6cc":"markdown","83df40ad":"markdown","aaacbd86":"markdown","41eee782":"markdown","22c22eb1":"markdown","a522fa36":"markdown","bc15fac8":"markdown","9759fd9e":"markdown","c2a49949":"markdown","9b128de8":"markdown","f4f9bf0c":"markdown","ce0b242d":"markdown","a6e591f5":"markdown","08fea4ee":"markdown","bdc55d2f":"markdown","40738f0e":"markdown","3840deb9":"markdown","d6c557e9":"markdown","cbc40af8":"markdown","478eb8c3":"markdown","9fea3220":"markdown","234e115b":"markdown","aa781410":"markdown","0cf082d5":"markdown","94d4dd91":"markdown","1260acb3":"markdown","fb389c01":"markdown","a8ef545e":"markdown","f7d18691":"markdown","d47c36dc":"markdown","bbe2fb07":"markdown","f6935977":"markdown","cc5ca36e":"markdown","26c47b25":"markdown","ac73484d":"markdown","223462fa":"markdown","a29f8576":"markdown","ea44b273":"markdown","801e500c":"markdown","fc489d43":"markdown","4b245d32":"markdown","03819112":"markdown","17c9dc38":"markdown","63e21f2a":"markdown","9eb735be":"markdown","1fbc3bde":"markdown","2dd2e743":"markdown","6bc010f2":"markdown","853ddaeb":"markdown","fbc38707":"markdown","e1931639":"markdown","b0ab96f1":"markdown","bee5b871":"markdown","988fd2bd":"markdown","0ee59f80":"markdown","57b4b37a":"markdown","182bd939":"markdown","5dbda444":"markdown","940271bd":"markdown","61adf77c":"markdown","ce3b78e3":"markdown","d0d18c35":"markdown","5acfe017":"markdown","5f5bd347":"markdown","79090dea":"markdown","9d2312c7":"markdown","25eefccb":"markdown","ca6f6ee8":"markdown","99677f0c":"markdown","c403dea9":"markdown","c768c4b7":"markdown","f97529e3":"markdown","0cc208a9":"markdown","25d69f0f":"markdown","06187565":"markdown","ab8c3f64":"markdown","d2c60038":"markdown","b6bbb44f":"markdown","88301823":"markdown","e76ac8f5":"markdown","22d68214":"markdown","0f19a6e4":"markdown","362789ad":"markdown","7886d888":"markdown","3bbcdd7b":"markdown","25d01f96":"markdown","739d57e8":"markdown"},"source":{"fa436083":"pip install emd","bf5d288e":"import sys \nimport warnings\nimport numpy as np\nfrom numpy import log\nimport pandas as pd\nimport itertools\nimport datetime as dt\nimport calendar\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.figure_factory as ff\nimport pylab\nimport seaborn as sns # used for plot interactive graph.\n\n#from numpy.random import seed\n\nfrom scipy import signal\nfrom scipy import stats\nfrom scipy.stats import randint\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler # for standardization of da\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline # pipeline making\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn import metrics # for the check the error and accuracy of the model\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.cluster import KMeans\n\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport statsmodels.api as sm\n\nimport keras\nfrom keras import optimizers\nfrom keras.utils import plot_model\nfrom keras.models import Sequential, Model\nfrom keras.layers.convolutional import Conv1D, MaxPooling1D\nfrom keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Flatten\nfrom keras.utils import to_categorical\nfrom keras.optimizers import SGD \nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import np_utils\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\n\nimport tensorflow as tf\n#from tensorflow.random import set_seed #as tf_set_random_seed\n\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")\ninit_notebook_mode(connected=True)\n\n\n#############################################\n#\n# In kaggle kernal requires 'pip install emd' \n#\n#############################################\nimport emd","2ddac645":"plt.style.use('dark_background')\nplt.rcParams['axes.prop_cycle']\npd.set_option('precision', 2)\n\nnp.random.seed(42)\ntf.random.set_seed(42)\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))","76175c04":"# read csv into dataframe\ndf = pd.read_csv('..\/input\/hotel-load-and-solar\/hotel_load_and_solar_2016-05-19_2020-09-21.csv', parse_dates=['Datetime'], index_col=['Datetime'])\nprint(df.describe())\ndf","37286a6b":"df.isnull().sum()","27094c1b":"df.columns = ['meter','solar'] # convenient renaming (all units in kW)\ndf[df<.001] = 0\ndf['load'] = df['meter'].values + df['solar'].values","d1fc20da":"plt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(df['2016-11-14':'2016-11-20'])\nplt.legend(['Meter (kW)','Solar (kW)','Load (kW)'])","a36d3ab2":"dfds = df.resample('D').mean()\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(dfds['load'],label='Load (kW)')\nplt.legend()","25184c94":"delta = 4*24\nt_begin = 0\nt_end = 4*24\n\nL = df.shape[0]\nn = int(L\/delta) - 1 # keeps from getting too close to the end\n\nd = df['load'][t_begin:t_end].values.reshape(delta,1)\n\nfor i in range(n):\n    t_begin += delta\n    t_end += delta\n    d_new = df['load'][t_begin:t_end].values.reshape(delta,1)\n    d = np.concatenate([d,d_new],axis=1)\n\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(d,alpha=0.04)\nplt.xlabel('Timesteps [15 min]')\nplt.ylabel('Load (kW)')\nplt.title('Daily Data (Beginning at 0:00)')","60e6c94b":"delta = 4*24*7\nt_begin = 4*24*6 # first day of data is monday, so begin graphing on a sunday\nt_end = t_begin + delta\n\nL = df.shape[0]\nn = int(L\/delta) - 2 # keeps from getting too close to the end\n\nd = df['load'][t_begin:t_end].values.reshape(delta,1)\n\nfor i in range(n):\n    t_begin += delta\n    t_end += delta\n    d_new = df['load'][t_begin:t_end].values.reshape(delta,1)\n    d = np.concatenate([d,d_new],axis=1)\n\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(d,alpha=0.08)\nplt.xlabel('Timesteps [15 min]')\nplt.ylabel('Load (kW)')\nplt.title('Weekly Data (Beginning on Sunday)')","93d18bc7":"df = df[:'2020-01-31']","ca065e07":"plt.figure(num=None, figsize=(10, 5), dpi=80)\nplt.plot(df['2016-11-12':'2016-11-13']['solar'])\nplt.ylabel('Power (kW)')\nplt.title('Solar Bad Data Example')\ndf = df['2016-11-14':]","2038a8c8":"df.drop('meter',axis=1, inplace=True)\ndf.drop('solar',axis=1, inplace=True)","ceba6dc3":"threshold = 3\n\nz = np.abs(stats.zscore(df)) # vector of z-scores (absolute values of..)\nzi = np.where(z>threshold)[0] # array of outlier indices\nprint ('Number of z-score outliers: ',len(zi))","e3da0323":"plt.figure(num=None, figsize=(10, 5), dpi=80)\nplt.hist(zi, bins=range(int(min(zi)),int(max(zi)),10),color = \"skyblue\", ec=\"skyblue\")\nplt.xlabel('Index of outlier data')\nplt.ylabel('Occurrences')","ab22df1e":"km = KMeans(n_clusters=7,random_state=0).fit(zi.reshape(-1,1))\ncc = km.cluster_centers_\nprint('Mean index of the outlier clusters: \\n\\n', cc,'\\n')\n\nfor i in range(cc.shape[0]):\n    print('Cluster %d mean datetime: %s' % (i,df.index[int(cc[i])]))","aa0933da":"plt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(df['2017-8-6':'2017-8-15']['load'].values,label='2017')\nplt.plot(df['2018-8-6':'2018-8-15']['load'].values,label='2018')\ndf['2017-8-7 7:00':'2017-8-14 11:00']['load'] = df['2018-8-7 7:00':'2018-8-14 11:00']['load'].values\nplt.plot(df['2017-8-6':'2017-8-15']['load'].values,label='2017 fixed')\nplt.legend()","bffe0c33":"plt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(df['2018-08-23':'2018-08-25'].values,label='2018')\nplt.plot(df['2019-08-23':'2019-8-25'].values,label='2019')\ndf['2018-08-24 7:00':'2018-8-24 21:00'] = df['2019-08-24 7:00':'2019-8-24 21:00'].values\nplt.plot(df['2018-08-23':'2018-08-25'].values,label='2018 fixed')\nplt.legend()","19a14622":"plt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(df['2019-12-12':'2019-12-14'].values,label='2019')\nplt.plot(df['2018-12-12':'2018-12-14'].values,label='2018') \ndf['2019-12-13 10:00':'2019-12-13 20:00'] = df['2018-12-13 10:00':'2018-12-13 20:00'].values\nplt.plot(df['2019-12-12':'2019-12-14'].values,label='2019 fixed')\nplt.legend()","dadb4b60":"plt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(df['2017-10-22':'2017-10-24'].values,label='2017')\nplt.plot(df['2018-10-22':'2018-10-24'].values,label='2018')\ndf['2017-10-22 15:00':'2017-10-24 17:00']['load'] = df['2018-10-22 15:00':'2018-10-24 17:00']['load'].values\nplt.plot(df['2017-10-22':'2017-10-24'].values,label='2017 fixed')\nplt.legend()","a0160a32":"plt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(df['2016-12-15':'2016-12-16'].load.values,label='2016')\nplt.plot(df['2017-12-15':'2017-12-16'].load.values,label='2017')\ndf['2016-12-15 12:00':'2016-12-16 12:00'] = df['2017-12-15 12:00':'2017-12-16 12:00'].values\nplt.plot(df['2016-12-15':'2016-12-16'].load.values,label='2016 fixed')\nplt.legend()","cddb9659":"plt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(df['2019-7-8'].load.values,label='2019')\nplt.plot(df['2018-7-8'].load.values,label='2018')\ndf['2019-7-8 8:00':'2019-7-8 13:00'] = df['2018-7-8 8:00':'2018-7-8 13:00'].values\nplt.plot(df['2019-7-8'].load.values,label='2019 fixed')\nplt.legend()","2f505991":"plt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(df['2018-7-29':'2018-7-31'].values,label='2018')\nplt.plot(df['2019-7-29':'2019-7-31'].values,label='2019')\ndf['2018-7-30 1:00':'2018-7-30 11:00']['load'] = df['2019-7-30 1:00':'2019-7-30 11:00']['load'].values\nplt.plot(df['2018-7-29':'2018-7-31'].values,label='2018 fixed')\nplt.legend()","fd5210d2":"delta = 4*24\nt_begin = 0\nt_end = 4*24\n\nL = df.shape[0]\nn = int(L\/delta) - 1 # keeps from getting too close to the end\n\nd = df['load'][t_begin:t_end].values.reshape(delta,1)\n\nfor i in range(n):\n    t_begin += delta\n    t_end += delta\n    d_new = df['load'][t_begin:t_end].values.reshape(delta,1)\n    d = np.concatenate([d,d_new],axis=1)\n\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(d,alpha=0.04)\nplt.xlabel('Timesteps [15 min]')\nplt.ylabel('Load (kW)')\nplt.title('Daily Data (Beginning at 0:00)')","d0c9035c":"scaler = MinMaxScaler()\n\ndfn = df.copy(deep=True)\ndfn['load'] = scaler.fit_transform(df['load'].values.reshape(-1,1))\ndfn","aebb3447":"ddfn = dfn.diff().fillna(method='bfill') # witout .fillna() the first row would be NaN\nddfn","5b0976de":"plt.figure(num=None, figsize=(10, 5), dpi=80)\nplt.hist(df.values,bins=50)\nplt.xlabel('Load (kW)')\nplt.ylabel('Occurences')","e0465de8":"plt.figure(num=None, figsize=(10, 5), dpi=80)\nplt.hist(ddfn.values,bins=50)\nplt.xlabel('Load (kW)')\nplt.ylabel('Occurences')","64542ae7":"plt.figure(num=None, figsize=(10, 5), dpi=80)\n\nax1 = plt.subplot(121)\nres = stats.probplot(df.values.flatten(), dist=\"norm\", plot=plt)\nax1.set_title('Load QQ')\n\nax2 = plt.subplot(122)\nres = stats.probplot(ddfn.values.flatten(), dist=\"norm\", plot=plt)\nax2.set_title('Load QQ normalized and time-differenced')","58aa12e2":"df_rm = df.rolling(35041,center=True).mean() # window weights are equal\ndf_rv = df.rolling(35041,center=True).var() # window weights are equal\n\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\n\nplt.subplot(121)\nplt.plot(df_rm['load'],label='Load (kW)')\nplt.title('Rolling 1 Year Mean')\nplt.legend()\n\nplt.subplot(122)\nplt.plot(df_rv['load'],label='Load (kW)')\nplt.title('Rolling 1 Year Variance')\nplt.legend()\n","92aaa70e":"ddfn_rm = ddfn.rolling(35041,center=True).mean() # window weights are equal\nddfn_rv = ddfn.rolling(35041,center=True).var() # window weights are equal\n\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\n\nplt.subplot(121)\nplt.plot(ddfn_rm['load'],label='Load (kW)')\nplt.title('Rolling 1 Year Mean on Normalized and Differenced Timeseries')\nplt.legend()\n\nplt.subplot(122)\nplt.plot(ddfn_rv['load'],label='Load (kW)')\nplt.title('Rolling 1 Year Variance on Normalzied and Differenced Timeseries')\nplt.legend()\n","7f2c8831":"result = adfuller(df.values)\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n\tprint('\\t%s: %.3f' % (key, value))","c643bafe":"result = adfuller(ddfn.values)\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n\tprint('\\t%s: %.3f' % (key, value))","f1a5da22":"plot_acf(df.values,lags=4*24*7)\nplt.show()","ffd6104b":"plot_acf(ddfn.values,lags=4*24*7)\nplt.show()","14e36953":"plot_acf(df.values,lags=4*24*365)\nplt.show()","8b148c46":"plot_acf(ddfn.values,lags=4*24*365)\nplt.show()","1675da27":"split = 0.9 # 10% of data for test\ni_split = int(len(dfn)*split)\n\ndfn_test = dfn[i_split:].copy(deep=True)\ndfn = dfn[:i_split]","a8d79e6f":"split = 0.9 # 10% of data for test\ni_split = int(len(ddfn)*split)\n\nddfn_test = ddfn[i_split:].copy(deep=True)\nddfn = ddfn[:i_split]","b96fff7d":"dfn['time of day'] = (48 - np.abs(dfn.index.hour.values*4 + dfn.index.minute.values\/15 - 48))\/48","df4ab39b":"ddfn['time of day'] = (48 - np.abs(ddfn.index.hour.values*4 + ddfn.index.minute.values\/15 - 48))\/48","603a3d28":"# make tuesday=0\ndfn['day of week'] = dfn.index.dayofweek - 1\ndfn['day of week'][dfn['day of week'] == -1] += 7 \n\n# convert ramp to triangle and normalize\ndfn['day of week'] = (3 - np.abs(dfn['day of week'].values - 3))\/3","8983bdc8":"# make tuesday=0\nddfn['day of week'] = ddfn.index.dayofweek - 1\nddfn['day of week'][ddfn['day of week'] == -1] += 7 \n\n# convert ramp to triangle and normalize\nddfn['day of week'] = (3 - np.abs(ddfn['day of week'].values - 3))\/3","ceaa30b7":"plt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(dfn['2019-1-1':'2019-1-7']['day of week'],label='day of week')\nplt.plot(dfn['2019-1-1':'2019-1-7']['time of day'],label='time of day')\nplt.title('Timekeeping Triangle Indices')\nplt.legend()","21434fce":"#from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n\n#dr = pd.date_range(start='2016-01-01', end='2020-12-31')\n#dff = pd.DataFrame()\n#dff['Date'] = dr\n\n#cal = calendar()\n#holidays = cal.holidays(start=dr.min(), end=dr.max())\n\n#dff['Holiday'] = dff['Date'].isin(holidays)*1\n#dff","157eaf1b":"#ddfn['Date'] = ddfn.index.to_frame()\n#ddfn['Date'] = ddfn['Date'].apply(lambda x:x.date().strftime('%Y-%m-%d'))\n\n#cal2 = calendar()\n#holidays2 = cal2.holidays(start=ddfn.index.min(), end=ddfn.index.max())\n#ddfn['Date']","9ac934ad":"#ddfn['Holidays'] = ddfn['Date'].isin(holidays2)*1\n#ddfn['2018-12-25']","978ee519":"dayname={0:'Monday', 1:'Tuesday', 2:'Wednesday', 3:'Thursday', 4:'Friday', 5:'Saturday', 6:'Sunday'}\n\ndaily_max = df.resample('D').max()\ndaily_avg = df.resample('D').mean()\ndaily_min = df.resample('D').min()\ndaily_std = df.resample('D').std()\n\ndf_daily_max = pd.DataFrame()\ndf_daily_avg = pd.DataFrame()\ndf_daily_min = pd.DataFrame()\ndf_daily_std = pd.DataFrame()\n\nfor dow in range(7): # \"day of week\", 0 = monday  \n    df_daily_max[dayname[dow]] = daily_max[daily_max.index.dayofweek==dow].values[:167].flatten()\n    df_daily_avg[dayname[dow]] = daily_avg[daily_avg.index.dayofweek==dow].values[:167].flatten()\n    df_daily_min[dayname[dow]] = daily_min[daily_min.index.dayofweek==dow].values[:167].flatten()\n    df_daily_std[dayname[dow]] = daily_std[daily_std.index.dayofweek==dow].values[:167].flatten()    \n\n    \nplt.figure(num=None, figsize=(20, 10), dpi=80)\n\nplt.subplot('221')\nplt.title('daily maximums')\nplt.ylabel('Load (kW)')\ndf_daily_max.boxplot()\n\nplt.subplot('222')\nplt.title('daily averages')\nplt.ylabel('Load (kW)')\ndf_daily_avg.boxplot()\n\nplt.subplot('223')\nplt.title('daily minimums')\nplt.ylabel('Load (kW)')\ndf_daily_min.boxplot()\n\nplt.subplot('224')\nplt.title('daily std devs')\nplt.ylabel('Load (kW)')\ndf_daily_std.boxplot()\n\n","9dd5734c":"imf = emd.sift.sift(dfn['load'].values)\n\nprint('Number of different IMFs: ',imf.shape[1])\n\nfig = emd.plotting.plot_imfs(imf, scale_y=True, cmap=True)\n","3908263e":"imf_dif = emd.sift.sift(ddfn['load'].values)\n\nprint('Number of different IMFs: ',imf_dif.shape[1])\n\nfig = emd.plotting.plot_imfs(imf, scale_y=True, cmap=True)","5998e696":"dfn_emd = dfn.copy(deep=True)\n\nfor i in range(imf.shape[1]):\n    dfn_emd['IMF%s'%(i+1)] = imf[:,i]\n    \n\nc = dfn_emd.corr()\nc.style.background_gradient(cmap='coolwarm').set_precision(2)","66bbe70b":"ddfn_emd = ddfn.copy(deep=True)\n\nfor i in range(imf_dif.shape[1]):\n    ddfn_emd['IMF%s'%(i+1)] = imf_dif[:,i]\n    \n\nc = ddfn_emd.corr()\nc.style.background_gradient(cmap='coolwarm').set_precision(2)","6aaf10a5":"dfn_emd['IMFs'] = imf[:,2] + imf[:,3] + imf[:,4] + imf[:,9] + imf[:,10]\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(dfn[:'2016-11-20']['load'],'b',label='Original timeseries')\nplt.plot(dfn_emd[:'2016-11-20']['IMFs'],'g--',label='IMFs 3, 4, 5, 10, 11')\nplt.ylabel('Load (kW)')\nplt.legend()","a97b1a21":"ddfn_emd['IMFs'] = imf_dif[:,0] + imf_dif[:,1] + imf_dif[:,2] + imf_dif[:,3] + imf_dif[:,4]\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(ddfn[:'2016-11-20']['load'],'b',label='Original timeseries')\nplt.plot(ddfn_emd[:'2016-11-20']['IMFs'],'g--',label='IMFs 1-5')\nplt.ylabel('Load (kW)')\nplt.legend()","f2d58169":"dfn_emd.drop('IMF1',axis=1, inplace=True)\ndfn_emd.drop('IMF2',axis=1, inplace=True)\ndfn_emd.drop('IMF6',axis=1, inplace=True)\ndfn_emd.drop('IMF7',axis=1, inplace=True)\ndfn_emd.drop('IMF8',axis=1, inplace=True)\ndfn_emd.drop('IMF9',axis=1, inplace=True)\ndfn_emd.drop('IMFs',axis=1, inplace=True)","87b92e7d":"ddfn_emd.drop('IMF6',axis=1, inplace=True)\nddfn_emd.drop('IMF7',axis=1, inplace=True)\nddfn_emd.drop('IMF8',axis=1, inplace=True)\nddfn_emd.drop('IMF9',axis=1, inplace=True)\nddfn_emd.drop('IMF10',axis=1, inplace=True)\nddfn_emd.drop('IMF11',axis=1, inplace=True)\nddfn_emd.drop('IMF12',axis=1, inplace=True)\nddfn_emd.drop('IMFs',axis=1, inplace=True)","5c392b2c":"Xdf = dfn.copy(deep=True)\nXdf_emd = dfn_emd\n\nXdf.rename(columns={'load': 't'},inplace=True)\nXdf.rename(columns={'time of day': 'tod'},inplace=True)\nXdf.rename(columns={'day of week': 'dow'},inplace=True)","ad87d227":"Xddf = ddfn.copy(deep=True)\nXddf_emd = ddfn_emd\n\nXddf.rename(columns={'load': 't'},inplace=True)\nXddf.rename(columns={'time of day': 'tod'},inplace=True)\nXddf.rename(columns={'day of week': 'dow'},inplace=True)","46528403":"n_in = 4*24*3 # number of inputs\n\nfor i in range(1,n_in):\n    Xdf.insert(0, 'IMF3 t-%s'%i,  Xdf_emd['IMF3'].shift(i), True)  \n    Xdf.insert(0, 'IMF4 t-%s'%i,  Xdf_emd['IMF4'].shift(i), True)    \n    Xdf.insert(0, 'IMF5 t-%s'%i,  Xdf_emd['IMF5'].shift(i), True)    \n    Xdf.insert(0, 'IMF10 t-%s'%i, Xdf_emd['IMF10'].shift(i), True)    \n    Xdf.insert(0, 'IMF11 t-%s'%i, Xdf_emd['IMF11'].shift(i), True)   \n    Xdf.insert(0, 'tod t-%s'%i,   Xdf_emd['time of day'].shift(i), True)   \n    Xdf.insert(0, 'dow t-%s'%i,   Xdf_emd['day of week'].shift(i), True)   \n    Xdf.insert(0, 't-%s'%i,       Xdf_emd['load'].shift(i),    True)    \n    ","2fee412b":"n_in = 4*24*3 # number of inputs\n\nfor i in range(1,n_in):\n    Xddf.insert(0, 'IMF1 t-%s'%i,  Xddf_emd['IMF1'].shift(i), True)  \n    Xddf.insert(0, 'IMF2 t-%s'%i,  Xddf_emd['IMF2'].shift(i), True)    \n    Xddf.insert(0, 'IMF3 t-%s'%i,  Xddf_emd['IMF3'].shift(i), True)    \n    Xddf.insert(0, 'IMF4 t-%s'%i,  Xddf_emd['IMF4'].shift(i), True)    \n    Xddf.insert(0, 'IMF5 t-%s'%i,  Xddf_emd['IMF5'].shift(i), True)   \n    Xddf.insert(0, 'tod t-%s'%i,   Xddf_emd['time of day'].shift(i), True)   \n    Xddf.insert(0, 'dow t-%s'%i,   Xddf_emd['day of week'].shift(i), True)   \n    Xddf.insert(0, 't-%s'%i,       Xddf_emd['load'].shift(i),    True)    \n    \nXdf    ","58895977":"Ydf = pd.DataFrame(dfn['load'])\nYdf.rename(columns={'load': 't'},inplace=True)","c142800b":"Yddf = pd.DataFrame(ddfn['load'])\nYddf.rename(columns={'load': 't'},inplace=True)","0076cb6f":"h = 96\nn_out = 1\nYdf['t+96'] = Ydf['t'].shift(-96)\n\n# use for predicting (say) all t+1h, t+2h.. t+24h\nif 0:\n    h = 0 # horizon, not incorporated\n    n_out = 24  # number of outputs\n\n    for i in range(1,n_out+1):\n        Ydf['t+%sh'%i]=Ydf['t'].shift(-i)\n\nYdf.drop('t',axis=1,inplace=True)","31b71334":"h = 96\nn_out = 1\nYddf['t+96'] = Yddf['t'].shift(-96)\n\n# use for predicting (say) all t+1h, t+2h.. t+24h\nif 0:\n    h = 0 # horizon, not incorporated\n    n_out = 24  # number of outputs\n\n    for i in range(1,n_out+1):\n        Yddf['t+%sh'%i]=Yddf['t'].shift(-i)\n\nYddf.drop('t',axis=1,inplace=True)\n\nYddf","afafcebc":"Xdf = Xdf[n_in-1 : -(n_out+h-1)]","6a406275":"Xddf = Xddf[n_in-1 : -(n_out+h-1)]\nXddf","1acab391":"Ydf = Ydf[n_in-1 : -(n_out+h-1)]","1097375c":"Yddf = Yddf[n_in-1 : -(n_out+h-1)]\nYddf","ded20821":"months = 6\nk = 4*24*30*months\n\nXdf = Xdf[-k:]","ca6eac3a":"months = 6\nk = 4*24*30*months\n\nXddf = Xddf[-k:]\nXddf","b62767d6":"Ydf = Ydf[-k:]","1dcc246b":"Yddf = Yddf[-k:]\nYddf","cb9b1954":"split = 0.7\nL = Xdf.shape[0]\ni_split = int(L*split)\n\n\nXdf_train = Xdf[0:i_split]\nXdf_valid = Xdf[i_split:]\n\nYdf_train = Ydf[0:i_split]\nYdf_valid = Ydf[i_split:]\n\nprint('Xdf_train.shape: ',Xdf_train.shape)\nprint('Xdf_valid.shape: ',Xdf_valid.shape)\nprint('Ydf_train.shape: ',Ydf_train.shape)\nprint('Ydf_valid.shape: ',Ydf_valid.shape)","cc975c42":"split = 0.7\nL = Xddf.shape[0]\ni_split = int(L*split)\n\n\nXddf_train = Xddf[0:i_split]\nXddf_valid = Xddf[i_split:]\n\nYddf_train = Yddf[0:i_split]\nYddf_valid = Yddf[i_split:]\n\nprint('Xdf_train.shape: ',Xddf_train.shape)\nprint('Xdf_valid.shape: ',Xddf_valid.shape)\nprint('Ydf_train.shape: ',Yddf_train.shape)\nprint('Ydf_valid.shape: ',Yddf_valid.shape)","3a0aa54c":"Xdf_valid","624325e0":"Ydf_valid","67a9125b":"epochs = 1000\nbatch = 1024\nlr = 0.0001\npatience = 20\nneurons = 200\nadam = optimizers.Adam(lr)","6ce84fb0":"model_ann = Sequential()\n\nmodel_ann.add(Dense(neurons, activation='relu', input_dim=Xdf_train.shape[1]))\nmodel_ann.add(Dense(Ydf_train.shape[1]))\n\nmodel_ann.compile(loss='mse', optimizer=adam)\nmodel_ann.summary()","585bb4d9":"model_dann = Sequential()\n\nmodel_dann.add(Dense(neurons, activation='relu', input_dim=Xddf_train.shape[1]))\nmodel_dann.add(Dense(Yddf_train.shape[1]))\n\nmodel_dann.compile(loss='mse', optimizer=adam)\nmodel_dann.summary()","8e9fbbcd":"es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n\nann_history =  model_ann.fit(Xdf_train.values, Ydf_train.values, validation_data=(Xdf_valid, Ydf_valid), epochs=epochs, verbose=2, callbacks=[es])","4cf28d15":"es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n\ndann_history =  model_dann.fit(Xddf_train.values, Yddf_train.values, validation_data=(Xddf_valid, Yddf_valid), epochs=epochs, verbose=2, callbacks=[es])","8376876c":"plt.figure(num=None, figsize=(10, 5), dpi=80)\nplt.plot(ann_history.history['loss'][2:]) # first two losses can be orders of magnitudes higher\nplt.plot(ann_history.history['val_loss'][2:]) # first two losses can be orders of magnitudes higher\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch - 2')\nplt.legend(['training loss', 'validation loss'], loc='upper right')","df36beac":"plt.figure(num=None, figsize=(10, 5), dpi=80)\nplt.plot(dann_history.history['loss'][2:]) # first two losses can be orders of magnitudes higher\nplt.plot(dann_history.history['val_loss'][2:]) # first two losses can be orders of magnitudes higher\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch - 2')\nplt.legend(['training loss', 'validation loss'], loc='upper right')","04d00dc2":"Y_train_hat = model_ann.predict(Xdf_train.values)\nY_valid_hat = model_ann.predict(Xdf_valid.values)","20f5f123":"Yd_train_hat = model_dann.predict(Xddf_train.values)\nYd_valid_hat = model_dann.predict(Xddf_valid.values)\nprint('Train predictions shape ',Yd_train_hat.shape)\nprint('Valid predictions shape ',Yd_valid_hat.shape)","c86d1ec7":"measurements_t = scaler.inverse_transform(Ydf_train.values)\nmeasurements_v = scaler.inverse_transform(Ydf_valid.values)\n\npersistence_t = scaler.inverse_transform(Xdf_train['t'].values.reshape(-1,1))\npersistence_v = scaler.inverse_transform(Xdf_valid['t'].values.reshape(-1,1))\n\npredictions_t = scaler.inverse_transform(Y_train_hat)\npredictions_v = scaler.inverse_transform(Y_valid_hat)\n\nprint('Persist train rmse [kW]: {:.3f}'.format(np.sqrt(mean_squared_error(measurements_t, persistence_t))))\nprint('Persist valid rmse [kW]: {:.3f}'.format(np.sqrt(mean_squared_error(measurements_v, persistence_v))))\n\nprint('Train rmse [kW]: {:.3f}'.format(np.sqrt(mean_squared_error(measurements_t, predictions_t))))\nprint('Valid rmse [kW]: {:.3f}'.format(np.sqrt(mean_squared_error(measurements_v, predictions_v))))","5e174b9f":"dmeasurements_t = scaler.inverse_transform(Yddf_train.values)\ndmeasurements_v = scaler.inverse_transform(Yddf_valid.values)\n\ndpersistence_t = scaler.inverse_transform(Xddf_train['t'].values.reshape(-1,1))\ndpersistence_v = scaler.inverse_transform(Xddf_valid['t'].values.reshape(-1,1))\n\ndpredictions_t = scaler.inverse_transform(Yd_train_hat)\ndpredictions_v = scaler.inverse_transform(Yd_valid_hat)\n\nprint('Persist train rmse [kW]: {:.3f}'.format(np.sqrt(mean_squared_error(dmeasurements_t, dpersistence_t))))\nprint('Persist valid rmse [kW]: {:.3f}'.format(np.sqrt(mean_squared_error(dmeasurements_v, dpersistence_v))))\n\nprint('Train rmse [kW]: {:.3f}'.format(np.sqrt(mean_squared_error(dmeasurements_t, dpredictions_t))))\nprint('Valid rmse [kW]: {:.3f}'.format(np.sqrt(mean_squared_error(dmeasurements_v, dpredictions_v))))","a457344e":"k = 4*24*7\nt=np.arange(0,k)\n\nm = measurements_v[0:k]\np = persistence_v[0:k]\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(t,m,t,p)\nplt.ylabel('kW')\nplt.xlabel('timestep')\nplt.legend(['Y','Predicted Y'])\nplt.title('validation set 24 h persistence')","98a69531":"k = 4*24*7\nt=np.arange(0,k)\n\nm = dmeasurements_v[0:k]\np = dpersistence_v[0:k]\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(t,m,t,p)\nplt.ylabel('kW')\nplt.xlabel('timestep')\nplt.legend(['Y','Predicted Y'])\nplt.title('validation set 24 h persistence (differenced)')","1d1a08df":"k = 4*24*7\nt=np.arange(0,k)\n\nm = measurements_t[0:k]\np = predictions_t[0:k]\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(t,m,t,p)\nplt.ylabel('kW')\nplt.xlabel('timestep')\nplt.legend(['Y','Predicted Y'])\nplt.title('training set predictions')","b72c32c5":"k = 4*24*7\nt=np.arange(0,k)\n\nm = dmeasurements_t[0:k]\np = dpredictions_t[0:k]\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(t,m,t,p)\nplt.ylabel('kW')\nplt.xlabel('timestep')\nplt.legend(['Y','Predicted Y'])\nplt.title('training set predictions (differenced)')","f029b31a":"k = 4*24*7\nt=np.arange(0,k)\n\nm = measurements_v[0:k]\np = predictions_v[0:k]\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(t,m,t,p)\nplt.ylabel('kW')\nplt.xlabel('timestep')\nplt.legend(['Y','Predicted Y'])\nplt.title('validation set predictions')","c7df59e9":"k = 4*24*7\nt=np.arange(0,k)\n\nm = dmeasurements_v[0:k]\np = dpredictions_v[0:k]\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(t,m,t,p)\nplt.ylabel('kW')\nplt.xlabel('timestep')\nplt.legend(['Y','Predicted Y'])\nplt.title('validation set predictions (differenced)')","dc686262":"k = 4*24*7\nt=np.arange(0,k)\n\nmeasurements_v_i = scaler.inverse_transform(Ydf_valid.values)\n\n#Y_valid_hat[0] = Ydf_valid.values[0]\nY_valid_hat_i = np.cumsum(Y_valid_hat).reshape(-1,1)\npredictions_v_i = scaler.inverse_transform(Y_valid_hat_i)\n\nm = measurements_v_i[0:k]\np = predictions_v_i[0:k]\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(t,m,t,p)\nplt.ylabel('kW')\nplt.xlabel('timestep')\nplt.legend(['Y_valid','Predicted Y_valid'])\nplt.title('training set predictions')","27c5f614":"stooooppppp","8d2cdb5d":"X_train_3d = Xdfn_train.values.reshape(Xdfn_train.shape[0],Xdfn_train.shape[1],1)\nX_valid_3d = Xdfn_valid.values.reshape(Xdfn_valid.shape[0],Xdfn_valid.shape[1],1)\n\nY_train_3d = Ydfn_train.values.reshape(Ydfn_train.shape[0],Ydfn_train.shape[1],1)\nY_valid_3d = Ydfn_valid.values.reshape(Ydfn_valid.shape[0],Ydfn_valid.shape[1],1)\n\nprint('X_train_3d shape: ',X_train_3d.shape)\nprint('X_valid_3d shape: ',X_valid_3d.shape)\nprint('Y_train_3d shape: ',Y_train_3d.shape)\nprint('Y_valid_3d shape: ',Y_valid_3d.shape)\nprint('X_train[:3,:5,0]:\\n',X_train_3d[:3,:5,0])\n\n","b8fcd9d2":"model_cnn = Sequential()\nmodel_cnn.add(Conv1D(filters=4, kernel_size=10, activation='relu', input_shape=(X_train_3d.shape[1], 1)))\nmodel_cnn.add(MaxPooling1D(pool_size=2))\nmodel_cnn.add(Flatten())\nmodel_cnn.add(Dense(20, activation='relu'))\nmodel_cnn.add(Dense(20, activation='relu'))\nmodel_cnn.add(Dense(Y_train_3d.shape[1]))\nmodel_cnn.compile(loss='mse', optimizer=adam)\nmodel_cnn.summary()","f81b1614":"cnn_history = model_cnn.fit(X_train_3d, Y_train_3d, validation_data=(X_valid_3d, Y_valid_3d), epochs=epochs, verbose=2)","77303453":"cnn_train_pred = model_cnn.predict(X_train_3d)\ncnn_valid_pred = model_cnn.predict(X_valid_3d)\nprint('Ydf_train shape',Ydf_train.shape)\nprint('cnn_train_pred shape',cnn_train_pred.shape)\nprint('Train rmse: {:.3f}'.format(np.sqrt(mean_squared_error(Ydf_train.values, cnn_train_pred))))\nprint('Validation rmse: {:.3f}'.format(np.sqrt(mean_squared_error(Ydf_valid.values, cnn_valid_pred))))\nY_valid_hat_cnn = model_cnn.predict(X_valid_3d)\nprint('Y_valid_hat_cnn.shape:',Y_valid_hat_cnn.shape)","32744e33":"k = 4*24*7\nt=np.arange(0,k)\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(t,Y_valid_3d[0:k,0],t,Y_valid_hat_cnn[0:k,0])\nplt.ylabel('kW')\nplt.xlabel('timestep')\nplt.legend(['Y_valid','Predicted Y_valid'])","703e620a":"model_lstm = Sequential()\nmodel_lstm.add(LSTM(50, activation='relu', input_shape=(X_train_3d.shape[1], X_train_3d.shape[2])))\nmodel_lstm.add(Dense(Y_train_3d.shape[1]))\nmodel_lstm.compile(loss='mse', optimizer=adam)\nmodel_lstm.summary()","a3d7b524":"lstm_history = model_lstm.fit(X_train_3d, Y_train_3d, validation_data=(X_valid_3d, Y_valid_3d), epochs=epochs, verbose=2)","b513e79f":"Y_valid_hat_lstm = model_lstm.predict(X_valid_3d)\nY_valid_hat_lstm.shape","f1a66c92":"lstm_train_pred = model_lstm.predict(X_train_3d)\nlstm_valid_pred = model_lstm.predict(X_valid_3d)\nprint('Train rmse: {:.3f}'.format(np.sqrt(mean_squared_error(Ydf_train.values, lstm_train_pred))))\nprint('Validation rmse: {:.3f}'.format(np.sqrt(mean_squared_error(Ydf_valid.values, lstm_valid_pred))))","7d24a0c0":"k=72\nt=np.arange(0,k)\n\nplt.plot(t,Y_valid_3d[0:k,-1],t,Y_valid_hat_lstm[0:k,-1])\nplt.ylabel('kWh')\nplt.xlabel('hrs')\nplt.legend(['Y_valid','Y_valid_hat'])","ca5c416d":"t=np.arange(0,n_out)\n\nplt.plot(t,Y_valid_3d[0,:],t,Y_valid_hat_lstm[0,:])\nplt.ylabel('kWh')\nplt.xlabel('hrs')\nplt.legend(['Y_valid','Y_valid_hat'])","0654e687":"The solar PV connection is located behind the utility consumption meter, so we will assume all solar PV production is self consumed (the array is small compared to the load) and therefore the estimated native load (without solar) is `Meter (kW) + Solar (kW)`.","3dcffea1":"## Plots","cf5a0a04":"Cluster 4","cf63035e":"Hotel shuts down operations (due to covid-19) in mid-March 2020. Also notice the bad data in 2017.","5977b92f":"## Plot Every Week, Overlaid","5fb17aab":"### Single prediction","12ced204":"Pandas computes day of week such that monday=0 and sunday=6. The problems are that\n1. This does not seem to correlate with load (which is maximum on Friday, roughly speaking).\n2. The ramp function suggests that 0 is very far away from 6, but actually its just the day after. For this reason we prefer a triangle function, where subsequent days have similar values.\n\nTherefore we will shift the day of week index such that Friday is the maximum, and then convert it from a ramp to a triangle. Lastly we will normalize it.","02f232db":"### Rolling prediction","98cb6c9a":"Cluster 5","7fede5b4":"## Plots","92c2e985":"### Normality","35229e81":"### Model Output (Y)","8a4ac76a":"Choose which input data set (and shorten column names for compactness)","acf13f94":"### Rolling 12 hr prediction","429a74f1":"## Build Time Shifted Columns","700e272d":"## Fit","252ce6cc":"Na\u00efve persistence (24 h)","83df40ad":"# Organize Data for Supervisory Learning","aaacbd86":"Augmented Dicky-Fuller Test\n\nBased on the very low ADF p-value we can likely say the dataset is stationary (the null hypothesis is rejected). Also the ADF statistic is much lower than the critical values at 10, 5, and 1% significance.\n\nReminder: null hypothesis == non-stationarity\n- p-value <= 0.05: hypothesis rejected, \"suggests\" stationarity\n- p-value > 0.05: hypothesis cannot be rejected, \"suggests\" non-stationariy","41eee782":"Reshape? (no, already done above)","22c22eb1":"## Empirical Mode Decomposition (EMD)","a522fa36":"Root Mean Squared Error","bc15fac8":"# Viz","9759fd9e":"## Fit","c2a49949":"Having removed the outliers we can visually inspect the daily data again","9b128de8":"## Stationarity","f4f9bf0c":"## Split train vs validate","ce0b242d":"Now we don't need the meter and solar data anymore","a6e591f5":"Quantile-Quantile (QQ) Plot","08fea4ee":"## Plot Every Day, Overlaid","bdc55d2f":"# Feature Engineering","40738f0e":"## Train","3840deb9":"Notice the NaNs that appear. This is because we start at the beginning of the (cleaned) dataset and attempt to go back in time `n_in` data points - which isn't possible. All the NaNs will be cleared out in a final trim.","d6c557e9":"### Rolling Horizon Predictions","cbc40af8":"Or to automate this we can use a simple clustering algorithm.","478eb8c3":"## Build","9fea3220":"And then if we do the same on the normalized and differenced timeseries. Note that differencing has the larger impact on statistical properties of the timeseries.","234e115b":"One week","aa781410":"Note three areas of outlier-seeming data:\n1. A very small amount of data, tightly grouped, between 0 kW and ~400 kW\n2. A medium amount of less-tightly grouped data around ~500 kW\n3. A barely visible set of daily peaks in the afternoon and evening > 1750 kW ","0cf082d5":"## Build","94d4dd91":"## Plots","1260acb3":"## Timekeeping Triangle Indices ","fb389c01":"Partially reconstruct the original timeseries using only the four IMFs, which capture most but not all the variation.","a8ef545e":"Second, an error was identified in the solar data. Notes how on 2016-11-12 the solar PV power starts increasing after midday and continues well into the night, which is impossible. This is true for this day, and the previous few weeks also. So we will just throw out all the data before 2016-11-14.","f7d18691":"### Model Input (X)","d47c36dc":"## Normalize and Difference","bbe2fb07":"Select features based on correlation with the original timeseries: IMFs 3, 4, 5, and 10.","f6935977":"# Import Data","cc5ca36e":"# Test Data Split","26c47b25":"Training and validations losses","ac73484d":"Cluster 6","223462fa":"# Clean and Standardize Data","a29f8576":"## Results","ea44b273":"See the histogram for basic visual analysis of the distribution. The data is not very Gaussian. This may be an indicator of non-stationarity.","801e500c":"## Results","fc489d43":"# Intro\n\nThis is the code portion of my Politecnico di Milano M.Sc. thesis on electric load forecasting. It's a work in progress but will be complete around Dec 2020. ","4b245d32":"## Reduce data for hyperparamter tuning","03819112":"### Trim the edges (NANs)","17c9dc38":"First, we know we don't want the covid-affected data (hotel closed down in March 2020)","63e21f2a":"## One Week","9eb735be":"Save the test data to calculate the final generalizaton error ","1fbc3bde":"## First Data Clean","2dd2e743":"# EMD + ANN","6bc010f2":"Check for null data","853ddaeb":"Now use the z-score method to find the remaining outlier data","fbc38707":"## Configuration","e1931639":"Cluster 0","b0ab96f1":"Decompose (\"sift\") into Intrinsic Mode Functions (IMFs)","bee5b871":"## Training Parameters","988fd2bd":"## Holidays","0ee59f80":"One year","57b4b37a":"Make Predictions","182bd939":"# EMD + CNN (not run in this version)","5dbda444":"Each day the triangle index starts at value 0 at time 0:00, increases at every timestep to its maximum value of 1 at time 12:00, then decreases back to value 0 at time 0:00 the next day.","940271bd":"## Rolling Horizon Predictions (integrated)","61adf77c":"Note that package 'emd' is not installed by default, and disappears every session refresh. It should install using `pip install emd` below.","ce3b78e3":"## Outlier Replacement","d0d18c35":"The normalized and time differenced data","5acfe017":"# To Do\n\nLit \n- ~Review~\n\nCourses\n- ~Udacity - Intro to TensorFlow for Deep Learning~\n\nBooks\n- G\u00e9ron 2019 - Hands on Machine Learning\n\nViz\n- ~Daily and weekly plots~\n- ~Hist~\n\nData Prep\n- ~Generalized ESD for outliers~ Doesn't seem to work (why?)\n- ~Basic stats analysis: autocorrelation, Augmented Dicky-Fuller~\n- ~Z-score for outlier detection~\n- ~Outlier replacement~ Same day different year\n- ~Normalize data~\n- ~Diff data for ANN~\n\nModels\n- ~ANN, CNN, and LSTM~\n- ~Predict [t+1 .. t+24h] data points (4x24=96)~ Too complex\n- ~Only predict one horizon data pt~\n- Ensemble: lower error, compare stdev of predictions\n- Consider nowcast\n- Grid\/random search on hyperparameters\n- Bi-directional LSTM?\n\nFeature Engineering\n- ~Empirical mode decomposition~\n- ~Feature-timeseries cross correlation~\n- ~Triangle time-of-day index~\n- Day of wk, holidays, etc\n- ~LSTM past 2-3 days~\n- Exogenous: temp, irradiance, windspeed (check, don't *need* to include)\n\nMetrics\n- ~RMSE to penalize larger errors~\n- Benchmark: na\u00efve persistence\n- Accuracy?\n\nTraining\n- ~Set-seed~\n- Try training on less data to start (3 months?) \n- Cross-validation","5f5bd347":"## Autocorrelation","79090dea":"Visually locate any cluster of outliers (if at all). Indeed we see 7 independent clusters.","9d2312c7":"## Reshape X and Y into 3D array for CNN","25eefccb":"## Outlier Detection","ca6f6ee8":"Cluster 1","99677f0c":"Because the cleaned data is not very normally distributed, we can also check the ADF test of the time-differenced data, which is closer to Gaussian.","c403dea9":"Final train with use all the data","c768c4b7":"Locate the outlier data and choose data from the same day-of-year of a different year or the same day-of-week from a nearby week.","f97529e3":"Time of Day","0cc208a9":"Visualize the two triangle indices","25d69f0f":"Cluster 2","06187565":"## All Data, Resampled Daily","ab8c3f64":"# Statistical Analysis","d2c60038":"## Results","b6bbb44f":"Difference the normalized data","88301823":"Day of Week","e76ac8f5":"# EMD + LSTM (not run in this version)","22d68214":"Root Mean Squared Error (differenced)","0f19a6e4":"Looking for useful data transformations to reveal the patterns in the load","362789ad":"Stationarity is achieved when the statistical properties of a timeseries do not change with time. A very simple test of this could be computing the rolling mean and variance for a large window size. Note that if the data is not a typical gaussian distribution then mean and variance are less meaningful summary statistics.","7886d888":"Normalized (min-max scaled) data should enable better learning neural networks, for instance if different feature sets have very different magnitudes (e.g. load power up to 1800 kW, day of week up to 7). The cleaned data does not have enormous outliers, so most of the data will fit nicely in a 0-to-1 scale. ","3bbcdd7b":"Intregrate (we differenced the original data)","25d01f96":"Cluster 3","739d57e8":"# Dependencies"}}