{"cell_type":{"0dd6d606":"code","10ecb0c0":"code","9d0d3505":"code","dcf25f14":"code","d807a76c":"code","4d7ae604":"code","dc21bfc0":"code","b606a186":"code","ca9e2b40":"code","3341b2d3":"code","916d30f8":"code","53c244f0":"code","53b4b880":"code","6d10ccc4":"code","b140a8ee":"code","e87a1337":"code","f7deb468":"code","5bcac09c":"code","f2b0b050":"code","ee31facb":"code","6d2ecb1c":"code","27970f54":"code","6a107e54":"code","db4f6cb2":"code","bd780a49":"markdown","dd7fa830":"markdown","2d1ce085":"markdown","7d40b85c":"markdown","00fcca1f":"markdown","4280325e":"markdown","e568dbee":"markdown","e0fec1ec":"markdown","dfc33f3a":"markdown","0ff0ce5f":"markdown","3e573625":"markdown","a29b005b":"markdown","5e786be1":"markdown","1c8c9bb0":"markdown","e7ca7743":"markdown","691f6118":"markdown","ca9ac8af":"markdown","55dde24c":"markdown","a8e1c12e":"markdown","ee55fe09":"markdown","845def87":"markdown","4d035ce9":"markdown","73db0677":"markdown","6dfcef1b":"markdown","44202606":"markdown","161be11d":"markdown","eb239179":"markdown","cb291851":"markdown"},"source":{"0dd6d606":"!pip install sadedegel\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom sadedegel.bblock import Doc\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy.sparse import vstack, csr_matrix # dealing with sparse matrices in  tfidf embeddings\nfrom sklearn.preprocessing import normalize\nimport altair as alt\nfrom tqdm import tqdm\n\n","10ecb0c0":"df = pd.read_csv(\"..\/input\/toxic-comment-detection-multilingual-extended\/archive\/turkish\/troff-v1.0.tsv\", sep='\\t')\ndf","9d0d3505":"class_counts = df[\"label\"].value_counts()\nclass_counts = class_counts.reset_index()\nclass_counts = class_counts.rename(columns={\"label\":\"counts\", \"index\":\"label\"})\nclass_counts","dcf25f14":"alt.Chart(class_counts, height=200).mark_bar(color=\"lightblue\").encode(\n    x=alt.X(\"counts:Q\", title=\"Class count\"),\n    y=alt.Y(\"label:N\", title=\"Off. tweet type\")\n).configure_axis(\n    labelFontSize=12,\n    titleFontSize=15\n)\n","d807a76c":"df[\"text\"] = df[\"text\"].replace(\"@\\S* \", \"\", regex=True)\ndf.head()","4d7ae604":"# Step 1\ntweet_0 = Doc(df.iloc[103][\"text\"])\ntweet_0","dc21bfc0":"# Step 2\ntfidf_embedding_0 = tweet_0.tfidf_embeddings\ntfidf_embedding_0.shape","b606a186":"# Step 3\n# average on sentence dimension\n# (though in this example, there is only one sentence anyway )\ntfidf_embedding_0_avg = tfidf_embedding_0.mean(axis=0)\ntfidf_embedding_0_avg.shape\n\n# And we're done!","ca9e2b40":"\nX_str = df[\"text\"]\nX = []\n\nfor tweet in tqdm(X_str):\n    d = Doc(tweet)\n    X.append(csr_matrix(d.tfidf_embeddings.mean(axis=0)))\n\n\nX = vstack(X)\nprint(\"Shape of embeddings matrix: \", X.shape)","3341b2d3":"X = normalize(X)\n","916d30f8":"y = df[\"label\"]\nX_train, X_val, y_train, y_val = train_test_split(X,y, stratify=df[\"label\"], test_size=0.2)","53c244f0":"train_class_counts = y_train.value_counts().reset_index().rename(columns={\"label\":\"counts\", \"index\":\"label\"})\nval_class_counts = y_val.value_counts().reset_index().rename(columns={\"label\":\"counts\", \"index\":\"label\"})\n\nc1 = alt.Chart(train_class_counts, height=200, title=\"Train counts\").mark_bar(color=\"lightblue\").encode(\n    \n    x=alt.X(\"counts:Q\", title=\"Class count\"),\n    y=alt.Y(\"label:N\", title=\"Off. tweet type\")\n)\n\nc2 = alt.Chart(val_class_counts, height=200, title=\"Validation counts\").mark_bar(color=\"lightblue\").encode(\n    x=alt.X(\"counts:Q\", title=\"Class count\"),\n    y=alt.Y(\"label:N\", title=\"Off. tweet type\")\n)\n\nc1 | c2\n","53b4b880":"clf = LogisticRegression(random_state=0, max_iter=2000, class_weight=\"balanced\").fit(X_train, y_train) # train","6d10ccc4":"y_pred = clf.predict(X_val)\ny_pred","b140a8ee":"from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n\nlabel_list = sorted(list(y_val.unique()))\ncm = confusion_matrix(y_val, y_pred, labels=label_list)\nx, y = np.meshgrid(range(cm.shape[0]), range(cm.shape[0]))\n\n# Convert this grid to columnar data expected by Altair\nsource = pd.DataFrame({'x': x.ravel(),\n                     'y': y.ravel(),\n                     'z': cm.ravel()})\n\nfor i,label in enumerate(label_list):\n    source.loc[:, \"x\"] = source.loc[:,\"x\"].replace(i, label)\n    source.loc[:, \"y\"] = source.loc[:,\"y\"].replace(i, label)\n\n\n\nbase = alt.Chart(source, title=\"Confusion matrix\").encode(\n    x=alt.X(\"x:O\", axis=alt.Axis(title=\"Predicted\", titlePadding=30)),\n    y=alt.Y(\"y:O\", axis=alt.Axis(title=\"Ground truth\", titlePadding=30)),\n    color=alt.value('white')\n\n)\n\nhm = base.mark_rect(stroke='black')\n    \ntext = base.mark_text(baseline='middle', size=15).encode(\n    \n    text='z:Q',\n    color=alt.value('black')\n    \n)\n\n(hm+text).properties(width=300, height=300)","e87a1337":"print(\"Accuracy:\", accuracy_score(y_val, y_pred, normalize=True))\nprint(\"F1:\", f1_score(y_val, y_pred, average=\"macro\"))","f7deb468":"tweets = [\n    \"\"\"\"Avrupa'da yollara \u00fccret \u00f6demeyen ancak T\u00fcrkiye'de otobana 47,50 TL veren gurbet\u00e7inin isyan\u0131; 'Ben Avrupa'y\u0131 kar\u0131\u015f kar\u0131\u015f geziyorum 1 kuru\u015f \u00f6demiyorum, a\u011flamam geldi , 1 haftada t\u00fcm dengem bozuldu. Avrupa Kap\u0131lar\u0131 A\u00e7sa Herkes Ka\u00e7acak'\" \"\"\",\n    \"\"\"Ara\u015ft\u0131rmalar k\u00f6peklerin #Covid_19'u y\u00fczde 100 do\u011fru olarak tespit etti\u011fini g\u00f6sterince Finlandiya Helsinki Havaalan\u0131'nda \u00f6zel e\u011fitimli K\u00f6ssi ve Miina adl\u0131 #k\u00f6pek'ler pilot olarak \u00e7al\u0131\u015fmaya ba\u015flad\u0131. Umar\u0131m hayvanlara zulmedenler bu g\u00fczellikten mahrum kal\u0131r\"\"\",\n    \"\"\"Sovyet Tanklar\u0131 Berline girene kadar Almanlar Rusya'y\u0131 i\u015fgal ettiklerini zannediyordu. \u00c7\u00fcnk\u00fc Alman gazeteleri \u00f6yle yaz\u0131yordu.\"\"\",\n    \"\"\"Yalanc\u0131y\u0131 E\u015fek siksinmi Avrupada Almanya hari\u00e7 b\u00fct\u00fcn \u00fclkeler paral\u0131\"\"\",\n    \"\"\"Yarrana s\u0131\u00e7am\"\"\",\n    \"\"\"200 liraya rak\u0131 m\u0131 olur orospu \u00e7ocuklar\u0131 jet yak\u0131t\u0131 m\u0131 al\u0131yoruz\"\"\"\n]\n\ntweets_embeds = []\nfor tweet in tweets:\n    d = Doc(tweet)\n    tweets_embeds.append(csr_matrix(d.tfidf_embeddings.mean(axis=0)))\n\n\ntweets_embeds = vstack(tweets_embeds)\ntweets_embeds = normalize(tweets_embeds)\n\n\n\npreds = clf.predict(tweets_embeds)\npreds","5bcac09c":"cls_to_human = {\n    \"non\": \"Non-offensive\",\n    \"grp\": \"Offense towards group\",\n    \"prof\": \"Profanity\",\n    \"ind\": \"Offense towards individual\",\n    \"oth\": \"Other profanities\"\n}","f2b0b050":"cls_to_human[preds[0]]","ee31facb":"cls_to_human[preds[1]]","6d2ecb1c":"cls_to_human[preds[2]]","27970f54":"cls_to_human[preds[3]]","6a107e54":"cls_to_human[preds[4]]","db4f6cb2":"cls_to_human[preds[5]]","bd780a49":"---\n\n### References\nTurkish offensive tweets dataset: https:\/\/coltekin.github.io\/offensive-turkish\n\nSadedegel: https:\/\/sadedegel.ai\n","dd7fa830":"Now, just like with above example, all that's left to do is to construct the training array, which will store embeddings for every tweet. Note that this might take a few minutes as it processess everything.","2d1ce085":"## Offensive Turkish Tweets Classification\n### with Sadedegel and Sklearn\n#### by Askar Bozcan","7d40b85c":"---\n\n### Testing model on tweets found in the wild","00fcca1f":"### Data","4280325e":"<p style=\"background-color:#ffb0b0; padding:10px; border:1px solid black; border-radius:1px\">\n<a style=\"color:green; font-weight:bold\">Profanity<\/a><br>\n    Yarrana s\u0131\u00e7am\n<\/p>\n","e568dbee":"\nIt seems that our dataset is highly imbalanced with most tweets being inoffensive, which would severely hamper our model's perforamance if we don't account for it.\n<br>\n\n---","e0fec1ec":"---\n<br>\nNext step is to convert text into TFIDF embeddings. Luckily getting those in Sadedegel is pretty easy and all you have to do is:\n<ol>\n    <li style=\"font-weight:bold\"><span style=\"font-weight:normal\">Turn the text into a Doc object.<\/span><\/li>\n    <li style=\"font-weight:bold\"><span style=\"font-weight:normal\">Use .tfidf_embeddings property of Doc to get TFIDF embeddings in form of [NxV] where N is the number of sentences in the text and V is the size of vocabulary.<\/span><\/li>\n    <li style=\"font-weight:bold\"><span style=\"font-weight:normal\">Average embeddings over the sentences in a tweet to get a single vector for a tweet which would be what is fed to the classifier as vector representation of the tweet.<\/span><\/li>\n<\/ol>\n\nHere is an example on 1 tweet:","dfc33f3a":"<p style=\"background-color:#ebfcff; padding:10px; border:1px solid black; border-radius:1px\">\n<a style=\"color:green; font-weight:bold\">Non-offensive<\/a><br>\n\"Avrupa'da yollara \u00fccret \u00f6demeyen ancak T\u00fcrkiye'de otobana 47,50 TL veren gurbet\u00e7inin isyan\u0131;\n'Ben Avrupa'y\u0131 kar\u0131\u015f kar\u0131\u015f geziyorum 1 kuru\u015f \u00f6demiyorum, a\u011flamam geldi , \n1 haftada t\u00fcm dengem bozuldu. Avrupa Kap\u0131lar\u0131 A\u00e7sa Herkes Ka\u00e7acak'\"\n<\/p>","0ff0ce5f":"Now that we have our training data, let's split the data into train and validation set, in a stratified fashion to prevent train and validation sets having different label distributions.","3e573625":"<p style=\"background-color:#ebfcff; padding:10px; border:1px solid black; border-radius:1px\">\n<a style=\"color:green; font-weight:bold\">Non-offensive<\/a> <br>\n    Ara\u015ft\u0131rmalar k\u00f6peklerin #Covid_19'u y\u00fczde 100 do\u011fru olarak tespit etti\u011fini g\u00f6sterince Finlandiya Helsinki Havaalan\u0131'nda \u00f6zel e\u011fitimli K\u00f6ssi ve Miina adl\u0131 #k\u00f6pek'ler pilot olarak \u00e7al\u0131\u015fmaya ba\u015flad\u0131. Umar\u0131m hayvanlara zulmedenler bu g\u00fczellikten mahrum kal\u0131r\n<\/p>\n","a29b005b":"Great! Both train and validation have similar distributions, however class imbalance is not gone. There are many ways to address this issue but for our particular purpose we'll use LogisticRegression's `class_weight` attribute which will allow us to automatically weigh each class.","5e786be1":"For metric purposes, let's look at the confusion matrix and f1-score. Accuracy in this case is absolutely not the best metric due to highly unbalanced classes. If model just guessed \"non\", or non-offensive it would be quite accurate despite being useless as \"non\" class is a big majority class.","1c8c9bb0":"*A small disclaimer: The data at hand contains offensive content. Opinions presented in the tweets are those of their users and not Sadedegel team.*\n\n---","e7ca7743":"---\n\n### Train and validate","691f6118":"![logo-2.png](attachment:logo-2.png)","ca9ac8af":"---\n\n<br>\n\n**What are the class counts?**\n\nIt is imperative to know how imbalanced the dataset is to make right decisions regarding splitting the data.","55dde24c":"Now that our classifier is trained, let's predict our validation set.","a8e1c12e":"<p style=\"background-color:#ffb0b0; padding:10px; border:1px solid black; border-radius:1px\">\n<a style=\"color:green; font-weight:bold\">Profanity<\/a><br>\n200 liraya rak\u0131 m\u0131 olur orospu \u00e7ocuklar\u0131 jet yak\u0131t\u0131 m\u0131 al\u0131yoruz\n<\/p>","ee55fe09":"<\/p>\n<p style=\"background-color:#ffb0b0; padding:10px; border:1px solid black; border-radius:1px\">\n<a style=\"color:green; font-weight:bold\">Profanity<\/a><br>\nYalanc\u0131y\u0131 E\u015fek siksinmi\nAvrupada Almanya hari\u00e7 b\u00fct\u00fcn \u00fclkeler paral\u0131\n<\/p>\n","845def87":"Not bad, but can be much better. However considering we used basic logistic regression, no hyperparameter optimization at all and basic features (tf-idf) this is a pretty decent result.\n\n**How can we improve it?**\n* Use BERT embeddings, as they are much richer in semantic information compared to tf-idf embeddings\n* Use a more sophisticated classifier\n* Feature selection, as tfidf vectors are huge (20k+ features, mostly 0s)\n* Normalize text before getting the embeddings.","4d035ce9":"The data we have at hand is a compilation of 35284 Turkish tweets, labelled by their type of offensive language. (Note that user handles are censored for privacy.)\n\n**What are the labels?**\n\n<ul>\n    <li><b>non<\/b>offensive: Non-offensive tweets.<\/li>\n    <li><b>prof<\/b>anity, or \u201cuntargeted offense\u201d, is use of offensive language without a particular target. This is typically use of swear words, or other \u201cinappropriate\u201d language for stylistic reasons.<\/li>\n    <li><b>grp<\/b> (offense towards a group) means the author intends to offend a group that forms a \u201cunity\u201d based on gender, ethnicity, political affiliation, religious belief or similar aspects of a person\u2019s identity.<\/li>\n    <li><b>ind<\/b> (offensive towards an individual) is used for offensive targeted to an individual or group of individuals that are not related in well-defined manner.<\/li>\n    <li><b>oth<\/b> target of the offense is not one of the two cases above. This typically include organizations, events etc.<\/li>\n<\/ul>","73db0677":"Let's plot train and validation sets labels counts to ensure they have a similar label distribution.","6dfcef1b":"As a last preprocessing step, let's normalize samples in our embeddings matrix.","44202606":"### Preprocessing\n\nAlthough most of the necessary preprocessing will be done by Sadedegel, we need to remove @User handles beforehand as the handles are useless, and may even hamper our model. A regex will be used to replace them with a blank string.","161be11d":"Now, how about we test this on tweets found in the wild and see how our model will perform. 3 tweets non-offensive and 3 tweets of profanity class were picked. *Sadly, I had a hard time finding tweets of other profanity categories.*\n\nHere are the tweets (predictions with code cells above tweets and their labels, as labelled by <a style=\"color:green; font-weight:bold\">me<\/a>):","eb239179":"<p style=\"background-color:#ebfcff; padding:15px; border:1px solid black; border-radius:1px\">\n<a style=\"color:green; font-weight:bold\">Non-offensive<\/a> <br>\nSovyet Tanklar\u0131 Berline girene kadar Almanlar Rusya'y\u0131 i\u015fgal ettiklerini zannediyordu. \u00c7\u00fcnk\u00fc Alman gazeteleri \u00f6yle yaz\u0131yordu.\n<\/p>","cb291851":"However, what about accuracy and f1-score?"}}