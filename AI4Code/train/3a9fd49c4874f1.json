{"cell_type":{"8b224e2d":"code","e0d29954":"code","06e510ba":"code","6b1cef1f":"code","5787d0c8":"code","e6eece03":"code","66566adc":"code","9a129e96":"code","23f4dd5d":"code","263aba21":"code","d11eb98b":"code","2f6ae30e":"code","4c3a8162":"code","e7613441":"code","7401b641":"code","bbf81163":"code","6515aa8b":"code","b048efeb":"code","1d6919ff":"code","06d266f5":"code","2c2f9430":"code","7281bf12":"code","ec5327ba":"code","8ca1b567":"code","fb97b448":"code","d05fad0e":"code","3b886e33":"code","5dbf4910":"code","f9eb8be5":"code","93468686":"code","dc7acd71":"code","ab1b9fe8":"code","4ea760c1":"code","30e32b44":"code","f70f9ac1":"code","ab3eeeda":"code","5faae8ae":"code","5b9c1273":"code","e5d6f4a2":"code","76f7bf20":"code","bd7b4350":"code","001494c0":"code","83cce5ec":"code","683f1eef":"code","9fdb04d1":"code","0f779e14":"code","66116a54":"code","1fe6b46c":"code","a526ace9":"code","50b06e16":"code","441f2903":"code","755eb42b":"code","d8f4420d":"code","f47923bd":"code","f44d3e70":"code","b7ec82c1":"code","46bab012":"code","c562a4ab":"markdown","6adb656c":"markdown","5b090837":"markdown","63916b51":"markdown","fdece976":"markdown","a0ab3228":"markdown","b6ab0cc9":"markdown","0d3cd6b5":"markdown","187be24b":"markdown","46bf42be":"markdown","e3e99e72":"markdown","ffedd0cf":"markdown","c3ec02ce":"markdown","973cb4f9":"markdown","8da8e68c":"markdown","0849f9a8":"markdown","631a24be":"markdown","7ba12a02":"markdown","b9839f1d":"markdown","7250ce72":"markdown","cabecd20":"markdown","8ef3fbaf":"markdown","645f0d8d":"markdown","5ac33763":"markdown","dc9712f6":"markdown","5eefc65d":"markdown","c66734e0":"markdown","e1b00717":"markdown","61308854":"markdown","d17eb520":"markdown"},"source":{"8b224e2d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sp\nimport warnings\nimport os \nwarnings.filterwarnings(\"ignore\")\nimport datetime\n","e0d29954":"data=pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')\n","06e510ba":"data.head()      #displaying the head of dataset they gives the 1st to 5 rows of the data","6b1cef1f":"data.describe()      #description of dataset ","5787d0c8":"data.info()","e6eece03":"data.shape       #569 rows and 33 columns","66566adc":"data.columns     #displaying the columns of dataset","9a129e96":"data.value_counts","23f4dd5d":"data.dtypes","263aba21":"data.isnull().sum()","d11eb98b":"data.drop('Unnamed: 32', axis = 1, inplace = True)\n","2f6ae30e":"data","4c3a8162":"data.corr()","e7613441":"plt.figure(figsize=(18,9))\nsns.heatmap(data.corr(),annot = True, cmap =\"Accent_r\")\n\n\n\n","7401b641":"sns.barplot(x=\"id\", y=\"diagnosis\",data=data[160:190])\nplt.title(\"Id vs Diagnosis\",fontsize=15)\nplt.xlabel(\"Id\")\nplt.ylabel(\"Diagonis\")\nplt.show()\nplt.style.use(\"ggplot\")\n","bbf81163":"sns.barplot(x=\"radius_mean\", y=\"texture_mean\", data=data[170:180])\nplt.title(\"Radius Mean vs Texture Mean\",fontsize=15)\nplt.xlabel(\"Radius Mean\")\nplt.ylabel(\"Texture Mean\")\nplt.show()\nplt.style.use(\"ggplot\")\n","6515aa8b":" \nmean_col = ['diagnosis','radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\n\nsns.pairplot(data[mean_col],hue = 'diagnosis', palette='Accent')\n","b048efeb":"sns.violinplot(x=\"smoothness_mean\",y=\"perimeter_mean\",data=data)","1d6919ff":"plt.figure(figsize=(14,7))\nsns.lineplot(x = \"concavity_mean\",y = \"concave points_mean\",data = data[0:400], color='green')\nplt.title(\"Concavity Mean vs Concave Mean\")\nplt.xlabel(\"Concavity Mean\")\nplt.ylabel(\"Concave Points\")\nplt.show()\n\n","06d266f5":"worst_col = ['diagnosis','radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']\n\nsns.pairplot(data[worst_col],hue = 'diagnosis', palette=\"CMRmap\")","2c2f9430":"# Getting Features\n\nx = data.drop(columns = 'diagnosis')\n\n# Getting Predicting Value\ny = data['diagnosis']\n","7281bf12":"\n#train_test_splitting of the dataset\nfrom sklearn.model_selection import train_test_split \nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)\n","ec5327ba":"print(len(x_train))\n","8ca1b567":"print(len(x_test))","fb97b448":"print(len(y_train))","d05fad0e":"print(len(y_test))","3b886e33":"from sklearn.linear_model import LogisticRegression\nreg = LogisticRegression()\nreg.fit(x_train,y_train)                         \n","5dbf4910":"y_pred=reg.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",reg.score(x_train,y_train)*100)\n\n\n","f9eb8be5":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata\n\n\n\n\n","93468686":"print(accuracy_score(y_test,y_pred)*100)","dc7acd71":"from sklearn.model_selection import GridSearchCV\nparam = {\n         'penalty':['l1','l2'],\n         'C':[0.001, 0.01, 0.1, 1, 10, 20,100, 1000]\n}\nlr= LogisticRegression(penalty='l1')\ncv=GridSearchCV(reg,param,cv=5,n_jobs=-1)\ncv.fit(x_train,y_train)\ncv.predict(x_test)\n","ab1b9fe8":"print(\"Best CV score\", cv.best_score_*100)","4ea760c1":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(max_depth=6, random_state=123)\n\ndtree.fit(x_train,y_train)\n\n#y_pred = dtree.predict(x_test)\n","30e32b44":"y_pred=dtree.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",dtree.score(x_train,y_train)*100)\n\n","f70f9ac1":"print(accuracy_score(y_test,y_pred)*100)","ab3eeeda":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier()\nrfc.fit(x_train,y_train)\n\n","5faae8ae":"y_pred=rfc.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",rfc.score(x_train,y_train)*100)\n","5b9c1273":"print(accuracy_score(y_test,y_pred)*100)","e5d6f4a2":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=7)\n\nknn.fit(x_train,y_train)\n","76f7bf20":"y_pred=knn.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",knn.score(x_train,y_train)*100)\nprint(knn.score(x_test,y_test))\n","bd7b4350":"print(accuracy_score(y_test,y_pred)*100)\n","001494c0":"from sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\n","83cce5ec":"y_pred=svc.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",svc.score(x_train,y_train)*100)\nprint(svc.score(x_test,y_test))\n","683f1eef":"print(\"Training Score: \",svc.score(x_train,y_train)*100)","9fdb04d1":"from sklearn.ensemble import AdaBoostClassifier\nadb = AdaBoostClassifier(base_estimator = None)\nadb.fit(x_train,y_train)\n\n\n\n\n\n","0f779e14":"y_pred=adb.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",adb.score(x_train,y_train)*100)","66116a54":"print(accuracy_score(y_test,y_pred)*100)","1fe6b46c":"from sklearn.ensemble import GradientBoostingClassifier\ngbc=GradientBoostingClassifier()\ngbc.fit(x_train,y_train)\n","a526ace9":"y_pred=gbc.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",gbc.score(x_train,y_train)*100)\nprint(gbc.score(x_test,y_test))\n","50b06e16":"print(accuracy_score(y_test,y_pred)*100)","441f2903":"from xgboost import XGBClassifier\n\nxgb =XGBClassifier(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10)\n\nxgb.fit(x_train, y_train)\n","755eb42b":"y_pred=xgb.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",xgb.score(x_train,y_train)*100)\nprint(xgb.score(x_test,y_test))\n","d8f4420d":"print(\"Training Score: \",xgb.score(x_train,y_train)*100)","f47923bd":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata","f44d3e70":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(x_train,y_train)","b7ec82c1":"y_pred=gnb.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(accuracy_score(y_test,y_pred))\nprint(\"Training Score: \",gnb.score(x_train,y_train)*100)\nprint(gnb.score(x_test,y_test))\n","46bab012":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata","c562a4ab":"**So now we conclude the accuracy of different models:**\n\n**1. AdaBoost Classifier = 98.24 %**\n\n**2. XGB Classifier= 97.84 %**\n\n**3. Random Forest Classifier =96.57 %**\n\n**4. Gradient Boosting Classifier= 95.66%**\n\n**5. Decision Tree Classifier= 94.78 %**\n\n**6. K Neighbours Classifier= 70.18 %**\n\n**7. SVC = 63.80 %**\n\n**8. Naiye Bayes= 63.30 %**\n\n**9. Logistic Regression = 58.82%**\n","6adb656c":"**So we have to drop the Unnamed: 32 coulumn which contains NaN values**","5b090837":"# 1. Logistic Regression","63916b51":"**So we get a accuracy score of 63.29 % using Naive Bayes**","fdece976":"# TRAINING AND TESTING DATA","a0ab3228":"What Are the Symptoms of Breast Cancer?\n\nNew lump in the breast or underarm (armpit).\n\nThickening or swelling of part of the breast.\n\nIrritation or dimpling of breast skin.\n\n\nRedness or flaky skin in the nipple area or the breast.\n\nPulling in of the nipple or pain in the nipple area.\n\nNipple discharge other than breast milk, including blood.\n","b6ab0cc9":"**Task : To predict whether the cancer is benign or malignant**","0d3cd6b5":"# 6. AdaBoostClassifier","187be24b":"**So we get a accuracy score of 96.49 % using Random Forest Classifier**","46bf42be":"# 2. DECISION TREE CLASSIFIER","e3e99e72":"**So we get a accuracy score of 58.7 % using logistic regression**","ffedd0cf":"**So we get a accuracy score of 98.24 % using AdaBoostClassifier**","c3ec02ce":"# If you liked this notebook, please UPVOTE it.","973cb4f9":"**So we get a accuracy score of 70.17 % using KNeighborsClassifier**","8da8e68c":"# LOADING THE DATASET","0849f9a8":"# 8. XGBClassifier","631a24be":"# IMPORTING THE LIBRARIES","7ba12a02":"# 3. Random Forest Classifier","b9839f1d":"# 4. KNeighborsClassifier\n\n","7250ce72":"# 5. SVC","cabecd20":"**So we get a accuracy score of 95.61 % using GradientBoostingClassifier**","8ef3fbaf":"# MODELS","645f0d8d":"# VISUALIZING THE DATA","5ac33763":"**So we get a accuracy score of 97.80 % using  XGBClassifier**","dc9712f6":"**Ada Boost Classifier got the highest accuracy**","5eefc65d":"# Breast Cancer (Diagnostic) Data Set\n","c66734e0":"**So we get a accuracy score of 94.73 % using Decision Tree Classifier**","e1b00717":"**So we get a accuracy score of 63.7 % using SVC**","61308854":"#  7. Gradient Boosting Classifier","d17eb520":"# 9. Naive Bayes"}}