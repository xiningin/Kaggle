{"cell_type":{"9b63e6d8":"code","198d2769":"code","6fa185ee":"code","ed0dbcfa":"code","8125c70f":"code","00786993":"code","4fd6f8d0":"code","43239f38":"code","64574f9c":"code","21b9be7f":"code","2ffad1b2":"code","b818dad3":"code","83616fd1":"code","b947230d":"code","059a397f":"code","d56bb08d":"code","2c8c6263":"code","fead0180":"code","67fb52bc":"code","53b80906":"code","cee68f59":"code","1a6d6933":"code","8dc64db3":"code","cf46a447":"code","eb888513":"code","5d831ad2":"code","cfea84e2":"markdown","a75b155a":"markdown","3c0d90ba":"markdown","5fbaef2f":"markdown","e47c9674":"markdown","171e0a3d":"markdown","d84c6065":"markdown","1a4cccbd":"markdown","1d84e10f":"markdown","d0b946d1":"markdown","423cecfe":"markdown","cf093e4c":"markdown","2f0f4b9e":"markdown"},"source":{"9b63e6d8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nDAYS_BACK = 28","198d2769":"def add_snap_col(df_in):\n    \"\"\"adds a 'snap_day' column to a dataframe that contains a state_id column and the columns 'snap_CA', 'snap_TX', \n    and snap_WI\"\"\"\n    df = pd.get_dummies(df_in, columns=['state_id'])\n    df['snap_day'] = (df.snap_CA * df.state_id_CA) + (df.snap_WI * df.state_id_WI) + (df.snap_TX * df.state_id_TX)\n    del df['state_id_WI'], df['state_id_CA'], df['state_id_TX']\n    return df\n\n\n\ndef melt_merge_snap(df):\n    df = df.melt(['id', 'state_id'], var_name='d', value_name='demand')\n    df = df.merge(cal)\n    df = add_snap_col(df)\n    return df\n\n\n\ndef get_sub():\n    \"\"\"returns a tidy dataframe version of the sample submission, merged with the calendar data, \n    without the 'demand' column. It can be used to join to a group by series to make predictions  \"\"\"\n    # make a copy of the sample submission\n    sub = ss.copy()\n    # change the column names to match the last 28 days\n    sub.columns = ['id'] + ['d_' + str(1914+x) for x in range(28)]\n    # select only the rows with an id with the validation tag\n    sub = sub.loc[sub.id.str.contains('validation')]\n    # melt this dataframe and merge it with the calendar so we can join it with group_by series we create\n    sub = sub.melt('id', var_name='d', value_name='demand')\n    sub = sub.merge(cal)\n    \n    \n    # add state_id column so that we can add the snap_day column\n    sub['state_id'] = sub.id.str.split('_', expand=True)[3]\n    \n    # add the snap_day column\n    sub = add_snap_col(sub)\n    \n    return sub.drop('demand', axis='columns')\n\n\n\ndef join_sub_groupby(sub, group):\n    \"\"\" \n    Joins the sub dataframe created by get_sub to a groupby series\n    \"\"\"\n    return sub.join(group, on=group.index.names)\n\n\n\ndef make_sub(df_in, ss, filename='submission.csv'): \n    \"\"\"\n    Takes a dataframe in the form given by join_sub_groupby, or any dataframe with the proper index and and 'd' colums.\n    returns a csv submission file in the correct format\n    \"\"\"\n    # pivot df to get it into the proper format for submission\n    df = df_in.pivot(index='id', columns='d', values='demand')\n    # need to reset index to take care of columns. comment next line out to see what i mean \n    df.reset_index(inplace=True)\n    \n    submission = ss[['id']].copy()    \n    submission = submission.merge(df)    \n    # we must copy the dataframe to match the format of the submission file which is twice as long as what we have\n    submission = pd.concat([submission, submission], axis=0)\n    # reset the id colum to have the same values as the sample submission\n    submission['id'] = ss.id.values\n    # rename the columns to match the sample submission format \n    submission.columns = ['id'] + ['F' + str(i) for i in range(1,29)]\n    \n    submission.to_csv(filename, index=False)  ","6fa185ee":"PATH = '\/kaggle\/input\/m5-forecasting-accuracy\/'\ncal = pd.read_csv(f'{PATH}calendar.csv')\nsell_prices = pd.read_csv(f'{PATH}sell_prices.csv')\nss = pd.read_csv(f'{PATH}sample_submission.csv')\nstv = pd.read_csv(f'{PATH}sales_train_validation.csv')\nstv_id = stv[['id','state_id']]\nstv = stv.iloc[:, :-DAYS_BACK]","ed0dbcfa":"last_90 = pd.concat([stv_id, stv.iloc[:,-90:]], axis=1) # we include 0, and 5 to get the id and state id columns","8125c70f":"# last_28 = melt_merge_snap(last_28)\n# last_28.head()","00786993":"last_90 = melt_merge_snap(last_90)","4fd6f8d0":"last_90.head()","43239f38":"# get the demand for each product, grouped by weekday\n# by_weekday = last_28.groupby(['id','wday'])['demand'].mean()","64574f9c":"# by_weekday_snap = last_28.groupby(['id', 'wday', 'snap_day'])['demand'].mean()","21b9be7f":"by_weekday_snap_90 = last_90.groupby(['id', 'wday', 'snap_day'])['demand'].mean()","2ffad1b2":"# by_weekday_snap_365 = last_365.groupby(['id', 'wday', 'snap_day'])['demand'].mean()","b818dad3":"sub = get_sub()\nsub.head()","83616fd1":"# df_final_model_1 = join_sub_groupby(sub, by_weekday)","b947230d":"# df_final_model_2 = join_sub_groupby(sub, by_weekday_snap)","059a397f":"df_final_model_3 = join_sub_groupby(sub, by_weekday_snap_90)","d56bb08d":"# df_final_model_3_dark_magic = df_final_model_3.copy()\n# df_final_model_3_dark_magic['demand'] = df_final_model_3_dark_magic['demand'] * 1.04","2c8c6263":"# df_final_model_4 = join_sub_groupby(sub, by_weekday_snap_365)","fead0180":"# make_sub(df_final_model_1, ss, filename='model1sub.csv')","67fb52bc":"# make_sub(df_final_model_2, ss, filename='model2sub.csv')","53b80906":"make_sub(df_final_model_3, ss, filename='model3sub.csv')","cee68f59":"# make_sub(df_final_model_3_dark_magic, ss, filename='model3dmsub.csv')","1a6d6933":"# make_sub(df_final_model_4, ss, filename='model4sub.csv')","8dc64db3":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns; sns.set()\nimport gc\n\nfrom sklearn import preprocessing\nimport lightgbm as lgb\n\nfrom typing import Union\nfrom tqdm.notebook import tqdm_notebook as tqdm\n\nDATA_DIR = '\/kaggle\/input\/m5-forecasting-accuracy\/'\n\nclass WRMSSEEvaluator_dashboard(object):\n\n    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, \n                 calendar: pd.DataFrame, prices: pd.DataFrame):\n        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n        train_target_columns = train_y.columns.tolist()\n        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n\n        train_df['all_id'] = 'all'  # for lv1 aggregation\n\n        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')]\\\n                     .columns.tolist()\n        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')]\\\n                               .columns.tolist()\n\n        if not all([c in valid_df.columns for c in id_columns]):\n            valid_df = pd.concat([train_df[id_columns], valid_df], \n                                 axis=1, sort=False)\n\n        self.train_df = train_df\n        self.valid_df = valid_df\n        self.calendar = calendar\n        self.prices = prices\n\n        self.weight_columns = weight_columns\n        self.id_columns = id_columns\n        self.valid_target_columns = valid_target_columns\n\n        weight_df = self.get_weight_df()\n\n        self.group_ids = (\n            'all_id',\n            'state_id',\n            'store_id',\n            'cat_id',\n            'dept_id',\n            ['state_id', 'cat_id'],\n            ['state_id', 'dept_id'],\n            ['store_id', 'cat_id'],\n            ['store_id', 'dept_id'],\n            'item_id',\n            ['item_id', 'state_id'],\n            ['item_id', 'store_id']\n        )\n\n        for i, group_id in enumerate(tqdm(self.group_ids)):\n            train_y = train_df.groupby(group_id)[train_target_columns].sum()\n            scale = []\n            for _, row in train_y.iterrows():\n                series = row.values[np.argmax(row.values != 0):]\n                scale.append(((series[1:] - series[:-1]) ** 2).mean())\n            setattr(self, f'lv{i + 1}_scale', np.array(scale))\n            setattr(self, f'lv{i + 1}_train_df', train_y)\n            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)\\\n                    [valid_target_columns].sum())\n\n            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n            setattr(self, f'lv{i + 1}_weight', lv_weight \/ lv_weight.sum())\n\n    def get_weight_df(self) -> pd.DataFrame:\n        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns]\\\n                    .set_index(['item_id', 'store_id'])\n        weight_df = weight_df.stack().reset_index()\\\n                   .rename(columns={'level_2': 'd', 0: 'value'})\n        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n\n        weight_df = weight_df.merge(self.prices, how='left',\n                                    on=['item_id', 'store_id', 'wm_yr_wk'])\n        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n        weight_df = weight_df.set_index(['item_id', 'store_id', 'd'])\\\n                    .unstack(level=2)['value']\\\n                    .loc[zip(self.train_df.item_id, self.train_df.store_id), :]\\\n                    .reset_index(drop=True)\n        weight_df = pd.concat([self.train_df[self.id_columns],\n                               weight_df], axis=1, sort=False)\n        return weight_df\n\n    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n        valid_y = getattr(self, f'lv{lv}_valid_df')\n        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n        scale = getattr(self, f'lv{lv}_scale')\n        return (score \/ scale).map(np.sqrt) \n\n    def score(self, valid_preds: Union[pd.DataFrame, \n                                       np.ndarray]) -> float:\n        assert self.valid_df[self.valid_target_columns].shape \\\n               == valid_preds.shape\n\n        if isinstance(valid_preds, np.ndarray):\n            valid_preds = pd.DataFrame(valid_preds, \n                                       columns=self.valid_target_columns)\n\n        valid_preds = pd.concat([self.valid_df[self.id_columns], \n                                 valid_preds], axis=1, sort=False)\n\n        all_scores = []\n        for i, group_id in enumerate(self.group_ids):\n\n            valid_preds_grp = valid_preds.groupby(group_id)[self.valid_target_columns].sum()\n            setattr(self, f'lv{i + 1}_valid_preds', valid_preds_grp)\n            \n            lv_scores = self.rmsse(valid_preds_grp, i + 1)\n            setattr(self, f'lv{i + 1}_scores', lv_scores)\n            \n            weight = getattr(self, f'lv{i + 1}_weight')\n            lv_scores = pd.concat([weight, lv_scores], axis=1, \n                                  sort=False).prod(axis=1)\n            \n            all_scores.append(lv_scores.sum())\n            \n        self.all_scores = all_scores\n\n        return np.mean(all_scores)\n    \n\n    \ndef create_viz_df(df,lv):\n    \n    df = df.T.reset_index()\n    if lv in [6,7,8,9,11,12]:\n        df.columns = [i[0] + '_' + i[1] if i != ('index','') \\\n                      else i[0] for i in df.columns]\n    df = df.merge(calendar.loc[:, ['d','date']], how='left', \n                  left_on='index', right_on='d')\n    df['date'] = pd.to_datetime(df.date)\n    df = df.set_index('date')\n    df = df.drop(['index', 'd'], axis=1)\n    \n    return df\n\ndef create_dashboard(evaluator, by_level_only=False, model_name=None):\n    \n    wrmsses = [np.mean(evaluator.all_scores)] + evaluator.all_scores\n    labels = ['Overall'] + [f'Level {i}' for i in range(1, 13)]\n\n    ## WRMSSE by Level\n    plt.figure(figsize=(12,5))\n    ax = sns.barplot(x=labels, y=wrmsses)\n    ax.set(xlabel='', ylabel='WRMSSE')\n    \n    #######################ALTERATION##########################\n    title = 'WRMSSE by Level'\n    if model_name: \n        title = f'WRMSSE by Level for {model_name}'\n    plt.title(title, fontsize=20, fontweight='bold')\n    #######################ALTERATION-COMPLETE##########################\n\n  \n    for index, val in enumerate(wrmsses):\n        ax.text(index*1, val+.01, round(val,4), color='black', \n                ha=\"center\")\n        \n    #######################ALTERATION##########################\n    if by_level_only:       # stops function early for quick plotting of \n        plt.show()          # for quick plotting of levels\n        return\n    #######################ALTERATION-COMPLETE##########################\n\n    # configuration array for the charts\n    n_rows = [1, 1, 4, 1, 3, 3, 3, 3, 3, 3, 3, 3]\n    n_cols = [1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n    width = [7, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n    height = [4, 3, 12, 3, 9, 9, 9, 9, 9, 9, 9, 9]\n    \n    for i in range(1,13):\n        \n        scores = getattr(evaluator, f'lv{i}_scores')\n        weights = getattr(evaluator, f'lv{i}_weight')\n        \n        if i > 1 and i < 9:\n            if i < 7:\n                fig, axs = plt.subplots(1, 2, figsize=(12, 3))\n            else:\n                fig, axs = plt.subplots(2, 1, figsize=(12, 8))\n                \n            ## RMSSE plot\n            scores.plot.bar(width=.8, ax=axs[0], color='g')\n            axs[0].set_title(f\"RMSSE\", size=14)\n            axs[0].set(xlabel='', ylabel='RMSSE')\n            if i >= 4:\n                axs[0].tick_params(labelsize=8)\n            for index, val in enumerate(scores):\n                axs[0].text(index*1, val+.01, round(val,4), color='black', \n                            ha=\"center\", fontsize=10 if i == 2 else 8)\n            \n            ## Weight plot\n            weights.plot.bar(width=.8, ax=axs[1])\n            axs[1].set_title(f\"Weight\", size=14)\n            axs[1].set(xlabel='', ylabel='Weight')\n            if i >= 4:\n                axs[1].tick_params(labelsize=8)\n            for index, val in enumerate(weights):\n                axs[1].text(index*1, val+.01, round(val,2), color='black', \n                            ha=\"center\", fontsize=10 if i == 2 else 8)\n                    \n            fig.suptitle(f'Level {i}: {evaluator.group_ids[i-1]}', size=24 ,\n                         y=1.1, fontweight='bold')\n            plt.tight_layout()\n            plt.show()\n\n        trn = create_viz_df(getattr(evaluator, f'lv{i}_train_df')\\\n                            .iloc[:, -28*3:], i)\n        val = create_viz_df(getattr(evaluator, f'lv{i}_valid_df'), i)\n        pred = create_viz_df(getattr(evaluator, f'lv{i}_valid_preds'), i)\n\n        n_cate = trn.shape[1] if i < 7 else 9\n\n        fig, axs = plt.subplots(n_rows[i-1], n_cols[i-1], \n                                figsize=(width[i-1],height[i-1]))\n        if i > 1:\n            axs = axs.flatten()\n\n        ## Time series plot\n        for k in range(0, n_cate):\n\n            ax = axs[k] if i > 1 else axs\n\n            trn.iloc[:, k].plot(ax=ax, label='train')\n            val.iloc[:, k].plot(ax=ax, label='valid')\n            pred.iloc[:, k].plot(ax=ax, label='pred')\n            ax.set_title(f\"{trn.columns[k]}  RMSSE:{scores[k]:.4f}\", size=14)\n            ax.set(xlabel='', ylabel='sales')\n            ax.tick_params(labelsize=8)\n            ax.legend(loc='upper left', prop={'size': 10})\n\n        if i == 1 or i >= 9:\n            fig.suptitle(f'Level {i}: {evaluator.group_ids[i-1]}', size=24 , \n                         y=1.1, fontweight='bold')\n        plt.tight_layout()\n        plt.show()\n        \ntrain_df = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\ncalendar = pd.read_csv('..\/input\/m5-forecasting-accuracy\/calendar.csv')\nsell_prices = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sell_prices.csv')\ntrain_df = train_df.loc[:, :'d_' + str(1913)]\n\ntrain_fold_df = train_df.iloc[:, :-28]\nvalid_fold_df = train_fold_df.iloc[:, -28:].copy()\n# Instantiate an evaluator for scoring validation periodstarting day 1886\ne = WRMSSEEvaluator_dashboard(train_fold_df, valid_fold_df, calendar, sell_prices)","cf46a447":"m = pd.read_csv('model3sub.csv')","eb888513":"preds = m.iloc[:30490, 1:].values * 1.01","5d831ad2":"_ = e.score(preds)\ncreate_dashboard(e, by_level_only=True)","cfea84e2":"# Lets try different time ranges to use. I will always include the most recent data","a75b155a":"# Simple models: Just using the last known 28 days. \nmodel 1: avg of last 28, grouped by id, weekday           **LB score: .75238**\n\nmodel 2: avg of last 28, grouped by id, weekday, and snap **LB score: .72969**\n\n#### same as simple models, but different time periods\n\nmodel 3: avg of last 90, grouped by id, weekday, and snap **LB score: .71444**\n\nmodel 3_dark_magic: avg of last 90, grouped by id, weekday, and snap**LB score: .67402**\n\nmodel 4: avg of last 365, grouped by id, weekday, and snap **LB score: 0.88767**","3c0d90ba":"# Create a model utilizing only numpy and pandas \n## Goals: \n* Create basic and explainable models\n* Create models that can improve the application of more advanced techniques\n\n## Allowed: \n* Using insights gained from other analysis(visual, statistical, ML), but the final model must be constructed \"by hand\" with only numpy and pandas. \n\n\n\n## Not allowed: \n* using any other imports when constructing final csv for submission\n\n## Beginners: Please fork this as a starter. The functions I created to group the data and create submissions could be useful. \n\n## Intermediate\/Advanced: Use knowledge extracted from your advanced algorithms and apply it to create a simple, explainable model. \n ","5fbaef2f":"# Top scores and links to notebook posted here: \n*  0.67402[this notebook](https:\/\/www.kaggle.com\/chrisrichardmiles\/numpy-pandas-challenge-current-leader-71444\/edit)\n* 0.71444 [this notebook](https:\/\/www.kaggle.com\/chrisrichardmiles\/numpy-pandas-challenge-current-leader-71444\/edit)","e47c9674":"# Make a submission file of the model","171e0a3d":"# Model 1: group by id, and wday and average over demand","d84c6065":"# Join the sub dataframe to a group_by series and create our final dataframe","1a4cccbd":"# Model 2: groupby id, wday, snap_day","1d84e10f":"# Helper functions","d0b946d1":"# Melt the d_ columns, merge with calendar, and add a snap_day column. Snap column indicates if the item is snap eligible that day.","423cecfe":"# Prepare a copy of the submission file to merge with the groupby series","cf093e4c":"# Load data","2f0f4b9e":"# Now apply more insight to creating a simple model"}}