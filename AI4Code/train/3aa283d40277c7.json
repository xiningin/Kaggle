{"cell_type":{"181bda2f":"code","709aa8e3":"code","8bc9141f":"code","9397b141":"code","e7d372de":"code","546bd758":"code","7569e218":"code","f3d58a51":"code","95b14cce":"code","ece1c73d":"code","5a8ff54c":"code","0b5c0a32":"code","e442a747":"code","256927eb":"code","08d4f247":"code","1543f682":"code","b105c1a2":"code","e0f84004":"code","7d5e0185":"code","7d674404":"code","bc716c2d":"code","82eb021b":"code","01e5ac65":"code","17f1d59f":"markdown","b4ee40fb":"markdown","14d91328":"markdown","de2c46bb":"markdown","3489d1b2":"markdown","8d670ab7":"markdown","69bace6f":"markdown","d3883b7e":"markdown","0194a4ef":"markdown","2b065536":"markdown","23a1c2ca":"markdown","d13e4712":"markdown","b63f2f43":"markdown","71751148":"markdown","c0abef06":"markdown","664d6ac3":"markdown","78ed5cc0":"markdown","12e3f41a":"markdown","6c4cdbb0":"markdown","653c26b6":"markdown","f1425697":"markdown","1c9246ab":"markdown","f248e3e1":"markdown","a4c940fa":"markdown","f95be192":"markdown","ce7515f7":"markdown","aef59154":"markdown","c6c122ae":"markdown","4d638956":"markdown","503b61d2":"markdown","6977103e":"markdown","c1681b9d":"markdown","1bdca66b":"markdown","7324c2a1":"markdown","7b34710b":"markdown","8893cdb5":"markdown","7bfd235b":"markdown","a47c81c9":"markdown"},"source":{"181bda2f":"from sklearn import datasets \niris = datasets.load_iris()","709aa8e3":"print(\"keys of iris dataset: \\n{}\".format(iris.keys()))","8bc9141f":"print(iris['DESCR'][:193])","9397b141":"print(\"Target names: {}\".format(iris['target_names']))","e7d372de":"print(\"Feature names: {}\".format(iris['feature_names']))","546bd758":"iris['data'][:5]","7569e218":"iris['data'].shape","f3d58a51":"iris['target']","95b14cce":"iris['target_names']","ece1c73d":"X = iris.data[:, :2]\ny = iris.target","5a8ff54c":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(8,6))\nsns.scatterplot(x=X[:, 0], y=X[:, 1], hue=iris.target_names[y])","0b5c0a32":"X_new = [[6.0, 3.6]]","e442a747":"sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=iris.target_names[y])\nplt.plot(X_new[0][0], X_new[0][1], 'ro')","256927eb":"from sklearn.neighbors import KNeighborsClassifier\nknn_clf = KNeighborsClassifier(n_neighbors=1)\nknn_clf.fit(X, y)\n\nprediction = knn_clf.predict(X_new)\nprint(\"Predicted class: {}\".format(iris.target_names[prediction]))","08d4f247":"knn_clf = KNeighborsClassifier(n_neighbors=3)\nknn_clf.fit(X, y)\n\nprediction = knn_clf.predict(X_new)\nprint(\"Predicted class: {}\".format(iris.target_names[prediction]))","1543f682":"h = 0.02  # step size in the mesh","b105c1a2":"from matplotlib.colors import ListedColormap\n# Create color maps\ncmap_light = ListedColormap(['orange', 'cyan', 'cornflowerblue'])\ncmap_bold = ['darkorange', 'c', 'darkblue']","e0f84004":"import numpy as np\ndef decision_boundary(k):\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = KNeighborsClassifier(n_neighbors=k)\n    clf.fit(X, y)\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure(figsize=(8, 6))\n    plt.contourf(xx, yy, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=iris.target_names[y],\n                    palette=cmap_bold, alpha=1.0, edgecolor=\"black\")\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title(\"3-Class classification (k = %i')\"\n              % (k))\n    plt.xlabel(iris.feature_names[0])\n    plt.ylabel(iris.feature_names[1])","7d5e0185":"decision_boundary(k=1)","7d674404":"import numpy as np\nfor k in [1, 15, 30]: \n    decision_boundary(k)","bc716c2d":"X = iris.data\ny = iris.target","82eb021b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","01e5ac65":"import matplotlib.pyplot as plt\nscoreList = []\nfor k in range(1,30):\n    knn_clf=KNeighborsClassifier(n_neighbors=k)  \n    knn_clf.fit(X_train, y_train)\n    scoreList.append(knn_clf.score(X_test, y_test))\n    \nplt.plot(range(1,30), scoreList)\nplt.xticks(np.arange(1,30,1))\nplt.xlabel(\"K value\")\nplt.ylabel(\"Score\")\nplt.show()","17f1d59f":"For single neighbor the decision boundary follows the training data closely. Decision boundary for setosa is clear, but for versicolor and virginica it is bit distorted. ","b4ee40fb":"The meanings of the numbers are given by the iris['target_names'] array","14d91328":"0 means setosa, 1 means versicolor, and 2 means virginica.","de2c46bb":"The new data point is in Red color","3489d1b2":"To make the prediction for a new data point, the algorithm finds the closest data points in the training dataset, its 'nearest neighbors' and computes a simple majority vote of the nearest neighbors.","8d670ab7":"## Meet the Data","69bace6f":"Let us apply KNeighborsClassifier by setting n_neighbors parameter as 1. n_neighbors parameter controls the number of neighbor to consider while predicting the class of the new data point. Default value of n_neighbors is 5.","d3883b7e":"We color the plane according to the class that would be assigned to a point in this region.","0194a4ef":"## Evaluate the performance of knn on complete dataset for different numbers of neighbors.","2b065536":"Let us visualize the decision boundaries for 1, 15, 30 neighbors","23a1c2ca":"Let us include it in the plot.","d13e4712":"## Predict the new data point","b63f2f43":"Let us illustrate the prediction for all possible test points in the xy-plane by creating a mesh.","71751148":"We will start with considering first two features of Iris dataset and apply k-nearest neighbor algorithm as ploting predictions in 2-dimensional space will be convenient.","c0abef06":"Let us consider all the features.","664d6ac3":"## Decision Boundary","78ed5cc0":"For example let us consider 3 closest neighbors.","12e3f41a":"## Pros and Cons of k-NN","6c4cdbb0":"As we can see that the prediction this time is 'setosa' which is not the same as the prediction when we used only one neighbor. If we change number of neighbors to consider, classication can differ as it takes majority class label.","653c26b6":"In other words, considering only few neighbors corresponds to high model complexity. Considering many neighbors corresponds to low model complexity","f1425697":"Considering more and more neighbors leads to a smoother decision boundary. A smoother boundary corresponds to a simpler model.","1c9246ab":"The target array contains the species of each of the flowers that were measured, also as a NumPy array:","f248e3e1":"The rows in the data array correspond to flowers, while the columns represent the four measurements that were taken for each flower.","a4c940fa":"When considering more than one neighbor, knn uses voting to assign a label. This means that for each test point, knn counst how many neighbors belong to each class and assigns the majority class among the k-nearest neighbors.","f95be192":"The iris object returned by load_iris() is a bunch object, which is very similar to a dictionary. It contains keys and values:","ce7515f7":"The plot shows the test set accuracy on the y-axis against the setting of n_neighbors on the x-axis.\n\nIf we consider n_neighbors less than 7, model is too complex and gives low test accuracy\n\nIf we consider n_neighbors more than 25 neighbors the model becomes too simple and test accuracy drops.\n\nThe best performance is somewhere in the middle, between 7 and 25.","aef59154":"Neighbors-based classification is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training dataset. ","c6c122ae":"Now let us consider a new data point which we want classify based on class of nearest neighbors. ","4d638956":"The value of the key DESCR is a short description of the dataset.","503b61d2":"The data itself is contained in the target and data fields. data contains the numeric measurements of sepal length, sepal width, petal length, and petal width in a NumPy array.","6977103e":"Let us consider Iris dataset to explore k-nearest neighbor. Iris dataset is classical dataset used in machine learning and statistics. It is included in scikit-learn in the datasets modules.","c1681b9d":"* One of the strengths of k-NN is that the model is very easy to understand, and often gives reasonable performance without a lot of adjustments.\n\n* Using this algorithm is a good baseline method to try before considering more advanced techniques.\n \n* Building the nearest neighbors model is usually very fast, but when the training set is very large containing large numberof features or large number of samples, the prediction can be slow.\n \n* It also performs badly with datasets where most features are 0 (sparse datasets).","1bdca66b":"This notebooks explains k-Nearest Neighbors algorithm for classification.","7324c2a1":"Instead of considering only the closest neighbor, we can also consider an arbitrary number, k, of neighbors by setting n_neighbors parameter.","7b34710b":"The value of feature_names is a list of strings, giving the description of each feature","8893cdb5":"Closest neighboring data point belongs to class versicolor.The prediction of the one-nearest-neighbor algorithm is the label of that point.","7bfd235b":"Let us plot a scatter plot, which allows us to depicts the joint distribution of two features against the target species category. In seaborn we do this using hue semantics. ","a47c81c9":"The value of the key target_names is an array of strings, containing the species of flower that we want to predict"}}