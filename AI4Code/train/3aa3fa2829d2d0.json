{"cell_type":{"f984d22c":"code","10c6b0b7":"code","dfbfed44":"code","939b7df8":"code","0b720fe5":"code","e64b321f":"code","754f1275":"code","41b7ac80":"code","c47719b3":"code","aa441212":"code","3e16b44e":"code","cd26ee39":"code","1209767a":"code","b5811623":"code","30dd1c85":"code","78be6cc3":"code","57c80c42":"code","89aab0b7":"code","51bb8e52":"code","efc8b811":"code","ca5531ff":"code","a7946f54":"code","f62d29c6":"code","1757f5de":"code","aea5f89d":"code","8d8da3a1":"code","29a3f1b1":"code","58219ded":"code","8f668cde":"code","acdc7c22":"code","24dfe2a2":"code","85afdd27":"code","0f23a3c1":"code","3c423b98":"code","46354a36":"code","d046b63a":"code","78a5624d":"code","0ed8d4a6":"code","967b2d73":"code","8a355e22":"code","cd24d0d6":"code","31e0edfe":"code","961f9ae6":"code","de65d46b":"code","efae96ba":"code","053eebac":"code","e62efcc8":"code","2c8177bb":"code","b3e8be60":"code","6b881c4d":"code","ccc89e2e":"code","8789d7f9":"code","7e82f3b3":"code","6c2cd848":"code","b40cf8a4":"code","fd271ba9":"code","41ab1e5a":"code","a8ca35c5":"code","8134b7bf":"code","31a47030":"code","fbc578e9":"code","bd1101c0":"code","55b215e3":"code","ca88aaea":"markdown","f2d8f2cd":"markdown","b23225b6":"markdown","73c27fcf":"markdown","bafa24ab":"markdown","07e8b023":"markdown","5cc0a2e6":"markdown","e8852311":"markdown","ad1d6e3d":"markdown","67af4543":"markdown","fc45ca0d":"markdown","298f53f3":"markdown","b645e16a":"markdown"},"source":{"f984d22c":"import pandas as pd\nimport numpy as np\nimport matplotlib.dates as md\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import host_subplot\nimport mpl_toolkits.axisartist as AA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.covariance import EllipticEnvelope\nfrom mpl_toolkits.mplot3d import Axes3D\n%matplotlib inline","10c6b0b7":"!pip install featurewiz","dfbfed44":"import featurewiz as FW","939b7df8":"from featurewiz import FE_kmeans_resampler, FE_find_and_cap_outliers, EDA_find_outliers\nfrom featurewiz import FE_convert_all_object_columns_to_numeric, split_data_n_ways, FE_create_categorical_feature_crosses\nfrom featurewiz import FE_create_time_series_features, FE_concatenate_multiple_columns\nfrom featurewiz import simple_XGBoost_model\n","0b720fe5":"pd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.max_columns', 500)","e64b321f":"from load_kaggle import load_kaggle","754f1275":"subm, train, test = load_kaggle()\nprint(train.shape, test.shape)\ntrain.head(3)","41b7ac80":"cats, ids, cont, _ = FW.EDA_classify_and_return_cols_by_type(train)\nids","c47719b3":"FW.FE_","aa441212":"### Drop the following categorical vars according to the Blog Post here ###\n###     https:\/\/www.kaggle.com\/c\/tabular-playground-series-mar-2021\/discussion\/224530  #####  \ndrop_cols = ['cat5', 'cat7', 'cat8', 'cat10']\nprint(len(drop_cols))\ndrop_cols","3e16b44e":"target = 'target'\n#df[target] = (df[target] - np.mean(df[target]))\/np.std(df[target])\n#train[target] = np.log(train[target].values)\nidcols = ['id']\nfeatures = [x for x in list(test) if x not in idcols+drop_cols]","cd26ee39":"train = train[features+[target]]\ndf = train.copy(deep=True)\nprint(train.shape)\ntrain.head(1)","1209767a":"df[target].hist()","b5811623":"test = test[features]\nprint(test.shape)\ntest.head(1)","30dd1c85":"### After dropping features, let's classify columns again\ncats, ids, cont, _ = FW.EDA_classify_and_return_cols_by_type(train)\ncats","78be6cc3":"ls = ['interactions','groupby','target']","57c80c42":"output = FW.featurewiz(train, target, corr_limit=0.70,\n                    verbose=2, sep=',', header=0, test_data=test,\n                    feature_engg='interactions', category_encoders='')","89aab0b7":"trainm = output[0]\ntestm = output[1]\ntrainm.shape, testm.shape","51bb8e52":"trainm.head()","efc8b811":"traincat = FW.FE_create_categorical_feature_crosses(train, cats)\ntestcat = FW.FE_create_categorical_feature_crosses(test, cats)","ca5531ff":"traincat.drop(cats, axis=1, inplace=True)\ntestcat.drop(cats, axis=1, inplace=True)","a7946f54":"traincat.head(1)","f62d29c6":"'traincat' in traincat.columns.tolist()","1757f5de":"#traincat.drop(['cont3',], axis=1, inplace=True)\ntestm.drop(['cont3',], axis=1, inplace=True)","aea5f89d":"trainm1 = trainm.join(traincat)\ntestm1 = testm.join(testcat)\nprint(trainm1.shape, testm1.shape)","8d8da3a1":"feats = testm1.columns.tolist()","29a3f1b1":"X_XGB=trainm1[feats]\nY_XGB=trainm1[target]\nX_XGB_test=testm1[feats]","58219ded":"model, y_preds,y_probas = FW.simple_XGBoost_model(X_XGB=X_XGB, Y_XGB=Y_XGB,\n                                                  X_XGB_test=X_XGB_test, \n                               modeltype='Classification', log_y=False,\n                               GPU_flag=True, scaler=StandardScaler(), enc_method='label', verbose=1)","8f668cde":"y_preds[:,1]","acdc7c22":"### Base model above with no feature engg gets you ~0.88 score which is a very nice score.\nsubm[target] = y_preds[:,1]\nsubm.to_csv('submission.csv',index=False)\nsubm.head()","24dfe2a2":"#!pip install autoviz","85afdd27":"#from autoviz.AutoViz_Class import AutoViz_Class\n#AV = AutoViz_Class()\n#filename = \"\"\n#sep = \",\"\n#dft = AV.AutoViz(\n#    filename,\n#    sep=\",\",\n#    depVar=target,\n#    dfte=train,\n#    header=0,\n#    verbose=0,\n#    lowess=False,\n#    chart_format=\"svg\",\n#    max_rows_analyzed=30000,\n#    max_cols_analyzed=30,\n#)","0f23a3c1":"### Step 1: we create numeric interaction variables first ###\nintxn_vars = [('cont3', 'cont7'),('cont3', 'cont8'),('cont3', 'cont9'),('cont3', 'cont10'),('cont4', 'cont5'),\n             ('cont4', 'cont6'),('cont4', 'cont9'),('cont4', 'cont10')]\ndef FE_create_interaction_vars(df, intxn_vars):\n    \"\"\"\n    This handy function creates interaction variables among pairs of numeric vars you send in.\n    Your input must be a dataframe and a list of tuples. Each tuple must contain a pair of variables.\n    All variables must be numeric. Double check your input before sending them in.\n    \"\"\"\n    df = df.copy(deep=True)\n    for (each_intxn1,each_intxn2)  in intxn_vars:\n        new_col = each_intxn1 + '_x_' + each_intxn2\n        try:\n            df[new_col] = df[each_intxn1] * df[each_intxn2]\n        except:\n            continue\n    return df","3c423b98":"train = FE_create_interaction_vars(train, intxn_vars)\ntest = FE_create_interaction_vars(test, intxn_vars)","46354a36":"### we must bin the above newly created discrete variables into 4 or 6 buckets. We will choose 6 for now\nintx_cols = train.columns.tolist()[-8:]\nintx_dict = dict(zip(intx_cols, [6]*8))\ntrain, test = FW.FE_discretize_numeric_variables(train,intx_dict,test=test, strategy='gaussian')\nprint(train.shape, test.shape)\ntrain.head(1)","d046b63a":"preds = [x for x in list(test) if x not in idcols]\nlen(preds)","78a5624d":"y_preds,y_probas = simple_XGBoost_model(X_XGB=train[preds], Y_XGB=train[target], X_XGB_test=test[preds], \n                               modeltype='Classification', log_y=False,\n                               GPU_flag=True, scaler=StandardScaler(), enc_method='label', verbose=0)","0ed8d4a6":"### The CV scores are less with new features. ####### So this is not worth adding these features\n### <Let us discard the new interaction variables and go back to the old train, test data > ","967b2d73":"subm, train, test = load_kaggle()\nprint(train.shape, test.shape)\ntrain.head(3)","8a355e22":"### step 2: we bin the following numeric variables using gaussian mixture models\nbin_these = {'cont0': 4, 'cont1': 5, 'cont3': 2, 'cont4': 2, 'cont6': 3, 'cont8': 3, 'cont10': 10}\ntrain, test = FW.FE_discretize_numeric_variables(train,bin_these,test=test, strategy='gaussian')\nprint(train.shape, test.shape)","cd24d0d6":"preds = [x for x in list(test) if x not in idcols]\nlen(preds)","31e0edfe":"y_preds,y_probas = simple_XGBoost_model(X_XGB=train[preds], Y_XGB=train[target], X_XGB_test=test[preds], \n                               modeltype='Classification', log_y=False,\n                               GPU_flag=True, scaler=StandardScaler(), enc_method='label', verbose=0)","961f9ae6":"### The CV scores are not bad - let's keep these binned variables and add to them in next steps ##","de65d46b":"### step 3: next we create feature crosses of these categorical variables ###\ntrain = FW.FE_create_categorical_feature_crosses(train, ['cat4','cat18','cat13','cat2'])\ntest = FW.FE_create_categorical_feature_crosses(test, ['cat4','cat18','cat13','cat2'])\nprint(train.shape, test.shape)","efae96ba":"preds = [x for x in list(test) if x not in idcols]\nlen(preds)","053eebac":"y_preds,y_probas = simple_XGBoost_model(X_XGB=train[preds], Y_XGB=train[target], X_XGB_test=test[preds], \n                               modeltype='Classification', log_y=False,\n                               GPU_flag=True, scaler=StandardScaler(), enc_method='label', verbose=0)","e62efcc8":"### Absolutely no improvement - but we will keep these vars as long as performance is same! ####","2c8177bb":"### step 4: create groupby aggregates of the following numerics \nagg_nums = ['cont1','cont3']\ngroupby_vars = ['cat2','cat4']\ntrain_add, test_add = FW.FE_add_groupby_features_aggregated_to_dataframe(train[agg_nums+groupby_vars], agg_types=['mean','std'],\n                                groupby_columns=groupby_vars,\n                                ignore_variables=[] , test=test[agg_nums+groupby_vars])","b3e8be60":"train_copy = train.join(train_add.drop(groupby_vars+agg_nums, axis=1))\ntest_copy = test.join(test_add.drop(groupby_vars+agg_nums, axis=1))\nprint(train_copy.shape, test_copy.shape)\ntrain_copy.head(2)","6b881c4d":"preds = [x for x in list(test_copy) if x not in idcols]\nlen(preds)","ccc89e2e":"y_preds,y_probas = simple_XGBoost_model(X_XGB=train_copy[preds], Y_XGB=train[target], X_XGB_test=test_copy[preds], \n                               modeltype='Classification', log_y=False,\n                               GPU_flag=True, scaler=StandardScaler(), enc_method='label', verbose=0)","8789d7f9":"###### step 5: log transform these columns ##########\nlog_cols = {'cont5':'log', 'cont8':'log', 'cont7':'log'}\ntrain_copy = FW.FE_transform_numeric_columns(train_copy, log_cols)\ntest_copy = FW.FE_transform_numeric_columns(test_copy, log_cols)\ntrain_copy.head(2)","7e82f3b3":"train_best, test_best = FW.featurewiz(train_copy, target, test_data=test_copy,verbose=2)","6c2cd848":"def left_subtract(l1,l2):\n    lst = []\n    for i in l1:\n        if i not in l2:\n            lst.append(i)\n    return lst\n","b40cf8a4":"cats = train_copy.select_dtypes(include=\"object\").columns.tolist()\nlen(cats)","fd271ba9":"sel_nums = ['cont6', 'cont9', 'cont0_discrete', 'cont1_discrete', 'cont6_discrete', 'cont10_discrete', 'cont1_by_cat4_mean', 'cont1_by_cat4_std', 'cont3_by_cat4_mean', 'cont3_by_cat4_std', 'cont3_by_cat2_mean', 'cont1_by_cat2_std', 'cont5', 'cont8_discrete', 'cont1', 'cont4', 'cont3_discrete', 'cont10']\npreds = sel_nums+cats\nprint(len(preds))","41ab1e5a":"### using reduced list of variables, the score actually drops 2% points! wow #######\ny_preds,y_probas = simple_XGBoost_model(X_XGB=train_copy[preds], Y_XGB=train[target], X_XGB_test=test_copy[preds], \n                               modeltype='Classification', log_y=False,\n                               GPU_flag=True, scaler=StandardScaler(), enc_method='label', verbose=0)","a8ca35c5":"####\nm, feats, trainm, testm = Auto_ViML(train_copy[preds+[target]], target, test_copy[preds],\n                            sample_submission='',\n                            scoring_parameter='', KMeans_Featurizer=False,\n                            hyper_param='RS',feature_reduction=True,\n                             Boosting_Flag=True, Binning_Flag=False,\n                            Add_Poly=0, Stacking_Flag=True,Imbalanced_Flag=False,\n                            verbose=1)","8134b7bf":"y_preds1 = testm['target_proba_1'].values\ny_preds1","31a47030":"subm = test[idcols]\n#subm = pd.DataFrame()\nsubm[target] = y_preds1\nsubm.head()","fbc578e9":"disto","bd1101c0":"subm.to_csv(target+'_Binary_Classification_submission2.csv',index=False)","55b215e3":"# Autoviml got about 0.8746 in the Kaggle rankings. #######\n###  This is slightly lower than 0.8845 that Autoviml got a month ago but it is about same as featurewiz\n### The good news is that AutoviML and Featurewiz now produce results on a 300K dataset lightning fast\n### It takes less than 2 mins for Autoviml and Featurewiz to crunch this dataset! That's a huge leap.","ca88aaea":"## Use AutoViz to gain some insights - here's what I understood\nimportant vars - leave them as is\ncont1\ncont3\ncont5\ncont6\ncont8\n\nbin the following:\ncont0 4\ncont1 5\ncont3 2\ncont4 2\ncont6 3\ncont8 3\ncont10 10\n\ngroupby vars\ncont3 by cat2\ncont1 by cat4\ncont3 by cat4\n\n\ninteraction cat vars - feature crosses\ncat4 x cat18\ncat13 x cat4\ncat13 x cat2\n\ninteraction vars and then bin them\ncont3 x cont7\ncont3 x cont8\ncont3 x cont9\ncont3 x cont10\ncont4 x cont5\ncont4 x cont6\ncont4 x cont9\ncont4 x cont10\n\nBoolean cats - leave them as is\ncat0\ncat1\ncat12\ncat13\ncat14\ncat15\ncat16\n\nlog transform these\ncont5\ncont8\ncont7\n","f2d8f2cd":"# Select the best features created using Featurewiz","b23225b6":"train = FE_find_and_cap_outliers(train,[target], verbose=1)\n#test = FE_find_and_cap_outliers(test,nums,verbose=0)","73c27fcf":"# Just use this one line of code to get ~0.88 score in less than 2 mins!","bafa24ab":"# Install Featurewiz Library to Get the Max Benefits","07e8b023":"# This notebook got 0.8862 in one line of code (see below)\n##  Turn on the GPU Accelerator in this Notebook to get the fastest Results below using XGBoost","5cc0a2e6":"#### Lastly convert all object columns to numeric ############\ntrain_copy, test_copy = FE_convert_all_object_columns_to_numeric(train_copy,test_copy)\nprint(train_copy.shape, test_copy.shape)\ntrain_copy.head()","e8852311":"train = FE_create_time_series_features(train, 'Date')\ntest = FE_create_time_series_features(test, 'Date')\ntrain.head(1)","ad1d6e3d":"combs = ['Item_Category','Subcategory_1','Subcategory_2']\ntrain = FE_concatenate_multiple_columns(train, combs)\ntest = FE_concatenate_multiple_columns(test, combs)","67af4543":"train,_ = FW.FE_split_one_field_into_many(train, field='Product', splitter='-', filler='missing')\ntest,_ = FW.FE_split_one_field_into_many(test, field='Product', splitter='-', filler='missing')\ntrain.head(1)","fc45ca0d":"## Goal: Use Featurwiz to build a better ranking model in TPS\n1.  Big_Mart Sales Prediction Score: 1147  -- Rank 250 out of 41,361 = That's a Top <1% Rank!!\n1.  Loan Status Predictions Score 0.791  -- Rank 850 out of 67,424 - Top 1.25% Rank\n1.  Machine Hack Flight Ticket Score 0.9389 -- Rank 165 out of 2723 - Top 6% Rank!\n1.  Machine Hack Data Scientist Salary class Score 0.417 -- Rank 58 out of 1547 - Top 3.7% Rank! (Autoviml Score was 0.329 -- less than 0.417 of Featurewiz+Simple even though an NLP problem!)\n1.  MCHACK Book Price NLP Score 0.7336 -- Rank 104 Autoviml NLP problem and should have done better","298f53f3":"#output = split_data_n_ways(df,target, n_splits=2)","b645e16a":"## Feature Engineering using Featurewiz begins here using insights from AutoViz above"}}