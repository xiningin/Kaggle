{"cell_type":{"adf7d8a0":"code","e929c37b":"code","5f1aff94":"code","284e6f30":"code","32740d16":"code","d5404f54":"code","c97dfbe3":"code","412a6d77":"code","90aa1a4d":"code","c6399501":"code","2c8f3716":"code","3b5f18fd":"code","52998346":"code","51a0c25b":"code","3b8f7719":"code","b065185b":"code","a2798d9a":"code","9504ac7c":"code","8589389a":"code","3654b00f":"markdown","3ab92670":"markdown","6f51460e":"markdown","a1a0998f":"markdown","9ddf1e30":"markdown","51aa3f9c":"markdown"},"source":{"adf7d8a0":"import numpy as np \nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud ,STOPWORDS\nfrom PIL import Image","e929c37b":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5f1aff94":"train_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsub_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","284e6f30":"train_df.info()\ntrain_df.head()","32740d16":"train_df[train_df['target']==1]","d5404f54":"train_df['target'].plot(kind='hist')","c97dfbe3":"test_df.tail()","412a6d77":"sub_df.head()","90aa1a4d":"def missing_count(df):\n    df = pd.DataFrame(df.isnull().sum()).reset_index()\n    df.columns = ['column', 'missing_count']\n    return df\nmissing_count(train_df)","c6399501":"missing_count(test_df)","2c8f3716":"train_df['text_length'] = train_df['text'].apply(len)\ntrain_df['count_exclamation_marks'] = train_df['text'].apply(lambda x: x.count('!'))\ntrain_df['count_symbols'] = train_df['text'].apply(lambda x: sum(x.count(w) for w in '*&$%'))\ntrain_df['count_words'] = train_df['text'].apply(lambda x: len(x.split()))\ntrain_df['count_unique_words'] = train_df['text'].apply(lambda x: len(set(w for w in x.split())))\ntrain_df['per_unique'] = train_df['count_unique_words'] \/ train_df['count_words']","3b5f18fd":"train_df.head()","52998346":"train_df.describe()","51a0c25b":"train_df[train_df['target']==1].describe()","3b8f7719":"features = ('text_length', 'count_exclamation_marks', 'count_symbols', 'count_words','count_unique_words', 'per_unique')\ncolumns = ('target')\nrows = [{'target':train_df[f].corr(train_df['target'])} for f in features]\ntrain_df_correlations = pd.DataFrame(rows, index=features)\ntrain_df_correlations","b065185b":"plt.figure(figsize=(12, 6))\nsns.set(font_scale=1)\nax = sns.heatmap(train_df_correlations, vmin=-0.1, vmax=0.1, center=0.0)","a2798d9a":"from nltk.corpus import stopwords\ntrain_text = train_df[train_df['target']==1].loc[:, ['text']].dropna()\ndef get_stopwords(text = train_text['text'], n = 20):\n    stop = stopwords.words('english')\n    data = text.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n    data = data.str.replace('[^\\w\\s]','')\n    data = data.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n    fre_word = pd.Series(' '.join(data).split()).value_counts()[:n]\n    return fre_word\n\ntrain_stopwords = get_stopwords(text = train_df['text'], n = 20)\ntrain_stopwords","9504ac7c":"def word_cloud(text, title = \"Words Cloud\"):\n    stopword=set(STOPWORDS)\n    wc= WordCloud(background_color=\"black\",max_words=40,stopwords=stopword).generate(\" \".join(text))\n    plt.figure(figsize=(15,15))\n    plt.xticks([])\n    plt.yticks([])\n    plt.axis('off')\n    plt.title(title, fontsize=25)\n    plt.imshow(wc.recolor(colormap= 'gist_earth' , random_state=7), alpha=0.98)","8589389a":"target1_text = train_df[train_df['target']==1]\ntarget1_text = target1_text['text']\nword_cloud(target1_text)","3654b00f":"**WordCloud**","3ab92670":"**File Imformation**","6f51460e":"**Stopwords**","a1a0998f":"**Is there any missing values?**","9ddf1e30":"**Load Data**","51aa3f9c":"**Correlations**"}}