{"cell_type":{"37103690":"code","b4bf1777":"code","14c2c0d5":"code","39ff0623":"code","27b19f6e":"code","6493488c":"code","cde2def9":"markdown","6b9eb207":"markdown","54f6b978":"markdown"},"source":{"37103690":"import numpy as np\nimport cv2\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, BatchNormalization, Conv2D, MaxPool2D\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nimport matplotlib.pyplot as plt","b4bf1777":"train_dir = '\/kaggle\/input\/fer2013\/train'\nval_dir = '\/kaggle\/input\/fer2013\/test'\n\ntrain_datagen = ImageDataGenerator(rescale=1.\/255, rotation_range=30, shear_range=0.15, zoom_range=0.1,\n    brightness_range=[0.8,1.0], horizontal_flip=True)\nval_datagen = ImageDataGenerator(rescale=1.\/255)\n\n\n#adi nchouf f data augmentation w batch 64 w 32\ntrain_generator = train_datagen.flow_from_directory(\n        train_dir,\n        target_size=(48,48),\n        batch_size=32,\n        color_mode=\"grayscale\",\n        class_mode='categorical')\n\nvalidation_generator = val_datagen.flow_from_directory(\n        val_dir,\n        target_size=(48,48),\n        batch_size=32,\n        color_mode=\"grayscale\",\n        class_mode='categorical')","14c2c0d5":"model = Sequential()\nmodel.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', input_shape=(48,48,1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(rate=0.1))\n\nmodel.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(rate=0.2))\n\nmodel.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(rate=0.2))\n\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu',))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(rate=0.5))\nmodel.add(Dense(7, activation='softmax'))\n\n#Compilation of the model\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n#reduce the LR by half if the val_loss is not improved after 2 epochs.\nlr_reduction = ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=1, factor=0.5, min_lr=0.00001)\n\ncheckpoint = ModelCheckpoint(\"Facial_Expression_Recognition.h5\", monitor='val_accuracy',\n                             save_weights_only=True, mode='max', save_best_only=True, verbose=1)\n#model.summary()","39ff0623":"history = model.fit(\n        train_generator,\n        steps_per_epoch= len(train_generator),\n        epochs=32,\n        validation_data=validation_generator,\n        validation_steps=len(validation_generator),\n        callbacks= [lr_reduction,checkpoint])","27b19f6e":"plt.figure(0)\nplt.plot(history.history['accuracy'], label='training accuracy')\nplt.plot(history.history['val_accuracy'], label='val accuracy')\nplt.title('Accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()\nplt.figure(1)\nplt.plot(history.history['loss'], label='training loss')\nplt.plot(history.history['val_loss'], label='val loss')\nplt.title('Loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","6493488c":"emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n\nface_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n\n# To capture video from webcam. \ncap = cv2.VideoCapture(0)\n# To use a video file as input \n# cap = cv2.VideoCapture('filename.mp4')\n\nwhile True:\n    # Read the frame\n    _, img = cap.read()\n    # Convert to grayscale\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    # Detect the faces\n    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n    # Draw the rectangle around each face\n    for (x, y, w, h) in faces:\n        cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n        roi_gray = gray[y:y + h, x:x + w]\n        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n        emotion_prediction = emotion_model.predict(cropped_img)\n        maxindex = int(np.argmax(emotion_prediction))\n        cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n    # Display\n    cv2.imshow('Video', cv2.resize(img,(1200,860),interpolation = cv2.INTER_CUBIC))\n    # Stop if escape key is pressed\n    k = cv2.waitKey(30) & 0xff\n    if k==27:\n        break\n# Release the VideoCapture object\ncap.release()\ncv2.destroyAllWindows()","cde2def9":"you can achieve almost the same accuracy without the data augmentation part","6b9eb207":"# Basically, we are going to build a neural network to classify facials expression from the images into following categories : angry, disgust, feat, happy, sad, surprise, natural.","54f6b978":"You can use the code below to try the model with your webcam and predict emotions all you have to do is to download the Facial_Expression_Recognition.h5 file and load the wieghts in emotion_model variable, also you might need https:\/\/github.com\/opencv\/opencv\/blob\/master\/data\/haarcascades\/haarcascade_frontalface_default.xml if you don't have it already"}}