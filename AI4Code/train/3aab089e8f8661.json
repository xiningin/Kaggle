{"cell_type":{"01a63f57":"code","f0f86415":"code","d6cb200e":"code","3fbf4974":"markdown"},"source":{"01a63f57":"import torch\n\nclass Rotary(torch.nn.Module):\n    \n    def __init__(self, dim, base=10000):\n        super().__init__()\n        inv_freq = 1. \/ (base ** (torch.arange(0, dim, 2).float() \/ dim))\n        self.register_buffer('inv_freq', inv_freq)\n        self.seq_len_cached = None\n        self.cos_cached = None\n        self.sin_cached = None\n\n    def forward(self, x, seq_dim=1):\n        seq_len = x.shape[seq_dim]\n        if seq_len != self.seq_len_cached:\n            self.seq_len_cached = seq_len\n            t = torch.arange(x.shape[seq_dim], device=x.device).type_as(self.inv_freq)\n            freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n            self.cos_cached = emb.cos()[:, None, None, :]\n            self.sin_cached = emb.sin()[:, None, None, :]\n        return self.cos_cached, self.sin_cached\n        \n# rotary pos emb helpers:\ndef rotate_half(x):\n    x1, x2 = x[..., :x.shape[-1] \/\/ 2], x[..., x.shape[-1] \/\/ 2:]\n    return torch.cat((-x2, x1), dim=x1.ndim - 1) # dim=-1 triggers a bug in torch < 1.8.0\n\n@torch.jit.script\ndef apply_rotary_pos_emb(q, k, cos, sin):\n    return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)","f0f86415":"r = Rotary(512)","d6cb200e":"r(torch.rand(32, 32, 32))","3fbf4974":"# Rotary Postional Encoding\n```\nDeveloped in Beijing, this new technique quickly gained interest in the NLP circles. In short, it allows you to endow the transformer with relative positional embeddings at the cost of no learned parameters. You apply a rotary operation to the queries and keys prior to their dot product in attention. It is highly effective and it is recommended you have this on whenever there is implicit order in your input. - lucidrains\n\nRotary Positional Embedding (RoPE) is a new type of position encoding that unifies absolute and relative approaches. Developed by Jianlin Su in a series of blog posts earlier this year [12, 13] and in a new preprint [14], it has already garnered widespread interest in some Chinese NLP circles. This post walks through the method as we understand it, with the goal of bringing it to the attention of the wider academic community. In general we have found that across a large suite of setups including regular, linear, and local self-attention, it either matches or surpasses all other methods currently available for injecting positional information into transformers. - Eleuther AI\n```\nOriginal Blog (in Chinese): https:\/\/kexue.fm\/archives\/8265\n\nEleuther AI blog: https:\/\/blog.eleuther.ai\/rotary-embeddings\/\n\nPaper (RoFormer): https:\/\/arxiv.org\/pdf\/2104.09864.pdf\n\nIn X-Transformers: https:\/\/github.com\/lucidrains\/x-transformers"}}