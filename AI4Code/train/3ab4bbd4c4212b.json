{"cell_type":{"34a83327":"code","48a4f1d0":"code","915bab28":"code","2665b283":"code","08b32df6":"code","6fe450ba":"code","0c57ec9a":"code","edf9a047":"code","abf36f91":"code","0ee31aa9":"markdown"},"source":{"34a83327":"import subprocess\nimport re\nimport sys\nimport os\nimport glob\nimport warnings\nimport ctypes\n\n_MKL_ = 'mkl'\n_OPENBLAS_ = 'openblas'\n\n\nclass BLAS:\n    def __init__(self, cdll, kind):\n        if kind not in (_MKL_, _OPENBLAS_):\n            raise ValueError(f'kind must be {MKL} or {OPENBLAS}, got {kind} instead.')\n        \n        self.kind = kind\n        self.cdll = cdll\n        \n        if kind == _MKL_:\n            self.get_n_threads = cdll.MKL_Get_Max_Threads\n            self.set_n_threads = cdll.MKL_Set_Num_Threads\n        else:\n            self.get_n_threads = cdll.openblas_get_num_threads\n            self.set_n_threads = cdll.openblas_set_num_threads\n            \n\ndef get_blas(numpy_module):\n    LDD = 'ldd'\n    LDD_PATTERN = r'^\\t(?P<lib>.*{}.*) => (?P<path>.*) \\(0x.*$'\n\n    NUMPY_PATH = os.path.join(numpy_module.__path__[0], 'core')\n    MULTIARRAY_PATH = glob.glob(os.path.join(NUMPY_PATH, '_multiarray_umath.*so'))[0]\n    ldd_result = subprocess.run(\n        args=[LDD, MULTIARRAY_PATH], \n        check=True,\n        stdout=subprocess.PIPE, \n        universal_newlines=True\n    )\n\n    output = ldd_result.stdout\n\n    if _MKL_ in output:\n        kind = _MKL_\n    elif _OPENBLAS_ in output:\n        kind = _OPENBLAS_\n    else:\n        return\n\n    pattern = LDD_PATTERN.format(kind)\n    match = re.search(pattern, output, flags=re.MULTILINE)\n\n    if match:\n        lib = ctypes.CDLL(match.groupdict()['path'])\n        return BLAS(lib, kind)\n    \n\nclass single_threaded:\n    def __init__(self, numpy_module=None):\n        if numpy_module is not None:\n            self.blas = get_blas(numpy_module)\n        else:\n            import numpy\n            self.blas = get_blas(numpy)\n\n    def __enter__(self):\n        if self.blas is not None:\n            self.old_n_threads = self.blas.get_n_threads()\n            self.blas.set_n_threads(1)\n        else:\n            warnings.warn(\n                'No MKL\/OpenBLAS found, assuming NumPy is single-threaded.'\n            )\n\n    def __exit__(self, *args):\n        if self.blas is not None:\n            self.blas.set_n_threads(self.old_n_threads)\n            if self.blas.get_n_threads() != self.old_n_threads:\n                message = (\n                    f'Failed to reset {self.blas.kind} '\n                    f'to {self.old_n_threads} threads (previous value).'\n                )\n                raise RuntimeError(message)\n    \n    def __call__(self, func):\n        def _func(*args, **kwargs):\n            self.__enter__()\n            func_result = func(*args, **kwargs)\n            self.__exit__()\n            return func_result\n        return _func\n","48a4f1d0":"import numpy as np, pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\n\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.mixture.gaussian_mixture import _estimate_log_gaussian_prob\nfrom scipy.special import comb, logsumexp, expit\nfrom tqdm import tqdm_notebook\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","915bab28":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\ncols = [c for c in train.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]","2665b283":"class MyGM(GaussianMixture):\n    \n    def __init__(self, n_components=1, covariance_type='full', tol=1e-3,\n                 reg_covar=1e-6, max_iter=100, n_init=1, init_params='kmeans',\n                 weights_init=None, means_init=None, precisions_init=None,\n                 random_state=0, warm_start=False,\n                 verbose=0, verbose_interval=10, init_clusters=None, y=None, contamination=0.025, init_proba=None):\n        super().__init__(\n            n_components=n_components, tol=tol, reg_covar=reg_covar,\n            max_iter=max_iter, n_init=n_init, init_params=init_params,\n            random_state=random_state, warm_start=warm_start,\n            verbose=verbose, verbose_interval=verbose_interval)\n        self.init_clusters_ = np.asarray(init_clusters).astype('int')\n        self.y_ = y\n        self.contamination_ = contamination\n        self.init_proba_ = init_proba\n        \n    def _initialize_parameters(self, X, random_state):\n        \"\"\"Initialize the model parameters.\n        Parameters\n        ----------\n        X : array-like, shape  (n_samples, n_features)\n        random_state : RandomState\n            A random number generator instance.\n        \"\"\"\n        n_samples, _ = X.shape\n\n        if self.init_params == 'kmeans':\n            resp = np.zeros((n_samples, self.n_components))\n            label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n                                   random_state=random_state).fit(X).labels_\n            resp[np.arange(n_samples), label] = 1\n        elif self.init_params == 'random':\n            resp = random_state.rand(n_samples, self.n_components)\n            resp \/= resp.sum(axis=1)[:, np.newaxis]\n        elif self.init_params == 'clusters':\n            resp = np.zeros((n_samples, self.n_components))\n            resp[np.arange(self.init_clusters_.shape[0]), self.init_clusters_] = 1\n        elif self.init_params == 'proba':\n            resp = self.init_proba_.copy()\n            resp[np.arange(self.init_clusters_.shape[0]), self.init_clusters_] = 1\n            resp \/= resp.sum(axis=1)[:, np.newaxis]\n        else:\n            raise ValueError(\"Unimplemented initialization method '%s'\"\n                             % self.init_params)\n\n        self._initialize(X, resp)\n     \n    def estimate_log_ratio(self, X):\n            weighted_log_prob = self._estimate_weighted_log_prob(X)\n            \n            return logsumexp(weighted_log_prob[:, 1::2], axis=1) - logsumexp(weighted_log_prob[:, 0::2], axis=1)\n        \n    # override estimate step\n    # for this competition\n    def _estimate_log_prob(self, X):\n        base_pred = _estimate_log_gaussian_prob(\n            X, self.means_, self.precisions_cholesky_, self.covariance_type)\n        \n        if self.y_ is None:\n            new_pred = base_pred\n        else:\n            new_pred = base_pred.copy()\n            for i in range(self.init_clusters_.shape[0]):\n                yi = self.init_clusters_[i]\n                for j in range(base_pred.shape[1]):\n                    if (j - yi) % 2 == 0:\n                        new_pred[i, j] += np.log(1 - self.contamination_)\n                    else:\n                        new_pred[i, j] += np.log(self.contamination_)\n        return new_pred\n    ","08b32df6":"if test.shape[0] != 131073:\n    with single_threaded(np):\n        oof_preds = np.zeros(train.shape[0])\n        test_preds = np.zeros(test.shape[0])\n        num_models = 512\n        n_splits = 7\n        contamination = 0.025\n        n_cluster = 3\n        n_components = 2 * n_cluster\n        reg_covar=1.986808882\n        tol=0.012220889\n        n_random = 4\n        for RANDOM_STATE in tqdm_notebook(range(n_random)):\n            for i in tqdm_notebook(range(num_models)):\n\n                # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n                train2 = train[train['wheezy-copper-turtle-magic']==i]\n                test2 = test[test['wheezy-copper-turtle-magic']==i]\n                idx1 = train2.index\n                idx2 = test2.index\n                train2.reset_index(drop=True,inplace=True)\n\n                # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n                sel = VarianceThreshold(threshold=1.5).fit(train2[cols])\n                train3 = sel.transform(train2[cols])\n                test3 = sel.transform(test2[cols])\n                n_features = train3.shape[1]\n\n                skf = StratifiedKFold(n_splits=n_splits, random_state=42, shuffle=True)\n\n                for train_index, test_index in skf.split(train3, train2['target']):\n\n                    # initialize with train, 3 clusters each\n                    train_x_0 = train3[train_index][train2['target'].values[train_index] == 0]\n                    train_x_1 = train3[train_index][train2['target'].values[train_index] == 1]\n\n                    gm0 = GaussianMixture(n_components=n_cluster, weights_init=[1\/n_cluster] * n_cluster,\n                                          n_init=5, random_state=RANDOM_STATE)\n                    _ = gm0.fit(train_x_0)\n                    cls0 = gm0.predict(train_x_0)\n                    ms0 = gm0.means_\n                    ps0 = gm0.precisions_\n                    gm1 = GaussianMixture(n_components=n_cluster, #weights_init=[1\/n_cluster] * n_cluster,\n                                          n_init=5, reg_covar=reg_covar, tol=tol, random_state=RANDOM_STATE)\n                    _ = gm1.fit(train_x_1)\n                    cls1 = gm1.predict(train_x_1)\n                    ms1 = gm1.means_\n                    ps1 = gm1.precisions_\n\n                    train_cls = np.ones(train3[train_index].shape[0]).astype(np.int)\n                    train_cls[train2['target'].values[train_index] == 0] = cls0 * 2\n                    train_cls[train2['target'].values[train_index] == 1] = cls1 * 2 + 1\n\n                    train_ms = np.zeros((n_components, n_features))\n                    train_ms[0::2] = ms0\n                    train_ms[1::2] = ms1\n\n                    train_ps = np.zeros((n_components, n_features, n_features))\n                    train_ps[0::2] = ps0\n                    train_ps[1::2] = ps1\n\n                    # concat train and test\n                    train_test3 = np.concatenate([train3[train_index], test3], axis=0)\n\n                    gm_all = MyGM(\n                        n_components=n_components, init_params='clusters', init_clusters=train_cls, \n                        means_init=train_ms, precisions_init=train_ps,\n                        weights_init=[1\/n_components] * n_components, reg_covar=reg_covar, tol=tol, contamination=contamination, y=None,\n                    )\n                    _ = gm_all.fit(train_test3)\n                    gm_all.y_ = None\n\n                    if True:\n                        oof_pred6 = gm_all.estimate_log_ratio(train3[test_index])\n                        test_pred6 = gm_all.estimate_log_ratio(test3) \n                    else:\n                        oof_pred6 = gm_all.predict_proba(train3[test_index])\n                        oof_pred6 = oof_pred6[:, 1::2].sum(axis=1) \/ oof_pred6.sum(axis=1) \n                        test_pred6 = gm_all.predict_proba(test3)\n                        test_pred6 = test_pred6[:, 1::2].sum(axis=1) \/ test_pred6.sum(axis=1)\n                    oof_preds[idx1[test_index]] += oof_pred6\n                    test_preds[idx2] += test_pred6\n\n        oof_preds \/= n_random\n        test_preds \/= skf.n_splits * n_random\n        train_used = (train['wheezy-copper-turtle-magic'] < num_models)\n        auc = roc_auc_score(train.loc[train_used, 'target'], oof_preds[train_used])\n        print(f'AUC: {auc:.5}')","6fe450ba":"if test.shape[0] != 131073:\n    train_used = (train['wheezy-copper-turtle-magic'] < num_models)\n    auc = roc_auc_score(train.loc[train_used, 'target'], expit(oof_preds[train_used]))\n    print(f'AUC: {auc:.5}')","0c57ec9a":"sub = pd.read_csv('..\/input\/sample_submission.csv')\nif sub.shape[0] != 131073:\n    sub['target'] = expit(test_preds)\nsub.to_csv('submission.csv',index=False)","edf9a047":"plt.hist((oof_preds), bins=128, log=True)\nplt.title('Final oof predictions')\nplt.show()","abf36f91":"plt.hist(test_preds, bins=128, log=True)\nplt.title('Final oof predictions')\nplt.show()","0ee31aa9":"My GM, reusing Hoxosh likelihood, with some modification.\n"}}