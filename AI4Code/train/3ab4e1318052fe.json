{"cell_type":{"6e6b9ee2":"code","a57a622c":"code","68c1e4ae":"code","a1c70fd9":"code","0124fac4":"code","f1b88ee9":"code","93d0a451":"code","182807af":"code","783d07bf":"code","dbdaf572":"code","91857ad7":"code","3a0f315b":"code","e9519c67":"code","c2fedb9b":"code","a2099d1f":"code","b53f4d4f":"code","d76c96e2":"code","8c06e591":"code","45094ed4":"code","c8ceed06":"code","89298fb7":"code","dc6a6ff8":"code","391c715e":"code","63921ca9":"code","3cdf2f4a":"code","3104b79b":"code","160b882d":"code","169bffca":"code","15ee4143":"code","a7897611":"code","b6419ae0":"code","f99e93c8":"code","a3ab36b1":"code","d6c3946e":"code","aca860cb":"code","21221758":"code","0dd307ef":"code","71be48b8":"code","a4ac67a8":"code","95c300d0":"code","9c201e29":"code","a479e6bb":"code","cb72b1ad":"code","da1d831b":"code","1673478c":"code","dc164b3c":"code","5059ebfb":"code","e184bfcd":"code","f223ef46":"code","5d12997b":"code","9d032342":"code","556a8368":"code","fe6a6a3d":"code","e1db2bc2":"code","bbebc6e8":"code","682b6e17":"code","94abbdcc":"code","3429d299":"code","eec957bc":"code","840bfd20":"code","bf86b4c2":"code","29b2f272":"markdown","29fbf6cf":"markdown","ec81504b":"markdown","830cc6d4":"markdown","1036b3f7":"markdown","ea6f65c7":"markdown","1ce89eb5":"markdown"},"source":{"6e6b9ee2":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt \nimport tensorflow as tf\nimport seaborn as sns \nimport re\n","a57a622c":"pd.set_option('display.max_colwidth', 150)","68c1e4ae":"dataset = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndataset_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","a1c70fd9":"dataset","0124fac4":"\nplt.figure(figsize=(8,5))\nsns.countplot(x=\"target\", data=dataset , palette=\"dark\", linewidth=5)\nplt.show()","f1b88ee9":"piedata = dataset['target']\nplt.figure(figsize=(6,6))\npiedata.value_counts().plot(kind = 'pie',autopct = '%.2f%%')","93d0a451":"sns.countplot(y = dataset.keyword,order = dataset['keyword'].value_counts().sort_values(ascending=False).iloc[0:20].index)\nplt.title(\"Count of Keywords\")\n","182807af":"disastered_tweet = dataset.groupby('keyword')['target'].mean().sort_values(ascending=False).head(15)\nnon_disasterd_tweet  = dataset.groupby('keyword')['target'].mean().sort_values().head(15)\n\nplt.figure(figsize=(7,4))\nsns.barplot(disastered_tweet, disastered_tweet.index, color='red')\nplt.title('Keywords with highest % of disaster tweets')\nsns.barplot(non_disasterd_tweet, non_disasterd_tweet.index, color='blue')\nplt.title('Keywords with lowest % of disaster tweets')\n\nplt.show()","783d07bf":"plt.figure(figsize=(14,7))\nsns.countplot(y = dataset.location, order = dataset['location'].value_counts().sort_values(ascending=False).iloc[0:15].index)","dbdaf572":"raw_loc = dataset.location.value_counts()\ntop_loc_disaster = list(raw_loc[raw_loc>=10].index)\ntop_only_disaster = dataset[dataset.location.isin(top_loc_disaster)]\n\ntop_location = top_only_disaster.groupby('location')['target'].mean().sort_values(ascending=False)\nsns.barplot(x=top_location.index, y=top_location)\nplt.xticks(rotation=90)\nplt.show()","91857ad7":"dataset.keyword.fillna('None', inplace=True) \ndataset.location.fillna('None' , inplace = True )","3a0f315b":"dataset.isnull().sum()","e9519c67":"def decontraction(phrase):\n    \n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase =phrase.lower()\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    phrase = re.sub('\\[.*?\\]', ' ', phrase) \n    phrase = re.sub('https?:\/\/\\S+|www\\.\\S+', ' ', phrase)\n    phrase = re.sub('<.*?>+', ' ', phrase)\n    phrase = re.sub('\\n', ' ', phrase)\n    phrase = re.sub('\\w*\\d\\w*', ' ', phrase)\n    return phrase\n\ndataset.text = [decontraction(tweet) for tweet in dataset.text]","c2fedb9b":"\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.stem import PorterStemmer\nlemmatizer = WordNetLemmatizer()\nps = PorterStemmer()","a2099d1f":"nltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')","b53f4d4f":"corpus = []\nfor i in range(len(dataset.text)):\n    review = re.sub('[^a-zA-Z]' ,' ', dataset['text'][i])\n    review =review.lower()\n    review = review.split()\n    all_stopwords = stopwords.words('english')\n    all_stopwords.remove('not')\n    review = [ps.stem(word) for word in review  if not word in set(all_stopwords)]\n    review = ' '.join(review)\n    corpus.append(review)\nprint(corpus)","d76c96e2":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = TfidfVectorizer()\nav = CountVectorizer()\nX = cv.fit_transform(corpus).toarray()","8c06e591":"X","45094ed4":"len(X[0])","c8ceed06":"Y = dataset.iloc[:,-1].values\n","89298fb7":"Y","dc6a6ff8":"from sklearn.model_selection import train_test_split\nX_train, X_test , Y_train, Y_test = train_test_split(X,Y , test_size = 0.2, random_state = 42)","391c715e":"X_train.shape , X_test.shape , Y_train.shape , Y_test.shape","63921ca9":"from sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB()\nclassifier.fit(X_train, Y_train)","3cdf2f4a":"Y_pred =classifier.predict(X_test)","3104b79b":"from sklearn.metrics import confusion_matrix , accuracy_score\ncm = confusion_matrix(Y_test , Y_pred)\nsns.heatmap(cm)\naccuracy_score(Y_test , Y_pred)","160b882d":"Y_pred_train = classifier.predict(X_train)\ncm = confusion_matrix(Y_train , Y_pred_train)\nsns.heatmap(cm)\naccuracy_score(Y_train , Y_pred_train)","169bffca":"from tensorflow.keras.preprocessing.text import one_hot\nvoc_size = 10000\nonehot_rep = [ one_hot(words, voc_size) for words in corpus]","15ee4143":"onehot_rep","a7897611":"from tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential","b6419ae0":"sent_length = 20\nembedded_does= pad_sequences(onehot_rep , padding = 'pre', maxlen =sent_length)","f99e93c8":"embedded_does","a3ab36b1":"dim = 10\nmodel = Sequential()\nmodel.add(Embedding(voc_size  ,10 , input_length = sent_length ))\nmodel.compile('adam' , 'mse')\nmodel.summary()","d6c3946e":"print(model.predict(embedded_does))","aca860cb":"embedded_does[0]","21221758":"print(model.predict(embedded_does[0]))","0dd307ef":"X = np.array(embedded_does)","71be48b8":"X.shape , Y.shape","a4ac67a8":"X_train, X_test , Y_train, Y_test = train_test_split(X,Y , test_size = 0.2, random_state = 42)","95c300d0":"X_train.shape","9c201e29":"classifier.fit(X_train , Y_train)","a479e6bb":"Y_pred_train = classifier.predict(X_train)\nY_pred_1 = classifier.predict(X_test)","cb72b1ad":"Y_pred_1","da1d831b":"accuracy_score(Y_pred_1 , Y_test)","1673478c":"from xgboost import XGBClassifier\nclassifier_xgb = XGBClassifier()\nclassifier_xgb.fit(X_train , Y_train)","dc164b3c":"Y_pred_xgb = classifier_xgb.predict(X_test)","5059ebfb":"Y_pred_xgb","e184bfcd":"accuracy_score(Y_pred_xgb , Y_test)","f223ef46":"dataset_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","5d12997b":"dataset_test","9d032342":"dataset_test.text = [decontraction(tweet) for tweet in dataset_test.text]","556a8368":"def decontraction(phrase):\n    \n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = phrase.lower()\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    phrase = re.sub('\\[.*?\\]', ' ', phrase) \n    phrase = re.sub('https?:\/\/\\S+|www\\.\\S+', ' ', phrase)\n    phrase = re.sub('<.*?>+', ' ', phrase)\n    phrase = re.sub('\\n', ' ', phrase)\n    phrase = re.sub('\\w*\\d\\w*', ' ', phrase)\n    return phrase\n\ndataset_test.text = [decontraction(tweet) for tweet in dataset_test.text]","fe6a6a3d":"corpus_test = []\nfor i in range(len(dataset_test.text)):\n    \n    review = re.sub('[^a-zA-Z]' ,' ', dataset_test['text'][i])\n    review =review.lower()\n    review = review.split()\n    all_stopwords = stopwords.words('english')\n    all_stopwords.remove('not')\n    review = [ps.stem(word) for word in review  if not word in set(all_stopwords)]\n    review = ' '.join(review)\n    corpus_test.append(review)\nprint(corpus_test)","e1db2bc2":"onehot_rep = [ one_hot(words, voc_size) for words in corpus_test]","bbebc6e8":"embedded_does_test= pad_sequences(onehot_rep , padding = 'pre', maxlen =sent_length)","682b6e17":"embedded_does_test","94abbdcc":"X_test_dataset = np.array(embedded_does_test)","3429d299":"dim = 10\nmodel = Sequential()\nmodel.add(Embedding(voc_size  ,10 , input_length = sent_length ))\nmodel.compile('adam' , 'mse')\nmodel.summary()","eec957bc":"Y_pred_test_data = classifier_xgb.predict(X_test_dataset)","840bfd20":"Y_pred_test_data","bf86b4c2":"submission_file_test = pd.DataFrame({'Id':dataset_test['id'],'target':Y_pred_test_data})\nsubmission_file_test.to_csv('submission_file.csv',index=False)\nsubmission_file_test = pd.read_csv('submission_file.csv')\nsubmission_file_test.head(10)","29b2f272":"## NAives_Bayes ","29fbf6cf":"## Import libraries","ec81504b":"## Data_Cleaning ","830cc6d4":"## For Test dataset","1036b3f7":"## Import dataset","ea6f65c7":"## XGBoost","1ce89eb5":"## VIsualization"}}