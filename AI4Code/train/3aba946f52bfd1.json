{"cell_type":{"0138d4fc":"code","17617066":"code","68c115ad":"code","c84ef032":"code","b45e9915":"code","9d36c4b0":"code","43c80c50":"code","1c7dba04":"code","f622de29":"code","da60e219":"code","b33d25b9":"code","59bbb742":"code","6cb4c993":"code","4db74f2c":"code","e01fc10a":"code","4f84ebe3":"code","19ed5b8d":"code","07372207":"code","03ff4ba8":"code","3f72180d":"code","08d822e9":"code","ec99fe21":"code","948c01f6":"code","7a34f218":"code","8c6d6ede":"code","1f699e2f":"code","09c5b072":"code","c30b0da9":"code","c9b5e5a5":"code","991ce9bf":"code","df8c5c8f":"code","c7ee4b10":"code","1840b5ec":"code","a67a1055":"code","35fc7cbd":"code","9cf0f9ed":"code","a56815ab":"code","d43fbc8c":"code","814b2a2e":"code","bd79ba67":"code","faf291d1":"code","11bbed8f":"code","1e425fb4":"code","8c30a9ba":"code","e8fc2524":"code","71730bef":"code","919a2138":"code","f1e6d34c":"code","6a80ba13":"code","53b225d6":"code","7d474d4b":"code","b23ded04":"code","2f1b33dc":"code","c09018d2":"code","eb30ed63":"code","04bd8a32":"code","b68bf2fc":"code","b0dbbc69":"code","44931215":"code","5ee13c54":"code","88051f0d":"code","df7494a6":"code","0651c02c":"code","9d0cd7b4":"code","c6af0b74":"code","58897705":"markdown","00a9555c":"markdown","47189ebb":"markdown","76b0d8d1":"markdown","12c8bb50":"markdown","12a5cd51":"markdown","551565ae":"markdown","ad1eb11d":"markdown","291dba1c":"markdown","7fe50e5c":"markdown","86ff52b4":"markdown","9bab79d4":"markdown","b4f066fa":"markdown","248e16fd":"markdown","2de0ed10":"markdown","e1b49d2b":"markdown","bacbcc27":"markdown","fd26d320":"markdown","5adf815b":"markdown","23a7cc53":"markdown","301da2b4":"markdown","27202eb8":"markdown","d74b571a":"markdown","3b6c38d6":"markdown","92699f8c":"markdown","6deb1ac2":"markdown","9ff41347":"markdown","8d45a292":"markdown","e1c204f4":"markdown","6f5feb14":"markdown"},"source":{"0138d4fc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","17617066":"import numpy as np\nimport pandas as pd\n\nimport nltk\nimport string\nfrom nltk.corpus import stopwords\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\nfrom sklearn.model_selection import train_test_split\n\nfrom scipy.sparse import csr_matrix\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\n\n#import accuracy\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import confusion_matrix,accuracy_score, classification_report\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.linear_model import LogisticRegression","68c115ad":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","c84ef032":"tr = train.copy()","b45e9915":"tr.shape","9d36c4b0":"tr.head()","43c80c50":"tr.drop('id', axis = 1, inplace = True)","1c7dba04":"# Store Test_id , it will be require at the time of submission\n\ntest_id = test['id']","f622de29":"duplicates_record = tr[tr.duplicated(['text'], keep=False)]\nduplicates_record","da60e219":"duplicates_record.shape","b33d25b9":"tr.drop_duplicates(subset = ['text','target'], keep = 'first', inplace = True, ignore_index = True)","59bbb742":"duplicates_record = tr[tr.duplicated(['text'], keep=False)]\nduplicates_record.head(6)","6cb4c993":"tr.drop_duplicates(subset = ['text'], keep = False, inplace = True, ignore_index = True)","4db74f2c":"tr.isna().sum()","e01fc10a":"tr['keyword'].value_counts()","4f84ebe3":"tr['location'].nunique()","19ed5b8d":"tr[~tr['location'].isna()]","07372207":"tr['target'].value_counts()","03ff4ba8":"sns.countplot(tr['target']);","3f72180d":"Y = tr['target']\ntr.drop('target', axis = 1, inplace = True)","08d822e9":"tr.shape","ec99fe21":"test.shape","948c01f6":"test.head()","7a34f218":"tr = pd.concat([tr,test], axis = 0)","8c6d6ede":"tr.shape","1f699e2f":"alpha = [' ','a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']","09c5b072":"ps = nltk.PorterStemmer()\nwn = nltk.WordNetLemmatizer()","c30b0da9":"tr['keyword'].nunique()","c9b5e5a5":"tr['keyword'] = np.where(tr['keyword'].isna(),'missing', tr['keyword'])","991ce9bf":"tr['keyword'].unique()","df8c5c8f":"def cleanKeyword(text):\n    \n    text = text.lower()   # to convert to all lower case text.\n    text = text.replace('%20',' ')   # Remove '%20' if present.\n    text = ' '.join([ps.stem(word) for word in text.split(' ')]) # To bring the word to its root word.\n    \n    return text","c7ee4b10":"tr['clean_keyword'] = tr['keyword'].apply(cleanKeyword)","1840b5ec":"tr['clean_keyword'].unique()","a67a1055":"tr['clean_keyword'].nunique()","35fc7cbd":"stopWords = stopwords.words('english')","9cf0f9ed":"punct = string.punctuation\npunct","a56815ab":"#stopWords","d43fbc8c":"def cleanText(text):\n    \n    text = text.lower() # change to lower case\n    text = ''.join([char for char in text if char in alpha]) # remove anything that is not an alphabet.\n    text = ' '.join([ps.stem(word) for word in text.split(' ') if ((word not in stopWords) & (len(word)>1))])  # Bring the word to its root word.\n    \n    return text","814b2a2e":"tr['text_clean'] = tr['text'].apply(cleanText)","bd79ba67":"tr.head()","faf291d1":"tr['clean_tweet'] = tr['text_clean'] + ' ' + tr['clean_keyword']","11bbed8f":"vector = TfidfVectorizer(sublinear_tf=True, max_features=2700)\nX = vector.fit_transform(tr['clean_tweet'].values)","1e425fb4":"X_col = vector.get_feature_names()","8c30a9ba":"df = pd.DataFrame.sparse.from_spmatrix(X, columns = X_col)","e8fc2524":"df.head()","71730bef":"df.shape","919a2138":"test = df.iloc[7485:]","f1e6d34c":"test.reset_index(drop = True , inplace = True)","6a80ba13":"test.head()","53b225d6":"train_df = df.iloc[:7485]","7d474d4b":"test.shape","b23ded04":"\ncv = KFold(n_splits=10, random_state=1, shuffle=True)\n# create model\nmodel = LogisticRegression()\n# evaluate model\nscores = cross_val_score(model,train_df, Y, scoring='accuracy', cv=cv, n_jobs=-1)\n# report performance\nprint('Accuracy: ',((scores).mean()))","2f1b33dc":"model.fit(train_df, Y)","c09018d2":"count_vector = CountVectorizer(encoding='utf-8', max_features=2500)\nX_count = count_vector.fit_transform(tr['clean_tweet'].values)","eb30ed63":"X_count_col = count_vector.get_feature_names()","04bd8a32":"train_mat = pd.DataFrame.sparse.from_spmatrix(X_count, columns = X_count_col)","b68bf2fc":"train_mat.head()","b0dbbc69":"cv = KFold(n_splits=10, random_state=1, shuffle=True)\n# create model\nmodel = LogisticRegression()\n# evaluate model\nscores = cross_val_score(model,train_df, Y, scoring='accuracy', cv=cv, n_jobs=-1)\n# report performance\nprint('Accuracy: ',((scores).mean()))","44931215":"model.fit(train_df, Y)","5ee13c54":"y_pred = model.predict(test)","88051f0d":"len(y_pred)","df7494a6":"# Create a submisison dataframe and append the relevant columns\n\nsubmit=pd.DataFrame()\nsubmit['id'] = test_id\nsubmit['target'] = y_pred # our model predictions on the test dataset\nsubmit.head()","0651c02c":"len(submit) == len(test)","9d0cd7b4":"# Convert submisison dataframe to csv for submission to csv \n# for Kaggle submisison\nsubmit.to_csv('..\/Disaster tweet.csv', index=False)\nprint('Submission CSV is ready!')","c6af0b74":"submissions_check = pd.read_csv(\"..\/Disaster tweet.csv\")\nsubmissions_check.head()","58897705":"Here, i am combining the train and test dataset, before converting word to vectors.\nThe reason is if we do it separately then there is a good chance that the vector we will get in train set and test set will not be same and while predicting the test tweet the train and test features should be same.","00a9555c":"X_col will give the all the 2700 feature selected.","47189ebb":"Make a copy , Trying not to make any changes in original dataset","76b0d8d1":"Lets seprate out target variable.","12c8bb50":"Seems to be very much balanced dataset.","12a5cd51":"80.4 % accuracy not that bad.....","551565ae":"Lets not remove missing value present in Keyword, instead we can replace the NaN with Text 'missing'. in order to avoid data loss.","ad1eb11d":"Now lets do some text cleaning","291dba1c":"Much cleaner and shorter, Earlier there were total 221 unique values and now after cleaning it reduced to 167.","7fe50e5c":"Exactly same......","86ff52b4":"## **Submission**","9bab79d4":"'clean_keyword' feature alone do not contribute much in analysis, so lets combine it with 'text_clean' and create a brand new feature 'clean_tweet'.","b4f066fa":"Now lets separate out Test and train data.","248e16fd":"All looks good, lets create Model","2de0ed10":"Still we see there are various duplicate records are available but with one difference, this time for same tweet , one time target is 0 and for another it is 1. This seems to be corrupted records and ML model may get confuse, so its better to remove these records.","e1b49d2b":"Load the dataset","bacbcc27":"Now lets apply  word to vector method.","fd26d320":"Now Lets Check if there is any missing values.","5adf815b":"Lets Drop 'id' column as it is not usefull for analysis","23a7cc53":"Now lets look at target variable.","301da2b4":"Check for duplicate records","27202eb8":"Here, for max_features i am taking value as 2700, As i tried with several values from 500 to 3000, and 2700 was giving best accuracy.\n\nAlso it is important to select some value for max_features as if it is not selected then the method (TfidfVectorizer) will create feature for each word and you will end up in getting very very very large number of features (in this case it was more tha 25000) and while creating model you may get memory error.\n\nAnd by giving some values (lets say 2700) to max_features, it will select 2700 most important features.","d74b571a":"Now lets clean the Text variable.","3b6c38d6":"Around 179 duplicate records are found. Now lets consider (Text , target) and only keep the first record of duplicate record and remove remaining.","92699f8c":"Now lets Use Count Vectorizor and see if it is better than Tfidf","6deb1ac2":"Here is the 2 method (Stemmer and Lemmatizer) to bring the word to its root word.","9ff41347":"Keyword has 56 missing value , we will deal with those in sometime, and Location has many missing values also for this analysis , i am considering Location is not an important feature, So lets move on.","8d45a292":"Import some of the important libraries.","e1c204f4":"Now lets clean the keyword, as we see in some of the keyword '%20' is added not sure why and how, but lets remove those.","6f5feb14":"Now lets check again"}}