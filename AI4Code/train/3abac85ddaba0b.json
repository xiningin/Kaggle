{"cell_type":{"5ea18065":"code","d2a32db1":"code","72b265ee":"code","df4e1d69":"code","2e963c99":"code","93af1e2b":"code","8f8d2239":"code","1d418482":"code","e719b800":"code","09cba3ac":"code","b09a25c3":"code","4c071645":"code","11aa649a":"code","4ff83698":"code","795ccab1":"code","177f2060":"code","fc094f4a":"code","0216b052":"code","853fbf5d":"code","a0b6e052":"code","d32c657e":"code","793f3d44":"code","2aff8206":"markdown","b64d278a":"markdown","3653ab93":"markdown","ae325bb9":"markdown","45051cec":"markdown","e36302c9":"markdown","8150d146":"markdown","bf1b891b":"markdown","55bd087d":"markdown","50ddbece":"markdown","c685fe16":"markdown","c192dcf0":"markdown","8d88b5f8":"markdown","6f259ee3":"markdown"},"source":{"5ea18065":"import pandas as pd\ndata = pd.read_csv('\/kaggle\/input\/dataset\/Life_Expectancy_Data.csv')\ndata.head()","d2a32db1":"import matplotlib.pyplot as plt\nplt.hist(data['Life_expectancy'])\nplt.show()","72b265ee":"print('Shape of Data {}'.format(data.shape))","df4e1d69":"import seaborn as sns\ncorrmat = data.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True)\n\nfor i in range(len(corrmat.columns)):\n  for j in range(len(corrmat.index)):\n    if corrmat.iloc[i,j]>0.80 and corrmat.iloc[i,j] != 1.0:\n      print('Multi-Collinearity Feature {} and Feature {} --> Correlation Score {}'.format(corrmat.columns[i],corrmat.columns[j],corrmat.iloc[i,j]))","2e963c99":"data = data.drop(['thinness_5-9 years','GDP','infant_deaths'],axis=1)","93af1e2b":"corrmat['Life_expectancy']\n\n","8f8d2239":"fig, ax = plt.subplots()\nax.scatter(x = data['Adult_Mortality'], y = data['Life_expectancy'])\nplt.ylabel('Life_expectancy', fontsize=13)\nplt.xlabel('Adult_Mortality', fontsize=13)\nplt.show()","1d418482":"fig, ax = plt.subplots()\nax.scatter(x = data['Schooling'], y = data['Life_expectancy'])\nplt.ylabel('Life_expectancy', fontsize=13)\nplt.xlabel('Schooling', fontsize=13)\nplt.show()","e719b800":"fig, ax = plt.subplots()\nax.scatter(x = data['Income_composition_of_resources'], y = data['Life_expectancy'])\nplt.ylabel('Life_expectancy', fontsize=13)\nplt.xlabel('Income_composition_of_resources', fontsize=13)\nplt.show()","09cba3ac":"data = data.drop(data[data['Income_composition_of_resources']<0.2].index)\ndata = data.drop(data[data['Schooling']<2].index)\ndata = data.drop(data[data['Adult_Mortality']<80].index)","b09a25c3":"null_cols=[]\nfor col in data.columns:\n  if data[col].isnull().sum() !=0:\n    print('{} ---- null values : {} ---- data type : {}'.format(col, data[col].isnull().sum(), type(data[col][0])))\n    null_cols.append(col)","4c071645":"data = data.fillna(data.median())","11aa649a":"data = pd.get_dummies(data,columns=['Status','Country'])","4ff83698":"y = data['Life_expectancy']\nx = data.drop(['Life_expectancy'],axis=1)\n","795ccab1":"from sklearn.model_selection import train_test_split\nxtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size=0.2)","177f2060":"#)\n!pip install catboost\nfrom sklearn.linear_model import LinearRegression, Ridge, RidgeCV,Lasso,LassoCV\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor,AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nimport warnings\n\nlr=0.1\nn = 200\nkf = KFold(n_splits=20, random_state=42, shuffle=True)\nsvr_model = SVR(C=1)\ndecision_tree_model = DecisionTreeRegressor()\nrandom_forest_model = RandomForestRegressor(n_estimators=n)\nadaboost_model = AdaBoostRegressor(n_estimators=n,learning_rate=lr)\ngradientboost_model = GradientBoostingRegressor(learning_rate=lr,n_estimators = n)\ncatboost_model = CatBoostRegressor(learning_rate = lr,iterations = n, depth=3,loss_function='RMSE',verbose=0)\nridge_model = Ridge(alpha=0.1)\nridge_cv_model = RidgeCV(alphas=[0.1,0.01,0.001,1],cv=10)\nlasso_model = Lasso(alpha=0.1)\nlasso_cv_model = LassoCV(alphas=[0.1,0.01,0.001,1],cv=10)\n\n\n\n\n\n","fc094f4a":"#Params for each model are adjusted using GridSearchCV hyperparameter tuning\n#illustration\n\n# random_forest_regressor = AdaBoostRegressor()\n# from sklearn.model_selection import GridSearchCV\n# param_grid = {'n_estimators':[100,200,300,400,500,800,1000],'learning_rate':[1,0.1,0.001,0.001]}\n# grid_search = GridSearchCV(estimator = random_forest_regressor, param_grid = param_grid, \n#                         cv = 5, n_jobs = -1, verbose = 2)\n# grid_search.fit(xtrain,ytrain)\n# grid_search.best_params_\n# best_grid = grid_search.best_estimator_\n# ypred = best_grid.predict(xtest)\n# metrics(ytest,ypred)\n\n","0216b052":"def metrics(ytest,ypred):\n  return np.sqrt(mean_squared_error(ytest,ypred))","853fbf5d":"result = pd.DataFrame([],columns=['Model','CV_rmse','Prediction_rmse'])\ndef compute(model,i):\n  cv_rmse = np.sqrt(-cross_val_score(model, x, y, scoring=\"neg_mean_squared_error\", cv=kf))\n  model.fit(xtrain,ytrain)\n  ypred = model.predict(xtest)\n  result.loc[i] = [str(model)[:str(model).index('(')] ,cv_rmse.mean(),metrics(ytest,ypred) ]\n\n","a0b6e052":"models = [svr_model,decision_tree_model,random_forest_model,adaboost_model,gradientboost_model,ridge_model,ridge_cv_model,lasso_model,lasso_cv_model]\nfor model in range(len(models)):\n  compute(models[model],model)\nresult = result.sort_values('CV_rmse')\nwarnings.filterwarnings(\"ignore\")\nresult","d32c657e":"catboost_model.fit(xtrain, ytrain)\nypred = catboost_model.predict(xtest)\nwarnings.filterwarnings(\"ignore\")\nprint('CatBoost Regressor RMSE {}'.format(metrics(ytest,ypred)))","793f3d44":"from mlxtend.regressor import StackingCVRegressor\nstack_gen = StackingCVRegressor(regressors=(decision_tree_model,random_forest_model,catboost_model,adaboost_model,gradientboost_model,ridge_model,ridge_cv_model,lasso_model,lasso_cv_model),\n                                meta_regressor=ridge_model,\n                                use_features_in_secondary=True,cv=30)\nstack_gen.fit(np.array(xtrain),np.array(ytrain))\nypred = stack_gen.predict(np.array(xtest))\nwarnings.filterwarnings(\"ignore\")\nprint('StackingCV Regressor RMSE {}'.format(metrics(ytest,ypred)))\n","2aff8206":"# Dealing With Missing Values","b64d278a":"# Dealing With Categorical Data","3653ab93":"**StackingCVRegressor**","ae325bb9":"**CatBoost Regressor**","45051cec":"Schooling and Income_composition_of_resources are highly positive correlated\n\n::business::\n\nAdult_Mortality is highly negatively correlated","e36302c9":"# Plotting Dependent Variable (Target)","8150d146":"# Splitting Data","bf1b891b":"**4 pairs of features have high correlation **\n\n\n\n1.   infant_deaths and under-five_deaths\n2.   percentage_expenditure and  GDP\n3.   thinness_1-19 years and thinness_5-9 years\n4.   Income_composition_of_resources and Schooling\n\nBest Option to deal with multi-collinearity is to remove any one feature\n\n\n\n\n","55bd087d":"**Finding Correlation of Dependent Variable with all independent variables**","50ddbece":"# Applying Models","c685fe16":"# Removing Outliers","c192dcf0":"# Multi-Collinearity\n\nWhen independent Features are highly correlated i.e. have same nature, then they introduce the element of variance in the model. ","8d88b5f8":"# Import Data","6f259ee3":"**As all the features having missing values are in numeric nature, We'll replace with its median**"}}