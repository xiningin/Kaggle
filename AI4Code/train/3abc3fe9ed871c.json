{"cell_type":{"93bfbfe0":"code","86423ee6":"code","2f0b2a1c":"code","0442f11b":"code","c1c7036b":"code","7381de2f":"code","4d8090b8":"code","180acbce":"code","834c93db":"code","51178631":"code","cd8a883d":"code","5f2930a6":"code","5c97b1f9":"code","ba8a83ec":"code","006453f3":"code","55c71a9e":"code","ffa6b1c6":"code","2ea8acbe":"code","250ec799":"code","0cf15a17":"code","05e7eee4":"code","b571ba22":"code","ff9c4419":"code","2aa70cd6":"code","c87c1866":"code","3ccaa700":"code","62e92b92":"code","8bcabe0c":"code","8a8dc04e":"code","00bdca13":"code","4901b626":"code","f2fac049":"code","15382024":"code","c71b8bf2":"code","ad8da85e":"code","407bd0fe":"code","17c59dbf":"code","9ee546d4":"code","f712bf16":"code","dcfa93fd":"code","0a9fefaf":"code","c72d40ac":"code","5325e003":"code","cfc0587f":"code","95c43fbe":"code","7f94cdf5":"code","b76de903":"code","d574ca1f":"code","96c99dde":"code","4decc75e":"code","a4261f61":"code","eec3536f":"code","a823756c":"code","d76b3460":"code","6eb04fae":"code","2ded9535":"code","753ed906":"code","3298c3da":"code","9c1ded18":"code","2acedf6b":"code","443d2b1e":"code","374aeea8":"code","8e3b115f":"code","e670209e":"code","11d5763b":"code","6c63e77d":"code","ecb959ef":"code","cf5d2d01":"code","5098a519":"code","17e7ca8b":"code","d11338f4":"code","c18b3d46":"code","5a8fb6ab":"code","234a0256":"markdown","13820e88":"markdown","966e0ff1":"markdown","9ef02beb":"markdown","a73011b3":"markdown","bfab14b0":"markdown","425a6be4":"markdown","013f87b9":"markdown","4cd9ad78":"markdown","2bfda4de":"markdown","c8ecdebd":"markdown","cf744f95":"markdown","aee37363":"markdown","a9c09b63":"markdown","3262bdf9":"markdown","b9534486":"markdown","2441a93d":"markdown","c8d66845":"markdown","e76aca4a":"markdown","7ebb60f3":"markdown","df5ea60a":"markdown","7de3284f":"markdown","5605df90":"markdown","e23d6fcb":"markdown","e323820c":"markdown","293e7385":"markdown","3730617a":"markdown","69f9010b":"markdown","722188da":"markdown","b0acae6b":"markdown","6ebd7537":"markdown","cd259231":"markdown","45ce901d":"markdown","3e380889":"markdown","8e0fc6ef":"markdown","7be5f09e":"markdown","63e0bebf":"markdown","01e7f08f":"markdown","7896357d":"markdown","9c1a47dd":"markdown","cf7858ab":"markdown","eac44323":"markdown","7dea81ed":"markdown","abdd422f":"markdown","5e8de086":"markdown","94b4d019":"markdown","20a4c19f":"markdown","e5aea46c":"markdown","9709daf4":"markdown","6b30c79c":"markdown","62606648":"markdown","79a84000":"markdown","e0715bbd":"markdown","9db052ef":"markdown","f752aa18":"markdown"},"source":{"93bfbfe0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport warnings\nimport collections\nimport sklearn as sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer,TfidfVectorizer\nfrom sklearn.model_selection import cross_val_score, KFold, train_test_split\nfrom sklearn.feature_extraction import text \nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm, tree\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score\nimport gensim\nfrom collections import defaultdict\nfrom itertools import islice\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import GaussianNB\nfrom keras import Sequential\nfrom keras.layers import Bidirectional, GlobalMaxPool1D,Dense, Input, Embedding, Dropout, LSTM, CuDNNGRU\nfrom keras.models import Model\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\n\n\nimport time\n%matplotlib inline\nwarnings.filterwarnings('ignore')","86423ee6":"train_df = pd.DataFrame.from_csv(\"..\/input\/train.csv\")\ntest_df =  pd.DataFrame.from_csv(\"..\/input\/test.csv\")","2f0b2a1c":"train_df.head(5)","0442f11b":"text =\" \".join(train_df.question_text)\n # Create the wordcloud object\nwordcloud = WordCloud(width=1024, height=1024, margin=0).generate(text)\n \n# Display the generated image:\nfig,ax = plt.subplots(1,1,figsize=(10,10))\nax.imshow(wordcloud, interpolation='bilinear')\nax.axis(\"off\")\nax.margins(x=0, y=0)\nplt.show()","c1c7036b":"fig, ax = plt.subplots(1,1, figsize=(8,8))\nax.set_title(\"Target Status\")\nexplode=(0,0.1)\nlabels ='0','1'\nax.pie(list(dict(collections.Counter(list(train_df.target))).values()), explode=explode, labels=labels, autopct='%1.1f%%',shadow=True, startangle=90)","7381de2f":"del text","4d8090b8":"X_train, X_test, y_train, y_test = train_test_split(train_df.question_text, train_df.target, test_size=0.33, random_state=42)","180acbce":"#initiation of countVectorizer\ncount_vectorizer = CountVectorizer(min_df=5, stop_words='english')\nvect = count_vectorizer.fit(train_df.question_text)\n\nX_vect_train = vect.transform(X_train) # documents-terms matrix of training set\nX_vect_test = vect.transform(X_test) # documents-terms matrix of testing set\n\ntf_train_transformer = TfidfTransformer(use_idf=False).fit(X_vect_train)\ntf_test_transformer =  TfidfTransformer(use_idf=False).fit(X_vect_test)\n\nxtrain_tf = tf_train_transformer.transform(X_vect_train)\nxtest_tf = tf_test_transformer.transform(X_vect_test)","834c93db":"type(xtrain_tf),xtrain_tf.shape, train_df.shape","51178631":"xtrain_tf[:5].todense()","cd8a883d":"count_vectorizer.get_feature_names()[0:5]","5f2930a6":"count_vectorizer.get_feature_names()[30000:30005]","5c97b1f9":"count_vectorizer.get_feature_names()[-5:]","ba8a83ec":"[{k:count_vectorizer.vocabulary_[k]} for k in list(count_vectorizer.vocabulary_)[:10]]","006453f3":"list(sklearn.feature_extraction.text.ENGLISH_STOP_WORDS)[:10]","55c71a9e":"results_df = pd.DataFrame()","ffa6b1c6":"# MULTINOMINA_NAIVE_BAYES\nnb_ = MultinomialNB()\nnb_clf = nb_.fit(X=xtrain_tf, y=y_train)\nresults_df.set_value(\"NB\" , \"countVectorizer\" , accuracy_score(y_test,nb_clf.predict(xtest_tf)))","2ea8acbe":"# RANDOM_FORES_CLASSFIER\nrf_clf = RandomForestClassifier(n_estimators=25, max_depth=15,random_state=42)\nrf_clf.fit(X=xtrain_tf,y=y_train)\nresults_df.set_value(\"RF\" , \"countVectorizer\" , accuracy_score(y_test,rf_clf.predict(xtest_tf)))","250ec799":"#MLP_CLASSIFIER\nmlp_clf = MLPClassifier(solver='lbfgs', alpha=1e-4,hidden_layer_sizes=(20,10, 2), random_state=42)\nmlp_clf.fit(X=xtrain_tf, y=y_train)                         \nresults_df.set_value(\"MLP\" , \"countVectorizer\" , accuracy_score(y_test,mlp_clf.predict(xtest_tf)))","0cf15a17":"#LoggicRegression\nlreg_clf = LogisticRegression(solver='lbfgs', multi_class='multinomial',random_state=42)\nlreg_clf.fit(X=xtrain_tf, y=y_train)                         \nresults_df.set_value(\"LREG\" , \"countVectorizer\" , accuracy_score(y_test,lreg_clf.predict(xtest_tf)))","05e7eee4":"results_df","b571ba22":"fig,axes=plt.subplots(1,1,figsize=(8,8))\naxes.set_ylabel(\"Accuracy\")\nplt.ylim((.92,.97))\nresults_df.plot(kind=\"bar\",ax=axes)","ff9c4419":"text_ = train_df.question_text\ntargets_ = train_df.target\nclass GetSentences(object):\n    def __iter__(self):\n        counter = 0\n        for sentence_iter in text_:\n            tmp_sentence = sentence_iter\n            counter += 1\n            yield tmp_sentence.split()\nlen(text_)","2aa70cd6":"num_features = 200  # Word vector dimensionality\nmin_word_count = 5  # Minimum word count\nnum_workers = 4  # Number of threads to run in parallel\ncontext = 10  # Context window size\ndownsampling = 1e-3  # Downsample setting for frequent words\nget_sentence = GetSentences()\nmodel = gensim.models.Word2Vec(sentences=get_sentence, min_count=min_word_count, size=num_features, workers=4)\nw2v = dict(zip(model.wv.index2word, model.wv.syn0))","c87c1866":"next(iter(w2v.items()))","3ccaa700":"list(islice(model.wv.vocab.items(), 5))","62e92b92":"\"MOST SIMILAR WORD TO MAN: {} AND MOST SIMILAR WORD TO QUERA:{} \".format(model.most_similar(['man'],topn=1),model.most_similar([\"quora\"],topn=3))","8bcabe0c":"model.most_similar(positive=[\"united\",\"state\"])","8a8dc04e":"# most similar words to 'sex'\nmodel.most_similar(positive=[\"sex\"])","00bdca13":"model.most_similar(positive=['gangbang', 'sex'], negative=['man'], topn=5) ","4901b626":"model.most_similar(positive=['woman', 'king'], negative=['man'], topn=2) ","f2fac049":"model.save('word2vec.model')","15382024":"class MeanEmbeddingVectorizer(object):\n    def __init__(self, word2vec):\n        self.word2vec = word2vec\n        # if a text is empty we should return a vector of zeros\n        # with the same dimensionality as all the other vectors\n        # self.dim = len(word2vec.itervalues().next())\n        self.dim = len(next(iter(self.word2vec.items()))[1])\n\n    def fit(self, X, y):\n        return self\n\n    def transform(self, X):\n        return np.array([np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0) for words in X])\n\n\nclass TfidfEmbeddingVectorizer(object):\n    def __init__(self, word2vec):\n        self.word2vec = word2vec\n        self.word2weight = None\n        # self.dim = len(word2vec.itervalues().next())\n        self.dim = len(next(iter((word2vec.items()))))\n\n    def fit(self, X, y):\n        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n        tfidf.fit(X)\n        # if a word was never seen - it must be at least as infrequent\n        # as any of the known words - so the default idf is the max of\n        # known idf's\n        max_idf = max(tfidf.idf_)\n        self.word2weight = defaultdict(\n            lambda: max_idf,\n            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n\n        return self\n\n    def transform(self, X):\n        return np.array([\n            np.mean([self.word2vec[w] * self.word2weight[w]\n                     for w in words if w in self.word2vec] or\n                    [np.zeros(self.dim)], axis=0)\n            for words in X\n        ])\n\n","c71b8bf2":"\"\"\"\nNOTE:\nAs these processes are time consuming, I have simulated these classifiers on my local machine. Committing these codes (In each commit waiting until finishing whole the \nclassifiers was really annoying for me) so I have commented the lines which are representing training process. You can easily uncomment them and try to\ntrain classifier by yourself ;-).\n\"\"\"\n\nprint(\"EXTRA TREE ...\")\n# 1- MEAN VECTORIZER\netree_w2v = Pipeline([(\"w2v mean vectorizer\", MeanEmbeddingVectorizer(w2v)), (\"extra trees\", ExtraTreesClassifier(n_estimators=25))])\n# etree_w2v.fit(X=X_train, y=y_train)\n# results_df.set_value(\"ExtraTree\", \"w2v_mean\", accuracy_score(y_test, etree_w2v.predict(X_test)))\n\n# 2- TFIDF VECTORIZER\netree_w2v_tfidf = Pipeline([(\"w2v tfidf vectorizer\", TfidfEmbeddingVectorizer(w2v)), (\"extra trees\", ExtraTreesClassifier(n_estimators=25))])\n# etree_w2v_tfidf.fit(X=X_train, y=y_train)\n# results_df.set_value(\"ExtraTree\", \"w2v_tfidf\", accuracy_score(y_test, etree_w2v_tfidf.predict(X_test)))\n\n####SVM####\nprint(\"SVM ... \")\n# 1- MAIN VECTORIZER\nsvm_w2v = Pipeline([(\"w2v mean vectorizer\", MeanEmbeddingVectorizer(w2v)), (\"SVM\", LinearSVC(random_state=0, tol=1e-4))])\n# svm_w2v.fit(X=X_train, y=y_train)\n# results_df.set_value(\"SVM\", \"w2v_mean\", accuracy_score(y_test, etree_w2v_tfidf.predict(X_test)))\n\n# 2- TFIDF VECTORIZER\nsvm_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)), (\"SVM\", LinearSVC(random_state=0, tol=1e-4))])\n# svm_w2v_tfidf.fit(X=X_train, y=y_train)\n# results_df.set_value(\"SVM\", \"w2v_tfidf\", accuracy_score(y_test, svm_w2v_tfidf.predict(X_test)))\n\n####MLP####\nprint(\"MLP ... \")\n# 1- MAIN VECTORIZER\nmlp_w2v = Pipeline(\n    [(\"w2v mean vectorizer\", MeanEmbeddingVectorizer(w2v)),\n     (\"MLP\", MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(20,10, 2), random_state=1))])\n# mlp_w2v.fit(X=X_train, y=y_train)\n# results_df.set_value(\"MLP\", \"w2v_mean\", accuracy_score(y_test, mlp_w2v.predict(X_test)))\n\n# 2- TFIDF VECTORIZER\nmlp_w2v_tfidf = Pipeline(\n    [(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)),\n     (\"MLP\", MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(20,10,2), random_state=1))])\n# mlp_w2v_tfidf.fit(X=X_train, y=y_train)\n# results_df.set_value(\"MLP\", \"w2v_tfidf\", accuracy_score(y_test, mlp_w2v_tfidf.predict(X_test)))","ad8da85e":"results_df.set_value(\"NB\",\"w2v_mean\",0.476575)\nresults_df.set_value(\"ExtraTree\",\"w2v_mean\",0.939119)\nresults_df.set_value(\"SVM\",\"w2v_mean\",0.939144)\nresults_df.set_value(\"MLP\",\"w2v_mean\",0.939035)\nresults_df.set_value(\"LREG\",\"w2v_mean\",0.938836)\n\nresults_df.set_value(\"NB\",\"w2v_tfidf\",0.260479)\nresults_df.set_value(\"ExtraTree\",\"w2v_tfidf\",0.939144)\nresults_df.set_value(\"SVM\",\"w2v_tfidf\",0.939033)\nresults_df.set_value(\"MLP\",\"w2v_tfidf\",0.939035)\nresults_df.set_value(\"LREG\",\"w2v_tfidf\",0.953311)","407bd0fe":"results_df","17c59dbf":"fig,axes=plt.subplots(1,1,figsize=(8,8))\naxes.set_ylabel(\"Accuracy\")\naxes.set_title(\"word2vec results for 67% training and 23% testing\")\n# plt.ylim((.93,.95))\nresults_df[[\"w2v_mean\",\"w2v_tfidf\"]].dropna().plot(kind=\"bar\",ax=axes)","9ee546d4":"del w2v\ndel model\nimport gc; gc.collect()\ntime.sleep(10)","f712bf16":"EMBEDDING_FILE = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nglov = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))","dcfa93fd":"next(iter(glov.values()))","0a9fefaf":"\"\"\"\nNOTE:\nAs these processes are time consuming, I have simulated them on my local machine. Committing these codes (Waiting until finishing whole the trainin process for\nany commit really bothers me) so I have commented the lines which have belonged to training process. You can easily uncomment them and try to\ntrain classifier by yourself ;-).\n\"\"\"\n# GLOVE MODEL\n\n###NB####\nprint(\"Multinomina NB ...\")\nnb_glov = Pipeline([(\"glov mean vectorizer\", MeanEmbeddingVectorizer(glov)), (\"Guassian NB\", GaussianNB())])\n# nb_glov.fit(X=X_train, y=y_train)\n# results_df.set_value(\"GB\", \"glov_mean\", accuracy_score(y_test, nb_glov.predict(X_test)))\n\n# 2- TFIDF VECTORIZER\nnb_glov_tfidf = Pipeline([(\"glov tfidf vectorizer\", TfidfVectorizer(glov)), (\"transform\", TfidfTransformer()), (\"Guassian NB\", MultinomialNB())])\n# nb_glov_tfidf.fit(X=X_train, y=y_train)\n# results_df.set_value(\"GB\", \"glov_tfidf\", accuracy_score(y_test, nb_glov_tfidf.predict(X_test)))\n\n###EXTRA TREE####\nprint(\"EXTRA TREE ...\")\n# 1- MEAN VECTORIZER\netree_glov = Pipeline([(\"glov mean vectorizer\", MeanEmbeddingVectorizer(glov)), (\"extra trees\", ExtraTreesClassifier(n_estimators=25))])\n# etree_glov.fit(X=X_train, y=y_train)\n# results_df.set_value(\"ExtraTree\", \"glov_mean\", accuracy_score(y_test, etree_glov.predict(X_test)))\n\n# 2- TFIDF VECTORIZER\netree_glov_tfidf = Pipeline(\n    [(\"glov tfidf vectorizer\", TfidfVectorizer(glov)), (\"transform\", TfidfTransformer()), (\"extra trees\", ExtraTreesClassifier(n_estimators=25))])\n# etree_glov_tfidf.fit(X=X_train, y=y_train)\n# results_df.set_value(\"ExtraTree\", \"glov_tfidf\", accuracy_score(y_test, etree_glov_tfidf.predict(X_test)))\n\n####SVM####\nprint(\"SVM ... \")\n# 1- MAIN VECTORIZER\nsvm_glov = Pipeline([(\"glov mean vectorizer\", MeanEmbeddingVectorizer(glov)), (\"SVM\", LinearSVC(random_state=42, tol=1e-5))])\n# svm_glov.fit(X=X_train, y=y_train)\n# results_df.set_value(\"SVM\", \"glov_mean\", accuracy_score(y_test, svm_glov.predict(X_test)))\n\n# 2- TFIDF VECTORIZER\nsvm_glov_tfidf = Pipeline(\n    [(\"glov tfidf vectorizer\", TfidfVectorizer(glov)), (\"transform\", TfidfTransformer()), (\"SVM\", LinearSVC(random_state=0, tol=1e-5))])\n# svm_glov_tfidf.fit(X=X_train, y=y_train)\n# results_df.set_value(\"SVM\", \"glov_tfidf\", accuracy_score(y_test, svm_glov_tfidf.predict(X_test)))\n\n####MLP####\nprint(\"MLP ... \")\n# 1- MAIN VECTORIZER\nmlp_glov = Pipeline(\n    [(\"glov mean vectorizer\", MeanEmbeddingVectorizer(glov)),\n     (\"MLP\", MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(20, 10, 2), random_state=42))])\n# mlp_glov.fit(X=X_train, y=y_train)\n# results_df.set_value(\"MLP\", \"glov_mean\", accuracy_score(y_test, mlp_glov.predict(X_test)))\n\n# 2- TFIDF VECTORIZER\nmlp_glov_tfidf = Pipeline(\n    [(\"glov tfidf vectorizer\", TfidfVectorizer(glov)), (\"transform\", TfidfTransformer()),\n     (\"MLP\", MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(20, 10, 2), random_state=42))])\n# mlp_glov_tfidf.fit(X=X_train, y=y_train)\n# results_df.set_value(\"MLP\", \"glov_tfidf\", accuracy_score(y_test, mlp_glov_tfidf.predict(X_test)))\n\n\n####LREG####\nprint(\"LREG ... \")\n# 1- MAIN VECTORIZER\nlreg_glov = Pipeline(\n    [(\"glov mean vectorizer\", MeanEmbeddingVectorizer(glov)),\n     (\"LREG\", LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=42))])\n# lreg_glov.fit(X=X_train, y=y_train)\n# results_df.set_value(\"LREG\", \"glov_mean\", accuracy_score(y_test, lreg_glov.predict(X_test)))\n\n# 2- TFIDF VECTORIZER\nlreg_glov_tfidf = Pipeline(\n    [(\"glov tfidf vectorizer\", TfidfVectorizer(glov)), (\"transform\", TfidfTransformer()),\n     (\"LREG\", LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=42))])\n# lreg_glov_tfidf.fit(X=X_train, y=y_train)\n# results_df.set_value(\"LEREG\", \"glov_tfidf\", accuracy_score(y_test, lreg_glov_tfidf.predict(X_test)))","c72d40ac":"results_df.set_value(\"NB\",\"glov_mean\",0.533364)\nresults_df.set_value(\"ExtraTree\",\"glov_mean\",0.939131)\nresults_df.set_value(\"SVM\",\"glov_mean\",0.939140)\nresults_df.set_value(\"MLP\",\"glov_mean\",0.938973)\nresults_df.set_value(\"LREG\",\"glov_mean\",0.938836)\n\nresults_df.set_value(\"NB\",\"glov_tfidf\",0.941720)\nresults_df.set_value(\"ExtraTree\",\"glov_tfidf\",0.945798)\nresults_df.set_value(\"SVM\",\"glov_tfidf\",0.953854)\nresults_df.set_value(\"MLP\",\"glov_tfidf\",0.939035)\nresults_df.set_value(\"LREG\",\"glov_tfidf\",0.953311)","5325e003":"# fig,axes=plt.subplots(1,1,figsize=(15,8))\n# axes.set_ylabel(\"Accuracy\")\n# axes.set_title(\"GLOVE results for 67% training and 23% testing\")\n# results_df.plot(kind=\"bar\",ax=axes)","cfc0587f":"del glov\nimport gc; gc.collect()\ntime.sleep(10)","95c43fbe":"EMBEDDING_FILE = '..\/input\/embeddings\/paragram_300_sl999\/paragram_300_sl999.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nprogram = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE,encoding=\"latin1\"))","7f94cdf5":"next(iter(program.values()))","b76de903":"\"\"\"\nNOTE:\nAs these processes are time consuming, I have simulated them on my local machine. Committing these codes (Waiting until finishing whole the trainin process for\nany commit really bothers me) so I have commented the lines which have belonged to training process. You can easily uncomment them and try to\ntrain classifier by yourself ;-).\n\"\"\"\n\nprint(\"Multinomina NB ...\")\nnb_program = Pipeline([(\"program mean vectorizer\", MeanEmbeddingVectorizer(program)), (\"Guassian NB\", GaussianNB())])\n# nb_program.fit(X=X_train, y=y_train)\n# results_df.set_value(\"GB\", \"program_mean\", accuracy_score(y_test, nb_program.predict(X_test)))\n\n# 2- TFIDF VECTORIZER\nnb_program_tfidf = Pipeline([(\"program tfidf vectorizer\", TfidfVectorizer(program)), (\"transform\", TfidfTransformer()), (\"Guassian NB\", MultinomialNB())])\n# nb_program_tfidf.fit(X=X_train, y=y_train)\n# results_df.set_value(\"GB\", \"program_tfidf\", accuracy_score(y_test, nb_program_tfidf.predict(X_test)))\n\n\n# PROGRAM-300 MODEL\n####EXTRA TREE####\nprint(\"EXTRA TREE ...\")\n# 1- MEAN VECTORIZER\netree_program = Pipeline([(\"program mean vectorizer\", MeanEmbeddingVectorizer(program)), (\"extra trees\", ExtraTreesClassifier(n_estimators=20))])\n# etree_program.fit(X=X_train, y=y_train)\n# results_df.set_value(\"ExtraTree\", \"program_mean\", accuracy_score(y_test, etree_program.predict(X_test)))\n\n\n# 2- TFIDF VECTORIZER\netree_program_tfidf = Pipeline([(\"program tfidf vectorizer\", TfidfEmbeddingVectorizer(program)), (\"extra trees\", ExtraTreesClassifier(n_estimators=20))])\n# etree_program_tfidf.fit(X=X_train, y=y_train)\n# results_df.set_value(\"ExtraTree\", \"program_tfidf\", accuracy_score(y_test, etree_program_tfidf.predict(X_test)))\n\n####SVM####\nprint(\"SVM ... \")\n#1- MAIN VECTORIZER\nsvm_program = Pipeline([(\"program mean vectorizer\", MeanEmbeddingVectorizer(program)), (\"SVM\", LinearSVC(random_state=0, tol=1e-5))])\n# svm_program.fit(X=X_train, y=y_train)\n# results_df.set_value(\"SVM\", \"program_mean\", accuracy_score(y_test, svm_program.predict(X_test)))\n\n# 2- TFIDF VECTORIZER\nsvm_program_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(program)), (\"SVM\", LinearSVC(random_state=0, tol=1e-5))])\n# svm_program_tfidf.fit(X=X_train, y=y_train)\n# results_df.set_value(\"SVM\", \"program_tfidf\", accuracy_score(y_test, svm_program_tfidf.predict(X_test)))\n\n####MLP####\nprint(\"MLP ... \")\n# 1- MAIN VECTORIZER\nmlp_program = Pipeline(\n    [(\"program mean vectorizer\", MeanEmbeddingVectorizer(program)),\n     (\"MLP\", MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(20,10, 2), random_state=42))])\n# mlp_program.fit(X=X_train, y=y_train)\n# results_df.set_value(\"MLP\", \"program_mean\", accuracy_score(y_test, mlp_program.predict(X_test)))\n\n# 2- TFIDF VECTORIZER\nmlp_program_tfidf = Pipeline(\n    [(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(program)),\n     (\"MLP\", MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(20,10, 2), random_state=42))])\n# mlp_program_tfidf.fit(X=X_train, y=y_train)\n# results_df.set_value(\"MLP\", \"program_tfidf\", accuracy_score(y_test, mlp_program_tfidf.predict(X_test)))\n\n####LREG####\nprint(\"LREG ... \")\n# 1- MAIN VECTORIZER\nlreg_program = Pipeline(\n    [(\"program mean vectorizer\", MeanEmbeddingVectorizer(program)),\n     (\"LREG\", LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=42))])\n# lreg_program.fit(X=X_train, y=y_train)\n# results_df.set_value(\"LREG\", \"program_mean\", accuracy_score(y_test, lreg_program.predict(X_test)))\n\n# 2- TFIDF VECTORIZER\nlreg_program_tfidf = Pipeline(\n    [(\"program tfidf vectorizer\", TfidfVectorizer(program)), (\"transform\", TfidfTransformer()),\n     (\"LREG\", LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=42))])\n# lreg_program_tfidf.fit(X=X_train, y=y_train)\n# results_df.set_value(\"LREG\", \"program_tfidf\", accuracy_score(y_test, lreg_program_tfidf.predict(X_test)))","d574ca1f":"results_df.set_value(\"NB\",\"program_mean\",0.559782)\nresults_df.set_value(\"ExtraTree\",\"program_mean\",0.939124)\nresults_df.set_value(\"SVM\",\"program_mean\",0.939026)\nresults_df.set_value(\"MLP\",\"program_mean\",0.939035)\nresults_df.set_value(\"LREG\",\"program_mean\",0.938989)\n\nresults_df.set_value(\"NB\",\"program_tfidf\",0.941720)\nresults_df.set_value(\"ExtraTree\",\"program_tfidf\",0.945717)\nresults_df.set_value(\"SVM\",\"program_tfidf\",0.953854)\nresults_df.set_value(\"MLP\",\"program_tfidf\",0.939035)\nresults_df.set_value(\"LREG\",\"program_tfidf\",0.953311)","96c99dde":"# fig,axes=plt.subplots(1,1,figsize=(15,8))\n# axes.set_ylabel(\"Accuracy\")\n# axes.set_title(\"PROGRAM results for 67% training and 23% testing\")\n# results_df.plot(kind=\"bar\",ax=axes)","4decc75e":"del program\nimport gc; gc.collect()\ntime.sleep(10)","a4261f61":"model = gensim.models.KeyedVectors.load_word2vec_format(\n    fname='..\/input\/embeddings\/GoogleNews-vectors-negative300\/GoogleNews-vectors-negative300.bin',\n    binary=True)\ngoogle = dict(zip(model.wv.index2word, model.wv.syn0))","eec3536f":"next(iter(google.values()))","a823756c":"\"\"\"\nNOTE:\nAs these processes are time consuming, I have simulated them on my local machine. Committing these codes (Waiting until finishing whole the trainin process for\nany commit really bothers me) so I have commented the lines which have belonged to training process. You can easily uncomment them and try to\ntrain classifier by yourself ;-).\n\"\"\"\n\n\n\nprint(\"Multinomina NB ...\")\ngoogle_google = Pipeline([(\"google mean vectorizer\", MeanEmbeddingVectorizer(google)), (\"Guassian NB\", GaussianNB())])\n# google_google.fit(X=X_train, y=y_train)\n# results_df.set_value(\"GB\", \"google_mean\", accuracy_score(y_test, google_google.predict(X_test)))\n\n# 2- TFIDF VECTORIZER\ngoogle_google_tfidf = Pipeline([(\"google tfidf vectorizer\", TfidfVectorizer(google)), (\"transform\", TfidfTransformer()), (\"Guassian NB\", MultinomialNB())])\n# google_google_tfidf.fit(X=X_train, y=y_train)\n# results_df.set_value(\"GB\", \"google_tfidf\", accuracy_score(y_test, google_google_tfidf.predict(X_test)))\n\n\n# GOOGLE_NEWS_VEC MODEL\n####EXTRA TREE####\nprint(\"EXTRA TREE ...\")\n# 1- MEAN VECTORIZER\netree_google = Pipeline([(\"google mean vectorizer\", MeanEmbeddingVectorizer(google)), (\"extra trees\", ExtraTreesClassifier(n_estimators=20))])\n# etree_google.fit(X=X_train, y=y_train)\n# results_df.set_value(\"ExtraTree\", \"google_mean\", accuracy_score(y_test, etree_google.predict(X_test)))\n\n# 2- TFIDF VECTORIZER\netree_google_tfidf = Pipeline([(\"google tfidf vectorizer\", TfidfEmbeddingVectorizer(google)), (\"extra trees\", ExtraTreesClassifier(n_estimators=20))])\n# etree_google_tfidf.fit(X=X_train, y=y_train)\n# results_df.set_value(\"ExtraTree\", \"google_tfidf\", accuracy_score(y_test, etree_google_tfidf.predict(X_test)))\n\n####SVM####\nprint(\"SVM ... \")\n#1- MAIN VECTORIZER\nsvm_google = Pipeline([(\"google mean vectorizer\", MeanEmbeddingVectorizer(google)), (\"SVM\", LinearSVC(random_state=0, tol=1e-5))])\n# svm_google.fit(X=X_train, y=y_train)\n# results_df.set_value(\"SVM\", \"google_mean\", accuracy_score(y_test, svm_google.predict(X_test)))\n\n# 2- TFIDF VECTORIZER\nsvm_google_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(google)), (\"SVM\", LinearSVC(random_state=0, tol=1e-5))])\n# svm_google_tfidf.fit(X=X_train, y=y_train)\n# results_df.set_value(\"SVM\", \"google_tfidf\", accuracy_score(y_test, svm_google_tfidf.predict(X_test)))\n\n####MLP####\nprint(\"MLP ... \")\n# 1- MAIN VECTORIZER\nmlp_google = Pipeline(\n    [(\"google mean vectorizer\", MeanEmbeddingVectorizer(google)),\n     (\"MLP\", MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1))])\n# mlp_google.fit(X=X_train, y=y_train)\n# results_df.set_value(\"MLP\", \"google_mean\", accuracy_score(y_test, mlp_google.predict(X_test)))\n\n# 2- TFIDF VECTORIZER\nmlp_google_tfidf = Pipeline(\n    [(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(google)),\n     (\"MLP\", MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1))])\n# mlp_google_tfidf.fit(X=X_train, y=y_train)\n# results_df.set_value(\"MLP\", \"google_tfidf\", accuracy_score(y_test, mlp_google_tfidf.predict(X_test)))\n\n####LREG####\nprint(\"LREG ... \")\n# 1- MAIN VECTORIZER\nlreg_google = Pipeline(\n    [(\"google mean vectorizer\", MeanEmbeddingVectorizer(google)),\n     (\"LREG\", LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=42))])\n# lreg_google.fit(X=X_train, y=y_train)\n# results_df.set_value(\"LREG\", \"google_mean\", accuracy_score(y_test, lreg_google.predict(X_test)))\n\n# 2- TFIDF VECTORIZER\nlreg_google_tfidf = Pipeline(\n    [(\"google tfidf vectorizer\", TfidfVectorizer(google)), (\"transform\", TfidfTransformer()),\n     (\"LREG\", LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=42))])\n# lreg_google_tfidf.fit(X=X_train, y=y_train)\n# results_df.set_value(\"LREG\", \"google_tfidf\", accuracy_score(y_test, lreg_google_tfidf.predict(X_test)))","d76b3460":"results_df.set_value(\"NB\",\"google_mean\",0.527487)\nresults_df.set_value(\"ExtraTree\",\"google_mean\",0.939126)\nresults_df.set_value(\"SVM\",\"google_mean\",0.939038)\nresults_df.set_value(\"MLP\",\"google_mean\",0.939035)\nresults_df.set_value(\"LREG\",\"google_mean\",0.938806)\n\nresults_df.set_value(\"NB\",\"google_tfidf\",0.941720)\nresults_df.set_value(\"ExtraTree\",\"google_tfidf\",0.945448)\nresults_df.set_value(\"SVM\",\"google_tfidf\",0.953854)\nresults_df.set_value(\"MLP\",\"google_tfidf\",0.939035)\nresults_df.set_value(\"LREG\",\"google_tfidf\",0.953311)","6eb04fae":"# fig,axes=plt.subplots(1,1,figsize=(15,8))\n# axes.set_ylabel(\"Accuracy\")\n# axes.set_title(\"GOOGLE_NEWS results for 67% training and 23% testing\")\n# results_df.plot(kind=\"bar\",ax=axes)","2ded9535":"del google\ndel model\nimport gc; gc.collect()\ntime.sleep(10)","753ed906":"model = gensim.models.KeyedVectors.load_word2vec_format(fname='..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec',encoding='utf-8')\nwiki = dict(zip(model.wv.index2word, model.wv.syn0))","3298c3da":"next(iter(wiki.values()))","9c1ded18":"\"\"\"\nNOTE:\nAs these processes are time consuming, I have simulated them on my local machine. Committing these codes (Waiting until finishing whole the trainin process for\nany commit really bothers me) so I have commented the lines which have belonged to training process. You can easily uncomment them and try to\ntrain classifier by yourself ;-).\n\"\"\"\n\n\nprint(\"Multinomina NB ...\")\nwiki_wiki = Pipeline([(\"wiki mean vectorizer\", MeanEmbeddingVectorizer(wiki)), (\"Guassian NB\", GaussianNB())])\n# wiki_wiki.fit(X=X_train, y=y_train)\n# results_df.set_value(\"GB\", \"wiki_mean\", accuracy_score(y_test, wiki_wiki.predict(X_test)))\n\n# 2- TFIDF VECTORIZER\nwiki_wiki_tfidf = Pipeline([(\"wiki tfidf vectorizer\", TfidfVectorizer(wiki)), (\"transform\", TfidfTransformer()), (\"Guassian NB\", MultinomialNB())])\n# wiki_wiki_tfidf.fit(X=X_train, y=y_train)\n# results_df.set_value(\"GB\", \"wiki_tfidf\", accuracy_score(y_test, wiki_wiki_tfidf.predict(X_test)))\n\n####EXTRA TREE####\nprint(\"EXTRA TREE ...\")\n# 1- MEAN VECTORIZER\netree_wiki = Pipeline([(\"wiki mean vectorizer\", MeanEmbeddingVectorizer(wiki)), (\"extra trees\", ExtraTreesClassifier(n_estimators=20))])\n# etree_wiki.fit(X=X_train, y=y_train)\n# results_df.set_value(\"ExtraTree\", \"wiki_mean\", accuracy_score(y_test, etree_wiki.predict(X_test)))\n\n# 2- TFIDF VECTORIZER\netree_wiki_tfidf = Pipeline([(\"wiki tfidf vectorizer\", TfidfVectorizer(wiki)), (\"transform\", TfidfTransformer())\n                             , (\"extra trees\", ExtraTreesClassifier(n_estimators=20))])\n# etree_wiki_tfidf.fit(X=X_train, y=y_train)\n# results_df.set_value(\"ExtraTree\", \"wiki_tfidf\", accuracy_score(y_test, etree_wiki_tfidf.predict(X_test)))\n\n####SVM####\nprint(\"SVM ... \")\n#1- MAIN VECTORIZER\nsvm_wiki = Pipeline([(\"wiki mean vectorizer\", MeanEmbeddingVectorizer(wiki)), (\"SVM\", LinearSVC(random_state=0, tol=1e-5))])\n# svm_wiki.fit(X=X_train, y=y_train)\n# results_df.set_value(\"SVM\", \"wiki_mean\", accuracy_score(y_test, svm_wiki.predict(X_test)))\n\n# 2- TFIDF VECTORIZER\nsvm_wiki_tfidf = Pipeline([(\"wiki tfidf vectorizer\", TfidfVectorizer(wiki)), (\"transform\", TfidfTransformer()),\n                           (\"SVM\", LinearSVC(random_state=0, tol=1e-5))])\n# svm_wiki_tfidf.fit(X=X_train, y=y_train)\n# results_df.set_value(\"SVM\", \"wiki_tfidf\", accuracy_score(y_test, svm_wiki_tfidf.predict(X_test)))\n\n####MLP####\nprint(\"MLP ... \")\n# 1- MAIN VECTORIZER\nmlp_wiki = Pipeline(\n    [(\"wiki mean vectorizer\", MeanEmbeddingVectorizer(wiki)),\n     (\"MLP\", MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1))])\n# mlp_wiki.fit(X=X_train, y=y_train)\n# results_df.set_value(\"MLP\", \"wiki_mean\", accuracy_score(y_test, mlp_wiki.predict(X_test)))\n\n# 2- TFIDF VECTORIZER\nmlp_wiki_tfidf = Pipeline(\n    [(\"wiki tfidf vectorizer\", TfidfVectorizer(wiki)), (\"transform\", TfidfTransformer()),\n     (\"MLP\", MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1))])\n# mlp_wiki_tfidf.fit(X=X_train, y=y_train)\n# results_df.set_value(\"MLP\", \"wiki_tfidf\", accuracy_score(y_test, mlp_wiki_tfidf.predict(X_test)))\n\n####LREG####\nprint(\"LREG ... \")\n# 1- MAIN VECTORIZER\nlreg_wiki = Pipeline(\n    [(\"wiki mean vectorizer\", MeanEmbeddingVectorizer(wiki)),\n     (\"LREG\", LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=42))])\n# lreg_wiki.fit(X=X_train, y=y_train)\n# results_df.set_value(\"LREG\", \"wiki_mean\", accuracy_score(y_test, lreg_wiki.predict(X_test)))\n\n# 2- TFIDF VECTORIZER\nlreg_wiki_tfidf = Pipeline(\n    [(\"wiki tfidf vectorizer\", TfidfVectorizer(wiki)), (\"transform\", TfidfTransformer()),\n     (\"LREG\", LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=42))])\n# lreg_wiki_tfidf.fit(X=X_train, y=y_train)\n# results_df.set_value(\"LREG\", \"wiki_tfidf\", accuracy_score(y_test, lreg_wiki_tfidf.predict(X_test)))","2acedf6b":"results_df.set_value(\"NB\",\"wiki_mean\",0.939035)\nresults_df.set_value(\"ExtraTree\",\"wiki_mean\",0.939249)\nresults_df.set_value(\"SVM\",\"wiki_mean\",0.925018)\nresults_df.set_value(\"MLP\",\"wiki_mean\",0.074982)\nresults_df.set_value(\"LREG\",\"wiki_mean\",0.939035)\n\nresults_df.set_value(\"NB\",\"wiki_tfidf\",0.941720)\nresults_df.set_value(\"ExtraTree\",\"wiki_tfidf\",0.944973)\nresults_df.set_value(\"SVM\",\"wiki_tfidf\",0.953854)\nresults_df.set_value(\"MLP\",\"wiki_tfidf\",0.950935)\nresults_df.set_value(\"LREG\",\"wiki_tfidf\",0.953311)","443d2b1e":"results_df","374aeea8":"del wiki\ndel model\nimport gc; gc.collect()\ntime.sleep(10)","8e3b115f":"fig,axes=plt.subplots(1,1,figsize=(15,8))\nplt.ylim((.5,1))\naxes.set_ylabel(\"Accuracy\")\naxes.set_title(\"Traditional Classfieris Results for 67% Training and 23% Testing with Two Types of Embedding\")\nresults_df[results_df.index != \"RF\"].plot(kind=\"bar\",ax=axes)","e670209e":"for name in dir():\n    if not name.startswith('_'):\n        del globals()[name]\nfor name in dir():\n    if not name.startswith('_'):\n        del locals()[name]\nimport gc; gc.collect()","11d5763b":"import pandas as pd\nfrom keras import Sequential\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D,Dense, Input, Embedding, Dropout, LSTM, CuDNNGRU\nfrom keras import Sequential\nfrom keras.models import Model\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.model_selection import train_test_split\ntrain_df = pd.DataFrame.from_csv(\"..\/input\/train.csv\")\nX_train, X_test, y_train, y_test = train_test_split(train_df.question_text, train_df.target, test_size=0.33, random_state=42)","6c63e77d":"num_words = 5000\nmaxlen = 100\ntokenizer = Tokenizer(num_words=num_words)\ntokenizer.fit_on_texts(list(X_train))\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)","ecb959ef":"X_train[0]","cf5d2d01":"len(X_train[0]),len(X_train[10])","5098a519":"max_len = 150\nX_train = pad_sequences(X_train, maxlen=maxlen)\nX_test = pad_sequences(X_test, maxlen=maxlen)","17e7ca8b":"len(X_train[0]),len(X_train[10])","d11338f4":"embedding_size = 300\nmodel = Sequential()\nmodel.add(Embedding(num_words, embedding_size))\n## Bidirectional wrapper for RNNs. It involves duplicating the first recurrent\n## layer in the network so that there are now two layers side-by-side, then \n## providing the input sequence as-is as input to the first layer and providing\n## a reversed copy of the input sequence to the second.\nmodel.add(Bidirectional(CuDNNGRU(64, return_sequences=True))) \nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(16, activation=\"relu\"))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1, activation=\"sigmoid\"))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","c18b3d46":"model.summary()","5a8fb6ab":"model.fit(x=X_train, y=y_train, batch_size=512, epochs=1, validation_data=(X_test, y_test))","234a0256":"<a id=\"45\"><\/a> <br>\n#  9-EMBEDDING METHODS_WIKI_NEWS","13820e88":"<a id=\"27\"><\/a> <br>\n**D. Multinominal NB**","966e0ff1":"Now, Lets remove variables for having clean workspace for rest of the kernel.","9ef02beb":"<a id=\"36\"><\/a> <br>\n**C. TEXTS and VECTORS RELATIONS**","a73011b3":"<a id=\"0\"><\/a> <br>\n## Kernel Headlines\n1. [Introduction](#1)\n    1.  [Concept of Text Mining](#2)\n    2.  [Text Mining Importance and Difficulties](#3)\n    3.  [Text Mining In Buisiness](#4)\n    4.  [Dealing With Unstructured Data](#5)\n    5.  [Dealing With Dirty Space](#6)\n    6.  [Why Representation is Complex ?!](#7)\n    7. [Wath Does BagOfWords Means?!](#8)\n    8. [Ignoring Grammer and NLP-Based Features](#9)\n    9. [Feature Extraction Using BOW](#10)\n    10. [Term Frequency (TF)](#11)\n    11. [Inverse Term Frequency (IDF)](#12)\n    12. [Simple Example (TF-IDF)](#13)\n    13. [Preprocessing In Traditional Manners](#14)\n2. [Embedding Methods](#15)\n     1. [What Does Embedding Means?!](#16)\n     2. [What Does N-Gram Means?!](#17)\n     3. [Main Disadvantages of N-Gram](#18)\n     4. [What Does Embeddings Provide For Text Mining](#19)\n3. [Take a Glance to Data](#20)\n     1. [WordCloud](#21)\n     2. [Target Distribution](#22)\n4. [Start Vectorizing by Simple CountVectorizer](#23)\n    1. [Make Your Hands Dirty;-) ](#24)\n    2. [Some Suggestions About Text Vectorizing ](#25)\n    3. [Classification](#26)\n        4. [Naive Bayes](#27)\n        5. [RandomForest](#28)\n        6. [MLP](#29)\n        7. [LogicRegression](#30)\n        8. [Classifiers Accuracy Comparisons on CountVectorizer](#31)\n        9. [Curse of Dimentionality](#32)\n5. [Embedding Methods_Training Our Word2Vec Model](#33)\n\t1. [Introduction](#34)\n\t2. [Python Tip. Using Generators To Avoid Memory Error](#35)\n\t3. [Texts and Vectors Relations](#36)\n\t4. [Training Using Pandas Piplines](#37)\n\t5. [Classifier Accuracy Comparison On Our Word2Vec Model](#38)\n6. [Embedding Methods_Glove](#39)\n\t1. [Introduction](#40)\n\t2. [Be careful about our method](#41)\n\t3. [GLOVE initiation](#42)\n7. [Embedding Methods_Program_300Vectors](#43)\n8. [Embedding Methods_GoogleNewsVectors](#44)\n9. [Embedding Methods_Wiki_Vectors](#45)\n10. [Conclusion on Traditonal Methods](#46)\n11. [Moving to DeepMoldes](#47)\n    1. [Introduction and Roadmap](#48)\n\t2. [Prepairing Data for Using in Keras](#49)\n12. [References](#100)","bfab14b0":"<a id=\"26\"><\/a> <br>\n**C. CLASSIFICATION**","425a6be4":"<a id=\"100\"><\/a> <br>\n#  11-REFERENCES\n\n1- [Datascience for Buisiness.](https:\/\/www.amazon.com\/Data-Science-Business-Data-Analytic-Thinking\/dp\/1449361323).\n\n2- [tf-idf website.](http:\/\/www.tfidf.com\/)\n\n3- [Google Machine Learning Developement website.](https:\/\/developers.google.com\/machine-learning\/crash-course\/embeddings\/video-lecture)\n\n4- [Text Classification With Word2Vec.](http:\/\/nadbordrozd.github.io\/blog\/2016\/05\/20\/text-classification-with-word2vec\/)\n\n5-[How use word embedding layers for DeepLearning with Keras.](https:\/\/machinelearningmastery.com\/use-word-embedding-layers-deep-learning-keras\/)\n\n6-Personal experiences in similar projects.\n","013f87b9":"For example, we want have 5000 word and embedding size we want is 300. So, we have Embedding(input_dim=5000, output_dim=300). The output of the Embedding layer is a 2D vector with one embedding for each word in the input sequence of words (input document). If you wish to connect a Dense layer directly to an Embedding layer, you must first flatten the 2D output matrix to a 1D vector using the Flatten layer.","4cd9ad78":"<a id=\"49\"><\/a> <br>\n* **B. PREPARING DATA FOR USING IN KERAS**\n\nWe select X_train and X_test as the same way we had done in previous section.\n\nLets initialize the transformation process.\n","2bfda4de":"<a id=\"21\"><\/a> <br>\n**A. TEST WORDCLOUD**","c8ecdebd":"<a id=\"24\"><\/a> <br>\n**A. MAKE YOUR HANDS DIRTY ;-) **\n\nStarting with CountVectorizer.\n\nAs we mentioned in introduction, for converting string to feature spaces, we need do some transforms on string. As a result of this sort of transonsformations, the string will be converted to vectors space. One of teh most simplest ways for vectorizing the text and converting it to a formal dataset (which could be passed to classification algorithms) is countVectorizer. Simply, the count of each word will be calculated. In the second step, by using TermFrequency transformations, for any word and for any document, the relative proportions will be form a formal dataset.\n\nLets start with a countVectorizer...","cf744f95":"There are also non-english featuers. These sort of features can reduce the accuracy.","aee37363":"As you can see there is vector for each word. The better trained models the higher accuracy we can get. Now you can understand what does the embedding models can do for us.","a9c09b63":"<a id=\"48\"><\/a> <br>\n* **A. INTRODUCTION AND ROADMAP**\n\nCongratulations. Now you have enough information about \n        \n*What does text mining means ?!*\n\n*How to convert text to the apprehensible language for MachineLearning algorithms ?!*\n\n*What does countVectorizer and TfIdf vectorizer means ?! *\n\n*What embedding is ?!*\n\n*How to map embeddings to our data ?!*\n    \n","3262bdf9":"<a id=\"31\"><\/a> <br>\n**H. CLASSIFIERS ACCURACY COMPARISON ON COUNT_VECTORIZER**\n\nCongratulation ! \n\nYou have tested large variety of classifiers.\nLets visualize the result of tested classifiers.","b9534486":"Keras offers an Embedding layer that can be used for neural networks on text data. It requires that the input data be integer encoded, so that each word is represented by a unique integer. This data preparation step can be performed using the Tokenizer API also provided with Keras. The Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset.\nIt is a flexible layer that can be used in a variety of ways, such as:\n\n1. It can be used alone to learn a word embedding that can be saved and used in another model later.\n\n2. It can be used as part of a deep learning model where the embedding is learned along with the model itself.\n\n3. It can be used to load a pre-trained word embedding model, a type of transfer learning.\n","2441a93d":"<a id=\"22\"><\/a> <br>\n**B. TARGET DISTRIBUTION**","c8d66845":"<a id=\"47\"><\/a> <br>\n#  11-MOVING TO DEEP MODELS","e76aca4a":"As you can see, the process is completely similar to the previous section.","7ebb60f3":"Training Word2Vec. \nWe will ignore words which have repetition less than 5 times and scrolling default window size equal to 5. Changing these parameters can change our model accuracy. We will check it in next steps.","df5ea60a":"<a id=\"46\"><\/a> <br>\n#  10-CONCLUSION ON TRADITIONAL METHODS\n","7de3284f":"Now, you can see the issule has been solved. Now, whole the X_train has the same dimention. Now, it can be passed to the classfieirs. \n\nWe have used CuDNNGRU which is Fast GRU implementation backed by cuDNN. So, first of all turn on GPU for your kernel in the your kernel setting. If you are dealing with a system which is does not contain GPU, replace the CuDNNGRU with LSTM method. more information could be found [here.](https:\/\/stackoverflow.com\/questions\/49183538\/simple-example-of-cudnngru-based-rnn-implementation-in-tensorflow)","5605df90":"<a id=\"1\"><\/a> <br>\n#  1-Introduction\n\nIn this competition we are dealing with text processing and embedding vectors. First of all, lets recap some basic and fundamental concepts.\n<a id=\"2\"><\/a> <br>\n* **A. Concept of Text Mining**\n\nIn principle, text is just another form of data, and text processing is just a special case of representation engineering. In reality, dealing with text requires dedicated pre-processing steps and sometimes specific expertise on the part of the data science In this introduction we can only scratch the surface, to give a basic overview of the techniques and issues involved in typical business applications. First, let\u2019s discuss why text is so important and why it\u2019s difficult.\n<a id=\"3\"><\/a> <br>\n* **B. Text Mining Importance and Difficulties**\n\nExploiting this vast amount of data requires converting it to a meaningful form. The Internet may be the home of \u201cnew media,\u201d but much of it is the same form as old media. It contains a vast amount of text in the form of personal web pages, Quera, Twitter feeds, email, Facebook status updates, product descriptions, Reddit comments, blog postings\u2014the list goes on. Underlying the search engines (Google and Bing) that we use everyday are massive amounts of text-oriented data science. Indeed, the thrust of Web 2.0 was about Internet sites allowing users to interact with one another as a community, and to generate much added content of a site. This user-generated content and interaction usually takes the form of text.\n\n<a id=\"4\"><\/a> <br>\n* **C. TEXT MINING IN BUSINESS**\n\nIn businesses such as our case study (Quera), understanding customer feedback often requires understanding text. This isn\u2019t always the case; admittedly, some important consumer attitudes are represented explicitly as data or can be inferred through behavior, for example via five-star ratings, click-through patterns, conversion rates, and so on. We can also pay to have data collected and quantified through focus groups and online surveys. But in many cases if we want to \u201clisten to the customer\u201d we\u2019ll actually have to read what she\u2019s written\u2014in product reviews, customer feedback forms, opinion pieces, and email messages.\n<a id=\"5\"><\/a> <br>\n* **D. DEALING WITH UNSTRUCTURED DATA**\n\nWhy Text Is Difficult Text is often referred to as \u201cunstructured\u201d data. This refers to the fact that text does not have the sort of structure that we normally expect for data: tables of records with fields having fixed meanings (essentially, collections of feature vectors), as well as links between the tables. Words can have varying lengths and text fields can have varying numbers of words.\n<a id=\"6\"><\/a> <br>\n* **E. DEALING WITH DIRTY SPACE**\n\nSometimes word order matters, sometimes not. As data, text is relatively dirty .People write ungrammatically, they misspell words, they run words together, they abbreviate unpredictably, and punctuate  randomly. Even when text is flawlessly expressed it may contain synonyms (multiple words with the same meaning) and homographs (one spelling shared among multiple words with different meanings).\n<a id=\"7\"><\/a> <br>\n* **F. WHY REPRESENTATION IS COMPLEX !**\n\nRepresentation Having discussed how difficult text can be, let\u2019s go through the basic steps to transform a body of text into a set of data that can be fed into a data mining algorithm. The general strategy in text mining is to use the simplest (least expensive) or small. A document is composed of individual tokens or terms. For now, think of a token or term as just a word; as we go on we\u2019ll show how they can be different from what are customarily thought of as words. A collection of documents is called a corpus.\n<a id=\"8\"><\/a> <br>\n* **G. WATH DOES BAG OF WORDS MAENS?!**\n\nBag of Words It is important to keep in mind the purpose of the text representation task. In essence, we are taking a set of documents\u2014each of which is a relatively free-form sequence of words\u2014and turning it into our familiar feature-vector form. Each document is one instance but we don\u2019t know in advance what the features will be. The approach we introduce first is called \u201cbag of words.\u201d As the name implies, the approach is to treat every document as just a collection of individual words.\n<a id=\"9\"><\/a> <br>\n* **H. IGNORING GRAMMER AND NLP-BASED FEATURES**\n\nBOW approach ignores grammar, word order, sentence structure, and (usually) punctuation. It treats every word in a document as a potentially important keyword of the document. The representation is  straightforward and inexpensive to generate, and tends to work well for many tasks.\n<a id=\"10\"><\/a> <br>\n* **I. FEATURE EXTRACTION USING BOW**\n\nSo if every word is a possible feature, what will be the feature\u2019s value in a given document? There are several approaches to this. In the most basic approach, each word is a token, and each document is represented by a one (if the token is present in the document) or a zero (the token is not present in the document). This approach simply reduces a document to the set of words contained in it.\n<a id=\"11\"><\/a> <br>\n* **J. TERM-FREQUENCY (TF)**\n\nThe next step up is to use the word count (frequency) in the document instead of just a zero or one. This allows us to differentiate between how many times a word is used; in some applications, the importance of a term in a document should increase with the number of times that term occurs. This is called the term frequency representation. It could simply defined by the following equation: \n*TF(t) = (Number of times term t appears in a document) \/ (Total number of terms in the document).*\n<a id=\"12\"><\/a> <br>\n* **K. INVERSE DOCUMENT FREQUENCY (IDF)**\n\nwhich measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following: \n*IDF(t) = log_e(Total number of documents \/ Number of documents with term t in it).*\nidf is on the best methods for Measuring Sparseness\n<a id=\"13\"><\/a> <br>\n* **L. Example FROM (tfidf.com)**\n\nConsider a document containing 100 words wherein the word cat appears 3 times. The term frequency (i.e., tf) for cat is then (3 \/ 100) = 0.03.\nNow, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is\ncalculated as log(10,000,000 \/ 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.\n<a id=\"14\"><\/a> <br>\n* **M. PRE-PROCESSING WHICH ARE CONVENTIONAL (SPECIALLY FOR TRADITIONAL MANNERS).**\n\nFirst, the case has been normalized: every term is in lowercase. This is so that words like Skype and SKYPE are counted as the same thing. Second, many words have been stemmed : their suffixes removed, so that verbs like announces , announced and announcing are all reduced to the term announce. Finally, stopwords have been removed. A stopword is a very common word in English (or whatever language is being parsed). The words the , and , of , and on are considered stopwords in English so they are typically removed.\n\n<a id=\"15\"><\/a> <br>\n#  2-EMBEDDING METHODS\n<a id=\"16\"><\/a> <br>\n* **A. WHAT DOES EMBEDDING MEANS ?!**\n\nAn embedding is a relatively low-dimensional space into which you can translate high-dimensional vectors. Embeddings make it easier to do machine learning on large inputs like sparse vectors representing words. Ideally, an embedding captures some of the semantics of the input by placing semantically similar inputs close together in the embedding space. An embedding can be learned and reused across models.\n<a id=\"17\"><\/a> <br>\n* **B. What Does N-GRAM Means?!**\n\nAs presented, the bag-of-words representation treats every individual word as a term, discarding word order entirely. In some cases, word order is important and you want to preserve some information about it in the representation. A next step up in complexity is to include sequences of adjacent words as terms. For example, we could include pairs of adjacent words so that if a document contained the sentence \u201cThe quick brown fox jumps.\u201d it would be transformed into the set of its constitutent words { quick , brown , fox , jumps }, plus the tokens quick_brown , brown_fox , and fox_jumps . This general representation tactic is called n-grams . Adjacent pairs are commonly called bi-grams. If you hear a data scientist mention representing text as \u201cbag of n-grams up to three\u201d it simply means she\u2019s representing each document using as\nfeatures its individual words, adjacent word pairs, and adjacent word triples. N-grams are useful when particular phrases are significant but their component words may not be. In a business news story, the appearance of the tri-gram exceed_analyst_expectation is more meaningful than simply knowing that the individual words analyst , expectation , and exceed appeared somewhere in a story. An advantage of using n-grams is that they are easy to generate; they require no linguistic knowledge or complex parsing algorithm.\n<a id=\"18\"><\/a> <br>\n* **C. MAIN DISADVANTAGE OF N-GRAM**\n\nThe main disadvantage of n-grams is that they greatly increase the size of the feature set. There are far more word pairs than individual words, and still more word triples. The number of features generated can quickly get out of hand. Data mining using n-grams almost always needs some special consideration for dealing with massive numbers of features, such as a feature selection stage or special consideration to computational storage space. \n<a id=\"19\"><\/a> <br>\n* **D. WHAT DOES EMBEDDING PROVIDE FOR US IN TEXT MINING ?!**\n\nIn the most simplest way, we can say embedding means replacing each word of our corpus with the correspondence feature space that pre-trained embedding models have provided. Some organizations have trained a deep learning models on popular datasets such as wikipedia. If you want to know how they have train the models check [this url](https:\/\/radimrehurek.com\/gensim\/tut1.html#from-strings-to-vectors). Training a deep learning model needs process and time. So, we cant do it by our personal laptops. Fortunately, some of these pretrained embeddings (i.e models which are existed in data directory of this competition) are published with opensource licences. We need only convert our text to the target feature spaces and do a classification task with wide range of classifier we have known.\n\n<a id=\"20\"><\/a> <br>\n#  3-Take a Glance To Data\n\nOk. lets do simple WordCloud for easily getting start ... ","e23d6fcb":"Any of these rows, reveals the vector of each document. The columns are also correspondence to words existed in the dataset. You can access the feature names by 'get_feature_names()' method.\nLets look around some sections of features.","e323820c":"<a id=\"44\"><\/a> <br>\n#  8-EMBEDDING METHODS_GOOGLE_NEWS_VECTORS","293e7385":"We need to have a standard dimentions for whole the X_train records. As you can see above, the dimantion of one vector is 14; and it is 22 for anothre record. we need to convert them to standard dimentoin.\nHopefully, like Tokenizer, keras also provide pad_sequence to solving this issue. Lets do it. \n\nLets consider the padding size 150. It means we ignore the words 151 and plus in sentences (if exist)","3730617a":"![](https:\/\/www.themavencircle.com\/wp-content\/uploads\/2016\/03\/Neuro-linguistic-Programming-650x435.jpg)","69f9010b":"The kernel is under development and more analysis in both sides of concept descriptions and code developements have been queued to come in near future.\nBe in touch with it ;-)\n\nThe rest of tutorial will be completed  as soon as possible ;-).\n\nAny comment, idea or hint will be appreciated.\n\n**Your upvote will be motivation for me for continuing the kernel ;-)**\n","722188da":"Don't forget we have used limit number of layers and neurons. Results absolutely could be better by tuning parameters in more precise way.\n\nRandomForest also could revealed better results if it trains with more decision trees.\n\n**Simply, we can improve accuracy of these classifiers with more supervised training.**","b0acae6b":"Or in more related way to competition challenges...","6ebd7537":"<a id=\"37\"><\/a> <br>\n**D. TRAINING USING PANDAS PIPLINES**","cd259231":"<a id=\"42\"><\/a> <br>\n**C. GLOVE INITIATIONS.**\n","45ce901d":"    **-. SAVING THE MODEL **","3e380889":"    **-. MOST SIMILAR WORDS TO ... **","8e0fc6ef":"    **-.  [ X+Y ]  - [ Z ] ... **","7be5f09e":"The point could be mentioed is approximately whole the classifiers and whole the embedding methods (except our trained w2v) have the similar accuracy another point could be mentioned is in overall range of accuracy in tfidf model is greater than mean-vectorizer models.","63e0bebf":"<a id=\"43\"><\/a> <br>\n#  7-EMBEDDING METHODS_PROGRAM_300","01e7f08f":"Until now, we have vector for each word. How we can convert this vectors to formal dataset which could be passed to traning algorithms ?!\n\nSimply manner could be mean of vectors in any dataset record. Another manner could be the same tfidf transformers. \n\nLets implement them for using in our pipeline.\n\nLets split Train and Test for training the classfieirs.","7896357d":"The Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n\n1. input_dim: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n2. output_dim: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n3. input_length: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000.\n\n","9c1a47dd":"Now we have created our model considering deep learning approach. \n\nAnother point could be touched on is, because of being time consuing procedures, I have limited the epoch numbers on 1. It is clear that the greater number of epochs is proportional to the more accuracy you can get.\n\nI will try to present some concise doctuments  about the difference existed in various types of deep learning models. and represent them in future commits.","cf7858ab":"Because of high dimentional problems we have encountered with, by default it returns the matrix in sparse mode. For representing in normal feature space, you can use todense method.","eac44323":"<a id=\"28\"><\/a> <br>\n**E. RF**","7dea81ed":"<a id=\"25\"><\/a> <br>\n**B. SOME SUGGESTIONS ABOUT TEXT VECTORIZING  **\n\nOk. Good.\n\nLets review some facts about text processing in this special case.\n\nAs it is obvious you can see there are various range of features (from foreign language to astonishing digits and numbers) in feature space.\nWe can do some modifications on the count_vectorizer. Some preprocessing could be done are:\n\n* Cleaning meaningless words from dataset.\n\n* Using threshold for repetition of words. for example discarding the words which are not repeated in whole records. it  could be setted by min_df in count_vectorizer\n\n* Defining threshold for minimum repetition of words. for example any word that has repetitioin fewer than 10 times, should be ignored.\n\n* Removing Stopwords. Stop words are common words that have low information for classification because they exist on any sentences. For example, words such as 'the','is', 'by' and this sort of words, have belonged to stop_words list. Removing them from dataset can increase the performance accuracy. Count_vectorizer supports using stop words by defatult. You can use it by CountVectorizer(stop_words=LIST_OF_STOP_WORDS). It is wise to see improvements in classifier performance by adding effect of stop_words to it.","abdd422f":"<a id=\"29\"><\/a> <br>\n**F. MLP**","5e8de086":"<a id=\"39\"><\/a> <br>\n#  6-EMBEDDING METHODS_GLOVE\n\n<a id=\"40\"><\/a> <br>\n**A. INTRODUCTION**\n\nIn prevous section we represented the results of own word2vec model. Now, kets do a classification with glov embedded models.\nIn the most simplest way, we can say for every embedding models we have a dictionary containing key (word) and values (list of double values which are correspondence to relation to other words which are exist on that dataset).\n\n\n<a id=\"41\"><\/a> <br>\n**B. BE CAREFUL ABOUT OUR MANNER.**\n\n**The point should be mentioned is that because of time and memory limitations, we have limit our training data to 67 percent of samples. The most reason for the low accuracies and a gap which is existed in most of the classifiers is related to type of our selection. In most of the classifiers, we also limit the parameters. for example in ExtraTree we use only 25 classifiers which is absolutely low. Or in the neural network MLP we have used two layer perceptron model which is approximately unsatisfied model. By the way, lets continue the process. We will try to extract all the information which is needed and use them in final step.**\n","94b4d019":"In this section, we will look at how we can do a similar procedure in deep learning world.  We will use Keras as our main deep learning interface. The back engine is also tensorflow.  In the next sections we will look at the embedding methods and how we can apply them in our manner using Keras api's.\n    ","20a4c19f":"Same to the process we had done in countVectorizer, we need to tokenize the text and convert them to list of integeres. But how ?!\n\nKeras provides the tokenizer method for doing it easily.","e5aea46c":"In this section we will use pipline facilities in training the data.\n\nAbout the pipelines there are two points should be mentioned. \n\nFrom the documentation:\n1. Pipelines can Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be \u2018transforms\u2019, that is, they must implement fit and transform methods. The final estimator only needs to implement fit.\n2. The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. \n","9709daf4":"<a id=\"38\"><\/a> <br>\n**E. CLASSIFIERS ACCURACY COMPARISON ON OUR WORD2VEC MODEL**\n","6b30c79c":"<a id=\"30\"><\/a> <br>\n**G. LoggicReg**","62606648":"<a id=\"33\"><\/a> <br>\n#  5-EMBEDDING METHODS_WORD2VEC\n\n<a id=\"34\"><\/a> <br>\n**A. INTRODUCTION**\n\nNow you know how to deal with texts. At first step, we need to extract features from text; Then there is a need to investigate classifiers and tune them for getting better accuracy.  In previous sections you trained a classifier based on the count vectorizer. Now, lets move on to newer vectorizing method. The name is Word2Vec. As its name is simply representing, by using this manner any word will be replaced by a correspondence vectors. In this manner, training parameters are playing important role. lets try word2vec on our dataset to understand what does it means.\n\n**As these processes are time consuming, I have simulated these classifiers on my local machine. Committing these codes (In each commit waiting until finishing whole the  classifiers was really annoying for me) so I have commented the lines which are representing training process. You can easily uncomment them and try to  train classifier by yourself ;-). **\n\n<a id=\"35\"><\/a> <br>\n**B. PYTHON TIP. USE GENERATORS TO AVOID MEMORY ERROR**\n\nTo train a model on text, we need pass following steps: \n Reading text and keeping in memory,\n Tokenizing the text\n Iterating on specified window\n \n In use cases such as texts (especially large texts) keeping whole the text in memory is memory consumable. Using generators, can reduce the overhead of\n iterating. Python generators are a simple way of creating iterators. All the overhead we mentioned above are automatically handled by generators in\n Python. Simply speaking, a generator is a function that returns an object (iterator) which we can iterate over (one value at a time).\n","79a84000":"Now, We have the dataset which is ready for classification.\nLets do simple classifications for starting classification on first vectorized space (count vectorize).","e0715bbd":"As you can see, we have a vector for word 'the'. It means we can replace 'the' with correspondence vector. Lets do some interesting calculations with our word2vec model ;-)","9db052ef":"<a id=\"23\"><\/a> <br>\n#  4-Start Vectorizing by Simple CountVectorizer","f752aa18":"<a id=\"32\"><\/a> <br>\n**I. CURSE OF DIMENTIONALITY**\n\n\nLook again at the dimension of train data. It has 49650 features !!! Dealing with such a huge feature space is really challenging.\nIf you do classification (similar to what we have done in previous part) you will get MemoryError. \nOne of the most challenging factors in data mining is dimensionality. Read about curse of dimensionality [here](https:\/\/en.wikipedia.org\/wiki\/Curse_of_dimensionality) if you want to know more about it.\n\nFor our example we have 49650 features. Absolutely classifiers such as DecisionTree, KNeighborsClassifier or SVM will return MemoryError.  \n"}}