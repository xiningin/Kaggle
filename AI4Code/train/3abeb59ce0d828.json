{"cell_type":{"04107c34":"code","4958e066":"code","f055bf2d":"code","aed048a0":"code","7afedaaa":"code","10b72395":"code","eaa751f7":"code","432ddf13":"code","5cf50aab":"code","bb2a30d5":"code","8651c12c":"code","049472c9":"code","bf6f1b98":"code","f4f78a62":"code","e48bca7a":"code","13d876b8":"code","ecc73a09":"code","24ea5ba6":"code","254f2f83":"code","1aa820dc":"code","b53e2b36":"code","d31c4481":"code","f2faadac":"code","39a815c3":"code","ce3df7bd":"code","0f836bb0":"code","e7299308":"code","a74f18e7":"code","7ca729c2":"code","154cc155":"code","5a71e421":"code","f46e0531":"code","90f00cae":"markdown","a457eda1":"markdown","b8ec9fe1":"markdown","194ae81e":"markdown","b2d541c3":"markdown","0a6b9d66":"markdown","db8a8a6c":"markdown","02654929":"markdown","c90c38fa":"markdown","1eee64ae":"markdown","10c07f59":"markdown","117a081b":"markdown","167101cc":"markdown","87572834":"markdown","d58d0d05":"markdown","acd058c8":"markdown","4e126e38":"markdown","e0a00dd8":"markdown","129ac793":"markdown"},"source":{"04107c34":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4958e066":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sp","f055bf2d":"train=pd.read_csv('..\/input\/30daysmlchallenges\/train.csv')\ntrain.head()","aed048a0":"test=pd.read_csv('..\/input\/30daysmlchallenges\/test.csv')\ntest.head()","7afedaaa":"train.info()","10b72395":"train.shape","eaa751f7":"test.shape","432ddf13":"train.isnull().sum()","5cf50aab":"test.isnull().sum()","bb2a30d5":"train.describe().T.style.bar().background_gradient(cmap='coolwarm')","8651c12c":"test.describe().T.style.bar().background_gradient(cmap='coolwarm')","049472c9":"CAT_FEATURES = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9']\nNUM_FEATURES = ['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8',\n                'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\nALL_FEATURES = CAT_FEATURES+NUM_FEATURES","bf6f1b98":"plt.figure(figsize=(20,8))\nplt.subplots_adjust(hspace=0.5, wspace=0.5)\n#sns.set_palette(\"Spectral\")\nfor i, col in enumerate(CAT_FEATURES):\n    plt.subplot(2, 5, i+1)\n    sns.barplot(x=col, y=\"target\", data=train,\n                estimator=lambda x: len(x) \/ len(train) * 100,\n                order=np.sort(train[col].unique()))\n    plt.title(col)\nplt.show()","f4f78a62":"plt.figure(figsize=(20,8))\nplt.subplots_adjust(hspace=0.5, wspace=0.5)\nfor i, col in enumerate(CAT_FEATURES):\n    plt.subplot(2, 5, i+1)\n    sns.barplot(x=col, y=\"cont0\", data=test,\n                estimator=lambda x: len(x) \/ len(test) * 100,\n                order=np.sort(test[col].unique()))\n    plt.title(col)\nplt.show()\n","e48bca7a":"# Pointing out categorical features\ncategoricals = [item for item in train.columns if 'cat' in item]","13d876b8":"# Dealing with categorical data using get_dummies\ndummies = pd.get_dummies(train.append(test)[categoricals])\ntrain[dummies.columns] = dummies.iloc[:len(train), :]\ntest[dummies.columns] = dummies.iloc[len(train): , :]\ndel(dummies)","ecc73a09":"# Dealing with categorical data using OrdinalEncoder (only when there are 3 or more levels)\nordinal_encoder = OrdinalEncoder()\ntrain[categoricals[3:]] = ordinal_encoder.fit_transform(train[categoricals[3:]]).astype(int)\ntest[categoricals[3:]] = ordinal_encoder.transform(test[categoricals[3:]]).astype(int)\ntrain = train.drop(categoricals[:3], axis=\"columns\")\ntest = test.drop(categoricals[:3], axis=\"columns\")","24ea5ba6":"features = train.drop(['target'], axis=1)\nX=features\ny = train['target']","254f2f83":"X_test=test.copy()","1aa820dc":"X_test.shape","b53e2b36":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=42, test_size=0.2)","d31c4481":"X_train.shape","f2faadac":"y_train.shape","39a815c3":"from sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=16, random_state=0)\nkm = KMeans(n_clusters=32, random_state=0)\n\npca.fit(X_train)\nkm.fit(pca.transform(X_train))\n\nprint(np.unique(km.labels_, return_counts=True))\n\ny_stratified = km.labels_","ce3df7bd":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nnum_trees = 100\nmodel_rf = RandomForestRegressor(n_estimators=num_trees)\nmodel_rf.fit(X_train,y_train)\npreds_valid = model_rf.predict(X_valid)\nprint(mean_squared_error(y_valid, preds_valid, squared=False))","0f836bb0":"from lightgbm import LGBMRegressor","e7299308":"lgbm_params = {'n_estimators' : 10000, 'max_depth' : 2, 'learning_rate' : 0.1, 'subsample' : 0.95, 'colsample_bytree' : 0.85, 'reg_alpha' : 30.0, 'reg_lambda' : 25.0 , 'num_leaves' : 4, 'max_bin' : 512}\nmodel_lgbm = LGBMRegressor(**lgbm_params)\nmodel_lgbm.fit(X_train, y_train)\npreds_valid = model_lgbm.predict(X_valid)\nprint(mean_squared_error(y_valid, preds_valid, squared=False))\n","a74f18e7":"from xgboost import XGBRegressor","7ca729c2":"model_xgb = XGBRegressor()\nmodel_xgb.fit(X_train, y_train)\npreds_valid = model_xgb.predict(X_valid)\nprint(mean_squared_error(y_valid, preds_valid, squared=False))","154cc155":"from mlxtend.preprocessing import minmax_scaling","5a71e421":"a1 = model_rf.feature_importances_\na2 = model_lgbm.feature_importances_\na3 = model_xgb.feature_importances_\n\nfrom sklearn.preprocessing import minmax_scale\naxis_x  = X.columns.values\naxis_y1 = minmax_scaling(a1, columns=[0])\naxis_y2 = minmax_scaling(a2, columns=[0])\naxis_y3 = minmax_scaling(a3, columns=[0])\n\n\nplt.style.use('seaborn-whitegrid') \nplt.figure(figsize=(16, 6), facecolor='lightgray')\nplt.title(f'\\nF e a t u r e   I m p o r t a n c e s\\n', fontsize=14)  \n\nplt.scatter(axis_x, axis_y1, s=20, label='Random Forest') \nplt.scatter(axis_x, axis_y2, s=20, label='Light GBM')\nplt.scatter(axis_x, axis_y3, s=20, label='XGBoost') \n\nplt.legend(fontsize=12, loc=2)\nplt.show()","f46e0531":"# Use the models to generate predictions\npred_1 = model_rf.predict(X_test)\npred_2 = model_lgbm.predict(X_test)\npred_3 = model_xgb.predict(X_test)\n\n# Make sure to check that the weights sum up to ~1 by averaging the weights later \npreds = [pred_1, pred_2, pred_3]\nweights = [0., 10000., 9.] \n\nsample_submission = pd.read_csv(\"..\/input\/30daysmlchallenges\/sample_submission.csv\")\nsample_submission.target = 0.0\n\nfor pred, weight in zip(preds, weights):\n    sample_submission.target += weight * pred \/ sum(weights)\n\nsample_submission.to_csv('submission.csv', index=False)","90f00cae":"# Train the expanded model on only the important features\nmodel_rf.fit(important_train_features, y_train);\n# Make predictions on test data\nnew_predictions = model_rf.predict(important_test_features)\n# Performance metrics\nerrors = mean_squared_error(y_valid, new_predictions, squared=False)\nprint('RMSE:', errors)\n# Calculate rmse\nrmse = np.mean(100 * (errors \/ y_valid))\n# Calculate and display accuracy\naccuracy = 100 - rmse\nprint('Accuracy:', round(accuracy, 2), '%.')","a457eda1":"X AND Y Variables","b8ec9fe1":"# list of x locations for plotting\nx_values = list(range(len(importances)))\n# Make a bar chart\nplt.bar(x_values, importances, orientation = 'vertical', color = 'r', edgecolor = 'k', linewidth = 1.2)\n# Tick labels for x axis\nplt.xticks(x_values, features, rotation='vertical')\n# Axis labels and title\nplt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');","194ae81e":"Data Preprocessing","b2d541c3":"XG BOOST","0a6b9d66":"FEATURE IMPORTANCE","db8a8a6c":"Random Forest\u00b6\n","02654929":"# Get numerical feature importances\nimportances = list(model_rf.feature_importances_)\n\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(features, importances)]\n\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]","c90c38fa":"Import the Libraries","1eee64ae":"PCA","10c07f59":"#label encoding for categorical data\nfrom sklearn.preprocessing import (StandardScaler ,LabelEncoder,OneHotEncoder, OrdinalEncoder)\n\ncat_features = [feature for feature in train.columns if 'cat' in feature]\nfor col in cat_features:\n    encoder = LabelEncoder()\n    for col in cat_features:\n        train[col] = encoder.fit_transform(train[col])\n        test[col] = encoder.transform(test[col])","117a081b":"FEATURE TRANSFORMATION","167101cc":"MODEL ALGORITHMS\u00b6\n","87572834":"# Extract the names of the most important features\nimportant_feature_names = [feature[0] for feature in feature_importances[0:17]]\n# Create training and testing sets with only the important features\nimportant_train_features = X_train[important_feature_names]\nimportant_test_features = X_valid[important_feature_names]\n# Sanity check on operations\nprint('Important train features shape:', important_train_features.shape)\nprint('Important test features shape:', important_test_features.shape)","d58d0d05":"plt.rcParams[\"axes.labelsize\"] = 12\nrf_prob_train = model_rf.predict(X_train) - y_train\nplt.figure(figsize=(6,6))\nsp.stats.probplot(rf_prob_train, plot=plt, fit=True)\nplt.title('Train Probability Plot for Random Forest', fontsize=10)\nplt.show()\n\nrf_prob_test = model_rf.predict(X_valid) - y_valid\nplt.figure(figsize=(6,6))\nsp.stats.probplot(rf_prob_test, plot=plt, fit=True)\nplt.title('Test Probability Plot for Random Forest', fontsize=10)\nplt.show()","acd058c8":"Load the dataset","4e126e38":"SUBMISSION CSV","e0a00dd8":"LGBM","129ac793":"TRAIN TEST SPLIT"}}