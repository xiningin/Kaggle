{"cell_type":{"2f4fe2f2":"code","150a81cc":"code","a2e4eae9":"code","8e719d26":"code","d2b22b97":"code","57082949":"code","00c95ab6":"code","89413062":"code","ef4a88d5":"code","8bad0eea":"code","78190ce1":"code","3bfb0ea7":"code","9173637d":"code","0b38aa0a":"code","2f92bbe2":"code","4ca645a3":"code","84a9fe20":"code","c79944a8":"code","be8e9ef0":"code","8e99d4d9":"code","2871f693":"code","95264250":"code","c5549176":"code","30e23609":"code","b61ba69e":"code","20e2a545":"code","68344f53":"code","b5035248":"code","9dc1a7b8":"code","523b6451":"code","e7dabacf":"markdown","afbac44a":"markdown","8a6cecab":"markdown","a47535db":"markdown","6ff6f110":"markdown","bfeddd41":"markdown","bb56d1f3":"markdown"},"source":{"2f4fe2f2":"import numpy as np\nimport pandas as pd\nimport sqlite3\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sn","150a81cc":"#Using sqlite3 to read the data\ncon = sqlite3.connect('..\/input\/database.sqlite')","a2e4eae9":"#Filtering positive(5 & 4 stars) and negative(1 & 2 stars) reviews and discarding 3 star reviews.\nfiltered_data = pd.read_sql_query(\"\"\"\nSELECT *\nFROM Reviews\nWHERE Score != 3\n\"\"\", con)","8e719d26":"#Give reviews greater than 3 positive and less than 3 as negative\ndef partition(x):\n    if x < 3:\n        return 'negative'\n    return 'positive'","d2b22b97":"#Changing the reviews based on the star value\nactualScore = filtered_data[\"Score\"]\npositiveNegative = actualScore.map(partition)\nfiltered_data['Score'] = positiveNegative","57082949":"filtered_data.shape","00c95ab6":"filtered_data.head()","89413062":"display = pd.read_sql_query(\"\"\"\nSELECT *\nFROM Reviews\nWHERE Score != 3 AND UserId = \"AR5J8UI46CURR\"\nORDER BY ProductID\n\"\"\", con)\ndisplay","ef4a88d5":"#Sorting data according to ProductId in ascending order\nsorted_data = filtered_data.sort_values('ProductId', axis=0, ascending=True)","8bad0eea":"#Deduplication of entries\nfinal = sorted_data.drop_duplicates(subset={\"UserId\", \"ProfileName\", \"Time\", \"Text\"}, keep=\"first\", inplace=False)","78190ce1":"final.shape #If we comare this with the size of input data, we can see that there is a significat reduction. Size of the read data was final.shape #If we comare this with the size of input data, we can see that there is a significat reduction. Size of the read data was (525814, 10) ","3bfb0ea7":"#Lets check how much data is left after removing duplicates\n(final['Id'].size*1.0)\/(filtered_data['Id'].size*1.0)*100","9173637d":"#In reviews the helpful numerator has to be greater that helpful denominator. But there are some reviews that has a probelm with this\ndisplay = pd.read_sql_query(\"\"\"\nSELECT *\nFROM Reviews\nWHERE Score != 3 AND Id = 44737 OR Id = 64422\nORDER BY ProductID\n\"\"\", con)\ndisplay","0b38aa0a":"final = final[final.HelpfulnessNumerator <= final.HelpfulnessDenominator]","2f92bbe2":"#We have this many reviews left\nfinal.shape","4ca645a3":"#Number of positive and negative reviews in dataset\nfinal['Score'].value_counts()","84a9fe20":"import scipy\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()","c79944a8":"final_counts = count_vect.fit_transform(final['Text'].values)","be8e9ef0":"type(final_counts) #Here the final count is a sparse matrix","8e99d4d9":"final_counts.get_shape()","2871f693":"import re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\n\nfrom tqdm import tqdm\nimport os","95264250":"stop = set(stopwords.words('english'))\nsno = SnowballStemmer('english')","c5549176":"print(stop)","30e23609":"print(sno.stem('tasty'))","b61ba69e":"print(sno.stem('worked'))","20e2a545":"def cleanHtml(sentence):\n    cleanr = re.compile('<.?>')\n    cleartext = re.sub(cleanr, ' ', sentence)\n    return cleartext\ndef cleanPunc(sentence):\n    cleaned = re.sub(r'[?|!|\\'|\\\"|#]', r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|\/]', r'',cleaned)\n    return cleaned","68344f53":"#removing html and punctuations\ni = 0\nstr1 = ' '\nfinal_string = []\nall_positive_words=[] #store positive reviews\nall_negative_words=[] #store negative reviews\ns = ''\n\nfor sent in final['Text'].values:\n    filtered_sentence=[]\n    sent = cleanHtml(sent)\n    for w in sent.split():\n        for cleaned_word in cleanPunc(w).split():\n            if((cleaned_word.isalpha()) & (len(cleaned_word)>2)):\n                if(cleaned_word.lower() not in stop):\n                    s=(sno.stem(cleaned_word.lower())).encode('utf8')\n                    filtered_sentence.append(s)\n                    if(final['Score'].values)[i] == 'positive':\n                        all_positive_words.append(s)\n                    if(final['Score'].values)[i] == 'positive':\n                        all_negative_words.append(s)\n                else:\n                    continue\n            else:\n                continue\n    str1=b\" \".join(filtered_sentence)\n    final_string.append(str1)\n    i+=1\n","b5035248":"final['CleanedText'] = final_string","9dc1a7b8":"final.head(3)","523b6451":"conn = sqlite3.connect('final.sqlite')\nc=conn.cursor()\nconn.text_factory = str\nfinal.to_sql('Reviews', conn, schema=None, if_exists='replace')","e7dabacf":"#### There can be so many duplicte values in the data that we get. So its extremely important to remove the duplicates. If we feed garbage data we get garbage results. Example garbage data is shown below.","afbac44a":"## Reading Data","8a6cecab":"## Cleaning Data","a47535db":"# Amazon Food Review Study","6ff6f110":"## Bag of Words(BoW)","bfeddd41":"### Now lets remove the duplicate values","bb56d1f3":"## Importing basic modules"}}