{"cell_type":{"a899af71":"code","db66b903":"code","45404887":"code","fe2a3c24":"code","9ca82131":"code","0407f8d7":"code","e1c7b9bd":"code","80c35098":"code","b78979e6":"code","96c55663":"code","48c48919":"code","0e89b428":"code","c076ac19":"code","062e1210":"code","154fb423":"code","a5d3de87":"code","5ecb82ab":"code","95251aa1":"code","b9efe68e":"code","91a17879":"code","f41aad26":"code","4cc67f94":"code","05cb60ee":"code","68fca078":"code","72bf2099":"code","98018f62":"code","047c083a":"code","78203fa4":"code","c166ffb5":"code","37587a32":"code","b3147d47":"code","ee98f67d":"code","485d8626":"code","0c49fa4b":"code","0c241df0":"code","3958397a":"code","9ba3a342":"code","5ed26677":"code","15f0914a":"code","5f9a42b2":"code","40a2869b":"code","0f3c2173":"code","10737cf2":"code","6b4b21da":"code","df660f51":"code","bbfa7366":"code","bc10bf7e":"code","ff54a4c7":"code","680ea77d":"code","e248f81b":"code","79b6334f":"code","27922447":"code","1c6fe84b":"code","7b024acd":"markdown","16aeff26":"markdown","daf78165":"markdown","5468d895":"markdown","47f10667":"markdown","123af107":"markdown","ba3a7650":"markdown","964a7617":"markdown","226f3876":"markdown","4500ffe8":"markdown","b7867022":"markdown","e3a84ca8":"markdown","70c10672":"markdown","39535fe3":"markdown","a678e9ff":"markdown"},"source":{"a899af71":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","db66b903":"# reading train data\ndf = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\n\n# data shape\ndf.shape","45404887":"#unique data types\nprint(df.dtypes.unique())","fe2a3c24":"# exclude all the object type columns\nn_df = df.select_dtypes(exclude='object')\nprint(n_df.shape)","9ca82131":"# describe data\nprint(n_df.describe())","0407f8d7":"# data spliting\nX = n_df.copy()\ny = X.pop('SalePrice')\nprint(X.shape)\nprint(y.shape)","e1c7b9bd":"# count the missing values\nprint(X.isnull().sum())","80c35098":"# let's plot 'LotFrontage'\nf, ax = plt.subplots(figsize=(20, 2))\nsns.heatmap(X.corr().iloc[2:3,:], annot=True, linewidths=.8, fmt= '.1f',ax=ax)\nplt.show()","b78979e6":"# plotting \nsns.kdeplot(X.LotFrontage, label='LotFrontage', shade=True)\n# let's look details\nX.LotFrontage.describe()","96c55663":"X['LotFrontage'].replace({np.nan:X.LotFrontage.mean()}, inplace=True)\nprint(X.LotFrontage.isnull().sum())","48c48919":"# correaltion with GrarageYrBlt\n# print(X.GarageYrBlt.)\nf, ax = plt.subplots(figsize=(20,2))\nsns.heatmap(X.corr().iloc[25:26, :], annot=True, linewidths=0.8, fmt='.1f', ax=ax)\nplt.show()","0e89b428":"# let's describe\nprint(X.GarageYrBlt.describe())\n#plotting\nsns.kdeplot(X.GarageYrBlt, Label='GarageYrBlt', cbar=True, shade=True)","c076ac19":"# fill missing value\nX['GarageYrBlt'].replace({np.nan:X.GarageYrBlt.median()}, inplace=True)\nX.GarageYrBlt.isnull().sum()","062e1210":"#let's see how impact on SalePrice\nf, ax = plt.subplots(figsize=(20, 2))\nsns.heatmap(X.corr().iloc[8:9,:], annot=True, linewidths=.8, fmt='.1f',ax=ax)\nplt.show()","154fb423":"#plotting\nsns.kdeplot(X.MasVnrArea, label='MasVnrArea', cbar=True, shade=True)\n# describe\nprint(X.MasVnrArea.describe())","a5d3de87":"#replacing\nX['MasVnrArea'].replace({np.nan:0}, inplace=True)\nX.MasVnrArea.isnull().sum()","5ecb82ab":"#let's check there is any missing value in X\nX.isnull().sum()","95251aa1":"# removing Id, since It won't be used for the prediction.\nidx = X.pop('Id')\nX.shape","b9efe68e":"f, ax = plt.subplots(figsize=(20, 2))\nsns.heatmap(X.corr().iloc[35:36, :], annot=True, linewidths=.8, fmt='.1f', ax=ax)\nplt.show()","91a17879":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n#choosing k=30 best features\nbest_fit = SelectKBest(chi2, k=30).fit(X, y)\ndfScores = pd.DataFrame(best_fit.scores_)\ndfColumns = pd.DataFrame(X.columns)\n\nfeatures_score = pd.concat([dfColumns, dfScores], axis=1)\nfeatures_score.head()","f41aad26":"#naming columns of features_score\nfeatures_score.columns = ['Class', 'Score']\nfeatures_score.head()","4cc67f94":"#print top 30 class according to score\nprint(features_score.nlargest(30, 'Score'))\nfeatures_score.shape","05cb60ee":"#final features\nfeatures = list(features_score.Class[:30])\nprint(len(features))","68fca078":"# update X data frame with features\nX = X[features]\nX.shape","72bf2099":"from sklearn import preprocessing\nX = preprocessing.StandardScaler().fit(X).transform(X)","98018f62":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=4)\nforest_model = RandomForestRegressor(random_state=1)\nforest_model.fit(X, y)\npredictions = forest_model.predict(val_X)\nprint(mean_absolute_error(val_y, predictions))","047c083a":"from sklearn import linear_model\nregr = linear_model.LinearRegression()\nregr.fit(X, y)\nregr_preds = regr.predict(val_X)\nprint(mean_absolute_error(val_y, regr_preds))\n","78203fa4":"test_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest_data.head()","c166ffb5":"#igonre object data type\ntest_X = test_data.select_dtypes(exclude='object')\ntest_X = test_X.copy()\ntest_X.shape","37587a32":"# count missing value for each column\nprint(test_X.isnull().sum())","b3147d47":"# for 'GarageYrBlt', 'MasVnrArea' and 'LotFrontage' doing the same for test data as i have done for train data \n\ntest_X['GarageYrBlt'].replace({np.nan:test_X.GarageYrBlt.mean()}, inplace=True)\ntest_X['MasVnrArea'].replace({np.nan:0}, inplace=True)\ntest_X['LotFrontage'].replace({np.nan:test_X.LotFrontage.mean()}, inplace=True)","ee98f67d":"# count again missing values\nprint(test_X.isnull().sum())","485d8626":"## 1. BsmtFinSF1\nprint(test_X.BsmtFinSF1.describe())\nsns.kdeplot(test_X.BsmtFinSF1, label='BsmtFinSF1', cbar=True, shade=True)\n","0c49fa4b":"# since 50% value close to mean, I'm replacing with mean\ntest_X['BsmtFinSF1'].replace({np.nan:test_X.BsmtFinSF1.mean()}, inplace=True)\n","0c241df0":"## 2. BsmtFinSF2\ntest_X.BsmtFinSF2.describe()","3958397a":"#since 75% value is zero, so, I'm replacing with zero\ntest_X['BsmtFinSF2'].replace({np.nan:0}, inplace=True)","9ba3a342":"## 3. BsmtUnfSF\n\nprint(test_X.BsmtUnfSF.describe())\nsns.kdeplot(test_X.BsmtUnfSF, label='BsmtUnfSF', cbar=True, shade=True)","5ed26677":"# replacing with 50% value (median value)\ntest_X['BsmtUnfSF'].replace({np.nan:test_X.BsmtUnfSF.median()}, inplace=True)","15f0914a":"## 4. TotalBsmtSF\nprint(test_X.TotalBsmtSF.describe())\nsns.kdeplot(test_X.TotalBsmtSF, label='TotalBsmtSF', cbar=True, shade=True)","5f9a42b2":"#replacing with median value\ntest_X['TotalBsmtSF'].replace({np.nan:test_X.TotalBsmtSF.median()}, inplace=True)","40a2869b":"## 5. BsmtFullBath\nprint(test_X.BsmtFullBath.describe())\nsns.kdeplot(test_X.BsmtFullBath, label='BsmtFullBath', cbar=True, shade=True)\n","0f3c2173":"# replacing with mean\/median, I'm using mean\ntest_X['BsmtFullBath'].replace({np.nan:test_X.BsmtFullBath.mean()}, inplace=True)","10737cf2":"print(test_X.BsmtHalfBath.describe())\n#since 75% values are zero, so I'm replacing with zero\n\ntest_X['BsmtHalfBath'].replace({np.nan:0}, inplace=True)","6b4b21da":"## 7. GarageCars\nprint(test_X.GarageCars.describe())\nsns.kdeplot(test_X.GarageCars, label='GarageCars', cbar=True, shade=True)","df660f51":"#replacing with median\ntest_X['GarageCars'].replace({np.nan:test_X.GarageCars.median()}, inplace=True)","bbfa7366":"## 8. GarageArea\nprint(test_X.GarageArea.describe())\nsns.kdeplot(test_X.GarageArea, label='GarageArea', cbar=True, shade=True)","bc10bf7e":"#replacing with mean value\ntest_X['GarageArea'].replace({np.nan:test_X.GarageArea.mean()}, inplace=True)","ff54a4c7":"# test data correlation\nf, ax = plt.subplots(figsize=(20, 10))\nsns.heatmap(test_X.corr(), annot=True, linewidths=.8, fmt='.1f', ax=ax)\nplt.show()","680ea77d":"test_X = test_X[features]\ntest_X.isnull().sum()","e248f81b":"test_X = preprocessing.StandardScaler().fit(test_X).transform(test_X)","79b6334f":"test_predictions = forest_model.predict(test_X)","27922447":"# output for submission\noutput = pd.DataFrame({'Id': test_data.Id, 'SalePrice': test_predictions})\noutput.head()","1c6fe84b":"# output to csv\noutput.to_csv('submission.csv', index=False)","7b024acd":"As we seen that 'LotFrontage' highly correlated with 'LotArea', '1stFlrSF', 'GrLivArea'. So, It needs to be filled all the missing values.","16aeff26":"# pre processing","daf78165":"# Model Selection","5468d895":"## Data Normalization","47f10667":"# Feature Selection","123af107":"50% values are near to the mean value, so replacing missing value with median","ba3a7650":"50% values are near to the mean value, so replacing missing value with mean value would be better i think.","964a7617":"test data normalization","226f3876":"# Handling Missing Values","4500ffe8":"# Working with Test Data ","b7867022":"# Data Source Access","e3a84ca8":"# Data Analysis","70c10672":"since, 50% values are 0, so replacing missing values with zero.","39535fe3":"# Linear Regression","a678e9ff":"## Random Forest"}}