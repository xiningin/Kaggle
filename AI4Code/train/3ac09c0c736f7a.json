{"cell_type":{"bb40073c":"code","dad37e94":"code","023a483b":"code","bd7c3d2b":"code","2e2329bd":"code","32b612e2":"code","225d90af":"code","21e1d4ee":"code","64a272fb":"code","a859c8c7":"code","56255bb9":"code","73e7845d":"code","6e6fe804":"code","66758aef":"code","84ec87af":"code","5a715844":"code","2d808d5a":"code","961500ad":"code","32ba813b":"code","37124bfb":"code","9b72dd30":"code","6a777927":"code","1ddaae65":"code","f1b9e43e":"code","3888c173":"code","d586b8e7":"code","f0382ae1":"markdown","83940181":"markdown","574f180d":"markdown","83019293":"markdown","0d4c7c85":"markdown","2b7b4bb1":"markdown","d80028fb":"markdown","97dc7d0c":"markdown","42990265":"markdown","2127aec3":"markdown","6fc9d97c":"markdown","01d75397":"markdown","a5e7825c":"markdown","3bbbc38f":"markdown","be3d76b7":"markdown","9d523055":"markdown","fbf7fe96":"markdown","dde704fd":"markdown"},"source":{"bb40073c":"import numpy as np                 # linear algebra\nimport pandas as pd                # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt    # graphs\nimport seaborn as sns; sns.set()   # pretty visualisations\n\n# load the dataset\ndf = pd.read_csv('..\/input\/creditcard.csv')","dad37e94":"# manual paramaters\nTEST_RATIO = 0.2\nRANDOM_SEED = 777","023a483b":"# let's quickly convert the columns to lower case and rename the Class column \n# so as to not cause syntax errors\ndf.columns = map(str.lower, df.columns)\ndf.rename(columns={'class': 'label'}, inplace=True)\n\n# print first 5 rows to get an initial impression of the data we're dealing with\ndf.head()","bd7c3d2b":"label_counts = df.label.value_counts()\nclean, fraud = label_counts[0], label_counts[1]\ntotal = len(df)\nfraud_perc = round(fraud \/ total * 100, 3)\n\nprint(label_counts)\nprint(f'Fraud represents {fraud_perc}% of the total dataset')","2e2329bd":"# distribution of amounts by label\nprint(df.groupby('label').agg({'amount': ['median', 'mean', 'max']}))\ndf.amount.hist(by=df.label, bins=20);","32b612e2":"# add calculated field\n\"\"\" Add a negligible amount to avoid a RuntimeWarning: divide by zero encountered in log10 \"\"\"\ndf['log10_amount'] = np.log10(df.amount + 0.00001)\n\n# keep the label field at the back\ndf = df[\n    [col for col in df if col not in ['label', 'log10_amount']]\n    + ['log10_amount', 'label']\n]\n\n# plot distribution \ndf.log10_amount.hist(by=df.label, bins=7, range=(-2,5), density=True);","225d90af":"# average cash gained if 1 more fraud is successfully flagged\navg_fraud_cost = round(np.mean(df.loc[df.label==1].amount.values), 1)\n\nprint(f'The average cost per uncaught fraudulent transaction is \u20ac {avg_fraud_cost}')","21e1d4ee":"# average amount of transactions processed per hour\nAVG_TRANSACTIONS_PER_HOUR = 12\n\n# source: https:\/\/www.ofx.com\/en-au\/forex-news\/historical-exchange-rates\/yearly-average-rates\/\ngbp_eur_avg_rate_2013 = 1.177964\n\n# source: https:\/\/stats.oecd.org\/Index.aspx?DataSetCode=AV_AN_WAGE\nwages_2013 = {\n    'united kingdom': 33086 * gbp_eur_avg_rate_2013,\n    'belgium': 42838,\n    'netherlands': 45275,\n    'france': 36044,\n    'ireland': 44236,\n    'luxembourg': 59567,\n    'germany': 36572\n}\n\n# average wage\nyearly_wages = list(wages_2013.values())\navg_yearly_wage = np.mean(yearly_wages)\navg_hourly_wage = avg_yearly_wage \/ 52 \/ 40 # weeks, hours\nprint(f\"\"\"The average 2013 yearly wage across our selection of European financials hubs \nwas around \u20ac {avg_yearly_wage:,.2f}\\n\"\"\")\n\n# conclusion\navg_monitoring_cost = avg_hourly_wage \/ AVG_TRANSACTIONS_PER_HOUR\n\nprint(f'By extension, that means the average hourly wage was \u20ac {avg_hourly_wage:.2f}\/h')\nprint(f'That makes the average cost per transaction monitored \u20ac {avg_monitoring_cost:.2f}\/h')","64a272fb":"# visualising the marginal cost function\ncount_of_false_positives = np.arange(0, avg_fraud_cost \/ avg_monitoring_cost * 2)\nmarginal_cost_function = count_of_false_positives * -avg_monitoring_cost + avg_fraud_cost\nintersect = int(avg_fraud_cost \/ avg_monitoring_cost)\n\nplt.title('marginal gain per extra fraud caught')\nplt.ylabel('amount in \u20ac')\nplt.xlabel('count of false positives')\nplt.plot(marginal_cost_function)\nplt.plot(intersect, 0, 'ro')\nplt.show();\n\nprint(f'After {intersect} false positives, it wasn\\'t worth catching the extra fraud.')","a859c8c7":"fraud = len(df.loc[df.label == 1])\nprint(f'Reimbursing all fraud would on average cost ~ \u20ac {round(fraud\/2 * avg_fraud_cost,1):,} \/ day')","56255bb9":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report","73e7845d":"# splitting our label classification from our dataset and dropping redundant columns\nfeatures = df.drop(['time', 'amount', 'label'], axis=1)\nlabels = df.label\ncolumn_names = list(features.columns)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    features.values, \n    labels.values,\n    shuffle=True,\n    stratify=labels.values,\n    test_size=TEST_RATIO,\n    random_state=RANDOM_SEED\n) ","6e6fe804":"# Bundling the costs in a dictionary for use in our functions\ncosts = {\n    'avg_fraud_cost': avg_fraud_cost,\n    'avg_monitoring_cost': avg_monitoring_cost,\n    'test_ratio': TEST_RATIO\n}    \n\ndef cost_function(fp, fn, costs):\n    return fp * costs['avg_monitoring_cost'] + fn * costs['avg_fraud_cost']\n\ndef misclassification_cost(classification_matrix, costs):\n    false_negatives = classification_matrix[1,0]\n    false_positives = classification_matrix[0,1]\n\n    cost = cost_function(fp=false_positives, fn=false_negatives, costs=costs)\n    \n    validation_cost = f'\\nThe misclassification cost on this validation set \\\n({TEST_RATIO * 100}% of data) is \u20ac {cost:,.2f}'\n    estimation_cost = f'\\nExtrapolated to an entire day\\'s worth of transactions \\\n(\/{TEST_RATIO}\/2 days), we get ~ \u20ac {cost\/TEST_RATIO\/2:,.2f}'\n    \n    return validation_cost + estimation_cost\n\ndef evaluation_report(ground_truth, predicted, costs):\n    report = classification_report(ground_truth, predicted)\n    conf_matrix = confusion_matrix(ground_truth, predicted)\n    cost = misclassification_cost(conf_matrix, costs)\n    \n    return f'{report}\\n{conf_matrix}\\n{cost}'","66758aef":"# lbfgs will be the new default solver\n# specified as argument to supress warning message\nLr = LogisticRegression(solver='lbfgs') \n\n# fit & Predict\nLr.fit(X_train, y_train)\ny_pred = Lr.predict(X_test)\n\n# evaluate our model\nprint(evaluation_report(y_test, y_pred, costs))","84ec87af":"# instantiate regressor\nLr = LogisticRegression(solver='lbfgs', class_weight='balanced')\n\n# fit & Predict\nLr.fit(X_train, y_train)\ny_pred = Lr.predict(X_test)\n\n# evaluate our model\nprint(evaluation_report(y_test, y_pred, costs))","5a715844":"# custom weight\nfraud_weight = round(avg_fraud_cost \/ avg_monitoring_cost, 1)\nprint(f'The weight we\\'ve attributed to fraud is: {fraud_weight} to 1\\n')\n\n# instantiate regressor\nLr_Weight = LogisticRegression(solver='lbfgs', class_weight={\n    1: fraud_weight,\n    0: 1\n})\n\n# fit & Predict\nLr_Weight.fit(X_train, y_train)\ny_pred = Lr_Weight.predict(X_test)\n\n# evaluate our model\nprint(evaluation_report(y_test, y_pred, costs))","2d808d5a":"def plot_data(X, y, title='', name=\"graph.png\"):\n    plt.figure(figsize=(16,8));\n    \n    # clean\n    plt.scatter(\n        X[y == 0, 12], X[y == 0, 11], \n        label='clean', \n        alpha=0.2, \n        linewidth=0.05, \n        color='#9db68c' # green\n    );\n    \n    # fraud\n    plt.scatter(\n        X[y == 1, 12], X[y == 1, 11],\n        label='fraud',\n        alpha=0.5,\n        linewidth=0.05, \n        color='#db5e5e' # red\n    );\n    \n    plt.title(title);\n    plt.legend(loc='best');\n    plt.savefig(name);","961500ad":"plot_data(X_train, y_train,\n          title='Before SMOTE:',\n          name='before_smote.png')","32ba813b":"fraud_in_train_set = y_train.sum()\nprint(f'There are {fraud_in_train_set} fraud transactions in the training set')","37124bfb":"from imblearn.over_sampling import SMOTE\n\n# Define the resampling method\nresampler = SMOTE(\n    random_state=RANDOM_SEED,\n    \n    # synthetically create a multiple of fraud observations currently in the training set\n    sampling_strategy={1: int(fraud_in_train_set * fraud_weight)}\n)\n\n# Create the resampled feature set\nX_train_resampled, y_train_resampled = resampler.fit_sample(X_train, y_train)","9b72dd30":"print(f'There are now {y_train_resampled.sum():,} fraud transactions in the oversampled training set')","6a777927":"plot_data(X_train_resampled, y_train_resampled,\n          title='After SMOTE:',\n          name='after_smote.png')","1ddaae65":"# instantiating & training\nLr = LogisticRegression(solver='lbfgs')\nLr.fit(X_train_resampled, y_train_resampled)\n\n# predict on our previous test-set so we have comparable results\ny_pred = Lr.predict(X_test)\n\n# evaluate our model\nprint(evaluation_report(y_test, y_pred, costs))","f1b9e43e":"def cost_per_threshold(sklearn_classifier, X, y, costs):\n    \"\"\" return four lists:\n            1) list of thresholds (defined in this function) \n            2) count of false negatives (uncaught fraud)\n            3) count of false positives (overreported transactions)\n            4) total cost per threshold iteration\n    \"\"\"\n    \n    # shorter variable\n    clf = sklearn_classifier\n    \n    # lists to be passed to plot\n    thresholds = np.arange(start=0.05, stop=1, step=0.025)\n    fn, fp = [], []\n\n    # looping through thresholds to plot cost curve\n    for threshold in thresholds:\n\n        # custom threshold\n        y_pred = (clf.predict_proba(X)[:,1] > threshold).astype(int)\n\n        # extracting performance indicators\n        false_positives = confusion_matrix(y, y_pred)[0,1]\n        false_negatives = confusion_matrix(y, y_pred)[1,0]\n\n        # add to lists\n        fp.append(false_positives)\n        fn.append(false_negatives)\n\n    # convert to numpy arrays\n    fp = np.asarray(fp)\n    fn = np.asarray(fn)\n    c = cost_function(fp, fn, costs)\n    \n    # return values\n    return thresholds, fp, fn, c","3888c173":"def plot_misclassification(thresholds, fp, fn, c):\n    \n    # two y-axes sharing the same plot\n    fig, (left, right) = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n\n    \"\"\" LEFT PLOT \"\"\"\n    left.title.set_text('Misclassification Counts')\n\n    # 1st y-axis\n    color = 'tab:orange'\n    left.set_xlabel('threshold')\n    left.set_ylabel('false positives', color=color)\n    left.plot(thresholds, fp, color=color)\n    left.tick_params(axis='y', labelcolor=color)\n\n    # 2nd y-axis\n    left_2nd_y = left.twinx() # share the same x-axis\n    color = 'tab:red'\n    left_2nd_y.set_ylabel('false negatives', color=color) \n    left_2nd_y.plot(thresholds, fn, color=color)\n    left_2nd_y.tick_params(axis='y', labelcolor=color)\n\n\n    \"\"\" RIGHT PLOT \"\"\"\n    right .title.set_text('Misclassification Costs')\n\n    # plotting costs\n    color = 'tab:blue'\n    right.set_xlabel('threshold')\n    right.set_ylabel('total (\u20ac)', color=color)\n    right.plot(thresholds, c, color=color)\n    right.tick_params(axis='y', labelcolor=color)\n\n    \n    \"\"\" ANNOTATE \"\"\"\n    # calculate minimum cost threshold\n    optimal_threshold = min(zip(thresholds, c), key=lambda t: t[1])\n    optimal_threshold_index = np.where(thresholds==optimal_threshold[0])[0] \n    optimal_threshold_fp = fp[optimal_threshold_index]\n    optimal_threshold_fn = fn[optimal_threshold_index]\n    \n    right.plot(*optimal_threshold, 'go');\n    \n    # display cost optimum\n    print(f'The cost minimum occurs at threshold {optimal_threshold[0]:.3f}, where the misclassification are as follows')\n    print(f'    False positives: {optimal_threshold_fp}')\n    print(f'    False negatives: {optimal_threshold_fn}')\n    print('')\n    print(f'Misclassification costs on test sample: {optimal_threshold[1]:,.2f}')\n    print(f'Extrapolated daily cost: {optimal_threshold[1]\/TEST_RATIO\/2:,.2f}')\n    \n    \"\"\" DISPLAY \"\"\"\n    # prevent clipping and overlap between graphs\n    fig.tight_layout()  \n    plt.show();","d586b8e7":"# compute results at each threshold level\nthresholds, fp, fn, c = cost_per_threshold(Lr, X_test, y_test, costs)\n\n# visualise them\nplot_misclassification(thresholds, fp, fn, c)","f0382ae1":"# Conclusion\n\nThe purpose of this notebook is twofold:\n\n1. highlight the fact that **choosing the right performance metric** is important when \n    * **classes are unbalanced** \n    * the **consequences of misclassification (type I & II) are asymmetrical**\n    \n    \n2. to show that **machine learning algorithms can be highly effective at calibrating the rate of false negatives vs false positives, thereby optimizing a cost function** - more so than rule-based systems.\n\nIf you found this kernel helpful, or appreciated the alternative angle of approaching this dataset from a business perspective, please upvote it.\n","83940181":"# Cost of monitoring","574f180d":"The results are similar. We shouldn't be overly surprised by this, of course. After all, **SMOTE was configured to multiply the amount of fraud samples we already had by the fraud_weight we had found as a result of our cost comparison**.\n\nIt does seem that **SMOTE slightly outperforms a simple configuration of the LogisticRegression's class_weight parameter when it comes to catching fraud**. The synthetic imputation allowed our model to generalize just a little bit better than simply training on the same data points over a multiple of iterations. We would have to cross-validate to check this.\n\n# Finetuning Thresholds\nTo further improve, we can look at the computed class **probabilities** of a classifier and **shift the threshold** at which we deem a case to be fraud. This allows us to further tune the **cost-efficiency** of our model.","83019293":"### Avoid data leakage\nNote that we **resample only the training set**, so as to avoid leaking test data to the new model trained on the oversampled set.\nAfter all, **oversampling changes the shape of the dataset, so future train_test_split() wouldn't produce the same distinctly isolated sets**, even with the same random_state parameter. We definitely want to avoid training on data we then test on.","0d4c7c85":"Clearly, this solves our cost function much better. We have reduced the costs associated with fraud detection by almost an order of magnitude.\n\nAlthough we have a couple of extra cases of fraud slipping through, we make up for it by saving tremendously on monitoring costs.\n\n**It's now up to us to improve the model** across the board. ","2b7b4bb1":"## Resulting ratio\nAt ~ \u20ac 1.75 vs ~ \u20ac 122 per transaction, respectively, the **cost ratio between type I and II errors is roughly [1:70]**.\n\nIn other words, **reducing the false negative rate (catching a fraud transaction) is 70 times as important as reducing the false positive rate**. However, it also means that it's **only worth reducing our false negative rate** if the **marginal cost** of doing so is **below an extra 70 transactions falsely flagged as fraud**.","d80028fb":"# Cost of fraud","97dc7d0c":"Better recall, as we're catching more fraud, but we're losing out on the surplus due to a slew of transactions we now have to investigate.\n\n## Custom class weight\nLet's apply the same method, but pass onto the model the cost ratio we have calculated prior.","42990265":"### Reading a confusion matrix\nTo keep this already lengthy notebook a bit more comptact, we'll be printing out the confusion matrices.\nThat means they'll be unlabeled, though, so here's a quick refresher.\n![image.png](attachment:image.png)","2127aec3":"It's already much better than doing nothing... but **recall isn't great** at this point. Of course, we haven't set up the model to prioritise it.\nWe can attribute **different weights to the classes** to solve for this.\n\n>https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html    \n>The \u201cbalanced\u201d mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples \/ (n_classes * np.bincount(y)). <br>\n","6fc9d97c":"# Baseline Model\nLet's blindly apply a **logistic regression model** for us as a **benchmark** for future iterations. <br>\nWe should be able to get away with this reasonably well, since the dataset has been preprocessed already (anonymised through PCA, it seems).","01d75397":"# SMOTE\n\nThe *Synthetic Minority Oversampling TEchnique* is a promising way to feed the model more fraud data, albeit artificially. <br>\nA great blog post on the topic: http:\/\/rikunert.com\/SMOTE_explained","a5e7825c":"As is usually the case with monetary amounts, the distribution is extremely skewed with a long tail. <br>\nSimilar things happen when analysing salaries, for example. <br> \nAn alternative is to take the log values, instead, and return the (usually) normally distributed equivalent.","3bbbc38f":"# The business problem\nA perfect model automatically captures all the fraud and flags none of the clean transactions, with no costs incurred.\nThis is obviously utopic, and the exercise is one of **finding the optimal minimum and intersect between two cost functions**: \n\n1. the **cost of fraud**\n    * = false negative (type II error)\n    * cost of reimbursement or cost of insurance premiums protecting you against uncaught fraud cases\n    * we will not consider intangibles like reputational loss\n2. the **cost of human labour** for investigating the flagged transactions\n    * = false positive (type I error)\n    * employee(s) salary \n    * guesstimates will have to be made as to how many transactions an employee could realistically investigate","be3d76b7":"Although the most common fraud attempts seem to be on tiny amounts, we see a more even distribution. \nFraudsters seem to try and steal higher amounts on average. \n\nWhen determining the weight for our type I and II errors, we have to keep the dataset's description in mind:\n> The datasets contains transactions made by credit cards in September 2013 by european cardholders. <br>\n> This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. \n\nWe make the following assumptions:\n\n* **representativeness of the dataset**\n    * the extract is only two days' worth of transactions, and it's all we have, so we have to simply assume that it is an accurate representation of \"business as usual\"\n    * in other words, we assume that the average fraud cost we calculated is not some (low) outlier, compared to data covering larger time spans \n* **cost of fraud**\n    * the average accounts for the occasional, worst-case scenario outlier better than the median, and is thus our measure of choice to determine the cost of Type II errors\n* **cost of monitoring**\n    * we assume no low-wage outsourcing, and expect this type of oversight\/monitoring activity would most likely be done from a European financial hub\n      (e.g. London, Brussels, Amsterdam, Paris, Dublin, Luxembourg, Frankfurt)\n    * take the average gross pay in 2013 across these cities' respective countries\n    * a company capable of implementing machine learning in production surely has the technical capability to build a streamlined transaction monitoring software stack. \n      let's assume this allows employees to check the validity of each transaction in an average of 5 minutes. ","9d523055":"# Effect of SMOTE on performance","fbf7fe96":"# Accuracy vs. Precision & Recall\nWhen dealing with unbalanced datasets like this, choosing the correct performance metric is important.\nKoo Ping Shung wrote an [excellent blog post](https:\/\/towardsdatascience.com\/accuracy-precision-recall-or-f1-331fb37c5cb9) on the topic. \n\nEssentially, rather than aiming for overall accuracy on the entire dataset, we care more about catching most of the fraud cases (**recall**), whilst keeping the cost at which this is achieved under control (**precision**). Usually, this is captured in the **f1 score**. However, **asymmetrical costs for type I and II errors** complicate the matters and mean we have to rely on our own cost function.\n\nWhat's for certain is that we can reject accuracy as a measure. Consider this: **simply classifying everything as not-fraud would yield 99.83% accuracy on this dataset**, but it would completely defeat the point.\n\n# The cost per misclassified transaction\nLet's take a look at our data and the dataset's description so we can determine what weight to attribute to each type of error. We assume the amount is in the same currency, say EUR.","dde704fd":"# Cost of doing nothing"}}