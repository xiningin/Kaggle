{"cell_type":{"353d95fb":"code","dba9a9bc":"code","5981f2b7":"code","19942bb5":"code","8aefd1c6":"code","87f4d007":"code","33fb3899":"code","c7493984":"markdown","64fab802":"markdown","09de0cb9":"markdown","e778b836":"markdown","08066c4e":"markdown","8124147f":"markdown"},"source":{"353d95fb":"\"\"\"\nLOAD DATA\n\"\"\"\n\nimport numpy as np \nimport pandas as pd \nimport json\n\n\ntrain_df = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')\nsub_df = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv')\n\ntrain = np.array(train_df)\ntest = np.array(test_df)\n\n!mkdir -p data\n\n\"\"\"\nSETTINGS\n\"\"\"\n\nuse_cuda = True # whether to use GPU or not","dba9a9bc":"train_df.head()","5981f2b7":"%%time\n\n\"\"\"\nPrepare training data in QA-compatible format\n\"\"\"\n\n# Adpated from https:\/\/www.kaggle.com\/cheongwoongkang\/roberta-baseline-starter-simple-postprocessing\ndef find_all(input_str, search_str):\n    l1 = []\n    length = len(input_str)\n    index = 0\n    while index < length:\n        i = input_str.find(search_str, index)\n        if i == -1:\n            return l1\n        l1.append(i)\n        index = i + 1\n    return l1\n\ndef do_qa_train(train):\n\n    output = []\n    for line in train:\n        context = line[1]\n\n        qas = []\n        question = line[-1]\n        qid = line[0]\n        answers = []\n        answer = line[2]\n        if type(answer) != str or type(context) != str or type(question) != str:\n            print(context, type(context))\n            print(answer, type(answer))\n            print(question, type(question))\n            continue\n        answer_starts = find_all(context, answer)\n        for answer_start in answer_starts:\n            answers.append({'answer_start': answer_start, 'text': answer.lower()})\n            break\n        qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n\n        output.append({'context': context.lower(), 'qas': qas})\n        \n    return output\n\nqa_train = do_qa_train(train)\n\nwith open('data\/train.json', 'w') as outfile:\n    json.dump(qa_train, outfile)","19942bb5":"%%time\n\n\"\"\"\nPrepare testing data in QA-compatible format\n\"\"\"\n\ndef do_qa_test(test):\n    output = []\n    for line in test:\n        context = line[1]\n        qas = []\n        question = line[-1]\n        qid = line[0]\n        if type(context) != str or type(question) != str:\n            print(context, type(context))\n            print(answer, type(answer))\n            print(question, type(question))\n            continue\n        answers = []\n        answers.append({'answer_start': 1000000, 'text': '__None__'})\n        qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n        output.append({'context': context.lower(), 'qas': qas})\n    return output\n\nqa_test = do_qa_test(test)\n\nwith open('data\/test.json', 'w') as outfile:\n    json.dump(qa_test, outfile)","8aefd1c6":"!pip install '\/kaggle\/input\/simple-transformers-pypi\/seqeval-0.0.12-py3-none-any.whl' -q\n!pip install '\/kaggle\/input\/simple-transformers-pypi\/simpletransformers-0.22.1-py3-none-any.whl' -q","87f4d007":"%%time\n\n\nfrom simpletransformers.question_answering import QuestionAnsweringModel\n\nMODEL_PATH = '\/kaggle\/input\/transformers-pretrained-distilbert\/distilbert-base-uncased-distilled-squad\/'\n\n# Create the QuestionAnsweringModel\nmodel = QuestionAnsweringModel('distilbert', \n                               MODEL_PATH, \n                               args={'reprocess_input_data': True,\n                                     'overwrite_output_dir': True,\n                                     'learning_rate': 5e-5,\n                                     'num_train_epochs': 3,\n                                     'max_seq_length': 192,\n                                     'doc_stride': 64,\n                                     'fp16': False,\n                                    },\n                              use_cuda=use_cuda)\n\nmodel.train_model('data\/train.json')","33fb3899":"%%time\n\npredictions = model.predict(qa_test)\npredictions_df = pd.DataFrame.from_dict(predictions)\n\nsub_df['selected_text'] = predictions_df['answer']\n\nsub_df.to_csv('submission.csv', index=False)\n\nprint(\"File submitted successfully.\")","c7493984":"### Submission","64fab802":"### 1. Problem formulation\n\nWe formulate the task as question answering problem: given a question and a context, we train a transformer model to find the **answer** in the `text` column (the context).\n\nWe have:\n 1. Question: `sentiment` column (`positive` or `negative`)\n 2. Context:  `text` column\n 3. Answer: `selected_text` column\n\n\n### 2. Getting started with QA\n\nA great resource to quickly recap question answering is this great amazing Stanford Lecture: [Question Answering](https:\/\/web.stanford.edu\/class\/cs124\/lec\/watsonqa.pdf).\n\n#### 2.1 Other free online resources:\n\n - [Youtube: Stanford CS224N - Question Answering](https:\/\/www.youtube.com\/watch?v=yIdF-17HwSk)\n - [Medium: Building a Question-Answering System from Scratch\u2014 Part 1](https:\/\/towardsdatascience.com\/building-a-question-answering-system-part-1-9388aadff507)\n - [Github: awesome question answering](https:\/\/github.com\/seriousran\/awesome-qa)\n\n\n### 3. Learning QA from scratch\n\nThe final project of the Stanford course CS224n, **Natural Language Processing with Deep Learning** consist of creating (almost) from scratch a Question-Ansering system using deep neural nets and transformers. [Here](https:\/\/web.stanford.edu\/class\/cs224n\/project\/default-final-project-handout.pdf) you can find the handout of 24 pages. For the most enthusiast out there: you may want to do this project and implement your Question-Answering system. It's probably the best way to fully understand and learn what QA is about.\n\n### 4. Model: DistilBERT + SQuAD\n\nThe current version of the notebook makes use of the `distilbert-base-uncased-distilled-squad` model.\n\nDistilBERT paper: [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https:\/\/arxiv.org\/abs\/1910.01108)\n\n> As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and\/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.\n\nThe distilBERT model has already been fine-tuned on a question-answering challenge: SQuAD, the [Stanford Question Answering Dataset](https:\/\/rajpurkar.github.io\/SQuAD-explorer\/). This is the main reason why it performs already well out-of-the-box (0.666 score in the LB).\n\n#### 4.1 Training time\n\nThanks to the limited size of the transformer model, the notebook runs quite fast, training time is about 20 minutes with _GPU_.\n\n\n### 5. Dataset publicly available\n\n#### 5.1 DistilBERT + SQuAD model\nBecause Tweet Sentiment Extraction's notebooks must have internet switched off, I already downloaded and stored the transformer model in a public Kaggle dataset: [Transformers pre-trained distilBERT models](https:\/\/www.kaggle.com\/jonathanbesomi\/transformers-pretrained-distilbert). In future, I plan to upload all [distilBERT pre-trained models](https:\/\/huggingface.co\/transformers\/pretrained_models.html) to the same dataset so that we can easily play around with many models and configuration.\n\n#### 5.2 Simple Transformers PyPI\n\nTo keep the code to-the-point, this notebook makes use of an external python package: [simpletransformers](https:\/\/github.com\/ThilinaRajapakse\/simpletransformers). For your convenience, the wheel files to install the package have already been stored in this database: [Simple Transformers PyPI](https:\/\/www.kaggle.com\/jonathanbesomi\/simple-transformers-pypi).\n\n\n### 6. Acknowledgement\n\n- [RoBERTa Baseline Starter (+ simple postprocessing)](https:\/\/www.kaggle.com\/cheongwoongkang\/roberta-baseline-starter-simple-postprocessing)","09de0cb9":"Install [simple-transformers](https:\/\/github.com\/ThilinaRajapakse\/simpletransformers), a tool to train and test transformers model easily.","e778b836":"### Train model\n\nTrain the `distilbert-base-uncased-distilled-squad` model","08066c4e":"<br><br>\n\n<center><font size=\"5\">\ud83d\udcdd Question-Answering Starter pack with \ud83e\udd17transformers<\/font><\/center>\n   \n<br>\n\n<center>\n<font size=\"3\">\n  In this notebook, by making use of  <a href=\"http:\/\/https:\/\/github.com\/huggingface\/transformers\">transformers<\/a> we express the learning problem as a <strong>question-answering system<\/strong>.\n  \n  <br><br>\n  \n  The code and the notebook-format have been designed to be easy-to-understand for beginners but hopefully also useful for advanced Kagglers.\n  \n  <br><br>\n  \n  Any comment\/feedback is very appreciated. Disclaimer: work in progress, I will add new resources and comments soon. \n  \n    \n<\/font>\n<\/center>\n","8124147f":"### Prepare data in QA format\n\nExample-format:\n\n```\ntrain_data = [\n    {\n        'context': \"This tweet sentiment extraction challenge is great\",\n        'qas': [\n            {\n                'id': \"00001\",\n                'question': \"positive\",\n                'answers': [\n                    {\n                        'text': \"is great\",\n                        'answer_start': 43\n                    }\n                ]\n            }\n        ]\n    }\n    ]\n```"}}