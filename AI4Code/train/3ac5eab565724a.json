{"cell_type":{"5d7bc9f1":"code","79c8d78d":"code","16eae69d":"code","b22685c7":"code","953e9abe":"code","24d7a23e":"code","f79c1084":"code","8006196b":"code","72ad3fb0":"code","7eb45b42":"code","aa356682":"code","a464295c":"code","4461c9a9":"code","be0d30b8":"code","a7021ef2":"code","3f73051b":"code","d80aedb2":"code","54f9174e":"code","5d9d8f85":"code","fa7fcd14":"code","d749e29d":"code","b0e9a656":"code","86190afa":"code","2242bd9c":"code","f51fcfb0":"code","c57ae6e7":"code","3bca5949":"code","1e032fce":"code","b8ff2d7a":"code","152e9288":"code","ca68a8ae":"code","8efb2d97":"code","25098ac9":"code","3fa220b1":"code","22547603":"code","77bbb422":"code","b92545e5":"code","53ca3c0c":"code","143c4384":"code","3c1009da":"code","2c0eb9ae":"code","455e39e6":"code","e961ed70":"code","5983c64a":"code","d88d5db7":"code","d161d30c":"code","2ff80821":"code","0952ad2a":"code","f37280fb":"code","2cf48ba1":"code","e214102d":"code","52d747a3":"code","45d8c27e":"code","26d2a49f":"code","56ffc084":"code","e086914c":"code","eeb881a3":"code","51586ef9":"code","9ff13dae":"markdown","353888b0":"markdown","f07489f1":"markdown","5e65614b":"markdown","152571d5":"markdown","7b8f5c8c":"markdown","1cb457f1":"markdown","abfb7e5d":"markdown","d4ba3928":"markdown","452ac736":"markdown","e3c97736":"markdown","65109eac":"markdown","cf30f3b3":"markdown","b9a0877b":"markdown","16ce2f60":"markdown","88e4a8ef":"markdown","7be5f10f":"markdown","98bdf144":"markdown","35811c5e":"markdown"},"source":{"5d7bc9f1":"import copy\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nimport pandas_profiling as pp\nfrom sklearn import preprocessing\n\n# models\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\n# NN models\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras import optimizers\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nfrom sklearn.linear_model import LinearRegression,LogisticRegression, SGDRegressor, RidgeCV\n\n# model tuning\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe, space_eval\n\n# import warnings filter\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)","79c8d78d":"data = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")","16eae69d":"data = data[data['age'] < 60]","b22685c7":"data.head(3)","953e9abe":"data.info()","24d7a23e":"pp.ProfileReport(data)","f79c1084":"# Thanks to: https:\/\/www.kaggle.com\/littleraj30\/eda-model-building-in-depth-on-heart-disease\nfig,ax=plt.subplots(1, 2, figsize = (14,5))\nsns.countplot(data=data, x='target', ax=ax[0],palette='Set2')\nax[0].set_xlabel(\"Disease Count \\n [0]->No [1]->Yes\")\nax[0].set_ylabel(\"Count\")\nax[0].set_title(\"Heart Disease Count\")\ndata['target'].value_counts().plot.pie(explode=[0.1,0.0],autopct='%1.1f%%',ax=ax[1],shadow=True, cmap='Greens')\nplt.title(\"Heart Disease\")\n","8006196b":"# Thanks to: https:\/\/www.kaggle.com\/littleraj30\/eda-model-building-in-depth-on-heart-disease\nfig,ax=plt.subplots(1,2,figsize=(14,5))\nsns.countplot(x='sex',data=data,hue='target',palette='Set1',ax=ax[0])\nax[0].set_xlabel(\"0 ->Female , 1 ->Male\")\ndata.sex.value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%',shadow=True, explode=[0.1,0], cmap='Reds')\nax[1].set_title(\"0 ->Female , 1 -> Male\")","72ad3fb0":"# Thanks to: https:\/\/www.kaggle.com\/littleraj30\/eda-model-building-in-depth-on-heart-disease\nfig,ax=plt.subplots(1,2,figsize=(14,5))\nsns.countplot(x='fbs',data=data,hue='target',palette='Set3',ax=ax[0])\nax[0].set_xlabel(\"0-> fps <120 , 1-> fps>120\",size=12)\ndata.fbs.value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%',shadow=True, explode=[0.1,0],cmap='Oranges')\nax[1].set_title(\"0 -> fps <120 , 1 -> fps>120\",size=12)","7eb45b42":"# Thanks to: https:\/\/www.kaggle.com\/littleraj30\/eda-model-building-in-depth-on-heart-disease\nfig,ax=plt.subplots(1,2,figsize=(14,5))\nsns.countplot(x='restecg',data=data,hue='target',palette='Set3',ax=ax[0])\nax[0].set_xlabel(\"resting electrocardiographic\",size=12)\ndata.restecg.value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%',shadow=True,\n                                     explode=[0.005,0.05,0.05],cmap='Blues')\nax[1].set_title(\"resting electrocardiographic\",size=12)","aa356682":"# Thanks to: https:\/\/www.kaggle.com\/littleraj30\/eda-model-building-in-depth-on-heart-disease\nfig,ax=plt.subplots(1,2,figsize=(14,5))\nsns.countplot(x='slope',data=data,hue='target',palette='Set1',ax=ax[0])\nax[0].set_xlabel(\"peak exercise ST segment\",size=12)\ndata.slope.value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%',shadow=True,explode=[0.005,0.05,0.05],cmap='Blues')\n\nax[1].set_title(\"peak exercise ST segment \",size=12)","a464295c":"# Thanks to: https:\/\/www.kaggle.com\/littleraj30\/eda-model-building-in-depth-on-heart-disease\nfig,ax=plt.subplots(1,2,figsize=(14,5))\nsns.countplot(x='ca',data=data,hue='target',palette='Set2',ax=ax[0])\nax[0].set_xlabel(\"number of major vessels colored by flourosopy\",size=12)\ndata.ca.value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%',shadow=True,cmap='Oranges')\nax[1].set_title(\"number of major vessels colored by flourosopy\",size=12)","4461c9a9":"# Thanks to: https:\/\/www.kaggle.com\/littleraj30\/eda-model-building-in-depth-on-heart-disease\nfig,ax=plt.subplots(1,2,figsize=(14,5))\nsns.countplot(x='thal',data=data,hue='target',palette='Set2',ax=ax[0])\nax[0].set_xlabel(\"number of major vessels colored by flourosopy\",size=12)\ndata.thal.value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%',shadow=True,cmap='Oranges')\nax[1].set_title(\"number of major vessels colored by flourosopy\",size=12)","be0d30b8":"# Thanks to: https: https:\/\/www.kaggle.com\/ahmadjaved097\/classifying-heart-disease-patients\nmale =len(data[data['sex'] == 1])\nfemale = len(data[data['sex']== 0])\n\nplt.figure(figsize=(8,6))\n\n# Data to plot\nlabels = 'Male','Female'\nsizes = [male,female]\ncolors = ['brown', 'purple']\nexplode = (0, 0)  # explode 1st slice\n \n# Plot\nplt.pie(sizes, explode=explode, labels=labels, colors=colors,\nautopct='%1.1f%%', shadow=True, startangle=90)\n \nplt.axis('equal')\nplt.show()","a7021ef2":"# Thanks to: https: https:\/\/www.kaggle.com\/ahmadjaved097\/classifying-heart-disease-patients\nplt.figure(figsize=(8,6))\n\n# Data to plot\nlabels = 'Chest Pain Type:0','Chest Pain Type:1','Chest Pain Type:2','Chest Pain Type:3'\nsizes = [len(data[data['cp'] == 0]),len(data[data['cp'] == 1]),\n         len(data[data['cp'] == 2]),\n         len(data[data['cp'] == 3])]\ncolors = ['skyblue', 'grey','orange','gold']\nexplode = (0, 0,0,0)  # explode 1st slice\n \n# Plot\nplt.pie(sizes, explode=explode, labels=labels, colors=colors,\nautopct='%1.1f%%', shadow=True, startangle=180)\n \nplt.axis('equal')\nplt.show()","3f73051b":"# Thanks to: https: https:\/\/www.kaggle.com\/ahmadjaved097\/classifying-heart-disease-patients\nplt.figure(figsize=(8,6))\n\n# Data to plot\nlabels = 'fasting blood sugar < 120 mg\/dl','fasting blood sugar > 120 mg\/dl'\nsizes = [len(data[data['fbs'] == 0]),len(data[data['cp'] == 1])]\ncolors = ['red', 'yellowgreen','orange','gold']\nexplode = (0.1, 0)  # explode 1st slice\n \n# Plot\nplt.pie(sizes, explode=explode, labels=labels, colors=colors,\nautopct='%1.1f%%', shadow=True, startangle=180)\n \nplt.axis('equal')\nplt.show()","d80aedb2":"# Clone data for FE \ntrain_fe = copy.deepcopy(data)\ntarget_fe = train_fe['target']\ndel train_fe['target']","54f9174e":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\nX = train_fe\nz = target_fe","5d9d8f85":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\n#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(X, z, test_size=0.2, random_state=0)\ntrain_set = lgb.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgb.Dataset(Xval, Zval, silent=False)","fa7fcd14":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\nparams = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 5000 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'zero_as_missing': True,\n        'seed':0,        \n    }\n\nmodelL = lgb.train(params, train_set = train_set, num_boost_round=1000,\n                   early_stopping_rounds=50,verbose_eval=10, valid_sets=valid_set)","d749e29d":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nlgb.plot_importance(modelL,ax = axes,height = 0.5)\nplt.show();plt.close()","b0e9a656":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\nfeature_score = pd.DataFrame(train_fe.columns, columns = ['feature']) \nfeature_score['score_lgb'] = modelL.feature_importance()","86190afa":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\n#%% split training set to validation set \ndata_tr  = xgb.DMatrix(Xtrain, label=Ztrain)\ndata_cv  = xgb.DMatrix(Xval   , label=Zval)\nevallist = [(data_tr, 'train'), (data_cv, 'valid')]","2242bd9c":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\nparms = {'max_depth':8, #maximum depth of a tree\n         'objective':'reg:squarederror',\n         'eta'      :0.3,\n         'subsample':0.8,#SGD will use this percentage of data\n         'lambda '  :4, #L2 regularization term,>1 more conservative \n         'colsample_bytree ':0.9,\n         'colsample_bylevel':1,\n         'min_child_weight': 10}\nmodelx = xgb.train(parms, data_tr, num_boost_round=200, evals = evallist,\n                  early_stopping_rounds=30, maximize=False, \n                  verbose_eval=10)\n\nprint('score = %1.5f, n_boost_round =%d.'%(modelx.best_score,modelx.best_iteration))","f51fcfb0":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nxgb.plot_importance(modelx,ax = axes,height = 0.5)\nplt.show();plt.close()","c57ae6e7":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\nfeature_score['score_xgb'] = feature_score['feature'].map(modelx.get_score(importance_type='weight'))\nfeature_score","3bca5949":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\n# Standardization for regression model\ntrain_fe = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(train_fe),\n    columns=train_fe.columns,\n    index=train_fe.index\n)","1e032fce":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\n# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(train_fe, target_fe)\ncoeff_logreg = pd.DataFrame(train_fe.columns.delete(0))\ncoeff_logreg.columns = ['feature']\ncoeff_logreg[\"score_logreg\"] = pd.Series(logreg.coef_[0])\ncoeff_logreg.sort_values(by='score_logreg', ascending=False)","b8ff2d7a":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\n# the level of importance of features is not associated with the sign\ncoeff_logreg[\"score_logreg\"] = coeff_logreg[\"score_logreg\"].abs()\nfeature_score = pd.merge(feature_score, coeff_logreg, on='feature')","152e9288":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\n# Linear Regression\n\nlinreg = LinearRegression()\nlinreg.fit(train_fe, target_fe)\ncoeff_linreg = pd.DataFrame(train_fe.columns.delete(0))\ncoeff_linreg.columns = ['feature']\ncoeff_linreg[\"score_linreg\"] = pd.Series(linreg.coef_)\ncoeff_linreg.sort_values(by='score_linreg', ascending=False)","ca68a8ae":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\ncoeff_linreg[\"score_linreg\"] = coeff_linreg[\"score_linreg\"].abs()\nfeature_score = pd.merge(feature_score, coeff_linreg, on='feature')\nfeature_score = feature_score.fillna(0)\nfeature_score = feature_score.set_index('feature')\nfeature_score","8efb2d97":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# Thanks to: https:\/\/www.kaggle.com\/nanomathias\/feature-engineering-importance-testing\n# MinMax scale all importances\nfeature_score = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(feature_score),\n    columns=feature_score.columns,\n    index=feature_score.index\n)\n\n# Create mean column\nfeature_score['mean'] = feature_score.mean(axis=1)\n\n# Plot the feature importances\nfeature_score.sort_values('mean', ascending=False).plot(kind='bar', figsize=(20, 10))","25098ac9":"feature_score.sort_values('mean', ascending=False)","3fa220b1":"# Thanks to: Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# Create total column with different weights\nfeature_score['total'] = 0.5*feature_score['score_lgb'] + 0.3*feature_score['score_xgb'] \\\n                       + 0.1*feature_score['score_logreg'] + 0.1*feature_score['score_linreg']\n\n# Plot the feature importances\nfeature_score.sort_values('total', ascending=False).plot(kind='bar', figsize=(20, 10))","22547603":"feature_score.sort_values('total', ascending=False)","77bbb422":"target_name = 'target'\ndata_target = data[target_name]\ndata = data.drop([target_name], axis=1)","b92545e5":"train, test, target, target_test = train_test_split(data, data_target, test_size=0.2, random_state=0)","53ca3c0c":"train.head(3)","143c4384":"test.head(3)","3c1009da":"train.info()","2c0eb9ae":"test.info()","455e39e6":"#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(train, target, test_size=0.2, random_state=0)","e961ed70":"# Random Forest\n\nrandom_forest = GridSearchCV(estimator=RandomForestClassifier(), param_grid={'n_estimators': [100, 300]}, cv=5).fit(train, target)\nrandom_forest.fit(train, target)\nacc_random_forest = round(random_forest.score(train, target) * 100, 2)\nprint(acc_random_forest,random_forest.best_params_)","5983c64a":"acc_test_random_forest = round(random_forest.score(test, target_test) * 100, 2)\nacc_test_random_forest","d88d5db7":"def hyperopt_lgb_score(params):\n    clf = LGBMClassifier(**params)\n    current_score = cross_val_score(clf, train, target, cv=10).mean()\n    print(current_score, params)\n    return current_score \n \nspace_lgb = {\n            'learning_rate': hp.quniform('learning_rate', 0, 0.05, 0.0001),\n            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n            'max_depth':  hp.choice('max_depth', np.arange(2, 12, dtype=int)),\n            'num_leaves': hp.choice('num_leaves', 2*np.arange(2, 2**11, dtype=int)),\n            'min_child_weight': hp.quniform('min_child_weight', 1, 9, 0.025),\n            'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.005),\n            'objective': 'binary',\n            'boosting_type': 'gbdt',\n            }\n \nbest = fmin(fn=hyperopt_lgb_score, space=space_lgb, algo=tpe.suggest, max_evals=10)\nprint('best:')\nprint(best)","d161d30c":"params = space_eval(space_lgb, best)\nparams","2ff80821":"LGB_Classifier = LGBMClassifier(**params)\nLGB_Classifier.fit(train, target)\nacc_LGB_Classifier = round(LGB_Classifier.score(train, target) * 100, 2)\nacc_LGB_Classifier","0952ad2a":"acc_test_LGB_Classifier = round(LGB_Classifier.score(test, target_test) * 100, 2)\nacc_test_LGB_Classifier","f37280fb":"fig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nlgb.plot_importance(LGB_Classifier,ax = axes,height = 0.5)\nplt.show();\nplt.close()","2cf48ba1":"def hyperopt_gb_score(params):\n    clf = GradientBoostingClassifier(**params)\n    current_score = cross_val_score(clf, train, target, cv=10).mean()\n    print(current_score, params)\n    return current_score \n \nspace_gb = {\n            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n            'max_depth': hp.choice('max_depth', np.arange(2, 10, dtype=int))            \n        }\n \nbest = fmin(fn=hyperopt_gb_score, space=space_gb, algo=tpe.suggest, max_evals=10)\nprint('best:')\nprint(best)","e214102d":"params = space_eval(space_gb, best)\nparams","52d747a3":"# Gradient Boosting Classifier\n\ngradient_boosting = GradientBoostingClassifier(**params)\ngradient_boosting.fit(train, target)\nacc_gradient_boosting = round(gradient_boosting.score(train, target) * 100, 2)\nacc_gradient_boosting","45d8c27e":"acc_test_gradient_boosting = round(gradient_boosting.score(test, target_test) * 100, 2)\nacc_test_gradient_boosting","26d2a49f":"models = pd.DataFrame({\n    'Model': ['Random Forest', 'LGBMClassifier', 'GradientBoostingClassifier'],\n    \n    'Score_train': [acc_random_forest, acc_LGB_Classifier,\n              acc_gradient_boosting],\n    'Score_test': [acc_test_random_forest, acc_test_LGB_Classifier,\n              acc_test_gradient_boosting]\n                    })","56ffc084":"models.sort_values(by=['Score_train', 'Score_test'], ascending=False)","e086914c":"models.sort_values(by=['Score_test', 'Score_train'], ascending=False)","eeb881a3":"models['Score_diff'] = abs(models['Score_train'] - models['Score_test'])\nmodels.sort_values(by=['Score_diff'], ascending=True)","51586ef9":"# Plot\nplt.figure(figsize=[25,6])\nxx = models['Model']\nplt.tick_params(labelsize=14)\nplt.plot(xx, models['Score_train'], label = 'Score_train')\nplt.plot(xx, models['Score_test'], label = 'Score_test')\nplt.legend()\nplt.title('Score of 20 popular models for train and test datasets')\nplt.xlabel('Models')\nplt.ylabel('Score, %')\nplt.xticks(xx, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()","9ff13dae":"<a class=\"anchor\" id=\"5\"><\/a>\n## 5. Comparison of the all feature importance diagrams \n##### [Back to Table of Contents](#0.1)","353888b0":"## 6. Preparing to modeling <a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","f07489f1":"<a class=\"anchor\" id=\"4.3\"><\/a>\n### 4.3 Logistic Regression\n[Back to Table of Contents](#0.1)","5e65614b":"## 7. Tuning models and test for all features <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)","152571d5":"<a class=\"anchor\" id=\"4.2\"><\/a>\n### 4.2 XGB\n[Back to Table of Contents](#0.1)","7b8f5c8c":"### 7.3 GradientBoostingClassifier <a class=\"anchor\" id=\"7.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","1cb457f1":" ## 4. FE: building the feature importance diagrams <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","abfb7e5d":"### 7.1 Random Forests <a class=\"anchor\" id=\"7.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","d4ba3928":"## 3. EDA <a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","452ac736":"<a class=\"anchor\" id=\"4.4\"><\/a>\n### 4.4 Linear Regression\n##### [Back to Table of Contents](#0.1)","e3c97736":"## 8. Models evaluation <a class=\"anchor\" id=\"8\"><\/a>\n\n[Back to Table of Contents](#0.1)","65109eac":"<a class=\"anchor\" id=\"0.1\"><\/a>\n\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [Download datasets](#2)\n1. [EDA](#3)\n1. [FE: building the feature importance diagrams](#4)\n    -  [4.1 LGBM](#4.1)\n    -  [4.2 XGB](#4.2) \n    -  [4.3 Logistic Regression](#4.3) \n    -  [4.4 Linear Regression](#4.4)\n1. [Comparison of the all feature importance diagrams ](#5)\n1. [Preparing to modeling](#6)  \n1. [Tuning models and test for all features](#7)\n    -  [Random Forests with GridSearchCV](#7.1)\n    -  [LGBM Classifier with HyperOpt](#7.2)\n    -  [GradientBoostingClassifier with HyperOpt](#7.3) \n1. [Models evaluation](#8)","cf30f3b3":"## 2. Download datasets <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","b9a0877b":"## Acknowledgements\n#### This kernel uses such good kernels:\n   - https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n   - https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\\\n   - https:\/\/www.kaggle.com\/littleraj30\/eda-model-building-in-depth-on-heart-disease\n   - https:\/\/www.kaggle.com\/ahmadjaved097\/classifying-heart-disease-patients\n   - https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\n   - https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n   - https:\/\/www.kaggle.com\/nanomathias\/feature-engineering-importance-testing\n   - https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n   - https:\/\/www.kaggle.com\/kabure\/titanic-eda-model-pipeline-keras-nn","16ce2f60":"## 1. Import libraries <a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","88e4a8ef":"### 7.2 LGBM Classifier <a class=\"anchor\" id=\"7.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","7be5f10f":"<a class=\"anchor\" id=\"4.1\"><\/a>\n### 4.1 LGBM \n[Back to Table of Contents](#0.1)","98bdf144":"Thanks to https:\/\/www.kaggle.com\/kabure\/titanic-eda-model-pipeline-keras-nn","35811c5e":"Thanks to https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions"}}