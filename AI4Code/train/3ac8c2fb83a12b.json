{"cell_type":{"5079cb2b":"code","df8b46cc":"code","a79222c5":"code","8a50129c":"code","af1ff438":"code","cefd00a0":"code","bf95f34f":"code","27046e57":"code","7451fa3c":"code","5f864235":"code","16dee7f9":"code","19fd1feb":"code","44581361":"code","8009acb5":"markdown","cb786516":"markdown","7e9adc6e":"markdown","4f866f66":"markdown","dd5b2d47":"markdown","409ae576":"markdown","d4b92f6a":"markdown","7c6be27d":"markdown","3594bb57":"markdown","10177303":"markdown"},"source":{"5079cb2b":"from functools import partial\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras","df8b46cc":"images_dir = '\/kaggle\/input\/satellite-images-of-water-bodies\/Water Bodies Dataset\/Images'\nmasks_dir = '\/kaggle\/input\/satellite-images-of-water-bodies\/Water Bodies Dataset\/Masks'\n\ndirname, _, filenames = next(os.walk(images_dir))","a79222c5":"@tf.function\ndef load_img_with_mask(image_path, images_dir: str = 'Images', masks_dir: str = 'Masks',images_extension: str = 'jpg', masks_extension: str = 'jpg') -> dict:\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_jpeg(image, channels=3)\n\n    mask_filename = tf.strings.regex_replace(image_path, images_dir, masks_dir)\n    mask_filename = tf.strings.regex_replace(mask_filename, images_extension, masks_extension)\n    mask = tf.io.read_file(mask_filename)\n    mask = tf.image.decode_image(mask, channels=1, expand_animations = False)\n    return (image, mask)","8a50129c":"%matplotlib inline\nn_examples = 3\nexamples = [load_img_with_mask(os.path.join(images_dir, filenames[i])) for i in range(n_examples)]\n\nfig, axs = plt.subplots(n_examples, 2, figsize=(14, n_examples*7), constrained_layout=True)\nfor ax, (image, mask) in zip(axs, examples):\n    ax[0].imshow(image)\n    ax[1].imshow(mask)","af1ff438":"fig, axs = plt.subplots(2, 2, figsize=(14, 14), constrained_layout=True)\nexample_image, example_mask = examples[0]\nexample_image = np.array(example_image)\nexample_mask = np.squeeze(example_mask)\naxs[0][0].imshow(example_image)\naxs[0][1].hist(example_image[:, :, 0].ravel(), bins = 256, color = 'red', alpha = 0.5)\naxs[0][1].hist(example_image[:, :, 1].ravel(), bins = 256, color = 'Green', alpha = 0.5)\naxs[0][1].hist(example_image[:, :, 2].ravel(), bins = 256, color = 'Blue', alpha = 0.5)\n\naxs[1][0].imshow(example_mask)\nhistogram, bin_edges = np.histogram(example_mask, bins=256)\naxs[1][1].plot(bin_edges[0:-1], histogram)\n;","cefd00a0":"@tf.function\ndef resize_images(images, masks, max_image_size=1500):\n    shape = tf.shape(images)\n    scale = (tf.reduce_max(shape) \/\/ max_image_size) + 1\n    target_height, target_width = shape[-3] \/\/ scale, shape[-2] \/\/ scale\n    images = tf.cast(images, tf.float32)\n    masks = tf.cast(masks, tf.float32)\n    if scale != 1:\n        images = tf.image.resize(images, (target_height, target_width), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n        masks = tf.image.resize(masks, (target_height, target_width), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    return (images, masks)\n\n@tf.function\ndef scale_values(images, masks, mask_split_threshold = 128):\n    images = tf.math.divide(images, 255)\n    masks = tf.where(masks > mask_split_threshold, 1, 0)\n    return (images, masks)\n\n@tf.function\ndef pad_images(images, masks, pad_mul=16, offset=0):\n    shape = tf.shape(images)\n    height, width = shape[-3], shape[-2]\n    target_height = height + tf.math.floormod(tf.math.negative(height), pad_mul)\n    target_width = width + tf.math.floormod(tf.math.negative(width), pad_mul)\n    images = tf.image.pad_to_bounding_box(images, offset, offset, target_height, target_width)\n    masks = tf.cast(tf.image.pad_to_bounding_box(masks, offset, offset, target_height, target_width), tf.uint8)\n    return (images, masks)","bf95f34f":"batch_size = 32\ntest_set_size = 300\nvalidation_set_size = 250\n\ndataset = tf.data.Dataset.list_files(images_dir + '\/*.jpg', seed=42)\n\ntest_dataset = dataset.take(test_set_size)\ndataset = dataset.skip(test_set_size)\ntest_dataset = test_dataset.map(load_img_with_mask)\ntest_dataset = test_dataset.map(scale_values)\ntest_dataset = test_dataset.shuffle(20)\ntest_dataset = test_dataset.map(lambda img, mask: resize_images(img, mask, max_image_size=2500))\ntest_dataset = test_dataset.map(pad_images)\ntest_dataset = test_dataset.batch(1).prefetch(5)\n\n\nvalidation_dataset = dataset.take(validation_set_size)\ntrain_dataset = dataset.skip(validation_set_size)\nvalidation_dataset = validation_dataset.map(load_img_with_mask)\nvalidation_dataset = validation_dataset.map(scale_values)\nvalidation_dataset = validation_dataset.shuffle(20)\nvalidation_dataset = validation_dataset.map(resize_images)\nvalidation_dataset = validation_dataset.map(pad_images)\nvalidation_dataset = validation_dataset.batch(1).prefetch(5)\n\ntrain_dataset = train_dataset.map(load_img_with_mask)\ntrain_dataset = train_dataset.map(scale_values)\ntrain_dataset = train_dataset.shuffle(20)\ntrain_dataset = train_dataset.map(resize_images)\ntrain_dataset = train_dataset.map(pad_images)\ntrain_dataset = train_dataset.batch(1).prefetch(5)\n","27046e57":"def get_unet(hidden_activation='relu', initializer='he_normal', output_activation='sigmoid'):\n    PartialConv = partial(keras.layers.Conv2D,\n     activation=hidden_activation,\n     kernel_initializer=initializer,      \n     padding='same')\n    \n    # Encoder\n    model_input = keras.layers.Input(shape=(None, None, 3))\n    enc_cov_1 = PartialConv(32, 3)(model_input)\n    enc_cov_1 = PartialConv(32, 3)(enc_cov_1)\n    enc_pool_1 = keras.layers.MaxPooling2D(pool_size=(2, 2))(enc_cov_1)\n    \n    enc_cov_2 = PartialConv(64, 3)(enc_pool_1)\n    enc_cov_2 = PartialConv(64, 3)(enc_cov_2)\n    enc_pool_2 = keras.layers.MaxPooling2D(pool_size=(2, 2))(enc_cov_2)\n    \n    enc_cov_3 = PartialConv(128, 3)(enc_pool_2)\n    enc_cov_3 = PartialConv(128, 3)(enc_cov_3)\n    enc_pool_3 = keras.layers.MaxPooling2D(pool_size=(2, 2))(enc_cov_3)\n    \n    # Center\n    center_cov = PartialConv(256, 3)(enc_pool_3)\n    center_cov = PartialConv(256, 3)(center_cov)\n    \n    # Decoder\n    upsampling1 = keras.layers.UpSampling2D(size=(2, 2))(center_cov)\n    dec_up_conv_1 = PartialConv(128, 2)(upsampling1)\n    dec_merged_1 = tf.keras.layers.Concatenate(axis=3)([enc_cov_3, dec_up_conv_1])\n    dec_conv_1 = PartialConv(128, 3)(dec_merged_1)\n    dec_conv_1 = PartialConv(128, 3)(dec_conv_1)\n    \n    upsampling2 = keras.layers.UpSampling2D(size=(2, 2))(dec_conv_1)\n    dec_up_conv_2 = PartialConv(64, 2)(upsampling2)\n    dec_merged_2 = tf.keras.layers.Concatenate(axis=3)([enc_cov_2, dec_up_conv_2])\n    dec_conv_2 = PartialConv(64, 3)(dec_merged_2)\n    dec_conv_2 = PartialConv(64, 3)(dec_conv_2)\n    \n    upsampling3 = keras.layers.UpSampling2D(size=(2, 2))(dec_conv_2)\n    dec_up_conv_3 = PartialConv(32, 2)(upsampling3)\n    dec_merged_3 = tf.keras.layers.Concatenate(axis=3)([enc_cov_1, dec_up_conv_3])\n    dec_conv_3 = PartialConv(32, 3)(dec_merged_3)\n    dec_conv_3 =  PartialConv(32, 3)(dec_conv_3)\n    \n    output = keras.layers.Conv2D(1, 1, activation=output_activation)(dec_conv_3)\n    \n    return tf.keras.Model(inputs=model_input, outputs=output)","7451fa3c":"model = get_unet()\n\noptimizer = tf.keras.optimizers.Nadam()\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer)","5f864235":"model.summary()","16dee7f9":"early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\nlr_reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, verbose=1)\n\nepochs = 80\nhistory = model.fit(train_dataset, validation_data=validation_dataset, epochs=epochs, callbacks=[early_stopping, lr_reduce])","19fd1feb":"n_examples = 10\n\nfig, axs = plt.subplots(n_examples, 3, figsize=(14, n_examples*7), constrained_layout=True)\nfor ax, ele in zip(axs, test_dataset.take(n_examples)):\n    image, y_true = ele\n    prediction = model.predict(image)[0]\n    prediction = tf.where(prediction > 0.5, 255, 0)\n    ax[0].set_title('Original image')\n    ax[0].imshow(image[0])\n    ax[1].set_title('Original mask')\n    ax[1].imshow(y_true[0])\n    ax[2].set_title('Predicted area')\n    ax[2].imshow(prediction)","44581361":"meanIoU = tf.keras.metrics.MeanIoU(num_classes=2)\nfor ele in test_dataset.take(test_set_size):\n    image, y_true = ele\n    prediction = model.predict(image)[0]\n    prediction = tf.where(prediction > 0.5, 1, 0)\n    meanIoU.update_state(y_true[0], prediction)\nprint(meanIoU.result().numpy())","8009acb5":"# 5. Testing the model","cb786516":"Some images seem to be rotated, it seems worth exploring if the original pictures are in the dataset. If that's the case then testing would be probably spoiled. Additionally in some of these images parts padded during this rotation are marked as water bodies and in some they are not. It may be a good idea to transform the images to get rid of this contradicting ground truths.","7e9adc6e":"* For the main images there is a lot of values around 0, probably mainly because of the rotation. Values are centered around the middle, it may be a good idea to improve the contrast, maybe using histsogram equalization.\n* Mask were stored in jpeg format, so there are no 2 discrete values, we should separate them using some threshold in the middle","4f866f66":"# 4. Creating the model","dd5b2d47":"I will use tf.data.Dataset as it's quite easy to fetch the data and apply transformations utilizing optimized tf functions and prefetching.\nThere are a couple of transformation functions necessary before training our model:\n* resize_images: some images wouldn't fit in the memory due to enormous size differences, with some of them having over 3000 in height or width. Only training images over some size threshold are rescaled.\n*  convert_masks: splitting masks into 2 discrete values after they were encoded in jpeg\n*  scale_values: Dividing images (not masks) to the range [0, 1]  makes the model converge faster\n*  pad_images: Due to the specificity of the the used neural net: additional connections between nonconsecutive layers and the same size of input and output image the image sizes should be divisible by the number of downsamplings (taking into consideration the strides). Additionally as we accept arbitrary image sizes we shouldn't just pad to some predetermined values","409ae576":"I decided to try training U-net model as it is performing very well in image segmentation task. My implementation has a lower number of channels due to memory restrictions. It also may be simple enough for binary classification.\nHere is the image used in the original paper done by Olaf Ronneberger, Philipp Fischer and Thomas Brox:\n\n\n![image.png](attachment:ee3220a2-6854-4296-8227-0a45c03965f1.png)","d4b92f6a":"# 2. Exploratory data analysis","7c6be27d":"# 1. Loading the data","3594bb57":"# 3. Preprocessing the data","10177303":"\nThe results seem to be quite good for most of the pictures, but of course there is still a lot of room to improvement. Don't hesitate to comment and share your remarks!"}}