{"cell_type":{"bb6757a7":"code","557a927b":"code","7d20c951":"code","180cf818":"code","65e5c9ab":"code","1f3fe284":"code","98bd9b3e":"markdown","830ca2ae":"markdown","5be7c9b7":"markdown","91dfecf8":"markdown","23670a36":"markdown","b7efaa39":"markdown","79093c31":"markdown","0c584555":"markdown"},"source":{"bb6757a7":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.applications import vgg16\n\nbase_image_path = keras.utils.get_file(\"osaka.jpg\", \"https:\/\/taiyounotou-expo70.jp\/sys\/wp-content\/themes\/taiyounotou\/build\/img\/page\/about\/index\/intro.jpg\")\nstyle_reference_image_path = keras.utils.get_file(\"winslow.jpg\", \"https:\/\/ae01.alicdn.com\/kf\/HTB1J.dmQXXXXXcdXFXXq6xXFXXXS\/TOP-art-Winslow-Homer-Seascape-The-Herring-Net-fishing-oil-painting-GOOD-PRINT-ART-oil-painting.jpg\")\nresult_prefix = \"osaka_generated\"\n\n# Adjust weights here.\ntotal_variation_weight = 1e-5\nstyle_weight = 1e-5\ncontent_weight = 1e-5\nwidth, height = 500, 500\nimg_nrows, img_ncols = 500, 500","557a927b":"from IPython.display import Image, display\ndisplay(Image(base_image_path))\ndisplay(Image(style_reference_image_path))","7d20c951":"def preprocess_image(image_path):\n    img = keras.preprocessing.image.load_img(image_path, target_size=(img_nrows, img_ncols))\n    img = keras.preprocessing.image.img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = vgg16.preprocess_input(img)\n    return tf.convert_to_tensor(img)\n\ndef deprocess_image(x):\n    x = x.reshape((img_nrows, img_ncols, 3))\n    x[:, :, 0] += 103.939\n    x[:, :, 1] += 116.779\n    x[:, :, 2] += 123.68\n    # 'BGR'->'RGB'\n    x = x[:, :, ::-1]\n    x = np.clip(x, 0, 255).astype(\"uint8\")\n    return x\n\ndef gram_matrix(x):\n    x = tf.transpose(x, (2, 0, 1))\n    features = tf.reshape(x, (tf.shape(x)[0], -1))\n    gram = tf.matmul(features, tf.transpose(features))\n    return gram\n\ndef style_loss(style, combination):\n    S = gram_matrix(style)\n    C = gram_matrix(combination)\n    size = img_nrows * img_ncols\n    return tf.reduce_sum(tf.square(S - C)) \/ (4.0 * (3 ** 2) * (size ** 2))\n\ndef content_loss(base, combination):\n    return tf.reduce_sum(tf.square(combination - base))\n\ndef total_variation_loss(x):\n    a = tf.square(x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, 1:, : img_ncols - 1, :])\n    b = tf.square(x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, : img_nrows - 1, 1:, :])\n    return tf.reduce_sum(tf.pow(a + b, 1.25))","180cf818":"model = vgg16.VGG16(weights=\"imagenet\", include_top=False)\noutputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\nfeature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)\nstyle_layer_names = [\"block1_conv1\",\"block2_conv1\",\"block3_conv1\",\"block4_conv1\",\"block5_conv1\"]\ncontent_layer_name = \"block5_conv2\"\n\ndef compute_loss(combination_image, base_image, style_reference_image):\n    input_tensor = tf.concat([base_image, style_reference_image, combination_image], axis=0)\n    features = feature_extractor(input_tensor)\n    loss = tf.zeros(shape=())\n    layer_features = features[content_layer_name]\n    base_image_features = layer_features[2, :, :, :]\n    combination_features = layer_features[2, :, :, :]\n    loss = loss + content_weight * content_loss(base_image_features, combination_features)\n    for layer_name in style_layer_names:\n        layer_features = features[layer_name]\n        style_reference_features = layer_features[1, :, :, :]\n        combination_features = layer_features[2, :, :, :]\n        sl = style_loss(style_reference_features, combination_features)\n        loss += (style_weight \/ len(style_layer_names)) * sl\n    loss += total_variation_weight * total_variation_loss(combination_image)\n    return loss\n\ndef compute_loss_and_grads(combination_image, base_image, style_reference_image):\n    with tf.GradientTape() as tape:\n        loss = compute_loss(combination_image, base_image, style_reference_image)\n    grads = tape.gradient(loss, combination_image)\n    return loss, grads","65e5c9ab":"optimizer = keras.optimizers.SGD(keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=100.0, decay_steps=100, decay_rate=0.96))\nbase_image = preprocess_image(base_image_path)\nstyle_reference_image = preprocess_image(style_reference_image_path)\ncombination_image = tf.Variable(preprocess_image(base_image_path))\niterations = 100\nfor i in range(1, iterations + 1):\n    loss, grads = compute_loss_and_grads(combination_image, base_image, style_reference_image)\n    optimizer.apply_gradients([(grads, combination_image)])\n    if i <= 90 and i % 10 == 0:\n        print(\"Iteration %d: loss=%.2f\" % (i, loss))\n    \n    if i == 100:\n        print(\"Iteration %d: loss=%.2f\" % (i, loss))\n        img = deprocess_image(combination_image.numpy())\n        fname = result_prefix + \"_at_iteration_%d.png\" % i\n        keras.preprocessing.image.save_img(fname, img)","1f3fe284":"result_im = '.\/osaka_generated_at_iteration_100.png'\nImage(result_im)","98bd9b3e":"## This programming code is <font color=red>copied and customized<\/font> from one of <B>Keras style transfer<\/B> code samples.   \n### [https:\/\/keras.io\/examples\/generative\/neural_style_transfer\/](https:\/\/keras.io\/examples\/generative\/neural_style_transfer\/)","830ca2ae":"## 3. Load pre-traing VGG16 model and set the model.","5be7c9b7":"## 1. Show images","91dfecf8":"## 5. Show the result","23670a36":"<HR>","b7efaa39":"## 4. Let's paint a new image.","79093c31":"<HR>","0c584555":"## 2.Resize and format pictures into appropriate tensors"}}