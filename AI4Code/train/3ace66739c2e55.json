{"cell_type":{"0cf7fcf4":"code","fffa46cc":"code","fc0d06bf":"code","c2a09d2e":"code","93a142aa":"markdown","d2acd8bb":"markdown","632c21e6":"markdown"},"source":{"0cf7fcf4":"import os\nimport gc\nimport sys\nimport cv2\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold,StratifiedKFold, GroupKFold\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.optimizer import Optimizer\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim.lr_scheduler import (CosineAnnealingWarmRestarts, CosineAnnealingLR, \n                                      ReduceLROnPlateau)\n\nfrom transformers import (AutoModel, AutoTokenizer, \n                          AutoModelForSequenceClassification,\n                          get_constant_schedule_with_warmup,get_cosine_schedule_with_warmup)\n\n","fffa46cc":"# data reading\ntrain_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsample = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\n\nclass CLRPDataset(nn.Module):\n    def __init__(self,df,tokenizer,max_len=128):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.targets = df['target'].to_numpy()\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],\n                                return_tensors='pt', # this will add a extra dimension\n                                max_length=self.max_len,\n                                padding='max_length',\n                                truncation=True)\n        \n        target = torch.tensor(self.targets[idx],dtype=torch.float)\n        return encode, target\n    \n    def __len__(self):\n        return len(self.excerpt)","fc0d06bf":"# finetune setting\nconfig = {\n    'lr': 1e-5,\n    'wd':1e-1,\n    'batch_size':16,\n    'max_len':256,\n    'epochs':4,\n    'nfolds':5,\n    'seed':42,\n}\n\ndef rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=config['seed'])\n\ndef loss_fn(outputs,targets):\n    outputs = outputs.logits.squeeze(-1)\n    return torch.sqrt(nn.MSELoss()(outputs,targets))\n\ndef train_loop(train_loader, model, loss_fn, device,optimizer,lr_scheduler=None):\n    model.train()\n    total_loss = 0\n    for i, (inputs,targets) in enumerate(train_loader):\n        optimizer.zero_grad()\n        inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n        targets = targets.to(device)\n        outputs = model(**inputs)\n        loss = loss_fn(outputs,targets)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n    total_loss \/= len(train_loader)\n    return total_loss\n\ndef valid_loop(valid_loader, model, loss_fn, device):\n    model.eval()\n    total_loss = 0\n    valid_predictions = list()\n    with torch.no_grad():\n        for i, (inputs,targets) in enumerate(valid_loader):\n            inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n            targets = targets.to(device)\n\n            outputs = model(**inputs)\n            loss = loss_fn(outputs,targets)\n            total_loss += loss.item()\n            outputs = outputs.logits.squeeze(-1).cpu().detach().numpy().tolist()\n#                 outputs = outputs.cpu().detach().numpy().tolist()\n            valid_predictions.extend(outputs)\n        total_loss \/= len(valid_loader)\n    return total_loss ,valid_predictions\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","c2a09d2e":"# training\ntrain = train_data\nkfold = KFold(n_splits=config['nfolds'],shuffle=True,random_state=config['seed'])\nfor k , (train_idx,valid_idx) in enumerate(kfold.split(X=train)):\n    x_train,x_valid = train.loc[train_idx],train.loc[valid_idx]\n\n    MODEL_PATH = 'roberta-large'\n    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH,num_labels=1)\n    model.to(device)\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n\n    train_ds = CLRPDataset(x_train,tokenizer,config['max_len'])\n    train_dl = DataLoader(train_ds,\n                          batch_size = config[\"batch_size\"],\n                          shuffle=True,\n                          num_workers = 4,\n                          pin_memory=True,\n                          drop_last=False\n                         )\n\n    valid_ds = CLRPDataset(x_valid,tokenizer,config['max_len'])\n    valid_dl = DataLoader(valid_ds,\n                          batch_size = config[\"batch_size\"],\n                          shuffle=False,\n                          num_workers = 4,\n                          pin_memory=True,\n                          drop_last=False,\n                         )\n\n    optimizer = optim.AdamW(model.parameters(),lr=config['lr'],weight_decay=config['wd'])\n    lr_scheduler = None\n\n    best_loss = 99999\n    best_valid_predictions = list()\n    \n    for i in range(config[\"epochs\"]):\n        train_loss = train_loop(train_dl,model,loss_fn,device,optimizer,lr_scheduler=lr_scheduler)\n        valid_loss,valid_predictions = valid_loop(valid_dl,model,loss_fn,device)\n\n        valid_targets = x_valid['target'].to_list()\n\n        if lr_scheduler:\n            lr_scheduler.step()\n            \n        if valid_loss <= best_loss:\n            best_loss = valid_loss\n            best_valid_predictions = valid_predictions\n#                 torch.save(model.state_dict(),f'.\/model{k}\/model{k}.bin')\n            model.save_pretrained(f'.\/model{k}')\n            tokenizer.save_pretrained(f'.\/model{k}')\n\n\n","93a142aa":"# Acknowledgement: \nThis is just a cleaner version for https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-train-tpu to finetune RoberTa with 5-fold CV. Output models can be found in https:\/\/www.kaggle.com\/maunish\/clrp-roberta-svm\/data. I do this just to solve some confusion for code. If you like, just give credit to [Maunish](https:\/\/www.kaggle.com\/maunish). \n\nThe confusion is this line of code `inputs = {key:val.reshape(val.shape[0], -1).to(device) for key, val in inputs.items()}`. Finally, I figure out that pytorch dataset will output extra dimension due to huggingface tokenizer with `return_tensors='pt'`.","d2acd8bb":"## Imports \ud83d\udcd7","632c21e6":"## Getting Data \ud83d\udcbe"}}