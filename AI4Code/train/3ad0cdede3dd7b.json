{"cell_type":{"c754cb1e":"code","1d49b575":"code","782ee43f":"code","4a843fb0":"code","0219f008":"code","8608b9df":"code","d8e9b5e4":"code","811c45a6":"code","191f56cb":"code","ec61cab5":"code","0e913a95":"code","b921d46f":"code","b2f394ff":"code","d01a781a":"code","2169eabc":"code","281e0a37":"code","8ee0a954":"code","63fb389b":"code","7fab778d":"code","595f845a":"code","e58a49ef":"code","45700edd":"code","67a8ac8f":"code","4f9162ad":"code","21e183dc":"code","64d37a11":"code","5249600c":"code","6acb2e49":"code","1989f7a0":"code","679325ee":"code","b22d1dc3":"code","ab227734":"code","8a9461f2":"code","1a594305":"code","16b636d2":"code","c670745e":"code","84c5a8f5":"code","32bf6ec5":"code","235cdb17":"code","84591e34":"code","6d0456b7":"code","ef22c158":"code","25dc9ad3":"code","63992f30":"code","a2191ac1":"code","3f8890b5":"code","261df1ec":"code","a56c8625":"code","ce3ecc4b":"code","702db8d9":"code","283ce27a":"markdown","b55196d8":"markdown","6558866a":"markdown","500972ce":"markdown","b29bc86a":"markdown","cd910ac1":"markdown","644bda2a":"markdown","b22fd1aa":"markdown","53e14af0":"markdown","0fb79ec2":"markdown","d139292e":"markdown","681b05aa":"markdown","755b31a7":"markdown","0888e2ce":"markdown","5bca4f1f":"markdown","a3819d7b":"markdown","6061b656":"markdown","207cc904":"markdown","a4806223":"markdown","327fa0f3":"markdown","91ac54de":"markdown","ff8a2a7c":"markdown"},"source":{"c754cb1e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport math\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nfrom numpy import exp\nfrom scipy.stats import boxcox\nfrom operator import itemgetter\nfrom sklearn import preprocessing\nfrom numpy.random import randn\nimport statsmodels.api as sm\nimport pylab as py\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport sklearn.metrics as metrics\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nfrom sklearn import tree\nfrom sklearn.tree import _tree\n\nfrom sklearn.ensemble import RandomForestRegressor \nfrom sklearn.ensemble import RandomForestClassifier \n\nfrom sklearn.ensemble import GradientBoostingRegressor \nfrom sklearn.ensemble import GradientBoostingClassifier \n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1d49b575":"df = pd.read_csv(\"..\/input\/home-equity-line-of-credit-data\/HMEQ_Loss.csv\")","782ee43f":"df.head()","4a843fb0":"df.info()","0219f008":"df.isna().any()","8608b9df":"df.isnull().sum()","d8e9b5e4":"#Example of counting just one missing column\n\ndf['JOB'].isna().sum()","811c45a6":"#Fill Nulls\n#df['TARGET_LOSS_AMT'] =df['TARGET_LOSS_AMT'].fillna(0)\n\n#Rest of columns\ndf['MORTDUE_CLEAN'] =df['MORTDUE'].fillna(value=df['MORTDUE'].mean())\ndf['VALUE_CLEAN'] =df['VALUE'].fillna(value=df['VALUE'].mean())\ndf['YOJ_CLEAN'] =df['YOJ'].fillna(value=df['YOJ'].median())\ndf['DEROG_CLEAN'] =df['DEROG'].fillna(value=df['DEROG'].median())\ndf['DELINQ_CLEAN'] =df['DELINQ'].fillna(value=df['DELINQ'].median())\ndf['CLAGE_CLEAN'] =df['CLAGE'].fillna(value=df['CLAGE'].median())\ndf['NINQ_CLEAN'] =df['NINQ'].fillna(value=df['NINQ'].median())\ndf['CLNO_CLEAN'] =df['CLNO'].fillna(value=df['CLNO'].median())\ndf['DEBTINC_CLEAN'] =df['DEBTINC'].fillna(value=df['DEBTINC'].median())\n# Clean Text Columns\ndf['JOB'] = df['JOB'].fillna('Other')\ndf['REASON'] = df['REASON'].fillna('Other')","191f56cb":"df.isnull().sum()","ec61cab5":"df.describe()","0e913a95":"#Create Lists to separate data\nnumerical = ['LOAN', 'MORTDUE_CLEAN', 'VALUE_CLEAN', 'YOJ_CLEAN', 'DEROG_CLEAN', 'DELINQ_CLEAN', 'CLAGE_CLEAN', 'NINQ_CLEAN'\n    ,'DEBTINC_CLEAN']\ncategorical = ['REASON', 'JOB']","b921d46f":"sns.pairplot(df[numerical], corner=True)","b2f394ff":"df[numerical].hist(bins=15, figsize=(15, 10), layout=(3, 4));","d01a781a":"plt.figure(figsize=(16, 6))\nheatmap = sns.heatmap(df[numerical].corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=12);","2169eabc":"# Precursor! Remember not all outliers are bad\nsns.displot(df, x=\"LOAN\", kind=\"kde\")","281e0a37":"ax = sns.boxplot(x=df[\"LOAN\"])","8ee0a954":"sns.displot(df, x=\"YOJ_CLEAN\", kind=\"kde\")","63fb389b":"ax = sns.boxplot(x=df[\"YOJ_CLEAN\"])","7fab778d":"# Skewness is a very powerful tool in the Data Analyst toolbox\n\n\"\"\"\nSkewness is a measure of symmetry, or more precisely, the lack of symmetry. A distribution, or data set, \nis symmetric if it looks the same to the left and right of the center point. Kurtosis is a measure of whether \nthe data are heavy-tailed or light-tailed relative to a normal distribution.\n\nGood article on math behind it:\nhttps:\/\/brownmath.com\/stat\/shape.htm\n\"\"\"\n\nprint(df[\"LOAN\"].skew())\ndf[\"LOAN\"].describe()","595f845a":"# Select duplicate rows except last occurrence based on all columns\nduplicateRowsDF = df[df.duplicated(['LOAN', 'VALUE'])]\n\n# Other option, use first or last \n# duplicateRowsDF =  df[df.duplicated(keep='last')]\n\nprint(\"Duplicate Rows except last occurrence based on all columns are :\")\nprint(duplicateRowsDF)\n\n#Sort columns\n# df.sort_values(by=['LOAN', 'VALUE'], ascending=False)\n#Filter out rows\n# df= df.drop_duplicates(subset=['LOAN', 'VALUE'], keep='last').copy()","e58a49ef":"def outlier_treatment(datacolumn):\n    sorted(datacolumn)\n    Q1,Q3 = np.percentile(datacolumn , [25,75])\n    IQR = Q3 - Q1\n    lower_range = Q1 - (3 * IQR)\n    upper_range = Q3 + (3 * IQR)\n    return lower_range,upper_range\n\n","45700edd":"lower_range,upper_range = outlier_treatment(df.YOJ_CLEAN)\nprint(lower_range,upper_range)\nprint('Count how many fit in this IQR:',len(df[(df.YOJ_CLEAN < lower_range) | (df.YOJ_CLEAN > upper_range)]))\n","67a8ac8f":"def z_score(datacolumn):\n    sorted(datacolumn)\n    zscore=stats.zscore(datacolumn)\n    return zscore\n\nzscore = z_score(df.YOJ_CLEAN)\nprint(zscore)","4f9162ad":"#Lets fix YOJ\ndf['YOJ_Z'] = ( df.YOJ_CLEAN - df.YOJ_CLEAN.mean() ) \/ df.YOJ_CLEAN.std()","21e183dc":"sns.displot(df, x=\"YOJ_Z\", kind=\"kde\")","64d37a11":"len(df[(df.YOJ_Z < -3) | (df.YOJ_Z > 3)])","5249600c":"# Option 1 is best! \n\n# But for brevity sake, here is them all in one cell\n\n# 1. Fill to predefined value\ndf[\"YOJ_CLEAN\"] = np.where(df[\"YOJ_CLEAN\"] >39,df[\"YOJ_CLEAN\"]==39,df[\"YOJ_CLEAN\"])\n\n# 2. Fill to median\/mean\/mode\n#df[\"YOJ_CLEAN\"] = np.where(df[\"YOJ_CLEAN\"] >39,df[\"YOJ_CLEAN\"].mean(),df[\"YOJ_CLEAN\"])\n\n\n# 3. Drop entirely\n# df.drop(df[ (df[\"YOJ_CLEAN\"] > u) | (df[\"YOJ_CLEAN\"] < l) ].index , inplace=True)\n\n# 4. Document but do nothing\n","6acb2e49":"#Validate it worked!\nlower_range,upper_range = outlier_treatment(df.YOJ_CLEAN)\nprint(lower_range,upper_range)\nprint('Count how many fit in this IQR:',len(df[(df.YOJ_CLEAN < lower_range) | (df.YOJ_CLEAN > upper_range)]))","1989f7a0":"# Select the ones you want\nprescale = df[['MORTDUE_CLEAN','VALUE']].copy()","679325ee":"sns.displot(prescale, x=\"MORTDUE_CLEAN\", kind=\"kde\")","b22d1dc3":"sns.jointplot(data=prescale, x='MORTDUE_CLEAN', y='VALUE')","ab227734":"# the scaler object (model)\nscaler = StandardScaler()\n\n# fit and transform the data\nscaled = scaler.fit_transform(prescale) ","8a9461f2":"print(prescale)","1a594305":"df_scaled = pd.DataFrame(scaled, columns = ['MORTDUE_CLEAN','VALUE'])\nprint(df_scaled)","16b636d2":"sns.displot(df_scaled, x=\"MORTDUE_CLEAN\", kind=\"kde\")","c670745e":"scaled.mean(axis = 0)","84c5a8f5":"mm_scaler = preprocessing.MinMaxScaler()\nmm = mm_scaler.fit_transform(prescale)","32bf6ec5":"df_mm = pd.DataFrame(mm, columns = ['MORTDUE_CLEAN','VALUE'])\n","235cdb17":"df_mm.sort_values(by='MORTDUE_CLEAN', ascending=False)","84591e34":"sns.displot(df_mm, x=\"MORTDUE_CLEAN\", kind=\"kde\")","6d0456b7":"# Select the ones you want\ntsb = df[['VALUE_CLEAN']].copy()\ntransbox =tsb.values.flatten()\n","ef22c158":"print(type(transbox))","25dc9ad3":"transform = boxcox(transbox, 0)","63992f30":"transform","a2191ac1":"# Before!\nsns.displot(transbox, kind=\"kde\")","3f8890b5":"# After!\nsns.displot(transform, kind=\"kde\")","261df1ec":"# fit lognormal distribution\nshape, loc, scale = stats.lognorm.fit(transbox, loc=0)\npdf_lognorm = stats.lognorm.pdf(transbox, shape, loc, scale)\n\n# fit normal distribution\nmean, std = stats.norm.fit(transbox, loc=0)\npdf_norm = stats.norm.pdf(transbox, mean, std)\n\n# fit weibull distribution\nshape, loc, scale = stats.weibull_min.fit(transbox, loc=0)\npdf_weibull_min = stats.weibull_min.pdf(transbox, shape, loc, scale)","a56c8625":"pdf_lognorm","ce3ecc4b":"sns.displot(pdf_lognorm, kind=\"kde\")","702db8d9":"# Random data points generated\ndata_points = np.random.normal(0, 1, 100)    \n  \nsm.qqplot(data_points, line ='45')\npy.show()","283ce27a":"![bad_data.jfif](attachment:0fce27ce-d6e7-4e5d-8621-dffa885336ad.jfif)","b55196d8":"# Standardize Values","6558866a":"# Impute NULLS","500972ce":"https:\/\/machinelearningmastery.com\/standardscaler-and-minmaxscaler-transforms-in-python\/ \n<br>\nhttps:\/\/towardsdatascience.com\/how-and-why-to-standardize-your-data-996926c2c832\n<br>\nhttps:\/\/towardsdatascience.com\/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02","b29bc86a":"# Imputing Values Now","cd910ac1":"Great article on box plots:\nhttps:\/\/towardsdatascience.com\/understanding-boxplots-5e2df7bcbd51\n\n![Z-Score.png](attachment:8aa99766-66a0-4223-8ca1-406ff5ee3a33.png)","644bda2a":"![BoxCox.PNG](attachment:b6d95f2a-5070-4e98-abd8-db7a99e99c31.PNG)","b22fd1aa":"#### We are going to focus on extreme outliers which are defined at 3 * IQR ","53e14af0":"https:\/\/machinelearningmastery.com\/how-to-transform-data-to-fit-the-normal-distribution\/ Credit!\n<br>\nA large portion of the field of statistics is concerned with methods that assume a Gaussian distribution: the familiar bell curve.\n\nIf your data has a Gaussian distribution, the parametric methods are powerful and well understood. This gives some incentive to use them if possible. Even if your data does not have a Gaussian distribution.\n\nIt is possible that your data does not look Gaussian or fails a normality test, but can be transformed to make it fit a Gaussian distribution. This is more likely if you are familiar with the process that generated the observations and you believe it to be a Gaussian process, or the distribution looks almost Gaussian, except for some distortion.\n\nIn this tutorial, you will discover the reasons why a Gaussian-like distribution may be distorted and techniques that you can use to make a data sample more normal.\n\n##### Power Transforms\nThe distribution of the data may be normal, but the data may require a transform in order to help expose it.\n\nFor example, the data may have a skew, meaning that the bell in the bell shape may be pushed one way or another. In some cases, this can be corrected by transforming the data via calculating the square root of the observations.\n\nAlternately, the distribution may be exponential, but may look normal if the observations are transformed by taking the natural logarithm of the values. Data with this distribution is called log-normal.\n\nTo make this concrete, below is an example of a sample of Gaussian numbers transformed to have an exponential distribution.\n\nTaking the square root and the logarithm of the observation in order to make the distribution normal belongs to a class of transforms called power transforms. The Box-Cox method is a data transform method that is able to perform a range of power transforms, including the log and the square root. The method is named for George Box and David Cox.\n\nMore than that, it can be configured to evaluate a suite of transforms automatically and select a best fit. It can be thought of as a power tool to iron out power-based change in your data sample. The resulting data sample may be more linear and will better represent the underlying non-power distribution, including Gaussian.\n\nThe boxcox() SciPy function implements the Box-Cox method. It takes an argument, called lambda, that controls the type of transform to perform.\n\nBelow are some common values for lambda:\n\n* lambda = -1. is a reciprocal transform.\n* lambda = -0.5 is a reciprocal square root transform.\n* lambda = 0.0 is a log transform.\n* lambda = 0.5 is a square root transform.\n* lambda = 1.0 is no transform.\n\nFor example, because we know that the data is lognormal, we can use the Box-Cox to perform the log transform by setting lambda explicitly to 0.","0fb79ec2":"### Question: Is it better to impute as new columns or clean existing columns?\n\n##### It depends. Normally in production data pipelines, I overwrite. However, in EDA purposes of this demo, I create new column.","d139292e":"# Q-Q Plots\n\nVisual inspection can be done in a different way with Q-Q plots. The red straight line is the fitted theoretical Gaussian distribution function. If the scatter plot is closer to the red straight line, it means that the data is very close to Gaussian distribution. Deviation from the red line indicates that the data is most likely not Gaussian.","681b05aa":"# Checking NAs\/Nulls","755b31a7":"![skw.png](attachment:29f62c15-4c56-4c63-9e41-2301cca7dcee.png)","0888e2ce":"# Duplicate Identification","5bca4f1f":"# Basic EDA","a3819d7b":"![test_score_dist.png](attachment:cb32245b-b6a7-4d31-808a-f5f6b08d3239.png)\n\nhttps:\/\/aegis4048.github.io\/transforming-non-normal-distribution-to-normal-distribution","6061b656":"# Normalize Values","207cc904":"# Outlier Detection\n\n#### There are two beginner ways to identify outliers: IQR and Z-Score","a4806223":"![Z Score.PNG](attachment:2b3162f3-b3f4-40d5-8238-37358e9d6921.PNG)","327fa0f3":"# Z Scores","91ac54de":"# Transform to Normal Distribution","ff8a2a7c":"## We have four options here:\n#### 1. Fill to predefined value\n#### 2. Fill to median\/mean\/mode\n#### 3. Drop entirely\n#### 4. Document but do nothing\n\n"}}