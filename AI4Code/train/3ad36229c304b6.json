{"cell_type":{"0106d146":"code","4dbb7b03":"code","b692ee56":"code","1d18c1e6":"code","a89b6b10":"code","1e7b2910":"code","0c63797b":"code","ab056e1f":"code","54c25de6":"code","f9d29491":"code","3a66b3d6":"code","88b51a14":"code","1769363c":"code","a92a51c7":"code","7e9ecb5d":"code","413f4187":"code","289c7e21":"code","d968d266":"code","72feaf02":"code","0204a8e5":"code","07bc342a":"code","0f243a42":"code","15954401":"code","7e21e04d":"code","27744b55":"code","bfa81049":"code","edc23ad4":"code","444d7026":"code","cb297ffc":"code","44bb0add":"code","c8fa1656":"code","004079af":"code","c462c472":"code","81a24ca1":"code","6874b286":"code","79d590bf":"markdown","c2a774d4":"markdown","95d393bd":"markdown","482691bb":"markdown","5e0b82a3":"markdown","b4b9cd1e":"markdown","78fd0395":"markdown","6161a434":"markdown","d5bf5fc2":"markdown","cff46b21":"markdown","86fc4d23":"markdown"},"source":{"0106d146":"# credit to this kernel: https:\/\/www.kaggle.com\/vishwas21\/tps-oct-21-eda-modeling\/notebook\n# I modified from there.","4dbb7b03":"import os\nimport gc\ngc.enable()\nimport time\nimport random\nimport warnings\nimport pdb\nfrom datetime import datetime\nimport pickle\n\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn import impute\nfrom sklearn import metrics\nfrom sklearn import ensemble\nfrom sklearn import linear_model\nfrom sklearn import decomposition\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\n\n","b692ee56":"today = int(str(datetime.now().year) + \n str(datetime.now().month).rjust(2, '0') + \n str(datetime.now().day).rjust(2, '0'))\ntoday","1d18c1e6":"nrows = None\ntrees = 10000\n\n# nrows = 1000\n# trees = 10\nverbose=True","a89b6b10":"warnings.filterwarnings('ignore')\n\nSEED = today\nnp.random.seed(SEED)\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)\n\nsns.set_style(\"darkgrid\")\nmpl.rcParams['figure.dpi'] = 600\n%matplotlib inline","1e7b2910":"train_df = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/train.csv', \n                       nrows = nrows)\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv', nrows=nrows)\n\nprint('Quick view of training data: ')\ntrain_df.head()","0c63797b":"TARGET = 'target'\nFEATURES = [col for col in train_df.columns if col not in ['id', TARGET]]\n# # print(f'Training data:\\n\\t Number of rows: {train_df.shape[0]}, Number of columns: {train_df.shape[1]}')\n# # print(f'Testing data:\\n\\t Number of rows: {test_df.shape[0]}, Number of columns: {test_df.shape[1]}')\n","ab056e1f":"# print('Basic statistics of training data:')\n# train_df[FEATURES+[TARGET]].describe()","54c25de6":"# print('Basic statistics of testing data:')\n# test_df[FEATURES].describe()","f9d29491":"# print(f'Number of missing values in training data: {train_df.isna().sum().sum()}')\n# print(f'Number of missing values in testing data: {test_df.isna().sum().sum()}')","3a66b3d6":"# this is interesting.  It searches for categorical features by looking at the nunique values and calling\n# it categorical (which would include boolean) if its less than 25.  \n\n# df = pd.concat([train_df[FEATURES], test_df[FEATURES]], axis=0)\n\n# cat_features = [col for col in FEATURES if df[col].nunique() < 25]\n# cont_features = [col for col in FEATURES if df[col].nunique() >= 25]\n\n# del df\n# print(f'Total number of features: {len(FEATURES)}')\n# print(f'Number of categorical features: {len(cat_features)}')\n# print(f'Number of continuos features: {len(cont_features)}')\n\n# plt.pie([len(cat_features), len(cont_features)], \n#         labels=['Categorical', 'Continuos'],\n#         colors=['#76D7C4', '#F5B7B1'],\n#         textprops={'fontsize': 13},\n#         autopct='%1.1f%%')\n# plt.show()","88b51a14":"# this takes a long time.  How does this help me?\n\n# print(\"Feature distribution of continous features: \")\n# ncols = 5\n# nrows = int(len(cont_features) \/ ncols + (len(FEATURES) % ncols > 0))\n\n# fig, axes = plt.subplots(nrows, ncols, figsize=(18, 150), facecolor='#EAEAF2')\n\n# for r in range(nrows):\n#     for c in range(ncols):\n#         col = cont_features[r*ncols+c]\n#         sns.kdeplot(x=train_df[col], ax=axes[r, c], color='#58D68D', label='Train data')\n#         sns.kdeplot(x=test_df[col], ax=axes[r, c], color='#DE3163', label='Test data')\n#         axes[r, c].set_ylabel('')\n#         axes[r, c].set_xlabel(col, fontsize=8, fontweight='bold')\n#         axes[r, c].tick_params(labelsize=5, width=0.5)\n#         axes[r, c].xaxis.offsetText.set_fontsize(4)\n#         axes[r, c].yaxis.offsetText.set_fontsize(4)\n# plt.show()","1769363c":"# looks like all of these are actually binary.\n\n# print(\"Feature distribution of categorical features: \")\n# ncols = 5\n# nrows = int(len(cat_features) \/ ncols + (len(FEATURES) % ncols > 0))\n\n# fig, axes = plt.subplots(nrows, ncols, figsize=(18, 45), facecolor='#EAEAF2')\n\n# for r in range(nrows):\n#     for c in range(ncols):\n#         col = cat_features[r*ncols+c]\n#         sns.histplot(x=train_df[col], ax=axes[r, c], color='#58D68D', label='Train data')\n#         sns.histplot(x=test_df[col], ax=axes[r, c], color='#DE3163', label='Test data')\n#         axes[r, c].set_ylabel('')\n#         axes[r, c].set_xlabel(col, fontsize=8, fontweight='bold')\n#         axes[r, c].tick_params(labelsize=5, width=0.5)\n#         axes[r, c].xaxis.offsetText.set_fontsize(4)\n#         axes[r, c].yaxis.offsetText.set_fontsize(4)\n# plt.show()","a92a51c7":"# Looks like all the categorical features are binary.","7e9ecb5d":"# print(\"Target Distribution: \")\n\n# target_df = pd.DataFrame(train_df[TARGET].value_counts()).reset_index()\n# target_df.columns = [TARGET, 'count']\n\n# fig, ax = plt.subplots(1, 1, figsize=(25, 8), facecolor='#EAEAF2')\n# sns.barplot(y=TARGET, x='count', data=target_df, palette=['#58D68D', '#DE3163'], ax=ax, orient='h')\n# ax.set_xlabel('Count', fontsize=16)\n# ax.set_ylabel('Target', fontsize=16)\n# plt.show()","413f4187":"# each row gets a mean, std, min, and a max.  Nice.\n# easy and interesting\n\ntrain_df[\"mean\"] = train_df[FEATURES].mean(axis=1)\ntrain_df[\"std\"] = train_df[FEATURES].std(axis=1)\ntrain_df[\"min\"] = train_df[FEATURES].min(axis=1)\ntrain_df[\"max\"] = train_df[FEATURES].max(axis=1)\n\ntest_df[\"mean\"] = test_df[FEATURES].mean(axis=1)\ntest_df[\"std\"] = test_df[FEATURES].std(axis=1)\ntest_df[\"min\"] = test_df[FEATURES].min(axis=1)\ntest_df[\"max\"] = test_df[FEATURES].max(axis=1)\n\nFEATURES.extend(['mean', 'max', 'min', 'max'])","289c7e21":"def format_time(seconds):\n    \"\"\"\n    Formates time in human readable form\n\n    Args:\n        seconds: seconds passed in a process\n    Return:\n        formatted string in form of MM:SS or HH:MM:SS\n    \"\"\"\n    h = int(seconds \/\/ 3600)\n    m = int((seconds % 3600) \/\/ 60)\n    s = int(seconds % 60)\n\n    result = ''\n\n    if h > 0:\n        if h < 10:\n            h = '0' + str(h)\n        else:\n            h = str(h)\n        h += ' Hr'\n        result += h\n        result += ' '\n    \n    if m > 0:\n        if m < 10:\n            m = '0' + str(m)\n        else:\n            m = str(m)\n        m += ' min'\n        result += m\n        result += ' '\n\n    if s < 10:\n        s = '0' + str(s)\n    else:\n        s = str(s)\n    s += ' sec'\n    result += s\n    \n    return result\n\nINT8_MIN    = np.iinfo(np.int8).min\nINT8_MAX    = np.iinfo(np.int8).max\nINT16_MIN   = np.iinfo(np.int16).min\nINT16_MAX   = np.iinfo(np.int16).max\nINT32_MIN   = np.iinfo(np.int32).min\nINT32_MAX   = np.iinfo(np.int32).max\n\nFLOAT16_MIN = np.finfo(np.float16).min\nFLOAT16_MAX = np.finfo(np.float16).max\nFLOAT32_MIN = np.finfo(np.float32).min\nFLOAT32_MAX = np.finfo(np.float32).max\n\n\ndef memory_usage(data, detail=1):\n    if detail:\n        display(data.memory_usage())\n    memory = data.memory_usage().sum() \/ (1024*1024)\n    print(\"Memory usage : {0:.2f}MB\".format(memory))\n    return memory\n\n\ndef compress_dataset(data):\n    memory_before_compress = memory_usage(data, 0)\n    print()\n    length_interval      = 50\n    length_float_decimal = 4\n\n    print('='*length_interval)\n    for col in data.columns:\n        col_dtype = data[col][:100].dtype\n\n        if col_dtype != 'object':\n            print(\"Name: {0:24s} Type: {1}\".format(col, col_dtype))\n            col_series = data[col]\n            col_min = col_series.min()\n            col_max = col_series.max()\n\n            if col_dtype == 'float64':\n                print(\" variable min: {0:15s} max: {1:15s}\".format(str(np.round(col_min, length_float_decimal)), str(np.round(col_max, length_float_decimal))))\n                if (col_min > FLOAT16_MIN) and (col_max < FLOAT16_MAX):\n                    data[col] = data[col].astype(np.float16)\n                    print(\"  float16 min: {0:15s} max: {1:15s}\".format(str(FLOAT16_MIN), str(FLOAT16_MAX)))\n                    print(\"compress float64 --> float16\")\n                elif (col_min > FLOAT32_MIN) and (col_max < FLOAT32_MAX):\n                    data[col] = data[col].astype(np.float32)\n                    print(\"  float32 min: {0:15s} max: {1:15s}\".format(str(FLOAT32_MIN), str(FLOAT32_MAX)))\n                    print(\"compress float64 --> float32\")\n                else:\n                    pass\n                memory_after_compress = memory_usage(data, 0)\n                print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) \/ memory_before_compress))\n                print('='*length_interval)\n\n            if col_dtype == 'int64':\n                print(\" variable min: {0:15s} max: {1:15s}\".format(str(col_min), str(col_max)))\n                type_flag = 64\n                if (col_min > INT8_MIN\/2) and (col_max < INT8_MAX\/2):\n                    type_flag = 8\n                    data[col] = data[col].astype(np.int8)\n                    print(\"     int8 min: {0:15s} max: {1:15s}\".format(str(INT8_MIN), str(INT8_MAX)))\n                elif (col_min > INT16_MIN) and (col_max < INT16_MAX):\n                    type_flag = 16\n                    data[col] = data[col].astype(np.int16)\n                    print(\"    int16 min: {0:15s} max: {1:15s}\".format(str(INT16_MIN), str(INT16_MAX)))\n                elif (col_min > INT32_MIN) and (col_max < INT32_MAX):\n                    type_flag = 32\n                    data[col] = data[col].astype(np.int32)\n                    print(\"    int32 min: {0:15s} max: {1:15s}\".format(str(INT32_MIN), str(INT32_MAX)))\n                    type_flag = 1\n                else:\n                    pass\n                memory_after_compress = memory_usage(data, 0)\n                print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) \/ memory_before_compress))\n                if type_flag == 32:\n                    print(\"compress (int64) ==> (int32)\")\n                elif type_flag == 16:\n                    print(\"compress (int64) ==> (int16)\")\n                else:\n                    print(\"compress (int64) ==> (int8)\")\n                print('='*length_interval)\n\n    print()\n    memory_after_compress = memory_usage(data, 0)\n    print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) \/ memory_before_compress))\n    \n    return data\n","d968d266":"%%time\ntrain_df = compress_dataset(train_df)\ntest_df = compress_dataset(test_df)","72feaf02":"# every feature is scaled - even the categoricals.\n\n# every though everything is 0 to 1 (in this dataset at least), it's scaled anyways to surround 0.\n\nscaler = preprocessing.StandardScaler()\nfor col in FEATURES:\n    train_df[col] = scaler.fit_transform(train_df[col].to_numpy().reshape(-1,1))\n    test_df[col] = scaler.transform(test_df[col].to_numpy().reshape(-1,1))\n    \nX = train_df[FEATURES].to_numpy().astype(np.float32)\nY = train_df[TARGET].to_numpy().astype(np.float32)\nX_test = test_df[FEATURES].to_numpy().astype(np.float32)\n\ndel train_df, test_df\ngc.collect()\nnp.save(\"X_11Oct2021\", X, fix_imports=False)\nnp.save(\"Y_11Oct2021\", Y, fix_imports=False)\nnp.save(\"X_test_11Oct2021\", X_test, fix_imports=False)","0204a8e5":"X = np.load(\"X_11Oct2021.npy\")\nY = np.load(\"Y_11Oct2021.npy\")\nX_test = np.load(\"X_test_11Oct2021.npy\")\n","07bc342a":"# no early stopping.  Must have determined before that 9500 trees (for example in xgboost's case) is the \n# right amount.\n\n# in spite of no early stopping, an eval set is pulled out so that out of fold predictions are generated\n# for ensemblings presumably.  \n\nxgb_params = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'max_depth': 6,\n    'n_estimators': trees, #9500,\n    'learning_rate': 0.007279718158350149,\n    'subsample': 0.7,\n    'colsample_bytree': 0.2,\n    'colsample_bylevel': 0.6000000000000001,\n    'min_child_weight': 56.41980735551558,\n    'reg_lambda': 75.56651890088857,\n    'reg_alpha': 0.11766857055687065,\n    'gamma': 0.6407823221122686,\n    'tree_method': 'gpu_hist',\n    'gpu_id': 0,\n    'predictor': 'gpu_predictor',\n#     'verbosity': 2,\n}\n\nlgb_params = {\n    'objective' : 'binary',\n    'metric' : 'auc',\n    'num_leaves' : 7,\n    'learning_rate' : 0.08,\n    'device' : 'gpu',\n    'feature_pre_filter': False, \n    'reg_alpha': 9.314037635261775, \n    'reg_lambda': 0.10613573572440353,\n    'num_leaves': 7,\n    'colsample_bytree': 0.4, \n    'subsample': 0.8391963650875751, \n    'subsample_freq': 5, \n    'min_child_samples': 100,\n    'num_iterations': trees, #10000,\n    'n_estimators': 20000\n}\n\ncatb_params = {    \n    \"objective\": \"CrossEntropy\",\n    \"eval_metric\" : \"AUC\",\n    \"task_type\": \"GPU\",\n    \"grow_policy\": \"SymmetricTree\",\n    \"learning_rate\": 0.08,\n    \"n_estimators\":  trees, #10_000,\n    \"random_strength\" : 1.0,\n    \"max_bin\": 128,\n    \"l2_leaf_reg\": 0.002550319996478972,\n    \"max_depth\": 4,\n    \"min_data_in_leaf\": 193,\n    'verbose': 0\n}","0f243a42":"from xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nmodels = [\n\n    [CatBoostClassifier, catb_params, 'catb'],\n    [LGBMClassifier, lgb_params, 'lgbm'],\n    [XGBClassifier, xgb_params, 'xgb'],\n\n]","15954401":"# stratified kfolds versus regular one.  Makes sense.\n\n# I think shrinking the size of the data makes the training go faster. At least it appears that this xgb is moving\n# through the trees faster than mine.  I should test if it's the compression that does this or perhaps one of the\n# parameters\n\n# This runs each model twice with 2 different seeds interestingly.  \n\n# instead of training on the whole dataset, it's doing a prediction on the test \n# data using each of the folds training model * 1\/5.  Hmm.  \n\nfrom collections import defaultdict\n\noof_df = defaultdict(lambda : [])\nnum_boost_dict = defaultdict(lambda : [])\ntest_df = defaultdict(lambda : np.zeros((X_test.shape[0])))\n\nSEEDS = [today % 10000, today]\nN_FOLDS = 5\nstart = time.time()\n\nskfolds = model_selection.StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\n\nfor fold, (t, v) in enumerate(skfolds.split(X, Y)):\n    x_train, x_val = X[t], X[v]\n    y_train, y_val = Y[t], Y[v]\n    \n    oof_df[TARGET].extend(y_val)\n    print('-'*38, f\"\\n{'-'*15} FOLD-{fold} {'-'*15}\")\n    print('-'*38)\n\n    for i in range(len(SEEDS)):\n        for class_name, class_params, name in models:\n            tic = time.time()\n            if name in ['qda', 'gnb']:\n                if i > 0:\n                    continue\n            else:\n                class_params['random_state'] = SEEDS[i]\n\n            clf = class_name(**class_params)\n            if name in ['xgb', 'lgbm']:\n                clf = clf.fit(x_train, y_train, eval_set=[(x_val, y_val)], \n                              eval_metric='auc', verbose=verbose, early_stopping_rounds=trees)\n            elif name in ['catb']:\n                clf = clf.fit(x_train, y_train, eval_set=[(x_val, y_val)], \n                              verbose_eval=verbose) # early_stopping_rounds=10000)\n            else:\n                assert False\n\n            if name == 'xgb':\n                num_boost_rounds = (clf.best_iteration if clf.best_iteration != 0 else \n                                    class_params['n_estimators']-1)\n            if name == 'lgbm':\n                num_boost_rounds = (clf.best_iteration_ if clf.best_iteration_ is not None else \n                                    class_params['num_iterations'])                \n            elif name in ['catb']:\n                num_boost_rounds = clf.best_iteration_      \n            num_boost_dict[f'{name}_{SEEDS[i]}'].append(num_boost_rounds)  \n            if name == 'catb':\n                preds = clf.predict_proba(x_val, ntree_end=num_boost_rounds+1)[:, 1].tolist()\n                test_df[f'{name}_{SEEDS[i]}'] += (clf.predict_proba(\n                    X_test, ntree_end=num_boost_rounds+1)[:, 1] \/ N_FOLDS)\n            elif name == 'xgb':\n                preds = clf.predict_proba(x_val, ntree_limit=num_boost_rounds+1)[:, 1].tolist()\n                test_df[f'{name}_{SEEDS[i]}'] += (clf.predict_proba(\n                    X_test, ntree_limit=num_boost_rounds+1)[:, 1] \/ N_FOLDS)                \n            else:\n                preds = clf.predict_proba(x_val)[:, 1].tolist() # lgbm uses best iteration automatically.\n                test_df[f'{name}_{SEEDS[i]}'] += (clf.predict_proba(X_test)[:, 1] \/ N_FOLDS)\n            oof_df[f'{name}_{SEEDS[i]}'].extend(preds)\n            # \n\n            score = metrics.roc_auc_score(y_val, preds)\n            print(f\"MODEL: {name}\\tSEED: {SEEDS[i]}\\tSCORE: {score}\\tTIME: {format_time(time.time()-tic)}\")\n        \n            del clf\n            gc.collect()\n    del x_train, x_val, y_train, y_val\n    gc.collect()\n\nprint('='*38)\nfor k, v in oof_df.items():\n    if k != TARGET:\n        score = metrics.roc_auc_score(oof_df[TARGET], v)\n        print(f'Overall ROC AUC of {k}: {score}')\n        \noof_df = pd.DataFrame(oof_df)\ntest_df = pd.DataFrame(test_df)\n\nprint()\nprint(f'TOTAL TIME: {format_time(time.time() - start)}')","7e21e04d":"oof_df.head()","27744b55":"oof_df.to_csv('oof_df.csv', index=False)\npickle.dump(dict(num_boost_dict), open(\"num_boost_dict_10Oct2021.pkl\", \"wb\"))\ntest_df.to_csv('test_df.csv', index=False)","bfa81049":"oof_df = pd.read_csv('oof_df.csv')\ntest_df = pd.read_csv('test_df.csv')","edc23ad4":"# looks like it's building lots of ensemblers.  Hmm.\n\nFEATURES = [col for col in oof_df.columns if col not in [TARGET]]\nX = oof_df[FEATURES].to_numpy()\nY = oof_df[TARGET].to_numpy()\nX_test = test_df[FEATURES].to_numpy()","444d7026":"from xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegression,LinearRegression\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nmodels = [\n    [CatBoostClassifier, catb_params, 'catb'],\n    [QuadraticDiscriminantAnalysis, {}, 'qda'],\n    [GaussianNB, {}, 'gnb'],\n    [LogisticRegression, {}, 'log'],\n    [LinearRegression, {}, 'reg']\n]","cb297ffc":"from collections import defaultdict\n\noof_df = defaultdict(lambda : [])\ntest_df = defaultdict(lambda : np.zeros((X_test.shape[0])))\n\nN_FOLDS = 5\nstart = time.time()\n\nskfolds = model_selection.StratifiedKFold(n_splits=N_FOLDS, shuffle=False) #, random_state=SEED)\n\nfor fold, (t, v) in enumerate(skfolds.split(X, Y)):\n    x_train, x_val = X[t], X[v]\n    y_train, y_val = Y[t], Y[v]\n    \n    oof_df[TARGET].extend(y_val)\n    print('-'*38, f\"\\n{'-'*15} FOLD-{fold} {'-'*15}\")\n    print('-'*38)\n\n    for class_name, class_params, name in models:\n        tic = time.time()\n\n        clf = class_name(**class_params)\n        clf = clf.fit(x_train, y_train)\n        if name != 'reg':\n            preds = clf.predict_proba(x_val)[:, 1].tolist()\n            test_df[f'{name}'] += (clf.predict_proba(X_test)[:, 1] \/ N_FOLDS)\n        else:\n            preds = clf.predict(x_val).tolist()\n            test_df[f'{name}'] += (clf.predict(X_test) \/ N_FOLDS)   \n        \n        oof_df[f'{name}'].extend(preds)\n        score = metrics.roc_auc_score(y_val, preds)\n        print(f\"MODEL: {name}\\tSCORE: {score}\\tTIME: {format_time(time.time()-tic)}\")\n\n        del clf\n        gc.collect()\n        \n    del x_train, x_val, y_train, y_val\n    gc.collect()\n\nprint('='*38)\nfor k, v in oof_df.items():\n    if k != TARGET:\n        score = metrics.roc_auc_score(oof_df[TARGET], v)\n        print(f'Overall ROC AUC of {k}: {score}')\n        \noof_df = pd.DataFrame(oof_df)\ntest_df = pd.DataFrame(test_df)\n\nprint()\nprint(f'TOTAL TIME: {format_time(time.time() - start)}')","44bb0add":"oof_df.to_csv('oof_df_l2.csv', index=False)\ntest_df.to_csv('test_df_l2.csv', index=False)","c8fa1656":"oof_l1 = pd.read_csv('oof_df.csv')\ntest_l1 = pd.read_csv('test_df.csv')\n\nfor col in oof_l1:\n    if col != TARGET:\n        oof_df[col] = oof_l1[col]\n        test_df[col] = test_l1[col]\n        \noof_df.head()","004079af":"FEATURES = [col for col in oof_df.columns if col not in [TARGET]]\nX = oof_df[FEATURES].to_numpy()\nY = oof_df[TARGET].to_numpy()\nX_test = test_df[FEATURES].to_numpy()","c462c472":"from collections import defaultdict\n\ntest_preds = np.zeros((test_df.shape[0]))\noof_y = []\noof_preds = []\n\nN_FOLDS = 5\nstart = time.time()\n\nskfolds = model_selection.StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\nfor fold, (t, v) in enumerate(skfolds.split(X, Y)):\n    x_train, x_val = X[t], X[v]\n    y_train, y_val = Y[t], Y[v]\n    \n    print(f\"\\n{'-'*15} FOLD-{fold} {'-'*15}\")\n    tic = time.time()\n\n    clf = LinearRegression()\n    clf = clf.fit(x_train, y_train)\n    preds = clf.predict(x_val).tolist()\n    test_preds += (clf.predict(X_test) \/ N_FOLDS)\n    oof_y.extend(y_val.tolist())\n    oof_preds.extend(preds)    \n\n    score = metrics.roc_auc_score(y_val, preds)\n    \n    print(f\"MODEL: log\\tSCORE: {score}\\tTIME: {format_time(time.time()-tic)}\")\n    \n    del x_train, x_val, y_train, y_val, clf\n    gc.collect()\n\nscore = metrics.roc_auc_score(oof_y, oof_preds)\nprint(f'Overall ROC AUC of reg: {score}')\n\nprint()\nprint(f'TOTAL TIME: {format_time(time.time() - start)}')","81a24ca1":"submission = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv', nrows=nrows)\nsubmission[TARGET] = test_preds\n\nsubmission.head()","6874b286":"submission.to_csv('submission.csv', index=False)","79d590bf":"### Level - 3","c2a774d4":"## Data preprocessing","95d393bd":"### Level - 2","482691bb":"## Utils","5e0b82a3":"## Modeling","b4b9cd1e":"## Data","78fd0395":"## EDA","6161a434":"## Feature Engineering","d5bf5fc2":"## Submission","cff46b21":"## Imports","86fc4d23":"### Level - 1"}}