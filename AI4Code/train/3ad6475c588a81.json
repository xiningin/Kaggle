{"cell_type":{"537a58ab":"code","7e789715":"code","4ba561fe":"code","9fe5afc0":"code","c9dc41e6":"code","ff1672bf":"markdown","b14b8cec":"markdown","9b7c1c0d":"markdown","e504016c":"markdown","106810c2":"markdown","b1ceb8b0":"markdown","9263f380":"markdown"},"source":{"537a58ab":"import numpy as np\nimport random","7e789715":"def sigmoid(x):\n    return 1\/(1+np.exp(-x))\n    \ndef derivative_sigmoid(x):\n    return x*(1-x)\n    \ndef relu(x):\n    return np.maximum(0,x)\n    \ndef derivative_relu(x):\n    a=x\n    for i in range(x.shape[0]):\n        for j in range(x.shape[1]):\n            if x[i][j] >0:\n                a[i][j]=1\n            else:\n                a[i][j]=0\n    return a \n\ndef predict(w1,w2,w3,b1,b2,b3,x_test):\n    z1 = np.dot(w1,x_test.T)+b1\n    a1 = relu(z1)\n    z2 = np.dot(w2,a1)+b2\n    a2 = relu(z2)\n    z3 = np.dot(w3,a2)+b3\n    output = sigmoid(z3)\n    return output\n    \n    \n\nclass MYNN:\n    def __init__(self,layers,x,y,learning_rate=0.001):\n        \n        m = x.shape[0]\n        \n        self.w1 = np.random.rand(layers[1],layers[0])\n        self.b1 = np.random.rand(layers[1],1)\n        \n        self.w2 = np.random.rand(layers[2],layers[1])\n        self.b2 = np.random.rand(layers[2],1)\n        \n        self.w3 = np.random.rand(layers[3],layers[2])\n        self.b3 = np.random.rand(layers[3],1)\n        \n        self.input = x.T\n        self.y = y\n        \n        self.output = np.zeros(y.shape)\n        \n        self.alpha = learning_rate\n        \n    \n    def forwardpropogation(self):\n        \n        self.z1 = np.dot(self.w1,self.input)+self.b1\n        self.a1 = relu(self.z1)\n        \n        self.z2 = np.dot(self.w2,self.a1)+self.b2\n        self.a2 =  relu(self.z2)\n        \n        self.z3 = np.dot(self.w3,self.a2)+self.b3\n        self.output = sigmoid(self.z3)\n    \n    def backwardpropogation(self):\n\n        self.dz3 = self.output-self.y\n        self.dw3 = 1\/self.y.shape[1]*np.dot(self.dz3,self.a2.T)\n        self.db3 = 1\/self.y.shape[1]*np.sum(self.dz3,axis=1,keepdims=True)\n        \n        self.dz2 = np.dot(self.dw3.T,self.dz3)*derivative_relu(self.z2)\n        self.dw2 = 1\/self.y.shape[1]*np.dot(self.dz2,self.a1.T)\n        self.db2 = 1\/self.y.shape[1]*np.sum(self.dz2,axis=1,keepdims=True)\n        \n        self.dz1 = np.dot(self.dw2.T,self.dz2)*derivative_relu(self.z1)\n        self.dw1 = 1\/self.y.shape[1]*np.dot(self.dz1,self.input.T)\n        self.db1 = 1\/self.y.shape[1]*np.sum(self.dz1,axis=1,keepdims=True)\n        \n        self.w1=self.w1-self.alpha*self.dw1\n        self.b1=self.b1-self.alpha*self.db1\n        \n        self.w2=self.w2-self.alpha*self.dw2\n        self.b2=self.b2-self.alpha*self.db2\n        \n        self.w3=self.w3-self.alpha*self.dw3\n        self.b3=self.b3-self.alpha*self.db3\n        \n        return self.w1,self.b1,self.w2,self.b2,self.w3,self.b3\n        \n    def cost(self):\n        cost=-1\/self.y.shape[1]*np.sum((self.y*np.log(self.output)+(1-self.y)*np.log(1-self.output)))\n        print(cost)\n        return cost","4ba561fe":"layers=[3,4,5,1]\nx=np.array([[0,1,2],[1,2,1],[3,4,6],[0,0,1],[1,0,1],[1,5,5],[6,1,4],[1,1,2],[2,1,1],[3,1,8]])\n\ny=np.array([[0,0,1,0,0,1,1,0,0,1]])\nprint(x.shape)\nnn= MYNN(layers,x,y)\ncost=[]\nfor i in range(20000):\n    nn.forwardpropogation()\n    w1,b1,w2,b2,w3,b3=nn.backwardpropogation()\n    cost.append(nn.cost())","9fe5afc0":"print(w1,b1,w2,b2,w3,b3)","c9dc41e6":"x_test=np.array([[1,0,1],[7,7,5]])\nx_test.shape\nprint(predict(w1,w2,w3,b1,b2,b3,x_test))","ff1672bf":"![1_akHYKaEdaOUazZBmdoNlzg.png](attachment:1_akHYKaEdaOUazZBmdoNlzg.png)","b14b8cec":"Now let us test the neural network with some data.\n\nSince the sum of elements in the first testing example is less than 5, it's sigmoid output should be close to 0.\n\nSince the sum of elements in the second testing example is more than 5, it's sigmoid output should be close to 1.","9b7c1c0d":"**Creating a three layered neural network. The first two layers will have relu activation function and the third layer will have sigmoid activation function.**\n\nThe class MYNN has forward propagation and backward propagation functions which will be run for every iteration or for every step of neural network. The cost function will calculate the cost of neural network after every iteration.\n\nThe functions sigmoid and relu will be called during forward propagation.\n\nThe functions derivative_sigmoid and derivative_relu will be called during backward propagation.\n\nThe predict function will be called when testing the neural network on new data.","e504016c":"Let us consider a sample data. The data basically has 3 independent features and 1 target feature.\nWe have designed the dataset such that if the sum of all numbers in a row is less than 5 then the target is 0 and when the sum of numbers in a row is greater than 10 the the target is 1.\n\nWe have set the default learning rate as 0.001 and we will the training using same learning rate.\n\nSince the data is very small we will be training for more epochs.","106810c2":"Let's see the weights and bias of our neural network trained on the above data","b1ceb8b0":"**In this notebook we will build a neural network and then train the neural network using a simple data and then test it.**","9263f380":"The cost of neural network is calculated after each epoch and is appended to cost list."}}