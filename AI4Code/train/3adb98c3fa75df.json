{"cell_type":{"44b4f86f":"code","de802633":"code","04428d53":"code","b0c903d1":"code","2388bec1":"code","40d899a2":"code","8f46087e":"code","e141907f":"code","a9f07318":"code","0c9c3de0":"code","fbfd2fc6":"code","575cc40f":"code","a693a2bc":"code","3b87d9f4":"code","5a9384c3":"code","9db8e1d1":"code","0569cb2b":"code","b4a4c17e":"code","24156203":"code","172d60ce":"code","95ec4216":"code","15c5af8e":"code","a780b7ff":"code","a93d6b44":"code","53ea89b5":"code","c32e9b1e":"code","8966833b":"code","689386f4":"code","4b7d8d7d":"code","4d2c6d47":"code","8107ff86":"code","f68102db":"code","c71b14e8":"code","f23216f1":"code","62302beb":"code","66379158":"markdown","0ba88362":"markdown","3b0b54c9":"markdown","8f3382e7":"markdown","c133cc07":"markdown","42ae2437":"markdown","82789c2e":"markdown","5b1d4201":"markdown","78bd08e1":"markdown","762f08be":"markdown","a58b5c03":"markdown","caeca72e":"markdown","f98ea5a3":"markdown"},"source":{"44b4f86f":"# Import libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Review files in the folder\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Switch on setting to allow all outputs to be displayed\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","de802633":"# Create the initial training DataFrame\ntrain = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv')","04428d53":"# Review the first five observations\ntrain.head()","b0c903d1":"# Check the test dataset\ntest.head()","2388bec1":"# Drop the id variable\nX = train.drop(['id', 'loss'], axis=1)\ny = train['loss']\ntest_id = test['id']\ntest_x = test.drop(['id'], axis=1)\n\n# Review train and test after dropping id\nX.head()\ntest_x.head()","40d899a2":"# Shape of the dataframe\nprint(train.shape)\n# Find the number of rows within a dataframe\nprint(len(train))\n# Extracting information from the shape tuple\nprint(f'Number of rows: {train.shape[0]} \\nNumber of columns: {train.shape[1]}')","8f46087e":"# Review the high level summary details for each variable\ntrain.describe()","e141907f":"# Variable types\ntrain.dtypes.value_counts()\n# All variables are numeric so don't have to worry about working with strings","a9f07318":"# Proportion of missing values by column\ndef isnull_prop(df):\n    total_rows = df.shape[0]\n    missing_val_dict = {}\n    for col in df.columns:\n        missing_val_dict[col] = [df[col].isnull().sum(), (df[col].isnull().sum() \/ total_rows)]\n    return missing_val_dict\n\n# Apply the missing value method\nnull_dict = isnull_prop(train)","0c9c3de0":"# Create a dataframe of the missing value information\ndf_missing = pd.DataFrame.from_dict(null_dict, orient=\"index\", columns=['missing', 'miss_percent'])\ndf_missing.loc[(df_missing['missing'] > 0)]","fbfd2fc6":"# Method - review the distribution of the target variable\ndef sns_displot(df, col):\n    # set the histogram, mean and median\n    sns.displot(df[col], kde=False)\n    plt.axvline(x=df[col].mean(), linewidth=3, color='g', label=\"mean\", alpha=0.5)\n    plt.axvline(x=df[col].median(), linewidth=3, color='y', label=\"median\", alpha=0.5)\n\n    # set title, legends and labels\n    plt.xlabel(f'{col}')\n    plt.ylabel(\"Count\")\n    plt.title(f'Distribution of {col}', size=14)\n    plt.legend([\"mean\", \"median\"]);\n\n    print(f'Mean {col} value {df[col].mean()} \\n Median {col} value {df[col].median()} \\n Min {col} value {df[col].min()} \\n Max {col} value {df[col].max()}')","575cc40f":"sns_displot(train, 'loss')","a693a2bc":"# Lets understand the common values\nprint(f'Average rate of zero: {train.loc[(train.loss == 0), \"loss\"].count() \/ train.shape[0]}')\nprint(f'{train.loss.value_counts()}')","3b87d9f4":"# Import modules\n# Preprocessing\nfrom sklearn.preprocessing import scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Classifiers\n# from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\n# Performance metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\n\n# Dimension Reduction techniques\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import RFE\nfrom yellowbrick.model_selection import feature_importances\nfrom sklearn.feature_selection import SelectFromModel","5a9384c3":"# Lets create the binary y_target variable\ny_target = np.where(y > 0, 1, 0)\ny_target.view()\nprint(f'Proportion of loss values {np.average(y_target)}')","9db8e1d1":"sc = StandardScaler()\nX_scaled = sc.fit_transform(X)\n\npca = PCA(n_components=10)\nX_pca = pca.fit_transform(X_scaled)","0569cb2b":"# define the pipeline\nsteps = [\n        ('scaler', StandardScaler()),\n        ('pca', PCA(n_components=10)), \n        ('m', LogisticRegression())\n]\nmodel = Pipeline(steps=steps)\n# evaluate model\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(model, X, y_target, scoring='accuracy', cv=cv, n_jobs=-1)\n# report performance\nprint('Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))","b4a4c17e":"# Lets try a Random Forest\nrf = RandomForestClassifier(n_estimators=100, max_depth=3,\n                                 bootstrap=True, n_jobs=-1,\n                                 random_state=0)\nsc = StandardScaler()\nX_scaled = sc.fit_transform(X)\n\nrf.fit(X_scaled, y_target)\n\nfeature_imp = pd.Series(rf.feature_importances_, \n                        index=X.columns).sort_values(ascending=False)\n\nprint('Feature importances: ', rf.feature_importances_)\nprint(sns.barplot(x=feature_imp, y=feature_imp.index))\nplt.xlabel('Feature Importance Score', fontsize=12)\nplt.ylabel('Features', fontsize=12)\nplt.title(\"Visualizing Important Features\", fontsize=15, pad=15)","24156203":"# Lets put a threshold on the feature importance score\nselector = SelectFromModel(rf, threshold=0.05)\nfeatures_important = selector.fit_transform(X, y_target)\n\nX_vars = X.loc[:, selector.get_support()]\nX_vars.head()","172d60ce":"# Lets try reviewing the data with the standard scaler switched off\n# Lets try a Random Forest\nrf = RandomForestClassifier(n_estimators=100, max_depth=3,\n                                 bootstrap=True, n_jobs=-1,\n                                 random_state=0)\n\nrf.fit(X, y_target)\n\nfeature_imp = pd.Series(rf.feature_importances_, \n                        index=X.columns).sort_values(ascending=False)\n\nprint('Feature importances: ', rf.feature_importances_)\nprint(sns.barplot(x=feature_imp, y=feature_imp.index))\nplt.xlabel('Feature Importance Score', fontsize=12)\nplt.ylabel('Features', fontsize=12)\nplt.title(\"Visualizing Important Features\", fontsize=15, pad=15)","95ec4216":"# Lets put a threshold on the feature importance score\nselector = SelectFromModel(rf, threshold=0.05)\nfeatures_important = selector.fit_transform(X, y_target)\n\nX_vars1 = X.loc[:, selector.get_support()]\nX_vars1.head()","15c5af8e":"# Create the train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_vars1, \n                                                    y, \n                                                    test_size=0.3, \n                                                    random_state=42)\n# X_train, X_test, y_train, y_test = train_test_split(X_vars, \n#                                                     y, \n#                                                     test_size=0.3, \n#                                                     random_state=42)","a780b7ff":"# Create the evaluation metric - RMSE\nfrom sklearn.metrics import mean_squared_error as mse\n\ndef rmse(actual, predicted):\n    mse_val = mse(actual, predicted)\n    return np.sqrt(mse_val)","a93d6b44":"#train the NB2 model on the training data set\nimport statsmodels.api as sm\n\nneg_bin = sm.GLM(y_train, X_train,family=sm.families.NegativeBinomial()).fit()\n\n#print the training summary\nprint(neg_bin.summary())","53ea89b5":"# Lets predict with a negative binomial model\ny_pred = np.round(neg_bin.predict(X_test))\nrmse(y_test, y_pred)","c32e9b1e":"y_pred.view()\ny_test.head()","8966833b":"# Train the Zero Inflated Poisson model\nzip_reg = sm.ZeroInflatedPoisson(endog=y_train, exog=X_train, exog_infl=X_train, inflation='logit').fit()\nprint(zip_reg.summary())","689386f4":"# Poisson Regression\nfrom sklearn.linear_model import PoissonRegressor\n\npoisson_glm = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"regressor\", PoissonRegressor(alpha=1e-12, max_iter=300))\n])\npoisson_glm.fit(X_train, y_train)\ny_pred = poisson_glm.predict(X_test)\nrmse(y_test, y_pred)","4b7d8d7d":"# Predictions\nzip_predictions = zip_reg.predict(X_test,exog_infl=X_test)\npredicted_counts=np.round(zip_predictions)\nprint(f'RMSE : {rmse(y_test, predicted_counts)}')","4d2c6d47":"# # Work to do to get this working correctly\n# fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(16, 6), sharey=True)\n# fig.subplots_adjust(bottom=0.2)\n# n_bins = 20\n# for row_idx, label, df in zip(range(2),\n#                               [\"train\", \"test\"],\n#                               [y_train, y_test]):\n#     df.hist(bins=np.linspace(-1, 30, n_bins),\n#                          ax=axes[row_idx, 0])\n\n#     axes[row_idx, 0].set_title(\"Data\")\n#     axes[row_idx, 0].set_yscale('log')\n#     axes[row_idx, 0].set_xlabel(\"y (observed Frequency)\")\n#     axes[row_idx, 0].set_ylim([1e1, 5e5])\n#     axes[row_idx, 0].set_ylabel(label + \" samples\")\n\n#     for idx, model in enumerate([dummy, ridge, poisson_glm]):\n#         y_pred = model.predict(X_test)\n\n#         pd.Series(y_pred).hist(bins=np.linspace(-1, 4, n_bins),\n#                                ax=axes[row_idx, idx+1])\n#         axes[row_idx, idx + 1].set(\n#             title=model[-1].__class__.__name__,\n#             yscale='log',\n#             xlabel=\"y_pred (predicted expected Loss)\"\n#         )\n# plt.tight_layout();","8107ff86":"# Create model submission method using the test_x and test_id variables\ndef submission(model, csv_name):\n    pred = model.predict(test_x)\n    df = pd.DataFrame(data={'id': test_id, 'loss': pred})\n    df = df.set_index('id')\n    return df.to_csv(f\"Submission_file_{csv_name}.csv\")","f68102db":"# Create the code for the ZIP prediction\ntest_x_new = test_x.loc[:, X_train.columns]","c71b14e8":"# submission(neg_bin, \"neg_bin_glm\")","f23216f1":"# Scaled datasets\ndef submission_scaled(model, csv_name):\n    pred = model.predict(test_x_new)\n    df = pd.DataFrame(data={'id': test_id, 'loss': pred})\n    df = df.set_index('id')\n    return df.to_csv(f\"Submission_file_{csv_name}.csv\")","62302beb":"# submission_scaled(neg_bin, \"neg_bin\")\n# submission_scaled(zip_reg, \"zip_reg\")\nsubmission_scaled(poisson_glm, \"poisson\")","66379158":"# Poisson","0ba88362":"## Missing values review","3b0b54c9":"# Dimension Reduction - Binary classifier","8f3382e7":"## Make submission","c133cc07":"There is a positive skew present as the mean is greater than the median. A large proportion of the values are zero so not all rows have experienced the same loss. ","42ae2437":"# Model analysis - Negative Binomial","82789c2e":"It might make more sense to split the challenge into two separate issues. First predict if there was a loss or not. Assign 0 to loss values of 0 and 1 to all other values. Then a second element of the task would be to predict the loss for values greater than zero.\n***\n1. The first challenge would be a binary logistic regression task\n2. Perform a poisson regression to predict the losses greater than 0\n***\nThis task can be achieved by using the Zero-Inflated Poisson Regression","5b1d4201":"## Baseline model\nChecking the sample_submission.csv within the public leaderboard shows a Root Mean Squared Error score of 10.53201. Aim is to perform initial EDA and build a few baseline models and begin to perform hyperparameter tuning.","78bd08e1":"# EDA","762f08be":"Before we get into Model Predictions we need to understand which independent variables help to predict the binary loss classifier. By converting the target variable to a binary classifier first we can explore which models help to predict loss before we aim to predict the loss value","a58b5c03":"# Model analysis - Zero Inflated Poisson regression","caeca72e":"## Target variable - Loss","f98ea5a3":"# Tabular Playground series - August 2021\n\nThe dataset is used for this competition is synthetic, but based on a real dataset and generated using a [CTGAN](https:\/\/github.com\/sdv-dev\/CTGAN). The original dataset deals with calculating the loss associated with a loan defaults. Although the features are anonymized, they have properties relating to real-world features."}}