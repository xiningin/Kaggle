{"cell_type":{"4cd03d36":"code","4622530d":"code","1c451755":"code","f1b15443":"code","ee43815b":"code","dbc2e057":"code","6290bcde":"code","c0d8aab5":"code","75bb2779":"code","2fed12ba":"code","509efef5":"code","88dbc828":"code","e17ac239":"code","5bab6006":"code","993d21d5":"code","dffe5934":"code","4d8290de":"code","cb0ef395":"code","5f5c2bd7":"code","018001e1":"code","9ccc9c38":"code","3d79fb56":"code","b8b483c6":"code","5c80ac96":"code","b97b1819":"code","c6f4fa41":"code","6996f3fd":"code","a57a9ce3":"code","2ed9d39d":"code","b6bdb8b2":"code","fa57f1f9":"code","1a6f0002":"code","3177bf6f":"code","a808b3c0":"code","df4c75ff":"markdown","53d66be7":"markdown","60b3aea5":"markdown","fce37946":"markdown","9b9df2a3":"markdown","b6c9f8ba":"markdown","2e17da16":"markdown","416adc6b":"markdown","946396ca":"markdown","e8f75f98":"markdown","77c0b8cb":"markdown","ce28059d":"markdown","ef3bfb23":"markdown","45c82e77":"markdown","b550ebe2":"markdown","434141fa":"markdown","8fb6645d":"markdown","a6a708e3":"markdown","92a74d5c":"markdown","f5193533":"markdown","51599906":"markdown","7e47b86e":"markdown","5259b0e8":"markdown","54ef29e1":"markdown","e4255a08":"markdown","82e25c06":"markdown","f79cc21f":"markdown","3490a0fe":"markdown","ce485386":"markdown"},"source":{"4cd03d36":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4622530d":"# Additional Imports \n\n# essential libraries\nimport math\nimport random\nfrom datetime import timedelta\nfrom IPython.core.display import HTML\n#import googlemaps\nfrom datetime import datetime\n\n\n\n# storing and anaysis\nimport numpy as np\nimport pandas as pd\n\n\n%matplotlib inline\n\n\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport folium\n\n# converter\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters() \n\n#offline plotting \nfrom plotly.offline import plot, iplot, init_notebook_mode\ninit_notebook_mode(connected=True)","1c451755":"from scipy import signal\n\ndef get2deriv(df, index, column, value):\n    #pivot table of 2nd derive (growth factor)\n    newdf = df.pivot_table(\n        index=index, columns=column, values=value, aggfunc=np.sum\n    ).fillna(method=\"ffill\").fillna(0)\n    # Growth factor: (delta Number_n) \/ (delta Number_n)\n    #newdf['shift1'] = newdf.shift(1, axis=0)\n    #newdf['shift2'] = newdf.shift(2, axis=0)\n    newdf =  (newdf-newdf.shift(1, axis=0))\/(newdf.shift(1, axis=0)-newdf.shift(2, axis=0))\n    #newdf = newdf.drop(['shift1', 'shift2', 'value'], axis=1)\n    \n    newdf = newdf.replace(np.inf, np.nan).fillna(1.0)\n    # Rolling mean (window: 7 days)\n    newdf = newdf.rolling(3).mean().dropna().loc[:df[index].max(), :]\n    # round: 0.01\n    growth_value_df = newdf.round(2)\n    #growth_value_df = smoothergf(growth_value_df, 0.5, 5)\n    growth_value_df.tail()\n    \n    frame = growth_value_df.copy()\n    N, K = frame.shape\n    data = {'2ndderiv': frame.to_numpy().ravel('F'),\n            column: np.asarray(frame.columns).repeat(N),\n            index: np.tile(np.asarray(frame.index), K)}\n    return pd.DataFrame(data, columns=[index, column, '2ndderiv'])\n\ndef get2derivworld(df, index, value):\n    #pivot table of 2nd derive (growth factor)\n    newdf = df.pivot_table(\n        index=index, values=value, aggfunc=np.sum\n    ).fillna(method=\"ffill\").fillna(0)\n    # Growth factor: (delta Number_n) \/ (delta Number_n)\n    #newdf['shift1'] = newdf.shift(1, axis=0)\n    #newdf['shift2'] = newdf.shift(2, axis=0)\n    newdf =  (newdf-newdf.shift(1, axis=0))\/(newdf.shift(1, axis=0)-newdf.shift(2, axis=0))\n    #newdf = newdf.drop(['shift1', 'shift2', 'value'], axis=1)\n    \n    newdf = newdf.replace(np.inf, np.nan).fillna(1.0)\n    # Rolling mean (window: 7 days)\n    newdf = newdf.rolling(3).mean().dropna().loc[:df[index].max(), :]\n    # round: 0.01\n    growth_value_df = newdf.round(2)\n    #growth_value_df = smoothergf(growth_value_df, 0.5, 5)\n    growth_value_df.tail()\n    \n    frame = growth_value_df.copy()\n    N, K = frame.shape\n    data = {'2ndderiv': frame.to_numpy().ravel('F'),\n            index: np.tile(np.asarray(frame.index), K)}\n    return pd.DataFrame(data, columns=[index, '2ndderiv'])\n\ndef get1deriv(df, index, column, value):\n    #pivot table of 2nd derive (growth factor)\n    newdf = df.pivot_table(\n        index=index, columns=column, values=value, aggfunc=\"sum\"\n    ).fillna(method=\"ffill\").fillna(0)\n    # Growth factor: (delta Number_n) \/ (delta Number_n)\n    newdf = newdf.diff()\n    newdf = newdf.replace(np.inf, np.nan).fillna(1.0)\n    # Rolling mean (window: 7 days)\n    newdf = newdf.rolling(3).mean().dropna().loc[:df[index].max(), :]\n    # round: 0.01\n    growth_value_df = newdf.round(2)\n    #growth_value_df = smoothergf(growth_value_df, 0.5, 3)\n    growth_value_df.tail()\n    \n    frame = growth_value_df.copy()\n    N, K = frame.shape\n    data = {'1stderiv': frame.to_numpy().ravel('F'),\n            column: np.asarray(frame.columns).repeat(N),\n            index: np.tile(np.asarray(frame.index), K)}\n    return pd.DataFrame(data, columns=[index, column,'1stderiv'])\n\ndef get1derivworld(df, index, value):\n    #pivot table of 2nd derive (growth factor)\n    newdf = df.pivot_table(\n        index=index,values=value, aggfunc=\"sum\"\n    ).fillna(method=\"ffill\").fillna(0)\n    # Growth factor: (delta Number_n) \/ (delta Number_n)\n    newdf = newdf.diff()\n    newdf = newdf.replace(np.inf, np.nan).fillna(1.0)\n    # Rolling mean (window: 7 days)\n    newdf = newdf.rolling(3).mean().dropna().loc[:df[index].max(), :]\n    # round: 0.01\n    growth_value_df = newdf.round(2)\n    #growth_value_df = smoothergf(growth_value_df, 0.5, 3)\n    growth_value_df.tail()\n    \n    frame = growth_value_df.copy()\n    N, K = frame.shape\n    data = {'1stderiv': frame.to_numpy().ravel('F'),\n            index: np.tile(np.asarray(frame.index), K)}\n    return pd.DataFrame(data, columns=[index, '1stderiv'])\n\ndef getdaily(df, index, column, value):\n    #pivot table of 2nd derive (growth factor)\n    newdf = df.pivot_table(\n        index=index, columns=column, values=value, aggfunc=\"sum\"\n    ).fillna(method=\"ffill\").fillna(0)\n    # Growth factor: (delta Number_n) \/ (delta Number_n)\n    newdf = newdf.diff()\n    newdf = newdf.replace(np.inf, np.nan).fillna(1.0)\n    # Rolling mean (window: 7 days)\n    newdf = newdf.rolling(3).mean().dropna().loc[:df[index].max(), :]\n    # round: 0.01\n    growth_value_df = newdf.round(2)\n    growth_value_df.tail()\n    \n    frame = growth_value_df.copy()\n    N, K = frame.shape\n    data = {'daily_case': frame.to_numpy().ravel('F'),\n            column: np.asarray(frame.columns).repeat(N),\n            index: np.tile(np.asarray(frame.index), K)}\n    return pd.DataFrame(data, columns=[index, column,'daily_case'])\n\ndef getlag(df, index, column, value):\n    #pivot table of 2nd derive (growth factor)\n    newdf = df.pivot_table(\n        index=index, columns=column, values=value, aggfunc=np.sum\n    ).fillna(method=\"ffill\").fillna(0)\n    # Growth factor: (delta Number_n) \/ (delta Number_n)\n    newdf = newdf.shift(-7, axis=0)\n    \n    newdf = newdf.replace(np.inf, np.nan).fillna(0.0)\n    # Rolling mean (window: 7 days)\n    #newdf = newdf.rolling(3).mean().dropna().loc[:df[index].max(), :]\n    # round: 0.01\n    growth_value_df = newdf\n    \n    growth_value_df.tail()\n    \n    frame = growth_value_df.copy()\n    N, K = frame.shape\n    data = {'lag': frame.to_numpy().ravel('F'),\n            column: np.asarray(frame.columns).repeat(N),\n            index: np.tile(np.asarray(frame.index), K)}\n    return pd.DataFrame(data, columns=[index, column,'lag'])\n\ndef getlagdeath(df, index, column, value):\n    #pivot table of 2nd derive (growth factor)\n    newdf = df.pivot_table(\n        index=index, columns=column, values=value, aggfunc=np.sum\n    ).fillna(method=\"ffill\").fillna(0)\n    # Growth factor: (delta Number_n) \/ (delta Number_n)\n    newdf = newdf.shift(-7, axis=0)\n    \n    newdf = newdf.replace(np.inf, np.nan).fillna(0.0)\n    # Rolling mean (window: 7 days)\n    #newdf = newdf.rolling(3).mean().dropna().loc[:df[index].max(), :]\n    # round: 0.01\n    growth_value_df = newdf\n    \n    growth_value_df.tail()\n    \n    frame = growth_value_df.copy()\n    N, K = frame.shape\n    data = {'lag_death': frame.to_numpy().ravel('F'),\n            column: np.asarray(frame.columns).repeat(N),\n            index: np.tile(np.asarray(frame.index), K)}\n    return pd.DataFrame(data, columns=[index, column,'lag_death'])\n\n#global measure\ndef add_daily_measures(df):\n    df.loc[0,'Daily Cases'] = df.loc[0,'ConfirmedCases']\n    df.loc[0,'Daily Deaths'] = df.loc[0,'Fatalities']\n    for i in range(1,len(df)):\n        df.loc[i,'Daily Cases'] = df.loc[i,'ConfirmedCases'] - df.loc[i-1,'ConfirmedCases']\n        df.loc[i,'Daily Deaths'] = df.loc[i,'Fatalities'] - df.loc[i-1,'Fatalities']\n    #Make the first row as 0 because we don't know the previous value\n    df.loc[0,'Daily Cases'] = 0\n    df.loc[0,'Daily Deaths'] = 0\n    return df\n\n#smoothens out data for plotting (one line)\ndef smoother(df, index, col):\n    noise = 2 * np.random.random(len(df[index])) - 1 # uniformly distributed between -1 and 1\n    y_noise = df[col] + noise\n    y_col = signal.savgol_filter(y_noise, 53, 3)\n    return y_col\n\n#smoothens growth factor \ndef smoothergf(inputdata,w,imax):\n    data = 1.0*inputdata\n    data = data.replace(np.nan,1)\n    data = data.replace(np.inf,1)\n    #print(data)\n    smoothed = 1.0*data\n    normalization = 1\n    for i in range(-imax,imax+1):\n        if i==0:\n            continue\n        smoothed += (w**abs(i))*data.shift(i,axis=0)\n        normalization += w**abs(i)\n    smoothed \/= normalization\n    return smoothed\n\n\n################## only for treemap ###############\n\nclass country_utils():\n    def __init__(self):\n        self.d = {}\n    \n    def get_dic(self):\n        return self.d\n    \n    def get_country_details(self,country):\n        \"\"\"Returns country code(alpha_3) and continent\"\"\"\n        try:\n            country_obj = pycountry.countries.get(name=country)\n            if country_obj is None:\n                c = pycountry.countries.search_fuzzy(country)\n                country_obj = c[0]\n            continent_code = pc.country_alpha2_to_continent_code(country_obj.alpha_2)\n            continent = pc.convert_continent_code_to_continent_name(continent_code)\n            return country_obj.alpha_3, continent\n        except:\n            if 'Congo' in country:\n                country = 'Congo'\n            elif country == 'Diamond Princess' or country == 'Laos' or country == 'MS Zaandam'\\\n            or country == 'Holy See' or country == 'Timor-Leste':\n                return country, country\n            elif country == 'Korea, South' or country == 'South Korea':\n                country = 'Korea, Republic of'\n            elif country == 'Taiwan*':\n                country = 'Taiwan'\n            elif country == 'Burma':\n                country = 'Myanmar'\n            elif country == 'West Bank and Gaza':\n                country = 'Gaza'\n            else:\n                return country, country\n            country_obj = pycountry.countries.search_fuzzy(country)\n            continent_code = pc.country_alpha2_to_continent_code(country_obj[0].alpha_2)\n            continent = pc.convert_continent_code_to_continent_name(continent_code)\n            return country_obj[0].alpha_3, continent\n    \n    def get_iso3(self, country):\n        return self.d[country]['code']\n    \n    def get_continent(self,country):\n        return self.d[country]['continent']\n    \n    def add_values(self,country):\n        self.d[country] = {}\n        self.d[country]['code'],self.d[country]['continent'] = self.get_country_details(country)\n    \n    def fetch_iso3(self,country):\n        if country in self.d.keys():\n            return self.get_iso3(country)\n        else:\n            self.add_values(country)\n            return self.get_iso3(country)\n        \n    def fetch_continent(self,country):\n        if country in self.d.keys():\n            return self.get_continent(country)\n        else:\n            self.add_values(country)\n            return self.get_continent(country)","f1b15443":"#added temp variance to weather_country\nweather_country = pd.read_csv('\/kaggle\/input\/weather-data-5\/training_data_with_weather_info_week_5.csv', parse_dates=['Date'])\nweather_country = weather_country.rename(columns={\"country+province\": \"country_province\"})\nweather_country['Date'] = pd.to_datetime(weather_country['Date'], format = '%Y-%m-%d')\nweather_country['temp variance'] = weather_country['max'] - weather_country['min']\nweather_country = weather_country.drop(columns = ['stp', 'slp', 'dewp'])\n#weather_country['LatLong'] = \"(\"+ str(weather_country['Lat'].round(5)) + \",\" + str(weather_country['Long'].round(5)) + \")\"\n#weather_country['LatLong'].sample(10)\nweather_country[\"latlong\"] = list(zip(weather_country.Lat.round(6), weather_country.Long.round(6)))\nweather_country.sample(1)\n\n#c = weather_country.loc[weather_country['Country_Region'] == 'US']\n","ee43815b":"#added population and density per country to weather_country\npp = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-locations-population\/locations_population.csv\")\npop = pd.read_csv(\"..\/input\/population-by-country-2020\/population_by_country_2020.csv\")\n# select only population\npop = pop.iloc[:, :10]\n# rename column names\npop.columns = ['Country_Region', 'Population', 'Year Change', 'Net Change', 'Density P\/km^2', 'Land Area','Migrants','Fert. Rate','Med. Age', 'Urban Pop %']\npop = pop.drop(columns = ['Year Change', 'Net Change','Land Area','Migrants','Fert. Rate'])\npop['Urban Pop %'] = pop['Urban Pop %'].replace({'N.A.':'0 %'})\npop['Urban Pop %'] = pop['Urban Pop %'].str.rstrip('%').astype('float')\n\n# update populaion\ncols = ['Burma', 'Congo (Brazzaville)', 'Congo (Kinshasa)', \"Cote d'Ivoire\", 'Czechia', \n        'Kosovo', 'Saint Kitts and Nevis', 'Saint Vincent and the Grenadines', \n        'Taiwan*', 'US', 'West Bank and Gaza']\npops = [54409800, 89561403, 5518087, 26378274, 10708981, 1793000, \n        53109, 110854, 23806638, 330541757, 4569000]\ndense = [83, 16, 16, 83, 138, 159, 205, 284, 673, 35, 758]\nmedage = [29.2, 16.7, 19.5, 20.3, 43.3, 30.5, 36.5, 35.3, 42.3, 38.5, 21.9]\nurbanpop = [31, 44, 67, 51, 74, 50, 31, 52, 78, 82, 76]\nnew_df = pd.DataFrame({'Country_Region': cols, 'Population': pops, 'Density P\/km^2': dense, 'Med. Age': medage, 'Urban Pop %': urbanpop})\npop.update(new_df)\npop.replace(['South Korea', 'North Korea'], ['Korea, South', 'Korea, North'])\n# merged data\nweather_country = pd.merge(weather_country, pop, on='Country_Region', how='left')\nweather_country['Population'] = weather_country['Population'].fillna(0)\nweather_country.sample(3)\n\n","dbc2e057":"#display(weather_country.loc[weather_country['Population'] != 0.0])\n\n#filled the population and density for places that weren't recorded \nfor i in weather_country.index:\n    if weather_country.loc[i, \"Country_Region\"] == 'Bangladesh':\n        weather_country.loc[i, \"Population\"] = 164336258\n        weather_country.loc[i, \"Density P\/km^2\"] = 1265\n        weather_country.loc[i, \"Med. Age\"] = 27.9\n        weather_country.loc[i, \"Urban Pop %\"] = 37\n    elif weather_country.loc[i, \"Country_Region\"] == 'Brazil':\n        weather_country.loc[i, \"Population\"] = 212228418\n        weather_country.loc[i, \"Density P\/km^2\"] = 25\n        weather_country.loc[i, \"Med. Age\"] = 33.2\n        weather_country.loc[i, \"Urban Pop %\"] = 87\n    elif weather_country.loc[i, \"Country_Region\"] == 'Kuwait':\n        weather_country.loc[i, \"Urban Pop %\"] = 100\n    elif weather_country.loc[i, \"Country_Region\"] == 'Holy See':\n        weather_country.loc[i, \"Urban Pop %\"] = 100\n        weather_country.loc[i, \"Med. Age\"] = 60\n    elif weather_country.loc[i, \"Country_Region\"] == 'Andorra':\n        weather_country.loc[i, \"Med. Age\"] = 44.9\n    elif weather_country.loc[i, \"Country_Region\"] == 'San Marino':\n        weather_country.loc[i, \"Med. Age\"] = 44.5\n    elif weather_country.loc[i, \"Country_Region\"] == 'Dominica':\n        weather_country.loc[i, \"Med. Age\"] = 34\n    elif weather_country.loc[i, \"Country_Region\"] == 'Liechtenstein':\n        weather_country.loc[i, \"Med. Age\"] = 43.4        \n    elif weather_country.loc[i, \"Country_Region\"] == 'Diamond Princess':\n        weather_country = weather_country.drop(index=i)\n    elif weather_country.loc[i, \"Country_Region\"] == 'India':\n        weather_country.loc[i, \"Population\"] = 1380004385 \n        weather_country.loc[i, \"Density P\/km^2\"] = 464\n        weather_country.loc[i, \"Urban Pop %\"] = 34\n        weather_country.loc[i, \"Med. Age\"] = 28.7\n    elif weather_country.loc[i, \"Country_Region\"] == 'China':\n        weather_country.loc[i, \"Population\"] = 1438116346 \n        weather_country.loc[i, \"Density P\/km^2\"] = 153\n        weather_country.loc[i, \"Med. Age\"] = 38.4\n        weather_country.loc[i, \"Urban Pop %\"] = 59\n    elif weather_country.loc[i, \"Country_Region\"] == 'West Bank and Gaza':\n        weather_country.loc[i, \"Population\"] = 4569000 \n        weather_country.loc[i, \"Density P\/km^2\"] = 758\n        weather_country.loc[i, \"Med. Age\"] = 21.9\n        weather_country.loc[i, \"Urban Pop %\"] = 76\n    elif weather_country.loc[i, \"Country_Region\"] == 'Indonesia':\n        weather_country.loc[i, \"Population\"] = 272884327 \n        weather_country.loc[i, \"Density P\/km^2\"] = 151\n        weather_country.loc[i, \"Med. Age\"] = 31.1\n        weather_country.loc[i, \"Urban Pop %\"] = 55\n    elif weather_country.loc[i, \"Country_Region\"] == 'US':\n        weather_country.loc[i, \"Urban Pop %\"] = 82\n    elif weather_country.loc[i, \"Country_Region\"] == 'Venezuela':\n        weather_country.loc[i, \"Urban Pop %\"] = 88\n    elif weather_country.loc[i, \"Country_Region\"] == 'Monaco':\n        weather_country.loc[i, \"Urban Pop %\"] = 100\n        weather_country.loc[i, \"Med. Age\"] = 53.1\n    elif weather_country.loc[i, \"Country_Region\"] == 'Singapore':\n        weather_country.loc[i, \"Urban Pop %\"] = 100\n    elif weather_country.loc[i, \"Country_Region\"] == 'Japan':\n        weather_country.loc[i, \"Population\"] = 126559084 \n        weather_country.loc[i, \"Density P\/km^2\"] = 347\n        weather_country.loc[i, \"Med. Age\"] = 48.6\n        weather_country.loc[i, \"Urban Pop %\"] = 92\n    elif weather_country.loc[i, \"Country_Region\"] == 'Sao Tome and Principe':\n        weather_country.loc[i, \"Population\"] = 218241\n        weather_country.loc[i, \"Density P\/km^2\"] = 228\n        weather_country.loc[i, \"Med. Age\"] = 19.3\n        weather_country.loc[i, \"Urban Pop %\"] = 73\n    elif weather_country.loc[i, \"Country_Region\"] == 'Korea, South':\n        weather_country.loc[i, \"Population\"] = 51259674 \n        weather_country.loc[i, \"Density P\/km^2\"] = 527\n        weather_country.loc[i, \"Med. Age\"] = 43.2\n        weather_country.loc[i, \"Urban Pop %\"] = 81\n    elif weather_country.loc[i, \"Country_Region\"] == 'Russia':\n        weather_country.loc[i, \"Population\"] = 145920988 \n        weather_country.loc[i, \"Density P\/km^2\"] = 9\n        weather_country.loc[i, \"Med. Age\"] = 40.3\n        weather_country.loc[i, \"Urban Pop %\"] = 74\n    elif weather_country.loc[i, \"Country_Region\"] == 'MS Zaandam':\n        weather_country = weather_country.drop(index=i)\n    elif weather_country.loc[i, \"Country_Region\"] == 'Korea, South':\n        weather_country = weather_country.drop(index=i)\n    elif weather_country.loc[i, \"Country_Region\"] == 'Pakistan':\n        weather_country.loc[i, \"Population\"] = 219922471 \n        weather_country.loc[i, \"Density P\/km^2\"] = 287\n        weather_country.loc[i, \"Med. Age\"] = 22\n        weather_country.loc[i, \"Urban Pop %\"] = 37\n    elif weather_country.loc[i, \"Country_Region\"] == 'Mexico':\n        weather_country.loc[i, \"Population\"] = 128633396 \n        weather_country.loc[i, \"Density P\/km^2\"] = 66\n        weather_country.loc[i, \"Med. Age\"] = 29.3\n        weather_country.loc[i, \"Urban Pop %\"] = 80\n    elif weather_country.loc[i, \"Country_Region\"] == 'Nigeria':\n        weather_country.loc[i, \"Population\"] = 204968096\n        weather_country.loc[i, \"Density P\/km^2\"] = 226\n        weather_country.loc[i, \"Med. Age\"] = 18.6\n        weather_country.loc[i, \"Urban Pop %\"] = 50\n    else:\n        weather_country.loc[i, \"Country_Region\"] = weather_country.loc[i, \"Country_Region\"]  \n\n# Cases per population \nweather_country['Urban Pop %'].fillna(0, inplace=True)\nweather_country['Urban Pop %'] = weather_country['Urban Pop %']\/ 100.0\nweather_country['Cases_Million_People'] = round((weather_country['ConfirmedCases'] \/ weather_country['Population']) * 1000000)\nweather_country['ln(Cases \/ Million People)'] = np.log(weather_country.Cases_Million_People + 1)\n\nweather_country.sample(3)\n\n\neastasia  = ['Taiwan*', 'Taiwan', 'Mongolia', 'China', 'Japan', 'South Korea']\neurope = ['Latvia', 'Switzerland', 'Liechtenstein', 'Italy', 'Norway', 'Austria', 'Albania',\n          'United Kingdom', 'Iceland', 'Finland', 'Luxembourg', 'Belarus', 'Bulgaria', \n          'Guernsey', 'Poland', 'Moldova', 'Spain', 'Bosnia and Herzegovina', 'Portugal', \n          'Germany', 'Monaco', 'San Marino', 'Andorra', 'Slovenia', 'Montenegro', 'Ukraine',\n          'Lithuania', 'Netherlands', 'Slovakia', 'Czechia', 'Malta', 'Hungary', 'Jersey', \n          'Serbia', 'Kosovo', 'France', 'Croatia', 'Sweden', 'Estonia', 'Denmark', \n          'North Macedonia', 'Greece', 'Ireland', 'Romania', 'Belgium']\na = []\nfor i in weather_country.index:\n    if weather_country.loc[i, \"Country_Region\"] in eastasia:\n        a.append(\"East Asia\")\n    elif weather_country.loc[i, \"Country_Region\"] in europe:\n        a.append(\"Europe\")\n    elif weather_country.loc[i, \"Country_Region\"] == 'US':\n        a.append(\"US\")\n    else:\n        a.append(\"Rest Of World\")\n\nweather_country[\"Country_Group\"] = a\nweather_country.sample(1)","6290bcde":"second = get2deriv(weather_country, 'Date', 'latlong', 'ConfirmedCases')\nweather_country = pd.merge(weather_country, second, on=['latlong','Date'], how='left')\n\ndaily = getdaily(weather_country, 'Date', 'latlong', 'ConfirmedCases')\nweather_country = pd.merge(weather_country, daily, on=['latlong','Date'], how='left')\n\nweather_country[\"daily_case\"] = weather_country[\"daily_case\"].fillna(0.0)\n\nfirst = get1deriv(weather_country, 'Date', 'latlong', 'daily_case')\nweather_country = pd.merge(weather_country, first, on=['latlong','Date'], how='left')\n\nlag = getlag(weather_country, 'Date', 'latlong', 'ConfirmedCases')\nweather_country = pd.merge(weather_country, lag, on=['latlong','Date'], how='left')\n\nlag2 = getlagdeath(weather_country, 'Date', 'latlong', 'Fatalities')\nweather_country = pd.merge(weather_country, lag2, on=['latlong','Date'], how='left')\n\nweather_country[\"lag\"] = weather_country[\"lag\"].fillna(0.0)\nweather_country[\"lag_death\"] = weather_country[\"lag_death\"].fillna(0.0)\n\nfirst = get1deriv(weather_country, 'Date', 'latlong', 'lag')\nfirst = first.rename(columns={\"1stderiv\": \"lag_1stderiv\"})\nweather_country = pd.merge(weather_country, first, on=['latlong','Date'], how='left')\n\n\nweather_country[\"2ndderiv\"] = weather_country[\"2ndderiv\"].fillna(1.0)\nweather_country[\"1stderiv\"] = weather_country[\"1stderiv\"].fillna(0.0)\nweather_country[\"lag\"] = weather_country[\"lag\"].fillna(0.0)\nweather_country[\"lag_death\"] = weather_country[\"lag_death\"].fillna(0.0)\nweather_country[\"lag_1stderiv\"] = weather_country[\"lag_1stderiv\"].fillna(0.0)\n#display(weather_country.loc[weather_country['Country_Region'] == 'Germany'])\nweather_country.sample(3)","c0d8aab5":"updated = pd.read_csv('\/kaggle\/input\/novel-corona-virus-2019-dataset\/covid_19_data.csv', parse_dates=['ObservationDate'])\nlatlongupdate = pd.read_csv('\/kaggle\/input\/novel-corona-virus-2019-dataset\/time_series_covid_19_confirmed.csv')\nupdated[\"countstate\"] = tuple(zip(updated['Country\/Region'], updated['Province\/State']))\nupdated['Province\/State'].replace(np.nan, \"Not Reported\", inplace=True)\n#updated[\"ObservationDate\"]=pd.to_datetime(updated[\"ObservationDate\"])\nupdated = updated.rename(columns={\"ObservationDate\": \"Date\"})\nupdated['Date'] = pd.to_datetime(updated['Date'], format = '%Y-%m-%d')\n\na = updated[['Country\/Region', 'Province\/State']]\n\na = a.drop_duplicates()\na.reset_index()\na['identify'] = a.index\n\n#adding active cases \nupdated = pd.merge(updated, a, on=['Country\/Region', 'Province\/State'], how='left')\nupdated['active cases'] = updated['Confirmed'] - updated['Recovered'] - updated['Deaths']\n\n#adding derivatives \nsecond = get2deriv(updated, 'Date', 'identify', 'Confirmed')\nupdated = pd.merge(updated, second, on=['identify','Date'], how='left')\n\nfirst = get1deriv(updated, 'Date', 'identify', 'Confirmed')\nupdated = pd.merge(updated, first, on=['identify','Date'], how='left')\n\nupdated[\"2ndderiv\"] = updated[\"2ndderiv\"].fillna(1.0)\nupdated[\"1stderiv\"] = updated[\"1stderiv\"].fillna(0.0)\n\n#adding regions\neastasia  = ['Taiwan*', 'Taiwan', 'Mongolia', 'China', 'Mainland China','Japan', 'Korea, South','South Korea', 'Hong Kong']\neurope = ['Latvia', 'Switzerland', 'Liechtenstein', 'Italy', 'Norway', 'Austria', 'Albania',\n          'United Kingdom', 'Iceland', 'Finland', 'Luxembourg', 'Belarus', 'Bulgaria', \n          'Guernsey', 'Poland', 'Moldova', 'Spain', 'Bosnia and Herzegovina', 'Portugal', \n          'Germany', 'Monaco', 'San Marino', 'Andorra', 'Slovenia', 'Montenegro', 'Ukraine',\n          'Lithuania', 'Netherlands', 'Slovakia', 'Czechia', 'Malta', 'Hungary', 'Jersey', \n          'Serbia', 'Kosovo', 'France', 'Croatia', 'Sweden', 'Estonia', 'Denmark', \n          'North Macedonia', 'Greece', 'Ireland', 'Romania', 'Belgium', 'UK']\na = []\nupdated['country_group'] = updated['Confirmed']\nupdated['country_group'] = 0\n\nfor i in updated.index:\n    if updated.loc[i, \"Country\/Region\"] in eastasia:\n        updated.loc[i, \"country_group\"] = \"East Asia\"\n    elif updated.loc[i, \"Country\/Region\"] in europe:\n        updated.loc[i, \"country_group\"] = \"Europe\"\n    elif updated.loc[i, \"Country\/Region\"] == 'US':\n        updated.loc[i, \"country_group\"] = \"US\"\n    else:\n        updated.loc[i, \"country_group\"] = \"Rest Of The World\"\n\nupdated[updated['country_group']==\"Rest Of The World\"].head(3)\n\n\nupdated.sample(3)","75bb2779":"\"\"\"\n#latlongupdate['countstate'] = tuple(zip(latlongupdate['Country\/Region'], latlongupdate['Province\/State']))\njust_latlong = latlongupdate[['Country\/Region','Province\/State', 'Lat', 'Long']].copy()\n\n#display(updated[updated['Country\/Region']==\"South Korea\"])\njust_latlong = just_latlong.replace('Korea, South', 'South Korea')\n\n\n#display(just_latlong[just_latlong['Country\/Region'].str.contains(\"Korea\")])\njust_latlong['Province\/State'].replace(np.nan, \"Not Reported\", inplace=True)\n\nwc = weather_country[['Country_Region', 'Province_State', 'Lat', 'Long']].copy()\nwc = wc.rename(columns={'Country_Region': \"Country\/Region\",'Province_State': 'Province\/State' })\nwc['Province\/State'].replace(np.nan, \"Not Reported\", inplace=True)\n\nnota = wc[~wc['Province\/State'].isin(just_latlong['Province\/State'])]\nnota = nota.groupby(['Country\/Region','Province\/State'],as_index=False)['Lat','Long'].mean()\nc = ['US', 'Macau','UK', 'UK', 'US', 'UK', 'UK', 'UK', 'UK', 'Germany', 'US', 'US', 'UK', 'UK', 'UK', 'UK', 'UK',\n     'Czech Republic', 'The Bahamas', 'Republic of the Congo', 'Ivory Coast', 'Netherlands', 'Denmark', 'Palestine']\nr =['Chicago', 'Macau', 'Isle of Man', 'Montserrat', 'Northern Mariana Islands', 'Turks and Caicos Islands', \n    'Falkland Islands (Malvinas)', 'Gibraltar','Cayman Islands', 'Bavaria', 'American Samoa', 'United States Virgin Islands', \n    'Channel Islands', 'Bermuda', 'Anguilla', 'British Virgin Islands', 'Not Reported', 'Not Reported', 'Not Reported', 'Not Reported', 'Not Reported',\n   'Netherlands', 'Denmark', 'Not Reported']\nla=[41.8339042,23.6356074, 54.2278829, 16.691357, 17.3076967, 21.5741504, -51.7206292, 36.1295735, 19.5081819, 48.8992765, -14.061727, 18.0672779,\n   49.4582161, 32.3194245, 18.390315,18.5222738, 52.7602022, 49.7856662, 24.4229244, -0.6811523, 7.4662967, 52.1951016, 56.2128538, 31.8858324]\nlo=[-88.0121503,114.4376334,-4.8523185,-60.2272795,143.2420346,-72.3505781,-60.6489884,-5.3883195,-81.1347306,9.1651538,-170.6672906,-65.2991668,\n    -2.942905,-64.8364403,-63.4803453,-64.7114365,-6.813662,13.2321306,-78.2108881,10.3858451,-7.7921532,3.0367463,9.3001434,34.331614]\npp = pd.DataFrame(data={'Country\/Region': c, 'Province\/State':r, 'Lat':la, 'Long':lo})\njust_latlong = just_latlong.append(nota)\njust_latlong = just_latlong.append(pp)\n\njust_latlong.sample(10)\nprint(len(c))\nprint(len(r))\nprint(len(la))\nprint(len(lo))\n\n#with_updated.sample(2)\nwith_updated.isnull().any()\nwith_updated['bool_loc'] = pd.notnull(with_updated[\"Lat\"]) \na = with_updated[with_updated['Lat']=='Not Reported']\n#a[a['Country\/Region']!='UK'].sample(50)\n#a = with_updated[with_updated['Country\/Region']=='UK']\n#a[a['Province\/State']=='Not Reported'].sample(50)\n#b[b['Province\/State']== 'United Kingdom'].sample(7)\n#display(with_updated[with_updated['Country\/Region']=='Czechia'])\nprint(len(a))\nprint(len(with_updated))\n\n\"\"\"","2fed12ba":"local = weather_country.copy()\nlocal.replace(np.inf, np.nan).fillna(0)\n\nlocal = local.groupby(['Country_Region','Province_State','latlong', 'Lat', 'Long'],as_index=False)['1stderiv','2ndderiv'].mean()\nlocal['sderiv'] = local['2ndderiv']\np = local[local.sderiv >= 1.2]\nf = local[local.sderiv <= 0.8]\nt = local['sderiv'] <1.2\na = local['sderiv'] > 0.8\ng = local[t & a]\n\np['sderiv'] = p['2ndderiv']\np['ln(2ndderiv)'] = np.log(p.sderiv + 1)\npx.set_mapbox_access_token('pk.eyJ1IjoibW9ydXRraW4iLCJhIjoiY2s1cnJhMzczMGdjaDNtcnR0M2h0NnR6cSJ9.87UtlJlwluWbZq4ioist-g')\ndf = px.data.carshare()\np['sderiv'] = p['2ndderiv']*10\nfig = px.scatter_mapbox(p, lat=\"Lat\", lon=\"Long\",     color=\"2ndderiv\", size=\"2ndderiv\",\n                  color_continuous_scale=px.colors.cyclical.IceFire, size_max=50, zoom=3)\nfig.update_layout(\n    hovermode='closest',\n    title_text= \"Locations With An Accelerating Growth Rate (GR > 1.2) on 03\/07-03\/13\",\n    mapbox=dict(\n        center=go.layout.mapbox.Center(\n            lat=41.1254,\n            lon=-98.2651\n        ),\n        pitch=0,\n        zoom=3\n    )\n)\nfig.update_layout(legend_title_text='Growth Rate')\n\nfig.show()","509efef5":"world = updated.copy()\nworld = updated.groupby(['Date'],as_index=False)['active cases', 'Confirmed', 'Deaths'].sum()\nworld['active cases'][world['active cases'] < 0] = 0\n\n\n\nfig = go.Figure(data=[\n    go.Bar(name='Cases', x=world['Date'], y=world['active cases']),\n    go.Bar(name='Deaths', x=world['Date'], y=world['Deaths'])\n])\n# Change the bar mode\nfig.update_layout(\n    title='Daily Cases Growth',\n    xaxis_title=\"Date\",\n    yaxis_title=\"# of Daily Cases and Deaths\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=14,\n        color=\"#7f7f7f\"\n    )\n)\n\nfig.update_layout(barmode='overlay',\n                  title = {'y':0.9,\n        'x':0.5,'text':'Worldwide Daily Cases and Deaths count over time', 'xanchor': 'center', 'yanchor': 'top'})\nfig.show()\n\nfig1 = go.Figure()\nfig1.add_trace(go.Scatter(x=world['Date'], y=world['active cases'],\n                    mode='lines',\n                    name='active cases'))\n\n\nfig1.update_layout(\n    title='Daily Cases Growth',\n    xaxis_title=\"Date\",\n    yaxis_title=\"# of Daily Cases\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=14,\n        color=\"#7f7f7f\"\n    )\n)\n\nfig1.update_layout(\n    title={\n        'text': 'World Daily Case Count over time',\n        'y':0.9,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'}\n)\n\nfig1.show()\n\n","88dbc828":"cases1 = updated.copy()\n#cases['rh'] = cases['rh'].fillna(0)\ngrp4 = cases1.groupby(['Date', 'Country\/Region'])['active cases', 'Confirmed', 'Deaths', 'Recovered'].sum()\n\ngrp4['ln_ConfirmedCases'] = np.log(grp4.Confirmed + 1) \ngrp4 = grp4.reset_index()\ngrp4['Date'] = grp4['Date'].dt.strftime('%m\/%d\/%Y')\ngrp4['Country'] =  grp4['Country\/Region']\n\nfig = px.scatter_geo(grp4, locations=\"Country\", locationmode='country names', \n                     color=\"ln_ConfirmedCases\",size= \"ln_ConfirmedCases\", hover_name=\"Country\/Region\",hover_data = [grp4.Confirmed, grp4.Deaths, grp4.Recovered ],projection=\"natural earth\",\n                     animation_frame=\"Date\",width=900, height=700,\n                        color_continuous_scale=\"portland\",\n                     title='World Map of Log Coronavirus Cases Change Over Time (14 = 1.18M)')\n\nfig.update(layout_coloraxis_showscale=True)\n","e17ac239":"cases = weather_country.copy()\ncases['rh'] = cases['rh'].fillna(0)\ngrp2 = cases.groupby(['Date', 'Country_Region'])['ConfirmedCases', 'daily_case', 'Fatalities'].sum()\ngrp1 = cases.groupby(['Date', 'Country_Region'])['rh', 'temp'].mean()\ngrp = pd.merge(grp2, grp1, on=['Country_Region', 'Date'], how='left')\n\ngrp['ln_ConfirmedCases'] = np.log(grp.ConfirmedCases + 1) \ngrp = grp.reset_index()\ngrp['Date'] = grp['Date'].dt.strftime('%m\/%d\/%Y')\ngrp['Country'] =  grp['Country_Region']\n\n\ngrp['dumtemp'] = grp['temp']+50\ngrp['dumtemp'] = grp['dumtemp'].round(2)\n\nfig = px.scatter_geo(grp, locations=\"Country\", locationmode='country names', \n                     color=\"temp\",size= \"dumtemp\", hover_name=\"Country_Region\",hover_data = [grp.ConfirmedCases, grp.daily_case, grp.Fatalities ],projection=\"natural earth\",\n                     animation_frame=\"Date\",width=900, height=700,\n                    color_continuous_scale=\"portland\", title='COVID-19: Temperature By Country\/Region Over Time')\n\nfig.update(layout_coloraxis_showscale=True)\nfig.show()\n","5bab6006":"fig1 = px.scatter_geo(grp, locations=\"Country\", locationmode='country names', \n                     color=\"rh\",size= \"rh\", hover_name=\"Country_Region\",hover_data = [grp.ConfirmedCases, grp.daily_case, grp.Fatalities ],projection=\"natural earth\",\n                     animation_frame=\"Date\",width=900, height=700,\n                        color_continuous_scale=\"portland\",\n                     title='COVID-19: Relative Humidity By Country\/Region Over Time')\n\nfig1.update(layout_coloraxis_showscale=True)\nfig1.show()","993d21d5":"df = weather_country.copy()\ndf['latlongcount'] = list(zip(df.Country_Region, df.Province_State))\n#ind = df['latlongcount'].to_numpy()\nnew = df[['Date','Country_Region','latlongcount', '1stderiv', 'Density P\/km^2', 'temp', 'ConfirmedCases', 'Population']].copy()\n#new['Date'] = pd.tslib.Timestamp(new['Date'])\nnew = new[new.Date == new.Date.max()]\n\nnew1 = new.groupby(['Country_Region'],as_index=False)['1stderiv', 'Population', 'Density P\/km^2', 'temp'].mean()\nnew2 = new.groupby(['Country_Region'],as_index=False)['ConfirmedCases'].sum()\nnew = pd.merge(new1, new2, on='Country_Region', how='left')\nnew['Cases_Million_People'] = round((new['ConfirmedCases'] \/ new['Population']) * 1000000)\n\n#df = px.data.gapminder()\nfig = px.scatter(new, x=\"Density P\/km^2\", y=\"1stderiv\",\n           size=\"Cases_Million_People\", color=\"temp\", hover_name=\"Country_Region\",\n           log_y=True, log_x=True, size_max=55)\nfig.update_layout(title='Rate of New Cases \/ Density (Pop\/km^2) With Temperature(C) Color Gradient')\nfig.update_layout(\n    title={\n        'y':0.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'},\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=14,\n        color=\"#7f7f7f\"\n    )\n)","dffe5934":"c = weather_country.copy()\nc['Province_State'] = c['Province_State'].fillna(\"Total\")\n#c = c.drop(columns = ['1stderiv', '2ndderiv'])\nc2 = c.groupby(['Date', 'Country_Group', 'Country_Region'],as_index=False)['daily_case'].sum()\nc = c.groupby(['Date', 'Country_Group', 'Country_Region'],as_index=False)['temp'].mean()\nc = pd.merge(c, c2, on=['Date', 'Country_Group', 'Country_Region'], how='left')\nc = c[['Date','Country_Group','Country_Region', 'daily_case', 'temp']]\n\neurope = c[c['Country_Group']==\"Europe\"]\neurope_n = europe[europe['Country_Region'].isin(['France', 'Germany', 'Netherlands', 'United Kingdom', 'Spain', 'Belgium'])]\n\nfig5 = px.line(europe_n, x=\"Date\", y=\"temp\", color='Country_Region', title='Average Temperature (C) Change Over Time Per Country In Europe')\nfig5.update_layout(\n    xaxis_title=\"Date\",\n    yaxis_title=\"Temperature (C)\",\n    title={\n        'y':0.9,\n        'x':0.45,\n        'xanchor': 'center',\n        'yanchor': 'top'}\n    #font=dict(\n    #    family=\"Courier New, monospace\",\n    #    size=14,\n    #    color=\"#7f7f7f\"\n    #)\n)\nfig5.show()\n\nfig6 = px.line(europe_n, x=\"Date\", y=\"daily_case\", color='Country_Region', title='Average Rate Of Cases Change Over Time In Countries In Europe')\nfig6.update_layout(\n    xaxis_title=\"Date\",\n    yaxis_title=\"Rate of Daily Cases\",\n    title={\n        'y':0.9,\n        'x':0.45,\n        'xanchor': 'center',\n        'yanchor': 'top'}\n    #font=dict(\n    #    family=\"Courier New, monospace\",\n    #    size=14,\n    #    color=\"#7f7f7f\"\n    #)\n)\nfig6.show()","4d8290de":"updated_groups = updated.groupby(['Date', 'country_group'],as_index=False)['active cases', 'Confirmed'].sum()\nupdated_groups['active cases'][updated_groups['active cases'] < 0] = 0\nupdated_groups = updated_groups.groupby('country_group').resample('W-Mon', on='Date').sum().reset_index().sort_values(by='Date')\nupdated_groupsl = updated_groups.copy()\n\nwd1 = weather_country.groupby(['Date', 'Country_Group'],as_index=False)['daily_case', 'ConfirmedCases'].sum()\nwd1['daily_case'][wd1['daily_case'] < 0] = 0\nwd1 = wd1.groupby('Country_Group').resample('W-Mon', on='Date').sum().reset_index().sort_values(by='Date')\nwd2 = wd1.copy()\n\n#added mean temp \na = weather_country.copy()\nupdated_groups3 = a.groupby(['Date', 'Country_Group'],as_index=False)['temp'].mean()\nupdated_groups3 = updated_groups3.rename(columns={\"Country_Group\": \"country_group\"})\nupdated_groups = pd.merge(updated_groups, updated_groups3, on=['country_group','Date'], how='left')\nupdated_groups['temp'] = updated_groups['temp'].fillna(\"hi\")\n\n#only kept the data points where temp is recorded \nupdated_groups = updated_groups[updated_groups.temp != 'hi']\n\n#adding derivatives for the regions UPDATEDGROUPS\nsecond = get2deriv(updated_groupsl, 'Date', 'country_group', \"active cases\")\nupdated_groupsl = pd.merge(updated_groupsl, second, on=['country_group','Date'], how='left')\n\nfirst = get1deriv(updated_groupsl, 'Date', 'country_group', \"active cases\")\nupdated_groupsl = pd.merge(updated_groupsl, first, on=['country_group','Date'], how='left')\n\nupdated_groupsl[\"2ndderiv\"] = updated_groupsl[\"2ndderiv\"].fillna(1.0)\nupdated_groupsl[\"1stderiv\"] = updated_groupsl[\"1stderiv\"].fillna(0.0)\n\n#adding derivatives for the regions WEATHER-COUNTRY\nsecond = get2deriv(wd2, 'Date', 'Country_Group', \"daily_case\")\nwd2 = pd.merge(wd2, second, on=['Country_Group','Date'], how='left')\n\nfirst = get1deriv(wd2, 'Date', 'Country_Group', \"daily_case\")\nwd2 = pd.merge(wd2, first, on=['Country_Group','Date'], how='left')\n\nwd2[\"2ndderiv\"] = wd2[\"2ndderiv\"].fillna(1.0)\nwd2[\"1stderiv\"] = wd2[\"1stderiv\"].fillna(0.0)\n\nupdated_groupsl = updated_groupsl.loc[updated_groupsl['Date'] <='2020-05-11']\n\n\n\n##########using weather data ###############\nwupdated = weather_country.copy()\nwupdated_groups1 = wupdated.groupby(['Date', 'Country_Group'],as_index=False)['daily_case', 'ConfirmedCases'].sum()\nwupdated_groups2 = wupdated.groupby(['Date', 'Country_Group'],as_index=False)['rh', 'temp'].mean()\nwupdated_groups = pd.merge(wupdated_groups1, wupdated_groups2, on=['Country_Group','Date'], how='left')\nwupdated_groups = wupdated_groups.groupby('Country_Group').resample('W-Mon', on='Date').sum().reset_index().sort_values(by='Date')\n\nsecond = get2deriv(wupdated_groups, 'Date', 'Country_Group', 'ConfirmedCases')\nwupdated_groups = pd.merge(wupdated_groups, second, on=['Date', 'Country_Group'], how='left')\n\nfirst = get1deriv(wupdated_groups, 'Date', 'Country_Group', 'daily_case')\nwupdated_groups = pd.merge(wupdated_groups, first, on=['Date', 'Country_Group'], how='left')\n\nwupdated_groups[\"2ndderiv\"] = wupdated_groups[\"2ndderiv\"].fillna(1.0)\nwupdated_groups[\"1stderiv\"] = wupdated_groups[\"1stderiv\"].fillna(0.0)","cb0ef395":"fig = px.line(updated_groupsl, x=\"Date\", y=\"active cases\", color='country_group')\n\n\n\nfig.update_layout(\n    title='Regional Daily Cases over time until 05\/11',\n    xaxis_title=\"Date\",\n    yaxis_title=\"# of Daily Cases\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=14,\n        color=\"#7f7f7f\"\n    )\n)\n\nfig.show()","5f5c2bd7":"fig = px.line(updated_groupsl, x=\"Date\", y=\"1stderiv\", color='country_group')\n\n\n\nfig.update_layout(\n    title='Regional Daily Rate Of Change over time until 5\/11',\n    xaxis_title=\"Date\",\n    yaxis_title=\"# of Daily Cases Rate\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=14,\n        color=\"#7f7f7f\"\n    )\n)\n\nfig.show()","018001e1":"ca = weather_country.copy()\nca['Province_State'] = ca['Province_State'].fillna(\"Total\")\n#c = c.drop(columns = ['1stderiv', '2ndderiv'])\ncb = ca.groupby(['Date', 'Country_Group'],as_index=False)['daily_case'].sum()\nca = ca.groupby(['Date', 'Country_Group'],as_index=False)['temp'].mean()\nca = pd.merge(ca, cb, on=['Date', 'Country_Group'], how='left')\n\nfig1 = px.line(ca, x=\"Date\", y=\"temp\", color='Country_Group', title='Average Temperature Change Over Time Per Region until 4\/11')\nfig1.update_layout(\n    xaxis_title=\"Date\",\n    yaxis_title=\"Temperature (C)\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=14,\n        color=\"#7f7f7f\"\n    )\n)\n\nfig1.show()\n\n#fig2 = px.line(ca, x=\"Date\", y=\"daily_case\", color='Country_Group', title='Average Temperature Change Over Time Per Region')\n#fig2.show()\n","9ccc9c38":"df = weather_country.copy()\ndf['latlongcount'] = df['Country_Region'] + ',' + df['Province_State']\n#ind = df['latlongcount'].to_numpy()\nnew = df[['Date','latlongcount', 'Country_Group','lag_1stderiv', 'lag','Density P\/km^2', 'temp', 'ConfirmedCases', 'Population', 'rh']].copy()\n#new['Date'] = pd.tslib.Timestamp(new['Date'])\nnew = new[new.Date > '2020-04-30']\nnew = new[new.Date < '2020-05-05']\n\nnew1 = new.groupby(['latlongcount', 'Country_Group'],as_index=False)['lag_1stderiv', 'lag','Population', 'Density P\/km^2', 'temp', 'rh'].mean()\nnew2 = new.groupby(['latlongcount'],as_index=False)['ConfirmedCases'].max()\nnew = pd.merge(new1, new2, on='latlongcount', how='left')\nnew['Cases_Million_People'] = round((new['ConfirmedCases'] \/ new['Population']) * 1000000)\n\n#df = px.data.gapminder()\nfig = px.scatter(new, x=\"temp\", y=\"lag_1stderiv\",\n            color=\"Country_Group\", hover_name=\"latlongcount\",\n           log_y=True,log_x=True, size_max=55)\nfig.update_layout(title='4\/30-5\/05 Average Rate of New Cases \/ Temperature(C)')\nfig.update_layout(\n    xaxis_title=\"Temperature (C)\",\n    yaxis_title=\"Case Rate\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=14,\n        color=\"#7f7f7f\"\n    )\n)\n\nfig.show()","3d79fb56":"fig1 = px.scatter(new, x=\"rh\", y=\"lag_1stderiv\",\n            color=\"Country_Group\", hover_name=\"latlongcount\",\n           log_y=True,log_x=True, size_max=55)\nfig1.update_layout(title='4\/30-5\/05 Average Rate of New Cases \/ Realtive Humidity')\nfig1.update_layout(\n    xaxis_title=\"Relative Humidity\",\n    yaxis_title=\"Case Rate\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=14,\n        color=\"#7f7f7f\"\n    )\n)\n\nfig1.show()","b8b483c6":"fig = px.scatter_3d(new, x='rh', y='temp', z='ConfirmedCases',\n              color='Country_Group', log_z=True)\nfig.update_layout(title='4\/3-4\/11 Average Rate of New Cases \/ Temp (C) \/ Relative Humidity')\nfig.update_layout(\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=14,\n        color=\"#7f7f7f\"\n    )\n)\n\nfig.show()","5c80ac96":"weather_country.tail(3)","b97b1819":"from sklearn import linear_model\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n\ndef lin_reg(X_train, Y_train, X_test, regr):\n    # Create linear regression object\n    #regr = linear_model.LinearRegression()\n\n    # Train the model using the training sets\n    regr.fit(X_train, Y_train)\n    param = regr.get_params()\n\n    # Make predictions using the testing set\n    y_pred = regr.predict(X_test)\n    \n    return regr, y_pred\n","c6f4fa41":"data = weather_country.copy()\n# Apply log transformation to all ConfirmedCases and Fatalities columns, except for trends\ndata['ConfirmedCases'] = data['ConfirmedCases'].astype('float64')\ndata['Fatalities'] = data['Fatalities'].astype('float64')\n\n# Replace infinites\ndata.replace([np.inf, -np.inf], 0, inplace=True)\ndata = data.fillna(0)\nfeatures = ['Country_Region','Province_State','ConfirmedCases','Population','wdsp', 'prcp','fog','temp', 'rh', 'min', 'max','temp variance','Density P\/km^2',\n       'day_from_jan_first','Med. Age','Urban Pop %' ]\ndata = data[features]\n\ndata.sort_values(by=['day_from_jan_first'])\ntrain = data.loc[data['day_from_jan_first'] <=109]\ntest = data.loc[data['day_from_jan_first'] > 110]\n\ntrainy = train[['ConfirmedCases']].copy()\ntrainy_nolog = trainy.copy()\ntrainy = trainy.apply(lambda x: np.log1p(x))\n\ntrainx = train.drop(columns = ['Country_Region', 'Province_State','ConfirmedCases']).copy()\n\ntesty = test[['ConfirmedCases']].copy()\ntesty_nolog = test[['ConfirmedCases']].copy()\ntesty = testy.apply(lambda x: np.log1p(x))\n\ntestx = test.drop(columns = ['Country_Region', 'Province_State','ConfirmedCases']).copy()\n\n\n\ndata.sample(10)\nparam = 0\n\nregr = linear_model.LinearRegression()\nr, ypred= lin_reg(trainx, trainy, testx, regr)\n\nr2 = r2_score(testy, ypred)\nmse = mean_absolute_error(testy, ypred)\n#msle = mean_squared_log_error(testy, ypred)\nprint(\"R squared Value (Variability Explained)\")\nprint(r2)\nprint(' ')\nprint(\"Mean Absolute Error Value (Variance)\")\nprint(mse)\n\n\n","6996f3fd":"s = test.copy()\ns['predicted'] = ypred\ns['real'] = testy\na = s.groupby(['day_from_jan_first'],as_index=False)['predicted', 'real'].sum()\n#s.head(10)\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=a['day_from_jan_first'], y=a['real'],\n                    mode='lines',\n                    name='Real Confirmed Cases'))\nfig.add_trace(go.Scatter(x=a['day_from_jan_first'], y=a['predicted'],\n                    mode='lines',\n                    name='Predicted Confirmed Cases' ))\n\n#fig.add_shape(\n#        # Line Horizontal\n#            type=\"line\",\n#            x0=90,\n#            y0=1600,\n#            x1=102,\n#            y1=1600,\n#            line=dict(\n#                color=\"LightSeaGreen\",\n#                width=2,\n#                dash=\"dashdot\",\n#            ),\n#    )\n\n\nfig.update_layout(title='Regression Predictive Model: 04\/20\/2020 - 05\/15\/2020')\nfig.update_layout(\n    #font=dict(\n    #    family=\"Courier New, monospace\",\n    #    size=14,\n    #    color=\"#7f7f7f\"\n    #)\n)\n\n\nfig.show()","a57a9ce3":"\nrfcla = RandomForestRegressor(n_estimators=100, max_samples=0.8,\n                        random_state=1)\n# We train model\nrfcla.fit(trainx, trainy)\npredictions = rfcla.predict(testx)\n#roc_value = roc_auc_score(testy, predictions)\n\nfi = pd.DataFrame({'feature': list(trainx.columns),\n                   'importance': rfcla.feature_importances_}).\\\n                    sort_values('importance', ascending = False)\n\nr2 = r2_score(testy, predictions)\nprint(\"R squared Value (Variability Explained)\")\nprint(r2)\nprint(' ')\nprint(\"Mean Squared Error Value (Variance):\")\nmse = mean_squared_error(testy, predictions)\nprint(mse)\nfi.head()","2ed9d39d":"data1 = weather_country.copy()\n# Apply log transformation to all ConfirmedCases and Fatalities columns, except for trends\ndata1['lag']  = data1['lag'].astype('float64')\ndata1['lag_death'] = data1['lag_death'].astype('float64')\ndata1['lag'] = data1['lag'].apply(lambda x: np.log1p(x))\ndata1['lag_death'] = data1['lag_death'].apply(lambda x: np.log1p(x))\n\n# Replace infinites\ndata1.replace([np.inf, -np.inf], 0, inplace=True)\ndata1 = data1.fillna(0)\nfeatures1 = ['Country_Region','Province_State','lag','Population','wdsp','prcp','fog','temp', 'rh', 'min', 'max','temp variance','Density P\/km^2',\n       'day_from_jan_first','Med. Age','Urban Pop %' ]\ndata1 = data1[features1]\n\n\n#data1.sort_values(by=['day_from_jan_first'])\ntrain1 = data1[data1['day_from_jan_first'] <=104]\ntest1 = data1[data1['day_from_jan_first'] > 105]\ntest1 = test1[test1['day_from_jan_first'] < 120]\ntrainy1 = train1[['lag']].copy()\ntrainx1 = train1.drop(columns = ['Country_Region', 'Province_State','lag']).copy()\ntesty1 = test1[['lag']].copy()\ntestx1 = test1.drop(columns = ['Country_Region', 'Province_State','lag']).copy()","b6bdb8b2":"data1.sample(10)\nparam = 0\nregr = linear_model.LinearRegression()\n# Linear regression model\ndef lin_reg(X_train, Y_train, X_test):\n    # Create linear regression object\n    #regr = linear_model.LinearRegression()\n\n    # Train the model using the training sets\n    regr.fit(X_train, Y_train)\n    param = regr.get_params()\n\n    # Make predictions using the testing set\n    y_pred = regr.predict(X_test)\n    \n    return regr, y_pred\nr1, ypred1= lin_reg(trainx1, trainy1, testx1)\n\nr21 = r2_score(testy1, ypred1)\nmse1 = mean_absolute_error(testy1, ypred1)\n#msle = mean_squared_log_error(testy, ypred)\nprint(\"R squared Value\")\nprint(r21)\nprint(\"     \")\nprint(\"Mean Absolute Error Value\")\nprint(mse1)\n#print(\"Mean Squared Log Error Value\")\n#print(msle)","fa57f1f9":"k = test1.copy()\n\n\nk['predicted'] = ypred1\nk['real'] = testy1\nd = k.groupby(['day_from_jan_first'],as_index=False)['predicted', 'real'].sum()\n#s.head(10)\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=d['day_from_jan_first'], y=d['real'],\n                    mode='lines',\n                    name='Real Confirmed Cases'))\nfig.add_trace(go.Scatter(x=d['day_from_jan_first'], y=d['predicted'],\n                    mode='lines',\n                    name='Predicted Confirmed Cases' ))\n\nfig.update_layout(title='Regression Predictive Model WITH 7 day Lag')\n\n\nfig.show()","1a6f0002":"rfcla1 = RandomForestRegressor(n_estimators=100, max_samples=0.8,\n                        random_state=1)\n# We train model\nrfcla1.fit(trainx1, trainy1)\npredictions1 = rfcla1.predict(testx1)\n#roc_value = roc_auc_score(testy, predictions)\n\nfi1 = pd.DataFrame({'feature': list(trainx1.columns),\n                   'importance': rfcla1.feature_importances_}).\\\n                    sort_values('importance', ascending = False)\nfi1.head()\n\nr2 = r2_score(testy1, predictions1)\nprint(\"R squared Value (Variability Explained)\")\nprint(r2)\nprint(' ')\nprint(\"Mean Squared Error Value (Variance):\")\nmse = mean_squared_error(testy1, predictions1)\nprint(mse)\nfi.head()","3177bf6f":"k['rf'] = predictions1\nc = k.groupby(['day_from_jan_first'],as_index=False)['real', 'rf'].sum()\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=c['day_from_jan_first'], y=c['real'],\n                    mode='lines',\n                    name='Real Confirmed Cases'))\nfig.add_trace(go.Scatter(x=c['day_from_jan_first'], y=c['rf'],\n                    mode='lines',\n                    name='Predicted Confirmed Cases' ))\nfig.update_layout(title='Random Forest Predictive Model WITH 7 day Lag')\n\n\n\n\nfig.show()\n","a808b3c0":"fig = px.scatter_3d(new, x='rh', y='temp', z='lag',\n              color='Country_Group', log_z=True)\nfig.update_layout(title='4\/3-4\/11 Average Rate of New Cases With Lag\/ Temp (C) \/ Relative Humidity')\nfig.update_layout(\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=14,\n        color=\"#7f7f7f\"\n    ),\n    scene = dict(\n                    xaxis_title='Rel. Humidity',\n                    yaxis_title='Temp. (C)',\n                    zaxis_title='Cases Rate w\/ Lag')\n)\n\nfig.show()","df4c75ff":"### Predictive Models Conclusion With Lag: \n\n<p style= \"text-indent: 25px;\"> When looking at the linear regression model, we see a R^2 value of ```0.53``` which means that half of the variability can be explained in the regression line and the errors between the real value and predicted is a lot lower than without lag. We can see that the model does significantly better for linear regression when accounting for a 7-day lag. In addtition, when we perform a Random Forest ensemble, we see an even more drastic increase in improvement in correlation between the variables and the results. The R^2 value for the Random Forest Regressor (RF) is ```0.93``` which says that 93% of the variability in the data can be explained in this random forest model. Again, the difference in the two models can be due to the fact that RF is more robust to outliers, and it can predict non-linear data while linear regression can only predict linear data. <\/p>\n\n<p style= \"text-indent: 25px;\"> The feature importance table above shows us the top 5 important variables used by RF to making the prediction. We see that the important variables do not change much with lag. <\/p>\n\n<b>Result:<\/b> Predictive power in models improve significantly with lag-time.","53d66be7":"<p style= \"text-indent: 25px;\"> Similarly, the <b>\u201cCOVID-19: Temperature By Country\/Region Over Time\u201d<\/b> graph below <u>shows how daily average temperatures change across the world with time<\/u>. South of the equator and in the tropics, countries all had dark orange or red markers in January and February, signifying average temperatures between approximately 70\u00b0F and 90\u00b0F. Moving towards March and April, the temperate regions in this area started to lower in temperature to averages of between 60\u00b0F and 75\u00b0F. Regions in the temperate zones north of the Equator fluctuated between 30\u00b0F to 65\u00b0F. Europe saw distinct warmer regions along its West coast compared to the rest of the continent. <\/p>\n","60b3aea5":"## A Global Overview: \n\n<p style= \"text-indent: 25px;\"> In the <b>\u201cCoronavirus Cases Change over Time Graph\u201d<\/b> below, a log scale is used to compare the number of cases across the world over time. As the number of cases increases at a given site, the marker at that site moves from blue to red. China, where the virus spread began, was the only country with a red marker in January. By the end of January, cases had been reported across East and Southeast Asia and Australia. Individual cases had also been reported in Canada, Germany and France, but none of those countries were implementing wide-scale testing. <\/p>\n\n<p style= \"text-indent: 25px;\">  By March 1st, China remained the only location with a red marker, and orange markers appeared in South Korea, Italy, Iran and Japan. Countries transitioning to green markers included Spain, France and Germany. The majority of the sites within Europe and the Middle East were reporting cases at this time. The United States, Saudi Arabia, the African Continent (excluding Nigeria, Egypt and Algeria), and the South American Continent (excluding Brazil and Ecuador) had reported zero cases at this point in time. <\/p>\n\n<p style= \"text-indent: 25px;\"> By March 20, the map was covered in markers \u2014 testing had gone up dramatically. The red to dark orange zones were now South Korea, China, Iran, Italy, Germany, France, Spain, and the United States. All of the areas that were not previously tracking cases were reporting case spread. By April 7th, the coverage of the map looked roughly the same, except that all of the markers had increased in size to represent the overall increase in the number of cases reported. At this point in time, the marker in China had become more orange as the country\u2019s mitigation efforts yielded results. The United States now housed the epicenter of the pandemic. <\/p>\n","fce37946":"<h1><center>The Relationship Between COVID-19 Spread, Temperature, and Relative Humidity<\/center><\/h1>\n<h3><center>By Momo Rutkin, Mohammed Syed, and Ambika Natarajan <\/center><\/h3>\n<h3><center>ENVS\/CHEM 328 - Emory University Spring 2020 <\/center><\/h3>\n\n","9b9df2a3":"<p style= \"text-indent: 25px;\">The <b>\u201cRate of New Cases \/ Density (Pop\/km^2) With Temperature Color Gradient\u201d<\/b> graph shows how the rate of change of case spread at specific locations can be correlated with population density and temperature. Both axes follow a logarithmic scale, the x-axis representing the population density and the y-axis representing the rate of cases reported. The size of the markers represent cases per million people. The color of the markers references temperature as before. From this graph alone, it is difficult to construct an argument. The locations with the faster rates seem to be at temperatures between 55\u00b0F to 65\u00b0F, but there is no overwhelming trend. The similarity in the size of the markers at higher rates of spread could suggest that there are perhaps case density thresholds that promote faster periods of virus spread.<\/p>","b6c9f8ba":"<p style= \"text-indent: 25px;\"> The use of temperature and humidity are likely more applicable on a larger, subcontinent scale. For an analysis at this level, satellite points from the United States, Europe and East Asia will be compared. The figure below for <b>the Average Rate of Cases with respect to Temperature<\/b> graph shows that the majority of cases fall within the range of 40\u00b0F to 70\u00b0F, suggesting a larger range than the ranges used for comparison within Europe. The majority of the satellite points from Europe actually fall outside of this window, on either side of 80\u00b0F. Their rates within this window are around the range of 0.4 to 9.0 cases per day. East Asia sees similar rates within the 40\u00b0F to 70\u00b0F window, with rates ranging around 0.05 to 21.4 cases per day. <\/p>\n\n<p style= \"text-indent: 25px;\"> The United States differs greatly in this same window, with a range of about 11.8 to 9.9 thousand cases per day. This is significant, because the initiation of mitigation procedures occurred around early to mid-March for both Europe and the United States, whereas for East Asia, some nations had been implementing mitigation practices since January (Gan, 2020). While European countries mainly responded with a federally-mandated lockdown, the variation of policies in the United States as well as costly access to healthcare were likely major contributors to the variation in rates (IMF, 2020). Based on this graph, the temperature difference could be a factor as well. <\/p>\n","2e17da16":"## References \n","416adc6b":"## Conclusions and Policy Implementation:\n","946396ca":"<a href ><img src=\"https:\/\/i.ibb.co\/mTtqxP5\/Screenshot-2020-05-05-20-33-31.png\" alt=\"Screenshot-2020-05-05-20-33-31\" border=\"0\"><\/a>\n<a href><img src=\"https:\/\/i.ibb.co\/0BNcnGT\/Screenshot-2020-05-05-20-33-54.png\" alt=\"Screenshot-2020-05-05-20-33-54\" border=\"0\"><\/a>\n<a href><img src=\"https:\/\/i.ibb.co\/J7TTmsx\/Screenshot-2020-05-05-20-34-18.png\" alt=\"Screenshot-2020-05-05-20-34-18\" border=\"0\"><\/a>","e8f75f98":"<p style= \"text-indent: 25px;\"> The graph that shows <b> the average rate of new cases vs. temperature vs. relative humidity <\/b> bewlow could be another indicator that temperatures between 40\u00b0F and 70\u00b0F and relative humidity values between 60% to 80% might be optimal for the spread of the virus. When viewing the graph with temperature as the primary x-axis, a somewhat parabolic shape is observed, with a peak around 50\u00b0F. When viewing the graph with relative humidity as the primary x-axis, the majority of the parabolic arrangement of satellite points fits within the 60% to 80% relative humidity window. <\/p>","77c0b8cb":"### Extracted and Added: \n* daily cases\n* cases per million people\n* daily case rate (1st deriv)\n* daily case growth factor (2nd deriv)","ce28059d":"### Predictor #2: Random Forest ","ef3bfb23":"### Merged JHU COVID-19 Forecasting Competition and NOAA JSOD Data","45c82e77":"## Introduction:\n\n<p style= \"text-indent: 25px;\"> Around the world, daily life has been obstructed by the spread of SARS-CoV-2, a virus that is unfamiliar to the human population. The exact time and place of transmission from an animal reservoir to a human host is disputed, but the first observed outbreak was in Wuhan, China, in December of 2019. The arrival of the Lunar New Year meant that many people travelled out of Wuhan to visit their hometowns, spreading the virus throughout China. Apart from domestic travel, international travel contributed to the creation of a pandemic, concern rising when people who had not travelled started testing positive for the virus. This indicated community spread of COVID-19 (Wu, 2020). <\/p>\n\n<p style= \"text-indent: 25px;\"> Travel is one of the leading contributors of disease spread, but once the virus is in several locations, additional factors such as population density, access to adequate healthcare, and the effectiveness of policy implementation can all have an impact on the spread of the virus. Another factor to consider with several subcomponents is regional climate. Certain temperatures and humidity levels can impact the ability for humans to contract a disease (LaFave, 2020). Additionally, the duration for which a virus survives might have a relationship with temperature, known as its seasonality (Langlois, 2020). The analysis reported in this paper does not separate these two variables \u2014 instead it seeks to draw any correlation with temperature and humidity and use that information as a starting point for further inquiry. <\/p>\n\n## Data Collection\n\n<p style= \"text-indent: 25px;\"> The data used in this study has been primary collected from the <a href=\"https:\/\/www.kaggle.com\/c\/covid19-global-forecasting-week-1\/discussion\">Johns Hopkins University Center for Systems Science and Engineering\u2019s COVID-19 Forecasting Competition<\/a>, <a href=\"https:\/\/www.kaggle.com\/noaa\/gsod\">the NOAA GSOD dataset<\/a>, and <a href=\"https:\/\/www.kaggle.com\/tanuprabhu\/population-by-country-2020\">WorldOMeter's Population By Country 2020<\/a>. The data sets were chosen for their relatively clean and up-to-date data. After merging the data sets, the data of interest consists of <u>322 satellite reference points from 116 countries<\/u> from <u>2020-01-22 to 2020-04-11<\/u>. We have the dataset and the purpose of the data listed below <\/p>\n\n### Joined Dataset From 01\/22\/2020 - Present (05\/14\/2020)\n\n* [JHU COVID-19 Forecasting Competition](https:\/\/www.kaggle.com\/c\/covid19-global-forecasting-week-1\/discussion) \n    * Province_State\n    * Country_Region\n    * Date\n    * ConfirmedCases\n    * Fatalities\n    * Lat and Long \n    \n\n* [the NOAA GSOD dataset](https:\/\/www.kaggle.com\/noaa\/gsod)\n    * Relative Humidity (rh) per day \n    * Temperature (min, max, average) in Celcius per day\n    * Wind Speed (wdsp) per day\n    * Precipitation per day \n    * Temperature Variance per day \n    \n    \n* [WorldOMeter's Population By Country 2020](https:\/\/www.kaggle.com\/tanuprabhu\/population-by-country-2020)\n    * Urban Population percentage per country \n    * Density per country (P\/km^2)\n    * Population per country \n    * Median age per country \n   \n### Dataset From 1\/22\/2020 - Present (05\/14\/2020)   \n* [Novel Corona Virus 2019 Dataset](https:\/\/www.kaggle.com\/sudalairajkumar\/novel-corona-virus-2019-dataset)\n    * Province_State\n    * Country_Region\n    * Date\n    * ConfirmedCases\n    * Fatalities\n","b550ebe2":"<p style= \"text-indent: 25px;\"> <b>The Average Rate of Cases versus Relative Humidity Graph<\/b> below gives a slightly different perspective. Most of Europe\u2019s cases fall between 70% to 80%, whereas most of the cases in the US fall between 60% to 70%, with some cases between 50% to 60%. This suggests that drier climates might have higher transmission rates for the virus. East Asia has lower rates of case spread overall, but there are no distinct groupings by relative humidity within this category. <\/p>\n","434141fa":"<p style= \"text-indent: 25px;\"> <u> Global inconsistencies in reporting cannot be corrected within the data set <\/u>, but attention to news articles discussing inconsistencies in reporting in specific locations can be helpful for a more critical analysis. The graph below, for example, shows how a <u> significant increase in the reporting of cases in Chicago, Illinois, from March 07 - March 13 <\/u> caused the acceleration of case spread to appear significantly higher than that of cities such as New York, which is now the epicenter of the pandemic (Correal, 2020). <\/p>\n\nZoom out in the graph to see how the U.S. growth factors compare to other locations around the world. ","8fb6645d":"## A Regional Breakdown: Within Europe\n\n<p style= \"text-indent: 25px;\"> The impact of temperature on sites in different countries can be assessed in the context of Europe. The comparison will incorporate 3 sets of countries: <b>the United Kingdom and Spain, Belgium and Germany, and the Netherlands and France<\/b>. These countries have been paired based on similar temperature trends. <\/p>\n","a6a708e3":"<p style= \"text-indent: 25px;\"> <u> This means that the data is also more reliable when examined over a longer interval of time<\/u>, assuming that regional testing either remains at a consistent level or increases gradually. <\/p>\t\n\n<p style= \"text-indent: 25px;\"> In the <b>\u201cWorld Daily Case Count over Time Graph\u201d <\/b>below, the growth of cases worldwide increased exponentially between mid-February and late March. By April, it appeared as if the increase in cases started to taper off, as per the general logistic trend that the pandemic would be expected to follow. <\/p>\t","92a74d5c":"## A Regional Breakdown: The United States, Europe and East Asia\n\n","f5193533":"## Data Projection: \n\n<p style= \"text-indent: 25px;\"> Based on the data collected below, a projection was run for the predicted cases per location based on a number of climate and population determinants which includes temperature and relative humidity. These models were chosen for their robustness to outliers and predictive accuracy on log transformed data <\/p>\n\n### The models used for the predictive models includes:\n* **Linear Regression** \n    * <u>metrics:<\/u> \n        * Mean Absolute Error (MAE)\n        * <u>reason:<\/u> more robust to outliers \n        * R Squared (R^2)\n        * <u>reason:<\/u> shows how well our line fits the data \n* **Random Forest** \n    * \n        * Mean Squared Error (NMSE)\n        * <u>reason:<\/u> need to highlight variance \n        * R Squared (R^2)\n        * <u>reason:<\/u> shows how well our line fits the data \n\n* <u>NOTE:<\/u> The <b>R^2 value<\/b> has a couple of different scales, the scale used for this speicific R^2 metric is from negative infinity to 1. A negative score indicates that the result cannot fit because of the non-linearity of the data, and does not capture the variance in the data at all. \n    * the equation is: ```1 - residual sum of square \/ total sum of squares```\n\n### The variables used for the predictive models include:\n* Population\n* Wind Speed daily \n* Precipitation daily \n* Fog daily \n* Average Temperature (C) daily \n* Min Temperature (C) daily \n* Max Temperature (C) daily \n* Temperature Variance (C) daily \n* Relative Humidity \n* Density Pop\/km^2 per Country \n* Median Age per Country \n* Urban Pop % per Country \n\n### Training and Test Set Length \n* <u>train set<\/u>: 01\/22\/2020 - 03\/22\/2020 (80% of the data) 8-week training \n* <u>test set<\/u>: 03\/23\/2020 - 04\/11\/2020 (20% of the data) 3-week prediction ","51599906":"<p style= \"text-indent: 25px;\"> <b>The data suggests that cooler, slightly humid regions promote more rapid case spread at the subcontinent scale. <\/b> This study would benefit from additionally analyzing trends in human susceptibility to the virus as well as the seasonality of the virus, should that data become available. <\/p>\n\n<p style= \"text-indent: 25px;\"> Additionally, average temperatures were used instead of daily variances in temperature. Susceptibility to the virus could also be a result of exposure to a range of temperatures in a given day. <\/p>\n\n<p style= \"text-indent: 25px;\"> Going forward, policymakers should note these temperature and relative humidity values as nations start to relax social distancing guidelines. The Spring and Fall temperate seasons are likely to see higher rates of case spread. A humid, but cool, Summer evening might enable faster case spread than a hot, humid Summer afternoon. Understanding broad trends for when human mobility is relatively safe will aid both policymakers and the general public in combatting the pandemic. <\/p>","7e47b86e":"## Result: Final Dataset (updated) from 01-22-2020 to 05-14-2020 \n### DOES NOT contain climate determinant variables  ","5259b0e8":"### Netherlands and France\n<p style= \"text-indent: 25px;\"> The Netherlands and France both had warmer daily average temperatures ranging from 70.94\u00b0F to 78.92\u00b0F and 65.37\u00b0F to 73.04\u00b0F, respectively. There is no overlap between their trends, but on February 4th there was a difference in temperature between the countries of about 0.5\u00b0F. The rates of cases for these two countries changed greatly with time. On March 15, the Netherlands reported a rate of 33.2 cases per day while France reported a rate of 38.7 cases per day. On March 28, the Netherlands reported a rate of 235.1 cases per day while France reported a rate of 388.9 cases per day. On April 4th, this difference reached its maximum with the Netherlands reporting a rate of 193.9 cases per day and France reporting a rate of 1,405.4 cases per day.<\/p> \n\n### Belgium and Germany\n<p style= \"text-indent: 25px;\"> Belgium and Germany also displayed similar temperature trends ranging from 34.60\u00b0F to 62.70\u00b0F and 27.1\u00b0F to 55.6\u00b0F, respectively. There is a high level of overlap between their temperature trends. The difference in rates for Belgium and Germany was even more pronounced than it was for the Netherlands and France. On March 15th, Belgium reported a rate of 163.5 cases per day while Germany reported a rate of 1,060 cases per day. On March 29, Belgium reported a rate of 1776.0 cases per day while Germany reported a rate of 5612 cases per day. On April 4th, Belgium reported a rate of 1541.5 cases per day while Germany reported a rate of 5649.0 cases per day. <\/p>\n\n### United Kingdom and Spain\n<p style= \"text-indent: 25px;\"> The United Kingdom and Spain also had similar intermediate temperature ranges from 60.91\u00b0F to 68.29\u00b0F and 52.9\u00b0F to 60.10\u00b0F. There is some overlap between their temperature ranges. Their differences in rates of cases were even more pronounced. On March 15th, the United Kingdom reported a rate of 15.6 cases per day compared to Spain\u2019s 1,283 cases per day. On March 28, the United Kingdom reported a rate of 250 cases per day in comparison to Spain\u2019s 7,724.5 cases per day. On April 4th, the rate for the UK was at 377.5 cases per day in comparison to the rate for Spain at 7,051.5 cases per day. <\/p>\n\n\n### Conclusion from comparison\n<p style= \"text-indent: 25px;\"> <u>From these comparisons alone, it is difficult to draw a distinct correlation between temperature and case spread. <\/u> On the basis of the rate curves, rather than temperature, it looks like Spain should be paired with Germany, France should be paired with Belgium, and the Netherlands should be paired with the UK. <\/p>\n\n<p style= \"text-indent: 25px;\"> While most of the pairings involve a country from the intermediate temperatures being paired with a country from an extreme temperature category, the only real anomaly is the pairing of the Netherlands with the United Kingdom. This could perhaps be the result of the implementation of certain policies. Both countries initially considered herd immunity strategies, shutting down institutions and direct contact services but leaving other services and businesses open while maintaining distancing (Holligan, 2020). The United Kingdom also experienced difficulties in obtaining functional testing kits as of April 16th, in an effort to step up testing from 10,000 people per day to 100,000 people per day (David, 2020). This may have impacted the scale of cases reported. <\/p>\n\n<p style= \"text-indent: 25px;\"> While insufficient information might rule out a straightforward relationship for the Netherlands and the UK, a more reliable comparison might be France and Belgium. France went into a rigid lockdown early in the onset of their epidemic and had a relatively high hospital capacity (Nossiter, 2020). Belgium has also attempted to be very transparent in their case counts, reporting a higher death rate than most countries by including deaths that are presumed, rather than simply confirmed, to be the result of the coronavirus (\u201cBelgium unveils plans to lift lockdown\u201d, 2020). The differences in temperatures might be too wide to draw any significant conclusions. <\/p>\n","54ef29e1":"### Prediction Take 2: Prediction With Lag Time \n<p style= \"text-indent: 25px;\"> <u>Something that we did not consider when trying to compare climate determinants and cases was the lag in response to the climate determinants.<\/u> Many countries, including the U.S., test people when they start showing signs of the virus such as dry coughs and high fevers. These symptoms kick in, on average, in around 6-7 days. As a result, we can assume that the people that had caught the virus due to a speicfic climate condition would not show signs of contagion until around a week has passed. As a result, we thought to run the predictive model with this lag in mind.  <\/p>\n\n### Predictor #1: Linear Regression With Lag \n","e4255a08":"<p style= \"text-indent: 25px;\"> The <b>\u201cCOVID-19: Relative Humidity By Country\/Region Over Time\u201d<\/b> graph differs significantly from the temperature graph, and is therefore useful in furthering the regional climate analysis. As of January 22nd, much of North Africa, South Africa and Namibia have relative humidities of around 20% to 30%. The rest of the world generally saw higher average relative humidities ranging between 50% and 85%. Compared to daily average temperature, this variable changed rapidly. For example, on April 7th, several Eastern European countries reported relative humidity values in the ranges of between 10% to 40%, but the relative humidity values for these same countries had been consistently much higher. <\/p>\n","82e25c06":"### Predictor #1: Linear Regression ","f79cc21f":"### Predictive Models Conclusion \n\n<p style= \"text-indent: 25px;\"> When looking at the linear regression model, we see a R^2 value of ```-0.77``` which means that none of the variability can be explained in the regression line despite the mean absolute error being relatively low. However, when we perform a Random Forest ensemble which averages the results of 100 decidion regression trees, we start to see some correlation between the variables and the results. The R^2 value for the Random Forest Regressor (RF) is ```0.187``` which is significantly higher than the regressor. This increase in accuracy can be due to the fact that RF is more robust to outliers and it can predict non-linear data while linear regression can only predict linear data. <\/p>\n\n<p style= \"text-indent: 25px;\"> The feature importance table above shows us the top 5 important variables used by RF to making the prediction. We see here that population is the most important factor by far, followed by the ```date```, ```median age per country```, ```minumum daily temperature```, and ```density per country``` <\/p>\n\n<b>Result:<\/b> Climate importance in prediction cannot be seen through the data in this format.\n\n","3490a0fe":"## Result: Final Dataset (weather_country) from 01-22-2020 to 05-14-2020","ce485386":"### Joined WorldOMeter's Population By Country 2020 With Previous Dataset "}}