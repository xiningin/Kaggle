{"cell_type":{"53e1ceb7":"code","6c90bd84":"code","349ec280":"code","acc9051a":"code","4ad510fd":"code","c84f375c":"code","b095f20b":"code","5397d8de":"code","dd27dd0b":"code","d15d38eb":"code","38294473":"code","4a6476e2":"code","c0929235":"markdown","7db0a9a4":"markdown","daf6c16e":"markdown","d19f8c5f":"markdown","364fa582":"markdown","d0acbcc7":"markdown","3cc106a7":"markdown","e9fd9960":"markdown","8e89292b":"markdown","a9dea253":"markdown"},"source":{"53e1ceb7":"import numpy as np\nimport pandas as pd\n\nimport json\nimport re\nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom math import nan\n\nfrom future.utils import iteritems\nfrom operator import itemgetter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical, plot_model\nfrom keras.models import Model, Input\nfrom keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, BatchNormalization, SpatialDropout1D, Flatten, Lambda\nimport keras as k\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nimport tensorflow as tf","6c90bd84":"train = pd.read_csv('\/kaggle\/input\/train-token\/cleaned_train.csv')\ncleaned_test = pd.read_csv('\/kaggle\/input\/train-token\/cleaned_test.csv')\ntest = pd.read_csv('\/kaggle\/input\/scl-2021-ds\/test.csv')\n\n# laundry\ntrain['raw_address'] = train['raw_address'].str.replace(r'\\blaun\\b', 'laundry', regex=True)\ncleaned_test['raw_address'] = cleaned_test['raw_address'].str.replace(r'\\blaun\\b', 'laundry', regex=True)\ntest['raw_address'] = test['raw_address'].str.replace(r'\\blaun\\b', 'laundry', regex=True)\n\n# verify that cleaning up as the data has improved the matches\nPOI_in_raw_address = 75351\ntrain['POI_in_raw_address'] = np.where(train['POI'].isnull(), False, train.apply(lambda x: str(x.POI) in str(x.raw_address), axis=1))\nmapping_improved_POI = train['POI_in_raw_address'].sum() >= POI_in_raw_address\nprint(\"Mapping improved POI matches in raw_address:\", mapping_improved_POI, train['POI_in_raw_address'].sum())\n\nstreet_in_raw_address = 212470\ntrain['street_in_raw_address'] = np.where(train['street'].isnull(), False, train.apply(lambda x: str(x.street) in str(x.raw_address), axis=1))\nmapping_improved_street = train['street_in_raw_address'].sum() >= street_in_raw_address\nprint(\"Mapping improved street matches in raw_address:\",mapping_improved_street, train['street_in_raw_address'].sum())\n\n# if train targets can be found in test raw_address, assign as matched targets in cleaned_test\nvc = train[train['POI'].str.count(' ')>1]['POI'].value_counts()\ncommon_POI = (list(vc[vc > 1].index))\nfor POI in common_POI:\n    for i, raw_address in enumerate(cleaned_test['raw_address']):\n        if POI in raw_address:\n            cleaned_test.loc[i, 'match_POI'] = POI\n            \nvc = train[train['street'].str.count(' ')>1]['street'].value_counts()\ncommon_street = (list(vc[vc > 1].index))\nfor street in common_street:\n    for i, raw_address in enumerate(cleaned_test['raw_address']):\n        if street in raw_address:\n            cleaned_test.loc[i, 'match_street'] = street\n            \n# see number of matches\nprint(\"matches in cleaned test:\")\nprint(cleaned_test[['match_POI', 'match_street']].notna().sum())\n\ntest = test.merge(cleaned_test[['id', 'match_POI', 'match_street']], how='left', on='id')\ntest.head()","349ec280":"def tokenize_address_by_sep(df, sep = ' '):\n    df['raw_address'] = df['raw_address'].str.replace(\",\",\", \")\n    df['raw_address'] = df['raw_address'].str.replace(\", \",\" , \")\n    df['raw_address'] = df['raw_address'].str.replace(\"  \",\" \")\n    data = pd.concat([pd.Series(row['id'], row['raw_address'].split(sep))\n                      for _, row in df.iterrows()]).reset_index()\n    data.columns = ['word', 'sentence_idx']\n    data['tag'] = np.nan\n    \n    # auto tag all commas as comma\n    data['tag'] = np.where(data['word']==',', 'comma', data['tag'])\n    \n    data = data[['sentence_idx','word','tag']]\n    return data\n\ntest_tokens = tokenize_address_by_sep(test)\ntest_tokens['tag'] = test_tokens['tag'].astype(str)\ntest_tokens = test_tokens[test_tokens['word'].notnull()].reset_index(drop=True)\n\n\ntrain_tokens = pd.read_csv('\/kaggle\/input\/train-token\/train_tokens.csv')\ntrain_tokens = train_tokens[train_tokens['word'].notnull()].reset_index(drop=True)\ntrain_tokens['tag'] = train_tokens['tag'].astype(str)\n\ntrain_tokens['word'] = train_tokens['word'].str.replace(r'\\blaun\\b', 'laundry', regex=True)\ntest_tokens['word'] = test_tokens['word'].str.replace(r'\\blaun\\b', 'laundry', regex=True)\n\ndisplay(train_tokens.head())\n\ntrain_tokens['tag'].value_counts()","acc9051a":"class SentenceGetter(object):\n    def __init__(self, dataset):\n        self.n_sent = 1\n        self.dataset = dataset\n        self.empty = False\n        agg_func = lambda s: [(w, t) for w,t in zip(s[\"word\"].values.tolist(),\n                                                    s[\"tag\"].values.tolist())]\n        self.grouped = self.dataset.groupby(\"sentence_idx\").apply(agg_func)\n        self.sentences = [s for s in self.grouped]\n\n    def get_next(self):\n        try:\n            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n            self.n_sent += 1\n            return s\n        except:\n            return None\n        \ndef preprocess(train, test, return_y=False):\n    data = pd.concat([train, test])\n\n    # get the sentences for train, test, and data\n    sentences_train = SentenceGetter(train).sentences\n    sentences_test = SentenceGetter(test).sentences\n    sentences = SentenceGetter(data).sentences\n    maxlen = max([len(s) for s in sentences])\n    print(\"Max sequence length:\", maxlen)\n    \n    words = list(set(data[\"word\"].values))\n    n_words = len(words)\n    word2idx = {w: i for i, w in enumerate(words)}\n    word2idx[\"\"] = n_words\n    n_words = len(words)+1\n    print(\"Vocab size of train and test combined:\", n_words)\n\n    tags = []\n    for tag in set(data[\"tag\"].values):\n        if tag is nan or isinstance(tag, float):\n            tags.append('nan')\n        else:\n            tags.append(tag)\n    n_tags = len(tags)\n\n    \n    tag2idx = {t: i for i, t in enumerate(tags)}\n    idx2tag = {v: k for k, v in iteritems(tag2idx)}\n    most_freq_tag = train['tag'].value_counts().index[0]\n    \n    def get_X_and_y (sentences, return_y=return_y):\n    \n        X = [[word2idx[w[0]] for w in s] for s in sentences]\n        X = pad_sequences(maxlen=maxlen, sequences=X, padding=\"post\", value=len(word2idx))\n\n        if return_y:\n            y = [[tag2idx[w[1]] for w in s] for s in sentences]\n            print(\"pad y with most freq tag:\", most_freq_tag)\n            y = pad_sequences(maxlen=maxlen, sequences=y, padding=\"post\", value=tag2idx[most_freq_tag])\n            y = [to_categorical(i, num_classes=n_tags) for i in y]\n\n            return X, y\n        else:\n            return X\n        \n    X_train, y_train = get_X_and_y(sentences_train, return_y=True)\n    if return_y==False:\n        X_test = get_X_and_y(sentences_test, return_y=False)\n        y_test = []\n    else:\n        X_test, y_test = get_X_and_y(sentences_test)\n    return X_train, y_train, X_test, y_test, n_words, idx2tag, word2idx, n_tags\n\nX_train, y_train, X_test, _, n_words, idx2tag, word2idx, n_tags = preprocess(train_tokens, test_tokens)\nidx2word = {v: k for k, v in iteritems(word2idx)}\ntag2idx = {v: k for k, v in iteritems(idx2tag)}\nprint(\"pad x with:\", idx2word[len(word2idx)])","4ad510fd":"\ndef categorical_accuracy(y_true, y_pred):\n    return k.backend.cast(k.backend.equal(k.backend.argmax(y_true, axis=1),\n                                          k.backend.argmax(y_pred, axis=1)),\n                          k.backend.floatx())\n\ndef get_model(X_train):\n    k.backend.clear_session()\n    \n    # use maxlen here for dimension of embedding and lstm units\n    maxlen = X_train.shape[1]\n    Input_layer = Input(shape=(maxlen,))\n    model = Embedding(input_dim = n_words+1, output_dim = maxlen, input_length = maxlen)(Input_layer)\n    model = Bidirectional(LSTM(units=maxlen,return_sequences = True, recurrent_dropout = 0.1))(model)\n\n    model = SpatialDropout1D(0.1)(model)\n    model = TimeDistributed(Dense(n_tags*32, activation = 'relu'))(model)\n    model = SpatialDropout1D(0.1)(model)\n    model = TimeDistributed(Dense(n_tags*32, activation = 'relu'))(model)\n    model = SpatialDropout1D(0.1)(model)\n    model = TimeDistributed(Dense(n_tags*32, activation = 'relu'))(model)\n    model = SpatialDropout1D(0.1)(model)\n    model = TimeDistributed(Dense(n_tags*32, activation = 'relu'))(model)\n    model = SpatialDropout1D(0.1)(model)\n    model = TimeDistributed(Dense(n_tags*16, activation = 'relu'))(model)\n    model = TimeDistributed(Dense(n_tags*8, activation = 'relu'))(model)\n    model = TimeDistributed(Dense(n_tags*4, activation = 'relu'))(model)\n    model = TimeDistributed(Dense(n_tags*2, activation = 'relu'))(model)\n    Output = TimeDistributed(Dense(n_tags, activation = 'softmax'))(model)\n\n    model = Model(Input_layer, Output)\n    adam = k.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=[categorical_accuracy])\n    #print(model.summary())\n    return model\n\ndef plot_history(history):\n    epoch_with_lowest_val_loss = min(enumerate(model.history.history['val_loss']), key=itemgetter(1))[0]+1\n    print(\"best epoch: \", epoch_with_lowest_val_loss)\n    # Plot the graph \n    plt.style.use('ggplot')\n    \n    accuracy = history.history['categorical_accuracy']\n    val_accuracy = history.history['val_categorical_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(accuracy) + 1)\n    plt.figure(figsize=(12, 5))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(x, accuracy, 'b', label='Training acc')\n    plt.plot(x, val_accuracy, 'r', label='Validation acc')\n    plt.axvline(x=epoch_with_lowest_val_loss, color='k', linestyle='--')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.axvline(x=epoch_with_lowest_val_loss, color='k', linestyle='--')\n    plt.title('Training and validation loss')\n    plt.legend()\n    \n    plt.show()","c84f375c":"# save the best model based on validation loss\nfilepath=\"validation_best_epoch.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\ncallbacks_list = [early_stop, checkpoint]\n\nmodel = get_model(X_train)\n# reshape y_train so that the sum of probability of each tag adds up to 1 for each word\nhistory = model.fit(X_train, np.array(y_train), \n                    batch_size=256, \n                    shuffle=True, \n                    epochs=5, \n                    validation_split=0.1, \n                    verbose=1, callbacks=callbacks_list)\n\n# get the best epoch\nplot_history(history)","b095f20b":"# show model architecture\nmodel.load_weights(filepath)\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","5397d8de":"# train one more epoch\nhistory = model.fit(X_train, np.array(y_train), \n                    batch_size=256, \n                    shuffle=True, \n                    epochs=1, \n                    verbose=1)\n\ny_pred = model.predict(X_test, verbose=1)","dd27dd0b":"from future.utils import iteritems\nfrom operator import itemgetter\n\n# Assign tags to words inside each row of raw_address\nidx2word = {v: k for k, v in iteritems(word2idx)}\n\nfor i in range(len(test)):\n    \n    raw_address = test.loc[i, \"raw_address\"].replace(\" nan \",\" \").replace(\",\",\" \").replace(\"   \",\" \").replace(\"  \",\" \").strip()\n    \n    # seq of tagging in each raw_address\n    sequence = [idx2tag[idx] for idx in np.argmax(y_pred[i], axis=1)]\n    \n    # seq of words in each raw_address\n    word = [str(idx2word[idx]) for idx in X_test[i]]\n    \n    # if street is present in the predicted tag sequence\n    if 'street_start' in sequence:\n        # if street has more than one word, get the predicted string from index\n        if 'street_end' in sequence:\n            street = (\" \".join(word[sequence.index(\"street_start\"):sequence.index(\"street_end\")+1])).replace(\"   \",\" \").replace(\"  \",\" \").strip()\n\n        # if street has only one word, use the predicted word\n        else:\n            street = (word[sequence.index(\"street_start\")])\n        if street in raw_address:\n            test.loc[i, 'street_pred'] = street\n        else:\n            print(\"row \"+str(i)+\": predicted street '\"+street+\"' is not found in '\"+raw_address+\"'\")\n            \n    # if POI is present in the predicted tag sequence\n    if 'POI_start' in sequence:\n        # if POI has more than one word, get the predicted string from index\n        if 'POI_end' in sequence:\n            POI = (\" \".join(word[sequence.index(\"POI_start\"):sequence.index(\"POI_end\")+1])).replace(\"   \",\" \").replace(\"  \",\" \").strip()\n        # if POI has only one word, sue the predicted word\n        else:\n            POI = (word[sequence.index(\"POI_start\")])\n        if POI in raw_address:\n            test.loc[i, 'POI_pred'] = POI\n        else:\n            print(\"row \"+str(i)+\": predicted POI '\"+POI+\"' is not found in '\"+raw_address+\"'\")","d15d38eb":"test.head()","38294473":"# fill null using matched targets from train, and remove commas\ntest['POI_pred'] = test['match_POI'].fillna(test['POI_pred']).str.replace(\",\",\"\").str.strip()\ntest['street_pred'] = test['match_street'].fillna(test['street_pred']).str.replace(\",\",\"\").str.strip()\n\ntest['POI\/street'] = test['POI_pred'].fillna('').astype(str)+'\/'+test['street_pred'].fillna('').astype(str)\n\ntest[['id', 'raw_address', 'POI\/street']].head()","4a6476e2":"test[['id', 'POI\/street']].to_csv('submission_binary_crossentropy.csv', index=False)","c0929235":"# Load [Tokenised Data](https:\/\/www.kaggle.com\/chongkairu\/scl-2021-data-science-part-2-tag-train-words)","7db0a9a4":"# raw test, with commas","daf6c16e":"We're going to define our own categorical accuracy that is more indicative of the eventual score\n\ny.shape = (len(y), maxlen, n_tags)\nThe default categorical_accuracy argmax to axis=-1, which in our case is n_tags, this gives us the percentage that the assigned tag for each word is correct.\nHowever, because we pad y with nan, the % matched is overly inflated by that.\nwe're changing it to argmax on axis = 1 which is maxlen instead, this gives us the percentage that the predicted word for each tag is correct.","d19f8c5f":"# Fit and Predict","364fa582":"# Decode back to strings","d0acbcc7":"# Validate using a dropout the same size as test","3cc106a7":"we now remove commas because when we kept the commas we keep getting very long predicted strings which contain commas","e9fd9960":"# Model","8e89292b":"# Load [Cleaned Data](https:\/\/www.kaggle.com\/zeyalt\/scl-2021-data-science-part-1-data-cleaning)","a9dea253":"# Process tokens into vectors"}}