{"cell_type":{"06f76f65":"code","e05e010e":"code","e82367b6":"code","f36471e7":"code","13032084":"code","ea995778":"code","c664d9db":"code","8735857b":"code","e3b0dfb7":"code","fc7293a7":"code","4404fe2f":"code","e976fe87":"code","e5b986d1":"code","0cbadc33":"code","3cc5ca54":"code","596e1667":"code","e0b725e5":"code","b448a620":"code","9c5d0b38":"code","e1067e55":"code","b418d868":"code","53730bc9":"code","990fb2d8":"code","28e3cd79":"code","2995d4cc":"code","fc3e58a9":"code","4f14c776":"code","bffdd726":"code","791a2cf7":"code","80421e8c":"code","08fca54b":"markdown","d43cc122":"markdown","20b41dab":"markdown","a6ab1966":"markdown","b1ec01ab":"markdown","21e4a295":"markdown","d0ea51d7":"markdown","eee86ad3":"markdown","2d10b6c8":"markdown","f017c257":"markdown","b77f37e2":"markdown","41ffa5ff":"markdown","c6d3c266":"markdown","2055feef":"markdown","c633313e":"markdown","87d1dcfc":"markdown","049a808e":"markdown","9e3a6c77":"markdown","6c176ea0":"markdown","1375a6af":"markdown","c479508b":"markdown","a9f16b71":"markdown","14fa926a":"markdown","7f9a80f4":"markdown","a0f55aba":"markdown","c0921a54":"markdown","031ccea2":"markdown","96f260be":"markdown","5abae4ff":"markdown","34c6a136":"markdown","e07216a4":"markdown","005279c5":"markdown","fbfccf7a":"markdown","f6b89fb7":"markdown","e1067abb":"markdown","811d30cc":"markdown","8f29c7eb":"markdown","ce49f951":"markdown"},"source":{"06f76f65":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport missingno as msno\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, confusion_matrix\n\nimport warnings\nwarnings.filterwarnings('ignore')","e05e010e":"# load data\ntrain = pd.read_csv(\"..\/input\/widsdatathon2021\/TrainingWiDS2021.csv\")\ntest = pd.read_csv(\"..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv\")\ntest = test.sort_values(by='encounter_id')     # to order `test data` as per `solution template`\nsol = pd.read_csv(\"..\/input\/widsdatathon2021\/SolutionTemplateWiDS2021.csv\")\n\n# split out train labels\ntrain_labels = train[['diabetes_mellitus']]\ntrain = train.drop(['Unnamed: 0', 'diabetes_mellitus'], axis=1)\ntest = test.drop(['Unnamed: 0'], axis=1)\n\nprint(\"Train:\", train.shape)\nprint(\"Train label:\", train_labels.shape)\nprint(\"Test:\", test.shape)","e82367b6":"train.head()","f36471e7":"# group columns by datatypes\n# source: https:\/\/stackoverflow.com\/questions\/22470690\/get-list-of-pandas-dataframe-columns-based-on-data-type\n\ncols_by_dtype = train.columns.to_series().groupby(train.dtypes).groups\ncols_by_dtype = {k.name: v for k, v in cols_by_dtype.items()}\ncols_by_dtype","13032084":"msno.matrix(train[cols_by_dtype['int64']], color = (0.52, 0, 0))\nplt.show()","ea995778":"msno.matrix(train[cols_by_dtype['object']], color = (0.52, 0, 0))\nplt.show()","c664d9db":"msno.matrix(train[cols_by_dtype['float64'][:30]], color = (0.52, 0, 0))\nplt.show()","8735857b":"msno.matrix(train[cols_by_dtype['float64'][30:60]], color = (0.52, 0, 0))\nplt.show()","e3b0dfb7":"msno.matrix(train[cols_by_dtype['float64'][60:90]], color = (0.52, 0, 0))\nplt.show()","fc7293a7":"msno.matrix(train[cols_by_dtype['float64'][90:120]], color = (0.52, 0, 0))\nplt.show()","4404fe2f":"msno.matrix(train[cols_by_dtype['float64'][120:]], color = (0.52, 0, 0))\nplt.show()","e976fe87":"def miss_val_percent(df, thresh = 70.0):\n    \"\"\"\n    Remove all features that have a missing value percentage greater than 70%\n    \"\"\"\n    mvp = ((df.isnull().sum()) \/ (len(df))) * 100\n    return mvp[mvp > thresh]","e5b986d1":"# all with more than 70% values missing; remove them; do it for both train and test features\n\n# create retained features list\nretained_features = set(train.columns) - set(miss_val_percent(train).index)\n\n# subset train and test sets\ntrain = train[retained_features]\ntest = test[retained_features]\n\n# check\nprint(\"Train:\", train.shape)\nprint(\"Test:\", test.shape)","0cbadc33":"# let's update our cols_by_dtype dictionary\ncols_by_dtype = train.columns.to_series().groupby(train.dtypes).groups\ncols_by_dtype = {k.name: v for k, v in cols_by_dtype.items()}\ncols_by_dtype","3cc5ca54":"def get_row_miss_percent(df):\n    \"\"\"\n    Adds a feature into the dataframe indicating the missing\n    value percentage of the corresponding row\n    \"\"\"\n    ncols = df.shape[1]\n    df['miss_percent'] = (df.isnull().sum(axis=1) \/ ncols) * 100\n    \n    return df","596e1667":"# calculate missing percent per row for train and test\nget_row_miss_percent(train)\nget_row_miss_percent(test)\n\n# subset train\ntrain = train.loc[train['miss_percent'] < 30]\n\n# check\nprint(\"Train:\", train.shape)\nprint(\"Test:\", test.shape)","e0b725e5":"print(\"The missing values in the train dataset are:\", train.isnull().sum().sum())","b448a620":"msno.bar(train[cols_by_dtype['object']], color = (0.52, 0, 0))\nplt.show()","9c5d0b38":"# create the categorical imputer object\ncat_imputer = SimpleImputer(missing_values = np.nan, strategy = \"constant\", fill_value = \"missing\")\n\n# subset data to have only categorical features\ntrain_cat = train[cols_by_dtype['object']]\ntest_cat = test[cols_by_dtype['object']]\n\n# impute\ntrain_cat.iloc[:, :] = cat_imputer.fit_transform(train_cat)\ntest_cat.iloc[:, :] = cat_imputer.fit_transform(test_cat)","e1067e55":"# create the continuous imputer object\ncont_imputer = SimpleImputer(missing_values = np.nan, strategy = \"median\")\n\n# subset data to have only continuous features\ntrain_cont_int = train[cols_by_dtype['int64']]\ntrain_cont_float = train[cols_by_dtype['float64']]\ntest_cont_int = test[cols_by_dtype['int64']]\ntest_cont_float = test[cols_by_dtype['float64']]\n\n# impute\ntrain_cont_int.iloc[:, :] = cont_imputer.fit_transform(train_cont_int)\ntrain_cont_float.iloc[:, :] = cont_imputer.fit_transform(train_cont_float)\ntest_cont_int.iloc[:, :] = cont_imputer.fit_transform(test_cont_int)\ntest_cont_float.iloc[:, :] = cont_imputer.fit_transform(test_cont_float)","b418d868":"new_train = pd.concat([train_cat, train_cont_int, train_cont_float], axis=1)\nnew_test = pd.concat([test_cat, test_cont_int, test_cont_float], axis=1)\n\n# check\nprint(\"Train:\", new_train.shape)\nprint(\"Test:\", new_test.shape)","53730bc9":"# update train labels\nnew_train_labels = train_labels.iloc[train.index]\n\n# check\nprint(\"Train labels:\", new_train_labels.shape)","990fb2d8":"print(\"The missing values in the train dataset are:\", new_train.isnull().sum().sum())","28e3cd79":"# one hot encode your data\n\nnew_train = pd.get_dummies(new_train)\nnew_test = pd.get_dummies(new_test)\n\n# remove cols from train that are not present in test\nnew_train = new_train[new_test.columns]\n\n# check\nprint(\"Train:\", new_train.shape)\nprint(\"Test:\", new_test.shape)","2995d4cc":"# drop `encounter_id` as it's an identifier column\nnew_train = new_train.drop([\"encounter_id\"],axis=1)\nnew_test = new_test.drop([\"encounter_id\"],axis=1)\n\n# create standard scaler object with training data\nscaler = StandardScaler().fit(new_train)\n\n# apply scaler to train and test data\nnew_train = pd.DataFrame(scaler.transform(new_train))\nnew_test = pd.DataFrame(scaler.transform(new_test))\n\n# check\nprint(\"Train:\", new_train.shape)\nprint(\"Test:\", new_test.shape)","fc3e58a9":"# split data (consider only training data for now)\nX_train, X_test, y_train, y_test = train_test_split(new_train, new_train_labels, test_size=0.2, random_state=42)\n\n# make a logistic regression object; use default parameters for now\nlogreg = LogisticRegression()\n\n# fit classifier\nlogreg.fit(X_train, y_train)\n\n# make predictions\ny_pred = logreg.predict(X_test)\n\n# evaluate with a confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# print accuracy\nprint(\"Accuracy:\", int(round(logreg.score(X_test, y_test), 2)*100), '%')\n\n# credits to code below - https:\/\/realpython.com\/logistic-regression-python\/#logistic-regression-in-python-with-scikit-learn-example-1\nsns.heatmap(cm, annot=True, cbar=False, cmap='YlGnBu', fmt='d', linewidths=0.5)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Class')\nplt.ylabel('Actual Class')\nplt.show()","4f14c776":"# use training data from above\npd.DataFrame(y_train['diabetes_mellitus'].value_counts())","bffdd726":"# predict probability estimates\ny_score = logreg.predict_proba(X_test)[:, 1]  # get for positive labels only\nauroc_scores = roc_auc_score(y_test.values, y_score)\nauroc_scores","791a2cf7":"# update submission\nsol['diabetes_mellitus'] = logreg.predict_proba(new_test)[:, 1]","80421e8c":"# make submission\nsol.to_csv(\"submission.csv\", index=False)","08fca54b":"Now, the big question is how do we deal with these missing values?\n\n","d43cc122":"Now, what we need to realise is that there are also instances in this dataset that have a lot of missing values. Each instance is a patient and if we do not have a required amount of vital information about a patient, it would be very difficult to classify the patient as diabetic or not.\n\nHence, such instances in a basic sense won't be helpful. So, let's find these instances and remove them.","20b41dab":"- `icu_type` and `icu_stay_type` have no missing values\n- As a simple imputation strategy, let's impute all missing values with a constant value, \"missing\"","a6ab1966":"Now, we switch our attention to the continuous features. In order to impute continuous features, we shall impute the missing values in a feature F with the median of all non-missing values in F.","b1ec01ab":"### Missing values in `Float` features","21e4a295":"First, let's download some of the libraries that we shall be using in this notebook.","d0ea51d7":"First, let's identify how many features in the training set have missingness percentage of more than 70%. We shall remove these features from the train set.","eee86ad3":"Look a bit more carefully and make a simple calculation. It should be clear that **~78% of the samples are patients with no diabetes**. Therefore, even if we do not run a model and instead just assign all our test samples as \"not diabetic\", we still will be right 78% of the time. or our accuracy will be 78%. \n\nSo, the accuracy is a not-so-useful metric in this problem!","2d10b6c8":"The following dataframe depicts the count of each target label in our training data. Notice how \"imabalanced\" this sample is!","f017c257":"We can see that there are different types of features in the dataset. So, let's use a dictionary to store each feature under its corresponding data type.","b77f37e2":"Hello! This notebook is intended to be a beginner-level Machine Learning guide. For the purpose of the same, the [Women in Data Science 2021 Datathon dataset](https:\/\/www.kaggle.com\/c\/widsdatathon2021) is used.\n\nTo give you some context, the WiDS Datathon 2021 focuses on patient health, with an emphasis on the chronic condition of diabetes, through data from MIT\u2019s GOSSIS (Global Open Source Severity of Illness Score) initiative. \n\nThe focus of the competition can be summarized as a task where you need to build a model that can correctly classify whether a patient is diabetic or not based on the patient's vitals that are collected in the first 24 hours of intensive care. More about this can be found [here](https:\/\/www.kaggle.com\/c\/widsdatathon2021).\n\nWithout further ado, let's get cracking at this!","41ffa5ff":"### Dealing with missing values","c6d3c266":"Now, we can see that we have no more missing values. It is time to get into the next step.","2055feef":"### Categorical Encoding -> One-hot Encoding","c633313e":"## Steps in Building our Model\n\nApplied Machine Learning can essentially be divided into 4 major steps ([Source](https:\/\/machinelearningmastery.com\/what-is-data-preparation-in-machine-learning\/)) :\n\n**1. Define the Problem**  \n    - What is the type of task?\n    - What relevant data is to be collected?\n    - Explore the data\n**2. Prepare Data**  \n    - Transform the data you have into a form more suitable for modelling\n**3. Evaluate Models**  \n    - Split data into train and validation and test sets\n    - Decide performance metrics\n    - Establish a baseline\n    - Hyperparameter tuning\n**4. Finalize Model**  \n    - Select the best model\n    - Productionize it - Put it into action!\n\nIn this notebook's current iteration, all the above steps shall not be included. Instead, the focus shall be on building a very simple model that we could even consider as a [baseline](https:\/\/blog.insightdatascience.com\/always-start-with-a-stupid-model-no-exceptions-3a22314b9aaa) for all further models.","87d1dcfc":"Let's have a look at the first 5 rows of the train dataset.","049a808e":"### Handling missing values\n\nMissing values are well, missing :) \n\nMostly, these are bits of data that are not present in the final collected data because of data collection or encoding issues. They are not desirable as they skew the analysis and usually end up contributing to unsatisfactory model performance.\n\nThis [notebook](https:\/\/www.kaggle.com\/parulpandey\/a-guide-to-handling-missing-values-in-python) by Grandmaster Parul Pandey gives a comprehensive guide on dealing with missing values. We shall pick up a few of the basic techniques mentioned here in this notebook.","9e3a6c77":"- Many of the features that are of `float` datatype seem to have a great deal of missing values","6c176ea0":"As a first step, we need to recognize that our machine learning models used in this notebook can deal with only numerical data. In other words, we need to convert our categorical features into numerical forms. We call this categorical encoding. \n\nOne kind of categorical encoding is \"one-hot encoding\" and we shall use it here. ([Read more about it here](https:\/\/www.kaggle.com\/dansbecker\/using-categorical-data-with-one-hot-encoding))","1375a6af":"### Misssing values in `Object` features","c479508b":"The above confusion matrix indicates this is not a great model. But why? \n\nBecause there are a lot of false negatives i.e a lot of patients with diabetes are being classified as not having it! This, as you can imagine is truly a very dangerous output. But again, why is this happening? After all, the accuracy was 81%!","a9f16b71":"## 3. Evaluate Model\n\nIt is now time to build our first model with the pre-processed data.\n\nAs per the competition evaluation rules, the Area under the Receiver Operating Characteristics(ROC) will be used ([more](https:\/\/www.kaggle.com\/learn-forum\/53782)).","14fa926a":"First, let's have a look at our categorical features.","7f9a80f4":"Not a bad AUROC score, but we know the model is not great anyways from the confusion matrix.\n\nHowever, let's see how our \"simple\" model performs on the leaderboard by submitting it.","a0f55aba":"Well, we still do have a lot of missing values in our dataset. Therefore, we shall now switch over to the next strategy of dealing with missing values - **\"Imputation\"**. \n\nImputation simply means filling in a missing value with something else.\n\nIn our case, we have different types of features, contained in the `cols_by_dtype` dictionary. For each type of feature, we shall use a different imputation strategy.","c0921a54":"## 2. Prepare Data\n\nIn the context of this notebook's current version, data preparation shall include very basic operations such as handling missing values, categorical encoding and standardization.","031ccea2":"### Standardization\n\nNow, let's standardize our data. Standardization is required as each feature we have has a different scale. ([Read more about it here](https:\/\/machinelearningmastery.com\/standardscaler-and-minmaxscaler-transforms-in-python\/))","96f260be":"- With the above operation, we have retained 69.2% of our features in the dataset\n    - Losing out on so much data can be counterproductive, however we shall stick to this for the scope of this tutorial","5abae4ff":"## 1. Define the Problem\n\nThe Problem ->  \nGiven the information of vitals of a patient X, predict whether patient X has diabetes mellitus or not\n\nThe Task ->  \nBinary Classification\n\nThe Data ->  \nThe competition provides us with 5 tabular datasets.\n\n`TrainingWiDS2021.csv` : The training data with 130,157 instances and 180 features(including the target feature represented by `diabetes_mellitus`)  \n`UnlabeledWiDS2021.csv` : The testing data with 10,234 instances and 179 features  \n`SampleSubmissionWiDS2021.csv` : A sample submission  \n`SolutionTemplateWiDS2021.csv` : Solution template for competition  \n`DataDictionaryWiDS2021.csv` : File describing all the features in the dataset  \n\n**NOTE**\n- Each row will be referred to as an instance or observation\n- Each column will be referred to as a feature","34c6a136":"- `hospital_admit_source` has a lot of missing values","e07216a4":"Let's build a very basic model and test its performance.","005279c5":"# Learn Machine Learning with WiDS 2021","fbfccf7a":"- None of the features that are of `integer` type have any missing values","f6b89fb7":"Since the number of features in our dataset are large, we shall investigate a subset of features for missing values and repeat this procedure till we cover all features.","e1067abb":"This little trick will help us pre-process features of the same type together without having to worry about features of other types.","811d30cc":"### Missing values in `Integer` features","8f29c7eb":"Let's use the Area under the ROC metric (the one that will be evaluated in this competition) then.","ce49f951":"Now, let's concatenate the preprocessed dataframes after imputation and reform the train and test datasets."}}