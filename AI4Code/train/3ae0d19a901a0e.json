{"cell_type":{"b6daddee":"code","4f09e7dc":"code","5b046abf":"code","0a31e339":"code","e0a6d88e":"code","f73e4ad7":"code","0fb40055":"code","4b4be713":"code","b50bac15":"code","670ab2ae":"code","b5d3f1dd":"code","9d411326":"code","0624ffec":"code","a8e08807":"code","5372e547":"code","5f58f602":"code","808d0c80":"code","25847906":"code","cbd65cb9":"markdown","5fd54d33":"markdown","4fc91d4c":"markdown","a0e887c8":"markdown","a08ed4d5":"markdown","75c2ad4e":"markdown","9cb5db8e":"markdown","3bcf77e0":"markdown","b08cb4de":"markdown","0c5caf61":"markdown","3094125a":"markdown"},"source":{"b6daddee":"from fastai.conv_learner import *\nfrom fastai.dataset import *\n\nimport pandas as pd\nimport numpy as np\nimport os\nfrom sklearn.model_selection import train_test_split","4f09e7dc":"PATH = '.\/'\nTRAIN = '..\/input\/train\/'\nTEST = '..\/input\/test\/'\nSEGMENTATION = '..\/input\/train_ship_segmentations.csv'\nexclude_list = ['6384c3e78.jpg','13703f040.jpg', '14715c06d.jpg',  '33e0ff2d5.jpg',\n                '4d4e09f2a.jpg', '877691df8.jpg', '8b909bb20.jpg', 'a8d99130e.jpg', \n                'ad55c3143.jpg', 'c8260c541.jpg', 'd6c7f17c7.jpg', 'dc3e7c901.jpg',\n                'e44dffe88.jpg', 'ef87bad36.jpg', 'f083256d8.jpg'] #corrupted image","5b046abf":"nw = 4   #number of workers for data loader\narch = resnet34 #specify target architecture","0a31e339":"train_names = [f for f in os.listdir(TRAIN)]\ntest_names = [f for f in os.listdir(TEST)]\nfor el in exclude_list:\n    if(el in train_names): train_names.remove(el)\n    if(el in test_names): test_names.remove(el)\n#5% of data in the validation set is sufficient for model evaluation\ntr_n, val_n = train_test_split(train_names, test_size=0.05, random_state=42)","e0a6d88e":"class pdFilesDataset(FilesDataset):\n    def __init__(self, fnames, path, transform):\n        self.segmentation_df = pd.read_csv(SEGMENTATION).set_index('ImageId')\n        super().__init__(fnames, transform, path)\n    \n    def get_x(self, i):\n        img = open_image(os.path.join(self.path, self.fnames[i]))\n        if self.sz == 768: return img \n        else: return cv2.resize(img, (self.sz, self.sz))\n    \n    def get_y(self, i):\n        if(self.path == TEST): return 0\n        masks = self.segmentation_df.loc[self.fnames[i]]['EncodedPixels']\n        if(type(masks) == float): return 0 #NAN - no ship \n        else: return 1\n    \n    def get_c(self): return 2 #number of classes","f73e4ad7":"def get_data(sz,bs):\n    #data augmentation\n    aug_tfms = [RandomRotate(20, tfm_y=TfmType.NO),\n                RandomDihedral(tfm_y=TfmType.NO),\n                RandomLighting(0.05, 0.05, tfm_y=TfmType.NO)]\n    tfms = tfms_from_model(arch, sz, crop_type=CropType.NO, tfm_y=TfmType.NO, \n                aug_tfms=aug_tfms)\n    ds = ImageData.get_ds(pdFilesDataset, (tr_n[:-(len(tr_n)%bs)],TRAIN), \n                (val_n,TRAIN), tfms, test=(test_names,TEST))\n    md = ImageData(PATH, ds, bs, num_workers=nw, classes=None)\n    md.is_multi = False\n    return md","0fb40055":"sz = 256 #image size\nbs = 64  #batch size\n\nmd = get_data(sz,bs)\nlearn = ConvLearner.pretrained(arch, md, ps=0.5) #dropout 50%\nlearn.opt_fn = optim.Adam","4b4be713":"learn.lr_find()\nlearn.sched.plot()","b50bac15":"learn.fit(2e-3, 1)","670ab2ae":"learn.unfreeze()\nlr=np.array([1e-4,5e-4,2e-3])","b5d3f1dd":"learn.fit(lr, 1, cycle_len=2, use_clr=(20,8))","9d411326":"learn.sched.plot_lr()","0624ffec":"learn.save('Resnet34_lable_256_1')","a8e08807":"log_preds,y = learn.predict_with_targs(is_test=True)\nprobs = np.exp(log_preds)[:,1]\npred = (probs > 0.5).astype(int)","5372e547":"df = pd.DataFrame({'id':test_names, 'p_ship':probs})\ndf.to_csv('ship_detection.csv', header=True, index=False)","5f58f602":"sz = 384 #image size\nbs = 32  #batch size\n\nmd = get_data(sz,bs)\nlearn = ConvLearner.pretrained(arch, md, ps=0.5) #dropout 50%\nlearn.opt_fn = optim.Adam\nlearn.unfreeze()\nlr=np.array([1e-4,5e-4,2e-3])","808d0c80":"learn.load('Resnet34_lable_256_1')","25847906":"#learn.fit(lr\/2, 1, cycle_len=2, use_clr=(20,8)) #lr is smaller since bs is only 32\n#learn.save('Resnet34_lable_384_1')","cbd65cb9":"### Data","5fd54d33":"I begin with finding the optimal learning rate. The following function runs training with different lr and records the loss. Increase of the loss indicates onset of divergence of training. The optimal lr lies in the vicinity of the minimum of the curve but before the onset of divergence. Based on the following plot, for the current setup the divergence starts at ~0.01, and the recommended learning rate is ~0.002.","4fc91d4c":"### Prediction","a0e887c8":"### Model","a08ed4d5":"### Training on high resolution images","75c2ad4e":"## Overview\nResnet34 is commonly used as an encoder for U-net and SSD, boosting the model performance and training time since you do not need to train the model from scratch. However, in particular cases it makes sense to do fine-tuning of Resnet34 model before using it as a decoder for object localization or image segmentation. In this competition the size of ship masks is much smaller than the size of images that leads to quite unbalanced training with ~1 positive pixel per 1000 negative ones. If images with no ships are used, instead of ~1:1000 you will end up with ~1:10000 unbalance, which is quite tough. Moreover, the training time is ~4 times longer since you need to process more images in each epoch. So, it is reasonable to drop empty images and focus only on ones with ships. Meanwhile, since the current dataset is quite different from ImageNet, the empty images are quite helpful in fine-tuning your encoder on a pseudo task - ship detection. Moreover, when the training of your U-net or SSD model is completed, you can run the model on images without ships, add false positives (~4000 in my case) as negative example to you training set, and train the model for several additional epochs. Finally, a good model focused on a single task, ship detection, can boost the final score when you stack up it with U-net or SSD. If you predict a ship for an empty image you will get automatically zero score for it, and since PLB has ~85% of empty images, prediction of empty images is quite important.\n\nIn this notebook I want to share how to pretrain Resnet34 (or higher end models) on a ship detection task. After training of the head layers of the model on 256x256 rescaled images for one epoch the accuracy has reached 93.7%. The following fine-tuning of entire model for 2 more epochs with learning rate annealing boosted the accuracy to ~97%. If the training is continued for several epochs with a new data set composed of images of 384x384 resolution, the accuracy could be boosted to ~98%. Unfortunately, continuing training the model on full resolution, 768x768, images leaded to reduction of the accuracy that is likely attributed to insufficient model capacity.","9cb5db8e":"Training the head part of the model with constant learning rate for one epoch. ","3bcf77e0":"Unfreeze the model and train it with differential learning rate. The lr of the head part is still 2e-3, while the middle layers of the model a trained with 5e-4 lr, and the base is trained with even smaller lr, 1e-4, since low level detector do not vary much from one image data set to another.","b08cb4de":"Since each epoch on higher resolution images, like 384x384 or 768x768, takes quite long time, training the model from scratch on these images is quite inefficient. Fortunately, modern convolutional nets support input images of arbitrary resolution. To decrease the training time, one can start training the model on low resolution images first and continue training on higher resolution images for only a few epochs. In addition, a model pretrained on low resolution images first generalizes better since a pixel information is less available and high order features are tended to be used.","0c5caf61":"The training has been run with learning rate annealing. Periodic lr increase followed by slow decrease drives the system out of steep minima (when lr is high) towards broader ones (which are explored when lr decreases) that enhances the ability of the model to generalize and reduces overfitting. Due to time limit, only once cycle has been run. But ideally several cycles must be run with gradual increase of the image size, etc. 256x256, 384x384, 768x768, to reach better performance of the model.","3094125a":"Due to the kernal run time limit, the following code does not have enough time to be executed. I hope it is possible to rerun this kernel with disabled cells from \"Model\" part and included data from the previous commit to complete training of higher resolution images. Each epoch on images 383x384 takes about 1.5 hours. Probably, some paths, like \"learn.models_path\", may need to be changed."}}