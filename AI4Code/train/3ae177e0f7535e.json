{"cell_type":{"e55451fa":"code","c5f124e5":"code","46936c7c":"code","4a4fd538":"code","75df6c07":"code","207579ed":"code","04602d10":"code","79293a2b":"code","5f98b004":"code","aa62195b":"code","deda56ec":"code","d960ecfb":"code","5c8d620b":"code","0bf72da8":"code","81d660ad":"code","5312e6c8":"code","c07801ce":"code","19047403":"code","c913d165":"code","a003efce":"code","3a9b8b37":"code","8473dda8":"markdown","8ee2bb29":"markdown","e34a5c64":"markdown","938c492d":"markdown","2e5bb4be":"markdown","bec0a140":"markdown","981ca2c2":"markdown","37ff5846":"markdown","5b630790":"markdown","265c602b":"markdown","a2363996":"markdown"},"source":{"e55451fa":"debug = False","c5f124e5":"import gc\nimport itertools\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport random\n\n#the basics\nimport pandas as pd, numpy as np, seaborn as sns\nimport math, json\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\nfrom sklearn.cluster import KMeans\n\n#for model evaluation\nfrom sklearn.model_selection import train_test_split, KFold, GroupKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ExponentialLR, CosineAnnealingLR\nfrom torch.utils.data import DataLoader, TensorDataset\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nSEED = 2020\n\n\ndef seed_everything(seed=2020):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n\n\nseed_everything(SEED)","46936c7c":"class RMSELoss(nn.Module):\n    def __init__(self, eps=1e-6):\n        super().__init__()\n        self.mse = nn.MSELoss()\n        self.eps = eps\n\n    def forward(self, yhat, y):\n        loss = torch.sqrt(self.mse(yhat, y) + self.eps)\n        return loss\n\n\nclass MCRMSELoss(nn.Module):\n    def __init__(self, num_scored=3):\n        super().__init__()\n        self.rmse = RMSELoss()\n        self.num_scored = num_scored\n\n    def forward(self, yhat, y):\n        score = 0\n        for i in range(self.num_scored):\n            score += self.rmse(yhat[:, :, i], y[:, :, i]) \/ self.num_scored\n\n        return score","4a4fd538":"import pandas as pd\n\n\ndef load_json(path):\n    return pd.read_json(path, lines=True)\n\ndf = load_json('\/kaggle\/input\/stanford-covid-vaccine\/train.json')\ndf_test = load_json('\/kaggle\/input\/stanford-covid-vaccine\/test.json')\nsample_sub = pd.read_csv('\/kaggle\/input\/stanford-covid-vaccine\/sample_submission.csv')\n\nif debug:\n    df = df[:200]\n    df_test = df_test[:200]\ndf = df[df.SN_filter == 1]","75df6c07":"print(set(df[\"sequence\"].sum()))\nprint(set(df[\"structure\"].sum()))\nprint(set(df[\"predicted_loop_type\"].sum()))\n# {'G', 'A', 'C', 'U'}\n# {')', '(', '.'}\n# {'X', 'E', 'M', 'B', 'H', 'S', 'I'}\nsequence_and_structure = [i + j for i in \"GACU\" for j in \"().\"]\nsequence_and_predicted_loop_type = [i + j for i in \"GACU\" for j in \"XEMBHSI\"]\nstructure_and_predicted_loop_type = [i + j for i in \"().\" for j in \"XEMBHSI\"]\n","207579ed":"def merge_seq_seq(seq):\n    half = len(seq)\/\/2\n    new_seq = []\n    for i in range(len(seq)\/\/2):\n        new_seq.append(seq[i] + seq[i+half])\n    return new_seq\n","04602d10":"create_feture = True\npreprocess_cols=[\"sequence\", \"structure\", \"predicted_loop_type\"]\nif create_feture:\n    df[\"sequence_and_structure\"] = (df[\"sequence\"] + df[\"structure\"]).apply(merge_seq_seq)\n    #df[\"sequence_and_predicted_loop_type\"] = (df[\"sequence\"] + df[\"predicted_loop_type\"]).apply(merge_seq_seq)\n    #df[\"structure_and_predicted_loop_type\"] = (df[\"structure\"] + df[\"predicted_loop_type\"]).apply(merge_seq_seq)\n    df_test[\"sequence_and_structure\"] = (df_test[\"sequence\"] + df_test[\"structure\"]).apply(merge_seq_seq)\n    #df_test[\"sequence_and_predicted_loop_type\"] = (df_test[\"sequence\"] + df_test[\"predicted_loop_type\"]).apply(merge_seq_seq)\n    #df_test[\"structure_and_predicted_loop_type\"] = (df_test[\"structure\"] + df_test[\"predicted_loop_type\"]).apply(merge_seq_seq)\n    preprocess_cols=[\"sequence\", \"structure\", \"predicted_loop_type\", \"sequence_and_structure\"]#, \"sequence_and_predicted_loop_type\", \"structure_and_predicted_loop_type\"]\n    def read_bpps_sum(df):\n        bpps_arr = []\n        for mol_id in df.id.to_list():\n            bpps_arr.append(np.load(f\"..\/input\/stanford-covid-vaccine\/bpps\/{mol_id}.npy\").sum(axis=1))\n        return bpps_arr\n\n    def read_bpps_max(df):\n        bpps_arr = []\n        for mol_id in df.id.to_list():\n            bpps_arr.append(np.load(f\"..\/input\/stanford-covid-vaccine\/bpps\/{mol_id}.npy\").max(axis=1))\n        return bpps_arr\n\n    def read_bpps_nb(df):\n        #mean and std from https:\/\/www.kaggle.com\/symyksr\/openvaccine-deepergcn \n        bpps_nb_mean = 0.077522\n        bpps_nb_std = 0.08914\n        bpps_arr = []\n        for mol_id in df.id.to_list():\n            bpps = np.load(f\"..\/input\/stanford-covid-vaccine\/bpps\/{mol_id}.npy\")\n            bpps_nb = (bpps > 0).sum(axis=0) \/ bpps.shape[0]\n            bpps_nb = (bpps_nb - bpps_nb_mean) \/ bpps_nb_std\n            bpps_arr.append(bpps_nb)\n        return bpps_arr \n    df['bpps_sum'] = read_bpps_sum(df)\n    df_test['bpps_sum'] = read_bpps_sum(df_test)\n    df['bpps_max'] = read_bpps_max(df)\n    df_test['bpps_max'] = read_bpps_max(df_test)\n    df['bpps_nb'] = read_bpps_nb(df)\n    df_test['bpps_nb'] = read_bpps_nb(df_test)\n","79293a2b":"target_cols = [\"reactivity\", \"deg_Mg_pH10\", \"deg_Mg_50C\"]\n\ntokens = [i for i in \"().ACGUBEHIMSX\"] + sequence_and_structure + sequence_and_predicted_loop_type + structure_and_predicted_loop_type\ntoken2int = {x:i for i, x in enumerate(tokens)}\n\n\ndef preprocess_inputs(df, cols):\n    base_fea = np.transpose(\n        np.array(\n            df[cols].applymap(lambda seq: [token2int[x] for x in seq]).values.tolist()\n        ), (0, 2, 1)\n    )\n    if create_feture:\n        bpps_sum_fea = np.array(df['bpps_sum'].to_list())[:,:,np.newaxis]\n        bpps_max_fea = np.array(df['bpps_max'].to_list())[:,:,np.newaxis]\n        bpps_nb_fea = np.array(df['bpps_nb'].to_list())[:,:,np.newaxis]\n        return np.concatenate([base_fea,bpps_sum_fea,bpps_max_fea,bpps_nb_fea], 2)\n    else:\n        return base_fea\n\ntrain_inputs = torch.tensor(preprocess_inputs(df, preprocess_cols)).to(device)\nprint(\"input shape: \", train_inputs.shape)\ntrain_labels = torch.tensor(\n    np.array(df[target_cols].values.tolist()).transpose(0, 2, 1)\n).float().to(device)","5f98b004":"df.head(3)","aa62195b":"models = [\"LSTM\", \"LSTM_short\", \"GRU\", \"blend1\", \"blend2\", \"blend3\", \"blend4\"]\nmodels = [\n    {\"model\": \"LSTM\", \"dropout\": 0.4, \"embed_dim\": 100, \"hidden_dim\": 128, \"hidden_layers\": 3},\n    {\"model\": \"LSTM\", \"dropout\": 0.4, \"embed_dim\": 100, \"hidden_dim\": 256, \"hidden_layers\": 3}\n]\n","deda56ec":"class Wave_Block(nn.Module):\n\n    def __init__(self, in_channels, out_channels, dilation_rates, kernel_size):\n        super(Wave_Block, self).__init__()\n        self.num_rates = dilation_rates\n        self.convs = nn.ModuleList()\n        self.filter_convs = nn.ModuleList()\n        self.gate_convs = nn.ModuleList()\n\n        self.convs.append(nn.Conv1d(in_channels, out_channels, kernel_size=1))\n        dilation_rates = [2 ** i for i in range(dilation_rates)]\n        for dilation_rate in dilation_rates:\n            self.filter_convs.append(\n                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=int((dilation_rate*(kernel_size-1))\/2), dilation=dilation_rate))\n            self.gate_convs.append(\n                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=int((dilation_rate*(kernel_size-1))\/2), dilation=dilation_rate))\n            self.convs.append(nn.Conv1d(out_channels, out_channels, kernel_size=1))\n\n    def forward(self, x):\n        x = self.convs[0](x)\n        res = x\n        for i in range(self.num_rates):\n            x = torch.tanh(self.filter_convs[i](x)) * torch.sigmoid(self.gate_convs[i](x))\n            x = self.convs[i + 1](x)\n            res = res + torch.nn.functional.pad(x, (0, 1, 0, 0))\n        return res\n\n\nclass MyModel(nn.Module):\n    def __init__(\n        self, seq_len=107, pred_len=68, dropout=0.4, embed_dim=100, hidden_dim=64, hidden_layers=3, main_layer=\"LSTM\"\n        ):\n        super(MyModel, self).__init__()\n        self.pred_len = pred_len\n        self.main_layer = main_layer\n        if main_layer == \"GRU\":\n            hidden_dim \/\/= 3\n        \n        self.embeding = nn.Embedding(num_embeddings=len(token2int), embedding_dim=embed_dim)\n        self.lstm_layer = nn.LSTM(\n            input_size=embed_dim * len(preprocess_cols)+ 3,\n            hidden_size=hidden_dim,\n            num_layers=hidden_layers,\n            dropout=dropout,\n            bidirectional=True,\n            batch_first=True\n        )\n        self.gru_layer = nn.GRU(\n            input_size=embed_dim * len(preprocess_cols) + 3,\n            hidden_size=hidden_dim,\n            num_layers=hidden_layers,\n            dropout=dropout,\n            bidirectional=True,\n            batch_first=True\n        )\n        self.hidden_lstm_layer = nn.LSTM(\n            input_size=hidden_dim*2,\n            hidden_size=hidden_dim,\n            num_layers=1,\n            dropout=dropout,\n            bidirectional=True,\n            batch_first=True\n        )\n        self.hidden_gru_layer = nn.GRU(\n            input_size=hidden_dim*2,\n            hidden_size=hidden_dim,\n            num_layers=1,\n            dropout=dropout,\n            bidirectional=True,\n            batch_first=True\n        )\n        self.wave_block = Wave_Block(hidden_dim*2, hidden_dim*2, 12, hidden_dim)\n        self.linear1 = nn.Linear(hidden_dim * 2, hidden_dim)\n        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n        self.linear3 = nn.Linear(hidden_dim, len(target_cols))\n\n    def forward(self, seqs):\n        if create_feture:\n            categorical_feats = seqs[:, :, :len(preprocess_cols)].long()\n            numerical_feats = seqs[:, :, len(preprocess_cols):].float()\n            \n            embed = self.embeding(categorical_feats)\n            reshaped = torch.reshape(embed, (-1, embed.shape[1], embed.shape[2] * embed.shape[3]))\n            reshaped = torch.cat([reshaped, numerical_feats], axis=2)\n        else:\n            embed = self.embeding(seqs)\n            reshaped = torch.reshape(embed, (-1, embed.shape[1], embed.shape[2] * embed.shape[3]))\n            reshaped = torch.cat([reshaped, numerical_feats], axis=2)\n        if self.main_layer == \"LSTM\":\n            output, hidden = self.lstm_layer(reshaped)\n        elif self.main_layer == \"GRU\":\n            output, hidden = self.gru_layer(reshaped)\n        elif self.main_layer == \"blend1\":\n            output, hidden = self.lstm_layer(reshaped)\n            output, hidden = self.hidden_lstm_layer(output)\n        elif self.main_layer == \"blend2\":\n            output, hidden = self.gru_layer(reshaped)\n            output, hidden = self.hidden_gru_layer(output)\n        elif self.main_layer == \"blend3\":\n            output, hidden = self.gru_layer(reshaped)\n            output, hidden = self.hidden_gru_layer(output)\n        elif self.main_layer == \"blend4\":\n            output, hidden = self.lstm_layer(reshaped)\n            output, hidden = self.hidden_lstm_layer(output)\n        elif self.main_layer == \"WaveNet\":\n            output, hidden = self.lstm_layer(reshaped)\n            output = output.permute(0, 2, 1)\n            output = self.wave_block(output)\n            output = output.permute(0, 2, 1)\n        truncated = output[:, : self.pred_len, :]\n        truncated = self.linear1(truncated)\n        truncated = self.linear2(truncated)\n        truncated = self.linear2(truncated)\n        out = self.linear3(truncated)\n        return out\n\ncriterion = MCRMSELoss(len(target_cols))\n\ndef compute_loss(batch_X, batch_Y, model, optimizer=None, is_train=True, scheduler=None):\n    model.train(is_train)\n    pred_Y = model(batch_X)\n    loss = criterion(pred_Y, batch_Y)\n    if is_train:\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        if scheduler:\n            scheduler.step()\n        \n    return loss.item(), pred_Y\n","d960ecfb":"FOLDS = 4\nEPOCHS = 100\nif debug:\n    EPOCHS = 2\nBATCH_SIZE = 64\nVERBOSE = 2\nLR = 0.016","5c8d620b":"public_df = df_test.query(\"seq_length == 107\").copy()\nprivate_df = df_test.query(\"seq_length == 130\").copy()\n\npublic_inputs = torch.tensor(preprocess_inputs(public_df, preprocess_cols)).to(device)\nprivate_inputs = torch.tensor(preprocess_inputs(private_df, preprocess_cols)).to(device)\n\npublic_loader = DataLoader(TensorDataset(public_inputs), shuffle=False, batch_size=BATCH_SIZE)\nprivate_loader = DataLoader(TensorDataset(private_inputs), shuffle=False, batch_size=BATCH_SIZE)\n","0bf72da8":"if debug:\n    kmeans_model = KMeans(n_clusters=50, random_state=110).fit(preprocess_inputs(df, preprocess_cols)[:,:,0])\nelse:\n    kmeans_model = KMeans(n_clusters=200, random_state=110).fit(preprocess_inputs(df, preprocess_cols)[:,:,0])\nkmeans_labels = kmeans_model.labels_","81d660ad":"model_histories = {str(model_id): [] for model_id in models}\nmodel_oof_preds = {str(model_id): np.zeros((df.shape[0], 68, len(target_cols))) for model_id in models}\nmodel_private_preds = {str(model_id): np.zeros((private_df.shape[0], 130, len(target_cols))) for model_id in models}\nmodel_public_preds = {str(model_id): np.zeros((public_df.shape[0], 107, len(target_cols))) for model_id in models}\n\ncriterion = MCRMSELoss()\ngkf = GroupKFold(FOLDS)\n\n\nfor model_dict in models:\n    model_id = str(model_dict)\n    model_name = model_dict[\"model\"]\n    dropout = model_dict[\"dropout\"]\n    embed_dim = model_dict[\"embed_dim\"]\n    hidden_layers = model_dict[\"hidden_layers\"]\n    \n    model_oof_pred = []\n    oof_idxes = []\n    for k, (train_index, val_index) in enumerate(gkf.split(train_inputs, df[\"reactivity\"], kmeans_labels)):\n        oof_idxes.append(val_index)\n        train_dataset = TensorDataset(train_inputs[train_index], train_labels[train_index])\n        val_dataset = TensorDataset(train_inputs[val_index], train_labels[val_index])\n\n        train_loader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n        val_loader = DataLoader(val_dataset, shuffle=True, batch_size=BATCH_SIZE)\n\n        model = MyModel(dropout=dropout, embed_dim=embed_dim, hidden_layers=hidden_layers, main_layer=model_name).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=LR)\n        scheduler = None\n        scheduler = CosineAnnealingLR(optimizer, T_max=20, eta_min=0.008)\n        # scheduler = ExponentialLR(optimizer, gamma=0.95)\n\n        train_losses = []\n        val_losses = []\n        for epoch in tqdm(range(EPOCHS)):\n            train_losses_batch = []\n            val_losses_batch = []\n            for (batch_X, batch_Y) in train_loader:\n                train_loss, _ = compute_loss(batch_X, batch_Y, model, optimizer=optimizer, is_train=True, scheduler=scheduler)\n                train_losses_batch.append(train_loss)\n            for (batch_X, batch_Y) in val_loader:\n                model.eval()\n                val_loss, val_pred = compute_loss(batch_X, batch_Y, model, optimizer=optimizer, is_train=False)\n                val_losses_batch.append(val_loss)\n            train_losses.append(np.mean(train_losses_batch))    \n            val_losses.append(np.mean(val_losses_batch))\n        model_state = model.state_dict()\n        torch.save(model_state, f\"{model_id}_fold{k}.pth\")\n        \n        del model\n\n        model_histories[model_id].append({\"train_loss\": train_losses, \"val_loss\": val_losses})\n\n        model_short = MyModel(seq_len=107, pred_len=107, dropout=dropout, embed_dim=embed_dim, hidden_layers=hidden_layers, main_layer=model_name).to(device)\n        model_short.load_state_dict(model_state)\n        model_short.eval()\n        model_public_pred = np.ndarray((0, 107, len(target_cols)))\n        for batch in public_loader:\n            batch_X = batch[0]\n            pred = model_short(batch_X).detach().cpu().numpy()\n            model_public_pred = np.concatenate([model_public_pred, pred], axis=0)\n        model_public_preds[model_id] += model_public_pred \/ FOLDS\n\n        model_long = MyModel(seq_len=130, pred_len=130, dropout=dropout, embed_dim=embed_dim, hidden_layers=hidden_layers, main_layer=model_name).to(device)\n        model_long.load_state_dict(model_state)\n        model_long.eval()\n        model_private_pred = np.ndarray((0, 130, len(target_cols)))\n        for batch in private_loader:\n            batch_X = batch[0]\n            pred = model_long(batch_X).detach().cpu().numpy()\n            model_private_pred = np.concatenate([model_private_pred, pred], axis=0)\n        model_private_preds[model_id] += model_private_pred \/ FOLDS\n        \n        oof_loader = DataLoader(val_dataset, shuffle=False, batch_size=BATCH_SIZE)\n        model_oof = MyModel(seq_len=107, pred_len=68, dropout=dropout, embed_dim=embed_dim, hidden_layers=hidden_layers, main_layer=model_name).to(device)\n        model_oof.load_state_dict(model_state)\n        model_oof.eval()\n        oof_pred = np.ndarray((0, 68, len(target_cols)))\n        for batch in oof_loader:\n            batch_X = batch[0]\n            pred = model_oof(batch_X).detach().cpu().numpy()\n            oof_pred = np.concatenate([oof_pred, pred], axis=0)\n        model_oof_pred.append(oof_pred)\n\n        del model_short, model_long\n        gc.collect()\n    oof_idxes = np.concatenate(oof_idxes)\n    order = np.argsort(oof_idxes)\n    model_oof_preds[model_id] = np.concatenate(model_oof_pred)[order]\n","5312e6c8":"if True:\n    ensemble_losses = []\n    alphas = []\n    with torch.no_grad():\n        for alpha in np.linspace(0, 1, 100):\n            alphas.append(alpha)\n            loss = criterion(torch.tensor(model_oof_preds[str(models[0])]*alpha + model_oof_preds[str(models[1])]*(1-alpha)), torch.tensor(train_labels.detach().cpu().numpy()))\n            ensemble_losses.append(float(loss))\n    plt.plot(ensemble_losses)\n    best_alpha = alphas[np.argmin(ensemble_losses)]\n    best_loss = ensemble_losses[np.argmin(ensemble_losses)]\n    print(f\"best_alpha is {best_alpha}, best loss is {best_loss} ,lr :{LR}\")","c07801ce":"\nfig, ax = plt.subplots(1, 1, figsize = (20, 10))\n\nlegend = []\n\nfor model_dict in models:\n    model_id = str(model_dict)\n    train_loss = np.zeros(EPOCHS)\n    val_loss = np.zeros(EPOCHS)\n    for history in model_histories[model_id]:\n        train_loss = np.array(history['train_loss']) \/ FOLDS\n        val_loss = np.array(history['val_loss']) \/ FOLDS\n    ax.plot(train_loss)\n    ax.plot(val_loss)\n\n    legend.append(f'{model_id}_train')\n    legend.append(f'{model_id}_validation')\n\n    ax.set_ylabel('Loss')\n    ax.set_xlabel('Epoch')\n    print(f\"{history['val_loss'][-1]} : valid last {model_id}  ,lr :{LR}\")\n\nax.legend(legend)","19047403":"public_df = df_test.query(\"seq_length == 107\").copy()\nprivate_df = df_test.query(\"seq_length == 130\").copy()\n\npublic_inputs = preprocess_inputs(public_df, preprocess_cols)\nprivate_inputs = preprocess_inputs(private_df, preprocess_cols)","c913d165":"preds_model = {str(model_id): [] for model_id in models}\nsubmissions = {}\n\nfor model_dict in models:\n    model_id = str(model_dict)\n    for df, preds in [(public_df, model_public_preds[model_id]), (private_df, model_private_preds[model_id])]:\n        for i, uid in enumerate(df.id):\n            single_pred = preds[i]\n\n            single_df = pd.DataFrame(single_pred, columns=target_cols)\n            single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n            preds_model[model_id].append(single_df)\n\n    preds_df = pd.concat(preds_model[model_id])\n    print(preds_df.head())\n    submission = sample_sub[['id_seqpos']].merge(preds_df, on=['id_seqpos'])\n    submission['deg_pH10'] = 0\n    submission['deg_50C'] = 0\n    print(submission.head())\n    submission.to_csv(f'submission_{model_id}.csv', index=False)\n    submissions[model_id] = submission\n    print('Submission saved')\n","a003efce":"if True:\n    id_seqpos = submissions[str(models[0])][\"id_seqpos\"]\n    ensemble_submission = submissions[str(models[0])].drop(\"id_seqpos\",axis=1)*best_alpha + submissions[str(models[1])].drop(\"id_seqpos\",axis=1)*(1-best_alpha)\n    ensemble_submission[\"id_seqpos\"] = id_seqpos\n    ensemble_submission.to_csv(f'submission_lstm_gru_ensemble.csv', index=False)","3a9b8b37":"if not debug:\n    !curl -X POST -H 'Content-type: application\/json' --data '{\"text\":\"commit done! \"}' <your_webhook_url>","8473dda8":"### KFold Training and Inference","8ee2bb29":"```python\ncriterion = MCRMSELoss()\npredictions = model(data)\nloss = criterion(predictions, targets)\n```","e34a5c64":"## usage","938c492d":"### Submission","2e5bb4be":"## MCRMSELoss","bec0a140":"### preprocess","981ca2c2":"this notebook based https:\/\/www.kaggle.com\/masashisode\/pytorch-implementation-of-mcrmseloss\nspecial thanks for https:\/\/www.kaggle.com\/masashisode","37ff5846":"### Dataload","5b630790":"if you use `SN_filter`, you can get the LB-like score. \nLet's see how it works.","265c602b":"## Model","a2363996":"## ensemble submission with two LSTM"}}