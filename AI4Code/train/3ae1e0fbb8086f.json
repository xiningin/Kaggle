{"cell_type":{"da81e05f":"code","5c55cf37":"code","ed92c2a0":"code","c8843ad9":"code","5dc8939f":"code","741fb842":"code","dea1f414":"code","7c3a59f2":"code","ebf7eddd":"code","e0dc7a69":"code","dd3ae511":"code","9ebf6ac1":"code","8089c9a4":"code","72dccb30":"code","4962ef40":"markdown","07d8581d":"markdown","27dedb7c":"markdown","0a0992f1":"markdown","7c41eecf":"markdown","5e4c8248":"markdown","d9e5513d":"markdown"},"source":{"da81e05f":"import numpy as np \nimport pandas as pd \nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport optuna\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ndf_train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv')\ndf_sample_submission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')\n\nprint('Dataframes Created')","5c55cf37":"df_train.shape, df_test.shape","ed92c2a0":"from scipy import stats\nfrom scipy.stats import norm\n\nf,ax = plt.subplots(nrows=1,ncols=2,figsize=(16,8))\nsns.histplot(x=df_train.loss,kde=True,ax=ax[0])\nres=stats.probplot(df_train['loss'],plot=plt)\n","c8843ad9":"# cols = 4\n# rows = int(len(features)\/(cols+1))\n# f,ax = plt.subplots(nrows=rows,ncols=cols,figsize=(80,160),sharex=False)\n# plt.subplots_adjust(hspace = 0.3)\n\n# i=0\n# for r in range(0,rows,1):\n#     for c in range(0,cols,1):\n#         if i>=len(features):\n#             ax[r, c].set_visible(False)\n#         else:\n#             scatter = ax[r, c].scatter(df_train[features[i]].values,\n#                                         df_train[\"loss\"],\n#                                         )\n#             ax[r, c].set_title(features[i], fontsize=14, pad=5)\n#             ax[r, c].tick_params(axis=\"y\", labelsize=11)\n#             ax[r, c].tick_params(axis=\"x\", labelsize=11)\n                                  \n#         i+=1\n# plt.show()","5dc8939f":"features = [c for c in df_train.columns if c not in ('loss','kfold','id')]\nhist_features = df_train[features].hist(figsize = (130, 160), bins=50, grid = False, xlabelsize=8, ylabelsize=8, layout = (101,4))","741fb842":"# Kfold (fold=5)\ndf_train['kfold'] = -1\n            \nkf = KFold(n_splits=5,shuffle=True,random_state=42)\nfor fold, (idx_train,idx_valid) in enumerate(kf.split(df_train)):\n    df_train.loc[idx_valid,'kfold'] = fold\n\n# Selecting features\nfeatures = [c for c in df_train.columns if c not in ('loss','kfold','id')]\ndf_test = df_test[features]\n\ndf_train.head(3)","dea1f414":"def hp_optim(trial):\n    preds = []\n    scores = []\n    X_train = df_train[df_train.kfold!=fold].reset_index(drop=True)\n    y_train = X_train.loss\n\n    X_valid = df_train[df_train.kfold==fold].reset_index(drop=True)\n    y_valid = X_valid.loss\n\n    X_test = df_test.copy()\n\n    X_train = X_train[features]\n    X_valid = X_valid[features]\n\n    params = { \n                    'n_estimators': trial.suggest_int('n_estimators',400,10000,400),\n                    \"random_state\": 42,\n                    \"tree_method\": 'gpu_hist',\n                    \"predictor\": \"gpu_predictor\",\n                    \"objective\": \"reg:squarederror\",\n                    \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.005, 0.05),\n                    \"colsample_bytree\": trial.suggest_loguniform(\"colsample_bytree\", 0.2, 0.6),\n                    \"subsample\": trial.suggest_loguniform(\"subsample\", 0.4, 0.8),\n                    \"alpha\": trial.suggest_loguniform(\"alpha\", 0.01, 10.0),\n                    \"lambda\": trial.suggest_loguniform(\"lambda\", 1e-8, 10.0),\n                    \"gamma\": trial.suggest_loguniform(\"lambda\", 1e-8, 10.0),\n                    \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 10, 1000),\n                    \"max_depth\": trial.suggest_int(\"max_depth\", 4, 12)\n                }\n\n    model = XGBRegressor(**params)\n    model.fit(X_train,y_train, early_stopping_rounds=300, eval_set=[(X_valid, y_valid)], verbose=1000)\n\n    ypred_valid = model.predict(X_valid)\n    ypred_test = model.predict(X_test)\n\n    preds.append(ypred_test)\n    rmse = mean_squared_error(y_valid,ypred_valid,squared=False)\n    scores.append(rmse)\n        \n    return np.mean(scores)\n\nprint('created')","7c3a59f2":"study = optuna.create_study(direction=\"minimize\")\nstudy.optimize(hp_optim, n_trials=5)","ebf7eddd":"print('Number of finished trials:', len(study.trials))\nprint(\"\\nBest params: \",study.best_params)","e0dc7a69":"optuna.visualization.plot_optimization_history(study)","dd3ae511":"# See which hyperparameters are more important to tweak suggested ranges \noptuna.visualization.plot_param_importances(study)","9ebf6ac1":"preds = []\n\nfor f in range(5):\n    X_train = df_train[df_train.kfold!=fold].reset_index(drop=True)\n    y_train = X_train.loss\n\n    X_valid = df_train[df_train.kfold==fold].reset_index(drop=True)\n    y_valid = X_valid.loss\n\n    X_test = df_test.copy()\n\n    X_train = X_train[features]\n    X_valid = X_valid[features]\n    \n    params={'n_estimators': 3600, 'learning_rate': 0.018352422312250018, 'colsample_bytree': 0.30280947434330335, 'subsample': 0.65778738291577, 'alpha': 0.016255685356625964, 'lambda': 5.112552884270486e-08, 'min_child_weight': 68.33661984654934, 'max_depth': 5}\n\n    model = XGBRegressor(**params,random_state= 42)\n    model.fit(X_train,y_train)\n\n    ypred_valid = model.predict(X_valid)\n    ypred_test = model.predict(X_test)\n\n    preds.append(ypred_test)\n        \n    print(fold, mean_squared_error(y_valid, ypred_valid, squared=False))\n\npreds = np.mean(np.column_stack(preds), axis=1)","8089c9a4":"df_sample_submission.head()","72dccb30":"df_sample_submission.loss = preds\ndf_sample_submission.to_csv(\"submission_TPS_1.csv\", index=False)","4962ef40":"# TPS (August-2021)","07d8581d":"## Importing Libraries and loading data in Pandas Dataframe","27dedb7c":"## ML Model","0a0992f1":"### Final Model","7c41eecf":"## EDA","5e4c8248":"Looks like Tweedie distribution","d9e5513d":"### Hyperparameter Optimization using Optim"}}