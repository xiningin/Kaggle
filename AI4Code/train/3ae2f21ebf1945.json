{"cell_type":{"6fd5d61d":"code","e30dc4c8":"code","b1a4dc31":"code","d353d5d9":"code","7108a374":"code","b7264fea":"code","8e584bc0":"code","07f18fca":"code","9804ff72":"code","535785bf":"code","fd59328f":"code","4f2d2f95":"code","9473fabc":"code","99d22fb8":"code","954b4f32":"code","5422b29e":"code","f0b7eae3":"code","eb134e9a":"code","752cc3ff":"code","808d2aa1":"code","60778d78":"code","5ea89f85":"code","084a2485":"code","b161f866":"code","ffe8bdb0":"code","c0910591":"code","ed149568":"code","3f6ec904":"code","c2e09c60":"markdown","b4c9fae4":"markdown","ea680b4a":"markdown","cff8f112":"markdown","5338613c":"markdown","354dc1d5":"markdown","88b6bd17":"markdown","c72ff1d9":"markdown"},"source":{"6fd5d61d":"# import libraries\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.svm import SVC\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_predict,cross_val_score, KFold,StratifiedKFold","e30dc4c8":"# load data\ntrain_data = pd.read_csv(\"..\/input\/train.csv\", header=0)\ntest_data = pd.read_csv(\"..\/input\/test.csv\", header=0)\nsub = pd.read_csv(\"..\/input\/sample_submission.csv\", header=0)\n\ntrain_data = train_data.drop(['ID'], axis=1)\ntest_data = test_data.drop(['ID'], axis=1)","b1a4dc31":"temp = pd.get_dummies(train_data['resting_electrocardiographic_results'],prefix='resting_electrocardiographic_results')\n\nfor col in temp.columns:\n    train_data[col] = temp[col]\n    \ntrain_data = train_data.drop(['resting_electrocardiographic_results'], axis=1)\n\ntemp = pd.get_dummies(test_data['resting_electrocardiographic_results'],prefix='resting_electrocardiographic_results')\n\nfor col in temp.columns:\n    test_data[col] = temp[col]\n    \ntest_data = test_data.drop(['resting_electrocardiographic_results'], axis=1)","d353d5d9":"temp = pd.get_dummies(train_data['thal'],prefix='thal')\n\nfor col in temp.columns:\n    train_data[col] = temp[col]\n    \ntrain_data = train_data.drop(['thal'], axis=1)\n\ntemp = pd.get_dummies(test_data['thal'],prefix='thal')\n\nfor col in temp.columns:\n    test_data[col] = temp[col]\n    \ntest_data = test_data.drop(['thal'], axis=1)","7108a374":"temp = pd.get_dummies(train_data['number_of_major_vessels'],prefix='number_of_major_vessels')\n\nfor col in temp.columns:\n    train_data[col] = temp[col]\n    \n#train_data = train_data.drop(['number_of_major_vessels'], axis=1)\n\ntemp = pd.get_dummies(test_data['number_of_major_vessels'],prefix='number_of_major_vessels')\n\nfor col in temp.columns:\n    test_data[col] = temp[col]\n    \n#test_data = test_data.drop(['number_of_major_vessels'], axis=1)","b7264fea":"chest_bin = []\n\nfor v in train_data.chest.values:\n    if v>3.5:\n        chest_bin.append(4)\n    elif v > 3:\n        chest_bin.append(3.5)\n    elif v > 2.5:\n        chest_bin.append(3)\n    elif v > 2:\n        chest_bin.append(2.5)\n    elif v > 1.5:\n        chest_bin.append(2)\n    elif v > 1:\n        chest_bin.append(1.5)\n    elif v > 0.5:\n        chest_bin.append(1)\n    elif v > 0:\n        chest_bin.append(0.5)\n    else:\n        chest_bin.append(0)\n\ntrain_data['chest_bin'] = chest_bin\n\nchest_bin = []\n\nfor v in test_data.chest.values:\n    if v>3.5:\n        chest_bin.append(4)\n    elif v > 3:\n        chest_bin.append(3.5)\n    elif v > 2.5:\n        chest_bin.append(3)\n    elif v > 2:\n        chest_bin.append(2.5)\n    elif v > 1.5:\n        chest_bin.append(2)\n    elif v > 1:\n        chest_bin.append(1.5)\n    elif v > 0.5:\n        chest_bin.append(1)\n    elif v > 0:\n        chest_bin.append(0.5)\n    else:\n        chest_bin.append(0)\n\ntest_data['chest_bin'] = chest_bin","8e584bc0":"temp = pd.get_dummies(train_data['chest_bin'],prefix='chest_bin')\n\nfor col in temp.columns:\n    train_data[col] = temp[col]\n    \ntrain_data = train_data.drop(['chest_bin'], axis=1)\ntrain_data = train_data.drop(['chest'], axis=1)\n\ntemp = pd.get_dummies(test_data['chest_bin'],prefix='chest_bin')\n\nfor col in temp.columns:\n    test_data[col] = temp[col]\n    \ntest_data = test_data.drop(['chest_bin'], axis=1)\ntest_data = test_data.drop(['chest'], axis=1)","07f18fca":"train_data.columns","9804ff72":"plt.figure(figsize=(18,15))\nsns.heatmap(train_data.corr())","535785bf":"# funtion to get accuracy\ndef get_score(y_temp_l):\n    y_pred_l = []\n    for i in y_temp_l:\n        if i > 0.5:\n            y_pred_l.append(1)\n        else:\n            y_pred_l.append(0)\n    print(accuracy_score(y_pred_l,test_y))","fd59328f":"feature_col = [col for col in train_data.columns if col != 'class']\nX_train, X_test, train_y, test_y = train_test_split(train_data[feature_col],train_data['class'].values,test_size = 0.2, random_state=42)","4f2d2f95":"%%time\n# Fitting a simple Logistic Regression \nclf_log = LogisticRegression(C=1.0)\nclf_log.fit(X_train, train_y)\npredictions = clf_log.predict_proba(X_test)\npredictions = [i[1] for i in predictions]\npredictions_log = predictions\nget_score(predictions)","9473fabc":"#%%time\n# Fitting a svm\n#clf = SVC(C=1.0, probability=True)\n#clf.fit(X_train, train_y)\n#predictions = clf.predict_proba(X_test)\n#predictions = [i[1] for i in predictions]\n#predictions_sv = predictions\n#get_score(predictions)","99d22fb8":"%%time\n# Fitting a random forest\nclf_r = RandomForestClassifier(n_estimators=200, max_depth=7,random_state=0)\nclf_r.fit(X_train, train_y)\npredictions = clf_r.predict_proba(X_test)\npredictions = [i[1] for i in predictions]\npredictions_r = predictions\nget_score(predictions)","954b4f32":"%%time\n# Fitting xgboost\nclf_x = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf_x.fit(X_train, train_y)\npredictions = clf_x.predict_proba(X_test)\npredictions = [i[1] for i in predictions]\npredictions_x = predictions\nget_score(predictions)","5422b29e":"%%time\n# Fitting lightgbm\nclf_l = lgb.LGBMClassifier(boosting_type= 'gbdt',\n          objective= 'binary',\n          nthread= 4, # Updated from nthread\n          metric = 'binary_error',\n         seed  = 47,\n        depth =  5)\nclf_l.fit(X_train, train_y)\npredictions = clf_l.predict_proba(X_test)\npredictions = [i[1] for i in predictions]\npredictions_l = predictions\nget_score(predictions)","f0b7eae3":"%%time\n# Fitting catboost\nclf_c = CatBoostClassifier(iterations=500, learning_rate=0.07, verbose=False, \n                           depth =  5,loss_function='Logloss', thread_count = 4,\n                           eval_metric='Accuracy')\n\nclf_c.fit(X_train, train_y)\npredictions = clf_c.predict_proba(X_test)\npredictions = [i[1] for i in predictions]\npredictions_c = predictions\nget_score(predictions)","eb134e9a":"# fitting neural network\n# scale the data before any neural net:\nscl = preprocessing.StandardScaler()\nxtrain_scl = scl.fit_transform(X_train)\nxvalid_scl = scl.transform(X_test)\n\nmodel = Sequential()\n\nmodel.add(Dense(29, input_dim=29, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(30, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(30, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(1, activation='sigmoid'))\n\n# compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.fit(xtrain_scl, y=train_y, batch_size=64, \n          epochs=3, verbose=1, \n          validation_data=(xvalid_scl, test_y))\n\npredictions = model.predict_proba(xvalid_scl)\npredictions = [i for i in predictions]\npredictions_n = predictions\nget_score(predictions)","752cc3ff":"kfold = KFold(n_splits=5, random_state=7)","808d2aa1":"# fitting lightgbm\nresults = cross_val_score(lgb.LGBMClassifier(boosting_type= 'gbdt',\n          objective= 'binary',\n          nthread= 4, # Updated from nthread\n          metric = 'binary_error',\n         seed  = 47), train_data[feature_col],train_data['class'].values, cv=kfold)\n\nprint(results)\nprint(np.mean(results))\nprint(np.std(results))","60778d78":"# fitting xgboost\nresults = cross_val_score(xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\n                          , train_data[feature_col],train_data['class'].values, cv=kfold)\n\nprint(results)\nprint(np.mean(results))\nprint(np.std(results))","5ea89f85":"# fitting catboost\nresults = cross_val_score(CatBoostClassifier(iterations=500, learning_rate=0.1, verbose=False, depth =  8,loss_function='Logloss')\n                          , train_data[feature_col],train_data['class'].values, cv=kfold)\n\nprint(results)\nprint(np.mean(results))\nprint(np.std(results))","084a2485":"predictions = (np.array(predictions_x) + np.array(predictions_l))\/2\nget_score(predictions)","b161f866":"predictions_x = clf_x.predict_proba(test_data)\npredictions_x = [i[1] for i in predictions_x]","ffe8bdb0":"predictions_l = clf_l.predict_proba(test_data)\npredictions_l = [i[1] for i in predictions_l]","c0910591":"predictions_c = clf_c.predict_proba(test_data)\npredictions_c = [i[1] for i in predictions_c]","ed149568":"predictions = (np.array(predictions_x) + np.array(predictions_l) + np.array(predictions_c))\/3\n\np = []\n\nfor i in predictions:\n    if i > 0.5:\n        p.append(1)\n    else:\n        p.append(0)\n        \nsub['class'] = p\nsub.to_csv(\"submission_1.csv\",index=False)","3f6ec904":"p = []\n\nfor i in predictions_c:\n    if i > 0.5:\n        p.append(1)\n    else:\n        p.append(0)\n        \nsub['class'] = p\nsub.to_csv(\"submission_2.csv\",index=False)","c2e09c60":"## Cross validate best models","b4c9fae4":"## Explore correlation ","ea680b4a":"## Explore Models ","cff8f112":"## Data Processing\n\nOne-hot encoding and binning data","5338613c":"## Top 1 - submission","354dc1d5":"## Ensemble - Averaging two models","88b6bd17":"## Train different models with tuning\n\n** Models **\n - Logistic Regression\n - SVM\n - Randomforest\n - XGBoost\n - LightGBM\n - Catboost\n - Neural Network","c72ff1d9":"## Top 2 - submission"}}