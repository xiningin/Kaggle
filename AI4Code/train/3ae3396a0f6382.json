{"cell_type":{"c8f45048":"code","b22b02b5":"code","b2d8c4d7":"code","4c6bd3c3":"code","b544ecc7":"code","623bf84a":"code","bbb32707":"code","ed933e44":"code","bcbc835c":"code","5d3b6dbb":"code","d745eaa3":"code","c74757f3":"code","a6313cab":"code","a028edc9":"code","c435acb6":"code","b812acb7":"code","275dfa76":"code","2bdf790f":"code","7dbf2e66":"code","8c096b8f":"code","a418d67f":"code","48cdb782":"code","3ca7bc20":"code","a85121d6":"code","b0e6fe4c":"code","91960e8c":"code","612945f5":"code","a222b638":"code","9b3dd30f":"code","89742b58":"code","ce716d47":"code","ce56e15f":"code","e9dff0a3":"code","c71af3ef":"code","42b1aef7":"code","0bb41b5d":"code","55b73290":"code","175347ab":"code","1cbc7674":"code","1c44d540":"code","56be6bb1":"code","8a376f2a":"code","64e44298":"code","08af5e2f":"code","1fae1894":"code","018fe0bc":"code","98cee762":"code","c52c35b5":"code","951c5b7c":"code","8c0f609a":"code","fa49cd92":"code","48337811":"code","f1b3cfce":"code","202c1215":"code","c0faecbd":"code","d20bca1c":"code","8977e679":"code","55023d30":"code","af074151":"code","33a3d4a3":"code","97b1d27b":"code","0fbd26da":"code","d26656c9":"code","d8a1b1c1":"code","173af1ce":"code","d9e93ff2":"code","770827f5":"code","b85dcbbc":"code","501f34ef":"code","f5d40f0e":"code","440e2943":"code","41f80c64":"code","fed5af07":"code","1c9d4508":"code","fe3568a6":"code","b96fd1fa":"code","5361e9d9":"code","0df487dc":"code","53520d7a":"code","c05540f8":"code","ca6c01ce":"code","e9dd754a":"code","9b7e46ed":"code","0a78ab5b":"code","7d32f4b1":"code","a4598f82":"code","54a84ec3":"code","9c9c87c7":"code","0b79bd8b":"code","6cdd279c":"code","9b3d455d":"code","789718dd":"code","cfa21d36":"code","eb5e0620":"code","cf67bc1b":"code","12d7049d":"code","a3d7c7b9":"code","d3d7f3d7":"code","7fcb250e":"code","6172f27b":"code","4fc8918c":"code","ad386d00":"code","e22edb33":"code","a091eb4e":"code","294d9467":"code","b9d4b58f":"code","42f25c86":"markdown","d21d9b60":"markdown","59278660":"markdown","b537662f":"markdown","9135cef4":"markdown","aa179cc3":"markdown","0353abdd":"markdown","7962e254":"markdown","2b2de54c":"markdown","f5e5dab6":"markdown","678caaa3":"markdown","bcebf300":"markdown","25de6559":"markdown","7d08049a":"markdown","c65ccb75":"markdown","57ff9e07":"markdown","b69243d7":"markdown","ba917a45":"markdown","b9d383fd":"markdown","d2440763":"markdown","d4ba33f0":"markdown","378c17b7":"markdown","4e927b50":"markdown","80d8562e":"markdown","8bcbbdd4":"markdown","10c322f5":"markdown","01620add":"markdown","27efbcbe":"markdown","a85bf363":"markdown","fd9ca801":"markdown","cfc28ae5":"markdown","44fb51de":"markdown","28d867c9":"markdown","a33fcc81":"markdown","3a857ebe":"markdown","ea7d9b07":"markdown","2772a5bc":"markdown","1d4536ba":"markdown","83cc7985":"markdown","2d2982d3":"markdown","a230d90d":"markdown","1e5d51fd":"markdown","05eb5ea1":"markdown","f20a25b9":"markdown","2d087804":"markdown","ad1c19b0":"markdown","33231f31":"markdown","a937e131":"markdown","031f25b5":"markdown","76917def":"markdown","e9064873":"markdown","f9761ce5":"markdown","2b83733f":"markdown","d22c7cca":"markdown","35bbfbde":"markdown","ca14fcf8":"markdown","891bee08":"markdown","0f847f94":"markdown","f26ea229":"markdown","be2c56a2":"markdown","5fe027fd":"markdown","36c009b3":"markdown","0299ced2":"markdown","ccd0de31":"markdown","fbaf168d":"markdown","4356ed78":"markdown","5c9731b0":"markdown","984a81c7":"markdown","0aa723bd":"markdown","8d147886":"markdown","00111422":"markdown"},"source":{"c8f45048":"!pip install scikit-learn==0.22","b22b02b5":"import numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import StratifiedShuffleSplit, KFold, GridSearchCV, RandomizedSearchCV\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.inspection import plot_partial_dependence\n\nfrom sklearn.linear_model import Lasso, Ridge, SGDRegressor\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\n\nimport df_pipeline as dfp  # all the custom pipeline parts\nimport explore_data as exp  # all function to quickly explore the data\n\nimport warnings\n\npd.set_option('max_columns', 200)","b2d8c4d7":"df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n\ndf_train.head()","4c6bd3c3":"col_mis = exp.list_missing(df_train)","b544ecc7":"df_train.hist(bins=50, figsize=(20,15))\nplt.show()","623bf84a":"df_train['target'] = np.log1p(df_train.SalePrice)\ndel df_train['SalePrice']","bbb32707":"df_train = df_train[df_train.GrLivArea < 4500].reset_index(drop=True)","ed933e44":"def make_test(train, test_size, random_state, strat_feat=None):\n    if strat_feat:\n        \n        split = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n\n        for train_index, test_index in split.split(train, train[strat_feat]):\n            train_set = train.loc[train_index]\n            test_set = train.loc[test_index]\n            \n    return train_set, test_set","bcbc835c":"train_set, test_set = make_test(df_train, \n                                test_size=0.2, random_state=654, \n                                strat_feat='Neighborhood')","5d3b6dbb":"train_set.Neighborhood.value_counts(normalize=True)","d745eaa3":"df_train.Neighborhood.value_counts(normalize=True)","c74757f3":"class general_cleaner(BaseEstimator, TransformerMixin):\n    '''\n    This class applies what we know from the documetation.\n    It cleans some known missing values\n    If flags the missing values\n\n    This process is supposed to happen as first step of any pipeline\n    '''\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        #LotFrontage\n        X.loc[X.LotFrontage.isnull(), 'LotFrontage'] = 0\n        #Alley\n        X.loc[X.Alley.isnull(), 'Alley'] = \"NoAlley\"\n        #MSSubClass\n        X['MSSubClass'] = X['MSSubClass'].astype(str)\n        #MissingBasement\n        fil = ((X.BsmtQual.isnull()) & (X.BsmtCond.isnull()) & (X.BsmtExposure.isnull()) &\n              (X.BsmtFinType1.isnull()) & (X.BsmtFinType2.isnull()))\n        fil1 = ((X.BsmtQual.notnull()) | (X.BsmtCond.notnull()) | (X.BsmtExposure.notnull()) |\n              (X.BsmtFinType1.notnull()) | (X.BsmtFinType2.notnull()))\n        X.loc[fil1, 'MisBsm'] = 0\n        X.loc[fil, 'MisBsm'] = 1 # made explicit for safety\n        #BsmtQual\n        X.loc[fil, 'BsmtQual'] = \"NoBsmt\" #missing basement\n        #BsmtCond\n        X.loc[fil, 'BsmtCond'] = \"NoBsmt\" #missing basement\n        #BsmtExposure\n        X.loc[fil, 'BsmtExposure'] = \"NoBsmt\" #missing basement\n        #BsmtFinType1\n        X.loc[fil, 'BsmtFinType1'] = \"NoBsmt\" #missing basement\n        #BsmtFinType2\n        X.loc[fil, 'BsmtFinType2'] = \"NoBsmt\" #missing basement\n        #BsmtFinSF1\n        X.loc[fil, 'BsmtFinSF1'] = 0 # No bsmt\n        #BsmtFinSF2\n        X.loc[fil, 'BsmtFinSF2'] = 0 # No bsmt\n        #BsmtUnfSF\n        X.loc[fil, 'BsmtUnfSF'] = 0 # No bsmt\n        #TotalBsmtSF\n        X.loc[fil, 'TotalBsmtSF'] = 0 # No bsmt\n        #BsmtFullBath\n        X.loc[fil, 'BsmtFullBath'] = 0 # No bsmt\n        #BsmtHalfBath\n        X.loc[fil, 'BsmtHalfBath'] = 0 # No bsmt\n        #FireplaceQu\n        X.loc[(X.Fireplaces == 0) & (X.FireplaceQu.isnull()), 'FireplaceQu'] = \"NoFire\" #missing\n        #MisGarage\n        fil = ((X.GarageYrBlt.isnull()) & (X.GarageType.isnull()) & (X.GarageFinish.isnull()) &\n              (X.GarageQual.isnull()) & (X.GarageCond.isnull()))\n        fil1 = ((X.GarageYrBlt.notnull()) | (X.GarageType.notnull()) | (X.GarageFinish.notnull()) |\n              (X.GarageQual.notnull()) | (X.GarageCond.notnull()))\n        X.loc[fil1, 'MisGarage'] = 0\n        X.loc[fil, 'MisGarage'] = 1\n        #GarageYrBlt\n        X.loc[X.GarageYrBlt > 2200, 'GarageYrBlt'] = 2007 #correct mistake\n        X.loc[fil, 'GarageYrBlt'] = X['YearBuilt']  # if no garage, use the age of the building\n        #GarageType\n        X.loc[fil, 'GarageType'] = \"NoGrg\" #missing garage\n        #GarageFinish\n        X.loc[fil, 'GarageFinish'] = \"NoGrg\" #missing\n        #GarageQual\n        X.loc[fil, 'GarageQual'] = \"NoGrg\" #missing\n        #GarageCond\n        X.loc[fil, 'GarageCond'] = \"NoGrg\" #missing\n        #Fence\n        X.loc[X.Fence.isnull(), 'Fence'] = \"NoFence\" #missing fence\n        #Pool\n        fil = ((X.PoolArea == 0) & (X.PoolQC.isnull()))\n        X.loc[fil, 'PoolQC'] = 'NoPool' \n        \n        del X['Id']\n        del X['MiscFeature']  # we already know it doesn't matter\n        \n        return X","a6313cab":"train_cleaned = train_set.copy()  # I want to work on train_set again later from scratch\n\ntrain_cleaned = general_cleaner().fit_transform(train_cleaned)\n\nmis_cols = exp.list_missing(train_cleaned)","a028edc9":"high_corr = exp.plot_correlations(train_cleaned, 'target', limit=10, annot=True)","c435acb6":"high_corr = exp.plot_correlations(train_cleaned, 'target', limit=20)","b812acb7":"exp.plot_distribution(train_cleaned, 'target')","275dfa76":"for col in high_corr[1:6].index:\n    exp.plot_distribution(train_cleaned, col, correlation=high_corr)","2bdf790f":"exp.corr_target(train_cleaned, 'target', list(high_corr[1:12].index))","7dbf2e66":"exp.corr_target(train_cleaned, 'target',\n                [col for col in high_corr.index if 'Qual' in col or 'Car' in col],\n                x_estimator=np.median)","8c096b8f":"exp.find_cats(train_cleaned, 'target', thrs=0.3, critical=0.05)","a418d67f":"exp.segm_target(train_cleaned, 'BsmtQual', 'target')","48cdb782":"exp.segm_target(train_cleaned, 'KitchenQual', 'target')","3ca7bc20":"exp.segm_target(train_cleaned, 'ExterQual', 'target')","a85121d6":"exp.segm_target(train_cleaned, 'LotShape', 'target')","b0e6fe4c":"exp.segm_target(train_cleaned, 'MasVnrType', 'target')","91960e8c":"exp.segm_target(train_cleaned, 'CentralAir', 'target')","612945f5":"exp.segm_target(train_cleaned, 'GarageFinish', 'target')","a222b638":"exp.plot_bivariate(train_cleaned, 'GrLivArea', 'target', hue='HouseStyle', alpha=0.7)","9b3dd30f":"exp.plot_bivariate(train_cleaned, 'GrLivArea', 'target', hue='ExterQual', alpha=0.7)","89742b58":"class tr_numeric(BaseEstimator, TransformerMixin):\n    def __init__(self, SF_room=True, bedroom=True, bath=True, lot=True, service=True):\n        self.columns = []  # useful to well behave with FeatureUnion\n        \n\n    def fit(self, X, y=None):\n        return self\n    \n\n    def remove_skew(self, X, column):\n        X[column] = np.log1p(X[column])\n        return X\n\n    def transform(self, X, y=None):\n        for col in ['GrLivArea', '1stFlrSF', 'LotArea']:\n            X = self.remove_skew(X, col)\n\n        self.columns = X.columns\n        return X\n    \n\n    def get_feature_names(self):\n        return self.columns\n    \n    \nclass make_ordinal(BaseEstimator, TransformerMixin):\n    '''\n    Transforms ordinal features in order to have them as numeric (preserving the order)\n    If unsure about converting or not a feature (maybe making dummies is better), make use of\n    extra_cols and unsure_conversion\n    '''\n    def __init__(self, cols, extra_cols=None, include_extra=True):\n        self.cols = cols\n        self.extra_cols = extra_cols\n        self.mapping = {'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n        self.include_extra = include_extra\n    \n\n    def fit(self, X, y=None):\n        return self\n    \n\n    def transform(self, X, y=None):\n        if self.extra_cols:\n            if self.include_extra:\n                self.cols += self.extra_cols\n            else:\n                for col in self.extra_cols:\n                    del X[col]\n        \n        for col in self.cols:\n            X.loc[:, col] = X[col].map(self.mapping).fillna(0)\n        return X\n\n\nclass recode_cat(BaseEstimator, TransformerMixin):        \n    '''\n    Recodes some categorical variables according to the insights gained from the\n    data exploration phase.\n    '''\n    def fit(self, X, y=None):\n        return self\n    \n    \n    def tr_GrgType(self, data):\n        data['GarageType'] = data['GarageType'].map({'Basment': 'Attchd',\n                                                  'CarPort': 'Detchd', \n                                                  '2Types': 'Attchd' }).fillna(data['GarageType'])\n        return data\n    \n    \n    def tr_LotShape(self, data):\n        fil = (data.LotShape != 'Reg')\n        data['LotShape'] = 1\n        data.loc[fil, 'LotShape'] = 0\n        return data\n    \n    \n    def tr_LandCont(self, data):\n        fil = (data.LandContour == 'HLS') | (data.LandContour == 'Low')\n        data['LandContour'] = 0\n        data.loc[fil, 'LandContour'] = 1\n        return data\n    \n    \n    def tr_MSZoning(self, data):\n        data['MSZoning'] = data['MSZoning'].map({'RH': 'RM', # medium and high density\n                                                 'C (all)': 'RM', # commercial and medium density\n                                                 'FV': 'RM'}).fillna(data['MSZoning'])\n        return data\n    \n    \n    def tr_Alley(self, data):\n        fil = (data.Alley != 'NoAlley')\n        data['Alley'] = 0\n        data.loc[fil, 'Alley'] = 1\n        return data\n    \n    \n    def tr_LotConfig(self, data):\n        data['LotConfig'] = data['LotConfig'].map({'FR3': 'Corner', # corners have 2 or 3 free sides\n                                                   'FR2': 'Corner'}).fillna(data['LotConfig'])\n        return data\n    \n    \n    def tr_BldgType(self, data):\n        data['BldgType'] = data['BldgType'].map({'Twnhs' : 'TwnhsE',\n                                                 '2fmCon': 'Duplex'}).fillna(data['BldgType'])\n        return data\n    \n    \n    def tr_MasVnrType(self, data):\n        data['MasVnrType'] = data['MasVnrType'].map({'BrkCmn': 'BrkFace'}).fillna(data['MasVnrType'])\n        return data\n\n\n    def tr_HouseStyle(self, data):\n        data['HouseStyle'] = data['HouseStyle'].map({'1.5Fin': '1.5Unf',\n                                                     '2.5Fin': '2Story',\n                                                     '2.5Unf': '2Story',\n                                                     'SLvl': 'SFoyer'}).fillna(data['HouseStyle'])\n        return data\n    \n    \n    def transform(self, X, y=None):\n        X = self.tr_GrgType(X)\n        X = self.tr_LotShape(X)\n        X = self.tr_LotConfig(X)\n        X = self.tr_MSZoning(X)\n        X = self.tr_Alley(X)\n        X = self.tr_LandCont(X)\n        X = self.tr_BldgType(X)\n        X = self.tr_MasVnrType(X)\n        X = self.tr_HouseStyle(X)\n        return X\n    \n\nclass drop_columns(BaseEstimator, TransformerMixin):\n    '''\n    Drops columns that are not useful for the model\n    '''\n    def __init__(self):\n        self.columns = []\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        to_drop = [col for col in X.columns if 'NoGrg' in col]  # dropping dummies that are redundant\n        to_drop += [col for col in X.columns if 'NoBsmt' in col]\n        # other not useful columns\n        to_drop += [col for col in X.columns if 'MSSubClass' in col]\n        to_drop += [col for col in X.columns if 'Neighborhood' in col]  # maybe useful in the future\n        to_drop += [col for col in X.columns if 'Condition1' in col]\n        to_drop += [col for col in X.columns if 'Condition2' in col]\n        to_drop += [col for col in X.columns if 'ExterCond' in col]  # maybe make it ordinal\n        to_drop += [col for col in X.columns if 'Exterior1st' in col]\n        to_drop += [col for col in X.columns if 'Exterior2nd' in col]\n        to_drop += [col for col in X.columns if 'Functional' in col]\n        to_drop += [col for col in X.columns if 'Heating_' in col]  # we don't want to drop the dummies of HeatingQC too\n        to_drop += [col for col in X.columns if 'PoolQC' in col]\n        to_drop += [col for col in X.columns if 'RoofMatl' in col]\n        to_drop += [col for col in X.columns if 'RoofStyle' in col]\n        to_drop += [col for col in X.columns if 'SaleCondition' in col]\n        to_drop += [col for col in X.columns if 'SaleType' in col]\n        to_drop += [col for col in X.columns if 'Utilities' in col]\n        to_drop += [col for col in X.columns if 'BsmtCond' in col]  # maybe ordinal\n        to_drop += [col for col in X.columns if 'Electrical' in col]\n        to_drop += [col for col in X.columns if 'Foundation' in col]\n        to_drop += [col for col in X.columns if 'LandSlope' in col]\n        to_drop += [col for col in X.columns if 'Street' in col]\n        to_drop += [col for col in X.columns if 'Fence' in col]\n        to_drop += [col for col in X.columns if 'PavedDrive' in col]\n\n        for col in to_drop:\n            try:\n                del X[col]\n            except KeyError:\n                pass\n            \n        self.columns = X.columns\n        return X\n    \n    def get_feature_names(self):\n        return list(self.columns)","ce716d47":"numeric_pipe = Pipeline([('fs', dfp.feat_sel('numeric')),\n                         ('imputer', dfp.df_imputer(strategy='median')),\n                         ('transf', tr_numeric())])\n\n\ncat_pipe = Pipeline([('fs', dfp.feat_sel('category')),\n                     ('imputer', dfp.df_imputer(strategy='most_frequent')), \n                     ('ord', make_ordinal(['BsmtQual', 'KitchenQual','GarageQual',\n                                           'GarageCond', 'ExterQual', 'HeatingQC'])), \n                     ('recode', recode_cat()), \n                     ('dummies', dfp.dummify())])\n\n\nprocessing_pipe = dfp.FeatureUnion_df(transformer_list=[('cat_pipe', cat_pipe),\n                                                 ('num_pipe', numeric_pipe)])\n\n\nfull_pipe = Pipeline([('gen_cl', general_cleaner()), \n                      ('processing', processing_pipe), \n                      ('scaler', dfp.df_scaler()), \n                      ('dropper', drop_columns())])\n\ntmp = train_set.copy()\n\nfull_pipe.fit_transform(tmp).head()","ce56e15f":"def cv_score(df_train, y_train, kfolds, pipeline, imp_coef=False):\n    oof = np.zeros(len(df_train))\n    train = df_train.copy()\n    \n    feat_df = pd.DataFrame()\n    \n    for n_fold, (train_index, test_index) in enumerate(kfolds.split(train.values)):\n            \n        trn_data = train.iloc[train_index][:]\n        val_data = train.iloc[test_index][:]\n        \n        trn_target = y_train.iloc[train_index].values.ravel()\n        val_target = y_train.iloc[test_index].values.ravel()\n        \n        pipeline.fit(trn_data, trn_target)\n\n        oof[test_index] = pipeline.predict(val_data).ravel()\n\n        if imp_coef:\n            try:\n                fold_df = get_coef(pipeline)\n            except AttributeError:\n                fold_df = get_feature_importance(pipeline)\n                \n            fold_df['fold'] = n_fold + 1\n            feat_df = pd.concat([feat_df, fold_df], axis=0)\n       \n    if imp_coef:\n        feat_df = feat_df.groupby('feat')['score'].agg(['mean', 'std'])\n        feat_df['abs_sco'] = (abs(feat_df['mean']))\n        feat_df = feat_df.sort_values(by=['abs_sco'],ascending=False)\n        del feat_df['abs_sco']\n        return oof, feat_df\n    else:    \n        return oof\n\n\ndef grid_search(data, target, estimator, param_grid, scoring, cv, random=False):\n    \n    if random:\n        grid = RandomizedSearchCV(estimator=estimator, param_distributions=param_grid, cv=cv, scoring=scoring, \n                                  n_iter=random, n_jobs=-1, random_state=434, iid=False)\n    else:\n        grid = GridSearchCV(estimator=estimator, param_grid=param_grid, \n                            cv=cv, scoring=scoring, n_jobs=-1, return_train_score=False)\n    \n    pd.options.mode.chained_assignment = None  # turn on and off a warning of pandas\n    tmp = data.copy()\n    grid = grid.fit(tmp, target)\n    pd.options.mode.chained_assignment = 'warn'\n    \n    result = pd.DataFrame(grid.cv_results_).sort_values(by='mean_test_score', \n                                                        ascending=False).reset_index()\n    \n    del result['params']\n    times = [col for col in result.columns if col.endswith('_time')]\n    params = [col for col in result.columns if col.startswith('param_')]\n    \n    result = result[params + ['mean_test_score', 'std_test_score'] + times]\n    \n    return result, grid.best_params_, grid.best_estimator_\n\n\ndef get_coef(pipe):\n    imp = pipe.steps[-1][1].coef_.tolist()\n    feats = pipe.steps[-2][1].get_feature_names()\n    result = pd.DataFrame({'feat':feats,'score':imp})\n    result['abs_res'] = abs(result['score'])\n    result = result.sort_values(by=['abs_res'],ascending=False)\n    del result['abs_res']\n    return result\n\n\ndef get_feature_importance(pipe):\n    imp = pipe.steps[-1][1].feature_importances_.tolist() #it's a pipeline\n    feats = pipe.steps[-2][1].get_feature_names()\n    result = pd.DataFrame({'feat':feats,'score':imp})\n    result = result.sort_values(by=['score'],ascending=False)\n    return result\n\n\ndef _plot_diagonal(ax):\n    xmin, xmax = ax.get_xlim()\n    ymin, ymax = ax.get_ylim()\n    low = min(xmin, xmax)\n    high = max(xmin, xmax)\n    scl = (high - low) \/ 100\n    \n    line = pd.DataFrame({'x': np.arange(low, high ,scl), # small hack for a diagonal line\n                         'y': np.arange(low, high ,scl)})\n    ax.plot(line.x, line.y, color='black', linestyle='--')\n    \n    return ax\n\n\ndef plot_predictions(data, true_label, pred_label, feature=None, hue=None, legend=False):\n    \n    tmp = data.copy()\n    tmp['Prediction'] = pred_label\n    tmp['True Label'] = true_label\n    tmp['Residual'] = tmp['True Label'] - tmp['Prediction']\n    \n    diag = False\n    alpha = 0.7\n    label = ''\n    \n    fig, ax = plt.subplots(1,2, figsize=(15,6))\n    \n    if feature is None:\n        feature = 'True Label'\n        diag = True\n    else:\n        legend = 'full'\n        sns.scatterplot(x=feature, y='True Label', data=tmp, ax=ax[0], label='True',\n                         hue=hue, legend=legend, alpha=alpha)\n        label = 'Predicted'\n        alpha = 0.4\n\n    sns.scatterplot(x=feature, y='Prediction', data=tmp, ax=ax[0], label=label,\n                         hue=hue, legend=legend, alpha=alpha)\n    if diag:\n        ax[0] = _plot_diagonal(ax[0])\n    \n    sns.scatterplot(x=feature, y='Residual', data=tmp, ax=ax[1], \n                    hue=hue, legend=legend, alpha=0.7)\n    ax[1].axhline(y=0, color='r', linestyle='--')\n    \n    ax[0].set_title(f'{feature} vs Predictions')\n    ax[1].set_title(f'{feature} vs Residuals')","e9dff0a3":"models = [('lasso', Lasso(alpha=0.01)), ('ridge', Ridge()), ('sgd', SGDRegressor()), \n          ('forest', RandomForestRegressor(n_estimators=200)), ('xtree', ExtraTreesRegressor(n_estimators=200)), \n          ('svr', SVR()), \n          ('kneig', KNeighborsRegressor()),\n          ('xgb', xgb.XGBRegressor(n_estimators=200, objective='reg:squarederror')), \n          ('lgb', lgb.LGBMRegressor(n_estimators=200))]\n\nmod_name = []\nrmse_train = []\nrmse_test = []\nmae_train = []\nmae_test = []\n\nfolds = KFold(5, shuffle=True, random_state=541)\n\ny = train_set['target'].copy()\ndel train_set['target']\ny_test = test_set['target']\ndel test_set['target']\n\nwarnings.filterwarnings(\"ignore\", \n                        message=\"The dummies in this set do not match the ones in the train set, we corrected the issue.\")\n\nfor model in models:\n    \n    train = train_set.copy()\n    test = test_set.copy()\n    print(model[0])\n    mod_name.append(model[0])\n    \n    pipe = [('gen_cl', general_cleaner()),\n            ('processing', processing_pipe),\n            ('scl', dfp.df_scaler()),\n            ('dropper', drop_columns())] + [model]\n    \n    model_pipe = Pipeline(pipe)\n            \n    inf_preds = cv_score(train, y, folds, model_pipe)\n    \n    model_pipe.fit(train, y)  # refit on full train set\n    \n    preds = model_pipe.predict(test)\n    \n    rmse_train.append(mean_squared_error(y, inf_preds))\n    rmse_test.append(mean_squared_error(y_test, preds))\n    mae_train.append(mean_absolute_error(np.expm1(y), np.expm1(inf_preds)))\n    mae_test.append(mean_absolute_error(np.expm1(y_test), np.expm1(preds)))\n    \n    print(f'\\tTrain set RMSE: {round(np.sqrt(mean_squared_error(y, inf_preds)), 4)}')\n    print(f'\\tTrain set MAE: {round(mean_absolute_error(np.expm1(y), np.expm1(inf_preds)), 2)}')\n    print(f'\\tTest set RMSE: {round(np.sqrt(mean_squared_error(y_test, preds)), 4)}')\n    print(f'\\tTest set MAE: {round(mean_absolute_error(np.expm1(y_test), np.expm1(preds)), 2)}')\n    \n    print('_'*40)\n    print('\\n')\n    \nresults = pd.DataFrame({'model_name': mod_name, \n                        'rmse_train': rmse_train, 'rmse_test': rmse_test,\n                        'mae_train': mae_train, 'mae_test': mae_test})\n\nresults","c71af3ef":"lasso_pipe = Pipeline([('gen_cl', general_cleaner()),\n                       ('processing', processing_pipe),\n                       ('scl', dfp.df_scaler()), \n                       ('dropper', drop_columns()), \n                       ('lasso', Lasso(alpha=0.01))])\n\nlasso_oof, lasso_coef = cv_score(train_set, y, folds, lasso_pipe, imp_coef=True)\n\nlasso_coef","42b1aef7":"plot_predictions(train_set, y, lasso_oof)","0bb41b5d":"lasso_coef[lasso_coef['mean']==0].sample(10)","55b73290":"for col in ['GrLivArea', '1stFlrSF', 'LotArea']:\n    train_cleaned[col] = np.log1p(train_cleaned[col])\n\nexp.corr_target(train_cleaned, 'target', ['GrLivArea', '1stFlrSF', 'LotArea'])","175347ab":"train_cleaned = make_ordinal(['BsmtQual', 'KitchenQual',\n                              'GarageQual','GarageCond', \n                              'ExterQual', 'HeatingQC']).fit_transform(train_cleaned)\n\nexp.corr_target(train_cleaned, 'target', ['BsmtQual', 'KitchenQual',\n                                          'GarageQual','GarageCond',\n                                          'ExterQual', 'HeatingQC'], x_estimator=np.mean)","1cbc7674":"train_cleaned = recode_cat().fit_transform(train_cleaned)\n\nexp.segm_target(train_cleaned, 'GarageType', 'target')","1c44d540":"def SF_per_room(data):\n    data['sf_per_room'] = data['GrLivArea'] \/ data['TotRmsAbvGrd']\n    return data\n\ndef bedroom_prop(data):\n    data['bedroom_prop'] = data['BedroomAbvGr'] \/ data['TotRmsAbvGrd']\n    return data\n\ntrain_cleaned = SF_per_room(train_cleaned)\ntrain_cleaned = bedroom_prop(train_cleaned)\n\nexp.corr_target(data=train_cleaned, \n                cols=['GrLivArea', 'TotRmsAbvGrd', 'BedroomAbvGr', 'sf_per_room', 'bedroom_prop'], \n                target='target')","56be6bb1":"def total_bath(data):\n    data['total_bath'] = data[[col for col in data.columns if 'FullBath' in col]].sum(axis=1) \\\n                        + 0.5 * data[[col for col in data.columns if 'HalfBath' in col]].sum(axis=1)\n    return data\n\ntrain_cleaned = total_bath(train_cleaned)\nexp.corr_target(data=train_cleaned, \n                cols=[col for col in train_cleaned if 'Bath' in col] + ['total_bath'], \n                target='target')","8a376f2a":"def lot_prop(data):\n    data['lot_prop'] = data['LotArea'] \/ data['GrLivArea']\n    return data\n\ntrain_cleaned = lot_prop(train_cleaned)\n\nexp.corr_target(data=train_cleaned,\n                cols=['LotArea', 'GrLivArea', 'lot_prop'],\n                target='target')","64e44298":"def service_area(data):\n    data['service_area'] = data['TotalBsmtSF'] + data['GarageArea']\n    return data\n\ntrain_cleaned = service_area(train_cleaned)\n\nexp.corr_target(data=train_cleaned,\n                cols=['TotalBsmtSF', 'GarageArea', 'service_area'],\n                target='target')","08af5e2f":"def tr_neighborhood(data, y=None):\n    # mean and standard deviation of the price per Neighborhood\n    means = data.groupby('Neighborhood')['target'].mean()\n    stds = data.groupby('Neighborhood')['target'].std()\n    data['Neig_target_mean'] = data['Neighborhood'].map(means)\n    data['Neig_target_std'] = data['Neighborhood'].map(stds)\n    # mean and standard deviation of the house size per Neighborhood\n    means = data.groupby('Neighborhood')['GrLivArea'].mean()\n    stds = data.groupby('Neighborhood')['GrLivArea'].std()\n    data['Neig_GrLivArea_mean'] = data['Neighborhood'].map(means)\n    data['Neig_GrLivArea_std'] = data['Neighborhood'].map(stds)\n    \n    return data\n\ntrain_cleaned = tr_neighborhood(train_cleaned)\n\nexp.corr_target(train_cleaned, 'target', ['GrLivArea',\n                                          'Neig_target_mean','Neig_target_std',\n                                          'Neig_GrLivArea_mean', 'Neig_GrLivArea_std' \n                                          ])","1fae1894":"def tr_mssubclass(data, y=None):\n    # mean and standard deviation of the price per Neighborhood\n    means = data.groupby('MSSubClass')['target'].mean()\n    stds = data.groupby('MSSubClass')['target'].std()\n    data['MSSC_target_mean'] = data['MSSubClass'].map(means)\n    data['MSSC_target_std'] = data['MSSubClass'].map(stds)\n    # mean and standard deviation of the house size per Neighborhood\n    means = data.groupby('MSSubClass')['GrLivArea'].mean()\n    stds = data.groupby('MSSubClass')['GrLivArea'].std()\n    data['MSSC_GrLivArea_mean'] = data['MSSubClass'].map(means)\n    data['MSSC_GrLivArea_std'] = data['MSSubClass'].map(stds)\n    \n    return data\n\ntrain_cleaned = tr_mssubclass(train_cleaned)\n\nexp.corr_target(train_cleaned, 'target', ['GrLivArea',\n                                          'MSSC_target_mean','MSSC_target_std',\n                                          'MSSC_GrLivArea_mean', 'MSSC_GrLivArea_std' \n                                          ])","018fe0bc":"class general_cleaner(BaseEstimator, TransformerMixin):\n    '''\n    This class applies what we know from the documetation.\n    It cleans some known missing values\n    If flags the missing values\n\n    This process is supposed to happen as first step of any pipeline\n    '''\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        #LotFrontage\n        X.loc[X.LotFrontage.isnull(), 'LotFrontage'] = 0\n        #Alley\n        X.loc[X.Alley.isnull(), 'Alley'] = \"NoAlley\"\n        #MSSubClass\n        X['MSSubClass'] = X['MSSubClass'].astype(str)\n        #MissingBasement\n        fil = ((X.BsmtQual.isnull()) & (X.BsmtCond.isnull()) & (X.BsmtExposure.isnull()) &\n              (X.BsmtFinType1.isnull()) & (X.BsmtFinType2.isnull()))\n        fil1 = ((X.BsmtQual.notnull()) | (X.BsmtCond.notnull()) | (X.BsmtExposure.notnull()) |\n              (X.BsmtFinType1.notnull()) | (X.BsmtFinType2.notnull()))\n        X.loc[fil1, 'MisBsm'] = 0\n        X.loc[fil, 'MisBsm'] = 1 # made explicit for safety\n        #BsmtQual\n        X.loc[fil, 'BsmtQual'] = \"NoBsmt\" #missing basement\n        #BsmtCond\n        X.loc[fil, 'BsmtCond'] = \"NoBsmt\" #missing basement\n        #BsmtExposure\n        X.loc[fil, 'BsmtExposure'] = \"NoBsmt\" #missing basement\n        #BsmtFinType1\n        X.loc[fil, 'BsmtFinType1'] = \"NoBsmt\" #missing basement\n        #BsmtFinType2\n        X.loc[fil, 'BsmtFinType2'] = \"NoBsmt\" #missing basement\n        #BsmtFinSF1\n        X.loc[fil, 'BsmtFinSF1'] = 0 # No bsmt\n        #BsmtFinSF2\n        X.loc[fil, 'BsmtFinSF2'] = 0 # No bsmt\n        #BsmtUnfSF\n        X.loc[fil, 'BsmtUnfSF'] = 0 # No bsmt\n        #TotalBsmtSF\n        X.loc[fil, 'TotalBsmtSF'] = 0 # No bsmt\n        #BsmtFullBath\n        X.loc[fil, 'BsmtFullBath'] = 0 # No bsmt\n        #BsmtHalfBath\n        X.loc[fil, 'BsmtHalfBath'] = 0 # No bsmt\n        #FireplaceQu\n        X.loc[(X.Fireplaces == 0) & (X.FireplaceQu.isnull()), 'FireplaceQu'] = \"NoFire\" #missing\n        #MisGarage\n        fil = ((X.GarageYrBlt.isnull()) & (X.GarageType.isnull()) & (X.GarageFinish.isnull()) &\n              (X.GarageQual.isnull()) & (X.GarageCond.isnull()))\n        fil1 = ((X.GarageYrBlt.notnull()) | (X.GarageType.notnull()) | (X.GarageFinish.notnull()) |\n              (X.GarageQual.notnull()) | (X.GarageCond.notnull()))\n        X.loc[fil1, 'MisGarage'] = 0\n        X.loc[fil, 'MisGarage'] = 1\n        #GarageYrBlt\n        X.loc[X.GarageYrBlt > 2200, 'GarageYrBlt'] = 2007 #correct mistake\n        X.loc[fil, 'GarageYrBlt'] = X['YearBuilt']  # if no garage, use the age of the building\n        #GarageType\n        X.loc[fil, 'GarageType'] = \"NoGrg\" #missing garage\n        #GarageFinish\n        X.loc[fil, 'GarageFinish'] = \"NoGrg\" #missing\n        #GarageQual\n        X.loc[fil, 'GarageQual'] = \"NoGrg\" #missing\n        #GarageCond\n        X.loc[fil, 'GarageCond'] = \"NoGrg\" #missing\n        #Fence\n        X.loc[X.Fence.isnull(), 'Fence'] = \"NoFence\" #missing fence\n        #Pool\n        fil = ((X.PoolArea == 0) & (X.PoolQC.isnull()))\n        X.loc[fil, 'PoolQC'] = 'NoPool' \n        \n        # not useful features\n        del X['Id']\n        del X['MiscFeature']  # we already know it doesn't matter\n        del X['Condition1']\n        del X['Condition2']\n        del X['Exterior1st']\n        del X['Exterior2nd']\n        del X['Functional']\n        del X['Heating']\n        del X['PoolQC']\n        del X['RoofMatl']\n        del X['RoofStyle']\n        del X['SaleCondition']\n        del X['SaleType']\n        del X['Utilities']\n        del X['BsmtFinType1']\n        del X['BsmtFinType2']\n        del X['BsmtFinSF1']\n        del X['BsmtFinSF2']\n        del X['Electrical']\n        del X['Foundation']\n        del X['Street']\n        del X['Fence']\n        del X['LandSlope']\n        del X['LowQualFinSF']\n        del X['FireplaceQu']\n        del X['PoolArea']\n        del X['MiscVal']\n        del X['MoSold']\n        del X['YrSold']\n        \n         # after model iterations\n        del X['KitchenAbvGr']\n        del X['GarageQual']\n        del X['GarageCond'] \n        \n        return X","98cee762":"class tr_numeric(BaseEstimator, TransformerMixin):\n    def __init__(self, SF_room=True, bedroom=True, bath=True, lot=True, service=True):\n        self.columns = []  # useful to well behave with FeatureUnion\n        self.SF_room = SF_room\n        self.bedroom = bedroom\n        self.bath = bath\n        self.lot = lot\n        self.service = service\n     \n\n    def fit(self, X, y=None):\n        return self\n    \n\n    def remove_skew(self, X, column):\n        X[column] = np.log1p(X[column])\n        return X\n\n\n    def SF_per_room(self, X):\n        if self.SF_room:\n            X['sf_per_room'] = X['GrLivArea'] \/ X['TotRmsAbvGrd']\n        return X\n\n\n    def bedroom_prop(self, X):\n        if self.bedroom:\n            X['bedroom_prop'] = X['BedroomAbvGr'] \/ X['TotRmsAbvGrd']\n            del X['BedroomAbvGr'] # the new feature makes it redundant and it is not important\n        return X\n\n\n    def total_bath(self, X):\n        if self.bath:\n            X['total_bath'] = (X[[col for col in X.columns if 'FullBath' in col]].sum(axis=1) +\n                             0.5 * X[[col for col in X.columns if 'HalfBath' in col]].sum(axis=1))\n            del X['FullBath']  # redundant \n\n        del X['HalfBath']  # not useful anyway\n        del X['BsmtHalfBath']\n        del X['BsmtFullBath']\n        return X\n\n\n    def lot_prop(self, X):\n        if self.lot:\n            X['lot_prop'] = X['LotArea'] \/ X['GrLivArea']\n        return X \n\n\n    def service_area(self, X):\n        if self.service:\n            X['service_area'] = X['TotalBsmtSF'] + X['GarageArea']\n            del X['TotalBsmtSF']\n            del X['GarageArea']\n        return X\n    \n\n    def transform(self, X, y=None):\n        for col in ['GrLivArea', '1stFlrSF', 'LotArea']:\n            X = self.remove_skew(X, col)\n\n        X = self.SF_per_room(X)\n        X = self.bedroom_prop(X)\n        X = self.total_bath(X)\n        X = self.lot_prop(X)\n        X = self.service_area(X)\n\n        self.columns = X.columns\n        return X\n    \n\n    def get_feature_names(self):\n        return self.columns\n    \n    \nclass drop_columns(BaseEstimator, TransformerMixin):\n    '''\n    Drops columns that are not useful for the model\n    The decisions come from several iterations\n    '''\n    def __init__(self, lasso=False, ridge=False, forest=False, xgb=False, lgb=False):\n        self.columns = []\n        self.lasso = lasso\n        self.ridge = ridge\n        self.forest = forest\n        self.xgb = xgb\n        self.lgb = lgb\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        to_drop = [col for col in X.columns if 'NoGrg' in col]  # dropping dummies that are redundant\n        to_drop += [col for col in X.columns if 'NoBsmt' in col]\n\n        if self.lasso:\n            to_drop += [col for col in X.columns if 'BsmtExposure' in col]\n            to_drop += [col for col in X.columns if 'BsmtCond' in col]\n            to_drop += [col for col in X.columns if 'ExterCond' in col]\n            to_drop += [col for col in X.columns if 'HouseStyle' in col] \n            to_drop += [col for col in X.columns if 'LotShape' in col] \n            to_drop += [col for col in X.columns if 'LotFrontage' in col]\n            to_drop += [col for col in X.columns if 'GarageYrBlt' in col] \n            to_drop += [col for col in X.columns if 'GarageType' in col] \n            to_drop += ['OpenPorchSF', '3SsnPorch'] \n        if self.ridge: \n            to_drop += [col for col in X.columns if 'BsmtExposure' in col]\n            to_drop += [col for col in X.columns if 'BsmtCond' in col]\n            to_drop += [col for col in X.columns if 'ExterCond' in col] \n            to_drop += [col for col in X.columns if 'LotFrontage' in col]\n            to_drop += [col for col in X.columns if 'LotShape' in col] \n            to_drop += [col for col in X.columns if 'HouseStyle' in col] \n            to_drop += [col for col in X.columns if 'GarageYrBlt' in col]\n            to_drop += [col for col in X.columns if 'GarageCars' in col] \n            to_drop += [col for col in X.columns if 'BldgType' in col] \n            to_drop += ['OpenPorchSF', '3SsnPorch']\n        if self.forest: \n            to_drop += [col for col in X.columns if 'BsmtExposure' in col]\n            to_drop += [col for col in X.columns if 'BsmtCond' in col]\n            to_drop += [col for col in X.columns if 'ExterCond' in col] \n            to_drop += ['OpenPorchSF', '3SsnPorch'] \n        if self.xgb:\n            to_drop += [col for col in X.columns if 'BsmtExposure' in col]\n            to_drop += [col for col in X.columns if 'BsmtCond' in col]\n            to_drop += [col for col in X.columns if 'ExterCond' in col]\n        if self.lgb: \n            to_drop += [col for col in X.columns if 'LotFrontage' in col] \n            to_drop += [col for col in X.columns if 'HouseStyle' in col]\n            to_drop += ['MisBsm'] \n            \n        \n        for col in to_drop:\n            try:\n                del X[col]\n            except KeyError:\n                pass\n            \n        self.columns = X.columns\n        return X\n    \n    def get_feature_names(self):\n        return list(self.columns)\n    \n    \nclass make_ordinal(BaseEstimator, TransformerMixin):\n    '''\n    Transforms ordinal features in order to have them as numeric (preserving the order)\n    If unsure about converting or not a feature (maybe making dummies is better), make use of\n    extra_cols and unsure_conversion\n    '''\n    def __init__(self, cols, extra_cols=None, include_extra='include'):\n        self.cols = cols\n        self.extra_cols = extra_cols\n        self.mapping = {'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n        self.include_extra = include_extra  # either include, dummies, or drop (any other option)\n    \n\n    def fit(self, X, y=None):\n        return self\n    \n\n    def transform(self, X, y=None):\n        if self.extra_cols:\n            if self.include_extra == 'include':\n                self.cols += self.extra_cols\n            elif self.include_extra == 'dummies':\n                pass\n            else:\n                for col in self.extra_cols:\n                    del X[col]\n        \n        for col in self.cols:\n            X.loc[:, col] = X[col].map(self.mapping).fillna(0)\n        return X\n    \n    \nclass recode_cat(BaseEstimator, TransformerMixin):        \n    '''\n    Recodes some categorical variables according to the insights gained from the\n    data exploration phase.\n    '''\n    def __init__(self, mean_weight=10, te_neig=True, te_mssc=True):\n        self.mean_tot = 0\n        self.mean_weight = mean_weight\n        self.smooth_neig = {}\n        self.smooth_mssc = {}\n        self.te_neig = te_neig\n        self.te_mssc = te_mssc\n    \n    \n    def smooth_te(self, data, target, col):\n        tmp_data = data.copy()\n        tmp_data['target'] = target\n        mean_tot = tmp_data['target'].mean()\n        means = tmp_data.groupby(col)['target'].mean()\n        counts = tmp_data.groupby(col)['target'].count()\n\n        smooth = ((counts * means + self.mean_weight * mean_tot) \/ \n                       (counts + self.mean_weight))\n        return mean_tot, smooth\n    \n    def fit(self, X, y):\n        if self.te_neig:\n            self.mean_tot, self.smooth_neig = self.smooth_te(data=X, target=y, col='Neighborhood')\n\n        if self.te_mssc:\n            self.mean_tot, self.smooth_mssc = self.smooth_te(X, y, 'MSSubClass')\n            \n        return self\n    \n    \n    def tr_GrgType(self, data):\n        data['GarageType'] = data['GarageType'].map({'Basment': 'Attchd',\n                                                     'CarPort': 'Detchd',\n                                                     '2Types': 'Attchd' }).fillna(data['GarageType'])\n        return data\n    \n    \n    def tr_LotShape(self, data):\n        fil = (data.LotShape != 'Reg')\n        data['LotShape'] = 1\n        data.loc[fil, 'LotShape'] = 0\n        return data\n    \n    \n    def tr_LandCont(self, data):\n        fil = (data.LandContour == 'HLS') | (data.LandContour == 'Low')\n        data['LandContour'] = 0\n        data.loc[fil, 'LandContour'] = 1\n        return data\n    \n    \n    def tr_LandSlope(self, data):\n        fil = (data.LandSlope != 'Gtl')\n        data['LandSlope'] = 0\n        data.loc[fil, 'LandSlope'] = 1\n        return data\n    \n    \n    def tr_MSZoning(self, data):\n        data['MSZoning'] = data['MSZoning'].map({'RH': 'RM', # medium and high density\n                                                 'C (all)': 'RM', # commercial and medium density\n                                                 'FV': 'RM'}).fillna(data['MSZoning'])\n        return data\n    \n    \n    def tr_Alley(self, data):\n        fil = (data.Alley != 'NoAlley')\n        data['Alley'] = 0\n        data.loc[fil, 'Alley'] = 1\n        return data\n    \n    \n    def tr_LotConfig(self, data):\n        data['LotConfig'] = data['LotConfig'].map({'FR3': 'Corner', # corners have 2 or 3 free sides\n                                                   'FR2': 'Corner'}).fillna(data['LotConfig'])\n        return data\n    \n    \n    def tr_BldgType(self, data):\n        data['BldgType'] = data['BldgType'].map({'Twnhs' : 'TwnhsE',\n                                                 '2fmCon': 'Duplex'}).fillna(data['BldgType'])\n        return data\n    \n    \n    def tr_MasVnrType(self, data):\n        data['MasVnrType'] = data['MasVnrType'].map({'BrkCmn': 'BrkFace'}).fillna(data['MasVnrType'])\n        return data\n\n\n    def tr_HouseStyle(self, data):\n        data['HouseStyle'] = data['HouseStyle'].map({'1.5Fin': '1.5Unf',\n                                                     '2.5Fin': '2Story',\n                                                     '2.5Unf': '2Story',\n                                                     'SLvl': 'SFoyer'}).fillna(data['HouseStyle'])\n        return data\n\n\n    def tr_Neighborhood(self, data):\n        if self.te_neig:\n            data['Neighborhood'] = data['Neighborhood'].map(self.smooth_neig).fillna(self.mean_tot)\n        return data\n    \n    def tr_MSSubClass(self, data):\n        if self.te_mssc:\n            data['MSSubClass'] = data['MSSubClass'].map(self.smooth_mssc).fillna(self.mean_tot)\n        return data\n    \n    \n    def transform(self, X, y=None):\n        X = self.tr_GrgType(X)\n        X = self.tr_LotShape(X)\n        X = self.tr_LotConfig(X)\n        X = self.tr_MSZoning(X)\n        X = self.tr_Alley(X)\n        X = self.tr_LandCont(X)\n        X = self.tr_BldgType(X)\n        X = self.tr_MasVnrType(X)\n        X = self.tr_HouseStyle(X)\n        X = self.tr_Neighborhood(X)\n        X = self.tr_MSSubClass(X)\n        return X","c52c35b5":"numeric_pipe = Pipeline([('fs', dfp.feat_sel('numeric')),\n                         ('imp', dfp.df_imputer(strategy='median')),\n                         ('transf', tr_numeric())])\n\n\ncat_pipe = Pipeline([('fs', dfp.feat_sel('category')),\n                     ('imp', dfp.df_imputer(strategy='most_frequent')), \n                     ('ord', make_ordinal(['BsmtQual', 'KitchenQual', 'ExterQual', 'HeatingQC'], \n                                          extra_cols=['BsmtExposure', 'BsmtCond', 'ExterCond'],\n                                          include_extra='include')), \n                     ('recode', recode_cat()), \n                     ('dummies', dfp.dummify())])\n\n\nprocessing_pipe = dfp.FeatureUnion_df(transformer_list=[('cat', cat_pipe),\n                                                 ('num', numeric_pipe)])\n\n\nfull_pipe = Pipeline([('gen_cl', general_cleaner()), \n                      ('proc', processing_pipe), \n                      ('scaler', dfp.df_scaler()), \n                      ('dropper', drop_columns())])\n\ntmp = train_set.copy()\n\ntmp = full_pipe.fit_transform(tmp, y)  # we need to provide y too for the target encoding\n\ntmp.head()","951c5b7c":"full_pipe.get_params()","8c0f609a":"results.loc[results.model_name=='lasso', ]","fa49cd92":"lasso_pipe = Pipeline([('gen_cl', general_cleaner()),\n                       ('proc', processing_pipe),\n                       ('scaler', dfp.df_scaler()),\n                       ('dropper', drop_columns()), \n                       ('lasso', Lasso(alpha=0.01))])","48337811":"lasso_params = {'lasso__alpha': [0.05, 0.01, 0.005, 0.001], \n                'lasso__tol': [0.005, 0.001, 0.0005, 0.0001],\n                'proc__cat__dummies__drop_first': [True, False], \n                'proc__cat__recode__te_mssc': [True, False], \n                'proc__cat__recode__te_neig': [True, False],\n                'proc__cat__ord__include_extra': ['include', 'dummies'], \n                'proc__num__transf__SF_room': [True, False], \n                'proc__num__transf__bath': [True, False], \n                'proc__num__transf__bedroom': [True, False], \n                'proc__num__transf__lot': [True, False], \n                'proc__num__transf__service': [True, False],\n                'scaler__method': ['standard', 'robust'], \n                'dropper__drop': [True, False]}","f1b3cfce":"lasso_params = {'lasso__alpha': [0.002, 0.001, 0.0009, 0.0008], \n                'lasso__tol': [0.005, 0.001, 0.0005, 0.0001],\n                'proc__cat__dummies__drop_first': [True, False],\n                'proc__cat__ord__include_extra': ['include', 'dummies'], \n                'proc__num__transf__SF_room': [True, False],\n                'proc__num__transf__bedroom': [True, False], \n                'proc__num__transf__lot': [True, False],\n                'scaler__method': ['standard', 'robust']}","202c1215":"result_lasso, bp_lasso, best_lasso = grid_search(train_set, y, lasso_pipe, \n                                                 param_grid=lasso_params, cv=folds, scoring='neg_mean_squared_error', \n                                                 random=100)\n\n# todo: apparently dropping the extra columns to transform to ordinal gives problems. It has to be fixed","c0faecbd":"result_lasso.head(10)","d20bca1c":"bp_lasso","8977e679":"get_coef(best_lasso)","55023d30":"tmp = test_set.copy()\npreds = best_lasso.predict(tmp)\nprint(f'Test set RMSE: {round(np.sqrt(mean_squared_error(y_test, preds)), 4)}')\nprint(f'Test set MAE: {round(mean_absolute_error(np.expm1(y_test), np.expm1(preds)), 2)}')\nplot_predictions(test_set, y_test, preds)","af074151":"numeric_lasso = Pipeline([('fs', dfp.feat_sel('numeric')),\n                         ('imp', dfp.df_imputer(strategy='median')),\n                         ('transf', tr_numeric(lot=False, \n                                               bedroom=False, \n                                               SF_room=False))])\n\ncat_lasso = Pipeline([('fs', dfp.feat_sel('category')),\n                     ('imp', dfp.df_imputer(strategy='most_frequent')), \n                     ('ord', make_ordinal(['BsmtQual', 'KitchenQual', 'ExterQual', 'HeatingQC'], \n                                          extra_cols=['BsmtExposure', 'BsmtCond', 'ExterCond'],\n                                          include_extra='include')), \n                     ('recode', recode_cat()), \n                     ('dummies', dfp.dummify(drop_first=True))])\n\n\nprocessing_lasso = dfp.FeatureUnion_df(transformer_list=[('cat', cat_lasso),\n                                                 ('num', numeric_lasso)])\n\nlasso_pipe = Pipeline([('gen_cl', general_cleaner()),\n                       ('proc', processing_lasso),\n                       ('scaler', dfp.df_scaler(method='standard')),\n                       ('dropper', drop_columns(lasso=True)), \n                       ('lasso', Lasso(alpha=0.001, tol=0.005))])","33a3d4a3":"lasso_oof, coefs = cv_score(train_set, y, folds, lasso_pipe, imp_coef=True)\n\ncoefs","97b1d27b":"print(f'Train set RMSE: {round(np.sqrt(mean_squared_error(y, lasso_oof)), 4)}')\nprint(f'Train set MAE: {round(mean_absolute_error(np.expm1(y), np.expm1(lasso_oof)), 2)}')\nplot_predictions(train_set, y, lasso_oof)\n\ntmp = train_set.copy()\nlasso_pipe.fit(tmp, y)\ntmp = test_set.copy()\npreds = lasso_pipe.predict(tmp)\nprint(f'Test set RMSE: {round(np.sqrt(mean_squared_error(y_test, preds)), 4)}')\nprint(f'Test set MAE: {round(mean_absolute_error(np.expm1(y_test), np.expm1(preds)), 2)}')\nplot_predictions(test_set, y_test, preds)","0fbd26da":"results.loc[results.model_name=='ridge', ]","d26656c9":"ridge_pipe = Pipeline([('gen_cl', general_cleaner()),\n                       ('proc', processing_pipe),\n                       ('scaler', dfp.df_scaler()),\n                       ('dropper', drop_columns()), \n                       ('ridge', Ridge())])","d8a1b1c1":"ridge_params = {'ridge__alpha': [2, 1.7, 1.5, 1.3, 1, 0.9], \n                'ridge__tol': [0.005, 0.001, 0.0005, 0.0001],\n                'proc__cat__dummies__drop_first': [True, False],\n                'proc__cat__ord__include_extra': ['include', 'dummies'], \n                'proc__num__transf__SF_room': [True, False],\n                'proc__num__transf__lot': [True, False],\n                'scaler__method': ['standard', 'robust']}","173af1ce":"result_ridge, bp_ridge, best_ridge = grid_search(train_set, y, ridge_pipe, \n                                                 param_grid=ridge_params, cv=folds, scoring='neg_mean_squared_error', \n                                                 random=100)","d9e93ff2":"result_ridge.head(10)","770827f5":"bp_ridge","b85dcbbc":"get_coef(best_ridge)","501f34ef":"tmp = test_set.copy()\npreds = best_ridge.predict(tmp)\nprint(f'Test set RMSE: {round(np.sqrt(mean_squared_error(y_test, preds)), 4)}')\nprint(f'Test set MAE: {round(mean_absolute_error(np.expm1(y_test), np.expm1(preds)), 2)}')\nplot_predictions(test_set, y_test, preds)","f5d40f0e":"numeric_ridge = Pipeline([('fs', dfp.feat_sel('numeric')),\n                         ('imp', dfp.df_imputer(strategy='median')),\n                         ('transf', tr_numeric(SF_room=False))])\n\n\ncat_ridge = Pipeline([('fs', dfp.feat_sel('category')),\n                     ('imp', dfp.df_imputer(strategy='most_frequent')), \n                     ('ord', make_ordinal(['BsmtQual', 'KitchenQual', 'ExterQual', 'HeatingQC'], \n                                          extra_cols=['BsmtExposure', 'BsmtCond', 'ExterCond'],\n                                          include_extra='include')), \n                     ('recode', recode_cat()), \n                     ('dummies', dfp.dummify(drop_first=True))])\n\n\nprocessing_ridge = dfp.FeatureUnion_df(transformer_list=[('cat', cat_ridge),\n                                                 ('num', numeric_ridge)])\n\nridge_pipe = Pipeline([('gen_cl', general_cleaner()),\n                       ('proc', processing_ridge),\n                       ('scaler', dfp.df_scaler(method='robust')),\n                       ('dropper', drop_columns(ridge=True)), \n                       ('ridge', Ridge(alpha=2, tol=0.0001))])","440e2943":"ridge_oof, coefs = cv_score(train_set, y, folds, ridge_pipe, imp_coef=True)\n\ncoefs","41f80c64":"print(f'Train set RMSE: {round(np.sqrt(mean_squared_error(y, ridge_oof)), 4)}')\nprint(f'Train set MAE: {round(mean_absolute_error(np.expm1(y), np.expm1(ridge_oof)), 2)}')\nplot_predictions(train_set, y, ridge_oof)\n\ntmp = train_set.copy()\nridge_pipe.fit(tmp, y)\ntmp = test_set.copy()\npreds = ridge_pipe.predict(tmp)\nprint(f'Test set RMSE: {round(np.sqrt(mean_squared_error(y_test, preds)), 4)}')\nprint(f'Test set MAE: {round(mean_absolute_error(np.expm1(y_test), np.expm1(preds)), 2)}')\nplot_predictions(test_set, y_test, preds)","fed5af07":"results.loc[results.model_name=='forest', ]","1c9d4508":"forest_pipe = Pipeline([('gen_cl', general_cleaner()),\n                       ('proc', processing_pipe),\n                       ('scaler', dfp.df_scaler()),\n                       ('dropper', drop_columns()), \n                       ('forest', RandomForestRegressor(n_estimators=300, n_jobs=-1, random_state=32))])","fe3568a6":"forest_params = {'forest__max_depth': [10, 20, 30, None],\n                 'forest__max_features': ['auto', 'sqrt', 'log2'], \n                 'forest__min_samples_leaf': [1, 3, 5, 10],\n                 'forest__min_samples_split': [2, 4, 6, 8],\n                'proc__cat__dummies__drop_first': [True, False],\n                'proc__cat__ord__include_extra': ['include', 'dummies'], \n                'proc__num__transf__SF_room': [True, False], \n                'proc__num__transf__bath': [True, False], \n                'proc__num__transf__bedroom': [True, False], \n                'proc__num__transf__lot': [True, False], \n                'proc__num__transf__service': [True, False]}","b96fd1fa":"result_forest, bp_forest, best_forest = grid_search(train_set, y, forest_pipe, \n                                                 param_grid=forest_params, cv=folds, scoring='neg_mean_squared_error', \n                                                 random=100)","5361e9d9":"result_forest.head(10)","0df487dc":"bp_forest","53520d7a":"get_feature_importance(best_forest)","c05540f8":"tmp = test_set.copy()\npreds = best_forest.predict(tmp)\nprint(f'Test set RMSE: {round(np.sqrt(mean_squared_error(y_test, preds)), 4)}')\nprint(f'Test set MAE: {round(mean_absolute_error(np.expm1(y_test), np.expm1(preds)), 2)}')\nplot_predictions(test_set, y_test, preds)","ca6c01ce":"numeric_forest = Pipeline([('fs', dfp.feat_sel('numeric')),\n                         ('imp', dfp.df_imputer(strategy='median')),\n                         ('transf', tr_numeric(SF_room=False,\n                                               bedroom=False, \n                                               lot=False))])\n\n\ncat_forest = Pipeline([('fs', dfp.feat_sel('category')),\n                     ('imp', dfp.df_imputer(strategy='most_frequent')), \n                     ('ord', make_ordinal(['BsmtQual', 'KitchenQual', 'ExterQual', 'HeatingQC'], \n                                          extra_cols=['BsmtExposure', 'BsmtCond', 'ExterCond'],\n                                          include_extra='include')), \n                     ('recode', recode_cat()), \n                     ('dummies', dfp.dummify(drop_first=True))])\n\n\nprocessing_forest = dfp.FeatureUnion_df(transformer_list=[('cat', cat_forest),\n                                                 ('num', numeric_forest)])\n\nforest_pipe = Pipeline([('gen_cl', general_cleaner()),\n                       ('proc', processing_forest),\n                       ('scaler', dfp.df_scaler(method='robust')),\n                       ('dropper', drop_columns(forest=True)), \n                       ('forest', RandomForestRegressor(n_estimators=1500, max_depth=30, \n                                                        max_features='sqrt',\n                                                        n_jobs=-1, random_state=32))])","e9dd754a":"forest_oof, coefs = cv_score(train_set, y, folds, forest_pipe, imp_coef=True)\n\ncoefs","9b7e46ed":"print(f'Train set RMSE: {round(np.sqrt(mean_squared_error(y, forest_oof)), 4)}')\nprint(f'Train set MAE: {round(mean_absolute_error(np.expm1(y), np.expm1(forest_oof)), 2)}')\nplot_predictions(train_set, y, forest_oof)\n\ntmp = train_set.copy()\nforest_pipe.fit(tmp, y)\ntmp = test_set.copy()\npreds = forest_pipe.predict(tmp)\nprint(f'Test set RMSE: {round(np.sqrt(mean_squared_error(y_test, preds)), 4)}')\nprint(f'Test set MAE: {round(mean_absolute_error(np.expm1(y_test), np.expm1(preds)), 2)}')\nplot_predictions(test_set, y_test, preds)","0a78ab5b":"results[results.model_name == 'xgb']","7d32f4b1":"def xgb_train(train, target, pipe, kfolds):\n\n    oof = np.zeros(len(train))\n    pd.options.mode.chained_assignment = None \n    \n    feat_df = pd.DataFrame()\n\n    for fold_, (trn_idx, val_idx) in enumerate(kfolds.split(train.values, target.values)):\n        print(\"fold n\u00b0{}\".format(fold_))\n        \n        trn_data = train.iloc[trn_idx].copy()\n        val_data = train.iloc[val_idx].copy()\n        \n        trn_target = target.iloc[trn_idx]\n        val_target = target.iloc[val_idx]\n        \n        trn_data = pipe.fit_transform(trn_data, trn_target)\n        val_data = pipe.transform(val_data)\n    \n        clf = xgb.XGBRegressor(n_estimators=10000, objective='reg:squarederror',\n                               max_depth=3, colsample_bytree=0.5, subsample=0.5,\n                               reg_alpha=0.4, reg_lambda=0.6,\n                               learning_rate=0.01, n_jobs=-1, random_state=31).fit(\n            trn_data, trn_target,\n            eval_set=[(val_data, val_target)], \n            eval_metric='rmse', early_stopping_rounds=200, verbose=2000)\n        \n        oof[val_idx] = clf.predict(pipe.transform(train.iloc[val_idx]),\n                                   ntree_limit=clf.best_iteration)\n        \n        fold_df = pd.DataFrame()\n        fold_df[\"feat\"] = trn_data.columns\n        fold_df[\"score\"] = clf.feature_importances_     \n        fold_df['fold'] = fold_ + 1\n        feat_df = pd.concat([feat_df, fold_df], axis=0)\n       \n\n    feat_df = feat_df.groupby('feat')['score'].agg(['mean', 'std'])\n    feat_df['abs_sco'] = (abs(feat_df['mean']))\n    feat_df = feat_df.sort_values(by=['abs_sco'],ascending=False)\n    del feat_df['abs_sco']\n\n    print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof, target)**0.5))\n    pd.options.mode.chained_assignment = 'warn'\n    \n    return oof, feat_df","a4598f82":"numeric_xgb = Pipeline([('fs', dfp.feat_sel('numeric')),\n                         ('imp', dfp.df_imputer(strategy='median')),\n                         ('transf', tr_numeric(bedroom=False, \n                                               lot=False))])\n\n\ncat_xgb = Pipeline([('fs', dfp.feat_sel('category')),\n                     ('imp', dfp.df_imputer(strategy='most_frequent')), \n                     ('ord', make_ordinal(['BsmtQual', 'KitchenQual', 'ExterQual', 'HeatingQC'], \n                                          extra_cols=['BsmtExposure', 'BsmtCond', 'ExterCond'],\n                                          include_extra='include')), \n                     ('recode', recode_cat()), \n                     ('dummies', dfp.dummify(drop_first=True))])\n\n\nprocessing_xgb = dfp.FeatureUnion_df(transformer_list=[('cat', cat_xgb),\n                                                 ('num', numeric_xgb)])\n\nxgb_pipe = Pipeline([('gen_cl', general_cleaner()),\n                       ('proc', processing_xgb),\n                       ('scaler', dfp.df_scaler(method='robust')),\n                       ('dropper', drop_columns(xgb=True))])","54a84ec3":"tmp = train_set.copy()\n\noof_xgb, f_i = xgb_train(tmp, y, xgb_pipe, folds)\n\nf_i","9c9c87c7":"print(f'Train set RMSE: {round(np.sqrt(mean_squared_error(y, oof_xgb)), 4)}')\nprint(f'Train set MAE: {round(mean_absolute_error(np.expm1(y), np.expm1(oof_xgb)), 2)}')\nplot_predictions(train_set, y, oof_xgb)","0b79bd8b":"results[results.model_name == 'lgb']","6cdd279c":"def lgb_train(train, target, pipe, kfolds):\n\n    oof = np.zeros(len(train))\n    pd.options.mode.chained_assignment = None \n    \n    feat_df = pd.DataFrame()\n    \n    for fold_, (trn_idx, val_idx) in enumerate(kfolds.split(train.values, target.values)):\n        print(\"fold n\u00b0{}\".format(fold_))\n        \n        trn_data = train.iloc[trn_idx].copy()\n        val_data = train.iloc[val_idx].copy()\n        \n        trn_target = target.iloc[trn_idx]\n        val_target = target.iloc[val_idx]\n        \n        trn_data = pipe.fit_transform(trn_data, trn_target)\n        val_data = pipe.transform(val_data)\n    \n        clf = lgb.LGBMRegressor(n_estimators=20000, learning_rate=0.01,\n                                num_leaves=4, max_depth=3,\n                                subsample=0.8, colsample_bytree=0.5).fit(\n            trn_data, trn_target,\n            eval_set=[(val_data, val_target)], \n            eval_metric='rmse', early_stopping_rounds=200, verbose=1000)\n        \n        oof[val_idx] = clf.predict(pipe.transform(train.iloc[val_idx]),\n                                   ntree_limit=clf.best_iteration_)\n        \n        fold_df = pd.DataFrame()\n        fold_df[\"feat\"] = trn_data.columns\n        fold_df[\"score\"] = clf.feature_importances_    \n        fold_df['fold'] = fold_ + 1\n        feat_df = pd.concat([feat_df, fold_df], axis=0)\n       \n\n    feat_df = feat_df.groupby('feat')['score'].agg(['mean', 'std'])\n    feat_df['abs_sco'] = (abs(feat_df['mean']))\n    feat_df = feat_df.sort_values(by=['abs_sco'],ascending=False)\n    del feat_df['abs_sco']\n\n    print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof, target)**0.5))\n    pd.options.mode.chained_assignment = 'warn'\n    \n    return oof, feat_df","9b3d455d":"numeric_lgb = Pipeline([('fs', dfp.feat_sel('numeric')),\n                         ('imp', dfp.df_imputer(strategy='median')),\n                         ('transf', tr_numeric(bedroom=False,\n                                               SF_room=False,\n                                               lot=False))])\n\n\ncat_lgb = Pipeline([('fs', dfp.feat_sel('category')),\n                     ('imp', dfp.df_imputer(strategy='most_frequent')), \n                     ('ord', make_ordinal(['BsmtQual', 'KitchenQual', 'ExterQual', 'HeatingQC'], \n                                          extra_cols=['BsmtExposure', 'BsmtCond', 'ExterCond'],\n                                          include_extra='include')), \n                     ('recode', recode_cat()), \n                     ('dummies', dfp.dummify(drop_first=True))])\n\n\nprocessing_lgb = dfp.FeatureUnion_df(transformer_list=[('cat', cat_lgb),\n                                                 ('num', numeric_lgb)])\n\nlgb_pipe = Pipeline([('gen_cl', general_cleaner()),\n                       ('proc', processing_lgb),\n                       ('scaler', dfp.df_scaler(method='robust')),\n                       ('dropper', drop_columns(lgb=True))])","789718dd":"tmp = train_set.copy()\n\noof_lgb, f_i = lgb_train(tmp, y, lgb_pipe, folds)\n\nf_i","cfa21d36":"print(f'Train set RMSE: {round(np.sqrt(mean_squared_error(y, oof_lgb)), 4)}')\nprint(f'Train set MAE: {round(mean_absolute_error(np.expm1(y), np.expm1(oof_lgb)), 2)}')\nplot_predictions(train_set, y, oof_lgb)","eb5e0620":"err_an  = train_set.copy()\n\nerr_an['lasso_oof'] = lasso_oof\nerr_an['ridge_oof'] = ridge_oof\nerr_an['forest_oof'] = forest_oof\nerr_an['xgb_oof'] = oof_xgb\nerr_an['lgb_oof'] = oof_lgb\nerr_an['target'] = y\n\nerr_an.head()","cf67bc1b":"exp.plot_correlations(err_an[[col for col in err_an.columns if '_oof' in col]+['target']], \n                      target='target', annot=True)","12d7049d":"oof_cols = [col for col in err_an.columns if '_oof' in col]\n\nfor col in oof_cols:\n    name = col.replace('_oof', '_res')\n    err_an[name] = err_an['target'] - err_an[col]\n    \nexp.corr_target(err_an, 'target', \n            [col for col in err_an.columns if '_oof' in col]+\n                [col for col in err_an.columns if '_res' in col])","a3d7c7b9":"exp.plot_correlations(err_an, target='lgb_res')","d3d7f3d7":"err = exp.segm_target(err_an, 'Neighborhood', 'lgb_res')\ntar = exp.segm_target(err_an, 'Neighborhood', 'target')\n\ntot = pd.merge(err.reset_index(), tar.reset_index(), on='Neighborhood', suffixes=('_res', '_target'))\n\ndel tot['count_target']\n\ntot","7fcb250e":"tot.corr()","6172f27b":"exp.segm_target(err_an, 'Exterior1st', 'lgb_res')","4fc8918c":"err_an[err_an.Exterior1st == 'MetalSd'].describe() - err_an[err_an.Exterior1st != 'MetalSd'].describe()","ad386d00":"def high_low_errors(data, *, res_list=None, n_samples=50,\n                    target=None, pred_list=None, mean=False, \n                    abs_err=True, common=False):\n    \n    df = data.copy()\n    if pred_list:\n        res_list = []\n        for col in pred_list:\n            name = col + '_res'\n            res_list.append(name)\n            df[name] = df[target] - df[col]\n    \n    errors = {}\n    \n    if mean:\n        df['mean_res'] = df[res_list].mean(axis=1)\n        res_list += ['mean_res']\n\n    for col in res_list:\n        if abs_err:\n            if col == 'abs_err':\n                name = 'abs_err'\n            else:\n                name = 'abs_' + col\n            df[name] = abs(df[col])\n        else:\n            name = col\n        \n        high_err = df.sort_values(name, ascending=False).head(n_samples)\n        low_err = df.sort_values(name, ascending=False).tail(n_samples)\n        \n        try:\n            errors[name] = high_err.describe(include='all').drop(index=['top', 'count', 'freq']).fillna(0) - \\\n                        low_err.describe(include='all').drop(index=['top', 'count', 'freq']).fillna(0)\n        except KeyError:\n            errors[name] = high_err.describe().fillna(0) - low_err.describe().fillna(0)\n        \n    return errors","e22edb33":"h_v_l = high_low_errors(err_an, res_list=[col for col in err_an.columns if '_res' in col], mean=True)\n\nh_v_l['abs_mean_res']","a091eb4e":"imps = get_feature_importance(forest_pipe)\n\nimps.head(9)","294d9467":"features = imps.head(9).feat.values\n# we need the data to be tranformed, so I break the pipe in 2 parts\nproc = Pipeline([('gen_cl', general_cleaner()),\n                 ('proc', processing_forest),\n                 ('scaler', dfp.df_scaler(method='robust')),\n                 ('dropper', drop_columns(forest=True))])\ntmp = proc.fit_transform(train_set.copy(), y)\nls_tm = RandomForestRegressor(n_estimators=1500, max_depth=30,\n                              max_features='sqrt',\n                              n_jobs=-1, random_state=32)\nls_tm.fit(tmp, y)\n\nfig, ax = plt.subplots(3,3, figsize=(15,10))\n\nplot_partial_dependence(ls_tm, tmp, features, ax=ax,\n                        n_jobs=-1, grid_resolution=50)\n\nfig.subplots_adjust(hspace=0.3)","b9d4b58f":"features = [('OverallQual', 'service_area'), ('OverallQual', 'GrLivArea'), \n            ('OverallQual', 'Neighborhood'), ('Neighborhood', 'service_area')]\n\nfig, ax = plt.subplots(2,2, figsize=(12,12))\n\nplot_partial_dependence(ls_tm, tmp, features, ax=ax,\n                        n_jobs=-1, grid_resolution=20)\n\nfig.subplots_adjust(hspace=0.3)","42f25c86":"Which shows little to nothing. However, for categorical features, we can start focusing on the feature that was used both to stratify our folds (and test set) and then to be target encoded: Neighborhood.","d21d9b60":"Looking at the residual plots, it appears evident that all the models we trained so far are underestimating the price of low costs houses and overestimating the more expensive ones. This could be because we used some target encoding or simply that we are overestimating, for example, the importance of the house size.\n\nSince the predictions do not change much from model to model, we can simply focus on one of them. For example, let's focus on LightGBM.\n\nWe can try to see if there are interesting relations between the residuals and the original features.","59278660":"## Tune LightGBM\n\nVery similarly to the previous case, we tune some hyperparameters locally and display only the latest phase","b537662f":"This residual plot is fairly problematic and we will address it later on. \n\nWe also notice that quite a few features do not matter for our predictions","9135cef4":"Here it looks it makes a difference only in having a regular vs irregular lot shape.","aa179cc3":"Another way of looking at these features is to combine them in a plot together with another numerical feature. We know that `GrLivArea` is going to be important and our function told us that `HouseStyle` has interesting categories in it. We can combine them like this","0353abdd":"Once again, we use these information to iterate a few more times on the various configurations. Here you can see, for example, that we drop a different set of features. The decision was driven by the study of the coefficients in cross-validation. If a coefficient was varying a lot from fold to fold, we performed an iteration without that coefficient and checked if the model was improving.","7962e254":"# Error Analysis, interpretation, and model improvement\n\nWe got our first improvements, our models are already better than, for example, a simple regression using the size of the house in combination with the neighborhood.\n\nIt is a good moment to stop and think about what is our model saying and what is missing.\n\n***This section is made out of intuition, I am genuinely unaware if what follows makes sense mathematically. Any feedback on this part will be particularly appreciated***\n\nFirst, let's put our prediction together with the data that led to them.","2b2de54c":"Not as evident as before but the mean target encoding looks promising. Being a very sparse feature, we will need to be careful with this encoding as there are a few very rare features and this, as mentioned before, will lead to overfitting (especially with some algorithms).\n\n\n## Feature selection\n\nWe have seen from the previous models already that some features do not look important at all. Not only they slow down the learning, but they can even make our models learn the wrong relationships. We thus now modify the pipeline in order to get rid of these features (it also makes the processing a bit quicker, computation-wise)","f5e5dab6":"We can see how all the predictions are very correlated to one another (and with the target, this is good)","678caaa3":"We are now ready to use these parameters in our search or the optimal model.\n\n# Hyperparameter tuning\n\nThe key, and for some the novelty, of this phase is to tune both the parameters of the chosen algorithm (like the regularization in `Lasso`) and the one controlling on the creation of new features or on how to scale the data at the same time.\n\nIt is important, as before, to not fall too deep into the rabbit hole we need to be pragmatic. Most of the parameters we want to tune will have such a small influence on the model and such a high cost in terms of computation time that we have to be able to explore the sections of the parameter space that really matter.\n\nTo achieve that, it is a good practice to quickly iterate on different configurations and keep track of the effects of our choices. Even if this means coming back to the previous section and create something new. Depending on when you read this notebook, you will see different iterations of this phase (or probably the final iteration). I will try to explain as good as I can why some things were done and some other weren't.\n\nAs of now, the full pipeline (without the model) looks like this.","bcebf300":"The model has now these coefficients","25de6559":"We see that around the mean (the 0 in this graphs), there is generally a big step up in price predicted. \n\nOr, we can combine 2 features to get different insights","7d08049a":"With coefficients","c65ccb75":"Since we already have GarageCars to describe the Garage and this new feature is very correlated with the basement SF, we could consider if it is better to use it and drop the original two.\n\nAt last, for now, we can use a very powerful but dangerous encoding technique to make a better use of `Neighborhood`. We want to use *target encoding* and, as we will see, it will give us a very predictive feature. However, one must be cautious about using this kind of encoding as it easily lead to severe overfitting.\n\nTwo things can prevent the overfitting: the use of pipeline (so we are sure that the encoding happens only on the training set) and some smoothing techniques (so that there is some noise in the feature and the signal is not unrealistically clean). We will implement these features shortly but, for now, we explore the possibilities we have.","57ff9e07":"# A first look at the data and a validation strategy\n\nThe goal here is only to get an idea about the structure, the data types, the missing values. We should try to not take any decisions just yet because we did not set up any validation strategy yet. \n\nNote also that we do not load the test set, for the purposes of this notebook the test set is needed only for the final submission. Imagine the test set as the one that your client sends you a month after you delivered your model.","b69243d7":"We indeed observe a linear relation between the target and some of these features. For others it will be probably be better to use a different encoding.\n\nAt last, at least regarding what we have already implemented, we recoded a few categories following common sense. The goal here is to not train on too rare categories as this can lead to poor generalizability of the model. We report here only one of those transformations as an example","ba917a45":"As you can see, we fill the missing values either with the median or with the mode, we remove the skew from some numerical features, we make some categorical feature ordinal, and we recode some categories accordingly to what we have already seen in the data exploration phase. By also using those insights, we drop some columns to make our model simpler. A few of these columns might be useful later on but for now we can safely drop them.\n\nAt the end of this pipeline, we will put a model. We know we need a regressor, but we still don't know what model is going to be good enough for us. We thus need to implement some methods to evaluate the models. At that point, we can simply try some models and focus on the most promising one.\n\nThere is no point now to play around with hyperparameters as we still have to try to improve the quality of our data before, which will improve our model far more than a fine tuned parameter","b9d383fd":"Our effort of tuning Lasso a bit more, considering the creation of features and their selection as part of the tuning, led to an improvement of the model performance from the original 0.123 to the current 0.117. In terms of mean absolute error, we go from a little bit more than 15000 dollars to a bit less than 15000. In a real project, it is a good idea to keep track of the time and effort we put in fine tuning our model and evaluate if the project gets a significant benefit from this time and effort.\n\nIn this case, the tuning took us not more than 2 hours and a model that is about 1000 dollars more accurate is worth 2 hours of work.\n\nTo be fair, we should not compare the results obtained on the training set. Even though we are using our pipeline and k-fold cross-validation, some information is leaking from the training folds to the test folds in the form of our decisions iteration after iteration. In other words, each iteration we tweak the model to get a better score and this is a good strategy to tune it. However, we are, in a sense, *overfitting the test* folds.\n\nTherefore a proper indication of what our model is doing can only come from the test set. It is very important that we use this set only to monitor the performance and not to take further modeling decisions or we risk of overfitting that set too.\n\nOn the test set, the model improved of 0.009 in RMSE (7% better) and of about 2000 dollars (12% better).\n\n## Tuning Ridge\n\nThe strategy stays the same","d2440763":"We see, for example, that the negative skew is also present in GrLivArea, a feature that we all expect playing a big role in determining the final price. \n\nLet's try to get more insights with a bivariate analysis","d4ba33f0":"Once again, the iterations led to a 6% improvement for RMSE and to a model 1500 dollars more accurate. While the result is worse than the previous 2 models, the tuning took also very little time (almost completely run time).\n\n## Tuning XGBoost\n\nThe strategy here will be a little different as we want to make use of early stopping and tune the other hyperparameters. For some reasons, running the grid_search function above here on Kaggle, is resulting in an endless run. Luckily it is not the case locally and we can tune some of the hyperparameters following this method.\n\nHowever, the most important hyperparameters to tune are arguably the learning rate and the number of estimators. We thus implement a simple method to make use of the early stopping rounds.","378c17b7":"While the model is not performing very well, it is also very easy to tune (or, to say it better, tuning it doesn't improve it that much). The *final* version of the RandomForest is then","4e927b50":"Yes, we removed the skewness with that logarithm transformation in the previous section.\n\nLet's continue by looking at the numerical features most correlated with the target.","80d8562e":"This notebook aims to show how we could approach this competition in a realistic setting. This means in a setting where we genuinely do not have the test set and we have to deliver a model that performs reasonably well. Moreover, we want to be able to explain where our model shines and where it is not as trustworthy.\n\nThe key concepts for a realistic end-to-end project that we will explore are:\n\n* efficient validation strategy, because we will rely solely on this to chose and perfect our final model\n* use of pipelines, because we want to simply apply our final result to unseen data (the test set)\n* feature engineering and feature selection, because we want to do both quickly and efficiently\n* utility scripts are your friends, because nobody likes having so much code in the notebook.\n\nThe steps we will follow are:\n\n**Note: this notebook is still a work in progress, in bold the sections that are already there**\n\n* **data cleaning**\n* **data exploration**\n* **first models**\n* **feature engineering**\n* **hyperparameter tuning**\n* **error analysis**\n* stacking\n* model explainability\n* final submission\n\n*Note: from version 23, I needed a newer version of scikit-learn, please indulge me in the occasional execution error and remember to turn the internet on if you fork this notebook*","8bcbbdd4":"Before getting insights from the data, let's take the final step of the instructions that came with the data (i.e. the data description) and have a general cleaning","10c322f5":"As mentioned before, it is easy to go too deep in the rabbit hole in this phase. I have been writing this section for an hour already and I feel confident enough about the data to run the first models.\n\n# First models, validation, and error analysis.\n\nThis section has multiple goals, the main one is to bridge to the next section about feature engineering. We want to implement a system of evaluating, selecting, and tuning our models that is robust enough to the data processing phase. If we do this part correctly, we will be able to quickly iterate between the processing and modeling phase, being confident that we can correctly assess the validity of our actions. \n\nAs explained already in this notebook https:\/\/www.kaggle.com\/lucabasa\/understand-and-use-a-pipeline, we need different transformations for numeric and categorical features. We will use the insights gained in the previous section to make a few new custom transformers.","01620add":"We will implement a cleaning procedure that follows the documentation shortly but, before even creating the evaluation environment, we want to remove 2 outliers that the documentation recommends to remove. See this step as following the instructions that came with the data.","27efbcbe":"It looks we found something: the bigger the house, the more it costs, but the bigger the rooms (on average), the less the house is expensive.\n\nNext, we have a lot of bath features that individually are not saying too much. Let's try to make something out of them.","a85bf363":"# Feature engineering and Feature selection\n\nWe have already implemented a few transformations for our base models. For example, we have already removed the skew from some numerical features and we can see that the result is promising","fd9ca801":"In other words, houses with that particular exterior\n\n* Are less likely to have land in front of them\n* Are lower in quality, in particular they never hit the perfect score. This could be interpreted as a bias in the data collection\n* Were built less recently, which makes perfect sense as building techniques change with time\n* Have much smaller basements and garages\n* Are much smaller in general\n* Are less likely to have a fireplace\n* Cost less in general\n\nWe could then consider to include this feature as well and see how the model reacts.\n\nAnother approach would be to explore the entries with the biggest errors. For example","cfc28ae5":"The discrete values of GarageCars and OverallQual invite in using some estimator to see if the patter is clear as it looks.","44fb51de":"A few noticeable things are:\n\n* the more houses from a neighborhood, the smaller the error on average. This is also the pattern with the price, so we have to be mindful of that (neighborhood with more examples tend to have lower costs on average).\n* We can confirm that neighborhood for which we have houses with higher average cost also get a higher error (not in the absolute sense).\n\nThis makes me consider if it would be a good idea to not use the target encoding variables and see if that pattern in the error disappears.\n\nAnother possible test is to see if some variables we did not include, for example","28d867c9":"This is indeed a clearer signal and we should consider using this feature and dropping the other bath features.\n\nWe also have quite some features regarding external area (porch, Lot, Pool), but with the exception of LotArea, they are not very relevant. We thus can try to see if some interaction with the internal area is relevant.","a33fcc81":"### To be continued. Thank your reading this unfinished notebook, please feel free to ask for clarification or suggest other things I should cover in the next sections (even if most of them are already planned)","3a857ebe":"As we can see, the differences among different categories are more clear. This will definitely help the models.\n\n\n## New features and what suggested them\n\nWe will now use some insights gained during the data exploration phase to make better features.\n\nWe noticed a good correlation between TotRmsAbvGrd, GrLivArea, and BedroomAbvGr, which is nothing shocking. Let's see what an interaction between the two looks like.","ea7d9b07":"It looks like the target encoding with the mean is very powerful and not very correlated with the other features. We will use this one.\n\nThe same could be done with MSSubclass, being a very mysterious feature.","2772a5bc":"Why a class? Because we want to be able to reproduce every step on unseen data and a class makes easier to have a very clear set of steps to follow. More explanation on this and on the use of Pipelines (that will come later) can be found in this other notebook https:\/\/www.kaggle.com\/lucabasa\/understand-and-use-a-pipeline\n\nTo use this class and clean our data, we can simply do","1d4536ba":"Now, since we want to evaluate our model in order to be able to say how good or bad it can be in certain situations, we need to create our test set (not the test set provided by kaggle, that one will arrive in a month after we are done). Every insight and every decision will come from something we will do on the train set, leaving the evaluation of our choices as pure as possible.\n\nIf we were doing the data exploration phase before this step, we would have used information coming from both sets to take decisions, a luxury that in a realistic situation we won't have.\n\nHow to split the data? Giving the size of the training set, it makes sense to use a 80-20 split. A random split will do the job just fine but we can also use some knowledge of the problem. Since all that matters for a house is *location, location, location* , we can make the split in order to correctly represent the distribution of the houses across the various Neighborhoods (i.e. we can *stratify* the split).","83cc7985":"This shows how the `MetalSd` exterior leads to a particularly different pattern in the distribution of the error. A direct inspection of these houses shows the following","2d2982d3":"And the following in fold and out of fold predictions","a230d90d":"Nothing game-changing here but, with the same spirity, we can see what role what we can call service areas (Basement and Garage) are playing.","1e5d51fd":"It seems there is a very clear relation between the quality of the basement and the sale price. Moreover, it invites in recoding `BsmtQual` into an ordinal feature.\n\nThe same holds for `KitchenQual`","05eb5ea1":"The choice was driven by a combination of factors, for example:\n\n* the feature has mainly one value\n* the feature does not display any particular pattern (and we want to keep the model simple)\n* The feature should not play a role in determining the cost (this comes with the knowledge and biases of the author)\n\nThen there is a number of feature we want to create and we are not particularly sure if they will be helpful or not. For example, we have seen that some features can be safely converted into ordinal ones but others do not display an equally clear pattern and might be more useful as dummies. As another example, we created a new feature for the total number of baths and it looks promising but we are not sure if it is worth dropping all the other original features in its favor. \n\nWe can stay here all day trying to think about the right combination of transformations we need for the optimal set of features but we are getting better in the use of pipelines, it is time to exploit that.\n\nWe can design our transformers in order to have parameters that determine whether or not a feature should be transformed\/created. In this way, we can tune those parameters with GridSearch or RandomSearch and obtain the best set for every model.\n\n*Note: the code is hidden but rich of insights (or so I like to believe), feel free to expand the next cell*","f20a25b9":"Pretty cumbersome but at the bottom of it we have all the hyperparameters we want to tune. Let's tune model by model.\n\n## Tuning Lasso\n\nAs a reminder, in the model selection phase we have as a baseline for Lasso given by","2d087804":"# Stacking, blending, having fun\n\n... to be continued ....","ad1c19b0":"Or use this plot to just investigate further the features we have analyzed before","33231f31":"And, by looking at the distribution of the data, we have some odd values","a937e131":"If we compute the residuals and plot them, the pattern looks even more evident.","031f25b5":"We can start from these to see if we get some insight","76917def":"The other features found above do not show anything in particular on this kind of plots, so we won't display them. We can have a quick look at other features not found by that function, for example","e9064873":"We see that most of the missing values were actually very well explained by the documentation, we will deal with the remaining ones later on.\n\nPlease note that we don't know what is going to be missing in the future, thus, even if there were no missing values remaining in our data, we should implement procedures to clean the data anyway (or, in alternative, throw appropriate errors so that the team maintaing the model will know quickly what to do).","f9761ce5":"Nothing particularly shocking here:\n\n* Overall quality and price are very correlated\n* GarageCars and GarageArea are very correlated\n* Other fairly expected things\n\nLet's have a look at some distributions, starting with the target variable.","2b83733f":"The parameter space I want to explore, searching for the best configuration, is","d22c7cca":"Very well, we have a good combination of numerical, ordinal, and categorical features (which is also why this is a good competition to start with). We also have quite a few missing values (most of which are explained in the data description).","35bbfbde":"Moreover, we have already transformed some categorical features into ordinal ones","ca14fcf8":"To come back to the spirit of the Kaggle competition, this result is extremely close to the one that you obtain in public leaderboard if you simply predict on the provided test set. This would approximatevely score in the top 34% and, if we retrain on the full test, top 31%. This is not very competitive but the good news is that we can focus on increase this cross validated result to also obtain a better score in the competition. \n\nAfter a few iterations, we get rid of the features that are not helpful and with the *final* pipeline for Lasso","891bee08":"After this search, the best configuration is given by","0f847f94":"All the information I want to get from these plots are about odd distributions, not only I see there is a skew in some continuous variables, but also that some features have mostly one value.\n\nOne way to remove the skeweness is to simply take the logarithm of the variable. This is what we are going to do now with the target variable","f26ea229":"We will continue with the top5: Lasso, RandomForest, XGBoost, LGBoost, Ridge. We will have these results as baseline, being them the one we obtained by simply looking at the data.\n\nWe can investigate each of these models by looking at their predictions and at how those predictions are made. For example","be2c56a2":"The model has then these coefficients","5fe027fd":"This time our 2 hours of work (I am considering all the iterations made up until this moment) led to a 3% improvement in RMSE and to a model about 1000 dollars more accurate.\n\n## Tuning RandomForest\n\nThe scaler should not matter as in general scaling the data for a tree-based algorithm will only increase the speed and not the outcome.","36c009b3":"Yes, it is very clear indeed. Let's move on to the categorical features.\n\n## Categorical features\n\nIn this dataset we don't have many features and, thanks to the function in the `explore_data` utility script, we can quickly shuffle through all of them to find the interesting one. If this was not possible, one approach would be to find the features whose cateogories exhibit a significantly different distribution of the target variable. In other words, for example, if the distribution of prices for houses withouth a Fence is significantly different than the ones with a Fence, the next function will catch it and tell us.","0299ced2":"* target encoding always helps","ccd0de31":"While the performance on our test set (which should not influence our hyperparameter tuning but it is nice to keep track of its evolution) is","fbaf168d":"# Get insights from the data\n\nThis phase is tricky because it is easy to go too deep into the rabbit hole. We want to have a comprehensive understanding of the data but at the same time we don't want to spend too much time on it. While in this dataset the number of features is fairly limited, in other cases a slow approach can cost us weeks of work and, at this stage, we don't know if this work is worth weeks of our time.\n\nWe will thus focus on the following:\n\n* Correlation with the target\n* Univariate and bivariate analysis of the interesting features\n* Interesting segmentations with categorical features\n\nOnce again, please note how we don't use the full training set but only what we have created in the previous section.\n\n## Numerical features\n\nTo first identify what can be interesting, we can look the correlations with the target. This is not by any means enough or necessary to make a feature interesting but, in absence of other knowledge, it is a good first step.","4356ed78":"* Alley is a low cardinality feature, that difference might be interesting\n* High errors have bigger LotFrontage but much smaller LotArea\n* Low errors are built more recently\n* High errors have a bigger basement but also more unfinished\n* High errors are much bigger in general\n* It appears we are not capturing the MiscVal\n* The high are negative on average, meaning that they overestimate the price.\n* Low errors are coming from more expensive houses.\n\n## Interpreting the results\n\nWe may have to explain to the stakeholders of the project what is our model doing to make its prediction. This is a very complicated question when we step away from the simplest models (that, to be honest, are performing well enough if we compare them to the more complex ones here presented). However, some simple things might just do the job.\n\nLet's focus on our RandomForest, we know already how off the predictions are and, if we don't want to scroll up this notebook, we can see what features are the most important.","5c9731b0":"These are 32768 different configurations to test on 5 folds. Even though Lasso is pretty quick and the data are not big, it can take quite a long time.\n\nAfter a few iterations, and this is the last time I put all the steps of the process or this notebook becomes unbearable, we can restrict our search a bit. Namely,\n\n* All the target encoding is giving higher score\n* Dropping the variables at the end is never helpful\n* The alpha parameter has to be searched on a different range\n* The service area does not help\n* total_bath always helps\n\nThese are still a lot of configurations. We will thus test only 200 random combinations","984a81c7":"* the bedroom proportion is never helping\n* dropping variables never helps\n* target encoding always help","0aa723bd":"Let's then create a pipeline for this model","8d147886":"And we see that the proportion of houses per Neighborhood is approximatively preserved","00111422":"Or of `ExterQual`"}}