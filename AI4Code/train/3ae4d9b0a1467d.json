{"cell_type":{"dfe747ac":"code","e2d3ad71":"code","86bef46e":"code","02e2ae65":"code","8587b178":"code","8759f84b":"code","4dab757f":"code","1cf6c5e8":"code","7606fcc0":"code","0eabcf56":"code","bdf372c1":"code","e0f6daa1":"code","8dd1fae4":"code","e7235af3":"code","1ea691d8":"code","d0d8adf9":"code","5a2eb128":"code","d9441911":"code","7b709ca7":"code","28c8b25b":"code","fb8e8813":"code","c5f4c31a":"code","6c6ef734":"code","be669a0f":"code","1e1730fe":"code","33f296ff":"code","f32c8838":"code","cf79f7d3":"code","348c187f":"markdown","990a23c2":"markdown","d7ac2955":"markdown","b8355109":"markdown","3dadf85e":"markdown","8f21bfba":"markdown","5c6e5f1f":"markdown","8dbe9c39":"markdown","b42a25d5":"markdown"},"source":{"dfe747ac":"!pip install -U torch torchvision;","e2d3ad71":"import os\nfrom tqdm.autonotebook import tqdm, trange\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\n\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\n\nfrom sklearn.model_selection import train_test_split\nfrom pathlib import Path\nfrom torch.utils.data import DataLoader\n\nfrom torch.utils.data import Dataset\nfrom sklearn.preprocessing import LabelEncoder\nimport pickle\nfrom PIL import Image","86bef46e":"import random \nSEED = 7\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","02e2ae65":"TRAIN_DIR = Path('..\/input\/journey-springfield\/train')\nTEST_DIR = Path('..\/input\/journey-springfield\/testset')\n\n#\u044f \u0440\u0430\u0431\u043e\u0442\u0430\u044e \u0432 kaggle-\u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0435\n\nDATA_MODES = ['train', 'val', 'test']\nRESCALE_SIZE = 224\nDEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nCUDA_LAUNCH_BLOCKING=1\nBATCH_SIZE=128\n\nprint(DEVICE)","8587b178":"class SimpsonsDataset(Dataset):\n\n    def __init__(self, files, mode):\n        super().__init__()\n        self.files = sorted(files)\n        self.mode = mode\n\n        if self.mode not in DATA_MODES:\n            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n            raise NameError\n\n        self.len_ = len(self.files)\n     \n        self.label_encoder = LabelEncoder()\n\n        if self.mode != 'test':\n            self.labels = [path.parent.name for path in self.files]\n            self.label_encoder.fit(self.labels)\n\n            with open('label_encoder.pkl', 'wb') as le_dump_file:\n                  pickle.dump(self.label_encoder, le_dump_file)\n                      \n    def __len__(self):\n        return self.len_\n      \n    def load_sample(self, file):\n        image = Image.open(file)\n        image.load()\n        return image\n  \n    def __getitem__(self, index):\n        transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n        ])\n        data_transforms = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomAutocontrast(p=0.5),])\n        x = self.load_sample(self.files[index])\n        x = self._prepare_sample(x)\n        x = np.array(x \/ 255, dtype='float32')\n        if self.mode == 'train':\n          x = data_transforms(x)\n        else:\n          x = transform(x)\n        if self.mode == 'test':\n            return x\n        else:\n            label = self.labels[index]\n            label_id = self.label_encoder.transform([label])\n            y = label_id.item()\n            return x, y\n        \n    def _prepare_sample(self, image):\n        image = image.resize((RESCALE_SIZE, RESCALE_SIZE))\n        return np.array(image)","8759f84b":"train_val_files = sorted(list(TRAIN_DIR.rglob('*.jpg')))\ntest_files = sorted(list(TEST_DIR.rglob('*.jpg')))\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_val_labels = [path.parent.name for path in train_val_files]\ntrain_files, val_files = train_test_split(train_val_files, test_size=0.25, \\\n                                          stratify=train_val_labels)","4dab757f":"n_classes = len(np.unique(train_val_labels))\nn_classes","1cf6c5e8":"train_labels = [path.parent.name for path in train_files] # \u043a\u043b\u0430\u0441\u0441\u044b train\nval_labels = [path.parent.name for path in val_files]     # \u043a\u043b\u0430\u0441\u0441\u044b val","7606fcc0":"def create_dct_path_labels(train_files, train_labels):\n    dct_simpsons = {}\n    for label_i in np.unique(train_labels).tolist():\n        dct_simpsons[label_i] = []\n\n    for path_i, label_i in zip(train_files, train_labels):\n        dct_simpsons[label_i].append(path_i)\n\n    return dct_simpsons\n\ndef print_dct(dct_simpsons):\n    for key in dct_simpsons:\n        print(f\"{key}\\t{dct_simpsons[key]}\")","0eabcf56":"dct_path_train = create_dct_path_labels(train_files, train_labels)","bdf372c1":"# \u0414\u043e\u043f\u043e\u043b\u043d\u0438\u043c \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0443 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043c\u0435\u043d\u0435\u0435 100 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a, \u0434\u043e 100 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u0432 \u043a\u043b\u0430\u0441\u0441\u0435\nfor person in dct_path_train:\n    if len(dct_path_train[person]) < 100:\n        dct_path_train[person] = dct_path_train[person] * (100 \/\/ len(dct_path_train[person]))\n        dct_path_train[person].extend(dct_path_train[person][:100 - len(dct_path_train[person])])\n\n# \u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u0447\u0442\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u043e\u0441\u044c \nfor person in dct_path_train:\n    print(f\"{person}\\t{len(dct_path_train[person])}\")","e0f6daa1":"def create_dct_from_labels(train_val_labels):\n\n    dct_simpsons = {}\n    for label_i in np.unique(train_val_labels).tolist():\n        dct_simpsons.update({label_i:train_val_labels.count(label_i)})\n\n    return dct_simpsons\n\nnew_train_files = []\n\nfor person in dct_path_train:\n    new_train_files.extend(dct_path_train[person])\n\nnew_train_label = [path.parent.name for path in new_train_files]","8dd1fae4":"val_dataset = SimpsonsDataset(val_files, mode='val')\nnew_train_dataset = SimpsonsDataset(new_train_files, mode='train')","e7235af3":"def fit_epoch(model, train_dataloader, criterion, optimizer):\n    running_loss = 0.0\n    running_corrects = 0\n    processed_data = 0\n  \n    for inputs, labels in train_dataloader:\n        inputs = inputs.to(DEVICE) #\u043f\u0435\u0440\u0435\u043d\u043e\u0441 \u0442\u0435\u043d\u0437\u043e\u0440\u043e\u0432 \u043d\u0430 \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442\u0443\n        labels = labels.to(DEVICE)\n        optimizer.zero_grad() #\u043e\u0431\u043d\u0443\u043b\u044f\u0435\u043c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u044b, \u0447\u0442\u043e\u0431\u044b \u043e\u043d\u0438 \u043d\u0435 \u043d\u0430\u043a\u0430\u043f\u043b\u0438\u0432\u0430\u043b\u0438\u0441\u044c\n\n        outputs = model(inputs) #\u043f\u0440\u043e\u0433\u043e\u043d\u044f\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0438\u0437 \u0442\u0440\u0435\u0439\u043d\u043b\u043e\u0430\u0434\u0435\u0440\u0430 \u0447\u0435\u0435\u0437 \u043d\u0430\u0448\u0443 \u0441\u0435\u0442\u044c (\u043c\u043e\u0434\u0435\u043b\u044c). \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u043e, \u043f\u043e\u0447\u0435\u043c\u0443 \u043d\u0435 model.forward(inputs)?\n        loss = criterion(outputs, labels) #\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u043b\u043e\u0441\u0441\n        loss.backward()\n        optimizer.step()\n        preds = torch.argmax(outputs, 1) #\u0432\u0441\u0435, \u0447\u0442\u043e \u0431\u043e\u043b\u044c\u0448\u0435 1, \u043f\u0440\u0438\u0432\u043e\u0434\u0438\u043c \u043a 1\n        running_loss += loss.item() * inputs.size(0) #\u043d\u0435\u043f\u043e\u043d\u044f\u0442\u043d\u043e, \u043a\u0430\u043a \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 .size(0)\n        running_corrects += torch.sum(preds == labels.data) #\u0435\u0441\u043b\u0438 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u0441\u043e\u0432\u043f\u0430\u0434\u0430\u0435\u0442 \u0441 \u043e\u0442\u0432\u0435\u0442\u043e\u043c, \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0432\u0430\u0435\u043c \u043d\u0430 1 running_corrects\n        processed_data += inputs.size(0) \n              \n    train_loss = running_loss \/ processed_data #\u043f\u043e\u0442\u0435\u0440\u0438 \u043d\u0430 \u0442\u0440\u0435\u0439\u043d\u0435\n    train_acc = running_corrects.cpu().numpy() \/ processed_data #\u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043d\u0430 \u0442\u0440\u0435\u0439\u043d\u0435\n    return train_loss, train_acc","1ea691d8":"def eval_epoch(model, val_dataloader, criterion):\n    model.eval()\n    running_loss = 0.0\n    running_corrects = 0\n    processed_size = 0\n\n    for inputs, labels in val_dataloader:\n        inputs = inputs.to(DEVICE)\n        labels = labels.to(DEVICE)\n\n        with torch.set_grad_enabled(False):\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            preds = torch.argmax(outputs, 1)\n\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n        processed_size += inputs.size(0)\n    val_loss = running_loss \/ processed_size\n    val_acc = running_corrects.double() \/ processed_size\n    return val_loss, val_acc","d0d8adf9":"def train(train_files, val_files, model, epochs, batch_size):\n    train_dataloader = DataLoader(new_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n    history = []\n    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n\n    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n       \n        opt = torch.optim.Adam(model.parameters(), lr=3e-4)\n        criterion = nn.CrossEntropyLoss()\n\n        for epoch in range(epochs):\n            train_loss, train_acc = fit_epoch(model, train_dataloader, criterion, opt)\n            print(\"loss\", train_loss)\n            \n            val_loss, val_acc = eval_epoch(model, val_dataloader, criterion)\n            history.append((train_loss, train_acc, val_loss, val_acc))\n            \n            pbar_outer.update(1)\n            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n            \n    return history","5a2eb128":"model = models.alexnet(pretrained=True)","d9441911":"layers_to_unfreeze = 5\n\nfor param in model.features[:-layers_to_unfreeze].parameters():\n    param.requires_grad = False\n    \nnum_features = 9216\nmodel.classifier = nn.Linear(9216, n_classes)\n\nmodel = model.to(DEVICE)\n","7b709ca7":"history = train(new_train_dataset, val_dataset, model=model, epochs=8, batch_size=BATCH_SIZE)","28c8b25b":"torch.save(model.state_dict(), 'Simpsons_AlexNet.pth')\nmodel.load_state_dict(torch.load('Simpsons_AlexNet.pth'))","fb8e8813":"loss, acc, val_loss, val_acc = zip(*history)","c5f4c31a":"plt.figure(figsize=(10, 6))\nplt.plot(loss, label=\"train_loss\")\nplt.plot(val_loss, label=\"val_loss\")\nplt.legend(loc='best')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.show()","6c6ef734":"plt.figure(figsize=(10, 6))\nplt.plot(acc, label=\"train_acc\")\nplt.plot(val_acc, label=\"val_acc\")\nplt.legend(loc='best')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"acc\")\nplt.show()","be669a0f":"def predict(model, test_loader):\n    with torch.no_grad(): #\u0431\u0435\u0437 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043e\u0432\n        logits = []\n    \n        for inputs in test_loader:\n            inputs = inputs.to(DEVICE)\n            model.eval()\n            outputs = model(inputs).cpu()\n            logits.append(outputs)\n            \n    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n    return probs","1e1730fe":"def predict_one_sample(model, inputs, device=DEVICE):\n    \"\"\"\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435, \u0434\u043b\u044f \u043e\u0434\u043d\u043e\u0439 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438\"\"\"\n    with torch.no_grad():\n        inputs = inputs.to(device)\n        model.eval()\n        logit = model(inputs).cpu()\n        probs = torch.nn.functional.softmax(logit, dim=-1).numpy()\n    return probs","33f296ff":"random_characters = int(np.random.uniform(0,1000))\nex_img, true_label = val_dataset[random_characters]\nprobs_im = predict_one_sample(model, ex_img.unsqueeze(0))\n\nidxs = list(map(int, np.random.uniform(0,1000, 20)))\nimgs = [val_dataset[id][0].unsqueeze(0) for id in idxs]\n\nprobs_ims = predict(model, imgs)\n\nlabel_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))\n\ny_pred = np.argmax(probs_ims,-1)\n\nactual_labels = [val_dataset[id][1] for id in idxs]\n\npreds_class = [label_encoder.classes_[i] for i in y_pred]","f32c8838":"test_dataset = SimpsonsDataset(test_files, mode=\"test\")\ntest_loader = DataLoader(test_dataset, shuffle=False, batch_size=64)\nprobs = predict(model, test_loader) #\u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u0441 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044f\u043c\u0438 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430\n\npreds = label_encoder.inverse_transform(np.argmax(probs, axis=1)) #\u0432\u0435\u043a\u0442\u043e\u0440 \u0432 \u043d\u0430\u0438\u0431\u043e\u043b\u044c\u0448\u0438\u043c\u0438 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044f\u043c\u0438\ntest_filenames = [path.name for path in test_dataset.files]","cf79f7d3":"# \u0414\u041e\u0411\u0410\u0412\u041b\u0415\u041d\u041e: \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0441\u0430\u0431\u043c\u0438\u0442\u0430\nimport pandas as pd\ndf = pd.DataFrame()\ndf['Id'] = test_filenames\ndf['Expected'] = preds\ndf.to_csv(Path('.\/submission_.csv'), index=False)\n# \u0444\u0430\u0439\u043b \u043f\u043e\u044f\u0432\u0438\u0442\u0441\u044f \u0443 \u0432\u0430\u0441 \u043d\u0430 \u0433\u0443\u0433\u043b \u0434\u0438\u0441\u043a\u0435","348c187f":"# \u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0441\u043a\u0430\u044f \u0421.\u0410. - 11.21","990a23c2":"# \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u043d\u0430 \u043a\u0430\u0433\u043b\u0435 0.96599","d7ac2955":"## 4. \u0424\u0443\u043d\u043a\u0446\u0438\u0438 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 (\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044b \u0431\u0435\u0437 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0439)","b8355109":"## 5. Transfer learning \u0441 \u0447\u0430\u0441\u0442\u0438\u0447\u043d\u043e\u0439 \u0437\u0430\u043c\u043e\u0440\u043e\u0437\u043a\u043e\u0439 \u0441\u043b\u043e\u0435\u0432 \u043d\u0430 \u0441\u0435\u0442\u0438 AlexNet","3dadf85e":"\u0422\u0430\u043a\u0436\u0435 \u0431\u044b\u043b\u0438 \u043f\u0440\u043e\u0432\u0435\u0434\u0435\u043d\u044b \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u044b:\n1. \u041f\u0440\u043e\u0441\u0442\u0430\u044f \u0441\u0435\u0442\u044c CNN \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c BatchNorm, Dropout. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 0.92561\n2. AlexNet \u0441 \u0437\u0430\u043c\u043e\u0440\u043e\u0437\u043a\u043e\u0439 \u0432\u0441\u0435\u0445 \u0441\u043b\u043e\u0435\u0432. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 0.83740\n3. Inception \u0441 \u0437\u0430\u043c\u043e\u0440\u043e\u0437\u043a\u043e\u0439 \u0432\u0441\u0435\u0445 \u0441\u043b\u043e\u0435\u0432. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 0.65143\n4. Inception \u0431\u0435\u0437 \u0437\u0430\u043c\u043e\u0440\u043e\u0437\u043a\u0438 \u0441\u043b\u043e\u0435\u0432. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 0.95430","8f21bfba":"## 6. \u041f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f","5c6e5f1f":"## 2. \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445. \u041f\u0440\u043e\u0432\u0435\u0434\u0435\u043d\u0430 \u0431\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u043e\u0432\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 (\u043a\u043e\u0434 \u0443\u043a\u0440\u0430\u0434\u0435\u043d \u0438\u0437 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u043e\u0432 \u043a \u0437\u0430\u0434\u0430\u0447\u0435). \u0412 \u043a\u043b\u0430\u0441\u0441\u0430\u0445, \u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u043c\u0435\u043d\u044c\u0448\u0435 100, \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u043f\u0440\u043e\u0441\u0442\u043e \u0434\u0443\u0431\u043b\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u0434\u043e 100. \u041f\u0440\u0438 \u044d\u0442\u043e\u043c \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u0434\u043e\u043b\u0436\u043d\u0430 \u043f\u043e\u043c\u043e\u0447\u044c \u0441 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\u043c.","8dbe9c39":"## 0. \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438, \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0435 \u0447\u0438\u0441\u043b\u0430, \u0437\u0430\u0434\u0430\u0435\u043c \u043a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\u044b","b42a25d5":"## 1. \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043a\u043b\u0430\u0441\u0441 \u0434\u043b\u044f \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430. \u0414\u043e\u0431\u0430\u0432\u0438\u043b\u0430 \u0432 \u043d\u0435\u0433\u043e \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0434\u043b\u044f \u0440\u0435\u0436\u0438\u043c\u0430 'train'."}}