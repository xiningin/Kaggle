{"cell_type":{"e5784eb3":"code","cf154cba":"code","1553ab4d":"code","5fb5e6f6":"code","1982f0b7":"code","7fbadfca":"code","e8667852":"code","f0009cef":"code","d8e51998":"code","66e9ddd4":"code","3959a518":"code","220309fd":"code","d12fff59":"code","1202aad3":"code","4ce9ef61":"code","6c16910f":"code","4716f7d9":"code","9589ceb2":"code","1198d49f":"code","4a662b29":"code","bccb2ca3":"code","1d2600b5":"markdown","6f4d4930":"markdown","6f74588e":"markdown","a9256b33":"markdown","89f2721c":"markdown","2ec7c7c4":"markdown","4d8ec505":"markdown","08fadaa2":"markdown","b613ffd8":"markdown","3913ca52":"markdown","8c6b542b":"markdown","62ef3c0e":"markdown","41cdea65":"markdown","1e92911e":"markdown","b18bf5f7":"markdown","e0d8c1f1":"markdown","e18d88e3":"markdown","4cb2be50":"markdown","c5825a93":"markdown","774e729b":"markdown","0f79285a":"markdown","772cbac5":"markdown"},"source":{"e5784eb3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","cf154cba":"dataset = pd.read_csv('\/kaggle\/input\/startup-logistic-regression\/50_Startups.csv')\n","1553ab4d":"dataset.info()","5fb5e6f6":"dataset.describe()","1982f0b7":"#scatterplot\nsns.set()\ncols = ['Profit', 'R&D Spend', 'Marketing Spend', 'Administration']\nsns.pairplot(dataset[cols], height = 2.5)\nplt.show();","7fbadfca":"%matplotlib inline\nplt.figure(figsize=(12,10))\ncor = dataset.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()\n","e8667852":"corr_Profit=cor[\"Profit\"].sort_values(ascending=False)\nprint(corr_Profit)","f0009cef":"Q1 = dataset.quantile(0.25)\nQ3 = dataset.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","d8e51998":"print(dataset < (Q1 - 1.5 * IQR)) |(dataset > (Q3 + 1.5 * IQR))","66e9ddd4":"dataset_outl = dataset[~((dataset < (Q1 - 1.5 * IQR)) |(dataset > (Q3 + 1.5 * IQR))).any(axis=1)]\ndataset_outl.shape","3959a518":"#Normality\n#histogram and normal probability plot\nfrom scipy.stats import norm\nfrom scipy import stats\nsns.distplot(dataset['Profit'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(dataset['Profit'], plot=plt)","220309fd":"# splitting the dataset into  independent variable X and dependent(target) y\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, 4].values","d12fff59":"# Encoding categorical data\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\n \nct = ColumnTransformer([('encoder', OneHotEncoder(),[3])], remainder='passthrough')\n \nX = np.array(ct.fit_transform(X), dtype=np.float)\n","1202aad3":"X = X[:, 1:]","4ce9ef61":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n# drop the 1 column in X\nRFE_regressor = LinearRegression()\n#Initializing RFE model\nrfe = RFE(RFE_regressor, 2)# random number(2)\n#Transforming data using RFE\nX_rfe = rfe.fit_transform(X,y)  \n#Fitting the data to model\nRFE_regressor.fit(X,y)\nprint(rfe.support_)\nprint(rfe.ranking_)","6c16910f":"#-------before calc high score it says all true ----\n\nfrom sklearn.model_selection  import train_test_split\nfrom sklearn.linear_model import LinearRegression\n#no of features\nnof_list=np.arange(1,4)            \nhigh_score=0\n#Variable to store the optimum features\nnof=0           \nscore_list =[]\nfor n in range(len(nof_list)):\n    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\n    model = LinearRegression()\n    rfe = RFE(model,nof_list[n])\n    X_train_rfe = rfe.fit_transform(X_train,y_train)\n    X_test_rfe = rfe.transform(X_test)\n    model.fit(X_train_rfe,y_train)\n    score = model.score(X_test_rfe,y_test)\n    score_list.append(score)\n    if(score>high_score):\n        high_score = score\n        nof = nof_list[n]\nprint(\"Optimum number of features: %d\" %nof)\nprint(\"Score with %d features: %f\" % (nof, high_score))\n","4716f7d9":"#Initializing RFE model\nrfe = RFE(RFE_regressor, 3)\n#Transforming data using RFE\nX_rfe = rfe.fit_transform(X,y)  \n#Fitting the data to model\nRFE_regressor.fit(X,y)\nprint(rfe.support_)\nprint(rfe.ranking_)\n","9589ceb2":"RFE_features=X[:,[0,1,2]] # using the features with only True values","1198d49f":"# Fitting Multiple Linear Regression to the Training set\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)","4a662b29":"print(\"score: \",regressor.score(X_train,y_train))\nprint(\"Model slope:    \", regressor.coef_)\nprint(\"Model intercept:\", regressor.intercept_)","bccb2ca3":"# Predicting the Test set results\ny_pred = regressor.predict(X_test)\nprint(\"score: \",regressor.score(X_test,y_test))\nprint(\"Model slope:    \", regressor.coef_)\nprint(\"Model intercept:\", regressor.intercept_)","1d2600b5":"# Predict","6f4d4930":"There are different wrapper methods: \n* Backward Elimination\n* Forward Selection \n* Bidirectional Elimination \n* RFE.\n\nHere I am applying the RFE for selecting the most correlated features with target varaible\nRFE (Recursive Feature Elimination)\nIt then gives the ranking of all the variables, 1 being most important. It also gives its support, True being relevant feature and False being irrelevant feature.\nHere we took LinearRegression model with 2 features and the selection of number \u20182\u2019 was random,Lets find the ranking for features","6f74588e":"\nThe correlation coefficient ranges from \u20131 to 1. When it is close to 1, it means that there is a strong positive correlation; for example,the Profit tends to go up when R&D Spend goes up. When the coefficient is close to \u20131, it means that there is a strong negative correlation;  you can see the Administration is less correlated than other with Profit. Finally, coefficients close to zero mean that there is no linear correlation.","a9256b33":" This is the minimum, first quartile, median, third quartile, and maximum. So, the minimum is our smallest value. The first quartile often called the 25th percentile is where 25 percent of the data falls below that value. The median which is sometimes called the 50th percentile is where 50 percent falls below that value and the third quartile often called the 75th percentile is where 75 percent falls below that value and then finally, we have our maximum value","89f2721c":"## Deleting the Outliers","2ec7c7c4":"An Outlier is the data that does not fit with other data in dataset ,like error while data correction or ture outliers i.e the data in out range.Outliers can be found using different methods:\n* Box plot\n* Z-Score\n* IQRS Score","4d8ec505":"# Importing the dataset","08fadaa2":"\n## A histogram for each numerical attribute\n","b613ffd8":" Now we need to find the optimum number of features, for which the accuracy is the highest. We do that by using loop starting with 1 feature and going up to 4. We then take the one for which the accuracy is highest.","3913ca52":"# Checking for Outliers","8c6b542b":"## Finding, Removing Outliers using IQR Score","62ef3c0e":"# Correlation Matrix","41cdea65":"# Feature Selection\n You need not use every feature in your model. You can feed only important feature,Now lets see how to figure out the important one. As we encoded our data in last step we got only numerical data and there are serveral methods for Numeric feature selection\nThere are many methods to do Feature selection:\n* Filter Method\n* wrapper methods\n* Embedded Method","1e92911e":"\n## Avoiding the Dummy Variable Trap\n","b18bf5f7":" The interquartile range (IQR), also called the midspread or middle 50%, or technically H-spread, is a measure of statistical dispersion, being equal to the difference between 75th and 25th percentiles, or between upper and lower quartiles, IQR = Q3 \u2212 Q1.","e0d8c1f1":"The data printed below has two values False and True whereas True Indicates the Presence of the Outliers and False indicates the absence","e18d88e3":"# Checking the target variable","4cb2be50":"# Fitting the model","c5825a93":"\n There are 50 instances\/rows in the dataset which means that it is fairly small when compared to real world datasets in machine learning. Notice that there are 4 columns has datatype float64(Numnerical Attribute) and 1 column has obiect type(Categorical Attribute). Lets first check the numerical Attributes","774e729b":"#Introduction\n  \nMultiple linear Regression is to examine how multiple independent variables are related to a dependent variable.In this notebook i am concerned about Feature Selection. It is the important step which impact the model and its performance. if we put all the variable into the model even the best model will perform worst as Garbage In Garbage Out.So we need to select the most relevant features and drop the irrelevant features\n","0f79285a":"The optimum number of features is 3 .lets apply back into Linear Regression with 3 feature to get the ranking","772cbac5":"## Preparing the Data for Modeling\n The input features which we pass to the model should contain only the numerical data which the model perfomance and accuracy, so we need to convert the categorical varaible state ito the Numerical attribure by using one-hot encoding and column transformer\n"}}