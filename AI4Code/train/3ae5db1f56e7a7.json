{"cell_type":{"4709d6ab":"code","e7475041":"code","ae8b2a15":"code","5d1fe062":"code","bd36a276":"code","a860638a":"code","59911a56":"code","a9525785":"code","7a062eb7":"code","eaba5872":"code","09ebdd24":"code","25c072a7":"code","17ba5d10":"code","3a13bf49":"code","904e3fc9":"code","dd86f05e":"code","ca002aba":"code","c1f0224b":"code","a901aad7":"code","47ecfa5d":"code","ec77c734":"code","fda1c196":"code","865e6529":"code","b3b1a051":"code","cf60453d":"code","32dcb601":"code","69f13d99":"markdown","3f1de70f":"markdown","4aa4f7db":"markdown","99ed1119":"markdown","38942b5e":"markdown","c76cdaa3":"markdown","9bd3190d":"markdown","9084cb13":"markdown","a6ca5233":"markdown","6cfe6641":"markdown","27d5e5fe":"markdown","e4d183b9":"markdown","cbaf8830":"markdown","a8bf1d80":"markdown","f780b1f1":"markdown"},"source":{"4709d6ab":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.datasets.fashion_mnist import load_data\nfrom keras.models import Model\nfrom keras.layers import Flatten, Dense, Dropout, BatchNormalization, Input, Conv2D, MaxPooling2D\nfrom keras.optimizers import Adam\nfrom keras.utils import plot_model\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\n\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)","e7475041":"(X_train_full, y_train_full), (X_test, y_test) = load_data()","ae8b2a15":"print('X_train_full shape:', X_train_full.shape)\nprint('y_train_full shape:', y_train_full.shape)\nprint('X_test shape:', X_test.shape)\nprint('y_test shape:', y_test.shape)","5d1fe062":"X_train, X_valid = X_train_full[:-5000] \/ 255., X_train_full[-5000:] \/ 255.\ny_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\nX_test = X_test \/ 255.","bd36a276":"X_train = X_train[..., np.newaxis]\nX_valid = X_valid[..., np.newaxis]\nX_test = X_test[..., np.newaxis]","a860638a":"print('X_train shape:', X_train.shape)\nprint('y_train shape:', y_train.shape)\nprint('X_valid shape:', X_valid.shape)\nprint('y_valid shape:', y_valid.shape)","59911a56":"class_names = [\"T-shirt\/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]","a9525785":"print(class_names[y_train[0]])\nplt.imshow(X_train[0].squeeze(), cmap='binary')\nplt.axis('off')\nplt.show()","7a062eb7":"n_rows = 5\nn_cols = 10\nplt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\nfor row in range(n_rows):\n    for col in range(n_cols):\n        index = n_cols * row + col\n        plt.subplot(n_rows, n_cols, index + 1)\n        plt.imshow(X_train[index].squeeze(), cmap=\"binary\", interpolation=\"nearest\")\n        plt.axis('off')\n        plt.title(class_names[y_train[index]], fontsize=12)\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\nplt.show()","eaba5872":"EPOCHS = 30\nBATCH_SIZE = 32\nLEARNING_RATE = 0.0001","09ebdd24":"def FashionNet(input_shape=(28, 28, 1)):\n\n    input = Input(shape=input_shape)\n\n    layer1 = Conv2D(filters=64, kernel_size=3, padding='SAME', activation='relu')(input)\n\n    layer2 = Conv2D(filters=64, kernel_size=3, padding='SAME', activation='relu')(layer1)\n    layer2 = MaxPooling2D(pool_size=2)(layer2)\n    layer2 = Dropout(0.2)(layer2)\n    layer2 = BatchNormalization()(layer2)\n\n    layer3 = Conv2D(128, 3, activation='relu', padding='SAME')(layer2)\n\n    layer4 = Conv2D(128, 3, padding='SAME', activation='relu')(layer3)\n    layer4 = MaxPooling2D(pool_size=2)(layer4)\n    layer4 = Dropout(0.2)(layer4)\n    layer4 = BatchNormalization()(layer4)\n\n    layer5 = Conv2D(256, 3, activation='relu', padding='SAME')(layer4)\n    layer5 = MaxPooling2D(2)(layer5)\n    layer5 = Dropout(0.2)(layer5)\n    layer5 = BatchNormalization()(layer5)\n\n    layer6 = Flatten()(layer5)\n\n    layer7 = Dense(1024, activation='relu')(layer6)\n    layer7 = Dropout(0.4)(layer7)\n\n    layer8 = Dense(512, activation='relu')(layer7)\n    \n    output = Dense(10, activation='softmax')(layer8)\n\n    fashion_model = Model(inputs=input, outputs=output)\n\n    fashion_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    fashion_model.summary()\n\n    return fashion_model","25c072a7":"fashion_net = FashionNet()","17ba5d10":"history = fashion_net.fit(X_train, y_train,\n                batch_size=BATCH_SIZE,\n                epochs=EPOCHS, \n                validation_data=(X_valid, y_valid))","3a13bf49":"predictions = fashion_net.predict(X_test)","904e3fc9":"y_pred = np.argmax(predictions, axis=1)","dd86f05e":"y_pred","ca002aba":"y_test","c1f0224b":"part_predictions = []\npart_actual_labels = []\nfor i, j in zip(y_pred[1000:1010], y_test[1000:1010]):\n    part_predictions.append(class_names[i])\n    part_actual_labels.append(class_names[j])","a901aad7":"partial_data = {'Predictions' : part_predictions, 'Actual Labels' : part_actual_labels}\npart_predict = pd.DataFrame(partial_data)\npart_predict","47ecfa5d":"n_img = 5\nstart_idx = 100\nX_new = X_test[start_idx: start_idx + n_img]","ec77c734":"plt.figure(figsize=(2.5 * n_img, 2.4))\nidx = 0\nfor index, image in enumerate(X_new, start=start_idx):\n    plt.subplot(1, n_img, idx + 1)\n    plt.imshow(image.reshape(28, 28), cmap='binary')\n    plt.axis('off')\n    plt.title(f'Prediction: {class_names[y_pred[index]]} \\nActual Label: {class_names[y_test[index]]}', fontsize=12)\n    idx += 1\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\nplt.show()","fda1c196":"loss, accuracy = fashion_net.evaluate(X_test, y_test)","865e6529":"print(f'Loss: {round(loss, 2)}')\nprint(f'Accuracy: {round(accuracy * 100, 2)}%')","b3b1a051":"print(classification_report(y_test, y_pred))","cf60453d":"print(f'F1 score: {round(f1_score(y_test, y_pred, average=\"macro\") * 100, 2)}%')","32dcb601":"pd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.gca().set_xlim(0, EPOCHS)\nplt.title(\"Loss and Accuracy vs Epochs\", fontsize=18)\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss\/Accuracy\")\nplt.legend(loc=\"center\")\nplt.show()","69f13d99":"# Model Training","3f1de70f":"# Introduction\n\nThis notebook aims to develop a multi class classifier for Fashion MNIST Dataset using Deep Learnnig libraries like Keras and Tensorflow. I have used the idea of **Convolution Neural Network** in this notebbook.\n\nFollowing is the Hyperparameters values that I have used in this notebook:  \n\n- Learning Rate - 0.0001\n- Batch Size - 32\n- Epochs - 30\n- Optimizer - Adam\n- Loss Function - Sparse Categorical Cross Entropy\n- Activation Function - Relu and Softmax\n\nI have got **93.75% Test Accuracy and 93.75% F1-score without using Transfer Learning**. The reason I haven't used Transfer Learning is because I was curious to develop my own model.","4aa4f7db":"# Load Datasets","99ed1119":"# Import Modules","38942b5e":"As I have discussed in one of the above cell about our model precision on `Shirt class or class 6`, you can see that our model has given 78% precision on `Shirt class or class 6` and 85% recall on `Shirt class or class 6` which are very low.     \n\n**78% precision means that out of 100 images where we predicted `Shirt`, only 78 out of them are of `Shirt` actually.  \n85% recall means that out of all `Shirt` images present in our datasets, our model has detected 85% of the `Shirt` images correctly.**","c76cdaa3":"# Data Preprocessing\n\nOur dataset is almost processed, I have just scaled down the image pixel range from `0-255` to `0-1`. I have selected top 55k images as train data and last 5k images as validation data.  ","9bd3190d":"# Prediction using a Model","9084cb13":"Let's have a look at the top 100 images and their labels from the train datasets.","a6ca5233":"# Data Visualization","6cfe6641":"Lets create a dataframe of Model Prediction and Actual Labels of any 10 samples from the test set and compare the results.","27d5e5fe":"# Model Building\n\nThis is one of the most controversial and research topic. I have used `Conv2D`, `MaxPooling2D`, `Dropout`, `BatchNormalization`, `Flatten` and `Dense` layers.  I have used `sparse_categorical_crossentropy`  as our class labels in the range of `0-9`. If your clas labels are one-hot-encoded, use `categorical_crossentropy` as a loss function.  \n\nI have tried lot of different combinations of different layers and found this as one of the good choice. You may disagree to me on this.   \n\nI have found [this](https:\/\/stackoverflow.com\/questions\/39691902\/ordering-of-batch-normalization-and-dropout) as one of the really good question on stackoverflow which might help you in building your CNN model. ","e4d183b9":"# Model Evaluation","cbaf8830":"As you can observe from the above partial prediction dataframe that our model has misclassified the `T-shirt\/top` as `Shirt` 2 out of the 2 times. It is pretty understandable as Shirt and T-shirt looks almost similar.  \n\nIt might happen that our model would give less precision on class `Shirt` as our model has misclassified the `T-shirt\/top` as `Shirt` and less recall on class `T-shirt\/top` as none of the `T-shirt\/top` has classified correctly.  \n\nIf you are having confusion in Precision and Recall, let me clarify it:\n- Consider a binary classification scenario where there are two classes namely `Shirt` and `T-shirt\/top (Not Shirt)`.  \n    - Precision\n        - **Of all images where we predicted `Shirt`, what fraction of images are of shirt actually.**\n        - True Positive \/ Total Predicted Positive = True Positive \/ (True Positive + False Positive)\n    - Recall\n        - **Of all actual images of `Shirt`, what fraction of images we correctly detect as `Shirt`.**  \n        - True Positive \/ Total Actual Positive = True Positive \/ (True Positive + False Negative)","a8bf1d80":"Lets pick any 5 random sample from the test set, plot them with Model Prediction and Actual Labels. ","f780b1f1":"## Loss\/Accuracy vs Epochs Plot"}}