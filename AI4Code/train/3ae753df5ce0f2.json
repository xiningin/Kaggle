{"cell_type":{"bb3b6e16":"code","87e30187":"code","1d8e5a7c":"code","90adfd02":"code","98fc11fa":"code","00eeb596":"code","9d7f85b9":"code","940e30fd":"code","faed19c9":"code","d4006110":"code","97d9c4ce":"code","bcf7a699":"code","c1798ece":"code","b6b05ed8":"code","bf3b214e":"code","3740725c":"code","f20ec180":"code","8ca19e5d":"code","9eba70c0":"code","3b214e66":"code","a5aebcd1":"code","0617a77c":"code","dea857ed":"markdown","d4773742":"markdown","1adc4188":"markdown","24502842":"markdown","f5cebb66":"markdown","7a9255e7":"markdown","91c3b0f7":"markdown","a50ba0c2":"markdown","4ce6d1ce":"markdown","e6b2fca7":"markdown","59741961":"markdown","48e3c4be":"markdown","22bc33a1":"markdown","7843cbfd":"markdown","9f79df09":"markdown","f1a469f9":"markdown","2aacda51":"markdown","a3836a3c":"markdown","6634f023":"markdown","44a045c2":"markdown","fd4d6d2b":"markdown","3f866f0a":"markdown","c16a276f":"markdown"},"source":{"bb3b6e16":"## All purpose library\nimport pandas as pd\nimport numpy as np\n\n## NLP library\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\n## ML Library\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.model_selection import RepeatedStratifiedKFold,cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\n\n## Visualization library\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\n\n## Ignoring Warning during trainings \nimport warnings\nwarnings.filterwarnings('ignore')","87e30187":"## using pandas read_csv funtion to load csv files\ntrain=pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest=pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\n\n## Displying the dataframe of both training and testing\nprint(\"Training Data\")\ndisplay(train.head(3))\nprint(\"Testing Data\")\ndisplay(test.head(3))","1d8e5a7c":"## Shape of Datasets\nprint(\"Train Dataset shape:\\n\",train.shape,\"\\n\") ## (7613 rows, 5 Columns)\nprint(\"Test Dataset shape:\\n\",test.shape) ## (3263 rows, 4 Columns)","90adfd02":"## using isnull will give us bollean data and suming all true will give exact number of missing values.\nprint(\"Train Dataset missing data:\\n\",train.isnull().sum(),\"\\n\")\nprint(\"Test Dataset missing data:\\n\",test.isnull().sum())","98fc11fa":"## using pandas value counts on target will give us number of 0's with is non disaster tweets,\n## and 1's which is disaster tweets. \nVCtrain=train['target'].value_counts().to_frame()\n\n## seaborn barplot to display barchart\nsns.barplot(data=VCtrain,x=VCtrain.index,y=\"target\",palette=\"viridis\")\nVCtrain","00eeb596":"## Going deep into disaster Tweets\ndisplay(\"Random sample of disaster tweets:\",train[train.target==1].text.sample(3).to_frame())\ndisplay(\"Random sample of non disaster tweets:\",train[train.target==0].text.sample(3).to_frame())","9d7f85b9":"common_keywords=train[\"keyword\"].value_counts()[:20].to_frame()\nfig=plt.figure(figsize=(15,6))\nsns.barplot(data=common_keywords,x=common_keywords.index,y=\"keyword\",palette=\"viridis\")\nplt.title(\"Most common keywords\",size=16)\nplt.xticks(rotation=70,size=12);","940e30fd":"\ntrain[train.text.str.contains(\"disaster\")].target.\\\n value_counts().to_frame().rename(index={1:\"Disaster\",0:\"normal\"}).\\\n  plot.pie(y=\"target\",figsize=(12,6),title=\"Tweets with Disaster mentioned\");","faed19c9":"train.location.value_counts()[:10].to_frame()","d4006110":"# lowering the text\ntrain.text=train.text.apply(lambda x:x.lower() )\ntest.text=test.text.apply(lambda x:x.lower())\n#removing square brackets\ntrain.text=train.text.apply(lambda x:re.sub('\\[.*?\\]', '', x) )\ntest.text=test.text.apply(lambda x:re.sub('\\[.*?\\]', '', x) )\ntrain.text=train.text.apply(lambda x:re.sub('<.*?>+', '', x) )\ntest.text=test.text.apply(lambda x:re.sub('<.*?>+', '', x) )\n#removing hyperlink\ntrain.text=train.text.apply(lambda x:re.sub('https?:\/\/\\S+|www\\.\\S+', '', x) )\ntest.text=test.text.apply(lambda x:re.sub('https?:\/\/\\S+|www\\.\\S+', '', x) )\n#removing puncuation\ntrain.text=train.text.apply(lambda x:re.sub('[%s]' % re.escape(string.punctuation), '', x) )\ntest.text=test.text.apply(lambda x:re.sub('[%s]' % re.escape(string.punctuation), '', x) )\ntrain.text=train.text.apply(lambda x:re.sub('\\n' , '', x) )\ntest.text=test.text.apply(lambda x:re.sub('\\n', '', x) )\n#remove words containing numbers\ntrain.text=train.text.apply(lambda x:re.sub('\\w*\\d\\w*' , '', x) )\ntest.text=test.text.apply(lambda x:re.sub('\\w*\\d\\w*', '', x) )\n\ntrain.text.head()","97d9c4ce":"disaster_tweets = train[train['target']==1]['text']\nnon_disaster_tweets = train[train['target']==0]['text']","bcf7a699":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[16, 8])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(disaster_tweets))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Disaster Tweets',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(non_disaster_tweets))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Non Disaster Tweets',fontsize=40);","c1798ece":"#Tokenizer\ntoken=nltk.tokenize.RegexpTokenizer(r'\\w+')\n#applying token\ntrain.text=train.text.apply(lambda x:token.tokenize(x))\ntest.text=test.text.apply(lambda x:token.tokenize(x))\n#view\ndisplay(train.text.head())","b6b05ed8":"nltk.download('stopwords')\n#removing stop words\ntrain.text=train.text.apply(lambda x:[w for w in x if w not in stopwords.words('english')])\ntest.text=test.text.apply(lambda x:[w for w in x if w not in stopwords.words('english')])\n#view\ntrain.text.head()","bf3b214e":"test.text.head()","3740725c":"#stemmering the text and joining\nstemmer = nltk.stem.PorterStemmer()\ntrain.text=train.text.apply(lambda x:\" \".join(stemmer.stem(token) for token in x))\ntest.text=test.text.apply(lambda x:\" \".join(stemmer.stem(token) for token in x))\n#View\ntrain.text.head()","f20ec180":"count_vectorizer = CountVectorizer()\ntrain_vectors_count = count_vectorizer.fit_transform(train['text'])\ntest_vectors_count = count_vectorizer.transform(test[\"text\"])\n\n","8ca19e5d":"# Fitting a simple Logistic Regression on Counts\nCLR = LogisticRegression(C=2)\nscores = cross_val_score(CLR, train_vectors_count, train[\"target\"], cv=6, scoring=\"f1\")\nscores","9eba70c0":"# Fitting a simple Naive Bayes\nNB_Vec = MultinomialNB()\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\nscores = cross_val_score(NB_Vec, train_vectors_count, train[\"target\"], cv=cv, scoring=\"f1\")\nscores","3b214e66":"NB_Vec.fit(train_vectors_count, train[\"target\"])","a5aebcd1":"pred=NB_Vec.predict(test_vectors_count)","0617a77c":"sample_submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\nsample_submission[\"target\"] = pred\nsample_submission.to_csv(\"submission.csv\", index=False)","dea857ed":"## Using Logistic Regression for Training Model","d4773742":"### Most common keywords","1adc4188":"## Visualization","24502842":"## Final Submission into Competition ","f5cebb66":"This is the best score I can come up with experimenting on various text vectors, text cleaning, and simple model implementations. ","7a9255e7":"## Text Vectorization\nMachine learning algorithms most often take numeric feature vectors as input. Thus, when working with text documents, we need a way to convert each document into a numeric vector.\n\n**In this case Countvectorizer is best performing.**","91c3b0f7":"![Disaster](https:\/\/images.unsplash.com\/photo-1536245344390-dbf1df63c30a?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=752&q=80)","a50ba0c2":"## Tokenization\nTokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.","4ce6d1ce":"## Word cloud of tweets","e6b2fca7":"# You can submit your score in this competition and see where you stand in leaderboard.\n\n## If you like my work do upvote \ud83d\udc46 it and share it with others.","59741961":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https:\/\/deepnote.com?utm_source=created-in-deepnote-cell&projectId=64278d77-b455-4fcb-b98a-076ff504a9ee' target=\"_blank\">\n<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image\/svg+xml;base64,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' > <\/img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote<\/span><\/a>","48e3c4be":"# Natural Language Processing with Disaster Tweets\n**Predict which Tweets are about real disasters and which ones are not**\n\nTwitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nIn this competition, you\u2019re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one\u2019s aren\u2019t. You\u2019ll have access to a dataset of 10,000 tweets that were hand classified. If this is your first time working on an NLP problem, we've created a quick tutorial to get you up and running.\n\n**Disclaimer:** The dataset for this competition contains text that may be considered profane, vulgar, or offensive.\n","22bc33a1":"### Checking Missing Values","7843cbfd":"## Using pie chart ","9f79df09":"## Analyzing dataset","f1a469f9":"## Fitting model and predicting the test data.","2aacda51":"### Going deep into disaster tweets","a3836a3c":"## Stemming\nStemming and Lemmatization in Python NLTK are text normalization techniques for Natural Language Processing. These techniques are widely used for text preprocessing. The difference between stemming and lemmatization is that stemming is faster as it cuts words without knowing the context, while lemmatization is slower as it knows the context of words before processing.\n\n**In this case PoerterStemmer performed well then lemmatization**","6634f023":"### Location of Tweets","44a045c2":"## Introduction \nThis notebook is strictly for beginners and it is your entryway to the world of natural language processing. I have used a dataset from [Kaggle competition] (\"https:\/\/www.kaggle.com\/c\/nlp-getting-started\") and used simple tools for cleaning and training text data.\n\n**I will show you how to : **\n- Analyze dataset\n- Visualization of Keywords\n- Cleaning data\n- Wordcloud\n- Tokenization\n- Vectorization\n- Training with a simple model\n- Model Metrics (F1)\n- predictions from the test dataset.","fd4d6d2b":"## Importing Required Library ","3f866f0a":"## Text Cleaning","c16a276f":"## Using Simple Naive Bayes\nOur simple Logistics Regression worked poor in F1 score, so I decided to chose another model for training, you can chose any gradient boosting or simple linear model to train our data."}}