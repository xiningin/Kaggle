{"cell_type":{"50bdb4f1":"code","28c929cc":"code","1de7414b":"code","0646ced5":"code","24b72408":"code","ce446a39":"code","d9e43055":"code","0d15fb77":"code","fd555c29":"code","d6f96b51":"code","7fc7d6ef":"code","3875298c":"code","037d9685":"code","c2d495df":"code","8ffe2e88":"code","15667964":"code","ee51d19a":"code","55567bb1":"code","1eed6219":"code","568946b0":"code","be7252f7":"code","8dc495fc":"code","f105c0f9":"code","a319c13b":"code","beaf6477":"code","cb570d2c":"code","f9f9a941":"code","0966a494":"code","2b799b12":"code","d601b2df":"code","f39eb7ca":"code","9a8155c2":"code","5cfd9a09":"code","489a2b0f":"code","87fe5bcc":"code","816cbf9c":"code","9855eedb":"code","33ad44dd":"code","b0a7c4bc":"code","09a4bce3":"code","1de85d73":"code","bdff2afb":"code","70e1dbb4":"code","958ba7be":"code","3f84ab67":"code","c344fa65":"code","1b4b9452":"code","38fee213":"code","5190442e":"code","bb7fe747":"code","1038c1e7":"code","9a3fa47f":"code","29fc2606":"code","4e136223":"code","5d6f2f9a":"code","5390edcf":"markdown","d1d68784":"markdown","3af1550e":"markdown","4762cd36":"markdown"},"source":{"50bdb4f1":"#https:\/\/www.analyticsvidhya.com\/blog\/2018\/02\/the-different-methods-deal-text-data-predictive-python\/\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pickle\nfrom multiprocessing import Pool\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","28c929cc":"#libraries to import ! you should read about every one og them ! \n\nimport pandas as pd\nimport numpy as np\nimport re\nimport numpy as np\nimport pandas as pd\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\nfrom textblob import TextBlob\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import decomposition, ensemble\n\nimport pandas, xgboost, numpy, textblob, string\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, r2_score, roc_auc_score, roc_curve, classification_report\nfrom sklearn.model_selection import train_test_split\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","1de7414b":"train_data = pd.read_csv('..\/input\/moviereviewsentimentanalysiskernelsonly\/train.tsv\/train.tsv', sep=\"\\t\")\ntest_data = pd.read_csv('..\/input\/moviereviewsentimentanalysiskernelsonly\/test.tsv\/test.tsv', sep=\"\\t\")\nsub = pd.read_csv('..\/input\/moviereviewsentimentanalysiskernelsonly\/sampleSubmission.csv', sep=\",\")","0646ced5":"train = pd.read_csv('..\/input\/moviereviewsentimentanalysiskernelsonly\/train.tsv\/train.tsv', sep=\"\\t\")\ntest = pd.read_csv('..\/input\/moviereviewsentimentanalysiskernelsonly\/test.tsv\/test.tsv', sep=\"\\t\")\nsub = pd.read_csv('..\/input\/moviereviewsentimentanalysiskernelsonly\/sampleSubmission.csv', sep=\",\")","24b72408":"train_data.rename(columns={'Phrase':'text' , 'Sentiment':'target'}, inplace=True)\ntest_data.rename(columns={'Phrase':'text'}, inplace=True)","ce446a39":"train_data['text'][1]","d9e43055":"import re\ndef  clean_text(df, text_field, new_text_field_name):\n    df[new_text_field_name] = df[text_field].str.lower() #lowercase\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)|^rt|http.+?\", \"\", elem))  \n    # remove numbers\n    #remove.............. (#re sub \/ search\/ ..)\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n    \n    return df\n#read about dataframes ! TABLEAU !! \n#data_clean : new dataframe\ndata_clean = clean_text(train_data, 'text', 'text_clean')\ndata_clean_test = clean_text(test_data,'text', 'text_clean')\ndata_clean.head()","0d15fb77":"import nltk.corpus\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\nstop #the list of the stopwords ! ","fd555c29":"data_clean['text_clean'] = data_clean['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ndata_clean.head()","d6f96b51":"#Tokenization : word_tokenize ! \nimport nltk \nnltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize, word_tokenize\ndata_clean['text_tokens'] = data_clean['text_clean'].apply(lambda x: word_tokenize(x))\ndata_clean.head()","7fc7d6ef":"#stemming #PorterStemmer \nfrom nltk.stem import PorterStemmer \nfrom nltk.tokenize import word_tokenize\ndef word_stemmer(text):\n    stem_text = [PorterStemmer().stem(i) for i in text]\n    return stem_text\ndata_clean['text_clean_tokens'] = data_clean['text_tokens'].apply(lambda x: word_stemmer(x))\ndata_clean.head()","3875298c":"nltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\ndef word_lemmatizer(text):\n    lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]\n    return lem_text\ndata_clean['text_clean_tokens'] = data_clean['text_tokens'].apply(lambda x: word_lemmatizer(x))\ndata_clean.head()","037d9685":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\n\n\ndata_clean['text_clean'] = data_clean['text_clean'].apply(lambda x: remove_URL(x))","c2d495df":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndata_clean['text_clean'] = data_clean['text_clean'].apply(lambda x: remove_html(x))","8ffe2e88":"# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndata_clean['text_clean'] = data_clean['text_clean'].apply(lambda x: remove_emoji(x))","15667964":"import string\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\ndata_clean['text_clean'] = data_clean['text_clean'].apply(lambda x: remove_punct(x))","ee51d19a":"freq = pd.Series(' '.join(data_clean['text_clean']).split()).value_counts()[:10]\n\nfreq = list(freq.index)\ndata_clean['text_clean'] = data_clean['text_clean'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))","55567bb1":"X_train, X_test, Y_train, Y_test = train_test_split(data_clean['text_clean'], \n                   \n                                                    data_clean['target'], \n                                                    test_size = 0.2,\n                                                    random_state = 10)\n\n#HYPERPARAMETERS ! \n#SPLITTING THE DATA ! ","1eed6219":"\ntfidf = TfidfVectorizer(encoding='utf-8',\n                       ngram_range=(1,3),\n                       max_df=1.0,\n                       min_df=10,\n                       max_features=500,\n                       norm='l2',\n                       sublinear_tf=True)","568946b0":"train_features = tfidf.fit_transform(X_train).toarray()\nprint(train_features.shape)","be7252f7":"test_features = tfidf.transform(X_test).toarray()\nprint(test_features.shape)","8dc495fc":"train_labels = Y_train\ntest_labels = Y_test","f105c0f9":"\nimport pandas as pd\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score","a319c13b":"mnb_classifier = MultinomialNB()","beaf6477":"mnb_classifier.fit(train_features, train_labels)","cb570d2c":"mnb_prediction = mnb_classifier.predict(test_features)","f9f9a941":"training_accuracy = accuracy_score(train_labels, mnb_classifier.predict(train_features))\nprint(training_accuracy)","0966a494":"testing_accuracy = accuracy_score(test_labels, mnb_prediction)\nprint(testing_accuracy)","2b799b12":"print(classification_report(test_labels, mnb_prediction))","d601b2df":"conf_matrix = confusion_matrix(test_labels, mnb_prediction)\nprint(conf_matrix)","f39eb7ca":"loj = linear_model.LogisticRegression()\nloj_model = loj.fit(train_features, train_labels)\ny_pred = loj_model.predict(test_features)\n\n\naccuracy_score(test_labels,y_pred)","9a8155c2":"test_vectorizer =tfidf.transform( data_clean_test['text_clean']).toarray()","5cfd9a09":"test_vectorizer.shape","489a2b0f":"final_predictions = mnb_classifier.predict(test_vectorizer)","87fe5bcc":"final_predictions","816cbf9c":"submission_df = pd.DataFrame()","9855eedb":"submission_df['PhraseId'] = data_clean_test['PhraseId']\nsubmission_df['target'] = final_predictions","33ad44dd":"submission_df","b0a7c4bc":"submission_df['target'].value_counts()","09a4bce3":"submission = submission_df.to_csv('Result.csv',index = False)","1de85d73":"seed = 0\n\nimport random\nimport numpy as np\nimport tensorflow as tf\ntf.random.set_seed(seed) ","bdff2afb":"import pandas as pd\n\ntrain = pd.read_csv('..\/input\/moviereviewsentimentanalysiskernelsonly\/train.tsv\/train.tsv',  sep=\"\\t\")\ntest = pd.read_csv('..\/input\/moviereviewsentimentanalysiskernelsonly\/test.tsv\/test.tsv',  sep=\"\\t\")","70e1dbb4":"train.head()","958ba7be":"train['Sentiment'].value_counts()","3f84ab67":"def format_data(train, test, max_features, maxlen):\n    \"\"\"\n    Convert data to proper format.\n    1) Shuffle\n    2) Lowercase\n    3) Sentiments to Categorical\n    4) Tokenize and Fit\n    5) Convert to sequence (format accepted by the network)\n    6) Pad\n    7) Voila!\n    \"\"\"\n    from keras.preprocessing.text import Tokenizer\n    from keras.preprocessing.sequence import pad_sequences\n    from keras.utils import to_categorical\n    \n    train = train.sample(frac=1).reset_index(drop=True)\n    train['Phrase'] = train['Phrase'].apply(lambda x: x.lower())\n    test['Phrase'] = test['Phrase'].apply(lambda x: x.lower())\n\n    X = train['Phrase']\n    test_X = test['Phrase']\n    Y = to_categorical(train['Sentiment'].values)\n\n    tokenizer = Tokenizer(num_words=max_features)\n    tokenizer.fit_on_texts(list(X))\n\n    X = tokenizer.texts_to_sequences(X)\n    X = pad_sequences(X, maxlen=maxlen)\n    test_X = tokenizer.texts_to_sequences(test_X)\n    test_X = pad_sequences(test_X, maxlen=maxlen)\n\n    return X, Y, test_X\n","c344fa65":"maxlen = 125\nmax_features = 15000\n\nX, Y, test_X = format_data(train, test, max_features, maxlen)","1b4b9452":"X","38fee213":"Y","5190442e":"test_X","bb7fe747":"from sklearn.model_selection import train_test_split\nX_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.25, random_state=seed)","1038c1e7":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, BatchNormalization\nfrom tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.engine import InputSpec, Layer\nfrom keras.optimizers import Adam\nfrom tensorflow.keras import Sequential\n\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\n\n","9a3fa47f":"model = Sequential()\n\n# Input \/ Embdedding\nmodel.add(Embedding(max_features, 150, input_length=maxlen))\n\n# CNN\nmodel.add(SpatialDropout1D(0.2))\n\nmodel.add(Conv1D(32, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\n\nmodel.add(Conv1D(64, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\n\nmodel.add(Flatten())\n\n# Output layer\nmodel.add(Dense(5, activation='sigmoid'))","29fc2606":"epochs = 5\nbatch_size = 32","4e136223":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=epochs, batch_size=batch_size, verbose=1)","5d6f2f9a":"sub = pd.read_csv('..\/input\/moviereviewsentimentanalysiskernelsonly\/sampleSubmission.csv')\n\nsub['Sentiment'] = model.predict_classes(test_X, batch_size=batch_size, verbose=1)\nsub.to_csv('sub_cnn.csv', index=False)","5390edcf":"cnn \n**What are Convolutional Neural Networks and why are they important?**\n\nConvolutional Neural Networks (ConvNets or CNNs) are a category of Neural Networks that have proven very effective in areas such as image recognition and classification. ConvNets have been successful in identifying faces, objects and traffic signs apart from powering vision in robots and self driving cars.\n\nThere are four main operations in the ConvNet :\n\n1- Convolution : The Convolution Step\nConvNets derive their name from the \u201cconvolution\u201d operator. The primary purpose of Convolution in case of a ConvNet is to extract features from the input image. Convolution preserves the spatial relationship between pixels by learning image features using small squares of input data.\nNote : in practice a CNN learnes the values of these filtres on its own during the training process (although we still need to specify parameters such as nbr of filtres,filter size, architecture of the network etc.. before the training process) \nthe more number of filtres we have , the more features get extractedand the better our network becomes at recognizinf pattaerns in unseen cases. \n\nthe size of the feature Map (convolved feature) is controlled by 3 parameters : depth,stride,zero-padding. \n\n\nKeywords : Filter ** Kernel ** feature detector ** stride ** matrices ** dot product **  Feature Map ** \n\n2- Non Linearity (ReLU) : RELU stands for rectified Linear unit and is a non linear operation.  output = MAx(zero, input).\nrelu is applied per pixel and replaces all negative pixel values in the feature map by zero. \nthe purpos of relu is to introduce non linearity in our convnet. \n\ninput feature map ( black = negative , white = positive values ) ===RELU=== rectified feature Map ( only non-negative values) \n[there are other non linear functions such as tanh or sigmoid can also be used instead of relu) but relu has been found to perform better in most situations. \n] \n3- Pooling or Sub Sampling : spatial pooling reduces the dimentionality of each feature map but retains the most important information. \nspatial pooling can be of different types : Max, average,sum,etc..\n\nthe function of pooling is to progressively reduce the spatial size of the input representation. in particular pooling : 1- makes the input representations smaller and more manageable. 2- reduces the nbr of p\u00e2rameters and computations in the network, therefore controlling overfitting. 3- makes the network invariant to small transformations and distortions.\n\n\n4- Classification (Fully Connected Layer) : The Fully Connected layer is a traditional Multi Layer Perceptron that uses a softmax activation function in the output layer (other classifiers like SVM can also be used, but will stick to softmax in this post). The term \u201cFully Connected\u201d implies that every neuron in the previous layer is connected to every neuron on the next layer\n\nThe output from the convolutional and pooling layers represent high-level features of the input image. The purpose of the Fully Connected layer is to use these features for classifying the input image into various classes based on the training dataset.\n\nThese operations are the basic building blocks of every Convolutional Neural Network, so understanding how these work is an important step to developing a sound understanding of ConvNets. \n\n","d1d68784":"**Testing** ","3af1550e":"***Logistic Regression*****","4762cd36":"The regular expression above is meant to find any four digits at the beginning of a string, which suffices for our case. The above is a raw string (meaning that a backslash is no longer an escape character), which is standard practice with regular expressions.\nregex = r'^(\\d{4})'\n"}}