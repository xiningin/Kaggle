{"cell_type":{"50892ca2":"code","dc9ed97d":"code","be5636ac":"code","65fd8f43":"code","70697951":"code","0dc2223e":"code","c07cc078":"code","eb7eafa4":"code","9bb53f5d":"code","5f6ce1b2":"code","42fc976d":"code","673595f1":"code","d45ae775":"code","a3722cec":"code","660ce830":"code","c894accc":"code","70c22797":"code","6c1b9953":"code","d95fc7e3":"code","1f70c652":"code","bc140deb":"code","c3408278":"code","049c5b5a":"code","eb83ba86":"code","2fb0bc82":"code","ddb31baa":"code","7a2130a1":"code","0d597cc6":"code","e540b421":"code","8bd86938":"code","db6e2675":"code","4e6da6c6":"markdown","ebe1dc5c":"markdown","340d91f9":"markdown","ef71fbce":"markdown","58c4d85a":"markdown","890c2468":"markdown","cf8097dc":"markdown","1996d5fa":"markdown","dc5b1a80":"markdown","51830061":"markdown","2111f444":"markdown"},"source":{"50892ca2":"!pip install -U transformers","dc9ed97d":"import torch, torchvision\nprint(torch.__version__, torch.cuda.is_available())","be5636ac":"!python -m pip install 'git+https:\/\/github.com\/facebookresearch\/detectron2.git'","65fd8f43":"!pip install -U datasets seqeval","70697951":"from datasets import load_dataset \ndatasets = load_dataset(\"nielsr\/funsd\")","0dc2223e":"datasets","c07cc078":"labels = datasets['train'].features['ner_tags'].feature.names\nprint(labels)","eb7eafa4":"id2label = {v: k for v, k in enumerate(labels)}\nlabel2id = {k: v for v, k in enumerate(labels)}\nprint(label2id)\nprint(id2label)","9bb53f5d":"from PIL import Image, ImageDraw, ImageFont\nexample = datasets[\"train\"][6]\nprint(example.keys())\nimage = Image.open(example['image_path'])\nimage = image.convert(\"RGB\")\nnew_image = image.resize((600, 800))\nnew_image","5f6ce1b2":"from PIL import Image\nfrom transformers import LayoutLMv2Processor\nfrom datasets import Features, Sequence, ClassLabel, Value, Array2D, Array3D\nfrom transformers import pipeline\n\nprocessor = LayoutLMv2Processor.from_pretrained(\"microsoft\/layoutlmv2-base-uncased\", revision=\"no_ocr\")\n\n# we need to define custom features\nfeatures = Features({\n    'image': Array3D(dtype=\"int64\", shape=(3, 224, 224)),\n    'input_ids': Sequence(feature=Value(dtype='int64')),\n    'attention_mask': Sequence(Value(dtype='int64')),\n    'token_type_ids': Sequence(Value(dtype='int64')),\n    'bbox': Array2D(dtype=\"int64\", shape=(512, 4)),\n    'labels': Sequence(ClassLabel(names=labels)),\n})\n\ndef preprocess_data(examples):\n  images = [Image.open(path).convert(\"RGB\") for path in examples['image_path']]\n  words = examples['words']\n  boxes = examples['bboxes']\n  word_labels = examples['ner_tags']\n  \n  encoded_inputs = processor(images, words, boxes=boxes, word_labels=word_labels,\n                             padding=\"max_length\", truncation=True)\n  \n  return encoded_inputs\n\ntrain_dataset = datasets['train'].map(preprocess_data, batched=True, remove_columns=datasets['train'].column_names,features=features)\ntest_dataset = datasets['test'].map(preprocess_data, batched=True, remove_columns=datasets['test'].column_names,features=features)","42fc976d":"train_dataset","673595f1":"processor.tokenizer.decode(train_dataset['input_ids'][0])","d45ae775":"print(train_dataset['labels'][0])","a3722cec":"train_dataset.set_format(type=\"torch\", device=\"cuda\")\ntest_dataset.set_format(type=\"torch\", device=\"cuda\")","660ce830":"\ntrain_dataset.features.keys()","c894accc":"from torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=2)","70c22797":"batch = next(iter(train_dataloader))\n\nfor k,v in batch.items():\n  print(k, v.shape)","6c1b9953":"from transformers import LayoutLMv2ForTokenClassification, AdamW\nimport torch\nfrom tqdm.notebook import tqdm\n\nmodel = LayoutLMv2ForTokenClassification.from_pretrained('microsoft\/layoutlmv2-base-uncased',\n                                                          num_labels=len(labels))\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\nglobal_step = 0\nnum_train_epochs = 10\n\nt_total = len(train_dataloader) * num_train_epochs # total number of training steps \n\n#put the model in training mode\nmodel.train() \nfor epoch in range(num_train_epochs):  \n   print(\"Epoch:\", epoch)\n   for batch in tqdm(train_dataloader):\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(**batch) \n        loss = outputs.loss\n        \n        # print loss every 100 steps\n        if global_step % 100 == 0:\n          print(f\"Loss after {global_step} steps: {loss.item()}\")\n\n        loss.backward()\n        optimizer.step()\n        global_step += 1","d95fc7e3":"from datasets import load_metric\n\nmetric = load_metric(\"seqeval\")\n\n# put model in evaluation mode\nmodel.eval()\nfor batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n    with torch.no_grad():\n        input_ids = batch['input_ids'].to(device)\n        bbox = batch['bbox'].to(device)\n        image = batch['image'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        token_type_ids = batch['token_type_ids'].to(device)\n        labels = batch['labels'].to(device)\n\n        # forward pass\n        outputs = model(input_ids=input_ids, bbox=bbox, image=image, attention_mask=attention_mask, \n                        token_type_ids=token_type_ids, labels=labels)\n        \n        # predictions\n        predictions = outputs.logits.argmax(dim=2)\n\n        # Remove ignored index (special tokens)\n        true_predictions = [\n            [id2label[p.item()] for (p, l) in zip(prediction, label) if l != -100]\n            for prediction, label in zip(predictions, labels)\n        ]\n        true_labels = [\n            [id2label[l.item()] for (p, l) in zip(prediction, label) if l != -100]\n            for prediction, label in zip(predictions, labels)\n        ]\n\n        metric.add_batch(predictions=true_predictions, references=true_labels)\n\nfinal_score = metric.compute()\nprint(final_score)","1f70c652":"example = datasets[\"test\"][3]\nprint(example.keys())","bc140deb":"from PIL import Image, ImageDraw, ImageFont\n\nimage = Image.open(example['image_path'])\nimage = image.convert(\"RGB\")\nimage","c3408278":"encoded_inputs = processor(image, example['words'], boxes=example['bboxes'], word_labels=example['ner_tags'],\n                           padding=\"max_length\", truncation=True, return_tensors=\"pt\")","049c5b5a":"labels = encoded_inputs.pop('labels').squeeze().tolist()\nfor k,v in encoded_inputs.items():\n  encoded_inputs[k] = v.to(device)","eb83ba86":"# forward pass\noutputs = model(**encoded_inputs)","2fb0bc82":"outputs.logits.shape","ddb31baa":"def unnormalize_box(bbox, width, height):\n     return [\n         width * (bbox[0] \/ 1000),\n         height * (bbox[1] \/ 1000),\n         width * (bbox[2] \/ 1000),\n         height * (bbox[3] \/ 1000),\n     ]\n\npredictions = outputs.logits.argmax(-1).squeeze().tolist()\ntoken_boxes = encoded_inputs.bbox.squeeze().tolist()\n\nwidth, height = image.size\n\ntrue_predictions = [id2label[prediction] for prediction, label in zip(predictions, labels) if label != -100]\ntrue_labels = [id2label[label] for prediction, label in zip(predictions, labels) if label != -100]\ntrue_boxes = [unnormalize_box(box, width, height) for box, label in zip(token_boxes, labels) if label != -100]","7a2130a1":"print(true_predictions)","0d597cc6":"print(true_labels)","e540b421":"from PIL import ImageDraw\n\ndraw = ImageDraw.Draw(image)\n\nfont = ImageFont.load_default()\n\ndef iob_to_label(label):\n    label = label[2:]\n    if not label:\n      return 'other'\n    return label\n\nlabel2color = {'question':'blue', 'answer':'green', 'header':'orange', 'other':'violet'}\n\nfor prediction, box in zip(true_predictions, true_boxes):\n    predicted_label = iob_to_label(prediction).lower()\n    draw.rectangle(box, outline=label2color[predicted_label])\n    draw.text((box[0] + 10, box[1] - 10), text=predicted_label, fill=label2color[predicted_label], font=font)\n\nimage","8bd86938":"example.keys()","db6e2675":"image = Image.open(example['image_path'])\nimage = image.convert(\"RGB\")\n\ndraw = ImageDraw.Draw(image)\n\nfor word, box, label in zip(example['words'], example['bboxes'], example['ner_tags']):\n  actual_label = iob_to_label(id2label[label]).lower()\n  box = unnormalize_box(box, width, height)\n  draw.rectangle(box, outline=label2color[actual_label], width=2)\n  draw.text((box[0] + 10, box[1] - 10), actual_label, fill=label2color[actual_label], font=font)\n\nimage","4e6da6c6":"# References","ebe1dc5c":"* Transformers (for the LayoutLMv2 model)\n* Detectron2 (which LayoutLMv2 requires for its visual backbone)\n* Seqeval (for metrics)\n* Datasets (for data preprocessing)\n","340d91f9":"# Inference\nLet's test the trained model on the first image of the test set:","ef71fbce":"\nCompare this to the ground truth:","58c4d85a":"# **Fine tuning LayoutLMv2 On FUNSD**","890c2468":"# Evaluation\nNext, let's evaluate the model on the test set.","cf8097dc":"# Train the model\nHere we train the model in native PyTorch. We use the AdamW optimizer.","1996d5fa":"# Preprocess data","dc5b1a80":"https:\/\/github.com\/NielsRogge\/Transformers-Tutorials\/blob\/master\/LayoutLMv2\/FUNSD\/Fine_tuning_LayoutLMv2ForTokenClassification_on_FUNSD.ipynb","51830061":"# Install dependencies","2111f444":"# Loading data"}}