{"cell_type":{"cfc0fb8b":"code","d60a2e50":"code","be9246c4":"code","38efd370":"code","1c252ee7":"code","79ab5aea":"code","aa6d8cac":"code","270c38d1":"code","a193d1a0":"code","2075749c":"code","6f2ca3a9":"code","86e34c4f":"code","efd056e0":"code","9ff99c60":"code","89413621":"code","054be0a0":"code","886ace7b":"code","29394356":"code","1aee4079":"code","c833b362":"code","b95469a8":"code","d4569b64":"code","fd8bc5c9":"code","7261cee5":"code","8a739081":"code","f832f4c1":"code","ae6b352b":"code","e93fbe03":"code","682af40b":"code","74a98840":"code","c8641b8e":"code","808f0cb0":"code","126ae371":"code","080d117f":"code","5cb82a4d":"code","d987caf8":"code","308e2ab6":"code","f49be02b":"code","8359ef72":"code","be257491":"code","2694ab13":"code","70af5198":"code","af3fa680":"markdown","6be04d9c":"markdown","f6a9eb66":"markdown","21434d9e":"markdown","5f113308":"markdown","3ff26b74":"markdown","fb5dd51d":"markdown","b62bcfcc":"markdown","3e4672fa":"markdown","3c0a5730":"markdown","e70cc693":"markdown","7efd3c0e":"markdown","ceae22e7":"markdown","af89985b":"markdown","248098ad":"markdown","c84e34a7":"markdown","f6934f0b":"markdown","1189483d":"markdown","f540e8e9":"markdown","2eeabf5c":"markdown","bc129279":"markdown","8ff54e7e":"markdown","e4c3d4f7":"markdown","ac6f34e9":"markdown","bdf2e26f":"markdown","3953422a":"markdown","ce586405":"markdown","530ef847":"markdown","a25151dd":"markdown","45f8c146":"markdown","3386766f":"markdown","104c4675":"markdown","a2c34a72":"markdown","c872f96c":"markdown","04155f5d":"markdown","02ba622c":"markdown","4f880553":"markdown","666ab40f":"markdown","30c8682c":"markdown","961d96a0":"markdown","207a696a":"markdown"},"source":{"cfc0fb8b":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns","d60a2e50":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport warnings\nwarnings.filterwarnings(\"ignore\")","be9246c4":"dataset = pd.read_csv('\/kaggle\/input\/heart-failure-prediction\/heart.csv')\ndataset.head()","38efd370":"dataset.shape","1c252ee7":"dataset.isnull().sum()","79ab5aea":"dataset[dataset.duplicated()]","aa6d8cac":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nlables = ['Sex','ChestPainType','RestingECG','ExerciseAngina','ST_Slope']\nfor i in lables:\n    dataset[i] = LabelEncoder().fit_transform(dataset[i])\ndataset[i] = le.fit_transform(dataset[i])","270c38d1":"X = dataset.drop(columns=['HeartDisease'])\ny = dataset['HeartDisease']","a193d1a0":"X.head()","2075749c":"continuous_features = [i for i in X.columns if X[i].nunique() > 10]\nprint(f'continuous feature: {continuous_features}')","6f2ca3a9":"plt.style.use('fivethirtyeight')\ni = 1\nplt.figure(figsize=(18,16)) \nfor feature in continuous_features:\n    plt.subplot(round(len(continuous_features)\/2)+1, 2, i)\n    sns.histplot(x=X[feature], kde=True, bins=50, hue=y)\n    plt.xlabel(feature,size=12)\n    plt.ylabel(\"Count\",size=12)\n    i += 1\n    \nplt.show()","86e34c4f":"mask = X['Cholesterol'] > 0\nX, y = X[mask], y[mask]\n## plot the data again\ni = 1\nplt.figure(figsize=(18,16)) \nfor feature in continuous_features:\n    plt.subplot(round(len(continuous_features)\/2)+1, 2, i)\n    sns.histplot(x=X[feature], kde=True, bins=50, hue=y)\n    plt.xlabel(feature,size=12)\n    plt.ylabel(\"Count\",size=12)\n    i += 1\n    \nplt.show()","efd056e0":"from sklearn.model_selection import train_test_split\nX_train_, X_test_, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","9ff99c60":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = X_train_.copy()\nX_train.loc[:, continuous_features] = sc.fit_transform(X_train_.loc[:, continuous_features])\nX_test = X_test_.copy()\nX_test.loc[:, continuous_features] = sc.transform(X_test_.loc[:, continuous_features])\nX_train.head()","89413621":"X_test.head()","054be0a0":"# perform a polynomial features transform of the train and test sets\n#trans = PolynomialFeatures(degree=3)\n#X_train = trans.fit_transform(X_train)\n#X_test = trans.fit_transform(X_test)\n# summarize\n#print(X_train.shape, X_test.shape)","886ace7b":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)","29394356":"y_pred = classifier.predict(X_test)","1aee4079":"from sklearn.metrics import classification_report, roc_curve, confusion_matrix, accuracy_score, recall_score, auc\nprint(classification_report(y_test,y_pred))","c833b362":"cm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint('Random Forest Classifier has an accuracy score of {:.2f} and recall score of class  {:.2f}'.format((accuracy_score(y_test, y_pred) *100),(recall_score(y_test, y_pred) *100)))","b95469a8":"from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)","d4569b64":"y_pred = classifier.predict(X_test)","fd8bc5c9":"print(classification_report(y_test,y_pred))","7261cee5":"cm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint('Naive Bayes Classifier has an accuracy score of {:.2f} and recall score of {:.2f}'.format((accuracy_score(y_test, y_pred) *100),(recall_score(y_test, y_pred) *100)))","8a739081":"from sklearn.svm import SVC\nclassifier = SVC(kernel = 'rbf', random_state = 0)\nclassifier.fit(X_train, y_train)","f832f4c1":"y_pred = classifier.predict(X_test)","ae6b352b":"print(classification_report(y_test,y_pred))","e93fbe03":"cm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint('Kernel SVM Classifier has an accuracy score of {:.2f} and recall score of {:.2f}'.format((accuracy_score(y_test, y_pred) *100),(recall_score(y_test, y_pred) *100)))","682af40b":"import xgboost as xgb\nclassifier = xgb.XGBClassifier(use_label_encoder=False)\nclassifier.fit(X_train, y_train, eval_metric=\"aucpr\")","74a98840":"y_pred = classifier.predict(X_test)","c8641b8e":"print(classification_report(y_test,y_pred))","808f0cb0":"cm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint('XGBoost Classifier has an accuracy score of {:.2f} and recall score of {:.2f}'.format((accuracy_score(y_test, y_pred) *100),(recall_score(y_test, y_pred) *100)))","126ae371":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold","080d117f":"# A parameter grid for XGBoost\nparams = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }","5cb82a4d":"# Apply iterative randomized search to find the best parameter values combination\nskf = StratifiedKFold(n_splits=5, shuffle = True, random_state = 1001)\n\nclassifier_tuned = RandomizedSearchCV(classifier, param_distributions=params, n_iter=5, \n                                   scoring='recall', n_jobs=4, cv=skf.split(X_train,y_train), \n                                   verbose=3, random_state=1001,return_train_score=False)\nclassifier_tuned.fit(X_train, y_train,eval_metric=\"aucpr\")\nprint(classifier_tuned.best_params_)","d987caf8":"# Generate the new model with the tuned parameters\nclassifier = xgb.XGBClassifier(**classifier_tuned.best_params_,use_label_encoder=False)\nclassifier.fit(X_train, y_train, eval_metric=\"aucpr\")","308e2ab6":"y_pred = classifier.predict(X_test)","f49be02b":"print(classification_report(y_pred,y_test))","8359ef72":"cm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint('Tuned XGBoost Classifier has an accuracy score of {:.2f} and recall score of {:.2f}'.format((accuracy_score(y_test, y_pred) *100),(recall_score(y_test, y_pred) *100)))","be257491":"fpr, tpr, thresholds = roc_curve(y_test, y_pred)\nroc_auc = auc(fpr, tpr)\nplt.figure(figsize=(4,5))\nlw = 2\nplt.plot(fpr, tpr, color='red', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='darkblue', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","2694ab13":"import shap\nexplainer = shap.Explainer(classifier, X_train)\nshap_values = explainer(X_train, check_additivity=False )\nshap.plots.waterfall(shap_values[0])","70af5198":"explainer = shap.Explainer(classifier, X_test)\nshap_values = explainer(X_test, check_additivity=False )\nshap.plots.beeswarm(shap_values)","af3fa680":"## 1. Random Forest Classification ","6be04d9c":"# Code","f6a9eb66":"### 3.2. Predict the Test set results","21434d9e":"#### 4.1. Train the model on the Training set","5f113308":"Polynomial Feature Transform didn't improve the reuslts. However, it can be tested with different degrees and different models to see if it can improve the prediction or not.","3ff26b74":"The plot above sorts features by the sum of SHAP value magnitudes over all samples, and uses SHAP values to show the distribution of the impacts each feature has on the model output. The color represents the feature value (red high, blue low). ","fb5dd51d":"# Build and compare differnt models","b62bcfcc":"## Visualizing the data\nWe plot the histogram of the continuous features to understand the data distribution and detect the outliers if there is any.","3e4672fa":"# <h1><center>Explanable Machine Learning for Heart Failure Prediction<\/center><\/h1>","3c0a5730":"### 2.2. Predict the Test set results","e70cc693":"### 4. XGBoost","7efd3c0e":"#### 4.3. Make the Classificatoin Report and Confusion Matrix","ceae22e7":"## Importing the libraries","af89985b":"There is no missing or duplicate data","248098ad":"Hyperparmeter tuning slightly  enhamce the accuracy score. Let's plot the ROC curve fot it:","c84e34a7":"## Polynomial Feature Transform","f6934f0b":"#### 1.2. Predict the Test set results","1189483d":"#### 4.2. Predict the Test set results","f540e8e9":"### 2.1. Train the model on the Training set","2eeabf5c":"### 2.3. Make the Classificatoin Report and Confusion Matrix","bc129279":"## Spliting the features (X) and the output (y)","8ff54e7e":"### 3.3. Make the Classificatoin Report and Confusion Matrix","e4c3d4f7":"## Spliting the dataset into Training set and Test set","ac6f34e9":"## 3. Kernel SVM ","bdf2e26f":"## Taking care of missing or duplicate data","3953422a":"The above explanation shows features each contributing to push the model output from the base value (the average model output over the training dataset we passed) to the model output. \nFeatures pushing the prediction to class 1 (Heart Disease) are shown in red, those pushing the prediction to class 0 (No Heart Disease) are in blue.\n\nWe can also plot the SHAP values of every feature for every test sample.","ce586405":"Based on the accuracy score, XGBoost is slightly better than other models. To improve it's performance we tune its hyperparameters.","530ef847":"## Encoding  the label variables\n'Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', and 'ST_Slope' should be converted from string to numeric values.","a25151dd":"# Conclusion\nComparing the performance of the four different techniques, XGBoost outperformed the others by the accuracy and recall scores of 88.7 and 90.3, respectively. Tuning the hyperparameters of XGBoost could increase the accuracy and recall scores of prediction to 89.3 and 91.7, respectively.\nThe SHAP values were calculated globally and locally to explain the impact of each feature on the model, as well as the symptoms of each patient that led to their associated diagnosis. From the SHAP value plots, we can conclude that:\n* **ST_Slope** has the highest contribution to the prediction. \n* High ST_Slope values (**upsloping**) pushes the prediction to class 0 (**Normal**), while the **flat slope** of the peak exercise ST segment pushes the model output to **heart disease**. \n* Patients with **ASYM ChestPainType**, which is encoded to 0 (blue colour in the graph) are _more likely_ to have **heart disease**.\n* **Males** (encoded to 1) are _more likely_ to have **heart disease** than females.* \n* People who have **high oldpeak**, are more prone to heart disease.\n* Higher the **exercise angina**, the higher the chance of having heart disease\n* **Older patients** are more at risk of having heart disease","45f8c146":"#### 1.3. Make the Classificatoin Report and Confusion Matrix","3386766f":"# Interpret the results with SHAP Values\nTo get an overview of which features are most important for the model we can plot the SHAP values of every feature:","104c4675":"## Importing the dataset","a2c34a72":"Here four differnet classifiers (Random Forest, Naive Bayes, Kernel SVM, and XGBOOST) are trained and compared.<br>\n* Since we are dealing with health data and having false negatives is not acceptable, the best score to look at is the **recall** score of class 1 (in this case, Heart Disease).","c872f96c":"#### 1.1. Train the model on the Training set","04155f5d":"It seems there is missing data for Cholestreol; it cannot be zero. So, we remove the data with Cholestreol = 0.","02ba622c":"## 2. Naive Bayes","4f880553":"### 3.1.Train the model on the Training set","666ab40f":"# Hyperparameter tuning","30c8682c":"![Heart_failure_results.PNG](attachment:cba3b6a8-cf66-4fe7-b4f5-d2230f51cf78.PNG)","961d96a0":"## Feature Scaling\n*Apply it only on continuous features*","207a696a":"## Introduction\nCardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worldwide. Four out of 5CVD deaths are due to heart attacks and strokes, and one-third of these deaths occur prematurely in people under 70 years of age. Heart failure is a common event caused by CVDs and this dataset contains 11 features that can be used to predict a possible heart disease.\nPeople with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.\n\n## Objectives\nThe main objective of this work is to predict if a patient is at risk of having heart disease or not, based on the common features of patients with heart disease. To that end, four different methods, including **random forest**, **na\u00efve bayes**, **kernel SVM**, and **XGBoost** are exploited and their performance is compared to find the best model for the prediction of heart disease.\nFinally,  the obtained results are interpreted with **SHAP values** to see the effect of each feature on the prediction. It can help clinicians to better understand the reasons behind the prediction.\n\n\n## Description of the dataset\nThis dataset was created by combining different datasets already available independently but not combined before. In this dataset, 5 heart datasets with a total of **918 observations** are combined over **11 common features** which makes it the largest heart disease dataset available so far for research purposes. The features are as the following:\n\n1. *Age*: age of the patient [years]\n2. *Sex*: sex of the patient [M: Male, F: Female]\n3. *ChestPainType*: chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n4. *RestingBP*: resting blood pressure [mm Hg]\n5. *Cholesterol*: serum cholesterol [mm\/dl]\n6. *FastingBS*: fasting blood sugar [1: if FastingBS > 120 mg\/dl, 0: otherwise]\n7. *RestingECG*: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n8. *MaxHR*: maximum heart rate achieved [Numeric value between 60 and 202]\n9. *ExerciseAngina*: exercise-induced angina [Y: Yes, N: No]\n10. *Oldpeak*: oldpeak = ST [Numeric value measured in depression]\n11. *ST_Slope*: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n- *HeartDisease*: output class [1: heart disease, 0: Normal]\n\nsource: https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/heart-disease\/; https:\/\/www.kaggle.com\/fedesoriano\/heart-failure-prediction"}}