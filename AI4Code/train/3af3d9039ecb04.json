{"cell_type":{"63188a25":"code","e6a33d0e":"code","eb9db14e":"code","1f24a3f9":"code","6bc7ad3e":"code","1d892abb":"code","9468730c":"code","8b7c7a13":"code","0d0f0c56":"code","448c320e":"code","1ebaf709":"code","cdd0b4e3":"code","885ff4a0":"code","6a3df837":"code","92d23abb":"code","f962fd4d":"code","c6450d98":"code","aabab3b3":"code","89dd36d7":"code","bddd7906":"code","6e537074":"code","d42c88d5":"code","945f4593":"code","6dc9971f":"code","e1672267":"code","93cceef2":"code","0593dbae":"code","e924a845":"code","f75d316a":"code","ab683925":"code","b5f40fdd":"code","73f55052":"code","a914b9f7":"code","9b20aae1":"code","1d1b52f7":"code","d52fb0fd":"code","bea37c44":"code","49b27c62":"code","a3dc214c":"code","fe8bc035":"code","39d67dab":"code","032f4575":"code","a2ed6b13":"code","79d55adb":"code","f3f58d3a":"code","0de4a142":"code","ba87951e":"code","78dfbc63":"code","a4205513":"code","ee006305":"code","e37360d2":"code","883f9673":"code","748dbc24":"code","17bdd375":"code","ae63bdee":"code","0537225d":"code","41a4ec55":"code","498ca6be":"code","ba7ffbd3":"code","38ceb87c":"code","7e5df282":"code","9181b6c2":"markdown","25a43c89":"markdown","b6e79f42":"markdown","64614aa0":"markdown","f1a5f081":"markdown","8527781a":"markdown","0ec46e7e":"markdown","83b1d08f":"markdown","4e9832fe":"markdown","e5767fe1":"markdown","7215b88c":"markdown","691ba992":"markdown","a180f091":"markdown","94979a8f":"markdown","f6cd4723":"markdown","7ac286aa":"markdown","426bdf9f":"markdown","2813fabc":"markdown","9ead54ff":"markdown","6afd8ae3":"markdown","aa7576f2":"markdown","08cf438d":"markdown","e13c8611":"markdown","e6253043":"markdown","ba69999a":"markdown","15aca402":"markdown","e9e77f7c":"markdown","af607a2c":"markdown","c23879ac":"markdown","bbdf05f4":"markdown","92094ad5":"markdown","5951503a":"markdown","c293d166":"markdown","6fc88c67":"markdown","8457e3cf":"markdown","bed08e23":"markdown","f08389fb":"markdown","553e59e9":"markdown","49ea6eba":"markdown","a2df19ce":"markdown","7373c059":"markdown","05b63e3c":"markdown","0634e9a6":"markdown","4ab77b0b":"markdown","46d8291f":"markdown","3aa1070c":"markdown","a0a40c66":"markdown","df2eba5b":"markdown"},"source":{"63188a25":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n#Importing Packages\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy.random as nr\nimport scipy.stats as ss\nimport math\nimport statsmodels.stats.weightstats as ws\nfrom statsmodels.stats.power import tt_ind_solve_power\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\nfrom sklearn import preprocessing\nimport sklearn.model_selection as ms\nfrom sklearn import linear_model, mixture\nimport sklearn.metrics as sklm\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering, DBSCAN\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import StandardScaler\n\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e6a33d0e":"#Importing the Data\nins = pd.read_csv('..\/input\/insurance.csv')\nins.head()","eb9db14e":"ins.shape","1f24a3f9":"ins.dtypes","6bc7ad3e":"#Checking for the Type of the Array\ntype(ins)","1d892abb":"#Checking for missing Data\nfor col in ins.columns:\n    print (col + ' ' +'missing values:' + str((ins[col].isnull().sum())) or str(ins[col].isna().sum()))","9468730c":"#Describing the Statistical Properties of the Data\nins.describe().round(3)","8b7c7a13":"#Inspecting the corrolation between the features\nins.corr()","0d0f0c56":"corrs = ins[['age', 'bmi', 'charges', 'children']].corr()\nsns.heatmap(corrs, linewidths = 0.5, annot=True, center=0, cmap=\"YlGnBu\")","448c320e":"#looking at the corrlation between charges and the rest of the numerical data.\nins.corr()['charges']","1ebaf709":"def r_z(r): ## transform distribution\n    return math.log((1 + r) \/ (1 - r)) \/ 2.0\n\ndef z_r(z): ## inverse transform distribution \n    e = math.exp(2 * z)\n    return((e - 1) \/ (e + 1))\n\ndef r_conf_int(r, alpha, n):\n    # Transform r to z space\n    z = r_z(r)\n    # Compute standard error and critcal value in z\n    se = 1.0 \/ math.sqrt(n - 3)\n    z_crit = ss.norm.ppf(1 - alpha\/2)\n\n    ## Compute CIs with transform to r\n    lo = z_r(z - z_crit * se)\n    hi = z_r(z + z_crit * se)\n    return (lo, hi)\n\ndef correlation_sig(df, col1, col2):\n    pearson_cor = ss.pearsonr(x = df[col1], y = df[col2])\n    conf_ints = r_conf_int(pearson_cor[0], 0.05, 1000)\n    print('Correlation: ' + col1 + ' and ' + col2 + ' = %4.3f with CI of %4.3f to %4.3f and p_value %4.3e' \n        % (pearson_cor[0], conf_ints[0], conf_ints[1], pearson_cor[1]))\n\ncorrelation_sig(ins, 'charges', 'age') \ncorrelation_sig(ins, 'charges', 'bmi')\ncorrelation_sig(ins, 'charges', 'children')","cdd0b4e3":"\nins.hist(layout = (3, 3), figsize=(12, 9), color='blue', grid=False, bins=15)","885ff4a0":"#Setting up the Frequency Table\ndef count_unique(ins, cols):\n    for col in cols:\n        print('\\n' + 'For column ' + col)\n        print(ins[col].value_counts())\n\ncat_cols = ['age', 'charges', 'bmi', 'children', 'sex', \n            'smoker']\ncount_unique(ins, cat_cols)","6a3df837":"# Visualizing the frequency of the smoker category:\ncounts = ins['smoker'].value_counts() \ncounts.plot.bar(color = 'blue', grid=False) \n      ","92d23abb":"#For the numeric data, we can compare them with the combined histogram and KDNs plot:\ndef plot_density_hist(ins, cols, bins = 10, hist = False):\n    for col in cols:\n        sns.set_style(\"whitegrid\")\n        sns.distplot(ins[col], bins = bins, rug=True, hist = hist)\n        plt.title('Histogram of ' + col) \n        plt.xlabel(col) \n        plt.ylabel('')\n        plt.show()\n        \nplot_cols = ['bmi', 'charges', 'age', 'children']\nplot_density_hist(ins, plot_cols, bins = 20, hist = True)   ","f962fd4d":"#Checking the skewness of charges\nskew = ss.skewtest(ins['charges'])\nskew","c6450d98":"pair_cols = [\"age\", \"charges\", \"children\", \"bmi\", 'smoker', 'sex'] \nsns.pairplot(ins[pair_cols], hue='smoker', palette=\"Set2\", diag_kind=\"kde\", size=2)\n","aabab3b3":"def plot_scatter(ins, cols, col_y = 'charges'):\n    for col in cols:\n        fig = plt.figure(figsize=(7,6)) # define plot area\n        ax = fig.gca() # define axis   \n        ins.plot.scatter(x = col, y = col_y, ax = ax)\n        ax.set_title('Scatter plot of ' + col_y + ' vs. ' + col) \n        ax.set_xlabel(col) \n        ax.set_ylabel(col_y)\n        plt.show()\n\nnum_cols = ['age', 'bmi', 'children']\nplot_scatter(ins, num_cols)   ","89dd36d7":"def plot_desity_2d(ins, cols, col_y = 'charges', kind ='kde'):\n    for col in cols:\n        sns.set_style(\"whitegrid\")\n        sns.jointplot(col, col_y, data=ins, kind=kind, )\n        plt.xlabel(col) # Set text for the x axis\n        plt.ylabel(col_y)# Set text for y axis\n        plt.show()\n\nplot_desity_2d(ins, num_cols)","bddd7906":"sns.jointplot('age', 'children', data=ins, kind='kde')","6e537074":"ins_smoker = ins[ins.smoker == 'yes']\ncorrs = ins_smoker[['age', 'bmi', 'charges', 'children']].corr()\nsns.heatmap(corrs, linewidths = 0.5, annot=True, center=0, cmap=\"YlGnBu\")\nplt.title('Smokers')","d42c88d5":"ins_non_smoker = ins[ins.smoker == 'no']\ncorrsn = ins_non_smoker[['age', 'bmi', 'charges', 'children']].corr()\nsns.heatmap(corrsn, linewidths = 0.5, annot=True, center=0, cmap=\"YlGnBu\")\nplt.title('Non-Smokers')","945f4593":"\nsns.lmplot('age', 'charges', ins, x_jitter=.15, y_jitter=.15, col=\"smoker\",hue='sex', scatter_kws={'alpha':0.1}, fit_reg = True)\nsns.lmplot('bmi', 'charges', ins, x_jitter=.15, y_jitter=.15, col=\"smoker\", hue='sex',scatter_kws={'alpha':0.1}, fit_reg = True)\nsns.lmplot('children', 'charges', ins, x_jitter=.15, y_jitter=.15, col=\"smoker\",hue='sex', scatter_kws={'alpha':0.1}, fit_reg = True)\n","6dc9971f":"# We now compare charges by grouping them by the smoker category and look at the mean and std.\ncharges_grouped = ins[['charges','smoker']].groupby('smoker')\nprint(' Mean by smoker')\nprint(charges_grouped.mean().round(2))\nprint('\\n Standard deviation by smoker')\nprint(charges_grouped.std().round(2))","e1672267":"\ndef t_test_two_samples(a, b, alpha, alternative='two-sided'):\n    diff = a.mean() - b.mean()\n    res = ss.ttest_ind(a, b)\n    means = ws.CompareMeans(ws.DescrStatsW(a), ws.DescrStatsW(b))\n    confint = means.tconfint_diff(alpha=alpha, alternative=alternative, usevar='unequal') \n    degfree = means.dof_satt()\n    index = ['DegFreedom', 'Difference', 'Statistic', 'PValue', 'Low95CI', 'High95CI']\n    return pd.Series([degfree, diff, res[0], res[1], confint[0], confint[1]], index = index)   \n   \ntest = t_test_two_samples(ins.loc[ins.smoker == 'yes', 'charges'], ins.loc[ins.smoker == 'no', 'charges'], 0.05)\ntest","93cceef2":"d = (32050.23 - 8434.27)\/(np.std(ins.loc[ins.smoker == 'yes', 'charges']))\nprint('d = ' + str(d))\ntt_ind_solve_power(effect_size=d, nobs1 = 1337, alpha=0.05, power=None, ratio=1, alternative='two-sided')","0593dbae":"tt_ind_solve_power(effect_size=2, nobs1 = None, alpha=0.05, power=0.8, ratio=1, alternative='two-sided')","e924a845":"charges_grouped_sex = ins[['charges','sex']].groupby('sex')\nprint(' Mean by sex')\nprint(charges_grouped_sex.mean().round(2))\nprint('\\n Standard deviation by sex')\nprint(charges_grouped_sex.std().round(2))","f75d316a":"#We determine the power of the t-test by first claculating d-value:\nd = (13956.75 - 12569.58)\/(np.std(ins.loc[ins.sex == 'male', 'charges']))\nprint('d = ' + str(d))\ntt_ind_solve_power(effect_size=d, nobs1 = 1337, alpha=0.05, power=None, ratio=1, alternative='two-sided')","ab683925":"tt_ind_solve_power(effect_size=0.107, nobs1 = None, alpha=0.05, power=0.8, ratio=1, alternative='two-sided')","b5f40fdd":"#The Above difference due to smoking can be illustrated through Box and violin charts as seen below\ndef plot_box(ins, cols, col_y = 'charges'):\n    for col in cols:\n        sns.set_style(\"whitegrid\")\n        sns.boxplot(col, col_y, data=ins)\n        plt.xlabel(col) \n        plt.ylabel(col_y)\n        plt.show()\n        \ncat_cols = ['sex', 'smoker', 'region', 'children']\nplot_box(ins, cat_cols)  ","73f55052":"#Violin Plot: it helps get a sense on the effect each category has on the charges label:\ndef plot_violin(ins, cols, col_y = 'charges'):\n    for col in cols:\n        sns.set_style(\"whitegrid\")\n        sns.violinplot(col, col_y, data=ins,  hue='smoker')\n        plt.xlabel(col) \n        plt.ylabel(col_y)\n        plt.show()\ncat_cols = ['sex', 'smoker', 'region', 'children']       \nplot_violin(ins, cat_cols)","a914b9f7":"ins['charges_log'] = np.log(ins['charges'])\nplt.hist(ins['charges_log'], bins = 10)","9b20aae1":"ins.corr()['charges_log']","1d1b52f7":"Features = ins['smoker']\nenc = preprocessing.LabelEncoder()\nenc.fit(Features)\nFeatures = enc.transform(Features)\nohe = preprocessing.OneHotEncoder()\nencoded = ohe.fit(Features.reshape(-1,1))\nFeatures = encoded.transform(Features.reshape(-1,1)).toarray()\n\ndef encode_string(cat_feature):\n    enc = preprocessing.LabelEncoder()\n    enc.fit(cat_feature)\n    enc_cat_feature = enc.transform(cat_feature)\n    ## Now, apply one hot encoding\n    ohe = preprocessing.OneHotEncoder()\n    encoded = ohe.fit(enc_cat_feature.reshape(-1,1))\n    return encoded.transform(enc_cat_feature.reshape(-1,1)).toarray()\n    \ncategorical_columns = ['sex', 'region']\n\nfor col in categorical_columns:\n    temp = encode_string(ins[col])\n    Features = np.concatenate([Features, temp], axis = 1)\n\nprint(Features.shape)\nprint(Features[:4, :])  ","d52fb0fd":"#Now we concatenate the numeric features together\nFeatures = np.concatenate([Features, np.array(ins[['age', 'bmi', 'children']])], axis = 1)\nFeatures[:2,:]","bea37c44":"#We split the data set using a Bernoulli random sampling and we use 200 smaples to be used as tests.\nnr.seed(1234)\nlabels = np.array(ins['charges_log'])\nindx = range(Features.shape[0])\nindx = ms.train_test_split(indx, test_size = 200)\nx_train = Features[indx[0],:]\ny_train = np.ravel(labels[indx[0]])\nx_test = Features[indx[1],:]\ny_test = np.ravel(labels[indx[1]])","49b27c62":"#We must now scale the numeric values so the larger ones will not bias the model.\nscaler = preprocessing.StandardScaler().fit(x_train[:,8:])\nx_train[:,8:] = scaler.transform(x_train[:,8:])\nx_test[:,8:] = scaler.transform(x_test[:,8:])\nprint(x_train.shape)\nx_train[:6,:]","a3dc214c":"#We can now build the ML model using linear regression package of scikit-learn:\nlin_mod = linear_model.LinearRegression(fit_intercept = False)\nlin_mod.fit(x_train, y_train)\nprint(lin_mod.intercept_)\nprint(lin_mod.coef_)","fe8bc035":"def print_metrics(y_true, y_predicted, n_parameters):\n    ## computing R^2 and the adjusted R^2\n    r2 = sklm.r2_score(y_true, y_predicted)\n    r2_adj = r2 - (n_parameters - 1)\/(y_true.shape[0] - n_parameters) * (1 - r2)\n    \n    print('Mean Square Error      = ' + str(sklm.mean_squared_error(y_true, y_predicted)))\n    print('Root Mean Square Error = ' + str(math.sqrt(sklm.mean_squared_error(y_true, y_predicted))))\n    print('Mean Absolute Error    = ' + str(sklm.mean_absolute_error(y_true, y_predicted)))\n    print('Median Absolute Error  = ' + str(sklm.median_absolute_error(y_true, y_predicted)))\n    print('R^2                    = ' + str(r2))\n    print('Adjusted R^2           = ' + str(r2_adj))\n   \ny_score = lin_mod.predict(x_test) \nprint_metrics(y_test, y_score, 6) ","39d67dab":"def hist_resids(y_test, y_score):\n    \n    resids = np.subtract(y_test.reshape(-1,1), y_score.reshape(-1,1))\n\n    sns.distplot(resids)\n    plt.title('Histogram of residuals')\n    plt.xlabel('Residual value')\n    plt.ylabel('count')\n    \nhist_resids(y_test, y_score)   ","032f4575":"def resid_qq(y_test, y_score):\n    resids = np.subtract(y_test.reshape(-1,1), y_score.reshape(-1,1))\n    ss.probplot(resids.flatten(), plot = plt)\n    plt.title('Residuals vs. predicted values')\n    plt.xlabel('Predicted values')\n    plt.ylabel('Residual')\n    \nresid_qq(y_test, y_score)   ","a2ed6b13":"def resid_plot(y_test, y_score):\n    resids = np.subtract(y_test.reshape(-1,1), y_score.reshape(-1,1))\n    sns.regplot(y_score, resids, fit_reg=False)\n    plt.title('Residuals vs. predicted values')\n    plt.xlabel('Predicted values')\n    plt.ylabel('Residual')\n\nresid_plot(y_test, y_score) ","79d55adb":"y_score_untransform = np.exp(y_score)\ny_test_untransform = np.exp(y_test)\nresid_plot(y_test_untransform, y_score_untransform)","f3f58d3a":"adab = AdaBoostRegressor()  \nadab.fit(x_train, y_train)","0de4a142":"yada_score= adab.predict(x_test)","ba87951e":"print_metrics(y_test, yada_score, 6) ","78dfbc63":"def hist_resids(y_test, yada_score):\n    \n    resids = np.subtract(y_test.reshape(-1,1), yada_score.reshape(-1,1))\n\n    sns.distplot(resids)\n    plt.title('Histogram of residuals')\n    plt.xlabel('Residual value')\n    plt.ylabel('count')\n    \nhist_resids(y_test, yada_score) ","a4205513":"yada_score_untransform = np.exp(yada_score)\ny_test_untransform = np.exp(y_test)\nresid_plot(y_test_untransform, yada_score_untransform)","ee006305":"label = np.array(ins['smoker'])\nenc = preprocessing.LabelEncoder()\nenc.fit(label)\nlabel = enc.transform(label)\nlabel.shape","e37360d2":"#We proceed as before.\ndef encode_string(cat_features):\n    enc = preprocessing.LabelEncoder()\n    enc.fit(cat_features)\n    enc_cat_features = enc.transform(cat_features)\n    ohe = preprocessing.OneHotEncoder()\n    encoded = ohe.fit(enc_cat_features.reshape(-1,1))\n    return encoded.transform(enc_cat_features.reshape(-1,1)).toarray()\n\ncategorical_columns = ['sex']\n\nFeatures = encode_string(ins['region'])\nfor col in categorical_columns:\n    temp = encode_string(ins[col])\n    Features = np.concatenate([Features, temp], axis = 1)\n\nFeatures = np.concatenate([Features, np.array(ins[['bmi', 'charges','children', 'age']])], axis = 1)\nprint(Features.shape)\nprint(Features[:2, :])","883f9673":"nr.seed(1144)\nindx = range(Features.shape[0])\nindx = ms.train_test_split(indx, test_size = 200)\nx_train = Features[indx[0],:]\ny_train = np.ravel(label[indx[0]])\nx_test = Features[indx[1],:]\ny_test = np.ravel(label[indx[1]])\nx_train.shape\n","748dbc24":"#Scaling the numeric values\nscaler = preprocessing.StandardScaler().fit(x_train[:,6:])\nx_train[:,6:] = scaler.transform(x_train[:,6:])\nx_test[:,6:] = scaler.transform(x_test[:,6:])\nx_train[:2]","17bdd375":"logistic_mod = linear_model.LogisticRegression(fit_intercept = False, class_weight={0:0.8, 1:0.2}) \nlogistic_mod.fit(x_train, y_train)\nprint(logistic_mod.intercept_)\nprint(logistic_mod.coef_)","ae63bdee":"probabilities = logistic_mod.predict_proba(x_test)\nprint(probabilities[:5,:])","0537225d":"def score_model(probs, threshold):\n    return np.array([1 if x > threshold else 0 for x in probs[:,1]])\nscores = score_model(probabilities, 0.5)\nprint(np.array(scores[:100]))\nprint(y_test[:100])","41a4ec55":"#We set up a confusion table to examine our scores:\ndef print_metrics(label, scores):\n    metrics = sklm.precision_recall_fscore_support(label, scores)\n    conf = sklm.confusion_matrix(label, scores)\n    print('                 Confusion matrix')\n    print('                 Score positive    Score negative')\n    print('Actual positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])\n    print('Actual negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])\n    print('')\n    print('Accuracy  %0.2f' % sklm.accuracy_score(label, scores))\n    print(' ')\n    print('           Positive      Negative')\n    print('Num case   %6d' % metrics[3][0] + '          %6d' % metrics[3][1])\n    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])\n    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])\n    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])\n   \nprint_metrics(y_test, scores) ","498ca6be":"#Plotting the ROC curve\ndef plot_auc(label, probs):\n    fpr, tpr, threshold = sklm.roc_curve(label, probs[:,1])\n    auc = sklm.auc(fpr, tpr)\n        \n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, color = 'orange', label = 'AUC = %0.2f' % auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n    \nplot_auc(y_test, probabilities)  ","ba7ffbd3":"col_dic = {0:'blue',1:'green',2:'orange',3:'gray'}\nkmeans = KMeans(n_clusters=2, random_state=0, n_init=20, algorithm='full', copy_x=True, verbose=0)\nassignments_km = kmeans.fit_predict(ins[['bmi', 'age', 'children']])\nassign_color_km = [col_dic[x] for x in assignments_km]\nc = assign_color_km\nf, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(9, 4.5), tight_layout=True)\nax1.scatter(ins['bmi'], ins['charges'], color=c)\nax2.scatter(ins['age'], ins['charges'], color=c)\n","38ceb87c":"agc_2 = AgglomerativeClustering(n_clusters=3, linkage='complete', affinity='manhattan')\nassignments_ag2 = agc_2.fit_predict(ins[['bmi', 'age', 'children']])\nassign_color_ag2 = [col_dic[x] for x in assignments_ag2]\nc1=assign_color_ag2\nplt.scatter(ins['age'], ins['charges'], color=c1)","7e5df282":"mix = mixture.GaussianMixture(n_components = ins[['bmi', 'age', 'children']])\nassign_color_mix = [col_dic[x] for x in assignments_ag2]\nc=assign_color_mix\nplt.scatter(ins['age'], ins['charges'], color=c)\n","9181b6c2":"**From the above plots, scatter and contour we infer the following:**  \n1- No plot exhibits a colinear behavior with another one, which indicates that the effect of these categories on charges are most likely independent from each other and should all be considered in the machine learning model.  \n2- The charges-age plot:  \nIt exhibits a strange pattern with what it looks like three parallel lines. the lowest one is the denser one with the charges increasing with age, which makes sense. However, we then have something like a jump in the charges leaving an empty area and then another pattern resembling the lower one, but with less density. Then we have another jump and another similar behavior but with a lower density than the previous two. \nMy initial guess is this has to do with how insurance companies work; that insurance payments jumps in cost from a certain package to another one.  \n3- Charges-children plot:  \nWe see that the charges are somehow fairly distributed between the families having 0-3 children, with some outliers of course. Charges drops for families having more than 3 children, which is very interesting. Do larger families live healthier lives than those with less children? Or maybe they have no time or resources to take a very good care of themselves so they neglect some health issues, unless it is very urgent, compared to those with smaller families who have better chances of taking much care about themselves?  \nNow, the above chart indicates that even though most of the younger generation have low average medical charges, as expected, some, on the other hand, have high charges. This most probably is not related to children, as those younger-agers don't seem to have children as shown in the plot below between the age-children categories.  So, some of the younger generation are being charged with high medical charges, same as those of 50 years and older. And as we are going to see later on, those are of the smokers = yes category.  \n4-charges-bmi plot:  \nNow this is a very interesting plot. Here we see two distributions of the data:  \nOne horizontal, which is the major one as obvious from the contour plot, and another less-dense diagonal one, which I believe is the more logical one, with the health charges increasing with the increase of the bmi index all the way to obesity.  \nThe horizontal line is centered around 30 bmi, close to the accepted values, indicating that those people with average weight suffer less health problems than those overweight.  \nHowever, there is some proportion of those who are really overweight but with small insurance charges. This could be due to either:  \n     1- those people having no medical problems,  \n     2- or they still young of age, which is very common these days, so the health risks of their overweight has not yet manifested,   \n     3-or they have not the financial ability to take a good care of themselves and pay high medical insurance rates.  \n","25a43c89":"It looks ok, but diverging on the far right. To examine more we plot a residual plot:","b6e79f42":"We can see that the correlation between the data is positive but weak. \nThe highest correlation is between medical charges and age (0.3), which is not that big either. ","64614aa0":"We see that the model is not that good after all. There is a big divergent, especially for large values of charges. \nThis, I believe, is mainly due to the divergent behavior charges exhibited with the smoker category. So I believe that we need to do the same modeling but for the smokers and non-smokers separately and see how it will work.\nBut before that I will do another regression using the Adaboost model and see if I can get a better fitting.","f1a5f081":"**Procedures:**  \n    1- Visualizing and inspect the Data to get as much an insight about it as we can.  \n    2- Clean, Transform and Feature Engineer the data as needed.  \n    3- Construct Machine Learning Models and test them.  \n","8527781a":"To further check my young-age\/few-children assumption, I plot these two categories together using a joint KDE plot to have a better visualization. Form the plot, we find that the young-agers and old-agers are the ones with the least number of children\/dependents as is expected, however, with the denser area is around the young agers. ","0ec46e7e":"The magnitude of some of the coefficients is too large (10^12) which makes me wonder! \n We move now to evaluating the model to see how accurate it is:","83b1d08f":"It is obvious how much the smoker and non-smoker differ in their medical charges, and also by their standard deviation; std of smokers is more than double of that of non-smoker. Hence while non-smokers seem to have their medical charges varying little about the mean, in other words, their medical health seems to be, more or less, similar. Non-smokers, on the other hand, vary a lot around the mean, which indicates that their bodies respond differently to smoking (e.g. some may develop lung cancer while others won't)","4e9832fe":"The accuracy of the classification model is 94% and of the 200 test values, the model only miss-labeled 13 of them. This is also obvious from the ROC curve with an area of 99%.   \nThis seems too good to be true, if it is not for the fact that, as we saw earlier, smoker and nonsmoker values are very much separated. This makes it much easier for the system to predict them correctly.  \nNevertheless, the model seems to have missed a big chunk of the smoker category only 66% accurate.  \nInterestingly, when I ran the same model but without class_weight, the results were much better, only 5 missed from the non-smoker category and with accuracy of 99%. So, I guess when the two classification labels are very much separated, maybe it is better not to use weight correction.  \nThat been said, a new set of data that may have a larger overlapping between the smoker two cases, may not be predicted as well as this one did.  ","e5767fe1":"Eventhogh the number of features in this dataset is relatively small, still it tells a lot.  \nOne very important story we get from this dataset is the effect smoking has on our lives, as well as on our pockets.  \nAs we will discover, smoking pops out as the main factor behind the variance of the data, especially on charges, which is an indicator on the overall well-being of the sample's  individuals.  \nIn fact, the difference between smokers and non-smokers is so striking that predicting whether someone is a smoker or not from the data via a ML model is proved to be so easy and so precise, with 99% accuracy.   \nThus, along with the other interesting insights we gained form this table, I hope that this data will be a warning for smokers, and non-smokers, about the hazardous nature of this substance.","7215b88c":"Below are the probabilities of smoker category, yes or no, using the logistic function.","691ba992":"# Classification\nNow I do a classification model to predict whether someone is a smoker or not from his insurance data. \nFirst, we start by digitizing the smoker category, however not in a binary code, but in single digit for each subcategory, e.g. 1 for Yes and 0 for no. This is so we won't get a double entry into the algorithm model. This is why I don't use the one-hot-encoder here.","a180f091":"To appreciate the above value of 5, we run the same test but for sex category now, as we find that the difference that sex imply on the charges is minimal. ","94979a8f":"We start by encoding the categorical features into binary dummy variables and then using the hot-key-method. For the sex and smoker categories, the binary code will consist of double digits [1, 0] or [0, 1] because we only have two sub categories in each (e.g. male\/female, smoker\/non-smoker).  \nFor the region category we will have 4 digits (3 zeros and one 1) because we have four subcategories to take into account.","f6cd4723":"It looks good, somehow skewed to the right.  \nNext we plot the residuals vs the predicted values in a Q-Q plot:","7ac286aa":"Now we set a threshold value to turn the above probabilities into binary digits of 1 and 0 to compare with the test values. We chose a threshold of 0.5.","426bdf9f":"We now run  a 2-tailed t-test to test whether the difference we see above in the smoking categeory could be due to chance, in other words, due to sampling error (H0) or is it a real difference (H1) ","2813fabc":"As obvious from the above two plots, the classification model is not working properly. For the bmi-charges I was expecting two clusters for the corresponding smokers and non-smokers categories. This is not what I got obviously.  \nFor the age-charges plot, it is obvious we need to use 3 k's instead of 2, and also to use a different clustering model than K-means, which I show below for only two of the clustering models that I used.\nStill, I didn't get what I was expecting. I tried many other clustering models, but I still couldn\u2019t get a clustering of the three separated horizontal lines.\nAny help or advice regarding clustering will be appreciated.","9ead54ff":"Both *d* and *power* are very large (100% power), indicating that the number of samples (1337) is more than adequate to detect the difference in the two categories.  \nSo, how many participants do we need to get this difference and for a power let say = 0.8 (80%)?","6afd8ae3":"Next we determine the power of our t-test by first calculating the d-value (size effect) which is the difference between the means of the smokers and non-smokers, divided by the std of the smoker=yes category","aa7576f2":"# Clustering\nI now try to build a clustering model. ","08cf438d":"It looks fine, but with some divergence. However, all the above was done using the charges_log. We need now to go back to the original form of the charges and test the model.","e13c8611":"Both d and power are much smaller now, the power is till ok though 70%, this is because the sample size is large enough.\nSo, how many participant do we need to get this difference and for a power let say = 0.8 (80%)?","e6253043":"It is obvious from the above plots that smoking has a huge effect on how charges is related to the other categories. In fact, the strange behavior we saw earlier in the previous scatter plots are almost gone when we separated them based on the smoking category.  \nIn two plots, the smoker category showed a gap in the charges data. this could be due the smoking habits of the smokers; those heavy smokers require more medical charges than moderate or light smokers.   \nIt is also very interesting to see how young adults with no children and are smokers differ very much in their charges than their non-smoking peers. Hence we conclude that smoking is a very big problem for young adults today.  \nSex, on the other hand, doesn't seem to produce a noticeable difference. \nI ran the same test but for the region category and I didn't find that much of a difference between the four regions.\n","ba69999a":"5 only! \nIn other words, we only needed 5 participants to be able to detect the difference in medical charges between smokers and non-smokers.\nThis should be considered a real warning to smokers.","15aca402":"From the above box plot we find again that the biggest effect on  charges is the smoker category.","e9e77f7c":"Next, we calculate the confidence intervals  CI for the corrolation values * r* between the various categories. This requires transformation from the r-space into the z-space and then back to the r-space. ","af607a2c":"The most striking Violin plot is between Charges and smoker, where the non-smoker plot is horizontally flattened out, minimizing its effect on the amount of medical charges, while that of smokers is vertically extended indicating a large positive correlation between smoking and the required medical charges.","c23879ac":"1. It is obvious from the above pairplot the huge impact  smoker category has on the insurance charges rates, compared to the other categories. In fact, the smoker=yes are the main reason behind the bump we saw earlier in the charges histogram, between the values of 30000 and 50000, the one responsible for the right-skewing of the chart.   \n2. Also, smokers with children between 1 and 3, have higher insurance charges than non-smokers with the same number of children.  \n3. Smoking doesn't seem to have an effect on the average bmi of the sample. \n4. age does have an effect on raising the charges, but again it is when combined with smoking that the charges get substantially bigger.\n","bbdf05f4":"The value of *t* is very big, as expected, = 46, and the p-value is basically 0, hence for alpha = 0.05 we reject the null hypothesis H0 and we accept H1; that what we are seeing cannot be generated by mistake.\nHowever, the confidence interval is very big, on the order 10^4, and both values of the interval are positive, which indicates  we are 95% confident that the difference between the charges of  smokers and non-smokers is always on the order of 10^4 more, and never less.","92094ad5":"**Histogram Observations:**  \n-We can see that only the bmi feature is normally distributed, with a mean slightly above the maximum accepted value = 30.  \n-Age seems to be uniformly distributed, except for the young ages at the far left, where we have more data coming from this age group.  \n-Charges and children features are right-skewed.  \n-For children, this is expected,  as  people prefer to have few (or no children) these days than to have larger fmailies. And also, as parnets  get older, their children won't be cosidred as dependents anymore.  So it makes sense to have more smaples with fewer children.  \n-The skewness of Charges indicates that there are few people who are being charged higher than average. This may lead to some bias in the study.\n-The skwness and non-normal distibution of these categories is partly responsible for the low correlation we find between them.","5951503a":"The CI around the r values are small, hence the r values obtained are well defined. And so are the p-values. However, \nthe p-value of Charges\/children is much bigger than the other two, which are basically = 0. But still it is less thna the threshold value of alpha = 0.05.","c293d166":"# Regression  \nWe start with the regression model. The steps we need to take are as follows:  \n1- Transform the label value (charges) into a more normal distribution (which we already did),  \n2- Transform the datafram into a numpy array to be read by scikit.learn package,  \n3- Transform the categorial values into binary dummy variables.  \n4- Split the data set into train and test data sets,   \n5- Scale the numeric variables as to all have the same weight in the machine learning algorithm,    \n6-Fit the linear regression model using sciki.learn package,  \n7- Test and evaluate the performance of the ML model and see if it need improvement.","6fc88c67":"We now build the non-linear model. Notice that I included a class-weight correction to take into consideration that the number of smokers is much less than nonsmoker.","8457e3cf":"This is close to our own sample population, however, it is orders of magnitude greater than 5. Hence, detecting a difference in charges based on smoker category is much easier than doing the same based on gender.","bed08e23":"As we saw previously, the charges plot suffers a very big right-skew. This is not good for machine learning. So, we need to do some data engineering first, such as taking the log of charges to make them more normally distributed.","f08389fb":"Well, we can gain a better insight on the above charts by looking at the correlation between heat map for the two categories of smoker feature separately. As shown below, we see that when we look at smokers only, the correlation between charges and bmi increases noticeably, (from 0.2 to 0.81) while charges-age corr. increases slightly. However, when looking at non-smokers only, it is the correlation between age and charges that increases now, while bmi-chrages corr. actually decreases.  \nWhat this implies is the following:  \nfor non-smokers, it is logical that their medical charges will increase with age. in fact, for those people, even high bmi doesn't seem to affect their medical charges much.  \nHowever, for smokers, their high bmi starts to pose greater risk on their health, and starting from a younger age (thus the age doesn\u2019t seem to be as important anymore). Thus, smoking and obesity are a very dangerous mix.\n","553e59e9":" # To be continued... Maybe!","49ea6eba":"# About the Data   \n**age**: Age of primary beneficiary.  \n**sex**: Insurance contractor gender: female, male.   \n**bmi**: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight (kg \/ m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9.   \n**children**: Number of children covered by health insurance \/ Number of dependents.  \n**smoker**: Whether the contractor is a smoker or not.  .  \n**region**: The beneficiary's residential area in the US, northeast, southeast, southwest, northwest.  \n**charges**: Individual medical costs billed by health insurance.  \n","a2df19ce":"# Machine Learning\nWe now conduct the following ML models on the data:  \n1- A simple regression model to predict charges from the date,  \n2-An Adaboost model to predict the charges form the data,  \n3- A classification model to predict whether some is a smoker or non smoker,  \n4- A clustering model.\n","7373c059":"We now do a 2-D scatter plot between the numerical values and charges. This will help us dig deeper into the relationships between \nthese variables especially, the relationship charges have with the other variables as to give us an insight on how to proceed in building the machine learning model. ","05b63e3c":"Let us use the scatter plot to find the relationship between the charges and the rest of the features depending on smoking as a column and sex as a hue.","0634e9a6":"Checking Data Types","4ab77b0b":"The data is much clearer now. (Notice how all the data is normalized on the y axis such that the area under the plot is equal to 1.)  \n1- Notice how the bmi is fairly normal with few outliers to the right, but not much to make a noticable skew.  \n2- We knew that the charges were right-skewed but now we can see a samll bump between 30000 and 40000$ whcih we may need to investigate further.  \n3- Again, the number of participants around the age of 20 is too large compared to the rest. this could be the reason why the \nchildren plot is very much skewd to the right and has no bell shape at all.","46d8291f":"As obvious from the above, this model is far better than the linear regression one. the MSE is much smaller '0.02' and the residuals are very few and with much smaller dispersion, between 5000 and -5000 compared to +30000 and -50000 in the previous model. Hence this is a far superior model in this case.","3aa1070c":"We plot the residuals in a histogram, the more normal and narrow the plot around 0 the better the model is.","a0a40c66":"The above frequency table help us get some insight about the data, especially for the categorical ones:  \n1- The sex column indicates that we have a fair distribution between males and females, which is good for machine learning.  \n2- However, the smoker is not evenly distributed; the non-smokers are much more than the smoker ones. This could cause a problem when building a ML model. We may need to increase the weight of the on-smoker category.  \n3- Interestingly, almost half of the sample have no children. Those most probably be of the younger age group (around 22 years) which was a large group as we saw earlier from the histogram plot. Also, older parents will not have their children as dependent, so they will not be counted.  \n4- The ages of the sample ranges from 22 to 69 years old.  \n5- The sample is fairly distributed between the four considered regions.","df2eba5b":"The above plots suffer from a some over-plotting of the data, especially the bmi-charges one. To resolve this we can use transparency in the data points or use a different type of plot, the contour plot."}}