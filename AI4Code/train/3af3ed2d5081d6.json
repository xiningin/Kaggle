{"cell_type":{"c4f30996":"code","f17858d7":"code","a2f95403":"code","724d4e65":"code","122f0ca7":"code","66f2ac62":"code","e552f427":"code","65d42c01":"code","8398726d":"code","03f607e5":"code","9bb5dd55":"code","c4dc67b3":"code","286b3e36":"code","7efc447b":"code","3eaf60a9":"code","7925179b":"code","588e3eac":"code","3cf8a50d":"code","dcb8c9b3":"code","48d2db6e":"code","aae8eac3":"code","f90e58cf":"code","30375ba6":"code","f7f1e92e":"code","36fc731f":"code","afa3e597":"code","c0e45c38":"code","a832c346":"code","6b4b4db1":"code","e46d4e13":"code","64878285":"code","7d496cde":"code","08392467":"code","af509137":"code","59fe9d42":"code","35883598":"code","f14235d4":"code","0f1f5377":"code","5b05c86b":"code","86b174f5":"code","e2916b16":"code","54c2c009":"code","b1b5b5b4":"code","a609acba":"code","d28361bf":"code","0ced61dc":"code","bdc1d5cb":"code","ea58245c":"code","8de85bb4":"code","5938a57a":"code","5cd72d20":"code","c9544954":"code","52d89342":"code","feeb29ed":"code","6a851dfc":"code","abf75c0f":"code","01b2617b":"code","1c401f8a":"code","a1c7c4a3":"code","40c2a196":"code","48363f53":"code","d3fe3048":"code","64892c65":"code","560a2bdb":"code","88a3115c":"code","5d75904d":"code","6e4377bc":"code","18463a55":"code","6dbca605":"code","d688b585":"code","5702cda9":"code","1eafc539":"code","ed0faa87":"code","57e76d61":"code","55254468":"code","c68e6bc2":"code","c99a0f3d":"code","275f6301":"code","69d9c923":"code","16fcab73":"code","b86e7f6c":"code","8dcf56f5":"code","ad96d0f9":"code","61d4d915":"code","36a52e0a":"code","531136bb":"code","d5a33fd4":"code","08b6ab6f":"code","7f68d041":"code","42286a07":"code","c0b13815":"code","bb533f99":"code","d7a9c727":"code","43f1d64e":"code","7186cf76":"code","549351ca":"code","c85587cc":"code","5de9fc98":"code","cf35c6eb":"code","ac418a35":"code","c33115d5":"code","04055f09":"code","5423e446":"code","17e23de7":"code","1237326e":"code","82190feb":"code","872133ef":"code","e14cbe21":"code","e7a1343b":"code","bae5945f":"code","a37710c9":"code","90e33541":"code","e00586bb":"code","2864345b":"code","75673af7":"code","90191f77":"markdown","e870c632":"markdown","b1e2ec97":"markdown","bd1f09d3":"markdown","e2049ec0":"markdown","86ce80a1":"markdown","79b1fa1b":"markdown","7ca42124":"markdown","ba1df34d":"markdown","07257ce8":"markdown","983dea9f":"markdown","0cc31605":"markdown","ad7b6314":"markdown","b079a2ab":"markdown","da53890e":"markdown","8ddd50c9":"markdown","fd7151fa":"markdown","59d035e9":"markdown","b71bfbce":"markdown","507ed7e1":"markdown","c01717a4":"markdown","741069bd":"markdown","3275f0c1":"markdown","14674584":"markdown","14b1c2cb":"markdown"},"source":{"c4f30996":"!pip install graphviz","f17858d7":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nfrom datetime import datetime\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgbm\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score, roc_curve\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\n\nfrom sklearn.tree import export_graphviz\nimport graphviz\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nPREPARE_PREDICTION = False","a2f95403":"DIR_PATH = '\/Users\/carlosperezricardo\/Documents\/data\/'\nDIR_PATH = '..\/input\/easymoneygrupo5\/'\n\ncommercial_activity = pd.read_csv(DIR_PATH+'commercial_activity_df.csv', encoding='utf-8')\ncommercial_activity.drop(columns=['Unnamed: 0'], inplace=True)\n\nproducts = pd.read_csv(DIR_PATH+'products_df.csv', encoding='utf-8')\nproducts.drop(columns=['Unnamed: 0'], inplace=True)\n\nsociodemographic = pd.read_csv(DIR_PATH+'sociodemographic_df.csv', encoding='utf-8')\nsociodemographic.drop(columns=['Unnamed: 0'], inplace=True)","724d4e65":"df_whole = pd.merge( commercial_activity, products, on = ['pk_cid','pk_partition'] )\ndf_whole = pd.merge( df_whole, sociodemographic, on=['pk_cid','pk_partition'] )","122f0ca7":"boolean_cols = [\"short_term_deposit\", \"loans\", \"mortgage\", \"funds\", \"securities\",\"long_term_deposit\", \"em_account_pp\", \\\n                \"credit_card\", \"payroll_account\", \"emc_account\", \"debit_card\", \"em_account_p\", \"em_acount\", \"payroll\", \"pension_plan\"] \n\nfor x in boolean_cols:\n    df_whole[x] = df_whole[x].astype(bool)","66f2ac62":"df_whole.head(3).T","e552f427":"del commercial_activity, products, sociodemographic","65d42c01":"partitions = ['2018-01-28','2018-02-28','2018-03-28','2018-04-28','2018-05-28','2018-06-28', \\\n    '2018-07-28','2018-08-28','2018-09-28','2018-10-28','2018-11-28','2018-12-28','2019-01-28', \\\n        '2019-02-28','2019-03-28','2019-04-28','2019-05-28']\n\nlist_products = ['short_term_deposit','loans','mortgage','funds','securities',\n    'long_term_deposit','em_account_pp','credit_card','pension_plan',\n    'payroll_account','emc_account','debit_card','em_account_p','em_acount']\n\nproducts_dict = {\"short_term_deposit\":\"ahorro e inversi\u00f3n\", \"loans\":\"financiaci\u00f3n\", \"mortgage\":\"financiaci\u00f3n\", \n    \"funds\":\"ahorro e inversi\u00f3n\", \"securities\":\"ahorro e inversi\u00f3n\", \"long_term_deposit\":\"ahorro e inversi\u00f3n\", \n    \"em_account_pp\":\"cuenta\", \"credit_card\":\"financiaci\u00f3n\", \"payroll_account\":\"cuenta\", \"pension_plan\":\"ahorro e inversi\u00f3n\", \n    \"emc_account\":\"cuenta\", \"debit_card\":\"financiaci\u00f3n\", \"em_account_p\":\"cuenta\", \"em_acount\":\"cuenta\"}\n\ncost_product = {'cuenta':10, 'ahorro e inversi\u00f3n':40, 'financiaci\u00f3n':60}","8398726d":"def determinar_altas(data):\n    data.columns=['pk_partition','pk_cid','product']\n    data['prev'] = data.groupby('pk_cid')['product'].shift(1)\n    data['diff'] = data['product'] - data['prev']\n\n    # Solo queremos 1 \n    data['diff'] = np.where( (data['product']==1) & (data['diff'].isna()) & (data['pk_partition']!='2018-01-28'), 1, data['diff'] )\n    data['diff'].fillna(0,inplace=True)\n    data['diff'] = np.where( data['diff'] == -1, 0, data['diff'] )\n\n    return data['diff']","03f607e5":"def determinar_bajas(data):\n    data.columns=['pk_partition','pk_cid','product']\n    data['prev'] = data.groupby('pk_cid')['product'].shift(1)\n    data['diff'] = data['product'] - data['prev']\n\n    # Solo queremos -1 pero los codificamos como 1 (m\u00e1s facil) \n    data['diff'] = np.where( (data['product']==1) & (data['diff'].isna()) & (data['pk_partition']!='2018-01-28'), 1, data['diff'] )\n    data['diff'].fillna(0,inplace=True)\n    data['diff'] = np.where( data['diff'] == 0, 0, data['diff'] )\n    data['diff'] = np.where( data['diff'] == -1, 1, data['diff'] )\n\n    return data['diff']","9bb5dd55":"df = df_whole.copy(deep=True)","c4dc67b3":"for prod in list_products:\n    df[prod+'_altas'] = determinar_altas( df_whole[['pk_partition','pk_cid',prod]] )\n    df[prod+'_bajas'] = determinar_bajas( df_whole[['pk_partition','pk_cid',prod]] )\n    print(prod)","286b3e36":"# Total altas\nproducts_dict2 = {\"short_term_deposit\":\"ahorros\", \"loans\":\"financiacion\", \"mortgage\":\"financiacion\", \n    \"funds\":\"ahorros\", \"securities\":\"ahorros\", \"long_term_deposit\":\"ahorros\", \n    \"em_account_pp\":\"cuenta\", \"credit_card\":\"financiacion\", \"payroll_account\":\"cuenta\", \"pension_plan\":\"ahorros\", \n    \"emc_account\":\"cuenta\", \"debit_card\":\"financiacion\", \"em_account_p\":\"cuenta\", \"em_acount\":\"cuenta\"}\n\ninit_cols = ['total_altas_cuenta','total_altas_ahorros','total_altas_financiacion',\n    'altas_cuenta','altas_ahorros','altas_financiacion',\n    'total_bajas_cuenta','total_bajas_ahorros','total_bajas_financiacion'] \n\nfor col in init_cols:\n    df[col] = 0\n\nfor key, value in products_dict2.items():\n    df['total_altas_'+value] += df[key+'_altas']\n    df['altas_'+value] +=  df['total_altas_'+value]\n    df['total_bajas_'+value] += df[key+'_bajas'] \n    \n    # memory optimization\n    df['altas_'+value] = df['total_altas_'+value].astype('int8')\n    df['total_altas_'+value] = df['total_altas_'+value].astype('int8')\n    df['total_altas_'+value] = df['total_bajas_'+value].astype('int8')\n    \n    df.drop([key+'_bajas',key+'_altas'], axis=1, inplace=True)\n\ndf['total_altas'] = df['total_altas_cuenta'] + df['total_altas_ahorros'] + df['total_altas_financiacion']\ndf['total_bajas'] = df['total_bajas_cuenta'] + df['total_bajas_ahorros'] + df['total_bajas_financiacion']","7efc447b":"df['total'] = df['altas_cuenta'] + df['altas_ahorros'] + df['altas_financiacion']","3eaf60a9":"df = df.sort_values(['pk_cid','pk_partition'])\ndf.head(3).T","7925179b":"df['pk_partition'] = pd.to_datetime(df['pk_partition'])","588e3eac":"df.info()","3cf8a50d":"le = LabelEncoder()\nle.fit(df['entry_channel'])\ndf['entry_channel'] = le.transform(df['entry_channel'])","dcb8c9b3":"le = LabelEncoder()\nle.fit(df['gender'])\ndf['gender'] = le.transform(df['gender'])","48d2db6e":"del le","aae8eac3":"def rolling_shift(df, col, window, calc):\n    if calc == 'sum':\n        rolling = df.groupby('pk_cid')[[col]].rolling(window = window, min_periods=1).sum()\n    elif calc == 'max':\n        rolling = df.groupby('pk_cid')[[col]].rolling(window = window, min_periods=1).max()\n    \n    rolling.reset_index(inplace=True)\n    #rolling['shift'] = rolling.groupby('pk_cid')[[col]].shift(1)\n    rolling[col].fillna(0,inplace=True)\n    rolling[col] = rolling[col].astype('int')\n        \n    return rolling[col]","f90e58cf":"bajas_shifts = []\n\ntransf = {'1m':1,'3m':3}\ncols = ['total_bajas','total_bajas_cuenta','total_bajas_ahorros','total_bajas_financiacion']\n\nfor total in cols:\n    for key, window in transf.items():\n        if window == 1:\n            if PREPARE_PREDICTION:\n                df[total+'_'+key+'_shift'] = df[total]\n            else:\n                df[total+'_'+key+'_shift'] = df[total].shift(1)\n        else:\n            df[total] = rolling_shift(df, total, window, 'sum')\n            if PREPARE_PREDICTION:\n                df[total+'_'+key+'_shift'] = df[total]\n            else:\n                df[total+'_'+key+'_shift'] = df[total].shift(1)\n            \n        df[total+'_'+key+'_shift'].fillna(0, inplace=True)\n        df[total+'_'+key+'_shift'] = df[total+'_'+key+'_shift'].astype('int8')\n    \n        bajas_shifts.append(total+'_'+key+'_shift')\n        \n    print(total)\n    df.drop(total, axis=1, inplace=True)","30375ba6":"altas_shifts = []\n\ntransf = {'1m':1,'2m':2,'3m':3,'6m':6}\ncols = ['total_altas','total_altas_cuenta','total_altas_ahorros','total_altas_financiacion']\n\nfor total in cols:\n    for key, window in transf.items():\n        if window == 1:\n            if PREPARE_PREDICTION:\n                df[total+'_'+key+'_shift'] = df[total]\n            else:\n                df[total+'_'+key+'_shift'] = df[total].shift(1)\n        else:\n            df[total] = rolling_shift(df, total, window, 'sum')\n            if PREPARE_PREDICTION:\n                df[total+'_'+key+'_shift'] = df[total]\n            else:\n                df[total+'_'+key+'_shift'] = df[total].shift(1)\n            \n        df[total+'_'+key+'_shift'].fillna(0, inplace=True)\n        df[total+'_'+key+'_shift'] = df[total+'_'+key+'_shift'].astype('int8')\n        \n        altas_shifts.append(total+'_'+key+'_shift')\n\n    print(total)\n    df.drop(total, axis=1, inplace=True)","f7f1e92e":"df['cuentas_fans'] = np.where( (df['total_altas_cuenta_3m_shift'] >= 1), 1, 0 )\ndf['cuentas_fans'] = np.where( (df['total_altas_cuenta_3m_shift'] >= 2), 2, df['cuentas_fans'] )\ndf['cuentas_fans'] = np.where( df['total_altas_cuenta_3m_shift'] >= 3, 3, df['cuentas_fans'] )\nprint(df['cuentas_fans'].value_counts())\n\ndf['ahorros_fans'] = np.where( df['total_altas_ahorros_3m_shift'] > 1, 1, 0 )\nprint(df['ahorros_fans'].value_counts())\n\ndf['financiacion_fans'] = np.where( df['total_altas_financiacion_3m_shift'] > 1, 1, 0 )\nprint(df['financiacion_fans'].value_counts())\n\ndf['fans'] = np.where( df['total_altas_3m_shift'] >= 1, 1, 0 )\ndf['fans'] = np.where( df['total_altas_3m_shift'] >= 2, 2, df['fans'] )\ndf['fans'] = np.where( df['total_altas_3m_shift'] >= 3, 3, df['fans'] )\nprint(df['fans'].value_counts())","36fc731f":"data = df[['pk_cid','pk_partition','total']]","afa3e597":"data['pk_partition'] = data['pk_partition'].astype(object)\naltas = data[ data['total'] >= 1]\n\n\naltas_pt = pd.pivot_table(altas, values='total', index=['pk_cid'],\n                    columns=['pk_partition'], aggfunc=np.max)\n    \n    #altas_pt.fillna( '2001-01-01', inplace=True)\n    #print(altas_pt)\n    #print(altas_pt.isnull().sum())\n    \ndata = pd.merge( data, altas_pt, how='left', on='pk_cid' )","c0e45c38":"cols = ['pk_cid','pk_partition']\ndata.columns = cols + partitions # pone las particiones como datetime\n\nfor part, i in zip(partitions, range(len(partitions))):\n    data[part] = np.where(data[part] >= 1, 1, data[part])\n\n    # No han comprado nada ese mes\n    data[part] = np.where(data[part] == 0, -1, data[part])\n    # Han comprado 1 producto o mas\n    data[part] = np.where(data[part] == 1, i, data[part])\n    # No aparece info de ellos en esa partition NaN o no compraron nada\n    data[part] = np.where(data[part].isna(), -1, data[part])\n        \n    data[part] = data[part].replace( {i:part, -1:'2001-01-01'})\n#   print( data[part].value_counts() )\n    data[part] = pd.to_datetime(data[part])","a832c346":"data['pk_partition'] = pd.to_datetime(data['pk_partition'])\n    #data[partitions].fillna('2021-01-01', inplace=True)\n    #print(data)\n\n## CALCULAMOS TIEMPO DESDE ULTIMA COMPRA\ndata['last_compra'] = datetime(2001,1,1)\n\nfor part in partitions:\n    data['last_compra'] = np.where( (data[part] > data['last_compra']) & (part <= data['pk_partition']), data[part], data['last_compra'])\n\ndata['tiempo_ult_compra'] = round(((data['pk_partition'] - data['last_compra'])\/np.timedelta64(1, 'M')))\n\ndata['prev'] = data.groupby('pk_cid')['tiempo_ult_compra'].shift(1)\n\ndata['tiempo_ult_compra'] = np.where( data['tiempo_ult_compra']==0, data['prev']+1, data['tiempo_ult_compra'])","6b4b4db1":"data['tiempo_ult_compra'] = np.where( data['tiempo_ult_compra'] < 0, 20, data['tiempo_ult_compra'] )\ndata['tiempo_ult_compra'] = np.where( data['tiempo_ult_compra'] > 100, 20, data['tiempo_ult_compra'] )\ndata['tiempo_ult_compra'] = np.where( data['tiempo_ult_compra'].isna(), 20, data['tiempo_ult_compra'] )","e46d4e13":"data['tiempo_ult_compra'].value_counts()","64878285":"data_to_join = data[['pk_cid','pk_partition','tiempo_ult_compra']]\n\n# Anado si ha comprado antes\ndata_to_join['compra_antes'] = np.where(data_to_join['tiempo_ult_compra'] == 20, 0, 1)\n#data_to_join['pk_partition'] = data_to_join['pk_partition']","7d496cde":"df['pk_partition'] = pd.to_datetime(df['pk_partition'])\n\ndf = pd.merge(df, data_to_join, on=['pk_cid','pk_partition'], how='left')\n\ndel data, data_to_join, altas_pt, altas\n\ndf = df[df['tiempo_ult_compra'] != 20]","08392467":"df = df[ (df['deceased'] != 'S') | ((df['age'] >= 18) & (df['age'] <= 90))]\n#df = df[ (df['age'] >= 18) & (df['age'] <= 90)]","af509137":"# Tramificacion de edades\nage_tram = df[df['pk_partition']=='2018-05-28']['age']\nage_tram.hist()","59fe9d42":"age_divisions = {1:[0,20], 2:[20,40], 3:[40,60], 4:[60,120]}\ndf['age_tram'] = 0\nfor div, values in age_divisions.items():\n    df['age_tram'] = np.where( (df['age'] > values[0]) & (df['age'] <= values[1]), div, df['age_tram'] )","35883598":"# Tramificacion de edades\nsalary_tram = df[df['pk_partition']=='2018-05-28']['salary']\nquartiles = salary_tram.describe()\nquartiles","f14235d4":"salary_divisions = {1:[0,4e4], 2:[4e4,8e4], 3:[8e4,12e4], 4:[12e4,2e10]}\ndf['salary'].fillna(-1,inplace=True)\n\ndf['salary_tram'] = 0\n\nfor div, values in salary_divisions.items():\n    print(values)\n    df['salary_tram'] = np.where( (df['salary'] > values[0]) & (df['salary'] <= values[1]), div, df['salary_tram'] )","0f1f5377":"df.fillna(0, inplace=True)","5b05c86b":"def gb_feature_generator(df_, gb_list, target, calculations, filter_positives=False):\n\n    #meses = [1,2,3]\n    #gb_list = ['pk_partition','cluster']\n    #target = ['altas_shift_1']\n    #calculations = ['mean','sum']\n   \n        \n    if filter_positives and df_[df_[target] != 0].shape[0] != 0:\n        data = df_[df_[target] != 0]\n        gb_df = data.groupby(gb_list)[target].agg(calculations)\n    else:\n        gb_df = df_.groupby(gb_list)[target].agg(calculations)\n\n    new_cols = []\n    for cal in calculations:\n        if len(gb_list) == 2:\n            new_cols.append(target+'_'+gb_list[1]+'_'+cal)\n        elif len(gb_list) == 3:\n            new_cols.append(target+'_'+gb_list[1]+'_'+gb_list[2]+'_'+cal)\n        #print(new_cols)    \n    gb_df.columns = new_cols\n    gb_df.reset_index(inplace=True)\n    #print(gb_df.head())\n    df_ = pd.merge(df_, gb_df, how='left', on=gb_list)\n    #df_.drop('index', axis=1, inplace=True)\n    \n    return df_","86b174f5":"df['pk_partition'] = pd.to_datetime(df['pk_partition'])\ndf['month'] = df['pk_partition'].dt.month\ndf['year'] = df['pk_partition'].dt.year","e2916b16":"print('Initial shape: ',df.shape)\n\n# Per cuentas_fans\n\ngb_list = ['pk_partition', 'cuentas_fans']\n\nfeature_list = ['total_altas_3m_shift','total_altas_cuenta_1m_shift','total_altas_cuenta_2m_shift',\n    'total_altas_cuenta_3m_shift','total_altas_ahorros_3m_shift','total_altas_financiacion_3m_shift']\n\ndf.reset_index(inplace=True)\n\nfor key in feature_list:\n    lista = gb_list.copy()\n    lista.append(key)\n    df_ = gb_feature_generator(df[lista], gb_list, key, ['sum','mean'], filter_positives=True)\n    for col in df_.columns:\n        df[col] = df_[col]\n\ntry:\n    df.drop('index',axis=1,inplace=True)\nexcept:\n    pass\n\nprint(gb_list)\n\nprint('Final shape: ',df.shape)","54c2c009":"print('Initial shape: ',df.shape)\n\n# Per ahorros_fans\n\ngb_list = ['pk_partition', 'ahorros_fans']\n\nfeature_list = ['total_altas_3m_shift','total_altas_cuenta_3m_shift',\n 'total_altas_ahorros_1m_shift','total_altas_ahorros_2m_shift',\n 'total_altas_ahorros_3m_shift','total_altas_financiacion_3m_shift']\n\nfor key in feature_list:\n    lista = gb_list.copy()\n    lista.append(key)\n    df_ = gb_feature_generator(df[lista], gb_list, key, ['sum','mean'], filter_positives=True)\n    for col in df_.columns:\n        df[col] = df_[col]\n\ntry:\n    df.drop('index',axis=1,inplace=True)\nexcept:\n    pass\n\nprint(gb_list)\n\nprint('Final shape: ',df.shape)","b1b5b5b4":"print('Initial shape: ',df.shape)\n\n# Per financiacion_fans\n\ngb_list = ['pk_partition', 'financiacion_fans']\n\nfeature_list = ['total_altas_3m_shift','total_altas_cuenta_3m_shift',\n 'total_altas_ahorros_3m_shift','total_altas_financiacion_1m_shift',\n 'total_altas_financiacion_2m_shift','total_altas_financiacion_3m_shift']\n\nfor key in feature_list:\n    lista = gb_list.copy()\n    lista.append(key)\n    df_ = gb_feature_generator(df[lista], gb_list, key, ['sum','mean'], filter_positives=True)\n    df[df_.columns] = df_\n\nprint(gb_list)\n\nprint('Final shape: ',df.shape)","a609acba":"print('Initial shape: ',df.shape)\n\n# Per salary_tram\n\ngb_list = ['pk_partition','salary_tram']\n\nfeature_list = ['total_altas_cuenta_3m_shift','total_altas_ahorros_3m_shift','total_altas_financiacion_3m_shift']\n\nfor key in feature_list:\n    lista = gb_list.copy()\n    lista.append(key)\n    df_ = gb_feature_generator(df[lista], gb_list, key, ['sum','mean'], filter_positives=True)\n    df[df_.columns] = df_\n\ntry:\n    df.drop('index',axis=1,inplace=True)\nexcept:\n    pass\n\nprint(gb_list)\n\nprint('Final shape: ',df.shape)","d28361bf":"print('Initial shape: ',df.shape)\n\n# Per age_tram\n\ngb_list = ['pk_partition','age_tram']\n\nfeature_list = ['total_altas_cuenta_3m_shift','total_altas_ahorros_3m_shift','total_altas_financiacion_3m_shift']\n\nfor key in feature_list:\n    lista = gb_list.copy()\n    lista.append(key)\n    df_ = gb_feature_generator(df[lista], gb_list, key, ['sum','mean'], filter_positives=True)\n    df[df_.columns] = df_\n\ntry:\n    df.drop('index',axis=1,inplace=True)\nexcept:\n    pass\n\nprint(gb_list)\n\nprint('Final shape: ',df.shape)","0ced61dc":"print('Initial shape: ',df.shape)\n\n# Per age_tram and salary_tram\n\ngb_list = ['pk_partition','age_tram','salary_tram']\n\nfeature_list = ['total_altas_3m_shift']\n\nfor key in feature_list:\n    lista = gb_list.copy()\n    lista.append(key)\n    df_ = gb_feature_generator(df[lista], gb_list, key, ['sum','mean'], filter_positives=True)\n    df[df_.columns] = df_\n\ntry:\n    df.drop('index',axis=1,inplace=True)\nexcept:\n    pass\n\nprint(gb_list)\n\nprint('Final shape: ',df.shape)","bdc1d5cb":"print('Initial shape: ',df.shape)\n\n# Per fans\n\ngb_list = ['pk_partition','fans']\n\nfeature_list = ['total_altas_1m_shift','total_altas_2m_shift','total_altas_3m_shift']\n\nfor key in feature_list:\n    lista = gb_list.copy()\n    lista.append(key)\n    df_ = gb_feature_generator(df[lista], gb_list, key, ['sum','mean'], filter_positives=True)\n    df[df_.columns] = df_\n\ntry:\n    df.drop('index',axis=1,inplace=True)\nexcept:\n    pass\n\nprint(gb_list)\n\nprint('Final shape: ',df.shape)","ea58245c":"del df_, gb_list, feature_list","8de85bb4":"df['compras_cuenta'] = np.where(df['altas_cuenta']>=1,1,0) \ndf['compras_ahorros'] =  np.where(df['altas_ahorros']>=1,1,0) \ndf['compras_financiacion'] = np.where(df['altas_financiacion']>=1,1,0)\n\ndf['compras'] = df['compras_cuenta'] + df['compras_ahorros'] + df['compras_financiacion']\nmax_ = int(max(df['compras'].value_counts().index)+1)\nprint(max_)","5938a57a":"print(df['compras_cuenta'].value_counts())\nprint(df['compras_ahorros'].value_counts())\nprint(df['compras_financiacion'].value_counts())","5cd72d20":"df['compras'].value_counts()","c9544954":"list_ = list_products.copy()\ndf['pk_partition'] = pd.to_datetime(df['pk_partition'])\nlist_.append('pk_partition')\nlist_.append('pk_cid')\n\ncompras_todas = {}\n\nfor number in range(1,max_):\n    part_df = df[df['compras'] == number][list_].set_index(['pk_cid','pk_partition'])\n\n    #print(part_df[list_products] != 0)\n    for key, row in part_df.iterrows():\n        #print(key, row[row != 0].index.to_list())\n        compras_todas[key] = row[row != 0].index.to_list()     \n        \n    print(number)\n    #for num in range(1,number+1):\n    #    df['prod'+str(num)] = np.where( , cost_product[products_dict[prods[num-1]]], df['prod'+str(num)] )","52d89342":"import pickle\nf = open(\"compras_todas.pkl\",\"wb\")\npickle.dump(compras_todas,f)\nf.close()","feeb29ed":"compras_df = pd.DataFrame.from_dict(compras_todas, orient='index')\n\ncompras_melted = compras_df.melt(ignore_index = False)\ncompras_melted['tipo_producto'] = compras_melted['value'].replace(products_dict)\ncompras_melted['precio'] = compras_melted['tipo_producto'].replace(cost_product)\ncompras_melted = compras_melted.reset_index()\n\ncompras_melted['pk_cid'] = compras_melted['index'].apply( lambda x: x[0] )\ncompras_melted['pk_partition'] = compras_melted['index'].apply( lambda x: x[1] )\n\n#compras_melted = compras_melted[~compras_melted['pk_partition'].isna()]\ncompras_melted = compras_melted[~compras_melted['tipo_producto'].isna()]\n\ncompras_melted\n\n#compras_melted[['pk_cid','pk_partition','tipo_producto','precio']]","6a851dfc":"compras_melted.info()\ncompras_melted['pk_partition'] = compras_melted['pk_partition'].apply(lambda x: str(x)[:11])\ndf['pk_partition'] = df['pk_partition'].apply(lambda x: str(x)[:11])","abf75c0f":"del compras_todas, compras_df, part_df","01b2617b":"df = pd.merge(df, compras_melted[['pk_cid','pk_partition','value','tipo_producto','precio']], on=['pk_cid','pk_partition'], how='left')\ndel compras_melted","1c401f8a":"df.info()","a1c7c4a3":"df['precio'].value_counts(dropna=False)","40c2a196":"df['precio'] = np.where(df['precio'].isna(),0,df['precio'])\n#df['value'] = np.where(df['value'].isna(),0,df['value'])","48363f53":"df['value'].value_counts(dropna=False)","d3fe3048":"df['id'] = range(df.shape[0])","64892c65":"cost_10 = set(list(df['id'].sample(int(df.shape[0]*0.35))))\ncost_60 = set(list(df['id'].sample(int(df.shape[0]*0.35))))\n\nprint(len(cost_10))\nprint(len(cost_60))","560a2bdb":"df['precio'].fillna(0,inplace=True)\ndf['precio'] = np.where( (df['precio'] == 0) & (df['id'].isin(cost_10)), 10, df['precio'])\ndf['precio'] = np.where( (df['precio'] == 0) & (df['id'].isin(cost_60)), 60, df['precio'])\ndf['precio'] = np.where( df['precio'] == 0, 40, df['precio'])","88a3115c":"print(df['precio'].value_counts(dropna=False))\ndf.drop('id',axis=1,inplace=True)","5d75904d":"df['compras'] = np.where(df['compras']>=1,1,0)","6e4377bc":"df[['compras','precio']].value_counts()","18463a55":"df.shape","6dbca605":"for x in list_products:\n    df.drop(x, axis=1, inplace=True)","d688b585":"df.info()","5702cda9":"remove_cols = ['value','tipo_producto','segment','payroll','country_id','region_code','pk_partition','entry_date','deceased','compra_antes',\\\n    'total','total_altas','total_altas_cuenta','total_altas_financiacion','total_altas_ahorros','altas_ahorros','altas_cuenta','altas_financiacion']\n\nfor col in remove_cols:\n    try:\n        df.drop(col, axis=1, inplace=True)\n    except:\n        pass","1eafc539":"df.columns.to_list()","ed0faa87":"df.isnull().sum()[ df.isnull().sum() != 0 ]","57e76d61":"df.fillna(0,inplace=True)","55254468":"targets = ['compras','compras_cuenta','compras_ahorros','compras_financiacion']","c68e6bc2":"if PREPARE_PREDICTION:\n    prediction_df = df[(df['year']==2019)&(df['month']==5)]\n    prediction_df = prediction_df.groupby('pk_cid').last()\n    \n    prediction_df['tiempo_ult_compra'] += 1\n    prediction_df['tiempo_ult_compra'] = np.where(prediction_df['compras']>1, 1, prediction_df['tiempo_ult_compra'])\n\n    prediction_df['month'] += 1\n\n    pred_features = prediction_df.columns.to_list()\n\n    for item in targets+remove_cols:\n        try:\n            pred_features.remove(item)\n        except:\n            pass\n\n    prediction_df[pred_features].to_csv(DIR_PATH+'clients_prediction.csv')\n\n    assert 1 != 0 #must finish\n                ","c99a0f3d":"def balancear_dataset(df, target, dist, totales, tipo, times=1):\n    totales_ = totales.copy()\n    \n    compras = ['compra_ahorros','compra_cuenta','compra_financiacion']\n    boolean = False\n    if target in compras:\n        compras.remove(target)\n        boolean = True\n    \n    subset_positives = df[ (df[target]==True) ]\n    df_ = subset_positives.copy(deep=True)\n    \n    if boolean:\n        df = df[(df[compras[0]]==False) & (df[compras[1]]==False)]\n    \n    if tipo == 'undersampling':\n        subset_negatives = df[ (df[target] == 0) ].sample( df_.shape[0]*dist )\n    elif tipo == 'oversampling':\n        for time in range(times-1):\n            df_ = df_.append( subset_positives, ignore_index=True )\n        if dist == 0:\n            subset_negatives = df[ (df[target] == 0) ]\n        else:\n            subset_negatives = df[ (df[target] == 0) ].sample( df_.shape[0]*dist )\n\n    df_final = subset_negatives.copy(deep=True)\n    df_final = df_final.append( df_, ignore_index=True )\n    \n    totales_.remove(target)\n    #print(totales_)\n        \n    df_final.drop(totales_, axis='columns', inplace=True)\n    \n    del totales_\n    \n    return df_final","275f6301":"def dataset_split(df_final, target, balanceo, totales, dist, times=1):\n    features = df_final.columns.to_list()  \n    X = df_final[ features ]\n    y = df_final[ target ]\n\n    X_validation = X[(X['year']==2019) & (X['month']==5)]\n    y_validation = y.loc[X_validation.index]\n    \n    #X_train_test = X[(X['year']!=2019) & (X['month']!=5)]\n    #y_train_test = y.loc[X_train_test.index]\n    \n    # Balancear dataset \n    df_balanceo = balancear_dataset(df_final, target, dist, totales, \"oversampling\", times)\n    \n    features = df_balanceo.columns.to_list()\n    remove_cols = ['country_id','compra_antes',target]\n    for x in remove_cols:\n        try:\n            features.remove(x)\n        except:\n            pass\n    \n    X_train_test = df_balanceo[ features ]\n    y_train_test = df_balanceo[ target ]\n    X_validation = X_validation[ features ]\n    \n    # Train and Test\n    X_train, X_test, y_train, y_test =  train_test_split(X_train_test, y_train_test, test_size=0.2, random_state=42)\n    \n    return X_train, y_train, X_test, y_test, X_validation, y_validation ","69d9c923":"def metricas(y_test, y_pred, y_test_score, y_train, y_train_pred, y_train_score):\n    print(\"----------- TEST -----------\")\n    print(\"Accuracy:\",accuracy_score(y_test, y_pred))\n    print(\"Precision:\",precision_score(y_test, y_pred))\n    print(\"Recall:\",recall_score(y_test, y_pred))\n    print(\"F1-score:\",f1_score(y_test, y_pred))\n    print(\"ROC AUC score:\",roc_auc_score(y_test, y_test_score))\n    print(\"----------- TRAIN -----------\")\n    print(\"Accuracy:\",accuracy_score(y_train, y_train_pred))\n    print(\"Precision:\",precision_score(y_train, y_train_pred))\n    print(\"Recall:\",recall_score(y_train, y_train_pred))\n    print(\"F1-score:\",f1_score(y_train, y_train_pred))\n    print(\"ROC AUC score:\",roc_auc_score(y_train, y_train_score))","16fcab73":"def confusion_matrix_figure(y_test, y_pred):\n    fig, ax = plt.subplots(figsize=(8,8))\n\n    cf_matrix = confusion_matrix(y_test, y_pred)\n    cf_matrix_ = cf_matrix.copy()\n    cf_matrix_[1,1] = cf_matrix[0,0]\n    cf_matrix_[0,1] = cf_matrix[1,0]\n    cf_matrix_[0,0] = cf_matrix[1,1]\n    cf_matrix_[1,0] = cf_matrix[0,1]\n    cf_matrix_\n\n    group_names = ['True Pos','False Neg','False Pos','True Neg']\n    group_counts = [\"{0:0.0f}\".format(value) for value in\n                    cf_matrix_.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in\n                         cf_matrix_.flatten()\/np.sum(cf_matrix)]\n    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n              zip(group_names,group_counts,group_percentages)]\n\n    labels = np.asarray(labels).reshape(2,2)\n    ax = sns.heatmap(cf_matrix_, annot=labels, fmt='', cmap='Greens', annot_kws={\"size\": 20},\n                    yticklabels=['Positive','Negative'], xticklabels=['Positive','Negative'])\n    ax.set_xlabel('Predicted',fontsize=23)\n    ax.set_ylabel('Real',fontsize=23)\n    ax.tick_params(labelsize=20)\n    \n    return fig","b86e7f6c":"df['compras'].value_counts()","8dcf56f5":"X_train, y_train, X_test, y_test, X_validation, y_validation = \\\n    dataset_split(df, 'compras', \"oversampling\", targets, dist=0, times=1)\n\nfeatures = X_train.columns.to_list()\nfeatures.remove('pk_cid')\n#features.remove('cluster')","ad96d0f9":"print('TRAIN: ',y_train.shape[0]) \nprint(y_train.value_counts(normalize=True),'\\n')\n\nprint('TEST: ', y_test.shape[0])\nprint(y_test.value_counts(normalize=True),'\\n')\n\nprint('VALIDATION: ',y_validation.shape[0])\nprint(y_validation.value_counts(normalize=True))","61d4d915":"#model = lgbm.LGBMClassifier(n_estimators=500, max_depth=20, random_state=42, min_child_samples=250)\nmodel = DecisionTreeClassifier( max_depth=50, min_samples_leaf=350, random_state=42 )\n\nmodel.fit(X_train[features],y_train)\n\ny_train_pred = model.predict(X_train[features])\ny_train_score = model.predict_proba(X_train[features])[:,1]\n\n#Predict the response for test dataset\ny_pred = model.predict(X_test[features])\ny_test_score = model.predict_proba(X_test[features])[:,1]","36a52e0a":"metricas(y_test, y_pred, y_test_score, y_train, y_train_pred, y_train_score)","531136bb":"y_pred_valida = model.predict(X_validation[features])\ny_valida_score = model.predict_proba(X_validation[features])[:,1]\n\nprint(\"----------- VALIDATION -----------\")\nprint(\"Accuracy:\",accuracy_score(y_validation, y_pred_valida))\nprint(\"Precision:\",precision_score(y_validation, y_pred_valida))\nprint(\"Recall:\",recall_score(y_validation, y_pred_valida))\nprint(\"F1-score:\",f1_score(y_validation, y_pred_valida))\nprint(\"ROC AUC score:\",roc_auc_score(y_validation, y_valida_score))\n\nfig = confusion_matrix_figure(y_validation, y_pred_valida)\nfig.show()","d5a33fd4":"fig = confusion_matrix_figure(y_train, y_train_pred)","08b6ab6f":"tree_data = export_graphviz(\n    decision_tree = model,\n    out_file=None,\n    #*,\n    max_depth=3,\n    feature_names= features,\n    class_names=['No Compra','Compra'],\n    #label='all',\n    filled=True,\n    #leaves_parallel=False,\n    impurity=True,\n    #node_ids=False,\n    proportion=True,\n    rotate=True,\n    rounded=True,\n    #special_characters=False,\n    precision=3,\n)\n\ngraph = graphviz.Source(tree_data, format='png')\ngraph","7f68d041":"pd.Series( model.feature_importances_, index=features ).sort_values(ascending=False).head(30)","42286a07":"df['compras_cuenta'].value_counts()","c0b13815":"X_train, y_train, X_test, y_test, X_validation, y_validation = \\\n    dataset_split(df, 'compras_cuenta', \"oversampling\", targets, dist=0, times=2)\n\nfeatures = X_train.columns.to_list()\nfeatures.remove('pk_cid')\nfeatures.remove('precio')\n#features.remove('cluster')","bb533f99":"print('TRAIN: ',y_train.shape[0]) \nprint(y_train.value_counts(normalize=True),'\\n')\n\nprint('TEST: ', y_test.shape[0])\nprint(y_test.value_counts(normalize=True),'\\n')\n\nprint('VALIDATION: ',y_validation.shape[0])\nprint(y_validation.value_counts(normalize=True))","d7a9c727":"#model = lgbm.LGBMClassifier(n_estimators=500, max_depth=20, random_state=42, min_child_samples=250)\nmodel = DecisionTreeClassifier( max_depth=50, min_samples_leaf=500, random_state=42 )\n\nmodel.fit(X_train[features],y_train)\n\ny_train_pred = model.predict(X_train[features])\ny_train_score = model.predict_proba(X_train[features])[:,1]\n\n#Predict the response for test dataset\ny_pred = model.predict(X_test[features])\ny_test_score = model.predict_proba(X_test[features])[:,1]","43f1d64e":"y_pred_valida = model.predict(X_validation[features])\ny_valida_score = model.predict_proba(X_validation[features])[:,1]\n\nprint(\"----------- VALIDATION -----------\")\nprint(\"Accuracy:\",accuracy_score(y_validation, y_pred_valida))\nprint(\"Precision:\",precision_score(y_validation, y_pred_valida))\nprint(\"Recall:\",recall_score(y_validation, y_pred_valida))\nprint(\"F1-score:\",f1_score(y_validation, y_pred_valida))\nprint(\"ROC AUC score:\",roc_auc_score(y_validation, y_valida_score))\n\n#fig = confusion_matrix_figure(y_train, y_train_pred)\nfig = confusion_matrix_figure(y_validation, y_pred_valida)","7186cf76":"tree_data = export_graphviz(\n    decision_tree = model,\n    out_file=None,\n    #*,\n    max_depth=3,\n    feature_names= features,\n    class_names=['No Compra','Compra'],\n    #label='all',\n    filled=True,\n    #leaves_parallel=False,\n    impurity=True,\n    #node_ids=False,\n    proportion=True,\n    rotate=True,\n    rounded=True,\n    #special_characters=False,\n    precision=3,\n)\n\ngraph = graphviz.Source(tree_data, format='png')\ngraph","549351ca":"df['compras_ahorros'].value_counts()","c85587cc":"X_train, y_train, X_test, y_test, X_validation, y_validation = \\\n    dataset_split(df, 'compras_ahorros', \"oversampling\", targets, dist=0, times=2)\n\nfeatures = X_train.columns.to_list()\nfeatures.remove('pk_cid')\nfeatures.remove('precio')\n#features.remove('cluster')","5de9fc98":"print('TRAIN: ',y_train.shape[0]) \nprint(y_train.value_counts(normalize=True),'\\n')\n\nprint('TEST: ', y_test.shape[0])\nprint(y_test.value_counts(normalize=True),'\\n')\n\nprint('VALIDATION: ',y_validation.shape[0])\nprint(y_validation.value_counts(normalize=True))","cf35c6eb":"#model = lgbm.LGBMClassifier(n_estimators=500, max_depth=20, random_state=42, min_child_samples=250)\nmodel = DecisionTreeClassifier( max_depth=50, min_samples_leaf=500, random_state=42 )\n\nmodel.fit(X_train[features],y_train)\n\ny_train_pred = model.predict(X_train[features])\ny_train_score = model.predict_proba(X_train[features])[:,1]\n\n#Predict the response for test dataset\ny_pred = model.predict(X_test[features])\ny_test_score = model.predict_proba(X_test[features])[:,1]","ac418a35":"y_pred_valida = model.predict(X_validation[features])\ny_valida_score = model.predict_proba(X_validation[features])[:,1]\n\nprint(\"----------- VALIDATION -----------\")\nprint(\"Accuracy:\",accuracy_score(y_validation, y_pred_valida))\nprint(\"Precision:\",precision_score(y_validation, y_pred_valida))\nprint(\"Recall:\",recall_score(y_validation, y_pred_valida))\nprint(\"F1-score:\",f1_score(y_validation, y_pred_valida))\nprint(\"ROC AUC score:\",roc_auc_score(y_validation, y_valida_score))\n\n#fig = confusion_matrix_figure(y_train, y_train_pred)\nfig = confusion_matrix_figure(y_validation, y_pred_valida)","c33115d5":"tree_data = export_graphviz(\n    decision_tree = model,\n    out_file=None,\n    #*,\n    max_depth=3,\n    feature_names= features,\n    class_names=['No Compra','Compra'],\n    #label='all',\n    filled=True,\n    #leaves_parallel=False,\n    impurity=True,\n    #node_ids=False,\n    proportion=True,\n    rotate=True,\n    rounded=True,\n    #special_characters=False,\n    precision=3,\n)\n\ngraph = graphviz.Source(tree_data, format='png')\ngraph","04055f09":"df['compras_financiacion'].value_counts()","5423e446":"X_train, y_train, X_test, y_test, X_validation, y_validation = \\\n    dataset_split(df, 'compras_financiacion', \"oversampling\", targets, dist=0, times=2)\n\nfeatures = X_train.columns.to_list()\nfeatures.remove('pk_cid')\nfeatures.remove('precio')\n#features.remove('cluster')","17e23de7":"print('TRAIN: ',y_train.shape[0]) \nprint(y_train.value_counts(normalize=True),'\\n')\n\nprint('TEST: ', y_test.shape[0])\nprint(y_test.value_counts(normalize=True),'\\n')\n\nprint('VALIDATION: ',y_validation.shape[0])\nprint(y_validation.value_counts(normalize=True))","1237326e":"#model = lgbm.LGBMClassifier(n_estimators=550, max_depth=50, random_state=42, min_child_samples=250)\nmodel = DecisionTreeClassifier( max_depth=50, min_samples_leaf=500, random_state=42 )\n\nmodel.fit(X_train[features],y_train)\n\ny_train_pred = model.predict(X_train[features])\ny_train_score = model.predict_proba(X_train[features])[:,1]\n\n#Predict the response for test dataset\ny_pred = model.predict(X_test[features])\ny_test_score = model.predict_proba(X_test[features])[:,1]","82190feb":"y_pred_valida = model.predict(X_validation[features])\ny_valida_score = model.predict_proba(X_validation[features])[:,1]\n\nprint(\"----------- VALIDATION -----------\")\nprint(\"Accuracy:\",accuracy_score(y_validation, y_pred_valida))\nprint(\"Precision:\",precision_score(y_validation, y_pred_valida))\nprint(\"Recall:\",recall_score(y_validation, y_pred_valida))\nprint(\"F1-score:\",f1_score(y_validation, y_pred_valida))\nprint(\"ROC AUC score:\",roc_auc_score(y_validation, y_valida_score))\n\n#fig = confusion_matrix_figure(y_train, y_train_pred)\nfig = confusion_matrix_figure(y_validation, y_pred_valida)","872133ef":" tree_data = export_graphviz(\n    decision_tree = model,\n    out_file=None,\n    #*,\n    max_depth=3,\n    feature_names= features,\n    class_names=['No Compra','Compra'],\n    #label='all',\n    filled=True,\n    #leaves_parallel=False,\n    impurity=True,\n    #node_ids=False,\n    proportion=True,\n    rotate=True,\n    rounded=True,\n    #special_characters=False,\n    precision=3,\n)\n\ngraph = graphviz.Source(tree_data, format='png')\ngraph","e14cbe21":"X_train, y_train, X_test, y_test, X_validation, y_validation = \\\n    dataset_split(df, 'compras', \"oversampling\", targets, dist=0, times=1)\n\nfeatures = X_train.columns.to_list()\nfeatures.remove('pk_cid')\n#features.remove('cluster')","e7a1343b":"X_train.columns.to_list()","bae5945f":"model = lgbm.LGBMClassifier(n_estimators=550, max_depth=50, random_state=42, min_child_samples=250)\n#model = DecisionTreeClassifier( max_depth=50, min_samples_leaf=500, random_state=42 )\n\nmodel.fit(X_train[features],y_train)\n\ny_train_pred = model.predict(X_train[features])\ny_train_score = model.predict_proba(X_train[features])[:,1]\n\n#Predict the response for test dataset\ny_pred = model.predict(X_test[features])\ny_test_score = model.predict_proba(X_test[features])[:,1]","a37710c9":"y_pred_valida = model.predict(X_validation[features])\ny_valida_score = model.predict_proba(X_validation[features])[:,1]\n\nprint(\"----------- VALIDATION -----------\")\nprint(\"Accuracy:\",accuracy_score(y_validation, y_pred_valida))\nprint(\"Precision:\",precision_score(y_validation, y_pred_valida))\nprint(\"Recall:\",recall_score(y_validation, y_pred_valida))\nprint(\"F1-score:\",f1_score(y_validation, y_pred_valida))\nprint(\"ROC AUC score:\",roc_auc_score(y_validation, y_valida_score))\n\nfig = confusion_matrix_figure(y_validation, y_pred_valida)\nfig.show()","90e33541":"pd.Series( model.feature_importances_, index=features ).sort_values(ascending=False).head(30)","e00586bb":"import pickle \n\nfilename = 'modelo_compra_general.sav'\npickle.dump(model, open(filename, 'wb'))","2864345b":"summary = X_train.copy(deep=True)\nsummary['real'] = y_train\nsummary['predicted'] = y_train_pred\nsummary['proba'] = y_train_score\nsummary['diff'] = summary['predicted']-summary['real']\n\nsummary[summary['diff'] == 1].describe()","75673af7":"summary[summary['diff'] == 1].sort_values('proba',ascending=False).head(30)","90191f77":"# Tarea 3: Recomendaci\u00f3n (Modelo para predicci\u00f3n de compra)\n\n<a href=\"https:\/\/www.amazon.es\/Hands-Unsupervised-Learning-Using-Python\/dp\/1492035645\">Volver a P\u00e1gina Principal<\/a>\n\nParte de la soluci\u00f3n de la <a href=\".\/tarea3_recomendacion.ipynb\">Tarea 3: Recomendaci\u00f3n<\/a> est\u00e1 en crear un modelo que permita obtener una probabilidad de cu\u00e1les recomendaciones de la parte anterior tienen mayor probabilidad de \u00e9xito con el fin de poder escoger entre la larga lista de recomendaciones reduciendo as\u00ed el riesgo y optando por las opciones de las que estamos m\u00e1s seguros. \n\nEste modelo se trata de un modelo de Machine Learning supervisado de clasificaci\u00f3n binaria, donde 1 o True significa que ha comprado y 0 que no ha comprado. Se desarrolla 1 modelo general aunque se prueban 3 modelos para cada tipo de producto.\nDurante el desarrollo de este notebook se ir\u00e1n preparando y limpiando los datos del modelo para finalmente obtener un modelo muy aceptable con un ROC AUC score de 0.89. \n\nPara simplificar el problema se considera como compra si un cliente se da de alta en ese producto. Sin embargo, como se comenta en tareas anteriores este no es el caso y el producto no se cobra hasta pasados 3 meses de permanencia. Tener en cuenta el cumplimiento de la permanencia para el modelo hubiera sido otro problema muy distinto y bastante m\u00e1s complejo. \n\n## Tabla de Contenidos <a class=\"anchor\" id=\"0\"><\/a>\n\n1. [Data Preparation](#1) <br>\n    1.1 [Determinar altas](#11) <br>\n    1.2 [Codificar variables de texto](#12) <br>\n    1.3 [Altas y bajas en los \u00faltimos meses](#13) <br>\n    1.4 [Tiempo desde la \u00faltima compra](#14) <br>\n    1.5 [Tramificaci\u00f3n de variables num\u00e9ricas](#15) <br>\n    1.6 [Feature Generator](#16) <br>\n    1.7 [Preparaci\u00f3n del target](#17) <br>\n    1.8 [Precio del producto a comprar](#18) <br>\n    1.9 [Preparaci\u00f3n de la predicci\u00f3n](#19) <br>\n2. [Data Cleaning](#2) <br>\n    2.1 [Preparaci\u00f3n de la predicci\u00f3n](#21) <br>\n3. [Model Construction](#3) <br>\n    3.1. [Modelo General](#31) <br>\n    3.2. [Modelo Cuentas](#32) <br>\n    3.3. [Modelo Ahorros](#33) <br>\n    3.4. [Modelo Financiaci\u00f3n](#34) <br>\n    3.5. [Selecci\u00f3n del modelo](#35) <br>\n4. [Conclusiones](#4) <br>","e870c632":"Tambi\u00e9n se definen una serie de funciones para visualizar las m\u00e9tricas y la matriz de confusi\u00f3n para un modelo de clasificaci\u00f3n como el que estamos tratando:","b1e2ec97":"Se utiliza un DecisionTreeClassifier para as\u00ed poder visualizar el \u00e1rbol de decisi\u00f3n.","bd1f09d3":"## Modelo Financiaci\u00f3n <a class=\"anchor\" id=\"34\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nEl target es *compra_financiacion* y ocurre exactamente lo mismo que con los modelos anteriores, dataset desbalanceado, pocos datos y el modelo no es capaz de encontrar diferencias entre compras y no compras. ","e2049ec0":"## Selecci\u00f3n del modelo <a class=\"anchor\" id=\"35\"><\/a>\n\n[Tabla de Contenidos](#0) \n\nFinalmente, se opta por el [Modelo General](#31) que es el que mejor resultados obtiene.\n\nEl valor de Recall es bajo tanto en test, train como en validaci\u00f3n. Y deber\u00e1 prestarse mucha antenci\u00f3n con los errores de Tipo II (hip\u00f3tesis aceptada cuando era falsa - condenar a un inocente). Es decir por la tem\u00e1tica del problema donde se cada email tiene un coste, se prefiere reducir el n\u00famero de emails a clientes que no ten\u00edan ning\u00fan deseo a comprar. \n\nFinalmente, guardamos el modelo que ser\u00e1 utilizado en la Tarea de Recomendaci\u00f3n. ","86ce80a1":"## Altas y bajas en los \u00faltimos meses <a class=\"anchor\" id=\"13\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nEn esta secci\u00f3n se determinan las altas y bajas en los \u00faltimos. Para ello se utiliza la funci\u00f3n rolling que permite hacer un resumeen (max, mean, min...) de los meses que se indiquen. Para la predicci\u00f3n no se realiza el \u00faltimo shift. \n","79b1fa1b":"Vamos a evaluar las caracter\u00edsticas de los errores de tipo II que suele cometer el modelo. Lo que destaca en este caso es la alta probabilidad que aparece en los errores de tipo II.","7ca42124":"## Preparaci\u00f3n del target <a class=\"anchor\" id=\"17\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nComo se observa algunos clientes compran varios productos en el mismo mes. El target a predecir ser\u00e1 predecir la compra de un tipo de producto por lo que se limitan las compras a 1. \n\nY para el caso donde en un mismo mes se producen varias compras, se duplica el n\u00famero de registros, para as\u00ed no perder informaci\u00f3n de cada cliente.","ba1df34d":"## Preparaci\u00f3n de la predicci\u00f3n <a class=\"anchor\" id=\"21\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nPara el caso de la predicci\u00f3n (dataset que se introducir\u00e1 en la tarea 3) se guarda el estado y datos de los clientes en el mes futuro con el objetivo de poder realizar la predicci\u00f3n de las recomendaciones para el pr\u00f3ximo mes. ","07257ce8":"## Modelo Cuentas <a class=\"anchor\" id=\"32\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nEn este modelo para la predicci\u00f3n de compras en cuentas el target a predecir es *compra_cuenta*. Al reducir el n\u00famero de compras el dataset est\u00e1 desbalanceado por lo que se balancea. Si el dataset no se balancea, el modelo determinar\u00e1 no compra en todos o casi todos los casos, dado que los casos de compra son minoritarios. \n\nLa idea es aumentar los datos de compra suficientemente ya sea con undersampling o oversampling, de manera que el modelo vea m\u00e1s casos de compra y se \"atreva\" a predecir compra. \n\nSin embargo, los resultados obtenidos son bastante desastrosos y esto se debe a los pocos datos y la poca variedad de compras para el tipo de producto cuenta (siendo este el tipo de producto m\u00e1s comprado, as\u00ed que resultados peores pueden aparecer en los otros tipos).","983dea9f":"Guardamos el modelo en un pickle para as\u00ed poder utilizarlo en otro script o notebook.","0cc31605":"## Codificar variables de texto <a class=\"anchor\" id=\"12\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nA continuaci\u00f3n pasamos a valor num\u00e9rico los campos de entry_channel y gender con un LabelEncoder.","ad7b6314":"Los siguientes resultados no son los esperados. Destaca el baj\u00edsimo recall y en la confusion matrix evaluada en validaci\u00f3n, se observa que pese a que el modelo se atreva a predecir compra, se equivoca m\u00e1s que acierta y a\u00fan as\u00ed se le escapan muchas compras. Se trata de un modelo poco fiable y que no ofrece mucho valor al negocio. ","b079a2ab":"## Feature Generator <a class=\"anchor\" id=\"16\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nSimilarmente a la competici\u00f3n de Time Series con Nico, se implementa una funci\u00f3n de Feature Generator donde se generan atributos conjuntos. De esta manera se pueden agrupar comportamientos frente al target, como por ejemplo: los del grupo de edad inferior cuantos productos compraron el mes anterior. El modelo mejora sustancialmente al incorporar estos atributos dado que ayuda a generalizar mejor. \n\nSe realizan los siguientes grupos de atributos:\n- pk_partition, cuentas_fans\n- pk_partition, ahorros_fans\n- pk_partition, financiacion_fans\n- pk_partition, salary_tram\n- pk_partition, age_tram\n- pk_partition, age_tram ,salary_tram\n- total_altas_1m_shift, total_altas_2m_shift, total_altas_3m_shift","da53890e":"# Model construction <a class=\"anchor\" id=\"3\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nEn este apartado se construyen 4 modelos diferentes. \n- [Modelo General](#31)\n- [Modelo Cuentas](#32)\n- [Modelo Ahorros](#33)\n- [Modelo Financiaci\u00f3n](#34)\n\nSe construye un modelo general donde se tienen en cuenta todas las compras, y tambi\u00e9n se construyen otros 3 modelos dependiendo del tipo de producto (cuentas, ahorros e inversi\u00f3n o financiaci\u00f3n) con el objetivo de ver si dividiendo las compras es posible obtener un mejor score que con un modelo general. \n\nM\u00e1s adelante se ver\u00e1 que debido a que el dataset est\u00e1 desbalanceado y que el problema resulta bastante complicado de predecir, los resultados no son tan buenos como se espera. \n\nY a continuaci\u00f3n se crea una funci\u00f3n que permitir\u00e1 balancear el dataset. El balanceo puede realizarse de dos maneras:\n- **Undersampling**. En este caso se aumenta el n\u00famero de registros de la clase minoritaria.\n- **Oversampling** En este caso se reduce el n\u00famero de registros de la clase mayoritaria, de manera que la distribuci\u00f3n no sea tan desproporcionada. ","8ddd50c9":"## Modelo General <a class=\"anchor\" id=\"31\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nEn este primer modelo, el target a predecir es *compra*, este incluye cualquier compra de cualquier producto o tipo de producto, por lo que el dataset es m\u00e1s rico y est\u00e1 mejor balanceado (por lo que no se balancea). El \u00fanico inconveniente que presenta este m\u00e9todo es la aletoriedad de la variable precio que es la variable que nos permite discernir entre que tipo de producto se compra o no. \n\nAl tener que rellenar la variable precio en los registros sin compra, la calidad del modelo podr\u00eda verse afectada. Sin embargo, pese a este hecho este modelo es el que mejor resultados da y los resultados finales son bastante satisfactorios delante del problema al que nos enfrentamos.","fd7151fa":"Y finalmente se eval\u00faa el modelo en validaci\u00f3n y se observa la confusion matrix. El ROC AUC score es bastante alto, sin embargo esto se debe al balanceo de los datos, donde falla este modelo es en el baj\u00edsimo Recall y la baja precision que tiene y es que al modelo le cuesta predecir positivo y se le escapan muchos casos positivos, y cuando lo hace se equivoca bastante. ","59d035e9":"**Importaci\u00f3n de librer\u00edas y dataset**\n\n","b71bfbce":"## Modelo Ahorros <a class=\"anchor\" id=\"33\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nEn este caso el caso a predecir es *compra_ahorros*, al igual que en el caso anterior, el dataset se balancea en este caso con un oversampling. Y ocurre m\u00e1s de lo mismo, al modelo le cuesta detectar las compras y cuando lo hace se equivoca demasiado. \n\nSi se reduce el n\u00famero de casos positivos (compras) para que el modelo aprenda que las compras son poco frecuentes, entonces predice siempre que no es compra. A\u00fan as\u00ed mantiene una distribuci\u00f3n constante entre el n\u00famero de predicciones de compra acertadas y erradas.","507ed7e1":"# Data Preparation <a class=\"anchor\" id=\"1\"><\/a>\n## Determinar altas y bajas <a class=\"anchor\" id=\"11\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nEn esta subtarea es importante detectar las altas de productos. La funci\u00f3n implementada es la misma que para la <a href=\"https:\/\/www.amazon.es\/Hands-Unsupervised-Learning-Using-Python\/dp\/1492035645\">Tarea 2: Segmentaci\u00f3n **PONER LINK BIEN!!!**<\/a>. Las altas ser\u00e1 nuestro target a predecir. Dado que queremos predecir cuando un usuario se da de alta o no, se contabiliza el estado de todos los productos en esa partici\u00f3n por cada cliente. Finalmente, la columna total indica 1 o mayor que 1, si un cliente se dio de alta en cualquier producto durante ese mes, o 0 si no se dio de alta. ","c01717a4":"## Tiempo desde la \u00faltima compra <a class=\"anchor\" id=\"14\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nComo es de esperar un mismo cliente no tiene el mismo comportamiento cada mes, es decir no realiza una compra cada mes sino que realiza una compra (se da de alta) un mes y lo vuelve a hacer dentro de 3 meses, por ejemplo. Una m\u00e9trica interesante a obtener es el tiempo que ha transcurrido desde su \u00faltima compra. \n\nEl siguiente c\u00f3digo tiene en cuenta este hecho. Un problema que plantea esta m\u00e9trica y que afectar\u00e1 al futuro modelo es qu\u00e9 pasa con los clientes que realizan su primera compra o clientes que no han comprado nada, qu\u00e9 tiempo desde la \u00faltima compra se supone. \n\nEn este apartado tambi\u00e9n se genera una nueva m\u00e9trica inter\u00e9s muy relacionada al tiempo desde la \u00faltima compra, conocer si el cliente ha comprado antes o no. ","741069bd":"# Conclusiones <a class=\"anchor\" id=\"4\"><\/a>\n\n[Tabla de Contenidos](#0) \n\nEsta tarea ha resultado ser la m\u00e1s compleja de todas sin ninguna duda, la creaci\u00f3n del modelo de Compra ha sido complicada. En un principio se utiliz\u00f3 variables como el *cluster* de la Tarea 2, variables que enmascaraban informaci\u00f3n de los clientes en el futuro; lo que se conoce como Data Leakage, aun as\u00ed el modelo tampoco ten\u00eda un score aceptable; sin embargo se observ\u00f3 que a\u00f1adirla mejoraba el modelo.\n\nEsto se debe a que en modelos en lugar de trabajar \u00fanicamente con datos de registros particulares, poder obtener atributos del comportamiento pasado de un grupo (ya sea clientes de la misma edad, clientes que han comprado anteriormente varios productos de financiaci\u00f3n...) ayudan al modelo y le permiten generalizar mejor. Este planteamiento viene totalmente inspirado de la competici\u00f3n de Kaggle de Time Series con Nico, es por ello que muchas de estos atributos tienen un factor de Time Series. \n\nEs importante destacar que el problema se eval\u00faa de forma estacionaria, es decir se pretende que el modelo aprenda que razones, combinaci\u00f3n de atributos, estados, etc. hacen que compre o no compre. (En el \u00e1rbol de decisi\u00f3n puede observarse qu\u00e9 atributos son los que impulsan a un cliente a decidir comprar o no). Y la validaci\u00f3n se realiza para el \u00faltimo mes.\n\nFinalmente, se puede afirmar que el modelo implementado es satisfactorio aunque tiene bastante margen de mejora, aunque necesitar\u00eda un estudio mucho m\u00e1s exhaustivo (m\u00e1s a\u00fan del que se le ha hecho).","3275f0c1":"## Precio del producto a comprar <a class=\"anchor\" id=\"18\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nExisten 14 productos diferentes, \u00e9stos pueden ser agrupados en diferentes tipos de productos; y cada tipo tiene un precio. En este apartado se crea una columna nueva que indica la inversi\u00f3n o precio del producto en el que el cliente se dio de alta. \n\nSin embargo, aparece el siguiente problema con esta variable, qu\u00e9 precio ponemos aquellos registros donde no hubo ninguna compra, en este caso se opta por rellenar estos registros de manera aleatoria con una distribuci\u00f3n equitativa entre 10, 40 y 60 euros. De esta manera se podr\u00e1 trabajar con un modelo General con todos los productos y en cada registro se supondr\u00e1 que el cliente deseaba comprar un producto con X precio, de esta manera el modelo tendr\u00e1 en cuenta cuando un cliente compra o no (teniendo en cuenta el precio del producto y la inversi\u00f3n que supone para \u00e9l), se espera que ante productos m\u00e1s caros, los clientes sean un poco m\u00e1s reacios a darse de alta. ","14674584":"## Tramificaci\u00f3n de variables num\u00e9ricas <a class=\"anchor\" id=\"15\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nEn este apartado se categorizan o tramifican las variables num\u00e9ricas: edad y salario; con el objetivo de poder utilizarlas en el Feature Generator. \n\nEn este paso tambi\u00e9n se descartan los clientes que han fallecido y los clientes menores de 18 a\u00f1os y mayores de 90. ","14b1c2cb":"# Data Cleaning <a class=\"anchor\" id=\"2\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nEl siguiente paso es limpiar el dataset de nulos y quedarse \u00fanicamente con las columnas relevantes para el modelo. Muchas de estas columnas no ser\u00e1n introducidas al modelo, pero ser\u00eda interesante encontrar nuevas m\u00e9tricas y tenerlas en cuenta para as\u00ed realizar un modelo m\u00e1s complejo y que tenga en cuenta m\u00e1s diversidad de casos. "}}