{"cell_type":{"9cbd940d":"code","b2fd881b":"code","c5bab66b":"code","c84bcc59":"code","566aec88":"code","311f9aaf":"code","d51f2e27":"code","428bc88b":"code","0bb9fee8":"code","2afaeb41":"code","7bd699b7":"code","8d8b0f2d":"code","874890a7":"code","e659bdb9":"code","6ac4ede6":"code","b9774c52":"code","9e2a0dcd":"code","552f2edd":"code","3c46e05c":"code","ee3e71f7":"code","8f7092ac":"code","690144b1":"code","a806e20e":"code","deb17fd7":"code","cdaaa03c":"code","e222eac0":"code","10b98272":"code","3cce5150":"code","00c20f06":"code","c8dee403":"code","444611b7":"code","45017161":"code","c6b17eb5":"code","ba5aa289":"code","ba7d6397":"code","a8e88f72":"code","0c4e9f68":"code","6b25ca08":"code","2f245086":"code","618485f5":"code","c1726769":"code","5b836aaa":"code","814c37d8":"code","74100441":"code","3e06458c":"code","69e6dc30":"code","8aac3bae":"code","071c0030":"code","ed180393":"code","b48064d1":"code","9f91cd12":"code","a4f88b33":"code","34a68298":"code","a4baff9b":"code","6740fc40":"code","5e051009":"markdown","5f825d51":"markdown","a662a9d5":"markdown","fdf6c865":"markdown","6b7cb935":"markdown","a60fc355":"markdown","51e2bf10":"markdown","03329b9c":"markdown","66ca0ce6":"markdown","085b7a24":"markdown","a427b702":"markdown","be6f404e":"markdown","236bf390":"markdown","65a4b28c":"markdown","58b10afa":"markdown","3852423c":"markdown","7d3cd6fa":"markdown","43f4704f":"markdown","fcb0b270":"markdown","c54c25ac":"markdown","62521899":"markdown","254214e3":"markdown","5c3f3d52":"markdown","5bc5564e":"markdown","8ec89554":"markdown","1dd942b1":"markdown","88d84056":"markdown","7efc6023":"markdown","4c22b581":"markdown","2dfb233d":"markdown","b9dcd51b":"markdown","254ea2da":"markdown"},"source":{"9cbd940d":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n\n#pd.set_option('display.max_columns',125)\n#pd.set_option('display.max_rows',200)","b2fd881b":"sharing=pd.read_csv(\"..\/input\/bike-sharing\/day.csv\")","c5bab66b":"sharing.head(10)","c84bcc59":"sharing.shape","566aec88":"sharing.info()","311f9aaf":"sharing.describe()","d51f2e27":"# just to be sure checking null values\nsharing.isnull().sum()\n# no null found","428bc88b":"sharing.nunique().sort_values()","0bb9fee8":"var_drop=['casual','registered','dteday','instant','atemp'] #variables to drop\nsharing=sharing.drop(var_drop,axis=1)\nsharing.head()","2afaeb41":"sharing.weathersit=sharing.weathersit.map({1: 'Clear', 2: 'Mist + Cloudy' , 3: 'Light Snow', 4: 'Heavy Rain'})\nsharing.season=sharing.season.map({1:'spring', 2:'summer', 3:'fall', 4:'winter'})\nsharing.weekday=sharing.weekday.map({0:'Sunday', 1:'Monday',2:'Tuesday',3:'Wednesday',4:'Thrusday',5:'Friday',6:'Saturday'})\nsharing.mnth=sharing.mnth.map({1:'January',2:'February',3:'March',4:'April',5:'May',6:'June',7:'July',8:'August',9:'September'\n                              ,10:'October',11:'November',12:'December'})\nsharing.head()","7bd699b7":"sharing.info()\n#all the non-binary columns are now object type","8d8b0f2d":"# for continous Numericalvariables\nnum_vars=['temp','hum','windspeed','cnt']\nL=len(num_vars)\nplt.figure(figsize=(10,10))\nfor i in range(1,L+1):\n    plt.subplot((L\/\/2)+1,2,i)\n    sharing[num_vars[i-1]].plot.box()","874890a7":"#Looks like windspeed has some outliers\nsharing.windspeed.quantile([0.0,0.25,0.50,0.75,0.95,1])\n# 0.75 and 1.00 quantile as high gap, but as the unit of windspeed\n# is not given, keeping this as it is.","e659bdb9":"# for Categorical variables\ncat_vars=['season','yr','mnth','holiday','weekday','workingday','weathersit']\nplt.figure(figsize=(25,25))\nL=len(cat_vars)\nfor i in range(1,L):\n    plt.subplot(L\/\/2,2,i)\n    sharing[cat_vars[i-1]].value_counts().plot.bar()\n    plt.title(cat_vars[i-1])\n    ","6ac4ede6":"# holiday and workingday has a very imbalanced ratio","b9774c52":"# cnt against Categorical variables\ncat_vars=['season','yr','mnth','holiday','weekday','workingday','weathersit']\nplt.figure(figsize=(30,20))\nL=len(cat_vars)\nfor i in range(1,L):\n    plt.subplot(L\/\/2,2,i)\n    #sharing[cat_vars[i-1]].value_counts().plot.bar()\n    sns.boxplot(x=cat_vars[i-1],y='cnt',data=sharing)    ","9e2a0dcd":"# cnt against numerical variables\nnum_vars=['temp','hum','windspeed']\nplt.figure(figsize=(10,12))\nL=len(num_vars)\nfor i in range(1,L+1):\n    plt.subplot((L\/\/2)+1,2,i)\n    sns.scatterplot(x='cnt',y=num_vars[i-1],data=sharing)","552f2edd":"sharing.corr()","3c46e05c":"plt.figure(figsize=(10,12))\nsns.heatmap(data=sharing.corr(),annot=True,cmap='YlGnBu')","ee3e71f7":"# dummyfying all the categorical variables\nto_dummy_vars=['season','mnth','weekday','weathersit']\nfor i in to_dummy_vars:\n    buffer=pd.get_dummies(sharing[i],drop_first=True)\n    sharing=pd.concat([sharing,buffer],axis=1)\n    sharing=sharing.drop(i,axis=1)","8f7092ac":"sharing.info()\n#all the categorical variables converted into object\/uint8\n#now we can proceed","690144b1":"#splitting Train, Test in 70:30\nsharing_train, sharing_test = train_test_split(sharing,train_size=0.7,random_state=100)","a806e20e":"scaler=MinMaxScaler()","deb17fd7":"num_vars=['temp','hum','windspeed','cnt']\nsharing_train[num_vars]=scaler.fit_transform(sharing_train[num_vars])\nsharing_train.head()","cdaaa03c":"# shape of test train split\nprint(sharing_train.shape)\nprint(sharing_test.shape)","e222eac0":"#seperating dependent and independent variables \ny_train=sharing_train.pop('cnt')\nX_train=sharing_train","10b98272":"lm=LinearRegression()\nlm.fit(X_train,y_train)","3cce5150":"#using RFE to reduce to 15 columns\nrfe=RFE(lm,15)\nrfe=rfe.fit(X_train,y_train)","00c20f06":"#checking the list RFE has selected\nlist(zip(X_train.columns,rfe.support_,rfe.ranking_))","c8dee403":"#reducing non-impartant columns in training data\ncol=X_train.columns[rfe.support_]\nX_train_rfe=X_train[col]","444611b7":"X_train_rfe.head()","45017161":"X_train_sm=sm.add_constant(X_train_rfe)\nlr_model=sm.OLS(y_train,X_train_sm).fit()\nlr_model.summary()","c6b17eb5":"#P-value of holiday is very big\n#but also checking VIF\n\nVIF= pd.DataFrame()\nVIF['features']=X_train_rfe.columns\nVIF['vif']=[vif(X_train_rfe.values,i) for i in range(X_train_rfe.shape[1])]\nVIF.vif=round(VIF.vif,2)\nVIF.sort_values(by='vif',ascending=False,inplace=True)\nVIF","ba5aa289":"# p-value shows that holiday is insignificant\n# as p-value removal takes higher precedence than VIF\n# dropping holiday and modelling again","ba7d6397":"X_train_rfe=X_train_rfe.drop('holiday',axis=1)\nX_train_sm=sm.add_constant(X_train_rfe)\n\nlr_model=sm.OLS(y_train,X_train_sm).fit()\nlr_model.summary()","a8e88f72":"# all p-vals are less than 0.05\n# let's check VIF now\n\nVIF= pd.DataFrame()\nVIF['features']=X_train_rfe.columns\nVIF['vif']=[vif(X_train_rfe.values,i) for i in range(X_train_rfe.shape[1])]\nVIF.vif=round(VIF.vif,2)\nVIF.sort_values(by='vif',ascending=False,inplace=True)\nVIF","0c4e9f68":"# dropping hum as it has high vif and modelling again","6b25ca08":"X_train_rfe=X_train_rfe.drop('hum',axis=1)\nX_train_sm=sm.add_constant(X_train_rfe)\n\nlr_model=sm.OLS(y_train,X_train_sm).fit()\nlr_model.summary()","2f245086":"# let's check VIF now\n\nVIF= pd.DataFrame()\nVIF['features']=X_train_rfe.columns\nVIF['vif']=[vif(X_train_rfe.values,i) for i in range(X_train_rfe.shape[1])]\nVIF.vif=round(VIF.vif,2)\nVIF.sort_values(by='vif',ascending=False,inplace=True)\nVIF","618485f5":"# as workingday has higher VIF than 2\n# dropping it and re-modelling","c1726769":"X_train_rfe=X_train_rfe.drop('workingday',axis=1)\nX_train_sm=sm.add_constant(X_train_rfe)\n\nlr_model=sm.OLS(y_train,X_train_sm).fit()\nlr_model.summary()","5b836aaa":"# let's check VIF now\n\nVIF= pd.DataFrame()\nVIF['features']=X_train_rfe.columns\nVIF['vif']=[vif(X_train_rfe.values,i) for i in range(X_train_rfe.shape[1])]\nVIF.vif=round(VIF.vif,2)\nVIF.sort_values(by='vif',ascending=False,inplace=True)\nVIF","814c37d8":"# as Saturday has high p-value than 0.05\n# dropping it and re-modelling","74100441":"X_train_rfe=X_train_rfe.drop('Saturday',axis=1)\nX_train_sm=sm.add_constant(X_train_rfe)\n\nlr_model=sm.OLS(y_train,X_train_sm).fit()\nlr_model.summary()","3e06458c":"# let's check VIF now\n\nVIF= pd.DataFrame()\nVIF['features']=X_train_rfe.columns\nVIF['vif']=[vif(X_train_rfe.values,i) for i in range(X_train_rfe.shape[1])]\nVIF.vif=round(VIF.vif,2)\nVIF.sort_values(by='vif',ascending=False,inplace=True)\nVIF","69e6dc30":"# temp has a VIF greater than 5\n# but as we know from EDA it's a important variable\n# in buisness terms temp has higher correlation with cnt\n# so dropping the next highest VIF ie windspeed and re-modelling","8aac3bae":"X_train_rfe=X_train_rfe.drop('windspeed',axis=1)\nX_train_sm=sm.add_constant(X_train_rfe)\n\nlr_model=sm.OLS(y_train,X_train_sm).fit()\nlr_model.summary()","071c0030":"# let's check VIF now\n\nVIF= pd.DataFrame()\nVIF['features']=X_train_rfe.columns\nVIF['vif']=[vif(X_train_rfe.values,i) for i in range(X_train_rfe.shape[1])]\nVIF.vif=round(VIF.vif,2)\nVIF.sort_values(by='vif',ascending=False,inplace=True)\nVIF","ed180393":"#this is the final Model and its R2 score\nprint(lr_model.summary())\n\ny_train_pred = lr_model.predict(X_train_sm)\n#***************************************************************************************\nr2_score=round(r2_score(y_true= y_train,y_pred= y_train_pred),4)\n#***************************************************************************************\nprint(f\"\\n R^2 of train set is: {r2_score}\")","b48064d1":"y_train_pred = lr_model.predict(X_train_sm)\nres = y_train - y_train_pred\nsns.distplot(res)\nplt.show()","9f91cd12":"# Scaling numerical variables\nnum_vars=['temp','hum','windspeed','cnt']\nsharing_test[num_vars]=scaler.transform(sharing_test[num_vars])\nsharing_test.head()","a4f88b33":"# Making X and Y test sets\ny_test = sharing_test.pop('cnt')\nX_test = sharing_test\nX_test.head()","34a68298":"# dropping variables from X_test which were dropped in Model building\nvars=['yr', 'temp', 'spring', 'summer', 'winter', 'July',\n       'September', 'Sunday', 'Light Snow', 'Mist + Cloudy']\nX_test=X_test[vars]\nX_test_sm=sm.add_constant(X_test)\nX_test_sm.head()","a4baff9b":"y_test_pred = lr_model.predict(X_test_sm)\n#***************************************************************************************\nfrom sklearn.metrics import r2_score\nr2_score_test=round(r2_score(y_true= y_test,y_pred= y_test_pred),4)\n#***************************************************************************************\nprint(f\"R^2 of test set is: {r2_score_test}\")","6740fc40":"Best_fit_line=lr_model.params.reset_index()\nBest_fit_line.columns=['Variable','Coefficient']\nBest_fit_line.sort_values(by='Coefficient',ascending=False)","5e051009":"- There are 730 non-null rows and 16 columns\n- a mix of integer, float and object data type\n- [yr], [holiday], [workingday] looks like binary columns \n- None of the columns seems to have any significant outliers","5f825d51":"#### Prepairing data for modelling\n \n <ins>Splitting test train data<ins>","a662a9d5":"## Model building\n###     Using RFE","fdf6c865":"#### Check for null and unique values","6b7cb935":"#### Final Comments","a60fc355":"## Problem Statement:","51e2bf10":"- Residual is a normal distribution\n- with mean centered around zero\n\n#### Prediction","03329b9c":"#### Equation of best fit line","66ca0ce6":"#### According to this and Data Dictionary\n- yr(0: 2018, 1:2019), holiday(1:Yes, 0:No), workingday (1:Yes, 0:No) are binary\n- weathersit (1: Clear, 2: Mist + Cloudy , 3: Light Snow, 4: Heavy Rain) is categorical\n- season (1:spring, 2:summer, 3:fall, 4:winter) is categorical\n- weekday (0:Sunday, 1:Monday, ... , 6:Saturday) is categorical\n- instant is just a index which is not needed\n- dteday is date which is also not needed in model (we have year and month for that)\n- casual, registered are summed in cnt, so they are also not needed\n- atemp is just derived from temp so keeping only one of them\n\n#### Dropping and Transforming variables","085b7a24":"#### Model is now reduced to 10 variables\n#### with a R^2 of 0.824 and adjusted R^2 of 0.820\n#### VIFs are also less than 5","a427b702":"## Data Prepration\n\n#### Creating Dummy variables for categorical features","be6f404e":"## Residual Analysis and Evaluation\n\n#### Residual","236bf390":"at 95% confidence we can say that these variable are affecting 'cnt'<br>\nfinal equation<br><br>\ncnt = <br>\n0.153360 + 0.503843 * temp + 0.232903 * yr + 0.082904 * winter + 0.076610 * September + 0.037937 * summer - 0.044912 * Sunday -            0.048508 * July - 0.077804 * spring - 0.078456 * Mist + Cloudy - 0.299455 * Light Snow\n<br>\n<br>\ntemp is highest positively affecting variable and Light Snow is the highest negatively affecting variable.\n","65a4b28c":"- temp shows linear relation\n- hum and windspeed looks like random","58b10afa":"#### Correlation","3852423c":"        Iteration : 4","7d3cd6fa":"- cnt and temp has maximum correlation\n- cnt and yr is second best correlated","43f4704f":"###  Using StatsModels\n    \n        Iteration : 1","fcb0b270":"        Iteration : 3","c54c25ac":"## EDA\n\n#### Importing the libraries.","62521899":"        Iteration : 5","254214e3":"<ins>Rescaling of variables<ins>","5c3f3d52":"#  Bike Sharing Assignment","5bc5564e":"#### Checking structure of data","8ec89554":"        Iteration : 2","1dd942b1":"#### Reading the Data set","88d84056":"#### Performing Bi-variate analysis","7efc6023":"#### Performing Univariate analysis","4c22b581":"A bike-sharing system is a service in which bikes are made available for shared use to individuals. Many bike share systems allow people to borrow a bike from a \"dock\" which is usually computer-controlled wherein the user enters the payment information.\n\n**BoomBikes** a bike-sharing company, aspires to understand the demand for shared bikes among the people and wants to know:\n    - Which variables are significant in predicting the demand for shared bikes\n    - How well those variables describe the bike demands","2dfb233d":"## Structure of Analysis\n    1.EDA\n    2.Data preparation \n    3.Model building\n    4.Residual Analysis and evaluation\n    5.Final Comments","b9dcd51b":"        Iteration : 6","254ea2da":"- **Season** : highest count of rental bikes was seen in fall \n- **Year** : Clearly count of rental bikes increased in 2019\n- **Month** : Best months have been from June to September\n- Being a **Holiday**, **Weekday**, **Workingday** doesn't make much difference as medians are very close\n- **holiday** : count of rental bikes is slightly higher on non-holiday"}}