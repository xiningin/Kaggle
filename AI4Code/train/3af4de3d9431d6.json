{"cell_type":{"664baa4d":"code","7045e7b7":"code","b43764fe":"code","0e5f497b":"code","27b5e023":"code","e3491f2a":"code","7a9ed4d2":"code","beed3c36":"code","3814d96c":"code","cdef01dd":"code","51a00a9d":"code","170caeb0":"code","9cf8becb":"code","1d4a4d5f":"code","24d64741":"code","67ff21f8":"code","564767a9":"code","ad66153f":"code","dbc93b31":"code","3029ab03":"code","de61babe":"code","7a67b52c":"code","9da31403":"code","5a631d72":"code","67943f56":"code","1fc9d72d":"code","6818925c":"code","427fa8aa":"code","3f859dc4":"code","03fff10f":"code","996ea169":"code","03d6559e":"code","8810e317":"code","ccd96363":"code","3480a017":"code","2e0e433f":"code","bff12a3a":"code","7d584858":"code","668b16ed":"code","f2521768":"code","faa6899e":"code","283feb0c":"code","ad3ce1b4":"code","977a9276":"code","dfadc940":"code","d84201ae":"code","22d39c66":"code","1685b294":"code","cce42c19":"code","2c4d6342":"markdown","e5da649f":"markdown","dfe345f2":"markdown","ac617f3e":"markdown","3823c5f0":"markdown","83527422":"markdown","6e4cb281":"markdown"},"source":{"664baa4d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nprint(os.listdir(\"..\/input\"))\ntrain='..\/input\/Train.csv'\ntrain_set1=pd.read_csv(train)\ntest='..\/input\/Test.csv'\ntest_set=pd.read_csv(test)","7045e7b7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nprint(os.listdir(\"..\/input\"))\ntrain='..\/input\/Train.csv'\ntrain_set1=pd.read_csv(train)\ntest='..\/input\/Test.csv'\ntest_set=pd.read_csv(test)","b43764fe":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity='all'","0e5f497b":"train_set1.shape\ntest_set.shape","27b5e023":"train_set=pd.concat([train_set1,test_set])\ntrain_set.head(5)\ntrain_set.describe()","e3491f2a":"train_set.info()","7a9ed4d2":"train_set.isnull().sum()\nfor cols in train_set.columns:\n    if (train_set[cols].dtype=='object'):\n        print(train_set[cols].value_counts())","beed3c36":"#Checking for the availability of items in outlets\nitem_count=train_set['Item_Identifier'].value_counts().sort_values()\nplt.hist(item_count,bins=np.arange(0,11,1))\ntrain_set['Item_Identifier'].describe()\nplt.show()","3814d96c":"item_count_df=pd.DataFrame(item_count)\nitem_cnt=(item_count_df['Item_Identifier'].value_counts()\/sum(item_count_df['Item_Identifier'])*100)\nplt.xlabel('Number of items in outstores')\nitem_count_df['Item_Identifier'].value_counts().plot(kind='bar',color='orange')\n\n#The below graph shows the availabilty and distribution of items in number of stores.\n#As we can see that there are 13 items that are available only in 7 store, likewise 305 items in 8,504 items in 10, 737 in 9 stores","cdef01dd":"# Item Weight\n\n# We can see \"Item Weight\" has missing values. From the distribution in data we can see that there are hikes \n# in between and even falls\ntrain_set['Item_Weight'].describe()\ntrain_wtou_null=train_set['Item_Weight'].dropna()\nplt.hist(train_wtou_null,bins=30)\nplt.show()","51a00a9d":"# Item_Fat_Content\n\n# We can see the inconsistency in data, as there are only 2 unique categories as \"Low Fat\" and \"Regular\"\n# We will need to convert \"LF\" and \"low fat\" into \"Low Fat\" and \"reg\" into \"Regular\"\n# From the bar chart, we can see that about 65% of the items have low fat.\n# That is people perfer purchasing low fat Item\n\ntrain_set['Item_Fat_Content'].value_counts()\ntrain_set.loc[train_set['Item_Fat_Content']=='LF','Item_Fat_Content']='Low Fat'\ntrain_set.loc[train_set['Item_Fat_Content']=='low fat','Item_Fat_Content']='Low Fat'\ntrain_set.loc[train_set['Item_Fat_Content']=='reg','Item_Fat_Content']='Regular'\ncontent_cnt=pd.DataFrame(train_set.groupby(['Item_Identifier','Item_Fat_Content'])['Item_Fat_Content'].count())\ncontent_cnt=content_cnt.rename(columns={'Item_Fat_Content':'Cnt'}).reset_index()\ncontent_cnt['Item_Fat_Content'].value_counts()\/sum(content_cnt['Item_Fat_Content'].value_counts())\n(content_cnt['Item_Fat_Content'].value_counts()\/sum(content_cnt['Item_Fat_Content'].value_counts())).plot(kind='bar')","170caeb0":"#Item Visibility\n#we can see that most of the items have visibility less that 5%,There are few outliers and see that the distribution is rightskewed\n#calculating the 95th and 99th percentiles,\n\nb,c,d= np.percentile(train_set['Item_Visibility'],[90,95,99])\nprint(b,c,d)\ntrain_set['Item_Visibility'].describe()\ntrain_set['Item_Visibility'].hist(bins=20)","9cf8becb":"# If we from the below chart we can see that average visibility Small Outlet is more as compared to other outlet size \ntrain_set.groupby('Outlet_Size')['Item_Visibility'].mean()\nsns.scatterplot(data=train_set,y='Item_Type',x='Item_Visibility',hue='Outlet_Size')","1d4a4d5f":"# fromthe below histogram we can that it a multimodal distribution \ntrain_set['Item_MRP'].describe()\ntrain_set['Item_MRP'].hist(bins=30)","24d64741":"#Most of the item sold belongs to category \"Fruits and Vegetables\" and \"Snack Foods\".\n# Seafood have low demand\ntrain_set['Item_Type'].value_counts().plot(kind='bar',color='orange')","67ff21f8":"# By considering the combination of item and item type, will give us the unique count of Item type \ntype_cnt=pd.DataFrame(train_set.groupby(['Item_Identifier','Item_Type'])['Item_Type'].count())\ntype_cnt=type_cnt.rename(columns={'Item_Type':'Cnt'}).reset_index()\ntype_cnt['Item_Type'].value_counts()\/sum(type_cnt['Item_Type'].value_counts())*100\n(type_cnt['Item_Type'].value_counts()\/sum(type_cnt['Item_Type'].value_counts())*100).plot(kind='bar',color='orange')","564767a9":"#From the below analysis we can say that outlet_identifier 'OUT010' and 'OUT019' are Grocery Stores.Hence the\n# chances of getting the items belonging to some specific catgories decreases \ntrain_set['Outlet_Identifier'].value_counts().plot(kind='bar',color='orange')\n#train_set.sort_values(by='Item_Identifier')\ntrain_set.groupby(['Outlet_Identifier','Outlet_Type'])['Outlet_Identifier'].unique()","ad66153f":"# Outlet_Establishment_Year.\n# Converting the Outlet_Establishment_Year in Date Format\ntrain_set['Outlet_Establishment_Year']=train_set['Outlet_Establishment_Year'].astype(str).apply(lambda x: pd.to_datetime(x, format='%Y%'))\ntrain_set['Year']=train_set['Outlet_Establishment_Year'].dt.year\ntrain_set['Year'].value_counts()","dbc93b31":"train_set['Outlet_Size'].value_counts()\ntrain_set['Outlet_Location_Type'].value_counts()\ntrain_set['Outlet_Type'].value_counts()","3029ab03":"train_set.groupby(['Outlet_Location_Type','Outlet_Type','Outlet_Size'])['Outlet_Identifier'].count()","de61babe":"# We can see that there are outliers present in data\ntrain_set['Item_Outlet_Sales'].hist()\ntrain_set['Item_Outlet_Sales'].describe()","7a67b52c":"outlet_size_mode= train_set.pivot_table(values='Outlet_Size', columns = 'Outlet_Type',aggfunc=lambda x: x.mode())\noutlet_size_mode","9da31403":"def impute_size_mode(cols):\n    Size = cols[0]\n    Type = cols[1]\n    if pd.isnull(Size):\n        return outlet_size_mode.loc['Outlet_Size'][outlet_size_mode.columns == Type][0]\n    else:\n        return Size\n    \ntrain_set['Outlet_Size'] = train_set[['Outlet_Size','Outlet_Type']].apply(impute_size_mode,axis=1)","5a631d72":"item_wt=train_set.pivot_table(values='Item_Weight',columns='Item_Identifier',aggfunc='mean')\n\ndef itm_wt(cols):\n    itm=cols[0]\n    wt=cols[1]\n    if(pd.isnull(wt)):\n        return item_wt.loc['Item_Weight'][item_wt.columns== itm][0]\n    else:\n        return wt\n    \ntrain_set['Item_Weight']=train_set[['Item_Identifier','Item_Weight']].apply(itm_wt,axis=1)","67943f56":"#We also found that the visibility of the item cannot be zero hence, we can subsitute the mean of 'Item Visibility' where \n# Item visibility is zero\n# We can see the hike in the chart as we imputed the mean.\n\ntrain_set.loc[train_set['Item_Visibility']==0.000000,'Item_Visibility']=np.mean(train_set['Item_Visibility'])\nsns.distplot(train_set['Item_Visibility'],bins=50)\nplt.show()\n\ntrain_set.isnull().sum()","1fc9d72d":"# We can see that the net sales of the regular fat item is slightly high.\npd.pivot_table(data=train_set,columns='Item_Fat_Content',values='Item_Outlet_Sales',aggfunc='mean').plot(kind='bar')\npd.pivot_table(data=train_set,columns='Item_Fat_Content',values='Item_MRP',aggfunc='mean')\ntrain_set.groupby(['Item_Fat_Content'])['Item_Identifier'].count()\nstats.spearmanr(train_set['Item_Fat_Content'],train_set['Item_MRP'])\n","6818925c":"sales_loc=pd.pivot_table(data=train_set,index=['Item_Fat_Content','Outlet_Location_Type'],values='Item_Outlet_Sales',aggfunc='mean').reset_index()\nsales_loc\nplt.figure(figsize=(5,5))\nsns.barplot(data=sales_loc,x='Item_Fat_Content',hue='Outlet_Location_Type',y='Item_Outlet_Sales')","427fa8aa":"plt.figure(figsize=(8,8))\nplt.subplot(2,1,2)\ntrain_set['Item_Type'].value_counts().plot(kind='bar',color='orange')\npd.pivot_table(data=train_set,index='Item_Type',values=['Item_Outlet_Sales','Item_MRP'],aggfunc='mean')\nplt.subplot(2,1,1)\nsns.barplot(data=train_set,y='Item_Type',x='Item_Outlet_Sales',color='orange')","3f859dc4":"#if we see the average sales,tier 2,3 have more average sales as compared to other cities. It may be due to tier 2,tier 3 are moving to\n#towards adopting new products, may be the products have more discounts and sales may increase\nloc_sales=pd.pivot_table(data=train_set,index=['Outlet_Location_Type'],values='Item_Outlet_Sales',aggfunc='mean').reset_index()\nloc_sales\nplt.figure(figsize=(8,8))\nax = sns.boxplot(x=\"Outlet_Location_Type\", y=\"Item_Outlet_Sales\", data=train_set)\n#sns.boxplot(data=loc_sales,x='Item_Outlet_Sales',hue='Outlet_Location_Type')","03fff10f":"train_set.isnull().sum()","996ea169":"train_set.groupby(['Outlet_Identifier','Outlet_Type','Outlet_Size'])['Item_Outlet_Sales'].mean().plot(kind='bar',color='blue')\n#outlet size doesnt make more difference in the sales, if we compare the sales outlet type wise, then Supermarket Type1 with \n#size : [Small,Medium,high] shows almost same average sales. ","03d6559e":"#As the OUT010 and OUT019 are small grocery store,the number of items available are less as compared to othe stores \n#therefore tha average sales of this outlets is less\ntrain_set.groupby(['Outlet_Identifier','Outlet_Size','Outlet_Location_Type'])['Item_Identifier',].count()\ntrain_set.groupby(['Outlet_Identifier'])['Item_Identifier'].count().plot(kind='bar',color='orange')","8810e317":"# if we see that average sales for the item, it's pretty high in Tier 3 city, this can be due to people from this cities\n#moving to desire and try on new products that are available in supermarket of Tier 1 cities\ntrain_set.groupby(['Outlet_Size','Outlet_Location_Type'])['Item_Outlet_Sales'].mean().plot(kind='bar',color='blue')","ccd96363":"ItemType_sales=pd.pivot_table(data=train_set,index=['Item_Type','Outlet_Identifier'],values='Item_Outlet_Sales',aggfunc='mean').reset_index()\nplt.figure(figsize=(15,15))\ng=sns.FacetGrid(data=train_set,row='Outlet_Identifier',aspect=2)\ng=g.map(plt.bar,'Item_Type','Item_Outlet_Sales')\nfor ax in g.axes.flat:\n    for label in ax.get_xticklabels():\n        label.set_rotation(90)","3480a017":"#Adding an Item Category Variable by observing the Item Identifier,","2e0e433f":"def create_Category(data):\n    if str(data['Item_Identifier']).startswith('NC'):\n        return 'Non Consumable'\n    elif str(data['Item_Identifier']).startswith('FD'):\n        return 'Food'\n    else:\n        return 'Drinks'\n    #train_set['check1']= train_set['Item_Identifier'].apply(lambda x: x[0:2])\n\ntrain_set['Item_Category']= train_set.apply(create_Category,axis=1)\n\ntrain_set['Item_Category'].value_counts()","bff12a3a":"cat_sales=pd.pivot_table(data=train_set,index=['Item_Category','Outlet_Identifier','Outlet_Type'],values='Item_Outlet_Sales',aggfunc='mean').reset_index()\ng=sns.FacetGrid(data=cat_sales,col='Item_Category',hue='Outlet_Type',legend_out=True)\ng=g.map(plt.bar,'Outlet_Identifier','Item_Outlet_Sales')\n\nfor ax in g.axes.flat:\n    for label in ax.get_xticklabels():\n        label.set_rotation(90)","7d584858":"# After  dividing the items in to item category, we can find that category \"Non consumable\" will have no fats hence \n# adding a level to Item_Fat_Content as \"No Fats\" \n\ntrain_set.loc[train_set['Item_Category']=='Non Consumable','Item_Fat_Content']='No Fat'\ntrain_set['Item_Fat_Content'].value_counts()","668b16ed":"train_set['Visibility_log']=np.log10(train_set['Item_Visibility'])\nplt.hist(train_set['Visibility_log'])","f2521768":"train_set=pd.get_dummies(train_set,columns=['Item_Fat_Content'])\n#train_set=pd.concat([train_set,train_fat_content])","faa6899e":"train_set=pd.get_dummies(train_set,columns=['Item_Type','Outlet_Identifier','Outlet_Location_Type','Outlet_Size','Outlet_Type','Item_Category'])","283feb0c":"list1=['Item_MRP','Item_Visibility', 'Item_Weight',\n       'Year', 'Visibility_log', 'Item_Fat_Content_Low Fat',\n       'Item_Fat_Content_No Fat', 'Item_Fat_Content_Regular',\n       'Item_Type_Baking Goods', 'Item_Type_Breads', 'Item_Type_Breakfast',\n       'Item_Type_Canned', 'Item_Type_Dairy', 'Item_Type_Frozen Foods',\n       'Item_Type_Fruits and Vegetables', 'Item_Type_Hard Drinks',\n       'Item_Type_Health and Hygiene', 'Item_Type_Household', 'Item_Type_Meat',\n       'Item_Type_Others', 'Item_Type_Seafood', 'Item_Type_Snack Foods',\n       'Item_Type_Soft Drinks', 'Item_Type_Starchy Foods',\n       'Outlet_Identifier_OUT010', 'Outlet_Identifier_OUT013',\n       'Outlet_Identifier_OUT017', 'Outlet_Identifier_OUT018',\n       'Outlet_Identifier_OUT019', 'Outlet_Identifier_OUT027',\n       'Outlet_Identifier_OUT035', 'Outlet_Identifier_OUT045',\n       'Outlet_Identifier_OUT046', 'Outlet_Identifier_OUT049',\n       'Outlet_Location_Type_Tier 1', 'Outlet_Location_Type_Tier 2',\n       'Outlet_Location_Type_Tier 3', 'Outlet_Size_High', 'Outlet_Size_Medium',\n       'Outlet_Size_Small', 'Outlet_Type_Grocery Store',\n       'Outlet_Type_Supermarket Type1', 'Outlet_Type_Supermarket Type2',\n       'Outlet_Type_Supermarket Type3', 'Item_Category_Drinks',\n       'Item_Category_Food', 'Item_Category_Non Consumable','Item_Outlet_Sales']\ntrain_set=train_set[list1]","ad3ce1b4":"#dividing the train_set into training and test set\ntrain=train_set.loc[train_set['Item_Outlet_Sales'] > 0]\ntest=train_set.loc[train_set['Item_Outlet_Sales'].isnull()]\ntrain.shape\ntest.shape","977a9276":"train.head(2)\ntest.head(2)","dfadc940":"del test['Item_Outlet_Sales']","d84201ae":"train.shape\ntest.shape","22d39c66":"#preparing the Validation data \nX_train=train.iloc[:, :-1]\nY_train=train.iloc[:,-1]\ntest.shape","1685b294":"from sklearn.linear_model import LinearRegression\n\nregresor= LinearRegression()\nregresor.fit(X_train,Y_train)","cce42c19":"y_pred=regresor.predict(test)\ny_pred1=pd.DataFrame(y_pred)\ny_pred1.to_csv('pred', sep='\\t', encoding='utf-8')","2c4d6342":"Understanding the Features:\n\nThe dataset has, 7 Categorical , 4 continuous variable and 1 discrete variable.\nBut we can see \"Outlet_Establishment_Year\" which is an year is in Int format,will need to convert it to Date.\nThere are missing values available in the data. \nWe can see that features : \"Item Weight\" and \"Outlet_Size\" have missing values we will need to impute this.","e5da649f":"If we see the Item_Outlet_sales variable, we can see that distribution is not normal and has outliers.\nEven the Item Visibility variable the minimum value is zero, which is something we need to see as the visibility of item cannot be zero.","dfe345f2":"Handling the Missing Values","ac617f3e":"Univariate Analysis\n\nAnalysing Item Identifier","3823c5f0":"Understanding the Categorical Variables and its disturbution","83527422":"For our analysis concatinating the train and test,and later will split again.","6e4cb281":"Understanding the data: \n    We can observe that train_set1 has 8523 rows an 12 columns while the test data has 5681 rows and 11 columnsexcluding the dependent values."}}