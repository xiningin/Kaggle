{"cell_type":{"9339cd5b":"code","3ad457ab":"code","5129e3b5":"code","492ee5ea":"code","66ea2df1":"code","47131aa6":"code","93ea063f":"code","3500a02f":"code","95d5acd6":"code","8dd4faa6":"code","df1e9221":"code","df0dd6c2":"code","d6596372":"code","e305f89c":"code","3e03dbdf":"code","7c71757c":"code","6811f09f":"code","baa422ad":"code","86c54136":"code","9eaeacc1":"code","2f71e26a":"code","ec181533":"code","43961b58":"code","c4fdea73":"code","ff67524a":"code","943fd949":"code","a2fd6f72":"code","4e59c933":"code","4c4815f1":"code","de7b0ff6":"code","a6df6659":"code","f32301ac":"code","9c8f19f6":"code","9a1eb183":"code","056e51f2":"code","c8094138":"code","cf2d44e4":"code","5161a96e":"code","cab34c43":"code","c17d2058":"code","e7e6bb32":"code","6bd4642b":"code","0075064c":"code","88b5e088":"code","9dff5f42":"code","a56a0ac0":"code","43d18612":"code","774aece3":"code","4fc79525":"code","9c678ab0":"code","3100d959":"code","61672f23":"code","6af498c5":"code","faa7be3d":"code","65247f26":"code","fdd81053":"code","a4e98931":"code","28bd3d70":"code","0c1c8cf9":"code","94ed5867":"code","9eadb91b":"code","8e10c9be":"code","f3ffb0e9":"code","83094dc5":"code","7dfdaf91":"code","01f06ad6":"code","4464d502":"code","7378942a":"code","38b1e172":"code","8cac3811":"markdown","426cbbc4":"markdown","80a2523c":"markdown","35646b24":"markdown","4608942b":"markdown","e529e4e0":"markdown","b171f00c":"markdown"},"source":{"9339cd5b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3ad457ab":"#\u30ed\u30fc\u30f3\u8fd4\u6e08\u306e\uff12\u5024\u5206\u985e\n#0\u306f\u8fd4\u3057\u305f\u4eba\uff11\u306f\u8cb8\u3057\u5012\u308c\u305f\u4eba\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\nfrom pandas import DataFrame, Series\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom category_encoders import OrdinalEncoder, OneHotEncoder, TargetEncoder\nfrom tqdm import tqdm_notebook as tqdm\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nimport shap\n\n\nfrom datetime import datetime as dt\n\nfrom tensorflow.keras.layers import Dense ,Dropout, BatchNormalization, Input, Embedding, SpatialDropout1D, Reshape, Concatenate\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.metrics import AUC\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold, train_test_split\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\n\nimport random\nfrom collections import Counter, defaultdict","5129e3b5":"df_train = pd.read_csv('..\/input\/datarobot\/train.csv', index_col=0, parse_dates=['issue_d','earliest_cr_line'] )\n#df_train = pd.read_csv('..\/input\/homework-for-students2\/train.csv', index_col=0)\ndf_test = pd.read_csv('..\/input\/datarobot\/test.csv', index_col=0,parse_dates=['issue_d','earliest_cr_line'] )","492ee5ea":"df_train.info()","66ea2df1":"##pandasprofiling\u3092\u4f7f\u7528\u3057\u3066\u30c7\u30fc\u30bf\u5168\u4f53\u306e\u72b6\u6cc1\u3092\u53ef\u8996\u5316\n# import pandas_profiling as pan\n# pan.ProfileReport(df_train)\n","47131aa6":"# #\u5143\u306e\u30c7\u30fc\u30bf\u4ee5\u5916\u306b\u7528\u610f\u3055\u308c\u3066\u3044\u308b\u30c7\u30fc\u30bf\u3092\u30a4\u30f3\u30dd\u30fc\u30c8\uff08\u4f7f\u7528\u3059\u308b\u306e\u304b\u306f\u5f8c\u307b\u3069\u691c\u8a0e\uff09\n# #\u5dde\u306e\u7def\u5ea6\u3068\u7d4c\u5ea6\u3068\u753a\n# state=pd.read_csv('..\/input\/homework-for-students4plus\/statelatlong.csv')\n# #\u5404\u5dde\u306eGDP\u306e\u72b6\u6cc1\n# gdp=pd.read_csv('..\/input\/homework-for-students4plus\/US_GDP_by_State.csv')\n# #\u30ab\u30e9\u30e0\u540d\u3068\u305d\u306e\u5185\u5bb9\u306e\u8aac\u660e\n# description=pd.read_csv('..\/input\/homework-for-students4plus\/description.csv')\n# #\u90f5\u4fbf\u756a\u53f7\u306b\u95a2\u4fc2\u3059\u308b\u9805\u76ee\uff08zipcodetype\u306f\u4f7f\u3048\u308b\u304b\u3082\u3057\u308c\u306a\u3044\uff09\n# free_zip=pd.read_csv('..\/input\/homework-for-students4plus\/free-zipcode-database.csv')","93ea063f":"#test\u30c7\u30fc\u30bf\u306b\u306f2016\u5e74\u306e\u30c7\u30fc\u30bf\u306e\u307f\u3001train\u30c7\u30fc\u30bf\u306e\u25cb\u25cb\u5e74\u4ee5\u964d\u306e\u60c5\u5831\u3092\u53d6\u5f97\ndf_train = df_train[df_train['issue_d'].dt.year >= 2013]","3500a02f":"##X\u3068y\u306b\u5206\u5272\u3057\u307e\u3059","95d5acd6":"y_train = df_train.loan_condition\nX_train = df_train.drop(['loan_condition'], axis=1)\n\nX_test = df_test","8dd4faa6":"#\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3068\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30c7\u30fc\u30bf\u3092\u7d50\u5408\nX_all = pd.concat([X_train,X_test],axis=0)","df1e9221":"# fig = plt.figure()\n# ax = fig.add_subplot(1,1,1)\n\n# a=X_all['loan_amnt'].apply(np.log1p).hist()\n# b=X_all['loan_amnt'].hist()\n\n# ax.hist(a,bins=50,alpha=0.5,color='r')\n# ax.hist(b,bins=50,alpha=0.5,color='b')\n# ax.show()","df0dd6c2":"# X_state=X_all.groupby(['addr_state','grade']).mean()\nX_state=X_all.groupby('addr_state').mean()","d6596372":"####\u8981\u5909\u66f4####\nif X_all.earliest_cr_line is not None :\n    X_all['term']=(X_all.issue_d.dt.year *12 + X_all.issue_d.dt.month)-(X_all.earliest_cr_line.dt.year*12 + X_all.earliest_cr_line.dt.month)\n#     X_all['term\/12']=((X_all.issue_d.dt.year *12 + X_all.issue_d.dt.month)-(X_all.earliest_cr_line.dt.year*12 + X_all.earliest_cr_line.dt.month))\/12\n    X_all['term']=X_all['term'].apply(np.log1p)","e305f89c":"# X_all['term'].apply(np.log1p).hist()","3e03dbdf":"################\nX_all= X_all.drop(['issue_d'],axis=1)\nX_all['earliest_cr_line'] = X_all.earliest_cr_line.dt.year * 10 + X_all.earliest_cr_line.dt.month","7c71757c":"#\u6b20\u640d\u5024\u306e\u6570\u30d5\u30e9\u30b0\nX_all['num']=X_all.isnull().sum(axis=1)\n\nX_all['dti'].fillna(X_all['dti'].median(),inplace=True)\n\nX_all['dti_a']=X_all['dti']\/100*X_all['annual_inc']\/12\nX_all['dti_b']=X_all['dti']-X_all['installment']\n\nX_all['tot_cur_bal_a']=X_all['tot_cur_bal']\/X_all['annual_inc']\n\nX_all['loan_1']=X_all['loan_amnt']\/X_all['installment']\nX_all['loan_2']=X_all['loan_amnt']\/X_all['annual_inc']\nX_all['loan_3']=X_all['loan_amnt']\/X_all['revol_bal']\nX_all['loan_4']=X_all['loan_amnt']\/X_all['tot_cur_bal']\n\nX_all['ins_1']=X_all['installment']\/X_all['annual_inc']\nX_all['ins_2']=X_all['installment']\/X_all['revol_bal']\nX_all['ins_3']=X_all['installment']\/X_all['tot_cur_bal']\n\nX_all['revol_bal_a']=X_all['revol_bal']\/X_all['annual_inc']\nX_all['revol_bal_b']=X_all['revol_bal']\/X_all['tot_cur_bal']\n\n## \u50cd\u3044\u3066\u3044\u306a\u3044\u4eba\u306e\u30d5\u30e9\u30b0\n# X_all['unemp_flg']=X_all['emp_title'].notnull().replace({True:0,False:1})\n# X_all['unemp_flg']=X_all['unemp_flg'].astype('category')\n\nX_all['sub_grade_freq']=X_all.groupby('sub_grade')['sub_grade'].transform('count')\nX_all['sub_grade_inc_med']=X_all.groupby('sub_grade')['annual_inc'].transform('median')\nX_all['sub_grade_inc_med_diff']=X_all['annual_inc']-X_all['sub_grade_inc_med']\nX_all['sub_grade_bal_med']=X_all.groupby('sub_grade')['tot_cur_bal'].transform('median')\nX_all['sub_grade_bal_med_diff']=X_all['tot_cur_bal']-X_all['sub_grade_bal_med']\nX_all['sub_grade_rev_bal_med']=X_all.groupby('sub_grade')['revol_bal'].transform('median')\nX_all['sub_grade_rev_bal_med_diff']=X_all['revol_bal']-X_all['sub_grade_rev_bal_med']\n\nX_all['loan_per_inc']=X_all['loan_amnt']\/(X_all['annual_inc'] + 100)\n\nX_all['borrow_times']=round(X_all['revol_bal']\/X_all['revol_util'],0).apply(np.log1p)\n\n\n# #\u67a0\u5185\u3067\u501f\u308a\u305f\u304a\u91d1\nX_all['money']=round(X_all['revol_bal']\/X_all['total_acc'],0).apply(np.log1p)\n\nX_all['earliest_cr_line'] = pd.to_numeric(X_all['earliest_cr_line'], errors='coerce')\nX_all['earliest_cr_line'] = X_all.dropna(subset=['earliest_cr_line'])\nX_all['earliest_cr_line'] =pd.to_datetime(X_all['earliest_cr_line'])\nX_all['earliest_cr_line_year'] = X_all['earliest_cr_line'].dt.year\nX_all['earliest_cr_line_month'] = X_all['earliest_cr_line'].dt.month.astype(str)\nX_all['earliest_cr_line']=X_all['earliest_cr_line'].astype(int)\n# X_all['earliest_cr_line_year']=X_all[X_all['earliest_cr_line_year']]\n# X_all['relative_earliest_cr_line']=X_all['issue_d']-X_all['earliest_cr_line']\n\nX_all['purpose_freq']=X_all.groupby('purpose')['purpose'].transform('count')\nX_all['purpose_inc_med']=X_all.groupby('purpose')['annual_inc'].transform('median')\nX_all['purpose_inc_med_diff']=X_all['annual_inc']-X_all['purpose_inc_med']\nX_all['purpose_loan_med']=X_all.groupby('purpose')['loan_amnt'].transform('median')\nX_all['purpose_loan_med_diff'] =X_all['loan_amnt']-X_all['purpose_loan_med']\n# X_all['purpose_loan_span_med']=X_all.groupby('purpose')['loan_amnt'].transform('median')\n# X_all['purpose_loan_span_men_diff']=X_all['loan_span']-X_all['purpose_loan_span_med']\n\n\nX_all['addr_state_freq'] = X_all.groupby('addr_state')['addr_state'].transform('count')\nX_all['addr_state_inc_med']=X_all.groupby('addr_state')['annual_inc'].transform('median')\nX_all['addr_state_inc_med_diff']=X_all['annual_inc']-X_all['addr_state_inc_med']\n\nX_all['home_ownership_freq']=X_all.groupby('home_ownership')['home_ownership'].transform('count')\n\nX_all['home_ownership_inc_med']=X_all.groupby('home_ownership')['annual_inc'].transform('median')\n\nX_all['home_ownership_med_inc']=X_all['annual_inc']-X_all['home_ownership_inc_med']\n\n# X_all['grade_pur']=X_all[['grade','purpose']].apply(lambda x:'{}_{}'.format(x[0],x[1]),axis=1)\n# X_all['grade_home']=X_all[['grade','home_ownership']].apply(lambda x:'{}_{}'.format(x[0],x[1]),axis=1)\nX_all['installment']=X_all.installment.apply(np.log1p)\nX_all['dti']=X_all.dti.apply(np.log1p)\nX_all['revol_bal'] = X_all.revol_bal.apply(np.log1p)\nX_all['delinq_2yrs']=X_all.delinq_2yrs.apply(np.log1p)\nX_all['revol_util']=X_all.revol_util.apply(np.log1p)\nX_all['total_acc']=X_all.total_acc.apply(np.log1p)\nX_all['collections_12_mths_ex_med'] = X_all.collections_12_mths_ex_med.apply(np.log1p)\nX_all['acc_now_delinq']=X_all.acc_now_delinq.apply(np.log1p)\nX_all['tot_coll_amt'] = X_all.tot_coll_amt.apply(np.log1p)\nX_all['tot_cur_bal'] = X_all.tot_cur_bal.apply(np.log1p)\n\n\n\n\n\nX_all.replace([np.inf, -np.inf],np.nan, inplace=True)\n","6811f09f":"X_all.info()","baa422ad":"nums=[]\nfor col in X_all.columns:\n    if X_all[col].dtype !='object':\n        nums.append(col)\n        print(col,X_all[col].nunique())","86c54136":"X_all[nums]=X_all[nums].replace([np.inf, -np.inf],1)\nX_all[nums]=X_all[nums].fillna(X_all[nums].median())\n# X_all[nums]=X_all.fillna(X_all[nums].median(), inplace=True)","9eaeacc1":"##\u6a19\u6e96\u5316\u3082\u3057\u304f\u306fminmaxscaler\n# scaler = StandardScaler()\n# # scaler = MinMaxScaler()\n# scaler.fit_transform(X_all[nums])","2f71e26a":"#\u30ac\u30a6\u30b7\u30a2\u30f3\u5909\u63db\nfrom sklearn.preprocessing import quantile_transform\nX_all[nums]=quantile_transform(X_all[nums],n_quantiles=200,random_state=0,output_distribution='normal')","ec181533":"grade_mapping = {'A':1,'B':2,'C':3,'D':4,'E':5,'F':6,'G':6}\nsub_grade_mapping = {'A1':1,'A2':2,'A3':3,'A4':4,'A5':5,\n                    'B1':6,'B2':7,'B3':8,'B4':9,'B5':10,\n                    'C1':11,'C2':12,'C3':13,'C4':14,'C5':15,\n                    'D1':16,'D2':17,'D3':18,'D4':19,'D5':20,\n                    'E1':21,'E2':22,'E3':23,'E4':24,'E5':25,\n                    'F1':26,'F2':27,'F3':28,'F4':29,'F5':30,\n                    'G1':31,'G2':32,'G3':33,'G4':34,'G5':35,}\nX_all['grade']=X_all['grade'].map(grade_mapping)\nX_all['sub_grade']=X_all['sub_grade'].map(sub_grade_mapping)","43961b58":"# # #\u5bb6\u3092\u6240\u6709\u3057\u3066\u3044\u308b\u304b\u3069\u3046\u304b\nhome_mapping={'OWN':1,'MORTAGE':2,'RENT':3,'OTHER':4,'NONE':5,'ANY':-9999}\nX_all['home_ownership_1']=X_all['home_ownership'].map(home_mapping)","c4fdea73":"# # #emp_length\nemp_map={'< 1 year':0,'1 year':1,'2 year':2,'3 year':3,'4 year':4,'5 year':5,'6 year':6,'7 year':7,'8 year':8,'9 year':9,'10+ year':10}\nX_all['emp_length']=X_all['emp_length'].map(emp_map)\nX_all['emp_length'].fillna(-1,inplace=True)\n\n\n\n# X_all['emp_length'].fillna('-1',inplace=True)\n# X_all['emp_length']=X_all['emp_length'].str.strip()\n# X_all['emp_length']=X_all['emp_length'].str.replace('[a-zA-Z]','')\n# X_all['emp_length']=X_all['emp_length'].str.replace('10\\+','15')\n# X_all['emp_length']=X_all['emp_length'].str.replace('< 1','0').astype(int)","ff67524a":"X_all['zip_code']=X_all['zip_code'].str[:3]","943fd949":"X_all['title']=X_all['title'].str.lower()\nstitle = set(X_test['title'].str.lower()) ^ set(X_train['title'].str.lower())\nX_all.loc[X_all['title'].isin(stitle),'title']='#train only#'\nX_all['title'].fillna('#null#',inplace=True)","a2fd6f72":"# X_all['loan_level']=X_all['loan_amnt'].map(lambda x : 1 if x < 7500 else 2 if x >= 7500 and x <18000\n#                                                   else 3 if  x > 18000 and x <27000 else 4)\n# X_all['annual_inc_level']=X_all['annual_inc'].map(lambda x :1 if x < 30000\n#                                                  else 2 if x >=30000 and x < 45000\n#                                                  else 3 if x >=45000 and x < 70000\n#                                                  else 4 if x >=70000 and x < 125000\n#                                                  else 5 if x >=125000\n#                                                  else -9999)\n# X_all['installment_level']=X_all['installment'].map(lambda x :1  if x < 250\n#                                                  else 2 if x >=250 and x < 350\n#                                                  else 3 if x >=350 and x < 550\n#                                                  else 4 )\n\n# X_all['unemp_flg']=X_all['emp_title'].notnull().replace({True:0,False:1})\n# X_all['unemp_flg']=X_all['unemp_flg'].astype('category')","4e59c933":"txt_all=X_all['emp_title'].str.lower()\nX_all.drop(['emp_title','earliest_cr_line_month','purpose_inc_med','purpose_loan_med','addr_state_inc_med','home_ownership_inc_med','sub_grade_inc_med','sub_grade_bal_med','sub_grade_rev_bal_med'],axis=1,inplace=True)","4c4815f1":"# # dtype\u304cobject\uff08\u6570\u5024\u3067\u306a\u3044\u3082\u306e\uff09\u306e\u30ab\u30e9\u30e0\u540d\u3068\u30e6\u30cb\u30fc\u30af\u6570\u3092\u78ba\u8a8d\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\n# cats = []\n# for col in X_all.columns:\n#     if X_all[col].dtype == 'object':\n#         cats.append(col)\n        \n#         print(col, X_all[col].nunique())","de7b0ff6":"# X_all.info()","a6df6659":"cat1 = []\nnum1 = []\n\nfor col in X_all.columns:\n    if X_all[col].dtype == 'object':\n#         if col != 'emp_title':\n            cat1.append(col)\n    else:\n#         if col != 'issue_d':\n            num1.append(col)\nX_all_1=X_all            \nX_train_1 = X_all.iloc[:X_train.shape[0],:]\nX_test_1=X_all.iloc[X_train.shape[0]:,:]\nX_train_1=X_train_1.replace(-1,0)\nX_test_1= X_test_1.replace(-1,0)","f32301ac":"###\u30ab\u30a6\u30f3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\nX_all['grade_cun']=X_all['grade'].map(X_all['grade'].value_counts())\nX_all['sub_grade_cun']=X_all['sub_grade'].map(X_all['sub_grade'].value_counts())\nX_all['emp_lnegth']=X_all['emp_length'].map(X_all['emp_length'].value_counts())\nX_all['zip_code_cun']=X_all['zip_code'].map(X_all['zip_code'].value_counts())\nX_all['title_cun']=X_all['title'].map(X_all['title'].value_counts())\nX_all['home_ownership_cun']=X_all['home_ownership'].map(X_all['home_ownership'].value_counts())\nX_all['purpose_cun']=X_all['purpose'].map(X_all['purpose'].value_counts())\nX_all['addr_state_cun']=X_all['addr_state'].map(X_all['addr_state'].value_counts())\nX_all['initial_list_status_cun']=X_all['initial_list_status'].map(X_all['initial_list_status'].value_counts())\nX_all['application_type_cnt']=X_all['application_type'].map(X_all['application_type'].value_counts())","9c8f19f6":"# X_all=X_all.drop(['grade','initial_list_status','application_type','acc_now_delinq','installment','title'],axis=1)\n# X_all=X_all.drop(['zip_code','application_type_cnt','initial_list_status_cun','purpose_cun','purpose_cun','home_ownership_cun','sub_grade_cun'],axis=1)\n\n# ,'earliest_cr_line_month'","9a1eb183":"# dtype\u304cobject\uff08\u6570\u5024\u3067\u306a\u3044\u3082\u306e\uff09\u306e\u30ab\u30e9\u30e0\u540d\u3068\u30e6\u30cb\u30fc\u30af\u6570\u3092\u78ba\u8a8d\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\ncats = []\nfor col in X_all.columns:\n    if X_all[col].dtype == 'object':\n        cats.append(col)\n        \n        print(col, X_all[col].nunique())","056e51f2":"#ordinal encoding\nencoder=OrdinalEncoder(cols=cats)\nX_all[cats]=encoder.fit_transform(X_all[cats])","c8094138":"# X_all.fillna(-9999,inplace=True)d\nX_train = X_all.iloc[:X_train.shape[0],:]\nX_test=X_all.iloc[X_train.shape[0]:,:]","cf2d44e4":"X_test","5161a96e":"# from sklearn.model_selection import KFold\n\n# #\u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306efold\u3054\u3068\u306btarget encoding\u3092\u3084\u308a\u76f4\u3059\n# kf = KFold(n_splits=5,shuffle=True,random_state=71)\n# for i, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n    \n#     #\u5b66\u7fd2\u30c7\u30fc\u30bf\u304b\u3089\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u3092\u5206\u3051\u308b\n#     train_x,val_x=X_train.iloc[train_idx].copy(),X_train.iloc[val_idx]\n#     train_y,val_y=y_train.iloc[train_idx],y_train.iloc[val_idx]\n    \n#     #\u5909\u6570\u3092\u30eb\u30fc\u30d7\u3057\u3066target encoding\n#     for col in cats:\n#         #\u5b66\u7fd2\u30c7\u30fc\u30bf\u5168\u4f53\u3067\u5404\u30ab\u30c6\u30b4\u30ea\u30fc\u306b\u304a\u3051\u308btarget\u306e\u5e73\u5747\u3092\u8a08\u7b97\n#         data_tmp = pd.DataFrame({col:train_x[col],'target':train_y})\n#         target_mean = data_tmp.groupby(col)['target'].mean()\n#         #\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u306e\u30ab\u30c6\u30b4\u30ea\u3092\u7f6e\u63db\n#         X_test.loc[:,col]=X_test[col].map(target_mean)\n        \n#         #\u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u5909\u63db\u5f8c\u306e\u5024\u3092\u683c\u7d0d\u3059\u308b\u914d\u5217\u3092\u6e96\u5099\n#         tmp=np.repeat(np.nan,train_x.shape[0])\n#         kf_encoding = KFold(n_splits=4,shuffle=True,random_state=72)\n#         for idx_1,idx_2 in kf_encoding.split(train_x):\n#             target_mean = data_tmp.iloc[idx_1].groupby(col)['target'].mean()\n#             tmp[idx_2] = train_x[col].iloc[idx_2].map(target_mean)\n        \n#         X_train.loc[:,col]=tmp","cab34c43":"X_all = pd.concat([X_train,X_test],axis=0)","c17d2058":"#\u6587\u5b57\u5217\u51e6\u7406\ntfidf = TfidfVectorizer(strip_accents='ascii',stop_words='english', max_features=150,ngram_range=(1,2))\ntxt_all = tfidf.fit_transform(txt_all.fillna('#'))\nX_all = pd.concat([X_all,pd.DataFrame(txt_all.todense(),index=X_all.index)],axis=1)","e7e6bb32":"# from sklearn.decomposition import PCA\n# pca = PCA(n_components = 0.9,whiten = False)\n# # pca.fit(tfidf)\n# # pca.n_components_\n# x = pca.transform(tfidf.toarray())\n","6bd4642b":"# # target encoding\n# # \u4eca\u5ea6\u306fTarget Encoding\n# target = 'loan_condition'\n# X_temp = pd.concat([X_train, y_train], axis=1)\n\n# for col in cats:\n\n# #     # X_test\u306fX_train\u3067\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3059\u308b\n#     summary = X_temp.groupby([col])[target].mean()\n#     X_test[col] = X_test[col].map(summary) \n\n\n#     # X_train\u306e\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u3092oof\u3067\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3059\u308b\n#     skf = StratifiedKFold(n_splits=6, random_state=71, shuffle=True)\n#     enc_train = Series(np.zeros(len(X_train)), index=X_train.index)\n\n#     for i, (train_ix, val_ix) in enumerate((skf.split(X_train, y_train))):\n#         X_train_, _ = X_temp.iloc[train_ix], y_train.iloc[train_ix]\n#         X_val, _ = X_temp.iloc[val_ix], y_train.iloc[val_ix]\n\n#         summary = X_train_.groupby([col])[target].mean()\n#         enc_train.iloc[val_ix] = X_val[col].map(summary)\n        \n#     X_train[col]  = enc_train\n","0075064c":"# from sklearn.model_selection import KFold\n\n# #\u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306efold\u3054\u3068\u306btarget encoding\u3092\u3084\u308a\u76f4\u3059\n# kf = KFold(n_splits=5,shuffle=True,random_state=71)\n# for i, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n    \n#     #\u5b66\u7fd2\u30c7\u30fc\u30bf\u304b\u3089\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u3092\u5206\u3051\u308b\n#     train_x,val_x=X_train.iloc[train_idx].copy(),X_train.iloc[val_idx]\n#     train_y,val_y=y_train.iloc[train_idx],y_train.iloc[val_idx]\n    \n#     #\u5909\u6570\u3092\u30eb\u30fc\u30d7\u3057\u3066target encoding\n#     for col in cats:\n#         #\u5b66\u7fd2\u30c7\u30fc\u30bf\u5168\u4f53\u3067\u5404\u30ab\u30c6\u30b4\u30ea\u30fc\u306b\u304a\u3051\u308btarget\u306e\u5e73\u5747\u3092\u8a08\u7b97\n#         data_tmp = pd.DataFrame({col:train_x[col],'target':train_y})\n#         target_mean = data_tmp.groupby(col)['target'].mean()\n#         #\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u306e\u30ab\u30c6\u30b4\u30ea\u3092\u7f6e\u63db\n#         X_test.loc[:,col]=X_test[col].map(target_mean)\n        \n#         #\u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u5909\u63db\u5f8c\u306e\u5024\u3092\u683c\u7d0d\u3059\u308b\u914d\u5217\u3092\u6e96\u5099\n#         tmp=np.repeat(np.nan,train_x.shape[0])\n#         kf_encoding = KFold(n_splits=4,shuffle=True,random_state=72)\n#         for idx_1,idx_2 in kf_encoding.split(train_x):\n#             target_mean = data_tmp.iloc[idx_1].groupby(col)['target'].mean()\n#             tmp[idx_2] = train_x[col].iloc[idx_2].map(target_mean)\n        \n#         X_train.loc[:,col]=tmp","88b5e088":"# XXX.to_csv('X_train.csv')","9dff5f42":"\n####\u3072\u3068\u3064\u3081\u306e\u30e2\u30c7\u30ea\u30f3\u30b0\n# # \u4eca\u5ea6\u306f\u30ab\u30c6\u30b4\u30ea\u7279\u5fb4\u91cf\u3082\u8ffd\u52a0\u3057\u3066\u30e2\u30c7\u30ea\u30f3\u30b0\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\n# # CV\u3057\u3066\u30b9\u30b3\u30a2\u3092\u898b\u3066\u307f\u308b\u3002\u5c64\u5316\u62bd\u51fa\u3067\u826f\u3044\u304b\u306f\u5225\u9014\u3088\u304f\u8003\u3048\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002\n# scores = []\n\n# skf = StratifiedKFold(n_splits=5, random_state=71, shuffle=True)\n\n# for i, (train_ix, test_ix) in tqdm(enumerate(skf.split(X_train, y_train))):\n#     X_train_, y_train_ = X_train.values[train_ix], y_train.values[train_ix]\n#     X_val, y_val = X_train.values[test_ix], y_train.values[test_ix]\n    \n    \n#     clf = GradientBoostingClassifier()\n    \n#     clf.fit(X_train_, y_train_)\n#     y_pred = clf.predict_proba(X_val)[:,1]\n#     score = roc_auc_score(y_val, y_pred)\n#     scores.append(score)\n    \n#     print('CV Score of Fold_%d is %f' % (i, score))","a56a0ac0":"# print('Feature Importances:')\n# for i, feat in enumerate(X_train):\n#     print('\\t{0:20s} : {1:>.6f}'.format(feat, imp[i]))\n# # # imp","43d18612":"# # # \u5e73\u5747\u30b9\u30b3\u30a2\u3092\u7b97\u51fa\n# np.array(scores).mean()","774aece3":"# def stratified_group_k_fold(X, y, groups, k, seed=None):\n#     labels_num = np.max(y) + 1\n#     y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n#     y_distr = Counter()\n#     for label, g in zip(y, groups):\n#         y_counts_per_group[g][label] += 1\n#         y_distr[label] += 1\n\n#     y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n#     groups_per_fold = defaultdict(set)\n\n#     def eval_y_counts_per_fold(y_counts, fold):\n#         y_counts_per_fold[fold] += y_counts\n#         std_per_label = []\n#         for label in range(labels_num):\n#             label_std = np.std([y_counts_per_fold[i][label] \/ y_distr[label] for i in range(k)])\n#             std_per_label.append(label_std)\n#         y_counts_per_fold[fold] -= y_counts\n#         return np.mean(std_per_label)\n    \n#     groups_and_y_counts = list(y_counts_per_group.items())\n#     random.Random(seed).shuffle(groups_and_y_counts)\n\n#     for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])):\n#         best_fold = None\n#         min_eval = None\n#         for i in range(k):\n#             fold_eval = eval_y_counts_per_fold(y_counts, i)\n#             if min_eval is None or fold_eval < min_eval:\n#                 min_eval = fold_eval\n#                 best_fold = i\n#         y_counts_per_fold[best_fold] += y_counts\n#         groups_per_fold[best_fold].add(g)\n\n#     all_groups = set(groups)\n#     for i in range(k):\n#         train_groups = all_groups - groups_per_fold[i]\n#         test_groups = groups_per_fold[i]\n\n#         train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n#         test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n\n#         yield train_indices, test_indices","4fc79525":"from lightgbm import LGBMClassifier\n\nscores =[]\n\ntotal_score = 0\ny_tests = np.zeros(len(X_test.index))\n\nskf=StratifiedKFold(n_splits=5,random_state=71,shuffle=True)\n# epochs = [1000,899,782,1222,1111]\n\nclf = LGBMClassifier(boosting_type='gbdt',class_weight=None,colsample_bytree=0.71,importance_type='split',learning_rate=0.04,max_depth=6,min_child_samples=20,min_child_weight=0.001,min_split_gain=0,n_estimators=9999,n_jobs=-1,num_leaves=31,objective=None,random_state=71,reg_alpha=1.0,reg_lambda=1.0,silent=True,subsample=0.9,subsample_for_bin=200000,subsample_freq=0)\n# clf = LGBMClassifier()\n\nfor i, (train_ix,test_ix) in enumerate(tqdm(skf.split(X_train,y_train))):\n    X_train_,y_train_=X_train.values[train_ix],y_train.values[train_ix]\n    X_val,y_val = X_train.values[test_ix],y_train.values[test_ix]\n    \n#     print('Magic num:%d' % epochs[i])\n#     clf = LGBMClassifier()\n    clf.fit(X_train_,y_train_,early_stopping_rounds=200,eval_metric='auc',eval_set=[(X_val,y_val)])\n    y_pred = clf.predict_proba(X_val)[:,1]\n    score = roc_auc_score(y_val,y_pred)\n    scores.append(score)\n    y_tests += clf.predict_proba(X_test)[:,1]\n    \nfor i in range(0,len(scores)):\n    print('CV Score of Fold_%d is %f' % (i,scores[i]))\n    total_score += scores[i]\nprint('Avg CV Score: %f' %(total_score\/len(scores)))\n\n\ny_test = y_tests\/len(scores)\n\n","9c678ab0":"# sample submission\u3092\u8aad\u307f\u8fbc\u3093\u3067\u3001\u4e88\u6e2c\u5024\u3092\u4ee3\u5165\u306e\u5f8c\u3001\u4fdd\u5b58\u3059\u308b\nsubmission = pd.read_csv('..\/input\/datarobot\/sample_submission.csv', index_col=0)\n\nsubmission.loan_condition = y_test\nsubmission.to_csv('submission_26.1.csv')","3100d959":"# import xgboost as xgb\n\n\n# scores =[]\n\n# total_score = 0\n# y_tests = np.zeros(len(X_test.index))\n\n# skf=StratifiedKFold(n_splits=4,random_state=71,shuffle=True)\n# # epochs = [1000,899,782,1222,1111]\n\n# clf1 = xgb.XGBClassifier(class_weight=None,colsample_bytree=0.71,importance_type='split',learning_rate=0.004,max_depth=5,min_child_weight=0.001,n_estimators=9999,n_jobs=-1,objective=None,random_state=71,reg_alpha=1.0,reg_lambda=1.0,subsample=0.9,early_stopping_round\n                         \n#                          s=1000)\n# # clf = LGBMClassifier()\n\n# for i, (train_ix,test_ix) in enumerate(tqdm(skf.split(X_train,y_train))):\n#     X_train_,y_train_=X_train.values[train_ix],y_train.values[train_ix]\n#     X_val,y_val = X_train.values[test_ix],y_train.values[test_ix]\n    \n# #     print('Magic num:%d' % epochs[i])\n# #     clf = LGBMClassifier()\n#     clf1.fit(X_train_,y_train_,early_stopping_rounds=150,eval_metric='auc',eval_set=[(X_val,y_val)])\n#     y_pred = clf1.predict_proba(X_val)[:,1]\n#     score = roc_auc_score(y_val,y_pred)\n#     scores.append(score)\n#     y_tests += clf1.predict_proba(X_test)[:,1]\n    \n# for i in range(0,len(scores)):\n#     print('CV Score of Fold_%d is %f' % (i,scores[i]))\n#     total_score += scores[i]\n# print('Avg CV Score: %f' %(total_score\/len(scores)))\n\n\n# y_test_xg = y_tests\/len(scores)\n\n","61672f23":"# y_pred = y_test_xg + y_test\n\n# # sample submission\u3092\u8aad\u307f\u8fbc\u3093\u3067\u3001\u4e88\u6e2c\u5024\u3092\u4ee3\u5165\u306e\u5f8c\u3001\u4fdd\u5b58\u3059\u308b\n# submission = pd.read_csv('..\/input\/datarobot\/sample_submission.csv', index_col=0)\n\n# submission.loan_condition = y_pred\n# submission.to_csv('submission_25.2.csv')","6af498c5":"# import catboost as cb\n\n","faa7be3d":"# from tensorflow.keras.layers import Dense ,Dropout, BatchNormalization, Input, Embedding, SpatialDropout1D, Reshape, Concatenate\n# from tensorflow.keras.optimizers import Adam\n# from tensorflow.keras.models import Model\n# from tensorflow.keras.callbacks import EarlyStopping\n# from tensorflow.keras.metrics import AUC","65247f26":"# df_train = pd.read_csv('..\/input\/homework-for-students4plus\/train.csv', index_col=0, parse_dates=['issue_d'], skiprows=lambda x: x%20!=0)\n# #df_train = pd.read_csv('..\/input\/train.csv', index_col=0)\n# y_train = df_train.loan_condition\n# X_train = df_train.drop(['loan_condition'], axis=1)\n\n# X_test = pd.read_csv('..\/input\/homework-for-students4plus\/test.csv', index_col=0, parse_dates=['issue_d'])","fdd81053":"# X_all[cat1]","a4e98931":"# # # train\/test\n# # # \u7279\u5fb4\u91cf\u30bf\u30a4\u30d7\u3054\u3068\u306b\u5206\u5272\u3059\u308b\n# cat_train = X_train_1[cat1]\n# # txt_train = X_train.emp_title\n# num_train = X_train_1[num1]\n\n# cat_test = X_test_1[cat1]\n# # txt_test = X_test.emp_title\n# num_test = X_test_1[num1]\n\n# # cat_all = X_all[cat]\n# # # txt_all = X_all.emp_title\n# # num_all = X_all[num]\n","28bd3d70":"# from sklearn.preprocessing import StandardScaler\n\n# scaler = StandardScaler()\n# num_train = scaler.fit_transform(num_train.fillna(num_train.median()))\n# num_test = scaler.transform(num_test.fillna(num_test.median()))\n\n# # num_train = num_train.fillna(num_train.median())\n# # num_test = num_test.fillna(num_test.median())","0c1c8cf9":"# for col in tqdm(cat1):\n#     oe = OrdinalEncoder(return_df=False)\n    \n#     cat_train[col] = oe.fit_transform(cat_train[[col]])\n#     cat_test[col] = oe.transform(cat_test[[col]]) ","94ed5867":"# X_train_1","9eadb91b":"# # # \u30d0\u30e9\u3057\u3066\u30ea\u30b9\u30c8\u306b\u3059\u308b\n# cat_train = [cat_train.values[:, k].reshape(-1,1) for k in range(len(cat1))]\n# cat_test = [cat_test.values[:, k].reshape(-1,1) for k in range(len(cat1))]\n\n# # # cat_all = [cat_all.values[:, k].reshape(-1,1) for k in range(len(cat))]","8e10c9be":"# len(cat1)","f3ffb0e9":"# # # numeric\u3068categorical\u3092\u7d50\u5408\u3057\u3066\u4e00\u3064\u306e\u30ea\u30b9\u30c8\u306b\u3059\u308b\n# X_train_1 = [num_train] + cat_train\n# X_test_1 = [num_test] + cat_test\n\n# # X_all  = [num_all]+cat_all","83094dc5":"# len(X_test_1)\n# len(X_train_1)\n# X_train_1","7dfdaf91":"# # \u30b7\u30f3\u30d7\u30eb\u306aNN\n# def create_model():\n#     num= Input(shape=(num_train.shape[1],))\n#     out_num = Dense(194, activation='relu')(num)\n#     out_num = BatchNormalization()(out_num)\n#     out_num = Dropout(0.5)(out_num)\n    \n#     inputs = [num]\n#     outputs = [out_num]\n    \n#     # \u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\u306fembedding\u3059\u308b\n#     for c in cat1:\n#         num_unique_values = int(X_all_1[c].nunique())\n# #         \n#         emb_dim = int(min(np.ceil((num_unique_values)\/2), 50))\n#         inp = Input(shape=(1,))\n#         inputs.append(inp)\n#         out = Embedding(num_unique_values+2, emb_dim, name=c)(inp)\n#         out = SpatialDropout1D(0.3)(out)\n#         out = Reshape((emb_dim, ))(out)\n#         outputs.append(out)\n    \n#     x = Concatenate()(outputs)\n#     x = BatchNormalization()(x)\n#     x = Dropout(0.5)(x)\n#     x = Dense(64, activation='relu')(x)\n#     x = BatchNormalization()(x)\n#     x = Dropout(0.5)(x)\n#     x = Dense(64, activation='relu')(x)\n#     x = BatchNormalization()(x)\n#     x = Dropout(0.5)(x)\n#     outp = Dense(1, activation='sigmoid')(x)\n#     model = Model(inputs=inputs, outputs=outp)\n#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[AUC()])\n    \n#     return model","01f06ad6":"# model = create_model()\n\n# es = EarlyStopping(monitor='val_loss', patience=0)\n\n# model.fit(X_train_1, y_train, batch_size=32, epochs=999, validation_split=0.2, callbacks=[es])","4464d502":"# X_train_1\n# ","7378942a":"# y_NN = model.predict(X_test_1)","38b1e172":"# from keras.layers import Dense,Dropout\n# from keras.models import Sequential \n# from sklearn.metrics import log_loss\n# from sklearn.preprocessing import StandardScaler\n\n# #\u30c7\u30fc\u30bf\u306e\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\n# scaler = StandardScaler()\n# tr_x = scaler.fit_transform(X_train_1)\n# va_x = scaler.transform(va_x)\n# test_x = scaler.transform(test_x)\n\n# #\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30e2\u30c7\u30eb\u306e\u69cb\u7bc9\n# model = Sequential()\n# model.add(Dense(256,activation='relu',input_shape=(train_x.shape[1],)))\n# model.add(Dropout(0.2))\n# model.add(Dense(256,activation='relu'))\n# model.add(Dropout(0.2))\n# model.add(Dense(1,activation='sigmoid'))\n\n# model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n\n# #\u5b66\u7fd2\u306e\u9032\u884c\n# #\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u3082\u30e2\u30c7\u30eb\u306b\u6e21\u3057\u3001\u5b66\u7fd2\u306e\u9032\u884c\u3068\u3068\u3082\u306b\u30b9\u30b3\u30a2\u304c\u3069\u3046\u5909\u308f\u308b\u304b\u30e2\u30cb\u30bf\u30ea\u30f3\u30b0\u3059\u308b\n# batch_size = 128\n# epochs = 10\n# history = model.fit(tr_x,tr_y,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(va_x,va_y))\n\n# #\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u3067\u306e\u30b9\u30b3\u30a2\u78ba\u8a8d\n# va_pred = model.predict(va_x)\n# score = log_loss(va_y,va_pred,eps=1e-7)\n# print(f'logloss: {score:.4f}')\n\n\n# pred = model.predict(test_x)","8cac3811":"# NN\u306e\u4e0b\u6e96\u5099","426cbbc4":"# XGboost","80a2523c":"# staratified_group_k_fold","35646b24":"# \u3053\u3053\u307e\u3067","4608942b":"# > ***NN***","e529e4e0":"NN\u304c\u901a\u308b\u304b\u78ba\u8a8d\n","b171f00c":"# catboost"}}