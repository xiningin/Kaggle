{"cell_type":{"7d261999":"code","8fa262b2":"code","90654d99":"code","f9afff57":"code","ca607f5f":"code","f612188c":"code","71d165d7":"code","85b715d9":"code","bf26d0e2":"code","1068bdee":"code","71934ab2":"code","8b2c4aae":"code","e8bb5b3c":"code","3ddfc343":"code","ae12db65":"code","c85acf55":"code","bedecad3":"code","fb2be825":"markdown","5097f270":"markdown","2f4c1234":"markdown","f591349a":"markdown","0bb12068":"markdown","5a2a9840":"markdown","ef1b9a24":"markdown","11dca16b":"markdown","e377a477":"markdown","82d6b4a6":"markdown","9c8a5527":"markdown","e998ea4f":"markdown","f2467f7f":"markdown","2915819b":"markdown","2b3a871d":"markdown","f0a7d4bc":"markdown","b1982d0d":"markdown","5dfa5f0f":"markdown","9ac1758a":"markdown","f206be24":"markdown"},"source":{"7d261999":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the data\ntrain_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-3\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-3\/eval.csv\")","8fa262b2":"train_data.head()","90654d99":"print(train_data.columns)\nprint(\"-------------------------\")\nprint(test_data.columns)","f9afff57":"train_data.drop(columns=['pubchem_id', 'id', 'Eat']).hist(layout=(107, 12), figsize=(35, 500), bins=30)","ca607f5f":"# Training set\nprint(\"Training Set\\n=========================================\")\nNARows = train_data.isna().sum().sum()\nprint(\"Total number of rows (across all columns) that have NaN value = \" + str(NARows))\n\nNAAllRows = train_data.isna().sum()\nprint(\"Num of rows in columns that have NaN value:\")\nprint(NAAllRows)\n\n# Testing set\nprint(\"\\n\\nTest Set\\n=========================================\")\nNARows = test_data.isna().sum().sum()\nprint(\"Total number of rows (across all columns) that have NaN value = \" + str(NARows))\n\nNAAllRows = test_data.isna().sum()\nprint(\"Num of rows in columns that have NaN value:\")\nprint(NAAllRows)","f612188c":"train_data.drop(columns=['pubchem_id'], inplace=True)","71d165d7":"import tensorflow as tf\nfrom keras import backend\nfrom keras.layers import Dense, Dropout, Normalization, BatchNormalization\nfrom keras.models import Sequential, load_model\nfrom keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\n\n# Subset predictors and target columns\npredictors = train_data[train_data.columns.difference(['id', 'Eat'])]\ntarget = train_data.Eat\n\n# Since there is no root mean squared error loss function in Keras, make one\ndef root_mean_squared_error(y_true, y_pred):\n    return backend.sqrt(backend.mean(backend.square(y_pred - y_true)))\n\n# Create early stopping monitor\nearly_stopping_monitor = EarlyStopping(patience=7, monitor='val_root_mean_squared_error', mode='min')\n\n# Normalize the input data\n# note: normalizing tanked my models\nnormalizer = Normalization()\nnormalizer.adapt(predictors.values)","85b715d9":"'''\n- 9 hidden layers\n- Mostly 600 neurons\n- relu activation\n- adam optimizer\n'''\n\n# Step 1: specify architecture\nmodel1 = Sequential()\nmodel1.add(Dense(1275, activation='relu', input_shape=(1275,)))\nmodel1.add(Dense(900, activation='relu'))\nmodel1.add(Dense(600, activation='relu'))\nmodel1.add(Dense(600, activation='relu'))\nmodel1.add(Dense(600, activation='relu'))\nmodel1.add(Dense(600, activation='relu'))\nmodel1.add(Dense(600, activation='relu'))\nmodel1.add(Dense(600, activation='relu'))\nmodel1.add(Dense(600, activation='relu'))\nmodel1.add(Dense(600, activation='relu'))\nmodel1.add(Dense(1))\n\n# Step 2: compile\nadam = Adam(learning_rate=0.001)  # Default: 0.001\nmodel1.compile(optimizer=adam, loss=root_mean_squared_error, metrics=[tf.keras.metrics.RootMeanSquaredError()])\n\n# Step 3: fit\nhistory = model1.fit(predictors, target, epochs=250, validation_split=0.2, callbacks=[])\n\n# Step 4: analyze\nmodel1_history_table = pd.DataFrame(history.history)\nmodel1_history_table.plot(figsize=(8,5))\nplt.gca().set_ylim(0, 0.6)\nplt.show()","bf26d0e2":"'''\n50 epochs, default learning rate: 0.1989 vs 0.2526\n50 epochs, 0.01 learning rate: 0.1944 vs 0.2042\n-----\nI changed the layer config.\n50 epochs, default learning rate: 0.1777 vs 0.1977\n50 epochs, default learning rate, w\/ normalization: 0.2907 vs 0.2425\n50 epochs, default learning rate, w\/ batch normalization: 0.7369 vs 0.3066\n100 epochs, default learning rate: 0.1438 vs 0.1769\n-----\nI added a hidden layer.\n100 epochs, default learning rate: 0.1266 vs 0.1693\n-----\nI added a hidden layer.\n100 epochs, default learning rate: 0.1286 vs 0.1535\n150 epochs, default learning rate: 0.1143 vs 0.1569\n-----\nI added a hidden layer.\n150 epochs, default learning rate: 0.1189 vs 0.1485\n200 epochs, default learning rate: bad\n80 epochs, default learning rate: 0.1390 vs 0.1559\n100 epochs: 0.1328 vs 0.1446\n-----\nI added a hidden layer.\n100 epochs: 0.1279 vs 0.1953\n93 epochs: 0.1442 vs 0.1582\n250 epochs: 0.0960 vs 0.1241\n'''","1068bdee":"sns.displot(model1_history_table, x='val_root_mean_squared_error')\nplt.show()\nprint(model1_history_table.val_root_mean_squared_error.describe())","71934ab2":"'''\n- 6 hidden layers\n- All 900 neurons\n- relu activation\n- adam optimizer\n'''\n\n# Step 1: specify architecture\nmodel2 = Sequential()\n# model2.add(normalizer)\nmodel2.add(Dense(1275, activation='relu', input_shape=(1275,)))\nmodel2.add(Dense(900, activation='relu'))\nmodel2.add(Dense(900, activation='relu'))\nmodel2.add(Dense(900, activation='relu'))\nmodel2.add(Dense(900, activation='relu'))\nmodel2.add(Dense(900, activation='relu'))\nmodel2.add(Dense(900, activation='relu'))\nmodel2.add(Dense(1))\n\n# Step 2: compile\nadam = Adam(learning_rate=0.001)  # Default: 0.001\nmodel2.compile(optimizer=adam, loss=root_mean_squared_error, metrics=[tf.keras.metrics.RootMeanSquaredError()])\n\n# Step 3: fit\nhistory = model2.fit(predictors, target, epochs=250, validation_split=0.2, callbacks=[])\n\n# Step 4: analyze\nmodel2_history_table = pd.DataFrame(history.history)\nmodel2_history_table.plot(figsize=(8,5))\nplt.gca().set_ylim(0, 1)\nplt.show()","8b2c4aae":"'''\n50 epochs, default learning rate: 0.2005 vs 0.2152\n50 epochs, default learning rate w\/ normalization: 0.4489 vs 0.5353\n50 epochs, default learning rate: 0.3888 vs 0.4278\n100 epochs, default learning rate: 0.1585 vs 0.2001\n3 @ 900 each\n100 epochs: 0.1440 vs 0.1978\n4 @ 900 each\n100 epochs: 0.1410 vs 0.1935\n6 @ 900 each\n100 epochs: 0.1375 vs 0.1632\n250 epochs: 0.1059 vs 0.1584\n'''","e8bb5b3c":"sns.displot(model2_history_table, x='val_root_mean_squared_error')\nplt.show()\nprint(model2_history_table.val_root_mean_squared_error.describe())","3ddfc343":"'''\n- 9 hidden layers\n- Same number of neurons across all (besides first)\n- relu activation\n- adam optimizer\n'''\n\n# Step 1: specify architecture\nmodel3 = Sequential()\nmodel3.add(Dense(1275, activation='relu', input_shape=(1275,)))\nmodel3.add(Dense(600, activation='relu'))\nmodel3.add(Dense(200, activation='relu'))\nmodel3.add(Dense(200, activation='relu'))\nmodel3.add(Dense(200, activation='relu'))\nmodel3.add(Dense(200, activation='relu'))\nmodel3.add(Dense(200, activation='relu'))\nmodel3.add(Dense(200, activation='relu'))\nmodel3.add(Dense(200, activation='relu'))\nmodel3.add(Dense(200, activation='relu'))\nmodel3.add(Dense(1))\n\n# Step 2: compile\nadam = Adam(learning_rate=0.001)  # Default: 0.001\nmodel3.compile(optimizer=adam, loss=root_mean_squared_error, metrics=[tf.keras.metrics.RootMeanSquaredError()])\n\n# Step 3: fit\nhistory = model3.fit(predictors, target, epochs=250, validation_split=0.2, callbacks=[])\n\n# Step 4: analyze\nmodel3_history_table = pd.DataFrame(history.history)\nmodel3_history_table.plot(figsize=(8,5))\nplt.gca().set_ylim(0, 0.6)\nplt.show()","ae12db65":"'''\n50 epochs, default learning rate: 0.2163 vs 0.2602\n50 epochs, default learning rate w\/ normalization: 0.2820 vs 0.3363\n100 epochs, default learning rate: 0.1592 vs 0.2010\n100 epochs, default learning rate w\/ normalization: 0.1960 vs 0.2218\n200 epochs, default learning rate w\/ normalization: 0.1730 vs 0.2311\n\nI reduced the layer size of the layers.\n100 epochs, default learning rate: 0.1514 vs 0.1981\n\nI added a layer.\n100 epochs, default learning rate: 0.1509 vs 0.1904\n\nI reduced the layer size of the layers (100).\n100 epochs, default learning rate: 0.1482 vs 0.2088\n\nI removed a layer.\n100 epochs, default learning rate: 0.1613 vs 0.2088\n\nI increased the layer size (300).\n100 epochs, default learning rate: 0.1589 vs 0.1959\n\nI added a layer and decreased the layer size (200).\n100 epochs, default learning rate: 0.1415 vs 0.2127\n120 epochs, default learning rate: 0.1439 vs 0.2227\n\nI added a layer.\n100 epochs, default learning rate: 0.1398 vs 0.1677\n\nI added two layers.\n250 epochs: 0.0932 vs 0.1611\n'''","c85acf55":"sns.displot(model3_history_table, x='val_root_mean_squared_error')\nplt.show()\nprint(model3_history_table.val_root_mean_squared_error.describe())","bedecad3":"# Take out id column from test_data\ntesting_columns = test_data.columns.difference(['id', 'pubchem_id'])\n\nfinal_predictions = model1.predict(test_data[testing_columns])\nfinal_predictions = final_predictions.reshape(len(final_predictions),)\n\noutput = pd.DataFrame({'id': test_data.id, 'Eat': final_predictions})\nprint(output.to_string())\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","fb2be825":"# Feature Engineering","5097f270":"Cool, no missing values in the dataset.","2f4c1234":"Drop the `pubchem_id` feature as it is unrelated to the other molecular features.","f591349a":"# Exploratory Data Analysis","0bb12068":"Model 1 showed the most promising results: best metric vs. validation metric comparison, and relatively stable metric vs. validation metric graph.","5a2a9840":"### Model 2","ef1b9a24":"### Checking for Missing Values","11dca16b":"# Generating Submission File","e377a477":"Nope. Just the same except the test dataset is missing the `Eat` column, which should be obvious.\n\n*Sigh*. Let's have a look at the features, shall we?","82d6b4a6":"Validation score summary statistics:","9c8a5527":"Validation score summary statistics:","e998ea4f":"Validation score summary statistics:","f2467f7f":"Oooook, wow. A lot of the later features only have values that are really hugging 0, with some outliers. Given this low variance, we can say that the features toward the beginning are more important.","2915819b":"### Model 3","2b3a871d":"# Data Cleaning","f0a7d4bc":"That's... a lot of feature columns. Any differences in between the train and test datasets?","b1982d0d":"# Pre-req Code","5dfa5f0f":"**Initial submission**\n- No pubchemid column\n- 5 hidden layers: 900, 700, 500, 300, 100 neurons; all using 'relu';\n- Compiling\n  - 'adam' optimizer\n- Fitting\n  - 30 epochs\n  - 20% validation_split","9ac1758a":"### Model 1","f206be24":"# Model Construction"}}