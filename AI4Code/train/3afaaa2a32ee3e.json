{"cell_type":{"4111fc1e":"code","a800b78d":"code","ed8b1f8b":"code","2efd1cbb":"code","717330e4":"code","145ec445":"code","a2b9aefd":"code","b320ce5b":"code","bdc58003":"code","e8204c93":"code","1ac28e9b":"code","8a2e0a1b":"code","a3e72b38":"code","bd208a91":"code","a3a4b9a4":"code","210a3120":"code","e00b2e2c":"code","3b5be1f4":"code","2a80af4c":"code","af261ee8":"code","0317c39a":"code","3bf869ed":"code","681b92db":"code","d44af60e":"code","284a7268":"code","0840998d":"code","b2a2c5f6":"code","960d08fd":"code","9a38fc99":"code","ab2b816d":"code","07bca271":"code","b0951d7a":"code","64d39235":"code","44010bfd":"code","4046efae":"code","d4d3ecda":"code","eefcf489":"code","52949d11":"code","2b06103b":"code","ce5aaa6d":"code","7593b254":"code","64f0bb9a":"code","5e7be459":"code","64e30a19":"code","b5f334c1":"code","6fd85563":"code","dd3b3cb1":"code","b8d7ee42":"code","e3d6719a":"code","791b01a2":"code","4be8f190":"code","214d70c0":"code","1b7daf4e":"code","9ec31483":"code","a5386526":"code","047aa05b":"code","92e97ec1":"code","25c9ed78":"code","c7163f76":"code","41c779f1":"code","4e820896":"code","92305154":"code","0b0c5581":"code","f9e5547c":"code","70127c55":"code","b3b1e266":"code","b28ab0aa":"code","773a30c4":"markdown","fbf749e1":"markdown","405b9963":"markdown","300c021d":"markdown","df85b040":"markdown","fb093e7d":"markdown","9d84b315":"markdown","8367570c":"markdown","6d548988":"markdown","e24f214f":"markdown","98fe1bf4":"markdown","863cb0d9":"markdown","de0486df":"markdown","4121a2ab":"markdown","eff437e5":"markdown","d5f41be2":"markdown"},"source":{"4111fc1e":"import warnings\nwarnings.filterwarnings(\"ignore\")","a800b78d":"#importing libraries\n\nimport numpy as np    # very useful for multi dimensional arrays and for mathematical operations\nimport pandas as pd   # Used to load our datasets and present it in the form of a dataframes\nimport matplotlib.pyplot as plt   # Used for data visualization \nimport seaborn as sns\nimport string\nimport re\n\n#Scikit - learn libraries for preprocessing and data modeling\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.impute import KNNImputer\nfrom sklearn_pandas import CategoricalImputer   #Library outside of sklearn to impute missing values\n\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier","ed8b1f8b":"# reading training and test files using pandas\n\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\n\ntest  = pd.read_csv('..\/input\/titanic\/test.csv')\n\nsub = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\n","2efd1cbb":"df_train = train.copy()\ndf_test = test.copy()","717330e4":"# train.head() will show only the top 5 rows in our train dataset \ntrain.head()","145ec445":"# Similarly, test.head() will show the top 5 rows in our test dataset\ntest.head()","a2b9aefd":"print(\"Train dataset shape: \", train.shape) #this command tells us the shape of our dataset, here we have 891 rows and 12 columns\nprint(\"Test dataset shape: \", test.shape)   # in our test dataset we have 418 rows and 11 columns","b320ce5b":"#checking for null values in our train and test data\nprint(\"**********************Train Set*************************************\")\nprint((train.isnull().sum()))\nprint(\"********************************************************************\\n\")\n","bdc58003":"print(\"**********************Test Set**************************************\")\nprint(test.isnull().sum())\nprint(\"********************************************************************\")","e8204c93":"# Let's first visualize how many people survived. Here, I have used seaborn for visualization. \n# What is seaborn? \n# Seaborn is a Python data visualization library based on matplotlib. \n#It provides a high-level interface for drawing attractive and informative statistical graphics.\n\nplt.figure(figsize = (5, 5)) # setting the size of the figure\n\nsns.set(style=\"darkgrid\")    # setting the style of my vidualizations\n\nsns.countplot(x = 'Survived', data = train, palette= ['Red', 'Blue'])    #Creating a barplot with count of survived or not survived\n\nplt.title(\"Survived (0 vs 1)\")   # setting the title of my plot\n\nplt.show()","1ac28e9b":"#Lets visualize the number of survivors bases on their gender. \n\nplt.figure(figsize = (5,5))\n\nsurvived_sex = train[train['Survived']==1]['Sex'].value_counts().sort_index(ascending = False)   # series containing the count of Male and Female survived  \nnot_survived_sex = train[train['Survived']==0]['Sex'].value_counts()       # series containing the count of Male and Female not survived\n\ndataframe1 = pd.DataFrame({'Survived':survived_sex, 'Not-Survived':not_survived_sex})    #Creating a dataframe from the above two series\ndataframe1.index = dataframe1.index.str.capitalize()               \n\ndataframe1.plot(kind='bar', stacked= 'True', color = ['Blue', 'Red'], alpha = 0.8)  #plotting a stacked barplot from the above dataframe\nplt.ylabel('Count of people')\nplt.xticks(rotation= \"horizontal\")\nplt.show()","8a2e0a1b":"#plotting the scatterplot of Age vs Fare based on whether the person survived or not. Since, both age and Fare are important factors\n#let's see the relationship between the Age and Fare among the survivors of titanic disaster\n\nplt.figure(figsize=(15,8))  # setting the size of the figure\n\ncolor = {0: 'Red', 1: 'Blue'}    #setting the color for survived and not survived\n\nplt.scatter(train['Age'], train['Fare'], c =train['Survived'].apply(lambda x: color[x]), alpha = 0.8 ) #scatter plot between age and fair\n\nplt.title('Age vs Fare')  #setting the title of our plot\nplt.xlabel(\"Age\")         #setting the xlabel\nplt.ylabel(\"Fare\")        #setting the ylable\n\nplt.show()","a3e72b38":"# Lets see the distributions of our Age columns. And as mentioned above majority of people does belong to the age of 20-50. \n\nplt.figure(figsize = (10, 6))\nsns.distplot(train['Age'], bins = 30, color = \"blue\")\nplt.title(\"\")\nplt.legend()\nplt.show()","bd208a91":"# Lets see the distributions of our Age columns. And as mentioned above majority of people does belong to the age of 20-50. \n\nplt.figure(figsize = (10, 6))\nsns.distplot(train['Fare'], bins = 50, color = \"blue\")\nplt.title(\"\")\nplt.legend()\nplt.show()","a3a4b9a4":"train['Fare'].describe()","210a3120":"# lets visualize the Pclass column. Pclass is nothing but a ticket class and it represents the socio-economic status (SES), 1st = Upper, \n# 2nd = Middle, 3rd = Lower\n\nplt.figure(figsize = (5,5))\n\n#we are using catplot here to combine a countplot() and a FacetGrid. This allows grouping within additional categorical variables. \nsns.catplot(x = 'Pclass', data = train, col=\"Survived\", kind = 'count')\nplt.show()","e00b2e2c":"# lets visualize the Embarked column. \nplt.figure(figsize = (5,5))\n\nsns.catplot(x = 'Embarked', data = train, col=\"Survived\", kind = 'count')\nplt.show()","3b5be1f4":"train.head()","2a80af4c":"# Lets first convert categorical values to ordinal. And we have to make sure that whatever we are changing to training dataset\n# we have to make the same changes in our test dataset\n\n\n#mapping 0 to male and 1 to female\ntrain['Sex'] = train['Sex'].map({\"male\": 0, \"female\": 1})  \ntest['Sex'] = test[\"Sex\"].map({\"male\": 0, \"female\": 1})\n\n#before converting the embarked values to 0, 1 and 2. Lets first impute the missing values.\n\ncg = CategoricalImputer()\ntrain['Embarked'] = cg.fit_transform(train['Embarked'].values.reshape(-1,1))\ntest['Embarked'] = cg.transform(test['Embarked'].values.reshape(-1,1))\n\n#similarly, mapping 0 to S, 1 to C and 2 to Q\ntrain['Embarked'] = train[\"Embarked\"].map({\"S\": 0, \"C\": 1, \"Q\": 2})\ntest['Embarked'] = test[\"Embarked\"].map({\"S\": 0, \"C\": 1, \"Q\": 2})\n\n# For other categorical variables we will handle it later","af261ee8":"train.head()","0317c39a":"# Lets try to impute missing values using KNNImputer for Age, Embarked and Fare. \n\nknn_train = train.copy()\nknn_test =  test.copy()\n\nknn_imputer = KNNImputer(n_neighbors=4)\n\nknn_train.iloc[:, 5] = knn_imputer.fit_transform(knn_train.iloc[:,5].values.reshape(-1,1))\nknn_test.iloc[:, 4] = knn_imputer.transform(knn_test.iloc[:,4].values.reshape(-1,1))\n\nknn_imputer = KNNImputer(n_neighbors=4)\nknn_imputer.fit(knn_train.iloc[:,9].values.reshape(-1,1))\nknn_test.iloc[:,8] = knn_imputer.transform(knn_test.iloc[:,8].values.reshape(-1,1))\n\n\n","3bf869ed":"# For Cabin we can use the first letter of the cabin. And for missing values in Cabin lets denote a letter N\n\nknn_train['Cabin'] = knn_train['Cabin'].apply(lambda x: str(x)[0].upper())\nknn_test['Cabin'] = knn_test['Cabin'].apply(lambda x: str(x)[0].upper())","681b92db":"knn_train.isnull().sum()","d44af60e":"knn_test.isnull().sum()","284a7268":"# Since SibSP and Parch means siblings\/spouse and parent\/children, so we can combine these two columns and create one \n\nknn_train['Family'] = knn_train['SibSp'] + knn_train['Parch']\nknn_test['Family'] = knn_test['SibSp'] + knn_test['Parch']","0840998d":"# Lets also use MinMax Scaler to standardize the values of fare and age. And the reasong I am using \n# minmax scaler is to get the values between 0 and 1, since all the other values in our dataset are also either 0 or 1.\n\nmx = MinMaxScaler()\n\nknn_train['Fare'] = mx.fit_transform(knn_train['Fare'].values.reshape(-1,1))\nknn_test['Fare'] = mx.transform(knn_test['Fare'].values.reshape(-1,1))\n\nmx = MinMaxScaler()\n\nknn_train['Age'] = mx.fit_transform(knn_train['Age'].values.reshape(-1,1))\nknn_test['Age'] = mx.transform(knn_test['Age'].values.reshape(-1,1))","b2a2c5f6":"# For tickets column also, lets take out the text part and for the number part, we will assign some letter to it.\n\n#Function to get the text out of the \"Ticket\" column\ndef ticket_text(x): \n    x = str(x)\n    \n    if(x.isdigit()):  #isdigit() is used to check if there is any digit in the string or not\n        return 'X'\n    else: \n        x = x.strip(string.digits)    #stripping the number out of the text\n        x = x.strip(\" \")              # stripping the extra space out\n        x = x.translate(str.maketrans(\"\",\"\", string.punctuation ))   # removing punctutaion, since there are lot of same tickets but have punctutaion in it\n        x = x.replace(\" \",\"\")\n        return x\n    \nknn_train['Ticket'] = knn_train['Ticket'].apply(lambda x: ticket_text(x))   #applying the function to transform the tickets columns\nknn_test['Ticket'] = knn_test['Ticket'].apply(lambda x: ticket_text(x))     # transforming the test set","960d08fd":"knn_train.head()","9a38fc99":"knn_test.head()","ab2b816d":"#now lets drop some columns that we dont want for our modeling\n# dropping columns from both training and test set\n\nknn_train = knn_train.drop(['PassengerId','Name', 'SibSp', 'Parch'], axis = 1)\nknn_test = knn_test.drop(['PassengerId','Name', 'SibSp','Parch'], axis = 1)\n\n","07bca271":"knn_test['Ticket'].value_counts()","b0951d7a":"knn_train.shape","64d39235":"# now we need to create dummy variables. And for that I will first stack my train and test data. If I dont stack it, then there will be\n# different number of columns due to tickets\n\nstacked_data = pd.concat([knn_train, knn_test])\nstacked_data.head()","44010bfd":"stacked_data = stacked_data.reset_index(drop=True)","4046efae":"# Now lets create dummy variables for the ordinal columns \n\nstacked_data = pd.get_dummies(stacked_data, columns = ['Pclass', 'Ticket', 'Cabin', 'Embarked'])\n\n# We also need to discard one column from each of the dummy variable created otherwise there will be a correlation problem.\n\nstacked_data = stacked_data.drop(['Pclass_1', 'Ticket_A4', \"Cabin_A\", \"Embarked_0\"], axis = 1)\n","d4d3ecda":"# Now lets get back our original train and test data\n\nknn_train = stacked_data.iloc[:891, :]\nknn_test = stacked_data.iloc[891:, :]","eefcf489":"# checking the shape of training dataset\nknn_train.shape","52949d11":"# checking the shape of test dataset\nknn_test.shape","2b06103b":"# removing the target variable from my training and test data set \n# this will be used in the train_test_split\n\nX = knn_train.drop(['Survived'], axis =1 )\ny = knn_train['Survived']\n\nknn_test = knn_test.drop(['Survived'], axis =1)\n\n#creating training and testing dataset for our model\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.3)","ce5aaa6d":"#1. Lets first start with KNN using gridsearch\n#KNN using gridsearch\n\nKNN_model = KNeighborsClassifier()\nparam_grid = {'n_neighbors': [1,3,5,7,9,11,13,15]}\ncv = StratifiedKFold(n_splits=10, random_state=0)\ngrid = GridSearchCV(KNN_model, param_grid, cv = cv, scoring='accuracy',\n                    return_train_score=True)\ngrid.fit(X_train, y_train)\n\nprint(\"Best Parameter: {}\".format(grid.best_params_))\nprint(\"Best Cross Vlidation Score: {}\".format(grid.best_score_))","7593b254":"# checking accuracy on our (30%) test set \ny_test_hat = grid.predict(X_test)\naccuracy_score(y_test,y_test_hat)","64f0bb9a":"#Initializing model again and this time will use our whole training dataset to learn the model\nknn_best = KNeighborsClassifier(n_neighbors=9)\nknn_best.fit(X, y)\n\n#use kaggle test set for predictions\ny_test_hat_1 = knn_best.predict(knn_test)\n\n\n","5e7be459":"#Logistic Regression using gridsearch\n\nlr  = LogisticRegression()\nparam_grid = {'max_iter': [5000, 10000], 'solver': ['sag', 'saga', 'liblinear'] }\n\ncv = StratifiedKFold(n_splits=10)\n\ngrid1 = GridSearchCV(lr, param_grid, cv = cv, scoring='accuracy',\n                    return_train_score=True)\ngrid1.fit(X_train, y_train)\n\nprint(\"Best Parameter: {}\".format(grid1.best_params_))\nprint(\"Best Cross Vlidation Score: {}\".format(grid1.best_score_))","64e30a19":"# checking accuracy on our (30%) test set \ny_test_hat = grid1.predict(X_test)\naccuracy_score(y_test,y_test_hat)","b5f334c1":"#Initializing model again and this time will use our whole training dataset to learn the model\nlr_best = LogisticRegression(max_iter = 5000, solver = \"sag\")\nlr_best.fit(X, y)\n\n#use kaggle test set for predictions\ny_test_hat_2 = lr_best.predict(knn_test)\n\n\n","6fd85563":"#Random Forest\n\nrandom_forest_model  = RandomForestClassifier()\nparam_grid = {'n_estimators': [100,200,300,500]}\n\ncv = StratifiedKFold(n_splits=10, shuffle=True)\n\ngrid2 = GridSearchCV(random_forest_model, param_grid, cv = cv, scoring='accuracy',\n                    return_train_score=True)\ngrid2.fit(X_train, y_train)\n\nprint(\"Best Parameter: {}\".format(grid2.best_params_))\nprint(\"Best Cross Vlidation Score: {}\".format(grid2.best_score_))","dd3b3cb1":"# checking accuracy on our (30%) test set\ny_test_hat = grid2.predict(X_test)\naccuracy_score(y_test,y_test_hat)","b8d7ee42":"#Initializing best model  and this time will use our whole training dataset to learn the model\n\nrandom_forest_best = RandomForestClassifier(max_depth = 6)\nrandom_forest_best.fit(X, y)\n\n#use kaggle test set for predictions\ny_test_hat_3 = random_forest_best.predict(knn_test)\n\n\n#creating dataframe for submission\nrandom_forest_dataframe=pd.DataFrame(y_test_hat_3,columns=['Survived'])\n","e3d6719a":"#Gradient Boosting Classifier\n\n\ngrbt_model  = GradientBoostingClassifier()\n\nparam_grid = {'n_estimators': [100,200,300,500]}\n\ncv = StratifiedKFold(n_splits=10, shuffle=True)\n\ngrid2 = GridSearchCV(grbt_model, param_grid, cv = cv, scoring='accuracy',\n                    return_train_score=True)\ngrid2.fit(X_train, y_train)\n\nprint(\"Best Parameter: {}\".format(grid2.best_params_))\nprint(\"Best Cross Vlidation Score: {}\".format(grid2.best_score_))","791b01a2":"# checking accuracy on our (30%) test set\ny_test_hat = grid2.predict(X_test)\naccuracy_score(y_test,y_test_hat)","4be8f190":"#Initializing best model  and this time will use our whole training dataset to learn the model\n\ngrbt_best = GradientBoostingClassifier(n_estimators = 300)\ngrbt_best.fit(X, y)\n\n#use kaggle test set for predictions\ny_test_hat_4 = grbt_best.predict(knn_test)\n\n\n#creating dataframe for submission\ngrbt_dataframe=pd.DataFrame(y_test_hat_4,columns=['Survived'])","214d70c0":"#neaural net using gridsearch\n\nmlp_model  = MLPClassifier()\nparam_grid = {'hidden_layer_sizes': [100,[2,100],[3,150]], 'activation':['logistic', 'tanh'], 'learning_rate':['constant','adaptive']}\n\ncv = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n\ngrid = GridSearchCV(mlp_model, param_grid, cv = cv, scoring='accuracy',\n                    return_train_score=True)\ngrid.fit(X_train, y_train)\n\nprint(\"Best Parameter: {}\".format(grid.best_params_))\nprint(\"Best Cross Vlidation Score: {}\".format(grid.best_score_))","1b7daf4e":"# checking accuracy on our (30%) test set\ny_test_hat = grid.predict(X_test)\naccuracy_score(y_test,y_test_hat)","9ec31483":"#Initializing best model  and this time will use our whole training dataset to learn the model\n\nmlp_best = MLPClassifier(activation='tanh', hidden_layer_sizes=100, learning_rate = 'adaptive')\nmlp_best.fit(X, y)\n\n#use kaggle test set for predictions\ny_test_hat_2 = mlp_best.predict(knn_test)\n\n#creating dataframe for submission\nmlp_dataframe=pd.DataFrame(y_test_hat_2,columns=['Survived'])\n\n","a5386526":"#neaural net using gridsearch\n\nxgboost_model  = XGBClassifier()\n\nxgboost_model.fit(X_train, y_train)\n\ncv = KFold(n_splits=10, shuffle=True)\n\nkf_cv_scores = cross_val_score(xgboost_model, X_train, y_train, cv=cv )\n\nprint(\"K-fold CV average score: %.2f\" % kf_cv_scores.mean())\n\n","047aa05b":"# checking accuracy on our (30%) test set\ny_test_hat = grid.predict(X_test)\naccuracy_score(y_test,y_test_hat)\n","92e97ec1":"#Initializing best model  and this time will use our whole training dataset to learn the model\n\nxgb_best = XGBClassifier()\nxgb_best.fit(X, y)\n\n#use kaggle test set for predictions\ny_test_hat_2 = xgb_best.predict(knn_test)\n\n#creating dataframe for submission\nxgb_dataframe=pd.DataFrame(y_test_hat_2,columns=['Survived'])\n\n","25c9ed78":"X = df_train.drop(['Survived'], axis = 1)\ny = df_train['Survived']\n\nknn_train.head()","c7163f76":"! pip install pycaret --user\nfrom pycaret.classification import *","41c779f1":"classifier1 = setup(data = knn_train,target = 'Survived', session_id = 345)","4e820896":"compare_models()","92305154":"tuned_model = tune_model('lightgbm')","0b0c5581":"knn_train.head()","f9e5547c":"knn_test","70127c55":"y_pred = predict_model(tuned_model, data=knn_test)","b3b1e266":"sub['Survived'] = round(y_pred['Score']).astype(int)\nsub.to_csv('submission.csv',index=False)\nsub.head()","b28ab0aa":"## To be Continued.....","773a30c4":"#### From the plot we can clearly see that the number of people who paid more fare survived in comparison to the people who paid less. Also, we can observe that most people were between the age of 20-50.","fbf749e1":"### Lets use pycaret","405b9963":"## 3. Data Cleaning\n\nData cleaning is the process of detecting and correcting inaccurate observations from a data set. It also refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data","300c021d":"### Titanic Kaggle Competition\n\nThis is my first project in the Kaggle competition. And in this notebook I tried to use very simple concepts of data science whcich are quite effective and will be helpful for the people who are starting to learn data science.\n\n\nIf this notebook is helpful for you, please give it an upvote! Enjoy!","df85b040":"## 2. Data Exploration and Visualization\n\nOnce our data has been loaded, we move onto data exploration which is the inital phase of our analysis and here, we use visual exploration to understand what is in a dataset and the characteristics of the data,","fb093e7d":"![image.png](attachment:image.png)","9d84b315":"#### 2. Logistic Regression","8367570c":"#### 1. KNN Classifier","6d548988":"#### 6. XGBoost Classifier","e24f214f":"#### 4. Gradient Boosting Classifier\n","98fe1bf4":"## 1. Loading Libraries and Data \n\nImporting libraries is the firs step of any data science project. Here, in this project I have used scikit-learn or sklearn for my machine learning tasks. Scikit Learn is one the most important library that every data analyst should know since it includes tools for many of the standard machine-learning tasks (such as clustering, classification, regression, etc.). Also, the documentation of scikit learn is really good so I would encourage you guys to go and check it out(https:\/\/scikit-learn.org\/stable\/).","863cb0d9":"## 4. Data Modeling\n\nOnce we are done with data cleansing step, we will now start building our machine learning models. And for that we need to split our data into \ntraining data and test and we use train_test_split for that.","de0486df":"#### 5. MLP Classifier (Multi-layer Perceptron)","4121a2ab":"#### 3. Random Forest Classifier","eff437e5":"#### From the above plot and from the distribution of Age, we can see that there is a huge variation in the minimum and maximum value. And therefore, I will standardize the values using standard scaler. ","d5f41be2":"<b>Let's first handle the missing values in our dataset. So, the columns which have null values are: <\/b>\n\nTrain Dataset:<br>\nAge       - 177<br>\nCabin     - 687<br>\nEmbarked  - 2<br>\n\nTest Dataset:<br>\nAge       - 86<br>\nFare      - 1<br>\nCabin     - 327<br>\nEmbarked  - 0<br>"}}