{"cell_type":{"b7bcd9af":"code","fcc4ca50":"code","b4e774cb":"code","f1d864c0":"code","e6a7f4ae":"code","48dde065":"code","f7d9606d":"code","5b138862":"code","576f27ed":"code","14653136":"code","98f20930":"code","c475c7ef":"code","95cf449b":"code","17d64a96":"code","094670af":"code","42e96dd3":"code","0563aae2":"code","e482e463":"code","a6e56e99":"code","4d2b3a32":"code","003eb799":"code","4a18d772":"code","87450a1a":"code","4900d0b6":"code","1527a0ef":"code","dcc46948":"code","242ef986":"code","a466cf25":"markdown","08a11fd0":"markdown","459f28ab":"markdown","4612d384":"markdown","d4e2d02d":"markdown","ab9cf475":"markdown","4f609cb3":"markdown","f25062f7":"markdown","c03807de":"markdown","971cb357":"markdown","69981e5e":"markdown","9d191768":"markdown","6af53ce9":"markdown","c33ffb50":"markdown","171e925d":"markdown","69147896":"markdown","0c6f1cbf":"markdown","c442812d":"markdown","67da3974":"markdown","1a8f6269":"markdown","e323c6f6":"markdown","3fb01e79":"markdown","ca7358dc":"markdown"},"source":{"b7bcd9af":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd","fcc4ca50":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","b4e774cb":"# Distribution graphs (histogram\/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) \/ nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\n","f1d864c0":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()\n","e6a7f4ae":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()\n","48dde065":"nRowsRead = 1000 # specify 'None' if want to read whole file\n# creditcard.csv may have more rows in reality, but we are only loading\/previewing the first 1000 rows\ndf1 = pd.read_csv('\/kaggle\/input\/creditcard.csv', delimiter=',', nrows = nRowsRead)\ndf1.dataframeName = 'creditcard.csv'\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')","f7d9606d":"df1.head(5)","5b138862":"plotPerColumnDistribution(df1, 10, 5)","576f27ed":"plotCorrelationMatrix(df1, 8)","14653136":"plotScatterMatrix(df1, 20, 10)","98f20930":"df1.columns","c475c7ef":"y = df1['Class']","95cf449b":"x = df1.drop(columns=['Time', 'Amount'])","17d64a96":"x = x.to_numpy()\ny = y.to_numpy()","094670af":"y[np.where(y == 0)] = -1","42e96dd3":"w = np.ones(len(x[0]))","0563aae2":"def hinge_loss_fn(y, y_hat):\n    return np.maximum(0,(np.ones(len(y)) - y * y_hat))","e482e463":"def forward(x, w):\n    return np.dot(x, w)","a6e56e99":"def svm_fn(x, y, w, epoch):\n    if (y * forward(x, w)) < 1:\n        return ( (x * y) + (-2  * (1\/epoch) * w) )\n    else:\n        return (-2 * (1\/epoch) * w)","4d2b3a32":"def optimize(w, dw, lr):\n    w += lr * dw\n    return w","003eb799":"def svm(x, y, w, epochs=1601, lr=0.001):\n\n    losses = []\n    \n    for epoch in range(1,epochs):\n        for i, _ in enumerate(x):\n            dw = svm_fn(x[i], y[i], w, epoch)\n            w = optimize(w, dw, lr)\n                \n        if(epoch % 100 == 0):\n            loss = hinge_loss_fn(y, np.dot(x, w)).mean()\n            losses.append(loss)\n            print(\"Epoch \", epoch, \" - Hinge Loss \", loss)\n\n    return w, losses","4a18d772":"w = np.zeros(len(x[0]))","87450a1a":"w, losses = svm(x, y, w)","4900d0b6":"pred = forward(x, w)","1527a0ef":"plt.title('SVM training')\nplt.xlabel('# of epochs by 200')\nplt.ylabel('# of loss percentage')\nplt.plot(losses)","dcc46948":"def accuracy(y_hat, y):\n    # get the number of card frauds predicted\n    pred_fraud = len(np.where(np.ceil(pred) > 0)[0])\n    # actual fraud numbers\n    actual_fraud = len(np.where(y > 0)[0])\n    \n    return pred_fraud\/actual_fraud","242ef986":"accuracy(pred, y)","a466cf25":"The next hidden code cells define functions for plotting data. Click on the \"Code\" button in the published kernel to reveal the hidden code.","08a11fd0":"<img align=\"left\" src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/4\/4f\/Credit-cards.jpg\/220px-Credit-cards.jpg\"\/>","459f28ab":"## Credit Card Fraud Detection\n### Support-Vector Machines (SVM)","4612d384":"<a id=\"svm_fn\"><\/a>\n#### SVM function","d4e2d02d":"<a id=\"training_fn\"><\/a>\n#### Training function","ab9cf475":"As we can see, the model created a classifier that detects between fraud and non-fraud for credit cards.<br>\n<br>\nPlease don't forget to like my notebook, it gives me motivation to create quality content.","4f609cb3":"<a id=\"exploratory-analysis\"><\/a>\n# Exploratory Analysis\n\nTo begin this exploratory analysis, first use `matplotlib` to import libraries and define functions for plotting the data. Depending on the data, not all plots will be made. (Hey, I'm just a kerneling bot, not a Kaggle Competitions Grandmaster!)","f25062f7":"<a id=\"implementation\"><\/a>\n# Implementation","c03807de":"<a id=\"results\"><\/a>\n# Results analysis","971cb357":"<a id=\"definition\"><\/a>\n### Definition\n\n<img align=\"left\" style=\"padding: 50px\" src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/7\/72\/SVM_margin.png\/300px-SVM_margin.png\">\n\n<p style=\"padding: 10px\"> In machine learning, support-vector machines (SVMs, also support-vector networks[1]) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting) <\/p>\n\n[https:\/\/en.wikipedia.org\/wiki\/Support_vector_machine](https:\/\/en.wikipedia.org\/wiki\/Support_vector_machine)","69981e5e":"Distribution graphs (histogram\/bar graph) of sampled columns:","9d191768":"Now you're ready to read in the data and use the plotting functions to visualize the data.","6af53ce9":"### Table of Contents\n\n1. [Exploratory Analysis](#exploratory-analysis) <br>\n2. [Split the data for training](#split) <br>\n3. [Implementation](#implementation) <br>\n3.1 [Loss function](#loss_fn) <br>\n3.2 [Forward function](#forward_fn) <br>\n3.3 [SVM function](#svm_fn) <br>\n3.4 [Optimize function](#optimize_fn) <br>\n3.5 [Training function](#training_fn) <br>\n4. [Results analysis](#results) <br>\n5. [Conclusion](#conclusion)","c33ffb50":"<a id=\"conclusion\"><\/a>\n# Conclusion","171e925d":"<a id=\"loss_fn\"><\/a>\n#### Loss function","69147896":"Let's take a quick look at what the data looks like:","0c6f1cbf":"There is 1 csv file in the current version of the dataset:\n","c442812d":"Correlation matrix:","67da3974":"<a id=\"forward_fn\"><\/a>\n#### Forward function","1a8f6269":"<a id=\"split\"><\/a>\n### Split the data for training","e323c6f6":"<a id=\"optimize_fn\"><\/a>\n#### Optimize function (SGD)","3fb01e79":"#### Let's check 1st file: \/kaggle\/input\/creditcard.csv","ca7358dc":"Scatter and density plots:"}}