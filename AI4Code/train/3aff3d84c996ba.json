{"cell_type":{"8849e02c":"code","2f04aa5a":"code","bde25d27":"code","d8035d87":"code","a5078370":"code","b0cb85fc":"code","77cd9183":"code","4b040874":"code","60b2b146":"code","d92e5683":"code","a01e5933":"code","909b27a7":"code","81e644da":"code","29bf6445":"code","7e27c9e1":"code","3cf93cd6":"code","bc4e3921":"code","ee3c926f":"code","2e752eaf":"code","9946c60a":"code","5c275f63":"code","a40414eb":"code","51620e40":"code","b1f20efb":"code","5f9b4dfc":"code","b385d028":"code","db7e432f":"code","8552de37":"code","810e8e02":"code","c8d78684":"code","26480c7b":"code","ffa088d8":"code","222db4b5":"code","265542eb":"code","ebd7f0e7":"code","21116977":"code","731ab882":"code","7b46180f":"code","3194e7c6":"code","ebfb05a1":"code","d2462587":"code","34f8c369":"code","cd9daefd":"code","c0ca9c4a":"code","c59209f0":"code","9585f0cf":"code","774e1a22":"code","9362ccaa":"code","446bbd52":"code","9a4c1bc9":"code","66650482":"code","831f5dac":"code","426c99e3":"code","fcc40c91":"code","68ddc50b":"code","9e0c608c":"code","12787550":"code","07b3540a":"code","25a5ee88":"code","bb60fcb5":"code","229e5cab":"code","9414e65e":"code","b9dbe11c":"code","16e3daef":"code","3df27bfb":"code","c7d1e9d7":"code","32517ed3":"code","5327a597":"markdown","6d4f7fab":"markdown","c59b6750":"markdown","cd606530":"markdown","42dac5bf":"markdown","48c8cb65":"markdown","52cca10d":"markdown","2d2e4ee3":"markdown","b76e23ac":"markdown","49519c50":"markdown","2480a3fa":"markdown","361d5e07":"markdown","0c3eb9f4":"markdown","fe6c1bd8":"markdown","630becff":"markdown","327ec84c":"markdown","f8439cd4":"markdown","443aa4d2":"markdown","523d0011":"markdown"},"source":{"8849e02c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","2f04aa5a":"df_income_train = pd.read_csv(\"train.csv\")\ndf_income_test =  pd.read_csv(\"test.csv\")","bde25d27":"df_income_train.head()","d8035d87":"df_income_train.info()","a5078370":"df_income_test.head()","b0cb85fc":"df_income_test.info()","77cd9183":"#List the columns for different datatypes:\nprint('Integer Type: ')\nprint(df_income_train.select_dtypes(np.int64).columns)\nprint('\\n')\nprint('Float Type: ')\nprint(df_income_train.select_dtypes(np.float64).columns)\nprint('\\n')\nprint('Object Type: ')\nprint(df_income_train.select_dtypes(np.object).columns)","4b040874":"df_income_train.select_dtypes('int64').head()","60b2b146":"#Find columns with null values\nnull_counts=df_income_train.select_dtypes('int64').isnull().sum()\nnull_counts[null_counts > 0]","d92e5683":"df_income_train.select_dtypes('float64').head()","a01e5933":"#Find columns with null values\nnull_counts=df_income_train.select_dtypes('float64').isnull().sum()\nnull_counts[null_counts > 0]","909b27a7":"df_income_train.select_dtypes('object').head()","81e644da":"#Find columns with null values\nnull_counts=df_income_train.select_dtypes('object').isnull().sum()\nnull_counts[null_counts > 0]","29bf6445":"mapping={'yes':1,'no':0}\n\nfor df in [df_income_train, df_income_test]:\n    df['dependency'] =df['dependency'].replace(mapping).astype(np.float64)\n    df['edjefe'] =df['edjefe'].replace(mapping).astype(np.float64)\n    df['edjefa'] =df['edjefa'].replace(mapping).astype(np.float64)\n    \ndf_income_train[['dependency','edjefe','edjefa']].describe()","7e27c9e1":"# 1.  v2a1 (total nulls: 6860) : Monthly rent payment \n# Columns related to  Monthly rent payment\n# tipovivi1, =1 own and fully paid house\n# tipovivi2, \"=1 own,  paying in installments\"\n# tipovivi3, =1 rented\n# tipovivi4, =1 precarious \n# tipovivi5, \"=1 other(assigned,  borrowed)\"","3cf93cd6":"data = df_income_train[df_income_train['v2a1'].isnull()].head()\n\ncolumns=['tipovivi1','tipovivi2','tipovivi3','tipovivi4','tipovivi5']\ndata[columns]","bc4e3921":"# Variables indicating home ownership\nown_variables = [x for x in df_income_train if x.startswith('tipo')]\n\n# Plot of the home ownership variables for home missing rent payments\ndf_income_train.loc[df_income_train['v2a1'].isnull(), own_variables].sum().plot.bar(figsize = (10, 8),\n                                                                        color = 'green',\n                                                              edgecolor = 'k', linewidth = 2);\nplt.xticks([0, 1, 2, 3, 4],\n           ['Owns and Paid Off', 'Owns and Paying', 'Rented', 'Precarious', 'Other'],\n          rotation = 20)\nplt.title('Home Ownership Status for Households Missing Rent Payments', size = 18);","ee3c926f":"#Looking at the above data it makes sense that when the house is fully paid, there will be no monthly rent payment.\n#Lets add 0 for all the null values.\nfor df in [df_income_train, df_income_test]:\n    df['v2a1'].fillna(value=0, inplace=True)\n\ndf_income_train[['v2a1']].isnull().sum()","2e752eaf":"# 2.  v18q1 (total nulls: 7342) : number of tablets household owns \n# Columns related to  number of tablets household owns \n# v18q, owns a tablet","9946c60a":"# Since this is a household variable, it only makes sense to look at it on a household level, \n# so we'll only select the rows for the head of household.\n\n# Heads of household\nheads = df_income_train.loc[df_income_train['parentesco1'] == 1].copy()\nheads.groupby('v18q')['v18q1'].apply(lambda x: x.isnull().sum())\n","5c275f63":"plt.figure(figsize = (8, 6))\ncol='v18q1'\ndf_income_train[col].value_counts().sort_index().plot.bar(color = 'blue',\n                                             edgecolor = 'k',\n                                             linewidth = 2)\nplt.xlabel(f'{col}'); plt.title(f'{col} Value Counts'); plt.ylabel('Count')\nplt.show();","a40414eb":"#Looking at the above data it makes sense that when owns a tablet column is 0, there will be no number of tablets household owns.\n#Lets add 0 for all the null values.\nfor df in [df_income_train, df_income_test]:\n    df['v18q1'].fillna(value=0, inplace=True)\n\ndf_income_train[['v18q1']].isnull().sum()","51620e40":"# 3.  rez_esc (total nulls: 7928) : Years behind in school  \n# Columns related to Years behind in school \n# Age in years\n\n# Lets look at the data with not null values first.\ndf_income_train[df_income_train['rez_esc'].notnull()]['age'].describe()","b1f20efb":"# From the above , we see that when min age is 7 and max age is 17 for Years,\n# then the 'behind in school' column has a value.\n\ndf_income_train.loc[df_income_train['rez_esc'].isnull()]['age'].describe()","5f9b4dfc":"df_income_train.loc[(df_income_train['rez_esc'].isnull() & \n                     ((df_income_train['age'] > 7) & \n                      (df_income_train['age'] < 17)))]['age'].describe()\n#There is one value that has Null for the 'behind in school' column with age between 7 and 17 ","b385d028":"df_income_train[(df_income_train['age'] ==10) & \n                df_income_train['rez_esc'].isnull()].head()\n\ndf_income_train[(df_income_train['Id'] =='ID_f012e4242')].head()\n\n# there is only one member in household for the member with age 10 and\n# who is 'behind in school'. This explains why the member is \n# behind in school.","db7e432f":"# from above we see that  the 'behind in school' column has null values \n# Lets use the above to fix the data\n\nfor df in [df_income_train, df_income_test]:\n    df['rez_esc'].fillna(value=0, inplace=True)\n    \ndf_income_train[['rez_esc']].isnull().sum()","8552de37":"# meaneduc (total nulls: 5) : average years of education for adults (18+)  \n# Columns related to average years of education for adults (18+)  \n# edjefe, years of education of male head of household, based on the interaction of escolari (years of education),\n#    head of household and gender, yes=1 and no=0\n# edjefa, years of education of female head of household, based on the interaction of escolari (years of education), \n#    head of household and gender, yes=1 and no=0 \n# instlevel1, =1 no level of education\n# instlevel2, =1 incomplete primary ","810e8e02":"data = df_income_train[df_income_train['meaneduc'].isnull()].head()\n\ncolumns=['edjefe','edjefa','instlevel1','instlevel2']\ndata[columns][data[columns]['instlevel1']>0].describe()","c8d78684":"# from the above, we find that meaneduc is null when no level of education is 0\n\nfor df in [df_income_train, df_income_test]:\n    df['meaneduc'].fillna(value=0, inplace=True)\ndf_income_train[['meaneduc']].isnull().sum()","26480c7b":"# SQBmeaned (total nulls: 5) : square of the mean years of education of adults (>=18) in the household 142  \n# Columns related to average years of education for adults (18+)  \n# edjefe, years of education of male head of household, based on the interaction of escolari (years of education),\n#    head of household and gender, yes=1 and no=0\n# edjefa, years of education of female head of household, based on the interaction of escolari (years of education), \n#    head of household and gender, yes=1 and no=0 \n# instlevel1, =1 no level of education\n# instlevel2, =1 incomplete primary","ffa088d8":"data = df_income_train[df_income_train['SQBmeaned'].isnull()].head()\n\ncolumns=['edjefe','edjefa','instlevel1','instlevel2']\ndata[columns][data[columns]['instlevel1']>0].describe()","222db4b5":"# from the above, we find that SQBmeaned is null when no level of education is 0\n\nfor df in [df_income_train, df_income_test]:\n    df['SQBmeaned'].fillna(value=0, inplace=True)\ndf_income_train[['SQBmeaned']].isnull().sum()","265542eb":"#Lets look at the overall data\n\nnull_counts = df_income_train.isnull().sum()\nnull_counts[null_counts > 0].sort_values(ascending=False)","ebd7f0e7":"# Groupby the household and figure out the number of unique values\nall_equal = df_income_train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n\n# Households where targets are not all equal\nnot_equal = all_equal[all_equal != True]\nprint('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))","21116977":"#Lets check one household\ndf_income_train[df_income_train['idhogar'] == not_equal.index[0]][['idhogar', 'parentesco1', 'Target']]","731ab882":"#Lets use Target value of the parent record (head of the household) and update rest. But before that lets check\n# if all families has a head. \n\nhouseholds_head = df_income_train.groupby('idhogar')['parentesco1'].sum()\n\n# Find households without a head\nhouseholds_no_head = df_income_train.loc[df_income_train['idhogar'].isin(households_head[households_head == 0].index), :]\n\nprint('There are {} households without a head.'.format(households_no_head['idhogar'].nunique()))","7b46180f":"# Find households without a head and where Target value are different\n\nhouseholds_no_head_equal = households_no_head.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\nprint('{} Households with no head have different Target value.'.format(sum(households_no_head_equal == False)))","3194e7c6":"# Set poverty level of the members and the head of the house within a family.\n# Iterate through each household\n\nfor household in not_equal.index:\n    # Find the correct label (for the head of household)\n    true_target = int(df_income_train[(df_income_train['idhogar'] == household) & (df_income_train['parentesco1'] == 1.0)]['Target'])\n    \n    # Set the correct label for all members in the household\n    df_income_train.loc[df_income_train['idhogar'] == household, 'Target'] = true_target\n    \n    \n# Groupby the household and figure out the number of unique values\nall_equal = df_income_train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n\n# Households where targets are not all equal\nnot_equal = all_equal[all_equal != True]\nprint('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))","ebfb05a1":"#Lets look at the dataset and plot head of household and Target\n# 1 = extreme poverty \n# 2 = moderate poverty \n# 3 = vulnerable households \n# 4 = non vulnerable households \n\ntarget_counts = heads['Target'].value_counts().sort_index()\ntarget_counts","d2462587":"target_counts.plot.bar(figsize = (8, 6),linewidth = 2,edgecolor = 'k',title=\"Target vs Total_Count\")","34f8c369":"# extreme poverty is the smallest count in the train dataset. The dataset is biased.","cd9daefd":"#Lets remove them\nprint(df_income_train.shape)\ncols=['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', \n        'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq']\n\n\nfor df in [df_income_train, df_income_test]:\n    df.drop(columns = cols,inplace=True)\n\nprint(df_income_train.shape)","c0ca9c4a":"id_ = ['Id', 'idhogar', 'Target']\n\nind_bool = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n            'instlevel9', 'mobilephone']\n\nind_ordered = ['rez_esc', 'escolari', 'age']\n\nhh_bool = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n           'pisonatur', 'pisonotiene', 'pisomadera',\n           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', \n           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',\n           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', \n           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',\n           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', \n           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n           'lugar4', 'lugar5', 'lugar6', 'area1', 'area2']\n\nhh_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n              'r4t3', 'v18q1', 'tamhog','tamviv','hhsize','hogar_nin',\n              'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', 'qmobilephone']\n\nhh_cont = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding']","c59209f0":"#Check for redundant household variables\n\nheads = df_income_train.loc[df_income_train['parentesco1'] == 1, :]\nheads = heads[id_ + hh_bool + hh_cont + hh_ordered]\nheads.shape","9585f0cf":"# Create correlation matrix\ncorr_matrix = heads.corr()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n\nto_drop","774e1a22":"corr_matrix.loc[corr_matrix['tamhog'].abs() > 0.9, corr_matrix['tamhog'].abs() > 0.9]","9362ccaa":"sns.heatmap(corr_matrix.loc[corr_matrix['tamhog'].abs() > 0.9, corr_matrix['tamhog'].abs() > 0.9],\n            annot=True, cmap = plt.cm.Accent_r, fmt='.3f');","446bbd52":"# There are several variables here having to do with the size of the house:\n# r4t3, Total persons in the household\n# tamhog, size of the household\n# tamviv, number of persons living in the household\n# hhsize, household size\n# hogar_total, # of total individuals in the household\n# These variables are all highly correlated with one another.","9a4c1bc9":"cols=['tamhog', 'hogar_total', 'r4t3']\nfor df in [df_income_train, df_income_test]:\n    df.drop(columns = cols,inplace=True)\n\ndf_income_train.shape","66650482":"# Check for redundant Individual variables\n\nind = df_income_train[id_ + ind_bool + ind_ordered]\nind.shape","831f5dac":"# Create correlation matrix\ncorr_matrix = ind.corr()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n\nto_drop","426c99e3":"# This is simply the opposite of male! We can remove the male flag.\n\nfor df in [df_income_train, df_income_test]:\n    df.drop(columns = 'male',inplace=True)\n\ndf_income_train.shape","fcc40c91":"# lets check area1 and area2 also\n# area1, =1 zona urbana \n# area2, =2 zona rural \n# area2 redundant because we have a column indicating if the house is in a urban zone\n\nfor df in [df_income_train, df_income_test]:\n    df.drop(columns = 'area2',inplace=True)\n\ndf_income_train.shape","68ddc50b":"#Finally lets delete 'Id', 'idhogar'\n\ncols=['Id','idhogar']\nfor df in [df_income_train, df_income_test]:\n    df.drop(columns = cols,inplace=True)\n\ndf_income_train.shape","9e0c608c":"x_features=df_income_train.iloc[:,0:-1]\ny_features=df_income_train.iloc[:,-1]\nprint(x_features.shape)\nprint(y_features.shape)","12787550":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,confusion_matrix,f1_score,classification_report\n\nx_train,x_test,y_train,y_test=train_test_split(x_features,y_features,test_size=0.2,random_state=1)\nrmclassifier = RandomForestClassifier()","07b3540a":"rmclassifier.fit(x_train,y_train)","25a5ee88":"y_predict = rmclassifier.predict(x_test)","bb60fcb5":"print(accuracy_score(y_test,y_predict))\nprint(confusion_matrix(y_test,y_predict))\nprint(classification_report(y_test,y_predict))","229e5cab":"y_predict_testdata = rmclassifier.predict(df_income_test)","9414e65e":"y_predict_testdata","b9dbe11c":"from sklearn.model_selection import KFold,cross_val_score","16e3daef":"seed=7\n\nkfold=KFold(n_splits=5,random_state=seed,shuffle=True)\n\nrmclassifier=RandomForestClassifier(random_state=10,n_jobs = -1)\n\nprint(cross_val_score(rmclassifier,x_features,y_features,cv=kfold,scoring='accuracy'))\n\nresults=cross_val_score(rmclassifier,x_features,y_features,cv=kfold,scoring='accuracy')\n\nprint(results.mean()*100)","3df27bfb":"num_trees= 100\n\nrmclassifier=RandomForestClassifier(n_estimators=100, random_state=10,n_jobs = -1)\n\nprint(cross_val_score(rmclassifier,x_features,y_features,cv=kfold,scoring='accuracy'))\n\nresults=cross_val_score(rmclassifier,x_features,y_features,cv=kfold,scoring='accuracy')\n\nprint(results.mean()*100)","c7d1e9d7":"# lets visualize how our model uses the different features and which features have greater effect.\nrmclassifier.fit(x_features,y_features)\n\nlabels = list(x_features)\n\nfeature_importances = pd.DataFrame({'feature': labels, 'importance': rmclassifier.feature_importances_})\n\nfeature_importances=feature_importances[feature_importances.importance>0.015]\n\nfeature_importances.head()","32517ed3":"feature_importances.sort_values(by=['importance'], ascending=True, inplace=True)\nfeature_importances['positive'] = feature_importances['importance'] > 0\nfeature_importances.set_index('feature',inplace=True)\nfeature_importances.head()\n\nfeature_importances.importance.plot(kind='barh', figsize=(11, 6),color = feature_importances.positive.map({True: 'blue', False: 'red'}))\nplt.xlabel('Importance')","5327a597":"Looking at the train and test dataset we noticed that the following:\n\n\nTrain dataset:\n\n\nRows: 9557 entries, 0 to 9556\nColumns: 143 entries, Id to Target\nColumn dtypes: float64(8), int64(130), object(5)\n\nTest dataset:\nRows: 23856 entries, 0 to 23855\nColumns: 142 entries, Id to agesq\ndtypes: float64(8), int64(129), object(5)\n\nThe important piece of information here is that we don\u2019t have \u2018Target\u2019 feature in Test Dataset. There are 5 object type, 130(Train set)\/ 129 (test set) integer type and 8 float type features.","6d4f7fab":"1.4 Explore Test dataset\n\nView first 5 records of test dataset","c59b6750":"Step 5: Check the accuracy using random forest with cross validation.","cd606530":"3.3 Lets check for any bias in the dataset","42dac5bf":"Looking at the different types of data and null values for each feature.\n\nWe found the following: \n\n1. No null values for Integer type features.\n2. No null values Object types for  features. \n3. For float type v2a1 6860 v18q1 7342 rez_esc 7928 meaneduc 5 SQBmeaned 5","48c8cb65":"5.1 Checking the score using default 10 trees","52cca10d":"From the above figure, meaneduc,dependency,overcrowding has significant influence on the model.","2d2e4ee3":"3.4 Lets look at the Squared Variables\n\n\u2018SQBescolari\u2019,\n\u2018SQBage\u2019,\n\u2018SQBhogar_total\u2019,\n\u2018SQBedjefe\u2019,\n\u2018SQBhogar_nin\u2019,\n\u2018SQBovercrowding\u2019,\n\u2018SQBdependency\u2019,\n\u2018SQBmeaned\u2019,\n\u2018agesq\u2019","b76e23ac":"1.3 Explore Train dataset\n\nView first 5 records of train dataset","49519c50":"3.3 Lets look at the target column\n\nLets see if records belonging to same household has same target\/score.","2480a3fa":"Looking at the accuracy score, RandomForestClassifier with cross validation has the highest accuracy score of 94.60%","361d5e07":"# DESCRIPTION\nIdentify the level of income qualification needed for the families in Latin America.\n\n## Problem Statement Scenario:\nMany social programs have a hard time ensuring that the right people are given enough aid. It\u2019s tricky when a program focuses on the poorest segment of the population. This segment of the population can\u2019t provide the necessary income and expense records to prove that they qualify.\n\nIn Latin America, a popular method called Proxy Means Test (PMT) uses an algorithm to verify income qualification. With PMT, agencies use a model that considers a family\u2019s observable household attributes like the material of their walls and ceiling or the assets found in their homes to classify them and predict their level of need.\n\nWhile this is an improvement, accuracy remains a problem as the region\u2019s population grows and poverty declines.\n\nThe Inter-American Development Bank (IDB)believes that new methods beyond traditional econometrics, based on a dataset of Costa Rican household characteristics, might help improve PMT\u2019s performance.\n\n\n\n\n","0c3eb9f4":"1.5 Find columns with null values","fe6c1bd8":"##### Step 1: Understand the Data\n1.1 Import necessary Libraries","630becff":"1.2 Load Data","327ec84c":"Step 4: Predict the accuracy using random forest classifier.","f8439cd4":"5.2 Checking the score using 100 trees","443aa4d2":"3.2 Lets fix the column with null values\nAccording to the documentation for these columns:\n\n1. v2a1 (total nulls: 6860) : Monthly rent payment\n2. v18q1 (total nulls: 7342) : number of tablets household owns\n3. rez_esc (total nulls: 7928) : Years behind in school\n4. meaneduc (total nulls: 5) : average years of education for adults (18+)\n5. SQBmeaned (total nulls: 5) : square of the mean years of education of adults (>=18) in the household 142","523d0011":"3.1 Lets fix the column with mixed values.\n\nAccording to the documentation for these columns:\n\ndependency: Dependency rate, calculated = (number of members of the household younger than 19 or older than 64)\/(number of member of household between 19 and 64)\n\nedjefe: years of education of male head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n\nedjefa: years of education of female head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n\nFor these three variables, it seems \u201cyes\u201d = 1 and \u201cno\u201d = 0. We can correct the variables using a mapping and convert to floats."}}