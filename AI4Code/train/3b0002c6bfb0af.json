{"cell_type":{"555e8b96":"code","816d239d":"code","0e6d2ff9":"code","362f3c98":"code","2fb027cc":"code","736ae332":"code","ecb15139":"code","435cb5ef":"code","674e308c":"code","38688238":"code","d3b2aaf6":"code","6f7da159":"markdown","7f42d0d6":"markdown","76979258":"markdown","50a7f3dc":"markdown","747bd17e":"markdown","f9f7a92e":"markdown"},"source":{"555e8b96":"!pip install -q --upgrade pip\n!pip install -q transformers","816d239d":"import pandas as pd\nimport numpy as np\nimport os\nfrom tqdm.notebook import tqdm\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\nfrom difflib import SequenceMatcher\nimport re\nimport torch","0e6d2ff9":"def deEmojify(text):\n    regrex_pattern = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           \"]+\", flags = re.UNICODE)\n    return regrex_pattern.sub(r'',text)","362f3c98":"data_path = \"..\/input\/super-ai-engineer-scg-thai-qa-individual\/\"\ncontext_test_path = data_path + \"test\/\"\nquestion_test_path = data_path + \"test.csv\"\n\ncontext_test_files = os.listdir(context_test_path)\ntest = pd.read_csv(question_test_path)\ncontext_test = {}","2fb027cc":"for fname in tqdm(context_test_files):\n    with open(context_test_path + fname, encoding=\"utf-8\") as f:\n        lines = f.readlines()\n        hashtag = \"\"\n        raw = \"\"\n        for line in lines:\n            if not line.startswith(\"#\") and len(line.strip()) >= 2:\n                line = deEmojify(line)\n                line = line.replace(\"\u201d\", \"\").replace(\"\u201c\", \"\").replace(\"\\\"\", \"\").replace(\"\\ufeff\", \"\").replace(\"\\u200b\", \"\")\n                line = line.strip()\n                raw += line + \"\\n\"\n            elif line.startswith(\"#\"):\n                hashtag += line.strip()\n    context_test[fname] = {\"context\": raw, \"hashtag\": hashtag}","736ae332":"test = pd.read_csv(question_test_path)\ntest[\"article_id\"] = test[\"article_path\"].apply(lambda x: x.replace(\"test\/\", \"\"))\ntest[\"context\"] = test[\"article_id\"].apply(lambda x: context_test.get(x)[\"context\"])\ntest[\"hashtag\"] = test[\"article_id\"].apply(lambda x: context_test.get(x)[\"hashtag\"])\ntest[\"Predicted\"] = \"\"","ecb15139":"pretrained = \"deepset\/xlm-roberta-large-squad2\"\ntokenizer = AutoTokenizer.from_pretrained(pretrained)\nmodel = AutoModelForQuestionAnswering.from_pretrained(pretrained)","435cb5ef":"if torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\nmodel.to(device);","674e308c":"def find_answer(question, context, tokenizer=tokenizer, model=model, device=device):\n    def predict(inputs, model):\n        input_ids = inputs[\"input_ids\"].tolist()[0]\n        text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n        answer_start_scores, answer_end_scores = model(**inputs)\n        answer_start = torch.argmax(answer_start_scores)  \n        answer_score = torch.max(answer_start_scores)\n        answer_end = torch.argmax(answer_end_scores) + 1\n        answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n        return answer, answer_score\n    \n    inputs = tokenizer(question, context, add_special_tokens=True, return_tensors=\"pt\").to(device)\n    input_size = inputs.input_ids.size()[1]\n    answer, answer_score = \"<s>\", -1\n    if input_size > 512:\n        max_length = 2000\n        chunks = [context[i:i+max_length] for i in range(0, len(context), max_length)]\n        max_chunk_answer_score = -1\n        for chunk in chunks:\n            inputs = tokenizer(question, chunk, add_special_tokens=True, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n            chunk_answer, chunk_answer_score = predict(inputs, model)\n            num_chunk_answer_score = chunk_answer_score.detach().cpu().numpy()\n            if chunk_answer != \"<s>\" and num_chunk_answer_score > max_chunk_answer_score:\n                answer, answer_score = chunk_answer, chunk_answer_score\n                max_chunk_answer_score = num_chunk_answer_score\n    else:\n        answer, answer_score = predict(inputs, model)\n    return answer, answer_score","38688238":"for idx, row in tqdm(test.iterrows()):\n    question = row[\"question\"].strip()\n    context = row[\"context\"].strip()\n    hashtag = row[\"hashtag\"].strip()\n    lines = context.split(\"\\n\")\n    predict = \"\"\n    \n    #hashtag answer\n    hashtag = hashtag.replace(\"\\n\", \" \")\n    question = question.replace(\"?\", \"\")\n    question_hashtag = \" \".join([h for h in question.split(\" \") if h.startswith(\"#\")])\n    answer_hashtag = \" \".join([h for h in hashtag.split(\" \") if h.startswith(\"#\")])\n    \n    # hashtag defined\n    if \"\u0e2a\u0e48\u0e27\u0e19\u0e02\u0e22\u0e32\u0e22\" in question and \"#\" in question:\n        for q in question_hashtag.split(\"#\"):\n            for h in hashtag.split(\"#\"):\n                if q.strip() and h.strip() and q in h:\n                    predict += h.replace(q, \"\")\n        predict = predict.strip()\n        continue\n    \n    # hashtag answer\n    if \"#\" in question or \"\u0e41\u0e2e\u0e0a\u0e41\u0e17\u0e01\" in question or \"\u0e41\u0e2e\u0e0a\u0e41\u0e17\u0e47\u0e01\" in question or \"hashtag\" in question.lower():\n        if question_hashtag:\n            for h in question_hashtag.split(\" \"):\n                answer_hashtag = answer_hashtag.replace(h, \"\")\n        predict = answer_hashtag.strip()\n    else:\n        predict, score = find_answer(question, context)\n    \n    if \"<s>\" in predict:\n        max_size = -1\n        ans_line = -1\n        line_start = -1\n        line_end = -1\n        for line_num, line in enumerate(lines):\n            s = SequenceMatcher(None, line, question)\n            match = s.find_longest_match(0, len(line), 0, len(question))\n            if match.size > max_size:\n                max_size = match.size\n                ans_line = line_num\n                line_start = match.a\n                line_end = match.a + match.size\n        predict = lines[ans_line][line_start+max_size:min(line_end+50, len(lines[ans_line]))]\n        predict = predict.strip()\n    test.loc[idx, \"Predicted\"] = predict.strip()","d3b2aaf6":"submit = pd.read_csv(\"..\/input\/super-ai-engineer-scg-thai-qa-individual\/test.csv\")\nsubmit.loc[test.index,\"Predicted\"] = test.Predicted\nsubmit.loc[test.index,\"context\"] = test.context\nsubmit.loc[test.index,\"question\"] = test.question\nsubmit.to_csv(\"test.csv\", index=False)\nrst = submit[[\"Id\", \"Predicted\"]]\nrst.to_csv(\"submission.csv\", index=False)","6f7da159":"## Question Answering Model","7f42d0d6":"# SCG","76979258":"## Predict Answer","50a7f3dc":"### Test","747bd17e":"## Read Data","f9f7a92e":"## Import Packages"}}