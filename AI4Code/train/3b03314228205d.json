{"cell_type":{"d1647a2a":"code","87c22c42":"code","d7d9bb30":"code","6833b049":"code","7f636b82":"code","4a89d9bb":"code","cc46b57f":"code","74be0c31":"code","cb9edaae":"code","2e579aa8":"code","d8392a69":"code","a4a39488":"code","e0309f0d":"code","6327da76":"code","687403eb":"code","a6ebfc04":"code","844d247f":"markdown","c52b35ac":"markdown","fcc2ba23":"markdown","c846a80a":"markdown","3b3845dd":"markdown","82d4d76e":"markdown","a8361318":"markdown"},"source":{"d1647a2a":"import sys\nsys.path.append('..\/input\/pytorch-image-models\/pytorch-image-models-master')\nimport pandas as pd, numpy as np, os\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\nimport math\nimport os\n\nimport warnings\nimport random\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom typing import *\nimport albumentations\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nimport cv2\n\nimport numpy as np\nimport pandas as pd\nimport timm\nimport torch\nimport torch.nn.functional as F\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim.optimizer import Optimizer\n\nfrom torchvision import models\nfrom tqdm.notebook import tqdm\nimport pandas as pd\nwarnings.filterwarnings(\"ignore\")\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\nfrom IPython.display import clear_output \nclear_output()","87c22c42":"import albumentations as A\n\nCONFIG = {\n    \"COMPETITION_NAME\": \"SETI-ALIENS\",\n    \"MODEL\": {\"MODEL_FACTORY\": \"timm\", \"MODEL_NAME\": \"resnet18d\"},\n    \"WORKSPACE\": \"Kaggle\",\n    \"DATA\": {\n        \"TARGET_COL_NAME\": \"target\",\n        \"IMAGE_COL_NAME\": \"id\",\n        \"NUM_CLASSES\": 1,\n        \"CLASS_LIST\": [0, 1],\n        \"IMAGE_SIZE\": 512,\n        \"CHANNEL_MODE\": \"spatial_6ch\",\n        \"USE_MIXUP\": True\n    },\n    \"CROSS_VALIDATION\": {\"SCHEMA\" : 'StratifiedKFold', \"NUM_FOLDS\": 4},\n    \"TRAIN\": {\n        \"DATALOADER\": {\n            \"batch_size\": 32,\n            \"shuffle\": True, #using random sampler\n            \"num_workers\": 4,\n            \"drop_last\": False,\n        },\n        \"SETTINGS\": {\n            \"IMAGE_SIZE\": 512,\n            \"NUM_EPOCHS\": 8,\n            \"USE_AMP\": True,\n            \"USE_GRAD_ACCUM\": False,\n            \"ACCUMULATION_STEP\": 1,\n            \"DEBUG\": False,\n            \"VERBOSE\": True,\n            \"VERBOSE_STEP\": 10,\n        },\n    },\n    \"VALIDATION\": {\n        \"DATALOADER\": {\n            \"batch_size\": 32,\n            \"shuffle\": False,\n            \"num_workers\": 4,\n            \"drop_last\": False,\n        }\n    },\n    \"TEST\": {\n        \"DATALOADER\": {\n            \"batch_size\": 32,\n            \"shuffle\": False,\n            \"num_workers\": 4,\n            \"drop_last\": False,\n        }\n    },\n    \"OPTIMIZER\": {\n        \"NAME\": \"AdamW\",\n        \"OPTIMIZER_PARAMS\": {\"lr\": 1e-4, \"eps\": 1.0e-8, \"weight_decay\": 1.0e-3},\n    },\n    \"SCHEDULER\": {\n        \"NAME\": \"CosineAnnealingWarmRestarts\",\n        \"SCHEDULER_PARAMS\": {\n            \"T_0\": 4,\n            \"T_mult\": 1,\n            \"eta_min\": 1.0e-7,\n            \"last_epoch\": -1,\n            \"verbose\": True,\n        # \"NAME\": \"CosineAnnealingLR\",\n        # \"SCHEDULER_PARAMS\": {\n        #     \"T_max\": 16,\n        #     \"eta_min\": 1.0e-7,\n        #     \"last_epoch\": -1,\n        #     \"verbose\": True,\n        },\n        \"CUSTOM\": \"GradualWarmupSchedulerV2\",\n        \"CUSTOM_PARAMS\": {\"multiplier\": 10, \"total_epoch\": 1},\n        \"VAL_STEP\": False,\n    },\n    \"CRITERION_TRAIN\": {\n        \"NAME\": \"BCEWithLogitsLoss\",\n        \"LOSS_PARAMS\": {\n            \"weight\": None,\n            \"size_average\": None,\n            \"reduce\": None,\n            \"reduction\": \"mean\",\n            \"pos_weight\": None\n        },\n    },\n    \"CRITERION_VALIDATION\": {\n        \"NAME\": \"BCEWithLogitsLoss\",\n        \"LOSS_PARAMS\": {\n            \"weight\": None,\n            \"size_average\": None,\n            \"reduce\": None,\n            \"reduction\": \"mean\",\n            \"pos_weight\": None\n        },\n    },\n    \"TRAIN_TRANSFORMS\": {\n        # \"RandomResizedCrop\": {\"height\": 384, \"width\": 384, \"scale\": [0.9, 1.0], \"p\": 1},\n        \n        \"VerticalFlip\": {\"p\": 0.5},\n        \"HorizontalFlip\": {\"p\": 0.5},\n        \"Resize\": {\"height\": 512, \"width\": 512, \"p\": 1},\n        #\"Normalize\": {\"mean\": (0.485, 0.456, 0.406), \"std\": (0.229, 0.224, 0.225)},\n       \n    },\n    \"VALID_TRANSFORMS\": {\n        \"Resize\": {\"height\": 512, \"width\": 512, \"p\": 1},\n        #\"Normalize\": {\"mean\": (0.485, 0.456, 0.406), \"std\": (0.229, 0.224, 0.225)},\n    },\n    \"TEST_TRANSFORMS\": {\n        \"Resize\": {\"height\": 384, \"width\": 384, \"p\": 1},\n        #\"Normalize\": {\"mean\": (0.485, 0.456, 0.406), \"std\": (0.229, 0.224, 0.225)},\n    },\n    \"PATH\": {\n        \"DATA_DIR\": \"\/content\/\",\n        \"TRAIN_CSV\": \"..\/input\/seti-breakthrough-listen\/train_labels.csv\",\n#         \"TRAIN_PATH\": \"\/content\/jpeg-melanoma-384x384\/train\",\n        \n#         \"TEST_CSV\": \"\/content\/jpeg-melanoma-384x384\/test.csv\",\n#         \"TEST_PATH\": \"\/content\/jpeg-melanoma-384x384\/test\",\n        \"SAVE_WEIGHT_PATH\": \".\/\",\n        \"OOF_PATH\": \".\/\",\n        \"LOG_PATH\": \".\/log.txt\",\n        # \"SAVE_WEIGHT_PATH\": \"\/content\/drive\/MyDrive\/Kaggle Projects\/[Kaggle] SETI Breakthrough Listen - E.T. Signal Search\/JuneV1\/\",\n        # \"OOF_PATH\": \"\/content\/drive\/MyDrive\/Kaggle Projects\/[Kaggle] SETI Breakthrough Listen - E.T. Signal Search\/JuneV1\/\",\n        # \"LOG_PATH\": \"\/content\/drive\/MyDrive\/Kaggle Projects\/[Kaggle] SETI Breakthrough Listen - E.T. Signal Search\/JuneV1\/log.txt\",\n    },\n    \"SEED\": 19921930,\n    \"DEVICE\": \"cuda\",\n    \"GPU\": \"V100\",\n}\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nconfig = CONFIG","d7d9bb30":"def seed_all(seed: int = 1930):\n    \"\"\"Seed all random number generators.\"\"\"\n    print(\"Using Seed Number {}\".format(seed))\n\n    os.environ[\"PYTHONHASHSEED\"] = str(\n        seed\n    )  # set PYTHONHASHSEED env var at fixed value\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.cuda.manual_seed(seed)  # pytorch (both CPU and CUDA)\n    np.random.seed(seed)  # for numpy pseudo-random generator\n    random.seed(seed)  # set fixed value for python built-in pseudo-random generator\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.enabled = False\n\n\ndef seed_worker(_worker_id):\n    \"\"\"Seed a worker with the given ID.\"\"\"\n    worker_seed = torch.initial_seed() % 2 ** 32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n    \nseed_all(config['SEED'])","6833b049":"train = pd.read_csv(CONFIG['PATH']['TRAIN_CSV'])\n# test = pd.read_csv('..\/input\/seti-breakthrough-listen\/sample_submission.csv')\n\ndef get_train_file_path(image_id):\n    if config['WORKSPACE'] == 'Kaggle':\n\n        return \"..\/input\/seti-breakthrough-listen\/train\/{}\/{}.npy\".format(image_id[0], image_id)\n    elif config['WORKSPACE'] == 'Colab':\n        return \"\/content\/seti-breakthrough-listen\/{}\/{}.npy\".format(image_id[0], image_id)\n\n# def get_test_file_path(image_id):\n#     return \"..\/input\/seti-breakthrough-listen\/test\/{}\/{}.npy\".format(image_id[0], image_id)\n\ntrain['file_path'] = train['id'].apply(get_train_file_path)\n# test['file_path'] = test['id'].apply(get_test_file_path)\n\ndisplay(train.head())","7f636b82":"def make_folds(train_csv: pd.DataFrame, config) -> pd.DataFrame:\n    \"\"\"Split the given dataframe into training folds.\"\"\"\n    # TODO: add options for cv_scheme as it is cumbersome here.\n    if config['CROSS_VALIDATION']['SCHEMA'] == \"StratifiedKFold\":\n        df_folds = train_csv.copy()\n        skf = StratifiedKFold(\n            n_splits=config['CROSS_VALIDATION']['NUM_FOLDS'], shuffle=True, random_state=config['SEED']\n        )\n\n        for fold, (train_idx, val_idx) in enumerate(\n            skf.split(\n                X=df_folds[config['DATA']['IMAGE_COL_NAME']], y=df_folds[config['DATA']['TARGET_COL_NAME']]\n            )\n        ):\n            df_folds.loc[val_idx, \"fold\"] = int(fold + 1)\n        df_folds[\"fold\"] = df_folds[\"fold\"].astype(int)\n        print(df_folds.groupby([\"fold\", config['DATA']['TARGET_COL_NAME']]).size())\n\n    elif config.cv_schema == \"GroupKfold\":\n        df_folds = train_csv.copy()\n        gkf = GroupKFold(n_splits=config.num_folds)\n        groups = df_folds[config.group_kfold_split].values\n        for fold, (train_index, val_index) in enumerate(\n            gkf.split(X=df_folds, y=df_folds[config.class_col_name], groups=groups)\n        ):\n            df_folds.loc[val_index, \"fold\"] = int(fold + 1)\n        df_folds[\"fold\"] = df_folds[\"fold\"].astype(int)\n        try:\n            print(df_folds.groupby([\"fold\", config.class_col_name]).size())\n        except:\n            display(df_folds)\n\n    else:  # No CV Schema used in this file, but custom one\n        df_folds = train_csv.copy()\n        try:\n            print(df_folds.groupby([\"fold\", config.class_col_name]).size())\n        except:\n            display(df_folds)\n\n    return df_folds","4a89d9bb":"df_folds =  make_folds(train, config)\nF1 = df_folds[df_folds['fold'] == 1]\nF2 = df_folds[df_folds['fold'] == 2]\nF3 = df_folds[df_folds['fold'] == 3]\nF4 = df_folds[df_folds['fold'] == 4]\nfolds_df = pd.concat([F1, F2, F3, F4])\n\n\nfolds_df = folds_df.reset_index(drop=True)\ny_true_df = folds_df[['id', 'target']]","cc46b57f":"\n# my_folds_sequence = folds_df['id'].values\n# oof_991 = pd.read_csv(\"..\/input\/forwardensemble\/oof_991.csv\")\n# oof_991_dict = dict(zip(oof_991.id, oof_991.target))        \n# sorted_arr = []\n# for ids in my_folds_sequence:\n#     # print(ids)\n#     sorted_arr.append(oof_991_dict[ids])\n# print(roc_auc_score(folds_df['target'].values, sorted_arr))\n# oof_991['target'] = sorted_arr\n# oof_991.to_csv(\"oof_91.csv\", index=False)","74be0c31":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nfrom typing import List\nfrom sklearn.metrics import roc_auc_score\n\n\nclass ForwardEnsemble:\n    def __init__(\n        self,\n        dir: str,\n        oof: pd.DataFrame,\n        weight_interval: int,\n        patience: int,\n        min_increase: float,\n        target_column_names: List[str],\n        pred_column_names: List[str],\n    ):\n        super().__init__()\n        self.dir = dir\n        FILES = os.listdir(dir)\n        self.oof_list = np.sort([f for f in FILES if \"oof\" in f])\n        self.num_oofs = len(self.oof_list)\n\n        self.oof = oof  # the oof csv with n rows m columns where n is the number of images in the dataset, and m be the number of target columns * number of oof you have\n        self.weight_interval = weight_interval\n        self.patience = patience\n        self.min_increase = min_increase\n        self.target_column_names = (\n            target_column_names  # target_cols = oof[0].iloc[:, 1:12].columns.tolist()\n        )\n        self.pred_column_names = (\n            pred_column_names  # pred_cols = oof[0].iloc[:, 15:].columns.tolist()\n        )\n\n        self.col_len = len(target_column_names)\n\n        self.num_test_images = len(oof[0])\n\n        # get ground truth\n        self.y_true = y_true_df['target'].values\n\n        self.all_oof_preds = np.zeros(\n            (self.num_test_images, self.num_oofs * self.col_len)\n        )\n\n        # append all oof preds to all_oof_preds: for example - k=0 -> all_oof_preds[:,0:11] = self.oof[0][['ETT - Abnormal OOF', etc]].values\n        for k in range(self.num_oofs):\n            self.all_oof_preds[\n                :,\n                int(k * self.col_len) : int((k + 1) * self.col_len),\n            ] = oof[k][pred_column_names].values\n            \n        print(self.all_oof_preds)\n        print(self.num_oofs)\n        \n        self.model_i_score, self.model_i_index, self.model_i_weight = 0, 0, 0\n\n    def __len__(self):\n        return len(\n            self.column_names\n        )  # get number of prediction columns, in multi-label, should have more than 1 column, while in binary, there is only 1\n\n    def macro_multilabel_auc(self, label, pred):\n        \"\"\" Also works for binary AUC like Melanoma\"\"\"\n        aucs = []\n#         for i in range(self.col_len):\n#             print(label[:,i])\n#             print()\n#             print(pred[:, i])\n#             print(roc_auc_score(label[:, i], pred[:, i]))\n        aucs.append(roc_auc_score(label, pred))\n        return np.mean(aucs)\n\n    def compute_best_oof(self):\n        _all = []\n        for k in range(self.num_oofs):\n            print(self.all_oof_preds[:, 0])\n            auc = self.macro_multilabel_auc(\n                self.y_true,\n                self.all_oof_preds[\n                    :,k\n                ],\n            )\n            _all.append(auc)\n            print(\"Model %i has OOF AUC = %.4f\" % (k, auc))\n        best_auc, best_oof_index = np.max(_all), np.argmax(_all)\n        return best_auc, best_oof_index\n\n    def forward_ensemble(self):\n        DUPLICATES = False\n        old_best_auc, best_oof_index = self.compute_best_oof()\n        chosen_model = [best_oof_index]\n        optimal_weights = []\n        for oof_index in range(self.num_oofs):\n            curr_model = self.all_oof_preds[\n                :,\n                int(best_oof_index * self.col_len) : int(\n                    (best_oof_index + 1) * self.col_len\n                ),\n            ]\n            for i, k in enumerate(chosen_model[1:]):\n                # this step is confusing because it overwrites curr_model in the previous step. basically curr_model is reset to the best oof model initially, and then loop through to get the best oof\n                curr_model = (\n                    optimal_weights[i]\n                    * self.all_oof_preds[\n                        :, int(k * self.col_len) : int((k + 1) * self.col_len)\n                    ]\n                    + (1 - optimal_weights[i]) * curr_model\n                )\n\n            print(\"Searching for best model to add\")\n\n            # try add each model\n            for i in range(self.num_oofs):\n                print(i, \", \", end=\"\")\n                if not DUPLICATES and (i in chosen_model):\n                    continue\n                best_weight_index, best_score, patience_counter = 0, 0, 0\n                for j in range(self.weight_interval):\n                    temp = (j \/ self.weight_interval) * self.all_oof_preds[\n                        :, int(i * self.col_len) : int((i + 1) * self.col_len)\n                    ] + (1 - j \/ self.weight_interval) * curr_model\n                    auc = self.macro_multilabel_auc(self.y_true, temp)\n\n                    if auc > best_score:\n                        best_score = auc\n                        best_weight_index = j \/ self.weight_interval\n                    else:\n                        patience_counter += 1\n                        # in this loop, if 10 increment in j does not lead to any increase in AUC, we break out\n                    if patience_counter > self.patience:\n                        break\n                    if best_score > self.model_i_score:\n                        self.model_i_score = best_score\n                        self.model_i_index = i\n                        self.model_i_weights = best_weight_index\n\n            increment = self.model_i_score - old_best_auc\n            if increment <= self.min_increase:\n                print(\"No more significant increase\")\n                break\n            # DISPLAY RESULTS\n            print()\n            print(\n                \"Ensemble AUC = %.4f after adding model %i with weight %.3f. Increase of %.4f\"\n                % (\n                    self.model_i_score,\n                    self.model_i_index,\n                    self.model_i_weights,\n                    increment,\n                )\n            )\n            print()\n\n            old_best_auc = self.model_i_score\n            chosen_model.append(self.model_i_index)\n            optimal_weights.append(self.model_i_weights)\n            print(chosen_model)\n        return chosen_model, optimal_weights\n\n\nif __name__ == \"__main__\":\n    PATH = \"..\/input\/forwardensemble\"\n    FILES = os.listdir(PATH)\n    OOF = np.sort([f for f in FILES if \"oof\" in f])\n    OOF_CSV = [pd.read_csv(os.path.join(PATH,k)) for k in OOF]\n    \n    print(\"We have %i oof files...\" % len(OOF))\n    print()\n    print(OOF)\n    SUB = np.sort([f for f in FILES if \"sub\" in f])\n    SUB_CSV = [pd.read_csv(os.path.join(PATH,k)) for k in SUB]\n\n    print(\"We have %i submission files...\" % len(SUB))\n    print()\n    print(SUB)\n    target_cols = [\n        \"target\"\n    ]\n\n    pred_cols = [\n        \"target\"\n    ]\n    for i,j in zip(target_cols, pred_cols):\n        print(i,j)\n        _target_cols = [i]\n        _pred_cols = [j]\n        forward_ens = ForwardEnsemble(\n            dir=PATH,\n            oof=OOF_CSV,\n            weight_interval=1000, # 200\n            patience=20, # 10\n            min_increase=0.0003, # 0.00003\n            target_column_names=_target_cols,\n            pred_column_names=_pred_cols,\n        )\n        m, w = forward_ens.forward_ensemble()\n        \n\n        x = np.zeros(( len(OOF_CSV[0]), len(OOF)*len(_pred_cols)))\n        for k in range(len(OOF)):\n            x[:, int(k*len(_pred_cols)):int((k+1)*len(_pred_cols))] = OOF_CSV[k][_pred_cols].values    \n            \n        _target_cols = [i]\n        _pred_cols = [j]\n        md = x[:, int(m[0]*len(_pred_cols)):int((m[0]+1)*len(_pred_cols))]\n        for i, k in enumerate(m[1:]):\n            md = w[i]*x[:, int(k*len(_pred_cols)):int((k+1)*len(_pred_cols))] + (1-w[i])*md\n            \n        plt.hist(md,bins=100)\n        plt.title('Ensemble OOF predictions')\n        plt.show()\n        df = OOF_CSV[0].copy()\n        df[_target_cols] = md\n\n#         y = np.zeros((len(SUB_CSV[0]), len(SUB) * len(_pred_cols)))\n#         for k in range(len(SUB)):\n#             y[:, int(k * len(_pred_cols)) : int((k + 1) * len(_pred_cols))] = SUB_CSV[k][\n#                 _target_cols\n#             ].values\n\n#         md2 = y[:, int(m[0] * len(_pred_cols)) : int((m[0] + 1) * len(_pred_cols))]\n#         for i, k in enumerate(m[1:]):\n#             md2 = (\n#                 w[i] * y[:, int(k * len(_pred_cols)) : int((k + 1) * len(_pred_cols))]\n#                 + (1 - w[i]) * md2\n#             )\n#         plt.hist(md2, bins=100)\n#         plt.show()\n\n#         df = SUB_CSV[0].copy()\n#         df[_target_cols] = md2\n#         df.to_csv(\"ensemble_sub.csv\", index=False)\n#         df.head()","cb9edaae":"print('We are using models',m)\nprint('with weights',w)\n# print('and achieve ensemble AUC = %.4f'%old)","2e579aa8":"md","d8392a69":"df = OOF_CSV[0].copy()\ndf.pred = md\ndf.to_csv('ensemble_oof.csv',index=False)\ndf.head()","a4a39488":"SUB = np.sort( [f for f in FILES if 'sub' in f] )\nSUB_CSV = [pd.read_csv(os.path.join(PATH,k)) for k in SUB]\n\nprint('We have %i submission files...'%len(SUB))\nprint(); print(SUB)","e0309f0d":"# VERFIY THAT SUBMISSION FILES MATCH OOF FILES\na = np.array( [ int( x.split('_')[1].split('.')[0]) for x in SUB ] )\nb = np.array( [ int( x.split('_')[1].split('.')[0]) for x in OOF ] )\nif len(a)!=len(b):\n    print('ERROR submission files dont match oof files')\nelse:\n    for k in range(len(a)):\n        if a[k]!=b[k]: print('ERROR submission files dont match oof files')","6327da76":"y = np.zeros(( len(SUB_CSV[0]), len(SUB)*len(pred_cols)))\nfor k in range(len(SUB)):\n    y[:, int(k*len(pred_cols)):int((k+1)*len(pred_cols))] = SUB_CSV[k][target_cols].values","687403eb":"y = np.zeros(( len(SUB_CSV[0]), len(SUB)*len(pred_cols)))\nfor k in range(len(SUB)):\n    y[:, int(k*len(pred_cols)):int((k+1)*len(pred_cols))] = SUB_CSV[k][target_cols].values\n\nmd2 = y[:, int(m[0]*len(pred_cols)):int((m[0]+1)*len(pred_cols))]\nfor i, k in enumerate(m[1:]):\n    md2 = w[i]*y[:, int(k*len(pred_cols)):int((k+1)*len(pred_cols))] + (1-w[i])*md2\nplt.hist(md2,bins=100)\nplt.show()","a6ebfc04":"df = SUB_CSV[0].copy()\ndf[target_cols] = md2\ndf.to_csv('submission.csv',index=False)\ndf.head()","844d247f":"# Load SUB Files","c52b35ac":"# Forward Ensembling","fcc2ba23":"# Load Files","c846a80a":"# Seeding","3b3845dd":"# Dependencies","82d4d76e":"# Config","a8361318":"# Build SUB Ensemble"}}