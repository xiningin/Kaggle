{"cell_type":{"fda3f58a":"code","42c0ef56":"code","f069a6f8":"code","6521d92f":"code","91fe8d65":"markdown"},"source":{"fda3f58a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","42c0ef56":"from tpot import TPOTClassifier\nfrom sklearn.model_selection import train_test_split\n\n#load the data\ntitanic = pd.read_csv('..\/input\/train.csv')\n#drop the empty ones\ntitanic.dropna(inplace=True)\n#replace strings with numbered lists\ntitanic = pd.get_dummies(titanic, columns=['Sex', 'Embarked', 'Pclass'], drop_first=True)\n#fill empty spaces with reasonable values\ntitanic[ 'Age' ] = titanic.Age.fillna( titanic.Age.mean() )\ntitanic[ 'Fare' ] = titanic.Fare.fillna( titanic.Fare.mean() )\n#create title column\ntitle = pd.DataFrame()\n# we extract the title from each name\ntitle[ 'Title' ] = titanic[ 'Name' ].map( lambda name: name.split( ',' )[1].split( '.' )[0].strip() )\n\n# a map of more aggregated titles\nTitle_Dictionary = {\n                    \"Capt\":       \"Officer\",\n                    \"Col\":        \"Officer\",\n                    \"Major\":      \"Officer\",\n                    \"Jonkheer\":   \"Royalty\",\n                    \"Don\":        \"Royalty\",\n                    \"Sir\" :       \"Royalty\",\n                    \"Dr\":         \"Officer\",\n                    \"Rev\":        \"Officer\",\n                    \"the Countess\":\"Royalty\",\n                    \"Dona\":       \"Royalty\",\n                    \"Mme\":        \"Mrs\",\n                    \"Mlle\":       \"Miss\",\n                    \"Ms\":         \"Mrs\",\n                    \"Mr\" :        \"Mr\",\n                    \"Mrs\" :       \"Mrs\",\n                    \"Miss\" :      \"Miss\",\n                    \"Master\" :    \"Master\",\n                    \"Lady\" :      \"Royalty\"\n\n                    }\n\n# we map each title\ntitle[ 'Title' ] = title.Title.map( Title_Dictionary )\ntitle = pd.get_dummies( title.Title )\ncabin = pd.DataFrame()\n\n#create cabin columns\n# replacing missing cabins with U (for Uknown)\ncabin[ 'Cabin' ] = titanic.Cabin.fillna( 'U' )\n\n# mapping each Cabin value with the cabin letter\ncabin[ 'Cabin' ] = cabin[ 'Cabin' ].map( lambda c : c[0] )\n\n# dummy encoding ...\ncabin = pd.get_dummies( cabin['Cabin'] , prefix = 'Cabin' )\n\n#use the ticket \ndef cleanTicket( ticket ):\n    ticket = ticket.replace( '.' , '' )\n    ticket = ticket.replace( '\/' , '' )\n    ticket = ticket.split()\n    ticket = map( lambda t : t.strip() , ticket )\n    ticket = list(filter( lambda t : not t.isdigit() , ticket ))\n    if len( ticket ) > 0:\n        return ticket[0]\n    else: \n        return 'XXX'\n\nticket = pd.DataFrame()\n\n# Extracting dummy variables from tickets:\nticket[ 'Ticket' ] = titanic[ 'Ticket' ].map( cleanTicket )\nticket = pd.get_dummies( ticket[ 'Ticket' ] , prefix = 'Ticket' )\n\nfamily = pd.DataFrame()\n\n# introducing a new feature : the size of families (including the passenger)\nfamily[ 'FamilySize' ] = titanic[ 'Parch' ] + titanic[ 'SibSp' ] + 1\n\n# introducing other features based on the family size\nfamily[ 'Family_Single' ] = family[ 'FamilySize' ].map( lambda s : 1 if s == 1 else 0 )\nfamily[ 'Family_Small' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 2 <= s <= 4 else 0 )\nfamily[ 'Family_Large' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 5 <= s else 0 )\n\ntitanic = pd.concat([titanic, family, ticket, cabin, title], axis=1)\ntitanic.drop(['PassengerId','Name','Ticket', 'Cabin'],axis=1,inplace=True)\ntitanic.info()\nX_train, X_test, y_train, y_test = train_test_split(titanic.drop('Survived',axis=1), titanic['Survived'],\n                                                    train_size=0.75, test_size=0.25)\n\n","f069a6f8":"#The actual TPOT code\npipeline_optimizer = TPOTClassifier(generations=7, population_size=20, cv=5,\n                                    random_state=42, verbosity=2)\npipeline_optimizer.fit(X_train, y_train)\nprint(pipeline_optimizer.score(X_test, y_test))\npipeline_optimizer.export('tpot_exported_pipeline.py')\n","6521d92f":"#%load tpot_exported_pipeline.py","91fe8d65":"# AutoML using TPOT test\nTPOT (https:\/\/automl.info\/tpot\/) is an AutoML library that is installed by defualt on the kaggle kernel.\nIt can't do complex feature engineering, naturally, so I have added the basic ones from the tutorial.\nThe last cell in this Notebook is the code generated by my run. As TPOT uses genetic algorithms you might get different results.\n\nThe file is saved as tpot_exported_pipeline.py and you can load it by executing the magic %load at the beggining of an empty cell. \n\nNote that this kernel does not generate a submission. For that, you should process the test with the train, as specified in the tutorial, change the generated file to point at the data and run it, using the results to create a submission, again as detailed in the tutorial\n\n## Credit where credit is due: \n## The features were taken from https:\/\/www.kaggle.com\/helgejo\/an-interactive-data-science-tutorial"}}