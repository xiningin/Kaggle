{"cell_type":{"b87146ca":"code","b1a6a9a8":"code","fb05dd47":"code","59894b8a":"code","0b5ca613":"code","5f80e9ec":"code","2789576b":"code","2718af08":"code","3da0298c":"code","86f05292":"code","142b85f0":"code","3f92985b":"code","2eaf1353":"code","692421fe":"code","a8e48ccb":"code","ec0d7644":"code","ada2ec7b":"code","2db733b9":"code","490e4cd6":"code","0f4ecb34":"code","f332a549":"code","13fde583":"code","11e3a916":"code","4687f798":"code","1d05a9f5":"code","0a8521be":"code","999bca18":"markdown","661bc354":"markdown","0d626158":"markdown","89a6d415":"markdown","5ea778be":"markdown","8dcab8ea":"markdown","e26a0607":"markdown","18021088":"markdown","9c023860":"markdown","30dc4fde":"markdown","1bf247df":"markdown","438461a2":"markdown","827c6e25":"markdown","db0a299c":"markdown","425c4d98":"markdown","4caba2da":"markdown"},"source":{"b87146ca":"import pandas as pd\nimport numpy as np\nimport os\nimport string\n\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom wordcloud import WordCloud\n\nimport tensorflow as tf\nprint(tf.__version__)","b1a6a9a8":"os.listdir(\"\/kaggle\/input\")","fb05dd47":"file_name = \"\/kaggle\/input\/Names.txt\"\n\nwith open(file_name,'r') as f:\n    names = f.read().split(\"\\n\")[:-1]","59894b8a":"print(\"Number of Names: \",len(names))\nprint(\"\\nMax Length of a Name: \",max(map(len,names))-1)","0b5ca613":"fig = go.Figure(data=[go.Table(\n                header = dict(values = [\"Names\"]),\n                cells = dict(values = [[name for name in np.random.choice(names,size=15)]]))])\n    \nfig.update_layout(title = \"Randomly Chosen Names\")\n\nfig.show()","5f80e9ec":"fig = ff.create_distplot([list(map(len,names))],\n                        group_labels=[\"Length\"])\n    \nfig.update_layout(title=\"Name-Length Distribution\")\n\nfig.show()","2789576b":"#Selecting Names with length not more than 10\n\nMAX_LENGTH = 10\nnames = [name for name in names if len(name)<=MAX_LENGTH]\nprint(\"Number of Names: \",len(names))\n\nassert max(map(len,names))<=MAX_LENGTH, f\"Names length more than {MAX_LENGTH}\"","2718af08":"start_token = \" \" # so that the network knows that we're generating a first token\n\n# this is the token for padding, we will add fake pad token at the end of names \n# to make them of equal size for further batching\npad_token = \"#\"\n\n#Adding start token in front of all Names\nnames = [start_token+name for name in names]\nMAX_LENGTH += 1\n\n# set of tokens\ntokens = sorted(set(\"\".join(names + [pad_token])))\n\ntokens = list(tokens)\nn_tokens = len(tokens)\nprint(\"Tokens: \",tokens)\nprint ('n_tokens:', n_tokens)","3da0298c":"token_to_id = dict(zip(tokens,range(len(tokens))))\nprint(token_to_id)\n\ndef to_matrix(names, max_len=None, pad=token_to_id[pad_token], dtype=np.int32):\n    \"\"\"Casts a list of names into rnn-digestable padded matrix\"\"\"\n\n    max_len = max_len or max(map(len, names))\n    names_ix = np.zeros([len(names), max_len], dtype) + pad\n\n    for i in range(len(names)):\n        name_ix = list(map(token_to_id.get, names[i]))\n        names_ix[i, :len(name_ix)] = name_ix\n\n    return names_ix","86f05292":"print('\\n'.join(names[::5000]))\nprint(to_matrix(names[::5000]))","142b85f0":"X = to_matrix(names)\nX_train = np.zeros((X.shape[0],X.shape[1],n_tokens),np.int32)\ny_train = np.zeros((X.shape[0],X.shape[1],n_tokens),np.int32)\n\nfor i, name in enumerate(X):\n    for j in range(MAX_LENGTH-1):\n        X_train[i,j,name[j]] = 1\n        y_train[i,j,name[j+1]] = 1\n    X_train[i,MAX_LENGTH-1,name[MAX_LENGTH-1]] = 1\n    y_train[i,MAX_LENGTH-1,token_to_id[pad_token]] = 1","3f92985b":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(\"TPU Detected\")\n    \nexcept ValueError:\n    print(\"TPU not Detected\")\n    tpu = None\n\n# TPUStrategy for distributed training\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\nelse: # default strategy that works on CPU and single GPU\n    strategy = tf.distribute.get_strategy()","2eaf1353":"name_count = X.shape[0]\nprint(\"Names in training set: \",name_count)\n\nif tpu:\n    BATCH_SIZE = 128 * strategy.num_replicas_in_sync\nelse:\n    BATCH_SIZE = 64\n\nprint(\"Setting Batch size to: \",BATCH_SIZE)\n    \nSTEPS_PER_EPOCH = np.ceil(name_count\/BATCH_SIZE)\nprint(\"Steps per epoch: \",STEPS_PER_EPOCH)\n\n# GCS_PATH = KaggleDatasets().get_gcs_path()\n# print(\"GCS Path: \",GCS_PATH)\n\nAUTO = tf.data.experimental.AUTOTUNE\nignore_order = tf.data.Options()\nignore_order.experimental_deterministic = False","692421fe":"train_dataset = (\n    tf.data.Dataset.from_tensor_slices((X,y_train))\n    .shuffle(5000)\n    .cache()\n    .repeat()\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO))","a8e48ccb":"num_rnn_units = 256\nembedding_size = 16\n\ndef make_model():\n    model = tf.keras.models.Sequential()\n\n    model.add(tf.keras.layers.Embedding(n_tokens,embedding_size,input_length=MAX_LENGTH))\n#     model.add(tf.keras.layers.LSTM(num_rnn_units,return_sequences=True,activation='elu',input_shape=(X_train.shape[1],X_train.shape[2])))\n#     model.add(tf.keras.layers.LSTM(num_rnn_units,return_sequences=True,activation='elu'))\n#     model.add(tf.keras.layers.Dropout(0.2))\n#     model.add(tf.keras.layers.LSTM(num_rnn_units,return_sequences=True,activation='elu'))\n    model.add(tf.keras.layers.SimpleRNN(num_rnn_units,return_sequences=True,activation='elu'))\n    model.add(tf.keras.layers.SimpleRNN(num_rnn_units,return_sequences=True,activation='elu'))\n    model.add(tf.keras.layers.Dense(n_tokens,activation='softmax'))\n\n    model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001))\n\n    return model","ec0d7644":"with strategy.scope():\n    \n    model = make_model()\n    \n    model.summary()","ada2ec7b":"class CyclicLR(tf.keras.callbacks.Callback):\n    \n    def __init__(self,base_lr=1e-5,max_lr=1e-3,stepsize=10):\n        super().__init__()\n        \n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.stepsize = stepsize\n        self.iterations = 0\n        self.history = {}\n        \n    def clr(self):\n        cycle = np.floor((1+self.iterations)\/(2*self.stepsize))\n        x = np.abs(self.iterations\/self.stepsize - 2*cycle + 1)\n        \n        return self.base_lr + (self.max_lr - self.base_lr)*(np.maximum(0,1-x))\n    \n    def on_train_begin(self,logs={}):\n        tf.keras.backend.set_value(self.model.optimizer.lr, self.base_lr)\n    \n    def on_batch_end(self,batch,logs=None):\n        logs = logs or {}\n        \n        self.iterations += 1\n        \n        self.history.setdefault('lr', []).append(tf.keras.backend.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        tf.keras.backend.set_value(self.model.optimizer.lr, self.clr())\n        ","2db733b9":"def generateName(model=model,seed_phrase=start_token,max_length=MAX_LENGTH):\n    \n    assert len(seed_phrase)<max_length, f\"Length of the Seed-phrase is more than Max-Length: {max_length}\"\n    \n    name = [seed_phrase]\n    x = np.zeros((1,max_length),np.int32)\n\n    x[0,0:len(seed_phrase)] = [token_to_id[token] for token in seed_phrase]\n    \n#     x = np.zeros((1,max_length,n_tokens),np.int32)\n    \n#     for i in range(len(seed_phrase)):\n#         x[0,i,token_to_id[seed_phrase[i]]] = 1\n    \n    for i in range(len(seed_phrase),max_length):\n        \n#         x_seq = (tf.data.Dataset.from_tensor_slices(x).batch(1))        \n        \n        probs = list(model.predict(x)[0,i-1])\n        \n        probs = probs\/np.sum(probs)\n        \n        index = np.random.choice(range(n_tokens),p=probs)\n        \n        if index == token_to_id[pad_token]:\n            break\n            \n#         x[0,i,index] = 1\n        x[0,i] = index\n        \n        name.append(tokens[index])\n    \n    return \"\".join(name)\n\n\n# def generateNamesLoop(epoch,logs):\n#     if epoch%10==0:\n#         print(\"\\n--------------------------------------\")\n#         print(f\"Names generated after epoch-{epoch}:\")\n        \n#         for i in range(5):\n#             print(generateName())\n        \n#         print(\"--------------------------------------\")","490e4cd6":"%%time\n\n# printNames = tf.keras.callbacks.LambdaCallback(on_epoch_end=generateNamesLoop)\n\ncyclicLR = CyclicLR(base_lr=1e-4,max_lr=1e-3,stepsize=6000)\n\nEPOCHS = 1000\n\nhistory = model.fit(train_dataset,steps_per_epoch=STEPS_PER_EPOCH,epochs=EPOCHS,callbacks=[cyclicLR])","0f4ecb34":"fig = go.Figure()\n\nfig.add_trace(go.Scatter(x=np.arange(1,len(history.history['loss'])+1),\n                        y=history.history['loss'],\n                        mode='lines+markers',\n                        name='Training loss'))\n\nfig.update_layout(title_text=\"Training loss\")\n\nfig.show()","f332a549":"weights = '\/kaggle\/working\/IndianNames(2SimpleRNN1000).h5'\n\nmodel.save_weights(weights)","13fde583":"predictor = make_model()\n\npredictor.load_weights(weights)","11e3a916":"# When the Seed Phrase is start-token\n\nseed_phrase = \" \"\nfor _ in range(20):\n    name = generateName(predictor,seed_phrase=seed_phrase)\n    if name not in names:\n        print(f\"{name.lstrip()} (New Name)\")\n    else:\n        print(name.lstrip())","4687f798":"# When seed-phrase is a single Alphabet\n\nseed_phrase = f\" {np.random.choice(list(string.ascii_uppercase))}\"\nfor _ in range(20):\n    name = generateName(predictor,seed_phrase=seed_phrase)\n    if name not in names:\n        print(f\"{name.lstrip()} (New Name)\")\n    else:\n        print(name.lstrip())","1d05a9f5":"# When seed-phrase is some combination of Alphabets\n\nseed_phrase = f\" {np.random.choice(list(string.ascii_uppercase))}{np.random.choice(list(string.ascii_lowercase))}\"\nfor _ in range(20):\n    name = generateName(predictor,seed_phrase=seed_phrase)\n    if name not in names:\n        print(f\"{name.lstrip()} (New Name)\")\n    else:\n        print(name.lstrip())","0a8521be":"new_names = []\n\nwhile len(new_names) is not 200:\n    name = generateName(predictor)\n    if name not in names:\n        new_names.append(name.lstrip())\n\nwordcloud = WordCloud(background_color=\"white\",height=400,width=1900).generate(\" \".join(new_names))\n\nfig, ax = plt.subplots(figsize=(20, 10))\nax.imshow(wordcloud, interpolation='bilinear',aspect='auto')\nax.axis(\"off\")\nplt.show()","999bca18":"### Saving the weights","661bc354":"### Cast everything from symbols into identifiers \nTensorflow string manipulation is a bit tricky, so we'll work around it. We'll feed our recurrent neural network with ids of characters from our dictionary.\n\nTo create such dictionary, let's assign token_to_id","0d626158":"### Model training","89a6d415":"## Generating Names using Recurrent Neural Networks ","5ea778be":"## Introduction\n![What is your Name?](https:\/\/media.giphy.com\/media\/TayI4SCiq0dJ6\/giphy.gif)\n\n\nStruggle to find a name for the variable?  \nLet's see how you'll come up with a name for your **son\/daughter**. Surely no human has expertize over what is a good child name, so let us train **RNN** instead \ud83d\ude01\n\n**About the Dataset**  \nI scraped the Indian-Baby-Names from this [website](https:\/\/babynames.extraprepare.com). I used python for web-scraping. The code for scraping is available [here](https:\/\/github.com\/memr5\/Machine-Learning-Portfolio\/blob\/master\/Deep%20Learning\/Indian%20Baby%20Names%20Generator\/Scraper.py).  \nThe Dataset contains names of both boys & girls.\n\n<font color=\"red\" size=5>Please!!! Upvote this kernel if you find it useful.<\/font>","8dcab8ea":"### RNN Model","e26a0607":"## Load Data\nThis notebook has been designed so as to allow you to quickly swap names for something similar: deep learning article titles, IKEA furniture, pokemon names, etc.","18021088":"## Text Processing\nFirst we need to collect a \"vocabulary\" of all unique tokens i.e. unique characters. We can then encode inputs as a sequence of character ids.","9c023860":"## Key Takeaways\n* Having this much amount of names to train the model will consume **more time** if one uses **CPU** but using **TPU** for the training the model will consume **less time**.\n* One can train for more **longer name length** to see the results.\n* We can add **more layers of RNNs** to see how the results differ.","30dc4fde":"## Defining a recurrent neural network\n\n![RNN](https:\/\/miro.medium.com\/max\/1400\/1*v9gT-OV_xOnrR5n8wMmqyQ.png)\n\nA Recurrent Neural Network uses outputs from the previous states with the current inputs to produce the output. That i why RNNs are used for the sequence data.\n\nSince we're training a language model, there should also be:\n* An embedding layer that converts character id x_t to a vector.\n* An output layer that predicts probabilities of next phoneme based on h_t+1","1bf247df":"### Function to Generate Names \n\nWe will generate names character by character starting with `start_token`:\n\n<img src=\"https:\/\/github.com\/hse-aml\/intro-to-dl\/blob\/master\/week5\/char-nn.png?raw=1\" width=600>","438461a2":"<font color=\"red\" size=5>Please!!! Upvote this kernel if you find it useful.<\/font>","827c6e25":"## Importing necessary libraries","db0a299c":"## Exploring the Data","425c4d98":"### Word Cloud of New Names","4caba2da":"## Results"}}