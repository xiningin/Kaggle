{"cell_type":{"a06745c6":"code","4c1d1d5c":"code","931aacee":"code","e6707dd0":"code","d8201d15":"code","80d7b383":"code","7b76ddcd":"code","0871352b":"code","a9b13f2f":"code","cb8e9b7b":"code","d779a2ea":"code","51234997":"code","b422061b":"code","67e110c4":"code","7fcc1fac":"code","53f142ad":"code","621e626c":"code","8266c3e1":"code","ffd611f0":"code","b7084f65":"code","36a1a514":"code","023a9861":"code","58a00f48":"code","a660e7d7":"code","75654be0":"code","ef05047a":"code","898fbf68":"code","ffb06f71":"code","209ac5e0":"code","7a54e902":"code","1b2ddcf1":"code","c8c31892":"code","977b917d":"code","d7354b52":"code","f55f2660":"code","8f48e2f8":"code","2c552b2b":"code","ad66e57c":"code","7664b190":"code","70a39bd1":"code","de7b9552":"code","5da1807e":"code","ac086799":"code","9e3b0937":"code","31dc808d":"code","98c1bea4":"code","7e1f0fd7":"code","6bec80cf":"code","4f8ea31a":"code","14b7d0b0":"code","ecb04ba9":"code","2b7b45e2":"code","86df36ab":"code","fda07128":"code","cc9356c1":"code","26e656cd":"code","3908b50c":"code","7c051305":"code","ae82857d":"code","489302ac":"code","fb203aaf":"code","f044ff7d":"code","99356bcd":"code","7517c609":"code","851a8204":"code","d4e68a01":"code","c0ecf12e":"code","8f63149d":"code","0ac5a602":"code","9942b2a0":"code","398f8259":"code","43c9d661":"code","636be34f":"code","62b30269":"code","5e1366e2":"code","1b1c9dad":"code","63ecb71d":"code","3f7b35b8":"code","9bf1c3c0":"code","910ec9e2":"code","65f071aa":"markdown","3ad5634f":"markdown","073e23aa":"markdown","8d0acf75":"markdown","765f526c":"markdown","9836ba7c":"markdown","8977eebf":"markdown","851701b8":"markdown","ed421d1d":"markdown","6b878614":"markdown","c330cae8":"markdown","4a9fa73b":"markdown","5e9711ab":"markdown","f7b1e977":"markdown","9bc8a0e8":"markdown","11fed634":"markdown","ba6197a1":"markdown","c262cf20":"markdown","c7c15c26":"markdown","30d2fe65":"markdown","591368a8":"markdown","9ab8077a":"markdown","f308740d":"markdown","3af4be0f":"markdown","cdc59b49":"markdown","5183bf6c":"markdown","eef10329":"markdown","48d49ef3":"markdown","baaf75a5":"markdown"},"source":{"a06745c6":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport glob\nfrom datetime import datetime\nimport random\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\nOUTPUT_DIR = '.\/'","4c1d1d5c":"train = pd.read_csv('..\/input\/shigglecup-1st\/DATA\/train.csv')\nprint(train.shape)\ntrain.head()","931aacee":"test = pd.read_csv('..\/input\/shigglecup-1st\/DATA\/test.csv')\nprint(test.shape)\ntest.head()","e6707dd0":"%matplotlib inline\nfrom matplotlib_venn import venn2\nfrom matplotlib import pyplot\nvenn2(subsets=(set(train['pokemon'].unique()), set(test['pokemon'].unique())),\n      set_labels=('Train', 'Test'))\npyplot.show()","d8201d15":"%matplotlib inline\nfrom matplotlib_venn import venn2\nfrom matplotlib import pyplot\nvenn2(subsets=(set(train['species_id'].unique()), set(test['species_id'].unique())),\n      set_labels=('Train', 'Test'))\npyplot.show()","80d7b383":"%matplotlib inline\nfrom matplotlib_venn import venn2\nfrom matplotlib import pyplot\nvenn2(subsets=(set(train['type_2'].unique()), set(test['type_2'].unique())),\n      set_labels=('Train', 'Test'))\npyplot.show()","7b76ddcd":"train.sort_values('target', ascending=False)","0871352b":"train.sort_values('target', ascending=False)[['pokemon','attack', 'defense', 'hp',\n                                              'special_attack', 'special_defense', 'speed']]","a9b13f2f":"train['target'] = train['target'].apply(lambda x: np.log1p(x))\ntrain['target'].hist()","cb8e9b7b":"# \u30bf\u30d6\u30f3\u30cd\u9664\u5916\n# train = train[train['id']!=2]\nprint(train.shape)","d779a2ea":"train['nan_count'] = train.isnull().sum(axis=1)\ntest['nan_count'] = test.isnull().sum(axis=1)\nprint(train.shape, test.shape)","51234997":"# train['clean_pokemon'] = train['pokemon'].str.replace('-x', '').str.replace('-y', '').str.replace('-mega', '')\ntrain['clean_pokemon'] = train['pokemon'].str.replace('-.*', '')\ntrain['in_hyphen'] = np.where(train['pokemon'].str.contains('-'), 1, 0)\ntrain['in_mega'] = np.where(train['pokemon'].str.contains('mega'), 1, 0)\n# train['name_len'] = train['clean_pokemon'].str.len()\ntest['clean_pokemon'] = test['pokemon'].str.replace('-.*', '')\ntest['in_hyphen'] = np.where(test['pokemon'].str.contains('-'), 1, 0)\ntest['in_mega'] = np.where(test['pokemon'].str.contains('mega'), 1, 0)\n# test['name_len'] = test['clean_pokemon'].str.len()\nprint(train.shape, test.shape)","b422061b":"train['weight_per_height'] = train['weight'] \/ train['height']\ntest['weight_per_height'] = test['weight']\/test['height']\nprint(train.shape, test.shape)","67e110c4":"train['type_count'] = np.where(train['type_2'].isnull(), 1, 2)\ntest['type_count'] = np.where(test['type_2'].isnull(), 1, 2)\nprint(train.shape, test.shape)","7fcc1fac":"train['types'] = train['type_1'] + '_' + train['type_2'].astype(str)\ntest['types'] = test['type_1'] + '_' + test['type_2'].astype(str)\nprint(train.shape, test.shape)","53f142ad":"stats = ['attack', 'defense', 'hp', 'special_attack', 'special_defense', 'speed']\nstats_attack = ['attack', 'special_attack','speed']\nstats_defense = ['defense', 'special_defense','hp']","621e626c":"train['stats_sum'] = train[stats].sum(axis=1)\ntrain['stats_attack_sum'] = train[stats_attack].sum(axis=1)\ntrain['stats_defence_sum'] = train[stats_defense].sum(axis=1)\ntrain['stats_att_per_def'] = train['stats_attack_sum'] \/ train['stats_defence_sum']\ntest['stats_sum'] = test[stats].sum(axis=1)\ntest['stats_attack_sum'] = test[stats_attack].sum(axis=1)\ntest['stats_defence_sum'] = test[stats_defense].sum(axis=1)\ntest['stats_att_per_def'] = test['stats_attack_sum'] \/ test['stats_defence_sum']\nprint(train.shape, test.shape)","8266c3e1":"# import itertools\n# for col1, col2 in list(itertools.combinations(stats, 2)):\n#     train[f'{col1}_{col2}_sum'] = train[col1] + train[col2]\n#     train[f'{col1}_{col2}_diff'] = train[col1] - train[col2]\n#     train[f'{col1}_{col2}_div'] = train[col1] \/ train[col2]\n#     test[f'{col1}_{col2}_sum'] = test[col1] + test[col2]\n#     test[f'{col1}_{col2}_diff'] = test[col1] - test[col2]\n#     test[f'{col1}_{col2}_div'] = test[col1] \/ test[col2]\n# print(train.shape, test.shape)","ffd611f0":"train['abt_count'] = np.where(train['ability_2'].isnull(), 1, 2)\ntest['abt_count'] = np.where(test['ability_2'].isnull(), 1, 2)\nprint(train.shape, test.shape)","b7084f65":"train['egg_count'] = np.where(train['egg_group_2'].isnull(), 1, 2)\ntrain['no_egg_flag'] = np.where(train['egg_group_1']=='no-eggs', 1, 0)\ntest['egg_count'] = np.where(test['egg_group_2'].isnull(), 1, 2)\ntest['no_egg_flag'] = np.where(test['egg_group_1']=='no-eggs', 1, 0)\nprint(train.shape, test.shape)","36a1a514":"# train['evolved_flag'] = np.where(train['evolves_from_species_id'].isnull(), 0, 1)\n# test['evolved_flag'] = np.where(test['evolves_from_species_id'].isnull(), 0, 1)\n# print(train.shape, test.shape)","023a9861":"evo_from_list = list(train[~train['evolves_from_species_id'].isnull()]['evolves_from_species_id'].unique())\nevo_from_list = evo_from_list + list(test[~test['evolves_from_species_id'].isnull()]['evolves_from_species_id'].unique())\nevo_from_list = list(set(evo_from_list))\nprint(len(evo_from_list))\nevo_df = train[train['species_id'].isin(evo_from_list)][['species_id','target']].rename(columns={'species_id':'evolves_from_species_id', 'target':'evo_from_target'})\nevo_df = evo_df.drop_duplicates()\ntrain = pd.merge(train, evo_df, on='evolves_from_species_id', how='left')\ntest = pd.merge(test, evo_df, on='evolves_from_species_id', how='left')\nprint(train.shape, test.shape)","58a00f48":"train['flag'] = 1\ntest['flag'] = 2\nwhole_df = pd.concat([train, test])\nwhole_df.shape","a660e7d7":"# evo_df = pd.DataFrame()\n# for chain_id in whole_df[~whole_df['evolution_chain_id'].isnull()]['evolution_chain_id'].unique():\n# # for chain_id in whole_df[whole_df['evolution_chain_id']==51]['evolution_chain_id'].unique():\n#     evo_ids = whole_df[whole_df['evolution_chain_id']==chain_id]['evolves_from_species_id'].fillna(-1).unique()\n#     evo_ids.sort()\n#     _df = whole_df[whole_df['evolution_chain_id']==chain_id].sort_values('species_id')[['species_id', 'evolves_from_species_id']].fillna(-1).reset_index(drop=True)\n#     for i, ids in enumerate(evo_ids):\n#         _df.loc[_df['evolves_from_species_id']==ids,  'evo_point'] = (i+1) \/ len(evo_ids)\n#         _df.loc[_df['evolves_from_species_id']==ids,  'evo_flag'] = len(evo_ids)\n#     evo_df = pd.concat([evo_df, _df])","75654be0":"evo_df = pd.DataFrame()\nfor chain_id in whole_df[~whole_df['evolution_chain_id'].isnull()]['evolution_chain_id'].unique():\n# for chain_id in whole_df[whole_df['evolution_chain_id']==67]['evolution_chain_id'].unique():\n# for chain_id in whole_df[whole_df['evolution_chain_id']==51]['evolution_chain_id'].unique():\n# for chain_id in whole_df[whole_df['evolution_chain_id']==47]['evolution_chain_id'].unique():\n# for chain_id in whole_df[whole_df['evolution_chain_id']==144]['evolution_chain_id'].unique():\n    _df = whole_df[whole_df['evolution_chain_id']==chain_id].sort_values(['stats_sum'])[['species_id', 'stats_sum', 'evolves_from_species_id']].fillna(-1).reset_index(drop=True)\n    evo_ids = list(_df['evolves_from_species_id'].unique())\n    if evo_ids[0] != -1:\n        n = evo_ids.index(-1)\n        evo_ids[0], evo_ids[n] = evo_ids[n], evo_ids[0]  \n    for i, ids in enumerate(evo_ids):\n        _df.loc[_df['evolves_from_species_id']==ids, 'evo_point'] = (i+1) \/ len(evo_ids)\n        _df.loc[_df['evolves_from_species_id']==ids, 'evo_flag'] = len(evo_ids)\n    evo_df = pd.concat([evo_df, _df])","ef05047a":"evo_df = evo_df.drop_duplicates()\nwhole_df = pd.merge(whole_df, evo_df[['species_id', 'evo_point', 'evo_flag']], on='species_id', how='left')\nwhole_df['evo_point'] = whole_df['evo_point'].fillna(1)\nwhole_df['evo_flag'] = whole_df['evo_flag'].fillna(1)\nwhole_df.shape","898fbf68":"rate_col = stats+['stats_sum', 'stats_attack_sum', 'stats_defence_sum']\nfor col in rate_col:\n    whole_df[col] = whole_df[col] \/ whole_df[col].max()\nprint(whole_df.shape)","ffb06f71":"whole_df['weight_height_sum'] =( whole_df['weight'] \/ whole_df['weight'].max()) + (whole_df['height'] \/ whole_df['height'].max())\nwhole_df['weight_height_sum'] =( whole_df['weight'] \/ whole_df['weight'].max()) + (whole_df['height'] \/ whole_df['height'].max())\nprint(whole_df.shape)","209ac5e0":"name_df = pd.DataFrame()\n_df = pd.DataFrame(whole_df['clean_pokemon'].value_counts())\nnames = _df[_df['clean_pokemon']>=2].reset_index()['index'].unique()\nfor p in list(names):\n    _df = whole_df[whole_df['clean_pokemon']==p].copy()\n    _df['same_pokemon_target_mean'] = _df['target'].mean()\n    name_df = pd.concat([name_df, _df])\nname_df = name_df[['pokemon', 'same_pokemon_target_mean']]\nname_df","7a54e902":"whole_df = pd.merge(whole_df, name_df, on='pokemon', how='left')\nwhole_df['same_pokemon_target_mean'] = whole_df['same_pokemon_target_mean'].fillna(-1)\nprint(whole_df.shape)","1b2ddcf1":"has_hyphen = whole_df[whole_df['pokemon'].str.contains('-')]['pokemon'].str.replace('-.*', '')\nwhole_df[whole_df['clean_pokemon'].isin(has_hyphen)]\nwhole_df['has_hyphen'] = np.where(whole_df['clean_pokemon'].isin(has_hyphen), 1, 0)\nprint(whole_df.shape)","c8c31892":"has_max = []\nfor col in stats:\n    has_max.append(whole_df.loc[whole_df[col]==whole_df[col].max(), 'pokemon'].reset_index(drop=True)[0])\nwhole_df['has_max'] = np.where(whole_df['pokemon'].isin(has_max), 1, 0)\nprint(whole_df.shape)","977b917d":"from category_encoders import CountEncoder, OrdinalEncoder\n\ndef Categorical_Encoder(df:pd.DataFrame, col_list:list, ENCODER:str, target_col:str) -> pd.DataFrame:\n    \n    '''\n    \u7279\u5b9a\u306e\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u306a\u5217\u306b\u5bfe\u3057\u3066\u3001\n    Count or Ordinal or CountRank\u306e\u3044\u305a\u308c\u304b\u306e\u30a8\u30f3\u30b3\u30fc\u30c9\u3092\u51e6\u7406\n\n\n    Parameters\n    ----------\n    df: pd.DataFrame\n        \u5bfe\u8c61DataFrame\n    col_list:list\n        \u5909\u63db\u3057\u305f\u3044\u5217\u306e\u30ea\u30b9\u30c8\n    ENCODER:str\n        \u5909\u63db\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u540d\n        CountEncoder(), OrdinalEncoder(), CountRankEncoder()\n    target_col:str\n        CountRankEncoder()\u3092\u884c\u3046\u969b\u306b\u3001Rank\u3092\u8a08\u7b97\u3059\u308b\u5bfe\u8c61\u306e\u30ab\u30e9\u30e0\n\n    Returns\n    -------\n    df.copy(): pd.DataFrame\n        encode\u5217\u304c\u8ffd\u52a0\u3055\u308c\u305fDataFrame\n\n    '''\n\n    for col in col_list:\n        \n        if ENCODER == 'CountEncoder()':\n            print(f'Encoder : {ENCODER} | Encode Col : {col}')\n            encoder = CountEncoder()\n            enc = encoder.fit(df[col])\n            df[f'{str(ENCODER)[:-2]}_{col}'] = enc.transform(df[col])\n\n        elif ENCODER == 'OrdinalEncoder()':\n            print(f'Encoder : {ENCODER} | Encode Col : {col}')\n            encoder = OrdinalEncoder()\n            enc = encoder.fit(df[col])\n            df[f'{str(ENCODER)[:-2]}_{col}'] = enc.transform(df[col])\n\n        elif ENCODER == 'CountRankEncoder()':\n            print(f'Encoder : {ENCODER} | Encode Col : {col} | Target Col : {target_col}')\n            count_rank = df.groupby(col)[target_col].count().rank(ascending=False)\n            df[f'{ENCODER[:-2]}_{col}'] = df[col].map(count_rank)\n        else:\n            break\n\n    return df.copy()","d7354b52":"cat_cols = ['clean_pokemon', 'species_id', 'type_1', 'type_2','types', \n            'ability_1', 'ability_2', 'ability_hidden', 'egg_group_1','egg_group_2','shape', \n            'evolution_chain_id', 'generation_id','evolves_from_species_id','shape_id',]","f55f2660":"whole_df = Categorical_Encoder(whole_df, cat_cols, 'CountEncoder()', 'id')\nwhole_df = Categorical_Encoder(whole_df, cat_cols, 'OrdinalEncoder()', 'id')\nwhole_df = Categorical_Encoder(whole_df, cat_cols, 'CountRankEncoder()', 'id')\nwhole_df.shape","8f48e2f8":"_df = pd.DataFrame(whole_df.groupby('types')['stats_sum'].mean()).reset_index()\n_df = _df.rename(columns={'stats_sum':'stats_sum_mean'})\nwhole_df = pd.merge(whole_df, _df, on='types', how='left')\nwhole_df.shape","2c552b2b":"_df = pd.DataFrame(whole_df.groupby('types')['stats_sum'].max()).reset_index()\n_df = _df.rename(columns={'stats_sum':'stats_sum_max'})\nwhole_df = pd.merge(whole_df, _df, on='types', how='left')\nwhole_df.shape","ad66e57c":"_df = pd.DataFrame(whole_df.groupby('types')['stats_sum'].min()).reset_index()\n_df = _df.rename(columns={'stats_sum':'stats_sum_min'})\nwhole_df = pd.merge(whole_df, _df, on='types', how='left')\nwhole_df.shape","7664b190":"_df = pd.DataFrame(whole_df.groupby('types')['stats_attack_sum'].mean()).reset_index()\n_df = _df.rename(columns={'stats_attack_sum':'stats_attack_sum_mean'})\nwhole_df = pd.merge(whole_df, _df, on='types', how='left')\nwhole_df.shape","70a39bd1":"_df = pd.DataFrame(whole_df.groupby('types')['stats_attack_sum'].max()).reset_index()\n_df = _df.rename(columns={'stats_attack_sum':'stats_attack_sum_max'})\nwhole_df = pd.merge(whole_df, _df, on='types', how='left')\nwhole_df.shape","de7b9552":"_df = pd.DataFrame(whole_df.groupby('types')['stats_attack_sum'].min()).reset_index()\n_df = _df.rename(columns={'stats_attack_sum':'stats_attack_sum_min'})\nwhole_df = pd.merge(whole_df, _df, on='types', how='left')\nwhole_df.shape","5da1807e":"_df = pd.DataFrame(whole_df.groupby('types')['stats_defence_sum'].mean()).reset_index()\n_df = _df.rename(columns={'stats_defence_sum':'stats_defence_sum_mean'})\nwhole_df = pd.merge(whole_df, _df, on='types', how='left')\nwhole_df.shape","ac086799":"_df = pd.DataFrame(whole_df.groupby('types')['stats_defence_sum'].max()).reset_index()\n_df = _df.rename(columns={'stats_defence_sum':'stats_defence_sum_max'})\nwhole_df = pd.merge(whole_df, _df, on='types', how='left')\nwhole_df.shape","9e3b0937":"_df = pd.DataFrame(whole_df.groupby('types')['stats_defence_sum'].min()).reset_index()\n_df = _df.rename(columns={'stats_defence_sum':'stats_defence_sum_min'})\nwhole_df = pd.merge(whole_df, _df, on='types', how='left')\nwhole_df.shape","31dc808d":"# audino\nwhole_df['audino'] = np.where(whole_df['clean_pokemon'].str.contains('audino'), 1, 0)\nwhole_df.shape","98c1bea4":"train = whole_df[whole_df['flag']==1]\ntest = whole_df[whole_df['flag']==2]\nprint(train.shape, test.shape)","7e1f0fd7":"!pip install pytorch-tabnet -q","6bec80cf":"from sklearn import preprocessing, model_selection\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom sklearn.metrics import mean_squared_error\nimport pickle\n\nimport torch\nimport torch.nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n## pytorch_tabnet\u304c\u306a\u3051\u308c\u3070\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom pytorch_tabnet.pretraining import TabNetPretrainer\n\nimport optuna.integration.lightgbm as opt_lgb","4f8ea31a":"class OPT_CONFIG:\n    params = dict(\n        objective = 'root_mean_squared_error',\n        metric = 'rmse',\n        random_state = 42,\n        force_col_wise = True,\n        verbosity = 0,\n    ),\n    TIME = 600 # \u30d1\u30e9\u30e1\u30fc\u30bf\u30b5\u30fc\u30c1\u3059\u308b\u6642\u9593(s)\n    NUM = 1000 # \u5404\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u306e\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u6570\n    EARLY = 10 # early stopping\u306e\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u6570","14b7d0b0":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    def __init__(self):\n        self.target_col='target'\n        self.seed=42\n        self.n_fold=7 # 6\n        self.trn_fold=[0,1,2,3,4,5,6] # [0,1,2,3,4,5,6]\n#         self.train=True\n        self.EPOCHS=300 # tabnet\n        self.PATIENCE=20 # tabnet\n        self.BATCH_SIZE=32 # tabnet\n        self.V_BATCH=32 # tabnet\n        self.WORKERS=2 # tabnet\n        self.LOSS='rmse' # tabnet\nCONFIG = CFG()","ecb04ba9":"# Fold = KFold(n_splits=CONFIG.n_fold, shuffle=True, random_state=CONFIG.seed)\n# for n, (train_index, val_index) in enumerate(Fold.split(train)):\n#     train.loc[val_index, 'fold'] = int(n)\n# train['fold'] = train['fold'].astype(int)\n# train.groupby('fold').size()","2b7b45e2":"train['target_group'] = pd.qcut(train['target'], CONFIG.n_fold, labels=range(CONFIG.n_fold)).astype(int)\nFold = StratifiedKFold(n_splits=CONFIG.n_fold, shuffle=True, random_state=CONFIG.seed)\nfor n, (train_index, val_index) in enumerate(Fold.split(train, train['target_group'])):\n    train.loc[val_index, 'fold'] = int(n)\ntrain['fold'] = train['fold'].astype(int)\ntrain.groupby(['fold', 'target_group']).size()","86df36ab":"def target_encoding(train: pd.DataFrame(),test: pd.DataFrame(), target: str, target_enc_list: list, fold_num: int):\n    '''\n    fold\u6bce\u306btarget encoding\u3092\u8a08\u7b97\u3059\u308b\u3002test\u306ftraining\u5168\u4f53\u3067\u8a08\u7b97\u3059\u308b\n\n\t\tParameters\n\t\t----------\n\t\ttrain: pd.DataFrame\n\t\t\ttrain data\n\t\ttest:pd.DataFrame\n\t\t\ttest data\n\t\ttarget:str\n\t\t\ttarget encoding\u3092\u8a08\u7b97\u3059\u308b\u76ee\u7684\u5909\u6570\n\t\ttarget_enc_list:list\n\t\t\ttarget encoding\u3092\u8a08\u7b97\u3059\u308b\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u306e\u30ea\u30b9\u30c8\n\t\tfold_num:int\n\t\t\tfold\u6570\n\n\t\tReturns\n\t\t-------\n\t\ttrain_out_df, test_out_df: pd.DataFrame\n\t\t\t\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u6bce\u306etarget encoding\u5217\u304c\u8ffd\u52a0\u3055\u308c\u305ftrain\u3068test DataFrame\n    '''\n\n    train_out_df = pd.DataFrame()\n    test_out_df = pd.DataFrame()\n\n    for c in target_enc_list:\n        print(f'encode col : {c}')\n        # target\u3092\u4ed8\u52a0\n        data_tmp = pd.DataFrame({c: train[c], 'target': train[target]})\n        # \u5909\u63db\u5f8c\u306e\u5024\u3092\u683c\u7d0d\u3059\u308b\u914d\u5217\u3092\u6e96\u5099\n        tmp = np.repeat(np.nan, train.shape[0])\n\n        ###test\n        # test\u306ftrain\u5168\u4f53\u3067\u306etarget mean\u3092\u4ed8\u4e0e\n        target_mean = data_tmp.groupby(c)['target'].mean()\n        test[c+'_'+target+'_target_enc'] = test[c].map(target_mean)\n        test[c+'_'+target+'_target_enc'] = test[c+'_'+target+'_target_enc'].astype('float')\n        test_out_df = pd.concat([test_out_df, test[c+'_'+target+'_target_enc']], axis=1)\n\n        ###train\n        # \u5b66\u7fd2\u30c7\u30fc\u30bf\u304b\u3089\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u3092\u5206\u3051\u308b\n        for fold in range(fold_num):\n            tr_idx = train[train.fold != fold].index\n            va_idx = train[train.fold == fold].index\n\n            # \u5b66\u7fd2\u30c7\u30fc\u30bf\u306b\u3064\u3044\u3066\u3001\u5404\u30ab\u30c6\u30b4\u30ea\u306b\u304a\u3051\u308b\u76ee\u7684\u5909\u6570\u306e\u5e73\u5747\u3092\u8a08\u7b97\n            target_mean = data_tmp.iloc[tr_idx].groupby(c)['target'].mean()\n\n            # \u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u306b\u3064\u3044\u3066\u3001\u5909\u63db\u5f8c\u306e\u5024\u3092\u4e00\u6642\u914d\u5217\u306b\u683c\u7d0d\n            tmp[va_idx] = train[c].iloc[va_idx].map(target_mean)\n\n        # \u5909\u63db\u5f8c\u306e\u30c7\u30fc\u30bf\u3067\u5143\u306e\u5909\u6570\u3092\u7f6e\u63db\n        train[c+'_'+target+'_target_enc'] = tmp\n        train[c+'_'+target+'_target_enc'] = train[c+'_'+target+'_target_enc'].astype('float')\n        train_out_df = pd.concat([train_out_df, train[c+'_'+target+'_target_enc']], axis=1)\n    \n    return train_out_df, test_out_df","fda07128":"te_list = ['OrdinalEncoder_types', 'OrdinalEncoder_ability_1']\n_train, _test = target_encoding(train ,test, CONFIG.target_col, te_list, CONFIG.n_fold)\nprint(train.shape, test.shape)","cc9356c1":"# ====================================================\n# Utils\n# ====================================================\ndef init_logger(log_file=OUTPUT_DIR+'train.log'):\n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = init_logger()\n\ndef seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_torch(seed=CONFIG.seed)","26e656cd":"# ====================================================\n# Helper Functions\n# ====================================================\ndef visualize_importance(models, feat_train_df):\n    \"\"\"lightGBM \u306e model \u914d\u5217\u306e feature importance \u3092 plot \u3059\u308b\n    CV\u3054\u3068\u306e\u30d6\u30ec\u3092 boxen plot \u3068\u3057\u3066\u8868\u73fe\u3057\u307e\u3059.\n\n    args:\n        models:\n            List of lightGBM models\n        feat_train_df:\n            \u5b66\u7fd2\u6642\u306b\u4f7f\u3063\u305f DataFrame\n    \"\"\"\n    feature_importance_df = pd.DataFrame()\n    for i, model in enumerate(models):\n        _df = pd.DataFrame()\n        _df['feature_importance'] = model.feature_importance(importance_type='gain')\n        _df['column'] = feat_train_df.columns\n        _df['fold'] = i + 1\n        feature_importance_df = pd.concat([feature_importance_df, _df], axis=0, ignore_index=True)\n\n    order = feature_importance_df.groupby('column')\\\n        .sum()[['feature_importance']]\\\n        .sort_values('feature_importance', ascending=False).index[:50]\n\n    fig, ax = plt.subplots(figsize=(max(6, len(order) * .4), 7))\n    sns.boxenplot(data=feature_importance_df, x='column', y='feature_importance', order=order, ax=ax, palette='viridis')\n    ax.tick_params(axis='x', rotation=90)\n    ax.grid()\n    fig.tight_layout()\n    return fig, ax","3908b50c":"params_lgbm = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'learning_rate': 0.01,\n        'objective': 'rmse',\n        'metric': 'rmse',\n        'max_depth': -1,\n        'n_jobs': -1,\n        'feature_fraction': 0.7,\n        'bagging_fraction': 0.7,\n        'lambda_l2': 1,\n        'verbose': -1, # 0\n        'random_state': CONFIG.seed,\n        #'bagging_freq': 5\n}","7c051305":"# cats = ['generation_id','evolves_from_species_id','shape_id',]\ncats = ['in_hyphen', 'in_mega', 'type_count', 'abt_count', 'egg_count', 'no_egg_flag', 'evo_flag',\n        'OrdinalEncoder_clean_pokemon', 'OrdinalEncoder_species_id', 'OrdinalEncoder_type_1',\n        'OrdinalEncoder_type_2', 'OrdinalEncoder_types', 'OrdinalEncoder_ability_1',\n        'OrdinalEncoder_ability_2', 'OrdinalEncoder_ability_hidden', 'OrdinalEncoder_egg_group_1',\n        'OrdinalEncoder_egg_group_2', 'OrdinalEncoder_shape', 'OrdinalEncoder_evolution_chain_id', \n        'OrdinalEncoder_generation_id', 'OrdinalEncoder_evolves_from_species_id', 'OrdinalEncoder_shape_id', 'has_max']\ndel_col = ['id', 'pokemon', 'species_id','target','type_1','type_2', 'types', 'ability_1', 'ability_2',\n           'ability_hidden', 'egg_group_1','egg_group_2','shape', 'fold', 'pred_lgb', 'color_1',\n           'color_2', 'color_f', 'url_image', 'image_exist', 'flag', 'clean_pokemon', 'pred_cat',\n           'evolution_chain_id', 'target_group', 'pred_tab','generation_id', 'evolves_from_species_id', 'shape_id']\nfeature_col = [col for col in train.columns if col not in del_col]\nprint('We consider {} features'.format(len(feature_col)))\n\ntrain['pred_lgb'] = 0\ntrain['pred_cat'] = 0","ae82857d":"# \u6b20\u640d\u5024\u51e6\u7406\nwhole_df = pd.concat([train, test])\nfor col in ['evo_from_target', 'CountEncoder_evolution_chain_id', 'OrdinalEncoder_evolution_chain_id',\n            'CountRankEncoder_type_2', 'CountRankEncoder_ability_2', 'CountRankEncoder_ability_hidden', \n            'CountRankEncoder_shape', 'CountRankEncoder_evolution_chain_id', 'CountEncoder_generation_id',\n            'CountEncoder_evolves_from_species_id', 'CountEncoder_shape_id', 'OrdinalEncoder_generation_id',\n            'OrdinalEncoder_evolves_from_species_id', 'OrdinalEncoder_shape_id', 'CountRankEncoder_egg_group_2',\n            'CountRankEncoder_generation_id', 'CountRankEncoder_evolves_from_species_id', 'CountRankEncoder_shape_id']:\n    whole_df[col] = whole_df[col].fillna(-1)\nfor col in ['OrdinalEncoder_types_target_target_enc', 'OrdinalEncoder_ability_1_target_target_enc']:\n    whole_df[col] = whole_df[col].fillna(whole_df[col].mean())\n    whole_df[col] = whole_df[col].fillna(whole_df[col].mean())\ntrain = whole_df[whole_df['flag']==1]\ntest = whole_df[whole_df['flag']==2]\nprint(train.shape, test.shape)\ntrain[feature_col].isnull().sum()","489302ac":"# # ====================================================\n# # Train loop\n# # ====================================================\n# def train_loop(folds, fold):\n#     LOGGER.info(f\"========== fold: {fold} training ==========\")\n#     print(datetime.now())\n    \n#     trn_idx = folds[folds['fold'] != fold].index\n#     val_idx = folds[folds['fold'] == fold].index\n    \n#     X_train = folds[feature_col].loc[trn_idx].reset_index(drop=True)\n#     y_train = folds[CONFIG.target_col].loc[trn_idx].reset_index(drop=True).values\n#     X_val = folds[feature_col].loc[val_idx].reset_index(drop=True)\n#     y_val = folds[CONFIG.target_col].loc[val_idx].reset_index(drop=True).values\n#     print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n#     ##############################################################\n#     # LGB\n#     ##############################################################\n#     train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cats, weight=1\/np.power(y_train,2))\n#     val_data = lgb.Dataset(X_val, label=y_val, categorical_feature=cats, weight=1\/np.power(y_val,2))\n#     model_lgb = lgb.train(params_lgbm, \n#                       train_data, \n#                       5000, \n#                       valid_sets=val_data, \n#                       verbose_eval= 1000,\n#                       early_stopping_rounds=500\n#                      )    \n#     preds = model_lgb.predict(X_val)\n#     train.loc[val_idx, 'pred_lgb'] = preds\n#     score_lgb = mean_squared_error(y_val, preds) ** .5\n#     print('Fold {} {}: {}'.format(fold+1, 'lgb', score_lgb))\n#     LOGGER.info('Fold {} {}: {}'.format(fold+1, 'lgb', score_lgb))\n#     # save model\n#     pickle.dump(model_lgb, open(f'lgb_model_{fold+1}.pkl', 'wb'))\n    \n#     return model_lgb, score_lgb","fb203aaf":"# scores_folds = {}\n# scores_folds['lgb'] = []\n# models = []\n# for fold in range(CONFIG.n_fold):\n#     if fold in CONFIG.trn_fold:\n#         model_lgb, score_lgb = train_loop(train, fold)\n#         models.append(model_lgb)\n#         scores_folds['lgb'].append(score_lgb)\n\n# lgb_oof_df = train[[CONFIG.target_col, 'pred_lgb']].copy()\n# # CV result\n# LOGGER.info(f\"========== CV ==========\")\n# print('=========== LGB ===========')\n# score = mean_squared_error(lgb_oof_df[CONFIG.target_col].values, lgb_oof_df['pred_lgb'].values) ** .5\n# print('RMSLE {}: {} - Folds: {}'.format('lgb', score, scores_folds['lgb']))\n# LOGGER.info('RMSLE {}: {} - Folds: {}'.format('lgb', score, scores_folds['lgb']))\n\n# # save result\n# lgb_oof_df.to_csv(OUTPUT_DIR +'lgb_oof_df.csv', index=False)\n\n# # feature importance\n# fig, ax = visualize_importance(models, train[feature_col])","f044ff7d":"# ====================================================\n# Train loop\n# ====================================================\ndef train_loop(folds, fold):\n    LOGGER.info(f\"========== fold: {fold} training ==========\")\n    print(datetime.now())\n    \n    trn_idx = folds[folds['fold'] != fold].index\n    val_idx = folds[folds['fold'] == fold].index\n    \n    X_train = folds[feature_col].loc[trn_idx].reset_index(drop=True)\n    y_train = folds[CONFIG.target_col].loc[trn_idx].reset_index(drop=True).values\n    X_val = folds[feature_col].loc[val_idx].reset_index(drop=True)\n    y_val = folds[CONFIG.target_col].loc[val_idx].reset_index(drop=True).values\n    print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n    ##############################################################\n    # LGB\n    ##############################################################\n    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cats, weight=1\/np.power(y_train,2))\n    val_data = lgb.Dataset(X_val, label=y_val, categorical_feature=cats, weight=1\/np.power(y_val,2))\n    opt_params = {\n        \"objective\": \"root_mean_squared_error\",\n        \"metric\": \"rmse\",\n        'random_state' : 42,\n        'force_col_wise' : True,\n        \"verbosity\": -1,\n    }\n    model_lgb = opt_lgb.train(opt_params,\n                              train_data,\n                              time_budget = OPT_CONFIG.TIME,\n                              valid_sets = val_data,\n                              verbose_eval = False,\n                              verbosity = -1,\n                              num_boost_round = OPT_CONFIG.NUM,\n                              show_progress_bar = False,\n                              early_stopping_rounds = OPT_CONFIG.EARLY\n                             )\n#     model_lgb = lgb.train(params_lgbm, \n#                       train_data, \n#                       5000, \n#                       valid_sets=val_data, \n#                       verbose_eval= 1000,\n#                       early_stopping_rounds=500\n#                      )\n    best_params = model_lgb.params\n    print(\"Best params:\", best_params)\n    preds = model_lgb.predict(X_val, num_iteration=model_lgb.best_iteration)\n    train.loc[val_idx, 'pred_lgb'] = preds\n    score_lgb = mean_squared_error(y_val, preds) ** .5\n    print('Fold {} {}: {}'.format(fold+1, 'lgb', score_lgb))\n    LOGGER.info('Fold {} {}: {}'.format(fold+1, 'lgb', score_lgb))\n    # save model\n    pickle.dump(model_lgb, open(f'lgb_model_{fold+1}.pkl', 'wb'))\n    \n    ##############################################################\n    # cat\n    ##############################################################\n    model_cat = CatBoostRegressor(iterations=500,\n                                 learning_rate=0.01,\n                                 depth=10,\n                                 eval_metric='RMSE',\n                                 random_seed = CONFIG.seed,\n                                 bagging_temperature = 0.2,\n                                 od_type='Iter',\n                                 metric_period = 50,\n                                 od_wait=20)\n    model_cat.fit(X_train, y_train,\n                  eval_set=(X_val, y_val),\n                  use_best_model=True,\n                  verbose=200)\n    preds = model_cat.predict(X_val)\n    train.loc[val_idx, 'pred_cat'] = preds\n    score_cat = mean_squared_error(y_val, preds) ** .5\n    print('Fold {} {}: {}'.format(fold+1, 'cat', score_cat))\n    LOGGER.info('Fold {} {}: {}'.format(fold+1, 'cat', score_cat))\n    # save model\n    pickle.dump(model_cat, open(f'cat_model_{fold+1}.pkl', 'wb'))\n    \n    ##############################################################\n    # tab\n    ##############################################################\n    params = dict(\n        n_d = 32,\n        n_a = 32,\n        n_steps = 1,\n        gamma = 0.9,\n        lambda_sparse = 0,\n        optimizer_fn = optim.Adam, # \u521d\u624b\u306fAdam\n        optimizer_params = dict(lr=2e-2, weight_decay=1e-5),\n        mask_type = 'entmax',\n        scheduler_params = dict(mode='min', patience=5, min_lr=1e-5, factor=0.9),\n        scheduler_fn = optim.lr_scheduler.ReduceLROnPlateau, # CosineAnnieling\u3067\u3082\u53ef\n        seed = CONFIG.seed,\n        verbose = 50\n    )\n    print('---- Pretrain -----')\n    pretrain_model = TabNetPretrainer(**params)\n    pretrain_model.fit(X_train.values,\n                     eval_set = [X_val.values],\n                     max_epochs = CONFIG.EPOCHS,\n                     patience = CONFIG.PATIENCE,\n                     batch_size = CONFIG.BATCH_SIZE,\n                     virtual_batch_size = CONFIG.V_BATCH,\n                     num_workers = CONFIG.WORKERS,\n                     drop_last = True\n                     )\n    ## Pretrained\u30e2\u30c7\u30eb\u3092FineTuning\n    print('---- FineTuning -----')\n    model_tab = TabNetRegressor(**params)\n    (X_train.values.shape , X_val.values.shape)\n    model_tab.fit(X_train = X_train.values,\n                y_train = y_train.reshape(-1,1),\n                eval_set = [(X_val.values, y_val.reshape(-1,1))],\n                eval_metric = [CONFIG.LOSS],\n                max_epochs = CONFIG.EPOCHS,\n                patience = CONFIG.PATIENCE,\n                batch_size = CONFIG.BATCH_SIZE,\n                virtual_batch_size = CONFIG.V_BATCH,\n                num_workers = CONFIG.WORKERS,\n                drop_last = False,\n#                 from_unsupervised = pretrain_model\n                )\n    preds = model_tab.predict(X_val.values)\n    print(preds.shape)\n    train.loc[val_idx, 'pred_tab'] = preds\n    score_tab = mean_squared_error(y_val, preds) ** .5\n    print('Fold {} {}: {}'.format(fold+1, 'tab', score_tab))\n    LOGGER.info('Fold {} {}: {}'.format(fold+1, 'tab', score_tab))\n    # save model\n    saved_filepath = model_tab.save_model(f'tab_model_{fold+1}')\n    \n    return model_lgb, score_lgb, model_cat, score_cat, model_tab, score_tab","99356bcd":"scores_folds = {}\nscores_folds['lgb'] = []\nscores_folds['cat'] = []\nscores_folds['tab'] = []\nmodels = []\nfor fold in range(CONFIG.n_fold):\n    if fold in CONFIG.trn_fold:\n        model_lgb, score_lgb, model_cat, score_cat, model_tab, score_tab = train_loop(train, fold)\n        models.append(model_lgb)\n        scores_folds['lgb'].append(score_lgb)\n        scores_folds['cat'].append(score_cat)\n        scores_folds['tab'].append(score_tab)\n\nlgb_oof_df = train[[CONFIG.target_col, 'pred_lgb']].copy()\ncat_oof_df = train[[CONFIG.target_col, 'pred_cat']].copy()\ntab_oof_df = train[[CONFIG.target_col, 'pred_tab']].copy()\n# CV result\nLOGGER.info(f\"========== CV ==========\")\nprint('=========== LGB ===========')\nscore = mean_squared_error(lgb_oof_df[CONFIG.target_col].values, lgb_oof_df['pred_lgb'].values) ** .5\nprint('RMSLE {}: {} - Folds: {}'.format('lgb', score, scores_folds['lgb']))\nLOGGER.info('RMSLE {}: {} - Folds: {}'.format('lgb', score, scores_folds['lgb']))\n\nprint('=========== CAT ===========')\nscore = mean_squared_error(cat_oof_df[CONFIG.target_col].values, cat_oof_df['pred_cat'].values) ** .5\nprint('RMSLE {}: {} - Folds: {}'.format('cat', score, scores_folds['cat']))\nLOGGER.info('RMSLE {}: {} - Folds: {}'.format('cat', score, scores_folds['cat']))\n\nprint('=========== TAB ===========')\nscore = mean_squared_error(tab_oof_df[CONFIG.target_col].values, tab_oof_df['pred_tab'].values) ** .5\nprint('RMSLE {}: {} - Folds: {}'.format('tab', score, scores_folds['tab']))\nLOGGER.info('RMSLE {}: {} - Folds: {}'.format('tab', score, scores_folds['tab']))\n\n# save result\nlgb_oof_df.to_csv(OUTPUT_DIR +'lgb_oof_df.csv', index=False)\ncat_oof_df.to_csv(OUTPUT_DIR +'cat_oof_df.csv', index=False)\ntab_oof_df.to_csv(OUTPUT_DIR +'tab_oof_df.csv', index=False)\n\n# feature importance\nfig, ax = visualize_importance(models, train[feature_col])","7517c609":"plt.figure(figsize=(16, 5),tight_layout=True)\nsns.distplot(train['target'], label='train')\nsns.distplot(lgb_oof_df['pred_lgb'], label='lgb_oof')\nsns.distplot(cat_oof_df['pred_cat'], label='cat_oof')\nsns.distplot(tab_oof_df['pred_tab'], label='tab_oof')\nplt.legend()\nplt.show()","851a8204":"train['error_lgb'] = (train['target'] - train['pred_lgb']) ** 2\ntrain['error_cat'] = (train['target'] - train['pred_cat']) ** 2\ntrain['error_tab'] = (train['target'] - train['pred_tab']) ** 2\ntrain.sort_values('error_lgb', ascending=False).head(50)","d4e68a01":"# ====================================================\n# Stacking\n# ====================================================\ndef stacking(folds, fold):\n    LOGGER.info(f\"========== fold: {fold} training ==========\")\n    print(datetime.now())    \n    trn_idx = folds[folds['fold'] != fold].index\n    val_idx = folds[folds['fold'] == fold].index\n    \n    X_train = folds[['pred_lgb', 'pred_tab', 'pred_cat']].loc[trn_idx].reset_index(drop=True)\n#     X_train = folds[['pred_lgb', 'pred_tab']].loc[trn_idx].reset_index(drop=True)\n    y_train = folds[CONFIG.target_col].loc[trn_idx].reset_index(drop=True).values\n    X_val = folds[['pred_lgb', 'pred_tab', 'pred_cat']].loc[val_idx].reset_index(drop=True)\n#     X_val = folds[['pred_lgb', 'pred_tab']].loc[val_idx].reset_index(drop=True)\n    y_val = folds[CONFIG.target_col].loc[val_idx].reset_index(drop=True).values\n    print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n    ##############################################################\n    # Ridge\n    ##############################################################\n    model = Ridge(alpha =20, random_state=CONFIG.seed).fit(X_train, y_train)\n    preds = model.predict(X_val)\n    train.loc[val_idx, 'pred_ridge'] = preds\n    score = mean_squared_error(y_val, preds) ** .5\n    print('Fold {} {}: {}'.format(fold+1, 'ridge', score))\n    LOGGER.info('Fold {} {}: {}'.format(fold+1, 'ridge', score))\n    # save model\n    pickle.dump(model, open(f'ridge_model_{fold+1}.pkl', 'wb'))\n\n    return model, score","c0ecf12e":"from sklearn.linear_model import Ridge\nscores_folds['ridge'] = []\nfor fold in range(CONFIG.n_fold):\n    if fold in CONFIG.trn_fold:\n        model_ridge, score_ridge = stacking(train, fold)\n        scores_folds['ridge'].append(score_ridge)\n\nridge_oof_df = train[[CONFIG.target_col, 'pred_ridge']].copy()\n\nLOGGER.info(f\"========== CV ==========\")\nprint('=========== Ridge ===========')\nscore = mean_squared_error(ridge_oof_df[CONFIG.target_col].values, ridge_oof_df['pred_ridge'].values) ** .5\nprint('RMSLE {}: {} - Folds: {}'.format('ridge', score, scores_folds['ridge']))\nLOGGER.info('RMSLE {}: {} - Folds: {}'.format('ridge', score, scores_folds['ridge']))\n\n# save result\nridge_oof_df.to_csv(OUTPUT_DIR +'ridge_oof_df.csv', index=False)","8f63149d":"train['pred_mean'] = train[['pred_lgb', 'pred_tab']].mean(axis=1)\nmean_oof_df = train[[CONFIG.target_col, 'pred_mean']].copy()\nLOGGER.info(f\"========== CV ==========\")\nprint('=========== Mean ===========')\nscore = mean_squared_error(ridge_oof_df[CONFIG.target_col].values, mean_oof_df['pred_mean'].values) ** .5\nprint('RMSLE {}: {}'.format('mean', score))\nLOGGER.info('RMSLE {}: {}'.format('ridge', score))","0ac5a602":"MODEL_PATH = '.\/'\ndel test[CONFIG.target_col]","9942b2a0":"# ====================================================\n# inference lgb\n# ====================================================\ntest['target_lgb'] = 0\nmodels = [ MODEL_PATH + f'lgb_model_{fold+1}.pkl' for fold in CONFIG.trn_fold]\nfor path in models:\n    model = pickle.load(open(path, 'rb'))\n    print(test[feature_col].shape)\n    test['target_lgb'] += model.predict(test[feature_col])\n\ntest['target_lgb'] = test['target_lgb']\/CONFIG.n_fold","398f8259":"# ====================================================\n# inference cat\n# ====================================================\ntest['target_cat'] = 0\nmodels = [ MODEL_PATH + f'cat_model_{fold+1}.pkl' for fold in CONFIG.trn_fold]\nfor path in models:\n    model = pickle.load(open(path, 'rb'))\n    print(test[feature_col].shape)\n    print(model.predict(test[feature_col]).shape)\n    test['target_cat'] += model.predict(test[feature_col])\n\ntest['target_cat'] = test['target_cat']\/CONFIG.n_fold","43c9d661":"# ====================================================\n# inference tab\n# ====================================================\ntest['target_tab'] = 0\nmodels = [ MODEL_PATH + f'tab_model_{fold+1}.zip' for fold in CONFIG.trn_fold]\nfor path in models:\n    model = TabNetRegressor()\n    model.load_model(path)\n    print(test[feature_col].shape)\n    test['target_tab'] += model.predict(test[feature_col].values)[:, 0]\n\ntest['target_tab'] = test['target_tab']\/CONFIG.n_fold","636be34f":"# ====================================================\n# stacking ridge\n# ====================================================\ntest['target_ridge'] = 0\nmodels = [ MODEL_PATH + f'ridge_model_{fold+1}.pkl' for fold in CONFIG.trn_fold]\nfor path in models:\n    model = pickle.load(open(path, 'rb'))\n    print(test[['target_lgb', 'target_tab', 'target_cat']].shape)\n    test['target_ridge'] += model.predict(test[['target_lgb', 'target_tab', 'target_cat']])\n\ntest['target_ridge'] = test['target_ridge']\/CONFIG.n_fold","62b30269":"# ====================================================\n# mean\n# ====================================================\ntest['target_mean'] = test[['target_lgb', 'target_tab']].mean(axis=1)","5e1366e2":"plt.figure(figsize=(16, 5),tight_layout=True)\nsns.distplot(train['target'], label='train')\nsns.distplot(test['target_lgb'], label='lgb_preds')\n# sns.distplot(test['target_lgb'], label='lgb_preds')\n# sns.distplot(test['target_cat'], label='cat_preds')\nsns.distplot(test['target_tab'], label='tab_preds')\n# sns.distplot(test['target_ridge'], label='ridge_preds')\nsns.distplot(test['target_mean'], label='mean_preds')\nplt.legend()\nplt.show()","1b1c9dad":"test['target_lgb'] = test['target_lgb'].apply(lambda x: np.expm1(x))\ndisplay(test.sort_values('target_lgb', ascending=False)[['pokemon','target_lgb']].head(50))\ntest[['id', 'target_lgb']].rename(columns={'target_lgb':'target'}).to_csv(OUTPUT_DIR + 'submission_lgb.csv', index=False)\ndisplay(test[['id', 'target_lgb']])","63ecb71d":"test['target_cat'] = test['target_cat'].apply(lambda x: np.expm1(x))\ndisplay(test.sort_values('target_cat', ascending=False)[['pokemon','target_cat']].head(50))\ntest[['id', 'target_cat']].rename(columns={'target_cat':'target'}).to_csv(OUTPUT_DIR + 'submission_cat.csv', index=False)\ndisplay(test[['id', 'target_cat']])","3f7b35b8":"test['target_tab'] = test['target_tab'].apply(lambda x: np.expm1(x))\ndisplay(test.sort_values('target_tab', ascending=False)[['pokemon','target_tab']].head(50))\ntest[['id', 'target_tab']].rename(columns={'target_tab':'target'}).to_csv(OUTPUT_DIR + 'submission_tab.csv', index=False)\ndisplay(test[['id', 'target_tab']])","9bf1c3c0":"test['target_ridge'] = test['target_ridge'].apply(lambda x: np.expm1(x))\ndisplay(test.sort_values('target_ridge', ascending=False)[['pokemon','target_ridge']].head(50))\ntest[['id', 'target_ridge']].rename(columns={'target_ridge':'target'}).to_csv(OUTPUT_DIR + 'submission_ridge.csv', index=False)\ndisplay(test[['id', 'target_ridge']])","910ec9e2":"test['target_mean'] = test['target_mean'].apply(lambda x: np.expm1(x))\ndisplay(test.sort_values('target_mean', ascending=False)[['pokemon','target_mean']].head(50))\ntest[['id', 'target_mean']].rename(columns={'target_mean':'target'}).to_csv(OUTPUT_DIR + 'submission_mean.csv', index=False)\ndisplay(test[['id', 'target_mean']])","65f071aa":"## status","3ad5634f":"# Helper Functions","073e23aa":"# egg_group","8d0acf75":"## pokemon","765f526c":"# evolves_from_species_id","9836ba7c":"# Same Name Target","8977eebf":"# Categorical Encoder","851701b8":"# heights + weight all","ed421d1d":"# Stats_attack_sum by types","6b878614":"# Status all","c330cae8":"# Has Hyphen","4a9fa73b":"## height & weight","5e9711ab":"# Stats_sum by types","f7b1e977":"# Target Encoding","9bc8a0e8":"# CONFIG","11fed634":"# audino","ba6197a1":"# About this notebook\n- PyTorch lgb cat code\n- StratifiedKFold 6 folds","c262cf20":"# Stats_defence_sum by types","c7c15c26":"# Data Loading","30d2fe65":"# Training","591368a8":"# Feature Engineering","9ab8077a":"# NaN count","f308740d":"## type","3af4be0f":"# Inference","cdc59b49":"# Quick EDA","5183bf6c":"# ability","eef10329":"# Utils","48d49ef3":"# CV Split","baaf75a5":"# Has Max"}}