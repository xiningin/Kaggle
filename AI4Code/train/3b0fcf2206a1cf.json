{"cell_type":{"031feeb7":"code","9fb3b2c2":"code","5e82467d":"code","43e21c16":"code","446bf066":"code","61910889":"code","808d6216":"code","06b52900":"code","26aa37b7":"code","1e59be9f":"code","72e51875":"code","d3a1e948":"code","5ff14b07":"code","1495be06":"code","aad10d02":"code","b11465f0":"code","ca3878db":"code","6759102e":"code","fbbd5fa3":"code","1d9eb838":"code","0417a784":"code","257044d5":"code","110f72c1":"code","3aa22e72":"code","55f4c5b8":"code","2f609688":"code","50c0c223":"code","a7047f25":"code","46ebd3c7":"markdown","061d9b10":"markdown","7deeb096":"markdown","675a3ef4":"markdown","e44827e9":"markdown","3c0c44c2":"markdown","c6752bf3":"markdown","69404ab2":"markdown","d960e80e":"markdown","aaa55fab":"markdown","4bd22057":"markdown","cb22af2b":"markdown","8399933a":"markdown","f3645359":"markdown","7af6aebd":"markdown","dc10bd18":"markdown","23dadd75":"markdown","1b869c0d":"markdown","9ba27297":"markdown","0ef5bcb8":"markdown","e812e539":"markdown","9efb0c7b":"markdown","8cf592a6":"markdown","df98a004":"markdown","5bbc2677":"markdown","d0f271dd":"markdown","2da38c7d":"markdown","a888bc1e":"markdown","ffb94fc1":"markdown","80aadc4a":"markdown","f44b9244":"markdown","0384abd5":"markdown","2a7e250c":"markdown","efe34520":"markdown","29b0ae72":"markdown","b99c200d":"markdown","315cbc5b":"markdown"},"source":{"031feeb7":"import os \nimport random\nimport numpy as np\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\nfrom tensorflow_addons.metrics import F1Score\n\n\nprint(\"Libraries imported\")","9fb3b2c2":"path = (\"..\/input\/kermany2018\/oct2017\/OCT2017 \")\nos.listdir(path)","5e82467d":"train_dir = path + '\/train'\nos.listdir(train_dir)","43e21c16":"val_dir = path + '\/val'\nos.listdir(val_dir)","446bf066":"test_dir = path + '\/test'\nos.listdir(test_dir)","61910889":"vgg19 = tf.keras.applications.VGG19(\n    include_top = False,\n    weights = 'imagenet',\n    input_tensor = None,\n    input_shape = (150,150,3),\n    pooling  = None,\n    classes = 1000\n)","808d6216":"vgg19.trainable = False\nprint(\"Trainable set to false\")","06b52900":"model_vgg = tf.keras.models.Sequential([\n    \n    vgg19,\n    \n    tf.keras.layers.Conv2D(filters = 256, kernel_size = (3, 3), padding = 'same'),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.Dense(units=1024,activation='relu'),\n    \n    tf.keras.layers.Dropout(0.25),\n    \n    tf.keras.layers.Conv2D(filters = 128, kernel_size = (3,3), padding = 'same'),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.Dense(units=512,activation='relu'),\n    \n    tf.keras.layers.Dropout(0.25),\n    \n    tf.keras.layers.Conv2D(filters = 64, kernel_size = (3,3), padding = 'same'),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.Dense(units=256,activation='relu'),\n    \n    tf.keras.layers.Dropout(0.25),\n    \n    tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), padding = 'same'),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.Dense(units=128,activation='relu'),\n    \n    tf.keras.layers.Dropout(0.25),\n    \n    tf.keras.layers.Conv2D(filters = 16, kernel_size = (3,3), padding = 'same'),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(units=64,activation='relu'),\n    \n    tf.keras.layers.Dropout(0.25),\n    \n    tf.keras.layers.Dense(units = 4,activation = 'softmax')\n])","26aa37b7":"F1_Score = tfa.metrics.F1Score(num_classes=4, threshold=0.5)","1e59be9f":"vgg_metrics = [\"accuracy\", F1_Score]","72e51875":"model_vgg.compile(optimizer = 'adadelta', loss = 'binary_crossentropy', metrics = vgg_metrics)\nmodel_vgg.summary()","d3a1e948":"train_datagen = ImageDataGenerator(rescale = 1.\/255)\ntrain_generator = train_datagen.flow_from_directory(train_dir, target_size = (150, 150), class_mode = 'categorical', batch_size = 500)","5ff14b07":"validation_datagen = ImageDataGenerator(rescale = 1.\/255)\nvalidation_generator = validation_datagen.flow_from_directory(val_dir, target_size = (150, 150), class_mode = 'categorical', batch_size = 16)","1495be06":"test_datagen = ImageDataGenerator(rescale = 1.\/255)\ntest_generator = test_datagen.flow_from_directory(test_dir, target_size = (150, 150), class_mode = 'categorical', batch_size = 50)","aad10d02":"vgg_history = model_vgg.fit(\n    train_generator,\n    steps_per_epoch = (83484\/500),\n    epochs = 40,\n    validation_data = validation_generator,\n    validation_steps = (32\/16),\n    max_queue_size=100,\n    workers = 4 ,\n    use_multiprocessing=True,\n    verbose = 1)\n\n","b11465f0":"testing_model = model_vgg.evaluate(test_generator)\n","ca3878db":"training_accuracy = (vgg_history.history['accuracy'][-1]*100)","6759102e":"validation_accuracy = (vgg_history.history['val_accuracy'][-1]*100)","fbbd5fa3":"testing_accuracy = testing_model[1]*100","1d9eb838":"training_loss = (vgg_history.history['loss'][-1]*100)","0417a784":"validation_loss = (vgg_history.history['val_loss'][-1]*100)","257044d5":"testing_loss = testing_model[0]*100","110f72c1":"f1_score = (vgg_history.history['f1_score'][-1]*100)","3aa22e72":"val_f1_score = (vgg_history.history['val_f1_score'][-1]*100)","55f4c5b8":"testing_f1_score = (testing_model[2])","2f609688":"print(\"Training Accuracy : \" + str(training_accuracy) + \" %\")\nprint(\"Validation Accuracy : \" + str(validation_accuracy) + \" %\")\nprint(\"Testing Accuracy : \" + str(testing_accuracy) + \" %\")","50c0c223":"print(\"Training Loss : \" + str(training_loss) + \" %\")\nprint(\"Validation Loss : \" + str(validation_loss) + \" %\")\nprint(\"Testing Loss : \" + str(testing_loss) + \" %\")","a7047f25":"print(\"F1 Score : \" + str(f1_score))\nprint(\"Validation F1 Score : \" + str(val_f1_score))\nprint(\"Testing F1 Score : \" + str(testing_f1_score))","46ebd3c7":"## Testing the model","061d9b10":"## Rescaling Images and Defining generators","7deeb096":"## Storing F1 Scores","675a3ef4":"### Accuracy","e44827e9":"### Validation Dataset","3c0c44c2":"### Loss","c6752bf3":"## Compiled Result","69404ab2":"### Defining the layers","d960e80e":"#### Storing the training accuracy","aaa55fab":"#### Storing the Validation F1 Score","4bd22057":"### Defining Metrics","cb22af2b":"#### Storing the Training Loss","8399933a":"#### Storing the testing accuracy","f3645359":"### Compiling the full model by addidng optimiser, loss and metrics","7af6aebd":"#### Storing the Validation Loss","dc10bd18":"#### Storing the Training Loss","23dadd75":"#### Storing the Training F1 Score","1b869c0d":"## Defining the master directory path","9ba27297":"## Storing Loss\n","0ef5bcb8":"### Training Dataset","e812e539":"# ClaRet - Retinal Scan Classification","9efb0c7b":"## Defining The VGG19 model","8cf592a6":"## Defining the Transfer Learning Model Architecture - VGG19","df98a004":"### Storing accuracies","5bbc2677":"### Testing Dataset","d0f271dd":"## Storing the path of the training, validation and testing directories","2da38c7d":"### F1 Scores","a888bc1e":"## Model Compilation\n","ffb94fc1":"### Testing Directory","80aadc4a":"### Validation Directory","f44b9244":"#### Storing the Testing F1 Score","0384abd5":"### Training Directory","2a7e250c":"### Not letting VGG19 train all over again","efe34520":"## Importing Libraries and Frameworks","29b0ae72":"## Compiling Results","b99c200d":"#### Storing the validation accuracy","315cbc5b":"## Training the model"}}