{"cell_type":{"7b46a169":"code","9d2368f8":"code","c81ea5ad":"code","0853ae38":"code","75655cdf":"code","a6072c3c":"code","e5a8ab61":"code","f2140a97":"code","64f0bda6":"code","784fcf2c":"code","46ffaaa4":"code","4ca783e1":"code","16f94d7b":"code","24568b3e":"code","cd260cb9":"code","ecee8117":"code","dbd1bbc9":"code","c77444b3":"code","a344553d":"code","d9c95984":"code","bc731ab0":"code","db909f2d":"code","e51c656d":"code","61409320":"code","a9e29e9b":"code","86be9e60":"code","4155bd96":"code","4b8bb472":"code","0c4bf764":"code","9950b560":"code","eb7c8de8":"code","b5b3b582":"code","b1387076":"code","b7c3b904":"code","fe3c717d":"code","453f95c1":"code","bc5dc6d6":"code","8501a151":"code","add83b31":"code","ca288301":"code","f662ebc4":"code","d0f60876":"code","656c064b":"code","6205f48b":"code","09df137e":"code","e81d4528":"code","284c5630":"code","3783f4d8":"code","b4c39f0a":"code","76357eb4":"code","51588e87":"code","3887d3eb":"code","ca6ee3e4":"code","a230d37e":"code","e784d6f6":"code","c8e36f92":"code","c67bf879":"code","0c92eb56":"code","18592ec8":"code","eb9e9a98":"code","a301a7be":"code","17e9673f":"code","e69ede2c":"code","351ff43e":"code","3e989775":"code","3ab2d776":"markdown","07f93306":"markdown","f7c1dfe0":"markdown","3b756011":"markdown","0b263747":"markdown","b3d86782":"markdown","19358425":"markdown","5f69719f":"markdown","3f478dce":"markdown","f51e4115":"markdown","d14b64f8":"markdown","8d168bdb":"markdown","a6e1bcff":"markdown","9249d227":"markdown","8d484b59":"markdown","e013891c":"markdown","b8cc93a4":"markdown","b7e024b8":"markdown","86b55cde":"markdown","3b4ffda6":"markdown","abb864f1":"markdown","40247fe1":"markdown","e5a50cb9":"markdown","b88bd1fe":"markdown","cd5aa457":"markdown","0caef149":"markdown","6942f4fe":"markdown","629a9fc6":"markdown","e8e9ff3f":"markdown","cee1b7c7":"markdown","e69398ec":"markdown","1a88a7fd":"markdown","10715db5":"markdown","74c55bf5":"markdown","61a1497f":"markdown"},"source":{"7b46a169":"# STAT8017 Data mining techniques \u2013 Group project\n# Data Analysis of Cardiovascular Disease Dataset\n#------------------------------------------------------------------------------------------\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn\nfrom itertools import product\n\n# data transformation & splitting\nfrom sklearn.preprocessing import RobustScaler, label_binarize\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\n# decision tree, logistic\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nimport graphviz\nfrom sklearn.linear_model import LogisticRegressionCV\n\n# clustering analysis\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\nimport scipy.cluster.hierarchy as sch\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import jaccard_score, adjusted_rand_score, silhouette_score, calinski_harabasz_score, roc_curve, auc, accuracy_score, classification_report, confusion_matrix\nfrom sklearn.metrics.cluster import contingency_matrix\n\n# ensemble methods, MLP\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.neural_network import MLPClassifier\n#------------------------------------------------------------------------------------------   \n# Input data files are available in the read-only \"..\/input\/\" directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9d2368f8":"# read data\ndf = pd.read_csv('\/kaggle\/input\/cardiovascular-disease-dataset\/cardio_train.csv', sep = ';', index_col = 'id')\n\n# preview\npd.options.display.float_format = '{:,.2f}'.format\ndisplay(df.head())","c81ea5ad":"# check blank rows\ndf.isnull().sum() ","0853ae38":"# before data cleaning\ndisplay(df.describe())","75655cdf":"# convert age from days to years\ndf['age'] = df['age']\/365","a6072c3c":"# convert gender to 0=female and 1=male\ndf['gender'] = df['gender'] - 1","e5a8ab61":"# calculate BMI\ndf['BMI'] = df['weight']\/(np.power(df['height']\/100, 2))\n\n# BMI Distribution\nprint(df['BMI'].describe())\nseaborn.histplot(data = df, x = 'BMI', bins = 100)\nplt.show()","f2140a97":"# remove BMI > 150\ndrop_criteria_bmi = df[df['BMI'] > 150].index\n\n# number of records to be removed\nprint(drop_criteria_bmi.size)\n\n# remove records\ndf.drop(drop_criteria_bmi, inplace = True)","64f0bda6":"# ap_hi is higher than 250 or lower than 60\ndrop_criteria_aphi = df[(df['ap_hi'] > 210) | (df['ap_hi'] < 60)].index\n\n# ap_lo is higher than 200 or lower than 10\ndrop_criteria_aplo = df[(df['ap_lo'] > 140) | (df['ap_lo'] < 30)].index\n\n# ap_lo is higher than 'ap_hi\ndrop_criteria_ap = df[df['ap_lo'] > df['ap_hi']].index \n\n# number of records to be removed\ndrop_criteria = drop_criteria_aphi.union(drop_criteria_aplo)\ndrop_criteria.union(drop_criteria_ap)\nprint(drop_criteria.size)","784fcf2c":"# remove the records\ndf.drop(drop_criteria, inplace = True)","46ffaaa4":"# after data cleaning\ndisplay(df.describe())\n\n# distribution of response variable\ndisplay(pd.DataFrame(df['cardio'].value_counts()))","4ca783e1":"def pie_chart(df, col, labels):\n    data = df[col].value_counts().to_numpy()\n    def absolute_value(val):\n        a  = np.round(val \/ 100 * data.sum(), 0)\n        return str('%0.0f' % a) + '\\n(' + ('%0.2f' % val) + '%)'\n    plt.pie(data, labels = labels, autopct=absolute_value)\n    plt.legend(title=col)\n    plt.show() \n\npie_chart(df, 'gender', ['Female', 'Male'])\npie_chart(df, 'cardio', ['No', 'Yes'])\npie_chart(df, 'cholesterol', ['Normal', 'Above normal', 'Well above normal'])\npie_chart(df, 'gluc', ['Normal', 'Above normal', 'Well above normal'])\npie_chart(df, 'smoke', ['No', 'Yes'])\npie_chart(df, 'alco', ['No', 'Yes'])\npie_chart(df, 'active', ['No', 'Yes'])","16f94d7b":"df_subset = df[['age', 'height', 'weight', 'ap_hi', 'ap_lo']]\nflierprops = dict(markerfacecolor='lightblue', marker='o',markeredgecolor='lightblue') \n# <=> rs = {'markerfacecolor'='lightblue', 'marker'='o'}\nboxprops = dict(facecolor='lightblue',color = 'lightblue') # color: box line color; facecolor: fill-in color\nplt.figure(figsize=(10, 5))\nplt.boxplot(df_subset.values,labels=df_subset.columns,\n           flierprops=flierprops,boxprops=boxprops,\n            patch_artist=True)\nplt.show()\ndf_subset.boxplot()","24568b3e":"from sklearn.preprocessing import QuantileTransformer\nquantile_transformer = QuantileTransformer(random_state=0)\nX_trans = quantile_transformer.fit_transform(df_subset)\npd.DataFrame(X_trans, columns=df_subset.columns).hist(bins = 5)\nplt.tight_layout()","cd260cb9":"flierprops = dict(markerfacecolor='lightblue', marker='o',markeredgecolor='lightblue') \n# <=> rs = {'markerfacecolor'='lightblue', 'marker'='o'}\nboxprops = dict(facecolor='lightblue',color = 'lightblue') # color: box line color; facecolor: fill-in color\nplt.figure(figsize=(10, 5))\nplt.boxplot(df_subset.values,labels=df_subset.columns,\n           flierprops=flierprops,boxprops=boxprops,\n            patch_artist=True)\nplt.show()","ecee8117":"%%time\n\n# pair-plot\nseaborn.pairplot(df, vars = ['age', 'height', 'weight', 'ap_hi', 'ap_lo'], hue = 'cardio')\nplt.show()","dbd1bbc9":"# gender pair-plot\nseaborn.pairplot(df[df.gender == 0], vars = ['age', 'height', 'weight', 'ap_hi', 'ap_lo'], hue = 'cardio')\nplt.show()\nseaborn.pairplot(df[df.gender == 1], vars = ['age', 'height', 'weight', 'ap_hi', 'ap_lo'], hue = 'cardio')\nplt.show()","c77444b3":"# correlation heatmap\nplt.figure(figsize=(16, 8))\nseaborn.heatmap(df.corr(), annot=True, fmt='.3f')","a344553d":"# explanatory variables\nx = df.drop(columns = ['cardio', 'BMI'])\n\n# response variable\ny = df['cardio']","d9c95984":"# split data\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.4, random_state = 8017)\n\n# RobustScaler ALL variables\nscaler = RobustScaler()\nscaler.fit(x_train)\nx_train = pd.DataFrame(scaler.transform(x_train), index=x_train.index, columns=x_train.columns)\nx_test = pd.DataFrame(scaler.transform(x_test), index=x_test.index, columns=x_test.columns)\n\npd.options.display.float_format = '{:,.4f}'.format\ndisplay(x_train.head(5))\ndisplay(x_train.describe())","bc731ab0":"%%time\n\n# parameters candidates\nparameters = {'max_depth':range(2,32)}\n\n# fitting\nDecisionTree_GSCV = GridSearchCV(DecisionTreeClassifier(random_state=8017), \n                                 parameters, n_jobs=-1, verbose=3, return_train_score=True)\nDecisionTree_GSCV.fit(x_train, y_train)\nDecisionTree_model = DecisionTree_GSCV.best_estimator_","db909f2d":"# plot training & testing scores\ntrain_dt_scores = DecisionTree_GSCV.cv_results_['mean_train_score']\ntest_dt_scores = DecisionTree_GSCV.cv_results_['mean_test_score']\n\nplt.plot(train_dt_scores, \"g.--\")\nplt.plot(test_dt_scores, \"g.-\")\nplt.ylim(0.4, 1.05)\nplt.xticks(range(30), range(2, 32))\nplt.legend([\"DT training score\", \"DT test score\"])\nplt.axvline(np.argmax(test_dt_scores), linestyle=\"dotted\", color=\"red\")\nplt.annotate(np.max(test_dt_scores).round(4), (np.argmax(test_dt_scores), np.max(test_dt_scores)), xycoords=\"data\",\n                 xytext=(50, 25), textcoords=\"offset pixels\", arrowprops=dict(facecolor=\"black\", shrink=0.1), fontsize=10,\n                 horizontalalignment=\"center\", verticalalignment=\"top\")\nplt.show()","e51c656d":"# accuracy scores\nprint(DecisionTree_model.get_params())\nprint(f\"Training Score: {round(DecisionTree_model.score(x_train, y_train),4)}\")\nprint(f\"Testing Score: {round(DecisionTree_model.score(x_test, y_test),4)}\")","61409320":"# feature importances\nd = {'feature importance':list(DecisionTree_model.feature_importances_)}\ntable = pd.DataFrame(d, index=x_train.columns)\n\ndisplay(  table.sort_values('feature importance', ascending=False)  )","a9e29e9b":"%%time\n\n# fitting\nLogistic_model =  LogisticRegressionCV(Cs = 50, cv = 5, random_state=8017)\nLogistic_model.fit(x_train, y_train)\nprint(Logistic_model.get_params())","86be9e60":"# regularaization candidates\nprint('Candidates of Regularization Parameter C:')\nprint(Logistic_model.Cs_, '\\n')\n\n# accuracy scores\nprint(f\"Training Score: {round(Logistic_model.score(x_train, y_train),4)}\")\nprint(f\"Testing Score: {round(Logistic_model.score(x_test, y_test),4)}\")","4155bd96":"# fitted parameters\nprint(f'Best Regularization Parameter C = {round(Logistic_model.C_[0],4)}')\nprint(f'intercept = {round(Logistic_model.intercept_[0],4)}')\nd = {'estimates' : list(Logistic_model.coef_[0]),\n     'absolute' : np.abs(list(Logistic_model.coef_[0]))\n    }\ntable = pd.DataFrame(d, index=x_train.columns)\n\ndisplay(  table.sort_values('absolute', ascending=False).drop(columns='absolute')  )","4b8bb472":"# sample the first 2500 records only due to computation limit\nsX = x_train[['age', 'ap_hi']][0:2500].to_numpy()\nsY = y_train[0:2500].to_numpy()","0c4bf764":"# function to plot decision boundary\ndef plot_decision_boundary(x, y, model, title):\n    \n    h = 0.02\n    x_min, x_max = x[:, 0].min()-0.1, x[:, 0].max() +0.1\n    y_min, y_max = x[:, 1].min()-0.1, x[:, 1].max() +0.1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n    # Obtain labels for each point in mesh. Use last trained model.\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.clf()\n    plt.imshow(Z, interpolation='nearest',\n               extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n               cmap=plt.cm.Paired,\n               aspect='auto', origin='lower')\n\n    plt.scatter(x[:, 0:1], x[:, 1:2], c=y, edgecolors='k')\n    plt.title(title, fontsize = 20)","9950b560":"# K-means random\nkmean1 = KMeans(n_clusters=2, init='random', random_state=0)\nkmean1.fit(sX)\nplot_decision_boundary(sX, sY, kmean1, \"Prediction Boundary of K-Means\")\nplt.plot(kmean1.cluster_centers_[:, 0], kmean1.cluster_centers_[:, 1], '*', markersize=20, color=\"red\")","eb7c8de8":"# K-means++\nkmean2 = KMeans(n_clusters=2, init='k-means++', random_state=0)\nkmean2.fit(sX)\nplot_decision_boundary(sX, sY, kmean2, \"Prediction Boundary of K-Means ++\")\nplt.plot(kmean2.cluster_centers_[:, 0], kmean2.cluster_centers_[:,1], '*', markersize=20, color=\"red\")","b5b3b582":"# training accuracy scores\nkmeans1_pred = kmean1.predict(sX) # K-means random\nkmeans2_pred = kmean2.predict(sX) # K-means++\nprint('K-means(random) training accuracy: ', accuracy_score(sY, kmeans1_pred))\nprint('K-means++ training accuracy: ', accuracy_score(sY, kmeans2_pred), '\\n')\n\n# testing accuracy scores\nkmeans1_pred_test = kmean1.predict(x_test[['age','ap_hi']]) # K-means random\nkmeans2_pred_test = kmean2.predict(x_test[['age','ap_hi']]) # K-means++\nprint('K-means(random) testing accuracy: ', accuracy_score(y_test, kmeans1_pred_test))\nprint('K-means++ testing accuracy: ', accuracy_score(y_test, kmeans2_pred_test))","b1387076":"# function to plot dengrogram\ndef plot_dendrogram(model, **kwargs): # provided by Mathew Kallada. \n\n    # Children of hierarchical clustering\n    children = model.children_\n\n    # Distances between each pair of children\n    # Since we don't have this information, we can use a uniform one for plotting\n    distance = np.arange(children.shape[0])\n\n    # The number of observations contained in each cluster level\n    no_of_observations = np.arange(2, children.shape[0] + 2)\n\n    # Create linkage matrix and then plot the dendrogram\n    linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)\n    \n    sch.dendrogram(linkage_matrix, **kwargs)","b7c3b904":"# ward's linkage and complete linkage\nH_C_ward = AgglomerativeClustering(n_clusters=2) # default linkage is ward. \nH_C_complete = AgglomerativeClustering(n_clusters=2, linkage='complete')\n\n# dendrogram (on 250 records only)\nhc_ward_pred = H_C_ward.fit_predict(sX[0:250])\nhc_complete_pred = H_C_complete.fit_predict(sX[0:250])\n\nfig = plt.figure(figsize=(25, 10))\nax = fig.add_subplot(1, 2, 1)\nplot_dendrogram(H_C_ward)\nax.set_title('Linkage method is ward')\n\nax = fig.add_subplot(1, 2, 2)\nZ2 = plot_dendrogram(H_C_complete)\nax.set_title('Linkage method is complete')\nplt.show()","fe3c717d":"# accuracy score (on 2500 records)\nhc_ward_pred = H_C_ward.fit_predict(sX)\nhc_complete_pred = H_C_complete.fit_predict(sX)\n\nprint(\"ward's linkage training accuracy: \", accuracy_score(sY, hc_ward_pred))\nprint('complete linkage training accuracy: ', accuracy_score(sY, hc_complete_pred))","453f95c1":"# DBSCAN\ndbscan = DBSCAN(eps=0.26, min_samples=20)\ndbscan_pred = dbscan.fit_predict(sX)\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(dbscan_pred)) - (1 if -1 in dbscan_pred else 0)\nn_noise_ = list(dbscan_pred).count(-1)\n\nprint('Estimated number of clusters: %d' % n_clusters_)\nprint('Estimated number of noise points: %d' % n_noise_)","bc5dc6d6":"# plot\nplt.scatter(x=sX[:,0], y=sX[:,1], c=dbscan_pred, edgecolors='k')\nplt.show()\n\n# accuracy score\nprint(\"DBSCAN training accuracy: \", accuracy_score(sY, dbscan_pred))","8501a151":"# Gaussian Mixture\ngmm = GaussianMixture(n_components=2, covariance_type='full', max_iter=20, random_state=8017) \ngmm.fit(sX)\nplot_decision_boundary(sX, sY, gmm, \"Gaussian Mixture\")","add83b31":"# training accuracy scores\ngmm_pred = gmm.predict(sX)\nprint('Gaussian Mixture Model training accuracy: ', accuracy_score(sY, gmm_pred), '\\n')\n\n# testing accuracy scores\ngmm_pred_test = gmm.predict(x_test[['age','ap_hi']])\nprint('Gaussian Mixture Model testing accuracy: ', accuracy_score(y_test, gmm_pred_test))","ca288301":"# function to calculate entropy score\ndef get_entropy(y, pred, n_class):\n    p = np.zeros((n_class, n_class))\n    tb = contingency_matrix(y, pred)\n    for i in range(n_class):\n        for j in range(n_class):\n            p[i, j] = tb[i, j]\/np.sum(tb[i, :])\n            \n    E = np.zeros((n_class, 1))\n    for i in range(n_class):\n        for j in range(n_class):\n            if (p[i, j] != 0):\n                E[i] = E[i] - p[i, j] * np.log(p[i, j])\n    Entropy = np.dot(np.sum(tb, 1) \/ np.sum(tb), E)\n    return Entropy","f662ebc4":"# accuracy scores of all clustering\nresult = pd.DataFrame({'Model':['K-means (Random)','K-means (K-means++)','Dendrogram (Ward)','Dendrogram (Complete)','DBSCAN','Gaussian Mixture Model'],\n                       'Training Accuracy': [accuracy_score(sY, kmeans1_pred),\n                                             accuracy_score(sY, kmeans2_pred),\n                                             accuracy_score(sY, hc_ward_pred),\n                                             accuracy_score(sY, hc_complete_pred),\n                                             accuracy_score(sY, dbscan_pred),\n                                             accuracy_score(sY, gmm_pred)],\n                       \n                       # external measurement\n                       'Entropy': [get_entropy(sY, kmeans1_pred, 2)[0], \n                                  get_entropy(sY, kmeans2_pred, 2)[0], \n                                  get_entropy(sY, hc_ward_pred, 2)[0], \n                                  get_entropy(sY, hc_complete_pred, 2)[0],\n                                  get_entropy(sY, dbscan_pred, 2)[0], \n                                  get_entropy(sY, gmm_pred, 2)[0]],\n                       'Adjusted Rand Index': [adjusted_rand_score(sY, kmeans1_pred), \n                                              adjusted_rand_score(sY, kmeans2_pred),\n                                              adjusted_rand_score(sY, hc_ward_pred), \n                                              adjusted_rand_score(sY, hc_complete_pred), \n                                              adjusted_rand_score(sY, dbscan_pred), \n                                              adjusted_rand_score(sY, gmm_pred)],\n                       \n                       # internal measurement\n                       'Silhouette Coefficient': [silhouette_score(sX, kmeans1_pred),\n                                                 silhouette_score(sX, kmeans2_pred), \n                                                 silhouette_score(sX, hc_ward_pred),\n                                                 silhouette_score(sX, hc_complete_pred),\n                                                 silhouette_score(sX, dbscan_pred),\n                                                 silhouette_score(sX, gmm_pred)],\n                       'Calinski Harabasz Score': [calinski_harabasz_score(sX, kmeans1_pred),  #Ratio of between-cluster dispersion to within-cluster dispersion\n                                                   calinski_harabasz_score(sX, kmeans2_pred), \n                                                   calinski_harabasz_score(sX, hc_ward_pred),\n                                                   calinski_harabasz_score(sX, hc_complete_pred),\n                                                   calinski_harabasz_score(sX, dbscan_pred),\n                                                   calinski_harabasz_score(sX, gmm_pred)]\n                      })\nresult","d0f60876":"%%time\n\n# parameters candidates\nparameters = {'base_estimator__max_depth': [4,6,8,12,24],\n              'n_estimators': [20, 50, 100, 200]}\n\n# fitting\nBagging_GSCV = GridSearchCV(BaggingClassifier(DecisionTreeClassifier(), random_state=8017), \n                            parameters, n_jobs=-1, verbose=3, return_train_score=True)\nBagging_GSCV.fit(x_train, y_train)\nBagging_model = Bagging_GSCV.best_estimator_\nBagging_model","656c064b":"train_bagging_scores = Bagging_GSCV.cv_results_['mean_train_score']\ntest_bagging_scores = Bagging_GSCV.cv_results_['mean_test_score']\n\n#plt.plot(test_dt_scores, 'go-')\nplt.plot(train_bagging_scores, 'ro--')\nplt.plot(test_bagging_scores, 'ro-')\nplt.ylim(0.4, 1.05)\nplt.xticks(range(20), range(2, 22))\nplt.legend([\"Bagging training score\", \"Bagging test score\"])\nplt.axvline(np.argmax(test_bagging_scores), linestyle=\"dotted\", color=\"red\")\nplt.annotate(np.max(test_bagging_scores).round(4), (np.argmax(test_bagging_scores), np.max(test_bagging_scores)), xycoords=\"data\",\n                 xytext=(-40, 30), textcoords=\"offset pixels\", arrowprops=dict(facecolor=\"black\", shrink=0.1), fontsize=10,\n                 horizontalalignment=\"center\", verticalalignment=\"top\")\nplt.show()","6205f48b":"# accuracy scores\nprint(Bagging_model.get_params())\nprint(f\"Training Score: {round(Bagging_model.score(x_train, y_train),4)}\")\nprint(f\"Testing Score: {round(Bagging_model.score(x_test, y_test),4)}\")","09df137e":"%%time\n\n# parameters candidates\nparameters = {'n_estimators': [20, 50, 100, 200],\n              'max_depth':[6,8,12,24,48]}\n\n# fitting\nRandomForest_GSCV = GridSearchCV(RandomForestClassifier(random_state=8017), \n                                 parameters, n_jobs=-1, verbose=3)\nRandomForest_GSCV.fit(x_train, y_train)\nRandomForest_model = RandomForest_GSCV.best_estimator_\nRandomForest_model","e81d4528":"# accuracy scores\nprint(RandomForest_model.get_params())\nprint(f\"Training Score: {round(RandomForest_model.score(x_train, y_train),4)}\")\nprint(f\"Testing Score: {round(RandomForest_model.score(x_test, y_test),4)}\")","284c5630":"%%time\n\n# parameters candidates\nparameters = {'base_estimator__max_depth': [2,3,4,6,8,12],\n              'n_estimators': [20, 50, 100, 200]}\n\n# fitting\nAdaboost_GSCV = GridSearchCV(AdaBoostClassifier(DecisionTreeClassifier(), random_state=8017), \n                             parameters, n_jobs=-1, verbose=3)\nAdaboost_GSCV.fit(x_train, y_train)\nAdaboost_model = Adaboost_GSCV.best_estimator_\nAdaboost_model","3783f4d8":"# accuracy scores\nprint(Adaboost_model.get_params())\nprint(f\"Training Score: {round(Adaboost_model.score(x_train, y_train),4)}\")\nprint(f\"Testing Score: {round(Adaboost_model.score(x_test, y_test),4)}\")","b4c39f0a":"%%time\n\n# parameters candidates\nparameters = {'max_depth': [2,4,6,8,10,12],\n              'n_estimators': [200],\n              'learning_rate': [0.01]}\n\n# fitting\nGradientBoost_GSCV = GridSearchCV(GradientBoostingClassifier(random_state=8017), \n                             parameters, n_jobs=-1, verbose=3)\nGradientBoost_GSCV.fit(x_train, y_train)\nGradientBoost_model = GradientBoost_GSCV.best_estimator_\nGradientBoost_model","76357eb4":"# accuracy scores\nprint(GradientBoost_model.get_params())\nprint(f\"Training Score: {round(GradientBoost_model.score(x_train, y_train),4)}\")\nprint(f\"Testing Score: {round(GradientBoost_model.score(x_test, y_test),4)}\")","51588e87":"%%time\n\n# parameters candidates\nparameters = {'C': np.logspace(-4, 4, 50)}\n\n# fitting\nLinearSVC_GSCV = GridSearchCV(LinearSVC(dual=False, random_state=8017), \n                              parameters, n_jobs=-1, verbose=3)\nLinearSVC_GSCV.fit(x_train, y_train)\nLinearSVC_model = LinearSVC_GSCV.best_estimator_\nLinearSVC_model\n","3887d3eb":"# accuracy scores\nprint(LinearSVC_model.get_params())\nprint(f\"Training Score: {round(LinearSVC_model.score(x_train, y_train),4)}\")\nprint(f\"Testing Score: {round(LinearSVC_model.score(x_test, y_test),4)}\")","ca6ee3e4":"%%time\n\n# fitting\nSVC_model = SVC(kernel='rbf', random_state=8017)\nSVC_model.fit(x_train, y_train)\nSVC_model","a230d37e":"%%time\n\n# accuracy scores\nprint(SVC_model.get_params())\nprint(f\"Training Score: {round(SVC_model.score(x_train, y_train),4)}\")\nprint(f\"Testing Score: {round(SVC_model.score(x_test, y_test),4)}\")","e784d6f6":"# layer sizes candidates\nls = [x for x in [4,8,16,32]] + [x for x in product([2,4,8], [4,8])] + [x for x in product([2,4,8], [8,16], [4,8])]\nls","c8e36f92":"%%time\n\n# parameters candidates\nparameters = {'hidden_layer_sizes': ls}\n\n# fitting\nMLP_GSCV = GridSearchCV(MLPClassifier(random_state=8017), \n                        parameters, n_jobs=-1, verbose=3)\nMLP_GSCV.fit(x_train, y_train)\nMLP_model = MLP_GSCV.best_estimator_\nMLP_model","c67bf879":"# top 10 cv scores for the MLP candidates\npd.DataFrame(MLP_GSCV.cv_results_).sort_values('rank_test_score').head(10)","0c92eb56":"# scores\nprint(MLP_model)\nprint(f\"Training Score: {round(MLP_model.score(x_train, y_train),4)}\")\nprint(f\"Testing Score: {round(MLP_model.score(x_test, y_test),4)}\")","18592ec8":"# accuracy scores of all models\nresult = pd.DataFrame({'Model':['Decision Tree','Logistic',\n                                'Bagging','Random Forest','Adaboost','Gradient Boost',\n                                'Linear SVC','Non-linear SVC','MLP'],\n                       'Prediction Accuracy': [DecisionTree_model.score(x_test, y_test),\n                                               Logistic_model.score(x_test, y_test),\n                                               Bagging_model.score(x_test, y_test),\n                                               RandomForest_model.score(x_test, y_test),\n                                               Adaboost_model.score(x_test, y_test),\n                                               GradientBoost_model.score(x_test, y_test),\n                                               LinearSVC_model.score(x_test, y_test),\n                                               SVC_model.score(x_test, y_test),\n                                               MLP_model.score(x_test, y_test)]}\n                     )\nresult.sort_values('Prediction Accuracy', ascending=False)","eb9e9a98":"def plot_confusion_matrix(classifier, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    pred_train = classifier.predict_proba(x_train)\n    pred_test = classifier.predict_proba(x_test)\n    acc_train = accuracy_score(y_train, np.argmax(pred_train, 1))\n    acc_test = accuracy_score(y_test, np.argmax(pred_test, 1))\n\n    print(\"Training ACC:\", round(acc_train, 4), \"Testing ACC:\", round(acc_test, 4))\n    cm = confusion_matrix(y_test, np.argmax(pred_test, 1))\n    print(\"Confusion matrix: \\n\", cm)\n    print(\"Testing:\\n\",classification_report(y_test, np.argmax(pred_test, 1), target_names=classes))\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","a301a7be":"plot_confusion_matrix(RandomForest_model, classes=['No','Yes'],\n                      title='Confusion matrix of Random Forest')","17e9673f":"plot_confusion_matrix(DecisionTree_model, classes=['No','Yes'],\n                      title='Confusion matrix of Decision Tree')","e69ede2c":"plot_confusion_matrix(Logistic_model, classes=['No','Yes'],\n                      title='Confusion matrix of Logistic Regression')","351ff43e":"yy_test = label_binarize(y_test, classes=[0, 1])\nplt.figure(figsize=(20, 20))\ndef plot_roc_curve(classifier, label):\n    # Compute ROC curve and ROC area for each class\n    fpr = []\n    tpr = []\n    roc_auc = []\n    pred_test = classifier.predict_proba(x_test)\n    fpr, tpr, _ = roc_curve(yy_test, pred_test[:, 1])\n    roc_auc = auc(fpr, tpr)\n    \n    plt.plot(fpr, tpr, label=label+(' (area = %0.4f)' % roc_auc))\ndef show_roc_curve():\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()","3e989775":"plot_roc_curve(RandomForest_model, label='Random Forest')\nplot_roc_curve(DecisionTree_model, label='Decision Tree')\nplot_roc_curve(Logistic_model, label='Logistic Regression')\nshow_roc_curve()","3ab2d776":"### DBSCAN","07f93306":"### Examining the variables:","f7c1dfe0":"### Clustering Performance","3b756011":"# ROC Curve","0b263747":"# Data Transformation & Train-Test Splitting ","b3d86782":"# Classification Report & Confusion Matrix","19358425":"# MLP","5f69719f":"# Data Cleaning","3f478dce":"**The data is balanced. There is a fairly even split between individuals with the disease and without the disease.**","f51e4115":"### Data cleaning result:","d14b64f8":"# Compare All Models","8d168bdb":"# Ensemble Methods","a6e1bcff":"**K-means has the highest training accuracy.**\n\n**Dendrogram (Complete) works the best in entropy, while K-means (Random) works the best on adjusted rand index.**\n\n**It can be observed that Dendrogram (Complete) works the best in silhouette score, while K-means (K-means++) works the best regarding calinski harabasz score.**","9249d227":"# Logistic Regression","8d484b59":"# Decision Tree","e013891c":"### Bagging Classifier","b8cc93a4":"### Gradient Boosting","b7e024b8":"### Adaboost","86b55cde":"### Random Forest Classifier","3b4ffda6":"### Non-Linear SVC","abb864f1":"# Support Vector Machine","40247fe1":"**AP_HI and AP_LO : Blood pressure should always be positive, not exceeding a certain threshold (300). AP_HI > AP_LO checking should be enforced.**","e5a50cb9":"### Agglomerative Clustering","b88bd1fe":"### K-means","cd5aa457":"**Height and Weight : Using BMI as an indicator to remove records that do not make sense.**","0caef149":"**None of the variables have missing values.**","6942f4fe":"**Age : Converting to years for ease of understanding.**","629a9fc6":"# Pairplots by gender","e8e9ff3f":"**Gender: Converting female to 0 and male to 1**","cee1b7c7":"### Check missing values:","e69398ec":"### Gaussian Mixture","1a88a7fd":"### Linear SVC","10715db5":"# Visualizing Variables","74c55bf5":"# Cluster Analysis\n**Using the two most important features: `age` and `ap_hi`**","61a1497f":"# Correlation heatmap"}}