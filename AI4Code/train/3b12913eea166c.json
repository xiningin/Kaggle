{"cell_type":{"cfa95aa7":"code","9f3fd46b":"code","48d84fdb":"code","3ae7de27":"code","99f9fd4b":"code","8ce3ddef":"code","eddca175":"code","a839452d":"code","035497bc":"code","329884bc":"code","49673ad1":"code","9123885a":"code","64065c78":"code","1f5ea650":"code","731f2584":"code","2bac6a27":"code","b8865d5e":"code","ccd02f64":"code","8de6c60b":"code","24e1c167":"code","9131bb98":"code","2d211cc8":"code","634ec9c3":"code","480abdfe":"code","44e35e76":"code","a928aa8c":"code","ff020b00":"code","4ecc39f6":"code","68f8af6b":"code","8467bf3b":"code","ee64c564":"code","d932f365":"code","af38f035":"code","2387e78f":"code","691f14f5":"code","a8f0448d":"code","3ceb399c":"code","5a9d1a78":"code","08baa867":"code","93f6f086":"code","afdc595a":"code","c77add9b":"code","c480fe8d":"code","7ec197ab":"code","c6e4891f":"code","9c85bff6":"code","143a576f":"code","2985a609":"code","0eaf3449":"code","e59cc9ed":"code","5011794f":"code","9e7bf300":"code","bee21cf2":"code","71e34854":"code","3dd40a0b":"code","4e66a12c":"code","0adf601e":"code","7dd52781":"code","b97355ef":"code","039bc1ab":"code","a507733f":"code","33c8086c":"code","f40f0ea4":"code","97228bbf":"code","96f42b30":"code","fdff235f":"code","9f2d2084":"markdown","4e60efbf":"markdown","5ec4efa4":"markdown","7947bc85":"markdown","62b0ab99":"markdown","51e7bf61":"markdown","3efa9829":"markdown","c7eb948e":"markdown","41546edb":"markdown","92d9b8d6":"markdown","d8d05dd7":"markdown","a6015196":"markdown","12789793":"markdown","995c1615":"markdown","23fc66af":"markdown","f086927d":"markdown","84fcf6f4":"markdown","020fbd5c":"markdown","dd422fc0":"markdown","4d4bece3":"markdown","0adf53dd":"markdown","c9f5d549":"markdown","fe9c70a0":"markdown","907ad4ae":"markdown","c935160d":"markdown","71bf4775":"markdown","0a442dd1":"markdown","99a77c17":"markdown","752ca4f9":"markdown","1601e705":"markdown"},"source":{"cfa95aa7":"import os\nimport pandas as pd","9f3fd46b":"path = \"..\/input\/human-activity-recognition-with-smartphones\/\"","48d84fdb":"df_train = pd.read_csv(os.path.join(path,'train.csv'))\ndf_test = pd.read_csv(os.path.join(path,'test.csv'))","3ae7de27":"df_train.head()","99f9fd4b":"df_train.shape","8ce3ddef":"df_test.shape","eddca175":"df_train['Activity'].value_counts()","a839452d":"df_train.columns","035497bc":"df_train.describe()","329884bc":"## Checking for Duplicates","49673ad1":"print(\"Total Duplicates Train: {} \\n\".format(sum(df_train.duplicated())))\nprint(\"Total Duplicates in Test: {} \\n\".format(sum(df_test.duplicated())))","9123885a":"## Checking for null values","64065c78":"print(\"Total Null values in Train: {}\\n\".format(df_train.isnull().values.sum()))\nprint(\"Total Null values in Test: {} \\n\".format(df_test.isnull().values.sum()))","1f5ea650":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","731f2584":"plt.figure(figsize = (15,8))\nplt.title('Subjects')\nsns.countplot(x = 'subject', data = df_train);","2bac6a27":"plt.figure(figsize = (16,8))\nplt.title(\"Subject with Each Activity\")\nsns.countplot(hue = 'Activity', x='subject',data = df_train);\nplt.show()","b8865d5e":"plt.figure(figsize = (12,8))\nsns.countplot(x = 'Activity', data = df_train);","ccd02f64":"df_train.head()","8de6c60b":"columns = df_train.columns\n\n## Removing ()\n\ncolumns = columns.str.replace('[()]','')\ncolumns = columns.str.replace('[-]','')\ncolumns = columns.str.replace('[,]','')\n","24e1c167":"df_train.columns = columns\ndf_test.columns = columns","9131bb98":"df_train.columns","2d211cc8":"sns.set_palette(\"Set1\", desat=0.80)\nfacetgrid = sns.FacetGrid(df_train, hue='Activity', size=6,aspect=2)\nfacetgrid.map(sns.distplot,'tBodyAccMagmean', hist=False)\\\n    .add_legend()\nplt.annotate(\"Stationary Activities\", xy=(-0.956,17), xytext=(-0.9, 23), size=20,\\\n            va='center', ha='left',\\\n            arrowprops=dict(arrowstyle=\"simple\",connectionstyle=\"arc3,rad=0.1\"))\n\nplt.annotate(\"Moving Activities\", xy=(0,3), xytext=(0.2, 9), size=20,\\\n            va='center', ha='left',\\\n            arrowprops=dict(arrowstyle=\"simple\",connectionstyle=\"arc3,rad=0.1\"))\nplt.show()","634ec9c3":"## \n\nplt.figure(figsize = (12,8))\nplt.subplot(1,2,1)\nplt.title(\"Static Activities (closer view)\")\nsns.distplot(df_train[df_train[\"Activity\"]==\"SITTING\"]['tBodyAccMagmean'], hist = False, label = 'Sitting');\nsns.distplot(df_train[df_train[\"Activity\"]==\"STANDING\"]['tBodyAccMagmean'], hist = False, label = 'Standing');\nsns.distplot(df_train[df_train[\"Activity\"]==\"LAYING\"]['tBodyAccMagmean'], hist = False, label = 'Laying');\nplt.axis([-1.02, -0.5, 0, 35])\nplt.subplot(1,2,2)\nplt.title(\"Dynamic Activities (closer view)\")\nsns.distplot(df_train[df_train[\"Activity\"]==\"WALKING\"][\"tBodyAccMagmean\"], hist = False, label =\"Sitting\");\nsns.distplot(df_train[df_train[\"Activity\"]==\"WALKING_UPSTAIRS\"]['tBodyAccMagmean'], hist = False, label = 'Laying');","480abdfe":"plt.figure(figsize = (10,7))\nsns.boxplot(x = 'Activity', y ='tBodyAccMagmean', data = df_train, showfliers = False);\nplt.ylabel('Body Acceleration Magnitude mean')\nplt.title('Boxplot of tBodyAccMagmean column across various activities')\nplt.axhline(y =- 0.7, xmin = 0.1, xmax = 0.9, dashes = (3,3))\nplt.axhline(y = 0.020, xmin = 0.4, dashes = (3,3))\nplt.xticks(rotation = 90)\nplt.show()","44e35e76":"plt.figure(figsize = (10,7))\nsns.boxplot(x = 'Activity', y = 'angleXgravityMean', data = df_train, showfliers = False)\nplt.axhline(y = 0.08, xmin = 0.1 , xmax = 0.9, dashes = (3,3))\nplt.ylabel(\"Angle between X-axis and gravityMean\")\nplt.title(\"Box plot of angleXgravityMean column across various activities\")\nplt.xticks(rotation = 90)\nplt.show()","a928aa8c":"plt.figure(figsize = (10,7))\nsns.boxplot(x = 'Activity', y = 'angleYgravityMean', data = df_train, showfliers = False)\nplt.ylabel(\"Angle between Y-axis and gravityMean\")\nplt.title(\"Box plot of angleYgravitymean column across various activities\")\nplt.xticks(rotation = 90)\nplt.axhline(y = -0.35, xmin = 0.01, dashes = (3,3))\nplt.show()","ff020b00":"from sklearn.manifold import TSNE","4ecc39f6":"X_for_tsne = df_train.drop(['subject','Activity'], axis = 1)","68f8af6b":"%time\ntsne = TSNE(random_state = 42, n_components = 2, verbose = 1, perplexity = 50, n_iter = 1000).fit_transform(X_for_tsne)","8467bf3b":"plt.figure(figsize = (12,8))\nsns.scatterplot(x = tsne[:,0], y = tsne[:,1], hue = df_train[\"Activity\"], palette = \"bright\")","ee64c564":"y_train = df_train.Activity\nX_train = df_train.drop(['subject','Activity'], axis = 1)\ny_test = df_test.Activity\nX_test = df_test.drop(['subject','Activity'], axis = 1)\nprint('Training data size:', X_train.shape)\nprint('Test data size:', X_test.shape)","d932f365":"model_score = pd.DataFrame(columns = (\"Model\",\"Score\"))","af38f035":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","2387e78f":"parameters = {'C':np.arange(10,61,10),'penalty':['l2','l1']}\nlr_classifier = LogisticRegression()\nlr_classifier_rs = RandomizedSearchCV(lr_classifier, param_distributions = parameters, cv = 5, random_state = 42)\nlr_classifier_rs.fit(X_train, y_train)\ny_pred = lr_classifier_rs.predict(X_test)","691f14f5":"lr_accuracy = accuracy_score(y_true = y_test, y_pred = y_pred)\nprint(\"Accuracy using Logisitc Regression:\", lr_accuracy)","a8f0448d":"model_score = model_score.append(pd.DataFrame({'Model':[\"LogisticRegression\"],'Score':[lr_accuracy]}))","3ceb399c":"lr_classifier_rs.best_estimator_","5a9d1a78":"## plotting confusion matrix\n\ndef plot_confusion_matrix(cm, lables):\n    fig, ax = plt.subplots(figsize = (12,8))\n    im = ax.imshow(cm, interpolation = 'nearest', cmap = plt.cm.Blues)\n    ax.figure.colorbar(im, ax = ax)\n    ax.set(xticks = np.arange(cm.shape[1]))\n    yticks = np.arange(cm.shape[0])\n    ylabel = 'True label'\n    xlabel = 'Predicted label'\n    plt.xticks(rotation = 90)\n    thresh = cm.max() \/ 2\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, int(cm[i,j]), ha = \"center\", va = \"center\", color = \"white\" if cm[i,j]> thresh else \"black\")\n            fig.tight_layout()","08baa867":"cm = confusion_matrix(y_test.values, y_pred)\nplot_confusion_matrix(cm, np.unique(y_pred))","93f6f086":"## function to get best random search attributes\n\ndef get_best_randomsearch_results(model):\n    print(\"Best estimator:\", model.best_estimator_)\n    print(\"Best set of parameters:\", model.best_params_)\n    print(\"Best score:\", model.best_score_)","afdc595a":"## getting best random search attributes\n\nget_best_randomsearch_results(lr_classifier_rs)","c77add9b":"from sklearn.svm import LinearSVC","c480fe8d":"parameters = {'C': np.arange(1,12,2)}\nlr_svm = LinearSVC(tol = 0.00005)\nlr_svm_rs = RandomizedSearchCV(lr_svm, param_distributions = parameters, random_state = 42)\nlr_svm_rs.fit(X_train, y_train)\ny_pred = lr_svm_rs.predict(X_test)","7ec197ab":"lr_svm_accuracy = accuracy_score(y_true = y_test, y_pred = y_pred)\nprint(\"Accuracy using Linear SVM:\", lr_svm_accuracy)","c6e4891f":"model_score = model_score.append(pd.DataFrame({'Model':[\"LinearSVM\"],'Score':[lr_svm_accuracy]}))","9c85bff6":"cm = confusion_matrix(y_test.values, y_pred)\nplot_confusion_matrix(cm, np.unique(y_pred))","143a576f":"## getting best random search attributes\nget_best_randomsearch_results(lr_svm_rs)","2985a609":"from sklearn.svm import SVC","0eaf3449":"np.linspace(2,22, 6)","e59cc9ed":"parameters = {'C':[2,4,8,16], 'gamma':[0.125, 0.250, 0.5, 1]}\nkernel_svm = SVC(kernel = 'rbf')\nkernel_svm_rs = RandomizedSearchCV(kernel_svm, param_distributions = parameters, random_state = 42)\nkernel_svm_rs.fit(X_train, y_train)\ny_pred = kernel_svm_rs.predict(X_test)","5011794f":"kernel_svm_accuracy = accuracy_score(y_true = y_test, y_pred = y_pred)\nprint(\"Accuracy using Kernel SVM:\", kernel_svm_accuracy)","9e7bf300":"model_score = model_score.append(pd.DataFrame({'Model':[\"KernelSVM\"],'Score':[kernel_svm_accuracy]}))","bee21cf2":"cm = confusion_matrix(y_test.values, y_pred)\nplot_confusion_matrix(cm, np.unique(y_pred))","71e34854":"## getting best random search attributes\n\nget_best_randomsearch_results(kernel_svm_rs)","3dd40a0b":"from sklearn.tree import DecisionTreeClassifier","4e66a12c":"parameters = {'max_depth':np.arange(2,10,2)}\ndt_classifier = DecisionTreeClassifier()\ndt_classifier_rs = RandomizedSearchCV(dt_classifier,param_distributions=parameters,random_state = 42)\ndt_classifier_rs.fit(X_train, y_train)\ny_pred = dt_classifier_rs.predict(X_test)","0adf601e":"dt_accuracy = accuracy_score(y_true = y_test, y_pred = y_pred)\nprint(\"Accuracy using Decision tree:\", dt_accuracy)","7dd52781":"model_score = model_score.append(pd.DataFrame({'Model':[\"DecisionTrees\"],'Score':[dt_accuracy]}))","b97355ef":"cm = confusion_matrix(y_test.values, y_pred)\nplot_confusion_matrix(cm, np.unique(y_pred))","039bc1ab":"## getting best estimators\n\nget_best_randomsearch_results(dt_classifier_rs)","a507733f":"from sklearn.ensemble import RandomForestClassifier","33c8086c":"params = {'n_estimators': np.arange(20,101,10), 'max_depth':np.arange(2,16,2)}\nrf_classifier = RandomForestClassifier()\nrf_classifier_rs = RandomizedSearchCV(rf_classifier, param_distributions=params,random_state = 42)\nrf_classifier_rs.fit(X_train, y_train)\ny_pred = rf_classifier_rs.predict(X_test)","f40f0ea4":"rf_accuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy using Random Forest:\", rf_accuracy)","97228bbf":"model_score = model_score.append(pd.DataFrame({'Model':[\"RandomForest\"],'Score':[rf_accuracy]}))","96f42b30":"cm = confusion_matrix(y_test.values, y_pred)\nplot_confusion_matrix(cm, np.unique(y_pred))","fdff235f":"model_score.head()","9f2d2084":"### 2.2 Checking for NaN\/null values and Duplicates","4e60efbf":"We can see, some () 'bracket' between the feature's name. We will remove all these brackets quickly. So it's easier for us to type correctly later.","5ec4efa4":"We can clearly see, each subjects has almost equal or less amount of data. There is no any huge amount of gap between them.","7947bc85":"#### 5.4 Decision tree model with Hyperparameter tuning and cross validation","62b0ab99":"#### 3.3 Analysing Angle between Y-axis and gravityMean feature","51e7bf61":"### 2.4 Correcting some feature names\n","3efa9829":"## 5. Models, HyperparamterTuning and Cross Validations\n- Logistic Regression \n- Linear SVM\n- Kernel SVM\n- Decision Tree\n- Random Forest\n\n","c7eb948e":"We will also, use box plot to visulaize","41546edb":".","92d9b8d6":"#### 5.5 Random Forest model using Hyperparameter tuning and cross validation","d8d05dd7":"#### 4.2 Linear SVM model with Hyperparameter tuning and cross validation","a6015196":"#### 3.4 Visualizing data using t-SNE\n\nUsing t-SNE data can be visualized from a extermely high dimensional space to a low dimensional space and still it retains lots of actual information. Given training data has 561 unique featuers, using t-SNE let's visualze it to a 2D space.","12789793":"#### 5.1 Logistic regression model with Hyperparameter tuning and cross validation","995c1615":"<b> Observation: <\/b>\n- If angleXgravityMean > 0.01 then Activity is <b> Laying <\/b>\n- We can classify all datapoints belonging to Laying activity with just a single if else statement\n\n","23fc66af":"Also, we can easily seperate WALKING_DOWNSTAIRS activity from others using boxplot.\n\n`` \nif (tBodyAccMagmean > 0.02):\n    Activity = \"WALKING_DOWNSTARIS\"\nelse:\n    Activity = \"others\"\n``\n\nBut still 25% of WALKING_DOWNSTAIRS observations are below 0.02 which are misclassified as others so this condition makes an error of 25% in classification.","f086927d":"<b>Observations:<\/b>\n- Laying is totally different position\n- Walking, Walking_downstaris, Walking_upstairs are some kind of similar so they are clustered together\n- And, Standing and Sitting are also some kind of same position.","84fcf6f4":"Since we have already observed the data and the features. So we will skip the part.","020fbd5c":"#### 2. Stationary and Moving activities are completely different","dd422fc0":"## 2. Dataset Cleaning\n\n- 2.1 Outliers\n- 2.2 Filling null values\n- 2.3 Check for data imbalance","4d4bece3":"## 4. Data Preprocessing\n\n\n\n\n\n\n\n#### 4.1 Splitting training and testing","0adf53dd":"## What's our Plan?\n\n\n### `Outline`\n\n- <b>1. Read Dataset <\/b>\n\n\n- <b>2. Datset Cleaning <\/b>\n    - 2.1 Outliers\n    - 2.2 Filling null values\n    - 2.3 Check for data imbalance\n    - 2.4 Correcting some feature names\n\n\n   \n- <b>3. Exploratory Data Analysis <\/b>\n\n\n- <b>4. Data Preprocessing <\/b>\n    - 4.1 Encoding categorical variables\n    - 4.2 Normalization\n    - 4.3 Split Training and testing\n    \n    \n    \n- <b>5. Models, Hyperparameter Tuning and Cross Validation<\/b>\n    - 5.1 Logistic Regression \n    - 5.2 Naive Bayes \n    - 5.3 K-Nearest Neighbor\n    - 5.4 Decision Tree\n    - 5.5 Random Forest\n    - 5.5 Support Vector Machine\n    \n    \n","c9f5d549":"## 3. Exploratory Data Analysis\n\n\n#### Static and Dynamic Activites\n\n- Static activities are (sit, stand, lie and down) thus there is no any motion of an object. \n- Dynamic activities (Walking, WalkingUpStairs, WalkingDownStairs) motion info will be significant\n\n","fe9c70a0":"Using boxplot agian, we can come with conditions to seperate static activities from dynamic activities.\n\n`` if(tBodyAccMagmean <= -0.8):\n      Activity = \"static\"\n  if(tBodyAccMagmean >= -0.6):\n      Activity = \"dynamic\"\n ``\n ","907ad4ae":"#### 3.2 Analysing Angle between X-axis and gravityMean feature","c935160d":"There is no any possibility of having Outliers. All the values are squeezed between -1 to 1. ","71bf4775":"# Human Activity Recognition\n\nIn this notebook, we are trying to predict the Activity of a user. As you can it is a Muliclassification Problem. This notebook is to build a model that can predict whether a person is `Laying`, `Standing` , `Sitting`, `Walking`, `Walking_upstairs`, or `Walking_downstairs`\n\nInitially, the information in this dataset is the measurements from the accelerometer, gyroscope, magnetometer, and GPS of the smartphone. \n\n#### Data Information \nFrom the website: \n\nhttp:\/\/archive.ics.uci.edu\/ml\/datasets\/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions\n\nThe experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (`WALKING`, `WALKING_UPSTAIRS`, `WALKING_DOWNSTAIRS`, `SITTING`, `STANDING`, `LAYING`) wearing a smartphone <b>(Samsung Galaxy S II) <\/b> on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data.\n\nThe sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings\/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain.\n\n### Let's talk about the features (columns)\n\nWe see, there are `563 individual features(columns)`. \n\n1. The features selected for this database come from the <b> accelerometer <\/b> and <b> gyroscope <\/b> 3-axial raw signals <b> tAcc-XYZ <\/b>. These time domain signals (prefix <b>'t'<\/b> to denote time) were captured at a constant rate of 50 Hz. Then they were filtered using a median filter and a 3rd order low pass Butterworth filter with a corner frequency of 20 Hz to remove noise. \n\n\n2. Similarly, the acceleration signal was then separated into body and gravity acceleration signals <b> (tBodyAcc-XYZ and tGravityAcc-XYZ) <\/b> using another low pass Butterworth filter with a corner frequency of 0.3 Hz. \n\n\n3. Subsequently, the body linear acceleration and angular velocity were derived in time to obtain <b> Jerk signals (tBodyAccJerk-XYZ <\/b> and <b> tBodyGyroJerk-XYZ) <\/b>. Also the magnitude of these three-dimensional signals were calculated using the Euclidean norm `(tBodyAccMag`, `tGravityAccMag`, `tBodyAccJerkMag`, `tBodyGyroMag`, `tBodyGyroJerkMag)`\n\n`jerk is the rate at which an object's acceleration changes with respect to time`\n\n\n4. Finally a Fast Fourier Transform (FFT) was applied to some of these signals producing\n\n`fBodyAcc-XYZ, fBodyAccJerk-XYZ, fBodyGyro-XYZ, fBodyAccJerkMag, fBodyGyroMag, fBodyGyroJerkMag. `\n\n(Note the 'f' to indicate frequency domain signals). \n\nThese signals were used to estimate variables of the feature vector for each pattern:  \n\n\n5. <b>'-XYZ' <\/b> is used to denote 3-axial signals in the X, Y and Z directions.\n\n    - tBodyAcc-XYZ\n    - tGravityAcc-XYZ\n    - tBodyAccJerk-XYZ\n    - tBodyGyro-XYZ\n    - tBodyGyroJerk-XYZ\n    - tBodyAccMag\n    - tGravityAccMag\n    - tBodyAccJerkMag\n    - tBodyGyroMag\n    - tBodyGyroJerkMag\n    - fBodyAcc-XYZ\n    - fBodyAccJerk-XYZ\n    - fBodyGyro-XYZ\n    - fBodyAccMag\n    - fBodyAccJerkMag\n    - fBodyGyroMag\n    - fBodyGyroJerkMag`\n    \n    \n\n6. The set of variables that were estimated from these signals are: \n\n    - `mean()`: Mean value\n    - `std()`: Standard deviation\n    - `mad()`: Median absolute deviation \n    - `max()`: Largest value in array\n    - `min()`: Smallest value in array\n    - `sma()`: Signal magnitude area\n    - `energy()`: Energy measure. Sum of the squares divided by the number of values. \n    - `iqr()`: Interquartile range \n    - `entropy()`: Signal entropy\n    - `arCoeff()`: Autorregresion coefficients with Burg order equal to 4\n    - `correlation()`: correlation coefficient between two signals\n    - `maxInds()`: index of the frequency component with largest magnitude\n    - `meanFreq()`: Weighted average of the frequency components to obtain a mean frequency\n    - `skewness()`: skewness of the frequency domain signal \n    - `kurtosis()`: kurtosis of the frequency domain signal \n    - `bandsEnergy()`: Energy of a frequency interval within the 64 bins of the FFT of each window.\n    - `angle()`: Angle between to vectors.\n    \n\n7. Additional vectors obtained by averaging the signals in a signal window sample. These are used on the angle() variable:\n\n    `gravityMean\n     tBodyAccMean\n     tBodyAccJerkMean\n     tBodyGyroMean\n     tBodyGyroJerkMean`\n \n That's too much information. ","0a442dd1":"Let's take a closer look at them","99a77c17":"### 2.3 Check for imbalanced dataset","752ca4f9":"### 2.1 Oultiers","1601e705":"#### 5.3 Kernel SVM model with Hyperparameter tuning and cross validation"}}