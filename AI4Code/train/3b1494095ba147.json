{"cell_type":{"bc7d0013":"code","9942713c":"code","cae77dfa":"code","d9ebc8da":"code","fad6bfbd":"code","38d59be2":"code","b3216e11":"code","20dfe443":"code","b42c1510":"code","0898975a":"code","273835d0":"code","72f96627":"code","ed4b9fc8":"code","5f087e14":"code","35739362":"code","23029462":"code","e87630df":"code","151884df":"code","90114b11":"code","6e2918ae":"code","3c225546":"code","ac5c4c84":"code","2ae95ebb":"markdown","67d3e8f7":"markdown","c9aee8fa":"markdown","1118733a":"markdown","8f7903a2":"markdown","d8f3be77":"markdown","c2e12543":"markdown","27b4a3e6":"markdown","2521041a":"markdown","c2f40718":"markdown","a99fae32":"markdown","a4889f98":"markdown","610949ee":"markdown","cd1f4fe1":"markdown","1776fdb5":"markdown","5aedc8c3":"markdown","fc026cf2":"markdown","104c5f0c":"markdown","5bfd3b16":"markdown","a4b6ffea":"markdown","b41b9d26":"markdown","bcd22002":"markdown","f0e8d0d6":"markdown","f01e3123":"markdown","bf2b3b60":"markdown","c1ac7608":"markdown","5b229490":"markdown","346a40b0":"markdown","9b61fb88":"markdown","064244bc":"markdown","83d45729":"markdown","8afc5e75":"markdown","ffcd3243":"markdown","f6450a15":"markdown","e058a581":"markdown","5282d3bc":"markdown","b3f74161":"markdown","806e94e7":"markdown","2a8713e9":"markdown","7d443aff":"markdown","4068e842":"markdown","3688544b":"markdown","7ada9e39":"markdown","034799c2":"markdown","235c4223":"markdown","f331fc5a":"markdown","69fa8abf":"markdown","89b0869b":"markdown","cc25af44":"markdown","bc8f2638":"markdown","8a53f53e":"markdown","cca039f2":"markdown","437ca21f":"markdown","fe2fc247":"markdown","71b3480a":"markdown","de59a0f1":"markdown"},"source":{"bc7d0013":"import os\nimport gc\nimport cv2\nimport json\nimport time\n\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom keras.utils import to_categorical\n\nimport seaborn as sns\nimport plotly.express as px\nfrom matplotlib import colors\nimport matplotlib.pyplot as plt\nimport plotly.figure_factory as ff\n\nimport torch\nT = torch.Tensor\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.utils.data import Dataset, DataLoader","9942713c":"SIZE = 1000\nEPOCHS = 50\nCONV_OUT_1 = 50\nCONV_OUT_2 = 100\nBATCH_SIZE = 128\n\nTEST_PATH = Path('..\/input\/abstraction-and-reasoning-challenge\/')\nSUBMISSION_PATH = Path('..\/input\/abstraction-and-reasoning-challenge\/')\n\nTEST_PATH = TEST_PATH \/ 'test'\nSUBMISSION_PATH = SUBMISSION_PATH \/ 'sample_submission.csv'","cae77dfa":"test_task_files = sorted(os.listdir(TEST_PATH))\n\ntest_tasks = []\nfor task_file in test_task_files:\n    with open(str(TEST_PATH \/ task_file), 'r') as f:\n        task = json.load(f)\n        test_tasks.append(task)","d9ebc8da":"Xs_test, Xs_train, ys_train = [], [], []\n\nfor task in test_tasks:\n    X_test, X_train, y_train = [], [], []\n\n    for pair in task[\"test\"]:\n        X_test.append(pair[\"input\"])\n\n    for pair in task[\"train\"]:\n        X_train.append(pair[\"input\"])\n        y_train.append(pair[\"output\"])\n    \n    Xs_test.append(X_test)\n    Xs_train.append(X_train)\n    ys_train.append(y_train)","fad6bfbd":"matrices = []\nfor X_test in Xs_test:\n    for X in X_test:\n        matrices.append(X)\n        \nvalues = []\nfor matrix in matrices:\n    for row in matrix:\n        for value in row:\n            values.append(value)\n            \ndf = pd.DataFrame(values)\ndf.columns = [\"values\"]","38d59be2":"data_path = Path('\/kaggle\/input\/abstraction-and-reasoning-challenge\/')\ntraining_path = data_path \/ 'training'\ntraining_tasks = sorted(os.listdir(training_path))\n\nfor i in [1, 19, 8, 15, 9]:\n\n    task_file = str(training_path \/ training_tasks[i])\n\n    with open(task_file, 'r') as f:\n        task = json.load(f)\n\n    def plot_task(task):\n        \"\"\"\n        Plots the first train and test pairs of a specified task,\n        using same color scheme as the ARC app\n        \"\"\"\n        cmap = colors.ListedColormap(\n            ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n             '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n        norm = colors.Normalize(vmin=0, vmax=9)\n        fig, ax = plt.subplots(1, 4, figsize=(15,15))\n        ax[0].imshow(task['train'][0]['input'], cmap=cmap, norm=norm)\n        width = np.shape(task['train'][0]['input'])[1]\n        height = np.shape(task['train'][0]['input'])[0]\n        ax[0].set_xticks(np.arange(0,width))\n        ax[0].set_yticks(np.arange(0,height))\n        ax[0].set_xticklabels([])\n        ax[0].set_yticklabels([])\n        ax[0].tick_params(length=0)\n        ax[0].grid(True)\n        ax[0].set_title('Train Input')\n        ax[1].imshow(task['train'][0]['output'], cmap=cmap, norm=norm)\n        width = np.shape(task['train'][0]['output'])[1]\n        height = np.shape(task['train'][0]['output'])[0]\n        ax[1].set_xticks(np.arange(0,width))\n        ax[1].set_yticks(np.arange(0,height))\n        ax[1].set_xticklabels([])\n        ax[1].set_yticklabels([])\n        ax[1].tick_params(length=0)\n        ax[1].grid(True)\n        ax[1].set_title('Train Output')\n        ax[2].imshow(task['test'][0]['input'], cmap=cmap, norm=norm)\n        width = np.shape(task['test'][0]['input'])[1]\n        height = np.shape(task['test'][0]['input'])[0]\n        ax[2].set_xticks(np.arange(0,width))\n        ax[2].set_yticks(np.arange(0,height))\n        ax[2].set_xticklabels([])\n        ax[2].set_yticklabels([])\n        ax[2].tick_params(length=0)\n        ax[2].grid(True)\n        ax[2].set_title('Test Input')\n        ax[3].imshow(task['test'][0]['output'], cmap=cmap, norm=norm)\n        width = np.shape(task['test'][0]['output'])[1]\n        height = np.shape(task['test'][0]['output'])[0]\n        ax[3].set_xticks(np.arange(0,width))\n        ax[3].set_yticks(np.arange(0,height))\n        ax[3].set_xticklabels([])\n        ax[3].set_yticklabels([])\n        ax[3].tick_params(length=0)\n        ax[3].grid(True)\n        ax[3].set_title('Test Output')\n        plt.tight_layout()\n        plt.show()\n\n    plot_task(task)","b3216e11":"px.histogram(df, x=\"values\", title=\"Numbers present in matrices\")","20dfe443":"means = [np.mean(X) for X in matrices]\nfig = ff.create_distplot([means], group_labels=[\"Means\"], colors=[\"green\"])\nfig.update_layout(title_text=\"Distribution of matrix mean values\")","b42c1510":"heights = [np.shape(matrix)[0] for matrix in matrices]\nwidths = [np.shape(matrix)[1] for matrix in matrices]","0898975a":"fig = ff.create_distplot([heights], group_labels=[\"Height\"], colors=[\"magenta\"])\nfig.update_layout(title_text=\"Distribution of matrix heights\")","273835d0":"fig = ff.create_distplot([widths], group_labels=[\"Width\"], colors=[\"red\"])\nfig.update_layout(title_text=\"Distribution of matrix widths\")","72f96627":"plot = sns.jointplot(widths, heights, kind=\"kde\", color=\"blueviolet\")\nplot.set_axis_labels(\"Width\", \"Height\", fontsize=14)\nplt.show(plot)","ed4b9fc8":"plot = sns.jointplot(widths, heights, kind=\"reg\", color=\"blueviolet\")\nplot.set_axis_labels(\"Width\", \"Height\", fontsize=14)\nplt.show(plot)","5f087e14":"def replace_values(a, d):\n    return np.array([d.get(i, -1) for i in range(a.min(), a.max() + 1)])[a - a.min()]\n\ndef repeat_matrix(a):\n    return np.concatenate([a]*((SIZE \/\/ len(a)) + 1))[:SIZE]\n\ndef get_new_matrix(X):\n    if len(set([np.array(x).shape for x in X])) > 1:\n        X = np.array([X[0]])\n    return X\n\ndef get_outp(outp, dictionary=None, replace=True):\n    if replace:\n        outp = replace_values(outp, dictionary)\n\n    outp_matrix_dims = outp.shape\n    outp_probs_len = outp.shape[0]*outp.shape[1]*10\n    outp = to_categorical(outp.flatten(),\n                          num_classes=10).flatten()\n\n    return outp, outp_probs_len, outp_matrix_dims","35739362":"class ARCDataset(Dataset):\n    def __init__(self, X, y, stage=\"train\"):\n        self.X = get_new_matrix(X)\n        self.X = repeat_matrix(self.X)\n        \n        self.stage = stage\n        if self.stage == \"train\":\n            self.y = get_new_matrix(y)\n            self.y = repeat_matrix(self.y)\n        \n    def __len__(self):\n        return SIZE\n    \n    def __getitem__(self, idx):\n        inp = self.X[idx]\n        if self.stage == \"train\":\n            outp = self.y[idx]\n\n        if idx != 0:\n            rep = np.arange(10)\n            orig = np.arange(10)\n            np.random.shuffle(rep)\n            dictionary = dict(zip(orig, rep))\n            inp = replace_values(inp, dictionary)\n            if self.stage == \"train\":\n                outp, outp_probs_len, outp_matrix_dims = get_outp(outp, dictionary)\n                \n        if idx == 0:\n            if self.stage == \"train\":\n                outp, outp_probs_len, outp_matrix_dims = get_outp(outp, None, False)\n        \n        return inp, outp, outp_probs_len, outp_matrix_dims, self.y","23029462":"class BasicCNNModel(nn.Module):\n    def __init__(self, inp_dim=(10, 10), outp_dim=(10, 10)):\n        super(BasicCNNModel, self).__init__()\n        \n        CONV_IN = 3\n        KERNEL_SIZE = 3\n        DENSE_IN = CONV_OUT_2\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n        self.dense_1 = nn.Linear(DENSE_IN, outp_dim[0]*outp_dim[1]*10)\n        \n        if inp_dim[0] < 5 or inp_dim[1] < 5:\n            KERNEL_SIZE = 1\n\n        self.conv2d_1 = nn.Conv2d(CONV_IN, CONV_OUT_1, kernel_size=KERNEL_SIZE)\n        self.conv2d_2 = nn.Conv2d(CONV_OUT_1, CONV_OUT_2, kernel_size=KERNEL_SIZE)\n\n    def forward(self, x, outp_dim):\n        x = torch.cat([x.unsqueeze(0)]*3)\n        x = x.permute((1, 0, 2, 3)).float()\n        self.conv2d_1.in_features = x.shape[1]\n        conv_1_out = self.relu(self.conv2d_1(x))\n        self.conv2d_2.in_features = conv_1_out.shape[1]\n        conv_2_out = self.relu(self.conv2d_2(conv_1_out))\n        \n        self.dense_1.out_features = outp_dim\n        feature_vector, _ = torch.max(conv_2_out, 2)\n        feature_vector, _ = torch.max(feature_vector, 2)\n        logit_outputs = self.dense_1(feature_vector)\n        \n        out = []\n        for idx in range(logit_outputs.shape[1]\/\/10):\n            out.append(self.softmax(logit_outputs[:, idx*10: (idx+1)*10]))\n        return torch.cat(out, axis=1)","e87630df":"def transform_dim(inp_dim, outp_dim, test_dim):\n    return (test_dim[0]*outp_dim[0]\/inp_dim[0],\n            test_dim[1]*outp_dim[1]\/inp_dim[1])\n\ndef resize(x, test_dim, inp_dim):\n    if inp_dim == test_dim:\n        return x\n    else:\n        return cv2.resize(flt(x), inp_dim,\n                          interpolation=cv2.INTER_AREA)\n\ndef flt(x): return np.float32(x)\ndef npy(x): return x.cpu().detach().numpy()\ndef itg(x): return np.int32(np.round(x))","151884df":"idx = 0\nstart = time.time()\ntest_predictions = []\n\nfor X_train, y_train in zip(Xs_train, ys_train):\n    print(\"TASK \" + str(idx + 1))\n\n    train_set = ARCDataset(X_train, y_train, stage=\"train\")\n    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n\n    inp_dim = np.array(X_train[0]).shape\n    outp_dim = np.array(y_train[0]).shape\n    network = BasicCNNModel(inp_dim, outp_dim).cuda()\n    optimizer = Adam(network.parameters(), lr=0.01)\n    \n    for epoch in range(EPOCHS):\n        for train_batch in train_loader:\n            train_X, train_y, out_d, d, out = train_batch\n            train_preds = network.forward(train_X.cuda(), out_d.cuda())\n            train_loss = nn.MSELoss()(train_preds, train_y.cuda())\n            \n            optimizer.zero_grad()\n            train_loss.backward()\n            optimizer.step()\n\n    end = time.time()        \n    print(\"Train loss: \" + str(np.round(train_loss.item(), 3)) + \"   \" +\\\n          \"Total time: \" + str(np.round(end - start, 1)) + \" s\" + \"\\n\")\n    \n    X_test = np.array([resize(flt(X), np.shape(X), inp_dim) for X in Xs_test[idx-1]])\n    for X in X_test:\n        test_dim = np.array(T(X)).shape\n        test_preds = npy(network.forward(T(X).unsqueeze(0).cuda(), out_d.cuda()))\n        test_preds = np.argmax(test_preds.reshape((10, *outp_dim)), axis=0)\n        test_predictions.append(itg(resize(test_preds, np.shape(test_preds),\n                                           tuple(itg(transform_dim(inp_dim,\n                                                                   outp_dim,\n                                                                   test_dim))))))\n    idx += 1","90114b11":"def flattener(pred):\n    str_pred = str([row for row in pred])\n    str_pred = str_pred.replace(', ', '')\n    str_pred = str_pred.replace('[[', '|')\n    str_pred = str_pred.replace('][', '|')\n    str_pred = str_pred.replace(']]', '|')\n    return str_pred","6e2918ae":"test_predictions = [[list(pred) for pred in test_pred] for test_pred in test_predictions]\n\nfor idx, pred in enumerate(test_predictions):\n    test_predictions[idx] = flattener(pred)\n    \nsubmission = pd.read_csv(SUBMISSION_PATH)\nsubmission[\"output\"] = test_predictions","3c225546":"submission.head()","ac5c4c84":"submission.to_csv(\"submission.csv\", index=False)","2ae95ebb":"### Convert submission to .csv format","67d3e8f7":"From the above graph, we can see that matrix widths also have a uniform distribution (with significantly less skew). The distribution is also somewhat uniform with a mean of approximately 16.","c9aee8fa":"## Look at a few train\/test input\/output pairs <a id=\"look-at-few\"><\/a>\n\nThese are some of the pairs present in the training data. I use functions from Walter's excellent starter kernel to plot these pairs.","1118733a":"### Define function to flatten submission matrices","8f7903a2":"## Loss (MSE) <a id=\"loss\"><\/a>","d8f3be77":"## Backpropagation and optimization (Adam) <a id=\"backprop\"><\/a>","c2e12543":"# My approach <a id=\"my-approach\"><\/a>","27b4a3e6":"## Load the ARC data <a id=\"load-the-arc-data\"><\/a>","2521041a":"<img src=\"https:\/\/i.imgur.com\/I3n0Q2k.png\" width=\"750px\">","c2f40718":"The basic steps in my data processing pipeline are given above. These steps can be summarized as:\n\n1. **Handle matrix (input and output) dimensions:** Ensure consistent dimensions among inputs and outputs\n2. **Randomly augment input and output matrices:** Mutate the matrix values in order to generate new data for each task\n3. **Return input-output pairs along with dimension information:** Return the X-y data with dimensions","a99fae32":"<img src=\"https:\/\/i.imgur.com\/ott07Lh.png\" width=\"750px\">","a4889f98":"## Matrix mean values <a id=\"matrix-mean-values\"><\/a>","610949ee":"# Ending note <a id=\"ending-note\"><\/a>\n\n<font color=\"red\" size=4>Please upvote this kernel if you like it. It motivates me to produce more quality content :)<\/font>","cd1f4fe1":"In the above diagram, we can see that Newton's Chain rule is used to calculate the gradient of the loss function *w.r.t.* the parameters in the model.","1776fdb5":"My approach to this problem imvolves simple data augmentation techniques and a supervised 2D CNN model to make predictions. The model takes a 2D matrix as input and outputs the softmax probabilities of different values occuring in the output matrix. But since we have only few training examples for each task, I create new input-output pairs by randomly switching colors. The extra augmented data helps the model capture patterns more easily.","5aedc8c3":"### Prepare submission dataframe","fc026cf2":"From the above graph, we can clearly see that the number distribution has a string positive skew. Most numbers in the matrices are clearly 0. This is reflected by the dominance of black color in most matrices.","104c5f0c":"<img src=\"https:\/\/i.imgur.com\/rD53yoI.png\" width=\"350px\">","5bfd3b16":"### The idea behind a loss function","a4b6ffea":"In my postprocessing, I follow the steps given below:\n\n1. **Get output probabilites from the CNN model:** \n\n<code>npy(network.forward(T(X).unsqueeze(0), out_d))<\/code>\n2. **Perform argmax on probabilities to get indices of maximum prbabilities:**\n\n<code>np.argmax(test_preds.reshape((10, *outp_dim)), axis=0)<\/code>\n3. **Resize the output matrix to match the dimension ratios and round off:**\n\n<code>itg(resize(test_preds, np.shape(test_preds), tuple(itg(transform_dim(inp_dim, outp_dim, test_dim))))))<\/code>","b41b9d26":"## Number frequency <a id=\"number-frequency\"><\/a>","bcd22002":"From the above graph, we can see that lower means are more common than higher means. The graph, once again, has a strong positive skew. This is further proof that black is the most dominant color in the matrices.","f0e8d0d6":"## Data processing <a id=\"data-processing\"><\/a>","f01e3123":"From the above graph, we can see that matrix heights have a much more uniform distribution (with significantly less skew). The distribution is somewhat normal with a mean of approximately 15.","bf2b3b60":"### Get testing tasks","c1ac7608":"<img src=\"https:\/\/i.imgur.com\/IEeg94y.png\" width=\"550px\">","5b229490":"<img src=\"https:\/\/i.imgur.com\/WeQbG9M.png\" width=\"275px\">","346a40b0":"The general update equation above is used to optimize the parameters using the gradients calculated above. Note that more complex algorithms like Adam use more complex update equations than the ones specified above.\n\nIn the code, the line <code>train_loss.backward()<\/code> and <code>optimizer.step()<\/code> perform backpropagation and optimization respectively.","9b61fb88":"Welcome to the Abstraction and Reasoning Challenge (ARC), a potential major step towards achieving artificial general intelligence (AGI). In this competition, we are challenged to build an algorithm that can perform reasoning tasks it has never seen before. Classic machine learning problems generally involve one specific task which can be solved by training on millions of data samples. But in this challenge, we need to build an algorithm that can learn patterns from a minimal number of examples.\n\nIn this notebook, I will be demonstrating how one can use **data augmentation** and **supervised machine learning** to build a baseline model to solve this problem.\n\n<font color=\"red\" size=3>Please upvote this kernel if you like it. It motivates me to produce more quality content :)<\/font>","064244bc":"## Height vs. Width <a id=\"height-vs-width\"><\/a>","83d45729":"# Submission <a id=\"submission\"><\/a>","8afc5e75":"From the above graphs, we can see that heights and widths have a strong positive correlation, *i.e.* greater widths generally result in greater heights. This is consistent with the fact that most matrices are square-shaped.","ffcd3243":"### Train the model and postprocess probabilties","f6450a15":"## Matrix widths <a id=\"matrix-widths\"><\/a>","e058a581":"# Introduction\n\n<img src=\"https:\/\/i.imgur.com\/x2XjjFV.jpg\" width=\"250px\">","5282d3bc":"### PyTorch CNN model","b3f74161":"### Helper functions","806e94e7":"# Modeling <a id=\"modeling\"><\/a>","2a8713e9":"# Contents\n\n* [<font size=4>Preparing the ground<\/font>](#preparing-the-ground)\n    * [Import libraries and define hyperparameters](#import-libraries-and-define-hyperparameters)\n    * [Load the ARC data](#load-the-arc-data)\n    \n\n* [<font size=4>Basic exploration<\/font>](#basic-exploration)\n    * [Look at few train\/test input\/output pairs](#look-at-few)\n    * [Number frequency](#number-frequency)\n    * [Matrix mean values](#matrix-mean-values)\n    * [Matrix heights](#matrix-heights)\n    * [Matrix widths](#matrix-widths)\n    * [Height vs. Width](#height-vs-width)\n    \n\n* [<font size=4>My approach<\/font>](#my-approach)\n    * [Data processing](#data-processing)\n    * [Modeling](#modeling)\n\n\n* [<font size=4>Training and postprocessing<\/font>](#training-and-postprocessing)\n    * [Loss (MSE)](#loss)\n    * [Backpropagation and optimization (Adam)](#backprop)\n\n\n* [<font size=4>Submission<\/font>](#submission)\n\n\n* [<font size=4>Ending note<\/font>](#ending-note)","7d443aff":"<img src=\"https:\/\/i.imgur.com\/cpUtXRR.png\" width=\"600px\">","4068e842":"<img src=\"https:\/\/i.imgur.com\/pa9C1rz.png\" width=\"400px\">","3688544b":"### PyTorch DataLoader","7ada9e39":"# Basic exploration <a id=\"basic-exploration\"><\/a>","034799c2":"<img src=\"https:\/\/i.imgur.com\/H96WieH.png\" width=\"300px\">","235c4223":"# Preparing the ground <a id=\"preparing-the-ground\"><\/a>","f331fc5a":"It can be seen from the above diagram that the same training pairs are augmented (100s of times) to produce a large dataset. This dataset is used to train the CNN for each task. The CNN predicts a probability distribution over the \"pixels\" or values in the matrix. This probability distribution is used to generate the final output matrix.\n\nThe trained CNN model can be used to make predictions on the test samples as follows:","69fa8abf":"I train the model using PyTorch's autograd functionality. Specifically, I use the **Adam** optimizer and the **MSE** loss function.","89b0869b":"# Training and postprocessing <a id=\"training-and-postprocessing\"><\/a>","cc25af44":"### Helper functions","bc8f2638":"### Extract training and testing data","8a53f53e":"As shown above, the target vector *t* and the output vector *o* are diverging from each other. The loss function measures the degree to which these two diverge, *i.e.* the size of *o - t*. Here, *t* is the actual pixel probability and *o* is the predicted pixel probability. The mean squared error calculates the average squared error between *o* and *t*. \n\nIn the code, the line <code>train_loss = nn.MSELoss()(train_preds, train_y)<\/code> calculates the MSE loss.","cca039f2":"### Train the CNN model on loop","437ca21f":"\n\nI use a basic CNN model that takes 2D input and returns 2D output. The sequential architecture is follows:\n\n1. (Conv2D + ReLU) **x** 2\n2. MaxPool **x** 2\n3. Dense\n4. Softmax\n\nThe softmax probabilities are converted to the final 2D matrix through argmax and resize functions.","fe2fc247":"## Matrix heights <a id=\"matrix-heights\"><\/a>","71b3480a":"<img src=\"https:\/\/i.imgur.com\/yEVIBzj.png\" width=\"350px\">","de59a0f1":"## Import libraries and define hyperparameters <a id=\"import-libraries-and-define-hyperparameters\"><\/a> "}}