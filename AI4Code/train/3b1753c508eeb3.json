{"cell_type":{"35a3827f":"code","732715c2":"code","13a3191c":"code","170f2a13":"code","7241eeb0":"code","7429d4c3":"code","cf6b7d01":"code","90cdd1c3":"code","34325123":"code","becf5cc1":"code","14763dce":"code","63719324":"code","30130814":"code","701e191f":"code","2a8dab22":"code","f36b5ea0":"code","885bb7f2":"code","7ce71955":"code","dfc1742c":"code","5cdc0798":"code","bd01cc69":"markdown","5bef4e49":"markdown","54fd7e0f":"markdown","da1405bf":"markdown","e59e912e":"markdown","98788048":"markdown","cafadf5b":"markdown","c3825bf5":"markdown","6b3641f2":"markdown","6693f902":"markdown","a3da4e8c":"markdown","62c53427":"markdown","4b464ccc":"markdown","e2fedcf3":"markdown","743dceab":"markdown","f708d1ae":"markdown","2ae46957":"markdown"},"source":{"35a3827f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","732715c2":"import zipfile\nimport warnings\nimport pandas as pd\nimport numpy as np\nfrom numpy import mean, std\nimport datatable as dt\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.decomposition import PCA, NMF\n\n#warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\")\n%pylab\n%matplotlib inline","13a3191c":"# Memory saving function credit to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype.name\n\n        if col_type not in ['object', 'category', 'datetime64[ns, UTC]']:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","170f2a13":"%%time\n\ndf_train = (\n    dt.fread('..\/input\/jane-street-market-prediction\/train.csv', max_nrows=200000)\n      .to_pandas()\n      .pipe(reduce_mem_usage)\n)\ndf_train.head()","7241eeb0":"# Drop rows with 'weight'=0 \ndf_train = df_train[df_train['weight']!=0]\n#df_train = df_train.loc[df_train.weight != 0]\n\n# Create 'action' column (target)\n# The objetive is maximise the utility function pi=\u2211j(weightij\u2217respij\u2217actionij) where positive values of resp will increase pi\n#df_train['action'] = df_train['resp'].apply(lambda x:x>0).astype(int)\ndf_train['action'] = (df_train['resp'] > 0).astype('int')\n\n#Select the model\u00b4s variables (features alone)\nfeatures = [c for c in df_train.columns if 'feature' in c]\n#df_train = pd.concat([df_train[features],df_train.action], axis=1)\n\nX = df_train[features]\ny = df_train['action']","7429d4c3":"###INF Values##\nX.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n###NAN Values##\n\n#First estrategy\n#Fill nan values with 0\nX.fillna(0,inplace=True)\n\n#Second estrategy\n#Fill nan values with median\n#X_median = X.median()\n#X = X.fillna(train_median)\n\n\n#Third estrategy\n#Lineal Interpolation\n#X.interpolate(method='linear', inplace=True)\n","cf6b7d01":"X.isnull().sum().sum()","90cdd1c3":"# define the model from PyCaret Optimization\netc = ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n                     criterion='gini', max_depth=None, max_features='auto',\n                     max_leaf_nodes=None, max_samples=None,\n                     min_impurity_decrease=0.0, min_impurity_split=None,\n                     min_samples_leaf=1, min_samples_split=2,\n                     min_weight_fraction_leaf=0.0, n_estimators=100,\n                     oob_score=False, verbose=0,\n                     warm_start=False)","34325123":"%%time\n\npca= PCA()\net = ExtraTreesClassifier()\n\npipe_pca = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler()),\n    ('pca', pca),\n    ('model', et)])\n\n# Parameters of pipelines can be set using \u2018__\u2019 separated parameter names:\nparam_grid = {\n    'pca__n_components': range(4,16,4),\n}\n\nsearch = GridSearchCV(pipe_pca, param_grid=param_grid, n_jobs=-1)\nsearch.fit(X, y)\nprint(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\nprint(search.best_params_)","becf5cc1":"%%time\n\npca= PCA(n_components=12)\n#et = ExtraTreesClassifier(criterion='entropy', random_state=0, n_estimators=100, max_depth=75)\n\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n\npipe_pca = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')), #, fill_value = 0\n    ('scaler', StandardScaler()),\n    ('pca', pca),\n    ('model', etc)])\n\n# Parameters of pipelines can be set using name +\u2018__\u2019+ parameter names:\nparam_grid = {\n    #'pca__n_components': range(4,16,4),\n    'model__criterion': ['entropy'],#['gini', 'entropy'], #grid.best_params_ = {'criterion': 'entropy'}\n    'model__n_estimators': [100],#range(25,125,25),#grid.best_params_ = {'n_estimators': 100}\n    'model__max_depth': [75], #range(1,100,25),#grid.best_params_ = {'max_depth': 100}\n    #'max_features': [100],#range(50,150,50),#grid.best_params_ = {'n_estimators': 100}\n    #'model__min_samples_leaf': range(1,30,10),#grid.best_params_ = {'min_samples_leaf': 1}\n     #'min_samples_split': range(1,50,5),\n    #'et__random_state': [0],\n}\n\net_pca = GridSearchCV(pipe_pca, param_grid=param_grid, n_jobs=-1, cv=cv)\net_pca.fit(X, y)\nprint(\"Best parameter (CV score=%0.3f):\" % et_pca.best_score_)\nprint(et_pca.best_params_)","14763dce":"ExtraTreesClassifier().get_params().keys()","63719324":"%%time\n\ndf_all = (\n    dt.fread('..\/input\/jane-street-market-prediction\/train.csv')\n      .to_pandas()\n      .pipe(reduce_mem_usage)\n)\ndf_all.head()","30130814":"# Drop rows with 'weight'=0 \ndf_all = df_all[df_all['weight']!=0]\n#df_train = df_train.loc[df_train.weight != 0]\n\n# Create 'action' column (target)\n# The objetive is maximise the utility function pi=\u2211j(weightij\u2217respij\u2217actionij) where positive values of resp will increase pi\n#df_train['action'] = df_train['resp'].apply(lambda x:x>0).astype(int)\ndf_all['action'] = (df_all['resp'] > 0).astype('int')\n\n#Select the model\u00b4s variables (features alone)\nfeatures = [c for c in df_all.columns if 'feature' in c]\n#df_train = pd.concat([df_train[features],df_train.action], axis=1)\n\nX_all = df_all[features]\ny_all = df_all['action']\n\n###INF Values##\nX_all.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n###NAN Values##\n\n#Fill nan values with 0\nX_all.fillna(0,inplace=True)","701e191f":"# Train and test data\ntrain_x, test_x, train_y, test_y = train_test_split(X_all, y_all, test_size=0.3)","2a8dab22":"from sklearn.metrics import accuracy_score, auc, confusion_matrix, f1_score, precision_score, recall_score, roc_curve\n\ndef metrics_models(y_true, y_pred):\n    from sklearn.metrics import accuracy_score, auc, confusion_matrix, f1_score, precision_score, recall_score, roc_curve\n\n    # Obtenci\u00f3n de matriz de confusi\u00f3n\n    confusion_matrix = confusion_matrix(y_true, y_pred)\n\n    print(\"La matriz de confusi\u00f3n es \")\n    print(confusion_matrix)\n\n    print('Precisi\u00f3n:', accuracy_score(y_true, y_pred))\n    print('Exactitud:', precision_score(y_true, y_pred))\n    print('Exhaustividad:', recall_score(y_true, y_pred))\n    print('F1:', f1_score(y_true, y_pred))\n\n    false_positive_rate, recall, thresholds = roc_curve(y_true, y_pred)\n    roc_auc = auc(false_positive_rate, recall)\n\n    print('AUC:', auc(false_positive_rate, recall))\n\n    plot(false_positive_rate, recall, 'b')\n    plot([0, 1], [0, 1], 'r--')\n    title('AUC = %0.2f' % roc_auc)","f36b5ea0":"#Test with train and test set to see if there is overfit in model performance\ny_pred = et_pca.predict(train_x)\n\nmetrics_models(train_y, y_pred)","885bb7f2":"#We see if there is overfit in the df test\ny_pred_test = et_pca.predict(test_x)\nmetrics_models(test_y, y_pred_test)","7ce71955":"#free some space\ndel train_x, test_x, train_y, test_y, df_train","dfc1742c":"from tqdm import tqdm\nimport janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set","5cdc0798":"#Reference:https:\/\/www.kaggle.com\/vivekanandverma\/eda-xgboost-hyperparameter-tuning\nfor (test_df, pred_df) in tqdm(iter_test):\n    if test_df['weight'].item() > 0:\n        X_test = test_df.loc[:, test_df.columns.str.contains('feature')]\n        y_preds = et_pca.predict(X_test)\n        pred_df.action = y_preds\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","bd01cc69":"## Preprocesing \u2699  \n\n* We calculate the target from the variable 'resp'\n* We erase cases where the weight is equal to 0. Weighted operations 0 were intentionally included in the dataset to be complete, although such operations will not contribute to the assessment of the score. Then we'll ignore it.","5bef4e49":"We observe what dimensionality with the PCA method and hyperparameters of the ETC model can give us the best performance. Test the model with cross-validation ([RepeatedStratifiedKFold](https:\/\/medium.com\/@venkatasujit272\/overview-of-cross-validation-3785d5414ece)). It is exciting to use in this case all the data because the cross-validation itself divides the data into training and validation.  \nSeveral iterations are performed with different parameters so that the process consumes less memory.\n\n### We automate the process with Pipeline\nTo apply PCA method, we need to normalize the data so we introduce in the pipeline the allocation of the median to the possible missing data (although we already process it previously) and normalize the data, then apply the dimensionality reduction and finally the selected prediction method.","54fd7e0f":"## Reducing dimensionality || PCA","da1405bf":"## **Libraries** \ud83d\udcd8","e59e912e":"## Market Prediction Using ExtraTreesClassifier \ud83d\ude80","98788048":"We generate a function to be able to automate model metrics","cafadf5b":"## Define model \ud83d\udd0d || ExtraTreesClassifier  \n\nThe choice of this model is defined by the performance comparison we developed from this [Notebook](https:\/\/www.kaggle.com\/jlfdatascience\/automl-approach-model-selection)  \n\n![image.png](attachment:image.png)![](http:\/\/..\/input\/images\/Compare_accuracy_Model.jpg)","c3825bf5":"## **Loading and Reduce Memory** \ud83d\udcbe \n\nWe load the data, trying to reduce the memory overload  \nReference: https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage","6b3641f2":"## EDA \ud83d\udd2c\nT\nhe goal of this notebook is not to do a thorough data exploration, as there are great jobs that develop this task. Based on these findings, we will try to develop a quick method optimized to predict Jane Street's challenge\n\nReference:https:\/\/www.kaggle.com\/muhammadmelsherbini\/jane-street-extensive-eda","6693f902":"## Submitting","a3da4e8c":"There doesn't seem to be any overfits","62c53427":"* ## Test model in All dataset\n* Load all dataset\n* We divide the dataset into train and test to check the performance of the trained model.","4b464ccc":"### Preprocessing","e2fedcf3":"**Description**: The supplied dataset contains a set of features, feature_, goes 0 ... 129, representing actual stock market data. Each row in the data set represents a trading opportunity, for which we will be predicting a stock value: 1 to make the trade and 0 to pass it. Each operation has an associated weight and resp, which together represents a return on the trade. In the training set, tren.csv, you are given a resp value, as well as several other resp_ values of 1,2,3,4 that represent returns at different time horizons.\n\nIn Test set we don't have resp value, and other resp_{1,2,3,4} data, so we have to use only feature_{0...129} to make prediction.","743dceab":"* treat infinite values\n* treat null values","f708d1ae":"### Contents\n\n* Libraries\n* Loading and Reduce Memory\n* EDA\n* Preprocesing\n* Define model || ExtraTreesClassifier\n* Reducing dimensionality || PCA\n* Test model in All dataset\n* Submitting\n","2ae46957":"In the case of this challenge there is a time limitation for model execution so it is advisable to decrease the dimensionality. To do this we will use PCA and cross-validation what are the optimal component numbers to be able to get better performance."}}