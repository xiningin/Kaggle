{"cell_type":{"64db2ba4":"code","3cb551cf":"code","a838a2fa":"code","4be76606":"code","6bae32f4":"code","faf8ac14":"code","a808e3c9":"code","3141161b":"code","2052cf70":"code","7d0073dd":"code","e82adceb":"code","0a346511":"code","21c67329":"code","d858b9a1":"markdown","cde023d1":"markdown","6e816184":"markdown","acca161a":"markdown","aa0856bd":"markdown","53190ad2":"markdown","0e527452":"markdown","e603256f":"markdown","6eec91a8":"markdown","77d8f513":"markdown","04460638":"markdown","cda433fe":"markdown","3dfe6869":"markdown","0285f0a3":"markdown","43297b0b":"markdown","26c1db3c":"markdown","a12ac861":"markdown"},"source":{"64db2ba4":"!pip install pmdarima\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot\nfrom pylab import rcParams\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose as SDecompose\nfrom statsmodels.graphics.tsaplots import plot_pacf, plot_acf\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.tsa.stattools import adfuller\nfrom pmdarima.arima import auto_arima\n\npd.options.mode.chained_assignment = None\n\ndf = pd.read_csv('\/kaggle\/input\/electric-power-consumption-data-set\/household_power_consumption.txt', delimiter=';', \n                na_values=['nan','?'], dtype={'Date':str,'Time':str,'Global_active_power':np.float64,\n                'Global_reactive_power':np.float64, 'Voltage':np.float64 ,'Global_intensity':np.float64,\n                'Sub_metering_1':np.float64, 'Sub_metering_2':np.float64,'Sub_metering_3': np.float64})","3cb551cf":"df['DTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\ndf = df.drop(['Date', 'Time'], axis=1)\ndf = df.set_index('DTime')\n\ndf['Global_active_power'].plot(figsize=(20, 5))\ndf['Global_reactive_power'].plot()\npyplot.show()","a838a2fa":"print(df.isna().sum())\nsampled_df = df.bfill().resample('W').mean()\nprint(sampled_df.corr())","4be76606":"sampled_df.Global_reactive_power.plot(figsize=(20, 10), color='y', legend=True)\nsampled_df.Global_active_power.plot(color='r', legend=True)\nsampled_df.Sub_metering_1.plot(color='b', legend=True)\nsampled_df.Global_intensity.plot(color='g', legend=True)\npyplot.show()","6bae32f4":"rcParams['figure.figsize'] = 15, 6\n\nfig, axes = pyplot.subplots(4, 2)\n\nmul_decomposition = SDecompose(sampled_df.Global_reactive_power, model='multiplicative')\nadd_decomposition = SDecompose(sampled_df.Global_reactive_power, model='additive')\n\naxes[0][0].plot(mul_decomposition.observed)\naxes[0][0].set_title(\"Multiplicative Decomposition\")\naxes[1][0].plot(mul_decomposition.trend)\naxes[2][0].plot(mul_decomposition.seasonal)\naxes[3][0].plot(mul_decomposition.resid)\n\naxes[0][1].plot(add_decomposition.observed)\naxes[0][1].set_title(\"Additive Decomposition\")\naxes[1][1].plot(add_decomposition.trend)\naxes[2][1].plot(add_decomposition.seasonal)\naxes[3][1].plot(add_decomposition.resid)\npyplot.show()","faf8ac14":"split = int(0.75 * len(sampled_df))\nsampled_train, sampled_test = sampled_df[:split], sampled_df[split:]\n\nplot_acf(sampled_train.Global_reactive_power, lags=30, zero=False)\nplot_pacf(sampled_train.Global_reactive_power, lags=30, zero=False)\n\npyplot.show()","a808e3c9":"print(adfuller(sampled_train.Global_reactive_power))\n\nshifted_power = sampled_train.Global_reactive_power.diff(1)[1:]\n\nprint(adfuller(shifted_power))","3141161b":"params, seasonal_params = (1, 1, 1), (1, 0, 1, 7)\n\nmod = SARIMAX(sampled_train.Global_reactive_power, order=params, seasonal_order=seasonal_params, \n              enforce_stationarity=False, enforce_invertibility=False)\nresults = mod.fit()\nresults.summary()","2052cf70":"model_auto = auto_arima(sampled_train.Global_reactive_power, max_order=None, max_p=4, max_q=10, max_P=4, max_Q=10, \n                max_D=1, m=7, alpha=0.05, trend='t', information_criteria='oob', out_of_sample=int(0.02*len(sampled_train)),\n                maxiter=200, suppress_warnings=True)\n\nmodel_auto.summary()","7d0073dd":"model_auto = auto_arima(sampled_train.Global_reactive_power, exogenous=sampled_train[['Global_intensity', 'Sub_metering_1',  \n                'Sub_metering_2', 'Sub_metering_3', 'Voltage']], max_order=None, max_p=4, max_q=10, max_P=4, max_Q=10, \n                max_D=1, m=7, alpha=0.05, trend='ct', information_criteria='oob', out_of_sample=int(0.02*len(sampled_train)),\n                maxiter=200, suppress_warnings=True)\n\nmodel_auto.summary()","e82adceb":"model_auto = auto_arima(sampled_train.Global_reactive_power, exogenous=sampled_train[['Global_intensity', 'Sub_metering_1',  \n                'Sub_metering_2', 'Sub_metering_3', 'Voltage']], max_order=None, max_p=4, max_q=10, max_P=4, max_Q=10, \n                max_D=1, m=7, alpha=0.05, trend=None, information_criteria='oob', out_of_sample=int(0.02*len(sampled_train)),\n                maxiter=200, suppress_warnings=True)\n\nmodel_auto.summary()","0a346511":"sampled_train['Predicted_Global_reactive_power'] = pd.DataFrame(model_auto.predict_in_sample(exogenous=\n    sampled_train[['Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3', 'Voltage']]), \n    index = sampled_train.Global_reactive_power.index, columns=['Global_reactive_power'])\nsampled_test['Predicted_Global_reactive_power_test'] = pd.DataFrame(model_auto.predict(n_periods=53, exogenous=\n    sampled_test[['Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3', 'Voltage']]), \n     index = sampled_test.Global_reactive_power.index, columns=['Global_reactive_power'])\n\nax = sampled_train.Global_reactive_power.plot(figsize=(20, 5), color='red', legend=True)\nsampled_test.Global_reactive_power.plot(ax=ax, color='red')\n\nsampled_train['Predicted_Global_reactive_power'].plot(color='blue', ax=ax, legend=True)\nsampled_test['Predicted_Global_reactive_power_test'].plot(color='green', ax=ax, legend=True)\n\npyplot.show()","21c67329":"resid = model_auto.resid()\nresid_test = sampled_test['Predicted_Global_reactive_power_test']-sampled_test['Global_reactive_power']\n\nax=sampled_train['Predicted_Global_reactive_power'].plot(color='green')\nsampled_test['Predicted_Global_reactive_power_test'].plot(color='green', ax=ax)\n\nresid.plot(ax=ax, color='blue')\nresid_test.plot(ax=ax, color='black')\n\nax.set_title('Residuals vs Predicted Values')\nax.legend(['Predicted Values', 'Residual in training set', 'Residual in testing set'])\n\npyplot.show()","d858b9a1":"Plotting the actual data from the training set against the predicted values shows that while the model isn't very good at adjusting to shocks in the series, it performs well overall. The same can be said of the predicted values.\n\nTo see just how much data is getting left out of our predictions, let's look at the residuals of the model.","cde023d1":"Plotting the residuals tells us how much information our model was simply not able to learn. For a good model, it must not have any seasonality and must look like white noise, i.e. mean=0 and constant covariance. Looking at the residuals, we see clearly that it resembles white noise and it's magnitude isn't far off from the residuals we obtained by using seasonal_decompose.\n\nSo fianlly, we successfully obtain a model that is able to understand and predict our time series. ","6e816184":"We start off by importing functions and classes we'd need. Sice the data is in the form of a time series, statsmodels comes in handy here as it provides functions such as plot_acf, seasonal_decompose etc, which are extremely handy during initial analysis. It also has ARIMA models like SARIMAX and ARIMA which help us while contructing the regression model itself.","acca161a":"An easy of doing so is using the pmdarima module for it's auto_arima class, which keeps iterating over multiple parameter combinations to give us one with the best fit. First, we start off with the same range of p, q and P, Q as we had decided before but no exogenous (external) variables. We will also decide between it having a constant+linear trend (trend='ct') or having just linear trend (trend='t'), but for now we use only trend.  ","aa0856bd":"We start off by setting the index value for the entire dataframe by using DateTime which gives us one less column to analyse and a more accurate index. Next we get a look at plots of the first two columns.","53190ad2":"# Power Consumption Prediction using Time Series Analysis","0e527452":"We see that the models it picks out as best performing are both simple moving average models of the order 2 with no differencing and lower AIC than before. The one with constant and linear trend has higher AIC than the other as well as insignificant values for both intercept and drift. After comparing both, we pick the latter as the more favorable chocie, due to marginally lower AIC.\n\nHowever, wee see that out model has no seasonal component despite it being very evident in the plots we had created. We may attribute this to using the other columns, like Global_intensity and Submetering_1 and Submetering_3, both of which have significant coefficients. Since they themselves are time series with seasonalities of their own, it makes sense that the model obtained the information about seasonal variations from those columns itself.  ","e603256f":"Since this plot is very dense, we create another dataframe which resamples datapoints for each week and takes the mean for them; we now have a time series sampled_df which can be used for our analysis. We can also then plot these resampled versions.","6eec91a8":"We see that the winning model is a SARIMA with order (0, 1, 2) and seasonal_order (0, 0, 1, 7). Also this model gives better log likeihood for the model and lower AIC, along with significant coefficients for some of the layers, but not all. For example, the drift (which represents the slop of the linear function) has a value very close to 0 and pvalue 0.859, implying it doesn't contribute much to the forecasting.\n\nLet's try again with exogenous variables, ocne with trend='t' and once with trend='ct'.","77d8f513":"Since we see a great number of null values, we use backfilling to fill each empty value with the next value on the column. We call it before we resample and then get the correlation of the new dataframe. \n\nLooking at the correlation for all the columns, we see that the global_active power and global_intensity are heavily correlated while the others are not. So if we're looking to predict Global_reactive_power, we can just include only one of these two columns.","04460638":"The plot shows us a bit of seasonality in the dataframe and it can be analysed more by looking into the decomposition of the series. Using statsmodel's seasonal_decompose, we can try to find trend, seasonal and residual(noise) components and analyze for more insight. \n\nHaving seasonality will tell us that our model isn't stationary and if we assume that then the residual must resemble white noise.","cda433fe":"### Seasonality and Stationarity","3dfe6869":"The log likelihood for any model needs to be high while the AIC needs to be low. Since we haven't tried out other models, we don't yet know if the current score is great or not. However, we can also udge the fit by the coefficients and whether they have significant predictive powers or not.\n\nSo the coefficients for ma.L1, ma.S.L7 and sigma2 are significant as implied by the P>|z| value being 0, but that for ar.L1 does not seem to pass this test. Changing the parameters to different values will help in moving towards a model with a better fit. Moreover, including other columns from the dataset, like voltage and global_intensity, might boost it's performance.","0285f0a3":"The ACF function for the data shows high significance (ACF values outside the blue area) for lags up till 10 and the PACF shows significance till 4. This tells us that if we were to construct an ARMA model, p would be in the range 0 to 4 while q would be in the range 0 to 10. \n\nNow the next thing to do is to check stationarity and how many times the time series would need to be differenced for us to remove the stationarity in our time series. The Dickey Fuller checks our data against the null hypothesis that it is not stationary.","43297b0b":"The high test-statistic (-2.19) falls outside the 10% check and the p-value of 0.2 means the null hypothesis is not rejected and the series is not stationary. But if we shift the datapoints by just 1, we can see that it becomes stationary with an extremely small p value and a very low test statistic. This means that the order of integration (or d) for the model can be chosen as 1, since we only had to difference our model once to get a stationary model.","26c1db3c":"### Creating SARIMA models\n\nNow we can start modeling by using a basic SARIMA model that uses (1, 1, 1), (1, 1, 1, 7) as (p, d, q)(P, D, Q, s) parameters respectively.","a12ac861":"Seasonal_decompose also allows us to choose between an additive or a multiplicative model. \nWe see the trend being the same across both the decompositions. However, the seasonality is significant and stable, also it doesn't seem to be increasing over time. This implies that the seasonal variations are simply added to the trend and not multiplied.\n\nIf we look at the Y-Axis range, we see how the residuals are greater in the multiplicative model. This means there's more information in the series that it may not be able to model well enough and instead just classifies as noise. \nThis hints at the additive model being better for our series.\n\nSince we know our series has seasonality, we know it will require a model which analyses the seasonal component differently, like a SARIMA or SARIMAX model. We know the frequency of our series is weekly so we can use the seasonality to be 7. For the other parameters, p, d, q and P, D, Q for the seasonal component, we need to look at their ACF and PACF plots. \n\nNow before we go further, it makes sense to split sampled_df into test and training sets so that we do not accidentally use any insight obtained by plotting the test set while designing our model. We go with a 75%-25% split."}}