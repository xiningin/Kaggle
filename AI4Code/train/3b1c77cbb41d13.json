{"cell_type":{"9ab760dc":"code","72b3ac90":"code","7613b46c":"code","84027dfe":"code","72dfb4ba":"code","bc08614a":"code","67442b2c":"code","eb35c77d":"code","ad3a70e9":"code","4c5a6ef8":"code","51d22b8e":"code","92140371":"code","d97985dd":"code","b311af11":"code","959f92d7":"code","4eda5d72":"code","72a72d14":"code","10c05be3":"code","273cc9aa":"code","813e373e":"code","6462368e":"code","6aad2673":"code","c2b3705e":"code","049c3570":"code","259565e1":"code","9acbeec6":"code","bb92657a":"code","f68c8c7c":"code","843f88a4":"code","1af3966f":"code","0ecd9025":"code","95108dd2":"code","f0c9ca13":"code","2f529fde":"code","a3ed6f52":"code","d09c1718":"code","2d66293e":"code","144332c1":"code","0dfd7117":"code","19aa5111":"code","8aff0731":"code","5b3fc290":"code","1d68b590":"code","892d0092":"code","2579e143":"code","6b38db0b":"code","77658acc":"code","51238ca4":"code","6f41f547":"code","2d161abc":"code","9f97f25c":"code","668e3e4b":"code","ee41ea41":"code","955bc7b7":"code","e5d5c343":"code","a84acf53":"code","f71e152c":"code","2f118b8d":"code","6bb025cf":"code","67e2b7f5":"code","84764db2":"code","65dd4fc0":"code","824156b7":"code","ae6e1792":"code","a9edb057":"code","ccace9a3":"code","652839e5":"markdown","b2eb9bd5":"markdown","35179436":"markdown","6d2bdbc0":"markdown","aae8cf84":"markdown","ac53ab58":"markdown","a057ff9e":"markdown","b7538fbe":"markdown"},"source":{"9ab760dc":"# Importing Packages\n\nimport numpy as np\nimport pandas as pd\nimport statistics\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nos.getcwd()","72b3ac90":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7613b46c":"# Reading File\n\nr = pd.read_excel('\/kaggle\/input\/loan-eligibility\/loan eligibility.xlsx')\ndf = pd.DataFrame(r)\n\n# Understanding the data\n\ndf.head()","84027dfe":"# Checking shape : Total number of rows and columns\n\ndf.shape","72dfb4ba":"# Checking data_types\n\ndf.info()","bc08614a":"# Checking unique values\n\ndf.nunique()","67442b2c":"# Describe the data\n\ndf.describe()","eb35c77d":"# Renaming the Columns\n\ndf.rename(columns={'Loan ID':'loan_id','Customer ID':'customer_id','Loan Status':'loan_status','Current Loan Amount':'current_loan_amt',\n                   'Term':'term','Credit Score':'credit_score','Annual Income':'annual_income','Years in current job':'yrs_current_job',\n                   'Home Ownership':'home_ownership','Purpose':'purpose','Monthly Debt':'monthly_debt','Years of Credit History':'yrs_credit_history',\n                   'Months since last delinquent':'months_last delinquent','Number of Open Accounts':'num_open_account',\n                   'Number of Credit Problems':'num_credit_problem','Current Credit Balance':'current_credit_balance',\n                   'Maximum Open Credit':'max_open_credit','Bankruptcies':'bankruptcies','Tax Liens':'tax_liens'},inplace=True)","ad3a70e9":"# Identifying Categorical & Numerical Cols\n\ncols = df.columns\nnum_col = df._get_numeric_data().columns.to_list()\ncat_col = list(set(cols)-set(num_col))\n\nprint('Numerical Columns')\nprint(num_col)\nprint('\\nCategorical Columns')\nprint(cat_col)","4c5a6ef8":"# Checking for Duplicates\n\ndfd = df[df.duplicated()]\ndfd.shape","51d22b8e":"# Removing Duplicates\n\ndf.drop(dfd.index,inplace=True)\ndf.shape","92140371":"# Dealing with Null values\n\ndf.isnull().sum()","d97985dd":"# Since months_last delinquent is a numerical variable having more than 50% null values \n# So will drop that entire column\n\ndf.drop(['months_last delinquent'],axis=1,inplace=True)\ndf.isnull().sum()","b311af11":"# Since yrs_current_job is a categorical variable having null values \n# Dropping null records \n\ndfj = df[df['yrs_current_job'].isnull()]\ndf.drop(dfj.index,inplace=True)\n\ndf.isnull().sum()","959f92d7":"# Since both Credit Score and Annual income are important features \n# Dropping null records\n\ndfc = df[df['credit_score'].isnull()]\ndf.drop(dfc.index,inplace=True)\n\ndfa = df[df['annual_income'].isnull()]\ndf.drop(dfa.index,inplace=True)\n\ndf.isnull().sum()","4eda5d72":"# Deleting all rows having null values could lead to loss of information\n# Replacing rest null records with Median in Numerical cols\n\nfor i in df._get_numeric_data().columns:\n    df[i].fillna(df[i].median(),inplace=True)\n\ndf.isnull().sum()","72a72d14":"# Correlation Matrix\n\nplt.figure(figsize=(10,7))\ncorr = df.corr()\nsns.heatmap(corr,annot=True,linewidths = 1)\nplt.show()","10c05be3":"# Loan Status Vs Number of Credit Problems\n# Most of customers charged off due to high credit problems\/issues like penalty for late payment etc\n\nsns.barplot(x=df['loan_status'],y=df['num_credit_problem'])\nplt.show()","273cc9aa":"# Loan Purpose Distribution\n# Maximum loan taken for Debt Consolidation\n\nplt.title('Purpose')\ndf['purpose'].value_counts().plot(kind='barh',color='brown')\nplt.show()","813e373e":"# Loan Status Distribution\n# 75 % customers fully paid their debts to creditors\n\nmyexplode = [0, 0.1]\ndf['loan_status'].value_counts().plot.pie(figsize=(6,6),autopct='%.2f%%',explode=myexplode)\nplt.show()","6462368e":"# Bankruptcies Vs Term\n# For Short term loans customer cannot repay debts to creditors, hence have more chances of bankruptcy \n\nsns.barplot(x=df['bankruptcies'],y=df['term'])\nplt.show()","6aad2673":"# Home Ownership Distribution\n# Most of customers lives at residence which are home mortgaged   \n\ndf['home_ownership'].value_counts().plot(kind='area',color='orange')\nplt.show()","c2b3705e":"# Annual Income Vs Years in Current Job\n# 10+ years expericed customers have maximum annual package \n# Annual income of customer is directly proportional to number of years spent in current job\n\nsns.barplot(x=df['annual_income'],y=df['yrs_current_job'])\nplt.show()","049c3570":"# Monthly Debt Vs Home Ownership\n# Most of customers have high Monthly Debt for Home Mortgage \n\nsns.barplot(x=df['monthly_debt'],y=df['home_ownership'])\nplt.show()","259565e1":"# Home Ownership Vs Tax Liens\n# For Have Mortgage maximum tax liens were imposed \n\nsns.barplot(y=df['tax_liens'],x=df['home_ownership'],orient='v')\nplt.show()","9acbeec6":"# Home Ownership Vs Loan Status\n# Most of customers living at rent charged off \n\nsns.countplot(df['home_ownership'],data=df,hue=df['loan_status'])\nplt.show()","bb92657a":"# Current Loan Amount Vs Years in Current Job\n# 2 years expericed customers have higest current loan amount due \n\nplt.figure(figsize=(9,5))\nsns.lineplot(x=df['yrs_current_job'],y=df['current_loan_amt'],marker='o')\nplt.show()","f68c8c7c":"# Current Loan Amount Vs Purpose\n# Results showed High Loan amount were granted for purpose like wedding, educational expenses, moving, buying a car etc\n\nplt.figure(figsize=(9,5))\nsns.barplot(x=df['current_loan_amt'],y=df['purpose'],orient='h')\nplt.show()","843f88a4":"# Detecting the Outliers\n\nfor i in df._get_numeric_data().columns:\n    sns.boxplot(df[i])\n    plt.show()","1af3966f":"# Handling the Potential Outliers\n\n# Credit_score\n# Since credit_score is a 3-digit number that represents the creditworthiness of an individual. \n# It typically ranges between 300 and 900 and always have integer format, hence removing the outliers\n# Deleting records where credit_score is greater than 900\n\ndfc = df[df['credit_score']>900]\ndf.drop(dfc.index,inplace=True)\ndf.shape","0ecd9025":"# Since we can't have 0 Open Accounts \n# So will drop those records\n\ndfa = df[df['num_open_account']==0]\ndf.drop(dfa.index,inplace=True)\ndf.shape","95108dd2":"# Dropping Customer_id & Loan_id columns\n# Irrelevant for Analysis \n\ndf.drop(['loan_id','customer_id'],axis=1,inplace=True)\ndf.shape","f0c9ca13":"# Since values are very large in columns- Current_loan_amt, Monthly debt, Annual_income, Current_credit_balance & Max_open_credit \n# It's better to take log values or use Winsorization as it brings data to a smaller range & do feature scaling on top of data\n\ndf['current_loan_amt'] = (df['current_loan_amt']+1).transform(np.log)\ndf['monthly_debt'] = (df['monthly_debt']+1).transform(np.log)\ndf['annual_income'] = (df['annual_income']+1).transform(np.log)\ndf['current_credit_balance'] = (df['current_credit_balance']+1).transform(np.log)\ndf['max_open_credit'] = (df['max_open_credit']+1).transform(np.log)\ndf['credit_score'] = (df['credit_score']+1).transform(np.log)","2f529fde":"# Describing the data after trimming\/capping the outliers\n\ndf.describe()","a3ed6f52":"# Visualising the data after trimming\/capping the outliers\n\nfor i in df._get_numeric_data().columns:\n    sns.boxplot(df[i])\n    plt.show()","d09c1718":"# Load from Scikit Learn\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,confusion_matrix","2d66293e":"# Since Home_ownership, Yrs_current_job, Loan_status, Purpose & Term is Categorical, coverting it to Numerical form\n\nfrom sklearn import preprocessing \nle = preprocessing.LabelEncoder() \n\ndf['home_ownership'] = le.fit_transform(df['home_ownership'])\ndf['yrs_current_job'] = le.fit_transform(df['yrs_current_job'])\ndf['loan_status'] = le.fit_transform(df['loan_status'])\ndf['purpose'] = le.fit_transform(df['purpose'])\ndf['term'] = le.fit_transform(df['term'])","144332c1":"df[['home_ownership','yrs_current_job','loan_status','purpose','term']].nunique()","0dfd7117":"# Create Dummy Variables for Home_ownership, Purpose & Term \n# Since Yrs_current_job is ordrinal we won't be using for Dummy Variables\n# One-Hot Encoding\n\nc1 = pd.get_dummies(df['home_ownership'],drop_first=True,prefix='home_ownership')\nc2 = pd.get_dummies(df['purpose'],drop_first=True,prefix='purpose')\nc3 = pd.get_dummies(df['term'],drop_first=True,prefix='term')\nc4 = pd.get_dummies(df['yrs_current_job'],drop_first=True,prefix='yrs_job')\n\ndfc = pd.concat([c1,c2,c3,c4],axis=1)\ndfc.head()","19aa5111":"# Actual Dummies\n\ndf.drop(['yrs_current_job','home_ownership','purpose','term'],axis=1,inplace=True)\ndfd = pd.concat([df,dfc],axis=1)\ndfd.head()","8aff0731":"# Independent variable\nX = dfd.drop(['loan_status'], axis = 1)              \n\n# Dependent variable\ny = dfd['loan_status']","5b3fc290":"# Loan Status Distribution\n# Using SMOTE oversampling method to handle the imbalance problem \n\nsns.countplot(df['loan_status'])\nplt.show()","1d68b590":"# Imbalanced variable\n\ny.value_counts()","892d0092":"# SMOTE(Synthetic Minority Oversampling Technique) to handle imbalance dataset\n\nfrom imblearn.over_sampling import SMOTE \n\nsm = SMOTE(sampling_strategy='minority') \nX_sm,y_sm = sm.fit_sample(X,y)","2579e143":"# After SMOTE, balanced variable\n\nnp.bincount(y_sm)","6b38db0b":"# Train-Test Split\n\nX_train,X_test,y_train,y_test = train_test_split(X_sm,y_sm,stratify=y_sm,test_size=.3,random_state=0)","77658acc":"# Checking data split\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","51238ca4":"# This stratify parameter makes a split so that the proportion of values in the sample produced \n# will be the same as the proportion of values provided to parameter stratify.\n\n# For example, if variable y is a binary categorical variable with values 0 and 1 \n# and there are 25% of zeros and 75% of ones, stratify=y will make sure that your random split has 25% of 0's and 75% of 1's.","6f41f547":"# Checking balanced features\n\ny_train.value_counts()","2d161abc":"y_test.value_counts()","9f97f25c":"# Logistic Regression fit\n\nlg = LogisticRegression()\nlg.fit(X_train,y_train)\n\n# Predictions\n\ny_train_pred = lg.predict(X_train)\ny_test_pred = lg.predict(X_test)\n\n\nprint(y_train_pred)\nprint(y_test_pred)","668e3e4b":"lg.score(X_test,y_test)","ee41ea41":"# Validating on train\nprint('Recall Score for training data is',recall_score(y_train,y_train_pred))\nprint('Precision Score for training data is',precision_score(y_train,y_train_pred))\nprint('F1 Score for training data is',f1_score(y_train,y_train_pred))\nprint('Accuracy Score for training data is',accuracy_score(y_train,y_train_pred))\nprint('\\n')\n\n# Validating on test\nprint('Recall Score for testing data is',recall_score(y_test,y_test_pred))\nprint('Precision Score for testing data is',precision_score(y_test,y_test_pred))\nprint('F1 Score for testing data is',f1_score(y_test,y_test_pred))\nprint('Accuracy Score for testing data is',accuracy_score(y_test,y_test_pred))","955bc7b7":"# Confusion matrix\n\nprint('Confusion Matrix for training data is:')\nprint(confusion_matrix(y_train, y_train_pred))\nprint('\\n')\nprint('Confusion Matrix for testing data is:')\nprint(confusion_matrix(y_test,y_test_pred))","e5d5c343":"from sklearn.tree import DecisionTreeClassifier\n\ndc = DecisionTreeClassifier(max_depth=30,min_samples_split=7)\ndc.fit(X_train,y_train)\n\n# Predictions\n\ny_train_pred = dc.predict(X_train)\ny_test_pred = dc.predict(X_test)\n\n\nprint(y_train_pred)\nprint(y_test_pred)","a84acf53":"dc.score(X_test,y_test)","f71e152c":"# Validating on train\nprint('Recall Score for training data is',recall_score(y_train,y_train_pred))\nprint('Precision Score for training data is',precision_score(y_train,y_train_pred))\nprint('F1 Score for training data is',f1_score(y_train,y_train_pred))\nprint('Accuracy Score for training data is',accuracy_score(y_train,y_train_pred))\nprint('\\n')\n\n# Validating on test\nprint('Recall Score for testing data is',recall_score(y_test,y_test_pred))\nprint('Precision Score for testing data is',precision_score(y_test,y_test_pred))\nprint('F1 Score for testing data is',f1_score(y_test,y_test_pred))\nprint('Accuracy Score for testing data is',accuracy_score(y_test,y_test_pred))","2f118b8d":"# Confusion matrix\n\nprint('Confusion Matrix for training data is:')\nprint(confusion_matrix(y_train, y_train_pred))\nprint('\\n')\nprint('Confusion Matrix for testing data is:')\nprint(confusion_matrix(y_test,y_test_pred))","6bb025cf":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(max_depth=30,min_samples_split=5)\nrf.fit(X_train,y_train)\n\n# Predictions\n\ny_train_pred = rf.predict(X_train)\ny_test_pred = rf.predict(X_test)\n\nprint(y_train_pred)\nprint(y_test_pred)","67e2b7f5":"rf.score(X_test,y_test)","84764db2":"# Validating on train\nprint('Recall Score for training data is',recall_score(y_train,y_train_pred))\nprint('Precision Score for training data is',precision_score(y_train,y_train_pred))\nprint('F1 Score for training data is',f1_score(y_train,y_train_pred))\nprint('Accuracy Score for training data is',accuracy_score(y_train,y_train_pred))\nprint('\\n')\n\n# Validating on test\nprint('Recall Score for testing data is',recall_score(y_test,y_test_pred))\nprint('Precision Score for testing data is',precision_score(y_test,y_test_pred))\nprint('F1 Score for testing data is',f1_score(y_test,y_test_pred))\nprint('Accuracy Score for testing data is',accuracy_score(y_test,y_test_pred))","65dd4fc0":"# Confusion matrix\n\nprint('Confusion Matrix for training data is:')\nprint(confusion_matrix(y_train, y_train_pred))\nprint('\\n')\nprint('Confusion Matrix for testing data is:')\nprint(confusion_matrix(y_test,y_test_pred))","824156b7":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbc = GradientBoostingClassifier()\ngbc.fit(X_train,y_train)\n\n# Predictions\n\ny_train_pred = gbc.predict(X_train)\ny_test_pred = gbc.predict(X_test)\n\nprint(y_train_pred)\nprint(y_test_pred)","ae6e1792":"gbc.score(X_test,y_test)","a9edb057":"# Validating on train\nprint('Recall Score for training data is',recall_score(y_train,y_train_pred))\nprint('Precision Score for training data is',precision_score(y_train,y_train_pred))\nprint('F1 Score for training data is',f1_score(y_train,y_train_pred))\nprint('Accuracy Score for training data is',accuracy_score(y_train,y_train_pred))\nprint('\\n')\n\n# Validating on test\nprint('Recall Score for testing data is',recall_score(y_test,y_test_pred))\nprint('Precision Score for testing data is',precision_score(y_test,y_test_pred))\nprint('F1 Score for testing data is',f1_score(y_test,y_test_pred))\nprint('Accuracy Score for testing data is',accuracy_score(y_test,y_test_pred))","ccace9a3":"# Confusion matrix\n\nprint('Confusion Matrix for training data is:')\nprint(confusion_matrix(y_train,y_train_pred))\nprint('\\n')\nprint('Confusion Matrix for testing data is:')\nprint(confusion_matrix(y_test,y_test_pred))","652839e5":"#### Feature Engineering","b2eb9bd5":"#### Logistic Regression Model","35179436":"#### Data Cleanup ","6d2bdbc0":"#### EDA","aae8cf84":"#### Gradient Boosting Classifier","ac53ab58":"#### Decision Tree Classifier","a057ff9e":"#### Random Forest Classifier","b7538fbe":"#### Logistic Regression Model"}}