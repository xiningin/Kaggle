{"cell_type":{"58404c38":"code","92f8666c":"code","d5fdbaed":"code","6a3a4399":"code","6a07e0c0":"code","5c13b45f":"code","5a74e2a5":"code","e25682b6":"code","b48e3604":"code","df6fdb9a":"code","3b4b9513":"code","d8445835":"code","79b66524":"code","6cbb5a7c":"code","74529668":"code","de68c65c":"code","cb43758d":"code","72d7d6b7":"code","5b96e6ea":"code","2b1c06ef":"code","eb5d6348":"code","6c17c84b":"code","845578c7":"code","5fc353dc":"code","68ce8ba0":"code","5cdeeef4":"code","52e99a7e":"code","c8866e92":"code","dc2972ed":"code","d9439773":"code","7b57ff42":"code","89f49df2":"code","bb5e8fd2":"code","19e20246":"code","f4d8d75e":"code","e35b628a":"code","032d7fa2":"code","94164f0d":"code","68548f58":"code","3debadd2":"code","4aa86095":"code","0fc67066":"code","be9b3e97":"code","5c5e73e3":"code","ae3ed867":"code","ba8e5e4c":"code","3171326c":"code","71095488":"code","af465ebe":"code","070235c6":"code","1e228a36":"code","ff5e53f4":"code","f43b4455":"code","89c1556b":"code","056a8b19":"code","32b6ff47":"code","90a0de6a":"code","6c864b80":"code","f8890536":"code","f3030125":"code","94c4df12":"code","0dfb7bdb":"code","23a854bf":"code","8cfaa835":"code","142c84c1":"markdown","c443ec15":"markdown","ff11f6f0":"markdown","1021078d":"markdown","abe2411f":"markdown","576c2730":"markdown","a1c38b5e":"markdown","42be7802":"markdown","43d10e44":"markdown","99428459":"markdown","8ad00a51":"markdown","b55a4c12":"markdown","501a623b":"markdown","29523dfb":"markdown","4cf2b334":"markdown","6fb51019":"markdown","51a944e9":"markdown","c76f98fc":"markdown","23fda234":"markdown","2338b881":"markdown","d0082187":"markdown","cdef0aa4":"markdown","c5192dee":"markdown","d79902e3":"markdown","9a1993a4":"markdown","377a36b6":"markdown","5067643d":"markdown","5d4a1ec9":"markdown","9bea56c8":"markdown","d4c92d5a":"markdown","2aa64d63":"markdown","21de9e86":"markdown","1eef1a8d":"markdown","d2cfa747":"markdown","0b660a87":"markdown","030b15d0":"markdown","b25309ef":"markdown","44360e67":"markdown","fe2dc8f9":"markdown","e6982504":"markdown","38c67f22":"markdown","e5d511ad":"markdown","793e8bdb":"markdown","fc725d1b":"markdown","2c8dcf1b":"markdown","1994b835":"markdown","2fff9768":"markdown","391db5e3":"markdown","1a1252d2":"markdown","3950e979":"markdown","25733f06":"markdown","51553707":"markdown","e36a1559":"markdown","ee1ab94c":"markdown","71c758d7":"markdown","5a869e30":"markdown","ad5cb3fb":"markdown","0d099bc9":"markdown","efe4f3c2":"markdown","a1fe98e7":"markdown","af2acedb":"markdown","7c30d207":"markdown","9b13fc2f":"markdown","9df4fbd3":"markdown","444a954d":"markdown","838ea3d2":"markdown","43ea34a1":"markdown","100b296f":"markdown","f8e6be76":"markdown","12cb079c":"markdown","80cc10ca":"markdown","c8fb42a1":"markdown","64dc6c7a":"markdown","615142fd":"markdown","2033fa9b":"markdown","9377d9a1":"markdown","d1b93f34":"markdown","6fab6e88":"markdown","0571f7d0":"markdown","7e1266f1":"markdown","21eaa0d4":"markdown","b3e7d57c":"markdown","9dc934e0":"markdown","3d005f4e":"markdown","90ea8ed2":"markdown"},"source":{"58404c38":"import pandas as pd \nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\nimport gc\nfrom datetime import datetime \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom catboost import CatBoostClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\n\npd.set_option('display.max_columns', 100)","92f8666c":"RFC_METRIC = 'gini'  #metric used for RandomForrestClassifier\nNUM_ESTIMATORS = 100 #number of estimators used for RandomForrestClassifier\nNO_JOBS = 4 #number of parallel jobs used for RandomForrestClassifier\n\n#VALIDATION\nVALID_SIZE = 0.20 # simple validation using train_test_split\n\n#CROSS-VALIDATION\nNUMBER_KFOLDS = 5 #number of KFolds for cross-validation\n\nRANDOM_STATE = 2018\n\nMAX_ROUNDS = 1000 #lgb iterations\nEARLY_STOP = 50 #lgb early stop \nOPT_ROUNDS = 1000  #To be adjusted based on best validation rounds\nVERBOSE_EVAL = 50 #Print out metric result\n\nIS_LOCAL = False\n\nimport os\n\nif(IS_LOCAL):\n    PATH=\"..\/input\/default-of-credit-card-clients-dataset\"\nelse:\n    PATH=\"..\/input\"\nprint(os.listdir(PATH))","d5fdbaed":"data_df = pd.read_csv(PATH+\"\/UCI_Credit_Card.csv\")","6a3a4399":"print(\"Default Credit Card Clients data -  rows:\",data_df.shape[0],\" columns:\", data_df.shape[1])","6a07e0c0":"data_df.head()","5c13b45f":"data_df.describe()","5a74e2a5":"total = data_df.isnull().sum().sort_values(ascending = False)\npercent = (data_df.isnull().sum()\/data_df.isnull().count()*100).sort_values(ascending = False)\npd.concat([total, percent], axis=1, keys=['Total', 'Percent']).transpose()","e25682b6":"temp = data_df[\"default.payment.next.month\"].value_counts()\ndf = pd.DataFrame({'default.payment.next.month': temp.index,'values': temp.values})\nplt.figure(figsize = (6,6))\nplt.title('Default Credit Card Clients - target value - data unbalance\\n (Default = 0, Not Default = 1)')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x = 'default.payment.next.month', y=\"values\", data=df)\nlocs, labels = plt.xticks()\nplt.show()","b48e3604":"plt.figure(figsize = (14,6))\nplt.title('Amount of credit limit - Density Plot')\nsns.set_color_codes(\"pastel\")\nsns.distplot(data_df['LIMIT_BAL'],kde=True,bins=200, color=\"blue\")\nplt.show()","df6fdb9a":"data_df['LIMIT_BAL'].value_counts().shape","3b4b9513":"data_df['LIMIT_BAL'].value_counts().head(5)","d8445835":"class_0 = data_df.loc[data_df['default.payment.next.month'] == 0][\"LIMIT_BAL\"]\nclass_1 = data_df.loc[data_df['default.payment.next.month'] == 1][\"LIMIT_BAL\"]\nplt.figure(figsize = (14,6))\nplt.title('Default amount of credit limit  - grouped by Payment Next Month (Density Plot)')\nsns.set_color_codes(\"pastel\")\nsns.distplot(class_1,kde=True,bins=200, color=\"red\")\nsns.distplot(class_0,kde=True,bins=200, color=\"green\")\nplt.show()","79b66524":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\ns = sns.boxplot(ax = ax1, x=\"SEX\", y=\"LIMIT_BAL\", hue=\"SEX\",data=data_df, palette=\"PRGn\",showfliers=True)\ns = sns.boxplot(ax = ax2, x=\"SEX\", y=\"LIMIT_BAL\", hue=\"SEX\",data=data_df, palette=\"PRGn\",showfliers=False)\nplt.show();","6cbb5a7c":"var = ['BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6']\n\nplt.figure(figsize = (8,8))\nplt.title('Amount of bill statement (Apr-Sept) \\ncorrelation plot (Pearson)')\ncorr = data_df[var].corr()\nsns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,linewidths=.1,vmin=-1, vmax=1)\nplt.show()","74529668":"var = ['PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5']\n\nplt.figure(figsize = (8,8))\nplt.title('Amount of previous payment (Apr-Sept) \\ncorrelation plot (Pearson)')\ncorr = data_df[var].corr()\nsns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,linewidths=.1,vmin=-1, vmax=1)\nplt.show()","de68c65c":"var = ['PAY_0','PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n\nplt.figure(figsize = (8,8))\nplt.title('Repayment status (Apr-Sept) \\ncorrelation plot (Pearson)')\ncorr = data_df[var].corr()\nsns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,linewidths=.1,vmin=-1, vmax=1)\nplt.show()","cb43758d":"def boxplot_variation(feature1, feature2, feature3, width=16):\n    fig, ax1 = plt.subplots(ncols=1, figsize=(width,6))\n    s = sns.boxplot(ax = ax1, x=feature1, y=feature2, hue=feature3,\n                data=data_df, palette=\"PRGn\",showfliers=False)\n    s.set_xticklabels(s.get_xticklabels(),rotation=90)\n    plt.show();","72d7d6b7":"boxplot_variation('MARRIAGE','AGE', 'SEX',8)","5b96e6ea":"boxplot_variation('EDUCATION','AGE', 'MARRIAGE',12)","2b1c06ef":"boxplot_variation('AGE','LIMIT_BAL', 'SEX',16)","eb5d6348":"boxplot_variation('MARRIAGE','LIMIT_BAL', 'EDUCATION',12)","6c17c84b":"target = 'default.payment.next.month'\npredictors = [  'LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', \n                'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', \n                'BILL_AMT1','BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',\n                'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']","845578c7":"train_df, val_df = train_test_split(data_df, test_size=VALID_SIZE, random_state=RANDOM_STATE, shuffle=True )","5fc353dc":"train_df_bkp = train_df.copy()\nval_df_bkp = val_df.copy()","68ce8ba0":"clf = RandomForestClassifier(n_jobs=NO_JOBS, \n                             random_state=RANDOM_STATE,\n                             criterion=RFC_METRIC,\n                             n_estimators=NUM_ESTIMATORS,\n                             verbose=False)","5cdeeef4":"clf.fit(train_df[predictors], train_df[target].values)","52e99a7e":"preds = clf.predict(val_df[predictors])","c8866e92":"tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': clf.feature_importances_})\ntmp = tmp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (7,4))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=tmp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)\nplt.show()   \n","dc2972ed":"cm = pd.crosstab(val_df[target].values, preds, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cm, \n            xticklabels=['Not Default', 'Default'],\n            yticklabels=['Not Default', 'Default'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\nplt.title('Confusion Matrix', fontsize=14)\nplt.show()","d9439773":"roc_auc_score(val_df[target].values, preds)","7b57ff42":"cat_features = ['EDUCATION', 'SEX', 'MARRIAGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']","89f49df2":"train_f_df = pd.get_dummies(train_df_bkp, columns = cat_features)\nval_f_df = pd.get_dummies(val_df_bkp, columns = cat_features)","bb5e8fd2":"print(\"Default of Credit Card Clients train data -  rows:\",train_f_df.shape[0],\" columns:\", train_f_df.shape[1])\nprint(\"Default of Credit Card Clients val  data -  rows:\",val_f_df.shape[0],\" columns:\", val_f_df.shape[1])","19e20246":"train_fa_df, val_fa_df = train_f_df.align(val_f_df, join='outer', axis=1, fill_value=0)","f4d8d75e":"print(\"Default of Credit Card Clients train data -  rows:\",train_fa_df.shape[0],\" columns:\", train_fa_df.shape[1])\nprint(\"Default of Credit Card Clients val  data -  rows:\",val_fa_df.shape[0],\" columns:\", val_fa_df.shape[1])","e35b628a":"train_fa_df.head(5)","032d7fa2":"val_fa_df.head(5)","94164f0d":"target_f = 'default.payment.next.month'\npredictors_f = ['AGE', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4',\n       'BILL_AMT5', 'BILL_AMT6', 'EDUCATION_0', 'EDUCATION_1',\n       'EDUCATION_2', 'EDUCATION_3', 'EDUCATION_4', 'EDUCATION_5',\n       'EDUCATION_6', 'LIMIT_BAL', 'MARRIAGE_0', 'MARRIAGE_1',\n       'MARRIAGE_2', 'MARRIAGE_3', 'PAY_0_-1', 'PAY_0_-2', 'PAY_0_0',\n       'PAY_0_1', 'PAY_0_2', 'PAY_0_3', 'PAY_0_4', 'PAY_0_5', 'PAY_0_6',\n       'PAY_0_7', 'PAY_0_8', 'PAY_2_-1', 'PAY_2_-2', 'PAY_2_0', 'PAY_2_1',\n       'PAY_2_2', 'PAY_2_3', 'PAY_2_4', 'PAY_2_5', 'PAY_2_6', 'PAY_2_7',\n       'PAY_2_8', 'PAY_3_-1', 'PAY_3_-2', 'PAY_3_0', 'PAY_3_1', 'PAY_3_2',\n       'PAY_3_3', 'PAY_3_4', 'PAY_3_5', 'PAY_3_6', 'PAY_3_7', 'PAY_3_8',\n       'PAY_4_-1', 'PAY_4_-2', 'PAY_4_0', 'PAY_4_1', 'PAY_4_2', 'PAY_4_3',\n       'PAY_4_4', 'PAY_4_5', 'PAY_4_6', 'PAY_4_7', 'PAY_4_8', 'PAY_5_-1',\n       'PAY_5_-2', 'PAY_5_0', 'PAY_5_2', 'PAY_5_3', 'PAY_5_4', 'PAY_5_5',\n       'PAY_5_6', 'PAY_5_7', 'PAY_5_8', 'PAY_6_-1', 'PAY_6_-2', 'PAY_6_0',\n       'PAY_6_2', 'PAY_6_3', 'PAY_6_4', 'PAY_6_5', 'PAY_6_6', 'PAY_6_7',\n       'PAY_6_8', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4',\n       'PAY_AMT5', 'PAY_AMT6', 'SEX_1', 'SEX_2']","68548f58":"clf.fit(train_fa_df[predictors_f], train_df[target_f].values)","3debadd2":"preds = clf.predict(val_fa_df[predictors_f])","4aa86095":"tmp = pd.DataFrame({'Feature': predictors_f, 'Feature importance': clf.feature_importances_})\ntmp = tmp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (16,4))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=tmp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)\nplt.show()","0fc67066":"cm = pd.crosstab(val_fa_df[target].values, preds, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cm, \n            xticklabels=['Not Default', 'Default'],\n            yticklabels=['Not Default', 'Default'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\nplt.title('Confusion Matrix', fontsize=14)\nplt.show()","be9b3e97":"roc_auc_score(val_fa_df[target].values, preds)","5c5e73e3":"clf = AdaBoostClassifier(random_state=RANDOM_STATE,\n                         algorithm='SAMME.R',\n                         learning_rate=0.8,\n                             n_estimators=NUM_ESTIMATORS)","ae3ed867":"clf.fit(train_df[predictors], train_df[target].values)","ba8e5e4c":"preds = clf.predict(val_df[predictors])","3171326c":"tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': clf.feature_importances_})\ntmp = tmp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (7,4))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=tmp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)\nplt.show()   ","71095488":"cm = pd.crosstab(val_df[target].values, preds, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cm, \n            xticklabels=['Not Default', 'Default'],\n            yticklabels=['Not Default', 'Default'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\nplt.title('Confusion Matrix', fontsize=14)\nplt.show()","af465ebe":"roc_auc_score(val_df[target].values, preds)","070235c6":"clf = CatBoostClassifier(iterations=500,\n                             learning_rate=0.02,\n                             depth=12,\n                             eval_metric='AUC',\n                             random_seed = RANDOM_STATE,\n                             bagging_temperature = 0.2,\n                             od_type='Iter',\n                             metric_period = VERBOSE_EVAL,\n                             od_wait=100)","1e228a36":"clf.fit(train_df[predictors], train_df[target].values,verbose=True)","ff5e53f4":"preds = clf.predict(val_df[predictors])","f43b4455":"tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': clf.feature_importances_})\ntmp = tmp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (7,4))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=tmp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)\nplt.show()   ","89c1556b":"cm = pd.crosstab(val_df[target].values, preds, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cm, \n            xticklabels=['Not Fraud', 'Fraud'],\n            yticklabels=['Not Fraud', 'Fraud'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\nplt.title('Confusion Matrix', fontsize=14)\nplt.show()","056a8b19":"roc_auc_score(val_df[target].values, preds)","32b6ff47":"# Prepare the train and valid datasets\ndtrain = xgb.DMatrix(train_df[predictors], train_df[target].values)\ndvalid = xgb.DMatrix(val_df[predictors], val_df[target].values)\n\n#What to monitor (in this case, **train** and **valid**)\nwatchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n\n# Set xgboost parameters\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eta'] = 0.039\nparams['silent'] = True\nparams['max_depth'] = 2\nparams['subsample'] = 0.8\nparams['colsample_bytree'] = 0.9\nparams['eval_metric'] = 'auc'\nparams['random_state'] = RANDOM_STATE","90a0de6a":"model = xgb.train(params, \n                dtrain, \n                MAX_ROUNDS, \n                watchlist, \n                early_stopping_rounds=EARLY_STOP, \n                maximize=True, \n                verbose_eval=VERBOSE_EVAL)","6c864b80":"fig, (ax) = plt.subplots(ncols=1, figsize=(8,5))\nxgb.plot_importance(model, height=0.8, title=\"Features importance (XGBoost)\", ax=ax, color=\"green\") \nplt.show()","f8890536":"params = {\n          'boosting_type': 'gbdt',\n          'objective': 'binary',\n          'metric':'auc',\n          'learning_rate': 0.05,\n          'num_leaves': 7,  # we should let it be smaller than 2^(max_depth)\n          'max_depth': 4,  # -1 means no limit\n          'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n          'max_bin': 100,  # Number of bucketed bin for feature values\n          'subsample': 0.9,  # Subsample ratio of the training instance.\n          'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n          'colsample_bytree': 0.7,  # Subsample ratio of columns when constructing each tree.\n          'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n          'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n          'nthread': 8,\n          'verbose': 0,\n          'scale_pos_weight':50, # because training data is sightly unbalanced \n         }","f3030125":"categorical_features = ['SEX', 'EDUCATION', 'MARRIAGE','PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']","94c4df12":"dtrain = lgb.Dataset(train_df[predictors].values, \n                     label=train_df[target].values,\n                     feature_name=predictors,\n                     categorical_feature=categorical_features)\n\ndvalid = lgb.Dataset(val_df[predictors].values,\n                     label=val_df[target].values,\n                     feature_name=predictors,\n                     categorical_feature=categorical_features)","0dfb7bdb":"evals_results = {}\n\nmodel = lgb.train(params, \n                  dtrain, \n                  valid_sets=[dtrain, dvalid], \n                  valid_names=['train','valid'], \n                  evals_result=evals_results, \n                  num_boost_round=MAX_ROUNDS,\n                  early_stopping_rounds=EARLY_STOP,\n                  verbose_eval=VERBOSE_EVAL, \n                  feval=None)\n\ndel dvalid\ngc.collect()","23a854bf":"fig, (ax) = plt.subplots(ncols=1, figsize=(8,5))\nlgb.plot_importance(model, height=0.8, title=\"Features importance (LightGBM)\", ax=ax,color=\"red\") \nplt.show()","8cfaa835":"kf = KFold(n_splits = NUMBER_KFOLDS, random_state = RANDOM_STATE, shuffle = True)\nfor train_index, test_index in kf.split(data_df):\n    train_X, valid_X = data_df.iloc[train_index], data_df.iloc[test_index]\n\n    dtrain = lgb.Dataset(train_X[predictors].values, label=train_X[target].values,\n                     feature_name=predictors)\n\n    dvalid = lgb.Dataset(valid_X[predictors].values, label=valid_X[target].values,\n                     feature_name=predictors)\n\n    evals_results = {}\n    model =  lgb.train(params, \n                  dtrain, \n                  valid_sets=[dtrain, dvalid], \n                  valid_names=['train','valid'], \n                  evals_result=evals_results, \n                  num_boost_round=MAX_ROUNDS,\n                  early_stopping_rounds=EARLY_STOP,\n                  verbose_eval=VERBOSE_EVAL, \n                  feval=None)","142c84c1":"## <a id=\"42\">Check missing data<\/a>  \n\nLet's check if there is any missing data.","c443ec15":"We also make copies of the train_df and val_df for later usage.","ff11f6f0":"The ROC-AUC score obtained with CatBoostClassifier is 0.66.","1021078d":"## <a id=\"62\">AdaBoostClassifier<\/a>\n\n\nAdaBoostClassifier stands for Adaptive Boosting Classifier <a href='#8'>[5]<\/a>.\n\n### Prepare the model\n\nLet's set the parameters for the model and initialize the model.","abe2411f":"## <a id=\"64\">XGBoost<\/a>","576c2730":"With the dummified features, the improvement of the AUC score is quite small.","a1c38b5e":"The ROC-AUC score obtained with AdaBoostClassifier is 0.65.","42be7802":"### Confusion matrix\n\nLet's visualize the confusion matrix.","43d10e44":"The most important features are **PAY_0**, **AGE**, **BILL_AMT1**, **LIMIT_BAL**, **BILL_AMT2**, **BILL_AMT3**.\n\n\n### Confusion matrix\n\nLet's show a confusion matrix for the results we obtained. ","99428459":"## Set parameters\n\nHere we set few parameters for the analysis and models.","8ad00a51":"There are no correlations between amounts of previous payments for April-Sept 2005.\n\nLet's check the correlation between Repayment status in April - September 2005.","b55a4c12":"Let's start with a RandomForrestClassifier <a href='#8'>[3]<\/a>   model.","501a623b":"The best validation score (ROC-AUC) was **0.78**, for round **453**.","29523dfb":"## Credit limit vs. sex\n\nLet's check the credit limit distribution vs. sex. For the sex, 1 stands for male and 2 for female.","4cf2b334":"The average validation **AUC** for the 5 folds obtained was **0.78**, with values between **0.778** and **0.79**.","6fb51019":"## <a id=\"61\">RandomForestClassifier<\/a>\n\n\n","51a944e9":"There are 30,000 distinct credit card clients.  \n\nThe average value for the amount of credit card limit is 167,484. The standard deviation is unusually large, max value being 1M.\n\nEducation level is mostly graduate school and university.\n\nMost of the clients are either marrined or single (less frequent the other status).\n\nAverage age is 35.5 years, with a standard deviation of 9.2.\n\nAs the value 0 for default payment means 'not default' and value 1 means 'default', the mean of 0.221 means that there are 22.1% of credit card contracts that will default next month (will verify this in the next sections of this analysis).\n","c76f98fc":"# <a id=\"7\">Conclusions<\/a>","23fda234":"Let's run a model using the training set for training. Then, we will use the validation set for validation. \n\n### Metric\n\nWe will use as validation criterion **GINI**, which formula is **GINI = 2 * (AUC) - 1**, where **AUC** is the **Receiver Operating Characteristic - Area Under Curve (ROC-AUC)** <a href='#8'>[4]<\/a>.  Number of estimators is set to **100** and number of parallel jobs is set to **4**.\n\nWe start by initializing the RandomForestClassifier.","2338b881":"Because train and validation data does not have the same number of columns, we will align them.","d0082187":"Let's define the categorical features.","cdef0aa4":"XGBoost is a gradient boosting algorithm <a href='#8'>[7]<\/a>.\n\nLet's prepare the model.","c5192dee":"### Train the model\n\nLet's train the model. ","d79902e3":"### Fit the model\n\nLet's fit the model.","9a1993a4":"We also calculate area under curve (receiver operator characteristic).","377a36b6":"Let's calculate also the ROC-AUC.\n\n\n### Area under curve","5067643d":"Largest group of amount of credit limit is apparently for amount of 50K. Let's verify this.","5d4a1ec9":"# <a id=\"2\">Load packages<\/a>\n\n## Load packages","9bea56c8":"Let's define the target and predictors lists.","d4c92d5a":"## <a id=\"43\">Data unbalance<\/a>","2aa64d63":"It looks like Married status 3 (others), with mean values over 40 and Q4 values over 60 means mostly vidowed or divorced whilst Married status 0 could be not specified or divorced, as Q1 values are above values for married of both sexes.\n\nMarried males have mean age above married women. Unmarried males have mean value for age above unmarried women as well but closer. Q3 abd Q4 values for married man are above corresponding values for married women.\n\n\nLet's show the boxplots with age distribution grouped by education and marriage.\n\nEducation status meaning is:\n\n* 1 : graduate school\n* 2 : university\n* 3 : high school\n* 4 : others\n* 5 : unknown\n* 6 : unknow\n","21de9e86":"# <a id=\"3\">Read the data<\/a>","1eef1a8d":"### Prepare the model\n","d2cfa747":"A number of **6,636** out of **30,000** (or **22%**) of clients will default next month. The data has not a large unbalance with respect of the target value (default.payment.next.month).","0b660a87":"### Predict the target values\n\nLet's now predict the **target** values for the **val_df** data, using predict function.","030b15d0":"## Features correlation\n\n\nFor the numeric values, let's represent the features correlation.\n\n\nLet's check the correlation of Amount of bill statement in April - September 2005.","b25309ef":"### Define predictors and target values\n\nLet's define the predictor features and the target features. Categorical features, if any, are also defined. In our case, there are no categorical feature.","44360e67":"# <a id=\"1\">Introduction<\/a>  \n\n\n## Dataset\n\nThis dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from **April 2005** to **September 2005**. \n\n## Content\n\nThere are 25 variables:\n\n* **ID**: ID of each client\n* **LIMIT_BAL**: Amount of given credit in NT dollars (includes individual and family\/supplementary credit\n* **SEX**: Gender (1=male, 2=female)\n* **EDUCATION**: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n* **MARRIAGE**: Marital status (1=married, 2=single, 3=others)\n* **AGE**: Age in years\n* **PAY_0**: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, ... 8=payment delay for eight months, 9=payment delay for nine months and above)\n* **PAY_2**: Repayment status in August, 2005 (scale same as above)\n* **PAY_3**: Repayment status in July, 2005 (scale same as above)\n* **PAY_4**: Repayment status in June, 2005 (scale same as above)\n* **PAY_5**: Repayment status in May, 2005 (scale same as above)\n* **PAY_6**: Repayment status in April, 2005 (scale same as above)\n* **BILL_AMT1**: Amount of bill statement in September, 2005 (NT dollar)\n* **BILL_AMT2**: Amount of bill statement in August, 2005 (NT dollar)\n* **BILL_AMT3**: Amount of bill statement in July, 2005 (NT dollar)\n* **BILL_AMT4**: Amount of bill statement in June, 2005 (NT dollar)\n* **BILL_AMT5**: Amount of bill statement in May, 2005 (NT dollar)\n* **BILL_AMT6**: Amount of bill statement in April, 2005 (NT dollar)\n* **PAY_AMT1**: Amount of previous payment in September, 2005 (NT dollar)\n* **PAY_AMT2**: Amount of previous payment in August, 2005 (NT dollar)\n* **PAY_AMT3**: Amount of previous payment in July, 2005 (NT dollar)\n* **PAY_AMT4**: Amount of previous payment in June, 2005 (NT dollar)\n* **PAY_AMT5**: Amount of previous payment in May, 2005 (NT dollar)\n* **PAY_AMT6**: Amount of previous payment in April, 2005 (NT dollar)\n* **default.payment.next.month**: Default payment (1=yes, 0=no)\n\n","fe2dc8f9":"Let's also visualize the features importance.\n\n### Features importance","e6982504":"<h1><center><font size=\"6\">Default of Credit Card Clients - Predictive Models<\/font><\/center><\/h1>\n\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/e\/ee\/Creditcardwcontactless.png\" width=\"400\"><\/img>\n\n\n# <a id='0'>Content<\/a>\n\n- <a href='#1'>Introduction<\/a>  \n- <a href='#2'>Load packages<\/a>  \n- <a href='#3'>Read the data<\/a>  \n- <a href='#4'>Check the data<\/a>  \n    - <a href='#41'>Glimpse the data<\/a>  \n    - <a href='#42'>Check missing data<\/a>\n    - <a href='#43'>Check data unbalance<\/a>\n- <a href='#5'>Data exploration<\/a>\n- <a href='#6'>Predictive models<\/a>  \n    - <a href='#61'>RandomForrestClassifier<\/a> \n    - <a href='#62'>AdaBoostClassifier<\/a>\n    - <a href='#63'>CatBoostClassifier<\/a> \n    - <a href='#64'>XGBoost<\/a> \n    - <a href='#65'>LightGBM<\/a> \n- <a href='#7'>Conclusions<\/a>\n- <a href='#8'>References<\/a>\n","38c67f22":"### Features importance\n\nLet's see also the features importance.","e5d511ad":"Let's calculate also the ROC-AUC.\n\n\n### Area under curve","793e8bdb":"Let's check data unbalance with respect with *target* value, i.e. **default.payment.next.month**.","fc725d1b":"Let's now predict the **target** values for the **val_df** data, using **predict** function.","2c8dcf1b":"#### Confusion matrix\n\nLet's show a confusion matrix for the results we obtained. ","1994b835":"## <a id=\"63\">CatBoostClassifier<\/a>\n\n\nCatBoostClassifier is a gradient boosting for decision trees algorithm with support for handling categorical data <a href='#8'>[6]<\/a>.\n\n### Prepare the model\n\nLet's set the parameters for the model and initialize the model.","2fff9768":"### Confusion matrix\n\nLet's visualize the confusion matrix.","391db5e3":"Let's train the **RandonForestClassifier** using the **train_df** data and **fit** function.","1a1252d2":"There is no missing data in the entire dataset.","3950e979":"Let's now predict the **target** values for the **val_df** data, using **predict** function.","25733f06":"# <a id=\"8\">References<\/a>\n\n[1] Default Credit Card Clients Dataset,  https:\/\/www.kaggle.com\/uciml\/default-of-credit-card-clients-dataset\/  \n[2] Principal Component Analysis, Wikipedia Page, https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis  \n[3] RandomForrestClassifier, http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html  \n[4] ROC-AUC characteristic, https:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic#Area_under_the_curve   \n[5] AdaBoostClassifier, http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html   \n[6] CatBoostClassifier, https:\/\/tech.yandex.com\/catboost\/doc\/dg\/concepts\/python-reference_catboostclassifier-docpage\/  \n[7] XGBoost Python API Reference, http:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html  \n[8] LightGBM Python implementation, https:\/\/github.com\/Microsoft\/LightGBM\/tree\/master\/python-package  \n[9] LightGBM algorithm, https:\/\/www.microsoft.com\/en-us\/research\/wp-content\/uploads\/2017\/11\/lightgbm.pdf   \n\n","51553707":"Let's check again the data.","e36a1559":"## Amount of credit limit ","ee1ab94c":"### Training and validation using cross-validation\n\nLet's use now cross-validation. We will use cross-validation (KFolds) with 5 folds. Data is divided in 5 folds and, by rotation, we are training using 4 folds (n-1) and validate using the 5th (nth) fold.","71c758d7":"The limit credit amount is quite balanced between sexes. The males have a slightly smaller Q2 and larger Q3 and Q4 and a lower mean. The female have a larger outlier max value (1M NT dollars).","5a869e30":"The most important features are **AGE**, **LIMIT_BAL**, **BILL_AMT1**, **PAY_0_2**, **BILL_AMT2**, **BILL_AMT3**.","ad5cb3fb":"## <a id=\"41\">Glimpse the data<\/a>\n\nWe start by looking to the data features (first 5 rows).","0d099bc9":"We investigated the data, checking for data unbalancing, visualizing the features and understanding the relationship between different features.   \n\nWe then investigated five predictive models:  \n* We started with **RandomForrestClassifier**, for which we obtained an AUC scode of **0.66**. \n   For the **RandomForrestClassifier** we also experimented with **OneHotEncoder**, replacing the categorical features with dummified values (introducing one dummy variable for each category). The AUC score did not improved significantly in this case.\n* Next we used  an **AdaBoostClassifier** model, with lower AUC score (**0.65**).   \n* We followed with an **CatBoostClassifier** model, with lower AUC score (**0.66**).   \n* Then we experimented with a **XGBoost** model, for which the AUC score obtained was **0.77**.   \n* We then presented the data to a **LightGBM** model. We used both train-validation split and cross-validation to evaluate the model effectiveness to predict the target value, i.e. detecting if a credit card client  will default next month. With both methods for LightGBM the obtained values of AUC for the validation set were around **0.78**.","efe4f3c2":"### Predict the target values\n\nLet's now predict the **target** values for the **val_df** data, using predict function.","a1fe98e7":"## Amount of credit limit grouped by default payment next month\n\nLet's visualize the density plot for amount of credit limit (LIMIT_BAL), grouped by default payment next month.","af2acedb":"Correlation is decreasing with distance between months. Lowest correlations are between Sept-April.\n\n\nLet's check the correlation of Amount of previous payment in April - September 2005.","7c30d207":"### Prepare the model\n\nWe initialize the DMatrix objects for training and validation, starting from the datasets. We also set some of the parameters used for the model tuning.","9b13fc2f":"We also calculate area under curve (receiver operator characteristic)","9df4fbd3":"Let's look into more details to the data.","444a954d":"### Split data in train and validation set\n\nLet's define train and validation sets.","838ea3d2":"## Age, sex and credit amount limit\n\n\nLet's show the  boxplots with credit amount limit distribution grouped by age and sex.\n\n","43ea34a1":"Let's prepare the model, creating the **Dataset**s data structures from the train and validation sets.\n\nWe will also initialize the Datasets with the list of the categorical features (**lgb** has a special treatment for categorical values).","100b296f":"# <a id=\"6\">Predictive models<\/a>  \n\n","f8e6be76":"# <a id=\"5\">Data exploration<\/a>","12cb079c":"## Marriage status, education level and credit amount limit\n\n\nLet's show the  boxplots with credit amount limit distribution grouped by marriage status and education level.","80cc10ca":"### Features importance\n\nLet's see also the features importance.","c8fb42a1":"### Plot variable importance","64dc6c7a":"## <a id=\"65\">LightGBM<\/a>\n\n\nLet's continue with another gradient boosting algorithm, LightGBM <a href='#8'>[7]<\/a> <a href='#8'>[8]<\/a>.\n\n\n### Define model parameters\n\nLet's set the parameters for the model.","615142fd":"There are 81 distinct values for amount of credit limit.","2033fa9b":"Most of defaults are for credit limits 0-100,000 (and density for this interval is larger for defaults than for non-defaults). Larger defaults number are for the amounts of **50,000**, **20,000** and **30,000**.","9377d9a1":"Correlation is decreasing with distance between months. Lowest correlations are between Sept-April.\n","d1b93f34":"The **ROC-AUC** score obtained with **RandomForrestClassifier** is **0.66**.\n\n\nLet's use for RandomForrestClassifier **dummified variables** for the **categorical features**.\n\n\nWe start by defining the categorical features.\n\n\n### RandomForrest with OneHotEncoder","6fab6e88":"# <a id=\"4\">Check the data<\/a>","0571f7d0":"Let's also visualize the features importance. \n\n#### Features importance","7e1266f1":"Indeed, the largest number of credit cards are with limit of 50,000 (3365), followed by 20,000 (1976) and 30,000 (1610).","21eaa0d4":"Best validation score  was obtained for round **265**, for which **AUC ~= 0.78**.\n\nLet's plot variable importance.","b3e7d57c":"Mean, Q3 and Q4 values are increasing for both male and female with age until aroung 35 years and then they are oscilating and get to a maximum of Q4 for males at age 64.\n\nMean values are generally smaller for males than for females, with few exceptions, for example at age 39, 48, until approximately 60, where mean values for males are generally larger than for females.\n\n","9dc934e0":"Let's train the **RandonForestClassifier** using the **train_fa_df** data and **fit** function.","3d005f4e":"### Run the model\n\nLet's run the model, using the **train** function.","90ea8ed2":"## Sex, Education, Age and Marriage\n\n\nLet's show sex, education, age and marriage distributions.\n\nWe start by showing the boxplots with age distribution grouped by marriage status and sex.\n\nMarriage status meaning is:\n\n* 0 : unknown (let's consider as others as well)\n* 1 : married\n* 2 : single\n* 3 : others\n\nSex meaning is:\n\n* 1 : male\n* 2 : female\n"}}