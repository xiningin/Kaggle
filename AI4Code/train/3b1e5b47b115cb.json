{"cell_type":{"91da9c0b":"code","9e45af27":"code","192240bb":"code","f6fa10db":"code","05aeb740":"code","b30457e9":"code","ac80f4ec":"code","51e650aa":"code","fced7c99":"code","e37ab281":"code","eed0c5a9":"code","33bf73d9":"code","093ad67e":"code","e7004755":"code","02eebf96":"code","7e4a3676":"code","65a0fe37":"code","2054920e":"code","9270f2bc":"code","e53aab23":"code","29c35f41":"code","7403440c":"code","68e37ba5":"code","9ecc4f39":"code","03db58f4":"code","5c36c4bc":"markdown","fed4ff94":"markdown","5e62b77c":"markdown","9abc984f":"markdown","19b305fa":"markdown","b7bd094f":"markdown","dd559768":"markdown","8f1fec16":"markdown","ff03d9b2":"markdown","d7aefbfb":"markdown","5d8a57c2":"markdown","a4964cc0":"markdown","58283551":"markdown"},"source":{"91da9c0b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9e45af27":"data = pd.read_csv(\"\/kaggle\/input\/ogulcancol\/data.csv\")\ndata.head()","192240bb":"data.info()","f6fa10db":"data.drop([\"Unnamed: 32\" , \"id\"] , axis=1 , inplace=True)","05aeb740":"data.head()","b30457e9":"sns.countplot(x=data.diagnosis)\nplt.xlabel(\"DIAGNOSIS\" , color=\"red\" , fontsize=15)\nplt.ylabel(\"FREQUENCY\" , color=\"red\" , fontsize=15)\nplt.title(\"Frequency of Diagnosis\" , color = \"blue\" , fontsize = 20)\nplt.show()","ac80f4ec":"countBenign = len(data[data.diagnosis==\"B\"])\ncountMalignant = len(data[data.diagnosis==\"M\"])\n\nprint(\"Malignant tumor percentage: {}\".format((countMalignant\/len(data.diagnosis))*100))\nprint(\"Benign tumor percentage: {}\".format((countBenign\/len(data.diagnosis))*100))","51e650aa":"data.groupby([\"diagnosis\"]).mean()","fced7c99":"M=data[data.diagnosis==\"M\"]\nB=data[data.diagnosis==\"B\"]","e37ab281":"plt.scatter(M.radius_mean , M.texture_mean , color=\"red\" , label=\"k\u00f6t\u00fc\" , alpha =0.7)\nplt.scatter(B.radius_mean , B.texture_mean , color=\"green\" , label=\"iyi\" , alpha=0.7)\nplt.legend()\nplt.xlabel(\"Radius Mean\" , color=\"red\" , fontsize=13)\nplt.ylabel(\"Texture Mean\" , color=\"red\" , fontsize=13)\nplt.title(\"Scatter Plot\")\nplt.show()","eed0c5a9":"data.diagnosis.replace(to_replace=[\"M\" , \"B\"] , value=[1,0] , inplace=True)","33bf73d9":"y = data.diagnosis.values\nx_data = data.drop([\"diagnosis\"] , axis=1)","093ad67e":"x = (x_data - np.min(x_data))\/(np.max(x_data)- np.min(x_data)).values","e7004755":"x.head()","02eebf96":"from sklearn.model_selection import train_test_split\nx_train ,x_test, y_train  , y_test = train_test_split(x,y,test_size=0.2,random_state=42)","7e4a3676":"x_train = x_train.T","65a0fe37":"\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T","2054920e":"print(x_test.shape)\nprint(x_train.shape)\nprint(y_train.shape)\nprint(y_test.shape)","9270f2bc":"def initialize_weights_and_bias(dimension):\n    \n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b\nw,b = initialize_weights_and_bias(30)","e53aab23":"def sigmoid(z):\n    y_head=1\/(1+np.exp(-z))\n    return y_head\n","29c35f41":"def forward_backward_propagation(w,b,x_train,y_train):\n    #forward\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]\n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost,gradients\n","7403440c":"def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","68e37ba5":"def predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","9ecc4f39":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 30\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    # Print test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 300)    ","03db58f4":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nprint(\"test accuracy {}\".format(lr.score(x_test.T,y_test.T)))","5c36c4bc":"# Implementing Update Parameters","fed4ff94":"## NORMAL\u0130ZAT\u0130ON","5e62b77c":"# Implementing Initializing Parameters and Sigmoid Function","9abc984f":"## Initializing Parameters","19b305fa":"# Logistic Regression with Sklearn","b7bd094f":"#  Implementing Prediction\n","dd559768":"# Dataset Train-Test Split","8f1fec16":"# Implementing Forward and Backward Propagation","ff03d9b2":"# Implementing Logistic Regression","d7aefbfb":"# Dataset Tan\u0131t\u0131m\u0131 Ve Normalization","5d8a57c2":"## LOOK DATA","a4964cc0":"## Sigmoid Function","58283551":"B : \u0130yi huylu\n\nM : K\u00f6t\u00fc huylu"}}