{"cell_type":{"cad3b727":"code","fb1c843b":"code","ea5b3af7":"code","b0d0f58f":"code","2aac7cf6":"code","5300d49f":"code","0e3ab327":"code","4ecfcf5f":"code","eec94adb":"code","4d9749d8":"code","25155c0d":"code","04e80215":"code","76315214":"code","4aa23b64":"code","45e05d0a":"code","6ea6a93a":"code","0e68c56c":"code","70cc1ab4":"code","5f763644":"code","bf0cc1d8":"code","e75ddf00":"code","0d83b8f8":"code","d8071118":"code","2f3e864a":"code","b908e596":"code","455e3ff9":"code","8ce616cb":"code","a30b390d":"code","11a50476":"code","27d02264":"code","d70ec76e":"code","b7181f8f":"code","790d2e29":"code","e3ac1e5a":"code","6d4b0f38":"code","76599d54":"code","4b953ffa":"code","6045ca28":"code","5518b924":"code","0ebeac45":"code","c35e90a6":"code","05caf026":"code","0cf47695":"code","5a346b62":"code","ad57fb86":"code","f73ddaa1":"code","80bed9a7":"code","70190d3b":"code","44cc29df":"code","5f015a96":"code","21063402":"code","30137ef8":"code","d65ca132":"code","ff5fcb14":"code","2f951c57":"code","3e56566d":"code","849f540b":"code","832b1eff":"code","29b84986":"code","d3032238":"code","c0bf7c82":"code","2de56167":"code","3deead33":"code","66dba0fd":"code","59127a22":"code","a2335974":"code","f47005cb":"markdown","dc9bc2eb":"markdown","32f220fa":"markdown","1c4864d5":"markdown","580a3c55":"markdown","bd95e4e5":"markdown","78770abe":"markdown","789f3e0e":"markdown","40a06f58":"markdown","05f246af":"markdown","58a0367f":"markdown","e72aec2e":"markdown","e38985b1":"markdown","77d02506":"markdown","2bed0ad7":"markdown","fbd3a4ad":"markdown","fce92ca2":"markdown","0e814b55":"markdown","ff766a87":"markdown","da1a1af7":"markdown","1fa6f5a9":"markdown","7b111b2f":"markdown","a56d8218":"markdown","7460f577":"markdown","436a910b":"markdown","34af31ae":"markdown","850972ae":"markdown","0918ab99":"markdown","14636ec9":"markdown","1b1b982d":"markdown","5e61e0ba":"markdown","4f7c5c29":"markdown","b6d5f384":"markdown","dfc9eaa0":"markdown","a85ccb4b":"markdown","63084be7":"markdown","6e72ba4b":"markdown","c749478f":"markdown","3f729d44":"markdown","7919b8d1":"markdown","f599a280":"markdown","abd8f1fa":"markdown","d23edd45":"markdown","041146d3":"markdown","3681f3d6":"markdown","7ac0a056":"markdown","3648c487":"markdown","f0cced61":"markdown","835bb34d":"markdown"},"source":{"cad3b727":"!pip install stegano                     # steganalysis library\n!pip install -q efficientnet_pytorch     # Convolutional Neural Net from Google Research","fb1c843b":"import stegano\nfrom stegano import lsb\n\n# System\nimport cv2\nimport os, os.path\nfrom PIL import Image              # from RBG to YCbCr\n\n# Basics\nimport pandas as pd\nimport numpy as np\nfrom numpy import pi                # for DCT\nfrom numpy import r_                # for DCT\nimport scipy                        # for cosine similarity\nfrom scipy import fftpack           # for DCT\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg    # to check images\n%matplotlib inline\nfrom tqdm.notebook import tqdm      # beautiful progression bar\n\n# SKlearn\nfrom sklearn.model_selection import KFold\nfrom sklearn import metrics\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch import FloatTensor, LongTensor\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport torch.nn.functional as F\n\n# Data Augmentation for Image Preprocessing\nfrom albumentations import (ToFloat, Normalize, VerticalFlip, HorizontalFlip, Compose, Resize,\n                            RandomBrightness, RandomContrast, HueSaturationValue, Blur, GaussNoise)\nfrom albumentations.pytorch import ToTensorV2, ToTensor\nfrom efficientnet_pytorch import EfficientNet\nfrom torchvision.models import resnet34\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ea5b3af7":"print(list(os.listdir(\"..\/input\/v2-effnet-epoch-6-auc-08023\")))","b0d0f58f":"# Create a new image with secret message\nmsg_to_hide = \"Message encoded blue cats from Mars are coming to enslave us all be aware!!!!!!\"\nsecret = lsb.hide(\"..\/input\/suki-image\/capture27.png\", \n                    msg_to_hide, \n                    auto_convert_rgb=True)\nsecret.save(\".\/SukiSecret.png\")\n\n# Reveal the hidden message\nprint(lsb.reveal(\".\/SukiSecret.png\"))\n\n# See the 2 images side by side (no apparent difference, but WE KNOW the text is there.)\nf, ax = plt.subplots(1, 2, figsize=(14,5))\n                           \noriginal = mpimg.imread('..\/input\/suki-image\/capture27.png')\noriginal_plot = ax[0].imshow(original)\n\naltered = mpimg.imread('.\/SukiSecret.png')\naltered_plot = ax[1].imshow(altered)","2aac7cf6":"# From image to array \n# (vectorize the matrix to be able to feed it to the cosine function)\noriginal_vector = np.array(original).flatten()\naltered_vector = np.array(altered).flatten()\n\nprint('Original shape:', original_vector.shape, '\\n' +\n      'Altered shape:', altered_vector.shape)\n\n\n# Distance between the original image and itself (should be 0, because they are identical)\ndist1 = np.sum(original_vector - original_vector)\nprint('Dist1:', dist1)\n\n# Distance between the original image and altered image\ndist2 = np.sum(original_vector - altered_vector)\nprint('Dist2:', dist2)","5300d49f":"# ---- STATICS ----\nbase_path = '..\/input\/alaska2-image-steganalysis'\n\ndef read_images_path(dir_name='Cover', test = False):\n    '''series_name: 0001.jpg, 0002.jpg etc.\n    series_paths: is the complete path to a certain image.'''\n    \n    # Get name of the files\n    series_name = pd.Series(os.listdir(base_path + '\/' + dir_name))\n    if test:\n        series_name = pd.Series(os.listdir(base_path + '\/' + 'Test'))\n    \n    # Create the entire path\n    series_paths = pd.Series(base_path + '\/' + dir_name + '\/' + series_name)\n    \n    return series_paths","0e3ab327":"# Read in the data\ncover_paths = read_images_path('Cover', False)\njmipod_paths = read_images_path('JMiPOD', False)\njuniward_paths = read_images_path('JUNIWARD', False)\nuerd_paths = read_images_path('UERD', False)\ntest_paths = read_images_path('Test', True)","4ecfcf5f":"def show15(title = \"Default\"):\n    '''Shows n amount of images in the data'''\n    plt.figure(figsize=(16,9))\n    plt.suptitle(title, fontsize = 16)\n    \n    for k, path in enumerate(cover_paths[:15]):\n        cover = mpimg.imread(path)\n        \n        plt.subplot(3, 5, k+1)\n        plt.imshow(cover)\n        plt.axis('off')","eec94adb":"show15(title = \"15 Original Images\")","4d9749d8":"image_sample = mpimg.imread(cover_paths[0])\n\nprint('Image sample shape:', image_sample.shape)\nprint('Image sample size:', image_sample.size)\nprint('Image sample data type:', image_sample.dtype)","25155c0d":"def show_images_alg(n = 3, title=\"Default\"):\n    '''Returns a plot of the original Image and Encoded ones.\n    n: number of images to display'''\n    \n    f, ax = plt.subplots(n, 4, figsize=(16, 7))\n    plt.suptitle(title, fontsize = 16)\n    \n\n    for index in range(n):\n        cover = mpimg.imread(cover_paths[index])\n        ipod = mpimg.imread(jmipod_paths[index])\n        juni = mpimg.imread(juniward_paths[index])\n        uerd = mpimg.imread(uerd_paths[index])\n\n        # Plot\n        ax[index, 0].imshow(cover)\n        ax[index, 1].imshow(ipod)\n        ax[index, 2].imshow(juni)\n        ax[index, 3].imshow(uerd)\n        \n        # Add titles\n        if index == 0:\n            ax[index, 0].set_title('Original', fontsize=12)\n            ax[index, 1].set_title('IPod', fontsize=12)\n            ax[index, 2].set_title('Juni', fontsize=12)\n            ax[index, 3].set_title('Uerd', fontsize=12)","04e80215":"show_images_alg(n = 3, title = \"Algorithm Difference\")","76315214":"def show_ycbcr_images(n = 3, title = \"Default\"):\n    '''Shows n images as: original RGB, YCbCr and Y, Cb, Cr channels split'''\n    \n    # 4: original image, YCbCr image, Y, Cb, Cr (separate chanels)\n    fig, ax = plt.subplots(n, 5, figsize=(16, 7))\n    plt.suptitle(title, fontsize = 16)\n\n    for index, path in enumerate(cover_paths[:n]):\n        # Read in the original image and convert\n        original_image = Image.open(path)\n        ycbcr_image = original_image.convert('YCbCr')\n        (y, cb, cr) = ycbcr_image.split()\n\n        # Plot\n        ax[index, 0].imshow(original_image)\n        ax[index, 1].imshow(ycbcr_image)\n        ax[index, 2].imshow(y)\n        ax[index, 3].imshow(cb)\n        ax[index, 4].imshow(cr)\n\n        # Add Title\n        if index==0:\n            ax[index, 0].set_title('Original', fontsize=12)\n            ax[index, 1].set_title('YCbCr', fontsize=12)\n            ax[index, 2].set_title('Y', fontsize=12)\n            ax[index, 3].set_title('Cb', fontsize=12)\n            ax[index, 4].set_title('Cr', fontsize=12)","4aa23b64":"show_ycbcr_images(n = 3, title = \"YCbCr Channels\")","45e05d0a":"# Read in an Image Example\nimage = mpimg.imread(cover_paths[2])\n\nplt.figure(figsize = (6, 6))\nplt.imshow(image)\nplt.title('Original Image', fontsize=16)\nplt.axis('off');","6ea6a93a":"# Define 2D DCT\ndef dct2(a):\n    # Return the Discrete Cosine Transform of arbitrary type sequence x.\n    return fftpack.dct(fftpack.dct( a, axis=0, norm='ortho' ), axis=1, norm='ortho')\n\n# Perform a blockwise DCT\nimsize = image.shape\ndct = np.zeros(imsize)\n\n# Do 8x8 DCT on image (in-place)\nfor i in r_[:imsize[0]:8]:\n    for j in r_[:imsize[1]:8]:\n        dct[i:(i+8),j:(j+8)] = dct2( image[i:(i+8),j:(j+8)] )","0e68c56c":"# ---- STATICS ----\npos = 128   # can be changed\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n# Display original\nax1.imshow(image[pos:pos+8,pos:pos+8],cmap='gray')\nax1.set_title(\"An 8x8 block : Original Image\", fontsize=16)\n\n# Display the dct of that block\nax2.imshow(dct[pos:pos+8,pos:pos+8],cmap='gray',vmax= np.max(dct)*0.01,vmin = 0, extent=[0,pi,pi,0])\nax2.set_title(\"An 8x8 DCT block\", fontsize = 16);","70cc1ab4":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n# Original image\nax1.imshow(image);\nax1.set_title(\"Original Image\", fontsize = 16);\n\n# DCT Blocks\nax2.imshow(dct,cmap='gray',vmax = np.max(dct)*0.01,vmin = 0)\nax2.set_title(\"DCT blocks\", fontsize = 14);","5f763644":"# Threshold\nthresh = 0.02\ndct_thresh = dct * (abs(dct) > (thresh*np.max(dct)))\n\n\nplt.figure(figsize=(14, 6))\nplt.imshow(dct_thresh, cmap='gray', vmax = np.max(dct)*0.01, vmin = 0)\nplt.title(\"Thresholded 8x8 DCTs of the image\", fontsize = 16)\n\npercent_nonzeros = np.sum( dct_thresh != 0.0 ) \/ (imsize[0]*imsize[1]*1.0)\nprint(\"Keeping only {}% of the DCT coefficients\".format(round(percent_nonzeros*100.0, 3)))","bf0cc1d8":"def set_seed(seed = 1234):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed()\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Device available now:', device)","e75ddf00":"# ----- STATICS -----\nsample_size = 256\nnum_classes = 4\n# -------------------\n\n# Read in Data\n\n# --- 10 classes ---\n# train_df = pd.read_csv('..\/input\/alaska2trainvalsplit\/alaska2_train_df.csv', \n#                        header=0, names=['Path', 'Label'], dtype = {'Label':np.int32})\n# valid_df = pd.read_csv('..\/input\/alaska2trainvalsplit\/alaska2_val_df.csv', \n#                        header=0, names=['Path', 'Label'], dtype = {'Label':np.int32})\n\n# --- 4 classes ---\ntrain_df = pd.read_csv('..\/input\/alaska2-trainvalid-4-class-csv\/alaska2_train_data_4classes.csv', \n                       header=0, names=['Path', 'Label'], dtype = {'Label':np.int32})\nvalid_df = pd.read_csv('..\/input\/alaska2-trainvalid-4-class-csv\/alaska2_valid_data_4classes.csv', \n                       header=0, names=['Path', 'Label'], dtype = {'Label':np.int32})\n\n# Sample out Data\ndef sample_data(dataframe, sample_size, num_classes, train=True):\n    '''Sample same number of images for each label.'''\n    if train:\n        size = int(0.75 * sample_size)\n    else:\n        size = int(0.25 * sample_size)\n        \n    # Number of images in class\n    no = int(np.floor(size\/num_classes))\n    labels = [i for i in range(num_classes)]\n    new_data = pd.DataFrame()\n    \n    # For each label\n    for label in labels:\n        # Sample out data\n        data = dataframe[dataframe['Label'] == label].sample(no, random_state=123)\n        new_data = pd.concat([new_data, data], axis=0)\n        \n    return new_data","0d83b8f8":"# Sample out data\ntrain_df = sample_data(train_df, num_classes=num_classes, \n                       sample_size=sample_size, train=True).reset_index(drop=True)\nvalid_df = sample_data(valid_df, num_classes=num_classes, \n                       sample_size=sample_size, train=False).reset_index(drop=True)","d8071118":"print('Train Data Size:', len(train_df), '\\n' +\n      'Valid Data Size:', len(valid_df), '\\n' +\n      '----------------------', '\\n' +\n      'Total:', len(train_df) + len(valid_df))\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (14, 5))\n\nsns.countplot(x = train_df['Label'], ax = ax1, palette = sns.color_palette(\"GnBu_d\", 10))\nsns.countplot(x = valid_df['Label'], ax = ax2, palette = sns.color_palette(\"YlOrRd\", 10))\n\nax1.set_title('Train Data', fontsize=16)\nax2.set_title('Valid Data', fontsize=16);","2f3e864a":"class AlaskaDataset(Dataset):\n    '''Alaska2 Dataset.\n    If data is test or eval, it skips the transformations applied to training part.'''\n            \n    def __init__(self, dataframe, is_test, is_val, vertical_flip=0.5, horizontal_flip=0.5):\n        self.dataframe, self.is_test, self.is_val = dataframe, is_test, is_val\n        self.vertical_flip, self.horizontal_flip = vertical_flip, horizontal_flip\n        # Flag to mark Testing and Evaluation Datasets\n        flag = is_test or is_val\n        \n        # If data is NOT Train\n        if flag:\n            self.transform = Compose([Resize(512, 512), \n                                      Normalize(),\n                                      ToFloat(max_value=255),\n                                      ToTensor()])\n        else:\n            # Compose transforms and handle all transformations regarding bounding boxes\n            self.transform = Compose([Resize(512, 512), \n                                      VerticalFlip(p = vertical_flip),\n                                      HorizontalFlip(p = horizontal_flip),\n                                      Normalize(),\n                                      ToFloat(max_value=255),\n                                      # Convert image and mask to torch.Tensor\n                                      ToTensor()])        \n        \n    \n    # So len(data) returns the size of dataset\n    def __len__(self):\n        return len(self.dataframe)\n    \n    # Very important function for Data Loader\n    def __getitem__(self, index):\n        \n        if self.is_test: \n            path = self.dataframe.loc[index][0]\n        else:\n            path, label = self.dataframe.loc[index]\n        \n        # ::-1 to not overload memory\n        image = cv2.imread(path)[:, :, ::-1]\n        image = self.transform(image=image)\n        image = image['image']\n        \n        if self.is_test:\n            return image\n        else:\n            return image, label","b908e596":"class EfficientNetwork(nn.Module):\n    def __init__(self, output_size, b1=False, b2=False):\n        super().__init__()\n        self.b1, self.b2 = b1, b2\n        \n        # Define Feature part\n        if b1:\n            self.features = EfficientNet.from_pretrained('efficientnet-b1')\n        elif b2:\n            self.features = EfficientNet.from_pretrained('efficientnet-b2')\n        else:\n            self.features = EfficientNet.from_pretrained('efficientnet-b0')\n        \n        # Define Classification part\n        if b1:\n            self.classification = nn.Linear(1280, output_size)\n        elif b2:\n            self.classification = nn.Linear(1408, output_size)\n        else:\n            self.classification = nn.Linear(1280, output_size)\n        \n        \n    def forward(self, image, prints=False):\n        if prints: print('Input Image shape:', image.shape)\n        \n        image = self.features.extract_features(image)\n        if prints: print('Features Image shape:', image.shape)\n            \n        if self.b1:\n            image = F.avg_pool2d(image, image.size()[2:]).reshape(-1, 1280)\n        elif self.b2:\n            image = F.avg_pool2d(image, image.size()[2:]).reshape(-1, 1408)\n        else:\n            image = F.avg_pool2d(image, image.size()[2:]).reshape(-1, 1280)\n        if prints: print('Image Reshaped shape:', image.shape)\n        \n        out = self.classification(image)\n        if prints: print('Out shape:', out.shape)\n        \n        return out","455e3ff9":"# Create an example model (B2)\nmodel_example = EfficientNetwork(output_size=num_classes, b1=False, b2=True)","8ce616cb":"# Data object and Loader\nexample_data = AlaskaDataset(train_df, is_test=False, is_val=False)\nexample_loader = torch.utils.data.DataLoader(example_data, batch_size = 1, shuffle=True)\n\n# Get a sample\nfor image, labels in example_loader:\n    images_example = image\n    labels_example = torch.tensor(labels, dtype=torch.long)\n    break\nprint('Images shape:', images_example.shape)\nprint('Labels:', labels, '\\n')\n\n# Outputs\nout = model_example(images_example, prints=True)\n\n# Criterion example\ncriterion_example = nn.CrossEntropyLoss()\nloss = criterion_example(out, labels_example)\nprint('Loss:', loss.item())","a30b390d":"class ResNet34Network(nn.Module):\n    def __init__(self, output_size):\n        super().__init__()\n        \n        # Define Feature part\n        self.features = resnet34(pretrained=True)\n        \n        # Define Classification part\n        self.classification = nn.Linear(1000, output_size)\n        \n        \n    def forward(self, image, prints=False):\n        if prints: print('Input Image shape:', image.shape)\n        \n        image = self.features(image)\n        if prints: print('Features Image shape:', image.shape)\n        \n        out = self.classification(image)\n        if prints: print('Out shape:', out.shape)\n        \n        return out","11a50476":"# Create an example model\nmodel_example = ResNet34Network(output_size=num_classes)","27d02264":"# Data object and Loader\nexample_data = AlaskaDataset(train_df, is_test=False, is_val=False)\nexample_loader = torch.utils.data.DataLoader(example_data, batch_size = 1, shuffle=True)\n\n# Get a sample\nfor image, labels in example_loader:\n    images_example = image\n    labels_example = torch.tensor(labels, dtype=torch.long)\n    break\nprint('Images shape:', images_example.shape)\nprint('Labels:', labels, '\\n')\n\n# Outputs\nout = model_example(images_example, prints=True)\n\n# Criterion example\ncriterion_example = nn.CrossEntropyLoss()\nloss = criterion_example(out, labels_example)\nprint('Loss:', loss.item())","d70ec76e":"# ----- STATICS -----\nvertical_flip = 0.5\nhorizontal_flip = 0.5\n# -------------------","b7181f8f":"# Data Objects\ntrain_data = AlaskaDataset(train_df, is_test=False, is_val=False, \n                           vertical_flip=vertical_flip, horizontal_flip=horizontal_flip)\nvalid_data = AlaskaDataset(valid_df, is_test=False, is_val=True, \n                           vertical_flip=vertical_flip, horizontal_flip=horizontal_flip)","790d2e29":"def alaska_weighted_auc(y_true, y_valid):\n    tpr_thresholds = [0.0, 0.4, 1.0]\n    weights = [2, 1]\n    \n    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_valid, pos_label=1)\n    \n    # Size of subsets\n    areas = np.array(tpr_thresholds[1:]) - np.array(tpr_thresholds[:-1])\n    \n    # The total area is normalized by the sum of weights such that the final weighted AUC is between 0 and 1.\n    normalization = np.dot(areas, weights)\n    \n    competition_metric = 0\n    for idx, weight in enumerate(weights):\n        y_min = tpr_thresholds[idx]\n        y_max = tpr_thresholds[idx + 1]\n        mask = (y_min < tpr) & (tpr < y_max)\n\n        x_padding = np.linspace(fpr[mask][-1], 1, 100)\n\n        x = np.concatenate([fpr[mask], x_padding])\n        y = np.concatenate([tpr[mask], [y_max] * len(x_padding)])\n        # Normalize such that curve starts at y = 0\n        y = y - y_min \n        score = metrics.auc(x, y)\n        submetric = score * weight\n        best_subscore = (y_max - y_min) * weight\n        competition_metric += submetric\n        \n    return competition_metric \/ normalization","e3ac1e5a":"def train(model, epochs, batch_size, num_workers, learning_rate, weight_decay, \n          version = 'vx', plot_loss=False):\n    # Create file to save logs\n    f = open(f\"logs_{version}.txt\", \"w+\")\n    \n    # Best AUC value\n    best_auc = None    \n    \n    # Data Loaders\n    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers,\n                                              drop_last=True, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, num_workers=num_workers,\n                                              drop_last=True, shuffle=True)\n\n    # Criterion\n    criterion = torch.nn.CrossEntropyLoss()\n    # Optimizer\n    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    # Scheduler\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='max',\n                                                           patience=1, verbose=True, factor=0.4)\n\n\n    train_losses = []\n    evaluation_losses = []\n\n    for epoch in range(epochs):\n\n        # Sets the model in training mode\n        model.train()\n\n        train_loss = 0\n\n        for images, labels in train_loader:\n            # Need to access the images\n            images = images.to(device, dtype=torch.float)\n            labels = labels.to(device, dtype=torch.long)\n\n            # Clear gradients\n            optimizer.zero_grad()\n\n            # Make prediction\n            out = model(images)\n\n            # Compute loss and Backpropagate\n            loss = criterion(out, labels)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n\n        # Compute average epoch loss\n        epoch_loss_train = train_loss \/ batch_size\n        train_losses.append(epoch_loss_train)\n\n\n        # ===== Evaluate =====\n        model.eval()\n\n        evaluation_loss = 0\n        actuals, predictions = [], []\n\n        # To disable gradients\n        with torch.no_grad():\n            for images, labels in valid_loader:\n                images = images.to(device, dtype=torch.float)\n                labels = labels.to(device, dtype=torch.long)\n\n                # Prediction\n                out = model(images)\n                loss = criterion(out, labels)\n                actuals.extend(labels.cpu().numpy().astype(int))\n                predictions.extend(F.softmax(out, 1).cpu().numpy())\n\n                evaluation_loss += loss.item()\n\n        # Compute epoch loss\n        epoch_loss_eval = evaluation_loss\/batch_size\n        evaluation_losses.append(epoch_loss_eval)\n\n        # Prepare predictions and actuals\n        predictions = np.array(predictions)\n        # Choose label (array)\n        predicted_labels = predictions.argmax(1)\n\n        # ----- Accuracy -----\n        accuracy = (predicted_labels == actuals).mean()\n\n        # Compute AUC\n        new_preds = np.zeros(len(predictions))\n        temp = predictions[predicted_labels != 0, 1:]\n\n        new_preds[predicted_labels != 0] = temp.sum(1)\n        new_preds[predicted_labels == 0] = 1 - predictions[predicted_labels == 0, 0]\n        actuals = np.array(actuals)\n        actuals[actuals != 0] = 1\n\n        auc_score = alaska_weighted_auc(actuals, new_preds)\n\n\n        with open(f\"logs_{version}.txt\", 'a+') as f:\n            print('Epoch: {}\/{} | Train Loss: {:.3f} | Eval Loss: {:.3f} | AUC: {:3f} | Acc: {:3f}'.\\\n                     format(epoch+1, epochs, epoch_loss_train, epoch_loss_eval, auc_score, accuracy), file=f)\n        \n        print('Epoch: {}\/{} | Train Loss: {:.3f} | Eval Loss: {:.3f} | AUC: {:3f} | Acc: {:3f}'.\\\n              format(epoch+1, epochs, epoch_loss_train, epoch_loss_eval, auc_score, accuracy))\n\n        # Update AUC\n        # If AUC is improving, then we also save model\n        if best_auc == None:\n            best_auc = auc_score\n            torch.save(model.state_dict(),\n                       f\"Epoch_{epoch+1}_ValLoss_{epoch_loss_eval:.3f}_AUC_{auc_score:.3f}.pth\")\n            continue\n            \n        if auc_score > best_auc:\n            best_auc = auc_score\n            torch.save(model.state_dict(),\n                       f\"Epoch_{epoch+1}_ValLoss_{epoch_loss_eval:.3f}_AUC_{auc_score:.3f}.pth\")\n        \n        # Update scheduler (for learning_rate)\n        scheduler.step(auc_score)\n        \n    # Plots the loss of Train and Valid\n    if plot_loss:\n        plt.figure(figsize=(14,5))\n        plt.plot(train_losses, c='#fdc975ff', lw = 3)\n        plt.plot(evaluation_losses, c='#29896bff', lw = 3)\n        plt.legend(['Train Loss', 'Evaluation Loss'])\n        plt.title('Losses over Epochs');","6d4b0f38":"# ----- STATICS -----\nversion = 'v8'\nepochs = 10\nbatch_size = 32\nnum_workers = 8\nlearning_rate = 0.0001\nweight_decay = 0.00001\nplot_loss = False\n# -------------------","76599d54":"# # Efficient Net B0\n# eff_net0 = EfficientNetwork(output_size = num_classes, b1=False, b2=False).to(device)\n\n# # Load any pretrained model\n# eff_net0.load_state_dict(torch.load('..\/input\/v2-effnet-epoch-6-auc-08023\/10Class_epoch_14_val_loss_77.34_auc_0.78_EffNetB0.pth'))\n\n# # Uncomment and train the model\n# train(model=eff_net0, epochs=epochs, batch_size=batch_size, num_workers=num_workers, \n#       learning_rate=learning_rate, weight_decay=weight_decay, plot_loss=plot_loss)","4b953ffa":"# Efficient Net B2\neff_net2 = EfficientNetwork(output_size = num_classes, b1=False, b2=True).to(device)\n\n# # Add previous trained model:\n# eff_net2.load_state_dict(torch.load('..\/input\/v2-effnet-epoch-6-auc-08023\/Epoch_7_ValLoss_58.146_AUC_0.799.pth'))\n\n# Uncomment and train the model\n# train(model=eff_net2, epochs=epochs, batch_size=batch_size, num_workers=num_workers, \n#       learning_rate=learning_rate, weight_decay=weight_decay, plot_loss=plot_loss)","6045ca28":"# # ResNet34\n# eff_net34 = ResNet34Network(output_size = num_classes).to(device)\n\n# # Uncomment and train the model\n# train(model=eff_net34, epochs=epochs, batch_size=batch_size, num_workers=num_workers, \n#       learning_rate=learning_rate, weight_decay=weight_decay, plot_loss=plot_loss)","5518b924":"# Extract a sample of paths\ndirectory = '..\/input\/alaska2-image-steganalysis\/'\nname = pd.Series(sorted(os.listdir(directory + 'Test\/')))\npath = pd.Series(directory + 'Test\/' + name)\n\n# Create dataframe\ntest_df = pd.DataFrame(data=path, columns=['Path'])\n\n# Dataset\ntest_data = AlaskaDataset(test_df, is_test=True, is_val=False,\n                          vertical_flip=vertical_flip, horizontal_flip=horizontal_flip)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle=False)","0ebeac45":"# list(os.listdir('..\/working\/Epoch_17_ValLoss_36.195_AUC_0.794.pth'))\n\n# Import model if necessary:\n# Load any pretrained model\n# eff_net2.load_state_dict(torch.load('..\/working\/Epoch_17_ValLoss_36.195_AUC_0.794.pth'))","c35e90a6":"# Evaluation Mode\neff_net2.eval()\n\npredictions = []\n\nwith torch.no_grad():\n    for k, images in enumerate(test_loader):\n        images = images.to(device)\n        out0 = eff_net2(images)\n        \n        # Flip vertical\n        images_vertical = images.flip(2)\n        out1 = eff_net2(images_vertical)\n        \n        # Flip again original\n        images_flip = images.flip(3)\n        out2 = eff_net2(images_flip)\n        \n        # 50% results from flip + 50% result from normal\n        outputs = (0.25*out1 + 0.25*out2)\n        outputs = (outputs + 0.5*out0)\n\n        predictions.extend(F.softmax(outputs, 1).cpu().numpy())","05caf026":"# Making the predictions the same manner as in Train Function\npredictions = np.array(predictions)\npredicted_labels = predictions.argmax(1)\nnew_preds = np.zeros(len(predictions))\ntemp = predictions[predicted_labels != 0, 1:]\nnew_preds[predicted_labels != 0] = temp.sum(1)\nnew_preds[predicted_labels == 0] = 1 - predictions[predicted_labels == 0, 0]","0cf47695":"ss = pd.read_csv('..\/input\/alaska2-image-steganalysis\/sample_submission.csv')\nss['Label'] = new_preds\n\nss.to_csv(f'submission_{version}.csv', index=False)","5a346b62":"!pip install -q efficientnet_pytorch > \/dev\/null","ad57fb86":"from glob import glob\nfrom sklearn.model_selection import GroupKFold\nimport cv2\nfrom skimage import io\nimport torch\nfrom torch import nn\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nimport sklearn\n\nSEED = 42\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)","f73ddaa1":"%%time\n\ndataset = []\n\nfor label, kind in enumerate(['Cover', 'JMiPOD', 'JUNIWARD', 'UERD']):\n    for path in glob('..\/input\/alaska2-image-steganalysis\/Cover\/*.jpg'):\n        dataset.append({\n            'kind': kind,\n            'image_name': path.split('\/')[-1],\n            'label': label\n        })\n\nrandom.shuffle(dataset)\ndataset = pd.DataFrame(dataset)\n\ngkf = GroupKFold(n_splits=5)\n\ndataset.loc[:, 'fold'] = 0\nfor fold_number, (train_index, val_index) in enumerate(gkf.split(X=dataset.index, y=dataset['label'], groups=dataset['image_name'])):\n    dataset.loc[dataset.iloc[val_index].index, 'fold'] = fold_number","80bed9a7":"# dataset = pd.read_csv('..\/input\/alaska2-public-baseline\/groupkfold_by_shonenkov.csv')","70190d3b":"def get_train_transforms():\n    return A.Compose([\n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.5),\n            A.Resize(height=512, width=512, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.0)\n\ndef get_valid_transforms():\n    return A.Compose([\n            A.Resize(height=512, width=512, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.0)","44cc29df":"DATA_ROOT_PATH = '..\/input\/alaska2-image-steganalysis'\n\ndef onehot(size, target):\n    vec = torch.zeros(size, dtype=torch.float32)\n    vec[target] = 1.\n    return vec\n\nclass DatasetRetriever(Dataset):\n\n    def __init__(self, kinds, image_names, labels, transforms=None):\n        super().__init__()\n        self.kinds = kinds\n        self.image_names = image_names\n        self.labels = labels\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n        kind, image_name, label = self.kinds[index], self.image_names[index], self.labels[index]\n        image = cv2.imread(f'{DATA_ROOT_PATH}\/{kind}\/{image_name}', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n        if self.transforms:\n            sample = {'image': image}\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n        target = onehot(4, label)\n        return image, target\n\n    def __len__(self) -> int:\n        return self.image_names.shape[0]\n\n    def get_labels(self):\n        return list(self.labels)","5f015a96":"fold_number = 0\n\ntrain_dataset = DatasetRetriever(\n    kinds=dataset[dataset['fold'] != fold_number].kind.values,\n    image_names=dataset[dataset['fold'] != fold_number].image_name.values,\n    labels=dataset[dataset['fold'] != fold_number].label.values,\n    transforms=get_train_transforms(),\n)\n\nvalidation_dataset = DatasetRetriever(\n    kinds=dataset[dataset['fold'] == fold_number].kind.values,\n    image_names=dataset[dataset['fold'] == fold_number].image_name.values,\n    labels=dataset[dataset['fold'] == fold_number].label.values,\n    transforms=get_valid_transforms(),\n)","21063402":"image, target = train_dataset[0]\nnumpy_image = image.permute(1,2,0).cpu().numpy()\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\n    \nax.set_axis_off()\nax.imshow(numpy_image);","30137ef8":"from sklearn import metrics\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n        \n        \ndef alaska_weighted_auc(y_true, y_valid):\n    \"\"\"\n    https:\/\/www.kaggle.com\/anokas\/weighted-auc-metric-updated\n    \"\"\"\n    tpr_thresholds = [0.0, 0.4, 1.0]\n    weights = [2, 1]\n\n    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_valid, pos_label=1)\n\n    # size of subsets\n    areas = np.array(tpr_thresholds[1:]) - np.array(tpr_thresholds[:-1])\n\n    # The total area is normalized by the sum of weights such that the final weighted AUC is between 0 and 1.\n    normalization = np.dot(areas, weights)\n\n    competition_metric = 0\n    for idx, weight in enumerate(weights):\n        y_min = tpr_thresholds[idx]\n        y_max = tpr_thresholds[idx + 1]\n        mask = (y_min < tpr) & (tpr < y_max)\n        # pdb.set_trace()\n\n        x_padding = np.linspace(fpr[mask][-1], 1, 100)\n\n        x = np.concatenate([fpr[mask], x_padding])\n        y = np.concatenate([tpr[mask], [y_max] * len(x_padding)])\n        y = y - y_min  # normalize such that curve starts at y=0\n        score = metrics.auc(x, y)\n        submetric = score * weight\n        best_subscore = (y_max - y_min) * weight\n        competition_metric += submetric\n\n    return competition_metric \/ normalization\n        \nclass RocAucMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.y_true = np.array([0,1])\n        self.y_pred = np.array([0.5,0.5])\n        self.score = 0\n\n    def update(self, y_true, y_pred):\n        y_true = y_true.cpu().numpy().argmax(axis=1).clip(min=0, max=1).astype(int)\n        y_pred = 1 - nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,0]\n        self.y_true = np.hstack((self.y_true, y_true))\n        self.y_pred = np.hstack((self.y_pred, y_pred))\n        self.score = alaska_weighted_auc(self.y_true, self.y_pred)\n    \n    @property\n    def avg(self):\n        return self.score","d65ca132":"class LabelSmoothing(nn.Module):\n    def __init__(self, smoothing = 0.05):\n        super(LabelSmoothing, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n\n    def forward(self, x, target):\n        if self.training:\n            x = x.float()\n            target = target.float()\n            logprobs = torch.nn.functional.log_softmax(x, dim = -1)\n\n            nll_loss = -logprobs * target\n            nll_loss = nll_loss.sum(-1)\n    \n            smooth_loss = -logprobs.mean(dim=-1)\n\n            loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n\n            return loss.mean()\n        else:\n            return torch.nn.functional.cross_entropy(x, target)","ff5fcb14":"import warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nclass Fitter:\n    \n    def __init__(self, model, device, config):\n        self.config = config\n        self.epoch = 0\n        \n        self.base_dir = '.\/'\n        self.log_path = f'{self.base_dir}\/log.txt'\n        self.best_summary_loss = 10**5\n\n        self.model = model\n        self.device = device\n\n        param_optimizer = list(self.model.named_parameters())\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ] \n\n        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.lr)\n        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n        self.criterion = LabelSmoothing().to(self.device)\n        self.log(f'Fitter prepared. Device is {self.device}')\n\n    def fit(self, train_loader, validation_loader):\n        for e in range(self.config.n_epochs):\n            if self.config.verbose:\n                lr = self.optimizer.param_groups[0]['lr']\n                timestamp = datetime.utcnow().isoformat()\n                self.log(f'\\n{timestamp}\\nLR: {lr}')\n\n            t = time.time()\n            summary_loss, final_scores = self.train_one_epoch(train_loader)\n\n            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, final_score: {final_scores.avg:.5f}, time: {(time.time() - t):.5f}')\n            self.save(f'{self.base_dir}\/last-checkpoint.bin')\n\n            t = time.time()\n            summary_loss, final_scores = self.validation(validation_loader)\n\n            self.log(f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, final_score: {final_scores.avg:.5f}, time: {(time.time() - t):.5f}')\n            if summary_loss.avg < self.best_summary_loss:\n                self.best_summary_loss = summary_loss.avg\n                self.model.eval()\n                self.save(f'{self.base_dir}\/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n                for path in sorted(glob(f'{self.base_dir}\/best-checkpoint-*epoch.bin'))[:-3]:\n                    os.remove(path)\n\n            if self.config.validation_scheduler:\n                self.scheduler.step(metrics=summary_loss.avg)\n\n            self.epoch += 1\n\n    def validation(self, val_loader):\n        self.model.eval()\n        summary_loss = AverageMeter()\n        final_scores = RocAucMeter()\n        t = time.time()\n        for step, (images, targets) in enumerate(val_loader):\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Val Step {step}\/{len(val_loader)}, ' + \\\n                        f'summary_loss: {summary_loss.avg:.5f}, final_score: {final_scores.avg:.5f}, ' + \\\n                        f'time: {(time.time() - t):.5f}', end='\\r'\n                    )\n            with torch.no_grad():\n                targets = targets.to(self.device).float()\n                batch_size = images.shape[0]\n                images = images.to(self.device).float()\n                outputs = self.model(images)\n                loss = self.criterion(outputs, targets)\n                final_scores.update(targets, outputs)\n                summary_loss.update(loss.detach().item(), batch_size)\n\n        return summary_loss, final_scores\n\n    def train_one_epoch(self, train_loader):\n        self.model.train()\n        summary_loss = AverageMeter()\n        final_scores = RocAucMeter()\n        t = time.time()\n        for step, (images, targets) in enumerate(train_loader):\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Train Step {step}\/{len(train_loader)}, ' + \\\n                        f'summary_loss: {summary_loss.avg:.5f}, final_score: {final_scores.avg:.5f}, ' + \\\n                        f'time: {(time.time() - t):.5f}', end='\\r'\n                    )\n            \n            targets = targets.to(self.device).float()\n            images = images.to(self.device).float()\n            batch_size = images.shape[0]\n\n            self.optimizer.zero_grad()\n            outputs = self.model(images)\n            loss = self.criterion(outputs, targets)\n            loss.backward()\n            \n            final_scores.update(targets, outputs)\n            summary_loss.update(loss.detach().item(), batch_size)\n\n            self.optimizer.step()\n\n            if self.config.step_scheduler:\n                self.scheduler.step()\n\n        return summary_loss, final_scores\n    \n    def save(self, path):\n        self.model.eval()\n        torch.save({\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'best_summary_loss': self.best_summary_loss,\n            'epoch': self.epoch,\n        }, path)\n\n    def load(self, path):\n        checkpoint = torch.load(path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        self.best_summary_loss = checkpoint['best_summary_loss']\n        self.epoch = checkpoint['epoch'] + 1\n        \n    def log(self, message):\n        if self.config.verbose:\n            print(message)\n        with open(self.log_path, 'a+') as logger:\n            logger.write(f'{message}\\n')","2f951c57":"from efficientnet_pytorch import EfficientNet\n\ndef get_net():\n    net = EfficientNet.from_pretrained('efficientnet-b2')\n    net._fc = nn.Linear(in_features=1408, out_features=4, bias=True)\n    return net\n\nnet = get_net().cuda()","3e56566d":"class TrainGlobalConfig:\n    num_workers = 4\n    batch_size = 16 \n    n_epochs = 100\n    lr = 0.0001\n\n    # -------------------\n    verbose = True\n    verbose_step = 1\n    # -------------------\n\n    # --------------------\n    step_scheduler = False  # do scheduler.step after optimizer.step\n    validation_scheduler = True  # do scheduler.step after validation stage loss\n\n#     SchedulerClass = torch.optim.lr_scheduler.OneCycleLR\n#     scheduler_params = dict(\n#         max_lr=0.001,\n#         epochs=n_epochs,\n#         steps_per_epoch=int(len(train_dataset) \/ batch_size),\n#         pct_start=0.1,\n#         anneal_strategy='cos', \n#         final_div_factor=10**5\n#     )\n    \n    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n    scheduler_params = dict(\n        mode='min',\n        factor=0.5,\n        patience=1,\n        verbose=False, \n        threshold=0.0001,\n        threshold_mode='abs',\n        cooldown=0, \n        min_lr=1e-8,\n        eps=1e-08\n    )\n    # --------------------","849f540b":"from catalyst.data.sampler import BalanceClassSampler\n\ndef run_training():\n    device = torch.device('cuda:0')\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        sampler=BalanceClassSampler(labels=train_dataset.get_labels(), mode=\"downsampling\"),\n        batch_size=TrainGlobalConfig.batch_size,\n        pin_memory=False,\n        drop_last=True,\n        num_workers=TrainGlobalConfig.num_workers,\n    )\n    val_loader = torch.utils.data.DataLoader(\n        validation_dataset, \n        batch_size=TrainGlobalConfig.batch_size,\n        num_workers=TrainGlobalConfig.num_workers,\n        shuffle=False,\n        sampler=SequentialSampler(validation_dataset),\n        pin_memory=False,\n    )\n\n    fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)\n#     fitter.load(f'{fitter.base_dir}\/last-checkpoint.bin')\n    fitter.fit(train_loader, val_loader)","832b1eff":"# run_training()","29b84986":"file = open('..\/input\/alaska2-public-baseline\/log.txt', 'r')\nfor line in file.readlines():\n    print(line[:-1])\nfile.close()","d3032238":"checkpoint = torch.load('..\/input\/alaska2-public-baseline\/best-checkpoint-033epoch.bin')\nnet.load_state_dict(checkpoint['model_state_dict']);\nnet.eval();","c0bf7c82":"checkpoint.keys()","2de56167":"class DatasetSubmissionRetriever(Dataset):\n\n    def __init__(self, image_names, transforms=None):\n        super().__init__()\n        self.image_names = image_names\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n        image_name = self.image_names[index]\n        image = cv2.imread(f'{DATA_ROOT_PATH}\/Test\/{image_name}', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n        if self.transforms:\n            sample = {'image': image}\n            sample = self.transforms(**sample)\n            image = sample['image']\n\n        return image_name, image\n\n    def __len__(self) -> int:\n        return self.image_names.shape[0]","3deead33":"dataset = DatasetSubmissionRetriever(\n    image_names=np.array([path.split('\/')[-1] for path in glob('..\/input\/alaska2-image-steganalysis\/Test\/*.jpg')]),\n    transforms=get_valid_transforms(),\n)\n\n\ndata_loader = DataLoader(\n    dataset,\n    batch_size=8,\n    shuffle=False,\n    num_workers=2,\n    drop_last=False,\n)","66dba0fd":"%%time\n\nresult = {'Id': [], 'Label': []}\nfor step, (image_names, images) in enumerate(data_loader):\n    print(step, end='\\r')\n    \n    y_pred = net(images.cuda())\n    y_pred = 1 - nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,0]\n    \n    result['Id'].extend(image_names)\n    result['Label'].extend(y_pred)","59127a22":"submission = pd.DataFrame(result)\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","a2335974":"submission['Label'].hist(bins=100);","f47005cb":"## 4.2 Algorithms\nThere are 3 main different algorithms applied to the original image and used to encode information into it:\n* JMiPOD \n* JUNIWARD\n* UERD\n\n> All images have the corresponding encoding at the same name.","dc9bc2eb":"# 2. Libraries","32f220fa":"## 7.2 Predict Model","1c4864d5":"# 2. PyTorch Dataset \ud83d\udee0\n\n### Dataset Class\n[Link here with example and more explanations](https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html)\n\n`torch.utils.data.Dataset` is an abstract class representing a dataset. Your custom dataset should inherit `Dataset` and override the following methods:\n* `__len__` so that len(dataset) returns the size of the dataset.\n* `__getitem__` to support the indexing such that dataset[i] can be used to get i'th sample\n\n> Note1: Some images are 512x512, so we'll resize first to have everything the same shape\n\n> Note2: [This discussion](https:\/\/www.kaggle.com\/c\/alaska2-image-steganalysis\/discussion\/155392) explains why is better NOT to resize the images further (information within image is scarce, you don't want to make it even more scarce).\n\n<img src='https:\/\/i.imgur.com\/Q69gqzt.png' width=400>","580a3c55":"# EfficientNet","bd95e4e5":"### Checking Similarity \ud83c\udf1c\ud83c\udf1b\nWe can check how similar are the images by substracting one matrix from the other. \n\nLet's check the **similarity** of the abote 2 images, to see if there is any hidden information in the altered image:","78770abe":"## 3.2 ResNet34\n\n* Deep architecture for CNNs that solve vanishing gradient problem\n* Weights can be applied to other Classification problems (like we have here :) )\n\n> Note: [Full blog post here](https:\/\/towardsdatascience.com\/understanding-and-visualizing-resnets-442284831be8)\n<img src='https:\/\/miro.medium.com\/max\/1400\/1*Y-u7dH4WC-dXyn9jOG4w0w.png' width=500>","789f3e0e":"## --------------------------------------THANK YOU-------------------------------------------","40a06f58":"### Look at an 8x8 block: original vs DCT coeff","05f246af":"## 5.2 Visualize DCT Coefficients:\n\n### Discrete Cosine Transform Example\n* [finite sequence of data points in terms of a sum of cosine functions oscillating at different frequencies](https:\/\/en.wikipedia.org\/wiki\/Discrete_cosine_transform)\n* transformation tenchnique for data compression \n* uses *cosine* rather than *sine* (faster computation)\n\n> [The following code is completely taken from this page: JPEG DCT Demo](https:\/\/inst.eecs.berkeley.edu\/~ee123\/sp16\/Sections\/JPEG_DCT_Demo.html#Display-all-DCT-blocks)","58a0367f":"<h1><center>\u2744Model\u2744: Multiclass PyTorch with EffNet<\/center><\/h1>\n\n<div class=\"alert alert-block alert-warning\">\n<p>Note: Inspiration for this notebook is from <a href='https:\/\/www.kaggle.com\/meaninglesslives\/alaska2-cnn-multiclass-classifier?scriptVersionId=33731720'>Alaska2 CNN Multiclass Classifier<\/a> <\/p>\n<\/div>\n\n# 1. Preparing the data \ud83d\udd0e\n\n## 1.1 Set the seeds\ud83c\udf31","e72aec2e":"# Inference","e38985b1":"# Class Balance \"on fly\" from [@CatalystTeam](https:\/\/github.com\/catalyst-team\/catalyst)","77d02506":"# Main Ideas\n\n- 4 Classes\n- GroupKFold splitting\n- Class Balance\n- Flips\n- Label Smoothing\n- EfficientNetB2\n- ReduceLROnPlateau","2bed0ad7":"<img src='https:\/\/i.imgur.com\/PclbNN8.png'>\n\n\n<h1><center>\u2744Alaska2\u2744 Competiton: EDA and Understanding<\/center><\/h1>\n\n# 1. Competition Outline\n\n<div class=\"alert alert-block alert-info\">\nSteganography is the method of hiding secret data in any image\/audio\/video. In a nutshell, the main motive of steganography is to hide the intended information within any image\/audio\/video that doesn\u2019t appear to be secret just by looking at<\/div>\n\n<img src='https:\/\/i.imgur.com\/jo2sZgO.png' width=500>\n\n## 1.1 Description\n\n* **Current Methods** : Produce unreliable results, raising false alarms\n* **Data** : images acquired with ~ 50 different cameras and processed in different fashions\n\n## 1.2 Evaluation\n* submissions are evaluated on the *weighted AUC*\n* each region of the ROC curve is weighted according to these chosen parameters:\n    * `tpr_thresholds = [0.0, 0.4, 1.0]`\n    * `weights = [2, 1]`\n\n<img src='https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F1951250%2Ff250ff6a4e04bac332fa14d539ed813e%2FKaggle.png?generation=1588207999884987&alt=media' width=400>\n\n## 1.3 Sumbission file\nFor each Id (image) in the test set, you must provide a score that indicates *how likely this image contains hidden data*: the higher the score, the more it is assumed that image contains secret data.\n\n## 1.4 Data\n**Files**\n* `Cover\/` contains 75k unaltered images meant for use in training.\n* `JMiPOD\/` contains 75k examples of the JMiPOD algorithm applied to the cover images.\n* `JUNIWARD\/` contains 75k examples of the JUNIWARD algorithm applied to the cover images.\n* `UERD\/` contains 75k examples of the UERD algorithm applied to the cover images.\n* `Test\/` contains 5k test set images. These are the images for which you are predicting.\n* `sample_submission.csv` contains an example submission in the correct format.","fbd3a4ad":"# Simple Augs: Flips","fce92ca2":"# Dependencies","0e814b55":"# 4. Alaska2 Images - EDA\n\n## 4.1 Read in the data\nThere are 75k files in Cover, JMiPOD, JUNIWARD and UERD and 5k files in Test. We can't read the image arrays all at once, because the available RAM is not enough to perform this task.","ff766a87":"In any case original dataframe with splitting:","da1a1af7":"# 4. Data Preparation","1fa6f5a9":"#### Check if the result is OK","7b111b2f":"# Config","a56d8218":"> Method of computing New Predictions is from here: [Alaska2 CNN Multiclass Classifier](https:\/\/www.kaggle.com\/meaninglesslives\/alaska2-cnn-multiclass-classifier). Below you ca also find a helpful schema of the code below:\n<img src='https:\/\/i.imgur.com\/MEV7VQh.png' width = 600>","7460f577":"# Label Smoothing","436a910b":"### Images shape, size, data type\n* all images are 512 x 512 x 3\n* all images are of size 786,432 \n* all images are uint8 type","34af31ae":"# Alaska2 Baseline PyTorch\n\nHi everyone!\n\nMy name is Alex Shonenkov, I am DL\/NLP\/CV\/TS research engineer. Especially I am in Love with NLP & DL.\n\nI would like to share with you my starter pipeline for solving this competition :)","850972ae":"# 3. Example: Stegano Library\n\n**Images encoded data**: Usually images have 3 channels (RGB), composed by pixels, which *describe* the image (the colors).\n<img src='https:\/\/upload.wikimedia.org\/wikipedia\/en\/thumb\/9\/9c\/Steganography.png\/465px-Steganography.png' width=400>\n\n> [Stegano package documentation](https:\/\/buildmedia.readthedocs.org\/media\/pdf\/stegano\/latest\/stegano.pdf) : this package hides the *hidden information* in the *least significant bits* of the image. These bits have the property of being very random (white noise), so they don't *fire* when some change is applied to them. ","0918ab99":"# Training\n\nI have used 1xV100 for training model, in kaggle kernel it works also. You can make fork and check it, but I would like to share with you my logs","14636ec9":"# 3. Network \ud83d\udee0\n\n## 3.1 Efficient Net\n\n<div class=\"alert alert-block alert-info\">\n<img src='https:\/\/i.imgur.com\/H6AnLaj.png' width='90' align='left'><\/img>\n<p><a href='https:\/\/www.youtube.com\/watch?v=3svIm5UC94I'>EfficientNet Explained!<\/a><\/p>\n<p>Henry AI Labs<\/p>\n<\/div>\n\n**What is Efficient Net?**\n* Dedeloped by Google AI Research: rethinks the way we SCALE CNNs up\n* Scaling up can be done in many ways:\n    * Width: adding more feature maps\n    * Depth: adding more layers\n    * Resolution: increasing the resolution of the input image\n    \n<img src='https:\/\/i.imgur.com\/JN5H0ae.png' width=700>\n\n> There are many EfficientNets: from B0 to B7, all with different performances.\n<img src='https:\/\/i.imgur.com\/VMTiu5R.png' width=300>","1b1b982d":"# 5. Train and Weighted AUC Functions \ud83d\udcbe\n\n> Note: Function from  notebook [Weighted AUC Metric (Updated)](https:\/\/www.kaggle.com\/anokas\/weighted-auc-metric-updated)\n\n* Optimizer:\n    * The original `Adam` algorithm was proposed in `Adam: A Method for Stochastic Optimization`\n    * The `AdamW` variant was proposed in `Decoupled Weight Decay Regularization`\n    \n<div class=\"alert alert-block alert-success\">\n<p>Note to self: Try different Criterions and Optimizers.<\/p>\n<\/div>","5e61e0ba":"# 5. JPEG Images: Where is the information?\n\n* [Inspo and Understanding here](https:\/\/www.kaggle.com\/tanulsingh077\/steganalysis-complete-understanding-and-model)\n* [From Image to JPEG here](https:\/\/www.graphicsmill.com\/docs\/gm5\/UnderstandingofJPEGEncodingParameters.htm)\n\n<img src='https:\/\/i.imgur.com\/c54ht2c.png' width=600>\n\n\n### Hiding information\n1. In the YCbCr channels: this is an old approach and can be easily found (as we saw in the Suki example)\n2. DCT Coefficients of the YCbCr channels: the payload (secret information) is randomly distributed\n\n> Either YCbCr or DCT Coeff can be used as **inputs** to the neural net\n\n## 5.1 Visualize YCbCr Channels:","4f7c5c29":"### Show some Images","b6d5f384":"# Another Karnel for Model Improvement","dfc9eaa0":"## 1.2 Data\ud83d\udcc1\n\n* `.extend()`: Extend list by appending elements from the iterable.\n\n> The code below does the following:\n<img src ='https:\/\/i.imgur.com\/zLK7iRb.png' width=500>","a85ccb4b":"### Create DCT Function:","63084be7":"### Display ALL DCT blocks against the original image","6e72ba4b":"# References\n\n* [Definition of Steganalysis (Wiki)](https:\/\/en.wikipedia.org\/wiki\/Steganalysis)\n* [Definition of Steganography (Wiki)](https:\/\/en.wikipedia.org\/wiki\/Steganography)\n* [Steganalysis: Complete Understanding and models](https:\/\/www.kaggle.com\/tanulsingh077\/steganalysis-complete-understanding-and-model)\n* [Stegano Documentation](https:\/\/buildmedia.readthedocs.org\/media\/pdf\/stegano\/latest\/stegano.pdf)\n* [Understanding JPEG Encoding](https:\/\/www.graphicsmill.com\/docs\/gm5\/UnderstandingofJPEGEncodingParameters.htm)\n* [Discrete Cosine Transform](https:\/\/en.wikipedia.org\/wiki\/Discrete_cosine_transform)\n* [JPEG DCT Demo](https:\/\/inst.eecs.berkeley.edu\/~ee123\/sp16\/Sections\/JPEG_DCT_Demo.html#Display-all-DCT-blocks)\n* [ALASKA2 Steganalysis: EfficientNet-B7 PyTorch](https:\/\/www.kaggle.com\/tarunpaparaju\/alaska2-steganalysis-efficientnet-b3-pytorch)\n* [Pytorch.utils.data documentation](https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html)\n* [Efficient Net Explained](https:\/\/www.youtube.com\/watch?v=3svIm5UC94I)","c749478f":"# 6. Training Model","3f729d44":"### How is the Model working?\n\n> A schema of the example below:\n<img src='https:\/\/i.imgur.com\/FIHb9Ko.png' width=500>","7919b8d1":"<div class=\"alert alert-block alert-success\">\n<p>EffNet B0 (10 class): 30,000 sample_size | 19 epochs | 16 batch_size | <strong>LB Score is 0.822.<\/strong><\/p>\n<p>EffNet B2 (10 class): 70,000 sample_size | 15 epochs | 16 batch_size | <strong>LB Score is 0.805.<\/strong><\/p>\n<p>Working to make it better.<\/p>\n<\/div>\n\n# To Continue...\n\n<div class=\"alert alert-block alert-warning\"> \n<p>If you liked this, upvote!<\/p>\n<p>Cheers!<\/p>\n<\/div>","f599a280":"# Metrics","abd8f1fa":"### How is the Model working?\n<img src='https:\/\/i.imgur.com\/Xc0kARY.png' width=500>","d23edd45":"In checkpoint you can find states for optimizer and scheduler if you need","041146d3":"### Submission File\n\n> Note: The submission will actually deliver only if you train on more datapoints (this is just a very small sample so the notebook runs fast :) )","3681f3d6":"### Bonus: Threshold DCT Coefficients\n> Keeping only a percentage of the coefficients","7ac0a056":"# Fitter","3648c487":"# Dataset","f0cced61":"# 7. Inference \ud83d\udcc8\n\n## 7.1 Data","835bb34d":"# GroupKFold splitting\n\nI think group splitting by image_name is really important for correct validation in this competition ;) "}}