{"cell_type":{"9d6b875f":"code","9023d7dd":"code","f4cd9f9f":"code","7893cda3":"code","cbdab8f0":"code","fb433292":"code","9348cd3f":"code","7f014827":"code","347ce4c7":"code","ff3a3e73":"code","74191e32":"code","09fd52b7":"code","7eb2c768":"code","844989eb":"code","87d656a7":"code","69570e7a":"code","30ba940e":"code","1f4478c2":"code","79437b05":"code","e396c491":"code","8905a806":"code","a1f6081e":"code","4df09e19":"code","cbd4bf9d":"code","21e752bb":"code","6eeec313":"code","d9498f59":"code","a313aa28":"code","9b85ec64":"code","8b0d9fdf":"code","723ce8ed":"code","226e45d2":"code","66a06364":"code","cb91900f":"code","09affb31":"code","e664fa61":"code","3c306630":"code","47ec7eb0":"code","23015614":"code","4f776277":"code","e8e013d5":"code","a6a1b095":"code","93b8d85c":"code","f369bb29":"code","a7a4017a":"code","bba3b8cf":"markdown","dbda4b1f":"markdown","f40f559c":"markdown","1a1e30a1":"markdown","95feec9f":"markdown","3730a1cf":"markdown","4132dd35":"markdown"},"source":{"9d6b875f":"import sys\nimport pandas as pd; import numpy as np\nimport scipy\n\nimport IPython; from IPython import display\n\nimport sklearn\nimport random; import time\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom subprocess import check_output","9023d7dd":"#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.tools.plotting import scatter_matrix\n\n#%matplotlib inline = show plots in Jupyter Notebook browser\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8","f4cd9f9f":"data_raw = pd.read_csv('..\/input\/train.csv')\ndata_val  = pd.read_csv('..\/input\/test.csv')\ndata1 = data_raw.copy(deep = True)\n\n#however passing by reference is convenient, because we can clean both datasets at once\ndata_cleaner = [data1, data_val]","7893cda3":"print (data_raw.info())\ndata_raw.sample(10)","cbdab8f0":"#correlation on train data\nprint('Train'.center(30,'+'))\nprint(data1.corr())\nprint('Test'.center(30,'+'))\nprint(data_val.corr())","fb433292":"data1.hist()","9348cd3f":"corr_matrix = data1.corr()\ncorr_matrix['Survived'].sort_values(ascending=False)","7f014827":"from pandas.plotting import scatter_matrix\nscatter_matrix(data1)","347ce4c7":"attributes = ['Pclass','Age','SibSp','Parch','Fare']\nfor i in attributes:\n    data_raw.plot(kind='scatter',x=i,y='Survived',figsize=(4,3),alpha=0.1)","ff3a3e73":"#male female\nmale_df = data1[data1.Sex == 'male']\nfemale_df = data1[data1.Sex == 'female']\n\nmale_age_survived_ratio_list = []\nfemale_age_survived_ratio_list = []\nfor i in range(0, int(max(data1.Age))+1, 10):\n    male_df_of_age = male_df[(male_df.Age >= i) & (male_df.Age < i+9)]\n    female_df_of_age = female_df[(female_df.Age >= i) & (female_df.Age < i+9)]\n\n    male_s = len(male_df_of_age[male_df_of_age.Survived == 1])\n    female_s = len(female_df_of_age[female_df_of_age.Survived == 1])\n\n    male_total = len(male_df_of_age)\n    female_total = len(female_df_of_age)\n\n    if male_total  == 0:\n        male_age_survived_ratio_list.append(0.5)\n    else:\n        male_age_survived_ratio_list.append(male_s\/male_total)\n\n    if female_total == 0:\n        female_age_survived_ratio_list.append(0.5)\n    else:\n        female_age_survived_ratio_list.append(female_s\/female_total)\n\nprint(male_age_survived_ratio_list, female_age_survived_ratio_list)\n\nx_labels = []\nfor i in range(0, int(max(data1.Age))+1, 10):\n    x_labels.append(str(i) + '-' + str(i+9))\n\nplt.figure(figsize=(16,8))\nx1 = [i for i in range(0, int(max(data1.Age))+ 1, 10)]\nx2 = [i + 2 for i in range(0, int(max(data1.Age))+ 1, 10)]\nplt.bar(x1, male_age_survived_ratio_list, tick_label=x_labels, width=3, color='blue')\nplt.bar(x2,female_age_survived_ratio_list, tick_label=x_labels, width=3, color='red')\nplt.tick_params(labelsize = 15)","74191e32":"data1['Family_size'] = data1['SibSp'] + data1['Parch']\ndata1.plot(kind='scatter',x='Family_size',y='Survived',alpha=0.1)\ndata1.sample(10)","09fd52b7":"print('Train'.center(30,'+'))\nprint('Train columns with null values:\\n', data1.isnull().sum())\n\nprint('Test'.center(30,'+'))\nprint('Test\/Validation columns with null values:\\n', data_val.isnull().sum())\n\ndata1.describe(include = 'all')","7eb2c768":"###COMPLETING: complete or delete missing values in train and test\/validation dataset\nfor dataset in data_cleaner:    \n    #complete missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #complete embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #complete missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n#delete the cabin feature\/column and others previously stated to exclude in train dataset\nprint(data1.isnull().sum())\nprint()\nprint(data_val.isnull().sum())\n\ndrop_column = ['PassengerId','Cabin', 'Ticket']\ndata1.drop(drop_column, axis=1, inplace = True)\n\nprint(data1.isnull().sum())\nprint()\nprint(data_val.isnull().sum())","844989eb":"###CREATE: Feature Engineering for train and test\/validation dataset\nfor dataset in data_cleaner:    \n    #Discrete variables\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n    dataset['IsAlone'] = 1 #initialize to yes\/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no\/0 if family size is greater than 1\n\n    #quick and dirty code split title from name: http:\/\/www.pythonforbeginners.com\/dictionary\/python-split\n    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n\n\n    #Continuous variable bins; qcut vs cut: https:\/\/stackoverflow.com\/questions\/30211923\/what-is-the-difference-between-pandas-qcut-and-pandas-cut\n    #Fare Bins\/Buckets using qcut or frequency bins: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.qcut.html\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n\n    #Age Bins\/Buckets using cut or value bins: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.cut.html\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n\n\n    \n#cleanup rare title names\n#print(data1['Title'].value_counts())\nstat_min = 10 #while small is arbitrary, we'll use the common minimum in statistics: http:\/\/nicholasjjackson.com\/2012\/03\/08\/sample-size-is-10-a-magic-number\/\ntitle_names = (data1['Title'].value_counts() < stat_min) #this will create a true false series with title name as index\n\n#apply and lambda functions are quick and dirty code to find and replace with fewer lines of code: https:\/\/community.modeanalytics.com\/python\/tutorial\/pandas-groupby-and-python-lambda-functions\/\ndata1['Title'] = data1['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\nprint(data1['Title'].value_counts())\nprint(\"-\"*10)\n\ndel data1['Family_size']\n#preview data again\ndata1.info()\ndata_val.info()\ndata1.sample(10)","87d656a7":"#CONVERT: convert objects to category using Label Encoder for train and test\/validation dataset\n\n#code categorical data\nlabel = LabelEncoder()\nfor dataset in data_cleaner:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n\n\n#define y variable aka target\/outcome\nTarget = ['Survived']\n\n#define x variables for original features aka feature selection\ndata1_x = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] #pretty name\/values for charts\ndata1_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare'] #coded for algorithm calculation\ndata1_xy =  Target + data1_x\nprint('Original X Y: ', data1_xy, '\\n')\n\n\n#define x variables for original w\/bin features to remove continuous variables\ndata1_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\ndata1_xy_bin = Target + data1_x_bin\nprint('Bin X Y: ', data1_xy_bin, '\\n')\n\n\n#define x and y variables for dummy features original\ndata1_dummy = pd.get_dummies(data1[data1_x])\ndata1_x_dummy = data1_dummy.columns.tolist()\ndata1_xy_dummy = Target + data1_x_dummy\nprint('Dummy X Y: ', data1_xy_dummy, '\\n')\n\n\n#print(data1_x_bin.head())\ndata1_dummy.head()","69570e7a":"print('Train columns with null values: \\n', data1.isnull().sum())\nprint(\"-\"*10)\nprint (data1.info())\nprint(\"-\"*10)\n\nprint('Test\/Validation columns with null values: \\n', data_val.isnull().sum())\nprint(\"-\"*10)\nprint (data_val.info())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')","30ba940e":"data1.sample(10)","1f4478c2":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nstdsc = StandardScaler()\nmms = MinMaxScaler()\ndata1_std_ms = pd.DataFrame(stdsc.fit_transform(mms.fit_transform(data1[data1_x_bin+data1_x_calc+Target])))\ndata1_dummy_std_ms = pd.DataFrame(stdsc.fit_transform(mms.fit_transform(data1_dummy[data1_x_dummy])))\ndata1_std_ms.columns = [data1_x_bin+data1_x_calc+Target]\ndata1_dummy_std_ms.columns = [data1_x_dummy]","79437b05":"train1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1_std_ms[data1_x_calc], data1_std_ms[Target], random_state = 0)\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1_std_ms[data1_x_bin], data1_std_ms[Target] , random_state = 0)\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data1_dummy_std_ms[data1_x_dummy], data1_std_ms[Target], random_state = 0)\n\nprint(\"Data1 Shape: {}\".format(data1.shape))\nprint(\"Train1 Shape: {}\".format(train1_x.shape))\nprint(\"Test1 Shape: {}\".format(test1_x.shape))\n\ntrain1_x_bin.head()","e396c491":"#Discrete Variable Correlation by Survival using\n#group by aka pivot table: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.groupby.html\n\"\"\"\ngroupby\u95a2\u6570\u304c\u3084\u3063\u3066\u3044\u308b\u3053\u3068\u306f\u305f\u3060\u306e\u30b0\u30eb\u30fc\u30d7\u5206\u3051\u3067\u3001\u305d\u306e\u5f8c\u306e\u51e6\u7406\u306f\u6211\u3005\u306e\u65b9\u3067\u81ea\u7531\u306b\u8a2d\u5b9a\u53ef\u80fd\u3002\n\"\"\"\n\nfor x in data1_x:\n    if data1[x].dtype != 'float64' :\n        print('Survival Correlation by:', x)\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n        print('-'*10, '\\n')\n        \n\nprint(pd.crosstab(data1['Title'],data1[Target[0]]))\n","8905a806":"#graph distribution of quantitative data\nplt.figure(figsize=[16,12])\n\n\"\"\"\no is treated as a Outlier.\nminimun\n25\u30d1\u30fc\u30bb\u30f3\u30bf\u30a4\u30eb\t\u7b2c\u4e00\u56db\u5206\u4f4d\u6570\n50\u30d1\u30fc\u30bb\u30f3\u30bf\u30a4\u30eb\t\u7b2c\u4e8c\u56db\u5206\u4f4d\u6570\uff08\u4e2d\u592e\u5024\uff09\n75\u30d1\u30fc\u30bb\u30f3\u30bf\u30a4\u30eb\t\u7b2c\u4e09\u56db\u5206\u4f4d\u6570\nmaximum\n\"\"\"\nplt.subplot(231)\nplt.boxplot(x=data1['Fare'], showmeans = True, meanline = True)\nplt.title('Fare Boxplot')\nplt.ylabel('Fare ($)')\n\nplt.subplot(232)\nplt.boxplot(data1['Age'], showmeans = True, meanline = True)\nplt.title('Age Boxplot')\nplt.ylabel('Age (Years)')\n\nplt.subplot(233)\nplt.boxplot(data1['FamilySize'], showmeans = True, meanline = True)\nplt.title('Family Size Boxplot')\nplt.ylabel('Family Size (#)')\n\nplt.subplot(234)\nplt.hist(x = [data1[data1['Survived']==1]['Fare'], data1[data1['Survived']==0]['Fare']], \n         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('Fare Histogram by Survival')\nplt.xlabel('Fare ($)')\nplt.ylabel('# of Passengers')\nplt.legend()\n\nplt.subplot(235)\nplt.hist(x = [data1[data1['Survived']==1]['Age'], data1[data1['Survived']==0]['Age']], \n         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('Age Histogram by Survival')\nplt.xlabel('Age (Years)')\nplt.ylabel('# of Passengers')\nplt.legend()\n\nplt.subplot(236)\nplt.hist(x = [data1[data1['Survived']==1]['FamilySize'], data1[data1['Survived']==0]['FamilySize']], \n         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('Family Size Histogram by Survival')\nplt.xlabel('Family Size (#)')\nplt.ylabel('# of Passengers')\nplt.legend()\n","a1f6081e":"#we will use seaborn graphics for multi-variable comparison: https:\/\/seaborn.pydata.org\/api.html\n\n#graph individual features by survival\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\n\nsns.barplot(x = 'Embarked', y = 'Survived', data=data1, ax = saxis[0,0])\nsns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=data1, ax = saxis[0,1])\nsns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data1, ax = saxis[0,2])\n\nsns.pointplot(x = 'FareBin', y = 'Survived',  data=data1, ax = saxis[1,0])\nsns.pointplot(x = 'AgeBin', y = 'Survived',  data=data1, ax = saxis[1,1])\nsns.pointplot(x = 'FamilySize', y = 'Survived', data=data1, ax = saxis[1,2])","4df09e19":"#graph distribution of qualitative data: Pclass\n#we know class mattered in survival, now let's compare class and a 2nd feature\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\n\nsns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data1, ax = axis1)\naxis1.set_title('Pclass vs Fare Survival Comparison')\n\nsns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data1, split = True, ax = axis2)\naxis2.set_title('Pclass vs Age Survival Comparison')\n\nsns.boxplot(x = 'Pclass', y ='FamilySize', hue = 'Survived', data = data1, ax = axis3)\naxis3.set_title('Pclass vs Family Size Survival Comparison')","cbd4bf9d":"#graph distribution of qualitative data: Sex\n#we know sex mattered in survival, now let's compare sex and a 2nd feature\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=data1, ax = qaxis[0])\naxis1.set_title('Sex vs Embarked Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=data1, ax  = qaxis[1])\naxis1.set_title('Sex vs Pclass Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone', data=data1, ax  = qaxis[2])\naxis1.set_title('Sex vs IsAlone Survival Comparison')","21e752bb":"#more side-by-side comparisons\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\n\n#how does family size factor with sex & survival compare\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\n\n#how does class factor with sex & survival compare\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis2)","6eeec313":"#how does embark port factor with class, sex, and survival compare\n#facetgrid: https:\/\/seaborn.pydata.org\/generated\/seaborn.FacetGrid.html\ne = sns.FacetGrid(data1, col = 'Embarked')\ne.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')\ne.add_legend()","d9498f59":"#plot distributions of age of passengers who survived or did not survive\na = sns.FacetGrid( data1, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , data1['Age'].max()))\na.add_legend()","a313aa28":"#histogram comparison of sex, class, and age by survival\nh = sns.FacetGrid(data1, row = 'Sex', col = 'Pclass', hue = 'Survived')\nh.map(plt.hist, 'Age', alpha = .75)\nh.add_legend()","9b85ec64":"#pair plots of entire dataset\npp = sns.pairplot(data1, hue = 'Survived', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\npp.set(xticklabels=[])","8b0d9fdf":"#correlation heatmap of dataset\n\"\"\"VERY GOOD!!!!!!\"\"\"\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(data1)","723ce8ed":"#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n    XGBClassifier()    \n    ]\n\n\n\n#split dataset in cross-validation with this splitter class: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n#note: this is an alternative to train_test_split\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60\/30 split intentionally leaving out 10%\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n#create table to compare MLA predictions\nMLA_predict = data1[Target]\n\n#index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__ #WHAT DOSE IT WORK?? >> Understood\n    print(MLA_name)\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    #score model with cross validation: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n    cv_results = model_selection.cross_validate(alg, data1[data1_x_bin], data1[Target], cv  = cv_split)\n\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    #if this is a non-bias random sample, then +\/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen! VERY GOOD!!!!!\n    \n\n    #save MLA predictions - see section 6 for usage\n    alg.fit(data1[data1_x_bin], data1[Target])\n    MLA_predict[MLA_name] = alg.predict(data1[data1_x_bin])\n    \n    row_index+=1\n\n    \n#print and sort table: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.sort_values.html\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare\n#MLA_predict\n\n","226e45d2":"#barplot using https:\/\/seaborn.pydata.org\/generated\/seaborn.barplot.html\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#prettify using pyplot: https:\/\/matplotlib.org\/api\/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')","66a06364":"for index, row in data1.iterrows(): \n    #random number generator: https:\/\/docs.python.org\/2\/library\/random.html\n    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \n        data1.set_value(index, 'Random_Predict', 1) #predict survived\/1\n    else: \n        data1.set_value(index, 'Random_Predict', 0) #predict died\/0\n    \n\n#score random guess of survival. Use shortcut 1 = Right Guess and 0 = Wrong Guess\n#the mean of the column will then equal the accuracy\ndata1['Random_Score'] = 0 #assume prediction wrong\ndata1.loc[(data1['Survived'] == data1['Random_Predict']), 'Random_Score'] = 1 #set to 1 for correct prediction\nprint('Coin Flip Model Accuracy: {:.2f}%'.format(data1['Random_Score'].mean()*100))\n\n#we can also use scikit's accuracy_score function to save us a few lines of code\n#http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\nprint('Coin Flip Model Accuracy w\/SciKit: {:.2f}%'.format(metrics.accuracy_score(data1['Survived'], data1['Random_Predict'])*100))\n","cb91900f":"pivot_female = data1[data1.Sex=='female'].groupby(['Sex','Pclass', 'Embarked','FareBin'])['Survived'].mean()\nprint('Survival Decision Tree w\/Female Node: \\n',pivot_female)\n\npivot_male = data1[data1.Sex=='male'].groupby(['Sex','Title'])['Survived'].mean()\nprint('\\n\\nSurvival Decision Tree w\/Male Node: \\n',pivot_male)","09affb31":"\"\"\"VERY GOOOOOOOD!!!!!!!!!\"\"\"\n#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\ndef mytree(df):\n    \n    #initialize table to store predictions\n    Model = pd.DataFrame(data = {'Predict':[]})\n    male_title = ['Master'] #survived titles\n\n    for index, row in df.iterrows():\n\n        #Question 1: Were you on the Titanic; majority died\n        Model.loc[index, 'Predict'] = 0\n\n        #Question 2: Are you female; majority survived\n        if (df.loc[index, 'Sex'] == 'female'):\n                  Model.loc[index, 'Predict'] = 1\n\n        #Question 3A Female - Class and Question 4 Embarked gain minimum information\n\n        #Question 5B Female - FareBin; set anything less than .5 in female node decision tree back to 0       \n        if ((df.loc[index, 'Sex'] == 'female') & \n            (df.loc[index, 'Pclass'] == 3) & \n            (df.loc[index, 'Embarked'] == 'S')  &\n            (df.loc[index, 'Fare'] > 8)\n\n           ):\n                  Model.loc[index, 'Predict'] = 0\n\n        #Question 3B Male: Title; set anything greater than .5 to 1 for majority survived\n        if ((df.loc[index, 'Sex'] == 'male') &\n            (df.loc[index, 'Title'] in male_title)\n            ):\n            Model.loc[index, 'Predict'] = 1\n        if df.loc[index, 'FamilySize'] > 4:\n            Model.loc[index, 'Predict'] = 0\n        \n        \n    return Model\n\n\n#model data\nTree_Predict = mytree(data1)\nprint('Decision Tree Model Accuracy\/Precision Score: {:.2f}%\\n'.format(metrics.accuracy_score(data1['Survived'], Tree_Predict)*100))\n\n#Where recall score = (true positives)\/(true positive + false negative) w\/1 being best:http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score\n#And F1 score = weighted average of precision and recall w\/1 being best: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\nprint(metrics.classification_report(data1['Survived'], Tree_Predict))\n\n\"\"\"\nPrevious(Online)\nDecision Tree Model Accuracy\/Precision Score: 82.04%\n\n             precision    recall  f1-score   support\n\n          0       0.82      0.91      0.86       549\n          1       0.82      0.68      0.75       342\n\navg \/ total       0.82      0.82      0.82       891\n\"\"\"\n","e664fa61":"#Plot Accuracy Summary\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Compute confusion matrix\ncnf_matrix = metrics.confusion_matrix(data1['Survived'], Tree_Predict)\nnp.set_printoptions(precision=2)\n\nclass_names = ['Dead', 'Survived']\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names,\n                      title='Confusion matrix, without normalization')\n\n# Plot normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, \n                      title='Normalized confusion matrix')\n\n","3c306630":"#base model\ndtree = tree.DecisionTreeClassifier(random_state = 0)\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\ndtree.fit(data1[data1_x_bin], data1[Target])\n\nprint('BEFORE DT Parameters: ', dtree.get_params())\nprint(\"BEFORE DT Training w\/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT Test w\/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT Test w\/bin score 3*std: +\/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n#print(\"BEFORE DT Test w\/bin set score min: {:.2f}\". format(base_results['test_score'].min()*100))\nprint('-'*10)\n\n\n#tune hyper-parameters: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\nparam_grid = {'criterion': ['gini', 'entropy'],  #scoring methodology; two supported formulas for calculating information gain - default is gini\n              #'splitter': ['best', 'random'], #splitting methodology; two supported strategies - default is best\n              'max_depth': [2,4,6,8,10,None], #max depth tree can grow; default is none\n              #'min_samples_split': [2,5,10,.03,.05], #minimum subset size BEFORE new split (fraction is % of total); default is 2\n              #'min_samples_leaf': [1,5,10,.03,.05], #minimum subset size AFTER new split split (fraction is % of total); default is 1\n              #'max_features': [None, 'auto'], #max features to consider when performing split; default none or all\n              'random_state': [0] #seed or control random number generator: https:\/\/www.quora.com\/What-is-seed-in-random-number-generation\n             }\n\n#print(list(model_selection.ParameterGrid(param_grid)))\n\n#choose best model with grid_search: #http:\/\/scikit-learn.org\/stable\/modules\/grid_search.html#grid-search\n#http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_grid_search_digits.html\ntune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\ntune_model.fit(data1[data1_x_bin], data1[Target])\n\n#print(tune_model.cv_results_.keys())\n#print(tune_model.cv_results_['params'])\nprint('AFTER DT Parameters: ', tune_model.best_params_)\n#print(tune_model.cv_results_['mean_train_score'])\nprint(\"AFTER DT Training w\/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n#print(tune_model.cv_results_['mean_test_score'])\nprint(\"AFTER DT Test w\/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\nprint(\"AFTER DT Test w\/bin score 3*std: +\/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\nprint('-'*10)\n\n\n#duplicates gridsearchcv\n#tune_results = model_selection.cross_validate(tune_model, data1[data1_x_bin], data1[Target], cv  = cv_split)\n\n#print('AFTER DT Parameters: ', tune_model.best_params_)\n#print(\"AFTER DT Training w\/bin set score mean: {:.2f}\". format(tune_results['train_score'].mean()*100)) \n#print(\"AFTER DT Test w\/bin set score mean: {:.2f}\". format(tune_results['test_score'].mean()*100))\n#print(\"AFTER DT Test w\/bin set score min: {:.2f}\". format(tune_results['test_score'].min()*100))\n#print('-'*10)\n","47ec7eb0":"#base model\nprint('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape) \nprint('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)\n\nprint(\"BEFORE DT RFE Training w\/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT RFE Test w\/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT RFE Test w\/bin score 3*std: +\/- {:.2f}\". format(base_results['test_score'].std()*100*3))\nprint('-'*10)\n\n\n\n#feature selection\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_split)\ndtree_rfe.fit(data1[data1_x_bin], data1[Target])\n\n#transform x&y to reduced features and fit new model\n#alternative: can use pipeline to reduce fit and transform steps: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html\nX_rfe = data1[data1_x_bin].columns.values[dtree_rfe.get_support()]\nrfe_results = model_selection.cross_validate(dtree, data1[X_rfe], data1[Target], cv  = cv_split)\n\n#print(dtree_rfe.grid_scores_)\nprint('AFTER DT RFE Training Shape New: ', data1[X_rfe].shape) \nprint('AFTER DT RFE Training Columns New: ', X_rfe)\n\nprint(\"AFTER DT RFE Training w\/bin score mean: {:.2f}\". format(rfe_results['train_score'].mean()*100)) \nprint(\"AFTER DT RFE Test w\/bin score mean: {:.2f}\". format(rfe_results['test_score'].mean()*100))\nprint(\"AFTER DT RFE Test w\/bin score 3*std: +\/- {:.2f}\". format(rfe_results['test_score'].std()*100*3))\nprint('-'*10)\n\n\n#tune rfe model\nrfe_tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\nrfe_tune_model.fit(data1[X_rfe], data1[Target])\n\n#print(rfe_tune_model.cv_results_.keys())\n#print(rfe_tune_model.cv_results_['params'])\nprint('AFTER DT RFE Tuned Parameters: ', rfe_tune_model.best_params_)\n#print(rfe_tune_model.cv_results_['mean_train_score'])\nprint(\"AFTER DT RFE Tuned Training w\/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n#print(rfe_tune_model.cv_results_['mean_test_score'])\nprint(\"AFTER DT RFE Tuned Test w\/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\nprint(\"AFTER DT RFE Tuned Test w\/bin score 3*std: +\/- {:.2f}\". format(rfe_tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\nprint('-'*10)","23015614":"#Graph MLA version of Decision Tree: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.export_graphviz.html\n\"\"\"VERY GOOD\"\"\"\nimport graphviz \ndot_data = tree.export_graphviz(dtree, out_file=None, \n                                feature_names = data1_x_bin, class_names = True,\n                                filled = True, rounded = True)\ngraph = graphviz.Source(dot_data) \ngraph","4f776277":"#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\n#there are some 1's, but enough blues and light reds to create a \"super algorithm\" by combining them\ncorrelation_heatmap(MLA_predict)","e8e013d5":"#why choose one model, when you can pick them all with voting classifier\n#http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.VotingClassifier.html\n#removed models w\/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\n\"\"\"VERY GOOD!!!!!!!!!!!!\"\"\"\nvote_est = [\n    #Ensemble Methods: http:\/\/scikit-learn.org\/stable\/modules\/ensemble.html\n    ('ada', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etc',ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassifier()),\n\n    #Gaussian Processes: http:\/\/scikit-learn.org\/stable\/modules\/gaussian_process.html#gaussian-process-classification-gpc\n    ('gpc', gaussian_process.GaussianProcessClassifier()),\n    \n    #GLM: http:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#logistic-regression\n    ('lr', linear_model.LogisticRegressionCV()),\n    \n    #Navies Bayes: http:\/\/scikit-learn.org\/stable\/modules\/naive_bayes.html\n    ('bnb', naive_bayes.BernoulliNB()),\n    ('gnb', naive_bayes.GaussianNB()),\n    \n    #Nearest Neighbor: http:\/\/scikit-learn.org\/stable\/modules\/neighbors.html\n    ('knn', neighbors.KNeighborsClassifier()),\n    \n    #SVM: http:\/\/scikit-learn.org\/stable\/modules\/svm.html\n    ('svc', svm.SVC(probability=True)),\n    \n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n   ('xgb', XGBClassifier())\n\n]\n\n\n#Hard Vote or majority rules\nvote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\nvote_hard_cv = model_selection.cross_validate(vote_hard, data1[data1_x_bin], data1[Target], cv  = cv_split)\nvote_hard.fit(data1[data1_x_bin], data1[Target])\n\nprint(\"Hard Voting Training w\/bin score mean: {:.2f}\". format(vote_hard_cv['train_score'].mean()*100)) \nprint(\"Hard Voting Test w\/bin score mean: {:.2f}\". format(vote_hard_cv['test_score'].mean()*100))\nprint(\"Hard Voting Test w\/bin score 3*std: +\/- {:.2f}\". format(vote_hard_cv['test_score'].std()*100*3))\nprint('-'*10)\n\n\n#Soft Vote or weighted probabilities\nvote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\nvote_soft_cv = model_selection.cross_validate(vote_soft, data1[data1_x_bin], data1[Target], cv  = cv_split)\nvote_soft.fit(data1[data1_x_bin], data1[Target])\n\nprint(\"Soft Voting Training w\/bin score mean: {:.2f}\". format(vote_soft_cv['train_score'].mean()*100)) \nprint(\"Soft Voting Test w\/bin score mean: {:.2f}\". format(vote_soft_cv['test_score'].mean()*100))\nprint(\"Soft Voting Test w\/bin score 3*std: +\/- {:.2f}\". format(vote_soft_cv['test_score'].std()*100*3))\nprint('-'*10)\n","a6a1b095":"#IMPORTANT: THIS SECTION IS UNDER CONSTRUCTION!!!! 12.24.17\n#UPDATE: This section was scrapped for the next section; as it's more computational friendly.\n\n#WARNING: Running is very computational intensive and time expensive\n#code is written for experimental\/developmental purposes and not production ready\n\n\n#tune each estimator before creating a super model\n#http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html\ngrid_n_estimator = [50,100,300]\ngrid_ratio = [.1,.25,.5,.75,1.0]\ngrid_learn = [.01,.03,.05,.1,.25]\ngrid_max_depth = [2,4,6,None]\ngrid_min_samples = [5,10,.03,.05,.10]\ngrid_criterion = ['gini', 'entropy']\ngrid_bool = [True, False]\ngrid_seed = [0]\n\nvote_param = [{\n#            #http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html\n            'ada__n_estimators': grid_n_estimator,\n            'ada__learning_rate': grid_ratio,\n            'ada__algorithm': ['SAMME', 'SAMME.R'],\n            'ada__random_state': grid_seed,\n    \n            #http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n            'bc__n_estimators': grid_n_estimator,\n            'bc__max_samples': grid_ratio,\n            'bc__oob_score': grid_bool, \n            'bc__random_state': grid_seed,\n            \n            #http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n            'etc__n_estimators': grid_n_estimator,\n            'etc__criterion': grid_criterion,\n            'etc__max_depth': grid_max_depth,\n            'etc__random_state': grid_seed,\n\n\n            #http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n            'gbc__loss': ['deviance', 'exponential'],\n            'gbc__learning_rate': grid_ratio,\n            'gbc__n_estimators': grid_n_estimator,\n            'gbc__criterion': ['friedman_mse', 'mse', 'mae'],\n            'gbc__max_depth': grid_max_depth,\n            'gbc__min_samples_split': grid_min_samples,\n            'gbc__min_samples_leaf': grid_min_samples,      \n            'gbc__random_state': grid_seed,\n    \n            #http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n            'rfc__n_estimators': grid_n_estimator,\n            'rfc__criterion': grid_criterion,\n            'rfc__max_depth': grid_max_depth,\n            'rfc__min_samples_split': grid_min_samples,\n            'rfc__min_samples_leaf': grid_min_samples,   \n            'rfc__bootstrap': grid_bool,\n            'rfc__oob_score': grid_bool, \n            'rfc__random_state': grid_seed,\n        \n            #http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n            'lr__fit_intercept': grid_bool,\n            'lr__penalty': ['l1','l2'],\n            'lr__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n            'lr__random_state': grid_seed,\n            \n            #http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n            'bnb__alpha': grid_ratio,\n            'bnb__prior': grid_bool,\n            'bnb__random_state': grid_seed,\n    \n            #http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n            'knn__n_neighbors': [1,2,3,4,5,6,7],\n            'knn__weights': ['uniform', 'distance'],\n            'knn__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n            'knn__random_state': grid_seed,\n            \n            #http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html#sklearn.svm.SVC\n            #http:\/\/blog.hackerearth.com\/simple-tutorial-svm-parameter-tuning-python-r\n            'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n            'svc__C': grid_max_depth,\n            'svc__gamma': grid_ratio,\n            'svc__decision_function_shape': ['ovo', 'ovr'],\n            'svc__probability': [True],\n            'svc__random_state': grid_seed,\n    \n    \n            #http:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n            'xgb__learning_rate': grid_ratio,\n            'xgb__max_depth': [2,4,6,8,10],\n            'xgb__tree_method': ['exact', 'approx', 'hist'],\n            'xgb__objective': ['reg:linear', 'reg:logistic', 'binary:logistic'],\n            'xgb__seed': grid_seed    \n\n        }]\n\n\n\n\n#Soft Vote with tuned models\n#grid_soft = model_selection.GridSearchCV(estimator = vote_soft, param_grid = vote_param, cv = 2, scoring = 'roc_auc')\n#grid_soft.fit(data1[data1_x_bin], data1[Target])\n\n#print(grid_soft.cv_results_.keys())\n#print(grid_soft.cv_results_['params'])\n#print('Soft Vote Tuned Parameters: ', grid_soft.best_params_)\n#print(grid_soft.cv_results_['mean_train_score'])\n#print(\"Soft Vote Tuned Training w\/bin set score mean: {:.2f}\". format(grid_soft.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n#print(grid_soft.cv_results_['mean_test_score'])\n#print(\"Soft Vote Tuned Test w\/bin set score mean: {:.2f}\". format(grid_soft.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n#print(\"Soft Vote Tuned Test w\/bin score 3*std: +\/- {:.2f}\". format(grid_soft.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n#print('-'*10)\n\n\n#credit: https:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/EnsembleVoteClassifier\/\n#cv_keys = ('mean_test_score', 'std_test_score', 'params')\n#for r, _ in enumerate(grid_soft.cv_results_['mean_test_score']):\n#    print(\"%0.3f +\/- %0.2f %r\"\n#          % (grid_soft.cv_results_[cv_keys[0]][r],\n#             grid_soft.cv_results_[cv_keys[1]][r] \/ 2.0,\n#             grid_soft.cv_results_[cv_keys[2]][r]))\n\n\n#print('-'*10)\n","93b8d85c":"#WARNING: Running is very computational intensive and time expensive.\n#Code is written for experimental\/developmental purposes and not production ready!\n\n\n#Hyperparameter Tune with GridSearchCV: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html\ngrid_n_estimator = [10, 50, 100, 300]\ngrid_ratio = [.1, .25, .5, .75, 1.0]\ngrid_learn = [.01, .03, .05, .1, .25]\ngrid_max_depth = [2, 4, 6, 8, 10, None]\ngrid_min_samples = [5, 10, .03, .05, .10]\ngrid_criterion = ['gini', 'entropy']\ngrid_bool = [True, False]\ngrid_seed = [0]\n\n\ngrid_param = [\n            [{\n            #AdaBoostClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html\n            'n_estimators': grid_n_estimator, #default=50\n            'learning_rate': grid_learn, #default=1\n            #'algorithm': ['SAMME', 'SAMME.R'], #default=\u2019SAMME.R\n            'random_state': grid_seed\n            }],\n       \n    \n            [{\n            #BaggingClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'max_samples': grid_ratio, #default=1.0\n            'random_state': grid_seed\n             }],\n\n    \n            [{\n            #ExtraTreesClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'criterion': grid_criterion, #default=\u201dgini\u201d\n            'max_depth': grid_max_depth, #default=None\n            'random_state': grid_seed\n             }],\n\n\n            [{\n            #GradientBoostingClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n            #'loss': ['deviance', 'exponential'], #default=\u2019deviance\u2019\n            'learning_rate': [.05], #default=0.1 -- 12\/31\/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n            'n_estimators': [300], #default=100 -- 12\/31\/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n            #'criterion': ['friedman_mse', 'mse', 'mae'], #default=\u201dfriedman_mse\u201d\n            'max_depth': grid_max_depth, #default=3   \n            'random_state': grid_seed\n             }],\n\n    \n            [{\n            #RandomForestClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'criterion': grid_criterion, #default=\u201dgini\u201d\n            'max_depth': grid_max_depth, #default=None\n            'oob_score': [True], #default=False -- 12\/31\/17 set to reduce runtime -- The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 146.35 seconds.\n            'random_state': grid_seed\n             }],\n    \n            [{    \n            #GaussianProcessClassifier\n            'max_iter_predict': grid_n_estimator, #default: 100\n            'random_state': grid_seed\n            }],\n        \n    \n            [{\n            #LogisticRegressionCV - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n            'fit_intercept': grid_bool, #default: True\n            #'penalty': ['l1','l2'],\n            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], #default: lbfgs\n            'random_state': grid_seed\n             }],\n            \n    \n            [{\n            #BernoulliNB - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n            'alpha': grid_ratio, #default: 1.0\n             }],\n    \n    \n            #GaussianNB - \n            [{}],\n    \n            [{\n            #KNeighborsClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n            'n_neighbors': [1,2,3,4,5,6,7], #default: 5\n            'weights': ['uniform', 'distance'], #default = \u2018uniform\u2019\n            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n            }],\n            \n    \n            [{\n            #SVC - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html#sklearn.svm.SVC\n            #http:\/\/blog.hackerearth.com\/simple-tutorial-svm-parameter-tuning-python-r\n            #'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n            'C': [1,2,3,4,5], #default=1.0\n            'gamma': grid_ratio, #edfault: auto\n            'decision_function_shape': ['ovo', 'ovr'], #default:ovr\n            'probability': [True],\n            'random_state': grid_seed\n             }],\n\n    \n            [{\n            #XGBClassifier - http:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n            'learning_rate': grid_learn, #default: .3\n            'max_depth': [1,2,4,6,8,10], #default 2\n            'n_estimators': grid_n_estimator, \n            'seed': grid_seed  \n             }]   \n        ]\n\n\n\nstart_total = time.perf_counter() #https:\/\/docs.python.org\/3\/library\/time.html#time.perf_counter\nfor clf, param in zip (vote_est, grid_param): #https:\/\/docs.python.org\/3\/library\/functions.html#zip\n\n    #print(clf[1]) #vote_est is a list of tuples, index 0 is the name and index 1 is the algorithm\n    #print(param)\n    \n    \n    start = time.perf_counter()        \n    best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc')\n    best_search.fit(data1[data1_x_bin], data1[Target])\n    run = time.perf_counter() - start\n\n    best_param = best_search.best_params_\n    print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run))\n    clf[1].set_params(**best_param) \n\n\nrun_total = time.perf_counter() - start_total\nprint('Total optimization time was {:.2f} minutes.'.format(run_total\/60))\n\nprint('-'*10)","f369bb29":"#Hard Vote or majority rules w\/Tuned Hyperparameters\ngrid_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\ngrid_hard_cv = model_selection.cross_validate(grid_hard, data1[data1_x_bin], data1[Target], cv  = cv_split)\ngrid_hard.fit(data1[data1_x_bin], data1[Target])\n\nprint(\"Hard Voting w\/Tuned Hyperparameters Training w\/bin score mean: {:.2f}\". format(grid_hard_cv['train_score'].mean()*100)) \nprint(\"Hard Voting w\/Tuned Hyperparameters Test w\/bin score mean: {:.2f}\". format(grid_hard_cv['test_score'].mean()*100))\nprint(\"Hard Voting w\/Tuned Hyperparameters Test w\/bin score 3*std: +\/- {:.2f}\". format(grid_hard_cv['test_score'].std()*100*3))\nprint('-'*10)\n\n#Soft Vote or weighted probabilities w\/Tuned Hyperparameters\ngrid_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\ngrid_soft_cv = model_selection.cross_validate(grid_soft, data1[data1_x_bin], data1[Target], cv  = cv_split)\ngrid_soft.fit(data1[data1_x_bin], data1[Target])\n\nprint(\"Soft Voting w\/Tuned Hyperparameters Training w\/bin score mean: {:.2f}\". format(grid_soft_cv['train_score'].mean()*100)) \nprint(\"Soft Voting w\/Tuned Hyperparameters Test w\/bin score mean: {:.2f}\". format(grid_soft_cv['test_score'].mean()*100))\nprint(\"Soft Voting w\/Tuned Hyperparameters Test w\/bin score 3*std: +\/- {:.2f}\". format(grid_soft_cv['test_score'].std()*100*3))\nprint('-'*10)\n\n\n#12\/31\/17 tuned with data1_x_bin\n#The best parameter for AdaBoostClassifier is {'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0} with a runtime of 33.39 seconds.\n#The best parameter for BaggingClassifier is {'max_samples': 0.25, 'n_estimators': 300, 'random_state': 0} with a runtime of 30.28 seconds.\n#The best parameter for ExtraTreesClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0} with a runtime of 64.76 seconds.\n#The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 34.35 seconds.\n#The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 76.32 seconds.\n#The best parameter for GaussianProcessClassifier is {'max_iter_predict': 10, 'random_state': 0} with a runtime of 6.01 seconds.\n#The best parameter for LogisticRegressionCV is {'fit_intercept': True, 'random_state': 0, 'solver': 'liblinear'} with a runtime of 8.04 seconds.\n#The best parameter for BernoulliNB is {'alpha': 0.1} with a runtime of 0.19 seconds.\n#The best parameter for GaussianNB is {} with a runtime of 0.04 seconds.\n#The best parameter for KNeighborsClassifier is {'algorithm': 'brute', 'n_neighbors': 7, 'weights': 'uniform'} with a runtime of 4.84 seconds.\n#The best parameter for SVC is {'C': 2, 'decision_function_shape': 'ovo', 'gamma': 0.1, 'probability': True, 'random_state': 0} with a runtime of 29.39 seconds.\n#The best parameter for XGBClassifier is {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0} with a runtime of 46.23 seconds.\n#Total optimization time was 5.56 minutes.","a7a4017a":"#prepare data for modeling\nprint(data_val.info())\nprint(\"-\"*10)\n#data_val.sample(10)\n\n\n\n#handmade decision tree - submission score = 0.77990\ndata_val['Survived'] = mytree(data_val).astype(int)\nsubmit = data_val[['PassengerId','Survived']]\nsubmit.to_csv(\"..\/working\/handmade_submit.csv\", index=False)\n\n#decision tree w\/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\n#submit_dt = tree.DecisionTreeClassifier()\n#submit_dt = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\n#submit_dt.fit(data1[data1_x_bin], data1[Target])\n#print('Best Parameters: ', submit_dt.best_params_) #Best Parameters:  {'criterion': 'gini', 'max_depth': 4, 'random_state': 0}\n#data_val['Survived'] = submit_dt.predict(data_val[data1_x_bin])\n\n\n#bagging w\/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\n#submit_bc = ensemble.BaggingClassifier()\n#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {'n_estimators':grid_n_estimator, 'max_samples': grid_ratio, 'oob_score': grid_bool, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n#submit_bc.fit(data1[data1_x_bin], data1[Target])\n#print('Best Parameters: ', submit_bc.best_params_) #Best Parameters:  {'max_samples': 0.25, 'n_estimators': 500, 'oob_score': True, 'random_state': 0}\n#data_val['Survived'] = submit_bc.predict(data_val[data1_x_bin])\n\n\n#extra tree w\/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\n#submit_etc = ensemble.ExtraTreesClassifier()\n#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n#submit_etc.fit(data1[data1_x_bin], data1[Target])\n#print('Best Parameters: ', submit_etc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\n#data_val['Survived'] = submit_etc.predict(data_val[data1_x_bin])\n\n\n#random foreset w\/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\n#submit_rfc = ensemble.RandomForestClassifier()\n#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n#submit_rfc.fit(data1[data1_x_bin], data1[Target])\n#print('Best Parameters: ', submit_rfc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\n#data_val['Survived'] = submit_rfc.predict(data_val[data1_x_bin])\n\n\n\n#ada boosting w\/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\n#submit_abc = ensemble.AdaBoostClassifier()\n#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={'n_estimators': grid_n_estimator, 'learning_rate': grid_ratio, 'algorithm': ['SAMME', 'SAMME.R'], 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n#submit_abc.fit(data1[data1_x_bin], data1[Target])\n#print('Best Parameters: ', submit_abc.best_params_) #Best Parameters:  {'algorithm': 'SAMME.R', 'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0}\n#data_val['Survived'] = submit_abc.predict(data_val[data1_x_bin])\n\n\n#gradient boosting w\/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\n#submit_gbc = ensemble.GradientBoostingClassifier()\n#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={'learning_rate': grid_ratio, 'n_estimators': grid_n_estimator, 'max_depth': grid_max_depth, 'random_state':grid_seed}, scoring = 'roc_auc', cv = cv_split)\n#submit_gbc.fit(data1[data1_x_bin], data1[Target])\n#print('Best Parameters: ', submit_gbc.best_params_) #Best Parameters:  {'learning_rate': 0.25, 'max_depth': 2, 'n_estimators': 50, 'random_state': 0}\n#data_val['Survived'] = submit_gbc.predict(data_val[data1_x_bin])\n\n#extreme boosting w\/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\n#submit_xgb = XGBClassifier()\n#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {'learning_rate': grid_learn, 'max_depth': [0,2,4,6,8,10], 'n_estimators': grid_n_estimator, 'seed': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n#submit_xgb.fit(data1[data1_x_bin], data1[Target])\n#print('Best Parameters: ', submit_xgb.best_params_) #Best Parameters:  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0}\n#data_val['Survived'] = submit_xgb.predict(data_val[data1_x_bin])\n\n\n#hard voting classifier w\/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\n#data_val['Survived'] = vote_hard.predict(data_val[data1_x_bin])\ndata_val['Survived'] = grid_hard.predict(data_val[data1_x_bin])\nsubmit = data_val[['PassengerId','Survived']]\nsubmit.to_csv(\"..\/working\/submit_hard.csv\", index=False)\n\n\n#soft voting classifier w\/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\n#data_val['Survived'] = vote_soft.predict(data_val[data1_x_bin])\n#data_val['Survived'] = grid_soft.predict(data_val[data1_x_bin])\n\n\n#submit file\ndata_val['Survived'] = (mytree(data_val).astype(int) + grid_hard.predict(data_val[data1_x_bin])) * 0.5\nsubmit = data_val[['PassengerId','Survived']]\nsubmit.to_csv(\"..\/working\/submit_emsenble_hard_and_hand.csv\", index=False)\n\nprint('Validation Data Distribution: \\n', data_val['Survived'].value_counts(normalize = True))\nsubmit.sample(10)\n\n","bba3b8cf":"<a id=\"ch9\"><\/a>\n# 5.12 Tune Model with Hyper-Parameters\nWhen we used [sklearn Decision Tree (DT) Classifier](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves opportunity to see how various hyper-parameter settings will change the model accuracy.  [(Click here to learn more about parameters vs hyper-parameters.)](https:\/\/www.youtube.com\/watch?v=EJtTNboTsm8)\n\nHowever, in order to tune a model, we need to actually understand it. That's why I took the time in the previous sections to show you how predictions work. Now let's learn a little bit more about our DT algorithm.\n\nCredit: [sklearn](http:\/\/scikit-learn.org\/stable\/modules\/tree.html#classification)\n\n>**Some advantages of decision trees are:**\n* Simple to understand and to interpret. Trees can be visualized.\n* Requires little data preparation. Other techniques often require data normalization, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.\n* The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n* Able to handle both numerical and categorical data. Other techniques are usually specialized in analyzing datasets that have only one type of variable. See algorithms for more information.\n* Able to handle multi-output problems.\n* Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by Boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.\n* Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n* Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n\n> **The disadvantages of decision trees include:**\n* Decision-tree learners can create over-complex trees that do not generalize the data well. This is called overfitting. Mechanisms such as pruning (not currently supported), setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n* Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n* The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\n* There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n* Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.\n\n\n\nBelow are available hyper-parameters and [defintions](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier):\n> class sklearn.tree.DecisionTreeClassifier(criterion=\u2019gini\u2019, splitter=\u2019best\u2019, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)\n\n\nWe will tune our model using [ParameterGrid](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.ParameterGrid.html#sklearn.model_selection.ParameterGrid), [GridSearchCV](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV), and customized [sklearn scoring](http:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html); [click here to learn more about ROC_AUC scores](http:\/\/www.dataschool.io\/roc-curves-and-auc-explained\/). We will then visualize our tree with [graphviz](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.export_graphviz.html#sklearn.tree.export_graphviz). [Click here to learn more about ROC_AUC scores](http:\/\/www.dataschool.io\/roc-curves-and-auc-explained\/).\n","dbda4b1f":"<a id=\"ch8\"><\/a>\n## 5.1 Evaluate Model Performance\nLet's recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~82% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we're only going to increase our accuracy by 1\/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So, keep that in mind when improving your model.\n\n### Data Science 101: Determine a Baseline Accuracy ###\nBefore we decide how-to make our model better, let's determine if our model is even worth keeping. To do that, we have to go back to the basics of data science 101. We know this is a binary problem, because there are only two possible outcomes; passengers survived or died. So, think of it like a coin flip problem. If you have a fair coin and you guessed heads or tail, then you have a 50-50 chance of guessing correct. So, let's set 50% as the worst model performance; because anything lower than that, then why do I need you when I can just flip a coin?\n\nOkay, so with no information about the dataset, we can always get 50% with a binary problem. But we have information about the dataset, so we should be able to do better. We know that 1,502\/2,224 or 67.5% of people died. Therefore, if we just predict the most frequent occurrence, that 100% of people died, then we would be right 67.5% of the time. So, let's set 68% as bad model performance, because again, anything lower than that, then why do I need you, when I can just predict using the most frequent occurrence.\n\n### Data Science 101: How-to Create Your Own Model ###\nOur accuracy is increasing, but can we do better? Are there any signals in our data? To illustrate this, we're going to build our own decision tree model, because it is the easiest to conceptualize and requires simple addition and multiplication calculations. When creating a decision tree, you want to ask questions that segment your target response, placing the survived\/1 and dead\/0 into homogeneous subgroups. This is part science and part art, so let's just play the 21-question game to show you how it works. If you want to follow along on your own, download the train dataset and import into Excel. Create a pivot table with survival in the columns, count and % of row count in the values, and the features described below in the rows.\n\nRemember, the name of the game is to create subgroups using a decision tree model to get survived\/1 in one bucket and dead\/0 in another bucket. Our rule of thumb will be the majority rules. Meaning, if the majority or 50% or more survived, then everybody in our subgroup survived\/1, but if 50% or less survived then if everybody in our subgroup died\/0. Also, we will stop if the subgroup is less than 10 and\/or our model accuracy plateaus or decreases. Got it? Let's go!\n\n***Question 1: Were you on the Titanic?*** If Yes, then majority (62%) died. Note our sample survival is different than our population of 68%. Nonetheless, if we assumed everybody died, our sample accuracy is 62%.\n\n***Question 2: Are you male or female?*** Male, majority (81%) died. Female, majority (74%) survived. Giving us an accuracy of 79%.\n\n***Question 3A (going down the female branch with count = 314): Are you in class 1, 2, or 3?*** Class 1, majority (97%) survived and Class 2, majority (92%) survived. Since the dead subgroup is less than 10, we will stop going down this branch. Class 3, is even at a 50-50 split. No new information to improve our model is gained.\n\n***Question 4A (going down the female class 3 branch with count = 144): Did you embark from port C, Q, or S?*** We gain a little information. C and Q, the majority still survived, so no change. Also, the dead subgroup is less than 10, so we will stop. S, the majority (63%) died. So, we will change females, class 3, embarked S from assuming they survived, to assuming they died. Our model accuracy increases to 81%. \n\n***Question 5A (going down the female class 3 embarked S branch with count = 88):*** So far, it looks like we made good decisions. Adding another level does not seem to gain much more information. This subgroup 55 died and 33 survived, since majority died we need to find a signal to identify the 33 or a subgroup to change them from dead to survived and improve our model accuracy. We can play with our features. One I found was fare 0-8, majority survived. It's a small sample size 11-9, but one often used in statistics. We slightly improve our accuracy, but not much to move us past 82%. So, we'll stop here.\n\n***Question 3B (going down the male branch with count = 577):*** Going back to question 2, we know the majority of males died. So, we are looking for a feature that identifies a subgroup that majority survived. Surprisingly, class or even embarked didn't matter like it did for females, but title does and gets us to 82%. Guess and checking other features, none seem to push us past 82%. So, we'll stop here for now.\n\nYou did it, with very little information, we get to 82% accuracy. On a worst, bad, good, better, and best scale, we'll set 82% to good, since it's a simple model that yields us decent results. But the question still remains, can we do better than our handmade model? \n\nBefore we do, let's code what we just wrote above. Please note, this is a manual process created by \"hand.\" You won't have to do this, but it's important to understand it before you start working with MLA. Think of MLA like a TI-89 calculator on a Calculus Exam. It's very powerful and helps you with a lot of the grunt work. But if you don't know what you're doing on the exam, a calculator, even a TI-89, is not going to help you pass. So, study the next section wisely.\n\nReference: [Cross-Validation and Decision Tree Tutorial](http:\/\/www.cs.utoronto.ca\/~fidler\/teaching\/2015\/slides\/CSC411\/tutorial3_CrossVal-DTs.pdf)","f40f559c":"<a id=\"ch12\"><\/a>\n# Step 7: Optimize and Strategize\n## Conclusion\nIteration one of the Data Science Framework, seems to converge on 0.77990 submission accuracy. Using the same dataset and different implementation of a decision tree (adaboost, random forest, gradient boost, xgboost, etc.) with tuning does not exceed the 0.77990 submission accuracy. Interesting for this dataset, the simple decision tree algorithm had the best default submission score and with tuning achieved the same best accuracy score.\n\nWhile no general conclusions can be made from testing a handful of algorithms on a single dataset, there are several observations on the mentioned dataset. \n1. The train dataset has a different distribution than the test\/validation dataset and population. This created wide margins between the cross validation (CV) accuracy score and Kaggle submission accuracy score.\n2. Given the same dataset, decision tree based algorithms, seemed to converge on the same accuracy score after proper tuning.\n3. Despite tuning, no machine learning algorithm, exceeded the homemade algorithm. The author will theorize, that for small datasets, a manmade algorithm is the bar to beat. \n\nWith that in mind, for iteration two, I would spend more time on preprocessing and feature engineering. In order to better align the CV score and Kaggle score and improve the overall accuracy.\n\n","1a1e30a1":"<a id=\"ch6\"><\/a>\n# Step 4: Perform Exploratory Analysis with Statistics\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.","95feec9f":"## 3.24 Da-Double Check Cleaned Data\n\nNow that we've cleaned our data, let's do a discount da-double check!","3730a1cf":"<a id=\"ch10\"><\/a>\n## 5.13 Tune Model with Feature Selection\nAs stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. [Sklearn](http:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).","4132dd35":"<a id=\"ch11\"><\/a>\n# Step 6: Validate and Implement\nThe next step is to prepare for submission using the validation data. "}}