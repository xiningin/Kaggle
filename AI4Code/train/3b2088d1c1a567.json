{"cell_type":{"cb4bf263":"code","2a874e0e":"code","b7dd1cfe":"code","264a48dc":"code","f305b202":"code","c8bcf085":"code","e487619b":"code","d16fcd74":"code","ed640ced":"code","bb3269c2":"code","bf8dca80":"code","d530317d":"code","af22050e":"code","494b4757":"code","e641ae72":"code","ff8ad7d4":"code","87633abc":"code","02de5924":"code","3d62f7f7":"code","61ee2688":"code","cffd2652":"code","503a640e":"code","61d8945e":"code","5e86bc42":"code","6a9261ca":"code","8c35ee69":"markdown","187a9f16":"markdown","a06924ac":"markdown","ef0c1581":"markdown","3f1faf71":"markdown","c4607d97":"markdown","385dbbf2":"markdown","d9dfb754":"markdown","240296b6":"markdown","b082d65c":"markdown","67062b32":"markdown"},"source":{"cb4bf263":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nimport nltk\nimport regex as re\nfrom nltk.stem import WordNetLemmatizer\nimport string\nfrom scipy.special import softmax\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2a874e0e":"from sklearn import model_selection\nfrom sklearn.metrics import mean_squared_error\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n#to install datasets library\n!pip install datasets\n!pip install fsspec==0.9.0\nfrom datasets import Dataset,load_metric","b7dd1cfe":"train = pd.read_csv(r'\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv(r'\/kaggle\/input\/nlp-getting-started\/test.csv')","264a48dc":"train.head()","f305b202":"#Removing punchuation from sentences and stop words\n\nnltk.download('stopwords', quiet=True)\nstopwords = nltk.corpus.stopwords.words('english')\ndef remove_pun_stopwords(text):\n    text = re.sub(r'[^\\w\\s]','',text)\n    text = [i.lower() for i in text.lower().split() if i not in stopwords]\n    return(' '.join(text))","c8bcf085":"def clean_text(text):\n\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","e487619b":"def clean(text):\n    text = remove_pun_stopwords(text)\n    text = clean_text(text)\n    return text","d16fcd74":"train['text'] = train['text'].apply(clean)\ntest['text'] = test['text'].apply(clean)","ed640ced":"lemmatizer = WordNetLemmatizer()\n\ndef word_lemmatizer(text):\n    \n    text = [lemmatizer.lemmatize(i) for i in text.split()]\n    return(' '.join(text))\n\ntrain['text'] = train['text'].apply(word_lemmatizer)\ntest['text'] = test['text'].apply(word_lemmatizer)","bb3269c2":"def create_folds(data, num_splits):\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1,random_state=10).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=num_splits)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data","bf8dca80":"#Creating K - folds for training and validation\ntrain = create_folds(train, num_splits=5)\ntrain = train.rename(columns={'target':'label'})","d530317d":"batch_size = 16\nmax_length = 256","af22050e":"train_dataset = Dataset.from_pandas(train[train.kfold != 0].reset_index(drop=True))\nvalid_dataset = Dataset.from_pandas(train[train.kfold == 0].reset_index(drop=True))","494b4757":"tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\ndef tokenize(batch): return tokenizer(batch['text'], padding=True,truncation=True, max_length=max_length)","e641ae72":"train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\nvalid_dataset = valid_dataset.map(tokenize, batched=True, batch_size=len(valid_dataset))","ff8ad7d4":"# disable W&B logging as we don't have access to the internet\n%env WANDB_DISABLED=True","87633abc":"metric = load_metric(\"accuracy\")\nmodel = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english', num_labels=2)\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\nargs = TrainingArguments(\n    \"outputs_dir\",\n    evaluation_strategy = \"epoch\",\n    learning_rate=2e-5,\n    fp16=True,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=3,\n    seed=7,\n    weight_decay=0.005,\n    load_best_model_at_end=True\n)\n\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n    tokenizer=tokenizer,\ncompute_metrics=compute_metrics\n)\n\ncolumns_to_return = ['input_ids', 'label', 'attention_mask']\ntrain_dataset.set_format(type='torch', columns=columns_to_return)\nvalid_dataset.set_format(type='torch', columns=columns_to_return)","02de5924":"trainer.train()","3d62f7f7":"test_dataset = Dataset.from_pandas(test)\ntest_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))","61ee2688":"columns_to_return = ['input_ids', 'attention_mask']\ntest_dataset.set_format(type='torch', columns=columns_to_return)","cffd2652":"test_preds = trainer.predict(test_dataset)\n#using softmax to convert to probabilities\nprobabilities = softmax(test_preds[0], axis=1)\nprobabilities\n","503a640e":"test_ids = test['id'].values\n\nsubmission = pd.DataFrame({\n    'id': test_ids,\n    'target': probabilities[:,1]\n})","61d8945e":"submission.loc[submission['target']<0.5,'target'] = 0\nsubmission.loc[submission['target']>=0.5,'target'] = 1\nsubmission['target'] = submission['target'].astype(int)","5e86bc42":"submission['target'].value_counts()","6a9261ca":"submission.to_csv('submission.csv',index = False)","8c35ee69":"> **Lemmatization**\n> \n> Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word. Stemming follows an algorithm with steps to perform on the words which makes it faster.\n> \n> > We will use Lemmatization here as it will keep an actual word.","187a9f16":"**Please comment and upvote. Thanks**","a06924ac":"> * Remove punctuations, numbers, brackets, links and special character.","ef0c1581":"> * **Defining batch size and max length**","3f1faf71":"# Text Cleaning","c4607d97":"> * **Tokenizing the test data set**","385dbbf2":"> * **Tokenizing the data** ","d9dfb754":"**Modelling**\n\nHere we are using **distilbert-base-uncased model** however you can use any other model as well. You can go through the following link and try different models:\n\n[https:\/\/huggingface.co\/models?pipeline_tag=text-classification](http:\/\/)","240296b6":"**Hugging Face Model**\n\nIn the script I have tried to do prediction using hugging face model after performing text cleaning.\n\nIn the script I have performed the following steps:\n\n1. text cleaning\n2. Custom K-fold\n3. Setting up Hugging face model\n4. Prediction","b082d65c":"> * **Splitting data in train and validation**","67062b32":"> **Custom K fold to make training and validation set**"}}