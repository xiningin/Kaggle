{"cell_type":{"3acaf8c2":"code","61fbae39":"code","28085e9d":"code","49ca1289":"code","4f26d499":"code","69264801":"code","14623e8b":"markdown","8a2188ac":"markdown","6075e46f":"markdown","c008364f":"markdown","1f8f6d6b":"markdown","f2ec41fd":"markdown"},"source":{"3acaf8c2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","61fbae39":"from pandas.plotting import scatter_matrix\ndf_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\ny = df_train['Survived']\nfeatures = ['Pclass', 'Sex','SibSp', 'Parch', 'Fare']\nX = pd.get_dummies(df_train[features])\n## Plotting features for data exploration\nX.hist()\n#X.plot(kind='density', subplots=True, layout=(3,3), sharex=False)\n#scatter_matrix(X)\nplt.show()","28085e9d":"## Evaluating the best max_depth, and best n_estimators on training data only\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n# split data into training and validation data, for both features and target\n# The split is based on a random number generator. Supplying a numeric value to\n# the random_state argument guarantees we get the same split every time we\n# run this script.\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n\nmaes = []\nmax_depths = []\nn_estimators = []\ndef get_mae(max_depth):\n    # Define model\n    model = RandomForestClassifier(n_estimators=100, max_depth=max_depth, random_state=1)\n    # Fit model\n    model.fit(train_X, train_y)\n    # get predicted \n    predictions = model.predict(val_X)\n    ##Howm many are incorrect? \n    return mean_absolute_error(predictions, val_y)\nfor i in range(1,30):\n        maes.append(get_mae(i))\n        max_depths.append(i)\n        \n\nmapped_depth_to_mae = dict(zip(max_depths, maes))\noptimal_depth = min(mapped_depth_to_mae, key=mapped_depth_to_mae.get)\n#print(mapped_depth_to_mae)\nprint(\"The best Max Depth was: \",optimal_depth)\n\n\n","49ca1289":"## Evaluating the best n_estimators on training data only\n# split data into training and validation data, for both features and target\n# The split is based on a random number generator. Supplying a numeric value to\n# the random_state argument guarantees we get the same split every time we\n# run this script.\n##Carrying forward the optimal depth from above.\nmaes = []\nn_estimators_list = []\ndef get_mae(n_estimators):\n    # Define model\n    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=optimal_depth, random_state=1)\n    # Fit model\n    model.fit(train_X, train_y)\n    # get predicted \n    predictions = model.predict(val_X)\n    ##Howm many are incorrect? \n    return mean_absolute_error(predictions, val_y)\n## Low resolution check\nfor i in range(1,500,20):\n        maes.append(get_mae(i))\n        n_estimators_list.append(i)\n        \n\n\nmapped_nestimator_to_mae = dict(zip(n_estimators_list, maes))\noptimal_nestimator = min(mapped_nestimator_to_mae, key=mapped_nestimator_to_mae.get)\n#print(mapped_nestimator_to_mae)\nprint(\"The initial best n_estimator was: \",optimal_nestimator)\n\nmaes = []\nn_estimators_list = []\n## Higher resolution check\nfor i in range((optimal_nestimator-11),(optimal_nestimator+10),1):\n        maes.append(get_mae(i))\n        n_estimators_list.append(i)\nmapped_nestimator_to_mae = dict(zip(n_estimators_list, maes))\noptimal_nestimator = min(mapped_nestimator_to_mae, key=mapped_nestimator_to_mae.get)\n#print(mapped_nestimator_to_mae)\nprint(\"The optimised best n_estimator was: \",optimal_nestimator)\n        \n\n","4f26d499":"## Re-writing random forest to use optiaml max_depth\nX_test = pd.get_dummies(df_test[features])\nX_test['Fare'] = X_test['Fare'].fillna(0)\nmodel = RandomForestClassifier(n_estimators=optimal_nestimator, max_depth=optimal_depth, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)","69264801":"output = pd.DataFrame({'PassengerId': df_test.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\n\n\n","14623e8b":"# Exploratory data analytics\n* Feature selection\n* Graphing features to gain insights and understanding ","8a2188ac":"# Finding optimal n_estimators ","6075e46f":"# Using final parameters in output","c008364f":"# Finding optimal Max Depth\n","1f8f6d6b":"# Train Test Split and Evaluating Model Parameters ","f2ec41fd":"# Titanic Classifier - Random forest\n* My first real machine learning project.\n* Emphasis on learning about the parameters used in a Random Forest.\n* Emphasis on learning about data pre-proccessing, ie one-hot encoding or ordinal encoding for  categorical values \n\n"}}