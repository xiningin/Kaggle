{"cell_type":{"5005eb0f":"code","8cbc63ac":"code","cc7c7851":"code","76ecf5dc":"code","7c41045e":"code","9e991bf4":"code","3c0bac0e":"code","752d5f0f":"code","efa52e41":"code","48d58069":"code","1629b95e":"code","a4c82086":"code","e029cc51":"code","0674d408":"code","bf7b3a1a":"code","2d985dfa":"code","6735d72a":"code","30f5fc25":"code","2a31e7be":"code","639d2c39":"code","04b4f9f1":"code","81081c8e":"code","a31a3fcb":"code","4b3f2dd3":"code","22ec89ed":"code","1fd55d02":"code","88270c0f":"code","e16760f4":"code","588c73c1":"code","eac87d29":"code","76b722ad":"code","8a70666d":"code","99a3060c":"code","3f2e8df7":"code","84322af8":"code","3670ef67":"code","009f6a12":"code","eed626a3":"code","755f510b":"code","3535d97f":"code","40f0a1d8":"code","83c7e8a7":"code","2e4b8d0f":"code","43c30192":"code","59137540":"code","8b738f5a":"code","66b47fae":"code","79516fc0":"markdown","fc7a1518":"markdown","4633cd8e":"markdown","b4da557b":"markdown","0000134f":"markdown","1b5daf1f":"markdown","8f1e98e5":"markdown","4cd47adf":"markdown","e63397ef":"markdown","f0e43f8a":"markdown","ab854cc8":"markdown","f279ca72":"markdown","a07e793a":"markdown","18cdf16a":"markdown","41b3c1a7":"markdown","2782c61c":"markdown","6c1e3de8":"markdown","5515dbaa":"markdown","770eb33e":"markdown","d3b51ac5":"markdown","d78b2c92":"markdown","e622da5f":"markdown","1d7d3d27":"markdown","3f021568":"markdown","1b7de357":"markdown","60ab4e39":"markdown","930337ae":"markdown","aa94fa4d":"markdown","5ba196b0":"markdown","8c5ffabd":"markdown","3cf1c18f":"markdown","657109a5":"markdown","3e41b38a":"markdown","917bbbfa":"markdown","04cff27d":"markdown","3eddf9d7":"markdown","820d58d3":"markdown","898137e8":"markdown","7565df05":"markdown","eae7d6f1":"markdown","6c4a882c":"markdown","804b396c":"markdown","5479af47":"markdown","600616ef":"markdown","322651ba":"markdown","e22e024c":"markdown","da5ee552":"markdown","b7ce80dc":"markdown"},"source":{"5005eb0f":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","8cbc63ac":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n","cc7c7851":"train.head()","76ecf5dc":"# find categorical variables\ncategorical = [var for var in train.columns if train[var].dtype=='O']\nprint('There are {} categorical variables'.format(len(categorical)))","7c41045e":"# find numerical variables\nnumerical = [var for var in train.columns if train[var].dtype!='O']\nprint('There are {} numerical variables'.format(len(numerical)))","9e991bf4":"data = [train,test]\nfor dataset in data:\n    #Filter categorical variables\n    categorical_columns = [x for x in dataset.dtypes.index if dataset.dtypes[x]=='object']    \n    # Exclude ID cols and source:\n    categorical_columns = [x for x in categorical_columns if x not in ['PassengerId','Ticket','Name','Cabin']]\n    #Print frequency of categories\n    \nfor col in categorical_columns:\n    print ('\\nFrequency of Categories for variable %s'%col)\n    print (train[col].value_counts())","3c0bac0e":"train.isnull().sum()","752d5f0f":"train.isnull().mean()","efa52e41":"plt.figure(figsize=(15,6))\nplt.subplot(1, 2, 1)\nfig = train.boxplot(column='Age')\nfig.set_title('')\nfig.set_ylabel('Age')\n\nplt.subplot(1, 2, 2)\nfig = train.boxplot(column='Fare')\nfig.set_title('')\nfig.set_ylabel('Fare')","48d58069":"plt.figure(figsize=(15,6))\n\nplt.subplot(1, 2, 1)\nfig = train.Age.hist(bins=20)\nfig.set_ylabel('Number of passengers')\nfig.set_xlabel('Age')\n\nplt.subplot(1, 2, 2)\nfig = train.Fare.hist(bins=20)\nfig.set_ylabel('Number of passengers')\nfig.set_xlabel('Fare')","1629b95e":"train[train.Embarked.isnull()]","a4c82086":"## Creating a dummy variable, which indicates that whether cabin value is missing\ntrain['cabin_null'] = np.where(train.Cabin.isnull(),1,0)\ntrain.groupby(['Survived'])['cabin_null'].mean()","e029cc51":"train['age_null'] = np.where(train.Age.isnull(),1,0)\ntrain.groupby(['Survived'])['age_null'].mean()","0674d408":"# lets look at the actual percentage of passengers on the upper Fare ranges\ntotal_passengers = np.float(train.shape[0])\n\nprint('total number of passengers: {}'.format(train.shape[0]))\n\nprint('passengers that paid more than 65: {} %'.format((\n    train[train.Fare > 65].shape[0]\/ total_passengers)*100))\nprint('passengers that paid more than 100: {} %'.format((\n    train[train.Fare > 100].shape[0]\/ total_passengers)*100))","bf7b3a1a":"#at the most extreme outliers\ntrain[train.Fare>300]","2d985dfa":"print('Number of categories in the variable Name: {}'.format(\n    len(train.Name.unique())))\n\nprint('Number of categories in the variable Gender: {}'.format(\n    len(train.Sex.unique())))\n\nprint('Number of categories in the variable Ticket: {}'.format(\n    len(train.Ticket.unique())))\n\nprint('Number of categories in the variable Cabin: {}'.format(\n    len(train.Cabin.unique())))\n\nprint('Number of categories in the variable Embarked: {}'.format(\n    len(train.Embarked.unique())))\n\nprint('Total number of passengers in the Titanic: {}'.format(len(train)))","6735d72a":"drop_column = ['cabin_null','age_null']\ntrain.drop(drop_column , axis =1  ,inplace = True )","30f5fc25":"train.isnull().sum()","2a31e7be":"test.isnull().sum()","639d2c39":"data_cleaner = [test , train]\nfor dataset in data_cleaner:    \n    #completing missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #completing embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #completing missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n#delete the train feature\ntrain.drop(['Ticket'], axis=1, inplace = True)\ntest.drop(['Ticket'] , axis=1 , inplace = True)","04b4f9f1":"drop_column = ['Cabin']\ntrain.drop(drop_column , axis =1  ,inplace = True )\ntest.drop(drop_column , axis =1  ,inplace = True )","81081c8e":"full_data = [train,test]\nfor dataset in full_data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare_Band'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare_Band'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare_Band'] = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare_Band'] = 3\n    dataset['Fare_Band'] = dataset['Fare_Band'].astype(int)\n    dataset.drop(['Fare' ], axis = 1 , inplace =True)","a31a3fcb":"full_data = [test , train]\nfor dataset in full_data:\n    \n    dataset.loc[ dataset['Age'] <= 10, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 10) & (dataset['Age'] <= 15), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 15) & (dataset['Age'] <= 20), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 20) & (dataset['Age'] <= 25), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 25) & (dataset['Age'] <= 30), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 30) & (dataset['Age'] <= 45), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 45) & (dataset['Age'] <= 60), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 60, 'Age'] = 7\n    dataset['Age'] = dataset['Age'].astype(int)","4b3f2dd3":"full_data = [test , train]\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n","22ec89ed":"train.drop(['Name'],axis = 1, inplace = True)\ntest.drop(['Name'],axis = 1, inplace = True )","1fd55d02":"train['family_size'] = train['SibSp'] + train['Parch'] + 1 \ntest['family_size'] = test['SibSp'] + test['Parch'] + 1 \ntest['IsAlone'] = 1 \ntrain['IsAlone'] = 1 \ntrain['IsAlone'].loc[train['family_size'] > 1] = 0\ntest['IsAlone'].loc[test['family_size'] > 1] = 0 \ntest.drop(['SibSp' , 'Parch'], axis = 1 , inplace =True)\ntrain.drop(['SibSp','Parch' ], axis = 1 , inplace =True)","88270c0f":"test.isnull().sum()","e16760f4":"g = sns.FacetGrid(train, col=\"Survived\",  row=\"Sex\", hue = \"Embarked\" ,  size = 5 )\ng.map(plt.hist, \"Pclass\" ,edgecolor = \"w\").add_legend()","588c73c1":"plt.figure(figsize = [12,6])\nsns.violinplot(x=\"Fare_Band\", y=\"Age\", data=train, hue='Survived',palette='coolwarm')","eac87d29":"train[['family_size', 'Survived']].groupby(['family_size'], as_index=False).mean()","76b722ad":"axes = sns.factorplot('family_size','Survived', hue = 'Sex',data=train,aspect = 3)","8a70666d":"plt.figure(figsize=(10,10))\nsns.heatmap(train.drop('PassengerId',axis=1).corr(), square=True, annot=True)","99a3060c":"X = train.drop('Survived' , axis = 1 )\ny = train['Survived']\n\nfrom sklearn.model_selection import train_test_split\nX_train ,X_test , y_train , y_test = train_test_split(X , y , test_size = 0.3 , random_state =102)","3f2e8df7":"X_train=X_train.drop(['PassengerId'],axis=1)\nX_test = X_test.drop(['PassengerId'],axis=1)","84322af8":"#Importing all models\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score","3670ef67":"logmodel = LogisticRegression()\nlogmodel.fit(X_train , y_train)\npred_l = logmodel.predict(X_test)\nacc_l = accuracy_score(y_test , pred_l)*100\nacc_l","009f6a12":"random_forest = RandomForestClassifier(n_estimators= 100)\nrandom_forest.fit(X_train, y_train)\npred_rf = random_forest.predict(X_test)\nacc_rf = accuracy_score(y_test , pred_rf)*100\nacc_rf","eed626a3":"knn = KNeighborsClassifier(n_neighbors = 3)\n\nknn.fit(X_train, y_train)\n\npred_knn = knn.predict(X_test)\n\nacc_knn = accuracy_score(y_test , pred_knn)*100\nacc_knn","755f510b":"gaussian = GaussianNB()\n\ngaussian.fit(X_train, y_train)\n\npred_gb = gaussian.predict(X_test)\n\nacc_gb = accuracy_score(y_test , pred_gb)*100\nacc_gb","3535d97f":"svc = SVC()\n\nsvc.fit(X_train, y_train)\n\npred_svc = svc.predict(X_test)\n\nacc_svc = accuracy_score(y_test , pred_svc)*100\nacc_svc","40f0a1d8":"decision_tree = DecisionTreeClassifier()\n\ndecision_tree.fit(X_train, y_train)\n\npred_dt = decision_tree.predict(X_test)\n\nacc_dt = accuracy_score(y_test , pred_dt)*100\nacc_dt","83c7e8a7":"sgd = SGDClassifier()\n\nsgd.fit(X_train, y_train)\n\npred_sgd = sgd.predict(X_test)\n\nacc_sgd = accuracy_score(y_test , pred_sgd)*100\nacc_sgd","2e4b8d0f":"## Arranging the Accuracy results\nmodels = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Random Forrest','K- Nearest Neighbour' ,\n             'Naive Bayes' , 'C-Support Vector Classifier' , 'Decision Tree' , 'Stochastic Gradient Descent'],\n    'Score': [acc_l , acc_rf , acc_knn , acc_gb , acc_svc , \n              acc_dt , acc_sgd]})\nmodels.sort_values(by='Score', ascending=False)","43c30192":"df_test =  test.drop(['PassengerId'],axis=1)\n\np_l = logmodel.predict(df_test)\np_svc = svc.predict(df_test)\np_rf = random_forest.predict(df_test)\np_dt = decision_tree.predict(df_test)","59137540":"predict_combine = np.zeros((df_test.shape[0]))\nfor i in range(0, test.shape[0]):\n    temp = p_rf[i]+p_svc[i]+p_l[i]+p_dt[i]\n    if temp>=2:\n        predict_combine[i] = 1\npredict_combine = predict_combine.astype('int')","8b738f5a":"submission = pd.DataFrame({\n       \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": predict_combine\n    })\n\nsubmission.to_csv(\"submission.csv\", encoding='utf-8', index=False)","66b47fae":"submission.head()","79516fc0":"#### B.1. Types of Variables","fc7a1518":"We find with increase in family size the survival rate decreases.","4633cd8e":"Importing models from scikit learn module. The objective is to classify the passenger survivior into two classes: 0 or 1, hence this is a binary classification for which we will be using classifiers. Following part of this notebook compares and finds the best model suitable for the data based upon accuracy metrics.  ","b4da557b":"## C. Feature Scaling and Engineering","0000134f":"# Titanic Survival Prediction\n---\nThis is the legendary Titanic ML competition \u2013 the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck. More detals [here](https:\/\/www.kaggle.com\/c\/titanic\/overview) and ofcourse [Kaggle's YouTube Video](https:\/\/www.youtube.com\/watch?v=8yZMXCaFshs).\n","1b5daf1f":"The Passengers did survive and they were having same ticket, same Cabin, Same Fare but somehow doesn't have the Embarked data which is possible to be generated during time of buiding the dataset. Hence the Feature has random missing values.","8f1e98e5":"### E.3. K-Nearest Neighbours","4cd47adf":"- Observations\n    - From above graph we observe that more number of females survived as compared to males. The female survivors were more from the first class and male from third class were the most to die.\n    - The 3rd class people were the most affected, that is they less survived where as 1st class people survived is maximum than others. \n    - The second class has almost equal survived and  couldn't survive number of people. And also we notice many of the passengers Embarked from \"S\".\n","e63397ef":"#### B.7. Categorical Values : ","f0e43f8a":"#### B.2. Detecting Missing  Values:","ab854cc8":"### B.5. Analyzing the Age feature:","f279ca72":"The train dataset has 12 features\/variables with missing values in features: **Age** (19.86%), **Cabin** (77.10%) and **Embarked** (00.22%) <br> \n\n- Analysing and Assumming of the missing data: <br>\n     - The **Cabin** feature has maximum missing values of about 77%. For many of the people who did not survive,the cabin they were staying in, could not be established. The people who survived could be asked for that information. <br>\n      \n     - The **Age** feature has missing values of about 22%.For the people who did not survive, the age they had could not be established. The people who survived could be asked for that information. <br>\n      \n     They have some reasaon why the data is missing, or there is some systematic missing of values, hence this falls under *Missing data Not at Random (**MNAR**) category.* <br>\n\n   Talking about Embarked missing values:<br>\n     - The **Embarked** the missing values is about 00.22% which was completely at random(**MCAR**), explained below.  ","a07e793a":"> Plan: Import data -> Feature Engineering -> Data Visualization -> Deveopment of Model -> Testing Model -> Predictions -> Submission ","18cdf16a":"#### C.2.1. Cabin Feature","41b3c1a7":"#### C.2.2. Fare Feature","2782c61c":"## E. Model Training and Predicting\n---","6c1e3de8":"### E.2. Random Forest","5515dbaa":"# Thankyou!\n\nAuthor: Pratik Kumar<br>\nResources: [Udemy Course](https:\/\/www.udemy.com\/share\/1020r6AkcfcV5TQXo=\/), Scikit-Learn docs","770eb33e":"**There is a systematic loss of data: people who did not survive tend to have more information missing. Presumably, the method chosen to gather the information, contributes to the generation of these missing data.**","d3b51ac5":"### E.4. Gaussian Naive Bayes Classifier","d78b2c92":"## A. Importing Data and Libraries","e622da5f":"### B.6. Analyzing the Fare feature :","1d7d3d27":"The above figures indicates that the missing data is more in the case of passengers not survived(=0).<br>\n\n**There is a systematic loss of data: people who did not survive tend to have more information missing. Presumably, the method chosen to gather the information, contributes to the generation of these missing data.**","3f021568":"Also we need to remove Id of passengers for prediction,","1b7de357":"#### B.3. Outliers detection :","60ab4e39":"#### C.2.6. Family Size","930337ae":"#### C.1. Handling the Missing Values:<br>\nThe missing Values are in the following features,\n","aa94fa4d":"### E.6. Decision Tree","5ba196b0":"#### C.2.4. Sex and Embarked Feature","8c5ffabd":"Mostly farebands are greater at the Age Group **\"4\"**. Survival also has *greater area corresponding to age group **\"4\"**.*","3cf1c18f":"The distribution of Fare is skewed, so in principle, we shouldn't estimate outliers using the mean plus minus 3 standard deviations methods, which assumes a normal distribution of the data.","657109a5":"### B.4. Analyzing Cabin feature :","3e41b38a":"### E.5. C-Support Vector Classifier","917bbbfa":"##### Viewing the Categorical terms :","04cff27d":"#### C.2.3. Age Feature","3eddf9d7":"## D. Visualizations\n---","820d58d3":"### B.3. Analyzing the Embarked feature:","898137e8":"#### C.2.5. Droping the Name feature","7565df05":"Spitting the data in ro train and test","eae7d6f1":"There *is **unusual** high values of **Fares** observed*, the reason is found as follows:","6c4a882c":"## B. Dataset exploration: \n---","804b396c":"These three people have the same ticket number, indicating that they were travelling together. The Fare price in this case, 512 is the price of 3 tickets, and not one. **This is why, it is unusually high.**","5479af47":"### C.2. Encoding","600616ef":"- Undestanding the Correlation matrix:\n    -  The FareBand and Pclass are highly correlated(-0.63) although negative, next to them is FareBand and IsAlone correlation(-0.57).\n    -  The Sex and Survived also have good correlation of (-0.54).\n    -  But as observed IsAlone and Family_size has the largest negative correlation (-0.69) is liable as the Family size and being alone are two opposite categories.","322651ba":"### E.1. Logistic Regression","e22e024c":"## Ensemble Learning","da5ee552":"### E.7. Linear classifiers with SGD training.","b7ce80dc":"## Submission"}}