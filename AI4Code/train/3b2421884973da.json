{"cell_type":{"e186187f":"code","9abf4420":"code","8b1f3f23":"code","394f3c45":"code","1537394a":"code","c0af0a0f":"code","a37dee6f":"code","b9b9fd6b":"code","27553284":"code","b60453a1":"code","9cfbe312":"code","c9abf20c":"code","c39d6819":"code","ef0ae8a8":"code","97edb40b":"code","9ea819cb":"code","ecf90f4d":"code","fe01c7d3":"code","b308eaa8":"code","6048219a":"code","bb28ec6e":"code","00ede805":"code","fffc3934":"code","80580653":"code","86e98b34":"code","9aad9a6c":"code","6571d29e":"code","f829cf92":"code","b1063336":"code","75262c54":"code","2689fa1c":"markdown","d0bc7155":"markdown","e407c48c":"markdown","119891a7":"markdown","b1f3895e":"markdown","4888d1a6":"markdown","a1e79357":"markdown","b994de41":"markdown","f662f923":"markdown","0c201448":"markdown","e04811d6":"markdown","bd6d56df":"markdown","817e6b81":"markdown","0f48c62c":"markdown","ef5bdc55":"markdown","b34c8eaf":"markdown","58d11069":"markdown","a7fab4c3":"markdown","15519517":"markdown","02721c65":"markdown","e1bda23c":"markdown","1267ddf1":"markdown","a2d4dfcc":"markdown","d749b909":"markdown","467c9d6f":"markdown","80f1dfca":"markdown","30a3611d":"markdown","c2900687":"markdown","a3f8a132":"markdown","5ec8e129":"markdown"},"source":{"e186187f":"# 1-Importing the Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nimport seaborn as sns\nfrom scipy.stats import beta\nfrom scipy.stats import f\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier","9abf4420":"# 2-Import Data\n# description --> https:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data\ndata = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv') ","8b1f3f23":"data.head()","394f3c45":"data.info()","1537394a":"data = data.iloc[:, 1:-1]\n\nvariables = data.iloc[:, 1:]\nlabels = data.iloc[:, 0]","c0af0a0f":"data.describe()","a37dee6f":"# For illustration purposes\ndata_visualization = data.iloc[:, [0, 1, 2, 3, 4, 14, 21, 22, 23, 24]]","b9b9fd6b":"# Visualisation of the data using a box plot\nfig=plt.figure(figsize=(16,8), dpi= 100, facecolor='w', edgecolor='k')\nax = sns.boxplot(data=data_visualization, orient=\"v\", palette=\"Set2\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=0)","27553284":"# outlier detection\nQ1 = variables.quantile(0.25)\nQ3 = variables.quantile(0.75)\nIQR = Q3 - Q1\nLower_Whisker = Q1 - 1.5 * IQR\nUpper_Whisker = Q3 + 1.5 * IQR\n\nprint('Lower Whisker:')\nprint(Lower_Whisker)\nprint('\\n\\n\\nUpperWhisker:')\nprint(Upper_Whisker)\n\nLower_outliers = variables < Lower_Whisker\nUpper_outliers = variables > Upper_Whisker\n\nLower_outliers_index = Lower_outliers.any(axis=1)\nUpper_outliers_index = Upper_outliers.any(axis=1)","b60453a1":"print(f'There are {sum(Lower_outliers_index)} points below the lower whisker')\nprint(f'There are {sum(Upper_outliers_index)} points above the upper whisker')","9cfbe312":"variables.skew()","c9abf20c":"median = variables.median()\nvariables = variables.where((variables >= Lower_Whisker) & (variables <= Upper_Whisker), median, axis=1)","c39d6819":"variables.skew()","ef0ae8a8":"# Pair plot\nsns.pairplot(data_visualization, hue='diagnosis')","97edb40b":"correlation = variables.corr('pearson')\nplt.figure(figsize=(25,25), dpi= 100, facecolor='w', edgecolor='k')\nax = sns.heatmap(correlation.round(2), cmap='RdYlGn_r', linewidths=0.5, annot=True,\n                 cbar=True, square=True, fmt='0.2f')\nplt.yticks(rotation=0)\nax.tick_params(labelbottom=False, labeltop=True)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45)\nplt.title('Correlation matrix')","9ea819cb":"'''''\n# Observations and variables\nobservations_index = list(data.index)\nvariables_name = list(data.columns[1:])\n\nfor i in range(len(variables_name)):\n    for j in range(len(variables_name)):\n        if i != j & j > i:\n            if correlation.iloc[i, j] > 0.9:\n                print(\"{} and {} are {} correlated\".format(variables_name[i], variables_name[j],\n                                                           correlation.iloc[i, j].round(3)))\n'''''","ecf90f4d":"# variables = variables.drop(['perimeter_mean', 'area_mean', 'radius_worst', 'perimeter_worst', 'perimeter_se'], axis=1)","fe01c7d3":"# Label Encoding\nlabels_encoder_response = LabelEncoder()\nlabels = labels_encoder_response.fit_transform(labels)","b308eaa8":"# train and test split\nX_train, X_test, y_train, y_test = train_test_split(variables, labels, test_size=0.2, random_state=0)\n\n# standardization\nsc_training = StandardScaler()\nX_train = sc_training.fit_transform(X_train)\nX_test = sc_training.transform(X_test)\n\n# Principal component analysis\npca = PCA()\nZ_train = pca.fit_transform(X_train)\nZ_test = pca.transform(X_test)","6048219a":"# Eigenvalues\nEigen_Values = pca.explained_variance_\nell = pca.explained_variance_ratio_\n\n\n# Scree plot\nplt.subplots(1, 2, figsize = (20, 10))\n\nax1 = plt.subplot(1, 2, 1)\nx = np.arange(len(Eigen_Values)) + 1\nax1.plot(x, Eigen_Values \/ Eigen_Values.sum(), 'ro-', lw=2)\nax1.set_xticks(x, [\"\" + str(i) for i in x])\nax1.set_xlabel('Number of components')\nax1.set_ylabel('Explained variance')\nax1.set_title('Scree Plot')\n\n# Pareto plot\nax2 = plt.subplot(1, 2, 2)\nind = np.arange(1, len(ell) + 1)\nax2.bar(ind, ell, align='center', alpha=0.5)\nax2.plot(np.cumsum(ell))\nax2.set_xlabel('Number of components')\nax2.set_ylabel('Cumulative explained variance')\nax1.set_title('Pareto Plot')\n\nfor x, y in zip(ind, np.cumsum(ell)):\n    label = \"{:.2f}\".format(y)\n    if float(label) >= 0.79:\n        plt.annotate(\"cumulative explained variance: \" + label + \"\\n\" +\n                     \"Number of PC: \" + str(x),  # this is the text\n                     (x, y),  # this is the point to label\n                     textcoords='figure fraction',  # how to position the text\n                     xytext=(.8, 0.5),  # distance from text to points (x,y)\n                     arrowprops=dict(facecolor='black', shrink=0.1),\n                     horizontalalignment='left',\n                     verticalalignment='bottom',\n                     ha='center')  # horizontal alignment can be left, right or center\n        NUMBER_OF_PCs = x # for further use\n        break","bb28ec6e":"Z_train = Z_train[:, :NUMBER_OF_PCs]\nZ_test = Z_test[:, :NUMBER_OF_PCs]","00ede805":"Model_Score = []\n\n# Logistic Regression\nclassifier_lr = LogisticRegression(random_state=0)\nscores = cross_val_score(classifier_lr, Z_train, y_train, cv=10, scoring='accuracy')\nlr_train_score_mean = scores.mean()\nlr_train_score_std = scores.std()\nclassifier_lr.fit(Z_train, y_train)\nmodel_name = 'Logistic Regression'\nlr_test_score = classifier_lr.score(Z_test, y_test)\n\nscore = list((model_name, lr_train_score_mean.round(4), lr_train_score_std.round(4), lr_test_score.round(4)))\nModel_Score.append(score)","fffc3934":"# Making the confusion matrix\ny_predicted_lr = classifier_lr.predict(Z_test)\ncm_lr = confusion_matrix(y_test, y_predicted_lr)\n\nprint(cm_lr)","80580653":"# SVC\ndef svm(degree, kernel, gamma, x_train, x_test, train_label, test_label):\n    if kernel == 'poly':\n        support_vector_machine = SVC(kernel='poly', degree=degree, random_state=0)\n        cv_score = cross_val_score(support_vector_machine, x_train, train_label, cv=10)\n        svm_train_score_mean = cv_score.mean()\n        svm_train_score_std = cv_score.std()\n        support_vector_machine.fit(x_train, train_label)\n        svm_test_score = support_vector_machine.score(x_test, test_label)\n        name = 'SVM with ' + str(degree) + '-degree polynomial kernel'\n    elif kernel == 'rbf':\n        support_vector_machine = SVC(kernel='rbf', gamma=gamma, random_state=0)\n        cv_score = cross_val_score(support_vector_machine, x_train, train_label, cv=10)\n        svm_train_score_mean = cv_score.mean()\n        svm_train_score_std = cv_score.std()\n        support_vector_machine.fit(x_train, train_label)\n        svm_test_score = support_vector_machine.score(x_test, test_label)\n        name = 'SVM with rbf kernel and ' + str(gamma) + ' coefficient'\n    else:\n        support_vector_machine = SVC(kernel='sigmoid', gamma=gamma, random_state=0)\n        cv_score = cross_val_score(support_vector_machine, x_train, train_label, cv=10)\n        svm_train_score_mean = cv_score.mean()\n        svm_train_score_std = cv_score.std()\n        support_vector_machine.fit(x_train, train_label)\n        svm_test_score = support_vector_machine.score(x_test, test_label)\n        name = 'SVM with sigmoid kernel and ' + str(gamma) + ' coefficient'\n    return support_vector_machine, list((name, svm_train_score_mean.round(4), svm_train_score_std.round(4),\n                                         svm_test_score.round(4)))\n\nsvc_models = [\n    [None, 'rbf', 'scale'], [None, 'rbf', 'auto'], [None, 'sigmoid', 'scale'], [None, 'sigmoid', 'auto'],\n    [1, 'poly', None], [2, 'poly', None], [3, 'poly', 'None'], [4, 'poly', 'None'], [5, 'poly', 'None']\n]\n\nfor i, x in enumerate(svc_models):\n    _, score = svm(svc_models[i][0], svc_models[i][1], svc_models[i][2], Z_train, Z_test, y_train, y_test)\n    Model_Score.append(score)","86e98b34":"# Random Forest Classification\nacc_score = []\nstd_score = []\nmax_rf_ne = 50\nfor ne in range(1, max_rf_ne):\n    classifier_rf = RandomForestClassifier(n_estimators=ne, random_state=0)\n    scores = cross_val_score(classifier_rf, Z_train, y_train, cv=10, scoring='accuracy')\n    rf_train_score_mean = scores.mean()\n    rf_train_score_std = scores.std()\n    acc_score.append(rf_train_score_mean)\n    std_score.append(rf_train_score_std)\n\nbest_rf_acc = max(acc_score)\nbest_rf_ne = acc_score.index(max(acc_score))\n\nclassifier_rf = RandomForestClassifier(n_estimators=best_rf_ne, random_state=0)\nclassifier_rf.fit(Z_train, y_train)\nrf_test_score = classifier_rf.score(Z_test, y_test)\n\nf, ax = plt.subplots()\nax.plot(range(1, max_rf_ne), acc_score, marker='o')\nax.set_title('accuracy')\n\nmodel_name = 'Random Forest with ' + str(best_rf_ne) + ' estimators'\n\nscore = list((model_name, rf_train_score_mean.round(4), rf_train_score_std.round(4), rf_test_score.round(4)))\nModel_Score.append(score)","9aad9a6c":"# ANN\ndef build_classifier():\n    classifier = Sequential()\n    classifier.add(Dense(units=16, kernel_initializer='uniform', activation='relu', input_dim=NUMBER_OF_PCs))\n    classifier.add(Dense(units=16, kernel_initializer='uniform', activation='relu'))\n    classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n    classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return classifier\n\n\nclassifier_ann = KerasClassifier(build_fn=build_classifier, batch_size=16, epochs=100, verbose=0)\naccuracies = cross_val_score(estimator=classifier_ann, X=Z_train, y=y_train, cv=10)\nann_score = accuracies.mean()\nann_std = accuracies.std()\n\nclassifier_ann.fit(Z_train, y_train)\nann_test_score = classifier_ann.score(Z_test, y_test)\n\nmodel_name = 'Two-hidden-layer ANN with 25 batch size and 100 epochs'\nscore = list((model_name, np.round(ann_score, 4), np.round(ann_std, 4), np.round(ann_test_score, 4)))\nModel_Score.append(score)","6571d29e":"# Max voting\ndef max_voting(estimators, x_train, x_test, train_label, test_label):\n    mv_classifier = VotingClassifier(estimators=estimators, voting='hard')\n    cv_score = cross_val_score(mv_classifier, x_train, train_label, cv=10)\n    mv_train_score_mean = cv_score.mean()\n    mv_train_score_std = cv_score.std()\n    mv_classifier.fit(x_train, train_label)\n    name = 'Max voting'  # Add the name of the base models (estimators)\n    mv_test_score = mv_classifier.score(x_test, test_label)\n    return mv_classifier, list((name, mv_train_score_mean.round(4), mv_train_score_std.round(4),\n                                mv_test_score.round(4)))\n\nmodel_1 = RandomForestClassifier(n_estimators=best_rf_ne)\nmodel_2 = LogisticRegression(random_state=0)\nmodel_3 = SVC(kernel='poly', degree=2)\nmodel_4 = SVC(kernel='poly', degree=3)\nmodel_5 = SVC(kernel='poly', degree=4)\nmodel_6 = SVC(kernel='poly', degree=5)\nmodel_7 = SVC(kernel='rbf', gamma='scale')\nmodel_8 = SVC(kernel='rbf', gamma='auto')\nmodel_9 = SVC(kernel='sigmoid', gamma='scale')\nmodel_10 = SVC(kernel='sigmoid', gamma='auto')\n\n# version 10\nclassifier_mv, score = max_voting([('rf', model_1), ('lr', model_2), ('SVM_2', model_3), ('SVM_3', model_4),\n                                   ('SVM_4', model_5), ('SVM_5', model_6), ('SVM_rbf_scale', model_7),\n                                   ('SVM_rbf_auto', model_8), ('SVM_sigmoid_scale', model_9),\n                                   ('SVM_sigmoid_auto', model_10)], Z_train, Z_test, y_train, y_test)\n\nModel_Score.append(score)","f829cf92":"# Results\nModel_Score = pd.DataFrame(Model_Score, columns=['Model', 'Train Score Average', 'Train Score SD', 'Test Score'])\nModel_Score = Model_Score.sort_values(by=['Test Score'], ascending=False)","b1063336":"Model_Score","75262c54":"''''' #TO DO \nparameters = {'C':[1, 10, 100]}\nsvc = SVC(kernel='poly', degree=3, random_state=0)\nclf = GridSearchCV(svc, parameters)\nclf.fit(Z_train, y_train)\n\nprint(\"Best parameters set found on development set:\")\nprint()\nprint(clf.best_params_)\nprint()\nprint(\"Grid scores on development set:\")\nprint()\nmeans = clf.cv_results_['mean_test_score']\nstds = clf.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, clf.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n          % (mean, std * 2, params))\nprint()\n\nprint(\"Detailed classification report:\")\nprint()\nprint(\"The model is trained on the full development set.\")\nprint(\"The scores are computed on the full evaluation set.\")\nprint()\ny_true, y_pred = y_test, clf.predict(Z_test)\nprint(classification_report(y_true, y_pred))\nprint()\n'''''","2689fa1c":"<a id=\"9\"><\/a> <br>\n## Label Encoding\nMany ML algorithms cannot handle categorical variables without further manipulation. Therefore, as we mentioned earlier, one of the other tasks in the data preprocessing is label encoding. Label encoding can be easily seen as an approach to convert the categorical values into suitable numeric values in a way that computers can understand. Two of the frequently used approaches to encode the labels in python are called LabelEncoder and OneHotEncoder. While LabelEncoder considers one attribute for each unique category (class), OneHotEncoder assigns a unique integer to each category (class). The below image is retrieved from [here](https:\/\/miro.medium.com\/max\/875\/0*T5jaa2othYfXZX9W.) and illustrates two different approaches very well. Although there might be some issues with OneHotEncoder in some problems, there is no difference to use which approach in our problem since it is a binary classification. With that being said, we used LabelEncoder in the below code.","d0bc7155":"<a id=\"15\"><\/a> <br>\n## Max Voting","e407c48c":"<a id=\"5\"><\/a> <br>\n## Outliers\nThis plot shows that there are outliers in some of the features that we need to take care of them (take a look at the area_mean box plot and recall its std). There are two frequently used methods to detect outliers: 1) Interquartile Range(IQR) Method and 2)Z Score method. We are going to use the first method here, but more information on the second method can be found in [here](https:\/\/medium.com\/analytics-vidhya\/outlier-treatment-9bbe87384d02). IQR method makes an acceptable range for data points value and considers those points that fall outside this range as outliers. The formula for this range is calculated as below:\n\n\\begin{equation}\nIQR = Q3 - Q1 \\\\\nUW = Q3 + 1.5 * IQR \\\\\nLW = Q1 - 1.5 * IQR \\\\\n\\end{equation}\n\nwhere $Q3$, $Q1$, $UW$, and $LW$ are the third quantile, the first quantile, upper whisker, and lower whisker which are the acceptable upper and lower limits, respectively. I like to call the points above the upper whisker as the upper outliers and the points below the lower whisker as lower outliers from now on.\n![](https:\/\/miro.medium.com\/max\/520\/1*SNZCse_n6-hau15Gn01RLA.png)\n\nThe below code detects the outliers in each attribute.","119891a7":"We can also take a look at the confusion matrix of each model by running the below code.","b1f3895e":"<a id=\"16\"><\/a> <br>\n## Results","4888d1a6":"<a id=\"12\"><\/a> <br>\n## Support Vector Classifier","a1e79357":"<a id=\"7\"><\/a> <br>\n## Correlation Matrix\nAs it is clear, there are linear relationships among some of the attributes. This may cause a problem called the multicollinearity issue. To check whether this problem exists or not, we can always check the correlation among attributes. The below code shows the correlation matrix. Pearson method is used to calculate the correlation.","b994de41":"In the below code, among two highly correlated attributes, we kept one and get rid of the other.","f662f923":"<a id=\"2\"><\/a> <br>\n## Preprocessing Introduction\nThe first step is to preprocess the data. Commonly, there are below steps in data preprocessing in ML:\n1. Taking care of missing values\n2. Taking care of outliers\n3. Taking care of duplicate values\n4. Feature aggregation\n5. Encoding categorical data\n6. Splitting the dataset into train and test set\n7. Dimensionality reduction\n8. Feature scaling\n\nBased on what we have, there is no need to do step 4, but the rest is required and will be discussed step-by-step. First, we take a look at the first 6 rows of the data. ","0c201448":"<a id=\"3\"><\/a> <br>\n## Summary Statistics\nThe below code shows a table consists of important statistics to know the data better. ","e04811d6":"<a id=\"1\"><\/a> <br>\n## Import Libraries\nFirst, we import the required libraries and import the dataset.","bd6d56df":"<a id=\"4\"><\/a> <br>\n## Box Plots","817e6b81":"<a id=\"11\"><\/a> <br>\n## Training Predictive Models\nNow, data preprocessing is done and we can go for the next part which is training prediction models. Here, we used Logistic Regression, SVM, Artificial Neural Network, and two ensemble learning methods known as Random Forest and Max Voting. To get a relevant measure of bias and variance and compare different models more reliably, we have applied K-fold Cross-Validation for each model as well. It is worthy to note that each model's hyperparameters need to be tuned, but here, we have done hyperparameter tuning just for Random Forest and SVM. To tune the Random Forest hyperparameter, we tried different values for the number of trees in the forest between [1, 50], and choose the value which has the highest 10-fold Cross-Validated accuracy score. Also, for tuning SVM's hyperparameter, we have tried different configurations of the hyperparameters to get the best result.\n\nThe results of the different models will be saved in a variable called **Model_Score** to compare at the end.","0f48c62c":"When we take look at the std values, we realize that the std values for some features are high (like area_mean). This may be a sign of existing outliers. To check this observation, we can look at the boxplot of each attribute. For illustration purposes, the boxplots of 10 attributes are displayed. If you want to take a look at the box plot for all of the attributes, just place **data** instead of **data_visualization**.","ef5bdc55":"Another way of indicating the presence of outliers in the data is the **skewness** value which explains the extent to which the data is normally distributed. This value should normally fall in [-1, +1] range. Any major derivation from the upper limit (+1) shows a right-skewed distribution as well as the presence of the upper outliers, and any major derivation from the lower limit (-1) shows a left-skewed distribution as well as the presence of the lower outliers. For example, as the boxplot shows, upper outliers exist in the area_se attribute and its skewness value is way above 1.","b34c8eaf":"As it is shown, the last column consists of missing values, and it is needed to be delated. We also split attributes from the label and save them in two different variables.","58d11069":"The final results are acceptable. Generally, the difference between train and test results could be due to overfitting, lack of data, and so on. However, in our case, this difference is because of using PCA in a small dataset. Remember that our chosen number of PCs explains near 79% of the variance and this could have an effect in training ML models on the small datasets. As I mentioned at the beginning of this notebook, **we want to use PCA just for practice here **. ","a7fab4c3":"<a id=\"13\"><\/a> <br>\n## Random Forest","15519517":"<a id=\"14\"><\/a> <br>\n## ANN","02721c65":"After replacing outliers with the median, we can take a look at the skewness value of each attribute. We can see now that most of the attributes have a skewness value between 1 and -1.  ","e1bda23c":"<a id=\"12\"><\/a> <br>\n## Logistic Regression","1267ddf1":"To choose the number of Principal Components (PCs) to consider, we can take advantage of the explained variance. Explained variance is the proportion of variability accounted by each PC. As we mentioned before, the variance of the $i^{\\text{th}}$ PC is equal to $\\lambda_{i}$. Consequently, the amount of variance explained by the $i^{\\text{th}}$ principal component is equal to:\n\n\\begin{equation} \\label{Explained_Var_1}\nL_{i} = \\frac{\\lambda_{i}}{\\sum_{j=1}^{P}{\\lambda_{j}}}\n\\end{equation}\n\nWhere P is the total number of attributes or PCs. After that, we can use the **elbow method** (similar to what we use in choosing the number of clusters in clustering) and scree plot to choose the best number of PCs.  **elbow** is the point that after which the distortion\/inertia starts decreasing in a linear fashion.\n\nBelow code first, calculate eigenvalues and explained variance and then, plot both scree plot and Pareto plot. By looking at the scree plot, 6 PCs can be selected as the optimal number of the PCs and 79% of the variance is explained by the first six PCs.","a2d4dfcc":"## Introduction\nCancer is one of the most common diseases all around the world that takes the lives of many people every year. Among all types of cancer, breast cancer has become frequent among women and it causes the death of many of them. According to [here](https:\/\/canceratlas.cancer.org\/the-burden\/breast-cancer\/), in 140 out of 184 major countries in the world, this type of cancer has been diagnosed more than other types of cancers, and in 101 major countries, this is the most common cause of cancer death. Although breast cancer is the most prevailing type of cancer responsible for a large number of deaths every year, if it is diagnosed in an early stage, it can be cured.\n\nMachine Learning (ML) is a subfield of Artificial Intelligence (AI) in which we use algorithms and computational statistics and let the machines and computers learn automatically from data without explicitly programming. For example, rather than writing many codes to detect a dog or cat with lots of if-else and loops commands, we provide a sufficient number of dog and cat pictures along with their labels and let the machine learn how to identify them in a picture. In the past, due to the lack of data and computation power of computers, it was not possible to do so. However, in recent years, with the growth of technology, lots of data is produced and computer computation power is improved so much. \n\nWith lots of medical data and powerful tools to train an ML model, some started using ML and data analytics tools in the healthcare industry. One of the interesting subjects in analyzing medical data is the diagnosis of cancer from medical images. \n\nNevertheless, some models require a great amount of time to be trained and cannot be used in the real world due to its complexity. Lots of researches have been conducted by many people to introduce a better, faster, and more comprehensive way to analyze the data and extract useful information. Nonetheless, high dimensional data is a problem for most of the data mining algorithms, and data dimension reduction is needed to improve the speed of the training. **Although this dataset is relatively small and it is not needed to use dimension reduction techniques, we are going to use Principal Component Analysis (PCA) in this notebook just for practice.******","d749b909":"## Contents\n* [Import Libraries](#1)\n* [Preprocessing Introduction](#2)\n* [Summary Statistics](#3)\n* [Box Plots](#4)\n* [Outliers](#5)\n* [Pair Plot](#6)\n* [Correlation Matrix](#7)\n* [Multicollinearity Problem](#8)\n* [Label Encoding](#9)\n* [PCA](#10)\n* [Training Predictive Models](#11)\n* [Logistic Regression](#12)\n* [Random Forest](#13)\n* [ANN](#14) \n* [Max Voting](#15) \n* [Results](#16) ","467c9d6f":"There are different ways to deal with outliers after detecting them. One of the efficient ways is to change the values of the outliers by the median. The below code does this.","80f1dfca":"<a id=\"6\"><\/a> <br>\n## Pair Plot\nOne of the best plots to check the relationships between attributes and their distributions is pair plot. The below code shows the pair plot filtered by labels which are diagnoses.","30a3611d":"<a id=\"8\"><\/a> <br>\n## Multicollinearity Problem\nThe correlation between some attributes, like radius_mean and perimeter_mean, is high and may cause the multicollinearity problem. To prevent this problem, between two highly correlated attributes, we can keep one and get rid of the other one. The below code shows those attributes that their correlation is more than 0.9.","c2900687":"<a id=\"10\"><\/a> <br>\n## PCA\nIn the ML area, data is the life-giving fuel of every model. Nowadays, we are dealing with some complex data, i.e. multi-dimensional data, which has lots of attributes and observations. In these kinds of data, there are too many variables to consider and it may affect the training of some ML models. Moreover, visualization and interpolation of these datasets are hard. To avoid these problems, it is better to use dimension reduction techniques before applying different ML models. Dimension reduction techniques reduce the feature space, so, it eases the processing time. Also, when we are dealing with fewer attributes, it is easier to make different plots and interpret them. There are many such techniques in the literature, but most of them fall in into two below categories:\n\n1. **Feature selection**: In which we reduce the feature space dimension by eliminating some attributes.\n2. **Feature extraction**: These techniques try to create new independent variables by combining old variables.\n\nHowever, the first technique has a major disadvantage which is we may lose some information by dropping any attribute. \n\nPCA is a feature extraction technique in which we use a linear transformation of feature vectors into uncorrelated vectors. \n\nWe note our data matrix by $X_{n*p}$, where $n$ is the number of observations, i.e. rows, and $p$ is the number of attributes, i.e. columns.\nAssume we have centered our data matrix so that we make sure every attribute has a zero mean, and they contribute toward computing variance evenly. We call the centered or adjusted data matrix $Y_{n*p}$. Then we should compute the covariance matrix $C_{p*p}$ as follows:\n\n\\begin{equation} \\label{cov_mat_1}\nC = \\frac{1}{n-1} Y^{\\prime} Y\n\\end{equation}\n\nIn the next step, we use Eigen-decomposition to calculate the eigenvectors and eigenvalues of the covariance matrix as follow:\n\n\\begin{equation} \\label{cov_mat_2}\nC = A \\Delta A^{\\prime} = \\sum_{j=1}^{p} \\lambda_{j} a_{j} a^{\\prime}_{j}\n\\end{equation},\n\nwhere $A$ is a $p*p$ matrix of eigenvectors, $a_{j}$ is the $j^{\\text{th}}$ eigenvector of $S$, and $\\Delta_{j}$ is the $j^{\\text{th}}$ eigenvalue.\n\nFinally, we can project our $n*p$ dataset into the new dimension by computing $Z$ as follow:\n\n\\begin{equation} \\label{cov_mat_3}\nZ = Y A\n\\end{equation}\n\nWhere var($Z_{j}$) = $\\lambda_{j}$ and corr($Z_{i}$, $Z_{j}$) = 0 if $i$ is not equal to $j$. But, NO WORRY!!! All of the above math is handled by the scikit learn library. The below code shows the implementation of the PCA algorithm in python using scikit learn library. It is also worthy to note that the PCA algorithm should be fitted only on the training data. Therefore, it is required to split data into training and test set before fitting this algorithm.","a3f8a132":"Breast Cancer Wisconsin (Diagnosis) is a well-known dataset in healthcare on which lots of research has been conducted. This dataset consists of features that are extracted from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe the cell nuclei features present in the image. These features are as follows: (1) radius, (2) texture, (3) perimeter, (4) area, (5) smoothness, (6) compactness, (7) concavity, (8) concave points, (9) symmetry, and (10) fractal dimension.\nEach feature has three more information, namely mean, standard error, and worst which is the mean of the three larges values. Therefore, the dataset includes 30 tumor features extracted from an FNA image, a patient ID, and a response variable.\n\nThis data is taken from the UCI repository and includes 569 observations, 212 Malignant, and 357 Benign.","5ec8e129":"As it is clear, the last column consists of NaN values. To check whether there are more missing values in other attributes as well as how many missing values exist in the last columns, we can use **info** method in pandas."}}