{"cell_type":{"f7c48e9f":"code","137cbece":"code","b586a4ef":"code","1d0f5a80":"code","50a914c8":"code","db5ccd37":"code","0ff30e2b":"code","f193f54c":"code","bb0f3328":"code","69d89d42":"code","efb4f73d":"code","2d6d8fa2":"code","f8481645":"code","db020117":"code","8c6cad52":"code","590fa23c":"code","a55d24c7":"code","91a67d74":"code","79885de5":"code","5886ca06":"code","01c99bd4":"code","f492b520":"code","9cebe51f":"code","c2c7d3ea":"code","3e3d60b8":"code","88afe730":"code","c0aab089":"code","4e7d81d1":"code","68a2c938":"code","cbf35ac1":"code","631f503d":"code","126e6088":"code","4d2ebda8":"markdown","c9b22933":"markdown","ea89be70":"markdown","0e5fff7d":"markdown","151f282e":"markdown","221502bb":"markdown","aba439ca":"markdown","ec81bcf5":"markdown","08e4a62d":"markdown","8b3d4414":"markdown","997bdee3":"markdown"},"source":{"f7c48e9f":"#Importing Required Libaries \n\n!pip install dice_ml\n!pip install openpyxl\n\nimport pandas as pd\nimport random\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import classification_report\nimport shap\nimport dice_ml\nimport lime\n\nrandom.seed(123)  #Reproducability of the results","137cbece":"#Loading the dataset\n\ndf=pd.read_excel(\"..\/input\/can-you-buy-a-new-car\/Data.xlsx\")\ndel df[\"Number\"]\ndisplay(df)","b586a4ef":"print(\"the names of the features in the dataset are \",list(df.columns))\nprint(\"Size of the data set is \",df.shape)","1d0f5a80":"#sns.pairplot(df,hue=\"decision\")\n#plt.savefig(\"1.png\",bbox_inches=\"tight\",dpi=500)\n\ndef pie_chart(values,label,t):\n    print(values)\n    plt.figure(figsize=(6,6))\n    plt.title(t,fontsize=14)\n    plt.pie(values,labels=label,shadow = True,autopct='%1.1f%%',textprops={'fontsize': 12})\n    #plt.legend(loc='best')\n    plt.show()\n    \ndef bar_plot(name):\n    plt.figure(figsize=(6,6))\n    sns.set_style('ticks')\n    sns.set(font_scale = 1)\n    plt.title(f\"{name} data distribution by decision  \",fontsize=12)\n    sns.histplot(data=df, x=name, hue=\"decision\",multiple=\"dodge\", shrink=.8)\n    if (name==\"income\" or name==\"Region\" or name==\"Education_level\"):\n        plt.xticks(rotation=90)\n    plt.show()\n    \n\nfor name in df.columns:\n    val=df[name].value_counts() #Converting a nimerical variable into sum of the categorocal variable\n    val=val[0:10] #Up to the 10 labels\n    names=val.index.tolist()\n    if (name !=\"decision\"):\n        pie_chart(val,names,f\"Precentage of data categories according to {name}\")\n        bar_plot(name)\n","50a914c8":"#%% Model Building (logistic Regression)\n# Data Preprocessing (Important)\n### Here all the columns have tobe converted into the categorical format.There is no any continous data present here.All the data is taken as the answers for multiple category questions.\n\nfor name in df.columns:\n    df[name]=df[name].astype(\"category\")\n\ndisplay(df.describe())","db5ccd37":"# ## ML Algorithms cannot work with the string type data.So we have to convert all the data into numerical format\n# \n# 1. For example (female=0,male=1)\n# 2. Health(good=0,bad=1).....\n\ndf_num = pd.DataFrame(columns=df.columns)\nfor name in df.columns:\n    df_num[name]=df[name].cat.codes\n\ndisplay(df_num.head(10))   ","0ff30e2b":"#Checking wheather the data is balanced\npie_chart(df[\"decision\"].value_counts(),df[\"decision\"].value_counts().index.tolist(),\"Precentage of data categories according to Decision\")\n","f193f54c":"##Seems like the data is not balanced\n#Random Over samplong to make a balanced dataset\n\nfrom imblearn.over_sampling import RandomOverSampler \nros = RandomOverSampler(random_state=0)\nR,S= ros.fit_resample(df_num.iloc[:,0:8],df_num[\"decision\"])\n\ndf_numpy=[R,S]\ndf_balanced=pd.concat(df_numpy,axis=1)\n\npie_chart(df_balanced[\"decision\"].value_counts(),df[\"decision\"].value_counts().index.tolist(),\"Precentage of data categories according to Decision\")","bb0f3328":"#Extracting featues and target columns\n\ndata_features=df_balanced.iloc[:,0:8]\ndata_targets=df_balanced[\"decision\"]\n\n#Creating two datasets (1 for making the model(Training dataset),Other one for test and validate the performance of the model(Testing dataset))\n# \n# ### There are 372 records in the dataset\n#     80% of data for training set (372*0.8 ~ 297)\n#     Then rest 20% of data for testing set (372*0.2 ~ 75)\n\ntrain_x,test_x,train_y,test_y=train_test_split(data_features,data_targets,test_size=0.2)\nprint(train_x.shape,test_x.shape,train_y.shape,test_y.shape)\n","69d89d42":"#Training logistic regression model\n\nmatplotlib.rc_file_defaults()\nmodel_lr= LogisticRegression(random_state=42)\nmodel_lr.fit(train_x,train_y)\npredictions_lr=model_lr.predict(test_x)\nprint(model_lr.get_params())","efb4f73d":"print(\"LR-Training Accuracy score: \"+str(round(accuracy_score(train_y,model_lr.predict(train_x)),4)))\nprint(\"LR-Testing Accuracy score: \"+str(round(accuracy_score(test_y,predictions_lr),4)))\n\nplot_confusion_matrix(model_lr,test_x,test_y)\nprint(classification_report(test_y,predictions_lr,target_names=[\"Yes\",\"No\"]))","2d6d8fa2":"model_coff=abs(model_lr.coef_[0])\nfeature=data_features.columns\n\nimportant_features=pd.DataFrame(list(zip(feature,model_coff)),columns=[\"Feature\",\"Importance\"])\ndisplay(important_features)\n\nplt.figure(figsize=(8,6))\nplt.bar([i for i in data_features.columns], [abs(j) for j in model_coff])\nplt.title(\"Importance of features (Male and Female)\")\nplt.xticks(rotation=90)\nplt.show()","f8481645":"#Lime Visualizations\n\nfrom lime import lime_tabular\nexplainer = lime_tabular.LimeTabularExplainer(np.array(train_x), feature_names=train_x.columns)\n\nidx=10\nexplanation = explainer.explain_instance(np.array(test_x)[idx], model_lr.predict_proba, num_features=5)\nexplanation.show_in_notebook()","db020117":"#Female Model\n\ndata=df_balanced.loc[df_balanced[\"gender\"]==0]\ndata_features=data.iloc[:,0:8]\ndata_targets=data[\"decision\"]\n\ntrain_x,test_x,train_y,test_y=train_test_split(data_features,data_targets,test_size=0.2)\nprint(train_x.shape,test_x.shape,train_y.shape,test_y.shape)","8c6cad52":"model_lr_female= LogisticRegression(random_state=42)\nmodel_lr_female.fit(train_x,train_y)\npredictions_lr_female=model_lr_female.predict(test_x)\nprint(model_lr_female.get_params())","590fa23c":"print(\"LR_Female-Training Accuracy score: \"+str(round(accuracy_score(train_y,model_lr_female.predict(train_x)),4)))\nprint(\"LR_Female-Testing Accuracy score: \"+str(round(accuracy_score(test_y,predictions_lr_female),4)))\n\nplot_confusion_matrix(model_lr_female,test_x,test_y)\nprint(classification_report(test_y,predictions_lr_female,target_names=[\"Yes\",\"No\"]))","a55d24c7":"model_coff=abs(model_lr_female.coef_[0])\n\nimportant_features=pd.DataFrame(list(zip(data_features.columns,model_coff)),columns=[\"Feature\",\"Importance\"])\ndisplay(important_features)\n\nplt.figure(figsize=(8,6))\nplt.bar([i for i in data_features.columns], [abs(j) for j in model_coff])\nplt.title(\"Importance of features (Female)\")\nplt.xticks(rotation=90)\nplt.show()","91a67d74":"model_lr_male= LogisticRegression(random_state=42)\nmodel_lr_male.fit(train_x,train_y)\npredictions_lr_male=model_lr_male.predict(test_x)\nprint(model_lr_male.get_params())","79885de5":"print(\"LR_Male-Training Accuracy score: \"+str(round(accuracy_score(train_y,model_lr_male.predict(train_x)),4)))\nprint(\"LR_Male-Testing Accuracy score: \"+str(round(accuracy_score(test_y,predictions_lr_male),4)))\n\nplot_confusion_matrix(model_lr_male,test_x,test_y)\nprint(classification_report(test_y,predictions_lr_male,target_names=[\"Yes\",\"No\"]))","5886ca06":"model_coff=abs(model_lr_male.coef_[0])\n\nimportant_features=pd.DataFrame(list(zip(data_features.columns,model_coff)),columns=[\"Feature\",\"Importance\"])\ndisplay(important_features)\n\nplt.figure(figsize=(8,6))\nplt.bar([i for i in data_features.columns], [abs(j) for j in model_coff])\nplt.title(\"Importance of features\")\nplt.xticks(rotation=90)\nplt.show()","01c99bd4":"#xgboost\nimport xgboost as xgb\nxgb_param={\n    \"max_depth\":3,\n    \"eta\":0.01,\n    \"silent\":0,\n    \"eval_metric\":\"auc\",\n    \"subsample\":0.8,\n    \"colsample_bytree\":0.8,\n    \"objective\":\"binary:logistic\",\n    \"seed\":0\n    }\n\ndTrain=xgb.DMatrix(train_x,train_y,feature_names=list(train_x.columns),enable_categorical=True)\ndTest=xgb.DMatrix(test_x,test_y,feature_names=list(test_x.columns),enable_categorical=True)\n\nEvals=[(dTrain,\"train\"),(dTest,\"dTest\")]\nmodel_xgb=xgb.train(params=xgb_param, dtrain=dTrain, num_boost_round=2000,\n              evals=Evals, maximize=True, \n              verbose_eval=50)\n","f492b520":"predictions_xgb_test=model_xgb.predict(dTest)\np_test=[0 if i<= 0.5 else 1 for i in predictions_xgb_test]\n\npredictions_xgb_train=model_xgb.predict(dTrain)\np_train=[0 if i<= 0.5 else 1 for i in predictions_xgb_train]\n\nprint(\"XGB-Training Accuracy score: \"+str(round(accuracy_score(train_y,p_train),4)))\nprint(\"XGB-Testing Accuracy score: \"+str(round(accuracy_score(test_y,p_test),4)))\n\n#plot_confusion_matrix(model_xgb,test_x,test_y)\nprint(classification_report(test_y,p_test,target_names=[\"Yes\",\"No\"]))","9cebe51f":"#Cat boost classifier\nimport catboost\nprint('catboost version:', catboost.__version__)\nfrom catboost import CatBoostClassifier \n\n\nros = RandomOverSampler(random_state=0)\nR,S= ros.fit_resample(df.iloc[:,0:8],df[\"decision\"])\n\ndf_numpy=[R,S]\ndf_add=pd.concat(df_numpy,axis=1)\n\n#pie_chart(df_add[\"decision\"].value_counts(),df[\"decision\"].value_counts().index.tolist(),\"Precentage of data categories according to Decision\")\n\n\nfeatures_names =list(df.columns[0:-1])\ndata_features=df_add.iloc[:,0:8]\ndata_targets=df_add[\"decision\"]\n\ntrain_x,test_x,train_y,test_y=train_test_split(data_features,data_targets,test_size=0.2)\n \nparams = {'iterations':5000,\n        'learning_rate':0.01,\n        'cat_features':features_names,\n        'depth':3,\n        'eval_metric':'AUC',\n        'verbose':200,\n        'od_type':\"Iter\", # overfit detector\n        'od_wait':500, # most recent best iteration to wait before stopping\n        'random_seed': 1\n          }\n\ncat_model = CatBoostClassifier(**params)\ncat_model.fit(train_x, train_y,   \n          eval_set=(test_x, test_y), \n          use_best_model=True, # True if we don't want to save trees created after iteration with the best validation score\n          plot=True);\n","c2c7d3ea":"pred_cat_model=cat_model.predict(test_x)\nprint(\"Cat Boost Classifier -Training Accuracy score: \"+str(round(accuracy_score(train_y,cat_model.predict(train_x)),4)))\nprint(\"Cat Boost Classifier -Testing Accuracy score: \"+str(round(accuracy_score(test_y,pred_cat_model),4)))\n\nplot_confusion_matrix(cat_model,test_x,test_y)\nprint(classification_report(test_y,pred_cat_model,target_names=[\"Yes\",\"No\"]))","3e3d60b8":"#%%Shap Visualizations\nfrom catboost import CatBoostClassifier, Pool\nshap_values = cat_model.get_feature_importance(Pool(train_x, label=train_y,cat_features=features_names) ,\n                                               type=\"ShapValues\")\nexpected_value = shap_values[0,-1]\nshap_values = shap_values[:,:-1]\n\nshap.initjs()\nindex=20\nshap.force_plot(expected_value, shap_values[index,:], train_x.iloc[index,:])\n","88afe730":"shap.summary_plot(shap_values,train_x)","c0aab089":"explainer = shap.TreeExplainer(cat_model)\nshap_values = explainer.shap_values(train_x)\n\n# visualize the training set predictions\nshap.force_plot(explainer.expected_value, shap_values,train_x)\n","4e7d81d1":"shap.summary_plot(shap_values, feature_names=train_x.columns, plot_type='bar')","68a2c938":"shap_sum = np.abs(shap_values).mean(axis=0)\nimportance_df = pd.DataFrame([train_x.columns.tolist(), shap_sum.tolist()]).T\nimportance_df.columns = ['column_name', 'shap_importance']\nimportance_df = importance_df.sort_values('shap_importance', ascending=False)\nimportance_df","cbf35ac1":"#%%Counter factuals\ndice_data = dice_ml.Data(dataframe=df_add,continuous_features=[],outcome_name='decision')\ndice_model= dice_ml.Model(model=cat_model, backend=\"sklearn\")\nexplainer = dice_ml.Dice(dice_data,dice_model, method=\"random\")\n\n#%%See Counterfactuals\ne1 = explainer.generate_counterfactuals(test_x[0:1], total_CFs=2, desired_class=\"opposite\")\ne1.visualize_as_dataframe(show_only_changes=True)\n","631f503d":"#%%Example 2\ne2 = explainer.generate_counterfactuals(test_x[4:5],\n                                  total_CFs=5,\n                                  desired_class=\"opposite\",\n                                  features_to_vary=[\"age\", \"Education_level\", \"Region\",\"Family_size\"]\n                                  )\ne2.visualize_as_dataframe(show_only_changes=True)","126e6088":"#%%\n# # Predicting Time\n# ## Input details of the head of household\n\ntry:\n    gen=int(input(\"Gender \\n Female - 0 \\n Male - 1   \\n \"))\n\n    age=int(input(\"Age \\n\" ))\n    if(age>50):\n        ag=3\n    elif(age>40):\n        ag=2\n    elif(age>31):\n        ag=1\n    else:\n        ag=0\n\n    health=int(input(\"Health Condition \\n Good - 0 \\n Poor - 1   \\n \"))\n\n    Education=int(input(\"Education Level \\n Higher Vocational - 0 \\n Junior School - 1 \\n PHD - 2 \\n Technical School - 3 \\n high school - 4 \\n postgraduate - 5 \\n primary school - 6 \\n undergraduate - 7 \\n\"))\n\n    si=int(input(\"Family Size \\n\" ))\n    if(si>=7):\n        fam=2\n    elif(si>=4):\n        fam=1\n    elif(si>=2):\n        fam=0\n    elif(si==1):\n        fam=3\n\n\n    inco=int(input(\"Income \\n\" ))\n    if(inco>=500):\n        income=3\n    elif(inco>=100):\n        income=0\n    elif(inco>=50):\n        income=4\n    elif(inco>30):\n        income=2\n    elif(inco>=16):\n        income=1\n    elif(inco>=9):\n        income=6\n    elif(inco<8):\n        income=6\n    #print(income)\n\n    risk=int(input(\"Risk Level \\n A value between 1 and 10 \\n\" ))\n\n\n    keys=df.Region.unique()\n    values=df_num.Region.unique()\n    dict_Region=dict(zip(keys,values))\n    dict_Region\n    rg=input(\"Input the name of the Region, you belongs to \\n\" )\n    region=dict_Region[rg]\n\n\n    input_array=np.array([[gen,ag,health,Education,fam,income,risk,region]])\n    ans=model_lr.predict(input_array)\n\n    if(ans==0):\n        print(\"\\n\\n Prediction = Can buy a new Car - Positive Output\")\n    if(ans==1):\n        print(\"\\n\\n Prediction = You Cann't buy a new car- Negative Output\")\n\nexcept ValueError:\n  print(\"Invalid Input \\n\\n\")\n","4d2ebda8":"# Can You Afford to Buy a New Car?\n\nPrepared by **P.M Ranasinghe** \n\n\n**The dataset contains the information about the people affordability to buy a new car in China.**<br>\nDiscuss how the factors including age, education background, family size, living region, income, health condition influence the decision to spend money on buying a new car.\n<br>Forcus more on how the gender affect the final outcome","c9b22933":"## Model Building","ea89be70":"### Training complex ML models on complete dataset\n\n**xgboost**","0e5fff7d":"<img src=\"https:\/\/thumbs.dreamstime.com\/b\/car-hand-model-dark-background-44846742.jpg\">","151f282e":"### Predicting Time","221502bb":"<img src=\"https:\/\/www.mathworks.com\/discovery\/deep-learning\/_jcr_content\/mainParsys\/band_2123350969_copy_1983242569\/mainParsys\/columns_1635259577\/1\/columns_1433445209_c_860958964\/2\/thumbnail_copy.adapt.full.medium.jpg\/1621572640991.jpg\">","aba439ca":"**Cat boost classifier**","ec81bcf5":"### Considering Both Male and Female Household heads","08e4a62d":"### Considering Only Male Household heads","8b3d4414":"## Data Exploration","997bdee3":"### Considering Only Female Household heads"}}