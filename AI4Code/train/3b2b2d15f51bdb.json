{"cell_type":{"80c970d3":"code","47eab854":"code","44ed1d5a":"code","6496b945":"code","6b720087":"code","4912826f":"code","a931bbac":"code","1dd6576a":"code","05375531":"code","de24b175":"code","ce2f617d":"code","739379ac":"code","725d266c":"code","19a50ab9":"code","b6a060f2":"code","58f10baa":"code","8fe14f09":"code","911e17a1":"code","78c7cb21":"code","3a9846fd":"code","4a64abfb":"code","66bdb892":"code","bf51e978":"code","34f9c286":"code","b72086ba":"code","4fa71e61":"code","fa228cc9":"code","92adfce1":"code","394817c0":"code","b0fd6e67":"code","569db37f":"code","bfeb9b03":"code","707bca13":"code","9cc131a0":"code","553166d2":"code","bf77f000":"code","e14979f5":"code","76ed9454":"code","d7afb349":"code","695a8c7e":"code","f1be9e63":"code","cd26e1ed":"code","07fcfde5":"code","0ca9d26c":"markdown","70fcb84c":"markdown","2fcf57d0":"markdown","44287374":"markdown","311ea0f3":"markdown","d090a25a":"markdown","d82a0fef":"markdown","3a706afe":"markdown","7d779196":"markdown","1433ff83":"markdown","5e48ee75":"markdown"},"source":{"80c970d3":"import pandas as pd\nimport numpy as np\nimport datetime\nfrom wordcloud import WordCloud\n\ndata = pd.read_csv('\/kaggle\/input\/ecommerce-data\/data.csv',encoding=\"ISO-8859-1\")\n\ndata = data[~data['CustomerID'].isna()].reset_index(drop=True)\nprint (len(data))\nprint (data.keys())\nimport matplotlib.pyplot as plt\ndata['TotalRevenue'] = data['Quantity']*data['UnitPrice']\/1#interpretation\ndata.iloc[5000:5005]","47eab854":"data['Month'] = data['InvoiceDate'].apply(lambda x : x.split('\/')[0]).astype(np.uint8)\ndata['Day'] = data['InvoiceDate'].apply(lambda x : x.split('\/')[1]).astype(np.uint8)\ndata['Year'] = data['InvoiceDate'].apply(lambda x : x.split('\/')[2].split(' ')[0]).astype(np.uint16)\ndata['Time'] = data['InvoiceDate'].apply(lambda x : x.split('\/')[2].split(' ')[1])\ndata['Hour']= data['Time'].apply(lambda x : x.split(':')[0]).astype(np.uint8)\ndata['Min']= data['Time'].apply(lambda x : x.split(':')[1]).astype(np.uint8)\ndata['datetime'] = list(map(lambda x : datetime.date(x[0],x[1],x[2]),data[['Year','Month','Day']].values) )\ndata['Day_week'] = list(map(lambda x :x.weekday(),data['datetime'].values)) # datetime.date(2017,12,25)\ndata['Day_week'] = data['Day_week'].map({0:'1mon',1:'2tue',2:'3wed',3:'4thur',4:'5fri',5:'6sat',6:'0sun'})\ndata = data.sort_values('datetime')\nprint (data.head())","44ed1d5a":"temp = data.groupby('CustomerID')['datetime'].apply(np.unique)\ndist = list(map(lambda x : x.days,np.concatenate(list(map(lambda x : x[1:]-x[:-1],temp)))))\nLambda =  1.\/np.mean(dist)\nx= np.linspace(0,360,1000)\nplt.plot(x,Lambda*np.exp(-Lambda*x),label='estimated exp distribution, mean = %s'%int(1\/Lambda))\nplt.hist(dist,bins=range(0,361,30),normed=1)\nplt.xticks(range(0,360,30))\nplt.title('Time between orders',fontsize=12,fontweight='bold')\nplt.xlabel('Time between orders',fontsize=12,fontweight='bold')\nplt.ylabel('Probability Density',fontsize=12,fontweight='bold')\nplt.legend()\nplt.show()","6496b945":"\n#exploratory statistics\ntemp  = data.groupby(['Month'])['TotalRevenue'].apply(np.sum)\nplt.plot(temp);\nplt.ylim(0,1.1*max(temp.values))\nplt.ylabel('Total revenue over year',fontsize=12,fontweight='bold');\nplt.xlabel('Month',fontsize=12,fontweight='bold');\nplt.show()\n#exploratory statistics\ntemp  = data.groupby(['Country'])['TotalRevenue'].apply(np.sum)\nplt.plot(temp);\nplt.ylim(0,1.1*max(temp.values))\nplt.ylabel('Total revenue over year',fontsize=12,fontweight='bold');\nplt.xlabel('Country',fontsize=12,fontweight='bold');\nplt.xticks(temp.index,temp.index,rotation=90)\nplt.show()\n#exploratory statistics\ntemp  = data.groupby(['Day'])['TotalRevenue'].apply(np.sum)\nplt.plot(temp);\nplt.ylim(0,1.1*max(temp.values))\nplt.ylabel('Total revenue over year',fontsize=12,fontweight='bold');\nplt.xlabel('Day of month',fontsize=12,fontweight='bold');\nplt.show()\n#exploratory statistics\ntemp  = data.groupby(['Day_week'])['TotalRevenue'].apply(np.sum)\nplt.plot(temp);\nplt.ylim(0,1.1*max(temp.values))\nplt.ylabel('Total revenue over year',fontsize=12,fontweight='bold');\nplt.xlabel('Day of week',fontsize=12,fontweight='bold');\nplt.show()\n#exploratory statistics\ntemp  = data.groupby(['Hour'])['TotalRevenue'].apply(np.sum)\nplt.plot(temp);\nplt.ylim(0,1.1*max(temp.values))\nplt.ylabel('Total revenue over year',fontsize=12,fontweight='bold');\nplt.xlabel('Hour',fontsize=12,fontweight='bold');\nplt.show()\n#exploratory statistics\ntemp  = data.groupby(['Min'])['TotalRevenue'].apply(np.sum)\nplt.plot(temp);\nplt.ylim(0,1.1*max(temp.values))\nplt.ylabel('Total revenue over year',fontsize=12,fontweight='bold');\nplt.xlabel('Min',fontsize=12,fontweight='bold');\nplt.show()","6b720087":"#ignoring customers who causes losses because they are small portion of revenue\ntemp  = data.groupby(['CustomerID'])['TotalRevenue'].apply(np.sum).reset_index(drop=0)\nbins=range(0,300001,50000)\nxx=plt.hist(temp['TotalRevenue'],bins=bins)\nprint (xx)\nplt.close()\nplt.subplots(figsize=(10,10))\nxxx=plt.hist(temp['TotalRevenue'],weights = temp['TotalRevenue'].values\/1000, bins=bins)\nfor i in range(len(xxx[1])-1):\n    plt.text(xxx[1][i],xxx[0][i]+0.03,str(int(xx[0][i]))+' Customers',fontsize=12,fontweight='bold')\nplt.xticks(np.array(bins)+25000,['Spent\\n' + str(bins[x]\/1000)+'k-'+str(bins[x+1]\/1000)+'k' for x in  range(len(bins)-1)],rotation=60)\nplt.xlabel('Spending',fontsize=15,fontweight='bold')\nplt.ylabel('Contribution to Revenue in Millions',fontsize=15,fontweight='bold')\nplt.show()","4912826f":"#combine certain stock types which are obviously simillar differetn colour t-shirts\ndef f(x):\n    if  x[:-1].isdigit() and x[-1].isdigit() is False:\n        return x[:-1]\n    else : return x\ndata['StockCode_NR'] = data['StockCode'].apply(f)\n\n#returns cost 0.1%, so ignore first\ndata = data[data['TotalRevenue']>0].reset_index(drop=True)","a931bbac":"# are there any subgroups of important customer\ntemp = data.groupby(['CustomerID',])['TotalRevenue'].apply(sum).reset_index(drop=False)\ntemp2 = temp.sort_values('TotalRevenue').iloc[::-1].reset_index(drop=True)\nresult,val = [] ,0\nfor i in range(len(temp2)):\n    val += temp2.iloc[i]['TotalRevenue']\n    result += [val,]\ntemp2['cummulative_profit'] = np.array(result)\/val\nfor i in [0.01,0.1,1,2,5,10,20,50]:\n    num = int(len(temp2)*1.*i\/100)\n    print (num,'Top %s percent of customers,%s percent of profit'%(i,100*np.round(temp2.loc[num]['cummulative_profit'],2)))\nplt.plot(temp2['cummulative_profit']);\nplt.xlabel('Customers ranked by revenue',fontsize=12,fontweight='bold')\nplt.ylabel('Cummulative revenue',fontsize=12,fontweight='bold')\nplt.show()","1dd6576a":"# are there any subgroups of important customer\ntemp = data.groupby(['StockCode_NR'])['TotalRevenue'].apply(sum).reset_index(drop=False)\ntemp['totalRev_over_time'] = data.groupby(['StockCode_NR'])['TotalRevenue'].apply(np.sum).reset_index(drop=True)\ntemp2 = temp.sort_values('totalRev_over_time').iloc[::-1].reset_index(drop=True)\n\nresult,val = [0,] ,0\nfor i in range(len(temp2)):\n    val += temp2.iloc[i]['TotalRevenue']\n    result += [val,]\ntemp2['cummulative_profit'] = np.array(result[1:])\/val\ntemp2['Total_profit'] = (np.array(result[1:])-np.array(result[:-1]))\/val\nfor i in [0.01,0.1,1,2,5,10,20,50,60]:\n    num = int(len(temp2)*1.*i\/100)\n    print (num,'Top %s percent of products,%s percent of profit'%(i,100*np.round(temp2.loc[num]['cummulative_profit'],2)))\nplt.plot(temp2['cummulative_profit'])\nx = 600 #coresponds to  97% products\n#print (np.sum(temp2['totalRev_over_time']>x),'threshold=',x,',',np.round(np.mean(temp2['totalRev_over_time']>x),2),'of products account for',np.round(np.sum(temp2[temp2['totalRev_over_time']>x]['TotalRevenue'])\/np.sum(temp2['TotalRevenue']),3),'of revenue\\n\\n')\ntemp3 = temp2[temp2['totalRev_over_time']>x]\ndata2 = pd.merge(data,temp3[['StockCode_NR','totalRev_over_time','Total_profit']]\n         ,on='StockCode_NR',how='inner')\nplt.xlabel('Products ranked by revenue',fontsize=12,fontweight='bold')\nplt.ylabel('Cummulative revenue',fontsize=12,fontweight='bold')\nplt.show()","05375531":"customer_info = data.groupby(['CustomerID',])['TotalRevenue'].apply(sum).reset_index(drop=False)\ncustomer_info = customer_info.sort_values('TotalRevenue').iloc[::-1].reset_index(drop=False)\n\nfor year in pd.unique(data.Year):\n    for month in sorted(map(int,pd.unique(data.Month))):\n        temp0  =  data2[(data2.Year==year) & (data2.Month==month)]\n        if len(temp0) == 0:\n            continue\n        temp = temp0.groupby(['CustomerID',])['TotalRevenue'].apply(sum).reset_index(drop=False)\n        temp2 = temp.sort_values('TotalRevenue').iloc[::-1].reset_index(drop=True)\n        result,val = [] ,0\n        for i in range(len(temp2)):\n            val += temp2.iloc[i]['TotalRevenue']\n            result += [val,]\n        temp2['cummulative_profit'] = np.array(result)\/val\n        temp2.rename(columns={'TotalRevenue' : 'Revenue_%s_%s'%(year,month)},inplace=True)\n        customer_info = pd.merge(customer_info,temp2[temp2.keys()[:2]],on='CustomerID',how='left').fillna(0)\n\nfrom collections import Counter\ntemp2 = data.groupby(['CustomerID',])['Country'].apply(lambda x:Counter(list(x)).most_common(1)[0][0]).reset_index(drop=False)\ncustomer_info = pd.merge(customer_info,temp2[temp2.keys()[:2]],on='CustomerID',how='left').fillna(0)","de24b175":"dictt_StockCode = {}\ncounter = 0\nfor i in data2.groupby(['totalRev_over_time','StockCode_NR'])['TotalRevenue'].apply(sum).reset_index(drop=False)['StockCode_NR'].iloc[::-1]:\n    dictt_StockCode[i] = counter\n    counter += 1\ndata2['StockCode_NR_int'] = data2['StockCode_NR'].map(dictt_StockCode)\n#print (counter)","ce2f617d":"'''0 Top 0.01 percent of products,2.0 percent of profit\n3 Top 0.1 percent of products,6.0 percent of profit\n32 Top 1 percent of products,20.0 percent of profit\n64 Top 2 percent of products,28.000000000000004 percent of profit\n160 Top 5 percent of products,44.0 percent of profit\n320 Top 10 percent of products,59.0 percent of profit\n640 Top 20 percent of products,76.0 percent of profit'''\nfrom wordcloud import WordCloud\nprint ('products... largest sales')\nranges =[(0,32,20),(32,64,8),(64,160,16),(160,320,15),(320,640,17),(0,320,60)]\nfor j in ranges:\n    stock_code_decribe = data2.groupby('StockCode_NR_int')['Description'].apply(list).reset_index(drop=False)\n    closest_pdts = range(j[0],j[1])\n    #print stock_code_decribe[stock_code_decribe.StockCode_NR_int.isin(closest_pdts)]\n    text = ''\n    for k in stock_code_decribe[stock_code_decribe.StockCode_NR_int.isin(closest_pdts)]['Description'].values:\n        for i in k[::10]:\n            text = text+' ' + i\n    wordcloud = WordCloud().generate(text)\n    wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\n    plt.title('Products %s to %s, contributing %s percent revenue'%j)\n    plt.imshow(wordcloud);plt.show()","739379ac":"#find the stats price of each product,\ndata2_product = data2.groupby('StockCode_NR_int')['UnitPrice'].apply(np.array).reset_index(drop=False)\ndata2_product['UnitPrice_median'] = data2_product['UnitPrice'].apply(np.median)\ndata2_product['UnitPrice_25%'] = data2_product['UnitPrice'].apply(lambda x : np.percentile(x,25))\ndata2_product['UnitPrice_75%'] = data2_product['UnitPrice'].apply(lambda x : np.percentile(x,75))\ndel data2_product['UnitPrice'] \ndata2 = pd.merge(data2,data2_product,on='StockCode_NR_int',how='left')\ndata2['Discount_given'] = 1.*(1*(data2['UnitPrice'] < data2['UnitPrice_median']) +\n                           1*(data2['UnitPrice'] < data2['UnitPrice_25%']) +\n                           1*(data2['UnitPrice'] < data2['UnitPrice_75%']) )","725d266c":"train = data2[((data2['Year']==2010)*(data2['Month'].isin([12,]))) | \\\n              ((data2['Year']==2011)*(data2['Month'].isin([1,2,3,4])))]\ntest = data2[ ((data2['Year']==2011)*(data2['Month'].isin([5])))] #this is validation\ntest2 = data2[ ((data2['Year']==2011)*(data2['Month'].isin([6,])))] #jun 2011\ntest3 = data2[ ((data2['Year']==2011)*(data2['Month'].isin([7,])))] #jul 2011\ntest4 = data2[ ((data2['Year']==2011)*(data2['Month'].isin([6,7,])))] #test data is from jun-jul 2011\ntest5 = data2[ ((data2['Year']==2011)*(data2['Month'].isin([6,7,8,9,10,11])))] #test data is from jun-nov 2011\n\nlen(train),len(test),len(test2),len(test3),len(test4),len(test5)","19a50ab9":"#get all orders from customer during time period \nInvoice = train.groupby(['CustomerID',]).apply(lambda x :(list(x['StockCode_NR_int']),\n                                                         list(x['Quantity']))).reset_index(drop=0)\n#print Invoice.head()\ncustomer_info_orders = pd.merge(customer_info,Invoice,on='CustomerID',how='outer')\ncustomer_info_orders.rename(columns={0 : 'orders'},inplace=True)\n\nInvoice_future = test.groupby(['CustomerID',]).apply(lambda x :(list(x['StockCode_NR_int']),\n                                                         list(x['Quantity']))).reset_index(drop=0)\n#print Invoice.head()\ncustomer_info_orders = pd.merge(customer_info_orders,Invoice_future,on='CustomerID',how='outer')\ncustomer_info_orders.rename(columns={0 : 'orders_future'},inplace=True)\n\nInvoice_future = test2.groupby(['CustomerID',]).apply(lambda x :(list(x['StockCode_NR_int']),\n                                                         list(x['Quantity']))).reset_index(drop=0)\n#print Invoice.head()\ncustomer_info_orders = pd.merge(customer_info_orders,Invoice_future,on='CustomerID',how='outer')\ncustomer_info_orders.rename(columns={0 : 'orders_future2'},inplace=True)\n\nInvoice_future = test3.groupby(['CustomerID',]).apply(lambda x :(list(x['StockCode_NR_int']),\n                                                         list(x['Quantity']))).reset_index(drop=0)\n#print Invoice.head()\ncustomer_info_orders = pd.merge(customer_info_orders,Invoice_future,on='CustomerID',how='outer')\ncustomer_info_orders.rename(columns={0 : 'orders_future3'},inplace=True)\n\nInvoice_future = test4.groupby(['CustomerID',]).apply(lambda x :(list(x['StockCode_NR_int']),\n                                                         list(x['Quantity']))).reset_index(drop=0)\n#print Invoice.head()\ncustomer_info_orders = pd.merge(customer_info_orders,Invoice_future,on='CustomerID',how='outer')\ncustomer_info_orders.rename(columns={0 : 'orders_future4'},inplace=True)\n\nInvoice_future = test5.groupby(['CustomerID',]).apply(lambda x :(list(x['StockCode_NR_int']),\n                                                         list(x['Quantity']))).reset_index(drop=0)\n#print Invoice.head()\ncustomer_info_orders = pd.merge(customer_info_orders,Invoice_future,on='CustomerID',how='outer')\ncustomer_info_orders.rename(columns={0 : 'orders_future5'},inplace=True)\n\n#get discount given to each customer during training phase\ndiscount_rating = train.groupby('CustomerID')['Discount_given'].apply(np.mean).reset_index(drop=0)\ncustomer_info_orders = pd.merge(customer_info_orders,discount_rating,on='CustomerID',how='outer')\n\n#get num_orders in time frame\norders = train.groupby('CustomerID')['InvoiceNo'].apply(lambda x : len(np.unique(x))).reset_index(drop=0)\norders.rename(columns={'InvoiceNo' : 'Num_invoices_during_train'},inplace=True)\ncustomer_info_orders = pd.merge(customer_info_orders,orders,on='CustomerID',how='outer')\n\n\ncustomer_info_orders = customer_info_orders.sort_values('CustomerID').reset_index(drop=True)","b6a060f2":"customer_info_orders.head()","58f10baa":"matrix_past = np.zeros((len(customer_info_orders),counter)).astype(np.float32)\nfor i in range(len(customer_info_orders)):\n    try:\n        pdt,quantity = customer_info_orders.iloc[i]['orders']\n        for j,k in zip(pdt,quantity):\n            matrix_past[i,j] += k\n    except TypeError : None #no orders presetn\nmatrix_past = 1*(matrix_past>0)\nimport gc \ngc.collect()\nprint (matrix_past)","8fe14f09":"matrix_future = np.zeros((len(customer_info_orders),counter)).astype(np.float32)\nfor i in range(len(customer_info_orders)):\n    try:\n        pdt,quantity = customer_info_orders.iloc[i]['orders_future']\n        for j,k in zip(pdt,quantity):\n            matrix_future[i,j] += k\n    except TypeError : None #no orders presetn\n\nimport gc \ngc.collect()","911e17a1":"matrix_future2 = np.zeros((len(customer_info_orders),counter)).astype(np.float32)\nfor i in range(len(customer_info_orders)):\n    try:\n        pdt,quantity = customer_info_orders.iloc[i]['orders_future2']\n        for j,k in zip(pdt,quantity):\n            matrix_future2[i,j] += k\n    except TypeError : None #no orders presetn\n\nimport gc \ngc.collect()","78c7cb21":"matrix_future3 = np.zeros((len(customer_info_orders),counter)).astype(np.float32)\nfor i in range(len(customer_info_orders)):\n    try:\n        pdt,quantity = customer_info_orders.iloc[i]['orders_future3']\n        for j,k in zip(pdt,quantity):\n            matrix_future3[i,j] += k\n    except TypeError : None #no orders presetn\n\nimport gc \ngc.collect()","3a9846fd":"matrix_future4 = np.zeros((len(customer_info_orders),counter)).astype(np.float32)\nfor i in range(len(customer_info_orders)):\n    try:\n        pdt,quantity = customer_info_orders.iloc[i]['orders_future4']\n        for j,k in zip(pdt,quantity):\n            matrix_future4[i,j] += k\n    except TypeError : None #no orders presetn\n\nimport gc \ngc.collect()","4a64abfb":"matrix_future5 = np.zeros((len(customer_info_orders),counter)).astype(np.float32)\nfor i in range(len(customer_info_orders)):\n    try:\n        pdt,quantity = customer_info_orders.iloc[i]['orders_future5']\n        for j,k in zip(pdt,quantity):\n            matrix_future5[i,j] += k\n    except TypeError : None #no orders presetn\n\nimport gc \ngc.collect()","66bdb892":"# recommendation system , for our case since entries are small, we use SVD to do matrix factorization\n# instead of a less expensive but approximate algorithm like alternating last squares. WARNING This is not scalable. \nU,D,V = np.linalg.svd(matrix_past)","bf51e978":"num = 1000\nU2=U[:,:num]\nD2=D[:num]\nV2=V[:num]\nZ = sum(D)\nprint (num,'dims can represent:',sum(D2)\/Z,'of matrix')\nplt.plot(list(map(lambda x : np.sum(D[:x])\/Z,range(len(D2)))))\nplt.show()\n#print matrix_past\n#np.matmul(np.matmul(U2,np.diag(D2)),V2)","34f9c286":"for i in range(counter):\n    customer_info_orders['pdt_'+str(i)] = matrix_past[:,i].astype(np.uint16)\nfor i in range(U2.shape[1]):\n    customer_info_orders['svd_'+str(i)] = U2[:,i].astype(np.float32)","b72086ba":"colsA =  ['Revenue_2011_' + str(x) for x in [1,2,3,4]]+['Revenue_2010_12']\n#print np.corrcoef(customer_info_orders['target2'].values,\n#           np.mean(np.log10(1+customer_info_orders[colsA].values),1))","4fa71e61":"customer_info_orders.keys()[3:8]","fa228cc9":"# SQUARED LOGLOSS\n#predict customer orders\nimport xgboost as  xgb\n'''\ntrain = data2[((data2['Year']==2010)*(data2['Month'].isin([12,]))) | \\\n              ((data2['Year']==2011)*(data2['Month'].isin([1,2,3,4])))]\ntest = data2[ ((data2['Year']==2011)*(data2['Month'].isin([5])))]\ntest2 = data2[ ((data2['Year']==2011)*(data2['Month'].isin([6,])))]\ntest3 = data2[ ((data2['Year']==2011)*(data2['Month'].isin([7,])))]\ntest4 = data2[ ((data2['Year']==2011)*(data2['Month'].isin([6,7,])))]\ntest5 = data2[ ((data2['Year']==2011)*(data2['Month'].isin([6,7,8,9,10,11])))]\n\nlen(train),len(test),len(test2),len(test3),len(test4),len(test5)\n'''\ncustomer_info_orders.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in customer_info_orders.columns.values]\n\npredictors = list(customer_info_orders.keys()[3:8])+['Num_invoices_during_train','mean_log_rev_train','Discount_given','Country']+\\\n                     list([x for x in customer_info_orders.keys() if 'svd' in str(x)]) [:3]\ncolsA =  ['Revenue_2011_' + str(x) for x in [1,2,3,4]]+['Revenue_2010_12']\ncustomer_info_orders['mean_log_rev_train']= np.log10(1+np.mean(customer_info_orders[colsA].values,1))\nz = customer_info_orders[['pdt_'+str(x) for x in range(100)]].describe().loc['mean']\n#predictors += list(z[z>0.03].index)\ntarget = 'target'\ngc.collect()\nfor i in range(2,6):\n    cols = [[0,],[0,],[6,],[7,],[8,],[6,7,8]][i]\n    cols = ['Revenue_2011_'+str(x) for x in cols]\n    customer_info_orders[target+str(i)]= np.log10(1+np.mean(customer_info_orders[cols].values,1))\n\nif True:\n    customer_info_orders2 = customer_info_orders.copy()\n    dicttt = {}\n    counter =0\n    for i in sorted(pd.unique(customer_info_orders2.Country)):\n        dicttt[i] = counter\n        counter += 1\n    customer_info_orders2 ['Country'] = customer_info_orders2 ['Country'].map(dicttt)\n    customer_info_orders2 = customer_info_orders2[customer_info_orders2['mean_log_rev_train']>0] #remove datapoints which have not brought stuff\n    customer_info_orders2[target]= np.log10(1+np.mean(customer_info_orders2[['Revenue_2011_5',]].values,1))\n    train_id = [x for x in range(len(customer_info_orders2)) if x%5!=0]\n    test_id = [x for x in range(len(customer_info_orders2)) if x%5==0]\n    dtrain = customer_info_orders2.iloc[train_id]\n    dcv = customer_info_orders2.iloc[test_id]\n    dtest = customer_info_orders2\n    gc.collect()\n    params = {}\n    params[\"objective\"] = \"reg:linear\"\n    params[\"eta\"] = 0.01\/10\n    params[\"min_child_weight\"] = 1\n    params[\"subsample\"] = 0.3\n    params[\"colsample_bytree\"] = 0.3\n    params[\"scale_pos_weight\"] = 1.0\n    params[\"silent\"] = 1\n    params[\"verbose\"] = 1\n    params[\"max_depth\"] = 5\n    #params[\"nthread\"] = 6\n    params[\"nthread\"] = -1\n    early_stopping_rounds = 50\n    plst = list(params.items())\n    xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n    xgcv = xgb.DMatrix(dcv[predictors].values, label=dcv[target].values)\n    xgtest2 = xgb.DMatrix(dtest[predictors].values, label=dtest[target+'2'].values)\n    xgtest3 = xgb.DMatrix(dtest[predictors].values, label=dtest[target+'3'].values)\n    xgtest4 = xgb.DMatrix(dtest[predictors].values, label=dtest[target+'4'].values)\n    xgtest5 = xgb.DMatrix(dtest[predictors].values, label=dtest[target+'5'].values)\n    #    xgb.train(xgtrain[predictors],dtrain['Demanda_uni_equil'],eval_set=evallist ,\n    #            eval_metric='rmse', early_stopping_rounds=early_stopping_rounds)\n    watchlist  = [ (xgtrain,'train'),(xgtest2,'2month'),(xgtest3,'3month'),\n                  (xgtest4,'4month'),(xgtest5,'5month'),(xgcv,'eval')][:]\n    a = {}\n    model=xgb.train(plst,xgtrain,10*5510,watchlist,verbose_eval =1000,feval=None,\n                    early_stopping_rounds=early_stopping_rounds*3,evals_result=a)\n    print (np.corrcoef(np.nan_to_num(np.log10(model.predict(xgtest2)[test_id])),\n                      customer_info_orders2[target].iloc[test_id]))\n    val = 0\n    for j in range(2,6):\n        j = str(j)\n        customer_info_orders2['pred'+j] = model.predict(xgtest2)\n        xxxx = np.corrcoef(model.predict(xgtest2),customer_info_orders2['target'+j])\n        xxxx = np.mean((model.predict(xgtest2)-customer_info_orders2['target'+j])**2)**.5\n        val += xxxx\n        print (xxxx)\n        \n    print ('final validation over four test sets',val\/4)","92adfce1":"plt.plot(customer_info_orders2['mean_log_rev_train'],customer_info_orders2['target5'],'ro',markersize=0.5)\nplt.xlabel('mean revenue from month 1-5',weight='bold',fontsize=12)\nplt.ylabel('mean revenue from month 7-12',weight='bold',fontsize=12)\nplt.title('Past revenue vs future revenue',weight='bold',fontsize=12)\nplt.xticks(range(5),[10**x for x in range(5)])\nplt.yticks(range(5),[10**x for x in range(5)])\nplt.show()\nplt.close()\nplt.plot(np.log10(1+customer_info_orders2['Revenue_2011_3']),customer_info_orders2['target5'],'ro',markersize=0.5)\nplt.xlabel('mean revenue from month 3',weight='bold',fontsize=12)\nplt.ylabel('mean revenue from month 7-12',weight='bold',fontsize=12)\nplt.title('Month 4 vs future revenue',weight='bold',fontsize=12)\nplt.xticks(range(5),[10**x for x in range(5)])\nplt.yticks(range(5),[10**x for x in range(5)])\nplt.show()\nplt.plot(np.log10(1+customer_info_orders2['Revenue_2011_4']),\n         np.log10(1+customer_info_orders2['Revenue_2011_5']),'ro',markersize=0.5)\nplt.xlabel('mean revenue from month 3',weight='bold',fontsize=12)\nplt.ylabel('mean revenue from month 7-12',weight='bold',fontsize=12)\nplt.title('Month 4 vs future revenue',weight='bold',fontsize=12)\nplt.xticks(range(5),[10**x for x in range(5)])\nplt.yticks(range(5),[10**x for x in range(5)])\nplt.show()\nfrom sklearn.linear_model import LogisticRegression\n\nLogisticRegression()\ndef four_degree_polynomial(X,y,n=3):\n    X1 = X[y>0]\n    y1 = y[y>0]\n    X2 = np.stack([X1**i for i in range(0,n)],-1)\n    B = np.matmul(np.matmul(np.linalg.inv(np.matmul(X2.T,X2)),X2.T),y1.values)\n    X3 = np.linspace(min(X),max(X),100)\n    X4 = np.stack([X3**i for i in range(0,n)],-1)\n    y4 = np.matmul(X4,B)\n    return X3,y4,LogisticRegression().fit(np.stack([X**i for i in range(0,n)],-1),1*(y>0)).predict_proba(X4)[:,1]\n\nplt.plot(customer_info_orders2['Discount_given'],customer_info_orders2['target5'],'ro',markersize=0.5)\na,b,c = four_degree_polynomial(customer_info_orders2['Discount_given'],customer_info_orders2['target5'],n=4)\nplt.plot(a,b,'m')\n#plt.plot(a,3*(np.log10(c)+1),'c')\nplt.xlabel('Discount Given, scale 0-3 ',weight='bold',fontsize=12)\nplt.ylabel('mean revenue from month 7-12',weight='bold',fontsize=12)\nplt.title('Month 4 vs future revenue',weight='bold',fontsize=12)\n#plt.xticks(range(5),[10**x for x in range(5)])\nplt.yticks(range(5),[10**x for x in range(5)])\nplt.show()\n\n\nplt.plot(customer_info_orders2['svd_0'],customer_info_orders2['target5'],'ro',markersize=0.5)\na,b,c = four_degree_polynomial(customer_info_orders2['svd_0'],customer_info_orders2['target5'],n=3)\nplt.plot(a,b,'m')\n#plt.plot(a,3*(np.log10(c)+1),'c')\nplt.xlabel('Embedding: First dimension',weight='bold',fontsize=12)\nplt.ylabel('mean revenue from month 7-12',weight='bold',fontsize=12)\nplt.title('Month 4 vs future revenue',weight='bold',fontsize=12)\n#plt.xticks(range(5),[10**x for x in range(5)])\nplt.yticks(range(5),[10**x for x in range(5)])\nplt.show()\n\nplt.plot(model.predict(xgtest2),customer_info_orders2['target5'],'ro',markersize=0.5)\n#plt.plot(a,3*(np.log10(c)+1),'c')\nplt.xlabel('Predicted',weight='bold',fontsize=12)\nplt.ylabel('Actual future revenue from month 7-12',weight='bold',fontsize=12)\nplt.title('Actual vs Predicted',weight='bold',fontsize=12)\nplt.xticks(range(5),[10**x for x in range(5)])\nplt.yticks(range(5),[10**x for x in range(5)])\nplt.show()","394817c0":"%pip install xgbfir\nimport xgbfir\nxgbfir.saveXgbFI(model,MaxTrees =2000,feature_names=predictors )\nxgb_explain = pd.read_excel('.\/XgbFeatureInteractions.xlsx')\nprint (xgb_explain.sort_values('Gain').iloc[::-1,:2])","b0fd6e67":"col = ['r','m','b','g'][::-1]\npercentile = [(0,1),(1,5),(5,20),(20,100)][::-1]\nsize= [3,2,1,0.3][::-1]\nlabels = ['Top 1% contributing 32% rev','1-5%, 18% Rev','5-20%, 25% Rev','20-100%, 25% Rev'][::-1]\ntemp = customer_info_orders[customer_info_orders['mean_log_rev_train'] >0].sort_values('mean_log_rev_train').iloc[::-1].reset_index(drop=True)\nfor i in range(4):\n    a,b = 0.01*len(temp)*percentile[i][0],0.01*len(temp)*percentile[i][1]\n    a,b = int(a),int(b)\n    plt.plot(temp['svd_0'].iloc[a:b],temp['svd_1'].iloc[a:b],'o',\n             color=col[i],markersize=size[i],label=labels[i])\n    print (len(temp['svd_0'].iloc[a:b]))\nplt.ylim(-0.25,0.4)\nplt.xlim(-0.01,0.180)\nplt.legend()\nplt.xlabel('dimension 1',fontweight='bold',fontsize=14)\nplt.ylabel('dimension 2',fontweight='bold',fontsize=14)\n\nplt.title('Four clusters',fontweight='bold',fontsize=14)\nplt.plot([0.03,0.03],[-1,1],'k')\nplt.plot([-1,1],[0,0],'k')\nplt.show()","569db37f":"num = np.array(range(3))\nU2=U[:,num]\nD2=D[num]\nV2=V[num]\n\nCustomer_embeddings = U2\nProduct_embeddings = V2","bfeb9b03":"#visualise each dimension\nfor i in range(0,3):\n    plt.plot(range(len(V2[0,:])),V2[i,:],'ro',markersize=.5);\n    plt.xlabel('Product number, ordered by revenue generated',\n               fontweight='bold',fontsize=14)\n    plt.ylabel('%s embedding dimension'%(i+1),\n               fontweight='bold',fontsize=14)\n    \n    plt.show()","707bca13":"\n#clustering products into 6 classes\nfrom sklearn.cluster import KMeans\nfrom wordcloud import WordCloud\n\nn=6\nV2 = V2[:,:600]\nkmeans = KMeans(n_clusters=n,random_state=123).fit(V2.T)\nfor i in range(n):\n    plt.plot(V2[1,:][kmeans.labels_==i],V2[2,:][kmeans.labels_==i],'o',markersize=2.5,label='cluster'+str(i));\nplt.xlabel('dimension 2',fontweight='bold',fontsize=14)\nplt.title('Clustering products into 6 clusters')\nplt.ylabel('dimension 3',fontweight='bold',fontsize=14)\nplt.legend()\nplt.show()\nfor i in range(n):\n    plt.plot(V2[0,:][kmeans.labels_==i],V2[1,:][kmeans.labels_==i],'o',markersize=2.5,label='cluster'+str(i));\nplt.xlabel('dimension 1',fontweight='bold',fontsize=14)\nplt.title('Clustering products into 6 clusters')\nplt.ylabel('dimension 2',fontweight='bold',fontsize=14)\nplt.legend()\nplt.show()\nfor i in range(n):\n    plt.plot(V2[0,:][kmeans.labels_==i],V2[2,:][kmeans.labels_==i],'o',markersize=2.5,label='cluster'+str(i));\nplt.xlabel('dimension 1',fontweight='bold',fontsize=14)\nplt.title('Clustering products into 6 clusters')\nplt.ylabel('dimension 3',fontweight='bold',fontsize=14)\nplt.legend()\nplt.show()\nstock_code_decribe = data2.groupby('StockCode_NR_int')['Description'].apply(np.unique).reset_index(drop=False)\nstock_code_decribe_top600 = stock_code_decribe.iloc[:600]\nstock_code_decribe_top600['labels']= kmeans.labels_ \nfig,ax = plt.subplots(3,2,figsize=(10,8))\n\nfor j in range(n):\n    print ('cluster',j)\n    text = ''\n    pdt_cluster_n = stock_code_decribe_top600[stock_code_decribe_top600['labels']==j]\n    for i in pdt_cluster_n['Description'].apply(np.unique).values:\n            text = text+' ' + i[0]\n    wordcloud = WordCloud().generate(text)\n    #print (np.mean(pdt_cluster_n.index[:40]))\n    wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\n    print (list(map(lambda x : str(x[0]),pdt_cluster_n['Description'].apply(np.unique).values[:20])))\n    ax[j\/\/2,j%2].imshow(wordcloud)\n    ax[j\/\/2,j%2].set_title('cluster_%s'%j)\n    ax[j\/\/2,j%2].axis('off')\nplt.show()","9cc131a0":"#looking at custers\ncluster1 = customer_info_orders2[(customer_info_orders2['mean_log_rev_train']>0.0)&(customer_info_orders2['svd_0']<0.030)&(customer_info_orders2['svd_1']>0.00)]\ncluster2 = customer_info_orders2[(customer_info_orders2['mean_log_rev_train']>0.0)&(customer_info_orders2['svd_0']>0.030)&(customer_info_orders2['svd_1']>0.00)]\ncluster3 = customer_info_orders2[(customer_info_orders2['mean_log_rev_train']>0.0)&(customer_info_orders2['svd_0']<0.030)&(customer_info_orders2['svd_1']<0.00)]\ncluster4 = customer_info_orders2[(customer_info_orders2['mean_log_rev_train']>0.0)&(customer_info_orders2['svd_0']>0.030)&(customer_info_orders2['svd_1']<0.00)]\nproduct_cluster_cols = ['pdt_cluster_'+str(x) for x in range(6)]\ndf = pd.DataFrame(None,columns=product_cluster_cols)\n\nkmeans2 = KMeans(n_clusters=4).fit(Customer_embeddings)\ncounter =0\n\nfor cluster in [cluster1,cluster2,cluster3,cluster4]:\n    print ('these %s customers contribute %s of revenue ' %(1.*len(cluster)\/len(temp),np.sum(10**cluster['mean_log_rev_train'])\/np.sum(10**temp['mean_log_rev_train'])))\n\n    U2_cluster = U2[cluster.index]\n    cluster_affinity_product = np.matmul(U2_cluster,V2[:,:600])\n    most_affinity_products = np.argsort(np.sum(cluster_affinity_product,0))[::-1]\n    text = ''\n    dist = np.sum((np.mean(U2_cluster,0)-kmeans.cluster_centers_)**2,1)**.5\n    #dist =  np.sum(((kmeans2.cluster_centers_[counter])-kmeans.cluster_centers_)**2,1)**.5 #using kmeans to cluster\n    df = df.append(pd.DataFrame([1\/dist],columns=product_cluster_cols))\n    #print (dist,np.argsort(dist))\n\n    for i in stock_code_decribe[stock_code_decribe.StockCode_NR_int.isin(most_affinity_products[:50])]['Description'].apply(np.unique):\n                text = text+' ' + i[0]\n    wordcloud = WordCloud().generate(text)\n    wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\n    plt.title('closest 50 products for cluster')\n    plt.imshow(wordcloud);plt.show()\n    counter  += 1","553166d2":"from math import pi\n \n# Set data\ndf2 = pd.DataFrame({\n'group': ['A','B','C','D'],\n'var1': [38, 1.5, 30, 4],\n'var2': [29, 10, 9, 34],\n'var3': [8, 39, 23, 24],\n'var4': [7, 31, 33, 14],\n'var5': [28, 15, 32, 14]\n})\nfig,ax = plt.subplots(1,1,figsize=(7,7))\ndf['group'] = ['cluster_%s'%i for i in range(1,5)]\ndf = df[sorted(df.keys())].reset_index(drop=1)\n# ------- PART 1: Create background\n \n# number of variable\ncategories=list(df)[1:]\nN = len(categories)\n \n# What will be the angle of each axis in the plot? (we divide the plot \/ number of variable)\nangles = [n \/ float(N) * 2 * pi for n in range(N)]\nangles += angles[:1]\n \n# Initialise the spider plot\nax = plt.subplot(111, polar=True)\n \n# If you want the first axis to be on top:\nax.set_theta_offset(pi \/ 2)\nax.set_theta_direction(-1)\n \n# Draw one axe per variable + add labels labels yet\nplt.xticks(angles[:-1], categories)\n \n# Draw ylabels\nax.set_rlabel_position(0)\n#plt.yticks([0,100], [0,100], color=\"grey\", size=7)\nplt.ylim(0,np.max(df.iloc[:,1:].values))\n \n \n# ------- PART 2: Add plots\n \n# Plot each individual = each line of the data\n \n# Ind1\nvalues=df.loc[0].drop('group').values.flatten().tolist()\nvalues += values[:1]\nax.plot(angles, values, linewidth=1, linestyle='solid', label=\"cluster 1, 54% customers, 39% revenue\")\nax.fill(angles, values, 'r', alpha=0.1)\n \n# Ind2\nvalues=df.loc[1].drop('group').values.flatten().tolist()\nvalues += values[:1]\nax.plot(angles, values, linewidth=1, linestyle='solid', label=\"cluster 2, 5% customers, 15% revenue\")\nax.fill(angles, values, 'g', alpha=0.1)\n\n# Ind3\nvalues=df.loc[2].drop('group').values.flatten().tolist()\nvalues += values[:1]\nax.plot(angles, values, linewidth=1, linestyle='solid', label=\"cluster 3, 25% customers, 28% revenue\")\nax.fill(angles, values, 'b', alpha=0.1)\n\n# Ind3\nvalues=df.loc[3].drop('group').values.flatten().tolist()\nvalues += values[:1]\nax.plot(angles, values, linewidth=1, linestyle='solid', label=\"cluster 4, 5% customers, 19% revenue\")\nax.fill(angles, values, 'y', alpha=0.1)\n \n# Add legend\nplt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\nplt.show()","bf77f000":"#recommendation\npdt_cluster_n = stock_code_decribe_top600[stock_code_decribe_top600['labels']==1]\nuser1333 = cluster2.sort_values('TotalRevenue').iloc[::-1][['svd_0','svd_1','svd_2']].iloc[0]\nprint (user1333 )","e14979f5":"matrix_future5[1333,pdt_cluster_n.index]\n\nprint (pdt_cluster_n.index)\nmatrix_past[1333,pdt_cluster_n.index]","76ed9454":"pdt_cluster_n['affinity_to_customer1333'] = np.sum((np.expand_dims(user1333,-1)-Product_embeddings[:,pdt_cluster_n.index])**2,0)\npdt_cluster_n['brought_customer1333'] = matrix_past[1333,pdt_cluster_n.index]\npdt_cluster_n.sort_values('affinity_to_customer1333').iloc[::-1]","d7afb349":"#get products with largest values for svd_0\nfrom wordcloud import WordCloud\n\nclosest_to_inside = np.argsort(V2[0,:])[::-1]\nstock_code_decribe = data2.groupby('StockCode_NR_int')['Description'].apply(np.unique).reset_index(drop=False)\nclosest_pdts = closest_to_inside[:30]\nprint ('products... largest sales',closest_pdts)\n#print stock_code_decribe[stock_code_decribe.StockCode_NR_int.isin(closest_pdts)]\ntext = ''\nfor i in stock_code_decribe[stock_code_decribe.StockCode_NR_int.isin(closest_pdts)]['Description'].values:\n    text = text + i[0]\nwordcloud = WordCloud().generate(text)\nwordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\nprint (list( map(lambda x : str(x[0]),\n          stock_code_decribe[stock_code_decribe.StockCode_NR_int.isin(closest_pdts)]['Description'].values[:20])))\nplt.imshow(wordcloud)\nplt.show()","695a8c7e":"\n#get products with largest values for svd_1\nclosest_to_inside = np.argsort(V2[1,:])[::-1]\nstock_code_decribe = data2.groupby('StockCode_NR_int')['Description'].apply(np.unique).reset_index(drop=False)\nclosest_pdts = closest_to_inside[:30]\nprint ('products... largest sales')\n#print stock_code_decribe[stock_code_decribe.StockCode_NR_int.isin(closest_pdts)]\ntext = ''\nfor i in stock_code_decribe[stock_code_decribe.StockCode_NR_int.isin(closest_pdts)]['Description'].values:\n    text = text + i[0]\nwordcloud = WordCloud().generate(text)\nwordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\nprint (list(map(lambda x : str(x[0]),\n          stock_code_decribe[stock_code_decribe.StockCode_NR_int.isin(closest_pdts)]['Description'].values[:20])))\n\nplt.imshow(wordcloud)\nplt.show()","f1be9e63":"#get products with smallest values for svd_1\nclosest_to_inside = np.argsort(V2[1,:])[::1]\nstock_code_decribe = data2.groupby('StockCode_NR_int')['Description'].apply(np.unique).reset_index(drop=False)\nclosest_pdts = closest_to_inside[:30]\nprint ('products... largest sales')\n#print stock_code_decribe[stock_code_decribe.StockCode_NR_int.isin(closest_pdts)]\ntext = ''\nfor i in stock_code_decribe[stock_code_decribe.StockCode_NR_int.isin(closest_pdts)]['Description'].values:\n    text = text + i[0]\nwordcloud = WordCloud().generate(text)\nwordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\nprint (list(map(lambda x : str(x[0]),\n          stock_code_decribe[stock_code_decribe.StockCode_NR_int.isin(closest_pdts)]['Description'].values[:20])))\n\nplt.imshow(wordcloud)\nplt.show()","cd26e1ed":"# looking at the top20% for the first half and see which ones did not make it in second half\n#later\ncustomer_info_orders['Rev_first_half'] = np.mean(customer_info_orders[[x for x in customer_info_orders.keys() if 'Revenue_' in str(x)][:6]],1)\ncustomer_info_orders['Rev_second_half'] = np.mean(customer_info_orders[[x for x in customer_info_orders.keys() if 'Revenue_' in str(x)][6:]],1)\ncustomer_info_orders['Churn'] = (1*(customer_info_orders['Rev_second_half'] ==0 ))\n#customer_info_orders['Rev_future'] = np.log10(1+np.abs(customer_info_orders['Rev_future']))*np.sign(customer_info_orders['Rev_future'])\n\n\ncolsA =  ['Revenue_2011_' + str(x) for x in [1,2,3,4,5]]+['Revenue_2010_12']\ncustomer_info_orders['mean_log_rev_firsthalf']= np.log10(1+np.mean(customer_info_orders[colsA].values,1))\n\nfor i in ['Revenue_2011_' + str(x) for x in [1,2,3,4,5]]+['Revenue_2010_12']:\n    customer_info_orders['log_r'+i[1:]] = np.log10(1+customer_info_orders[i])\n\npredictors = ['log_r'+i[1:] for i in list(customer_info_orders.keys()[3:9])]+ ['Num_invoices_during_train','Discount_given','mean_log_rev_firsthalf','constant']+\\\n                 list([x for x in customer_info_orders.keys() if 'svd' in str(x)]) [:3] \n\ntarget= 'Churn'\n\nimport statsmodels.formula.api as smf\n\n#print predictors\ncustomer_info_orders['constant']=1\ncustomer_info_orders2 = customer_info_orders[customer_info_orders['Rev_first_half']!=0].reset_index(drop=True)\n#customer_info_orders2 = customer_info_orders2.sort_values(target).reset_index(drop=True)\nz = customer_info_orders2[['pdt_'+str(x) for x in range(100)]].describe().loc['mean']\n#predictors += list(z[z>0.03].index)\nfor i in predictors:\n    customer_info_orders2[i]=customer_info_orders2[i].fillna(np.mean(customer_info_orders2[i].dropna()))\ndtrain = customer_info_orders2\ntrain_id = [x for x in range(len(dtrain)) if x%5!=0]\ntest_id = [x for x in range(len(dtrain)) if x%5==0]\ndtrain1 = customer_info_orders2.iloc[train_id].reset_index(drop=True)\ndtrain2 = customer_info_orders2.iloc[test_id].reset_index(drop=True)\n#print customer_info_orders2[[target,'Rev_first_half','Rev_second_half']]","07fcfde5":"import sklearn\nfrom sklearn.metrics import roc_auc_score\n\nimport statsmodels.api as sm\nfor alpha in [0.02,][:]:\n    if alpha!=0.0511:\n        dtrain_scaled = (dtrain[predictors]-np.mean(dtrain[predictors].values,0))\/np.std(dtrain[predictors].values,0)\n        dtrain1_scaled = (dtrain1[predictors]-np.mean(dtrain1[predictors].values,0))\/np.std(dtrain1[predictors].values,0)\n        dtrain2_scaled = (dtrain2[predictors]-np.mean(dtrain1[predictors].values,0))\/np.std(dtrain1[predictors].values,0)\n        dtrain_scaled['constant'] = 1\n        dtrain1_scaled['constant'] = 1\n        dtrain2_scaled['constant'] = 1\n        predictors_noNan = np.array(predictors)[~dtrain1_scaled.describe().loc['mean'].isnull()]\n        model = sm.GLM(dtrain1[target],dtrain1_scaled[predictors_noNan],family=sm.families.Binomial()).fit_regularized(alpha= alpha)\n        predictors_nonZero = list(predictors_noNan[model.params!=0])\n        model_unbias = sm.GLM(dtrain1[target],dtrain1_scaled[predictors_nonZero],family=sm.families.Binomial()).fit()\n        model_unbias_full = sm.GLM(dtrain[target],dtrain_scaled[predictors_nonZero],family=sm.families.Binomial()).fit()\n        print  (alpha,len(predictors_nonZero))\n        print ('train R2',roc_auc_score(dtrain1[target],model_unbias.predict(dtrain1_scaled[predictors_nonZero])))\n        print ('test R2',roc_auc_score(dtrain2[target],model_unbias.predict(dtrain2_scaled[predictors_nonZero])))\n        print (model_unbias_full.summary())","0ca9d26c":"# Predict future revenue from past revenue.\n# # # evaluation criteria is rmse(log(y_pred))","70fcb84c":"# Get customer-items matrix. Keeping to 0 if item not brought, and 1 if item brought\n# # ## Row reprsent customers and columns represents items 1-1853 (contributing 97% of revenue)","2fcf57d0":"# Get datetime","44287374":"# Customer revenue have different scales, hence use log scale","311ea0f3":"Recommendation for one customer, can be repeated for the other customers","d090a25a":"# Predicting Churn","d82a0fef":"# Plot important factors vs target5 (log(average rev for Jun-Nov 2011))","3a706afe":"# Dimentionality reduction of customer-items matrix using SVD","7d779196":"# Get orders in training set for all customers","1433ff83":"# Convert data to train, validation, four test sets test2-test5","5e48ee75":"# Clustering products, using three dimensions significant in customer revenue prediction"}}