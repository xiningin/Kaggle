{"cell_type":{"7e94d43c":"code","cffb2388":"code","02392335":"code","394e28c7":"code","e5d41e12":"code","d7707083":"code","fb3c67eb":"code","a4e558bc":"code","a05fbacd":"code","ecbb5501":"code","fca6a755":"code","b1f86f10":"code","c7a2d94c":"code","39ef8847":"code","19874cb0":"code","430cb675":"code","494557ef":"code","da2dc455":"code","f9b225e5":"code","34e33bed":"code","c44ce431":"code","ba0ee59e":"code","76f07b1c":"code","9c3c749d":"code","a2f70a4a":"code","41215dcd":"code","62a971f6":"code","879ae8d6":"markdown","92b7d158":"markdown","f607e5e2":"markdown","278f375b":"markdown","beead839":"markdown","74d26fe0":"markdown","758f5e0b":"markdown","b9e7db9f":"markdown","16ff9a6a":"markdown","1f0ce1bf":"markdown","fb920ec0":"markdown","7d3874c6":"markdown","4eb985fe":"markdown","6b439b3c":"markdown","365b5b4c":"markdown","8f613c2b":"markdown","e10d1866":"markdown","579e4459":"markdown","683dbd9e":"markdown","89a61d44":"markdown","682670ab":"markdown","4d210692":"markdown"},"source":{"7e94d43c":"import pandas as pd\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib.pyplot as plt\nleague = pd.read_csv('..\/input\/league-of-legends-diamond-ranked-games-10-min\/high_diamond_ranked_10min.csv')\npd.set_option(\"max_columns\", None)\nleague","cffb2388":"league.drop(['gameId'], axis = 1, inplace=True)","02392335":"y = league.blueWins\ny = np.array(y)\nx = league\nknn = KNeighborsClassifier(n_neighbors = 5)\nX = []\nAcc = []\nfor i in league:\n    temp = league[i]\n    temp = pd.DataFrame(temp)\n    knn.fit(temp, y)\n    y_pred = knn.predict(temp)\n    acc = metrics.accuracy_score(y, y_pred)\n    Acc.append(acc)\n    print(i ,\" \",acc)\n    if acc > 0.75 and acc < 1: # Creating an array of the important features(above 0.75 accuracy for feature)\n        X.append(i)","394e28c7":"col = np.arange(len(league.columns))\nfig = plt.figure()\naxes = fig.add_axes([0,0,2,1])\nplt.bar(col, Acc, width=0.5, alpha=0.7)\nplt.xlabel('column number')\nplt.ylabel('% Accuracy')\naxes.set_xticks(col)\nplt.title(\"Accuracy prediction with one feature\")\nprint(\"The next graph is from the data printed above\")\nplt.show()","e5d41e12":"fig = plt.figure()\naxes = fig.add_axes([0,0,2,1])\nplt.scatter(league['redGoldDiff'], y)\nplt.xlabel('Red Gold Difference')\nplt.ylabel('1 blue won')\nplt.title(\"Scatter graph to see the edge cases\")\nplt.show()","d7707083":"league.drop(league.loc[(league['redGoldDiff'] < -4500) & (league['blueWins'] == 0)].index, axis = 0, inplace=True)\nleague.drop(league.loc[(league['redGoldDiff'] >  4500) & (league['blueWins'] == 1)].index, axis = 0, inplace=True)","fb3c67eb":"league.drop(league.loc[(league['redExperienceDiff'] < -4500) & (league['blueWins'] == 0)].index, axis = 0, inplace=True)\nleague.drop(league.loc[(league['redExperienceDiff'] >  4500) & (league['blueWins'] == 1)].index, axis = 0, inplace=True)","a4e558bc":"y = league.blueWins\ny = np.array(y)","a05fbacd":"relevantDF = league[X]\nK = [1,3,5,7,10,25,50,100]\nprint(\"This is the knn moudle with only the important data\")\nfor k in K:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    X_train, X_test, y_train, y_test = train_test_split(relevantDF, y, test_size=0.2,random_state=1)\n    knn.fit(X_train , y_train)\n    y_pred = knn.predict(X_test)\n    print(\"knn =\", k,\": \",metrics.accuracy_score(y_test, y_pred))","ecbb5501":"print(\"This is the knn moudle with all the data\")\nfor k in K:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    X_train, X_test, y_train, y_test = train_test_split(league, y, test_size=0.2,random_state=1)\n    knn.fit(X_train , y_train)\n    y_pred = knn.predict(X_test)\n    print(\"knn =\", k,\": \",metrics.accuracy_score(y_test, y_pred))","fca6a755":"print(relevantDF.shape)\nprint(league.shape)","b1f86f10":"import seaborn as sns\nX.append('blueWins')\nrelevantDF = league[X]\naxes = plt.subplots(1, 1, figsize=(10, 8))\naxes = sns.heatmap(relevantDF.corr(), annot=True , linewidths=0.5)","c7a2d94c":"relevantDF = league[['blueExperienceDiff' ,'blueGoldDiff' ,'redTotalGold']]\nknn = KNeighborsClassifier(n_neighbors = 50)\nX_train, X_test, y_train, y_test = train_test_split(relevantDF, y, test_size=0.2,random_state=1)\nknn.fit(X_train , y_train)\ny_pred = knn.predict(X_test)\nprint(\"knn = 50\",\": \",metrics.accuracy_score(y_test, y_pred))","39ef8847":"league.drop(['blueWins'], axis = 1, inplace=True)\nleague.drop(['blueWardsPlaced','blueWardsDestroyed'], axis = 1, inplace=True)\nleague.drop(['redWardsPlaced','redWardsDestroyed'], axis = 1, inplace=True)","19874cb0":"print(\"Amount of victories for each team (0 - red victory , 1 - blue victory)\")\nprint(pd.value_counts(y))","430cb675":"print(\"Accuracy for dummy moudle in the test group \",y_test.mean())\nprint(\"Accuracy for dummy moudle for all the group \",y.mean())","494557ef":"for k in K:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(league, y)\n    y_pred = knn.predict(x)\n    print(\"knn =\", k,\": \",metrics.accuracy_score(y, y_pred))","da2dc455":"monster = league[['redEliteMonsters' , 'blueEliteMonsters']]\nknn = KNeighborsClassifier(n_neighbors=5)\nX_train, X_test, y_train, y_test = train_test_split(monster, y, test_size=0.2,random_state=1)\nknn.fit(X_train , y_train)\ny_pred2 = knn.predict(X_test)\nprint(\"knn = 5\",\":\",metrics.accuracy_score(y_test, y_pred2))\nprob2 = pd.DataFrame(knn.predict_proba(X_test))\nprob2.head()","f9b225e5":"knn = KNeighborsClassifier(n_neighbors=50)\nX_train, X_test, y_train, y_test = train_test_split(relevantDF, y, test_size=0.2,random_state=1)\nknn.fit(X_train , y_train)\ny_pred1 = knn.predict(X_test)\ncheck = pd.DataFrame(knn.predict_proba(X_test))#converting it to df to use loc\nind = check.loc[(check[0] > 0.45) & (check[0] < 0.55)].index\nprob1 = (knn.predict_proba(X_test))\nfor i in ind:\n    prob1[i][0] = prob2.loc[i][0]\n    prob1[i][1] = prob2.loc[i][1]\nprob1[prob1 > 0.5] = 1\nprob1[prob1 <= 0.5] = 0\nprint('Accuracy after improving the module: ' ,metrics.accuracy_score(y_test, prob1[:,1]))\nprint('Improved by 0.5%')","34e33bed":"confusion = metrics.confusion_matrix(y_test, prob1[:,1])\nprint(\"confusion matrix: \")\nprint(confusion)","c44ce431":"TP = confusion[1, 1]\nprint(\"True positive: \" ,TP)\nTN = confusion[0, 0]\nprint(\"True nagetive: \" ,TN)\nFP = confusion[0, 1]\nprint(\"False positive: \" ,FP)\nFN = confusion[1, 0]\nprint(\"False nagetive: \" ,FN)","ba0ee59e":"print(\"Accuracy: \",(TP + TN) \/ (TP + TN + FP + FN))\nprint(\"Classification Error: \",1-(TP + TN) \/ (TP + TN + FP + FN))\nprint(\"Recall: \", TP\/(TP+FN))\nprint(\"Precision: \", TP\/(TP+FP))\nprint(\"False positive rate: \", FP\/(TN+FP))","76f07b1c":"b = 1\nF = (1+b*b)*TP\/((1+b*b)*TP + b*FN + FP)\nprint(F)","9c3c749d":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nX_train, X_test, y_train, y_test = train_test_split(relevantDF, y, test_size=0.2, random_state=1)\ngnb = GaussianNB()\ny_pred = gnb.fit(X_train, y_train).predict(X_test)\nprint(\"Accuracy when using the Gaussian module:\" ,accuracy_score(y_test, y_pred))","a2f70a4a":"X_train, X_test, y_train, y_test = train_test_split(relevantDF, y, test_size=0.3, random_state=2)\ny_pred = gnb.predict_proba(X_test)\ny_pred = y_pred[:, 1] # taking only the positive outcomes\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)#using a saved function that to calculate the roc curve\nplt.plot([0,1], [0,1], label='Dummy')\nplt.plot(fpr, tpr, marker='', label='Gaussian')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.grid(True)\nplt.show()","41215dcd":"K = np.arange(1,100)\nscores = []\nfor k in K:\n    knn = KNeighborsClassifier(n_neighbors = k)\n    CV = cross_val_score(knn, relevantDF ,y, cv = 10 ,scoring='accuracy')\n    scores.append(CV.mean())\nplt.plot(K, scores)\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Average Cross-Validated Accuracy')\nplt.grid(True)\nplt.show()","62a971f6":"print(\"The maximum average score when using cross validation is :\",max(scores))\nprint(\"When the knn = \", scores.index(max(scores))+1)","879ae8d6":"### Cross Validiation:","92b7d158":"Deleting the colum of 'gameId' because it doesn't contribute for understanding the data\n","f607e5e2":"### *KNN: Cofusion Matrix*","278f375b":"When the knn is 1 so there is 100% accuracy this is called overfitting because with all the existing data it will perform perfect but once we insert new data to predict this kind of moudle won't give us the best results, I would have take knn = 3\nbecause it will give a 82% accuracy with enough neighbours for comparison.\n\n** This moudle is more accurate then the ones we saw above because here we didn't split the data to test group and learning group","beead839":"Predicting the victory of a team just from one column of information to form a basic knowledge of what column to keep and what to discard","74d26fe0":"### Analyzing data correlation:","758f5e0b":"With only three features we got 74% Accuracy.\nOnly 0.0092% accuracy difference when using all the data!","b9e7db9f":"The amount of wards is not helping basically you place a ward so you will see if someone is coming to help the enemy kill you but we have a column for the kills.","16ff9a6a":"### KNN module with no test and train group:","1f0ce1bf":"### <b>Improving the module:<\/b>\n\nAfter getting to accuarcy of 74% I wanted to improve it so I took all the indexes that the predicted probabilty was 0.45-0.55\nmeaning there's a chance of mistake in the prediction.\nso I took 2 different features built a new module with them and relied on that module, even though this module alone has accuracy of 61%","fb920ec0":"If we were to use dummy moudle it would be insuffisant with accuracy of about 50% and in the knn moudle we get about 74% accuracy","7d3874c6":"We can see from the results above that when we are taking the important data with 10 culomn we are getting approximately the same result as if we are taking all the features in the data","4eb985fe":"### *ROC Graph for Gaussian module:*","6b439b3c":"# League Of Legends - Classification Problem\n","365b5b4c":"### <b>Classification with Gaussian module<\/b>","8f613c2b":"In the next cell I will get rid of the edge cases:\n\nWhenever a team had a huge lead on gold and experience but still lost the game I will consider it as an abnormal senerio","e10d1866":"We can tell that the most important information to take in order to predict wich team would win is about the gold and experience","579e4459":"Here we can see an attempt to improve to module with Elite monster data but with very little succsess for the amount of work","683dbd9e":"### Dimensionality Redaction:","89a61d44":"### Dummy module option:","682670ab":"![image.png](attachment:image.png)","4d210692":"From the heat map above we can see that we can take even less features because some of the features can be repetitive like 'blueGoldDiff' and 'redGoldDiff'"}}