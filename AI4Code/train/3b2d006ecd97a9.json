{"cell_type":{"a5073edd":"code","4159f30d":"code","dd44736a":"code","8efba450":"code","97d17795":"code","658d2762":"code","21e14fe6":"code","3fb2dad8":"code","d8154533":"code","acf4abd7":"code","5008e0f3":"code","3be3b90f":"code","6916aea1":"code","5f49167d":"code","f70e173a":"code","f9cfa5b2":"code","bd068eeb":"code","02b76815":"code","5cc55d22":"code","a21edd6c":"markdown","2b7a47a4":"markdown","4635055b":"markdown","72c60241":"markdown","29243b82":"markdown","165b3c52":"markdown"},"source":{"a5073edd":"from sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport cv2\n\nimport numpy as np\nimport pandas as pd\nimport copy\nimport os\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom IPython.display import clear_output\nfrom collections import OrderedDict\n\nimport torch\nfrom torch import optim\nfrom torch import nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nfrom torchvision import transforms, models\nfrom torch.utils.data import DataLoader, Dataset, Subset\n\nimport warnings\nwarnings.filterwarnings('ignore')","4159f30d":"class RiddikulusDataset(Dataset):\n    def __init__(self, path, device, transform=None):\n        self.path = path\n        self.device = device\n        self.transform = transform\n        paths = self.get_paths(path)\n        \n        paths = np.concatenate(np.array(paths))\n        paths = np.concatenate(paths)\n        \n        categories = self.get_categories(paths)\n        \n        paths, categories = shuffle(paths, categories)\n        \n        self.idx_to_categories = {k:v for k,v in enumerate(np.unique(categories))}\n        self.categories_to_idx = {v:k for k,v in enumerate(np.unique(categories))}\n        \n        self.paths, self.categories = paths, categories\n        \n    def get_paths(self, path):\n        if(path.count('\/') == 5):\n            return path\n        paths = []\n        for sub in os.listdir(path):\n            paths.append(path + '\/' + sub)\n        return [self.get_paths(p) for p in paths]\n    \n    def get_categories(self, paths):\n        categories = []\n        for path in paths:\n            categories.append(path.split('\/')[4])\n        return np.array(categories)\n    \n    def __len__(self):\n        return len(self.paths)\n    \n    def __getitem__(self, idx):\n        image = cv2.imread(self.paths[idx])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        category = self.categories_to_idx[self.categories[idx]]\n        category = np.eye(len(self.categories_to_idx))[category]\n        \n        if self.transform is not None:\n            image = self.transform(image=image)[\"image\"]\n            \n        image = torch.tensor(image).to(self.device)\n        category = torch.tensor(category)\n        label = category.type(torch.FloatTensor)\n        label = label.to(self.device)\n        \n        return image, label","dd44736a":"transform = A.Compose(\n    [\n        A.Resize(28, 28),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ]\n)","8efba450":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","97d17795":"path = '..\/input\/riddikulus'\nriddik_ds = RiddikulusDataset(path, device=device, transform=transform)","658d2762":"def visualize(dataset, idx=0, samples=10, cols=5):\n    dataset = copy.deepcopy(dataset)\n    dataset.transform = A.Compose([t for t in dataset.transform if not isinstance(t, (A.Normalize, ToTensorV2))])\n    rows = samples \/\/ cols\n    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 6))\n    for i in range(samples):\n        image, lbl = dataset[i]\n        lbl = np.argmax(lbl.cpu().numpy())\n        ax.ravel()[i].set_title(dataset.idx_to_categories[lbl], \n                                fontsize=18, color='#ba3a15')\n        ax.ravel()[i].imshow(image.cpu().numpy())\n        ax.ravel()[i].set_axis_off()\n    plt.tight_layout()\n    plt.show()","21e14fe6":"visualize(riddik_ds)","3fb2dad8":"batch_size = 32\nnr_categories = len(riddik_ds.categories_to_idx.keys())\nmax_idx = int(5480 * 16)\n\nsubset = Subset(riddik_ds, np.arange(max_idx))","d8154533":"class ResNet18(nn.Module):\n    def __init__(self, nr_categories):\n        super(ResNet18, self).__init__()\n        self.model = models.resnet18(pretrained=False)\n        path = '..\/input\/pretrained-pytorch\/resnet18-5c106cde.pth'\n        self.model.load_state_dict(torch.load(path))\n        for param in self.model.parameters():\n            param.requires_grad = False\n        num_ftrs = self.model.fc.in_features\n        self.model.avgpool = nn.AdaptiveAvgPool2d(output_size=(1,1))\n        self.model.fc = nn.Sequential(nn.Flatten(),\n        nn.Linear(512, 128),\n        nn.ReLU(),\n        nn.Dropout(0.2),\n        nn.Linear(128, nr_categories),\n        nn.Sigmoid())\n    \n    def forward(self, x):\n        return self.model.forward(x)","acf4abd7":"class SqueezeNet(nn.Module):\n    def __init__(self, nr_categories):\n        super(SqueezeNet, self).__init__()\n        self.model = models.squeezenet1_0(pretrained=False)\n        path = '..\/input\/pretrained-pytorch\/squeezenet1_0-a815701f.pth'\n        self.model.load_state_dict(torch.load(path))\n        for param in self.model.parameters():\n            param.requires_grad = False\n        self.model.avgpool = nn.AdaptiveAvgPool2d(output_size=(1,1))\n        self.model.classifier = nn.Sequential(nn.Flatten(),\n        nn.Linear(512, 128),\n        nn.ReLU(),\n        nn.Dropout(0.2),\n        nn.Linear(128, nr_categories),\n        nn.Sigmoid())\n    \n    def forward(self, x):\n        return self.model.forward(x)","5008e0f3":"modelz = [SqueezeNet(nr_categories).to(device),\n          ResNet18(nr_categories).to(device)]","3be3b90f":"criteron = nn.BCELoss().to(device)","6916aea1":"train, validate = train_test_split(\n                    np.arange(max_idx), test_size=0.33, \n                    random_state=42, shuffle=False)","5f49167d":"nr_epochs = 40\nnr_models = 2\n\naccuracies = np.zeros((nr_models, nr_epochs, len(subset)))\n\nbest_weights = [{} for i in range(nr_models)]\nbest_accs = [0 for i in range(nr_models)]\n\nfor f in range(nr_models):\n\n    dataloader = DataLoader(subset, batch_size=batch_size, shuffle=True)\n    optimizer = torch.optim.Adam(modelz[f].parameters(), lr= 1e-3)\n    \n    for e in range(nr_epochs):\n\n        correct = 0\n        for i, (X, y) in enumerate(dataloader):\n            y_hat = modelz[f].forward(X)\n\n            y_max = torch.max(y_hat, 1)[1]\n            y_one = torch.max(y, 1)[1].to(device)\n            equal = torch.eq(y_max, y_one).type(torch.int32)\n            correct += (equal == 1).sum(dim=0).item() \/ len(y_hat)\n\n            acc = correct \/ (i+1)\n            accuracies[f][e][i] = acc\n            \n            if(acc > best_accs[f]):\n                best_accs[f] = acc\n                best_weights[f] = modelz[f].state_dict()\n\n            loss = criteron(y_hat, y)\n\n            if((e+1) % 10 == 0 and (i+1) % 1000 == 0):\n                #clear_output(wait=True)\n                print('Model:{:2d}, Epoch:{:6d}, i:{:5d}\/{:5d}, Loss:{:1.3f}, Accs:{:1.3f}'\n                          .format(int(f + 1), int(e + 1), int(i + 1), \n                                  len(dataloader), loss, accuracies[f][e][i]))\n\n            if i in train:\n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n    \n# get the best weights\nfor i in range(len(best_weights)):\n    model_name = str(type(modelz[i])).split('.')[-1].split(\"'\")[0]\n    print(\"Model:{:10s} - Best_Accuracy:{:1.3f}\"\n             .format(model_name, best_accs[i]))\n    modelz[i].load_state_dict(best_weights[i])\n    \naccuracies = np.array(accuracies)","f70e173a":"plt.title(\"Accuracies per model\")\nmid = int(len(accuracies) \/ 2)\nplt.plot(accuracies[0][:,mid], 'blue', label='SqueezeNet')\nplt.plot(accuracies[1][:,mid], 'orange', label='AlexNet')\nplt.legend()","f9cfa5b2":"class Ensemble(nn.Module):\n    def __init__(self, models, nr_categories=13):\n        super(Ensemble, self).__init__()\n        self.modelA = models[0]\n        self.modelB = models[1]\n        # remove last linear layer\n        self.modelA.fc = nn.Identity()\n        self.modelB.fc = nn.Identity()\n        \n        self.classifier = nn.Linear(26, nr_categories)\n        \n    def forward(self, x):\n        x1 = self.modelA(x.clone())\n        x1 = x1.view(x1.size(0), -1)\n        x2 = self.modelB(x)\n        x2 = x2.view(x2.size(0), -1)\n        x = torch.cat((x1, x2), dim=1)\n        \n        x = self.classifier(F.relu(x))\n        return x","bd068eeb":"ensemble = Ensemble(modelz, nr_categories=nr_categories).to(device)\n\ncriterion = nn.BCELoss().to(device)\noptimizer = optim.Adam(ensemble.parameters(),lr=0.003)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)","02b76815":"nr_epochs = 20\naccuracies = np.zeros((nr_epochs, len(subset)))\n\nfor e in range(nr_epochs):\n    \n    correct = 0\n    dataloader = DataLoader(subset, batch_size=batch_size, shuffle=True)\n    \n    for i, (X, y) in enumerate(dataloader):\n        y_hat = modelz[f].forward(X)\n\n        y_max = torch.max(y_hat, 1)[1]\n        y_one = torch.max(y, 1)[1].to(device)\n        \n        equal = torch.eq(y_max, y_one).type(torch.int32)\n        correct += (equal == 1).sum(dim=0).item() \/ len(y)\n\n        acc = correct \/ (i+1)\n        accuracies[e][i] = acc\n\n        loss = criteron(y_hat, y)\n\n        if((e+1) % 5 == 0 and (i+1) % 2000 == 0):\n            #clear_output(wait=True)\n            print('Epoch:{:3d}, i:{:3d}, Loss:{:1.3f}, Acc:{:1.3f}'\n                     .format(e, i, loss.item(), acc))\n\n        if i in train:\n            loss.backward()\n            scheduler.step()\n            optimizer.zero_grad()\n            \naccuracies = np.array(accuracies)","5cc55d22":"plt.title('Ensemble Accuracy')\nmid = int(len(accuracies) \/ 2)\nplt.plot(accuracies[:,mid], 'blue')","a21edd6c":"<h1 id=\"models\" style=\"border-radius: 25px; color:#ecbc15; background:#ba3a15;\"> \n    <center>Models\n        <a class=\"anchor-link\" href=\"#model\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","2b7a47a4":"<h1 id=\"analyze\" style=\"border-radius: 25px; color:#ecbc15; background:#ba3a15;\"> \n    <center>Analyze\n        <a class=\"anchor-link\" href=\"#analyze\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","4635055b":"<h1 id=\"ensemble\" style=\"border-radius: 25px; color:#ecbc15; background:#ba3a15;\"> \n    <center>Ensemble\n        <a class=\"anchor-link\" href=\"#ensemble\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","72c60241":"<h1 id=\"train\" style=\"border-radius: 25px; color:#ecbc15; background:#ba3a15;\"> \n    <center>Training\n        <a class=\"anchor-link\" href=\"#train\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","29243b82":"<h1 id=\"dataset\" style=\"border-radius: 25px; color:#ecbc15; background:#ba3a15;\"> \n    <center>Dataset\n        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","165b3c52":"<div>\n    <img style=\"border-radius: 25px;\" src=\"https:\/\/storage.googleapis.com\/kaggle-datasets-images\/1084374\/1824959\/c6e67712e9ba385ce218b14f0596587b\/dataset-cover.jpg\" \/>\n<\/div>"}}