{"cell_type":{"5766fdf2":"code","87bee16d":"code","0c91ebe4":"code","315fe323":"code","0ef05337":"code","14d7ca4b":"code","9f66c1e7":"code","85648296":"code","7e9f3dd6":"code","fff3621b":"code","20caabc7":"code","b236a55c":"code","0023a94d":"code","a2891577":"code","d27c139c":"code","b06a7a90":"code","eca80d23":"code","4d18b8a9":"code","c4c617fa":"code","b763fd45":"code","511e8429":"code","4a4e1c95":"code","b2726406":"code","36fcea04":"code","7830416d":"code","aea71c6b":"code","0f57d5ca":"code","b19a8abf":"code","eee0922a":"code","a2dbb820":"code","e0308c09":"code","3f27f08a":"code","59b9046d":"code","7a4f8b30":"code","0be46199":"code","47852a52":"code","f3cfbd59":"code","90f9666b":"code","7f4a60fb":"code","22c7768e":"code","b4314e8f":"code","09cdf8a3":"code","202057ac":"code","7c37f136":"code","04a52177":"code","3e3fe3d7":"code","4d671e8b":"code","0af6d045":"code","58f02c01":"code","5e7b90d5":"code","7f905c17":"code","b13a8519":"code","cf3de073":"code","aa34e04d":"code","33ad9ab5":"code","9939c615":"code","becda151":"code","125678ed":"code","aac04158":"code","540f74e5":"code","846b6699":"code","1b754a10":"code","359f6a45":"code","a36bb643":"code","0616fe32":"code","96fa3923":"code","1b222b39":"code","57159762":"code","09537326":"code","f318d3e2":"code","4d8640da":"code","d70baeb0":"code","8818efde":"code","2b6e58bd":"code","1c4e2292":"code","d2e8b700":"code","aee799f6":"code","532a6a39":"code","720c2a26":"code","b18ef8ae":"code","dcbaa2bb":"code","58cc19d6":"code","3f83593b":"code","59f160f3":"code","e48ee83f":"code","e131c464":"code","19562aa3":"code","b16a2339":"code","609a2d8f":"code","2a53c994":"code","35b8381d":"code","aed6f4e6":"code","a32a97e2":"code","b5a1f74a":"code","c15cdbea":"code","fbc40422":"code","e4bb3947":"code","23e48c3a":"code","e693650d":"code","ce3d0577":"code","b97879eb":"code","a7e0d918":"code","5eef2110":"code","d03f5205":"markdown","748f4f82":"markdown","7071e72f":"markdown","8cdd2d68":"markdown","0e93a3e4":"markdown","9bc3b9bc":"markdown","b998976f":"markdown","cd4e54c4":"markdown","870063e3":"markdown","191da516":"markdown","6545a857":"markdown","2ed27d37":"markdown","b18abad4":"markdown","d37df3b8":"markdown","f50359ba":"markdown","78f96af2":"markdown","9a2ee549":"markdown","490a2e84":"markdown","63abb201":"markdown","9e3c3399":"markdown","eeb82a1a":"markdown","eb5cddb2":"markdown","e64dcfad":"markdown","508566d2":"markdown","a4761fa5":"markdown","2c1bfda7":"markdown","6fc8eb9c":"markdown","f9583d96":"markdown","36c6afd8":"markdown","fcb8cce6":"markdown","25ce6ebf":"markdown","b0e1258e":"markdown","9f4e6723":"markdown","45d4c6d2":"markdown","56361745":"markdown","539be5b4":"markdown","395ea656":"markdown","71317f9b":"markdown","5d49d69a":"markdown","da36d021":"markdown","68aadaa1":"markdown","5dd48d43":"markdown","d3375a8b":"markdown","f3a15c9a":"markdown","4d580619":"markdown","95053f3d":"markdown","09aedd5f":"markdown","5b71a96f":"markdown","9e7c545c":"markdown","b1eaae10":"markdown","3ea42a17":"markdown","d9394734":"markdown","16bf7de0":"markdown","b5c36b1c":"markdown","ed793a5d":"markdown","c26a88c9":"markdown","b847e30f":"markdown","93623aaa":"markdown","b290c18c":"markdown","d7715676":"markdown","c46e83ab":"markdown","7272dad8":"markdown","5c6c63e2":"markdown","cef267bb":"markdown","a138ddda":"markdown","6f0d420e":"markdown","06fdb902":"markdown","0e2281ff":"markdown","09ea4892":"markdown","4d5a1582":"markdown","0b5e31a8":"markdown","e6dafdcd":"markdown","9bdd6844":"markdown","c919ffeb":"markdown","3a71e68c":"markdown","5e95dbea":"markdown","f8b492a8":"markdown","dee970f3":"markdown"},"source":{"5766fdf2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, accuracy_score, confusion_matrix\n\nimport warnings\nwarnings.simplefilter('ignore')","87bee16d":"data_titanic = pd.read_csv('..\/input\/titanic\/train.csv')\ndata_titanic.head(3)","0c91ebe4":"data_titanic_num = data_titanic[['Sex', 'Age', 'SibSp', 'Parch', 'Embarked', 'Survived']].copy()\ndata_titanic_num[\"Sex\"] = data_titanic_num[\"Sex\"].map({\"female\":0, \"male\":1})\ndata_titanic_num[\"Embarked\"] = data_titanic_num[\"Embarked\"].map({\"S\":0, \"C\":1, \"Q\": 2})\ndata_titanic_num = data_titanic_num.dropna()  # without NAN \ndata_titanic_num","315fe323":"data_health = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\ndata_health = data_health.drop_duplicates()\ndata_health.tail(3)","0ef05337":"data_water = pd.read_csv('..\/input\/ammonium-prediction-in-river-water\/PB_1996_2019_NH4.csv', sep=';')\ndata_water.tail(3)","14d7ca4b":"data_water.info()","9f66c1e7":"data_nlp = pd.read_csv('..\/input\/nlp-reports-news-classification\/water_problem_nlp_ua_for_Kaggle_100.csv', delimiter=';', \n                 header=0, encoding='cp1251')\ndata_nlp.tail(3)","85648296":"# Download one file\nimport requests\nprint('Download daily data from the Portal of RNBO of Ukraine')\nmyfile = requests.get('https:\/\/api-covid19.rnbo.gov.ua\/charts\/main-data?mode=ukraine&fbclid=IwAR1vNXEE0nkmorUmGP4StG4cLrj1Z9VoX3c3Bi8dfltr0elgOj4b0M3ONvk')\nopen('filename', 'wb').write(myfile.content)\ndata_covid = pd.read_json('filename')\ndata_covid[:3]","7e9f3dd6":"# For any part of the notebook\nimport time\nstart_time = time.time()\nprint('Start time:', time.strftime(\"%H:%M:%S\",time.gmtime(start_time)))\n\n# ... code.... some cells\n\nsecond_time = time.time() - start_time\nprint('Time Taken:', time.strftime(\"%H:%M:%S\",time.gmtime(second_time)))","fff3621b":"# For the given code in the one cell\nfrom tqdm import tqdm\nx = 0\nfor i in tqdm(range(1000)):\n    x *= np.pi**100","20caabc7":"# Autoviz for automatic EDA\n!pip install xlrd\n!pip install autoviz\nfrom autoviz.AutoViz_Class import AutoViz_Class","b236a55c":"data_health.head(3)","0023a94d":"data_health.to_csv('data_EDA.csv', index=False)","a2891577":"AV = AutoViz_Class()\ndata = pd.read_csv('.\/data_EDA.csv')\ndf = AV.AutoViz(filename=\"\", sep=',', depVar='target', dfte=data, header=0, verbose=1, lowess=False, \n                chart_format='svg',  max_cols_analyzed=15)","d27c139c":"!pip install pandas-profiling==2.11.0","b06a7a90":"import pandas_profiling as pp\npp.ProfileReport(data_health)","eca80d23":"data_health.describe()","4d18b8a9":"df = data_health[['sex', 'cp', 'trestbps', 'fbs', 'restecg', 'target']].copy()","c4c617fa":"from sklearn.decomposition import PCA\npca = PCA(n_components=3)\n#df = data_titanic_num.copy()\n#target = df.pop('Survived')\n#df = data_health.copy()\ntarget = df.pop('target')\npca.fit(df)\npca_samples = pca.transform(df)\nps = pd.DataFrame(pca_samples)\nps.head()","b763fd45":"# Thanks to https:\/\/www.kaggle.com\/tanetboss\/user-clustering-for-anime-recommendation (clustering)\nfrom mpl_toolkits.mplot3d import Axes3D\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (6, 4)\nplt.style.use('ggplot')\n%config InlineBackend.figure_formats = {'png', 'retina'}\n\ntocluster = pd.DataFrame(ps[[0,1,2]])\nplt.rcParams['figure.figsize'] = (16, 9)\n\nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(tocluster[0], tocluster[2], tocluster[1])\n\nplt.title('Data points in 3D PCA axis', fontsize=20)\nplt.show()","511e8429":"# Thanks to https:\/\/www.kaggle.com\/tanetboss\/user-clustering-for-anime-recommendation (clustering)\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nn_max = 15\nn_start = 2\nscores = []\ninertia_list = np.empty(n_max)\n\nfor i in range(n_start,n_max):\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(tocluster)\n    inertia_list[i] = kmeans.inertia_\n    scores.append(silhouette_score(tocluster, kmeans.labels_))\n\nn_max_shift = 2  # find maximum after this index of score\nn_clusters = np.argmax(scores[n_max_shift:])+(n_start+n_max_shift) # it's my upgrade\nplt.plot(range(0,n_max),inertia_list,'-o')\nplt.xlabel('Number of cluster')\nplt.axvline(x=n_clusters, color='blue', linestyle='--')\nplt.ylabel('Inertia')\nplt.show()\n\nplt.plot(range(2,n_max), scores);\nplt.title('Results KMeans')\nplt.xlabel('n_clusters');\nplt.axvline(x=n_clusters, color='blue', linestyle='--')\nplt.ylabel('Silhouette Score');\nplt.show()","4a4e1c95":"# Thanks to https:\/\/www.kaggle.com\/tanetboss\/user-clustering-for-anime-recommendation (clustering)\nclusterer = KMeans(n_clusters=n_clusters,random_state=30).fit(tocluster)\ncenters = clusterer.cluster_centers_\nc_preds = clusterer.predict(tocluster)\n\nprint(centers)\n\nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(tocluster[0], tocluster[2], tocluster[1], c = c_preds)\nplt.title('Data points in 3D PCA axis', fontsize=20)\n\nplt.show()","b2726406":"# Thanks to https:\/\/www.kaggle.com\/tanetboss\/user-clustering-for-anime-recommendation (clustering)\nfig = plt.figure(figsize=(10,8))\nplt.scatter(tocluster[1],tocluster[0],c = c_preds)\nfor ci,c in enumerate(centers):\n    plt.plot(c[1], c[0], 'o', markersize=8, color='red', alpha=1)\n\nplt.xlabel('x_values')\nplt.ylabel('y_values')\n\nplt.title('Data points in 2D PCA axis', fontsize=20)\nplt.show()","36fcea04":"# Thanks to https:\/\/www.kaggle.com\/tunguz\/mnist-2d-t-sne-with-rapids\n# You need Add Kaggle dataset https:\/\/www.kaggle.com\/cdeotte\/rapids - takes 1 min\nimport sys\n!cp ..\/input\/rapids\/rapids.0.18.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + [\"\/opt\/conda\/envs\/rapids\/lib\/python3.6\"] + [\"\/opt\/conda\/envs\/rapids\/lib\/python3.6\/site-packages\"] + sys.path\n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","7830416d":"# 2D with TSNE\nimport cudf, cuml\nfrom cuml.manifold import TSNE","aea71c6b":"data = data_health.copy()\ndata","0f57d5ca":"tsne = TSNE(n_components=2)\ndata_2D = tsne.fit_transform(data)\nplt.scatter(data_2D[:,0], data_2D[:,1], c = data['target'], s = 40)","b19a8abf":"import plotly.express as px\nfig = px.scatter(\n    x=data_2D[:, 0], y=data_2D[:, 1], \n    color=data['target'], hover_name=data['target'].astype('str'), height=700)\nfig.show()","eee0922a":"# # Thanks to https:\/\/www.kaggle.com\/tunguz\/mnist-2d-t-sne-with-rapids\n# # You need Add Kaggle dataset https:\/\/www.kaggle.com\/cdeotte\/rapids - takes 1 min\n# import sys\n# !cp ..\/input\/rapids\/rapids.0.18.0 \/opt\/conda\/envs\/rapids.tar.gz\n# !cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz\n# sys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + [\"\/opt\/conda\/envs\/rapids\/lib\/python3.6\"] + [\"\/opt\/conda\/envs\/rapids\/lib\/python3.6\/site-packages\"] + sys.path\n# !cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","a2dbb820":"# Thanks to https:\/\/www.kaggle.com\/tunguz\/mnist-2d-t-sne-with-rapids\n# Thanks to https:\/\/www.kaggle.com\/xhlulu\/disasternlp-t-sne-with-tfhub-rapids-plotly\n#import cudf, cuml\nfrom cuml.manifold import UMAP\ndata = data_health.copy()\numap3d = UMAP(n_components=3)\ndata_3D = umap3d.fit_transform(data)","e0308c09":"import plotly.express as px\nfig = px.scatter_3d(\n    x=data_3D[:, 0], y=data_3D[:, 1], z=data_3D[:, 2], \n    color=data_health['target'], hover_name=data_health['target'].astype('str'), size_max=3, height=700)\nfig.update_traces(marker=dict(size=3))\nfig.show()","3f27f08a":"!pip install sweetviz","59b9046d":"import sweetviz as sv\ndata_report = sv.analyze([data_health, 'Data'])","7a4f8b30":"# Save result to Output section of the notebook\ndata_report.show_html()","0be46199":"data_health_sex = data_health.copy()\ndata_health_sex['sex'] = data_health_sex['sex'].map({1 : \"male\", 0 : \"female\"})\ndata_health_sex","47852a52":"my_report = sv.compare_intra(data_health_sex, data_health_sex[\"sex\"] == \"male\", [\"Male\", \"Female\"])","f3cfbd59":"# Save result to Output section of the notebook\nmy_report.show_html()","90f9666b":"# Dickey Fuller Test Function\ndef test_stationarity(timeseries):\n    # Perform Dickey-Fuller test:\n    from statsmodels.tsa.stattools import adfuller\n    print('Results of Dickey-Fuller Test:')\n    print (\"==============================================\")\n    \n    dftest = adfuller(timeseries, autolag='AIC')\n    \n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#lags Used', 'Number of Observations Used'])\n    \n    for key, value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    \n    print(dfoutput)","7f4a60fb":"timeseries = data_water['NH4'].replace(np.nan, 0)","22c7768e":"# Stationarity Check - Lets do a quick check on Stationarity with Dickey Fuller Test \n# Convert the DF to series first\ntest_stationarity(timeseries)","b4314e8f":"# Dickey Fuller Test Function with plots\n# Thanks to https:\/\/www.kaggle.com\/abhi170599\/bitcoin-price-prediction-with-machine-learning\ndef evaluate_stationarity(timeseries, timeframe=7):\n    from statsmodels.tsa.stattools import adfuller, acf, pacf\n    roll_mean = timeseries.rolling(window=timeframe).mean()\n    roll_std  = timeseries.rolling(window=timeframe).std()\n    \n    # plot the rolling statistics\n    orig = plt.plot(timeseries,color='blue',label='Original')\n    mean = plt.plot(roll_mean,color='red',label='Rolling Mean')\n    std  = plt.plot(roll_std,color='black',label='Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean and Standard Deviation')\n    plt.show(block=False)\n    \n    # perform ADF test\n    df_test = adfuller(timeseries,autolag='AIC')\n    df_output = pd.Series(df_test[0:4],index=['Test Statistics','p-value','#lags used','Number of Observations Used'])\n    for k,v in df_test[4].items():\n        df_output['Critical Value ({})'.format(k)]=v\n    print(df_output)    ","09cdf8a3":"evaluate_stationarity(timeseries, timeframe=30)","202057ac":"evaluate_stationarity(timeseries, timeframe=7)","7c37f136":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","04a52177":"reduce_mem_usage(data_titanic)","3e3fe3d7":"def rstr(df): \n    return df.apply(lambda x: [x.unique()])\nprint(rstr(data_titanic))","4d671e8b":"data_titanic.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))","0af6d045":"data_water[['ID_Station', 'Distance']].groupby(by=['Distance']).mean()","58f02c01":"def fe_creation(df):\n    df['age2'] = df['age']\/\/10\n    df['trestbps2'] = df['trestbps']\/\/10\n    df['chol2'] = df['chol']\/\/40\n    df['thalach2'] = df['thalach']\/\/40\n    df['oldpeak2'] = df['oldpeak']\/\/0.4\n    for i in ['sex', 'age2', 'fbs', 'restecg', 'exang','thal', ]:\n        for j in ['cp','trestbps2', 'chol2', 'thalach2', 'oldpeak2', 'slope', 'ca']:\n            df[i + \"_\" + j] = df[i].astype('str') + \"_\" + df[j].astype('str')\n    return df","5e7b90d5":"pd.set_option('max_columns', 70)\ndata_health = fe_creation(data_health)\ndata_health.head(3)","7f905c17":"from sklearn.preprocessing import LabelEncoder\ndef df_encoding(df):\n    numerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    categorical_columns = []\n    features = df.columns.values.tolist()\n    for col in features:\n        if df[col].dtype in numerics: continue\n        categorical_columns.append(col)\n    print('Categorical columns:', categorical_columns)\n\n    for col in categorical_columns:\n        if col in df.columns:\n            le = LabelEncoder()\n            le.fit(list(df[col].astype(str).values))\n            df[col] = le.transform(list(df[col].astype(str).values))\n    \n    return df","b13a8519":"data_health = df_encoding(data_health)","cf3de073":"data_health.info()","aa34e04d":"data_health.describe()","33ad9ab5":"# In progress... (from my notebook for NY Taxi competition)\n# df ['jrk'] = 0\n# df.loc[(df['pickup_longitude'].between(-73.7841,-73.7721)) & \n#    (df['pickup_latitude'].between(40.6613, 40.6213)),'jrk'] = 1\n# df.loc[(df['dropoff_longitude'].between(-73.7841, -73.7721)) &\n#    (df['dropoff_latitude'].between(40.6613, 40.6213)),'jrk'] = 1","9939c615":"# In progress... (from my notebook for NY Taxi competition)\n# outliers=np.array([False]*len(data))\n# Out = pd.DataFrame()\n# Out = Out.assign(outliers=outliers)\n# outliers[data.fare_amount > 100]=True\n# outliers[data.fare_amount < 2.8]=True\n# outliers[data.distance > 5]=True\n# outliers[data.distance < 0]=True\n# outliers[data.passenger_count < 1]=True\n# outliers[data.passenger_count > 6]=True\n# outliers[data.pickup_longitude > -72.9]=True\n# outliers[data.pickup_longitude < -74.5]=True\n# outliers[data.pickup_latitude > 42]=True\n# outliers[data.pickup_latitude < 40]=True\n# outliers[data.dropoff_longitude > -72.9]=True\n# outliers[data.dropoff_longitude < -74.5]=True\n# outliers[data.pickup_latitude > 42]=True\n# outliers[data.pickup_latitude < 40]=True\n# print('There are total %d entries of ouliers'% sum(outliers))\n# X = data[outliers==False].iloc[:,3:F]\n# z = data[outliers==False]['fare_amount'].values","becda151":"train = data_health.copy()\ntarget = train.pop('target')\ntrain.head(2)","125678ed":"num_features_opt = 25   # the number of features that we need to choose as a result\nnum_features_max = 35   # the somewhat excessive number of features, which we will choose at each stage\nfeatures_best = []","aac04158":"# Threshold for removing correlated variables\nthreshold = 0.9\n\ndef highlight(value):\n    if value > threshold:\n        style = 'background-color: pink'\n    else:\n        style = 'background-color: palegreen'\n    return style\n\n# Absolute value correlation matrix\ncorr_matrix = data_health.corr().abs().round(2)\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper.style.format(\"{:.2f}\").applymap(highlight)","540f74e5":"# Select columns with correlations above threshold\ncollinear_features = [column for column in upper.columns if any(upper[column] > threshold)]\nfeatures_filtered = data_health.drop(columns = collinear_features)\nprint('The number of features that passed the collinearity threshold: ', features_filtered.shape[1])\nfeatures_best.append(features_filtered.columns.tolist())","846b6699":"from sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\nlsvc = LinearSVC(C=0.1, penalty=\"l1\", dual=False).fit(train, target)\nmodel = SelectFromModel(lsvc, prefit=True)\nX_new = model.transform(train)\nX_selected_df = pd.DataFrame(X_new, columns=[train.columns[i] for i in range(len(train.columns)) if model.get_support()[i]])\nfeatures_best.append(X_selected_df.columns.tolist())","1b754a10":"from sklearn.linear_model import LassoCV\nfrom sklearn.feature_selection import SelectFromModel\nlasso = LassoCV(cv=3).fit(train, target)\nmodel = SelectFromModel(lasso, prefit=True)\nX_new = model.transform(train)\nX_selected_df = pd.DataFrame(X_new, columns=[train.columns[i] for i in range(len(train.columns)) if model.get_support()[i]])\nfeatures_best.append(X_selected_df.columns.tolist())","359f6a45":"# Visualization from https:\/\/towardsdatascience.com\/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e\n# but to k='all'\nfrom sklearn.feature_selection import SelectKBest, chi2\nbestfeatures = SelectKBest(score_func=chi2, k='all')\nfit = bestfeatures.fit(train, target)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(train.columns)\n\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Feature','Score']  #naming the dataframe columns\nfeatures_best.append(featureScores.nlargest(num_features_max,'Score')['Feature'].tolist())\nprint(featureScores.nlargest(len(dfcolumns),'Score')) ","a36bb643":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nrfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=num_features_max, step=10, verbose=5)\nrfe_selector.fit(train, target)\nrfe_support = rfe_selector.get_support()\nrfe_feature = train.loc[:,rfe_support].columns.tolist()\nfeatures_best.append(rfe_feature)\nprint(str(len(rfe_feature)), 'selected features')","0616fe32":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\nembeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=200), threshold='1.25*median')\nembeded_rf_selector.fit(train, target)\nembeded_rf_support = embeded_rf_selector.get_support()\nembeded_rf_feature = train.loc[:,embeded_rf_support].columns.tolist()\nfeatures_best.append(embeded_rf_feature)\nprint(str(len(embeded_rf_feature)), 'selected features')","96fa3923":"# Check whether all features have a sufficiently different meaning\nfrom sklearn.feature_selection import VarianceThreshold\nselector = VarianceThreshold(threshold=10)\nselector.fit_transform(data_health)\nVarianceThreshold_feature = list(np.array(data_health.columns)[selector.get_support(indices=False)])\nfeatures_best.append(VarianceThreshold_feature)\nprint(str(len(VarianceThreshold_feature)), 'selected features')","1b222b39":"features_best","57159762":"# The element is in at least one list of optimal features\nmain_cols_max = features_best[0]\nfor i in range(len(features_best)-1):\n    main_cols_max = list(set(main_cols_max) | set(features_best[i+1]))\nmain_cols_max","09537326":"# The element is in all lists of optimal features\nmain_cols_min = features_best[0]\nfor i in range(len(features_best)-1):\n    main_cols_min = list(set(main_cols_min).intersection(set(features_best[i+1])))\nmain_cols_min","f318d3e2":"# Most common items in all lists of optimal features\nmain_cols = []\nmain_cols_opt = {feature_name : 0 for feature_name in data_health.columns.tolist()}\nfor i in range(len(features_best)):\n    for feature_name in features_best[i]:\n        main_cols_opt[feature_name] += 1\ndf_main_cols_opt = pd.DataFrame.from_dict(main_cols_opt, orient='index', columns=['Num'])\ndf_main_cols_opt.sort_values(by=['Num'], ascending=False).head(num_features_opt)","4d8640da":"main_cols = df_main_cols_opt.nlargest(num_features_opt, 'Num').index.tolist()\nif not 'target' in main_cols:\n    main_cols.append('target')\nmain_cols","d70baeb0":"data_titanic_num.head(3)","8818efde":"X = data_titanic_num.copy()\nz = X.pop('Survived')","2b6e58bd":"# Split data to 2 (or more - train\/valid\/test) subsets\nfrom sklearn.model_selection import train_test_split, ShuffleSplit, KFold\n\n# Split training set to validation subsets\nXtrain, Xval, Ztrain, Zval = train_test_split(X, z, test_size=0.2, random_state=0)\nXtrain.shape","1c4e2292":"# Random permutation cross-validator\ncv_train = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)","d2e8b700":"# K-Folds cross-validator\nkf = KFold(n_splits=5, shuffle = True)","aee799f6":"import xgboost as xgb\nfrom xgboost import plot_tree\nfrom matplotlib.pylab import rcParams\nfrom sklearn.model_selection import GridSearchCV\n\ndef xgb_training(train, target_train, cv_train):\n    # XGB Classifier Training\n    eval_metric_model = 'error'\n    xgbr = xgb.XGBClassifier() \n    param_grid_xgb = {'objective': ['binary:hinge'],\n                     'eval_metric': [eval_metric_model],\n                     'random_state': [0]}\n\n    # Training model\n    xgb_CV = GridSearchCV(xgbr, param_grid=param_grid_xgb, cv=cv_train, verbose=False)\n    xgb_CV.fit(train, target_train)\n    xgbp = xgb_CV.best_params_\n    print(xgbp)\n\n    # Feature importance diagrame\n    xgb_model = xgb.XGBClassifier(objective='binary:hinge',\n                                  eval_metric='error',\n                                  random_state=0).fit(train, target_train)\n    fig =  plt.figure(figsize = (15,15))\n    axes = fig.add_subplot(111)\n    xgb.plot_importance(xgb_model,ax = axes,height = 0.5)\n    plt.show();plt.close()\n    \n    # Plot single tree\n    plt.rcParams.update({'font.size': 8})\n    rcParams['figure.figsize'] = 80,40\n    plot_tree(xgb_model, num_trees=0, rankdir='LR')\n    ax = plot_tree(xgb_model, node_attr={'shape': 'record', 'height': '.1'}, show_info = 'split_gain')\n    plt.show()\n    \n    return xgb_CV, xgbp, xgb_model","532a6a39":"xgb_CV, xgbp, xgb_model = xgb_training(Xtrain, Ztrain, cv_train)","720c2a26":"#print(xgbp)","b18ef8ae":"# # In progress...\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import accuracy_score\n# def vr_training(train, target_train, valid, target_valid, cv_train, result, knnp, rfp, xgbp):\n#     # Voting Classifier Training\n\n#     # Training model\n#     estimators = [('knn', KNeighborsClassifier()),\n#                   ('etr', ExtraTreesClassifier()),\n#                   ('rf', RandomForestClassifier()),\n#                   ('xgb', xgb.XGBClassifier(objective='binary:hinge',\n#                                             eval_metric='error',\n#                                             learning_rate=xgbp['learning_rate'],\n#                                             max_depth=xgbp['max_depth'],\n#                                             n_estimators=xgbp['n_estimators'],\n#                                             reg_lambda=xgbp['reg_lambda'],\n#                                             random_state=random_state))]\n#     model_vr = VotingClassifier(estimators=estimators)\n#     vr = GridSearchCV(estimator=model_vr, param_grid={}, cv=cv_train)\n#     vr.fit(train, target_train)\n\n#     # Prediction for training data\n#     y_train_vr = vr.predict(train)\n\n#     # Accuracy of model\n#     acc = round(accuracy_score(target_train, y_train_vr)*100,1)\n#     print(f'Accuracy of Voting Classifier model training is {acc}')\n\n#     # Print rounded acc to 2 decimal values after the text\n#     y_val_vr = vr.predict(valid)\n#     acc_valid = round(accuracy_score(target_valid, y_val_vr)*100,1)\n#     print(f'Accuracy of Voting Classifier model prediction for valid dataset is {acc_valid} \\n')\n    \n#     return vr","dcbaa2bb":"#!pip install --upgrade scikit-learn==0.23.2\n#!pip install pycaret","58cc19d6":"#from pycaret.classification import *","3f83593b":"# model = setup(data = data_titanic_num, \n#               target = 'Survived',\n#               numeric_imputation = 'mean',\n#               imputation_type='iterative',\n#               categorical_features = ['Sex','Embarked'],\n#               normalize = True,\n#               silent = True\n#              )","59f160f3":"#compare_models()","e48ee83f":"# In progress...\nfrom sklearn.metrics import confusion_matrix\ndef plot_cm3(target_train, train_pred, target_valid, valid_pred, target_test, test_pred, title, figsize=(12,3)):\n    # Building the 3 confusion matrices train\/valid\/test\n    \n    def cm_calc(y_true, y_pred):\n        cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n        cm_sum = np.sum(cm, axis=1, keepdims=True)\n        cm_perc = cm \/ cm_sum.astype(float) * 100\n        annot = np.empty_like(cm).astype(str)\n        nrows, ncols = cm.shape\n        for i in range(nrows):\n            for j in range(ncols):\n                c = cm[i, j]\n                p = cm_perc[i, j]\n                if i == j:\n                    s = cm_sum[i]\n                    annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n                elif c == 0:\n                    annot[i, j] = ''\n                else:\n                    annot[i, j] = '%.1f%%\\n%d' % (p, c)\n        cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n        cm.index.name = 'Actual'\n        cm.columns.name = 'Predicted'\n        return cm, annot\n\n    \n    # Building the confusion matrices\n    # Tip.\n    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=figsize, sharex=True)\n#     font_size = 11\n#     SMALL_SIZE = 10\n#     MEDIUM_SIZE = 12\n#     BIGGER_SIZE = 14    \n#     plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n#     plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n#     plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n#     plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n#     plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n#     plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n#     plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n#     plt.rc('figure', titlesize=BIGGER_SIZE) \n#     #plt.rcParams.update({'font.size': font_size}) - alternative\n    \n    # Training data\n    ax = axes[0]\n    ax.set_title(\"for training data\")\n    cm0, annot0 = cm_calc(target_train, train_pred)    \n    sns.heatmap(cm0, cmap= \"YlGnBu\", annot=annot0, fmt='', ax=ax)\n    \n    # Validation data\n    ax = axes[1]\n    ax.set_title(\"for validation data\")\n    cm1, annot1 = cm_calc(target_valid, valid_pred)\n    sns.heatmap(cm1, cmap= \"YlGnBu\", annot=annot1, fmt='', ax=ax)\n\n    # Test data\n    ax = axes[2]\n    cm2, annot2 = cm_calc(target_test, test_pred)\n    sns.heatmap(cm2, cmap= \"YlGnBu\", annot=annot2, fmt='', ax=ax)\n    \n    fig.suptitle(title, y=1.05)\n    plt.show()","e131c464":"from sklearn.metrics import confusion_matrix\ndef plot_cm2(target_train, train_pred, target_valid, valid_pred, title, figsize=(8,3)):\n    # Building the 2 confusion matrices train\/valid\n    \n    def cm_calc(y_true, y_pred):\n        cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n        cm_sum = np.sum(cm, axis=1, keepdims=True)\n        cm_perc = cm \/ cm_sum.astype(float) * 100\n        annot = np.empty_like(cm).astype(str)\n        nrows, ncols = cm.shape\n        for i in range(nrows):\n            for j in range(ncols):\n                c = cm[i, j]\n                p = cm_perc[i, j]\n                if i == j:\n                    s = cm_sum[i]\n                    annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n                elif c == 0:\n                    annot[i, j] = ''\n                else:\n                    annot[i, j] = '%.1f%%\\n%d' % (p, c)\n        cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n        cm.index.name = 'Actual'\n        cm.columns.name = 'Predicted'\n        return cm, annot\n\n    \n    # Building the confusion matrices\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=figsize, sharex=True)\n\n    # Training data\n    ax = axes[0]\n    ax.set_title(\"for training data\")\n    cm0, annot0 = cm_calc(target_train, train_pred)    \n    sns.heatmap(cm0, cmap= \"YlGnBu\", annot=annot0, fmt='', ax=ax)\n    \n    # Validation data\n    ax = axes[1]\n    ax.set_title(\"for validation data\")\n    cm1, annot1 = cm_calc(target_valid, valid_pred)\n    sns.heatmap(cm1, cmap= \"YlGnBu\", annot=annot1, fmt='', ax=ax)\n\n    \n    fig.suptitle(title, y=1.05)\n    plt.show()","19562aa3":"train_pred = xgb_model.predict(Xtrain)\nvalid_pred = xgb_model.predict(Xval)\nplot_cm2(Ztrain, train_pred, Zval, valid_pred, \"Confusion matrices for dataset data_titanic_num\", figsize=(8,3))","b16a2339":"from sklearn.model_selection import learning_curve\n\n# Thanks to https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py\ndef plot_learning_curve(estimator, title, X, y, cv=None, axes=None, ylim=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5), random_state=0):\n    \"\"\"\n    Generate 2 plots: \n    - the test and training learning curve, \n    - the training samples vs fit times curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    axes : array of 3 axes, optional (default=None)\n        Axes to use for plotting the curves.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n          - None, to use the default 5-fold cross-validation,\n          - integer, to specify the number of folds.\n          - :term:`CV splitter`,\n          - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer\/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    train_sizes : array-like, shape (n_ticks,), dtype float or int\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the dtype is float, it is regarded as a\n        fraction of the maximum size of the training set (that is determined\n        by the selected validation method), i.e. it has to be within (0, 1].\n        Otherwise it is interpreted as absolute sizes of the training sets.\n        Note that for classification the number of samples usually have to\n        be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    \n    random_state : random_state\n    \n    \"\"\"\n    plt.figure(figsize=(18,5))\n    plt.title(title)\n    if ylim is not None:\n        plt.set_ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n\n    if cv is None:\n        cv = ShuffleSplit(n_splits=cv_n_split, test_size=0.2, random_state=random_state)\n    \n    train_sizes, train_scores, test_scores = \\\n        learning_curve(estimator=estimator, X=X, y=y, cv=cv,\n                       train_sizes=train_sizes, return_times=False)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    # Plot learning curve\n    plt.grid()\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    plt.legend(loc=\"best\")\n\n    plt.show()\n    return","609a2d8f":"# Draw plot_learning_curve\nlc_title = \"Learning curve plot for the xgboost model for dataset data_titanic_num\"\nplot_learning_curve(xgb_model, lc_title, Xtrain, Ztrain, cv=cv_train)","2a53c994":"from sklearn.model_selection import validation_curve\n\n# Thanks to https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/\ndef plot_validation_curve(estimator, title, X, y, cv=None, axes=None, ylim=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5), random_state=0):\n    \"\"\"\n    Generate 2 plots: \n    - the valid and training learning curve, \n    - the training samples vs fit times curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    axes : array of 3 axes, optional (default=None)\n        Axes to use for plotting the curves.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n          - None, to use the default 5-fold cross-validation,\n          - integer, to specify the number of folds.\n          - :term:`CV splitter`,\n          - An iterable yielding (train, valid) splits as arrays of indices.\n\n        For integer\/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    train_sizes : array-like, shape (n_ticks,), dtype float or int\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the dtype is float, it is regarded as a\n        fraction of the maximum size of the training set (that is determined\n        by the selected validation method), i.e. it has to be within (0, 1].\n        Otherwise it is interpreted as absolute sizes of the training sets.\n        Note that for classification the number of samples usually have to\n        be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    \n    random_state : random_state\n    \n    \"\"\"\n    plt.figure(figsize=(18,5))\n    plt.title(title)\n    if ylim is not None:\n        plt.set_ylim(*ylim)\n    plt.xlabel(\"max_depth from 3 to 7\")\n    plt.ylabel(\"Score\")\n\n    if cv is None:\n        cv = ShuffleSplit(n_splits=cv_n_split, test_size=0.2, random_state=random_state)\n    \n    train_scores, valid_scores = \\\n        validation_curve(estimator=estimator, X=X, y=y, param_name=\"max_depth\", param_range=[3, 4, 5, 6, 7], cv=cv)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    valid_scores_mean = np.mean(valid_scores, axis=1)\n    valid_scores_std = np.std(valid_scores, axis=1)\n\n    # Plot learning curve\n    plt.grid()\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    plt.fill_between(train_sizes, valid_scores_mean - valid_scores_std,\n                         valid_scores_mean + valid_scores_std, alpha=0.1,\n                         color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    plt.plot(train_sizes, valid_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    plt.legend(loc=\"best\")\n\n    plt.show()\n    return","35b8381d":"# Draw plot_validation_curve\nvc_title = \"Validation curve plot for the xgboost model for dataset data_titanic_num\"\nplot_validation_curve(xgb_model, vc_title, Xtrain, Ztrain, cv=cv_train)","aed6f4e6":"from sklearn.metrics import roc_curve, classification_report\n\ndef roc_auc_plot(model, train, target_train, valid, target_valid):\n    # Draw ROC-UAC plots and print report: precision, recall, f1-score, support\n\n    def roc_plot(model, target, df, title):\n        # Calc and draw ROC plot for df and target\n        sns.set(font_scale=1.5)\n        sns.set_color_codes(\"muted\")\n        plt.figure(figsize=(5, 4))\n        fpr, tpr, thresholds = roc_curve(target, model.predict_proba(df)[:,1], pos_label=1)\n        plt.plot(fpr, tpr, lw=2, label='ROC curve ')\n        plt.plot([0, 1], [0, 1])\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title(title)\n        plt.show()        \n\n    # Report\n    print('Classification report for training data\\n',\n          classification_report(target_train, model.predict(train), target_names=['0', '1']))\n    print('Classification report for validation data\\n',\n          classification_report(target_valid, model.predict(valid), target_names=['0', '1']))\n\n    roc_plot(model, target_train, train, \"ROC curve for training data\")\n    roc_plot(model, target_valid, valid, \"ROC curve for validation data\")","a32a97e2":"# Draw ROC-AUC plot\nroc_auc_plot(xgb_model, Xtrain, Ztrain, Xval, Zval)","b5a1f74a":"# In progress...\ndef plot_prediction(df, cols_y_list, target, forecast, log_y=False):\n    # Draws a plot - the features cols_y_list (y) and dates_x dates from the dataframe df\n    # all data are scaling into [0,1]\n    # target data drawing as black wide vertical lines in the dates with target = 1\n    # predicted data drawing as red thin vertical lines in the dates with predicted data = 1\n    # with log_y = False or True\n    \n    # Get data for y with input features data and scaling it into [0,1]\n    df = df.reset_index(drop=False)\n    xf = df.index.tolist()\n    \n    # The feature with the largest importance\n    yf = df[cols_y_list[0]]\n    fig = go.Figure(layout=go.Layout(width=900, height=600))\n\n    # Target\n    target_index_list = [i for i, x in enumerate(target) if x == 1]\n    for i in range(len(target_index_list)):\n        target_index_i =  target_index_list[i]\n        fig.add_shape(dict(type=\"line\", x0=target_index_i, y0=0, x1=target_index_i, y1=1, \n                           line=dict(color=\"black\", width=3)))\n    \n    # Predicted data\n    predicted_index_list = [i for i, x in enumerate(forecast) if x == 1]\n    for i in range(len(predicted_index_list)):\n        pred_index_i =  predicted_index_list[i]\n        fig.add_shape(dict(type=\"line\", x0=pred_index_i, y0=0, x1=pred_index_i, y1=1, \n                           line=dict(color=\"yellow\", width=3, dash=\"dot\")))\n    \n    # Other features\n    for i in range(len(cols_y_list)):\n        yf = df[cols_y_list[i]]\n        fig.add_trace(go.Scatter(x=xf, y=(yf-yf.min())\/(yf.max()-yf.min()), mode='lines', name=cols_y_list[i]))\n\n    fig.show()\n\n# Result visualization\n#plot_prediction(data, [], target, prediction)","c15cdbea":"import shap\nshap_values = shap.TreeExplainer(xgb_model).shap_values(Xtrain)\nshap.summary_plot(shap_values, Xtrain)","fbc40422":"shap.dependence_plot(\"Age\", shap_values, Xtrain)","e4bb3947":"shap_interaction_values = shap.TreeExplainer(xgb_model).shap_interaction_values(Xtrain.iloc[:,:])\nshap.summary_plot(shap_interaction_values, Xtrain.iloc[:,:])","23e48c3a":"shap_values = shap.TreeExplainer(xgb_model).shap_values(Xval)\nshap_interaction_values = shap.TreeExplainer(xgb_model).shap_interaction_values(Xval)\nshap.summary_plot(shap_values, Xval, plot_type=\"bar\")","e693650d":"def data_force_plot_n(model, X):\n\n    shap.initjs()\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(X)\n    \n    return shap.force_plot(explainer.expected_value, shap_values[0,:], X.iloc[0,:])","ce3d0577":"data_force_plot_n(xgb_model, Xval)","b97879eb":"def data_force_plot_all(model, X):\n\n    shap.initjs()\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(X)\n    \n    return shap.force_plot(explainer.expected_value, shap_values[0:len(Xval),:], X.iloc[0:len(X),:])","a7e0d918":"data_force_plot_all(xgb_model, Xval)","5eef2110":"# From https:\/\/www.kaggle.com\/vbmokin\/titanic-top-score-one-line-of-the-prediction\ntraindf = pd.read_csv('..\/input\/titanic\/train.csv').set_index('PassengerId')\ntestdf = pd.read_csv('..\/input\/titanic\/test.csv').set_index('PassengerId')\ndf = pd.concat([traindf, testdf], axis=0, sort=False)\ndf['Title'] = df.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\ndf['IsWomanOrBoy'] = ((df.Title == 'Master') | (df.Sex == 'female'))\ndf['LastName'] = df.Name.str.split(',').str[0]\nfamily = df.groupby(df.LastName).Survived\ndf['WomanOrBoyCount'] = family.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).count())\ndf['WomanOrBoyCount'] = df.mask(df.IsWomanOrBoy, df.WomanOrBoyCount - 1, axis=0)\ndf['FamilySurvivedCount'] = family.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).sum())\ndf['FamilySurvivedCount'] = df.mask(df.IsWomanOrBoy, df.FamilySurvivedCount - df.Survived.fillna(0), axis=0)\ndf['WomanOrBoySurvived'] = df.FamilySurvivedCount \/ df.WomanOrBoyCount.replace(0, np.nan)\ndf['Alone'] = (df.WomanOrBoyCount == 0)\ndf = pd.concat([df.WomanOrBoySurvived.fillna(0), df.Alone, df.Sex.replace({'male': 0, 'female': 1})], axis=1)\ntest_x = df.loc[testdf.index]\ntest_x['Survived'] = (((test_x.WomanOrBoySurvived <= 0.238) & (test_x.Sex > 0.5) & (test_x.Alone > 0.5)) | \\\n          ((test_x.WomanOrBoySurvived > 0.238) & \\\n           ~((test_x.WomanOrBoySurvived > 0.55) & (test_x.WomanOrBoySurvived <= 0.633))))\npd.DataFrame({'Survived': test_x['Survived'].astype(int)}, \\\n             index=testdf.index).reset_index().to_csv('submission.csv', index=False)","d03f5205":"### Tip 7.6. Explanation model predictions with SHAP<a class=\"anchor\" id=\"7.6\"><\/a>","748f4f82":"### Tip 5.10. FS by the SelectFromModel with LinearSVC <a class=\"anchor\" id=\"5.8.2\"><\/a>","7071e72f":"### Competition [Titanic: Machine Learning from Disaster](https:\/\/www.kaggle.com\/c\/titanic)","8cdd2d68":"## It's done 34 tips: 1, 2, 3.1, 3.2, 4.1-4.9, 5.1-5.5, 5.8-5.15, 6.1, 6.2, 7.1-7.4, 7.6, 8\n\n### Added Tip 4.9. Time Series Stationarity Check","0e93a3e4":"### Tip 6.2. Xgboost model training with GridSearchCV and Plot tree<a class=\"anchor\" id=\"6.2\"><\/a>","9bc3b9bc":"## 2. Data download<a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","b998976f":"### Tip 5.11. FS by the SelectFromModel with Lasso <a class=\"anchor\" id=\"5.8.3\"><\/a>","cd4e54c4":"#### See more in the notebook [50 Tips: Data Science (tabular data) for beginner](https:\/\/www.kaggle.com\/vbmokin\/50-tips-data-science-tabular-data-for-beginner)","870063e3":"## 3. Auxiliary functions<a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","191da516":"Thanks to [documentation of SweetViz](https:\/\/pypi.org\/project\/sweetviz\/)","6545a857":"### Tip 7.3. Drawing validation_curve plot<a class=\"anchor\" id=\"7.3\"><\/a>","2ed27d37":"## 6. Modeling<a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","b18abad4":"This code for AutoViz used EDA tool from the kernel [Data Visualization in just one line of code!!](https:\/\/www.kaggle.com\/nareshbhat\/data-visualization-in-just-one-line-of-code)","d37df3b8":"## 1. Import main libraries<a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","f50359ba":"### Tip 5.9. FS with the Pearson correlation<a class=\"anchor\" id=\"5.8.1\"><\/a>","78f96af2":"### Tip 5.4. Creation double and triple features <a class=\"anchor\" id=\"5.4\"><\/a>","9a2ee549":"### Tip 4.4. Determining the number of clusters (Kmeans)<a class=\"anchor\" id=\"4.4\"><\/a>","490a2e84":"In progress...","63abb201":"### Tip 3.1. Time is taken for notebook execution (in H:M:S)<a class=\"anchor\" id=\"3.1\"><\/a>","9e3c3399":"### Tip 5.8 (begin). Feature selection <a class=\"anchor\" id=\"5.8\"><\/a>","eeb82a1a":"### Tip 6.1. Splitting data with ShuffleSplit & KFold<a class=\"anchor\" id=\"6.1\"><\/a>","eb5cddb2":"### Tip 4.9. Time Series Stationarity Check<a class=\"anchor\" id=\"4.9\"><\/a>","e64dcfad":"#### See more in the notebook [50 Tips: Data Science (tabular data) for beginner](https:\/\/www.kaggle.com\/vbmokin\/50-tips-data-science-tabular-data-for-beginner)","508566d2":"### Tip 5.14. FS by the Recursive Feature Elimination (RFE) with Random Forest<a class=\"anchor\" id=\"5.8.6\"><\/a>","a4761fa5":"### Tip 6.3. Ensemble of models on basic of the xgboost<a class=\"anchor\" id=\"6.3\"><\/a>","2c1bfda7":"### Tip 4.3. PCA and 3D plot <a class=\"anchor\" id=\"4.3\"><\/a>","6fc8eb9c":"## 8. BONUS<a class=\"anchor\" id=\"8\"><\/a>\n\n[Back to Table of Contents](#0.1)","f9583d96":"### Tip 6.4. Modeling with PyCaret<a class=\"anchor\" id=\"6.4\"><\/a>","36c6afd8":"#### See more in the notebook [50 Tips: Data Science (tabular data) for beginner](https:\/\/www.kaggle.com\/vbmokin\/50-tips-data-science-tabular-data-for-beginner)","fcb8cce6":"I hope you find this notebook useful and enjoyable.\n\nYour comments and feedback are most welcome.\n\n[Go to Top](#0)","25ce6ebf":"### Tip 4.7. Automatic EDA (SweetViz)<a class=\"anchor\" id=\"4.7\"><\/a>","b0e1258e":"### Tip 5.8 (end). Selection the best features<a class=\"anchor\" id=\"5.8.8\"><\/a>","9f4e6723":"### Tip 4.2. Automatic EDA (Pandas-profiling)<a class=\"anchor\" id=\"4.2\"><\/a>","45d4c6d2":"### Tip 5.6. Creation feature with pandas.between <a class=\"anchor\" id=\"5.6\"><\/a>","56361745":"## 5. FE<a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","539be5b4":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## Table of Contents\n\n1. [Import main libraries](#1)\n1. [Data download](#2)\n1. [Auxiliary functions](#3)\n    - [Tip 3.1. Time is taken for notebook execution (in H:M:S)](#3.1)\n    - [Tip 3.2. Time is taken for notebook execution (with smart progress meter)](#3.2)\n1. [EDA & Data cleaning](#4)\n    - [Tip 4.1. Automatic EDA (AutoViz)](#4.1)\n    - [Tip 4.2. Automatic EDA (Pandas-profiling)](#4.2)\n    - [Tip 4.3. PCA and 3D plot](#4.3)\n    - [Tip 4.4. Determining the number of clusters (Kmeans)](#4.4)    \n    - [Tip 4.5. TSNE with RAPIDS (2D with matlplotlib and plotly)](#4.5)\n    - [Tip 4.6. UMAP with RAPIDS (3D with plotly)](#4.6)\n    - [Tip 4.7. Automatic EDA (SweetViz)](#4.7)\n    - [Tip 4.8. SweetViz: comparing two subsets of the same dataframe (e.g. Male vs Female)](#4.8)\n    - [Tip 4.9. Time Series Stationarity Check](#4.9)    \n1. [FE](#5)\n    - [Tip 5.1. Reduce memory usage for DataFrame](#5.1)\n    - [Tip 5.2. Unique values for all columns in DataFrame](#5.2)\n    - [Tip 5.3. Data grouping in DataFrame](#5.3)\n    - [Tip 5.4. Creation double and triple features](#5.4)\n    - [Tip 5.5. Search and encoding categorical columns](#5.5)    \n    - [Tip 5.6. Creation feature with pandas.between](#5.6)\n    - [Tip 5.7. Filtering anomalous values](#5.7)\n    - [Tip 5.8 (begin). Feature selection](#5.8)\n     -  [Tip 5.9. FS with the Pearson correlation](#5.8.1)\n     -  [Tip 5.10. FS by the SelectFromModel with LinearSVC](#5.8.2) \n     -  [Tip 5.11. FS by the SelectFromModel with Lasso](#5.8.3) \n     -  [Tip 5.12. FS by the SelectKBest with Chi-2](#5.8.4)\n     -  [Tip 5.13. FS by the Recursive Feature Elimination (RFE) with Logistic Regression](#5.8.5) \n     -  [Tip 5.14. FS by the Recursive Feature Elimination (RFE) with Random Forest](#5.8.6)\n     -  [Tip 5.15. FS by the VarianceThreshold](#5.8.7)             \n    -  [Tip 5.8 (end). Selection the best features](#5.8.8)\n1. [Modeling](#6)\n    - [Tip 6.1. Splitting data with ShuffleSplit & KFold](#6.1)\n    - [Tip 6.2. Xgboost model training with GridSearchCV and Plot tree](#6.2)\n    - [Tip 6.3. Ensemble of models on basic of the xgboost](#6.3)\n    - [Tip 6.4. Modeling with PyCaret](#6.4)\n1. [Analysis and visualization of modeling results](#7)\n    - [Tip 7.1. Drawing confuse matrix](#7.1)\n    - [Tip 7.2. Drawing learning_curve plot](#7.2)\n    - [Tip 7.3. Drawing validation_curve plot](#7.3)\n    - [Tip 7.4. Drawing ROC_AUC plot](#7.4)\n    - [Tip 7.5. Drawing plot with prediction and target data (Plotly)](#7.5)\n    - [Tip 7.6. Explanation model predictions with SHAP](#7.6)    \n1. [BONUS](#8)\n    - [Tip 8.1. Submission data from DataFrame to Kaggle competition](#8.1)","395ea656":"There are many techniques for **selection features**, see example:\n- [sklearn library documentation](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.feature_selection)\n- [a collection of notebooks](https:\/\/www.kaggle.com\/vbmokin\/data-science-for-tabular-data-advanced-techniques#3)\n- my notebook [Titanic - Featuretools (automatic FE&FS)](https:\/\/www.kaggle.com\/vbmokin\/titanic-featuretools-automatic-fe-fs)\n- my notebook [Merging FE & Prediction - xgb, lgb, logr, linr](https:\/\/www.kaggle.com\/vbmokin\/merging-fe-prediction-xgb-lgb-logr-linr)","71317f9b":"Thanks to https:\/\/www.kaggle.com\/liananapalkova\/automated-feature-engineering-for-titanic-dataset","5d49d69a":"#### See more in the notebook [50 Tips: Data Science (tabular data) for beginner](https:\/\/www.kaggle.com\/vbmokin\/50-tips-data-science-tabular-data-for-beginner)","da36d021":"### Tip 3.2. Time is taken for notebook execution (with smart progress meter)<a class=\"anchor\" id=\"3.2\"><\/a>","68aadaa1":"**Conclusion**\n\nThe Test Statistics value is smaller than critical value. So we can reject the Null Hypothesis.\nHence Statistically (and obviously from the plot) the Time series is Stationary.","5dd48d43":"<a class=\"anchor\" id=\"0\"><\/a>\n# 50 Advanced Tips for Data Science for tabular data (in progress...)\n## Frequently used useful code for:\n* Data cleaning\n* FE\n* Modeling\n* Analysing, and visualization of modeling results\n* Prediction and submitting of modeling results\netc.\n\n### With BONUS - the short solution for Competition [Titanic: Machine Learning from Disaster](https:\/\/www.kaggle.com\/c\/titanic) with LB=0.80382 (Top 2%)\n\n### See also the notebook [50 Tips: Data Science (tabular data) for beginner](https:\/\/www.kaggle.com\/vbmokin\/50-tips-data-science-tabular-data-for-beginner)\n\nLater I will publish another notebook for EDA.","d3375a8b":"### Result: the file \"submission.csv\" gives LB = 0.80382 (Top 4%)","f3a15c9a":"### Tip 5.5. Search and encoding categorical columns <a class=\"anchor\" id=\"5.5\"><\/a>","4d580619":"### Tip 5.12. FS by the SelectKBest with Chi-2 <a class=\"anchor\" id=\"5.8.4\"><\/a>","95053f3d":"### Tip 5.13. FS by the Recursive Feature Elimination (RFE) with Logistic Regression<a class=\"anchor\" id=\"5.8.5\"><\/a>","09aedd5f":"This tip from the notebook [Heart Disease - Automatic AdvEDA & FE & 20 models](https:\/\/www.kaggle.com\/vbmokin\/heart-disease-automatic-adveda-fe-20-models)","5b71a96f":"### Tip 4.1. Automatic EDA (AutoViz)<a class=\"anchor\" id=\"4.1\"><\/a>","9e7c545c":"Thanks to:\n\n* FE from the https:\/\/www.kaggle.com\/liananapalkova\/automated-feature-engineering-for-titanic-dataset\n\n* Visualization from the https:\/\/www.kaggle.com\/vbmokin\/three-lines-of-code-for-titanic-top-20","b1eaae10":"### Tip 5.3. Data grouping in DataFrame<a class=\"anchor\" id=\"5.3\"><\/a>","3ea42a17":"The maximum value of Silhouette Score should be as close as possible to 1 (at least more than 0.7), otherwise, another clustering method should be used.","d9394734":"Thanks to [MNIST Original : 2D tSNE, 3D UMAP with RAPIDS](https:\/\/www.kaggle.com\/vbmokin\/mnist-original-2d-tsne-3d-umap-with-rapids)","16bf7de0":"## 4. EDA & Data cleaning<a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","b5c36b1c":"Thanks to:\n* https:\/\/www.kaggle.com\/sz8416\/6-ways-for-feature-selection\n* https:\/\/towardsdatascience.com\/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e","ed793a5d":"Thanks to:\n* https:\/\/www.kaggle.com\/sz8416\/6-ways-for-feature-selection\n* https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFE.html","c26a88c9":"Thanks to [MNIST Original : 2D tSNE, 3D UMAP with RAPIDS](https:\/\/www.kaggle.com\/vbmokin\/mnist-original-2d-tsne-3d-umap-with-rapids)","b847e30f":"### Tip 7.1. Drawing confuse matrix<a class=\"anchor\" id=\"7.1\"><\/a>","93623aaa":"### Tip 7.4. Drawing ROC_AUC plot<a class=\"anchor\" id=\"7.4\"><\/a>","b290c18c":"Thanks to [sklearn.feature_selection.VarianceThreshold](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold) - Feature selector that removes all low-variance features.","d7715676":"### Tip 5.7. Filtering anomalous values <a class=\"anchor\" id=\"5.7\"><\/a>","c46e83ab":"#### See more in the notebook [50 Tips: Data Science (tabular data) for beginner](https:\/\/www.kaggle.com\/vbmokin\/50-tips-data-science-tabular-data-for-beginner)","7272dad8":"In progress...","5c6c63e2":"## Acknowledgements\n\n### Datasets:\n* for Classification task solutions - competition's dataset [Titanic - Machine Learning from Disaster](https:\/\/www.kaggle.com\/c\/titanic)\n* for Classification task solutions - [Heart Disease UCI](https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci)\n* for Regression task solutions - my dataset [Ammonium prediction in river water](https:\/\/www.kaggle.com\/vbmokin\/ammonium-prediction-in-river-water)\n* from API for Regression task solutions - official data of COVID-19 in Ukraine (https:\/\/covid19.rnbo.gov.ua\/)\n* for NLP task - [NLP : Reports & News Classification](https:\/\/www.kaggle.com\/vbmokin\/nlp-reports-news-classification)\n\n### Notebooks:\n* [50 Tips: Data Science (tabular data) for beginner](https:\/\/www.kaggle.com\/vbmokin\/50-tips-data-science-tabular-data-for-beginner)\n* [Data Science for tabular data: Advanced Techniques](https:\/\/www.kaggle.com\/vbmokin\/data-science-for-tabular-data-advanced-techniques)\n* [EDA for tabular data: Advanced Techniques](https:\/\/www.kaggle.com\/vbmokin\/eda-for-tabular-data-advanced-techniques)\n* [Heart Disease - Automatic AdvEDA & FE & 20 models](https:\/\/www.kaggle.com\/vbmokin\/heart-disease-automatic-adveda-fe-20-models)\n* [COVID in UA: Prophet with 4, Nd seasonality](https:\/\/www.kaggle.com\/vbmokin\/covid-in-ua-prophet-with-4-nd-seasonality)\n* [Top score : one line of the prediction](https:\/\/www.kaggle.com\/vbmokin\/titanic-top-score-one-line-of-the-prediction)\n* [Data Visualization in just one line of code!!](https:\/\/www.kaggle.com\/nareshbhat\/data-visualization-in-just-one-line-of-code)\n* [DisasterNLP: t-SNE with TFHub, RAPIDS, Plotly](https:\/\/www.kaggle.com\/xhlulu\/disasternlp-t-sne-with-tfhub-rapids-plotly)\n* [MNIST 2D t-SNE with Rapids](https:\/\/www.kaggle.com\/tunguz\/mnist-2d-t-sne-with-rapids)\n* [MNIST Original : 2D tSNE, 3D UMAP with RAPIDS](https:\/\/www.kaggle.com\/vbmokin\/mnist-original-2d-tsne-3d-umap-with-rapids)\n* [\ud83d\udcca Automatic EDA Libraries \ud83d\udcda Comparisson](https:\/\/www.kaggle.com\/andreshg\/automatic-eda-libraries-comparisson\/notebook#4.-%F0%9F%93%8A-SweetViz-%F0%9F%93%9A)\n* [Time Series Basic Analysis](https:\/\/www.kaggle.com\/sandipdatta\/time-series-basic-analysis)\n* [PyCaret Introduction (Classification & Regression)](https:\/\/www.kaggle.com\/frtgnn\/pycaret-introduction-classification-regression)\n* https:\/\/www.dataschool.io\/python-pandas-tips-and-tricks\/\n* https:\/\/github.com\/rougier\/numpy-100\n* https:\/\/www.kaggle.com\/abhi170599\/bitcoin-price-prediction-with-machine-learning","cef267bb":"### Tip 7.2. Drawing learning_curve plot<a class=\"anchor\" id=\"7.2\"><\/a>","a138ddda":"## 7. Analysis and visualization of modeling results<a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)","6f0d420e":"%%time - for one cell - This should be written in the very first line of the cell, even before the comments","06fdb902":"#### See more in the notebook [50 Tips: Data Science (tabular data) for beginner](https:\/\/www.kaggle.com\/vbmokin\/50-tips-data-science-tabular-data-for-beginner)","0e2281ff":"### Tip 4.5. TSNE with RAPIDS (2D with matlplotlib and plotly)<a class=\"anchor\" id=\"4.5\"><\/a>","09ea4892":"### Tip 5.15. FS by the VarianceThreshold<a class=\"anchor\" id=\"5.8.7\"><\/a>","4d5a1582":"### Tip 5.1. Reduce memory usage for DataFrame<a class=\"anchor\" id=\"5.1\"><\/a>","0b5e31a8":"### Tip 8.1. Submission data from DataFrame to Kaggle competition<a class=\"anchor\" id=\"8.1\"><\/a>","e6dafdcd":"### Tip 4.8. SweetViz: comparing two subsets of the same dataframe (e.g. Male vs Female)<a class=\"anchor\" id=\"4.8\"><\/a>","9bdd6844":"### Tip 4.6. UMAP with RAPIDS (3D with plotly)<a class=\"anchor\" id=\"4.6\"><\/a>","c919ffeb":"### Tip 5.2. Unique values for all columns in DataFrame<a class=\"anchor\" id=\"5.2\"><\/a>","3a71e68c":"Thenks to https:\/\/www.kaggle.com\/andreshg\/automatic-eda-libraries-comparisson\/notebook#4.-%F0%9F%93%8A-SweetViz-%F0%9F%93%9A","5e95dbea":"#### See more in the notebook [50 Tips: Data Science (tabular data) for beginner](https:\/\/www.kaggle.com\/vbmokin\/50-tips-data-science-tabular-data-for-beginner)","f8b492a8":"Thanks to [Time Series Basic Analysis](https:\/\/www.kaggle.com\/sandipdatta\/time-series-basic-analysis)","dee970f3":"### Tip 7.5. Drawing plot with prediction and target data (Plotly)<a class=\"anchor\" id=\"7.5\"><\/a>"}}