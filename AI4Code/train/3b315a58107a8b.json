{"cell_type":{"ab91bc24":"code","c72c449b":"code","b08e1c3f":"code","80350516":"code","ea4003fd":"code","0e3749a1":"code","3a3ca4af":"code","b3d31a3f":"code","377c1748":"code","dfe5d2e1":"code","cf29d044":"code","95030cf8":"code","89f2f3f5":"code","250c5485":"code","9c86b0af":"code","7932551f":"code","a889f8f8":"code","58e42bc5":"code","aed9dd1f":"markdown","dc30be31":"markdown","bb608b8f":"markdown","61e3fb0c":"markdown","6f79a28d":"markdown"},"source":{"ab91bc24":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c72c449b":"data = pd.read_csv(\"\/kaggle\/input\/entity-annotated-corpus\/ner_dataset.csv\",encoding = 'latin1')\ndata = data.fillna(method = 'ffill')\ndata.head()\n# data.shape","b08e1c3f":"data.nunique()","80350516":"words = list(set(data[\"Word\"].values))\nwords.append(\"ENDPAD\")\nnum_words = len(words)\nnum_words","ea4003fd":"words_tag = list(set(data[\"Tag\"].values))\n# words_tag.append(\"ENDPAD\")\nnum_words_tag = len(words_tag)\nnum_words_tag","0e3749a1":"num_words,num_words_tag\ngroup = data.groupby(data[\"Sentence #\"])\n# group.groups","3a3ca4af":"class Get_sentence(object):\n    def __init__(self,data):\n        self.n_sent=1\n        self.data = data\n        agg_func = lambda s:[(w,p,t) for w,p,t in zip(s[\"Word\"].values.tolist(),\n                                                     s[\"POS\"].values.tolist(),\n                                                     s[\"Tag\"].values.tolist())]\n        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n        self.sentences = [s for s in self.grouped]","b3d31a3f":"getter = Get_sentence(data)\nsentence = getter.sentences\nsentence[0]","377c1748":"word_idx = {w : i+1 for i ,w in enumerate(words)}\ntag_idx =  {t : i for i ,t in enumerate(words_tag)}","dfe5d2e1":"plt.hist([len(s) for s in sentence],bins= 50)\nplt.xlabel(\"Length of Sentences\")\nplt.show()","cf29d044":"from tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\n\nmax_len = 50\nX = [[word_idx[w[0]] for w in s] for s in sentence]\nX = pad_sequences(maxlen = max_len,sequences = X,padding = 'post',value = num_words-1)\ny = [[tag_idx[w[2]] for w in s] for s in sentence]\ny = pad_sequences(maxlen = max_len,sequences = y,padding = 'post',value = tag_idx['O'])\ny = [to_categorical(i,num_classes = num_words_tag) for i in  y]","95030cf8":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X,y,test_size = 0.1,random_state=1)","89f2f3f5":"from tensorflow.keras import Model,Input\nfrom tensorflow.keras.layers import LSTM,Embedding,Dense\nfrom tensorflow.keras.layers import TimeDistributed, SpatialDropout1D,Bidirectional","250c5485":"input_word = Input(shape = (max_len,))\nmodel = Embedding(input_dim = num_words,output_dim = max_len,input_length = max_len)(input_word)\nmodel = SpatialDropout1D(0.1)(model)\nmodel = Bidirectional(LSTM(units=100,return_sequences = True, recurrent_dropout = 0.1))(model)\nout = TimeDistributed(Dense(num_words_tag,activation = 'softmax'))(model)\nmodel = Model(input_word,out)\nmodel.summary()","9c86b0af":"model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics = ['accuracy'])","7932551f":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping \n# from livelossplot import PlotLossesKeras\nearly_stopping = EarlyStopping(monitor = 'val_accuracy',patience =2,verbose = 0,mode = 'max',restore_best_weights = False)\ncallbacks = [early_stopping]\n\nhistory = model.fit(\n    x_train,np.array(y_train),\n    validation_split =0.2,\n    batch_size = 64,\n    epochs = 3,\n    verbose =1\n)\n\n\n","a889f8f8":"model.evaluate(x_test,np.array(y_test))","58e42bc5":"i = np.random.randint(0, x_test.shape[0])\np = model.predict(np.array([x_test[i]]))\n# print(np.shape(p))\n# print(p)\np = np.argmax(p, axis=-1)\n\n\ny_true = np.argmax(np.array(y_test), axis=-1)[i]\n\nprint(\"{:15}{:5}\\t{}\\n\".format(\"Word\", \"True\", \"Pred\"))\nprint(\"-\"*30)\n\nfor (w, t, pred) in zip(x_test[i], y_true, p[0]):\n    print(\"{:15}{}\\t{}\".format(words[w-1], words_tag[t], words_tag[pred]))","aed9dd1f":"> # Building Model","dc30be31":"# Retreive  Sentences and Corresponding Tags","bb608b8f":"> Training Model","61e3fb0c":"> # Padding input Sentences and creating train\/test split","6f79a28d":"# Mappings between sentences and tags"}}