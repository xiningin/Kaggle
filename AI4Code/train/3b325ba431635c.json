{"cell_type":{"56a7d4d2":"code","a92b22d4":"code","de0aa8b6":"code","fcb1faaa":"code","3980b9cb":"code","a12b5602":"code","d24d1a22":"code","f5eba110":"code","736f070f":"code","d394d541":"code","a1437962":"code","1323a53f":"code","da1846e5":"code","8b6215db":"code","9c1c5831":"code","234fc7ab":"code","9ba95d06":"code","e49e576b":"code","f9efa405":"code","253c73e2":"code","c0dcdd93":"code","24ccd6ee":"code","71d9e3ff":"code","aac479e3":"code","d551fd52":"code","c596fd10":"code","a3080c15":"code","a2fc9b5a":"code","4901dd95":"code","fd7fdd40":"code","9915eeca":"code","1ff3631d":"code","aa4890e2":"code","d11c0a27":"code","d62fa704":"code","d02713e1":"code","27cd0de6":"code","4f114b8f":"code","3109c757":"code","5b683427":"code","637f595f":"code","89830308":"code","75ca7c15":"markdown","71295c0d":"markdown","0fc3454b":"markdown","3a0b1625":"markdown","9fec5635":"markdown","4fd38f2b":"markdown","ae116849":"markdown","15332479":"markdown","874d00fc":"markdown","b760717b":"markdown","a5869795":"markdown","f1f02bd6":"markdown","7b574b50":"markdown","9edcf191":"markdown","42805f9e":"markdown","adb9f1c9":"markdown","c2935dfd":"markdown","8d25704f":"markdown","4ddf7114":"markdown","8dc1b08b":"markdown","32aed9ef":"markdown","f139c0ff":"markdown","84173e85":"markdown","0ca2c239":"markdown","89f6bcb1":"markdown","4419a024":"markdown","ddec2010":"markdown","8797bd5a":"markdown","ffc8b75a":"markdown","a89341c6":"markdown","77df929c":"markdown","8eb5de13":"markdown","ff961344":"markdown","a105924d":"markdown","22f14895":"markdown","c1808d6c":"markdown"},"source":{"56a7d4d2":"import numpy as np\nimport pandas as pd\nfrom collections import defaultdict, OrderedDict, Counter\nimport pickle\nimport json\nfrom tqdm.auto import tqdm\nimport pathlib\nimport sys","a92b22d4":"!pip install wikipedia\nimport wikipedia\n\nfrom gensim.test.utils import common_texts, get_tmpfile, datapath\nfrom gensim.models import Word2Vec, KeyedVectors\nfrom gensim import corpora\nfrom gensim.summarization.textcleaner import split_sentences\nfrom gensim.scripts.word2vec2tensor import word2vec2tensor\nfrom pprint import pprint\nimport os.path\nimport nltk\nimport spacy\nimport re\nimport string\nimport os\nfrom nltk.stem import WordNetLemmatizer\n# from tqdm import tqdm\nfrom multiprocessing import cpu_count\n\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\nmodule_path = os.path.abspath(os.path.join('..'))\nif module_path not in sys.path:\n    sys.path.append(module_path)\n# import Tools.literature as lit\n\nnltk.download('punkt')\nnltk.download('wordnet')\n\nlemmatizer = WordNetLemmatizer()\nstopWords = set(stopwords.words('english'))","de0aa8b6":"!pip install sentence-transformers\nfrom sentence_transformers import models, SentenceTransformer\n\nimport transformers\nimport torch\nimport scipy\nimport itertools\nimport bisect\n# from tqdm import notebook as tqdm #print the entire progress bar on one row in a jupyter notebook\nimport csv\nimport gc\n","fcb1faaa":"# LITERATURE \n\n# -*- coding: utf-8 -*-\n\nDataDir = pathlib.Path('..\/input\/CORD-19-research-challenge\/')\nEngDir = pathlib.Path('..\/input\/uploaded\/')\n\nmdFile = 'metadata.csv'\nenglishPapers = 'englishPapers.pkl'\n\nidList = ['doi','pmcid','Microsoft Academic Paper ID','WHO #Covidence','sha']\n\nclass Container(dict):\n\n    def __init__(self, **kwargs):\n        dict.__init__(self, kwargs)\n        self.__dict__ = self\n        \nclass Literature():\n    \n    def __init__(self):\n        self.maps = Container(PubmedToSha = {}, ShaToPubmed = {})\n        self.metadata = pd.DataFrame(None)\n        self.abstracts = pd.DataFrame(None)\n        self.fullText = defaultdict(dict)\n        self.englishPapers = {}\n        \n    def getMetaData(self, fn: str = mdFile, wd = DataDir):\n        \"\"\"\n        Import the raw Kaggle metadata file\n        Later; match to this dataFrame using sha values\n        \"\"\"\n        self.metadata = pd.read_csv(wd \/ fn, sep=',', header=0)\n\n    def createMaps(self):\n        \"\"\"\n        Create two dictionaries: (k,v) = (Pubmed,Sha) and (k,v) = (Sha,Pubmed)\n        The former is larger than the latter.  The former can map to an empty string, not the latter\n        \"\"\"\n        foo = lambda x: x if type(x)==str else ''\n        \n        self.maps['PubmedToSha'] = {k:foo(v) for (k,v) in zip(self.metadata['pubmed_id'],self.metadata['sha'])}\n        \n        mask = [type(s) == str for s in self.metadata['sha']]\n        filt = self.metadata[mask][['pubmed_id','sha']]\n        self.maps['ShaToPubmed'] = {k:v for (k,v) in zip(self.metadata['sha'],self.metadata['pubmed_id'])}\n        \n    #Capture all main text data as a list. Some can be very deep json (as over 3,000 levels!)\n    def getMainText(self,sha_set = None, path=DataDir):\n        \n        # this is full path\n        f_list = set(path.rglob('*.json'))\n\n        # Reduce f_list if sha_set is given \n        if(sha_set is not None):\n            \n            #sha_set needs .json extension\n            sha_set = set([x + '.json' for x in sha_set])\n            \n            # build dictionary; base_path:full_path\n            path_dict = {}\n            for each in f_list:\n                path_dict[os.path.basename(each)] = each\n                        \n            f_list = [full for base, full in path_dict.items() if base in sha_set]\n\n        for paper in f_list:\n        #        for paper in tqdm(path.rglob('*.json')):\n            df_single = pd.read_json(paper,orient='index')\n\n            # Catch errors\n            try:\n                bodyText = df_single.loc['body_text',:].values\n                paragraphs = [x['text'] for j in range(len(bodyText)) for x in bodyText[j]]\n                sha = paper.name.split('.')[0]\n                self.fullText.update({sha:paragraphs})\n\n            except:\n                print('Missing text or failed to load for\\n\\t{}'.format(paper))\n                raise\n    '''\n        Retrieve the set of SHA keys corresponding to English papers in the corpus.\n        The user is expected to filter the contents of the Literature() object by this set.\n        \n        For instance, self.fullText is keyed by SHA, so use self.englishPapers as a filter.\n        Likewise, filter PubMed->SHA and SHA->PubMed maps and metadata DF rows by english SHAs.\n    '''          \n    def getEnglishText(self,path=EngDir,papers=englishPapers):\n        \n        with open(path \/ papers, 'rb') as f:\n            ep = pickle.load(f)\n          \n        self.englishPapers = ep","3980b9cb":"# import Tools.literature as lit\n\nprint(DataDir)\nprint(EngDir)\nprint(mdFile)\n\nreprocess = True\n# data_source_aws = False\n# if(data_source_aws): reprocess = False\n    \nif(reprocess):\n    corpus = Literature()\n    corpus.getMetaData()\n#     corpus.getMainText() # might want to do this after identifying english papers?","a12b5602":"corpus.getEnglishText()\n# type(corpus.englishPapers)","d24d1a22":"# pass in the sha values we want \ncorpus.getMainText(sha_set = corpus.englishPapers) # 5.5 GB RAM","f5eba110":"print(len(corpus.fullText.keys()))\n# print(sys.getsizeof(corpus.fullText),sys.getsizeof(corpus.metadata))","736f070f":"# with open('..\/data\/corpus.pkl', 'rb') as f:\n#     corpus = pickle.load(f)\n# englishPapers = corpus.englishPapers\npapers = [''.join(corpus.fullText[paperKey]) for paperKey in corpus.englishPapers]\n# print(len(papers),sys.getsizeof(papers))\nprint('Number of papers: {}')","d394d541":"# PubMed model from https:\/\/www.ncbi.nlm.nih.gov\/CBBresearch\/Wilbur\/IRET\/DATASET\/\n# API Doc. https:\/\/radimrehurek.com\/gensim\/models\/keyedvectors.html\npubWordVecs = KeyedVectors.load_word2vec_format('..\/input\/pubmed\/pubmed_s100w10_min.bin', binary=True)  # C bin format","a1437962":"keywords = [\"coronavirus\", \"infection\", \"fever\", \"pneumonia\", \"lung\",\\\n            \"cough\",\"rna\", \"receptor\", \"cov\", \"antiviral\"]\npubmedVecs = []\nfor word in keywords:\n    word = lemmatizer.lemmatize(word)\n    pubmedVecs.append((word, pubWordVecs.similar_by_word(word, topn=10)))","1323a53f":"pickleExists = False\n\nif os.path.isfile('..\/data\/sentences.pkl'):\n    pickleExists = True\n    with open('..\/data\/sentences.pkl', 'rb') as f:\n        sentences = pickle.load(f)\n    print(\"Sentences loaded from file.\")\nelse:\n    print('No sentence file, processing sentences.')\n    allTxt = ''.join([''.join(paper) for paper in tqdm(papers) if type(paper)==str]) # Ensure no empty papers\n    allTxt = re.sub(r'\\.(?=[^ \\W\\d])','. ', allTxt)\n    raw_sentences = split_sentences(allTxt)\n    \n    del allTxt\n\n    raw_sentences = [[sentence] for sentence in tqdm(raw_sentences)]","da1846e5":"print(len(raw_sentences))","8b6215db":"# From https:\/\/www.geeksforgeeks.org\/regex-in-python-to-put-spaces-between-words-starting-with-capital-letters\/\ndef putSpace(input): \n    # Needed to split certain \"joined sentences\" such as ...coronavirusThe... where the first sentence ends\n    # with \"coronavirus\" and the second one starts with \"The\"\n    # regex [A-Z][a-z]* means any string starting with capital character\n    # followed by many lowercase letters  \n    words = re.findall('[A-Z][a-z]*', input) \n  \n    # Change first letter of each word into lower \n    # case \n    result = [] \n    for word in words: \n        word = chr( ord (word[0]) + 32) + word[1:] \n        result.append(word) \n    return ' '.join(result)\n\ndef tokenizeSentence(sent):\n    '''\n        Process sentence:\n            - Remove URL\n            - Remove most punctuation\n            - Tokenize words\n            - Lemmatize sentence, throwing away tokens without letters (pure numbers)\n                - Group together the inflected forms of a word so they can be analysed\n                  as a single item, identified by the word's lemma, or dictionary form\n    '''\n\n    raw_sent = putSpace(sent[0])\n    raw_sent = raw_sent.lower()\n    raw_sent = re.sub(r'http\\S+', 'URL', raw_sent, flags=re.MULTILINE)\n    raw_sent = re.sub(r'[.!?]',' ', raw_sent)    \n    raw_sent = re.sub(r'[^\\w\\s\\-\\+]','', raw_sent)\n    split_sent = [word for word in raw_sent.split() if word not in stopWords and len(word)>2]\n    sent = [lemmatizer.lemmatize(token) for token in sent if not token.isdigit() and token not in stopWords]\n    if split_sent:\n        return split_sent\n\nif pickleExists==False:    \n    sentences = [tokenizeSentence(sent) for sent in tqdm(raw_sentences)]\n    sentences = list(filter(None, sentences))\n#     with open('..\/data\/sentences.pkl', 'wb') as f:\n#         corpus = pickle.dump(sentences, f)","9c1c5831":"del raw_sentences","234fc7ab":"vecsExist = False\nif os.path.isfile('..\/data\/KaggleVecs.bin'):\n    print('Embedding file found, loading...')\n    vecsExist = True\n    kaggleWordVecs = KeyedVectors.load_word2vec_format('..\/data\/KaggleVecs.bin', binary=True)  # C bin format\n\nif vecsExist==False:\n    print('Training Word2Vec model...')\n    # Model will default to vecs of length 100 unless told otherwise in size arg.\n    model = Word2Vec(sentences, size=200, min_count=0, workers=cpu_count())\n    kaggleWordVecs = model.wv\n#     kaggleWordVecs.save_word2vec_format('..\/input\/w2vdata\/KaggleVecs.bin', binary=True)\n    del model","9ba95d06":"keywords = [\"coronavirus\", \"infection\", \"fever\", \"pneumonia\", \"lung\",\\\n            \"cough\",\"rna\", \"cov\", \"antiviral\"]\n\nkaggleVecs = []\nfor word in keywords:\n    kaggleVecs.append((lemmatizer.lemmatize(word), kaggleWordVecs.similar_by_word(word, topn=3)))","e49e576b":"for pubmed, kaggle in zip(pubmedVecs, kaggleVecs):\n    print('\\nPubmed:')\n    pprint(pubmed)\n    print('\\nKaggle:')\n    pprint(kaggle)","f9efa405":"lookup_word = 'inhibitors'\ntry:\n    for word in kaggleWordVecs.similar_by_word(lookup_word.lower(), topn=3):\n        word = lemmatizer.lemmatize(word[0])\n        print(word, '\\n')\n        searchResults = wikipedia.search(word)\n        if searchResults:\n            print(searchResults, '\\n')\n            try:\n                print(searchResults[0] + ': ' + wikipedia.summary(searchResults[0]), '\\n')\n            except:\n                try:\n                    print(word + ': ' + wikipedia.summary(word), '\\n')\n                except:\n                    for word in searchResults:\n                        try:\n                            print(next_word + ': ' + wikipedia.summary(next_word), '\\n')\n                            break\n                        except:\n                            continue\n                        \n        else:\n            print('No search results for', word[0], '\\n')\nexcept KeyError:\n    print(lookup_word, \"not found in vocabulary.\")","253c73e2":"# keep raw_sentences_str, papers\ndel sentences\nkaggleVecs = []\ndel kaggleWordVecs","c0dcdd93":"MODELS_PRETRAINED = {\n    'scibert': 'allenai\/scibert_scivocab_cased',\n    'biobert': 'monologg\/biobert_v1.1_pubmed',\n    'covidbert': ' deepset\/covid_bert_base',\n}\n\nMODELS_FINETUNED = {\n    'scibert-nli': 'gsarti\/scibert-nli',\n    'biobert-nli': 'gsarti\/biobert-nli',\n    'covidbert-nli': 'gsarti\/covidbert-nli'\n}\n\nMODELS = {**MODELS_PRETRAINED, **MODELS_FINETUNED}","24ccd6ee":"my_model = 'scibert-nli'\nMODELS[my_model]","71d9e3ff":"if my_model in MODELS_FINETUNED.keys():# Build the SentenceTransformer directly\n    word_embedding_model = models.BERT(\n        MODELS[my_model],\n        max_seq_length=128, \n        do_lower_case=True \n    )\n    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n                           pooling_mode_mean_tokens=True,\n                           pooling_mode_cls_token=False,\n                           pooling_mode_max_tokens=False)\n    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\nelse:\n    print('No BERT model found')","aac479e3":"# Number of papers to work with\n# negative 1 (-1) means use full corpus\n\nstop_at = 1000","d551fd52":"if(stop_at == -1):\n    a = corpus.fullText\n    del corpus.fullText\n    \nelse:    \n    # Take n papers for now\n    a = {}\n    count = 0\n    for k,v in corpus.fullText.items():\n        if(count >= stop_at): break\n        count +=1\n        a.update({k:v})","c596fd10":"len(corpus.fullText), len(a)","a3080c15":"ALL_INFO = pd.DataFrame(data=a.items(), columns=['sha', 'full_text'])","a2fc9b5a":"# Creating a master list containing a list of each paper text\npaper_txt = ALL_INFO['full_text']\n\n# Defining list of sha values in order of corpus sentence text\nsha_list = ALL_INFO['sha']\n","4901dd95":"ALL_INFO['paper_sent'] = ALL_INFO['full_text'].astype(str).apply(split_sentences)","fd7fdd40":"# Counting the number of sentences per paper PRIOR TO CLEANING\nALL_INFO['n_sentences_PRIOR'] = ALL_INFO.paper_sent.apply(len)","9915eeca":"def clean_sent(sent):\n    '''\n        Process sentence:\n            - Remove URL\n            - Remove punctuation\n        model.encode() DOES NOT accept NoneType\n    '''\n    sent = re.sub(r'http\\S+', 'URL', sent, flags=re.MULTILINE)\n    sent = re.sub(r'[^\\w\\s]','', sent)\n    if sent:\n        return sent","1ff3631d":"ALL_INFO['cleaned'] = ALL_INFO.paper_sent.apply(lambda x: [clean_sent(y) for y in x])","aa4890e2":"ALL_INFO['cleaned'] = ALL_INFO.cleaned.apply(lambda x: [y for y in x if (y is not None)])#\nALL_INFO['cleaned'] = ALL_INFO.cleaned.apply(lambda x: [y for y in x if len(y) > 25])#[len(y) for y in x ])#if len(y) > 20])","d11c0a27":"# Counting the number of sentences per paper\nALL_INFO['n_sentences'] = ALL_INFO.cleaned.apply(len)\nALL_INFO['cum_sum'] = ALL_INFO.n_sentences.cumsum()","d62fa704":"cleaned_corpus_sent = list(itertools.chain(*ALL_INFO['cleaned']))\n","d02713e1":"# Look up SHA from index of sentence with bisect method\ndef get_sha(index_list, sha_list, sent_idx):\n    p_ind = bisect.bisect_right(index_list, sent_idx)\n    return sha_list[p_ind]","27cd0de6":"test_sha = get_sha(ALL_INFO.cum_sum.astype(int), ALL_INFO.sha.astype(str), 110)\nprint(test_sha)\nprint(corpus.metadata[corpus.metadata.sha == test_sha].authors)","4f114b8f":"cleaned_corpus_sent_emb = model.encode(cleaned_corpus_sent, show_progress_bar=True)","3109c757":"def ask_question(query\n                 , model\n                 , index_list\n                 , sha_list\n                 , corpus_meta\n                 , corpus_text\n                 , corpus_embed\n                 , top_k=5):\n    \"\"\"\n    Adapted from https:\/\/www.kaggle.com\/dattaraj\/risks-of-covid-19-ai-driven-q-a\n\n    query: input question being asked of the corpus\n    model: model used to encode sentences\n    index_list: list of non-inclusive ending index of paper sentence index values (cumulative)\n    sha_list: list of SHA ids\n    corpus_meta: corpus variable that contains the metadata (i.e., corpus.metadata)\n    corpus_text: single list of corpus sentences (text)\n    corpus_embed: encoded corpus of sentences for all papers (vectors)\n    top_k: top k results to be returned\n    \n    \"\"\"\n    # Read and encode queries\n    queries = [query]\n    query_embeds = model.encode(queries, show_progress_bar=False)\n    \n    # Loop through list of input queries\n    for query, query_embed in zip(queries, query_embeds):\n        \n        # Calculate cosine distance of query to corpus sentence embeddings and sort\n        distances = scipy.spatial.distance.cdist([query_embed], corpus_embed, \"cosine\")[0]\n        distances = zip(range(len(distances)), distances)\n        distances = sorted(distances, key=lambda x: x[1])\n        \n        # Initiate results to be returned and printed\n        sha_results = []\n        \n        #while loop to keep track of sha count? max counters will be k \n        for count, (idx, distance) in enumerate(distances[0:100]): #(distances[0:top_k*10])\n            \n            #get SHA value based on index of\n            sent_sha = get_sha(index_list, sha_list, idx)\n            \n            #add check that not from same paper (keep count, return unique papers)\n            if sent_sha not in sha_results:\n                sha_results.append(sent_sha)\n                print('Result #{0} ({1}%):\\nINDEX: {2}\\n SHA: {3}\\n TITLE: {4}\\n AUTHOR(S): {5}\\n EXCERPT: {6}\\n\\n\\\n                      '.format(count + 1\n                                , round((1 - distance)*100., 2) #round(1 - distance, 4)\n                                , idx\n                                , sent_sha\n                                , [corpus_meta[corpus_meta.sha == sent_sha].title]\n                                , [corpus_meta[corpus_meta.sha == sent_sha].authors]\n                                , '. '.join(corpus_text[idx-2:idx+3]) )) #return a few sentences surrounding matched sentence\n\n            else:\n                sha_results.append(sent_sha)\n\n            if len(Counter(sha_results).keys()) == top_k:\n                break\n\n        print(Counter(sha_results))\n\n    return","5b683427":"my_query = 'What is  glycoprotein structure?'","637f595f":"# How many top results to print\nn_results = 5","89830308":"ask_question(my_query\n                 , model\n                 , ALL_INFO['cum_sum'] #paper_sentidx\n                 , ALL_INFO['sha'] #sha_list\n                 , corpus.metadata\n                 , cleaned_corpus_sent\n                 , cleaned_corpus_sent_emb\n                 , top_k=n_results)\n\n\n# ALL_INFO.columns() #'sha', 'full_text', 'paper_sent', 'n_sentences_PRIOR', 'cleaned', 'n_sentences', 'cum_sum'","75ca7c15":"# Covid-19 NLP topic association: Common word, sentence, and paper associations","71295c0d":"## Topic Lookup:\n","0fc3454b":"Initiate the model with word embedding for each sentence then mean pooling to create a sentence embedding (vector).","3a0b1625":"<a id='asking'><\/a>\n## Ask your question!\nWrite your query below.","9fec5635":"# Modeling Approach\nBased on the nature of the literature (science and virology), our theory was that a sciBERT model may be more appropriate for NLP as opposed to a standard BERT model.\n\n![scibert_specs.png](attachment:scibert_specs.png)\n\nWe analyzes CORD-19 data to assess vaccines and therapeutics with Word2Vec and BERT modeling of corpus. Each are stand-alone tools but can be used in tandem. Word2vec model explores word associations within the working corpus. \n\nBERT model determines most similar sentence(s) in corpus from custom sentence input (contextualized search). It can modify input with word associations from Word2vec.\n","4fd38f2b":"## Work on a subset of the data for now ","ae116849":"![flowchart.png](attachment:flowchart.png)","15332479":"## Generating our own Word2Vec Model: Import data","874d00fc":"## BERT sentence similarity\nFollowing along from the submission from gsarti https:\/\/github.com\/gsarti\/covid-papers-browser and the transformer code from https:\/\/github.com\/UKPLab\/sentence-transformers.\n\nBidirectional Encoding Representations for Transformers (BERT) models incorporate context into the word\/sentence embeddings, increasing performance over other word embeddings that only consider each subsequent word. Specialty BERT models, such as sciBERT, are pretrained on scientific publications making this a strong candidate for analyzing the CORD-19 dataset.\nHere we apply sciBERT encoding of the noted subset of the CORD-19 dataset to create a sentence similarity query. Each sentence is embedded as a vector, which is then compared to the input query also embedded with the same model. We use cosine distance to quantify the similarity between two vectors. The output return top-k sentence matches from unique papers, along with associated metadata (e.g., paper title), for the given input query.\n\nOutput:  \n* ranking of result\n* percent match ((1-distance)*100.)\n* sentence index\n* sha\n* title\n* authors\n* single sentence (might do surrounding few sentences)","b760717b":"<a id='#w2v'><\/a>\n# Word2vec associations","a5869795":"<a id='load_corpus'><\/a>\n## Importing the corpus\nLoad in the text from JSON using the custom python modules.","f1f02bd6":"# Executive Summary\n\nIn response to the COVID-19 pandemic, the White House and a coalition of leading research groups have prepared the COVID-19 Open Research Dataset (CORD-19) that is being hosted by Kaggle. CORD-19 is a resource of over 51,000 scholarly articles, including over 40,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses.\n\nGiven the large amount of literature, it is difficult for health professionals to do meaningful searches of the data to find the most relevant papers for a particular question.\n\nWe applied a two-fold approach to mine relevant data from the CORD-19 dataset.  \n* First, we implemented an AI framework that identifies word associations and similarity throughout the literature.   These word associations may be useful to inform policy makers, scientists, and experts of connected topics and to shed light on research directions that would otherwise be overlooked. \n* Second, we implemented a bi-directional sentence embedding approach that allows a user to extract relevant text from the literature as well the associated metadata (author, title, etc.) based on an input text query (i.e. asking a question).  \n\n\nThe utility of these two AI tools is that they allow an individual to extract relevant information from a very large data set with greater efficiency than a manually guided search.  Further, the word associations identified may be used as direct input to the question asking to yield insightful results.\n\nFor e.g. \u201cWhat drugs inhibit glycoprotein?\u201d   \nReturn sentence: 'In addition Koszalka summarized research performed on inhibitors of the glycoproteins gp120 and gp41'\n","7b574b50":"# Data\nOur corpus from the CORD19 dataset is as of March 27th. It comprises of  ~44,000 documents, of which ~29,000 were with full text. We dropped ~2000 documents that were either not English or were pre-printed articles with not legible text. Using a NLP language detector, a paper had to be labeled as \u2018English\u2019, with a 99.99% probability of being in that language.\n\nFiltering corpus: removing non-English papers  \nApplied Spacy package's 'language detector' utility. (A pretrained model derived from postings to the internet.) For every paper, utility reports 1) most likely language and 2) probability paper is in that language. Built a set of document SHA and PMC ids.  Criterion: 1) language is English ('en') and 2) probability is 99.9% or better. The set is used to filter out English papers from the entire Kaggle corpus (pickled file).\n\nFiltering corpus: removing preprint articles  \nBased on the 4GB Kaggle download accessed on March 27th:  \n~44k papers in the entire corpus  \n~29k papers with JSON full text  \n~950 papers are not classified as English (with sufficient certainty)\n\nPreprint articles:  \nNumber JSON files with \"preprint\u201c in the fulltext:\t953  \nNumber JSON files with \"allowed without permission\" in the fulltext:\t308  \n954 of these preprint articles are unique; 944 can be matched to a metadata entry  \n\nLess than 3% of the JSON corpus are \u201cpreprint\u201d articles.Preprint articles often have garbled text, such as: 'CCBY 40 International license It is made available under a authorfunder who has granted medRxiv a license to display the preprint in perpetuityis the which was not peereviewed The copyright holder for this preprint ','URL doi medRxiv preprint'\n","9edcf191":"## Load or train word2vec model:","42805f9e":"## Selecting English Papers","adb9f1c9":"# Overview: \n* Data\n* Modeling Approach\n* Challenges & Next Steps\n","c2935dfd":"Use pickle file of selected SHAs to read JSON files of subset of English-only papers.","8d25704f":"<a id='load_model'><\/a>\n## Models","4ddf7114":"## Compare results to those from pubmed embeddings:\n\nThis will give insight into how our model trained on the Covid-19 dataset compares to a model trained on general medical literature.","8dc1b08b":"![psail.png](attachment:psail.png)\n\nSubmitted by Publicis Sapient AI Labs Data Scientists:  \nLauren Bittle, Sam Burck, Matthew Belley, Heather Rodney, Larry Berk, Deepa Mahidhara, Edoardo Turano, Ramon Perez","32aed9ef":"## Import PubMed vectors:\nThis is a set of key-value pairs generated by a word2vec model trained on a large set of pubmed data. Keys are words, values are embedded (vectorized) representations of those words.","f139c0ff":"# Challenges & Next Steps\nThere is a large memory overhead to encode all sentences in the entire CORD19 corpus.  Therefore, we demonstrate a proof of concept with the bi-directional sentence encoding using a subset (n=1000) papers.  The method can be extended to the full corpus given additional memory and processing power.  Despite this limitation to a subset of the full CORD19 corpus, we have demonstrated the utility and power of combining two NLP models to create a comprehensive search tool.\n\nAs future steps, we would implement more targeted text pre-processing, particularly for bio-medical text, to more robustly handle technical terms and abbreviations.","84173e85":"We found that the word \"glycoprotein\" has the closest association to our lookup word, inhibitors. The image below is a closeup of from within the cluster containing the word inhibitor.\n\n![Inhibitors_Protein.png](attachment:Inhibitors_Protein.png)\nGenerated with t-SNE using https:\/\/projector.tensorflow.org\/\n","0ca2c239":"## Word2Vec Association\nWe are using an approach based on a paper titled \"Unsupervised word embeddings capture latent knowledge from materials science literature\": https:\/\/www.nature.com\/articles\/s41586-019-1335-8\n\nThis notebook expands on the work done by Tarun Paparaju in his notebook titled \"COVID-19 Dataset : Gaining actionable insights\". His notebook can be found here: https:\/\/www.kaggle.com\/tarunpaparaju\/covid-19-dataset-gaining-actionable-insights\n\nThe idea here is to look for associations between keywords and other words which may be relevant but unknown. One can do this to, for example, look for unknown biomolecules related to a biomolecule that we know is integral to the coronavirus infecting a host. Doing this can help research progress in different directions that would otherwise not have been considered.\n\nTopic Lookup: \nNext, we look up our words associated with our \"lookup_word\" on Wikipedia. Note that some Wikipedia summaries will be on topics related to a different case\/use of the lookup_word. When this is the case, it makes sense to manually choose word\/topics from the search results. Querying and displaying summaries for all search results would simply flood the notebook with too much text output.\n\nThe format for this output is:  \n* lookup_word  \n* Wikipedia search results  \n* Summary\n\nIf no summary is found for the first search result for the lookup word's associated words, the code will attempt to lookup a summary for the associated word itself. If this fails, the code will attempt to find a summary from one of the remaining search results.\n","89f6bcb1":"## Checkout closest words to list of keywords, we will use this later:","4419a024":"<a id='emb_corpus'><\/a>\n## Producing sentence embeddings for the corpus with model\nEmbedding each sentence of the working corpus. This may require several hours depending on size of working corpus.","ddec2010":"#### Set number of papers (stop_at) to encode with BERT model\n\nHere we work with a subset of the working corpus to run the model encoding (embedding) and return some results.","8797bd5a":"## Preprocessing:\nPut text into a format that can be used to train our model, and perform preprocessing to make our data more suitable for training a word2vec model.","ffc8b75a":"Run the query.","a89341c6":"## Removing variables unique to word2vec model","77df929c":"<a id='#bert'><\/a>\n# Running sciBERT sentence similarity on corpus","8eb5de13":"#### Set the desired model - here scibert-nli","ff961344":"# Importing python packages","a105924d":"Set number (k) of results to return.","22f14895":"<a id='#answers'><\/a>\n## Question function: Querying by cosine distance\n\nDefine function to take query and search the (embedded) corpus for best matching sentence by cosine distance. If multiple sentences from a single paper (i.e., SHA value) in top-k, return only top for each paper so that top-k results are unique papers. Count of paper frequency is considered until top-k unique results are returned.\n\nAdapted from https:\/\/github.com\/gsarti\/covid-papers-browser\/blob\/master\/scripts\/interactive_search.py\n\nOutput:  \n* ranking of result\n* percent match ((1-distance)*100.)\n* sentence index\n* sha\n* title\n* authors\n* single sentence (might do surrounding few sentences)","c1808d6c":"<a id='#load_data'><\/a>\n# Loading CORD-19 JSON data into working corpus"}}