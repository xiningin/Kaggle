{"cell_type":{"002f031b":"code","471c02f7":"code","d3e9e7c6":"code","4c4ae869":"code","d51a8b32":"code","712ede19":"code","80a26d19":"code","582c087c":"code","afff0441":"code","df4b5469":"code","c416e255":"code","1a82e70c":"code","bcb9f282":"code","b9b551b9":"code","67868ef7":"markdown","7d5be3a2":"markdown","29491904":"markdown","e25d443a":"markdown","b5fc0bd7":"markdown","c47ad81d":"markdown","0cd7ac35":"markdown","0be337b8":"markdown"},"source":{"002f031b":"import numpy as np # linear algebra\nfrom numpy import newaxis\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.models import Sequential\nfrom keras import optimizers\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n\nprint ('import completed')","471c02f7":"# Enter in how much steps we will enroll the network.\n# RNN\/LSTM\/GRU can be taught patterns over times series as big as the number of times you enrol them, and no bigger (fundamental limitation). \n# So by design these networks are deep\/long to catch recurrent patterns.\nEnrol_window = 100\n\nprint ('enrol window set to',Enrol_window )","d3e9e7c6":"# Support functions\nsc = MinMaxScaler(feature_range=(0,1))\ndef load_data(datasetname, column, seq_len, normalise_window):\n    # A support function to help prepare datasets for an RNN\/LSTM\/GRU\n    data = datasetname.loc[:,column]\n\n    sequence_length = seq_len + 1\n    result = []\n    for index in range(len(data) - sequence_length):\n        result.append(data[index: index + sequence_length])\n    \n    if normalise_window:\n        #result = sc.fit_transform(result)\n        result = normalise_windows(result)\n\n    result = np.array(result)\n\n    #Last 10% is used for validation test, first 90% for training\n    row = round(0.9 * result.shape[0])\n    train = result[:int(row), :]\n    np.random.shuffle(train)\n    x_train = train[:, :-1]\n    y_train = train[:, -1]\n    x_test = result[int(row):, :-1]\n    y_test = result[int(row):, -1]\n\n    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))  \n\n    return [x_train, y_train, x_test, y_test]\n\ndef normalise_windows(window_data):\n    # A support function to normalize a dataset\n    normalised_data = []\n    for window in window_data:\n        normalised_window = [((float(p) \/ float(window[0])) - 1) for p in window]\n        normalised_data.append(normalised_window)\n    return normalised_data\n\ndef predict_sequence_full(model, data, window_size):\n    #Shift the window by 1 new prediction each time, re-run predictions on new window\n    curr_frame = data[0]\n    predicted = []\n    for i in range(len(data)):\n        predicted.append(model.predict(curr_frame[newaxis,:,:])[0,0])\n        curr_frame = curr_frame[1:]\n        curr_frame = np.insert(curr_frame, [window_size-1], predicted[-1], axis=0)\n    return predicted\n\ndef predict_sequences_multiple(model, data, window_size, prediction_len):\n    #Predict sequence of <prediction_len> steps before shifting prediction run forward by <prediction_len> steps\n    prediction_seqs = []\n    for i in range(int(len(data)\/prediction_len)):\n        curr_frame = data[i*prediction_len]\n        predicted = []\n        for j in range(prediction_len):\n            predicted.append(model.predict(curr_frame[newaxis,:,:])[0,0])\n            curr_frame = curr_frame[1:]\n            curr_frame = np.insert(curr_frame, [window_size-1], predicted[-1], axis=0)\n        prediction_seqs.append(predicted)\n    return prediction_seqs\n\ndef plot_results(predicted_data, true_data): \n    fig = plt.figure(facecolor='white') \n    ax = fig.add_subplot(111) \n    ax.plot(true_data, label='True Data') \n    plt.plot(predicted_data, label='Prediction') \n    plt.legend() \n    plt.show() \n    \ndef plot_results_multiple(predicted_data, true_data, prediction_len):\n    fig = plt.figure(facecolor='white')\n    ax = fig.add_subplot(111)\n    ax.plot(true_data, label='True Data')\n    #Pad the list of predictions to shift it in the graph to it's correct start\n    for i, data in enumerate(predicted_data):\n        padding = [None for p in range(i * prediction_len)]\n        plt.plot(padding + data, label='Prediction')\n        plt.legend()\n    plt.show()\n\nprint ('Support functions defined')","4c4ae869":"# Load the data\ndataset = pd.read_csv('..\/input\/sinwave\/Sin Wave Data Generator.csv')\ndataset[\"Wave\"][:].plot(figsize=(16,4),legend=False)","d51a8b32":"# Prepare the dataset, note that all data foer the sinus wave is already normalized between 0 and 1\n# A label is the thing we're predicting\n# A feature is an input variable, in this case a stock price\n\nfeature_train, label_train, feature_test, label_test = load_data(dataset, 'Wave', Enrol_window, False)\n\nprint ('Datasets generated')","712ede19":"# The LSTM model I would like to test\n# Note: replace LSTM with GRU or RNN if you want to try those\n\nmodel = Sequential()\nmodel.add(LSTM(50, return_sequences=True, input_shape=(feature_train.shape[1],1)))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(100, return_sequences=False))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation = \"linear\"))\n\nmodel.compile(loss='mse', optimizer='adam')\n\nprint ('model compiled')\n\nprint (model.summary())\n","80a26d19":"#Train the model\nmodel.fit(feature_train, label_train, batch_size=512, epochs=10, validation_data = (feature_test, label_test))\n","582c087c":"#Let's use the model and predict the wave\npredictions= predict_sequence_full(model, feature_test, Enrol_window)\nplot_results(predictions,label_test)\n  ","afff0441":"# Let's get the stock data\ndataset = pd.read_csv('..\/input\/stock-time-series-20050101-to-20171231\/IBM_2006-01-01_to_2018-01-01.csv', index_col='Date', parse_dates=['Date'])\ndataset.head()","df4b5469":"# Prepare the dataset, note that the stock price data will be normalized between 0 and 1\n# A label is the thing we're predicting\n# A feature is an input variable, in this case a stock price\n# Selected 'Close' (stock pric at closing) attribute for prices. Let's see what it looks like\n\nfeature_train, label_train, feature_test, label_test = load_data(dataset, 'Close', Enrol_window, True)\n\ndataset[\"Close\"][:'2016'].plot(figsize=(16,4),legend=True)\ndataset[\"Close\"]['2017':].plot(figsize=(16,4),legend=True) # 10% is used for thraining data which is approx 2017 data\nplt.legend(['Training set (First 90%, approx before 2017)','Test set (Last 10%, approax 2017 and beyond)'])\nplt.title('IBM stock price')\nplt.show()\n\n","c416e255":"# The same LSTM model I would like to test, lets see if the sinus prediction results can be matched\n# Note: replace LSTM with GRU or RNN if you want to try those\n\nmodel = Sequential()\nmodel.add(LSTM(50, return_sequences=True, input_shape=(feature_train.shape[1],1)))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(100, return_sequences=False))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation = \"linear\"))\n\nmodel.compile(loss='mse', optimizer='adam')\n\nprint ('model compiled')\n\nprint (model.summary())","1a82e70c":"#Train the model\nmodel.fit(feature_train, label_train, batch_size=512, epochs=5, validation_data = (feature_test, label_test))\n","bcb9f282":"#Let's use the model and predict the stock\npredicted_stock_price = model.predict(feature_test)\nplot_results(predicted_stock_price,label_test)\n","b9b551b9":"predictions = predict_sequences_multiple(model, feature_test, Enrol_window, 50)\nplot_results_multiple(predictions, label_test, 50)  \n","67868ef7":"# Learn by example RNN\/LSTM\/GRU time series\n\nI know I cannot predict stock prices based on historic data, but still the Recurring Neural network examples (RNN or LSTM or GRU, etc) to predict stock prices are appealing, who knows I might discover something:-)\n\nWelcome to my second  notebook on Kaggle. I did record my notes so it might help others in their journey to understand Neural Networks by examples (in this case using Recurring Networks stock predictions.) After seeing many youtube video's and various courses on Neural Networks found the Kaggle Keras course and examples helping me a lot to move from powerpoint understanding to run my own Neural Networks using Keras. Many thanks to this community! The least I could do is to contribute back, hence this notebook.  \n\n","7d5be3a2":"<a id='sec32'><\/a>\n##  You can't predict future stock prices on historic data\nOuch, can't use that to put some real money in the stock market.  We basically knew already that you cant predict future stock prices on historic data. Pick for example the grey or purple line, it probably learned the stock went down last 100 sequence so it predicts it will go down, what would be a recognizable pattern to predict the trend will break and would go back up again after point 200.  So it is nog recognisable in the historic data, else the algorithem would have found it. Maybe with a richer data set with correleated stocks? Other (News?) items? Etc Anyhow, still a nice learning example which helped me to practice with LSTM (but could also picked GRU or RNN, fw simple code changes in the model)","29491904":"<a id='sec31'><\/a>\n## Everybody on Kaggle rich! \nThis looks incredible correct, but \"if it is to good to be true, it is probably not true\".  \nLet's step back and actually see what we did. We created a testset of 100 (dependend on how you set the enrol_window) actual datapoints and ask to predict nr 101 (which is probably anhow close to nr 100). And we did so for each point in this graph. Hence the fantastic result, it wasn't that hard (Remember that you are looking at normalised data)\n\nLike the sinewave example we need to predict a new point based on the actual last 100 points, the next point on 99 actual points and 1 prediction, the next point on 98 actuals and 2 predictions, and so forth.   \nLets make some 50 predictions ahead in the future and do this every 50 times to get a bearing how the model predicts","e25d443a":"<a id='sec21'><\/a>\n## Results\nActually not a bad result because remember the models predicts 500 steps in the future and more-over after the enrol_window length (eg 100 setps) the predictions are being made on predictions, so eny error quickly multiplies ba magnitudes.  \nHaving this confidence let's try to predicts some stock prices","b5fc0bd7":"<a id='sec3'><\/a>\n# IBM stock prediction\nCould we predict stock prices with the neural network? Let's try it on some actual data. The daily stock prices of IBM stock are available for 2006-2017. Let's try to predict that last 10% of the data (approx the 2017 data) based on all the data before.","c47ad81d":"<a id='sec2'><\/a>\n# Sinus wave proof of concept\nFirst let's run some tests on a plain sinus wave and see of the Neural Network can predict it right, kind of proof of concept","0cd7ac35":"<a id='sec1'><\/a>\n# Introduction\n\nPlease watch this video about RNN\/LSTM\/GRU time series prediction, it gives you a good overview, it inspired me to reproduce the steps taken in this notebook:  \nhttps:\/\/www.youtube.com\/watch?v=2np77NOdnwk  \nThe code base:  \nhttps:\/\/github.com\/jaungiers\/LSTM-Neural-Network-for-Time-Series-Prediction  \nI would like to acknowledge this tutorial for providing ideas and code, learning by example:  \nhttps:\/\/www.kaggle.com\/thebrownviking20\/intro-to-recurrent-neural-networks-lstm-gru\/notebook  \nIf you are new to Neural Networks you might want to have a look at my first notebook:  \nhttps:\/\/www.kaggle.com\/charel\/learn-neural-networks-by-example-mnist-digits  \n","0be337b8":"# Table of Contents\n  \n1. [Introduction](#sec1)\n1. [Sinus wave proof of concept](#sec2)\n    1. [Results](#sec21)\n1. [IBM stock prediction](#sec3)\n    1. [Everybody on Kaggle rich!](#sec31)\n    1. [you can't predict future stock prices on historic data](#sec32)\n"}}