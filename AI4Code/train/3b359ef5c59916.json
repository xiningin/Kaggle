{"cell_type":{"f78b1a6f":"code","593c9e6f":"code","4866bd89":"code","d9f00fc9":"code","0f746ea6":"code","8e38498f":"code","2e677b4c":"code","1ef6d08e":"code","5f4b5fd7":"code","81e67238":"code","e53433bf":"code","5511d9a1":"code","1d843428":"code","b252bf2a":"code","d0974eda":"code","506cb3c8":"code","d3d5fe82":"code","1cf63caa":"code","8c3a46e6":"code","5c17b90f":"code","6a62fc42":"code","ef9f4769":"code","7a3d22df":"code","d5dff3ed":"code","9cae6237":"code","ebc0a8c1":"code","737808f0":"code","9571c572":"code","0e422ab4":"code","7231ca8f":"code","cd6128c3":"code","59e6f364":"code","e2dbc8bb":"markdown","f631a2a4":"markdown","d57d29d7":"markdown","ef3b4521":"markdown","9ff011a0":"markdown","a4425397":"markdown","2179d296":"markdown","1698d05a":"markdown","5673d314":"markdown","ce573438":"markdown","ce2ec06e":"markdown","edb9df88":"markdown"},"source":{"f78b1a6f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","593c9e6f":"petrol_data = pd.read_csv(\"\/kaggle\/input\/petrol-consumption\/petrol_consumption.csv\")\npetrol_data.head()","4866bd89":"data_reg_y = petrol_data['Petrol_Consumption']\ndata_reg_X = petrol_data.drop(['Petrol_Consumption'], axis=1)","d9f00fc9":"bill_data = pd.read_csv(\"\/kaggle\/input\/bill_authentication\/bill_authentication.csv\")\nbill_data.head()","0f746ea6":"data_class_y = bill_data['Class']\ndata_class_X = bill_data.drop(['Class'], axis=1)","8e38498f":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split","2e677b4c":"train_class_X, test_class_X, train_class_y, test_class_y = train_test_split(data_class_X, data_class_y, shuffle = True)","1ef6d08e":"random_class = RandomForestClassifier()\nrandom_class.fit(train_class_X, train_class_y)\nrandom_class.score(test_class_X, test_class_y)","5f4b5fd7":"from sklearn.ensemble import RandomForestRegressor","81e67238":"train_reg_X, test_reg_X, train_reg_y, test_reg_y = train_test_split(data_reg_X, data_reg_y, shuffle = True)","e53433bf":"random_reg = RandomForestRegressor()\nrandom_reg.fit(train_reg_X, train_reg_y)\nrandom_reg.score(test_reg_X, test_reg_y)","5511d9a1":"def get_potential_splits(data,random_subspace):\n    \n    potential_splits = {}\n    column_indices = list(range(data.shape[1]-1))\n    \n    \n    if random_subspace and random_subspace < data.shape[1]:\n        column_indices = random.sample(population = column_indices,k = random_subspace)\n    \n    for column_index in  column_indices :\n            \n            values =data[:,column_index] \n            \n            if FEATURE_TYPES[column_index] == 'Continious':\n                \n                unique_values = np.unique(values)\n                potential_splits[column_index] = []\n                \n                for i in range(len(unique_values)-1):\n                    current_value = unique_values[i]\n                    next_value = unique_values[i+1]\n                    potential_split = (current_value+next_value)\/2\n                \n                    potential_splits[column_index].append(potential_split)\n            \n            else:\n                potential_splits[column_index]=list(set(values))\n             \n            \n    return potential_splits","1d843428":"def determine_type_of_feature(data):\n    \n    feature_types = []\n    threshold = 15\n    \n    for column_index in range(data.shape[1]-1):\n        \n        unique_values = np.unique(data[:,column_index])\n            \n        if(len(unique_values)<=threshold)or isinstance(unique_values[0],str):\n            feature_types.append('Categorical')\n        else:\n            feature_types.append('Continious')\n            \n    return feature_types","b252bf2a":"def split_data(data,split_column,split_value):\n    \n    values = data[:,split_column]\n    type_of_feature = FEATURE_TYPES[split_column] \n    \n    if type_of_feature == 'Continious':\n        data_above = data[values > split_value]\n        data_below = data[values <= split_value]\n    else:\n        data_below = data[values == split_value]\n        data_above = data[values != split_value]\n        \n    return data_below,data_above","d0974eda":"def gini(data):\n    \n    label_column= data[:,-1]\n    _,counts = np.unique(label_column,return_counts=True)\n    \n    p=counts\/counts.sum()\n    gini =1- np.dot(p,p)\n    \n    return gini","506cb3c8":"def entropy(data):\n    \n    label_columns = data[:,-1]\n    _,counts = np.unique(label_columns,return_counts= True)\n    \n    p = counts\/counts.sum()\n    entropy = sum(p*-np.log2(p))\n    \n    \n    return entropy","d3d5fe82":"def overall_metric(data_below,data_above,metric_function):\n    \n    n=len(data_above)+len(data_below)\n    p_data_below = len(data_below)\/n\n    p_data_above = len(data_above)\/n\n    \n    overall_metric = p_data_above*metric_function(data_above) + p_data_below*metric_function(data_below)\n    \n    return overall_metric","1cf63caa":"def get_best_split(data, potential_splits, metric_function = gini):\n    \n    first_iteration = True\n    for column_index in potential_splits:\n        for value in potential_splits[column_index]:\n            \n            data_below,data_above = split_data(data,split_column=column_index,split_value = value)\n            current_metric = overall_metric(data_above,data_below,metric_function)\n            \n            if first_iteration:\n                \n                best_metric = current_metric\n                first_iteration = False\n            \n            if current_metric <= best_metric :\n                \n                best_metric = current_metric\n                best_column =column_index\n                best_value = value\n                \n                \n    return best_column,best_value","8c3a46e6":"def  check_purity(data):\n    label_columns = data[:,-1]\n    \n    if len(np.unique(label_columns))==1:\n        return True\n    else:\n        return False","5c17b90f":"def create_leaf(data):\n    \n    label_columns = data[:,-1]\n    unique_labels,counts = np.unique(label_columns,return_counts =True)\n    \n    index = counts.argmax()\n    leaf = unique_labels[index]\n    \n    return leaf","6a62fc42":"def bootstrap(data,n_bootstrap):\n    \n    indices =np.random.randint(low=0,high=len(data),size=n_bootstrap)\n    \n    return data[indices]\n","ef9f4769":"def decision_tree_algorithm(data,counter =0, max_depth =5,min_samples = 10,random_subspace=None,metric_function = gini):\n    \n    if counter == 0:\n    \n        global FEATURE_TYPES\n        FEATURE_TYPES = determine_type_of_feature(data)\n    \n    \n    if (check_purity(data)) or (counter == max_depth) or (len(data) < min_samples):\n        return create_leaf(data)\n    \n    else:\n        \n        counter += 1\n        \n        potential_splits = get_potential_splits(data, random_subspace)\n        column_index,split_value = get_best_split(data, potential_splits, metric_function)\n        data_below,data_above = split_data(data, column_index, split_value)\n         \n        if len(data_below)==0 or len(data_above)==0 :\n            return create_leaf(data)\n        \n        \n        type_of_feature = FEATURE_TYPES[column_index]\n        #column_name = COLUMN_NAMES[column_index]\n        \n        if type_of_feature == 'Continious':\n            question = \"{} <= {}\".format(column_index,split_value)\n        else:\n            question =\"{} = {}\".format(column_index,split_value)\n        sub_tree={question:[]}\n        \n        yes_answer = decision_tree_algorithm(data_below, counter, max_depth, min_samples,random_subspace  ,metric_function )\n        no_answer = decision_tree_algorithm(data_above, counter, max_depth, min_samples,random_subspace  ,metric_function )\n        \n        if yes_answer == no_answer:\n            sub_tree =yes_answer\n        else:\n            sub_tree[question].append(yes_answer)\n            sub_tree[question].append(no_answer)\n       \n        return sub_tree","7a3d22df":"def decision_tree_classifer(example,tree):\n    question = list(tree.keys())[0]\n    column_index,comparison_operator,value =question.split()\n    column_index =int(column_index)\n    \n    if comparison_operator == \"<=\":\n        if example[column_index] <= float(value):\n            answer = tree[question][0]\n        else:\n            answer = tree[question][1]\n    \n    \n    else:\n        if str(example[column_index]) == value:\n            answer = tree[question][0]\n        else:\n            answer = tree[question][1]\n\n    if not isinstance(answer,dict):\n        return answer\n    \n    else:\n        residual_tree = answer\n        return decision_tree_classifer(example, residual_tree)","d5dff3ed":"def random_forest_algorithm(train_data, n_trees,max_depth = 5,min_samples =10,random_state = 123, n_features = 3, n_bootstrap=50,metric_function =gini):\n    \n    np.random.seed(random_state)\n    forest = []\n    for i in range(n_trees):\n        \n        bootstrapped_data = bootstrap(train_data,n_bootstrap)\n        tree = decision_tree_algorithm(data = bootstrapped_data, counter=0, random_subspace = n_features, max_depth = max_depth,metric_function=metric_function)\n        forest.append(tree)\n        \n    return forest","9cae6237":"def random_tree_classifier(example,forest):\n    \n    results =[]\n    for index in range(len(forest)):\n        \n        result = decision_tree_classifer(example, forest[index] )\n        results.append(result)\n        \n    mode = max(set(results),key=results.count)\n    return mode","ebc0a8c1":"def classify_data(test_df,forest):\n    \n    Predictions = test_df.apply(func = random_tree_classifier, axis = 1, raw=True,args=(forest,))\n    \n    return Predictions\n","737808f0":"def calculate_accuracy(labels,predictions):\n        \n \n    accuracy = np.array(labels == predictions).mean()\n    \n    return accuracy","9571c572":"train_df = pd.concat([train_class_X, train_class_y], axis=1)\ntrain_df.head()","0e422ab4":"test_df = pd.concat([test_class_X, test_class_y], axis=1)\ntest_df.head()","7231ca8f":"forest=random_forest_algorithm(train_df.values,n_trees = 5,n_features=2,n_bootstrap=100,random_state =120)","cd6128c3":"predictions = classify_data(test_df.iloc[:,:-1],forest)","59e6f364":"labels = test_df.iloc[:,-1]\n\nprint(\"Accuracy is : {}\".format(calculate_accuracy(predictions,labels)*100))","e2dbc8bb":"# Scratch Implementation of Random Forrest Classifier","f631a2a4":"# Implementing our scratch Model","d57d29d7":"## Random Forest Algorithm","ef3b4521":"## Decision Tree Classifier","9ff011a0":"# Data Cleaning & Feature Engineering","a4425397":"# Sklearn implentation of Regressor","2179d296":"## Metric Functions","1698d05a":"# Sklearn Implementation of Classifier","5673d314":"## Decision Tree Algorithm","ce573438":"## Accuracy","ce2ec06e":"## Helper Functions","edb9df88":"## Random Forest Classifier"}}