{"cell_type":{"85e60481":"code","518996b2":"code","40c5ed3c":"code","295a1083":"code","f43a4add":"code","89d87889":"code","2f82799a":"code","cca3b5b0":"code","41505634":"code","5b4f53f8":"code","ca8ace23":"code","421b7a8a":"code","de17a71a":"code","0522652b":"code","a98dc855":"code","46f7dd85":"code","905e3ab3":"code","3ad9489e":"code","d69bd05a":"code","4cd86bc6":"code","eceef854":"code","ef851f12":"code","28eaeb47":"code","6a23a103":"code","4bb3b2a5":"code","e53843bc":"code","96f01a17":"code","c66c7e5a":"code","c731c178":"code","24e110c4":"code","28bae9ed":"code","2549c291":"code","402a5426":"code","a3b5dc89":"code","7915867f":"code","68d717b1":"code","f364f623":"code","9acbe0ff":"code","ac8ee9be":"code","4eeea13c":"code","92f0e900":"code","0122ec10":"code","fe19c0d2":"code","db3fef51":"code","6e1a7526":"code","dfc17f9c":"code","9659d4d0":"code","ea102940":"code","f20a25a6":"code","7ee23ff8":"code","d80a7c14":"code","d490b519":"code","f597566b":"code","489372ee":"code","d4b02829":"code","217b17e7":"code","c447aa02":"code","509fe4ef":"code","f69e2a3d":"code","62df0417":"code","64700353":"code","885aecbb":"code","89c26d77":"code","881e176d":"code","1c4a5a21":"code","a0d77889":"code","dadd4ed6":"code","3b9a7477":"code","6cfa10c5":"code","1dd7e8dd":"code","7ec5b902":"code","b20c4b04":"code","24f598aa":"code","abacc031":"code","501b4e7b":"code","389d3e68":"code","997699f3":"code","db593ad8":"code","193a44cf":"code","6cf5e154":"code","ee92a005":"markdown","d0bed016":"markdown","5cfaba2f":"markdown","b655e477":"markdown","8eb1842e":"markdown","d499c99d":"markdown","c742a4c0":"markdown","8c17c4a1":"markdown","f4ff3d7b":"markdown","73ee4057":"markdown","d643f6f5":"markdown","0634f5d7":"markdown","7c03f9ea":"markdown","58dc2bab":"markdown","da2dffbd":"markdown","b0ce4d42":"markdown","c0ed2376":"markdown","f7296067":"markdown","79213c09":"markdown","994cc967":"markdown","f29ecb2e":"markdown","ad797ebb":"markdown","5c674789":"markdown","56944e95":"markdown","3304f366":"markdown","3994c8a6":"markdown","bd2553ce":"markdown","94ee4438":"markdown","652d9e59":"markdown","3b25190b":"markdown","d4450f16":"markdown","0ac640e4":"markdown","8a35e8e4":"markdown","acdac152":"markdown","8d2e80e9":"markdown","f30fe763":"markdown","ba7676ec":"markdown","d068c003":"markdown","d8fcdc23":"markdown","9151c887":"markdown","be5f1293":"markdown","73d43252":"markdown","b4e90d3e":"markdown","902a086c":"markdown","9905b2c3":"markdown","9be1a59f":"markdown"},"source":{"85e60481":"import time\nnotebookstart= time.time()\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport gc\nfrom IPython.display import display\nprint(\"Data:\\n\",os.listdir(\"..\/input\/tweets\/\"))\n\n# Models Packages\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import feature_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.ensemble import RandomForestRegressor\n# Gradient Boosting\nimport lightgbm as lgb\n\n# Tf-Idf\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.pipeline import FeatureUnion\nfrom scipy.sparse import hstack, csr_matrix\nfrom nltk.corpus import stopwords \n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n%matplotlib inline\n\n# Ignore Future Warnings\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","518996b2":"# Load Data\nPATH = \"..\/input\/tweets\/\" # Google's Cloud Directory (Linux Backend)\ndf = pd.read_excel(PATH + \"tweets.xlsx\", sheet_name=\"Sheet1\")#.sample(5000) # Uncomment to Debug..\nprint(\"Dataset Shape: {} Rows, {} Columns\".format(*df.shape))\n\n# Parse Date Time\ndf[\"TweetPostedTime\"] = pd.to_datetime(df['TweetPostedTime'])\ndf[\"UserSignupDate\"] = pd.to_datetime(df['UserSignupDate'])\n\n# Glance\ndisplay(df.head())","40c5ed3c":"# Remove Columns with 95%+ Missing\nmissing = round(df.isnull().sum()\/ df.shape[0]*100).reset_index().rename({\"index\":\"columns\",0:\"missing\"}, axis =1 )\nhigh_missing_columns = missing.loc[missing.missing > 95, \"columns\"]\nprint(\"Columns to remove (95% missing Values and Over)\\n\", list(high_missing_columns))\ndf.drop(high_missing_columns,axis =1, inplace= True)","295a1083":"# Remove Retweets..\nprint(\"Retweet Percent Breakdown:\\n\", round(df[\"TweetRetweetFlag\"].value_counts(normalize=True)*100))\ndf = df.loc[df[\"TweetRetweetFlag\"] == False, :]\nprint(\"\\nDataframe Dimension after removing retweets: {} Rows, {} Columns\".format(*df.shape))\ndf.drop(\"TweetRetweetFlag\", axis = 1, inplace=True) # Obsolete variable","f43a4add":"def custom_describe(df):\n    \"\"\"\n    Custom Describe function to extract important information\n    \"\"\"\n    unique_count = []\n    for x in df.columns:\n        mode = df[x].mode().iloc[0]\n        unique_count.append([x,\n                             len(df[x].unique()),\n                             df[x].isnull().sum(),\n                             mode,\n                             df[x][df[x]==mode].count(),\n                             df[x].dtypes])\n    print(\"Dataframe Dimension: {} Rows, {} Columns\".format(*df.shape))\n    return pd.DataFrame(unique_count, columns=[\"Column\",\"Number of Unique Values\",\"Missing Count\",\"Mode\",\"Mode Occurence\",\"dtype\"]).set_index(\"Column\")\n\n# Visualize\ncustom_describe(df)","89d87889":"# Source: My Taxi Far Prediction Project - https:\/\/www.kaggle.com\/nicapotato\/taxi-rides-time-analysis-and-oof-lgbm\ndef prepare_time_features(df, time_var):\n    df['hour_of_day'] = df[time_var].dt.hour\n    df['week'] = df[time_var].dt.week\n    df['month'] = df[time_var].dt.month\n    df[\"year\"] = df[time_var].dt.year\n    df['day_of_year'] = df[time_var].dt.dayofyear\n    df['week_of_year'] = df[time_var].dt.weekofyear\n    df[\"weekday\"] = df[time_var].dt.weekday\n    df[\"quarter\"] = df[time_var].dt.quarter\n    df[\"day_of_month\"] = df[time_var].dt.day\n    \n    return df\n# Run Function..\ndf = prepare_time_features(df, \"TweetPostedTime\")\n\n# Calculate Account Age- The age is relative to most recent tweet in df\ndf[\"account_age_days\"] = (df.TweetPostedTime.max()- df[\"UserSignupDate\"]).astype('timedelta64[D]').astype(int)\n# Remove time-formatted variables\ndf.drop(['TweetPostedTime', \"UserSignupDate\"], axis = 1, inplace=True)","2f82799a":"# Extract Tweet Source\ndf[\"TweetSource\"] = df[\"TweetSource\"].apply(lambda url: url.split(\">\")[1].split(\"<\/a\")[0])\nprint(df[\"TweetSource\"].value_counts()[:5])","cca3b5b0":"# Drop Redundant Variables\ndf.drop([\"UserName\",\"UserScreenName\", \"TweetID\", \"UserLink\"], axis =1, inplace=True)\n\n# Label Encoder:\ncategorical_to_encode = [\"UserID\", \"UserExpandedLink\", \"TweetSource\"]\nlbl = preprocessing.LabelEncoder()\nfor col in categorical_to_encode:\n    df[col] = lbl.fit_transform(df[col].astype(str))","41505634":"# Seperating Features for Modeling\ntext_features = [\"UserLocation\",\"TweetBody\", \"UserDescription\"]\nbasic_features = [x for x in df.columns if x not in text_features + [\"TweetHashtags\", \"TweetID\"]]","5b4f53f8":"# Efficient One Hot Encoder for Hastags\n# Source: My Cuisine Predicting Project - https:\/\/www.kaggle.com\/nicapotato\/this-model-is-bland-simple-logistic-starter\nvect = CountVectorizer(tokenizer=lambda x: [i.strip() for i in x.split(',')], lowercase=False)\ndummies = vect.fit_transform(df['TweetHashtags'].str.lower().apply(lambda x: [str(x)]).apply(','.join))\nprint(\"Vectorized Hashtag Sample : \", vect.get_feature_names()[14:22])\nvect_names = vect.get_feature_names()\n\n# Merge new features and Delete original variable\ndf = pd.concat([df.reset_index(drop=True), pd.DataFrame(dummies.todense(),columns=vect.get_feature_names())], axis=1)\nprint(\"Vocab Length: \", len(vect.get_feature_names()))\nprint(\"All Data Shape: \", df.shape)\ndf.drop(\"TweetHashtags\",axis=1,inplace=True)\ndel dummies; gc.collect();","ca8ace23":"# Once again my work - Avito NLP Model\n# https:\/\/www.kaggle.com\/nicapotato\/bow-meta-text-and-dense-features-lgbm\/code\nfor cols in text_features:\n    df[cols] = df[cols].astype(str) \n    df[cols] = df[cols].astype(str).fillna('FILLNA') # FILL NA\n    df[cols] = df[cols].str.lower() # Lowercase all text, so that capitalized words dont get treated differently\n    df[cols + '_num_words'] = df[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n    df[cols + '_num_unique_words'] = df[cols].apply(lambda comment: len(set(w for w in comment.split())))\n    df[cols + '_words_vs_unique'] = df[cols+'_num_unique_words'] \/ df[cols+'_num_words'] * 100 # Count Unique Words","421b7a8a":"f, ax = plt.subplots(figsize = [8,4])\nsns.kdeplot(df[\"TweetRetweetCount\"], ax= ax)\nax.set_title(\"RetweetCount Distribution (Continuous Dependent Variable)\")\nax.set_xlabel(\"Retweets\")\nax.set_ylabel(\"Density Distribution\")\nplt.show()","de17a71a":"f, ax = plt.subplots(figsize=[10,7])\ndate_feats = [\"week\",\"month\",\"hour_of_day\",\"year\",\"day_of_year\",\"week_of_year\",\"weekday\",\"quarter\",\"day_of_month\"]\nsns.heatmap(df[[x for x in basic_features if x not in date_feats + [\"UserID\",\"TweetSource\"]]].corr(),\n            annot=False, fmt=\".2f\",cbar_kws={'label': 'Correlation Coefficient'},cmap=\"plasma\",ax=ax, linewidths=.5)\nax.set_title(\"Dense Features Correlation Matrix\")\nplt.savefig('correlation_matrix.png')","0522652b":"print(\"Correlation Coefficient: \",df[\"TweetRetweetCount\"].corr(df[\"TweetFavoritesCount\"]))\nf, ax = plt.subplots(figsize = [8,4])\nsns.regplot(data = df, x = \"TweetRetweetCount\", y = \"TweetFavoritesCount\", ax= ax)\nax.set_title(\"Retweet Count and Favorite Count Correlation..\")\nplt.show()","a98dc855":"df.drop([\"TweetFavoritesCount\"], axis = 1, inplace = True)","46f7dd85":"# Mean Square Error - Model Selection Metric\ndef mse(y_true, y_pred):\n    return np.mean((y_true-y_pred)**2)\nscoring = make_scorer(mse, greater_is_better=False)\n\n# Helper Function to visualize feature importance for my Decision Trees..\n# Source: My Titanic ML guide - https:\/\/www.kaggle.com\/nicapotato\/titanic-voting-pipeline-stack-and-guide\nplt.rcParams['figure.figsize'] = (8, 8)\ndef test_score_and_feature_imp(model, predictors):\n    MO = model.fit(X_train, y_train)\n    print(\"Test Score: \", mse(y_test, model.predict(X_test)))\n    feat_imp = pd.DataFrame({\"Features\" : predictors, \"Importance\" : MO.feature_importances_}).sort_values(by = \"Importance\",ascending=False)[:30]\n    sns.barplot(x=\"Importance\", y=\"Features\", data=feat_imp)\n    plt.title(\"Feature Importance\")\n    plt.show()","905e3ab3":"print(\"Missing Values?\",df[[x for x in df.columns if x not in text_features + [\"TweetHashtags\", \"TweetID\"]]].isnull().sum().any())\nfeatures = [x for x in df.columns if x not in text_features + [\"TweetHashtags\", \"TweetID\"]]\n\n# Define y\/x\ny = df[\"TweetRetweetCount\"]\nX = df[[x for x in features if x not in [\"TweetRetweetCount\"]]]\n\n# Train \/ Test Split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=11)\n\nprint(\"Train\/Test - X\/y Shape\")\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","3ad9489e":"regressor = RandomForestRegressor(random_state=0, n_estimators = 10, max_depth = 4)\nscore = cross_val_score(regressor, X, y, cv=5, scoring = scoring)\nprint(\"CV - Mean: {} +\/- {}\".format(round(-score.mean(), 2), round(score.std(),2 )))\ntest_score_and_feature_imp(regressor, X.columns)","d69bd05a":"# Source: My Seed Diversification Notebook for Santander Bank\n# https:\/\/www.kaggle.com\/nicapotato\/lgbm-cv-tuning-and-seed-diversification\nlgtrain = lgb.Dataset(df[features],y ,feature_name = \"auto\")\n\nprint(\"Light Gradient Boosting Regressor: \")\nlgbm_params =  {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'mse',\n                }\n\nmodelstart= time.time()\n# Find Optimal Parameters \/ Boosting Rounds\nlgb_cv = lgb.cv(\n    params = lgbm_params,\n    train_set = lgtrain,\n    num_boost_round=2000,\n    stratified=False,\n    nfold = 5,\n    verbose_eval=150,\n    seed = 23,\n    early_stopping_rounds=75)\n\nmymetric = \"l2\"\noptimal_rounds = np.argmin(lgb_cv[mymetric + '-mean'])\nbest_cv_score = min(lgb_cv[mymetric + '-mean'])\n\nprint(\"\\nOptimal Round: {}\\nOptimal Score: {} + {}\".format(\n    optimal_rounds,best_cv_score,lgb_cv[mymetric + '-stdv'][optimal_rounds]))\nprint(\"Model Runtime: %0.2f Minutes\"%((time.time() - modelstart)\/60))","4cd86bc6":"lgb_clf = lgb.train(\n    lgbm_params,\n    lgb.Dataset(X_train,y_train ,feature_name = \"auto\"),\n    num_boost_round=optimal_rounds + 1,\n    verbose_eval=100\n)\nprint(\"Test Score: \", mse(y_test, lgb_clf.predict(X_test)))\n\n# Feature Importance Plot\nf, ax = plt.subplots(figsize=[7,10])\nlgb.plot_importance(lgb_clf, max_num_features=50, ax=ax)\nplt.title(\"Light GBM Feature Importance\")\nplt.show()","eceef854":"# Free Up Memory with Garbage Collector..\ndel X, X_train, X_test, y_train, y_test, regressor; gc.collect()","ef851f12":"# Source:\n# My Advertisement Quality Prediction Algorithm for Avito\n# https:\/\/www.kaggle.com\/nicapotato\/bow-meta-text-and-dense-features-lgbm\/code\n\n# # Prep Text\ntext_df = df.loc[:,text_features]\n    \n# TF-IDF\ndefault_preprocessor = CountVectorizer().build_preprocessor()\ndef build_preprocessor(field):\n    field_idx = list(text_df.columns).index(field)\n    return lambda x: default_preprocessor(x[field_idx])\n\ntfidf_para = {\n    \"stop_words\": \"english\",\n    \"analyzer\": 'char',\n    \"token_pattern\": r'\\w{1,}',\n    \"ngram_range\": (2, 6),\n    \"sublinear_tf\": True,\n    \"dtype\": np.float32,\n    \"norm\": 'l2',\n    \"smooth_idf\":False\n}\n\nvectorizer = FeatureUnion([\n        ('TweetBody',TfidfVectorizer(\n            max_features= 500,\n            **tfidf_para,\n            preprocessor=build_preprocessor('TweetBody'))),\n        ('UserDescription',TfidfVectorizer(\n            **tfidf_para,\n            max_features=500,\n            preprocessor=build_preprocessor('UserDescription'))),\n        ('UserLocation',CountVectorizer(\n            ngram_range=(1, 1),\n            max_features=100,\n            preprocessor=build_preprocessor('UserLocation')))\n    ])\n\nstart_vect=time.time()\ntext_df = vectorizer.fit_transform(text_df.values)\ntext_vocab = vectorizer.get_feature_names()\nprint(\"Vectorization Runtime: %0.2f Minutes\"%((time.time() - start_vect)\/60))","28eaeb47":"# Remove Raw Text Features\ndf.drop(text_features + [\"TweetRetweetCount\"], axis=1 ,inplace= True)\n\n# Sparse Matrix\nfulldf = hstack([csr_matrix(df.values), text_df])\nfulldf_feature_names = list(df.columns) + text_vocab\nprint(\"Final DF Shape: {} Rows, {} Columns\".format(*fulldf.shape))\nprint(\"Structured Features: \", len(df.columns))\nprint(\"Text Features: \", len(text_vocab))\n\n# Clear Memory\ndel df, text_df; gc.collect();","6a23a103":"# Source: My Seed Diversification Notebook for Santander Bank\n# https:\/\/www.kaggle.com\/nicapotato\/lgbm-cv-tuning-and-seed-diversification\nlgtrain = lgb.Dataset(fulldf,y ,feature_name = fulldf_feature_names)\n\nprint(\"Light Gradient Boosting Regressor: \")\nlgbm_params =  {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'mse',\n                }\n\nmodelstart= time.time()\n# Find Optimal Parameters \/ Boosting Rounds\nlgb_cv = lgb.cv(\n    params = lgbm_params,\n    train_set = lgtrain,\n    num_boost_round=2000,\n    stratified=False,\n    nfold=5,\n    verbose_eval=150,\n    seed=23,\n    early_stopping_rounds=75)\n\nmymetric = \"l2\"\noptimal_rounds = np.argmin(lgb_cv[mymetric + '-mean'])\nbest_cv_score = min(lgb_cv[mymetric + '-mean'])\n\nprint(\"\\nOptimal Round: {}\\nOptimal Score: {} + {}\".format(\n    optimal_rounds,best_cv_score,lgb_cv[mymetric + '-stdv'][optimal_rounds]))\nprint(\"Model Runtime: %0.2f Minutes\"%((time.time() - modelstart)\/60))","4bb3b2a5":"# Test \/ Train \/ Split\nX_train, X_test, y_train, y_test = train_test_split(fulldf, y, test_size=.3, random_state=11)\n\n# Run on Train Set\nlgb_clf = lgb.train(\n    lgbm_params,\n    lgb.Dataset(X_train,y_train ,feature_name = fulldf_feature_names),\n    num_boost_round=optimal_rounds + 1,\n    verbose_eval=100\n)\n# Test Score\nprint(\"Test Score: \", mse(y_test, lgb_clf.predict(X_test)))\n\n# Feature Importance Plot\nf, ax = plt.subplots(figsize=[7,10])\nlgb.plot_importance(lgb_clf, max_num_features=50, ax=ax)\nplt.title(\"Light GBM Feature Importance\")\nplt.show()","e53843bc":"print(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)\/60))","96f01a17":"# Math and Data Processing\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random\nimport gc\nimport os\nimport time\nnotebookstart= time.time()\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n%matplotlib inline\nfrom IPython.display import display\n\n# Ignore Future Warnings\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nprint(\"Data:\\n\",os.listdir(\"..\/input\/sgr-analyst\"))","c66c7e5a":"# Load Data into Memory\ndf = pd.read_excel(\"..\/input\/sgr-analyst\/SGR Analyst Task.xlsx\", sheet_name=\"Consulting market size\")\nprint(\"Dataset:\")\ndisplay(df.head())","c731c178":"f, ax = plt.subplots(1,2,figsize=[12,5])\n# Figure 1\nsns.countplot(y = df[\"Region\"], order= df[\"Region\"].value_counts().index[:], ax=ax[0])\nax[0].set_title(\"Region Occurence Count\")\n# Figure 2\nsns.countplot(y = df[\"Report region\"], order= df[\"Report region\"].value_counts().index[:], ax=ax[1])\nax[1].set_title(\"Report Region Occurence Count\")\n# Format\nplt.tight_layout(pad=0)\nplt.show()\n","24e110c4":"print(\"Glancing at Classes..\")\nfor x in [\"Country\",\"Industry\",\"Sector\",\"Service\",\"Firm Type\"]:\n    print(\"Classes in the {} Variable:\\n {}\\n\".format(x,df[x].unique()))","28bae9ed":"# Rename Variables\ndf.columns = ['Region', 'Report region', 'Country', 'Industry', 'Sector', 'Service',\n       'Firm Type', '2013', '2014', '2015', '2016', '2017',\n       '2018', '2019']\n\n# Stack Growth Variable into a single variable\nmelt_df = pd.melt(df, id_vars=list(df.columns[:7]), value_vars=list(df.columns[7:]), value_name='Growth', var_name = \"Year\")\nmelt_df[\"Year\"] = melt_df[\"Year\"].astype(int)\nprint(\"Stack Data\")\ndisplay(melt_df.head())","2549c291":"# Density Plot by Year\nf, ax = plt.subplots(figsize=[12,5])\nfor year in melt_df.Year.unique():\n    sns.kdeplot(melt_df.loc[melt_df.Year == year, \"Growth\"], label = year)\nax.set_title(\"Growth Density Distribution\")\nax.set_xlabel(\"Growth\")\nax.set_ylabel(\"Density\")\nplt.show()","402a5426":"plot_vars = [\"Region\",\"Industry\",\"Service\",\"Firm Type\"]\nf, ax = plt.subplots(len(plot_vars)+ 1,1, figsize = [10,16], sharey = True)\n\n# All Data\nsns.pointplot(x=\"Year\", y=\"Growth\",data=melt_df, err_style=\"bars\", ci=68, ax=ax[0])\nax[0].set_title(\"Growth Average by Year\")\n\n# By Specific Variable\nfor i,var in enumerate(plot_vars):\n    sns.pointplot(x= \"Year\", y= \"Growth\", hue= var, data= melt_df, err_style=\"bars\", ci= 67, ax= ax[i+1])\n    ax[i+1].set_title(\"{} Growth Average by Year\".format(var.capitalize()))\n    ax[i+1].legend(fontsize='large', loc='center left', bbox_to_anchor=(1, 0.5))\nplt.tight_layout(pad=0)\nplt.show()","a3b5dc89":"print(\"Original Dataset\")\ndisplay(df.head())\ndiff = df.iloc[:,-7:].diff(axis=1).iloc[:,1:]\n\nprint(\"Measuring Difference Year to Year Dataset\")\ndiff = pd.concat([df.iloc[:,:7], diff], axis=1)\ndisplay(diff.head())\n\nprint(\"Stack Growth Difference Dataset\")\ndiff_melt_df = pd.melt(diff, id_vars=list(diff.columns[:7]), value_vars=list(diff.columns[7:]), value_name='Growth', var_name = \"Year\")\ndiff_melt_df[\"Year\"] = diff_melt_df[\"Year\"].astype(int)\ndisplay(diff_melt_df.head())","7915867f":"f, ax = plt.subplots(1,2, figsize = [14,5], sharex= False)\nsns.pointplot(x=\"Year\", y=\"Growth\", hue= \"Sector\", data=melt_df, ax=ax[0], ci=None)\nax[0].set_title(\"Sector Growth Average by Year\")\nax[0].legend_.remove()\n\nsns.pointplot(x=\"Year\", y=\"Growth\", hue= \"Sector\", data = diff_melt_df, ax=ax[1], ci=None)\nax[1].set_title(\"Sector Average Growth Difference by Year\")\nax[1].set_ylabel(\"Growth Difference\")\nax[1].legend_.remove()\n\nplt.show()","68d717b1":"# Compute Average Growth\ndf[\"mean\"] = df.loc[:,['2013', '2014', '2015', '2016', '2017', '2018', '2019']].mean(axis = 1)\ndf[\"stdev\"] =  df.loc[:,['2013', '2014', '2015', '2016', '2017', '2018', '2019']].std(axis = 1)\n\n# Compute Average Growth Difference\ndiff[\"mean\"] =  diff.loc[:,['2014', '2015', '2016', '2017', '2018', '2019']].mean(axis = 1)\ndiff[\"stdev\"] =  diff.loc[:,['2014', '2015', '2016', '2017', '2018', '2019']].std(axis = 1)\n\n# Heatmap\nf, ax = plt.subplots(1,2, figsize=[15,7], sharey=True)\n# Plot 1\ntps = df.pivot_table(values=['mean'], index='Sector',columns='Service',aggfunc='mean')\ntps.columns = tps.columns.droplevel()\ntps = tps.div(tps.sum(axis=1), axis=0)\nsns.heatmap(tps, annot=False, fmt=\".2f\",cmap= sns.light_palette(\"blue\", as_cmap=True), ax=ax[0])\nax[0].set_title(\"Average Growth's\\nInfluence of Services on Sectors\")\ndel tps\n# Plot 2\ntps = diff.pivot_table(values=['mean'], index='Sector',columns='Service',aggfunc='mean')\ntps.columns = tps.columns.droplevel()\ntps = tps.div(tps.sum(axis=1), axis=0)\nsns.heatmap(tps, annot=False, fmt=\".2f\",cbar_kws={'label': 'Percent Influence'},cmap= sns.light_palette(\"blue\", as_cmap=True), ax=ax[1])\nax[1].set_title(\"Average Growth Difference's\\nInfluence of Services on Sectors\")\ndel tps\n# Layout\nplt.tight_layout(pad=0)\nplt.show()","f364f623":"f, ax = plt.subplots(1,2,figsize=[13,10], sharey=True)\n\n# Stacked Bar Plot 1\ntps = df.pivot_table(values=['mean'], index='Sector',columns='Service',aggfunc='mean')\ntps.columns = tps.columns.droplevel()\ntps = tps.div(tps.sum(1), axis=0)\ntps.plot(kind='barh', stacked=True, ax=ax[0])\nax[0].legend(fontsize='large', loc='center left', bbox_to_anchor=(1, 0.5))\nax[0].set_xlabel(\"Average Growth Influence\")\nax[0].set_title(\"Average Growth's\\nInfluence of Services on Sectors\")\nax[0].legend_.remove()\ndel tps\n\n# Stacked Bar Plot 2\ntps = diff.pivot_table(values=['mean'], index='Sector',columns='Service',aggfunc='mean')\ntps.columns = tps.columns.droplevel()\ntps = tps.div(tps.sum(1), axis=0)\ntps.plot(kind='barh', stacked=True, ax=ax[1])\nax[1].legend(fontsize='large', loc='center left', bbox_to_anchor=(1, 0.5))\nax[1].set_xlabel(\"Average Growth Influence\")\nax[1].set_title(\"Average Growth Diffrence's\\nInfluence of Services on Sectors\")\ndel tps\ngc.collect()\n\nplt.tight_layout(pad=0)\nplt.show()","9acbe0ff":"plot_vars = sorted(melt_df[\"Sector\"].unique())\nf, ax = plt.subplots(len(plot_vars),2, figsize = [15,100], squeeze= True)\n\n# By Specific Variable\nfor i,var in enumerate(plot_vars):\n    # Column 1 Plots\n    sns.pointplot(x= \"Year\", y= \"Growth\", hue= \"Service\", data= melt_df.loc[melt_df.Sector == var,:], ci= 25, ax= ax[i, 0])\n    ax[i, 0].set_title(\"{}'s AvgGrowth by Year\".format(var.title()))\n    ax[i, 0].set_ylabel(\"Average Difference\")\n    ax[i, 0].legend_.remove()\n    \n    # Column 2 Plots\n    sns.pointplot(x= \"Year\", y= \"Growth\", hue= \"Service\", data= diff_melt_df.loc[diff_melt_df.Sector == var,:], ci= 25, ax= ax[i, 1])\n    ax[i,1].set_title(\"{}'s Avg Diff Growth by Year\".format(var.title()))\n    ax[i, 1].set_ylabel(\"Average Growth Difference\")\n    ax[i, 1].legend(fontsize='large', loc='center left', bbox_to_anchor=(1, 0.5))\n\n# Layout\nplt.tight_layout(pad=1)\nplt.show()","ac8ee9be":"top5_countries  = diff[[\"mean\",\"Country\"]].groupby(\"Country\").mean().sort_values(by=\"mean\", ascending= False)[:5]\nprint(\"Top 5 Fastest Growing Countries - Measured by Average Growth Difference by Year\")\ndisplay(top5_countries.T)\nprint(\"Glancing at the African Continent figures..\")\ndisplay(diff.loc[df.Region == \"Africa\", [\"mean\",\"Country\"]].groupby(\"Country\").mean().sort_values(by= \"mean\", ascending=False).T)","4eeea13c":"# Heatmap\nf, ax = plt.subplots(1,1, figsize=[8,7], sharey=True)\ntps = diff.loc[diff.Country.isin(top5_countries.index),:].pivot_table(values=['mean'], index='Country',columns='Firm Type',aggfunc='mean')\ntps.columns = tps.columns.droplevel()\ntps = tps.div(tps.sum(axis=1), axis=0)\ntps = tps.loc[top5_countries.index]\n\n# Plot Figure\nsns.heatmap(tps, annot=True, fmt=\".2f\",cbar_kws={'label': 'Percent Influence'},cmap= sns.light_palette(\"red\", as_cmap=True), ax=ax)\nax.set_title(\"Average Growth Difference's\\nInfluence of Firm Type on Top 5 Countries\")\nax.set_ylabel(\"Ranked Countries [Top to Bottom]\")\ndel tps\n# Layout\nplt.yticks(rotation=0) \nplt.tight_layout(pad=0)\nplt.show()","92f0e900":"df.head()","0122ec10":"# Variable to Predict\ny = df[['2013', '2014', '2015', '2016', '2017', '2018', '2019']]\nyears = len(y.columns)\n\n# Scale years to low magnitude\nX = np.array(range(1,8))\n\n# Fit Line Function\nfrom statistics import mean\ndef best_fit_slope_and_intercept(xs,ys):\n    \"\"\"\n    Fit OLS Regression Line\n    \"\"\"\n    m = (((mean(xs)*mean(ys)) - mean(xs*ys)) \/ ((mean(xs)*mean(xs)) - mean(xs*xs)))\n    b = mean(ys) - m*mean(xs)\n    return m, b","fe19c0d2":"def predict_extrapolate(X, y):\n    # Linear Regression from Scratch\n    m, b = best_fit_slope_and_intercept(X,y)\n    regression_line = [(m*x)+b for x in X]\n    X_extrapolate = np.array(list(range(8, 11)))\n    regression_line_future = [(m*x)+b for x in X_extrapolate]\n    \n    # Polynomial Regression with Numpy (2 Degrees)\n    poly_coefs = np.polyfit(X,y, 2)\n    ffit = np.polyval(poly_coefs, list(range(1,11)))\n\n    # Linear Line\n    plt.scatter(X,y,color='#003F72', label = \"Training\")\n    plt.scatter(X_extrapolate,regression_line_future,color='g', label= \"Linear Prediction\")\n    plt.plot(list(range(1,11)), regression_line + regression_line_future, color='g')\n    \n    # Polynomial Line\n    # plt.scatter(X,y_train,color='#003F72', label = \"Training\")\n    plt.plot(list(range(1,11)), ffit)\n    plt.scatter(X_extrapolate,ffit[-3:],color='r', label= \"2nd Polynomial Prediction\")\n    \n    plt.legend(loc=4)\n    plt.show()","db3fef51":"for inter in range(10):\n    random_index = random.randint(1,y.shape[0])\n    print(\"Sample: \", random_index, \"\\n\")\n    dictionary = df.iloc[random_index,:7].to_dict()\n    key = [x for x in dictionary]\n    [print(\"{}: {}\".format(k, dictionary[k])) for k in key]\n    predict_extrapolate(X,np.array(y.iloc[random_index,:]))\n    print(\"-------------------------------------------------\\n\")","6e1a7526":"def polynomial_fit_predict(X, y):\n    poly_coefs = np.polyfit(X,y, 2)\n    ffit = np.polyval(poly_coefs, list(range(8,11)))\n    return ffit","dfc17f9c":"%%time\ndf[[\"2020\",\"2021\",\"2022\"]] = pd.DataFrame(df[['2013', '2014', '2015', '2016', '2017', '2018', '2019']].apply(\n    lambda s: polynomial_fit_predict(X,s), axis=1).values.tolist())","9659d4d0":"print(\"Dataset with Prediction Sample:\")\ndf.drop([\"mean\",\"stdev\"],axis=1).sample(8)","ea102940":"print(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)\/60))","f20a25a6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport time\nnotebookstart = time.time()\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split, cross_validate, cross_val_predict, KFold\nfrom sklearn.metrics import mean_absolute_error\nimport lightgbm as lgb\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport itertools\n\nimport math\nimport gc\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\n\npd.options.display.max_rows = 999\npd.options.display.width = 300\npd.options.display.max_columns = 500\n\nplot_title_size = 16","7ee23ff8":"def plot_cloud(string, ax, title = \"WordCloud\", cmap = \"plasma\"):\n    wordcloud = WordCloud(width=800, height=500,\n                          collocations=True,\n                          background_color=\"white\",\n                          colormap=cmap\n                          ).generate(string)\n\n    ax.imshow(wordcloud, interpolation='bilinear')\n    ax.set_title(title,  fontsize=18)\n    ax.axis('off')\n\ndef big_word_cloud(plot_df, plt_set, columns, figsize, cmap = \"plasma\"):\n    \"\"\"\n    Iteratively Plot WordClouds\n    \"\"\"\n    rows = math.ceil(len(plt_set)\/columns)\n    n_plots = rows*columns\n    f,ax = plt.subplots(rows, columns, figsize = figsize)\n    for i in range(0,n_plots):\n        ax = plt.subplot(rows, columns, i+1)\n        if i < len(plt_set):\n            str_col = plt_set[i]\n            string = \" \".join(plot_df.loc[plot_df[str_col].notnull(),str_col].astype(str).str.lower().str.replace(\"none\", \"\").str.title())\n            string += 'EMPTY'\n            ax = plt.subplot(rows, 2, i+1)\n            plot_cloud(string, ax, title = \"{} - {} Missing\".format(str_col.title(), plot_df[str_col].isnull().sum()), cmap = cmap)\n        else:\n            ax.axis('off')\n    plt.tight_layout(pad=0)\n\ndef big_count_plotter(plot_df, plt_set, columns, figsize, custom_palette = sns.color_palette(\"Dark2\", 15), top_n = 15):\n    \"\"\"\n    Iteratively Plot all categorical columns\n    Has category pre-processing - remove whitespace, lower, title, and takes first 30 characters.\n    \"\"\"\n    rows = math.ceil(len(plt_set)\/columns)\n    n_plots = rows*columns\n    f,ax = plt.subplots(rows, columns, figsize = figsize)\n    for i in range(0,n_plots):\n        ax = plt.subplot(rows, columns, i+1)\n        if i < len(plt_set):\n            c_col = plt_set[i]\n            plt_tmp = plot_df.loc[plot_df[c_col].notnull(),c_col].astype(str).str.lower().str.strip().str.title().apply(lambda x: x[:30])\n            plot_order = plt_tmp.value_counts().index[:top_n]\n            sns.countplot(y = plt_tmp, ax = ax, order = plot_order, palette = custom_palette)\n            ax.set_title(\"{} - {} Missing\".format(c_col.title(), plot_df[c_col].isnull().sum()), fontsize = plot_title_size)\n            ax.set_ylabel(\"{} Categories\".format(c_col.title()))\n            ax.set_xlabel(\"Count\")\n        else:\n            ax.axis('off')\n\n    plt.tight_layout(pad=1)\n    \ndef big_boxplotter(plot_df, plt_set, columns, figsize, custom_palette = sns.color_palette(\"Dark2\", 15), quantile = .99):\n    rows = math.ceil(len(plt_set)\/columns)\n    n_plots = rows*columns\n    f,ax = plt.subplots(rows, columns, figsize = figsize)\n    palette = itertools.cycle(custom_palette)\n    for i in range(0,n_plots):\n        ax = plt.subplot(rows, columns, i+1)\n        if i < len(plt_set):\n            cont_col = plt_set[i]\n            plt_tmp = plot_df.loc[plot_df[cont_col].notnull(),cont_col].astype(float)\n            col_max = plt_tmp.max()\n            if quantile:\n                plt_tmp = plt_tmp[plt_tmp < plt_tmp.quantile(quantile)]\n            sns.boxplot(plt_tmp, color = next(palette))\n            ax.set_title(\"{}\\n{} Missing - {} Max\".format(cont_col.title(), plot_df[cont_col].isnull().sum(), col_max),\n                        fontsize = plot_title_size)\n            ax.set_xlabel(\"Values (Upper Limit is {}th Quantile)\".format(str(quantile)))\n            ax.set_ylabel(\"{} Boxplot\".format(cont_col))\n        else:\n            ax.axis('off')\n\n    plt.tight_layout(pad=1)\n    \ndef big_ts_count_plotter(plot_df, plt_set, columns, ts_var, figsize, custom_palette = sns.color_palette(\"Dark2\", 15), top_n = 5):\n    \"\"\"\n    Iteratively Plot all categorical columns over time\n    \"\"\"\n    rows = math.ceil(len(plt_set)\/columns)\n    n_plots = rows*columns\n    f,ax = plt.subplots(rows, columns, figsize = figsize)\n    for i in range(0,n_plots):\n        ax = plt.subplot(rows, columns, i+1)\n        if i < len(plt_set):\n            tmp_var = plt_set[i]\n            plot_df.loc[plot_df[ts_var].notnull(),:].set_index(ts_var)['placeholder'].resample('1m').count().plot(ax=ax, label=\"Full Count\")\n            for tv in plot_df[tmp_var].dropna().value_counts().index[:top_n]:\n                plot_df.loc[(plot_df[ts_var].notnull()) & (plot_df[tmp_var] == tv),:]\\\n                    .set_index(ts_var)['placeholder'].resample('1m').count().plot(ax=ax, label=tv)\n            ax.legend(fontsize='large', loc='center left', bbox_to_anchor=(1, 0.5))\n            ax.set_title(\"Monthly Count of {} Variable Through Time\".format(tmp_var), fontsize = plot_title_size)\n        else:\n            ax.axis('off')\n\n    plt.tight_layout(pad=1)\n\n    \ndef big_ts_mean_plotter(plot_df, plt_set, columns, ts_var, std_multiplier, figsize, custom_palette = sns.color_palette(\"Dark2\", 15)):\n    \"\"\"\n    Iteratively Plot all continuous columns over time\n    \"\"\"\n    rows = math.ceil(len(plt_set)\/columns)\n    n_plots = rows*columns\n    f,ax = plt.subplots(rows, columns, figsize = figsize)\n    palette = itertools.cycle(custom_palette)\n    \n    for i in range(0,n_plots):\n        ax = plt.subplot(rows, columns, i+1)\n        if i < len(plt_set):\n            c = next(palette)\n            tmp_var = plt_set[i]\n            monthly_aggs = (plot_df\n                           .set_index(ts_var)\n                           .resample('1m')\n                           .agg({tmp_var: ['mean','std']}))\n\n            monthly_aggs.columns = pd.Index([e[0] +\"_\"+ e[1] for e in monthly_aggs.columns.tolist()])\n\n            monthly_aggs = monthly_aggs.assign(\n                upper = monthly_aggs[tmp_var + \"_mean\"] + (monthly_aggs[tmp_var + \"_std\"] * std_multiplier),\n                lower = monthly_aggs[tmp_var + \"_mean\"] - (monthly_aggs[tmp_var + \"_std\"] * std_multiplier)\n            )\n\n            monthly_aggs[tmp_var+'_mean'].plot(ax=ax, color=c, label=\"{} Average\".format(tmp_var))\n            ax.fill_between(monthly_aggs.index, monthly_aggs.lower, monthly_aggs.upper, color=c, alpha=0.2)\n            ax.legend(fontsize='large', loc='upper center', bbox_to_anchor=(0.5, -0.12))\n            ax.set_title(\"Monthly Count of {} Variable Through Time\".format(tmp_var), fontsize = plot_title_size)\n        else:\n            ax.axis('off')\n\n    plt.tight_layout(pad=1)\n\ndef prepare_time_features(df, ts_var):\n#     df['hour_of_day'] = df[ts_var].dt.hour\n#     df['week'] = df[ts_var].dt.week\n    df['month'] = df[ts_var].dt.month\n    df[\"year\"] = df[ts_var].dt.year\n#     df['day_of_year'] = df[ts_var].dt.dayofyear\n    df['week_of_year'] = df[ts_var].dt.weekofyear\n#     df[\"weekday\"] = df[ts_var].dt.weekday\n    df[\"quarter\"] = df[ts_var].dt.quarter\n#     df[\"day_of_month\"] = df[ts_var].dt.day\n    \n    return df\n\ndef time_slicer(plot_df, timeframes, value, agg_method, color=\"purple\", figsize=[12,12]):\n    \"\"\"\n    Function to count observation occurrence through different lenses of time.\n    \"\"\"\n    f, ax = plt.subplots(len(timeframes), figsize = figsize)\n    for i,x in enumerate(timeframes):\n        plot_df.loc[:,[x,value]].groupby([x]).agg({value:agg_method}).plot(ax=ax[i],color=color)\n        ax[i].set_ylabel(value.replace(\"_\", \" \").title())\n        ax[i].set_title(\"{} by {}\".format(value.replace(\"_\", \" \").title(), x.replace(\"_\", \" \").title()))\n        ax[i].set_xlabel(\"\")\n        ax[i].set_ylim(0,)\n    ax[len(timeframes)-1].set_xlabel(\"Time Frame\")\n    plt.tight_layout(pad=0)","d80a7c14":"df = pd.read_csv(\"..\/input\/housing-manchester\/Airbnb_Manchester.csv\")\nprint('DF shape: {} rows, {} columns'.format(*df.shape))\n\nprint(\"Number of unique Properties: {}\".format(df['Property ID'].nunique()))\nprint(\"Number of unique Host: {}\".format(df['Host ID'].nunique()))\n\ndisplay(df.sample(5))","d490b519":"df['Reporting Month'] = pd.to_datetime(df['Reporting Month'])\ndf = prepare_time_features(df, ts_var = 'Reporting Month')\ndf['placeholder'] = 1","f597566b":"# Get the last active date of property\ndf = pd.merge(df,\n         df.loc[df.Active == True,:].groupby('Property ID')['Reporting Month'].agg('max').rename(\"property_last_active\").reset_index(),\n         how='left', on = 'Property ID')\n# Get the first active date of property\ndf = pd.merge(df,\n         df.loc[df.Active == True,:].groupby('Property ID')['Reporting Month'].agg('min').rename(\"property_first_active\").reset_index(),\n         how='left', on = 'Property ID')\n# Get the first active date of property\ndf = pd.merge(df,\n         df.loc[df.Active == True,:].groupby('Host ID')['Reporting Month'].agg('max').rename(\"host_last_active\").reset_index(),\n         how='left', on = 'Host ID')\ndf = pd.merge(df,\n         df.loc[df.Active == True,:].groupby('Host ID')['Reporting Month'].agg('min').rename(\"host_first_active\").reset_index(),\n         how='left', on = 'Host ID')\n\n# Create features for Host and Properties around activity over time\ndf = df.assign(\n    # Since First Active in days\n    days_since_properties_first_active = (df['Reporting Month'] - df['property_first_active']).dt.days,\n    days_since_host_first_active = (df['Reporting Month'] - df['host_first_active']).dt.days,\n    # Until last Active in days\n    days_until_properties_last_active = (df['property_last_active'] - df['Reporting Month']).dt.days,\n    days_until_host_last_active = (df['host_last_active'] - df['Reporting Month']).dt.days,\n    # Total life in days\n    total_days_property_active = (df['property_last_active'] - df['property_first_active']).dt.days,\n    total_days_host_active = (df['host_last_active'] - df['host_first_active']).dt.days,\n    # Host Related Features\n    host_owned_properties = df.groupby('Host ID')[\"Property ID\"].transform('nunique')\n)","489372ee":"print(df.Active.value_counts(normalize=True))","d4b02829":"drop_cols =  [\n    'Revenue (USD)',\n    'ADR (USD)',\n    'Country',\n    'State',\n    'City',\n    'Zipcode',\n    'Currency Native'\n]\n\ndf = df.loc[df.Active == True,:].drop(drop_cols, axis=1)\nprint('DF shape: {} rows, {} columns'.format(*df.shape))","217b17e7":"df.sample(5)","c447aa02":"sns.boxplot(df.groupby('Host ID')[\"Property ID\"].agg('nunique'))\nplt.title(\"Uniqued Properties owned by HostID\")\nplt.show()","509fe4ef":"count_plot_cols = [\n    'Property Type',\n    'Listing Type',\n    'Bedrooms'\n]\n\nbig_count_plotter(plot_df = df,\n                  plt_set = count_plot_cols,\n                  columns = 2,\n                  figsize = [15,10])","f69e2a3d":"big_ts_count_plotter(plot_df = df.loc[df.Active == True,:],\n                      plt_set = count_plot_cols,\n                      columns = 2,\n                      ts_var = 'Reporting Month',\n                      figsize = [20,10])","62df0417":"f, ax = plt.subplots(1,2, figsize = [14,5])\ntmp_plot = df.groupby('year').agg({\"Host ID\": \"nunique\", \"Property ID\": \"nunique\"})\ntmp_plot.plot(ax=ax[0])\nax[0].set_ylim(0, )\nax[0].set_title(\"Number of Unique Host and Properties\", fontsize = plot_title_size)\n\ntmp_plot['Properties to Host Ratio'] = tmp_plot['Property ID'] \/ tmp_plot['Host ID']\ntmp_plot['Properties to Host Ratio'].plot(ax=ax[1])\nax[1].set_ylim(1, 2)\nax[1].set_title(\"Properties to Host Ratio\", fontsize = plot_title_size)\nax[1].set_ylabel(\"Properties to Host Ratio\")\nplt.tight_layout(pad=2)\nplt.show()","64700353":"continuous_cols = [\n    'Occupancy Rate',\n    'Revenue (Native)',\n    'ADR (Native)',\n    'Number of Reservations',\n    'Reservation Days',\n    'Available Days',\n    'Blocked Days',\n    'days_since_properties_first_active',\n    'days_since_host_first_active',\n    'days_until_properties_last_active',\n    'days_until_host_last_active',\n    'total_days_property_active',\n    'total_days_host_active'\n]\n\nbig_boxplotter(plot_df = df,\n               plt_set = continuous_cols,\n               columns = 3,\n               figsize = [20,25],\n               quantile = .98)","885aecbb":"df.loc[df['Revenue (Native)'] > 4000,:].sort_values(by='Revenue (Native)', ascending=False).iloc[:10]","89c26d77":"continuous_cols = [\n    'Occupancy Rate',\n    'Revenue (Native)',\n    'ADR (Native)',\n    'Number of Reservations',\n    'Reservation Days',\n    'Available Days',\n    'Blocked Days',\n    'days_since_properties_first_active',\n    'days_since_host_first_active',\n    'days_until_properties_last_active',\n    'days_until_host_last_active',\n    'total_days_property_active',\n    'total_days_host_active'\n]\n\nf, ax = plt.subplots(figsize = [8,5])\nsns.heatmap(df.loc[:,continuous_cols].corr(method = 'spearman'),\n            annot=False, fmt=\".2f\",cbar_kws={'label': 'Correlation Coefficient'},\n            cmap=\"coolwarm\",ax=ax, linewidths=.3, vmin=-1, vmax=1)\nax.set_title(\"Continuous Variables Correlation Matrix\", fontsize = 14)\nplt.show()","881e176d":"upper_quantile = .90\nlower_qunatile = .10\n\nsns.jointplot(x=\"Longitude\", y=\"Latitude\", data=df.loc[\n    (df['Longitude'] < df['Longitude'].quantile(upper_quantile)) &\n    (df['Longitude'] > df['Longitude'].quantile(lower_qunatile)) &\n    (df['Latitude'] < df['Latitude'].quantile(upper_quantile)) &\n    (df['Latitude'] > df['Latitude'].quantile(lower_qunatile))\n    ,[\"Longitude\", \"Latitude\"]], kind=\"kde\", color = 'green')\nplt.title(\"2D Density Plot\");","1c4a5a21":"continuous_cols = [\n    'Occupancy Rate',\n    'Revenue (Native)',\n    'ADR (Native)',\n    'Number of Reservations',\n    'Reservation Days',\n    'Available Days',\n    'Blocked Days'\n]\n\nbig_ts_mean_plotter(plot_df = df,\n                      plt_set = continuous_cols,\n                      columns = 2,\n                      std_multiplier = .5,\n                      ts_var = 'Reporting Month',\n                      figsize = [20,25])","a0d77889":"time_slicer(plot_df = df,\n            agg_method = 'mean',\n            timeframes=['year', 'quarter', \"month\"],\n            value = \"Occupancy Rate\", color=\"green\", figsize = [10,7])","dadd4ed6":"time_slicer(plot_df = df,\n            agg_method = 'sum',\n            timeframes=['year', 'quarter', \"month\"],\n            value = \"Revenue (Native)\", color=\"red\", figsize = [10,7])","3b9a7477":"columns_for_annual_sum = [\n    'Revenue (Native)',\n    'Number of Reservations',\n    'Reservation Days',\n    'Available Days',\n    'Blocked Days'\n]\n\nstatic_columns = [\n    'Property ID',\n    'Property Type',\n    'Listing Type',\n    'Bedrooms',\n    'Latitude',\n    'Longitude',\n    'year',\n    'Active',\n    'Host ID',\n    'total_days_property_active',\n    'total_days_host_active',\n    'host_owned_properties']","6cfa10c5":"model_df = df.loc[df.year == 2018,:].copy()\nprint('Model DF shape: {} rows, {} columns'.format(*model_df.shape))\n\nagg_df = model_df.groupby(['year','Property ID']).agg({k:'sum' for k in columns_for_annual_sum}).reset_index()\nprint('agg_df DF shape: {} rows, {} columns'.format(*agg_df.shape))\nyear_df = pd.merge(\n    agg_df,\n    model_df.loc[:,static_columns].drop_duplicates(), on = ['year','Property ID'], how = 'left', validate = 'one_to_one')\nprint('year_df DF shape: {} rows, {} columns'.format(*year_df.shape))\ndisplay(year_df.sample(5))\ndel agg_df, model_df","1dd7e8dd":"# Rebuild lost features\nyear_df = year_df.assign(\n    native_ADR = year_df['Revenue (Native)'] \/ year_df[\"Reservation Days\"], # Revenue \/ Reservation days = ADR\n    occupancy = year_df['Reservation Days'] \/ 365\n).set_index('Property ID')\n\nyear_df['native_ADR'].fillna(0, inplace=True)","7ec5b902":"year_df['Reservation Days'].describe()","b20c4b04":"dependent_variables  = 'Reservation Days'\ncategorical_variables = [\n    'Property Type',\n    'Listing Type'\n]\ncontinuous_variables = [\n    'native_ADR',\n    'host_owned_properties',\n    'Bedrooms',\n    'Latitude',\n    'Longitude',\n    'total_days_property_active',\n    'total_days_host_active',\n]","24f598aa":"def dummy_variables(process_df, model_categorical):\n    df = process_df.copy()\n    dummy_df = pd.DataFrame()\n    for cat_col in model_categorical:\n        df[cat_col] = df[cat_col].str.lower().str.strip()\n        print(\"{} Missing Values for {} - Dtype {}\".format(df[cat_col].isnull().sum(), cat_col, df[cat_col].dtypes))\n        df.loc[~df[cat_col].isin(df[cat_col].value_counts().index[:10]), cat_col] = cat_col + 'Other'\n        \n        \n        dummies = pd.get_dummies(df[cat_col])\n        dummies.columns = [x + \"_\" + cat_col for x in dummies.columns]\n        dummy_df = pd.concat([dummy_df, dummies],axis = 1)\n        \n    return dummy_df\n\ndef standardscaler_forpd(process_df, model_continuous):\n    df = process_df.loc[:,model_continuous].copy()\n    for col in df.columns:\n        scaler = StandardScaler()\n        df[col] = scaler.fit_transform(df[col].values.reshape(-1, 1))\n        \n    return df","abacc031":"# Dummy variables for categorical..\ndummy_df = dummy_variables(process_df = year_df, model_categorical = categorical_variables)\nprint(dummy_df.shape)\n\n# Standard Scaler for Continuous..\ncont_df = standardscaler_forpd(process_df = year_df, model_continuous = continuous_variables)\nprint(cont_df.shape)\ndisplay(cont_df.describe())\n\nX = pd.concat([cont_df, dummy_df], axis =1)\ny = year_df[dependent_variables].copy()\nprint(\"Dependent Variable Shape\",y.shape)\nprint(\"Training Set Shape\",X.shape)\ndel dummy_df, cont_df","501b4e7b":"X['intercept'] = 1\nmodel = LinearRegression(fit_intercept = False)\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\npredicted = cross_validate(model, X, y, cv=kf, scoring = 'neg_mean_absolute_error')\nprint(\"CV MAE: {:.4f} +\/- {:.4f}\".format(abs(predicted['test_score'].mean()), predicted['test_score'].std()))\n\npredicted = cross_val_predict(model, X, y, cv=kf)\n\n# https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_cv_predict.html#sphx-glr-auto-examples-model-selection-plot-cv-predict-py\nfig, ax = plt.subplots()\nax.scatter(y, predicted, edgecolors=(0, 0, 0))\nax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\nax.set_xlabel('Measured')\nax.set_ylabel('Predicted')\nax.set_title('Cross Validation Predictions\\nAnalyze Model Residuels vs. Actual Out of Fold')\nplt.show()","389d3e68":"print(\"Light Gradient Boosting Regressor: \")\nlgbm_params =  {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'mae',\n    \"learning_rate\": 0.05,\n#     \"num_leaves\": 180,\n#     \"feature_fraction\": 0.50,\n#     \"bagging_fraction\": 0.50,\n#     'bagging_freq': 4,\n#     \"max_depth\": -1,\n#     \"reg_alpha\": 0.3,\n#     \"reg_lambda\": 0.1,\n#     \"min_split_gain\":0.2,\n#     \"min_child_weight\":10,\n#     'zero_as_missing':True\n                }\nlgtrain = lgb.Dataset(X, y ,feature_name = \"auto\")\n\nlgb_cv = lgb.cv(\n    params = lgbm_params,\n    train_set = lgtrain,\n    num_boost_round=2000,\n    stratified=False,\n    nfold = 5,\n    verbose_eval=50,\n    seed = 23,\n    early_stopping_rounds=75)","997699f3":"X_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, test_size=0.10, random_state=23)\n\nlgtrain = lgb.Dataset(X_train, y_train)\nlgvalid = lgb.Dataset(X_valid, y_valid)\n\nlgb_reg = lgb.train(\n    lgbm_params,\n    lgtrain,\n    num_boost_round=1000,\n    valid_sets=[lgtrain, lgvalid],\n    valid_names=['train','valid'],\n    early_stopping_rounds=50,\n    verbose_eval=100)\nprint(\"Model Evaluation Stage\")\nprint('Validation Set MAE:', mean_absolute_error(y_valid, lgb_reg.predict(X_valid)))\n\n# Feature Importance Plot\nf, ax = plt.subplots(figsize=[7,10])\nlgb.plot_importance(lgb_reg, max_num_features=50, ax=ax)\nplt.title(\"Light GBM Feature Importance\")\nplt.show()","db593ad8":"# https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_cv_predict.html#sphx-glr-auto-examples-model-selection-plot-cv-predict-py\nfig, ax = plt.subplots()\nax.scatter(y_valid, lgb_reg.predict(X_valid), edgecolors=(0, 0, 0))\nax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\nax.set_xlabel('Measured')\nax.set_ylabel('Predicted')\nax.set_title('Single Validation Predictions\\nAnalyze Model Residuels')\nplt.show()","193a44cf":"sns.distplot(y_valid - lgb_reg.predict(X_valid))\nplt.title(\"Distribution of Errors\")\nplt.xlabel(\"Residuals\")\nplt.show()","6cf5e154":"print(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)\/60))","ee92a005":"## Question of Hotness\nNow that I have explored the data, I am prepared to determine my dependent variable. My take on hotness is that it is primarily a reflection of what customers deem desirable. For this reason, many aspects of the revenue variables are disqualified, even though an economic argument could be made that the more money people are willing to spend, the more they must like a given property. This line of thinking may be distorted by the fact that ADR (Average Daily Revenue in given month) has an average of 50, but also a significant right tail that leads into the three hundred levels and over.\n\nThis leaves the availability related variables. While occupancy rate could have high potential, it may also be distorted by apartments with high blocked days and average reservation days scenarios.\n\nThis leaves me between reservation days and number of reservations. This is a tough trade off because on the one hand, reservation days suggests that available and rented apartments will climb to the top of the hotness metric, but it is at risk of scenarios where a single individuals rents an apartment over a long period of time. In this case, the motivations for renting are more likely tied to need for shelter.  <br>\nNumber of reservations, on the other hand, is representative of customers going through the apartment scoping process and selecting a given apartment X times in a given month. This product therefore most often persuades people to buy it.\n\n## Data Aggregation (10%)\n- Aggregate the data by Property ID to create a table of yearly averages of the suitable fields (such as revenue, occupancy etc)\n- Convert the dataset that is in the form of monthly incomes to one which is yearly, and only keep data between 2018 and 2019","d0bed016":"## Toy Tweet Project\n_Nick Brooks, August 2018_","5cfaba2f":"**Additional Information About Categorical Variables:** <br>\nThe rest of the categorical variables Country, Industry, Sector, Service, and Firm Type are equally distributed across their respective classes. Next I want to see what their subclasses are.","b655e477":"**Interpretation:** <br>\nIn general, the trends points to a slight growth acceleration, with classes of diffrent growth levels and growth acceleration patterns.\n\n***\n\n## Q1: Can you identify which service has the most influence on growth in each sector from 2015-2019?\u00b6\nJudging by the Service Growth Average by Year plot above, it appears like Risk is the dominant service across the board, although HR and Technology have seen a rate of accelerated growth from 2017 and onwards, so the trend suggests that they may overtake the impact of Risk in the next couple years.\n\n**Create New Features: Growth Difference Year by Year** <br>\nSnapshots of dataset progression to facilitate explanation.","8eb1842e":"**Correlations:**","d499c99d":"#### Main takeaways: <br>\n- There are two epicenter neighbourhoods\n- There are also some mild satelite areas in the general south west direction.","c742a4c0":"## All Features Model\n\n1. Gradient Boosting - Model chosen because of its ability to efficiently process large\/wide data.","8c17c4a1":"LGBM is able to get a better score of 34 (CV score), which is 40% improvement over regression. It also provides a feature importance plot which helps to double check whether there are un-reasonable features in the mix (leaky).","f4ff3d7b":"#### Load Data\n- Enforce data types\n- Create simple time features","73ee4057":"## Toy Analyst Task\u00b6\n_Nick Brooks, August 2018_\n\n**Assignment: **\n\n1. Can you identify which service has the most influence on growth in each sector from 2015-2019?\n2. Which firm type is having the biggest influence on growth in the top 5 fastest growing countries between 2015-2019?\n3. Can you forecast the current data set out to 2022? Use any methodology and external data sources you think are relevant.\n4. Present these results in whatever way you think is best, eg; dashboard, charts, graphs, tables, etc , <br>\n\nIf you can show your proficiency in R, and Tableau, that would be great but feel free to use any other programme you are comfortable with.","d643f6f5":"## Filter Table\n- I have observed that around 40% of the dataset has non-active houses at a given month. These cases are void of revenue information, and do not contribute additional information. In fact, they provide a risk of misguiding the analysis.\n- The Country, State, City, Currency only have a single factor suiting Manchester. ZipCode is 100% Null. This will be ignored.\n- I will frame this analysis in terms of pounds, because this currency is local and will ignore exchange rates.","0634f5d7":"**Glancing at Sector Growth: Defining Influence in Two Different Ways** <br> \nSince there are 29 Sectors, there will be too many lines to attach to a name. My first step is to visualize all the lines to get a overview.","7c03f9ea":"Randomforest seems to put all of the prediction power into UserFollersCount, furthermore, it experiences massive performance variance accross cross-validation folds.\n\n### **Light Gradient Boosting Model**","58dc2bab":"## Manchester AirBnb Assignment\n_By Nick Brooks, January 2020_\n\n****Column Explanations****:**\n- **Property ID:** Airbnb Property ID\n- **Property Type:** Actual property type of the property being listed on Airbnb (Detached house, Flat etc)\n- **Listing Type:** The listing choice selected between Entire home, Apartment and Private room Bedrooms:** Number of bedrooms\n- **Reporting Month:** The month corresponding to the provided data\n- **Occupancy Rate:** Occupancy rate achieved in the given month\n- **Revenue (USD):** Revenue achieved in the given month (USD)\n- **Revenue (Native):** Revenue achieved in the given month (In currency of booking)\n- **ADR (USD):** Average daily revenue achieved in the given month (USD)\n- **ADR (Native):** Average daily revenue achieved in the given month (In currency of booking)\n- **Number of Reservations:** Number of reservations made for the listing in the given month\n- **Reservation Days:** Number of days listing was booked in the month\n- **Available Days:** Number of days listing was not booked in the month\n- **Blocked Days:** Number of days listing was blocked by the host in the month\n- **Active:** Whether property is active on Airbnb in the month or not\n- **Currency Native:** Currency of the booking being made\n- **Host ID:** Host identifier","da2dffbd":"**Take-Away: ** <br>\nThis heatmap suggests that across the board, Type P has the biggest influence on the top 5 growing countries. However, it is interesting to note that Type A dominates Canada, and Type M dominates Portugal.\n\n\n***\n\n## Q3: Can you forecast the current data set out to 2022? Use any methodology and external data sources you think are relevant.\n**Method:** Row-Wise Linear Models - Regression\n\n**Reasoning: **<br>\nThis is a very simple dataset with a limited number of variables. Furthermore, there is no seasonality, random-walk, or cyclical behavior to be found. Therefore, I believe that a simple linear will suffice to capture the upward trend. I hypothesize that using a polynomial term will improve the model by enabling it to capture the growth rate trend I have noticed. That being said, any significant disturbance in the system, such as a financial crisis, will render these projections obsolete, especially the polynomial projection, which is less robust than the linear prediction.","b0ce4d42":"## Feature Engineering:\n1. Time Features and Account Age\n2. Extract URL Domain\n3. Label Encoding and Removing Redundant Variables.. \n4. Count Vectorizing each Hastag in the TweetHastag feature\n5. Meta Text Features","c0ed2376":"#### Main takeaways:\n- Appears to be a linear growth rate. This finding should be validated against unique properties.\n- Without the time-element, the top factors in categories appear more unequal than the reality. This is because the total difference is the sum of minor difference across (4 years * 12 months) ~48 montly periods","f7296067":"**Understanding the Growth Variable:** <br>\nThese lines are very close together. It appears like as the years increase, the kurtosis decreases and the right tail becomes more extreme. This suggests growth increases over time, exponentially.","79213c09":"### Simple Basline: Random Forest","994cc967":"#### First impressions:\n- Panel Data, monthly time intervals. Between 2014 and 2019, which would very much reflect the dawn and boom of AirBnb.\n- Not much information about the actual housing products. \n- Operational features around revenue and occupancy.\n- GPS may be powerful feature to understand trendy neighbourhoods.\n\nTLDR - Fairly simple dataset with a decent number of observations (~200k)\n\n## Data Cleaning and Exploration (40%)\n- Clean the dataset of unwanted data and outliers\n- Deal with missing data values\n\n### Build Features:\n- How long Host \/ Property has been in action\n- Buckets for how enterpricing the Host is. (Owned properties)\n- Distance from Manchester City Center\n- Time features","f29ecb2e":"## Some Visualizations\n\n1. Dependent Variable Distribution\n2. Correlations","ad797ebb":"#### Main takeaways: <br>\n- Over the last 4 years, there has been a minor increase in the average number of properties held by a host.\n- 2019 suggests a dip because the year is not complete yet.","5c674789":"**Growth and Categorical Variables:** <br>\nI am curious to see how growth interacts with variables such as Region, Industry, Service, and Firm Type.","56944e95":"**Glancing at the Growth Rates** <br>\nBut first, I must stack the data so that the growth variable has one column and the year variable has its own column.","3304f366":"## Modeling without Text\n\n1. Random Forest\n2. Gradient Boosting","3994c8a6":"## Pre-Processing..\n1. Missing Values\n2. Remove Retweets","bd2553ce":"## Modelling (10%)\n- Use these features plus the ones available in the data already and do some basic analysis and construct a Machine Learning model of your choice (whatever you think is suitable) for generating the prediction\n- Evaluate the model using appropriate evaluation metrics and explain your choice","94ee4438":"### Quick Preliminary Data Exploration\nThe first thing I want to do is explore the distribution for my features. This will enable me to understand the consistency of the dataset.\n\nGoing forward, I am assuming that the yearly values represent the yearly growth in millions of dollars.","652d9e59":"**Reflection on Polynomial Solution: ** <br>\nFrom the sample case visualizations, the polynomial solution seems to capture the acceleration aspect of the trend, but I am concerned that it is extrapolating too aggresively. Perhaps a more ideal solution would take the average of the linear and polynomial solution in order to restrain the extrapolation, creating a more robust and conservative prediction.\n\n**Roll out Polynomial predictions to entire dataset..**","3b25190b":"Good to see low fluctuations across folds, however, looking at the prediction output, linear regression is unable to higher values of my dependent variable.","d4450f16":"## Feature Engineering and Exploration (40%)\n- Normalise the data and carry out relevant transformations such as Integer encoding or One-Hot encoding if necessary\n- Engineer basic features (apart from the ones available) for predicting which properties are considered \u2018hot\u2019 (interpret this in your own way)","0ac640e4":"#### Main takeaways: <br>\n- Using Spearman rank for more robust (non-parametric) correlation test.\n- The revenue and availability features are highly correlated (Share data-generation process)\n- Days columns are highly correlated (Share data-generation process).\n- I'm surprised to see that the days property has been on the platform does not correlate with more occupancy. I would of thought that there would be a feedback loop where successful AirBnBs are less likely to drop off, and that successful AirBnbs get more comments and attention, hence more rents (network effect)","8a35e8e4":"I just looks like there was an expensive Condominium that was rented out for 20 days at 7.8K pounds a day!","acdac152":"Tremendous numbers coming out of Ethiopia. Since it grew 6 times faster than the runner up Canada, I am a little bit suspicious of the reliability of this source. Ideally I would follow up with data's source to ensure there is no mistake.","8d2e80e9":"\n\n***\n\n## Text Processing:\n\n1. Process Tweet Body, User Description with TF-IDF and user location with CountVectorizer.\n    - Character grams chosen to capture different languages and twitter slang lingo","f30fe763":"**How to Interpret: ** <br>\nThe color reflects the weight each service has on the sector. The weight is computed by averaging the growth of the sector by service, then normalizing these value by the sum of sectors growth. Each row sums to 1.\n\n**Interpretation: ** <br>\nThe general takeaway is that average growth suggests Risk and HR\/Change are the major services, while growth rate is coming predominantely out of Technology while Risk plays a much smaller role.\n\n***\n\nNext I want to showcase my intial attempt at this viz and explain why it failed to simplify this large amount of information..","ba7676ec":"#### Main takeaways\n- House type is dominated by single bedroom Apartments or Houses.","d068c003":"**Red Flag - TweetFavoriteCount may be Leaky**\n\nReflection: Tweet Favorite Count is similar to Tweet Retweet Count in the sense that they represent the response of the twitter community after the tweet is posted.  My understanding of this problem is to predict the retweet potential of the tweet characteristics in itself. Therefore, I shall **remove the TweetFavoritesCount** variable. Note that if this were more of a inferential project, I would keep the variable and measure the effect of the causation and feedback loop between these variables.","d8fcdc23":"***","9151c887":"**Interpretation: ** <br>\nThe plot above serves to understand the general trend, as well as to spot potential outliers. An interesting finding here is that growth rates differ between sectors. Furthermore, there appears to be some sectors that experience low growth for a few years before sky rocketing. This is an important consideration when ranking sectors because some metrics may not effectively capture this.\n\n**Action:** <br>\nTherefore, I will create a new feature which tracks growth change over time periods in order to the capture up and coming aspects of the data [Plot 2]. To capture general influence over the years, I will use the arithemtic mean [Plot 1].\n\n**Result: ** <br>\nAs I suspected, when ranking by average growth difference, different sectors appear at the very top, and there is a even more drastic difference between sectors.\n\n**Heatmap to Weigh Service Influence: **<br>\nHeatmaps are my go-to visualization for variables with 10+ categories.","be5f1293":"## Q2: Which firm type is having the biggest influence on growth in the top 5 fastest growing countries between 2015-2019?","73d43252":"#### Quick EDA","b4e90d3e":"**Criticism: ** <br>\nWhile this viz attemps to communicate the same data as the heatmap above, the positioning on the bars make it difficult to compare influence between sectors and between metrics.\n\n**Deeper Dive into Service Influence on Sector over Time: ** <br>\nHere is a resource to dive deeper into the over-time influence of services on each sector.","902a086c":"#### Main takeaways: <br>\n- There appears to be some seasonal patterns to the occupancy features. The pattern is an annual cycle of slow buildup until a peak in the fall, followed by a stabalizing drop.\n- Should followup to see whether there is an annual trend to observe.\n- There is a trend of aparments being available for a greater number of days over the years.","9905b2c3":"#### Main takeaways: <br>\n- Host have been on the platform for an average of 2 years.\n- A property is available for 15 days on average a month, but also has a very wide variance.\n- A property is blocked for around 3 days on average a month.\n- Occupancy rate is around 35% on average\n- The revenue columns seem to have a outlier, perhaps a handful of mega-profitable houses. These should be double checked..","9be1a59f":"#### Main takeaways: <br>\n- Minor increase in occupancy rate over the years.\n- Annual cycle is lowest at .35 during new years, and .55 in september."}}