{"cell_type":{"f2c96994":"code","4dd9c211":"code","7b305afb":"code","656734a9":"code","17a7961d":"code","98e216ff":"code","5ea8fed8":"code","cd19cf76":"code","7608b3e7":"code","60443cf2":"code","00dbf88b":"code","6ce3aa20":"code","920bcf0c":"code","0033f980":"code","beeb5835":"code","d73c3d46":"code","7b10ea73":"markdown","792465e6":"markdown","4be3e9f8":"markdown","5386f1a7":"markdown","4223aa1f":"markdown","90ba3546":"markdown","e5854ed9":"markdown","8f6168d7":"markdown"},"source":{"f2c96994":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4dd9c211":"data = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')\ndata.head()","7b305afb":"# shape of data\ndata.shape","656734a9":"data.describe()","17a7961d":"data.info()","98e216ff":"data.isnull().any()","5ea8fed8":"data.target.unique","cd19cf76":"from collections import Counter\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport pandas as pd\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D","7608b3e7":"sns.set(style = \"darkgrid\")\n\nfig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection = '3d')\n\nx = data['age']\ny = data['sex']\nz = data['trestbps']\n\nax.set_xlabel(\"age\")\nax.set_ylabel(\"sex\")\nax.set_zlabel(\"trestbps\")\n\nax.scatter(x, y, z)\n\nplt.show()","60443cf2":"def entropy(y):\n    hist = np.bincount(y)\n    ps = hist\/len(y)\n    return - np.sum([p * np.log2(p) for p in ps if p > 0])","00dbf88b":"class Node:\n    \n    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n        self.feature = feature\n        self.threshold = threshold\n        self.left = left\n        self.right = right\n        self.value = value\n    \n    def is_leaf_node(self):\n        return self.value is not None","6ce3aa20":"class Decission_Tree:\n    def __init__(self, min_samples_split=2, max_depth=100, n_feats=None):\n        self.min_samples_split = min_samples_split\n        self.max_depth = max_depth\n        self.n_feats = n_feats\n        self.root = None\n    \n    def predict(self, X):\n        X = X.to_numpy()\n        return np.array([self.traverse_tree(x, self.root) for x in X])\n    \n    def fit(self, X, y):\n        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n        X = X.to_numpy()\n        y = y.to_numpy()\n        self.root = self.grow_tree(X, y)\n    \n    def grow_tree(self, X, y, depth=0):\n        n_samples, n_features = X.shape\n        n_labels = len(np.unique(y))\n        \n        # stopping criteria\n        if (depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split):\n            leaf_value = self.most_common_label(y)\n            return Node(value=leaf_value)\n        \n        # array of random columns in Dataset\n        feats_idxs = np.random.choice(n_features, self.n_feats, replace=False)\n                \n        best_feat, best_tresh = self.best_criteria(X, y, feats_idxs)\n        \n        left_idx, right_idx = self.split(X[:, best_feat], best_tresh)\n        left = self.grow_tree(X[left_idx, :], y[left_idx], depth+1)\n        right = self.grow_tree(X[right_idx, :], y[right_idx], depth+1)\n        return Node(best_feat, best_tresh, left, right)\n    \n    def best_criteria(self, X, y, feats_idxs):\n        best_gain = -1\n        split_idx, split_tresh = None, None\n        \n        for feats_idx in feats_idxs:\n            X_col = X[:, feats_idx]\n            thresholds = np.unique(X_col)\n            for threshold in thresholds:\n                gain = self.information_gain(y, X_col, threshold)\n                if gain > best_gain:\n                    best_gain = gain\n                    split_idx = feats_idx\n                    split_tresh = threshold\n            \n        return split_idx, split_tresh\n    \n    def information_gain(self, y, X_col, thresh):\n        parent_entropy = entropy(y)\n        \n        left, right = self.split(X_col, thresh)\n\n        if len(left) == 0 or len(right) == 0:\n            return 0\n        \n        n = len(y)\n        n_l, n_r = len(left), len(right)\n        e_l, e_r = entropy(y[left]), entropy(y[right])\n        \n        child_entropy = (n_l \/ n) * e_l + (n_r \/ n) * e_r\n        \n        ig = parent_entropy - child_entropy\n        return ig\n    \n    def split(self, X_col, split_tresh):\n        \n        left_idxs = np.argwhere(X_col <= split_tresh).flatten()\n        right_idxs = np.argwhere(X_col > split_tresh).flatten()\n        \n        return left_idxs, right_idxs\n    \n    def most_common_label(self, y):\n        counter = Counter(y)\n        most_common = counter.most_common(1)[0][0]\n        return most_common\n    \n    def traverse_tree(self, x, node):\n        if node.is_leaf_node():\n            return node.value\n        \n        if x[node.feature] <= node.threshold:\n            return self.traverse_tree(x, node.left)\n        \n        return self.traverse_tree(x, node.right)","920bcf0c":"def accuracy(y_true, y_pred):\n    acc = np.sum(y_true == y_pred)\/len(y_true)\n    return acc","0033f980":"if __name__ == '__main__':\n    X = data.drop(['target', 'oldpeak'], axis=1)\n    y = data.target\n    \n    X_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n    \n    DT = Decission_Tree(max_depth=7)\n    DT.fit(X_train, y_train)\n    \n    y_pred = DT.predict(x_test)","beeb5835":"m = {'y_test' : y_test.to_numpy(), 'y_pred' : y_pred}\n\nframe = pd.DataFrame(m) \n\nframe.head()","d73c3d46":"acc = accuracy(y_test, y_pred)\n    \nprint('accuracy : ', acc)","7b10ea73":"### Create Node ","792465e6":"### Calculate Accuracy","4be3e9f8":"### Read Data from Dataset","5386f1a7":"### Main Function","4223aa1f":"### import all important libraries ","90ba3546":"### 3D graph of target values","e5854ed9":"### Decission Tree","8f6168d7":"### Calculate Entropy"}}