{"cell_type":{"59c2e0c4":"code","bc104222":"code","dd696f28":"code","dabc4f9c":"code","8ca096e9":"code","125ea40c":"code","c93300ea":"code","8c0c0420":"code","5b452e2e":"code","83d594fc":"markdown","f8fdece1":"markdown","43b2dd69":"markdown","60f3116b":"markdown","c4fd053e":"markdown","327989a7":"markdown","a45f592c":"markdown","07ec8771":"markdown","9da66559":"markdown"},"source":{"59c2e0c4":"!pip install pyclustering","bc104222":"import pandas as pd # dataframe manipulation\nimport numpy as np # linear algebra\n\n# data visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom yellowbrick.cluster import KElbowVisualizer # cluster visualizer\n\n# sklearn kmeans\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics.cluster import contingency_matrix\n\n# pyclustering kmeans\nfrom pyclustering.cluster.kmeans import kmeans\nfrom pyclustering.utils.metric import distance_metric\nfrom pyclustering.cluster.center_initializer import random_center_initializer\nfrom pyclustering.cluster.encoder import type_encoding\nfrom pyclustering.cluster.encoder import cluster_encoder","dd696f28":"iris = pd.read_csv('..\/input\/iris\/Iris.csv')\niris.head()","dabc4f9c":"X = iris.iloc[:, 1:5].values # feature matrix\ny = iris.iloc[:, -1].values # class matrix","8ca096e9":"# Instantiate the clustering model and visualizer\nmodel = KMeans()\nvisualizer = KElbowVisualizer(model, k=(1, 11))\n\nvisualizer.fit(X) # Fit the data to the visualizer\nvisualizer.show() # Finalize and render the figure\nplt.show()","125ea40c":"# instatiate KMeans class and set the number of clusters\nkm_model = KMeans(n_clusters=3, random_state=10)\n\n# call fit method with data \nkm = km_model.fit_predict(X)\n\n# coordinates of cluster center\ncentroids = km_model.cluster_centers_ \n\n# cluster label for each data point\nlabels = km_model.labels_ ","c93300ea":"plt.figure(figsize=(8,6))\nplt.scatter(\n    X[km == 0, 0], X[km == 0, 1],\n    s=25, c='lightgreen',\n    marker='s', edgecolor='black',\n    label='cluster 1'\n)\n\nplt.scatter(\n    X[km == 1, 0], X[km == 1, 1],\n    s=25, c='yellow',\n    marker='o', edgecolor='black',\n    label='cluster 2'\n)\n\nplt.scatter(\n    X[km == 2, 0], X[km == 2, 1],\n    s=25, c='lightblue',\n    marker='v', edgecolor='black',\n    label='cluster 3'\n)\n\n# visualise centroids\nplt.scatter(\n    centroids[:, 0], centroids[:, 1],\n    s=300, marker='*',\n    c='red', edgecolor='black',\n    label='centroids'\n)\nplt.legend(scatterpoints=1)\nplt.grid()\nplt.show()","8c0c0420":"def purity_score(y_true, y_pred):\n    # compute contingency matrix (also called confusion matrix)\n    confusion_matrix = contingency_matrix(y_true, y_pred)\n    # return purity\n    return np.sum(np.amax(confusion_matrix, axis=0)) \/ np.sum(confusion_matrix)\n\n# Report Purity Score\npurity = purity_score(y, labels)\nprint(f\"The purity score is {round(purity*100, 2)}%\")","5b452e2e":"# define dictionary for distance measures\ndistance_measures = {'euclidean': 0, 'squared euclidean': 1, 'manhattan': 2, 'chebyshev': 3, \n                    'canberra': 5, 'chi-square': 6}\n\n# function defined to compute purity score using pyclustering for various distance measures\ndef pyPurity(dist_measure):\n    initial_centers = random_center_initializer(X, 3, random_state=5).initialize()\n    # instance created for respective distance metric\n    instanceKm = kmeans(X, initial_centers=initial_centers, metric=distance_metric(dist_measure))\n    # perform cluster analysis\n    instanceKm.process()\n    # cluster analysis results - clusters and centers\n    pyClusters = instanceKm.get_clusters()\n    pyCenters = instanceKm.get_centers()\n    # enumerate encoding type to index labeling to get labels\n    pyEncoding = instanceKm.get_cluster_encoding()\n    pyEncoder = cluster_encoder(pyEncoding, pyClusters, X)\n    pyLabels = pyEncoder.set_encoding(0).get_clusters()\n    # function purity score is defined in previous section\n    return purity_score(y, pyLabels)\n\n# print results\nfor measure, value in distance_measures.items():\n    print(f\"The purity score for {measure} distance is {round(pyPurity(value)*100, 2)}%\")","83d594fc":"### 5. Report Purity Score\nHere, `Purity score` is computed using Ground Truth or External accessment. Since, the dataset \nconsists of a true class label, the accuracy of the cluster label computed using KMeans is compared \nagainst the true label class. ","f8fdece1":"### 2. Seperate feature and class matrix ","43b2dd69":"### Essential Libraries\n`Note` Make sure internet is enabled for the kernal while installing `pyclustering`.","60f3116b":"### 6. Kmeans using `pyclustering` for different Distance Metrics\n- Here, K-Means is performed using `pyclustering` library for various distance metrics like Manhattan, Chebyshev, euclidean etc. \n- Minkowski distance is just the generalisation of euclidean (p=2), manhattan (p=1) and chebyshev distance (p=Inf). \n- Although for pyclustering, initial centers can be initialised using k++ algorithm, but, I have randomly initialised the initial center to compare the results with sklearn kmeans. \n- Highest purity is achieved using `Chi-Square` as the distance metric.","c4fd053e":"### 4. Visualise Clusters","327989a7":"### 3. Determine Optimal number of clusters using Elbow method\nThe optimal number of clusters (k) can be determined using the elbow method. As number of clusters increase, the sum of squares error (SSE) within the cluster decreases because the data points gets closer to their respective cluster center. With elbow method the objective is to find k where SSE decreases most rapidly. Therefore, from figure it can be seen that the optimal number of clusters is 3. \nThis is same as the actual number of classes in the dataset.","a45f592c":"### INTRODUCTION\n- Clustering is the task of identifying similar instances and assigning them to clusters. Distance metric plays a cruicial role in identifying these similar data points and forming respective clusters. K-Means uses euclidean distance, as the default distance metric, for clustering. Therefore, it is importmant to play around with different distance measures for any dataset. \n- Here, I have used library `pyclustering` to implement kmeans clustering on the legendary `iris` dataset using euclidean, squared euclidean, manhattan, chebyshev, canberra and chi-square distances. The performance of clustering with these different distance measures is evaluated using a purity score (ground assesment for clustering). ","07ec8771":"### 1. Read CSV","9da66559":"### 3. Perform K-Means clustering using sklearn\nAs we can see KMeans in sklearn does not have a option to change the distance metric and by default uses euclidean distance. Lets first check the performance using euclidean and then compare with other distance in section 6."}}