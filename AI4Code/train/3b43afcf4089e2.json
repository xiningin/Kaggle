{"cell_type":{"2089a00f":"code","7161dd49":"code","9e8bd824":"code","b169ad66":"code","44576223":"code","13572d21":"code","9e72e2fe":"code","1e14d779":"code","9bc56d84":"code","efb47721":"code","bdf71598":"code","ab593692":"code","5f0bc882":"code","7b9d591b":"code","1d5e7fb7":"markdown","6cb77ac6":"markdown","f94a3709":"markdown","b08135d7":"markdown"},"source":{"2089a00f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom functools import reduce\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","7161dd49":"M1 = pd.read_csv('..\/input\/diversity\/LGBM.798.csv')\nM2 = pd.read_csv('..\/input\/ingredients\/WEIGHT_AVERAGE_RANK2.csv')\nM3 = pd.read_csv('..\/input\/neural\/sub_nn.csv')\nM4 = pd.read_csv('..\/input\/genetic\/pure_submission.csv')\nM5 = pd.read_csv('..\/input\/diversity\/xgb.796.csv')","9e8bd824":"# Function for merging dataframes efficiently \ndef merge_dataframes(dfs, merge_keys):\n    dfs_merged = reduce(lambda left,right: pd.merge(left, right, on=merge_keys), dfs)\n    return dfs_merged","b169ad66":"dfs = [M1,M2,M3,M4,M5]\nmerge_keys=['SK_ID_CURR']\ndf = merge_dataframes(dfs, merge_keys=merge_keys)","44576223":"df.columns = ['SK_ID_CURR','T1','T2','T3','T4','T5']\ndf.head()","13572d21":"pred_prob = 0.5 * df['T2'] + 0.5 * df['T1']\npred_prob.head()","9e72e2fe":"sub = pd.DataFrame()\nsub['SK_ID_CURR'] = df['SK_ID_CURR']\nsub['target']= pred_prob","1e14d779":"sub.to_csv('ldit.csv', index=False)","9bc56d84":"B_prob = 0.6 * df['T1'] + 0.2 * df['T3'] + 0.2 * df['T4']","efb47721":"B_prob.head()","bdf71598":"SUB = pd.DataFrame()\nSUB['SK_ID_CURR'] = df['SK_ID_CURR']\nSUB['TARGET'] = B_prob\nSUB.to_csv('Blendss.csv', index=False)","ab593692":"df_c = df.copy()\ndf_c = df.drop(['SK_ID_CURR'],axis=1)\nCorr_Mat = df_c.corr()\nprint(Corr_Mat) # Correlation matrix of five submission files\nsns.heatmap(Corr_Mat)","5f0bc882":"corr_pred = 0.6 * df['T2'] + 0.05 * df['T3'] + 0.05 * df['T4'] + 0.1 * df['T5'] + 0.2 * df['T1']\ncorr_pred.head()","7b9d591b":"SuB = pd.DataFrame()\nSuB['SK_ID_CURR'] = df['SK_ID_CURR']\nSuB['TARGET'] = corr_pred\nSuB.to_csv('corr_blend.csv', index=False)","1d5e7fb7":"## Blending lowest correlated models","6cb77ac6":"## Blend with one rank weighted submission [0.8 LB]","f94a3709":"## Diversified blend [0.799 LB]\n\n\n**The blending ingredients are taken from three different type of models.**","b08135d7":"**Thank you everyone for showing your appreciation and support. It's my first gold medal in kernels and I hope to publish far better kernels than this in near future.**"}}