{"cell_type":{"614b36bb":"code","bca9b8d5":"code","f3a3a702":"code","2e666cc8":"code","b10639ba":"code","9f6ff600":"code","ee40e86a":"code","8a14442c":"code","000d9297":"code","896872c6":"code","8cba6b72":"code","c79428f5":"code","9246700a":"code","32108134":"code","4ef2396c":"code","d1e58996":"code","75bcea1b":"code","e455ca8d":"code","38027ca0":"code","ea50ad20":"code","41378919":"code","3eaa56ac":"code","f46cef27":"code","87dc257d":"markdown","e85b9552":"markdown","18e396cb":"markdown","f3fb0a5b":"markdown","b7570e4a":"markdown"},"source":{"614b36bb":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory","bca9b8d5":"img_height, img_width = IMG_SIZE = (200, 200)\nbatch_size = 32\n\ndata_dir = \"\/kaggle\/input\/plastic-recycling-codes\/seven_plastics\/\"","f3a3a702":"train_ds = image_dataset_from_directory(\n  data_dir,\n  validation_split=0.1,\n  subset=\"training\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","2e666cc8":"val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.1,\n  subset=\"validation\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","b10639ba":"# List the classes\n\nclass_names = train_ds.class_names\nprint(class_names)","9f6ff600":"# Get a look at what we are training on\n\nimage_batch, label_batch = next(iter(train_ds))\n\nplt.figure(figsize=(10, 10))\nfor i in range(9):\n  ax = plt.subplot(3, 3, i + 1)\n  plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n  label = label_batch[i]\n  plt.title(class_names[label])\n  plt.axis(\"off\")","ee40e86a":"# Prefetch images to memory, to make training faster\n\nAUTOTUNE = tf.data.AUTOTUNE\n\ntrain_ds = train_ds.prefetch(buffer_size=AUTOTUNE)","8a14442c":"data_augmentation = tf.keras.Sequential([\n  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n  tf.keras.layers.experimental.preprocessing.RandomContrast(0.2),\n  tf.keras.layers.experimental.preprocessing.RandomZoom(0.2),\n])","000d9297":"preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input","896872c6":"# Create the base model from the pre-trained model MobileNet V2\nIMG_SHAPE = IMG_SIZE + (3,)\nbase_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')","8cba6b72":"image_batch, label_batch = next(iter(train_ds))\nfeature_batch = base_model(image_batch)\nprint(feature_batch.shape)","c79428f5":"# Keep the pre-trained weights, set this to true to retrain the whole model.\nbase_model.trainable = False","9246700a":"global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\nfeature_batch_average = global_average_layer(feature_batch)\nprint(feature_batch_average.shape)","32108134":"num_classes = len(class_names)\n\nprediction_layer = tf.keras.layers.Dense(num_classes)\nprediction_batch = prediction_layer(feature_batch_average)\nprint(prediction_batch.shape)","4ef2396c":"inputs = tf.keras.Input(shape=(img_width, img_height, 3))\nx = data_augmentation(inputs)\nx = preprocess_input(x)\nx = base_model(x, training=False)\nx = global_average_layer(x)\nx = tf.keras.layers.Dropout(0.2)(x)\noutputs = prediction_layer(x)\nmodel = tf.keras.Model(inputs, outputs)","d1e58996":"base_learning_rate = 0.0001\nmodel.compile(optimizer=tf.keras.optimizers.Adam(), # lr=base_learning_rate),\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","75bcea1b":"model.summary()","e455ca8d":"initial_epochs = 300\n\nloss0, accuracy0 = model.evaluate(val_ds)","38027ca0":"print(\"initial loss: {:.2f}\".format(loss0))\nprint(\"initial accuracy: {:.2f}\".format(accuracy0))","ea50ad20":"history = model.fit(train_ds,\n                    epochs=initial_epochs,\n                    validation_data=val_ds)","41378919":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([0,1.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","3eaa56ac":"export_path = \"\/kaggle\/working\/resin_id_saved_model\"\nmodel.save(export_path)\n\nexport_path","f46cef27":"# Understand our Input data files. For example, \n# how many files are there, what are the resolutions\n\nimport os\nfrom collections import Counter\nimport PIL\nimport PIL.Image\n\nfor dirname, _, filenames in os.walk(data_dir):\n    c = Counter()\n    \n    for filename in filenames:\n        with PIL.Image.open(str(os.path.join(dirname, filename))) as im:\n            c.update([im.size])\n    \n    print(os.path.basename(dirname))\n    \n    for key, value in c.items():\n        print(key, value)\n\n    print()\n    \n\n# \/kaggle\/input\/plastic-recycling-codes\/seven_plastics\/6_polystyrene_PS\/MNIV8565.jpg\n# \/kaggle\/input\/plastic-recycling-codes\/seven_plastics\/6_polystyrene_PS\/IMG_E6426.jpg\n\n","87dc257d":"# Load images","e85b9552":"## Build our Model","18e396cb":"## Load MobileNet\nWe are doing transfer learning from the pretrained MobileNet model. With transfer learning we use features from the n-1 layer of the base model. We chop off the last layer and add our own Dense layer to do our predictions.\n\nWe can also explore using a different base model. Like `tf.keras.applications.ResNet50`","f3fb0a5b":"In this notebook I use transfer learning to train a ModelNet to classify the resin identification codes.\n\nA pre-trained model is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task. You either use the pretrained model as is or use transfer learning to customize this model to a given task.\n\nReference [transfer learning guide](https:\/\/colab.research.google.com\/github\/tensorflow\/docs\/blob\/master\/site\/en\/tutorials\/images\/transfer_learning.ipynb#scrollTo=hRTa3Ee15WsJ)","b7570e4a":"## Data Augmentation\n\nUsing Tensorflow to apply random transformations to the given data. We do this because with the limited data that we had we only get a ~65% accuracy. With this approach we can generate more data than what we are given."}}