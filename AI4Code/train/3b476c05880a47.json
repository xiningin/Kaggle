{"cell_type":{"2ccb52e4":"code","6a48ec6f":"code","39f91ff9":"code","b350fae9":"code","d347fda7":"code","95b5e83e":"code","b7fa512b":"code","5c65fb7c":"code","c9fa2f88":"code","227597af":"code","b5377e43":"code","c3e3561f":"code","f1cda6f1":"code","ac83eaf6":"code","17123a22":"code","3ebf631e":"code","aaae73be":"code","16d08915":"code","6591becb":"code","488bde4e":"code","09463708":"code","c517b275":"code","606155ff":"code","5f19e894":"code","6c9ad7d5":"code","bce2de38":"code","ccd7c542":"code","8cb6b03e":"code","2d4117e9":"code","bcfc06a3":"code","6c2f8657":"code","8ffb84f1":"code","5b1e2134":"code","629ef74d":"code","6a5f78d9":"code","1c154975":"code","31e920bd":"code","fc7bd003":"code","31bb31a9":"markdown","31a66e68":"markdown","9fb19dfe":"markdown","b251634f":"markdown","ea2ea02f":"markdown","f47147ce":"markdown","8987226a":"markdown","10faaa56":"markdown","36c9248c":"markdown","7e3d5653":"markdown","87823284":"markdown","10690b7d":"markdown","04848bb4":"markdown","2715ed90":"markdown","cfca30d5":"markdown","80694747":"markdown","fc72c775":"markdown","d29427f7":"markdown","c93516e4":"markdown","24d8c080":"markdown","7fc0bb6e":"markdown","e8398163":"markdown","d3e6071a":"markdown","99819816":"markdown","24bd8dd1":"markdown","0207c1e0":"markdown","7fe68173":"markdown","df6b8f77":"markdown","8337d20f":"markdown","f8c0d2c7":"markdown","d6b68a03":"markdown","8e6505b9":"markdown","3ed1780c":"markdown"},"source":{"2ccb52e4":"import pandas as pd","6a48ec6f":"sms = pd.read_table(\"..\/input\/sms-data-labelled-spam-and-non-spam\/SMSSpamCollection\", header=None)","39f91ff9":"print(sms)","b350fae9":"sms.head()","d347fda7":"sms.describe()","95b5e83e":"y = sms[0]\ny.value_counts()","b7fa512b":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ny_enc = le.fit_transform(y)","5c65fb7c":"y_enc","c9fa2f88":"raw_text = sms[1]\nraw_text","227597af":"pd.isnull(sms)","b5377e43":"import matplotlib.pyplot as plt\nimport seaborn as sns","c3e3561f":"sms.columns=['label', 'msg']","f1cda6f1":"\nsms.head()","ac83eaf6":"\nsms[\"length\"] = sms[\"msg\"].apply(len)","17123a22":"sms.head()","3ebf631e":"sns.distplot(sms[\"length\"], kde=False);","aaae73be":"!pip3 install contractions\nimport contractions","16d08915":"sms['no_contract'] = sms['msg'].apply(lambda x: [contractions.fix(word) for word in x.split()])","6591becb":"sms.head()","488bde4e":"sms[\"msg_str\"] = [' '.join(map(str, l)) for l in sms['no_contract']]","09463708":"sms.head()","c517b275":"import nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize","606155ff":"text = \"Hi, I would like to tokenize this sentence.\"\nprint(word_tokenize(text))","5f19e894":"sms['tokenized'] = sms['msg_str'].apply(word_tokenize)\nsms.head()","6c9ad7d5":"sms.sample(frac=0.05)","bce2de38":"sms['lower'] = sms['tokenized'].apply(lambda x: [word.lower() for word in x])\nsms.head()","ccd7c542":"import string\npunc = string.punctuation\nsms['no_punc'] = sms['lower'].apply(lambda x: [word for word in x if word not in punc])\nsms.head()","8cb6b03e":"!pip3 install pyspellchecker","2d4117e9":"from spellchecker import SpellChecker\n\nspell = SpellChecker()\n\n# find those words that may be misspelled\nmisspelled = spell.unknown(['something', 'is', 'hapenning', 'here'])\n\nfor word in misspelled:\n    # Get the one `most likely` answer\n    print(spell.correction(word))\n\n    # Get a list of `likely` options\n    print(spell.candidates(word))","bcfc06a3":"nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))","6c2f8657":"sms['stopwords_removed'] = sms['no_punc'].apply(lambda x: [word for word in x if word not in stop_words])\nsms.head()","8ffb84f1":"nltk.download('averaged_perceptron_tagger')","5b1e2134":"sms['pos_tags'] = sms['stopwords_removed'].apply(nltk.tag.pos_tag)\nsms.head()","629ef74d":"nltk.download('wordnet')\nfrom nltk.corpus import wordnet","6a5f78d9":"def get_wordnet_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN","1c154975":"sms['wordnet_pos'] = sms['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\nsms.head()","31e920bd":"from nltk.stem import WordNetLemmatizer","fc7bd003":"wnl = WordNetLemmatizer()\nsms['lemmatized'] = sms['wordnet_pos'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\nsms.head()","31bb31a9":"## Step 4: Spell Checking\n\nFor spell checking, we will use Microsoft's TextBlob, which is a simple spelling correction mechanism","31a66e68":"# 1. Description of Dataset","9fb19dfe":"![](https:\/\/media.tenor.com\/images\/ac89ce16c5c20c35c97eeafafecc426b\/tenor.gif)","b251634f":"We are going to add a new column \u201cno_stopwords\u201d which will remove the stopwords from the \u201cno_punc\u201d column since it has been tokenized, had been converted to lowercase and punctuation was removed. Once again a for-loop within a lambda function will iterate over the tokens in \u201cno_punc\u201d and only return the tokens which do not exist in our \u201cstop_words\u201d variable.","ea2ea02f":"Instead of taking the easy way out with stemming, let\u2019s apply lemmatization to our data but it requires some additional steps compared to stemming. First, we have to apply parts of speech tags, in other words, determine the part of speech (ie. noun, verb, adverb, etc.) for each word","f47147ce":"## Step 2: Tokenization\n\nWe will begin by breaking apart the corpus into a vocabulary of unique terms, and this is called tokanization.\n\nWe can tokenize individual terms and generate what's called a bag of words model. You may notice this model has a glaring pitfall: it fails to capture the innate structure of human language. We can also tokenize using nltk, which is the leading platform for building Python programs to work with human language data","8987226a":"The idea of stemming is to reduce different forms of word usage into its root word. For example, \u201cdrive\u201d, \u201cdrove\u201d, \u201cdriving\u201d, \u201cdriven\u201d, \u201cdriver\u201d are derivatives of the word \u201cdrive\u201d and very often researchers want to remove this variability from their corpus. Compared to lemmatization, stemming is certainly the less complicated method but it often does not produce a dictionary-specific morphological root of the word. In other words, stemming the word \u201cpies\u201d will often produce a root of \u201cpi\u201d whereas lemmatization will find the morphological root of \u201cpie\u201d.","10faaa56":"We are going to be using NLTK\u2019s word lemmatizer which needs the parts of speech tags to be converted to wordnet\u2019s format. We\u2019ll write a function which make the proper conversion and then use the function within a list comprehension to apply the conversion","36c9248c":"## 1.2. Statistics\n\n\nThere is one collection:\n\n- The SMS Spam Collection v.1 (text file: smsspamcollection) has a total of 4,827 SMS legitimate messages (86.6%) and a total of 747 (13.4%) spam messages.\n\n\n## 1.3. Format\n\n\nThe files contain one message per line. Each line is composed by two columns: one with label (ham or spam) and other with the raw text. Here are some examples:\n\nham   What you doing?how are you?\n\nham   Ok lar... Joking wif u oni...\n\nham   dun say so early hor... U c already then say...\n\nham   MY NO. IN LUTON 0125698789 RING ME IF UR AROUND! H*\n\nNote: messages are not chronologically sorted.","7e3d5653":"![](https:\/\/media.tenor.com\/images\/ac89ce16c5c20c35c97eeafafecc426b\/tenor.gif)","87823284":"![](https:\/\/i.pinimg.com\/originals\/9d\/70\/21\/9d70219a6d25fa56d771af6a393a26b7.gif)","10690b7d":"##  If You Found This Helpful, Do Upvote Please. Good Luck !","04848bb4":"# 2. Code implementations for below topics:\n\n### 1.EDA (Exploratory Data Analysis)\n\n### 2.Visualization\n\n###  3.Pre-Processing\n\n- Step 1: Contraction Mapping \/ Expanding Contractions\n- Step 2: Tokenization\n- Step 3: Noise Cleaning - spacing, special characters, lowercasing\n- Step 4: Spell Checking\n- Step 5: \u2018Stop Words\u2019 Identification\n- Step 6: Stemming\/Lemmatization","2715ed90":"Next, we'll remove all punctuation since they serve little value once we begin to analyze our data. Continuing the previous pattern, we will create a new column which has the punctuation removed. We will again utilize a for-loop within a lambda function to iterate over the tokens but this time using an IF condition to only output alpha characters","cfca30d5":"![](https:\/\/media.tenor.com\/images\/ac89ce16c5c20c35c97eeafafecc426b\/tenor.gif)","80694747":"Also, we would want the expanded contractions to be tokenized separately, therefore we convert the lists under the \"no_contract\" column back into strings","fc72c775":"Now, we can apply the tokenizer to our dataset. We will apply NLTK.word_tokenize() function to the \u201cmsg_str\u201d column and create a new column named \u201ctokenized","d29427f7":"![](https:\/\/reflektion.com\/wp-content\/uploads\/2017\/12\/practical-ai-and-voice-commerce-and-natural-language-processing.gif)","c93516e4":"We will first demonstrate a simple example how we implement this spellchecker and how it is able to not only identify misspelled words, but also suggest the most likely corrected spelling along with other likely options","24d8c080":"## Visualizations","7fc0bb6e":"## Step 3: Noise Cleaning - spacing, special characters, lowercasing","e8398163":"## Step 6: Stemming\/Lemmatization","d3e6071a":"## Pre-Processing\nThere are many feature engineering strategies for transforming text data into features. Some involve assigning each unique word-like term to a feature and counting the number of occurrences per training example. However, if we were to perform this strategy right now, we'd end up with an absurd number of features, a result of the myriad possible terms. The classifier would take too long to train and likely overfit. As a result, each NLP problem requires a tailored approach to determine which terms are relevant and meaningful, and this is where we begin our pre-processing","99819816":"## SMS Spam Collection v.1\n\n\nThe SMS Spam Collection v.1 (hereafter the corpus) is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam. \n\n1.1. Compilation\n----------------\n\nThis corpus has been collected from free or free for research sources at the Web:\n\n- A collection of between 425 SMS spam messages extracted manually from the Grumbletext Web site. This is a UK forum in which cell phone users make public claims about SMS spam messages, most of them without reporting the very spam message received. The identification of the text of spam messages in the claims is a very hard and time-consuming task, and it involved carefully scanning hundreds of web pages. The Grumbletext Web site is: http:\/\/www.grumbletext.co.uk\/\n- A list of 450 SMS ham messages collected from Caroline Tag's PhD Theses available at http:\/\/etheses.bham.ac.uk\/253\/1\/Tagg09PhD.pdf\n- A subset of 3,375 SMS ham messages of the NUS SMS Corpus (NSC), which is a corpus of about 10,000 legitimate messages collected for research at the Department of Computer Science at the National University of Singapore. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available. The NUS SMS Corpus is avalaible at: http:\/\/www.comp.nus.edu.sg\/~rpnlpir\/downloads\/corpora\/smsCorpus\/\n- The amount of 1,002 SMS ham messages and 322 spam messages extracted from the SMS Spam Corpus v.0.1 Big created by Jos\u00e9 Mar\u00eda G\u00f3mez Hidalgo and public available at: http:\/\/www.esp.uem.es\/jmgomez\/smsspamcorpus\/","24bd8dd1":"Next, we'll remove all punctuation since they serve little value once we begin to analyze our data. Continuing the previous pattern, we will create a new column which has the punctuation removed. We will again utilize a for-loop within a lambda function to iterate over the tokens but this time using an IF condition to only output alpha characters.","0207c1e0":"![](https:\/\/media.tenor.com\/images\/ac89ce16c5c20c35c97eeafafecc426b\/tenor.gif)","7fe68173":"- It looks like there are far fewer training examples for spam than ham\u2014we'll take this imbalance into account in the analysis.\n\n- In addition, we need to encode the class labels in the target variable as numbers to ensure compatibility with some models in Scikit-learn. Because we have binary classes, let's use LabelEncoder and set 'spam' = 1 and 'ham' = 0.\n\n- LabelEncoder is a function of scikit learn's preprocessing capabilities, which helps to encode target labsls with values between 0 and the (# of classes) - 1","df6b8f77":"\nAnother important part of any dataset is missing values. When this happens, the dataset can lose expressiveness, which may lead to weak or at times biased analyses. Practically, this means that when you\u2019re missing values for certain features, the chances of your classification or predictions for the data being off only increase.\n\nTo identify the rows that contain missing values, you can use isnull(). In the result that you\u2019ll get back, you\u2019ll see True or False appearing in each cell: True will indicate that the value contained within the cell is a missing value, False means that the cell contains a \u2018normal\u2019 value","8337d20f":"## EDA (Exploratory Data Analysis)","f8c0d2c7":"## Step 5: \u2018Stop Words\u2019 Identification\n\nSome words in the English language, while necessary, don't contribute much to the meaning of a phrase. These words, such as \"when\", \"had\", \"those\" or \"before\", are called stop words and should be filtered out.","d6b68a03":"## Step 1: Contraction Mapping \/ Expanding Contractions\n\nContractions are words that we write with an apostrophe. Examples of contractions are words like \u201cain\u2019t\u201d or \u201caren\u2019t\u201d. Since we want to standardize our text, it makes sense to expand these contractions. We are going to add a new column to our dataframe called \u201cno_contract\u201d and apply a lambda function to the \"msg\" field which will expand any contractions","8e6505b9":"Clearly there's a lot going on here: digits, gratuitous whitespace, and all varieties of punctuation. Some terms are randomly capitalized, others are in all-caps. Since these terms might show up in any one of the training examples in countless forms, we need a way to ensure each training example is on equal footing via a preprocessing step called normalization. This form of noise cleaning takes care of spacing and any special characters.\n\nTransforming all words to lowercase is also a very common pre-processing step. In this case, we will once again append a new column named \u201clower\u201d to the dataframe which will transform all the tokenized words into lowercase. However, because we have to iterate over multiple words we will use a simple for-loop within a lambda function to apply the \u201clower\u201d function to each word.","3ed1780c":"Next, we place the SMS message data into its own table. We must convert this corpus into useful numerical features so we can train this classifier and this is where NLP works its magic!"}}