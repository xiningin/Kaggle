{"cell_type":{"535a6111":"code","a5584a5d":"code","cf0bf990":"code","be23fe2d":"code","8fcfe740":"code","0f3ebef9":"code","008bcc1b":"code","a70484b8":"code","7b787bf9":"code","685f1c39":"code","b1dc1984":"code","c422c214":"code","56cbb3c7":"code","c348c959":"code","f5b0f3f4":"code","85f2c060":"code","80a17fc2":"code","7f23d004":"code","c90fc788":"code","c9f391f5":"code","31aa5464":"code","c719ac8b":"code","fa1c1e47":"code","22e6264b":"code","e0a85024":"code","141bec30":"code","b5f09c86":"code","368e54db":"code","b972a870":"code","31a8c863":"markdown","7968006f":"markdown","d6e83e0f":"markdown","651c2ef3":"markdown","2f9c4a4e":"markdown","8213713f":"markdown","8599a052":"markdown","79075c6a":"markdown","77dc9046":"markdown","011917ab":"markdown","fbca415a":"markdown"},"source":{"535a6111":"from datetime import datetime\n\nprint(\"last update: {}\".format(datetime.now())) ","a5584a5d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","cf0bf990":"import matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split,  GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier,  ExtraTreesClassifier\nimport lightgbm as lgb\nfrom sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score, roc_auc_score\nfrom sklearn.svm import SVC \nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMModel,LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom mlxtend.classifier import StackingCVClassifier\nfrom sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\nimport numpy as np\nnp.random.seed(0)","be23fe2d":"# Read the data\nX_original = pd.read_csv('\/kaggle\/input\/learn-together\/train.csv', index_col='Id')\nX_test = pd.read_csv('\/kaggle\/input\/learn-together\/test.csv', index_col='Id')\nX = X_original.drop('Cover_Type', axis = 1)\ny = X_original['Cover_Type']","8fcfe740":"X.head()","0f3ebef9":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nfrom sklearn.multiclass import OneVsRestClassifier\n\nExperiments = {\"Algo\":[\"RandomForestClassifier\", \"LGBMClassifier\", \"KNeighborsClassifier\", 'XGBClassifier'],\n              \"object\": [lambda: OneVsRestClassifier(RandomForestClassifier(n_estimators = 1000, max_features = 'sqrt')),\n                        lambda: OneVsRestClassifier(LGBMClassifier(learning_rate =0.05, n_estimators=1000)),\n                        lambda: OneVsRestClassifier(KNeighborsClassifier()),\n                        #lambda: SVC(gamma='scale', kernel='rbf', probability=True),\n                        lambda: OneVsRestClassifier(XGBClassifier(learning_rate =0.05, n_estimators=1000))],\n               \"prediction\": [[] for _ in range(5)]}\n\n\nscale = StandardScaler()\n#X_copy.iloc[:,0:10] = scale.fit_transform(X.iloc[:,0:10])\n[_.shape for _ in train_test_split(X, y, test_size = 0.5)]","008bcc1b":"X[X['Aspect']==0]","a70484b8":"X[X['Horizontal_Distance_To_Hydrology'] == 0]","7b787bf9":"# O stand for missing values\ndict = {}\nfor col in X.columns.tolist()[:10]:\n    dict[col] = X[X[col] == 0].shape[0]\ndict    ","685f1c39":"# explicitly require this experimental feature\nfrom sklearn.experimental import enable_iterative_imputer  # noqa\n# now you can import normally from sklearn.impute\nfrom sklearn.impute import IterativeImputer\n# regressor to use for imputing\nfrom sklearn.ensemble import ExtraTreesRegressor\n\n#estimator = ExtraTreesRegressor(n_estimators=10, random_state=0) estimator = estimator, \nimputer = IterativeImputer(missing_values = 0, max_iter=50, random_state=0)\nX_mat = imputer.fit_transform(X.iloc[:,:10].values)\n ","b1dc1984":"X_test_mat = imputer.transform(X_test.iloc[:,:10].values)","c422c214":"X_mat.shape","56cbb3c7":"X_cat = X.iloc[:,10:]\nX_test_cat = X_test.iloc[:,10:]","c348c959":"train_X = np.hstack((X_mat, X_cat.values))\ntest_X = np.hstack((X_test_mat, X_test_cat.values))","f5b0f3f4":"train_X_df = pd.DataFrame(train_X, columns = X.columns.tolist(), index = X.index)\ntest_X_df = pd.DataFrame(test_X, columns = X_test.columns.tolist(), index = X_test.index)\ntrain_X_df.head()","85f2c060":"X.head()","80a17fc2":"test_X_df.head()","7f23d004":"# O stand for missing values\ndict2 = {}\nfor col in train_X_df.columns.tolist()[:10]:\n    dict2[col] = train_X_df[train_X_df[col] == 0].shape[0]\ndict2  ","c90fc788":"# Meta Classifier\nmeta_cls = XGBClassifier(learning_rate =0.1, n_estimators=500)","c9f391f5":"list_estimators = [RandomForestClassifier(n_estimators=400, max_features = 'sqrt',\n                                random_state=1, n_jobs=-1), \n                   XGBClassifier(learning_rate =0.1, n_estimators=400, random_state=1, n_jobs=-1), \n                   LGBMClassifier(n_estimators=400,verbosity=0, random_state=1, n_jobs=-1),\n                   KNeighborsClassifier(n_jobs = -1)]\nbase_methods = list(zip(Experiments[\"Algo\"], list_estimators))\n#base_methods ","31aa5464":"y.values","c719ac8b":"train_X","fa1c1e47":"# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(train_X, y.values, test_size = 0.15, random_state=0)","22e6264b":"from mlxtend.classifier import StackingCVClassifier\n\nstate = 1\nstack = StackingCVClassifier(classifiers=list_estimators,\n                             meta_classifier=meta_cls,\n                             cv=3,\n                             use_probas=True,\n                             verbose=1, \n                             random_state=state,\n                             n_jobs=-1)\n","e0a85024":"stack = stack.fit(X_train,y_train)","141bec30":"X_valid","b5f09c86":"y_val_pred = stack.predict(X_valid)","368e54db":"#performances\ny_vp_val = stack.predict(X_valid)\ny_vp_train = stack.predict(X_train)\nprint('f1_score', f1_score(y_valid, y_vp_val, average='weighted'))\nprint('acc_score_train', accuracy_score(y_train, y_vp_train))\nprint('acc_score_valid', accuracy_score(y_valid, y_vp_val))","b972a870":"preds_test = stack.predict(test_X)\n# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'Cover_Type': preds_test})\noutput.to_csv('submission.csv', index=False)","31a8c863":"Final check for missing values","7968006f":"## Imputing missing values\n\nMultivariate imputer that estimates each feature from all the others.\n\nA strategy for imputing missing values by modeling each feature with missing values as a function of other features in a round-robin fashion.","d6e83e0f":"As we can see there are 110 rows with 0 as aspect values","651c2ef3":"First we start define our meta classifier. For This kaggle kernel instead of OneVsOneClassifier we will use OneVsRestClassifier.\nWe could try OneVsOneClassifier using colab Notebook due to computation cost","2f9c4a4e":"0 stand to be missing values","8213713f":"## Check for missing values","8599a052":"# Stacking\n","79075c6a":"## Reconstruct train and test data","77dc9046":"We could do something to reduce overfitting, as creating news features from existing variables.","011917ab":"## 1- Without Creating New Features and isolating outliers","fbca415a":"# Fourth part Stacking"}}