{"cell_type":{"cc3b4022":"code","fcbfd6c7":"code","1144f6b9":"code","1087299e":"code","c89d074d":"code","69a1cae5":"code","210bd466":"code","d739e289":"code","6557f5fd":"code","970ed60b":"code","0ea67ef2":"code","e277b930":"code","986248f7":"code","6d1b62d9":"code","0b26e407":"code","2802c978":"code","9de43b7c":"code","18b18279":"code","45392a8b":"code","2722a270":"code","9a571b6c":"code","2e0023e8":"code","06e620cd":"code","f1ed145f":"code","cafe94ab":"code","3b7f3a1a":"code","7a2e2052":"code","999931f0":"code","a8140ec8":"code","67723f28":"code","7ca612b3":"code","7e158b46":"code","70948004":"code","8b3fcc22":"code","a6828d2e":"code","3f81d2dd":"code","e4c571cd":"code","61e5e400":"code","8f76df24":"code","c937dc90":"code","9f6207ac":"code","e0963bf1":"code","064efb22":"code","191f54d9":"code","b22f6039":"code","578538ef":"code","1155bb2b":"code","b0e197f1":"code","23cd4332":"code","be12d922":"code","70dd6079":"code","ff5d3f2c":"code","3f4a471c":"code","92811660":"code","906f6a74":"code","f6200e63":"code","abcb8aab":"code","987ebc2c":"code","ef7047e1":"code","b0ddd77d":"code","c8e3d681":"code","066dabf8":"code","b840a14f":"code","fa94e819":"code","d9e899ae":"code","e6f0a802":"code","1e152f52":"code","b608f609":"code","61b07e87":"code","01cc173a":"code","7881ad97":"code","36ffef79":"code","e245cf6a":"code","d0b3ec3b":"code","f3d40d5e":"code","bf2e343b":"code","3830b621":"code","aaf5c254":"code","9e355bb7":"code","47db2f70":"code","9a012f4a":"code","24835d31":"code","db6d8d49":"code","1d0811e6":"code","543b71ab":"code","f387f143":"code","1dbe0998":"code","59f1fe6b":"code","d4d52ef5":"code","a11f04fc":"code","f3710e41":"code","bd6520f3":"code","13cdf368":"code","79f01d0f":"code","6438207f":"code","4377b1ae":"code","a480966b":"code","577d6938":"code","54076e48":"code","2abdc702":"code","0dd2d84c":"code","024c3538":"code","48c59316":"markdown","a9884ced":"markdown","5d2553fe":"markdown","51a6ca56":"markdown","02a75570":"markdown","71e4565b":"markdown","141f9b3a":"markdown","f1f348ff":"markdown","80414387":"markdown","e10484b3":"markdown","7193d9aa":"markdown","a1e39cf9":"markdown","94b9f5ef":"markdown","7c19a07c":"markdown","f0655ef3":"markdown","b1fc99c9":"markdown","dfb0a4f4":"markdown","2d33f819":"markdown","961e66f7":"markdown","c12a2ac5":"markdown","c2ea7f0e":"markdown","2dbf1b07":"markdown","e469248d":"markdown","93aecf9f":"markdown","49d9554e":"markdown","c0a570fc":"markdown","ea746c42":"markdown","d044684c":"markdown","69c5c46a":"markdown","3c609be3":"markdown","33096e1c":"markdown","23f342b1":"markdown","ba35a734":"markdown","92478084":"markdown","1a6a29d0":"markdown","1c73ecd9":"markdown","761cdcb8":"markdown","60f7a922":"markdown","7b2bfacc":"markdown","47a6327c":"markdown","5cea8760":"markdown","fe4671c6":"markdown","649cc61b":"markdown","65e41bea":"markdown","965fa5ec":"markdown","b1a37545":"markdown","f51f0002":"markdown","e32a01df":"markdown","939a6d68":"markdown"},"source":{"cc3b4022":"# importing all needed libraries \n\nimport time\nimport datetime\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# display the output of plotting commands inline within frontends, directly below the code cell that produced it.\n%matplotlib inline\nplt.style.use('ggplot')\n\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom sklearn import model_selection, preprocessing, metrics\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.externals.six import StringIO  \nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, export_graphviz\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error,confusion_matrix, classification_report\nimport pydot\nfrom IPython.display import Image\n\nfrom sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV\nfrom sklearn.model_selection import GridSearchCV\nimport sklearn.linear_model as skl_lm\nfrom sklearn.preprocessing import scale \n\n\n# print all files available in the data folder\nimport os\nprint(os.listdir(\"..\/input\/elo-merchant-category-recommendation\/\"))\n\n# ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","fcbfd6c7":"# Set matplotlib figure sizes to 10 and 6, font size to 12.\nfrom matplotlib import rcParams\nrcParams['figure.figsize'] = (10, 6)\nrcParams['font.size'] = 12","1144f6b9":"# it takes more than 50s to read, because there are 29 million lines in historical_transactions\ntrain = pd.read_csv('..\/input\/elo-merchant-category-recommendation\/train.csv', parse_dates=['first_active_month'])\ntest = pd.read_csv('..\/input\/elo-merchant-category-recommendation\/test.csv', parse_dates=['first_active_month'])\nhistorical_transactions = pd.read_csv('..\/input\/elo-merchant-category-recommendation\/historical_transactions.csv', parse_dates=['purchase_date'])\nnew_merchant_transactions = pd.read_csv('..\/input\/elo-merchant-category-recommendation\/new_merchant_transactions.csv', parse_dates=['purchase_date'])\nmerchants = pd.read_csv('..\/input\/elo-merchant-category-recommendation\/merchants.csv')","1087299e":"#historical_transactions = historical_transactions.sample(n=5000000, random_state=1111)  # random_state is the seed \n#new_merchant_transactions = new_merchant_transactions.sample(n=500000, random_state=1111)  # random_state is the seed ","c89d074d":"train_data = pd.read_excel('..\/input\/Data_Dictionary.xlsx', sheet_name='train')\nhistory_data = pd.read_excel('..\/input\/Data_Dictionary.xlsx', sheet_name='history')\nnew_merchant_period = pd.read_excel('..\/input\/Data_Dictionary.xlsx', sheet_name='new_merchant_period')\nmerchant = pd.read_excel('..\/input\/Data_Dictionary.xlsx', sheet_name='merchant')","69a1cae5":"# description of the train data\ntrain_data","210bd466":"history_data","d739e289":"new_merchant_period","6557f5fd":"merchant","970ed60b":"train.shape","0ea67ef2":"historical_transactions.shape","e277b930":"new_merchant_transactions.shape","986248f7":"merchants.shape","6d1b62d9":"'''The function prints out the number and persentage of null values a dataframe column has.'''\ndef print_null(df):\n    for col in df:\n        if df[col].isnull().any():\n            print('%s has %.0f null values: %.3f%%'%(col, df[col].isnull().sum(), df[col].isnull().sum()\/df[col].count()*100))","0b26e407":"# Checking the types of the column values\nprint(merchants.dtypes)","2802c978":"# Checking for missing data\nprint_null(merchants)","9de43b7c":"#Now, let's look at column histograms:\n\ncat_cols = ['active_months_lag6','active_months_lag3','most_recent_sales_range', 'most_recent_purchases_range','category_1','active_months_lag12','category_4', 'category_2']\nnum_cols = ['numerical_1', 'numerical_2','merchant_group_id','merchant_category_id','avg_sales_lag3', 'avg_purchases_lag3', 'subsector_id', 'avg_sales_lag6', 'avg_purchases_lag6', 'avg_sales_lag12', 'avg_purchases_lag12']\n\n# Removing infinite values and replacing them with NAN\nmerchants.replace([-np.inf, np.inf], np.nan, inplace=True)\n\nplt.figure(figsize=[15, 15])\nplt.suptitle('Merchants table histograms', y=1.02, fontsize=20)\nncols = 4\nnrows = int(np.ceil((len(cat_cols) + len(num_cols))\/4))\nlast_ind = 0\nfor col in sorted(list(merchants.columns)):\n    #print('processing column ' + col)\n    if col in cat_cols:\n        last_ind += 1\n        plt.subplot(nrows, ncols, last_ind)\n        vc = merchants[col].value_counts()\n        x = np.array(vc.index)\n        y = vc.values\n        inds = np.argsort(x)\n        x = x[inds].astype(str)\n        y = y[inds]\n        plt.bar(x, y, color=(0, 0, 0, 0.7))\n        plt.title(col, fontsize=15)\n    if col in num_cols:\n        last_ind += 1\n        plt.subplot(nrows, ncols, last_ind)\n        merchants[col].hist(bins = 50, color=(0, 0, 0, 0.7))\n        plt.title(col, fontsize=15)\n    plt.tight_layout()","18b18279":"#Now, let's look at correlations between columns in merchants.csv:\n\ncorrs = np.abs(merchants.corr())\nordered_cols = (corrs).sum().sort_values().index\nnp.fill_diagonal(corrs.values, 0)\nplt.figure(figsize=[10,10])\nplt.imshow(corrs.loc[ordered_cols, ordered_cols], cmap='plasma', vmin=0, vmax=1)\nplt.colorbar(shrink=0.7)\nplt.xticks(range(corrs.shape[0]), list(ordered_cols), rotation=90)\nplt.yticks(range(corrs.shape[0]), list(ordered_cols))\nplt.title('Heat map of coefficients of correlation between merchant\\'s features', fontsize=17)\nplt.show()","45392a8b":"x = np.array([12, 6, 3]).astype(str)\nsales_rates = merchants[['avg_sales_lag3', 'avg_sales_lag6', 'avg_sales_lag12']].mean().values\npurchase_rates = merchants[['avg_purchases_lag3', 'avg_purchases_lag6', 'avg_purchases_lag12']].mean().values\nplt.bar(x, sales_rates, width=0.3, align='edge', label='average sales', edgecolor=[0.2]*3)\nplt.bar(x, purchase_rates, width=-0.3, align='edge', label='average purchases', edgecolor=[0.2]*3)\nplt.legend()\nplt.title('Avergage sales and number of purchases\\nover the last 12, 6, and 3 months', fontsize=17)\nplt.show()","2722a270":"# Target distribution in the train dataframe\nplt.hist(train['target'], bins= 50)\nplt.title('Loyalty score')\nplt.xlabel('Loyalty score')\nplt.show()","9a571b6c":"((train['target']<-30).sum() \/ train['target'].count()) * 100 # percentage of outliers","2e0023e8":"print(max(train['first_active_month']))\nprint(max(test['first_active_month']))","06e620cd":"d1 = train['first_active_month'].value_counts().sort_index()\nd2 = test['first_active_month'].value_counts().sort_index()\ndata = [go.Scatter(x=d1.index, y=d1.values, name='train'), go.Scatter(x=d2.index, y=d2.values, name='test')]\nlayout = go.Layout(dict(title = \"Counts of first active\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Count'),\n                  ),legend=dict(\n                orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))","f1ed145f":"# binarize authorized_flag, replace Y with 1 and N with 0\nhistorical_transactions['authorized_flag'] = historical_transactions['authorized_flag'].map({'Y':1, 'N':0})","cafe94ab":"# authorized_flag distribution in historical transactions\n(\"At average \" + str(historical_transactions['authorized_flag'].mean() * 100) + \"% transactions are authorized\")\nhistorical_transactions['authorized_flag'].value_counts().plot(kind='bar', title='authorized_flag value counts');","3b7f3a1a":"historical_transactions['installments'].value_counts()","7a2e2052":"historical_transactions.groupby(['installments'])['authorized_flag'].mean()","999931f0":"# We know from the Dictionary that Purchase Amount is normalized\nfor i in [-1, 0]:\n    n = historical_transactions.loc[historical_transactions['purchase_amount'] < i].shape[0]\n    print(\"There are \" + str(n) + \" transactions with purchase_amount less than \" + str(i) + \".\")\nfor i in [0, 10, 100]:\n    n = historical_transactions.loc[historical_transactions['purchase_amount'] > i].shape[0]\n    print(\"There are \" + str(n) + \" transactions with purchase_amount more than \" + str(i) + \".\")","a8140ec8":"max(historical_transactions['purchase_amount'])","67723f28":"# Unique values in historical transactions\nfor col in ['city_id', 'merchant_category_id', 'merchant_id', 'state_id', 'subsector_id']:\n    print(\"There are \" + str(historical_transactions[col].nunique()) + \" unique values in \" + str(col) + \".\")","7ca612b3":"# binarize authorized_flag, replace Y with 1 and N with 0\nnew_merchant_transactions['authorized_flag'] = new_merchant_transactions['authorized_flag'].map({'Y':1, 'N':0})","7e158b46":"# authorized_flag distribution in new merchant transactions\nprint(\"At average \" + str(new_merchant_transactions['authorized_flag'].mean() * 100) + \"% transactions are authorized\")\nnew_merchant_transactions['authorized_flag'].value_counts().plot(kind='bar', title='authorized_flag value counts');","70948004":"new_merchant_transactions['installments'].value_counts()","8b3fcc22":"# We know from the Dictionary that Purchase Amount is normalized\nfor i in [-1, 0]:\n    n = new_merchant_transactions.loc[new_merchant_transactions['purchase_amount'] < i].shape[0]\n    print(\"There are \" + str(n) + \" transactions with purchase_amount less than \" + str(i) + \".\")\nfor i in [0, 10, 100]:\n    n = new_merchant_transactions.loc[new_merchant_transactions['purchase_amount'] > i].shape[0]\n    print(\"There are \" + str(n) + \" transactions with purchase_amount more than \" + str(i) + \".\")","a6828d2e":"# Unique values in new merchant transactions\nfor col in ['city_id', 'merchant_category_id', 'merchant_id', 'state_id', 'subsector_id']:\n    print(\"There are \" + str(new_merchant_transactions[col].nunique()) + \" unique values in \" + str(col) + \".\")","3f81d2dd":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","e4c571cd":"def impute_na(X_train, df, variable):\n    # make temporary df copy\n    temp = df.copy()\n    \n    # extract random from train set to fill the na\n    # temp[variable].isnull().sum() is the size of our sample\n    random_sample = X_train[variable].dropna().sample(temp[variable].isnull().sum(), random_state=1111, replace=True)\n    \n    # pandas needs to have the same index in order to merge datasets\n    random_sample.index = temp[temp[variable].isnull()].index\n    temp.loc[temp[variable].isnull(), variable] = random_sample\n    return temp[variable]","61e5e400":"# It was noticed that clipping the outliers does not improve the model. \n# Maybe because the tree based models that were used are robust to outliers anyway.\n'''Function to clip outliers\ndef clipping_outliers(X_train, df, var):\n    # Calculate the IQR\n    IQR = X_train[var].quantile(0.75) - X_train[var].quantile(0.25)\n    # Get the data that is located in the lower bound\n    lower_bound = X_train[var].quantile(0.25) - 6 * IQR\n    # Get the data that is located in the upper bound\n    upper_bound = X_train[var].quantile(0.75) + 6 * IQR\n    # Extract the data out of the dataframe that is located between the bounds\n    no_outliers = len(df[df[var]>upper_bound]) + len(df[df[var]<lower_bound])\n    print('There are %i outliers in %s: %.3f%%' %(no_outliers, var, no_outliers\/len(df)))\n    df[var] = df[var].clip(lower_bound, upper_bound)\n    return df\n'''","8f76df24":"'''# WE'RE NOT USING MERCHANTS ANYMORE\n# Merchants null\nmerchants = merchants.replace([np.inf,-np.inf], np.nan)  # How does this change the values?\nprint('Merchants null')\nprint_null(merchants)\n\n# We fill null values in the merchants data with the mean value of the column.\nnull_cols = ['avg_purchases_lag3','avg_sales_lag3', 'avg_purchases_lag6','avg_sales_lag6','avg_purchases_lag12','avg_sales_lag12']\nfor col in null_cols:\n    merchants[col] = merchants[col].fillna(merchants[col].mean())\n\n# Fill category_2 with random sampling from available data\nmerchants['category_2'] = impute_na(merchants, merchants, 'category_2')\n'''","c937dc90":"'''# WE'RE NOT USING MERCHANTS ANYMORE\nmerchants['category_1'] = merchants['category_1'].map({'Y':1, 'N':0})\nmerchants['category_4'] = merchants['category_4'].map({'Y':1, 'N':0})\n\nmap_cols = ['most_recent_purchases_range', 'most_recent_sales_range']\nfor col in map_cols:\n    merchants[col] = merchants[col].map({'A':5,'B':4,'C':3,'D':2,'E':1})\n\nnumeric_cols = ['numerical_1','numerical_2'] + null_cols + map_cols\n\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(12,12))\nsns.heatmap(merchants[numeric_cols].astype(float).corr(), linewidths=0.1, vmax=1.0, vmin=-1., square=True, cmap=colormap, linecolor='white', annot=True)\nplt.title('Pair-wise correlation')\n\nmerchants.head()\n'''","9f6207ac":"max(new_merchant_transactions['purchase_date']) # when did the last transaction happen?","e0963bf1":"# The last date to calculate time lags from \nREF_DATE = datetime.datetime.strptime('2018-12-31', '%Y-%m-%d')","064efb22":"# Create columns that calculate the number of days from the transaction day to the reference day (2018-12-31)\nhistorical_transactions['days_to_date'] = ((REF_DATE - historical_transactions['purchase_date']).dt.days) \n#historical_transactions['days_to_date'] = historical_transactions['days_to_date'] #+ df_hist_trans['month_lag']*30\nnew_merchant_transactions['days_to_date'] = ((REF_DATE - new_merchant_transactions['purchase_date']).dt.days)#\/\/30\n\n### Here we're concatinatig historical transactions with new transactions, since they both have the same columns and form\n### and therefore do not need to be joined together. ### \ntransactions = pd.concat([historical_transactions, new_merchant_transactions])  \n\n# Create column months_ro_date: this is the number of months from transaction date to reference date (2018-12-31)\ntransactions['months_to_date'] = transactions['days_to_date']\/\/30\ntransactions = transactions.drop(columns=['days_to_date'])\n\n# Reduce memory usage\ntransactions = reduce_mem_usage(transactions)\n\ntransactions.head()","191f54d9":"# We do not need the 2 dataframes anymore, beccause we have all the data needed in transactions.\ndel historical_transactions\ndel new_merchant_transactions","b22f6039":"'''# WE'RE NOT USING MERCHANTS ANYMORE\n# Merge trasactions with merchant data\ntransactions = pd.merge(transactions, merchants, how='left', left_on='merchant_id', right_on='merchant_id')\ntransactions.head()\n'''","578538ef":"'''# WE'RE NOT USING MERCHANTS ANYMORE\n# Take the 2 last characters out from the column names of the transactions data frame.\nt = list(transactions)\ntrans_cols = []\nfor e in t:\n    trans_cols.append(e[:-2])\n'''","1155bb2b":"'''# WE'RE NOT USING MERCHANTS ANYMORE\nseen = {}\ndupes = []\n\nfor x in trans_cols:\n    if x not in seen:\n        seen[x] = 1\n    else:\n        if seen[x] == 1:\n            dupes.append(x)\n        seen[x] += 1\ndupes  # there are duplicate columns in transactions, which end in _x and _y\n'''","b0e197f1":"'''# WE'RE NOT USING MERCHANTS ANYMORE\ntransactions = transactions.drop(columns=['category_1_y', 'category_2_y', 'city_id_y', 'state_id_y', 'merchant_category_id_y',\n                                        'merchant_category_id_y', 'subsector_id_y'])\n\ntransactions.rename(columns={'category_1_x': 'category_1', \n                            'category_2_x': 'category_2',\n                            'city_id_x': 'city_id',\n                            'state_id_x': 'state_id',\n                            'merchant_category_id_x': 'merchant_category_id',\n                            'merchant_category_id_x': 'merchant_category_id',\n                            'subsector_id_x': 'subsector_id'}, inplace=True)\n'''","23cd4332":"# Null ratio\nprint('Null ratio')\nprint_null(transactions)","be12d922":"# The function prints out the most common values of one column\ndef most_frequent(x):\n    return x.value_counts().index[0]","70dd6079":"print(\"merchant_id\", most_frequent(transactions['merchant_id'])) ##A:'M_ID_00a6ca8a8a'\n# print(\"category_4\", most_frequent(merchants['category_4'])) ##A:'0.0'\n# print(\"most_recent_sales_range\", most_frequent(merchants['most_recent_sales_range'])) ##A:'1.0'\n# print(\"most_recent_purchases_range: \", most_frequent(merchants['most_recent_purchases_range'])) ##A:'1.0'\nprint(\"category_2: \", most_frequent(transactions['category_2']))\nprint(\"category_3: \", most_frequent(transactions['category_3']))","ff5d3f2c":"# Fill null by most frequent data\ntransactions['category_2'].fillna(1.0,inplace=True)\ntransactions['category_3'].fillna('A',inplace=True)\ntransactions['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)","3f4a471c":"'''# WE'RE NOT USING MERCHANTS ANYMORE\n# Fill the merchant columns that have null values with random values.\n\n# nan_cols = transacations.columns[transactions.isna().any()].tolist()\nnan_cols = ['active_months_lag3','active_months_lag6','active_months_lag12','avg_purchases_lag3','avg_sales_lag3', 'avg_purchases_lag6','avg_sales_lag6','avg_purchases_lag12','avg_sales_lag12']\nfor col in nan_cols:\n    transactions[col] = impute_na(transactions, transactions, col)\n\n# merchants['category_4'].fillna(0.0,inplace=True)\n# merchants['most_recent_sales_range'].fillna(1.0,inplace=True)\n'''","92811660":"print('Null ratio')\nprint_null(transactions) # There are no more null values in the transactions dataframe for the moment","906f6a74":"# Encoding (Mapping\/ Dummy vars)\n# Binarizing Y to 1 and N to 0\n# transactions['authorized_flag'] = transactions['authorized_flag'].map({'Y':1,'N':0})  # already done for authoried_flag\n# Category 1 has only 2 distinct values\ntransactions['category_1'] = transactions['category_1'].map({'Y':1,'N':0})\n\n\n# pd.get_dummies when applied to a column of categories where we have one category per observation \n# will produce a new column (variable) for each unique categorical value. \n# It will place a one in the column corresponding to the categorical value present for that observation.\ndummies = pd.get_dummies(transactions[['category_2', 'category_3']], prefix = ['cat_2','cat_3'], columns=['category_2','category_3'])\ntransactions = pd.concat([transactions, dummies], axis=1) # axis=1 joins all the columns\n \ntransactions.head()\ntransactions = reduce_mem_usage(transactions)","f6200e63":"transactions['weekend'] = (transactions['purchase_date'].dt.weekday >=5).astype(int)\ntransactions['hour'] = transactions['purchase_date'].dt.hour\ntransactions['day'] = transactions['purchase_date'].dt.day\n\n# Calculate the weeks left till Christmas (2017-12-25)\ntransactions['weeks_to_Xmas_2017'] = ((pd.to_datetime('2017-12-25') - transactions['purchase_date']).dt.days\/\/7).apply(lambda x: x if x>=0 and x<=60 else 0)\n# Calculate the weeks left till Black Friday (2017-11-25)\ntransactions['weeks_to_BFriday'] = ((pd.to_datetime('2017-11-25') - transactions['purchase_date']).dt.days\/\/7).apply(lambda x: x if x>=0 and x<=60 else 0)\n#Mothers Day: May 14 2017 and 2018\ntransactions['Mothers_Day_2017']=(pd.to_datetime('2017-06-04')-transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 60 else 0)\ntransactions['Mothers_Day_2018']=(pd.to_datetime('2018-05-13')-transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 60 else 0)\n#fathers day: August 13 2017\ntransactions['Fathers_day_2017']=(pd.to_datetime('2017-08-13')-transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 60 else 0)\n#Childrens day: October 12 2017\ntransactions['Children_day_2017']=(pd.to_datetime('2017-10-12')-transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 60 else 0)\n#Valentine's Day : 12th June, 2017\ntransactions['Valentine_Day_2017']=(pd.to_datetime('2017-06-12')-transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 60 else 0)\n# Carnival in Brasil 27.02.2017 - 28.02.2017\ntransactions['Carnival_2017']=(pd.to_datetime('2017-02-27')-transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 60 else 0)\n# Carnival in Brasil 09.02.2018\ntransactions['Carnival_2018']=(pd.to_datetime('2018-02-09')-transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 60 else 0)\n# Easter: 14.04.2017 - Restaurants\ntransactions['Easter_2017']=(pd.to_datetime('2017-04-14')-transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 60 else 0)\n# Tiradentes: 21.04.2017 - Restaurants\ntransactions['Tiradentes_2017']=(pd.to_datetime('2017-04-21')-transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 60 else 0)\n# Labour day: 01.05.2017 - Restaurants\ntransactions['Labour_day_2017']=(pd.to_datetime('2017-05-01')-transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 60 else 0)\n# Independence day 01.09.2017 - Restaurants\ntransactions['Independence_day_2017']=(pd.to_datetime('2017-09-01')-transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 60 else 0)","abcb8aab":"# Categorize time in 4 categories: (0) from 5 to 11, (1) from 12 to 16, (2) from 17 to 20 and (3) from 21 to 4\n#Hypothesis: when do people go out to restaurants?\ndef get_session(hour):\n    hour = int(hour)\n    if hour > 4 and hour < 12:\n        return 0\n    elif hour >= 12 and hour < 17:\n        return 1\n    elif hour >= 17 and hour < 21:\n        return 2\n    else:\n        return 3\n    \ntransactions['hour'] = transactions['hour'].apply(lambda x: get_session(x))","987ebc2c":"# Categorize day in 3 categories: (0) 0 to 10, (1) 11 to 20  and (2) over 20\n# Hypothesis: People have more money or less money in the beginning or end of the month.\ndef get_day(day):\n    if day <= 10:\n        return 0\n    elif day <=20:\n        return 1\n    else:\n        return 2\n\ntransactions['day'] = transactions['day'].apply(lambda x: get_day(x))","ef7047e1":"transactions.head()","b0ddd77d":"def aggregate_trans(df):\n    agg_func = {\n        'authorized_flag': ['mean', 'std'],\n        'category_1': ['mean'],\n        'cat_2_1.0': ['mean'],\n        'cat_2_2.0': ['mean'],\n        'cat_2_3.0': ['mean'],\n        'cat_2_4.0': ['mean'],\n        'cat_2_5.0': ['mean'],\n        'cat_3_A': ['mean'],\n        'cat_3_B': ['mean'],\n        'cat_3_C': ['mean'],\n        ###'numerical_1':['nunique','mean','std'], # merchants\n        #'most_recent_sales_range': ['mean','std'], # merchants\n        #'most_recent_purchases_range': ['mean','std'], # merchants\n        ###'avg_sales_lag12':['mean','std'], # merchants\n        ###'avg_purchases_lag12':['mean','std'], # merchants\n        ###'active_months_lag12':['nunique'], # merchants\n        'merchant_id': ['nunique'],\n        'merchant_category_id': ['nunique'],  # counts unique values of id for the rows that were groupped by card_id\n        ###'state_id': ['nunique'], #\n        'city_id': ['nunique'],\n        ###'subsector_id': ['nunique'], # merchants\n        ###'merchant_group_id': ['nunique'], # merchants\n        'installments': ['sum','mean', 'max', 'min', 'std'],\n        'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n        'weekend': ['mean', 'std'],\n        'hour': ['mean', 'std'],\n        'day': ['mean', 'std'],\n        'weeks_to_Xmas_2017': ['mean', 'sum'],\n        'weeks_to_BFriday': ['mean', 'sum'],\n        'purchase_date': ['count'],\n        'months_to_date': ['mean', 'max', 'min', 'std'],\n        'Mothers_Day_2017': ['mean', 'sum'],\n        'Fathers_day_2017': ['mean', 'sum'],\n        'Children_day_2017': ['mean', 'sum'],\n        'Valentine_Day_2017': ['mean', 'sum'],\n        'Mothers_Day_2018': ['mean', 'sum'],\n        'Labour_day_2017': ['mean', 'sum'],\n        'Independence_day_2017': ['mean', 'sum'],\n        'Easter_2017': ['mean', 'sum'],\n        'Tiradentes_2017': ['mean', 'sum'],\n        'Carnival_2017': ['mean', 'sum'],\n        'Carnival_2018': ['mean', 'sum']\n    }\n    #'mer_category_4': ['mean'],\n    #'mer_avg_sales_lag6':['nunique', 'mean','std'],\n    #'mer_avg_purchases_lag6':['nunique', 'mean','std'],\n    #'months_to_date': ['mean', 'max', 'min', 'std'],\n    agg_df = df.groupby(['card_id']).agg(agg_func)  # WHAT IS DF?\n    agg_df.columns = ['_'.join(col)for col in agg_df.columns.values]\n    agg_df.reset_index(inplace=True)\n    return agg_df","c8e3d681":"def aggregate_per_month(history):\n    \n    # Group the dataframe by card_id and month_lag\n    grouped = history.groupby(['card_id', 'month_lag'])\n    # Convert the data type of the column installments to integer\n    history['installments'] = history['installments'].astype(int)\n    # Add aggregate functions count, sum, mean, min, max, std to the dataframe\n    # agg_func is a dictionary that assigns the aggregate functions to the columns they will be applied on\n    agg_func = {\n            'purchase_amount': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n            'installments': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n            }\n\n    #Aggregate using The above mentioned functions over the dictionary keys (purchase_amount, installments).\n    intermediate_group = grouped.agg(agg_func)\n    # Rename the columns add '-' between column name and aggregate function name\n    intermediate_group.columns = ['_'.join(col).strip() for col in intermediate_group.columns.values]\n    # Reset the index of the dataframe after aggregating\n    intermediate_group.reset_index(inplace=True)\n\n    # Group by card_id and add functions mean and std\n    end_df = intermediate_group.groupby('card_id').agg(['mean', 'std'])\n    end_df.columns = ['_'.join(col).strip() for col in end_df.columns.values]\n    end_df.reset_index(inplace=True)\n    \n    return end_df","066dabf8":"# Aggregate transactions\nagg_transactions = aggregate_trans(transactions)\nagg_transactions_permonth = aggregate_per_month(transactions)","b840a14f":"# Merge aggregated transactions\nagg_trans = pd.merge(agg_transactions, agg_transactions_permonth, how='left', on='card_id')\nagg_trans = reduce_mem_usage(agg_trans)","fa94e819":"agg_trans.head()","d9e899ae":"# Delete agg_transactions and agg_transactions_per_month because they were already merged together\ndel agg_transactions\ndel agg_transactions_permonth","e6f0a802":"# Columns that have na values in them\nnan_cols = agg_trans.columns[agg_trans.isna().any()].tolist()\nnan_cols","1e152f52":"## Replace infinite values with their\nagg_trans = agg_trans.replace([np.inf,-np.inf], np.nan)\n#agg_trans = agg_trans.fillna(value=0)  # Take the mean?\n\n# agg_trans.mean(): calculates the mean of every column of the dataframe\n#agg_trans = agg_trans.fillna(value=agg_trans.mean())\n# It works faster when definined the columns containing nan values\nagg_trans[nan_cols] = agg_trans[nan_cols].fillna(value=agg_trans[nan_cols].mean())","b608f609":"# Droping outliers worsened the performance\n# What if we drop outliers from target variable\n#train['outliers'] = 0\n#train.loc[train['target'] < -30, 'outliers'] = 1\n#train['outliers'].value_counts()","61b07e87":"#train = train[train.outliers != 1]\n#train = train.drop(columns=['outliers'])\n#train.shape","01cc173a":"# Add random values to the null values of the column\ntest['first_active_month'] = impute_na(test, train, 'first_active_month')","7881ad97":"# Merge the train and test dataframes with the agg_trans dataframe\ntrain = pd.merge(train, agg_trans, on='card_id', how='left')\ntest = pd.merge(test, agg_trans, on='card_id', how='left')","36ffef79":"# Create columns year and month out of the column first_active_month \ntrain[\"year\"] = train[\"first_active_month\"].dt.year\ntest[\"year\"] = test[\"first_active_month\"].dt.year\ntrain[\"month\"] = train[\"first_active_month\"].dt.month\ntest[\"month\"] = test[\"first_active_month\"].dt.month","e245cf6a":"# Get numerical features\nnumerical = [var for var in train.columns if train[var].dtype!='O']\nprint('There are {} numerical variables'.format(len(numerical)))\n\n# Get discrete features\ndiscrete = []\nfor var in numerical:\n    if len(train[var].unique())<8:\n        discrete.append(var)\n        \nprint('There are {} discrete variables'.format(len(discrete)))\n\n# Get continuous features\ncontinuous = [var for var in numerical if var not in discrete and var not in ['card_id', 'first_active_month','target']]\nprint('There are {} continuous variables'.format(len(continuous)))","d0b3ec3b":"# Detect all null columns\ntrain_null = train.columns[train.isnull().any()].tolist()\ntest_null = test.columns[test.isnull().any()].tolist()\n\n# Get a set out of the null columns. The set only contains unique values.\nin_first = set(train_null)\nin_second = set(test_null)\n\n# Get columns that are in the test dataframe but not in the train dataframe\nin_second_but_not_in_first = in_second - in_first\n\n# Create list of null columns\nnull_cols = train_null + list(in_second_but_not_in_first)","f3d40d5e":"# Filling null\nfor col in null_cols:\n    if col in continuous:\n        # if it is a continuous column, fill with 0-s\n        train[col] = train[col].fillna(0)#df_train[col].astype(float).mean())\n        test[col] = test[col].fillna(0)#df_train[col].astype(float).mean())\n    if col in discrete:\n        # if it is a descrete columns fill with random values\n        train[col] = impute_na(train, train, col)\n        test[col] = impute_na(test, train, col)","bf2e343b":"print('Final null')\n# There are no more null values in the dataframes\nprint_null(train)\nprint_null(test)","3830b621":"# Take card_id, first_active_month and target out of the list of the columns to use for training models, so that no errors are caused.\ncols_to_use = list(train)\ncols_to_use.remove('card_id')\ncols_to_use.remove('first_active_month')\ncols_to_use.remove('target')","aaf5c254":"# Take card_id, first_active_month and target out of the list of the columns to use for training models, so that no errors are caused.\nnames = list(train)\nnames.remove('card_id')\nnames.remove('first_active_month')\nnames.remove('target')","9e355bb7":"# Define a function that returns the cross-validation rmse error \ndef rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, train[names], train['target'], scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse)","47db2f70":"alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\ncv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() \n            for alpha in alphas]\n\ncv_ridge = pd.Series(cv_ridge, index = alphas)\ncv_ridge.plot(title = \"Validation - Just Do It\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")\n#cv_ridge.min()","9a012f4a":"# Fit the training data to RidgeCV model\nridgeCV = RidgeCV(alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]).fit(train[names], train['target'])\nrmse_cv(ridgeCV).mean()","24835d31":"# Predict the loyalty score of the test data\nridgeCV_pred = ridgeCV.predict(test[names])","db6d8d49":"#Submitting the prediction of the ridgecv regression.\nsubmit = pd.DataFrame({\"card_id\":test[\"card_id\"].values})\nsubmit[\"target\"] = ridgeCV_pred\nsubmit.to_csv(\"elo_submission_ridgeCV.csv\", index=False)","1d0811e6":"# Fit the training data to LassoCV model\nlassoCV = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(train[names], train['target'])\nrmse_cv(lassoCV).mean()","543b71ab":"# Predict the loyalty score of the test data\nlassoCV_pred = lassoCV.predict(test[names])","f387f143":"# Submitting the prediction of the lasso regression.\nsubmit = pd.DataFrame({\"card_id\":test[\"card_id\"].values})\nsubmit[\"target\"] = lassoCV_pred\nsubmit.to_csv(\"elo_submission_lasso.csv\", index=False)","1dbe0998":"# Take a look at the coefficients\ncoef = pd.Series(lassoCV.coef_, index = train[names].columns)\n\nprint(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n\n# We are taking the first (highest) and last (lowest) 10 values of the coeffiecients and then we plot them.\nimp_coef = pd.concat([coef.sort_values().head(10),coef.sort_values().tail(10)])\n\nplt.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")","59f1fe6b":"# Choosing max depth 3\n# Fit the training data to the Regression Tree model\nregr2 = DecisionTreeRegressor(max_depth=3)\nregr2.fit(train[names], train['target'])\n\n# Predict the loyalty score of the test data\npred_tree = regr2.predict(test[names])","d4d52ef5":"# Submitting the prediction of a regression decision tree.\nsubmit = pd.DataFrame({\"card_id\":test[\"card_id\"].values})\nsubmit[\"target\"] = pred_tree\nsubmit.to_csv(\"elo_submission_regression_tree.csv\", index=False)","a11f04fc":"# This function creates images of tree models using pydot\ndef print_tree(estimator, features, class_names=None, filled=True):\n    tree = estimator\n    names = features\n    color = filled\n    classn = class_names\n    \n    dot_data = StringIO()\n    export_graphviz(estimator, out_file=dot_data, feature_names=features, class_names=classn, filled=filled)\n    graph = pydot.graph_from_dot_data(dot_data.getvalue())\n    return(graph)","f3710e41":"# Print tree\ngraph, = print_tree(regr2, features=cols_to_use)\nImage(graph.create_png())","bd6520f3":"# Fit the training data to Random Forest model\nregr1 = RandomForestRegressor(max_features=13, random_state=1)\nregr1.fit(train[names], train['target'])","13cdf368":"# Predict the loyalty score of the test data\npred_forest = regr1.predict(test[names])\n\n# Submitting the prediction of a random forest now.\nsubmit = pd.DataFrame({\"card_id\":test[\"card_id\"].values})\nsubmit[\"target\"] = pred_forest\nsubmit.to_csv(\"elo_submission_random_forest_13.csv\", index=False)","79f01d0f":"# Print feature importance\nfig, ax = plt.subplots(figsize=(12,27))\nImportance = pd.DataFrame({'Importance':regr1.feature_importances_*100}, index=train[names].columns)\nImportance.sort_values('Importance', axis=0, ascending=True).plot(kind='barh', color='g', ax=ax )\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None","6438207f":"# Fit the training data to Boosting model\n# n_estimators: The number of boosting stages to perform.\nregr_b = GradientBoostingRegressor(n_estimators=600, learning_rate=0.005, random_state=1)\nregr_b.fit(train[names], train['target'])","4377b1ae":"# Print feature importance\nfig, ax = plt.subplots(figsize=(12,27))\nfeature_importance = regr_b.feature_importances_*100\nrel_imp = pd.Series(feature_importance, index=train[names].columns).sort_values(inplace=False)\nprint(rel_imp)\nrel_imp.T.plot(kind='barh', color='r', ax=ax)\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None","a480966b":"# Predict the loyalty score of the test data\npred_b = regr_b.predict(test[names])","577d6938":"# Submitting the prediction of a boosting method.\nsubmit = pd.DataFrame({\"card_id\":test[\"card_id\"].values})\nsubmit[\"target\"] = pred_b\nsubmit.to_csv(\"elo_submission_boosting.csv\", index=False)","54076e48":"def run_lgb(train_X, train_y, val_X, val_y, test_X):\n    \n    # Define the model parameters    \n    params = {'num_leaves': 111,\n             'min_data_in_leaf': 150,  # was 149 \n             'objective':'regression',\n             'max_depth': 9,\n             'learning_rate': 0.005,\n             \"boosting\": \"gbdt\",\n             \"feature_fraction\": 0.75,\n             \"bagging_freq\": 1,\n             \"bagging_fraction\": 0.70,\n             \"bagging_seed\": 11,\n             \"metric\": 'rmse',\n             \"lambda_l1\": 0.25,  # was 0.26\n             \"random_state\": 1111,\n             \"verbosity\": -1}\n\n    # Convert train dataframe to a dataset\n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    evals_result = {}\n    # Fit the training data to the lgb model\n    model = lgb.train(params, lgtrain, 10000, valid_sets=[lgval], early_stopping_rounds=100, verbose_eval=100, evals_result=evals_result)\n    # Predict the loyalty score of the test data\n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    return pred_test_y, model, evals_result\n\ntrain_X = train[names]\ntest_X = test[names]\ntrain_y = train.target.values\n\npred_test = 0  # Initialize pred_test\n\n# Running a k-fold cross validation\nkf = model_selection.KFold(n_splits=5, random_state=1111, shuffle=True)\nfor dev_index, val_index in kf.split(train):\n    dev_X, val_X = train_X.loc[dev_index,:], train_X.loc[val_index,:]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    \n    pred_test_tmp, model, evals_result = run_lgb(dev_X, dev_y, val_X, val_y, test_X)\n    pred_test += pred_test_tmp\npred_test \/= 5. ","2abdc702":"# Plotting the first 50 features with highest importance\nfig, ax = plt.subplots(figsize=(12,10))\nlgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nax.grid(False)\nplt.title(\"LightGBM - Feature Importance\", fontsize=15)\nplt.show()","0dd2d84c":"#Submitting the prediction of the lighgbm model.\nsubmit = pd.DataFrame({\"card_id\":test_df[\"card_id\"].values})\nsubmit[\"target\"] = pred_test\nsubmit.to_csv(\"elo_submission_lightgbm.csv\", index=False)","024c3538":"model_results = pd.read_excel('..\/input\/model-results\/submissions_table.xlsx')\nmodel_results","48c59316":"<a id=\"1\"><\/a> <br>\n## 1. Inspirational Kernels\nThese are the kernels that helped us \n* [Simple Data Exploration with Python [LB : 3.764]](https:\/\/www.kaggle.com\/chocozzz\/simple-data-exploration-with-python-lb-3-764) \n* [Making Sense of Elo Data (EDA)](https:\/\/www.kaggle.com\/batalov\/making-sense-of-elo-data-eda)\n* [Ridge + LightGBM + feature Engineering + Bayesian](https:\/\/www.kaggle.com\/ashishpatel26\/ridge-lightgbm-feature-engineering-bayesian)\n* [Elo World](https:\/\/www.kaggle.com\/fabiendaniel\/elo-world)\n* [Simple Exploration Notebook - Elo](https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-elo)\n* [Elo EDA and models](https:\/\/www.kaggle.com\/artgor\/elo-eda-and-models)\n* [Simple LightGBM without blending](https:\/\/www.kaggle.com\/mfjwr1\/simple-lightgbm-without-blending)\n* [Data Science Classes 8 & 10](http:\/\/)","a9884ced":"It seems as if installments with 999 as value have not been authorized. ","5d2553fe":"Here we add new features to our transaction data. ","51a6ca56":"**Merchants' preprocessing**","02a75570":"<a id=\"11\"><\/a> <br>\n## 11. Model Performance Comparison and Conclusion","71e4565b":"**1. Description summary**\n\nElo is a leading Brazilian payment provider. It cooperates with different merchants to offer customers various discounts and promotions. However, Elo is not sure how these promotions affect both merchants and customers. For this reason the company wants to look into a more personalised approach. \n\nElo is looking to develop algorithms to tailor its discounts and promotions to each individual based on their loyalty.\n\n\n**2. Our understanding of the problem**\n\nThe goal of this competition is to understand the relationship between some variables and the target value. For this Elo needs a model that will predict people\u2019s reaction to discounts and promotions based on their past behaviour and personal details.","141f9b3a":"**4) Exploration of the merchants dataframe**","f1f348ff":"It looks like the sales are steadily growing over time.\n\nSince we saw in the discussion board, in quite a few other kernels and in a few of our commits that the merchants data does not improve the model, we decided not to use it in the next steps of the project. ","80414387":"\nCreate knowledge-based features:\n\n* Weekend or not\n* Hour of the day: categorize into Morning (5 to 12), Afternoon (12 to 17), Evening (17 to 22) and Night (22 to 5)\n* Day of month: categorize into Early (<10), Middle (>10 and <20) and Late (>20)\n* Time to christmas 2017, Black Friday 2017 and time to many other national holidays. Purchase amounts could increase significantly around these times.","e10484b3":"<a id=\"8\"><\/a> <br>\n## 8. Random Forest","7193d9aa":"Function to fill discrete missing values in with random sampling","a1e39cf9":"<a id=\"4\"><\/a> <br>\n## 4. Feature engineering","94b9f5ef":"**Aggregating features**","7c19a07c":"In the new merchant transactions as well most of the installments have the value 0 oor 1.","f0655ef3":"## Notebook  Content\n1. [Inspirational Kernels](#1)\n2. [EDA](#2)\n3. [Loading the data](#3)\n4. [Feature engineering](#4)\n5. [Train](#5)\n6. [Ridge and Lasso](#6)\n7. [Regression Tree](#7)\n8. [Random Forest](#8)\n9. [Boosting](#9)\n10. [Lightgbm](#10)\n11. [ Model Performance Comparison and Conclusion](#11)","b1fc99c9":"1. Function to reduce memory usage from kernel: Elo World.","dfb0a4f4":"<a id=\"10\"><\/a> <br>\n## 10. Lightgbm","2d33f819":"The first step of the project was to get familiar with the available data. Our method was to look into each fileand explore its shape, features, size, values etc. ","961e66f7":"<a id=\"6\"><\/a> <br>\n## 6. Ridge and Lasso","c12a2ac5":"<a id=\"9\"><\/a> <br>\n## 9. Boosting","c2ea7f0e":"<a id=\"7\"><\/a> <br>\n## 7. Regression Tree","2dbf1b07":" numerical_1 and numerical_2 seem not to be numerical but on the contrary - categorical \n","e469248d":"**5) Exploration of the train and test dataframes**","93aecf9f":"**Transaction data**","49d9554e":"In the new merchant transactions dataframe all transactions are authorized.","c0a570fc":"Aggregate other columns of the transactions dataframe grouping by the card_id.","ea746c42":"**1) Description of the columns of all the data. **","d044684c":"In order to predict the loyalty score of Elo customers we conducted EDA, followed by pre-processing, then engineered additional features and lastly we fitted the data to different models.\n\nAfter the testing of these various models, the LightGBM gave us the best RMSE, thus we used exatlly this model for our final submission.\n\nWe saw that a few features have higher importnace than the others and namely, ...\n\n","69c5c46a":"<a id=\"5\"><\/a> <br>\n## 5. Train and Test","3c609be3":"**New merchant transactions**","33096e1c":"**2) Shapes of the different dataframes**","23f342b1":"There are many  transactions with a purchase_amount between -1 and 0. Maybe this suggests that the purchase has been standardized (scaled). Nonetheless, the highest purchase amount in downsampled data is over 6 Million.","ba35a734":"* numerical_1 and numerical_2 are highly correlated\n* avg_sales and avg_purchases within the last 3, 6, and 12 months are highly correlated\n* mechant_group_id is loosely correlated with numerical_1, 2, city_id, and sales statistics\n* merchant_category_id shows little correlation with merchant_group_id, city_id, or really anything else.\n* category_1 is slightly correlated with the merchant's location (city_id and state_id)","92478084":"Predict with a decesion tree regression.\n\nThe decision tree is used to fit the data with addition noisy observation. As a result, it learns local linear regressions approximating the fitted curve.\nParameter max_depth: The maximum depth of the tree. If none, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. If the maximum depth of the tree is set too high, then the decision trees learn too fine details of the training data, so they learn from the noise and thus overfit.","1a6a29d0":"> - Unlike fitting a single large decision tree to the data, which amounts to fitting the data hard and potentially overfitting, the boosting approach instead learns slowly.\n> - Given the current model, we fit a decision tree to the residuals from the model.  We then add this new decision tree into the fitted function in order to update the residuals.\n> - Each of these trees can be rather small, with just a few terminal nodes, determined by the parameter d in the algorithm.\n> - By fitting small trees to the residuals, we slowly improve \u02c6f in areas where it does not perform well.  The shrinkage parameter \u03bb slows the process down even further, allowing more and different shaped trees to attack the residuals.\n[Source](https:\/\/lagunita.stanford.edu\/c4x\/HumanitiesScience\/StatLearning\/asset\/trees.pdf)\n","1c73ecd9":">Light GBM is a gradient boosting framework that uses tree-based learning algorithm. [Light GBM source](https:\/\/medium.com\/@pushkarmandot\/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc)\n\n>Light GBM grows the trees vertically (leaf-wise), while other tree-algorithms grow horizontally (level-wise). It will choose the leaf with max delta loss to grow. The algorithm can handle large amounts of data and takes lower memory to run. It focuses on accuracy of results. Light GBM is sensitive to overfitting and thus it is not advisable to use it with small amounts of data. [Light GBM source](https:\/\/medium.com\/@pushkarmandot\/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc)\n\n>It covers more than 100 parameters and we couldn't possibly look at them all. Here are the main ones:\n\n> * num_leaves: number of leaves in full tree (default: 31)\n> * min_data_in_leaf: the minimum number (default value: 20) of records a leaf could have. It also could help with overfitting.\n> * max_depth: describes the maximum depth of the tree. It handles model overfitting. When the model is overfitted, lowering the max_depth could help.\n> * learning_rate: determines the impact of each tree on the final outcome. It slows down the algorithm to learn more. It controls the magnitude of the change each new tree brings to the estimate. \n> * boosting: defines the type of algorithm to run. Default: gdbt (gradient boosting decision tree). There also is rf (random forest), dart (dropouts meet multiple additive regression trees), gross (gradient-based one-side sampling)\n> * feature_fraction: used when the boosting is random forest. 0.8 feature fraction means LightGBM will select 80% of parameters randomly in each iteration for building trees.\n> * bagging_fraction: specifies the fraction of the data to be used for each iteration and is generally used to speed up training and avoid overfitting.\n> * bagging_freq: is used for faster speed\n> * metric: specifies loss for model building\n> * lambda_l1: specifies regularization. Typical value ranges from 0 to 1.\n","761cdcb8":"Historical transactions has more than 29 Million rows, we're going to sample the data so that we can join it faster together while working and so that our kernel does not die while running. New merchant transactions has about 2 Million rows.\n\nIn the case of multiple commits, we applied a downsampling technique through taking a random sample of the 5 Million from historical transactions and of 500 thousand from new merchant transactions. In the final version the notebook was ran without any downsampling. ","60f7a922":"<a id=\"3\"><\/a> <br>\n## 3. Loading the data","7b2bfacc":"Aggregate transactions by card_id and month_lag.","47a6327c":"**3) Function to print out null values**","5cea8760":"<a id=\"2\"><\/a> <br>\n## 2. EDA","fe4671c6":"We see there are 1,09 % of outliers in the data. We did try to run the models with and without the outliers and noticed that it did not improve our submission score. Thus, we are not dropping them in the feature engineering..","649cc61b":"\n> Random forest is an ensemble method. An ensemble method consists of aggregating multiple outputs made by a diverse set of predictors to obtain better results. The purpose of these methods is to average out the outcome of individual predictions by diversifying the set of predictors, thus lowering the variance, to arrive at a powerful prediction model that reduces overfitting the training set.\n\n> The random forest is an ensemble of Decision Trees (weak learners). They are trained via the bagging method. Bagging or Bootstrap Aggregating, consists of randomly sampling subsets of the training data, fitting a model to these smaller data sets, and aggregating the predictions. This method allows several instances to be used repeatedly for the training stage given that we are sampling with replacement. Tree bagging consists of sampling subsets of the training set, fitting a Decision Tree to each, and aggregating their result.\nRandom forest introduces more randomness by applying the bagging method to the feature space. Instead of searching greedily for the best predictors to create branches, it randomly samples the elements of the predictor space, thus adding more diversity and reducing the variance of the trees at the cost of equal or higher bias (The Variance-Bias Trade-off). This process is known as \"feature bagging\". [Random Forest Source](http:\/\/www.kdnuggets.com\/2017\/10\/random-forests-explained.html)","65e41bea":"Is there a difference between the train and the test data sets when it comes to the first active month?. We see that in the last month the counts for the data decline.The test dataframe has less values for first_active_month. This probably because it also has less records.","965fa5ec":"**RidgeCV**","b1a37545":"There are many transactions with a purchase_amount between -1 and 0 (scaled value?).","f51f0002":"Most common values for installments are 0 and 1.","e32a01df":"**LassoCV**","939a6d68":"**Historical Transactions**"}}