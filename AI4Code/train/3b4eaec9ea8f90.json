{"cell_type":{"fc4ccc70":"code","76c9ef59":"code","c52e331f":"code","193e73ae":"code","ad36ac04":"code","ab44b81c":"code","deee0558":"code","b60e71cd":"code","72c1dad3":"code","bc4d648b":"code","4485ef6a":"code","2f67c003":"code","872c2b27":"code","770b6a4a":"code","db1127da":"code","5bd44b40":"code","c00dc3dc":"code","b734c71e":"code","35580cff":"code","8f5688dc":"code","5325fac1":"code","bfafd001":"code","2019b3db":"code","433526dc":"code","e831bb6d":"code","fe0a974f":"code","25939712":"code","a9c61c33":"code","2a8a94ba":"code","a68971f9":"code","17c47c6a":"code","ac4b718b":"code","473a7d00":"code","830d9c9d":"code","e5c279f1":"code","7ca3800c":"code","4f85b9e1":"code","ead703ff":"code","7a6630e6":"code","f6e2a40d":"code","b2e3fe29":"code","c3995d74":"code","20dd8aa7":"code","f30afec7":"code","ee146029":"code","32d8e32b":"code","6be9cf1e":"code","0b40d326":"code","7221f1ac":"code","7d13f1fc":"code","9017de59":"code","f7b60496":"code","22586b17":"code","89d196f9":"code","f53b02f6":"code","9e06f127":"code","aa8d9f7b":"code","d54c006c":"code","5f954516":"code","2df8479e":"code","3ba70e1a":"code","89c5f469":"code","93c2dde8":"code","0877f8e8":"code","3fbf9daa":"code","928b0b8f":"code","c508ca16":"code","fa67d4c7":"code","21dc323a":"code","ecbe0500":"code","4ad42d3a":"code","4cfb8c7d":"code","da1a486e":"code","90f62af6":"code","84f0898b":"code","5e018c50":"code","51680494":"code","dd239500":"code","489b5173":"code","3444b1d9":"code","1818649d":"code","47f301ce":"code","345f494b":"code","11470c0b":"code","91c373e8":"code","1641e8ee":"code","69109de3":"code","beff158f":"code","f832a38a":"code","9def82a7":"code","e3709b6e":"code","cc10c6b0":"code","b3eb1f8b":"code","86a35c43":"code","28e9f76f":"code","cabd5428":"code","88846a3d":"code","9ffd0543":"code","1be8b8ad":"code","4b75e578":"code","4f9a7894":"code","af3a071e":"code","e8608b2d":"code","6fe59df4":"code","58e3e0d5":"code","c27f3d70":"code","323cf14b":"code","b8586a73":"code","e2016e61":"code","7bb33ec9":"code","21d5d012":"code","de69d393":"code","05826c32":"code","59bed1b1":"code","45491289":"code","12b58063":"code","8d099cad":"code","ddd1b91b":"code","ec200b73":"code","79897132":"code","6344c8e5":"code","e704411d":"code","8fa24fdf":"code","13b7bdee":"code","0750e3b8":"code","8e8e38ba":"code","946dcd19":"code","ee90c84f":"code","f2077c5f":"code","a10f9f5e":"code","7a7a743e":"code","1092beff":"code","d90e9f82":"code","8cfa7881":"code","59d9c560":"code","678405a3":"code","63cdbc0f":"code","ea75bd91":"code","81c11ed0":"code","e9f3cbb0":"code","0f194e1f":"code","ab5bffb2":"code","66848146":"code","62e8fb24":"code","48ca871d":"code","b228283c":"code","6ffef13a":"code","2db10f53":"code","f47d7a70":"code","e2c76d28":"code","41819c7c":"code","a72619f8":"code","b484d49d":"code","5f259342":"code","7dc01c50":"code","752d119d":"code","2c30114a":"code","b966f2e8":"code","3f799a80":"code","ef9fba8e":"code","d1d940ed":"code","4e1375de":"code","e6a3f288":"code","ce3293b4":"code","f7dec8c3":"code","c86e3469":"code","6d72c473":"code","f1a550e5":"code","d189d2d8":"code","ba3c2f83":"code","bc91f4d6":"code","0addd61f":"markdown","65ba31b1":"markdown","3d1a183b":"markdown","5bef41c6":"markdown","1064726f":"markdown","771a4957":"markdown","f815ade9":"markdown","7c7b07db":"markdown","afc217c0":"markdown","36f57196":"markdown","02191791":"markdown","2a6b9518":"markdown","c9a1b3ed":"markdown","e8db6830":"markdown","2a8bb9ab":"markdown","790196f6":"markdown","6db75cf9":"markdown","e735c322":"markdown","5aff1119":"markdown","95985a86":"markdown","2a53e4cc":"markdown","87eef2c8":"markdown","01ddef41":"markdown","471cc9fc":"markdown","b860af85":"markdown","75ecd17a":"markdown","7d87175e":"markdown","45b84947":"markdown","f841087f":"markdown","c4451ed7":"markdown","969a5d32":"markdown","29d91c15":"markdown","82cdb3d5":"markdown","05608ae0":"markdown","aeb8d707":"markdown","3d18d359":"markdown","74c9a345":"markdown","45ec0e8f":"markdown","9a1e767c":"markdown","3505ec3d":"markdown","66ebda42":"markdown","8a437b57":"markdown","fae9d1e7":"markdown","c5f03a67":"markdown","38402a9a":"markdown"},"source":{"fc4ccc70":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","76c9ef59":"# !pip install pretrainedmodels\n\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\n!pip install fastai==1.0.52\nimport fastai\n\nfrom fastai import *\nfrom fastai.tabular import *\n\n# from torchvision.models import *\n# import pretrainedmodels\n\nfrom utils import *\nimport sys\n\nfrom fastai.callbacks.hooks import *\n\nfrom fastai.callbacks.tracker import EarlyStoppingCallback\nfrom fastai.callbacks.tracker import SaveModelCallback","c52e331f":"import pandas as pd \nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nfrom sklearn.manifold import TSNE\n\n\nimport gc\nfrom datetime import datetime \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn import svm\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport xgboost as xgb\n\nfrom scipy.special import erfinv\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.utils.data import *\nfrom torch.optim import *\nfrom fastai.tabular import *\nimport torch.utils.data as Data\nfrom fastai.basics import *\nfrom fastai.callbacks.hooks import *\nfrom tqdm import tqdm_notebook as tqdm","193e73ae":"from sklearn.metrics import roc_auc_score\n\ndef auroc_score(input, target):\n    input, target = input.cpu().numpy()[:,1], target.cpu().numpy()\n    return roc_auc_score(target, input)\n\nclass AUROC(Callback):\n    _order = -20 #Needs to run before the recorder\n\n    def __init__(self, learn, **kwargs): self.learn = learn\n    def on_train_begin(self, **kwargs): self.learn.recorder.add_metric_names(['AUROC'])\n    def on_epoch_begin(self, **kwargs): self.output, self.target = [], []\n    \n    def on_batch_end(self, last_target, last_output, train, **kwargs):\n        if not train:\n            self.output.append(last_output)\n            self.target.append(last_target)\n                \n    def on_epoch_end(self, last_metrics, **kwargs):\n        if len(self.output) > 0:\n            output = torch.cat(self.output)\n            target = torch.cat(self.target)\n            preds = F.softmax(output, dim=1)\n            metric = auroc_score(preds, target)\n            return add_metrics(last_metrics, [metric])","ad36ac04":"def to_gauss(x): return np.sqrt(2)*erfinv(x)  #from scipy\n\ndef normalize(data, exclude=None):\n    # if not binary, normalize it\n    norm_cols = [n for n, c in data.drop(exclude, 1).items() if len(np.unique(c)) > 2]\n    n = data.shape[0]\n    for col in norm_cols:\n        sorted_idx = data[col].sort_values().index.tolist()# list of sorted index\n        uniform = np.linspace(start=-0.99, stop=0.99, num=n) # linsapce\n        normal = to_gauss(uniform) # apply gauss to linspace\n        normalized_col = pd.Series(index=sorted_idx, data=normal) # sorted idx and normalized space\n        data[col] = normalized_col # column receives its corresponding rank\n    return data","ab44b81c":"df_all = pd.read_csv('..\/input\/creditcard.csv')","deee0558":"df_all.shape","b60e71cd":"df_all.columns","72c1dad3":"df_all['Class'].value_counts()","bc4d648b":"plt.figure(figsize=(12,12))\nsns.countplot(df_all['Class']).set_title('Dist of Class variables')","4485ef6a":"df_all.head()","2f67c003":"df_all.describe()","872c2b27":"class_0 = df_all.loc[df_all['Class'] == 0][\"Time\"]\nclass_1 = df_all.loc[df_all['Class'] == 1][\"Time\"]\n\nhist_data = [class_0, class_1]\ngroup_labels = ['Not Fraud', 'Fraud']\n\nfig = ff.create_distplot(hist_data, group_labels, show_hist=False, show_rug=False)\nfig['layout'].update(title='Credit Card Transactions Time Density Plot', xaxis=dict(title='Time [s]'))\niplot(fig, filename='dist_only')","770b6a4a":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\ns = sns.boxplot(ax = ax1, x=\"Class\", y=\"Amount\", hue=\"Class\",data=df_all, palette=\"PRGn\",showfliers=True)\ns = sns.boxplot(ax = ax2, x=\"Class\", y=\"Amount\", hue=\"Class\",data=df_all, palette=\"PRGn\",showfliers=False)\nplt.show();","db1127da":"tmp = df_all[['Amount','Class']].copy()\nclass_0 = tmp.loc[tmp['Class'] == 0]['Amount']\nclass_1 = tmp.loc[tmp['Class'] == 1]['Amount']\nclass_0.describe()","5bd44b40":"class_1.describe()","c00dc3dc":"plt.figure(figsize = (14,14))\nplt.title('Credit Card Transactions features correlation plot (Pearson)')\ncorr = df_all.corr()\nsns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,linewidths=.1,cmap=\"Reds\")\nplt.show()","b734c71e":"s = sns.lmplot(x='V20', y='Amount',data=df_all, hue='Class', fit_reg=True,scatter_kws={'s':2})\ns = sns.lmplot(x='V7', y='Amount',data=df_all, hue='Class', fit_reg=True,scatter_kws={'s':2})\nplt.show()","35580cff":"s = sns.lmplot(x='V2', y='Amount',data=df_all, hue='Class', fit_reg=True,scatter_kws={'s':2})\ns = sns.lmplot(x='V5', y='Amount',data=df_all, hue='Class', fit_reg=True,scatter_kws={'s':2})\nplt.show()\ngc.collect()","8f5688dc":"var = df_all.columns.values\n\ni = 0\nt0 = df_all.loc[df_all['Class'] == 0]\nt1 = df_all.loc[df_all['Class'] == 1]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(8,4,figsize=(16,28))\n\nfor feature in var:\n    i += 1\n    plt.subplot(8,4,i)\n    sns.kdeplot(t0[feature], bw=0.5,label=\"Class = 0\")\n    sns.kdeplot(t1[feature], bw=0.5,label=\"Class = 1\")\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","5325fac1":"non_fraud = df_all[df_all['Class'] == 0].sample(2000)\nfraud = df_all[df_all['Class'] == 1]\n\ndf = non_fraud.append(fraud).sample(frac=1).reset_index(drop=True)\nX = df.drop(['Class'], axis = 1).values\nY = df[\"Class\"].values","bfafd001":"from sklearn.manifold import TSNE\n\ndef tsne_plot(x1, y1, name=\"graph.png\"):\n    tsne = TSNE(n_components=2)\n    X_t = tsne.fit_transform(x1)\n\n    plt.figure(figsize=(12, 8))\n    plt.scatter(X_t[np.where(y1 == 0), 0], X_t[np.where(y1 == 0), 1], marker='o', color='g', linewidth='1', alpha=0.8, label='Non Fraud')\n    plt.scatter(X_t[np.where(y1 == 1), 0], X_t[np.where(y1 == 1), 1], marker='o', color='r', linewidth='1', alpha=0.8, label='Fraud')\n\n    plt.legend(loc='best');\n    plt.savefig(name);\n    plt.show();\n    \ntsne_plot(X, Y, \"original.png\")","2019b3db":"gc.collect()","433526dc":"df_train = df_all","e831bb6d":"idx = df_train.columns.values[0:30]","fe0a974f":"df_train['sum'] = df_train[idx].sum(axis=1)  \ndf_train['min'] = df_train[idx].min(axis=1)\ndf_train['max'] = df_train[idx].max(axis=1)\ndf_train['mean'] = df_train[idx].mean(axis=1)\ndf_train['std'] = df_train[idx].std(axis=1)\ndf_train['skew'] = df_train[idx].skew(axis=1)\ndf_train['kurt'] = df_train[idx].kurtosis(axis=1)\ndf_train['med'] = df_train[idx].median(axis=1)","25939712":"norm_data = normalize(df_train, exclude=['Class'])","a9c61c33":"df_train_new = norm_data.drop(['Class'], axis=1)\ncont_names = df_train_new.columns\ndep_var = 'Class'\nprocs = [FillMissing, Categorify]\ncat_names=[]","2a8a94ba":"data = (TabularList.from_df(norm_data, procs = procs, cont_names=cont_names, cat_names=cat_names)\n        .split_by_rand_pct(0.3, seed=42)\n        .label_from_df(cols=dep_var)\n        .databunch(bs=1024))","a68971f9":"# data.add_test(TabularList.from_df(df_test, cont_names=cont_names))","17c47c6a":"data.show_batch()","ac4b718b":"df_t = data.train_ds.inner_df\ndf_v = data.valid_ds.inner_df","473a7d00":"df = df_t.append(df_v, ignore_index=True)","830d9c9d":"X = df.drop(['Class'], axis=1).values\nY = df['Class'].values","e5c279f1":"df.Class.value_counts()","7ca3800c":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom scipy.spatial.distance import pdist\n\nimport numpy as np\nimport numpy as np\nfrom pandas import *\nimport matplotlib.pyplot as plt\n#from hcluster import pdist, linkage, dendrogram\nfrom numpy.random import rand\n\nX_ = df.T.values #Transpose values \nY_ = pdist(X_)\nZ_ = linkage(Y_)\n\nplt.figure(figsize=(24,24))\n#dendrogram(Z, labels = df.columns, orientation='bottom')\nfig = ff.create_dendrogram(Z_, labels=df.columns, color_threshold=1.5)\nfig.update_layout(width=1500, height=1000)\nfig.show()","4f85b9e1":"corr_df = pd.DataFrame(df.drop(\"Class\", axis=1).apply(lambda x: x.corr(df.Class)))\ncorr_df.columns = ['corr']\ncorr_df.sort_values(by='corr')","ead703ff":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nforest = SelectFromModel(ExtraTreesClassifier(bootstrap=True, criterion='gini', max_depth=16, max_features='auto',\n            max_leaf_nodes=None, min_impurity_decrease=0.0,\n            min_impurity_split=None, min_samples_leaf=7,\n            min_samples_split=9, min_weight_fraction_leaf=0.0,\n            n_estimators=300, n_jobs=1, oob_score=False, random_state=42,\n            verbose=0, warm_start=False), threshold='median')\n\nforest.fit(X, Y)","7a6630e6":"df_without_label = df.drop(['Class'], axis=1)\nselected_feat= df_without_label.columns[(forest.get_support())]","f6e2a40d":"print(selected_feat)","b2e3fe29":"importances = forest.estimator_.feature_importances_\n\ndata={'Feature_Name':df.drop(['Class'], axis=1).columns,\n      'Feature_Importance': importances\n     }\n\nfeature_df=pd.DataFrame(data)\n\nfeature_df.sort_values(by=['Feature_Importance'],ascending=False,inplace=True)\n\nfig, ax = plt.subplots(figsize=(15,25))\nsns.barplot(data=feature_df,y='Feature_Name',x='Feature_Importance')","c3995d74":"df = norm_data[selected_feat]\ndf['Class'] = norm_data.Class","20dd8aa7":"df.Class.value_counts()","f30afec7":"cont_names = ['V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12',\n       'V14', 'V16', 'V17', 'V18', 'V21', 'V27', 'min', 'med']\n\n\ndep_var = 'Class'\n\nprocs = [FillMissing, Categorify, Normalize]\n\ncat_names= []","ee146029":"data = (TabularList.from_df(df, procs = procs, cont_names=cont_names, cat_names=cat_names)\n        .split_by_rand_pct(0.3, seed=42)\n        .label_from_df(cols=dep_var)\n        .databunch(bs=1024))","32d8e32b":"from fastai.callbacks import *\n\nlearn = tabular_learner(data, layers=[200,100], metrics=accuracy,  ps=[0.2, 0.1], \n                        emb_drop=0.04)","6be9cf1e":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","0b40d326":"lr = 1e-2\nlearn.fit_one_cycle(3, max_lr=lr,  pct_start=0.3, wd = 0.3)","7221f1ac":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","7d13f1fc":"lr=1e-5\nlearn.fit_one_cycle(3, max_lr=lr,  pct_start=0.3, wd = 0.2)","9017de59":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","f7b60496":"lr=1e-6\nlearn.fit_one_cycle(3, max_lr=lr,  pct_start=0.3, wd = 0.3)","22586b17":"learn.recorder.plot_losses()","89d196f9":"learn.save('1st-round')\nlearn.load('1st-round')","f53b02f6":"interp = ClassificationInterpretation.from_learner(learn)\n\nlosses,idxs = interp.top_losses()\n\nlen(data.valid_ds)==len(losses)==len(idxs)","9e06f127":"interp.plot_confusion_matrix(figsize=(8,8), dpi=60)","aa8d9f7b":"gc.collect()","d54c006c":"class SaveFeatures():\n    features=None\n    def __init__(self, m): \n        self.hook = m.register_forward_hook(self.hook_fn)\n        self.features = None\n    def hook_fn(self, module, input, output): \n        out = output.detach().cpu().numpy()\n        if isinstance(self.features, type(None)):\n            self.features = out\n        else:\n            self.features = np.row_stack((self.features, out))\n    def remove(self): \n        self.hook.remove()","5f954516":"sf = SaveFeatures(learn.model.layers[4])","2df8479e":"_= learn.get_preds(data.train_ds)","3ba70e1a":"label = [data.classes[x] for x in (list(data.train_ds.y.items))]","89c5f469":"len(label)","93c2dde8":"df_new = pd.DataFrame({'label': label})","0877f8e8":"array = np.array(sf.features)","3fbf9daa":"x=array.tolist()","928b0b8f":"df_new['img_repr'] = x","c508ca16":"del df_train; gc.collect()","fa67d4c7":"d2 = pd.DataFrame(df_new.img_repr.values.tolist(), index = df_new.index).rename(columns = lambda x: 'img_repr{}'.format(x+1))","21dc323a":"df_new_2 = df_new.join(d2)","ecbe0500":"del d2; gc.collect()","4ad42d3a":"df_new_2.drop(['img_repr'], axis=1, inplace=True)","4cfb8c7d":"non_fraud = df_new_2[df_new_2['label'] == 0].sample(2000)\nfraud = df_new_2[df_new_2['label'] == 1]\n\ndf = non_fraud.append(fraud).sample(frac=1).reset_index(drop=True)\nX = df.drop(['label'], axis = 1).values\nY = df[\"label\"].values","da1a486e":"tsne_plot(X, Y, \"original.png\")","90f62af6":"# df_new_2.drop(['img_repr'], axis=1, inplace=True)\n\n# from imblearn.over_sampling import SMOTE\n# from imblearn.combine import SMOTETomek\n\n# sm = SMOTE(ratio='minority', random_state=42)\n# df_resampled, y_resampled = sm.fit_sample(df_new_2, df_new_2['label'])\n# df_resampled = pd.DataFrame(df_resampled, columns = df_new_2.columns)\n# df_new_2['label'].mean(), df_resampled['label'].mean()","84f0898b":"# df_new_2 = df_resampled","5e018c50":"# del df_resampled; gc.collect()","51680494":"sf = SaveFeatures(learn.model.layers[4])","dd239500":"_=learn.get_preds(DatasetType.Valid)","489b5173":"label = [data.classes[x] for x in (list(data.valid_ds.y.items))]","3444b1d9":"df_new_valid = pd.DataFrame({'label': label})","1818649d":"array = np.array(sf.features)","47f301ce":"x=array.tolist()","345f494b":"df_new_valid['img_repr'] = x","11470c0b":"d2 = pd.DataFrame(df_new_valid.img_repr.values.tolist(), index = df_new_valid.index).rename(columns = lambda x: 'img_repr{}'.format(x+1))","91c373e8":"df_new_valid_2 = df_new_valid.join(d2)","1641e8ee":"del d2; \ndel sf.features\ngc.collect()","69109de3":"df_new_valid_2.drop(['img_repr'], axis=1, inplace=True)","beff158f":"non_fraud = df_new_valid_2[df_new_valid_2['label'] == 0].sample(1000)\nfraud = df_new_valid_2[df_new_valid_2['label'] == 1]\n\ndf = non_fraud.append(fraud).sample(frac=1).reset_index(drop=True)\nX = df.drop(['label'], axis = 1).values\nY = df[\"label\"].values","f832a38a":"tsne_plot(X, Y, 'original_png')","9def82a7":"gc.collect()","e3709b6e":"X = df_new_2\ny = df_new_2.label.copy()\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify = y, random_state=42)","cc10c6b0":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","b3eb1f8b":"X_train = X_train.drop(\"label\", axis =1)\ny_train = y_train\n\nX_test = X_test.drop(\"label\", axis =1)\ny_test = y_test","86a35c43":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","28e9f76f":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, attributes_names):\n        self.attributes_names = attributes_names\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        return X[self.attributes_names].values","cabd5428":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# numerical pipeline\n\nnum_pipeline = Pipeline([\n    \n    ('select_data', DataFrameSelector(X_train.columns)),\n    ('Std_Scaler', StandardScaler())\n])\n\nX_train_transformed = num_pipeline.fit_transform(X_train)\nX_test_transformed = num_pipeline.fit_transform(X_test)","88846a3d":"X_train_transformed.shape, X_test_transformed.shape","9ffd0543":"from sklearn.ensemble import RandomForestClassifier\nimport time\n\nstart = time.time()\n\nrf_clf = ExtraTreesClassifier(bootstrap=True, criterion='gini', max_depth=20, max_features='auto',\n            max_leaf_nodes=None, min_impurity_decrease=0.0,\n            min_impurity_split=None, min_samples_leaf=29,\n            min_samples_split=7, min_weight_fraction_leaf=0.0,\n            n_estimators=95, n_jobs=1, oob_score=False, random_state=42,\n            verbose=0, warm_start=False)\n\nrf_clf.fit(X_train_transformed, y_train)\n\nend = time.time()\n\nprint(\"run_time:\", (end-start)\/(60*60))","1be8b8ad":"from sklearn.model_selection import cross_val_predict\n\nimport time\n\nstart = time.time()\n\ny_train_pred_rf = cross_val_predict(rf_clf, X_train_transformed, y_train, cv=3, verbose=5)\n\nend = time.time()\n\nprint(\"run_time:\", (end-start)\/(60*60))","4b75e578":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_train, y_train_pred_rf)","4f9a7894":"type(y_train_pred_rf), type(X_train_transformed)","af3a071e":"X = pd.DataFrame(X_train_transformed)\nX['label'] = y_train_pred_rf","e8608b2d":"non_fraud = X[X['label'] == 0].sample(1000)\nfraud = X[X['label'] == 1]\n\ndf = non_fraud.append(fraud).sample(frac=1).reset_index(drop=True)\nX = df.drop(['label'], axis = 1).values\nY = df[\"label\"].values","6fe59df4":"tsne_plot(X, Y, 'original_png')","58e3e0d5":"from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, classification_report, cohen_kappa_score\n\nprint(precision_score(y_train, y_train_pred_rf))\nprint(recall_score(y_train, y_train_pred_rf))\nprint(f1_score(y_train, y_train_pred_rf))\nprint(cohen_kappa_score(y_train, y_train_pred_rf))\n\nprint(classification_report(y_train, y_train_pred_rf))","c27f3d70":"# import scipy.stats as st\n# from sklearn.model_selection import RandomizedSearchCV\n\n# one_to_left = st.beta(10, 1)  \n# from_zero_positive = st.expon(0, 50)\n\n# params = {  \n#     \"n_estimators\": st.randint(50, 300),\n#     \"max_depth\": st.randint(3, 40),\n#    \"min_samples_leaf\": st.randint(3, 40),\n#     \"min_samples_split\": st.randint(3, 20),\n#     'max_features': ['auto', 0.2, 0.5]\n# }\n\n# gs = RandomizedSearchCV(rf_clf, params, cv=3)","323cf14b":"# gs.fit(X_train_transformed, y_train)  ","b8586a73":"# gs.best_params_","e2016e61":"from sklearn.model_selection import cross_val_predict\n\ny_probas_rf = cross_val_predict(rf_clf, X_train_transformed, y_train, cv=3, method=\"predict_proba\", verbose=0)\ny_scores_rf = y_probas_rf[:,1]","7bb33ec9":"roc_score_rf = roc_auc_score(y_train, y_scores_rf)\nroc_score_rf","21d5d012":"from sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, thresholds = precision_recall_curve(y_train, y_scores_rf)\n\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label = \"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"r-\", label = \"Recall\")\n    plt.xlabel(\"Thresholds\")\n    plt.legend(loc=\"upper left\")\n    \nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nplt.show()","de69d393":"from sklearn.metrics import roc_curve\n\nfpr_rf, tpr_rf, threshold_rf = roc_curve(y_train, y_scores_rf)\n\ndef plot_roc_curve(fpr_rf, tpr_rf, figsize=(15,12), label = None):\n    plt.plot(fpr_rf, tpr_rf, linewidth = 2, label = label)\n    plt.plot([0,1], [0,1], \"k--\")\n    plt.axis([0,1,0,1])\n    plt.xlabel(\"False Pos Rate\")\n    plt.ylabel('True Neg Rate')\n    \n    \nplot_roc_curve(fpr_rf, tpr_rf)","05826c32":"plt.plot(thresholds, recalls[1:], 'b', label='Threshold-Recall curve')\nplt.title('Recall for different threshold values')\nplt.xlabel('Thresholds')\nplt.ylabel('Recall')\nplt.show()","59bed1b1":"y_pred_test_rf = rf_clf.predict(X_test_transformed)","45491289":"confusion_matrix(y_test, y_pred_test_rf)","12b58063":"X = pd.DataFrame(X_test_transformed)\nX['label'] = y_pred_test_rf","8d099cad":"non_fraud = X[X['label'] == 0].sample(2000)\nfraud = X[X['label'] == 1]\n\ndf = non_fraud.append(fraud).sample(frac=1).reset_index(drop=True)\nX = df.drop(['label'], axis = 1).values\nY = df[\"label\"].values","ddd1b91b":"tsne_plot(X, Y, 'org_png')","ec200b73":"from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, classification_report, cohen_kappa_score\n\nprint(precision_score(y_test, y_pred_test_rf))\nprint(recall_score(y_test, y_pred_test_rf))\nprint(f1_score(y_test, y_pred_test_rf))\nprint(cohen_kappa_score(y_test, y_pred_test_rf))\n\nprint(classification_report(y_test, y_pred_test_rf))","79897132":"y_probas_rf = rf_clf.predict_proba(X_test_transformed)\ny_scores_rf = y_probas_rf[:,1]\nroc_score_rf = roc_auc_score(y_test, y_scores_rf)\nroc_score_rf","6344c8e5":"from sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, thresholds = precision_recall_curve(y_test, y_scores_rf)\n\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label = \"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"r-\", label = \"Recall\")\n    plt.xlabel(\"Thresholds\")\n    plt.legend(loc=\"upper left\")\n    \nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nplt.show()","e704411d":"from sklearn.metrics import roc_curve\n\nfpr_rf, tpr_rf, threshold_rf = roc_curve(y_test, y_scores_rf)\n\ndef plot_roc_curve(fpr_rf, tpr_rf, figsize=(15,12), label = None):\n    plt.plot(fpr_rf, tpr_rf, linewidth = 2, label = label)\n    plt.plot([0,1], [0,1], \"k--\")\n    plt.axis([0,1,0,1])\n    plt.xlabel(\"False Pos Rate\")\n    plt.ylabel('True Neg Rate')\n      \nplot_roc_curve(fpr_rf, tpr_rf)","8fa24fdf":"X = df_new_valid_2\ny = df_new_valid_2.label.copy()","13b7bdee":"X_val = X.drop(\"label\", axis =1)\ny_val = y","0750e3b8":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# numerical pipeline\n\nnum_pipeline = Pipeline([\n    \n    ('select_data', DataFrameSelector(X_val.columns)),\n    ('Std_Scaler', StandardScaler())\n])\n\n\nX_val_transformed = num_pipeline.fit_transform(X_val)","8e8e38ba":"y_pred_test_rf_val = rf_clf.predict(X_val_transformed)","946dcd19":"confusion_matrix(y_val, y_pred_test_rf_val)","ee90c84f":"X = pd.DataFrame(X_val_transformed)\nX['label'] = y_pred_test_rf_val ","f2077c5f":"non_fraud = X[X['label'] == 0].sample(1000)\nfraud = X[X['label'] == 1]\n\ndf = non_fraud.append(fraud).sample(frac=1).reset_index(drop=True)\nX = df.drop(['label'], axis = 1).values\nY = df[\"label\"].values","a10f9f5e":"tsne_plot(X, Y, 'orig_ong')","7a7a743e":"gc.collect()","1092beff":"from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, classification_report, cohen_kappa_score\n\nprint(precision_score(y_val, y_pred_test_rf_val))\nprint(recall_score(y_val, y_pred_test_rf_val))\nprint(f1_score(y_val, y_pred_test_rf_val))\nprint(cohen_kappa_score(y_val, y_pred_test_rf_val))\n\nprint(classification_report(y_val, y_pred_test_rf_val))","d90e9f82":"y_probas_rf = rf_clf.predict_proba(X_val_transformed)\ny_scores_rf = y_probas_rf[:,1]\nroc_score_rf = roc_auc_score(y_val, y_scores_rf)\nroc_score_rf","8cfa7881":"from sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, thresholds = precision_recall_curve(y_val, y_scores_rf)\n\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label = \"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"r-\", label = \"Recall\")\n    plt.xlabel(\"Thresholds\")\n    plt.legend(loc=\"upper left\")\n    \nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nplt.show()","59d9c560":"from sklearn.metrics import roc_curve\n\nfpr_rf, tpr_rf, threshold_rf = roc_curve(y_val, y_scores_rf)\n\ndef plot_roc_curve(fpr_rf, tpr_rf, figsize=(15,12), label = None):\n    plt.plot(fpr_rf, tpr_rf, linewidth = 2, label = label)\n    plt.plot([0,1], [0,1], \"k--\")\n    plt.axis([0,1,0,1])\n    plt.xlabel(\"False Pos Rate\")\n    plt.ylabel('True Neg Rate')\n      \nplot_roc_curve(fpr_rf, tpr_rf)","678405a3":"gc.collect()","63cdbc0f":"import lightgbm as lgb\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import StratifiedKFold\nwarnings.filterwarnings('ignore')","ea75bd91":"param = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.20,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.20,\n    'learning_rate': 1e-3,\n    'max_depth': 10,  \n    'metric':'auc',\n    'min_data_in_leaf': 30,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 20,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary', \n    'verbosity': 1,\n    'sub_feature':0.5\n}","81c11ed0":"features_train = [c for c in df_new_2.columns if c not in ['label']]\ntarget_train = df_new_2['label']","e9f3cbb0":"features_val = [c for c in df_new_valid_2.columns if c not in ['label']]\ntarget_val = df_new_valid_2['label']","0f194e1f":"scaler = StandardScaler()\ndf_new_2[features_train] = scaler.fit_transform(df_new_2[features_train])\ndf_new_valid_2[features_val] = scaler.transform(df_new_valid_2[features_val])","ab5bffb2":"trn_data = lgb.Dataset(df_new_2[features_train], label=target_train)\nval_data = lgb.Dataset(df_new_valid_2[features_val], label=target_val)\n\nnum_round=50000\nclf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data,val_data], verbose_eval=500, early_stopping_rounds = 2000)\npreds = clf.predict(df_new_valid_2[features_val], num_iteration=clf.best_iteration)   ","66848146":"for i in range(0,preds.shape[0]):\n    if preds[i]>=.5:\n        preds[i]=1\n    else:  \n        preds[i]=0","62e8fb24":"X = pd.DataFrame(df_new_valid_2)\nX['label'] = preds \nX.head()","48ca871d":"non_fraud = X[X['label'] == 0].sample(1000)\nfraud = X[X['label'] == 1]\n\ndf = non_fraud.append(fraud).sample(frac=1).reset_index(drop=True)\nX = df.drop(['label'], axis = 1).values\nY = df[\"label\"].values","b228283c":"tsne_plot(X, Y, 'lgb_png')","6ffef13a":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nplt.figure()\ncm = confusion_matrix(target_val, preds)\nlabels = ['0', '1']\nplt.figure(figsize=(8,6))\nsns.heatmap(cm, xticklabels = labels, yticklabels = labels, annot = True, fmt='d', cmap=\"Blues\", vmin = 0.2);\nplt.title('Confusion Matrix')\nplt.ylabel('True Class')\nplt.xlabel('Predicted Class')\nplt.show()","2db10f53":"from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, classification_report, cohen_kappa_score\n\nprint(precision_score(target_val, preds))\nprint(recall_score(target_val, preds))\nprint(f1_score(target_val, preds))\nprint(cohen_kappa_score(target_val, preds))\n\nprint(classification_report(target_val, preds))","f47d7a70":"norm_data.shape","e2c76d28":"X = norm_data\ny = norm_data.Class.copy()\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify = y, random_state=42)","41819c7c":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","a72619f8":"df_new_2.shape","b484d49d":"X_train = df_new_2[df_new_2.label == 0]\nX_train = df_new_2.drop(['label'], axis=1)\nX_test = df_new_valid_2.drop(['label'], axis=1)\ny_test = df_new_valid_2.label\n\nX_train = X_train.values\ny_train = y_train.values\nX_test = X_test.values\ny_test = y_test.values\nprint(y_test.size)","5f259342":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","7dc01c50":"class Autoencoder(nn.Module):\n    def __init__(self):\n        super(Autoencoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(X_train.shape[1], 20),\n            nn.Tanh(),\n            nn.Linear(20, 10),\n            nn.Tanh(),\n            nn.Linear(10, 5),\n            nn.LeakyReLU(),\n            )\n        \n        self.decoder = nn.Sequential(\n           nn.Linear(5, 10),\n           nn.Tanh(),\n           nn.Linear(10, 20),\n           nn.Tanh(),\n           nn.Linear(20, X_train.shape[1]),\n           nn.LeakyReLU()\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x","752d119d":"model = Autoencoder().double().cpu()","2c30114a":"num_epochs = 150\nminibatch_size = 32\nlearning_rate = 1e-3","b966f2e8":"import pandas as pd\nimport numpy as np\nimport pickle\n\nfrom torch.autograd import Variable\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\n\nimport torch.utils.data as data_utils\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pylab import rcParams\n\nfrom sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n                             roc_curve, recall_score, classification_report, f1_score,\n                             precision_recall_fscore_support)\n\nsns.set(style='whitegrid', palette='muted', font_scale=1.5)\n\nrcParams['figure.figsize'] = 14, 8","3f799a80":"train_loader = data_utils.DataLoader(X_train, batch_size=minibatch_size, shuffle=True)","ef9fba8e":"test_loader = data_utils.DataLoader(X_test, batch_size=1, shuffle=False)","d1d940ed":"criterion = nn.MSELoss()\noptimizer = torch.optim.Adadelta(\nmodel.parameters(), lr=learning_rate, weight_decay=1e-4)","4e1375de":"history = {}\nhistory['train_loss'] = []\nhistory['test_loss'] = []","e6a3f288":"for epoch in range(num_epochs):\n    h = np.array([])\n    for data in train_loader:\n#         print(type(data))\n#         data = Variable(data).cpu()\n#         print(type(data))\n        # ===================forward=====================\n        output = model(data)\n        loss = criterion(output, data)\n        h = np.append(h, loss.item())\n        \n        # ===================backward====================\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    # ===================log========================\n    mean_loss = np.mean(h)\n    print('epoch [{}\/{}], loss:{:.4f}'\n          .format(epoch + 1, num_epochs, mean_loss))\n    history['train_loss'].append(mean_loss)\n    \n\ntorch.save(model.state_dict(), '.\/credit_card_model.pth')","ce3293b4":"plt.plot(history['train_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.show()","f7dec8c3":"model","c86e3469":"pred_losses = {'pred_loss' : []}\nmodel.eval()\nwith torch.no_grad():\n    for data in test_loader:\n        inputs = data\n        outputs = model(inputs)\n        loss = criterion(outputs, inputs).data.item()\n        pred_losses['pred_loss'].append(loss)\n        \n        \nreconstructionErrorDF = pd.DataFrame(pred_losses)\nreconstructionErrorDF['Class'] = y_test","6d72c473":"reconstructionErrorDF.describe()","f1a550e5":"fig = plt.figure()\nax = fig.add_subplot(111)\nnormal_error_df = reconstructionErrorDF[(reconstructionErrorDF['Class']== 0) & (reconstructionErrorDF['pred_loss'] < 10)]\n_ = ax.hist(normal_error_df.pred_loss.values, bins=10)","d189d2d8":"fig = plt.figure()\nax = fig.add_subplot(111)\nfraud_error_df = reconstructionErrorDF[(reconstructionErrorDF['Class']== 1) ]\n_ = ax.hist(fraud_error_df.pred_loss.values, bins=10)","ba3c2f83":"fpr, tpr, thresholds = roc_curve(reconstructionErrorDF.Class, reconstructionErrorDF.pred_loss)\nroc_auc = auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.001, 1])\nplt.ylim([0, 1.001])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show();","bc91f4d6":"threshold = 2\nLABELS = [\"Normal\", \"Fraud\"]\n\ny_pred = [1 if e > threshold else 0 for e in reconstructionErrorDF.pred_loss.values]\nconf_matrix = confusion_matrix(reconstructionErrorDF.Class, y_pred)\nplt.figure(figsize=(12, 12))\nsns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\", \n            cmap=plt.cm.get_cmap('Blues'));\nplt.title(\"Confusion matrix\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()","0addd61f":"Let's see how does our data look like","65ba31b1":"We have created ROC AUC metrics which goes well with Fastai as a metric","3d1a183b":"As we can see that classes are all jumbled up. Our job is to classify Red ones and Green ones separately.","5bef41c6":"In this plot, we will plot Boxplots to see how does the credit card transactions amount varies between normal and fraudelent transactions","1064726f":"# Performance on Unseen Test","771a4957":"In below code snippets, we will create Fastai Learner and train the model","f815ade9":"# Performance on Valid Data","7c7b07db":"# Extra Trees Classifer","afc217c0":"# Embeddings for Valid Data","36f57196":"# Custom Metrics","02191791":"For our prediction task, we will only use those most important features and remove other unimportant features","2a6b9518":"# Data","c9a1b3ed":"Let's see how does precision score, recall score, F1 score, Cohen Kappa score look like","e8db6830":"# Plots","2a8bb9ab":"Let's calculate ROC AUC score and plot ROC AUC curve along with Precision Recall curve","790196f6":"Before we see which features are the most important ones, lets see correlation of each feature with target variable and dendrogram","6db75cf9":"Usual training through Fastai methods have resulted quite well. Out of around 136 fraudelent records, model is predicting 113 correctly.","e735c322":"Let's see how does the distribution of normal and fraudelent transactions look like","5aff1119":"Let's first import all required libraries","95985a86":"We can see that names of features are masked.","2a53e4cc":"After training using ET Classifier, our classification of Reds and Greens looks cleaner now.","87eef2c8":"# Loading Libraries and Data","01ddef41":"This is quite nice result.\n\nThere are two options for the overall objective:\n1. Either we want to reduce False Negatives i.e. cases which are normal but predicted as fraudelent OR\n2. We want to reduce False Positives i.e. cases which are fraudelent but predicted as normal transactions","471cc9fc":"Let's see how does our data look like now in 2D space after training through Fastai library. We can still see few Reds in the areas of Greens. Let's see if we can better this classification","b860af85":"Let's see how does our data look like in 2D space using TSNE","75ecd17a":"Now lets run ET Classifier to select top features of the data. Here, we have chosen threshold as Median","7d87175e":"Precision, Recall, F1 score, Cohen Kappa score on Test Data","45b84947":"Here we can see that V20 and V7 are heavily positively correlated with Amount\n\nand, V2 and V5 are heavily negatively correlated with Amount","f841087f":"We can also see the heatmap (correlation plot) of the data","c4451ed7":"Above are our top features which are contributing the most in the prediction task.\n\nNow, lets plot the feature importance","969a5d32":"This function will be used to normalize the data. Its called Rank - Gaussian Normalization technique. In very simple terms, for continious data in a column are sorted as per their values and ranks are determined. These ranks are then normalized using Gaussian distribution.\n\nI found this technique of normalizing the continious data in dataset really helpful.","29d91c15":"# Pytorch AutoEncoder","82cdb3d5":"# Introduction\n\nIn this Notebook, we will build a fraud detection model which classifies normal credit card transactions and fraudelent transactions separately.\n\nAs it is expected normal transactions dominates the data with less than only 1% of transactions as fraudelent.\n\nWe have only used Training data for the purpose of building classifier. In the training data, we have around 250k records whereas only around 400 data recorded as fraudelent. Most of the features are masked and normalized.\n\nWe will use various packages to solve this challenge but primarily, we will use Fastai and Sklearn libraries.\n\nIn this Notebook, we will use following approaches to build the classifier:\n\n* Using Fastai's Deep Learning Tabular Model\n* Using Pytorch's Hook functionality to extract the learnings from the trained model\n* Using Extra Trees Classifier to further train on these extracted learnings\n\nIn last, as an experiment, I used AutoEncoder method as well which performed really well.\n\nSo, let's dive in!","05608ae0":"In this plot, we can see for all features, how does the distribution of Normal and Fraudelent transactions look like","aeb8d707":"# Data preparation for Fastai","3d18d359":"# Fastai Hooks & Embeddings for Train data","74c9a345":"# Normalizing Data","45ec0e8f":"Let's see few plots.\n\nFirst plot is to see how does the plot of time looks like when it comes to normal and fraudelent transactions\n","9a1e767c":"In below code snippets, we will first extract the learnings from the second last layer of the model and retrain on that using Extra Trees Classifier to improve the accuracy of the model","3505ec3d":"Below is the Confusion Matrix","66ebda42":"This is how data looks like in 2D space for Validation data","8a437b57":"This is our data looks like in 2D space for unseen test dataset","fae9d1e7":"# Light GBM","c5f03a67":"Now, lets see how does our model perform on completely unseen validation data which our ET classifer never saw.","38402a9a":"Let's see the description of Normal and Fraudelent transactions"}}