{"cell_type":{"84ee0465":"code","fcb6d74e":"code","e08d820f":"code","636ec3cb":"code","e16d6791":"code","68482f36":"code","1509247c":"code","edcc3938":"code","f9c859e9":"code","bad42056":"code","3ee2e852":"code","0340fcb3":"code","76c9dd62":"code","4d56defb":"code","00722b2a":"code","8ce158b2":"code","2fe5bcaf":"code","6af86062":"code","220a1d69":"code","be3f4143":"code","44423595":"code","2280a613":"code","0a12db2d":"code","a811e6ec":"code","8438cf8e":"code","8699ecb9":"code","db8d26f9":"code","c771b7f1":"code","1f497af7":"markdown","7adf1912":"markdown","da5e65ec":"markdown","096be7d4":"markdown","1097cdc9":"markdown","76abb9af":"markdown"},"source":{"84ee0465":"import datetime\nimport numpy as np\nimport pandas as pd\nimport datatable as dt\npd.set_option(\"display.max_columns\", None, \"display.max_rows\", None)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates","fcb6d74e":"%%time\ntrain = dt.fread('..\/input\/g-research-crypto-forecasting\/train.csv').to_pandas()","e08d820f":"train = train.set_index('timestamp')\ntrain.head()","636ec3cb":"for i in range(14):\n    print(i, len(train[train.Asset_ID == i])%60)","e16d6791":"train = train.sort_index()\nind = train.index.unique()\ndef reindex(df):\n    df = df.reindex(range(ind[0],ind[-1]+60,60),method='nearest')\n    # fill null values with zeros for visualization\n    df = df.fillna(0).fillna(0)\n    return df\ntrain = train.groupby('Asset_ID').apply(reindex).reset_index(0, drop=True).sort_index()","68482f36":"# ensure dataframe is well padded\nfor i in range(14):\n    print(i, len(train[train.Asset_ID == i])%60)","1509247c":"# ensure there is no null values in the dataframe\ntrain.isnull().sum()","edcc3938":"train = train.reset_index()\n# sort dataframe by timestamp then by asset id for consistency\ntrain = train.sort_values(by=['timestamp', 'Asset_ID'])\n# convert date to datetime to enable extracting more useful info\ntrain['date'] = pd.to_datetime(train.timestamp, unit='s')\ntrain.drop(['timestamp'], axis=1, inplace=True)","f9c859e9":"# extract useful features from the datetime feature\ntrain['min_'] = pd.DatetimeIndex(train['date']).minute.astype('uint8')\ntrain['hr_'] = pd.DatetimeIndex(train['date']).hour.astype('uint8')\ntrain['day_'] = pd.DatetimeIndex(train['date']).day.astype('uint8')\ntrain['day_in_week'] = pd.DatetimeIndex(train['date']).day_of_week.astype('uint8')\ntrain['week'] = pd.DatetimeIndex(train['date']).week.astype('uint8')\ntrain['month'] = pd.DatetimeIndex(train['date']).month.astype('uint8')\ntrain['is_month_start'] = pd.DatetimeIndex(train['date']).is_month_start.astype(int).astype('uint8')\ntrain['is_month_end'] = pd.DatetimeIndex(train['date']).is_month_end.astype(int).astype('uint8')\ntrain['is_quarter_start'] = pd.DatetimeIndex(train['date']).is_quarter_start.astype(int).astype('uint8')\ntrain['is_quarter_end'] = pd.DatetimeIndex(train['date']).is_quarter_end.astype(int).astype('uint8')\ntrain['is_year_start'] = pd.DatetimeIndex(train['date']).is_year_start.astype(int).astype('uint8')\ntrain['is_year_end'] = pd.DatetimeIndex(train['date']).is_year_end.astype(int).astype('uint8')","bad42056":"# convert float columns to float64 to reduce memory usage\nfloat_cols = []\nfor col in train.columns.tolist():\n    if train[col].dtypes == 'float64':\n        float_cols.append(col)\n    \ntrain[float_cols] = train[float_cols].astype('float16')","3ee2e852":"# select only date from 2021 for computational simplicity\ndf = train[train['date'] >= datetime.datetime(2021, 1, 1)].set_index('date', drop=True)\nprint(df.shape)","0340fcb3":"# from the line plot we can extract info on trends and seasonality\ndf[df['Asset_ID'] == 1][['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target']].plot(subplots=True, figsize=(15, 25))\nplt.show()","76c9dd62":"# first method to smoothen the curves is by resampling (aggregating) data over a wider period of time and taking the mean of these values\ndf_month = df[df['Asset_ID'] == 1].resample(\"M\").mean()\n\nfig, axes = plt.subplots(8, 1, figsize=(15, 25), sharex=True)\nfor name, ax in zip(['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target'], axes):\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n    ax.bar(df_month.index, df_month[name], width=17, align='center')\n    ax.set_ylabel(\"\")\n    ax.set_title(name)\n    if ax != axes[-1]:\n        ax.set_xlabel('')","4d56defb":"# boxplots provides us with more precious information where some important stats as median, interquartile range, outliers can be \n# explored\nfig, axes = plt.subplots(8, 1, figsize=(15, 25), sharex=True)\nfor name, ax in zip(['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target'], axes):\n    sns.boxplot(data = df[df['Asset_ID'] == 1], x='month', y=name, ax=ax)\n    ax.set_ylabel(\"\")\n    ax.set_title(name)\n    if ax != axes[-1]:\n        ax.set_xlabel('')\nplt.savefig(\"fig_4.png\")","00722b2a":"# again we perform resampling and rolling, but this time on a daily basis which might provide more specific info\ndf_day = df[df['Asset_ID'] == 1].resample(\"D\").mean()\ndf_day = df_day.fillna(0)","8ce158b2":"# from the line plot we can extract info on daily trends and seasonality\ndf_day[df_day['Asset_ID'] == 1][['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target']].plot(subplots=True, figsize=(15, 25))\nplt.show()","2fe5bcaf":"# we still see a lot of spikes which are not helpful for modelling\n# let's smooth them down by resampling to a lower freq and rolling\ndf_week = df[df['Asset_ID'] == 1].resample(\"W\").mean()","6af86062":"df_week = df_week.astype('float64')\ndf_week = df_week.fillna(method='pad')","220a1d69":"# here we compare the daily trends and seasonality to the weekly ones\nfig, axes = plt.subplots(8, 1, figsize=(15, 25), sharex=True)\nfor name, ax in zip(['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target'], axes):\n    ax.plot(df_day[name], marker='.', linestyle='-', linewidth = 0.5, label='Daily', color='black')\n    ax.plot(df_week[name], marker='o', markersize=8, linestyle='-', label='Weekly', color='coral')\n    ax.set_ylabel(name)\n    ax.legend()\n    ax.set_ylabel(\"\")\n    ax.set_title(name)\n    if ax != axes[-1]:\n        ax.set_xlabel('')","be3f4143":"cols = ['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target']\nfor col in cols:\n    df_week[f'{col}_Change'] = df_week[col].div(df_week[col].shift())\n    \ndf_week = df_week.fillna(method='bfill')","44423595":"# plot the weekly change =====> thisWeekReading \/ previousWeekReading\n# this enables us to trace the trend of the ratio of these features. \n# wherever the value is greater than one, this means that this feature \n# has reported an increment from the previous week \ndf_week[df_week['Asset_ID'] == 1][[f'{col}_Change' for col in ['Count', 'Open', 'High', 'Low', 'Close', \n                                                               'Volume', 'VWAP', 'Target']]].plot(subplots=True, figsize=(15, 25))\nplt.show()","2280a613":"# in many cases percent change is informative\ncols = ['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target']\nfor col in cols:\n    df_week[f'{col}_pct_change'] = df_week[col].pct_change()*100\n    \ndf_week = df_week.fillna(method='bfill')","0a12db2d":"# plot the weekly change =====> thisWeekReading \/ previousWeekReading\ndf_week[df_week['Asset_ID'] == 1][[f'{col}_pct_change' for col in ['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target']]]\\\n                                                                    .plot(kind='bar', subplots=True, figsize=(15, 25))\nplt.show()","a811e6ec":"# here for each column we plot the original values versus the expanding mean (cumulative mean) versus the expanding std\n\ncols = ['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target']\nfor col in cols:\n    df_week[f'{col}_expanding_mean'] = df_week[col].expanding().mean()\n    df_week[f'{col}_expanding_std'] = df_week[col].expanding().std()\n\ndf_week = df_week.fillna(method='bfill')","8438cf8e":"# here for each column we plot the original values versus the expanding mean (cumulative mean) versus the expanding std\n\nfig, axes = plt.subplots(8, 1, figsize=(15, 25), sharex=False)\nfor i, (name, ax) in enumerate(zip(['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target'], axes)):\n    plt.subplot(8, 1, i+1)\n    ax = df_week[f'{name}'].plot(label=name)\n    ax = df_week[f'{name}_expanding_mean'].plot(label=f'{name} expanding mean')\n    ax = df_week[f'{name}_expanding_std'].plot(label=f'{name} expanding std')\n    ax.set_ylabel(name)\n    ax.legend()\n    ax.set_ylabel(\"\")\n    ax.set_title(name)\n    if ax != axes[-1]:\n        ax.set_xlabel('')\n\nplt.show()","8699ecb9":"df_hour = df[df['Asset_ID'] == 1].resample(\"H\").mean().astype('float64')\ndf_hour = df_hour.fillna(method='pad')\n\ndf_hour['day_'] = df_hour['day_'].astype(int)\ndf_hour['hr_'] = df_hour['hr_'].astype(int)","db8d26f9":"hour_day_df = pd.pivot_table(df_hour, \n                             values=\"Close\",\n                             index=['hr_'],\n                             columns=['day_'],\n                             fill_value=0,\n                             margins=True)","c771b7f1":"# this heat map enables us to keep track of hours of all days over all months and see which hours of day had the \n# the highest mean close value and which ones had the smallest values \nplt.figure(figsize=(15, 15))\nax = sns.heatmap(hour_day_df, cmap='RdYlGn_r', robust=True, fmt='.0f', \n                 annot=True, linewidths=.5, annot_kws={'size':9, 'rotation': 90}, \n                 cbar_kws={'shrink':.8, 'label':'Close'})                       \n    \nax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=10)\nax.set_xticklabels(ax.get_xticklabels(), rotation=0, fontsize=10)\nplt.title('Average Close', fontdict={'fontsize':18},    pad=14)\nplt.show()","1f497af7":"#### *We can see that on average the days with the high opening values take place during the half of the month, especially between the 9th and 17th day*  ","7adf1912":"#### *We can see how the Target and the features are extremely noisy and hence we will try to smoothen these curves using some techniques. This might enable us to extract valuable information from our data*","da5e65ec":"#### *In this section will display only the data of Bitcoin (Asset_ID = 1) for the sake of simplicity. You can also explore other assets.*","096be7d4":"# *Standard Libraries*","1097cdc9":"#### *These plots assures the high volatility of the data, where data statistics such as mean and standard deviation are varying over time.*","76abb9af":"# *EDA*"}}