{"cell_type":{"029a8975":"code","1e92410b":"code","1d2b28ec":"code","d05decb6":"code","a00f1836":"code","fd7b54fd":"code","212cf30e":"code","609b6eb1":"code","21e0aded":"markdown","d364d38c":"markdown"},"source":{"029a8975":"import pandas as pd\nfrom pathlib import Path\n\npath = Path('rossmann')\ntrain_df = pd.read_pickle('..\/input\/rossman-fastai-sample\/train_clean').drop(['index', 'Date'], axis='columns')\ntest_df = pd.read_pickle('..\/input\/rossman-fastai-sample\/test_clean')","1e92410b":"train_df.head()","1d2b28ec":"from sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nimport numpy as np\n\n\ncat_vars = [\n    'Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',\n    'Promo2Weeks', 'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear',\n    'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',\n    'SchoolHoliday_fw', 'SchoolHoliday_bw'\n]\ncont_vars = [\n    'CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',\n    'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', \n    'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n    'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday'\n]\ntarget_var= 'Sales'\n\n\nclass ColumnFilter:\n    def fit(self, X, y):\n        return self\n    \n    def transform(self, X):\n        return X.loc[:, cat_vars + cont_vars]\n        \n\nclass GroupLabelEncoder:\n    def __init__(self):\n        self.labeller = LabelEncoder()\n    \n    def fit(self, X, y):\n        self.encoders = {col: None for col in X.columns if col in cat_vars}\n        for col in self.encoders:\n            self.encoders[col] = LabelEncoder().fit(\n                X[col].fillna(value='N\/A').values\n            )\n        return self\n    \n    def transform(self, X):\n        X_out = []\n        categorical_part = np.hstack([\n            self.encoders[col].transform(X[col].fillna(value='N\/A').values)[:, np.newaxis]\n            for col in cat_vars\n        ])\n        return pd.DataFrame(categorical_part, columns=cat_vars)\n\n\n# class GroupOneHotEncoder:\n#     def __init__(self):\n#         self.ohe = OneHotEncoder(categories='auto', sparse=False)\n#\n#     def fit(self, X):\n#         self.encoders = {col: None for col in X.columns}\n#         for col in self.encoders:\n#             self.encoders[col] = OneHotEncoder(categories='auto', sparse=False).fit(\n#                 X[col].fillna(value='N\/A').values[:, np.newaxis]\n#             )\n#         return self\n#\n#     def transform(self, X):\n#         X_out = []\n#         return np.hstack(\n#             self.encoders[col].transform(X[col].fillna(value='N\/A').values[:, np.newaxis]) for\n#             col in X.columns\n#         )\n\n\nclass GroupNullImputer:\n    def fit(self, X, y):\n        return self\n        \n    def transform(self, X):\n        return X.loc[:, cont_vars].fillna(0)\n\n\nclass Preprocessor:\n    def __init__(self):\n        self.cf = ColumnFilter()\n        self.gne = GroupNullImputer()\n        \n    def fit(self, X, y=None):\n        self.gle = GroupLabelEncoder().fit(X, y=None)\n        return self\n    \n    def transform(self, X):\n        X_out = self.cf.transform(X)\n        X_out = np.hstack((self.gle.transform(X_out).values, self.gne.transform(X_out).values))\n        X_out = pd.DataFrame(X_out, columns=cat_vars + cont_vars)\n        return X_out\n\n\nX_train_sample = Preprocessor().fit(train_df).transform(train_df.loc[:999])\ny_train_sample = train_df[target_var].loc[:999]","d05decb6":"import torch\nfrom torch import nn\nimport torch.utils.data\n# ^ https:\/\/discuss.pytorch.org\/t\/attributeerror-module-torch-utils-has-no-attribute-data\/1666\n\n\nclass FeedforwardTabularModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.batch_size = 512\n        self.base_lr, self.max_lr = 0.001, 0.003\n        self.n_epochs = 5\n        self.cat_vars_embedding_vector_lengths = [\n            (1115, 80), (7, 4), (3, 3), (12, 6), (31, 10), (2, 2), (25, 10), (26, 10), (4, 3),\n            (3, 3), (4, 3), (23, 9), (8, 4), (12, 6), (52, 15), (22, 9), (6, 4), (6, 4), (3, 3),\n            (3, 3), (8, 4), (8, 4)\n        ]\n        self.loss_fn = torch.nn.MSELoss()\n        self.score_fn = torch.nn.MSELoss()\n        \n        # Layer 1: embeddings.\n        self.embeddings = []\n        for (in_size, out_size) in self.cat_vars_embedding_vector_lengths:\n            emb = nn.Embedding(in_size, out_size)\n            self.embeddings.append(emb)\n\n        # Layer 1: dropout.\n        self.embedding_dropout = nn.Dropout(0.04)\n        \n        # Layer 1: batch normalization (of the continuous variables).\n        self.cont_batch_norm = nn.BatchNorm1d(16, eps=1e-05, momentum=0.1)\n        \n        # Layers 2 through 9: sequential feedforward model.\n        self.seq_model = nn.Sequential(*[\n            # nn.Linear(in_features=215, out_features=1, bias=True)\n            nn.Linear(in_features=215, out_features=1000, bias=True),\n            nn.ReLU(),\n            nn.BatchNorm1d(1000, eps=1e-05, momentum=0.1),\n            nn.Dropout(p=0.001),\n            nn.Linear(in_features=1000, out_features=500, bias=True),\n            nn.ReLU(),\n            nn.BatchNorm1d(500, eps=1e-05, momentum=0.1),\n            nn.Dropout(p=0.01),\n            nn.Linear(in_features=500, out_features=1, bias=True)\n        ])\n    \n    \n    def forward(self, x):\n        # Layer 1: embeddings.\n        inp_offset = 0\n        embedding_subvectors = []\n        for emb in self.embeddings:\n            index = torch.tensor(inp_offset, dtype=torch.int64)\n            inp = torch.index_select(x, dim=1, index=index).long()\n            out = emb(inp)\n            out = out.view(out.shape[2], out.shape[0], 1).squeeze()\n            embedding_subvectors.append(out)\n            inp_offset += 1\n        out_cat = torch.cat(embedding_subvectors)\n        out_cat = out_cat.view(out_cat.shape[::-1])\n        \n        # Layer 1: dropout.\n        out_cat = self.embedding_dropout(out_cat)\n        \n        # Layer 1: batch normalization (of the continuous variables).\n        out_cont = self.cont_batch_norm(x[:, inp_offset:])\n        \n        out = torch.cat((out_cat, out_cont), dim=1)\n        \n        # Layers 2 through 9: sequential feedforward model.\n        out = self.seq_model(out)\n        \n        # TODO: debug.\n        # Weight instantiation.\n        # for name, param in model.named_parameters():\n        #     if 'bias' in name:\n        #          nn.init.constant_(param, 0.0)\n        #     elif 'batch_norm' in name:\n        #         pass\n        #     else:\n        #         nn.init.kaiming_normal_(param)\n            \n        return out\n        \n        \n    def fit(self, X, y):\n        self.train()\n        \n        # TODO: set a random seed to invoke determinism.\n        # cf. https:\/\/github.com\/pytorch\/pytorch\/issues\/11278\n\n        X = torch.tensor(X, dtype=torch.float32)\n        y = torch.tensor(y, dtype=torch.float32)\n        \n        # The build of PyTorch on Kaggle has a blog that prevents us from using\n        # CyclicLR with ADAM. Cf. GH#19003.\n        # optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n        # scheduler = torch.optim.lr_scheduler.CyclicLR(\n        #     optimizer, base_lr=base_lr, max_lr=max_lr,\n        #     step_size_up=300, step_size_down=300,\n        #     mode='exp_range', gamma=0.99994\n        # )\n        optimizer = torch.optim.Adam(model.parameters(), lr=(self.base_lr + self.max_lr) \/ 2)\n        batches = torch.utils.data.DataLoader(\n            torch.utils.data.TensorDataset(X, y),\n            batch_size=self.batch_size, shuffle=True\n        )\n        \n        for epoch in range(self.n_epochs):\n            for i, (X_batch, y_batch) in enumerate(batches):\n                y_pred = model(X_batch).squeeze()\n                # scheduler.batch_step()  # Disabled due to a bug, see above.\n                loss = self.loss_fn(y_pred, y_batch)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            print(\n                f\"Epoch {epoch + 1}\/{self.n_epochs}, Loss {loss.detach().numpy()}\"\n            )\n    \n    \n    def predict(self, X):\n        self.eval()\n        with torch.no_grad():\n            y_pred = model(torch.tensor(X, dtype=torch.float32))\n        return y_pred.squeeze()\n    \n    \n    def score(self, X, y):\n        y_pred = self.predict(X)\n        y = torch.tensor(y, dtype=torch.float32)\n        return self.score_fn(y, y_pred)","a00f1836":"model = FeedforwardTabularModel()\nmodel.cpu()\nmodel.fit(X_train_sample.values, y_train_sample.values)","fd7b54fd":"model.score(X_train_sample.values, y_train_sample.values)","212cf30e":"from sklearn.model_selection import KFold\n\ndef fit_cv():\n    preprocessor = Preprocessor().fit(train_df)\n    X = train_df.head(10000).drop('Sales', axis='columns')\n    y = train_df.head(10000).Sales\n    splits = KFold(n_splits=3, shuffle=True).split(X, y)\n    \n    for i, (train_idx, test_idx) in enumerate(splits):\n        X_train = preprocessor.transform(X.iloc[train_idx])\n        X_test = preprocessor.transform(X.iloc[test_idx])\n        y_train = y.iloc[train_idx]\n        y_test = y.iloc[test_idx]\n        \n        model = FeedforwardTabularModel()\n        model.fit(X_train.values, y_train.values)\n        \n        score = model.score(X_test.values, y_test.values)\n        print(f\"Achieved a score of {score} on cross-validation fold {i + 1}.\")","609b6eb1":"fit_cv()","21e0aded":"# PyTorch Experimentation\n\n## Replicating a linear fast.ai model\n\nReplicate the following model, taken from the Fast.AI Part 1 curriculum:\n\n```\nTabularModel(\n  (embeds): ModuleList(\n    (0): Embedding(1116, 81)\n    (1): Embedding(8, 5)\n    (2): Embedding(4, 3)\n    (3): Embedding(13, 7)\n    (4): Embedding(32, 11)\n    (5): Embedding(3, 3)\n    (6): Embedding(26, 10)\n    (7): Embedding(27, 10)\n    (8): Embedding(5, 4)\n    (9): Embedding(4, 3)\n    (10): Embedding(4, 3)\n    (11): Embedding(24, 9)\n    (12): Embedding(9, 5)\n    (13): Embedding(13, 7)\n    (14): Embedding(53, 15)\n    (15): Embedding(22, 9)\n    (16): Embedding(7, 5)\n    (17): Embedding(7, 5)\n    (18): Embedding(4, 3)\n    (19): Embedding(4, 3)\n    (20): Embedding(9, 5)\n    (21): Embedding(9, 5)\n    (22): Embedding(3, 3)\n    (23): Embedding(3, 3)\n  )\n  (emb_drop): Dropout(p=0.04, inplace=False)\n  (bn_cont): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layers): Sequential(\n    (0): Linear(in_features=233, out_features=1000, bias=True)\n    (1): ReLU(inplace=True)\n    (2): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Dropout(p=0.001, inplace=False)\n    (4): Linear(in_features=1000, out_features=500, bias=True)\n    (5): ReLU(inplace=True)\n    (6): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Dropout(p=0.01, inplace=False)\n    (8): Linear(in_features=500, out_features=1, bias=True)\n  )\n)\n```\n\nThe fast.ai model is constructed using the fast.ai library, including fast.ai preprocessing steps. For this exercise I will reverse-engineer this model in PyTorch. The objective is to ultimately get somewhat comparable performance, though my model will not include many of the tricks fast.ai inserts into things to improve their training results. The objective of this project is to acquire more familiarity with writing PyTorch models.\n\nThe resulting model is CPU-bound.","d364d38c":"## Data preprocessing"}}