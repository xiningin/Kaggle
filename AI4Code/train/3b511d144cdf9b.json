{"cell_type":{"03c0969d":"code","f20be735":"code","833f6e3d":"code","7d919552":"code","439470fa":"code","d9d4eb51":"code","843d8cb5":"code","9b186a0d":"code","4ceabb95":"code","8037c28a":"markdown","5b589e56":"markdown","beb07ada":"markdown","48119c05":"markdown","cf18c181":"markdown","4e34515d":"markdown"},"source":{"03c0969d":"#Load packages\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold","f20be735":"#Load data; drop target and ID's\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\n\ntrain.drop(train[['ID_code', 'target']], axis=1, inplace=True)\ntest.drop(test[['ID_code']], axis=1, inplace=True)","833f6e3d":"#Create label array and complete dataset\ny1 = np.array([0]*train.shape[0])\ny2 = np.array([1]*test.shape[0])\ny = np.concatenate((y1, y2))\n\nX_data = pd.concat([train, test])\nX_data.reset_index(drop=True, inplace=True)","7d919552":"#Initialize splits&LGBM\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=13)\n\nlgb_model = lgb.LGBMClassifier(max_depth=-1,\n                                   n_estimators=100,\n                                   learning_rate=0.1,\n                                   objective='binary', \n                                   n_jobs=-1)\n                                   \ncounter = 1","439470fa":"#Train 5-fold adversarial validation classifier\nfor train_index, test_index in skf.split(X_data, y):\n    print('\\nFold {}'.format(counter))\n    X_fit, X_val = X_data.loc[train_index], X_data.loc[test_index]\n    y_fit, y_val = y[train_index], y[test_index]\n    \n    lgb_model.fit(X_fit, y_fit, eval_metric='auc', \n              eval_set=[(X_val, y_val)], \n              verbose=100, early_stopping_rounds=10)\n    counter+=1","d9d4eb51":"#Load more packages\nfrom scipy.stats import ks_2samp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore')","843d8cb5":"#Perform KS-Test for each feature from train\/test. Draw its distribution. Count features based on statistics.\n#Plots are hidden. If you'd like to look at them - press \"Output\" button.\nhypothesisnotrejected = []\nhypothesisrejected = []\n\nfor col in train.columns:\n    statistic, pvalue = ks_2samp(train[col], test[col])\n    if pvalue>=statistic:\n        hypothesisnotrejected.append(col)\n    if pvalue<statistic:\n        hypothesisrejected.append(col)\n        \n    plt.figure(figsize=(8,4))\n    plt.title(\"Kolmogorov-Smirnov test for train\/test\\n\"\n              \"feature: {}, statistics: {:.5f}, pvalue: {:5f}\".format(col, statistic, pvalue))\n    sns.kdeplot(train[col], color='blue', shade=True, label='Train')\n    sns.kdeplot(test[col], color='green', shade=True, label='Test')\n\n    plt.show()","9b186a0d":"len(hypothesisnotrejected), len(hypothesisrejected)","4ceabb95":"print(hypothesisrejected)","8037c28a":"As we can see, 185 features successfully passed Kolmogorov-Smirnov test. We cannot reject null hypothesis that those features in train and test sets came from the same distribution. 15 features haven't passed this test and probably require our attention.","5b589e56":"### Kolmogorov-Smirnov Test","beb07ada":"Average AUC across folds is stable and concentrates around 0.5. It means that we can hardly distinguish train set from test set using adversarial validation.\n\nNow let's expand our investigation of dataset and look at distribution of features in train and test sets with respect to [Kolmogorov-Smirnov test](https:\/\/en.wikipedia.org\/wiki\/Kolmogorov%E2%80%93Smirnov_test).","48119c05":"## Conslusion:\n\nFrom adversarial validation we have no evidence that train and test sets come from different distributions. AUC around 0.50 states that LGBM can hardly distinguish train observations from test. These datasets are quite similar. Local validation schemas and public LB track should correctly reflect your efforts in this competition.\n\nFrom Kolmogorov-Smirnov Test we can also state that both sets are quite similar. Hypothesis that samples are drawn from the same distribution can be rejected only for 15 out of 200 features based on KS-Test. Probably, we should pay more attention to those 15 features.","cf18c181":"## Abstract\n\nThe aim of this notebook is to check whether train and test sets are significantly different. Can we trust our local validation schemas and public LB? I'll use adversarial validation and Kolmogorov-Smirnov Test for these purposes.","4e34515d":"### Adversarial Validation"}}