{"cell_type":{"90fe9048":"code","46c86d08":"code","30792941":"code","a8aec3c9":"code","34117767":"code","b6dd0a73":"code","768c4096":"code","7a738bb5":"code","77c9ed82":"code","667e0753":"code","9dc86ca8":"code","48f749f3":"code","5079f01c":"markdown","53aee2f3":"markdown","e431c32b":"markdown","0ae07a6a":"markdown","0e0589f5":"markdown","3ea2fbf8":"markdown","3a31e8b5":"markdown","bec4b467":"markdown","e4345c8c":"markdown","c6559e56":"markdown","7d674eab":"markdown","47b2095f":"markdown"},"source":{"90fe9048":"import tensorflow as tf\nprint(tf.__version__)","46c86d08":"import os\nbase_dir = '\/kaggle\/input\/rockpaperscissors\/rps-cv-images'\nprint(os.listdir(base_dir))\n","30792941":"rock_dir = os.path.join(base_dir,'rock')\npaper_dir = os.path.join(base_dir,'paper')\nscissors_dir = os.path.join(base_dir,'scissors')","a8aec3c9":"print(\"Rock : \",len(os.listdir(rock_dir)))\nprint(\"Paper : \",len(os.listdir(paper_dir)))\nprint(\"Scissors : \",len(os.listdir(scissors_dir)))","34117767":"# define data augmentation configuration\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nbatch_size=32\nimg_rows,img_cols=120,120\nnum_classes=3\n\ndatagen = ImageDataGenerator(\n        rescale=1\/255.0,\n        zoom_range=0.25,\n        rotation_range=10,\n        horizontal_flip=True,\n        vertical_flip=True,\n        fill_mode='nearest',\n        validation_split=0.4)\n\n# setup generator\ntrain_generator = datagen.flow_from_directory(\n        base_dir,\n        target_size=(img_rows,img_cols),\n        batch_size=batch_size,\n        class_mode='categorical',\n        color_mode='rgb',\n        subset='training')\nvalidation_generator = datagen.flow_from_directory(\n        base_dir,\n        target_size=(img_rows,img_cols),\n        batch_size=batch_size,\n        class_mode='categorical',\n        color_mode='rgb',\n        subset='validation')\n","b6dd0a73":"from tensorflow.keras import Input\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense","768c4096":"# Create a Sequential model by passing a list of layers to the Sequential constructor\nfrom tensorflow.keras.layers import Dropout\nimg_rows,img_cols=120,120\nmodel = Sequential([Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu', \n                           input_shape=(img_rows,img_cols,3)),\n                    MaxPooling2D(pool_size=(2,2)),\n                    Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'),\n                    MaxPooling2D(pool_size=(2,2)),\n                    Dropout(rate=0.25),\n                    Conv2D(filters=64, kernel_size=(3,3), padding='same',activation='relu'),\n                    MaxPooling2D(pool_size=(2,2)),\n                    Conv2D(filters=128, kernel_size=(3,3), padding='same',activation='relu'),\n                    MaxPooling2D(pool_size=(2,2)),\n                    Dropout(rate=0.25),\n                    Flatten(),\n                    Dropout(rate=0.5),\n                    Dense(units=512, activation='relu'),\n                    Dense(units=num_classes, activation='softmax')])","7a738bb5":"print(model.summary())","77c9ed82":"accuracythreshold=96e-2\n\nclass AccCallback(tf.keras.callbacks.Callback): \n    def on_epoch_end(self, epoch, logs={}): \n        if(logs.get('accuracy') >= accuracythreshold):   \n          print(\"\\nReached %2.2f%% accuracy, stop training!\" %(accuracythreshold*100))   \n          self.model.stop_training = True","667e0753":"# Compile the Model\nfrom tensorflow.keras.optimizers import RMSprop\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=RMSprop(),\n              metrics=['accuracy'])\n","9dc86ca8":"nb_train_samples=1314\nnb_validation_samples=874\nepochs=55\n\nhistory=model.fit(\n    train_generator,\n    steps_per_epoch=nb_train_samples \/\/ batch_size,\n    epochs=epochs,\n    callbacks=[AccCallback()],\n    validation_data=validation_generator,\n    validation_steps=nb_validation_samples \/\/ batch_size)\n","48f749f3":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nfrom matplotlib.pyplot import figure,subplot,plot,legend,title,show\nfigure(figsize=(8,8))\nsubplot(1, 2, 1)\nplot(epochs, acc, 'r', label='Training Accuracy')\nplot(epochs, val_acc, 'b', label='Validation Accuracy')\nlegend(loc='lower right')\ntitle('Training and Validation Accuracy')\nshow\n\nsubplot(1, 2, 2)\nplot(epochs, loss, 'r', label='Training Loss')\nplot(epochs, val_loss, 'b', label='Validation Loss')\nlegend(loc='upper right')\ntitle('Training and Validation Loss')\nshow","5079f01c":"print the number of images in each class","53aee2f3":"<h2>Plot of Accuracy & Plot of Loss<\/h2>\n\nFrom the plot of accuracy we can see whether the model could probably be trained more or not. From the plot of loss, we can see whether the model has comparable performance on both train and validation datasets (labeled test) or not. If these parallel plots start to depart consistently, it might be a sign to stop training at an earlier epoch.\n\n","e431c32b":"<h1>Convolutional Neural Networks for Beginner<\/h1>\n> by: Akmarina Khairunnisa\n\n\nConvolutional Neural Networks (CNNs) are well suited for classifying images since they utilize spatial information. We will learn about how to write a CNN program in the most powerful, popular, and scalable machine learning stack available; **TensorFlow** and **Keras**. The program must be able to,\n1. Recognize the shape of the hands that make up scissor, rock, or paper based on dataset Rock-Paper-Scissors Image\n2. The dataset must be divided into train sets and validation sets\n3. The validation set size must be 40% of the total dataset\n4. Model training does not exceed 30 minutes\n5. The accuracy of the model is at least 96%\n\nThere are many complex tasks that can be done with computer vision and can be solved by composing CNNs into larger and more complex architectures.\n - Classification\n - Semantic Segmentation\n - Classification and Localizing\n - Object Detection\n - Instance Segmentation\n\nThis project requires us to carry out classification tasks. In the classification task we will have to report the class of object found in the image, but also the coordinates of the bounding box where the object appears in the image. This type of task assumes that there is only one instance of the object in an image.","0ae07a6a":"<h2>Image Preprocessing (Data Augmentation)<\/h2>\n\nData Augmentation is a method of **artificially creating a new dataset for training from the existing training dataset** to improve the performance of deep learning neural networks with the amount of data available. It is a form of regularization which makes our model generalize better than before. It must be noted that Data Augmentation **doesn't add more training data**, instead it replaces the original training batch with the new randomly transformed batch.\n\nHere we have used a Keras `ImageDataGenerator` object to apply data augmentation to the images for randomly translating, resizing, rotating, etc. Each new batch of our data is randomly adjusting according to the parameters supplied to `ImageDataGenerator`.\n\n\nThe datasets we download contains folders of data corresponding to the respective classes. This led to the need for a method that takes the path to a directory and generates batches of augmented data. In Keras this is done using the `flow_from_directory(directory)` method for the augmentation because the labels are inferred from directory structure.\n\n**Arguments used in this code**\n\n\n- `rescale` is a value by which we will multiply the data before any other processing. Our original images consist in RGB coefficients in the 0 - 255, but such values would be too high for our models to process (given a typical learning rate), so we target values between 0 and 1 instead by scaling with a 1.\/255 factor.\n\n- `zoom_range` is a value for randomly zooming inside pictures\n\n- `rotation_range` is a value in degrees (0 - 180) range which randomly rotate pictures\n\n- `horizontal_flip` is a boolean which randomly flip inputs horizontally.\n\n- `vertical_flip` is a boolean Randomly flip inputs vertically.\n\n- `validation_split`: Optional float between 0 and 1, fraction of data to reserve for validation. One of the requirements that must be met in this project is: validation data must be 40 percent of the total dataset. So, we will set the `validation_split` to 0.4\n\n- `fill_mode` is One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}. \nIt is the strategy used for filling in newly created pixels, which can appear after a rotation or a width\/height shift.\nPoints outside the boundaries of the input are filled according to the given mode: \n    - 'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k) \n    - 'nearest': aaaaaaaa|abcd|dddddddd \n    - 'reflect': abcddcba|abcd|dcbaabcd \n    - 'wrap': abcdabcd|abcd|abcdabcd\n\n- `subset`: One of \"training\" or \"validation\". It is used only if validation_split is set.","0e0589f5":"<h2>Module for Image Processing<\/h2>\n<h4>Tensorflow 2.0<\/h4>\n\nmake sure to use tensorflow 2.0 or later because the 2.0 release has the\nimproved usability, clarity, and flexibility of TensorFlow","3ea2fbf8":"<h2>Directory Management<\/h2>\n\nThe `os` module in python provides many functions to interact with the file system. It is used in this project to manage the file which is saved in `\/kaggle\/input\/` and create some directories for better data management.\nSome functions will be used in here are,\n- `os.path.join` is used to join one or more path components intelligently. This function concatenates various path components with exactly one directory\n- `os.listdir` returns a list containing the names of the entries in the directory given by path. The list is in arbitrary order.","3a31e8b5":"<h2>Callbacks<\/h2>\n\nUsage of callbacks via the built-in `fit()` loop. We will pass a custom callback to the `.fit()` method of a model. The relevant methods of the callbacks will then be called at each stage of the training. To meet the requirement that \"the `accuracy` must be above 96%\", a callback  that will automatically stop the training when the accuracy is above 96% will be made.","bec4b467":"<h2>Build CNN Architecture<\/h2>\n\n<h3>Create a Sequential Model<\/h3>\n\nA `Sequential` model is appropriate for a plain stack of layers where each layer has **exactly one input tensor and one output tensor**. `Sequential` groups a linear stack of layers into a tf.keras.Model.`Sequential` provides training and inference features on this model.\n\nThere are several steps in building the CNN model in this project:\n- **Convolutional**\n    > Import `Conv2D` to carry out convolutional operations in training images.\n- **Pooling**\n    > A pooling layer is usually incorporated between two successive convolutional layers. The pooling layer reduces the number of parameters and computation by down-sampling the representation. The pooling function can be max or average (Ke et al., 2018). Max pooling is commonly used as it works better, so `MaxPooling2D` is used for its pooling operation.\n- **Flatten**\n    > Flattening is converting the data into a 1-dimensional array to create a single long feature vector for inputting it to the next layer. We will import `Flatten` for its flattening function.\n- **Full Connection**\n    >Import `Dense` to run our full neural network connection\n\n<h4>Import the Module<\/h4>\n\n","e4345c8c":"<h2> Model Training <\/h2>\n\nIn training the model, we use the `.fit` function.\nTensorFlow is in the process of deprecating the `.fit_generator` method which supported data augmentation. If we are using `tensorflow==2.2.0` or `tensorflow-gpu==2.2.0` (or higher), then we must use the `.fit` method (which now supports data augmentation) \n\nWhen we call the `.fit()` function in our code, it makes assumptions:\n- Keras is first calling the generator function(dataAugmentaion)\n- Generator function(dataAugmentaion) provides a batch_size of 32 to our .fit_generator() function.\n- The `.fit_()` function first accepts a batch of the dataset, then performs backpropagation on it, and then updates the weights in our model.\n- For the number of epochs specified the process is repeated.\n\n**Arguments used in this code**\n- **object** : the keras object model to train. It could be defined as one of two ways\n  - **X** and **Y**: training data and training labels. They can be Vector, array or matrix each\n  - **generator** : a single output of the generator makes a single batch and hence all arrays in the list must be having the length equal to the size of the batch. \nThe generator is expected to loop over its data infinite number of times, it should never return or exit. The output of a generator is a list of the form: \n    - (inputs, targets) \n    - (input, targets, sample_weights) \n    \n- `batch_size` : It specifies the number of samples per gradient.\n\n- `epochs` :  Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch, epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs, but merely until the epoch of index epochs is reached.\n\n- `callbacks`: List of callbacks to apply during training.\n\n- `validation_data` can be either: \n   - an inputs and targets list \n   - a generator \n   - an inputs, targets, and sample_weights list which can be used to evaluate the loss and metrics for any model after any epoch has ended.\n   \n- `shuffle` : whether we want to shuffle our training data before each epoch.\n\n- `steps_per_epoch` : it specifies the total number of steps taken before. one epoch has finished and started the next epoch. By default it values is set to NULL. We can calculate the value of `steps_per_epoch` as the total number of samples in your dataset divided by the batch size.\n\n- `validation_steps` :only if the validation_data is a generator then only this argument can be used. It specifies the total number of steps taken from the generator before it is stopped at every epoch and its value is calculated as the total number of validation data points in your dataset divided by the validation batch size.","c6559e56":"upgrade TensorFlow **only if** you got the older version of it, with this code\n\n```!pip install tensorflow --upgrade```","7d674eab":"<h2>Model Compiler<\/h2>\n\nNow that we have finished building the CNN model, it's time to compile.\n\n**Arguments used in this code**\n- `optimizer` is a parameter to determine the stochastic gradient descent algorithm\n- `loss` is a parameter to determine the loss function\n- `metrics` is a parameter to determine the performance metric","47b2095f":"<h3>Convolutional using Conv2D<\/h3>\n\nWe are going to use the Keras `Conv2D` class to implement a simple CNN.\n\n**Arguments used in this code**\n- `filters` explains the number of filters that the Convolutional layer will learn. Layers closer to the actual input image learn fewer convolutional filters, while layers closer to the output predictions will learn more filters. The exact range of the values may be different for each person, but start with a smaller number of filters and only increase when necessary.\n\n- `kernel_size` is a 2-tuple specifying the width and height of the convolution window. **The `kernel_size` must be an odd integer**. Typical values for `kernel_size` include: `(1,1)` , `(3,3)` , `(5,5)` , `(7,7)`. It's rare to see kernel sizes larger than 7x7. If our input images are greater than 128x128 we may choose to use a `kernel_size` > 3 to learn larger features and then quickly reduce spatial dimensions - then start working with 3x3 kernels to help\n\n- The `padding` parameter to the Keras `Conv2D` class can take on one of two values of `valid` or `same`. With the `same` for the `padding`, the output volume size will match the input volume size because the spatial dimensions of the volume are preserved.\n\n- The `activation` parameter to the `Conv2D` class is simply a convenience parameter, allowing us to supply a string specifying the name of the activation function we want to apply after performing the convolution. \n\n<h3>Pooling<\/h3>\n\nPooling aims to reduce the size of the image as much as possible. Here also we try to reduce the total number of nodes in the next layer. Pooling in the form of a 2x2 matrix as minimum pixel loss and precise region where features are allocated.\n\n**Argument used in this code**\n- `pool_size` is an integer or tuple of 2 integers explaining window size over which to take the maximum. (2, 2) will take the max value over a 2x2 pooling window.\n\n<h3>Dropout Regularization<\/h3>\n\nDropout Regularization is applied to the input. The `Dropout`'s purpose is to help your network generalize and not overfit. Neurons from the current layer, with probability p, will randomly disconnect from neurons in the next layer so that the network has to rely on the existing connections.\n\n**Argument used in this code**\n- `rate` is a float between 0 and 1. It is a Fraction of the input units to drop. The Dropout layer randomly sets input units to 0 with a frequency of `rate` at each step during training time, \n\n<h3>Flatten<\/h3>\n\nNext is the flattening process. The pooling data that we have are in the form of 2-dimensional arrays and then converted to single vector one-dimensional data.\n\n<h3>Dense<\/h3>\n\n`Dense` layer is just a regular densely-connected NN layer. `Dense` layer on Keras is a layer that can be used as a hidden layer and output layer on an MLP.\n\n**Arguments used in this code**\n- `units` is a Positive integer which explains dimensionality of the output space.\n\n- The `activation` parameter specifying the name of the activation function we want to apply in the Dense layer. For dataset that have 3 or more classes, use the Softmax activation function in the output layer - the output of this layer are the prediction values themselves. The softmax activation function will choose which class has the highest probability."}}