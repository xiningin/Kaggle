{"cell_type":{"dff9ad83":"code","fd9b1e5f":"code","76686490":"code","a06c2d18":"code","6385e298":"code","238a6f31":"code","44a0df20":"code","e06293c9":"code","d988bb9e":"code","9c927efc":"code","27776893":"code","b71338f0":"code","66f0b493":"code","e6ca9f60":"code","e071b876":"code","bf4f1c65":"markdown","be241066":"markdown","3cb0684b":"markdown","8bcfbc27":"markdown","e229ba8d":"markdown","a78c8e55":"markdown","767795a1":"markdown","0c6d09b2":"markdown","22aad8a7":"markdown","cac5fac3":"markdown","7a62ea7a":"markdown","eddfc5e4":"markdown","ad41601e":"markdown","3f067b98":"markdown","f8045060":"markdown","22c128f2":"markdown","1b3eb01d":"markdown","36eb8f68":"markdown"},"source":{"dff9ad83":"import os\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom glob import glob\nimport pickle as pkl\nimport scipy.misc\nimport time\nimport math\nfrom PIL import Image\n# import tensorflow as tf # make sure tensorflow version is 1.x.x\n\n# if you have tensorflow 2.0, uncomment these 2 lines to use tensorflow v1 \nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()","fd9b1e5f":"preprocess = True # change if not converting image size\nfrom_checkpoint = False # change if training from saved model","76686490":"data_dir = '..\/input\/architectural-images-for-gans' # Data\nresized_data_dir = \".\/resized_data\" # Directory to save Resized data\n\nif preprocess == True:\n    # Create resized folder if not exist\n    if not os.path.exists(resized_data_dir):\n        os.mkdir(resized_data_dir)\n\n    for each in os.listdir(data_dir):\n        # Read the image\n        image = cv2.imread(os.path.join(data_dir, each))\n        image = cv2.resize(image, (128, 128))\n        # Save the resized image\n        cv2.imwrite(os.path.join(resized_data_dir, each), image)","a06c2d18":"# Helper functions\n\ndef get_image(image_path, width, height, mode):\n    \"\"\"\n    Read image from image_path\n    :param image_path: Path of image\n    :param width: Width of image\n    :param height: Height of image\n    :param mode: Mode of image\n    :return: Image data\n    \"\"\"\n    image = Image.open(image_path)\n    return np.array(image.convert(mode))\n\n\ndef get_batch(image_files, width, height, mode):\n    data_batch = np.array(\n        [get_image(sample_file, width, height, mode) for sample_file in image_files]).astype(np.float32)\n\n    # Make sure the images are in 4 dimensions\n    if len(data_batch.shape) < 4:\n        data_batch = data_batch.reshape(data_batch.shape + (1,))\n\n    return data_batch\n\n\ndef images_square_grid(images, mode):\n    \"\"\"\n    Save images as a square grid\n    :param images: Images to be used for the grid\n    :param mode: The mode to use for images\n    :return: Image of images in a square grid\n    \"\"\"\n    # Get maximum size for square grid of images\n    save_size = math.floor(np.sqrt(images.shape[0]))\n\n    # Scale to 0-255\n    images = (((images - images.min()) * 255) \/ (images.max() - images.min())).astype(np.uint8)\n\n    # Put images in a square arrangement\n    images_in_square = np.reshape(\n            images[:save_size*save_size],\n            (save_size, save_size, images.shape[1], images.shape[2], images.shape[3]))\n    if mode == 'L':\n        images_in_square = np.squeeze(images_in_square, 4)\n\n    # Combine images to grid image\n    new_im = Image.new(mode, (images.shape[1] * save_size, images.shape[2] * save_size))\n    for col_i, col_images in enumerate(images_in_square):\n        for image_i, image in enumerate(col_images):\n            im = Image.fromarray(image, mode)\n            new_im.paste(im, (col_i * images.shape[1], image_i * images.shape[2]))\n\n    return new_im\n\nclass Dataset(object):\n    \"\"\"\n    Dataset\n    \"\"\"\n    def __init__(self, data_files):\n        \"\"\"\n        Initalize the class\n        :param dataset_name: Database name\n        :param data_files: List of files in the database\n        \"\"\"\n        IMAGE_WIDTH = 128\n        IMAGE_HEIGHT = 128\n\n        self.image_mode = 'RGB'\n        image_channels = 3\n\n        self.data_files = data_files\n        self.shape = len(data_files), IMAGE_WIDTH, IMAGE_HEIGHT, image_channels\n\n    def get_batches(self, batch_size):\n        \"\"\"\n        Generate batches\n        :param batch_size: Batch Size\n        :return: Batches of data\n        \"\"\"\n        IMAGE_MAX_VALUE = 255\n\n        current_index = 0\n        while current_index + batch_size <= self.shape[0]:\n            data_batch = get_batch(\n                self.data_files[current_index:current_index + batch_size],\n                *self.shape[1:3],\n                self.image_mode)\n            \n            current_index += batch_size\n\n            yield data_batch \/ IMAGE_MAX_VALUE - 0.5","6385e298":"show_imgs = 15\ndata_imgs = get_batch(glob(os.path.join(resized_data_dir, '*.jpg'))[:show_imgs], 64, 64, 'RGB')\nplt.imshow(images_square_grid(data_imgs, 'RGB'))","238a6f31":"def gan_model_inputs(real_dim, z_dim):\n    \"\"\"\n    Creates the inputs for the model.\n    \n    Arguments:\n    ----------\n    :param real_dim: tuple containing width, height and channels\n    :param z_dim: The dimension of Z\n    ----------\n    Returns:\n    Tuple of (tensor of real input images, tensor of z (noise) data, Generator learning rate, Discriminator learning rate)\n    \"\"\"\n    real_inputs = tf.placeholder(tf.float32, (None, *real_dim), name='real_inputs')\n    z_inputs = tf.placeholder(tf.float32, (None, z_dim), name=\"z_inputs\")\n    generator_learning_rate = tf.placeholder(tf.float32, name=\"generator_learning_rate\")\n    discriminator_learning_rate = tf.placeholder(tf.float32, name=\"discriminator_learning_rate\")\n    \n    return real_inputs, z_inputs, generator_learning_rate, discriminator_learning_rate\n","44a0df20":"def build_generator(z, output_channel_dim, is_train=True):\n    ''' Builds the generator network.\n    \n        Arguments:\n        ---------\n        z : Input tensor for the generator\n        output_channel_dim : Shape of the generator output\n        n_units : Number of units in hidden layer\n        reuse : Reuse the variables with tf.variable_scope\n        alpha : leak parameter for leaky ReLU\n        ---------\n        Returns\n        gen_output : Output from the generator. \n    '''\n    with tf.variable_scope(\"generator\", reuse= not is_train):\n        \n        # First Fully Connected layer of size 8x8x1024\n        fully_connected_l1 = tf.layers.dense(z, 8*8*1024)\n        \n        # Reshape the layer\n        fully_connected_l1 = tf.reshape(fully_connected_l1, (-1, 8, 8, 1024))\n        \n        # Applying Leaky ReLU Activation\n        fully_connected_l1 = tf.nn.leaky_relu(fully_connected_l1, alpha=alpha)\n\n        \n        # Input of size 8x8x1024\n        # passed through Deconvolution layer 1 + Batch Normalization + LeakyReLU (Activation)\n        layer1_deconv = tf.layers.conv2d_transpose(inputs = fully_connected_l1,\n                                  filters = 512,\n                                  kernel_size = [5,5],\n                                  strides = [2,2],\n                                  padding = \"SAME\",\n                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n                                name=\"layer1_deconv\")\n        \n        layer1_batch_norm = tf.layers.batch_normalization(inputs = layer1_deconv, training=is_train, epsilon=1e-5, name=\"layer1_batch_norm\")\n        \n        # Outputs image of size 16x16x512\n        layer1_deconv_output = tf.nn.leaky_relu(layer1_batch_norm, alpha=alpha, name=\"layer1_deconv_output\")\n        \n        # Input of size 16x16x512\n        # passed through Deconvolution layer 2 + Batch Normalization + LeakyReLU (Activation)\n        layer2_deconv = tf.layers.conv2d_transpose(inputs = layer1_deconv_output,\n                                  filters = 256,\n                                  kernel_size = [5,5],\n                                  strides = [2,2],\n                                  padding = \"SAME\",\n                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n                                name=\"layer2_deconv\")\n        \n        layer2_batch_norm = tf.layers.batch_normalization(inputs = layer2_deconv, training=is_train, epsilon=1e-5, name=\"layer2_batch_norm\")\n        \n        # Outputs image of size 32x32x256\n        layer2_deconv_output = tf.nn.leaky_relu(layer2_batch_norm, alpha=alpha, name=\"layer2_deconv_output\")\n        \n        \n        # Input of size 32x32x256\n        # passed through Deconvolution layer 3 + Batch Normalization + LeakyReLU (Activation)\n        layer3_deconv = tf.layers.conv2d_transpose(inputs = layer2_deconv_output,\n                                  filters = 128,\n                                  kernel_size = [5,5],\n                                  strides = [2,2],\n                                  padding = \"SAME\",\n                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n                                name=\"layer3_deconv\")\n        \n        layer3_batch_norm = tf.layers.batch_normalization(inputs = layer3_deconv, training=is_train, epsilon=1e-5, name=\"layer3_batch_norm\")\n       \n        # Outputs image of size 64x64x128\n        layer3_deconv_output = tf.nn.leaky_relu(layer3_batch_norm, alpha=alpha, name=\"layer3_deconv_output\")\n\n        # Input of size 64x64x128\n        # passed through Deconvolution layer 4 + Batch Normalization + LeakyReLU (Activation)\n        layer4_deconv = tf.layers.conv2d_transpose(inputs = layer3_deconv_output,\n                                  filters = 64,\n                                  kernel_size = [5,5],\n                                  strides = [2,2],\n                                  padding = \"SAME\",\n                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n                                name=\"layer4_deconv\")\n        \n        layer4_batch_norm = tf.layers.batch_normalization(inputs = layer4_deconv, training=is_train, epsilon=1e-5, name=\"layer4_batch_norm\")\n        \n        # Outputs image of size 128x128x64       \n        layer4_deconv_output = tf.nn.leaky_relu(layer4_batch_norm, alpha=alpha, name=\"layer4_deconv_output\")\n\n        \n        # Final layer input of size 128x128x64\n        # passed through Deconvolution layer 5 + tanh (Activation)\n        logits = tf.layers.conv2d_transpose(inputs = layer4_deconv_output,\n                                  filters = 3,\n                                  kernel_size = [5,5],\n                                  strides = [1,1],\n                                  padding = \"SAME\",\n                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n                                name=\"logits\")\n        \n        # Outputs image of size 128x128x3         \n        gen_output = tf.tanh(logits, name=\"output\")\n        \n        return gen_output","e06293c9":"def build_discriminator(x, is_reuse=False, alpha = 0.2):\n    ''' Builds the discriminator network.\n    \n        Arguments:\n        ---------\n        :param x : Input tensor for the discriminator\n        :param n_units: Number of units in hidden layer\n        :param reuse : Reuse the variables with tf.variable_scope\n        :param alpha : leak parameter for leaky ReLU\n        ---------\n        Returns:\n        Tuple of (discriminator output, logits) \n    '''\n    with tf.variable_scope(\"discriminator\", reuse = is_reuse): \n        \n        # Input layer 128x128x3 image \n        # Passed through Convolution layer + Batch Normalization + LeakyReLU (Activation)   \n        layer1_conv = tf.layers.conv2d(inputs = x,\n                                filters = 64,\n                                kernel_size = [5,5],\n                                strides = [2,2],\n                                padding = \"SAME\",\n                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n                                name='layer1_conv')\n        \n        layer1_batch_norm = tf.layers.batch_normalization(layer1_conv,\n                                                   training = True,\n                                                   epsilon = 1e-5,\n                                                     name = 'layer1_batch_norm')\n        # Outputs a 64x64x64 image\n        layer1_conv_output = tf.nn.leaky_relu(layer1_batch_norm, alpha=alpha, name=\"layer1_conv_output\")\n        \n        \n        # Input image of size 64x64x64\n        # Convolution layer + Batch Normalization + LeakyReLU (Activation)  \n        layer2_conv = tf.layers.conv2d(inputs = layer1_conv_output,\n                                filters = 128,\n                                kernel_size = [5, 5],\n                                strides = [2, 2],\n                                padding = \"SAME\",\n                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n                                name='layer2_conv')\n        \n        layer2_batch_norm = tf.layers.batch_normalization(layer2_conv,\n                                                   training = True,\n                                                   epsilon = 1e-5,\n                                                     name = 'layer2_batch_norm')\n        # Outputs image of size 32x32x128\n        layer2_conv_output = tf.nn.leaky_relu(layer2_batch_norm, alpha=alpha, name=\"layer2_conv_output\")\n\n        \n        \n         # Input image of size 32x32x128\n        # Convolution layer + Batch Normalization + LeakyReLU (Activation) \n        layer3_conv = tf.layers.conv2d(inputs = layer2_conv_output,\n                                filters = 256,\n                                kernel_size = [5, 5],\n                                strides = [2, 2],\n                                padding = \"SAME\",\n                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n                                name='layer3_conv')\n        \n        layer3_batch_norm = tf.layers.batch_normalization(layer3_conv,\n                                                   training = True,\n                                                   epsilon = 1e-5,\n                                                name = 'layer3_batch_norm')\n        # Outputs image of size 16x16x256\n        layer3_conv_output = tf.nn.leaky_relu(layer3_batch_norm, alpha=alpha, name=\"layer3_conv_output\")\n\n        \n        \n        # Input image of size 16x16x256 \n        # Convolution layer + Batch Normalization + LeakyReLU (Activation)  \n        layer4_conv = tf.layers.conv2d(inputs = layer3_conv_output,\n                                filters = 512,\n                                kernel_size = [5, 5],\n                                strides = [1, 1],\n                                padding = \"SAME\",\n                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n                                name='layer4_conv')\n        \n        layer4_batch_norm4 = tf.layers.batch_normalization(layer4_conv,\n                                                   training = True,\n                                                   epsilon = 1e-5,\n                                                name = 'layer4_batch_norm')\n        # Outputs image of size 16x16x512\n        layer4_conv_output = tf.nn.leaky_relu(layer4_batch_norm4, alpha=alpha, name=\"layer4_conv_output\")\n\n        \n        \n        # Input image of size 16x16x512 \n        # Convolution layer + Batch Normalization + LeakyReLU (Activation)  \n        layer5_conv = tf.layers.conv2d(inputs = layer4_conv_output,\n                                filters = 1024,\n                                kernel_size = [5, 5],\n                                strides = [2, 2],\n                                padding = \"SAME\",\n                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n                                name='layer5_conv')\n        \n        layer5_batch_norm = tf.layers.batch_normalization(layer5_conv,\n                                                  training = True,\n                                                   epsilon = 1e-5,\n                                                name = 'layer5_batch_norm')\n        # Outputs image of size 8x8x1024\n        layer5_conv_output = tf.nn.leaky_relu(layer5_batch_norm, alpha=alpha, name=\"layer5_conv_output\")\n\n         \n        # Flatten the image\n        flatten_img = tf.reshape(layer5_conv_output, (-1, 8*8*1024))\n        \n        # Logits\n        logits = tf.layers.dense(inputs = flatten_img,\n                                units = 1,\n                                activation = None)\n        \n        # Binary value of discriminator output\n        disc_output = tf.sigmoid(logits)\n        \n        return disc_output, logits","d988bb9e":"def gan_model_loss(input_real, input_z, output_channel_dim, alpha):\n    \"\"\"\n    Get the loss for the discriminator and generator\n    \n    Arguments:\n    ---------\n    :param input_real: Images from the real dataset\n    :param input_z: Z input\n    :param out_channel_dim: The number of channels in the output image\n    ---------\n    Returns: \n    A tuple of (discriminator loss, generator loss)\n    \"\"\"\n    # Build the Generator network \n    g_model_output = build_generator(input_z, output_channel_dim)   \n    \n    # Build the discriminator network \n    \n    # For real inputs\n    real_d_model, real_d_logits = build_discriminator(input_real, alpha=alpha)\n    \n    # For fake inputs (generated output from the generator model)\n    fake_d_model, fake_d_logits = build_discriminator(g_model_output, is_reuse=True, alpha=alpha)\n    \n    # Calculate losses for each network\n    d_loss_real = tf.reduce_mean(\n                  tf.nn.sigmoid_cross_entropy_with_logits(logits=real_d_logits, \n                                                          labels=tf.ones_like(real_d_model)))\n    d_loss_fake = tf.reduce_mean(\n                  tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_d_logits, \n                                                          labels=tf.zeros_like(fake_d_model)))\n    \n    # Discriminator loss is the sum of real and fake loss\n    d_loss = d_loss_real + d_loss_fake\n\n    # Generator loss\n    g_loss = tf.reduce_mean(\n             tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_d_logits,\n                                                     labels=tf.ones_like(fake_d_model)))\n    \n    return d_loss, g_loss","9c927efc":"def gan_model_optimizers(d_loss, g_loss, disc_lr, gen_lr, beta1):\n    \"\"\"\n    Get optimization operations\n    \n    Arguments:\n    ----------\n    :param d_loss: Discriminator loss Tensor\n    :param g_loss: Generator loss Tensor\n    :param disc_lr: Placeholder for Learning Rate for discriminator\n    :param gen_lr: Placeholder for Learning Rate for generator\n    :param beta1: The exponential decay rate for the 1st moment in the optimizer\n    ----------\n    Returns: \n    A tuple of (discriminator training operation, generator training operation)\n    \"\"\"    \n    # Get the trainable_variables, split into G and D parts\n    train_vars = tf.trainable_variables()\n    gen_vars = [var for var in train_vars if var.name.startswith(\"generator\")]\n    disc_vars = [var for var in train_vars if var.name.startswith(\"discriminator\")]\n    \n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    \n    # Generator update\n    gen_updates = [op for op in update_ops if op.name.startswith('generator')]\n    \n    # Optimizers\n    with tf.control_dependencies(gen_updates):\n        disc_train_opt = tf.train.AdamOptimizer(learning_rate = disc_lr, beta1 = beta1).minimize(d_loss, var_list = disc_vars)\n        gen_train_opt = tf.train.AdamOptimizer(learning_rate = gen_lr, beta1 = beta1).minimize(g_loss, var_list = gen_vars)\n        \n    return disc_train_opt, gen_train_opt","27776893":"def generator_output(sess, n_images, input_z, output_channel_dim, image_mode, image_path):\n    \"\"\"\n    Save output from the generator.\n    \n    Arguments:\n    ----------\n    :param sess: TensorFlow session\n    :param n_images: Number of Images to display\n    :param input_z: Input Z Tensor (noise vector)\n    :param output_channel_dim: The number of channels in the output image\n    :param image_mode: The mode to use for images (\"RGB\" or \"L\")\n    :param image_path: Path to save the generated image\n    ----------\n    \"\"\"\n    cmap = None if image_mode == 'RGB' else 'gray'\n    z_dimension = input_z.get_shape().as_list()[-1]\n    example_z = np.random.uniform(-1, 1, size=[n_images, z_dimension])\n\n    samples = sess.run(\n        build_generator(input_z, output_channel_dim, False),\n        feed_dict={input_z: example_z})\n\n    images_grid = images_square_grid(samples, image_mode)\n    \n    # Save image to the image path\n    images_grid.save(image_path, 'JPEG')\n    ","b71338f0":"def train_gan_model(epoch, batch_size, z_dim, learning_rate_D, learning_rate_G, beta1, get_batches, data_shape, data_image_mode, alpha):\n    \"\"\"\n    Train the GAN model.\n    \n    Arguments:\n    ----------\n    :param epoch: Number of epochs\n    :param batch_size: Batch Size\n    :param z_dim: Z dimension\n    :param learning_rate: Learning Rate\n    :param beta1: The exponential decay rate for the 1st moment in the optimizer\n    :param get_batches: Function to get batches\n    :param data_shape: Shape of the data\n    :param data_image_mode: The image mode to use for images (\"RGB\" or \"L\")\n    ----------\n    \"\"\"\n    # Create our input placeholders\n    input_images, input_z, lr_G, lr_D = gan_model_inputs(data_shape[1:], z_dim)\n        \n    # getting the discriminator and generator losses\n    d_loss, g_loss = gan_model_loss(input_images, input_z, data_shape[3], alpha)\n    \n    # Optimizers\n    d_opt, g_opt = gan_model_optimizers(d_loss, g_loss, lr_D, lr_G, beta1)\n    \n    i = 0\n    \n    version = \"firstTrain\"\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        \n        # Saving the model\n        saver = tf.train.Saver()\n        \n        num_epoch = 0\n        print(\"Starting the model training...\")\n        \n        # If training from saved checkpoint\n        if from_checkpoint == True:\n            saver.restore(sess, \"..\/input\/saved-models\/models\/model.ckpt\")\n            # Save the generator output\n            img_save_path = \".\/generated_images\/\" \n            # Create folder if not exist\n            if not os.path.exists(img_save_path):\n                os.makedirs(img_save_path)\n            image_path = img_save_path + \"\/\" + \"generated_fromckpt.PNG\"\n            generator_output(sess, 4, input_z, data_shape[3], data_image_mode, image_path)\n            \n        else:\n            for epoch_i in range(epoch):        \n                num_epoch += 1\n                print(\"Training model for epoch_\", epoch_i)\n\n                if num_epoch % 5 == 0:\n\n                    # Save model every 5 epochs\n                    save_path = saver.save(sess, \".\/models\/model.ckpt\")\n                    print(\"Model has been saved.\")\n\n                for batch_images in get_batches(batch_size):\n                    # Random noise\n                    batch_z = np.random.uniform(-1, 1, size=(batch_size, z_dim))\n\n                    i += 1\n\n                    # Run optimizers\n                    _ = sess.run(d_opt, feed_dict={input_images: batch_images, input_z: batch_z, lr_D: learning_rate_D})\n                    _ = sess.run(g_opt, feed_dict={input_images: batch_images, input_z: batch_z, lr_G: learning_rate_G})\n\n                    # Every 5 epochs\n                    if i % 5 == 0:\n                        \n                        # Calculate the training loss\n                        train_loss_d = d_loss.eval({input_z: batch_z, input_images: batch_images})\n                        train_loss_g = g_loss.eval({input_z: batch_z})\n\n                        # path to save the generated image\n                        image_name = str(i) + \"_epoch_\" + str(epoch_i) + \".jpg\"\n                        img_save_path = \".\/generated_images\/\" \n                        \n                        # Create folder if not exist\n                        if not os.path.exists(img_save_path):\n                            os.makedirs(img_save_path)\n                        image_path = img_save_path + \"\/\" + image_name\n                        \n                        # Print the values of epoch and losses\n                        print(\"Epoch {}\/{}...\".format(epoch_i+1, epoch),\n                              \"Discriminator Loss: {:.4f}...\".format(train_loss_d),\n                              \"Generator Loss: {:.4f}\".format(train_loss_g))\n                        \n                        # Save the generator output\n                        generator_output(sess, 4, input_z, data_shape[3], data_image_mode, image_path) \n                ","66f0b493":"# Size of latent (noise) vector to generator\nz_dim = 100 \n\n# Learning rates\nlearning_rate_D = .00005 \nlearning_rate_G = 2e-4 \n\n# Batch size\nbatch_size = 4\n\n# Number of epochs\nnum_epochs = 500\n\n# Decay rates\nalpha = 0.2\nbeta1 = 0.5","e6ca9f60":"# Load the training data\ntraining_dataset = Dataset(glob(os.path.join(resized_data_dir, '*.jpg')))\n\n# Train the model\nwith tf.Graph().as_default():\n    train_gan_model(num_epochs, batch_size, z_dim, learning_rate_D, learning_rate_G, beta1, training_dataset.get_batches,\n          training_dataset.shape, training_dataset.image_mode, alpha)","e071b876":"show_images = 8\n# Plot the generated images\ndata_images = get_batch(glob(os.path.join(\"..\/input\/saved-models\/generated_images\/\", '*.jpg'))[:show_images], 64, 64, 'RGB')\n\nplt.imshow(images_square_grid(data_images, 'RGB'))","bf4f1c65":"## DCGAN Architecture","be241066":"## Generator and Discriminator losses\n","3cb0684b":"### Importing the libraries","8bcfbc27":"### Show output\n\nWe will be saving the output of the generator during training in order to determine how well the GAN is training.","e229ba8d":"### Plot the generated images","a78c8e55":"## Conclusion\n\nHence, we have successfully generated architectural images using GANs. \n\nDespite the generated images aren\u2019t of high-quality, these results prove that GANs can be quite helpful as a tool in the creative field. The following results were a result of training the model on a standard CPU for several hours. Upon training on a high-end GPU\/TPU, the results can be expected to improve a lot.","767795a1":"### Explore the data\n\nLet us display our image samples.","0c6d09b2":"## Training the model\n\nWe will be training the model with the hyperparameters such as epochs, batch size, learning rate, etc. Here, we are saving the model after each five epochs as well as the generated image in each ten batches of image training. Along with it, we are also calculating and displaying the g_loss and d_loss.","22aad8a7":"### 3. The Discriminator network\n\n- Similarly, the discriminator network uses a convolutional neural network architecture that takes in the real images input in size 128x128x3. The filter size is doubled in every layer such that the image size decreases. \n- The network contains several layers of batch normalization and Leaky ReLu with the final layer which flattens the image and passes it through a sigmoid function. \n- The final output is generated through the sigmoid activation function in the last layer, which results in a binary output 0 (fake) or 1 (real).\n\n![](https:\/\/drive.google.com\/uc?export=view&id=1U3M7FXOLvb9THav0DIUg_jtZC31GFI-d)","cac5fac3":"### 1. Input to the network\n\nCreating TensorFow placeholders for the inputs to the network.\n\n- Real input images placeholder `real_dim`.\n- Z input placeholder `z_dim`, i.e., the noise vector.\n- Learning rate placeholder for generator.\n- Learning rate placeholder for discriminator.\n<br>","7a62ea7a":"### Initializing Hyperparameters\n\nLike most neural networks, GANs are <b> very sensitive to hyperparemeters <\/b>\nIn general, the discriminator loss around 0.3 determines that it is correctly classifying images as fake or real about 50% of the time.","eddfc5e4":"Here is a GIF that shows the how the images were generated in several training steps.\n\n![](https:\/\/drive.google.com\/uc?export=view&id=10tjvYiDH5CZ0EWYmlXlL95VSo7hkRRBn)","ad41601e":"**Generative Adversarial Networks (GANs)** are a class of deep neural networks consisting of two neural networks competing against each other. We have the generator and discriminator.\nIn this tutorial, we will be generating unique architectures using GANs. ","3f067b98":"### Optimizers\n\nAfter calculating the losses, we need to update the generator and discriminator separately.\n\nTo do this, we need to get the variables for each part by using `tf.trainable_variables()` which creates a list of all the variables we\u2019ve defined in our graph. We will then use the variables to update the training paramters of the networks.","f8045060":"# Generating Unique Architectures using GANs","22c128f2":"### 2. The Generator Network\n\n- The DCGAN Generator uses a de-convolutional neural network architecture that takes in the random noise vector (z) as the input, passes it through several layers of batch normalization and Leaky ReLu with filter size halved at every layer. \n- The Leaky ReLu activation is used to avoid gradient vanishing and is used in all layers except the last layer. \n- The final output is generated through a tanh activation function in the last layer, which results a 128x128x3 size image.\n\n![](https:\/\/drive.google.com\/uc?export=view&id=1144Xp0GEsqEc97OKt5_BGi51T5spl5Zf)","1b3eb01d":"### Data\n- We are using the [wikiarts](https:\/\/www.wikiart.org\/en\/paintings-by-genre\/abstract?select=featured#!#filterName:featured,viewType:masonry) data. Download the dataset [here](https:\/\/drive.google.com\/file\/d\/1ciCghKm-N8bXpiJ1qqlYBvmms_rlmKb0\/view).\n- Change `do_preprocess = True` to resize the images.","36eb8f68":"### Resize images to 128x128"}}