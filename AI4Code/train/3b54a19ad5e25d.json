{"cell_type":{"0a9d597f":"code","8da4f89f":"code","c1ce73f3":"code","79c076fe":"code","30d1f548":"code","fd62a4da":"code","b817d069":"code","31f86922":"code","7ef16418":"code","f691d7ea":"code","9d8beb40":"code","2655b701":"code","2e84fde0":"code","950f6e52":"code","b7268064":"code","a821bd47":"code","6030aaf3":"code","aacd6ea4":"code","986a68d0":"code","ff735afe":"code","43f93dfe":"code","b526d8ba":"code","139a92b0":"code","8ddedcc3":"code","42ed3eb4":"code","71d93dea":"code","1f8f75aa":"code","72d63f01":"code","7bbca5ce":"code","55df61aa":"code","697b6c3f":"markdown","c1dbad2d":"markdown","c23535d3":"markdown","c2d8275e":"markdown","4c07b305":"markdown","15a3133c":"markdown","68ead1a4":"markdown","189b5f5d":"markdown","8f11edf6":"markdown","afcc5645":"markdown","ac8ae3e8":"markdown","6e8119f2":"markdown","b7fbb965":"markdown","83bfc0ae":"markdown","4c099827":"markdown","703bd2f7":"markdown","847d0d85":"markdown","4842a241":"markdown","de42f568":"markdown","ba89e731":"markdown","55b33a2c":"markdown","ea690185":"markdown","ed600536":"markdown","2dbeb0d7":"markdown","6bda9b94":"markdown","57420e2b":"markdown"},"source":{"0a9d597f":"# Global variables \ndataset_filename = \"..\/input\/Womens Clothing E-Commerce Reviews.csv\"","8da4f89f":"# Import libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport string\nimport re\nimport numpy as np \nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom scipy.spatial.distance import cosine\nfrom numpy import array\nfrom sklearn.decomposition import TruncatedSVD\n\n%matplotlib inline","c1ce73f3":"# Loading dataset\ndata = pd.read_csv(dataset_filename, index_col=0)","79c076fe":"# Looking at some of the top rows of dataset\ndata.head()","30d1f548":"# Description\ndata.describe()","fd62a4da":"# Info\ndata.info()","b817d069":"# Printing the unique Clothing IDs along with their frequencies\ndata['Clothing ID'].value_counts()[:5]","31f86922":"# From here, we can conclude Clothing ID 1078 as most common. So, we will be using this for the rest of our project\ndatax = data.loc[data['Clothing ID'] == 1078 , :] # We will be calling this data as datax\ndatax.head()","7ef16418":"datax.info()","f691d7ea":"corpus = [review for (id,review) in datax['Review Text'].iteritems() if isinstance(review,str)]","9d8beb40":"# Creating dictionary of review to id\nreview_to_id_dict = {review : id for (id,review) in enumerate(corpus)}","2655b701":"corpus_tokenized = np.array([review.split() for review in corpus])\n\nprint(corpus_tokenized[:5])","2e84fde0":"for i in range(5):\n  print(i, corpus[i])","950f6e52":"corpus1 = []\n\nfor review in corpus:\n  if isinstance(review,str):\n    review = review.split()\n    review = [re.sub('[^A-Za-z]+', '', x) for x in review]\n    review = [x.lower() for x in review if len(x) > 0]\n    corpus1.append(' '.join(review))","b7268064":"# We are using nltk list of stopwords\nstopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you',\n             \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her',\n             'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them',\n             'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom',\n             'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are',\n             'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n             'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and',\n             'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at',\n             'by', 'for', 'with', 'about', 'against', 'between', 'into',\n             'through', 'during', 'before', 'after', 'above', 'below', 'to',\n             'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under',\n             'again', 'further', 'then', 'once', 'here', 'there', 'when',\n             'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n             'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own',\n             'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will',\n             'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll',\n             'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn',\n             \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\",\n             'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma',\n             'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",\n             'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\",\n             'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n\n# Extending stopwords with space\nstopwords.append('')\n\n# Converting it to a set\nstopwords = set(stopwords)","a821bd47":"# Removing stopwords and storing it into a new dict\n\ncorpus_sr = [] # Corpus after removing stopwords\n\nfor review in corpus1 :\n  if isinstance(review, str):\n    review = review.split()\n    new_review = []\n    for x in review:\n      if x not in stopwords:\n        new_review.append(x)\n    corpus_sr.append(\" \".join(new_review))","6030aaf3":"# Creating a list of all words present in review\nword_list = []\n\nfor review in corpus_sr:\n  word_list.extend(review.split())\n\nword_list = list(set(word_list))\n\nprint(word_list[:10])","aacd6ea4":"# Stemming\nimport nltk\nfrom nltk.stem.porter import PorterStemmer\nporter_stemmer = PorterStemmer()\n\ncorpus_stemmed = []\n\nfor review in corpus_sr:\n  review = [porter_stemmer.stem(x) for x in review.split()]\n  corpus_stemmed.append(' '.join(review))\n\nfor i in range(5):\n  print(corpus_sr[i])\n  print(corpus_stemmed[i])","986a68d0":"# Lemmatization\nimport nltk\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n\ncorpus_lemmatized = []\n\nfor review in corpus_sr:\n  review = [wordnet_lemmatizer.lemmatize(x) for x in review.split()]\n  corpus_lemmatized.append(' '.join(review))\n\nfor i in range(5):\n  print(corpus_sr[i])\n  print(corpus_lemmatized[i])","ff735afe":"corpus_tokenized = np.array([review.split() for review in corpus_lemmatized])\n\nprint(corpus_tokenized[:5])","43f93dfe":"vocabulary = []\nfor review in corpus_lemmatized:\n  vocabulary.extend(review.split())\nvocabulary = list(set(vocabulary))\n\n# Printing some of the first elements of word_list and number of words present in it\nprint(vocabulary[:5])\nprint(len(vocabulary))\n\n# Tokenizing\nword_to_id = {word:id for id,word in enumerate(vocabulary)}\nid_to_word = {id:word for id,word in enumerate(vocabulary)}","b526d8ba":"m = len(corpus_lemmatized) # m = number of reviews \nn = len(vocabulary) # n = number of unique words\n\ntfm = np.zeros((m, n),dtype=int) # Term frequency matrix\nfor i in range(m):\n  words = corpus_lemmatized[i].split()\n  for j in range(len(words)):\n    word = words[j]\n    tfm[i][word_to_id[word]] += 1 ","139a92b0":"tmpm = tfm != 0 # Temporary matrix\ndft = tmpm.sum(axis = 0) #the number of documents where term t appears\ntfidfm = np.multiply(tfm, np.log(m\/dft))","8ddedcc3":"U, s, VT = np.linalg.svd(tfm)\n\nK = 2 # number of components\n\ntfm_reduced = np.dot(U[:,:K], np.dot(np.diag(s[:K]), VT[:K, :]))\ndocs_rep = np.dot(tfm, VT[:K, :].T)\nterm_rep = np.dot(tfm.T, U[:,:K])","42ed3eb4":"plt.scatter(docs_rep[:,0], docs_rep[:,1])\nplt.title(\"Document Representation\")\nplt.show()","71d93dea":"plt.scatter(term_rep[:,0], term_rep[:,1])\nplt.title(\"Term Representation\")\nplt.show()","1f8f75aa":"query = 'nice good'\n\n\nkey_word_indices = []\n\nfor x in query.split():\n  if x in word_to_id.keys():\n    key_word_indices.append(word_to_id[x])","72d63f01":"key_words_rep = term_rep[key_word_indices,:]     \nquery_rep = np.sum(key_words_rep, axis = 0)\n\nprint (query_rep)","7bbca5ce":"query_doc_cos_dist = [cosine(query_rep, doc_rep) for doc_rep in docs_rep]\nquery_doc_sort_index = np.argsort(np.array(query_doc_cos_dist))\n\nfor rank, sort_index in enumerate(query_doc_sort_index):\n    print (rank, query_doc_cos_dist[sort_index], corpus[sort_index])\n    if rank == 4 : \n      break","55df61aa":"query_vector = np.zeros((1,n))\nfor x in key_word_indices:\n  query_vector[0,x] += 1\n  \nquery_vector = np.multiply(query_vector, np.log(m\/dft))\n\nquery_doc_cos_dist = [cosine(query_vector, tfidfm[i]) for i in range(m)]\nquery_doc_sort_index = np.argsort(np.array(query_doc_cos_dist))\n\nx = []\n\nfor rank, sort_index in enumerate(query_doc_sort_index):\n    print (rank, query_doc_cos_dist[sort_index], corpus[sort_index])\n    x.append(corpus[sort_index])\n    if rank == 4 : \n      break","697b6c3f":"## IR using LSA with TF matrix","c1dbad2d":"## IR using TF-IDF matrix","c23535d3":"## Extracting our Text Corpus","c2d8275e":"# Question 1: Preprocess the corpus of customer reviews dataset","4c07b305":"## Selecting Clothing ID","15a3133c":"### Getting Set of Stopwords","68ead1a4":"## Comparision Between LSA and TF-IDF","189b5f5d":"## Creating Review to ID Dictionary","8f11edf6":"Query : nice good\n\n### Result by LSA with TF matrix\n\n\n*   0 3.906716264934218e-07 I really like this dress. i tried it on in the store and i had to take it home with me. the print is fun and bright and interesting, without being over the top. the grommets give it a little bit of edgy balance to the sweet, flowy shape. a note on the the fit: i am a 12\/14 and the 12 fit well. it is a bit loose in some places, like the waist, but it certainly doesn't look like a maternity dress. i think the looseness is a good part of the overall look. i am high-waisted, and the waistline hit\n*   4.924336040046384e-07 I really like the soft, flowy satin fabric and vibrant color of this dress! i am 5'5'' 125lbs, the s was too large, so i ordered the xs instead. things to note: pockets, elastic waistband with nice detailing on the sides, slip underneath, and large flutter sleeves. i think that women with smaller torsos should consider petite sizing.\n*  2.0761380433720333e-06 I didn't hate it, but i don't love it. i purchased it in 2 colors b\/c i thought i'd fall in love. it is just ok. maybe it's b\/c i have a larger chest, but i felt like it was too small in the chest area and too loose elsewhere. not the maxi fit i normally go for. however, the material is great and it's a simple sunday dress to lounge around in and run errands comfortably.\n* 2.6978156143497856e-06 This is the most flattering dress i have bought in years. the fact that it's machine washable was a huge plus. fabric is so comfortable and the length is perfect! i wear either m or l and went with the large for a little extra room, but either would have fit well.\n* 4.388323377790826e-06 I was lucky enough to get a hold of this intarsia sweater dress after the sale and i wish i had purchased this the first time round. it is absolutely stunning, flattering, comfortable and unique! i am 5'3\" and the regular hem fit me just fine at the ankles. i think this dress runs both tts to a bit large so i would size down if your small framed and stay your usual if your busty or broad shouldered. perfectly complements my taupe booties that i already owned and a my taupe maxi sweater that has\n\n\n### Result by TF-IDF\n* 0.5753370983500936 I love this dress . perfect fit and very good quality.\n* 0.6377201772710546 I bought this dress because i thought it would look good on her, and it does.\n* 0.6633070868863611 This dress really was huge, and not at all flattering. i don't know how they got it to look good on the model in the picture. material was nice and soft, but i really don't see how anyone could look good in this, no matter what your body shape is.\n* 0.7419458491496504 The pattern and fabric on this dress are very nice. there is just too much fabric. it's much too baggy but could make a nice maternity dress. i'll be returning it.\n* 0.7570324564069156 Really cute fun print. nice summer dress..\n\n\n\nFor this example, TF-IDF is giving comparatively better result.","afcc5645":"## Tokenize","ac8ae3e8":"### Stemming and Lemmatization","6e8119f2":"## Standardize Tokens","b7fbb965":"# Question 2: Remove stopwords, standardize tokens","83bfc0ae":"# Question 3: Build the Term-Frequency Inverse-Document-Frequency (TF-IDF) matrix and apply the Latent Semantic Analysis (LSA) method","4c099827":"### Plot documents in the LSA\/TF-IDF space","703bd2f7":"### Document-term matrix","847d0d85":"### Removing Stopwords from Reviews","4842a241":"# Dataset - Women's E-Commerce Clothing Reviews","de42f568":"## Setting up environment","ba89e731":"# Question 4: Compare the performance of Information Retrieval (IR) using both TF-IDF and LSA methods","55b33a2c":"### Creating Word List","ea690185":"## References \n\n* https:\/\/www.kaggle.com\/nicapotato\/womens-ecommerce-clothing-reviews\n*  http:\/\/math.mit.edu\/~gs\/linearalgebra\/ila0601.pdf\n*   http:\/\/web.mit.edu\/be.400\/www\/SVD\/Singular_Value_Decomposition.htm\n* https:\/\/machinelearningmastery.com\/singular-value-decomposition-for-machine-learning\/\n* https:\/\/www.analyticsvidhya.com\/blog\/2018\/08\/dimensionality-reduction-techniques-python\/\n* https:\/\/medium.com\/analytics-vidhya\/text-mining-101-a-stepwise-introduction-to-topic-modeling-using-latent-semantic-analysis-using-add9c905efd9\n\n","ed600536":"### Term frequency inverse document frequency matrix","2dbeb0d7":"### Removing All Symbols other than Alphabets and Converting all Letters to Lowercase","6bda9b94":"### Perform \u200bLSA\u200b using Singular Value Decomposition (\u200bSVD\u200b). Consider the TF matrix for SVD. You can also perform SVD on the TF-IDF matrix.","57420e2b":"## Performing Some Basic EDA"}}