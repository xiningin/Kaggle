{"cell_type":{"ad2d0df6":"code","b05b1f6f":"code","86e20bab":"code","45ea8fa4":"code","1e81ba01":"code","1ec8daa0":"code","5c96df44":"code","ce26dd1e":"code","226c4d94":"code","5ccdce79":"code","b232ee15":"code","42dae458":"code","37acfda2":"code","dd2d2737":"code","ce9c6a60":"code","7c06f79d":"code","8c3c9038":"code","f5b86b51":"code","8f6f831e":"code","3d35e41d":"code","17ed44c6":"code","b034a5b6":"code","8f6052f7":"code","0592eb4e":"code","cf628c27":"code","8debb45e":"code","a9e08f2a":"code","48e04816":"code","c111f792":"code","4ad72dbc":"code","6bed4207":"code","187b95e0":"code","599e6bca":"code","95831b50":"code","73e8306e":"code","8ad0bbd2":"code","75b4b74b":"code","cf363980":"code","066f5956":"code","c122eb80":"code","e0814df2":"code","fcfe19b4":"code","e22adcd4":"code","15510974":"code","0e86dfac":"code","cf9bebc6":"code","28891eac":"code","042f42e4":"code","4f96bef0":"code","3ea8f921":"code","b7c1727f":"code","8ab8fe5b":"code","86cd8cd7":"code","abbc1c39":"code","c9d2aada":"code","89de71e5":"code","b792a83a":"code","8d9981ff":"code","19b1f5b8":"code","4b39ebf7":"code","b8d4436c":"code","c81cc666":"code","8eeeb3cb":"code","ef10cc92":"code","35c58497":"code","d3a97a0b":"code","55314633":"code","1dd54fcb":"code","031d3531":"code","05961de4":"code","b85f08c8":"code","2bf07b23":"code","240f0cac":"code","bc242858":"code","a95142f4":"code","f7fb9ab4":"code","4ae79b32":"code","16cc8d3b":"code","57db0a15":"code","95c5baac":"code","dc99b11b":"code","e0949f8c":"code","d15e9c65":"code","e51be872":"code","85510710":"code","3c7baffd":"code","0a8d69cf":"code","db28cf2f":"code","8a726839":"code","539bea77":"code","701a1b4f":"code","35400fe7":"code","e2d20371":"code","eeb328fc":"code","1640ad76":"code","1e220507":"code","7b579122":"code","a801c9a8":"code","291cd9ff":"code","1a1952f8":"code","a0a1f7e8":"code","05e1967b":"code","d4c08bb8":"code","9f503cad":"code","daec94b9":"code","da87e9a7":"code","47523315":"code","65ac8a63":"code","a9084358":"code","fad92ebe":"code","f6763dcd":"code","5b72449a":"code","659053bc":"code","a2983793":"code","b3108bed":"code","0eb4988f":"code","1f65aac2":"code","dd86f87c":"markdown","81645507":"markdown","71e08d35":"markdown","f98c7395":"markdown","bd13eabd":"markdown","b2a6804e":"markdown","65fd8b15":"markdown","2e152984":"markdown","7a2b62b9":"markdown","b8eaeb6e":"markdown","69f236bf":"markdown","d0c7d58e":"markdown","af81dfa2":"markdown","98ee3a45":"markdown","dcd61063":"markdown","d1ceb775":"markdown","9280a152":"markdown","8aa36f4f":"markdown","ff7321bb":"markdown"},"source":{"ad2d0df6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b05b1f6f":"import warnings\nwarnings.filterwarnings('ignore')","86e20bab":"titanic = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntitanic.head()\n## checking the head of our data set","45ea8fa4":"titanic.info()\n## checking info of all columns","1e81ba01":"titanic.shape\n## checking shape of data set","1ec8daa0":"titanic.describe()\n## statistical information about numerical variable","5c96df44":"round(100*(titanic.isnull().sum()\/len(titanic)),2)\n## checking missing value percentage in all columns","ce26dd1e":"titanic.drop('Cabin',axis=1,inplace=True)\n## cabin almost have 77% of missing values hence remove this column from data set","226c4d94":"age_median = titanic['Age'].median(skipna=True)\ntitanic['Age'].fillna(age_median,inplace=True)\n## as there is 19% of missing values in age column hence it is not a good idea to remove this row wise or column wise hence impute those missing values with the median of age \n","5ccdce79":"titanic = titanic[titanic['Embarked'].isnull()!=True]\n## as embarked has a very small amount of missing values hence remove those rows which have missing values in embarked column \n","b232ee15":"titanic.shape\n## checking shape after removing null values","42dae458":"titanic_dub = titanic.copy()\n## creating copy of the data frame to check duplicate values","37acfda2":"titanic_dub.shape\n## comparing shapes of two data frames","dd2d2737":"titanic.shape\n## shape of original data frame","ce9c6a60":"import seaborn as sns\nimport matplotlib.pyplot as plt\n## importing libraries for data visualitation","7c06f79d":"plt.figure(figsize=(15,5), dpi=80)\nplt.subplot(1,4,1)\nsns.boxplot(y=titanic['Age'])\nplt.title(\"Outliers in 'Age'\")\n\nplt.subplot(1,4,2)\nax = sns.boxplot(y=titanic['Fare'])\nax.set_yscale('log')\nplt.title(\"Outliers in 'Fare'\")\n\nplt.subplot(1,4,3)\nsns.boxplot(y=titanic['SibSp'])\nplt.title(\"Outliers in 'SibSp'\")\n\n\nplt.subplot(1,4,4)\nsns.boxplot(y=titanic['Parch'])\nplt.title(\"Outliers in 'Parch'\")\n#ax.set_yscale('log')\nplt.tight_layout()\nplt.show()\n\n## plotting all four variables to check for outliers\n## it clearly shows that all four variables has some outliers","8c3c9038":"\n\nsns.catplot(x=\"SibSp\", col = 'Survived', data=titanic, kind = 'count', palette='pastel')\nsns.catplot(x=\"Parch\", col = 'Survived', data=titanic, kind = 'count', palette='pastel')\nplt.tight_layout()\nplt.show()\n\n## plotting of sibsp and parch in basis of survived and not survived","f5b86b51":"def alone(x):\n    if (x['SibSp']+x['Parch']>0):\n        return (1)\n    else:\n        return (0)\ntitanic['Alone'] = titanic.apply(alone,axis=1)\n## creating a function to make one variable which tells us whether a person is single or accompanied by some on the ship","8f6f831e":"sns.catplot(x=\"Alone\", col = 'Survived', data=titanic, kind = 'count', palette='pastel')\nplt.show()","3d35e41d":"## drop parch and sibsp\ntitanic = titanic.drop(['Parch','SibSp'],axis=1)\ntitanic.head()\n","17ed44c6":"sns.distplot(titanic['Fare'])\nplt.show()","b034a5b6":"titanic['Fare'] = titanic['Fare'].map(lambda x: np.log(x) if x>0 else 0)\n## converting fare into a logarithmic scale","8f6052f7":"sns.distplot(titanic['Fare'])\nplt.show()\n## again check the distribution of fare ","0592eb4e":"sns.catplot(x=\"Sex\", y=\"Survived\", col=\"Pclass\", data=titanic, saturation=.5, kind=\"bar\", ci=None, aspect=0.8, palette='deep')\nsns.catplot(x=\"Sex\", y=\"Survived\", col=\"Embarked\", data=titanic, saturation=.5, kind=\"bar\", ci=None, aspect=0.8, palette='deep')\nplt.show()\n\n## plotting of survive on basis of pclass","cf628c27":"survived_0 = titanic[titanic['Survived']==0]\nsurvived_1 = titanic[titanic['Survived']==1]\n## divided our dataset into survived or not survived to check the distribution of age in both the cases ","8debb45e":"survived_0.shape\n## checking shape of the data set that contains the data of passengers who not survived","a9e08f2a":"survived_1.shape\n## checking shape of the data set that contains the data of passengers who survived","48e04816":"sns.distplot(survived_0['Age'])\nplt.show()\n## checking distribution of age in not survived data set","c111f792":"sns.distplot(survived_1['Age'])\nplt.show()\n## checking distribution of age in survived dataset","4ad72dbc":"sns.boxplot(x='Survived',y='Fare',data=titanic)\nplt.show()\n## checking survival rate on basis of fare","6bed4207":"Pclass_dummy = pd.get_dummies(titanic['Pclass'],prefix='Pclass',drop_first=True)\nPclass_dummy.head()\n## creating dummy variables for pclass\n\n","187b95e0":"## joing dummy variables\ntitanic = pd.concat([titanic,Pclass_dummy],axis=1)\ntitanic.head()","599e6bca":"titanic.drop('Pclass',axis=1,inplace=True)\n## as there is no use of pclass after joining the columns that contains dummy variables  for pclass","95831b50":"Embarked_dummy = pd.get_dummies(titanic['Embarked'],drop_first=True)\nEmbarked_dummy.head()\n## creating dummy variables for embarked and dropping first column","73e8306e":"titanic = pd.concat([titanic,Embarked_dummy],axis=1)\ntitanic.drop('Embarked',axis=1,inplace=True)\n## joining dummy variables","8ad0bbd2":"titanic.head()\n## checking head of the data set after joining dummy variables","75b4b74b":"def sex_map(x):\n    if x == 'male':\n        return (1)\n    elif x == 'female':\n        return (0)\ntitanic['Sex'] = titanic['Sex'].apply(lambda x:sex_map(x))\n\n## creating function for convert sex into binary values","cf363980":"from sklearn.preprocessing import StandardScaler\n## import libraries for scaling data","066f5956":"scaler = StandardScaler()\ncols = ['Age','Fare']\ntitanic[cols] = scaler.fit_transform(titanic[cols])\ntitanic.head()\n\n## using standardization method of scaling for age and fare variables","c122eb80":"titanic.drop(['Name','Ticket'],axis=1,inplace=True)\n## dropping name and ticket column","e0814df2":"titanic.head()\n## checking head after converting all values","fcfe19b4":"titanic.set_index('PassengerId')\n## set index as passengerid\n","e22adcd4":"## creating heatmap for checking corelations of variables\nsns.heatmap(titanic.corr(),annot=True)\nplt.show()","15510974":"titanic.drop(['Pclass_2','Q'],axis=1,inplace=True)\n## removing highly co related dummy variables pclass_2 and q","0e86dfac":"sns.heatmap(titanic.corr(),annot=True)\nplt.show()\n## again checking co relations of variables","cf9bebc6":"y_train = titanic.pop('Survived')\nX_train = titanic\n## divided train data into x and y as independent and dependent variable","28891eac":"X_train = titanic[['Sex','Age','Fare','Alone','Pclass_3','S']]\nX_train.head()\n## selectig all columns insted of passengerid for our x\n## checking head of x after that","042f42e4":"import statsmodels.api as sm\n## import stats model to build our first model","4f96bef0":"logm1 = sm.GLM(y_train,sm.add_constant(X_train),family = sm.families.Binomial())\nres1 = logm1.fit()\nres1.summary()\n","3ea8f921":"from statsmodels.stats.outliers_influence import variance_inflation_factor","b7c1727f":"vif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","8ab8fe5b":"## removing alone as it has high p value \nX_train.drop('Alone',axis=1,inplace=True)","86cd8cd7":"logm2 = sm.GLM(y_train,sm.add_constant(X_train),family = sm.families.Binomial())\nres2 = logm2.fit()\nres2.summary()","abbc1c39":"vif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","c9d2aada":"X_train.drop('Fare',axis=1,inplace=True)","89de71e5":"logm3 = sm.GLM(y_train,sm.add_constant(X_train),family = sm.families.Binomial())\nres3 = logm3.fit()\nres3.summary()","b792a83a":"vif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","8d9981ff":"X_train.columns","19b1f5b8":"y_train_pred = res3.predict(sm.add_constant(X_train))\n","4b39ebf7":"y_train_pred[:10]","b8d4436c":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","c81cc666":"final_pred = pd.DataFrame({'Survived':y_train.values,'Survived_prob':y_train_pred})\nfinal_pred['PassengerId'] = np.arange(1,len(final_pred)+1)\nfinal_pred.head()\n","8eeeb3cb":"final_pred.info()","ef10cc92":"final_pred['predicted'] = final_pred['Survived_prob'].apply(lambda x: 1 if x>0.5 else 0)\nfinal_pred.head()","35c58497":"from sklearn import metrics","d3a97a0b":"confusion = metrics.confusion_matrix(final_pred.Survived,final_pred.predicted)\nprint(confusion)","55314633":"metrics.accuracy_score(final_pred.Survived,final_pred.predicted)","1dd54fcb":"## lets define all values of confusion matrix\nTN = confusion[0,0]\nFP = confusion[0,1]\nFN = confusion[1,0]\nTP = confusion[1,1]","031d3531":"## lets calculate sensitivity\nTP\/float(TP+FN)","05961de4":"## lets calculate specificity\nTN\/float(TN+FP)","b85f08c8":"## false positive rate \nFP\/ float(TN+FP)","2bf07b23":"## positive predictive value\nTP \/ float(TP+FP)","240f0cac":"## negative predictive value \nTN \/ float(TN+ FN)","bc242858":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","a95142f4":"fpr, tpr, thresholds = metrics.roc_curve( final_pred.Survived, final_pred.Survived_prob, drop_intermediate = False )","f7fb9ab4":"draw_roc(final_pred.Survived, final_pred.Survived_prob)","4ae79b32":"numbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    final_pred[i]= final_pred.Survived_prob.map(lambda x: 1 if x > i else 0)\nfinal_pred.head()","16cc8d3b":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(final_pred.Survived, final_pred[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","57db0a15":"cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","95c5baac":"final_pred['final_predicted'] = final_pred['Survived_prob'].apply(lambda x: 1 if x>0.3 else 0)\nfinal_pred.head()","dc99b11b":"final_confusion = metrics.confusion_matrix(final_pred.Survived,final_pred.final_predicted)\nprint(final_confusion)","e0949f8c":"metrics.accuracy_score(final_pred.Survived,final_pred.final_predicted)","d15e9c65":"## lets define all values of confusion matrix\nTN = final_confusion[0,0]\nFP = final_confusion[0,1]\nFN = final_confusion[1,0]\nTP = final_confusion[1,1]","e51be872":"## lets calculate sensitivity\nTP\/float(TP+FN)","85510710":"## lets calculate specificity\nTN\/float(TN+FP)","3c7baffd":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","0a8d69cf":"# Positive predictive value \nprint (TP \/ float(TP+FP))","db28cf2f":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","8a726839":"titanic_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntitanic_test.head()","539bea77":"titanic_test.info()","701a1b4f":"titanic_test['Sex'] = titanic_test['Sex'].apply(lambda x:sex_map(x))\ntitanic_test.head()","35400fe7":"titanic_test.drop(['Name','SibSp','Parch','Ticket','Cabin'],axis=1,inplace=True)","e2d20371":"titanic_test.head()","eeb328fc":"Pclass = pd.get_dummies(titanic_test['Pclass'],prefix = 'Pclass')\nPclass.head()","1640ad76":"titanic_test = pd.concat([titanic_test,Pclass],axis=1)\n","1e220507":"titanic_test.head()","7b579122":"Embarked = pd.get_dummies(titanic_test['Embarked'])","a801c9a8":"titanic_test = pd.concat([titanic_test,Embarked],axis=1)","291cd9ff":"titanic_test.head()","1a1952f8":"titanic_test.drop(['Pclass','Embarked','Pclass_1','Pclass_2','C','Q'],axis=1,inplace=True)","a0a1f7e8":"titanic_test[['Age','Fare']] = scaler.transform(titanic_test[['Age','Fare']])","05e1967b":"titanic_test.drop('Fare',axis=1,inplace=True)","d4c08bb8":"age_median = titanic_test['Age'].median(skipna=True)\ntitanic_test['Age'].fillna(age_median,inplace=True)","9f503cad":"titanic_test.info()","daec94b9":"X_test = titanic_test[['Sex', 'Age', 'Pclass_3', 'S']]","da87e9a7":"X_test.columns","47523315":"y_test_pred = res3.predict(sm.add_constant(X_test))","65ac8a63":"y_test_pred.head()","a9084358":"test_final = pd.DataFrame({'PassengerId': titanic_test.PassengerId,'Survived_prob':y_test_pred.values})\ntest_final.head()","fad92ebe":"test_final['Survived'] = test_final['Survived_prob'].apply(lambda x:1 if x>0.3 else 0)\ntest_final.head()","f6763dcd":"test_final.drop('Survived_prob',axis=1,inplace = True)","5b72449a":"test_final.to_csv(\"prediction_titanic.csv\",index=False)","659053bc":"final_pred.head()","a2983793":"ks_stat_check = final_pred.iloc[ : ,[1,14]]","b3108bed":"ks_stat_check.shape","0eb4988f":"## using function for calculate ks statistics\ndef ks(data=None,target=None, prob=None):\n    data['target0'] = 1 - data[target]\n    data['bucket'] = pd.qcut(data[prob], 10)\n    grouped = data.groupby('bucket', as_index = False)\n    kstable = pd.DataFrame()\n    kstable['min_prob'] = grouped.min()[prob]\n    kstable['max_prob'] = grouped.max()[prob]\n    kstable['events']   = grouped.sum()[target]\n    kstable['nonevents'] = grouped.sum()['target0']\n    kstable = kstable.sort_values(by=\"min_prob\", ascending=False).reset_index(drop = True)\n    kstable['event_rate'] = (kstable.events \/ data[target].sum()).apply('{0:.2%}'.format)\n    kstable['nonevent_rate'] = (kstable.nonevents \/ data['target0'].sum()).apply('{0:.2%}'.format)\n    kstable['cum_eventrate']=(kstable.events \/ data[target].sum()).cumsum()\n    kstable['cum_noneventrate']=(kstable.nonevents \/ data['target0'].sum()).cumsum()\n    kstable['KS'] = np.round(kstable['cum_eventrate']-kstable['cum_noneventrate'], 3) * 100\n\n    #Formating\n    kstable['cum_eventrate']= kstable['cum_eventrate'].apply('{0:.2%}'.format)\n    kstable['cum_noneventrate']= kstable['cum_noneventrate'].apply('{0:.2%}'.format)\n    kstable.index = range(1,11)\n    kstable.index.rename('Decile', inplace=True)\n    pd.set_option('display.max_columns', 9)\n    print(kstable)\n    \n    #Display KS\n    from colorama import Fore\n    print(Fore.RED + \"KS is \" + str(max(kstable['KS']))+\"%\"+ \" at decile \" + str((kstable.index[kstable['KS']==max(kstable['KS'])][0])))\n    return(kstable)","1f65aac2":"mydf = ks(data=ks_stat_check,target=\"final_predicted\", prob=\"Survived_prob\")\n","dd86f87c":"# **Data Quality Check**\n\nhandling missing values as well\n","81645507":"# plotting ROC curve","71e08d35":"checking accurecy of the model","f98c7395":"no use of passengerid name and ticket column in our prediction","bd13eabd":"sibsp and parch basically tells us that whether a person is accompanied by someone else or not \nso we can make two category by merging them to find whether a single person is acompanied by some one else or not ","b2a6804e":"those who are survived paid more fares","65fd8b15":"# creating dummy variables","2e152984":"# checking for outliers","7a2b62b9":"it clearly shows that those person who are not alone survived more","b8eaeb6e":"event means survived (1) and non events means not survived (0)\nthis matrix used to check the discriminatory power of the model","69f236bf":"# finding optimal cutoff point","d0c7d58e":"there are some highly co related dummy variables hence removing such columns","af81dfa2":"# duplicate check","98ee3a45":"females are more likely to be survived","dcd61063":"our final model is done","d1ceb775":"# checking model validation using ks statistics","9280a152":"It stands for Kolmogorov\u2013Smirnov which is named after Andrey Kolmogorov and Nikolai Smirnov. It compares the two cumulative distributions and returns the maximum difference between them. It is a non-parametric test which means you don't need to test any assumption related to the distribution of data. In KS Test, Null hypothesis states null both cumulative distributions are similar. Rejecting the null hypothesis means cumulative distributions are different.\nIn data science, it compares the cumulative distribution of events and non-events and KS is where there is a maximum difference between the two distributions. In simple words, it helps us to understand how well our predictive model is able to discriminate between events and non-events.","8aa36f4f":"young persons are survived more (age group between 20-40)","ff7321bb":"there is some skewness in the fare column \nhence removing the skewness using log function"}}