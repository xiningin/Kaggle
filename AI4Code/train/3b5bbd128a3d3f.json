{"cell_type":{"63f0b099":"code","aaccc228":"code","0d88a283":"code","9727e8ef":"code","2e003fff":"code","9205a521":"code","939d87fb":"code","c160e273":"code","83560c80":"code","d407ece7":"code","eafe4714":"code","8898d212":"code","9bec34ca":"code","6e9e94aa":"code","4798597d":"code","e1524b0b":"markdown","d9dadd5d":"markdown","93813e03":"markdown","ccacd051":"markdown","0666c863":"markdown"},"source":{"63f0b099":"from keras.layers import Dense, LSTM, Embedding, Activation\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import RMSprop\nfrom transformers import BertTokenizer\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')","aaccc228":"train_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntrain_df.head()","0d88a283":"tweets = train_df['text'].values\nfor i in range(5):\n    print('{} : {}'.format(i, tweets[i]))","9727e8ef":"tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')","2e003fff":"def encode_sentence(s):\n    tokens = list(tokenizer.tokenize(s))\n    tokens.append('[SEP]')\n    return tokenizer.convert_tokens_to_ids(tokens)","9205a521":"max_len = 50\nx_train = []\nfor tweet in tweets:\n    vec = encode_sentence(tweet)\n    x_train.append(vec[:max_len] + [0] * (max_len - len(vec)))","939d87fb":"x_train = np.array(x_train)\nn = np.amax(x_train)\nprint(x_train.shape)","c160e273":"y_train = train_df['target'].values\ny_train = np.array(y_train)\nprint(y_train.shape)","83560c80":"EPOCHS = 15\nBATCH_SIZE = 32\nmodel=Sequential()\nmodel.add(Embedding(n + 1, BATCH_SIZE, mask_zero=True))\nmodel.add(LSTM(BATCH_SIZE))\nmodel.add(Dense(2, activation = 'sigmoid'))\noptimizer = RMSprop(lr = 0.01)\nmodel.compile(loss = 'sparse_categorical_crossentropy', optimizer = optimizer)\nmodel.summary()","d407ece7":"model.fit(x_train, y_train, epochs = EPOCHS, batch_size = BATCH_SIZE)","eafe4714":"test_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntest_df.head()","8898d212":"tweets = test_df['text'].values\nx_test = []\nfor tweet in tweets:\n    vec = encode_sentence(tweet)\n    x_test.append(vec[:max_len] + [0] * (max_len - len(vec)))","9bec34ca":"y_test = [np.argmax(model.predict(np.array([x_test_]))) for x_test_ in x_test]","6e9e94aa":"sub = pd.DataFrame({'id':test_df['id'].values, 'target':y_test})\nsub.head()","4798597d":"sub.to_csv('.\/submission.csv', index = False)","e1524b0b":"**Read dataset**","d9dadd5d":"**Predict answer**","93813e03":"# **Tweet analysis using LSTM**","ccacd051":"**Read dataset**","0666c863":"**Vectorize word**"}}