{"cell_type":{"647cf14a":"code","a1114272":"code","0d8dfd58":"code","fe0c2174":"code","4d54b526":"code","5b092b9c":"code","8f996733":"code","0826b4e9":"code","c8e9196e":"code","580474da":"code","2cf4a836":"code","1a7c538f":"code","5e17494e":"code","67156cc0":"code","e9052e1a":"code","bf13b732":"code","479a26df":"markdown","422e60fa":"markdown","2eb79d1e":"markdown","c72f140a":"markdown","a8ea6e77":"markdown","47acdf5a":"markdown","b4d925f9":"markdown","a44bd772":"markdown"},"source":{"647cf14a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os","a1114272":"from tensorflow.keras import Model\nfrom tensorflow.keras.layers import Flatten, Dense, GlobalAveragePooling2D\nfrom tensorflow.keras import optimizers\nfrom keras_preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import callbacks\nfrom tqdm import tqdm\nimport cv2\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport seaborn as sns\n\n!pip install efficientnet\nimport efficientnet.tfkeras as efn","0d8dfd58":"# Dataset parameters:\nINPUT_DIR = os.path.join('..', 'input')\n\nDATASET_DIR = os.path.join(INPUT_DIR, 'landmark-recognition-2020')\nTRAIN_IMAGE_DIR = os.path.join(DATASET_DIR, 'train')\nTEST_IMAGE_DIR = os.path.join(DATASET_DIR, 'test')\nTRAIN_CSV_PATH = os.path.join(DATASET_DIR, 'train.csv')\n\nSUBMISSION_PATH = os.path.join(DATASET_DIR, 'sample_submission.csv')\n","fe0c2174":"TRAIN_DF = pd.read_csv(TRAIN_CSV_PATH)\nprint(f'TRAIN SIZE : {len(TRAIN_DF)}')","4d54b526":"TRAIN_DF['landmark_id'].value_counts()","5b092b9c":"TRAIN_DF.head(10)","8f996733":"train_df2 = pd.DataFrame(TRAIN_DF['landmark_id'].value_counts())\ntrain_df2.reset_index(inplace=True)\ntrain_df2.columns = ['landmark_id','count']\ntrain_df2","0826b4e9":"import matplotlib as mpl\nimport matplotlib.pylab as plt\n\nplt.figure(figsize = (14, 10))\nplt.title('Top 20 landmarks')\ng = sns.barplot(x=\"landmark_id\", y=\"count\", data=train_df2[:20], palette=\"pastel\")\n\n\nplt.show()","c8e9196e":"TEST_DF = pd.read_csv(SUBMISSION_PATH)\nprint(f'TEST SIZE : {len(TEST_DF)}')","580474da":"labels = []\ndata = []\nfor i in range(TRAIN_DF.shape[0]):\n    data.append(TRAIN_IMAGE_DIR + '\/' + TRAIN_DF['id'].iloc[i][0] + '\/' + TRAIN_DF['id'].iloc[i][1] + '\/' + TRAIN_DF['id'].iloc[i][2] + '\/' + TRAIN_DF['id'].iloc[i]+'.jpg')\n    labels.append(TRAIN_DF['landmark_id'].iloc[i])\n\ndf = pd.DataFrame(data)\ndf.columns = ['images']\ndf['target'] = labels \n    \n\ntest_data = []\nfor i in range(TEST_DF.shape[0]):\n    test_data.append(TEST_IMAGE_DIR + '\/' + TEST_DF['id'].iloc[i][0] + '\/' + TEST_DF['id'].iloc[i][1] + '\/' + TEST_DF['id'].iloc[i][2] + '\/' + TEST_DF['id'].iloc[i]+'.jpg')\n\ndf_test = pd.DataFrame(test_data)\ndf_test.columns = ['images']    \n\nX_train, X_val, y_train, y_val = train_test_split(df['images'], df['target'], test_size=0.1, random_state=1234)\n\ntrain=pd.DataFrame(X_train)\ntrain.columns=['images']\ntrain['target']=y_train\n\nvalidation=pd.DataFrame(X_val)\nvalidation.columns=['images']\nvalidation['target']=y_val","2cf4a836":"df2 = df[df['target']==66]\ndf2","1a7c538f":"import matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(70,70))\nrows = 1\ncols = len(df2)\n\n\nfor i in range(len(df2)):\n\n    img1 = cv2.imread(df2['images'].iloc[i])\n\n    ax1 = fig.add_subplot(rows, len(df2), i+1)\n    ax1.imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n    ax1.set_title(df2['target'].iloc[i])\n    ax1.axis(\"off\")\n\n \nplt.show()\nprint(f'size : {len(df2)}')","5e17494e":"def create_model(num_classes=None, input_size=224):\n    model = efn.EfficientNetB3(weights='noisy-student', include_top=False, input_shape=(input_size, input_size, 3))\n    x = GlobalAveragePooling2D()(model.output)\n    output = Dense(num_classes, activation='softmax')(x)\n    model = Model(model.input, output)\n\n    return model\n\n\ndef lr_scheduler(epoch, lr):\n    if epoch == 3 or epoch == 8:\n        return lr * 0.1\n    else:\n        return lr","67156cc0":"train_datagen = ImageDataGenerator(rescale=1.\/255,\n                                   zoom_range=[1.0, 1.2],\n                                   brightness_range=[0.8, 1.1],\n                                   width_shift_range=0.1,\n                                   height_shift_range=0.1,\n                                  )\nval_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator = train_datagen.flow_from_dataframe(\n    train,\n    x_col='images',\n    y_col='target',\n    target_size=(224, 224),\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    class_mode='raw')\n\nvalidation_generator = val_datagen.flow_from_dataframe(\n    validation,\n    x_col='images',\n    y_col='target',\n    target_size=(224, 224),\n    shuffle=False,\n    batch_size=BATCH_SIZE,\n    class_mode='raw')","e9052e1a":"model = create_model(8226, 224)\n#model.summary()\n\nopt = optimizers.Adam(lr=1e-4)\nmodel.compile(\n    loss = 'sparse_categorical_crossentropy',\n    metrics=['sparse_categorical_accuracy'],\n    optimizer=opt)\n\nnb_train_steps = train.shape[0]\/\/BATCH_SIZE\nnb_val_steps = validation.shape[0]\/\/BATCH_SIZE\n\nlearning_rate_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch=nb_train_steps,\n    epochs=EPOCHS,\n    validation_data=validation_generator,\n    callbacks=[learning_rate_callback],\n    validation_steps=nb_val_steps)","bf13b732":"for path in tqdm(df_test['images']):\n    img= cv2.imread(str(path))\n    img = cv2.resize(img, (224,224))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = img.astype(np.float32)\/255.\n    img=np.reshape(img,(1,224,224,3))\n    prediction=model.predict(img)\n    target.append(prediction[0][0])\n\nsubmission['target']=target\nsubmission.to_csv('submission.csv', index=False)","479a26df":"### Select landmark id & show data","422e60fa":"### Test Image Size","2eb79d1e":"### Count per landmark","c72f140a":"### Visualize image of selected randmark id\n#### Double-click the image to see a larger image.","a8ea6e77":"## Model - efficientNet B3","47acdf5a":"## Predict","b4d925f9":"#### This notebook was written for beginners.\n#### I want to perform data analysis. Check the number of train data and test data, and find out the distribution of each class. And print the image of each class.","a44bd772":"### Train image size"}}