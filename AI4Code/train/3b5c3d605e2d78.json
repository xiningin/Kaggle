{"cell_type":{"85fcb549":"code","8d27a8f0":"code","4478e704":"code","ed0fff3c":"code","aeba91ef":"code","c595c129":"code","016b0252":"code","0a99c257":"code","a6c4c8d1":"code","a6e80a83":"code","24c58a8f":"code","c9c78226":"code","e8bd51c7":"code","d9957c82":"code","aae7697b":"code","c17e29e8":"code","91be8779":"code","f3f479a0":"code","026c6a6f":"code","05939b54":"code","006b2960":"code","182b2776":"code","54ecd2ac":"code","5b6a67df":"code","467cdc32":"code","381a3846":"code","60e2409e":"code","fa16362b":"code","ccf5e284":"code","c750d28c":"code","1d0ce9d1":"code","9489fc02":"code","89682f75":"code","303ec3e8":"code","98153782":"code","f50ae81c":"code","38a73484":"code","8c8cfbbf":"code","ff173c2a":"code","9e0afdc5":"code","64b25efb":"code","d9c0e080":"code","81bdc9f9":"code","d74e0a61":"code","83a6377a":"markdown","009b23f4":"markdown","7bf29443":"markdown","ef691572":"markdown","3711499b":"markdown","b9231e9f":"markdown","ba521724":"markdown","4bf6a4cc":"markdown","0e011a4e":"markdown","f8dfb427":"markdown","88818813":"markdown","33b483e3":"markdown","5a5a2ad0":"markdown","002046ab":"markdown","aa1457c3":"markdown","8bc5a22a":"markdown","442b56af":"markdown","8262d2b0":"markdown","5e7ce708":"markdown","94118cbc":"markdown","f0b78c4b":"markdown","8682937e":"markdown","79c8cd6e":"markdown","967b0046":"markdown","7f533aa8":"markdown","add1295a":"markdown","22b279d5":"markdown","482dda57":"markdown","d1a580b7":"markdown","ba3b6863":"markdown","54e73b4f":"markdown","9832c77c":"markdown"},"source":{"85fcb549":"import os\nimport sys \nimport json\nimport glob\nimport random\nimport collections\nimport time\nimport re\nimport math\nimport numpy as np\nimport pandas as pd\nimport cv2\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation, rc\nimport seaborn as sns\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\n\nfrom random import shuffle\nfrom sklearn import model_selection as sk_model_selection\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom keras.metrics import AUC\n\nimport tensorflow as tf","8d27a8f0":"# Global params for animations\nrc('animation', html='jshtml')","4478e704":"data_directory = '..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification'\npytorch3dpath = \"..\/input\/efficientnetpyttorch3d\/EfficientNet-PyTorch-3D\"","ed0fff3c":"train_df = pd.read_csv(data_directory+\"\/train_labels.csv\")\ntrain_df['BraTS21ID5'] = [format(x, '05d') for x in train_df.BraTS21ID]\ntrain_df.head(3)","aeba91ef":"test = pd.read_csv(\n    data_directory+'\/sample_submission.csv')\n\ntest['BraTS21ID5'] = [format(x, '05d') for x in test.BraTS21ID]\ntest.head(3)","c595c129":"sns.set_style(\"whitegrid\")\nfig = plt.figure(figsize=(8,6))\n# Countplot with Seaborn\nax = sns.countplot(data=train_df,\n                   x=\"MGMT_value\")\n# Annotating bars\nfor p in ax.patches:\n    ax.annotate(\n        format(p.get_height(), '.0f'), \n               (p.get_x() + p.get_width() \/ 2., p.get_height()),\n        ha = 'center', va = 'center', \n        xytext = (0, 10), \n        textcoords = 'offset points')\n\nsns.despine(left=True, bottom=True)\nplt.title(\"MGMT value distribution in train labels\\n\",\n          fontsize=18, color=\"#0b0a2d\")\nplt.show()","016b0252":"def load_dicom(path):\n    dicom = pydicom.read_file(path)\n    data = dicom.pixel_array\n    data = data - np.min(data)\n    if np.max(data) != 0:\n        data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n    return data\n\ndef visualize_sample(\n    brats21id, \n    slice_i,\n    mgmt_value,\n    types=(\"FLAIR\", \"T1w\", \"T1wCE\", \"T2w\")):\n    \n    plt.figure(figsize=(20, 6))\n    patient_path = os.path.join(\n        data_directory+\"\/train\/\", \n        str(brats21id).zfill(5),\n    )\n    for i, t in enumerate(types, 1):\n        t_paths = sorted(\n            glob.glob(os.path.join(patient_path, t, \"*\")), \n            key=lambda x: int(x[:-4].split(\"-\")[-1]),\n        )\n        data = load_dicom(t_paths[int(len(t_paths) * slice_i)])\n        plt.subplot(1, 4, i)\n        plt.imshow(data, cmap=\"gray\")\n        plt.title(f\"{t}\", fontsize=10)\n        plt.axis(\"off\")\n    plt.show()","0a99c257":"list0=[315,176,153,164]\nfor i in list0:\n    _brats21id = train_df.iloc[i][\"BraTS21ID\"]\n    _mgmt_value = train_df.iloc[i][\"MGMT_value\"]\n    visualize_sample(brats21id=_brats21id, mgmt_value=_mgmt_value, slice_i=0.55)","a6c4c8d1":"list1=[184,315,155,228]\nfor i in list1:\n    _brats21id = train_df.iloc[i][\"BraTS21ID\"]\n    _mgmt_value = train_df.iloc[i][\"MGMT_value\"]\n    visualize_sample(brats21id=_brats21id, mgmt_value=_mgmt_value, slice_i=0.55)","a6e80a83":"def create_animation(ims):\n    fig = plt.figure(figsize=(10, 10))\n    plt.axis('off')\n    im = plt.imshow(ims[0], cmap=\"gray\")\n\n    def animate_func(i):\n        im.set_array(ims[i])\n        #return [im]\n    return animation.FuncAnimation(fig, animate_func, frames = len(ims), interval = 1000\/\/4)\n\ndef load_dicom_line(path):\n    t_paths = sorted(\n        glob.glob(os.path.join(path, \"*\")), \n        key=lambda x: int(x[:-4].split(\"-\")[-1]),\n    )\n    images = []\n    for filename in t_paths:\n        data = load_dicom(filename)\n        if data.max() == 0:\n            continue\n        images.append(data)\n        \n    return images","24c58a8f":"images = load_dicom_line(data_directory+\"\/train\/00176\/FLAIR\")\nanm_FLAIR0=create_animation(images)\nimages = load_dicom_line(data_directory+\"\/train\/00176\/T1w\")\nanm_T1W0=create_animation(images)\n\n\nimages = load_dicom_line(data_directory+\"\/train\/00184\/FLAIR\")\nanm_FLAIR1=create_animation(images)\nimages = load_dicom_line(data_directory+\"\/train\/00184\/T1w\")\nanm_T1W1=create_animation(images)","c9c78226":"anm_FLAIR0","e8bd51c7":"anm_T1W0","d9957c82":"anm_FLAIR1","aae7697b":"anm_T1W1","c17e29e8":"mri_types = ['FLAIR','T1w','T1wCE','T2w']\n\ntrain_dataset = train_df.copy()\nfor scan in mri_types:\n    train_dataset[scan + \"_count\"] = [\n        len(os.listdir(data_directory + \"\/train\/\"\n                       + str(p) \n                       + \"\/\" + scan))\n        for p in train_dataset.BraTS21ID5]","91be8779":"fig = plt.figure(figsize = (25,40))\nfor i, scan in enumerate(mri_types):\n    ax = plt.subplot(4,1,i+1)\n    plt.xticks(rotation=70)\n    sns.countplot(x=train_dataset[scan + \"_count\"], ax=ax)\n    ax.set_xlabel(\"Mean of number of MRI in scan folder\")\n    ax.set_ylabel(\"Count of patients\")\n    ax.set_title(\"Distribution of number of DCM file in {} scans\".format(scan),\n             fontsize=18, color=\"#0b0a2d\")\nplt.show()","f3f479a0":"train_dataset[(train_dataset[\"FLAIR_count\"] == int(train_dataset[\"FLAIR_count\"].mode()))\n              & (train_dataset[\"T1w_count\"] == int(train_dataset[\"T1w_count\"].mode()))\n              & (train_dataset[\"T1wCE_count\"] == int(train_dataset[\"T1wCE_count\"].mode()))\n              & (train_dataset[\"T2w_count\"] == int(train_dataset[\"T2w_count\"].mode()))]","026c6a6f":"test_dataset = test.copy()\nfor scan in mri_types:\n    test_dataset[scan + \"_count\"] = [\n        len(os.listdir(data_directory + \"\/test\/\"\n                       + str(p)\n                       + \"\/\" + scan))\n        for p in test_dataset.BraTS21ID5]","05939b54":"fig = plt.figure(figsize = (25,40))\nfor i, scan in enumerate(mri_types):\n    ax = plt.subplot(4,1,i+1)\n    plt.xticks(rotation=70)\n    sns.countplot(x=test_dataset[scan + \"_count\"], ax=ax)\n    ax.set_xlabel(\"Mean of number of MRI in scan folder\")\n    ax.set_ylabel(\"Count of patients\")\n    ax.set_title(\"Distribution of number of DCM file in TEST {} scans\".format(scan),\n             fontsize=18, color=\"#0b0a2d\")\nplt.show()","006b2960":"# Fixe MRI type to FLAIR\nmri_types_id = 0 # 0,1,2,3\n\n# Initial parameters\nIMAGE_SIZE = 80\nNUM_IMAGES = 64\nBATCH_SIZE = 4\n\nnum_folds = 5\nSelected_fold = 1 #1,2,3,4,5 ","182b2776":"# Preprocessing params\n# Scale for image crop\nSCALE = .8\n# Tile size for CLAHE equalizer\nTILE_SIZE = (8,8)\n# H param for denoising filter\nH_PARAM = 10","54ecd2ac":"train_df[\"Fold\"]=\"train\"\ntrain_df.head(5)","5b6a67df":"test.head(3)","467cdc32":"def mri_preprocessing(img, scale=SCALE, img_size=IMAGE_SIZE, tile_size=TILE_SIZE, h_param=H_PARAM):\n    # Crop image\n    center_x, center_y = img.shape[1] \/ 2, img.shape[0] \/ 2\n    width_scaled, height_scaled = img.shape[1] * scale, img.shape[0] * scale\n    left_x, right_x = center_x - width_scaled \/ 2, center_x + width_scaled \/ 2\n    top_y, bottom_y = center_y - height_scaled \/ 2, center_y + height_scaled \/ 2\n    img = img[int(top_y):int(bottom_y), int(left_x):int(right_x)]\n    img = cv2.resize(img, (img_size, img_size))\n    \n    # CLAHE Equalizer\n    img = np.uint8(cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX))\n    img = cv2.equalizeHist(img)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=tile_size)\n    img = clahe.apply(np.uint8(img))\n    \n    # Non-local mean filter denoising\n    img = cv2.fastNlMeansDenoising(\n        src=img,\n        dst=None,\n        h=h_param,\n        templateWindowSize=7,\n        searchWindowSize=21)\n    \n    return img","381a3846":"sample_img_path = ''.join([data_directory, '\/train\/00176\/FLAIR\/Image-25.dcm'])\nsample_img = pydicom.dcmread(sample_img_path)\nsample_img = sample_img.pixel_array\n\nfig = plt.figure(figsize=(12,6))\nax = plt.subplot(1,2,1)\nax.imshow(sample_img, cmap=\"gray\")\nax.set_title(\"Original image\")\nax1 = plt.subplot(1,2,2)\nimg_preproc = mri_preprocessing(sample_img)\nax1.imshow(img_preproc, cmap=\"gray\")\nax1.set_title(\"Preproceced image\")\nplt.show()","60e2409e":"def load_dicom_image(path, img_size=IMAGE_SIZE, voi_lut=True, rotate=0, preproc=True):\n    dicom = pydicom.read_file(path)\n    data = dicom.pixel_array\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n        \n    if rotate > 0:\n        rot_choices = [0, cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_90_COUNTERCLOCKWISE, cv2.ROTATE_180]\n        data = cv2.rotate(data, rot_choices[rotate])\n        \n    if (preproc==True):\n        data = mri_preprocessing(data)\n        \n    else:\n        data = cv2.resize(data, (img_size, img_size))\n        \n    return data\n\n\ndef load_dicom_images_3d(scan_id, num_imgs=NUM_IMAGES, img_size=IMAGE_SIZE, mri_type=mri_types[mri_types_id], split=\"train\", rotate=0):\n\n    files = sorted(glob.glob(f\"{data_directory}\/{split}\/{scan_id}\/{mri_type}\/*.dcm\"), \n               key=lambda var:[int(x) if x.isdigit() else x for x in re.findall(r'[^0-9]|[0-9]+', var)])\n\n    middle = len(files)\/\/2\n    num_imgs2 = num_imgs\/\/2\n    p1 = max(0, middle - num_imgs2)\n    p2 = min(len(files), middle + num_imgs2)\n    img3d = np.stack([load_dicom_image(f, rotate=rotate) for f in files[p1:p2]]) \n    if img3d.shape[0] < num_imgs:\n        n_zero = np.zeros((num_imgs - img3d.shape[0], img_size, img_size))\n        img3d = np.concatenate((img3d,  n_zero), axis = 0)\n        \n    if np.min(img3d) < np.max(img3d):\n        img3d = img3d - np.min(img3d)\n        img3d = img3d \/ np.max(img3d)\n            \n    return np.expand_dims(img3d,-1)","fa16362b":"a = load_dicom_images_3d(\"00046\")\nimage = a[0]\nprint(a.shape)\nprint(np.min(a), np.max(a), np.mean(a), np.median(a))\nprint(\"Dimension of the CT scan is:\", image.shape)\nplt.imshow(image, cmap=\"gray\")","ccf5e284":"from sklearn.model_selection import KFold,StratifiedKFold\nsfolder = StratifiedKFold(n_splits=5,random_state=13,shuffle=True)\nX = train_df[['BraTS21ID']]\ny = train_df[['MGMT_value']]\n\nfold_no = 1\nfor train, valid in sfolder.split(X,y):\n    if fold_no==Selected_fold:\n        train_df.loc[valid, \"Fold\"] = \"valid\"\n    fold_no += 1","c750d28c":"df_train=train_df[train_df.Fold==\"train\"]\ndf_valid=train_df[train_df.Fold==\"valid\"].iloc[:-1,:]\nprint(\"df_train=\",len(df_train),\"-- df_valid=\",len(df_valid))","1d0ce9d1":"from keras.utils import Sequence\nclass Dataset(Sequence):\n    def __init__(self,df,is_train=True,batch_size=BATCH_SIZE,shuffle=True):\n        self.idx = df[\"BraTS21ID\"].values\n        self.paths = df[\"BraTS21ID5\"].values\n        self.y =  df[\"MGMT_value\"].values\n        self.is_train = is_train\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n    def __len__(self):\n        return math.ceil(len(self.idx)\/self.batch_size)\n   \n    def __getitem__(self,ids):\n        id_path= self.paths[ids]\n        batch_paths = self.paths[ids * self.batch_size:(ids + 1) * self.batch_size]\n        \n        if self.y is not None:\n            batch_y = self.y[ids * self.batch_size: (ids + 1) * self.batch_size]\n        \n        if self.is_train:\n            list_x =  [load_dicom_images_3d(x,split=\"train\") for x in batch_paths]\n            batch_X = np.stack(list_x, axis=0)\n            return batch_X,batch_y\n        else:\n            list_x =  load_dicom_images_3d(id_path,split=\"test\")#str(scan_id).zfill(5)\n            batch_X = np.stack(list_x)\n            return batch_X\n    \n    def on_epoch_end(self):\n        if self.shuffle and self.is_train:\n            ids_y = list(zip(self.idx, self.y))\n            shuffle(ids_y)\n            self.idx, self.y = list(zip(*ids_y))","9489fc02":"train_dataset = Dataset(df_train,batch_size=BATCH_SIZE)\nvalid_dataset = Dataset(df_valid,batch_size=BATCH_SIZE)","89682f75":"for i in range(1):\n    images, label = train_dataset[i]\n    print(\"Dimension of the CT scan is:\", images.shape)\n    print(\"label=\",label)\n    plt.imshow(images[0,32,:,:,:], cmap=\"gray\")\n    plt.show()","303ec3e8":"def get_model(width=IMAGE_SIZE, height=IMAGE_SIZE, depth=NUM_IMAGES):\n    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n\n    inputs = keras.Input(shape=(depth, width, height, 1), batch_size=BATCH_SIZE)\n     \n    x = layers.Conv3D(filters=16, kernel_size=3, activation=\"relu\", padding=\"same\")(inputs)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n    \n    x = layers.Conv3D(filters=32, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n    \n    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.1)(x)\n    \n    x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.1)(x)\n\n    x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.2)(x)\n\n    x = layers.Conv3D(filters=512, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.2)(x)\n\n    x = layers.GlobalAveragePooling3D()(x)\n    x = layers.Dense(units=512, activation=\"relu\")(x)\n\n    outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n\n    # Define the model.\n    model = keras.Model(inputs, outputs, name=\"3dcnn\")\n\n    return model","98153782":"# Build model.\nmodel = get_model(width=IMAGE_SIZE, height=IMAGE_SIZE, depth=NUM_IMAGES)\nmodel.summary()","f50ae81c":"# Compile model.\ninitial_learning_rate = 0.0001\nlr_schedule = keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n)\nmodel.compile(\n    loss=\"binary_crossentropy\",\n    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n    metrics=[AUC(name='auc'),\"acc\"],\n)\n# Define callbacks.\nmodel_save = ModelCheckpoint(f'Brain_3d_cls_{mri_types[mri_types_id]}_Fold_{Selected_fold}.h5', \n                             save_best_only = True, \n                             monitor = 'val_auc', \n                             mode = 'max', verbose = 1)\nearly_stop = EarlyStopping(monitor = 'val_auc', \n                           patience = 5, mode = 'max', verbose = 1,\n                           restore_best_weights = True)","38a73484":"# Train the model, doing validation at the end of each epoch\nepochs = 50\nmodel.fit(\n    train_dataset,\n    validation_data=valid_dataset,\n    epochs=epochs,\n    shuffle=True,\n    verbose=1,\n    callbacks = [model_save, early_stop],\n)","8c8cfbbf":"fig, ax = plt.subplots(1, 3, figsize=(20, 7))\nax = ax.ravel()\n\nfor i, metric in enumerate([\"acc\",\"auc\",\"loss\"]):\n    ax[i].plot(model.history.history[metric])\n    ax[i].plot(model.history.history[\"val_\" + metric])\n    ax[i].set_title(\"Model {}\".format(metric))\n    ax[i].set_xlabel(\"epochs\")\n    ax[i].set_ylabel(metric)\n    ax[i].legend([\"train\", \"val\"])","ff173c2a":"test_dataset = Dataset(test,is_train=False,batch_size=1)\n\n\nfor i in range(1):\n    image = test_dataset[i]\n    print(\"Dimension of the CT scan is:\", image.shape)\n    plt.imshow(image[32,:,:,:], cmap=\"gray\")\n    plt.show()","9e0afdc5":"preds = model.predict(test_dataset)\npreds = preds.reshape(-1)","64b25efb":"preds","d9c0e080":"submission = pd.DataFrame({'BraTS21ID':test['BraTS21ID'],'MGMT_value':preds})\nsubmission","81bdc9f9":"submission.to_csv('submission.csv',index=False)","d74e0a61":"plt.figure(figsize=(5, 5))\nplt.hist(submission[\"MGMT_value\"]);","83a6377a":"## <span style=\"color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold\" id=\"section_2_2\">Keras custom Data Generator<\/span>\nThanks to the Sequence module of the Keras library, we are going to create a personalized image generator. This will prevent us from creating Numpy arrays or Tensors containing all the sequences which would quickly overload the memory.","009b23f4":"### Distribution of images in MRI types :\nWe are now going to check the number of DCM files to check if their number is the same for each category and for each patient. For that, we will complete a copy of train.csv with the calculated informations:","7bf29443":"### *T1w type :*","ef691572":"<h1 style=\"color:#0b0a2d; font-size:24px; text-transform: uppercase; font-weight:bold\">Acknowledgement<\/h1>\n\nThis Notebook is copied and edited from *Ammar Alhaj Ali* work :\n- [\ud83e\udde0Brain Tumor 3D [Training]](https:\/\/www.kaggle.com\/ammarnassanalhajali\/brain-tumor-3d-training)\n- [\ud83e\udde0Brain Tumor 3D [Inference]](https:\/\/www.kaggle.com\/ammarnassanalhajali\/brain-tumor-3d-inference)\n\nIn the first part, we will add to these Notebooks **a complete preprocessing of the images** (crop, equalization, denoising ...) in order to try to improve the results and **reduce the size of images**.","3711499b":"To better understand the representation of these MRI images, we can also **create an animation to visualize the sequence of images** of a certain category for a given patient.","b9231e9f":"### With MGMT = 1\n### *FLAIR type :*","ba521724":"## <span style=\"color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold\" id=\"section_2_2\">Define Folds<\/span>","4bf6a4cc":"Note that some values for each scan category are over-represented. On the other hand, the span ranges of the counters are important. This may be due, for example, to the use of different X-ray machines ...\n\nHowever, if we only consider patients with maximum values, the amount of data available may be too low for a complex machine learning algorithm.","0e011a4e":"# <span style=\"color:#0b0a2d; font-size:24px; text-transform: uppercase; font-weight:bold\" id=\"section_2\">3D CNN from scratch on unique MRI Type<\/span>\n\nIn this section, a convolutional neural network will be trained on a single type of MRI scan. We will take a **sequence of 64 consecutive images in 80x80 pixels** which will allow the scanners to be treated like videos.","f8dfb427":"There are only 151 patients whose scans contain the maximum of DCM images out of the 585 at the start. We therefore keep the entire dataset for the moment. **Lets have a look to the test dataset** :","88818813":"## <span style=\"color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold\" id=\"section_2_4\">Visualizing model performance<span>","33b483e3":"and configure data path :","5a5a2ad0":"## <span style=\"color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold\" id=\"section_2_5\">Submission of result predictions<\/span>","002046ab":"### With MGMT = 1","aa1457c3":"### With MGMT = 0\n### *FLAIR type :*","8bc5a22a":"## <span style=\"color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold\" id=\"section_2_4\">Make predictions with trained CNN Model<\/span>","442b56af":"## <span style=\"color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold\" id=\"section_2_3\">Training of the CNN<span>","8262d2b0":"## <span style=\"color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold\" id=\"section_1_2\">MRI data<\/span>\n\nTrain data contains one record per patient. For each patient, four sub-files are available *(FLAIR, T1w, T1wCE and T2w)* in which the MRI image sequences are distributed.\n\n![data_structure](http:\/\/www.mf-data-science.fr\/images\/projects\/data_structure.jpg)\n\nWe are going to take a look at what an MRI image looks like :","5e7ce708":"# References","94118cbc":"1. https:\/\/keras.io\/examples\/vision\/3D_image_classification\/\n1. https:\/\/www.kaggle.com\/rluethy\/efficientnet3d-with-one-mri-type\n","f0b78c4b":"Once the generators are created, we can project an image to check:","8682937e":"We can test these functions on a test sequence :","79c8cd6e":"![brain_baner](http:\/\/www.mf-data-science.fr\/images\/projects\/brain_baner.jpg)","967b0046":"<h1 style=\"color:#0b0a2d; font-size:24px; text-transform: uppercase; font-weight:bold\">Context<\/h1>\n\nThe goal of this competition, initiated by the **Radiological Society of North America *(RSNA)*** in partnership with the **Medical Image Computing and Computer Assisted Intervention Society *(the MICCAI Society)*** is to predict the methylation of the **MGMT promoter**, which is an important gene biomarker for treatment of brain tumors.\n\nThese predictions will be based on a database of **MRI *(magnetic resonance imaging)*** scans of several hundred patients.\n\n<h1 style=\"color:#0b0a2d; font-size:24px; text-transform: uppercase; font-weight:bold\">Data<\/h1>\n\nEach independent case has a dedicated folder identified by a five-digit number. Within each of these \u201ccase\u201d folders, there are four sub-folders, each of them corresponding to each of the structural multi-parametric MRI (mpMRI) scans, in DICOM format. The exact mpMRI scans included are:\n\n- Fluid Attenuated Inversion Recovery (FLAIR)\n- T1-weighted pre-contrast (T1w)\n- T1-weighted post-contrast (T1Gd)\n- T2-weighted (T2)\n\n| ![brain_baner](http:\/\/www.mf-data-science.fr\/images\/projects\/brain_tumor_types.png) | \n|:--:| \n| *Examples of the four MR sequence types included in this work* |","7f533aa8":"# <span style=\"color:#0b0a2d; font-size:24px; text-transform: uppercase; font-weight:bold\" id=\"section_1\">Exploratory data analysis (EDA)<\/span>\n\nFirst, we have to load the usefull Python libraries :","add1295a":"## <span style=\"color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold\" id=\"section_1_1\">Loading data<\/span>\n\nLet's load the **dataframe with training data and testing data**. We are adding a column that will contain the patient record ID of 5 characters.","22b279d5":"### With MGMT = 0","482dda57":"All of the subjects in this dataset appear to have a brain tumor. MGMT_Class = 0 refers to people who do not have the MGMT promoter methylation. MGMT_Class = 1 appears to be someone who has the MGMT promoter methylation. \n\nIt is a competition which gives **the probability of it in `MGMT_value`** feature. `BraTS21ID` is the patient's identification. We can see that the data structure is the same as for the submission file than the train file, knowing that here **MGMT_value is indeed equal to 0 or 1 and no longer a probability**.\n\nLet's look at the distribution of the values of this variable in the train set :","d1a580b7":"### *T1w type :*","ba3b6863":"## <span style=\"color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold\" id=\"section_2_1\">Images preprocessing<\/span>","54e73b4f":"## <span style=\"color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold\" id=\"section_2_2\">Define CNN Model<span>","9832c77c":"## <span style=\"color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold\" id=\"section_2_1\">Functions to load images<\/span>\nWe are going to use 2 functions: one to load the Dicom images, then a second to load a sequence of x images distributed evenly on each side of the central image. These functions will then be used in a custom image generator.\n\nHere we are going to make a modification to the original Notebook by modifying the shape of the resulting array. Indeed, to be in agreement with the Keras recommendations for its convolution layer, the shape must be (Batch_size, Number of frames, width, height, Number of layers) or (4,64,80,80,1,1)\n"}}