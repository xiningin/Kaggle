{"cell_type":{"5dff4df4":"code","ba30124e":"code","61d3b9ac":"code","4686b08f":"code","f78ff06b":"code","dc3c1a19":"code","5b3e04b1":"code","2e23085b":"code","4c4cf2c7":"code","576da420":"code","ba8f61eb":"code","adb72caf":"code","d116812f":"code","76cdfb9e":"code","297c1f0e":"code","1f64fef2":"code","d48ac274":"code","ddf76a88":"code","ece0ec18":"code","10bc1f95":"code","b9aa1264":"code","cd1654c8":"code","c3b16e64":"code","a57e41fa":"code","9d338ee3":"code","7e5ef027":"code","f40553f5":"code","3db6de52":"code","3d96f016":"code","470a9a94":"code","ac6d54b5":"code","a8d9b4d3":"code","f4e02b7f":"code","7b87eac4":"code","122ff192":"markdown","4adbb79b":"markdown","8b00fe57":"markdown","277c3fda":"markdown","7a006e63":"markdown","2c4fc650":"markdown","d6eab7c4":"markdown","1f4ef395":"markdown","080b2bd8":"markdown","4e26ea22":"markdown","fa358cb6":"markdown","ca6d1ef8":"markdown","c4f04c69":"markdown","3f0ed819":"markdown","531fc5f8":"markdown","d608386b":"markdown","db2d0329":"markdown","63221b2c":"markdown","08875c70":"markdown","f1e0e7ed":"markdown","a09d57cb":"markdown","36a52fa9":"markdown","aabc1007":"markdown","b27ade04":"markdown","b451774e":"markdown","9b9d3031":"markdown","b2ebc19e":"markdown","e5502786":"markdown","300f15f7":"markdown","9e7c4cad":"markdown","8028fcce":"markdown","7a413ee6":"markdown","a67aff50":"markdown","6f18be15":"markdown","7bbfd3b2":"markdown","ee0402ae":"markdown","5a62b56d":"markdown","8e9cb831":"markdown","60477e63":"markdown","35c21565":"markdown","9bbce4a3":"markdown","6eda56ea":"markdown","317e51e3":"markdown"},"source":{"5dff4df4":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\ndata_root = '..\/input\/learnplatform-covid19-impact-on-digital-learning'\nengagement_data_folder = os.path.join(data_root, 'engagement_data')\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ba30124e":"# Read and view first file from engagement_data folder\nengagement_sample = pd.read_csv(\n    os.path.join(engagement_data_folder, os.listdir(engagement_data_folder)[0])\n)\nengagement_sample.head()","61d3b9ac":"# Read and view products_info.cvs file\nproducts_info = pd.read_csv(os.path.join(data_root, 'products_info.csv'))\nproducts_info.head()","4686b08f":"# Read and view districts_info.csv file\ndistricts_info = pd.read_csv(os.path.join(data_root, 'districts_info.csv'))\ndistricts_info.head()","f78ff06b":"# Create function to read engagement data\ndef read_engagement_data(data_folder=engagement_data_folder):\n    \"\"\"\n    Returns dictionary of dataframes with key as filename\n    and value as pd.DataFrame.\n    >>> read_engagement_data()\n    {\"6345\": pd.DataFrame, ...}\n    \"\"\"\n    result = {}\n    for i in os.listdir(data_folder):\n        result[i[:-4]] = pd.read_csv(os.path.join(data_folder, i), parse_dates=[0,])\n    return result\n\n# Read engagement data\nengagement_data = read_engagement_data()\n\n# View data sample\nengagement_data[\"6345\"].head()","dc3c1a19":"# Create function to calculate monthly engagement_index mean\ndef mean_monthly_engagement_index(data=engagement_data):\n    \"\"\"\n    Calculates mean monthly engagement_index dropping 'Nan' values.\n    monthly_engagement_index()\n    >>> {\"6345\": pd.DataFrame, ...}\n    \"\"\"\n    result = {}\n    cols_filter = [\"time\", \"engagement_index\"]\n    cols_rename = {\"time\": \"month\", \"engagement_index\": \"mean_eng_idx\"}\n    for key, value in data.items():\n        new_value = value[cols_filter].fillna(0).copy()\n        new_value[\"time\"] = new_value[\"time\"].dt.month\n        new_value = new_value.groupby([\"time\"]).mean().reset_index()\n        new_value.rename(columns=cols_rename, inplace=True)\n        result[key] = new_value\n    return result\n\n# Calculate monthly engagement index mean\nmean_eng_idx = mean_monthly_engagement_index()\n\n# View data sample\nmean_eng_idx[\"6345\"]","5b3e04b1":"# Create function to merge monthly engagement index mean\ndef merge_mean_monthly_engagement_index(data=mean_eng_idx):\n    \"\"\"\n    Merge mean_eng_idx (mean monthly engagement index) of\n    every district into one pd.Dataframe and rename columns\n    with mean monthly engagement index values by district\n    id number.\n    \"\"\"\n    result = pd.DataFrame()\n    for key, value in data.items():\n        val = value.rename(columns={\"mean_eng_idx\": key})\n        if result.empty:\n            result = val\n        else:\n            result = pd.merge(result, val, how=\"left\", on=\"month\")\n    return result.fillna(0)\n\n# Merge monthly engagement index mean\nmean_eng_idx_merged = merge_mean_monthly_engagement_index()\n\n# View result\nmean_eng_idx_merged","2e23085b":"mean_eng_idx_merged.iloc[:, 1:].mean().describe()","4c4cf2c7":"# Find max and min index (label)\nidx_max = mean_eng_idx_merged.iloc[:, 1:].mean().idxmax()\nidx_min = mean_eng_idx_merged.iloc[:, 1:].mean().idxmin()\n\n# Create months values array for x axis\nmonths = mean_eng_idx_merged.month.values\n\n# Create line plots for 2 districts\nfig, axes = plt.subplots(1, 2, figsize=(18, 6))\nfig.suptitle(\"Districts with max and min engagement_index in 2020\")\naxes[0].set_title(f\"District {idx_max} (top outlier)\")\nsns.lineplot(ax=axes[0], x=months, y=mean_eng_idx_merged[idx_max].values)\naxes[1].set_title(f\"District {idx_min} (bottom outlier)\")\nsns.lineplot(ax=axes[1], x=months, y=mean_eng_idx_merged[idx_min].values)\nplt.show()","576da420":"# Create function to concat monthly engagement index mean\ndef concat_mean_monthly_engagement_index(data=mean_eng_idx, fix_missing=False):\n    \"\"\"\n    Concat mean_eng_idx (mean monthly engagement index) of\n    every district into one pd.Dataframe and add new colum\n    district_id with district id number. Fix missing\n    month values by adding zero (optional).\n    \"\"\"\n    result = []\n    fix_df = pd.DataFrame({\"month\": [i for i in range(1, 13)]})\n    \n    def fix(dataframe):\n        \"\"\"\n        Add missing months values.\n        \"\"\"\n        if len(dataframe) < 12:\n            #print(key, len(new_value), end=\" >> \")\n            dataframe = pd.merge(fix_df, dataframe, how=\"left\", on=\"month\")\n            dataframe.fillna(0)\n            #print(key, len(new_value))\n        return dataframe\n    \n    for key, value in data.items():\n        new_value = value.copy()\n        if fix_missing:\n            new_value = fix(new_value)\n        new_value[\"district_id\"] = key\n        result.append(new_value)\n    return pd.concat(result, ignore_index=True)\n\n# Concat monthly engagement index mean\nmean_eng_idx_concat = concat_mean_monthly_engagement_index(fix_missing=True)\n\n# View result\nmean_eng_idx_concat","ba8f61eb":"# Count month values, should be 233 if there are no missing values in dataset\n# or fix_missing=True and less than 233 otherwise\nmean_eng_idx_concat.month.value_counts()","adb72caf":"# Create function to boxenplot monthly mean engagement index for all districts\ndef plot_monthly_engagement_index(data, stripplot=True):\n    plt.figure(figsize=(18, 5))\n    plt.title(\"Monthly mean engagement index in 2020 (all districts)\")\n    if stripplot:\n        sns.stripplot(x=\"month\", y=\"mean_eng_idx\", data=data)\n    sns.boxenplot(x=\"month\", y=\"mean_eng_idx\", data=data)\n    \n# Boxenplot monthly mean engagement index for all districts  \nplot_monthly_engagement_index(mean_eng_idx_concat)\n\n# Plot line for mean engagement index (all districts)\nplt.figure(figsize=(18, 4))\nplt.title(\"Monthly mean engagement index in 2020 (all districts combined)\")\nsns.lineplot(x=\"month\", y=\"mean_eng_idx\", data=mean_eng_idx_concat.groupby(\"month\").mean().reset_index())\nplt.show()","d116812f":"# Create function to boxenplot monthly mean engagement index for all districts w\/o outliers\ndef plot_monthly_engagement_index_wo_outliers(data, top_limit, stripplot=True):\n    top_map = data[\"mean_eng_idx\"] < top_limit\n    plot_monthly_engagement_index(data[top_map], stripplot)\n\ntop_limit = 1000\n# Boxenplot monthly mean engagement index for all districts w\/o outliers\nplot_monthly_engagement_index_wo_outliers(mean_eng_idx_concat, top_limit)","76cdfb9e":"# Create function to find and plot common boundaries for last 4 months\ndef plot_monthly_engagement_index_with_bound(data, top_limit, stripplot=False,):\n    plot_monthly_engagement_index_wo_outliers(data, top_limit, stripplot)\n    reg_map = data[\"mean_eng_idx\"] < top_limit\n    upper_bound = data[reg_map].groupby(\"month\").describe()[-4:][(\"mean_eng_idx\", \"75%\")].max()\n    lower_bound = data[reg_map].groupby(\"month\").describe()[-4:][(\"mean_eng_idx\", \"25%\")].min()\n    plt.plot([upper_bound for i in range(12)], color=\"red\")\n    plt.plot([lower_bound for i in range(12)], color=\"red\")\n    \n# Boxenplot monthly mean engagement index for all districts with boundaries\nplot_monthly_engagement_index_with_bound(mean_eng_idx_concat, top_limit)\n\n# Calculate outliers % input\nreg_map = mean_eng_idx_concat[\"mean_eng_idx\"] < top_limit\ndiff = mean_eng_idx_concat.groupby(\"month\").mean().reset_index()\\\n    - mean_eng_idx_concat[reg_map].groupby(\"month\").mean().reset_index()\noutliers_input = diff \/ mean_eng_idx_concat.groupby(\"month\").mean().reset_index() * 100\noutliers_input.month = mean_eng_idx_concat.groupby(\"month\").mean().reset_index().month\noutliers_input.rename(columns={\"mean_eng_idx\": \"outliers_percent_input\"}, inplace=True)\n\n# Find top outliers\ntop_map = (mean_eng_idx_concat[\"mean_eng_idx\"] > top_limit)\njuly_map = (mean_eng_idx_concat[\"mean_eng_idx\"] > 400)\\\n            & (mean_eng_idx_concat[\"month\"] == 7)\ntop_map = top_map | july_map\ntop_outliers = mean_eng_idx_concat[top_map]\n\n# Plot line for mean engagement index (all districts)\nplt.figure(figsize=(18, 4))\nplt.title(\"Monthly mean engagement index in 2020 (all districts combined)\")\nsns.lineplot(\n    x=\"month\", y=\"mean_eng_idx\",\n    data=mean_eng_idx_concat[reg_map].groupby(\"month\").mean().reset_index(),\n    label=\"Top outliers excluded\",\n)\nsns.lineplot(\n    x=\"month\", y=\"mean_eng_idx\",\n    data=mean_eng_idx_concat.groupby(\"month\").mean().reset_index(),\n    label=\"Top outliers included\",\n)\nax2 = plt.twinx()\nax2.grid(False)\nfor i, txt in enumerate(outliers_input.outliers_percent_input.values):\n    ax2.annotate(\n        round(txt, 2),\n        (outliers_input.month.values[i],\n        outliers_input.outliers_percent_input.values[i]),\n        xytext=(outliers_input.month.values[i] + 0.1,\n        outliers_input.outliers_percent_input.values[i] + 0.1),\n    )\nsns.lineplot(\n    x=\"month\", y=\"outliers_percent_input\",\n    data=outliers_input,\n    label=\"Top outliers % input\",\n    ax=ax2,\n    linestyle=\"None\",\n    marker=\"o\",\n    color='k'\n)\nplt.legend()\nplt.show()\nplt.figure(figsize=(18, 4))\nplt.title(\"Monthly mean engagement index in 2020 (top outliers)\")\nsns.lineplot(x=\"month\", y=\"mean_eng_idx\", data=top_outliers.groupby(\"month\").mean())\nplt.show()","297c1f0e":"# Find districts with biggest mean engagement index\nb_map = mean_eng_idx_concat.groupby(\"month\").idxmax()\nbiggest_outliers = mean_eng_idx_concat.iloc[b_map.mean_eng_idx.values]\n\n# Calculate final rating\nfinal_rating = pd.concat(\n    [top_outliers, biggest_outliers]\n    ).district_id.value_counts()\n\n# Plot results\nplt.figure(figsize=(18, 10))\ngs = gridspec.GridSpec(2, 6)\ngs.update(wspace=0.4, hspace=0.3)\nax1 = plt.subplot(gs[0, :3])\nax2 = plt.subplot(gs[0, 3:])\nax3 = plt.subplot(gs[1, :2])\nax4 = plt.subplot(gs[1, 2:4])\nax5 = plt.subplot(gs[1, 4:]) \nplt.suptitle(\"Top outliers\", fontsize=\"16\")\n\nax1.set_title(\"Top outliers in 2020\")\nsns.stripplot(\n    ax=ax1, x=\"month\", y=\"mean_eng_idx\",\n    hue=\"district_id\", data=top_outliers\n)\n\nax2.set_title(\"Top positions in 2020\")\nsns.stripplot(\n    ax=ax2, x=\"month\", y=\"mean_eng_idx\",\n    hue=\"district_id\", data=biggest_outliers\n)\n\nax3.set_title(\"Top outliers scores in 2020\")\nsns.barplot(\n    ax=ax3,\n    x=top_outliers.district_id.value_counts().index,\n    y=top_outliers.district_id.value_counts().values,\n)\n\nax4.set_title(\"Top position scores in 2020\")\nsns.barplot(\n    ax=ax4,\n    x=biggest_outliers.district_id.value_counts().index,\n    y=biggest_outliers.district_id.value_counts().values,\n)\n\nax5.set_title(\"Final rating scores\")\nsns.barplot(ax=ax5, x=final_rating.index, y=final_rating.values)\nplt.show()","1f64fef2":"# Find upper and lower boundaries of middle segments in each month\nb_lim = mean_eng_idx_concat[\"mean_eng_idx\"] < top_limit\nmid_seg_cols = [(\"mean_eng_idx\", \"25%\"), (\"mean_eng_idx\", \"75%\")]\nlimits = mean_eng_idx_concat[b_lim].groupby(\"month\").describe()[mid_seg_cols]\nlimits.columns = ['_'.join(col) for col in limits.columns.values]\n\n# Filter middle segment\nraw = mean_eng_idx_concat[b_lim].merge(limits.reset_index(), on=\"month\")\nmid_seg_map = (raw.iloc[:, 1] <= raw.iloc[:, 4]) & (raw.iloc[:, 1] >= raw.iloc[:, 3])\nmid_seg = raw[mid_seg_map].iloc[:, 0:3].copy()\nmid_seg","d48ac274":"# Plot monthly mean engagement index of middle segments\nplt.figure(figsize=(18, 5))\nplt.title(\"Monthly mean engagement index in 2020 (middle segments)\")\nsns.lineplot(x=\"month\", y=\"mean_eng_idx\", data=mid_seg.groupby(\"month\").mean())\nplt.show()","ddf76a88":"# Count values of Primary Essential Function\ncount_prod_types = products_info[\"Primary Essential Function\"].value_counts()\nprint(count_prod_types.shape[0], \"types of products in total.\")\ncount_prod_types","ece0ec18":"# Create function to merge products and engagement index\ndef eng_prod_merge(prod=products_info, eng=engagement_data):\n    \"\"\"\n    \"\"\"\n    prd_inf = prod.rename(columns={\"LP ID\": \"lp_id\"})\n    result = None\n    for key, value in eng.items():\n        new_val = value.copy()\n        #new_val.dropna(inplace=True)\n        new_val.rename(columns={\"time\": \"month\"}, inplace=True)\n        new_val[\"month\"] = new_val[\"month\"].dt.month\n        new_val = new_val.groupby([\"month\", \"lp_id\"]).mean().reset_index()\n        new_val[\"district_id\"] = key\n        new_val = new_val.merge(prd_inf[[\"lp_id\", \"Primary Essential Function\"]], on=\"lp_id\")\n        new_val[\"lp_id\"] = new_val[\"lp_id\"].astype(int)\n        if result is None:\n            result = new_val.copy()\n        else:\n            result = pd.concat([result, new_val])\n    return result\n\ne_p_merged = eng_prod_merge()\ne_p_merged","10bc1f95":"# Find top ten products\ne_p_summary = e_p_merged[[\"lp_id\", \"pct_access\", \"engagement_index\"]].groupby(\"lp_id\").mean()\ntop_ten = e_p_summary.sort_values([\"engagement_index\"], ascending=False)[:10].reset_index()\ntop_ten = top_ten.merge(products_info.rename(columns={\"LP ID\": \"lp_id\"}), on=\"lp_id\")\ntop_ten","b9aa1264":"# Plot top ten products line charts\nfig, axes = plt.subplots(4, 3, figsize=(18, 12))\nplt.subplots_adjust(hspace=0.6)\nplt.suptitle(\"Top 10 products in 2020\\n with trend line\", fontsize=\"16\")\nfor i in range(12):\n    r, c = divmod(i, 3)\n    if i < top_ten.shape[0]:\n        _map = e_p_merged.lp_id == top_ten.iloc[i].lp_id\n        data = e_p_merged[_map].groupby(\"month\").mean().reset_index()\n        axes[r][c].set_title(f\"Product id: {top_ten.iloc[i].lp_id}, rating position # {i + 1}\")\n        sns.lineplot(ax=axes[r][c], x=\"month\", y=\"engagement_index\", data=data)\n        # Plot trend line if no data is missing\n        if len(data) == 12:\n            sns.lineplot(\n                ax=axes[r][c], x=[1, 12],\n                y=[data.iloc[:4].mean().engagement_index, data.iloc[-4:].mean().engagement_index]\n            )\n    else:\n        axes[r][c].axis(\"off\")\nplt.show()","cd1654c8":"# Find trend line slopes for all products\ntrends_dict = {}\nfor i in e_p_merged.lp_id.unique():\n    _map = e_p_merged.lp_id == i\n    data = e_p_merged[_map].set_index(\"lp_id\").groupby(\"month\").mean()#.reset_index()\n    if data.shape[0] == 12:\n        trends_dict[i] = data.iloc[-4:].mean().engagement_index\\\n            - data.iloc[:4].mean().engagement_index\n\nproduct_trends = pd.DataFrame(trends_dict.values(), trends_dict.keys()).reset_index()\nproduct_trends.rename(columns={\"index\": \"lp_id\", 0: \"trend\"}, inplace=True)\npos_trends = (product_trends[\"trend\"] > 0).sum()\nneg_trends = (product_trends[\"trend\"] < 0).sum()\nprint(\"Positive trend:\", pos_trends)\nprint(\"Negative trend:\", neg_trends)\nprint(\"Missing values:\", len(e_p_summary) - (pos_trends + neg_trends))\nprint(\"Total:\", len(e_p_summary))","c3b16e64":"# Identify missing products numbers\nmissing_products = set(products_info[\"LP ID\"]).difference(set(e_p_summary.index))\nmissing_products","a57e41fa":"# View missing products\nproducts_info[products_info[\"LP ID\"].isin(missing_products)]","9d338ee3":"# Check missing products vs engagement data\nall_districts = pd.concat(engagement_data.values())\nall_districts[all_districts.lp_id.isin(missing_products)]","7e5ef027":"# Add trend annotation to product trends\ndef trend(x):\n    if x < -1:\n        return \"negative\"\n    elif x > 1:\n        return \"positive\"\n    else:\n        return \"no trend\"\nproduct_trends[\"trend\"].transform(trend)\nproduct_trends[\"trend_annot\"] = product_trends[\"trend\"].transform(trend)\nproduct_trends","f40553f5":"# Check what kind of missing values we have\ntotal = len(districts_info)\ncol_names = districts_info.columns.to_list()[3:]\nprint(\"Total number of districts:\", total)\nprint(\"Districts with missing location data:\", total - len(districts_info.iloc[:, :3].dropna()))\nfor i, col_n in enumerate(col_names, 3):\n    print(\n        f\"Districts with missing {col_n} data:\",\n        total - len(districts_info.iloc[:, [0, i]].dropna())\n    )\nprint(\n    \"Districts with missing all the data:\",\n    districts_info[districts_info.isna().sum(axis=1) == 6].count().sum())","3db6de52":"# Drop distrcicts with Nan values in all columns\nd_inf_clean = districts_info\\\n    .set_index(\"district_id\")\\\n    .dropna(how=\"all\")\\\n    .reset_index()\n\n# Count districts by state\nprint(\n    \"Total number of states:\",\n    d_inf_clean.iloc[:, :2].groupby(\"state\").count().count()[0]\n)\nprint(\n    \"Total number of districts:\",\n    d_inf_clean.iloc[:, :2].groupby(\"state\").count().sum()[0]\n)\nd_inf_clean.iloc[:, :2]\\\n    .groupby(\"state\")\\\n    .count()\\\n    .sort_values(\"district_id\", ascending=False)\\\n    .rename(columns={\"district_id\": \"number_of_districts\"})","3d96f016":"# Combine all districts data with mean_eng_idx\nyear_mean = mean_eng_idx_concat[[\"district_id\", \"mean_eng_idx\"]]\\\n    .groupby(\"district_id\")\\\n    .mean()\\\n    .reset_index()\nyear_mean[\"district_id\"] = year_mean[\"district_id\"].astype(int)\nall_districts = d_inf_clean.merge(year_mean, on=\"district_id\")\nall_districts.head()","470a9a94":"# Top districts\n# Set index as 'district_id' and filter by final_rating.index\ntop_districts = all_districts\\\n    .set_index(\"district_id\")\\\n    .reindex(final_rating.index.astype(int))\ntop_districts.dropna(how=\"all\", inplace=True)\ntop_districts","ac6d54b5":"# Split columns data\ndef split(series, col1, col2, dict_, to=\"int\"):\n    for val in series.values:\n        if val is not np.nan:\n            val = val[1:-1]\n            if to == \"int\":\n                val1 = int(val[:val.find(\",\")])\n                val2 = int(val[val.find(\" \"):])\n            elif to == \"float\":\n                val1 = float(val[:val.find(\",\")])\n                val2 = float(val[val.find(\" \"):])\n        elif val is np.nan:\n            val1, val2 = np.nan, np.nan\n        \n        if col1 in dict_:\n            dict_[col1].append(val1)\n        elif col1 not in dict_:\n            dict_[col1] = [val1,]\n        \n        if col2 in dict_:\n            dict_[col2].append(val2)\n        elif col2 not in dict_:\n            dict_[col2] = [val2,]\n    return dict_\n\nsplitted = {}\nsplitted = split(all_districts[\"pct_black\/hispanic\"], \"pct_b\", \"pct_h\", splitted, to=\"float\")\nsplitted = split(all_districts[\"pct_free\/reduced\"], \"pct_free\", \"pct_reduced\", splitted, to=\"float\")\nsplitted = split(all_districts[\"county_connections_ratio\"], \"conn_r\", \"conn_rr\", splitted, to=\"float\")\nsplitted = split(all_districts[\"pp_total_raw\"], \"pp_loc\", \"pp_fed\", splitted)","a8d9b4d3":"# Add splitted data to dataframe\nfor key, val in splitted.items():\n    series = pd.Series(splitted[key], all_districts.index, name=key)\n    all_districts = all_districts.merge(series, left_index=True, right_index=True)\n\nall_districts","f4e02b7f":"# Plot pair correlations\nsns.pairplot(all_districts.iloc[:, 7:])","7b87eac4":"# Mean by state\nmean_dist = all_districts[[\"state\", \"mean_eng_idx\"]].groupby(\n    \"state\").mean().sort_values(\"mean_eng_idx\", ascending=False)\ncount = d_inf_clean.iloc[:, :2].groupby(\n    \"state\").count().sort_values(\"district_id\", ascending=False)\ncount.rename(columns={\"district_id\": \"n_of_districts\"}, inplace=True)\nmean_dist.merge(count, left_index=True, right_index=True)","122ff192":"**Columns description:**\n\n| Name | Description |\n| :--- | :----------- |\n| time | date in \"YYYY-MM-DD\" |\n| lp_id | The unique identifier of the product |\n| pct_access | Percentage of students in the district have at least one page-load event of a given product and on a given day |\n| engagement_index | Total page-load events per one thousand students of a given product and on a given day |\n\n<font color='green'>**Note:**<\/font> The engagement data are aggregated at school district level, and each file in the folder `engagement_data` represents data from one school district. The 4-digit file name represents `district_id` which can be used to link to district information in `district_info.csv`. The `lp_id` can be used to link to product information in `product_info.csv`.","4adbb79b":"It looks like 57 districts which do not have location data, also do not have all the other data in the dataset. Let's drop them as they will not give us any valuable information. ","8b00fe57":"There are some significant outliers in the first half of 2020. From May to July it looks like decreasing trend but in January and February trend is increasing. July is the worst month of the year. Let's drop some top outliers to better see data distribution on chart.","277c3fda":"#### 3.1.4 View difference between districts in 2020\n\nLet's see if there is any big difference between districts' `engagement_index` in 2020 to identify outliers and understand data distribution:","7a006e63":"### 3.2 Explore `products_info.csv`\n\n#### 3.2.1 Variety of product types\n\nLet's count values in `Primary Essential Function` column, to view variety of product types.","2c4fc650":"Middle segments demonstrate more stability and improvement in the last 4 months of 2020 comparing to the begining of the year.","d6eab7c4":"We have 66 positive trends, 256 negative trends, 47 with missing values and 3 products are missing in our summary as total number of products is 371. Let's identify the missing products to figure out the reason why they were skipped.","1f4ef395":"#### 3.3.2 How many states represented by school districts","080b2bd8":"It looks like products 36254, 37805, 88065 do not have any records in egagement data.","4e26ea22":"#### 3.3.4 By state rating\n\nLet's find mean engagement index by state and add number of districts in each state.","fa358cb6":"#### 3.1.6 Monthly mean egagement index\n\nLet's try to look at mean engagement index values distribution for all districts. But first we need to prepare data for it.","ca6d1ef8":"#### 3.1.8 Middle segments\n","c4f04c69":"There are 35 types of products. It is a little complicated starting point for data study. Let's make it a bit easier and find top 10 products with highest mean engagement index in 2020.","3f0ed819":"July was the most inactive month during 2020. Top outliers made more significant input to engagement index in the begining of the year gradually degreasing to 0 in July. After July their input was at maximum in September gradually decreasing again. In general top outliers input into monthly mean engagement index decreased almost as much as 5 times (in comparison with February). It is difficult to say now what was the reason for it. This fact needs more study in terms of characteristics of the top 372 products from `products_info.csv` file, engagement data `pct_access` and state interventions, practices or policies. Hopefully this extra data will help us to find a reasonable explanation.","531fc5f8":"## 4. Conclusion\n\n### Districts\nI tried to identify top outliers (districts) in my study as a first step. These top 10 districts are located in the following states in descending order by mean enagement index:\n\n1. New York (City) - district 9536\n2. District of Columbia (City) - district 6418\n3. Arizona (City) - district 9007\n4. Illinois (Suburb) - district 8815\n5. New York (Rural) - district 9515\n6. Utah (Suburb) - district 3692\n\nBut if we combine data by mean engagement index in each state including city and rural (all districts) we'll have the following top 10 results:\n\n1. Arizona - 1 district\n2. New York - 8 districts\n3. New Hampshire - 2 districts\n4. District Of Columbia - 3 districts\n5. Connecticut - 30 districts\n6. New Jersey - 2 districts\n7. Indiana - 7 districts\n8. Illinois - 18 districts\n9. Massachusetts - 21 districts\n10. Utah - 29 districts\n\nThere is some intersection of states within both lists. It can be considered as strong evidence that the listed states have very good engagement index in comparison with other states. No correlation was found between engagement index and such districts data as:\n\n* pct_black\/hispanic\n* pct_free\/reduced\n* county_connections_ratio\n* pp_total_raw\n\n### Trends and patterns\nAll districts had similar pattern of engagement index in 2020. Engagement index dropped in summer with the smallest minimum in July. There are 66 products with positive trend, 256 with negative trend. 47 products have missing values (mostly) in the begining of the year which makes impossible to calculate trend.\n\n### Products\n\nIt is obvious to outline 3 most popular products:\n* Google Docs - which was probably driven by the need of creating and exchanging documents and information.\n* Google Classroom - which was probably driven by the need of LMS, online classes and digital learning.\n* YouTube - which was probably driven by the need of educational videos.","d608386b":"## 5. Afterword\n\nThank you very much for your attention and time spent in reading my study. As it is my first analytics competition, and I realize that my notebook not so perfect that I would like it to be. But I hope it was helpful and you could get valuable insights from it. Thank you very much for this interesting experience.\n","db2d0329":"### 2.2 View data sample from `engagement_data` folder","63221b2c":"### 2.4 View data sample from `districts_info.csv` file","08875c70":"## 2. Data Description\n\nOriginal dataset contains daily edtech engagement data from over 200 school districts in 2020. There are three basic sets of files to get started with:\n\n* The `engagement_data` folder is based on LearnPlatform\u2019s Student Chrome Extension. The extension collects page load events of over 10K education technology products in our product library, including websites, apps, web apps, software programs, extensions, ebooks, hardwares, and services used in educational institutions. The engagement data have been aggregated at school district level, and each file represents data from one school district.\n* The `products_info.csv` file includes information about the characteristics of the top 372 products with most users in 2020.\n* The `districts_info.csv` file includes information about the characteristics of school districts, including data from NCES and FCC.","f1e0e7ed":"## 3. The state of digital learning in 2020.\n\nLet's explore the state of digital learning in 2020. I'd like to start with `engagement_data` first.\n\n### 3.1 Explore `engagement_data`\n\n#### 3.1.1 Load data\n\nBefore starting to make any data analisys it is good to write a helper function which will read multiple files into one dictionary of dataframes.","a09d57cb":"Find top 10 products with highest mean engagement index.","36a52fa9":"#### 3.1.2 Calculate monthly mean engagement index\n\nNow let's calculate monthly mean `engagement_index` for each district.","aabc1007":"**Columns description:**\n\n| Name | Description |\n| :--- | :----------- |\n| LP ID| The unique identifier of the product |\n| URL | Web Link to the specific product |\n| Product Name | Name of the specific product |\n| Provider\/Company Name | Name of the product provider |\n| Sector(s) | Sector of education where the product is used |\n| Primary Essential Function | The basic function of the product. There are two layers of labels here. Products are first labeled as one of these three categories: LC = Learning & Curriculum, CM = Classroom Management, and SDO = School & District Operations. Each of these categories have multiple sub-categories with which the products were labeled | \n\n\n<font color='green'>**Note:**<\/font> Some products may not have labels due to being duplicate, lack of accurate url or other reasons.","b27ade04":"#### 3.3.3 Correlation\n\nLet's combine some of the data we've explored so far into one dataset.","b451774e":"### 2.3 View data sample from `products_info.csv` file","9b9d3031":"Now we have top outliers and some information about them. Most interesting for further study are districts 9536, 6418 and probably 9007. District 9536 is the most persistent one. It appears 7 times among top outliers and keeps top position during 5 months in a row (collecting 12 points in final rating score) during decreasing trend and capturing the worst month July. District 6418 is a newcomer in top outliers since September. It won position from district 9536 in September and October, keeping top position during 4 months in a row till the end of the year. The second newcomer is district 9007. It had top position in August and the third position among top outliers in September disappearing from top outliers till the end of the year. Before making further top outliers study let's identify middle segments first.","b2ebc19e":"# LearnPlatform COVID-19 Impact on Digital Learning\n\n## 1. Introduction\n\n### Problem Statement\n\nThe COVID-19 Pandemic has disrupted learning for more than 56 million students in the United States. In the Spring of 2020, most states and local governments across the U.S. closed educational institutions to stop the spread of the virus. In response, schools and teachers have attempted to reach students remotely through distance learning tools and digital platforms. Until today, concerns of the exacaberting digital divide and long-term learning loss among America\u2019s most vulnerable learners continue to grow.\n\n### Challenge\n\n1. Explore the state of digital learning in 2020.\n2. How the engagement of digital learning relates to factors such as district demographics, broadband access, and state\/national level policies and events.","e5502786":"In 233 school districts mean `engagement_index` in 2020 vary from 3.61 to 1215.50 total page-load events per one thousand students per day. Such a big values distribution tells us that school districts differ in terms of using distance learning tools and digital platforms. The best result is almost 1 page-load event per student per day. The worst is 4.16 which means that in some districts students don't use digital learning platforms at all.\n\n#### 3.1.5 The biggest and the lowest mean engagement index examples\nLet's explore districts with the biggest and the lowest mean engagement index values in more details on below charts to try to identify data patterns for the both examples.","300f15f7":"Plot pair correlations.","9e7c4cad":"Let's plot 10 top products in 2020 to see their engagement index trend over all school districts.","8028fcce":"Do the same for top districts.","7a413ee6":"#### 3.1.3 Merge monthly districts data\nIn the below code cell we'll merge all districts monthly mean `engagement_index` into one dataframe.","a67aff50":"In this chart data became more distinct. And what is more interesting, the last 4 months look very similar and display some stability (even with top outliers). Moreover bottom line of the biggest box segments raised a little up. Let's plot common boundaries for the last 4 months and mean engagement index of all districts combined in 2020.","6f18be15":"At quick view product 61292 (LC - Sites, Resources & Reference - Streaming Services) stands out. It looks like it was created in June and had significant growth and took the 3rd rating position (there is no trend line as not all months data is available). On the other hand engagement index of product 24711 (LC - Study Tools) reduced by the end of the year. And it has almost identical negative slope of trend line with product 99916 (LC\/CM\/SDO - Other).\n\n#### 3.2.3 Positive and negative trends.\n\nLet's find which products have positive and negative trends.","7bbfd3b2":"### 3.3 Explore `districts_info.csv`\n\n#### 3.3.1 Check for missing data\nNow let's try to explore districts info data to understand how much we can get from this dataset for our analysis.","ee0402ae":"#### 3.2.2 Top 10 products in 2020\n\nPrepare data.","5a62b56d":"**Columns description:**\n\n| Name | Description |\n| :--- | :----------- |\n| district_id | The unique identifier of the school district |\n| state | The state where the district resides in |\n| locale | NCES locale classification that categorizes U.S. territory into four types of areas: City, Suburban, Town, and Rural. See [Locale Boundaries User's Manual](https:\/\/eric.ed.gov\/?id=ED577162) for more information. |\n| pct_black\/hispanic | Percentage of students in the districts identified as Black or Hispanic based on 2018-19 NCES data |\n| pct_free\/reduced | Percentage of students in the districts eligible for free or reduced-price lunch based on 2018-19 NCES data |\n| county_connections_ratio | `ratio` (residential fixed high-speed connections over 200 kbps in at least one direction\/households) based on the county level data from FCC From 477 (December 2018 version). See [FCC data](https:\/\/www.fcc.gov\/form-477-county-data-internet-access-services) for more information. |\n| pp_total_raw | Per-pupil total expenditure (sum of local and federal expenditure) from Edunomics Lab's National Education Resource Database on Schools \\(NERD\\$\\) project. The expenditure data are school-by-school, and we use the median value to represent the expenditure of a given school district. |\n\n<font color='green'>**Note:**<\/font> There are many missing data marked as `NaN` indicating that the data was suppressed to maximize anonymization of the dataset.\n","8e9cb831":"Interesting fact about top districts: District 9536 in New York (city locations) is on the top of the list. District 9515 is also located in New York state but rural.\n\nSplit columns to prepare data for pairplot.","60477e63":"From the above plot it is possible to see only 2 correlations: `pct_black\/hispanic` (`pct_h`\/`pct_b`) and `pp_total_raw` as local vs federal expenditures (`pp_loc` \/ `pp_fed`). No correlation found between mean engagement index and districts info data.","35c21565":"### 2.1 Import modules and setup directories","9bbce4a3":"For the 2 school districts (top and bottom outliers) data patterns of mean engagement index from month to month look also different.","6eda56ea":"After removing missing data we have 23 states represented by 176 districts. Connecticut is on the top and has 30 school districts in the dataset.","317e51e3":"#### 3.1.7 Top outliers\n\nLet's identify a group of top outliers and districts with the biggest mean engagement index in 2020."}}