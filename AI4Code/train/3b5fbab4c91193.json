{"cell_type":{"9aab7c6c":"code","aa8fa8d1":"code","b3e7d5ef":"code","0c3ec90d":"code","84bdfa7c":"code","4d99c33b":"code","b30cc0c0":"code","433a3b7d":"code","9c4cc668":"code","59a78731":"code","3a50c428":"code","7694d614":"code","aa8169b0":"code","7c65c224":"code","c3653795":"code","1dc89c50":"code","4c3f7a6d":"code","b7c29e44":"code","34800be1":"code","530276d8":"code","de0f714e":"code","3a87ac2c":"code","534a577f":"code","024f3c08":"code","e50f9783":"code","76339aa4":"code","d1d2acbb":"code","5887ce31":"code","f73f6159":"code","5470a10d":"code","2f4d6632":"code","17252e39":"code","d8c01c35":"code","be7f5d80":"markdown","90caa205":"markdown","e973200b":"markdown","e9baa802":"markdown","18bbe078":"markdown","3a8923bf":"markdown","fff817a0":"markdown","729041fb":"markdown","2d8a8e00":"markdown","40d47cc3":"markdown","6347840b":"markdown","ecb63c19":"markdown","fcfbd3ce":"markdown","2136b113":"markdown","60720190":"markdown","279a3a95":"markdown","88f74059":"markdown","9a40bdfc":"markdown","aea0ef5a":"markdown","00fffc87":"markdown"},"source":{"9aab7c6c":"#Load the librarys\nimport pandas as pd #To work with dataset\nimport numpy as np #Math library\nimport seaborn as sns #Graph library that use matplot in background\nimport matplotlib.pyplot as plt #to plot some parameters in seaborn\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\n# Import StandardScaler from scikit-learn\nfrom sklearn.preprocessing import StandardScaler,RobustScaler,MinMaxScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import GridSearchCV\n# Import train_test_split()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom datetime import datetime, date\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.model_selection import cross_val_score\nimport lightgbm as lgbm\nimport  tensorflow as tf \nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\n#import smogn\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\n# For training random forest model\nimport lightgbm as lgb\nfrom sklearn.neighbors import KNeighborsRegressor \n#import smong \nfrom sklearn.linear_model import LinearRegression, RidgeCV\nimport category_encoders as ce\nimport warnings\nwarnings.filterwarnings('ignore')","aa8fa8d1":"# import lux\n# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\n# Preview the data\ntrain.head()","b3e7d5ef":"train.isnull().sum().values","0c3ec90d":"train.describe()","84bdfa7c":"# Check the structure of the data\nprint(train.info())","4d99c33b":"train[train.select_dtypes(['float64']).columns] = train[train.select_dtypes(['float64']).columns].apply(pd.to_numeric)\ntrain[train.select_dtypes(['object']).columns] = train.select_dtypes(['object']).apply(lambda x: x.astype('category'))","b30cc0c0":"train.info()","433a3b7d":"n, bins, patches = plt.hist(x=train['target'], bins='auto', color='blue',alpha=0.7, rwidth=0.5)\nplt.xlabel(\"Loan Amount\")\nplt.show()","9c4cc668":"#Looking unique values\nl=dict(train.nunique())\nprint(l)","59a78731":"# Create arrays for the features and the response variable\ny = train['target'].to_numpy()\nX = train.drop(['id','target'], axis=1)","3a50c428":"# Split the dataset and labels into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,random_state=0)\nprint(\"{} rows in test set vs. {} in training set. {} Features.\".format(X_test.shape[0], X_train.shape[0], X_test.shape[1]))","7694d614":"# select non-numeric columns\ncat_columns = train.drop(['id','target'], axis=1).select_dtypes(exclude=['int64','float64']).columns","aa8169b0":"# select the float columns\nnum_columns = train.drop(['id','target'], axis=1).select_dtypes(include=['int64','float64']).columns","7c65c224":"# Create a histogram\ntrain.drop(['id','target'], axis=1).select_dtypes(include=['int64','float64']).hist()\nplt.show()","c3653795":"# Import PowerTransformer\nfrom sklearn.preprocessing import PowerTransformer\nnumdata=train.drop(['id','target'], axis=1).select_dtypes(include=['int64','float64'])\n# Instantiate PowerTransformer\npow_trans = PowerTransformer()\n\n# Train the transform on the data\npow_trans.fit(numdata[['cont4']])\n\n# Apply the power transform to the data\nnumdata['cont4_LG'] = pow_trans.transform(numdata[['cont4']])\n\n# Plot the data before and after the transformation\nnumdata[['cont4', 'cont4_LG']].hist()\nplt.show()","1dc89c50":"print(cat_columns)\nprint(num_columns)","4c3f7a6d":"num_columns=['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7',\n       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\ncat_columns=['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8',\n       'cat9']\nall_columns = (num_columns+cat_columns)\nprint(all_columns)","b7c29e44":"if set(all_columns) == set(train.drop(['id','target'], axis=1).columns):\n    print('Ok')\nelse:\n    # Let's see the difference \n    print('dans all_columns mais pas dans train  :', set(all_columns) - set(train.drop(['id','target'], axis=1).columns))\n    print('dans X.columns   mais pas dans all_columns :', set(train.drop(['id','target'], axis=1).columns) - set(all_columns))","34800be1":"# Create a series out of the Country column\ncat6 = train.cat6\n\n# Get the counts of each category\ncat6_counts = cat6.value_counts()\n\n# Print the count values for each category\nprint(cat6_counts)","530276d8":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"darkgrid\")\nsns.barplot(cat6_counts.index, cat6_counts.values, alpha=0.9)\nplt.title('Frequency Distribution of countries')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Country', fontsize=12)\nplt.show()","de0f714e":"labels = train['cat7'].astype('category').cat.categories.tolist()\ncounts = train['cat7'].value_counts()\nsizes = [counts[var_cat] for var_cat in labels]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\nax1.axis('equal')\nplt.show()\n\n","3a87ac2c":"fill_missing_then_OrdinalEncoder = make_pipeline(\n    SimpleImputer(strategy='most_frequent',add_indicator=True),\n    OrdinalEncoder()\n)","534a577f":"fill_missing_then_StandardScaler = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n    StandardScaler()\n)","024f3c08":"data_preprocess_OrdinalEncoder = make_column_transformer(\n    ( fill_missing_then_OrdinalEncoder , cat_columns),\n    ( fill_missing_then_StandardScaler, num_columns)\n)","e50f9783":"X_train.info()","76339aa4":"data_preprocess_OrdinalEncoder.fit(X_train)\ndata_preprocess_OrdinalEncoder.transform(X_train)\ndata_preprocess_OrdinalEncoder.transform(X_test)\nprint(\"Ok , Every thing is well \")","d1d2acbb":"cross_validation_design = KFold(n_splits=5,\n                                shuffle=True,\n                                random_state=77)\n\ncross_validation_design","5887ce31":"from xgboost import XGBRegressor\nXGBR = XGBRegressor(random_state=7)\nXGBR_MODEL = {}\n# D\u00e9finir la pipeline\nXGBR_MODEL['pipeline'] = Pipeline([\n                                  ('data_process', data_preprocess_OrdinalEncoder),\n                                  ('XGBR', XGBR)\n                                  ])\n\n# D\u00e9finir la grille\nXGBR_MODEL['hyperparams'] = {}\nXGBR_MODEL['hyperparams']['XGBR__n_estimators'] = [20,50,100]\n# Effectuer la GridSearch\nXGBR_MODEL['gridsearch'] = GridSearchCV(\n    estimator=XGBR_MODEL['pipeline'],\n    param_grid=XGBR_MODEL['hyperparams'],\n    cv=cross_validation_design,\n    scoring='neg_root_mean_squared_error'\n    )\n#Define SVR classifier\nXGBR_MODEL['gridsearch'].fit(X_train, y_train)\nXGBR_accuracy = XGBR_MODEL['gridsearch'].score(X_test, y_test)\nprint('SXGBR Accuracy : ', XGBR_accuracy)","f73f6159":"XGBR_MODEL['gridsearch'].cv_results_","5470a10d":"XGBR_MODEL['gridsearch'].best_params_","2f4d6632":"model_final = Pipeline([('data_cleaning', data_preprocess_OrdinalEncoder),\n                        ('rf', XGBRegressor(random_state=7,n_estimators = 100))\n                        ])\n\n# we fit the model with the best param on all data \nmodel_final.fit(X, y)","17252e39":"test_final= test.drop(['id'], axis=1)\n# Use the model to generate predictions\npredictions = model_final.predict(test_final)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': test.id,\n                       'target': predictions})\noutput.to_csv('submissionfirstpipeXGBROrdinal.csv', index=False)","d8c01c35":"output","be7f5d80":"## Num Features :","90caa205":"\n# Step 2: Load the data\n\nNext, we'll load the training and test data.\n\nWe set index_col=0 in the code cell below to use the id column to index the DataFrame. (If you're not sure how this works, try temporarily removing index_col=0 and see how it changes the result.)\n","e973200b":"# Null","e9baa802":"# Train RandomForest \/ Xgboost \n## XGBR on train normal data ","18bbe078":"# Pipe Cat ","3a8923bf":"# Distribution of Target","fff817a0":"# Introduction \nThis notebook is the continuation  of this work  :\nhttps:\/\/www.kaggle.com\/bannourchaker\/exploratory-data-analysis-knn-baseline-numfeatures\n\nIf you find my notebook useful just vote and Thanks  :) \n# Step 1: Import helpful libraries","729041fb":"# check that we have all column","2d8a8e00":"# Create test and train groups\n\nNow we\u2019ve got our dataframe ready we can split it up into the train and test datasets for our model to use. We\u2019ll use the Scikit-Learn train_test_split() function for this. By passing in the X dataframe of raw features, the y series containing the target, and the size of the test group (i.e. 0.1 for 10%), we get back the X_train, X_test, y_train and y_test data to use in the model.","40d47cc3":"# Define the model features and target\n## Extract X and y ","6347840b":"{'XGBR__n_estimators': 100}","ecb63c19":"# What should we do for each colmun\n## Separate features by dtype\n\nNext we\u2019ll separate the features in the dataframe by their datatype. There are a few different ways to achieve this. I\u2019ve used the select_dtypes() function to obtain specific data types by passing in np.number to obtain the numeric data and exclude=['np.number'] to return the categorical data. Appending .columns to the end returns an Index list containing the column names. For the categorical features, we don\u2019t want to include the target income column, so I\u2019ve dropped that.\n## Cat Features ","fcfbd3ce":"# Convert Dtypes : ","2136b113":"#  Submit to the competition\nWe'll begin by using the trained model to generate predictions, which we'll save to a CSV file.\n","60720190":"# Check\n","279a3a95":"# Explore the  data\n- Null Data\n- Categorical data \n- Is there Text data \n- wich columns will we use \n- IS there outliers that can destory our algo \n- IS there diffrent range  of data ","88f74059":"# Create complexe transformer  in order to  put all transformations in the same pipe \n    'num_columns' :Cleaning->Valeur Manquante -> Standar_Scaler\n    'cat_columns' : Cleaning -> Valeur Manquante -> Categorique [One Hot]\n\n\n    fill_missing_then_Standar_scaler = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n        StandardScaler()\n    )\n\n    \"Write a pattern to extract numbers and decimals\"\n    def return_number(string):\n        pattern = re.compile(r\"\\d+\\.\\d+\")\n        # Search the text for matches\n        number = re.match(pattern, string)\n        # If a value is returned, use group(0) to return the found value\n        if number is not None:\n            return float(number.group(0))\n\n    extraire_number_then_imput_then_scale = make_pipeline(\n        FunctionTransformer(extract_number),\n        fill_missing_then_Standar_scaler,\n    )    \n","9a40bdfc":"# Compose num+cat : ColumnTransformer\n## First Standar preprocess","aea0ef5a":"## Num Features ","00fffc87":"# Find Best Pipe\n## Step 1: Cross-Validation"}}