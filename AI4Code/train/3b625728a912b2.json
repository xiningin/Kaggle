{"cell_type":{"5e7d5b48":"code","6c0535cf":"code","1a9fb177":"code","59e66f52":"code","ba895b28":"code","182a6ad0":"code","1879400d":"code","96629a8e":"code","8059b8a4":"code","fe5752db":"code","d8e6b015":"code","3c9f1011":"markdown","b92b4544":"markdown","5dc09edd":"markdown","c9348523":"markdown"},"source":{"5e7d5b48":"# K-Means Clustering\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom functools import partial","6c0535cf":"# Importing the dataset\ndataset = pd.read_csv('..\/input\/carscsv\/cars.csv')","1a9fb177":"dataset.head()","59e66f52":"X = dataset.iloc[:,:-1].values","ba895b28":"X.drop(X.columns[[4, 2]], axis = 1, inplace = True)","182a6ad0":"X = pd.DataFrame(X)\nX.apply(pd.to_numeric, errors='ignore').dtypes\n","1879400d":"# Eliminating null values\nfor i in X.columns:\n    X[i] = X[i].fillna(int(X[i].mean()))\nfor i in X.columns:\n    print(X[i].isnull().sum())","96629a8e":"# Using the elbow method to find  the optimal number of clusters\nfrom sklearn.cluster import KMeans\nwcss = []\nfor i in range(1,11):\n    kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1,11),wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","8059b8a4":"!pip install --user \"pandas<1.0\"","fe5752db":"# Applying k-means to the cars dataset\nkmeans = KMeans(n_clusters=3,init='k-means++',max_iter=300,n_init=10,random_state=0) \ny_kmeans = kmeans.fit_predict(X)\nX = X.as_matrix(columns=None)","d8e6b015":"# Visualising the clusters\nplt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0,1],s=100,c='red',label='US')\nplt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1,1],s=100,c='blue',label='Japan')\nplt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2,1],s=100,c='green',label='Europe')\nplt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],s=300,c='yellow',label='Centroids')\nplt.title('Clusters of car brands')\nplt.legend()\nplt.show()","3c9f1011":"k-means clustering is rather easy to apply to even large data sets, particularly when using heuristics such as Lloyd's algorithm. It has been successfully used in market segmentation, computer vision, and astronomy among many other domains. It often is used as a preprocessing step for other algorithms, for example to find a starting configuration.","b92b4544":"use colab to run below lines because the as_matrix is removed","5dc09edd":"k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.\n\nThe problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the Gaussian mixture model allows clusters to have different shapes.","c9348523":"## K-Means Algorithm\n### By : Ayush Kumar Namdeo"}}