{"cell_type":{"fecbece6":"code","ff3befb1":"code","9f955ffb":"code","a5089101":"code","f4ad6edc":"code","6a30a928":"code","cdbef767":"code","9d80ffaa":"code","bddd924b":"code","0e7b1690":"code","1a0ffb5f":"code","6753ec1f":"code","27458fa8":"code","f7c1813a":"code","e521b284":"code","4b3e84fb":"code","d84d75fc":"code","497deb14":"code","59cf9284":"code","f2bb61c4":"code","0fae92b7":"code","74206b1f":"code","ad2ae861":"code","dcdf5e39":"code","39730e76":"code","78b4d576":"code","5525b91f":"code","6850799a":"code","337e42d8":"code","6580118c":"code","eeb67690":"code","48fdbbc1":"code","0da8559a":"code","3ba678f4":"markdown","111e2e26":"markdown","4dec8639":"markdown","2cdc8015":"markdown","777f2e94":"markdown","cd6dd27b":"markdown","31e8bae9":"markdown","ecdf2c0a":"markdown","d078cd98":"markdown","055888ac":"markdown"},"source":{"fecbece6":"# Input folders\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","ff3befb1":"# Input files\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9f955ffb":"# Import relevant packages\n\nimport wfdb\nfrom wfdb import processing\nimport os\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport scipy\nimport shutil\nfrom IPython.display import display \nimport keras\nfrom keras import optimizers, losses, activations, models\nfrom keras.layers import Dense, Input, Dropout, Convolution1D, MaxPool1D, GlobalMaxPool1D, GlobalAveragePooling1D, concatenate, MaxPool2D\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Conv1D","a5089101":"# directory for dataset\ndir1 = '\/kaggle\/input\/bidmc-congestive-heart-failure\/files\/'\ndir2 = '\/kaggle\/input\/mitbih-normal-sinus-rhythm-database\/mit-bih-normal-sinus-rhythm-database-1.0.0\/'\ndf1, df2 = list(), list()\n\n# all the header files from chf dataset\nfor i in os.listdir(dir1):\n    if i.endswith(\".hea\"):\n        df1.append(i)\n# all the annotations from normal dataset\nfor i in os.listdir(dir2):\n    if i.endswith(\".atr\"):\n        df2.append(i)        \n\ndf1, df2 = pd.DataFrame(df1), pd.DataFrame(df2) \ndf_1 = pd.read_csv(\"\/kaggle\/input\/bidmc-congestive-heart-failure\/files\/RECORDS\", header=None)\ndf_2 = pd.read_csv(\"\/kaggle\/input\/mitbih-normal-sinus-rhythm-database\/mit-bih-normal-sinus-rhythm-database-1.0.0\/RECORDS\", header=None)","f4ad6edc":"# Display 1 record and its dictionary (from:100 - to:2100)\nrecord = wfdb.rdrecord('\/kaggle\/input\/bidmc-congestive-heart-failure\/files\/chf01', sampfrom=100, sampto=2100)\nann = wfdb.rdann('\/kaggle\/input\/bidmc-congestive-heart-failure\/files\/chf01', 'ecg', sampfrom=100, sampto=2100)\nwfdb.plot_wfdb(record, ann)\ndisplay(record.__dict__)","6a30a928":"# # Display 1 record and its dictionary (full)\n# record = wfdb.rdrecord('\/kaggle\/input\/bidmc-congestive-heart-failure\/files\/chf01')\n# ann = wfdb.rdann('\/kaggle\/input\/bidmc-congestive-heart-failure\/files\/chf01', 'ecg')\n# wfdb.plot_wfdb(record, ann)\n# display(record.__dict__)","cdbef767":"# Sample out signal from record\n\nplt.plot(record.p_signal)\nprint(\"Signal length: \" + str(len(record.p_signal)))","9d80ffaa":"# Downsample to 128 hz\nfs = 128\nnew_sig, new_ann = wfdb.processing.resample_multichan(record.p_signal, ann, record.fs, fs_target = fs)\nrecord.fs = fs # Update record's fs\nprint(\"New frequency sample: \" + str(record.fs))\nplt.plot(new_sig)\nprint(\"New signal length: \" + str(len(new_sig))) ","bddd924b":"# Z-normalise\navg = new_sig.mean() # Count Mean once\nsd = new_sig.std() # Count SD once\nprint(\"The average is \" + str(avg) + \"and the std is \" + str(sd))\nnew_sig = (new_sig - avg)\/(sd)\nplt.plot(new_sig);","0e7b1690":"# Extracting a beat every 5s (Incorrect way)\n# new_sig = new_sig[::5]\n# print(\"New signal length: \" + str(len(new_sig)))\n# plt.plot(new_sig);","1a0ffb5f":"# Update with pre-processed signal\nrecord.p_signal = new_sig\nrecord.sig_len = len(new_sig)\nwfdb.plot_wfdb(record, new_ann)\ndisplay(record.__dict__)\n\n# Need to find a way to extract (N) heartbeats via annotations ","6753ec1f":"# qrs = wfdb.processing.XQRS(record.p_signal[0], record.fs)\npeak_indices = wfdb.processing.gqrs_detect(record.p_signal, record.fs)\n# wfdb.processing.compute_hr(record.sig_len, peak_indices, record.fs)\nprint(len(peak_indices))\nprint(peak_indices)\nprint(peak_indices[0], peak_indices[1])\nplt.plot(record.p_signal);\nplt.plot(record.p_signal[(peak_indices[0]):(peak_indices[1])]);\n# plt.plot(record.p_signal[(peak_indices[0]-30):(peak_indices[0]+50)]); # 30 samples before, 50 samples after\n# Figure out how to train with all these beats","27458fa8":"wfdb.io.show_ann_labels()\nwfdb.io.show_ann_classes()","f7c1813a":"# # Batch pre-process\n# list1 = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15']\n# for i in list1:\n#   sig, fields = wfdb.rdsamp(\"\/kaggle\/input\/bidmc-congestive-heart-failure\/files\/chf{}\".format(i))\n#   ann = wfdb.rdann('\/kaggle\/input\/bidmc-congestive-heart-failure\/files\/chf{}'.format(i), 'ecg')\n#   new_sig, new_ann = wfdb.processing.resample_multichan(sig, ann=ann, fs=fields['fs'], fs_target=128) # Downsample to 128 hz\n#   avg = new_sig.mean() # Count Mean once\n#   sd = new_sig.std() # Count SD once\n#   new_sig = (new_sig - avg)\/(sd) # Z-normalise\n#   # Extracting a beat every 5s?\n#   # Filter for normal beats?\n#   np.savetxt(\"CHF{}_sig.csv\".format(i), new_sig, delimiter=\",\")\n#   np.savetxt(\"CHF{}_ann.csv\".format(i), new_sig, delimiter=\",\")\n#   print(\"Processing signal of \" + str(i) + \" \/ \" + str(len(list1)) + \"...\")","e521b284":"# # Downsample from 250mhz\n\n# for file in bidmc:\n#     with open(file) as records:\n#         i = wfdb.rdrecord(records)\n#         new_sig = wfdb.processing.resample_sig(i, 250, 128)\n#         new_ann = wfdb.processing.resample_ann(i, 250, 128)\n# #    record = wfdb.rdrecord(i)\n\n# #wfdb.plot_wfdb(record=record, title='Record chf03 from BIDMC-CHF Dataset') # Plot\n# #display(record.__dict__) # Annotations","4b3e84fb":"# Load one full record\nrecord = wfdb.rdrecord('..\/input\/bidmc-congestive-heart-failure\/files\/chf03')\nwfdb.plot_wfdb(record=record, title='Record chf03 from BIDMC-CHF Dataset') # Plot\ndisplay(record.__dict__) # Annotations","d84d75fc":"# Read certain sections of the WFDB record using the 'rdsamp' function\n# This function returns a numpy array (signal) and a dictionary (headers)\nsignals, fields = wfdb.rdsamp('..\/input\/bidmc-congestive-heart-failure\/files\/chf03', sampfrom=0, sampto=1000000)\ndisplay(signals) # Numpy array (signal)\ndisplay(fields) # Dictionary (headers)","497deb14":"# Read a WFDB header file only (without the signals)\nrecord = wfdb.rdheader('..\/input\/bidmc-congestive-heart-failure\/files\/chf03')\ndisplay(record.__dict__)","59cf9284":"# Read part of a WFDB annotation file into a wfdb.Annotation object, and plot the samples\nannotation = wfdb.rdann('..\/input\/bidmc-congestive-heart-failure\/files\/chf03','ecg', sampfrom=100000, sampto=110000)\nannotation.fs = 360\nwfdb.plot_wfdb(annotation=annotation, time_units='minutes')","f2bb61c4":"# Read a WFDB record and annotation. Plot all channels, and the annotation on top of channel 0.\nrecord = wfdb.rdrecord('..\/input\/bidmc-congestive-heart-failure\/files\/chf03', sampto = 1500)\nannotation = wfdb.rdann('..\/input\/bidmc-congestive-heart-failure\/files\/chf03', 'ecg', sampto = 1500)\n\nwfdb.plot_wfdb(record=record, annotation=annotation,\n               title='Record chf03 from BIDMC-CHF Dataset',\n               time_units='seconds')","0fae92b7":"# Reading a record\n# record = wfdb.rdrecord('sample-data\/100', physical=False)\n\n# Rename\n# record.record_name = '100x'\n\n# The new file can now be read as\n# record_x = wfdb.rdrecord('100x')","74206b1f":"# View the standard WFDB annotation labels\n# wfdb.show_ann_labels()","ad2ae861":"# Use the gqrs detection algorithm and correct the peaks\n\ndef peaks_hr(sig, peak_inds, fs, title, figsize=(20, 10), saveto=None):\n    \"Plot a signal with its peaks and heart rate\"\n    # Calculate heart rate\n    hrs = processing.compute_hr(sig_len=sig.shape[0], qrs_inds=peak_inds, fs=fs)\n    \n    N = sig.shape[0]\n    \n    fig, ax_left = plt.subplots(figsize=figsize)\n    ax_right = ax_left.twinx()\n    \n    ax_left.plot(sig, color='#3979f0', label='Signal')\n    ax_left.plot(peak_inds, sig[peak_inds], 'rx', marker='x', color='#8b0000', label='Peak', markersize=12)\n    ax_right.plot(np.arange(N), hrs, label='Heart rate', color='m', linewidth=2)\n\n    ax_left.set_title(title)\n\n    ax_left.set_xlabel('Time (ms)')\n    ax_left.set_ylabel('ECG (mV)', color='#3979f0')\n    ax_right.set_ylabel('Heart rate (bpm)', color='m')\n    # Make the y-axis label, ticks and tick labels match the line color.\n    ax_left.tick_params('y', colors='#3979f0')\n    ax_right.tick_params('y', colors='m')\n    if saveto is not None:\n        plt.savefig(saveto, dpi=600)\n    plt.show()\n\n# Load the wfdb record and the physical samples\nrecord = wfdb.rdrecord('..\/input\/bidmc-congestive-heart-failure\/files\/chf03', sampfrom=0, sampto=1000, channels=[1])\n\n# Use the gqrs algorithm to detect qrs locations in the first channel\nqrs_inds = processing.gqrs_detect(sig=record.p_signal[:,0], fs=record.fs)\n\n# Plot results\npeaks_hr(sig=record.p_signal, peak_inds=qrs_inds, fs=record.fs,\n        title=\"GQRS peak detection on chf03\")\n    \n# Correct the peaks shifting them to local maxima\nmin_bpm = 20\nmax_bpm = 230\n#min_gap = record.fs * 60 \/ min_bpm\n# Use the maximum possible bpm as the search radius\nsearch_radius = int(record.fs * 60 \/ max_bpm)\ncorrected_peak_inds = processing.correct_peaks(record.p_signal[:,0], peak_inds=qrs_inds,\n                                               search_radius=search_radius, smooth_window_size=150)\n\n# Display results\nprint('Corrected gqrs detected peak indices:', sorted(corrected_peak_inds))\npeaks_hr(sig=record.p_signal, peak_inds=sorted(corrected_peak_inds), fs=record.fs,\n         title=\"Corrected GQRS peak detection on chf03\")","dcdf5e39":"# Use xqrs detection algorithm and compare against reference annotations\nsig, fields = wfdb.rdsamp('..\/input\/bidmc-congestive-heart-failure\/files\/chf03', channels=[0], sampto=1500)\nann_ref = wfdb.rdann('..\/input\/bidmc-congestive-heart-failure\/files\/chf03','ecg', sampto=1500)\n\n# Run qrs detection on signal\nxqrs = processing.XQRS(sig=sig[:,0], fs=fields['fs'])\nxqrs.detect()\n# Alternatively, use the gateway function to get the qrs indices directly\n# qrs_inds = processing.xqrs_detect(sig=sig[:,0], fs=fields['fs'])\n\n# Compare detected qrs complexes to reference annotation.\n# Note, first sample in 100.atr is not a qrs.\ncomparitor = processing.compare_annotations(ref_sample=ann_ref.sample[1:],\n                                            test_sample=xqrs.qrs_inds,\n                                            window_width=int(0.1 * fields['fs']),\n                                            signal=sig[:,0])\n\n# Print and plot the results\ncomparitor.print_summary()\ncomparitor.plot(title='xqrs detected qrs vs reference annotations')","39730e76":"def beat_annotations(annotation):\n    \"\"\" Get rid of non-beat markers \"\"\"\n    \"\"\"'N' for normal beats. Similarly we can give the input 'L' for left bundle branch block beats. 'R' for right bundle branch block\n        beats. 'A' for Atrial premature contraction. 'V' for ventricular premature contraction. '\/' for paced beat. 'E' for Ventricular\n        escape beat.\"\"\"\n    \n    good = ['N']   \n    ids = np.in1d(annotation.symbol, good)\n\n    # We want to know only the positions\n    beats = annotation.sample[ids]\n\n    return beats\n\ndef segmentation(records):\n    Normal = []\n    for e in records:\n        signals, fields = wfdb.rdsamp(e, channels = [0]) \n        ann = wfdb.rdann(e, 'atr')\n        good = ['N']\n        ids = np.in1d(ann.symbol, good)\n        imp_beats = ann.sample[ids]\n        beats = (ann.sample)\n        for i in imp_beats:\n            beats = list(beats)\n            j = beats.index(i)\n            if(j!=0 and j!=(len(beats)-1)):\n                x = beats[j-1]\n                y = beats[j+1]\n                diff1 = abs(x - beats[j])\/\/2\n                diff2 = abs(y - beats[j])\/\/2\n                Normal.append(signals[beats[j] - diff1: beats[j] + diff2, 0])\n    return Normal","78b4d576":"dbs = wfdb.get_dbs()\ndisplay(dbs)","5525b91f":"# Make a temporary download directory in your current working directory\ncwd = os.getcwd()\nmitdb = os.path.join(cwd, 'mitdb')\n\n# Download content\nwfdb.dl_database('mitdb', dl_dir=mitdb) # MIT-BIH Arrhythmia Database\n\n# Display the downloaded content in the folder\ndisplay(os.listdir(mitdb))\n\n# Cleanup: delete the downloaded directory\n# shutil.rmtree(dl_dir)","6850799a":"from os import listdir\nfrom PIL import Image as PImage\n\ndef loadImages(path):\n    # return array of images\n    \n    imagesList = listdir(path)\n    loadedImages = []\n    for image in imagesList:\n        if image.endswith(\".dat\"):\n            img = PImage.open(path + image)\n            loadedImages.append(img)\n\n    return loadedImages\n\npath = \"mitbh\/\"\n\n# your images in an array\nimgs = loadImages(path)\n\nfor img in imgs:\n    # you can show every image\n    img.show()","337e42d8":"from keras.callbacks import ModelCheckpoint\nfilepath = 'm1'\n\ntrain_path = 'mitdb'\nvalid_path = 'mitdb'\n\ncheckpoint = ModelCheckpoint(filepath,\n                            monitor='val_acc',\n                            verbose=1,\n                            save_best_only=True,\n                            mode='max')\n\nbatch_size = 32\n\nIMAGE_SIZE = [128, 128]","6580118c":"from keras.layers.normalization import BatchNormalization\n\nmodel = Sequential()\nmodel.add(Conv1D(64, 3, input_shape = IMAGE_SIZE ,kernel_initializer='glorot_uniform'))\n\n\nmodel.add(keras.layers.ELU())\n\nmodel.add(BatchNormalization())\n\nmodel.add(Conv1D(64, 3,kernel_initializer='glorot_uniform'))\n\nmodel.add(keras.layers.ELU())\n\nmodel.add(BatchNormalization())\n\nmodel.add(MaxPool1D(pool_size=2, strides= 2))\n\nmodel.add(Conv1D(128, 3,kernel_initializer='glorot_uniform'))\n\nmodel.add(keras.layers.ELU())\n\nmodel.add(BatchNormalization())\n\nmodel.add(Conv1D(128, 3,kernel_initializer='glorot_uniform'))\n\nmodel.add(keras.layers.ELU())\n\nmodel.add(BatchNormalization())\n\nmodel.add(MaxPool1D(pool_size=2, strides= 2))\n\nmodel.add(Conv1D(256, 3,kernel_initializer='glorot_uniform'))\n\nmodel.add(keras.layers.ELU())\n\nmodel.add(BatchNormalization())\n\nmodel.add(Conv1D(256, 3,kernel_initializer='glorot_uniform'))\n\nmodel.add(keras.layers.ELU())\n\nmodel.add(BatchNormalization())\n\nmodel.add(MaxPool1D(pool_size=2, strides= 2))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(2048))\n\n\nmodel.add(keras.layers.ELU())\n\nmodel.add(BatchNormalization())\n\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(7, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint(model.summary())\n\ngen = ImageDataGenerator()\n\ntest_gen = gen.flow_from_directory(valid_path, target_size=IMAGE_SIZE)\n\ntrain_gen = gen.flow_from_directory(train_path, target_size=IMAGE_SIZE)\n\ntrain_generator = gen.flow_from_directory(\n  train_path,\n  target_size=IMAGE_SIZE,\n  shuffle=True,\n  batch_size=batch_size,\n)\nvalid_generator = gen.flow_from_directory(\n  valid_path,\n  target_size=IMAGE_SIZE,\n  shuffle=True,\n  batch_size=batch_size,\n)\ncallbacks_list = [checkpoint]\n\nr = model.fit_generator(\n  train_generator,\n  validation_data=valid_generator,\n  epochs=5,\n  steps_per_epoch=356702\/\/batch_size,\n  validation_steps=39634\/\/batch_size,callbacks=callbacks_list\n)","eeb67690":"from keras.layers.normalization import BatchNormalization\n\nmodel = Sequential()\nmodel.add(Conv2D(64, (3,3),strides = (1,1), input_shape = IMAGE_SIZE + [3],kernel_initializer='glorot_uniform'))\n\n\nmodel.add(keras.layers.ELU())\n\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(64, (3,3),strides = (1,1),kernel_initializer='glorot_uniform'))\n\nmodel.add(keras.layers.ELU())\n\nmodel.add(BatchNormalization())\n\nmodel.add(MaxPool2D(pool_size=(2, 2), strides= (2,2)))\n\nmodel.add(Conv2D(128, (3,3),strides = (1,1),kernel_initializer='glorot_uniform'))\n\nmodel.add(keras.layers.ELU())\n\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(128, (3,3),strides = (1,1),kernel_initializer='glorot_uniform'))\n\nmodel.add(keras.layers.ELU())\n\nmodel.add(BatchNormalization())\n\nmodel.add(MaxPool2D(pool_size=(2, 2), strides= (2,2)))\n\nmodel.add(Conv2D(256, (3,3),strides = (1,1),kernel_initializer='glorot_uniform'))\n\nmodel.add(keras.layers.ELU())\n\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(256, (3,3),strides = (1,1),kernel_initializer='glorot_uniform'))\n\nmodel.add(keras.layers.ELU())\n\nmodel.add(BatchNormalization())\n\nmodel.add(MaxPool2D(pool_size=(2, 2), strides= (2,2)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(2048))\n\n\nmodel.add(keras.layers.ELU())\n\nmodel.add(BatchNormalization())\n\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(7, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint(model.summary())\n\ngen = ImageDataGenerator()\n\ntest_gen = gen.flow_from_directory(valid_path, target_size=IMAGE_SIZE)\n\ntrain_gen = gen.flow_from_directory(train_path, target_size=IMAGE_SIZE)\n\ntrain_generator = gen.flow_from_directory(\n  train_path,\n  target_size=IMAGE_SIZE,\n  shuffle=True,\n  batch_size=batch_size,\n)\nvalid_generator = gen.flow_from_directory(\n  valid_path,\n  target_size=IMAGE_SIZE,\n  shuffle=True,\n  batch_size=batch_size,\n)\ncallbacks_list = [checkpoint]\n\nr = model.fit_generator(\n  train_generator,\n  validation_data=valid_generator,\n  epochs=5,\n  steps_per_epoch=356702\/\/batch_size,\n  validation_steps=39634\/\/batch_size,callbacks=callbacks_list\n)","48fdbbc1":"import os\nimport csv\nimport gc\nimport pickle\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import medfilt\nimport scipy.stats\nimport pywt\nimport time\nimport sklearn\nfrom sklearn import decomposition\nfrom sklearn.decomposition import PCA, IncrementalPCA\n\n\nfrom numpy.polynomial.hermite import hermfit, hermval\n\n\n\ndef create_features_labels_name(DS, winL, winR, do_preprocess, maxRR, use_RR, norm_RR, compute_morph, db_path, reduced_DS, leads_flag):\n    \n    features_labels_name = db_path + 'features\/' + 'w_' + str(winL) + '_' + str(winR) + '_' + DS \n\n    if do_preprocess:\n        features_labels_name += '_rm_bsline'\n\n    if maxRR:\n        features_labels_name += '_maxRR'\n\n    if use_RR:\n        features_labels_name += '_RR'\n    \n    if norm_RR:\n        features_labels_name += '_norm_RR'\n\n    for descp in compute_morph:\n        features_labels_name += '_' + descp\n\n    if reduced_DS:\n        features_labels_name += '_reduced'\n        \n    if leads_flag[0] == 1:\n        features_labels_name += '_MLII'\n\n    if leads_flag[1] == 1:\n        features_labels_name += '_V1'\n\n    features_labels_name += '.p'\n\n    return features_labels_name\n\n\ndef save_wvlt_PCA(PCA, pca_k, family, level):\n    f = open('Wvlt_' + family + '_' + str(level) + '_PCA_' + str(pca_k) + '.p', 'wb')\n    pickle.dump(PCA, f, 2)\n    f.close\n\n\ndef load_wvlt_PCA(pca_k, family, level):\n    f = open('Wvlt_' + family + '_' + str(level) + '_PCA_' + str(pca_k) + '.p', 'rb')\n    # disable garbage collector       \n    gc.disable()# this improve the required loading time!\n    PCA = pickle.load(f)\n    gc.enable()\n    f.close()\n\n    return PCA\n# Load the data with the configuration and features selected\n# Params:\n# - leads_flag = [MLII, V1] set the value to 0 or 1 to reference if that lead is used\n# - reduced_DS = load DS1, DS2 patients division (Chazal) or reduced version, \n#                i.e., only patients in common that contains both MLII and V1 \ndef load_mit_db(DS, winL, winR, do_preprocess, maxRR, use_RR, norm_RR, compute_morph, db_path, reduced_DS, leads_flag):\n\n    features_labels_name = create_features_labels_name(DS, winL, winR, do_preprocess, maxRR, use_RR, norm_RR, compute_morph, db_path, reduced_DS, leads_flag) \n\n    if os.path.isfile(features_labels_name):\n        print(\"Loading pickle: \" + features_labels_name + \"...\")\n        f = open(features_labels_name, 'rb')\n        # disable garbage collector       \n        gc.disable()# this improve the required loading time!\n        features, labels, patient_num_beats = pickle.load(f)\n        gc.enable()\n        f.close()\n\n\n    else:\n        print(\"Loading MIT BIH arr (\" + DS + \") ...\")\n\n        # ML-II\n        if reduced_DS == False:\n            DS1 = [101, 106, 108, 109, 112, 114, 115, 116, 118, 119, 122, 124, 201, 203, 205, 207, 208, 209, 215, 220, 223, 230]\n            DS2 = [100, 103, 105, 111, 113, 117, 121, 123, 200, 202, 210, 212, 213, 214, 219, 221, 222, 228, 231, 232, 233, 234]\n\n        # ML-II + V1\n        else:\n            DS1 = [101, 106, 108, 109, 112, 115, 118, 119, 201, 203, 205, 207, 208, 209, 215, 220, 223, 230]\n            DS2 = [105, 111, 113, 121, 200, 202, 210, 212, 213, 214, 219, 221, 222, 228, 231, 232, 233, 234]\n\n        mit_pickle_name = db_path + 'python_mit'\n        if reduced_DS:\n            mit_pickle_name = mit_pickle_name + '_reduced_'\n\n        if do_preprocess:\n            mit_pickle_name = mit_pickle_name + '_rm_bsline'\n\n        mit_pickle_name = mit_pickle_name + '_wL_' + str(winL) + '_wR_' + str(winR)\n        mit_pickle_name = mit_pickle_name + '_' + DS + '.p'\n\n        # If the data with that configuration has been already computed Load pickle\n        if os.path.isfile(mit_pickle_name):\n            f = open(mit_pickle_name, 'rb')\n            # disable garbage collector       \n            gc.disable()# this improve the required loading time!\n            my_db = pickle.load(f)\n            gc.enable()\n            f.close()\n        else: # Load data and compute de RR features \n            if DS == 'DS1':\n                my_db = load_signal(DS1, winL, winR, do_preprocess)\n            else:\n                my_db = load_signal(DS2, winL, winR, do_preprocess)\n\n            print(\"Saving signal processed data ...\")\n            # Save data\n            # Protocol version 0 itr_features_balanceds the original ASCII protocol and is backwards compatible with earlier versions of Python.\n            # Protocol version 1 is the old binary format which is also compatible with earlier versions of Python.\n            # Protocol version 2 was introduced in Python 2.3. It provides much more efficient pickling of new-style classes.\n            f = open(mit_pickle_name, 'wb')\n            pickle.dump(my_db, f, 2)\n            f.close\n\n        \n        features = np.array([], dtype=float)\n        labels = np.array([], dtype=np.int32)\n\n        # This array contains the number of beats for each patient (for cross_val)\n        patient_num_beats = np.array([], dtype=np.int32)\n        for p in range(len(my_db.beat)):\n            patient_num_beats = np.append(patient_num_beats, len(my_db.beat[p]))\n\n        # Compute RR features\n        if use_RR or norm_RR:\n            if DS == 'DS1':\n                RR = [RR_intervals() for i in range(len(DS1))]\n            else:\n                RR = [RR_intervals() for i in range(len(DS2))]\n\n            print(\"Computing RR intervals ...\")\n\n            for p in range(len(my_db.beat)):\n                if maxRR:\n                    RR[p] = compute_RR_intervals(my_db.R_pos[p])\n                else:\n                    RR[p] = compute_RR_intervals(my_db.orig_R_pos[p])\n                    \n                RR[p].pre_R = RR[p].pre_R[(my_db.valid_R[p] == 1)]\n                RR[p].post_R = RR[p].post_R[(my_db.valid_R[p] == 1)]\n                RR[p].local_R = RR[p].local_R[(my_db.valid_R[p] == 1)]\n                RR[p].global_R = RR[p].global_R[(my_db.valid_R[p] == 1)]\n\n\n        if use_RR:\n            f_RR = np.empty((0,4))\n            for p in range(len(RR)):\n                row = np.column_stack((RR[p].pre_R, RR[p].post_R, RR[p].local_R, RR[p].global_R))\n                f_RR = np.vstack((f_RR, row))\n\n            features = np.column_stack((features, f_RR)) if features.size else f_RR\n        \n        if norm_RR:\n            f_RR_norm = np.empty((0,4))\n            for p in range(len(RR)):\n                # Compute avg values!\n                avg_pre_R = np.average(RR[p].pre_R)\n                avg_post_R = np.average(RR[p].post_R)\n                avg_local_R = np.average(RR[p].local_R)\n                avg_global_R = np.average(RR[p].global_R)\n\n                row = np.column_stack((RR[p].pre_R \/ avg_pre_R, RR[p].post_R \/ avg_post_R, RR[p].local_R \/ avg_local_R, RR[p].global_R \/ avg_global_R))\n                f_RR_norm = np.vstack((f_RR_norm, row))\n\n            features = np.column_stack((features, f_RR_norm))  if features.size else f_RR_norm\n\n        #########################################################################################\n        # Compute morphological features\n        print(\"Computing morphological features (\" + DS + \") ...\")\n\n        num_leads = np.sum(leads_flag)\n\n        # Raw\n        if 'resample_10' in compute_morph:\n            print(\"Resample_10 ...\")\n            start = time.time()\n\n            f_raw = np.empty((0, 10 * num_leads))\n\n            for p in range(len(my_db.beat)):\n                for beat in my_db.beat[p]:\n                    f_raw_lead = np.empty([])\n                    for s in range(2):\n                        if leads_flag[s] == 1:\n                            resamp_beat = scipy.signal.resample(beat[s], 10)\n                            if f_raw_lead.size == 1:\n                                f_raw_lead =  resamp_beat\n                            else:\n                                f_raw_lead = np.hstack((f_raw_lead, resamp_beat))\n                    f_raw = np.vstack((f_raw, f_raw_lead))\n\n            features = np.column_stack((features, f_raw))  if features.size else f_raw\n\n            end = time.time()\n            print(\"Time resample: \" + str(format(end - start, '.2f')) + \" sec\" )\n\n        if 'raw' in compute_morph:\n            print(\"Raw ...\")\n            start = time.time()\n\n            f_raw = np.empty((0, (winL + winR) * num_leads))\n\n            for p in range(len(my_db.beat)):\n                for beat in my_db.beat[p]:\n                    f_raw_lead = np.empty([])\n                    for s in range(2):\n                        if leads_flag[s] == 1:\n                            if f_raw_lead.size == 1:\n                                f_raw_lead =  beat[s]\n                            else:\n                                f_raw_lead = np.hstack((f_raw_lead, beat[s]))\n                    f_raw = np.vstack((f_raw, f_raw_lead))\n\n            features = np.column_stack((features, f_raw))  if features.size else f_raw\n\n            end = time.time()\n            print(\"Time raw: \" + str(format(end - start, '.2f')) + \" sec\" )\n        # LBP 1D\n        # 1D-local binary pattern based feature extraction for classification of epileptic EEG signals: 2014, unas 55 citas, Q2-Q1 Matematicas\n        # https:\/\/ac.els-cdn.com\/S0096300314008285\/1-s2.0-S0096300314008285-main.pdf?_tid=8a8433a6-e57f-11e7-98ec-00000aab0f6c&acdnat=1513772341_eb5d4d26addb6c0b71ded4fd6cc23ed5\n\n        # 1D-LBP method, which derived from implementation steps of 2D-LBP, was firstly proposed by Chatlani et al. for detection of speech signals that is non-stationary in nature [23]\n\n        # From Raw signal\n\n        # TODO: Some kind of preprocesing or clean high frequency noise?\n\n        # Compute 2 Histograms: LBP or Uniform LBP\n        # LBP 8 = 0-255\n        # U-LBP 8 = 0-58\n        # Uniform LBP are only those pattern wich only presents 2 (or less) transitions from 0-1 or 1-0\n        # All the non-uniform patterns are asigned to the same value in the histogram\n\n        if 'u-lbp' in compute_morph:\n            print(\"u-lbp ...\")\n\n            f_lbp = np.empty((0, 59 * num_leads))\n\n            for p in range(len(my_db.beat)):\n                for beat in my_db.beat[p]:\n                    f_lbp_lead = np.empty([])\n                    for s in range(2):\n                        if leads_flag[s] == 1:\n                            if f_lbp_lead.size == 1:\n\n                                f_lbp_lead = compute_Uniform_LBP(beat[s], 8)\n                            else:\n                                f_lbp_lead = np.hstack((f_lbp_lead, compute_Uniform_LBP(beat[s], 8)))\n                    f_lbp = np.vstack((f_lbp, f_lbp_lead))\n\n            features = np.column_stack((features, f_lbp))  if features.size else f_lbp\n            print(features.shape)\n\n        if 'lbp' in compute_morph:\n            print(\"lbp ...\")\n\n            f_lbp = np.empty((0, 16 * num_leads))\n\n            for p in range(len(my_db.beat)):\n                for beat in my_db.beat[p]:\n                    f_lbp_lead = np.empty([])\n                    for s in range(2):\n                        if leads_flag[s] == 1:\n                            if f_lbp_lead.size == 1:\n\n                                f_lbp_lead = compute_LBP(beat[s], 4)\n                            else:\n                                f_lbp_lead = np.hstack((f_lbp_lead, compute_LBP(beat[s], 4)))\n                    f_lbp = np.vstack((f_lbp, f_lbp_lead))\n\n            features = np.column_stack((features, f_lbp))  if features.size else f_lbp\n            print(features.shape)\n\n\n        if 'hbf5' in compute_morph:\n            print(\"hbf ...\")\n\n            f_hbf = np.empty((0, 15 * num_leads))\n\n            for p in range(len(my_db.beat)):\n                for beat in my_db.beat[p]:\n                    f_hbf_lead = np.empty([])\n                    for s in range(2):\n                        if leads_flag[s] == 1:\n                            if f_hbf_lead.size == 1:\n\n                                f_hbf_lead = compute_HBF(beat[s])\n                            else:\n                                f_hbf_lead = np.hstack((f_hbf_lead, compute_HBF(beat[s])))\n                    f_hbf = np.vstack((f_hbf, f_hbf_lead))\n\n            features = np.column_stack((features, f_hbf))  if features.size else f_hbf\n            print(features.shape)\n\n        # Wavelets\n        if 'wvlt' in compute_morph:\n            print(\"Wavelets ...\")\n\n            f_wav = np.empty((0, 23 * num_leads))\n\n            for p in range(len(my_db.beat)):\n                for b in my_db.beat[p]:\n                    f_wav_lead = np.empty([])\n                    for s in range(2):\n                        if leads_flag[s] == 1:\n                            if f_wav_lead.size == 1:\n                                f_wav_lead =  compute_wavelet_descriptor(b[s], 'db1', 3)\n                            else:\n                                f_wav_lead = np.hstack((f_wav_lead, compute_wavelet_descriptor(b[s], 'db1', 3)))\n                    f_wav = np.vstack((f_wav, f_wav_lead))\n                    #f_wav = np.vstack((f_wav, compute_wavelet_descriptor(b,  'db1', 3)))\n\n            features = np.column_stack((features, f_wav))  if features.size else f_wav\n\n\n        # Wavelets\n        if 'wvlt+pca' in compute_morph:\n            pca_k = 7\n            print(\"Wavelets + PCA (\"+ str(pca_k) + \"...\")\n            \n            family = 'db1'\n            level = 3\n\n            f_wav = np.empty((0, 23 * num_leads))\n\n            for p in range(len(my_db.beat)):\n                for b in my_db.beat[p]:\n                    f_wav_lead = np.empty([])\n                    for s in range(2):\n                        if leads_flag[s] == 1:\n                            if f_wav_lead.size == 1:\n                                f_wav_lead =  compute_wavelet_descriptor(b[s], family, level)\n                            else:\n                                f_wav_lead = np.hstack((f_wav_lead, compute_wavelet_descriptor(b[s], family, level)))\n                    f_wav = np.vstack((f_wav, f_wav_lead))\n                    #f_wav = np.vstack((f_wav, compute_wavelet_descriptor(b,  'db1', 3)))\n\n\n            if DS == 'DS1':\n                # Compute PCA\n                #PCA = sklearn.decomposition.KernelPCA(pca_k) # gamma_pca\n                IPCA = IncrementalPCA(n_components=pca_k, batch_size=10)# NOTE: due to memory errors, we employ IncrementalPCA\n                IPCA.fit(f_wav) \n\n                # Save PCA\n                save_wvlt_PCA(IPCA, pca_k, family, level)\n            else:\n                # Load PCAfrom sklearn.decomposition import PCA, IncrementalPCA\n                IPCA = load_wvlt_PCA( pca_k, family, level)\n            # Extract the PCA\n            #f_wav_PCA = np.empty((0, pca_k * num_leads))\n            f_wav_PCA = IPCA.transform(f_wav)\n            features = np.column_stack((features, f_wav_PCA))  if features.size else f_wav_PCA\n\n        # HOS\n        if 'HOS' in compute_morph:\n            print(\"HOS ...\")\n            n_intervals = 6\n            lag = int(round( (winL + winR )\/ n_intervals))\n\n            f_HOS = np.empty((0, (n_intervals-1) * 2 * num_leads))\n            for p in range(len(my_db.beat)):\n                for b in my_db.beat[p]:\n                    f_HOS_lead = np.empty([])\n                    for s in range(2):\n                        if leads_flag[s] == 1:\n                            if f_HOS_lead.size == 1:\n                                f_HOS_lead =  compute_hos_descriptor(b[s], n_intervals, lag)\n                            else:\n                                f_HOS_lead = np.hstack((f_HOS_lead, compute_hos_descriptor(b[s], n_intervals, lag)))\n                    f_HOS = np.vstack((f_HOS, f_HOS_lead))\n                    #f_HOS = np.vstack((f_HOS, compute_hos_descriptor(b, n_intervals, lag)))\n\n            features = np.column_stack((features, f_HOS))  if features.size else f_HOS\n            print(features.shape)\n\n        # My morphological descriptor\n        if 'myMorph' in compute_morph:\n            print(\"My Descriptor ...\")\n            f_myMorhp = np.empty((0,4 * num_leads))\n            for p in range(len(my_db.beat)):\n                for b in my_db.beat[p]:\n                    f_myMorhp_lead = np.empty([])\n                    for s in range(2):\n                        if leads_flag[s] == 1:\n                            if f_myMorhp_lead.size == 1:\n                                f_myMorhp_lead =  compute_my_own_descriptor(b[s], winL, winR)\n                            else:\n                                f_myMorhp_lead = np.hstack((f_myMorhp_lead, compute_my_own_descriptor(b[s], winL, winR)))\n                    f_myMorhp = np.vstack((f_myMorhp, f_myMorhp_lead))\n                    #f_myMorhp = np.vstack((f_myMorhp, compute_my_own_descriptor(b, winL, winR)))\n                    \n            features = np.column_stack((features, f_myMorhp))  if features.size else f_myMorhp\n\n        \n        labels = np.array(sum(my_db.class_ID, [])).flatten()\n        print(\"labels\")\n\n        # Set labels array!\n        print('writing pickle: ' +  features_labels_name + '...')\n        f = open(features_labels_name, 'wb')\n        pickle.dump([features, labels, patient_num_beats], f, 2)\n        f.close\n\n    return features, labels, patient_num_beats\n\n\n# DS: contains the patient list for load\n# winL, winR: indicates the size of the window centred at R-peak at left and right side\n# do_preprocess: indicates if preprocesing of remove baseline on signal is performed\ndef load_signal(DS, winL, winR, do_preprocess):\n\n    class_ID = [[] for i in range(len(DS))]\n    beat = [[] for i in range(len(DS))] # record, beat, lead\n    R_poses = [ np.array([]) for i in range(len(DS))]\n    Original_R_poses = [ np.array([]) for i in range(len(DS))]   \n    valid_R = [ np.array([]) for i in range(len(DS))]\n    my_db = mit_db()\n    patients = []\n\n    # Lists \n    # beats = []\n    # classes = []\n    # valid_R = np.empty([])\n    # R_poses = np.empty([])\n    # Original_R_poses = np.empty([])\n\n    size_RR_max = 20\n\n    pathDB = '\/home\/mondejar\/dataset\/ECG\/'\n    DB_name = 'mitdb'\n    fs = 360\n    jump_lines = 1\n\n    # Read files: signal (.csv )  annotations (.txt)    \n    fRecords = list()\n    fAnnotations = list()\n\n    lst = os.listdir(pathDB + DB_name + \"\/csv\")\n    lst.sort()\n    for file in lst:\n        if file.endswith(\".csv\"):\n            if int(file[0:3]) in DS:\n                fRecords.append(file)\n        elif file.endswith(\".txt\"):\n            if int(file[0:3]) in DS:\n                fAnnotations.append(file)        \n\n    MITBIH_classes = ['N', 'L', 'R', 'e', 'j', 'A', 'a', 'J', 'S', 'V', 'E', 'F']#, 'P', '\/', 'f', 'u']\n    AAMI_classes = []\n    AAMI_classes.append(['N', 'L', 'R'])                    # N\n    AAMI_classes.append(['A', 'a', 'J', 'S', 'e', 'j'])     # SVEB \n    AAMI_classes.append(['V', 'E'])                         # VEB\n    AAMI_classes.append(['F'])                              # F\n    #AAMI_classes.append(['P', '\/', 'f', 'u'])              # Q\n\n    RAW_signals = []\n    r_index = 0\n\n    #for r, a in zip(fRecords, fAnnotations):\n    for r in range(0, len(fRecords)):\n\n        print(\"Processing signal \" + str(r) + \" \/ \" + str(len(fRecords)) + \"...\")\n\n        # 1. Read signalR_poses\n        filename = pathDB + DB_name + \"\/csv\/\" + fRecords[r]\n        print(filename)\n        f = open(filename, 'rb')\n        reader = csv.reader(f, delimiter=',')\n        next(reader) # skip first line!\n        MLII_index = 1\n        V1_index = 2\n        if int(fRecords[r][0:3]) == 114:\n            MLII_index = 2\n            V1_index = 1\n\n        MLII = []\n        V1 = []\n        for row in reader:\n            MLII.append((int(row[MLII_index])))\n            V1.append((int(row[V1_index])))\n        f.close()\n\n\n        RAW_signals.append((MLII, V1)) ## NOTE a copy must be created in order to preserve the original signal\n        # display_signal(MLII)\n\n        # 2. Read annotations\n        filename = pathDB + DB_name + \"\/csv\/\" + fAnnotations[r]\n        print(filename)\n        f = open(filename, 'rb')\n        next(f) # skip first line!\n\n        annotations = []\n        for line in f:\n            annotations.append(line)\n        f.close\n        # 3. Preprocessing signal!\n        if do_preprocess:\n            #scipy.signal\n            # median_filter1D\n            baseline = medfilt(MLII, 71) \n            baseline = medfilt(baseline, 215) \n\n            # Remove Baseline\n            for i in range(0, len(MLII)):\n                MLII[i] = MLII[i] - baseline[i]\n\n            # TODO Remove High Freqs\n\n            # median_filter1D\n            baseline = medfilt(V1, 71) \n            baseline = medfilt(baseline, 215) \n\n            # Remove Baseline\n            for i in range(0, len(V1)):\n                V1[i] = V1[i] - baseline[i]\n\n\n        # Extract the R-peaks from annotations\n        for a in annotations:\n            aS = a.split()\n            \n            pos = int(aS[1])\n            originalPos = int(aS[1])\n            classAnttd = aS[2]\n            if pos > size_RR_max and pos < (len(MLII) - size_RR_max):\n                index, value = max(enumerate(MLII[pos - size_RR_max : pos + size_RR_max]), key=operator.itemgetter(1))\n                pos = (pos - size_RR_max) + index\n\n            peak_type = 0\n            #pos = pos-1\n            \n            if classAnttd in MITBIH_classes:\n                if(pos > winL and pos < (len(MLII) - winR)):\n                    beat[r].append( (MLII[pos - winL : pos + winR], V1[pos - winL : pos + winR]))\n                    for i in range(0,len(AAMI_classes)):\n                        if classAnttd in AAMI_classes[i]:\n                            class_AAMI = i\n                            break #exit loop\n                    #convert class\n                    class_ID[r].append(class_AAMI)\n\n                    valid_R[r] = np.append(valid_R[r], 1)\n                else:\n                    valid_R[r] = np.append(valid_R[r], 0)\n            else:\n                valid_R[r] = np.append(valid_R[r], 0)\n            \n            R_poses[r] = np.append(R_poses[r], pos)\n            Original_R_poses[r] = np.append(Original_R_poses[r], originalPos)\n        \n        #R_poses[r] = R_poses[r][(valid_R[r] == 1)]\n        #Original_R_poses[r] = Original_R_poses[r][(valid_R[r] == 1)]\n\n        \n    # Set the data into a bigger struct that keep all the records!\n    my_db.filename = fRecords\n\n    my_db.raw_signal = RAW_signals\n    my_db.beat = beat # record, beat, lead\n    my_db.class_ID = class_ID\n    my_db.valid_R = valid_R\n    my_db.R_pos = R_poses\n    my_db.orig_R_pos = Original_R_poses\n\n    return my_db","0da8559a":"# features, labels, patient_num_beats = load_mit_bh(DS, winL, winR, do_preprocess, maxRR, use_RR, norm_RR, compute_morph, db_path, reduced_DS, leads_flag)","3ba678f4":"# 2. Study\n## 2.1 Data\n**Content:**\n1. Verify local data\n2. Import relevant packages\n\n**Data:**\n1. MIT-BIH https:\/\/physionet.org\/content\/nsrdb\/1.0.0\/ - **275,974 beats** *(Control)* <br><\/br> \n18 long-term ECG recordings of normal healthy not-arrhythmic subjects(Females = 13; age range: 20 to 50) \n\n\n2. BIDMC https:\/\/physionet.org\/content\/chfdb\/1.0.0\/ - **214,531 beats** *(Intervention)* <br><\/br>\n15 subjects with severe CHF (i.e., NYHA classes III-IV) (Females = 4; age range: 22 to 63)\n\nTotal: 490,505 heart-beats","111e2e26":"## Toying with wfdb package\nReference: <br><\/br> \nhttps:\/\/wfdb.readthedocs.io\/en\/latest\/ <br><\/br>\nhttps:\/\/github.com\/MIT-LCP\/wfdb-python\/blob\/master\/demo.ipynb","4dec8639":"# Misc from this point on:","2cdc8015":"## 2.3 Beats classification\n1. Train-validation-test (50-25-25)\n2. Split on people, as each person's beat is highly intercorrelated, and should not be tested with another person.\n3. Perform this split 10 times, and find the average results (questionable to perform this for 1.0 accuracy?)\n4. Two evaluation: a) Individual beat classification (490k data), b) classification on 5 min of ECG (lesser data)\n5. Denoting beats as normal vs CHF, (N_0 vs N_1)\n\n## 2.4 Performance measures\n1. Accuracy (Acc)\n2. Precision (P)\n3. Sensitivity (Sens)\n4. Area under Curve (AUC)\n\n## 2.5 1D CNN Classifier\nReferences: <br><\/br>\nhttps:\/\/github.com\/CVxTz\/ECG_Heartbeat_Classification\/blob\/master\/code\/baseline_mitbih.py\n![image.png](attachment:image.png)\n\n**1. Input**<br><\/br>\n**2. Feature extraction:** <br><\/br><br><\/br>\nBlock 1:<br><\/br>\nmodel.add(Conv1D((1,10,20), (20, 80), strides=1, activation='relu'))<br><\/br>\nBatch normalization: bn = BN(y)<br><\/br>\nact = ReLU(bn)\n\nBlock 2:<br><\/br>\nmodel.add(Conv1D((1,15,20), (20, 80), strides=1, activation='relu'))<br><\/br>\nBatch normalization: bn = BN(y)<br><\/br>\nact = ReLU(bn)\n\nBlock 3:<br><\/br>\nGrad-CAM, 1x20x20 filter, 20x80 kernel size, strides = 1<br><\/br>\nBatch normalization: bn = BN(y)<br><\/br>\nact = ReLU(bn)\n\n**3. Multilayer Perceptron (MLP)**<br><\/br>\nFully-connected (FC) to a 30-neurons hidden layer \n\n**4. Output layer**<br><\/br>\nmodel.add(Activation('softmax'))\n\n## 2.6 Visualisation\nGrad-CAM technique\n\n## 2.7 Experimental settings\n- keras.optimizers.Adam(learning_rate=0.001) or \n- model.compile(loss='mean_squared_error', optimizer='adam')<br><\/br>\nStill very unsure about this part","777f2e94":"## Random Functions","cd6dd27b":" # Detection of CHF using ECG\n \n ## 1. Abstract\n**Goal:**<br><\/br>\nAs part of our group project in BC2407 Analytics II, we would attempt to **reproduce** the existing **1.0 accuracy model** on detecting congestive heart failure using 1-D convolutional neural networks with only one raw electrocardiogram(ECG) heartbeat only.\n\n\n**Target Study:**<br><\/br>\nhttps:\/\/www.sciencedirect.com\/science\/article\/pii\/S1746809419301776#bib0255\n\n**Motivation**<br><\/br>\nCongestive Heart Failure (CHF) or often referred to simply as \u201cheart failure,\u201d as fluid builds up around the heart and causes it to pump inefficiently. CHF is an important concern for its high prevalence, high mortality rates, and sustained healthcare cost.\n\n**Data:**\n1. MIT-BIH https:\/\/physionet.org\/content\/nsrdb\/1.0.0\/ - **275,974 beats** *(Control)* <br><\/br> \n18 long-term ECG recordings of normal healthy not-arrhythmic subjects(Females = 13; age range: 20 to 50) \n\n\n2. BIDMC https:\/\/physionet.org\/content\/chfdb\/1.0.0\/ - **214,531 beats** *(Intervention)* <br><\/br>\n15 subjects with severe CHF (i.e., NYHA classes III-IV) (Females = 4; age range: 22 to 63)\n\nTotal: 490,505 heart-beats\n\n**Similar studies:**<br><\/br>\nhttps:\/\/link.springer.com\/article\/10.1007%2Fs10489-018-1179-1#Sec9 <br><\/br>\nhttps:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC5105944\/\n\n**Resources:**<br><\/br>\nhttps:\/\/physionet.org\/ <br><\/br>\nhttps:\/\/pypi.org\/project\/wfdb\/ <br><\/br>\nhttps:\/\/www.youtube.com\/watch?v=ARqBONXh1EM\n","31e8bae9":"# Possibly useful functions","ecdf2c0a":"## 2.2 Preprocessing\nhttps:\/\/github.com\/mondejar\/ecg-classification <br><\/br>\nhttps:\/\/github.com\/mondejar\/ecg-classification\/blob\/master\/tensorflow\/create_traindataset_mitdb.py <br><\/br>\nhttps:\/\/github.com\/tejasa97\/ECG-Signal-Processing\/blob\/master\/ECG.py\n1. Downsample BIDMC to 128Hz to fit MIT-BIH\n2. Filter for only (N) normal beats\n3. Each heartbeat being z-normalised\n4. Since each subject has a large amount of up to 70,000 beats, the study extracted a beat every 5s","d078cd98":"# Other data set","055888ac":"### Download streaming content from Physiobank"}}