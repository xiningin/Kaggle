{"cell_type":{"53e7ef5a":"code","ab905e98":"code","6a5de37d":"code","946cdbc1":"code","6b7e00ee":"code","0b2dc4c9":"code","9dac1222":"code","4f66940b":"code","72212fd2":"code","21e63496":"code","0096fcfe":"code","0c783e96":"code","a9452395":"code","4159294e":"code","add18eea":"code","504c0d94":"code","ca9ed246":"code","329e085b":"code","b0d6960c":"code","7d239687":"code","57de32f3":"code","34f8dd9d":"code","4a9cc8d1":"code","9aa171d1":"code","91d79577":"code","2c17297f":"code","e042904a":"markdown","f4b0775f":"markdown","c9add549":"markdown","d8f7d9b5":"markdown"},"source":{"53e7ef5a":"!pip install --upgrade wandb","ab905e98":"# %reload_ext autoreload\n# %autoreload 2\n# %matplotlib inline\n\nimport wandb\nimport random\nimport cv2\nimport os\nimport math\nimport torch\nimport torchvision\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.io\nimport xml.etree.ElementTree as ET\n\n\nfrom pathlib import Path\nfrom torch.utils.data import Dataset, DataLoader, random_split, sampler\nfrom torchvision import models, datasets, models\nfrom PIL import Image\nfrom tqdm import tqdm","6a5de37d":"!wandb login 7b168206341802a4b8b484d898e5ec80a7d5bc8d","946cdbc1":"wandb.init(project=\"bcnn\", entity=\"authors\")","6b7e00ee":"SEED = 222\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)","0b2dc4c9":"def crop_image(breed, dog, data_dir):\n    \n    img = plt.imread(data_dir + 'images\/Images\/' + breed + '\/' + dog + '.jpg')\n    tree = ET.parse(data_dir + 'annotations\/Annotation\/' + breed + '\/' + dog)\n    xmin = int(tree.getroot().findall('object')[0].find('bndbox').find('xmin').text)\n    xmax = int(tree.getroot().findall('object')[0].find('bndbox').find('xmax').text)\n    ymin = int(tree.getroot().findall('object')[0].find('bndbox').find('ymin').text)\n    ymax = int(tree.getroot().findall('object')[0].find('bndbox').find('ymax').text)\n    img = img[ymin:ymax, xmin:xmax, :]\n    \n    return img","9dac1222":"data_dir = '\/kaggle\/input\/stanford-dogs-dataset\/'\nbreed_list = os.listdir(data_dir + 'images\/Images\/')\nplt.figure(figsize=(20, 20))\n\nfor i in range(4):\n    \n    plt.subplot(421 + (i*2))\n    \n    breed = np.random.choice(breed_list)\n    dog = np.random.choice(os.listdir(data_dir + 'annotations\/Annotation\/' + breed))\n    img = plt.imread(data_dir + 'images\/Images\/' + breed + '\/' + dog + '.jpg')\n    plt.imshow(img)\n    \n    tree = ET.parse(data_dir + 'annotations\/Annotation\/' + breed + '\/' + dog)\n    xmin = int(tree.getroot().findall('object')[0].find('bndbox').find('xmin').text)\n    xmax = int(tree.getroot().findall('object')[0].find('bndbox').find('xmax').text)\n    ymin = int(tree.getroot().findall('object')[0].find('bndbox').find('ymin').text)\n    ymax = int(tree.getroot().findall('object')[0].find('bndbox').find('ymax').text)\n    \n    plt.plot([xmin, xmax, xmax, xmin, xmin], [ymin, ymin, ymax, ymax, ymin])\n    \n    crop_img = crop_image(breed, dog, data_dir)\n    plt.subplot(422 + (i*2))\n    plt.imshow(crop_img)","4f66940b":"if 'cropped_data' not in os.listdir():\n    \n    os.mkdir('cropped_data')\n    \n    for breed in breed_list:\n        os.mkdir('cropped_data\/' + breed)\n    \nprint('Created {} folders to store cropped images of the different breeds.'.format(len(os.listdir('cropped_data'))))","72212fd2":"if 'cropped_data_test' not in os.listdir():\n    \n    os.mkdir('cropped_data_test')\n    \n    for breed in breed_list:\n        os.mkdir('cropped_data_test\/' + breed)\n    \nprint('Created {} folders to store cropped images of the different breeds.'.format(len(os.listdir('cropped_data_test'))))","21e63496":"test_mat = scipy.io.loadmat('\/kaggle\/input\/test-data\/test_list (1).mat')\ntest_files = set()\n\nfor i in range(test_mat[\"file_list\"].shape[0]):\n    test_files.add(test_mat[\"file_list\"][i][0][0])","0096fcfe":"### making a folder for predictions\nif 'prediction_samples' not in os.listdir():    \n    os.mkdir('prediction_samples')\n\ntest_count = 0\n\nfor breed in tqdm(os.listdir('cropped_data')):\n    \n    for file in os.listdir(data_dir + 'annotations\/Annotation\/' + breed):\n                \n        img = Image.open(data_dir + 'images\/Images\/' + breed + '\/' + file + '.jpg')\n        tree = ET.parse(data_dir + 'annotations\/Annotation\/' + breed + '\/' + file)\n        \n        xmin = int(tree.getroot().findall('object')[0].find('bndbox').find('xmin').text)\n        xmax = int(tree.getroot().findall('object')[0].find('bndbox').find('xmax').text)\n        ymin = int(tree.getroot().findall('object')[0].find('bndbox').find('ymin').text)\n        ymax = int(tree.getroot().findall('object')[0].find('bndbox').find('ymax').text)\n        img = img.crop((xmin,ymin,xmax,ymax))\n        img = img.convert('RGB')\n        \n        if breed+\"\/\"+file+\".jpg\" in test_files:\n            test_count += 1\n            img.save('cropped_data_test\/' + breed + '\/' + file + '.jpg')\n            \n            if test_count % 500 == 0:\n                img.save('prediction_samples\/'+breed + '.jpg')\n\n            \n        else:\n            \n            ### using some image files later for prediction.            \n            img.save('cropped_data\/' + breed + '\/' + file + '.jpg')","0c783e96":"img_count = 0\n\nfile_name = \"cropped_data_test\/\"\nfor folder in os.listdir(file_name):    \n    for _ in os.listdir(file_name + folder):    \n        img_count += 1\n\nprint('No. of Images: {}'.format(img_count))","a9452395":"# Data Augmentation\nbatch_size = 128\nimage_size = 224\n\nimage_transforms = {\n    \n    'train':torchvision.transforms.Compose([\n            torchvision.transforms.Resize(size=(224, 224)),\n            #torchvision.transforms.CenterCrop(224),\n            torchvision.transforms.RandomHorizontalFlip(),\n            torchvision.transforms.ToTensor(),\n            torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),\n                                             std=(0.229, 0.224, 0.225))\n    ]),\n    'val':torchvision.transforms.Compose([\n            torchvision.transforms.Resize(size=(224,224)),\n            #torchvision.transforms.CenterCrop(224),\n            torchvision.transforms.ToTensor(),\n            torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),\n                                             std=(0.229, 0.224, 0.225))\n        ])\n}","4159294e":"train_dataset = datasets.ImageFolder(root='cropped_data')\ntest_dataset = datasets.ImageFolder(root='cropped_data_test')","add18eea":"train_dataset.transform = image_transforms['train']\ntest_dataset.transform = image_transforms['val']","504c0d94":"train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)","ca9ed246":"trainiter = iter(train_loader)\nfeatures, labels = next(trainiter)\nprint(features.shape, labels.shape)","329e085b":"features = 2048\nfmap_size = 7\n\nclass BCNN(nn.Module):\n    \n    def __init__(self, fine_tune=False):\n        \n        super(BCNN, self).__init__()\n        \n        resnet = models.resnet50(pretrained=True)\n        \n        # freezing parameters\n        if not fine_tune:\n            \n            for param in resnet.parameters():\n                param.requires_grad = False\n        else:\n            \n            for param in resnet.parameters():\n                param.requires_grad = True\n\n        layers = list(resnet.children())[:-2]\n        self.features = nn.Sequential(*layers).cuda()        \n\n        self.fc = nn.Linear(features ** 2, 120)\n        self.dropout = nn.Dropout(0.5)\n        \n        # Initialize the fc layers.\n        nn.init.xavier_normal_(self.fc.weight.data)\n        \n        if self.fc.bias is not None:\n            torch.nn.init.constant_(self.fc.bias.data, val=0)\n        \n    def forward(self, x):\n        \n        ## X: bs, 3, 256, 256\n        ## N = bs\n        N = x.size()[0]\n        \n        ## x : bs, 1024, 14, 14\n        x = self.features(x)\n        \n        # bs, (1024 * 196) matmul (196 * 1024)\n        x = x.view(N, features, fmap_size ** 2)\n        x = self.dropout(x)\n        \n        # Batch matrix multiplication\n        x = torch.bmm(x, torch.transpose(x, 1, 2))\/ (fmap_size ** 2) \n        x = x.view(N, features ** 2)\n        x = torch.sqrt(x + 1e-5)\n        x = F.normalize(x)\n        \n        x = self.dropout(x)\n        x = self.fc(x)\n        \n        return x","b0d6960c":"model = BCNN().cuda()\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","7d239687":"wandb.watch(model)","57de32f3":"model.class_to_idx = train_dataset.class_to_idx\n\nmodel.idx_to_class = {\n    idx: class_\n    for class_, idx in model.class_to_idx.items()\n}","34f8dd9d":"def train(model, \n          criterion, \n          optimizer, \n          train_loader,\n          val_loader, \n          save_location, \n          early_stop=3, \n          n_epochs=10, \n          print_every=1):\n\n    #Initializing  variables\n    valid_acc_max = 0\n    stop_count = 0\n    model.epochs = 0\n    \n    #Loop starts here\n    for epoch in range(n_epochs):\n        \n        train_loss = 0\n        valid_loss = 0\n\n        train_acc = 0\n        valid_acc = 0\n\n        model.train()\n        \n        ### batch control\n        ii = 0\n        \n        for data, label in train_loader:\n            \n            ii += 1\n            \n            data, label = data.cuda(), label.cuda()\n            output = model(data)\n            \n            loss = criterion(output, label)\n            optimizer.zero_grad()\n            \n            loss.backward()\n            optimizer.step()\n            \n            # Track train loss by multiplying average loss by number of examples in batch\n            train_loss += loss.item() * data.size(0)\n            \n            \n            # Calculate accuracy by finding max log probability\n            # first output gives the max value in the row(not what we want), second output gives index of the highest val\n            _, pred = torch.max(output, dim=1)\n            \n            # using the index of the predicted outcome above, torch.eq() will check prediction index against label index to see if prediction is correct(returns 1 if correct, 0 if not)\n            correct_tensor = pred.eq(label.data.view_as(pred))\n            \n            #tensor must be float to calc average\n            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n            train_acc += accuracy.item() * data.size(0)\n            \n            if ii%10 == 0:\n                print(f'Epoch: {epoch}\\t{100 * (ii + 1) \/ len(train_loader):.2f}% complete.')\n        \n        model.epochs += 1\n        \n        with torch.no_grad():\n            \n            model.eval()\n            \n            for data, label in val_loader:\n                \n                data, label = data.cuda(), label.cuda()\n                output = model(data)\n                loss = criterion(output, label)\n                valid_loss += loss.item() * data.size(0)\n                \n                _, pred = torch.max(output, dim=1)\n                correct_tensor = pred.eq(label.data.view_as(pred))\n                accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n                valid_acc += accuracy.item() * data.size(0)\n            \n        train_loss = train_loss \/ len(train_loader.dataset)\n        valid_loss = valid_loss \/ len(val_loader.dataset)\n\n        train_acc = train_acc \/ len(train_loader.dataset)\n        valid_acc = valid_acc \/ len(val_loader.dataset)\n\n        wandb.log({\"Train Accuracy\": train_acc, \n                   \"Train Loss\": train_loss, \n                   \"Validation Accuracy\":valid_acc,\n                   \"Validation Loss\":valid_loss\n                  })\n            \n\n        if (epoch + 1) % print_every == 0:\n\n            print(f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}')\n            print(f'\\t\\tTraining Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}%')\n\n            if valid_acc > valid_acc_max:\n                \n                torch.save({\n                    'state_dict': model.state_dict()\n                }, save_location)\n                \n                stop_count = 0\n                valid_acc_max = valid_acc\n                best_epoch = epoch\n                \n            else:\n                \n                stop_count += 1\n                \n                # Below is the case where we handle the early stop case\n                if stop_count >= early_stop:\n                    \n                    print(f'\\nEarly Stopping Total epochs: {epoch}. Best epoch: {best_epoch} with best val acc: {100 * valid_acc_max:.2f}%')\n                    model.load_state_dict(torch.load(save_location)['state_dict'])\n                    model.optimizer = optimizer\n                    return model\n    \n    model.optimizer = optimizer\n\n    return model","4a9cc8d1":"model = train(\n    model,\n    criterion,\n    optimizer,\n    train_loader,\n    test_loader,\n    save_location='dog_bcnn_resnet50.pt',\n    early_stop=3,\n    n_epochs= 10,\n    print_every=1)","9aa171d1":"def test(model, test_loader, criterion):\n    \n    with torch.no_grad():\n        \n        model.eval()\n        test_acc = 0\n        \n        for data, label in test_loader:\n            data, label = data.cuda(), label.cuda()\n\n            output = model(data)\n\n            _, pred = torch.max(output, dim=1)\n            correct_tensor = pred.eq(label.data.view_as(pred))\n            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n            test_acc += accuracy.item() * data.size(0)\n\n        test_acc = test_acc \/ len(test_loader.dataset)\n        return test_acc","91d79577":"model.load_state_dict(torch.load('\/kaggle\/working\/dog_bcnn_resnet50.pt')['state_dict'])\n\ntest_acc = test(model.cuda(), test_loader, criterion)\nprint(f'The model has achieved an accuracy of {100 * test_acc:.2f}% on the test dataset')","2c17297f":"pred_data_dir = '.\/prediction_samples\/'\nimage_list = os.listdir(pred_data_dir)\n\n\nimages = []\npredictions = []\n\nwith torch.no_grad():\n    model.eval()\n    \n    for img in image_list[:16]:\n        \n        \n        img_read = Image.open(pred_data_dir + img)\n        images.append(img_read)\n        plt.imshow(img_read)\n        plt.show()\n\n        img_trans = image_transforms['val'](img_read)\n        \n        img_trans = torch.unsqueeze(img_trans, 0)\n        \n        output = model(img_trans.cuda())\n\n        _, pred = torch.max(output, dim=1)\n        \n        prediction = model.idx_to_class[pred.cpu().detach().numpy()[0]].split(\"-\")[1]\n        \n        print(\"Act:\"+img.split(\"-\")[1]+\"   Pred:\"+prediction)\n        predictions.append(\"Act:\"+img.split(\"-\")[1]+\"   Pred:\"+prediction)\n\nwandb.log({\"model_predictions\": [wandb.Image(fig, caption=predictions[i]) for i, fig in enumerate(images)]})","e042904a":"### Importing Required Libraries","f4b0775f":"Fine-Grained Image Classification (FGIC) is an area of expertise in image recognition where we get to differentiate minor categories such as dog breeds, bird species, airplanes, etc. ","c9add549":"### Upgrade Weights & Biases library","d8f7d9b5":"### Loading test data file names to test_files"}}