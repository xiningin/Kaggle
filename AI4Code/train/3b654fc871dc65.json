{"cell_type":{"f89060a8":"code","79e8fe49":"code","dd910184":"code","1b249cf3":"code","ab0d709b":"code","8f9cc6be":"code","7182074e":"code","4c66900c":"code","2cc50efd":"code","a7d3d39a":"code","68ec53f7":"code","bd65bbec":"code","1fcf8351":"code","683f5aee":"code","04980a6b":"code","6743316b":"code","8658cd7e":"code","8c30e876":"code","de185fa2":"code","2a2c3d55":"code","0374f9ec":"code","771146ef":"code","01ee2b94":"code","8b5f1dc0":"code","d22bc962":"code","01554325":"code","5e1bad26":"code","6b7a8561":"code","1f5e62e2":"code","8bbdba4a":"code","2e1b7ca0":"code","0af0a400":"code","626f8399":"code","33e5a1ff":"code","b6258fa8":"code","379f60f2":"code","528586a0":"code","3a436510":"markdown","e835d7f4":"markdown","6a47f583":"markdown","cbea865d":"markdown","e836e4c8":"markdown","ef7ad1f4":"markdown","27840f7b":"markdown","e36553f0":"markdown","82d64608":"markdown","68acd0e5":"markdown","8244ce6c":"markdown","0c0a5c20":"markdown","ad43a176":"markdown","81fee2c3":"markdown","388fa40b":"markdown","93aded29":"markdown","f029b3f8":"markdown","95dc6b83":"markdown","3bea6b09":"markdown","f57b3f79":"markdown","89cba066":"markdown","7dde82c4":"markdown","8994c0b3":"markdown","d7840f73":"markdown","f4a99e6c":"markdown","1c2ddced":"markdown","0545f6f1":"markdown","7acb05ed":"markdown","2561fa61":"markdown","3623ff3a":"markdown","3147411c":"markdown","a5e1fad2":"markdown","3807ead9":"markdown","ed342286":"markdown","0e7a4bb7":"markdown","4c36bdd3":"markdown","f42d4384":"markdown","5ed39bed":"markdown","b0ef7d70":"markdown","1e1bed26":"markdown","80e8064c":"markdown","6ba22820":"markdown","a0b1ab43":"markdown","73b21f4f":"markdown","c97c5b2a":"markdown","958a97d8":"markdown","99a1243a":"markdown","50313206":"markdown","109b5f4d":"markdown"},"source":{"f89060a8":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set_style('darkgrid')\n\nimport nltk\nfrom sklearn.preprocessing import LabelBinarizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import STOPWORDS,WordCloud\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\n\n\nfrom keras.preprocessing import text,sequence\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import LSTM,Dense,Dropout,Embedding\nfrom keras.callbacks import ReduceLROnPlateau\nimport tensorflow as tf","79e8fe49":"real_news=pd.read_csv('..\/input\/fake-and-real-news-dataset\/True.csv')\nfake_news=pd.read_csv('..\/input\/fake-and-real-news-dataset\/Fake.csv')","dd910184":"real_news.head()","1b249cf3":"fake_news.head()","ab0d709b":"real_news['Isfake']=0\nfake_news['Isfake']=1","8f9cc6be":"df=pd.concat([real_news,fake_news])","7182074e":"df.sample(5)","4c66900c":"df.isnull().sum()","2cc50efd":"sns.countplot(df.Isfake)","a7d3d39a":"df.title.count()","68ec53f7":"df.subject.value_counts()","bd65bbec":"plt.figure(figsize=(10,10))\nchart=sns.countplot(x='subject',hue='Isfake',data=df,palette='muted')\nchart.set_xticklabels(chart.get_xticklabels(),rotation=90,fontsize=10)","1fcf8351":"df['text']= df['text']+ \" \" + df['title']\ndel df['title']\ndel df['subject']\ndel df['date']","683f5aee":"stop_words=set(stopwords.words('english'))\npunctuation=list(string.punctuation)\nstop_words.update(punctuation)","04980a6b":"def string_html(text):\n    soup=BeautifulSoup(text,\"html.parser\")\n    return soup.get_text()\n\ndef remove_square_brackets(text):\n    return re.sub('\\[[^]]*\\]','',text)\n\ndef remove_URL(text):\n    return re.sub(r'http\\S+','',text)\n\ndef remove_stopwords(text):\n    final_text=[]\n    for i in text.split():\n        if i.strip().lower() not in stop_words:\n            final_text.append(i.strip())\n    return \" \".join(final_text)\n\ndef clean_text_data(text):\n    text=string_html(text)\n    text=remove_square_brackets(text)\n    text=remove_stopwords(text)\n    text=remove_URL(text)\n    return text","6743316b":"df['text']=df['text'].apply(clean_text_data)","8658cd7e":"plt.figure(figsize=(20,20))\nwordcloud=WordCloud(stopwords=STOPWORDS,height=600,width=1200).generate(\" \".join(df[df.Isfake==1].text))\nplt.imshow(wordcloud,interpolation='bilinear')","8c30e876":"plt.figure(figsize=(20,20))\nwordcloud=WordCloud(stopwords=STOPWORDS,height=600,width=1200).generate(\" \".join(df[df.Isfake==0].text))\nplt.imshow(wordcloud,interpolation='bilinear')","de185fa2":"X_train,X_test,y_train,y_test=train_test_split(df.text,df.Isfake,random_state=0)","2a2c3d55":"max_features=10000\nmax_len=300","0374f9ec":"tokenizer=text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)\ntokenizer_train=tokenizer.texts_to_sequences(X_train)\nX_train=sequence.pad_sequences(tokenizer_train,maxlen=max_len)","771146ef":"tokenizer_test=tokenizer.texts_to_sequences(X_test)\nX_test=sequence.pad_sequences(tokenizer_test,maxlen=max_len)","01ee2b94":"glove_file='..\/input\/glove-twitter\/glove.twitter.27B.100d.txt'","8b5f1dc0":"def get_coefs(word, *arr):\n    return word, np.asarray(arr,dtype='float32')\nembeddings_index=dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(glove_file,encoding=\"utf8\"))","d22bc962":"all_embs=np.stack(embeddings_index.values())\nemb_mean,emb_std=all_embs.mean(),all_embs.std()\nemb_size=all_embs.shape[1]\n\nword_index=tokenizer.word_index\nnb_words=min(max_features,len(word_index))\n\nembedding_matrix = np.random.normal(emb_mean,emb_std,(nb_words,emb_size))\n\nfor word,i in word_index.items():\n    if i>=max_features: continue\n    embedding_vector=embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i]=embedding_vector","01554325":"batch_size=256\nepochs=10\nemb_size=100","5e1bad26":"leaning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy',patience=2,verbose=10,factor=0.5,min_lr=0.00001)","6b7a8561":"model=Sequential()\nmodel.add(Embedding(max_features,output_dim=emb_size,weights=[embedding_matrix],input_length=max_len,trainable=False))\nmodel.add(LSTM(units=256,return_sequences=True,recurrent_dropout=0.25,dropout=0.25))\nmodel.add(LSTM(units=128,return_sequences=True,recurrent_dropout=0.25,dropout=0.25))\nmodel.add(LSTM(units=64,recurrent_dropout=0.1,dropout=0.1))\nmodel.add(Dense(units=32,activation='relu'))\nmodel.add(Dense(1,'sigmoid'))\nmodel.compile(optimizer=keras.optimizers.Adam(lr=0.01),loss='binary_crossentropy',metrics=['accuracy'])","1f5e62e2":"model.summary()","8bbdba4a":"history=model.fit(X_train,y_train,batch_size=batch_size,validation_data=(X_test,y_test),epochs=epochs,callbacks=[leaning_rate_reduction])","2e1b7ca0":"pred = model.predict_classes(X_test)\npred[5:10]","0af0a400":"epochs = [i for i in range(10)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs,train_acc,'go-',label='Training Accuracy')\nax[0].plot(epochs,val_acc,'ro-',label='Validation Accuracy')\nax[0].set_xlabel('Epochs')\nax[0].set_ylabel('Accuracy')\nax[0].legend()\n\nax[1].plot(epochs,train_loss,'go-',label='Training Loss')\nax[1].plot(epochs,val_loss,'ro-',label='Validation Loss')\nax[1].set_xlabel('Loss')\nax[1].set_ylabel('Accuracy')\nax[1].legend()\n\nplt.show()","626f8399":"cm=confusion_matrix(y_test,pred)","33e5a1ff":"cm=pd.DataFrame(cm,index=['Fake','Not Fake'],columns=['Fake','Not Fake'])","b6258fa8":"cm","379f60f2":"plt.figure(figsize=(10,10))\nsns.heatmap(cm,cmap=\"Blues\",linecolor='black',linewidth=1,annot=True,fmt='',xticklabels=['Fake','Not Fake'],yticklabels=['Fake','Not Fake'])\nplt.xlabel('Actual')\nplt.ylabel('Predicted')","528586a0":"print(f'Accuracy of the model on Training Data is - { model.evaluate(X_train,y_train)[1]*100:.2f}')\nprint(f'Accuracy of the model on Testing Data is -  {model.evaluate(X_test,y_test)[1]*100:.2f}')","3a436510":"We will now combine both of these data and add a column of 'Isfake' so that we can use all the data as once and the 'Isfake' column will also be our target column , which will determine if the news is fake or not.","e835d7f4":"Let's see our model in action ! ;)","6a47f583":"Let's train our model now !","cbea865d":"We will be removing punctuation , stopwords,URLS, html tags from our text data. <br>\nFor this we shall use beautifulsoup and re library which we imported earlier.","e836e4c8":"Now that we have defined the cleaning functions , let us use em' on our text data.","ef7ad1f4":"Let's initialize our callback.","27840f7b":"Now what about the real news?","e36553f0":"We have succesfully done the tokenization part , let's build our model now!<br>\nHere are the parameters I'm taking.","82d64608":"Now what is our accuracy on Test and Train set?\n","68acd0e5":"Those were some nice wordclouds , and clearly Donald Trump , United States , etc were very frequent.","8244ce6c":"How many of the given news are fake and how many of them are real?","0c0a5c20":"I wonder what words were the most used in fake news and real news and i guess you do too!<br>\nSo let's see what these frequent words are , and for that we will use wordcloud.","ad43a176":"How many subjects are there ? We can see that using value_counts()","81fee2c3":"# <a id=\"top_section\"><\/a>\n\n<div align='center'><font size=\"5\" color=\"#000000\"><b>Fake News Detector<br>(~99.9% accuracy)<\/b><\/font><\/div>\n<hr>\n<div align='center'><font size=\"5\" color=\"#000000\">A General Introduction<\/font><\/div>\n<hr>\n\n\n\n## Table of Contents\n\n* [Getting the Text Data Ready](#getting_the_data_ready)\n* [Visualizing the Data](#visualize_data)\n* [Cleaning the data](#clean_data)\n* [Frequent Words](#wordcloud)\n* [Tokenization](#tokenize)\n* [Building our Model](#model)\n* [Analyzing our Model](#analyze_model)\n* [Some Last Words](#sectionlst)\n    \n## Summary\n**In this kernel , I try to analyse and then build a model to predict whether the news given to us is fake or not<br>\nI have used Glove embeddings and LSTM layers to get an accuracy of 99.9% on Train data and 99.8% on test data.If you want to check out some of my other projects , here is my GITHUB link :P**<br><br>\n<a class=\"nav-link active\"  style=\"background-color:; color:Blue\"  href=\"https:\/\/github.com\/friskycodeur\" role=\"tab\">Prateek Maheshwari on GITHUB<\/a>\n\n<br>\n<a href=\"https:\/\/ibb.co\/nm4kTk1\"><img src=\"https:\/\/bsmedia.business-standard.com\/media-handler.php?mediaPath=https:\/\/bsmedia.business-standard.com\/_media\/bs\/img\/article\/2020-03\/16\/full\/1584358219-7432.jpg&width=1200\" alt=\"Fake news picture\" border=\"0\" height=300 width=300><\/a>\n\n\n### Here are the things I will try to cover in this Notebook:\n\n- Basic EDA of the text data.\n- Data cleaning\n- Making some awesome Word clouds\n- Using Glove embedding and tokenizer\n- Building our model \n\n### I highly appreciate your feedback, there might be some areas can be fixed or improved.\n\n## If you liked my work please dont forget to Upvote!","388fa40b":"Are there any null values?","93aded29":"The number of fake and real news are almost equal. <br>\nNow let us see how many unqiue titles are there. Are any of the titles repeated?","f029b3f8":"As there are no null values , we are saved from the hassle of making up for the missing values. Now we will visualize the data.","95dc6b83":"So how does our data look now ?","3bea6b09":"Having imported all the necessary libraries , now we will go ahead and load our data.","f57b3f79":"<a href=\"#top_section\" class=\"btn btn-primary\" style=\"color:white;\" >Back to Table of Content<\/a>","89cba066":"Let's see how much of the news in different subject are fake !","7dde82c4":"**First off , we will import all the necessary libraries we need. <br> Apart from the basic libraries , I am using nltk,beautifulsoup,re,string to help with our text data. <br> For our model building i will be using embedding , lstm , dropout and dense layers.I will also be using ReduceLRonPlateau as callback. <br><br>\nSo let's import all of these libraries.**","8994c0b3":"<a id='tokenize'><\/a>\n## Tokenization","d7840f73":"<a id='visualize_data'><\/a>\n## Visualizing the data ","f4a99e6c":"Now we will get the coefficients from the glove file and save it in embedding index variable.","1c2ddced":"# Some last words:\n\nThank you for reading! I'm still a beginner and want to improve myself in every way I can. So if you have any ideas to feedback please let me know in the comments section!\n\n\n<div align='center'><font size=\"3\" color=\"#000000\"><b>And again please upvote if you liked this notebook so it can reach more people, Thanks!<\/b><\/font><\/div>\n\n<img src='https:\/\/media.tenor.com\/images\/69d1d66198f1aac60ad244f6c004f372\/tenor.gif'>","0545f6f1":"We shall now tokenize our data ,i.e convert the text data into vectors.","7acb05ed":"Now we will import our GLOVE file , I am using the 100d version here.","2561fa61":"<img src='https:\/\/i.pinimg.com\/originals\/f5\/fb\/5e\/f5fb5efe6b9f1d5f11f19e69f67f5ccf.gif'>","3623ff3a":"Let's see how the accuracy and loss graphs of our model look now !","3147411c":"<a id='wordcloud'><\/a>\n## Frequent Words","a5e1fad2":"<a id='model'><\/a>\n## Building our model","3807ead9":"Let's see the fake news texts first !","ed342286":"<a id=01><\/a>\n### Importing necessary libraries","0e7a4bb7":"Let's build our model.Here are the layers I'm using:\n- Starting with an embedding layer\n- Then 3 LSTM layers\n- Then 2 Dense layers\n\nI am using Adam optimizer for our model.","4c36bdd3":"Now we will place all of the required columns in one and delete all the not-so-required columns.","f42d4384":"<a id='analyze_model'><\/a>\n## Analyzing our model ","5ed39bed":"To tokinize our data , I am using tokenizer here. There are other ways to tokenize data , you can also try them out.","b0ef7d70":"Using conactenate function of pandas :","1e1bed26":"What's happening in the next code tab:\n- We first take all the values of the embeddings and store it in all_embs.\n- Then we take the mean and standard deviation of all the embeddings.\n- We now take the word indices using .word_index function of tokenizer.\n- Then we will see what the length of each vector would be and save it in nb_words.\n- We make an embedding matrix.","80e8064c":"<a id='clean_data'><\/a>\n## Cleaning the data","6ba22820":"We are done with this now , we shall head towards cleaning our data!","a0b1ab43":"<a id=\"getting_the_data_ready\"><\/a>\n## Getting the Data Ready","73b21f4f":"Let's take a sneak peak at our data !","c97c5b2a":"<a id='load_data'><\/a>\n### Loading the data ","958a97d8":"We will now see how many of the samples were wrongly predicted using the confusion matrix. ","99a1243a":"We are all done with cleaning and have with us cleaned text data now.Next up are some awesome wordclouds.","50313206":"Our data may consist URLs , HTML tags which might make it difficult for our model to predict properly. To prevent that from happening we will clean our data so as to make our model more efficient.","109b5f4d":"Here is what is happening in the next code tab:\n- First we initialized the tokenizer with it's size being 10k.\n- Then we fit the training data on this tokenizer.\n- Then we convert the text to sequences and save it in X_train variable.\n- Lastly we add a padding layer around our sequence.\n\nHere is a example of what tokenizer does \n<img src='https:\/\/miro.medium.com\/max\/2414\/1*UhfwmhMN9sdfcWIbO5_tGg.jpeg'>"}}