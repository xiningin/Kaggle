{"cell_type":{"7b25aba2":"code","b8b7a462":"code","5aff0e7a":"code","efe2327b":"code","3e1a023b":"code","2fdee0d7":"code","4a7974df":"code","31e51d0f":"markdown","51953456":"markdown"},"source":{"7b25aba2":"from plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nimport plotly.plotly as py\nfrom plotly import tools\nfrom datetime import date\nimport pandas as pd\nimport numpy as np \nimport plotly.figure_factory as ff\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random \nimport warnings\nimport operator\nwarnings.filterwarnings(\"ignore\")\ninit_notebook_mode(connected=True)","b8b7a462":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\n\n## list of features to be used\nfeatures = [c for c in train.columns if c not in ['Id', 'Target']]\n\n## target variable \ntarget = train['Target'].values\ntarget_index = {1:0, 2:1, 3:2, 4:3}\ntarget = np.array([target_index[c] for c in target])","5aff0e7a":"def label_encoding(col):\n    le = LabelEncoder()\n    le.fit(list(train[col].values) + list(test[col].values))\n    train[col] = le.transform(train[col].astype(str))\n    test[col] = le.transform(test[col].astype(str))\n\nnum_cols = train._get_numeric_data().columns \ncat_cols = list(set(features) - set(num_cols))\nfor col in cat_cols:\n    label_encoding(col)","efe2327b":"totaal=train.append(test)","3e1a023b":"from sklearn.decomposition import TruncatedSVD\ntotaal=(train.append(test)).fillna(0)\ntotaal=totaal.drop(['Id','Target'],axis=1)  #.reset_index()\ntemp=totaal.iloc[:,1:135].divide(totaal['rooms'],axis=0)\ntotaal=(totaal.T.append(temp.T)).T\ntotaal=totaal.join(temp, lsuffix='', rsuffix='persons')\nsvd = TruncatedSVD(n_components=140, n_iter=7, random_state=42)\ne_=svd.fit_transform(totaal)\n#A_,e1_,e_,s_=robustSVD(e_,100)\nNew_features =  e_[:len(train)]\nTest_features= e_[-len(test):]\npd.DataFrame(New_features).plot.scatter(x=0,y=1,c=train['Target']+1)\npd.DataFrame(np.concatenate((Test_features,New_features))).plot.scatter(x=0,y=1,c=[1 for x in range(len(test))]+[2 for x in range(len(train))],colormap='viridis')    \n","2fdee0d7":"def cohen_effect_size(X, y):\n    \"\"\"Calculates the Cohen effect size of each feature.\n    \n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vector, where n_samples in the number of samples and\n            n_features is the number of features.\n        y : array-like, shape = [n_samples]\n            Target vector relative to X\n        Returns\n        -------\n        cohen_effect_size : array, shape = [n_features,]\n            The set of Cohen effect values.\n        Notes\n        -----\n        Based on https:\/\/github.com\/AllenDowney\/CompStats\/blob\/master\/effect_size.ipynb\n    \"\"\"\n    print(X.shape,y.shape,y.mean())\n    medi=y.mean()\n    group1, group2 = X[y<medi], X[y>=medi]\n    diff = group1.mean() - group2.mean()\n    var1, var2 = group1.var(), group2.var()\n    n1, n2 = group1.shape[0], group2.shape[0]\n    pooled_var = (n1 * var1 + n2 * var2) \/ (n1 + n2)\n    d = diff \/ np.sqrt(pooled_var)\n    return d","4a7974df":"excluded_feats = ['ID','Id','Target'] #['SK_ID_CURR']\n\nfeatures = [f_ for f_ in train.drop('Target',axis=1).columns if f_ not in excluded_feats]\nprint('Number of features %d' % len(features),train.shape,target.shape)\n#effect_sizes = cohen_effect_size(Xtrain[:len(ytrain)], ytrain)\neffect_sizes = cohen_effect_size(train.drop('Target',axis=1)[:len(target)],pd.DataFrame(train).reset_index().set_index('index')['Target'])\neffect_sizes.reindex(effect_sizes.abs().sort_values(ascending=False).nlargest(50).index)[::-1].plot.barh(figsize=(6, 10));\nprint('Features with the 30 largest effect sizes')\nsignificant_features2 = [f for f in features if np.abs(effect_sizes.loc[f]) > 0.1]\nprint('Significant features %d: %s' % (len(significant_features2), significant_features2))","31e51d0f":"# anyone suggestion what explains that difference","51953456":"# SVD reveals a difference between test and train\n## with such a lack of overlap its impossible to forecast test accurately"}}