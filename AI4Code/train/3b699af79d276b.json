{"cell_type":{"b4d74dc4":"code","ea50d1ca":"code","362bc649":"code","b4096429":"code","d508ebda":"code","578aeed5":"code","17e2ba24":"code","80609015":"code","d126bf3f":"code","a482750e":"code","e43843fd":"markdown","666b7683":"markdown","019f1cd8":"markdown","8936a0cd":"markdown","7fc0eb7b":"markdown","265944e5":"markdown","5ccee5d6":"markdown","6731be0e":"markdown"},"source":{"b4d74dc4":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport warnings\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nwarnings.filterwarnings(\"ignore\")\nplt.rcParams['figure.figsize'] = [20, 20]\n\ndef plot_images(images, labels, shape=(3,3)):\n    fig, p = plt.subplots(shape[0], shape[1])\n    i = 0\n    for x in p:\n        for ax in x:\n            ax.imshow(images[i])\n            ax.set_title(labels[i])\n            i += 1","ea50d1ca":"def unpickle(file):\n    import pickle\n    with open(file, 'rb') as fo:\n        dict = pickle.load(fo, encoding='bytes')\n    return dict","362bc649":"# Read qmnist data\nqmnist = unpickle(\"\/kaggle\/input\/qmnist-the-extended-mnist-dataset-120k-images\/MNIST-120k\")\n \n# Load test data\ntest = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")\n\n# we reshape and normalize the data\nX_qmnist = np.array(qmnist['data'], dtype=\"float32\") \/ 255\nX_qmnist = X_qmnist.reshape(-1, 28, 28, 1)\n\n# Convert labels to one hot vectors\ny_qmnist = tf.keras.utils.to_categorical(qmnist['labels'])\n\nX_test = np.array(test, dtype=\"float32\") \/ 255\nX_test = X_test.reshape(-1, 28, 28, 1)\n\nplot_images(X_qmnist[:9], y_qmnist[:9], shape=(3,3))","b4096429":"from tensorflow.keras.datasets import mnist\n\n# Load MNIST data\n(X_train_mnist, y_train_mnist), (X_test_mnist, y_test_mnist) = mnist.load_data()\n\nX_mnist = np.concatenate((X_train_mnist, X_test_mnist))\ny_mnist = np.concatenate((y_train_mnist, y_test_mnist))\n\n# Preprocess MNIST to match our preprocessing\nX_mnist = X_mnist.reshape(-1,28,28,1)\nX_mnist = X_mnist.astype(np.float32) \/ 255\ny_mnist = tf.keras.utils.to_categorical(y_mnist,num_classes=10)\n\n# Combine MNIST and QMNIST\nX_train = np.concatenate((X_qmnist, X_mnist))\ny_train = np.concatenate((y_qmnist, y_mnist))\n\n# final dataset shape\nprint(\"MNIST image dataset shape:\", X_qmnist.shape)\nprint(\"QMNIST image dataset shape:\", X_mnist.shape)\nprint(\"Final image dataset shape:\", X_train.shape)\n\nplot_images(X_train[:9], y_train[:9], shape=(3,3))","d508ebda":"datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    rotation_range=20,\n    width_shift_range=0.20,\n    shear_range=15,\n    zoom_range=0.10,\n    validation_split=0.25,\n    horizontal_flip=False\n)\n\ntrain_generator = datagen.flow(\n    X_train,\n    y_train, \n    batch_size=256,\n    subset='training',\n)\n\nvalidation_generator = datagen.flow(\n    X_train,\n    y_train, \n    batch_size=64,\n    subset='validation',\n)","578aeed5":"def create_model():\n    model = tf.keras.Sequential([\n        \n        tf.keras.layers.Reshape((28, 28, 1)),\n        tf.keras.layers.Conv2D(filters=32, kernel_size=(5,5), activation=\"relu\", padding=\"same\", input_shape=(28,28,1)),\n        tf.keras.layers.MaxPool2D((2,2)),\n        \n        tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation=\"relu\", padding=\"same\"),\n        tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation=\"relu\", padding=\"same\"),\n        tf.keras.layers.MaxPool2D((2,2)),\n        \n        tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation=\"relu\", padding=\"same\"),\n        tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation=\"relu\", padding=\"same\"),\n        tf.keras.layers.MaxPool2D((2,2)),\n\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(512, activation=\"sigmoid\"),\n        tf.keras.layers.Dropout(0.25),\n        \n        tf.keras.layers.Dense(512, activation=\"sigmoid\"),\n        tf.keras.layers.Dropout(0.25),\n        \n        tf.keras.layers.Dense(256, activation=\"sigmoid\"),\n        tf.keras.layers.Dropout(0.1),\n        \n        tf.keras.layers.Dense(10, activation=\"sigmoid\")\n    ])\n\n    model.compile(\n        optimizer=\"adam\",\n        loss = 'categorical_crossentropy',\n        metrics = ['accuracy']\n    )\n\n    return model\n\nmodel = create_model()","17e2ba24":"reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n                                                 factor=0.1,\n                                                 patience=5,\n                                                 min_lr=0.000001,\n                                                 verbose=1)\n\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(filepath='model.hdf5',\n                                                monitor='val_loss',\n                                                save_best_only=True,\n                                                save_weights_only=True,\n                                                verbose=1)","80609015":"history = model.fit_generator(train_generator, \n                              epochs=100, \n                              validation_data=validation_generator, \n                              callbacks=[reduce_lr,checkpoint], \n                              verbose=1)","d126bf3f":"model.load_weights('model.hdf5')\n\nfig, ax = plt.subplots(1,2, figsize=(15, 5))\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)\n\nfinal_loss, final_acc = model.evaluate(X_train,  y_train, verbose=2)\nprint(\"Model accuracy: \", final_acc, \", model loss: \", final_loss)","a482750e":"df = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\").astype(\"float32\") \/ 255.0\npredictions = tf.keras.backend.argmax(model.predict(df))\nsubmission = pd.DataFrame({'ImageId': range(1, len(predictions) + 1), \"Label\": predictions})\nsubmission.to_csv('submission.csv', index=False)","e43843fd":"### Data Augmentation\n\nTo provide more data during the training process, we are also going to use Data Augmentation.","666b7683":"# Submbit predictions","019f1cd8":"We will use the QMNIST extended data to boost the performance to the max, see https:\/\/www.kaggle.com\/fedesoriano\/qmnist-the-extended-mnist-dataset-120k-images","8936a0cd":"# CNN","7fc0eb7b":"## Read the data","265944e5":"# QMNIST: Using a Simple CNN to reach the Top 1% \n\nThe goal of this notebook is to classify with the best accuracy possible handwritten digits. The input is a (28,28) \"image\" in grey scale. This notebook is presents multiple technics to achieve 99.9 accuracy:\n\n- CNN\n- Denser Dataset (we use MNIST images and the extended QMNIST Dataset)\n- Data Augmentation","5ccee5d6":"In order to get the biggest amount of data possible we will train our model with both the MNIST and the QMNIST data.","6731be0e":"# Train model"}}