{"cell_type":{"eb3df25e":"code","ba1f9fb0":"code","e87815ef":"code","a208076e":"code","7155e47e":"code","fc6e13cc":"code","a454e293":"code","0ca30944":"code","3518efbe":"code","d0d6cc13":"code","96a3d703":"code","b7cc5f01":"code","d445cc1a":"code","2555f006":"code","e2344d63":"code","8288e40f":"code","1d053373":"code","6f96f400":"code","a50d52bd":"code","e298e713":"code","c628ede3":"code","fbd0d073":"code","8eda68af":"code","7a04e24b":"code","7f8a4ba2":"code","72a9ed29":"markdown","78af561d":"markdown","3cb4620f":"markdown","1d3e288d":"markdown","70e2a9e5":"markdown","af0a379f":"markdown","6d3680ca":"markdown","7a48df3c":"markdown","072b695d":"markdown","a3b2600a":"markdown","4b3c7aa8":"markdown","e02c323d":"markdown","5c0230ae":"markdown","55a24ef3":"markdown"},"source":{"eb3df25e":"import numpy as np # linear algebra\nimport os\nfrom nltk import *\nfrom nltk.tokenize import word_tokenize,wordpunct_tokenize\nfrom string import punctuation\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding\nfrom keras.models import load_model\nfrom nltk.corpus import stopwords\nimport h5py\nfilepath = '..\/input\/'\nfilename = os.listdir(filepath)\nprint(\"Using these files : \",filename)\n# Any results you write to the current directory are saved as output.","ba1f9fb0":"blog = ''\nnews = ''\nblog += open(filepath+filename[1],'r',encoding='utf8').read()\nnews += open(filepath+filename[2],'r',encoding='utf8').read()","e87815ef":"blog_tokens = wordpunct_tokenize(blog)\nnews_tokens = wordpunct_tokenize(news)","a208076e":"blog_tokens[:2],news_tokens[:2]","7155e47e":"# List Tokenize word\ntokenize_word_blog = []\ntokenize_word_news = []\nfor word in blog_tokens:\n    if len(word) >= 2 and word.isalpha() and word.lower()!='the':\n        word = word.replace('?','')\n        word = word.replace('.','')\n        word = word.replace('!','')\n        word = word.replace(';','')\n        word = word.replace(':','')\n        tokenize_word_blog.append(word.lower())\n\nfor word in news_tokens:\n    if len(word) >= 2 and word.isalpha() and word.lower()!='the':\n        word = word.replace('?','')\n        word = word.replace('.','')\n        word = word.replace('!','')\n        word = word.replace(';','')\n        word = word.replace(':','')\n        tokenize_word_blog.append(word.lower())","fc6e13cc":"final_tokenize_word = []\nfinal_tokenize_word += tokenize_word_blog[:40000]\nfinal_tokenize_word += tokenize_word_news[:40000]","a454e293":"final_tokenize_words = []\nfor i in final_tokenize_word:\n    if i=='ve':\n        final_tokenize_words.append('have')\n    elif i=='re':\n        final_tokenize_words.append('are')\n    elif i=='ll':\n        final_tokenize_words.append('will')\n    else:\n        final_tokenize_words.append(i)","0ca30944":"# pickle.dump(final_tokenize_words,open('tokenized_words.pkl','wb'))","3518efbe":"tokenizer = Tokenizer() # creating object of Tokenizer()\ntokenizer.fit_on_texts([final_tokenize_words])\nencoded = tokenizer.texts_to_sequences([final_tokenize_words])[0]","d0d6cc13":"encoded[:5]","96a3d703":"vocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size: %d' % vocab_size)","b7cc5f01":"sequences = list()\nfor i in range(1, len(encoded)):\n    sequences.append(encoded[i-1:i+1])\nprint('Total Sequences: %d' % len(sequences))","d445cc1a":"sequences[:5]","2555f006":"sequences = np.array(sequences) # Converting list to numpy array\nX, Y = sequences[:,0],sequences[:,1]","e2344d63":"Y = to_categorical(Y,num_classes=vocab_size)","8288e40f":"model = Sequential()\nmodel.add(Embedding(vocab_size, 400, input_length=1))\nmodel.add(LSTM(400,return_sequences=True))\nmodel.add(LSTM(400))\nmodel.add(Dense(vocab_size, activation='softmax'))\nprint(model.summary())","1d053373":"model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['acc'])","6f96f400":"model.fit(X, Y, epochs=200,batch_size=48, verbose=2)","a50d52bd":"# serialize model to HDF5\nmodel.save(\"new_model.h5\")\nprint(\"Saved model to disk\")","e298e713":"def generate_seq(word):\n    in_text, result = word, word\n    # generate a fixed number of words\n    for _ in range(3):\n        # encode the text as integer\n        encode = tokenizer.texts_to_sequences([in_text])[0]\n        encode = np.array(encode)\n        # predict a word in the vocabulary\n        yhat = model.predict_classes(encode, verbose=0)\n        # map predicted word index to word\n        out_word = ''\n        for word, index in tokenizer.word_index.items():\n            if index == yhat:\n                out_word = word\n                break\n        # append to input\n        in_text, result = out_word, result + ' ' + out_word\n    print(result)","c628ede3":"generate_seq('how')","fbd0d073":"generate_seq('so')","8eda68af":"generate_seq('you')","7a04e24b":"generate_seq('what')","7f8a4ba2":"generate_seq('when')","72a9ed29":"# **Split sequence to Input as X & Output as Y**","78af561d":"# **Fitting model on X and Y Data**","3cb4620f":"# **Creating word sequence**\n# **Creating Bi-grams of words**","1d3e288d":"# **Remove single character word and Removing punctuation**","70e2a9e5":"# **Reading Datasets**","af0a379f":"# **Generate a sequence from the model**","6d3680ca":"# **Importing Library**","7a48df3c":"# **Text 2 Tokens**","072b695d":"# **Taking 50k words from news datasets & 50k words from blog datasets**","a3b2600a":"# **Defining Sequential model**\n## Adding Embedding Layer for LookUp Table \n## Adding LSTM Layer of 200 nodes\n## Adding Dense Layer ( Using 'softmax' Activation func )","4b3c7aa8":"# **Compile network**","e02c323d":"# **Determine the vocabulary size**\n###  Determine number of Unique words","5c0230ae":"# **Converting Y to Categorical for Calculating loss = 'categorical_crossentropy'**","55a24ef3":"\n\n# **Converting Text to sequences Using Tokenizer from Keras**\n### Tokenizer will convert tokenized words to integer sequence\n### Tokenizer will also store value of each word into Dictionary like {'where' : 1254, 'what' : 653}"}}