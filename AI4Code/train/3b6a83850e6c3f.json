{"cell_type":{"f9f1ad35":"code","12c88b19":"code","bf204dea":"code","abed0567":"code","b0b46c69":"code","46705008":"code","574e3ef9":"code","e04ea885":"code","5f4d4776":"code","48a82282":"code","0f0047f2":"code","09639756":"code","b92011c9":"code","df30a400":"code","131fc374":"code","7a8d358c":"code","9c3cc579":"code","d62defda":"code","a13c6246":"code","d40214b9":"code","137f0192":"code","83de3560":"code","6201faf6":"code","21927bc5":"code","2156ff68":"code","52c48a84":"code","5d4a6e45":"code","1fb846af":"code","e5e6e6b3":"code","5647127f":"code","5d3ee03a":"code","f1de2d4c":"code","74b1790d":"code","65801ba8":"code","e2704210":"code","cc281431":"code","ebc6e0dd":"code","74f78087":"code","c611e645":"code","dd86f5e2":"code","06395536":"code","050e232a":"code","c496402b":"code","16a11b51":"code","08e22b34":"code","18ed66d0":"code","111c63e9":"code","8760b633":"code","7302a998":"code","13bf3192":"code","c0b6720b":"code","e0b7349e":"code","465f3ec8":"code","a81872c5":"code","e363ea06":"code","b88603cd":"code","2ba9bb34":"code","18278396":"code","8b9a7023":"code","f2f2be66":"code","6aac6682":"code","7ac9d367":"code","8e8c5a78":"code","67f4b242":"code","2df3e8e0":"code","a5b48ffc":"code","d57c139f":"code","5f81fe9b":"code","3509e9f6":"code","78db92b9":"code","df82159f":"code","6f4c9aee":"code","89822d49":"code","64ad8463":"code","29395c25":"code","e06b1f92":"code","d96387f1":"code","ab855329":"code","a65851fb":"code","34a1b042":"code","0bd26dfe":"code","4edc1931":"code","d1892bdb":"code","74c7bb25":"code","df7a4c2a":"code","79b2c31f":"code","dea48b9c":"code","8a61a5cd":"code","05a8609b":"code","4e858abd":"code","790ccbcd":"code","3e9bcf71":"code","08724fd2":"code","0ff00504":"code","ac399a5f":"code","a0ea8d1b":"code","a4c688d8":"code","aac98b64":"code","e5fa808f":"code","5b2db7af":"code","9c286aff":"code","bd6dd0b0":"code","7ced6e91":"code","f54017bd":"code","9a2c9eb5":"code","bf082f1b":"code","b32d907f":"code","38eec45b":"code","3c7586e5":"code","6e5d3f01":"code","8ed0aa8c":"code","7be8359e":"code","8b794ad1":"code","d0be123e":"code","5fa41fcf":"code","a76f8da2":"code","297a5c53":"code","c613381e":"code","472685cc":"code","ebcc06e8":"code","a22a6bc5":"code","60337c3e":"code","9c764612":"code","d1d21f12":"code","c53cc6e9":"code","1f7bc6ef":"code","d9ffb6b4":"code","7f202fdc":"markdown","ba7d9bc4":"markdown","0c30a563":"markdown","9cfbe2dc":"markdown","8034bebf":"markdown","88960bcc":"markdown","3a854912":"markdown","186ad10d":"markdown","856e8ab0":"markdown","24d8ed0c":"markdown","9e78a030":"markdown","52ac136d":"markdown","fb27f83f":"markdown","fcc10c15":"markdown","218336d2":"markdown","4eceb3af":"markdown","63558cc3":"markdown","27624904":"markdown","41ba642f":"markdown","d1867b93":"markdown","7f74be07":"markdown","83954704":"markdown","b8266210":"markdown","0b0c3d99":"markdown","cf894238":"markdown","9ace5a4d":"markdown"},"source":{"f9f1ad35":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","12c88b19":"# Table of Contents\n\n* [Cleaning](#section-one)\n* [Explore outliers and correlated features](#section-two)\n* [Un-concatenate train and test](#section-three)\n* [Clean outliers](#section-four)\n* [Data Imputing](#section-five)\n* [Explore target](#section-six)\n* [Supervised Learning](#section-seven)\n    - [Baseline : multiple linear regression](#subsection-one)\n    - [XGBOOST](#subsection-two)\n    - [KNN](#subsection-three)\n    - [Random Forest](#subsection-four)\n* [Performance tables](#section-eight)\n* [Save predictions](#section-nine)\n* [Graphical results : predicted vs true values](#section-ten)\n* [Best model on whole train set](#section-eleven)\n* [Predict on whole given test set](#section-twelve)\n* [Submission](#section-thirteen)","bf204dea":"import pycodestyle\n!pip install --index-url https:\/\/test.pypi.org\/simple\/ nbpep8","abed0567":"from nbpep8.nbpep8 import pep8","b0b46c69":"# Test example\nimport os,sys\na =  23\n\npep8(_ih)","46705008":"# Load the data set : train & test set already prepared\ntrain= pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest= pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","574e3ef9":"# Train and test shapes\nprint(\"train shape : \", train.shape, \"\\ntest shape : \", test.shape)","e04ea885":"train.info()","5f4d4776":"test.info()","48a82282":"df_train = pd.DataFrame(train)\ndf_test = pd.DataFrame(test)","0f0047f2":"pd.set_option('display.max_columns', None)\ndf_train.head()","09639756":"# Check difference between train and test pre made dataframes\n# Target = SalePrice\ntemp1 = list(df_train.columns.values)\ntemp2 = list(df_test.columns.values)\nprint(\"Difference:\", list(set(temp1)-set(temp2)))","b92011c9":"def Affichage_Data_Seuil(data, thresh, color='black', edgecolor='black',\n                         size_x=15, size_y=5, sizeLabelx=5,\n                         sizeTitle=15, sizeTxt=12):\n\n    plt.figure(figsize=(size_x, size_y))\n    plt.ylim(0, 100)\n    plt.gcf().subplots_adjust(bottom=0.5)\n\n    percentage = (data.isnull().mean()) * 100\n    percentage.sort_values(ascending=False).plot.bar(color=color,\n                                                     edgecolor=edgecolor)\n\n    plt.axhline(y=thresh, color='r', linestyle='-')\n\n    plt.title('Missing values per column %',\n              fontsize=sizeTitle,\n              weight='bold')\n\n    plt.text(len(data.isnull().sum() \/ len(data)) \/ 1.7,\n             thresh+10,\n             'Columns with more than %s%s missing values' % (thresh, '%'),\n             fontsize=sizeTxt,\n             weight='bold',\n             color='red',\n             ha='left',\n             va='top')\n\n    plt.text(len(data.isnull().sum() \/ len(data)) \/ 1.7,\n             thresh-3,\n             'Columns with less than %s%s missing values' % (thresh, '%'),\n             fontsize=sizeTxt,\n             weight='bold',\n             color='green',\n             ha='left',\n             va='top')\n\n    plt.xlabel('Columns', size=sizeLabelx, weight='bold')\n    plt.ylabel('% missing values', weight='bold')\n    plt.yticks(weight='bold')\n\n\n    return plt.show()\n\n\npep8(_ih)","df30a400":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Plot missing data inside df_train\nval = 20\nAffichage_Data_Seuil(df_train,\n                     val,\n                     color=sns.color_palette('Reds', 15),\n                     size_x=12,\n                     size_y=8)\n\n\npep8(_ih)","131fc374":"# Plot missing data inside df_test\nval = 20\nAffichage_Data_Seuil(df_test,\n                     val,\n                     color=sns.color_palette('Reds', 15),\n                     size_x=12,\n                     size_y=8)\n\n\npep8(_ih)","7a8d358c":"# Concatenate Train and test at first\ndf_train_no_target = df_train.copy()\n# Drop the target column\ndf_train_no_target = df_train.drop(['SalePrice'], axis=1)\n# Concatenate\ndf_all = pd.concat([df_train_no_target, df_test], ignore_index=True)\n\n\npep8(_ih)","9c3cc579":"# Quite observation to detect some possible outliers : example ; negative value for LotArea\ndf_all.describe()","d62defda":"# Delete columns with \"seuil\" or more missing data\nseuil = len(df_all)*(100-val)\/100\n\ndf_all_00 = df_all.copy()\ndf_all_00 = df_all_00.dropna(thresh=seuil, axis=1)\n\nprint(f\"Data shape before cleaning {df_all.shape}\")\nprint(f\"Data shape after cleaning {df_all_00.shape}\")\nprint(f\"We dropped {df_all.shape[1] - df_all_00.shape[1]} columns\")\n\n\npep8(_ih)","a13c6246":"# Select only columns containing numerical values (quick choice)\nCol_Float = list(df_all_00.select_dtypes(include=['float64', 'int64']).columns)\nCol_Float","d40214b9":"# Quite observation to detect some possible outliers : example ; negative value for LotArea\ndf_all.describe()","137f0192":"# Delete columns with \"seuil\" or more missing data\nseuil = len(df_all)*(100-val)\/100\n\ndf_all_01 = df_all.copy()\ndf_all_01 = df_all_01.dropna(thresh=seuil, axis=1)\n\nprint(f\"Data shape before cleaning {df_all.shape}\")\nprint(f\"Data shape after cleaning {df_all_01.shape}\")\nprint(f\"We dropped {df_all.shape[1]- df_all_01.shape[1]} columns\")","83de3560":"# Select only columns containing numerical values (quick choice)\nCol_Float = list(df_all_01.select_dtypes(include=['float64', 'int64']).columns)\nCol_Float","6201faf6":"df_all_01.columns","21927bc5":"# New dataframe with numerical values\ndf_all_01 = df_all_00.copy()\ndf_all_01 = df_all_01[Col_Float]\ndf_all_01.shape","2156ff68":"# Delete some columns considered as useless (example : ID)\ndf_all_02 = df_all_01.copy()\ndf_all_02 = df_all_02.drop(['Id'], axis=1)\ndf_all_02.shape","52c48a84":"# Check if some col can be removed due to low var threshold\nfrom sklearn.feature_selection import VarianceThreshold\n\nsel = VarianceThreshold(threshold=(0.01))\nsel.fit(df_all_02)\n\nprint('-----------------------------------------------------\\n')\nprint(\"Feature selection:\\n\", sel.get_support())\nprint(\"\\n\\nSelected features:\\n\", list(df_all_02.columns[sel.get_support()]))\nprint(\"\\n\\nRemoved features:\\n\", list(df_all_02.columns[~sel.get_support()]))\nprint('-----------------------------------------------------\\n')\n\n\npep8(_ih)","5d4a6e45":"def Matrice_Pearson_Data(Data, seuil, size_x=10, size_y=10):\n\n    titre = \"Linear correlation (Pearson) in between features\"\n\n    df_corr = Data.corr(method='pearson')\n\n    # Generate a mask for the upper triangle\n    mask = np.triu(np.ones_like(df_corr, dtype=bool))\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(size_x, size_y))\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    # sns.heatmap(df_corr, annot=True, mask=mask, annot_kws={'size':8}, center=0, cmap='coolwarm')\n\n    # Color levels only\n    sns.heatmap(df_corr, mask=mask, cmap='coolwarm')\n\n    # Choice to get full matrix\n    # sns.heatmap(df_corr, cmap='coolwarm')\n\n    # Adjust size of xlabels\n    plt.gcf().subplots_adjust(bottom=0.2)\n    plt.gcf().subplots_adjust(left=0.2)\n\n    plt.title(titre,\n              weight='bold',\n              fontsize=18)\n\n    plt.xticks(weight='bold')\n    plt.yticks(weight='bold')\n\n    # Select pairs with chosen threshold\n    threshold = seuil\n\n    # Plot correlated pairs\n    corr_pairs = df_corr.unstack().sort_values(kind=\"quicksort\")\n    strong_corr = (pd.DataFrame(corr_pairs[(abs(corr_pairs) > threshold)])\n                   .reset_index().rename(columns={0: 'corr_coeff'}))\n\n    strong_corr = strong_corr[(strong_corr.index % 2 == 0) & (strong_corr['level_0'] != strong_corr['level_1'])]\n    # strong_corr.sort_values('corr_coeff', ascending=False)\n\n    return plt.show(), strong_corr.sort_values('corr_coeff', ascending=False)\n\n\npep8(_ih)","1fb846af":"# Plot and save Pearson corr\nMatrice_Pearson_Data(df_train, 0.5)","e5e6e6b3":"# Plot and save Pearson corr\nMatrice_Pearson_Data(df_all_02, 0.5)","5647127f":"# We could go further with the variance inflation factor to delete correlated features\n# Choice to delete some redundant features (from Pearson correlation with threshold)\n# The purpose here is to avoid high multicolinearity (ie features explaining the same thing)\n\n# Delete features considered as useless at the same time\n\nCol_Correlated = ['GarageCars',\n                  'GarageYrBlt',\n                  'TotRmsAbvGrd']\n\ndf_all_03 = df_all_02.copy()\ndf_all_03 = df_all_03.drop(Col_Correlated, axis=1)\nprint(df_all_03.shape)\n\npep8(_ih)","5d3ee03a":"df_all_03.shape","f1de2d4c":"def Moustache_Data_Col(dataDF, width=20, height=8,\n                       titre='Boxplot', xval='', yval=''):\n\n    # Fig size\n    plt.figure(figsize=(width, height))\n\n    # Adjust size of ylabels\n    plt.gcf().subplots_adjust(left=0.2)\n\n    # Multiple plot\n    ax = sns.boxplot(data=dataDF, orient=\"h\", palette=\"Set2\")\n\n    # Title\n    plt.title(titre,\n              weight='bold',\n              fontsize=12)\n\n    # Labels x & y\n    plt.xlabel(xval, weight='bold', fontsize=14)\n    plt.ylabel(yval, weight='bold', fontsize=10)\n\n    return plt.show()\n\n\npep8(_ih)","74b1790d":"Moustache_Data_Col(df_all_03)","65801ba8":"# Clear the view (scales are different)\ndf_temp = df_all_03.drop(['LotArea'], axis=1)\nMoustache_Data_Col(df_temp)","e2704210":"Col_Num = list(df_all_03.columns)\nlen(Col_Num)","cc281431":"import math\n\n\ndef Plot_Hist_From_List(Data, Liste, PlotLine=5, L=30,\n                        H=70, C='green', D=False):\n\n    number = len(Liste)\n\n    i, j = 0, 0\n\n    COL_PLOT = PlotLine\n    ROW_PLOT = math.ceil(number\/COL_PLOT)\n\n    fig, axes = plt.subplots(nrows=ROW_PLOT, ncols=COL_PLOT,\n                             figsize=(L, H), squeeze=False)\n\n    for name in Liste:\n\n        # title = name_y + ' en fonction de ' + name_x\n\n        # axes[i][j].hist(x = Data[name], density=D, color = C)\n        sns.histplot(data=Data, x=name, stat=\"count\", ax=axes[i][j])\n        axes[i][j].set_ylabel(' ')\n        axes[i][j].set_xlabel(name)\n        # axs[i][j].set_title(title)\n\n        # Change line\/col\n        j += 1\n        if j % COL_PLOT == 0:\n            i += 1\n            j = 0\n\n    return plt.show()\n\n\npep8(_ih)","ebc6e0dd":"Plot_Hist_From_List(df_all_03, Col_Num)","74f78087":"def Plot_Boxplot_From_List(Data, Liste, PlotLine=5,\n                           L=30, H=70, C='green', D=False):\n\n    # Param\u00e8tres pour regler les lignes\n    number = len(Liste)\n\n    i, j = 0, 0\n\n    COL_PLOT = PlotLine\n    ROW_PLOT = math.ceil(number\/COL_PLOT)\n\n    fig, axes = plt.subplots(nrows=ROW_PLOT, ncols=COL_PLOT,\n                             figsize=(L, H), squeeze=False)\n\n    for name in Liste:\n\n        # titre = name_y + ' en fonction de ' + name_x  # Illisible \/ trop lourd\n\n        # axes[i][j].hist(x = Data[name], density=D, color = C)\n        sns.boxplot(data=Data, x=name, ax=axes[i][j])\n        axes[i][j].set_ylabel(' ')\n        axes[i][j].set_xlabel(name)\n        # axs[i][j].set_title(titre)\n\n        # Changement de ligne\/col\n        j += 1\n        if j % COL_PLOT == 0:\n            i += 1\n            j = 0\n\n    return plt.show()\n\n\npep8(_ih)","c611e645":"Plot_Boxplot_From_List(df_all_03, Col_Num)","dd86f5e2":"List_NaN_Top1 = ['EnclosedPorch',\n                 '3SsnPorch',\n                 'ScreenPorch',\n                 # 'PoolArea',\n                 'MiscVal',\n                 'LowQualFinSF']\n\nList_NaN_IQR = ['MSSubClass', 'LotFrontage', 'LotArea',\n                'OverallQual', 'OverallCond',\n                'YearBuilt',\n                'YearRemodAdd',\n                'MasVnrArea', 'BsmtFinSF1',\n                # 'BsmtFinSF2',\n                'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF',\n                '2ndFlrSF', 'GrLivArea',\n                'BsmtFullBath', 'FullBath', 'HalfBath',\n                'BedroomAbvGr', 'Fireplaces',\n                'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n                'MoSold', 'YrSold']\n\n\npep8(_ih)","06395536":"def NaN_Outliers_IQR(data, col_names):\n\n    for col in col_names:\n\n        # Quantiles\n        Q1 = data[col].quantile(0.25)\n        Q3 = data[col].quantile(0.75)\n\n        # IQR\n        IQR = Q3 - Q1\n\n        # Limits\n        LowerLim = Q1 - 1.5*IQR\n        UpperLim = Q3 + 1.5*IQR\n\n        # Outliers to NaN\n        data.loc[data[col] <= LowerLim, [col]] = np.nan\n        data.loc[data[col] >= UpperLim, [col]] = np.nan\n\n    return data\n\n\npep8(_ih)","050e232a":"def NaN_Top_percent(data, col_names):\n\n    for col in col_names:\n\n        borne = np.percentile(data[col], 99)\n\n        # Top 1% to NaN\n        data.loc[data[col] >= borne, [col]] = np.nan\n\n    return data\n\n\npep8(_ih)","c496402b":"print('Train shape', df_train.shape, 'Test Shape', df_test.shape)","16a11b51":"df_train_02 = df_all_03[:1460]\ndf_test_02 = df_all_03[1460:]\nprint('Train shape', df_train_02.shape, 'Test Shape', df_test_02.shape)\n\n\npep8(_ih)","08e22b34":"df_train_03 = df_train_02.copy()\n\n# Clean features that seem fit with 1.5*IQR\ndf_train_03 = NaN_Outliers_IQR(df_train_03, List_NaN_IQR)\n\n# Clean features that seem fit removing top1% values\ndf_train_03 = NaN_Top_percent(df_train_03, List_NaN_Top1)\n\n\npep8(_ih)","18ed66d0":"def seuil_nan_data(data, axisVal, seuil):\n\n    if(axisVal == 0):\n        name = 'columns'\n        str1 = 'of'\n\n    if(axisVal == 1):\n        name = 'observations'\n        str1 = 'of'\n\n    tab_nan = (data.isnull().sum(axis=axisVal)\/data.shape[axisVal])\n    count_nan = 0\n\n    for i in range(len(tab_nan)):\n        if(tab_nan[i] >= (seuil\/100)):\n            count_nan += 1\n\n    print('Number', str1, name, 'with more than',\n          seuil, '% NaN:', count_nan, name)\n\n\npep8(_ih)","111c63e9":"seuil_nan_data(df_train_02, 0, 5)","8760b633":"# Check cleaning \n# Go back if a feature is to severely impacted\n# We still want to keep as much data as possible\nseuil_nan_data(df_train_03, 0, 25)","7302a998":"# Plot missing data inside df_train\nval = 20\nAffichage_Data_Seuil(df_train_03,\n                     val,\n                     color=sns.color_palette('Reds', 15),\n                     size_x=12,\n                     size_y=8)\n\npep8(_ih)","13bf3192":"from sklearn.impute import KNNImputer\n\n# Imputer knn\nimputer = KNNImputer(n_neighbors=10) # Default n_neighbors set to 5","c0b6720b":"df_train_04 = df_train_03.copy()\ndf_train_04 = pd.DataFrame(imputer.fit_transform(df_train_04),\n                           columns=df_train_04.columns)\n\n# Checking\ndf_train_04.isna().sum()\n\n\npep8(_ih)","e0b7349e":"# KNN imputing done to test set\ndf_test_03 = df_test_02.copy()\ndf_test_03 = pd.DataFrame(imputer.fit_transform(df_test_03),\n                          columns=df_test_03.columns)\n\n# Checking\ndf_test_03.isna().sum()\n\n\npep8(_ih)","465f3ec8":"df_train_05 = df_train_04.copy()\ndf_train_05['SalePrice'] = df_train['SalePrice']\n# Beware : no observation has been delete before\ndf_train_05.shape\n\n\npep8(_ih)","a81872c5":"df_train_05.shape","e363ea06":"# Check missing data inside target\ndf_train_05['SalePrice'].isnull().sum()","b88603cd":"df_train_05.head(5)","2ba9bb34":"# Target distribution\n\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n\n# Check the distribution\n\nsns.distplot(df_train_05['SalePrice'], color=\"g\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\nsns.despine(trim=True, left=True)\n\nplt.show()\n\n\npep8(_ih)","18278396":"# Skewness of target\nprint(\"Skewness: %f\" % df_train_05['SalePrice'].skew())\n\n# Mode value\nprint('Mode: ', df_train_05['SalePrice'].mode())","8b9a7023":"df_train_05['SalePrice'].describe()","f2f2be66":"def Plot_Scatter_2_Features_ALL(Data, cols_x, cols_y, PltLine=5, L=30, H=70):\n\n    # Parameters\n    a = len(cols_x)\n    b = len(cols_y)\n    number = a*b\n\n    i, j = 0, 0\n    PLOTS_PER_ROW = PltLine\n    fig, axs = plt.subplots(math.ceil(number\/PLOTS_PER_ROW),\n                            PLOTS_PER_ROW,\n                            figsize=(L, H))\n\n    for name_y in cols_y:\n        for name_x in cols_x:\n\n            # titre = name_y + ' en fonction de ' + name_x\n            # Unreadable\n\n            axs[i][j].scatter(x=Data[name_x],\n                              y=Data[name_y],\n                              color='black',\n                              alpha=0.3)\n            axs[i][j].set_ylabel(name_y)\n            axs[i][j].set_xlabel(name_x)\n            # axs[i][j].set_title(titre)\n\n            # Change of row\/col\n            j += 1\n            if j % PLOTS_PER_ROW == 0:\n                i += 1\n                j = 0\n\n\n    return plt.show()\n\n\npep8(_ih)","6aac6682":"# Get the desired col names\ncols_scatter_x = list(df_train_04.columns)\ncols_scatter_y = ['SalePrice']\n# Brutal plots : target vs every feature\nPlot_Scatter_2_Features_ALL(df_train_05, cols_scatter_x, cols_scatter_y, PltLine=4)\n\n\npep8(_ih)","7ac9d367":"# Get the desired col names\ncols_scatter_x = ['OverallQual', 'YearBuilt', 'YearRemodAdd',\n                  'TotalBsmtSF', '1stFlrSF', 'GrLivArea',\n                  'FullBath']\ncols_scatter_y = ['SalePrice']\n# Brutal plots : target vs every feature\nPlot_Scatter_2_Features_ALL(df_train_05, cols_scatter_x, cols_scatter_y, PltLine=4, L=40, H=20)\n\n\npep8(_ih)","8e8c5a78":"# # Get the desired col names and explore\n# cols_x = 'YrSold'\n# cols_y = 'SalePrice'\n# # Plot boxplot for more adapted cases\n# Data = df_train_05.copy()\n# sns.boxplot(x=cols_x, y=cols_y, data=Data)\n\n# # Does not bring much, can be erased\n# df_train_05 = df_train_05.drop(['YrSold'], axis=1)\n# df_train_04 = df_train_04.drop(['YrSold'], axis=1)\n# df_test_03 = df_test_03.drop(['YrSold'], axis=1)\n\n# pep8(_ih)","67f4b242":"# Get the desired col names and explore\ncols_x = 'YearRemodAdd'\ncols_y = 'SalePrice'\n\n# Plot boxplot for more adapted cases\nData = df_train_05.copy()\nsns.boxplot(x=cols_x, y=cols_y, data=Data)\n\n\npep8(_ih)","2df3e8e0":"# Get the desired col names and explore\ncols_x = 'YearBuilt'\ncols_y = 'SalePrice'\n\n# Plot boxplot for more adapted cases\nData = df_train_05.copy()\nsns.boxplot(x=cols_x, y=cols_y, data=Data)\n\n\npep8(_ih)","a5b48ffc":"bins = pd.IntervalIndex.from_tuples([(0, 1), (1, 2), (2, 3), (3, 4)])\n\ncols_x = 'BedroomAbvGr'\ncols_y = 'SalePrice'\n\npalette = sns.color_palette(\"Paired\")\n\nsns.catplot(data=Data,\n            kind=\"box\",\n            x=pd.cut(Data[cols_x], bins=bins),\n            y=cols_y,\n            ci=\"sd\",\n            # color=\"#006d77\",\n            palette=palette,\n            # alpha=.5,\n            height=8,\n            aspect=1.5,\n            showmeans=True,\n            meanprops={\"marker\": \"o\",\n                       \"markerfacecolor\": \"white\",\n                       \"markeredgecolor\": \"black\",\n                       \"markersize\": \"10\"})\n\n\npep8(_ih)","d57c139f":"df_train_05.describe()","5f81fe9b":"bins = pd.IntervalIndex.from_tuples([(0, 10), (10, 20),\n                                     (20, 30), (30, 40),\n                                     (40, 50), (50, 60),\n                                     (60, 70), (70, 80),\n                                     (80, 90), (90, 100),\n                                     (100, 110), (110, 120)])\n\ncols_x = 'MSSubClass'\ncols_y = 'SalePrice'\n\npalette = sns.color_palette(\"Paired\")\n\nsns.catplot(data=Data,\n            kind=\"box\",\n            x=pd.cut(Data[cols_x], bins=bins),\n            y=cols_y,\n            ci=\"sd\",\n            # color=\"#006d77\",\n            palette=palette,\n            # alpha=.5,\n            height=8,\n            aspect=1.5,\n            showmeans=True,\n            meanprops={\"marker\": \"o\",\n                       \"markerfacecolor\": \"white\",\n                       \"markeredgecolor\": \"black\",\n                       \"markersize\": \"10\"})\n\n\npep8(_ih)","3509e9f6":"bins = pd.IntervalIndex.from_tuples([(0, 1), (1, 2), (2, 3)])\n\ncols_x = 'FullBath'\ncols_y = 'SalePrice'\n\npalette = sns.color_palette(\"Paired\")\n\nsns.catplot(data=Data,\n            kind=\"box\",\n            x=pd.cut(Data[cols_x], bins=bins),\n            y=cols_y,\n            ci=\"sd\",\n            # color=\"#006d77\",\n            palette=palette,\n            # alpha=.5,\n            height=8,\n            aspect=1.5,\n            showmeans=True,\n            meanprops={\"marker\": \"o\",\n                       \"markerfacecolor\": \"white\",\n                       \"markeredgecolor\": \"black\",\n                       \"markersize\": \"10\"})\n\nplt.savefig('Figures\/P8_Boxplot_FullBath_Vs_y.jpg')\n\n\npep8(_ih)","78db92b9":"bins = pd.IntervalIndex.from_tuples([(0, 1), (1, 2), (2, 3),\n                                     (3, 4), (4, 5), (6, 7),\n                                     (7, 8), (8, 9), (9, 10)])\n\ncols_x = 'OverallQual'\ncols_y = 'SalePrice'\n\npalette = sns.color_palette(\"Paired\")\n\nsns.catplot(data=Data,\n            kind=\"box\",\n            x=pd.cut(Data[cols_x], bins=bins),\n            y=cols_y,\n            ci=\"sd\",\n            # color=\"#006d77\",\n            palette=palette,\n            # alpha=.5,\n            height=8,\n            aspect=1.5,\n            showmeans=True,\n            meanprops={\"marker\": \"o\",\n                       \"markerfacecolor\": \"white\",\n                       \"markeredgecolor\": \"black\",\n                       \"markersize\": \"10\"})\n\nplt.savefig('Figures\/P8_Boxplot_OverallQual_Vs_y.jpg')\n\n\npep8(_ih)","df82159f":"df_train_04.head()","6f4c9aee":"from sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\n\n# We don't have target values for test set\n# Train \/ test split is required\nX = df_train_04\ny = df_train_05['SalePrice']\n\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X,\n                                                                    y,\n                                                                    test_size=0.20,\n                                                                    random_state=100)\n\nprint(\"Training: {} rows,\\nTest: {} rows.\\n\".format(X_train.shape[0],\n                                                    X_test.shape[0]))\n\n\npep8(_ih)","89822d49":"from sklearn import preprocessing\n\n# Robust Scaling\nstd_scale = preprocessing.RobustScaler().fit(X_train)\nX_train_Scaled = std_scale.transform(X_train)\nX_test_Scaled = std_scale.transform(X_test)\n\nprint('X_train_Scaled :', X_train_Scaled.shape,\n      '\\nX_test_Scaled :', X_test_Scaled.shape)\n\nprint('y_train :', y_train.shape,\n      '\\ny_test :', y_test.shape)\n\n# Reshape y vectors\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\n\nprint('y_train reshaped :', y_train.shape,\n      '\\ny_test reshaped :', y_test.shape)\n\n\npep8(_ih)","64ad8463":"from sklearn.utils.validation import check_consistent_length, check_array\n\n\ndef mean_absolute_percentage_error(y_true, y_pred,\n                                   sample_weight=None,\n                                   multioutput='uniform_average'):\n    \"\"\"Mean absolute percentage error regression loss.\n    Note here that we do not represent the output as a percentage in range\n    [0, 100]. Instead, we represent it in range [0, 1\/eps]. Read more in the\n    :ref:`User Guide <mean_absolute_percentage_error>`.\n    .. versionadded:: 0.24\n    Parameters\n    ----------\n    y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n        Ground truth (correct) target values.\n    y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n        Estimated target values.\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights.\n    multioutput : {'raw_values', 'uniform_average'} or array-like\n        Defines aggregating of multiple output values.\n        Array-like value defines weights used to average errors.\n        If input is list then the shape must be (n_outputs,).\n        'raw_values' :\n            Returns a full set of errors in case of multioutput input.\n        'uniform_average' :\n            Errors of all outputs are averaged with uniform weight.\n    Returns\n    -------\n    loss : float or ndarray of floats in the range [0, 1\/eps]\n        If multioutput is 'raw_values', then mean absolute percentage error\n        is returned for each output separately.\n        If multioutput is 'uniform_average' or an ndarray of weights, then the\n        weighted average of all output errors is returned.\n        MAPE output is non-negative floating point. The best value is 0.0.\n        But note the fact that bad predictions can lead to arbitarily large\n        MAPE values, especially if some y_true values are very close to zero.\n        Note that we return a large value instead of `inf` when y_true is zero.\n    Examples\n    --------\n    >>> from sklearn.metrics import mean_absolute_percentage_error\n    >>> y_true = [3, -0.5, 2, 7]\n    >>> y_pred = [2.5, 0.0, 2, 8]\n    >>> mean_absolute_percentage_error(y_true, y_pred)\n    0.3273...\n    >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n    >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n    >>> mean_absolute_percentage_error(y_true, y_pred)\n    0.5515...\n    >>> mean_absolute_percentage_error(y_true, y_pred, multioutput=[0.3, 0.7])\n    0.6198...\n    \"\"\"\n    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n        y_true, y_pred, multioutput)\n    check_consistent_length(y_true, y_pred, sample_weight)\n    epsilon = np.finfo(np.float64).eps\n    mape = np.abs(y_pred - y_true) \/ np.maximum(np.abs(y_true), epsilon)\n    output_errors = np.average(mape,\n                               weights=sample_weight, axis=0)\n    if isinstance(multioutput, str):\n        if multioutput == 'raw_values':\n            return output_errors\n        elif multioutput == 'uniform_average':\n            # pass None as weights to np.average: uniform mean\n            multioutput = None\n\n    return np.average(output_errors, weights=multioutput)\n\n\ndef _check_reg_targets(y_true, y_pred, multioutput, dtype=\"numeric\"):\n    \"\"\"Check that y_true and y_pred belong to the same regression task.\n    Parameters\n    ----------\n    y_true : array-like\n    y_pred : array-like\n    multioutput : array-like or string in ['raw_values', uniform_average',\n        'variance_weighted'] or None\n        None is accepted due to backward compatibility of r2_score().\n    Returns\n    -------\n    type_true : one of {'continuous', continuous-multioutput'}\n        The type of the true target data, as output by\n        'utils.multiclass.type_of_target'.\n    y_true : array-like of shape (n_samples, n_outputs)\n        Ground truth (correct) target values.\n    y_pred : array-like of shape (n_samples, n_outputs)\n        Estimated target values.\n    multioutput : array-like of shape (n_outputs) or string in ['raw_values',\n        uniform_average', 'variance_weighted'] or None\n        Custom output weights if ``multioutput`` is array-like or\n        just the corresponding argument if ``multioutput`` is a\n        correct keyword.\n    dtype : str or list, default=\"numeric\"\n        the dtype argument passed to check_array.\n    \"\"\"\n    check_consistent_length(y_true, y_pred)\n    y_true = check_array(y_true, ensure_2d=False, dtype=dtype)\n    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n\n    if y_true.ndim == 1:\n        y_true = y_true.reshape((-1, 1))\n\n    if y_pred.ndim == 1:\n        y_pred = y_pred.reshape((-1, 1))\n\n    if y_true.shape[1] != y_pred.shape[1]:\n        raise ValueError(\"y_true and y_pred have different number of output \"\n                         \"({0}!={1})\".format(y_true.shape[1], y_pred.shape[1]))\n\n    n_outputs = y_true.shape[1]\n    allowed_multioutput_str = ('raw_values', 'uniform_average',\n                               'variance_weighted')\n    if isinstance(multioutput, str):\n        if multioutput not in allowed_multioutput_str:\n            raise ValueError(\"Allowed 'multioutput' string values are {}. \"\n                             \"You provided multioutput={!r}\".format(\n                                 allowed_multioutput_str,\n                                 multioutput))\n    elif multioutput is not None:\n        multioutput = check_array(multioutput, ensure_2d=False)\n        if n_outputs == 1:\n            raise ValueError(\"Custom weights are useful only in \"\n                             \"multi-output cases.\")\n        elif n_outputs != len(multioutput):\n            raise ValueError((\"There must be equally many custom weights \"\n                              \"(%d) as outputs (%d).\") %\n                             (len(multioutput), n_outputs))\n    y_type = 'continuous' if n_outputs == 1 else 'continuous-multioutput'\n\n    return y_type, y_true, y_pred, multioutput\n\n\npep8(_ih)","29395c25":"from sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\n\nlr = linear_model.LinearRegression()\n\n\npep8(_ih)","e06b1f92":"import timeit\n# --------------------- FIT\nstart_time = timeit.default_timer()\n\n# ---------------------------------------------------\nlr.fit(X_train_Scaled, y_train)\n# ---------------------------------------------------\n\nTrain_Time_LR = timeit.default_timer() - start_time\nprint('Time :', Train_Time_LR, 's')\n# --------------------------------------------------------------------\n\n\npep8(_ih)","d96387f1":"# --------------------- PREDICT\nstart_time = timeit.default_timer()\n\n# ---------------------------------------------------\ny_pred_LR = lr.predict(X_test_Scaled)\n# ---------------------------------------------------\n\nPred_Time_LR = timeit.default_timer() - start_time\nprint('Time :', Pred_Time_LR, 's')\n# --------------------------------------------------------------------\n\n\npep8(_ih)","ab855329":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error","a65851fb":"# from sklearn.metrics import mean_absolute_percentage_error\n# Doesn't work, add direclty manually MAPE function","34a1b042":"from sklearn.metrics import mean_squared_log_error","0bd26dfe":"# Les scores de pr\u00e9cision du mod\u00e8le\nR_2_LR = r2_score(y_test, y_pred_LR)\nprint('r2 score is', R_2_LR)\n\nMSE_LR = mean_squared_error(y_test, y_pred_LR)\nprint('mean_sqrd_error =', MSE_LR)\n\nMAE_LR = mean_absolute_error(y_test, y_pred_LR)\nprint('mean_absolute_error =', MAE_LR)\n\nRMSE_LR = np.sqrt(mean_squared_error(y_test, y_pred_LR))\nprint('root_mean_squared error =', RMSE_LR)\n\nMAPE_LR = mean_absolute_percentage_error(y_test, y_pred_LR)*100\nprint('mean_absolute_percentage_error =', MAPE_LR, '%')\n\nMSLE_LR = mean_squared_log_error(y_test, y_pred_LR)*100\nprint('mean_squared_log_error =', MSLE_LR)\n\n\npep8(_ih)","4edc1931":"# !pip install xgboost","d1892bdb":"from xgboost.sklearn import XGBRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold","74c7bb25":"kfold = KFold(n_splits=5, shuffle=True, random_state=100)\n\n\npep8(_ih)","df7a4c2a":"param_XGB = {'n_estimators': [200, 500, 1000, 2000],\n             'colsample_bytree': [0.6, 0.8],\n             'max_depth': [5, 10, 15],\n             'reg_alpha': [1.1, 1.3, 1.5],\n             'reg_lambda': [1.1, 1.3, 1.5],\n             'subsample': [0.5, 0.7, 0.9],\n             'learning_rate': [0.01, 0.001]}\n\nscoring_XGB = 'neg_mean_absolute_error'  # 'r2'\n\nModel_XGB = XGBRegressor()  # tree_method='gpu_hist'\n\nGS_XGB = GridSearchCV(estimator=Model_XGB,\n                      param_grid=param_XGB,\n                      cv=kfold,\n                      n_jobs=-1,\n                      scoring=scoring_XGB)\n\n\npep8(_ih)","79b2c31f":"# --------------------- FIT\n\nstart_time = timeit.default_timer()\n\n# ---------------------------------------------------\nfit_Model_XGB = GS_XGB.fit(X_train_Scaled, y_train)\n# ---------------------------------------------------\n\nTrain_Time_XGB = timeit.default_timer() - start_time\nprint('Time :', Train_Time_XGB, 's')\n# --------------------------------------------------------------------\n\n\npep8(_ih)","dea48b9c":"# --------------------- PREDICT\nstart_time = timeit.default_timer()\n\n# -------------------------------------------------\ny_pred_XGB = fit_Model_XGB.predict(X_test_Scaled)\n# -------------------------------------------------\n\nPred_Time_XGB = timeit.default_timer() - start_time\nprint('Time :', Pred_Time_XGB, 's')\n# --------------------------------------------------------------------\n\n\npep8(_ih)","8a61a5cd":"print(fit_Model_XGB.best_params_)","05a8609b":"print(GS_XGB.best_estimator_)","4e858abd":"R_2_XGB = r2_score(y_test, y_pred_XGB)\nprint('r2 score is', R_2_XGB)\n\nMSE_XGB = mean_squared_error(y_test, y_pred_XGB)\nprint('mean_sqrd_error =', MSE_XGB)\n\nMAE_XGB = mean_absolute_error(y_test, y_pred_XGB)\nprint('mean_absolute_error =', MAE_XGB)\n\nRMSE_XGB = np.sqrt(mean_squared_error(y_test, y_pred_XGB))\nprint('root_mean_squared error =', RMSE_XGB)\n\nMAPE_XGB = mean_absolute_percentage_error(y_test, y_pred_XGB)*100\nprint('mean_absolute_percentage_error =', MAPE_XGB, '%')\n\nMSLE_XGB = mean_squared_log_error(y_test, y_pred_XGB)*100\nprint('mean_squared_log_error =', MSLE_XGB)\n\n\npep8(_ih)","790ccbcd":"data_X_Features = df_train_04\nsorted_idx = GS_XGB.best_estimator_.feature_importances_.argsort()\nplt.figure(figsize=(15, 12))\nplt.barh(data_X_Features.columns[sorted_idx],\n         GS_XGB.best_estimator_.feature_importances_[sorted_idx])\nplt.xlabel(\"Xgboost Feature Importance\")\n\nplt.savefig('Figures\/P8_Xgboost_Feature_Importance.jpg')\n\n\npep8(_ih)","3e9bcf71":"from sklearn import neighbors\n\nscoring_KNN = 'neg_mean_absolute_error'  # 'r2'\n\n# Create the parameter grid based on the results of random search\n\nparam_KNN = {\"n_neighbors\": range(1, 30),\n             \"weights\": [\"uniform\", \"distance\"],\n             'p': [1, 2],\n             'leaf_size': range(1, 50)}\n\nModel_KNN = neighbors.KNeighborsRegressor()\n\nGS_KNN = GridSearchCV(estimator=Model_KNN,\n                      param_grid=param_KNN,\n                      cv=kfold,\n                      n_jobs=-1,\n                      scoring=scoring_KNN)\n\n\npep8(_ih)","08724fd2":"# --------------------- FIT\nstart_time = timeit.default_timer()\n\n# -------------------------------------------------\nfit_Model_KNN = GS_KNN.fit(X_train_Scaled, y_train)\n# -------------------------------------------------\n\nTrain_Time_KNN = timeit.default_timer() - start_time\nprint('Time :', Train_Time_KNN, 's')\n# --------------------------------------------------------------------\n\n\npep8(_ih)","0ff00504":"# --------------------- PREDICT\n\nstart_time = timeit.default_timer()\n\n# --------------------------------------------\ny_pred_KNN = fit_Model_KNN.predict(X_test_Scaled)\n# --------------------------------------------\n\nPred_Time_KNN = timeit.default_timer() - start_time\nprint('Time :', Pred_Time_KNN, 's')\n# --------------------------------------------------------------------\n\n\npep8(_ih)","ac399a5f":"print(GS_KNN.best_estimator_)","a0ea8d1b":"print(GS_KNN.best_params_)","a4c688d8":"R_2_KNN = r2_score(y_test, y_pred_KNN)\nprint('r2 score is', R_2_KNN)\n\nMSE_KNN = mean_squared_error(y_test, y_pred_KNN)\nprint('mean_sqrd_error =', MSE_KNN)\n\nMAE_KNN = mean_absolute_error(y_test, y_pred_KNN)\nprint('mean_absolute_error =', MAE_KNN)\n\nRMSE_KNN = np.sqrt(mean_squared_error(y_test, y_pred_KNN))\nprint('root_mean_squared error =', RMSE_KNN)\n\nMAPE_KNN = mean_absolute_percentage_error(y_test, y_pred_KNN)*100\nprint('mean_absolute_percentage_error =', MAPE_KNN, '%')\n\nMSLE_KNN = mean_squared_log_error(y_test, y_pred_KNN)*100\nprint('mean_squared_log_error =', MSLE_KNN)\n\n\npep8(_ih)","aac98b64":"y_train = y_train.ravel()\ny_test = y_test.ravel()","e5fa808f":"from sklearn.ensemble import RandomForestRegressor\n\nscoring_RF = 'neg_mean_absolute_error'  # 'r2'\n\n# Create the parameter grid based on the results of random search\n\nparam_RF = {'bootstrap': [True],\n            'max_depth': [5, 10, 30, 40],\n            'max_features': [2, 5, 10],\n            'min_samples_leaf': [1, 3, 5, 10],\n            'min_samples_split': [2, 6, 10],\n            'n_estimators': [200, 500, 1000, 2000]}\n\nrf = RandomForestRegressor()\n\nGS_RF = GridSearchCV(estimator=rf,\n                     param_grid=param_RF,\n                     cv=kfold,\n                     n_jobs=-1,\n                     scoring=scoring_RF)\n\n# GS_RF = RandomForestRegressor()\n\n\npep8(_ih)","5b2db7af":"# --------------------- FIT\nstart_time = timeit.default_timer()\n\n# -------------------------------------------------\nfit_Model_RF = GS_RF.fit(X_train, y_train)\n# -------------------------------------------------\n\nTrain_Time_RF = timeit.default_timer() - start_time\nprint('Time :', Train_Time_RF, 's')\n# --------------------------------------------------------------------\n\n\npep8(_ih)","9c286aff":"# --------------------- PREDICT\n\nstart_time = timeit.default_timer()\n\n# --------------------------------------------\ny_pred_RF = fit_Model_RF.predict(X_test)\n# --------------------------------------------\n\nPred_Time_RF = timeit.default_timer() - start_time\nprint('Time :', Pred_Time_RF, 's')\n# --------------------------------------------------------------------\n\n\npep8(_ih)","bd6dd0b0":"print(GS_RF.best_estimator_)","7ced6e91":"print(GS_RF.best_params_)","f54017bd":"R_2_RF = r2_score(y_test, y_pred_RF)\nprint('r2 score is', R_2_RF)\n\nMSE_RF = mean_squared_error(y_test, y_pred_RF)\nprint('mean_sqrd_error =', MSE_RF)\n\nMAE_RF = mean_absolute_error(y_test, y_pred_RF)\nprint('mean_absolute_error =', MAE_RF)\n\nRMSE_RF = np.sqrt(mean_squared_error(y_test, y_pred_RF))\nprint('root_mean_squared error =', RMSE_RF)\n\nMAPE_RF = mean_absolute_percentage_error(y_test, y_pred_RF)*100\nprint('mean_absolute_percentage_error =', MAPE_RF, '%')\n\nMSLE_RF = mean_squared_log_error(y_test, y_pred_RF)*100\nprint('mean_squared_log_error =', MSLE_RF)\n\n\npep8(_ih)","9a2c9eb5":"sorted_idx = GS_RF.best_estimator_.feature_importances_.argsort()\nplt.figure(figsize=(15, 12))\nplt.barh(data_X_Features.columns[sorted_idx],\n         GS_RF.best_estimator_.feature_importances_[sorted_idx])\nplt.xlabel(\"Random forest Feature Importance\")\n\nplt.savefig('Figures\/P8_FeatureImp_RF.jpg')\n\npep8(_ih)","bf082f1b":"Results = {'Mod\u00e8le': ['Linear_Regression', 'XGBOOST', 'Random_Forest', 'KNN'],\n           'Fit_Time':  [Train_Time_LR, Train_Time_XGB, Train_Time_RF, Train_Time_KNN],\n           'R\u00b2_test': [R_2_LR, R_2_XGB, R_2_RF, R_2_KNN],\n           'MSE': [MSE_LR, MSE_XGB, MSE_RF, MSE_KNN],\n           'MAE': [MAE_LR, MAE_XGB, MAE_RF,  MAE_KNN],\n           'RMSE': [RMSE_LR, RMSE_XGB, RMSE_RF, RMSE_KNN],\n           'MAPE': [MAPE_LR, MAPE_XGB, MAPE_RF, MAPE_KNN],\n           'MSLE': [MSLE_LR, MSLE_XGB, MSLE_RF, MSLE_KNN]}\n\n\ndf_Results = pd.DataFrame(Results,\n                          columns=['Mod\u00e8le', 'Fit_Time', 'R\u00b2_test',\n                                   'MSE', 'MAE', 'RMSE', 'MAPE', 'MSLE'])\n\n\npep8(_ih)","b32d907f":"df_Results","38eec45b":"df_Pred_Test = pd.DataFrame({'Test': y_test.ravel(),\n                             'Prediction_LR': y_pred_LR.ravel(),\n                             'Prediction_XGB': y_pred_XGB.ravel(),\n                             'Prediction_RF': y_pred_RF.ravel(),\n                             'Prediction_KNN': y_pred_KNN.ravel()})\n\npep8(_ih)","3c7586e5":"# Ajout des colonnes pour les erreurs\ndf_Pred_Test['Error_LR'] = abs(df_Pred_Test['Test']-df_Pred_Test['Prediction_LR'])\/df_Pred_Test['Test']*100\ndf_Pred_Test['Error_XGB'] = abs(df_Pred_Test['Test']-df_Pred_Test['Prediction_XGB'])\/df_Pred_Test['Test']*100\ndf_Pred_Test['Error_RF'] = abs(df_Pred_Test['Test']-df_Pred_Test['Prediction_RF'])\/df_Pred_Test['Test']*100\ndf_Pred_Test['Error_KNN'] = abs(df_Pred_Test['Test']-df_Pred_Test['Prediction_KNN'])\/df_Pred_Test['Test']*100\n\npep8(_ih)","6e5d3f01":"df_Pred_Test","8ed0aa8c":"def color_to_use(num):\n    if np.isinf(num):\n        return \"#f7a6e3\"\n    elif num >= 75:\n        return \"#f7a6e3\"\n    elif num < 75 and num >= 50:\n        return \"#f7a6ba\"\n    elif num < 50 and num >= 25:\n        return \"#f7baa6\"\n    elif num < 25 and num >= 10:\n        return \"#f7e3a6\"\n    elif num < 10 and num >= 5:\n        return \"#e3f7a6\"\n    elif num < 5:\n        return \"#baf7a6\"\n    else:\n        return \"#baf7a6\"\n\n\npep8(_ih)","7be8359e":"df_Pred_Test['color_LR'] = df_Pred_Test['Error_LR'].apply(color_to_use)\ndf_Pred_Test['color_XGB'] = df_Pred_Test['Error_XGB'].apply(color_to_use)\ndf_Pred_Test['color_RF'] = df_Pred_Test['Error_RF'].apply(color_to_use)\ndf_Pred_Test['color_KNN'] = df_Pred_Test['Error_KNN'].apply(color_to_use)\n\ndf_Pred_Test.head()\n\n\npep8(_ih)","8b794ad1":"def plot_pred_vs_test(df_Pred_Test, error_col, color_col, pred_model):\n\n    df_Pred_Test = df_Pred_Test.sort_values(by=error_col)\n\n    dict_lab = {'#f7a6e3': '+75%',\n                '#f7a6ba': \"between 50% & 75%\",\n                '#f7baa6': \"between 25% & 50%\",\n                '#f7e3a6': \"between 10% & 25%\",\n                '#e3f7a6': \"between 10% & 5%\",\n                '#baf7a6': \"-5%\"}\n\n    fig = plt.figure(figsize=[10, 10])\n    # fig.patch.set_facecolor('#E0E0E0')\n    fig.patch.set_alpha(0.7)\n    title = pred_model + \" : Pred vs Test, error highlighted (%).\"\n    plt.title(title, size=16)\n\n    for c in df_Pred_Test[color_col].unique():\n\n        d = df_Pred_Test.loc[df_Pred_Test[color_col] == c]\n\n        sns.scatterplot(data=d,\n                        x=\"Test\",\n                        y=pred_model,\n                        color=c,\n                        label=dict_lab[c])\n\n    plt.xlabel('Test ', size=13)\n    plt.ylabel('Prediction ', size=13)\n    plt.grid(color='#dddddd')\n\n\npep8(_ih)","d0be123e":"# Parameters\nerror_col = \"Error_LR\"\ncolor_col = \"color_LR\"\npred_model = \"Prediction_LR\"\n\nplot_pred_vs_test(df_Pred_Test, error_col, color_col, pred_model)\nplt.ylim((0, 500000))\n\nplt.savefig('Figures\/P8_PredVsTrue_LR.jpg')\n\npep8(_ih)","5fa41fcf":"# Parameters\nerror_col = \"Error_XGB\"\ncolor_col = \"color_XGB\"\npred_model = \"Prediction_XGB\"\n\nplot_pred_vs_test(df_Pred_Test, error_col, color_col, pred_model)\nplt.ylim((0, 500000))\n\nplt.savefig('Figures\/P8_PredVsTrue_XGB.jpg')\n\npep8(_ih)","a76f8da2":"# Parameters\nerror_col = \"Error_KNN\"\ncolor_col = \"color_KNN\"\npred_model = \"Prediction_KNN\"\n\nplot_pred_vs_test(df_Pred_Test, error_col, color_col, pred_model)\nplt.ylim((0, 500000))\n\nplt.savefig('Figures\/P8_PredVsTrue_KNN.jpg')\n\npep8(_ih)","297a5c53":"# Parameters\nerror_col = \"Error_RF\"\ncolor_col = \"color_RF\"\npred_model = \"Prediction_RF\"\n\nplot_pred_vs_test(df_Pred_Test, error_col, color_col, pred_model)\nplt.ylim((0, 500000))\n\nplt.savefig('Figures\/P8_PredVsTrue_RF.jpg')\n\npep8(_ih)","c613381e":"def low_error_count(df, error, thresh):\n\n    val = df[error][df[error] <= thresh].count()\/df.shape[0]*100\n\n    print(error + \" cases with error below than\", val, \"(%).\")\n\n    return val\n\n\ndef high_error_count(df, error, thresh):\n\n    val = df[error][df[error] >= thresh].count()\/df.shape[0]*100\n\n    print(error + \" cases with error upper than\", val, \"(%).\")\n\n    return val\n\n\npep8(_ih)","472685cc":"# Global parameter\ndf = df_Pred_Test\nthresh = 5\n\nprint('\\n--------------- Low error ---------------------\\n')\n\n# Params\nerror = \"Error_XGB\"\nxgb_low = low_error_count(df, error, thresh)\n\n# Params\nerror = \"Error_KNN\"\nlr_low = low_error_count(df, error, thresh)\n\n# Params\nerror = \"Error_LR\"\nknn_low = low_error_count(df, error, thresh)\n\n# Params\nerror = \"Error_RF\"\nrf_low = low_error_count(df, error, thresh)\n\nprint('\\n--------------- High error ---------------------\\n')\n\nthresh = 25\n\n# Params\nerror = \"Error_XGB\"\nxgb_low = high_error_count(df, error, thresh)\n\n# Params\nerror = \"Error_KNN\"\nlr_low = high_error_count(df, error, thresh)\n\n# Params\nerror = \"Error_LR\"\nknn_low = high_error_count(df, error, thresh)\n\n# Params\nerror = \"Error_RF\"\nrf_low = high_error_count(df, error, thresh)\n\n\npep8(_ih)","ebcc06e8":"# Whole Train data\nX_train = df_train_04\ny_train = df_train_05['SalePrice']\n\nprint(X_train.shape)\n\n\npep8(_ih)","a22a6bc5":"# Robust Scaling\nstd_scale = preprocessing.RobustScaler().fit(X_train)\nX_train_Scaled = std_scale.transform(X_train)\n\nprint('X_train_Scaled :', X_train_Scaled.shape)\n\nprint('y_train :', y_train.shape)\n\n# Reshape y vectors\ny_train = y_train.values.reshape(-1, 1)\n\nprint('y_train reshaped :', y_train.shape)\n\n\npep8(_ih)","60337c3e":"kfold = KFold(n_splits=5, shuffle=True, random_state=100)\n\n\npep8(_ih)","9c764612":"param_XGB_Best = {'n_estimators': [1000],\n                  'colsample_bytree': [0.6],\n                  'max_depth': [5],\n                  'reg_alpha': [1.1],\n                  'reg_lambda': [1.5],\n                  'subsample': [0.5],\n                  'learning_rate': [0.01]}\n\nscoring_XGB_Best = 'neg_mean_absolute_error'  # 'r2'\n\nModel_XGB_Best = XGBRegressor()\n\nGS_XGB_Best = GridSearchCV(estimator=Model_XGB_Best,\n                           param_grid=param_XGB_Best,\n                           cv=kfold,\n                           n_jobs=-1,\n                           scoring=scoring_XGB_Best)\n\n\npep8(_ih)","d1d21f12":"# --------------------- FIT\nstart_time = timeit.default_timer()\n\n# ---------------------------------------------------\nfit_Model_XGB_Best = GS_XGB_Best.fit(X_train_Scaled, np.log(y_train))\n# ---------------------------------------------------\n\nTrain_Time_XGB_Best = timeit.default_timer() - start_time\nprint('Time :', Train_Time_XGB_Best, 's')\n# --------------------------------------------------------------------\n\n\npep8(_ih)","c53cc6e9":"# Robust Scaling\ntest = df_test_03.copy()\nstd_scale = preprocessing.RobustScaler().fit(test)\ntest_Scaled = std_scale.transform(test)\n\n\n# --------------------- PREDICT\nstart_time = timeit.default_timer()\n\n# -------------------------------------------------\ny_pred_XGB_Best = np.exp(fit_Model_XGB_Best.predict(test_Scaled))\n# -------------------------------------------------\n\nPred_Time_XGB_Best = timeit.default_timer() - start_time\nprint('Time :', Pred_Time_XGB_Best, 's')\n# --------------------------------------------------------------------\n\n\npep8(_ih)","1f7bc6ef":"df_test","d9ffb6b4":"test_id = df_test['Id']\n\nsubmission = pd.DataFrame({'Id': test_id,\n                           'SalePrice': y_pred_XGB_Best})\n\n# Save results\nsubmission.to_csv(\"submission.csv\", index=False)\n\n\npep8(_ih)","7f202fdc":"<a id=\"section-eight\"><\/a>\n# Performance tables","ba7d9bc4":"<a id=\"section-five\"><\/a>\n# Data Imputing","0c30a563":"<a id=\"section-one\"><\/a>\n# Cleaning","9cfbe2dc":"<a id=\"subsection-three\"><\/a>\n# KNN \nkfold Cross Validation + GridSearch for hyper-parameters","8034bebf":"At first, we have to add the target column to df_train_04 and check missing values","88960bcc":"<a id=\"subsection-twelve\"><\/a>\n# Predict on whole given Test set","3a854912":"<a id=\"subsection-one\"><\/a>\n# Baseline : multiple linear regression","186ad10d":"<a id=\"subsection-two\"><\/a>\n# XGBOOST \nkfold Cross Validation + GridSearch for hyper-parameters","856e8ab0":"<a id=\"subsection-ten\"><\/a>\n# Graphical results : predicted vs true values","24d8ed0c":"<a id=\"section-nine\"><\/a>\n# Save predictions","9e78a030":"Train and test have been split to retake initial form\n\nFollowing steps :\n- clean outliers inside df_train_02\n- keep them on purpose inside test set which is df_test_02","52ac136d":"<a id=\"subsection-eleven\"><\/a>\n# Best model on whole train set\n- Select best hyperparameters","fb27f83f":"<a id=\"subsection-thirteen\"><\/a>\n# Submission","fcc10c15":"<a id=\"section-seven\"><\/a>\n# Supervised Learning","218336d2":"We'd like to keep as much variability as possible, avoid deleting observations and avoid mean or median imputing\n\nFor these reasons, KNN imputer might be an interesting choice :\n- knn preserve the original data structure\n- sets plausible values\n- avoids distorsion of the imputed variable's distribution (1)\n\n(1) Beretta, L., Santaniello, A. Nearest neighbor imputation algorithms: a critical evaluation. BMC Med Inform Decis Mak 16, 74 (2016). https:\/\/doi.org\/10.1186\/s12911-016-0318-z","4eceb3af":"<a id=\"section-two\"><\/a>\n# Explore outliers and correlated features","63558cc3":"<a id=\"section-six\"><\/a>\n# Explore Target","27624904":"<a id=\"section-four\"><\/a>\n# Clean outliers","41ba642f":"Many hypothesis should be checked to grant reliability\n\nhttps:\/\/towardsdatascience.com\/assumptions-of-linear-regression-fdb71ebeaa8b","d1867b93":"I am getting an error value with catplot and yet the plot is okay.\n\nI've not found the issue since it's only happening on Kaggle.\n\nIf you have a solution, it'll be welcomed =)","7f74be07":"Method to check if programming is PEP8 friendly\n\nhttps:\/\/github.com\/nbpep8\/nbpep8","83954704":"Target is right skewed.\n\nWe can consider applying log function to the target to ease the learning process.\n\nThe result of applying log function will be to get nearer to normal distribution.","b8266210":"<a id=\"subsection-four\"><\/a>\n# Random Forest \nkfold Cross Validation + GridSearch for hyper-parameters","0b0c3d99":"Outliers can be removed agressively on some boxplots (1.5*IQR method)\n\nOn others, we might need to be more gentle to avoid losing to much data (remove top 1%)","cf894238":"<a id=\"section-three\"><\/a>\n# Un-concatenante train and test","9ace5a4d":"Explore variables which are the most correlated to 'SalePrice'"}}