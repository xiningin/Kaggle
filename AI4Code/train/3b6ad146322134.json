{"cell_type":{"4c7ddd1f":"code","75ac716d":"code","570caf36":"code","e545f835":"code","f1f0d6b0":"code","a4b5002e":"code","52267d94":"code","b44abdda":"code","b4d24f46":"code","74834528":"code","deea23e0":"code","9655b180":"code","daed3bc2":"code","1b9c9950":"code","479e7dd3":"code","7e69ab51":"code","ae810187":"code","55bb3886":"code","2c825495":"code","5cbd7249":"code","cb2045b8":"code","a48a6a41":"code","537c0b1a":"code","578c9be8":"code","49ae8d1f":"code","8957eec0":"code","674c4b5d":"code","b537857a":"code","cd2dc7dd":"code","2cde5a35":"code","e270d70d":"code","39574585":"code","a25ded55":"code","6a4ff7ac":"code","0a6957be":"code","751cd5e1":"code","ee1f3e4b":"code","b8411f7e":"code","4df7d7d0":"code","c026ad62":"code","44454a51":"code","c0b31715":"code","11e4ad11":"code","05a3dbe1":"code","1b44b886":"code","3edcbcca":"code","cc299454":"code","04968e32":"code","df75f1fe":"code","49377479":"code","3fe3e55b":"code","ec673810":"code","1898700a":"code","348fe205":"code","f676b4da":"code","216cd2ed":"code","154ccfda":"code","207c0a96":"code","ec725a2d":"code","ce241f66":"code","3f438ea3":"code","a7bf3a4e":"code","e33d27d0":"code","60b2b1fe":"code","e1b7bbd0":"code","e4d6f2a8":"code","caa5a281":"code","b57e4a6c":"code","0e22826c":"code","8000bbc3":"code","680f66ce":"code","8ca91215":"code","048243e9":"code","20bd87cd":"code","0660e001":"code","69523941":"code","b15ef0c2":"code","eaf6d497":"code","d8bd13e9":"code","54bf487e":"code","1bde1469":"code","b630c285":"code","a6ff5252":"code","7d08a167":"code","fb296018":"code","93300ab0":"code","ce7d5afb":"code","15b3331d":"code","59b62448":"code","43450ad8":"code","b1736543":"code","401c5e7a":"code","9714e24e":"code","9e9cecfa":"code","6da83fcd":"code","45a5903f":"code","b31b206e":"code","193d11a1":"code","416851af":"code","9346296e":"code","84b70489":"code","a655dfdd":"code","ea006136":"code","b2ebf437":"code","a333a183":"code","50a3f06a":"code","0871f777":"code","db546fd8":"code","fcd2de18":"code","8d2cb366":"code","a0f36a6c":"code","0be4af1b":"code","6f49336b":"code","f8785f47":"code","1dfe254b":"code","774a72ac":"code","f7b4aecf":"code","4f2a7e7f":"code","1fc6b903":"code","f77667c0":"code","3013dc45":"code","3ff4810f":"code","46c78f2e":"code","86b7a6e4":"code","face3ec2":"code","cc0e92a3":"code","7a322bc4":"code","66221b1e":"code","f8dbf6db":"code","513ab80b":"code","7c99a8f0":"code","b28175e6":"code","c6021ca6":"code","301b7f1a":"code","c21a5137":"code","d2439a88":"code","c5390d95":"code","95df2d4b":"code","9271faea":"code","b60c9aee":"code","e066857d":"code","31e599ef":"code","4998339b":"code","c32e3a4b":"code","eaef3c4b":"code","c00e32d2":"code","2362ef4a":"code","0ab31317":"code","068cd717":"code","14bebacd":"code","3f755b82":"code","0d273049":"code","180b0f4b":"code","16276c19":"code","022a3f8b":"code","87c19dc5":"code","cdc2711a":"code","77906259":"code","3e791d31":"code","cdc76728":"code","282ef080":"code","ffaf3119":"code","cbd8528f":"markdown","b77232db":"markdown","5fc45a5e":"markdown","c15a5ec5":"markdown","c63fab96":"markdown","0c080424":"markdown","95cfc01c":"markdown","5e303862":"markdown","1632649d":"markdown","3e6c9cb7":"markdown","000fac69":"markdown","0c38ee88":"markdown","3a27ba8e":"markdown","7eac8f21":"markdown","f0c2893e":"markdown","7cc35936":"markdown","d5fa7edc":"markdown","385ca6f4":"markdown","74d48f2a":"markdown","05ba83fa":"markdown","39c6d87d":"markdown","779f82f2":"markdown","0da86d2c":"markdown","0dd8363f":"markdown","e7fe9aca":"markdown","c0582007":"markdown","d7d0953a":"markdown","ffc72405":"markdown","c6add0cf":"markdown","38ac4f32":"markdown","bce962b8":"markdown","f0fc65e3":"markdown","b0e1d51c":"markdown","ef6aee51":"markdown","c4e296cb":"markdown","8307085d":"markdown","b2298f12":"markdown","9a7d232e":"markdown","39b3aae6":"markdown","9fb9dfcd":"markdown","7b516a2b":"markdown","8752e0bf":"markdown","a4252515":"markdown","4e3f44cf":"markdown","43354bf9":"markdown","8944ef9a":"markdown","fcc40eae":"markdown","70611dec":"markdown","fdc79236":"markdown","9c25f2d3":"markdown","4f2c2d28":"markdown","a6e652d3":"markdown","984bf83b":"markdown","92f4bf19":"markdown","f1aad543":"markdown","690b73ea":"markdown","bb722eb5":"markdown","eec46631":"markdown","7be6f355":"markdown","352c575c":"markdown","a3435a85":"markdown","b37f4225":"markdown","e235353c":"markdown","24a48f6c":"markdown","004a3789":"markdown","554357a2":"markdown","6a3145a5":"markdown","308d3b8c":"markdown","ffbb15e4":"markdown","0391cd07":"markdown","289b44e0":"markdown","bcb6e77e":"markdown","ede5eb50":"markdown","ce575871":"markdown","076a5548":"markdown","cbde95f1":"markdown","badb06a1":"markdown","8b5b2747":"markdown","486e574c":"markdown","b725dd3e":"markdown","dbf15208":"markdown","b69c6daf":"markdown","aebecf2e":"markdown","d99dd40e":"markdown","59de74fa":"markdown","d715c17c":"markdown","59f1a9cf":"markdown","eb71550e":"markdown","eef2c8aa":"markdown","e4e40203":"markdown","052ebdaa":"markdown","5dd5d716":"markdown","a639fcd6":"markdown","6dd2b848":"markdown","cf52a50e":"markdown","51aa7706":"markdown","fb0aa2a5":"markdown","8c281151":"markdown","a0a37e9c":"markdown","d734ad4d":"markdown","1f7c2943":"markdown","8997f927":"markdown","cee3ec11":"markdown","d37342ea":"markdown","f329a677":"markdown","af7cbee9":"markdown","90e60d45":"markdown","04677775":"markdown","0c327c3d":"markdown","65714d75":"markdown","9fbd3445":"markdown","b8fc47ab":"markdown","1a7684a7":"markdown","5fc279ac":"markdown","a504e72d":"markdown","ecdb6ce4":"markdown","7aee356e":"markdown","7e0b6cd6":"markdown","5418cffd":"markdown","657a0629":"markdown","56598398":"markdown","ec99fb71":"markdown","a0a5b555":"markdown","a5d9aced":"markdown","c7cfa15e":"markdown","ca7fef2b":"markdown","518fa170":"markdown","a21f2612":"markdown","2118dfcf":"markdown","9ad91167":"markdown","feea1787":"markdown"},"source":{"4c7ddd1f":"import datetime\nprint(\"Last updated:\")\nprint(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\"))","75ac716d":"import pandas as pd\nimport numpy as np\nimport sys\nimport os\nimport random\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nimport scipy\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.svm import SVC\nimport xgboost as xgb","570caf36":"IS_LOCAL = False\nif(IS_LOCAL):\n    PATH=\"..\/input\/titanic\/\"\nelse:\n    PATH=\"..\/input\/\"\nos.listdir(PATH)","e545f835":"train_df=pd.read_csv(PATH+'train.csv')\ntest_df=pd.read_csv(PATH+'test.csv')","f1f0d6b0":"train_df.sample(5).head()","a4b5002e":"test_df.sample(5).head()","52267d94":"print(\"Train: rows:{} cols:{}\".format(train_df.shape[0], train_df.shape[1]))\nprint(\"Test:  rows:{} cols:{}\".format(test_df.shape[0], test_df.shape[1]))","b44abdda":"def missing_data(data):\n    total = data.isnull().sum().sort_values(ascending = False)\n    percent = (data.isnull().sum()\/data.isnull().count()*100).sort_values(ascending = False)\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data(train_df)","b4d24f46":"missing_data(test_df)","74834528":"def get_categories(data, val):\n    tmp = data[val].value_counts()\n    return pd.DataFrame(data={'Number': tmp.values}, index=tmp.index).reset_index()","deea23e0":"def get_survived_categories(data, val):\n    tmp = data.groupby('Survived')[val].value_counts()\n    return pd.DataFrame(data={'Number': tmp.values}, index=tmp.index).reset_index()","9655b180":"def draw_trace_bar(data_df,color='Blue'):\n    trace = go.Bar(\n            x = data_df['index'],\n            y = data_df['Number'],\n            marker=dict(color=color),\n            text=data_df['index']\n        )\n    return trace\n\n\ndef plot_bar(data_df, title, xlab, ylab,color='Blue'):\n    trace = draw_trace_bar(data_df, color)\n    data = [trace]\n    layout = dict(title = title,\n              xaxis = dict(title = xlab, showticklabels=True, tickangle=0,\n                          tickfont=dict(\n                            size=10,\n                            color='black'),), \n              yaxis = dict(title = ylab),\n              hovermode = 'closest'\n             )\n    fig = dict(data = data, layout = layout)\n    iplot(fig, filename='draw_trace')\n","daed3bc2":"def plot_two_bar(data_df1, data_df2, title1, title2, xlab, ylab):\n    trace1 = draw_trace_bar(data_df1, color='Blue')\n    trace2 = draw_trace_bar(data_df2, color='Lightblue')\n    \n    fig = tools.make_subplots(rows=1,cols=2, subplot_titles=(title1,title2))\n    fig.append_trace(trace1,1,1)\n    fig.append_trace(trace2,1,2)\n    \n    fig['layout']['xaxis'].update(title = xlab)\n    fig['layout']['xaxis2'].update(title = xlab)\n    fig['layout']['yaxis'].update(title = ylab)\n    fig['layout']['yaxis2'].update(title = ylab)\n    fig['layout'].update(showlegend=False)\n    \n\n    iplot(fig, filename='draw_trace')","1b9c9950":"def plot_survived_bar(data_df, var, ytitle= 'Number of passengers',title= 'Number of survived and not survived passengers by {}'):\n    dfS = data_df[data_df['Survived']==1]\n    dfN = data_df[data_df['Survived']==0]\n\n    traceS = go.Bar(\n        x = dfS[var],y = dfS['Number'],\n        name='Survived',\n        marker=dict(color=\"Blue\"),\n        text=dfS['Number']\n    )\n    traceN = go.Bar(\n        x = dfN[var],y = dfN['Number'],\n        name='Not survived',\n        marker=dict(color=\"Red\"),\n        text=dfS['Number']\n    )\n    \n    data = [traceS, traceN]\n    layout = dict(title = title.format(var),\n          xaxis = dict(title = var, showticklabels=True), \n          yaxis = dict(title = ytitle),\n          hovermode = 'closest'\n    )\n    fig = dict(data=data, layout=layout)\n   \n    iplot(fig, filename='draw_trace')\n","479e7dd3":"plot_two_bar(get_categories(train_df,'Sex'), get_categories(test_df,'Sex'), \n             'Train data', 'Test data',\n             'Sex', 'Number of passengers')","7e69ab51":"plot_survived_bar(get_survived_categories(train_df,'Sex'), 'Sex')","ae810187":"plot_two_bar(get_categories(train_df,'Age'), get_categories(test_df,'Age'), \n             'Train data', 'Test data',\n             'Age', 'Number of passengers')","55bb3886":"plot_survived_bar(get_survived_categories(train_df,'Age'), 'Age')","2c825495":"plot_two_bar(get_categories(train_df,'SibSp'), get_categories(test_df,'SibSp'), \n             'Train data', 'Test data',\n             'SibSp', 'Number of passengers')","5cbd7249":"plot_survived_bar(get_survived_categories(train_df,'SibSp'), 'SibSp')","cb2045b8":"plot_two_bar(get_categories(train_df,'Parch'), get_categories(test_df,'Parch'), \n             'Train data', 'Test data',\n             'Parch', 'Number of passengers')","a48a6a41":"plot_survived_bar(get_survived_categories(train_df,'Parch'), 'Parch')","537c0b1a":"def draw_trace_histogram(data_df,color='Blue'):\n    trace = go.Histogram(\n            x = data_df['index'],\n            y = data_df['Number'],\n            marker=dict(color=color),\n            text=data_df['index']\n        )\n    return trace\n\ndef plot_two_histogram(data_df1, data_df2, title1, title2, xlab, ylab):\n    trace1 = draw_trace_histogram(data_df1, color='Blue')\n    trace2 = draw_trace_histogram(data_df2, color='Lightblue')\n    \n    fig = tools.make_subplots(rows=1,cols=2, subplot_titles=(title1,title2))\n    fig.append_trace(trace1,1,1)\n    fig.append_trace(trace2,1,2)\n    \n    fig['layout']['xaxis'].update(title = xlab)\n    fig['layout']['xaxis2'].update(title = xlab)\n    fig['layout']['yaxis'].update(title = ylab)\n    fig['layout']['yaxis2'].update(title = ylab)\n    fig['layout'].update(showlegend=False)\n    \n\n    iplot(fig, filename='draw_trace')","578c9be8":"def plot_survived_histogram(data_df, var):\n    dfS = data_df[data_df['Survived']==1]\n    dfN = data_df[data_df['Survived']==0]\n\n    traceS = go.Histogram(\n        x = dfS[var],y = dfS['Number'],\n        name='Survived',\n        marker=dict(color=\"Blue\"),\n        text=dfS['Number']\n    )\n    traceN = go.Histogram(\n        x = dfN[var],y = dfN['Number'],\n        name='Not survived',\n        marker=dict(color=\"Red\"),\n        text=dfS['Number']\n    )\n    \n    data = [traceS, traceN]\n    layout = dict(title = 'Number of survived and not survived passengers by {}'.format(var),\n          xaxis = dict(title = var, showticklabels=True), \n          yaxis = dict(title = 'Number of passengers'),\n          hovermode = 'closest'\n    )\n    fig = dict(data=data, layout=layout)\n   \n    iplot(fig, filename='draw_trace')","49ae8d1f":"plot_two_histogram(get_categories(train_df,'Fare'), get_categories(test_df,'Fare'), \n             'Train data', 'Test data',\n             'Fare', 'Passengers')","8957eec0":"plot_survived_histogram(get_survived_categories(train_df,'Fare'), 'Fare')","674c4b5d":"plot_two_bar(get_categories(train_df,'Embarked'), get_categories(test_df,'Embarked'), \n             'Train data', 'Test data',\n             'Embarked', 'Number of passengers')","b537857a":"plot_survived_bar(get_survived_categories(train_df,'Embarked'), 'Embarked')","cd2dc7dd":"plot_two_bar(get_categories(train_df,'Pclass'), get_categories(test_df,'Pclass'), \n             'Train data', 'Test data',\n             'Pclass', 'Number of passengers')","2cde5a35":"plot_survived_bar(get_survived_categories(train_df,'Pclass'), 'Pclass')","e270d70d":"train_df['Ticket'].value_counts().head(10)","39574585":"train_df['Cabin'].value_counts().head(10)","a25ded55":"train_df['Name'].head(10)","6a4ff7ac":"tmp = train_df.groupby(['Pclass', 'Sex'])['Survived'].value_counts()\ndf = pd.DataFrame(data={'Passengers': tmp.values}, index=tmp.index).reset_index()\nhover_text = []\nfor index, row in df.iterrows():\n    hover_text.append(('Pclass: {}<br>'+\n                      'Sex: {}<br>'+\n                      'Survived: {}<br>'+\n                      'Passengers: {}').format(row['Pclass'],\n                                            row['Sex'],\n                                            row['Survived'],\n                                            row['Passengers']))\ndf['hover_text'] = hover_text","0a6957be":"trace = go.Scatter(\n        x=df['Pclass'],\n        y=df['Sex'],\n        text=df['hover_text'],\n        mode='markers',\n        marker=dict(\n            sizemode='diameter',\n            sizeref=3,\n            size=df['Passengers'],\n            color = df['Survived'],\n            colorscale = 'Bluered',\n        )\n    )\ndata = [trace]\n\nlayout = dict(title = 'Number of surviving\/not surviving passengers by class and sex',\n          xaxis = dict(title = 'Class', showticklabels=True, type='category'), \n          yaxis = dict(title = 'Sex', type='category'),            \n          hovermode = 'closest',\n              height=400, width=600, \n         )\nfig=go.Figure(data=data, layout=layout)\niplot(fig, filename='bubble_plot')","751cd5e1":"tmp = train_df.groupby(['SibSp', 'Parch'])['Survived'].value_counts()\ndf = pd.DataFrame(data={'Passengers': tmp.values}, index=tmp.index).reset_index()\nhover_text = []\nfor index, row in df.iterrows():\n    hover_text.append(('Sibilings: {}<br>'+\n                      'Parents\/Children: {}<br>'+\n                      'Survived: {}<br>'+\n                      'Passengers: {}').format(row['SibSp'],\n                                            row['Parch'],\n                                            row['Survived'],\n                                            row['Passengers']))\ndf['hover_text'] = hover_text","ee1f3e4b":"trace = go.Scatter(\n        x=df['SibSp'],\n        y=df['Parch'],\n        text=df['hover_text'],\n        mode='markers',\n        marker=dict(\n            sizemode='diameter',\n            sizeref=4,\n            size=df['Passengers'],\n            color = df['Survived'],\n            colorscale = 'Bluered',\n        )\n    )\ndata = [trace]\n\nlayout = dict(title = 'Passengers by number of Sibilings and  parents\/children',\n          xaxis = dict(title = 'Sibilings', showticklabels=True, type='category'), \n          yaxis = dict(title = 'Parents\/Children', type='category'),            \n          hovermode = 'closest',\n              height=400, width=600, \n         )\nfig=go.Figure(data=data, layout=layout)\niplot(fig, filename='bubble_plot')","b8411f7e":"test_df['Survived'] = None\nall_df = pd.concat([train_df, test_df], axis=0)","4df7d7d0":"print(f'Combined data: {all_df.shape}')","c026ad62":"all_df.head()","44454a51":"from sklearn.preprocessing import LabelEncoder\ndef encrypt_single_column(data):\n    le = LabelEncoder()\n    le.fit(data.astype(str))\n    return le.transform(data.astype(str))","c0b31715":"features = ['Pclass','Sex','Embarked','SibSp','Parch']\n\nfor feature in features:\n    all_df[feature] = encrypt_single_column(all_df[feature])","11e4ad11":"X = all_df.loc[~(all_df.Fare.isna())]\ny = X['Fare'].values\nX = X[features]\nX_test = all_df.loc[all_df.Fare.isna()]    \nX_test = X_test[features]\nprint(f'X: {X.shape} y: {y.shape}, X_text: {X_test.shape}')","05a3dbe1":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn import tree\nclf = DecisionTreeRegressor()\nclf.fit(X, y)\ny_test = clf.predict(X_test)","1b44b886":"print(f'Fare: {y_test}')","3edcbcca":"all_df.loc[all_df.Fare.isna(), 'Fare'] = y_test","cc299454":"all_df.loc[all_df.Fare.isna()].shape","04968e32":"all_df = [train_df, test_df]","df75f1fe":"for dataset in all_df:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)","49377479":"np.transpose(pd.crosstab(train_df['Title'], train_df['Sex']))","3fe3e55b":"for dataset in all_df:\n    #unify `Miss`\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    #unify `Mrs`\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')","ec673810":"train_df[(train_df['Title'] == 'Dr') & (train_df['Sex'] == 'female')]","1898700a":"train_df[train_df['Cabin']=='D17']","348fe205":"train_df.loc[train_df.PassengerId == 797, 'Title'] = 'Mrs'","f676b4da":"train_df[train_df['Cabin']=='D17']","216cd2ed":"for dataset in all_df:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n     'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')","154ccfda":"train_df[['Title', 'Sex', 'Survived']].groupby(['Title', 'Sex'], as_index=False).mean()","207c0a96":"plot_survived_bar(get_survived_categories(train_df,'Title'), 'Title')","ec725a2d":"for dataset in all_df:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1","ce241f66":"train_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean()","3f438ea3":"plot_survived_bar(get_survived_categories(train_df,'FamilySize'), 'FamilySize')","a7bf3a4e":"for dataset in all_df:\n    dataset['Surname'] = dataset.Name.str.extract('([A-Za-z]+)\\,', expand=False)","e33d27d0":"tmp = train_df.groupby(['Surname'])['Survived'].value_counts()\ndf = pd.DataFrame(data={'Size of group with same Surname': tmp.values}, index=tmp.index).reset_index().sort_values(['Size of group with same Surname', 'Surname'], ascending=False)","60b2b1fe":"tmp = df.groupby(['Size of group with same Surname'])['Survived'].value_counts()\ndf = pd.DataFrame(data={'Number': tmp.values}, index=tmp.index).reset_index().sort_values(['Size of group with same Surname', 'Survived'], ascending=False)\ndf","e1b7bbd0":"plot_survived_bar(df, 'Size of group with same Surname', ytitle= 'Number of groups', title= 'Number of survived and not survived groups by {}')","e4d6f2a8":"for dataset in all_df:\n    dataset['Deck'] = dataset.Cabin.str.extract('^([A-Za-z]+)', expand=False)","caa5a281":"train_df[['Deck', 'Survived']].groupby(['Deck'], as_index=False).mean()","b57e4a6c":"plot_survived_bar(get_survived_categories(train_df,'Deck'), 'Deck')","0e22826c":"for dataset in all_df:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)","8000bbc3":"age_aprox = np.zeros((2,3))\nfor dataset in all_df:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            aprox_age = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n            age_aprox[i,j] = aprox_age.median()\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),'Age'] = \\\n                    age_aprox[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)","680f66ce":"tmp = train_df.groupby(['Title', 'Pclass'])['Survived'].value_counts()\ndf = pd.DataFrame(data={'Passengers': tmp.values}, index=tmp.index).reset_index()\ndf","8ca91215":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in all_df:\n    dataset['Title'] = dataset['Title'].map(title_mapping)   ","048243e9":"plot_survived_bar(get_survived_categories(train_df,'Title'), 'Title')","20bd87cd":"for dataset in all_df:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3","0660e001":"plot_survived_bar(get_survived_categories(train_df,'Fare'), 'Fare')","69523941":"for dataset in all_df:\n    dataset.loc[ dataset['Age'] <= 16, 'Age']  = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4","b15ef0c2":"plot_survived_bar(get_survived_categories(train_df,'Age'), 'Age')","eaf6d497":"for dataset in all_df:\n    dataset.loc[ dataset['FamilySize'] <= 1, 'FamilySize'] = 0\n    dataset.loc[(dataset['FamilySize'] > 1) & (dataset['FamilySize'] <= 4), 'FamilySize'] = 1\n    dataset.loc[ dataset['FamilySize'] > 4, 'FamilySize'] = 2","d8bd13e9":"for dataset in all_df:\n    dataset['Class*Age'] = dataset['Pclass'] * dataset['Age']","54bf487e":"plot_survived_bar(get_survived_categories(train_df,'Class*Age'), 'Class*Age')","1bde1469":"train_df.head(3)","b630c285":"test_df.head(3)","a6ff5252":"#We are using 80-20 split for train-test\nVALID_SIZE = 0.2\n#We also use random state for reproducibility\nRANDOM_STATE = 2018\n\ntrain, valid = train_test_split(train_df, test_size=VALID_SIZE, random_state=RANDOM_STATE, shuffle=True )","7d08a167":"predictors = ['Sex', 'Age']\ntarget = 'Survived'","fb296018":"train_X = train[predictors]\ntrain_Y = train[target].values\nvalid_X = valid[predictors]\nvalid_Y = valid[target].values","93300ab0":"RFC_METRIC = 'gini'  #metric used for RandomForrestClassifier\nNUM_ESTIMATORS = 100 #number of estimators used for RandomForrestClassifier\nNO_JOBS = 4 #number of parallel jobs used for RandomForrestClassifier","ce7d5afb":"clf = RandomForestClassifier(n_jobs=NO_JOBS, \n                             random_state=RANDOM_STATE,\n                             criterion=RFC_METRIC,\n                             n_estimators=NUM_ESTIMATORS,\n                             verbose=False)","15b3331d":"clf.fit(train_X, train_Y)","59b62448":"preds = clf.predict(valid_X)","43450ad8":"def plot_feature_importance():\n    tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': clf.feature_importances_})\n    tmp = tmp.sort_values(by='Feature importance',ascending=False)\n    plt.figure(figsize = (7,4))\n    plt.title('Features importance',fontsize=14)\n    s = sns.barplot(x='Feature',y='Feature importance',data=tmp)\n    s.set_xticklabels(s.get_xticklabels(),rotation=90)\n    plt.show()   ","b1736543":"plot_feature_importance()","401c5e7a":"clf.score(train_X, train_Y)\nacc = round(clf.score(train_X, train_Y) * 100, 2)\nprint(\"RandomForest accuracy (train set):\", acc)","9714e24e":"clf.score(valid_X, valid_Y)\nacc = round(clf.score(valid_X, valid_Y) * 100, 2)\nprint(\"RandomForest accuracy (validation set):\", acc)","9e9cecfa":"print(metrics.classification_report(valid_Y, preds, target_names=['Not Survived', 'Survived']))","6da83fcd":"def plot_confusion_matrix():\n    cm = pd.crosstab(valid_Y, preds, rownames=['Actual'], colnames=['Predicted'])\n    fig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\n    sns.heatmap(cm, \n                xticklabels=['Not Survived', 'Survived'],\n                yticklabels=['Not Survived', 'Survived'],\n                annot=True,ax=ax1,\n                linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\n    plt.title('Confusion Matrix', fontsize=14)\n    plt.show()","45a5903f":"plot_confusion_matrix()","b31b206e":"predictors = ['Sex', 'Age', 'Pclass', 'Fare']\ntarget = 'Survived'","193d11a1":"train_X = train[predictors]\ntrain_Y = train[target].values\nvalid_X = valid[predictors]\nvalid_Y = valid[target].values","416851af":"clf.fit(train_X, train_Y)","9346296e":"preds = clf.predict(valid_X)","84b70489":"plot_feature_importance()","a655dfdd":"clf.score(train_X, train_Y)\nacc = round(clf.score(train_X, train_Y) * 100, 2)\nprint(\"RandomForest accuracy (train set):\", acc)","ea006136":"clf.score(valid_X, valid_Y)\nacc = round(clf.score(valid_X, valid_Y) * 100, 2)\nprint(\"RandomForest accuracy (validation set):\", acc)","b2ebf437":"print(metrics.classification_report(valid_Y, preds, target_names=['Not Survived', 'Survived']))","a333a183":"plot_confusion_matrix()","50a3f06a":"predictors = ['Sex', 'Age', 'Pclass', 'Fare', 'Parch', 'SibSp']\ntarget = 'Survived'","0871f777":"train_X = train[predictors]\ntrain_Y = train[target].values\nvalid_X = valid[predictors]\nvalid_Y = valid[target].values","db546fd8":"clf.fit(train_X, train_Y)","fcd2de18":"preds = clf.predict(valid_X)","8d2cb366":"plot_feature_importance()","a0f36a6c":"clf.score(train_X, train_Y)\nacc = round(clf.score(train_X, train_Y) * 100, 2)\nprint(\"RandomForest accuracy (train set):\", acc)","0be4af1b":"clf.score(valid_X, valid_Y)\nacc = round(clf.score(valid_X, valid_Y) * 100, 2)\nprint(\"RandomForest accuracy (validation set):\", acc)","6f49336b":"print(metrics.classification_report(valid_Y, preds, target_names=['Not Survived', 'Survived']))","f8785f47":"plot_confusion_matrix()","1dfe254b":"predictors = ['Sex', 'Age', 'Pclass', 'Fare', 'Parch', 'SibSp', 'FamilySize', 'Title']\ntarget = 'Survived'","774a72ac":"train_X = train[predictors]\ntrain_Y = train[target].values\nvalid_X = valid[predictors]\nvalid_Y = valid[target].values","f7b4aecf":"clf.fit(train_X, train_Y)","4f2a7e7f":"preds = clf.predict(valid_X)","1fc6b903":"plot_feature_importance()","f77667c0":"clf.score(train_X, train_Y)\nacc = round(clf.score(train_X, train_Y) * 100, 2)\nprint(\"RandomForest accuracy (train set):\", acc)","3013dc45":"clf.score(valid_X, valid_Y)\nacc = round(clf.score(valid_X, valid_Y) * 100, 2)\nprint(\"RandomForest accuracy (validation set):\", acc)","3ff4810f":"print(metrics.classification_report(valid_Y, preds, target_names=['Not Survived', 'Survived']))","46c78f2e":"plot_confusion_matrix()","86b7a6e4":"predictors = ['FamilySize', 'Title', 'Class*Age']\ntarget = 'Survived'","face3ec2":"train_X = train[predictors]\ntrain_Y = train[target].values\nvalid_X = valid[predictors]\nvalid_Y = valid[target].values","cc0e92a3":"rf_clf = clf.fit(train_X, train_Y)","7a322bc4":"rf_clf","66221b1e":"preds = clf.predict(valid_X)","f8dbf6db":"plot_feature_importance()","513ab80b":"clf.score(train_X, train_Y)\nacc = round(clf.score(train_X, train_Y) * 100, 2)\nprint(\"RandomForest accuracy (train set):\", acc)","7c99a8f0":"clf.score(valid_X, valid_Y)\nacc = round(clf.score(valid_X, valid_Y) * 100, 2)\nprint(\"RandomForest accuracy (validation set):\", acc)","b28175e6":"print(metrics.classification_report(valid_Y, preds, target_names=['Not Survived', 'Survived']))","c6021ca6":"plot_confusion_matrix()","301b7f1a":"test_X = test_df[predictors]\npred_Y = clf.predict(test_X)","c21a5137":"submission = pd.DataFrame({\"PassengerId\": test_df[\"PassengerId\"],\"Survived\": pred_Y})\nsubmission.to_csv('submission.csv', index=False)","d2439a88":"rf_clf = clf.fit(train_X, train_Y)","c5390d95":"parameters = {\n    'n_estimators': (50, 75,100),\n    'max_features': ('auto', 'sqrt'),\n    'max_depth': (3,4,5),\n    'min_samples_split': (2,5,10),\n    'min_samples_leaf': (1,2,3)\n}","95df2d4b":"%%time\ngs_clf = GridSearchCV(rf_clf, parameters, n_jobs=-1, cv = 5, verbose = 5)\ngs_clf = gs_clf.fit(train_X, train_Y)","9271faea":"print('Best scores:',gs_clf.best_score_)\nprint('Best params:',gs_clf.best_params_)","b60c9aee":"preds = gs_clf.predict(valid_X)","e066857d":"clf.score(valid_X, valid_Y)\nacc = round(clf.score(valid_X, valid_Y) * 100, 2)\nprint(\"RandomForest accuracy (validation set):\", acc)","31e599ef":"print(metrics.classification_report(valid_Y, preds, target_names=['Not Survived', 'Survived']))","4998339b":"test_X = test_df[predictors]\npred_Y = gs_clf.predict(test_X)","c32e3a4b":"submission = pd.DataFrame({\"PassengerId\": test_df[\"PassengerId\"],\"Survived\": pred_Y})\nsubmission.to_csv('submission_hyperparam_optimization.csv', index=False)","eaef3c4b":"NUMBER_KFOLDS = 5\nkf = KFold(n_splits = NUMBER_KFOLDS, random_state = RANDOM_STATE, shuffle = True)","c00e32d2":"# Class to extend the Sklearn classifier\nclass SklearnBasicClassifier(object):\n    def __init__(self, clf, seed=2018, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def feature_importances(self,x,y):\n        print(self.clf.fit(x,y).feature_importances_)\n\n    def get_feature_importances(self,x,y):\n        return (self.clf.fit(x,y).feature_importances_)\n","2362ef4a":"ntrain = train_df.shape[0]\nntest = test_df.shape[0]\ndef get_oof_predictions(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NUMBER_KFOLDS, ntest))\n    \n    for i, (train_idx, valid_idx) in enumerate(kf.split(train_df)):\n        clf.train(x_train[train_idx], y_train[train_idx])\n        oof_train[valid_idx] = clf.predict(x_train[valid_idx])\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","0ab31317":"# AdaBoost parameters\nada_params = {\n    'n_estimators': 200,\n    'learning_rate' : 0.75\n}\n# CatBoost parameters\ncat_params = {\n    'iterations': 150,\n    'learning_rate': 0.02,\n    'depth': 12,\n    'bagging_temperature':0.2,\n    'od_type':'Iter',\n    'metric_period':400,\n}  \n# Extra Trees Parameters\next_params = {\n    'n_jobs': -1,\n    'n_estimators':100,\n    'max_depth': 8,\n    'min_samples_leaf': 3,\n    'verbose': 0\n}\n# Gradient Boosting parameters\ngbm_params = {\n    'n_estimators': 200,\n    'max_depth': 5,\n    'min_samples_leaf': 3,\n    'verbose': 0\n}\n# Random Forest parameters\nrfo_params = {\n    'n_jobs': -1,\n    'n_estimators': 50,\n    'max_depth': 5,\n    'min_samples_leaf': 5,\n    'max_features' : 'auto',\n    'verbose': 0\n}\n# Support Vector Classifier parameters \nsvc_params = {\n    'kernel' : 'linear',\n    'C' : 0.02\n}","068cd717":"# Create 6 objects that represent our 6 models\nada = SklearnBasicClassifier(clf=AdaBoostClassifier, seed=RANDOM_STATE, params=ada_params)\ncat = SklearnBasicClassifier(clf=CatBoostClassifier, seed=RANDOM_STATE, params=cat_params)\next = SklearnBasicClassifier(clf=ExtraTreesClassifier, seed=RANDOM_STATE, params=ext_params)\ngbm = SklearnBasicClassifier(clf=GradientBoostingClassifier, seed=RANDOM_STATE, params=gbm_params)\nrfo = SklearnBasicClassifier(clf=RandomForestClassifier, seed=RANDOM_STATE, params=rfo_params)\nsvc = SklearnBasicClassifier(clf=SVC, seed=RANDOM_STATE, params=svc_params)","14bebacd":"predictors = ['FamilySize', 'Title', 'Class*Age']\ntarget = 'Survived'","3f755b82":"y_train = train_df['Survived'].values\ntrain = train_df[predictors]\ntest = test_df[predictors]\nx_train = train.values\nx_test = test.values","0d273049":"print(\"Start training\")\nada_oof_train, ada_oof_test = get_oof_predictions(ada, x_train, y_train, x_test) # AdaBoost Classifier\nprint(\"End AdaBoost\")\ncat_oof_train, cat_oof_test = get_oof_predictions(cat, x_train, y_train, x_test) # CatBoost Classifier\nprint(\"End CatBoost\")\next_oof_train, ext_oof_test = get_oof_predictions(ext, x_train, y_train, x_test) # Extra Trees \nprint(\"End ExtraTrees\")\nrfo_oof_train, rfo_oof_test = get_oof_predictions(rfo,x_train, y_train, x_test) # Random Forest Classifier\nprint(\"End RandomForest\")\ngbm_oof_train, gbm_oof_test = get_oof_predictions(gbm,x_train, y_train, x_test) # Gradient Boost Classifier\nprint(\"End GradientBoost\")\nsvc_oof_train, svc_oof_test = get_oof_predictions(svc,x_train, y_train, x_test) # Support Vector Classifier\nprint(\"End training\")","180b0f4b":"ada_feature_importance = ada.get_feature_importances(x_train,y_train)\ncat_feature_importance = cat.get_feature_importances(x_train,y_train)\next_feature_importance = ext.get_feature_importances(x_train,y_train)\ngbm_feature_importance = gbm.get_feature_importances(x_train,y_train)\nrfo_feature_importance = rfo.get_feature_importances(x_train,y_train)","16276c19":"def plot_feature_importance(feature_importance, classifier):\n    tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': feature_importance[0:len(predictors)]})\n    tmp = tmp.sort_values(by='Feature importance',ascending=False)\n    plt.figure(figsize = (7,4))\n    plt.title('Features importance {}'.format(classifier),fontsize=14)\n    s = sns.barplot(x='Feature',y='Feature importance',data=tmp)\n    s.set_xticklabels(s.get_xticklabels(),rotation=90)\n    plt.show()   ","022a3f8b":"plot_feature_importance(ada_feature_importance, '- AdaBoost')\nplot_feature_importance(cat_feature_importance, '- CatBoost')\nplot_feature_importance(ext_feature_importance, '- ExtraTrees')\nplot_feature_importance(gbm_feature_importance, '- GradientBoosting')\nplot_feature_importance(rfo_feature_importance, '- RandomForest')","87c19dc5":"base_predictions_train = pd.DataFrame( {\n     'AdaBoost': ada_oof_train.ravel(),\n     'CatBoost': cat_oof_train.ravel(),\n     'ExtraTrees': ext_oof_train.ravel(),\n     'GradientBoost': gbm_oof_train.ravel(),\n     'RandomForest': rfo_oof_train.ravel(),\n     'SVM': svc_oof_train.ravel()\n    })\nbase_predictions_train.head(10)","cdc2711a":"trace = go.Heatmap(\n        z= base_predictions_train.astype(float).corr().values ,\n        x=base_predictions_train.columns.values,\n        y= base_predictions_train.columns.values,\n          colorscale='Rainbow',\n            showscale=True,\n            reversescale = False\n    )\ndata = [trace]\nlayout = dict(width = 600, height=600)\nfig = dict(data=data, layout=layout)\niplot(fig, filename='heatmap')","77906259":"x_train = np.concatenate(( ada_oof_train, cat_oof_train, ext_oof_train, gbm_oof_train, rfo_oof_train, svc_oof_train), axis=1)\nx_test = np.concatenate(( ada_oof_test, cat_oof_test, ext_oof_test, gbm_oof_test, rfo_oof_test, svc_oof_test), axis=1)","3e791d31":"clf = xgb.XGBClassifier(\n learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1)","cdc76728":"xgbm = clf.fit(x_train, y_train)","282ef080":"predictions = xgbm.predict(x_test)","ffaf3119":"submissionStacking = pd.DataFrame({ 'PassengerId': test_df[\"PassengerId\"],'Survived': predictions })\nsubmissionStacking.to_csv(\"submission_ensamble.csv\", index=False)","cbd8528f":"The accuracy for the train data classification improved further but the accuracy and precision for validation did not improved.    \n\nLet's add few more engineering features.\n\n\n### Model with {Sex, Age, Pclass, Fare, Parch, SibSp, FamilySize, Title} features","b77232db":"Let's plot also the feature importance.","5fc45a5e":"## <a id='32'>Sex, Age, SibSp, Parch<\/a>  \n\nLet's check now the data (in **train** and **test**) for `Sex`, `Age`, `SibSp` and `Parch`.","c15a5ec5":"## <a id='62'>Create the Out-of-Fold Predictions<\/a>\n\nLet's now define out-of-folds predictions. If we would train the base models on the full training data and generate predictions on the full test set and then output these for the second-level training we might go into trouble. The risk here is that the base model predictions would have seen the test set and thus overfitting when feeding those predictions.","c63fab96":"## <a id='35'>Multiple features visualization<\/a>\n\nLet's show the number of survived\/not survived passengers grouped by Class and Sex.","0c080424":"`Ticket` has, most probably, little predictive value.  \n\nLet's see also the `Cabin`.\n","95cfc01c":"## <a id='56'>Submission (model with hyperparameters optimization)<\/a>\n\n\nFirst, we predict for test data using the trained model.\n","5e303862":"The best survival rate is for passengers embarked in Cherbourg (more than 50%), the worst for passengers embarked in Southampton.","1632649d":"The result means that the total number of correct predictions divided by the total number of examples in the training set is around 0.77. Because we have a binary classification, this means:\n\n$$Accuracy =\\frac{ \\textrm{True Positives} + \\textrm{True Negatives}}{\\textrm{Number of Examples}}$$ \n\n\nThen we evaluate the accuracy for the validation set. ","3e6c9cb7":"Let's plot the parameters for the classifier.","000fac69":"Then we prepare the submission dataset and save it to the submission file.","0c38ee88":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  \n\n\n# <a id='4'>Features engineering<\/a>\n\nFrom the original features, we will create new features.","3a27ba8e":"There are **train** and **test** data as well as an example **submission** file.  \n\nLet's load the **train** and **test** data.","7eac8f21":"\nLet's show the number of survived\/not survived passengers grouped by SibSp and Parch.","f0c2893e":"  All rare female titles were saved.   \n  Married females had the highest survival rate besides these very rare cases, of 79%. Young unmarried women followed with 70% survival rate. \n  Lowest survival rate had the men with `Mr` title. \n  Between men, the ones with `Master` title had a much higher survival rate than  these, with 57%.","7cc35936":"We will start with a simple model, with just few predictors.\n\nLet's set now the predictors list and the target value. We start with two predictors, the `Sex` and `Pclass`.","d5fa7edc":"We replace the predicted value in the original (combined) data.","385ca6f4":"# <a id='2'>Prepare the data analysis<\/a>   \n\n\nBefore starting the analysis, we need to make few preparation: load the packages, load and inspect the data.\n","74d48f2a":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  \n\n\n## <a id='34'>Ticket, Cabin, Name<\/a>  \n\nLet's check now the data (in **train** and **test**) for  `Ticket`,  `Cabin` and`Name`.\n\nAll these are alphanumeric (contains both letters and numbers), like `Ticket` and `Cabin` or are text fields (`Name`). \n\nWe will have to process them in order to use as features.  \n\nLet's look to `Ticket` first.\n\n","05ba83fa":"And let's plot the `Fare` clusters grouped by the survived and not survived.","39c6d87d":"Let's initialize the GradientSearchCV parameters. We will set only few parameters, as following:\n\n* **n_estimators**: number of trees in the foreset;  \n* **max_features**: max number of features considered for splitting a node;  \n* **max_depth**: max number of levels in each decision tree;  \n* **min_samples_split**: min number of data points placed in a node before the node is split;  \n* **min_samples_leaf**: min number of data points allowed in a leaf node.\n","779f82f2":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  \n\n## <a id='54'>Submission<\/a>\n\n\nFirst, we predict for test data using the trained model.","0da86d2c":"We see that in the `Name` field we have the Family name, the title (which might indicate as well the social status or marital status), and the first name. So, `Name` is actually a quite rich feature column, which can be further exploited (and we will exploit in the next sections). ","0dd8363f":"With the fitted second level model we do the prediction for the test data.","e7fe9aca":"## <a id='21'>Load packages<\/a>\n\nWe load the packages used for the analysis. There are packages for data manipulation, visualization, models, hyperparameter optimization and model metrics..","c0582007":"Because she is traveling with a friend about the same age and with a `Mrs` title, we might want to set her as well as a `Mrs`. Let's do it.","d7d0953a":"And let's plot the `Title` grouped by the survived and not survived.","ffc72405":"The accuracy for the validation is much better than the accuracy for the training set. \nThis means we have higher bias than variation. The model does not learn too well the training set, we are not overfitting (yet). The validation is better, i.e. we are generalizing well. Before improving the variation, we will try to improve the bias, while looking how this affects the variation as well.\n\nLet's plot now the classification report for validation data.","c6add0cf":"Here we plot the confusion matrix.","38ac4f32":"The training set accuracy improved even more. In the same time, validation score is not very much improved. This indicates that most probably the model where we added more features is overfitting on the training set. Actually, the best model obtained until now was the one with {Sex, Age, Pclass, Fare} features, where accuracies for training set and validation set were 82% and 86% (with quite good precision for all categories and small recall for `Survived`).\n\nLet's try with a simpler model.\n\n\n### Model with {Title, FamilySize, Pclass} features\n\nThe simple model we will try now, with only three features, is actually using only engineered features.","bce962b8":"## <a id='44'>Estimate age<\/a>\n\nA relativelly large number of passengers have missing Age information. We will try to estimate this missing information from other available data, `Sex` and `Pclass`. Before doing this, we will also map sex values to numeric values.","f0fc65e3":"`Cabin` has the first letter that is, most probably, giving the information on the deck. This might have a predictive value and we will process further in the next sections.\n\n`Name` might contain multiple information. Let's check few of the `Name` fields.","b0e1d51c":"We fit the model.","ef6aee51":"Let's show now the correlation of the predictions using the first level models. The ensamble prediction is best when we have models with good accuracy and less correlated.","c4e296cb":"Both in **train** and **test** datasets, `Cabin` has more than 77% missing, `Age` more than 19%. `Embarked` is missing in 2 cases for **train** and `Fare` misses in  1 case for **test**.   \n\nWe will discuss, through this tutorial, the various possible methods to deal with missing data.","8307085d":"### Identify families by surname\n\nLet's also try to aggregate families by surname. For this, we will extract the surname from Name.","b2298f12":"Majority of the passengers were between 20 and 35 years old.\n\nThe survival rate for the age interval 15-35 years was quite small.\n","9a7d232e":"Let's create a Decision Tree model to predict missing Fare value.","39b3aae6":"<h1><center><font size=\"6\">Tutorial for Classification<\/font><\/center><\/h1>\n\n<h2><center><font size=\"4\">Dataset used: Titanic - Machine Learning from Disaster<\/font><\/center><\/h2>\n\n<br>\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/f\/fd\/RMS_Titanic_3.jpg\/640px-RMS_Titanic_3.jpg\" width=\"450\"><\/img>","9fb9dfcd":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  \n\n","7b516a2b":"Let's fit the model with the new data.","8752e0bf":"Let's prepare a simple model, using Random Forest. We set few algorithm parameters and initialize a clasiffier.","a4252515":"Let's see the best parameters.","4e3f44cf":"Let's see also the classification report for the validation set.","43354bf9":"\nLet's start by checking if there are missing data, unlabeled data or data that is inconsistently labeled. ","8944ef9a":"We check the features importance.","fcc40eae":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  ","70611dec":"Let's check the correlation between family size and survival rate.","fdc79236":"Let's check the accuracy for the validation set.","9c25f2d3":"Let's fit the model with the new predictors.","4f2c2d28":"We predict the validation set.","a6e652d3":"We succesfully set passenger #797 as a `Mrs`.\n\nLet's also group all the rare titles under a `Rare` title:","984bf83b":"We will pick this last model for submission.\nIt doesn't have the best accuracy for train set classification but have one of the best precision and accuracy for the validation set and also the smallest recall. \n\nLet's prepare the submission.","92f4bf19":"We plot the feature importance.","f1aad543":"And let's plot the `Age` clusters grouped by the survived and not survived.","690b73ea":"Let's plot the features importance. This shows the relative importance of the predictors features for the current model. With this information, we are able to select the features we will use for our gradually refined models.","bb722eb5":"## <a id='44'>More features engineering<\/a>  \n\n\nLet's map all titles to numeric values.","eec46631":"Family Size is mapped then to only 3 sizes (0 to 2), the first corresponding to the case when someone is alone.","7be6f355":"### Model with {Sex, Age, Pclass, Fare, Parch, SibSp} features","352c575c":"We predict the validation set.","a3435a85":"Let's see the accuracy for the training set and for the validation set.","b37f4225":"## <a id='43'>Extract Deck from Cabin<\/a>\n\nWe will extract the deck name from the Cabin name by separating the first character from each cabin name. \nUnfortunatelly, a very small number of passengers have `Cabin` information therefore also the `Deck` information will be only available for a reduced number of passengers.","e235353c":"Then, we fit the classifier with the train data prepared before.","24a48f6c":"There are few values given in this report \/ each category in the target (`Survived` or `Not survived`):\n\n* **Precision**  \n* **Recall**  \n* **F1-score**  \n\nLet's explain each of them:\n\n* **Precision** identifies the frequency with which a model was correct when predicting the positive class. That is:\n\n$$Precision = \\frac{\\textrm{True Positives}}{\\textrm{True Positives} + \\textrm{False Positives}}$$\n\n* **Recall** answers the following question: Out of all the possible positive labels, how many did the model correctly identify? That is:\n\n$$Recall = \\frac{\\textrm{True Positives}}{\\textrm{True Positives} + \\textrm{False Negatives}}$$\n\n* **F1-score** is the harmonic mean of **precision** and **recall**.\n\n$$\\textrm{F1-score} = 2  \\frac{Precision * Recall}{Precision + Recall}$$\n\n\nPrecison is better for the `Not survived` as well as Recall and F1-score.\n\nLet's also show the confusion matrix.","004a3789":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  \n\n\n## <a id='22'>Load the data<\/a>  \n\nLet's see first what data files do we have in the root directory. ","554357a2":"Most of the passengers traveled alone. From the passengers travelling alone, only 34% survived.\nThe passengers with only one or two sibbilings survived in around 50% of the cases. \nSurvival rates decrease considerably for number of sibilings or spouses of 3, 4, 5 and is practically 0 for 8.","6a3145a5":"## <a id='61'>Create the ensamble framework<\/a>\n\n\nWe start by creating a generic classifier, that extends the functionality of a simple classifier. This generic classifier will be instanciated with few different first level classifiers and then used in the ensamble. We are also using cross-validation (with KFolds).\n\nFirst step will be to create the folds used in cross validation.","308d3b8c":"Then, we prepare the submission dataset and export it in the submission file.","ffbb15e4":"We predict the validation set.","0391cd07":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  \n\n## <a id='55'>Hyperparameters optimization<\/a>\n\n\nLet's continue with tunning the model hyperparameters.   \nWe define a set of parameters with several values and will run an algorithm called Gradient Search to detect the best combination of parameters for our model.  \nFirst, let's fit the model and assign the output of it to **rf_clf**.","289b44e0":"First class passengers survived in a percent of 63% while less than 50% survived in 2nd class. For passengers in 3rd class, only 24% survived.","bcb6e77e":"Now we replaced the missing Age values with the one obtained from the approximations.","ede5eb50":"## <a id='63'>Train the first level models<\/a>  \n\n\nLet's define and train few first level models.\n\nWe will use the following first level models:\n\n\n* AdaBoost classifer\n* CatBoost Classifier\n* Extra Trees classifier  \n* Gradient Boosting classifer\n* Random Forest classifier  \n* Support Vector Machine\n\n\nLet's define the parameters used for training each classifier:","ce575871":"We also map `Fare` to 4 main fare segments and label them from 0 to 3.","076a5548":"Unmarried passengers and passengers with very large families had the lowest survival rate (30% singles and less than 20% the family members with families larger than 5). This can be explained by the fact that singles were most probably mens in lower classes while families with small number of children most probably saved at least a part of them (for example the mother with the childrens) cooperating between them to ensure salvation. Large families might had lower survival rates due to various reasons, including maybe difficulties to coordinate or more difficult decision on whom to embark on the boats (if not possible for all to embark). Another reason for larger families to survive might be related with the class.","cbde95f1":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  \n\n# <a id='3'>Data exploration<\/a>  \n\nWe check the shape of train and test dataframes and also show a selection of rows, to have an initial image of the data.\n\n","badb06a1":"The above graph is not showing the number of families survived or not survived. It is grouping family members that survived and family members that did not survived by Surname (family name). We did not checked if there are several different families with the same name or a family appears with a part of the family members in the survived lot and with a part of the family in the not survived lot (which we know it happens frequently, or example adult men in inferior class did not survived at all).","8b5b2747":"We fit the model.","486e574c":"We create 6 objects of type `SklearnBasicClassifier` that represent our 6 models(AdaBoost, CatBoost, ExtraTrees, GradientBoosting, RandomForest and Support Vector Machines).","b725dd3e":"Let's verify the average survival ratio for the passengers with the aggregated titles and sex.","dbf15208":"Let's predict with the validation data.","b69c6daf":"**Sex** is the dominant feature, followed by **Pclass** and **Fare**.  \nLet's see the accuracy for the training set and also for the validation set classification.","aebecf2e":"## <a id='36'>Imputation of missing data<\/a>  \n\nWe are creating a model for imputation of **Fare** data.  \n\nFirst, let's create a list with **train** and **test** datasets, to process both in the same time.\n","d99dd40e":"\n## <a id='50'>Split the data<\/a>  \n\nLet's split the training and validation set. We will use a 80-20 split.\nWe also set the matrices for train and validation and the vectors with the target values.","59de74fa":"Let's verify the relationship between `Title` and `Sex`.","d715c17c":"## <a id='66'>Submission (ensamble)<\/a>\n\nWe form the submission dataset and save it to the submission file.","59f1a9cf":"Survival rate increases considerably with the fare value. This confirm the image that richer people survived better. We will validate this observation as well with the class information.","eb71550e":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  \n\n\n## <a id='33'>Fare, Embarked, Pclass<\/a>  \n\nLet's check now the data (in **train** and **test**) for  `Fare`,  `Embarked` and`Pclass`.  \n","eef2c8aa":"She is traveling in Cabin `D17` in 1st class. Let's see if she is alone.","e4e40203":"## <a id='41'>Extract Title from Name<\/a>\n\nLet's start with processing the names. We will extract the title from the names.  \n\nWe create now a list of datasets (we name it as well all_df, we reuse this name):\n","052ebdaa":"And let's plot the `Class*Age` grouped by the survived and not survived.","5dd5d716":"The precision obtained for the test set is approx. **0.79**.","a639fcd6":"We see that we obtained an substantial improvement of the classification precision for the validation set.   Also the recall is better than in the case of the model with best accuracy and precision until now, besides this one.\n\nLet's also plot the confusion matrix.","6dd2b848":"## <a id='31'>Check for missing data<\/a>  \n\nLet's create a function that check for missing data in the two datasets (train and test).","cf52a50e":"\n## <a id='51'>Build a baseline model<\/a>  ","51aa7706":"The training set classification accuracy is still good, although lower than that of the previous, more complex model.\nThe validation set accuracy is the best obtained until now.\n\n**Title** is the most important feature.  \n\nLet's check now the classification report for the validation set.","fb0aa2a5":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  \n\n\n## <a id='52'>Evaluate the model<\/a>  \n\nLet's evaluate the model performance. \n\nWe are evaluating first the accuracy for the train set.\n\n","8c281151":"## <a id='42'>Build families<\/a>\n\n\n### Calculate Family Size\n\nFrom `SibSp` and `Parch` we create a new feature, `FamilySize`.","a0a37e9c":"Let's also plot the classification report for the validation set.","d734ad4d":"# <a id='1'>Introduction<\/a>  \n\nThis Kernel will take you through the process of **analyzing the data** to understand the **predictive values** of various **features** and the possible correlation between different features, **selection of features** with predictive value, **features engineering** to create features with higher predictive value, creation of a **baseline model**, succesive **refinement** of the model (we are using **RandomForest**) through selection of features and, at the end, **submission** of the best solution found. \n\nNext, we take the model and define a multi-dimmensional matrix of **hyperparameters** we would like to test. We use Gradient Search and cross-validation to select the best set of hyperparameters. The best model is then used for the **second submission**.\n\nNext, we will use **ensambling** (second level model trained with the output of first level models). We create several models (with the same set of parameters used with the previous model). We use **AdaBoost**, **CatBoost**, **ExtraTrees**, **GradientBoosting**, **RandomForest** and **SupportVectorMachines**. We are using **Out-Of-Folds** to avoid the risk that the base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions. The Out-Of-Folds are concatenated and feed to the second level model (XGBoost Classifier) and the prediction using the second level model is submitted (**third submission**) as the solution.\n\nThe dataset used for this tutorial is the famous now **Titanic** dataset.\n\n\n<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  ","1f7c2943":"Both **train** and **test** files contains the following values:  \n\n* **PassengerID** - the index of the passenger (in the dataset);  \n* **PClass** - the class of the passenger (from 1 to 3);\n* **Name** - the name of the passenger;\n* **Sex** - the sex of the passenger (female or male);  \n* **Age** - the age (where available) of the passenger;  \n* **SibSp** - the number of sibilings \/ spouses aboard of Titanic;  \n* **Parch** - the number of parents \/ children aboard of Titanic;  \n* **Ticket** - the ticket number;  \n* **Fare** - the passenger fare (ticket cost);  \n* **Cabin** - the cabin number;  \n* **Embarked** - the place of embarcation of the passenger (C = Cherbourg, Q = Queenstown, S = Southampton).  \n\nThe **train** data has as well the target value, **Survived**.\n\nIt is important, before going to create a model, to have a good understanding of the data. We will therefore explore the various features.","8997f927":"\n\n# <a id='0'>Content<\/a>\n\n- <a href='#1'>Introduction<\/a>  \n- <a href='#2'>Prepare the data analysis<\/a>  \n - <a href='#21'>Load packages<\/a>  \n - <a href='#22'>Load the data<\/a>   \n- <a href='#3'>Data exploration<\/a>   \n - <a href='#31'>Check for missing data<\/a>  \n - <a href='#32'>Sex, Age, SibSp, Parch<\/a>   \n - <a href='#33'>Fare, Embarked, Pclass<\/a>  \n - <a href='#34'>Ticket, Cabin, Name<\/a>   \n - <a href='#35'>Multiple features visualization<\/a>   \n - <a href='#36'>Imputation of missing data<\/a>   \n- <a href='#4'>Feature engineering<\/a>\n - <a href='#41'>Extract Title from Name<\/a>\n - <a href='#42'>Build families<\/a>\n - <a href='#43'>Extract Deck from Ticket<\/a>  \n - <a href='#44'>Estimate age<\/a>  \n - <a href='#45'>More features engineering<\/a>  \n- <a href='#5'>Predictive model for survival<\/a>\n - <a href='#50'>Split the data<\/a>  \n - <a href='#51'>Build a baseline model<\/a>  \n - <a href='#52'>Model evaluation<\/a>    \n - <a href='#53'>Model refinement<\/a> \n - <a href='#54'>Submission<\/a>  \n - <a href='#55'>Hyperparameters optimization<\/a>\n - <a href='#56'>Submission (model with hyperparameters optimization)<\/a>  \n- <a href='#6'>Model ensambling<\/a>\n - <a href='#61'>Create the ensamble framework<\/a>\n - <a href='#62'>Create the Out-Of-Fold Predictions<\/a>\n - <a href='#63'>Train the first level models<\/a>\n - <a href='#64'>Correlation of the results<\/a>\n - <a href='#65'>Build the second level (ensamble) model<\/a>\n - <a href='#66'>Submission (ensamble)<\/a>\n- <a href='#7'>References<\/a>    ","cee3ec11":"There are male only titles: `Capt`, `Col`, `Don`, `Jonkheer`, `Major`, `Master`, `Mr`, `Rev` and `Sir`. \nAs well, there are female only titles: `Countess`, `Lady`, `Miss`,  `Mlle`,  `Mme`, `Mrs`, `Ms`.\nThere is a female `Dr` (and other 6 males).\n\nMost of these titles are quite rare. We will either group them as `Rare` or correct them (for example, to reunite all young womens with the title `Miss`.\n\nLet's start by grouping all `Miss` and `Mrs` variations under a single name.","d37342ea":"## <a id='64'>Correlation of the results<\/a>\n\nLet's see the first few results of the predictions using first level models.","f329a677":"Let's check if this worked well.","af7cbee9":"The classification report for the validation set.","90e60d45":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  \n\n","04677775":"Let's plot feature importance.","0c327c3d":"# <a id='7'>References<\/a>\n\n[1] https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions  \n[2] https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python  \n[3] https:\/\/www.kaggle.com\/gpreda\/credit-card-fraud-detection-predictive-models  \n[4] https:\/\/www.kaggle.com\/gpreda\/honey-bee-subspecies-classification  ","65714d75":"We prepare as well the second level classifier. \n\nWe will use in this case a eXtreme Boost Classifier.","9fbd3445":"We apply the rule for extracting the title.","b8fc47ab":"We predict the validation set.","1a7684a7":"Let's check the features importance for the 5 out of 6 models. We will not include Support Vector Classifier, this one does not have feature importance available.","5fc279ac":"## <a id='65'>Build the second level (ensamble) model<\/a>\n\n\nWe prepare now, using the Out-Of-Folds values, the training and the test set for the second level model. We concatenate the OOFs from the 6 first level models.","a504e72d":"Similarly, we map `Age` to 5 main segments, labeled from 0 to 4. ","ecdb6ce4":"From the total female passengers, 74% survived.  \nIn the same time, from the total male passengers, only 18% survived.","7aee356e":"Let's check the validation classification report.","7e0b6cd6":"Let's see the train set and validation set classification accuracy.","5418cffd":"\n# <a id='5'>Predictive model for survival<\/a>  \n\nLet's start with creation of the predictive models. We will create a very simple model for starting.","657a0629":"The confusion matrix.","56598398":"We fit the model.","ec99fb71":"Then, let's set the female Dr as one of the female tipical roles.","a0a5b555":"### Model with {Sex, Age, Pclass, Fare} features","a5d9aced":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  \n\n## <a id='53'>Model refinement<\/a> \n\nLet's rebuild the model and add more features to it.","c7cfa15e":"For the number of children or parents equal to 0, the survival rate is only 34%. \nFor values of 1,2 and 3, the survival rate is 50%, decreasing for larger numbers.","ca7fef2b":"\n# <a id='6'>Model ensambling<\/a>\n\n\nLet's continue with creation of second level models. We will train several models and will then use these first level models to train a second level model. This method is powerfull and can enhance the performance of first level models, especially when there is little correlation between the results of first level models.\n","518fa170":"We initialize GridSearchCV with the classifier, the set of parameters, number of folds and also the level of verbose for printing out progress.","a21f2612":"Let's see now how looks like train and test set.","2118dfcf":"Let's also add one more feature, `Class*Age`, calculated as `Class` x `Age`.","9ad91167":"The plot shows just the data for the `Deck` information that could be extracted i.e. where a `Cabin` was defined.","feea1787":"The model performance with train data improved, i.e. the model is now representing better the training set.  The accuracy and precision with the validation set was as well improved. The recall and f1-score are smaller for `Survived`. \n\nLet's repeat the experiment adding more features."}}