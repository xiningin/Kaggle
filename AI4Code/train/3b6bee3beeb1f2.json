{"cell_type":{"1584844c":"code","97230330":"code","a7ebdb25":"code","027d3935":"code","a41c98e0":"code","b57bb31d":"code","8c348cfa":"code","5cf3ec0e":"code","aea2224a":"code","fe43009b":"code","73127447":"code","577ab7e0":"code","0f7bd6d5":"code","c1abd7b8":"code","f0c05a4d":"code","82084906":"code","a65add2a":"code","d2720a06":"code","a311e122":"code","8e173843":"code","7ee79380":"code","2e1a1611":"code","29bc71cf":"code","d00f7d05":"code","b5e00f54":"code","0fb921e1":"code","bcb194b9":"code","b9cdcf80":"code","30059d92":"code","289bc929":"markdown","3d58c8a2":"markdown","6693eba8":"markdown","561bdb89":"markdown","d9b781da":"markdown","312c6b3a":"markdown","d09d06f2":"markdown","d0979d57":"markdown","be656e71":"markdown","ce68b67b":"markdown","b134f575":"markdown","e0d3b779":"markdown","14c0f0a2":"markdown","0d0482ce":"markdown","a657a301":"markdown","6c1eb0cc":"markdown","9160dc90":"markdown","f9fac98c":"markdown"},"source":{"1584844c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy.stats import multivariate_normal as mvn\nimport math\n\nfrom numpy.linalg import slogdet, det, solve\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nimport time\n\nfrom sklearn.datasets import load_digits\nfrom sklearn.mixture.base import BaseMixture\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import normalize\n\nimport os, sys, email,re\nfrom nltk.corpus import stopwords\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","97230330":"samples = np.load('..\/input\/bayesian-samples\/samples.npz')\nX = samples['data']\npi0 = samples['pi0']\nmu0 = samples['mu0']\nsigma0 = samples['sigma0']\nplt.scatter(X[:, 0], X[:, 1], c='grey', s=30)\nplt.axis('equal')\nplt.show()\nprint(pi0, mu0)","a7ebdb25":"kmeans_test = KMeans(n_clusters= 3, init=\"k-means++\", max_iter=500, algorithm = 'auto')\nfitted = kmeans_test.fit(X)\nprediction = kmeans_test.predict(X)\nplt.scatter(X[:, 0], X[:, 1], c=prediction, s=30)\nplt.axis('equal')\nplt.show()\n\nkmeans_test.cluster_centers_","027d3935":"plt.figure(figsize = (10,8))\nfrom scipy.spatial.distance import cdist\ndef plot_kmeans(kmeans, X, n_clusters=4, rseed=0, ax=None):\n    labels = kmeans.fit_predict(X)\n\n    # plot the input data\n    ax = ax or plt.gca()\n    ax.axis('equal')\n    ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n\n    # plot the representation of the KMeans model\n    centers = kmeans.cluster_centers_\n    radii = [cdist(X[labels == i], [center]).max()\n             for i, center in enumerate(centers)]\n    for c, r in zip(centers, radii):\n        ax.add_patch(plt.Circle(c, r, fc='#CCCCCC', lw=3, alpha=0.5, zorder=1))\n        \nplot_kmeans(kmeans_test, X)","a41c98e0":"from sklearn.mixture import GaussianMixture\ngmm = GaussianMixture(n_components=3).fit(X)\nlabels = gmm.predict(X)\nplt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');\nprobs = gmm.predict_proba(X)\n# print(probs[:5].round(3))","b57bb31d":"# print(gmm._get_parameters()[0])\n# print(gmm._get_parameters()[1])\n#gmm.lower_bound_\n# kmeans mean values\n# array([[1.20514208, 5.82885457],\n#        [0.9733235 , 0.94371856],\n#        [6.40546402, 4.50975916]])","8c348cfa":"def calculate_mean_covariance(X, prediction):\n    C = 3\n    d = X.shape[1]\n    labels = np.unique(prediction)\n    initial_means = np.zeros((C, d))\n    initial_cov = np.zeros((C, d, d))\n    initial_pi = np.zeros(C)\n        \n    counter=0\n    for label in sorted(labels):\n        ids = np.where(prediction == label) # returns indices\n        initial_pi[counter] = len(ids[0]) \/ X.shape[0] \n        initial_means[counter,:] = np.mean(X[ids], axis = 0)\n        de_meaned = X[ids] - initial_means[counter,:]\n        Nk = X[ids].shape[0]\n        initial_cov[counter,:, :] = np.dot(initial_pi[counter] * de_meaned.T, de_meaned) \/ Nk\n        counter+=1\n    assert np.sum(initial_pi) == 1\n    return (initial_means, initial_cov, initial_pi)\n    \nn_clusters = 3\nkmeans = KMeans(n_clusters= n_clusters, max_iter=500, algorithm = 'auto')\nfitted = kmeans.fit(X)\nprediction = kmeans.predict(X)\n\nm, c, pi = calculate_mean_covariance(X, prediction)","5cf3ec0e":"class GMM:\n    \"\"\" Gaussian Mixture Model\n    \n    Parameters\n    -----------\n        k: int , number of gaussian distributions\n        \n        seed: int, will be randomly set if None\n        \n        max_iter: int, number of iterations to run algorithm, default: 200\n        \n    Attributes\n    -----------\n       centroids: array, k, number_features\n       \n       cluster_labels: label for each data point\n       \n    \"\"\"\n    def __init__(self, C, n_runs):\n        self.C = C # number of Guassians\/clusters\n        self.n_runs = n_runs\n        \n    \n    def get_params(self):\n        return (self.mu, self.pi, self.sigma)\n    \n    \n        \n    def calculate_mean_covariance(self, X, prediction):\n        \"\"\"Calculate means and covariance of different\n            clusters from k-means prediction\n        \n        Parameters:\n        ------------\n        prediction: cluster labels from k-means\n        \n        X: N*d numpy array data points \n        \n        Returns:\n        -------------\n        intial_means: for E-step of EM algorithm\n        \n        intial_cov: for E-step of EM algorithm\n        \n        \"\"\"\n        d = X.shape[1]\n        labels = np.unique(prediction)\n        self.initial_means = np.zeros((self.C, d))\n        self.initial_cov = np.zeros((self.C, d, d))\n        self.initial_pi = np.zeros(self.C)\n        \n        counter=0\n        for label in labels:\n            ids = np.where(prediction == label) # returns indices\n            self.initial_pi[counter] = len(ids[0]) \/ X.shape[0]\n            self.initial_means[counter,:] = np.mean(X[ids], axis = 0)\n            de_meaned = X[ids] - self.initial_means[counter,:]\n            Nk = X[ids].shape[0] # number of data points in current gaussian\n            self.initial_cov[counter,:, :] = np.dot(self.initial_pi[counter] * de_meaned.T, de_meaned) \/ Nk\n            counter+=1\n        assert np.sum(self.initial_pi) == 1    \n            \n        return (self.initial_means, self.initial_cov, self.initial_pi)\n    \n    \n    \n    def _initialise_parameters(self, X):\n        \"\"\"Implement k-means to find starting\n            parameter values.\n            https:\/\/datascience.stackexchange.com\/questions\/11487\/how-do-i-obtain-the-weight-and-variance-of-a-k-means-cluster\n\n        Parameters:\n        ------------\n        X: numpy array of data points\n        \n        Returns:\n        ----------\n        tuple containing initial means and covariance\n        \n        _initial_means: numpy array: (C*d)\n        \n        _initial_cov: numpy array: (C,d*d)\n        \n        \n        \"\"\"\n        n_clusters = self.C\n        kmeans = KMeans(n_clusters= n_clusters, init=\"k-means++\", max_iter=500, algorithm = 'auto')\n        fitted = kmeans.fit(X)\n        prediction = kmeans.predict(X)\n        self._initial_means, self._initial_cov, self._initial_pi = self.calculate_mean_covariance(X, prediction)\n        \n        \n        return (self._initial_means, self._initial_cov, self._initial_pi)\n            \n        \n        \n    def _e_step(self, X, pi, mu, sigma):\n        \"\"\"Performs E-step on GMM model\n\n        Parameters:\n        ------------\n        X: (N x d), data points, m: no of features\n        pi: (C), weights of mixture components\n        mu: (C x d), mixture component means\n        sigma: (C x d x d), mixture component covariance matrices\n\n        Returns:\n        ----------\n        gamma: (N x C), probabilities of clusters for objects\n        \"\"\"\n        N = X.shape[0] \n        self.gamma = np.zeros((N, self.C))\n\n        const_c = np.zeros(self.C)\n        \n        \n        self.mu = self.mu if self._initial_means is None else self._initial_means\n        self.pi = self.pi if self._initial_pi is None else self._initial_pi\n        self.sigma = self.sigma if self._initial_cov is None else self._initial_cov\n\n        for c in range(self.C):\n            # Posterior Distribution using Bayes Rule\n            self.gamma[:,c] = self.pi[c] * mvn.pdf(X, self.mu[c,:], self.sigma[c])\n\n        # normalize across columns to make a valid probability\n        gamma_norm = np.sum(self.gamma, axis=1)[:,np.newaxis]\n        self.gamma \/= gamma_norm\n\n        return self.gamma\n    \n    \n    def _m_step(self, X, gamma):\n        \"\"\"Performs M-step of the GMM\n        We need to update our priors, our means\n        and our covariance matrix.\n\n        Parameters:\n        -----------\n        X: (N x d), data \n        gamma: (N x C), posterior distribution of lower bound \n\n        Returns:\n        ---------\n        pi: (C)\n        mu: (C x d)\n        sigma: (C x d x d)\n        \"\"\"\n        N = X.shape[0] # number of objects\n        C = self.gamma.shape[1] # number of clusters\n        d = X.shape[1] # dimension of each object\n\n        # responsibilities for each gaussian\n        self.pi = np.mean(self.gamma, axis = 0)\n\n        self.mu = np.dot(self.gamma.T, X) \/ np.sum(self.gamma, axis = 0)[:,np.newaxis]\n\n        for c in range(C):\n            x = X - self.mu[c, :] # (N x d)\n            \n            gamma_diag = np.diag(self.gamma[:,c])\n            x_mu = np.matrix(x)\n            gamma_diag = np.matrix(gamma_diag)\n\n            sigma_c = x.T * gamma_diag * x\n            self.sigma[c,:,:]=(sigma_c) \/ np.sum(self.gamma, axis = 0)[:,np.newaxis][c]\n\n        return self.pi, self.mu, self.sigma\n    \n    \n    def _compute_loss_function(self, X, pi, mu, sigma):\n        \"\"\"Computes lower bound loss function\n        \n        Parameters:\n        -----------\n        X: (N x d), data \n        \n        Returns:\n        ---------\n        pi: (C)\n        mu: (C x d)\n        sigma: (C x d x d)\n        \"\"\"\n        N = X.shape[0]\n        C = self.gamma.shape[1]\n        self.loss = np.zeros((N, C))\n\n        for c in range(C):\n            dist = mvn(self.mu[c], self.sigma[c],allow_singular=True)\n            self.loss[:,c] = self.gamma[:,c] * (np.log(self.pi[c]+0.00001)+dist.logpdf(X)-np.log(self.gamma[:,c]+0.000001))\n        self.loss = np.sum(self.loss)\n        return self.loss\n    \n    \n    \n    def fit(self, X):\n        \"\"\"Compute the E-step and M-step and\n            Calculates the lowerbound\n        \n        Parameters:\n        -----------\n        X: (N x d), data \n        \n        Returns:\n        ----------\n        instance of GMM\n        \n        \"\"\"\n        \n        d = X.shape[1]\n        self.mu, self.sigma, self.pi =  self._initialise_parameters(X)\n        \n        try:\n            for run in range(self.n_runs):  \n                self.gamma  = self._e_step(X, self.mu, self.pi, self.sigma)\n                self.pi, self.mu, self.sigma = self._m_step(X, self.gamma)\n                loss = self._compute_loss_function(X, self.pi, self.mu, self.sigma)\n                \n                if run % 10 == 0:\n                    print(\"Iteration: %d Loss: %0.6f\" %(run, loss))\n\n        \n        except Exception as e:\n            print(e)\n            \n        \n        return self\n    \n    \n    \n    \n    def predict(self, X):\n        \"\"\"Returns predicted labels using Bayes Rule to\n        Calculate the posterior distribution\n        \n        Parameters:\n        -------------\n        X: ?*d numpy array\n        \n        Returns:\n        ----------\n        labels: predicted cluster based on \n        highest responsibility gamma.\n        \n        \"\"\"\n        labels = np.zeros((X.shape[0], self.C))\n        \n        for c in range(self.C):\n            labels [:,c] = self.pi[c] * mvn.pdf(X, self.mu[c,:], self.sigma[c])\n        labels  = labels .argmax(1)\n        return labels \n    \n    def predict_proba(self, X):\n        \"\"\"Returns predicted labels\n        \n        Parameters:\n        -------------\n        X: N*d numpy array\n        \n        Returns:\n        ----------\n        labels: predicted cluster based on \n        highest responsibility gamma.\n        \n        \"\"\"\n        post_proba = np.zeros((X.shape[0], self.C))\n        \n        for c in range(self.C):\n            # Posterior Distribution using Bayes Rule, try and vectorise\n            post_proba[:,c] = self.pi[c] * mvn.pdf(X, self.mu[c,:], self.sigma[c])\n    \n        return post_proba","aea2224a":"model = GMM(3, n_runs = 100)\n\nfitted_values = model.fit(X)\n\npredicted_values = model.predict(X)\n# compute centers as point of highest density of distribution\ncenters = np.zeros((3,2))\nfor i in range(model.C):\n    density = mvn(cov=model.sigma[i], mean=model.mu[i]).logpdf(X)\n    centers[i, :] = X[np.argmax(density)]\n    \nplt.figure(figsize = (10,8))\nplt.scatter(X[:, 0], X[:, 1],c=predicted_values ,s=50, cmap='viridis')\n\nplt.scatter(centers[:, 0], centers[:, 1],c='black', s=300, alpha=0.6);\n","fe43009b":"# print(\"initial means: \", model._initial_means)\n# print('--------------------------')\n# print(\"initial pi: \", model._initial_pi)\n# print('--------------------------')\n# print('GMM means: ', model.get_params()[1])\n# print('--------------------------')\n# print('GMM pi: ', model.get_params()[0])\n# print('--------------------------')\n# print('sklearn GMM means: ', gmm._get_parameters()[1])\n# print('--------------------------')\n# print('sklearn GMM pi: ', gmm._get_parameters()[0])","73127447":"# Credit to python data science handbook for the code to plot these distributions\nfrom matplotlib.patches import Ellipse\n\ndef draw_ellipse(position, covariance, ax=None, **kwargs):\n    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n    ax = ax or plt.gca()\n    \n    # Convert covariance to principal axes\n    if covariance.shape == (2, 2):\n        U, s, Vt = np.linalg.svd(covariance)\n        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n        width, height = 2 * np.sqrt(s)\n    else:\n        angle = 0\n        width, height = 2 * np.sqrt(covariance)\n    \n    # Draw the Ellipse\n    for nsig in range(1, 4):\n        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n                             angle, **kwargs))\n        ","577ab7e0":"plt.figure(figsize = (10,8))\nplt.scatter(X[:, 0], X[:, 1],c=predicted_values ,s=50, cmap='viridis')\nplt.scatter(centers[:, 0], centers[:, 1],c='black', s=300, alpha=0.6);\n\nw_factor = 0.2 \/ model.pi.max()\nfor pos, covar, w in zip(model.mu, model.sigma, model.pi):\n    draw_ellipse(pos, covar, alpha=w * w_factor)","0f7bd6d5":"def E_step(X, pi, mu, sigma):\n    \"\"\"Performs E-step on GMM model\n\n        Parameters:\n        ------------\n        X: (N x m), data points, m: no of features\n        pi: (C), weights of mixture components\n        mu: (C x m), mixture component means\n        sigma: (C x m x m), mixture component covariance matrices\n\n        Returns:\n        ----------\n        gamma: (N x C), probabilities of clusters for objects\n        \"\"\"\n    \n    N = X.shape[0] \n    C = pi0.shape[0] # number of clusters\n    gamma = np.zeros((N, C))\n\n    const_c = np.zeros(C)\n\n    for c in range(C):\n#         pi_c = pi0[c]\n#         mu_c = mu0[c, :] \n#         sigma_c = sigma0[c] \n\n        # Posterior Distribution using Bayes Rule\n        gamma[:,c] = pi0[c] * mvn.pdf(X, mu0[c,:], sigma0[c])\n\n    # normalize across columns to make a valid probability\n    gamma_norm = np.sum(gamma, axis=1)[:,np.newaxis]\n    gamma \/= gamma_norm\n\n    return gamma\n\ngamma = E_step(X, pi0, mu0, sigma0)\nprint(np.sum(gamma, axis=0))","c1abd7b8":"def M_step(X, gamma):\n    \"\"\"Performs M-step of the GMM\n    We need to update our priors, our means\n    and our covariance matrix.\n    \n    Parameters:\n    -----------\n    X: (N x d), data \n    gamma: (N x C), posterior distribution of lower bound \n    \n    Returns:\n    ---------\n    pi: (C)\n    mu: (C x d)\n    sigma: (C x d x d)\n    \"\"\"\n    N = X.shape[0] # number of objects\n    C = gamma.shape[1] # number of clusters\n    d = X.shape[1] # dimension of each object\n\n    ### YOUR CODE HERE\n    mu = np.zeros((C, d))\n    sigma = np.zeros((C, d, d))\n    \n    # responsibilities for each gaussian\n    pi = np.mean(gamma, axis = 0)\n    mu = np.dot(gamma.T, X) \/ np.sum(gamma, axis = 0)[:,np.newaxis]\n     \n    for c in range(C): # for each Gaussian\n        x_mu = X - mu[c, :] # (N x d)\n        \n        gamma_diag = np.diag(gamma[:,c])\n        x_mu = np.matrix(x_mu)\n        gamma_diag = np.matrix(gamma_diag)\n        \n        sigma_c = x_mu.T * gamma_diag * x_mu\n        sigma[c,:,:]=(sigma_c) \/ np.sum(gamma, axis = 0)[:,np.newaxis][c]\n    \n    return pi, mu, sigma\npi, mu, sigma = M_step(X, gamma)\nprint(pi, mu, sigma)","f0c05a4d":"def compute_loss_function(X, gamma, pi, mu, sigma):\n    N = X.shape[0]\n    C = gamma.shape[1]\n    loss = np.zeros((N,C))\n    \n    for c in range(C):\n        dist = mvn(mu[c], sigma[c],allow_singular=True)\n        loss[:,c] = gamma[:,c] * (np.log(pi[c]+0.00001)+dist.logpdf(X)-np.log(gamma[:,c]+0.000001))\n\n    return np.sum(loss)","82084906":"compute_loss_function(X, gamma, pi, mu, sigma)","a65add2a":"pi, mu, sigma = pi0, mu0, sigma0\ngamma = E_step(X, pi, mu, sigma)\npi, mu, sigma = M_step(X, gamma)\nloss = compute_loss_function(X, gamma, pi, mu, sigma)","d2720a06":"df = pd.read_csv('..\/input\/enron-email-dataset\/emails.csv',nrows = 35000)","a311e122":"# create list of email objects\nemails = list(map(email.parser.Parser().parsestr,df['message']))\n\n# extract headings such as subject, from, to etc..\nheadings  = emails[0].keys()\n\n# Goes through each email and grabs info for each key\n# doc['From'] grabs who sent email in all emails\nfor key in headings:\n    df[key] = [doc[key] for doc in emails]\n\n    \n##Useful functions\ndef get_raw_text(emails):\n    email_text = []\n    for email in emails.walk():\n        if email.get_content_type() == 'text\/plain':\n            email_text.append(email.get_payload())\n    return ''.join(email_text)\n\ndf['body'] = list(map(get_raw_text, emails))\ndf.head()\ndf['user'] = df['file'].map(lambda x: x.split('\/')[0])\n\n\n#Unique to and From\nprint('Total number of emails: %d' %len(df))\nprint('------------')\nprint('Number of unique received: %d '%df['To'].nunique())\nprint('------------')\nprint('Number of unique Sent: %d '%df['From'].nunique())\n\n\ndef clean_column(data):\n    if data is not None:\n        stopwords_list = stopwords.words('english')\n        #exclusions = ['RE:', 'Re:', 're:']\n        #exclusions = '|'.join(exclusions)\n        data =  data.lower()\n        data = re.sub('re:', '', data)\n        data = re.sub('-', '', data)\n        data = re.sub('_', '', data)\n        # Remove data between square brackets\n        data =re.sub('\\[[^]]*\\]', '', data)\n        # removes punctuation\n        data = re.sub(r'[^\\w\\s]','',data)\n        data = re.sub(r'\\n',' ',data)\n        data = re.sub(r'[0-9]+','',data)\n        # strip html \n        p = re.compile(r'<.*?>')\n        data = re.sub(r\"\\'ve\", \" have \", data)\n        data = re.sub(r\"can't\", \"cannot \", data)\n        data = re.sub(r\"n't\", \" not \", data)\n        data = re.sub(r\"I'm\", \"I am\", data)\n        data = re.sub(r\" m \", \" am \", data)\n        data = re.sub(r\"\\'re\", \" are \", data)\n        data = re.sub(r\"\\'d\", \" would \", data)\n        data = re.sub(r\"\\'ll\", \" will \", data)\n        data = re.sub('forwarded by phillip k allenhouect on    pm', '',data)\n        data = re.sub(r\"httpitcappscorpenroncomsrrsauthemaillinkaspidpage\", \"\", data)\n        \n        data = p.sub('', data)\n        if 'forwarded by:' in data:\n            data = data.split('subject')[1]\n        data = data.strip()\n        return data\n    return 'No Subject'\n\n\ndf['Subject_new'] = df['Subject'].apply(clean_column)\ndf['body_new'] = df['body'].apply(clean_column)\n\n\nfrom wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\nto_add = ['FW', 'ga', 'httpitcappscorpenroncomsrrsauthemaillinkaspidpage', 'cc', 'aa', 'aaa', 'aaaa',\n         'hou', 'cc', 'etc', 'subject', 'pm']\n\nfor i in to_add:\n    stopwords.add(i)","8e173843":"df['body_new'].head()","7ee79380":"from sklearn.feature_extraction.text import TfidfVectorizer\ndata = df['body_new']\n# data.head()\n\ntf_idf_vectorizor = TfidfVectorizer(stop_words = stopwords,#tokenizer = tokenize_and_stem,\n                             max_features = 5000)\n%time tf_idf = tf_idf_vectorizor.fit_transform(data)\ntf_idf_norm = normalize(tf_idf)\ntf_idf_array = tf_idf_norm.toarray()\npd.DataFrame(tf_idf_array, columns=tf_idf_vectorizor.get_feature_names()).head()","2e1a1611":"from sklearn.cluster import KMeans\nn_clusters = 3\nsklearn_pca = PCA(n_components = 2)\nY_sklearn = sklearn_pca.fit_transform(tf_idf_array)\nkmeans = KMeans(n_clusters= n_clusters, max_iter=600, algorithm = 'auto')\n%time fitted = kmeans.fit(Y_sklearn)\nprediction = kmeans.predict(Y_sklearn)\n\nplt.scatter(Y_sklearn[:, 0], Y_sklearn[:, 1],c=prediction ,s=50, cmap='viridis')\n\ncenters2 = fitted.cluster_centers_\nplt.scatter(centers2[:, 0], centers2[:, 1],c='black', s=300, alpha=0.6);","29bc71cf":"from matplotlib.patches import Ellipse\n\ndef draw_ellipse(position, covariance, ax=None, **kwargs):\n    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n    ax = ax or plt.gca()\n    \n    # Convert covariance to principal axes\n    if covariance.shape == (2, 2):\n        U, s, Vt = np.linalg.svd(covariance)\n        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n        width, height = 2 * np.sqrt(s)\n    else:\n        angle = 0\n        width, height = 2 * np.sqrt(covariance)\n    \n    # Draw the Ellipse\n    for nsig in range(1, 4):\n        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n                             angle, **kwargs))","d00f7d05":"from sklearn.mixture import GaussianMixture\nsklearn_pca = PCA(n_components = 2)\nY_sklearn = sklearn_pca.fit_transform(tf_idf_array)\ngmm = GaussianMixture(n_components=3, covariance_type='full').fit(Y_sklearn)\nprediction_gmm = gmm.predict(Y_sklearn)\nprobs = gmm.predict_proba(Y_sklearn)\n\ncenters = np.zeros((3,2))\nfor i in range(3):\n    density = mvn(cov=gmm.covariances_[i], mean=gmm.means_[i]).logpdf(Y_sklearn)\n    centers[i, :] = Y_sklearn[np.argmax(density)]\n\nplt.figure(figsize = (10,8))\nplt.scatter(Y_sklearn[:, 0], Y_sklearn[:, 1],c=prediction_gmm ,s=50, cmap='viridis')\nplt.scatter(centers[:, 0], centers[:, 1],c='black', s=300, alpha=0.6);","b5e00f54":"# diag, Gaussians are aligned with th axis\nplt.figure(figsize = (10,8))\nplt.scatter(Y_sklearn[:, 0], Y_sklearn[:, 1],c=prediction_gmm ,s=50, cmap='viridis', zorder=1)\nplt.scatter(centers[:, 0], centers[:, 1],c='black', s=300, alpha=0.6);\n\nw_factor = 0.2 \/ gmm.weights_.max()\nfor pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n    draw_ellipse(pos, covar, alpha=w*.75)","0fb921e1":"print(\"GMM weights: \", gmm.weights_)\nprint(\"GMM means: \", gmm.means_)\nprint(\"GMM Covariance: \", gmm.covariances_)\nprint('-----------------------------')\nprint(\"My model weights: \", model.pi)\nprint(\"My model means: \", model.mu)\nprint(\"My model Covariance: \", model.sigma)","bcb194b9":"plt.figure(figsize = (10,8))\nfrom scipy.spatial.distance import cdist\ndef plot_kmeans(kmeans, X, n_clusters=3, rseed=0, ax=None):\n    labels = kmeans.fit_predict(X)\n\n    # plot the input data\n    ax = ax or plt.gca()\n    ax.axis('equal')\n    ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n\n    # plot the representation of the KMeans model\n    centers = kmeans.cluster_centers_\n    radii = [cdist(X[labels == i], [center]).max()\n             for i, center in enumerate(centers)]\n    for c, r in zip(centers, radii):\n        ax.add_patch(plt.Circle(c, r, fc='#CCCCCC', lw=3, alpha=0.5, zorder=1))\n        \nplot_kmeans(kmeans, Y_sklearn)","b9cdcf80":"plt.figure(figsize = (10,8))\nplt.scatter(Y_sklearn[:, 0], Y_sklearn[:, 1],c=prediction_gmm ,s=50, cmap='viridis', zorder=1)\nplt.scatter(centers[:, 0], centers[:, 1],c='black', s=300, alpha=0.6);\n\nw_factor = 0.2 \/ gmm.weights_.max()\nfor pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n    draw_ellipse(pos, covar, alpha=w*.75)","30059d92":"model = GMM(3, n_runs = 50)\n\nfitted_values = model.fit(Y_sklearn)\npredicted_values = model.predict(Y_sklearn)\n\n# # compute centers as point of highest density of distribution\ncenters = np.zeros((3,2))\nfor i in range(model.C):\n    density = mvn(cov=model.sigma[i], mean=model.mu[i]).logpdf(Y_sklearn)\n    centers[i, :] = Y_sklearn[np.argmax(density)]\n    \nplt.figure(figsize = (10,8))\nplt.scatter(Y_sklearn[:, 0], Y_sklearn[:, 1],c=predicted_values ,s=50, cmap='viridis', zorder=1)\n\nplt.scatter(centers[:, 0], centers[:, 1],c='black', s=300, alpha=0.5, zorder=2);\n\nw_factor = 0.2 \/ model.pi.max()\nfor pos, covar, w in zip(model.mu, model.sigma, model.pi):\n    draw_ellipse(pos, covar, alpha = w)","289bc929":"# Test all functions\n- seem to be correct. Class implmentation doesnt give same answer\n- Could be because I have an incorrect initialisation","3d58c8a2":"# GMM sklearn","6693eba8":"# Draw Distributions","561bdb89":"# New E-Step","d9b781da":"# Test GMM class\n- class working \n- same as sklearn implementation","312c6b3a":"# Variational Lower Bound\n$$\\mathcal{L} = \\sum_{n=1}^{N} \\sum_{k=1}^{K} \\mathbb{E}[z_{n, k}] (\\log \\pi_k + \\log \\mathcal{N}(x_n | \\mu_k, \\sigma_k)) - \\sum_{n=1}^{N} \\sum_{k=1}^{K} \\mathbb{E}[z_{n, k}] \\log \\mathbb{E}[z_{n, k}]$$","d09d06f2":"# Test Functions from class","d0979d57":"# Plot clusters","be656e71":"# Draw Distributions around GMM clusters","ce68b67b":"# Test on Enron data set","b134f575":"# Compare sklearn and my own and test predict method","e0d3b779":"# GMM my implementation","14c0f0a2":"# K means sklearn","0d0482ce":"# TF-IDF","a657a301":"# Read in Data and Preprocess","6c1eb0cc":"# GMM should produce something similar","9160dc90":"# M Step","f9fac98c":"# GMM sklearn"}}