{"cell_type":{"5eb29803":"code","cd78a26c":"code","8a1a658c":"code","16785f7c":"code","2b72ef29":"code","3ceef866":"code","b0c70e09":"code","4e54a26b":"code","7ca057ae":"code","c2b7e8e4":"code","4c4385bc":"code","e776b869":"code","03432346":"code","bb935a80":"code","56535f25":"code","bc7dd23c":"code","6d1ab5ab":"markdown","bef0bb19":"markdown","40eb0b95":"markdown","b353bf77":"markdown","7db00a63":"markdown","b34cf934":"markdown"},"source":{"5eb29803":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n","cd78a26c":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","8a1a658c":"Y_train = train_df['Survived']\nY_test_PassengerId = test_df['PassengerId'] # Save for submission\n\nfeatures = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\ntrain_df = train_df[features] \ntest_df = test_df[features] ","16785f7c":"combined = [train_df, test_df] \n\nfor df in combined:     \n    # Filling missing values. \n    df['Age'].fillna(df['Age'].mean(),inplace=True)\n    df['Fare'].fillna(df['Fare'].mean(),inplace=True)\n    df['Embarked'].fillna(value='S',inplace=True)    \n    \n    # Converting categorical features to numeric\n    df['Sex'] = df['Sex'].replace(['female','male'],[0,1]).astype(int)\n    df['Embarked'] = df['Embarked'].replace(['S','Q','C'],[1,2,3]).astype(int)\n    \n    # Another way to convert categorical features to numeric\n    #df['Sex'] = df['Sex'].map({'male': 0, 'female': 1 }).astype(int)    \n    #df['Embarked'] = df['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype(int)     \n      \n    # Perform normalization\n    df.loc[ df['Fare'] <= 7.91, 'Fare'] = 0\n    df.loc[(df['Fare'] > 7.91) & (df['Fare'] <= 14.454), 'Fare'] = 1\n    df.loc[(df['Fare'] > 14.454) & (df['Fare'] <= 31), 'Fare']   = 2\n    df.loc[(df['Fare'] > 31) & (df['Fare'] <= 99), 'Fare']   = 3\n    df.loc[(df['Fare'] > 99) & (df['Fare'] <= 250), 'Fare']   = 4\n    df.loc[ df['Fare'] > 250, 'Fare'] = 5\n    df['Fare'] = df['Fare'].astype(int)\n\n    \n# Make sure that train data does not contain NaN\nassert not train_df.isnull().values.any()\n\n# Make sure that test data does not contain NaN\nassert not test_df.isnull().values.any()\n\n# Otherwise find columns with NaN value and deal with it: df.columns[df.isnull().any()].tolist())","2b72ef29":"train_df.head()","3ceef866":"test_df.head()","b0c70e09":"def sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of z\n\n    Arguments:\n    z -- A scalar or numpy array of any size.\n\n    Return:\n    s -- sigmoid(z)\n    \"\"\"\n\n    s = 1 \/ (1 + np.exp(-z))\n    \n    return s","4e54a26b":"def initialize_with_zeros(dim):\n    \"\"\"\n    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n    \n    Argument:\n    dim -- size of the w vector we want (or number of parameters in this case)\n    \n    Returns:\n    w -- initialized vector of shape (dim, 1)\n    b -- initialized scalar (corresponds to the bias)\n    \"\"\"\n    \n    w = np.zeros(shape=(dim, 1))\n    b = 0\n\n    assert(w.shape == (dim, 1))\n    assert(isinstance(b, float) or isinstance(b, int))\n    \n    return w, b","7ca057ae":"def propagate(w, b, X, Y):\n    \"\"\"\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b    \n    \"\"\"\n    \n    m = X.shape[1]\n    \n    # FORWARD PROPAGATION (FROM X TO COST)\n    A = sigmoid(np.dot(w.T, X) + b)                                                   # compute activation\n    cost = (- 1 \/ m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))          # compute cost\n        \n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    dw = (1 \/ m) * np.dot(X,(A - Y).T)\n    db = (1 \/ m) * np.sum(A - Y)\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    cost = np.squeeze(cost)\n    assert(cost.shape == ())\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return grads, cost","c2b7e8e4":"def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \"\"\"\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        \n        \n        # Cost and gradient calculation (\u2248 1-4 lines of code)\n        grads, cost = propagate(w, b, X, Y)\n                \n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        # update rule (\u2248 2 lines of code)\n        w = w - learning_rate * dw \n        b = b - learning_rate * db\n        \n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n        \n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs","4c4385bc":"def predict(w, b, X):\n    '''\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0\/1) for the examples in X\n    '''\n    \n    m = X.shape[1]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0], 1)\n    \n    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture   \n    A = sigmoid(np.dot(w.T, X) + b)\n    for i in range(A.shape[1]):\n        \n        # Convert probabilities A[0,i] to actual predictions p[0,i]\n        Y_prediction[0, i] = 1 if A[0, i] >= 0.5 else 0\n    \n    assert(Y_prediction.shape == (1, m))\n    \n    return Y_prediction","e776b869":"def model(X_train, Y_train, X_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n    \n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_of_features, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n    \n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"   \n    \n    # initialize parameters with zeros (\u2248 1 line of code)\n    w, b = initialize_with_zeros(X_train.shape[0])\n\n    # Gradient descent (\u2248 1 line of code)\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost=False)\n    \n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n    \n    # Predict test\/train set examples (\u2248 2 lines of code)\n    Y_prediction_train = predict(w, b, X_train)    \n    Y_prediction_test = predict(w, b, X_test)\n\n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n\n    \n    d = {\"costs\": costs,         \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"Y_prediction_test\": Y_prediction_test,\n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d","03432346":"# Make sure that dimensions correct\nX_train = np.array(train_df).T\nY_train = np.array(Y_train)\nY_train = Y_train.reshape(Y_train.shape[0], 1).T\nX_test = np.array(test_df).T\n\nassert X_train.shape[1] == Y_train.shape[1]\nassert X_train.shape[0] == X_test.shape[0]\nX_train.shape, Y_train.shape, X_test.shape","bb935a80":"d = model(X_train, Y_train, X_test, num_iterations = 50000, learning_rate = 0.0001, print_cost = True)","56535f25":"# Plot learning curve (with costs)\ncosts = np.squeeze(d['costs'])\nplt.plot(costs)\nplt.ylabel('cost')\nplt.xlabel('iterations (per hundreds)')\nplt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\nplt.show()","bc7dd23c":"submission = pd.DataFrame({\n        \"PassengerId\": Y_test_PassengerId,\n        \"Survived\": d['Y_prediction_test'].T.flatten().astype(int)\n    })\nsubmission.to_csv('submission.csv', index=False)","6d1ab5ab":"# Result\n\nYour submission scored 0.72248, which is not an improvement of your best score. Keep trying!","bef0bb19":"## Loading the Data","40eb0b95":"# Final thoughts: \n\nThere is a lot that could be done to improve the score, like feature extraction, normalization, regularization, hyperparameter tuning, etc.  \n\nTuned **scikit-learn** algorithms like *Random Forest* or *Decision Tree* would perform slightly better, and some would be able to reach above 80%. \n\nFor me the main goal was to implement Logistic Regression from scratch, and make it work! ","b353bf77":"## Importing the Libraries","7db00a63":"## Model\n\nThe model is an a simple Logistic Regression.\n\nI'm taking the famous Andrew Ng course [Neural Networks and Deep Learning](https:\/\/www.coursera.org\/learn\/neural-networks-deep-learning?specialization=deep-learning). \n\nThis implementation is a fork from the assignments code.\n\n### Mathematical explanation of the learning algorithm\n\nFor one example $x^{(i)}$:\n$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n\nThe cost is then computed by summing over all training examples:\n$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n\n**Key steps**:\n    - Initialize the parameters of the model\n    - Learn the parameters for the model by minimizing the cost  \n    - Use the learned parameters to make predictions (on the test set)\n    - Analyse the results and conclude","b34cf934":"## Data Preprocessing\n\nPossibe room for imporment is to create new features by combining existing ones.\nFor example extracting titles like 'Master' from 'Name' field. Or make a composite feature based on 'SibSp' and 'Parch'."}}