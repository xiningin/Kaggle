{"cell_type":{"4bb8c21d":"code","f12c8a75":"code","29001cc4":"code","48190c11":"code","44c4b1d4":"code","d3e35c17":"code","593d8d7e":"code","9bae963c":"code","6ae4787b":"code","3e0c158f":"code","89069124":"code","4aeae7fa":"code","d0529b73":"code","95ce6823":"code","25f7d488":"code","33847e3e":"code","dd8e7ffb":"code","69ee0610":"code","82a9f73b":"code","259889e8":"code","8df8da84":"code","512dd09c":"code","0755cc58":"code","c768a2c4":"code","abd132a4":"code","4aa24ef4":"code","5e6171a3":"code","a24e39b2":"code","d62d45ae":"code","b7a5c41a":"code","54c9fafc":"code","0e1b692a":"code","5b353ee7":"code","841e3ec4":"code","0326dfd3":"code","e4436745":"code","2d94bb99":"code","3d24b89e":"code","1c7140f5":"code","587dd25b":"code","e41f9580":"code","28e0b0fc":"code","4f146057":"code","e248f703":"code","98df0b07":"code","2db58452":"code","848a263e":"code","e58ad51d":"code","0061e97b":"code","c08e0122":"code","15c0517f":"code","6064d677":"code","9802941d":"code","89b0797c":"code","14d2111c":"code","144c9063":"code","0bcd14b2":"code","8c7d230b":"code","af7d17b2":"code","6d1eebb7":"code","a84b241e":"code","2a0eb53c":"code","1e98e018":"code","a65d1009":"code","9c5dda60":"markdown","d1ba7578":"markdown","fade9879":"markdown","37b5323c":"markdown","b42e438a":"markdown","90004ea3":"markdown","2fe2eae0":"markdown","c6cbde5d":"markdown","c4b943f3":"markdown","527053d7":"markdown","d994f0b2":"markdown","5253493e":"markdown","ddaa5819":"markdown","8a8d4e11":"markdown","93f3946e":"markdown","cbd1c0d6":"markdown","ed9e3edc":"markdown","4a9db8a7":"markdown","b7e2704b":"markdown","edc92dbc":"markdown","356e692d":"markdown"},"source":{"4bb8c21d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f12c8a75":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \nimport pandas as pd\nimport numpy as np","29001cc4":"train_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntrain_raw = train_df.copy()\ntest_raw = test_df.copy()\ncombine = [train_df, test_df]","48190c11":"for_info = pd.DataFrame(train_df.dtypes).T.rename(index = {0 : \"dtypes\"})\nfor_info = for_info.append(pd.DataFrame(train_df.isnull().sum()).T.rename(index = {0 : \"null count\"}))\nfor_info","44c4b1d4":"for_info_test = pd.DataFrame(test_df.dtypes).T.rename(index = {0 : \"dtypes\"})\nfor_info_test = for_info_test.append(pd.DataFrame(test_df.isnull().sum()).T.rename(index = {0 : \"null count\"}))\nfor_info_test","d3e35c17":"train_df.describe()","593d8d7e":"print(train_df['Survived'].value_counts())\nl=['Not Survived','Survived']\nax=train_df['Survived'].value_counts().plot.pie(autopct='%.2f%%',figsize=(6,6),labels=l)\n#autopct='%.2f%%' is to show the percentage text on the plot\nax.set_ylabel('')","9bae963c":"print(train_df['Sex'].value_counts())\nl=['male','female']\nax=train_df['Sex'].value_counts().plot.pie(autopct='%.2f%%',figsize=(6,6),labels=l)\n#autopct='%.2f%%' is to show the percentage text on the plot\nax.set_ylabel('')","6ae4787b":"sns.set(style = 'whitegrid')\nax = sns.kdeplot(train_df[\"Age\"])\nax.set_title(\"Age\")\nax.set_xlabel(\"Age\")","3e0c158f":"print(train_df['Pclass'].value_counts())\nl = [\"3\", \"1\", \"2\"]\nax =  train_df['Pclass'].value_counts().plot.pie(autopct='%.2f%%',figsize=(6,6),labels=l)\nax.set_ylabel('')","89069124":"print(train_df[\"Embarked\"].value_counts())\nl = [\"S\", \"C\", \"Q\"]\nax = train_df[\"Embarked\"].value_counts().plot.pie(autopct = \"%.2f%%\", figsize = (6,6), labels = l)\nax.set_ylabel('')","4aeae7fa":"train_df[['Sex','Survived']].groupby('Sex').mean()","d0529b73":"train_df[['Pclass',\"Survived\"]].groupby(\"Pclass\").mean()","95ce6823":"train_df[['Sex',\"Pclass\",'Survived']].groupby(['Sex','Pclass']).mean()","25f7d488":"sns.countplot(x = 'Pclass', hue = 'Survived' , data = train_df)","33847e3e":"train_df[['Embarked','Survived']].groupby('Embarked').mean()","dd8e7ffb":"train_df[[\"SibSp\", \"Survived\"]].groupby(\"SibSp\").mean()","69ee0610":"train_df[[\"Parch\", \"Survived\"]].groupby(\"Parch\").mean()","82a9f73b":"ax=train_df[['Parch','Survived']].groupby('Parch').mean().plot.line(figsize=(8,4))\nax.set_ylabel('Survival')\nsns.despine()","259889e8":"ax=train_df[['SibSp','Survived']].groupby('SibSp').mean().plot.line(figsize=(8,4))\nax.set_ylabel('Survival')\nsns.despine()","8df8da84":"a = sns.FacetGrid(train_df, col = 'Survived', hue = \"Sex\")\na.map(sns.distplot, \"Age\")","512dd09c":"a = sns.FacetGrid(train_df, col  = 'Pclass', row =\"Survived\",  hue = \"Sex\")\na.map(sns.distplot, \"Age\")","0755cc58":"a = sns.FacetGrid(train_df, col  = 'Pclass', row =\"Survived\",  hue = \"Sex\")\na.map(sns.distplot, \"Fare\")","c768a2c4":"a = sns.FacetGrid(train_df, col = \"Embarked\")\na.map(sns.pointplot, \"Pclass\", \"Survived\", \"Sex\")\na.add_legend()","abd132a4":"train_df.groupby([\"Embarked\", \"Sex\"])[\"Embarked\"].count()","4aa24ef4":"pd.crosstab(train_df.Pclass, train_df.Survived, margins = True). style. background_gradient(cmap = 'summer_r')","5e6171a3":"pd.crosstab([train_df.Sex,train_df.Survived],train_df.Pclass,margins=True).style.background_gradient(cmap='summer_r')","a24e39b2":"title = []\ntitle = train_df.Name.str.extract('([A-Za-z]+)\\.')\ntrain_df['title'] = title","d62d45ae":"print(title[0].value_counts())\n","b7a5c41a":"train_df[[\"title\",\"Age\",\"Sex\"]].groupby([\"title\",\"Sex\"]).mean()","54c9fafc":"# to train, test set\nfor dataset in combine:\n    title = dataset.Name.str.extract('([A-Za-z]+)\\.')\n    dataset['title'] = title\n    tt = pd.DataFrame(title[0].value_counts())\n    other_list = tt.index.values[4:]\n    #other_list = ['Dr', 'Rev', 'Mlle', 'Major', 'Col','Jonkheer', 'Don', 'Lady', 'Countess', 'Mme', 'Ms', 'Sir', 'Capt']\n\n    other = pd.DataFrame()\n    for i in other_list:\n        other = other.append(dataset[dataset[\"title\"] == i])\n    # miss 3\/4 position is 30\n        for i in other.index.values:\n            if other.Sex[i] == \"male\":\n                other.title[i] = \"Mr\"\n            elif other.Age[i] <30:\n                other.title[i] = \"Miss\"\n            else:\n                other.title[i] = \"Mrs\"\n    dataset.loc[other.index.values] = other","0e1b692a":"# to train, test set\nfor dataset in combine:\n    dataset['Sex'].replace([\"male\",\"female\"], [0,1], inplace =True)\n    dataset['title'].replace([\"Mr\",\"Mrs\",\"Miss\",\"Master\"],[0,1,2,3], inplace =True)","5b353ee7":"# to train, test set\nfor dataset in combine:\n    dataset.Embarked.fillna(\"S\",inplace = True)","841e3ec4":"train_df[[\"title\",\"Age\",\"Sex\"]].groupby([\"title\",\"Sex\"]).agg([\"mean\",\"median\",\"min\",\"max\"])","0326dfd3":"# to train, test set\nfor dataset in combine:\n    for i in range(4):\n        dataset.loc[(dataset.Age.isnull())&(dataset.title == i),'Age'] = dataset[dataset.title == i][\"Age\"].mean()\n","e4436745":"# to train, test set\nfor dataset in combine:\n    dataset[\"Family\"] = dataset.SibSp + dataset.Parch\n    dataset['Alone']=0\n    dataset.loc[dataset.Family==0,'Alone']=1","2d94bb99":"# to train, test set\nfor dataset in combine:\n    for i in range(4):\n        dataset.loc[(dataset.Age.isnull())&(dataset.title == i),'Age'] = dataset[dataset.title == i][\"Age\"].mean()\n        ","3d24b89e":"pd.cut(train_df.Age,7).value_counts()","1c7140f5":"#choose this\npd.cut(train_df.Age,8).value_counts()","587dd25b":"# to train, test set\nfor dataset in combine:\n    dataset[\"ageband\"] = pd.cut(dataset.Age,8, labels = [0,1,2,3,4,5,6,7])\n    dataset.ageband = dataset.ageband.astype(int)","e41f9580":"# to train, test set\nfor dataset in combine:\n    for i in range(4):\n        dataset.loc[(dataset.Fare.isnull())&(dataset.Pclass == i),'Fare'] = dataset[dataset.Pclass == i][\"Fare\"].median()\n","28e0b0fc":"train_cut_fare = train_df.loc[(train_df.Fare<55)&train_df.Fare>0]","4f146057":"train_cut_fare[[\"Pclass\",\"Fare\",\"Embarked\"]].groupby(\"Pclass\").agg([\"count\",\"mean\",\"median\",\"min\",\"max\"])","e248f703":"train_df.loc[(train_df.Fare>80)&train_df.Fare>0].count()","98df0b07":"sns.factorplot(x = \"Embarked\", y = \"Fare\", col = \"Pclass\", data = train_cut_fare, kind = \"box\")","2db58452":"sns.factorplot(x = \"Pclass\", y = \"Fare\", col = \"Embarked\", data = train_cut_fare, kind = \"box\")","848a263e":"train_df['fareband'] = pd.qcut(train_df.Fare,7)\ntrain_df.groupby(['fareband'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')","e58ad51d":"train_df['fareband'] = pd.qcut(train_df.Fare,6)\ntrain_df.groupby(['fareband'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')","0061e97b":"# to train, test set\nfor dataset in combine:\n    dataset['fareband'] = pd.qcut(dataset.Fare,6,labels = [0,1,2,3,4,5])\n    dataset.fareband = dataset.fareband.astype(int)\n","c08e0122":"# to train, test set\nfor dataset in combine:\n    dataset['Embarked'].replace([\"S\",\"C\",\"Q\"], [0,1,2], inplace =True)","15c0517f":"# to train, test set\nfor dataset in combine:\n    dataset.drop([\"PassengerId\", \"Name\", \"Age\", \"SibSp\", \"Parch\", \"Ticket\", \"Fare\",\"Cabin\"],axis =1, inplace =True)\n","6064d677":"data = train_df.copy()\ndata.info()","9802941d":"colormap = sns.diverging_palette(220, 10, as_cmap = True)\nsns.heatmap(data.corr(),annot=True,cmap=colormap,linewidths=0.2) #data.corr()-->correlation matrix\nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.show()","89b0797c":"#importing all the required ML packages\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn import svm #support vector Machine\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.naive_bayes import GaussianNB #Naive bayes\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix","14d2111c":"train,test=train_test_split(data,test_size=0.3,random_state=0,stratify=data['Survived'])\ntrain_X=train[train.columns[1:]]\ntrain_Y=train[train.columns[:1]]\ntest_X=test[test.columns[1:]]\ntest_Y=test[test.columns[:1]]\nX=data[data.columns[1:]]\nY=data['Survived']","144c9063":"model = LogisticRegression()\nmodel.fit(train_X, train_Y)\nprediction3 = model.predict(test_X)\nprint(\"Accuracy of the LosgsticRegression is \", metrics.accuracy_score(prediction3, test_Y))","0bcd14b2":"model=DecisionTreeClassifier()\nmodel.fit(train_X,train_Y)\nprediction4=model.predict(test_X)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction4,test_Y))","8c7d230b":"model=RandomForestClassifier(n_estimators=100)\nmodel.fit(train_X,train_Y)\nprediction7=model.predict(test_X)\nprint('The accuracy of the Random Forests is',metrics.accuracy_score(prediction7,test_Y))","af7d17b2":"from sklearn.model_selection import KFold \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nkfold = KFold(n_splits =10, random_state = 22)\nxyz = []\naccuracy = []\nstd = []\nclassifiers = [\"Logistic Regression\", \"Decision Tree\",\"Random Forest\"]\nmodels = [LogisticRegression(), DecisionTreeClassifier(), RandomForestClassifier(n_estimators = 100)]\nfor i in models:\n    model = i\n    cv_result = cross_val_score(model, X, Y, cv = kfold, scoring = \"accuracy\")\n    xyz.append(cv_result.mean())\n    std.append(cv_result.std())\n    accuracy.append(cv_result)\nnew_model_dataframe2 = pd.DataFrame({\"CV Mean\" : xyz, \"Std\": std}, index = classifiers)\nnew_model_dataframe2","6d1eebb7":"from sklearn.model_selection import GridSearchCV\nn_estimators = range(500,1500,100)\nhyper = {\"n_estimators\" : n_estimators}\ngd = GridSearchCV(estimator = RandomForestClassifier(random_state=0), param_grid= hyper, verbose = True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","a84b241e":"from sklearn.ensemble import VotingClassifier\nensemble_lin_rbf = VotingClassifier(estimators = [(\"RF\", RandomForestClassifier(n_estimators = 900,\n                                                                               random_state =0)),\n                                                   (\"LR\", LogisticRegression(C = 0.05)),\n                                                  (\"DT\", DecisionTreeClassifier(random_state = 0))],\n                                   voting = \"soft\").fit(train_X, train_Y)\nprint('The accuracy for ensembled model is:',ensemble_lin_rbf.score(test_X,test_Y))\ncross=cross_val_score(ensemble_lin_rbf,X,Y, cv = 10,scoring = \"accuracy\")\nprint('The cross validated score is',cross.mean())","2a0eb53c":"from sklearn.ensemble import GradientBoostingClassifier\ngrad=GradientBoostingClassifier(n_estimators=500,random_state=0,learning_rate=0.1)\nresult=cross_val_score(grad,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for Gradient Boosting is:',result.mean())","1e98e018":"import xgboost as xg\nxgboost=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\nresult=cross_val_score(xgboost,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for XGBoost is:',result.mean())","a65d1009":"f,ax=plt.subplots(2,2,figsize=(15,12))\n\nmodel = LogisticRegression()\nmodel.fit(train_X, train_Y)\nmodel_coef = model.coef_\npd.Series(abs(model_coef.flatten()), X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,0])\nax[0,0].set_title('Feature Importance in LogisticRegression')\n\nmodel=DecisionTreeClassifier()\nmodel.fit(train_X,train_Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,1])\nax[0,1].set_title('Feature Importance in DecisionTreeClassifier')\n\nmodel=RandomForestClassifier(n_estimators=500,random_state=0)\nmodel.fit(train_X,train_Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,0])\nax[1,0].set_title('Feature Importance in Random Forests')\n\nmodel=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,1],)\nax[1,1].set_title('Feature Importance in XgBoost')\nplt.show()","9c5dda60":"without Embarked C female survived rate is better","d1ba7578":"Survived rate of female is better than male","fade9879":"EDA","37b5323c":"Check without Mr, Miss, Mrs, Master\n\nALL of male replace to Mr\n\nyoung female be Miss\n\nold female be Mrs\n\nDr has one female(Mrs)","b42e438a":"Family, Alone","90004ea3":"Deal Null value","2fe2eae0":"Embarked","c6cbde5d":"20s, 30s are popular","c4b943f3":"Before think\nThe Fare is different from where they are aboard and which class\n\nAnd The Titanic move S->C->Q place \n\nbut any different from place\n\nMost of people who aboard at Q are Class 3","527053d7":"\nReference\nTitanic Data Science Solutions\n\nhttps:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n\nA Data Science Framework: To Achieve 99% Accuracy\n\nhttps:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\n\nEDA To Prediction(DieTanic)\n\nhttps:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic","d994f0b2":"GridSearchCV","5253493e":"Around 50 is stq and above 1 class 3\/4 fair","ddaa5819":"what is Fare == 0\n\nsomeone cheating in ship? or something data error?\n\nThe samething\nAll of them are male\/Alone\/S\nSome of them are class 1 \n","8a8d4e11":"Age 177 null\n\nCabin 687 null\n\nEmbarked 2 null","93f3946e":"1 class most higher survived\n3 class most lower survived","cbd1c0d6":"S is much more than other","ed9e3edc":"MISS = Girl under 12 years,\n\nMSTR = Boy under 12years,\n\nINF = Under 2 years.\n\nhttps:\/\/english.stackexchange.com\/questions\/320719\/abbreviation-for-master\n\nmaster is meaning of young boy","4a9db8a7":"age","b7e2704b":"Age band\n\nif age band size 5 - 16~32 is 502 values \n\nI think it's too much \n\nso make age band bigger \n\n6 to 7 isn't good\n\n8 can be cut 10\/20\/30\/40\/50\/60\/70\n\nSo I choose 8\n\n","edc92dbc":"1 class survived rate is over 50% and female 1&2 class survived rate is over 90%","356e692d":"3 class is much more than others"}}