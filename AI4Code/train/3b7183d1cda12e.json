{"cell_type":{"92d09c4c":"code","8c2cf122":"code","a8e0d5ff":"code","77bf8fbc":"code","d279cc58":"code","fb8ba69c":"code","dcdf67f6":"code","e97848f9":"code","ba6042c5":"code","06c8d13c":"code","0a717630":"code","4122acb1":"code","0b00f0d9":"code","664661fb":"code","314df18f":"code","f227e296":"code","392c167f":"code","e631c125":"code","3c98c01d":"code","470600e0":"code","fe98e71a":"code","cd0b64f8":"code","09cbf9ef":"code","520a024d":"code","e27b960f":"code","b4bdf768":"code","f1aa399d":"code","ed2ffb0b":"code","f0a4ff1f":"code","762d6d81":"code","bca89ec5":"code","787be24a":"code","796806ac":"code","4175e01a":"code","bcf7c7cc":"code","4ede0b77":"code","df7c876d":"code","d11d0976":"code","862b091f":"code","f40fd6dc":"code","44762a6b":"code","09c6420c":"code","117d2c2c":"code","60ac5b64":"code","34981288":"code","62f7619d":"code","7a7219e5":"code","ffc29477":"code","9533c458":"code","b02aabcd":"code","3fa9cace":"code","2ef6d749":"code","4e73b774":"code","a15c24e8":"code","ff62d957":"code","c4c09ee3":"code","f84b3c27":"code","7dbae53a":"code","343b1a5f":"code","c796dfed":"code","ac085cb1":"code","50e60968":"code","74d78380":"code","35a747ce":"code","6be9a861":"code","af363ab0":"code","64652877":"code","60050810":"code","339132d5":"code","9368cc5d":"code","2274170f":"code","922e128f":"code","02d17dc1":"code","5f7a6660":"code","55998c63":"code","441b81a0":"code","574c1c74":"code","6ace966e":"code","7472ecc5":"code","b7b649ee":"code","df1460e3":"code","b01e0688":"code","74d2451a":"code","1bb1c1b4":"code","81c3e14f":"code","d5104cbc":"code","6420a1e8":"code","7e1dc091":"code","2cdb7b66":"code","a63001fa":"code","8c419b47":"code","62e6ed16":"code","70bb35d1":"code","e03429a9":"code","74babbec":"code","49f268df":"code","7377d404":"code","440dc754":"code","7318d671":"code","ede27537":"code","2e957e74":"code","cbbe05d4":"code","55014440":"code","c41f475f":"code","77d2a61f":"code","9acbf7cb":"code","9c684552":"code","10200a90":"code","b31b4885":"code","23593b0f":"code","354990fd":"code","1cbdb927":"code","c6731c6a":"code","5f5f9a5b":"code","e072c2a5":"code","aef63d79":"code","88abde9e":"code","bf9cf341":"code","73028f0e":"code","e64b14b0":"code","4b55e8b3":"code","17ded144":"code","e217a2b6":"code","ba515073":"code","68350a95":"code","d231ff5a":"code","d640d3c7":"code","da149751":"code","792ba0db":"code","60b06244":"code","1bc71eb5":"markdown","21738822":"markdown","b8e6c345":"markdown","21a88fda":"markdown","e5368a0b":"markdown","542d7e47":"markdown","b3f5bcdb":"markdown","aa593cb9":"markdown","6899c972":"markdown","3eb8b366":"markdown","4c792e7c":"markdown","5e230370":"markdown","4d025b16":"markdown","028d0b2c":"markdown","405bb2cd":"markdown","bd2d8882":"markdown","1887b679":"markdown","09b8a623":"markdown","40ffb0f7":"markdown","e38b3025":"markdown","7fcb2048":"markdown","0461ddb4":"markdown","06927a0c":"markdown","ba52d541":"markdown","cc405f1c":"markdown","806603ac":"markdown","bfd3404f":"markdown","a638dd45":"markdown","f4814e75":"markdown","528cc5cb":"markdown","1f0ba354":"markdown","53e23bff":"markdown","291fc3be":"markdown","d08d5349":"markdown","90554e45":"markdown","60cbe0db":"markdown","2727b665":"markdown","90443ad3":"markdown","b7e94def":"markdown","90f9f712":"markdown","e56bde8f":"markdown","58f7ed28":"markdown","6957d795":"markdown","00269f27":"markdown"},"source":{"92d09c4c":"# import 'Pandas' \nimport pandas as pd \n\n# import 'Numpy' \nimport numpy as np\n\n# import subpackage of Matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n# import 'Seaborn' \nimport seaborn as sns\n\n# to suppress warnings \nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n# display all columns of the dataframe\npd.options.display.max_columns = None\n\n# display all rows of the dataframe\npd.options.display.max_rows = None\n \n# to display the float values upto 6 decimal places     \npd.options.display.float_format = '{:.6f}'.format\n\nimport statsmodels\nimport statsmodels.api as sm\n\n# import train-test split \nfrom sklearn.model_selection import train_test_split\n\n# import StandardScaler to perform scaling\nfrom sklearn.preprocessing import StandardScaler \n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n\n# import various functions from sklearn\nfrom sklearn import metrics\nfrom sklearn.metrics import cohen_kappa_score\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom xgboost import XGBClassifier\n\nplt.rcParams['figure.figsize']=[25,20]\n\nfrom sklearn import tree\nimport pydotplus\nfrom IPython.display import Image\n\nfrom sklearn.tree import plot_tree","8c2cf122":"df=pd.read_csv('airline_passenger_satisfaction.csv')","a8e0d5ff":"df.head()","77bf8fbc":"df.shape","d279cc58":"#The dataset has 129880 rows and 24 columns.","fb8ba69c":"df.info()","dcdf67f6":"plt.rcParams['figure.figsize']=[25,20]\ndf.hist()","e97848f9":"df.std()","ba6042c5":"df.skew()","06c8d13c":"df['satisfaction'].value_counts()","0a717630":"plt.rcParams['figure.figsize']=[10,7]\nsns.countplot(df['satisfaction'])\nplt.show()","4122acb1":"# The Unnamed column(column of IDs) is insignificant, hence dropping it.\ndf.drop('Unnamed: 0',inplace=True,axis=1)","0b00f0d9":"df.isnull().sum()","664661fb":"df['arrival_delay_in_minutes'].median()","314df18f":"df['arrival_delay_in_minutes'].fillna(0,inplace=True)","f227e296":"df.isnull().sum()","392c167f":"code={'neutral or dissatisfied':0,'satisfied':1}\ndf['satisfaction']=df['satisfaction'].map(code)","e631c125":"df.drop_duplicates(subset=None, inplace=True)","3c98c01d":"df.shape","470600e0":"plt.figure(figsize=(20,10))\ndf.boxplot()\nplt.xticks(rotation=45)\nplt.show()","fe98e71a":"df.boxplot(column=['departure_delay_in_minutes', 'arrival_delay_in_minutes'])","cd0b64f8":"df_num=df.select_dtypes(include=np.number)","09cbf9ef":"Q1 = df_num.drop('satisfaction',axis=1).quantile(0.25)\nQ3 = df_num.drop('satisfaction',axis=1).quantile(0.75)\nIQR = Q3 - Q1\n\nprint(IQR)","520a024d":"df = df[~((df_num< (Q1 - 1.5 * IQR)) |(df_num > (Q3 + 1.5 * IQR))).any(axis=1)]\n","e27b960f":"df.shape","b4bdf768":"df.reset_index(drop=True,inplace=True) # reseting the index","f1aa399d":"# There are 93648 rows after removal of outliers.","ed2ffb0b":"plt.figure(figsize=(25,10))\ncor=df.corr()\nsns.heatmap(cor,annot=True)\nplt.show()","f0a4ff1f":"df_cat=df.select_dtypes(include=object)","762d6d81":"df_cat.columns","bca89ec5":"df_encoded=pd.get_dummies(data=df,columns=df_cat.columns,drop_first=True)","787be24a":"df_encoded.head()","796806ac":"df_encoded.shape","4175e01a":"scaler=StandardScaler()\n\ndftoscale=df_encoded.select_dtypes(include=np.number)\ndftoscale.columns","bcf7c7cc":"# removing the categorical columns and scaling\ndftoscale.drop(['satisfaction', 'Gender_Male', 'customer_type_disloyal Customer',\n       'type_of_travel_Personal Travel', 'customer_class_Eco',\n       'customer_class_Eco Plus'],axis=1,inplace=True)\n\nvals=scaler.fit_transform(dftoscale)\ndfvals=pd.DataFrame(vals,columns=dftoscale.columns)","4ede0b77":"dffinal=pd.concat([dfvals,df_encoded[['Gender_Male', 'customer_type_disloyal Customer',\n       'type_of_travel_Personal Travel', 'customer_class_Eco',\n       'customer_class_Eco Plus','satisfaction']]],axis=1)","df7c876d":"dffinal.head(5)","d11d0976":"y=dffinal['satisfaction']\nX=dffinal.drop('satisfaction',axis=1)","862b091f":"vif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","f40fd6dc":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1)","44762a6b":"#adding constant","09c6420c":"X_train=sm.add_constant(X_train)\nX_test=sm.add_constant(X_test)","117d2c2c":"X_train.head()","60ac5b64":"logreg = sm.Logit(y_train, X_train).fit()\nlogreg.summary()","34981288":"y_train_prb=logreg.predict(X_train)\ny_pred_train=[ 0 if x < 0.5 else 1 for x in y_train_prb]\nprint(classification_report(y_train,y_pred_train))","62f7619d":"y_pred_prob=logreg.predict(X_test)\ny_pred = [ 0 if x < 0.5 else 1 for x in y_pred_prob]\nprint(classification_report(y_test,y_pred))","7a7219e5":"y=dffinal['satisfaction']\nX=dffinal.drop(['satisfaction','departure_delay_in_minutes','inflight_entertainment','flight_distance'],axis=1)","ffc29477":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1)","9533c458":"X_train=sm.add_constant(X_train)\nX_test=sm.add_constant(X_test)","b02aabcd":"logreg = sm.Logit(y_train, X_train).fit()\nlogreg.summary()","3fa9cace":"y_train_prb=logreg.predict(X_train)\ny_pred_train=[ 0 if x < 0.5 else 1 for x in y_train_prb]\nprint(classification_report(y_train,y_pred_train))","2ef6d749":"# after removing the insignificant variables , we find that the accuracy remains the same.","4e73b774":"def get_report(model):\n    y_pred_prob=model.predict(X_test)\n    \n    y_pred=[ 0 if x < 0.5 else 1 for x in y_pred_prob]\n    \n    return(classification_report(y_test,y_pred))","a15c24e8":"# function for confusion matrix \ndef conf_mat(model):\n    y_pred_prob=model.predict(X_test)\n    \n    y_pred=[ 0 if x < 0.5 else 1 for x in y_pred_prob]\n    \n    cm=confusion_matrix(y_test,y_pred)\n    \n    conf_matrix=pd.DataFrame(cm,columns=['Predicted_0','Predicted_1'],index=['Actual_0','Actual_1'])\n    \n    sns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = ListedColormap(['lightskyblue']), cbar = False, \n                linewidths = 0.1, annot_kws = {'size':25})\n\n    \n    plt.xticks(fontsize = 20)\n\n\n    plt.yticks(fontsize = 20)\n\n\n    plt.show()","ff62d957":"# define a function to plot the ROC curve and print the ROC-AUC score\ndef plot_roc(model):\n\n\n    \n    # predict the probability of target variable using X_test\n    # consider the probability of positive class by subsetting with '[:,1]'\n    y_pred_prob = model.predict(X_test)\n    \n    # the roc_curve() returns the values for false positive rate, true positive rate and threshold\n    # pass the actual target values and predicted probabilities to the function\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n    # plot the ROC curve\n    plt.plot(fpr, tpr)\n\n    # set limits for x and y axes\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n\n    # plot the straight line showing worst prediction for the model\n    plt.plot([0, 1], [0, 1],'r--')\n\n    # add plot and axes labels\n    # set text size using 'fontsize'\n    plt.title('ROC curve for Passenger Satisfaction Prediction Classifier', fontsize = 15)\n    plt.xlabel('False positive rate (1-Specificity)', fontsize = 15)\n    plt.ylabel('True positive rate (Sensitivity)', fontsize = 15)\n\n    # add the AUC score to the plot\n    # 'x' and 'y' gives position of the text\n    # 's' is the text \n    # use round() to round-off the AUC score upto 4 digits\n    plt.text(x = 0.02, y = 0.9, s = ('AUC Score:',round(metrics.roc_auc_score(y_test, y_pred_prob),4)))\n\n    # plot the grid\n    plt.grid(True)","c4c09ee3":"def kappa(model):\n    y_pred_prob=model.predict(X_test)\n    \n    y_pred=[ 0 if x < 0.5 else 1 for x in y_pred_prob]\n# print the kappa value\n    return(cohen_kappa_score(y_test,y_pred))","f84b3c27":"# create an empty dataframe to store the scores for various algorithms\nscore_card = pd.DataFrame(columns=['Model Name','AUC Score', 'Precision Score', 'Recall Score',\n                                       'Accuracy Score', 'Kappa Score', 'f1-score'])\n\n# append the result table for all performance scores\n# performance measures considered for model comparision are 'AUC Score', 'Precision Score', 'Recall Score','Accuracy Score',\n# 'Kappa Score', and 'f1-score'\n# compile the required information in a user defined function \ndef update_score_card(name,model,X_test,y_test):\n    \n    # let 'y_pred_prob' be the predicted values of y\n    y_pred_prob = model.predict_proba(X_test)[:,1]\n\n    # convert probabilities to 0 and 1 using 'if_else'\n    y_pred = model.predict(X_test)\n    \n    # assign 'score_card' as global variable\n    global score_card\n\n    # append the results to the dataframe 'score_card'\n    # 'ignore_index = True' do not consider the index labels\n    score_card = score_card.append({'Model Name':name,\n                                    'AUC Score' : metrics.roc_auc_score(y_test, y_pred_prob),\n                                    'Precision Score': metrics.precision_score(y_test, y_pred),\n                                    'Recall Score': metrics.recall_score(y_test, y_pred),\n                                    'Accuracy Score': metrics.accuracy_score(y_test, y_pred),\n                                    'Kappa Score':metrics.cohen_kappa_score(y_test, y_pred),\n                                    'f1-score': metrics.f1_score(y_test, y_pred)}, \n                                    ignore_index = True)","7dbae53a":"conf_mat(logreg)","343b1a5f":"# True Negatives are denoted by 'TN'\n# Actual 'O' values which are classified correctly\nTN = cm[0,0]\n\n# True Positives are denoted by 'TP'\n# Actual '1' values which are classified correctly\nTP = cm[1,1]\n\n# False Positives are denoted by 'FP'\n# it is the type 1 error\n# Actual 'O' values which are classified wrongly as '1'\nFP = cm[0,1]\n\n# False Negatives are denoted by 'FN'\n# it is the type 2 error\n# Actual '1' values which are classified wrongly as '0'\nFN = cm[1,0]","c796dfed":"print(get_report(logreg))","ac085cb1":"plot_roc(logreg)","50e60968":"kappa(logreg)","74d78380":"fpr,tpr,thresholds=roc_curve(y_test,y_pred_prob)","35a747ce":"# create a dataframe to store the values for false positive rate, true positive rate and threshold\nyoudens_table = pd.DataFrame({'TPR': tpr,\n                             'FPR': fpr,\n                             'Threshold': thresholds})\n\n# calculate the difference between TPR and FPR for each threshold and store the values in a new column 'Difference'\nyoudens_table['Difference'] = youdens_table.TPR - youdens_table.FPR\n\n# sort the dataframe based on the values of difference \n# 'ascending = False' sorts the data in descending order\n# 'reset_index' resets the index of the dataframe\n# 'drop = True' drops the previous index\nyoudens_table = youdens_table.sort_values('Difference', ascending = False).reset_index(drop = True)\n\n# print the first five observations\nyoudens_table.head()","6be9a861":"y_pred_youden = [ 0 if x < 0.524 else 1 for x in y_pred_prob]","af363ab0":"# create a confusion matrix\n# pass the actual and predicted target values to the confusion_matrix()\ncm = confusion_matrix(y_test, y_pred_youden)\n\nconf_matrix = pd.DataFrame(data = cm,columns = ['Predicted:0','Predicted:1'], index = ['Actual:0','Actual:1'])\n\n\nsns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = ListedColormap(['lightskyblue']), cbar = False, \n            linewidths = 0.1, annot_kws = {'size':25})\n\n# set the font size of x-axis ticks using 'fontsize'\nplt.xticks(fontsize = 20)\n\n# set the font size of y-axis ticks using 'fontsize'\nplt.yticks(fontsize = 20)\n\n# display the plot\nplt.show()","64652877":"# calculate various performance measures\nacc_table = classification_report(y_test, y_pred_youden)\n\n# print the table\nprint(acc_table)","60050810":"# compute the kappa value\nkappa = cohen_kappa_score(y_test, y_pred_youden)\n\n# print the kappa value\nprint('kappa value:',kappa)","339132d5":"# define a function to calculate the total_cost for a cut-off value\n# pass the actual values of y, predicted probabilities of y, cost for FN and FP\ndef calculate_total_cost(actual_value, predicted_value, cost_FN, cost_FP):\n\n    # pass the actual and predicted values to calculate the confusion matrix\n    cm = confusion_matrix(actual_value, predicted_value)           \n    \n    # create an array of the confusion matrix\n    cm_array = np.array(cm)\n    \n    # return the total_cost\n    return cm_array[1,0] * cost_FN + cm_array[0,1] * cost_FP\n\n# create an empty dataframe to store the cost for different probability cut-offs\ndf_total_cost = pd.DataFrame(columns = ['cut-off', 'total_cost'])\n\n# initialize i to '0' corresponding to the 1st row in the dataframe\ni = 0\n\n# assign the costs 0.128 and 0.122 to False Negatives and False Positives respectively\n# add the obtained 'cut_off' and 'total_cost' at the ith index of the dataframe\nfor cut_off in range(10, 100):\n    total_cost = calculate_total_cost(y_test,  y_pred_prob.map(lambda x: 1 if x > (cut_off\/100) else 0), 0.128, 0.122) \n    df_total_cost.loc[i] = [(cut_off\/100), total_cost] \n    \n    # increment the value of 'i' for each row index in the dataframe 'df_total_cost'\n    i += 1","9368cc5d":"df_total_cost.sort_values('total_cost', ascending = True).head(10)","2274170f":"# define a function to plot the ROC curve and print the ROC-AUC score\ndef plot_roc__(model):\n    \n    # predict the probability of target variable using X_test\n    # consider the probability of positive class by subsetting with '[:,1]'\n    y_pred_prob = model.predict_proba(X_test)[:,1]\n    \n    # the roc_curve() returns the values for false positive rate, true positive rate and threshold\n    # pass the actual target values and predicted probabilities to the function\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n    # plot the ROC curve\n    plt.plot(fpr, tpr)\n\n    # set limits for x and y axes\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n\n    # plot the straight line showing worst prediction for the model\n    plt.plot([0, 1], [0, 1],'r--')\n\n    # add plot and axes labels\n    # set text size using 'fontsize'\n    plt.title('ROC curve for Passenger Satisfaction Prediction Classifier', fontsize = 15)\n    plt.xlabel('False positive rate (1-Specificity)', fontsize = 15)\n    plt.ylabel('True positive rate (Sensitivity)', fontsize = 15)\n\n    # add the AUC score to the plot\n    # 'x' and 'y' gives position of the text\n    # 's' is the text \n    # use round() to round-off the AUC score upto 4 digits\n    plt.text(x = 0.02, y = 0.9, s = ('AUC Score:',round(roc_auc_score(y_test, y_pred_prob),4)))\n\n    # plot the grid\n    plt.grid(True)","922e128f":"gnb = GaussianNB()\n\ngnb_model = gnb.fit(X_train, y_train)\n","02d17dc1":"conf_mat(gnb_model)","5f7a6660":"print(classification_report(y_train,gnb_model.predict(X_train)))","55998c63":"print(get_report(gnb_model))","441b81a0":"plot_roc__(gnb_model)","574c1c74":"update_score_card(\"Naive Bayes\",gnb_model,X_test,y_test)\n\nscore_card","6ace966e":"tuned_paramaters = {'n_neighbors': np.arange(1,13, 2),\n                   'metric': ['hamming','euclidean','manhattan','chebyshev']}\n \n# instantiate the 'KNeighborsClassifier' \nknn_classification = KNeighborsClassifier()\n\n\nknn_grid = GridSearchCV(estimator = knn_classification, \n                        param_grid = tuned_paramaters, \n                        cv = 3, \n                        scoring = 'roc_auc')\n\nknn_grid.fit(X_train, y_train)\n\n\nprint('Best parameters for KNN Classifier: ', knn_grid.best_params_, '\\n')","7472ecc5":"accuracy_rate = []\n\n# use for loop to build a knn model for each K\nfor i in np.arange(1,13,2):\n     \n    knn = KNeighborsClassifier(i, metric = 'manhattan')\n\n    score = cross_val_score(knn, X_train, y_train, cv = 3)\n    \n    # calculate the mean score\n    score = score.mean()\n    \n    # compute error rate \n    accuracy_rate.append(score)\n\n# plot the error_rate for different values of K \nplt.plot(range(1,13,2), accuracy_rate)\n\n# add plot and axes labels\n# set text size using 'fontsize'\nplt.title('Accuracy Rate', fontsize = 15)\nplt.xlabel('K', fontsize = 15)\nplt.ylabel('Accuracy Rate', fontsize = 15)\n\nplt.xticks(np.arange(1, 13, step = 2))\nplt.axvline(x = 11, color = 'red')\n\n# display the plot\nplt.show()","b7b649ee":"knn_classification = KNeighborsClassifier(n_neighbors = 11,metric= 'manhattan')\n\nknn_model = knn_classification.fit(X_train, y_train)","df1460e3":"conf_mat(knn_model)","b01e0688":"print(classification_report(y_train,knn_model.predict(X_train)))","74d2451a":"print(get_report(knn_model))","1bb1c1b4":"plot_roc__(knn_model)","81c3e14f":"update_score_card(\"KNN(manhattan,k=11)\",knn_model,X_test,y_test)\n\nscore_card","d5104cbc":"tuned_para=[{'criterion':['gini','entropy'],'max_depth':[2,4,6],'min_samples_split':[4,5,6,7]}]\n\ndec=DecisionTreeClassifier()\n\ndec_grid=GridSearchCV(estimator=dec,\n                      param_grid=tuned_para,\n                      scoring='roc_auc'\n                      ,cv=5)\n\ndec_grid.fit(X_train,y_train)\n\nprint(dec_grid.best_params_)","6420a1e8":"dec=DecisionTreeClassifier(criterion='entropy',max_depth=6,min_samples_split=4)\ndec.fit(X_train,y_train)\ny_pred=dec.predict(X_test)\n\nfrom sklearn.tree import plot_tree\nplot_tree(dec,filled=True)\nplt.show()","7e1dc091":"conf_mat(dec)","2cdb7b66":"print(classification_report(y_train,dec.predict(X_train)))","a63001fa":"print(get_report(dec))","8c419b47":"plot_roc__(dec)","62e6ed16":"update_score_card(\"DecisionTreeClassifier(entropy,max_depth: 6,min_samples_split: 4)\",dec,X_test,y_test)\n\nscore_card","70bb35d1":"tunned_para=[{'n_estimators':[20,30,40,50],'criterion':['gini','entropy'],'max_depth':[2,3,4,5],'min_samples_split':[4,5,6]}]\n\nrfc=RandomForestClassifier()\n\nrfc_grid=GridSearchCV(estimator=rfc,param_grid=tunned_para,scoring='roc_auc',cv=3)\n\nrfc_grid.fit(X_train,y_train)\n\nprint(rfc_grid.best_params_)","e03429a9":"rfc=RandomForestClassifier()\n\nrfc_model=RandomForestClassifier(criterion= 'gini', max_depth= 5, min_samples_split= 6, n_estimators= 50).fit(X_train,y_train)\n\nscore=cross_val_score(estimator=rfc,X=X_train,y=y_train,cv=3,scoring='roc_auc')\n\nprint('average roc_auc is',score.mean())","74babbec":"conf_mat(rfc_model)","49f268df":"plot_roc__(rfc_model)","7377d404":"print(classification_report(y_train,rfc_model.predict(X_train)))","440dc754":"print(get_report(rfc_model))","7318d671":"update_score_card(\"RandomForestClassifier(gini,max_depth: 5, min_samples_split: 6, n_estimators: 50)\",rfc_model,X_test,y_test)\n\nscore_card","ede27537":"important_features = pd.DataFrame({'Features': X_train.columns, \n                                   'Importance': rfc_model.feature_importances_})\n\n# sort the dataframe in the descending order according to the feature importance\nimportant_features = important_features.sort_values('Importance', ascending = False)\n\n# create a barplot to visualize the features based on their importance\nsns.barplot(x = 'Importance', y = 'Features', data = important_features)\n\n# add plot and axes labels\n# set text size using 'fontsize'\nplt.title('Feature Importance', fontsize = 15)\nplt.xlabel('Importance', fontsize = 15)\nplt.ylabel('Features', fontsize = 15)\n\n# display the plot\nplt.show()","2e957e74":"tuning_parameters = {'learning_rate': [0.5,0.6,0.7,0.8,0.9,1],\n                     'n_estimators': [30,40,50,60,70]}\n\nada_boost=AdaBoostClassifier()\n\nada_grid = GridSearchCV(estimator = ada_boost, param_grid = tuning_parameters, cv = 5, scoring = 'roc_auc')\n\n# fit the model on X_train and y_train using fit()\nada_grid.fit(X_train, y_train)\n\n# get the best parameters\nprint('Best parameters for AdaBoost classifier: ', ada_grid.best_params_, '\\n')","cbbe05d4":"ada_boost=AdaBoostClassifier(learning_rate=1,n_estimators=70)\n\nada_boost.fit(X_train,y_train)","55014440":"conf_mat(ada_boost)","c41f475f":"print(classification_report(y_train,ada_boost.predict(X_train)))","77d2a61f":"print(get_report(ada_boost))","9acbf7cb":"plot_roc__(ada_boost)","9c684552":"update_score_card(\"AdaBoost(learning_rate=1, n_estimators=70)\",ada_boost,X_test,y_test)\n\nscore_card","10200a90":"tuning_parameters = {'learning_rate': [0.5,0.6,0.7,0.8,0.9,1],\n                     'max_depth': range(3,7),\n                     'n_estimators': [30,40,50,60,70]}\n\ngb = GradientBoostingClassifier()\n\ngb_grid = GridSearchCV(estimator = gb, param_grid = tuning_parameters, cv = 5, scoring = 'roc_auc')\n\n# fit the model on X_train and y_train using fit()\ngb_grid.fit(x_train, y_train)\n\n# get the best parameters\nprint('Best parameters for GBoost classifier: ', gb_grid.best_params_, '\\n')","b31b4885":"gb_model=GradientBoostingClassifier(learning_rate=0.5,max_depth=6,n_estimators=30)\n\ngb_model.fit(X_train,y_train)","23593b0f":"conf_mat(gb_model)","354990fd":"print(classification_report(y_train,gb_model.predict(X_train)))","1cbdb927":"print(get_report(gb_model))","c6731c6a":"plot_roc__(gb_model)","5f5f9a5b":"update_score_card(\"GradientBoosting(learning_rate=0.5, max_depth=6, n_estimators=30)\",gb_model,X_test,y_test)\n\nscore_card","e072c2a5":"tuning_parameters = {'learning_rate': [0.5,0.6,0.7,0.8,0.9,1],\n                     'max_depth': range(3,6),\n                     'n_estimators': [30,40,50],\n                    'gamma':[1,1.5,2]}\n\nxgb = XGBClassifier()\n\nxgb_grid = GridSearchCV(estimator = xgb, param_grid = tuning_parameters, cv = 5, scoring = 'roc_auc')\n\n# fit the model on X_train and y_train using fit()\nxgb_grid.fit(X_train, y_train)\n\n# get the best parameters\nprint('Best parameters for XGBoost classifier: ', xgb_grid.best_params_, '\\n')","aef63d79":"xgb_mode=XGBClassifier(learning_rate=0.5,gamma=2,n_estimators=50,max_depth=5).fit(X_train,y_train)","88abde9e":"print(classification_report(y_train,xgb_mode.predict(X_train)))","bf9cf341":"print(get_report(xgb_mode))","73028f0e":"plot_roc__(xgb_mode)","e64b14b0":"update_score_card(\"XGBClassifier(learning_rate=0.5,gamma=2,n_estimators=50,max_depth=5)\",xgb_mode,X_test,y_test)\n\nscore_card","4b55e8b3":"base_learners = [\n                 ('gnb_mod',gb_model),\n                 ('rand',rfc_model)\n                 ]\n\n# initialize stacking classifier \n# pass the base learners to the parameter, 'estimators'\n# pass the Naive Bayes model as the 'final_estimator'\/ meta model\nstack_model = StackingClassifier(estimators = base_learners, final_estimator = xgb_mode,cv=5)\n\n# fit the model on train dataset\nstack_model.fit(X_train, y_train)","17ded144":"print(classification_report(y_test,stack_model.predict(X_test)))","e217a2b6":"print(get_report(stack_model))","ba515073":"plot_roc__(stack_model)","68350a95":"update_score_card(\"Stack Model(naive bayes,randomForest,Xgb)\",stack_model,X_test,y_test)\n\nscore_card","d231ff5a":"score_card.sort_values('AUC Score',ascending=False)","d640d3c7":"from sklearn.svm import SVC\nclassifier = SVC(kernel='rbf', random_state = 1,probability=True)\nclassifier.fit(X_train,y_train)","da149751":"y_pred = classifier.predict(X_test)","792ba0db":"print(classification_report(y_test,y_pred))","60b06244":"update_score_card(\"SVM.SVC\",classifier,X_test,y_test)\n\nscore_card","1bc71eb5":"## Encoding the categorical columns","21738822":"# KNN","b8e6c345":"## Decision Tree","21a88fda":"# AIR PASSENGER SATISFACTION PREDICTION","e5368a0b":"## Checking for duplicates","542d7e47":"We see that the train and test prediction have an accuracy of 88%","b3f5bcdb":"### Youden's Table","aa593cb9":"We can see that the target variable is almost balanced.","6899c972":"We can see that the Kappa score has marginally increased.","3eb8b366":"## Importing necessary files","4c792e7c":"# Random Forest","5e230370":"### Train test splitting the data","4d025b16":"## Outlier Treatment","028d0b2c":"## Checking for correlation between features","405bb2cd":"We find that there is no multicollinearity between features.","bd2d8882":"## Defining necessary functions for further model performance analysis","1887b679":"# XG Boosting","09b8a623":"### Removing insignificant variables depending on the p value","40ffb0f7":"## Missing value treatment","e38b3025":"### Setting to best threshold value got from Youden's table.(0.524)","7fcb2048":"### Kappa Score","0461ddb4":"## Problem Statement\n\nCustomer Satisfaction is the key to any business. Here we analyse various parameters in Airlines Service and use the classification algorithms to predict the chances of a customer being satisfied.","06927a0c":"### Let's look at Accuracy plot for different values of K in case of Manhattan metric","ba52d541":"## Exploring other popular classification algorithms - SVM","cc405f1c":"# Ada Boost","806603ac":"# Naive Bayes","bfd3404f":"## Checking for multicollinearity through VIF method","a638dd45":"## Dropping columns that are insignificant for Analysis","f4814e75":"# Gradient Boosting","528cc5cb":"## Finding ideal cut-off value depending on FN and FP costs","1f0ba354":"## Assigning binary values to target variable","53e23bff":"# Stacking","291fc3be":"## Scaling the Features using StandardScaler.","d08d5349":"### ROC-Curve","90554e45":"## Model Building:","60cbe0db":"We find that the VIF values are all less than 5 so there is no multicollinearity between the features.","2727b665":" ## Distribution of target variable","90443ad3":"### Identifying important features with the random forest model.","b7e94def":"### According to the costs the ideal cutoff here is 0.57","90f9f712":"We find that the arrival delay column has 393 missing values.","e56bde8f":"## Distribution of the data","58f7ed28":"We find that  there are no duplicates, as the shape remains the same.","6957d795":"## Reading The Data","00269f27":"### Conclusion: The Xtreme Gradient Boosting model gives the best results with AUC score .9945\nHence this model can be considered for deployment."}}