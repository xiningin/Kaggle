{"cell_type":{"e0304e7b":"code","bdc38d57":"code","ec6bcdc9":"code","69f7bb5a":"code","1c29ea87":"code","1a5f243f":"code","5536a2c3":"code","61d177f6":"code","4f911040":"code","cd0aa8c6":"code","c95080f2":"code","5261e995":"code","731bf2e8":"code","df2efdf1":"code","32414e21":"code","305b34d6":"code","77800bc2":"code","0ff0d3b8":"code","db303a65":"code","fdec4578":"code","039216fd":"code","c3ae209e":"code","d1f701bb":"code","4830eef1":"code","23ad7c41":"code","436d60dd":"code","095048cb":"code","f9bad5a8":"code","bff41647":"code","3fe82da5":"code","6088aaa9":"code","37f8ab2f":"code","a6707673":"code","3473b12b":"code","30a55523":"code","77ba067c":"code","77eab9b5":"code","8198fc78":"code","76c840bd":"code","8aec8bd4":"code","93dfa99a":"code","28462f1a":"code","43224864":"code","a85e73b0":"code","335ebab8":"code","45c8985c":"code","2e0c20f4":"code","a0d5c45d":"code","c46bba4b":"code","5a27fbc0":"code","7e27be58":"code","6c8fa090":"code","40370a0e":"code","cff5a0d3":"code","0866d44a":"code","642ee388":"code","3a25150e":"code","b3972725":"code","3c850327":"code","225de53b":"code","fab8eaed":"code","b54557f2":"code","408c8e62":"code","e23e50d2":"code","1a48aafa":"code","d961ddf3":"code","7d5948ff":"code","a7232969":"code","bf52e301":"code","f1689d39":"code","212fc2fa":"code","bb46bea4":"code","8b4bb241":"code","8ac45a74":"code","9ae5972e":"code","98d9d9a7":"code","dd5cd42b":"code","3684fe38":"code","6bdeaaf2":"code","114da609":"code","2ccabcb5":"code","3524431e":"code","93f6acb0":"markdown","421a5e56":"markdown","178b2e11":"markdown","4404a153":"markdown","97a3d16c":"markdown","9230c944":"markdown","e62c7360":"markdown","61853657":"markdown","2e62b211":"markdown","4845b8dc":"markdown","5b81bc1e":"markdown","7bd2f254":"markdown","49b155e5":"markdown","033bb8d5":"markdown","c90aedb4":"markdown","5895293d":"markdown","ae817dfc":"markdown","30603b93":"markdown","799e766a":"markdown","2da44fb9":"markdown"},"source":{"e0304e7b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport math\nimport sklearn\nfrom concurrent.futures import ProcessPoolExecutor\nimport sklearn.metrics as metrics\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom scipy.cluster import hierarchy as hc\nfrom scipy.stats import spearmanr\nfrom IPython.display import display\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nprint(\"package versions\")\nprint(f'pandas version : {pd.__version__}')\nprint(f'sklearn version: {sklearn.__version__}')\nprint(f'numpy version : {np.__version__}')\nprint(f'seaborn version : {sns.__version__}')","bdc38d57":"sns.set_theme(context='notebook', style='darkgrid', palette='pastel', font='sans-serif', font_scale=1, color_codes=True, rc=None)","ec6bcdc9":"%load_ext autoreload\n%autoreload 2","69f7bb5a":"def is_numeric_dtype(arr_or_dtype):\n    return pd.api.types.is_numeric_dtype(arr_or_dtype)\n\ndef get_sample(df,n):\n    \"\"\" Gets a random sample of n rows from df, without replacement.\n\n    Parameters:\n    -----------\n    df: A pandas data frame, that you wish to sample from.\n    n: The number of rows you wish to sample.\n\n    Returns:\n    --------\n    return value: A random sample of n rows of df.\n\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n\n    >>> get_sample(df, 2)\n       col1 col2\n    1     2    b\n    2     3    a\n    \"\"\"\n    idxs = sorted(np.random.permutation(len(df))[:n])\n    return df.iloc[idxs].copy()\n\ndef numericalize(df, col, name, max_n_cat):\n    \"\"\" Changes the column col from a categorical type to it's integer codes.\n\n    Parameters:\n    -----------\n    df: A pandas dataframe. df[name] will be filled with the integer codes from\n        col.\n\n    col: The column you wish to change into the categories.\n    name: The column name you wish to insert into df. This column will hold the\n        integer codes.\n\n    max_n_cat: If col has more categories than max_n_cat it will not change the\n        it to its integer codes. If max_n_cat is None, then col will always be\n        converted.\n\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n\n    note the type of col2 is string\n\n    >>> train_cats(df)\n    >>> df\n\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n\n    now the type of col2 is category { a : 1, b : 2}\n\n    >>> numericalize(df, df['col2'], 'col3', None)\n\n       col1 col2 col3\n    0     1    a    1\n    1     2    b    2\n    2     3    a    1\n    \"\"\"\n    if not is_numeric_dtype(col) and ( max_n_cat is None or col.nunique()>max_n_cat):\n        df[name] = col.cat.codes+1\n\ndef scale_vars(df, mapper):\n    warnings.filterwarnings('ignore', category=sklearn.exceptions.DataConversionWarning)\n    if mapper is None:\n        map_f = [([n],StandardScaler()) for n in df.columns if is_numeric_dtype(df[n])]\n        mapper = DataFrameMapper(map_f).fit(df)\n    df[mapper.transformed_names_] = mapper.transform(df)\n    return mapper\n\ndef fix_missing(df, col, name, na_dict):\n    \"\"\" Fill missing data in a column of df with the median, and add a {name}_na column\n    which specifies if the data was missing.\n\n    Parameters:\n    -----------\n    df: The data frame that will be changed.\n\n    col: The column of data to fix by filling in missing data.\n\n    name: The name of the new filled column in df.\n\n    na_dict: A dictionary of values to create na's of and the value to insert. If\n        name is not a key of na_dict the median will fill any missing data. Also\n        if name is not a key of na_dict and there is no missing data in col, then\n        no {name}_na column is not created.\n\n\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})\n    >>> df\n       col1 col2\n    0     1    5\n    1   nan    2\n    2     3    2\n\n    >>> fix_missing(df, df['col1'], 'col1', {})\n    >>> df\n       col1 col2 col1_na\n    0     1    5   False\n    1     2    2    True\n    2     3    2   False\n\n\n    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})\n    >>> df\n       col1 col2\n    0     1    5\n    1   nan    2\n    2     3    2\n\n    >>> fix_missing(df, df['col2'], 'col2', {})\n    >>> df\n       col1 col2\n    0     1    5\n    1   nan    2\n    2     3    2\n\n\n    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})\n    >>> df\n       col1 col2\n    0     1    5\n    1   nan    2\n    2     3    2\n\n    >>> fix_missing(df, df['col1'], 'col1', {'col1' : 500})\n    >>> df\n       col1 col2 col1_na\n    0     1    5   False\n    1   500    2    True\n    2     3    2   False\n    \"\"\"\n    if is_numeric_dtype(col):\n        if pd.isnull(col).sum() or (name in na_dict):\n            df[name+'_na'] = pd.isnull(col)\n            filler = na_dict[name] if name in na_dict else col.median()\n            df[name] = col.fillna(filler)\n            na_dict[name] = filler\n    return na_dict\ndef proc_df(df, y_fld=None, skip_flds=None, ignore_flds=None, do_scale=False, na_dict=None,\n            preproc_fn=None, max_n_cat=None, subset=None, mapper=None):\n    \"\"\" proc_df takes a data frame df and splits off the response variable, and\n    changes the df into an entirely numeric dataframe.\n\n    Parameters:\n    -----------\n    df: The data frame you wish to process.\n\n    y_fld: The name of the response variable\n\n    skip_flds: A list of fields that dropped from df.\n\n    ignore_flds: A list of fields that are ignored during processing.\n\n    do_scale: Standardizes each column in df. Takes Boolean Values(True,False)\n\n    na_dict: a dictionary of na columns to add. Na columns are also added if there\n        are any missing values.\n\n    preproc_fn: A function that gets applied to df.\n\n    max_n_cat: The maximum number of categories to break into dummy values, instead\n        of integer codes.\n\n    subset: Takes a random subset of size subset from df.\n\n    mapper: If do_scale is set as True, the mapper variable\n        calculates the values used for scaling of variables during training time (mean and standard deviation).\n\n    Returns:\n    --------\n    [x, y, nas, mapper(optional)]:\n\n        x: x is the transformed version of df. x will not have the response variable\n            and is entirely numeric.\n\n        y: y is the response variable\n\n        nas: returns a dictionary of which nas it created, and the associated median.\n\n        mapper: A DataFrameMapper which stores the mean and standard deviation of the corresponding continuous\n        variables which is then used for scaling of during test-time.\n\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n\n    note the type of col2 is string\n\n    >>> train_cats(df)\n    >>> df\n\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n\n    now the type of col2 is category { a : 1, b : 2}\n\n    >>> x, y, nas = proc_df(df, 'col1')\n    >>> x\n\n       col2\n    0     1\n    1     2\n    2     1\n\n    >>> data = DataFrame(pet=[\"cat\", \"dog\", \"dog\", \"fish\", \"cat\", \"dog\", \"cat\", \"fish\"],\n                 children=[4., 6, 3, 3, 2, 3, 5, 4],\n                 salary=[90, 24, 44, 27, 32, 59, 36, 27])\n\n    >>> mapper = DataFrameMapper([(:pet, LabelBinarizer()),\n                          ([:children], StandardScaler())])\n\n    >>>round(fit_transform!(mapper, copy(data)), 2)\n\n    8x4 Array{Float64,2}:\n    1.0  0.0  0.0   0.21\n    0.0  1.0  0.0   1.88\n    0.0  1.0  0.0  -0.63\n    0.0  0.0  1.0  -0.63\n    1.0  0.0  0.0  -1.46\n    0.0  1.0  0.0  -0.63\n    1.0  0.0  0.0   1.04\n    0.0  0.0  1.0   0.21\n    \"\"\"\n    if not ignore_flds: ignore_flds=[]\n    if not skip_flds: skip_flds=[]\n    if subset: df = get_sample(df,subset)\n    ignored_flds = df.loc[:, ignore_flds]\n    df.drop(ignore_flds, axis=1, inplace=True)\n    df = df.copy()\n    if preproc_fn: preproc_fn(df)\n    if y_fld is None: y = None\n    else:\n        if not is_numeric_dtype(df[y_fld]): df[y_fld] = df[y_fld].cat.codes\n        y = df[y_fld].values\n        skip_flds += [y_fld]\n    df.drop(skip_flds, axis=1, inplace=True)\n\n    if na_dict is None: na_dict = {}\n    for n,c in df.items(): na_dict = fix_missing(df, c, n, na_dict)\n    if do_scale: mapper = scale_vars(df, mapper)\n    for n,c in df.items(): numericalize(df, c, n, max_n_cat)\n    df = pd.get_dummies(df, dummy_na=True)\n    df = pd.concat([ignored_flds, df], axis=1)\n    res = [df, y, na_dict]\n    if do_scale: res = res + [mapper]\n    return res\n\ndef set_plot_sizes(sml, med, big):\n    plt.rc('font', size=sml)          # controls default text sizes\n    plt.rc('axes', titlesize=sml)     # fontsize of the axes title\n    plt.rc('axes', labelsize=med)    # fontsize of the x and y labels\n    plt.rc('xtick', labelsize=sml)    # fontsize of the tick labels\n    plt.rc('ytick', labelsize=sml)    # fontsize of the tick labels\n    plt.rc('legend', fontsize=sml)    # legend fontsize\n    plt.rc('figure', titlesize=big)  # fontsize of the figure title","1c29ea87":"PATH = \"..\/input\/\"\n\ndf_raw = pd.read_feather('..\/input\/bluebook\/bulldozers-raw')\ndf_trn, y_trn, nas = proc_df(df_raw, 'SalePrice')\n","1a5f243f":"def split_vals(a,n): return a[:n], a[n:]\nn_valid = 12000\nn_trn = len(df_trn)-n_valid\nX_train, X_valid = split_vals(df_trn, n_trn)\ny_train, y_valid = split_vals(y_trn, n_trn)\nraw_train, raw_valid = split_vals(df_raw, n_trn)","5536a2c3":"def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","61d177f6":"print(df_raw.shape)","4f911040":"df_raw.head(50)","cd0aa8c6":"def set_rf_samples(n):\n    \"\"\" Changes Scikit learn's random forests to give each tree a random sample of\n    n random rows.\n    \"\"\"\n    RandomForestRegressor._generate_sample_indices = (lambda rs, n_samples:\n        RandomForestRegressor.check_random_state(rs).randint(0, n_samples, n))","c95080f2":"set_rf_samples(50000)","5261e995":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","731bf2e8":"%time preds = np.stack([t.predict(X_valid) for t in m.estimators_])\nnp.mean(preds[:,0]), np.std(preds[:,0])","df2efdf1":"def parallel_trees(m, fn, n_jobs=8):\n        return list(ProcessPoolExecutor(n_jobs).map(fn, m.estimators_))","32414e21":"def get_preds(t): return t.predict(X_valid)\n%time preds = np.stack(parallel_trees(m, get_preds))\nnp.mean(preds[:,0]), np.std(preds[:,0])","305b34d6":"x = raw_valid.copy()\nx['pred_std'] = np.std(preds, axis=0)\nx['pred'] = np.mean(preds, axis=0)\n#x.Enclosure.value_counts().plot.barh();\ncnt_plt=sns.countplot(y='Enclosure',data=x,order=x['Enclosure'].value_counts(ascending=True).index)","77800bc2":"flds = ['Enclosure', 'SalePrice', 'pred', 'pred_std']\nenc_summ = x[flds].groupby('Enclosure', as_index=False).mean()\nenc_summ","0ff0d3b8":"enc_summ = enc_summ[~pd.isnull(enc_summ.SalePrice)]\nenc_summ.plot('Enclosure', 'SalePrice', 'barh', xlim=(0,11));","db303a65":"enc_summ.plot('Enclosure', 'pred', 'barh', xerr='pred_std', alpha=0.6, xlim=(0,11));","fdec4578":"raw_valid.ProductSize.value_counts().plot.barh();","039216fd":"flds = ['ProductSize', 'SalePrice', 'pred', 'pred_std']\nsumm = x[flds].groupby(flds[0]).mean()\nsumm","c3ae209e":"def rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)","d1f701bb":"fi = rf_feat_importance(m, df_trn); fi[:10]","4830eef1":"fi.plot('cols', 'imp', figsize=(10,6), legend=False);","23ad7c41":"def plot_fi(fi): return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)","436d60dd":"to_keep = fi[fi.imp>0.005].cols; len(to_keep)","095048cb":"df_keep = df_trn[to_keep].copy()\nX_train, X_valid = split_vals(df_keep, n_trn)","f9bad5a8":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5,\n                          n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","bff41647":"fi = rf_feat_importance(m, df_keep)\nplot_fi(fi);\n","3fe82da5":"df_trn2, y_trn, nas = proc_df(df_raw, 'SalePrice', max_n_cat=7)\nX_train, X_valid = split_vals(df_trn2, n_trn)\n\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.6, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","6088aaa9":"df_trn2, y_trn, nas = proc_df(df_raw, 'SalePrice', max_n_cat=7)\nX_train, X_valid = split_vals(df_trn2, n_trn)\n\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.6, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","37f8ab2f":"fi = rf_feat_importance(m, df_trn2)\nplot_fi(fi[:25]);","a6707673":"corr = np.round(spearmanr(df_keep).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nprint(type(plt))\nfig = plt.figure(figsize=(16,10))\ndendrogram = hc.dendrogram(z, labels=df_keep.columns, orientation='left', leaf_font_size=16)\nplt.show()","3473b12b":"def get_oob(df):\n    m = RandomForestRegressor(n_estimators=30, min_samples_leaf=5, max_features=0.6, n_jobs=-1, oob_score=True)\n    x, _ = split_vals(df, n_trn)\n    m.fit(x, y_train)\n    return m.oob_score_","30a55523":"get_oob(df_keep)","77ba067c":"for c in ('saleYear', 'saleElapsed', 'fiModelDesc', 'fiBaseModel', 'Grouser_Tracks', 'Coupler_System'):\n    print(c, get_oob(df_keep.drop(c, axis=1)))","77eab9b5":"to_drop = ['saleYear', 'fiBaseModel', 'Grouser_Tracks']\nget_oob(df_keep.drop(to_drop, axis=1))","8198fc78":"os.makedirs('tmp', exist_ok=True)\nnp.save('tmp\/keep_cols.npy', np.array(df_keep.columns))","76c840bd":"# in latest version of numpy, default value for allow_pickle = False\nkeep_cols = np.load('tmp\/keep_cols.npy',allow_pickle=True)\ndf_keep = df_trn[keep_cols]","8aec8bd4":"def reset_rf_samples():\n    \"\"\" Undoes the changes produced by set_rf_samples.\n    \"\"\"\n    RandomForestRegressor._generate_sample_indices = (lambda rs, n_samples:\n        RandomForestRegressor.check_random_state(rs).randint(0, n_samples, n_samples))","93dfa99a":"reset_rf_samples()","28462f1a":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","43224864":"from pdpbox import pdp\nfrom plotnine import *","a85e73b0":"set_rf_samples(50000)","335ebab8":"plot_fi(rf_feat_importance(m, df_trn2)[:10]);","45c8985c":"plot_fi(rf_feat_importance(m, df_trn2)[:10]);","2e0c20f4":"x_all = get_sample(df_raw[df_raw.YearMade>1930], 500)","a0d5c45d":"!pip install scikit-misc","c46bba4b":"ggplot(x_all, aes('YearMade', 'SalePrice'))+stat_smooth(se=True, method='loess')","5a27fbc0":"x = get_sample(X_train[X_train.YearMade>1930], 500)","7e27be58":"def plot_pdp(feat, clusters=None, feat_name=None):\n    feat_name = feat_name or feat\n    p = pdp.pdp_isolate(m, x, x.columns, feat)\n    return pdp.pdp_plot(p, feat_name, plot_lines=True,\n                        cluster=clusters is not None,\n                        n_cluster_centers=clusters)","6c8fa090":"plot_pdp('YearMade')","40370a0e":"plot_pdp('YearMade', clusters=5)","cff5a0d3":"feats = ['saleElapsed', 'YearMade']\np = pdp.pdp_interact(m, x, x.columns, feats)\npdp.pdp_interact_plot(p, feats)","0866d44a":"plot_pdp(['Enclosure_EROPS w AC', 'Enclosure_EROPS', 'Enclosure_OROPS'], 5, 'Enclosure')","642ee388":"df_raw.YearMade[df_raw.YearMade<1950] = 1950\ndf_keep['age'] = df_raw['age'] = df_raw.saleYear-df_raw.YearMade","3a25150e":"X_train, X_valid = split_vals(df_keep, n_trn)\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.6, n_jobs=-1)\nm.fit(X_train, y_train)\nplot_fi(rf_feat_importance(m, df_keep));","b3972725":"%%capture\n!pip install treeinterpreter","3c850327":"from treeinterpreter import treeinterpreter as ti","225de53b":"df_train, df_valid = split_vals(df_raw[df_keep.columns], n_trn)","fab8eaed":"row = X_valid.values[None,0]; row","b54557f2":"prediction, bias, contributions = ti.predict(m, row)","408c8e62":"idxs = np.argsort(contributions[0])","e23e50d2":"[o for o in zip(df_keep.columns[idxs], df_valid.iloc[0][idxs], contributions[0][idxs])]","1a48aafa":"contributions[0].sum()","d961ddf3":"df_ext = df_keep.copy()\ndf_ext['is_valid'] = 1\ndf_ext.is_valid[:n_trn] = 0\nx, y, nas = proc_df(df_ext, 'is_valid')","7d5948ff":"m = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(x, y);\nm.oob_score_","a7232969":"fi = rf_feat_importance(m, x); fi[:10]","bf52e301":"feats=['SalesID', 'saleElapsed', 'MachineID']","f1689d39":"(X_valid[feats]\/1000).describe()","212fc2fa":"x.drop(feats, axis=1, inplace=True)","bb46bea4":"m = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(x, y);\nm.oob_score_","8b4bb241":"fi = rf_feat_importance(m, x); fi[:10]","8ac45a74":"set_rf_samples(50000)","9ae5972e":"feats=['SalesID', 'saleElapsed', 'MachineID', 'age', 'YearMade', 'saleDayofyear']","98d9d9a7":"X_train, X_valid = split_vals(df_keep, n_trn)\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","dd5cd42b":"for f in feats:\n    df_subs = df_keep.drop(f, axis=1)\n    X_train, X_valid = split_vals(df_subs, n_trn)\n    m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n    m.fit(X_train, y_train)\n    print(f)\n    print_score(m)","3684fe38":"reset_rf_samples()","6bdeaaf2":"df_subs = df_keep.drop(['SalesID', 'MachineID', 'saleDayofyear'], axis=1)\nX_train, X_valid = split_vals(df_subs, n_trn)\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","114da609":"plot_fi(rf_feat_importance(m, X_train));","2ccabcb5":"np.save('tmp\/subs_cols.npy', np.array(df_subs.columns))","3524431e":"m = RandomForestRegressor(n_estimators=160, max_features=0.5, n_jobs=-1, oob_score=True)\n%time m.fit(X_train, y_train)\nprint_score(m)","93f6acb0":"One thing that makes this harder to interpret is that there seem to be some variables with very similar meanings. Let's try to remove redundent features.","421a5e56":"# Confidence based on tree variance\n\n For model interpretation, there's no need to use the full dataset on each tree - using a subset will be both faster, and also provide better interpretability (since an overfit model will not provide much variance across trees).","178b2e11":"# Partial dependence","4404a153":"# Removing redundant features","97a3d16c":"## One-hot encoding","9230c944":"This is a copy of lesson 2 notebook from fast.ai course Introduction to Machine Learning for Coders. It was modified in the data input only, so that i can run on Kaggle kernels.\n\nOriginal Notebook : https:\/\/www.kaggle.com\/miwojc\/fast-ai-machine-learning-lesson-2\/\n\nThis is remastered version of the original notebook\n* Removed dependency on fastai 0.7\nCourse page: http:\/\/course.fast.ai\/ml.html\n","e62c7360":"# Extrapolation","61853657":"We can see that different trees are giving different estimates this this auction. In order to see how prediction confidence varies, we can add this into our dataset.","2e62b211":"# Tree interpreter","4845b8dc":"Looking good! Let's use this dataframe from here. We'll save the list of columns so we can reuse it later.","5b81bc1e":"## Set Seaborn theme","7bd2f254":"Let's try removing some of these related features to see if the model can be simplified without impacting the accuracy.","49b155e5":"We saw how the model averages predictions across the trees to get an estimate - but how can we know the confidence of the estimate? One simple way is to use the standard deviation of predictions, instead of just the mean. This tells us the relative confidence of predictions - that is, for rows where the trees give very different results, you would want to be more cautious of using those results, compared to cases where they are more consistent. Using the same example as in the last lesson when we looked at bagging:","033bb8d5":"# Our final model!","c90aedb4":"Here's our baseline.","5895293d":"It's not normally enough to just to know that a model can make accurate predictions - we also want to know how it's making predictions. The most important way to see this is with feature importance.\n","ae817dfc":"It looks like we can try one from each group for removal. Let's see what that does.\n","30603b93":"# Random Forest Model interpretation","799e766a":"When we use python to loop through trees like this, we're calculating each in series, which is slow! We can use parallel processing to speed things up:\n","2da44fb9":"proc_df's optional *max_n_cat* argument will turn some categorical variables into new columns.\n\nFor example, the column **ProductSize** which has 6 categories:\n\n* Large\n* Large \/ Medium\n* Medium\n* Compact\n* Small\n* Mini\n\ngets turned into 6 new columns:\n\n* ProductSize_Large\n* ProductSize_Large \/ Medium\n* ProductSize_Medium\n* ProductSize_Compact\n* ProductSize_Small\n* ProductSize_Mini\n\nand the column **ProductSize** gets removed.\n\nIt will only happen to columns whose number of categories is no bigger than the value of the *max_n_cat* argument.\n\nNow some of these new columns may prove to have more important features than in the earlier situation, where all categories were in one column."}}