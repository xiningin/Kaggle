{"cell_type":{"0b738f41":"code","517f42ea":"code","c5a0b60a":"code","89d3a55f":"code","bde95792":"code","929ca845":"code","e84cb4e5":"code","314f1ab2":"code","267fd9b1":"code","63aaea4c":"code","89edcab4":"code","6bd9a8ca":"code","8d9598ac":"code","2d2c2212":"code","1046b920":"code","35c6f6ee":"code","bc2d2d80":"code","7dcf5c0a":"code","53f1a0f6":"markdown","71bb7d0c":"markdown","9cf736ae":"markdown","7a1b45d5":"markdown","8ea75b52":"markdown","829d5c35":"markdown","713fe42b":"markdown","8c4fa4d7":"markdown"},"source":{"0b738f41":"import datetime\nimport itertools\nimport json\nimport os\nimport pickle\nimport regex\nimport time\nimport tqdm\n\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport numpy as np\nimport plotly.express\nimport sklearn.manifold\n\nimport gensim.models\nimport nltk.tokenize","517f42ea":"# Used to load raw articles. Only useful if the corpus needs to be processed and generated again.\ndata_root = '\/kaggle\/input\/CORD-19-research-challenge'\nfolder_names = ['biorxiv_medrxiv', 'comm_use_subset', 'custom_license', 'noncomm_use_subset']\n\n# Used to load the previously processed corpus and pre-trained models.\ndataset_root = '\/kaggle\/input\/covid19-research-corpus'\ncorpus_file = os.path.join(dataset_root, 'corpus')\npretrained_model = os.path.join(dataset_root,'w2v_1585759802.model')  # If not None, will use this model. Otherwise will train a new model.\npretrained_phraser = os.path.join(dataset_root, 'w2v_1585759802_phraser.pkl')  # If not None, will use a pre-trained phraser. Otherwise will build a new one.\nphraser_depth = 1  # The depth of phrases. Match this with the pretrained phraser setting if it is supplied.","c5a0b60a":"NUM_TOK = '<nUm>'\nNR_BASIC = regex.compile(r\"^[+-]?\\d*\\.?\\d+\\(?\\d*\\)?+$\", regex.DOTALL)\nTOKENIZER_PATTERN = r'''(?x)                # set flag to allow verbose regexps\n                        (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n                      | \\d+(?:\\.\\d+)?       # numbers, incl. currency and percentages\n                      | \\w+(?:[-']\\w+)*     # words w\/ optional internal hyphens\/apostrophe\n                      | \\.\\.\\.              # ellipsis\n                      | [][.,$%;\"'?():_`-]  # these are separate tokens; includes ], [\n            '''\n\n\ndef is_number(s):\n    \"\"\"Determines if the supplied string is a number.\n\n    Args:\n        s: The input string.\n\n    Returns:\n        True if the supplied string is a number (both . and , are acceptable), False otherwise.\n    \"\"\"\n    return NR_BASIC.match(s.replace(\",\", \"\")) is not None\n\n\ndef process_token(word, do_lowercase=True):\n    \"\"\"Pre-process a single token.\n    \n    Includes selective lowercasing to preserve abbreviations and\n    converting all numbers to a special token.\n    \n    Args:\n        word: The token.\n        do_lowercase: If True, performs lowercasing.\n\n    Returns:\n        The pre-processed token.\n    \"\"\"\n    if is_number(word):\n        return NUM_TOK\n    if do_lowercase:\n        if len(word) == 1 or (len(word) > 1 and word[0].isupper() and word[1:].islower()):\n            # Lowercase only if first letter alone is uppercase.\n            return word.lower()\n    return word\n    \n\ndef tokenize_and_process(text):\n    \"\"\"Tokenizes and pre-processes a piece of text.\n    \n    Includes selective lowercasing to preserve abbreviations,\n    as well as conversion of all numbers to special token <nUm>.\n    \n    Args:\n        text: The text to tokenize.\n\n    Returns:\n        A sequence of pre-processed tokens.\n    \"\"\"\n    tokens = nltk.tokenize.regexp_tokenize(text, pattern=TOKENIZER_PATTERN)\n    return [process_token(token) for token in tokens]\n\n\njson_roots = [os.path.join(data_root, folder_name, folder_name) for folder_name in folder_names]\ndef paragraph_generator():\n    \"\"\"Scans all json files in a directory and gets the content.\n    \n    Args:\n        json_roots: The root folders of articles in json format.\n        \n    Yeilds:\n        A paragraph as a string.\n    \"\"\"\n    for json_root in json_roots:\n        for filename in tqdm.tqdm(os.listdir(json_root)):\n            # Load the json.\n            with open(os.path.join(json_root, filename), 'r') as json_file:\n                content = json.load(json_file)\n            # Yeild the title.\n            yield content['metadata']['title'].strip()\n            # Yeild the text in abstract and full body.\n            for section in ['abstract', 'body_text']:\n                for paragraph in content[section]:\n                    yield paragraph['text'].strip()","89d3a55f":"# Pre-process and save a corpus if not supplied.\nif corpus_file is None:\n    if os.path.exists(\"corpus\"):\n        os.remove(\"corpus\")\n    with open('corpus', 'a+') as f:\n        for p in paragraph_generator():\n            f.write(' '.join(tokenize_and_process(p)) + '\\n')\n    corpus_file = 'corpus'","bde95792":"# Utility functions to produce common phrases\nCOMMON_TERMS = [\"-\", \"-\", b\"\\xe2\\x80\\x93\", b\"'s\", b\"\\xe2\\x80\\x99s\", \"from\", \"as\", \"at\", \"by\", \"of\", \"on\",\n                \"into\", \"to\", \"than\", \"over\", \"in\", \"the\", \"a\", \"an\", \"\/\", \"under\"]\nEXCLUDE_PUNCT = [\"=\", \".\", \",\", \":\", \"(\", \")\", \"<\", \">\", \"\\\"\", \"\u201c\", \"\u201d\", \"\u2265\", \"\u2264\", \"%\", \"$\", \";\", NUM_TOK]\n\ndef exclude_words(phrasegrams, words):\n    \"\"\"Given a list of words, excludes those from the keys of the phrase dictionary.\"\"\"\n    new_phrasergrams = {}\n    words_re_list = []\n    for word in words:\n        we = regex.escape(word)\n        words_re_list.append(\"^\" + we + \"$|^\" + we + \"_|_\" + we + \"$|_\" + we + \"_\")\n    word_reg = regex.compile(r\"\"+\"|\".join(words_re_list))\n    for gram in tqdm.tqdm(phrasegrams):\n        valid = True\n        for sub_gram in gram:\n            if word_reg.search(sub_gram.decode(\"unicode_escape\", \"ignore\")) is not None:\n                valid = False\n                break\n            if not valid:\n                continue\n        if valid:\n            new_phrasergrams[gram] = phrasegrams[gram]\n    return new_phrasergrams\n\n\ndef wordgrams(sent, depth, pc, th, ct, et, d=0):\n    \"\"\"Produces a phraser for word n-grams.\"\"\"\n    if depth == 0:\n        return sent, None\n    else:\n        phrases = gensim.models.phrases.Phrases(\n            sent,\n            common_terms=ct,\n            min_count=pc,\n            threshold=th)\n\n        grams = gensim.models.phrases.Phraser(phrases)\n        grams.phrasegrams = exclude_words(grams.phrasegrams, et)\n        d += 1\n        if d < depth:\n            return wordgrams(grams[sent], depth, pc, th, ct, et, d)\n        else:\n            return grams[sent], grams","929ca845":"class EpochLogger(gensim.models.callbacks.CallbackAny2Vec):\n    \"\"\"Callback to log information about training.\"\"\"\n    def __init__(self):\n        self.epoch = 0\n\n    def on_epoch_begin(self, model):\n        print(\"{} Starting epoch #{}\".format(\n            datetime.datetime.now(), self.epoch))\n\n    def on_epoch_end(self, model):\n        print(\"{} Finished epoch #{}\".format(\n            datetime.datetime.now(), self.epoch))\n        self.epoch += 1\n\nif pretrained_model is not None:\n    w2v_model = gensim.models.Word2Vec.load(pretrained_model)\n    if pretrained_phraser is not None:\n        with open(pretrained_phraser, 'rb') as f:\n            phraser = pickle.load(f)\n        sentences = gensim.models.word2vec.LineSentence(corpus_file)\n        while phraser_depth > 0:\n            sentences = phraser[sentences]\n            phraser_depth -= 1\nelse:\n    model_name = 'w2v_{}'.format(int(time.time()))\n    if pretrained_phraser is None:\n        print(\"{} Starting to build phrasegrams\".format(\n        datetime.datetime.now()))\n        # Depth 1: 16 min.\n        # Depth 2: 60 min.\n        sentences, phraser = wordgrams(\n            gensim.models.word2vec.LineSentence(corpus_file),\n            depth=phraser_depth,\n            pc=10,\n            th=12.0,\n            ct=COMMON_TERMS,\n            et=EXCLUDE_PUNCT)\n        phraser.save(os.path.join(model_name + \"_phraser.pkl\"))\n        print(\"{} Finished building phrasegrams\".format(\n            datetime.datetime.now()))\n    else:\n        with open(pretrained_phraser, 'rb') as f:\n            phraser = pickle.load(f)\n        sentences = gensim.models.word2vec.LineSentence(corpus_file)\n        while phraser_depth > 0:\n            sentences = phraser[sentences]\n            phraser_depth -= 1\n\n    print(\"{} Init the word2vec model\".format(datetime.datetime.now()))\n    # Phrase depth 1\n    # Before the 1st epoch starts (building vocab): 20 min.\n    # Each epoch:                                   35 min.\n    # Phrase depth 2\n    # Before the 1st epoch starts (building vocab): 40 min.\n    # Each epoch:                                   45 min.\n    w2v_model = gensim.models.Word2Vec(\n        sentences,\n        size=200,\n        window=8,\n        min_count=10,\n        sg=True,\n        hs=False,\n        alpha=0.01,\n        sample=0.0001,\n        negative=15,\n        workers=4,\n        sorted_vocab=True,\n        callbacks=[EpochLogger()],\n        iter=10)\n    w2v_model.save(model_name + '.model')","e84cb4e5":"keyword = 'COVID-19'\nwords_to_plot = 50","314f1ab2":"# Find most similar words to the keyword and visualization using t-SNE.\ntop_words = [w for w, _ in w2v_model.wv.most_similar(keyword, topn=words_to_plot)] + [keyword]\nprint('Most similar words to \\'{}\\': '.format(keyword), top_words[:10])\n\ndim_red = sklearn.manifold.TSNE(\n    n_components=2, \n    perplexity=15.0, \n    n_iter=10000, \n    metric='cosine', \n    random_state=0)\ntop_words_2d = dim_red.fit_transform(w2v_model.wv[top_words])\n\nfig = plotly.express.scatter(\n    x=top_words_2d[:,0], \n    y=top_words_2d[:,1], \n    text=top_words,\n    color=['blue'] * words_to_plot + ['orange'],\n    color_discrete_map={'orange':'#ff8c00', 'blue':'#1f77b4'}\n)\nfig.update_traces(textposition='top center')\nfig.update_layout(\n    xaxis_showgrid=False, \n    yaxis_showgrid=False, \n    xaxis_zeroline=False, \n    yaxis_zeroline=False,\n    plot_bgcolor='rgba(0, 0, 0, 0)',\n    paper_bgcolor='rgba(0, 0, 0, 0)',\n    xaxis_showticklabels=False,\n    yaxis_showticklabels=False,\n    yaxis_title='',\n    xaxis_title='',\n    showlegend=False,\n)\nfig.show()","267fd9b1":"KEYWORD_COLOR = 'orange'\n\ndef get_connecting_terms(words, model, topn=10, pairwise=False):\n    \"\"\"Gets topn connecting terms between the supplied list of words.\n    \n    Args:\n        words: The list of words.\n        model: The embedding model.\n        topn: Number of connecting words to identify.\n        pairwise: If true, will search for pairwise connections. Otherwise\n            searches for words that connect all of the input words.\n    \"\"\"\n    if len(words) > 5:\n        raise ValueError('At most 5 words allowed, {} supplied.'.format(len(words)))\n    if len(words) < 2:\n        raise ValueError('At least 2 words required, {} supplied.'.format(len(words)))\n    norm_syn1neg = w2v_model.trainables.syn1neg \/ np.linalg.norm(\n        w2v_model.trainables.syn1neg, axis=1, keepdims=True)\n    norm_word_vectors = model.wv[words] \/ np.linalg.norm(model.wv[words]  , axis=1, keepdims=True)\n    \n    top_words, similarities = {}, {}\n    if pairwise:\n        for idx1, idx2 in itertools.combinations(range(len(words)), 2):\n            exclude_indices = [model.wv.vocab.get(words[idx1]).index,\n                               model.wv.vocab.get(words[idx2]).index]\n            scores_w1 = np.dot(norm_syn1neg, norm_word_vectors[idx1])\n            scores_w2 = np.dot(norm_syn1neg, norm_word_vectors[idx2])\n            scores = scores_w1 + scores_w2\n            top_common_word_indices = np.argsort(scores)[::-1][:topn+2]\n            top_common_word_indices = [i for i in top_common_word_indices if i not in exclude_indices][:topn]\n            top_words[(words[idx1], words[idx2])] = (\n                [w2v_model.wv.index2word[i] for i in top_common_word_indices],\n                scores_w1[top_common_word_indices],\n                scores_w2[top_common_word_indices],\n            )\n            similarities[(words[idx1], words[idx2])] = np.dot(norm_word_vectors[idx1], norm_word_vectors[idx2])\n    else:\n        exclude_indices = [model.wv.vocab.get(word).index for word in words]\n        individual_scores = [np.dot(norm_syn1neg, norm_word_vectors[idx]) for idx, _ in enumerate(words)]\n        scores = np.sum(individual_scores, axis=0)\n        top_common_word_indices = np.argsort(scores)[::-1][:topn+2]\n        top_common_word_indices = [i for i in top_common_word_indices if i not in exclude_indices][:topn]\n        words_tuple = tuple(words)\n        top_words[words_tuple] = tuple(\n            [[w2v_model.wv.index2word[i] for i in top_common_word_indices]] + \n            [individual_score[top_common_word_indices] for individual_score in individual_scores],\n        )\n        similarities[words_tuple] = np.mean(\n            [model.wv.similarity(w1, w2) for w1, w2 in itertools.combinations(words, 2)])\n    return top_words, similarities\n\ndef graph_from_connecting_terms(graph_def):\n    \"\"\"Builds a graph using the output of get_connecting_terms.\"\"\"\n    G=nx.Graph()\n    for keywords, connections in graph_def.items():\n        G.add_nodes_from(keywords, color=KEYWORD_COLOR)\n        for word_and_scores in zip(*connections):\n            # Add the node with this connection word.\n            connection_word = word_and_scores[0]\n            G.add_node(connection_word, color=0)  # Color is adjusted later.\n            # Add edges from this connection word to all \n            G.add_edges_from([\n                (keywords[keyword_idx], connection_word, {'weight': keyword_score}) \n                for keyword_idx, keyword_score in enumerate(word_and_scores[1:])\n            ])\n    # Fix the colors for nodes with more than 2 edges.\n    for node in G.nodes:\n        if G.nodes[node]['color'] != KEYWORD_COLOR:\n            all_weights = [G.edges[edge]['weight'] for edge in G.edges(node)]\n            G.nodes[node]['color'] = sum(all_weights)\n    return G\n\ndef plot_graph(G):\n    \"\"\"Plots the graph of connecting terms.\"\"\"\n    pos=nx.spring_layout(G)  # positions for all nodes.\n    edge_weights = [nx.get_edge_attributes(G,'weight')[edge] for edge in G.edges]\n    all_node_colors = nx.get_node_attributes(G, 'color')\n    keyword_nodes = [node for node, color in all_node_colors.items() if color == KEYWORD_COLOR]\n    node_colors, connection_nodes = zip(*[(all_node_colors[node], node) for node in G.nodes if node not in keyword_nodes])\n    nx.draw(G, pos, nodelist=keyword_nodes, node_color=KEYWORD_COLOR, node_size = 100)\n    nx.draw(G, pos, with_labels=False, nodelist=connection_nodes, node_size = 100,\n            cmap=plt.cm.Blues, vmin=min(node_colors), vmax=max(node_colors), node_color=node_colors,\n            edge_cmap=plt.cm.Greys, edge_vmin=min(edge_weights), edge_vmax=max(edge_weights), edge_color=edge_weights)\n    # Plot the labels separately a little higher.\n    pos_higher = {}\n    all_pos_y = [v[1] for _, v in pos.items()]\n    y_offset = 0.04 * (max(all_pos_y) - min(all_pos_y))\n    for k, v in pos.items():\n        pos_higher[k] = (v[0], v[1] + y_offset)\n    nx.draw_networkx_labels(G, pos_higher, {n: n for n in G.nodes})\n    \ndef plot_connection_graph(keywords, topn=10, pairwise=False):\n    connecting_terms, similarities = get_connecting_terms(keywords, w2v_model, topn=topn, pairwise=pairwise)\n    G = graph_from_connecting_terms(connecting_terms)\n    plt.figure(figsize=(14, 12))\n    plt.margins(0.1)\n    plot_graph(G)\n    print('Connecting words:')\n    for keywords_and_connections in connecting_terms.items():\n        keywords, connections = keywords_and_connections[0], keywords_and_connections[1]\n        print(' >-< '.join(keywords) + ' (similarity {:.2f})'.format(similarities[keywords]), ':', connections[0])","63aaea4c":"plot_connection_graph(['COVID-19', 'social', 'mental'], topn=10)","89edcab4":"plot_connection_graph(['COVID-19', 'treatment', 'effective'], topn=20)","6bd9a8ca":"plot_connection_graph(['COVID-19', 'syndromes'], topn=20)","8d9598ac":"plot_connection_graph(['COVID-19', 'medication', 'vitamin'], topn=10)","2d2c2212":"plot_connection_graph(['COVID-19', 'hydroxychloroquine', 'doxorubicin'], topn=10)","1046b920":"# A set of words mentioned together in the same paragraph\n# This is used to search for explicit co-occurences of words.\n# Takes several minutes.\nword_sets = []\nfor i, sentence in enumerate(sentences):\n    if i % 1000 == 0:\n        print('Processed {} paragraphs.'.format(i), end='\\r')\n    word_sets.append(set(sentence))\nprint('Processed {} paragraphs.'.format(len(word_sets)))","35c6f6ee":"def get_hidden_connections(model, keyword,  nr_words=10, extra_words=None):\n    \"\"\"Return words that have close similarity but never mentioned with the keyword.\n    \n    This is done by combining word similarity with basic text search.\n    \n    Args:\n        keywords: The keyword to search for, e.g. 'covid'\n        nr_words: Number of hidden connections to return.\n        extra_words: A set of words to search in addition to the keyword, that \n            would signify a known connection. These are usually synonyms of the\n            keyword.\n            \n    Returns:\n        A list of words connected to the keyword.\n    \"\"\"\n    if keyword not in model.wv.vocab:\n        raise ValueError('Word \"{}\" not in the vocabulary.'.format(keyword))\n    connections = model.wv.most_similar(\n        positive=[model.trainables.syn1neg[model.wv.vocab[keyword].index]],\n        topn=5000,\n    )\n    hidden_connections = []\n    search_words = {keyword}\n    if extra_words is not None:\n        search_words = search_words.union(extra_words)\n    search_words = set(search_words)\n    for i, (connection, _) in enumerate(connections):\n        hidden = True  # Haven't found a match, still hidden\n        for word_set in word_sets:\n            if connection in word_set and search_words.intersection(word_set):\n                # If they happen together, this is not an interesting word.\n                hidden = False\n                break\n        if hidden:\n            hidden_connections.append((connection, i))    \n            if len(hidden_connections) == nr_words:\n                return hidden_connections\n    return hidden_connections","bc2d2d80":"# Change these to explore more.\nkeyword = 'COVID-19'\nwords_similar_to_keyword = ['nCoV', 'SARS-CoV-2', 'COVID', 'COVID-19_pneumonia', 'COVID-19_outbreak',\n                            'novel_coronavirus', 'covid-19', 'wuhan', 'COVID-19_epidemic', 'SARS-Cov-2',\n                            'COVID19', 'COVID-19_in_wuhan', 'COVID-2019', 'nCov', 'SARS', 'MERS']\n\nhidden_connections = get_hidden_connections(\n    w2v_model, keyword, nr_words=20, \n    extra_words=words_similar_to_keyword)\nhidden_connections","7dcf5c0a":"plot_connection_graph(['COVID-19', 'H18ps-H1ss', 'novel_S-OIV', 'MNM'], topn=10)","53f1a0f6":"## Training\n","71bb7d0c":"### Explore connections across a set of words \/ phrases","9cf736ae":"### Train a word2vec model","7a1b45d5":"### Interpreting the graphs\nThe orange nodes are the requested words, whereas the blue nodes are the words that connect them. The darker the blue node, the more it contributes to the connection of the requested words. This \"connection strength\" is determined by the sum of the weights of the edges connected to this node. The weight of the edge is given by the cosine similarity between the input and output embeddings of the two nodes it connects, represented also by the shade of the node. The darker the node, the larger it's weight. In a sense, the weight of the edge represents how likely the two nodes are to be mentioned close to each other in text.","8ea75b52":"# Word and Phrase Associations in CORD-19 Corpus\nThis notebook aims to explore associations between words and phrases using methods similar to [Tshitoyan et al., Nature 571, 95\u201398 (2019)](https:\/\/perssongroup.lbl.gov\/papers\/dagdelen-2019-word-embeddings.pdf). It heavily utilizes code from the [mat2vec github repo](https:\/\/github.com\/materialsintelligence\/mat2vec). The notebook includes:\n* Loading, tokenizing, processing and saving the pre-processed text corpus (pre-processed dataset available).\n* Training a phraser to find common phrases and training a word2vec model using the new vocab (pre-trained model available).\n* Visualizing important connections between a set of words and phrases as a graph (e.g. \"COVID-19\" and \"vitamin C\").\n* Uncovering terms with a strong connection to a specific keyword but never mentined together in the same paragraph (experimental, this should become more useful once an additional dataset with papers other than COVID-19 is added).\n![image.png](attachment:image.png)\n\n## Next steps\n* Fine-tune a pre-trained word2vec model on biomedical text with covid-19 data\n * Tried on 2 of the more popular models and none of them were saved in a trainable state (http:\/\/bio.nlplab.org\/, https:\/\/github.com\/ncbi-nlp\/BioSentVec). \n* There seems to be a fine-tuned BERT base available at https:\/\/huggingface.co\/deepset\/covid_bert_base. Try using this or similar BERT models for the same task.\n* Get a larger biomedical text dataset, e.g. pubmed abstracts.\n* Optimize the phrases and the word embeddings using a set of relevant analogies.\n* Make a website for interactive exploration (e.g. graph nodes as links to the most relevant papers, etc.).","829d5c35":"### Identify hidden connections for a specific word \/ phrase [in development]\nThis should become more useful if the corpus \/ embeddings include terms not directly related to COVID-19.","713fe42b":"## Load and pre-process the data\nYou can either leave most of the variables unset to re-train everything from scratch, or supply some pre-processed and pre-trained file locations for faster execution and exploration. Leave it as is to use one of the pre-trained models.","8c4fa4d7":"## Exploration\n### Visualize word embeddings"}}