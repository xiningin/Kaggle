{"cell_type":{"39d6f5a3":"code","b0bb844b":"code","fddd43f6":"code","42919844":"code","23ce43e3":"code","6910bd14":"code","e95adbf1":"code","28b7193a":"code","f211af19":"code","36edde08":"code","ab6987f0":"code","a630b724":"code","5ce9d5c2":"code","c2edd4a3":"code","7a811245":"code","d431a6c1":"code","a90bfa00":"code","03a60d94":"code","6d65ea6e":"code","49a09a97":"code","0dcd9190":"code","33aa1701":"code","97f60582":"code","1eacb4fd":"code","d127c594":"code","ccff6e0c":"code","ce73f334":"code","acf65dec":"code","5dc83258":"code","2e4b9eec":"code","01749900":"code","7f749f75":"code","73e0710a":"code","f9aa30d8":"code","2a11601c":"code","3df4f6ab":"code","78718145":"code","2036e563":"code","99a145ff":"code","f1721704":"code","a5c3e44b":"code","0ee02388":"code","34422dd3":"code","ff603960":"code","26368d34":"code","4f54795f":"code","f34a06f4":"code","124a4365":"code","99d5b56c":"code","dbbd01e7":"code","6c30400b":"code","d15aee68":"code","0b9da2b2":"code","3f9bb244":"code","a3b3fac8":"code","03c17d1a":"code","496a7357":"code","8ed4e50c":"code","b53c6135":"code","168ce1b3":"code","f846e350":"code","f6eeebbf":"code","24c334ec":"code","bafab6c6":"code","0aeb20d6":"code","fbec68b5":"code","2fb73cee":"code","f7e84d88":"code","9e27610c":"markdown","76887765":"markdown","c66f847b":"markdown","41e1ff3f":"markdown","3838914d":"markdown","966ea6f4":"markdown","d5418717":"markdown","5de0b8a1":"markdown","ec1eff6f":"markdown","9344479d":"markdown","05e91bdb":"markdown","0272034b":"markdown","c76d1ddb":"markdown","3bfefe9f":"markdown","bdbf1937":"markdown","976860e8":"markdown","8e85fc2b":"markdown","2801b565":"markdown","ea27e8bd":"markdown"},"source":{"39d6f5a3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b0bb844b":"train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\", index_col ='Id')\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\", index_col ='Id')\n","fddd43f6":"\ntrain.head(5)","42919844":"train.info()","23ce43e3":"print(\"List of Columns:\\n\" , train.columns)\nprint(\"Shape of train data:\", train.shape)\nprint(\"Shape of test data:\", test.shape)","6910bd14":"train.describe().T","e95adbf1":"# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_cols = [cname for cname in train.columns if\n                    train[cname].nunique() < 10 and \n                    train[cname].dtype == \"object\"]\nhigh_cardinality_cols = [cname for cname in train.columns if\n                    train[cname].nunique() > 10 and \n                    train[cname].dtype == \"object\"]","28b7193a":"print(\"List of categorical columns:\\n\", categorical_cols+high_cardinality_cols)\nprint(\"Number of Categorical columns:\",len(categorical_cols+high_cardinality_cols))\nprint(\"High Cardinality cols:\",high_cardinality_cols )","f211af19":"# Select numerical columns\nnumerical_cols = [cname for cname in train.columns if \n                  train[cname].nunique() >60 and train[cname].dtype in ['int64', 'float64']]\nnumeric_category = [cname for cname in train.columns if \n                  train[cname].nunique() <60 and train[cname].dtype in ['int64', 'float64']]","36edde08":"print(\"List of numerical columns:\\n\", numerical_cols+numeric_category)\nprint(\"Number of numerical columns:\",len(numerical_cols+numeric_category))\nprint(\"List of convertable feature:\",numeric_category, len(numeric_category) )","ab6987f0":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom sklearn.preprocessing import LabelEncoder\n%matplotlib inline","a630b724":"fig, ax = plt.subplots()\nfig.set_size_inches(14,14)\nsns.heatmap(train[numerical_cols].corr(),center = 0, annot=True)","5ce9d5c2":"numeric_corr = train[numerical_cols].corr()['SalePrice'][:-1]\nhigh_corr = numeric_corr[abs(numeric_corr) > 0.5].sort_values(ascending=False)\nprint(\"List of High Correlation of features to SalePrice:\\n\", high_corr)\n","c2edd4a3":"fig, ax = plt.subplots()\nfig.set_size_inches(14,14)\nsns.heatmap(train[numeric_category + ['SalePrice']].corr(),center = 0, annot=True)","7a811245":"categ_corr = train[numeric_category+['SalePrice']].corr()['SalePrice'][:-1]\nhigh_catcor = categ_corr[abs(categ_corr) > 0.5].sort_values(ascending=False)\nprint(\"List of High Correlated features to SalesPrice:\\n\",high_catcor)","d431a6c1":"#cols_none = ['PoolQC', \"MiscFeature\", \"Alley\", \"Fence\",\"FireplaceQu\",'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond','BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\"MasVnrType\"]\n#train[cols_none] = train[cols_none].fillna(\"None\")\n#train[\"LotFrontage\"] = train.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n#    lambda x: x.fillna(x.median()))\n#cols_zero = ['GarageYrBlt', 'GarageArea', 'GarageCars','BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', \"MasVnrArea\",'BsmtFullBath', 'BsmtHalfBath']\n#train[cols_zero] = train[cols_zero].fillna(0)\n#train['MSZoning'] = train['MSZoning'].fillna(train['MSZoning'].mode()[0])\n#train['Electrical'] = train['Electrical'].fillna(train['Electrical'].mode()[0])\n#missing = train.isnull().sum()\n#missing = missing[missing > 0]\n#missing","a90bfa00":"train[numerical_cols].hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8)","03a60d94":"train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])","6d65ea6e":"sns.distplot(train['SalePrice'] , fit = norm)","49a09a97":"f = plt.figure(figsize=(20,20))\nfor i in range(len(numerical_cols)):\n    f.add_subplot(5, 4, i+1)\n    sns.scatterplot(train[numerical_cols].iloc[:,i], train['SalePrice'])\nplt.tight_layout()\nplt.show()","0dcd9190":"f = plt.figure(figsize=(20,20))\nfor i in range(len(numeric_category)):\n    f.add_subplot(5, 4, i+1)\n    sns.scatterplot(train[numeric_category].iloc[:,i], train['SalePrice'])\nplt.tight_layout()\nplt.show()","33aa1701":"sns.set_style(\"whitegrid\")\nmissing = train.isnull().sum()\nmissing = missing[missing > 0]\nmissing.sort_values(inplace = True)\nmissing.plot.bar()","97f60582":"cols_none = ['PoolQC', \"MiscFeature\", \"Alley\", \"Fence\",\"FireplaceQu\",'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond','BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\"MasVnrType\"]\ntrain[cols_none] = train[cols_none].fillna(\"None\")\ntrain[\"LotFrontage\"] = train.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\ncols_zero = ['GarageYrBlt', 'GarageArea', 'GarageCars','BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', \"MasVnrArea\",'BsmtFullBath', 'BsmtHalfBath']\ntrain[cols_zero] = train[cols_zero].fillna(0)\ntrain['MSZoning'] = train['MSZoning'].fillna(train['MSZoning'].mode()[0])\ntrain['Electrical'] = train['Electrical'].fillna(train['Electrical'].mode()[0])\nmissing = train.isnull().sum()\nmissing = missing[missing > 0]\nmissing","1eacb4fd":"train = train.drop(train['LotFrontage'][train['LotFrontage']>200].index)","d127c594":"train = train.drop(train['BsmtFinSF1'][train['BsmtFinSF1']>2000].index)","ccff6e0c":"train = train.drop(train['LotArea'][train['LotArea']>100000].index)","ce73f334":"train = train.drop(train['BsmtFinSF2'][train['BsmtFinSF2']>1200].index)\ntrain = train.drop(train['WoodDeckSF'][train['WoodDeckSF']>800].index)\ntrain = train.drop(train['OpenPorchSF'][train['OpenPorchSF']>450].index)\ntrain = train.drop(train['EnclosedPorch'][train['EnclosedPorch']>500].index)\ntrain = train.drop(train['ScreenPorch'][train['SalePrice']>500000].index)","acf65dec":"train = train.drop(train['TotalBsmtSF'][train['TotalBsmtSF']>2700].index)","5dc83258":"train = train.drop(train['LotArea'][train['SalePrice']>700000].index)","2e4b9eec":"train = train.drop(train['YearBuilt'][(train['YearBuilt']<1900) & (train['SalePrice'] > 400000)].index)","01749900":"train = train.drop(train['MasVnrArea'][train['MasVnrArea']>1200].index)","7f749f75":"#MSSubClass=The building class\ntrain['MSSubClass'] = train['MSSubClass'].apply(str)\n\n#Changing OverallCond into a categorical variable\ntrain['OverallCond'] = train['OverallCond'].astype(str)\n\n#Year and month sold are transformed into categorical features.\ntrain['YrSold'] = train['YrSold'].astype(str)\n","73e0710a":"target = train['SalePrice'].values\ntrain.drop(['SalePrice'], axis=1, inplace=True)","f9aa30d8":"cat_cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\nfor c in cat_cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(train[c].values)) \n    train[c] = lbl.transform(list(train[c].values))\n\n# shape        \nprint('Shape: {}'.format(train.shape))\n","2a11601c":"#https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n#Skewed features\nnumeric_feats = train.dtypes[train.dtypes != \"object\"].index\n# Check the skew of all numerical features\nskewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","3df4f6ab":"#https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\nskewness = skewness[abs(skewness) > 0.75]\n\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    train[feat] = boxcox1p(train[feat], lam)\n\ntrain[skewed_features] = np.log1p(train[skewed_features])","78718145":"\ntrain = pd.get_dummies(train)\n","2036e563":"print(train.shape)","99a145ff":"from sklearn.linear_model import ElasticNet, Lasso\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","f1721704":"lasso = make_pipeline(RobustScaler(), Lasso())","a5c3e44b":"ENet = make_pipeline(RobustScaler(), ElasticNet())","0ee02388":"KRR = KernelRidge()","34422dd3":"GBoost = GradientBoostingRegressor()","ff603960":"model_xgb = xgb.XGBRegressor()","26368d34":"model_lgb = lgb.LGBMRegressor()","4f54795f":"forest_reg = RandomForestRegressor()","f34a06f4":"#Validation function\nn_folds = 5\n\n#def rmsle_cv(model):\n  #  kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n #   rmse= np.sqrt(-cross_val_score(model, train.values, target, scoring=\"neg_mean_squared_error\", cv = kf))\n#    return(rmse)\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    scores= np.sqrt(-cross_val_score(model, train.values, target, scoring=\"neg_mean_squared_error\", cv = kf))\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std(),'\\n')","124a4365":"rmsle_cv(lasso)","99d5b56c":"rmsle_cv(ENet)\n","dbbd01e7":"rmsle_cv(KRR)\n","6c30400b":"rmsle_cv(GBoost)","d15aee68":"rmsle_cv(model_xgb)","0b9da2b2":"rmsle_cv(model_lgb)","3f9bb244":"rmsle_cv(forest_reg)","a3b3fac8":"X_train, X_valid, y_train, y_valid = train_test_split(train, target, \n                                                                train_size=0.8, test_size=0.2,\n                                                                random_state=0)","03c17d1a":"from sklearn.model_selection import GridSearchCV\n\n#param_grid = [    {'n_estimators': [200,240,280,320,400], 'max_features': [50, 60, 80, 90, 100]},    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},  ]\n#forest_reg = RandomForestRegressor()\n#grid_search = GridSearchCV(forest_reg, param_grid, cv=5,scoring='neg_mean_squared_error', return_train_score=True)\n#grid_search.fit(X_train, y_train)\n","496a7357":"#grid_search.best_params_","8ed4e50c":"def rmsle(y_train, pred):\n    return np.sqrt(mean_squared_error(y_train, pred))","b53c6135":"forest_reg = RandomForestRegressor(max_features = 80,n_estimators = 280, random_state =42)\n\nforest_reg.fit(X_train, y_train)\nforest_reg_train_pred = forest_reg.predict(X_train)\nforest_pred = forest_reg.predict(X_valid)\nprint(rmsle(y_train, forest_reg_train_pred))","168ce1b3":"rmsle_cv(forest_reg)","f846e350":"#param_grid = [    {'alpha': [0.00005,0.0005,0.005,0.05,0.5], 'max_iter': [1000,2000,3000]},   ]\n#lasso = Lasso()\n#grid_search = GridSearchCV(lasso, param_grid, cv=5,scoring='neg_mean_squared_error', return_train_score=True)\n#grid_search.fit(X_train, y_train)\n","f6eeebbf":"#grid_search.best_params_","24c334ec":"#https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1, max_iter = 2000))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3,max_iter = 2000))\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=1825,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","bafab6c6":"print(\"Lasso\")\nrmsle_cv(lasso)\nprint(\"Elastic Net\")\nrmsle_cv(ENet)\nprint(\"Kernel Ridge Regressor\")\nrmsle_cv(KRR)\nprint(\"Gradient Boosting Regressor\")\nrmsle_cv(GBoost)\nprint(\"Extreme Gradient Boosting\")\nrmsle_cv(model_xgb)\nprint(\"Light GBM\")\nrmsle_cv(model_lgb)\nprint(\"Random Forest Regressor\")\nrmsle_cv(forest_reg)\n","0aeb20d6":"GBoost.fit(X_train, y_train)\nGBoost_train_pred = GBoost.predict(X_train)\nGBoost_pred = GBoost.predict(X_valid)\nprint(rmsle(y_train, GBoost_train_pred))","fbec68b5":"model_xgb.fit(X_train, y_train)\nmodel_xgb_train_pred = model_xgb.predict(X_train)\nmodel_xgb_pred = model_xgb.predict(X_valid)\nprint(rmsle(y_train, model_xgb_train_pred))","2fb73cee":"print(model_xgb.predict(X_train[:5]))\nprint(y_train[:5])","f7e84d88":"forest_reg.fit(X_train, y_train)\nforest_reg_train_pred = forest_reg.predict(X_train)\nforest_pred = forest_reg.predict(X_valid)\nprint(rmsle(y_train, forest_reg_train_pred))","9e27610c":"**Select and train models**","76887765":"This is a heatmap of numeric features that can be converted to categorical features. Most of them are not correlated. We see that TotRmsAbvGrd and BedroomAbvGrd are highly correlated, we can experiment with these later.","c66f847b":"**Fine-tune model**","41e1ff3f":"Histogram for each numerical attributes.","3838914d":"**Fixing numerical feautres that are truly categorical**","966ea6f4":"Summary of numerical attributes:","d5418717":"Scatterplot numerical attributes vs saleprice","5de0b8a1":"Check for missing values. We can see that Fireplace, Fence, MiscFeature, PoolQC has a lot of no values. We can assume that zero values has no fireplace, fence, alley or pool. ","ec1eff6f":"**Data Cleaning**\n\n**Fixing Missing Values**\n\nThere are missing values in our dataset. There are houses that has no garage, pool, fireplace etc. we can change the NaN input as \"none\". And other missing input we can get their mode and median. We can also use SImpleImputer() to fix mssing values.\n","9344479d":"Let's quick check the data.\nLook for NaN, null and missing values.\n","05e91bdb":"Make a heatmap to see the correlation of numeric features. \n\nList brown and blue correlation not including SalePrice:\n\n**GarageYrBlt and YearBuilt**\n\n**1stFlrSF and TotalBsmtSF**\n\n**GrLivArea and 2ndFlrSF**\n\n**BsmtUnfSF and BsmtFinSF**\n\n**GarageYrBlt and YearRemodAdd**\n\nWe have option to drop or retain correlated features.","0272034b":"**Frame the problem**\n\nBased on the competition's overview, we are tasked to predict sales prices and practice feature engineering, Random forest and gradient boosting. And later we will try to improve our results using advance algorithms.\n\nFor now we will load and explore the data.","c76d1ddb":"**Removing Outliers**","3bfefe9f":"We can see that there are a lot of zeros in the summary of numerical data. It may mean that it is a non-numeric features or numbered-qualitative features.","bdbf1937":"we have 37 numerical columns and 43 categorical columns. there are also columns that have null values.","976860e8":"Looks like a lot of our numerical features are skewed. Check features that can be log transform.","8e85fc2b":"**Evaluate Models**","2801b565":"**Handling Text and Categorical Attributes**","ea27e8bd":"Group features by numerical and categorical, so it will be easy to transform and fit.\nCheck for cardinality of categorical features. Low cardinality categories will be easy to transform.\nCheck for numeric features that has few unique values. We will check if its ok to convert to categorical feature.\n\n**Visualize data:**"}}