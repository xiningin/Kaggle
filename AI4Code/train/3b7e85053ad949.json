{"cell_type":{"70c8e3c6":"code","7f86955a":"code","7e8f011a":"code","7ef7fe37":"code","e615f4ae":"code","a2dc0539":"code","b6727131":"code","d3d011ca":"code","fb8336e6":"code","5ad41aef":"code","ce4d5762":"code","49b28420":"code","466b34b1":"code","909c8f93":"code","1e2cabcd":"code","d92570fd":"code","92b42ddd":"code","0e38dffe":"code","339bfa1e":"code","3c88e35e":"code","4cce2f2c":"code","f8b2435f":"code","ff15bbbc":"code","130a8270":"code","2804ff07":"code","5c7883cc":"code","1bf54b7a":"markdown","9e72e198":"markdown","342a89e3":"markdown","8508ceec":"markdown","fe813658":"markdown","3dae6271":"markdown","825cb407":"markdown","d1dec5fd":"markdown","4be05c71":"markdown","aee75979":"markdown","18d18c5d":"markdown"},"source":{"70c8e3c6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7f86955a":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\ntest_id=test.loc[:,['id']]\n","7e8f011a":"import re\nimport nltk\nimport os \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nfrom sklearn.metrics import roc_curve,roc_auc_score,auc,accuracy_score","7ef7fe37":"train.head() #Basic overview of data","e615f4ae":"test.head()","a2dc0539":"print(train.info()) #Info about the dataset\nprint('*'*50)\nprint(train['target'].value_counts()) #distribution of dependent variable\n","b6727131":"#number of unique values in each column:\nprint(\"Unique values in each column:-\")\nfor col in train.columns:\n    print(f\"{col}:- {train[col].nunique()}\")\n    ","d3d011ca":"#Top 10 keyword and their count\nplt.plot(figsize=(10,10))\nfig = plt.gcf()\nfig.set_size_inches(10,5,forward=True)\n\nvalues= train['keyword'].value_counts().sort_values(ascending=False).head(10)\nplt.bar(values.index,values,width=0.8,color='rgb')\nplt.xticks(rotation=45)\nplt.show()\n\n","fb8336e6":"#Top 10 locations and their count\nplt.plot()\nfig = plt.gcf()\nfig.set_size_inches(10,5,forward=True)\n\nvalues= train['location'].value_counts().sort_values(ascending=False).head(10)\nplt.bar(values.index,values,width=0.8,color='rgb')\nplt.xticks(rotation=45)\nplt.show()\n","5ad41aef":"#train test split\nfrom sklearn.model_selection import train_test_split\ndef get_split_data(data):\n    # we divide the whole data into 90% train and 10% validation set.\n    # then we will validate models n validation set  \n    trainx,valx,trainy,valy = train_test_split(data.iloc[:,:-1],data.iloc[:,-1],test_size=0.1)\n    train = pd.concat([trainx,trainy],axis=1)\n    validate =pd.concat([valx,valy],axis=1)\n    return train,validate\n\n#\ntrain.drop(['id','location','keyword'],axis=1,inplace=True)\ntest.drop(['id','keyword','location'],axis=1,inplace=True)\n","ce4d5762":"\n\ndef text_cleaning(s):\n    #we need to remmove all the trailing\/extra whitespaces\n    #lower all the sentences\n    #removing stopwords except 'not' and 'can' and removing special characters except '?'\n    #also remove @name i.e mentions in tweets\n    # replace 'n' by 'not'\n    s =s.lower()\n    s = re.sub(r\"\\'n\",' not',s)\n    # Remove @name\n    s = re.sub(r'(@.*?)[\\s]', ' ', s)\n    # Isolate and remove punctuations except '?'\n    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\\/\\,])', r' \\1 ', s)\n    s = re.sub(r'[^\\w\\s\\?]', ' ', s)\n    # Remove some special characters\n    s = re.sub(r'([\\;\\:\\|\u2022\u00ab\\n])', ' ', s)\n    # Remove stopwords except 'not' and 'can'\n    s = \" \".join([word for word in s.split()\n                  if word not in stopwords.words('english')\n                  or word in ['not', 'can']])\n    # Remove trailing whitespace\n    s = re.sub(r'\\s+', ' ', s).strip()\n    \n    return s\n\n    ","49b28420":"#split data\nTrain,validation = get_split_data(train)\ntrainy = Train.loc[:,['target']]\nvaly = validation.loc[:,['target']]\nTrain.drop(['target'],axis=1,inplace=True)\nvalidation.drop(['target'],axis=1,inplace=True)\n\n#preprocess and clean data\ntrainx_preprocess = np.array([text_cleaning(text) for text in Train['text']])\nvalx_preprocess = np.array([text_cleaning(text) for text in validation['text']])\n\n#tfidf vectorize data\ntfidf = TfidfVectorizer(ngram_range=(1,3),binary=True,\n                       smooth_idf=False)\ntrainx_tfidf = tfidf.fit_transform(trainx_preprocess)\nvalx_tfidf = tfidf.transform(valx_preprocess)","466b34b1":"from sklearn.model_selection import cross_val_score,StratifiedKFold\n\ndef evaluate_metric(model,trainx_tfidf,trainy):\n    #get AUC score of a model\n    fold = StratifiedKFold(5, shuffle=True,random_state=1)\n    #auc score\n    auc = cross_val_score(model,trainx_tfidf,trainy,cv=fold,scoring='roc_auc')\n    \n    return auc.mean()","909c8f93":"#checking optimal values of alpha(hyperparameter for our baseline model)\n\nfrom sklearn.naive_bayes import MultinomialNB\n\nres = pd.Series([evaluate_metric(MultinomialNB(i)) for i in np.arange(1,10,0.1)],index=np.arange(1,10,0.1))\nbest_param = np.round(res.idxmax(),2)\n\nprint(f\"Best alpha: '{best_param}'\")\nplt.plot(res)\nplt.title(\"AUC vs Alpha\")\nplt.xlabel('Alpha')\nplt.ylabel('AUC')\nplt.show()\n","1e2cabcd":"#evaluation\n\ndef evaluate_roc(predict_prob,y_true):\n    \n    preds =predict_prob[:,1]\n    fpr,tpr,threshold = roc_curve(y_true,preds)\n    roc_auc = auc(fpr,tpr)\n    print(f'ROC_AUC:- {roc_auc:.4f}')\n    \n    #get accuracy\n    y_pred  =np.where(preds>0.5,1,0)\n    accuracy = accuracy_score(y_true,y_pred)\n    print(f'Accuracy:- {accuracy*100:.2f}%')\n        \n\n    \n    # Plot ROC AUC\n    plt.title('ROC Curve')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n","d92570fd":"# computing predicted probabilities for baseline model\nnb_model = MultinomialNB(alpha=1.5)\nnb_model.fit(trainx_tfidf, trainy)\nprobs = nb_model.predict_proba(valx_tfidf)\n\n# Evaluate the classifier\nevaluate_roc(probs, valy)","92b42ddd":"#submission\npreprocessed_test = [text_cleaning(text) for text in test['text']]\ntext_tfidf= tfidf.transform(preprocessed_test)\n\n#predict\nop_probs =nb_model.predict_proba(text_tfidf)\npredict = np.where(op_probs[:,1]>0.5,1,0)\n\n#submission\nsample_submission['target']=predict\nsample_submission.to_csv(\"submission.csv\",index=False)","0e38dffe":"trainx_preprocess[0]","339bfa1e":"#checking with two dropped features\ntrn =train.copy()\ntst = test.copy()\n\ntst.replace(np.nan,'NA',inplace=True)\ntrn.replace(np.nan,'NA',inplace=True)\n\n#checking unique values\ntst['location'].value_counts().sort_values(ascending=False).head(5)\n","3c88e35e":"#we can drop loation but we need to make top 15 keywords as a data\nimp_features = trn['keyword'].value_counts().sort_values(ascending=False).head(15).index\n\n\ndef feature_add(data):\n    for col in imp_features:\n        data[col] = np.where(data['keyword']==col,1,0)\n    data.drop(['keyword'],axis=1,inplace=True)\n    return data\ntrn= feature_add(trn)\ntst =feature_add(tst)","4cce2f2c":"##### again we apply all that process as above\n\n#splitting train data into train and validation\nTrain,validation = get_split_data(trn)\ntrainy = Train.loc[:,['target']]\nvaly = validation.loc[:,['target']]\nTrain.drop(['id','target','location'],axis=1,inplace=True)\nvalidation.drop(['id','target','location'],axis=1,inplace=True)\n\n#preprocess and clean data\ntrainx_preprocess = np.array([text_cleaning(text) for text in Train['text']])\nvalx_preprocess = np.array([text_cleaning(text) for text in validation['text']])\n\n","f8b2435f":"#tfidf vectorize data\ntfidf = TfidfVectorizer(ngram_range=(1,3),binary=True,\n                       smooth_idf=False)\n\n\ntrainx_tfidf = tfidf.fit_transform(trainx_preprocess)\nvalx_tfidf = tfidf.transform(valx_preprocess)\nTrain =Train.reset_index(drop=True)\nvalidation = validation.reset_index(drop=True)\n","ff15bbbc":"#final_data\nfrom scipy.sparse import coo_matrix, hstack,csr_matrix\n\nimp_col= [col for col in Train.columns if col!='text']\n\nkeyword_sparse_train= csr_matrix(Train.loc[:,imp_col].values)\nkeyword_sparse_val = csr_matrix(validation.loc[:,imp_col].values)\n\n\ntrain_final = hstack([keyword_sparse_train,trainx_tfidf])\nval_final =  hstack([keyword_sparse_val,valx_tfidf])","130a8270":"#finding optimal values for alpha\n\nfrom sklearn.naive_bayes import MultinomialNB\n\nres = pd.Series([evaluate_metric(MultinomialNB(i),train_final,trainy) for i in np.arange(0.1,20,0.5)],index=np.arange(0.1,20,0.5))\nbest_param = np.round(res.idxmax(),2)\n\nprint(f\"Best alpha: '{best_param}'\")\nplt.plot(res)\nplt.title(\"AUC vs Alpha\")\nplt.xlabel('Alpha')\nplt.ylabel('AUC')\nplt.show()\n","2804ff07":"# computing predicted probabilities for baseline model\nnb_model = MultinomialNB(alpha=0.6)\nnb_model.fit(train_final, trainy)\nprobs = nb_model.predict_proba(val_final)\n\n# Evaluate the classifier\nevaluate_roc(probs, valy)","5c7883cc":"#submission\npreprocessed_test = [text_cleaning(text) for text in test['text']]\ntext_tfidf= tfidf.transform(preprocessed_test)\n\ntst.drop(['id','location'],axis=1,inplace=True)\n\nkeyword_sparse_tst = csr_matrix(tst.loc[:,imp_col].values)\ntst_final =  hstack([keyword_sparse_tst,text_tfidf])\n\n#predict\nop_probs =nb_model.predict_proba(tst_final)\npredict = np.where(op_probs[:,1]>0.5,1,0)\n\n#submission\nsample_submission['target']=predict\nsample_submission.to_csv(\"submission.csv\",index=False)","1bf54b7a":"#  Method2: Checking accuracy including 'keyword' column features","9e72e198":"## Baseline Model (Naive Bayes)\n","342a89e3":"# Important functions","8508ceec":"# Importing important libraries","fe813658":"# Basic EDA","3dae6271":"# Baseline Model( TFIDF + Naive baiye's)\nIn this baseline  model i basically used the TFIDF to vectorize the text data, then i used Naive bayes classifier to train our model with\n(90%)training data and test it as a bseline model in our validation data. Naive bayes classification is one of the most suitable classifier \nfor text data in sklearn.\n\nFurther we can also utilize the two dropped columns as a categorical variables by converting location data into countries or city and then keeping top 20 countries as its features. Similarly on keywords we can pick top 15 keywords data from that column.\n","825cb407":"## Submission","d1dec5fd":"## Hence we observed that accuracy improved a bit on validation set.\n## Previous:79.40%\n## Current: 79.92%","4be05c71":"## cleaning the data:","aee75979":"## TFIDF Vectorizer","18d18c5d":"# Importing data"}}