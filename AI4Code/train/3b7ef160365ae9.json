{"cell_type":{"0afb5b8c":"code","15662be4":"code","0920ff4d":"code","b0cfac9c":"code","c69664a1":"code","c4d7f016":"code","c5ec9cb5":"code","46eb1b4c":"code","4ff95c0f":"code","3595385d":"code","0311c131":"code","d6ffcc2f":"code","8afc9c38":"code","6811d17e":"code","952534fc":"code","06a8e8a3":"code","cc69451c":"code","ff124411":"code","9effd83c":"code","97cd540c":"code","b3c6e972":"code","47235b89":"code","9225f552":"code","451c693f":"code","8f449e05":"code","82501936":"code","51c91e2c":"code","f09bd655":"code","933baf0c":"code","c4467f24":"code","b222fe53":"code","781fc860":"code","a2f3784a":"code","dbfd4247":"code","019d2bce":"code","0562b467":"code","097c813f":"code","b6cad5a4":"code","a9b35b7b":"code","af5bd513":"code","40c236c1":"code","f7ec2ea5":"code","6a0752d7":"code","c85255e4":"code","e8fdf80d":"code","c085d8e7":"code","c51e186b":"code","841d402d":"markdown","96823030":"markdown","a46e33ce":"markdown","871290dc":"markdown","f490f395":"markdown","d100456a":"markdown","1278d0d3":"markdown","80347d92":"markdown","40385df3":"markdown","74bb2d66":"markdown","83eb1217":"markdown","6d9f52af":"markdown","63c64b34":"markdown","0a7d6584":"markdown","e415b621":"markdown","33acbc76":"markdown","f81a113d":"markdown","904aa0bc":"markdown","d707ba44":"markdown","ca4fe58e":"markdown","1e0f0545":"markdown","10273de5":"markdown","fe1390fa":"markdown","9f58bb42":"markdown","dbf999c9":"markdown","e593a53f":"markdown"},"source":{"0afb5b8c":"#Python Packages\nimport time #Used to timestamp submission files\n\n#Data Handling\nimport pandas as pd\nimport numpy as np\n\n#Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Machine Learning and data manipulation\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Ensure Notebook displays visuals\nsns.set_style('white')\n%matplotlib inline","15662be4":"#Access the project data\ndatapath = \"..\/input\/titanic\/\"\n\n#The files with paths for training and test data\ntrain_file = datapath + \"train.csv\"\ntest_file = datapath + \"test.csv\"","0920ff4d":"#Open dataframes for both training and test data\ntrain_df = pd.read_csv(train_file, index_col='PassengerId')\ntest_data = pd.read_csv(test_file)\n\n#Show head of training data\ntrain_df.head()","b0cfac9c":"train_df.info()\n#We can see that some columns are missing data in the training data","c69664a1":"#A heatmap can be used to visualize missing data, making it easier to see where focus is needed\n#Initial Formatting\nfig, axes = plt.subplots(figsize=(6,6), constrained_layout=True)\n\n#Graph\ntrain_miss = sns.heatmap(train_df.isnull(),yticklabels=False,cbar=False,cmap='cividis',ax=axes)\n\n#Additional Formatting\nfig_title = fig.suptitle('Missing Values',fontsize=24)\naxes_title = axes.set_title('Training Dataset',fontsize=16)\naxes_format = axes.set(ylabel=None)\naxes_xlabels = axes.set_xticklabels(train_miss.get_xticklabels(),size=14)","c4d7f016":"#We should also investigate the test data to see if we need to clean or handle any of that data.\n#Initial Formatting\nfig, axes = plt.subplots(figsize=(6,6), constrained_layout=True)\n\n#Graph\ntest_miss = sns.heatmap(test_data.isnull(),yticklabels=False,cbar=False,cmap='cividis',ax=axes)\n\n#Additional Formatting\nfig_title = fig.suptitle('Missing Values',fontsize=24)\naxes_title = axes.set_title('Testing Dataset',fontsize=16)\naxes_format = axes.set(ylabel=None)\naxes_xlabels = axes.set_xticklabels(train_miss.get_xticklabels(),size=14)","c5ec9cb5":"#Initial Formatting\nfig, axes = plt.subplots(figsize=(8,6), constrained_layout=True)\n\n#Graph\ncls_dst = sns.countplot(x='Survived',data=train_df,ax=axes,palette='colorblind')\n\n#Additional Formatting\nfig_title = fig.suptitle('Class Distribution',fontsize=24)\naxes_title = axes.set_title('Training Dataset',fontsize=16)\naxes_ylabel = axes.set_ylabel('Count',size=14)\naxes_yticklabel = axes.set_yticklabels([int(x) for x in cls_dst.get_yticks()],size=12)\naxes_xlabel = axes.set_xlabel('Survival',size=14)\naxes_xticklabel = axes.set_xticklabels(['Did Not Survive','Survived'],size=12)","46eb1b4c":"#Calculate class distribution\ndns = (train_df['Survived'].value_counts()[0]\/len(train_df))*100\nsurv = (train_df['Survived'].value_counts()[1]\/len(train_df))*100\n\n#Show distribution\nprint('Class Distribution - Training Dataset')\nprint(f'Did Not Survive: {dns:4.2f}%')\nprint(f'Survived: {surv:4.2f}%')","4ff95c0f":"#We can further dive into the data by using hue\n\n#Initial Formatting\nfig, axes = plt.subplots(figsize=(8,6), constrained_layout=True)\n\n#Graph\nsrv_sex = sns.countplot(x='Survived', hue='Sex', data=train_df,ax=axes,palette='colorblind')\n\n#Additional Formatting\nfig_title = fig.suptitle('Survival by Sex',fontsize=24)\naxes_title = axes.set_title('Training Dataset',fontsize=16)\naxes_ylabel = axes.set_ylabel('Count',size=14)\naxes_yticklabel = axes.set_yticklabels([int(x) for x in srv_sex.get_yticks()],size=12) \naxes_xlabel = axes.set_xlabel('Survival',size=14)\naxes_xticklabel = axes.set_xticklabels(['Did Not Survive','Survived'],size=12)\ngraph_legend = srv_sex.legend(['Male','Female'],fontsize=14,title='Sex',title_fontsize=14)","3595385d":"#Pivot tables can be used to see how various attributes correlate to survival\ntrain_df.pivot_table(values='Survived',index='Sex',aggfunc=np.mean)\n#We can see that 74.2% of females survived while only 18.9% of males survived","0311c131":"#Initial Formatting\nfig, axes = plt.subplots(figsize=(8,6), constrained_layout=True)\n\n#Graph\nsrv_pclass = sns.countplot(x='Survived', hue='Pclass', data=train_df,ax=axes,palette='colorblind')\n\n#Additional Formatting\nfig_title = fig.suptitle('Survival by Passenger Class',fontsize=24)\naxes_title = axes.set_title('Training Dataset',fontsize=16)\naxes_ylabel = axes.set_ylabel('Count',size=14)\naxes_yticklabel = axes.set_yticklabels([int(x) for x in srv_pclass.get_yticks()],size=12) \naxes_xlabel = axes.set_xlabel('Survival',size=14)\naxes_xticklabel = axes.set_xticklabels(['Did Not Survive','Survived'],size=14)\ngraph_legend = srv_pclass.legend(['1st Class','2nd Class','3rd Class'],fontsize=14,title='Passenger Class',title_fontsize=14)","d6ffcc2f":"train_df.pivot_table(values='Survived',index='Pclass',aggfunc=np.mean)\n#Class 2 passengers are almost split, leaning slightly towards not surviving, with only 47.3% surviving and 52.7% not surviving.","8afc9c38":"#Initial Formatting\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14,6), constrained_layout=True, sharey=True)\n\n#Graphs\nage_surv = sns.histplot(data=train_df[train_df['Survived']==1],x='Age',bins=30,ax=axes[1])\nage_dead = sns.histplot(data=train_df[train_df['Survived']==0],x='Age',bins=30,ax=axes[0])\n\n#Additional Formatting\nfigtitle = fig.suptitle('Survival by Age - Training Dataset',fontsize=24)\naxeszero_ylabel = axes[0].set_ylabel('Count',size=14)\naxeszero_yticklabel = axes[0].set_yticklabels([int(x) for x in age_dead.get_yticks()],size=12)\naxeszero_title = axes[0].set_title('Did Not Survive', fontsize=14)\naxeszero_xticklabel = axes[0].set_xticklabels([int(x) for x in age_dead.get_xticks()],size=12) \naxeszero_xlabel = axes[0].set_xlabel('Age',size=14)\naxesone_title = axes[1].set_title('Survived', fontsize=14)\naxesone_xticklabel = axes[1].set_xticklabels([int(x) for x in age_surv.get_xticks()],size=12)\naxesone_xlabel = axes[1].set_xlabel('Age',size=14)","6811d17e":"#Initial Formatting\nfig, axes = plt.subplots(figsize=(8,6), constrained_layout=True)\n\n#Graph\nagedist_pclass = sns.boxplot(x='Pclass',y='Age',data=train_df,palette='colorblind',ax=axes)\n\n#Additional Formatting\nfig_title = fig.suptitle('Age Distribution by Passenger Class',fontsize=24)\naxes_title = axes.set_title('Training Dataset',fontsize=16)\naxes_ylabel = axes.set_ylabel('Count',size=14)\naxes_yticklabel = axes.set_yticklabels([int(x) for x in agedist_pclass.get_yticks()],size=12)\naxes_xlabel = axes.set_xlabel('Passenger Class',size=14)\naxes_xticklabel = axes.set_xticklabels(['1st Class','2nd Class','3rd Class'],size=12)","952534fc":"train_df.pivot_table(values='Age',index='Pclass',aggfunc=np.mean)\n#We can use the mean age per class to impute missing age values","06a8e8a3":"#Initial Formatting\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14,6), constrained_layout=True, sharey=True)\n\n#Graphs\nfare_surv = sns.histplot(data=train_df[train_df['Survived']==1],x='Fare',bins=15,ax=axes[1])\nfare_dead = sns.histplot(data=train_df[train_df['Survived']==0],x='Fare',bins=15,ax=axes[0])\n\n#Additional Formatting\nfigtitle = fig.suptitle('Survival by Fare - Training Dataset',fontsize=24)\naxeszero_title = axes[0].set_title('Did Not Survive', fontsize=14)\naxeszero_ylabel = axes[0].set_ylabel('Count',size=14)\naxeszero_yticklabel = axes[0].set_yticklabels([int(x) for x in fare_dead.get_yticks()],size=12)\naxeszero_xticklabel = axes[0].set_xticklabels([int(x) for x in fare_dead.get_xticks()],size=12)\naxeszero_xlabel = axes[0].set_xlabel('Fare',size=14)\naxesone_title = axes[1].set_title('Survived', fontsize=14)\naxesone_xticklabel = axes[1].set_xticklabels([int(x) for x in fare_surv.get_xticks()],size=12)\naxesone_xlabel = axes[1].set_xlabel('Fare',size=14)","cc69451c":"#Initial Formatting\nfig, axes = plt.subplots(figsize=(8,6), constrained_layout=True)\n\n#Graph\nfaredist_pclass = sns.boxplot(x='Pclass',y='Fare',data=train_df,palette='colorblind',ax=axes, showfliers=False) #Omitting outliers to make the visual easier to read\n\n#Additional Formatting\nfig_title = fig.suptitle('Fare Distribution by Passenger Class',fontsize=24)\naxes_title = axes.set_title('Training Dataset',fontsize=16)\naxes_ylabel = axes.set_ylabel('Count',size=14)\naxes_yticklabel = axes.set_yticklabels([int(x) for x in faredist_pclass.get_yticks()],size=12)\naxes_xlabel = axes.set_xlabel('Passenger Class',size=14)\naxes_xticklabel = axes.set_xticklabels(['1st Class','2nd Class','3rd Class'],size=12)","ff124411":"train_df.pivot_table(values='Fare',index='Pclass',aggfunc=np.mean)\n#We can use the mean fare per class to impute missing age values","9effd83c":"#Instead of looking at the number of siblings\/children\/parents on the ship, looking at family simplifies two variables\n#To investigate if family helped, create a function to create a has family attribute\ndef has_family(columns):\n  sib = columns[0]\n  par = columns[1]\n\n  if (sib > 0) or (par > 0):\n    return 1\n  else:\n    return 0","97cd540c":"#Create the has family attribute\ntrain_df['has_fam'] = train_df[['SibSp','Parch']].apply(has_family,axis=1)","b3c6e972":"#Initial Formatting\nfig, axes = plt.subplots(figsize=(8,6), constrained_layout=True)\n\n#Graph\nsrv_fam = sns.countplot(x='Survived', hue='has_fam', data=train_df,palette='colorblind',ax=axes)\n\n#Additional Formatting\nfig_title = fig.suptitle('Survival by Family Status',fontsize=24)\naxes_title = axes.set_title('Training Dataset',fontsize=16)\naxes_ylabel = axes.set_ylabel('Count',size=14)\naxes_yticklabel = axes.set_yticklabels([int(x) for x in srv_fam.get_yticks()],size=12) \naxes_xlabel = axes.set_xlabel('Survival',size=14)\naxes_xticklabel = axes.set_xticklabels(['Did Not Survive','Survived'],size=14)\ngraph_legend = srv_fam.legend(['Traveled w\/o Family','Traveled w\/ Family'],fontsize=14,title='Family Status',title_fontsize=14)","47235b89":"train_df.pivot_table(values='Survived',index='has_fam',aggfunc=np.mean)\n#Looking at actual numbers, not having family means the passenger was more likely to not survive, making this seems like a useful attribute","9225f552":"#Let us just look at whether a passenger has a cabin.\ndef has_cabin(row):\n  if pd.isnull(row):\n    return 0\n  else:\n    return 1","451c693f":"#Create a has_cabin attribute to show whether a passenger had a cabin.\ntrain_df['has_cabin'] = train_df['Cabin'].apply(has_cabin)","8f449e05":"train_df.pivot_table(values='has_cabin',index='Pclass',aggfunc=np.sum)\n#We can see that while first class passengers have cabins, lower class passengers didn't","82501936":"#Initial Formatting\nfig, axes = plt.subplots(figsize=(8,6), constrained_layout=True)\n\n#Graph\nsrv_cabin = sns.countplot(x='Survived', hue='has_cabin', data=train_df,palette='colorblind',ax=axes)\n\n#Additional Formatting\nfig_title = fig.suptitle('Survival by Cabin Status',fontsize=24)\naxes_title = axes.set_title('Training Dataset',fontsize=16)\naxes_ylabel = axes.set_ylabel('Count',size=14)\naxes_yticklabel = axes.set_yticklabels([int(x) for x in srv_cabin.get_yticks()],size=12) \naxes_xlabel = axes.set_xlabel('Survival',size=14)\naxes_xticklabel = axes.set_xticklabels(['Did Not Survive','Survived'],size=14)\ngraph_legend = srv_cabin.legend(['No Cabin','Has Cabin'],fontsize=14,title='Cabin Status',title_fontsize=14)","51c91e2c":"train_df.pivot_table(values='Survived',index='has_cabin',aggfunc=np.mean)\n#Having a cabin was highly correlated with survival and vice versa","f09bd655":"#Open a the training dataframe for the ML task\n#This data will be messy\ntrain_messy = pd.read_csv(train_file, index_col='PassengerId')","933baf0c":"#Function to impute age based on clase\n#The average age per class was calculated from the training dataset\ndef impute_age(columns):\n  Age = columns[0]\n  Pclass = columns[1]\n\n  if pd.isnull(Age):\n    if Pclass == 1:\n      return 38\n    elif Pclass == 2:\n      return 30\n    else:\n      return 25\n  else:\n    return Age","c4467f24":"#Function to impute fare based on passenger clase\n#The average fare per class was calculated from the training dataset\ndef impute_fare(columns):\n  fare = columns[0]\n  Pclass = columns[1]\n\n  if pd.isnull(fare):\n    if Pclass == 1:\n      return 84.15\n    elif Pclass == 2:\n      return 20.66\n    else:\n      return 13.67\n  else:\n    return fare","b222fe53":"#Function to convert embark to numerical categorical variable\ndef convert_embark(embark):\n  if embark == 'C':\n    return 0\n  elif embark == 'Q':\n    return 1\n  else:\n    return 2","781fc860":"#Function to do initial cleansing of the dataset,\n#including imputing missing values.\ndef clean_attributes(df):\n  #Impute values for missing ages\n  df['Age'] = df[['Age','Pclass']].apply(impute_age,axis=1)\n\n  #Impute values for missing fares\n  df['Fare'] = df[['Fare','Pclass']].apply(impute_fare,axis=1)\n\n  #Create has_cabin\n  df['has_cabin'] = df['Cabin'].apply(has_cabin)\n\n  #Convert Embarked\n  df['Embarked'] = df['Embarked'].apply(convert_embark)\n\n  #Drop unneeded columns\n  df.drop(['Name','Ticket','Cabin'],axis=1,inplace=True)\n\n  #Drop any records with null values and return the dataframe\n  return pd.DataFrame(df)","a2f3784a":"#Function to process data through cleansing and feature generation\ndef process_data(df):\n\n  #clean the data first\n  df = clean_attributes(df)\n\n  #Then create features and shit\n  #Create has_family\n  df['has_fam'] = df[['SibSp','Parch']].apply(has_family,axis=1)\n\n  #Convert Sex\n  psex = pd.get_dummies(df['Sex'],drop_first=True)\n  df = pd.concat([df,psex],axis=1)\n\n  #Drop columns (Sex, Sibsp, Parch, Embarked)\n  df.drop(['Sex','SibSp','Parch'],axis=1,inplace=True)\n\n  feature_list = ['has_fam','has_cabin','male','Pclass','Age','Fare','Embarked']\n\n  feat_df = df[feature_list]\n\n  #Return the dataframe\n  return pd.DataFrame(feat_df)","dbfd4247":"#Generate X and y for training and validating the machine learning model\nX, y = process_data(train_messy.iloc[ : , 1:11 ]), train_messy['Survived']","019d2bce":"#Initial Formatting\nfig, axes = plt.subplots(figsize=(6,6), constrained_layout=True)\n\n#Graph\ntrain_cln = sns.heatmap(X.isnull(),yticklabels=False,cbar=False,cmap='cividis',ax=axes)\n\n#Additional Formatting\nfig_title = fig.suptitle('Missing Values - Cleaned Data',fontsize=24)\naxes_title = axes.set_title('Training Dataset',fontsize=16)\naxes_format = axes.set(ylabel=None)\naxes_xlabels = axes.set_xticklabels(train_cln.get_xticklabels(),size=12)","0562b467":"#Normalize the attributes to help the classifier\nscaler = MinMaxScaler(feature_range=(0,1))\nscaler.fit(X)\nsX = scaler.transform(X)","097c813f":"#Create a set for training and another for validating the model.\nsX_train, sX_val, y_train, y_val = train_test_split(sX,y,test_size=0.2,random_state=101)","b6cad5a4":"#Parameter grid for the gridsearch\n#These are the parameters that the gridsearch with methodical work through\n#to determine the best set for the task at hand.\nparams = {\n    'n_estimators': [300, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [3, 4, 5],\n    'criterion' :['gini', 'entropy']\n}\n#This parameter grid has been shortened to help the runtime of this notebook.","a9b35b7b":"#Create a Random Forest classifier object\nrf = RandomForestClassifier(random_state=101)\n\n#Create a Gridsearch object\nrf_gs = GridSearchCV(estimator=rf,param_grid=params,scoring='accuracy',cv=5,verbose=True)","af5bd513":"#Fit the Gridsearch object using the normalized training data\n#Note, this can take some time depending on the parameter grid\nrf_gs.fit(sX_train,y_train)","40c236c1":"#Generate predictions from the trained gridsearch object\ny_pred = rf_gs.predict(sX_val)","f7ec2ea5":"#Print out performance information for the gridsearch object\nprint(classification_report(y_val,y_pred))\nprint(f'The accuracy score of the best model is: {accuracy_score(y_val,y_pred)*100:4.2f}%')","6a0752d7":"#Create a dataframe of the confusion matrix data from the predictions of the \n#gridsearch object\ncm_df = pd.DataFrame(confusion_matrix(y_val,y_pred),columns=np.unique(y_val),index=np.unique(y_val))\ncm_df.index.name = \"Actual\"\ncm_df.columns.name = \"Predicted\"","c85255e4":"#Create confusion matrix heatmap of the gridsearch object predictions\n#Initial Formatting\nfig, axes = plt.subplots(figsize=(6,6), constrained_layout=True)\n\n#Graph\ncnfmtx_pipeline = sns.heatmap(data=cm_df, cmap='Blues',annot=confusion_matrix(y_val,y_pred),annot_kws={\"size\": 16}, fmt='d',ax=axes)\n\n#Additional Formatting\nfig_title = fig.suptitle('Confusion Matrix',fontsize=24)\naxes_title = axes.set_title('GridSearchCV Random Forest',fontsize=14)\naxes_Xax = axes.set_xlabel('Predicted', fontsize=16)\naxes_Yax = axes.set_ylabel('Actual', fontsize=16)\naxes_xtick = axes.set_xticklabels(labels=[0,1],fontsize=14)\naxes_ytick = axes.set_yticklabels(labels=[0,1],fontsize=14)","e8fdf80d":"#Open and prepare test data\ntest_messy = pd.read_csv(test_file)\ntest, p_id = process_data(test_messy.iloc[ : , 1:11 ]), test_messy['PassengerId']","c085d8e7":"#Visually confirm test data was properly processed and in good form\n#Initial Formatting\nfig, axes = plt.subplots(figsize=(6,6), constrained_layout=True)\n\n#Graph\ntest_cln = sns.heatmap(test.isnull(),yticklabels=False,cbar=False,cmap='cividis',ax=axes)\n\n#Additional Formatting\nfig_title = fig.suptitle('Missing Values - Cleaned Data',fontsize=24)\naxes_title = axes.set_title('Test Dataset',fontsize=16)\naxes_format = axes.set(ylabel=None)\naxes_xlabels = axes.set_xticklabels(train_cln.get_xticklabels(),size=12)","c51e186b":"#Normalize the test data with the fit scaler\nstest = scaler.transform(test)\n\n#Generate the predictions based on the normalized test data\ntest_pred = rf_gs.predict(stest)\n\n#Generate a dataframe for the test predictions\nsub_df = pd.DataFrame()\nsub_df['PassengerId'] = p_id\nsub_df['Survived'] = test_pred\n\n#Save the predictions as a csv file\nsub_df.to_csv(f'Titanic_Submission__{time.strftime(\"%Y%m%d_%H%M%S\")}.csv',index=False)","841d402d":"#### Sex\n\nBeing male meant you were more likely to not survive, while females were more likely to survive.\n\nThis will be a useful feature for the ML model.\n* This attribute will need to be converted to a numerical categorical attribute.","96823030":"#### Other Considerations\n\n**Embarked**\n\nThe Embarked attribute could have been analyzed for its correlation for survival to determine whether it is a useful feature. Subject matter knowledge would also be helpful for understanding this attribute and any apparent correlation it may have.\n* The attribute will be used by using a simple function to convert the attribute to numerical categorical attribute.\n\n**Name**\n\nThe Names attribute could have been analyzed; however, the messiness of the data make analysis nontrivial. While it seems to be a messy column, it does contain information on the titles of passengers (such as Mr., Mrs., Dr., etc.). Since having a prestigious title could also be correlated to being wealthy, such as being in 1st class and having a cabin, that could be another useful feature. \n* For simplicity, the Name attribute was not used for this project.\n\n\n**Ticket**\n\nThe Ticket attribute is a messy attribute and was not included. It may have potentially useful information but is an example of another column that would most likely benefit from subject matter knowledge and expertise.\n* For simplicity, the Ticket attribute was not used for this project.\n\n**Age** & **Fare**\n\nUsing the **mean** to impute missing values for these attributes may not be the best choice. Another measure of central tendancy, such as **mode**, could be a better choice for imputting values.\n* Thank you [nizarh](https:\/\/www.kaggle.com\/michaelwilder\/titanic-survival-eda-and-randomforest\/comments#1501044) for making this suggestion.\n","a46e33ce":"#### Passenger Class\n\nBeing a 3rd class passenger was also bad for survival chances (only 24.2% survived), while being in a higher class was better for survival (62.9% of 1st class passengers survived).\n\nThis will be a useful feature for the ML model.","871290dc":"# Data Cleansing and Feature Generation\n\nFrom EDA we have learned that we need to do the following steps:\n* Convert Sex to a numerical categorical attribute\n* Impute Age for records with missing Age values\n* Impute Fare for records with missing Fare values\n* Convert SibSp and Parch to a has_family attribute\n* Convert Cabin to a has_cabin attribute\n* Convert Embarked to a numerical categorical attribute\n","f490f395":"#### Age\n\nThe correlation between age and survival isn't as directly apparent, but there are some correlations.\n\nFor example, children under 5 years old had a very high likelihood of being among the survivors. Meanwhile individuals in their 20s-30s were at a higher likelihood of not surviving.","d100456a":"## First Look at the Data\n\nWith the first look at the dataset, it is clear there is potentially a lot of work to do to get the data ready for a ML classifier. There are multiple text columns, some of which contain text (Name and Ticket) while others are categorical (like Sex and Embarked). There are also multiple numeric columns, including categorical values and real numbers. Diving into the data with EDA will help provide information on how to handle the various attributes in the dataset, and potentially provide insight on useful features that could be generated.\n\n* Passenger Class (Pclass) is numeric\n* Name is a string\n* Sex is a string that is either male or female\n* Age is a float\n* SibSp and Parch are both integers\n* Ticket is a string\n* Fare is a float\n* Cabin is a string\n* Embarked is a char","1278d0d3":"With this subject matter knowledge, we can analysis the Cabin attribute properly and see that it provides a good correlation to survival.\n\nThe has_cabin feature will be a useful feature for the ML model.\n","80347d92":"#### Cabin\n\nThis column seems tempting to drop due to the large number of missing values and the messy state of the data within it. However, that would be incorrect.\n\nThe Cabin attribute highlights the usefulness of subject matter knowledge and expertise in data analytics and data mining projects. While most 1st class passengers would have had their own cabin, lower class passengers would have stayed in communal rooms without cabin numbers. So, there is a reason there are so many empty values within the Cabin attribute.\n","40385df3":"With functions written to clean, process, and generate features from the dataset, the training features X and the labels y can be created.\n\n","74bb2d66":"#### Class Distribution\n\nClass distribution can influence many choices in a ML project, so it is important to understand that as soon as possible.\n\nWhile more passengers did not survive than passengers that did, the class distribution isn't so heavily skewed that it has to be taken into account.","83eb1217":"### Analyzing Survival\n\nSince the task at hand is determining whether a passenger survived, the EDA will be mainly focused on looking at the various attributes and their relationship to survival.","6d9f52af":"## Exploratory Data Analysis","63c64b34":"As we can see, not having family on board was not good for a passenger's survival odds. These finding show that this 'has_family' feature could be useful for the ML task.\n\nThe has_family feature will be a useful feature for the ML model.","0a7d6584":"And we can visually verify that the training features is a quality dataset with no missing values.","e415b621":"# Introduction\n\nThis notebook is a brief exploration of exploratory data analysis (EDA) and how it can aid in data preparation and feature generation. A supervised machine learning (ML) task will be completed as an overarching project to explore these topics of interest.\n\nThe ML task being completed is from the Kaggle competition [Titanic - Machine Learning from Disaster](https:\/\/www.kaggle.com\/c\/titanic), in which the goal is to classify whether individual passengers on the Titanic may have survived based on their provided passenger information. To complete this task, the Scikit Learn Python library will be used, and a Random Forest Decision Tree classifier will be trained. Seaborn and Matplotlib will provide data visualization for EDA and other parts of the project. Finally, the Pandas and NumPy  libraries will be utilized for data handling.","33acbc76":"## Train Test Split\n\nSplit sX into training data and a set aside a set of validation data to use on the trained model to validate performance.","f81a113d":"# Submission\n\nWith a model trained and validated, the testing data can be prepared, fed into the ML model for predictions, and a submission created for the Kaggle competition.","904aa0bc":"# Data Exploration\n\nData exploration is crucial for any data mining project. It is important to understand the dataset you are working with and exploring that dataset can give the analyst a good insight into what parts of the dataset is useful and which aren't. Exploration can also help an analyst find out hidden information within the dataset that wasn't apparent at first. This EDA will allow us to understand the work that will be needed to prepare the data for further analysis later in the project.","d707ba44":"# Random forest - GridsearchCV\n\nTo complete the ML task, a Random Decision Tree Forest will be used to generate a trained ML model.\n\nTo tune the hyperparameter of the Random Forest classifier, a GridsearchCV with cross validation will be utilized to determine which set of hyperparameters provide the best model accuracy.","ca4fe58e":"#### Family\n\nThe SibSp and Parch attributes contain information on the number of family members on board. While it is possible to explore the relationship between the number of family members onboard and survival, and easier analysis that can be completed initially is too look at whether having family on board affects survival in the first place.","1e0f0545":"## Access Data\n\nThis notebook uses Google drive to access stored data. These cells should be updated if the notebook is run in a different location\/environment, or by another person.\n\nThis notebook uses the data from the Kaggle competition described above and can be found [here](https:\/\/www.kaggle.com\/c\/titanic\/data).","10273de5":"### Missing Values\n\nOne important thing to investigate is missing values within a dataset. If an attribute is missing values, the analyst must decide what to do with that attribute, which could include imputing new values into missing values or even dropping the attribute all together. It is important to consider each attribute to determine the best course of action when dealing with missing values.\n","fe1390fa":"## Normalization\n\nBecause there are both categorical attributes (such as has_fam and Pclass) along with numeric attributes (Age and Fare), normalizing the data will bring all features within the same range. This can help prevent a feature like Fare, which has much larger numbers than the rest of the dataset, from influencing the classifier. Normalization can be useful in some situations, however not all classifiers need to use normalized data.","9f58bb42":"As we can see, the Age attribute has some missing values (about 20% of values are missing in the training dataset), but that attribute seems like a good candidate for having values imputed for records missing age. The same can be said about the Fare and Embarked attributes.\n\nThe Cabin attribute is missing large numbers of values (about 77% of values are missing in the training dataset) and could be a candidate for being dropped. However, this feature should still be explored and researched some before determining whether to keep or drop the attribute.","dbf999c9":"We see that most 3rd class passengers are in their 20s and 30s, which helps explain the higher numbers of non-survivors in those age ranges.\n\nBecause passenger class is also correlated with passenger survival, and that attribute is complete (no missing values), the average age of passengers by class could be used to impute missing age values for records missing the age.\n\nThis will be a useful feature for the ML model.\n* New age values must be imputed for missing values. The average age per class will be used to impute age for missing values based on the record's class.\n","e593a53f":"#### Fare\n\nLike Age above, there is no major correlation of fare to survival apparent. Most notably, those who paid lower fares were more likely to not survive. This is in line with the rest of the data: passengers in 3rd class were more likely to not survive and a 3rd class ticket would cost less on average.\n\nSo, while there is not a strong apparent correlation for Fare, there seems to be some useful information captured within this attribute.\n\nBecause there is a correlation between Fare and Passenger Class, and there is a correlation between Passenger Class and Fare, this attribute will be used for the ML model.\n* New fare values must be imputed for missing values. The average fare per class will be used to impute fare for missing values based on the record's class.\n"}}