{"cell_type":{"585f4af2":"code","49135512":"code","90c0ebcf":"code","a57bd106":"code","67133a9f":"code","7ec75bbe":"code","559040d3":"code","52fb12d7":"code","da6304b6":"code","55b16f1f":"code","819d55a8":"code","290de861":"code","8e45a064":"code","6c49cb6d":"code","68e72cae":"code","944a59cc":"code","a064c17f":"code","75f8d116":"code","a076b13a":"code","5be835ea":"markdown","1b66ac1e":"markdown","a6b50d2a":"markdown","23a40a43":"markdown"},"source":{"585f4af2":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport os\nimport ast\nimport datetime as dt\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [16, 10]\nplt.rcParams['font.size'] = 14\nimport seaborn as sns\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport keras\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, Activation\nfrom keras.metrics import categorical_accuracy, top_k_categorical_accuracy, categorical_crossentropy\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom keras.optimizers import Adam\nfrom keras.applications import MobileNet\nfrom keras.applications.mobilenet import preprocess_input\nfrom keras.preprocessing.sequence import pad_sequences\n%matplotlib inline","49135512":"import time\ntic = time.time()","90c0ebcf":"np.random.seed(seed=1988)\ntf.set_random_seed(seed=1988)","a57bd106":"DP_DIR = '..\/input\/shuffle-csvs\/'\nINPUT_DIR = '..\/input\/quickdraw-doodle-recognition\/'\n\n\nBASE_SIZE = 256\nNCSVS = 100\nNCATS = 340\n\ndef f2cat(filename: str) -> str:\n    return filename.split('.')[0]\n\ndef list_all_categories():\n    files = os.listdir(os.path.join(INPUT_DIR, 'train_simplified'))\n    return sorted([f2cat(f) for f in files], key=str.lower)","67133a9f":"def avg_precision(actual, predicted, k=3):\n    if not actual:\n        return 0.\n    if len(predicted)>k:\n        predicted = predicted[:k]\n    score = 0.0\n    num_hits = 0.0\n    for i, n in enumerate(predicted):\n        if n in actual and n not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits\/(i + 1.)\n    return score\/min(len(actual),k)\n\ndef mapk(actual, predicted, k=3):\n    return np.mean([avg_precision(a,p,k) for a,p in zip(actual, predicted)])\n\ndef preds2catids(predictions):\n    return pd.DataFrame(np.argsort(-predictions, axis=1)[:,:3], columns=['a','b','c'])\n\ndef top_3_accuracy(y_true, y_pred):\n    return top_k_categorical_accuracy(y_true, y_pred, k=3)","7ec75bbe":"def _stack_it(stroke_vec):\n    \"\"\"preprocess the string and make \n    a standard Nx3 stroke vector\"\"\"\n    # unwrap the list\n    in_strokes = [(xi,yi,i)  \n     for i,(x,y) in enumerate(stroke_vec) \n     for xi,yi in zip(x,y)]\n    c_strokes = np.stack(in_strokes)\n    # replace stroke id with 1 for continue, 2 for new\n    c_strokes[:,2] = [1]+np.diff(c_strokes[:,2]).tolist()\n    c_strokes[:,2] += 1 # since 0 is no stroke\n    # pad the strokes with zeros\n    return pad_sequences(c_strokes.swapaxes(0, 1), \n                         maxlen=STROKE_COUNT, \n                         padding='post').swapaxes(0, 1)","559040d3":"def draw_cv2(raw_strokes, size=256, lw=6, time_color=True):\n    img = np.zeros((BASE_SIZE, BASE_SIZE), np.uint8)\n    for t, stroke in enumerate(raw_strokes):\n        for i in range(len(stroke[0])-1):\n            color = 255 - min(t, 10) * 13 if time_color else 255\n            _ = cv2.line(img, (stroke[0][i], stroke[1][i]),\n                        (stroke[0][i+1], stroke[1][i+1]), color, lw)\n    if size != BASE_SIZE:\n        return cv2.resize(img, (size,size))\n    else:\n        return img","52fb12d7":"batchsize = 256\nSTROKE_COUNT = 196\nSTEPS = 800\nEPOCHS = 20","da6304b6":"def image_generator_xd( batchsize, ks, lw=6, time_color=True):\n    while True:\n        for k in np.random.permutation(ks):\n            filename = os.path.join(DP_DIR, 'train_k{}.csv.gz'.format(k))\n            if not os.path.exists(filename):\n                continue\n            for df in pd.read_csv(filename, chunksize=batchsize):\n                # Generator of multiple batches. Each iter is on single file of batchsize\n                df['drawing'] = df['drawing'].apply(ast.literal_eval)\n                x = np.zeros((len(df), 196, 3))\n                for i, raw_strokes in enumerate(df.drawing.values):\n                    x[i, :, :] = _stack_it(raw_strokes)\n                y=keras.utils.to_categorical(df.y, num_classes=NCATS) # y should be equal to the word\n                yield x,y\n\ndef df_to_image_array_xd(df, lw=6, time_color=True):\n    df['drawing'] = df['drawing'].apply(ast.literal_eval)\n    x = np.zeros((len(df), 196, 3))\n    for i, raw_strokes in enumerate(df.drawing.values):\n        x[i,:,:] = _stack_it(raw_strokes)\n    return x","55b16f1f":"valid_df = pd.read_csv(os.path.join(DP_DIR, 'train_k{}.csv.gz'.format(NCSVS - 1)), nrows=34000)\nx_valid = df_to_image_array_xd(valid_df)\ny_valid = keras.utils.to_categorical(valid_df.y, num_classes=NCATS)\nprint(x_valid.shape, y_valid.shape)\nprint('Validation array memory {:.2f} GB'.format(x_valid.nbytes \/ 1024.**3 ))\n\n","819d55a8":"train_datagen = image_generator_xd( batchsize=batchsize, ks=range(NCSVS - 1))\nx, y = next(train_datagen)","290de861":"from keras.models import Sequential\nfrom keras.layers import BatchNormalization, Conv1D, LSTM, Dense, Dropout\nstroke_read_model = Sequential()\nstroke_read_model.add(BatchNormalization(input_shape = (None,)+x.shape[2:]))\n# filter count and length are taken from the script https:\/\/github.com\/tensorflow\/models\/blob\/master\/tutorials\/rnn\/quickdraw\/train_model.py\nstroke_read_model.add(Conv1D(48, (5,)))\nstroke_read_model.add(Dropout(0.3))\nstroke_read_model.add(Conv1D(64, (5,)))\nstroke_read_model.add(Dropout(0.3))\nstroke_read_model.add(Conv1D(96, (3,)))\nstroke_read_model.add(Dropout(0.3))\nstroke_read_model.add(LSTM(128, return_sequences = True))\nstroke_read_model.add(Dropout(0.3))\nstroke_read_model.add(LSTM(128, return_sequences = False))\nstroke_read_model.add(Dropout(0.3))\nstroke_read_model.add(Dense(512))\nstroke_read_model.add(Dropout(0.3))\nstroke_read_model.add(Dense(NCATS, activation = 'softmax'))\nstroke_read_model.compile(optimizer = 'adam', \n                          loss = 'categorical_crossentropy', \n                          metrics = ['categorical_accuracy', top_3_accuracy])\nstroke_read_model.summary()","8e45a064":"weight_path=\"{}_weights.best.hdf5\".format('stroke_lstm_model_generator')\n\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min', save_weights_only = True)\n\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, \n                                   verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\nearly = EarlyStopping(monitor=\"val_loss\", \n                      mode=\"min\", \n                      patience=5) # probably needs to be more patient, but kaggle time is limited\ncallbacks_list = [checkpoint, early, reduceLROnPlat]","6c49cb6d":"stroke_read_model.fit_generator(train_datagen, \n                      validation_data = (x_valid, y_valid), \n                                verbose=1,\n                                steps_per_epoch=STEPS,\n                      epochs = EPOCHS,\n                      callbacks = callbacks_list)","68e72cae":"valid_predictions = stroke_read_model.predict(x_valid, batch_size=128, verbose=1)\nmap3 = mapk(valid_df[['y']].values, preds2catids(valid_predictions).values)\nprint('Map3: {:.3f}'.format(map3))","944a59cc":"test = pd.read_csv(os.path.join(INPUT_DIR, 'test_simplified.csv'))\ntest.head()\nx_test = df_to_image_array_xd(test)\nprint(test.shape, x_test.shape)\nprint('Test array memory {:.2f} GB'.format(x_test.nbytes \/ 1024.**3 ))","a064c17f":"test_predictions = stroke_read_model.predict(x_test, batch_size=128, verbose=1)\n\ntop3 = preds2catids(test_predictions)\ntop3.head()\ntop3.shape\n\ncats = list_all_categories()\nid2cat = {k: cat.replace(' ', '_') for k, cat in enumerate(cats)}\ntop3cats = top3.replace(id2cat)\ntop3cats.head()\ntop3cats.shape","75f8d116":"test['word'] = top3cats['a'] + ' ' + top3cats['b'] + ' ' + top3cats['c']\nsubmission = test[['key_id', 'word']]\nsubmission.to_csv('lstm_generator_submission_{}.csv'.format(int(map3 * 10**4)), index=False)\nsubmission.head()\nsubmission.shape","a076b13a":"toc=time.time()\nprint('Total time taken: %0.2f sec'%(toc-tic))","5be835ea":"## Training with Image Generator","1b66ac1e":"* * # Keras Using LSTM on full Data\n\n","a6b50d2a":"## Setup\nImport the necessary libraries and a few helper functions.","23a40a43":"## Create Submission"}}