{"cell_type":{"3cc9aa92":"code","03e4ff33":"code","cc523181":"code","abfccd58":"code","e7d0d79b":"code","b251b10b":"code","00c06487":"code","6ce5eb7c":"code","2ccdcc36":"code","5b0cc7a2":"code","b5b9aad1":"code","b68507b5":"code","60fd38f5":"code","91ec7848":"code","4f2d82dc":"code","4eb6f03b":"code","65fecdcf":"code","0cece410":"code","c7bf9b63":"code","b3d45bc9":"code","3534d235":"code","6cd016ba":"code","14c63227":"code","9e4f4235":"code","a9d937b8":"code","3f0abc26":"code","80a9c8fe":"code","113bd965":"code","1775dc75":"code","1ae26b97":"code","e79e4d4a":"code","83cbcd9c":"code","2936ed88":"code","533c5302":"code","c5faf83f":"code","c3a9d250":"code","6e22b4b7":"code","0e3d4630":"code","95d4cf05":"code","4b5e3fb5":"code","1d92fb8c":"markdown","dacf7d32":"markdown","dc7071a4":"markdown","17cc5ae2":"markdown","69ffade7":"markdown","b66733fa":"markdown","9aa1f89f":"markdown","ef30ddfb":"markdown","b63d11f1":"markdown","13557b06":"markdown","d7cf7074":"markdown","9ed896b1":"markdown","ffbed641":"markdown","acdc3f31":"markdown","11aca238":"markdown","0dc6cbb4":"markdown","6a4543fb":"markdown","eab383c5":"markdown","c6521e1c":"markdown","c0121be3":"markdown","6771cc3c":"markdown","e13b7b79":"markdown","e37ecc94":"markdown","0b48e7af":"markdown","f9edf5e5":"markdown","cf970aae":"markdown","d7417e92":"markdown","60d4781c":"markdown","2b760ed5":"markdown","f79afdba":"markdown","b4432ec0":"markdown","e2884b49":"markdown","4a56b64f":"markdown","81ed089e":"markdown","836552ab":"markdown","12ead591":"markdown","cbb217de":"markdown","296aea24":"markdown"},"source":{"3cc9aa92":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# import library for machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_curve","03e4ff33":"train_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\ncombine = [train_df, test_df]","cc523181":"train_df.head()","abfccd58":"print(train_df.columns.values)","e7d0d79b":"print(\"train shape:\",train_df.shape)\nprint(\"test shape :\",test_df.shape)\n","b251b10b":"train_df.info()\nprint('_______________________________________________')\ntest_df.info()","00c06487":"train_df.isnull().sum()\n","6ce5eb7c":"\nfrom pandas_profiling import ProfileReport \n\nprofile = ProfileReport( train_df, title='Pandas profiling report ' , html={'style':{'full_width':True}})\n\nprofile.to_notebook_iframe()\n","2ccdcc36":"train_df.describe()\n","5b0cc7a2":"train_df.describe(include=['O'])","b5b9aad1":"print(\"Before\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)\n\ntrain_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_df, test_df]\n\n\"After\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape","b68507b5":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_df['Title'], train_df['Sex'])","60fd38f5":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","91ec7848":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_df.head()","4f2d82dc":"train_df = train_df.drop(['Name', 'PassengerId'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.shape, test_df.shape\n","4eb6f03b":"for dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_df.head()","65fecdcf":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')\ngrid = sns.FacetGrid(train_df, row='Pclass', col='Sex', size=4.2, aspect=2)\ngrid.map(plt.hist, 'Age', alpha=.75, bins=40)\ngrid.add_legend()","0cece410":"guess_ages = np.zeros((2,3))\nguess_ages","c7bf9b63":"for dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain_df.head()","b3d45bc9":"train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","3534d235":"for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ntrain_df.head()","6cd016ba":"train_df = train_df.drop(['AgeBand'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()","14c63227":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","9e4f4235":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","a9d937b8":"train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [train_df, test_df]\n\ntrain_df.head()","3f0abc26":"for dataset in combine:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n\ntrain_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)","80a9c8fe":"freq_port = train_df.Embarked.dropna().mode()[0]\nfreq_port","113bd965":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","1775dc75":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\ntrain_df.head()","1ae26b97":"test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\ntest_df.head()","e79e4d4a":"train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","83cbcd9c":"for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_df = train_df.drop(['FareBand'], axis=1)\ncombine = [train_df, test_df]\n    \ntrain_df.head(10)","2936ed88":"test_df.head(10)","533c5302":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","c5faf83f":"\n#Applying VotingClassifier Model \n\n'''\n#ensemble.VotingClassifier(estimators, voting=\u2019hard\u2019, weights=None,n_jobs=None, flatten_transform=None)\n'''\n\n#loading models for Voting Classifier\n\nLRModel_ = LogisticRegression(solver='lbfgs', multi_class='multinomial',random_state=33)\n\nGBCModel_ = GradientBoostingClassifier(n_estimators=1000, learning_rate=1.0,max_depth=7, random_state=33)\n\nDTModel_ = DecisionTreeClassifier(criterion = 'entropy',max_depth=10,random_state = 33)\n\nRFModel_ = RandomForestClassifier(n_estimators=100, criterion='gini',max_depth=10, random_state=33)\n\nSVCModel_ = SVC(kernel = 'rbf', random_state = 33,C = 0.1,degree = 2)\n\nGaussianNBModel_ = GaussianNB()\n\nKNNModel_ = KNeighborsClassifier(n_neighbors= 4 , weights ='uniform', algorithm='auto')\n\nNNModel_ = MLPClassifier(solver='lbfgs',hidden_layer_sizes=(100, 3),learning_rate='constant',activation='tanh')\n\nSGDModel_ = SGDClassifier(loss='log', penalty='l2', max_iter=10000, tol=1e-5)\n\n#loading Voting Classifier\nVotingClassifierModel = VotingClassifier(estimators=[('LRModel',LRModel_),('GBCModel',GBCModel_),('DTModel',DTModel_),('RFModel',RFModel_),('SVCModel',SVCModel_),('GaussianNBModel',GaussianNBModel_),('KNNModel',KNNModel_),('NNModel',NNModel_),('SGDModel',SGDModel_)], voting='hard')\n\nVotingClassifierModel.fit(X_train, Y_train)\n\n#Calculating Details\nprint('VotingClassifierModel Train Score is : ' , VotingClassifierModel.score(X_train, Y_train))\n\nprint('----------------------------------------------------')\n\n\n#Calculating Prediction\nY_predtrain = VotingClassifierModel.predict(X_train)\nprint('Predicted Value for VotingClassifierModel is : ' , Y_predtrain[:10])\n\n","c3a9d250":"\nAccScore = accuracy_score(Y_train,Y_predtrain, normalize=False)\nprint('Accuracy Score is : ', AccScore)","6e22b4b7":"\n\nF1Score = f1_score(Y_train,Y_predtrain, average='micro') #it can be : binary,macro,weighted,samples\nprint('F1 Score is : ', F1Score)","0e3d4630":"\n#Calculating Confusion Matrix\nCM = confusion_matrix(Y_train,Y_predtrain)\nprint('Confusion Matrix is : \\n', CM)\n\n# drawing confusion matrix\nsns.heatmap(CM, center = True)\nplt.show()","95d4cf05":"\n\n#Calculating Prediction\nY_pred = VotingClassifierModel.predict(X_test)\nprint('Predicted Value for VotingClassifierModel is : ' , Y_pred[:10])\n\n","4b5e3fb5":"\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred })\n\nsubmission.to_csv('mysubmission.csv',index=False)","1d92fb8c":"Convert the Fare feature to ordinal values based on the FareBand.","dacf7d32":"### Converting categorical feature to numeric\n\nWe can now convert the EmbarkedFill feature by creating a new numeric Port feature.","dc7071a4":"# now let's writ the model \nI used voting classifier algorithm with many  different algorithms\n","17cc5ae2":"### Completing a numerical continuous feature\n\nNow we should start estimating and completing features with missing or null values. We will first do this for the Age feature.\n\nWe can consider three methods to complete a numerical continuous feature.\n\n1. A simple way is to generate random numbers between mean and [standard deviation](https:\/\/en.wikipedia.org\/wiki\/Standard_deviation).\n\n2. More accurate way of guessing missing values is to use other correlated features. In our case we note correlation among Age, Gender, and Pclass. Guess Age values using [median](https:\/\/en.wikipedia.org\/wiki\/Median) values for Age across sets of Pclass and Gender feature combinations. So, median Age for Pclass=1 and Gender=0, Pclass=1 and Gender=1, and so on...\n\n3. Combine methods 1 and 2. So instead of guessing age values based on median, use random numbers between mean and standard deviation, based on sets of Pclass and Gender combinations.\n\nMethod 1 and 3 will introduce random noise into our models. The results from multiple executions might vary. We will prefer method 2.","69ffade7":"We can replace many titles with a more common name or classify them as `Rare`.","b66733fa":"Finally, we save the prediction in the following file","9aa1f89f":"We can create another feature called IsAlone.","ef30ddfb":"Correcting by dropping features\nThis is a good starting goal to execute. By dropping features we are dealing with fewer data points. Speeds up our notebook and eases the analysis.\n\nBased on our assumptions and decisions we want to drop the Cabin (correcting #2) and Ticket (correcting #1) features.\n\nNote that where applicable we perform operations on both training and testing datasets together to stay consistent.\n\n","b63d11f1":"We can also create an artificial feature combining Pclass and Age.","13557b06":"I hope it will be useful to you","d7cf7074":"![Screenshot_2020-04-26-16-27-36-91.png](attachment:Screenshot_2020-04-26-16-27-36-91.png)","9ed896b1":"We can not create FareBand.","ffbed641":"### Completing a categorical feature\n\nEmbarked feature takes S, Q, C values based on port of embarkation. Our training dataset has two missing values. We simply fill these with the most common occurance.","acdc3f31":"In this part, I enlisted the help of Mr. (Manav Sehgal). I benefited a lot from him. This is for the scientific safety\n### Creating new feature extracting from existing\n\nWe want to analyze if Name feature can be engineered to extract titles and test correlation between titles and survival, before dropping Name and PassengerId features.\n\nIn the following code we extract Title feature using regular expressions. The RegEx pattern `(\\w+\\.)` matches the first word which ends with a dot character within Name feature. The `expand=False` flag returns a DataFrame.\n\n**Observations.**\n\nWhen we plot Title, Age, and Survived, we note the following observations.\n\n- Most titles band Age groups accurately. For example: Master title has Age mean of 5 years.\n- Survival among Title Age bands varies slightly.\n- Certain titles mostly survived (Mme, Lady, Sir) or did not (Don, Rev, Jonkheer).\n\n**Decision.**\n\n- We decide to retain the new Title feature for model training.","11aca238":"Let us create Age bands and determine correlations with Survived.","0dc6cbb4":"### Converting a categorical feature\n\nNow we can convert features which contain strings to numerical values. This is required by most model algorithms. Doing so will also help us in achieving the feature completing goal.\n\nLet us start by converting Sex feature to a new feature called Gender where female=1 and male=0.","6a4543fb":"### Quick completing and converting a numeric feature\n\nWe can now complete the Fare feature for single missing value in test dataset using mode to get the value that occurs most frequently for this feature. We do this in a single line of code.\n\nNote that we are not creating an intermediate new feature or doing any further analysis for correlation to guess missing feature as we are replacing only a single value. The completion goal achieves desired requirement for model algorithm to operate on non-null values.\n\nWe may also want round off the fare to two decimals as it represents currency.","eab383c5":"We can convert the categorical titles to ordinal.","c6521e1c":"Let us drop Parch, SibSp, and FamilySize features in favor of IsAlone.","c0121be3":"I will also follow some old methods of data analysis to increase focus","6771cc3c":"*We can not remove the AgeBand feature.","e13b7b79":"Before everything . I will import the libraries","e37ecc94":"### Create new feature combining existing features\n\nWe can create a new feature for FamilySize which combines Parch and SibSp. This will enable us to drop Parch and SibSp from our datasets.","0b48e7af":"# the test dataset is.","f9edf5e5":"# In this kernel, I will explain data analysis and model writing in a very simple and very modern way\n# I will use voting classifier  algorithm and I will use the wonderful library (pandas_profiling ) in the analysis of the data","cf970aae":"# our data spliting is ","d7417e92":"# pandas_profiling library \nGenerates profile reports from a pandas DataFrame. The pandas df.describe() function is great but a little basic for serious exploratory data analysis. pandas_profiling extends the pandas DataFrame with df.profile_report() for quick data analysis.\n\nFor each column the following statistics - if relevant for the column type - are presented in an interactive HTML report:\n\n* Type inference: detect the types of columns in a dataframe.\n* Essentials: type, unique values, missing values\n* Quantile statistics like minimum value, Q1, median, Q3, maximum, range, interquartile range\n* Descriptive statistics like mean, mode, standard deviation, sum, median absolute deviation, coefficient of variation, kurtosis, skewness\n* Most frequent values\n* Histogram\n* Correlations highlighting of highly correlated variables, Spearman, Pearson and Kendall matrices\n* Missing values matrix, count, heatmap and dendrogram of missing values\n* Text analysis learn about categories (Uppercase, Space), scripts (Latin, Cyrillic) and blocks (ASCII) of text data.\n\n**I will put a link to get more information about this library**\n[https:\/\/github.com\/pandas-profiling\/pandas-profiling](http:\/\/)","60d4781c":"# Calculating F1 Score  : 2 * (precision * recall) \/ (precision + recall)\n","2b760ed5":"# Analyze by describing data","f79afdba":"Lets take help of confusion matrix to find out TP TN FP FN.A confusion matrix is a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class. This is the key to the confusion matrix. The confusion matrix shows the ways in which your classification model is confused when it makes predictions. It gives us insight not only into the errors being made by a classifier but more importantly the types of errors that are being made.\n\n","b4432ec0":"# Let us replace Age with ordinals based on these bands.","e2884b49":"In the end, I would like to say that I worked hard on this kernel\n\n# Please appreciate this work and leave me your vote and comment on any negative or positive amendment\n\nI will be happy with it\n\nAnd thanks for reading","4a56b64f":"\n# Calculating Accuracy Score  : ((TP + TN) \/ float(TP + TN + FP + FN))\n ","81ed089e":"1. Let us start by preparing an empty array to contain guessed Age values based on Pclass x Gender combinations.","836552ab":"Now we can safely drop the Name feature from training and testing datasets. We also do not need the PassengerId feature in the training dataset.","12ead591":"Now we iterate over Sex (0 or 1) and Pclass (1, 2, 3) to calculate guessed values of Age for the six combinations.","cbb217de":"# Making and Printing our predictions\n\n","296aea24":"# Acquire data"}}