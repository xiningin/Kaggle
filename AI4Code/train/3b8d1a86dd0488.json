{"cell_type":{"697b81ae":"code","1b667df3":"code","50e2bb2c":"code","4ad78c92":"code","1c298766":"code","486af1e8":"code","4b7c4f7e":"code","78639f37":"code","997a2fe8":"code","f0ad5a55":"code","34f62586":"code","f95713b9":"code","c3e04508":"code","32742168":"code","6d7cd8d8":"code","a0e52ca5":"code","b20e07c8":"code","380e11a6":"code","4a2db3bc":"code","7db63f70":"code","1ee9a9e1":"code","aacfe367":"code","25347ed7":"code","4d5ebb18":"code","2bcdea5a":"code","7ab7a5a8":"code","851a520b":"code","ad5bee01":"code","8078fa1f":"code","601fc6fc":"code","089706be":"code","7f4d3133":"code","3557a157":"code","9659b9d5":"code","f2582912":"code","3d3db7ec":"code","24ef5edc":"code","dc712f4c":"code","51206472":"code","97dac53c":"code","11f323fe":"code","d0f29880":"code","57fc5dcf":"code","3bd19409":"code","befc1b69":"code","c5a4686e":"code","18757108":"code","533b2e94":"code","be4f0b06":"code","72007e07":"code","1e8565d1":"code","32e84634":"code","a7ef725d":"code","759e5804":"code","71cd60f1":"code","34e95c61":"code","3b509bb3":"code","1180fff2":"code","48380d19":"code","cbb6d6a0":"code","70d51c09":"code","3259eed5":"code","d9967c84":"code","2a69f392":"code","400bb328":"code","0bd1bf71":"markdown","8c726b1f":"markdown","0541b236":"markdown","4e3285ba":"markdown","3a4632c0":"markdown","24ecec89":"markdown","07275b3a":"markdown","01b2e306":"markdown","5f6f36c8":"markdown","2258f5ca":"markdown","69199091":"markdown","895f2310":"markdown","3b9ac416":"markdown","55c58c88":"markdown","04aabc6e":"markdown","5ca68469":"markdown","f028a50a":"markdown","2e8bac18":"markdown","de5baf82":"markdown","07447e38":"markdown","f227403a":"markdown","285e865b":"markdown","bf806e54":"markdown","1b9d1131":"markdown","1fbdeb71":"markdown","76d1f777":"markdown","4fe9deeb":"markdown","0eac08c0":"markdown","022c1112":"markdown","636087a5":"markdown","4cc53778":"markdown","970ebf5a":"markdown","93030a22":"markdown"},"source":{"697b81ae":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set(style = 'whitegrid')\nimport matplotlib.pyplot as plt\nimport plotly.offline as ply\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ninit_notebook_mode()","1b667df3":"from sklearn.preprocessing import StandardScaler\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import precision_score,recall_score","50e2bb2c":"df = pd.read_csv(\"..\/input\/diabetes.csv\")","4ad78c92":"df.head(5)","1c298766":"df.info()","486af1e8":"df.describe().T","4b7c4f7e":"df.isna().sum()","78639f37":"df.isnull().sum()","997a2fe8":"sns.countplot(x=df.dtypes ,data=df)\nplt.ylabel(\"number of data type\")\nplt.xlabel(\"data types\")","f0ad5a55":"corr_df = df.corr()\nplt.figure(figsize=(20,10))\nsns.heatmap(corr_df, cmap = 'coolwarm', linecolor = 'white', linewidth =1, annot = True)","34f62586":"corr_df[\"Outcome\"].sort_values(ascending = False)","f95713b9":"values = pd.Series(df[\"Outcome\"]).value_counts()\ntrace = go.Pie(values=values)\nply.iplot([trace])","c3e04508":"plt.figure(figsize = (20,10))\nsns.scatterplot(x = df['Age'], y = df['BMI'], palette=\"ch:r=-.2,d=.3_r\", hue = df[\"Outcome\"])","32742168":"fig, ax = plt.subplots(4,2, figsize=(16,16))\nsns.distplot(df[\"Age\"], ax = ax[0,0])\nsns.distplot(df[\"Pregnancies\"], ax = ax[0,1])\nsns.distplot(df[\"Glucose\"], ax = ax[1,0])\nsns.distplot(df[\"BMI\"], ax = ax[1,1])\nsns.distplot(df[\"BloodPressure\"], ax = ax[2,0])\nsns.distplot(df[\"SkinThickness\"], ax = ax[2,1])\nsns.distplot(df[\"Insulin\"], ax = ax[3,0])\nsns.distplot(df[\"DiabetesPedigreeFunction\"], ax = ax[3,1])","6d7cd8d8":"sns.pairplot(df, hue='Outcome')","a0e52ca5":"df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)\n\ndf['Glucose'].fillna(df['Glucose'].mean(), inplace = True)\ndf['BloodPressure'].fillna(df['BloodPressure'].mean(), inplace = True)\ndf['SkinThickness'].fillna(df['SkinThickness'].mean(), inplace = True)\ndf['Insulin'].fillna(df['Insulin'].mean(), inplace = True)\ndf['BMI'].fillna(df['BMI'].mean(), inplace = True)","b20e07c8":"fig, ax = plt.subplots(4,2, figsize=(16,16))\nsns.distplot(df[\"Age\"], ax = ax[0,0])\nsns.distplot(df[\"Pregnancies\"], ax = ax[0,1])\nsns.distplot(df[\"Glucose\"], ax = ax[1,0])\nsns.distplot(df[\"BMI\"], ax = ax[1,1])\nsns.distplot(df[\"BloodPressure\"], ax = ax[2,0])\nsns.distplot(df[\"SkinThickness\"], ax = ax[2,1])\nsns.distplot(df[\"Insulin\"], ax = ax[3,0])\nsns.distplot(df[\"DiabetesPedigreeFunction\"], ax = ax[3,1])","380e11a6":"ss = StandardScaler()\nX = ss.fit_transform(df)","4a2db3bc":"X =  pd.DataFrame(ss.fit_transform(df.drop([\"Outcome\"],axis = 1),),\n        columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age'])","7db63f70":"X.head(5)","1ee9a9e1":"y = df['Outcome']","aacfe367":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.4,random_state=42, stratify=y)","25347ed7":"log_reg = LogisticRegression(random_state=42)","4d5ebb18":"log_reg.fit(X_train, y_train)","2bcdea5a":"logp = log_reg.predict(X_test)","7ab7a5a8":"y_train_pred_log = cross_val_predict(log_reg, X_train, y_train, cv=3)","851a520b":"confusion_matrix(y_train,y_train_pred_log)","ad5bee01":"print('Precision Score {}'.format(round(precision_score(y_test,logp),3)))\nprint('Recall Score {}'.format(round(recall_score(y_test,logp),3)))\nprint(\"ROC AUC {}\".format(round(roc_auc_score(y_test,logp),3)))","8078fa1f":"gbrt = GradientBoostingClassifier(random_state=42)","601fc6fc":"gbrt.fit(X_train, y_train)","089706be":"gbrtp = gbrt.predict(X_test)","7f4d3133":"y_gbrt = cross_val_predict(gbrt, X_train, y_train, cv=3)","3557a157":"confusion_matrix(y_train,y_gbrt)","9659b9d5":"print('Precision Score {}'.format(round(precision_score(y_test,gbrtp),3)))\nprint('Recall Score {}'.format(round(recall_score(y_test,gbrtp),3)))\nprint(\"ROC AUC {}\".format(round(roc_auc_score(y_test,gbrtp),3)))","f2582912":"forest_clf = RandomForestClassifier(random_state=42)","3d3db7ec":"forest_clf.fit(X_train,y_train)","24ef5edc":"ranp = forest_clf.predict(X_test)","dc712f4c":"y_train_pred_ran = cross_val_predict(forest_clf, X_train, y_train, cv=3)","51206472":"confusion_matrix(y_train,y_train_pred_ran)","97dac53c":"print('Precision Score {}'.format(round(precision_score(y_test,ranp),3)))\nprint('Recall Score {}'.format(round(recall_score(y_test,ranp),3)))\nprint(\"ROC AUC {}\".format(round(roc_auc_score(y_test,ranp),3)))","11f323fe":"param_grid = [{'n_estimators':np.arange(1,50)}]","d0f29880":"forest_reg = RandomForestClassifier(random_state=42)","57fc5dcf":"grid_search = GridSearchCV(forest_reg,param_grid,cv=5)","3bd19409":"grid_search.fit(X_train,y_train)","befc1b69":"grid_search.best_estimator_","c5a4686e":"print(\"Best Score {}\".format(str(grid_search.best_score_)))\nprint(\"Best Parameters {}\".format(str(grid_search.best_params_)))","18757108":"forest_g = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=46, n_jobs=None,\n            oob_score=False, random_state=42, verbose=0, warm_start=False)","533b2e94":"forest_g.fit(X_train,y_train)","be4f0b06":"rang = forest_g.predict(X_test)","72007e07":"y_train_pred_rang = cross_val_predict(forest_g, X_train, y_train, cv=3)","1e8565d1":"confusion_matrix(y_train,y_train_pred_rang)","32e84634":"print('Precision Score {}'.format(round(precision_score(y_test,rang),3)))\nprint('Recall Score {}'.format(round(recall_score(y_test,rang),3)))\nprint(\"ROC AUC {}\".format(round(roc_auc_score(y_test,rang),3)))","a7ef725d":"voting_clf = VotingClassifier(estimators=[('lr', log_reg),('rf', forest_g)], voting='hard')","759e5804":"voting_clf.fit(X_train,y_train)","71cd60f1":"votinglr= voting_clf.predict(X_test)","34e95c61":"y_train_pred_vt = cross_val_predict(voting_clf, X_train, y_train, cv=3)","3b509bb3":"confusion_matrix(y_train,y_train_pred_vt)","1180fff2":"print('Precision Score {}'.format(round(precision_score(y_test,votinglr),3)))\nprint('Recall Score {}'.format(round(recall_score(y_test,votinglr),3)))","48380d19":"voting_gb = VotingClassifier(estimators=[('lr', log_reg),('gb', gbrt)], voting='hard')\nvoting_gb.fit(X_train,y_train)\nvotinggb= voting_clf.predict(X_test)\ny_train_pred_vt = cross_val_predict(voting_gb, X_train, y_train, cv=3)","cbb6d6a0":"confusion_matrix(y_train,y_train_pred_vt)","70d51c09":"voting_rg = VotingClassifier(estimators=[('rf', forest_g),('gb', gbrt)], voting='hard')\nvoting_rg.fit(X_train,y_train)\nvotingrg= voting_rg.predict(X_test)\ny_train_pred_vt = cross_val_predict(voting_rg, X_train, y_train, cv=3)","3259eed5":"confusion_matrix(y_train,y_train_pred_vt)","d9967c84":"voting_lgr = VotingClassifier(estimators=[('lr', log_reg),('gb', gbrt),('rf',forest_g)], voting='hard')\nvoting_lgr.fit(X_train,y_train)\nvotinglgr= voting_lgr.predict(X_test)\ny_train_pred_vt = cross_val_predict(voting_lgr, X_train, y_train, cv=3)","2a69f392":"confusion_matrix(y_train,y_train_pred_vt)","400bb328":"for clf in (voting_clf,voting_gb,voting_rg,voting_lgr):\n    clf.fit(X_train,y_train)\n    y_pred = clf.predict(X_test)\n    print(round(roc_auc_score(y_test,y_pred),3))","0bd1bf71":"<h4>Logistic and Gradient Boosting<\/h4>","8c726b1f":"Recall\/Sensitivity - Positive correctly classified as Positive.\n\nRecall = TP\/(TP+FN) where TP = True Positive and FP = False Negative","0541b236":"<h3>Random Classifier<\/h3>","4e3285ba":"<h3>Creating train set and test set<\/h3>","3a4632c0":"Pair Plot can be used to visualize and understand one variable w.r.t. to another. The diagonal is a histogram which shows distribution of a single variable.\n\nHere hue is taken as Outcome, which clearly distincts the Outcome 0 and Outcome 1 in our visualization.","24ecec89":"Our data does not have any null or nan value. We can continue with our Exploratory Data Analysis","07275b3a":"![](http:\/\/i.imgur.com\/uducbAo.jpg)","01b2e306":"The Voting Classifier with highest score is of the combination of Logistic Regression and Gradient Boosting. \n\nPlease share your **feedback** and do upvote.","5f6f36c8":"As you can see below the number of outcome 0 is almost as twice as much as the number of outcome 1. This is a key point in our dataset and later it will be used when we split our dataset. Here outcome 1 that diabetes was found in our patient whereas in the case of outcome 0 diabetes was absent","2258f5ca":"<h4>Logistic and Random Forest Classification<\/h4>","69199091":"Precision - Accuracy of positive prediction\n\nP = TP\/(TP+FP) where TP = True Positive and FP = False Positive","895f2310":"<h4>Random Forest and Gradient Boosting<\/h4>","3b9ac416":"<h3>Voting classifier<\/h3>","55c58c88":"All our variables are positively correlated with different degree of correlation. Glucose has the highest correlation with Outcome and BloodPressure has the lowest","04aabc6e":"As previously stated, there are a number of variables which have 0 values. The steps I have taken to clean up this problem:\n1. Replaced the 0 with Nan. The reason behind this is that when the mean of the variable is calculated, the data field with 0 value will not be considered. This will give the true mean of the valid data entry.\n2. In the next step, we replace the value of NaN with our calculated mean.","5ca68469":"The objective of our notebook is to predict if the patient has diabetes or not. Our prediction will be based on certain diagnostic measurement provided in our dataset. A key point to note here is that this data is taken from a much larger dataset, and this dataset is based on the patient from Pima Indian heritage with at least 21 years old females. \n\nI have tried to explain the concept behind concept that I have used in our notebook, which might be helpful for beginners as well. Please **UPVOTE** if you find this notebook useful! ","f028a50a":"<h4>Logistic, Gradient Boosting, and Random Forest<\/h4>","2e8bac18":"<h1>Pima Indians Diabetes Database<\/h1>","de5baf82":"![](https:\/\/i.imgur.com\/tXvb3kL.jpg)","07447e38":"<h3>Gradient Boosting<\/h3>","f227403a":"<h2>Exploratory Data Analysis<\/h2>","285e865b":"Body Mass Index is a calculation which takes into two components, height and weight. Patient between the age of 20-30 have the highest number with BMI over 50. High BMI patients tends to have high chances of Outcome 1. ","bf806e54":"<h2>Modeling Our Data<\/h2>","1b9d1131":"<h3>Hyperparameter Optimization<\/h3>","1fbdeb71":"Below is an examination of correlation among the independent variables, and between the dependent and the independent variables. The Correlation Coefficient value can range from -1 to 1. If the correlation between two variables is -1, they are both highly negatively correlated, whereas if the correlation between two variables is +1, they both are highly positively correlated. A correlation coefficient of 0 indicated that there is no correlation between the two variables.","76d1f777":"Remember at the beginning of this notebook I pointed out that our data consist of almost twice as much outcome 0 as compared to outcome 1. This is the right time to speak about **Stratification**. \n\nThe general procedure is to randomly split our dataset in predefined proportion. In such scenario, there can be a posibility that our train set or test set may be dominated by Outcome 0 which can give incorrect results. Thus we use the stratification method which divides our dataset homogeneous subgroups called strata, which gives us proportionate set of data for both our train and test set.","4fe9deeb":"<h3>Logistic Regression<\/h3>","0eac08c0":"<h2>Structure of this notebook<\/h2>\n\n1. Loading and understanding the data\n2. Exploratory Data Analysis\n3. Cleaning the data and feature scaling\n4. Modeling the data\n5. Fine tuning and interpreting our result\n\nSince our dataset is based on the sector of healthcare, we will use precision and recall with AUC as our performance measurement","022c1112":"Distribution Plot after taking the mean.","636087a5":"We can also visualization the types of data we have in our dataset and count of each.","4cc53778":"Distribution Plot are great visualization through which we can understand the distribution of each variable in our dataset. As we can see from our distribution plots, Glucose, BloodPressure, SkinThickness, and Insulin all have 0. The minimum of value of such can not be zero. This part will be dealt in our data cleaning process.","970ebf5a":"<h3>Confusion Matrix<\/h3>\n\nConfusion Matrix is represented by rows and columns. Each row in the confusion matrix is called the actual class, and each column in the confusion matrix is called the predicted column. \nThe first row is called the negative class where in the case Outcome 0 is considered. The second row is called the positive class where Outcome 1 is considered. \nThe negative which are correctly classified are called *true negative*, whereas the negative wrongly classified as 1 are called *false positive*.\nThe positive wrongly classified as 0 are called *false negative*, whereas the positive correctly classified as 1 are called *true positive*.","93030a22":"<h2>Feature Scaling and Data Cleaning<\/h2>"}}