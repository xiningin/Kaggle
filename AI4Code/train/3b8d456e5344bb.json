{"cell_type":{"9da7a924":"code","b48440f7":"code","7bd38561":"code","cc0f9391":"code","ebdebc3c":"code","22873008":"code","aee4ff45":"code","2023bd4f":"code","b680dc37":"code","fc35040b":"code","68c5271e":"code","9624114e":"code","4ba9a7c6":"code","fed98ede":"code","8c22f826":"code","b9d3538e":"code","71f6b40a":"code","d8bdaa81":"code","5ac07d01":"code","184265ab":"code","a0cfb2a1":"code","e279c864":"code","72e0de7b":"code","7983a0a9":"code","57ad65b9":"code","5ad9a2b6":"code","3b69bc0e":"code","ae704c26":"code","4870458f":"code","d24e4c06":"code","a061bd79":"code","9a4b72bd":"code","59c9c435":"code","96877e9f":"code","12469e26":"code","6c01e35b":"code","4a395f76":"code","08923086":"code","ec9f0665":"code","276e210a":"code","0fd14555":"code","a6e18180":"code","ad9d9878":"code","fe97c52f":"code","9a685381":"code","ae8703f7":"code","14cc1a23":"code","223da7e4":"code","e7f09b12":"code","1210a996":"code","ed7c33c4":"code","0b2693da":"code","17fad3fd":"code","12ae4379":"code","b8208b10":"code","ea1b425d":"code","e26d7eb6":"code","fae73d19":"code","f13667fd":"code","6f788de2":"code","4e325e6e":"code","c50958d6":"code","6059a8f3":"code","c69a67db":"code","4f7a99e0":"code","99a82792":"code","964dfdb4":"code","0920de9e":"code","c2b16ea5":"code","e403c1ad":"code","487e7b21":"code","2cad50ea":"code","c1d13c9f":"code","ba96a624":"code","01c4eab1":"code","6ee63fb7":"code","a1962c86":"code","46ff03ab":"code","cf13e052":"code","3c0969d3":"code","db74c61f":"code","43115940":"code","b73a7652":"code","040488e2":"code","c1fb4644":"code","158ddaba":"code","cdbe301b":"markdown","875176ef":"markdown","5dc962d8":"markdown","e273132c":"markdown","6a77353d":"markdown","491ee5e7":"markdown","402128e4":"markdown","dd33e006":"markdown","0dc486ad":"markdown","de675925":"markdown","8a6de524":"markdown","5b4a48b4":"markdown","537b30b5":"markdown","c3cc981b":"markdown","4bc5b895":"markdown","97140cc6":"markdown"},"source":{"9da7a924":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b48440f7":"import warnings\nwarnings.filterwarnings('ignore')","7bd38561":"pd.set_option('display.max_columns',500)\npd.set_option('display.width',500)\npd.set_option('display.max_rows', 500)","cc0f9391":"from sklearn.impute import SimpleImputer\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport category_encoders as ce\n\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.model_selection import RandomizedSearchCV\n# from sklearn.feature_selection import RFECV\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n# from xgboost import XGBClassifier\n\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import classification_report","ebdebc3c":"df_train_data = pd.read_csv('\/kaggle\/input\/analytics-vidhya-job-a-thon-may-2021\/train_s3TEQDk.csv')\ndf_train_data.head()","22873008":"df_test_data = pd.read_csv('\/kaggle\/input\/analytics-vidhya-job-a-thon-may-2021\/test_mSzZ8RL.csv')\ndf_test_data.head()","aee4ff45":"train_data = df_train_data.copy()\ntest_data = df_test_data.copy()","2023bd4f":"train_data.shape, test_data.shape","b680dc37":"100*len(test_data)\/len(train_data)","fc35040b":"def details(df):\n    sum_null_values = df.isnull().sum()\n    percent_null_values = 100* (sum_null_values\/len(df))\n    data_type = df.dtypes\n    unique_values = df.nunique()\n\n    table = pd.concat([sum_null_values,percent_null_values,data_type,unique_values], axis=1)\n    table_col = table.rename(columns = {0 : 'Missing Values', 1 : '% of Total Missing Values', 2 : 'Data_Type', 3: 'Unique values'})\n    return table_col","68c5271e":"details(train_data)","9624114e":"details(test_data)","4ba9a7c6":"cols_obj = train_data.select_dtypes('object').columns\ncols_obj","fed98ede":"cols_num = train_data.select_dtypes('number').columns\ncols_num","8c22f826":"train_data.Credit_Product.value_counts(dropna=False)","b9d3538e":"train_data.Is_Active.value_counts().plot.bar()\nplt.show()","71f6b40a":"train_data.groupby('Is_Active')['Credit_Product'].count().plot.bar()\nplt.show()","d8bdaa81":"# mode_imputation = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n# train_data['Credit_Product'] = mode_imputation.fit_transform(train_data[['Credit_Product']]).ravel()\n# test_data['Credit_Product'] = mode_imputation.transform(test_data[['Credit_Product']]).ravel()","5ac07d01":"train_data['Credit_Product']= train_data['Credit_Product'].replace(np.nan, \"Others\")\ntest_data['Credit_Product']= test_data['Credit_Product'].replace(np.nan, \"Others\")","184265ab":"train_data.Credit_Product.value_counts().plot.bar()\nplt.show()","a0cfb2a1":"train_data.Gender.value_counts().plot.bar()\nplt.show()","e279c864":"train_data.Region_Code.value_counts().plot.bar()\nplt.show()","72e0de7b":"train_data.Occupation.value_counts().plot.bar()\nplt.show()","7983a0a9":"train_data.Channel_Code.value_counts().plot.bar()\nplt.show()","57ad65b9":"train_data.describe(percentiles=(0.01,0.05,0.1,0.25,0.5,0.75,0.9,0.95,0.99)).apply(lambda s: s.apply('{0:.5f}'.format))","5ad9a2b6":"train_data.Is_Lead.value_counts()","3b69bc0e":"round(100*train_data['Is_Lead'].value_counts(normalize=True),2).plot(kind='pie', figsize=(6, 6), autopct='%1.2f%%')\nplt.title(\"Lead and Non-Lead Distribution\")\nplt.legend([\"Non-Lead\", \"Lead\"])\nplt.show()","ae704c26":"# %%time\n# row,col,c = 14,3,1\n# fig = plt.figure(figsize=(30,80), dpi= 200)\n\n# for i in list(cols_obj):\n#     plt.subplot(row,col,c)\n#     plt.title(f'{i},subplot:{row}{col}{c}')\n#     plt.xlabel(i)\n#     train_data[i].value_counts().plot.bar()\n#     c = c + 1\n# plt.tight_layout()\n# plt.show()","4870458f":"sns.barplot(data=train_data, x='Is_Lead', y='Avg_Account_Balance')\nplt.show()","d24e4c06":"sns.barplot(data=train_data, y='Is_Lead', x='Gender')\nplt.show()","a061bd79":"fig, axes = plt.subplots(nrows=3, ncols = 2, figsize=(50,30))\n\nsns.countplot(x=\"Gender\", hue='Is_Lead', data=train_data, ax=axes[0][0])\naxes[0][0].set_title('Gender')\nplt.xticks(rotation=45)\n\nsns.countplot(x=\"Region_Code\", hue='Is_Lead', data=train_data, ax=axes[0][1])\naxes[0][1].set_title('Region_Code')\nplt.xticks(rotation=45)\n\nsns.countplot(x=\"Occupation\", hue='Is_Lead', data=train_data, ax=axes[1][0])\naxes[1][0].set_title('Occupation')\nplt.xticks(rotation=45)\n\nsns.countplot(x=\"Channel_Code\", hue='Is_Lead', data=train_data, ax=axes[1][1])\naxes[1][1].set_title('Channel_Code')\nplt.xticks(rotation=45)\n\nsns.countplot(x=\"Credit_Product\", hue='Is_Lead', data=train_data, ax=axes[2][0])\naxes[2][0].set_title('Credit_Product')\nplt.xticks(rotation=45)\n\nsns.countplot(x=\"Is_Active\", hue='Is_Lead', data=train_data, ax=axes[2][1])\naxes[2][1].set_title('Is_Active')\nplt.xticks(rotation=45)\n\nplt.show()","9a4b72bd":"#This can be kept before new df creation since its common\ntrain_data.Gender = train_data.Gender.map({'Male':1,'Female':0})\ntrain_data.Credit_Product = train_data.Credit_Product.map({'Yes':1,'No':0,'Others':3})\ntrain_data.Is_Active = train_data.Is_Active.map({'Yes':1,'No':0})","59c9c435":"test_data.Gender = test_data.Gender.map({'Male':1,'Female':0})\ntest_data.Credit_Product = test_data.Credit_Product.map({'Yes':1,'No':0,'Others':3})\ntest_data.Is_Active = test_data.Is_Active.map({'Yes':1,'No':0})","96877e9f":"#OHE\n#This can be kept before new df creation since its common\nohe = OneHotEncoder(sparse=False)\nohe_df1 = pd.DataFrame(ohe.fit_transform(train_data[['Region_Code','Occupation','Channel_Code']]),columns=ohe.get_feature_names())\nohe_df2 = pd.DataFrame(ohe.transform(test_data[['Region_Code','Occupation','Channel_Code']]),columns=ohe.get_feature_names())","12469e26":"train_data = pd.concat([train_data, ohe_df1],1)\ntest_data = pd.concat([test_data, ohe_df2],1)","6c01e35b":"train_data.head()","4a395f76":"train_data.drop(['Region_Code','Occupation','Channel_Code'],1,inplace=True)\ntest_data.drop(['Region_Code','Occupation','Channel_Code'],1,inplace=True)","08923086":"df_train = train_data.copy()\ndf_test = test_data.copy()","ec9f0665":"df_train['Age'] = pd.cut(df_train['Age'], [0,25,50,75,100], labels=['<25','25-50','50-75','>75'])\ndf_train['Vintage'] = pd.cut(df_train['Vintage'], [0,15,30,45,60,75,90,105,120,200], labels=['<15','15-30','30-45','45-60','60-75','75-90','90-105','105-120','>120'])\n\ndf_test['Age'] = pd.cut(df_test['Age'], [0,25,50,75,100], labels=['<25','25-50','50-75','>75'])\ndf_test['Vintage'] = pd.cut(df_test['Vintage'], [0,15,30,45,60,75,90,105,120,200], labels=['<15','15-30','30-45','45-60','60-75','75-90','90-105','105-120','>120'])","276e210a":"df_train['Age'].value_counts()","0fd14555":"df_train['Vintage'].value_counts()","a6e18180":"plt.figure(figsize=(16,8))\nplt.subplot(1,2,1)\nsns.countplot(y=\"Age\", data=df_train)\nplt.title('Age')\nplt.subplot(1,2,2)\nsns.countplot(y=\"Vintage\", data=df_train)\nplt.title('Vintage')","ad9d9878":"ohe_df3 = pd.DataFrame(ohe.fit_transform(df_train[['Age','Vintage']]),columns=ohe.get_feature_names())\nohe_df4 = pd.DataFrame(ohe.transform(df_test[['Age','Vintage']]),columns=ohe.get_feature_names())","fe97c52f":"df_train = pd.concat([df_train, ohe_df3],1)\ndf_test = pd.concat([df_test, ohe_df4],1)","9a685381":"df_train.drop(['Age','Vintage'],1,inplace=True)\ndf_test.drop(['Age','Vintage'],1,inplace=True)","ae8703f7":"df_train.head()","14cc1a23":"X = df_train.drop(['ID','Is_Lead'], 1)\ny = df_train['Is_Lead']\nX_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.7, random_state=23)","223da7e4":"df_train.shape","e7f09b12":"cols_obj = df_train.select_dtypes('object').columns\ncols_obj","1210a996":"cols_num = df_train.select_dtypes('number').columns\ncols_num","ed7c33c4":"plt.figure(figsize=(15,8))\n# for i in enumerate(cols):\n#     plt.subplot(1,3,i[0]+1)\nsns.distplot(X_train['Avg_Account_Balance'])\nplt.show()","0b2693da":"pt = PowerTransformer(copy=False)\nX_train[['Avg_Account_Balance']] = pt.fit_transform(X_train[['Avg_Account_Balance']])\nX_val[['Avg_Account_Balance']] = pt.transform(X_val[['Avg_Account_Balance']])\ndf_test[['Avg_Account_Balance']] = pt.transform(df_test[['Avg_Account_Balance']])","17fad3fd":"plt.figure(figsize=(15,8))\n# for i in enumerate(cols):\n#     plt.subplot(1,3,i[0]+1)\nsns.distplot(X_train['Avg_Account_Balance'])\nplt.show()","12ae4379":"X_train.head()","b8208b10":"X_train.shape, X_val.shape, y_train.shape, y_val.shape","ea1b425d":"# Models\nmodel_list = list()\n#AUC curve\nAUCROC_train = list()\nAUCROC_val = list()","e26d7eb6":"model_LR = LogisticRegression()","fae73d19":"def model_fit(model, X_train, y_train, X_val, y_val, algo=None):\n    \n    model_LR.fit(X_train, y_train)\n    \n    y_train_prob = model_LR.predict_proba(X_train)\n    y_train_pred = model_LR.predict(X_train)\n    y_val_prob = model_LR.predict_proba(X_val)\n    y_val_pred = model_LR.predict(X_val)\n    \n    matrix_train = confusion_matrix(y_train, y_train_pred)\n    matrix_val = confusion_matrix(y_val, y_val_pred)\n    report_train = classification_report(y_train, y_train_pred)\n    report_val = classification_report(y_val, y_val_pred)\n    auc_train = roc_auc_score(y_train, y_train_prob[:,1])\n    auc_val = roc_auc_score(y_val, y_val_prob[:,1])\n    \n    print('Confusion Matrix for train')\n    print('='*60)\n    print(matrix_train,\"\\n\")\n    print('Confusion Matrix for val')\n    print('='*60)\n    print(matrix_val,\"\\n\")\n    print('Classification Report for train')\n    print('='*60)\n    print(report_train,\"\\n\")\n    print('Classification Report for val')\n    print('='*60)\n    print(report_val,\"\\n\")\n    print('AUC-ROC for train')\n    print('='*60)\n    print(auc_train,'\\n')\n    print('AUC-ROC for val')\n    print('='*60)\n    print(auc_val,'\\n')\n    print('Roc-Auc-Curve for Train set')\n    print('='*60)\n    print(plot_roc_curve(model_LR, X_train, y_train),'\\n')\n    print('Roc-Auc-Curve for Val set')\n    print('='*60)\n    print(plot_roc_curve(model_LR, X_val, y_val),'\\n')\n    \n    model_list.append(algo)\n    AUCROC_train.append(auc_train)\n    AUCROC_val.append(auc_val)","f13667fd":"def model_fit_evaluation(model, params, X_train, y_train, X_val, y_val, algo=None):\n    \n    rcv = RandomizedSearchCV(model, params, cv=5, scoring='roc_auc', n_jobs=-1, verbose=1, random_state=23)\n    rcv.fit(X_train, y_train)\n    \n    rcv_best = rcv.best_estimator_\n    print('\\n')\n    print('best estimator : ', rcv_best)\n    print('best parameters: ', rcv.best_params_)\n    print('best score: ', rcv.best_score_)\n    print('\\n')\n\n    y_train_prob = rcv_best.predict_proba(X_train)\n    y_train_pred = rcv_best.predict(X_train)\n    y_val_prob = rcv_best.predict_proba(X_val)\n    y_val_pred = rcv_best.predict(X_val)\n    \n    matrix_train = confusion_matrix(y_train, y_train_pred)\n    matrix_val = confusion_matrix(y_val, y_val_pred)\n    report_train = classification_report(y_train, y_train_pred)\n    report_val = classification_report(y_val, y_val_pred)\n    auc_train = roc_auc_score(y_train, y_train_prob[:,1])\n    auc_val = roc_auc_score(y_val, y_val_prob[:,1])\n    \n    print('Confusion Matrix for train')\n    print('='*60)\n    print(matrix_train,\"\\n\")\n    print('Confusion Matrix for val')\n    print('='*60)\n    print(matrix_val,\"\\n\")\n    print('Classification Report for train')\n    print('='*60)\n    print(report_train,\"\\n\")\n    print('Classification Report for val')\n    print('='*60)\n    print(report_val,\"\\n\")\n    print('AUC-ROC for train')\n    print('='*60)\n    print(auc_train,'\\n')\n    print('AUC-ROC for val')\n    print('='*60)\n    print(auc_val,'\\n')\n    print('Roc-Auc-Curve for Train set')\n    print('='*60)\n    print(plot_roc_curve(rcv_best, X_train, y_train),'\\n')\n    print('Roc-Auc-Curve for Val set')\n    print('='*60)\n    print(plot_roc_curve(rcv_best, X_val, y_val),'\\n')\n    \n    model_list.append(algo)\n    AUCROC_train.append(auc_train)\n    AUCROC_val.append(auc_val)","6f788de2":"model_fit(model_LR, X_train, y_train, X_val, y_val, algo='Logistic Regression with Hyperparameters')","4e325e6e":"params_LR = {'C':np.logspace(-1, 5, 10), 'class_weight':[None,'balanced'], 'penalty':['l1','l2']}","c50958d6":"model_fit_evaluation(model_LR, params_LR, X_train, y_train, X_val, y_val, algo='Logistic Regression with Hyperparameter tuning')","6059a8f3":"df_train_tree = train_data.copy()\ndf_test_tree = test_data.copy()","c69a67db":"X = df_train_tree.drop(['ID','Is_Lead'], 1)\ny = df_train_tree['Is_Lead']\nX_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.7, random_state=23)","4f7a99e0":"model_DT = DecisionTreeClassifier(random_state=23)\nparams_DT = {\n    'max_depth': [10, 20, 50, 100, 200],\n    'min_samples_leaf': [10, 20, 50, 100, 200],\n    'min_samples_split' : [10, 20, 50, 100, 200],\n    'criterion': [\"gini\", \"entropy\"]\n}","99a82792":"model_fit(model_DT, X_train, y_train, X_val, y_val, algo='Decision Tree without Hyperparameters')","964dfdb4":"%%time\nmodel_fit_evaluation(model_DT, params_DT, X_train, y_train, X_val, y_val, algo='Decision Tree with Hyperparameter tuning')","0920de9e":"model_RF = RandomForestClassifier(oob_score = True, random_state=23)\nparams_RF = {\n    'n_estimators': [10, 20, 50, 100, 200],\n    'max_depth': [10, 20, 50, 100, 200],\n    'min_samples_leaf': [10, 20, 50, 100, 200],\n    'min_samples_split' : [10, 20, 50, 100, 200],\n    'criterion': [\"gini\", \"entropy\"]\n}","c2b16ea5":"model_fit(model_RF, X_train, y_train, X_val, y_val, algo='Random Forest without Hyperparameters')","e403c1ad":"%%time\nmodel_fit_evaluation(model_RF, params_RF, X_train, y_train, X_val, y_val, algo='Random Forest with Hyperparameter tuning')","487e7b21":"# model_XGB = XGBClassifier(random_state=23)\n# params_XGB = {\n#     'n_estimators': [5, 10, 20, 50, 100, 200],\n#     'max_depth': [5, 10, 20, 50, 100, 200],\n#     'sampling_method': ['uniform','gradient_based'],\n#     'subsample': [0.2, 0.4, 0.5, 0.6, 0.8, 1],\n#     'learning_rate': [0.01,0.05,0.1,0.2,0.3,0.5,1]\n# }","2cad50ea":"# model_fit(model_XGB, X_train, y_train, X_val, y_val, algo='XGB without Hyperparameters')","c1d13c9f":"#Not running due to lack of system properties\n# model_fit_evaluation(model_XGB, params_XGB, X_train, y_train, X_val, y_val, algo='XGB with Hyperparameter tuning')","ba96a624":"eval_df = pd.DataFrame({'model': model_list, 'train_AUC': AUCROC_train, 'val_AUC': AUCROC_val})\neval_df","01c4eab1":"%%time\nrcv = RandomizedSearchCV(model_RF, params_RF, cv=5, scoring='roc_auc', n_jobs=-1, verbose=1, random_state=23)\nrcv.fit(X, y)\n\nrcv_best = rcv.best_estimator_\nprint('\\n')\nprint('best estimator : ', rcv_best)\nprint('best parameters: ', rcv.best_params_)\nprint('best score: ', rcv.best_score_)\nprint('\\n')\n\ny_train_prob = rcv_best.predict_proba(X)\ny_train_pred = rcv_best.predict(X)\n\nmatrix_train = confusion_matrix(y, y_train_pred)\nreport_train = classification_report(y, y_train_pred)\nauc_train = roc_auc_score(y, y_train_prob[:,1])\n\nprint('Confusion Matrix for train')\nprint('='*60)\nprint(matrix_train,\"\\n\")\nprint('Classification Report for train')\nprint('='*60)\nprint(report_train,\"\\n\")\nprint('AUC-ROC for train')\nprint('='*60)\nprint(auc_train,'\\n')\nprint('Roc-Auc-Curve for Train set')\nprint('='*60)\nprint(plot_roc_curve(rcv_best, X, y),'\\n')","6ee63fb7":"rcv_best.feature_importances_\nfinal_df = pd.DataFrame({'Varname': X.columns, 'feature_imp':rcv_best.feature_importances_})\nfinal_df.sort_values(by='feature_imp', ascending=False)","a1962c86":"#Final prediction\npredictions = rcv_best.predict(df_test_tree.drop('ID', axis=1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\nsubmission = pd.DataFrame({ 'ID' : df_test_tree['ID'], 'Is_Lead': predictions })\nsubmission.shape","46ff03ab":"submission.head(10)","cf13e052":"test_data.head()","3c0969d3":"test_final = pd.merge(left=test_data, right=submission, left_on='ID', right_on='ID')\ntest_final.head()","db74c61f":"test_final_original = pd.merge(left=df_test_data, right=submission, left_on='ID', right_on='ID')\ntest_final_original.head()","43115940":"test_final['Age_binned'] = pd.cut(test_final['Age'], [0,25,50,75,100], labels=['<25','25-50','50-75','>75'])\ntest_final['Vintage_binned'] = pd.cut(test_final['Vintage'], [0,15,30,45,60,75,90,105,120,200], labels=['<15','15-30','30-45','45-60','60-75','75-90','90-105','105-120','>120'])\ntest_final.drop(['Age','Vintage'],1,inplace=True)\ntest_final.head()","b73a7652":"plt.figure(figsize=(16,8))\nplt.subplot(1,2,1)\nsns.countplot(y=\"Age_binned\", data=test_final)\nplt.title('Age')\nplt.subplot(1,2,2)\nsns.countplot(y=\"Vintage_binned\", data=test_final)\nplt.title('Vintage')","040488e2":"fig, axes = plt.subplots(nrows=3, ncols = 2, figsize=(50,30))\n\nsns.countplot(x=\"Gender\", hue='Is_Lead', data=test_final_original, ax=axes[0][0])\naxes[0][0].set_title('Gender')\nplt.xticks(rotation=45)\n\nsns.countplot(x=\"Region_Code\", hue='Is_Lead', data=test_final_original, ax=axes[0][1])\naxes[0][1].set_title('Region_Code')\nplt.xticks(rotation=45)\n\nsns.countplot(x=\"Occupation\", hue='Is_Lead', data=test_final_original, ax=axes[1][0])\naxes[1][0].set_title('Occupation')\nplt.xticks(rotation=45)\n\nsns.countplot(x=\"Channel_Code\", hue='Is_Lead', data=test_final_original, ax=axes[1][1])\naxes[1][1].set_title('Channel_Code')\nplt.xticks(rotation=45)\n\nsns.countplot(x=\"Credit_Product\", hue='Is_Lead', data=test_final_original, ax=axes[2][0])\naxes[2][0].set_title('Credit_Product')\nplt.xticks(rotation=45)\n\nsns.countplot(x=\"Is_Active\", hue='Is_Lead', data=test_final_original, ax=axes[2][1])\naxes[2][1].set_title('Is_Active')\nplt.xticks(rotation=45)\n\nplt.show()","c1fb4644":"sns.countplot(x=\"Age_binned\", hue='Is_Lead', data=test_final)\nplt.show()","158ddaba":"sns.countplot(x=\"Vintage_binned\", hue='Is_Lead', data=test_final)\nplt.show()","cdbe301b":"Distplot","875176ef":"Data Imbalance Check","5dc962d8":"Since we cannot delete rows with NaN's since the submission file shows same shape as test file, we need to impute this categorical column with mode, which is 'No'","e273132c":"ML","6a77353d":"Power transformer","491ee5e7":"Evaluation","402128e4":"Age and Vintage to be made bins","dd33e006":"Running the entire train dataset to predict on test data set","0dc486ad":"Predicting on test set","de675925":"Decision tree and Random Forest","8a6de524":"Binary Encoding","5b4a48b4":"Data Preparation for Machine Learning","537b30b5":"Feature importance","c3cc981b":"Keeping copy of dataset","4bc5b895":"Instead of imputing with mode value we can create a separate category","97140cc6":"Visualizations"}}