{"cell_type":{"bfe26996":"code","a892cf4e":"code","1e448acc":"code","0cfa7059":"code","d6e7eae3":"code","d4da0ea9":"code","75a8ecca":"code","a009b5a2":"code","13c40ed9":"code","54c94219":"code","4b48b3ba":"code","925634e4":"code","bce49e33":"code","00fedcf9":"code","a11af8af":"code","b189d529":"code","e061af0e":"code","573652ab":"code","ef7e3749":"code","5d4c5799":"code","1ca4a826":"code","73e99040":"code","0a3120ee":"code","b8d6fc17":"code","6d7c6df1":"code","19d0e8fd":"code","eb24d49f":"code","83677694":"code","59c90317":"code","47e9d665":"code","6bfeb2ca":"code","0c2884c2":"code","55963d77":"code","301f12b1":"code","ec59a6fe":"code","84dbfa19":"code","192d71ae":"code","2a244294":"code","4d28327f":"code","6a7e60a7":"code","2cae50ed":"code","e6a84585":"code","4918fb96":"code","f41a6b20":"code","71ae4b7b":"code","4f582148":"code","bf4d5a0d":"code","61f5b8fa":"code","5ac88735":"code","6b879807":"code","5860cd9d":"code","65f0fcb7":"code","07ee302d":"code","2a9ac8c1":"code","9712a5d0":"code","38dec05c":"markdown","bfbe26f3":"markdown","151bab59":"markdown","5213e04f":"markdown","cf35eff6":"markdown","9c0b8571":"markdown","c4f17f5d":"markdown","9248a720":"markdown","a0239fa8":"markdown","d7f2f404":"markdown","ef3a299c":"markdown","c445e7c1":"markdown","53db914a":"markdown","15f0956e":"markdown","37cd6078":"markdown","7719a811":"markdown","f88da7cd":"markdown","38de69e1":"markdown","17b86e7e":"markdown","058be0ed":"markdown","7c777fba":"markdown","b77b9ec2":"markdown","cb439abc":"markdown","a7564b81":"markdown","0a070d22":"markdown","b5654513":"markdown","4bc1a770":"markdown","a1db7615":"markdown","96ab7699":"markdown","d18df939":"markdown","95639541":"markdown","16b45e80":"markdown","1233cda4":"markdown","9c67361b":"markdown","10123d7f":"markdown","09085fdf":"markdown","af4c3b6a":"markdown","5e5c6fa9":"markdown","29cceb22":"markdown","e770afac":"markdown"},"source":{"bfe26996":"%%javascript\n$.getScript('https:\/\/kmahelona.github.io\/ipython_notebook_goodies\/ipython_notebook_toc.js')","a892cf4e":"# Import the basic python libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt \n\nget_ipython().run_line_magic('matplotlib', 'inline')\nsns.set(style='white', context='notebook', palette='deep')\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Read the datasets\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\nIDtest = test[\"PassengerId\"]\ntrain.info()\ntest.info()","1e448acc":"train.info() # We have 891 observations & 12 columns. See the mix of variable types.","0cfa7059":"test.info() # We have 417 observations & 11 columns (no response 'Survived' column).","d6e7eae3":"# Check missing values in train data set\ntrain_na = (train.isnull().sum() \/ len(train)) * 100\ntrain_na = train_na.drop(train_na[train_na == 0].index).sort_values(ascending=False)[:30]\nmiss_train = pd.DataFrame({'Train Missing Ratio' :train_na})\nmiss_train.head()\n\n# Check missing values in train data set\ntest_na = (test.isnull().sum() \/ len(test)) * 100\ntest_na = test_na.drop(test_na[test_na == 0].index).sort_values(ascending=False)[:30]\nmiss_test = pd.DataFrame({'Test Missing Ratio' :test_na})\nmiss_test.head()\n\n# Fill empty and NaNs values with NaN\ntrain = train.fillna(np.nan)\ntest = test.fillna(np.nan)","d4da0ea9":"# Analyze the count of survivors by Pclass\nax = sns.countplot(x=\"Pclass\", hue=\"Survived\", data=train)\ntrain[['Pclass', 'Survived']].groupby(['Pclass']).count().sort_values(by='Survived', ascending=False)","75a8ecca":"# Analyze the Survival Probability by Pclass\ng = sns.barplot(x=\"Pclass\",y=\"Survived\",data=train)\ng = g.set_ylabel(\"Survival Probability\")\ntrain[['Pclass', 'Survived']].groupby(['Pclass']).mean().sort_values(by='Survived', ascending=False)","a009b5a2":"# Count the number of passengers by gender\nax = sns.countplot(x=\"Sex\", hue=\"Survived\", data=train)\n# Analyze survival count by gender\ntrain[[\"Sex\", \"Survived\"]].groupby(['Sex']).count().sort_values(by='Survived', ascending=False)","13c40ed9":"# Analyze the Survival Probability by Gender\ng = sns.barplot(x=\"Sex\",y=\"Survived\",data=train)\ng = g.set_ylabel(\"Survival Probability\")\ntrain[[\"Sex\", \"Survived\"]].groupby(['Sex']).mean().sort_values(by='Survived', ascending=False)","54c94219":"# Let's explore the distribution of age by response variable (Survived)\nfig = plt.figure(figsize=(10,8),)\naxis = sns.kdeplot(train.loc[(train['Survived'] == 1),'Age'] , color='g',shade=True, label='Survived')\naxis = sns.kdeplot(train.loc[(train['Survived'] == 0),'Age'] , color='b',shade=True,label='Did Not Survived')\nplt.title('Age Distribution - Surviver V.S. Non Survivors', fontsize = 20)\nplt.xlabel(\"Passenger Age\", fontsize = 12)\nplt.ylabel('Frequency', fontsize = 12);","4b48b3ba":"sns.lmplot('Age','Survived',data=train)\n\n# We can also say that the older the passenger the lesser the chance of survival","925634e4":"# Analyze the count of survivors by SibSP\n\nax = sns.countplot(x=\"SibSp\", hue=\"Survived\", data=train)\ntrain[['SibSp', 'Survived']].groupby(['SibSp']).count().sort_values(by='Survived', ascending=False)","bce49e33":"# Analyze probability of survival by SibSP\ng  = sns.factorplot(x=\"SibSp\",y=\"Survived\",data=train,kind=\"bar\", size = 7 ,palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")\ntrain[[\"SibSp\", \"Survived\"]].groupby(['SibSp']).mean().sort_values(by='Survived', ascending=False)","00fedcf9":"# Analyze the count of survivors by Parch\n\nax = sns.countplot(x=\"Parch\", hue=\"Survived\", data=train)\ntrain[['Parch', 'Survived']].groupby(['Parch']).count().sort_values(by='Survived', ascending=False)","a11af8af":"# Analyze the Survival Probability by Parch\ng  = sns.factorplot(x=\"Parch\",y=\"Survived\",data=train,kind=\"bar\", size = 7 ,palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"Survival Probability\")\ntrain[[\"Parch\", \"Survived\"]].groupby(['Parch']).mean().sort_values(by='Survived', ascending=False)","b189d529":"train['Ticket'].head()","e061af0e":"from scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\nsns.distplot(train['Fare'] , fit=norm);\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['Fare'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],loc='best')\nplt.ylabel('Frequency')\nplt.title('Fare distribution')","573652ab":"# Let's check the unique values\ntrain['Cabin'].unique()","ef7e3749":"# Analyze the count of survivors by Embarked variable\nax = sns.countplot(x=\"Embarked\", hue=\"Survived\", data=train)\ntrain[['Embarked', 'Survived']].groupby(['Embarked']).count().sort_values(by='Survived', ascending=False)","5d4c5799":"# Analyze the Survival Probability by Embarked\ng  = sns.factorplot(x=\"Embarked\",y=\"Survived\",data=train,kind=\"bar\", size = 7 ,palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")\ntrain[[\"Embarked\", \"Survived\"]].groupby(['Embarked']).mean().sort_values(by='Survived', ascending=False)","1ca4a826":"# Age, Pclass & Survival\nsns.lmplot('Age','Survived',data=train,hue='Pclass')","73e99040":"# Age, Embarked, Sex, Pclass\ng = sns.catplot(x=\"Age\", y=\"Embarked\",  hue=\"Sex\", row=\"Pclass\",   data=train[train.Embarked.notnull()], \norient=\"h\", height=2, aspect=3, palette=\"Set3\",  kind=\"violin\", dodge=True, cut=0, bw=.2)","0a3120ee":"# Relation among Pclass, Gender & Survival Rate\ng = sns.catplot(x=\"Sex\", y=\"Survived\", col=\"Pclass\", data=train, saturation=.5, kind=\"bar\", ci=None, aspect=.6)","b8d6fc17":"# Relation among SibSP, Gender & Survival Rate\ng = sns.catplot(x=\"Sex\", y=\"Survived\", col=\"SibSp\", data=train, saturation=.5,kind=\"bar\", ci=None, aspect=.6)","6d7c6df1":"# Relation among Parch, Gender & Survival Rate\ng = sns.catplot(x=\"Sex\", y=\"Survived\", col=\"Parch\", data=train, saturation=.5,kind=\"bar\", ci=None, aspect=.6)","19d0e8fd":"# Let's combining train & test for quick feature engineering. \n# Variable source is a kind of tag which indicates data source in combined data\ntrain['source']='train'\ntest['source']='test'\ncombdata = pd.concat([train, test],ignore_index=True)\nprint (train.shape, test.shape, combdata.shape)","eb24d49f":"# Let's check the data\ncombdata.head()","83677694":"# PassengerID - Drop PassengerID\ncombdata.drop(labels = [\"PassengerId\"], axis = 1, inplace = True)","59c90317":"# Pclass - Use as it is\ncombdata['Pclass'].unique()","47e9d665":"# Name - Extract Salutation from Name variable\nsalutation = [i.split(\",\")[1].split(\".\")[0].strip() for i in combdata[\"Name\"]]\ncombdata[\"Title\"] = pd.Series(salutation)\ncombdata[\"Title\"].unique()","6bfeb2ca":"# Convert other salutations to fixed Title \ncombdata[\"Title\"] = combdata[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ncombdata[\"Title\"] = combdata[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ncombdata[\"Title\"] = combdata[\"Title\"].astype(int)\ncombdata[\"Title\"].unique()","0c2884c2":"combdata = pd.get_dummies(combdata, columns = [\"Title\"])","55963d77":"# Drop Name variable\ncombdata.drop(labels = [\"Name\"], axis = 1, inplace = True)\ncombdata.head()","301f12b1":"# Age\n\n## Fill Age with the median age of similar rows according to Sex, Pclass, Parch and SibSp\n# Index of NaN age rows\nmissing_index = list(combdata[\"Age\"][combdata[\"Age\"].isnull()].index)\n\nfor i in missing_index :\n    median_age = combdata[\"Age\"].median()\n    filled_age = combdata[\"Age\"][((combdata['Sex'] == combdata.iloc[i][\"Sex\"]) & \n                                (combdata['SibSp'] == combdata.iloc[i][\"SibSp\"]) & \n                                (combdata['Parch'] == combdata.iloc[i][\"Parch\"]) & \n                                (combdata['Pclass'] == combdata.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(filled_age) :\n        combdata['Age'].iloc[i] = filled_age\n    else :\n        combdata['Age'].iloc[i] = median_age","ec59a6fe":"# Sex - Create dummy variables\n#combdata[\"Sex\"] = combdata[\"Sex\"].map({\"male\": 0, \"female\":1}) or\ncombdata = pd.get_dummies(combdata, columns = [\"Sex\"])","84dbfa19":"# Create a variable representing family size from SibSp and Parch\ncombdata[\"Fsize\"] = combdata[\"SibSp\"] + combdata[\"Parch\"] + 1\n\n# Create new feature of family size\ncombdata['Single'] = combdata['Fsize'].map(lambda s: 1 if s == 1 else 0)\ncombdata['SmallF'] = combdata['Fsize'].map(lambda s: 1 if  s == 2  else 0)\ncombdata['MedF'] = combdata['Fsize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ncombdata['LargeF'] = combdata['Fsize'].map(lambda s: 1 if s >= 5 else 0)","192d71ae":"# Analyze the Survival Probability by Fsize\n\ng  = sns.factorplot(x=\"Fsize\",y=\"Survived\",data=combdata,kind=\"bar\", size = 7 ,palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"Survival Probability\")\ncombdata[[\"Fsize\", \"Survived\"]].groupby(['Fsize']).mean().sort_values(by='Survived', ascending=False)","2a244294":"# Drop FSize variable\ncombdata.drop(labels = [\"Fsize\"], axis = 1, inplace = True)","4d28327f":"# SibSp - Create dummy variables\ncombdata = pd.get_dummies(combdata, columns = [\"SibSp\"])","6a7e60a7":"# Parch - Create dummy variables\ncombdata = pd.get_dummies(combdata, columns = [\"Parch\"])","2cae50ed":"# Ticket - Extracting the ticket prefix. This might be a representation of class\/compartment.\n# If there is no prefix replace with U (Unknown). \n\nTicket = []\nfor i in list(combdata.Ticket):\n    if not i.isdigit() :\n        Ticket.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0])\n    else:\n        Ticket.append(\"U\")\n        \ncombdata[\"Ticket\"] = Ticket\ncombdata[\"Ticket\"].unique()\n\ncombdata = pd.get_dummies(combdata, columns = [\"Ticket\"], prefix=\"T\")","e6a84585":"# Fare - Check the number of missing value\ncombdata[\"Fare\"].isnull().sum()\n\n# Only 1 value is missing so we will fill the same with median\ncombdata[\"Fare\"] = combdata[\"Fare\"].fillna(combdata[\"Fare\"].median())\n\n# Use the numpy fuction log1p which  applies log(1+x) to all elements of the column\n#combdata[\"Fare\"] = np.log1p(combdata[\"Fare\"])\n\n#Check the new distribution \nsns.distplot(combdata['Fare'] , fit=norm);","4918fb96":"# Cabin - Replace the missing Cabin number by the type of cabin unknown 'U'\ncombdata[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'U' for i in combdata['Cabin'] ])","f41a6b20":"# Let's plot the survival probability by Cabin\ng  = sns.factorplot(x=\"Cabin\",y=\"Survived\",data=combdata,kind=\"bar\", size = 7 ,\n                    palette = \"muted\",order=['A','B','C','D','E','F','G','T','U'])\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","71ae4b7b":"# Create dummy variables\ncombdata = pd.get_dummies(combdata, columns = [\"Cabin\"], prefix=\"Cabin\")","4f582148":"# Embarked - Find the number of missing values\ncombdata[\"Embarked\"].isnull().sum()\n\n# Fill Embarked missing values of dataset set with mode 'S'\ncombdata[\"Embarked\"] = combdata[\"Embarked\"].fillna(\"S\")\n\n# Create dummy variables\ncombdata = pd.get_dummies(combdata, columns = [\"Embarked\"], prefix=\"Emb\")","bf4d5a0d":"# Create dummies for PClass Now\ncombdata = pd.get_dummies(combdata, columns = [\"Pclass\"], prefix=\"Pclass\")","61f5b8fa":"combdata.info()","5ac88735":"## Separate train dataset and test dataset using the index variable 'source'\n\ntrain_x = combdata.loc[combdata['source']==\"train\"]\ntest_x = combdata.loc[combdata['source']==\"test\"]\n\ntest_x.drop(labels=[\"Survived\"],axis = 1,inplace=True)\n\ntrain_x.drop(labels=[\"source\"],axis = 1,inplace=True)\ntest_x.drop(labels=[\"source\"],axis = 1,inplace=True)\n\n# You may want to drop some variables to avoid dummy variable trap\n# test.drop(labels=['source','Sex_male', 'Fsize', 'LargeF', 'SibSp_8','Parch_9','T_WEP','Cabin_T','Emb_Q'],axis = 1,inplace=True)\ntest_x.shape","6b879807":"## Separate train features and label \n\n#train_x[\"Survived\"] = train_x[\"Survived\"].astype(int)\nY_train = train_x[\"Survived\"].astype(int)\ntrain_x = train_x.drop(labels = [\"Survived\"],axis = 1)\n\n# You may want to drop some variables to avoid dummy variable trap\n# X_train = train.drop(labels = [\"Survived\", 'Sex_male', 'Fsize', 'LargeF', 'SibSp_8','Parch_9','T_WEP','Cabin_T','Emb_Q'],axis = 1)\n#X_train.shape\n\nX_train = train_x.values\nY_train = Y_train.values\ntest = test_x.astype(np.float64, copy=False)","5860cd9d":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\ntest = scaler.fit_transform(test)\n","65f0fcb7":"import keras\nimport tensorflow as tf\nfrom keras.optimizers import SGD\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout","07ee302d":"# Creating the model\nmodel = Sequential()\n\n# Inputing the first layer with input dimensions\nmodel.add(Dense(32,  activation='relu',  \n                input_dim=79,\n                kernel_initializer='uniform',\n                name='Hidden1'))\n\n# Adding an Dropout layer to prevent from overfitting\nmodel.add(Dropout(0.50))\n\n#adding second hidden layer \nmodel.add(Dense(16,\n                kernel_initializer='uniform',\n                activation='relu',\n                name='Hidden2'))\n\n# Adding another Dropout layer\nmodel.add(Dropout(0.50))\n\n# adding the output layer that is binary [0,1]\nmodel.add(Dense(1,\n                kernel_initializer='uniform',\n                activation='sigmoid', name='Output'))\n\n#Visualizing the model\nmodel.summary()\n\n#Creating an Stochastic Gradient Descent\nsgd = SGD(lr = 0.01, momentum = 0.9)\n\n# Compiling our model\nmodel.compile(optimizer = sgd, \n                   loss = 'binary_crossentropy', \n                   metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nmodel.fit(X_train, Y_train, \n               batch_size = 30, \n               epochs = 200, verbose=2)","2a9ac8c1":"# y_preds = model.predict(test)\n# y_final = (y_preds > 0.5).astype(int)\n# submission = pd.read_csv(\"..\/input\/gender_submission.csv\", index_col='PassengerId')\n# submission['Survived'] = y_final.astype(int)\n# submission.to_csv('NNPrediction1.csv')","9712a5d0":"scores = model.evaluate(X_train, Y_train, batch_size=30)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","38dec05c":"**Age**\n\nThe insight below connects back to \"Ladies and Kids First\" scene of the movie. It shows that a good number of babies & young kids survived.","bfbe26f3":"**SibSp**","151bab59":"# **Data preparation including feature engineering**","5213e04f":"**Pclass**","cf35eff6":"**Cabin**\n\nAlphanumeric variable. \n\n687 missing values in train & 327 missing values in test data - which needs to be treated. We can create more features using this Cabin variable. ","9c0b8571":"\n\nWhat we need to do to process following variables  - \n\n**PassengerID** - No action required\n\n**PClass** - Have only 3 numerical values. We will use it as it is.\n\n**Name** - Can be used to create new variable Title by extracting the salutation from name.\n\n**Sex** - Create dummy variables\n\n**Age** - Missing value treatment, followed by creating dummy variables\n\n**SibSP** - Create dummy variables\n\n**Parch** - Create dummy variables\n\n**Ticket** - Create dummy variables post feature engineering\n\n**Fare** - Missing value treatment followed by log normalization\n\n**Cabin** - Create dummy variables post feature engineering\n\n**Embarked** - Create dummy variables","c4f17f5d":"# **Conclusion**\n\nTitle, Sex_Female, Fare & PClass seems to be common features preferred for classification.\n\nWhile Title & Age feature represents the Age category of passengers the features like Fare, PClass, Cabin etc. represents the economic status. Based on findings we can conclude that Age, Gender & features representing social\/economic status were primary factors affecting the survival of passenger.\n","9248a720":"**Pclass**\n\nPclass is categorical variable. Let's look at the distribution.","a0239fa8":"> # **Additional analysis**\n\nLet's create few additional charts to see how different variables are related.","d7f2f404":"**PassengerId**\n\nNot relevant from modeling perspective so we will drop this variable later","ef3a299c":"**SibSP**\n\nThis variable refers to number of siblings\/spouse onboard. SibSP = 1 and SibSP = 2 shows higher chances of survival.","c445e7c1":"The Fare variable is right skewed. We need to transform this variable using log function and make it more normally distributed. We will do this during feature engineering process.","53db914a":"**Fare**","15f0956e":"**Name**","37cd6078":"**PassengerID**","7719a811":"**If you find this notebook helpful, Please upvote and\/or leave a comment**","f88da7cd":"# **Problem Identification** \n\n**Best Practice -** \n\nThe most important part of any project is correct problem identification. Before you jump to \"How to do this\" part like typical Data Scientists, understand \"What\/Why\" part.  \nUnderstand the problem first and draft a rough strategy on a piece of paper to start with. Write down things like what are you expected to do & what data you might need or let's say what all algorithms you plan to use. \n\nNow the <a href=\"https:\/\/www.kaggle.com\/c\/titanic\/\"> Titanic challenge<\/a>  hosted by Kaggle is a competition in which the goal is to predict the survival or the death of a given passenger based on a set of variables describing  age, sex, or passenger's class on the boat.\n\n![](http:\/\/www.tyro.com\/content\/uploads\/2016\/04\/blog-twenty-one-business-icebergs-sink-business-280416.jpg)\n\nSo it is a classification problem and you are expected to predict Survived as 1 and Died as 0.","38de69e1":"**Embarked**","17b86e7e":"**Parch**","058be0ed":"**Ticket**","7c777fba":"**Creating Family Size variable using SibSp & Parch**","b77b9ec2":"# **Creating a Model**","cb439abc":"This is my 2nd Kernel for this competition. Link to my previous kernel is pasted below.\n\n[Step By Step Tutorial For Beginners](http:\/\/www.kaggle.com\/rp1611\/step-by-step-tutorial-for-beginners)\n\nAll data visualization remains the same, however, instead of emsembling method, I have tried simple neural network in this Kernal. The purpose of this kernel is to show how a simple NN model can be constructed. The model provides accuracy of **87.99%** on training dataset. \n\nThe kernel is purely for learning purpose so I would keep the kernel simple and leave it as it is post training the model. The best way to learn is by practicing so please feel free to tweak the parameters and use the framework to further improve. \n\n**Remember...It's all about experimenting and learning.**\n\n\n**If you like this notebook or find this notebook helpful, Please upvote and\/or leave a comment**","a7564b81":"# **Model evaluation**","0a070d22":"**Age**","b5654513":"**Ticket**\n\nThis variable has alphanumeric value which might not be related to Survival directly but we can use this variable to create some additional features.","4bc1a770":"<h1 id=\"tocheading\">Table of Contents<\/h1>\n<div id=\"toc\"><\/div>","a1db7615":"Based on data above, female passengers had better chances of survival than male passengers","96ab7699":"**Name**\n\nNot relevant from analysis & modeling perspective. We will drop this feature later after creating a new variable as Title.","d18df939":"**Sex**","95639541":"# **Exploratory data analysis**\n\nOne important aspect of machine learning is to ensure that the variables show almost the same trend across train & test data. If not, it would lead to overfitting because model is representing a relationship which is not applicable in the test dataset. \n\nI will give you one example here. As we do variable analysis, try to replicate (wherever applicable) the code for test data and see if there is any major difference in data distribution. \n\n**Example** - Let's start with finding the number of missing values. If you compare the output you will see that missing value percentages do not vary much across train & test datasets.\n\nUse the groupby\/univariate\/bivariate analysis method to compare the distribution across Train & Test data","16b45e80":"# **What would be the workflow?**\n\nI will keep it simple & crisp rather than using buzz words & useless data science frameworks. Frankly speaking no one cares. \n\nThis will help you to stay on track. So here is the workflow.\n\n**1. Problem Identification**\n\n**2. What data do we have?**\n\n**3. Exploratory data analysis**\n\n**4. Data preparation including feature engineering**\n\n**5. Developing a model**\n\n**6. Model evaluation**\n\n**7. Conclusions**\n\n**That's all you need to solve a data science problem.**","1233cda4":"# **Article on medium publication**\n\n\nI also wrote an article on medium on the same topic. You can [click this clink](https:\/\/medium.com\/@rp1611\/model-ensembles-for-survival-prediction-a3ecc9f7c2ae) and access the blog.\n","9c67361b":"**Sex**\n\nBased on analysis below, female had better chances of survival. \n\n![](https:\/\/www.ajc.com\/rf\/image_large\/Pub\/p9\/AJC\/2018\/07\/12\/Images\/newsEngin.22048809_071418-titanic_Titanic-Image-7--2-.jpg)","10123d7f":"**Fare**\n\nLet's check the distribution first.","09085fdf":"**Parch**\n\nParch indicates number of parents \/ children aboard the Titanic. Note that Parch = 3 and Parch = 1 shows higher survival probabilities. ","af4c3b6a":"**Title**","5e5c6fa9":"**Embarked**\n\nC = Cherbourg, Q = Queenstown, S = Southampton\n\nLet's explore the variable with Survival rate. Embarked represents port of embarkation. As the analysis output below suggests Emabrked C shows high probabilities of survival.","29cceb22":"# **What data do we have?**\n\n\nLet's import necessary libraries & bring in the datasets in Python environment first. Once we have the datasets in Python environment we can slice & dice the data to understand what we have and what is missing.","e770afac":"Approximately 62% of Pclass = 1 passenger survived followed by 47% of Pclass2."}}