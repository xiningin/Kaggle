{"cell_type":{"427e48de":"code","3d93fcf1":"code","c0a4744d":"code","2b33ce4b":"code","124c33cc":"code","9534aa0e":"code","d2dfb4a3":"code","e0a6c73a":"code","750bfb8e":"code","955418e8":"code","e05b3bc9":"code","76c5bb07":"code","9ad83f14":"code","0b0fb5f4":"code","bdb7aa53":"code","07475f14":"code","01b59994":"code","2d58b1b6":"code","2d4a83e8":"markdown","65252268":"markdown","50c413f8":"markdown","78c9c72c":"markdown","adf8ef92":"markdown","47bdc331":"markdown","053e9098":"markdown","85886e04":"markdown","bd135fe8":"markdown","c58f9d9c":"markdown","22244c9a":"markdown","0a528471":"markdown"},"source":{"427e48de":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nimport nltk\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE\nimport os","3d93fcf1":"metadata = pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/all_sources_metadata_2020-03-13.csv')","c0a4744d":"metadata.head()","2b33ce4b":"metadata.info()","124c33cc":"metadata_filter = metadata[metadata.abstract.str.contains('ethics|ethical|social science|multidisciplinary research', \n                                                          regex= True, na=False)].reset_index(drop=True)","9534aa0e":"len(metadata_filter)","d2dfb4a3":"metadata_filter.abstract[1]","e0a6c73a":"REPLACE_BY_SPACE_RE = re.compile('[\/(){}\\[\\]\\|@,;]')\nSTOPWORDS = set(stopwords.words('english'))\n\ndef clean_text(text):\n    text = text.lower()\n    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n    return text\n\nmetadata_filter['clean_abstract'] = metadata_filter['abstract'].apply(clean_text)","750bfb8e":"vectorizer = TfidfVectorizer(stop_words='english', max_features=2000)\nX = vectorizer.fit_transform(metadata_filter['clean_abstract'])","955418e8":"tsne = TSNE(perplexity=4, random_state=42)\n\nX_tsne = tsne.fit_transform(X)\nX_tsne = pd.DataFrame(data=X_tsne, columns=['D1', 'D2'])","e05b3bc9":"fig, ax = plt.subplots(figsize=(12,8))\nsns.scatterplot(ax=ax,x = 'D1', y = 'D2', data=X_tsne, alpha=0.7)\nplt.show()","76c5bb07":"kmeans = KMeans(n_clusters=2, random_state=42)\nkmeans.fit(X)","9ad83f14":"X_tsne['CLUSTER'] = kmeans.predict(X)\n\nfig, ax = plt.subplots(figsize=(12,8))\nsns.scatterplot(ax=ax,x = 'D1', y = 'D2', hue = 'CLUSTER', data=X_tsne, alpha=0.7)\nplt.show()","0b0fb5f4":"metadata_filter['cluster'] = kmeans.predict(X)\nmetadata_filter.groupby('cluster')['cluster'].count()","bdb7aa53":"X1 = metadata_filter.loc[metadata_filter['cluster'] == 0, 'clean_abstract']\nX2 = metadata_filter.loc[metadata_filter['cluster'] == 1, 'clean_abstract']\n\ntf_vectorizer1 = CountVectorizer(max_features=2000, stop_words='english')\ntf_vectorizer2 = CountVectorizer(max_features=2000, stop_words='english')\n\nX1 = tf_vectorizer1.fit_transform(X1)\nX2 = tf_vectorizer2.fit_transform(X2)\n\nlda1 = LatentDirichletAllocation(n_components=5, random_state=42)\nlda2 = LatentDirichletAllocation(n_components=5, random_state=42)\n\nlda1.fit(X1)\nlda2.fit(X2)","07475f14":"def print_top_words(model, feature_names, n_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        message = \"Topic #%d: \" % topic_idx\n        message += \" \".join([feature_names[i]\n                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n        print(message)","01b59994":"print_top_words(lda1, tf_vectorizer1.get_feature_names(), 5)","2d58b1b6":"print_top_words(lda2, tf_vectorizer2.get_feature_names(), 5)","2d4a83e8":"#### Now let's try to find the subject to each cluster. We count how many time a word appear and run a LDA to group important words","65252268":"### Now let's print the main topics and their keywords","50c413f8":"#### We can see that we may have a central cluster and other sparse elements. Let's see if this is a real cluster","78c9c72c":"#### Now lets transform the words into a word matrix","adf8ef92":"#### We can see that the first cluster (the bigger one) has more subjects that we want, while the second has more clinical data.\n#### We went from 29500 articles to 116, so this method could be useful to optimize the search for important articles. We could use the full text, or add more words to the regex search, but for now is a good first try.","47bdc331":"#### We found 155 articles, lets take a look into one.","053e9098":"# What has been published about ethical and social science considerations?\n## COVID-19 Open Research Dataset Challenge (CORD-19)\n\nLet's try to find what people are talking in the COVID-19 papers about ethical and social science.\nIn this aproach we are going to try a unsupervisioned method to find articles that match our need.\nThis can be useful to save time when we have a lot of data.","85886e04":"#### Let's see if we can find anything using the papers abstract","bd135fe8":"#### First, load the data","c58f9d9c":"#### We have a little separation (not the best one... maybe if we had more data it could be better) but let's work with it. Let's add the cluster information into the metadata DF","22244c9a":"#### Let's see if we have too many different subjects before getting the keywords. We use TSNE to reduce the word matrix to 2 dimensions, so we can plot the results.","0a528471":"#### We can read all 155 of then, or we can use unsupervisioned learning to find patterns inside the text.\n#### First let's prepare the data, removing anything that is not a word, and removing common words (stopwords)"}}