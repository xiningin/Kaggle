{"cell_type":{"58660cba":"code","97d0bcff":"code","7db22f82":"code","e7e45f3d":"code","0f14ecff":"code","7327fd7f":"code","da039758":"code","c569f02a":"code","dd103142":"code","8dc75e6a":"code","de5384ad":"code","ab365f63":"code","a1a31e30":"code","223d3225":"code","531eec35":"code","bb147702":"code","424b058b":"code","afa29ff2":"code","b67c0219":"code","3ff95b16":"code","feba9c08":"code","51a382ed":"code","9b62b04c":"code","a69a7ae3":"markdown","f0db2965":"markdown","336b982f":"markdown","e3accfc2":"markdown","49a48f54":"markdown","c02cba33":"markdown","cce2c75e":"markdown"},"source":{"58660cba":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import f1_score\nfrom collections import Counter\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn import preprocessing \nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.decomposition import PCA\nfrom tqdm.notebook import tqdm\nimport gensim\nimport os\nos.listdir(\"..\/input\/ykc-cup-2nd\/\")","97d0bcff":"train = pd.read_csv(\"..\/input\/ykc-cup-2nd\/train.csv\")\ntest = pd.read_csv(\"..\/input\/ykc-cup-2nd\/test.csv\")\nsub = pd.read_csv(\"..\/input\/ykc-cup-2nd\/sample_submission.csv\")\ntrain.shape, test.shape, sub.shape","7db22f82":"train.head()","e7e45f3d":"test.head()","0f14ecff":"sub.head()","7327fd7f":"## train\u3068test\u3092\u304f\u3063\u3064\u3051\u3066\u4e00\u62ec\u3067\u7279\u5fb4\u91cf\u4f5c\u6210\u3092\u3059\u308b\ndf = pd.concat([train, test])\ndf = df.reset_index(drop=True)\ndf.shape","da039758":"train[train[\"department_id\"] == 3].head()\n## \u91ce\u83dc\u3068\u304b\u679c\u7269\uff1f","c569f02a":"train[train[\"department_id\"] == 12].head()\n## \u8abf\u5473\u6599\uff1f","dd103142":"train[train[\"department_id\"] == 16].head()\n##\u6d17\u6fef\u7528\u5177\u3068\u304b","8dc75e6a":"df[\"product_name\"] = df[\"product_name\"].apply(lambda words : words.lower().replace(\",\", \"\").replace(\"&\", \"\").split(\" \"))\ndf.head()","de5384ad":"## \u8a13\u7df4\u6e08\u307f\u306e\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u3092\u8aad\u307f\u8fbc\u3093\u3067\uff0cproduct_name\u306b\u542b\u307e\u308c\u308b\u5358\u8a9e\u3092\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3057\u3066\u5e73\u5747\u3092\u53d6\u308b\u3053\u3068\u3067\uff0c\u5404product_id\u306b\u5bfe\u3057\u3066\u7279\u5fb4\u91cf\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210\u3059\u308b\n\n## gensim\u3067.vec\u304b\u3089\u8aad\u307f\u8fbc\u3080\u3068\u304d\u306b\u6642\u9593\u304c\u304b\u304b\u308b\u306e\u3067\uff0c\u4ed6\u306enotebook\u3067pickle\u3067\u4fdd\u5b58\u3057\u305f\u3082\u306e\u3092\u4f7f\u7528\u3057\u3066\u3044\u308b\nmodel = pd.read_pickle(\"..\/input\/ykc-cup-2nd-save-fasttext\/fasttext_gensim_model.pkl\") \n\n## gensim\u3067vec\u304b\u3089\u8aad\u307f\u8fbc\u3080\u5834\u5408\uff08\uff15\u5206\u3050\u3089\u3044\u304b\u304b\u308b\uff09\n# model = gensim.models.KeyedVectors.load_word2vec_format('..\/input\/ykc-2nd\/wiki-news-300d-1M.vec\/wiki-news-300d-1M.vec')\n\nfrom collections import defaultdict\nunused_words = defaultdict(int)\ndef to_vec(x, model):\n    v = np.zeros(model.vector_size)\n    for w in x:\n        try:\n            v += model[w] ## \u5358\u8a9e\u304c\u8a13\u7df4\u6e08\u307f\u30e2\u30c7\u30eb\u306evocab\u306b\u3042\u3063\u305f\u3089\n        except:\n            unused_words[w] += 1 ## \u30d9\u30af\u30c8\u30eb\u304c\u5b58\u5728\u3057\u306a\u304b\u3063\u305f\u5358\u8a9e\u3092\u30e1\u30e2\n    v = v \/ (np.sqrt(np.sum(v ** 2)) + 1e-16) ## \u9577\u3055\u30921\u306b\u6b63\u898f\u5316\n    return v    \nvecs = df[\"product_name\"].apply(lambda x : to_vec(x, model))\nvecs = np.vstack(vecs)\nfasttext_pretrain_cols = [f\"fasttext_pretrain_vec{k}\" for k in range(vecs.shape[1])]\nvec_df = pd.DataFrame(vecs, columns=fasttext_pretrain_cols)\ndf = pd.concat([df, vec_df], axis = 1)\ndf.head()","ab365f63":"sorted(unused_words.items(), key=lambda x: x[1], reverse = True)[:100]","a1a31e30":"# ## fasttext\u304b\u3089\u5f97\u3089\u308c\u305f\u30d9\u30af\u30c8\u30eb\u30922\u6b21\u5143\u306b\u843d\u3068\u3057\u3066\u6563\u5e03\u56f3\u3092\u66f8\u3044\u3066\u307f\u308b\n# pc = PCA(2).fit_transform(df[fasttext_pretrain_cols])\n# for department_id in range(21):\n#     idx = np.where(df[\"department_id\"] == department_id)[0]\n#     plt.scatter(pc[idx,0], pc[idx,1], label = department_id, s = 0.5, alpha = 0.5)\n# plt.legend()","223d3225":"features = fasttext_pretrain_cols + [\"order_rate\", \"order_dow_mode\", \"order_hour_of_day_mode\"] ## \u4e88\u6e2c\u306b\u4f7f\u7528\u3059\u308b\u7279\u5fb4\u91cf\u306e\u540d\u524d\ntarget = \"department_id\" ## \u4e88\u6e2c\u5bfe\u8c61\nn_split = 5 ## cross validation\u306efold\u6570","531eec35":"## train\u3068test\u3092\u5206\u96e2\ntrain = df[~df[target].isna()]\ntest = df[df[target].isna()]","bb147702":"# normalization\nscaler = preprocessing.StandardScaler()\ntrain[features] = scaler.fit_transform(train[features])\ntest[features] = scaler.transform(test[features])","424b058b":"import math\nimport random\nfrom typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\n\n# keras\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import models\nfrom tensorflow.keras import losses\nfrom tensorflow.keras import utils\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\n\ndef seed_everything(seed : int) -> NoReturn :    \n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n\nseed_everything(1220)    \n\n# adapted from https:\/\/github.com\/ghmagazine\/kagglebook\/blob\/master\/ch06\/ch06-03-hopt_nn.py\nparams = {\n    'input_dropout': 0.0,\n    'hidden_layers': 2,\n    'hidden_units': 128,\n    'hidden_activation': 'relu', \n    'lr': 1e-3,\n    'batch_size': 64,\n    'epochs': 40\n}\n    \ndef nn_model(L : int):\n    \"\"\"\n    NN hyperparameters and models\n    \n    :INPUT: \n    \n    :L: the number of features (int)\n    \"\"\"\n\n    # NN model architecture\n    n_neuron = params['hidden_units']\n\n    inputs = layers.Input(shape=(L, ))\n    x = layers.Dense(n_neuron, activation=params['hidden_activation'])(inputs)\n    x = layers.BatchNormalization()(x)\n\n    # stack more layers\n    for i in np.arange(params['hidden_layers'] - 1):\n        x = layers.Dense(n_neuron \/\/ (2 * (i+1)), activation=params['hidden_activation'])(x)\n        x = layers.BatchNormalization()(x)\n    \n    # output\n    out = layers.Dense(21, activation='softmax', name = 'out')(x)\n    model = models.Model(inputs=inputs, outputs=out)\n\n    # compile\n    loss = tfa.losses.SigmoidFocalCrossEntropy() # focal loss for imbalanced data\n    model.compile(loss=loss, optimizer=optimizers.Adam(lr=params['lr']))\n    \n    return model\n\n    # callbacks\n    history = model.fit(train_set['X'], train_set['y'], callbacks=[er, ReduceLR],\n                        epochs=params['epochs'], batch_size=params['batch_size'],\n                        validation_data=(val_set['X'], val_set['y']))        \n\n    return model, fi\n","afa29ff2":"model = nn_model(len(features))\nmodel.summary()","b67c0219":"## cross validation\npreds_test = []\nscores = []\nkfold = KFold(n_splits=n_split, shuffle = True, random_state=42)\nfor i_fold, (train_idx, valid_idx) in enumerate(kfold.split(train)):\n    print(f\"--------fold {i_fold}-------\")\n    \n    ## train data\n    x_tr = train.loc[train_idx, features]\n    y_tr = train.loc[train_idx, target]\n\n    ## valid data\n    x_va = train.loc[valid_idx, features]\n    y_va = train.loc[valid_idx, target]\n    \n    ## one-hot encoding y\n    ohe = preprocessing.OneHotEncoder(sparse=False, categories='auto')\n    y_tr = ohe.fit_transform(y_tr.values.reshape(-1, 1))\n    y_va = ohe.transform(y_va.values.reshape(-1, 1))\n\n    ## define NN model\n    model = nn_model(len(features))\n    \n    ### callbacks\n    er = callbacks.EarlyStopping(patience=8, min_delta=params['lr'], restore_best_weights=True, monitor='val_loss')\n    ReduceLR = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=8, verbose=1, epsilon=params['lr'], mode='min')\n    checkpoint_filepath = ''\n    model_checkpoint_callback = callbacks.ModelCheckpoint(filepath=f'mybestweight_fold{i_fold}.hdf5', save_weights_only=True, monitor='val_loss', save_best_only=True)\n    \n    model.fit(x_tr.values, y_tr, callbacks=[er, ReduceLR, model_checkpoint_callback], epochs=params['epochs'], batch_size=params['batch_size'],\n                        validation_data=(x_va.values, y_va))  \n    \n    ## predict on valid\n    pred_val = model.predict(x_va.values)\n    \n    ## evaluate\n    score = {\n        \"logloss\"  : log_loss(y_va, pred_val),\n        \"f1_micro\" : f1_score(np.argmax(y_va, axis=1), np.argmax(pred_val, axis=1), average = \"micro\")}\n    print(score)\n    scores.append(score)\n    \n    ## predict on test\n    pred_test = model.predict(test[features].values)\n    preds_test.append(pred_test)","3ff95b16":"score_df = pd.DataFrame(scores)\nscore_df","feba9c08":"score_df.mean()","51a382ed":"## cv\u306e\u5404fold\u3067\u8a08\u7b97\u3057\u305f\u4e88\u6e2c\u5024\u306e\u5e73\u5747\u3092\u6700\u7d42\u7684\u306a\u4e88\u6e2c\u5024\u306b\npred_test_final = np.array(preds_test).mean(axis = 0)\npred_test_final = np.argmax(pred_test_final, axis = 1)","9b62b04c":"sub[\"department_id\"] = pred_test_final\nsub.to_csv(\"submission.csv\", index = False)\nsub.head()","a69a7ae3":"## Multi-layer perceptron","f0db2965":"## feature engineering","336b982f":"## train","e3accfc2":"## submission","49a48f54":"\u6016\u304c\u3089\u305a\u306bNN\u306b\u6311\u6226\uff08\u9707\u3048\u58f0\uff09","c02cba33":"## EDA","cce2c75e":"## read data "}}