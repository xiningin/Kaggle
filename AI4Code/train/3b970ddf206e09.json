{"cell_type":{"1f3f3284":"code","82aeb858":"code","ba1f17dc":"code","503136ad":"code","b43c015d":"code","1c43b68b":"code","f9c57d68":"code","6315ba17":"code","b699c73a":"code","6cd6b7a8":"code","cd757e87":"code","5f83f384":"code","1c57fdfa":"code","eb2bfb88":"code","2eab3f05":"code","96c946b6":"code","1ea528db":"code","84364d27":"code","0cbac8e0":"code","f44790f1":"code","feac2de2":"code","10d4d528":"code","c416c1a7":"code","825abb27":"code","e4ad8a55":"code","39528363":"code","7b6e7db0":"code","d41996f1":"code","2b88021c":"code","08330933":"code","ba77db06":"code","240d6b20":"code","0cfcdb1d":"code","e25bddf0":"code","e13dba25":"code","99a882bd":"code","c952f015":"code","ffed5492":"markdown","00873c76":"markdown","a49a12d5":"markdown","d5122769":"markdown","2ec7ead1":"markdown","9b427866":"markdown","1e31b3f2":"markdown","3c011d9a":"markdown","f43ced54":"markdown","9adfd1c1":"markdown","1e5f179f":"markdown","10a1dd6d":"markdown","dc1c4090":"markdown","1e0a0b3a":"markdown","7973e6ea":"markdown","426fdbb4":"markdown","1765cf88":"markdown","658e782b":"markdown","bf78ea2a":"markdown"},"source":{"1f3f3284":"# Importing all kinds of packages, in no particular order\n\n# Basics\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#\u00a0Data cleaning:\nimport re\nfrom collections import Counter \nimport nltk\nfrom nltk import word_tokenize, FreqDist\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('stopwords')\nfrom nltk.tokenize import TweetTokenizer\nimport string\nfrom spellchecker import SpellChecker\n# Modeling basics:\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n# For D2V + RF:\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nimport multiprocessing\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import utils\n# For Glove + LSTM:\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers import LSTM,Dense,Dropout,Embedding,Bidirectional\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import Sequential\nfrom tqdm import tqdm","82aeb858":"# Load data\ntrain=pd.read_csv(\"train.csv\")\ntest=pd.read_csv(\"test.csv\")","ba1f17dc":"train.head()","503136ad":"print(\"Number of tweets in training data: \",train.shape[0])\nprint(\"Number of tweets about real disasters in training data: \", sum(train.target))\nprint(\"Number of tweets about fake disasters in training data: \", sum(train.target==0))\nprint(\"Number of tweets in test data: \",test.shape[0])","b43c015d":"print(\"Examples of real disaster tweets: \\n\")\nfor tweet in train[train.target==1].sample(3)[\"text\"]:\n    print(tweet)\n    print(\"\\n\")\nprint(\"____________________________ \\n\")\nprint(\"Examples of not real disaster tweets: \\n\")\nfor tweet in train[train.target==0].sample(3)[\"text\"]:\n    print(tweet)\n    print(\"\\n\")","1c43b68b":"indices=np.arange(train.shape[0])\nnp.random.shuffle(indices)\n\nprint(\"Size of original training set:\",train.shape[0])\n\n# Let's split our data into a training and a validation set\nval=train.iloc[indices[0:1500]] \ntrain=train.iloc[indices[1500:]]\n\nprint(\"Size of new training set:\",train.shape[0])\nprint(\"Size of validation set:\",val.shape[0])","f9c57d68":"example_tweet=\"OMG this vid is SoO funny!!! 4 :) http:\/\/www.youtube.com\/watch?v=_Vaasd332a #youtube\"","6315ba17":"train[\"text\"]=train[\"text\"].apply(lambda x: x.lower())\nval[\"text\"]=val[\"text\"].apply(lambda x: x.lower())\ntest[\"text\"]=test[\"text\"].apply(lambda x: x.lower())","b699c73a":"def remove_things(tweet):\n    # remove old style retweet text \"RT\"\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n\n    # replace hyperlinks with the word \"hyperlink\"\n    tweet = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', 'hyperlink', tweet)\n    \n    # replace digits with the word \"digit\"\n    tweet = re.sub(r'[0-9]+', 'digit', tweet)\n\n    # remove hashtags (only removing the hash # sign from the word)\n    tweet = re.sub(r'#', 'retweet', tweet)\n\n    return tweet","6cd6b7a8":"remove_things(example_tweet)","cd757e87":"train[\"text\"]=train[\"text\"].apply(lambda x: remove_things(x))\nval[\"text\"]=val[\"text\"].apply(lambda x: remove_things(x))\ntest[\"text\"]=test[\"text\"].apply(lambda x: remove_things(x))","5f83f384":"tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n\n# Create a list of tokenized tweets\ntrain_tweetlist=[]\nval_tweetlist=[]\ntest_tweetlist=[]\n\n# Run tokenization in all 3 subsets of our data:\nfor tweet in train[\"text\"]:\n    tokenized_tweet=tokenizer.tokenize(tweet)\n    train_tweetlist.append(tokenized_tweet)\n\nfor tweet in val[\"text\"]:\n    tokenized_tweet=tokenizer.tokenize(tweet)\n    val_tweetlist.append(tokenized_tweet)\n    \nfor tweet in test[\"text\"]:\n    tokenized_tweet=tokenizer.tokenize(tweet)\n    test_tweetlist.append(tokenized_tweet)","1c57fdfa":"def remove_punct_and_stopwords(tweet_tokens, \n                            punct_to_remove=string.punctuation,\n                            stopwords_to_remove=nltk.corpus.stopwords.words('english')):\n    cleaned_tweet=[]\n    for word in tweet_tokens: # Go through every word in your tokens list\n        if (word not in stopwords_to_remove and  # remove stopwords\n            word not in punct_to_remove):  # remove punctuation\n            cleaned_tweet.append(word)\n    return cleaned_tweet","eb2bfb88":"train_tweetlist=list(map(remove_punct_and_stopwords, train_tweetlist))\nval_tweetlist=list(map(remove_punct_and_stopwords, val_tweetlist))\ntest_tweetlist=list(map(remove_punct_and_stopwords, test_tweetlist))","2eab3f05":"spell = SpellChecker()\ndef correct_spellings(tweet_tokens):\n    corrected_text = []\n    misspelled_words = spell.unknown(tweet_tokens)\n    for token in tweet_tokens:\n        if token in misspelled_words:\n            corrected_text.append(spell.correction(token))\n        else:\n            corrected_text.append(token)\n    return \" \".join(corrected_text)","96c946b6":"correct_spellings([\"niec\",\"gramarr\",\"proces\"])","1ea528db":"wordnet_lemmatizer = WordNetLemmatizer()\n\ndef lemmatize_tweet(tweet_tokens):\n    return [wordnet_lemmatizer.lemmatize(token) for token in tweet_tokens]","84364d27":"train_tweetlist=list(map(lemmatize_tweet, train_tweetlist))\nval_tweetlist=list(map(lemmatize_tweet, val_tweetlist))\ntest_tweetlist=list(map(lemmatize_tweet, test_tweetlist))","0cbac8e0":"def count_tweets(tweets, ys):\n    '''\n    Input:\n        tweets: a list of (processed) tweets\n        ys: a list corresponding to the sentiment of each tweet (either 0 or 1)\n    Output:\n        result: a dictionary mapping each pair to its frequency\n    ''' \n    result={} \n    \n    for y, tweet in zip(ys, tweets):     # loop through tweets\n        for word in tweet: # loop through words\n            # define the key, which is the word and label tuple\n            pair = (word,y)\n\n            # if the key exists in the dictionary, increment the count\n            if pair in result:\n                result[pair] += 1\n\n            # else, if the key is new, add it to the dictionary and set the count to 1\n            else:\n                result[pair] = 1\n    return result","f44790f1":"ys=list(train.target)\nreal_fake_freqs=count_tweets(train_tweetlist,ys)","feac2de2":"# Let's look at an example of our count dictionary\nprint(real_fake_freqs[(\"disaster\",1)])\nprint(real_fake_freqs[(\"disaster\",0)])","10d4d528":"k = Counter(real_fake_freqs) \nhigh_freq=k.most_common(100)\n# Finding most common (actual) words in each class\ntop_unigram_counts_real=[h[1] for h in high_freq if h[0][1]==1][4:24]\ntop_unigrams_real=[h[0][0] for h in high_freq if h[0][1]==1][4:24]\ntop_unigram_counts_fake=[h[1] for h in high_freq if h[0][1]==0][4:24]\ntop_unigrams_fake=[h[0][0] for h in high_freq if h[0][1]==0][4:24]","c416c1a7":"# Let's plot the most common words within each class \nfig, axes = plt.subplots(ncols=2, figsize=(16, 10), dpi=100)\nplt.tight_layout()\n\nsns.barplot(x=top_unigram_counts_real,y=top_unigrams_real,color='red',ax=axes[0])\nsns.barplot(x=top_unigram_counts_fake,y=top_unigrams_fake,color='blue',ax=axes[1])\n\nfor i in range(2):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=13)\n\naxes[0].set_title(f'Top {20} most common unigrams in Disaster Tweets', fontsize=15)\naxes[1].set_title(f'Top {20} most common unigrams in Non-disaster Tweets', fontsize=15)","825abb27":"def train_naive_bayes(freqs, train_x, train_y):\n    '''\n    Input:\n        freqs: dictionary from (word, label) to how often the word appears\n        train_x: a list of tweets\n        train_y: a list of labels correponding to the tweets (0,1)\n    Output:\n        logprior: the log prior. (equation 3 above)\n        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n    '''\n    loglikelihood = {}\n    logprior = 0\n\n    # calculate V\n    vocab = set([pair[0] for pair in freqs.keys()])\n    V = len(vocab)\n\n    # calculate N_real, N_fake, V_real, V_fake\n    N_real = N_fake = V_real = V_fake = 0\n    for pair in freqs.keys():\n        # if the label is positive (real)\n        if pair[1] > 0:\n            # Increment the number of positive words by the count for this (word, label) pair\n            N_real += freqs[pair]\n\n        # else, the label is negative (fake)\n        else:\n            # increment the number of negative words by the count for this (word,label) pair\n            N_fake += freqs[pair]\n\n    # Calculate D, the number of documents\n    D = len(train_y)\n\n    # Calculate D_real and D_fake\n    D_real = sum(train_y)\n    D_fake = D-D_real\n    \n    # Calculate logprior\n    logprior = np.log(D_real) - np.log(D_fake)\n\n    # Calculate the loglikelihood for each word in the vocabulary...\n    for word in vocab:\n        # get the real and fake frequency of the word\n        freq_real = real_fake_freqs.setdefault((word,1),0)\n        freq_fake = real_fake_freqs.setdefault((word,0),0)\n\n        # calculate the probability that each word is positive, and negative\n        p_w_real = (freq_real + 1) \/ (N_real + V)\n        p_w_fake = (freq_fake + 1) \/ (N_fake + V)\n\n        # calculate the log likelihood of the word\n        loglikelihood[word] = np.log(p_w_real\/p_w_fake)\n\n    return logprior, loglikelihood","e4ad8a55":"logprior, loglikelihood = train_naive_bayes(real_fake_freqs, train_tweetlist, ys)\nprint(logprior)","39528363":"def naive_bayes_predict(tweet, logprior, loglikelihood):\n    '''\n    Input:\n        tweet: a (processed) list of tokens\n        logprior: a number\n        loglikelihood: a dictionary of words mapping to numbers\n    Output:\n        p: the sum of all the loglikelihoods of each word in the tweet (if found in the dictionary) \n           + logprior (a number)\n\n    '''\n     # initialize probability to zero\n    p = 0\n    # add the logprior\n    p += logprior\n\n    for word in tweet:\n            # add the log likelihood of that word to the probability\n            #\u00a0if word was not in the training set, add 0\n            p += loglikelihood.setdefault(word,0)\n    return p","7b6e7db0":"example_tokenized_tweet = [\"restaurant\",\"closed\",\"due\",\"to\",\"virus\",\"what\",\"tragedy\"]\np = naive_bayes_predict(example_tokenized_tweet, logprior, loglikelihood)\nprint(p)","d41996f1":"def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n    \"\"\"\n    Input:\n        test_x: A list of tweets\n        test_y: the corresponding labels for the list of tweets\n        logprior: the logprior\n        loglikelihood: a dictionary with the loglikelihoods for each word\n    Output:\n        accuracy: (# of tweets classified correctly)\/(total # of tweets)\n    \"\"\"\n    accuracy = 0\n    y_hats = []\n    \n    for tweet in test_x:\n        # if the prediction is > 0\n        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n            # the predicted class is 1\n            y_hat_i = 1\n        else:\n            # otherwise the predicted class is 0\n            y_hat_i = 0\n\n        # append the predicted class to the list y_hats\n        y_hats.append(y_hat_i)\n\n    # error is the average of the absolute values of the differences between y_hats and test_y\n    error = np.mean(np.absolute(y_hats-test_y))\n    accuracy = 1-error\n\n    return accuracy","2b88021c":"test_naive_bayes(val_tweetlist, val.target, logprior, loglikelihood)","08330933":"# Build a vocabulary for Doc2Vec\ntrain_documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(train_tweetlist)]\nval_documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(val_tweetlist)]\n\n# Initialize Doc2Vec model - we will use the Distributed Bag of Words version\ncores = multiprocessing.cpu_count()\nmodel_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\nmodel_dbow.build_vocab([x for x in tqdm(train_documents)])\n\n# Training Doc2Vec model\nfor epoch in range(30):\n    model_dbow.train(utils.shuffle([x for x in tqdm(train_documents)]), total_examples=len(train_documents), epochs=1)\n    model_dbow.alpha -= 0.002\n    model_dbow.min_alpha = model_dbow.alpha\n    \n# Create Doc2Vec embedding matrix of documents:\ndef create_matrix_embeddings(model, tagged_docs):\n    regressors = np.vstack([ model.infer_vector(doc.words, steps=20) for doc in tagged_docs])\n    return regressors\n\nX_train_d2v = create_matrix_embeddings(model_dbow, train_documents)\nX_val_d2v = create_matrix_embeddings(model_dbow, val_documents)\n","ba77db06":"RF = RandomForestClassifier()\nRF.fit(X_train_d2v, train.target)\ny_hat_RF = RF.predict(X_val_d2v)\n\naccuracy_score(y_hat_RF,val.target)","240d6b20":"# First we initialize a tokenizer for GloVe\ntoken = Tokenizer()\ntoken.fit_on_texts(train_tweetlist)\ntrain_seq = token.texts_to_sequences(train_tweetlist)\npad_seq = pad_sequences(train_seq,maxlen=200) # making the sequences equal length\nvocab_size = len(token.word_index)+1\n\n# Prepare dictionary for word embeddings\nembedding_vector = {}\n# Load the pre-trained GloVe embeddings \n#     these are trained on twitter data and can be downloaded from:\n#    https:\/\/nlp.stanford.edu\/projects\/glove\/\nf = open('\/Users\/matevaradi\/Documents\/coding repositories\/Python\/Projects 2020\/\\\nreal_or_not\/glove.twitter.27B\/glove.twitter.27B.200d.txt')\nfor line in tqdm(f):\n    value = line.split(' ')\n    word = value[0]\n    coef = np.array(value[1:],dtype = 'float32')\n    embedding_vector[word] = coef\n    \n# Convert training data into an embedding matrix for LSTM\nembedding_matrix = np.zeros((vocab_size,200))\nfor word,i in tqdm(token.word_index.items()):\n    embedding_value = embedding_vector.get(word)\n    if embedding_value is not None:\n        embedding_matrix[i] = embedding_value","0cfcdb1d":"# Build LSTM model\nmodel = Sequential()\nmodel.add(Embedding(vocab_size,200,weights = [embedding_matrix],input_length=200,trainable = False))\nmodel.add(Bidirectional(LSTM(75))) # bidirectional LSTM layer with 75 cells\nmodel.add(Dense(32,activation = 'relu'))\nmodel.add(Dense(1,activation = 'sigmoid'))\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics = ['accuracy'])\nhistory = model.fit(pad_seq,train.target,epochs = 5,batch_size=256,validation_split=0.2)","e25bddf0":"# Transforming validation data to GloVe embeddings\nX_val_glove = token.texts_to_sequences(val_tweetlist)\nval_seq = pad_sequences(X_val_glove,maxlen=200)\n\n#\u00a0Getting predictions from LSTM model\ny_hat_LSTM_Glove = model.predict_classes(val_seq)\ny_hat_LSTM_Glove = y_hat_LSTM_Glove[:,0] ","e13dba25":"accuracy_score(y_hat_LSTM_Glove,val.target)","99a882bd":"# Transforming validation data to GloVe embeddings\nX_test_glove = token.texts_to_sequences(test_tweetlist)\ntest_seq = pad_sequences(X_test_glove,maxlen=200)\n\n#\u00a0Getting predictions from LSTM model\nsubmit_y = model.predict_classes(test_seq)\nsubmit_y = submit_y[:,0]","c952f015":"submit=np.transpose(np.vstack((test.id,submit_y)))\n#\u00a0Creat and save submission file\nsubmit_df=pd.DataFrame(data=submit,\n                      columns=[\"id\",\"target\"])\nsubmit_df.to_csv(\"submit.csv\",index=False)","ffed5492":"#### Step 3: Tokenization","00873c76":"## 2. Data Cleaning <a class=\"anchor\" id=\"clean\"><\/a>","a49a12d5":"We will skip this spell checking test as it takes a lot of time to run.\n\n\n#### Step 6: Lemmatization\nLemmatization means trimming words into a common root. This is usually done before running most NLP tasks.","d5122769":"#### Step 2: Remove hashtags, hyperlinks and retweet signs\nWe do not want to treat individual hyperlinks as a separate term of the vocabulary, but the fact the the tweet contains a link may be relevant. The same goes for all digits and the (old) retweet sign as well. ","2ec7ead1":"## 3. Models for classification <a class=\"anchor\" id=\"models\"><\/a>\n### 3.1 Naive Bayes Classifier <a class=\"anchor\" id=\"NBC\"><\/a>","9b427866":"For simplicity we will use the same cleaning steps for all of our models. These will be the following:\n\n1. Make text lowercase\n2. Remove hashtags, hyperlinks and Twitter marks\n3. Tokenization - Converting a sentence into list of words\n4. Remove stopwords and punctuations\n5. Spell checking - correcting mispelled words (optional, as it takes a lot of time to run)\n6. Lemmetization\/stemming - Tranforming any form of a word to its root word\n\n#### Step 1: Make text lowercase","1e31b3f2":"Our example of a fake tragedy receives a positive score, meaning that the model would classify it as real.\nLet us test how the model performs on our test set:","3c011d9a":"This implementation is taken from the Week 2 of Course 1 of the [NLP Specialization of deeplearning.ai on Coursera](https:\/\/www.coursera.org\/specializations\/natural-language-processing). For the explanations I used [this summary](https:\/\/web.stanford.edu\/~jurafsky\/slp3\/4.pdf) of Naive Bayes Classifiers. \n\nIn a Naive Bayes model we work with both the bag-of-words and the naive Bayes assumptions. The latter means that the words (features $f_1 ... f_n$) within a document are independent of each other given their class $c$:\n\n$P\\left(f_{1}, f_{2}, \\ldots, f_{n} \\mid c\\right)=P\\left(f_{1} \\mid c\\right) \\cdot P\\left(f_{2} \\mid c\\right) \\cdot \\ldots \\cdot P\\left(f_{n} \\mid c\\right)$\n\nThe Naive Bayes classifier chooses the most probable class for each document $d$ given this independence assumption:\n\n$c_{N B}=\\underset{c \\in C}{\\operatorname{argmax}} P(c) \\prod_{f \\in F} P(f \\mid c).$\n\nAs usual, to avoid underflow problems, we use logs in calculations:\n\n$c_{N B}=\\underset{c \\in C}{\\operatorname{argmax}}\\ \\log P(c)+\\sum_{i \\in \\text {positions}} \\log P\\left(w_{i} \\mid c\\right)$\n\n$P(c)$ here is the prior probability for belonging to class $c$, which is given in our case by the proportion of real disaster tweets for the *real* class and vice versa for the *fake* class. $P\\left(w_{i} \\mid c\\right)$ is the probability of a word appearing in class $c$. To calculate these probabilities for each class, first we will go through our tweets and count how frequently their words appear in both real and fake tweets.","f43ced54":"# Real or Fake?\n\nIn this notebook I show three different solutions for the [\"Real or Not Real or Not? NLP with Disaster Tweets\n\"](https:\/\/www.kaggle.com\/c\/nlp-getting-started\/overview) Kaggle practice competition. The three approaches I use for classifying whether a tweet is about a real disaster or not are the following: Naive Bayes, Random Forest Classifier on Doc2Vec embeddings, and LSTM on GloVe embeddings. \n\n## Table of Contents\n\n* [1 Preparations](#prep)\n* [2 Data Cleaning](#clean)\n* [3 Models for Classification](#models)\n    * [3.1 Naive Bayes Classifier](#NBC)\n    * [3.2 Random Forest on Doc2Vec embeddings](#RF)\n    * [3.2 LSTM on GloVe embeddings](#LSTM)     \n* [4 Submission](#submission)\n\n\n\n## 1. Preparations <a class=\"anchor\" id=\"prep\"><\/a>","9adfd1c1":"#### Training Naive Bayes Classifier\n\nNotation:\n* $c$ : class (*real* or *fake*)\n* $W$: vocabulary, or set of unique words in dictionary\n* $V=|W|$ : number of unique words in dictionary\n* $N_{real}$, $N_{fake}$ : number of words that appear in real and fake tweets respectively\n* $D$ : number of documents\n* $D_{real}$, $D_{fake}$ : the number of real and fake documents respectively\n\nAs mentioned above, we can estimate the prior $P(c)$ of class *real* by $\\hat{P}(real)=\\frac{D_{real}}{D}$.\n\nTo estimate $P\\left(w_{i} \\mid c\\right)$ we use the counts we just stored in a dictionary:\n\n$\\hat{P}\\left(w_{i} \\mid c\\right)=\\frac{\\operatorname{count}\\left(w_{i}, c\\right)+1}{\\sum_{w \\in W}(\\operatorname{count}(w, c)+1)}=\\frac{\\operatorname{count}\\left(w_{i}, c\\right)+1}{\\left(\\sum_{w \\in W} \\operatorname{count}(w, c)\\right)+W}$,\n\nwhere adding one to the numerator is called Laplace (or add-one) smooting and is necessary to avoid multipliaction by 0 when a word is not present in class.\n\nFor example if we wish to calculate the probability that the word \"disaster\" belongs to the *real* class:\n\n$P(\"disaster\"|c=real)=\\frac{count(\"disaster\"|real)+1}{N_{real}+W}$.\n\nIn training we go through each word and calculate the probabilty that it belongs to each class. We take and store these log ratio of these probabilites, $\\log \\frac{\\hat{P}\\left(w_{i} \\mid c=real\\right)}{\\hat{P}\\left(w_{i} \\mid c=fake\\right)}$. Similary for the log prior, we store $\\log \\frac{\\hat{P}(c=real)}{\\hat{P}(c=fake)}.$ ","1e5f179f":"Seems like our super simple model reaches an accuracy of around 79%. Not too bad. Let's see what more complex machine learning models can do.\n\n### 3.2 Random Forest Classifier on Doc2Vec embeddings <a class=\"anchor\" id=\"RF\"><\/a>\n\nDoc2Vec was created by the authors of Word2Vec and it works in a similar way, but it creates vector representations of entire documents instead of only words. For details, see the [original paper](https:\/\/cs.stanford.edu\/~quocle\/paragraph_vector.pdf). \n\nWe will transform our tweets into Doc2Vec embeddings and run a standard Random Forest classifier on these embeddings to predict whether a tweet is about a real disaster or not. ","10a1dd6d":"#### Step 5: Spell checking","dc1c4090":"The negative prior means that there given no other information we expect that a random tweet is more likely to be about a non-real disaster than a real one. This is simply because there are more fake disaster tweets in the training data than real ones.\n\nWhen predicting a class for a new tweet we use the formula:\n\n$\\log \\frac{\\hat{P}(c=real)}{\\hat{P}(c=fake)}+\\sum_w \\log \\frac{\\hat{P}\\left(w \\mid c=real\\right)}{\\hat{P}\\left(w \\mid c=fake\\right)}$,\n\nor put more simply, the sum of the logprior and the per word loglikelihoods. If this score is positive it will be classified as a real disaster tweet. Note that if a word was not present in the training examples, then it adds no information to the prediction. ","1e0a0b3a":"The dataset is not too imbalanced, but there are slightly more fake disaster tweets than real ones. ","7973e6ea":"#### Step 4: Removing stopwords and punctuation","426fdbb4":"## References\n\n\n* [NLP Specialization on Coursera](https:\/\/www.coursera.org\/specializations\/natural-language-processing)\n* [Naive Bayes Classifier explanation](https:\/\/web.stanford.edu\/~jurafsky\/slp3\/4.pdf)\n* [Doc2Vec original paper](https:\/\/cs.stanford.edu\/~quocle\/paragraph_vector.pdf)\n* [GloVe original paper](https:\/\/nlp.stanford.edu\/pubs\/glove.pdf)\n* [Understanding LSTMs](http:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/)\n\nBlogposts and Kaggle Notebooks:\n   * https:\/\/towardsdatascience.com\/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4\n   * https:\/\/medium.com\/@sarin.samarth07\/glove-word-embeddings-with-keras-python-code-52131b0c8b1d\n   * https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove","1765cf88":"This approach seems to be actually worse than Naive Bayes. Let's move on to another apporach.\n\n### 3.3 LSTM on GloVe embeddings <a class=\"anchor\" id=\"LSTM\"><\/a>\n\nGloVe is an alternative to Word2Vec, as it can also be used to create vector representations from words (or in our case, documents). The main difference is that Word2Vec is a \"predictive\" model that uses a neural network to create embeddings, whereas GloVe is a \"count-based\" and is built on the co-occurrences of words within documents. The original paper can be found [here](https:\/\/nlp.stanford.edu\/pubs\/glove.pdf).\n\nLong Short Term Memory (LSTM) networks is a kind of Recurrent Neural Network (RNN) architercture that is commonly used for NLP tasks. I find [this explanation](http:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/) particularly useful.","658e782b":"The target takes the value 1 when the tweet is about a real disaser and 0 otherwise.","bf78ea2a":"A first LSTM model (without extra engineering steps and playing around with extra layers) on GloVe embeddings is only slightly better than Naive Bayes, but much better than the previous combination of Word2Vec with Random Forests. To me, both of these results are somewhat surprising.\n\n## 4. Submission <a class=\"anchor\" id=\"submit\"><\/a>\n\nWe could play around the models and\/or the data for much longer, adding new layers or extracting new features, but for now we will just run the best method according to accuracy score (Glove+LSTM) on the test set and submit the results. "}}