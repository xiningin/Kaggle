{"cell_type":{"0d26179e":"code","4d3426cd":"code","c0acc808":"code","18ba13c1":"code","1157fa05":"code","39e8efab":"code","c4c8f6d9":"code","1a507ea0":"code","38e66cb6":"code","d2bc5e0e":"code","8eb7c0c4":"code","96d34ee5":"code","97f17caa":"code","9dd3e8f5":"code","659c5b49":"code","94e07d2b":"code","3abb2405":"code","0435963d":"code","6bf85ed1":"code","614e3d10":"code","9b1023d3":"code","8177d97f":"code","5b57d281":"code","b35f744c":"code","781c81ca":"code","5d3a8660":"code","3ae8d07e":"code","6e34bc02":"code","59abf99b":"code","37f1dece":"code","78c8ecfb":"code","25e7138c":"code","8198274c":"code","1df3ae52":"code","70c9adba":"code","5e289f67":"code","becb2ce3":"code","8a8c1268":"code","8d3db728":"code","bc458a7f":"code","3b2d6eb7":"code","2f75b70c":"code","77c6ec18":"code","164615d1":"code","12e439a0":"code","9f0cc975":"code","a8e72fc7":"code","5df0058e":"code","3b7edacd":"code","f8e45e20":"code","475fb0e5":"code","6202d58a":"code","653739f4":"code","c540937e":"code","abc21044":"code","3b0ebd73":"code","e8ef31e3":"code","77f51210":"code","73bb31c0":"code","08441dcb":"code","e977bbb1":"code","5d8172c0":"code","b55b7230":"code","b5fbebaf":"code","ea90b6f8":"code","0a191074":"code","e3c947ca":"code","7c1e3319":"code","e50f7f02":"code","0578f6d0":"code","76b7b1b0":"code","5e9deb14":"code","0140e82b":"code","ba659557":"code","25e2f1de":"code","11563600":"code","c8326340":"code","f1b78450":"code","77c53a4a":"code","875b95dc":"code","8b75ceb5":"code","39d7d989":"code","2a844264":"code","440a92fc":"code","3ebaf389":"code","0613591f":"code","e3197193":"code","4183bab0":"code","2b0da42a":"code","863d297f":"code","e8c2e2ae":"code","1c2d3286":"code","02ad1639":"code","49a2b8a2":"code","72620f40":"code","feacace1":"code","155f66d9":"code","cf1e27ff":"markdown","525bf4ac":"markdown","c62867d1":"markdown","1830a595":"markdown","07d24928":"markdown","89f8478b":"markdown","b131dd02":"markdown","e2990539":"markdown","b32a217e":"markdown","5f2bb6bb":"markdown","fe087b2d":"markdown","88d81076":"markdown","d6685aa9":"markdown","a4aca581":"markdown","515f361c":"markdown","240146d7":"markdown","adb457ea":"markdown","f94b0834":"markdown","436db3b9":"markdown","763cad8e":"markdown","e871f855":"markdown","e0b309b1":"markdown","c1269771":"markdown","74d3858f":"markdown","92f6f523":"markdown","7a5efe0a":"markdown","e083eb58":"markdown","3b6e6dcf":"markdown","7e7a6b8f":"markdown","71dd0806":"markdown","1915a450":"markdown","79a7c813":"markdown","a46137b1":"markdown","55b829e8":"markdown","aa9140fd":"markdown","e46c9849":"markdown","d6f5b276":"markdown","71eb7922":"markdown"},"source":{"0d26179e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport math\nfrom datetime import datetime as dt\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n%matplotlib inline\n\n# \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0439 \u0443\u0434\u043e\u0431\u043d\u044b\u0439 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442 \u0434\u043b\u044f \u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430:\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4d3426cd":"# \u0432\u0441\u0435\u0433\u0434\u0430 \u0444\u0438\u043a\u0441\u0438\u0440\u0443\u0439\u0442\u0435 RANDOM_SEED, \u0447\u0442\u043e\u0431\u044b \u0432\u0430\u0448\u0438 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u044b \u0431\u044b\u043b\u0438 \u0432\u043e\u0441\u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u043c\u044b!\nRANDOM_SEED = 42","c0acc808":"# \u0437\u0430\u0444\u0438\u043a\u0441\u0438\u0440\u0443\u0435\u043c \u0432\u0435\u0440\u0441\u0438\u044e \u043f\u0430\u043a\u0435\u0442\u043e\u0432, \u0447\u0442\u043e\u0431\u044b \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u044b \u0431\u044b\u043b\u0438 \u0432\u043e\u0441\u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u043c\u044b:\n!pip freeze > requirements.txt","18ba13c1":"# function which find duplicated values in column (1-has dupl, 0-doesn't have)\n    def dup_col(col):\n        duplicated_col = col.value_counts().loc[lambda x: x>1].index.tolist()\n        new_col = []\n        for x in col:\n            if x in duplicated_col:\n                new_col.append(1)\n            else:\n                new_col.append(0)\n        return new_col","1157fa05":"# function which calculates difference between last 2 reviews dates\ndef find_days(d):\n    ln_d = len(d)\n    if ln_d >1:\n        new_d = sorted(d)\n        ans = (new_d[ln_d-1]-new_d[ln_d-2]).days\n        return ans","39e8efab":"# check IQR\ndef iqr(dt, cl, pr=None):\n    IQR = dt.loc[:, cl].quantile(0.75) - dt.loc[:, cl].quantile(0.25)\n    perc25 = dt.loc[:, cl].quantile(0.25)\n    perc75 = dt.loc[:, cl].quantile(0.75)\n    lb = perc25 - 1.5*IQR\n    hb = perc75 + 1.5*IQR\n\n    if pd.isnull(pr):\n        return [IQR, perc25, perc75, lb, hb, dt.loc[:, cl].max(), dt.loc[:, cl].min(), cl]\n    else:\n        print(cl,\n              '25-\u0439 \u043f\u0435\u0440\u0446\u0435\u043d\u0442\u0438\u043b\u044c: {},'.format(perc25),\n              '75-\u0439 \u043f\u0435\u0440\u0446\u0435\u043d\u0442\u0438\u043b\u044c: {},'.format(perc75),\n              \"IQR: {}, \".format(IQR),\n              \"\u0413\u0440\u0430\u043d\u0438\u0446\u044b \u0432\u044b\u0431\u0440\u043e\u0441\u043e\u0432: [{lb}, {hb}].\".format(lb=lb, hb=hb))","c4c8f6d9":"# hist function\n# plots all hists together to have a look in general\n\n# features - list of columns to plot\n# rows, cols - number of rows & cols for hists\n# fg1,2 - figure size\n# b - number of bins per hist\ndef histograms_plot(dataframe, features, rows, cols, fg1, fg2, b):\n    fig = plt.figure(figsize=(fg1, fg2))\n    for i, feature in enumerate(features):\n        ax = fig.add_subplot(rows, cols, i+1)\n        dataframe[feature].hist(bins=b, ax=ax, facecolor='blue')\n        ax.set_title(feature+\" distribution\", color='#1D0772')\n\n        fig.tight_layout()\n    plt.show()","1a507ea0":"# plot one hist to check outliers closer or just to look deeper\n\n# tit - hist title\n# rhb - reviewed highest border in '\u0417\u0434\u0440\u0430\u0432\u044b\u0439 \u0441\u043c\u044b\u0441\u043b'\n# rlb - reviewed lowest border in '\u0417\u0434\u0440\u0430\u0432\u044b\u0439 \u0441\u043c\u044b\u0441\u043b'\n# one of rhb or rlb could be missed\n\ndef hist_check(dth, b, tit, rhb=None, rlb=None):\n    IQR = dth.quantile(0.75) - dth.quantile(0.25)\n    perc25 = dth.quantile(0.25)\n    perc75 = dth.quantile(0.75)\n    lb = perc25 - 1.5*IQR\n    hb = perc75 + 1.5*IQR\n\n    print(\n        '25-\u0439 \u043f\u0435\u0440\u0446\u0435\u043d\u0442\u0438\u043b\u044c: {},'.format(perc25),\n        '75-\u0439 \u043f\u0435\u0440\u0446\u0435\u043d\u0442\u0438\u043b\u044c: {},'.format(perc75),\n        \"IQR: {}, \".format(IQR),\n        \"\u0413\u0440\u0430\u043d\u0438\u0446\u044b \u0432\u044b\u0431\u0440\u043e\u0441\u043e\u0432: [{lb}, {hb}].\".format(lb=lb, hb=hb))\n\n    fig = plt.figure()\n    dth.loc[dth.between(\n        lb,\n        hb)].hist(bins=b, label='IQR')\n\n    if rlb != None and rhb != None:\n        dth.loc[(rlb <= dth) & (dth <= rhb)].hist(\n            alpha=0.5, bins=b,  label='\u0417\u0434\u0440\u0430\u0432\u044b\u0439 \u0441\u043c\u044b\u0441\u043b')\n    elif pd.isnull(rlb) and rhb != None:\n        dth.loc[dth <= rhb].hist(\n            alpha=0.5, bins=b,  label='\u0417\u0434\u0440\u0430\u0432\u044b\u0439 \u0441\u043c\u044b\u0441\u043b')\n    elif pd.isnull(rhb) and rlb != None:\n        dth.loc[rlb <= dth].hist(\n            alpha=0.5, bins=b,  label='\u0417\u0434\u0440\u0430\u0432\u044b\u0439 \u0441\u043c\u044b\u0441\u043b')\n\n    plt.legend()\n    plt.title(tit)\n    return [lb, hb]","38e66cb6":"scaler= MinMaxScaler()","d2bc5e0e":"DATA_DIR = '\/kaggle\/input\/sf-dst-restaurant-rating\/'\ndf_train = pd.read_csv(DATA_DIR+'\/main_task.csv')\ndf_test = pd.read_csv(DATA_DIR+'kaggle_task.csv')\nsample_submission = pd.read_csv(DATA_DIR+'\/sample_submission.csv')","8eb7c0c4":"df_train.info()","96d34ee5":"df_train.head(5)","97f17caa":"df_test.info()","9dd3e8f5":"df_test.head(5)","659c5b49":"sample_submission.head(5)","94e07d2b":"sample_submission.info()","3abb2405":"# \u0412\u0410\u0416\u041d\u041e! \u0434\u0440\u044f \u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u043e\u0439 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u043c \u0442\u0440\u0435\u0439\u043d \u0438 \u0442\u0435\u0441\u0442 \u0432 \u043e\u0434\u0438\u043d \u0434\u0430\u0442\u0430\u0441\u0435\u0442\ndf_train['sample'] = 1 # \u043f\u043e\u043c\u0435\u0447\u0430\u0435\u043c \u0433\u0434\u0435 \u0443 \u043d\u0430\u0441 \u0442\u0440\u0435\u0439\u043d\ndf_test['sample'] = 0 # \u043f\u043e\u043c\u0435\u0447\u0430\u0435\u043c \u0433\u0434\u0435 \u0443 \u043d\u0430\u0441 \u0442\u0435\u0441\u0442\ndf_test['Rating'] = 0 # \u0432 \u0442\u0435\u0441\u0442\u0435 \u0443 \u043d\u0430\u0441 \u043d\u0435\u0442 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f Rating, \u043c\u044b \u0435\u0433\u043e \u0434\u043e\u043b\u0436\u043d\u044b \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u044c, \u043f\u043e \u044d\u0442\u043e\u043c\u0443 \u043f\u043e\u043a\u0430 \u043f\u0440\u043e\u0441\u0442\u043e \u0437\u0430\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u043d\u0443\u043b\u044f\u043c\u0438\n\ndata = df_test.append(df_train, sort=False).reset_index(drop=True) # \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u043c","0435963d":"# get number type columns\nnum_cols = data.select_dtypes(include='number').columns","6bf85ed1":"data.info()","614e3d10":"data.sample(5)","9b1023d3":"data.Reviews[1]","8177d97f":"# find if there were any duplicates in the data\nprint(\"\u0414\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432 \u043d\u0430\u0439\u0434\u0435\u043d\u043e:\", data.duplicated(keep=False).sum())\n\n\n# Look at the nan data in general\nplt.subplots(figsize=(12, 7))\nsns.heatmap(data.isnull())","5b57d281":"# plot & look at variability of all quantitative data in general\nhistograms_plot(data, num_cols, 4, 4, fg1=10, fg2=10, b=100)","b35f744c":"# prepare all IQR to find columns with outliers\nal = []\nfor col in num_cols:\n    al.append(iqr(data, col))\n    \n# find columns with outliers => NaN in outlier column\nall_borders = pd.DataFrame(al, columns=['IQR', 'perc25', 'perc75', 'lb', 'hb', 'max_v', 'min_v', 'data'])\nall_borders.loc[(all_borders.hb >= all_borders.max_v) & (\n    all_borders.lb <= all_borders.min_v), 'outlier'] = 'no'\nall_borders","781c81ca":"data.nunique(dropna=False)","5d3a8660":"# create new column: if  Number_of_Reviews consist nan\ndata['Number_of_Reviews_isNAN'] = pd.isna(data['Number of Reviews']).astype('uint8')\ndata['Number_of_Reviews_isNAN'].value_counts()","3ae8d07e":"# need to look deeper at the data at first\ncol_name = 'Number of Reviews'\nbord = hist_check(data[col_name], 17, tit=col_name, rhb=10000)","6e34bc02":"# look at the Number of Reviews outliers\nsns.boxplot(data['Number of Reviews'])","59abf99b":"# according to the boxplot it seems that values higher than 5000 are real outliers\n# will replace higher values by 5000\ndata['Number of Reviews'] = data['Number of Reviews'].apply(lambda x: 5000 if x > 5000 else x)","37f1dece":"# Fill Number of Reviews missing values by median per City\n# for each column with nan's median,median per city, 0 filling was tried - selected the best with better mae\ndata[\"Number of Reviews\"] = data.groupby(\"City\")['Number of Reviews']\\\n.transform(lambda x: x.fillna(round(x.mean())))","78c8ecfb":"# add log2 column to make normal distribution - leads to worse mae, will remove in the end\ndata[\"Number of Reviews_log\"] = np.log2(data[\"Number of Reviews\"])\ncol_name = 'Number of Reviews_log'\nbord = hist_check(data[col_name], 17, tit=col_name, rhb=np.log2(10000))","25e7138c":"# create Cuisine style dummy variable - worse MAE\n# d_cus = data['Cuisine Style'].apply(lambda x: str(x).strip('[]')).str.get_dummies(sep=', ')\n# data = data.merge(d_cus, left_index = True, right_index = True)","8198274c":"# make column easy to work\ndata['Cuisine Style'] = data['Cuisine Style']\\\n                        .apply(lambda x:  np.nan if pd.isna(x) else str(x).strip('[]').replace(\"'\",\"\").split(\", \"))\n\n","1df3ae52":"# look at the list of most spread cuisines\ndata['Cuisine Style'].explode().value_counts(ascending=False)","70c9adba":"# create most spread cuisine dummy (select top 3, as top 5 leads to not so good mae) \ncus_list = data['Cuisine Style'].explode().value_counts(ascending=False)[:3].index.tolist()\nfor col in cus_list:\n    if col not in data.columns:\n        data[col] = data['Cuisine Style'].apply(lambda x: 1 if type(x)!=float and col in x   else 0)\n\n# create column if all cuisines are not in top 3 list\ndata['Cuisine_else_all'] = data['Cuisine Style']\\\n                            .apply(lambda x: 1 if type(x)!=float and all([cus not in cus_list for cus in x]) else 0)\n\n# both lead to worse MAE\n\n# if any cuisines are not in top 3 list\n# data['Cuisine_else_any'] = data['Cuisine Style'].apply(lambda x: 1 if type(x)!=float and any([cus not in cus_list for cus in x])   else 0)\n\n# if Cuisine is nan\n# data['Cuisine_Nan'] = np.where(pd.isna(data['Cuisine Style'].values), 1, 0) \n","5e289f67":"# calculate number of cuisines (1 if nan)\ndata['Cuisine_Style_number'] = data['Cuisine Style']\\\n                                .apply(lambda x: 1 if type(x)==float and pd.isna(x)  else len(x))","becb2ce3":"# create new column: if price range consist nan\ndata['Price_Range_nan'] = pd.isna(data['Price Range']).astype('uint8')\n","8a8c1268":"data['Price Range'].value_counts()","8d3db728":"# encode price range + fill nan's by mean per city \ndata['New_Price_Range'] = data['Price Range'].map({'$$ - $$$':2, '$':1, '$$$$':3})\n\ndata['New_Price_Range'] = data.groupby(\"City\")['New_Price_Range']\\\n                            .transform(lambda x: x.fillna(round(x.mean()))).astype(int)","bc458a7f":"# create id_ta without d\ndata['ID_TA_n'] = data.ID_TA.apply(lambda x: x[1:]).astype(str).astype(int)\n\n","3b2d6eb7":"# look at the ID_TA length \ndata.ID_TA.apply(lambda x: len(x[1:])).hist(bins=5)","2f75b70c":"# calculate len of id_ta - worse mae\n# data['ID_TA_len'] = data.ID_TA.apply(lambda x: len(x[1:]))","77c6ec18":"# check what duplicates we have here\ndata.ID_TA.value_counts().unique()","164615d1":"# create column with id_ta which has duplicates \ndata['ID_TA_dup'] = dup_col(data.ID_TA)","12e439a0":"# create column with review dates\npat = re.compile('\\'\\d+\\\/\\d+\\\/\\d+\\'?')\ndata['Review_date'] = data.Reviews\\\n                    .apply(lambda x: [] if pd.isna(x) else re.findall(pat,x) )\\\n                    .apply(lambda y:[pd.to_datetime(item) for item in y ])","9f0cc975":"# calculate difference of days between last two reviews \ndata['Review_date_dif'] = data.Review_date.apply(lambda x: find_days(x))\n\n# create column with nan in Review_date_dif - worse mae   \n# data['Review_date_dif_isNAN'] = pd.isna(data['Review_date_dif']).astype('uint8')","a8e72fc7":"# look at the Review_date_dif\ncol_name = 'Review_date_dif'\nbord = hist_check(data[col_name], 17, tit=col_name, rhb = 3000)","5df0058e":"# look at log2 column in order to make normal distribution \ncol_name = 'Review_date_dif'\nbord = hist_check(np.log2(data[col_name]+1), 17, tit=col_name, rhb = 3000)","3b7edacd":"# try different Review_date_dif na filling\n# data['Review_date_dif'] = data.groupby(\"City\")['Review_date_dif']\\\n# .transform(lambda x: x.fillna(round(x.mean()))).astype(int)\ndata['Review_date_dif'].fillna(round(data['Review_date_dif'].mean()), inplace=True)\n\n\n# create column with last review date & todays difference - worse mae\n# td = pd.to_datetime(\"today\")\n# data['last_rev_date'] = data.Review_date.apply(lambda x: np.nan if not x else (td-max(x)).days)\n\n\n# create column with last review date & the latest review at all difference\ndata['last_rev_date'] = data.Review_date.apply(lambda x: np.nan if not x else max(x))\nmd = max(data['last_rev_date'].dropna())\ndata['last_rev_date'] = data['last_rev_date'].apply(lambda x: (md-x).days)\n\n\n# try last review date to timestamp & rescale - worse mae\n# data['last_rev_date'] = data.Review_date.apply(lambda x: np.nan if not x else max(x).timestamp())#.values\n# data['last_rev_date'] = scaler.fit_transform(data[['last_rev_date']]).flatten().tolist()\n\n","f8e45e20":"# look at the last_rev_date\ncol_name = 'last_rev_date'\nbord = hist_check(data[col_name], 17, tit=col_name, rhb = 5000)","475fb0e5":"# try different last_rev_date na filling\n# data['last_rev_date'].fillna(0, inplace=True)\ndata['last_rev_date'].fillna(round(data['last_rev_date'].mean()), inplace=True)\n# data['last_rev_date'] = data.groupby(\"City\")['last_rev_date']\\\n# .transform(lambda x: x.fillna(round(x.mean()))).astype(int)","6202d58a":"# look at log2 column in order to make normal distribution \ncol_name = 'last_rev_date'\nbord = hist_check(np.log2(data[col_name]+1), 17, tit=col_name, rhb = 5000)","653739f4":"# get log2 last_rev_date & Review_date_dif in order to make normal distribution \ndata['last_rev_date'] = np.log2(data['last_rev_date']+1)\ndata['Review_date_dif'] = np.log2(data['Review_date_dif']+1)","c540937e":"plt.rcParams['figure.figsize'] = (10,7)\ndata['Ranking'].hist(bins=100)","abc21044":"data['City'].value_counts(ascending=True).plot(kind='barh')","3b0ebd73":"data['Ranking'][data['City'] =='London'].hist(bins=100)","e8ef31e3":"# look at top 10 Cities - we've seen the correlation between city & ranking\nfor x in (data['City'].value_counts())[0:10].index:\n    data['Ranking'][data['City'] == x].hist(bins=100)\nplt.show()","77f51210":"# look at the restaurant id duplicates\ndata.Restaurant_id.value_counts()[:10]","73bb31c0":"# rest id which has duplicates - could be restaurant network \n# adding network feature - leads to worse mae\n# data['Restaurant_id_dup'] = dup_col(data.Restaurant_id)\n\n# create column with number of restaurant from list per city  \ndata['rest_in_city'] = data.groupby('City')['Restaurant_id'].transform('count')","08441dcb":"# look at the correlation between ranking, number of restaurant per city & number of reviews\n# ranking, number of restaurant per city - have high correlation\ndisplay(data[[\"Ranking\",\"rest_in_city\",\"Number of Reviews\"]].corr())\nsns.pairplot(data, vars = [\"Ranking\",\"rest_in_city\",\"Number of Reviews\"], kind = 'reg',diag_kind=\"kde\")","e977bbb1":"# normalize ranking & number of restaurant\ndata['norm_rest'] = data['rest_in_city']\/data['Ranking']\ndata['norm_rank'] = data['Ranking']\/data['rest_in_city']\n","5d8172c0":"# multiply columns where we've seen correlation and try different\npf = PolynomialFeatures(2)\npoly_features = pf.fit_transform(data[['rest_in_city', 'Ranking','Number of Reviews']])\ncol_poly = ['c1','c2','c3','c4', 'c5', 'c6', 'c7', 'c8', 'c9', 'c10']\npoly_frame=pd.DataFrame(poly_features, columns=col_poly)\n# only c9 makes mae better\ndata= pd.concat([data, poly_frame['c9']], axis=1)#.drop(['c1'], axis = 1, inplace=True)","b55b7230":"# look at number of unique cuisines per city\ndata.groupby('City')['Cuisine Style'].apply(lambda x: len(np.unique(np.hstack(x)))).hist()","b5fbebaf":"# add number of unique cuisines per city as new feature\ndata['Cuisine_number_per_city'] = data.groupby('City')['Cuisine Style'].transform(lambda x: len(np.unique(np.hstack(x))))","ea90b6f8":"# look at the Number of Reviews per city\ndata.groupby('City')['Number of Reviews'].apply(lambda x: round(x.mean())).hist()","0a191074":"# add feature mean number of reviews per city - worse mae \n# it has low corr with number of restaurant in city\n# data['city_rev_numb'] = data.groupby('City')['Number of Reviews'].transform(lambda x: round(x.mean())).astype(int)","e3c947ca":"# all these lead to worse mae\n\n# data['norm_price'] = data['New_Price_Range']\/data['Cuisine_Style_number']\n# data['norm_rank_price'] = data['Ranking']\/data['New_Price_Range']\n# data['norm_price'] = data['New_Price_Range']\/data['Number of Reviews']\n# data['norm_rank_rev'] = data['Ranking']\/data['Number of Reviews']\n# data['dif_day_rank'] = data['Ranking']*data['Review_date_dif']\n# data['rank_price'] = data['Ranking']*data['New_Price_Range']\n# data['rest_city_price'] = data['rest_in_city']*data['New_Price_Range']\n \n# data['last_rev_diff_rank'] = data['Ranking']*data['last_rev_date']\n# data['last_rev_diff_rest'] = data['rest_in_city']*data['last_rev_date']\n# data['dif_day_norm'] = data['Review_date_dif']\/data['rest_in_city']\n# data['numb_rev_price'] = data['New_Price_Range']*data['Number of Reviews']\n \n# data.drop(['Review_date_dif'], axis = 1, inplace=True)","7c1e3319":"data['Rating'].value_counts(ascending=True).plot(kind='barh')","e50f7f02":"data['Ranking'][data['Rating'] == 5].hist(bins=100)","0578f6d0":"data['Ranking'][data['Rating'] < 4].hist(bins=100)","76b7b1b0":"col_name = 'Ranking'\nbord = hist_check(data[col_name], 17, tit=col_name, rhb = 25000)","5e9deb14":"#try to normalize ranking in another way - according to the IQR\ndata['Ranking_bin'] = np.where(data['Ranking'] > 5241, 3,\n                     np.where(data['Ranking'] < 972, 1,2))\n\ndata['Ranking_bin'].hist()","0140e82b":"# try max city ranking normalizing\ndata['Ranking_max_norm'] = data['Ranking'] \/ data['City'].map(data.groupby(['City'])['Ranking'].max())","ba659557":"# create City dummy variable, no need in nan column  as there is no nan values in that columns\ndata = pd.get_dummies(data, columns=[ 'City'])\n\n# create Restaurant_id dummy variable\ndata = pd.get_dummies(data, columns = ['Restaurant_id'])","25e2f1de":"dummy_col = [col for col in data.columns if ('City_' in col) or ('Restaurant_id_id' in col)]\n","11563600":"# remove not needed columns - lead to worse mae\nlong_run_data=data.copy()\nlong_run_data.drop(['Number of Reviews_log','Ranking','Number of Reviews'], axis = 1, inplace=True)\ndf_preproc = long_run_data.copy().select_dtypes(include=np.number)","c8326340":"plt.rcParams['figure.figsize'] = (15,10)\nsns.heatmap(df_preproc.drop(dummy_col, axis=1).corr(),)","f1b78450":"df_preproc.drop(dummy_col, axis=1).corr()\n","77c53a4a":"# \u043d\u0430 \u0432\u0441\u044f\u043a\u0438\u0439 \u0441\u043b\u0443\u0447\u0430\u0439, \u0437\u0430\u043d\u043e\u0432\u043e \u043f\u043e\u0434\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435\ndf_train = pd.read_csv(DATA_DIR+'\/main_task.csv')\ndf_test = pd.read_csv(DATA_DIR+'\/kaggle_task.csv')\ndf_train['sample'] = 1 # \u043f\u043e\u043c\u0435\u0447\u0430\u0435\u043c \u0433\u0434\u0435 \u0443 \u043d\u0430\u0441 \u0442\u0440\u0435\u0439\u043d\ndf_test['sample'] = 0 # \u043f\u043e\u043c\u0435\u0447\u0430\u0435\u043c \u0433\u0434\u0435 \u0443 \u043d\u0430\u0441 \u0442\u0435\u0441\u0442\ndf_test['Rating'] = 0 # \u0432 \u0442\u0435\u0441\u0442\u0435 \u0443 \u043d\u0430\u0441 \u043d\u0435\u0442 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f Rating, \u043c\u044b \u0435\u0433\u043e \u0434\u043e\u043b\u0436\u043d\u044b \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u044c, \u043f\u043e \u044d\u0442\u043e\u043c\u0443 \u043f\u043e\u043a\u0430 \u043f\u0440\u043e\u0441\u0442\u043e \u0437\u0430\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u043d\u0443\u043b\u044f\u043c\u0438\n\ndata = df_test.append(df_train, sort=False).reset_index(drop=True) # \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u043c\ndata.info()","875b95dc":"def preproc_data(df_input):\n    '''includes several functions to pre-process the predictor data.'''\n    \n    df_output = df_input.copy()\n     \n    # number of reviews    \n    df_output['Number_of_Reviews_isNAN'] = pd.isna(df_output['Number of Reviews']).astype('uint8')\n    df_output['Number of Reviews'] = df_output['Number of Reviews'].apply(lambda x: 5000 if x > 5000 else x)\n    df_output[\"Number of Reviews\"] = df_output.groupby(\"City\")['Number of Reviews']\\\n                                        .transform(lambda x: x.fillna(round(x.mean())))\n    \n    # cuisine style \n    df_output['Cuisine Style'] = df_output['Cuisine Style']\\\n                                .apply(lambda x:  np.nan if pd.isna(x) else str(x).strip('[]').replace(\"'\",\"\").split(\", \"))\n    cus_list = df_output['Cuisine Style'].explode().value_counts(ascending=False)[:3].index.tolist()\n    for col in cus_list:\n        if col not in df_output.columns:\n            df_output[col] = df_output['Cuisine Style'].apply(lambda x: 1 if type(x)!=float and col in x   else 0)\n    \n    df_output['Cuisine_else_all'] = df_output['Cuisine Style']\\\n                                    .apply(lambda x: 1 if type(x)!=float and all([cus not in cus_list for cus in x])   else 0)\n    df_output['Cuisine_Style_number'] = df_output['Cuisine Style']\\\n                                        .apply(lambda x: 1 if type(x)==float and pd.isna(x)  else len(x))\n    \n    #encode price range + fill nan's by mean per city & create nan column\n    df_output['Price_Range_nan'] = pd.isna(df_output['Price Range']).astype('uint8')\n    df_output['New_Price_Range'] = df_output['Price Range'].map({'$$ - $$$':2, '$':1, '$$$$':3})\n    df_output['New_Price_Range'] = df_output.groupby(\"City\")['New_Price_Range']\\\n                                    .transform(lambda x: x.fillna(round(x.mean()))).astype(int)\n        \n    # id_ta \n    df_output['ID_TA_n'] = df_output.ID_TA.apply(lambda x: x[1:]).astype(str).astype(int)\n    df_output['ID_TA_dup'] = dup_col(df_output.ID_TA)\n    \n    # add review date \n    pat = re.compile('\\'\\d+\\\/\\d+\\\/\\d+\\'?')\n    df_output['Review_date'] = df_output.Reviews\\\n                                .apply(lambda x: [] if pd.isna(x) else re.findall(pat,x) )\\\n                                .apply(lambda y:[pd.to_datetime(item) for item in y ])\n    # add review date diff & rev date nan\n    df_output['Review_date_dif'] = df_output.Review_date.apply(lambda x: find_days(x))\n    df_output['Review_date_dif'].fillna(round(df_output['Review_date_dif'].mean()), inplace=True)\n    \n    df_output['last_rev_date'] = df_output.Review_date.apply(lambda x: np.nan if not x else max(x))\n    md = max(df_output['last_rev_date'].dropna())\n    df_output['last_rev_date'] = df_output['last_rev_date'].apply(lambda x: (md-x).days)\n    df_output['last_rev_date'].fillna(round(df_output['last_rev_date'].mean()), inplace=True)\n    \n    df_output['last_rev_date'] = np.log2(df_output['last_rev_date']+1)\n    df_output['Review_date_dif'] = np.log2(df_output['Review_date_dif']+1)\n    \n    df_output['rest_in_city']=df_output.groupby('City')['Restaurant_id'].transform('count')\n    df_output['norm_rest'] = df_output['rest_in_city']\/df_output['Ranking']\n    df_output['norm_rank'] = df_output['Ranking']\/df_output['rest_in_city']\n    \n    pf = PolynomialFeatures(2)\n    poly_features = pf.fit_transform(df_output[['rest_in_city', 'Ranking','Number of Reviews']])\n    poly_frame=pd.DataFrame(poly_features, columns=['c1','c2','c3','c4', 'c5', 'c6', 'c7', 'c8', 'c9', 'c10'])\n    df_output= pd.concat([df_output, poly_frame['c9']], axis=1)#.drop(['c1'], axis = 1, inplace=True)\n    \n    df_output['Cuisine_number_per_city'] = df_output.groupby('City')['Cuisine Style']\\\n                                            .transform(lambda x: len(np.unique(np.hstack(x))))\n    \n    df_output['Ranking_bin'] = np.where(df_output['Ranking'] > 5241, 3,\n                                np.where(df_output['Ranking'] < 972, 1, 2))\n    \n      \n    df_output['Ranking_max_norm'] = df_output['Ranking'] \/ df_output['City'].map(df_output.groupby(['City'])['Ranking'].max())\n    \n    df_output.drop(['Ranking','Number of Reviews'], axis = 1, inplace=True)\n    \n    # dummy City\n    # create City dummy variable \n    df_output = pd.get_dummies(df_output, columns=[ 'City',])\n    \n    # need to be done but not in test - takes a lot of time\n    df_output = pd.get_dummies(df_output, columns = ['Restaurant_id'])\n    \n    \n    # ################### 5. Clean #################################################### \n    # select number type columns\n    object_columns = [s for s in df_output.columns if df_output[s].dtypes == 'object']\n    df_output.drop(object_columns, axis = 1, inplace=True)\n    \n    return df_output","8b75ceb5":"df_preproc = preproc_data(data)\ndf_preproc.sample(10)\n","39d7d989":"df_preproc.info()","2a844264":"# \u0422\u0435\u043f\u0435\u0440\u044c \u0432\u044b\u0434\u0435\u043b\u0438\u043c \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0447\u0430\u0441\u0442\u044c\ntrain_data = df_preproc.query('sample == 1').drop(['sample'], axis=1)\ntest_data = df_preproc.query('sample == 0').drop(['sample'], axis=1)\n\ny = train_data.Rating.values            # \u043d\u0430\u0448 \u0442\u0430\u0440\u0433\u0435\u0442\nX = train_data.drop(['Rating'], axis=1)","440a92fc":"# \u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0435 train_test_split \u0434\u043b\u044f \u0440\u0430\u0437\u0431\u0438\u0432\u043a\u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\n# \u0432\u044b\u0434\u0435\u043b\u0438\u043c 20% \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e (\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 test_size)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)","3ebaf389":"# \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c\ntest_data.shape, train_data.shape, X.shape, X_train.shape, X_test.shape","0613591f":"# \u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438:\nfrom sklearn.ensemble import RandomForestRegressor # \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442 \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438\nfrom sklearn import metrics # \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u044b \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438 \u043c\u043e\u0434\u0435\u043b\u0438","e3197193":"# \u0421\u043e\u0437\u0434\u0430\u0451\u043c \u043c\u043e\u0434\u0435\u043b\u044c (\u041d\u0410\u0421\u0422\u0420\u041e\u0419\u041a\u0418 \u041d\u0415 \u0422\u0420\u041e\u0413\u0410\u0415\u041c)\nmodel = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state=RANDOM_SEED)","4183bab0":"# \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u043c \u043d\u0430\u0431\u043e\u0440\u0435 \u0434\u0430\u043d\u043d\u044b\u0445\nmodel.fit(X_train, y_train)\n\n# \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u0430 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u043e\u0432 \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435.\n# \u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u0432 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0443\u044e y_pred\ny_pred = model.predict(X_test)","2b0da42a":"# round predicted rating as we have 0.5 step\ny_pred = (y_pred * 2).round()\/2","863d297f":"# \u0421\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f (y_pred) \u0441 \u0440\u0435\u0430\u043b\u044c\u043d\u044b\u043c\u0438 (y_test), \u0438 \u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043e\u043d\u0438 \u0432 \u0441\u0440\u0435\u0434\u043d\u0435\u043c \u043e\u0442\u043b\u0438\u0447\u0430\u044e\u0442\u0441\u044f\n# \u041c\u0435\u0442\u0440\u0438\u043a\u0430 \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u0442\u0441\u044f Mean Absolute Error (MAE) \u0438 \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043e\u0442 \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445.\nprint('MAE:', metrics.mean_absolute_error(y_test, y_pred))","e8c2e2ae":"# \u0432 RandomForestRegressor \u0435\u0441\u0442\u044c \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u0432\u044b\u0432\u0435\u0441\u0442\u0438 \u0441\u0430\u043c\u044b\u0435 \u0432\u0430\u0436\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438\nplt.rcParams['figure.figsize'] = (10,10)\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(15).plot(kind='barh')","1c2d3286":"test_data.sample(10)","02ad1639":"test_data = test_data.drop(['Rating'], axis=1)","49a2b8a2":"sample_submission","72620f40":"# round predicted rating as we have 0.5 step\npredict_submission = model.predict(test_data)\npredict_submission = (predict_submission * 2).round()\/2","feacace1":"predict_submission","155f66d9":"sample_submission['Rating'] = predict_submission\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission.head(10)","cf1e27ff":"seems that Ranking & Number of Reviews have outliers","525bf4ac":"# Validation MAE: 0.158875","c62867d1":"according to the boxplot it seems that values higher than  5000 are real outliers","1830a595":"### Feature correlation (https:\/\/ru.wikipedia.org\/wiki\/\u041a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u044f)\nLook at the all features correlation ","07d24928":"we could see 3 main waves of ranking distribution \nso will try to add one more ranking normalization","89f8478b":"![](https:\/\/www.pata.org\/wp-content\/uploads\/2014\/09\/TripAdvisor_Logo-300x119.png)\n# Predict TripAdvisor Rating\n## \u0412 \u044d\u0442\u043e\u043c \u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u0438 \u043d\u0430\u043c \u043f\u0440\u0435\u0434\u0441\u0442\u043e\u0438\u0442 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u044c \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0430 \u0432\u00a0TripAdvisor\n**\u041f\u043e \u0445\u043e\u0434\u0443 \u0437\u0430\u0434\u0430\u0447\u0438:**\n* \u041f\u0440\u043e\u043a\u0430\u0447\u0430\u0435\u043c\u00a0\u0440\u0430\u0431\u043e\u0442\u0443 \u0441 pandas\n* \u041d\u0430\u0443\u0447\u0438\u043c\u0441\u044f \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441 Kaggle Notebooks\n* \u041f\u043e\u0439\u043c\u0435\u043c \u043a\u0430\u043a \u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0443 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\n* \u041d\u0430\u0443\u0447\u0438\u043c\u0441\u044f \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441 \u043f\u0440\u043e\u043f\u0443\u0449\u0435\u043d\u043d\u044b\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 (Nan)\n* \u041f\u043e\u0437\u043d\u0430\u043a\u043e\u043c\u0438\u043c\u0441\u044f \u0441 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u043c\u0438 \u0432\u0438\u0434\u0430\u043c\u0438 \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n* \u041d\u0435\u043c\u043d\u043e\u0433\u043e \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c\u00a0[Feature Engineering](https:\/\/ru.wikipedia.org\/wiki\/\u041a\u043e\u043d\u0441\u0442\u0440\u0443\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435_\u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432) (\u0433\u0435\u043d\u0435\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043d\u043e\u0432\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438)\n* \u0418 \u0441\u043e\u0432\u0441\u0435\u043c \u043d\u0435\u043c\u043d\u043e\u0433\u043e \u0437\u0430\u0442\u0440\u043e\u043d\u0435\u043c ML\n* \u0418 \u043c\u043d\u043e\u0433\u043e\u0435 \u0434\u0440\u0443\u0433\u043e\u0435...   \n\n\n\n### \u0418 \u0441\u0430\u043c\u043e\u0435 \u0432\u0430\u0436\u043d\u043e\u0435, \u0432\u0441\u0435 \u044d\u0442\u043e \u0432\u044b \u0441\u043c\u043e\u0436\u0435\u0442\u0435 \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0441\u0430\u043c\u043e\u0441\u0442\u043e\u044f\u0442\u0435\u043b\u044c\u043d\u043e!\n\n*\u042d\u0442\u043e\u0442 \u041d\u043e\u0443\u0442\u0431\u0443\u043a \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u0441\u044f \u041f\u0440\u0438\u043c\u0435\u0440\u043e\u043c\/\u0428\u0430\u0431\u043b\u043e\u043d\u043e\u043c \u043a \u044d\u0442\u043e\u043c\u0443 \u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u044e (Baseline) \u0438 \u043d\u0435 \u0441\u043b\u0443\u0436\u0438\u0442 \u0433\u043e\u0442\u043e\u0432\u044b\u043c \u0440\u0435\u0448\u0435\u043d\u0438\u0435\u043c!*   \n\u0412\u044b \u043c\u043e\u0436\u0435\u0442\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0435\u0433\u043e \u043a\u0430\u043a \u043e\u0441\u043d\u043e\u0432\u0443 \u0434\u043b\u044f \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u0441\u0432\u043e\u0435\u0433\u043e \u0440\u0435\u0448\u0435\u043d\u0438\u044f.\n\n> \u0447\u0442\u043e \u0442\u0430\u043a\u043e\u0435 baseline \u0440\u0435\u0448\u0435\u043d\u0438\u0435, \u0437\u0430\u0447\u0435\u043c \u043e\u043d\u043e \u043d\u0443\u0436\u043d\u043e \u0438 \u043f\u043e\u0447\u0435\u043c\u0443 \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0442\u044c baseline \u043a \u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u044e \u0441\u0442\u0430\u043b\u043e \u0432\u0430\u0436\u043d\u044b\u043c \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043e\u043c \u043d\u0430 kaggle \u0438 \u0434\u0440\u0443\u0433\u0438\u0445 \u043f\u043b\u043e\u0449\u0430\u0434\u043a\u0430\u0445.   \n**baseline** \u0441\u043e\u0437\u0434\u0430\u0435\u0442\u0441\u044f \u0431\u043e\u043b\u044c\u0448\u0435 \u043a\u0430\u043a \u0448\u0430\u0431\u043b\u043e\u043d, \u0433\u0434\u0435 \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043a\u0430\u043a \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442 \u043e\u0431\u0440\u0430\u0449\u0435\u043d\u0438\u0435 \u0441 \u0432\u0445\u043e\u0434\u044f\u0449\u0438\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438 \u0447\u0442\u043e \u043d\u0443\u0436\u043d\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043d\u0430 \u0432\u044b\u0445\u043e\u0434\u0435. \u041f\u0440\u0438 \u044d\u0442\u043e\u043c \u041c\u041b \u043d\u0430\u0447\u0438\u043d\u043a\u0430 \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u043f\u0440\u043e\u0441\u0442\u043e\u0439, \u043f\u0440\u043e\u0441\u0442\u043e \u0434\u043b\u044f \u043f\u0440\u0438\u043c\u0435\u0440\u0430. \u042d\u0442\u043e \u043f\u043e\u043c\u043e\u0433\u0430\u0435\u0442 \u0431\u044b\u0441\u0442\u0440\u0435\u0435 \u043f\u0440\u0438\u0441\u0442\u0443\u043f\u0438\u0442\u044c \u043a \u0441\u0430\u043c\u043e\u043c\u0443 \u041c\u041b, \u0430 \u043d\u0435 \u0442\u0440\u0430\u0442\u0438\u0442\u044c \u0446\u0435\u043d\u043d\u043e\u0435 \u0432\u0440\u0435\u043c\u044f \u043d\u0430 \u0447\u0438\u0441\u0442\u043e \u0438\u043d\u0436\u0435\u043d\u0435\u0440\u043d\u044b\u0435 \u0437\u0430\u0434\u0430\u0447\u0438. \n\u0422\u0430\u043a\u0436\u0435 baseline \u044f\u0432\u043b\u044f\u0435\u0442\u044c\u0441\u044f \u0445\u043e\u0440\u043e\u0448\u0435\u0439 \u043e\u043f\u043e\u0440\u043d\u043e\u0439 \u0442\u043e\u0447\u043a\u043e\u0439 \u043f\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0435. \u0415\u0441\u043b\u0438 \u0442\u0432\u043e\u0435 \u0440\u0435\u0448\u0435\u043d\u0438\u0435 \u0445\u0443\u0436\u0435 baseline - \u0442\u044b \u044f\u0432\u043d\u043e \u0434\u0435\u043b\u0430\u0435\u0448\u044c \u0447\u0442\u043e-\u0442\u043e \u043d\u0435 \u0442\u043e \u0438 \u0441\u0442\u043e\u0438\u0442 \u043f\u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u0442\u044c \u0434\u0440\u0443\u0433\u043e\u0439 \u043f\u0443\u0442\u044c) \n\n\u0412 \u043a\u043e\u043d\u0442\u0435\u043a\u0441\u0442\u0435 \u043d\u0430\u0448\u0435\u0433\u043e \u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u044f baseline \u0438\u0434\u0435\u0442 \u0441 \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u0438\u043c\u0438 \u043f\u0440\u0438\u043c\u0435\u0440\u0430\u043c\u0438 \u0442\u043e\u0433\u043e, \u0447\u0442\u043e \u043c\u043e\u0436\u043d\u043e \u0434\u0435\u043b\u0430\u0442\u044c \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438, \u0438 \u0441 \u0438\u043d\u0441\u0442\u0440\u0443\u043a\u0446\u0438\u0435\u0439, \u0447\u0442\u043e \u0434\u0435\u043b\u0430\u0442\u044c \u0434\u0430\u043b\u044c\u0448\u0435, \u0447\u0442\u043e\u0431\u044b \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442.  \u0412\u043e\u043e\u0431\u0449\u0435 \u0433\u043e\u0442\u043e\u0432\u044b\u043c \u0440\u0435\u0448\u0435\u043d\u0438\u0435\u043c \u044d\u0442\u043e \u0441\u043b\u043e\u0436\u043d\u043e \u043d\u0430\u0437\u0432\u0430\u0442\u044c, \u0442\u0430\u043a \u043a\u0430\u043a \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442\u0441\u044f \u0432\u0441\u0435\u0433\u043e 2 \u0441\u0430\u043c\u044b\u0445 \u043f\u0440\u043e\u0441\u0442\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 (\u0430 \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u0435 \u0438\u0441\u043a\u043b\u044e\u0447\u0430\u044e\u0442\u0441\u044f).","b131dd02":"look at the ranking IQR","e2990539":"#### \u0417\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u0438 \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c \u0447\u0442\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u043e\u0441\u044c","b32a217e":"### ID_TA","5f2bb6bb":"![](https:\/\/cs10.pikabu.ru\/post_img\/2018\/09\/06\/11\/1536261023140110012.jpg)","fe087b2d":"### Prepare data to run ","88d81076":"### Reviews\nI worked only with Review dates","d6685aa9":"# Conclusions:\n1. Improved pandas skills\n2. Tried working in kaggle notebook\n3. Improved preprocessing skills\n4. Tried Feature Ingineering\n5. Tried Feature encoding","a4aca581":"### \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439","515f361c":"1. There is no highly correlated with Rating features\n2. Features don't highly correlate between each other","240146d7":"### Cuisine Style","adb457ea":"### 1. \u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 NAN  \u0438 \u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n\u0423 \u043d\u0430\u043b\u0438\u0447\u0438\u044f \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432 \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u0440\u0430\u0437\u043d\u044b\u0435 \u043f\u0440\u0438\u0447\u0438\u043d\u044b, \u043d\u043e \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0438 \u043d\u0443\u0436\u043d\u043e \u043b\u0438\u0431\u043e \u0437\u0430\u043f\u043e\u043b\u043d\u0438\u0442\u044c, \u043b\u0438\u0431\u043e \u0438\u0441\u043a\u043b\u044e\u0447\u0438\u0442\u044c \u0438\u0437 \u043d\u0430\u0431\u043e\u0440\u0430 \u043f\u043e\u043b\u043d\u043e\u0441\u0442\u044c\u044e. \u041d\u043e \u0441 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0430\u043c\u0438 \u043d\u0443\u0436\u043d\u043e \u0431\u044b\u0442\u044c \u0432\u043d\u0438\u043c\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u043c, **\u0434\u0430\u0436\u0435 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0432\u0430\u0436\u043d\u044b\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u043c!**   \n\u041f\u043e \u044d\u0442\u043e\u043c\u0443 \u043f\u0435\u0440\u0435\u0434 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u043e\u0439 NAN \u043b\u0443\u0447\u0448\u0435 \u0432\u044b\u043d\u0435\u0441\u0442\u0438 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043e \u043d\u0430\u043b\u0438\u0447\u0438\u0438 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0430 \u043a\u0430\u043a \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a ","f94b0834":"We have nan values only in Cuisine style, price range, number of reviews. If look deeper in review - there we also have nan values\n","436db3b9":"**\u041f\u0435\u0440\u0435\u0434 \u0442\u0435\u043c \u043a\u0430\u043a \u043e\u0442\u043f\u0440\u0430\u0432\u043b\u044f\u0442\u044c \u043d\u0430\u0448\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435, \u0440\u0430\u0437\u0434\u0435\u043b\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 \u0435\u0449\u0435 \u043e\u0434\u0438\u043d \u0442\u0435\u0441\u0442 \u0438 \u0442\u0440\u0435\u0439\u043d, \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438. \n\u042d\u0442\u043e \u043f\u043e\u043c\u043e\u0436\u0435\u0442 \u043d\u0430\u043c \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u043a\u0430\u043a \u0445\u043e\u0440\u043e\u0448\u043e \u043d\u0430\u0448\u0430 \u043c\u043e\u0434\u0435\u043b\u044c \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442, \u0434\u043e \u043e\u0442\u043f\u0440\u0430\u0432\u043a\u0438 submissiona \u043d\u0430 kaggle.**","763cad8e":"# DATA","e871f855":"# Functions\n","e0b309b1":"### City & Restaurant_id dummy variables","c1269771":"### \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430","74d3858f":"### Price Range","92f6f523":"# Model \n\u0421\u0430\u043c ML","7a5efe0a":"we could see that ranking depends on city, as different cities have different number of restaurants","e083eb58":"\u0423 \u043d\u0430\u0441 \u043c\u043d\u043e\u0433\u043e \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0435 \u0434\u043e\u0442\u044f\u0433\u0438\u0432\u0430\u044e\u0442 \u0438 \u0434\u043e 2500 \u043c\u0435\u0441\u0442\u0430 \u0432 \u0441\u0432\u043e\u0435\u043c \u0433\u043e\u0440\u043e\u0434\u0435, \u0430 \u0447\u0442\u043e \u0442\u0430\u043c \u043f\u043e \u0433\u043e\u0440\u043e\u0434\u0430\u043c?","3b6e6dcf":"they have normal distribution","7e7a6b8f":"\u041f\u043e \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u044e 'Price Range' \u044d\u0442\u043e - \u0426\u0435\u043d\u044b \u0432 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0435.  \n\u0418\u0445 \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u0441\u0442\u0430\u0432\u0438\u0442\u044c \u043f\u043e \u0432\u043e\u0437\u0440\u0430\u0441\u0442\u0430\u043d\u0438\u044e (\u0437\u043d\u0430\u0447\u0438\u0442 \u044d\u0442\u043e \u043d\u0435 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a). \u0410 \u044d\u0442\u043e \u0437\u043d\u0430\u0447\u0438\u0442, \u0447\u0442\u043e \u0438\u0445 \u043c\u043e\u0436\u043d\u043e \u0437\u0430\u043c\u0435\u043d\u0438\u0442\u044c \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u043c\u0438 \u0447\u0438\u0441\u043b\u0430\u043c\u0438, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 1,2,3  \n","71dd0806":"### Number of Reviews","1915a450":"# EDA \n[Exploratory Data Analysis](https:\/\/ru.wikipedia.org\/wiki\/\u0420\u0430\u0437\u0432\u0435\u0434\u043e\u0447\u043d\u044b\u0439_\u0430\u043d\u0430\u043b\u0438\u0437_\u0434\u0430\u043d\u043d\u044b\u0445) - \u0410\u043d\u0430\u043b\u0438\u0437 \u0434\u0430\u043d\u043d\u044b\u0445\n\u041d\u0430 \u044d\u0442\u043e\u043c \u044d\u0442\u0430\u043f\u0435 \u043c\u044b \u0441\u0442\u0440\u043e\u0438\u043c \u0433\u0440\u0430\u0444\u0438\u043a\u0438, \u0438\u0449\u0435\u043c \u0437\u0430\u043a\u043e\u043d\u043e\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438, \u0430\u043d\u043e\u043c\u0430\u043b\u0438\u0438, \u0432\u044b\u0431\u0440\u043e\u0441\u044b \u0438\u043b\u0438 \u0441\u0432\u044f\u0437\u0438 \u043c\u0435\u0436\u0434\u0443 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c\u0438.\n\u0412 \u043e\u0431\u0449\u0435\u043c \u0446\u0435\u043b\u044c \u044d\u0442\u043e\u0433\u043e \u044d\u0442\u0430\u043f\u0430 \u043f\u043e\u043d\u044f\u0442\u044c, \u0447\u0442\u043e \u044d\u0442\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 \u043c\u043e\u0433\u0443\u0442 \u043d\u0430\u043c \u0434\u0430\u0442\u044c \u0438 \u043a\u0430\u043a \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u0432\u0437\u0430\u0438\u043c\u043e\u0441\u0432\u044f\u0437\u0430\u043d\u044b \u043c\u0435\u0436\u0434\u0443 \u0441\u043e\u0431\u043e\u0439.\n\u041f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u0438\u0437\u043d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043f\u043e\u0437\u0432\u043e\u043b\u0438\u0442 \u0441\u0433\u0435\u043d\u0435\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043d\u043e\u0432\u044b\u0435, \u0431\u043e\u043b\u0435\u0435 \u0441\u0438\u043b\u044c\u043d\u044b\u0435 \u0438, \u0442\u0435\u043c \u0441\u0430\u043c\u044b\u043c, \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043d\u0430\u0448\u0443 \u043c\u043e\u0434\u0435\u043b\u044c \u043b\u0443\u0447\u0448\u0435.\n![](https:\/\/miro.medium.com\/max\/2598\/1*RXdMb7Uk6mGqWqPguHULaQ.png)","79a7c813":"# Cleaning and Prepping Data\n\u041e\u0431\u044b\u0447\u043d\u043e \u0434\u0430\u043d\u043d\u044b\u0435 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442 \u0432 \u0441\u0435\u0431\u0435 \u043a\u0443\u0447\u0443 \u043c\u0443\u0441\u043e\u0440\u0430, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043f\u043e\u0447\u0438\u0441\u0442\u0438\u0442\u044c, \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u043f\u0440\u0438\u0432\u0435\u0441\u0442\u0438 \u0438\u0445 \u0432 \u043f\u0440\u0438\u0435\u043c\u043b\u0435\u043c\u044b\u0439 \u0444\u043e\u0440\u043c\u0430\u0442. \u0427\u0438\u0441\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u2014 \u044d\u0442\u043e \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0439 \u044d\u0442\u0430\u043f \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u043f\u043e\u0447\u0442\u0438 \u043b\u044e\u0431\u043e\u0439 \u0440\u0435\u0430\u043b\u044c\u043d\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0438.   \n![](https:\/\/analyticsindiamag.com\/wp-content\/uploads\/2018\/01\/data-cleaning.png)","a46137b1":"\u041f\u043e\u0434\u0440\u043e\u0431\u043d\u0435\u0435 \u043f\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c:\n* City: \u0413\u043e\u0440\u043e\u0434 \n* Cuisine Style: \u041a\u0443\u0445\u043d\u044f\n* Ranking: \u0420\u0430\u043d\u0433 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0430 \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0434\u0440\u0443\u0433\u0438\u0445 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u043e\u0432 \u0432 \u044d\u0442\u043e\u043c \u0433\u043e\u0440\u043e\u0434\u0435\n* Price Range: \u0426\u0435\u043d\u044b \u0432 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0435 \u0432 3 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044f\u0445\n* Number of Reviews: \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0442\u0437\u044b\u0432\u043e\u0432\n* Reviews: 2 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0445 \u043e\u0442\u0437\u044b\u0432\u0430 \u0438 \u0434\u0430\u0442\u044b \u044d\u0442\u0438\u0445 \u043e\u0442\u0437\u044b\u0432\u043e\u0432\n* URL_TA: \u0441\u0442\u0440\u0430\u043d\u0438\u0446\u0430 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0430 \u043d\u0430 'www.tripadvisor.com' \n* ID_TA: ID \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0430 \u0432 TripAdvisor\n* Rating: \u0420\u0435\u0439\u0442\u0438\u043d\u0433 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0430","55b829e8":"# Data Preprocessing\nWill write everything in one function to make preprocessing faster","aa9140fd":"# import","e46c9849":"### Get polynomial features\n","d6f5b276":"# Submission\n\u0415\u0441\u043b\u0438 \u0432\u0441\u0435 \u0443\u0441\u0442\u0440\u0430\u0435\u0432\u0430\u0435\u0442 - \u0433\u043e\u0442\u043e\u0432\u0438\u043c Submission \u043d\u0430 \u043a\u0430\u0433\u043b","71eb7922":"### Ranking"}}