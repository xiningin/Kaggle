{"cell_type":{"c44e1c91":"code","2b2040b2":"code","ba009966":"code","a0a18a56":"code","bf91aad6":"code","5f81e0bd":"code","daa26054":"code","de9680f0":"code","13c1b5b4":"code","ee06afc4":"code","70e19d8f":"code","c4a49c54":"code","f72016c6":"code","2664cc1e":"code","882c0628":"code","bac30532":"code","48cadc3b":"code","d9b2a3de":"code","2ac6628f":"code","628e66f0":"code","efa673cf":"code","ff22e0b9":"code","b7ec3585":"code","253f7fd9":"code","d15af3e5":"code","7dfdf93c":"code","622aa74d":"code","e568d7b7":"code","85a2350b":"code","842e7caa":"code","b75defad":"code","12d03918":"code","f257f5d5":"markdown","9cf28644":"markdown","f1f2f45f":"markdown","3b078203":"markdown"},"source":{"c44e1c91":"# !pip install -q tensorflow_ranking\n# import tensorflow_ranking as tfr\n\n# Sync Notebook with VS Code\n!git clone https:\/\/github.com\/sarthak-314\/fast-nlp\nimport sys; sys.path.append('fast-nlp')\n\nfrom src import *\nfrom src.tflow import *","2b2040b2":"WEIGHTS_DIR = Path('\/kaggle\/input\/toxic-monster-model-internet\/')\n\nTOXIC_FEATURES = [\n    'severe_toxic', 'identity_attack', 'threat', \n    'toxic', 'insult', 'obscene', \n]","ba009966":"def build_scorer_model(backbone):\n    input_ids = tf.keras.Input((128,), dtype=tf.int32)\n    attention_mask = tf.keras.Input((128,), dtype=tf.int32)\n    \n    backbone_outputs = backbone(\n        input_ids=input_ids, \n        attention_mask=attention_mask, \n        return_dict=True, \n    )\n    x = backbone_outputs.pooler_output\n    score_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n    return tf.keras.Model([input_ids, attention_mask], outputs=score_layer(x))","a0a18a56":"def bert_init(self, initializer_range=0.02): \n    return tf.keras.initializers.TruncatedNormal(stddev=initializer_range)    \n            \ndef build_hidden_layer(hidden_layer_units=[], hidden_dropout=0.10, name='hidden_layer'): \n    if not hidden_layer_units: \n        return lambda x: x\n    hidden_layers = []\n    for units in hidden_layer_units: \n        hidden_layers.append(tf.keras.layers.Dropout(hidden_dropout))\n        hidden_layers.append(tf.keras.layers.Dense(\n            units=units, \n            activation=tfa.activations.mish, \n            kernel_initializer=bert_init(0.02),\n        ))\n    return tf.keras.Sequential(hidden_layers, name=name)","bf91aad6":"# TODO: Add more different models if this works. ","5f81e0bd":"%%hyperparameters \n\nmax_seq_len: 128\nrepresentation_hidden_layers: [1024]\n\ninteraction_hidden_layers: [256, 64, 16, 4]\nhidden_dropout: 0.10\n    \nbatch_size: 128\nlr: 1e-2\n    \nfold: 3","daa26054":"!wandb login '00dfbb85e215726cccc6f6f9ed40e618a7cf6539'\nSTRATEGY = tf_accelerator(bfloat16=False, jit_compile=False)","de9680f0":"def build_test_ds(df, tokenizer): \n    raw_dataset = datasets.Dataset.from_pandas(df)\n    dataset = raw_dataset.map(\n        lambda ex: tokenizer(ex['comment_text'], max_length=HP.max_seq_len, padding='max_length', truncation=True), \n        batched=True, num_proc=4, \n    )\n    dataset.set_format(type='numpy')\n    id_ds = tf.data.Dataset.from_tensor_slices(dataset['input_ids'].astype(np.int32))\n    mask_ds = tf.data.Dataset.from_tensor_slices(dataset['attention_mask'].astype(np.int32))\n    \n    ds = tf.data.Dataset.zip((id_ds, mask_ds))\n    ds = tf.data.Dataset.zip((ds, ds))\n    return ds.batch(2048).prefetch(tf.data.AUTOTUNE)\n\n\ndef build_cross_encoder_df(df):\n    df1, df2 = df.copy(), df.copy()\n    df1['A_comment'], df1['B_comment'], df1['y'] = df.more_toxic, df.less_toxic, 1.0\n    df2['A_comment'], df2['B_comment'], df2['y'] = df.less_toxic, df.more_toxic, 0.0\n    return pd.concat([df1, df2])\n\ndef df_to_tfds(df, is_valid=False): \n    A_indices = df.A_comment.str.strip().map(comment_text_to_id).values\n    B_indices = df.B_comment.str.strip().map(comment_text_to_id).values\n    \n    embeddings_ds = []\n    for embed in backbone_embeddings:\n        embeddings_ds.append(tf.data.Dataset.from_tensor_slices(embed[A_indices]))\n        embeddings_ds.append(tf.data.Dataset.from_tensor_slices(embed[B_indices]))\n    \n    input_ds = tf.data.Dataset.zip(tuple(embeddings_ds))\n    label_ds = tf.data.Dataset.from_tensor_slices(df.y.values)\n    ds = tf.data.Dataset.zip((input_ds, label_ds))\n    \n    if not is_valid: \n        ds = ds.shuffle(len(df), reshuffle_each_iteration=True).repeat()\n    ds = ds.batch(HP.batch_size)\n    if is_valid: \n        ds = ds.cache()\n    steps = len(df)\/\/HP.batch_size\n    return ds.prefetch(tf.data.AUTOTUNE), steps","13c1b5b4":"df = pd.read_csv('\/kaggle\/input\/toxic-dataframes\/valid.csv')\nold = pd.read_csv('..\/input\/toxic-public-dataframes\/old_pseudo_label.csv')\nmanual = pd.read_csv('..\/input\/toxic-dataframes\/manual_pair_labels.csv')\ndf = pd.concat([df, manual])\ncomments = np.unique(np.concatenate([df.more_toxic.values, df.less_toxic.values]))\ndfc = pd.DataFrame({'comment_text': comments})\n\nmanual['fold'] = -1\ntrain, valid = df[df.fold!=HP.fold], df[df.fold==HP.fold]\ntrain = pd.concat([train, manual]).sample(frac=1.)\n\ntrain, valid = build_cross_encoder_df(train), build_cross_encoder_df(valid)","ee06afc4":"# backbone_embeddings, backbone_hidden_dims = [], []\n# def temp_model(backbone): \n#     input_ids = tf.keras.Input((HP.max_seq_len,), dtype=tf.int32, name='input_ids')\n#     attention_mask = tf.keras.Input((HP.max_seq_len,), dtype=tf.int32, name='attention_mask')\n#     backbone_outputs = backbone(\n#         input_ids=input_ids, \n#         attention_mask=attention_mask, \n#         return_dict=True, \n#     )\n#     x = backbone_outputs.pooler_output\n#     return tf.keras.Model([input_ids, attention_mask], outputs=x)\n\n# def predict(ds, backbone): \n#     preds = temp_model(backbone).predict(ds, verbose=1)\n#     return np.squeeze(preds)\n\n# with STRATEGY.scope(): \n#     for backbone_name, weights in tqdm(zip(BACKBONE_NAMES, WEIGHTS), total=len(WEIGHTS)): \n#         folder = f'\/kaggle\/input\/toxic-internet-deep-model-backbones\/{backbone_name}\/'\n#         weights_file = f'\/kaggle\/input\/toxic-monster-model-internet\/{weights}'\n#         backbone = TFAutoModel.from_pretrained(folder)\n#         tokenizer = AutoTokenizer.from_pretrained(folder)\n#         scorer = build_scorer_model(backbone)\n#         scorer.load_weights(weights_file)\n#         test_ds =  build_test_ds(dfc, tokenizer)\n#         preds = predict(test_ds, backbone)\n        \n#         backbone_embeddings.append(preds)\n#         backbone_hidden_dims.append(backbone.config.hidden_size)\n        \n# del backbone, tokenizer; gc.collect()","70e19d8f":"# TODO: Store embeddings of all the strong models in a seprate notebook. Use this for concat\n\nWEIGHTS = [\n    'old_pseudo_label.h5', # LB: 0.827 (Roberta Base)\n    'robertal_old_pseudo_ep25_val77.h5', # LB: 0.819 (Roberta Large)\n    'bertweet_old_pseudo.h5', # LB: 0.817 (BerTweet Large)\n    'old_maxval774_val77_robertaltox_scorer.h5', # LB: 0.808 (ToxBERT)\n    'nolapval755_oldonlyv2_robertab_17thDec.h5', # LB: 0.791 (Old Only Paired)\n    # 'ruddit_val731_robertab.h5', # LB: 0.785 (Ruddit Roberta Base)\n]\n\nBACKBONE_NAMES = [\n    'roberta_base', \n    'roberta_large', \n    'bertweet_large', \n    'roberta_large', \n    'roberta_base', \n    # 'roberta_base', \n]\n\n# Load Embeddings\nbackbone_embeddings, backbone_hidden_dims = [], []\nfor weight_file, backbone_name in tqdm(zip(WEIGHTS, BACKBONE_NAMES), total=len(WEIGHTS)):\n    npy_file = weight_file.replace('.h5', '.npy')\n    npy_file = f'\/kaggle\/input\/toxic-embedding-extractor\/{npy_file}'\n    embeddings = np.load(npy_file)\n    backbone_embeddings.append(embeddings)\n    \n    hidden_dim = 1024 if 'large' in backbone_name else 768\n    backbone_hidden_dims.append(hidden_dim)\n    \n    \ncomment_text_to_id = {comment_text.strip(): idx for idx, comment_text in enumerate(dfc.comment_text.values)}\ntrain_ds, train_steps = df_to_tfds(train, is_valid=False)\nvalid_ds, valid_steps = df_to_tfds(valid, is_valid=True)","c4a49c54":"# Model Training Callbacks \naccuracy_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    'checkpoint_acc.h5', monitor='val_accuracy', mode='max', save_weights_only=True, save_best_only=True, verbose=1\n)\nloss_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    'checkpoint_loss.h5', monitor='val_loss', mode='min', save_weights_only=True, save_best_only=True, verbose=1\n)\nreduce_lr_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='loss', factor=0.75, patience=1, verbose=True, mode='min', min_lr=0, \n)\n\n# 0.77445 13th December\nHP.hidden_dropout = 0.10\nHP.representation_hidden_layers = []\nHP.interaction_hidden_layers = [1024, 256, 64, 16, 4]\nHP.skip_connection = False\n\nwith STRATEGY.scope(): \n    model = quick_compile_model(1e-3)\nhistory = model.fit(\n    train_ds, steps_per_epoch=train_steps, epochs=100, \n    validation_data=valid_ds, validation_steps=valid_steps, \n    callbacks=[reduce_lr_on_plateau, accuracy_checkpoint],\n    # verbose=0\n)\n# model.save_weights('freeze_tuned_model.h5')","f72016c6":"# 0.767 with all\n# 0.776 without last \n# 0.773 without bertweet. \n# 0.775 withot toxbert\n# 0.777 without ruddit","2664cc1e":"def build_model(): \n    model_inputs = []\n    A_embeddings, B_embeddings = [], []\n    for i, backbone_hidden_dim in enumerate(backbone_hidden_dims): \n        A_embedding = tf.keras.Input((backbone_hidden_dim,), dtype=tf.float32, name=f'A_{i}')\n        B_embedding = tf.keras.Input((backbone_hidden_dim,), dtype=tf.float32, name=f'B_{i}')\n        model_inputs.append(A_embedding)\n        model_inputs.append(B_embedding)\n        A_embeddings.append(A_embedding)\n        B_embeddings.append(B_embedding)\n    \n    R = build_hidden_layer(HP.representation_hidden_layers, HP.hidden_dropout, name='repr_hidden_layer')\n    A_x = tf.concat(A_embeddings, axis=-1)\n    A_x = R(A_x)\n    B_x = tf.concat(B_embeddings, axis=-1)\n    B_x = R(B_x)\n    \n    x = tf.concat([A_x, B_x], axis=-1)\n    inter_hidden_layer = build_hidden_layer(\n        HP.interaction_hidden_layers, HP.hidden_dropout, name='inter_hidden_layer'\n    )\n    if HP.skip_connection: \n        x = tf.concat([x, inter_hidden_layer(x)], axis=-1)\n    else: \n        x = inter_hidden_layer(x)\n    \n    score_layer = tf.keras.layers.Dense(1, activation='sigmoid', name='y')\n    x = score_layer(x)\n    return tf.keras.Model(model_inputs, outputs=x)\n    \ndef loss_fn(y_true, y_pred):\n    func = tf.keras.losses.BinaryCrossentropy(reduction=tf.losses.Reduction.SUM)\n#     func = tfr.keras.losses.PairwiseHingeLoss(temperature=1.0, reduction=tf.losses.Reduction.SUM)\n#     func = tfr.keras.losses.PairwiseSoftZeroOneLoss(temprature=1.0, reduction=tf.losses.Reduction.SUM)\n#     func = tfr.keras.losses.PairwiseLogisticLoss(temperature=1.0, reduction=tf.losses.Reduction.SUM)\n    return func(y_true, y_pred)\n\ndef get_compiled_model(): \n    with STRATEGY.scope(): \n        model = build_model()\n        lr_scheduler = lr_scheduler_factory(HP.warmup_epochs, HP.warmup_power, HP.lr_cosine_decay, train_steps)        \n        optimizer = optimizer_factory(HP.optimizer, lr_scheduler)\n        model.compile(\n            optimizer=optimizer, \n            loss=loss_fn, \n            metrics='accuracy', \n            run_eagerly=HARDWARE == 'CPU',\n            steps_per_execution=1024,\n        )    \n    return model\n        \ndef quick_compile_model(lr=1e-2): \n    with STRATEGY.scope(): \n        model = build_model()\n        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n        model.compile(\n            optimizer=optimizer, \n            loss=loss_fn, \n            metrics='accuracy', \n            steps_per_execution=1024, \n        )\n    return model","882c0628":"def build_representation_model(model): \n    comment_embeddings = []\n    for backbone_dim in backbone_hidden_dims: \n        x = tf.keras.Input((backbone_dim,), dtype=tf.float32)\n        comment_embeddings.append(x)\n    \n    # R = model.get_layer('repr_hidden_layer')\n    x = tf.concat(comment_embeddings, axis=-1)\n    # x = R(x)\n    return tf.keras.Model(comment_embeddings, outputs=x)\n\ndef build_interaction_model(model): \n    # x_dim = HP.representation_hidden_layers[-1]\n    x_dim = sum(backbone_hidden_dims)\n    \n    A_input = tf.keras.Input((x_dim,), dtype=tf.float32)\n    B_input = tf.keras.Input((x_dim,), dtype=tf.float32)\n    x = tf.concat([A_input, B_input], axis=-1)\n    \n    inter_hidden_layer = model.get_layer('inter_hidden_layer')\n    score_layer = model.get_layer('y')\n    if HP.skip_connection: \n        x = tf.concat([x, inter_hidden_layer(x)], axis=-1)\n    else: \n        x = inter_hidden_layer(x)\n    x = score_layer(x)\n    return tf.keras.Model([A_input, B_input], outputs=x)\n\n\nwith STRATEGY.scope(): \n    model.load_weights('checkpoint_acc.h5')\n    \n    repr_model = build_representation_model(model)\n    repr_model.save_weights('ensemble_repr.h5')\n    \n    inter_model = build_interaction_model(model)\n    inter_model.save_weights('ensemble_inter.h5')\n    \n    # tokenizer.save_pretrained('old_pseudo_tokenizer')","bac30532":"# with STRATEGY.scope(): \n#     model.load_weights('checkpoint_acc.h5')\n#     backbone.save_weights('old_pseudo_lb827_backbone.h5')\n#     model.get_layer('repr_hidden_layer').save_weights('old_pseudo_repr.h5')\n#     model.get_layer('inter_hidden_layer').save_weights('old_pseudo_inter.h5')\n#     model.get_layer('y').save_weights('old_pseudo_y.h5')","48cadc3b":"HP.backbone = 'roberta-large'\nHP.attention_dropout = 0.00\nHP.weights = 'comp_only_fold3_val737_robertal_sentiment_10thDec.h5'\n\nHP.representation_hidden_layers = []\nHP.interaction_hidden_layers = [128, 32, 8, 2]\nHP.hidden_dropout = 0.75","d9b2a3de":"with STRATEGY.scope(): \n    backbone = TFAutoModel.from_pretrained(\n        'roberta-large', \n        attention_probs_dropout_prob=HP.attention_dropout, \n        from_pt=HP.from_pytorch, \n    )\n    model = build_scorer_model(backbone)\n    model.load_weights(WEIGHTS_DIR\/HP.weights)\n    \ntokenizer = AutoTokenizer.from_pretrained(HP.backbone)","2ac6628f":"with STRATEGY.scope(): \n    backbone.trainable = False\n    model = quick_compile_model()\nhistory = model.fit(\n    freeze_train_ds, steps_per_epoch=freeze_train_steps, epochs=100, \n    validation_data=valid_ds, validation_steps=valid_steps, \n    callbacks=[reduce_lr_on_plateau],\n)\nmodel.save_weights('freeze_tuned_model.h5')","628e66f0":"with STRATEGY.scope(): \n    backbone.trainable = True\n    model = get_compiled_model()\n    model.load_weights('freeze_tuned_model.h5')\nhistory = model.fit(\n    train_ds, steps_per_epoch=train_steps\/\/HP.checkpoints_per_epoch+1, \n    epochs=HP.max_epochs*HP.checkpoints_per_epoch, \n    validation_data=valid_ds, validation_steps=valid_steps, \n    callbacks=[accuracy_checkpoint, loss_checkpoint],\n)","efa673cf":"with STRATEGY.scope(): \n    model.load_weights('checkpoint_acc.h5')\n    \n    repr_model = build_representation_model(model, backbone)\n    repr_model.save_weights('old_pseudo_repr.h5')\n    \n    inter_model = build_interaction_model(model)\n    inter_model.save_weights('old_pseudo_inter.h5')d","ff22e0b9":"# microsoft\/deberta-v2-xlarge, google\/electra-large-discriminator, Xuhui\/ToxDect-roberta-large\n# vinai\/bertweet-large, studio-ousia\/luke-large","b7ec3585":"%%hyperparameters \n\n## Model Architecture ## \nbackbone: roberta-base\nattention_dropout: 0.10\nfrom_pytorch: False\nmax_seq_len: 128\n\nrepresentation_hidden_layers: []\ninteraction_hidden_layers: [1024, 256, 64, 16, 4]\nhidden_dropout: 0.25\n\nweights: 'ruddit_val731_robertab.h5'\n\n## LR Scheduler ## \nlr_cosine_decay: \n    max_lr: 32e-6\n    lr_gamma: 0.5\n    decay_epochs: 0.75\n    min_lr: 0\n    step_gamma: 2\nwarmup_epochs: 0.5\nwarmup_power: 2\n\nbatch_size: 32\nvalid_batch_size: 32\nfreeze_batch_size: 512\n\n## Optimizer ##\noptimizer: \n    _target_: AdamW\n    weight_decay: 3e-5\n    max_grad_norm: 3.0\n    use_swa: True\n    epsilon: 1e-6\n    betas: [0.9, 0.999]\n    # average_decay: 0.999\n    # dynamic_decay: True\n\n## Model Training ## \nmax_epochs: 20\ncheckpoints_per_epoch: 4\n\n## Data Factory ## \nfold: 3\nadd_special_tokens: True","253f7fd9":"with STRATEGY.scope(): \n    backbone = TFAutoModel.from_pretrained(\n        HP.backbone, \n        attention_probs_dropout_prob=HP.attention_dropout, \n        from_pt=HP.from_pytorch, \n    )\n    model = build_scorer_model(backbone)\n    model.load_weights(WEIGHTS_DIR\/HP.weights)","d15af3e5":"# TODO: ADD SPECIAL TOKENS FOR SOFT AND HARD IN TRAIN\n\nTOXIC_FEATURES = [\n    'severe_toxic', 'identity_hate', 'threat', \n    'toxic', 'insult', 'obscene', \n]\nSPECIAL_TOKENS = [\n    '[SEVERE_TOXIC]', '[IDENTITY_HATE]', '[THREAT]', \n    '[TOXIC]', '[INSULT]', '[OBSCENE]', \n]\n\nFEAT_TO_TOKEN = {feat: token for feat, token in zip(TOXIC_FEATURES, SPECIAL_TOKENS)}\n\n    \ntokenizer = AutoTokenizer.from_pretrained(\n    HP.backbone, additional_special_tokens=SPECIAL_TOKENS, \n)","7dfdf93c":"def add_special_tokens_LT(row):\n    text = row.less_toxic \n    for feat in reversed(TOXIC_FEATURES): \n        if row[f'LT_{feat}'] > 0.5: \n            text = f'{FEAT_TO_TOKEN[feat]} ' + text\n    if (row.LT_toxic == -1) or (1 < row.LT_toxic < 0): \n        text = '[SOFT] ' + text\n    else: \n        text = '[HARD] ' + text\n    return text\n\ndef add_special_tokens_MT(row):\n    text = row.more_toxic \n    for feat in reversed(TOXIC_FEATURES): \n        if row[f'MT_{feat}'] > 0.5: \n            text = f'{FEAT_TO_TOKEN[feat]} ' + text\n    if (row.MT_toxic == -1) or (1 < row.MT_toxic < 0): \n        text = '[SOFT] ' + text\n    else: \n        text = '[HARD] ' + text\n    return text\n\ndef add_feature_values(df, old): \n    old_dict = old.set_index('comment_text').to_dict()\n    for feat in TOXIC_FEATURES: \n        df[f'MT_{feat}'] = df.more_toxic.map(old_dict[feat])\n        df[f'LT_{feat}'] = df.less_toxic.map(old_dict[feat])\n    return df","622aa74d":"df = pd.read_csv('\/kaggle\/input\/toxic-dataframes\/valid.csv')\nold = pd.read_csv('..\/input\/toxic-public-dataframes\/old_pseudo_label.csv')\nmanual = pd.read_csv('..\/input\/toxic-dataframes\/manual_pair_labels.csv')\nmanual = manual[manual.type=='manual']\nmanual['fold'] = HP.fold+1 # Hacky\n\n\ndf = pd.concat([df, manual])\ndf = add_feature_values(df, old)\ndf = df.fillna(-1)\ndf.more_toxic = df.apply(add_special_tokens_MT, axis=1)\ndf.less_toxic = df.apply(add_special_tokens_LT, axis=1)\n\ndf = df[['more_toxic', 'less_toxic', 'fold']]\ndf1, df2 = df.copy(), df.copy()\ndf1['A_comment'], df1['B_comment'], df1['y'] = df.more_toxic, df.less_toxic, 1.0\ndf2['A_comment'], df2['B_comment'], df2['y'] = df.less_toxic, df.more_toxic, 0.0\ndf = pd.concat([df1, df2])\n\n\ntrain, valid = df[df.fold!=HP.fold], df[df.fold==HP.fold]\ntrain","e568d7b7":"# Build Processed Datasets\ntrain_dataset = df_to_dataset(df=train, df_type='train')\nvalid_dataset = df_to_dataset(df=valid, df_type='valid')\n\n# Build Tensorflow Datasets\ntrain_ds, train_steps = build_tfds(train_dataset, dataset_type='train')\nvalid_ds, valid_steps = build_tfds(valid_dataset, dataset_type='valid')","85a2350b":"def build_model(backbone): \n    A_input_ids = tf.keras.Input((HP.max_seq_len,), dtype=tf.int32, name='A_input_ids')\n    A_attention_mask = tf.keras.Input((HP.max_seq_len,), dtype=tf.int32, name='A_attention_mask')\n    B_input_ids = tf.keras.Input((HP.max_seq_len,), dtype=tf.int32, name='B_input_ids')\n    B_attention_mask = tf.keras.Input((HP.max_seq_len,), dtype=tf.int32, name='B_attention_mask')\n    \n    A_backbone_outputs = backbone(input_ids=A_input_ids, attention_mask=A_attention_mask)\n    B_backbone_outputs = backbone(input_ids=B_input_ids, attention_mask=B_attention_mask)\n    \n    if 'pooler_output' in A_backbone_outputs:\n        A_x = A_backbone_outputs.pooler_output\n        B_x = B_backbone_outputs.pooler_output\n    else: \n        A_x = tf.reduce_mean(A_backbone_outputs.last_hidden_state, axis=1)\n        B_x = tf.reduce_mean(B_backbone_outputs.last_hidden_state, axis=1)\n    \n    repr_hidden_layer = build_hidden_layer(\n        HP.representation_hidden_layers, HP.hidden_dropout, name='repr_hidden_layer'\n    )\n    A_x = repr_hidden_layer(A_x)\n    B_x = repr_hidden_layer(B_x)\n    \n    x = tf.concat([A_x, B_x], axis=-1)\n    inter_hidden_layer = build_hidden_layer(\n        HP.interaction_hidden_layers, HP.hidden_dropout, name='inter_hidden_layer'\n    )\n    x = inter_hidden_layer(x)\n    \n    score_layer = tf.keras.layers.Dense(1, activation='sigmoid', name='y')\n    x = score_layer(x)\n    return tf.keras.Model([A_input_ids, A_attention_mask, B_input_ids, B_attention_mask], outputs=x)","842e7caa":"HP.hidden_dropout = 0.50\nwith STRATEGY.scope(): \n    backbone.trainable = False\n    model = quick_compile_model(1e-2)\nhistory = model.fit(\n    freeze_train_ds, steps_per_epoch=freeze_train_steps, epochs=25, \n    validation_data=valid_ds, validation_steps=valid_steps, \n    callbacks=[reduce_lr_on_plateau],\n)\nmodel.save_weights('freeze_tuned_model.h5')","b75defad":"HP.max_epochs = 100\nHP.hidden_dropout = 0.25\nwith STRATEGY.scope(): \n    backbone.trainable = True\n    model = get_compiled_model()\n    model.load_weights('freeze_tuned_model.h5')\nhistory = model.fit(\n    train_ds, steps_per_epoch=train_steps\/\/HP.checkpoints_per_epoch+1, \n    epochs=HP.max_epochs*HP.checkpoints_per_epoch, \n    validation_data=valid_ds, validation_steps=valid_steps, \n    callbacks=[accuracy_checkpoint, loss_checkpoint],\n)","12d03918":"%%time \ndef build_representation_model(model, backbone): \n    input_ids = tf.keras.Input((HP.max_seq_len,), dtype=tf.int32)\n    attention_mask = tf.keras.Input((HP.max_seq_len,), dtype=tf.int32)\n    \n    backbone_outputs = backbone(input_ids=input_ids, attention_mask=attention_mask)\n    x = backbone_outputs.pooler_output\n    \n    # repr_hidden_layer = model.get_layer('repr_hidden_layer')\n    # x = repr_hidden_layer(x)\n    return tf.keras.Model([input_ids, attention_mask], outputs=x)\n\nwith STRATEGY.scope(): \n    model.load_weights('checkpoint_acc.h5')\n    \n    repr_model = build_representation_model(model, backbone)\n    repr_model.save_weights('ruddit_sptoken_repr.h5')\n    \n    inter_model = build_interaction_model(model, backbone)\n    inter_model.save_weights('ruddit_sptoken_inter.h5')\n    \n    tokenizer.save_pretrained('ruddit_tokenizer')","f257f5d5":"# __Toxic Ensemble Trainer__\n\nTrain multiple ensemble models \n\n---\n### <a href='#hyperparameters'> \u2699\ufe0f Hyperparameters <\/a> | <a href='#data-factory'> \u2692 Data Factory <\/a> | <a href='#model-factory'> \ud83e\udde0 Model Factory <\/a> | <a href='#training'> \u26a1 Training <\/a> \n\n","9cf28644":"# Ensemble Models\n---","f1f2f45f":"# Model 2: Roberta Large LB 820\n---","3b078203":"# Model 3: Ruddit with Special Tokens (LB: 0.785)\n---"}}