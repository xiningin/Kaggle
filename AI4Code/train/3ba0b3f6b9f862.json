{"cell_type":{"7afd1e25":"code","62b53ec6":"code","2f3c4289":"code","51e98d71":"code","64e3e70c":"code","4a7cd047":"code","549fe162":"code","125a2e61":"code","126c9001":"code","80a2332b":"code","12a3318e":"code","6221c7e8":"code","9404b4d8":"code","195c228e":"code","e29ce94f":"code","498fb6bd":"code","caa71cb2":"code","9d6e424f":"code","cb313b3e":"code","4497be86":"code","b4caa686":"code","de8bfcbf":"code","acad888c":"code","34b7417e":"code","bf99c095":"code","f2e6f81a":"code","9c5e34bb":"code","d4d8bd56":"code","545e8a80":"code","27e7acae":"code","1f692b8e":"code","e80de8af":"code","49b412b6":"code","e18c025a":"code","cbf90244":"code","1b0e8f91":"code","2d5ec379":"code","04bd940a":"code","2d0e1c6a":"code","72fde830":"code","e444058e":"code","fea32bd4":"code","fc6e345d":"code","d30a52d0":"code","bebca276":"code","3c9ea8a2":"code","ad7f230e":"code","7d726035":"code","c7ed5f11":"code","75cdc36d":"code","c1c109f6":"markdown","d713272f":"markdown","38c9ed92":"markdown","3871494f":"markdown","0a4eb6e7":"markdown","ffcfce6a":"markdown","881c266c":"markdown","d608f2e0":"markdown","b3a17692":"markdown","abd1b4ff":"markdown","6b3d0d5d":"markdown","57072f29":"markdown","c6713868":"markdown","adf20b4e":"markdown","33c4c7a5":"markdown","75d5786a":"markdown","a31bcc7d":"markdown","97369301":"markdown","b1f54dfe":"markdown","0761ab0a":"markdown"},"source":{"7afd1e25":"#general\nimport os\nimport numpy as np\nimport gc\nimport sys\nimport pandas as pd\nimport time\n\n\n#preprocessing\nfrom category_encoders import CatBoostEncoder\nfrom sklearn.preprocessing import OneHotEncoder as ohe\nfrom sklearn.model_selection import StratifiedKFold,train_test_split\nfrom sklearn.preprocessing import StandardScaler as Scaler\n\n#feature selection:\nfrom sklearn.feature_selection import mutual_info_classif as mic\n\n#Minority Oversampling\nfrom imblearn.over_sampling import SMOTE\n\n\n#modelling\n#metrics\nfrom sklearn.metrics import roc_auc_score as auc\nfrom catboost import CatBoostClassifier as cb\n\n#hyperparam optimization\nimport optuna\nfrom optuna import Trial\n\n#thresholding\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import confusion_matrix\n\nimport warnings \nwarnings.filterwarnings('ignore')","62b53ec6":"train=pd.read_csv('..\/input\/ghf1-hackathon\/Training Data.csv')\ntest=pd.read_csv('..\/input\/ghf1-hackathon\/Test Data.csv')\nsample_sub=pd.read_csv('..\/input\/ghf1-hackathon\/Sample Prediction Dataset.csv')","2f3c4289":"train.reset_index(inplace=True,drop=True)\ntest.reset_index(inplace=True,drop=True)\n\ny=train.pop('risk_flag')\n\ntrain_ids=train.pop('Id')\ntest_ids=test.pop('id')","51e98d71":"def mapping_features(train,test):\n    car_ownership={'yes':1,\n                    'no':0}\n    \n    married={'single':0,\n             'married':1}\n    \n    house_own={'rented':1,\n             'norent_noown':0,\n             'owned':2}\n\n    \n    train['car_ownership'].replace(car_ownership,\n                               inplace=True)\n    test['car_ownership'].replace(car_ownership,\n                               inplace=True)\n\n    train['married'].replace(married,\n                         inplace=True)\n    test['married'].replace(married,\n                         inplace=True)\n\n    \n    \n    train['house_ownership'].replace(house_own,\n                                 inplace=True)\n    test['house_ownership'].replace(house_own,\n                                 inplace=True)\n    \n    \n    return train,test\n\n\n\ntrain,test=mapping_features(train,\n                            test)","64e3e70c":"from geopy.geocoders import Nominatim\ngeocoder= Nominatim(user_agent=\"myGeocoder\")\n\n\ndef get_coordinates(address,geocoder=geocoder):\n    \n    loc=geocoder.geocode(address)\n    \n    \n    #added sleep because API allows only one call per sec\n    time.sleep(1)\n    return loc\n\nget_coordinates('Fergusson college')","4a7cd047":"def city_features(row):\n    '''Mark capital cities and cities of economic Importance'''\n    \n    row['city']=row['city'].split('[')[0]\n    row['state']=row['state'].lower()\n    \n    \n    # adding features based on location:\n    \n    of_economic_imp=['Chennai','Kalyan-Dombivli','Hyderabad','Malegaon','Mumbai',\n                     'Delhi_city','Bangalore','New_Delhi','Mira-Bhayandar','Navi_Mumbai'\n                    'Vasai-Virar','Kolkata','Mangalore','Jabalpur','Thane','Pune',\n                    'Ambarnath','Panvel','Secunderabad','Noida','Visakhapatnam',\n                    'Jamshedpur','Pimpri-Chinchwad','Ahmedabad','Surat','Jaipur',\n                    'Nagpur','Indore','Patna']\n    \n    capital_cities=['Bhopal','Pondicherry','Chennai','Shimla','Hyderabad','Aizawl',\n                    'Mumbai','Kochi','Srinagar','Patna','Delhi_city',\n                    'Chandigarh_city','Jammu','Lucknow','Bangalore','New_Delhi',\n                   'Imphal','Kolkata','Gangtok','Dehradun','Aizawl','Raipur',\n                   'Ranchi','Thiruvananthapuram','Gandhinagar','Shillong','Kohima',\n                   'Itanagar','Dispur','Panaji','Jaipur','Agartala']\n    \n    \n    row['capital']=int(str(row['city']) in capital_cities)\n    row['econ_imp']=int(str(row['city']) in of_economic_imp)\n    \n    return row\n    ","549fe162":"def location_data(train,test,y):\n    '''Features from Location Data'''\n    \n    \n    #city features\n    train=train.apply(lambda x: city_features(x),axis=1)\n    test=test.apply(lambda x: city_features(x),axis=1)\n    \n    \n#     # location\n    train['location']=train.city +','+ train.state\n    test['location']=test.city + ','+ train.state\n    \n    train.drop(['state','city'],axis=1,inplace=True)\n    test.drop(['state','city'],axis=1,inplace=True)\n    \n    enc=CatBoostEncoder(cols=['location'])\n    \n    enc.fit(train,y)\n    \n    train=enc.transform(train)\n    test=enc.transform(test)\n   \n    return train,test\n    \n    ","125a2e61":"%%time\ntrain,test=location_data(train,test,y)","126c9001":"def ratio_features(row):\n    '''features created out of ratios of given features'''\n    \n    #income to experience ratio\n    if row['experience']!=0:\n        row['income_exp_ratio']=row['income']\/row['experience']\n        row['age_exp_ratio']=row['age']\/row['experience']\n        \n    else:\n        row['income_exp_ratio']=row['income']\/2\n        row['age_exp_ratio']=row['age']\/2\n    \n    #income to current job years ratio\n    if row['current_job_years']!=0:\n        row['income_joby_ratio']=row['income']\/row['current_job_years']\n        row['age_curjob_ratio']=row['age']\/row['current_job_years']\n        \n    else:\n        row['income_joby_ratio']=row['income']\n        row['age_curjob_ratio']=row['age']\n        \n    return row     \n    \ndef other_work_exp(row):\n    '''Probable other work exp based on age and given experience'''\n    \n    #probable other work exp     \n    #considering 25 to be the average age of starting earning:    \n          \n    if ((row['age']>35 and row['age']<45) and row['experience']<10):\n        row['P(oth_work)']=row['age']-(25+row['experience'])\n        \n    elif ((row['age']>45 and row['age']<60) and row['experience']<15):\n        row['P(oth_work)']=row['age']-(25+row['experience'])\n    \n    elif ((row['age']>65) and row['experience']<20):\n        row['P(oth_work)']=row['age']-(25+row['experience'])\n        \n    elif ((row['age']>30 and row['age']<35)and row['experience']<4):\n        row['P(oth_work)']=max(row['age']-(25+row['experience']),2)\n        \n    else:\n        # about 2 years of workexp is req for loans\n        row['P(oth_work)']=2        \n\n    return row\n\ndef emi_features(row):\n    \n    '''guess around if a person pays emi'''\n    # considering 15 years to be average repayment tenure and \n    # assuming person is paying for the house he owns.\n    row['P(pays_house_emi)'] = int((row['house_ownership']==2) and (row['current_house_years']<=15))\n    \n    #considering people above 50 probably would not buy car with a EMI,\n    #also assuming people without family wont buy a car on EMI\n    row['P(pays_car_emi)']  = int((row['car_ownership']==1) and \\\n                                  (row['age']<55) and \\\n                                  (row['married']==1))\n    return row\n    \n    \ndef create_features(df1):\n    \n    '''create some features from existing features'''\n    \n    \n    df=df1.copy(deep=True)  \n    \n    #ratio features\n    df=df.apply(lambda x:ratio_features(x),axis=1)\n    \n    #age income ratio\n    df['income_age_ratio']=df['income']\/df['age']\n\n    #documented other exp\n    df['other_exp']=df['experience']-df['current_job_years']\n    \n    #probable other exp based on age\n    df=df.apply(lambda x: other_work_exp(x),axis=1)\n    \n    #probable total exp:\n    df['P(total_exp)']=df['experience']+df['P(oth_work)']\n    \n    # in working age(18-65)\n    df['working_age']=df['age'].apply(lambda x : int((x>18) and (x<65)))\n    \n    #income to probable total exp:\n    df['P(income\/total_exp)']=df['income'] \/df['P(total_exp)']\n    \n    #pays house rent\n    df['pays_rent']=df['house_ownership'].apply(lambda x: int(x==1))\n    \n    #Possibly pays EMIs\n    df=df.apply(lambda x: emi_features(x),axis=1)\n    \n    return df\n    ","80a2332b":"%%time\nX=create_features(train)\nX_test=create_features(test)","12a3318e":"#there are ' ' in the test profession against the '_' in train profession\n\nX_test['profession']=X_test['profession'].apply(lambda x: str(x).replace(' ','_'))","6221c7e8":"# def feature_selection(n_iters):\ndef feature_selector(n_iters=10):\n    scores=[]\n    n=0\n    x=X.drop(['profession'],axis=1)\n    \n    while n<=n_iters:\n        mir_data=mic(x,y)\n        scores.append(mir_data)\n        n+=1\n        \n    mean_score=np.mean(a=np.array(scores),axis=0)\n    \n    mutual_info=pd.Series(mean_score,index=x.columns).sort_values(ascending=False)\n    \n    return mutual_info","9404b4d8":"feature_selector(1)","195c228e":"def job_segments(df):\n    '''Cluster the jobs into super segments'''\n    df1=df.copy(deep=True)\n    \n    def get_key(prof,d):\n        for i,j in d.items():\n            if  prof in j:\n                return i\n            \n    \n    \n    prof_map={'Engineering':['Mechanical_engineer','Software_Developer',\n                             'Technical_writer','Design_Engineer',\n                            'Chemical_engineer','Biomedical_Engineer',\n                            'Computer_hardware_engineer','Petroleum_Engineer',\n                            'Engineer','Analyst','Web_designer','Civil_engineer',\n                            'Industrial_Engineer','Financial_Analyst','Design_Engineer'],\n\n                'Design':['Architect','Designer',\n                        'Artist',\n                        'Graphic_Designer','Fashion_Designer'],\n\n                'Academic':['Librarian','Economist',\n                         'Microbiologist','Geologist',\n                          'Statistician','Scientist'],\n\n                'Service':['Flight_attendant','Hotel_Manager',\n                         'Secretary','Computer_operator',\n                        'Technician','Drafter','Chef'],\n\n                'Specialist':['Physician','Air_traffic_controller',\n                            'Surveyor','Dentist','Chartered_Accountant',\n                           'Aviator','Psychologist','Lawyer','Surgeon',\n                            'Technology_specialist'],\n\n                'Public':['Civil_servant','Police_officer',\n                        'Politician','Magistrate',\n                        'Firefighter','Official',\n                       'Army_officer'],\n                'Gig':['Comedian','Consultant']}\n\n    \n    \n    for prof in df1['profession'].unique():\n        \n        sup_prof=get_key(prof=prof,\n                        d=prof_map)\n        \n        \n        df1.loc[df1['profession']==prof,'super_profession']=sup_prof\n        \n        \n    return df1\n\n\n        \nX=job_segments(X)\nX_test=job_segments(X_test)","e29ce94f":"# ratio of Positive and Negative Samples\ny.value_counts(1)","498fb6bd":"categorical_columns=['profession','super_profession']","caa71cb2":"def objective(trial:Trial):\n    \n    #splitting training data \n    x_train,x_test,y_train,y_test=train_test_split(X,y,\n                                                   random_state=7,\n                                                   shuffle=True,\n                                                   train_size=0.7,\n                                                   stratify=y)\n    \n    #hyperparam_grid\n    params={   'verbose'        : 0,\n               'loss_function'  :'Logloss',\n               'depth'          :trial.suggest_int('depth',4,10),\n               'learning_rate'  :trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n               'l2_leaf_reg'    :trial.suggest_loguniform('l2_leaf_reg', 1e-2, 10.0),\n               'random_strength':trial.suggest_uniform('random_strength',1e-2,0.3),\n               'max_bin'        :trial.suggest_int('max_bin',64,254),\n#                'grow_policy'    :trial.suggest_categorical('grow_policy',\n#                                                            ['SymmetricTree','Depthwise','Lossguide']),\n               'iterations'     :trial.suggest_int('iterations',1000,2000),\n#                'max_leaves'     :trial.suggest_int('max_leaves',2,64),\n               \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.1, 0.8),\n               \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n               \"bootstrap_type\": 'MVS',\n#             trial.suggest_categorical(\"bootstrap_type\",\n#                                                            [\"Bayesian\", \"MVS\",'Bernoulli']),\n               'eval_metric': 'AUC'\n                }\n    \n    \n    try:\n        model = cb(**params)\n\n        model.fit(x_train,y_train,\n                 eval_set=[(x_test,y_test)],\n                 verbose=0,\n                 cat_features=categorical_columns,\n                 early_stopping_rounds=300)\n\n        \n        preds=[i[1] for i in model.predict_proba(x_test)]\n\n        acc= auc(y_test,preds)\n        \n        return acc\n\n    except Exception as e:\n        print(e)\n        return None","9d6e424f":"def get_best_params(time_out=10000):\n    '''time_out: time out in seconds'''\n    sampler = optuna.samplers.TPESampler(seed=7)  # Make the sampler behave in a deterministic way.\n    study=optuna.create_study(direction='maximize',sampler=sampler)\n    study.optimize(objective, n_trials=300, timeout=time_out)\n    \n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n    \n    return study.best_trial.params\n\n\nbest_params=get_best_params()","cb313b3e":"best_params['verbose']       = 0\nbest_params['loss_function'] ='Logloss'\nbest_params[\"bootstrap_type\"]= 'MVS'\nbest_params['eval_metric']   = 'AUC'\nbest_params['iterations']    = 5000\n\n\nbest_params","4497be86":"def k_fold_predict(k,\n                  params=best_params):\n    \n    \n    skf=StratifiedKFold(n_splits=k,\n                       shuffle=True,\n                       random_state=7)\n    \n    mean_preds=np.zeros(shape=(X_test.shape[0]))\n    train_check=np.zeros(shape=(X.shape[0]))   \n    \n    i=0\n    for train_idx,val_idx in skf.split(X,y):\n        x_t,x_v=X.iloc[train_idx],X.iloc[val_idx]\n        y_t,y_v=y.iloc[train_idx],y.iloc[val_idx]\n        \n        model=cb(**params)        \n        model.fit(x_t,y_t,\n                 cat_features=categorical_columns\n                 )\n        \n        \n        print('Validation AUC score for fold {} = {}'.format(i,\n                                                         auc(\n                                            y_v,\n                                            [i[1] for i in model.predict_proba(x_v)])))\n        i+=1\n        \n        #test predictions\n        test_p=np.array([i[1] for i in model.predict_proba(X_test)])\n        mean_preds+=test_p\n        \n        \n        #training preds\n        train_p=np.array([i[1] for i in model.predict_proba(X)])      \n        train_check+=train_p\n        \n        \n    mean_preds=mean_preds\/k\n    train_check=train_check\/k\n    \n    return mean_preds,train_check","b4caa686":"# %%time\ntest_pred,train_pred=k_fold_predict(7)\n\npreds=pd.DataFrame(test_pred,columns=['risk_flag'])\ntrain_preds=pd.DataFrame(train_pred,columns=['risk_flag'])\n\npreds['id']=test_ids\ntrain_preds['id']=train_ids","de8bfcbf":"train_auc=auc(y,\n             train_preds['risk_flag'])\nprint(f'AUC score for train set {train_auc}')","acad888c":"def roc_threshold(y_true,y_pred):\n    '''get best threshold from a roc_auc_curve\n    y_true: ground truth\n    y_pred: predicted probabilities'''\n\n\n    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n    # get the best threshold\n    J = tpr - fpr\n    ix = np.argmax(J)\n    best_thresh = thresholds[ix]\n    thresh_tpr=tpr[ix]\n    thresh_fpr=fpr[ix]\n    print(f'Best Threshold (TPR-FPR)= {best_thresh} \\n TPR :{thresh_tpr} \\n FPR :{thresh_fpr}')\n\n    #best threshold by geometric mean :\n    # calculate the g-mean for each threshold\n    gmeans = np.sqrt(tpr * (1-fpr))\n\n    # locate the index of the largest g-mean\n    ix1 = np.argmax(gmeans)\n    \n    \n    print('Best Threshold (geometric mean) = %f \\n G-Mean=%.3f' % (thresholds[ix1], gmeans[ix1]))\n\n    return best_thresh\n\nbest_thresh=roc_threshold(y,train_preds['risk_flag'])","34b7417e":"#best threshold from above method\nthresh=best_thresh\n\nthresh","bf99c095":"def get_bool(predictions_df,threshold):\n    '''get 1\/0 based on thresholds'''\n    \n    df=predictions_df.copy(deep=True)\n    \n    df['risk_flag']=df['risk_flag'].apply(lambda x: int(x>=threshold))\n    \n    df=df[['id','risk_flag']]\n    \n    return df\n\nsubmissions=get_bool(predictions_df=preds,\n                     threshold=thresh)","f2e6f81a":"\ntr=get_bool(predictions_df=train_preds,\n                     threshold=thresh)\n\nauc(y,tr['risk_flag'])","9c5e34bb":"# The Ratio of Positive and Negative examples to the total.\nsubmissions['risk_flag'].value_counts(1)","d4d8bd56":"submissions.shape","545e8a80":"submissions.to_csv('submission.csv',index=False)","27e7acae":"thresh2=thresh-0.015","1f692b8e":"def get_bool(predictions_df,threshold):\n    '''get 1\/0 based on thresholds'''\n    \n    df=predictions_df.copy(deep=True)\n    \n    df['risk_flag']=df['risk_flag'].apply(lambda x: int(x>=threshold))\n    \n    df=df[['id','risk_flag']]\n    \n    return df\n\nsubmissions=get_bool(predictions_df=preds,\n                     threshold=thresh2)","e80de8af":"\ntr=get_bool(predictions_df=train_preds,\n                     threshold=thresh2)\n\nauc(y,tr['risk_flag'])","49b412b6":"# The Ratio of Positive and Negative examples to the total.\nsubmissions['risk_flag'].value_counts(1)","e18c025a":"submissions.to_csv('submission1.csv',index=False)","cbf90244":"thresh3=thresh-0.02","1b0e8f91":"def get_bool(predictions_df,threshold):\n    '''get 1\/0 based on thresholds'''\n    \n    df=predictions_df.copy(deep=True)\n    \n    df['risk_flag']=df['risk_flag'].apply(lambda x: int(x>=threshold))\n    \n    df=df[['id','risk_flag']]\n    \n    return df\n\nsubmissions=get_bool(predictions_df=preds,\n                     threshold=thresh3)","2d5ec379":"tr=get_bool(predictions_df=train_preds,\n                     threshold=thresh3)\n\nauc(y,tr['risk_flag'])","04bd940a":"# The Ratio of Positive and Negative examples to the total.\nsubmissions['risk_flag'].value_counts(1)","2d0e1c6a":"submissions.to_csv('submission2.csv',index=False)","72fde830":"thresh4=thresh-0.03","e444058e":"def get_bool(predictions_df,threshold):\n    '''get 1\/0 based on thresholds'''\n    \n    df=predictions_df.copy(deep=True)\n    \n    df['risk_flag']=df['risk_flag'].apply(lambda x: int(x>=threshold))\n    \n    df=df[['id','risk_flag']]\n    \n    return df\n\nsubmissions=get_bool(predictions_df=preds,\n                     threshold=thresh4)","fea32bd4":"tr=get_bool(predictions_df=train_preds,\n                     threshold=thresh4)\n\nauc(y,tr['risk_flag'])","fc6e345d":"# The Ratio of Positive and Negative examples to the total.\nsubmissions['risk_flag'].value_counts(1)","d30a52d0":"submissions.to_csv('submission3.csv',index=False)","bebca276":"thresh5=thresh-0.036","3c9ea8a2":"def get_bool(predictions_df,threshold):\n    '''get 1\/0 based on thresholds'''\n    \n    df=predictions_df.copy(deep=True)\n    \n    df['risk_flag']=df['risk_flag'].apply(lambda x: int(x>=threshold))\n    \n    df=df[['id','risk_flag']]\n    \n    return df\n\nsubmissions=get_bool(predictions_df=preds,\n                     threshold=thresh5)","ad7f230e":"tr=get_bool(predictions_df=train_preds,\n                     threshold=thresh5)\n\nauc(y,tr['risk_flag'])","7d726035":"# The Ratio of Positive and Negative examples to the total.\nsubmissions['risk_flag'].value_counts(1)","c7ed5f11":"submissions.to_csv('submission4.csv',index=False)","75cdc36d":"#  Old params\n\n# 13\/8\/21 92.5 auc\n# best_params={\n#              'verbose'        : 0,\n#              'loss_function'  :'Logloss',\n#              'eval_metric': 'AUC',\n#              \"bootstrap_type\": 'MVS',           \n#              'depth': 9, \n#              'learning_rate': 0.07689113711214997, \n#              'l2_leaf_reg': 0.9296767227963647, \n#              'random_strength': 0.0991487399289666,\n#              'max_bin': 254, \n#              'iterations': 1727,\n#              'colsample_bylevel': 0.6456810156401686,\n#              'boosting_type': 'Plain'}\n\n\n# 12\/8\/21 :93 auc\n# best_params={\n#              'verbose'        : 0,\n#              'loss_function'  :'Logloss',\n#              'eval_metric': 'AUC',\n#              \"bootstrap_type\": 'MVS',           \n#              'depth': 10,\n#              'learning_rate': 0.048887169045477, \n#              'l2_leaf_reg': 9.637912687253579, \n#              'random_strength': 0.14125825814463858,\n#              'max_bin': 65, \n#              'iterations': 1987, \n#              'colsample_bylevel': 0.5998403438024634,\n#              'boosting_type': 'Plain'}\n\n\n# # 11\/8\/21:89auc\n# best_params={'verbose'        : 0,\n#              'loss_function'  :'Logloss',\n#              'eval_metric': 'AUC',\n#              'depth': 8,\n#              'learning_rate': 0.023218155139759035,\n#              'l2_leaf_reg': 0.014433979096775669,\n#              'random_strength': 0.09956727861050116,\n#              'max_bin': 177,\n#              'iterations': 1470,\n#              'colsample_bylevel': 0.6789825997216761,\n#              'boosting_type': 'Ordered',\n#              'bootstrap_type': 'MVS'}","c1c109f6":"**3**","d713272f":"# Preprocessing","38c9ed92":"# Loading data","3871494f":"**checking the AUC score on Predicted Training Set**","0a4eb6e7":"**Training and prediction on folds**","ffcfce6a":"**Mapping Categorical Features to relevant Values**","881c266c":"**NOTE: The P in  'P(feature)' denote possibly, as I am using crude hueristics to make these features**","d608f2e0":"# Problem Statement : Predict Customer Credit Risk Flag .","b3a17692":"# Rough work","abd1b4ff":"# Imports","6b3d0d5d":"**Converting Probablities into Flags (i.e, 0:No Risk, 1: Risk)**","57072f29":"**Submission File**","c6713868":"**Optimizing**","adf20b4e":"**4**","33c4c7a5":"**Convert Location Address to Coordinates**","75d5786a":"**5**","a31bcc7d":"# Optuna Hyperparameter optimization","97369301":"# Tweaking Threshold Val","b1f54dfe":"**Checking Feature Importances**","0761ab0a":"# Model Fitting and Prediction"}}