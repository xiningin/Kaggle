{"cell_type":{"12aae727":"code","73ed673a":"code","3d417619":"code","0b85bf0d":"code","465e2b75":"code","2a637821":"code","ff15a764":"code","88510b27":"code","2fa80240":"code","bb0df433":"code","8fc54091":"code","60f34891":"code","2ae246ad":"code","10519b55":"code","a1bf15cd":"code","7ac7a563":"code","761bc9eb":"code","5fd54015":"code","9e077c38":"code","867ea7a9":"code","4ef5f4cd":"code","4a8c4ae2":"code","ee5cf965":"code","afd09900":"code","2b09c572":"code","61690b78":"code","84a1df33":"code","ce2f075d":"code","cce98fab":"code","f69321bc":"code","259d6614":"code","d5ea16d9":"code","0815f681":"markdown","ce684cfe":"markdown","b9946cad":"markdown","ae8cbb96":"markdown","b61215fe":"markdown","320cc33a":"markdown","ce1d8142":"markdown","2075ed66":"markdown","69e9eef1":"markdown","0c2bde52":"markdown","4769de1c":"markdown","b055055c":"markdown","3aa0766f":"markdown","2d48e0fc":"markdown"},"source":{"12aae727":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","73ed673a":"df=pd.read_csv('..\/input\/red-wine-quality\/winequality-red.csv')","3d417619":"df.head()","0b85bf0d":"df.info()","465e2b75":"df.describe()","2a637821":"df.isnull().sum()","ff15a764":"df.shape","88510b27":"df['quality'].value_counts()","2fa80240":"import seaborn as sns\nsns.countplot(x = 'quality',data = df)","bb0df433":"f, ax = plt.subplots(figsize=(10, 8))\ncorr = df.corr()\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax,annot=True)","8fc54091":"new_df=df.replace(0,np.NaN)","60f34891":"new_df.isnull().sum()","2ae246ad":"new_df[\"citric acid\"].fillna(new_df[\"citric acid\"].mean(), inplace = True)","10519b55":"new_df.describe().T","a1bf15cd":"# converting the response variables(3-7) as binary response variables that is either good or bad\n\nnames = ['bad', 'good']\nbins = (2, 6.5, 8)\n\ndf['quality'] = pd.cut(df['quality'], bins = bins, labels = names)\n\n","7ac7a563":"df['quality'].value_counts()","761bc9eb":"#We have now labelled the quality into good and bad,now to convert them into numerical values\n\nfrom sklearn.preprocessing import LabelEncoder\nlabel_quality=LabelEncoder()\ndf['quality']= label_quality.fit_transform(df['quality'])\ndf['quality'].value_counts()","5fd54015":"sns.countplot(df['quality'])","9e077c38":"#FeatureSelection\nX=df.iloc[:,:11].values\ny=df.iloc[:,11].values\n#splitting X and y\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X, y, test_size = 0.20, random_state = 44 )","867ea7a9":"#Checking dimensions\nprint(\"X_train shape:\", X_train.shape)\nprint(\"X_test shape:\", X_test.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"y_test shape:\", y_test.shape)","4ef5f4cd":"# standard scaling \nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\n","4a8c4ae2":"# Logistic Regression Algorithm\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(random_state = 44)\nlogreg.fit(X_train, y_train)\n\n","ee5cf965":"# Support Vector Classifier Algorithm\nfrom sklearn.svm import SVC\nsvc = SVC(kernel = 'linear', random_state = 44)\nsvc.fit(X_train, y_train)","afd09900":"# Naive Bayes Algorithm\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train, y_train)\n","2b09c572":"# Decision tree Algorithm\nfrom sklearn.tree import DecisionTreeClassifier\ndectree = DecisionTreeClassifier(criterion = 'entropy', random_state = 44)\ndectree.fit(X_train, y_train)\n","61690b78":"# Random forest Algorithm\nfrom sklearn.ensemble import RandomForestClassifier\nranfor = RandomForestClassifier(n_estimators = 11, criterion = 'entropy', random_state = 44)\nranfor.fit(X_train, y_train)\n","84a1df33":"# Making predictions on test dataset\nY_pred_logreg = logreg.predict(X_test)\n#Y_pred_knn = knn.predict(X_test)\nY_pred_svc = svc.predict(X_test)\nY_pred_nb = nb.predict(X_test)\nY_pred_dectree = dectree.predict(X_test)\nY_pred_ranfor = ranfor.predict(X_test)","ce2f075d":"# Evaluating using accuracy_score metric\nfrom sklearn.metrics import accuracy_score\naccuracy_logreg = accuracy_score(y_test, Y_pred_logreg)\n#accuracy_knn = accuracy_score(y_test, Y_pred_knn)\naccuracy_svc = accuracy_score(y_test, Y_pred_svc)\naccuracy_nb = accuracy_score(y_test, Y_pred_nb)\naccuracy_dectree = accuracy_score(y_test, Y_pred_dectree)\naccuracy_ranfor = accuracy_score(y_test, Y_pred_ranfor)","cce98fab":"# Accuracy on test set\nprint(\"Logistic Regression: \" + str(accuracy_logreg * 100))\n#print(\"K Nearest neighbors: \" + str(accuracy_knn * 100))\nprint(\"Support Vector Classifier: \" + str(accuracy_svc * 100))\nprint(\"Naive Bayes: \" + str(accuracy_nb * 100))\nprint(\"Decision tree: \" + str(accuracy_dectree * 100))\nprint(\"Random Forest: \" + str(accuracy_ranfor * 100))","f69321bc":"# Confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, Y_pred_ranfor)\ncm","259d6614":"# Heatmap of Confusion matrix\nsns.heatmap(pd.DataFrame(cm), annot=True)","d5ea16d9":"# Classification report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, Y_pred_ranfor))","0815f681":"## Visualization","ce684cfe":"## Data pre-processing","b9946cad":"## Model Evaluation","ae8cbb96":"## Model Building","b61215fe":"Thank you please upvote. :)","320cc33a":"## Importing libraries and dataset","ce1d8142":"## Feature engineering\n","2075ed66":"Luckily we dont have any missing values to work with so our data is ready for some visualization part.","69e9eef1":"The mean quality was 5.6, with its Max (best quality score) being 8.0 & its Min (worst quality score) being 3.0. Now let's see if we have any missing values we need to take care of.","0c2bde52":"Correlation Matrix aka Heat Maps helps us to you see correlations between all variables.We can see whether something is positively or negatively correlated with our predictor (target).\n\nWe can see there is a strong positive correlation between alcohol & our predictor. In fact, this is the most correlated feature in our data set, with a value of 0.48!.Since alcohol feature was the percent alcohol content in a drink. This makes sense that a higher percent of alcohol content would yield a greater satisfaction for a customer purchasing red wine!\n\nNext, we can see the second strongest positive correlation, 0.25, between Sulphates & our quality predictor. It seems that people rate the quality higher when an additive (SO\u00b2) is contributed to the drink. Sulphates acts as an antimicrobial.\n\nLastly, the strongest negative correlation is the volatile acidity, with a correlation of -0.39! This is as expected because too high acetic acid levels can lead to an unpleasant, vinegar taste!\n","4769de1c":"Random forest gives the best accuracy score","b055055c":"# Predicting the Perfect Ratio of Red Wine Ingredients with Machine Learning Algorithms.\n\n","3aa0766f":"## Descriptive statistics\nOur data has only 1 type of data: Continuous (#): which is quantitative data that can be measured.","2d48e0fc":"## Description\n The objective of the dataset is to analyze the quality of red and white variants of the Portuguese \"Vinho Verde\" wine. The datasets consists of certain input variables(based on physicochemical tests) and one target variable, quality -score between 0 and 10(based on sensory data).\n\n## Goal\n1. Explore with various Regression Models & see which yields the greatest accuracy.\n2. Examine trends & correlations within our data\n3.  Determine which features are important in high quality Red Wine\n"}}