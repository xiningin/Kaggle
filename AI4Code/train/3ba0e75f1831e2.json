{"cell_type":{"945249ed":"code","d613f726":"code","2d03e7b8":"code","762a5413":"code","3cd5af5d":"code","8056a987":"code","5f1b54d8":"code","77c06323":"code","6c57d531":"code","d16ec890":"code","f8eb978c":"code","56b1418c":"code","b3ad11ed":"code","fab95954":"code","76627e18":"code","6eb3198c":"code","774cb867":"code","d950c851":"code","6550f4a2":"markdown","3887a8a8":"markdown","e0bdc663":"markdown","0556f662":"markdown","592b9b71":"markdown","742d8814":"markdown","70399c3e":"markdown","02f669a9":"markdown","795d6174":"markdown","09a9f407":"markdown","abddeabc":"markdown","6c84f52a":"markdown","424e3192":"markdown","0e6f2470":"markdown","e8186735":"markdown","a12da2bc":"markdown","0280d021":"markdown","3194544d":"markdown","d773953e":"markdown","0d9f0c2f":"markdown","fbf1dccc":"markdown","a3fa6948":"markdown","dea4d765":"markdown"},"source":{"945249ed":"\nimport os\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.patches import Patch\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import preprocessing\nfrom sklearn.metrics import f1_score as f1\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import *\n\n#-- Pytorch specific libraries import -----#\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data import DataLoader\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","d613f726":"df_data = pd.read_csv(\"..\/input\/web-page-phishing-detection-dataset\/dataset_phishing.csv\")\ndf_data.shape","2d03e7b8":"df_data.head(5)","762a5413":"#Encoding 'status' as label 1 & 0 , naming the field as target\ndf_data['target'] = pd.get_dummies(df_data['status'])['legitimate'].astype('int')\ndf_data.drop('status',axis = 1, inplace=True)\ndf_data[['url','target']].head(5)","3cd5af5d":"tmp = df_data.isnull().sum().reset_index(name='missing_val')\ntmp[tmp['missing_val']!= 0]","8056a987":"likely_cat = {}\nfor var in df_data.iloc[:,1:].columns:\n    likely_cat[var] = 1.*df_data[var].nunique()\/df_data[var].count() < 0.002 \n\nnum_cols = []\ncat_cols = []\nfor col in likely_cat.keys():\n    if (likely_cat[col] == False):\n        num_cols.append(col)\n    else:\n        cat_cols.append(col)\n","5f1b54d8":"#Taking all columns except URL \ncorr = df_data[num_cols].corr()\n\nfig = plt.figure(figsize=(12,12),dpi=80)\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, cmap='BuPu', robust=True, center=0,\n            square=True, linewidths=.5)\nplt.title('Correlation of Numerical(Continous) Features', fontsize=15,font=\"Serif\")\nplt.show()","77c06323":"df_distr =df_data.groupby('target')[num_cols].mean().reset_index().T\ndf_distr.rename(columns={0:'0_Label',1:\"1_Label\"}, inplace=True)\n\n#plt.style.use('ggplot')\nplt.rcParams['axes.facecolor']='w'\nax = df_distr[1:-3][['0_Label','1_Label']].plot(kind='bar', title =\"Distribution of Average values across Target\", figsize=(12, 8), legend=True, fontsize=12)\nax.set_xlabel(\"Numerical Features\", fontsize=14)\nax.set_ylabel(\"Average Values\", fontsize=14)\n#ax.set_ylim(0,500000)\nplt.show()","6c57d531":"sns.catplot(\"page_rank\", hue=\"target\", data=df_data, kind=\"count\", \n            palette={1:\"green\", 0:\"blue\"} ,height=5.0, aspect=11.7\/8.27 )","d16ec890":"#Train & Test Set\nX= df_data.iloc[: , 1:-1]\n#y = upsampled_df['Churn']\ny= df_data['target']\n\ntrain_x,test_x,train_y,test_y = train_test_split(X,y,random_state=42)\nprint(\"\\n--Training data samples--\")\nprint(train_x.shape)\n","f8eb978c":"###First use a MinMaxscaler to scale all the features of Train & Test dataframes\n\nscaler = preprocessing.MinMaxScaler()\nx_train = scaler.fit_transform(train_x.values)\nx_test =  scaler.fit_transform(test_x.values)\n\nprint(\"Scaled values of Train set \\n\")\nprint(x_train)\nprint(\"\\nScaled values of Test set \\n\")\nprint(x_test)\n\n\n###Then convert the Train and Test sets into Tensors\n\nx_tensor =  torch.from_numpy(x_train).float()\ny_tensor =  torch.from_numpy(train_y.values.ravel()).float()\nxtest_tensor =  torch.from_numpy(x_test).float()\nytest_tensor =  torch.from_numpy(test_y.values.ravel()).float()\n\nprint(\"\\nTrain set Tensors \\n\")\nprint(x_tensor)\nprint(y_tensor)\nprint(\"\\nTest set Tensors \\n\")\nprint(xtest_tensor)\nprint(ytest_tensor)","56b1418c":"#Define a batch size , \nbs = 64\n#Both x_train and y_train can be combined in a single TensorDataset, which will be easier to iterate over and slice\ny_tensor = y_tensor.unsqueeze(1)\ntrain_ds = TensorDataset(x_tensor, y_tensor)\n#Pytorch\u2019s DataLoader is responsible for managing batches. \n#You can create a DataLoader from any Dataset. DataLoader makes it easier to iterate over batches\ntrain_dl = DataLoader(train_ds, batch_size=bs)\n\n\n#For the validation\/test dataset\nytest_tensor = ytest_tensor.unsqueeze(1)\ntest_ds = TensorDataset(xtest_tensor, ytest_tensor)\ntest_loader = DataLoader(test_ds, batch_size=32)","b3ad11ed":"n_input_dim = train_x.shape[1]\n\n#Layer size\nn_hidden1 = 300  # Number of hidden nodes\nn_hidden2 = 100\nn_output =  1   # Number of output nodes = for binary classifier\n\n\nclass ChurnModel(nn.Module):\n    def __init__(self):\n        super(ChurnModel, self).__init__()\n        self.layer_1 = nn.Linear(n_input_dim, n_hidden1) \n        self.layer_2 = nn.Linear(n_hidden1, n_hidden2)\n        self.layer_out = nn.Linear(n_hidden2, n_output) \n        \n        \n        self.relu = nn.ReLU()\n        self.sigmoid =  nn.Sigmoid()\n        self.dropout = nn.Dropout(p=0.1)\n        self.batchnorm1 = nn.BatchNorm1d(n_hidden1)\n        self.batchnorm2 = nn.BatchNorm1d(n_hidden2)\n        \n        \n    def forward(self, inputs):\n        x = self.relu(self.layer_1(inputs))\n        x = self.batchnorm1(x)\n        x = self.relu(self.layer_2(x))\n        x = self.batchnorm2(x)\n        x = self.dropout(x)\n        x = self.sigmoid(self.layer_out(x))\n        \n        return x\n    \n\nmodel = ChurnModel()\nprint(model)","fab95954":"#Loss Computation\nloss_func = nn.BCELoss()\n#Optimizer\nlearning_rate = 0.001\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nepochs = 50","76627e18":"model.train()\ntrain_loss = []\nfor epoch in range(epochs):\n    #Within each epoch run the subsets of data = batch sizes.\n    for xb, yb in train_dl:\n        y_pred = model(xb)            # Forward Propagation\n        loss = loss_func(y_pred, yb)  # Loss Computation\n        optimizer.zero_grad()         # Clearing all previous gradients, setting to zero \n        loss.backward()               # Back Propagation\n        optimizer.step()              # Updating the parameters \n    #print(\"Loss in iteration :\"+str(epoch)+\" is: \"+str(loss.item()))\n    train_loss.append(loss.item())\nprint('Last iteration loss value: '+str(loss.item()))","6eb3198c":"plt.plot(train_loss)\nplt.show()","774cb867":"import itertools\n\ny_pred_list = []\nmodel.eval()\n#Since we don't need model to back propagate the gradients in test set we use torch.no_grad()\n# reduces memory usage and speeds up computation\nwith torch.no_grad():\n    for xb_test,yb_test  in test_loader:\n        y_test_pred = model(xb_test)\n        y_pred_tag = torch.round(y_test_pred)\n        y_pred_list.append(y_pred_tag.detach().numpy())\n\n#Takes arrays and makes them list of list for each batch        \ny_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n#flattens the lists in sequence\nytest_pred = list(itertools.chain.from_iterable(y_pred_list))","d950c851":"y_true_test = test_y.values.ravel()\nconf_matrix = confusion_matrix(y_true_test ,ytest_pred)\nprint(\"Confusion Matrix of the Test Set\")\nprint(\"-----------\")\nprint(conf_matrix)\nprint(\"Precision of the MLP :\\t\"+str(precision_score(y_true_test,ytest_pred)))\nprint(\"Recall of the MLP    :\\t\"+str(recall_score(y_true_test,ytest_pred)))\nprint(\"F1 Score of the Model :\\t\"+str(f1_score(y_true_test,ytest_pred)))","6550f4a2":"#### Test Dataset prediction on trained NN","3887a8a8":"**Idenitfying Categorical columns** : If column has unique values lower than 0.002% of total records then categorizing it as Categorical","e0bdc663":"#### URL Basics\n\n![URL Structure](https:\/\/i.imgur.com\/elrD4Vl.png)\n\n* **Domain name** portion is constrained since it has to be registered with a domain name Registrar\n* **Subdomain name** and **Path** are fully controllable by the phisher","0556f662":"### Pytorch Neural Net Model\n**Convert data into Pytorch Tensors**","592b9b71":"### Background on the Dataset \ud83d\udcdd\n**What is Phishing ?**\n\nIt is a form of fraud in which the attacker tries to learn sensitive information such as login credentials or account information by pretending as a reputable entity or person via email or other communication channel means\n\nPhishing is popular among attackers, since it is easier to trick someone into clicking a malicious link which seems legitimate\n\nThe **URL** of phishing websites may be **very similar to real websites** to the human eye, but they are different in IP. ","742d8814":"**Note:**\n\n* Aspects such has multicollinearity , variance captured by the features etc are applicable to Linear & Tree models as their performance tends to vary with these aspects.Since we are progressing with a Multi Layer Perceptron (MLP) , a neural net doesn't really care about all these transformations and raw data is what it needs to find the underlying patterns on it's own","70399c3e":"### DataLoader to pass the data in batches to the model","02f669a9":"**Exploring Page Ranks feature w.r.t Target variable**\n\n* Legitimate URLs form more of a gaussian distribution as the ranking increases , whereas the Phishing ones present a right skewed distribution.Presence of differentiating patterns here","795d6174":"### MLP (Model)\nDefine the Layers , Activation function , Number of nodes for the MultiLayerPerceptron\n\n#### Structure of MLP\n\n* 2 Hidden Layers\n* Normalizing the batch data usign batchnorm in between each layer\n* Using ReLU Activation function between the layers\n* Using dropout before sending to output\n* Sigmoid at the output layer to make probabilities between 0 to 1","09a9f407":"#### Distribution of Mean values of the Numerical features across Target variable","abddeabc":"#### Splitting dataset into Train & Test","6c84f52a":"**Observations** :Few highly correlated features as one would expect.Nothing jumps out extraordinary here.\n\n*length_words_raw* is expected to be highly correlated with *length_url* . Similar behaviour for other correlated feature pairs\n","424e3192":"#### Correlation Plot for the Numerical(continuous) features\n","0e6f2470":"#### Quick Observations\n* **URL** itself is present as a field\n* Target Variable is : **status** with \n    * legitimate (considered as Label 1)\n    * phishing   (considered as Label 0)\n* **87 Features** around URL , **all Numerical** ,are from three different classes\n    * 56 extracted from the Structure and Syntax of URLs (fields starting with : *nb_* , *shortest\/longest* , *ratio\/length* etc)\n    * 24 extracted from the Content of their correspondent pages \n    * 7 are extracted by querying External Services","e8186735":"#### Metrics that matter\n\n* Precision\n* Recall\n* F1 Score\n* Confusion Matrix","a12da2bc":"So here we used a Neural Net for a Tabular data classification problem and got pretty good performance.\n\n**If you found this notebook helpful then please upvote , i'll be grateful** :-)","0280d021":"**Plotting the loss shows that model pretty much stabilized after 30 epochs itself**","3194544d":"### Phishing dataset Exploration","d773953e":"**Observations**\n\n* Higher the length of URL ,or words in URL then more likely to be *Phishing* URL\n* Clear distinctive pattern of *nb_links* field.Higher implies *Legitimate* URL\n* *links_in_tags ,safe_anchor*  higher volume signals more towards *Legitmate* site\n","0d9f0c2f":"#### Defining parameters (pretty much standard setting for Binary class problem)\n* Loss computation function : Here using Binary Cross Entropy (BCE) which is defacto for Binary class problems\n* Learning rate : Setting as 0.001 (can be optimized further)\n* Optimizer : Using Adam and\n* Epochs of Training : setting as 50","fbf1dccc":"### Training the MLP Model\nNN Steps flow\n\n* Forward Propagation\n* Loss computation\n* Backpropagation\n* Updating the parameters","a3fa6948":"------------------------------------------------------------------------\nThis notebook is a Multi Layer Perceptron(MLP) implementation for a Tabular data classification problem using Pytorch . Are **Neural Nets an overkill or do they lack performance on Tabular data** ? \n\nWell recently there has been a lot of buzz around utility of NN's in tabular data.In a study , Gradient boosting methods still outperform most scenarios of tabular data world . Related reading in this paper - [Deep Learning is not all you need](https:\/\/arxiv.org\/pdf\/2106.03253.pdf)\n\nPlease remember to **upvote this notebook** for encouragement , i'll be grateful :-)\n\n--------------------------------------------------------------------------","dea4d765":"**No Missing values in the dataset**"}}