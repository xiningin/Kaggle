{"cell_type":{"4ac49d6b":"code","fa98cbfa":"code","1a8f3926":"code","a09c0f9e":"code","6debf7d6":"code","6e45cba6":"code","540906dc":"code","3d6c0330":"code","a5f29b6f":"code","d5c1433b":"code","e051ec51":"code","75f69c7b":"code","e6a2fff5":"code","00cc9cd9":"code","e12512c1":"code","85e3be6e":"code","6040d522":"markdown","fb18a086":"markdown","a7085ac0":"markdown","bdd810eb":"markdown"},"source":{"4ac49d6b":"context=open(\"\/kaggle\/input\/story-file\/story.txt\",\"r\")\ncontext=context.read()\ncontext","fa98cbfa":"import torch\nfrom transformers import pipeline, AutoModelForQuestionAnswering, AutoTokenizer\nimport pandas as pd\nimport matplotlib.pyplot as plt","1a8f3926":"model_list=[\"bert-large-cased-whole-word-masking-finetuned-squad\",\"distilbert-base-cased-distilled-squad\",\"bert-base-cased\"]\n\ncontext=context\n                                                                      \ndef load_model(model_name): #Fonction qui demande le mod\u00e8le a utiliser et qui permets de le charger.\n    if model_name in model_list:\n        global model\n        global tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(model_name) # Etape de tokenization\n        model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n        return True\n    else:\n        print(\"No model with that name\")\n        return None\n\ndef answer_function(question,verbose=True): #Fonction qui trouve la reponse a la question pos\u00e9e\n    #Encodage des tokens\n    inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"].tolist()[0] #Listes des encodages\n\n    #text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n    answer_start_scores=model(**inputs)[\"start_logits\"]\n    answer_end_scores = model(**inputs)[\"end_logits\"]\n\n    answer_start = torch.argmax(answer_start_scores)  # Get the most likely beginning of answer with the argmax of the score\n    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n\n    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n    soft=torch.nn.Softmax(dim=1)\n    if verbose:\n        print('Start score : '+str(soft(answer_start_scores).max().item()))\n        print('End score : '+str(soft(answer_end_scores).max().item()))\n\n    return(answer)","a09c0f9e":"#Script\ndef script_function():\n    model_loaded=None\n    while(True):\n        if model_loaded==None:\n            print(\"What model to use ?\\n\")\n            model_name=input()\n            model_loaded=load_model(model_name)\n        else:\n            print(\"Any question ?\")\n            text=input()\n            if text==\"No\":\n                print(\"Ok goodbye\")\n                break\n            else:\n                print(answer_function(text)+\"\\n\")","6debf7d6":"script_function()","6e45cba6":"question=\"Who are you ?\"\ninputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\") \n# Tokenization en concat\u00e9nant pour l'input la question et le contexte\n\ninput_ids = inputs[\"input_ids\"].tolist()[0]\n#extrait la liste des ids de chaque token\n\ntext_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n#pour visualiser le texte correspondant aux tokens\n\nanswer_start_scores=model(**inputs)[\"start_logits\"]\n#logits\/score pour chaque token en tant que \"d\u00e9but de r\u00e9ponse\" avant application de la fonction d'activation\nanswer_end_scores = model(**inputs)[\"end_logits\"]\n#logits\/score pour chaque token en tant que \"fin de r\u00e9ponse\" avant application de la fonction d'activation\n\nanswer_start = torch.argmax(answer_start_scores)  \nanswer_end = torch.argmax(answer_end_scores) + 1\n\nanswer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n#Re-conversion en texte","540906dc":"iptid=pd.DataFrame(input_ids, columns=[\"encodage\"])\ntknz=pd.DataFrame(text_tokens,columns=[\"token\"])\ndt=pd.concat([iptid,tknz],axis=1)\ndt.head(20).T","3d6c0330":"answer_start_scores=model(**inputs)[\"start_logits\"]\nanswer_end_scores = model(**inputs)[\"end_logits\"]\n\nplt.figure(figsize=(20,10))\n\nplt.scatter(range(30),answer_start_scores.tolist()[0][:30])\nfor i in range(30):\n    plt.text(i,0,text_tokens[i],fontsize=14)\n","a5f29b6f":"answer_start = torch.argmax(answer_start_scores)  \nanswer_end = torch.argmax(answer_end_scores) + 1\n\nplt.figure(figsize=(20,10))\n\nplt.scatter(range(170),answer_start_scores.tolist()[0])\nplt.scatter(range(170),answer_end_scores.tolist()[0])\nfor i in range(170):\n    plt.text(i,answer_start_scores.tolist()[0][i],text_tokens[i])\nplt.vlines([answer_start,answer_end],ymin=-12,ymax=12);\n\nplt.figure(figsize=(20,10))\n\nplt.scatter(range(30),answer_start_scores.tolist()[0][:30])\nplt.scatter(range(30),answer_end_scores.tolist()[0][:30])\nfor i in range(30):\n    plt.text(i,0,text_tokens[i],fontsize=14)\nplt.vlines([answer_start,answer_end],ymin=-12,ymax=12);","d5c1433b":"model_list=[\"bert-large-cased-whole-word-masking-finetuned-squad\",\"distilbert-base-cased-distilled-squad\",\"bert-base-cased\"]\nquestions=[\"Who are you ?\",\"When was you born ?\",\"What did you find ?\", \"What did you decide ?\"]\n\nfor i,model_name in enumerate(model_list):\n    print(model_name)\n    load_model(model_name)\n    print(\"Model Loaded\")\n    \n    question_test=\"Who are you ?\"\n    inputs = tokenizer.encode_plus(question_test, context, add_special_tokens=True, return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"].tolist()[0]\n    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n    \n    if i==0:\n        iptid=pd.DataFrame(input_ids, columns=[\"encodage\"+model_name])\n        tknz=pd.DataFrame(text_tokens,columns=[\"token\"+model_name])\n        dt=pd.concat([iptid,tknz],axis=1)        \n    else:\n        iptid=pd.DataFrame(input_ids, columns=[\"encodage\"+model_name])\n        tknz=pd.DataFrame(text_tokens,columns=[\"token\"+model_name])\n        dt=pd.concat([dt,iptid,tknz],axis=1)\n        \n    answers=[]\n    for question in questions:\n        print(question)\n        answers.append(answer_function(question,verbose=False))\n     \n    print(answers)\n    \n    ansdf=pd.DataFrame(answers, columns=[model_name])\n    print(ansdf)\n    \n    if i==0:\n        df=ansdf\n    else:\n        df=pd.concat([df,ansdf],axis=1)","e051ec51":"dt.head(20).T","75f69c7b":"df.index=questions","e6a2fff5":"df","00cc9cd9":"nlp = pipeline('question-answering')","e12512c1":"context = context\nprint(nlp(question=\"Who are you ?\", context=context))","85e3be6e":"print(nlp(question=\"When was you born?\", context=context))","6040d522":"# TP7 - DeepLearning\n## Transformers for question answering\n> Elodie - Alexandra - Sonico - Raphael - Alissa\n### Objectif : \n> Extraire des r\u00e9ponses pertinentes \u00e0 des questions en se basant sur un texte.\n### Outils : \n> Biblioth\u00e8que **\u201ctransformers\u201d** de Huggingface : https:\/\/huggingface.co\/transformers\/\n### Importation du texte","fb18a086":"# Step by step","a7085ac0":"# Petit Test avec la fonction pipeline","bdd810eb":"# Comparaison des mod\u00e8les"}}