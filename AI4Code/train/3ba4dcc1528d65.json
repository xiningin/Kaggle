{"cell_type":{"7b270ea4":"code","f36e3abc":"code","cb3a4704":"code","32ee4827":"code","bc6b28d8":"code","58527d44":"code","beb910ac":"code","6cda8206":"code","d980c4ac":"code","2bda8aef":"code","65e7ebfc":"code","b08e3cc2":"code","0ee7bfb5":"code","a54a8431":"code","2c085f89":"code","2e04c564":"code","f323a72c":"code","68c17774":"code","fdda818d":"code","45453bd5":"code","ea835515":"code","5b32ffd5":"code","8961003b":"code","5634294f":"code","5f638104":"code","4a148617":"code","0859414e":"code","65f15e4e":"code","7722c587":"code","c1adb17a":"code","11ecb2d2":"code","987dc3cc":"code","098f457a":"code","b287e99e":"code","6d55d5ca":"code","0d7758c7":"code","24cf2414":"code","418de850":"code","06107bf1":"code","d2f43768":"code","c4b2d9a4":"code","4b395e49":"code","cb044fd3":"code","d6c5aa5a":"code","f678fb0e":"code","a8f2599d":"code","eeab20ae":"code","7fbbe29e":"code","ef974c1f":"code","716dc072":"code","05a210c7":"code","b0010702":"code","f8a5b1ed":"code","f707da9c":"code","dd435501":"code","2202f785":"code","57635071":"code","21468861":"code","7633233d":"markdown","c586a13b":"markdown","cee8d9f9":"markdown","f83925ba":"markdown","58fa5928":"markdown","813b2ecd":"markdown","972bdc01":"markdown","435c130a":"markdown","6ff92cc7":"markdown","a4c84fbf":"markdown","11a86d3f":"markdown","6ae031a3":"markdown","f5f75928":"markdown","3b4ffdff":"markdown","1b17ec12":"markdown","ae6bacb2":"markdown","aad1fa7e":"markdown","3731c994":"markdown","d068775e":"markdown","7e5d620c":"markdown","40b60fc5":"markdown","c01b0f60":"markdown","b07458b9":"markdown","aa7ac03d":"markdown","681e2bd2":"markdown","b45f539c":"markdown","5b8736ef":"markdown","cb67172b":"markdown","acd62515":"markdown","ed9cfc19":"markdown","25701ea6":"markdown","61eaf290":"markdown","ab28e9f8":"markdown","da8009d5":"markdown"},"source":{"7b270ea4":"import numpy as np\nimport pandas as pd \nimport plotly.express as px\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom iso3166 import countries\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot","f36e3abc":"df = pd.read_csv(\"\/kaggle\/input\/covid19-tweets\/covid19_tweets.csv\")","cb3a4704":"df.head()","32ee4827":"df.info()","bc6b28d8":"missed = pd.DataFrame()\nmissed['column'] = df.columns\n\nmissed['percent'] = [round(100* df[col].isnull().sum() \/ len(df), 2) for col in df.columns]\nmissed = missed.sort_values('percent')\nmissed = missed[missed['percent']>0]\n\nfig = px.bar(\n    missed, \n    x='percent', \n    y=\"column\", \n    orientation='h', \n    title='Missed values percent for every column (percent > 0)', \n    height=400, \n    width=600\n)\n\nfig.show()","58527d44":"ds = df['user_name'].value_counts().reset_index()\nds.columns = ['user_name', 'tweets_count']\nds = ds.sort_values(['tweets_count'])\n\nfig = px.bar(\n    ds.tail(40), \n    x=\"tweets_count\", \n    y=\"user_name\", \n    orientation='h', \n    title='Top 40 users by number of tweets', \n    width=800, \n    height=800\n)\n\nfig.show()","beb910ac":"df = pd.merge(df, ds, on='user_name')","6cda8206":"data = df.sort_values('user_followers', ascending=False)\ndata = data.drop_duplicates(subset='user_name', keep=\"first\")\ndata = data[['user_name', 'user_followers', 'tweets_count']]\ndata = data.sort_values('user_followers')\n\nfig = px.bar(\n    data.tail(40), \n    x=\"user_followers\", \n    y=\"user_name\", \n    color='tweets_count',\n    orientation='h', \n    title='Top 40 users by number of followers', \n    width=800, \n    height=800\n)\n\nfig.show()","d980c4ac":"data = df.sort_values('user_friends', ascending=False)\ndata = data.drop_duplicates(subset='user_name', keep=\"first\")\ndata = data[['user_name', 'user_friends', 'tweets_count']]\ndata = data.sort_values('user_friends')\n\nfig = px.bar(\n    data.tail(40), \n    x=\"user_friends\", \n    y=\"user_name\", \n    color = 'tweets_count',\n    orientation='h', \n    title='Top 40 users by number of friends', \n    width=800, \n    height=800\n)\n\nfig.show()","2bda8aef":"df['user_created'] = pd.to_datetime(df['user_created'])\ndf['year_created'] = df['user_created'].dt.year\ndata = df.drop_duplicates(subset='user_name', keep=\"first\")\ndata = data[data['year_created']>1970]\ndata = data['year_created'].value_counts().reset_index()\ndata.columns = ['year', 'number']\n\nfig = px.bar(\n    data, \n    x=\"year\", \n    y=\"number\", \n    orientation='v', \n    title='User created year by year', \n    width=800, \n    height=600\n)\n\nfig.show()","65e7ebfc":"df.head(10)","b08e3cc2":"ds = df['user_location'].value_counts().reset_index()\nds.columns = ['user_location', 'count']\nds = ds[ds['user_location']!='NA']\nds = ds.sort_values(['count'])\n\nfig = px.bar(\n    ds.tail(40), \n    x=\"count\", \n    y=\"user_location\", \n    orientation='h', title='Top 40 user locations by number of tweets', \n    width=800, \n    height=800\n)\n\nfig.show()","0ee7bfb5":"def pie_count(data, field, percent_limit, title):\n    \n    data[field] = data[field].fillna('NA')\n    data = data[field].value_counts().to_frame()\n\n    total = data[field].sum()\n    data['percentage'] = 100 * data[field]\/total    \n\n    percent_limit = percent_limit\n    otherdata = data[data['percentage'] < percent_limit] \n    others = otherdata['percentage'].sum()  \n    maindata = data[data['percentage'] >= percent_limit]\n\n    data = maindata\n    other_label = \"Others(<\" + str(percent_limit) + \"% each)\"\n    data.loc[other_label] = pd.Series({field:otherdata[field].sum()}) \n    \n    labels = data.index.tolist()   \n    datavals = data[field].tolist()\n    \n    trace=go.Pie(labels=labels,values=datavals)\n\n    layout = go.Layout(\n        title = title,\n        height=600,\n        width=600\n        )\n    \n    fig = go.Figure(data=[trace], layout=layout)\n    iplot(fig)\n    \npie_count(df, 'user_location', 0.5, 'Number of tweets per location')","a54a8431":"ds = df['source'].value_counts().reset_index()\nds.columns = ['source', 'count']\nds = ds.sort_values(['count'])\n\nfig = px.bar(\n    ds.tail(40), \n    x=\"count\", \n    y=\"source\", \n    orientation='h', \n    title='Top 40 user sources by number of tweets', \n    width=800, \n    height=800\n)\n\nfig.show()","2c085f89":"df['hashtags'] = df['hashtags'].fillna('[]')\ndf['hashtags_count'] = df['hashtags'].apply(lambda x: len(x.split(',')))\ndf.loc[df['hashtags'] == '[]', 'hashtags_count'] = 0\n\ndf.head(10)","2e04c564":"df['hashtags_count'].describe()","f323a72c":"fig = px.scatter(\n    df, \n    x=df['hashtags_count'], \n    y=df['tweets_count'], \n    height=700,\n    width=700,\n    title='Total number of tweets for users and number of hashtags in every tweet'\n)\n\nfig.show()","68c17774":"ds = df['hashtags_count'].value_counts().reset_index()\nds.columns = ['hashtags_count', 'count']\nds = ds.sort_values(['count'])\nds['hashtags_count'] = ds['hashtags_count'].astype(str) + ' tags'\n\nfig = px.bar(\n    ds, \n    x=\"count\", \n    y=\"hashtags_count\", \n    orientation='h', \n    title='Distribution of number of hashtags in tweets', \n    width=800, \n    height=600\n)\n\nfig.show()","fdda818d":"ds = df[df['tweets_count']>10]\nds = ds.groupby(['user_name', 'tweets_count'])['hashtags_count'].mean().reset_index()\nds.columns = ['user', 'tweets_count', 'mean_count']\nds = ds.sort_values(['mean_count'])\n\nfig = px.bar(\n    ds.tail(40), \n    x=\"mean_count\", \n    y=\"user\", \n    color='tweets_count',\n    orientation='h', \n    title='Top 40 users with higher mean number of hashtags (at least 10 tweets per user)', \n    width=800, \n    height=800\n)\n\nfig.show()","45453bd5":"df['date'] = pd.to_datetime(df['date']) \ndf = df.sort_values(['date'])\ndf['day'] = df['date'].astype(str).str.split(' ', expand=True)[0]\ndf['time'] = df['date'].astype(str).str.split(' ', expand=True)[1]\ndf.head()","ea835515":"ds = df.groupby(['day', 'user_name'])['hashtags_count'].count().reset_index()\nds = ds.groupby(['day'])['user_name'].count().reset_index()\nds.columns = ['day', 'number_of_users']\nds['day'] = ds['day'].astype(str) + ':00:00:00'\nfig = px.bar(\n    ds, \n    x='day', \n    y=\"number_of_users\", \n    orientation='v',\n    title='Number of unique users per day', \n    width=800, \n    height=800\n)\nfig.show()","5b32ffd5":"ds = df['day'].value_counts().reset_index()\nds.columns = ['day', 'count']\nds = ds.sort_values('count')\nds['day'] = ds['day'].astype(str) + ':00:00:00'\nfig = px.bar(\n    ds, \n    x='count', \n    y=\"day\", \n    orientation='h',\n    title='Tweets distribution over days present in dataset', \n    width=800, \n    height=800\n)\nfig.show()","8961003b":"df['hour'] = df['date'].dt.hour\nds = df['hour'].value_counts().reset_index()\nds.columns = ['hour', 'count']\nds['hour'] = 'Hour ' + ds['hour'].astype(str)\nfig = px.bar(\n    ds, \n    x=\"hour\", \n    y=\"count\", \n    orientation='v', \n    title='Tweets distribution over hours', \n    width=800\n)\nfig.show()","5634294f":"def split_hashtags(x): \n    return str(x).replace('[', '').replace(']', '').split(',')\n\ntweets_df = df.copy()\ntweets_df['hashtag'] = tweets_df['hashtags'].apply(lambda row : split_hashtags(row))\ntweets_df = tweets_df.explode('hashtag')\ntweets_df['hashtag'] = tweets_df['hashtag'].astype(str).str.lower().str.replace(\"'\", '').str.replace(\" \", '')\ntweets_df.loc[tweets_df['hashtag']=='', 'hashtag'] = 'NO HASHTAG'\ntweets_df","5f638104":"ds = tweets_df['hashtag'].value_counts().reset_index()\nds.columns = ['hashtag', 'count']\nds = ds.sort_values(['count'])\nfig = px.bar(\n    ds.tail(20), \n    x=\"count\", \n    y='hashtag', \n    orientation='h', \n    title='Top 20 hashtags', \n    width=800, \n    height=700\n)\nfig.show()","4a148617":"df['tweet_length'] = df['text'].str.len()","0859414e":"fig = px.histogram(\n    df, \n    x=\"tweet_length\", \n    nbins=80, \n    title='Tweet length distribution', \n    width=800,\n    height=700\n)\nfig.show()","65f15e4e":"ds = df[df['tweets_count']>=10]\nds = ds.groupby(['user_name', 'tweets_count'])['tweet_length'].mean().reset_index()\nds.columns = ['user_name', 'tweets_count', 'mean_length']\nds = ds.sort_values(['mean_length'])\nfig = px.bar(\n    ds.tail(40), \n    x=\"mean_length\", \n    y=\"user_name\", \n    color='tweets_count',\n    orientation='h', \n    title='Top 40 users with the longest average length of tweet (at least 10 tweets)', \n    width=800, \n    height=800\n)\nfig.show()","7722c587":"ds = df[df['tweets_count']>=10]\nds = ds.groupby(['user_name', 'tweets_count'])['tweet_length'].mean().reset_index()\nds.columns = ['user_name', 'tweets_count', 'mean_length']\nds = ds.sort_values(['mean_length'])\nfig = px.bar(\n    ds.head(40), \n    x=\"mean_length\", \n    y=\"user_name\", \n    color='tweets_count',\n    orientation='h', \n    title='Top 40 users with the shortest average length of tweet (at least 10 tweets)', \n    width=800, \n    height=800\n)\nfig.show()","c1adb17a":"def build_wordcloud(df, title):\n    wordcloud = WordCloud(\n        background_color='gray', \n        stopwords=set(STOPWORDS), \n        max_words=50, \n        max_font_size=40, \n        random_state=666\n    ).generate(str(df))\n\n    fig = plt.figure(1, figsize=(14,14))\n    plt.axis('off')\n    fig.suptitle(title, fontsize=16)\n    fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","11ecb2d2":"build_wordcloud(df['text'], 'Prevalent words in tweets for all dataset')","987dc3cc":"test_df = df[df['user_name']=='GlobalPandemic.NET']\nbuild_wordcloud(test_df['text'], 'Prevalent words in tweets for GlobalPandemic.NET')","098f457a":"test_df = df[df['user_name']=='covidnews.ch']\nbuild_wordcloud(test_df['text'], 'Prevalent words in tweets for covidnews.ch')","b287e99e":"test_df = df[df['user_name']=='Open Letters']\nbuild_wordcloud(test_df['text'], 'Prevalent words in tweets for Open Letters')","6d55d5ca":"test_df = df[df['user_name']=='Hindustan Times']\nbuild_wordcloud(test_df['text'], 'Prevalent words in tweets for Hindustan Times')","0d7758c7":"test_df = df[df['user_name']=='Blood Donors India']\nbuild_wordcloud(test_df['text'], 'Prevalent words in tweets for Blood Donors India')","24cf2414":"build_wordcloud(df['user_description'], 'Prevalent words in tweets for Blood Donors India')","418de850":"vec = TfidfVectorizer(stop_words=\"english\")\nvec.fit(df['text'].values)\nfeatures = vec.transform(df['text'].values)","06107bf1":"kmeans = KMeans(n_clusters=2, random_state=0)\nkmeans.fit(features)","d2f43768":"res = kmeans.predict(features)\ndf['Cluster'] = res\ndf","c4b2d9a4":"df[df['Cluster'] == 0].head(20)['text'].tolist()","4b395e49":"df[df['Cluster'] == 1].head(20)['text'].tolist()","cb044fd3":"print('Number of samples for class 0: ', len(df[df['Cluster'] == 0]))\nprint('Number of samples for class 1: ', len(df[df['Cluster'] == 1]))","d6c5aa5a":"build_wordcloud(df[df['Cluster'] == 0]['text'], 'Wordcloud for cluster 0')","f678fb0e":"build_wordcloud(df[df['Cluster'] == 1]['text'], 'Wordcloud for cluster 1')","a8f2599d":"kmeans = KMeans(n_clusters=5, random_state=0)\nkmeans.fit(features)","eeab20ae":"res = kmeans.predict(features)\ndf['Cluster5'] = res\ndf","7fbbe29e":"for i in range(5):\n    print('Number of samples for class ' + str(i) + ': ', len(df[df['Cluster5'] == i]))","ef974c1f":"build_wordcloud(df[df['Cluster5'] == 0]['text'], 'Wordcloud for cluster 0')","716dc072":"build_wordcloud(df[df['Cluster5'] == 1]['text'], 'Wordcloud for cluster 1')","05a210c7":"build_wordcloud(df[df['Cluster5'] == 2]['text'], 'Wordcloud for cluster 2')","b0010702":"build_wordcloud(df[df['Cluster5'] == 3]['text'], 'Wordcloud for cluster 3')","f8a5b1ed":"build_wordcloud(df[df['Cluster5'] == 4]['text'], 'Wordcloud for cluster 4')","f707da9c":"df['location'] = df['user_location'].str.split(',', expand=True)[1].str.lstrip().str.rstrip()\nres = df.groupby(['day', 'location'])['text'].count().reset_index()","dd435501":"country_dict = {}\nfor c in countries:\n    country_dict[c.name] = c.alpha3\n    \nres['alpha3'] = res['location']\nres = res.replace({\"alpha3\": country_dict})\n\ncountry_list = ['England', 'United States', 'United Kingdom', 'London', 'UK']\n\nres = res[\n    (res['alpha3'] == 'USA') | \n    (res['location'].isin(country_list)) | \n    (res['location'] != res['alpha3'])\n]\n\ngbr = ['England', 'UK', 'London', 'United Kingdom']\nus = ['United States', 'NY', 'CA', 'GA']\n\nres = res[res['location'].notnull()]\nres.loc[res['location'].isin(gbr), 'alpha3'] = 'GBR'\nres.loc[res['location'].isin(us), 'alpha3'] = 'USA'\nres.loc[res['alpha3'] == 'USA', 'location'] = 'USA'\nres.loc[res['alpha3'] == 'GBR', 'location'] = 'United Kingdom'\nplot = res.groupby(['day', 'location', 'alpha3'])['text'].sum().reset_index()\nplot","2202f785":"fig = px.choropleth(\n    plot, \n    locations=\"alpha3\",\n    hover_name='location',\n    color=\"text\",\n    animation_frame='day',\n    projection=\"natural earth\",\n    color_continuous_scale=px.colors.sequential.Plasma,\n    title='Tweets from different countries for every day',\n    width=800, \n    height=600\n)\nfig.show()","57635071":"res = df.groupby(['day', 'location', 'user_name'])['text'].count().reset_index()\nres = res[['day', 'location', 'user_name']]\nres['alpha3'] = res['location']\nres = res.replace({\"alpha3\": country_dict})\n\ncountry_list = ['England', 'United States', 'United Kingdom', 'London', 'UK']\n\nres = res[\n    (res['alpha3'] == 'USA') | \n    (res['location'].isin(country_list)) | \n    (res['location'] != res['alpha3'])\n]\n\ngbr = ['England', 'UK', 'London', 'United Kingdom']\nus = ['United States', 'NY', 'CA', 'GA']\n\nres = res[res['location'].notnull()]\nres.loc[res['location'].isin(gbr), 'alpha3'] = 'GBR'\nres.loc[res['location'].isin(us), 'alpha3'] = 'USA'\nres.loc[res['alpha3'] == 'USA', 'location'] = 'USA'\nres.loc[res['alpha3'] == 'GBR', 'location'] = 'United Kingdom'\nplot = res.groupby(['day', 'location', 'alpha3'])['user_name'].count().reset_index()","21468861":"fig = px.choropleth(\n    plot, \n    locations=\"alpha3\",\n    hover_name='location',\n    color=\"user_name\",\n    animation_frame='day',\n    projection=\"natural earth\",\n    color_continuous_scale=px.colors.sequential.Plasma,\n    title='Numbers of active users for every day',\n    width=800, \n    height=600\n)\nfig.show()","7633233d":"Let's see percent of NaNs for every column. We will visualize only columns with at least 1 missed value.","c586a13b":"Now we will see top 40 users that like to use hashtags a little bit more than others. ","cee8d9f9":"And most friendly users.","f83925ba":"### Lets do the same but for hours","58fa5928":"<a id=\"4\"><\/a>\n<h2 style='background:green; border:0; color:white'><center>Tweets text analysis<\/center><h2>\n\n### Here we are going to check the `text` feature of the dataset.\n### Lets see general wordcloud for this column.","813b2ecd":"### Let's check more clusters for example 5.","972bdc01":"### Here I am going to show approach how to use plotly world map to demonstrate geographical distribution of tweets.","435c130a":"## So we can see that cluster 0 contains more or less positive tweets, but cluster 1 contains tweets with information about new cases, reports and regions.","6ff92cc7":"Lets create new feature - `hashtags_count` that will show us how many hashtags in the current tweet.","a4c84fbf":"### And show top 20 hashtags on tweets.","11a86d3f":"As we can see from chart coronavirus increases the number of new twitter users.","6ae031a3":"<a id=\"5\"><\/a>\n<h2 style='background:green; border:0; color:white'><center>Simple sentiment analysis<\/center><h2>\n\n### Lets do simple version of sentiment analysis. We just use Tfidf Vectorizer to get features and use Kmeans clustering algotithm to split data into 2 clusters.","f5f75928":"Let's do a first quick check of our dataset.","3b4ffdff":"<h1><center>Covid19 tweets. EDA. Visualization. Insides.<\/center><\/h1>\n\n<center><img src=\"https:\/\/ichef.bbci.co.uk\/news\/1024\/cpsprodpb\/031C\/production\/_112869700_gettyimages-1209519827-1.jpg\"><\/center>","1b17ec12":"### Lets split hashtags into separate column.","ae6bacb2":"### Now we are going to calculate the length for every tweet in dataset.","aad1fa7e":"And also we can see the pie plot for the full picture about users locations.","3731c994":"Let's see top 40 users by number of tweets.","d068775e":"### Let's also visualize WordCloud for user's description.","7e5d620c":"### Just split day and time into separate columns","40b60fc5":"Let's see top 40 most popular locations by the number of tweets.","c01b0f60":"### Lets see world clouds for top 5 users.","b07458b9":"Distribution of new feature over the number of tweets is expected - a lot of tweets with few number of hashtags and few tweets with huge number of hashtags.","aa7ac03d":"And see the values for new created column.","681e2bd2":"<a id=\"6\"><\/a>\n<h2 style='background:green; border:0; color:white'><center>Animation with geographical distribution of tweets<\/center><h2>","b45f539c":"### Now we are going to check how many tweets were for every day in our dataset.","5b8736ef":"### Hello everyone! Here I am going to present some basic analysis of this dataset. We will create some plots based on existing features, do starting sentiment analysis (based on clustering). Also we will create world  map animation and prepare a lot of other interesting things! Let's start!\n\n<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:green; border:0' role=\"tab\" aria-controls=\"home\"><center>Quick navigation<\/center><\/h3>\n\n* [1. Dataset Quick Overview](#1)\n* [2. Data Visualization](#2)\n* [3. Additional features analysis](#3)\n* [4. Tweets text analysis](#4)\n* [5. Simple sentiment analysis](#5)\n* [6. Animation with geographical distribution of tweets](#6)\n\n\n#### If you are interested in Dynamic monitoring of the tweets, please check another one my kernel: https:\/\/www.kaggle.com\/isaienkov\/covid19-dynamic-in-time-and-space-of-the-tweets","cb67172b":"Let's see most popular users.","acd62515":"### Number of unique users per day","ed9cfc19":"<a id=\"3\"><\/a>\n<h2 style='background:green; border:0; color:white'><center>3. Additional features analysis<center><h2>","25701ea6":"Let's see how coronavirus affect to new users creation.","61eaf290":"<a id=\"2\"><\/a>\n<h2 style='background:green; border:0; color:white'><center>2. Data Visualization<\/center><\/h2>","ab28e9f8":"<a id=\"1\"><\/a>\n<h2 style='background:green; border:0; color:white'><center>1. Dataset Quick Overview<\/center><h2>","da8009d5":"Now it's time to check last one categorical feature - `source`. Lets see top 40 sources by the number of tweets."}}