{"cell_type":{"a280651f":"code","f6f1efe3":"code","65d49552":"code","55340a05":"code","3d42f853":"code","729e7e5d":"code","c957e7e9":"code","49b93cdc":"code","50b899d0":"code","8c3385ca":"code","afe89c28":"code","e26797bb":"code","2587dddb":"code","cf403f27":"code","0b8963cb":"markdown","16d71691":"markdown","504f8ec0":"markdown","9c6bb154":"markdown","38e7fe73":"markdown","235cb429":"markdown","f8213388":"markdown","e96d3846":"markdown"},"source":{"a280651f":"#Importing Necessary Libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","f6f1efe3":"#Importing dataset\ncol = [ 'Class Name','Left weight','Left distance','Right weight','Right distance']\ndf = pd.read_csv('..\/input\/balancescale\/balance-scale.data',names=col,sep=',')\n","65d49552":"df.head()","55340a05":"df.info()","3d42f853":"sns.countplot(df['Class Name'])","729e7e5d":"sns.countplot(df['Left weight'],hue=df['Class Name'])","c957e7e9":"from sklearn.model_selection import train_test_split\nX = df.drop('Class Name',axis=1)\ny = df[['Class Name']]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,random_state=42)\n","49b93cdc":"from sklearn.tree import DecisionTreeClassifier\nclf_model = DecisionTreeClassifier(criterion=\"gini\", random_state=42,max_depth=3, min_samples_leaf=5)   \nclf_model.fit(X_train,y_train)","50b899d0":"y_predict = clf_model.predict(X_test)","8c3385ca":"from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\naccuracy_score(y_test,y_predict)","afe89c28":"target = list(df['Class Name'].unique())\nfeature_names = list(X.columns)","e26797bb":"from sklearn import tree\nimport graphviz\ndot_data = tree.export_graphviz(clf_model,\n                                out_file=None, \n                      feature_names=feature_names,  \n                      class_names=target,  \n                      filled=True, rounded=True,  \n                      special_characters=True)  \ngraph = graphviz.Source(dot_data)  \n\ngraph","2587dddb":"from sklearn.tree import export_text\nr = export_text(clf_model, feature_names=feature_names)\nprint(r)","cf403f27":"graph.save('graph1.jpg')","0b8963cb":"**Plotting Decision Tree**","16d71691":"# What is Decision Tree?\n\nthe algorithm uses training data to create rules that can be represented by a tree structure.\n\n![image.png](attachment:d8526dff-1725-40e9-9d2b-8f6c0905ec74.png)!\n\n**Making of Decision Tree**\n\nFor making a decision tree, at each level we have to make a selection of the attributes to be the root node. This is known as attributes selection. This is mainly done using :\n\nGini index.\n\nInformation gain.\n\nchi-square.","504f8ec0":"**Exploratory Data Analysis**","9c6bb154":"**Splitting Dataset into Training and Testing set**\n","38e7fe73":"**Training the Decision Tree Classifier**","235cb429":"**Test Accuracy**","f8213388":"# We will show the example of the decision tree classifier in Sklearn by using the Balance-Scale dataset. The goal of this problem is to predict whether the balance scale will tilt to left or right based on the weights on the two sides.","e96d3846":"**Reading Dataset**"}}