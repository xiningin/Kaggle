{"cell_type":{"c48a2f7f":"code","2a2db399":"code","06742b5f":"code","9fa5c396":"code","61df3f34":"code","b5e59e5d":"code","1313962f":"code","2ac94876":"code","8af00083":"code","d64e7bc7":"code","16614c73":"code","7ddeffb0":"code","968c69ac":"code","354a49ab":"code","1f4e053d":"code","90a390a7":"code","ce91882b":"code","365982a9":"code","8479d7d2":"code","92486c0b":"code","c2f5a497":"code","73d3cb60":"code","dbe09369":"code","afd5cc2c":"code","e5d12417":"code","9f5b8ad7":"code","da6e48c0":"code","ba1c7214":"code","9904f92a":"code","bc6b9ba2":"code","5c6fcbec":"code","6bd75afa":"code","8712e9e4":"code","ceac44fe":"code","db835650":"code","3ea766d4":"code","bd850493":"code","dbd2a0af":"code","4c5ce082":"code","7a4b8d1c":"code","fc5671c9":"code","1883fd3d":"code","44023669":"code","07bd2897":"code","615a72be":"code","875e83ec":"code","18a10a20":"code","ae0fa4b6":"code","72f7ccf6":"code","75d434b0":"code","e6722dca":"code","4c12e47f":"code","495ec65c":"code","d741a966":"code","e67081f6":"code","4f65fff8":"code","bb62ffb8":"code","53f18fba":"code","6b8b5a53":"code","12cbbe6a":"code","b81768d4":"code","07b0c37d":"code","d9b8701b":"code","959a33ee":"code","7e3a4895":"code","dc3fe1c8":"code","812130d5":"code","6c41a840":"code","67e6805b":"code","a4990cec":"code","071b0549":"code","f036f68f":"code","37f12747":"code","5e098c00":"code","e8bd8a3f":"code","67879236":"code","94573fa5":"code","8f1b387b":"code","90d4df7a":"code","9aee8986":"code","07e60697":"code","f48f6d0c":"code","bd7acfc8":"code","5ac85509":"code","fa647b39":"code","acb2542e":"code","08fcda8d":"code","c7c395d8":"code","daa0ae37":"code","97607a6e":"code","1fb73377":"code","3ae4fdba":"code","959262ac":"code","5b951abb":"code","baa941f0":"code","c7eda35a":"code","7370def9":"code","3cc115b7":"code","348c4c83":"code","834a59d4":"code","0e5e74a0":"code","ae1f817c":"code","51a64780":"code","c0494430":"code","fa41324b":"markdown","5052e422":"markdown","d5fcf5f5":"markdown","308b6b3f":"markdown","554cf1af":"markdown","8ff1e9bd":"markdown","6912e7c5":"markdown","e2159d7e":"markdown","d0397f56":"markdown","4a95c247":"markdown","a9e9dd6d":"markdown","eea2116d":"markdown","67247b91":"markdown","118ae974":"markdown","72165d1b":"markdown","5cf89c13":"markdown","32db756e":"markdown","444775d8":"markdown","cb5e10ab":"markdown","1346e684":"markdown","a0ff6870":"markdown","d93bdfa8":"markdown","44977427":"markdown","c89fe2f2":"markdown","38537ba9":"markdown","f7388119":"markdown","d481a942":"markdown","1c826fea":"markdown","abd71a16":"markdown","bf2cbc80":"markdown","e64ed245":"markdown","92dfc941":"markdown","3ce67a5e":"markdown","526c1e14":"markdown","80937815":"markdown","449590a3":"markdown","fa891426":"markdown","ca695ff0":"markdown","7e0f9dff":"markdown","c40ac1d9":"markdown","603271e5":"markdown","15489ff0":"markdown","df1677cc":"markdown","d655cfca":"markdown","4b2d2d3e":"markdown","eef0fb10":"markdown","9f26b27e":"markdown","3166f7c0":"markdown","b104c873":"markdown","a7586907":"markdown","98ad159c":"markdown","df31b19e":"markdown","580f25ff":"markdown","6bebfc26":"markdown","0761a634":"markdown","ccfe0eb8":"markdown","233a43f4":"markdown","799fb92a":"markdown","a50dbb0a":"markdown","2de4780d":"markdown","3862dd64":"markdown","56a5d613":"markdown","9fb6d8c2":"markdown","db3877a0":"markdown","6aa74443":"markdown","ff350e1f":"markdown","97d25c79":"markdown","1256f9bc":"markdown","bb131dce":"markdown","33efe777":"markdown","7d69914f":"markdown","2ac3287b":"markdown","9cb32373":"markdown","6fb4e7d7":"markdown","85f9fae0":"markdown","1d6aeeda":"markdown","56729cb5":"markdown","ba759c33":"markdown","6630375d":"markdown","3e6b9c92":"markdown","6f00e5dd":"markdown","de7f44c8":"markdown"},"source":{"c48a2f7f":"# pip install optuna","2a2db399":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder,StandardScaler,PowerTransformer, MinMaxScaler, RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_predict, train_test_split,GridSearchCV,cross_val_score\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge,ElasticNet\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\n\n\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.feature_selection import SelectKBest,SelectPercentile,f_classif,f_regression,mutual_info_regression,mutual_info_classif,SelectFromModel,RFE\n\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\n\nimport optuna\n\n#importing plotly and cufflinks in offline mode\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\nimport warnings\nwarnings.filterwarnings('ignore')","06742b5f":"pd.set_option('max_columns',100)\npd.set_option('max_rows',900)\n\npd.set_option('max_colwidth',200)","9fa5c396":"df = pd.read_csv(\"..\/input\/car-price-prediction\/CarPrice_Assignment.csv\")\ndf.head()","61df3f34":"df.shape","b5e59e5d":"df.info()","1313962f":"df.duplicated().sum()","2ac94876":"def missing (df):\n    missing_number = df.isnull().sum().sort_values(ascending=False)\n    missing_percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n    return missing_values\n\nmissing(df)","8af00083":"df.nunique()","d64e7bc7":"df.CarName.unique()","16614c73":"[x.split()[0] for x in df['CarName']]","7ddeffb0":"df['model'] = [x.split()[0] for x in df['CarName']]","968c69ac":"df['model'] = df['model'].replace({'maxda': 'mazda', \"Nissan\":\"nissan\",\n                                     'porcshce': 'porsche','porsche':'porsche', \n                                     'toyouta': 'toyota', 'vokswagen': 'volkswagen', \n                                     'vw': 'volkswagen'})","354a49ab":"df.drop([\"car_ID\", \"CarName\"], axis=1, inplace=True)","1f4e053d":"print(f'We have {df.shape[0]} instances with the {df.shape[1]-1} features and 1 output variable')","90a390a7":"numerical = df.drop(\"price\", axis=1).select_dtypes(include=\"number\").columns\ncategorical = df.select_dtypes(include=\"object\").columns\n\nprint(f'Numerical Columns:  {df[numerical].columns}')\nprint('\\n')\nprint(f'Categorical Columns: {df[categorical].columns}')","ce91882b":"df.price.describe()","365982a9":"print(f\"Skewness of price column is: {df.price.skew()}\")","8479d7d2":"df.price.iplot(kind=\"hist\")","92486c0b":"df.drop(\"price\", axis=1).describe()","c2f5a497":"df.drop(\"price\", axis=1).iplot(kind=\"hist\")","73d3cb60":"df[numerical].iplot(kind='histogram',subplots=True,bins=50)","dbe09369":"skew_limit = 0.75 \nskew_vals = df.drop(\"price\", axis=1).skew()\nskew_cols= skew_vals[abs(skew_vals) > skew_limit].sort_values(ascending=False)\nskew_cols","afd5cc2c":"skew_cols.index","e5d12417":"df[skew_cols.index].iplot(kind=\"hist\")","9f5b8ad7":"df[skew_cols.index].iplot(kind=\"histogram\", subplots=True, bins=50)","da6e48c0":"skew_cols","ba1c7214":"df_trans = df[skew_cols.index].copy()\npt = PowerTransformer(method='yeo-johnson')\ntrans= pt.fit_transform(df_trans)\ndf_trans = pd.DataFrame(trans, columns =skew_cols.index )\nprint(df_trans.skew())","9904f92a":"df_trans.iplot(kind='histogram',subplots=True,bins=50);","bc6b9ba2":"plt.figure(figsize=(16, 8))\nsns.heatmap(data=df.corr(), annot=True);","5c6fcbec":"df.drop(\"citympg\", axis=1, inplace=True)","6bd75afa":"df.head()","8712e9e4":"df.fueltype.unique()","ceac44fe":"print(df.groupby(\"fueltype\").price.mean())\ndf.groupby(\"fueltype\").price.mean().iplot(kind=\"histogram\", subplots=True, bins=50)","db835650":"df.aspiration.unique()","3ea766d4":"print(df.groupby(\"aspiration\").price.mean())\ndf.groupby(\"aspiration\").price.mean().iplot(kind=\"histogram\", subplots=True, bins=50)","bd850493":"df.doornumber.unique()","dbd2a0af":"print(df.groupby(\"doornumber\").price.mean())\ndf.groupby(\"doornumber\").price.mean().iplot(kind=\"histogram\", subplots=True, bins=50)","4c5ce082":"df.carbody.unique()","7a4b8d1c":"print(df.groupby(\"carbody\").price.mean().sort_values())\ndf.groupby(\"carbody\").price.mean().iplot(kind=\"histogram\", subplots=True, bins=50)","fc5671c9":"df.drivewheel.unique()","1883fd3d":"print(df.groupby(\"drivewheel\").price.mean().sort_values())\ndf.groupby(\"drivewheel\").price.mean().iplot(kind=\"histogram\", subplots=True, bins=50)","44023669":"df.enginelocation.unique()","07bd2897":"print(df.groupby(\"enginelocation\").price.mean())\ndf.groupby(\"enginelocation\").price.mean().iplot(kind=\"histogram\", subplots=True, bins=50)","615a72be":"df.enginetype.unique()","875e83ec":"print(df.groupby(\"enginetype\").price.mean().sort_values())\ndf.groupby(\"enginetype\").price.mean().iplot(kind=\"histogram\", subplots=True, bins=50)","18a10a20":"df[categorical].columns","ae0fa4b6":"df.cylindernumber.unique()","72f7ccf6":"print(df.groupby(\"cylindernumber\").price.mean().sort_values())\ndf.groupby(\"cylindernumber\").price.mean().iplot(kind=\"histogram\", subplots=True, bins=50)","75d434b0":"df.fuelsystem.unique()","e6722dca":"print(df.groupby(\"fuelsystem\").price.mean().sort_values())\ndf.groupby(\"fuelsystem\").price.mean().iplot(kind=\"histogram\", subplots=True, bins=50)","4c12e47f":"df.model.unique()","495ec65c":"print(df.groupby(\"model\").price.mean().sort_values())\ndf.groupby(\"model\").price.mean().iplot(kind=\"histogram\", subplots=True, bins=50)","d741a966":"df.shape","e67081f6":"df = pd.get_dummies(df, columns=categorical, drop_first=True)","4f65fff8":"df.shape","bb62ffb8":"df.head()","53f18fba":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score \nfrom sklearn.model_selection import cross_val_score, cross_validate\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Lasso, LassoCV, Ridge, RidgeCV, ElasticNet, ElasticNetCV","6b8b5a53":"def eval_metrics(actual, pred):\n    rmse = np.sqrt(mean_squared_error(actual, pred))\n    mae = mean_absolute_error(actual, pred)\n    mse = mean_squared_error(actual, pred)\n    score = r2_score(actual, pred)\n    return print(\"r2_score:\", score, \"\\n\",\"mae:\", mae, \"\\n\",\"mse:\",mse, \"\\n\",\"rmse:\",rmse)","12cbbe6a":"def train_val(y_train, y_train_pred, y_test, y_pred):\n    \n    scores = {\"train_set\": {\"R2\" : r2_score(y_train, y_train_pred),\n    \"mae\" : mean_absolute_error(y_train, y_train_pred),\n    \"mse\" : mean_squared_error(y_train, y_train_pred),                          \n    \"rmse\" : np.sqrt(mean_squared_error(y_train, y_train_pred))},\n    \n    \"test_set\": {\"R2\" : r2_score(y_test, y_pred),\n    \"mae\" : mean_absolute_error(y_test, y_pred),\n    \"mse\" : mean_squared_error(y_test, y_pred),\n    \"rmse\" : np.sqrt(mean_squared_error(y_test, y_pred))}}\n    \n    return pd.DataFrame(scores)","b81768d4":"X = df.drop('price', axis=1)\ny = df.price","07b0c37d":"from sklearn.model_selection import train_test_split","d9b8701b":"X_train,X_test,y_train,y_test=train_test_split(X, y, test_size=0.3, random_state=42)\n\nprint(\"Train features shape : \", X_train.shape)\nprint(\"Train target shape   : \", y_train.shape)\nprint(\"Test features shape  : \", X_test.shape)\nprint(\"Test target shape    : \", y_test.shape)","959a33ee":"# Since we have dummy features MinMaxScaler will be a better choice of scaling for our features.\nfrom sklearn.preprocessing import MinMaxScaler","7e3a4895":"scaler = MinMaxScaler()","dc3fe1c8":"X_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","812130d5":"from sklearn.linear_model import LinearRegression","6c41a840":"ln_model = LinearRegression()\nln_model.fit(X_train_scaled, y_train)\ny_pred = ln_model.predict(X_test_scaled)\ny_train_pred = ln_model.predict(X_train_scaled)\nln_r2 = r2_score(y_test, y_pred)\nln_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n\npd.options.display.float_format = '{:.3f}'.format\ntrain_val(y_train, y_train_pred, y_test, y_pred)","67e6805b":"elastic_model = ElasticNet(max_iter=10000)\n\nparam_grid ={\"alpha\":[0.001, 0.01, 0.02, 0.1, 1, 5,],\n            \"l1_ratio\":[0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1]}\n\ngrid_model = GridSearchCV(estimator = elastic_model, param_grid = param_grid, scoring = 'neg_root_mean_squared_error',\n                         cv =10, verbose =2)","a4990cec":"grid_model.fit(X_train_scaled, y_train)","071b0549":"y_pred = grid_model.predict(X_test_scaled)\ny_train_pred = grid_model.predict(X_train_scaled)\ngrid_r2 = r2_score(y_test, y_pred)\ngrid_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\ntrain_val(y_train, y_train_pred, y_test, y_pred)","f036f68f":"rmse_test =[]\nr2_test =[]\nmodel_names =[]\n\nnumerical2= df.drop(['price'], axis=1).select_dtypes('number').columns\n\nX= df.drop('price', axis=1)\ny= df['price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\ns = StandardScaler()\np= PowerTransformer(method='yeo-johnson', standardize=True)\n\nrr = Ridge()\nlas = Lasso()\nel= ElasticNet()\nknn = KNeighborsRegressor()\n\nmodels = [rr,las,el,knn]\n\nfor model in models:\n    ct = make_column_transformer((s,numerical2),(p,skew_cols.index),remainder='passthrough')  \n    pipe = make_pipeline(ct, model)\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    rmse_test.append(round(np.sqrt(mean_squared_error(y_test, y_pred)),2))\n    r2_test.append(round(r2_score(y_test, y_pred),2))\n    print (f'model : {model} and  rmse score is : {round(np.sqrt(mean_squared_error(y_test, y_pred)),2)}, r2 score is {round(r2_score(y_test, y_pred),2)}')\n\nmodel_names = ['Ridge','Lasso','ElasticNet','KNeighbors']\nresult_df = pd.DataFrame({'RMSE':rmse_test,'R2_Test':r2_test}, index=model_names)\nresult_df","37f12747":"from sklearn.ensemble import RandomForestRegressor","5e098c00":"rf_model = RandomForestRegressor(random_state=101)\nrf_model.fit(X_train, y_train)\ny_pred = rf_model.predict(X_test)\ny_train_pred = rf_model.predict(X_train)\nrf_r2 = r2_score(y_test, y_pred)\nrf_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\ntrain_val(y_train, y_train_pred, y_test, y_pred)","e8bd8a3f":"model = RandomForestRegressor(random_state=101)\n\nscores = cross_validate(model, X_train, y_train, scoring=['r2', 'neg_mean_absolute_error', 'neg_mean_squared_error',\n                                                          'neg_root_mean_squared_error'], cv =5)\ndf_scores = pd.DataFrame(scores)\nprint(df_scores.mean()[2:])\nprint(\"----------------------------------------------------------------\")\ndf_scores","67879236":"param_grid = {\"n_estimators\":[100, 300, 500, 800],\n             \"max_depth\":[None, 3, ],\n             \"max_features\":[\"auto\", 3],\n             \"min_samples_split\":[2, 4, 6]}\n\nmodel = RandomForestRegressor(random_state=101)\nrf_grid_model = GridSearchCV(estimator=model,\n                            param_grid=param_grid,\n                            scoring='neg_root_mean_squared_error',\n                            n_jobs = -1, verbose = 2).fit(X_train, y_train)\n\ny_pred = rf_grid_model.predict(X_test)\ny_train_pred = rf_grid_model.predict(X_train)\nrf_tuned_r2 = r2_score(y_test, y_pred)\nrf_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\ntrain_val(y_train, y_train_pred, y_test, y_pred)","94573fa5":"rf_grid_model.best_params_","8f1b387b":"dt_model = DecisionTreeRegressor(random_state=101)\ndt_model.fit(X_train, y_train)\ny_pred = dt_model.predict(X_test)\ny_train_pred = dt_model.predict(X_train)\ndt_r2 = r2_score(y_test, y_pred)\ndt_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\ntrain_val(y_train, y_train_pred, y_test, y_pred)","90d4df7a":"model = DecisionTreeRegressor(random_state=101)\n\nscores = cross_validate(model, X_train, y_train, scoring=['r2', 'neg_mean_absolute_error', 'neg_mean_squared_error',\n                                                          'neg_root_mean_squared_error'], cv =5)\ndf_scores = pd.DataFrame(scores)\nprint(df_scores.mean()[2:])\nprint(\"----------------------------------------------------------------\")\ndf_scores","9aee8986":"param_grid = {\"splitter\":[\"best\", \"random\"],\n              \"max_features\":[9, 12, 15],\n              \"max_depth\": [None,2,3],\n              \"min_samples_leaf\": [3,5,7],\n              \"min_samples_split\": [40, 50]}\n\nmodel = DecisionTreeRegressor(random_state=101)\ndt_grid_model = GridSearchCV(estimator=model,\n                            param_grid=param_grid,\n                            scoring='neg_root_mean_squared_error',\n                            n_jobs = -1, verbose = 2).fit(X_train, y_train)\n\ny_pred = dt_grid_model.predict(X_test)\ny_train_pred = dt_grid_model.predict(X_train)\ndt_tuned_r2 = r2_score(y_test, y_pred)\ndt_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\ntrain_val(y_train, y_train_pred, y_test, y_pred)","07e60697":"dt_grid_model.best_params_","f48f6d0c":"from sklearn.svm import SVR, LinearSVR","bd7acfc8":"svr_model = SVR()\nsvr_model.fit(X_train_scaled, y_train)\ny_pred = svr_model.predict(X_test_scaled)\ny_train_pred = svr_model.predict(X_train_scaled)\nsvr_r2 = r2_score(y_test, y_pred)\nsvr_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\ntrain_val(y_train, y_train_pred, y_test, y_pred)","5ac85509":"model = SVR()\n\nscores = cross_validate(model, X_train_scaled, y_train, scoring=['r2', 'neg_mean_absolute_error', 'neg_mean_squared_error',\n                                                                 'neg_root_mean_squared_error'], cv =5)\ndf_scores = pd.DataFrame(scores)\nprint(df_scores.mean()[2:])\nprint(\"----------------------------------------------------------------\")\ndf_scores","fa647b39":"param_grid = {'C':[10000, 100000, 500000],\n             'kernel':['linear','poly'],\n              'gamma':['scale','auto'],\n              'degree':[3,5,7],\n              'epsilon':[10, 15, 25]}\n\nmodel = SVR()\nsvr_grid_model = GridSearchCV(estimator=model,\n                              param_grid=param_grid,\n                              scoring='neg_root_mean_squared_error',\n                              n_jobs = -1, verbose = 2).fit(X_train_scaled, y_train)\n\ny_pred = svr_grid_model.predict(X_test_scaled)\ny_train_pred = svr_grid_model.predict(X_train_scaled)\nsvr_tuned_r2 = r2_score(y_test, y_pred)\nsvr_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\ntrain_val(y_train, y_train_pred, y_test, y_pred)","acb2542e":"svr_grid_model.best_params_","08fcda8d":"from sklearn.ensemble import AdaBoostRegressor","c7c395d8":"model = AdaBoostRegressor(random_state=101, n_estimators=50)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\ny_train_pred = model.predict(X_train)\ntrain_val(y_train, y_train_pred, y_test, y_pred)","daa0ae37":"from sklearn.model_selection import cross_validate, cross_val_score\nmodel = AdaBoostRegressor(random_state=101, n_estimators=50)\nscores = cross_validate(model, X_train, y_train, scoring=['r2', \n            'neg_mean_absolute_error','neg_mean_squared_error','neg_root_mean_squared_error'], cv =10)\npd.DataFrame(scores)\npd.DataFrame(scores).iloc[:, 2:].mean()","97607a6e":"from sklearn.model_selection import GridSearchCV","1fb73377":"param_grid = {\"n_estimators\":[30, 50, 100],\n              \"learning_rate\":[0.1, 0.8, 1]\n            }\n\nmodel = AdaBoostRegressor(random_state=101)\ngrid_model = GridSearchCV(estimator=model,\n                          param_grid=param_grid,\n                          scoring='neg_root_mean_squared_error',\n                          cv=10,\n                          n_jobs = -1)\n\ngrid_model.fit(X_train,y_train)\n\ny_pred = grid_model.predict(X_test)\ny_train_pred = grid_model.predict(X_train)\ntrain_val(y_train, y_train_pred, y_test, y_pred)","3ae4fdba":"grid_model.best_params_","959262ac":"from sklearn.ensemble import GradientBoostingRegressor","5b951abb":"model = GradientBoostingRegressor(random_state=101)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\ny_train_pred = model.predict(X_train)\ntrain_val(y_train, y_train_pred, y_test, y_pred)","baa941f0":"model = GradientBoostingRegressor(random_state=101)\nscores = cross_validate(model, X_train, y_train, scoring=['r2', \n            'neg_mean_absolute_error','neg_mean_squared_error','neg_root_mean_squared_error'], cv =10)\npd.DataFrame(scores)\npd.DataFrame(scores).iloc[:, 2:].mean()","c7eda35a":"param_grid = {\"n_estimators\":[200, 300, 500], \"subsample\":[0.1,0.8, 1], \"max_features\" : [None, 2, 3, 4],\n            \"learning_rate\": [0.01, 0.1, 0.5], 'max_depth':[1,2,3]}\n\nmodel = GradientBoostingRegressor(random_state=101)\ngrid_model = GridSearchCV(estimator=model,\n                          param_grid=param_grid,\n                          scoring='neg_root_mean_squared_error',\n                          cv=10,\n                          n_jobs = -1)\n\ngrid_model.fit(X_train, y_train)\ny_pred = grid_model.predict(X_test)\ny_train_pred = grid_model.predict(X_train)\n\npd.options.display.float_format = '{:.3f}'.format\ntrain_val(y_train, y_train_pred, y_test, y_pred)","7370def9":"grid_model.best_params_","3cc115b7":"#!pip install xgboost","348c4c83":"from xgboost import XGBRegressor","834a59d4":"model = XGBRegressor(random_state=101, silent=True)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\ny_train_pred = model.predict(X_train)\ntrain_val(y_train, y_train_pred, y_test, y_pred)","0e5e74a0":"model = XGBRegressor(random_state=101, silent=True)\nscores = cross_validate(model, X_train, y_train, scoring=['r2', \n            'neg_mean_absolute_error','neg_mean_squared_error','neg_root_mean_squared_error'], cv =10)\npd.DataFrame(scores).iloc[:, 2:].mean()","ae1f817c":"param_grid = {\"n_estimators\":[300,500, 1000],'max_depth':[3,5,6], \"learning_rate\": [0.01, 0.05, 0.1],\n             \"subsample\":[0.1, 0.5], \"colsample_bytree\":[0.1, 0.5]}\n\nmodel = XGBRegressor(random_state=101, silent=True)\ngrid_model = GridSearchCV(estimator=model,\n                          param_grid=param_grid,\n                          scoring='neg_root_mean_squared_error',\n                          cv=10,\n                          n_jobs = -1)\n\ngrid_model.fit(X_train, y_train)\ny_pred = grid_model.predict(X_test)\ny_train_pred = grid_model.predict(X_train)\ntrain_val(y_train, y_train_pred, y_test, y_pred)","51a64780":"grid_model.best_params_","c0494430":"importances = rf_model.feature_importances_\nfeature_names = [f'feature {i}' for i in range(X.shape[1])]\n\n# what are scores for the features\nfor i in range(len(rf_model.feature_importances_)):\n    if rf_model.feature_importances_[i] >0.001:\n        print(f'{X_train.columns[i]} : {round(rf_model.feature_importances_[i],3)}')\n\nprint()\n\nplt.bar([X_train.columns[i] for i in range(len(rf_model.feature_importances_))], rf_model.feature_importances_)\nplt.xticks(rotation=90)\nplt.rcParams[\"figure.figsize\"] = (14,8)\nplt.show()","fa41324b":"## Categorical Features","5052e422":"- Based on the Random Forest Regressor:\n\n**enginesize**\n\n**curbweight**\n\n**highway mpg**\n\n**horse power** have biggest importance scores.","d5fcf5f5":"## Feature Importance","308b6b3f":"**enginetype vs. price**","554cf1af":"**Note**: skew_limit=0.75 is our threshold-limit to evaluate skewness. Overall below abs(1) seems acceptable for the linear models. ","8ff1e9bd":"# 1. Exploratory Data Analysis","6912e7c5":"## B. Ridge & Lasso & Elasticnet & KNN with Scaler and Transformer","e2159d7e":"**GridSearchCV**","d0397f56":"- We have 8 different fuel system and price changes amongs them significantly.","4a95c247":"At first glance, our dataset seems quite good. No missing value and some object-type columns.","a9e9dd6d":"- Since random forest ml model produced the highest score, we will check the most important features for the given model.","eea2116d":"## Modelling and Model Performance","67247b91":"- Car ID column is repetition of the index and it has nothing to do with price. Therefore, I'll drop it.\n- Carname has 147 different entity. We need to find a way to reduce the variance.","118ae974":"### A. Linear Regression Model","72165d1b":"**aspiration vs. price**","5cf89c13":"- It gave us high scores.","32db756e":"- First scores seems quite impressive. Let's confirm them with **cross validation**.","444775d8":"- Turbo aspiration is more expensive than standard aspiration\n\n","cb5e10ab":"**Cross Validate**","1346e684":"**Have fun reading and all the best \ud83e\udd18**","a0ff6870":"**doornumber vs. price**","d93bdfa8":"## Train | Test Split","44977427":"We have no duplicated row.","c89fe2f2":"**fuelsystem vs. price**","38537ba9":"**GridSearchCV**","f7388119":"- Based on comparision of train_set and test_set results, we might have overfitting.","d481a942":"- Rear wheel drive cars are the most expensive ones. Front wheel cars the least expensive ones.","1c826fea":"**carbody vs. price**","abd71a16":"- After applying best parameters, we can see that our scores got better.","bf2cbc80":"- It is important to note that Random Forest Regressor gave importance score bigger than 0 to 16 features.\n- Model used 16 out of 63 features to get best prediction.","e64ed245":"##### Today we will continue to develop a model for predicting car prices.","92dfc941":"- Based on the model, porsche, buick, and jaguar are the most expensive ones.\n\n- Based on the model, chevrolet are the least expensive models.","3ce67a5e":"- We might have overfitting here. ","526c1e14":"- Diesel cars are more expensive than cars with gas.","80937815":"**GridSearchCV**","449590a3":"## Car Price Prediction Data\n\nDATA DICTONARY\n\n1 **Car_ID**: Unique id of each observation\n\n2 **Symboling**: Its assigned insurance risk rating, A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.\n\n3 **carCompany**: Name of car company\n\n4 **fueltype**: Car fuel type i.e gas or diesel\n\n5 **aspiration**: Aspiration used in a car\n\n6 **doornumber**: Number of doors in a car\n\n7 **carbody**: body of car\n\n8 **drivewheel**: type of drive wheel\n\n9 **enginelocation**: Location of car engine\n\n10 **wheelbase**: Weelbase of car (\n\n11 **carlength**: Length of car\n\n12 **carwidth**: Width of car\n\n13 **carheight**: height of car\n\n14 **curbweight**: The weight of a car without occupants or baggage.\n\n15 **enginetype**: Type of engine.\n\n16 **cylindernumber**: cylinder placed in the car\n\n17 **enginesize**: Size of car\n\n18 **fuelsystem**: Fuel system of car\n\n19 **boreratio**: Boreratio of car\n\n20 **stroke**: Stroke or volume inside the engine\n\n21 **compressionratio**: compression ratio of car\n\n22 **horsepower**: Horsepower\n\n23 **peakrpm**: car peak rpm\n\n24 **citympg**: Mileage in city\n\n25 **highwaympg**: Mileage on highway\n\n26 **price**: Price of car\n\nReference: https:\/\/www.kaggle.com\/hellbuoy\/car-price-prediction","fa891426":"**enginelocation vs. price**","ca695ff0":"- We have 7 different engine types and price changes amongs them significantly.","7e0f9dff":"**cylindernumber vs. price**","c40ac1d9":"## E. SVM","603271e5":"# 2. Model Selection ","15489ff0":"- We have 7 different cylinder numbers and price changes amongs them significantly.","df1677cc":"**Cross Validation**","d655cfca":"**Cross Validate**","4b2d2d3e":"# Hi all. \ud83d\ude4b","eef0fb10":"**model vs. price**","9f26b27e":"- Based on the price, there are differences among the carbody.\n\n- While Wagon cars the leats expensive ones, hardtop and the convertibles are the most expensive ones.","3166f7c0":"**drivewheel vs. price**","b104c873":"### Feature Scaling","a7586907":"## Converting Categorical Features into Numerical Features (GET_DUMMIES)","98ad159c":"- We have developed model to predict car price problem.\n\n- First, we made the detailed exploratory analysis.\n\n- We have decided which metric to use.\n- We analyzed both target and features in detail.\n- We transform categorical variables into numeric so we can use them in the model.\n- We transform numerical variables to reduce skewness and get close to normal distribution.\n- We use pipeline to avoid data leakage.\n- We looked at the results of the each model and selected the best one for the problem in hand.\n- We made hyperparameter tuning of the best model see the improvement\n- We looked at the feature importance.","df31b19e":"- At first sight, our scores seem good.","580f25ff":"**Cross Validation**","6bebfc26":"- We'll use **linear regression** model as a base model.\n\n- Then we will use **Ridge**, **Lasso**, **Elasticnet**, **KNeighborsRegressor**, and **Support Vector Machine Regressor**.\n\n- Then we will use **ensemble models**, like **Randomforest**, **Gradient Boosting**, and **Extra Trees**.\n\n- Finally we will look at the **XGBoost Regresson**.\n\n- And after evaluating the algorithm, we will select our best model.","0761a634":"## H. XG Boost Regressor","ccfe0eb8":"## Numerical Features","233a43f4":"It can be seen that our target column is quite right_skewed. ","799fb92a":"## G. Gradient Boosting Regressor","a50dbb0a":"- At first glance, we can see that we have several normally distributed columns.\n- We can also see some skewnesses.\n\nLet's look at them one by one.","2de4780d":"**fueltype vs. price**","3862dd64":"- Rear engine location almost 3 times expensive than front engine location.","56a5d613":"## C. Random Forest","9fb6d8c2":"With **power transformer** we managed to reduced our high skewnesses to a optimum level.","db3877a0":"**Cross Validation**","6aa74443":"**GridSearchCV**","ff350e1f":"**Cross Validation with GridSearchCV**","97d25c79":"# Conclusion","1256f9bc":"We need to deal with these high skewnesses. For this purpose, we will use **power transformer**.","bb131dce":"- With SVM, we got quite bad scores.","33efe777":"**Cross Validation**","7d69914f":"- It can be seen that CarName column contains both brand name and model name. We need to retrieve only brand name.\n- We have also some typos when it comes to brands names. We need to handle these typos, as well.","2ac3287b":"## F. Adaboost Regressor","9cb32373":"- Based on the data and data dictionary, We have prediction \/ regression problem.\n\n- We wil make prediction on the target variable **price**\n\n- And we will build a model to get best prediction on the price variable.\n\n- For that we will use **RMSE(Root Mean Squared Error)** and **R2 Score**.","6fb4e7d7":"**GridSearchCV**","85f9fae0":"- Baseline Model, in our case, **Linear Regression model**, did a quite a good job.\n- Now, let's validate these scores with **cross validation**.","1d6aeeda":"- Cars with four doors are slightly expensive than cars with 2 doors.","56729cb5":"**GridSearchCV**","ba759c33":"- From the threshold 0.9 perspective: **Highwaympg** and **citympg** has 0.97 correlation. We can drop one of them to avoid multicollinearity problems for the linear models.\n","6630375d":"We just dropped **car_ID** and **CarName** columns.","3e6b9c92":"- **price** column will be our target (label) column.","6f00e5dd":"## D. Decision Tree","de7f44c8":"- We could increase our R2 score with the best parameters."}}