{"cell_type":{"6fb7fa80":"code","9412fbe1":"code","cda936f8":"code","ec55d029":"code","aa1493bc":"code","6f81f4a2":"code","b913f2b8":"code","247d08bf":"code","fa60b230":"code","1557bb23":"code","e272149f":"code","debc1131":"code","f268838f":"code","e2c7d983":"code","3cc10097":"code","6950e159":"code","ffad055d":"code","f405bb52":"code","421541ec":"code","3b86e85c":"code","6b01260d":"code","893a09ff":"code","3b1a2053":"code","a63133c6":"code","c85fb681":"code","b3535fcf":"code","b95bf039":"code","e6f303b3":"code","46553db7":"code","33b8b8be":"code","7aef8c9b":"code","eba6ff59":"code","fa82f2c9":"code","7a0d05f5":"code","3009a609":"code","079527ce":"code","d1a60f1c":"code","4f90500a":"code","3e49f292":"code","e2fa2d00":"code","f4d78020":"code","fbdf157b":"markdown","61b5ca8a":"markdown","68ed383b":"markdown","d479c496":"markdown","391e7734":"markdown","94f1054e":"markdown","46ed79ce":"markdown","f7a0ea50":"markdown"},"source":{"6fb7fa80":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import Sequential\nfrom keras.layers import Dense\nfrom sklearn.model_selection import train_test_split","9412fbe1":"N = 200 #Number of observations per class\nD = 2   #Number of features\nK = 3   #Number of classes\nX = np.zeros((N * K, D))\ny = np.zeros(N * K, dtype = 'uint8')\nfor j in range(K):\n    ix = range(N * j, N * (j + 1))\n    r = np.linspace(0, 1, N) \n    np.random.seed(j)\n    t = np.linspace(j * 4,(j + 1) * 5, N) + np.random.randn(N) * 0.25 \n    X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]\n    y[ix] = j\n\n#Plot Data\ncdict = {0: 'red', 1: 'blue', 2: 'green'}\nplt.figure(figsize = (8, 8))\nfor i in np.unique(y):\n    indices = np.where(y == i)\n    plt.scatter(x = X[indices, 0], y = X[indices, 1], c = cdict[i], \n                label = i, marker = \"o\", alpha = 0.7)\nplt.legend()","cda936f8":"# Devide your dataset into train(80%) and test(20%).\n#Q2: Create a multi layer perceptron to classify the observations on train dataset.\n#    Use the model to predict on test dataset. Report the accuracy of your model.\n#    Visualize your results.","ec55d029":"X_train, X_test,y_train, y_test = train_test_split(X, y, test_size = 0.2)\nprint(f'The shape of X_train is: {X_train.shape}')\nprint(f'The shape of y_train is: {y_train.shape}')\nprint(f'The shape of X_test is: {X_test.shape}')\nprint(f'The shape of y_test is: {y_test.shape}')\n","aa1493bc":"# Build Model Architecture\nmodel_b = Sequential()\nmodel_b.add(Dense(3, activation = 'softmax', input_dim = 2))\nmodel_b.summary()","6f81f4a2":"# config the model\nopt = keras.optimizers.Adam(learning_rate = 0.001)\nmodel_b.compile(optimizer = opt, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])","b913f2b8":"# Train The Model\nhistory_b = model_b.fit(X_train, y_train, epochs = 64, batch_size = 32, validation_split = 0.1)","247d08bf":"plt.figure(figsize = (20, 9))\nplt.subplot(1, 2, 1)\nplt.plot(history_b.history['loss'], label = 'train')\nplt.plot(history_b.history['val_loss'], alpha = 0.7, label = 'test')\nplt.xlabel('Epochs', size = 20)\nplt.ylabel('Loss', size = 20)\nplt.legend()\nplt.title('Base Model: Loss-Epochs', size = 15)\n\nplt.subplot(1, 2, 2)\nplt.plot(history_b.history['accuracy'], label = 'train')\nplt.plot(history_b.history['val_accuracy'], alpha = 0.7, label = 'test')\nplt.xlabel('Epochs', size = 20)\nplt.ylabel('Accuracy', size = 20)\nplt.legend()\nplt.title('Base Model: Accuracy-Epochs', size = 15)\n","fa60b230":"#1-Layer Model\n#Define the Model Architecture\nmodel_1l = Sequential()\nweight_initializer_1 = tf.keras.initializers.RandomNormal(mean = 0, stddev = 2\/2)\nmodel_1l.add(Dense(32, input_dim = 2, activation = \"relu\", kernel_initializer = weight_initializer_1))\nweight_initializer_2 = tf.keras.initializers.RandomNormal(mean = 0, stddev = 2\/32)\nmodel_1l.add(Dense(3, activation = \"softmax\", kernel_initializer = weight_initializer_2))\nmodel_1l.summary()","1557bb23":"# config Model\nmodel_1l.compile(optimizer = opt, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])","e272149f":"# Train_The Model\nhistory_1l = model_1l.fit(X_train, y_train, epochs = 128, batch_size = 32, validation_split = 0.1)","debc1131":"plt.figure(figsize = (20, 9))\nplt.subplot(1, 2, 1)\nplt.plot(history_1l.history['loss'], label = 'train')\nplt.plot(history_1l.history['val_loss'], alpha = 0.7, label = 'test')\nplt.xlabel('Epochs', size = 20)\nplt.ylabel('Loss', size = 20)\nplt.legend()\nplt.title('1 layer Model: Loss-Epochs', size = 15)\n\nplt.subplot(1, 2, 2)\nplt.plot(history_1l.history['accuracy'], label = 'train')\nplt.plot(history_1l.history['val_accuracy'], alpha = 0.7, label = 'test')\nplt.xlabel('Epochs', size = 20)\nplt.ylabel('Accuracy', size = 20)\nplt.legend()\nplt.title('1 layer Model: Accuracy-Epochs', size = 15)\n","f268838f":"#3-Layer Model\n#Define the Model Architecture\nmodel_3l = Sequential()\nweight_initializer_1 = tf.keras.initializers.RandomNormal(mean = 0, stddev = 2\/2)\nmodel_3l.add(Dense(32, input_dim = 2, activation = \"relu\", kernel_initializer = weight_initializer_1))\nweight_initializer_2 = tf.keras.initializers.RandomNormal(mean = 0, stddev = 2\/32)\nmodel_3l.add(Dense(32, activation = \"relu\", kernel_initializer = weight_initializer_2))\nmodel_3l.add(Dense(32, activation = \"relu\", kernel_initializer = weight_initializer_2))\nmodel_3l.add(Dense(3, activation = \"softmax\", kernel_initializer = weight_initializer_2))\nmodel_3l.summary()","e2c7d983":"#Configure the Model\nopt = keras.optimizers.Adam(learning_rate = 0.001) #learning rate\nmodel_3l.compile(optimizer = opt, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])","3cc10097":"#Train the Model\nhistory_3l = model_3l.fit(X_train, y_train, epochs = 128, batch_size = 64, validation_split = 0.1, verbose = 1)","6950e159":"plt.figure(figsize = (20, 9))\nplt.subplot(1, 2, 1)\nplt.plot(history_3l.history['loss'], label = 'train')\nplt.plot(history_3l.history['val_loss'], alpha = 0.7, label = 'test')\nplt.xlabel('Epochs', size = 20)\nplt.ylabel('Loss', size = 20)\nplt.legend()\nplt.title('3 layer Model: Loss-Epochs', size = 15)\nplt.grid()\n\nplt.subplot(1, 2, 2)\nplt.plot(history_3l.history['accuracy'], label = 'train')\nplt.plot(history_3l.history['val_accuracy'], alpha = 0.7, label = 'test')\nplt.xlabel('Epochs', size = 20)\nplt.ylabel('Accuracy', size = 20)\nplt.legend()\nplt.title('3 layer Model: Accuracy-Epochs', size = 15)\nplt.grid()","ffad055d":"def model_b_func():\n  model = Sequential()\n  weight_initializer = tf.keras.initializers.RandomNormal(mean = 0, stddev = 2\/2)\n  model.add(Dense(3, activation = \"softmax\", kernel_initializer = weight_initializer))\n  model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n  return(model)","f405bb52":"def model_1l_func():\n  model = Sequential()\n  weight_initializer_1 = tf.keras.initializers.RandomNormal(mean = 0, stddev = 2\/2)\n  model.add(Dense(32, input_dim = 2, activation = \"relu\", kernel_initializer = weight_initializer_1))\n  weight_initializer_2 = tf.keras.initializers.RandomNormal(mean = 0, stddev = 2\/32)\n  model.add(Dense(3, activation = \"softmax\", kernel_initializer = weight_initializer_2))\n  model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n  return(model)","421541ec":"def model_3l_func():\n  model = Sequential()\n  weight_initializer_1 = tf.keras.initializers.RandomNormal(mean = 0, stddev = 2\/2)\n  model.add(Dense(32, input_dim = 2, activation = \"relu\", kernel_initializer = weight_initializer_1))\n  weight_initializer_2 = tf.keras.initializers.RandomNormal(mean = 0, stddev = 2\/32)\n  model.add(Dense(32, input_dim = 32, activation = \"relu\", kernel_initializer = weight_initializer_2))\n  model.add(Dense(32, input_dim = 32, activation = \"relu\", kernel_initializer = weight_initializer_2))\n  model.add(Dense(3, activation = \"softmax\", kernel_initializer = weight_initializer_2))\n  model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n  return(model)","3b86e85c":"#5-fold Cross Validation\nmodels = [model_b_func, model_1l_func, model_3l_func]\ncv_results_1 = []\n\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nnp.random.seed(123)\nfrom tensorflow import random\nrandom.set_seed(123)\n\n#Loop over three models\nfor m in range(len(models)):\n    model = KerasClassifier(build_fn = models[m], epochs = 128, batch_size = 32, verbose = 0)\n    kf = KFold(n_splits = 5)\n    result = cross_val_score(model, scoring = \"accuracy\", X = X_train, y = y_train, cv = kf)\n    cv_results_1.append(result)","6b01260d":"cv_results_1","893a09ff":"#Print the Cross-validation Scores\nprint(\"CV Mean Accuracy for Base Model = \", format(cv_results_1[0].mean(), '0.4f'))\nprint(\"CV Mean Accuracy for 1-Layer Model = \", format(cv_results_1[1].mean(), '0.4f'))\nprint(\"CV Mean Accuracy for 3-Layer Model = \", format(cv_results_1[2].mean(), '0.4f'))\n","3b1a2053":"#Focus on 3-Layer Model\nepochs = [64, 128, 256]\nbatch_size = [32, 64, 128]\ncv_results_2 = []\n\n#Loop over epochs and batch_size\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nnp.random.seed(123)\nfrom tensorflow import random\nrandom.set_seed(123)\n\nfor b in batch_size:\n  for e in epochs:\n    model = KerasClassifier(build_fn = model_3l_func, batch_size = b, epochs = e, verbose = 0)\n    kf = KFold(n_splits = 5)\n    result = cross_val_score(model, scoring = \"accuracy\", X = X_train, y = y_train, cv = kf)\n    cv_results_2.append(result)","a63133c6":"cv_results_2","c85fb681":"#Print the Cross-validation Scores\nc = 0\nfor b in range(len(batch_size)):\n    for e in range(len(epochs)):\n        print(\"CV Mean Accuracy for 3-Layer Model with batch_size = \", batch_size[b], \n            \" and epochs = \", epochs[e], \" is: \", format(cv_results_2[c].mean(), '0.4f'))\n        c += 1","b3535fcf":"#from keras.regularizers import l1 #for l1 regularization\n#from keras.regularizers import l2 #for l2 regularization\nfrom keras.regularizers import l1_l2 #for l1 and l2 regularization","b95bf039":"def model_3l_reg_func(l1_param = 0, l2_param = 0):\n  model = Sequential()\n  weight_initializer_1 = tf.keras.initializers.RandomNormal(mean = 0, stddev = 2\/2)\n  model.add(Dense(32, input_dim =2, activation = \"relu\", kernel_initializer = weight_initializer_1, kernel_regularizer = l1_l2(l1 = l1_param, l2 = l2_param)))\n  weight_initializer_2 = tf.keras.initializers.RandomNormal(mean = 0, stddev = 2\/32)\n  model.add(Dense(32, input_dim = 32, activation = \"relu\", kernel_initializer = weight_initializer_2, kernel_regularizer = l1_l2(l1 = l1_param, l2 = l2_param)))\n  model.add(Dense(32, input_dim = 32, activation = \"relu\", kernel_initializer = weight_initializer_2, kernel_regularizer = l1_l2(l1 = l1_param, l2 = l2_param)))\n  model.add(Dense(3, activation = \"softmax\", kernel_initializer = weight_initializer_2))\n  model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n  return(model)","e6f303b3":"#L1 Regularization\nl1_param = [0, 0.001, 0.005, 0.01]\ncv_results_3 = []\n\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nnp.random.seed(123)\nfrom tensorflow import random\nrandom.set_seed(123)\n\n#Loop over l1_param\nfor p in l1_param:\n  model = KerasClassifier(build_fn = model_3l_reg_func, l1_param = p, batch_size = 32, epochs = 256, verbose = 0)\n  kf = KFold(n_splits = 5)\n  result = cross_val_score(model, scoring = \"accuracy\", X = X_train, y = y_train, cv = kf)\n  cv_results_3.append(result)","46553db7":"cv_results_3","33b8b8be":"#Print the Cross-validation Scores\nfor i in range(len(l1_param)):\n    print(\"CV Mean Accuracy for 3-Layer Model with L1 Regularization and L1 param = \", l1_param[i], \n          \" is: \", format(cv_results_3[i].mean(), '0.4f'))","7aef8c9b":"def model_3l_dropout_func(rate = 0.1):\n  model = Sequential()\n  weight_initializer_1 = tf.keras.initializers.RandomNormal(mean = 0, stddev = 2\/2)\n  model.add(Dense(32, input_dim = 2, activation = \"relu\", kernel_initializer = weight_initializer_1))\n  model.add(keras.layers.Dropout(rate))\n  weight_initializer_2 = tf.keras.initializers.RandomNormal(mean = 0, stddev = 2\/32)\n  model.add(Dense(32, input_dim = 32, activation = \"relu\", kernel_initializer = weight_initializer_2))\n  model.add(keras.layers.Dropout(rate))\n  model.add(Dense(32, input_dim = 32, activation = \"relu\", kernel_initializer = weight_initializer_2))\n  model.add(keras.layers.Dropout(rate))\n  model.add(Dense(3, activation = \"softmax\", kernel_initializer = weight_initializer_2))\n  model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n  return(model)","eba6ff59":"#Dropout Regularization\ndropout_rate = [0, 0.1, 0.2, 0.3]\ncv_results_4 = []\n\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nnp.random.seed(123)\nfrom tensorflow import random\nrandom.set_seed(123)\n\n#Loop over dropout_rate\nfor r in dropout_rate:\n  model = KerasClassifier(build_fn = model_3l_dropout_func, rate = r, batch_size = 32, epochs = 256, verbose = 0)\n  kf = KFold(n_splits = 5)\n  result = cross_val_score(model, scoring = \"accuracy\", X = X_train, y = y_train, cv = kf)\n  cv_results_4.append(result)","fa82f2c9":"cv_results_4","7a0d05f5":"#Print the Cross-validation Scores\nfor i in range(len(dropout_rate)):\n    print(\"CV Mean Accuracy for 3-Layer Model with Dropout Rate = \", dropout_rate[i], \n          \" is: \", format(cv_results_4[i].mean(), '0.4f'))","3009a609":"#Final Model\nmodel_final = model_3l_dropout_func(rate = 0)","079527ce":"#Train the Final Model\nfrom tensorflow import random\nrandom.set_seed(1253)\nhistory_final = model_final.fit(X_train, y_train, epochs = 256, batch_size = 32, validation_split = 0.1, verbose = 1)","d1a60f1c":"plt.figure(figsize = (20, 9))\nplt.subplot(1, 2, 1)\nplt.plot(history_final.history['loss'], label = 'train')\nplt.plot(history_final.history['val_loss'], alpha = 0.7, label = 'test')\nplt.xlabel('Epochs', size = 20)\nplt.ylabel('Loss', size = 20)\nplt.legend()\nplt.title('model_final: Loss-Epochs', size = 15)\nplt.grid()\n\nplt.subplot(1, 2, 2)\nplt.plot(history_final.history['accuracy'], label = 'train')\nplt.plot(history_final.history['val_accuracy'], alpha = 0.7, label = 'test')\nplt.xlabel('Epochs', size = 20)\nplt.ylabel('Accuracy', size = 20)\nplt.legend()\nplt.title('model_final: Accuracy-Epochs', size = 15)\nplt.grid()","4f90500a":"y_pred = model_final.predict(X_test)\ny_pred = np.argmax(y_pred, axis = 1)\ny_pred","3e49f292":"y_test","e2fa2d00":"#Accuracy\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred) * 100","f4d78020":"#Confusion Matrix for Test Dataset\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, y_pred)","fbdf157b":"## Final Model","61b5ca8a":"# Build Deep learning Models\n### Base Model","68ed383b":"# Model Selection Useing Kfold cross validation","d479c496":"### Deep Network","391e7734":"## L1 and L2 Regularization","94f1054e":"## Dropout Regularization","46ed79ce":"# Plot Decision Boundary","f7a0ea50":"## Improving Model Accuracy"}}