{"cell_type":{"3508bc65":"code","a5829c83":"code","1bab0230":"code","af5e5aa6":"code","ca781d6f":"code","349fabed":"code","2c8104f4":"code","26a3335f":"code","a1017e2c":"code","d69c993c":"code","feeee882":"code","5e6a23fc":"code","62989aeb":"code","29984bbc":"code","02fe93b1":"code","1d9fb4a2":"code","5871b0d3":"code","8dee5a32":"code","d3a6ffb0":"code","afed7aae":"code","d3f42e33":"code","1e2d0a1e":"code","10654625":"code","6c154be1":"code","eb73b36a":"code","7257270e":"code","f992e076":"code","40e7a1b2":"code","614d539c":"code","41354b35":"code","4a41b53b":"code","7fdd02bb":"code","ae4a6bbc":"code","47f969a5":"code","be3f8fd7":"code","6b3b2c91":"code","0e4e6305":"code","ff7f0535":"code","dea82c01":"code","7ae10fa1":"code","3531c197":"code","78dc567d":"code","3cdad9f5":"code","05681a0f":"code","d9f7ab3d":"code","0ebd1559":"code","dbff5764":"code","0403907d":"code","c87f25e7":"code","98aa8cb0":"code","4dbeccae":"code","27a183f5":"code","e2eac99f":"code","36af2fdf":"markdown","81d66f48":"markdown","bf7bee3d":"markdown","690478ed":"markdown","6a3c437c":"markdown","31aab559":"markdown","c677b4ec":"markdown","83ace2e6":"markdown","b4a34278":"markdown","e172f273":"markdown","84b96425":"markdown","1f572db1":"markdown","1f2af347":"markdown"},"source":{"3508bc65":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a5829c83":"# Importing Libraries\n\nimport numpy as np # for linear algebra\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import KFold,train_test_split,cross_validate, ShuffleSplit\nimport tensorflow_addons as tfa\nfrom keras import backend as K\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score,f1_score\nimport torch as torch\nimport transformers as ppb\nimport spacy\n","1bab0230":"# Importing Dataset\ndf_train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","af5e5aa6":"df_train.info()","ca781d6f":"df_train.target.sum() \/ df_train.target.count()","349fabed":"plt.style.use('fivethirtyeight')\nsns.displot(data=df_train, x=\"target\")","2c8104f4":"def format_keyword(df):\n    df[\"keyword\"] = df[\"keyword\"].fillna(\".\")\n    df[\"keyword\"] = df.keyword.str.replace(\"%20\",\" \")","26a3335f":"format_keyword(df_train)","a1017e2c":"df_train.loc[df_train.target==0][\"keyword\"].value_counts()","d69c993c":"format_keyword(df_test)","feeee882":"df_test.info()","5e6a23fc":"df_count = df_train.text.str.split().str.len()\nmax(df_count)","62989aeb":"import re\ndef process_text(text):\n    text=text.replace(\"\\n\",\"\")\n    text = re.sub(r'@\\S+','',text)\n    text = re.sub(r'#\\S+','',text) \n    text = re.sub(r'https?:\/\/\\S+|www\\.\\S+|http?:\/\/\\S+','',text) \n    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-.\/:;<=>?@[\\]^_`{|}~\"\"\"), '', text)  \n    text = re.sub(r'[0-9]', '', text)\n    text = re.sub(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # removal of emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\",' ',text)\n    text = text.lower()\n    text = text.split()\n    res = \"\"\n    for token in text:\n        if not token==\" \":\n            res += token +\" \"\n    \n    return res","29984bbc":"sample_doc = \"!!! @user @twi #topic \\n\\nHello,   UTC20150805 World http:\/\/t.com\"\nprocess_text(sample_doc)","02fe93b1":"df_train[\"text\"] = df_train.text.transform(lambda x: process_text(x))\ndf_test[\"text\"] = df_test.text.transform(lambda x: process_text(x))","1d9fb4a2":"df_train[\"appears\"]=df_train.groupby(\"text\").text.transform(\"count\")","5871b0d3":"df_train[\"target_std\"]=df_train.groupby(\"text\").target.transform(np.std)\ndf_train[\"target_mean\"]=df_train.groupby(\"text\").target.transform(np.mean)","8dee5a32":"duplicate_ids = df_train.loc[df_train.target_std>0].sort_values(by=[\"appears\",\"text\"],ascending=False).index","d3a6ffb0":"duplicate_ids","afed7aae":"df_train = df_train.drop(index = duplicate_ids)","d3f42e33":"df_train = df_train.drop_duplicates(subset=[\"text\"])","1e2d0a1e":"df_train.reset_index(drop=True,inplace=True)\ndf_train","10654625":"nlp = spacy.load(\"en_core_web_lg\")\nkeyword_train = np.array([nlp(text).vector for text in df_train.keyword])\nkeyword_test = np.array([nlp(text).vector for text in df_test.keyword])","6c154be1":"def nlp_vectors(text):\n    res = []\n    doc = nlp(text)\n    for token in doc:\n        if not token.is_space:\n            res.append(token.vector)\n    return res\n\ndef build_nlp_vectors(df_text):\n    spacy_vectors = ([nlp_vectors(text) for text in df_text])\n    max_length = 0;\n    for vector in spacy_vectors:\n        max_length = max(max_length, len(vector))\n    print(f\"Maximum Length:{ max_length}\")\n    for i in range(len(spacy_vectors)):\n        while(len(spacy_vectors[i]) <max_length):\n            spacy_vectors[i].append([0]*300)\n    spacy_vectors = np.array(spacy_vectors)\n    print(f\"Shape of spacy vector:{spacy_vectors.shape}\")\n    return spacy_vectors","eb73b36a":"nlp_train = build_nlp_vectors(df_train.text)","7257270e":"tokenizer = ppb.DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\nbert_model = ppb.DistilBertModel.from_pretrained(\"distilbert-base-uncased\")","f992e076":"def process_data(df_text):\n    tokens = df_text.apply(lambda text: tokenizer.encode(text,add_special_tokens=True))\n    max_len = 0;\n    i = 0;\n    for token in tokens.values:\n        max_len = max(max_len,len(token))\n    print(f\"Max Length: {max_len}\")\n    \n    padded = np.array([i+[0]*(max_len-len(i)) for i in tokens.values])\n    attention_mask = np.where(padded !=0, 1,0)\n    input_ids = torch.tensor(padded)\n    attention_mask = torch.tensor(attention_mask)\n    with torch.no_grad():\n        last_hidden_states = bert_model(input_ids,attention_mask=attention_mask)\n    X = last_hidden_states[0][:,0,:].numpy()\n    print(X.shape)\n    return X","40e7a1b2":"X_train = process_data(df_train.text)","614d539c":"y_train = df_train.target","41354b35":"X_tr, X_val, nlp_tr, nlp_val, kw_tr, kw_val, y_tr, y_val = train_test_split(X_train,nlp_train, keyword_train, y_train, test_size=0.25, train_size=0.75,shuffle=True)","4a41b53b":"def build_nn():\n    model = tf.keras.Sequential()\n    model.add(layers.Input(shape=(768,)))\n    model.add(layers.Dense(128,activation='tanh'))\n    model.add(layers.Dropout(0.6))\n    model.add(layers.Dense(32,activation='tanh'))\n    model.add(layers.Dropout(0.6))\n    model.add(layers.Dense(8,activation='tanh'))\n    model.add(layers.Dense(1,activation='sigmoid'))\n    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n                      optimizer=tf.keras.optimizers.Adam(1e-4),\n                      metrics=['accuracy'])\n    return model","7fdd02bb":"fold = 4\ndef plot_history(history):\n    plt.figure(figsize=(4*fold,4*2))\n    for i in range(fold):\n        plt.subplot(2,fold,i+1)\n        plt.plot(history_by_fold[i].history[\"loss\"])\n        plt.plot(history_by_fold[i].history[\"val_loss\"])\n        plt.xlabel(\"Epochs\")\n        plt.ylabel(\"Loss\")\n        plt.legend([\"Train\",\"Validation\"])\n\n        plt.subplot(2,fold,fold+i+1)\n        plt.plot(history_by_fold[i].history[\"accuracy\"])\n        plt.plot(history_by_fold[i].history[\"val_accuracy\"])\n        plt.xlabel(\"Epochs\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend([\"Train\",\"Validation\"])","ae4a6bbc":"kfold = KFold(n_splits=4, shuffle=True, random_state=1)","47f969a5":"def eval_f1_score(X_val, y_val, model):\n    pred_val = (model.predict(X_val)>0.5)\n    f1 = f1_score(y_val,pred_val)\n    return f1","be3f8fd7":"EPOCHS = 100\nBATCH_SIZE = 64","6b3b2c91":"fold = 0\nhistory_by_fold = []\ncv_results = []\nfor train,val in kfold.split(X_train,y_train):\n    nn_model = build_nn()\n    history = nn_model.fit(X_train[train],y_train[train],\n                          validation_data=(X_train[val],y_train[val]),\n                          epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0)\n    scores = nn_model.evaluate(X_train[val],y_train[val],verbose=0)\n    print(f\"-- Fold {fold} -- \")\n    print(f\"{nn_model.metrics_names[0]}: {scores[0]}\")\n    print(f\"{nn_model.metrics_names[1]}: {scores[1]}\")\n    print(f\"F1 Score: {eval_f1_score(X_train[val],y_train[val],nn_model)}\")\n\n    cv_results.append(scores[1])\n    history_by_fold.append(history)\n    fold+=1\nprint(f\"{np.mean(cv_results)} +\\- {np.std(cv_results)}\")\nplot_history(history)","0e4e6305":"nn_model = build_nn()\nhistory = nn_model.fit(X_tr,y_tr, validation_data=(X_val,y_val),\n                      epochs=EPOCHS, batch_size=BATCH_SIZE,verbose=0)\nscores= nn_model.evaluate(X_val,y_val,verbose=0)\nprint(f\"Accuracy: {scores[1]}\")\nprint(f\"F1 Score: {eval_f1_score(X_val,y_val,nn_model)}\")","ff7f0535":"plt.figure(figsize=(12,6))\nplt.subplot(1,2,1)\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.legend([\"Train\",\"Validation\"])\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\n\nplt.subplot(1,2,2)\nplt.plot(history.history[\"accuracy\"])\nplt.plot(history.history[\"val_accuracy\"])\nplt.legend([\"Train\",\"Validation\"])\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")","dea82c01":"def build_LSTM():\n    lstm_model = tf.keras.Sequential()\n    lstm_model.add(layers.Input(shape=(None,300)))\n    lstm_model.add(layers.LSTM(16)) \n    lstm_model.add(layers.Dense(8, activation=\"tanh\"))\n    lstm_model.add(layers.Dense(8, activation=\"tanh\"))\n    lstm_model.add(layers.Dense(1,activation=\"sigmoid\"))\n\n    lstm_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n                      optimizer=tf.keras.optimizers.Adam(5e-5),\n                      metrics=['accuracy'])\n    return lstm_model\n","7ae10fa1":"EPOCHS =  30;\nBATCH_SIZE = 64;","3531c197":"kfold = KFold(n_splits=4, shuffle=True, random_state=1)","78dc567d":"fold = 0\nhistory_by_fold = []\ncv_results = []\nfor train, val in kfold.split(nlp_train,y_train):\n    lstm_model = build_LSTM()\n    history = lstm_model.fit(nlp_train[train],y_train[train],\n                            validation_data=(nlp_train[val],y_train[val]),\n                            epochs=EPOCHS,batch_size=BATCH_SIZE,verbose=0)\n    scores = lstm_model.evaluate(nlp_train[val],y_train[val],verbose=0)\n    \n    #df_test[f\"Fold{fold}\"] = lstm_model.predict(X_test)\n    #df_train[f\"Fold{fold}\"] = lstm_model.predict(X_train)\n    print(f\"-- Fold{fold} --\")\n    print(f\"{lstm_model.metrics_names[0]}: {scores[0]}\")\n    print(f\"{lstm_model.metrics_names[1]}: {scores[1]}\")\n    print(f\"F1 Score: {eval_f1_score(nlp_train[val],y_train[val],lstm_model)}\")\n\n    cv_results.append(scores[1])\n    history_by_fold.append(history)\n    fold+=1\nprint(f\"{np.mean(cv_results)} +\\- {np.std(cv_results)}\")","3cdad9f5":"plt.figure(figsize=(4*fold,4*2))\nfor i in range(fold):\n    plt.subplot(2,fold,i+1)\n    plt.plot(history_by_fold[i].history[\"loss\"])\n    plt.plot(history_by_fold[i].history[\"val_loss\"])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend([\"Train\",\"Validation\"])\n\n    plt.subplot(2,fold,fold+i+1)\n    plt.plot(history_by_fold[i].history[\"accuracy\"])\n    plt.plot(history_by_fold[i].history[\"val_accuracy\"])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend([\"Train\",\"Validation\"])","05681a0f":"lstm_model = build_LSTM()\nhistory = lstm_model.fit(nlp_tr,y_tr,validation_data=(nlp_val,y_val), epochs=EPOCHS, batch_size=BATCH_SIZE)","d9f7ab3d":"valid_predict  = (lstm_model.predict(nlp_val) > 0.5)\nf1 = f1_score(y_val, valid_predict)\nprint(f\" F1 Score: {f1}\")","0ebd1559":"\nplt.figure(figsize=(12,6))\nplt.subplot(1,2,1)\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.legend([\"Train\",\"Validation\"])\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\n\nplt.subplot(1,2,2)\nplt.plot(history.history[\"accuracy\"])\nplt.plot(history.history[\"val_accuracy\"])\nplt.legend([\"Train\",\"Validation\"])\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")","dbff5764":"lr_keywords = LogisticRegression(max_iter=500)\nlr_keywords.fit(kw_tr,y_tr)\nval_pred = lr_keywords.predict(kw_val)\nprint(f\"Accurcay: {accuracy_score(y_val, val_pred)}\")\nprint(f\"F1 score: {f1_score(y_val,val_pred)}\")","0403907d":"nn_tr_predict = nn_model.predict(X_tr)\nkw_tr_predict = lr_keywords.predict_proba(kw_tr)[:,1]\nlstm_tr_predict = lstm_model.predict(nlp_tr)\n\nnn_val_predict = nn_model.predict(X_val)\nkw_val_predict = lr_keywords.predict_proba(kw_val)[:,1]\nlstm_val_predict = lstm_model.predict(nlp_val)\n\nkw_tr_predict = kw_tr_predict.reshape((kw_tr_predict.shape[0],1))\nkw_val_predict = kw_val_predict.reshape((kw_val_predict.shape[0],1))\n\n\nconcat_tr = np.concatenate((nn_tr_predict, kw_tr_predict, lstm_tr_predict), axis=1)\nconcat_val = np.concatenate((nn_val_predict, kw_val_predict, lstm_val_predict), axis=1)","c87f25e7":"lr = LogisticRegression()\nlr.fit(concat_tr,y_tr)\nval_pred = lr.predict(concat_val)\nprint(f\"Accurcay: {accuracy_score(y_val, val_pred)}\")\nprint(f\"F1 score: {f1_score(y_val,val_pred)}\")","98aa8cb0":"X_test = process_data(df_test.text)","4dbeccae":"nlp_test = build_nlp_vectors(df_test.text)","27a183f5":"df_test[\"nn_predict\"]= nn_model.predict(X_test)\ndf_test[\"lstm_predict\"]= lstm_model.predict(nlp_test)\ndf_test[\"keyword_predict\"] = lr_keywords.predict_proba(keyword_test)[:,1]\nfeatures = [\"nn_predict\",\"keyword_predict\",\"lstm_predict\"]\n\ntest_features = df_test[features]\npredict = lr.predict(test_features)","e2eac99f":"output = pd.DataFrame({\"id\":df_test.id, \"target\":predict})\noutput.to_csv(\"submission.csv\",index=False)\noutput","36af2fdf":"### **Training** ###","81d66f48":"### **Recurrent Network with Long Short-Term Memory Cells** ###","bf7bee3d":"### **Transfer Learning Model** ###","690478ed":"## **Predicting Text** ##","6a3c437c":"### **kFold Cross Validation** ###","31aab559":"### **Clean Keyword** ###","c677b4ec":"### **Outline** ###\n\n1. Preprocessing Text\n2. Transfer Learning From the Pretrained BERT Model\n3. Recurrent Network with the Long Short-Term Memory Cells\n4. Classifier for Keywords\n5. Ensembler Learner\n6. Predicting Test Data","83ace2e6":"### **Clean Text** ###","b4a34278":"### **Processing Test Data** ###","e172f273":"### **Model Prediction** ###","84b96425":"### **Duplicate Tweet With Ambiguous Target** ###","1f572db1":"### **Ensembler Learning** ###","1f2af347":"### **Classification With Keywords** ###"}}