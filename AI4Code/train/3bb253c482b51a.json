{"cell_type":{"a4e080f2":"code","8ab5bb97":"code","e82dbac5":"code","98512836":"code","e337fe68":"code","a3d722d6":"code","55429dda":"code","8dfe2c9b":"code","5771e733":"code","5e2d52bc":"code","259831a7":"code","c2c81a93":"code","d5f89a5b":"code","3a2ada5e":"code","812a8e6c":"code","c0cf1e62":"code","5231ea19":"code","3d36f732":"code","a7b8750e":"code","52487db0":"code","85044736":"code","ac21124b":"code","76d8696e":"code","42f7217a":"code","9536a338":"code","8fd31d46":"code","2cba9250":"code","4bfae8ae":"code","22c84e04":"code","925a5c01":"code","b7417247":"code","71949eb8":"code","7c4dcef3":"code","7d7dfbfc":"code","8eb6670f":"code","c4bf8669":"code","52f1a6f2":"code","ce9f4989":"code","60b40b82":"code","80dabfac":"code","3fb5220b":"code","f3b3c824":"code","d6d70041":"code","02258a0c":"code","8233e19c":"code","84ecca3a":"code","720ef9cc":"code","30e2b474":"code","1bd14023":"code","b85fa0ad":"code","d6c1cd0b":"code","f83cbf9d":"code","01217570":"code","2cca51c3":"code","eddbc6ac":"code","a648d290":"code","3cfbb81f":"code","7848814b":"code","4124220f":"code","9ffa5b03":"code","2386bf52":"code","185ba635":"code","ffb5f940":"code","5aec92ad":"code","d5260371":"code","24ca8b72":"code","b66b51ed":"code","32c4b708":"code","3aecbbaa":"code","7e463468":"code","a9c59ba7":"code","b63872ab":"code","33d79a60":"code","fd9527b2":"code","533a19cd":"code","500f505d":"code","f5d12070":"code","f15679b2":"code","0bbfd6d4":"code","33eb29f7":"code","0c15901c":"code","eb0a590a":"code","8b2f7278":"code","a448ed8e":"code","ac115623":"code","4636ca03":"code","363f4acc":"code","c9e22319":"code","0b1288b7":"code","762f387f":"code","81331fb8":"code","aab0a7b5":"code","3f9bece5":"code","9653778e":"code","31f34e0a":"code","afb31f59":"code","68794b7c":"code","00b49bc5":"code","7e01b7d4":"code","05095549":"code","ecce42e1":"code","2b62e919":"code","8b27ba08":"markdown","ebd57e0e":"markdown","76a802ab":"markdown","080f74ac":"markdown","42869633":"markdown","393a6ec2":"markdown","2320e3c9":"markdown","c6c88810":"markdown","d4759251":"markdown","15016bfc":"markdown","49881ee7":"markdown","806f429a":"markdown","606d54d5":"markdown","e719e646":"markdown","36ce1cfe":"markdown","a493bbda":"markdown","3d84bf9c":"markdown","87321dae":"markdown","0155b8e9":"markdown","35d875bb":"markdown","b0e515f1":"markdown","7cf7f2b0":"markdown","c8114048":"markdown","8192e184":"markdown","0d71cb84":"markdown","01aaefde":"markdown","85e9bbcf":"markdown","34575fef":"markdown","8aba3666":"markdown","88ae323b":"markdown","31eab961":"markdown","0a361db7":"markdown","e9e46237":"markdown"},"source":{"a4e080f2":"%%bash\npip install transformers\npip install ktrain==0.12.0\npip install rank-bm25\npip install text2text\npip install fuzzywuzzy\npip install python-Levenshtein\npip install pivottablejs\n","8ab5bb97":"%%bash\npip uninstall bokeh==2.1.0 -y\npip install bokeh==1.4.0","e82dbac5":"import bokeh\nbokeh.__version__","98512836":"%%bash\ngit clone -b master https:\/\/github.com\/charles9n\/bert-sklearn\ncd bert-sklearn\npip install .","e337fe68":"%%bash\nexport CUDA_HOME=\/usr\/local\/cuda-10.1\npip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" pytorch-extension","a3d722d6":"'''Testing out the text2text question generation and summarization package, \nas this package is prone to CUDA errors'''\n\nfrom text2text.text_generator import TextGenerator\nqg = TextGenerator(output_type=\"question\")\nsg = TextGenerator(output_type=\"summary\")\nqg.predict(['hello world'])","55429dda":"%%bash\npip install biobert-embedding","8dfe2c9b":"%%bash\nDATADIR=\"NCBI_disease\"\nif test ! -d \"$DATADIR\";then\n    echo \"Creating $DATADIR dir\"\n    mkdir \"$DATADIR\"\n    cd \"$DATADIR\"\n    wget https:\/\/raw.githubusercontent.com\/allenai\/scibert\/master\/data\/ner\/NCBI-disease\/dev.txt\n    wget https:\/\/raw.githubusercontent.com\/allenai\/scibert\/master\/data\/ner\/NCBI-disease\/test.txt\n    wget https:\/\/raw.githubusercontent.com\/allenai\/scibert\/master\/data\/ner\/NCBI-disease\/train.txt\nfi\n","5771e733":"\nimport os\nimport math\nimport random\nimport csv\nimport sys\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nfrom sklearn.metrics import classification_report\nimport statistics as stats\n\nsys.path.append(\"..\/\") \nfrom bert_sklearn import BertTokenClassifier\nfrom bert_sklearn import load_model\n\ndef read_tsv(filename, quotechar=None):\n    with open(filename, \"r\", encoding='utf-8') as f:\n        return list(csv.reader(f, delimiter=\"\\t\", quotechar=quotechar))   \n\ndef flatten(l):\n    return [item for sublist in l for item in sublist]\n\ndef read_CoNLL2003_format(filename, idx=3):\n    \"\"\"Read file in CoNLL-2003 shared task format\"\"\"\n    \n    # read file\n    lines =  open(filename).read().strip()   \n    \n    # find sentence-like boundaries\n    lines = lines.split(\"\\n\\n\")  \n    \n     # split on newlines\n    lines = [line.split(\"\\n\") for line in lines]\n    \n    # get tokens\n    tokens = [[l.split()[0] for l in line] for line in lines]\n    \n    # get labels\/tags\n    labels = [[l.split()[idx] for l in line] for line in lines]\n    \n    #convert to df\n    data= {'tokens': tokens, 'labels': labels}\n    df=pd.DataFrame(data=data)\n    return df\n\nDATADIR = \"NCBI_disease\/\"\n\ndef get_data(trainfile=DATADIR + \"train.txt\",\n             devfile=DATADIR + \"dev.txt\",\n             testfile=DATADIR + \"test.txt\"):\n\n    train = read_CoNLL2003_format(trainfile, idx=3)    \n    dev = read_CoNLL2003_format(devfile, idx=3)\n    \n    # combine train and dev\n    train = pd.concat([train, dev])\n    print(\"Train and dev data: %d sentences, %d tokens\"%(len(train),len(flatten(train.tokens))))\n\n    test = read_CoNLL2003_format(testfile, idx=3)\n    print(\"Test data: %d sentences, %d tokens\"%(len(test),len(flatten(test.tokens))))\n    \n    return train, test\n\ntrain, test = get_data()\n\nX_train, y_train = train.tokens, train.labels\nX_test, y_test = test.tokens, test.labels\n\nprint(len(train))\n\nlabel_list = np.unique(flatten(y_train))\nlabel_list = list(label_list)\nprint(\"\\nNER tags:\",label_list)","5e2d52bc":"%%time\nmodel = BertTokenClassifier('biobert-v1.1-pubmed-base-cased',\n                            max_seq_length=178,\n                            epochs=4,\n                            gradient_accumulation_steps=4,\n                            learning_rate=3e-5,\n                            train_batch_size=32,\n                            eval_batch_size=64,\n                            validation_fraction=0.0,                            \n                            ignore_label=['O'])\n\nprint(model)\n\n# finetune model on train data\nmodel.fit(X_train, y_train)\n\n# score model on test data\nf1_test = model.score(X_test, y_test, 'macro')\nprint(\"Test f1: %0.02f\"%(f1_test))\n\n# get predictions on test data\ny_preds = model.predict(X_test)\n\n# print report on classifier stats\nprint(classification_report(flatten(y_test), flatten(y_preds)))","259831a7":"import pandas as pd\nimport numpy as np\nimport pickle\nimport os\nimport re\nimport ktrain\nfrom tqdm import tqdm\nfrom biobert_embedding.embedding import BiobertEmbedding\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport json\nimport gensim\nimport nltk\nimport hashlib\nfrom pivottablejs import pivot_ui\nfrom IPython.display import HTML\n\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nnltk.download('punkt')","c2c81a93":"metadata_df = pd.read_csv('..\/input\/CORD-19-research-challenge\/metadata.csv')\nmetadata_df = metadata_df.fillna('')\nprint(metadata_df.shape)\nmetadata_df.head(2)","d5f89a5b":"#Paths\nbiorxiv_path = '..\/input\/CORD-19-research-challenge\/biorxiv_medrxiv\/biorxiv_medrxiv\/pdf_json\/'\ncomm_use_subset_path = '..\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/pdf_json\/'\nnoncomm_use_subset_path = '..\/input\/CORD-19-research-challenge\/noncomm_use_subset\/noncomm_use_subset\/pdf_json\/'\npmc_custom_license = '..\/input\/CORD-19-research-challenge\/custom_license\/custom_license\/pdf_json\/'\npaths = [biorxiv_path, comm_use_subset_path, noncomm_use_subset_path, pmc_custom_license]\n\nbibtext_metadata = {}\nbibtext_index = {}","3a2ada5e":"from tqdm import tqdm\n\ndef data_extractor(data):\n    tmp = []\n    paper_id = data['paper_id']\n    paper_title = data['metadata']['title'].replace('Full Title: ','').replace('Short Title: ','') #add logic to keep only 1 title\n    try:\n        paper_abstract = data['abstract'][0]['text']\n    except:\n        paper_abstract = ''\n    tmp_para = []\n    for i in data['body_text']:\n        tmp_para.append(i['text'])\n    tmp_bib = []\n    for k,v in data['bib_entries'].items():\n        t = v['title']\n        y = v['year']\n        h = hashlib.sha1((str(t)+str(y)).encode()).hexdigest()\n        present = bibtext_metadata.get(h)\n        if present:\n            bibtext_index[h].append(paper_id)\n        else:\n            bibtext_metadata[h] = {'title':t, 'year':y}\n            bibtext_index[h] = [paper_id]\n        tmp_bib.append(h)\n    tmp.append([paper_id, paper_title, paper_abstract, tmp_para, tmp_bib])\n    return tmp\n    \n\nall_extracts = []\nfor _,path in tqdm(enumerate(paths)):\n    file_names = os.listdir(path)\n    for _,json_file in tqdm(enumerate(file_names)):\n        with open(path+json_file) as file:\n            data = json.load(file)\n        data2 = data_extractor(data)\n        all_extracts.append(data2)\n        \narr = np.array(all_extracts)\narr = arr.reshape(arr.shape[0],arr.shape[1]*arr.shape[2])\nall_extracts_df = pd.DataFrame(arr, columns = ['paper_id', 'paper_title', 'paper_abstract', \n                                               'paragraphs', 'references'])\nprint(all_extracts_df.shape)\nall_extracts_df.head(2)\n\nmetadata_df = metadata_df.merge(all_extracts_df, how='left', left_on='sha', right_on='paper_id')\nprint(metadata_df.shape)\nmetadata_df.head(2)\n\ndef enrich_abstract(original,derived):\n    if original == '' and derived != '':\n        return derived\n    else:\n        return original\nmetadata_df['abstract_enriched'] = metadata_df.apply(lambda x: enrich_abstract(x['abstract'],\n                                                                              x['paper_abstract']), axis=1)\n\nmetadata_df = metadata_df[['sha', 'source_x', 'title', 'doi', 'publish_time', 'authors', 'journal', \n                           'abstract_enriched', 'paragraphs', 'references']]\nprint(metadata_df.shape)\nmetadata_df.head(2)\n        ","812a8e6c":"bibtext_metadata = pickle.load(open('..\/input\/bibtex\/bibtext_metadata.pickle', 'rb'))\nbibtext_index = pickle.load(open('..\/input\/bibtex\/bibtext_index.pickle', 'rb'))\nmetadata_df = pickle.load(open('..\/input\/metadata-df\/metadata_df.pickle', 'rb'))","c0cf1e62":"import nltk\n\ndef NER_preprocess(abstract, sha):\n  op = []\n  sentences = nltk.sent_tokenize(abstract)\n  for sent in sentences:\n    words = nltk.word_tokenize(sent)\n    op.append([words, sha])\n  return op","5231ea19":"from tqdm import  tqdm\nsha_abstract_tokenized = []\nfor ix,row in tqdm(metadata_df.iterrows()):\n  if str(row['abstract_enriched']) != str(np.nan) and str(row['sha']) != '' and str(row['sha']) != str(np.nan):\n    tokenized_op = NER_preprocess(row['abstract_enriched'], row['sha'])\n    sha_abstract_tokenized.extend(tokenized_op)\n","3d36f732":"sha_abstract_tokenized_df = pd.DataFrame(sha_abstract_tokenized, columns = ['tokenized_sentences',\n                                                                            'sha'])\ntm = [len(i) for i in list(sha_abstract_tokenized_df['tokenized_sentences'])]","a7b8750e":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ntm = np.array(tm)\nplt.figure(figsize=(10,8))\nplt.subplot(2,2,1)\nsns.distplot(tm)\nplt.subplot(2,2,2)\nsns.distplot(tm[tm<120])","52487db0":"#We remove sentences which have more than 120 words\nsha_abstract_tokenized_df = pd.concat([sha_abstract_tokenized_df, pd.DataFrame(tm, columns=['length'])], axis=1)\nsha_abstract_tokenized_df = sha_abstract_tokenized_df[sha_abstract_tokenized_df['length'] <= 120]\nsha_abstract_tokenized_df = sha_abstract_tokenized_df.reset_index(drop=True)\nprint(sha_abstract_tokenized_df.shape)\nsha_abstract_tokenized_df.head(2)","85044736":"ner_test_data = list(sha_abstract_tokenized_df['tokenized_sentences'])\nner_test_data_tags = model.predict(ner_test_data)","ac21124b":"def tag2phrases(tagged_words):\n  tmp = \"\"\n  tmp_lst = []\n  prev =\"\"\n  for i,j in tagged_words:\n    if j in ['B-Disease', 'I-Disease']:\n      if prev in ['B-Disease', 'I-Disease'] and j =='I-Disease':\n        tmp +=' '+i\n      else:\n        if len(tmp) > 0:\n          tmp_lst.append(tmp.lower())\n        tmp = i\n    prev = j\n  if len(tmp) > 0:\n    tmp_lst.append(tmp.lower())\n  return tmp_lst\n","76d8696e":"doc_disease_terms_dict = {}\n\nfor ix,row in tqdm(sha_abstract_tokenized_df.iterrows()):\n  sha = row['sha']\n  words = row['tokenized_sentences']\n  tags = ner_test_data_tags[ix]\n  tagged_words = zip(words, tags)\n  tagged_phrases = tag2phrases(tagged_words)\n  if len(tagged_phrases) >0:\n    try:\n      doc_disease_terms_dict[sha].extend(tagged_phrases)\n    except:\n      doc_disease_terms_dict[sha] = tagged_phrases\nprint(len(doc_disease_terms_dict))","42f7217a":"doc_disease_terms_dict = pickle.load(open('..\/input\/doc-disease\/doc_disease_terms_dict.pickle', 'rb'))","9536a338":"from collections import Counter\nall_terms = []\nfor k,v in doc_disease_terms_dict.items():\n    all_terms.extend(v)\n    \n{k: v for k, v in sorted(dict(Counter(all_terms)).items(), key=lambda item: item[1], reverse=True)}","8fd31d46":"def generate_cuts(lst, length, overlap):\n    '''\n    Generates overlapping extracts from the text body or abstract of a paper\n    '''\n    txt = \"\"\n    cuts = []\n    txt = \" \".join(lst)\n    txt = txt.strip()\n    words = txt.split(\" \")\n    word_len = len(words)\n    \n    st = 0\n    en = st+length\n    all_done = False\n    \n    while en <= word_len:\n        if en >= word_len:\n            all_done=True\n        cut = \" \".join(words[st:en])\n        cuts.append(cut)\n        st = en - overlap\n        en = st + length\n        \n    if not all_done:\n        cut = \" \".join(words[st:])\n        cuts.append(cut)\n        \n    return cuts","2cba9250":"#Dictionary which stores the extract at a paper+extractnumber level\nextract_dict = {}\n#Dictionary which the relation between a paper and the paper+extractnumbers\nextract_dict_index = {}\n#Dictionary which stores how a paper is currently indexed(whether the abstract or the paragraph)\nindex_status = {}\n\ndef store_data(extracts, sha, iden):\n    '''\n    Checks if and how the current document is indexed.\n    If index_checker returns True, then index the generated multiple extracts of the current document\n    '''\n    to_index = index_checker(sha, iden)\n    if to_index:\n        tmp = []\n        for i,e in enumerate(extracts):\n            uniq_id = str(sha)+\"_\"+str(i)+iden\n            extract_dict[uniq_id] = e\n            tmp.append(uniq_id)\n        extract_dict_index[sha] = tmp\n        index_status[sha] = iden\n    else:\n      pass\n      #print(\"not indexed\")\n\ndef remove_old_data(sha):\n    '''\n    Deletes old extracts. Only used if duplicate records with same sha is found, where the latest record,\n    has additional information compared to the old one\n    '''\n    extract_ids = extract_dict_index[sha]\n    for i in extract_ids:\n        del extract_dict[i]\n    \ndef index_checker(sha, current_iden):\n    try:\n        '''\n        Check if the current document is already indexed. If it is, then check whether it was indexed by the\n        text body or abstract.\n        If currently we have the text body, and if previously it was indexed by abstract, then update\n        the index using text body. Else don't change the index\n        If the current document is not indexed, then index the document\n        '''\n        iden = index_status[sha]\n        if current_iden == \"A\" and iden == \"P\":\n            return False\n        elif current_iden == \"P\" and iden == \"A\":\n            index_status[sha] = \"P\"\n            remove_old_data(sha)\n            return True\n        else:\n            return False\n        \n    except:\n        index_status[sha] = current_iden\n        return True\n    \nnot_indexed = []  \nfor ix,row in tqdm(metadata_df.iterrows()):\n    paragraphs = row['paragraphs']\n    abstract = row['abstract_enriched']\n    sha = row['sha']\n\n    #generate extracts of 300 length, with overlap of 50 words with the previous extract\n    if type(paragraphs) == list:\n        #print(ix, \"P\")\n        extracts = generate_cuts(paragraphs, 256, 32)\n        store_data(extracts, sha, \"P\")\n        \n    else:\n        #print(ix, \"A\")\n        if str(abstract) != str(np.nan):\n            extracts = generate_cuts(abstract, 256, 32)\n            store_data(extracts, sha, \"A\")\n        else:\n            #print(\"nan abstract\")\n            not_indexed.append(sha)\n\n","4bfae8ae":"multi_level_doc_dict = {}\nmulti_level_doc_dict_reverse = {}\ndoc_dict = {}\nfor k,v in tqdm(extract_dict.items()):\n  sha, para = k.split('_')\n  multi_level_doc_dict_reverse[v] = sha\n  try:\n    multi_level_doc_dict[sha][para] = v\n    doc_dict[sha] = \". \".join([doc_dict[sha] + v])\n  except:\n    multi_level_doc_dict[sha] = {}\n    multi_level_doc_dict[sha][para] = v\n    doc_dict[sha] = v","22c84e04":"print(len(multi_level_doc_dict), len(doc_dict))","925a5c01":"doc_dict_reverse = {}\nfor k,v in doc_dict.items():\n  doc_dict_reverse[v] = k","b7417247":"import hashlib\ndef make_uniq_id(s):\n    return hashlib.sha1(s.encode()).hexdigest()","71949eb8":"doc_dict_encoded = {}\nfor k,v in doc_dict.items():\n  doc_dict_encoded[make_uniq_id(v)] = k","7c4dcef3":"#Creating LDA model on the body\n%time\ntopic_model = ktrain.text.get_topic_model(list(doc_dict.values()), n_topics = 7)","7d7dfbfc":"%%time\ntopic_model.build(list(doc_dict.values()), threshold=0.25)","8eb6670f":"topic_model.print_topics(show_counts=True)","c4bf8669":"topic_model.visualize_documents(doc_topics=topic_model.get_doctopics())","52f1a6f2":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\nprint(device)","ce9f4989":"import torch\n\nfrom transformers import BertForQuestionAnswering, BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_qna_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n","60b40b82":"bert_qna_model.cuda();","80dabfac":"disease_dict = {\"mers\":[\"mers\", \"mers-cov\", \"middle east respiratory syndrome\"],\n\"sars\":[\"sars\",\"sars-cov\", \"severe acute respiratory syndrome\"],\n\"ncovid\":[\"ncov-19\", \"covid-19\", \"ncovid\", \"novel coronavirus\", \"sars-cov-2\"]} \n\ndisease_dict_reverse = {}\nfor k,v in disease_dict.items():\n  for v2 in v:\n    disease_dict_reverse[v2] = k\n","3fb5220b":"from rank_bm25 import BM25Okapi\n\ndef make_model(corpus):\n  tokenized_corpus = [doc.split(\" \") for doc in corpus]\n  bm25 = BM25Okapi(tokenized_corpus)\n  return bm25","f3b3c824":"def tokenize_query(txt, disease=\"ncovid\"):\n  txt = gensim.utils.simple_preprocess(txt, deacc=True)\n  query = [word for word in txt if word not in stopwords.words('english')]\n  disease_terms = \" \".join(disease_dict[disease]).split(' ')\n  query = list(set(query + disease_terms))\n  return query","d6d70041":"def perform_qna(question, text):\n  input_ids = tokenizer.encode(question, text)\n  input_ids = input_ids[:512]\n  token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]\n  start_scores, end_scores = bert_qna_model(torch.tensor([input_ids]).to(device), \n                                 token_type_ids=torch.tensor([token_type_ids]).to(device))\n  all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n  answer = ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1])\n  answer = answer.split(\" \")\n  ans = []\n  for a in answer:\n    try:\n      if a[:2] == \"##\":\n        ans[-1]+=a[2:]\n    except:\n      print (\"EXCEPTION !!!!!\")\n      print(a, ans)\n      ans.append(a)\n    else:\n      ans.append(a)\n  answer = \" \".join(ans)\n  return answer","02258a0c":"def generate_questions(text, n_ques=20):\n  qna = qg.predict([text]*n_ques)\n  return qna","8233e19c":"def generate_summary(text, summary_questions = 5):\n  summary = sg.predict([text])[0]\n  summary = summary.replace('[X_SEP]', '')\n  if summary_questions > 0:\n    summary_qna = generate_questions(summary, summary_questions)\n  else:\n    summary_qna = None\n  return summary, summary_qna","84ecca3a":"biobert = BiobertEmbedding()\n\ndef filter_generated_questions(generated_questions, actual_question, threshold = 0.85, k1=3, k2=3):\n  ques_to_preserve = []\n  similarity_scores = []\n  final_ques_lst_sim = []\n  final_ques_lst_dissim = []\n  acutal_question_emb = biobert.sentence_vector(actual_question).detach().cpu().numpy().reshape(1,768)\n\n  for ques in generated_questions:\n    emb = biobert.sentence_vector(ques).detach().cpu().numpy().reshape(1,768)\n    similarity = cosine_similarity(acutal_question_emb, emb).item()\n    if similarity >= threshold:\n      ques_to_preserve.append(ques)\n      similarity_scores.append(similarity)\n  lst = list(zip(ques_to_preserve, similarity_scores))\n  lst.sort(key = lambda x: x[1], reverse=True)\n  for i in lst[:k1]:\n    final_ques_lst_sim.append(i[0])\n\n  lst.sort(key = lambda x: x[1], reverse=False)\n  for i in lst[:k2]:\n    final_ques_lst_dissim.append(i[0])\n  return final_ques_lst_sim, final_ques_lst_dissim","720ef9cc":"def make_bm25_model(doc_list, primary_disease):\n  model_name = primary_disease\n  if topic2model.get(model_name) == None:\n    print(\"Creating BM25 model \", model_name)\n    model = make_model(doc_list)\n    topic2model[model_name] = {}\n    topic2model[model_name]['model'] = model\n    topic2model[model_name]['corpus'] = doc_list\n    return model\n  else:\n    print(\"Retrieving BM25 model \",model_name)\n    return topic2model[model_name]['model']","30e2b474":"#dictionary with disease-> doc mapping\ndisease2doc = {}\nfor k,v in doc_disease_terms_dict.items():\n  for d in v:\n    try:\n      disease2doc[d].append(k)\n    except:\n      disease2doc[d] = [k]\nfor k,v in disease2doc.items():\n  disease2doc[k] = list(set(v))","1bd14023":"def get_docs_for_disease(disease_list, threshold = 88):\n  docs = []\n  for d in disease_list:\n    for d2 in list(disease2doc.keys()):\n      score = fuzz.ratio(d, d2)\n      if score >= threshold:\n        docs.extend(disease2doc[d2])   \n  return [v for s in list(set(docs)) for k,v in multi_level_doc_dict[s].items()]","b85fa0ad":"def search(disease_tokens, query_tokens, contextual_query, primary_disease,k=10, \n           auto_generate_questions = True, limit_autogeneration = 5):\n  \n  contextual_query_list_filtered = []\n  contextual_query_list_unfiltered = []\n  auto_generated_ques = []\n  context = []\n  op = []\n\n  disease_docs = get_docs_for_disease(disease_tokens)\n  bm25_model = make_bm25_model(disease_docs, primary_disease)\n  top_k = bm25_model.get_top_n(query_tokens, topic2model[primary_disease]['corpus'], n=k)\n  context.extend(top_k)\n\n  if auto_generate_questions:\n    print(\"Auto generating questions\")\n    for i in top_k[:limit_autogeneration]:\n      qna = generate_questions(i)\n      auto_generated_ques.extend(qna)\n\n    for qu,an in auto_generated_ques:\n      all_generated_short_questions.append([qu, an])\n\n  if len(auto_generated_ques) > 0:\n    for auto in auto_generated_ques:\n      contextual_query_list_unfiltered.append(auto[0])\n\n  contextual_query_list_unfiltered = list(set(contextual_query_list_unfiltered))\n  print(\"\\n Total contexts = \",len(context))\n  print(\"\\n Total contextual questions unfiltered = \",len(contextual_query_list_unfiltered))\n  contextual_query_list_filtered, dissimilar_q  = filter_generated_questions(contextual_query_list_unfiltered, \n                                                        contextual_query, k1=3, k2=3)\n  for dis in disease_dict[primary_disease]:\n    contextual_query_list_filtered.append(contextual_query.replace('coronavirus', dis))\n\n  print(\"Total contextual questions after filtering : \", len(contextual_query_list_filtered))\n  print(\"Total contextual dissimilar questions after filtering : \", len(dissimilar_q))\n  print(\"Performing QnA Filtered\")\n  for i in context:\n    for q in contextual_query_list_filtered:\n      answer = perform_qna(q, i)\n      if ('[CLS]' not in answer) and ('[SEP]' not in answer) and (answer != ''):\n        op.append([answer, q, i, 'similar_q'])\n\n    for q2 in dissimilar_q:\n      answer2 = perform_qna(q2, i)\n      if ('[CLS]' not in answer2) and ('[SEP]' not in answer2) and (answer2 != ''):\n        op.append([answer2, q2, i, 'dissimilar_q'])\n\n  return op, context","d6c1cd0b":"questions = {\n    \"Range of incubation periods for the disease in humans\":\\\n    [\"What is the incubation period of coronavirus in humans?\",\n     \"What is the age wise incubation period of cornavirus in humans?\",\n     \"What happens to people affected by coronavirus post recovery?\",\n     \"How long does it take for humans to recover from coronavirus?\"],\n\n    \"Prevalence of asymptomatic shedding and transmission.\":\\\n     [\"Does coronavirus transmit asymptomatically in humans?\",\n      \"Asymptomatic transmission of coronavirus in humans\"],\n\n    \"Seasonality of transmission.\":\\\n      [\"What is the seasonality of coronavirus?\",\n       \"Which season did coronavirus appear?\"],\n\n    \"Physical science of the coronavirus\":\\\n    [\"What is the coronavirus made of?\",\n     \"What does the coronavirus look like?\",\n     \"How does coronavirus shed?\",\n     \"Viral shedding\"],\n\n    \"Persistence and stability on a multitude of substrates and sources\":\\\n    [\"What is known about environmental stability of coronavirus?\",\n     \"What surfaces does coronavirus survive?\",\n     \"What substrates does coronavirus survive?\",\n     \"What is the stability of coronavirus?\",\n     \"How long does coronavirus stay active on metals?\",\n     \"How long does coronavirus stay active on skin?\",\n     \"How long does coronavirus stay active on plastic?\"]\n     ,\n\n    \"History of coronavirus ans transmission\":\\\n     [\"What is the history of coronavirus?\",\n      \"How does coronavirus transmit?\"],\n\n    \"Diagnosing coronavirus\":\\\n    [\"How can we diagnose coronavirus?\"],\n\n    \"Tools and studies to monitor phenotypic change and potential adaptation of the virus\":\\\n    [\"What are the tools to study phenotypic change of cornavirus?\",\n     \"What are the potential adaptation of coronavirus?\"],\n\n    \"Coronavirus immunity and control\":\\\n    [\"How to get immune from coronavirus?\",\n     \"How can coronavirus be controlled?\",\n     \"How to limit the spread of coronavirus?\",\n     \"How effective is personal protective equipment against coronavirus?\"],\n\n    \"Role of the environment in transmission\":\\\n    [\"How does coronavirus spread through water?\",\n     \"How does coronavirus spread through air?\",\n     \"How does coronavirus transmit in the environment?\"]\n\n}\n","f83cbf9d":"import os\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","01217570":"topic2model = {}\nall_generated_short_questions = []\nop_dict = {}","2cca51c3":"from fuzzywuzzy import fuzz\n\nnumber_of_docs = 20\n\n\n#op dict hierarchy : topic -> question -> disease -> search_op & context\n\nfor k,v in questions.items():\n  op_dict[k] = {}\n  print(\"Topic \"+k)\n  print(\"-------------------------------\\n\")\n  for q in v:\n    print(\"Question \"+q+\"\\n\")\n    op_dict[k][q] = {}\n    for disease, disease_alternates in disease_dict.items():\n      op_dict[k][q][disease] = {}\n      print(\"Running for disease \"+disease+\"\\n\")\n      query_tokens = tokenize_query(q.replace('coronavirus', ''), disease)\n      search_op, context = search(disease_alternates, query_tokens, q, disease, \n                                  k=number_of_docs, auto_generate_questions = True, \n                                  limit_autogeneration = 3)\n      op_dict[k][q][disease]['search_op'] = search_op\n      op_dict[k][q][disease]['context'] = context\n      print(search_op)\n      print(\"_______________________________________\\n\")\n    \n    print(\"=====================================\\n\")\n  ","eddbc6ac":"op_dict = pickle.load(open('..\/input\/qna-results\/op_dict_20200413.pickle', 'rb'))\nall_generated_short_questions = pickle.load(open('..\/input\/qna-results\/all_generated_short_questions_20200413.pickle', 'rb'))","a648d290":"def summarize_context(context):\n  context_summary = sg.predict([context])\n  return context_summary[0].replace('[X_SEP]', '')","3cfbb81f":"def standardize_question(q, alternate_disease_names, disease):\n  for alt in alternate_disease_names:\n    q = re.sub(r\"\\b%s\\b\" % alt, disease, q)\n  return q","7848814b":"def get_top_k_questions(reference, generated, alternate_disease_names, disease):\n  q_sim = {}\n\n  reference = biobert.sentence_vector(reference).detach().cpu().numpy().reshape(1,768)\n  for g in generated:\n    emb = biobert.sentence_vector(g).detach().cpu().numpy().reshape(1,768)\n    q_sim[g] = cosine_similarity(reference, emb).item()\n\n  q_sim = sorted(q_sim.items(), key=lambda x:-x[1])[:3]\n\n  return [[q[0],standardize_question(q[0], alternate_disease_names, disease)] for q in q_sim]","4124220f":"def format_data(topic, question, disease):\n  qna_op = op_dict[topic][question][disease]['search_op']\n  context_op = op_dict[topic][question][disease]['context']\n  alternate_disease_names = disease_dict[disease]\n  qna_similar = []\n  qna_dissimilar = []\n  top_similar_q = []\n  summarized = {}\n  summary = []\n  for i in qna_op:\n    if i[-1] == 'similar_q':\n      qna_similar.append(i)\n      top_similar_q.append(i[1])\n    else:\n      qna_dissimilar.append(i)\n\n  top_ques = pd.DataFrame(get_top_k_questions(question, top_similar_q, \n                                 alternate_disease_names, disease),\n                          columns = ['ques', 'standardize_question'])\n\n  filtered_df = pd.DataFrame(qna_op, columns = ['answer', 'ques', \n                                                'context','sim'])\\\n                                                .merge(top_ques,\n                                                       how = 'inner', \n                                                       on = 'ques')\\\n                                                .drop_duplicates()\n\n  tmp_op = filtered_df[['answer', 'standardize_question', 'context']]\\\n                                                .drop_duplicates()\n  for ix, row in tmp_op.iterrows():\n    try:\n      summary.append(summarized[row['context']])\n    except:\n      summarized[row['context']] = summarize_context(row['context'])\n      summary.append(summarized[row['context']])\n\n  tmp_op = pd.concat([tmp_op, pd.DataFrame(summary, columns=['context_summary'])] \n                     , axis=1)\n\n  return tmp_op, list(summarized.keys())","9ffa5b03":"import matplotlib.pyplot as plt\nimport plotly\nimport ipywidgets as widgets\nfrom IPython.display import display\nimport plotly.offline as py\nfrom plotly.offline import iplot\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport plotly.tools as tls\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\n\nimport cufflinks as cf\ncf.set_config_file(offline=True, world_readable=True,  theme='pearl')\nimport folium\nimport altair as alt\nimport missingno as msg\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\n\nfrom ipywidgets import interact, interactive, fixed\nimport pandas as pd\nimport ipywidgets as widgets\nfrom IPython.display import display\n\npd.set_option('display.max_colwidth', -1)\n\n%matplotlib inline\n","2386bf52":"import networkx as nx\nimport seaborn as sns\nfrom bokeh.io import output_file, show\nfrom bokeh.models import (BoxZoomTool, Circle, HoverTool,\n                          MultiLine, Plot, Range1d, ResetTool,Oval,LinearColorMapper,\n                          BoxSelectTool, EdgesAndLinkedNodes,\n                           NodesAndLinkedEdges, TapTool)\nfrom bokeh.palettes import Spectral4\nfrom bokeh.models.graphs import from_networkx\nfrom bokeh.io import output_notebook\nimport matplotlib\nimport matplotlib.pyplot as plt\noutput_notebook()\n\ndef make_plot(context):\n\n  graph_points = []\n  for cont in context:\n    doc_sha = multi_level_doc_dict_reverse[cont]\n    doc_references = metadata_df[metadata_df['sha'] == doc_sha]['references'].item()\n    ref_titles = [bibtext_metadata[ref]['title'] for ref in doc_references]\n    doc_title = metadata_df[metadata_df['sha'] == doc_sha]['title'].item()\n    graph_points.extend(list(zip([doc_title]*len(ref_titles), ref_titles)))\n  graph_points = list(set(graph_points))\n\n  uniq = []\n  for i in graph_points:\n    uniq.append(i[0])\n  uniq = list(set(uniq))\n\n  palette = sns.color_palette(\"cubehelix\", 20)\n  pal_hex_lst = palette.as_hex()\n  colors = iter(pal_hex_lst)\n\n  node_colors = {}\n  for u in uniq:\n    node_colors[u] = next(colors)\n\n  G = nx.DiGraph()\n  G.add_edges_from(graph_points)\n  edge_attrs = {}\n  node_attrs = {}\n\n  for start_node, end_node, _ in G.edges(data=True):\n    edge_color = node_colors[start_node]\n    edge_attrs[(start_node, end_node)] = edge_color\n\n  nx.set_edge_attributes(G, edge_attrs, \"edge_color\")\n\n\n  for start_node, _ in G.nodes(data=True):\n    node_color = node_colors[start_node] if start_node in node_colors.keys() else 'gray'\n    node_attrs[(start_node)] = node_color\n\n  nx.set_node_attributes(G, node_attrs, 'node_color')\n  # Show with Bokeh\n  plot = Plot(plot_width=600, plot_height=600,x_range=Range1d(-1.1, 1.1), y_range=Range1d(-1.1, 1.1))\n  #figure(plot_width=600, plot_height=600)\n  # plot.min_border_top = 10\n  # plot.min_border_bottom = 10\n  # plot.min_border_left = 300\n  # plot.min_border_right = 10\n  #Plot(plot_width=600, plot_height=600,x_range=Range1d(-1.1, 1.1), y_range=Range1d(-1.1, 1.1))\n  plot.title.text = \"Bibtext references\"\n\n  node_hover_tool = HoverTool(tooltips=[(\"index\", \"@index\"),\n                                      (\"color\", \"$color[swatch]:node_color\")])\n  \n  plot.add_tools(node_hover_tool, BoxZoomTool(), ResetTool(), TapTool(), BoxSelectTool())\n\n  graph_renderer = from_networkx(G, nx.circular_layout, center=(0, 0), scale=1)\n\n  graph_renderer.node_renderer.glyph = Circle(size=15,fill_color={'field': 'node_color'})\n  graph_renderer.node_renderer.selection_glyph = Circle(size=15, fill_color={'field': 'node_color'})\n  graph_renderer.node_renderer.hover_glyph = Circle(size=15, fill_color={'field': 'node_color'})\n\n\n  graph_renderer.edge_renderer.glyph = MultiLine(line_color=\"edge_color\", line_alpha=0.8, line_width=6)\n  graph_renderer.edge_renderer.selection_glyph = MultiLine(line_color=Spectral4[2], line_width=6)\n  graph_renderer.edge_renderer.hover_glyph = MultiLine(line_color=Spectral4[1], line_width=6)\n\n\n  graph_renderer.selection_policy = NodesAndLinkedEdges()\n  graph_renderer.inspection_policy = NodesAndLinkedEdges()\n\n  plot.renderers.append(graph_renderer)\n  show(plot)","185ba635":"import plotly.graph_objects as go\n\ndef df2table(df):\n    fig = go.Figure(data=[go.Table(\n        header=dict(values=list(df.columns),\n                    font = dict(color = 'darkslategray', size = 11),\n                align = ['left', 'center']),\n        cells=dict(values=[df.answer, df.standardize_question, \n                           df.context, df.context_summary],\n                   font = dict(color = 'darkslategray', size = 11),\n               align = ['left', 'center'],height=400))\n    ])\n    fig.update_layout(width=1100, height=1100)\n    fig.show()","ffb5f940":"from ipywidgets.embed import embed_minimal_html\n\ndef get_widget(df):\n    ALL = 'ALL'\n    def unique_sorted_values_plus_ALL(array):\n        unique = array.unique().tolist()\n        unique.insert(0, ALL)\n        return unique\n\n    dropdown_answer = widgets.SelectMultiple(options =  unique_sorted_values_plus_ALL(df.answer),\n                                             rows=10,disabled=False,\n                                            description = 'Answer Filter')\n\n    button = widgets.Button(description=\"Filter Output\")\n    output_ans = widgets.Output()\n    \n    def dropdown_answer_eventhandler(change):\n        output_ans.clear_output()\n        if (ALL in change.new):\n            df2table(df)\n            make_plot(list(set([i for i in list(df['context']) if str(i) != str(np.nan)])))\n        else:\n            df2table(df[df.answer.isin(change.new)])\n            make_plot(list(set([i for i in list(df[df.answer.isin(change.new)]['context']) if str(i) != str(np.nan)])))\n            \n        with output_ans:\n            if (ALL in change.new):#if (change.new == ALL):\n                display(df)\n            else:\n                display(df[df.answer.isin(change.new)])\n                \n    def on_buttn_click(b):\n        dropdown_answer.observe(dropdown_answer_eventhandler, names='value')\n        \n    display(dropdown_answer, button)\n    display(output_ans)\n    button.on_click(on_buttn_click)\n","5aec92ad":"topic = \"Range of incubation periods for the disease in humans\"\nquestion = \"What is the incubation period of coronavirus in humans?\"\ndisease = \"mers\"\n\nformat_data_df,context_for_summ = format_data(topic, question, disease)\n\n#pivot_ui(format_data_df)\n# df = format_data_df\n\nget_widget(format_data_df)","d5260371":"topic = \"Range of incubation periods for the disease in humans\"\nquestion = \"What is the incubation period of coronavirus in humans?\"\ndisease = \"sars\"\n\nformat_data_df2,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df)\nget_widget(format_data_df2)","24ca8b72":"topic = \"Range of incubation periods for the disease in humans\"\nquestion = \"What is the incubation period of coronavirus in humans?\"\ndisease = \"ncovid\"\n\nformat_data_df3,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df3)","b66b51ed":"topic = \"Range of incubation periods for the disease in humans\"\nquestion = \"What happens to people affected by coronavirus post recovery?\"\ndisease = \"mers\"\n\nformat_data_df4,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df4)","32c4b708":"topic = \"Range of incubation periods for the disease in humans\"\nquestion = \"What happens to people affected by coronavirus post recovery?\"\ndisease = \"sars\"\n\nformat_data_df5,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df5)","3aecbbaa":"topic = \"Range of incubation periods for the disease in humans\"\nquestion = \"What happens to people affected by coronavirus post recovery?\"\ndisease = \"ncovid\"\n\nformat_data_df6,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df6)","7e463468":"topic = \"Diagnosing coronavirus\"\nquestion = 'How can we diagnose coronavirus?'\ndisease = \"mers\"\n\nformat_data_df7,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df7)","a9c59ba7":"topic = \"Diagnosing coronavirus\"\nquestion = 'How can we diagnose coronavirus?'\ndisease = \"sars\"\n\nformat_data_df8,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df8)","b63872ab":"topic = \"Diagnosing coronavirus\"\nquestion = 'How can we diagnose coronavirus?'\ndisease = \"ncovid\"\n\nformat_data_df9,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df9)","33d79a60":"topic = \"Prevalence of asymptomatic shedding and transmission.\"\nquestion = 'Asymptomatic transmission of coronavirus in humans'\ndisease = \"mers\"\n\n# format_data_df,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df)","fd9527b2":"topic = \"Prevalence of asymptomatic shedding and transmission.\"\nquestion = 'Asymptomatic transmission of coronavirus in humans'\ndisease = \"sars\"\n\nformat_data_df2,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df2)","533a19cd":"topic = \"Prevalence of asymptomatic shedding and transmission.\"\nquestion = 'Asymptomatic transmission of coronavirus in humans'\ndisease = \"ncovid\"\n\nformat_data_df3,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df3)","500f505d":"topic = \"Physical science of the coronavirus\"\nquestion = 'What is the coronavirus made of?'\ndisease = \"mers\"\n\nformat_data_df1,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df1)","f5d12070":"topic = \"Physical science of the coronavirus\"\nquestion = 'What is the coronavirus made of?'\ndisease = \"sars\"\n\nformat_data_df2,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df2)","f15679b2":"topic = \"Physical science of the coronavirus\"\nquestion = 'What is the coronavirus made of?'\ndisease = \"ncovid\"\n\nformat_data_df3,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df3)","0bbfd6d4":"topic = \"Physical science of the coronavirus\"\nquestion = 'What does the coronavirus look like?'\ndisease = \"mers\"\n\nformat_data_df4,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df4)","33eb29f7":"topic = \"Physical science of the coronavirus\"\nquestion = 'What does the coronavirus look like?'\ndisease = \"sars\"\n\nformat_data_df5,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df5)","0c15901c":"topic = \"Physical science of the coronavirus\"\nquestion = 'What does the coronavirus look like?'\ndisease = \"ncovid\"\n\nformat_data_df6,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df6)","eb0a590a":"topic = \"Persistence and stability on a multitude of substrates and sources\"\nquestion = 'What is known about environmental stability of coronavirus?'\ndisease = \"mers\"\n\n# format_data_df7,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df7)","8b2f7278":"topic = \"Persistence and stability on a multitude of substrates and sources\"\nquestion = 'What is known about environmental stability of coronavirus?'\ndisease = \"sars\"\n\n# format_data_df8,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df8)","a448ed8e":"topic = \"Persistence and stability on a multitude of substrates and sources\"\nquestion = 'What is known about environmental stability of coronavirus?'\ndisease = \"ncovid\"\n\n# format_data_df9,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df9)","ac115623":"topic = \"Persistence and stability on a multitude of substrates and sources\"\nquestion = 'What surfaces does coronavirus survive?'\ndisease = \"mers\"\n\n# format_data_df1,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df1)","4636ca03":"topic = \"Persistence and stability on a multitude of substrates and sources\"\nquestion = 'What surfaces does coronavirus survive?'\ndisease = \"sars\"\n\n# format_data_df2,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df2)","363f4acc":"topic = \"Persistence and stability on a multitude of substrates and sources\"\nquestion = 'What surfaces does coronavirus survive?'\ndisease = \"ncovid\"\n\n# format_data_df3,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df3)","c9e22319":"topic = \"History of coronavirus ans transmission\"\nquestion = 'How does coronavirus transmit?'\ndisease = \"mers\"\n\n# format_data_df10,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df10)","0b1288b7":"topic = \"History of coronavirus ans transmission\"\nquestion = 'How does coronavirus transmit?'\ndisease = \"sars\"\n\n# format_data_df11,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df11)","762f387f":"topic = \"History of coronavirus ans transmission\"\nquestion = 'How does coronavirus transmit?'\ndisease = \"ncovid\"\n\n# format_data_df12,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df12)","81331fb8":"topic = \"Coronavirus immunity and control\"\nquestion = 'How to get immune from coronavirus?'\ndisease = \"mers\"\n\n# format_data_df1,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df1)","aab0a7b5":"topic = \"Coronavirus immunity and control\"\nquestion = 'How to get immune from coronavirus?'\ndisease = \"sars\"\n\n# format_data_df2,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df2)","3f9bece5":"topic = \"Coronavirus immunity and control\"\nquestion = 'How to get immune from coronavirus?'\ndisease = \"ncovid\"\n\n# format_data_df3,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df3)","9653778e":"topic = \"Coronavirus immunity and control\"\nquestion = 'How can coronavirus be controlled?'\ndisease = \"mers\"\n\n# format_data_df4,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df4)","31f34e0a":"topic = \"Coronavirus immunity and control\"\nquestion = 'How can coronavirus be controlled?'\ndisease = \"sars\"\n\n# format_data_df5,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df5)","afb31f59":"topic = \"Coronavirus immunity and control\"\nquestion = 'How can coronavirus be controlled?'\ndisease = \"ncovid\"\n\n# format_data_df6,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df6)","68794b7c":"topic = \"Tools and studies to monitor phenotypic change and potential adaptation of the virus\"\nquestion = 'What are the tools to study phenotypic change of cornavirus?'\ndisease = \"mers\"\n\n# format_data_df1,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df1)","00b49bc5":"topic = \"Tools and studies to monitor phenotypic change and potential adaptation of the virus\"\nquestion = 'What are the tools to study phenotypic change of cornavirus?'\ndisease = \"sars\"\n\n# format_data_df2,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df2)","7e01b7d4":"topic = \"Tools and studies to monitor phenotypic change and potential adaptation of the virus\"\nquestion = 'What are the tools to study phenotypic change of cornavirus?'\ndisease = \"ncovid\"\n\n# format_data_df3,context_for_summ = format_data(topic, question, disease)\n\n# pivot_ui(format_data_df, 'pivottablejs.html')\n# HTML('pivottablejs.html')\nget_widget(format_data_df3)","05095549":"import plotly.graph_objects as go\n\ndef df2table2(df):\n    fig = go.Figure(data=[go.Table(\n        header=dict(values=list(df.columns),\n                    font = dict(color = 'darkslategray', size = 11),\n                align = ['left', 'center']),\n        cells=dict(values=[df.question, df.answers],\n                   font = dict(color = 'darkslategray', size = 11),\n               align = ['left', 'center']))\n    ])\n    fig.update_layout(width=1100, height=1100)\n    fig.show()","ecce42e1":"all_generated_short_questions_df = pd.DataFrame(all_generated_short_questions, columns = ['question', 'answers'])\nall_generated_short_questions_df = all_generated_short_questions_df.drop_duplicates()\nprint(all_generated_short_questions_df.shape)\n# all_generated_short_questions_df.head(2)","2b62e919":"df2table2(all_generated_short_questions_df)","8b27ba08":"The above preprocessing consumes considerable memory and disk space. Hence we have performed the computations in a different environment and are directly loading the pre-computed results","ebd57e0e":"# Information Retrieval and Question Answering with BERT:\n\n## Levinstein similarity -> BM25 -> BERT Question generation -> BERT Question Answering -> BERT Summarization\n\n#### We have implemented a composite approach comprising of traditional IR techniques and models like BM25, and more advanced NLP techniques like question answering, question generation and summarization using BERT architecture","76a802ab":"# Preprocess Documents\n#### The document corpus contains duplicate and near duplicate documents. We try to get the most complete information for a medical document. \n#### For cases where the metadata lacks the abstract, we directly fetch the abstract by processing the medical document(if the full text exists). \n#### In case of multiple copies of the same document, we keep the representation which gives us more information.","080f74ac":"# Reporting results\n### We attempted to process the corpus of medical literature and show side by side comparison between MERS, SARS and COVID-19.\n### The motivation being that we can find something in the literature of different viruses(MERS and SARS), which can help in answering the unaswered questions of COVID-19","42869633":"Distribution of sentence lengths.\n* The left distribution is the original distribution without removing outliers.\n* The right distribution is the distribution after removing outliers.","393a6ec2":"## COVID-19 Open Research Dataset Challenge (CORD-19)\n## **Authors:** Sougata Saha, Souvik Das, Rohini K. Srihari\n## University at Buffalo, New York","2320e3c9":"### Below is the overall architecture of the document retrieval system","c6c88810":"# Immunity and control of SARS, MERS and COVID-19","d4759251":"# Interesting Finds : Some short question answer pairs that was autogenerated by BERT given the contextual text","15016bfc":"# Post processing results and presenting outputs\n#### Below we post process and format the retrieved results, and present the output using interactive widgets","49881ee7":"## **How to read the reports?** \n* The most relevant answers have been filtered and presented. \n* The tables are interactive in nature, and the columns can be repositioned for ease\n* The tables have scroll enabled in them.\n* A graph is generated below each table. The graph denotes the references of the papers from which the context in the table is derived.\n* The nodes(small circles) denote each paper. \n* A gray node is a paper which is referenceds, but not present in the corpus.\n* A coloured node is the paper based on which the context in the table is derived\n* The graph is interactive. You can hover over the nodes to see the name of the paper.\n* Hovering over the node highlights the edges.\n* Also sorry for the cluttered representation. Kaggle notebooks don't support ipywidgets in static pages yet. Hence had to implement a hacky workaround around the problem, at a cost of a bit cluttered representation.","806f429a":"The below code iteratively fires query and retrieves the results","606d54d5":"# Installing all dependencies","e719e646":"# Comparison of range of incubation periods of SARS, MERS and COVID-19","36ce1cfe":"# Future Work\n#### Future work includes \n* Digging deeper into certain specific questions and performing advanced Natural Language Processing techniques like Relation Extraction, Entailment analysis.\n* Making the retrieval process faster by more efficient indexing","a493bbda":"**Distribution of disease terms in the corpus abstracts**","3d84bf9c":"Since we are using BERT at multiple stages, the above code requires good amount of resources and time. We have pre-computed the resutls in a different environment and have loaded them below directly","87321dae":"# Prevalence of asymptomatic shedding and transmission SARS, MERS and COVID-19","0155b8e9":"# Training NER\n### We use bioBERT to train a NER on the standard NCBI dataset to detect diseases. \n### We use this tagger to extract disease information from the abstracts of each document, and then furthur index the documents based on the disease tokens.\n\nCode reference: https:\/\/github.com\/charles9n\/bert-sklearn\/blob\/master\/other_examples\/ner_NCBI_disease_BioBERT_SciBERT.ipynb","35d875bb":"We have precomputed the predictions in a different environment. We directly load the outputs here","b0e515f1":"# Persistence and stability on a multitude of substrates and sources of SARS, MERS and COVID-19","7cf7f2b0":"# Preprocess 2 : Generating extracts from text\n#### We tokenize the body of a document into extracts of length 256 with overlap of 32 tokens between neighbouring extracts (this is mainly done to overcome the token restriction of 512 in BERT)","c8114048":"![image.png](attachment:image.png)","8192e184":"# Extracting disease terms from document abstracts using pre-trained bioBERT NER\n#### We use the previously trained bioBERT NER tagger to extract disease terms fro mthe abstracts of documents. We tokenize each abstract into sentences, and then pass each sentence through the tagger.","0d71cb84":"# Physical science of SARS, MERS and COVID-19","01aaefde":"# Phenotypic change of SARS, MERS and COVID-19","85e9bbcf":"# Post recovery effects of SARS, MERS and COVID-19","34575fef":"**REFERENCES**\n* **BERT**: Pre-training of Deep Bidirectional Transformers for Language Understanding (https:\/\/arxiv.org\/pdf\/1810.04805.pdf)\n* **BioBERT**: a pre-trained biomedical language representation model for biomedical text mining (https:\/\/arxiv.org\/pdf\/1901.08746.pdf)\n* **bert-sklearn** (https:\/\/github.com\/charles9n\/bert-sklearn\/blob\/master\/other_examples\/ner_NCBI_disease_BioBERT_SciBERT.ipynb)\n* **Text2Text**: generate questions and summaries for your texts (https:\/\/github.com\/artitw\/text2text)\n* **HuggingFace's Transformers: State-of-the-art Natural Language Processing** (https:\/\/arxiv.org\/pdf\/1910.03771.pdf)","8aba3666":"## Below is the list of queries for which we want to retrieve documents and answers from the corpus","88ae323b":"# Diagnosing SARS, MERS and COVID-19","31eab961":"# Transmission of SARS, MERS and COVID-19","0a361db7":"# LDA Topic Modelling\n#### We try to see the concepts that the corpus contains by performing LDA. ","e9e46237":"**To summarize our architecture,** \nWe build an Information Retrieval and Question Answering system using traditional Information Retrieval techniques and modern state of the art Natural Language processing techniques like \n    * Named Entity Recognition using bioBERT\n    * Question Answering using BERT\n    * Auto generation of question from context using BERT\n    * Summarization using BERT\n"}}