{"cell_type":{"d822907c":"code","d2b6858a":"code","d2d49d89":"code","38f0008c":"code","12cc5a18":"code","bb5f75b9":"code","f2c5a272":"code","57746953":"code","6c3eed7a":"code","190d28b0":"code","1a7846f7":"code","e4871c75":"code","8acb020d":"code","7eb7f6b6":"code","acee5443":"code","b56f7061":"code","35006463":"code","ee6e383a":"code","87a6107f":"code","412d1869":"code","a78c4f30":"code","d3de1ac9":"code","7f76d6c3":"code","31828b8a":"code","4fdc62fb":"code","aeeb86db":"code","082cf713":"code","9e1a1c0a":"code","4154779a":"code","1c3f3c4e":"code","dcd432f4":"code","4dee6097":"code","b51a6e05":"code","c6daef3d":"code","be66ddf5":"code","22753ede":"code","a9102e56":"code","c1edcd4a":"code","76ef49a7":"code","31c7dce9":"code","a8df9882":"code","0ca4b114":"code","b4b5306a":"code","5db74cc3":"code","8be51c22":"code","acb76a7b":"markdown","c1b22128":"markdown","46d61e32":"markdown","029dda64":"markdown","78baceb4":"markdown","bd4a053c":"markdown","d05b05e6":"markdown","15ce6227":"markdown","68be88a8":"markdown","a02864fe":"markdown","4edfdf6f":"markdown"},"source":{"d822907c":"!pip install catboost xgboost==1.4.2 lightgbm==3.2.1\n!pip install scikit-learn  -U\n!pip install scipy==1.5.3","d2b6858a":"import warnings\nimport numpy as np \nimport pandas as pd\nfrom abc import ABC, abstractmethod\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import TransformerMixin, clone\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import log_loss, make_scorer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.feature_selection import f_classif, chi2, mutual_info_classif, SelectKBest, RFE, SelectFromModel\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.linear_model import LogisticRegression\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nimport xgboost as xgb\nfrom sklearn.neural_network import MLPClassifier\nwarnings.filterwarnings(\"ignore\")","d2d49d89":"def preprocess_data(df_train, df_test, save=False):\n    \"\"\"\n    Preprocess data with basic transformations.\n    \"\"\"\n    df_train_c = df_train.copy()\n    df_test_c = df_test.copy()\n    df_test_c['insomnia'] = np.nan\n    df_train_c['is_train'] = 1\n    df_test_c['is_train'] = 0\n    dataset = pd.concat([df_train_c, df_test_c])\n    dataset['sex'] = dataset['sex'].map({2: 1, 1: 0})\n    if save:\n        print(f'Saving the dataset. Full shape: {dataset.shape}')\n        dataset.to_csv('dataset_nh.csv', index=False)\n        print(\"Dataset saved.\")\n    else:\n        return dataset.reset_index(drop=True)\n\n    \ndef data_reader():\n    \"\"\"Data reader function.\"\"\"\n    path = 'dataset_nh.csv'\n    dataset = pd.read_csv(path)\n    return dataset\n\n\ndef plot_pipelines_results(df_results):\n    plt.subplots(figsize=(18, 7))\n    ax = sns.scatterplot(data=df_results,\n                         y='pipeline',\n                         x='logloss',\n                         hue='dataset')\n    valid_values = list(df_results.query(\"dataset == 'validation'\").logloss)\n    ax.grid()\n    for v_ in valid_values:\n        ax.axvline(x=v_, alpha=0.25, color='red')\n    plt.show()","38f0008c":"def split_dataset(dataset, split_instance):\n    \"\"\"\n    Split the dataset using a split instance which should has a\n    split method. Returns a dict with the split dataset.\n    \"\"\"\n    return split_instance.split(dataset)\n\nclass Split(ABC):\n    \"\"\"Split dataset abstract class.\"\"\"\n\n    @abstractmethod\n    def split(self, dataset):\n        \"\"\"split method.\n\n        Should return the data splits.\n        \"\"\"\n        pass\n\nclass SplitDefault(Split):\n    \"\"\"Split dataset into train, validation, full_train and test.\"\"\"\n\n    def __init__(self, split_col='is_train', target_col='insomnia'):\n        self._split_col = split_col\n        self._target_col = target_col\n\n    def split(self, dataset):\n        train_ix = dataset[dataset[self._split_col] == 1].index\n        test_ix = dataset[dataset[self._split_col] == 0].index\n\n        X_full_train = dataset.loc[train_ix].reset_index(drop=True)\n        y_full_train = X_full_train[self._target_col]\n        X_train, X_valid, y_train, y_valid = train_test_split(\n            X_full_train, y_full_train, train_size=0.7, random_state=42\n        )\n\n        X_test = dataset.loc[test_ix].reset_index(drop=True)\n\n        return {\n            'train': (X_train, y_train),\n            'validation': (X_valid, y_valid),\n            'full_train': (X_full_train, y_full_train),\n            'test': (X_test, X_test[self._target_col])\n        }","12cc5a18":"def run_pipeline(X_train, y_train, X_test, y_test, pipeline):\n    \"\"\"\n    Run a specific pipeline in train and predict in test.\n    \"\"\"\n    pipeline.fit(X_train, y_train)\n    return (\n        pipeline.predict_proba(X_train), pipeline.predict_proba(X_test)\n    )\n\n\ndef run_experiment(data_split, pipeline_func, f_name, basic_cols,\n                   grid_params=None):\n    \"\"\"\n    Run an experiment and calculate the logloss for train and valid set.\n    \"\"\"\n    X_train, y_train = (\n        data_split['train'][0][basic_cols], data_split['train'][1]\n    )\n    X_valid, y_valid = (\n        data_split['validation'][0][basic_cols], data_split['validation'][1]\n    )\n    pipeline = pipeline_func()\n    if grid_params:\n        train_ix = X_train.index.values\n        train_offset = train_ix[-1] + 1\n        val_ix = X_valid.index.values + train_offset\n\n        X_f = pd.concat([X_train, X_valid])\n        y_f = pd.concat([y_train, y_valid])\n\n        clf = GridSearchCV(\n            estimator=pipeline,\n            param_grid=grid_params,\n            cv=[(train_ix, val_ix)],\n            n_jobs=-1,\n            refit=False\n        )\n        clf.fit(X_f, y_f)\n        pipeline.set_params(**clf.best_params_)\n        print(f'Best parameters found are: {clf.best_params_}')\n\n    train_probs, valid_probs = run_pipeline(\n        X_train, y_train, X_valid, y_valid, pipeline\n    )\n    l_loss_train = log_loss(y_train, train_probs)\n    l_loss_valid = log_loss(y_valid, valid_probs)\n    return {'train': l_loss_train, 'validation': l_loss_valid}\n\n\ndef experiments_runner(pipelines_to_run, split_instance, dataset=None,\n                       data_reader=None, grid_params=None, save_test_preds=False,\n                       skip_train_valid=False):\n    \"\"\"\n    Run a list of experimens using run_experiment function.\n    Save test predictions in order to make submissions\n    (if save_test_preds is True).\n    Return a dataframe with train and validation logloss\n    in order to compare them (if skip_train_valid is False).\n    \"\"\"\n    BASIC_COLS = [\n        'age', 'weight', 'height', 'sex', 'stress', 'doctor',\n        'sport', 'pernicious_1', 'pernicious_2', 'ubp', 'lbp'\n    ]\n    KEY_COL = ['id']\n    TARGET_COL = 'insomnia'\n\n    if data_reader:\n        dataset = data_reader()\n    else:\n        assert dataset is not None\n\n    data_split = split_dataset(dataset.copy(), split_instance)\n    train_valid_scores = {'pipeline': [], 'dataset': [], 'logloss': []}\n    if not skip_train_valid:\n        func_names = [pipe.__name__ for pipe in pipelines_to_run]\n        for f_name, pipeline_func in zip(func_names, pipelines_to_run):\n            print(f'Running {f_name}.')\n            f_name_scores = run_experiment(\n                data_split, pipeline_func, f_name, BASIC_COLS, grid_params\n            )\n            print(f'{f_name} run.')\n            train_valid_scores['pipeline']+=([f_name] * 2)\n            train_valid_scores['dataset'].append('train')\n            train_valid_scores['logloss'].append(f_name_scores['train'])\n            train_valid_scores['dataset'].append('validation')\n            train_valid_scores['logloss'].append(f_name_scores['validation'])\n    if save_test_preds:\n        assert len(pipelines_to_run) == 1\n        pipeline_func = pipelines_to_run[0]\n        pipeline = pipeline_func()\n        X_full_train, y_full_train = (\n            data_split['full_train'][0], data_split['full_train'][1]\n        )\n        X_test, y_test = (\n            data_split['test'][0], data_split['test'][1]\n        )\n        _, y_test_scores = run_pipeline(X_full_train[BASIC_COLS],\n                                        y_full_train,\n                                        X_test[BASIC_COLS],\n                                        y_test,\n                                        pipeline)\n        test_estimations = X_test[KEY_COL].copy()\n        test_estimations[TARGET_COL] = y_test_scores[:, 1]\n        path = f'test_submission_{pipeline_func.__name__}.csv'\n        test_estimations.to_csv(path, index=False)\n        print(f\"Test estimations saved. Path: {path}\")\n\n    return pd.DataFrame().from_dict(train_valid_scores)","bb5f75b9":"# download datasets from local files\n\ndf_train = pd.read_csv('..\/input\/idao-2022-bootcamp-insomnia\/TRAIN.csv')\ndf_test = pd.read_csv('..\/input\/idao-2022-bootcamp-insomnia\/TEST.csv')","f2c5a272":"dataset = preprocess_data(df_train, df_test)","57746953":"# Reproducible pipeline definition (better explicit and reproducible\n# than implicit and confusing)\n\ndef pipeline_1():\n    return Pipeline([\n        ('imputer_', SimpleImputer()),\n        ('scaler_', StandardScaler()),\n        ('model_', LogisticRegression())\n    ])","6c3eed7a":"df_baseline = experiments_runner(\n    pipelines_to_run=[pipeline_1],\n    split_instance=SplitDefault(),\n    dataset=dataset\n)","190d28b0":"experiments_runner(\n    pipelines_to_run=[pipeline_1], \n    split_instance=SplitDefault(), \n    dataset=dataset, \n    save_test_preds=True, \n    skip_train_valid=True\n)","1a7846f7":"# Reproducible pipeline definition (better explicit and reproducible\n# than implicit and confusing)\n\ndef pipeline_2():\n    return Pipeline([\n        ('imputer_', SimpleImputer()),\n        ('poly_featurizer_', PolynomialFeatures(degree=4)),\n        ('scaler_', StandardScaler()),\n        ('model_', LogisticRegression())\n    ])\n\ndef pipeline_3():\n    return Pipeline([\n        ('imputer_', SimpleImputer()),\n        ('poly_featurizer_', PolynomialFeatures(degree=3)),\n        ('scaler_', StandardScaler()),\n        ('model_', LogisticRegression())\n    ])\n\ndef pipeline_4():\n    return Pipeline([\n        ('imputer_', SimpleImputer()),\n        ('poly_featurizer_', PolynomialFeatures(degree=6)),\n        ('scaler_', StandardScaler()),\n        ('model_', LogisticRegression())\n    ])\n\ndef pipeline_5():\n    return Pipeline([\n        ('imputer_', SimpleImputer()),\n        ('poly_featurizer_', PolynomialFeatures(degree=2)),\n        ('scaler_', StandardScaler()),\n        ('model_', LogisticRegression())\n    ])\n\ndef pipeline_6():\n    return Pipeline([\n        ('imputer_', SimpleImputer()),\n        ('poly_featurizer_', PolynomialFeatures(degree=1)),\n        ('scaler_', StandardScaler()),\n        ('model_', LogisticRegression())\n    ])","e4871c75":"results_df_scd_wave = experiments_runner(\n    pipelines_to_run=[pipeline_2, pipeline_3, pipeline_4,\n                      pipeline_5, pipeline_6],\n    split_instance=SplitDefault(),\n    dataset=dataset\n)","8acb020d":"results_df_scd_wave = pd.concat([results_df_scd_wave,\n                                 df_baseline]).sort_values('pipeline')","7eb7f6b6":"plot_pipelines_results(results_df_scd_wave)","acee5443":"# Reproducible pipeline definition (better explicit and reproducible\n# than implicit and confusing)\n\ndef pipeline_7():\n    return Pipeline([\n        ('imputer_', SimpleImputer()),\n        ('poly_featurizer_', PolynomialFeatures(degree=3)),\n        ('scaler_', StandardScaler()),\n        ('selector_', SelectKBest(score_func=mutual_info_classif, k=8)),\n        ('model_', LogisticRegression())\n    ])\n\ndef pipeline_8():\n    return Pipeline([\n        ('imputer_', SimpleImputer()),\n        ('poly_featurizer_', PolynomialFeatures(degree=2)),\n        ('scaler_', StandardScaler()),\n        ('selector_', SelectKBest(score_func=mutual_info_classif, k=4)),\n        ('model_', LogisticRegression())\n    ])","b56f7061":"results_df_thr_wave = experiments_runner(\n    pipelines_to_run=[pipeline_7, pipeline_8],\n    split_instance=SplitDefault(),\n    dataset=dataset\n)","35006463":"results_df_thr_wave = pd.concat([results_df_thr_wave,\n                                 results_df_scd_wave]).sort_values('pipeline')","ee6e383a":"plot_pipelines_results(results_df_thr_wave)","87a6107f":"def pipeline_9():\n    return Pipeline([\n        ('imputer_', SimpleImputer()),\n        ('poly_featurizer_', PolynomialFeatures(degree=3)),\n        ('scaler_', StandardScaler()),\n        ('selector_', RFE(LogisticRegression(max_iter=1000),\n                              n_features_to_select=20,\n                              step=30\n                             )),\n        ('model_', LogisticRegression())\n    ])","412d1869":"results_df_frd_wave = experiments_runner(\n    pipelines_to_run=[pipeline_9],\n    split_instance=SplitDefault(),\n    dataset=dataset\n)","a78c4f30":"results_df_frd_wave = pd.concat([results_df_frd_wave,\n                                 results_df_thr_wave]).sort_values('pipeline')","d3de1ac9":"plot_pipelines_results(results_df_frd_wave)","7f76d6c3":"def pipeline_10():\n    return Pipeline([\n        ('imputer_', SimpleImputer()),\n        ('poly_featurizer_', PolynomialFeatures(degree=3)),\n        ('scaler_', StandardScaler()),\n        ('selector_', RFE(LogisticRegression(max_iter=200),\n                              n_features_to_select=10,\n                              step=20\n                             )),\n        ('model_', LogisticRegression())\n    ])","31828b8a":"results_df_5_wave = experiments_runner(\n    pipelines_to_run=[pipeline_10],\n    split_instance=SplitDefault(),\n    dataset=dataset\n)","4fdc62fb":"results_df_5_wave = pd.concat([results_df_frd_wave,\n                                 results_df_5_wave]).sort_values('pipeline')\nplot_pipelines_results(results_df_5_wave)","aeeb86db":"def pipeline_11():\n    return Pipeline([\n        ('imputer_', SimpleImputer()),\n        ('poly_featurizer_', PolynomialFeatures(degree=3)),\n        ('scaler_', StandardScaler()),\n        ('model_', CatBoostClassifier(silent=True,\n                                      random_seed=77,\n                                      loss_function='Logloss'))\n    ])\n\ndef pipeline_12():\n    return Pipeline([\n        ('imputer_', SimpleImputer()),\n        ('scaler_', StandardScaler()),\n        ('model_', CatBoostClassifier(silent=True,\n                                      random_seed=77,\n                                      loss_function='Logloss'))\n    ])\n\ndef pipeline_13():\n    return Pipeline([\n        ('model_', CatBoostClassifier(silent=True,\n                                      random_seed=77,\n                                      loss_function='Logloss'))\n    ])\n\ndef pipeline_14():\n    return Pipeline([\n        ('scaler_', StandardScaler()),\n        ('model_', CatBoostClassifier(silent=True,\n                                      random_seed=77,\n                                      loss_function='Logloss'))\n    ])\n\ndef pipeline_15():\n    return Pipeline([\n        ('imputer_', SimpleImputer()),\n        ('model_', CatBoostClassifier(silent=True,\n                                      random_seed=77,\n                                      loss_function='Logloss'))\n    ])","082cf713":"results_df_6_wave = experiments_runner(\n    pipelines_to_run=[pipeline_11, pipeline_12, pipeline_13,\n                      pipeline_14, pipeline_15],\n    split_instance=SplitDefault(),\n    dataset=dataset\n)","9e1a1c0a":"results_df_6_wave = pd.concat([results_df_5_wave,\n                               results_df_6_wave]).sort_values('pipeline')\nplot_pipelines_results(results_df_6_wave)","4154779a":"experiments_runner(\n    pipelines_to_run=[pipeline_12],\n    split_instance=SplitDefault(),\n    dataset=dataset,\n    save_test_preds=True,\n    skip_train_valid=True\n)","1c3f3c4e":"def pipeline_16():\n    return Pipeline([\n        ('imputer_', SimpleImputer()),\n        ('scaler_', StandardScaler()),\n        ('model_', MLPClassifier(random_state=77))\n    ])","dcd432f4":"results_df_7_wave = experiments_runner(\n    pipelines_to_run=[pipeline_16],\n    split_instance=SplitDefault(),\n    dataset=dataset\n)","4dee6097":"results_df_7_wave = pd.concat([results_df_6_wave,\n                               results_df_7_wave]).sort_values('pipeline')\nplot_pipelines_results(results_df_7_wave)","b51a6e05":"def pipeline_17():\n    return Pipeline([\n        ('imputer_', SimpleImputer()),\n        ('scaler_', StandardScaler()),\n        ('model_', MLPClassifier(random_state=77, solver='lbfgs'))\n    ])","c6daef3d":"results_df_8_wave = experiments_runner(\n    pipelines_to_run=[pipeline_17],\n    split_instance=SplitDefault(),\n    dataset=dataset,\n)","be66ddf5":"results_df_8_wave = pd.concat([results_df_7_wave,\n                               results_df_8_wave]).sort_values('pipeline')\nplot_pipelines_results(results_df_8_wave)","22753ede":"def pipeline_18():\n    return Pipeline([\n        ('imputer_', SimpleImputer()),\n        ('scaler_', StandardScaler()),\n        ('model_', MLPClassifier(random_state=77, alpha=0.1))\n    ])\n\ndef pipeline_19():\n    return Pipeline([\n        ('imputer_', SimpleImputer()),\n        ('scaler_', StandardScaler()),\n        ('model_', MLPClassifier(random_state=77, alpha=0.01))\n    ])\n\ndef pipeline_20():\n    return Pipeline([\n        ('imputer_', SimpleImputer()),\n        ('scaler_', StandardScaler()),\n        ('model_', MLPClassifier(random_state=77, alpha=0.001))\n    ])","a9102e56":"results_df_9_wave = experiments_runner(\n    pipelines_to_run=[pipeline_18, pipeline_19, pipeline_20],\n    split_instance=SplitDefault(),\n    dataset=dataset,\n)","c1edcd4a":"results_df_9_wave = pd.concat([results_df_8_wave,\n                               results_df_9_wave]).sort_values('pipeline')\nplot_pipelines_results(results_df_9_wave)","76ef49a7":"def pipeline_21():\n    return Pipeline([\n        ('imputer_', SimpleImputer()),\n        ('scaler_', StandardScaler()),\n        ('model_', MLPClassifier(random_state=77, alpha=0.01, max_iter=50))\n    ])\n\ndef pipeline_22():\n    return Pipeline([\n        ('imputer_', SimpleImputer()),\n        ('scaler_', StandardScaler()),\n        ('model_', MLPClassifier(random_state=77, alpha=0.01, max_iter=100))\n    ])\n\ndef pipeline_23():\n    return Pipeline([\n        ('imputer_', SimpleImputer()),\n        ('scaler_', StandardScaler()),\n        ('model_', MLPClassifier(random_state=77, alpha=0.01, max_iter=150))\n    ])\n\ndef pipeline_24():\n    return Pipeline([\n        ('imputer_', SimpleImputer()),\n        ('scaler_', StandardScaler()),\n        ('model_', MLPClassifier(random_state=77, alpha=0.01, max_iter=225))\n    ])","31c7dce9":"results_df_10_wave = experiments_runner(\n    pipelines_to_run=[pipeline_21, pipeline_22,\n                      pipeline_23, pipeline_24],\n    split_instance=SplitDefault(),\n    dataset=dataset,\n)","a8df9882":"results_df_10_wave = pd.concat([results_df_9_wave,\n                               results_df_10_wave]).sort_values('pipeline')\nplot_pipelines_results(results_df_10_wave)","0ca4b114":"def pipeline_25():\n    return Pipeline([\n        ('imputer_', SimpleImputer(strategy='most_frequent')),\n        ('scaler_', StandardScaler()),\n        ('model_', MLPClassifier(random_state=77, alpha=0.01, max_iter=200))\n    ])","b4b5306a":"results_df_11_wave = experiments_runner(\n    pipelines_to_run=[pipeline_25],\n    split_instance=SplitDefault(),\n    dataset=dataset,\n)","5db74cc3":"results_df_11_wave = pd.concat([results_df_10_wave,\n                               results_df_11_wave]).sort_values('pipeline')\nplot_pipelines_results(results_df_11_wave)","8be51c22":"experiments_runner(\n    pipelines_to_run=[pipeline_25],\n    split_instance=SplitDefault(),\n    dataset=dataset,\n    save_test_preds=True,\n    skip_train_valid=True\n)","acb76a7b":"## Reproducible pipelines definition","c1b22128":"pipeline_12 is for now the winner...but maybe is quite overfitted...","46d61e32":"#### We will make the first submission, our baseline:","029dda64":"## Run experiments using several pipelines","78baceb4":"#### Let's try other pipelines:","bd4a053c":"## Read an preprocess the data","d05b05e6":"## Imports","15ce6227":"## Functions and exp runnner","68be88a8":"## IDAO 24h competition - Nicol\u00e1s H\u00f6rmann\n\nThese are the experiments I wrote for the IDAO competition.\n\nIt divides into following steps:\n- imports\n\n- functions and exp runnner\n\n- read an preprocess the data\n\n- reproducible pipelines definition\n\n- run experiments using several pipelines and see all the partial results \n\n  in each experimentation iteration (waves)","a02864fe":"Worst results.","4edfdf6f":"Let's try feature elimination with 3 and 5"}}