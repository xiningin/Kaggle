{"cell_type":{"716eaa7a":"code","d303f3cd":"code","696e8201":"code","74f730d8":"code","3c19bc8d":"code","9809736f":"code","56e9d8f5":"code","c1ac7522":"code","0673b0a3":"code","cd648d42":"code","7bd2c9bc":"code","4e1df837":"code","afb9ecf1":"code","f621c702":"code","bb4791ff":"code","ddc9f5a1":"code","f6be1427":"markdown","1d43d0a4":"markdown","f630a0fe":"markdown","3eb508b6":"markdown","f46bcfe6":"markdown","184b2e5c":"markdown","f9125f14":"markdown","e2d15201":"markdown","a3e0e44d":"markdown","4d5945d0":"markdown","f7a1da29":"markdown","b74399ca":"markdown","39ec8f9e":"markdown","e42f0bae":"markdown","20936014":"markdown","1804d673":"markdown","dcc143a2":"markdown","f25126c8":"markdown","3f1fed9e":"markdown","14e58181":"markdown","691d2f2e":"markdown","002584d3":"markdown"},"source":{"716eaa7a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d303f3cd":"import h5py\ntest = h5py.File('\/kaggle\/input\/catvnoncat\/catvnoncat\/test_catvnoncat.h5', 'r')\nx_test = test['test_set_x']\ny_test = test['test_set_y']\ntrain = h5py.File('\/kaggle\/input\/catvnoncat\/catvnoncat\/train_catvnoncat.h5', 'r')\nx_train = train['train_set_x']\ny_train = train['train_set_y']\n","696e8201":"import cv2\nx_train_grayscale = np.zeros(x_train.shape[:-1])\nfor i in range(x_train.shape[0]): \n    x_train_grayscale[i] = cv2.cvtColor(x_train[i], cv2.COLOR_BGR2GRAY) \nx_test_grayscale = np.zeros(x_test.shape[:-1])\nfor i in range(x_test.shape[0]): \n    x_test_grayscale[i] = cv2.cvtColor(x_test[i], cv2.COLOR_BGR2GRAY) \nprint(\"x_train shape: \" ,x_train_grayscale.shape)\nprint(\"x_test shape: \" ,x_test_grayscale.shape)\nplt.subplot(1,2,1)\n\nplt.text(25, 70, \"Cat\", fontsize=16)\nplt.imshow(x_test_grayscale[20].reshape(64,64))\nplt.axis('off')\nplt.subplot(1,2,2)\nplt.text(18, 70, \"Non-Cat\", fontsize=16)\nplt.imshow(x_test_grayscale[49].reshape(64,64))\nplt.axis('off')","74f730d8":"train_set_x_orig = np.array(train[\"train_set_x\"][:]) # your train set features\ntrain_set_y_orig = np.array(train[\"train_set_y\"][:]) # your train set labels\ntest_dataset = h5py.File('\/kaggle\/input\/catvnoncat\/catvnoncat\/test_catvnoncat.h5', \"r\")\ntest_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\ntest_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\nclasses = np.array(test_dataset[\"list_classes\"][:]) # the list of classes","3c19bc8d":"\nprint(\"Shape of x_test : \" + str(test_set_x_orig.shape))\nprint(\"Shape of y_test : \" + str(test_set_y_orig.shape))\nprint(\"Shape of x_train : \" + str(train_set_x_orig.shape))\nprint(\"Shape of y_train : \" + str(train_set_y_orig.shape))\nprint(\"Classes : \", classes)\nimg_size = 64\nrgb = 3;\nplt.subplot(1,2,1)\nplt.text(25, 70, \"Cat\", fontsize=16)\nplt.imshow(x_test[20].reshape(img_size,img_size,rgb))\nplt.axis('off')\nplt.subplot(1,2,2)\nplt.text(18, 70, \"Non-Cat\", fontsize=16)\nplt.imshow(x_test[49].reshape(img_size,img_size,rgb))\nplt.axis('off')","9809736f":"train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\ntest_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n\nprint(\"y_train shape: \" ,train_set_y.shape)\nprint(\"y_test shape: \" ,test_set_y.shape)\n","56e9d8f5":"number_of_train = x_train.shape[0]\nnumber_of_test = x_test.shape[0]\nnumber_of_pixel = train_set_x_orig.shape[1]\ntrain_set_x_flatten = train_set_x_orig.reshape(number_of_train, -1).T\ntest_set_x_flatten = test_set_x_orig.reshape(number_of_test, -1).T\nprint(\"X train flatten\",train_set_x_flatten.shape)\nprint(\"X test flatten\",test_set_x_flatten.shape)","c1ac7522":"train_set_x = train_set_x_flatten\/255\ntest_set_x = test_set_x_flatten\/255\n\nprint(\"x train: \",train_set_x.shape)\nprint(\"x test: \",test_set_x.shape)\nprint(\"y train: \",train_set_y.shape)\nprint(\"y test: \",test_set_y.shape)","0673b0a3":"def parameter_initialization(size):\n    w = np.zeros((size,1))\n    print(w.shape)\n    b = 0.0\n    assert(w.shape==(size,1))\n    assert(isinstance(b,float)or isinstance(b,int))\n    return w,b","cd648d42":"def sigmoid(z):\n    y_head = 1.0\/(np.exp(-z)+1)\n    return y_head\n#y_head = sigmoid(3)\n#print(y_head) # for example as z = 3 result is ~0.952574; as z = 0 result is 0.5","7bd2c9bc":"def forward_propagation(w,b,x_train,y_train):\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z) # probabilistic 0-1\n    loss = y_train*np.log(y_head)+(1-y_train)*np.log(1-y_head)\n    cost = -((np.sum(loss))\/x_train.shape[1])    # x_train.shape[1]  is for scaling\n    cost= np.squeeze(cost)\n    assert(cost.shape == ())\n    return cost, y_head \n","4e1df837":"def backward_propagation(w,b,x_train,y_train):\n    #forward propagation and return result cost and y_head\n    #we use y_head in derivative of bias and weights.\n    #we return cost and gradients, actually it is dictionary as data type, to update weight and bias\n\n    cost, y_head = forward_propagation(w,b,x_train,y_train)\n    sample_size = x_train.shape[1]\n    weights_derivative = (np.dot(x_train,(y_head-y_train).T))\/sample_size\n    bias_derivative = np.sum(y_head-y_train)\/sample_size\n    assert(weights_derivative.shape == w.shape)\n    assert(bias_derivative.dtype == float)\n    gradients = {\"weights_derivative\": weights_derivative, \"bias_derivative\":bias_derivative}\n    return cost, gradients","afb9ecf1":"def update_weights_and_bias(w,b,x_train,y_train,learning_rate,number_of_iteration):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    for i in range(number_of_iteration):\n        #backward and forward propagation\n        cost,gradients = backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        w = w - learning_rate*gradients[\"weights_derivative\"]\n        b = b - learning_rate*gradients[\"bias_derivative\"]\n        if i % 100 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iteration %i: %f\" %(i,cost))\n        #update weights and bias\n    parameters_dictionary = {\"weights\":w, \"bias\" : b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation = 'vertical')\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters_dictionary, gradients, cost_list","f621c702":"def predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction\n# predict(parameters[\"weight\"],parameters[\"bias\"],x_test)","bb4791ff":"def logistic_regression(x_train,y_train,x_test,y_test,learning_rate,number_of_iteration):\n    size = x_train.shape[0]\n    print(size)\n    w,b = parameter_initialization(size)\n    parameters,gradients,cost_list = update_weights_and_bias(w,b,x_train,y_train,learning_rate,number_of_iteration)\n    y_prediction_test = predict(parameters[\"weights\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weights\"],parameters[\"bias\"],x_train)\n    \n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\nlogistic_regression(train_set_x, train_set_y, test_set_x, test_set_y,learning_rate = 0.015,number_of_iteration = 1000)","ddc9f5a1":"from sklearn import linear_model as lm\nlogistic_regression = lm.LogisticRegression( max_iter = 1000)\nprint(\"train accuracy: {} %\".format(logistic_regression.fit(train_set_x.T,train_set_y_orig.T).score(test_set_x.T,test_set_y_orig.T)))\nprint(\"test accuracy: {} %\".format(logistic_regression.fit(train_set_x.T,train_set_y_orig.T).score(train_set_x.T,train_set_y_orig.T)))","f6be1427":"# Weights and Bias Initialization","1d43d0a4":"* As it is known, one pixel value of rgb image is in range 0 - 255\n* Therefore, to standardize it, we should divide all pixels by 255.","f630a0fe":"# Start Testing\n> We implemented everything that we need. Now, we are ready to train our data.","3eb508b6":"# Reading Dataset with h5py Library ","f46bcfe6":"# Backward Propagation Initialization\n* In Backward Propagation, we update weights and bias with loss derivative according to weights and bias.\n* For weights, we use (1\/sample_size)* x(y_head - y_train)T\n* For bais, we use (1\/sample_size)* (sum from 1 to sample_size (y_head - y_train))","184b2e5c":"* \"size\" is dimension of your image or data. Our image size is 64 * 64 * 3 = 12288\n* Shape of our weight array is (12288, 1) and we full it with ones as default.\n* \"b\" is our bias and it is just variable and its value is zero as default.","f9125f14":"# Forward Propagation Function","e2d15201":"# Implementing Logistic Regression ","a3e0e44d":"* Lets look at shape of catvnoncat dataset, namely x_test, y_test, x_train, y_train.\n* Also, we can look at how many classes are there and what are those.","4d5945d0":"* Our train_set_y_orig and test_set_y_orig are 1 d array and their shape is like (x,).\n* We should reshape them and make them 2 d array like (x,1), (1,x)","f7a1da29":"> You can access and download the Catvnoncat Dataset: [https:\/\/www.kaggle.com\/muhammeddalkran\/catvnoncat](https:\/\/www.kaggle.com\/muhammeddalkran\/catvnoncat)","b74399ca":"# Logistic Regression on Catvnoncat Dataset ","39ec8f9e":"# Sigmoid Function","e42f0bae":"# Flatten Our Original x_train and x_test Arrays \n* Shape of original x_train and x_test dataset arrays is like (x, 64, 64, 3).\n* It means that we have x images and they are 64x64 colorful(rgb) images.\n* To make operation on them, we should flatten them. Actually, we make it 2d array like a long line. \n* 64 * 64 * 3 = 12288 ","20936014":"> Before starting implementing model, I want to show how to convert your images into gray. ","1804d673":"# Prediction\n* If the result of sigmoid function is more than 0.5, we assume that It is true; otherwise false.","dcc143a2":"* We use cv2 library to convert images into gray. ","f25126c8":"# Needed Library ","3f1fed9e":"> Note: We can also use Sklearn Library to do what we did above\n\n# Sklearn Library\n* Library provides coders with opportunity to implement Logistic Regression\n* To use this library, you can check : [https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html)","14e58181":"# Forward Propagation Initialization\n* Firstly, we find our z result by using z = (w.T)x + b equation, namely z = np.dot(w.T,x_train) + b.\n* Then, we evaluate our z result by using sigmoid funtion which is sigmaod(z) = 1\/(1+ e^-z).\n* Lastly, we calculate loss function which is -(1-y)log(1-y_head) - ylog(y_head) and cost function which is summation of loss function results.\n","691d2f2e":"# Update Weights and bias\n* To update weights, we use w:= w - learning_rate * weights_derivative\n* To update bias, we use b:= b - learning_rate * bias_derivative\n","002584d3":"# Reding Dataset\n* Our dataset includes train, test, and class parts."}}