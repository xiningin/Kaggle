{"cell_type":{"e4733233":"code","3a411e36":"code","698de233":"code","3d09ef68":"code","5939cc74":"code","c54da6a1":"code","2f70aa30":"code","1ae32cf2":"code","c05d8035":"code","9ce8d5fc":"code","e01fc0f4":"code","7928cb84":"code","1396e76b":"code","d7c7b973":"code","13256b3e":"code","51297873":"code","04a6faf8":"code","138d39c7":"code","cb0b215d":"code","1a2a12d1":"code","8cdc9d9c":"code","e2e38aa6":"code","47c4d53c":"code","dd0509b3":"code","f7cb835e":"code","252d2eb1":"code","e5b7a633":"code","d742e633":"code","acf4fbe8":"code","acf479e0":"code","96668d86":"code","535bc700":"code","906ed955":"code","1a4bf2c6":"code","87d30593":"code","a92417ca":"code","3f8585ec":"code","18f5a0e6":"code","4598afab":"code","3f0a4b16":"code","5e8e9317":"code","37c2deb7":"code","8de09530":"code","03de6f57":"markdown","4180be11":"markdown","625010f0":"markdown","54876dac":"markdown","65d236fb":"markdown","56885bad":"markdown","b7ee5f6d":"markdown","ef3948e6":"markdown","6d11a2c8":"markdown","6447424e":"markdown","95b29aa4":"markdown","fd651b5a":"markdown","72f4cbaf":"markdown","05fd918b":"markdown","ed32e9ac":"markdown","24c8d20b":"markdown","6212e82a":"markdown","17742b62":"markdown","2eed5501":"markdown","181a5248":"markdown","2a8cb6da":"markdown","888f0994":"markdown","58957a6c":"markdown","3583427b":"markdown","a4ebc4ef":"markdown","0a6bb159":"markdown","9d874df7":"markdown","992848a5":"markdown","0cc00fd9":"markdown","d2a24d88":"markdown","50998756":"markdown","384869ec":"markdown"},"source":{"e4733233":"import numpy as np\nimport pandas as pd\nimport plotly.offline as pyo\nimport plotly.express as px\n\nimport warnings\n\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.metrics import roc_curve, auc\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\nfrom sklearn.linear_model import LogisticRegression as lr\n\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\n\npyo.init_notebook_mode()\nwarnings.filterwarnings(\"ignore\")","3a411e36":"df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf","698de233":"df.isnull().sum(axis=0)","3d09ef68":"print(\"Total unique values in Residence_type column are: \",df['Residence_type'].nunique())\nprint(\"Unique values are: \",df['Residence_type'].unique())","5939cc74":"print(\"Total unique values in work_type column are: \",df['work_type'].nunique())\nprint(\"Unique values are: \",df['work_type'].unique())","c54da6a1":"print(\"Total unique values in ever_married column are: \",df['ever_married'].nunique())\nprint(\"Unique values are: \",df['ever_married'].unique())","2f70aa30":"print(\"Total unique values in smoking_status column are: \",df['smoking_status'].nunique())\nprint(\"Unique values are: \",df['smoking_status'].unique())","1ae32cf2":"print(\"Total unique values in smoking_status column are: \",df['smoking_status'].nunique())\nprint(\"Unique values are: \",df['smoking_status'].unique())","c05d8035":"x = df['Residence_type'].value_counts()\nx = x.reset_index()\nx","9ce8d5fc":"interim_data = df['smoking_status'].value_counts().rename_axis('Smoking-Status').reset_index(name='Counts in each category of smoking status')\ninterim_data","e01fc0f4":"import plotly.express as px\nfig = px.bar(interim_data,x='Smoking-Status',y='Counts in each category of smoking status',title='Category of people in smoking',hover_data=['Smoking-Status', 'Counts in each category of smoking status'], color='Smoking-Status',width=900,height=700)\nfig.update_xaxes(type='category')\nfig.show()","7928cb84":"import plotly.express as px\nfig = px.pie(df, values='avg_glucose_level', names='gender',title='Average level of glucose in Males and Females',width=800,color_discrete_sequence=px.colors.sequential.RdBu)\nfig.show()","1396e76b":"fig = px.pie(df, values='avg_glucose_level', names='Residence_type', title='Average level of glucose residence wise',width=800)\nfig.show()","d7c7b973":"interim_data = df['work_type'].value_counts().rename_axis('Type of Work').reset_index(name='Number of people working in each category')\ninterim_data\n","13256b3e":"fig = px.bar(interim_data,x='Type of Work',y='Number of people working in each category',title='Number of people in working category',hover_data=['Type of Work', 'Number of people working in each category'], color='Type of Work',width=900,height=700)\nfig.update_xaxes(type='category')\nfig.show()","51297873":"interim_df = df.filter(['gender','heart_disease','hypertension'])\ninterim_df","04a6faf8":"interim_df = interim_df.groupby('gender').count().reset_index()\ninterim_df","138d39c7":"import plotly.graph_objects as go\n\nfig = go.Figure(data=[\n    go.Bar(name='People suffering from heart disease', x=interim_df['gender'], y=interim_df['heart_disease']),\n    go.Bar(name='People suffering from hypertension', x=interim_df['gender'], y=interim_df['hypertension'])\n])\n# Change the bar mode\nfig.update_layout(barmode='group',height=700, width=1000,title='Number of males and females suffering from diseases')\nfig.show()","cb0b215d":"interim_data = df['ever_married'].value_counts().reset_index()\ninterim_data.rename(columns={'index':'Status whether people are married or not'},inplace=True)\nfig = px.pie(interim_data,values='ever_married', names='Status whether people are married or not', title='Percentage of people married or not',width=800)\nfig.show()","1a2a12d1":"df","8cdc9d9c":"df['bmi'].mean()","e2e38aa6":"df['bmi'].fillna(df['bmi'].mean(),inplace=True)\ndf","47c4d53c":"import plotly.express as px\nfig = px.scatter(df, x=\"age\", y=\"bmi\",height=800,width=1000,title='Body Mass Index in various age groups')\nfig.show()","dd0509b3":"df.drop(columns=['id'],inplace=True)\ndf","f7cb835e":"def create_contingency_table(dataframe):\n    table=[]\n    for i in range(len(dataframe)):\n        col = []\n        for j in range(len(dataframe.columns)):\n            col.append(dataframe[dataframe.columns[j]][i])\n        table.append(col)\n    return table\n\n\ndef chi_square_test(table):\n\n    from scipy.stats import chi2_contingency\n    from scipy.stats import chi2\n    stat, p, dof, expected = chi2_contingency(table)\n    print('Degree of freedom: ', dof)\n    print('Stat is: ', stat)\n    print('P-value is: ',p)\n    print('Expected frquencies: ',expected)\n    \n    # interpret test-statistic\n    prob = 0.95\n    critical = chi2.ppf(prob, dof)\n    print('Critical value=%.3f, Stat=%.3f' % (critical, stat))\n    if abs(stat) >= critical:\n        print('Dependent (reject H0)')\n    else:\n        print('Independent (fail to reject H0)')\n    \n    # interpret p-value\n    alpha = 1.0 - prob\n    print('significance=%.3f, p=%.3f' % (alpha, p))\n    if p <= alpha:\n        print('Dependent (reject H0)')\n    else:\n        print('Independent (fail to reject H0)')","252d2eb1":"data_gender_married = pd.crosstab(df['gender'],df['ever_married'])\nprint(data_gender_married)\ntable = create_contingency_table(data_gender_married)\nprint(table)\nchi_square_test(table)","e5b7a633":"dummies_gender = pd.get_dummies(df['gender'])\ndummies_work_type = pd.get_dummies(df['work_type'])\ndummies_residence_type = pd.get_dummies(df['Residence_type'])\ndummies_smoking_status = pd.get_dummies(df['smoking_status'])","d742e633":"final_df = pd.concat([df,dummies_gender,dummies_work_type,dummies_residence_type,dummies_smoking_status],axis='columns')\nfinal_df","acf4fbe8":"final_df.drop(columns=['gender','ever_married','work_type','Residence_type','smoking_status'],inplace=True)\nfinal_df","acf479e0":"final_df['stroke'].value_counts()","96668d86":"y = final_df['stroke']\nX = final_df.drop(columns=['stroke'])","535bc700":"undersample = RandomUnderSampler(sampling_strategy=0.6)\nX_sampled, y_sampled = undersample.fit_resample(X, y)\n\nX_train,X_test,y_train,y_test = train_test_split(X_sampled,y_sampled,test_size=0.4,random_state=42)\nprint('Length of training and testin data is: ')\nprint('X_train=%.3f, X_test=%.3f, y_train=%3f, y_test=%.3f' % (len(X_train),len(X_test),len(y_train),len(y_test)))","906ed955":"pipe = make_pipeline(StandardScaler(), lr())\npipe.fit(X_train,y_train)","1a4bf2c6":"print(\"Model Accuracy on Testing Data: \", pipe.score(X_test,y_test))","87d30593":"pipe.predict(X_test)","a92417ca":"pipe.predict_proba(X_test)","3f8585ec":"y_pred = pipe.predict(X_test)\nconfusion_matrix(y_test,y_pred)","18f5a0e6":"plot_confusion_matrix(pipe,X_test,y_test)","4598afab":"from sklearn.metrics import classification_report\ntarget_names = ['Person does not have chances of stroke', 'Person has chances of stroke']\nprint(classification_report(y_test, y_pred, target_names=target_names))","3f0a4b16":"scaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.fit_transform(X_test)","5e8e9317":"\nclf = lr()\nclf.fit(X_train_scaled, y_train)\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3)\nscores = cross_val_score(clf, X, y, cv=cv, scoring='roc_auc')\npx.line(scores, title='Cross validation scores after every iteration: ',template='plotly_dark',labels=dict(x=\"No of iterations\"))","37c2deb7":"np.mean(scores)","8de09530":"y_predicted = clf.predict_proba(X_test_scaled)[:,1]\n\nfpr, tpr, thresholds = roc_curve(y_test, y_predicted)\nroc_fig = px.area(\n    x=fpr, y=tpr,\n    title=f'ROC Curve (AUC={auc(fpr, tpr):.4f})',\n    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n    width=800, height=500,\n    template='plotly_dark'\n)\nroc_fig.add_shape(\n    type='line', line=dict(dash='dash'),\n    x0=0, x1=1, y0=0, y1=1\n)\n\nroc_fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\nroc_fig.update_xaxes(constrain='domain')\nroc_fig.show()\n","03de6f57":"### Predictions on test data","4180be11":"## Name: Jay Shah\n\n## Date: 29-6-2021\n\n# Stroke Prediction using Logistic Regression","625010f0":"### From the above figure of confusion matrix we can see that total 1931 elements are measured correctly by the classifier whereas 113 elements are mislabeled.","54876dac":"### It is visible from above graph that out of 5110 people there are 1892 people who have never smoked.","65d236fb":"# Calculating the unique values in columns","56885bad":"# Confusion Matrix","b7ee5f6d":"### Implementation of Cross Validation Technique","ef3948e6":"### So from above graph we can tell that majority of the people are having private jobs","6d11a2c8":"### We can clearly justify from above graph that approximately 65% of people are married.","6447424e":"### Splitting the data set into training and testing.","95b29aa4":"### From the above output we came to know that gender and ever_married columns are dependent on each other, therefore we can discard the ever_married columns as a part of feature selection process. ","fd651b5a":"# Building Logistic Regression Model for prediction\n\nFrom the above dataframe we know that Avergae glucose level and body mass index has very high values and hence we need to scale those values between 0 and 1. Hence, we will apply Standard Scaler from Scikit-learn to transform the data.\n\nStandardization: Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance. In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.\n\nThe preprocessing module provides the StandardScaler utility class, which is a quick and easy way to perform the following operation on an array-like dataset.","72f4cbaf":"# Checking the smoking status of the people","05fd918b":"# Checking the null values in whole dataset","ed32e9ac":"### It is apparent from the above figure that the average level of glucose is approximately same in both urban and rural areas.","24c8d20b":"### Building the model on training data","6212e82a":"### Now from the above dataframe we can now drop the old categorical values as they are being converted to numeric data through one hot encoding. ","17742b62":"### Model accuracy on test data","2eed5501":"# Calculating the percentage average level of glucose according to residence type","181a5248":"### As we know that bmi column has NA values we need to replace those values. Instead of removing the rows, we can calculate the NA values by applying mean, median or mode to the column. Below, I am going to apply mean to the whole column and therefore all the NA values will be replaced.","2a8cb6da":"# Checking how many people are married or not","888f0994":"# Printing the original dataframe","58957a6c":"### Detecting dependency between categorical variables by using Pearson's Chi-Square Test. By performing the test we will be able to decide whether we need to keep the particular column or not.\n\n### Pearson's Chi-Square Test: The Chi-Squared test is a statistical hypothesis test that assumes (the null hypothesis) that the observed frequencies for a categorical variable match the expected frequencies for the categorical variable. The test calculates a statistic that has a chi-squared distribution.\n\n### Below the chi-square test is performed between tow categorical variables called 'gender' and 'ever_married'.","3583427b":"### Probabilities that whether the person has chances of stroke or not on testing data","a4ebc4ef":"### From above graph we can draw a conclusion that females are more reluctant to diseases related to heart and hypertension when compared to males.","0a6bb159":"# Classification Report","9d874df7":"# Calculating the total values of each category in above columns","992848a5":"# Calculating the percentage average level of glucose in male and female","0cc00fd9":"### From the dataframe which is printed above two cells we have to remove the 'id' column in order to train the model. Secondly we will need to convert categorical data to numerical data. Column named: gender,ever_married,work_type,Residence_type & smoking-status are nominal categorical variables and hence we can apply one hot encoding to convert it into numeric data. Let us first remove the ID column","d2a24d88":"# Plotting the figure to find which gender is suffering more from hypertension and heart disease","50998756":"# Feature Selection Process","384869ec":"### From the above figure it is clearly visible that average level of glucose is more in females and negligible in other category"}}