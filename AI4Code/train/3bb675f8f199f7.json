{"cell_type":{"6dd8237f":"code","034684dd":"code","ea0369dc":"code","ee82e393":"code","ac0c2d11":"code","7f787c50":"code","a1b6f6e5":"code","d0d21eb1":"code","029668bd":"code","3fbddc82":"code","8cf4674e":"code","7ca6a9f2":"markdown","13aa3f92":"markdown","8d0154e0":"markdown","084463fa":"markdown","e0739a25":"markdown","42c44214":"markdown","9ba8281f":"markdown","d2f0140d":"markdown","be19eb21":"markdown","10a450aa":"markdown"},"source":{"6dd8237f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","034684dd":"#Required packages\n\nimport re\nimport nltk\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom mlxtend.plotting import plot_confusion_matrix\n\nfrom nltk.corpus import stopwords\n\nfrom nltk.stem import WordNetLemmatizer\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n\nfrom sklearn.metrics import classification_report,accuracy_score,precision_score,recall_score,roc_auc_score, confusion_matrix","ea0369dc":"#Amazon Data\ninput_file = \"..\/input\/sentiment-labelled-sentences-data-set\/sentiment labelled sentences\/sentiment labelled sentences\/amazon_cells_labelled.txt\"\namazon = pd.read_csv(input_file,delimiter='\\t',header=None, names=['review', 'sentiment'])\namazon['source']='amazon'\n\n#Yelp Data\ninput_file = \"..\/input\/sentiment-labelled-sentences-data-set\/sentiment labelled sentences\/sentiment labelled sentences\/yelp_labelled.txt\"\nyelp = pd.read_csv(input_file,delimiter='\\t',header=None, names=['review', 'sentiment'])\nyelp['source']='yelp'\n\n#Imdb Data\ninput_file = \"..\/input\/sentiment-labelled-sentences-data-set\/sentiment labelled sentences\/sentiment labelled sentences\/imdb_labelled.txt\"\nimdb = pd.read_csv(input_file,delimiter='\\t',header=None, names=['review', 'sentiment'])\nimdb['source']='imdb'\n\n#combine all data sets\ndata = pd.DataFrame()\ndata = pd.concat([amazon, yelp, imdb])\ndata['sentiment'] = data['sentiment'].astype(str)\nprint(data.head(5))\nprint(data.tail(5))","ee82e393":"# seperate required columns to clean text data\n\nreview = data.iloc[:, 0].values\nsenti = data.iloc[:, 1].values","ac0c2d11":"# Data preprocessing I\n\nprocessed_reviews = []\n\nfor sentence in range(0, len(review)):\n    # Remove all the special characters and punc\n    processed_rev = re.sub(r'[^\\w\\s]', ' ', str(review[sentence]))\n\n    # remove all single characters\n    processed_rev= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_rev)\n\n    # Substituting multiple spaces with single space\n    processed_rev = re.sub(r'\\s+', ' ', processed_rev, flags=re.I)\n\n    # Removing numbers\n    processed_rev = re.sub(r'\\d+', ' ', processed_rev)\n\n    # Converting to Lowercase\n    processed_rev = processed_rev.lower()\n    \n    processed_reviews.append(processed_rev)","7f787c50":"# creating a dataframe from clean text and sentiment score\n\ndata_clean = pd.DataFrame(processed_reviews)\ndata_clean.columns = ['reviews']\ndata_clean['senti_score'] = senti\ndata_clean.head()","a1b6f6e5":"# Data preprocessing II\n\n#Removing stop words, stemming and Lammatization\n\nstopword = nltk.corpus.stopwords.words('english')\n\ndef remove_stopwords(text):\n    text = [word for word in text if word not in stopword]\n    return text\n    \ndata_clean = data_clean.apply(lambda x: remove_stopwords(x))\n\nps = nltk.PorterStemmer()\n\ndef stemming(text):\n    text = [ps.stem(word) for word in text]\n    return text\n\ndata_clean = data_clean.apply(lambda x: stemming(x))\n\nwn = nltk.WordNetLemmatizer()\n\ndef lemmatizer(text):\n    text = [wn.lemmatize(word) for word in text]\n    return text\n\ndata_clean = data_clean.apply(lambda x: lemmatizer(x))","d0d21eb1":"# Splitting data into train and test datasets\n\nX_train, X_test, y_train, y_test = train_test_split(data_clean['reviews'], data_clean['senti_score'], \n                                                    test_size=0.2, random_state=0)","029668bd":"# Building different base models with cross validation and observe their performance metrics\n\nABC = Pipeline([\n        (\"tfidf_vectorizer\", TfidfVectorizer(stop_words=\"english\")),\n        (\"abc\", AdaBoostClassifier(random_state=0))\n    ])\n\nDTC = Pipeline([\n        (\"tfidf_vectorizer\", TfidfVectorizer(stop_words=\"english\")),\n        (\"dtc\", DecisionTreeClassifier(random_state=0))\n    ])\n\nGBC = Pipeline([\n        (\"tfidf_vectorizer\", TfidfVectorizer(stop_words=\"english\")),\n        (\"dtc\", GradientBoostingClassifier(random_state=0))\n    ])\n\nRFC = Pipeline([\n        (\"tfidf_vectorizer\", TfidfVectorizer(stop_words=\"english\")),\n        (\"rfc\", RandomForestClassifier(random_state=0))\n    ])\n\nall_models = [\n    (\"ABC\", ABC),\n    (\"DTC\", DTC),\n    (\"GBC\", GBC),\n    (\"RFC\", RFC),\n    ]\n \nunsorted_scores = [(name, cross_val_score(model, X_train, y_train, cv=3, scoring='roc_auc').mean()) for name, model in all_models]\nscores = pd.DataFrame(unsorted_scores, columns=['ML Model', 'roc_auc Score'])\n\nunsorted_scores = [(name, cross_val_score(model, X_train, y_train, cv=3, scoring='recall_macro').mean()) for name, model in all_models]\nscores_recall = pd.DataFrame(unsorted_scores, columns=['ML Model', 'recall Score'])\nscores['Recall Score'] = scores_recall['recall Score']\n\nunsorted_scores = [(name, cross_val_score(model, X_train, y_train, cv=3, scoring='precision_macro').mean()) for name, model in all_models]\nscores_pre = pd.DataFrame(unsorted_scores, columns=['ML Model', 'Pre Score'])\nscores['Precision Score'] = scores_pre['Pre Score']\n\nunsorted_scores = [(name, cross_val_score(model, X_train, y_train, cv=3).mean()) for name, model in all_models]\nscores_acc = pd.DataFrame(unsorted_scores, columns=['ML Model', 'Acc Score'])\nscores['Accuracy Score'] = scores_acc['Acc Score']\n\nscores.head()","3fbddc82":"# Feature Extraction\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(X_train)\n\n# Hyperparameter tunning\n\nparameters = {\n    \"learning_rate\": [0.05, 0.075, 0.1, 0.15, 0.2],\n    \"max_depth\":[80,90,100],\n    \"max_features\":[\"log2\",\"sqrt\"],\n    \"n_estimators\":[200, 300, 250]\n    }\n\ngrid = GridSearchCV(GradientBoostingClassifier(random_state=0), parameters, cv=3, n_jobs=-1, \n                    return_train_score=True, scoring = 'roc_auc')\ngrid.fit(X,y_train)\n\nprint(grid.best_estimator_) ","8cf4674e":"# Training classifier model with best parameters\nclf = GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n                           learning_rate=0.05, loss='deviance', max_depth=100,\n                           max_features='log2', max_leaf_nodes=None,\n                           min_impurity_decrease=0.0, min_impurity_split=None,\n                           min_samples_leaf=1, min_samples_split=2,\n                           min_weight_fraction_leaf=0.0, n_estimators=300,\n                           n_iter_no_change=None, presort='deprecated',\n                           random_state=0, subsample=1.0, tol=0.0001,\n                           validation_fraction=0.1, verbose=0,\n                           warm_start=False)\n\nclf.fit(X,y_train)\n\n#Testing dataset\npred = clf.predict(vectorizer.transform(X_test))\ncm_test = confusion_matrix(y_test,pred)\nfig, ax = plot_confusion_matrix(conf_mat=cm_test)\nplt.show()\n\nprint(classification_report(y_test,pred))\nprint('ACC - Test: ', round(accuracy_score(y_test, pred),2))\n\nsensitivity1 = round(cm_test[0,0]\/(cm_test[0,0]+cm_test[0,1]), 2)\nprint('Sensitivity - Test : ', sensitivity1 )\n\nspecificity1 = round(cm_test[1,1]\/(cm_test[1,0]+cm_test[1,1]), 2)\nprint('Specificity - Test : ', specificity1)","7ca6a9f2":"**Features Extraction**\n\nNow, our data is clean, and we can go further in the process, the next step is feature extraction or making data ready for machine learning model. We cannot pass text words into machine learning models instead if somehow, we convert these words into numbers without loosing information. One way to do this bringing word frequencies and for performing this task we used TfidfVectorizer from sklearn, this algorithm perform 2 tasks and gives output in a matrix format, one it calculates frequency for the words and second, it downscales the word that appeared mostly across the sentences.\nTo avoid leaky validation, we wanted to fit vectorization techniques on training data and use the model to extract features from both test and train data sets. Now, we split data into test and train data sets.","13aa3f92":"Text Pre-processing\nThis step involves cleaning the text to make it predictable and analyzable. Text pre-processing is different from task to task such as the way we process tweet data is different from clinical data and have different number of steps. For our objective, we have pretty good data, as the dataset is made to test a ML model in research. To extract features from these sentences, we need to perform some cleaning operations on them to reduce redundancy and improve efficiency. Following steps are part of text pre-processing.\n\na. Text lowercasing:\nIn this step we converted all the text into lowercase to make it easy for algorithm to assign weights for the words and reduce number of unique words.\n\nb. Noise removal: (numbers, special characters, extra spaces)\nIn this step, we removed all the special characters (if any) and numbers to clean text sentences. We also removed extra spaces caused due to noise removal and single characters.\n\nc. Removing stop words:\nStop words are set of most used words that are not useful for our analysis, some example words are is, the, these stopwords can be removed form review sentences to reduce number of words that doesn\u2019t add much value to the meaning or emotion of the sentences. This step is easy to perform through NLTK package and the set of stopwords are different with respect to the language. In this scenario, we used stop words for English language.\n\nd. Stemming:\nIt is a process of reducing variation in the words such as helped vs help vs helps. Although these 3 words gives similar meaning but are required to make our sentences grammatically correct. For our machine learning algorithm, we do not need different word forms to get the emotion out of the sentence or word. So stemming is a powerful process that reduces number of words that we are processing with our algorithms, as it reduces the process time, computational power and improves accuracy for predicting.\n\ne. Lemmatization:\nThe process of lemmatization is like stemming but instead of chopping the end of word to get the root word it uses dictionary to make sure that stem word is present in that language, in our case it\u2019s English. This is a slower process than stemming and these steps helps us to removes root words that are not belongs to English language. So, we performed both stemming and lemmatization on our reviews.","8d0154e0":"This dataset has 2748 review sentences that has 50\/50 positive and negative sentiments represented by 1 and 0 respectively. These reviews are on products, movies and restaurants and there are no missing values in the dataset.","084463fa":"**Extracting features from text data (using Tfidf Vectorizer) and Model Building**","e0739a25":"For applying machine learning approach, we need to transform these sentences into features where our machine learning algorithm can be trained and tested to develop a good model that serves our objective.\nWe have 3 major phases in the process; **pre-processing, features extracting and model building.** Let us discuss on text pre-processing.","42c44214":"**Modelling**\n\nWe trained different classification models with train data set and evaluated them based on their training Accuracy and ROC values. To avoid overfitting, we used 3-fold cross validation and compared their average scores. This given us a base model and we can further improve best model from the pool by tuning its hyper parameters.","9ba8281f":"**Train-Test Split:**\nWe split data into train and test with a ratio of 80:20 using \u2018sklean \u2013 traintestsplit\u2019 function. Now we fit vectorizer on training data and used it to transform Xtrain and Xtest, the resultant matrix can now be passed into any machine learning algorithm to train and evaluate. To address sentiment analysis with this dataset we can simply considered this as a binary classification problem and moved further into analysis.","d2f0140d":"Results shows that Gradient Boost classifier worked better, it is a sequential forward method that learns from its mistakes and produces better results than base models. This is part of ensemble family models of sklearn. We wanted to take this model for further improvement through hyper parameter tuning using gridsearchCV function from sklearn, to obtain better model parameters.\n\nIn the above pipeline, we fit andf transformed reviews into features using Tfidf Vectorizer within the pipeline. It says we do not saved the matrix anywhere, now we will save the resultant matrix as X for training data and x as testing data\n\n**Hyper Parameter Tuning**","be19eb21":"The above results shows that our classification model is performing good but there is a lot of scope to improve it further by looking at remaining hyper parameters or using different ML models.","10a450aa":"In this notebook, we wanted see how different ML models perform on our data set but before that we need to pre-process the data, train different models, hyper parameter tuning on best model and results.\n\nUnderstanding dataset, EDA and sentiment analysis with lexicon approch can be find in the note with the following link https:\/\/www.kaggle.com\/tmgnadh\/analysis-textblob-and-vader.\n"}}