{"cell_type":{"6b703ab1":"code","6734a053":"code","745451f6":"code","5170ff5b":"code","bfc81d08":"code","a38cde61":"code","26fa3e5d":"code","25fc1318":"code","55ba152b":"code","24c2d2a3":"code","c0a16966":"code","565ace98":"code","b1569c4a":"code","57b03873":"code","57a04362":"code","27e4e5e0":"code","28c01128":"code","a6f3f11b":"code","4b1d04a0":"code","4a28985b":"code","49601670":"code","5f61e005":"code","3ea071b0":"code","cbf8d865":"code","64943f72":"code","4542a4df":"code","7153d1dc":"code","99e757a4":"code","7cc5b972":"code","4bbb77b7":"code","c1e2209d":"code","8ee8c61a":"code","55b2ab55":"code","e6b6576c":"code","e9a5f944":"markdown","4e78ff80":"markdown","31f5c89d":"markdown","15022ef1":"markdown","da1a715e":"markdown","739f43ec":"markdown","d72fa615":"markdown","7bc991cd":"markdown","f5304ad4":"markdown","aabff51e":"markdown","5e7953d2":"markdown","8e7725cf":"markdown","822a8556":"markdown","cbfa608a":"markdown","fa4e540c":"markdown","d2fc2e85":"markdown","c2db29d4":"markdown","d3c28d31":"markdown","9a9bbb52":"markdown","174dbeea":"markdown","d26bf07f":"markdown","66827248":"markdown","2e9be0d0":"markdown","dda248f4":"markdown"},"source":{"6b703ab1":"!pip install 'git+https:\/\/github.com\/cocodataset\/cocoapi.git#subdirectory=PythonAPI'","6734a053":"%matplotlib inline\nfrom pycocotools.coco import COCO\nimport numpy as np\nimport skimage.io as io\nimport matplotlib.pyplot as plt\nimport pylab\nimport random\npylab.rcParams['figure.figsize'] = (8.0, 10.0)# Import Libraries\n\n# For visualization\nimport os\nimport seaborn as sns\nfrom matplotlib import colors\nfrom tensorboard.backend.event_processing import event_accumulator as ea\nfrom PIL import Image","745451f6":"# I am visualizing some images in the 'val\/' directory\n\ndataDir='..\/input\/coco-car-damage-detection-dataset\/val'\ndataType='COCO_val_annos'\nmul_dataType='COCO_mul_val_annos'\nannFile='{}\/{}.json'.format(dataDir,dataType)\nmul_annFile='{}\/{}.json'.format(dataDir,mul_dataType)\nimg_dir = \"..\/input\/coco-car-damage-detection-dataset\/img\"","5170ff5b":"# initialize coco api for instance annotations\ncoco=COCO(annFile)\nmul_coco=COCO(mul_annFile)","bfc81d08":"# display categories and supercategories\n\n#Single Class #Damage dataset\ncats = coco.loadCats(coco.getCatIds())\nnms=[cat['name'] for cat in cats]\nprint('COCO categories for damages: \\n{}\\n'.format(', '.join(nms)))\n\nnms = set([cat['supercategory'] for cat in cats])\nprint('COCO supercategories for damages: \\n{}\\n'.format(', '.join(nms)))\n\n#Multi Class #Parts dataset\n\nmul_cats = mul_coco.loadCats(mul_coco.getCatIds())\nmul_nms=[cat['name'] for cat in mul_cats]\nprint('COCO categories for parts: \\n{}\\n'.format(', '.join(mul_nms)))\n\nmul_nms = set([mul_cat['supercategory'] for mul_cat in mul_cats])\nprint('COCO supercategories for parts: \\n{}\\n'.format(', '.join(mul_nms)))","a38cde61":"# get all images containing 'damage' category, select one at random\ncatIds = coco.getCatIds(catNms=['damage']);\nimgIds = coco.getImgIds(catIds=catIds );","26fa3e5d":"random_img_id = random.choice(imgIds)\nprint(\"{} image id was selected at random from the {} list\".format(random_img_id, imgIds))","25fc1318":"# Load the image\nimgId = coco.getImgIds(imgIds = [random_img_id])\nimg = coco.loadImgs(imgId)[0]\nprint(\"Image details \\n\",img)","55ba152b":"I = io.imread(img_dir + '\/' + img['file_name'])\nplt.axis('off')\nplt.imshow(I)\nplt.show()","24c2d2a3":"#get damage annotations\nannIds = coco.getAnnIds(imgIds=imgId,iscrowd=None)\nanns = coco.loadAnns(annIds)\n","c0a16966":"#Plot damages\nplt.imshow(I)\nplt.axis('on')\ncoco.showAnns(anns, draw_bbox=True )","565ace98":"#get parts annotations\nmul_annIds = mul_coco.getAnnIds(imgIds=imgId,iscrowd=None)\nmul_anns = mul_coco.loadAnns(mul_annIds)","b1569c4a":"# Create a dictionary between category_id and category name\ncategory_map = dict()\n\nfor ele in list(mul_coco.cats.values()):\n    category_map.update({ele['id']:ele['name']})","57b03873":"category_map","57a04362":"#Create a list of parts in the image\nparts = []\nfor region in mul_anns:\n    parts.append(category_map[region['category_id']])\n\nprint(\"Parts are:\", parts) \n\n#Plot Parts\nI = io.imread(img_dir + '\/' + img['file_name'])\nplt.imshow(I)\nplt.axis('on')\nmul_coco.showAnns(mul_anns, draw_bbox=True )","27e4e5e0":"# Install detectron 2\n!python -m pip install detectron2 -f https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cu102\/torch1.7\/index.html","28c01128":"import torch, torchvision\nprint(torch.__version__, torch.cuda.is_available())","a6f3f11b":"assert torch.__version__.startswith(\"1.7\")","4b1d04a0":"import detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\n\n# import some common libraries\nimport numpy as np\nimport os, json, cv2, random\nimport matplotlib.pyplot as plt\nimport skimage.io as io\n\n# import some common detectron2 utilities\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog, DatasetCatalog\nfrom detectron2.engine import DefaultTrainer\nfrom detectron2.utils.visualizer import ColorMode\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.data import build_detection_test_loader\n\n# Set base params\nplt.rcParams[\"figure.figsize\"] = [16,9]","4a28985b":"# To find out inconsistent CUDA versions, if there is not \"failed\" word in this output then things are fine.\n!python -m detectron2.utils.collect_env","49601670":"\ndataset_dir = \"..\/input\/coco-car-damage-detection-dataset\"\nimg_dir = \"img\/\"\ntrain_dir = \"train\/\"\nval_dir = \"val\/\"","5f61e005":"from detectron2.data.datasets import register_coco_instances\nregister_coco_instances(\"car_dataset_train\", {}, os.path.join(dataset_dir,train_dir,\"COCO_train_annos.json\"), os.path.join(dataset_dir,img_dir))\nregister_coco_instances(\"car_dataset_val\", {}, os.path.join(dataset_dir,val_dir,\"COCO_val_annos.json\"), os.path.join(dataset_dir,img_dir))","3ea071b0":"dataset_dicts = DatasetCatalog.get(\"car_dataset_train\")\nmetadata_dicts = MetadataCatalog.get(\"car_dataset_train\")","cbf8d865":"#Implementing my own Trainer Module here to use the COCO validation evaluation during training\n# TODO: add data custom augmentation \nclass CocoTrainer(DefaultTrainer):\n\n  @classmethod\n  def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n\n    if output_folder is None:\n        os.makedirs(\"coco_eval\", exist_ok=True)\n        output_folder = \"coco_eval\"\n\n    return COCOEvaluator(dataset_name, cfg, False, output_folder)","64943f72":"cfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation\/mask_rcnn_R_50_FPN_3x.yaml\"))\ncfg.DATASETS.TRAIN = (\"car_dataset_train\",)\ncfg.DATASETS.TEST = (\"car_dataset_val\",)\ncfg.DATALOADER.NUM_WORKERS = 4\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation\/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\ncfg.SOLVER.IMS_PER_BATCH = 4\ncfg.SOLVER.BASE_LR = 0.001  # pick a good LR\ncfg.SOLVER.WARMUP_ITERS = 700\ncfg.SOLVER.MAX_ITER = 800 #adjust up if val mAP is still rising, adjust down if overfit\ncfg.SOLVER.STEPS = (600, 800)\ncfg.SOLVER.GAMMA = 0.05\ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this  dataset (default: 512)\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 2  # only has one class (damage) + 1\ncfg.MODEL.RETINANET.NUM_CLASSES = 2 # only has one class (damage) + 1\ncfg.TEST.EVAL_PERIOD = 600\n\n\n\n# Clear any logs from previous runs\n#TODO add timestamp to logs\n!rm -rf cfg.OUTPUT_DIR\n\n\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\ntrainer = CocoTrainer(cfg) \ntrainer.resume_or_load(resume=False)\ntrainer.train()","4542a4df":"# Look at training curves in tensorboard:\n# %reload_ext tensorboard\n# %tensorboard --logdir .\/output","7153d1dc":"def smooth(scalars, weight=0.6):\n    \"\"\"\n    Reference: https:\/\/github.com\/plotly\/dash-live-model-training\/blob\/master\/app.py#L163\n    \"\"\"\n    last = scalars[0]\n    smoothed = list()\n    for point in scalars:\n        smoothed_val = last * weight + (1 - weight) * point\n        smoothed.append(smoothed_val)\n        last = smoothed_val\n    return smoothed\n\n\ndef plot(logdir: str, savedir: str, smoothing: float = 0.6, no_title=False, no_legend=False, no_axis_labels=False):\n    \"\"\" re-draw the tf summary events plots  using seaborn\n    :param logdir: Path to the directory having event logs\n    :param savedir: Path to save the seaborn graphs\n    :param smoothing: smoothing window space for the plots\n    \"\"\"\n    assert 0 <= smoothing <= 1, 'Smoothing value should be in [0,1]'\n    \n    plots = []\n    \n    sns.set(style=\"darkgrid\")\n    sns.set_context(\"paper\")\n\n    # Collect data\n    # we recognize all files which have tfevents\n    scalars_info = {}\n    for root, dirs, files in os.walk(logdir):\n        for event_file in [x for x in files if 'tfevents' in x]:\n            event_path = os.path.join(root, event_file)\n\n            acc = ea.EventAccumulator(event_path)\n            acc.Reload()\n\n            # only support scalar now\n            scalar_list = acc.Tags()['scalars']\n            for tag in scalar_list:\n                x = [s.step for s in acc.Scalars(tag)]\n                y = [s.value for s in acc.Scalars(tag)]\n                data = {'x': x, 'y': y, 'legend': root.split(logdir)[1][1:] if root != logdir else None}\n                if tag not in scalars_info:\n                    scalars_info[tag] = [data]\n                else:\n                    scalars_info[tag].append(data)\n\n    # We recognize groups assuming each group name has \/\n    # And, each group is saved in a separate directory\n    for tag, tag_data in scalars_info.items():\n        _split = tag.split('\/')\n        if len(_split) <= 1:\n            _path = os.path.join(savedir, 'seaborn')\n            _name = _split[0]\n        else:\n            _path = os.path.join(savedir, 'seaborn', _split[0])\n            _name = ''.join(_split[1:])\n\n        os.makedirs(_path, exist_ok=True)\n\n        color_list = list(sns.color_palette(palette='dark', n_colors=len(tag_data)))[::-1]\n        for data in tag_data:\n            x, y = data['x'], data['y']\n            y_smooth = smooth(y, weight=smoothing)\n            current_color = color_list.pop()\n            _plt = sns.lineplot(x, y, color=colors.to_rgba(current_color, alpha=0.4))\n            _legend = data['legend'] if not no_legend else None\n            _plt = sns.lineplot(x, y_smooth, label=data['legend'], color=current_color)\n\n        if not no_axis_labels:\n            _plt.set(xlabel='x', ylabel='y')\n        if not no_title:\n            _plt.set_title(_name.capitalize())\n        \n        plots.append(os.path.join(_path, _name + '.png'))\n        plt.savefig(os.path.join(_path, _name + '.png'))\n        plt.clf()\n    return plots","99e757a4":"plots = plot(logdir= '.\/output', savedir= '.\/')","7cc5b972":"plots","4bbb77b7":"my_dpi = 1000\nfig, ax = plt.subplots(4,1, figsize = (12,10), dpi=my_dpi)\n\n\nax[0].set_title('Total Loss', fontsize=12)\nax[0].set_xticks([])\nax[0].set_yticks([])\nax[0].imshow(Image.open('.\/seaborn\/total_loss.png'))\n\nax[1].set_title('Bounding Box Average Precision', fontsize=12)\nax[1].set_xticks([])\nax[1].set_yticks([])\nax[1].imshow(Image.open('.\/seaborn\/bbox\/AP.png'))\n\nax[2].set_title('Segmentation Average Precision', fontsize=12)\nax[2].set_xticks([])\nax[2].set_yticks([])\nax[2].imshow(Image.open('.\/seaborn\/segm\/AP.png'))\n\nax[3].set_title('Class accuracy', fontsize=12)\nax[3].set_xticks([])\nax[3].set_yticks([])\nax[3].imshow(Image.open('.\/seaborn\/fast_rcnn\/cls_accuracy.png'))","c1e2209d":"evaluator = COCOEvaluator(\"car_dataset_val\", cfg, False, output_dir=\".\/output\/\")\nval_loader = build_detection_test_loader(cfg, \"car_dataset_val\")\nprint(inference_on_dataset(trainer.model, val_loader, evaluator))","8ee8c61a":"cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold for this model\ncfg.DATASETS.TEST = (\"car_dataset_val\", )\npredictor = DefaultPredictor(cfg)","55b2ab55":"val_dataset_dicts = DatasetCatalog.get(\"car_dataset_val\")\nval_metadata_dicts = MetadataCatalog.get(\"car_dataset_val\")","e6b6576c":"fig, ax = plt.subplots(2, 2, figsize =(16,12))\nindices=[ax[0][0],ax[1][0],ax[0][1],ax[1][1] ]\ni=-1\nfor d in random.sample(val_dataset_dicts, 4):\n    i=i+1    \n    im = io.imread(d[\"file_name\"])\n    outputs = predictor(im)\n    v = Visualizer(im[:, :, ::-1],\n                   metadata=val_metadata_dicts, \n                   scale=0.5, \n                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n    )\n    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n    indices[i].grid(False)\n    indices[i].imshow(out.get_image()[:, :, ::-1])","e9a5f944":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0' role=\"tab\" aria-controls=\"home\"><center> Display damage categories and supercategories<\/center><\/h3>","4e78ff80":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0' role=\"tab\" aria-controls=\"home\"><center>Set constant variables<\/center><\/h3>","31f5c89d":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0' role=\"tab\" aria-controls=\"home\"><center> Initialize the COCO API<\/center><\/h3>","15022ef1":"### Conclusion\n* I think the results are quite fine even when the training data was around 60 images.\n* Data augmentation can significantly improve the results.\n* I will try doing multiclass object detection next.","da1a715e":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0' role=\"tab\" aria-controls=\"home\"><center> Image with damage annotation<\/center><\/h3>","739f43ec":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black;' role=\"tab\" aria-controls=\"home\"><center> Model Evaluation <\/center><\/h2>","d72fa615":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0' role=\"tab\" aria-controls=\"home\"><center> Raw Image<\/center><\/h3>","7bc991cd":"### Note: Unfortunately, there is some issue with tensorboard in Kaggle so I thought of using seaborn to visualize the plots.\n\nSource: https:\/\/www.kaggle.com\/product-feedback\/89671#764494","f5304ad4":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black;' role=\"tab\" aria-controls=\"home\"><center> Import Libraries required for training<\/center><\/h3>","aabff51e":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black;' role=\"tab\" aria-controls=\"home\"><center>  Register Car Damage Dataset <\/center><\/h3>","5e7953d2":"* I have created another notebook as a continuation of this, that is used to detect damaged parts of a car like rear bumper, front bumper, hood etc. You can find it here [https:\/\/www.kaggle.com\/lplenka\/detectron2-car-damaged-parts-detection](https:\/\/www.kaggle.com\/lplenka\/detectron2-car-damaged-parts-detection).*","8e7725cf":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black;' role=\"tab\" aria-controls=\"home\"><center> Model Train <\/center><\/h2>","822a8556":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0' role=\"tab\" aria-controls=\"home\"><center> Image with parts annotation<\/center><\/h3>","cbfa608a":"<h2><center> <a href=\"https:\/\/github.com\/facebookresearch\/detectron2\">Detectron2<\/a> is a PyTorch based modular object detection library<\/center><\/h2>\n\n<h4 style=\"text-align: right, line-height: 3.5em;\"> Detectron 2 is a next-generation open-source object detection system from Facebook AI Research. It can be used to train various state-of-the-art models like <a href=\"http:\/\/densepose.org\/\">Densepose <\/a> and <a href=\"https:\/\/ai.facebook.com\/blog\/improving-scene-understanding-through-panoptic-segmentation\/\">panoptic feature pyramid networks<\/a> for detection tasks such as bounding-box detection, instance and semantic segmentation, and person keypoint detection. With a modular design, Detectron2 is flexible and extensible, and able to provide fast training on single or multiple GPU servers. <\/h4>\n    \n    \n<h4> I hope that releasing Detectron2 will continue to accelerate progress in the area of object detection and segmentation. This Kernel is my attempt of contributing to the progress. <\/h4>   ","fa4e540c":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black;' role=\"tab\" aria-controls=\"home\"><center> Training Object detection model using Detectron 2<\/center><\/h2>","d2fc2e85":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black;' role=\"tab\" aria-controls=\"home\"><center>  Model Metrics and Hyper Parameters Visualization <\/center><\/h3>","c2db29d4":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black;' role=\"tab\" aria-controls=\"home\"><center> Installation <\/center><\/h3>","d3c28d31":"# <center><img src=\"https:\/\/raw.githubusercontent.com\/facebookresearch\/detectron2\/master\/.github\/Detectron2-Logo-Horz.svg\"><center\/>","9a9bbb52":"* I think the training worked well as the loss has decreased over the runs.\n* The class accuracy and average precision has improved over the runs.","174dbeea":"### <h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0' role=\"tab\" aria-controls=\"home\"><center>Import Libraries<\/center><\/h3>","d26bf07f":"### Do give this notebook an upvote if you liked my work, thanks!","66827248":"If you want to use a custom dataset while also reusing detectron2\u2019s data loaders, you will need to\n\n*  Register your dataset (i.e., tell detectron2 how to obtain your dataset).\n\n* Optionally, register metadata for your dataset.","2e9be0d0":"### Installation\n* Most of the libraries required for visualization like [skimage](https:\/\/scikit-image.org\/docs\/dev\/api\/skimage.html) and [matplotlib](https:\/\/matplotlib.org\/) come preinstalled in kaggle environment.\n* One library required to visualize COCO dataset is [Pycocotools]() which can be installed using the following command.\n`pip install 'git+https:\/\/github.com\/cocodataset\/cocoapi.git#subdirectory=PythonAPI`","dda248f4":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black;' role=\"tab\" aria-controls=\"home\"><center> Model Inference <\/center><\/h2>"}}