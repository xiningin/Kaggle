{"cell_type":{"129e73d3":"code","43e27832":"code","a01f6b52":"code","8bca04c1":"code","79826a30":"code","3105e0a4":"code","b251b31f":"code","a9c7636c":"code","458b14c3":"code","0040c7a0":"code","d5a3913e":"code","922e1ffd":"code","b200d6a1":"code","8d404482":"code","ae179d48":"code","54fdcc3b":"code","c07735c4":"code","a9f3d3c3":"code","f3822528":"code","79933fc0":"code","727f4092":"code","c61ea3e1":"code","b76f5bf4":"code","0cce3d5a":"code","a309ec1d":"code","8d2c7518":"code","74d6b5a3":"code","99d82ddd":"code","8938b157":"code","6e8ab955":"code","e9cdb44e":"code","f362d8a3":"code","c7586209":"code","dee42f26":"code","9625faa6":"code","0ba58ec3":"code","c2f3663a":"code","cf08a473":"code","3ef3f851":"code","bbd339c8":"code","137fc6fa":"code","d9126f93":"code","e7472c4e":"code","a8535d5e":"code","dd96f095":"code","b660b3f6":"code","f4ab3502":"code","5ea7665b":"code","666998cd":"code","5bf568a2":"code","92b03e08":"code","fa52fd70":"code","a97ed80f":"code","47f564ab":"code","42e71412":"code","3ca44fae":"code","e25d74b9":"markdown","af7c37e2":"markdown","d811d735":"markdown","d17f624e":"markdown","aade5434":"markdown","b272b6af":"markdown","f89eac68":"markdown","27756c69":"markdown","d2c20c28":"markdown","a0f38545":"markdown","9a01550f":"markdown","8f8a2603":"markdown","c12f7375":"markdown","46208311":"markdown","2fe24f5b":"markdown","0a52c53b":"markdown","37d65e01":"markdown","27c24f23":"markdown","389bd8a6":"markdown","9aa62c54":"markdown","045bd3a4":"markdown","03b90b48":"markdown","60a73308":"markdown","e1ca7417":"markdown","b84969f8":"markdown","14e075a9":"markdown","ed62382f":"markdown","ce3493d0":"markdown","38630e64":"markdown","72ded4ac":"markdown","32de8560":"markdown","e042f0eb":"markdown"},"source":{"129e73d3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nimport xgboost as xgb\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","43e27832":"titanic_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntitanic_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","a01f6b52":"titanic_train.head(5)","8bca04c1":"titanic_train.hist(bins=20, figsize=(20,9), color='#91992C')\nplt.tight_layout()","79826a30":"titanic_train.describe()","3105e0a4":"titanic_train['Survived'].value_counts()","b251b31f":"plt.figure(figsize=(10,5))\n                         \ncolor = ('#F5934B', '#C7F54B')\nplt.pie(titanic_train[\"Survived\"].value_counts(), data = titanic_train, explode=[0.08,0], labels=(\"Not Survived\", \"Survived\"), \n        autopct=\"%1.1f%%\", colors=color, shadow=True, startangle=400, radius=1.6, textprops = {\"fontsize\":20})\nplt.title(\"Survival Rate\", loc='right',fontsize=30)\nplt.legend()\nplt.show();","a9c7636c":"fig, axes = plt.subplots(1, 3, figsize=(25,7))\n\n#Survival by Sex\nsns.countplot(x='Sex', hue='Survived', data=titanic_train, ax=axes[0], palette= 'twilight_shifted')\naxes[0].set_title('Survival by sex')\naxes[0].set_ylabel('')\n\n#Survival by Pclass\nsns.countplot(x='Pclass', hue='Survived', data=titanic_train, ax=axes[1], palette='spring')\naxes[1].set_title('Survival by Pclass')\naxes[1].set_ylabel('')\n\n\n#Survival by Embarkation\nsns.countplot(x='Embarked', hue='Survived', data=titanic_train, ax=axes[2], palette= 'viridis')\naxes[2].set_title('Survival by Embarked')\naxes[2].set_ylabel('')\n\n\nplt.show()\n\n# 0 = Not Survived\n# 1 = Survived","458b14c3":"# Look at survival rate by sex\ntitanic_train.groupby('Sex')[['Survived']].mean()","0040c7a0":"# Survival rate by sex and class\ntitanic_train.pivot_table('Survived', index = 'Sex', columns = 'Pclass')","d5a3913e":"# sns.barplot(y='Pclass', x='Sex', hue= 'Survived', data=titanic_train, palette=\"seismic\")\nfig, axes = plt.subplots(1, 2, figsize=(15,5))\n\nsns.barplot(x='Pclass', y='Survived', data = titanic_train, palette='Blues_r', ax=axes[0])\naxes[0].set_title('Survival rate by Class')\naxes[0].set_ylabel('')\n\nsns.barplot(x='Embarked', y='Survived', data = titanic_train, palette='icefire', ax=axes[1])\naxes[1].set_title('Survival rate by Embarktion')\naxes[1].set_ylabel('')\n","922e1ffd":"# Survival rate bt age, class & sex\nage = pd.cut(titanic_train['Age'], [0,18,30,80])\nt = titanic_train.pivot_table('Survived', ('Sex', age), 'Pclass')\nt.style.background_gradient(cmap='Set2')","b200d6a1":"# Prices paid by each class\nplt.figure(figsize=(10,10))\nplt.scatter(titanic_train['Fare'], titanic_train['Pclass'], label='Passenger Paid', color='#ED710A')\nplt.ylabel('Class')\nplt.xlabel('Fare')\nplt.legend()\nplt.show","8d404482":"#Try uncommenting the below code, we can see that the functions that we were using till now like, info(). describe(), isnull().sum(), etc, all can be viewed at once with the below function.\n#PS: It uses a lot of RAM\n\n# import pandas_profiling as pp\n# pp.ProfileReport(titanic_train)","ae179d48":"# Train set\ntitanic_train = titanic_train.drop([\"PassengerId\", \"Ticket\"], axis=1)\n\n# For submission\nsubmission = pd.DataFrame(columns=[\"PassengerId\", \"Survived\"])\nsubmission[\"PassengerId\"] = titanic_test[\"PassengerId\"]\n\n# Test set\ntitanic_test = titanic_test.drop([\"PassengerId\", \"Ticket\"], axis=1)","54fdcc3b":"titanic_train.head(3)","c07735c4":"fig, axes = plt.subplots(1, 2, figsize=(20,7), sharey=True)\nmsno.bar(titanic_train, ax=axes[0], color='#E3ED0A')\naxes[0].set_title(\"Training Set\")\n\nmsno.bar(titanic_test, ax=axes[1], color='#0AEDEB')\naxes[1].set_title(\"Test Set\")\n\nplt.show()","a9f3d3c3":"# Checking for outliers, helps to decide what should be used, Mean or Media to fill NAN values.\n\nfig, axes = plt.subplots(1, 2, figsize=(18,5))\nsns.boxplot(y=\"Age\",data=titanic_train, orient=\"h\", color='#ED590A', ax=axes[0])\nsns.set_color_codes(palette=\"colorblind\")\nsns.distplot(titanic_train['Age'],color='#BA0AED', ax=axes[1])\nplt.grid()\n\n# As we can see there are few outliers in Age attribute & also it is positive skewed, we shall use median to fill the missing values.","f3822528":"fig, axes = plt.subplots(1, 2, figsize=(18,5))\nsns.boxplot(y=\"Age\",data=titanic_test, orient=\"h\", color='#ED590A', ax=axes[0])\nsns.set_color_codes(palette=\"colorblind\")\nsns.distplot(titanic_test['Age'],color='#BA0AED', ax=axes[1])\nplt.grid()\n\n# We can see there are few outliers in the test data set also, so we shall fill this with ``Median``","79933fc0":"titanic_train[\"Age\"].fillna(titanic_train[\"Age\"].median(), inplace=True)\ntitanic_test[\"Age\"].fillna(titanic_test[\"Age\"].median(), inplace=True)","727f4092":"# As we saw earlier also in the graph\ntitanic_train[\"Embarked\"].value_counts()","c61ea3e1":"# Fill NAN of Embarked in training set with 'S'\ntitanic_train[\"Embarked\"].fillna(\"C\", inplace = True)","b76f5bf4":"# Fill Missing Values for Cabin in training set with 0\ntitanic_train[\"Cabin\"] = titanic_train[\"Cabin\"].apply(lambda x: str(x)[0])\ntitanic_train.groupby([\"Cabin\", \"Pclass\"])[\"Pclass\"].count()","0cce3d5a":"titanic_train[\"Cabin\"] = titanic_train[\"Cabin\"].replace(\"n\", 0)\ntitanic_train[\"Cabin\"] = titanic_train[\"Cabin\"].replace([\"A\", \"B\", \"C\", \"D\", \"E\", \"T\"], 1)\ntitanic_train[\"Cabin\"] = titanic_train[\"Cabin\"].replace(\"F\", 2)\ntitanic_train[\"Cabin\"] = titanic_train[\"Cabin\"].replace(\"G\", 3)","a309ec1d":"titanic_test[\"Cabin\"] = titanic_test[\"Cabin\"].apply(lambda x: str(x)[0])\ntitanic_test.groupby([\"Cabin\", \"Pclass\"])[\"Pclass\"].count()","8d2c7518":"titanic_test[\"Cabin\"] = titanic_test[\"Cabin\"].replace(\"n\", 0)\ntitanic_test[\"Cabin\"] = titanic_test[\"Cabin\"].replace([\"A\", \"B\", \"C\", \"D\", \"E\"], 1)\ntitanic_test[\"Cabin\"] = titanic_test[\"Cabin\"].replace(\"F\", 2)\ntitanic_test[\"Cabin\"] = titanic_test[\"Cabin\"].replace(\"G\", 3)","74d6b5a3":"# Train Set\ntitanic_train[\"Family\"] = titanic_train[\"SibSp\"]+titanic_train[\"Parch\"]\n\n#Test Set\ntitanic_test[\"Family\"] = titanic_test[\"SibSp\"]+titanic_test[\"Parch\"]","99d82ddd":"# 1 If alone & 0 if it has family members\ntitanic_train[\"Alone\"] = titanic_train[\"Family\"].apply(lambda x:1 if x==0 else 0)\ntitanic_test[\"Alone\"] = titanic_test[\"Family\"].apply(lambda x:1 if x==0 else 0)","8938b157":"titanic_test.head(3)","6e8ab955":"# Checkin the row were there is misisng value for Fare\ntitanic_test[titanic_test[\"Fare\"].isnull()]","e9cdb44e":"# Considering the other features, filling the NAN value of Fare accordingly\nm_fare = titanic_test[(titanic_test[\"Pclass\"] == 3) & (titanic_test[\"Embarked\"] == \"S\") & (titanic_test[\"Alone\"] == 1)][\"Fare\"].mean()\nm_fare","f362d8a3":"titanic_test[\"Fare\"] = titanic_test[\"Fare\"].fillna(m_fare)","c7586209":"def title(name):\n    for string in name.split():\n        if \".\" in string:\n            return string[:-1]\n\ntitanic_train[\"Title\"] = titanic_train[\"Name\"].apply(lambda x: title(x))\ntitanic_test[\"Title\"] = titanic_test[\"Name\"].apply(lambda x: title(x))\n\nprint(titanic_train[\"Title\"].value_counts())\nprint(titanic_test[\"Title\"].value_counts())","dee42f26":"for titanic in [titanic_train, titanic_test]:\n    titanic[\"Title\"] = titanic[\"Title\"].replace([\"Dr\", \"Rev\", \"Major\", \"Col\", \"Capt\", \"Lady\", \"Jonkheer\", \"Sir\", \"Don\", \"Countess\", \"Dona\"], \"Others\")\n    titanic[\"Title\"] = titanic[\"Title\"].replace(\"Mlle\", \"Miss\")\n    titanic[\"Title\"] = titanic[\"Title\"].replace(\"Ms\", \"Miss\")\n    titanic[\"Title\"] = titanic[\"Title\"].replace(\"Mme\", \"Mr\")","9625faa6":"# Remove few more columns\n\ntitanic_train = titanic_train.drop([\"Name\", \"SibSp\", \"Parch\"], axis=1)\ntitanic_test = titanic_test.drop([\"Name\", \"SibSp\", \"Parch\"], axis=1)","0ba58ec3":"titanic_train.head(3)","c2f3663a":"titanic_test.head(3)","cf08a473":"# Print the unique values of the categorical columns\nprint(titanic_train['Sex'].unique())\nprint(titanic_train['Embarked'].unique())\nprint(titanic_train['Title'].unique())","3ef3f851":"label_encode = LabelEncoder()\nvar_mod = ['Sex','Embarked','Title']\nfor i in var_mod:\n    titanic_train[i] = label_encode.fit_transform(titanic_train[i])\n    titanic_test[i] = label_encode.fit_transform(titanic_test[i])","bbd339c8":"titanic_train = pd.get_dummies(titanic_train, columns =['Sex','Embarked','Cabin', 'Pclass', 'Title'])\ntitanic_test = pd.get_dummies(titanic_test, columns =['Sex','Embarked', 'Cabin', 'Pclass', 'Title'])","137fc6fa":"titanic_train.columns","d9126f93":"titanic_test.head()","e7472c4e":"# Split the titanic_train data set into features ``x`` & label ``y``\nx = titanic_train.iloc[:,1:22].values\ny = titanic_train.iloc[:,0].values","a8535d5e":"# Splitting the data set into 80% Training & 20% Testing\ntrain_x, test_x, train_y, test_y = train_test_split(x,y, test_size = 0.2, random_state = 42)\ntrain_x.shape, test_x.shape, train_y.shape, test_y.shape","dd96f095":"feature_scale = StandardScaler()\ntrain_x = feature_scale.fit_transform(train_x)\ntest_x = feature_scale.transform(test_x)","b660b3f6":"# Scaling titanic_test data set as well\nscale_test_data = feature_scale.fit_transform(titanic_test)","f4ab3502":"scale_test_data.shape","5ea7665b":"def models(train_x, train_y):\n    \n    #Logistic Regression\n    log_reg = LogisticRegression(solver = 'lbfgs', random_state = 42)\n    log_reg.fit(train_x,train_y)\n    \n    #KNN\n    knn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\n    knn.fit(train_x, train_y)\n    \n    #SVC Linear\n    svc_lin = SVC(kernel = 'linear', random_state=42)\n    svc_lin.fit(train_x, train_y)\n    \n    #SVC RBF\n    svc_rbf = SVC(kernel = 'rbf', random_state=42)\n    svc_rbf.fit(train_x, train_y)\n    \n    #Decision Tree Classifier\n    dec_tree = DecisionTreeClassifier(criterion='entropy', random_state=42)\n    dec_tree.fit(train_x, train_y)\n    \n    #Random Forest Classifier\n    rf = RandomForestClassifier(n_estimators = 500, criterion='entropy', max_depth = 6,random_state=42)\n    rf.fit(train_x, train_y)\n    \n    #XGBoost\n    xg = xgb.XGBClassifier(n_estimators=1000, max_depth = 4, learning_rate=0.01, reg_lambda=0.22, gamma=0.2, random_state=42)\n    xg.fit(train_x, train_y)\n\n    #Printing accuracy for every model\n    print('[0] Logistic Regression training accuracy: ', log_reg.score(train_x, train_y))\n    print('[1] KNN training accuracy: ', knn.score(train_x, train_y))\n    print('[2] SVC_Linear training accuracy: ', svc_lin.score(train_x, train_y))\n    print('[3] SVC_RBF training accuracy: ', svc_rbf.score(train_x, train_y))\n    print('[4] Decision Tree training accuracy: ', dec_tree.score(train_x, train_y))\n    print('[5] Random Forest training accuracy: ', rf.score(train_x, train_y))\n    print('[6] XGBoost training accuracy: ', xg.score(train_x, train_y))\n        \n    return log_reg, knn, svc_lin, svc_rbf, dec_tree, rf, xg","666998cd":"# Get and Train all the models\nmodel = models(train_x, train_y)","5bf568a2":"# Creating confusion matrix and see the accuracy for all the models for test data\n\nfor i in range( len(model) ):\n    cm  = confusion_matrix(test_y, model[i].predict(test_x))\n    \n    # Extract the confusion matrix parameters\n    TN, FP, FN, TP = confusion_matrix(test_y, model[i].predict(test_x)).ravel()\n    \n    test_score = (TP+TN) \/ (TP+TN+FP+FN)\n    \n    print(cm)\n    print('Model[{}] Testing Accuracy =\"{}\"'.format(i, test_score))\n    print()\n\n","92b03e08":"# Get Important Features for Random Forest\nxg = model[5]\nimportance = pd.DataFrame({'Features': titanic_train.iloc[:,1:22].columns, 'Importance' : np.round(xg.feature_importances_,3)})\nimportance = importance.sort_values('Importance', ascending = False).set_index('Features')\nimportance","fa52fd70":"# Visualize the important features\nimportance.plot.bar(color='b')\nplt.show()","a97ed80f":"# Printing the prediction of Random Forest\npred = model[6].predict(test_x)\nprint(pred)\n\nprint()\n\n# Printing the actual values\nprint(test_y)","47f564ab":"pred_rand_for = xg.predict(scale_test_data)\nsubmission[\"Survived\"] = pred_rand_for","42e71412":"submission.head(6)","3ca44fae":"submission.to_csv(\"Submission_Khushboo_XGB.csv\", index=False)","e25d74b9":"### Feature Scaling \/ Normalization","af7c37e2":"* > ## DATA MANUPULATION","d811d735":"## Filling Cabin missing values of test set","d17f624e":"### Clearly, survival chances of males is very low when compared to females.\n#### We can also recall from the movie, females & children were given 1st preference while loading on a life-boat & this statistics proves it.","aade5434":"#### If we ignore the passenger id, we can see all other features are on the same scale except \"AGE\"\n\nyou can remove the ``passenger id`` before doing this","b272b6af":"#### we can replace a few titles: like there is ```Ms`` which can be replaced with ``Miss``, as they refer to the same gender group. And few like Major, Capt, etc replace them with others","f89eac68":"### If you like my kernel do ``Upvote`` as that keeps me motivated\n### Any scope of improvement comments would really help...","27756c69":"### 1. Though there are more males on the ship but females survival chances are more than males.\n### 2. Majority of the passengers belonged to third class but they did not survive & the maximum passengers survived from the 1st class when compared to the rest.\n### 3. Southampton has a lion's share when it comes to boarding on the ship, but they could not defeat the survival.","d2c20c28":"### Check for missing values","a0f38545":"## Machine Learning with different algorithms","9a01550f":"#### Out of 891 records in ``Training set`` there are missing values in (Age, Cabin and Embarked) and \n#### Out of 418 records in ``Test set`` there are missing values in (Age, Fare and Cabin)\n","8f8a2603":"### Filling Age missing values of training & test data[](http:\/\/) set with ``Median``","c12f7375":"* > ## DATA EXPLORATION","46208311":"### Creating new variable ``Family Size`` & ``Alone``","2fe24f5b":"### Get some statistical information","0a52c53b":"### Deleting columns which are of no use","37d65e01":"## Let's see if there are any outliers in the Age feature in Test set","27c24f23":"### As expected the model that has performed the best is ``Xtreme Gradient Boosting``","389bd8a6":"## Let's see if there are any outliers in the Age feature in Training set","9aa62c54":"### As there is just 1 missing value of Fare in Test data set, we can fill it using ``Mean`` or ``Median``","045bd3a4":"### Converting categorical variable ['Sex', 'Embarked', 'Title'] to numeric values for both the data sets.\n##### This can also be done after spliting the dataset into train & test, however it can also be done before it.","03b90b48":"## Titanic was a unprecedented event and was a very controversial topic that had this accident could have been prevented or not..??\n\nIt raised a lot of questions in statisticians mind that did the survival of the passengers depend on the any factors..?\n\nHere, we analyse that factors like ``Age``, ``Gender``, ``Boarding Stations``, ``Class`` had any correlation with the survival. \n\nLet's find out here if any of these factors presence was related to the life of the passengers","60a73308":"### Some observations that we can make:\n\n1. Here we see that the youngest person on the ship was 5 months old i.e; s\/he was a baby\n2. The oldest person on the ship was 80 years old.\n3. The highest fare of the ship was \u20ac512 (might be British Pounds)","e1ca7417":"## Some Inferences from the above table\n\n#### Interesting - females from 2nd class of (infants + teenagers) have a surival rate of ``100%``\n#### A female who is 30yrs and above & belongs to 1st class, then the chances of survival is `98%`\n#### Survival rate of adult females from 3rd class is ``30.4%`` which is still way higher than the adult males of 2nd class i.e; ``10.6%``\n#### Highest survival chances in males is ``80%`` who belong to 1st class (infants + teenagers)\n#### Survival chances of males belonging to 18-30 age group from 2nd class is almost negliable - only 2.7%\n\n\n\n\n","b84969f8":"## Filling Cabin missing values of training set","14e075a9":"### 1st class has the highest survival rate\n### Females from 1st class have the highest survival rate of ``96.8%``\n### Survival rate of males overall is very less, survival of males in 1st class is ``36.8%`` which is still lower than the survival chances of females in 3rd class which is ``50%``","ed62382f":"### We can see majority of passengers borded from ``Southampton`` but we saw earlier the survival rate of ``Cherbourg`` is high so, we would fill the missing values with ``Cherbourg``.","ce3493d0":"### Survival rate of 1st class is higher than ``60%`` irrespective of the gender.\n### Survival rate of passengers embarking from ``Cherbourg`` almost above ``50%`` irrespective of the gender.","38630e64":"#### All the point on the scatter plot represents each passenger. We did see earlier that some passengers were travelling at 0 pounds as well & passengers who paid more than 100 pounds belong to 1st class","72ded4ac":"### Creating a Function with multiple models","32de8560":"\n#### Out of the total passengers travelling in titanic only 38.4% could survive which is not even the half of the passengers\n\n### Visualize the count of survivers using columns - ``Sex``, ``Pclass``, ``Embarked``","e042f0eb":"### Extract ``Title`` from name column "}}