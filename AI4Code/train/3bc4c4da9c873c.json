{"cell_type":{"2586257f":"code","e38025e1":"code","8f9902ac":"code","df1ea998":"code","d6a5e572":"code","e9d4147d":"code","def5f6d3":"code","58a064f5":"code","b33f33a9":"code","d51177e5":"code","d9f44b93":"code","23a830c9":"code","94697f20":"code","99685e9a":"code","d2ed5da0":"code","d9885773":"code","ab2d40ba":"code","c0ee7afe":"code","f63a835e":"code","dbdb0674":"code","624c0924":"code","eddbf6c9":"code","86b7dc4e":"markdown","c0281be1":"markdown","3e08068f":"markdown"},"source":{"2586257f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport re\nimport sklearn.metrics  as sklm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler","e38025e1":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\ndataset =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)\ndataset = dataset.fillna(np.nan)\n\ndataset.head(15)","8f9902ac":"def find_title(name):\n    result = re.search(r'[A-Z][a-z]*[.]', name)\n    if (result == None):\n        return ''\n    return result.group(0)\n\ndataset['Title'] = dataset.Name.apply(find_title)\ndataset.Title.unique()","df1ea998":"def map_title_to(title, _from, _to):\n    if title == _from:\n        return _to\n    return title\n\ndataset.Title = dataset.Title.apply(lambda x: map_title_to(x, 'Sir.','Mr.'))\ndataset.Title = dataset.Title.apply(lambda x: map_title_to(x, 'Lady.','Mrs.'))\ndataset.Title = dataset.Title.apply(lambda x: map_title_to(x, 'Dona.','Mrs.'))\ndataset.Title = dataset.Title.apply(lambda x: map_title_to(x, 'Don.','Mr.'))\ndataset.Title = dataset.Title.apply(lambda x: map_title_to(x, 'Capt.','Major.'))\ndataset.Title = dataset.Title.apply(lambda x: map_title_to(x, 'Col.','Major.'))\ndataset.Title = dataset.Title.apply(lambda x: map_title_to(x, 'Countess.','Mrs.'))\ndataset.Title = dataset.Title.apply(lambda x: map_title_to(x, 'Jonkheer.','Mr.'))\ndataset.Title = dataset.Title.apply(lambda x: map_title_to(x, 'Mlle.','Ms.'))\ndataset.Title = dataset.Title.apply(lambda x: map_title_to(x, 'Mme.','Mrs.'))\n\ndataset.Title.value_counts().plot(kind='bar')\nplt.show()","d6a5e572":"def set_median_age_by_title(title):\n    title_age_mask = dataset.loc[(dataset['Title'] == title) & (dataset['Age'].isna() )]\n    df = dataset.iloc[title_age_mask.index]\n    df.Age = dataset.Age.median()\n    dataset.iloc[title_age_mask.index] = df\n\nfor title in dataset.Title.unique():\n    set_median_age_by_title(title)","e9d4147d":"def set_age_interval(x):\n    if x < 5:\n        return 5\n    if x < 10:\n        return 10\n    if x < 20:\n        return 20\n    if x < 30:\n        return 30\n    if x < 40:\n        return 40\n    if x < 50:\n        return 50\n    if x < 60:\n        return 60\n    return 70\n     \n\ndataset.Age = dataset.Age.apply(set_age_interval)\ndataset.Age.value_counts().plot(kind='bar')\nplt.show()","def5f6d3":"dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\ndataset.drop(['Name','Ticket'], axis=1, inplace=True)  \ndataset.head()","58a064f5":"fare_midean = dataset['Fare'].median()\nvalues = {'Cabin':'Z','Embarked':'S','Fare':fare_midean}\ndataset.fillna(value=values,inplace=True)\ndataset['Cabin1'] = dataset['Cabin'].str[0]    \ndataset.drop(['Cabin'], axis=1, inplace=True)    \n    \n#encode labels\ndataset['Sex'] = LabelEncoder().fit_transform(dataset['Sex'].values)\ndataset['Cabin1'] = LabelEncoder().fit_transform(dataset['Cabin1'].values)\ndataset['Embarked'] = LabelEncoder().fit_transform(dataset['Embarked'].values)\ndataset['Title'] = LabelEncoder().fit_transform(dataset['Title'].values)\n\ndataset.head()","b33f33a9":"working_columns = ['Pclass','Sex','Age','Fare','Embarked','FamilySize','Cabin1','Title','SibSp','Parch']\ncolumns_to_scale = ['Pclass','Sex','Age','Fare','Embarked','FamilySize','Cabin1','Title','SibSp','Parch']\n\ndef scale_dataset(dataset):\n    scalers = {}\n    for column in columns_to_scale:\n        scaler = StandardScaler()\n        dataset[column] = scaler.fit_transform(dataset[column].values.reshape(-1, 1))\n        scalers[column] = scaler\n    return dataset, scalers\n\n# Splitting into labels and data\ndataset, scalers = scale_dataset(dataset)","d51177e5":"train_set = dataset.iloc[:train.shape[0]]\ntest_set = dataset.iloc[train.shape[0]:]\n\ntrain_set.tail()\ntest_set.head()","d9f44b93":"from sklearn.manifold import TSNE\ntsne_results = TSNE(n_components=2).fit_transform(train_set[working_columns].values)\n\ndf_subset = pd.DataFrame()\ndf_subset['tsne-2d-one'] = tsne_results[:,0]\ndf_subset['tsne-2d-two'] = tsne_results[:,1]\ndf_subset['survived'] = targets = train_set['Survived'].values.astype(int)\n\nplt.figure(figsize=(16,10))\nsns.scatterplot(\n    x=\"tsne-2d-one\", \n    y=\"tsne-2d-two\",\n    hue=\"survived\",\n    data=df_subset,\n    legend=\"full\",\n    alpha=0.5\n)","23a830c9":"from sklearn.neighbors import LocalOutlierFactor\n\ncolumns = [c for c in train_set.columns if c not in [\"Survived\", \"PassengerId\", \"Pclass\", \"Sex\", \"Cabin1\", \"Embarked\", 'FamilySize']]\n\nclf = LocalOutlierFactor(n_neighbors=5, contamination=0.5)\ny_pred = clf.fit_predict(train_set[columns].values)\nX_scores = clf.negative_outlier_factor_\n\nclf_df = pd.DataFrame( columns=['X_scores', 'y_pred'])\nclf_df.X_scores = X_scores\nclf_df.y_pred = y_pred\noutlier_index = clf_df.index[clf_df['y_pred'] == -1].tolist()\n\ntrain_set.drop(outlier_index)","94697f20":"y_source = train_set['Survived'].values.astype(int)\nX_source = train_set[working_columns].values\n\nX_test = test_set[working_columns].values\ny_test = test_set['Survived'].values.astype(int)","99685e9a":"g = sns.heatmap(train_set[working_columns].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","d2ed5da0":"# Splitting the dataset into the Training set and Test set\nX_train, X_test, y_train, y_test = train_test_split(X_source, y_source, test_size = 0.2)\n\nprint('X_train: {}'.format(X_train.shape))\nprint('y_train: {}'.format(y_train.shape))\nprint('X_test: {}'.format(X_test.shape))\nprint('y_test: {}'.format(y_test.shape))","d9885773":"from keras.callbacks import EarlyStopping\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization\nfrom keras.regularizers import l2\n\ndef create_model2(optimizer, units = 16, init = 'glorot_uniform', activation = 'relu', l2_init = 0.001, dropout=0.1):    \n    model = Sequential()\n    model.add(Dense(units=units, input_dim=10, activation=activation, kernel_initializer=init, kernel_regularizer=l2(l2_init)))\n    model.add(Dropout(dropout))\n    #model.add(BatchNormalization())\n    model.add(Dense(units=units, activation=activation, kernel_initializer=init, kernel_regularizer=l2(l2_init)))\n    model.add(BatchNormalization())\n    model.add(Dropout(dropout))\n    model.add(Dense(units=units, activation=activation, kernel_initializer=init))\n    #model.add(BatchNormalization())\n    #model.add(Dropout(dropout))\n    model.add(Dense(1, activation='sigmoid',kernel_initializer=init))\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy']) \n    return model","ab2d40ba":"#from keras.wrappers.scikit_learn import KerasClassifier\n#from sklearn.model_selection import GridSearchCV\n\n#estimator = KerasClassifier(build_fn=create_model2, verbose=0)\n\n# grid search epochs, batch size and optimizer\n#optimizers = [ 'adam']\n#init = ['glorot_uniform']\n#activations = ['relu']\n#epochs = [400]\n#batches = [64]\n#l2_inits = [1e-1, 1e-2, 1e-3, 3e-4]\n#dropouts = [0.1]\n#hyperparameters = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init, activation=activations, l2_init=l2_inits, dropout=dropouts)\n#grid = GridSearchCV(estimator=estimator, param_grid=hyperparameters)\n#grid_result = grid.fit(X_train, y_train)\n\n# summarize results\n#print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n#means = grid_result.cv_results_['mean_test_score']\n#stds = grid_result.cv_results_['std_test_score']\n#params = grid_result.cv_results_['params']\n#for mean, stdev, param in zip(means, stds, params):\n#    print(\"%f (%f) with: %r\" % (mean, stdev, param))","c0ee7afe":"# grid search values\n#values = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n#all_train, all_test = list(), list()\n\n#adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n\n#for l2_param in values:\n#    # define model\n#    model = create_model2(optimizer=adam, l2_init=l2_param)\n#    # fit model\n#    model.fit(X_train, y_train, epochs=400, verbose=0)\n#    # evaluate the model\n#    _, train_acc = model.evaluate(X_train, y_train, verbose=0)\n#    _, test_acc = model.evaluate(X_test, y_test, verbose=0)\n#    print('Param: %f, Train: %.3f, Test: %.3f' % (l2_param, train_acc, test_acc))\n#    all_train.append(train_acc)\n#    all_test.append(test_acc)","f63a835e":"#'activation': 'relu', 'batch_size': 64, 'dropout': 0.1, 'epochs': 400, 'init': 'glorot_uniform', 'l2_init': 0.0003, 'optimizer': 'adam'}\n\n\n#from numpy.random import seed\n#seed(1)\n#from tensorflow import set_random_seed\n#set_random_seed(2)\n\ncallbacks = [EarlyStopping(monitor='val_loss', patience=100)]\n\n#rmsprop = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n#adamax = optimizers.Adamax(lr=0.0009, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\nadam = optimizers.Adam(lr=3e-4, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n\nbest_model = create_model2(units=16, optimizer=adam, init='glorot_uniform', activation='relu', l2_init=0.3)\nhistory = best_model.fit(X_train, y_train, \n                        validation_data=[X_test, y_test],\n                        batch_size=64,\n                        epochs=1000,\n                        callbacks=callbacks,\n                        verbose=1)","dbdb0674":"# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='center right')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='center right')\nplt.show()","624c0924":"# evaluate the model\nscores = best_model.evaluate(X_train, y_train, verbose=0)\nprint(\"%s: %.2f%%\" % (best_model.metrics_names[1], scores[1]*100))\n\nscores = best_model.evaluate(X_test, y_test, verbose=0)\nprint(\"%s: %.2f%%\" % (best_model.metrics_names[1], scores[1]*100))","eddbf6c9":"predictions = best_model.predict_classes(test_set[working_columns].values)\n\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_set[\"PassengerId\"],\n        \"Survived\": predictions.ravel() \n    })\nsubmission.to_csv('submission.csv', index=False)","86b7dc4e":"## Using best parameters","c0281be1":"## GridSearchCV","3e08068f":"## Grid Search Regularization"}}