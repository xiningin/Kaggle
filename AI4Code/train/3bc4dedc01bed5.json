{"cell_type":{"3c4b9ea6":"code","9c0e7aa3":"code","9f6d4d69":"code","1bd59fd8":"code","de5aacac":"code","1d670a03":"code","64c4b193":"code","56de6237":"code","1cf24dff":"code","dfe18eb0":"code","801db22c":"code","a0e35448":"code","942b34bc":"code","0daab872":"markdown","dd68bc25":"markdown","6fbc711d":"markdown","8c278ffe":"markdown","021dd987":"markdown","9b1cbeb9":"markdown","953d1112":"markdown","07638783":"markdown","db5b0563":"markdown","59e8f833":"markdown","fb3db13f":"markdown","9d1dfe0d":"markdown","65e6c664":"markdown","4e79b500":"markdown"},"source":{"3c4b9ea6":"pip install audiomentations","9c0e7aa3":"#basic libraries\nimport numpy as np\nimport os,gc\nimport pandas as pd \nimport matplotlib.pyplot as plt \n\n#audio\nimport librosa \nfrom audiomentations import Compose,AddGaussianSNR,Shift,TimeStretch,TimeMask,FrequencyMask,PolarityInversion\nfrom IPython.display import Audio\n\n#Image\nfrom PIL import Image \n","9f6d4d69":"train_labels=pd.read_csv('..\/input\/birdclef-2021\/train_soundscape_labels.csv')\nmetadata=pd.read_csv('..\/input\/birdclef-2021\/train_metadata.csv')\ntest=pd.read_csv('..\/input\/birdclef-2021\/test.csv')\n\ntrain_dir='..\/input\/birdclef-2021\/train_short_audio'\ntest_dir='..\/input\/birdclef-2021\/test_soundscapes'","1bd59fd8":"train=metadata.query('rating >= 4.0')\n\n#lets see the distribution of count numbers\nrec_count=pd.Series(train.primary_label.value_counts())\nplt.hist(rec_count.values,width=15,bins=30)\nplt.xlabel('Num Recordings')\nplt.ylabel('count')","de5aacac":"count_gt_100=rec_count[rec_count.values>150].index\n\n#taking the data that has atleast 100 recordings rated >4:\n\ntrain_ds=train[train['primary_label'].isin(count_gt_100)]\nprint(f'Number of species in data are : {train_ds.primary_label.nunique()}')\nprint(f'Number of audio samples : {len(train_ds)}')","1d670a03":"#Audio Augmentation:\naugmentations = Compose([\n            FrequencyMask(min_frequency_band=0.005, max_frequency_band=0.10, p=0.25),\n            TimeStretch(min_rate=0.15,max_rate=.25,p=0.25),\n            AddGaussianSNR(min_SNR=0.001, max_SNR=.25, p=0.25)])","64c4b193":"#Global Params\nSample_rate = 32000\nSignal_length = 5 # seconds\nshape= (64, 256) # height x width\nFMIN = 500\nFMAX = 12500\nhop_len=(Sample_rate* Signal_length \/\/ (shape[1]-1))\n\n#Number of species in data\nnum_classes=train_ds.primary_label.nunique()\n","56de6237":"#making new dir to save spectrograms\ncwd=os.path.abspath(os.getcwd())\n\nos.mkdir(os.path.join(cwd,'Mel_specs'))\n\nspec_dir=os.path.join(cwd,'Mel_specs')\nspec_dir","1cf24dff":"#function to save stfts\n\ndef save_stft(path,label=None,idx=None,Augment=False,dir_path=spec_dir):\n    '''extracting mel-specs from given audio data and saving them to given folder'''\n    \n    \n    #loading files:\n    if label:\n        file_path=os.path.join(path,label,idx)\n    else:\n        file_path=path\n        \n        \n    sig,sr=librosa.load(file_path,sr=Sample_rate)\n    \n    stft_id=[]\n    labels=[]\n    n=0\n    for i in range(0,len(sig),int(Signal_length*Sample_rate)):\n        \n        window = sig[i:i + int(Signal_length * Sample_rate)]\n\n        # End of signal\n        if len(window) < int(Signal_length * Sample_rate):\n            break\n            \n            \n            \n        #Augment :   \n        if Augment:\n            window=augmentations(window,sample_rate=sr)\n            \n        # extracting mel-spectrograms:\n        mel_spec = librosa.feature.melspectrogram(window,sr,n_mels=shape[0],\n                                                hop_length=hop_len,\n                                                fmin=FMIN,fmax=FMAX)\n        \n        mel_spec = librosa.core.amplitude_to_db(mel_spec,ref=np.max)\n        \n        # Normalize\n        mel_spec -= mel_spec.min()\n        mel_spec \/= mel_spec.max()\n        \n        \n        #saving Image\n        \n        #image_id\n        ids=idx.split('.')[0]\n        \n        save_id=f'{ids}_{n}.jpg'\n        save_path=os.path.join(dir_path,save_id)\n        \n        n+=1\n        \n        image = Image.fromarray(mel_spec * 255.0).convert(\"L\")\n        image.save(save_path)\n        \n        #saving_image ids and labels\n        stft_id.append(save_id)\n        labels.append(label)\n        \n    return stft_id,labels","dfe18eb0":"spec_ids=[]\nlabels=[]\nnum_species=train_ds.primary_label.nunique()\nfor i,ids in enumerate(train_ds.primary_label.unique()):\n    \n    spec_df=train_ds[train_ds.primary_label==ids]\n    \n    #taking lesser number of samples from each species:\n    \n#     sampling n from given species\n    n=int(len(spec_df)\/3)\n    \n    for sound_ids in spec_df['filename'].sample(n):\n        spec_id,label=save_stft(path=train_dir,label=ids,idx=sound_ids,Augment=True)\n        \n        #appending to final list:\n        spec_ids.extend(spec_id)\n        labels.extend(label)\n        \n\nassert len(spec_ids) == len(labels)\nlen(labels)","801db22c":"#making a dataframe of recording_ids and labels:\ntrain_specs=pd.DataFrame({'Image_Id':spec_ids,'label':labels})\n\ndel spec_ids, labels ; gc.collect()\n\n#saving csv for further use:\ntrain_specs.to_csv('train_specs.csv',index=False)\ntrain_specs.head()","a0e35448":"sampled_df=train_specs.sample(25)","942b34bc":"def show_image(path,title):\n    plt.subplots(figsize=(8,4))\n    image=Image.open(path)\n    \n    plt.title(name)\n    plt.axis('off')\n    plt.grid(b=None)\n    plt.imshow(image)\n    \nfor i,row in sampled_df.iterrows():\n    idx=row.Image_Id\n    name=row.label\n    show_image(os.path.join(spec_dir,idx),name)","0daab872":"# Imports ","dd68bc25":"**saving ids and labels saved for further use**","6fbc711d":"# Objective \n**To Classify birds based on sound recordings provided in the training set, and to predict the birds species present in the test set. The Image meta-data is given in the train_metadata.csv .**","8c278ffe":"**My notebook on BirdClef EDA:** [ https:\/\/www.kaggle.com\/virajkadam\/birdclef-exploratory-data-analysis ]\n**Notebook on Birdclef training** [ https:\/\/www.kaggle.com\/virajkadam\/birdclef-bird-sound-classification\/edit ]","021dd987":"**Augmenting Data using Audiomentations Library**","9b1cbeb9":"# Resources:\n\n* Research Paper  [ http:\/\/ceur-ws.org\/Vol-1866\/paper_143.pdf ]\n* Guide Notebook [ https:\/\/www.kaggle.com\/stefankahl\/birdclef2021-model-training ]\n* My notebook on Rainforest Connect Audio : [ https:\/\/www.kaggle.com\/virajkadam\/rainforest-connect-custom-cnn ]\n* [ https:\/\/www.kaggle.com\/kmldas\/birdclef-2021-eda-model ]\n\n","953d1112":"**I will use recordings with more than 150 instances.**","07638783":"# Extracting Mel Spectrograms:","db5b0563":"**Loading Data as mel-spectrograms**","59e8f833":"**extracting specs for selected species**","fb3db13f":"# Loading Data ","9d1dfe0d":"**Will do the training and inference part in a seperate notebook, linked above.**","65e6c664":"**checking some sample Images**","4e79b500":"# Filtering audio data \n**Limiting the training data to 'rating'>=4. So to not take data with a lot of noise.**"}}