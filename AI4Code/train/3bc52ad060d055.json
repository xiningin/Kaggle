{"cell_type":{"6bd03769":"code","7a3c1bc8":"code","feee5a71":"code","04575c1b":"code","abcf7e8d":"code","c285518e":"code","e453954e":"code","23862fd4":"code","b110d928":"code","90bfec5a":"code","136e1b2d":"code","c85eb0d6":"code","3e83e3d3":"code","9bd0e89e":"code","2d15d5e7":"code","f692e4e7":"code","7a87fbe4":"code","4546e790":"code","b88d7154":"code","c4025d3e":"code","46abd0df":"code","93c43ed7":"code","86268371":"code","645299b3":"code","2ef0b577":"code","da7cb06a":"code","a22b67e3":"code","cfe8611b":"code","71850f9a":"code","8cd41792":"code","17805a8a":"code","fe963a6d":"code","46f44e20":"code","86c7753f":"code","44feac63":"code","26c4a085":"code","b0343425":"code","e209f8ea":"code","a4fdc5cb":"code","68bb9fde":"code","2b53d346":"code","14ad9928":"code","2f4615c8":"code","d22dc6d0":"markdown","33de96a9":"markdown","cc97a3c4":"markdown","02c197db":"markdown","ae46545d":"markdown","304cb1fc":"markdown","60396b0b":"markdown","24b9e495":"markdown"},"source":{"6bd03769":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport re","7a3c1bc8":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\n","feee5a71":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","04575c1b":"nltk.download('stopwords')","abcf7e8d":"from tqdm.notebook import tqdm","c285518e":"prefix = '\/kaggle\/input\/nlp-getting-started\/'","e453954e":"\ntest = pd.read_csv(prefix + 'test.csv')","23862fd4":"test.head()\n\n","b110d928":"test.shape","90bfec5a":"train = pd.read_csv(prefix + 'train.csv')","136e1b2d":"train.head()\n","c85eb0d6":"train.shape","3e83e3d3":"df = pd.concat([train,test])","9bd0e89e":"df.shape","2d15d5e7":"!pip install emoji\nimport emoji","f692e4e7":"def convert_emojis(text):\n    try:\n        text = emoji.demojize(text)\n    except:\n        pass\n    return text","7a87fbe4":"text = \"Hello \ud83d\ude02\"\nconvert_emojis(text)","4546e790":"df['text'] = df['text'].apply(convert_emojis)","b88d7154":"df","c4025d3e":"def cleanup_url(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub('', text)\n\ndf['text'] = df['text'].apply(cleanup_url)","46abd0df":"def cleanup_punctuations(text):\n    import string\n    table = str.maketrans('', '', string.punctuation)    \n    return text.translate(table)\n","93c43ed7":"df['text'] = df['text'].apply(cleanup_punctuations)","86268371":"w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n\nwnl = WordNetLemmatizer()\n\ndef lemmatize(sentence):\n    sentence_words = w_tokenizer.tokenize(sentence)\n    new_sentence_words = list()\n    \n    for sentence_word in sentence_words:\n        \n        new_sentence_word = wnl.lemmatize(sentence_word.lower(), wordnet.VERB)\n        new_sentence_words.append(new_sentence_word)\n        \n    new_sentence = ' '.join(new_sentence_words)\n    new_sentence = new_sentence.strip()\n    \n    return new_sentence\n","645299b3":"small_df = df[0:10]","2ef0b577":"small_df","da7cb06a":"df['text'] = df['text'].apply(lemmatize)","a22b67e3":"df","cfe8611b":"# just for checkpoint \ndf.to_csv('cleaned.2.csv')","71850f9a":"import torch\nfrom torch.nn import functional as F\nfrom torch.autograd import Variable\n\nfrom torchtext import data\nfrom torchtext import datasets\nfrom torchtext.vocab import Vectors, GloVe","8cd41792":"def prepare_csv(df_train, df_test, seed=27, val_ratio=0.3):\n    idx = np.arange(df_train.shape[0])\n    \n    np.random.seed(seed)\n    np.random.shuffle(idx)\n    \n    val_size = int(len(idx) * val_ratio)\n    \n    if not os.path.exists('cache'):\n        os.makedirs('cache')\n    \n    df_train.iloc[idx[val_size:], :][['id', 'target', 'text']].to_csv(\n        'cache\/dataset_train.csv', index=False\n    )\n    \n    df_train.iloc[idx[:val_size], :][['id', 'target', 'text']].to_csv(\n        'cache\/dataset_val.csv', index=False\n    )\n    \n    df_test[['id', 'text']].to_csv('cache\/dataset_test.csv',\n                   index=False)","17805a8a":"def get_iterator(dataset, batch_size, train=True,\n                 shuffle=True, repeat=False):\n    \n    device = torch.device('cuda:0' if torch.cuda.is_available()\n                          else 'cpu')\n    \n    dataset_iter = data.Iterator(\n        dataset, batch_size=batch_size, device=device,\n        train=train, shuffle=shuffle, repeat=repeat,\n        sort=False\n    )\n    \n    return dataset_iter","fe963a6d":"import logging\nimport os\nfrom copy import deepcopy\n\nLOGGER = logging.getLogger('tweets_dataset')\n\ndef get_dataset(fix_length=100, lower=False, vectors=None):\n    \n    if vectors is not None:\n        lower=True\n        \n    LOGGER.debug('Preparing CSV files...')\n    prepare_csv(train, test)\n    \n    TEXT = data.Field(sequential=True, \n                      lower=True, \n                      include_lengths=True, \n                      batch_first=True, \n                      fix_length=25)\n    LABEL = data.Field(use_vocab=True,\n                       sequential=False,\n                       dtype=torch.float16)\n    ID = data.Field(use_vocab=False,\n                    sequential=False,\n                    dtype=torch.float16)\n    \n    \n    LOGGER.debug('Reading train csv files...')\n    \n    train_temp, val_temp = data.TabularDataset.splits(\n        path='cache\/', format='csv', skip_header=True,\n        train='dataset_train.csv', validation='dataset_val.csv',\n        fields=[\n            ('id', ID),\n            ('target', LABEL),\n            ('text', TEXT)\n        ]\n    )\n    \n    LOGGER.debug('Reading test csv file...')\n    \n    test_temp = data.TabularDataset(\n        path='cache\/dataset_test.csv', format='csv',\n        skip_header=True,\n        fields=[\n            ('id', ID),\n            ('text', TEXT)\n        ]\n    )\n    \n    LOGGER.debug('Building vocabulary...')\n    \n    TEXT.build_vocab(\n        train_temp, val_temp, test_temp,\n        max_size=20000,\n        min_freq=10,\n        vectors=GloVe(name='twitter.27B', dim=200)  # We use it for getting vocabulary of words\n    )\n    LABEL.build_vocab(\n        train_temp\n    )\n    ID.build_vocab(\n        train_temp, val_temp, test_temp\n    )\n    \n    word_embeddings = TEXT.vocab.vectors\n    vocab_size = len(TEXT.vocab)\n    \n    train_iter = get_iterator(train_temp, batch_size=32, \n                              train=True, shuffle=True,\n                              repeat=False)\n    val_iter = get_iterator(val_temp, batch_size=32, \n                            train=True, shuffle=True,\n                            repeat=False)\n    test_iter = get_iterator(test_temp, batch_size=32, \n                             train=False, shuffle=False,\n                             repeat=False)\n    \n    \n    LOGGER.debug('Done preparing the datasets')\n    \n    return TEXT, vocab_size, word_embeddings, train_iter, val_iter, test_iter","46f44e20":"TEXT, vocab_size, word_embeddings, train_iter, val_iter, test_iter = get_dataset()","86c7753f":"class LSTMClassifier(torch.nn.Module):\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, weights):\n        super(LSTMClassifier, self).__init__()\n        \n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.word_embeddings = torch.nn.Embedding(vocab_size,\n                                                  embedding_dim)\n        self.word_embeddings.weight = torch.nn.Parameter(weights,\n                                                         requires_grad=False)\n        \n        self.dropout_1 = torch.nn.Dropout(0.3)\n        self.lstm = torch.nn.LSTM(embedding_dim,\n                                  hidden_dim,\n                                  n_layers,\n                                  dropout=0.3,\n                                  batch_first=True)\n        \n        self.dropout_2 = torch.nn.Dropout(0.3)\n        self.label_layer = torch.nn.Linear(hidden_dim, output_size)\n        \n        self.act = torch.nn.Sigmoid()\n        \n    def forward(self, x, hidden):\n        batch_size = x.size(0)\n        \n        x = self.word_embeddings(x)\n        \n        x = self.dropout_1(x)\n        \n        lstm_out, hidden = self.lstm(x, hidden)\n                \n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        out = self.dropout_2(lstm_out)\n        out = self.label_layer(out)    \n        \n        out = out.view(batch_size, -1, self.output_size)\n        out = out[:, -1, :]\n\n        out = self.act(out)\n        \n        return out, hidden\n    \n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        \n        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n        \n        return hidden","44feac63":"def train_model(model, train_iter, val_iter, optim, loss, num_epochs, batch_size=32):\n    h = model.init_hidden(batch_size)\n    \n    clip = 5\n    val_loss_min = np.Inf\n    \n    total_train_epoch_loss = list()\n    total_train_epoch_acc = list()\n        \n    total_val_epoch_loss = list()\n    total_val_epoch_acc = list()\n        \n    \n    device = torch.device('cuda:0' if torch.cuda.is_available()\n                           else 'cpu')\n    \n    for epoch in range(num_epochs):\n\n        model.train()\n        \n        train_epoch_loss = list()\n        train_epoch_acc = list()\n        \n        val_epoch_loss = list()\n        val_epoch_acc = list()\n        \n        for idx, batch in enumerate(tqdm(train_iter)):\n            h = tuple([e.data for e in h])\n\n            text = batch.text[0]\n            target = batch.target\n            target = target - 1\n            target = target.type(torch.LongTensor)\n\n            text = text.to(device)\n            target = target.to(device)\n\n            optim.zero_grad()\n            \n            if text.size()[0] is not batch_size:\n                continue\n            \n            prediction, h = model(text, h)\n                \n            loss_train = loss(prediction.squeeze(), target)\n            loss_train.backward()\n\n            num_corrects = (torch.max(prediction, 1)[1].\n                                view(target.size()).data == target.data).float().sum()\n\n            acc = 100.0 * num_corrects \/ len(batch)\n\n            train_epoch_loss.append(loss_train.item())\n            train_epoch_acc.append(acc.item())\n            \n            torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n            \n            optim.step()\n    \n        print(f'Train Epoch: {epoch}, Training Loss: {np.mean(train_epoch_loss):.4f}, Training Accuracy: {np.mean(train_epoch_acc): .2f}%')\n\n        model.eval()\n\n        with torch.no_grad():\n            for idx, batch in enumerate(tqdm(val_iter)):\n                val_h = tuple([e.data for e in h])\n\n                text = batch.text[0]\n                target = batch.target\n                target = target - 1\n                target = target.type(torch.LongTensor)\n                \n                text = text.to(device)\n                target = target.to(device)\n                \n                if text.size()[0] is not batch_size:\n                    continue\n\n                prediction, h = model(text, h)\n                loss_val = loss(prediction.squeeze(), target)\n\n                num_corrects = (torch.max(prediction, 1)[1].\n                                view(target.size()).data == target.data).float().sum()\n\n                acc = 100.0 * num_corrects \/ len(batch)\n\n                val_epoch_loss.append(loss_val.item())\n                val_epoch_acc.append(acc.item())\n                \n            print(f'Validation Epoch: {epoch}, Training Loss: {np.mean(val_epoch_loss):.4f}, Training Accuracy: {np.mean(val_epoch_acc): .2f}%')\n                \n            if np.mean(val_epoch_loss) <= val_loss_min:\n                print('Validation loss decreased ({:.6f} --> {:.6f})'.\n                      format(val_loss_min, np.mean(val_epoch_loss)))\n                \n                val_loss_min = np.mean(val_epoch_loss)\n                \n        total_train_epoch_loss.append(np.mean(train_epoch_loss))\n        total_train_epoch_acc.append(np.mean(train_epoch_acc))\n    \n        total_val_epoch_loss.append(np.mean(val_epoch_loss))\n        total_val_epoch_acc.append(np.mean(val_epoch_acc))\n    \n    return (total_train_epoch_loss, total_train_epoch_acc,\n            total_val_epoch_loss, total_val_epoch_acc)","26c4a085":"lr = 1e-4\nbatch_size = 32\noutput_size = 2\nhidden_size = 128\nembedding_length = 200\n\nmodel = LSTMClassifier(vocab_size=vocab_size, \n                       output_size=output_size, \n                       embedding_dim=embedding_length,\n                       hidden_dim=hidden_size,\n                       n_layers=2,\n                       weights=word_embeddings\n)\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available()\n                      else 'cpu')\n    \nmodel.to(device)\noptim = torch.optim.Adam(model.parameters(), lr=lr)\nloss = torch.nn.CrossEntropyLoss()\n    \ntrain_loss, train_acc, val_loss, val_acc = train_model(model=model,\n                                                       train_iter=train_iter,\n                                                       val_iter=val_iter,\n                                                       optim=optim,\n                                                       loss=loss,\n                                                       num_epochs=20,\n                                                       batch_size=batch_size)\n    ","b0343425":"plt.figure(figsize=(10, 6))\nplt.title('Loss vs Epoch')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\ng = sns.lineplot(range(len(train_loss)), train_loss, label='train')\ng = sns.lineplot(range(len(val_loss)), val_loss, label='test')\ng.set_xticks(range(len(val_loss)))\ng.set_xticklabels(range(len(val_loss)))\n","e209f8ea":"plt.figure(figsize=(10, 6))\nplt.title('Accuracy')\nplt.xlabel('Accuracy')\nplt.ylabel('Loss')\ng = sns.lineplot(range(len(train_acc)), train_acc, label='train')\ng = sns.lineplot(range(len(val_acc)), val_acc, label='test')\ng.set_xticks(range(len(val_acc)))\ng.set_xticklabels(range(len(val_acc)))","a4fdc5cb":"results_target = list()\n\nwith torch.no_grad():\n    for batch in tqdm(test_iter):\n        for text, idx in zip(batch.text[0], batch.id):\n            text = text.unsqueeze(0)\n            res, _ = model(text, hidden=None)\n\n            target = np.round(res.cpu().numpy())\n            \n            results_target.append(target[0][1])","68bb9fde":"sample_submission = pd.read_csv(prefix+'sample_submission.csv')","2b53d346":"sample_submission['target'] = list(map(int, results_target))","14ad9928":"sample_submission.to_csv('submission.csv', index=False)","2f4615c8":"#download the result CSV here\n\nfrom IPython.display import FileLink\nFileLink('\/kaggle\/working\/submission.csv')","d22dc6d0":"## Pytorch \n","33de96a9":"Emoticons may provide signals for real disaster, so we do not remove it, instead we translate to text.","cc97a3c4":"## Plotting Loss\nThis section plots loss that is calculated using Cross Entropy vs Epochs (X)","02c197db":"## Glove Embedding","ae46545d":"lemmatize sentence","304cb1fc":"## Disaster NLP\nThis is the notebook for Twitter disaster classification. \\\nThe main technique used here is LSTM that is trained with appropriate word embedding. \\\nThe main framework used here is Pytorch.\n\n\n","60396b0b":"## Plotting Accuracy\n\nThis section plots accuracy vs epochs. \\\nAccuracy is calculated by number of correct prediction divided by total prediction in the validation phase. ","24b9e495":"cleanup punctuations"}}