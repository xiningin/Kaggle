{"cell_type":{"0d878e61":"code","5fb698d0":"code","9ad3555d":"code","c3ab748e":"code","f1e99a8d":"code","666d2d60":"code","17dd8360":"code","1dfeb656":"code","a3119a5a":"code","8f541794":"code","58f7e429":"code","bc160899":"code","82a8c114":"code","d95fbbdf":"code","d63db205":"code","954fdbce":"code","448ecdc5":"code","155ddfc7":"code","8b85a48f":"code","3c12efdc":"code","8f3f2a4d":"code","a539b910":"code","d8318b7e":"code","8f95ea4b":"code","df54261d":"code","afe8f09d":"code","cf0444eb":"code","e5612705":"code","98c05009":"code","28fdac2b":"code","38c7104f":"code","b6ae7c67":"code","665427f7":"code","ca9b679a":"code","aed76093":"code","7ccf712a":"code","859e30de":"code","70008497":"code","2beafa60":"code","e923f5ab":"code","e414be8b":"code","5259bbc5":"code","6649a699":"code","4c21edb1":"code","63bcf9b9":"code","f4a90e2b":"code","30322f67":"code","bc301cfd":"code","2c214133":"code","ca826cfb":"markdown","97f5e116":"markdown","d4a08218":"markdown","669d666b":"markdown"},"source":{"0d878e61":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5fb698d0":"import pandas as pd\nimport pandas_profiling \n\nimport seaborn as sns\nimport matplotlib.style as style\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom scipy.stats import boxcox_normmax\nfrom scipy.special import boxcox1p\nfrom scipy.stats import norm, skew\nimport scipy.stats as stats\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import LocalOutlierFactor\n\nimport warnings\nwarnings.filterwarnings('ignore')","9ad3555d":"house_train= pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\nhouse_test= pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\nprint(\"Dataset shape:\",'house_train', house_train.shape, 'house_test', house_test.shape)","c3ab748e":"#house_train.profile_report()","f1e99a8d":"#SalesPrice correlation with all the feature\nplt.figure(figsize=(8, 12))\nhouse_train.corr()['SalePrice'].sort_values().plot(kind='barh')","666d2d60":"#Heatmap for top 10 Sales Price-features correlation\nk = 10\ncols = house_train.corr().nlargest(k, 'SalePrice')['SalePrice'].index\nk_corr_matrix = house_train[cols].corr()\nplt.figure(figsize=(12, 8))\nsns.heatmap(k_corr_matrix, annot=True, cmap=plt.cm.RdBu_r)","17dd8360":"#scatterplot to verify linear relationship\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(house_train[cols], size = 2)\nplt.show()","1dfeb656":"#Is Target variable Normal?\ntarget = house_train['SalePrice']\nf, axes = plt.subplots(1, 3, figsize=(15, 4))\nsns.distplot(target, kde=False, fit=stats.johnsonsu, ax=axes[0])\nsns.distplot(target, kde=False, fit=stats.norm, ax=axes[1])\nsns.distplot(target, kde=False, fit=stats.lognorm, ax=axes[ 2])","a3119a5a":"# applying log transformation\nhouse_train['SalePrice'] = np.log1p(house_train['SalePrice'])","8f541794":"# distribution histogram and normal probability plot\n(mu, sigma) = norm.fit(house_train['SalePrice'])\nsns.distplot(house_train['SalePrice'], fit=norm)\nplt.legend(['Normal dist ($\\mu=${:.2f}, $\\sigma=${:.2f})'.format(mu, sigma)])\n\nfig = plt.figure()\nstats.probplot(house_train['SalePrice'], plot=plt)\nplt.show()","58f7e429":"def detect_outliers(x, y, top=5, plot=True):\n    lof = LocalOutlierFactor(n_neighbors=40, contamination=0.1)\n    x_ =np.array(x).reshape(-1,1)\n    preds = lof.fit_predict(x_)\n    lof_scr = lof.negative_outlier_factor_\n    out_idx = pd.Series(lof_scr).sort_values()[:top].index\n    if plot:\n        f, ax = plt.subplots(figsize=(9, 6))\n        plt.scatter(x=x, y=y, c=np.exp(lof_scr), cmap='RdBu')\n    return out_idx","bc160899":"#GrLivArea-SalePrice outlier detection\nouts = detect_outliers(house_train['GrLivArea'], house_train['SalePrice'],top=5) \nouts","82a8c114":"#Separating qualitative(categorical) and quantitative(continuous) featues\nquantitative = [feature for feature in house_train.columns if house_train.dtypes[feature] != 'object']\nquantitative.remove('SalePrice')\nquantitative.remove('Id')\nqualitative = [feature for feature in house_train.columns if house_train.dtypes[feature] == 'object']","d95fbbdf":"#Outliers for all quantitative features\nfrom collections import Counter\nall_outliers=[]\n\nfor feature in quantitative:\n    try:\n        outs = detect_outliers(house_train[feature], house_train['SalePrice'],top=5, plot=False)\n    except:\n        continue\n    all_outliers.extend(outs)\n\nprint(Counter(all_outliers).most_common())\n\noutliers = [30, 88, 462, 523, 632, 1298, 1324] #\nfor i in outliers:\n    if i in all_outliers:\n        print(i)","d63db205":"#delete outliers from training dataset\nhouse_train = house_train.drop(house_train.index[outliers])\nhouse_train.shape","954fdbce":"house_train.reset_index(drop=True, inplace=True)\ny_train = house_train['SalePrice']\nX_train = house_train.drop(['SalePrice'], axis=1)\nX_test = house_test\n\nprint(\"Dataset shape:\",'X_train', X_train.shape, 'y_train', y_train.shape, 'X_test', X_test.shape)","448ecdc5":"# Filling NA's of the quantitative features  \nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics_train = []; numerics_test = []\nfor i in house_train.columns: \n    if house_train[i].dtype in numeric_dtypes:\n        numerics_train.append(i)\nhouse_train.update(house_train[numerics_train].fillna(0)) #Filling NA's of training dataset\n\nfor i in house_test.columns:\n    if house_test[i].dtype in numeric_dtypes:\n        numerics_test.append(i)\nhouse_test.update(house_test[numerics_test].fillna(0)) #Filling NA's of test dataset\n#house_train.shape","155ddfc7":"#Skewness check and correction using boxcop for quantitative\/continuous features\nskew_train = house_train[quantitative].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew_train = skew_train[skew_train > 0.5] #skewness value\nfor i in high_skew_train.index:\n    house_train[i] = boxcox1p(house_train[i], boxcox_normmax(house_train[i] + 1))\n\nskew_test = house_test[quantitative].apply(lambda x: skew(x)).sort_values(ascending=False)    \nhigh_skew_test = skew_test[skew_train > 0.5]\nfor i in high_skew_test.index:\n    house_test[i] = boxcox1p(house_test[i], boxcox_normmax(house_test[i] + 1))","8b85a48f":"#Standard scaling to verify boxcox transformation\nsale_price_scaled = StandardScaler().fit_transform(house_train['SalePrice'][:, np.newaxis])\n\nsns.distplot(sale_price_scaled, fit=norm)\n\nlow_range = sale_price_scaled[sale_price_scaled[:, 0].argsort()[:5]]\nhigh_range = sale_price_scaled[sale_price_scaled[:, 0].argsort()[-5:]]\nprint(f'outer range (low) of the distribution: \\n{low_range}')\nprint(f'outer range (high) of the distribution: \\n{high_range}')","3c12efdc":"# Combining train and test datasets\nall_data = pd.concat([X_train, house_test], axis=0, sort=False)\nall_data.drop(['Id'], axis=1, inplace=True)\nall_data.shape","8f3f2a4d":"# Some of the non-numeric predictors are stored as numbers; we convert them into strings \nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","a539b910":"#Calculating missing data\nna_total = all_data.isnull().sum().sort_values(ascending=False)\nna_ratio = (all_data.isnull().sum() \/ all_data.shape[0]).sort_values(ascending=False)\nmissing_data = pd.concat([na_total, na_ratio], axis=1, keys=['Total', 'Ratio'])\nmissing_data.head(10)","d8318b7e":"# Most value of these 4 features are missing and they have no pattern , just delete them\nall_data.drop(['PoolQC', 'Utilities', 'Street', 'MiscFeature', ], axis=1, inplace=True)","8f95ea4b":"#Filling NA with None for categorical features\nfor col in ('Alley','Fence','FireplaceQu','GarageQual','GarageFinish','GarageCond','GarageType','BsmtExposure',\n          'BsmtCond','BsmtQual','BsmtFinType2','BsmtFinType1'):\n     all_data[col] = all_data[col].fillna(0)","df54261d":"print(all_data[all_data['GarageCars'].isnull()][['GarageArea', 'GarageCars', 'GarageType', 'GarageYrBlt', 'GarageQual']])\nall_data['GarageArea'].fillna(0, inplace=True)\nall_data['GarageCars'].fillna(0, inplace=True)","afe8f09d":"print(all_data[all_data['TotalBsmtSF'].isnull()][\n    ['TotalBsmtSF', 'BsmtFinSF2', 'BsmtUnfSF', 'BsmtFinSF1', 'BsmtFullBath','BsmtHalfBath']])\nfor col in ('TotalBsmtSF', 'BsmtFinSF2', 'BsmtUnfSF', 'BsmtFinSF1', 'BsmtFullBath','BsmtHalfBath'):\n     all_data[col] = all_data[col].fillna(0)\n# all_data['TotalBsmtSF'].fillna(0, inplace=True)","cf0444eb":"all_data['MasVnrType'].fillna('None', inplace=True)\nall_data['HasMasVnr'] = all_data['MasVnrType'].apply(lambda x: 0 if x == 'None' else 1)","e5612705":"X_train = all_data.iloc[:len(y_train), :]\nX_test = all_data.iloc[len(y_train):, :]\nprint(\"Dataset shape:\",'X_train', X_train.shape, 'y_train', y_train.shape, 'X_test', X_test.shape)","98c05009":"# fill the NA with the mode, which means most categorical type of the feature-train &test\nX_train['MSZoning'] = X_train.groupby(['MSSubClass'])['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\nX_test['MSZoning'] = X_test.groupby(['MSSubClass'])['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\nfor col in ('Functional','Exterior1st','Electrical','KitchenQual','SaleType','Exterior2nd'):\n    X_train[col] = X_train[col].fillna(X_train[col].mode()[0])\n    X_test[col] = X_test[col].fillna(X_test[col].mode()[0])","28fdac2b":"X_train['LotFrontage'] = X_train.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\nX_train['GarageYrBlt'] = (X_train['YearBuilt'] + X_train['YearRemodAdd']) \/2\nX_train['MasVnrArea'] = X_train.groupby(['MasVnrType'])['MasVnrArea'].transform(lambda x: x.fillna(x.median()))\n\nX_test['LotFrontage'] = X_test.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\nX_test['GarageYrBlt'] = (X_test['YearBuilt'] + X_test['YearRemodAdd']) \/2\nX_test['MasVnrArea'] = X_test.groupby(['MasVnrType'])['MasVnrArea'].transform(lambda x: x.fillna(x.median()))","38c7104f":"X_train['YrBltAndRemod']=X_train['YearBuilt']+X_train['YearRemodAdd']\nX_train['TotalSF']=X_train['TotalBsmtSF'] + X_train['1stFlrSF'] + X_train['2ndFlrSF']\nX_train['TotalSqrFootage'] = (X_train['BsmtFinSF1'] + X_train['BsmtFinSF2'] +\n                                 X_train['1stFlrSF'] + X_train['2ndFlrSF'])\nX_train['TotalBathrooms'] = (X_train['FullBath'] + (0.5 * X_train['HalfBath']) +\n                               X_train['BsmtFullBath'] + (0.5 * X_train['BsmtHalfBath']))\nX_train['TotalPorchSF'] = (X_train['OpenPorchSF'] + X_train['3SsnPorch'] +\n                              X_train['EnclosedPorch'] + X_train['ScreenPorch'] +\n                              X_train['WoodDeckSF'])\n\nX_test['YrBltAndRemod']=X_test['YearBuilt']+X_test['YearRemodAdd']\nX_test['TotalSF']=X_test['TotalBsmtSF'] + X_test['1stFlrSF'] + X_test['2ndFlrSF']\nX_test['TotalSqrFootage'] = (X_test['BsmtFinSF1'] + X_test['BsmtFinSF2'] +\n                                 X_test['1stFlrSF'] + X_test['2ndFlrSF'])\nX_test['TotalBathrooms'] = (X_test['FullBath'] + (0.5 * X_test['HalfBath']) +\n                               X_test['BsmtFullBath'] + (0.5 * X_test['BsmtHalfBath']))\nX_test['TotalPorchSF'] = (X_test['OpenPorchSF'] + X_test['3SsnPorch'] +\n                              X_test['EnclosedPorch'] + X_test['ScreenPorch'] +\n                              X_test['WoodDeckSF'])","b6ae7c67":"X_train['has2ndfloor'] = X_train['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nX_train['hasgarage'] = X_train['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nX_train['hasbsmt'] = X_train['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nX_train['hasfireplace'] = X_train['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\nX_test['has2ndfloor'] = X_test['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nX_test['hasgarage'] = X_test['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nX_test['hasbsmt'] = X_test['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nX_test['hasfireplace'] = X_test['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","665427f7":"print(\"Dataset shape:\",'X_train', X_train.shape, 'y_train', y_train.shape, 'X_test', X_test.shape)","ca9b679a":"import category_encoders as ce\nohe = ce.OneHotEncoder(handle_unknown='ignore', use_cat_names=True)\nencoded_data=ohe.fit_transform(pd.concat([X_train,X_test], axis=0, sort=False)).reset_index(drop=True)\nX_train =  encoded_data.iloc[:len(y_train), :]\nX_test = encoded_data.iloc[len(y_train):, :]\nprint(\"Dataset shape:\",'X_train', X_train.shape, 'y_train', y_train.shape, 'X_test', X_test.shape)","aed76093":"# X_train2= pd.get_dummies(X_train).reset_index(drop=True)","7ccf712a":"#pd.concat([pd.DataFrame(X_train.columns),pd.DataFrame(X_test.columns)]).drop_duplicates(keep=False)","859e30de":"X_test.isnull().sum().sort_values(ascending=False)","70008497":"#Removes colums where the threshold of zero's is (> 99.95), means has only zero values \noverfit = []\nlen_X_train =len(X_train)\n\nfor i in X_train.columns:\n    counts = X_train[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len_X_train * 100 > 99.94 :\n        overfit.append(i)\n\noverfit = list(overfit)\noverfit.append('MSZoning_C (all)')\n\n#Converting numpy array to handle XGB feature mismatch error -https:\/\/github.com\/dmlc\/xgboost\/issues\/2334\nX_train = np.array(X_train.drop(overfit, axis=1).copy())\ny_train = np.array(y_train)\nX_test = np.array(X_test.drop(overfit, axis=1).copy())\n\nprint(\"Dataset shape:\",'X_train', X_train.shape, 'y_train', y_train.shape, 'X_test', X_test.shape)","2beafa60":"from sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.linear_model import ElasticNet, Lasso, Ridge, ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.svm import SVR\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom mlxtend.regressor import StackingCVRegressor","e923f5ab":"#cross_val_score to get the root mean square error, which is the score method for current regression problem\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mse(y, y_pred))\n\ndef cv_rmse(model, X_train=X_train):\n    rmse = np.sqrt(-cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)","e414be8b":"#parameters(for grid search)\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]","5259bbc5":"#ridge\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\n\n#lasso\nlasso = make_pipeline(\n    RobustScaler(),\n    LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kfolds))\n\n#elastic net\nelasticnet = make_pipeline(\n    RobustScaler(),\n    ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))\n\n#svm\nsvr = make_pipeline(RobustScaler(), SVR(\n    C=20,\n    epsilon=0.009,\n    gamma=0.0003,\n))\n\n#GradientBoosting\ngbr = GradientBoostingRegressor(n_estimators=3000,\n                                learning_rate=0.05,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)\n\n#lightgbm\nlightgbm = LGBMRegressor(\n    objective='regression',\n    num_leaves=4,\n    learning_rate=0.01,\n    n_estimators=5000,\n    max_bin=200,\n    bagging_fraction=0.75,\n    bagging_freq=5,\n    bagging_seed=7,\n    feature_fraction=0.2,\n    feature_fraction_seed=7,\n    verbose=-1,\n    #min_data_in_leaf=2,\n    #min_sum_hessian_in_leaf=11\n)\n\n#xgboost reg:squarederror replacing reg:linear\nxgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=3460,\n                       max_depth=5,\n                       min_child_weight=0,\n                       gamma=0,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:squarederror',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006)","6649a699":"#StackingCVRegressor\uff1aA 'Stacking Cross-Validation' regressor for scikit-learn estimators.\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr, xgboost, lightgbm),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","4c21edb1":"print('TEST score on CV')\n\nscore = cv_rmse(ridge) #cross_val_score(RidgeCV(alphas),X, y)\nprint(\"Ridge score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n\nscore = cv_rmse(lasso)\nprint(\"Lasso score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n\nscore = cv_rmse(elasticnet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n\nscore = cv_rmse(svr)\nprint(\"SVR score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n\nscore = cv_rmse(lightgbm)\nprint(\"Lightgbm score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n\nscore = cv_rmse(gbr)\nprint(\"GradientBoosting score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n\nscore = cv_rmse(xgboost)\nprint(\"Xgboost score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","63bcf9b9":"#train the stacking model\n#1.1 learn first-level model\n#1.2 construct a training set for second-level model\n#2. train the second-level model\n#3. re-learn first-level model on the entire train set\nprint('Training Model')\nstack_gen_model = stack_gen.fit(X_train, y_train) #Fit ensemble regressors and the meta-regressor\nprint('Model Trained')","f4a90e2b":"#submit prediction result\nprint('Predict submission')\nresult = np.floor(np.expm1(stack_gen_model.predict(X_test)))","30322f67":"submission=pd.DataFrame()\nsubmission['Id'] = house_test['Id']\nsubmission['SalePrice']= result\nsubmission.head()\nsubmission.to_csv(\"houseprice_submission.csv\", index=False)","bc301cfd":"# #The Shapiro-Wilk test tests the null hypothesis that the data was drawn from a normal distribution.\n# test_normality = lambda x: stats.shapiro(x.fillna(0))[1] < 0.01\n# normal = pd.DataFrame(house_train[quantitative])\n# normal = normal.apply(test_normality)\n# print(not normal.any())","2c214133":"# def encode(frame, feature):\n#     ordering = pd.DataFrame()\n#     ordering['val'] = frame[feature].unique()\n#     ordering.index = ordering.val\n#     ordering['spmean'] = frame[[feature, 'SalePrice']].groupby(feature).mean()['SalePrice']\n#     ordering = ordering.sort_values('spmean')\n#     ordering['ordering'] = range(1, ordering.shape[0]+1)\n#     ordering = ordering['ordering'].to_dict()\n    \n#     for cat, o in ordering.items():\n#         frame.loc[frame[feature] == cat, feature+'_E'] = o\n    \n# qual_encoded = []\n# for q in qualitative:  \n#     encode(train, q)\n#     qual_encoded.append(q+'_E')\n# print(qual_encoded)","ca826cfb":"> **Correlation** assumes data should be related linearly","97f5e116":"Standardize the data and see if there're any outlier points","d4a08218":"It is apparent that SalePrice ***doesn't follow normal distribution***, so before performing regression it has to be transformed","669d666b":"**Finding Outliers in Data**"}}