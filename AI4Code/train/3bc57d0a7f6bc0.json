{"cell_type":{"64cefec8":"code","2213b4e1":"code","b0b182fe":"code","f3ba640c":"code","2e2d7831":"code","5c482345":"code","50e85254":"code","778cb44a":"code","42907b05":"code","a34fa2f1":"code","b36ed744":"code","1d375049":"code","93ca0564":"code","a634eb8b":"code","716c5831":"code","9e7a3c56":"code","0eb3a643":"code","98ba7745":"code","ee61d4ff":"code","141ad621":"code","03655027":"code","7a2c7fb6":"code","c23c5053":"code","088623f9":"code","849b76f9":"markdown","c414b503":"markdown","cc7a2bd0":"markdown","d29ef777":"markdown","bdb74976":"markdown","371a6629":"markdown","66625598":"markdown","8893255b":"markdown","13917aea":"markdown","67123504":"markdown","c12da685":"markdown"},"source":{"64cefec8":"%matplotlib inline\nimport numpy as np \nimport pandas as pd \nimport os\nimport shutil\nimport glob\nimport random\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\nimport IPython\nfrom numpy.fft import rfft, irfft\nimport numpy as np\nimport random\nimport itertools\n\nfrom scipy.io import wavfile\nimport IPython.display as ipd\nimport matplotlib.pyplot as plt\nimport scipy as sp\nimport tensorflow as tf","2213b4e1":"#Mean Squared\ndef ms(x):\n    return (np.abs(x)**2.0).mean()\n\ndef normalize(y, x=None):\n    if x is not None:\n        x = ms(x)\n    else:\n        x = 1.0\n    return y * np.sqrt( x \/ ms(y) )\n\ndef white_noise(N, state=None):\n    state = np.random.RandomState() if state is None else state\n    return state.randn(N)\n\ndef pink_noise(N, state=None):\n    state = np.random.RandomState() if state is None else state\n    uneven = N%2\n    X = state.randn(N\/\/2+1+uneven) + 1j * state.randn(N\/\/2+1+uneven)\n    S = np.sqrt(np.arange(len(X))+1.) # +1 to avoid divide by zero\n    y = (irfft(X\/S)).real\n    if uneven:\n        y = y[:-1]\n    return normalize(y)\n\ndef blue_noise(N, state=None):\n    state = np.random.RandomState() if state is None else state\n    uneven = N%2\n    X = state.randn(N\/\/2+1+uneven) + 1j * state.randn(N\/\/2+1+uneven)\n    S = np.sqrt(np.arange(len(X)))# Filter\n    y = (irfft(X*S)).real\n    if uneven:\n        y = y[:-1]\n    return normalize(y)\n\ndef brown_noise(N, state=None):\n    state = np.random.RandomState() if state is None else state\n    uneven = N%2\n    X = state.randn(N\/\/2+1+uneven) + 1j * state.randn(N\/\/2+1+uneven)\n    S = (np.arange(len(X))+1)# Filter\n    y = (irfft(X\/S)).real\n    if uneven:\n        y = y[:-1]\n    return normalize(y)\n\ndef violet_noise(N, state=None):\n    state = np.random.RandomState() if state is None else state\n    uneven = N%2\n    X = state.randn(N\/\/2+1+uneven) + 1j * state.randn(N\/\/2+1+uneven)\n    S = (np.arange(len(X)))# Filter\n    y = (irfft(X*S)).real\n    if uneven:\n        y = y[:-1]\n    return normalize(y)","b0b182fe":"#\uc0ac\uc6a9\ud560 GPU\uc640 \ud504\ub85c\uc138\uc2a4\uac00 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \uba54\ubaa8\ub9ac \uc591\uc744 \uc120\ud0dd\ud558\ub294 \ud568\uc218\ndef get_tensorflow_configuration(device=\"0\", memory_fraction=1):\n    device = str(device)\n    config = tf.ConfigProto()\n    config.allow_soft_placement = True\n    config.gpu_options.per_process_gpu_memory_fraction = memory_fraction\n    config.gpu_options.visible_device_list = device\n    return(config)\n\n#\uc0ac\uc6a9\ud560 GPU \uc7a5\uce58\uc640 \uc0ac\uc804 \ud560\ub2f9\ub420 \uba54\ubaa8\ub9ac \ube44\uc728\uc744 \ucc98\ub9ac\ud558\ub294 TF \uc138\uc158\uc744 \uc2dc\uc791\ud558\ub294 \ud568\uc218\ndef start_tensorflow_session(device=\"0\", memory_fraction=1):\n    return(tf.Session(config=get_tensorflow_configuration(device=device, memory_fraction=memory_fraction)))\n\n#\ub9ac\ud3ec\ud305 \ud568\uc218(\uae30\ub85d)\ndef get_summary_writer(session, logs_path, project_id, version_id):\n    path = os.path.join(logs_path,\"{}_{}\".format(project_id, version_id)) \n    if os.path.exists(path):\n        shutil.rmtree(path)\n    summary_writer = tf.summary.FileWriter(path, graph_def=session.graph_def)\n    return(summary_writer)","f3ba640c":"#\uacbd\ub85c \uac80\uc0c9 \uae30\ub2a5\uc758 \ucd9c\ub825\uc744 \uc815\uaddc\ud654\ud558\ub294 \ub370 \uc0ac\uc6a9\ud558\uae30 \uc704\ud55c \ub370\ucf54\ub808\uc774\ud130 \ud568\uc218. \uc2ac\ub798\uc2dc\/\ubc31\uc2ac\ub798\uc2dc \ucc3d\uc758 \uacbd\uc6b0\ub97c \uc218\uc815\ud558\ub294 \ub370 \uc720\uc6a9.\ndef _norm_path(path):\n    def normalize_path(*args, **kwargs):\n        return os.path.normpath(path(*args, **kwargs))\n    return normalize_path\n\n#\uacbd\ub85c \uac80\uc0c9 \uae30\ub2a5\uc758 \ucd9c\ub825\uc774 \uc788\ub294\uc9c0 \ud655\uc778\ud558\uae30 \uc704\ud55c \ub370\ucf54\ub808\uc774\ud130 \ud568\uc218. \uc2ac\ub798\uc2dc\/\ubc31\uc2ac\ub798\uc2dc \ucc3d\uc758 \uacbd\uc6b0\ub97c \uc218\uc815\ud558\ub294 \ub370 \uc720\uc6a9.\ndef _assure_path_exists(path):\n    def assure_exists(*args, **kwargs):\n        p=path(*args, **kwargs)\n        assert os.path.exists(p), \"the following path does not exist: '{}'\".format(p)\n        return p\n    return assure_exists\n\n#\ucd9c\ub825 \uacbd\ub85c \uac80\uc0c9 \uae30\ub2a5\uc758 \ucd9c\ub825\uc5d0 \uc801\uc6a9\ub418\ub294 \uae30\ub2a5\uc744 \uadf8\ub8f9\ud654\ud558\uae30 \uc704\ud55c \ub370\ucf54\ub808\uc774\ud130 \ud568\uc218\ndef _is_output_path(path):\n    @_norm_path\n    @_assure_path_exists\n    def check_existence_or_create_it(*args, **kwargs):\n        if not os.path.exists(path(*args, **kwargs)):\n            \"Path does not exist... creating it: {}\".format(path(*args, **kwargs))\n            os.makedirs(path(*args, **kwargs))\n        return path(*args, **kwargs)\n    return check_existence_or_create_it\n\n#\uc785\ub825 \uacbd\ub85c \uac80\uc0c9 \uae30\ub2a5\uc758 \ucd9c\ub825\uc5d0 \uc801\uc6a9\ub418\ub294 \uae30\ub2a5\uc744 \uadf8\ub8f9\ud654\ud558\uae30 \uc704\ud55c \ub370\ucf54\ub808\uc774\ud130 \ud568\uc218\ndef _is_input_path(path):\n    @_norm_path\n    @_assure_path_exists\n    def check_existence(*args, **kwargs):\n        return path(*args, **kwargs)\n    return check_existence\n\n@_is_input_path\ndef get_train_path():\n    path = \"..\/input\/train\"\n    return path\n\n@_is_input_path\ndef get_test_path():\n    path = \"..\/input\/test\"\n    return path\n\n@_is_input_path\ndef get_train_audio_path():\n    path = os.path.join(get_train_path(), \"audio\")\n    return path\n\n@_is_input_path\ndef get_scoring_audio_path():\n    path = os.path.join(get_test_path(), \"audio\")\n    return path\n\n@_is_output_path\ndef get_submissions_path():\n    path = \"..\/working\/output\"\n    return path\n\n@_is_output_path\ndef get_silence_path():\n    path = \"..\/working\/silence\"\n    return path","2e2d7831":"flatten = lambda l: [item for sublist in l for item in sublist]\n\ndef batching(iterable, n=1):\n    l = len(iterable)\n    for ndx in range(0, l, n):\n        yield iterable[ndx:min(ndx + n, l)]\n\n#wav \ud30c\uc77c\uc758 \ud30c\uc77c \uacbd\ub85c\uac00 \uc8fc\uc5b4\uc9c0\uba74 \uc774 \ud568\uc218\ub294 \ud30c\uc77c\uc744 \uc77d\uace0 \uc815\uaddc\ud654\ud558\uace0 \ud328\ub529\ud558\uc5ec 16k \uc0d8\ud50c\uc774 \uc788\ub294\uc9c0 \ud655\uc778\ud568.\ndef read_wav(filepath, pad=True):\n    sample_rate, x = wavfile.read(filepath)\n    target = os.path.split(os.path.split(filepath)[0])[1]\n    assert sample_rate==16000\n    if pad:\n        return np.pad(x, (0, 16000-len(x)), mode=\"constant\")\/32768, target\n    else:\n        return x\/32768, target\n    \n#\ubc30\uce58 \ubaa9\ub85d\uc774 \uc8fc\uc5b4\uc9c0\uba74 Batch Generator\ub97c \ube4c\ub4dc\ndef get_batcher(list_of_paths, batch_size, label_encoder=None, scoring=False):\n    for filepaths in batching(list_of_paths, batch_size):\n        wavs, targets = zip(*list(map(read_wav, filepaths)))\n        if scoring:\n            yield np.expand_dims(np.row_stack(wavs), 2), filepaths\n        else:\n            if label_encoder is None:\n                yield np.expand_dims(np.row_stack(wavs), 2), np.row_stack(targets)\n            else:\n                yield np.expand_dims(np.row_stack(wavs), 2), np.expand_dims(label_encoder.transform(np.squeeze(targets)),1)","5c482345":"#BatchNormalization (\ubc11\uc758 inception_1d\uc758 normalization function parameter\ub85c \uc0ac\uc6a9\ub420 \uc608\uc815)\nclass BatchNorm(object):\n    def __init__(self, epsilon=1e-5, momentum=0.999, name=\"batch_norm\"):\n        with tf.variable_scope(name):\n            self.epsilon = epsilon\n            self.momentum = momentum\n            self.name = name\n\n    def __call__(self, x, train=True):\n        return tf.contrib.layers.batch_norm(x,\n                                            decay=self.momentum,\n                                            updates_collections=None,\n                                            epsilon=self.epsilon,\n                                            scale=True,\n                                            is_training=train,\n                                            scope=self.name)\n    \n    \n#\uad6c\uc870: CONV1D(64,1x1) -> CONV1D(128,3x3) -> CONV1D(128, 5x5) -> CONV1D(128, 7x7) -> MAXPOOL(16, 3x3) + CONV1D(16, 1x1)\n#-> MAXPOOL(16,5x5) + CONV1D(16,1x1) -> AVGPOOL(16,3x3) + CONV1D(16,1x1) -> AVGPOOL(16,5x5) + CONV1D(16,1x1)\ndef inception_1d(x, is_train, depth, norm_function, activ_function, name):\n    with tf.variable_scope(name):\n        x_norm = norm_function(name=\"norm_input\")(x, train=is_train)\n\n        # Branch 1: 64 x conv 1x1 \n        branch_conv_1_1 = tf.layers.conv1d(inputs=x_norm, filters=16*depth, kernel_size=1,\n                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                           padding=\"same\", name=\"conv_1_1\")\n        branch_conv_1_1 = norm_function(name=\"norm_conv_1_1\")(branch_conv_1_1, train=is_train)\n        branch_conv_1_1 = activ_function(branch_conv_1_1, \"activation_1_1\")\n\n        # Branch 2: 128 x conv 3x3 \n        branch_conv_3_3 = tf.layers.conv1d(inputs=x_norm, filters=16, kernel_size=1, \n                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                           padding=\"same\", name=\"conv_3_3_1\")\n        branch_conv_3_3 = norm_function(name=\"norm_conv_3_3_1\")(branch_conv_3_3, train=is_train)\n        branch_conv_3_3 = activ_function(branch_conv_3_3, \"activation_3_3_1\")\n\n        branch_conv_3_3 = tf.layers.conv1d(inputs=branch_conv_3_3, filters=32*depth, kernel_size=3, \n                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                           padding=\"same\", name=\"conv_3_3_2\")\n        branch_conv_3_3 = norm_function(name=\"norm_conv_3_3_2\")(branch_conv_3_3, train=is_train)\n        branch_conv_3_3 = activ_function(branch_conv_3_3, \"activation_3_3_2\")\n\n        # Branch 3: 128 x conv 5x5 \n        branch_conv_5_5 = tf.layers.conv1d(inputs=x_norm, filters=16, kernel_size=1, \n                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                           padding=\"same\", name=\"conv_5_5_1\")\n        branch_conv_5_5 = norm_function(name=\"norm_conv_5_5_1\")(branch_conv_5_5, train=is_train)\n        branch_conv_5_5 = activ_function(branch_conv_5_5, \"activation_5_5_1\")\n\n        branch_conv_5_5 = tf.layers.conv1d(inputs=branch_conv_5_5, filters=32*depth, kernel_size=5, \n                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                           padding=\"same\", name=\"conv_5_5_2\")\n        branch_conv_5_5 = norm_function(name=\"norm_conv_5_5_2\")(branch_conv_5_5, train=is_train)\n        branch_conv_5_5 = activ_function(branch_conv_5_5, \"activation_5_5_2\")\n\n        # Branch 4: 128 x conv 7x7\n        branch_conv_7_7 = tf.layers.conv1d(inputs=x_norm, filters=16, kernel_size=1, \n                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                           padding=\"same\", name=\"conv_7_7_1\")\n        branch_conv_7_7 = norm_function(name=\"norm_conv_7_7_1\")(branch_conv_7_7, train=is_train)\n        branch_conv_7_7 = activ_function(branch_conv_7_7, \"activation_7_7_1\")\n\n        branch_conv_7_7 = tf.layers.conv1d(inputs=branch_conv_7_7, filters=32*depth, kernel_size=5, \n                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                           padding=\"same\", name=\"conv_7_7_2\")\n        branch_conv_7_7 = norm_function(name=\"norm_conv_7_7_2\")(branch_conv_7_7, train=is_train)\n        branch_conv_7_7 = activ_function(branch_conv_7_7, \"activation_7_7_2\")\n\n        # Branch 5: 16 x (max_pool 3x3 + conv 1x1)\n        branch_maxpool_3_3 = tf.layers.max_pooling1d(inputs=x_norm, pool_size=3, strides=1, padding=\"same\", name=\"maxpool_3\")\n        branch_maxpool_3_3 = norm_function(name=\"norm_maxpool_3_3\")(branch_maxpool_3_3, train=is_train)\n        branch_maxpool_3_3 = tf.layers.conv1d(inputs=branch_maxpool_3_3, filters=16, kernel_size=1, \n                                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                              padding=\"same\", name=\"conv_maxpool_3\")\n\n        # Branch 6: 16 x (max_pool 5x5 + conv 1x1)\n        branch_maxpool_5_5 = tf.layers.max_pooling1d(inputs=x_norm, pool_size=5, strides=1, padding=\"same\", name=\"maxpool_5\")\n        branch_maxpool_5_5 = norm_function(name=\"norm_maxpool_5_5\")(branch_maxpool_5_5, train=is_train)\n        branch_maxpool_5_5 = tf.layers.conv1d(inputs=branch_maxpool_5_5, filters=16, kernel_size=1, \n                                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                              padding=\"same\", name=\"conv_maxpool_5\")\n\n        # Branch 7: 16 x (avg_pool 3x3 + conv 1x1)\n        branch_avgpool_3_3 = tf.layers.average_pooling1d(inputs=x_norm, pool_size=3, strides=1, padding=\"same\", name=\"avgpool_3\")\n        branch_avgpool_3_3 = norm_function(name=\"norm_avgpool_3_3\")(branch_avgpool_3_3, train=is_train)\n        branch_avgpool_3_3 = tf.layers.conv1d(inputs=branch_avgpool_3_3, filters=16, kernel_size=1,\n                                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                              padding=\"same\", name=\"conv_avgpool_3\")\n\n        # Branch 8: 16 x (avg_pool 5x5 + conv 1x1)\n        branch_avgpool_5_5 = tf.layers.average_pooling1d(inputs=x_norm, pool_size=5, strides=1, padding=\"same\", name=\"avgpool_5\")\n        branch_avgpool_5_5 = norm_function(name=\"norm_avgpool_5_5\")(branch_avgpool_5_5, train=is_train)\n        branch_avgpool_5_5 = tf.layers.conv1d(inputs=branch_avgpool_5_5, filters=16, kernel_size=1, \n                                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                              padding=\"same\", name=\"conv_avgpool_5\")\n\n        #\ud569\uce58\uae30\n        output = tf.concat([branch_conv_1_1, branch_conv_3_3, branch_conv_5_5, branch_conv_7_7, branch_maxpool_3_3, \n                           branch_maxpool_5_5, branch_avgpool_3_3, branch_avgpool_5_5], axis=-1)\n        return output","50e85254":"# \ud569\uc131\ud55c \ub178\uc774\uc988\uc640 \uc6d0\ub798 \ub178\uc774\uc988 \ub370\uc774\ud130 \uc14b \ud569\uce58\uae30\nfilepaths_noise = glob.glob(os.path.join(get_train_audio_path(), \"_background_noise_\", \"*.wav\"))\n\nnoise = np.concatenate(list(map(lambda x: read_wav(x, False)[0], filepaths_noise)))\nnoise = np.concatenate([noise, noise[::-1]])\nsynthetic_noise = np.concatenate([white_noise(N=16000*30, state=np.random.RandomState(655321)), \n                                  blue_noise(N=16000*30, state=np.random.RandomState(655321)),\n                                  pink_noise(N=16000*30, state=np.random.RandomState(655321)),\n                                  brown_noise(N=16000*30, state=np.random.RandomState(655321)),\n                                  violet_noise(N=16000*30, state=np.random.RandomState(655321)),\n                                  np.zeros(16000*60)])\nsynthetic_noise \/= np.max(np.abs(synthetic_noise))\nsynthetic_noise = np.concatenate([synthetic_noise, (synthetic_noise+synthetic_noise[::-1])\/2])\nall_noise = np.concatenate([noise, synthetic_noise])","778cb44a":"np.random.seed(655321)\nrandom.seed(655321)\n\npath = get_silence_path()\n\nif not os.path.exists(path):\n    os.makedirs(path) # It fails in kaggle kernel due to the read-only filesystem\n\nfor noise_clip_no in tqdm(range(8000)):\n    if noise_clip_no<=4000:\n        idx = np.random.randint(0, len(noise)-16000)\n        clip = noise[idx:(idx+16000)]\n    else:\n        idx = np.random.randint(0, len(synthetic_noise)-16000)\n        clip = synthetic_noise[idx:(idx+16000)]\n    wavfile.write(os.path.join(path, \"{0:04d}.wav\".format(noise_clip_no)), 16000, \n                               ((32767*clip\/np.max(np.abs(clip))).astype(np.int16)))","42907b05":"filepaths = glob.glob(os.path.join(get_train_audio_path(), \"**\/*.wav\"), recursive=True)\nfilepaths += glob.glob(os.path.join(get_silence_path(), \"**\/*.wav\"), recursive=True)\nfilepaths = list(filter(lambda fp: \"_background_noise_\" not in fp, filepaths))\nvalidation_list = open(os.path.join(get_train_path(), \"validation_list.txt\")).readlines()\ntest_list = open(os.path.join(get_train_path(), \"testing_list.txt\")).readlines()\nvalidation_list = list(map(lambda fn: os.path.join(get_train_audio_path(), fn.strip()), validation_list))\ntesting_list = list(map(lambda fn: os.path.join(get_train_audio_path(), fn.strip()), test_list))\ntraining_list = np.setdiff1d(filepaths, validation_list+testing_list).tolist()","a34fa2f1":"random.seed(655321)\nrandom.shuffle(filepaths)\nrandom.shuffle(validation_list)\nrandom.shuffle(testing_list)\nrandom.shuffle(training_list)","b36ed744":"# \uc81c\ub300\ub85c \uc218\ud589\ub418\uc5c8\ub294\uc9c0 \ud655\uc778\ud558\ub294 assert\ubb38\ub4e4\nassert all(map(lambda fp: os.path.splitext(fp)[1]==\".wav\", filepaths))\nassert len(filepaths)==64727 - 6 + 8000\nassert len(training_list) == len(filepaths) - 6798 - 6835 \nassert len(validation_list) == 6798\nassert len(testing_list) == 6835\n\nassert all(map(lambda fn: os.path.exists(os.path.join(fn)), validation_list))\nassert all(map(lambda fn: os.path.exists(os.path.join(fn)), testing_list))\nassert all(map(lambda fn: os.path.exists(os.path.join(fn)), training_list))\nassert set(validation_list + testing_list + training_list) == set(filepaths)\n\nassert len(np.intersect1d(validation_list, testing_list))==0\nassert len(np.intersect1d(training_list, testing_list))==0\nassert len(np.intersect1d(training_list, validation_list))==0","1d375049":"# \ud074\ub798\uc2a4 \ucc98\ub9ac\ncardinal_classes = list(set(map(lambda fp:os.path.split(os.path.split(fp)[0])[1], filepaths)))\nle_classes = LabelEncoder().fit(cardinal_classes)\nCounter(map(lambda fp:os.path.split(os.path.split(fp)[0])[1], filepaths))","93ca0564":"_gen_test = get_batcher(filepaths, 1000)\nbatch_a_wav, batch_a_target = next(_gen_test)\nbatch_b_wav, batch_b_target = next(_gen_test)\n_gen_test_le = get_batcher(filepaths, 1000, label_encoder=le_classes)\nbatch_le_wav, batch_le_target = next(_gen_test_le)\n\nassert batch_a_wav.shape == (1000, 16000, 1)\nassert batch_le_wav.shape == (1000, 16000, 1)\nassert batch_a_wav.shape == batch_b_wav.shape == batch_le_wav.shape\n\nassert np.sum(np.abs(batch_a_wav-batch_b_wav)) != 0\nassert len(batch_a_target) == len(batch_b_target) == len(batch_le_target)\nassert any(batch_a_target != batch_b_target)\n\nassert all(batch_le_target == np.expand_dims(le_classes.transform(np.squeeze(batch_a_target)),1))","a634eb8b":"class NameSpacer:\n    def __init__(self, **kwargs):\n        self.__dict__.update(kwargs)\n\nclass Architecture:\n    def __init__(self, class_cardinality, seq_len=16000, name=\"architecture\"):\n        self.seq_len = seq_len\n        self.class_cardinality = class_cardinality\n        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n        self.name=name\n        self.define_computation_graph()\n        \n        #Aliases\n        self.ph = self.placeholders\n        self.op = self.optimizers\n        self.summ = self.summaries\n\n    def define_computation_graph(self):\n        # Reset graph\n        tf.reset_default_graph()\n        self.placeholders = NameSpacer(**self.define_placeholders())\n        self.core_model = NameSpacer(**self.define_core_model())\n        self.losses = NameSpacer(**self.define_losses())\n        self.optimizers = NameSpacer(**self.define_optimizers())\n        self.summaries = NameSpacer(**self.define_summaries())\n\n    def define_placeholders(self):\n        with tf.variable_scope(\"Placeholders\"):\n            wav_in = tf.placeholder(dtype=tf.float32, shape=(None, self.seq_len, 1), name=\"wav_in\")\n            is_train = tf.placeholder(dtype=tf.bool, shape=None, name=\"is_train\")\n            target = tf.placeholder(dtype=tf.int32, shape=(None, 1), name=\"target\")\n            acc_dev = tf.placeholder(dtype=tf.float32, shape=None, name=\"acc_dev\")\n            loss_dev = tf.placeholder(dtype=tf.float32, shape=None, name=\"loss_dev\")\n            return({\"wav_in\": wav_in, \"target\": target, \"is_train\": is_train, \"acc_dev\": \n                    acc_dev, \"loss_dev\": loss_dev})\n        \n    def define_core_model(self):\n        with tf.variable_scope(\"Core_Model\"):\n            x = inception_1d(x=self.placeholders.wav_in, is_train=self.placeholders.is_train, \n                             norm_function=BatchNorm, activ_function=tf.nn.relu, depth=1,\n                             name=\"Inception_1_1\")\n            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n                             activ_function=tf.nn.relu, depth=1, name=\"Inception_1_2\")\n            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_1\")\n            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n                             activ_function=tf.nn.relu, depth=1, name=\"Inception_2_1\")\n            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n                             activ_function=tf.nn.relu, depth=1, name=\"Inception_2_3\")\n            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_2\")\n            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n                             activ_function=tf.nn.relu, depth=2, name=\"Inception_3_1\")\n            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n                             activ_function=tf.nn.relu, depth=2, name=\"Inception_3_2\")\n            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_3\")\n            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n                             activ_function=tf.nn.relu, depth=2, name=\"Inception_4_1\")\n            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n                             activ_function=tf.nn.relu, depth=2, name=\"Inception_4_2\")\n            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_4\")\n            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n                             activ_function=tf.nn.relu, depth=3, name=\"Inception_5_1\")\n            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n                             activ_function=tf.nn.relu, depth=3, name=\"Inception_5_2\")\n            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_5\")\n            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n                             activ_function=tf.nn.relu, depth=3, name=\"Inception_6_1\")\n            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n                             activ_function=tf.nn.relu, depth=3, name=\"Inception_6_2\")\n            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_6\")\n            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n                             activ_function=tf.nn.relu, depth=4, name=\"Inception_7_1\")\n            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n                             activ_function=tf.nn.relu, depth=4, name=\"Inception_7_2\")\n            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_7\")\n            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n                             activ_function=tf.nn.relu, depth=4, name=\"Inception_8_1\")\n            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n                             activ_function=tf.nn.relu, depth=4, name=\"Inception_8_2\")\n            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_8\")\n            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n                             activ_function=tf.nn.relu, depth=4, name=\"Inception_9_1\")\n            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n                             activ_function=tf.nn.relu, depth=4, name=\"Inception_9_2\")\n            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_9\")\n            x = tf.contrib.layers.flatten(x)\n            x = tf.layers.dense(BatchNorm(name=\"bn_dense_1\")(x,train=self.placeholders.is_train),\n                                128, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                name=\"dense_1\")\n            output = tf.layers.dense(BatchNorm(name=\"bn_dense_2\")(x,train=self.placeholders.is_train),\n                                self.class_cardinality, activation=None, kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                name=\"output\")\n            return({\"output\": output})\n        \n    def define_losses(self):\n        with tf.variable_scope(\"Losses\"):\n            softmax_ce = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.squeeze(self.placeholders.target), \n                                                                        logits=self.core_model.output,\n                                                                        name=\"softmax\")\n            return({\"softmax\": softmax_ce})\n\n    def define_optimizers(self):\n        with tf.variable_scope(\"Optimization\"):\n            op = self.optimizer.minimize(self.losses.softmax)\n            return({\"op\": op})\n\n    def define_summaries(self):\n        with tf.variable_scope(\"Summaries\"):\n            ind_max = tf.squeeze(tf.cast(tf.argmax(self.core_model.output, axis=1), tf.int32))\n            target = tf.squeeze(self.placeholders.target)\n            acc= tf.reduce_mean(tf.cast(tf.equal(ind_max, target), tf.float32))\n            loss = tf.reduce_mean(self.losses.softmax)\n            train_scalar_probes = {\"accuracy\": acc, \n                                   \"loss\": loss}\n            train_performance_scalar = [tf.summary.scalar(k, tf.reduce_mean(v), family=self.name) \n                                        for k, v in train_scalar_probes.items()]\n            train_performance_scalar = tf.summary.merge(train_performance_scalar)\n\n            dev_scalar_probes = {\"acc_dev\": self.placeholders.acc_dev, \n                                 \"loss_dev\": self.placeholders.loss_dev}\n            dev_performance_scalar = [tf.summary.scalar(k, v, family=self.name) for k, v in dev_scalar_probes.items()]\n            dev_performance_scalar = tf.summary.merge(dev_performance_scalar)\n            return({\"accuracy\": acc, \"loss\": loss, \"s_tr\": train_performance_scalar, \"s_de\": dev_performance_scalar})","716c5831":"net = Architecture(class_cardinality=len(cardinal_classes), name=\"wavception\")","9e7a3c56":"sess = start_tensorflow_session(device=\"1\")\nsw = get_summary_writer(sess, \"~\/.logs_tensorboard\/\", \"wavception\", \"V1\") # Adjust your tensorboard logs path here\nc=0","0eb3a643":"sess.run(tf.global_variables_initializer())","98ba7745":"np.random.seed(655321)\nrandom.seed(655321)","ee61d4ff":"for epoch in range(50000):\n    random.shuffle(training_list)\n    batcher = get_batcher(training_list, 16, le_classes)\n    for i, (batch_x, batch_y) in enumerate(batcher):\n        _, loss, acc, s = sess.run([net.op.op, net.losses.softmax, net.summ.accuracy, net.summ.s_tr],\n                                 feed_dict={net.ph.wav_in: batch_x, net.ph.target: batch_y, \n                                            net.ph.is_train: True})\n        print(\"[{0:04d}|{1:04d}] Accuracy train: {2:.2f}%\".format(epoch, i, acc*100))\n        sw.add_summary(s, c)\n        \n        if c%1000==0: # Validation\n            accuracies_dev=[]\n            losses_dev=[]\n            batcher = get_batcher(validation_list, 16, le_classes)\n            for i, (batch_x, batch_y) in enumerate(batcher):\n                acc, loss= sess.run([net.summ.accuracy, net.summ.loss], \n                               feed_dict={net.ph.wav_in: batch_x, net.ph.target: batch_y, \n                                          net.ph.is_train: False})\n                accuracies_dev.append(acc)\n                losses_dev.append(loss)\n            s = sess.run(net.summ.s_de, feed_dict={net.ph.acc_dev: np.mean(accuracies_dev),\n                                                        net.ph.loss_dev: np.mean(losses_dev)})\n            sw.add_summary(s, c)\n        c += 1","141ad621":"accuracies=[]\nbatcher = get_batcher(testing_list, 64, le_classes)\nfor i, (batch_x, batch_y) in tqdm(enumerate(batcher)):\n    acc= sess.run(net.summ.accuracy, feed_dict={net.ph.wav_in: batch_x, net.ph.target: batch_y, \n                                                     net.ph.is_train: False})\n    accuracies.append(acc)","03655027":"scoring_list = glob.glob(os.path.join(get_scoring_audio_path(), \"*.wav\"), recursive=True)","7a2c7fb6":"batcher = get_batcher(scoring_list, 80, le_classes, scoring=True)","c23c5053":"fns = []\nprds = []\nfor i, (batch_x, filepaths) in tqdm(enumerate(batcher)):\n    pred = sess.run(net.core_model.output, feed_dict={net.ph.wav_in: batch_x, net.ph.is_train: False})\n    fns.extend(map(lambda f:os.path.split(f)[1], filepaths))\n    prds.extend(map(lambda f:np.argmax(pred, axis=1).tolist(), pred))","088623f9":"df=pd.DataFrame({\"fname\":fns, \"label\": prds})\ndf.label = le_classes.inverse_transform(df.label)\ndf.loc[~df.label.isin([\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\", \"silence\"]), \"label\"]=\"unknown\"\ndf.to_csv(os.path.join(get_submissions_path(), \"submission.csv\"), index=False)","849b76f9":"### \ub370\uc774\ud130 \ub85c\ub4dc \ubc0f \uc900\ube44","c414b503":"### Block\uc744 \ub9cc\ub4dc\ub294 \ud568\uc218 (1D Inception)\n#### \uae30\uc874 CNN\ubcf4\ub2e4 \ud5a5\uc0c1\ub41c \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4.","cc7a2bd0":"### \ubaa8\ub378 \uc2e4\ud589","d29ef777":"### \ub370\uc774\ud130 Path\uc640 \uad00\ub828\ub41c \ud568\uc218\ub4e4","bdb74976":"### \ub178\uc774\uc988\ub97c \ub9cc\ub4e4\uc5b4 \uc8fc\ub294 \ud568\uc218\ub4e4  \n\ucd9c\ucc98: https:\/\/github.com\/python-acoustics\/python-acoustics\/blob\/master\/acoustics\/generator.py\n\n\n|Color|Power|Power Density|\n|------|:--------:|:---:|\n|White|+3 dB|0 dB|\n|Pink|0 dB|-3 dB|\n|Blue|+6 dB|+3 dB|\n|Brown|-3 dB|-6 dB|\n|Violet|+9 dB|+6 dB|","371a6629":"\ud14c\uc2a4\ud2b8","66625598":"### \uad6c\uc870 \ub514\uc790\uc778","8893255b":"### \uc608\uce21 \ubc0f \uc81c\ucd9c \ud30c\uc77c \uc0dd\uc131","13917aea":"### Tensorflow \uae30\ubcf8 \ub3d9\uc791 \uc218\ud589\uc744 \uc704\ud55c \ud568\uc218\ub4e4","67123504":"# 1D Inception Model Approach (KOR)\n## Origin: https:\/\/www.kaggle.com\/ivallesp\/wavception-v1-a-1-d-inception-approach-lb-0-76\n### Translated by siryuon","c12da685":"### Data Handling Tools"}}