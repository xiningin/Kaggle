{"cell_type":{"bc317d22":"code","7285afea":"code","380d6a5e":"code","13f06987":"code","7cf6fa01":"code","542a84e3":"code","85f9210f":"code","87da78f0":"code","18ca5fbf":"code","97c915bf":"code","697b086c":"code","ec4f1cf1":"code","3c59e949":"code","14afda41":"code","0f8b510c":"code","df347344":"code","0328bb8e":"code","9d998f46":"code","36edbd08":"code","e9f5f818":"code","c6849410":"code","64fe6746":"code","7ac3ad57":"code","e13ab9e7":"code","b7926df3":"code","6b30b2b5":"code","67ca3448":"code","555a3067":"code","3f11f181":"code","c7efa1d8":"code","b43625ea":"code","20395d4c":"code","f80d06e8":"code","3f498e64":"code","17b7e84d":"code","c06e8a34":"code","aad1503c":"code","72a166b0":"code","f0b32e16":"code","f0c3abfc":"code","fd5a0a03":"code","1dc5799e":"code","1badb1cf":"code","aaaf6f38":"code","78c6d3db":"code","1eb6459b":"code","f748b3ad":"code","afb30d27":"code","5526f1c2":"code","694d05e3":"code","dac3f1ed":"code","eeb35a7b":"code","d1cecc4e":"code","710dd49c":"code","2f6020cb":"code","b79b6725":"code","23974a71":"markdown","1a1bad96":"markdown","3d80ff01":"markdown","bf10b435":"markdown","89203cb4":"markdown","010e923b":"markdown","44729eb2":"markdown","57c2be57":"markdown","fa1dcb4a":"markdown","0aa7479b":"markdown","80b94513":"markdown","bef50faf":"markdown","47c342ea":"markdown","a878caa7":"markdown","4f3269d3":"markdown","9a331d7f":"markdown","34c10a56":"markdown","dc8bd45e":"markdown","2416d107":"markdown","8a3a3e76":"markdown","191eea0f":"markdown","bb60bff9":"markdown","905e74ea":"markdown","a112e9e1":"markdown","67005136":"markdown","fe5e320f":"markdown","9925aaed":"markdown","03c1d272":"markdown","1fb849ba":"markdown","2d2c0073":"markdown","e90a678d":"markdown","6390bee1":"markdown","b0cc23c8":"markdown","1af421bc":"markdown","1716a1cd":"markdown"},"source":{"bc317d22":"import numpy as np\nimport pandas as pd\nfrom sklearn import model_selection, metrics, ensemble, preprocessing, impute, linear_model, pipeline, compose, naive_bayes, tree, svm, feature_selection, neighbors\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBClassifier\nimport itertools\n\n\nsns.set_style('whitegrid')","7285afea":"train = pd.read_csv('..\/input\/titanic\/train.csv', index_col = 'PassengerId')\ntest = pd.read_csv('..\/input\/titanic\/test.csv', index_col = 'PassengerId')","380d6a5e":"train.head(10)","13f06987":"train.describe()","7cf6fa01":"valueCounts = dict()\n\nfor col in train.columns:\n    valueCounts[col] = train[col].value_counts()\n    \nvalueCounts['Survived']","542a84e3":"549 \/ (549+342)","85f9210f":"342 \/ (549+342)","87da78f0":"categorical = [col for col in train.columns if train[col].dtype == 'object' or train[col].nunique() <= 8]\n\nnumerical = [col for col in train.columns if col not in categorical]\n\ncategorical, numerical","18ca5fbf":"categorical.remove('Name')\ncategorical.remove('Ticket')\ncategorical.remove('Survived')\ncategorical.remove('Cabin')","97c915bf":"plt.figure(figsize=(24,13.5))\nsns.kdeplot(data=train['Age'])\nplt.show()\n\nplt.figure(figsize=(24,13.5))\nsns.boxplot(data=train, orient='h', x='Age')\nplt.show()","697b086c":"plt.figure(figsize=(24,13.5))\nsns.histplot(data=train['Age'])\nplt.show()","ec4f1cf1":"plt.figure(figsize=(24,13.5))\nsns.kdeplot(data=train['Fare'])\nplt.show()\n\nplt.figure(figsize=(24,13.5))\nsns.boxplot(data=train, x='Fare')\nplt.show()","3c59e949":"plt.figure(figsize=(24,13.5))\nsns.barplot(data=train, x='SibSp', y='Survived')\nplt.show()","14afda41":"plt.figure(figsize=(24,13.5))\nsns.barplot(data=train, x='Parch', y='Survived')\nplt.show()","0f8b510c":"train.groupby('Parch').mean()","df347344":"train.loc[:, 'Parch'].value_counts()","0328bb8e":"train[train['Parch'] == 4]","9d998f46":"train[train['Parch'] == 6]","36edbd08":"for col in categorical:\n    plt.figure(figsize=(24,13.5))\n    sns.barplot(x=train[col], y=train['Survived'], hue=train['Sex'])\n    plt.show()","e9f5f818":"for col in numerical:\n    plt.figure(figsize=(24,13.5))\n    sns.lmplot(data = train, x=col, y='Survived', hue='Sex', logistic=True, scatter=False, height=9, aspect=16\/9)\n    plt.show()","c6849410":"for col in categorical:\n    plt.figure(figsize=(24,13.5))\n    sns.barplot(x=train[col], y=train['Survived'], hue=train['Pclass'])\n    plt.show()","64fe6746":"for col in numerical:\n    plt.figure(figsize=(24,13.5))\n    sns.lmplot(data = train, x=col, y='Survived', hue='Pclass', logistic=True, scatter=False, height=9, aspect=16\/9)\n    plt.show()","7ac3ad57":"for col in categorical:\n    plt.figure(figsize=(24,13.5))\n    sns.barplot(x=train[col], y=train['Survived'], hue=train['Embarked'])\n    plt.show()","e13ab9e7":"for col in numerical:\n    plt.figure(figsize=(24,13.5))\n    sns.lmplot(data = train, x=col, y='Survived', hue='Embarked', logistic=True, scatter=False, height=9, aspect=16\/9)\n    plt.show()","b7926df3":"plt.figure(figsize=(24,13.5))\nsns.heatmap(data=train.corr(), annot=True)\nplt.show()","6b30b2b5":"X = train.drop(['Cabin', 'Ticket', 'Name', 'Survived'], axis=1)\n\ny = train.Survived","67ca3448":"categorical = [col for col in X.columns if X[col].dtype == 'object' or X[col].nunique() <= 8]\n\nnumerical = [col for col in X.columns if col not in categorical]\n\ncategorical, numerical","555a3067":"def encode(X):\n    df = X.copy()\n    df['Embarked'].replace({\n        'C': 3,\n        'S': 2,\n        'Q': 1\n    }, inplace=True)\n\n    df['Sex'].replace({\n        'male': 0,\n        'female': 1\n    }, inplace=True)\n    return df","3f11f181":"def points(X):\n    df = X.copy()\n    LowAge = df.loc[:, 'Age'].quantile(0.25)\n    HighAge = df.loc[:, 'Age'].quantile(0.75)\n    df.loc[:, 'Family'] = df['Parch']+df['SibSp']\n    df.loc[:, 'Wealth Score'] = df['Fare'] \/ df['Pclass']\n    HighWealth = df['Wealth Score'].quantile(0.6)\n    for row in df.index:\n        tally = 0\n        if df.loc[row, 'Pclass'] == 1:\n            tally += 2\n        elif df.loc[row, 'Pclass'] == 2:\n            tally += 1\n        else:\n            pass\n\n        if df.loc[row, 'Sex'] == 1:\n            tally += 1\n        else:\n            pass\n\n        if df.loc[row, 'Embarked'] == 3:\n            tally += 2\n        elif df.loc[row, 'Embarked'] == 2:\n            tally += 1\n        else:\n            pass\n        \n        if df.loc[row, 'SibSp'] in [0, 1, 2]:\n            tally += 1\n        else:\n            pass\n        \n        if df.loc[row, 'Sex'] == 1 and df.loc[row, 'Parch'] in [0, 1, 2]:\n            tally += 1\n        else:\n            pass\n        \n        if df.loc[row, 'Sex'] == 1 and df.loc[row, 'Pclass'] == 1:\n            tally += 3\n        elif df.loc[row, 'Sex'] == 1 and df.loc[row, 'Pclass'] == 2:\n            tally += 2\n        else:\n            pass\n\n        if df.loc[row, 'Sex'] == 1 and (df.loc[row, 'Age'] < LowAge or df.loc[row, 'Age'] > HighAge):\n            tally += 3\n        elif df.loc[row, 'Age'] < LowAge:\n            tally += 1\n        else:\n            pass\n\n        if df.loc[row, 'Wealth Score'] > HighWealth:\n            tally += 1\n        else:\n            pass\n\n        if df.loc[row, 'Embarked'] == 3 and df.loc[row, 'Parch'] in [0,1,2]:\n            tally += 2\n        elif df.loc[row, 'Embarked'] == 2 and df.loc[row, 'Parch'] in [1,3]:\n            tally += 1\n        else:\n            pass\n        \n        if df.loc[row, 'Embarked'] == 1 and df.loc[row, 'Parch'] == 0:\n            tally += 1\n        else:\n            pass\n        \n        if df.loc[row, 'Embarked'] == 2 and df.loc[row, 'SibSp'] in [0,1]:\n            tally += 1\n        elif df.loc[row, 'Embarked'] in [1, 3] and df.loc[row, 'SibSp'] in [0,1,2]:\n            tally += 1\n        else:\n            pass\n        \n        if df.loc[row, 'Pclass'] == 1 and df.loc[row, 'SibSp'] in [0,1,2]:\n            tally += 2\n        elif df.loc[row, 'Pclass'] == 1 and df.loc[row, 'SibSp'] == 3:\n            tally += 1\n        else:\n            pass\n        \n        if df.loc[row, 'Pclass'] == 2 and df.loc[row, 'SibSp'] in [0,1,3]:\n            tally += 1\n        else:\n            pass\n        \n        if df.loc[row, 'Pclass'] == 3 and df.loc[row, 'SibSp'] in [0,1,2]:\n            tally += 1\n        else:\n            pass\n        \n        if df.loc[row, 'Pclass'] == 1 and df.loc[row, 'Parch'] in [0,1,2]:\n            tally += 2\n        else:\n            pass\n        \n        if df.loc[row, 'Pclass'] == 2 and df.loc[row, 'Parch'] in [0, 1, 2, 3]:\n            tally += 1\n        else:\n            pass\n        \n        if df.loc[row, 'Pclass'] == 3 and df.loc[row, 'Parch'] == 1:\n            tally += 1\n        else:\n            pass\n\n        df.loc[row, 'Point Score'] = tally\n\n    df.loc[:, 'Overall Score'] = df['Point Score'] * df['Wealth Score']\n    High_Overall = 1250\n    HighPoints = 10\n    HighWealth = 200\n    \n    for row in df.index:\n        if df.loc[row, 'Overall Score'] >= 2400:\n            df.loc[row, 'High Overall'] = 1\n        else:\n            df.loc[row, 'High Overall'] = 0\n            \n        if df.loc[row, 'Point Score'] >= 13:\n            df.loc[row, 'High Points'] = 1\n        else:\n            df.loc[row, 'High Points'] = 0\n        \n        if df.loc[row, 'Wealth Score'] >= 200:\n            df.loc[row, 'High Wealth'] = 1\n        else:\n            df.loc[row, 'High Wealth'] = 0\n        \n    return df","c7efa1d8":"def prep(x):\n    df = x.copy()\n    df = points(encode(df))\n    return df.loc[:, ['Overall Score', 'Point Score', 'Wealth Score', 'High Overall', 'High Points']+categorical+numerical]","b43625ea":"plt.figure(figsize=(24,13.5))\nsns.lmplot(data=points(encode(train)), x='Point Score', y='Survived', logistic=True, scatter=False, height=9, aspect=16\/9)\nplt.show()","20395d4c":"plt.figure(figsize=(24,13.5))\nsns.lmplot(data=points(encode(train)), x='Wealth Score', y='Survived', logistic=True, scatter=False, height=9, aspect=16\/9)\nplt.show()","f80d06e8":"plt.figure(figsize=(24,13.5))\nsns.lmplot(data=points(encode(train)), x='Overall Score', y='Survived', logistic=True, scatter=False, height=9, aspect=16\/9)\nplt.show()","3f498e64":"plt.figure(figsize=(24,13.5))\nsns.heatmap(data=points(encode(train)).corr(), annot=True)\nplt.show()","17b7e84d":"full = pipeline.Pipeline(steps=[\n    ('preprocess', preprocessing.FunctionTransformer(prep)),\n    ('normalise', preprocessing.Normalizer()),\n    ('scale', preprocessing.MinMaxScaler()),\n    ('model', linear_model.LogisticRegression(C=1, solver='newton-cg', penalty='l2'))\n])","c06e8a34":"rffull = pipeline.Pipeline(steps=[\n    ('preprocess', preprocessing.FunctionTransformer(prep)),\n    ('normalize', preprocessing.Normalizer()),\n    ('scale', preprocessing.MinMaxScaler()),\n    ('model', ensemble.RandomForestClassifier(n_estimators=1300, min_samples_split=5, min_samples_leaf=4, max_depth=30, max_features='sqrt', bootstrap=False))\n])","aad1503c":"gbfull = pipeline.Pipeline(steps=[\n    ('preprocess', preprocessing.FunctionTransformer(prep)),\n    ('normalize', preprocessing.Normalizer()),\n    ('scale', preprocessing.MinMaxScaler()),\n    ('model', ensemble.GradientBoostingClassifier(validation_fraction=0.3,\n                                                    subsample=1,\n                                                    n_iter_no_change=5,\n                                                    n_estimators=360,\n                                                    min_samples_split=2,\n                                                    min_samples_leaf=1,\n                                                    max_depth=2,\n                                                    learning_rate=0.4))\n])","72a166b0":"adafull = pipeline.Pipeline(steps=[\n    ('preprocess', preprocessing.FunctionTransformer(prep)),\n    ('normalize', preprocessing.Normalizer()),\n    ('scale', preprocessing.MinMaxScaler()),\n    ('model', ensemble.AdaBoostClassifier(linear_model.LogisticRegression(C=1, solver='lbfgs', penalty='l2'), n_estimators=1000, learning_rate=0.05))\n])","f0b32e16":"bfull = pipeline.Pipeline(steps=[\n    ('preprocess', preprocessing.FunctionTransformer(prep)),\n    ('normalize', preprocessing.Normalizer()),\n    ('scale', preprocessing.MinMaxScaler()),\n    ('model', naive_bayes.GaussianNB()),\n])","f0c3abfc":"SVCfull = pipeline.Pipeline(steps=[\n    ('preprocess', preprocessing.FunctionTransformer(prep)),\n    ('normalize', preprocessing.Normalizer()),\n    ('scale', preprocessing.MinMaxScaler()),\n    ('model', svm.LinearSVC(C=0.85, penalty='l2', max_iter=4897))\n])","fd5a0a03":"knnfull = pipeline.Pipeline(steps=[\n    ('preprocess', preprocessing.FunctionTransformer(prep)),\n    ('normalize', preprocessing.Normalizer()),\n    ('scale', preprocessing.MinMaxScaler()),\n    ('model', neighbors.KNeighborsClassifier(weights='uniform', n_neighbors=6, algorithm='kd_tree', n_jobs=-1))\n])","1dc5799e":"for col in numerical:\n    X.loc[:, col] = X[col].fillna(X[col].median())\n    test.loc[:, col] = test[col].fillna(test[col].median())\n\nX.loc[:, categorical] = X[categorical].fillna(0)\ntest.loc[:, categorical] = test[categorical].fillna(0)","1badb1cf":"logistic = model_selection.cross_val_score(full, X, y, cv=model_selection.StratifiedKFold(n_splits=5), scoring='roc_auc', n_jobs=-1)\n\nlogacc = model_selection.cross_val_score(full, X, y, cv=model_selection.StratifiedKFold(n_splits=5), scoring='accuracy', n_jobs=-1)","aaaf6f38":"randomforest = model_selection.cross_val_score(rffull, X, y, cv=model_selection.StratifiedKFold(n_splits=5), scoring='roc_auc', n_jobs=-1)\n\nrfacc = model_selection.cross_val_score(rffull, X, y, cv=model_selection.StratifiedKFold(n_splits=5), scoring='accuracy', n_jobs=-1)","78c6d3db":"gradient_boost = model_selection.cross_val_score(gbfull, X, y, cv=model_selection.StratifiedKFold(n_splits=5), scoring='roc_auc', n_jobs=-1)\n\ngbacc = model_selection.cross_val_score(gbfull, X, y, cv=model_selection.StratifiedKFold(n_splits=5), scoring='accuracy', n_jobs=-1)","1eb6459b":"adaboost = model_selection.cross_val_score(adafull, X, y, cv=model_selection.StratifiedKFold(n_splits=5), scoring='roc_auc', n_jobs=-1)\n\nadaacc = model_selection.cross_val_score(adafull, X, y, cv=model_selection.StratifiedKFold(n_splits=5), scoring='accuracy', n_jobs=-1)","f748b3ad":"naiveBayes = model_selection.cross_val_score(bfull, X, y, cv=model_selection.StratifiedKFold(n_splits=5), scoring='roc_auc', n_jobs=-1)\n\nnaiveacc = model_selection.cross_val_score(bfull, X, y, cv=model_selection.StratifiedKFold(n_splits=5), scoring='accuracy', n_jobs=-1)","afb30d27":"svc = model_selection.cross_val_score(SVCfull, X, y, cv=model_selection.StratifiedKFold(n_splits=5), scoring='roc_auc', n_jobs=-1)\n\nsvcacc = model_selection.cross_val_score(SVCfull, X, y, cv=model_selection.StratifiedKFold(n_splits=5), scoring='accuracy', n_jobs=-1)","5526f1c2":"knn = model_selection.cross_val_score(knnfull, X, y, cv=model_selection.StratifiedKFold(n_splits=5), scoring='roc_auc', n_jobs=-1)\n\nknnacc = model_selection.cross_val_score(knnfull, X, y, cv=model_selection.StratifiedKFold(n_splits=5), scoring='accuracy', n_jobs=-1)","694d05e3":"modelscores = pd.DataFrame({\n    'Logistic Regression': logistic,\n    'Random Forest Classifier': randomforest,\n    'Gradient Boosting Classifier': gradient_boost,\n    'Adaboost Classifier': adaboost,\n    'Naive Bayes Classifier': naiveBayes,\n    'Support Vector Classifier': svc,\n    'K-Nearest-Neighbors Classifier': knn\n})\n\nmodelaccuracyscores = pd.DataFrame({\n    'Logistic Regression': logacc,\n    'Random Forest Classifier': rfacc,\n    'Gradient Boosting Classifier': gbacc,\n    'Adaboost Classifier': adaacc,\n    'Naive Bayes Classifier': naiveacc,\n    'Support Vector Classifier': svcacc,\n    'K-Nearest-Neighbors Classifier': knnacc\n})\n\nplt.figure(figsize=(24,13.5))\n\nsns.barplot(data=modelscores)\n\nplt.show()","dac3f1ed":"plt.figure(figsize=(24,13.5))\n\nsns.barplot(data=modelaccuracyscores)\n\nplt.show()","eeb35a7b":"modelscores.mean().sort_values(ascending=False)","d1cecc4e":"modelaccuracyscores.mean().sort_values(ascending=False)","710dd49c":"rffull.fit(X,y)\nfull.fit(X, y)\ngbfull.fit(X, y)\nadafull.fit(X, y)\nbfull.fit(X, y)\nSVCfull.fit(X, y)\nknnfull.fit(X, y)\n\nrfpreds = rffull.predict(test)\nlrpreds = full.predict(test)\ngbpreds = gbfull.predict(test)\nadapreds = adafull.predict(test)\nbayespreds = bfull.predict(test)\nsvcpreds = SVCfull.predict(test)\nknnpreds = knnfull.predict(test)","2f6020cb":"predsdict = {'random-forest': rfpreds, 'logistic-regression': lrpreds, 'gradient_boosting': gbpreds, 'adaboost': adapreds, 'naive-bayes': bayespreds, 'support-vector': svcpreds, 'k-nearest-neighbors': knnpreds}","b79b6725":"for predictions in predsdict.keys():\n    output = pd.DataFrame({\n        'PassengerId': test.index,\n        'Survived': predsdict[predictions]\n    })\n    \n    output.to_csv('{0}.csv'.format(predictions), index=False)","23974a71":"Point score seems to be a fairly good representation of the categorical variables.","1a1bad96":"## Class-wise visualisation","3d80ff01":"# Picking the best model","bf10b435":"We can now read the datasets.","89203cb4":"# Modelling","010e923b":"# Visualisation and Observation","44729eb2":"The chances for survival for passengers increases along with the class, from 3rd class to 1st class.\n\n* Sex\n    * The chances for survival increases from 3rd class to 1st class with both sexes, but significantly lower for men than women.\n    \n* Embarked\n    * Southampton\n        * The chances for survival simply increase along with class.\n    * Cherbourg\n        * The chances for survival also increase in the same way as Southampton.\n    * Quenstown\n        * The chances for survival are highest with 2nd class passengers, then 1st class passengers and lastly 3rd passengers.\n\n* SibSp\n    * 1st Class\n        * 1st class passengers' chances for survival increase with the number of siblings and\/or spouses travelling with them from 0-2, then drops slightly on 3.\n    * 2nd Class\n        * Chances for survival increase with the number of sublings and\/or spouses travelling with them from 0-3, with the exception of 2, which is between 0 and 1.\n    * 3rd Class\n        * Chances for survival increase from 0-2 and drop to around 0.05 with 3 and increases slightly with 4.\n        \n* Parch\n    * 1st Class\n        * Chances for survival increase from 0-2\n    * 2nd Class\n        * Chances for survival increase from 0-3\n    * 3rd Class\n        * Chances for survival are highest at 1, followed by 2, 3, 0 and then 5","57c2be57":"The only passengers with 6 parents\/children travelling with them did not survive.","fa1dcb4a":"These are the results of the plotting according to gender:\n\n* Age\n    * Female\n        * Chances of survival increases with age, suggests that older women were prioritised during the Titanic crisis. \n    * Male\n        * Chances of survival decrease with age, suggests that only male children were prioritised during the Titanic crisis.       \n        \n* Pclass\n    * The chances of survival decrease from 1st class to 3rd class, suggesting that richer people were prioritised. The women still have a much higher chance of survival, with even women in 3rd class having a higher chance of survival than men in 1st class.\n\n* Fare\n    * Female\n        * The chance of survival increases quickly from a high point of around 0.6 with the increase in money paid to board the ship.\n    * Male\n        * The chance of survival also increases quickly but from a low point of around 0.17 with the increase in money paid to board the ship.\n\n* Sex\n    * Women have a higher chance of survival than men.\n\n* Embarked\n    * Female\n        * Passengers embarking at Cherbourg have the highest chances of survival, followed by Queenstown and then Southampton.\n    * Male\n        * Passengers embarking at Cherbourg have the highest chances of survival, followed by Southampton and then Queenstown.","0aa7479b":"## Imputation","80b94513":"The adaboost classifier uses the logistic regression as its base estimator.","bef50faf":"## Gender-wise visualisation","47c342ea":"The number of siblings and\/or spouses a person has affects their chances of survival, highest when the value is from 0-2 and highest for male passengers when it is 1. The survival chances drop after 2. For the number of parents and\/or children, it is highest for female passengers at 0 and highest for male passengers at 1 or 2.","a878caa7":"The dataset has 11 columns excluding the passenger id column which we have used as the index. The survived column will be our target.","4f3269d3":"The survival chances seem to increase with the increase of parents and\/or children boarding with a passenger, with the exception of 2 and 5. The chances of survival for 4 parents and\/or children accompanying a passenger is 0, so let's find out the reason why,","9a331d7f":"* Pclass\n    * Southampton\n        * Those boarding in Southampton had the highest chances of survival if they were 1st class, followed by 2nd class and then 3rd class.\n    * Cherbourg\n        * The pattern with passenger class followed the same pattern with those boarding in Southampton\n    * Queenstown\n        * Those boarding in Queenstown had the highest chances of survival if they were 2nd class, followed by 1st class and then 3rd class.\n        \n* Sex\n    * Generally, passengers had higher chances of survival if they were female, and lower chances if they were men.\n    * Women had the highest chances of survival if they embarked at Southampton, then Queenstown and then Cherbourg.\n    * Men had the highest chances of survival if they embarked at Southampton, then Cherbourg and then Queenstown.\n    \n* Parch\n    * Southampton\n        * Chances of survival are highest at 1, followed by 3, 2, 0 and lastly 5.\n    * Cherbourg\n        * Chances of survival increase with number of parents and\/or children from 0-2\n    * Queenstown\n        * Only chances of survival at 0.\n\n* SibSp\n    * Southampton\n        * Chances of survival increase from 0-1 and decrease from 2-4\n    * Cherbourg\n        * Chances of survival increase from 0-2 \n    * Queenstown\n        * Chances of survival increase from 0-2\n        \n\nThose who embarked in Cherbourg had the highest chances of survival, followed by Queenstown and then Southampton on their own.","34c10a56":"# Stratified K-Fold Cross Validation","dc8bd45e":"It appears that the fewest number of passengers were travelling with 4 or 6 passengers.","2416d107":"## Correlation Visualisation","8a3a3e76":"## Univariate Visualisation","191eea0f":"The names of passengers will probably not play a part in whether they survived or not, so we will drop it later.","bb60bff9":"# Titanic Dataset Classification:","905e74ea":"## Summary of Visualisation:\n\nWomen have a higher chance of surviving than men. Children have a higher chance of surviving than aldults. Senior women also had quite a high chance of survival. Richer passengers were prioritised and have a higher chance of survival - *this can be classified as paying a higher fare and\/or being in a higher class like 1st class.* \n\nThose embarking in Cherbourg had the highest chances of surviving, followed by Southampton and then Queenstown. \n\nSurvival chances generally increase with fare and decrease with age. \n\nPeople embarking in Cherbourg have higher chances of survival if the parch variable is between 0-3. For those embarking in Southampton, their chances of survival were the highest if they travelled with 1, followed by 3, 2, 0 and lastly 5 parents and\/or children. In Cherbourg, the chances of suvival increase with number of parents and\/or children from 0-2. In Queenstown, the only chances of survival were if the parch variable was 0. \n\nFor SibSp, if a passenger embarked in Southampton their chances of survival incrased with the number of siblings and\/or spouses from 0-1 and decreased from 2-4. For both Cherbourg and Queenstown, the chances of survival increased with the number of siblings and\/or spouses from 0-2.\n\n1st class passengers' chances for survival increase with the number of siblings and\/or spouses travelling with them from 0-2 before dropping slightly on 3. 2nd class passengers' chances for survival increase with the number of sublings and\/or spouses travelling with them from 0-3, with the exception of 2, which is between 0 and 1. 3rd class passengers' chances for survival increase from 0-2 and drop to around 0.05 with 3 and increases slightly with 4 (sibsp).\n\n1st class passengers' chances for survival increase from 0-2 parents and\/or children with them. 2nd class passengers' chances for survival increase from 0-3 parents and\/or children with them. 3rd class passengers' chances for survival are highest at 1, followed by 2, 3, 0 and then 5 parents and\/or children with them.\n\nThe number of siblings and\/or spouses a person has affects their chances of survival, highest when the value is from 0-2 and highest for male passengers when it is 1. The survival chances drop after 2. For the number of parents and\/or children, it is highest for female passengers at 0 and highest for male passengers at 1 or 2.","a112e9e1":"First, let's import the relevant modules needed.","67005136":"Those who have 1 sibling and\/or spouse with them have the highest chances of survival, followed by 2, 0, 3 and 4.","fe5e320f":"# Data Preprocessing, Feature Generation and Parameter Tuning","9925aaed":"The only four passengers with 4 parents and\/or children travelling with them all died.","03c1d272":"The heatmap shows that the most strongly correlated pairs of features are: Parch and SibSp (positive), Pclass and Fare (negative). The features that are most correlated to the outcome are Fare and Pclass.","1fb849ba":"Let's visualise the data using seaborn and matplotlib.","2d2c0073":"## Boarding Location visualisation","e90a678d":"* Age\n    * Cherbourg\n        * The chances of survival start at 0.75 and decrease with age.\n    * Southampton\n        * The chances of survival start just above 0.4 and decrease with age.\n    * Queenstown\n        * The chances of survival start between 0.45 and 0.5 and decrease with age.\n\n* Fare\n    * Cherbourg\n        * The chances of survival increase with the fare, reaching nearly 1.\n    * Southampton\n        * The chances of surivical increase with the fare, seemingly at a faster rate than the line for Cherbourg.\n        * The line stops around 270, from which we can conclude that those boarding in Southampton might be 2nd class passengers.\n    * Queenstown\n        * It is unclear whether fare actually affects the chances of survival, as it appears that the line stops before 100, which may suggest that 3rd class passengers boarded in Queenstown, and the short line shows a slight decrease in survival chances.","6390bee1":"The boxplot and kernel density estimate plot show a positively skewed distribution, with more people paying less to board the ship. This may also imply that more of the passengers were 3rd or 2nd class than 1st class.","b0cc23c8":"* Age\n    * 1st class\n        * The chances of survival start very high with a low age at around 0.9 and decrease with the increase of age.\n    * 2nd class\n        * The chances of survival start slightly lower than with 1st class passengers at a low age, and decrease with the increase of age in the same way.\n    * 3rd class\n        * The chances of survival start at around 0.45 with a low age and decrease with the increase in age age, however the rate of decrease is slightly lower than with the higher classes.\n        \n* Fare\n    * 1st class\n        * The chances of survival increase greatly with the fare, showing that 1st class passengers who paid more had a higher chance of survival.\n    * 2nd class\n        * The chances of survival increase at a higher rate with the fare than with 1st class passengers, however the line stops at 100, showing that the 2nd class passengers paid less than 100.\n    * 3rd class\n        * The chances of survival increase extremely slightly with the fare, and the line stops at 100, showing that the 3rd class passengers also paid less than 100.","1af421bc":"61% of the passengers in the train set died, and 38% of the passengers survived, which shows that the target is skewed and we cannot use accuracy as a metric, because if the model predicted that all of the passengers in the test set would die then it would still have an accuracy of 61% - which is not the best but it is still above 50%. Instead we should use a metric like precision, recall or f1 score which is a mixture of the two.","1716a1cd":"The kde plot and boxplot shows that the data is slightly positively skewed, meaning that there are more people under the age of 28 than above the age of 28. We can put them in classes of 10 from 0 to 80."}}