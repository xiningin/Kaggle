{"cell_type":{"16581a29":"code","01db3b04":"code","7a2725b3":"code","bfa4d618":"code","14b232d0":"code","a417149f":"code","6637c6c7":"code","663db626":"code","d34d9a61":"code","3e961205":"code","8c9c6832":"markdown","90e98ca9":"markdown","257b0a6b":"markdown","e752f451":"markdown","f3bab741":"markdown","730134fb":"markdown","7513a975":"markdown","8da9025a":"markdown"},"source":{"16581a29":"!pip install torchbearer","01db3b04":"import os\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport torchvision\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid\nfrom torchvision.datasets import FashionMNIST\n\nimport torchbearer\nimport torchbearer.callbacks as callbacks\nfrom torchbearer import Trial, state_key\n\nMU = state_key('mu')\nLOGVAR = state_key('logvar')","7a2725b3":"class VAE(nn.Module):\n    def __init__(self, latent_size):\n        super(VAE, self).__init__()\n        self.latent_size = latent_size\n\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 32, 4, 1, 2),   # B,  32, 28, 28\n            nn.ReLU(True),\n            nn.Conv2d(32, 32, 4, 2, 1),  # B,  32, 14, 14\n            nn.ReLU(True),\n            nn.Conv2d(32, 64, 4, 2, 1),  # B,  64,  7, 7\n        )\n        \n        self.mu = nn.Linear(64 * 7 * 7, latent_size)\n        self.logvar = nn.Linear(64 * 7 * 7, latent_size)\n        \n        self.upsample = nn.Linear(latent_size, 64 * 7 * 7)\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(64, 32, 4, 2, 1), # B,  64,  14,  14\n            nn.ReLU(True),\n            nn.ConvTranspose2d(32, 32, 4, 2, 1, 1), # B,  32, 28, 28\n            nn.ReLU(True),\n            nn.ConvTranspose2d(32, 1, 4, 1, 2)   # B, 1, 28, 28\n        )\n\n    def reparameterize(self, mu, logvar):\n        if self.training:\n            std = torch.exp(0.5*logvar)\n            eps = torch.randn_like(std)\n            return eps.mul(std).add_(mu)\n        else:\n            return mu\n\n    def forward(self, x, state):\n        image = x\n        x = self.encoder(x).relu().view(x.size(0), -1)\n        \n        mu = self.mu(x)\n        logvar = self.logvar(x)\n        sample = self.reparameterize(mu, logvar)\n        \n        result = self.decoder(self.upsample(sample).relu().view(-1, 64, 7, 7))\n        \n        if state is not None:\n            state[torchbearer.Y_TRUE] = image\n            state[MU] = mu\n            state[LOGVAR] = logvar\n        \n        return result","bfa4d618":"# Hack to make this work with the torchvision dataset\n\nimport codecs\n\ndef get_int(b):\n    return int(codecs.encode(b, 'hex'), 16)\n\ndef read_label_file(path):\n    with open(path, 'rb') as f:\n        data = f.read()\n        assert get_int(data[:4]) == 2049\n        length = get_int(data[4:8])\n        parsed = np.frombuffer(data, dtype=np.uint8, offset=8)\n        return torch.from_numpy(parsed).view(length).long()\n\n\ndef read_image_file(path):\n    with open(path, 'rb') as f:\n        data = f.read()\n        assert get_int(data[:4]) == 2051\n        length = get_int(data[4:8])\n        num_rows = get_int(data[8:12])\n        num_cols = get_int(data[12:16])\n        parsed = np.frombuffer(data, dtype=np.uint8, offset=16)\n        return torch.from_numpy(parsed).view(length, num_rows, num_cols)\n\nroot = '..\/input'\nout = '..\/processed'\nif not os.path.exists(out):\n    os.mkdir(out)\n\ntraining_set = (\n    read_image_file(os.path.join(root, 'train-images-idx3-ubyte')),\n    read_label_file(os.path.join(root, 'train-labels-idx1-ubyte'))\n)\ntest_set = (\n    read_image_file(os.path.join(root, 't10k-images-idx3-ubyte')),\n    read_label_file(os.path.join(root, 't10k-labels-idx1-ubyte'))\n)\nwith open(os.path.join(out, 'training.pt'), 'wb') as f:\n    torch.save(training_set, f)\nwith open(os.path.join(out, 'test.pt'), 'wb') as f:\n    torch.save(test_set, f)","14b232d0":"transform = transforms.Compose([transforms.ToTensor()])  # No augmentation\ntrainset = FashionMNIST(root='..\/', train=True, transform=transform)\ntestset = FashionMNIST(root='..\/', train=False, transform=transform)\ntraingen = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=8)\ntestgen = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=8)","a417149f":"def beta_kl(mu_key, logvar_key, beta=5):\n    @callbacks.add_to_loss\n    def callback(state):\n        mu = state[mu_key]\n        logvar = state[logvar_key]\n        return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) * beta\n    return callback","6637c6c7":"def plot_progress(key=torchbearer.Y_PRED, num_images=100, nrow=10):\n    @callbacks.on_step_validation\n    @callbacks.once_per_epoch\n    def callback(state):\n        images = state[key]\n        image = make_grid(images[:num_images], nrow=nrow, normalize=True)[0, :, :]\n        plt.imshow(image.detach().cpu().numpy(), cmap=\"gray\")\n        plt.show()\n    \n    return callback","663db626":"model = VAE(latent_size=10)\noptimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-4)\ntrial = Trial(model, optimizer, nn.MSELoss(reduction='sum'), metrics=['acc', 'loss'], callbacks=[\n    beta_kl(MU, LOGVAR),\n    callbacks.ConsolePrinter(),\n    plot_progress()\n], verbose=1).with_generators(train_generator=traingen, test_generator=testgen)\ntrial.to('cuda')\ntrial.run(20)\ntrial.evaluate(verbose=0, data_key=torchbearer.TEST_DATA)","d34d9a61":"from random import shuffle\nSAMPLES = state_key('samples')\ndims = [i for i in range(model.latent_size)]\n\ndef draw_space(self, x, state=None):\n    image = x\n    x = self.encoder(x).relu().view(x.size(0), -1)\n    sample = self.mu(x)\n\n    samples = sample.unsqueeze(1).repeat(1, 10, 1)\n    space = torch.linspace(-3, 3, steps=10, device=x.device)\n    \n    for dim in dims:\n        samples[dim, :, dim] = space\n\n    sample = samples.view(-1, samples.size(2))\n    \n    result = self.decoder(self.upsample(sample).relu().view(-1, 64, 7, 7))\n        \n    if state is not None:\n        state[SAMPLES] = result\n    return None\n\nimport types\nmodel.forward = types.MethodType(draw_space, model)  # HACK!","3e961205":"trial = Trial(model, callbacks=[\n    plot_progress(SAMPLES, num_images=100, nrow=10)\n], verbose=1).with_generators(test_generator=testgen)\ntrial.to('cuda')\n_ = trial.for_test_steps(1).evaluate(data_key=torchbearer.TEST_DATA)","8c9c6832":"## KL Term\nThe final thing we need to train the model, is the KL divergence loss, with optional beta term. With torchbearer we can just use the add_to_loss decorator for this:","90e98ca9":"## Final Comments\nSo there we have it, a quick tour of beta-VAEs on FashionMNIST.  Notice that FashionMNIST doesn't disentangle quite as nicely as CelebA, probably because of the large variation in the images. Although, with a bit of hyperparameter tuning this can probably be improved.","257b0a6b":"# Fashion MNIST Beta-VAE\n## Model Definition\nIn this tutorial we will train a simple Beta-VAE on FashionMNIST with PyTorch and [torchbearer](https:\/\/github.com\/ecs-vlc\/torchbearer). Now that we have everything installed and imported, and our state keys defined, we can define our model. This is a simple, three layer CNN similar to the one used in the [Beta-VAE paper](https:\/\/openreview.net\/forum?id=Sy2fzU9gl). 4 by 4 convolutions with a bit of padding help us to easily work out the output sizes for each layer.","e752f451":"So, there you go, after a few epochs the model gives reasonable results. Of course, the next step is to sample from the latent space to see what the model has learned. In beta-VAE they do this by presenting a seed image to the network, and then taking a walk in one of the dimensions to see what property it represents. Let's monkey patch our model with some visualisation code to do this (note, this is either a really ugly hack or just a really pythonic solution, I'll let you decide).","f3bab741":"## Viewing the latent space\nNow, we have changed our model to take walks in the latent space, we can run it to see what happens.","730134fb":"## Training\nNow that we have all of the things we need, we can create a trial instance and train the model:","7513a975":"## Loading the Data\nOk, so now the model will encode the input to obtain a latent space, then sample from that space and decode to produce the output. It will also put anything that we need in state (in this case: the new target, the mean and the logvar). Next, we need some data, there's a small hack hidden here to make the torchvision dataset work with the csv files.","8da9025a":"## Visualisation\nSince this is a VAE, it would be helpful to visualise some of the training progress, so we know how its doing. For that we can write a callback, here using Matplotlib (imported earlietr with inline mode enabled)."}}