{"cell_type":{"093807f2":"code","7a32b5a5":"code","ebd39096":"code","205f5e92":"code","3f63b22f":"code","a1431e2d":"code","dcf2c6fc":"code","a1085b03":"code","249175f5":"code","a10af2cc":"code","e088866e":"code","6947bbbf":"code","5d19c075":"code","b704da06":"code","9eb7cb24":"code","a16dfd70":"code","d1186495":"code","4a986790":"code","92ad6264":"code","6928ee31":"code","fc1263da":"code","249b40fb":"code","2391e059":"code","23b53dae":"code","b936b33c":"code","2c0afc53":"code","6b055d7d":"code","04e6f075":"code","7d1e5fec":"code","1797fa00":"code","a9525bf5":"markdown","75130d1e":"markdown","bdfda7a2":"markdown","1f73f919":"markdown","0c6a6a05":"markdown","8291f2e9":"markdown","ab9cb2f9":"markdown","5a3b613b":"markdown","470fdc2b":"markdown","fa5ffd5e":"markdown","f6efb3d8":"markdown","7bfc2155":"markdown","6e80975c":"markdown","b777fea5":"markdown","67faa637":"markdown","0b792a95":"markdown"},"source":{"093807f2":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import FormatStrFormatter\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7a32b5a5":"train_df = pd.read_csv('\/kaggle\/input\/widsdatathon2022\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/widsdatathon2022\/test.csv')","ebd39096":"display(train_df.shape)\ndisplay(test_df.shape)","205f5e92":"train_df.head()","3f63b22f":"train_df.drop(['id'], axis=1, inplace=True)\ntest_df.drop(['id'], axis=1, inplace=True)","a1431e2d":"cat_features =[]\nnum_features =[]\n\nfor col in train_df.columns:\n    if train_df[col].dtype=='object':\n        cat_features.append(col)\n    else:\n        num_features.append(col)\n\nprint('Categoric features: ', cat_features)\nprint('There are {} categorical features in both train and test datasets'.format(len(cat_features)))\n\nnum_features.remove('site_eui')","dcf2c6fc":"train_df.describe().T.head(10).sort_values(by='std' , ascending = False)\\\n.style.background_gradient()","a1085b03":"def null_value_df(data):    \n    null_values_df = []    \n    for col in data.columns:\n        \n        if data[col].isna().sum() != 0:\n            pct_na = np.round((100 * (data[col].isna().sum())\/len(data)), 2) \n            \n            dict1 ={\n                'Features' : col,\n                'NA (count)': data[col].isna().sum(),\n                'NA (%)': (pct_na)\n            }\n            null_values_df.append(dict1)\n    return pd.DataFrame(null_values_df, index=None).sort_values(by='NA (count)',ascending=False)\n\n\ndf1 = null_value_df(train_df)\ndf2 = null_value_df(test_df)\n\nfig, ax = plt.subplots(1,2, figsize=(16, 5), sharey=True)\nfig.subplots_adjust(top=0.85)\nsns.barplot(data=df1, y= 'Features', x='NA (%)', ax= ax[0], label='train', palette='GnBu_r')\nax[0].set_title('Train')\nsns.barplot(data=df2, y= 'Features', x='NA (%)', ax =ax[1], label='test', palette='GnBu_r')\nax[1].set_title('Test')\nax[1].set_ylabel('')\nplt.suptitle('Features with null_values ', fontsize=20);","249175f5":"fig, ax = plt.subplots(3,2, figsize=(16, 12), sharey=False)\nfig.subplots_adjust(top=0.95)\nsns.countplot(x=train_df['Year_Factor'], label='train', ax = ax[0,0], palette='GnBu_r')\nax[0, 0].legend()\nsns.countplot(x=test_df['Year_Factor'], label='test', ax = ax[0,1], palette='GnBu_r')\nax[0,1].legend();\nax[0,1].set_ylabel('')\nsns.countplot(x=train_df['State_Factor'], label='train', ax = ax[1,0], palette='GnBu_r')\nax[1,0].legend();\nsns.countplot(x=test_df['State_Factor'], label='test', ax = ax[1, 1], palette='GnBu_r')\nax[1,1].legend();\nax[1,1].set_ylabel('')\nsns.countplot(x=train_df['building_class'], label='train', ax = ax[2,0], palette='GnBu_r')\nax[2,0].legend();\nsns.countplot(x=test_df['building_class'], label='test', ax = ax[2,1],palette='GnBu_r')\nax[2,1].legend();\nax[2,1].set_ylabel('')\nplt.suptitle('Categorical Features in the train and test datasets ', fontsize=20);","a10af2cc":"fig, ax = plt.subplots(1,2, figsize=(12, 20), sharey=True)\nfig.subplots_adjust(top=0.95)\nsns.countplot(y=train_df['facility_type'], label='train', ax = ax[0], palette='GnBu_r')\nsns.countplot(y=test_df['facility_type'], label='test', ax = ax[1], palette='GnBu_r')\nax[1].legend()\nax[1].set_ylabel('')\nplt.suptitle(\"Categories in the 'facility_type' features\", fontsize=20);","e088866e":"cols_with_na= ['days_with_fog', 'direction_max_wind_speed', 'direction_peak_wind_speed', 'max_wind_speed', 'energy_star_rating', 'year_built']\n\ndef density_plot(train, test, features, title):    \n    \n    L = len(features)\n    ncol= 5\n    nrow= int(np.ceil(L\/ncol))\n    remove_last= (nrow * ncol) - L\n    \n    fig, ax = plt.subplots(nrow, ncol,figsize=(24, 32), sharey=False, facecolor='#dddddd')\n    ax.flat[-remove_last].set_visible(False)\n    fig.subplots_adjust(top=0.95)\n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)\n        ax = sns.kdeplot(train[feature], shade=False,  color='#6495ED',  alpha=0.85, label='train')\n        ax = sns.kdeplot(test[feature], shade=False, color='#ff355d',  alpha=0.85, label='test')\n        ax.yaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n        ax.xaxis.set_label_position('top')\n        ax.set_ylabel('')\n        ax.set_yticks([])        \n        ax.set_xticks([])\n        \n        if feature in cols_with_na:\n                  \n            ax.set_facecolor('lightgray')\n                \n        i += 1\n\n    lines, labels = fig.axes[-1].get_legend_handles_labels()    \n    fig.legend(lines, labels, loc = 'upper center',borderaxespad= 4.0)\n\n    plt.suptitle(title, fontsize=20)\n    plt.show()\n\ndensity_plot(train_df, test_df, num_features, title='Numerical Features Distribution (train_test data)');","6947bbbf":"facility_type = train_df.pop('facility_type')\n\nfeatures = train_df.columns\ndef mixed_subplots(data, features, titleText='Title', ncol=4):\n    i = 1    \n    L = len(features)\n    nrow= int(np.ceil(L\/ncol))        \n    k = (nrow * ncol) - L\n    \n    fig, ax = plt.subplots(nrow, ncol, figsize=(24, 50), sharey=False)\n    \n    # this removes the last k (empty subplots)\n    for j in range(1, k+1):\n        ax.flat[-j].set_visible(False)\n    #\n\n    fig.subplots_adjust(top=0.96) \n    \n    for feature in features:\n        plt.subplot(nrow, ncol, i)\n        \n        if data[feature].nunique() <= 10:\n            ax = sns.boxplot(x=feature, y='site_eui', data= data, color='gold')\n                    \n        elif data[feature].nunique() < 20:\n            ax = sns.barplot(x=feature, y='site_eui', data= data, color='salmon')\n                       \n        else:\n            ax = sns.scatterplot(data=data, x=feature, y='site_eui', color='lightseagreen')\n         \n        if feature in cols_with_na:\n                  \n            ax.set_facecolor('lightgray')\n                    \n        plt.xlabel(feature, fontsize=10)\n        plt.grid()\n        i += 1\n    plt.suptitle(titleText, fontsize=20)    \n    plt.show() \n\ntrain_df = pd.concat([train_df, facility_type], axis=1)","5d19c075":"titleText = \"Correlation of features with target variable (site_eui)\"\nmixed_subplots(train_df, features[:-1], titleText, ncol=4)","b704da06":"fig, ax = plt.subplots(figsize=(12, 20), sharey=True)\nfig.subplots_adjust(top=0.95)\nsns.boxplot(y=facility_type, x=train_df['site_eui'], color='#1ABC9C')\nplt.suptitle(\"Correlation: Facility_Type vs Site_eui\", fontsize=20);","9eb7cb24":"from lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.impute import SimpleImputer\nimport gc\n\nfold = 5\nSEED = 42\nEarlyStopping = 100","a16dfd70":"y = train_df.pop('site_eui')","d1186495":"#### Scaling features (num_features)\n# scaler = StandardScaler()\n# train_df[num_features] = scaler.fit_transform(train_df[num_features])\n# test_df[num_features] = scaler.fit(test_df[num_features])\n# test_df[num_features] = scaler.transform(test_df[num_features])","4a986790":"# reminder what the  category features are\ncat_features","92ad6264":"cols_to_keep = [col for col in cat_features if set(train_df[col]) == set(test_df[col])]      \ncols_to_drop = list(set(cat_features) - set(cols_to_keep))\n        \nprint('Categorical columns that will be label encoded:', cols_to_keep)\nprint('\\nCategorical columns that will be dropped from the dataset:', cols_to_drop)","6928ee31":"le = LabelEncoder()\ndef LE(train_df, test_df):\n    for col in cols_to_keep:\n        train_df[col] = le.fit_transform(train_df[col])\n        test_df[col] = le.transform(test_df[col])\n    return train_df, test_df","fc1263da":"train_df.drop(['State_Factor'], axis=1, inplace=True)\ntest_df.drop(['State_Factor'], axis=1, inplace=True)","249b40fb":"train_df, test_df = LE(train_df, test_df)","2391e059":"imputer = SimpleImputer()\nimputed_train_df = pd.DataFrame(imputer.fit_transform(train_df))\nimputed_test_df = pd.DataFrame(imputer.transform(test_df))\n\nimputed_train_df.columns = train_df.columns\nimputed_test_df.columns = test_df.columns","23b53dae":"train_df = imputed_train_df\ntest_df = imputed_test_df\ntrain_df.head()","b936b33c":"gc.enable()\n\nkfold = KFold(n_splits=fold, shuffle=True, random_state=SEED)\noof_preds = np.zeros(train_df.shape[0])\nsubm_preds_lgbm = np.zeros(test_df.shape[0])\n\nfeatures = train_df.columns\n\nfor n_fold, (trn_idx, val_idx) in enumerate(kfold.split(train_df)):\n    trn_x, trn_y = train_df[features].iloc[trn_idx], y.iloc[trn_idx]\n    val_x, val_y = train_df[features].iloc[val_idx], y.iloc[val_idx]\n    \n    lgbm = LGBMRegressor(\n#               num_iterations = 15000,\n#               objective = \"regression\", \n#               learning_rate = 0.01,\n#               max_depth = 25,\n#               num_leaves = 124,\n              device_type = 'gpu',\n    )\n    \n    lgbm.fit(trn_x, trn_y,\n           eval_set =[(trn_x, trn_y), (val_x, val_y)],\n           eval_metric=\"rmse\", verbose=0, early_stopping_rounds = EarlyStopping\n           )\n    oof_preds[val_idx] = lgbm.predict(val_x, num_iterations=lgbm.best_iteration_)\n    subm_preds_lgbm += lgbm.predict(test_df[features], num_iteration=lgbm.best_iteration_)\/kfold.n_splits \n    \n    print('Fold {} RMSE : {:.6f}'.format(n_fold + 1, mean_squared_error(val_y, oof_preds[val_idx], squared=False)))\n    \n    del lgbm, trn_x, trn_y, val_x, val_y\n    gc.collect()    ","2c0afc53":"print(\"\\n\")\nprint('LGBM {} fold average RMSE score is: {:.6f}'.format(fold, (mean_squared_error(y, oof_preds, squared=False))))","6b055d7d":"gc.enable()\n\nkfold = KFold(n_splits=fold, shuffle=True, random_state=SEED)\noof_preds = np.zeros(train_df.shape[0])\nsubm_preds_catb = np.zeros(test_df.shape[0])\n\nfeatures = train_df.columns\n\nfor n_fold, (trn_idx, val_idx) in enumerate(kfold.split(train_df)):\n    trn_x, trn_y = train_df[features].iloc[trn_idx], y.iloc[trn_idx]\n    val_x, val_y = train_df[features].iloc[val_idx], y.iloc[val_idx]\n    \n    catb = CatBoostRegressor(\n#         loss_function='RMSE',\n#         max_depth = 7, \n#         learning_rate = 0.01,\n#         iterations = 15000,\n        devices = 'gpu',              \n    )\n    \n    catb.fit(trn_x, trn_y,\n           eval_set =[(trn_x, trn_y)], verbose=1000, \n           early_stopping_rounds = EarlyStopping\n           )\n    oof_preds[val_idx] = catb.predict(val_x)\n    subm_preds_catb += catb.predict(test_df[features])\/kfold.n_splits \n    \n    print('Fold {} RMSE : {:.6f}'.format(n_fold + 1, mean_squared_error(val_y, oof_preds[val_idx], squared=False)))\n    \n    del catb, trn_x, trn_y, val_x, val_y\n    gc.collect()","04e6f075":"print(\"\\n\")\nprint('Catboost {} fold average RMSE score is: {:.6f}'.format(fold, (mean_squared_error(y, oof_preds, squared=False))))","7d1e5fec":"subm_df = pd.read_csv('\/kaggle\/input\/widsdatathon2022\/sample_solution.csv')\nsubm_df['site_eui']= subm_preds_catb\nsubm_df.head()","1797fa00":"subm_df.to_csv(\"submission.csv\", index=False)","a9525bf5":"### Data Overview\n* Train data_set has 75757 rows and 64 columns\n* Test dataset has 9705 rows and 63 columns\n* Only features ``State_Factor``, ``building_class`` and ``facility_type`` are categoricals (dtype =='object'). The rest is numerical.","75130d1e":"### Target column","bdfda7a2":"### Base model (LGBM)","1f73f919":"### Categorical Features continued (Facility type)","0c6a6a05":"### Missing Values\n- Six columns in both train and test datasets have missing values\n- Test dataset have more missing values (%) than train dataset\n- Columns ``days_with_fog``, ``direction_max_wind_speed``, ``direction_peak_wind_speed``, and ``max_wind_speed`` have more than 50% missing values ","8291f2e9":"### Set-up","ab9cb2f9":"### Correlations: Features vs Target plots\n(Gray facecolored plots are columns with NA values)\n- Floor area, day_above_90F, day_above_100F seem to be inversly related to site_eui","5a3b613b":"### Numerical Features (distribution)\n(Gray facecolored plots are columns with NA values)\n- Wind speed data has the largest disparity between train and test dataset distribution. These columns are also among thos which have NA's.","470fdc2b":"### Submission\n- Choose one model or blend and submmit","fa5ffd5e":"### Label Encoding (cat features)","f6efb3d8":"### Base model (Catboost)","7bfc2155":"## WiDS 2022 : Building Energy Efficiency\n### Background\n\"Climate change is a globally relevant, urgent, and multi-faceted issue heavily impacted by energy policy and infrastructure. Addressing climate change involves mitigation (i.e. mitigating greenhouse gas emissions) and adaptation (i.e. preparing for unavoidable consequences). Mitigation of GHG emissions requires changes to electricity systems, transportation, buildings, industry, and land use.\n\nAccording to a report issued by the International Energy Agency (IEA), the lifecycle of buildings from construction to demolition were responsible for 37% of global energy-related and process-related CO2 emissions in 2020. Yet it is possible to drastically reduce the energy consumption of buildings by a combination of easy-to-implement fixes and state-of-the-art strategies. For example, retrofitted buildings can reduce heating and cooling energy requirements by 50-90 percent. Many of these energy efficiency measures also result in overall cost savings and yield other benefits, such as cleaner air for occupants. This potential can be achieved while maintaining the services that buildings provide.\" Source: [Competition Page](https:\/\/www.kaggle.com\/c\/widsdatathon2022\/overview)\n\n**The Task** : The WiDS Datathon 2022 focuses on a prediction task involving roughly 100k observations of building energy usage records collected over 7 years and a number of states within the United States. The dataset consists of building characteristics (e.g. floor area, facility type etc), weather data for the location of the building (e.g. annual average temperature, annual total precipitation etc) as well as the energy usage for the building and the given year, measured as Site Energy Usage Intensity (Site EUI). Each row in the data corresponds to the a single building observed in a given year. Your task is to predict the Site EUI for each row, given the characteristics of the building and the weather data for the location of the building.\n\n**Evaluation Metric**: The evaluation metric for this competition is Root Mean Squared Error (RMSE). The RMSE is commonly used measure of the differences between predicted values provided by a model and the actual observed values.","6e80975c":"### Modeling\n- Impute missing values\n- Label Encoding categorical features\n- Base model ``LBGMRegressor()`` and ``Catboost`` \n- Create submission file","b777fea5":"### Correlations: Features vs Target plots (continued)\n- On average, **grocery_store\/food_market**, **data_centers**, and **laboratory** have higher site_eui (target values)","67faa637":"### Missing value imputation","0b792a95":"### Categorical Features\n- Year_factor : train data has 6 categories where as test data has only one (year_factot 7).\n- State_factor : train data has 7 categories where as test data has 6. State_6 is only present in train data.\n- Building_class : more commercial buildings in test data where as more residential buildings are present in train dataset.\n- The feature facility_type is also not very similar in test and train datasets (unhide the second hidden cell below to see plot)"}}