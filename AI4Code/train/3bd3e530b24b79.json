{"cell_type":{"65e2d3f4":"code","03131202":"code","f710e5d1":"code","dad76200":"code","8d7ce168":"code","c83db99c":"code","6b1d0efe":"code","47cb1107":"code","a7375853":"code","5593e63e":"code","26824730":"code","dda915e6":"code","a97ea858":"code","14978a05":"code","7aa74a3b":"code","8b3b2e11":"code","8c65b720":"code","5b9ecef5":"code","b2a063c6":"code","005a9922":"code","dd589deb":"code","3d41c3f4":"code","82de3fa5":"code","6f6a465b":"code","0ca101b6":"code","331dd120":"code","ec3f4126":"code","1cf0d057":"code","64c66772":"code","ea5797f6":"code","18b65f9c":"code","f2d12ab4":"code","eaf960b5":"code","de42855a":"code","2fd6df71":"code","345ca515":"code","01811081":"code","b208b079":"code","ceacac1c":"code","bea93f1c":"code","1a21759f":"code","6ca87d46":"code","d1c9be25":"code","8d24df4a":"code","f024944d":"code","6b956a8d":"code","7425542b":"code","636c8c1f":"code","f4193d6f":"code","db4f33c0":"code","d3cbb4ad":"code","31bed603":"code","f7b2ad1d":"code","e998a45f":"code","68dae9e7":"code","2d160f32":"code","72cbb8c9":"code","731cf4bc":"code","6b6b1b19":"code","987861d5":"code","0350fea6":"code","2aa6c37a":"code","af2004eb":"code","4590416f":"code","cca86bd0":"code","06d8f09c":"code","73e67a98":"code","e632aeae":"code","281da33c":"code","c9139fc4":"code","c465bfe2":"code","7137264e":"code","80555539":"code","a875b2f0":"code","9bf2b99f":"code","96854c68":"code","af90d4ee":"code","a5546595":"code","966f9f8f":"code","17c09cf8":"code","03cf9134":"code","6bf36957":"code","221173bb":"code","413018f2":"code","82e462c5":"code","825730e1":"code","016fd771":"code","b10cbe67":"code","4c713750":"code","34996848":"code","d928524f":"code","66a47c19":"code","1c400970":"code","96abe65a":"code","f7abf085":"code","ef35ae46":"code","9b082e88":"code","cb2603e0":"code","e6471da2":"code","cf6f6c86":"code","ffe54358":"code","7d1b1ffe":"code","9f01dce1":"code","bca7266f":"code","82a8a5e7":"code","2cd2c623":"code","bfd96be4":"code","9a363413":"code","63bab132":"code","01f4eb4b":"code","92c33742":"code","a610cd9e":"code","d2b10135":"markdown","f4c4dca2":"markdown","255c8bc3":"markdown","21cb75d6":"markdown","7f68b782":"markdown","faca54b4":"markdown","0c6d180f":"markdown","c5d2b5d0":"markdown","06d2523a":"markdown","a3121c7e":"markdown","406186d5":"markdown","3f5bdcf5":"markdown","8741dca3":"markdown","cca915a4":"markdown","fe27ef6f":"markdown","3815a11d":"markdown","dda55e07":"markdown","a40320cf":"markdown","5f691a16":"markdown","fb7a4f62":"markdown","1d6c6c42":"markdown","bab7e9d0":"markdown","cc9dffc6":"markdown","7638c3f1":"markdown","6aa267a5":"markdown","01fb8fd2":"markdown","d1c81bf5":"markdown","701117e1":"markdown","bdeb7127":"markdown","2cd5c62f":"markdown","b42ab086":"markdown","c6479aee":"markdown","7279be93":"markdown"},"source":{"65e2d3f4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","03131202":"train_df = pd.read_csv(\"..\/input\/titanic\/train.csv\")","f710e5d1":"numeric_data = train_df.iloc[:, [0,1,2,5,6,7,9]]\nnumeric_data.head()","dad76200":"category_data = train_df.iloc[:, [3,4,8,10,11]]\ncategory_data.head()","8d7ce168":"sns.factorplot(\"Sex\", data = train_df, kind = \"count\")","c83db99c":"sns.factorplot(\"Sex\", kind = \"count\", data = train_df, hue = \"Survived\")","6b1d0efe":"sns.factorplot(\"Pclass\", data = train_df, kind = \"count\")","47cb1107":"sns.factorplot(\"Pclass\", data = train_df, hue = \"Sex\", kind = \"count\")","a7375853":"def titanic_children(passenger):\n    age, sex = passenger\n    if age < 16:\n        return \"child\"\n    else:\n        return sex\ntrain_df[\"person\"] = train_df[[\"Age\",\"Sex\"]].apply(titanic_children, axis = 1)","5593e63e":"sns.factorplot(\"Pclass\", data = train_df, hue = \"person\", kind = \"count\")","26824730":"train_df[\"Age\"].hist(bins = 70)","dda915e6":"as_fig = sns.FacetGrid(train_df, hue=\"Sex\", aspect = 5)\nas_fig.map(sns.kdeplot, \"Age\", shade = True)\noldest = train_df[\"Age\"].max()\nas_fig.set(xlim = (0, oldest))\nas_fig.add_legend()","a97ea858":"as_fig = sns.FacetGrid(train_df, hue=\"person\", aspect = 5)\nas_fig.map(sns.kdeplot, \"Age\", shade = True)\noldest = train_df[\"Age\"].max()\nas_fig.set(xlim = (0, oldest))\nas_fig.add_legend()","14978a05":"as_fig = sns.FacetGrid(train_df, hue=\"Pclass\", aspect = 5)\nas_fig.map(sns.kdeplot, \"Age\", shade = True)\noldest = train_df[\"Age\"].max()\nas_fig.set(xlim = (0, oldest))\nas_fig.add_legend()","7aa74a3b":"train_df[\"Embarked\"] = train_df[\"Embarked\"].fillna('S')\nsns.factorplot(\"Embarked\", data = train_df, kind = 'count')","8b3b2e11":"sns.factorplot(\"Embarked\", data = train_df, hue = \"Pclass\", kind = \"count\")","8c65b720":"sns.factorplot(\"Survived\", data = train_df, kind = \"count\", hue =\"Pclass\")","5b9ecef5":"sns.factorplot(\"Pclass\",\"Survived\", data = train_df, hue = \"person\")","b2a063c6":"sns.lmplot(\"Age\", \"Survived\", data = train_df)","005a9922":"sns.lmplot(\"Age\",\"Survived\", data = train_df, hue = \"Pclass\")","dd589deb":"sns.lmplot(\"Age\",\"Survived\", data = train_df, hue = \"Sex\")","3d41c3f4":"sns.lmplot(\"Age\",\"Survived\", data = train_df, hue = \"Embarked\")","82de3fa5":"# Import modules\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport re\nfrom sklearn import tree\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom ycimpute.imputer import knnimput\n\n# Figures inline and set visualization style\n%matplotlib inline\nsns.set()\n\n\n# Import data\ntrain_df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/titanic\/test.csv\")","6f6a465b":"train_df.columns","0ca101b6":"train_df.info()","331dd120":"train_df.describe().T","ec3f4126":"train_df[\"Survived\"].value_counts()","1cf0d057":"train_df[\"Pclass\"].value_counts()","64c66772":"train_df[\"Age\"].value_counts()","ea5797f6":"train_df[\"SibSp\"].value_counts()","18b65f9c":"train_df[\"Parch\"].value_counts()","f2d12ab4":"train_df[\"Fare\"].value_counts()","eaf960b5":"df_fare = train_df[\"Fare\"]\ndf_fare.head()","de42855a":"sns.boxplot(x = df_fare);","2fd6df71":"Q1 = df_fare.quantile(0.25)\nQ3 = df_fare.quantile(0.75)\nIQR = Q3 - Q1","345ca515":"Q1","01811081":"Q3","b208b079":"IQR","ceacac1c":"lower_limit = Q1 - 1.5*IQR\nupper_limit = Q3 + 1.5*IQR","bea93f1c":"lower_limit","1a21759f":"upper_limit","6ca87d46":"(df_fare < lower_limit) | (df_fare > upper_limit)","d1c9be25":"outlier_tf = (df_fare > upper_limit)\noutlier_tf.head()","8d24df4a":"df_fare[outlier_tf]","f024944d":"df_fare[outlier_tf].index","6b956a8d":"type(df_fare)","7425542b":"df_fare = pd.DataFrame(df_fare)","636c8c1f":"df_fare.shape","f4193d6f":"clear_tf = df_fare[~((df_fare < (lower_limit)) | (df_fare > (upper_limit)).any(axis=1))]","db4f33c0":"clear_tf.shape\n\n# 116 observations flew. When we evaluate our observation in two ways.","d3cbb4ad":"df_fare = train_df[\"Fare\"]","31bed603":"lower_limit","f7b2ad1d":"df_fare[outlier_tf]","e998a45f":"upper_limit","68dae9e7":"df_fare[outlier_tf] = upper_limit","2d160f32":"df_fare[outlier_tf] \n\n# I suppressed the values above the upper limit value to the upper limit value, that is, we have suppressed the values above our threshold value according to our threshold value.","72cbb8c9":"train_df = train_df.select_dtypes(include = [\"float64\", \"int64\"])\ndf = train_df.copy()\ndf= df.dropna()\ndf.head()","731cf4bc":"clf = LocalOutlierFactor(n_neighbors = 20, contamination = 0.1)\n\n# n_neighbors: refers to the neighborhood number.\n#contamination: an argument expressing density","6b6b1b19":"clf.fit_predict(df)\n\n# The clf object contains the formal properties of lOFT.\n# We perform algorithm execution.","987861d5":"df_scores = clf.negative_outlier_factor_","0350fea6":"df_scores[0:10]\n\n# the score of each observation unit we have, i.e. the density score or LOF score","2aa6c37a":"np.sort(df_scores)[0:20]\n\n# We sorted the df scores with the sort function","af2004eb":"threshold_value = np.sort(df_scores)[9]","4590416f":"outlier_tf = df_scores > threshold_value\n\n# I saved those above the threshold as outlier_tf","cca86bd0":"outlier_tf","06d8f09c":"new_df = df[df_scores > threshold_value]\n\n# We took all that fulfill this condition that is above the threshold.\n# that means accessing non-outlier values and deleting outliers.","73e67a98":"new_df","e632aeae":"df[df_scores < threshold_value]\n\n# When I reverse it, we see contrary observations.","281da33c":"df[df_scores == threshold_value]","c9139fc4":"print_worth = df[df_scores == threshold_value]\n\n# I saved it as the hang value;\n# Our aim is to perform the filling process according to this observation unit.","c465bfe2":"outliers = df[~outlier_tf]\n\n# I'm also recording the outliers","7137264e":"outliers","80555539":"res = outliers.to_records(index = False)","a875b2f0":"res[:] = print_worth.to_records(index = False)","9bf2b99f":"res","96854c68":"df[~outlier_tf]","af90d4ee":"df[~outlier_tf] = pd.DataFrame(res, index = df[~outlier_tf].index)","a5546595":"df[~outlier_tf]","966f9f8f":"from sklearn.impute import KNNImputer\ntrain_df = train_df.select_dtypes(include = [\"float64\",\"int64\"])\ntrain_df.head(20)","17c09cf8":"!pip install ycimpute","03cf9134":"var_names = list(train_df)\n\n# to keep the names of the variables I keep the names of the dataframe somewhere.","6bf36957":"n_df = np.array(train_df)\n\n# I'm creating a new array","221173bb":"n_df[0:10]","413018f2":"n_df.shape\n\n# number of observations and variables","82e462c5":"dff = knnimput.KNN(k=4).complete(n_df)\n\n# neighborhood number = 4, complete: means fill\n# He filled the observations he saw missing","825730e1":"type(dff)","016fd771":"dff = pd.DataFrame(dff, columns = var_names)\n\n# Convert to pandas dataframe...","b10cbe67":"type(dff)","4c713750":"train_df = train_df.select_dtypes(include = [\"float64\",\"int64\"])\ntrain_df.head(20)","34996848":"train_df.isnull().sum()","d928524f":"var_names = list(train_df)","66a47c19":"n_df = np.array(train_df)","1c400970":"from ycimpute.imputer import EM","96abe65a":"dff = EM().complete(n_df)","f7abf085":"dff = pd.DataFrame(dff, columns = var_names)","ef35ae46":"dff.isnull().sum()","9b082e88":"train_df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/titanic\/test.csv\")","cb2603e0":"survived_train = train_df.Survived # store target variable of training data in a safe place","e6471da2":"data = pd.concat([train_df.drop([\"Survived\"], axis = 1), test_df]) #concatenate training and test sets","cf6f6c86":"data.head(20)","ffe54358":"data.info()","7d1b1ffe":"data[\"Age\"] = data.Age.fillna(data.Age.median())\ndata[\"Fare\"] = data.Fare.fillna(data.Fare.median())\n# Impute missing numerical variables","9f01dce1":"data.info()\n# Check out info of data","bca7266f":"data = pd.get_dummies(data, columns = [\"Sex\"], drop_first = True)\ndata.head(20)","82a8a5e7":"# Select columns and view head\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp']]\ndata.head()","2cd2c623":"data.info()","bfd96be4":"data_train = data.iloc[:891]\ndata_test = data.iloc[891:]","9a363413":"X = data_train.values\ntest = data_test.values\ny = survived_train.values","63bab132":"# Instantiate model and fit to data\nclf = tree.DecisionTreeClassifier(max_depth=3)\nclf.fit(X, y)","01f4eb4b":"# Make predictions and store in 'Survived' column of df_test\nY_pred = clf.predict(test)\ntest_df['Survived'] = Y_pred","92c33742":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)","a610cd9e":"# Setup arrays to store train and test accuracies\ndep = np.arange(1, 9)\ntrain_accuracy = np.empty(len(dep))\ntest_accuracy = np.empty(len(dep))\n\n# Loop over different values of k\nfor i, k in enumerate(dep):\n    # Setup a Decision Tree Classifier\n    clf = tree.DecisionTreeClassifier(max_depth=k)\n\n    # Fit the classifier to the training data\n    clf.fit(X_train, y_train)\n\n    #Compute accuracy on the training set\n    train_accuracy[i] = clf.score(X_train, y_train)\n\n    #Compute accuracy on the testing set\n    test_accuracy[i] = clf.score(X_test, y_test)\n\n# Generate plot\nplt.title('clf: Varying depth of tree')\nplt.plot(dep, test_accuracy, label = 'Testing Accuracy')\nplt.plot(dep, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.xlabel('Depth of tree')\nplt.ylabel('Accuracy')\nplt.show()","d2b10135":"# footnote: even if we filled in the missing values with predictive methods in this way; Is there a structural problem before or is there a randomness problem? Are deficiencies in some variables dependent on other variables? Using these methods after examining and establishing the connection will give healthier results.","f4c4dca2":"# DATA V\u0130SUAL\u0130ZAT\u0130ON AND PREPROCESS\u0130NG","255c8bc3":"> It was the threshold value we set above.\nHow we determined the threshold value We created a density-based score that we call the LOF score.\nWe saved it as df_scores and from there I set the 9th index as the threshold value.\nWe observe its equivalent as a numerical value above now.","21cb75d6":"# OUTL\u0130ER TREATMENT","7f68b782":"> >Let's break down our original training data into training and test sets ","faca54b4":"> By visualizing the number of survival factor, we can examine survival \/ non-survival status by gender. Again, we can do it using factorplot () and adding another parameter with hue as above.","0c6d180f":"# SUPPRESS\u0130ON","c5d2b5d0":"> We need to extract more features, considering that we will create new columns containing hidden information. For example, when we need to calculate the number of children on board, we can subtract it from the Age column with some associations from the Sex column. and we will save it in the Person column like the above code.","06d2523a":"# EM","a3121c7e":"> Now let's see the Sex number over Pclass. We will do the same thing we did before. In the first and second grades, the number of males is almost equal to the number of females, but in the third grade the number of males almost doubles.","406186d5":"****As we increase the maximum depth, we will fit better and better with the exercise data because we will make decisions that define the exercise data. The accuracy of the exercise data will and will increase, but we see that this is not for test data: we are overly sleeping","3f5bdcf5":"# Machine Learning Model","8741dca3":"# DATA PREPROCESS\u0130NG","cca915a4":"> As we can see, the number of children in third grade is huge compared to first and second grade. But the number of men is almost the same. In this case, we can look at what the total number of people per year is.","fe27ef6f":"> *There are 2 numerical variables that have missing values.","3815a11d":"# Decision Tree Classifier\n\n>  It is a tree that allows you to classify data points, which are also known as target variables, based on feature variables.","dda55e07":"> We can do this with the hist () function, which calculates the histogram of the age, as seen in the code snippet above. It is simply counting the frequency of the variable in range.\n\n> As we can see, the frequency of people on board between 16 and 35 years old is greater than the age or child above.","a40320cf":"# LOCAL OUTL\u0130ER FACTOR (LOF)","5f691a16":"[](http:\/\/)# T\u0130TAN\u0130C\n![](http:\/\/static3.thetravelimages.com\/wordpress\/wp-content\/uploads\/2018\/11\/titanic1-e1542497861799.jpg)\n\n\n\n# 1- History\n\n# **The RMS Titanic, a luxury steamship, sank in the early hours of April 15, 1912, off the coast of Newfoundland in the North Atlantic after sideswiping an iceberg during its maiden voyage. Of the 2,240 passengers and crew on board, more than 1,500 lost their lives in the disaster.**\n\n\n\n\n# If you are using the kaggle first time\n\n# This github link is for [https:\/\/github.com\/nurrturkaslan](http:\/\/)\n\n# In this github link I explain everything I did in general when I first started Data Science.\n\n\n# 2- Exploring the data\n\n# The files we read in the previous screen are available on the data page for the Titanic competition on Kaggle. That page also has a data dictionary, which explains the various columns that make up the data set. Below are the descriptions contained in that data dictionary:\n\n# PassengerID - A column added by Kaggle to identify each row and make submissions easier\n# Survived - Whether the passenger survived or not and the value we are predicting (0=No, 1=Yes)\n# Pclass - The class of the ticket the passenger purchased (1=1st, 2=2nd, 3=3rd)\n# Sex - The passenger's sex\n# Age - The passenger's age in years\n# SibSp - The number of siblings or spouses the passenger had aboard the Titanic\n# Parch - The number of parents or children the passenger had aboard the Titanic\n# Ticket - The passenger's ticket number\n# Fare - The fare the passenger paid\n# Cabin - The passenger's cabin number\n# Embarked - The port where the passenger embarked (C=Cherbourg, Q=Queenstown, S=Southampton)","fb7a4f62":"> We can see above that the number of males is almost twice the number of females, but everyone knows that the number of surviving females is higher than the number of surviving males.","1d6c6c42":"> A column represents the class reserved for each passenger. For example; 1 = First class 2 or 3 When we look at the graph, we can see that almost half of the passenger is in the third class. I think the most logical All Passengers are in third class, always in an expensive transport","bab7e9d0":"> How to create our machine learning model with a decision tree classifier with the Python scikit-learn package","cc9dffc6":"# SUPPRESS\u0130ON","7638c3f1":"> Now let's go one step further and count the frequency of men \/ women per age. We do this by stacking multiple figures together, creating something called the FacetGrid. This FacetGrid, which consists of two tables each, is the kdeplot type representing man and woman.","6aa267a5":"> 'Sex_male' column; this column has 1 if this row is a male and 0 if this row is female.","01fb8fd2":"# KNN","d1c81bf5":"# Note: While applying the box plot method, when we observed the outlier_tf up and down compared to the mouse variable, we had too many observations, but we reached a less limited number of observations in LOF. In fact, it would be more correct if I said we applied a deletion method.","701117e1":"> > Now, let's iterate over the maximum depth values ranging from 1 to 9 and plot the accuracy of the models on training and test sets.","bdeb7127":"> > Why Choose max_depth=3 ?\n> > The depth of the tree is known as a hyperparameter, which means a parameter you need to decide before you fit the model to the data. If you choose a larger max_depth, you'll get a more complex decision boundary.\n\n* If your decision boundary is too complex, you can overfit to the data, which means that your model will be describing noise as well as signal.\n\n* If your max_depth is too small, you might be underfitting the data, meaning that your model doesn't contain enough of the signal.","2cd5c62f":"# It allows us to define values that may be outliers by scoring the observations based on density at their location. The local density of these points is compared with the neighbors of these points. If it is significantly lower than the density of its neighbors, it can be interpreted that this point is located in a less frequent area than the neighbors. Therefore, there is a neighborhood structure here and if the environment of a value is not intense, it means that this value is evaluated as an outlier. In short, we can say that it scores for each observation value.","b42ab086":"# F\u0130LL\u0130NG \u0130N M\u0130SS\u0130NG OBSERVAT\u0130ONS W\u0130TH MACH\u0130NE LEARN\u0130NG ALGOR\u0130THMS","c6479aee":"> It may be the most logical approach in LOF to perform the assignment according to the values in the index of the threshold value we have determined.","7279be93":"> The goal is to replace outliers with print values and I'll do that.\n\nNow, I will take action to solve the index problem in the outliers section above. After converting the outliers dataframe I named to an array without an index, we will arrange the print value, ie print_worth, and replace the last outliers with print_worth."}}