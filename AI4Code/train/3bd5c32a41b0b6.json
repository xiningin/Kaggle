{"cell_type":{"35f5dc93":"code","a2694c07":"code","3264dd21":"code","59225c88":"code","daa75eeb":"code","30c6c1b5":"code","42739cb7":"code","95cd27b1":"code","db540fda":"code","9d4c829c":"code","0cf61d96":"code","78b4a373":"code","28ba3f71":"code","f3edca36":"code","0099f10c":"code","88a0b364":"code","78a02d2a":"code","24c74982":"code","0367dace":"code","060a2bc9":"code","e30a3503":"code","012448e0":"code","101acbd1":"code","63a46c09":"code","46ccfdb9":"code","9a80a2d4":"code","6e4f7dfc":"code","e0246568":"code","4ddbdac7":"code","6fc62852":"code","88c7a93b":"code","10caa2f7":"code","15d3ef7f":"code","99d6d3bb":"code","f3605256":"code","141474fb":"code","bfaec15b":"code","f7b04e21":"code","3d7141eb":"code","744e97ae":"code","46787c1a":"code","4ce83955":"code","d0821524":"code","6ea555c3":"code","b6887876":"code","a1518fb1":"code","a2c81cce":"code","78ae0802":"code","d14d63cd":"code","287208f0":"code","ec33522e":"code","93586ad3":"code","504d40cc":"code","d02ccbea":"code","220633ed":"code","d98affcc":"code","0ec5b1e1":"code","8082ac4f":"code","a542259f":"code","a340c0d1":"code","fb210e86":"code","9eaa58f2":"code","1858f998":"code","b39aeacf":"code","463cb874":"code","0eda86d9":"code","f65d4c1e":"code","8eaedc40":"code","dc4e1173":"code","43a60418":"code","0f035e48":"code","ee69434c":"code","5301924d":"code","0c64af31":"code","2f9e284d":"code","78871ff7":"code","a5559b06":"code","ecb8ebb4":"code","6a0bbf99":"code","6fcda567":"code","9417c76b":"code","987ebe26":"code","96de3206":"code","7a4e66eb":"code","13de3e92":"code","18a01554":"code","d28a228a":"code","45c95788":"code","ecd0ecaa":"code","d13ec1de":"code","752735c6":"code","a6da17df":"code","df9c0485":"code","e7a9bfb8":"code","3631e2ea":"markdown","c981fed5":"markdown","fad9d16b":"markdown","791e6ddc":"markdown","b2ea43ae":"markdown","d064c4d5":"markdown","d1b36e0b":"markdown","a589d3fd":"markdown","7543a715":"markdown","bc339729":"markdown","ea5d3bf7":"markdown","d9834650":"markdown","be9ccde3":"markdown","60628989":"markdown","e62e9a07":"markdown","fdbdfdea":"markdown","55f498f0":"markdown","9d1a542c":"markdown","b5d2ffe6":"markdown","b349a095":"markdown","ed107b8d":"markdown","5720ebca":"markdown","ad9d8437":"markdown","0a0f8b25":"markdown","ffb4436c":"markdown","03aedb6a":"markdown","09895196":"markdown","5841cff1":"markdown","f5ac786e":"markdown","ee14f52e":"markdown","9e8bc7ef":"markdown","51a7f6ef":"markdown","ad3fe840":"markdown","0b16c889":"markdown","8a28b05e":"markdown","f05043c0":"markdown","eddf9ad0":"markdown","a2080add":"markdown","a6138c7b":"markdown","786d4dc7":"markdown","909e23d0":"markdown","d46a4abd":"markdown","e70774fa":"markdown","90e9b21c":"markdown","79850708":"markdown","8cf02d90":"markdown","56b73d30":"markdown"},"source":{"35f5dc93":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\n\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\n\nimport re\n# Tutorial about Python regular expressions: https:\/\/pymotw.com\/2\/re\/\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\n\nfrom tqdm import tqdm\nimport os\n\nimport plotly \nimport plotly.offline as offline\nimport plotly.graph_objs as go\nfrom collections import Counter\n\nimport os\nfrom scipy import sparse\nfrom scipy.sparse import csr_matrix\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport random\nfrom wordcloud import WordCloud\nimport nltk\nfrom tqdm import tqdm\nwarnings.filterwarnings(\"ignore\")\n\n#from mlxtend.classifier import StackingClassifier\n\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV","a2694c07":"import pickle\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt","3264dd21":"data=pd.read_csv('..\/input\/inputdata\/train.tsv', sep='\\t')\nx_test=pd.read_csv('..\/input\/test-data\/test_stg2.tsv', sep='\\t')\ndata.head()\n","59225c88":"x_test.head()","daa75eeb":"submission_final= pd.DataFrame(x_test['test_id'])\nsubmission_final.head(5)","30c6c1b5":"data.info()","42739cb7":"data.describe()","95cd27b1":"data.isnull().any()\n#df_train.brand_name.isnull().sum()\nx_test.isnull().any()","db540fda":"data = data.drop(data[(data.price < 3.0)].index)\n","9d4c829c":"data.shape","0cf61d96":"def transform_category_name(category_name):\n    try:\n        main, sub1, sub2= category_name.split('\/')\n        return main, sub1, sub2\n    except:\n        return ('missing', 'missing', 'missing')\n\ndata['category_main'], data['category_sub1'], data['category_sub2'] = zip(*data['category_name'].apply(transform_category_name))\nx_test['category_main'], x_test['category_sub1'], x_test['category_sub2'] = zip(*x_test['category_name'].apply(transform_category_name))\n\ndata.head()","78b4a373":"def fill_nan(data):\n  '''\n  Function to fill the NaN values in various columns\n  '''\n  data[\"item_description\"].fillna(\"No description yet\",inplace=True)\n  data[\"brand_name\"].fillna(\"missing\",inplace=True)\n  data[\"category_name\"].fillna(\"missing\",inplace=True)\n  return data","28ba3f71":"names = list(data['name'].values)","f3edca36":"import nltk\nnltk.download('punkt')","0099f10c":"df1=data[data['brand_name'].isna()]\n","88a0b364":"df1.head()","78a02d2a":"len(df1)","24c74982":"data.head()\ndata.brand_name.isnull().sum()\n","0367dace":"known_brands = data['brand_name'].unique()\nprint(known_brands)\nknown_brands = np.delete(known_brands, [0])\nprint(known_brands)","060a2bc9":"x_train= fill_nan(data)\nx_train.head()  \nx_test=fill_nan(x_test)\nx_test=x_test.drop(columns=['category_name'])\n\nx_test","e30a3503":"#x=data.drop(columns=['price','category_name'])\nx_train.head()\n","012448e0":"data[\"logPrice\"] = np.log(data['price']+1)\ndata.head()","101acbd1":"y_train=data.logPrice\ny=data.price\n","63a46c09":"plt.figure(figsize=(10,3))\nsns.distplot(y_train, hist=True, label=\"Price\")\nplt.legend()\nplt.show()","46ccfdb9":"plt.figure(figsize=(10, 8))\n\n# Histogram of the Price\nplt.style.use('fivethirtyeight')\nplt.hist(y, bins = 100, edgecolor = 'k');\nplt.xlabel('Price'); plt.ylabel('Number of items'); \nplt.title('Price Distribution');","9a80a2d4":"#Box-plot using log(price+1)\nfig, ax = plt.subplots(figsize=(14,5))\nplt.title('Price distribution', fontsize=15)\nsns.boxplot(data.logPrice,showfliers=False)\nax.set_xlabel('log(Price+1)',fontsize=15)\n\nplt.show()","6e4f7dfc":"#Histogram using log(price+1)\nfig, ax = plt.subplots(figsize=(14,8))\nax.hist(data.logPrice,bins=30,range=[0,8],label=\"Price\")\nplt.title('log Price distribution', fontsize=15)\nax.set_xlabel('log(Price+1)',fontsize=15)\nax.set_ylabel('No of items',fontsize=15)\n\nplt.show()","e0246568":"bins = [0, 64, 5000]\nlabels = ['cheap','expensive']\ndata['price_level'] = pd.cut(data['price'], bins=bins, labels=labels)\n\ndata.head()","4ddbdac7":"plt.figure(figsize=(6, 4))\n\n# Histogram of the Price\nplt.style.use('fivethirtyeight')\nplt.hist(x_train['shipping'].values, bins = 100, edgecolor = 'k');\nplt.xlabel('Price'); plt.ylabel('Number of items'); \nplt.title('Price Distribution');","6fc62852":"data.groupby('shipping')['price'].describe()\n","88c7a93b":"shipping = data[data['shipping']==1]['price']\nno_shipping = data[data['shipping']==0]['price']\n\nplt.figure(figsize=(12,7))\nplt.hist(shipping, bins=50, range=[0,250], alpha=0.7, label='Price With Shipping')\nplt.hist(no_shipping, bins=50, range=[0,250], alpha=0.7, label='Price With No Shipping')\nplt.title('Price Distrubtion With\/Without Shipping', fontsize=15)\nplt.xlabel('Price')\nplt.ylabel('Normalized Samples')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.legend(fontsize=15)\nplt.show()","10caa2f7":"#ref - https:\/\/machinelearningmastery.com\/parametric-statistical-significance-tests-in-python\/\n''' This code conducts a statistical significance test \nusing One way ANOVA to see if prices in shipping differ significantly '''\n\nfrom scipy.stats import f_oneway\n\nstat, p = f_oneway(x_train.price.values[x_train.shipping == 1], x_train.price.values[x_train.shipping == 0])\nprint('Statistics=%.9f, p=%.9f' % (stat, p))\n\n# interpret\nalpha = 0.05\nif p > alpha:\n    print('Same distributions (fail to reject H0)')\nelse:\n    print('Different distributions (reject H0)')\n","15d3ef7f":"print(\"No of unique values in item category is:\",data['category_name'].nunique())\nprint(\"No of unique values in item category is:\",data['category_name'].value_counts()[:10])\n","99d6d3bb":"print(\"No of unique values in main category: \",x_train['category_main'].nunique())\nprint(\"No of unique values in Sub_category1: \",x_train['category_sub1'].nunique())\nprint(\"No of unique values in Sub_category2: \",x_train['category_sub2'].nunique())","f3605256":"\n#Ref: https:\/\/seaborn.pydata.org\/generated\/seaborn.countplot.html\nfig, ax = plt.subplots(figsize=(13,6))\nsns.countplot(x='category_main', data=x_train, ax=ax)\nplt.title('Item count by main_category',fontsize=25)\nplt.ylabel('No of items',fontsize=25)\nplt.xlabel('')\nplt.xticks(rotation=70,fontsize=20)\nplt.yticks(fontsize=20)\n\nplt.show()","141474fb":"#main = pd.DataFrame(cat_train['category_main'].value_counts()).reset_index().rename(columns={'index': 'main', 'category_main':'count'})\nfig, axes = plt.subplots(figsize=(12, 5))\nmain = data[data['price']<100]\n# Use a color palette\nax = sns.boxplot( x=main[\"category_main\"], y=main[\"price\"], palette=\"Blues\")\nax.set_xticklabels(ax.get_xticklabels(),rotation=90, fontsize=12)\n\n","bfaec15b":"#Ref: https:\/\/www.kaggle.com\/konohayui\/mercari-price-suggestion-eda\nwc = WordCloud(max_words=300,width = 1200, height = 900).generate(\" \".join(x_train['category_main']))\nplt.figure(figsize = (18, 8))\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.show()","f7b04e21":"#Ref: https:\/\/stackoverflow.com\/questions\/48799185\/plot-histogram-matplotlib-with-labels-on-x-axis-instead-of-count\nprint(\"Top-10 subcategory_1:\")\nx_train.category_sub1.value_counts()[:10].plot(kind = 'bar',figsize = (13,5), title=\"Top_10 subcategory_1\",fontsize=20)","3d7141eb":"#Ref: https:\/\/www.kaggle.com\/konohayui\/mercari-price-suggestion-eda\nwc = WordCloud(max_words=300,width = 1200, height = 900).generate(\" \".join(x_train['category_sub1']))\nplt.figure(figsize = (18, 8))\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.show()","744e97ae":"#Ref: https:\/\/stackoverflow.com\/questions\/48799185\/plot-histogram-matplotlib-with-labels-on-x-axis-instead-of-count\nprint(\"Top-10 subcategory_2:\")\nx_train.category_sub2.value_counts()[:10].plot(kind = 'bar',figsize = (13,5), title=\"Top_10 subcategory_2\",fontsize=20)","46787c1a":"#Ref: https:\/\/www.kaggle.com\/konohayui\/mercari-price-suggestion-eda\nwc = WordCloud(max_words=300,width = 1200, height = 900).generate(\" \".join(x_train['category_sub2']))\nplt.figure(figsize = (18, 8))\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.show()","4ce83955":"print(\"No of unique values in brand are:\",x_train['brand_name'].nunique())\nprint(\"No of unique values in brand are is:\",x_train['brand_name'].value_counts())\nx_train.brand_name.value_counts()[:10].plot(kind = 'bar',figsize = (13,5), title=\"Brand\",fontsize=20)","d0821524":"# Display Top 20 Expensive Brands By Mean Price\ntop20_brand = data.groupby('brand_name', axis=0).mean()\ndf_expPrice = pd.DataFrame(top20_brand.sort_values('price', ascending = False)['price'][0:20].reset_index())\n\nax = sns.barplot(x=\"brand_name\", y=\"price\", data=df_expPrice)\nax.set_xticklabels(ax.get_xticklabels(),rotation=90, fontsize=15)\nax.set_title('Top 20 Expensive Brand', fontsize=15)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()# Displayd frequency ratio of brand names\n","6ea555c3":"\n#Ref: https:\/\/www.kaggle.com\/konohayui\/mercari-price-suggestion-eda\nwc = WordCloud(max_words=300,width = 1200, height = 900).generate(\" \".join(x_train.item_description.astype(str)))\nplt.figure(figsize = (18, 8))\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.show()","b6887876":"#this function assigns a value 1 if a product has brand_name else 0\"\"\"\n\ndef branded(data):\n   is_branded=[]\n   for i in data['brand_name']:\n      if i=='missing': \n          is_branded.append(0) \n      else: \n          is_branded.append(1)\n   return is_branded\nx_train['is_branded']=branded(x_train)\nx_test['is_branded']=branded(x_test)","a1518fb1":"\"\"\" \n    This function is used to guess the missing brand name.\n    It will check for an existing brand name mentioned in the item name section and also category of brnad name and populates the missing brand\n\"\"\"\ndef fill_brand(row):\n    brand, name, cn = row\n    brand = brand.lower()\n    if brand=='missing':\n        for brand in known_brands:\n            if str(brand) in name:\n                return brand         \n    return brand\nx_train['fillmissingbrands'] = x_train[['brand_name','name', 'category_main']].apply(fill_brand, axis = 1)\nx_test['fillmissingbrands'] = x_test[['brand_name','name', 'category_main']].apply(fill_brand, axis = 1)\n","a2c81cce":"# https:\/\/stackoverflow.com\/a\/47091490\/4084039\nimport re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","78ae0802":"# https:\/\/gist.github.com\/sebleier\/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]","d14d63cd":"# Combining all the above stundents \nfrom tqdm import tqdm\ndef preprocess_text(text_data):\n    preprocessed_text = []\n    # tqdm is for printing the status bar\n    for sentance in tqdm(text_data):\n        sent = decontracted(sentance)\n        sent = sent.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        # https:\/\/gist.github.com\/sebleier\/554280\n        sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n        preprocessed_text.append(sent.lower().strip())\n    return preprocessed_text","287208f0":"preprocessed_item_description = preprocess_text(x_train['item_description'].values)\nprint(preprocessed_item_description[:5])\npreprocessed_test_item_description = preprocess_text(x_test['item_description'].values)","ec33522e":"preprocessed_test_item_description[1]","93586ad3":"word_punctuation_tokenizer = nltk.WordPunctTokenizer()\nword_tokenized_corpus = [word_punctuation_tokenizer.tokenize(sent) for sent in preprocessed_test_item_description]\n","504d40cc":"#How to calculate number of words in a string in DataFrame: https:\/\/stackoverflow.com\/a\/37483537\/4084039\n\nx_train['preprocessed_item_description'] = preprocessed_item_description\nx_test['preprocessed_item_description'] = preprocessed_test_item_description\n\nword_count = x_train['preprocessed_item_description'].str.split().apply(len).value_counts()\nword_dict = dict(word_count)\nword_dict = dict(sorted(word_dict.items(), key=lambda kv: kv[1])[0:10])\nx_train['count'] = x_train['preprocessed_item_description'].apply(lambda x : len(str(x)))\nx_test['count'] = x_test['preprocessed_item_description'].apply(lambda x : len(str(x)))\n\n","d02ccbea":"plt.scatter(data['count'],data['price'],alpha=0.8)\nplt.title(\"Price Vs Description length\")\nplt.xlabel(\"Description length\")\nplt.ylabel(\"Price(USD)\")\nplt.show()","220633ed":"\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nnltk.download('vader_lexicon')\n","d98affcc":"#performing sentiment analysis \n\ndef sentiment_analysis(data):\n   sentiment_score = SentimentIntensityAnalyzer()\n   sentiment = []\n   for sentence in tqdm(data):\n       sentiment.append(sentiment_score.polarity_scores(sentence))\n   return sentiment\n\n\ntraining_sentiment_score=sentiment_analysis(x_train['item_description']) \ncv_sentiment_score=sentiment_analysis(x_test['item_description'])\n","0ec5b1e1":"#this function splits sentiment analysis score into four further features ie positive,negative,compound and neutral\n\ndef splitting_sentiment(sentiment_score):\n  positive=[]\n  negative=[]\n  neutral=[]\n  compound=[]\n  for i in tqdm(sentiment_score):\n    positive.append(i['pos'])\n    negative.append(i['neg'])\n    neutral.append(i['neu'])\n    compound.append(i['compound'])\n  return positive,negative,neutral,compound","8082ac4f":"#Training Data Sentiment Analysis\npos,neg,neu,comp=splitting_sentiment(training_sentiment_score)\nx_train['positive']=pos\nx_train['negative']=neg\nx_train['neutral']=neu\nx_train['compound']=comp\n","a542259f":"\n#x_test Data Sentiment Analysis\npos,neg,neu,comp=splitting_sentiment(cv_sentiment_score)\nx_test['positive']=pos\nx_test['negative']=neg\nx_test['neutral']=neu\nx_test['compound']=comp\n","a340c0d1":"x=x_train.drop(columns=['item_description'])\nx_train.head()","fb210e86":"sns.set(rc={'figure.figsize':(8,7)})\nsns.ax = sns.boxplot(x=x_train[\"item_condition_id\"], y=np.log(x_train['price']+1), palette=\"Blues\")\nplt.title('Violin Plots - Price variation in item_condition_id')\nplt.show()","9eaa58f2":"#ref - https:\/\/machinelearningmastery.com\/parametric-statistical-significance-tests-in-python\/\n# applying statistical significance test to confirm if the price values are not by chance and are according to item condition.\nfrom scipy.stats import f_oneway\nstat, p = f_oneway(x_train.price.values[x_train.item_condition_id == 1],\n                   x_train.price.values[x_train.item_condition_id == 2],\n                   x_train.price.values[x_train.item_condition_id == 3],\n                   x_train.price.values[x_train.item_condition_id == 4],\n                   x_train.price.values[x_train.item_condition_id == 5])\nprint('Statistics=%.9f, p=%.9f' % (stat, p))\n\nalpha = 0.05\nif p > alpha:\n    print('Same distributions (fail to reject H0)')\nelse:\n    print('Different distributions (reject H0)')","1858f998":"preprocessed_name = preprocess_text(x_train['name'].values)\nprint(preprocessed_name[:5])\nx_train['name'] = preprocessed_name\ntest_preprocessed_name=preprocess_text(x_test['name'].values)\nx_test['name']=test_preprocessed_name","b39aeacf":"# gives correlation with the price and sort \ncorrelations_data = x_train.corr()['price'].sort_values()\nprint(correlations_data)","463cb874":"columns = list(x_train.columns)\nplt.figure(figsize = (10, 10))\nsns.heatmap(x_train[columns].corr(), annot = True, linewidth = 0.5)\nplt.show()","0eda86d9":"x_train.columns","f65d4c1e":"from sklearn.model_selection import train_test_split\nx_train, x_cv, y_tr, y_cv = train_test_split(x_train, y_train, test_size=0.20)\n","8eaedc40":"vectorizer = CountVectorizer(binary=True)\nvectorizer.fit(x_train['name'].values) \n\ntrain_name = vectorizer.transform(x_train['name'].values)\ncv_name = vectorizer.transform(x_cv['name'].values)\ntest_name = vectorizer.transform(x_test['name'].values)","dc4e1173":"vectorizer = CountVectorizer(binary=True)\nvectorizer.fit(x_train['category_main'].values) \n\ntrain_category_main = vectorizer.transform(x_train['category_main'].values)\ncv_category_main = vectorizer.transform(x_cv['category_main'].values)\ntest_category_main = vectorizer.transform(x_test['category_main'].values)","43a60418":"vectorizer = CountVectorizer(binary=True)\nvectorizer.fit(x_train['category_sub1'].values) \ntrain_category_sub1 = vectorizer.transform(x_train['category_sub1'].values)\ncv_category_sub1 = vectorizer.transform(x_cv['category_sub1'].values)\ntest_category_sub1 = vectorizer.transform(x_test['category_sub1'].values)\n","0f035e48":"vectorizer = CountVectorizer(binary=True)\nvectorizer.fit(x_train['category_sub2'].values)\n\ntrain_category_sub2 = vectorizer.transform(x_train['category_sub2'].values)\ncv_category_sub2 = vectorizer.transform(x_cv['category_sub2'].values)\ntest_category_sub2 = vectorizer.transform(x_test['category_sub2'].values)\n","ee69434c":"vectorizer = CountVectorizer(binary=True) \nvectorizer.fit(x_train['brand_name'].values) \n\ntrain_brand_name = vectorizer.transform(x_train['brand_name'].values) \ncv_brand_name = vectorizer.transform(x_cv['brand_name'].values) \ntest_brand_name = vectorizer.transform(x_test['brand_name'].values)","5301924d":"from sklearn.preprocessing import OneHotEncoder\n\noe_style = OneHotEncoder()\ntrain_condition = oe_style.fit_transform(x_train[[\"item_condition_id\"]])\ncv_condition = oe_style.transform(x_cv[[\"item_condition_id\"]])\ntest_condition = oe_style.transform(x_test[[\"item_condition_id\"]])\n\npd.DataFrame(train_condition.toarray(), columns=oe_style.categories_).head()\n","0c64af31":"oe_style = OneHotEncoder()\ntrain_shipping = oe_style.fit_transform(x_train[[\"shipping\"]])\ncv_shipping = oe_style.transform(x_cv[[\"shipping\"]])\ntest_shipping = oe_style.transform(x_test[[\"shipping\"]])\n\npd.DataFrame(train_shipping.toarray(), columns=oe_style.categories_).head()","2f9e284d":"oe_style = OneHotEncoder()\ntrain_is_branded = oe_style.fit_transform(x_train[[\"is_branded\"]])\ncv_is_branded = oe_style.transform(x_cv[[\"is_branded\"]])\ntest_is_branded = oe_style.transform(x_test[[\"is_branded\"]])\n\npd.DataFrame(train_is_branded.toarray(), columns=oe_style.categories_).head()","78871ff7":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=10)\nvectorizer.fit(x_train['preprocessed_item_description'])\nX_train_itemdes = vectorizer.transform(x_train['preprocessed_item_description'])\nX_cv_itemdes = vectorizer.transform(x_cv['preprocessed_item_description'])\nX_test_itemdes = vectorizer.transform(x_test['preprocessed_item_description'])\n","a5559b06":"from sklearn.preprocessing import MinMaxScaler \nnormalizer = MinMaxScaler(feature_range=(-1, 1))\n\nnormalizer.fit(x_train['negative'].values.reshape(-1,1))\n\nX_train_neg_norm = normalizer.transform(x_train['negative'].values.reshape(-1,1))\nX_cv_neg_norm = normalizer.transform(x_cv['negative'].values.reshape(-1,1))\nX_test_neg_norm = normalizer.transform(x_test['negative'].values.reshape(-1,1))","ecb8ebb4":"normalizer = MinMaxScaler(feature_range=(-1, 1))\nnormalizer.fit(x_train['neutral'].values.reshape(-1,1))\n\nX_train_neu_norm = normalizer.transform(x_train['neutral'].values.reshape(-1,1))\nX_cv_neu_norm = normalizer.transform(x_cv['neutral'].values.reshape(-1,1)) \nX_test_neu_norm = normalizer.transform(x_test['neutral'].values.reshape(-1,1))\n","6a0bbf99":"normalizer = MinMaxScaler(feature_range=(-1, 1))\n\nnormalizer.fit(x_train['positive'].values.reshape(-1,1))\n\nX_train_pos_norm = normalizer.transform(x_train['positive'].values.reshape(-1,1)) \nX_cv_pos_norm = normalizer.transform(x_cv['positive'].values.reshape(-1,1)) \nX_test_pos_norm = normalizer.transform(x_test['positive'].values.reshape(-1,1))","6fcda567":"normalizer =MinMaxScaler(feature_range=(-1, 1))\n\nnormalizer.fit(x_train['compound'].values.reshape(-1,1))\n\nX_train_com_norm = normalizer.transform(x_train['compound'].values.reshape(-1,1))\nX_cv_com_norm = normalizer.transform(x_cv['compound'].values.reshape(-1,1))\nX_test_com_norm = normalizer.transform(x_test['compound'].values.reshape(-1,1))","9417c76b":"normalizer = MinMaxScaler(feature_range=(-1, 1))\n\nnormalizer.fit(x_train['count'].values.reshape(-1,1))\n\nX_train_count = normalizer.transform(x_train['count'].values.reshape(-1,1))\nX_cv_count = normalizer.transform(x_cv['count'].values.reshape(-1,1))\nX_test_count = normalizer.transform(x_test['count'].values.reshape(-1,1))","987ebe26":"normalizer = MinMaxScaler(feature_range=(-1, 1))\n\nnormalizer.fit(y_tr.values.reshape(-1,1))\n\nX_train_price_norm = normalizer.transform(y_tr.values.reshape(-1,1))\nX_cv_price_norm = normalizer.transform(y_cv.values.reshape(-1,1))\n","96de3206":"from scipy.sparse import hstack\nX_tr = hstack((train_name,train_category_main,train_category_sub1,train_category_sub2, train_brand_name, train_condition, train_shipping,X_train_itemdes,X_train_count,X_train_neg_norm,X_train_neu_norm,X_train_pos_norm,X_train_com_norm,train_is_branded)).tocsr()\nX_cv = hstack((cv_name,cv_category_main,cv_category_sub1,cv_category_sub2, cv_brand_name, cv_condition, cv_shipping,X_cv_itemdes,X_cv_count,X_cv_neg_norm,X_cv_neu_norm,X_cv_pos_norm,X_cv_com_norm,cv_is_branded)).tocsr()\nX_te = hstack((test_name,test_category_main,test_category_sub1,test_category_sub2, test_brand_name, test_condition, test_shipping,X_test_itemdes,X_test_count,X_test_neg_norm,X_test_neu_norm,X_test_pos_norm,X_test_com_norm,test_is_branded)).tocsr()\n","7a4e66eb":"from sklearn.linear_model import SGDRegressor, Ridge\n","13de3e92":"params = {\"alpha\":[0.000001,0.00001,0.0001,0.001,0.01,0.1,0,1,10,100,1000,10000,100000]}\nridge_model = Ridge(solver = \"lsqr\", fit_intercept=False)\nlr_reg = RandomizedSearchCV(ridge_model,param_distributions =params,n_jobs=-1,random_state=0)\nlr_reg.fit(X_tr, y_tr)\nlr_reg.best_params_","18a01554":"ridge_model2 = Ridge(alpha=0.01,solver = \"lsqr\", fit_intercept=False )\nridge_model2.fit(X_tr, y_tr)\ny_pred = ridge_model2.predict(X_cv)\nridge_RMSLE = np.sqrt(mean_squared_error(y_cv, y_pred))\nprint(ridge_RMSLE)","d28a228a":"ridge_model2 = Ridge(alpha=0.01,solver = \"lsqr\", fit_intercept=False )\nridge_model2.fit(X_tr, y_tr)\n\npred_ridge = ridge_model2.predict(X_te)\nprint(pred_ridge)\n","45c95788":"submission_final['price'] = np.expm1(pred_ridge)\nsubmission_final.to_csv(\"submission_price.csv\", index=False)","ecd0ecaa":"params2 = {\"alpha\":[0.0001,0.001,0.01,0.1,0,1,10,100]}\nmodel_sgd = SGDRegressor(loss='epsilon_insensitive',learning_rate='adaptive',max_iter=5,penalty='l2',fit_intercept=False,early_stopping=True)\nsgd_reg = RandomizedSearchCV(model_sgd,param_distributions =params2,n_jobs=-1,random_state=1)\nsgd_reg.fit(X_tr, y_tr)","d13ec1de":"sgd_reg.best_params_\n","752735c6":"model_sgd2 = SGDRegressor(penalty='l2',loss='epsilon_insensitive',learning_rate='adaptive',max_iter=5,fit_intercept=False,alpha=0,\n                     early_stopping=True)\nmodel_sgd2.fit(X_tr, y_tr)","a6da17df":"y_cv_pred = model_sgd2.predict(X_cv)\nsgd_score = np.sqrt(mean_squared_error(y_cv, y_cv_pred))\nprint(sgd_score)","df9c0485":"model_sgd2 = SGDRegressor(alpha=0,penalty='l2',loss='epsilon_insensitive',learning_rate='adaptive',max_iter=5,fit_intercept=False)\nmodel_sgd2.fit(X_tr, y_tr)\npred_sgd2 = model_sgd2.predict(X_te)\n","e7a9bfb8":"submission_final['price'] = np.expm1(pred_sgd2)\nsubmission_final.to_csv(\"submission_price_sgd.csv\", index=False)","3631e2ea":"vectorizer = CountVectorizer(binary=True)\nvectorizer.fit(x_train['brand_name'].values) # fit has to happen only on train data\n\n# we use the fitted CountVectorizer to convert the text to vector\ntrain_brand_name = vectorizer.transform(x_train['brand_name'].values)\ntrain_brand_name = vectorizer.transform(x_test['brand_name'].values)\n\nprint(\"After vectorizations\")\nprint(train_brand_name.shape)\n\nprint(vectorizer.get_feature_names())\nprint(\"=\"*100)","c981fed5":"**CONCLUSION:**\n1. We can conclude that all of the features except train-id has some significant effect on the price  including sentiment scores.\n2. We can tell that for shipping it is negatively related which states that lower prices requires buyers to pay and higher prices are paid mostly by seller.\n3. Also, we could see the description length (count) is positively linear so as the length of the description  increases price seems to be higher assuming mostly it would be a positive description.\n\n\n\n","fad9d16b":"**conclusion:**\n1. we can say that the distrinbution of prices among different conditions are significantly different.","791e6ddc":"Word plot gives most frequent words used in the field","b2ea43ae":"**Statistics summary** - Clearly, ANOVA test suggests that there is a significant difference and 'p' value is quite negligible - so we will be going forward with our conclusion that there is indeed a significant enough difference between the 2 shipping categories in the price values\n","d064c4d5":"**Calculating sentiment score.**","d1b36e0b":"**Splitting category feature**","a589d3fd":"**Checking if any column has missing data**","7543a715":"The 'log(price+1)' feature has the price range spread over 2.3-3.5 for most of the items","bc339729":"[link text](https:\/\/)","ea5d3bf7":"**RIDGE REGRESSOR**\nhttps:\/\/www.statisticshowto.com\/ridge-regression\/#:~:text=Ridge%20regression%20is%20a%20way,(correlations%20between%20predictor%20variables).\n\nhttps:\/\/towardsdatascience.com\/ridge-regression-for-better-usage-2f19b3a202db","d9834650":"**TEXT PRE-PROCESSING AND FEATURIZATION**\n\n\n","be9ccde3":"**Brand name distribution**","60628989":"**split train data into train and cv data**","e62e9a07":"# Display Top 20 Expensive Brands By Mean Price\n","fdbdfdea":"**Brand Name**","55f498f0":"**Fill the missing values as below**","9d1a542c":"**Sub Category 2**","b5d2ffe6":"We will be dropping rows that have price <3 as it is said that mercari doesn't allow products to be soled for <3 and so this must be an error","b349a095":"**Numerical features**","ed107b8d":"**Name feature**","5720ebca":"**CONCLUSION**\n>> We could see most of the brand names are missing .","ad9d8437":"\n**category distribution:**","0a0f8b25":"**Sub Category 1**","ffb4436c":"**STOCHASTIC GRADIENT DESCENT REGRESSOR**","03aedb6a":"\nWe have scaled down our 'price' feature logprice. We have added log(Price+1) to it as log(0) is undefined. hence if price for an item is 0, then the item will have no price as defined.","09895196":"Here, description length vs price has been given. From the plot, it can be said as length increases, price charged becomes lesser and lesser. Most of the items with lesser description length have more price value.","5841cff1":"**CATEGORICAL FEATURES:**","f5ac786e":"**Main category**","ee14f52e":"**shipping distribution:**","9e8bc7ef":"**CONCLUSION:**\n>> We can conclude that Women category is the highest, followed by clothing.","51a7f6ef":"# **Feature Engineering:**","ad3fe840":"**Name Feature:**","0b16c889":"**conclusion:**\n>>We can conclude that lower prices hhave shipping paid by buyers and higher prices by sellers.","8a28b05e":"**Correlation between features**","f05043c0":"**Shipping**","eddf9ad0":"**Price distribution**","a2080add":"# BRAND NAME","a6138c7b":"**Price Vs Description length**\n","786d4dc7":"**Item condition**","909e23d0":"**ML MODELLING**","d46a4abd":"# ***Text Features***","e70774fa":"**Item condition** **distribution** \n","90e9b21c":"**Item description:**\n","79850708":"**Is Branded**","8cf02d90":"**Binning Price Into Two Categories**","56b73d30":"**Merger sparse matrices**"}}