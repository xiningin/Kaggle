{"cell_type":{"87437bee":"code","13b51a4f":"code","9227cfa1":"code","9ad96bab":"code","43a1cba8":"code","580471f3":"code","3bca7cc1":"code","4e0336ce":"code","c5aa8903":"code","44297019":"code","923c29f6":"code","e4068b64":"code","bd659a50":"code","6be36317":"code","7a9a0322":"code","ad05d823":"code","afc3c5d6":"code","1e1b4b43":"code","6df6d182":"code","0dc15c1c":"code","7bfaf60d":"code","6f5f8aca":"code","b52a3e27":"markdown","bc490e46":"markdown","c1e9e89b":"markdown","53b45426":"markdown","49f9bfce":"markdown","97e9bb7b":"markdown","fec38b9a":"markdown","e77c3395":"markdown","6914f45e":"markdown"},"source":{"87437bee":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils import data\n\nimport pandas as pd\nimport numpy as np\n\nimport torchvision\nfrom torchvision import transforms\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\nfrom PIL import Image\n\nfrom matplotlib import pyplot as plt\n\nimport tqdm\n\nimport pickle\nimport os\nimport gc\n\n#python-levenshtein\nimport Levenshtein","13b51a4f":"REC_IMG_SIZE = (98, 34)\nMARGIN = (24,8)\nIMG_SIZE = tuple(x1 + x2 for x1, x2 in zip(REC_IMG_SIZE, MARGIN))\n\nNUM_CHARS = 22\nNUM_LENGTH = 9\n\nSQUEEZE_FACTOR = 2\nREC_SCORE_CUTOFF = 7.","9227cfa1":"#get rid of cyrillic doubles\ntranslation_table = str.maketrans(\"\u0410\u0412\u0415\u041a\u041c\u041d\u041e\u0420\u0421\u0422\u0423\u0425\", \"ABEKMHOPCTYX\")","9ad96bab":"#we will extract images slightly larger than we need so that we can apply xy translations augumentation\ndef expand_crop_box(crop_box):\n    w, h = crop_box[2] - crop_box[0], crop_box[3] - crop_box[1]\n    w = w*IMG_SIZE[0]\/REC_IMG_SIZE[0]\n    h = h*IMG_SIZE[1]\/REC_IMG_SIZE[1]\n    \n    x, y = (crop_box[0] + crop_box[2])\/2, (crop_box[1] + crop_box[3])\/2\n    \n    crop_box = [x - w\/2, y - h\/2, x + w\/2, y + h\/2]\n    return crop_box","43a1cba8":"ann = pd.read_json(os.path.join('data','train.json'))[:-1]\n\nimages = []\ntargets = []\nfor samples, im_path in tqdm.tqdm(zip(ann['nums'], ann['file']), total=len(ann)):\n    im_path = os.path.join('data',im_path)\n    image = Image.open(im_path).convert('RGB')\n    for sample in samples:\n        box = np.array(sample['box'])\n        x_min, y_min = box.min(axis=0)\n        x_max, y_max = box.max(axis=0)\n        targets.append(sample['text'].upper().translate(translation_table))\n        crop_box = [x_min, y_min, x_max, y_max]\n        crop_box = expand_crop_box(crop_box)\n        images.append(image.crop(crop_box).resize(IMG_SIZE).tobytes())\n        \nplates_corpus = {'img_mode': 'RGB', 'img_size': IMG_SIZE, 'img_bytes': images, 'nums': targets}\n\nwith open('plates.pickle', 'wb') as fp:\n    pickle.dump(plates_corpus, fp)","580471f3":"with open('plates.pickle', 'rb') as fp:\n    plates_data = pickle.load(fp)\n    \nchar_set = set([c for c in ''.join(plates_data['nums'])])\nchar_list = list(char_set)\nchar_list.sort()\n\nchar_dict = {char: idx for idx, char in enumerate(char_list)}\n\ninv_char_dict = {idx: char for idx, char in enumerate(char_list)}\ninv_char_dict[NUM_CHARS] = ''","3bca7cc1":"class PlatesDataset(data.Dataset):\n    def __init__(self, path, char_dict, train=True, pad_to = NUM_LENGTH):\n        super().__init__()\n        with open(path, 'rb') as fp:\n            self.data = pickle.load(fp)\n            if train:\n                begin, end = 0, int(0.8*len(self.data['img_bytes']))\n            else:\n                begin, end = int(0.8*len(self.data['img_bytes'])), len(self.data['img_bytes'])\n            for key in ['img_bytes', 'nums']:\n                self.data[key] = self.data[key][begin:end]\n            \n                \n        self.char_dict = char_dict\n        self.img_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        self.train = train\n        self.pad_to = pad_to\n        \n    def __len__(self):\n        return len(self.data['img_bytes'])\n    \n    def __getitem__(self, idx):\n        img = Image.frombytes(self.data['img_mode'], self.data['img_size'], self.data['img_bytes'][idx])\n        target = self.data['nums'][idx]\n        target = [self.char_dict[char] for char in target]\n        target = target + [len(self.char_dict) for _ in range(self.pad_to - len(target))]\n        \n        if self.train:\n            x_offset = np.random.randint(0, 1 + self.data['img_size'][0] - REC_IMG_SIZE[0])\n            y_offset = np.random.randint(0, 1 + self.data['img_size'][1] - REC_IMG_SIZE[1])\n        else:\n            x_offset = (self.data['img_size'][0] - REC_IMG_SIZE[0])\/2\n            y_offset = (self.data['img_size'][1] - REC_IMG_SIZE[1])\/2\n        \n        crop_box = np.array((0,0) + REC_IMG_SIZE) + np.array((x_offset, y_offset)*2)\n        img = img.crop(crop_box)\n        \n        img = self.img_transform(img)\n        target = torch.LongTensor(target)\n        return img, target","4e0336ce":"train_dataset = PlatesDataset('plates.pickle', char_dict = char_dict)\ntrain_loader = data.DataLoader(train_dataset, batch_size=128, num_workers=8, shuffle=True, drop_last=True)\n\nval_dataset = PlatesDataset('plates.pickle', char_dict = char_dict, train=False)\nval_loader = data.DataLoader(val_dataset, batch_size=128, num_workers=8)","c5aa8903":"# the extra layer made a huge difference in my experiments\n\nrec = torchvision.models.resnet34(pretrained=True)\nrec.fc = nn.Sequential(\n    nn.Linear(512, 2048, bias=False),\n    nn.BatchNorm1d(2048),\n    nn.LeakyReLU(),\n    nn.Dropout(),\n    nn.Linear(2048, (NUM_CHARS + 1)*NUM_LENGTH)\n)\nrec.avgpool = nn.AdaptiveMaxPool2d((1,1))\nrec = rec.cuda()","44297019":"opt = torch.optim.Adam(rec.parameters(), lr=1e-3)","923c29f6":"# around 10-15 epochs should be enough\n\nn_epochs = 50\n\nbest_val_loss = np.inf\n\ntrain_loss_hist = []\nval_loss_hist = []\nfor ep in range(n_epochs):\n    rec = rec.train()\n    train_loss = []\n    for batch in train_loader:\n        inputs, targets = batch\n        inputs = inputs.cuda()\n        targets = targets.cuda().reshape(-1)\n\n        outputs = rec(inputs).reshape(-1, (NUM_CHARS + 1))\n\n        loss = F.cross_entropy(outputs, targets)\n        train_loss.append(loss.item())\n\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n    train_loss = np.array(train_loss).mean()\n    train_loss_hist.append(train_loss)\n    \n    rec = rec.eval()\n    val_loss = []\n    for batch in val_loader:\n        inputs, targets = batch\n        inputs = inputs.cuda()\n        targets = targets.cuda().reshape(-1)\n\n        with torch.no_grad():\n            outputs = rec(inputs).reshape(-1, (NUM_CHARS + 1))\n\n        loss = F.cross_entropy(outputs, targets)\n        val_loss.append(loss.item())\n    val_loss = np.array(val_loss).mean()\n    val_loss_hist.append(val_loss)\n    \n    if val_loss < best_val_loss:\n        torch.save(rec.state_dict(), 'recog_checkpoint.pth')\n        \n    print(f\"Train: {train_loss_hist[-1]:.3f}, val: {val_loss_hist[-1]:.3f}\")","e4068b64":"def decode(idx_list):\n    return ''.join([inv_char_dict[int(idx)] for idx in idx_list])","bd659a50":"rec = rec.eval()\n\nlev_distances = []\nmean_scores = []\nfor batch in val_loader:\n    inputs, targets = batch\n    inputs = inputs.cuda()\n    \n    true_nums = [decode(idx_list) for idx_list in targets]\n    \n    with torch.no_grad():\n        scores, chars = rec(inputs).reshape(-1, NUM_LENGTH, (NUM_CHARS+1)).max(dim=2)\n        \n    mean_scores.append(scores.mean(dim=1))\n    \n    pred_nums = [decode(idx_list) for idx_list in chars]\n\n    for num_1, num_2 in zip(true_nums, pred_nums):\n        lev_distances.append(Levenshtein.distance(num_1, num_2))","6be36317":"np.array(lev_distances).mean()","7a9a0322":"mean_scores = torch.cat(mean_scores).cpu().numpy()\nplt.scatter(mean_scores, lev_distances)\nplt.show()","ad05d823":"def box_to_rect(box):\n    box = np.array(box)\n    x_min, y_min = box.min(axis=0)\n    x_max, y_max = box.max(axis=0)\n    return [x_min, y_min, x_max, y_max]\n\ndef crop_on_plate(image, boxes, crop_size):\n    box = boxes[np.random.randint(0, len(boxes))]\n    x_c, y_c = (box[0] + box[2])\/2, (box[1] + box[3])\/2\n    x_c = x_c - min(0, x_c - crop_size[0]\/2) - min(0, image.size[0] - x_c - crop_size[0]\/2)\n    y_c = y_c - min(0, y_c - crop_size[1]\/2) - min(0, image.size[1] - y_c - crop_size[1]\/2)\n    \n    crop_region = [x_c - crop_size[0]\/2, y_c - crop_size[1]\/2, x_c + crop_size[0]\/2, y_c + crop_size[1]\/2]\n    \n    image = image.crop(crop_region)\n    boxes = boxes - np.array(crop_region[:2]*2)\n\n    return image, boxes\n\ndef collate_fn(batch):\n    inputs = []\n    targets = []\n    for el in batch:\n        inputs.append(el[0])\n        targets.append(el[1])\n    inputs = torch.stack(inputs, dim=0)\n    \n    return inputs, targets\n\nclass DetectionDataset(data.Dataset):\n    def __init__(self, root, crop_size, train=True):\n        self.root = root\n        self.annot = pd.read_json(os.path.join(root,'train.json'))[:-1]\n        self.crop_size = crop_size\n        self.img_transform = transforms.Compose([\n            transforms.ToTensor(),\n        ])\n        if train:\n            self.annot = self.annot[:int(0.8*len(self.annot))]\n        else:\n            self.annot = self.annot[int(0.8*len(self.annot)):]\n        \n    def __len__(self):\n        return len(self.annot)\n    \n    def __getitem__(self, idx):\n        boxes, im_path = self.annot.iloc[idx]\n        im_path = os.path.join(self.root, im_path)\n        image = Image.open(im_path).convert('RGB')\n        boxes = [box_to_rect(b['box']) for b in boxes]\n        \n        image, boxes = crop_on_plate(image, boxes, self.crop_size)\n        \n        #we squeeze the image and the bounding boxes in the x dimension\n        #here is the reasoning: the majority of licence plates' bounding boxes are very thin, and\n        #they match poorly to the anchors used in the model. \n        #Hence squeezing the image brings their aspect ratio closer to 1, making them more anchor-friendly.\n        boxes[:, 0] = boxes[:, 0]\/SQUEEZE_FACTOR\n        boxes[:, 2] = boxes[:, 2]\/SQUEEZE_FACTOR\n        image = image.resize((int(image.size[0]\/SQUEEZE_FACTOR), image.size[1]))\n        \n        targets = {\n            'boxes': torch.FloatTensor(boxes),\n            'labels': torch.ones(len(boxes)).long(),\n        }\n        \n        image = self.img_transform(image)\n        \n        return image, targets","afc3c5d6":"train_dataset = DetectionDataset('data', crop_size=(1000,400))\ntrain_loader = data.DataLoader(train_dataset, batch_size=32, num_workers=8, shuffle=True, drop_last=True,\n                              collate_fn=collate_fn)\n\nval_dataset = DetectionDataset('data', crop_size=(1000,400), train=False)\nval_loader = data.DataLoader(val_dataset, batch_size=16, num_workers=8, collate_fn=collate_fn)","1e1b4b43":"#the model does some internal image resizing controlled by min_size and max_size parameters.\n#Numbers used here make sure this resizing has no effect.\n\ndet = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, min_size=400)\ndet.roi_heads.box_predictor = FastRCNNPredictor(det.roi_heads.box_predictor.cls_score.in_features, 2)\n# for p in model.parameters():\n#     p.requires_grad = False\n# for p in model.backbone.fpn.parameters():\n#     p.requires_grad = True\n# for p in model.rpn.parameters():\n#     p.requires_grad = True\n# for p in model.roi_heads.parameters():\n#     p.requires_grad = True\n    \ndet = det.cuda()","6df6d182":"opt = torch.optim.Adam(det.parameters(), lr=1e-3)","0dc15c1c":"#after a couple of epochs the loss more or less stabilizes\n#TODO: a proper validation\n\nn_epochs = 1\n\ntrain_loss_hist = []\nfor ep in range(n_epochs):\n    det = det.train()\n    train_loss = []\n    for b_num, batch in enumerate(train_loader):\n        inputs, targets = batch\n        inputs = inputs.cuda()\n        \n        for target in targets:\n            for key, val in target.items():\n                target[key] = val.cuda()\n\n        loss_dict = det(inputs, targets)\n\n        loss = sum([l for l in loss_dict.values()])\n        print(f\"it {b_num + 1}\/{len(train_loader)}, loss: {loss:.3f}\")\n        train_loss.append(loss.item())\n\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n    train_loss = np.array(train_loss).mean()\n    train_loss_hist.append(train_loss)\n    \n    torch.save(det.state_dict(), 'detect_checkpoint.pth')","7bfaf60d":"#rec.load_state_dict(torch.load('recog_checkpoint.pth'))\n#det.load_state_dict(torch.load('detect_checkpoint.pth'))\n\nrec = rec.eval().cuda()\ndet = det.eval().cuda()\n\nrec_transform = transforms.Compose([\n    transforms.Resize((34,98)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n\nfiles = os.listdir(os.path.join('data','test'))\nfiles = {int(file.split('.')[0]): file for file in files}\n\nf_name_list = []\nnums_list = []\nfor ind in tqdm.tqdm(range(len(files))):\n    f_name = 'test\/' + files[ind]\n    img = Image.open(os.path.join('.', 'data', f_name)).convert('RGB')\n    \n    #squeezing the input image for detection\n    img_for_detection = img.resize((int(img.size[0]\/SQUEEZE_FACTOR), img.size[1]))\n    with torch.no_grad():\n        det_results = det(transforms.functional.to_tensor(img_for_detection)[None,...].cuda())[0]\n    boxes = det_results['boxes']\n    scores = det_results['scores']\n    boxes = boxes[scores > .95].cpu().numpy()\n    lower_left = boxes[:,0].copy()\n    boxes = boxes[lower_left.argsort()]\n    \n    #restoring the aspect ratio of the bounding boxes\n    boxes[:,0] = boxes[:,0]*SQUEEZE_FACTOR\n    boxes[:,2] = boxes[:,2]*SQUEEZE_FACTOR\n    \n    nums = []\n    for box in boxes:\n        plate = img.crop(box)\n        plate = rec_transform(plate)[None,...].cuda()\n        with torch.no_grad():\n            rec_scores, rec_chars = rec(plate).reshape(-1, NUM_LENGTH, (NUM_CHARS+1)).max(dim=2)\n        rec_chars = decode(rec_chars[0])\n        \n        #dropping low recognition score plates\n        if rec_scores.mean() > REC_SCORE_CUTOFF:\n            nums.append(rec_chars)\n    \n    f_name_list.append(f_name)\n    nums_list.append(' '.join(nums))","6f5f8aca":"df_submit = pd.DataFrame({'file_name': f_name_list, 'plates_string': nums_list})\ndf_submit.to_csv('submit.csv', index=False)","b52a3e27":"## prepare the dataset","bc490e46":"## train the recognizer","c1e9e89b":"## prepare the dataset","53b45426":"# 3. Creating a submit","49f9bfce":"# Some global constants","97e9bb7b":"1. No RNN or self-attention.\n2. No mask prediction,just rectangular bounding boxes.\n3. Horizontal squeezing while detecting.","fec38b9a":"# 1. Training the character recognizer","e77c3395":"The mean score of the recognizer can be used to filter out false detections later on. I will use the value of 7.0 for the cutoff","6914f45e":"# 2. Training the detector"}}