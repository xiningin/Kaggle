{"cell_type":{"823377ed":"code","a98558f5":"code","ed53200f":"code","effe50c0":"code","442b2925":"code","9c1e30c7":"code","8601b996":"code","4d37dfe7":"code","0cd4e0f6":"code","e959b260":"code","c5f44201":"code","c973e66c":"code","2d91a594":"code","f357f1c6":"code","7f9dca90":"code","2cc7ea65":"code","2820d460":"code","3c067d86":"code","554a29ca":"code","c75e7540":"code","38ba736c":"code","21484313":"code","785a30d6":"code","5e961b39":"code","4d9492b0":"code","0b4a6a27":"code","b34cf47a":"code","fb587ff6":"code","92b95fe2":"code","4a8d033c":"code","4fd3f662":"code","c23e49b7":"code","c1224787":"code","daa8bce9":"code","fb545711":"code","73e83b05":"code","0fa46599":"code","f95baf76":"code","93f21d1c":"code","c73db868":"code","b5178226":"code","b5330f74":"code","71078129":"code","2a7e7e05":"code","82b3ace7":"code","411a1ce5":"code","904eb997":"code","11084ab0":"code","e781febb":"code","01d595f2":"code","d9888bee":"code","2521a33a":"code","935d7185":"code","28f973a4":"code","b11409a6":"code","ac9f2d1d":"code","cf88514c":"code","491ee218":"code","64b77520":"code","174e599d":"code","c5d1ea5b":"code","c234961f":"code","f9214319":"code","45c2a4f4":"code","5524ac29":"code","30d76618":"code","50e859df":"code","38c11248":"code","882f20ff":"code","8cf60aa6":"code","7544fb5c":"code","9b7a7747":"code","cc066054":"code","d55c1923":"code","0dce5e26":"code","f2d858d5":"code","a157b967":"code","b8de5250":"code","8dcbabb7":"code","84e7bfa7":"code","d128d5e2":"code","a8a994a9":"code","bfedc7df":"code","226fd2ff":"code","b82230d6":"code","b3a84c7a":"code","81f40fe8":"code","25b39f04":"code","042d4a89":"code","cbde9692":"code","b86412fd":"code","7576c05e":"code","f3d0e6b4":"code","070996da":"code","1a785638":"code","7d0f0fa2":"code","8b9749ea":"code","c827ea9a":"code","cf717e43":"code","bade81fa":"code","717647cc":"code","45394a18":"code","8dbc14a2":"code","cb223ead":"code","2a467e96":"code","5ed5ec9b":"code","378bc11c":"code","75c4004f":"code","48a27126":"code","df4fee89":"code","78dccb34":"markdown","9d61e377":"markdown","65967ead":"markdown","b68e5e30":"markdown","1746e788":"markdown","15c2f438":"markdown","4c76b939":"markdown","b64b357f":"markdown","c6ff2752":"markdown","dc743188":"markdown","c2e36d10":"markdown","da178018":"markdown","7d84e4e4":"markdown","44976da3":"markdown","102615d6":"markdown","096e6a5c":"markdown","22c8d53a":"markdown","bd12e909":"markdown","a85e0d6d":"markdown","30094081":"markdown","5575594f":"markdown","b3476f20":"markdown","fd032dcf":"markdown","e03ef1ba":"markdown","6c33e29d":"markdown","2c922f6c":"markdown","9f620989":"markdown","b6dac42f":"markdown","aba46e32":"markdown","8a32c3b2":"markdown","2ff2db6e":"markdown","8d80629c":"markdown","2cc01020":"markdown","3c6974c3":"markdown","a81550a1":"markdown","21b7db69":"markdown","bf78175f":"markdown","7fe094c6":"markdown","fd772545":"markdown","2e5f58a0":"markdown","27a3d144":"markdown","289cf1ba":"markdown","b91206af":"markdown","c3506879":"markdown","77a890d3":"markdown","18d6a5ee":"markdown","6174d542":"markdown","d70034e8":"markdown","da5e99f6":"markdown","3c8a3560":"markdown","e1eba6ce":"markdown","fceba51e":"markdown","f38d03cd":"markdown","9c41c8f5":"markdown"},"source":{"823377ed":"import re\nimport string\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport spacy\nfrom spacy.matcher import Matcher\nfrom spacy import displacy\nfrom spacy.util import minibatch, compounding\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\nfrom wordcloud import WordCloud\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import FunctionTransformer\n\n# from nltk.tokenize import sent_tokenize, word_tokenize\n# from nltk.corpus import stopwords\n# from nltk.stem.wordnet import WordNetLemmatizer\n# from nltk.stem.porter import PorterStemmer\n\nfrom wordcloud import WordCloud\n\nimport matplotlib.pyplot as plt\nfrom plotly import tools\nimport plotly.offline as py\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nimport seaborn as sns\n\nimport plotly.io as pio\npio.renderers","a98558f5":"pd.set_option(\"display.max_rows\", 100)\npd.set_option(\"display.max_columns\",100)\npd.set_option(\"display.max_colwidth\", 500)\npd.set_option('precision', 4)\ncolor = sns.color_palette()","ed53200f":"nlp = spacy.load('en_core_web_lg')\nstopwords = STOP_WORDS\nstopwords.update(set([\"https\",\"www\",\"num\",\"@\",\"#\",\"t\",\"co\",\"s\",\"ll\", 've'])) # adding some stop words to stop word list","effe50c0":"tw_train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntw_test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","442b2925":"tw_train.sample(10)","9c1e30c7":"tw_train.info()","8601b996":"import matplotlib.pyplot as plt\n\n# Pie chart\nlabels = 'Disaster', 'Not a Disaster'\nsizes=[tw_train['target'].loc[tw_train['target']==1].count() \\\n            ,tw_train['target'].loc[tw_train['target']==0].count()]\n\nexplode = (0.1, 0)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n\nfig1, ax1 = plt.subplots()\nfig1.set_size_inches(12.5, 7.5)\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%'\\\n        ,shadow=True, startangle=90,textprops={'fontsize': 14})\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.title(\"Disaster or not Disaster out of \"+str(sum(sizes))+\" Tweets\",\\\n          fontdict={'fontsize': 14, 'fontweight':'bold'})\nplt.show()","4d37dfe7":"unique_tweets=len(tw_train.groupby(['text']).nunique().index)\nnumber_of_tweets=len(tw_train.index)\nprint(f'There are {number_of_tweets-unique_tweets} not unique tweets at train data')\ndf_mislabeled = tw_train.groupby(['text']).nunique().sort_values(by='target', ascending=False)\ndf_mislabeled = df_mislabeled[df_mislabeled['target'] > 1]['target']\nprint(f'There are {len(df_mislabeled.index)} tagged as related and unrelated')","0cd4e0f6":"# Relabeling \n\ntw_train['target'].loc[tw_train['text'] == 'like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit']= 0\ntw_train['target'].loc[tw_train['text'] == 'Hellfire is surrounded by desires so be careful and don\u0089\u00db\u00aat let your desires control you! #Afterlife']= 0\ntw_train['target'].loc[tw_train['text'] == 'To fight bioterrorism sir.']= 0\ntw_train['target'].loc[tw_train['text'] == '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https:\/\/t.co\/rqWuoy1fm4']= 1\ntw_train['target'].loc[tw_train['text'] == 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97\/Georgia Ave Silver Spring']= 1\ntw_train['target'].loc[tw_train['text'] == '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption']= 0\ntw_train['target'].loc[tw_train['text'] == 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!']= 0\ntw_train['target'].loc[tw_train['text'] == 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE']= 1\ntw_train['target'].loc[tw_train['text'] == 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http:\/\/t.co\/JlzK2HdeTG']= 1\ntw_train['target'].loc[tw_train['text'] == \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\"]= 0\ntw_train['target'].loc[tw_train['text'] == \"wowo--=== 12000 Nigerian refugees repatriated from Cameroon\"]= 0\ntw_train['target'].loc[tw_train['text'] == \"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\"]= 0\ntw_train['target'].loc[tw_train['text'] == \"Hellfire! We don\u0089\u00db\u00aat even want to think about it or mention it so let\u0089\u00db\u00aas not do anything that leads to it #islam!\"]= 0\ntw_train['target'].loc[tw_train['text'] == \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\"]= 0\ntw_train['target'].loc[tw_train['text'] == \"Caution: breathing may be hazardous to your health.\"]= 1\ntw_train['target'].loc[tw_train['text'] == \"I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????\"]= 0\ntw_train['target'].loc[tw_train['text'] == \"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\"]= 0\ntw_train['target'].loc[tw_train['text'] == \"that horrible sinking feeling when you\u0089\u00db\u00aave been at home on your phone for a while and you realise its been on 3G this whole time\"]= 0\n","e959b260":"unique_tweets=len(tw_train.groupby(['text']).nunique().index)\nnumber_of_tweets=len(tw_train.index)\nprint(f'There are {number_of_tweets-unique_tweets} not unique tweets at train data')\ndf_mislabeled = tw_train.groupby(['text']).nunique().sort_values(by='target', ascending=False)\ndf_mislabeled = df_mislabeled[df_mislabeled['target'] > 1]['target']\nprint(f'There are {len(df_mislabeled.index)} tagged as related andunrelated')","c5f44201":"# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, figure_size=(15.0,10.0), \n                   title = None, title_size=40, image_color=False):\n    stop_words = STOP_WORDS\n    stop_words = stop_words.union({\"california\",\"wreck\"})\n    \n\n    wordcloud = WordCloud(stopwords=stopwords,background_color='white',      \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n   \nplot_wordcloud(tw_train['text'].loc[tw_train[\"target\"]==1], title=\"Word Cloud of disaster related tweets - Pre-Cleaning\")\nplot_wordcloud(tw_train['text'].loc[tw_train[\"target\"]==0], title=\"Word Cloud of disaster unrelated tweets - Pre-Cleaning\")","c973e66c":"train1_df = tw_train[tw_train[\"target\"]==1]\ntrain0_df = tw_train[tw_train[\"target\"]==0]\ncnt_1 = train1_df['location'].value_counts()\ncnt_1.reset_index()\ncnt_1 = cnt_1[:20,]\n\ncnt_0 = train0_df['location'].value_counts()\ncnt_0.reset_index()\ncnt_0 = cnt_0[:20,]\n\ntrace1 = go.Bar(\n                x = cnt_1.index,\n                y = cnt_1.values,\n                name = \"Number of tweets about real disaster location wise\",\n                marker = dict(color = 'rgba(255, 74, 55, 0.5)',\n                             line=dict(color='rgb(0,0,0)',width=1.5)),\n                )\ntrace0 = go.Bar(\n                x = cnt_0.index,\n                y = cnt_0.values,\n                name = \"Number of tweets other than real disaster location wise\",\n                marker = dict(color = 'rgba(79, 82, 97, 0.5)',\n                             line=dict(color='rgb(0,0,0)',width=1.5)),\n                )\n\n\ndata = [trace0,trace1]\nlayout = go.Layout(barmode = 'stack',title = 'Number of tweets in dataset according to 20 most tweeted locations')\nfig = go.Figure(data = data, layout = layout)\nfig.update_layout(\n    legend=dict(\n        x=0.65,\n        y=1,\n        traceorder=\"normal\",\n        font=dict(\n            family=\"sans-serif\",\n            size=11,\n            color=\"black\"\n        ),\n        bgcolor=\"LightSteelBlue\",\n        bordercolor=\"Black\",\n        borderwidth=2\n    )\n)\nfig.show(renderer=\"kaggle\")","2d91a594":"from collections import defaultdict\ntrain1_df = tw_train[tw_train[\"target\"]==1]\ntrain0_df = tw_train[tw_train[\"target\"]==0]\n\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in stopwords]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n## Get the bar chart from sincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"keyword\"]:\n    for word in generate_ngrams(str(sent)):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(30), 'blue')\n\n## Get the bar chart from insincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"keyword\"]:\n    for word in generate_ngrams(str(sent)):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(30), 'blue')\n\n# Creating two subplots\nfig = make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent keyword if tweet is not real disaster\", \n                                          \"Frequent keyword if tweet is real disaster\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=500, width=1000, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\nfig.show(renderer=\"kaggle\")","f357f1c6":"# creating spacy DOC objects\n\ntw_train['doc_obj'] = list(nlp.pipe(tw_train['text'])) \n","7f9dca90":"tw_test['doc_obj'] = list(nlp.pipe(tw_test['text'])) ","2cc7ea65":"# Remove punctuation\ndef remove_punc(doc):\n  no_punc =  [token for token in doc if token.is_punct == False]\n  return no_punc\n\n# Remove stop words\ndef remove_stop(doc):\n  no_stop = [token for token in doc if token.is_stop == False]\n  return no_stop\n\n# Remove numbers\ndef remove_num(doc):\n  no_num = [token for token in doc if token.like_num == False]\n  return no_num\n\n# Remove url and @user:\ndef remove_url(doc):\n  no_url = [token for token in doc if ((token.like_url == False) & ('@' not in token.text))]\n  return no_url\n\n\n# Lemmatizing + to lower case\ndef lemma_text(doc):\n    tokens=[]\n    for token in doc:      \n        if token.lemma_ != \"-PRON-\":\n            lemma = token.lemma_.lower().strip()\n        else:\n            lemma = token.lower_\n        tokens.append(lemma)\n    return tokens\n\n\n#create_string\ndef create_string(doc):\n  new_string = ' '.join([str(token) for token in doc])\n  return new_string","2820d460":"def clean_all(df):\n    df['clean'] = df['doc_obj'].apply(remove_url)\n    df['clean'] = df['clean'].apply(remove_punc)\n    df['clean'] = df['clean'].apply(remove_stop)\n    df['clean'] = df['clean'].apply(remove_num)\n    df['clean'] = df['clean'].apply(lemma_text)\n    df['clean_text_1'] = df['clean'].apply(create_string)","3c067d86":"clean_all(tw_train)\ntw_train[['text','clean_text_1']].head(15)","554a29ca":"clean_all(tw_test)\n","c75e7540":"tw_train['tuples'] = tw_train.apply(\n    lambda row: (row['clean_text_1'],row['target']), axis=1)\ntrain = tw_train['tuples'].tolist()\ntrain[3:6]","38ba736c":"def load_data(limit=0, split=0.8):\n    train_data = train\n    np.random.shuffle(train_data)\n    train_data = train_data[-limit:]\n    texts, labels = zip(*train_data)\n    cats = [{'Disaster_related': bool(y)} for y in labels]\n    split = int(len(train_data) * split)\n    return (texts[:split], cats[:split]), (texts[split:], cats[split:])\n\ndef evaluate(tokenizer, textcat, texts, cats):\n    docs = (tokenizer(text) for text in texts)\n    tp = 1e-8  # True positives\n    fp = 1e-8  # False positives\n    fn = 1e-8  # False negatives\n    tn = 1e-8  # True negatives\n    for i, doc in enumerate(textcat.pipe(docs)):\n        gold = cats[i]\n        for label, score in doc.cats.items():\n            if label not in gold:\n                continue\n            if score >= 0.5 and gold[label] >= 0.5:\n                tp += 1.\n            elif score >= 0.5 and gold[label] < 0.5:\n                fp += 1.\n            elif score < 0.5 and gold[label] < 0.5:\n                tn += 1\n            elif score < 0.5 and gold[label] >= 0.5:\n                fn += 1\n    precision = tp \/ (tp + fp)\n    recall = tp \/ (tp + fn)\n    f_score = 2 * (precision * recall) \/ (precision + recall)\n    return {'textcat_p': precision, 'textcat_r': recall, 'textcat_f': f_score}\n\n#(\"Number of texts to train from\",\"t\" , int)\nn_texts= len(tw_train)\n#You can increase texts count if you have more computational power.\n\n#(\"Number of training iterations\", \"n\", int))\nn_iter=13","21484313":"# add the text classifier to the pipeline if it doesn't exist\n# nlp.create_pipe works for built-ins that are registered with spaCy\nif 'textcat' not in nlp.pipe_names:\n    textcat = nlp.create_pipe('textcat')\n    nlp.add_pipe(textcat, last=True)\n# otherwise, get it, so we can add labels to it\nelse:\n    textcat = nlp.get_pipe('textcat')\n\n# add label to text classifier\ntextcat.add_label('Disaster_related')\n\n# load the dataset\nprint(\"Loading data...\")\n(train_texts, train_cats), (dev_texts, dev_cats) = load_data(limit=n_texts)\nprint(\"Using {} examples ({} training, {} evaluation)\"\n      .format(n_texts, len(train_texts), len(dev_texts)))\ntrain_data = list(zip(train_texts,\n                      [{'cats': cats} for cats in train_cats]))","785a30d6":"# get names of other pipes to disable them during training\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\nwith nlp.disable_pipes(*other_pipes):  # only train textcat\n    optimizer = nlp.begin_training()\n    print(\"Training the model...\")\n    print('{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('LOSS', 'P', 'R', 'F'))\n    for i in range(n_iter):\n        losses = {}\n        # batch up the examples using spaCy's minibatch\n        batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n        for batch in batches:\n            texts, annotations = zip(*batch)\n            nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n                       losses=losses)\n        with textcat.model.use_params(optimizer.averages):\n            # evaluate on the dev data split off in load_data()\n            scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n        print('{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}'  # print a simple table\n              .format(losses['textcat'], scores['textcat_p'],\n                      scores['textcat_r'], scores['textcat_f']))","5e961b39":"def model_score(doc):\n    with nlp.disable_pipes(*other_pipes):\n        tw_score = nlp(doc).cats.pop(\"Disaster_related\")\n    return tw_score","4d9492b0":"tw_train['model_score'] = tw_train['clean_text_1'].apply(model_score)","0b4a6a27":"tw_test['model_score'] = tw_test['clean_text_1'].apply(model_score)","b34cf47a":"#kaggle submission\ntextcat_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\ntextcat_submission['target'] = np.round(tw_test['model_score']).astype('int')\ntextcat_submission.to_csv('textcat_submission.csv', index=False)\ntextcat_submission.describe()","fb587ff6":"tw_train.loc[(((tw_train[\"model_score\"] > 0.99) & (tw_train[\"target\"] == 0)) | ((tw_train[\"model_score\"] < 0.01) & (tw_train[\"target\"] == 1)))\\\n             ,[\"id\",\"keyword\",\"text\",'clean_text_1',\"model_score\",\"target\"]].head(60)","92b95fe2":"# check if possible mistakes are keyword related\ntw_train.loc[(((tw_train[\"model_score\"] > 0.9) & (tw_train[\"target\"] == 0)) | ((tw_train[\"model_score\"] < 0.1) & (tw_train[\"target\"] == 1)))\\\n             ,[\"id\",\"keyword\",\"text\",'clean_text_1',\"model_score\",\"target\"]].groupby(['keyword','target']).count().sort_values('id',ascending=False).head(20)","4a8d033c":"# this is a Categorizer based tag correction done by manualy checking labeles where model score was > 0.99 and label was 0 or model score was < 0.01 and label was 1 \nid_error = [5193,5564,8489,322,3879,3679,3664,3650,3613,3810,1821,3061,3414,3253,3240,9701,3221,3037,2993,3005,2774,1821,1192,1196,1349,1051,1040,805,1910,796,513,519,442,4043,4911,7174,7356,8786,10130,10702,10729,9933]\ntw_train['target'].loc[(tw_train['id'].isin(id_error))] = tw_train['target'].loc[(tw_train['id'].isin(id_error))].apply(lambda x: 0 if x == 1 else 1)","4fd3f662":"tw_train.to_csv('after_spacy.csv', index=False)","c23e49b7":"def clean(tweet): \n            \n    # Special characters\n    tweet = re.sub(r\"\\x89\u00db_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00d2\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00d3\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00cfWhen\", \"When\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00cf\", \"\", tweet)\n    tweet = re.sub(r\"China\\x89\u00db\u00aas\", \"China's\", tweet)\n    tweet = re.sub(r\"let\\x89\u00db\u00aas\", \"let's\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00f7\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00aa\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\\x9d\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00a2\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00a2\u00e5\u00ca\", \"\", tweet)\n    tweet = re.sub(r\"from\u00e5\u00cawounds\", \"from wounds\", tweet)\n    tweet = re.sub(r\"\u00e5\u00ca\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c8\", \"\", tweet)\n    tweet = re.sub(r\"Jap\u00cc_n\", \"Japan\", tweet)    \n    tweet = re.sub(r\"\u00cc\u00a9\", \"e\", tweet)\n    tweet = re.sub(r\"\u00e5\u00a8\", \"\", tweet)\n    tweet = re.sub(r\"Suru\u00cc\u00a4\", \"Suruc\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c7\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5\u00a33million\", \"3 million\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c0\", \"\", tweet)\n    \n    # Contractions\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"don\\x89\u00db\u00aat\", \"do not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aam\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"you\\x89\u00db\u00aave\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"it\\x89\u00db\u00aas\", \"it is\", tweet)\n    tweet = re.sub(r\"doesn\\x89\u00db\u00aat\", \"does not\", tweet)\n    tweet = re.sub(r\"It\\x89\u00db\u00aas\", \"It is\", tweet)\n    tweet = re.sub(r\"Here\\x89\u00db\u00aas\", \"Here is\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aave\", \"I have\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"can\\x89\u00db\u00aat\", \"cannot\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"wouldn\\x89\u00db\u00aat\", \"would not\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89\u00db\u00aas\", \"That is\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"You\\x89\u00db\u00aare\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89\u00db\u00aat\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89\u00db\u00aat\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89\u00db\u00aall\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aad\", \"I would\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\n    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n    tweet = re.sub(r\"don\u00e5\u00abt\", \"do not\", tweet)   \n            \n    # Character entity references\n    tweet = re.sub(r\"&gt;\", \">\", tweet)\n    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n    \n    # Typos, slang and informal abbreviations\n    tweet = re.sub(r\"w\/e\", \"whatever\", tweet)\n    tweet = re.sub(r\"w\/\", \"with\", tweet)\n    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n    tweet = re.sub(r\"<3\", \"love\", tweet)\n    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n    tweet = re.sub(r\"8\/5\/2015\", \"2015-08-05\", tweet)\n    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n    tweet = re.sub(r\"8\/6\/2015\", \"2015-08-06\", tweet)\n    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)   \n    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n    \n    # Hashtags and usernames\n    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\n    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet) \n    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)     \n    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\n    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\n    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\n    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\n    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\n    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\n    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\n    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\n    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\n    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\n    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\n    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\n    tweet = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", tweet)\n    tweet = re.sub(r\"aRmageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\n    tweet = re.sub(r\"GodsLove\", \"God's Love\", tweet)\n    tweet = re.sub(r\"bookboost\", \"book boost\", tweet)\n    tweet = re.sub(r\"ibooklove\", \"I book love\", tweet)\n    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\n    tweet = re.sub(r\"realDonaldTrump\", \"Donald Trump\", tweet)\n    tweet = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", tweet)\n    tweet = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", tweet)\n    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\n    tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\n    tweet = re.sub(r\"Hostage&2\", \"Hostage & 2\", tweet)\n    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\n    tweet = re.sub(r\"RickPerry\", \"Rick Perry\", tweet)\n    tweet = re.sub(r\"frontpage\", \"front page\", tweet)\n    tweet = re.sub(r\"NewsInTweets\", \"News In Tweets\", tweet)\n    tweet = re.sub(r\"ViralSpell\", \"Viral Spell\", tweet)\n    tweet = re.sub(r\"til_now\", \"until now\", tweet)\n    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\n    tweet = re.sub(r\"ZippedNews\", \"Zipped News\", tweet)\n    tweet = re.sub(r\"MicheleBachman\", \"Michele Bachman\", tweet)\n    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\n    tweet = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", tweet)\n    tweet = re.sub(r\"abstorm\", \"Alberta Storm\", tweet)\n    tweet = re.sub(r\"Beyhive\", \"Beyonce hive\", tweet)\n    tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\n    tweet = re.sub(r\"DETECTADO\", \"Detected\", tweet)\n    tweet = re.sub(r\"RockyFire\", \"Rocky Fire\", tweet)\n    tweet = re.sub(r\"Listen\/Buy\", \"Listen \/ Buy\", tweet)\n    tweet = re.sub(r\"NickCannon\", \"Nick Cannon\", tweet)\n    tweet = re.sub(r\"FaroeIslands\", \"Faroe Islands\", tweet)\n    tweet = re.sub(r\"yycstorm\", \"Calgary Storm\", tweet)\n    tweet = re.sub(r\"IDPs:\", \"Internally Displaced People :\", tweet)\n    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n    tweet = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", tweet)\n    tweet = re.sub(r\"jimmyfallon\", \"jimmy fallon\", tweet)\n    tweet = re.sub(r\"justinbieber\", \"justin bieber\", tweet)  \n    tweet = re.sub(r\"UTC2015\", \"UTC 2015\", tweet)\n    tweet = re.sub(r\"Time2015\", \"Time 2015\", tweet)\n    tweet = re.sub(r\"djicemoon\", \"dj icemoon\", tweet)\n    tweet = re.sub(r\"LivingSafely\", \"Living Safely\", tweet)\n    tweet = re.sub(r\"FIFA16\", \"Fifa 2016\", tweet)\n    tweet = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", tweet)\n    tweet = re.sub(r\"bbcnews\", \"bbc news\", tweet)\n    tweet = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", tweet)\n    tweet = re.sub(r\"c4news\", \"c4 news\", tweet)\n    tweet = re.sub(r\"OBLITERATION\", \"obliteration\", tweet)\n    tweet = re.sub(r\"MUDSLIDE\", \"mudslide\", tweet)\n    tweet = re.sub(r\"NoSurrender\", \"No Surrender\", tweet)\n    tweet = re.sub(r\"NotExplained\", \"Not Explained\", tweet)\n    tweet = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", tweet)\n    tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n    tweet = re.sub(r\"KOTAWeather\", \"KOTA Weather\", tweet)\n    tweet = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", tweet)\n    tweet = re.sub(r\"KOIN6News\", \"KOIN 6 News\", tweet)\n    tweet = re.sub(r\"LiveOnK2\", \"Live On K2\", tweet)\n    tweet = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", tweet)\n    tweet = re.sub(r\"nikeplus\", \"nike plus\", tweet)\n    tweet = re.sub(r\"david_cameron\", \"David Cameron\", tweet)\n    tweet = re.sub(r\"peterjukes\", \"Peter Jukes\", tweet)\n    tweet = re.sub(r\"JamesMelville\", \"James Melville\", tweet)\n    tweet = re.sub(r\"megynkelly\", \"Megyn Kelly\", tweet)\n    tweet = re.sub(r\"cnewslive\", \"C News Live\", tweet)\n    tweet = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", tweet)\n    tweet = re.sub(r\"TweetLikeItsSeptember11th2001\", \"Tweet like it is september 11th 2001\", tweet)\n    tweet = re.sub(r\"cbplawyers\", \"cbp lawyers\", tweet)\n    tweet = re.sub(r\"fewmoretweets\", \"few more tweets\", tweet)\n    tweet = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", tweet)\n    tweet = re.sub(r\"cjoyner\", \"Chris Joyner\", tweet)\n    tweet = re.sub(r\"ENGvAUS\", \"England vs Australia\", tweet)\n    tweet = re.sub(r\"ScottWalker\", \"Scott Walker\", tweet)\n    tweet = re.sub(r\"MikeParrActor\", \"Michael Parr\", tweet)\n    tweet = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", tweet)\n    tweet = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", tweet)\n    tweet = re.sub(r\"realmandyrain\", \"Mandy Rain\", tweet)\n    tweet = re.sub(r\"GraysonDolan\", \"Grayson Dolan\", tweet)\n    tweet = re.sub(r\"ApolloBrown\", \"Apollo Brown\", tweet)\n    tweet = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", tweet)\n    tweet = re.sub(r\"TontitownGrape\", \"Tontitown Grape\", tweet)\n    tweet = re.sub(r\"AbbsWinston\", \"Abbs Winston\", tweet)\n    tweet = re.sub(r\"ShaunKing\", \"Shaun King\", tweet)\n    tweet = re.sub(r\"MeekMill\", \"Meek Mill\", tweet)\n    tweet = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", tweet)\n    tweet = re.sub(r\"GRupdates\", \"GR updates\", tweet)\n    tweet = re.sub(r\"SouthDowns\", \"South Downs\", tweet)\n    tweet = re.sub(r\"braininjury\", \"brain injury\", tweet)\n    tweet = re.sub(r\"auspol\", \"Australian politics\", tweet)\n    tweet = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", tweet)\n    tweet = re.sub(r\"calgaryweather\", \"Calgary Weather\", tweet)\n    tweet = re.sub(r\"weallheartonedirection\", \"we all heart one direction\", tweet)\n    tweet = re.sub(r\"edsheeran\", \"Ed Sheeran\", tweet)\n    tweet = re.sub(r\"TrueHeroes\", \"True Heroes\", tweet)\n    tweet = re.sub(r\"S3XLEAK\", \"sex leak\", tweet)\n    tweet = re.sub(r\"ComplexMag\", \"Complex Magazine\", tweet)\n    tweet = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", tweet)\n    tweet = re.sub(r\"CityofCalgary\", \"City of Calgary\", tweet)\n    tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\n    tweet = re.sub(r\"SummerFate\", \"Summer Fate\", tweet)\n    tweet = re.sub(r\"RAmag\", \"Royal Academy Magazine\", tweet)\n    tweet = re.sub(r\"offers2go\", \"offers to go\", tweet)\n    tweet = re.sub(r\"foodscare\", \"food scare\", tweet)\n    tweet = re.sub(r\"MNPDNashville\", \"Metropolitan Nashville Police Department\", tweet)\n    tweet = re.sub(r\"TfLBusAlerts\", \"TfL Bus Alerts\", tweet)\n    tweet = re.sub(r\"GamerGate\", \"Gamer Gate\", tweet)\n    tweet = re.sub(r\"IHHen\", \"Humanitarian Relief\", tweet)\n    tweet = re.sub(r\"spinningbot\", \"spinning bot\", tweet)\n    tweet = re.sub(r\"ModiMinistry\", \"Modi Ministry\", tweet)\n    tweet = re.sub(r\"TAXIWAYS\", \"taxi ways\", tweet)\n    tweet = re.sub(r\"Calum5SOS\", \"Calum Hood\", tweet)\n    tweet = re.sub(r\"po_st\", \"po.st\", tweet)\n    tweet = re.sub(r\"scoopit\", \"scoop.it\", tweet)\n    tweet = re.sub(r\"UltimaLucha\", \"Ultima Lucha\", tweet)\n    tweet = re.sub(r\"JonathanFerrell\", \"Jonathan Ferrell\", tweet)\n    tweet = re.sub(r\"aria_ahrary\", \"Aria Ahrary\", tweet)\n    tweet = re.sub(r\"rapidcity\", \"Rapid City\", tweet)\n    tweet = re.sub(r\"OutBid\", \"outbid\", tweet)\n    tweet = re.sub(r\"lavenderpoetrycafe\", \"lavender poetry cafe\", tweet)\n    tweet = re.sub(r\"EudryLantiqua\", \"Eudry Lantiqua\", tweet)\n    tweet = re.sub(r\"15PM\", \"15 PM\", tweet)\n    tweet = re.sub(r\"OriginalFunko\", \"Funko\", tweet)\n    tweet = re.sub(r\"rightwaystan\", \"Richard Tan\", tweet)\n    tweet = re.sub(r\"CindyNoonan\", \"Cindy Noonan\", tweet)\n    tweet = re.sub(r\"RT_America\", \"RT America\", tweet)\n    tweet = re.sub(r\"narendramodi\", \"Narendra Modi\", tweet)\n    tweet = re.sub(r\"BakeOffFriends\", \"Bake Off Friends\", tweet)\n    tweet = re.sub(r\"TeamHendrick\", \"Hendrick Motorsports\", tweet)\n    tweet = re.sub(r\"alexbelloli\", \"Alex Belloli\", tweet)\n    tweet = re.sub(r\"itsjustinstuart\", \"Justin Stuart\", tweet)\n    tweet = re.sub(r\"gunsense\", \"gun sense\", tweet)\n    tweet = re.sub(r\"DebateQuestionsWeWantToHear\", \"debate questions we want to hear\", tweet)\n    tweet = re.sub(r\"RoyalCarribean\", \"Royal Carribean\", tweet)\n    tweet = re.sub(r\"samanthaturne19\", \"Samantha Turner\", tweet)\n    tweet = re.sub(r\"JonVoyage\", \"Jon Stewart\", tweet)\n    tweet = re.sub(r\"renew911health\", \"renew 911 health\", tweet)\n    tweet = re.sub(r\"SuryaRay\", \"Surya Ray\", tweet)\n    tweet = re.sub(r\"pattonoswalt\", \"Patton Oswalt\", tweet)\n    tweet = re.sub(r\"minhazmerchant\", \"Minhaz Merchant\", tweet)\n    tweet = re.sub(r\"TLVFaces\", \"Israel Diaspora Coalition\", tweet)\n    tweet = re.sub(r\"pmarca\", \"Marc Andreessen\", tweet)\n    tweet = re.sub(r\"pdx911\", \"Portland Police\", tweet)\n    tweet = re.sub(r\"jamaicaplain\", \"Jamaica Plain\", tweet)\n    tweet = re.sub(r\"Japton\", \"Arkansas\", tweet)\n    tweet = re.sub(r\"RouteComplex\", \"Route Complex\", tweet)\n    tweet = re.sub(r\"INSubcontinent\", \"Indian Subcontinent\", tweet)\n    tweet = re.sub(r\"NJTurnpike\", \"New Jersey Turnpike\", tweet)\n    tweet = re.sub(r\"Politifiact\", \"PolitiFact\", tweet)\n    tweet = re.sub(r\"Hiroshima70\", \"Hiroshima\", tweet)\n    tweet = re.sub(r\"GMMBC\", \"Greater Mt Moriah Baptist Church\", tweet)\n    tweet = re.sub(r\"versethe\", \"verse the\", tweet)\n    tweet = re.sub(r\"TubeStrike\", \"Tube Strike\", tweet)\n    tweet = re.sub(r\"MissionHills\", \"Mission Hills\", tweet)\n    tweet = re.sub(r\"ProtectDenaliWolves\", \"Protect Denali Wolves\", tweet)\n    tweet = re.sub(r\"NANKANA\", \"Nankana\", tweet)\n    tweet = re.sub(r\"SAHIB\", \"Sahib\", tweet)\n    tweet = re.sub(r\"PAKPATTAN\", \"Pakpattan\", tweet)\n    tweet = re.sub(r\"Newz_Sacramento\", \"News Sacramento\", tweet)\n    tweet = re.sub(r\"gofundme\", \"go fund me\", tweet)\n    tweet = re.sub(r\"pmharper\", \"Stephen Harper\", tweet)\n    tweet = re.sub(r\"IvanBerroa\", \"Ivan Berroa\", tweet)\n    tweet = re.sub(r\"LosDelSonido\", \"Los Del Sonido\", tweet)\n    tweet = re.sub(r\"bancodeseries\", \"banco de series\", tweet)\n    tweet = re.sub(r\"timkaine\", \"Tim Kaine\", tweet)\n    tweet = re.sub(r\"IdentityTheft\", \"Identity Theft\", tweet)\n    tweet = re.sub(r\"AllLivesMatter\", \"All Lives Matter\", tweet)\n    tweet = re.sub(r\"mishacollins\", \"Misha Collins\", tweet)\n    tweet = re.sub(r\"BillNeelyNBC\", \"Bill Neely\", tweet)\n    tweet = re.sub(r\"BeClearOnCancer\", \"be clear on cancer\", tweet)\n    tweet = re.sub(r\"Kowing\", \"Knowing\", tweet)\n    tweet = re.sub(r\"ScreamQueens\", \"Scream Queens\", tweet)\n    tweet = re.sub(r\"AskCharley\", \"Ask Charley\", tweet)\n    tweet = re.sub(r\"BlizzHeroes\", \"Heroes of the Storm\", tweet)\n    tweet = re.sub(r\"BradleyBrad47\", \"Bradley Brad\", tweet)\n    tweet = re.sub(r\"HannaPH\", \"Typhoon Hanna\", tweet)\n    tweet = re.sub(r\"meinlcymbals\", \"MEINL Cymbals\", tweet)\n    tweet = re.sub(r\"Ptbo\", \"Peterborough\", tweet)\n    tweet = re.sub(r\"cnnbrk\", \"CNN Breaking News\", tweet)\n    tweet = re.sub(r\"IndianNews\", \"Indian News\", tweet)\n    tweet = re.sub(r\"savebees\", \"save bees\", tweet)\n    tweet = re.sub(r\"GreenHarvard\", \"Green Harvard\", tweet)\n    tweet = re.sub(r\"StandwithPP\", \"Stand with planned parenthood\", tweet)\n    tweet = re.sub(r\"hermancranston\", \"Herman Cranston\", tweet)\n    tweet = re.sub(r\"WMUR9\", \"WMUR-TV\", tweet)\n    tweet = re.sub(r\"RockBottomRadFM\", \"Rock Bottom Radio\", tweet)\n    tweet = re.sub(r\"ameenshaikh3\", \"Ameen Shaikh\", tweet)\n    tweet = re.sub(r\"ProSyn\", \"Project Syndicate\", tweet)\n    tweet = re.sub(r\"Daesh\", \"ISIS\", tweet)\n    tweet = re.sub(r\"s2g\", \"swear to god\", tweet)\n    tweet = re.sub(r\"listenlive\", \"listen live\", tweet)\n    tweet = re.sub(r\"CDCgov\", \"Centers for Disease Control and Prevention\", tweet)\n    tweet = re.sub(r\"FoxNew\", \"Fox News\", tweet)\n    tweet = re.sub(r\"CBSBigBrother\", \"Big Brother\", tweet)\n    tweet = re.sub(r\"JulieDiCaro\", \"Julie DiCaro\", tweet)\n    tweet = re.sub(r\"theadvocatemag\", \"The Advocate Magazine\", tweet)\n    tweet = re.sub(r\"RohnertParkDPS\", \"Rohnert Park Police Department\", tweet)\n    tweet = re.sub(r\"THISIZBWRIGHT\", \"Bonnie Wright\", tweet)\n    tweet = re.sub(r\"Popularmmos\", \"Popular MMOs\", tweet)\n    tweet = re.sub(r\"WildHorses\", \"Wild Horses\", tweet)\n    tweet = re.sub(r\"FantasticFour\", \"Fantastic Four\", tweet)\n    tweet = re.sub(r\"HORNDALE\", \"Horndale\", tweet)\n    tweet = re.sub(r\"PINER\", \"Piner\", tweet)\n    tweet = re.sub(r\"BathAndNorthEastSomerset\", \"Bath and North East Somerset\", tweet)\n    tweet = re.sub(r\"thatswhatfriendsarefor\", \"that is what friends are for\", tweet)\n    tweet = re.sub(r\"residualincome\", \"residual income\", tweet)\n    tweet = re.sub(r\"YahooNewsDigest\", \"Yahoo News Digest\", tweet)\n    tweet = re.sub(r\"MalaysiaAirlines\", \"Malaysia Airlines\", tweet)\n    tweet = re.sub(r\"AmazonDeals\", \"Amazon Deals\", tweet)\n    tweet = re.sub(r\"MissCharleyWebb\", \"Charley Webb\", tweet)\n    tweet = re.sub(r\"shoalstraffic\", \"shoals traffic\", tweet)\n    tweet = re.sub(r\"GeorgeFoster72\", \"George Foster\", tweet)\n    tweet = re.sub(r\"pop2015\", \"pop 2015\", tweet)\n    tweet = re.sub(r\"_PokemonCards_\", \"Pokemon Cards\", tweet)\n    tweet = re.sub(r\"DianneG\", \"Dianne Gallagher\", tweet)\n    tweet = re.sub(r\"KashmirConflict\", \"Kashmir Conflict\", tweet)\n    tweet = re.sub(r\"BritishBakeOff\", \"British Bake Off\", tweet)\n    tweet = re.sub(r\"FreeKashmir\", \"Free Kashmir\", tweet)\n    tweet = re.sub(r\"mattmosley\", \"Matt Mosley\", tweet)\n    tweet = re.sub(r\"BishopFred\", \"Bishop Fred\", tweet)\n    tweet = re.sub(r\"EndConflict\", \"End Conflict\", tweet)\n    tweet = re.sub(r\"EndOccupation\", \"End Occupation\", tweet)\n    tweet = re.sub(r\"UNHEALED\", \"unhealed\", tweet)\n    tweet = re.sub(r\"CharlesDagnall\", \"Charles Dagnall\", tweet)\n    tweet = re.sub(r\"Latestnews\", \"Latest news\", tweet)\n    tweet = re.sub(r\"KindleCountdown\", \"Kindle Countdown\", tweet)\n    tweet = re.sub(r\"NoMoreHandouts\", \"No More Handouts\", tweet)\n    tweet = re.sub(r\"datingtips\", \"dating tips\", tweet)\n    tweet = re.sub(r\"charlesadler\", \"Charles Adler\", tweet)\n    tweet = re.sub(r\"twia\", \"Texas Windstorm Insurance Association\", tweet)\n    tweet = re.sub(r\"txlege\", \"Texas Legislature\", tweet)\n    tweet = re.sub(r\"WindstormInsurer\", \"Windstorm Insurer\", tweet)\n    tweet = re.sub(r\"Newss\", \"News\", tweet)\n    tweet = re.sub(r\"hempoil\", \"hemp oil\", tweet)\n    tweet = re.sub(r\"CommoditiesAre\", \"Commodities are\", tweet)\n    tweet = re.sub(r\"tubestrike\", \"tube strike\", tweet)\n    tweet = re.sub(r\"JoeNBC\", \"Joe Scarborough\", tweet)\n    tweet = re.sub(r\"LiteraryCakes\", \"Literary Cakes\", tweet)\n    tweet = re.sub(r\"TI5\", \"The International 5\", tweet)\n    tweet = re.sub(r\"thehill\", \"the hill\", tweet)\n    tweet = re.sub(r\"3others\", \"3 others\", tweet)\n    tweet = re.sub(r\"stighefootball\", \"Sam Tighe\", tweet)\n    tweet = re.sub(r\"whatstheimportantvideo\", \"what is the important video\", tweet)\n    tweet = re.sub(r\"ClaudioMeloni\", \"Claudio Meloni\", tweet)\n    tweet = re.sub(r\"DukeSkywalker\", \"Duke Skywalker\", tweet)\n    tweet = re.sub(r\"carsonmwr\", \"Fort Carson\", tweet)\n    tweet = re.sub(r\"offdishduty\", \"off dish duty\", tweet)\n    tweet = re.sub(r\"andword\", \"and word\", tweet)\n    tweet = re.sub(r\"rhodeisland\", \"Rhode Island\", tweet)\n    tweet = re.sub(r\"easternoregon\", \"Eastern Oregon\", tweet)\n    tweet = re.sub(r\"WAwildfire\", \"Washington Wildfire\", tweet)\n    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n    tweet = re.sub(r\"57am\", \"57 am\", tweet)\n    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n    tweet = re.sub(r\"JacobHoggard\", \"Jacob Hoggard\", tweet)\n    tweet = re.sub(r\"newnewnew\", \"new new new\", tweet)\n    tweet = re.sub(r\"under50\", \"under 50\", tweet)\n    tweet = re.sub(r\"getitbeforeitsgone\", \"get it before it is gone\", tweet)\n    tweet = re.sub(r\"freshoutofthebox\", \"fresh out of the box\", tweet)\n    tweet = re.sub(r\"amwriting\", \"am writing\", tweet)\n    tweet = re.sub(r\"Bokoharm\", \"Boko Haram\", tweet)\n    tweet = re.sub(r\"Nowlike\", \"Now like\", tweet)\n    tweet = re.sub(r\"seasonfrom\", \"season from\", tweet)\n    tweet = re.sub(r\"epicente\", \"epicenter\", tweet)\n    tweet = re.sub(r\"epicenterr\", \"epicenter\", tweet)\n    tweet = re.sub(r\"sicklife\", \"sick life\", tweet)\n    tweet = re.sub(r\"yycweather\", \"Calgary Weather\", tweet)\n    tweet = re.sub(r\"calgarysun\", \"Calgary Sun\", tweet)\n    tweet = re.sub(r\"approachng\", \"approaching\", tweet)\n    tweet = re.sub(r\"evng\", \"evening\", tweet)\n    tweet = re.sub(r\"Sumthng\", \"something\", tweet)\n    tweet = re.sub(r\"EllenPompeo\", \"Ellen Pompeo\", tweet)\n    tweet = re.sub(r\"shondarhimes\", \"Shonda Rhimes\", tweet)\n    tweet = re.sub(r\"ABCNetwork\", \"ABC Network\", tweet)\n    tweet = re.sub(r\"SushmaSwaraj\", \"Sushma Swaraj\", tweet)\n    tweet = re.sub(r\"pray4japan\", \"Pray for Japan\", tweet)\n    tweet = re.sub(r\"hope4japan\", \"Hope for Japan\", tweet)\n    tweet = re.sub(r\"Illusionimagess\", \"Illusion images\", tweet)\n    tweet = re.sub(r\"SummerUnderTheStars\", \"Summer Under The Stars\", tweet)\n    tweet = re.sub(r\"ShallWeDance\", \"Shall We Dance\", tweet)\n    tweet = re.sub(r\"TCMParty\", \"TCM Party\", tweet)\n    tweet = re.sub(r\"marijuananews\", \"marijuana news\", tweet)\n    tweet = re.sub(r\"onbeingwithKristaTippett\", \"on being with Krista Tippett\", tweet)\n    tweet = re.sub(r\"Beingtweets\", \"Being tweets\", tweet)\n    tweet = re.sub(r\"newauthors\", \"new authors\", tweet)\n    tweet = re.sub(r\"remedyyyy\", \"remedy\", tweet)\n    tweet = re.sub(r\"44PM\", \"44 PM\", tweet)\n    tweet = re.sub(r\"HeadlinesApp\", \"Headlines App\", tweet)\n    tweet = re.sub(r\"40PM\", \"40 PM\", tweet)\n    tweet = re.sub(r\"myswc\", \"Severe Weather Center\", tweet)\n    tweet = re.sub(r\"ithats\", \"that is\", tweet)\n    tweet = re.sub(r\"icouldsitinthismomentforever\", \"I could sit in this moment forever\", tweet)\n    tweet = re.sub(r\"FatLoss\", \"Fat Loss\", tweet)\n    tweet = re.sub(r\"02PM\", \"02 PM\", tweet)\n    tweet = re.sub(r\"MetroFmTalk\", \"Metro Fm Talk\", tweet)\n    tweet = re.sub(r\"Bstrd\", \"bastard\", tweet)\n    tweet = re.sub(r\"bldy\", \"bloody\", tweet)\n    tweet = re.sub(r\"MetrofmTalk\", \"Metro Fm Talk\", tweet)\n    tweet = re.sub(r\"terrorismturn\", \"terrorism turn\", tweet)\n    tweet = re.sub(r\"BBCNewsAsia\", \"BBC News Asia\", tweet)\n    tweet = re.sub(r\"BehindTheScenes\", \"Behind The Scenes\", tweet)\n    tweet = re.sub(r\"GeorgeTakei\", \"George Takei\", tweet)\n    tweet = re.sub(r\"WomensWeeklyMag\", \"Womens Weekly Magazine\", tweet)\n    tweet = re.sub(r\"SurvivorsGuidetoEarth\", \"Survivors Guide to Earth\", tweet)\n    tweet = re.sub(r\"incubusband\", \"incubus band\", tweet)\n    tweet = re.sub(r\"Babypicturethis\", \"Baby picture this\", tweet)\n    tweet = re.sub(r\"BombEffects\", \"Bomb Effects\", tweet)\n    tweet = re.sub(r\"win10\", \"Windows 10\", tweet)\n    tweet = re.sub(r\"idkidk\", \"I do not know I do not know\", tweet)\n    tweet = re.sub(r\"TheWalkingDead\", \"The Walking Dead\", tweet)\n    tweet = re.sub(r\"amyschumer\", \"Amy Schumer\", tweet)\n    tweet = re.sub(r\"crewlist\", \"crew list\", tweet)\n    tweet = re.sub(r\"Erdogans\", \"Erdogan\", tweet)\n    tweet = re.sub(r\"BBCLive\", \"BBC Live\", tweet)\n    tweet = re.sub(r\"TonyAbbottMHR\", \"Tony Abbott\", tweet)\n    tweet = re.sub(r\"paulmyerscough\", \"Paul Myerscough\", tweet)\n    tweet = re.sub(r\"georgegallagher\", \"George Gallagher\", tweet)\n    tweet = re.sub(r\"JimmieJohnson\", \"Jimmie Johnson\", tweet)\n    tweet = re.sub(r\"pctool\", \"pc tool\", tweet)\n    tweet = re.sub(r\"DoingHashtagsRight\", \"Doing Hashtags Right\", tweet)\n    tweet = re.sub(r\"ThrowbackThursday\", \"Throwback Thursday\", tweet)\n    tweet = re.sub(r\"SnowBackSunday\", \"Snowback Sunday\", tweet)\n    tweet = re.sub(r\"LakeEffect\", \"Lake Effect\", tweet)\n    tweet = re.sub(r\"RTphotographyUK\", \"Richard Thomas Photography UK\", tweet)\n    tweet = re.sub(r\"BigBang_CBS\", \"Big Bang CBS\", tweet)\n    tweet = re.sub(r\"writerslife\", \"writers life\", tweet)\n    tweet = re.sub(r\"NaturalBirth\", \"Natural Birth\", tweet)\n    tweet = re.sub(r\"UnusualWords\", \"Unusual Words\", tweet)\n    tweet = re.sub(r\"wizkhalifa\", \"Wiz Khalifa\", tweet)\n    tweet = re.sub(r\"acreativedc\", \"a creative DC\", tweet)\n    tweet = re.sub(r\"vscodc\", \"vsco DC\", tweet)\n    tweet = re.sub(r\"VSCOcam\", \"vsco camera\", tweet)\n    tweet = re.sub(r\"TheBEACHDC\", \"The beach DC\", tweet)\n    tweet = re.sub(r\"buildingmuseum\", \"building museum\", tweet)\n    tweet = re.sub(r\"WorldOil\", \"World Oil\", tweet)\n    tweet = re.sub(r\"redwedding\", \"red wedding\", tweet)\n    tweet = re.sub(r\"AmazingRaceCanada\", \"Amazing Race Canada\", tweet)\n    tweet = re.sub(r\"WakeUpAmerica\", \"Wake Up America\", tweet)\n    tweet = re.sub(r\"\\\\Allahuakbar\\\\\", \"Allahu Akbar\", tweet)\n    tweet = re.sub(r\"bleased\", \"blessed\", tweet)\n    tweet = re.sub(r\"nigeriantribune\", \"Nigerian Tribune\", tweet)\n    tweet = re.sub(r\"HIDEO_KOJIMA_EN\", \"Hideo Kojima\", tweet)\n    tweet = re.sub(r\"FusionFestival\", \"Fusion Festival\", tweet)\n    tweet = re.sub(r\"50Mixed\", \"50 Mixed\", tweet)\n    tweet = re.sub(r\"NoAgenda\", \"No Agenda\", tweet)\n    tweet = re.sub(r\"WhiteGenocide\", \"White Genocide\", tweet)\n    tweet = re.sub(r\"dirtylying\", \"dirty lying\", tweet)\n    tweet = re.sub(r\"SyrianRefugees\", \"Syrian Refugees\", tweet)\n    tweet = re.sub(r\"changetheworld\", \"change the world\", tweet)\n    tweet = re.sub(r\"Ebolacase\", \"Ebola case\", tweet)\n    tweet = re.sub(r\"mcgtech\", \"mcg technologies\", tweet)\n    tweet = re.sub(r\"withweapons\", \"with weapons\", tweet)\n    tweet = re.sub(r\"advancedwarfare\", \"advanced warfare\", tweet)\n    tweet = re.sub(r\"letsFootball\", \"let us Football\", tweet)\n    tweet = re.sub(r\"LateNiteMix\", \"late night mix\", tweet)\n    tweet = re.sub(r\"PhilCollinsFeed\", \"Phil Collins\", tweet)\n    tweet = re.sub(r\"RudyHavenstein\", \"Rudy Havenstein\", tweet)\n    tweet = re.sub(r\"22PM\", \"22 PM\", tweet)\n    tweet = re.sub(r\"54am\", \"54 AM\", tweet)\n    tweet = re.sub(r\"38am\", \"38 AM\", tweet)\n    tweet = re.sub(r\"OldFolkExplainStuff\", \"Old Folk Explain Stuff\", tweet)\n    tweet = re.sub(r\"BlacklivesMatter\", \"Black Lives Matter\", tweet)\n    tweet = re.sub(r\"InsaneLimits\", \"Insane Limits\", tweet)\n    tweet = re.sub(r\"youcantsitwithus\", \"you cannot sit with us\", tweet)\n    tweet = re.sub(r\"2k15\", \"2015\", tweet)\n    tweet = re.sub(r\"TheIran\", \"Iran\", tweet)\n    tweet = re.sub(r\"JimmyFallon\", \"Jimmy Fallon\", tweet)\n    tweet = re.sub(r\"AlbertBrooks\", \"Albert Brooks\", tweet)\n    tweet = re.sub(r\"defense_news\", \"defense news\", tweet)\n    tweet = re.sub(r\"nuclearrcSA\", \"Nuclear Risk Control Self Assessment\", tweet)\n    tweet = re.sub(r\"Auspol\", \"Australia Politics\", tweet)\n    tweet = re.sub(r\"NuclearPower\", \"Nuclear Power\", tweet)\n    tweet = re.sub(r\"WhiteTerrorism\", \"White Terrorism\", tweet)\n    tweet = re.sub(r\"truthfrequencyradio\", \"Truth Frequency Radio\", tweet)\n    tweet = re.sub(r\"ErasureIsNotEquality\", \"Erasure is not equality\", tweet)\n    tweet = re.sub(r\"ProBonoNews\", \"Pro Bono News\", tweet)\n    tweet = re.sub(r\"JakartaPost\", \"Jakarta Post\", tweet)\n    tweet = re.sub(r\"toopainful\", \"too painful\", tweet)\n    tweet = re.sub(r\"melindahaunton\", \"Melinda Haunton\", tweet)\n    tweet = re.sub(r\"NoNukes\", \"No Nukes\", tweet)\n    tweet = re.sub(r\"curryspcworld\", \"Currys PC World\", tweet)\n    tweet = re.sub(r\"ineedcake\", \"I need cake\", tweet)\n    tweet = re.sub(r\"blackforestgateau\", \"black forest gateau\", tweet)\n    tweet = re.sub(r\"BBCOne\", \"BBC One\", tweet)\n    tweet = re.sub(r\"AlexxPage\", \"Alex Page\", tweet)\n    tweet = re.sub(r\"jonathanserrie\", \"Jonathan Serrie\", tweet)\n    tweet = re.sub(r\"SocialJerkBlog\", \"Social Jerk Blog\", tweet)\n    tweet = re.sub(r\"ChelseaVPeretti\", \"Chelsea Peretti\", tweet)\n    tweet = re.sub(r\"irongiant\", \"iron giant\", tweet)\n    tweet = re.sub(r\"RonFunches\", \"Ron Funches\", tweet)\n    tweet = re.sub(r\"TimCook\", \"Tim Cook\", tweet)\n    tweet = re.sub(r\"sebastianstanisaliveandwell\", \"Sebastian Stan is alive and well\", tweet)\n    tweet = re.sub(r\"Madsummer\", \"Mad summer\", tweet)\n    tweet = re.sub(r\"NowYouKnow\", \"Now you know\", tweet)\n    tweet = re.sub(r\"concertphotography\", \"concert photography\", tweet)\n    tweet = re.sub(r\"TomLandry\", \"Tom Landry\", tweet)\n    tweet = re.sub(r\"showgirldayoff\", \"show girl day off\", tweet)\n    tweet = re.sub(r\"Yougslavia\", \"Yugoslavia\", tweet)\n    tweet = re.sub(r\"QuantumDataInformatics\", \"Quantum Data Informatics\", tweet)\n    tweet = re.sub(r\"FromTheDesk\", \"From The Desk\", tweet)\n    tweet = re.sub(r\"TheaterTrial\", \"Theater Trial\", tweet)\n    tweet = re.sub(r\"CatoInstitute\", \"Cato Institute\", tweet)\n    tweet = re.sub(r\"EmekaGift\", \"Emeka Gift\", tweet)\n    tweet = re.sub(r\"LetsBe_Rational\", \"Let us be rational\", tweet)\n    tweet = re.sub(r\"Cynicalreality\", \"Cynical reality\", tweet)\n    tweet = re.sub(r\"FredOlsenCruise\", \"Fred Olsen Cruise\", tweet)\n    tweet = re.sub(r\"NotSorry\", \"not sorry\", tweet)\n    tweet = re.sub(r\"UseYourWords\", \"use your words\", tweet)\n    tweet = re.sub(r\"WordoftheDay\", \"word of the day\", tweet)\n    tweet = re.sub(r\"Dictionarycom\", \"Dictionary.com\", tweet)\n    tweet = re.sub(r\"TheBrooklynLife\", \"The Brooklyn Life\", tweet)\n    tweet = re.sub(r\"jokethey\", \"joke they\", tweet)\n    tweet = re.sub(r\"nflweek1picks\", \"NFL week 1 picks\", tweet)\n    tweet = re.sub(r\"uiseful\", \"useful\", tweet)\n    tweet = re.sub(r\"JusticeDotOrg\", \"The American Association for Justice\", tweet)\n    tweet = re.sub(r\"autoaccidents\", \"auto accidents\", tweet)\n    tweet = re.sub(r\"SteveGursten\", \"Steve Gursten\", tweet)\n    tweet = re.sub(r\"MichiganAutoLaw\", \"Michigan Auto Law\", tweet)\n    tweet = re.sub(r\"birdgang\", \"bird gang\", tweet)\n    tweet = re.sub(r\"nflnetwork\", \"NFL Network\", tweet)\n    tweet = re.sub(r\"NYDNSports\", \"NY Daily News Sports\", tweet)\n    tweet = re.sub(r\"RVacchianoNYDN\", \"Ralph Vacchiano NY Daily News\", tweet)\n    tweet = re.sub(r\"EdmontonEsks\", \"Edmonton Eskimos\", tweet)\n    tweet = re.sub(r\"david_brelsford\", \"David Brelsford\", tweet)\n    tweet = re.sub(r\"TOI_India\", \"The Times of India\", tweet)\n    tweet = re.sub(r\"hegot\", \"he got\", tweet)\n    tweet = re.sub(r\"SkinsOn9\", \"Skins on 9\", tweet)\n    tweet = re.sub(r\"sothathappened\", \"so that happened\", tweet)\n    tweet = re.sub(r\"LCOutOfDoors\", \"LC Out Of Doors\", tweet)\n    tweet = re.sub(r\"NationFirst\", \"Nation First\", tweet)\n    tweet = re.sub(r\"IndiaToday\", \"India Today\", tweet)\n    tweet = re.sub(r\"HLPS\", \"helps\", tweet)\n    tweet = re.sub(r\"HOSTAGESTHROSW\", \"hostages throw\", tweet)\n    tweet = re.sub(r\"SNCTIONS\", \"sanctions\", tweet)\n    tweet = re.sub(r\"BidTime\", \"Bid Time\", tweet)\n    tweet = re.sub(r\"crunchysensible\", \"crunchy sensible\", tweet)\n    tweet = re.sub(r\"RandomActsOfRomance\", \"Random acts of romance\", tweet)\n    tweet = re.sub(r\"MomentsAtHill\", \"Moments at hill\", tweet)\n    tweet = re.sub(r\"eatshit\", \"eat shit\", tweet)\n    tweet = re.sub(r\"liveleakfun\", \"live leak fun\", tweet)\n    tweet = re.sub(r\"SahelNews\", \"Sahel News\", tweet)\n    tweet = re.sub(r\"abc7newsbayarea\", \"ABC 7 News Bay Area\", tweet)\n    tweet = re.sub(r\"facilitiesmanagement\", \"facilities management\", tweet)\n    tweet = re.sub(r\"facilitydude\", \"facility dude\", tweet)\n    tweet = re.sub(r\"CampLogistics\", \"Camp logistics\", tweet)\n    tweet = re.sub(r\"alaskapublic\", \"Alaska public\", tweet)\n    tweet = re.sub(r\"MarketResearch\", \"Market Research\", tweet)\n    tweet = re.sub(r\"AccuracyEsports\", \"Accuracy Esports\", tweet)\n    tweet = re.sub(r\"TheBodyShopAust\", \"The Body Shop Australia\", tweet)\n    tweet = re.sub(r\"yychail\", \"Calgary hail\", tweet)\n    tweet = re.sub(r\"yyctraffic\", \"Calgary traffic\", tweet)\n    tweet = re.sub(r\"eliotschool\", \"eliot school\", tweet)\n    tweet = re.sub(r\"TheBrokenCity\", \"The Broken City\", tweet)\n    tweet = re.sub(r\"OldsFireDept\", \"Olds Fire Department\", tweet)\n    tweet = re.sub(r\"RiverComplex\", \"River Complex\", tweet)\n    tweet = re.sub(r\"fieldworksmells\", \"field work smells\", tweet)\n    tweet = re.sub(r\"IranElection\", \"Iran Election\", tweet)\n    tweet = re.sub(r\"glowng\", \"glowing\", tweet)\n    tweet = re.sub(r\"kindlng\", \"kindling\", tweet)\n    tweet = re.sub(r\"riggd\", \"rigged\", tweet)\n    tweet = re.sub(r\"slownewsday\", \"slow news day\", tweet)\n    tweet = re.sub(r\"MyanmarFlood\", \"Myanmar Flood\", tweet)\n    tweet = re.sub(r\"abc7chicago\", \"ABC 7 Chicago\", tweet)\n    tweet = re.sub(r\"copolitics\", \"Colorado Politics\", tweet)\n    tweet = re.sub(r\"AdilGhumro\", \"Adil Ghumro\", tweet)\n    tweet = re.sub(r\"netbots\", \"net bots\", tweet)\n    tweet = re.sub(r\"byebyeroad\", \"bye bye road\", tweet)\n    tweet = re.sub(r\"massiveflooding\", \"massive flooding\", tweet)\n    tweet = re.sub(r\"EndofUS\", \"End of United States\", tweet)\n    tweet = re.sub(r\"35PM\", \"35 PM\", tweet)\n    tweet = re.sub(r\"greektheatrela\", \"Greek Theatre Los Angeles\", tweet)\n    tweet = re.sub(r\"76mins\", \"76 minutes\", tweet)\n    tweet = re.sub(r\"publicsafetyfirst\", \"public safety first\", tweet)\n    tweet = re.sub(r\"livesmatter\", \"lives matter\", tweet)\n    tweet = re.sub(r\"myhometown\", \"my hometown\", tweet)\n    tweet = re.sub(r\"tankerfire\", \"tanker fire\", tweet)\n    tweet = re.sub(r\"MEMORIALDAY\", \"memorial day\", tweet)\n    tweet = re.sub(r\"MEMORIAL_DAY\", \"memorial day\", tweet)\n    tweet = re.sub(r\"instaxbooty\", \"instagram booty\", tweet)\n    tweet = re.sub(r\"Jerusalem_Post\", \"Jerusalem Post\", tweet)\n    tweet = re.sub(r\"WayneRooney_INA\", \"Wayne Rooney\", tweet)\n    tweet = re.sub(r\"VirtualReality\", \"Virtual Reality\", tweet)\n    tweet = re.sub(r\"OculusRift\", \"Oculus Rift\", tweet)\n    tweet = re.sub(r\"OwenJones84\", \"Owen Jones\", tweet)\n    tweet = re.sub(r\"jeremycorbyn\", \"Jeremy Corbyn\", tweet)\n    tweet = re.sub(r\"paulrogers002\", \"Paul Rogers\", tweet)\n    tweet = re.sub(r\"mortalkombatx\", \"Mortal Kombat X\", tweet)\n    tweet = re.sub(r\"mortalkombat\", \"Mortal Kombat\", tweet)\n    tweet = re.sub(r\"FilipeCoelho92\", \"Filipe Coelho\", tweet)\n    tweet = re.sub(r\"OnlyQuakeNews\", \"Only Quake News\", tweet)\n    tweet = re.sub(r\"kostumes\", \"costumes\", tweet)\n    tweet = re.sub(r\"YEEESSSS\", \"yes\", tweet)\n    tweet = re.sub(r\"ToshikazuKatayama\", \"Toshikazu Katayama\", tweet)\n    tweet = re.sub(r\"IntlDevelopment\", \"Intl Development\", tweet)\n    tweet = re.sub(r\"ExtremeWeather\", \"Extreme Weather\", tweet)\n    tweet = re.sub(r\"WereNotGruberVoters\", \"We are not gruber voters\", tweet)\n    tweet = re.sub(r\"NewsThousands\", \"News Thousands\", tweet)\n    tweet = re.sub(r\"EdmundAdamus\", \"Edmund Adamus\", tweet)\n    tweet = re.sub(r\"EyewitnessWV\", \"Eye witness WV\", tweet)\n    tweet = re.sub(r\"PhiladelphiaMuseu\", \"Philadelphia Museum\", tweet)\n    tweet = re.sub(r\"DublinComicCon\", \"Dublin Comic Con\", tweet)\n    tweet = re.sub(r\"NicholasBrendon\", \"Nicholas Brendon\", tweet)\n    tweet = re.sub(r\"Alltheway80s\", \"All the way 80s\", tweet)\n    tweet = re.sub(r\"FromTheField\", \"From the field\", tweet)\n    tweet = re.sub(r\"NorthIowa\", \"North Iowa\", tweet)\n    tweet = re.sub(r\"WillowFire\", \"Willow Fire\", tweet)\n    tweet = re.sub(r\"MadRiverComplex\", \"Mad River Complex\", tweet)\n    tweet = re.sub(r\"feelingmanly\", \"feeling manly\", tweet)\n    tweet = re.sub(r\"stillnotoverit\", \"still not over it\", tweet)\n    tweet = re.sub(r\"FortitudeValley\", \"Fortitude Valley\", tweet)\n    tweet = re.sub(r\"CoastpowerlineTramTr\", \"Coast powerline\", tweet)\n    tweet = re.sub(r\"ServicesGold\", \"Services Gold\", tweet)\n    tweet = re.sub(r\"NewsbrokenEmergency\", \"News broken emergency\", tweet)\n    tweet = re.sub(r\"Evaucation\", \"evacuation\", tweet)\n    tweet = re.sub(r\"leaveevacuateexitbe\", \"leave evacuate exit be\", tweet)\n    tweet = re.sub(r\"P_EOPLE\", \"PEOPLE\", tweet)\n    tweet = re.sub(r\"Tubestrike\", \"tube strike\", tweet)\n    tweet = re.sub(r\"CLASS_SICK\", \"CLASS SICK\", tweet)\n    tweet = re.sub(r\"localplumber\", \"local plumber\", tweet)\n    tweet = re.sub(r\"awesomejobsiri\", \"awesome job siri\", tweet)\n    tweet = re.sub(r\"PayForItHow\", \"Pay for it how\", tweet)\n    tweet = re.sub(r\"ThisIsAfrica\", \"This is Africa\", tweet)\n    tweet = re.sub(r\"crimeairnetwork\", \"crime air network\", tweet)\n    tweet = re.sub(r\"KimAcheson\", \"Kim Acheson\", tweet)\n    tweet = re.sub(r\"cityofcalgary\", \"City of Calgary\", tweet)\n    tweet = re.sub(r\"prosyndicate\", \"pro syndicate\", tweet)\n    tweet = re.sub(r\"660NEWS\", \"660 NEWS\", tweet)\n    tweet = re.sub(r\"BusInsMagazine\", \"Business Insurance Magazine\", tweet)\n    tweet = re.sub(r\"wfocus\", \"focus\", tweet)\n    tweet = re.sub(r\"ShastaDam\", \"Shasta Dam\", tweet)\n    tweet = re.sub(r\"go2MarkFranco\", \"Mark Franco\", tweet)\n    tweet = re.sub(r\"StephGHinojosa\", \"Steph Hinojosa\", tweet)\n    tweet = re.sub(r\"Nashgrier\", \"Nash Grier\", tweet)\n    tweet = re.sub(r\"NashNewVideo\", \"Nash new video\", tweet)\n    tweet = re.sub(r\"IWouldntGetElectedBecause\", \"I would not get elected because\", tweet)\n    tweet = re.sub(r\"SHGames\", \"Sledgehammer Games\", tweet)\n    tweet = re.sub(r\"bedhair\", \"bed hair\", tweet)\n    tweet = re.sub(r\"JoelHeyman\", \"Joel Heyman\", tweet)\n    tweet = re.sub(r\"viaYouTube\", \"via YouTube\", tweet)\n           \n    # Urls\n    tweet = re.sub(r'^https?:\\\/\\\/.*[\\r\\n]*', '', tweet, flags=re.MULTILINE)\n    # html\n    tweet = re.sub(r\"<.*?>\", \"\", tweet)\n    # www\n    tweet = re.sub(r\"www\\.\\S+\", \"www\", tweet)\n    \n        \n    # Words with punctuations and special characters\n    punctuations = '!?+&*[]-%.:\/();$=><|{}^' + \"'`\"\n    for p in punctuations:\n        tweet = tweet.replace(p, f' {p} ')\n        \n    # ... and ..\n    tweet = tweet.replace('. . . ', ' ... ')\n    if '. . . ' not in tweet:\n        tweet = tweet.replace('. . ', ' ... ')      \n        \n    # Acronyms\n    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\n    tweet = re.sub(r\"m\u00cc\u00bcsica\", \"music\", tweet)\n    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\n    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)    \n    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)  \n    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)  \n    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\n    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\n    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)  \n    tweet = re.sub(r\"alwx\", \"Alabama Weather\", tweet)\n    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)    \n    #tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet)\n    tweet = re.sub(r\"Suruc\", \"Sanliurfa\", tweet)   \n    \n    # Grouping same words without embeddings\n    tweet = re.sub(r\"Bestnaijamade\", \"bestnaijamade\", tweet)\n    tweet = re.sub(r\"SOUDELOR\", \"Soudelor\", tweet)\n\n    tweet = re.sub('(\\d)+', 'NUM', tweet)\n    \n    return tweet","c1224787":"# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n","daa8bce9":"def separate_tagges(tweet):  \n    punctuations = '#@'\n    for p in punctuations:\n        tweet = tweet.replace(p, f' {p} ')\n    return tweet","fb545711":"def semi_clean(TEXT):\n  TEXT['text_semi_clean']=\"\" \n  #TEXT['text_semi_clean']=TEXT['text'].apply(remove_emoji)\n  if TEXT['text_semi_clean'][0]==\"\":\n    TEXT['text_semi_clean']=TEXT['text'].apply(clean)\n  else:\n    TEXT['text_semi_clean']=TEXT['text_semi_clean'].apply(clean)\n  TEXT['text_semi_clean']=TEXT['text_semi_clean'].apply(separate_tagges)\n  return TEXT","73e83b05":"\nimport string\npunct=string.punctuation\n\ndef text_data_cleaning(sentence):\n    doc = nlp(sentence, disable=['tagger','parser', 'ner'])\n    doc = [token for token in doc if token.like_url == False]\n    tokens = []\n  \n    for token in doc:\n        if token.lemma_ != \"-PRON-\":\n            temp = token.lemma_.lower().strip()\n        else:\n            temp = token.lower_\n        tokens.append(temp)\n    \n    cleaned_tokens = []\n    for token in tokens:\n        if token not in STOP_WORDS and token not in punct:\n            cleaned_tokens.append(token)\n\n    cleaned_sentence=\" \"\n    for i , word in enumerate(cleaned_tokens):\n      if i == 0:\n       cleaned_sentence=word \n      else:\n        cleaned_sentence = cleaned_sentence + \" \" + word\n        \n\n    \n    return cleaned_sentence","0fa46599":"def final_clean_text(TEXT):\n  TEXT['clean_text']=\"\"\n  TEXT['clean_text']=TEXT['text_semi_clean'].apply(text_data_cleaning)\n  return TEXT\n\n","f95baf76":"tw_train=semi_clean(tw_train)\ntw_train=final_clean_text(tw_train)\ntw_test=semi_clean(tw_test)\ntw_test=final_clean_text(tw_test)\ntw_train.sample(20)","93f21d1c":"tw_test.head()","c73db868":"\ndef plot_wordcloud(text, mask=None, figure_size=(15.0,10.0), \n                   title = None, title_size=40, image_color=False):\n    stop_words = stopwords\n    #stop_words = stop_words.union({\"california\",\"wreck\"})\n    \n\n    wordcloud = WordCloud(stopwords=stop_words,background_color='white',      \n                    random_state = 42,\n                    width=700, \n                    height=330,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n   \nplot_wordcloud(tw_train['clean_text'].loc[tw_train[\"target\"]==1], title=\"Word Cloud of disaster related tweets - After-Cleaning\")\nplot_wordcloud(tw_train['clean_text'].loc[tw_train[\"target\"]==0], title=\"Word Cloud of disaster unrelated tweets - After-Cleaning\")","b5178226":"from collections import defaultdict\ntrain1_df = tw_train[tw_train[\"target\"]==1]\ntrain0_df = tw_train[tw_train[\"target\"]==0]\n\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in stopwords]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n## Get the bar chart from sincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"text_semi_clean\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(20), 'red')\n\n## Get the bar chart from insincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text_semi_clean\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(20), 'red')\n\n# Creating two subplots\nfig = make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words - not a disaster\", \n                                          \"Frequent words - a disaster\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=400, width=650, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\nfig.show(renderer=\"kaggle\")","b5330f74":"stopwords.update(set([\"num\"]))","71078129":"train1_df = tw_train[tw_train[\"target\"]==1]\ntrain0_df = tw_train[tw_train[\"target\"]==0]\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in stopwords]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n## Get the bar chart from sincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"clean_text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(20), 'red')\n\n## Get the bar chart from insincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"clean_text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(20), 'red')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words if tweet is not real disaster\", \n                                          \"Frequent words if tweet is real disaster\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=400, width=650, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\nfig.show(renderer=\"kaggle\")","2a7e7e05":"train1_df = tw_train[tw_train[\"target\"]==1]\ntrain0_df = tw_train[tw_train[\"target\"]==0]\n#stopwords = set(STOPWORDS)\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token in stopwords]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n## Get the bar chart from sincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"text_semi_clean\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted_0 = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted_0.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted_0.head(20), 'red')\n\n## Get the bar chart from insincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text_semi_clean\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted_1 = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted_1.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted_1.head(20), 'red')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words - tweet is not real\", \n                                          \"Frequent words - tweet is real\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=400, width=650, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\nfig.show(renderer=\"kaggle\")","82b3ace7":"# word count comparing labeles\ntw_train['num_words'] = tw_train['text'].apply(lambda x: len(str(x).split()))\ntw_train.groupby('target')['num_words'].mean()","411a1ce5":"fd_sorted_0.rename(columns={'wordcount': 'wordcount_0'},inplace=True)\nfd_sorted_1.rename(columns={'wordcount': 'wordcount_1'},inplace=True)\nfd_sorted=fd_sorted_0.merge(fd_sorted_1,how='outer',on='word')\nfd_sorted.head(10)","904eb997":"#Finding importent stop words\nfd_sorted['wordcount_0'].loc[fd_sorted['wordcount_0'].isnull()] = 0\nfd_sorted['wordcount_1'].loc[fd_sorted['wordcount_1'].isnull()] = 0\nfd_sorted['abs_ratio']=0.0\nmin_nuber_of_words=50\nmin_ratio=2.5\nfor word in fd_sorted['word']:\n  wc0=fd_sorted['wordcount_0'].loc[fd_sorted['word']==word].values\n  wc1=fd_sorted['wordcount_1'].loc[fd_sorted['word']==word].values                                \n  if (wc0>0) and (wc1>0):\n    abs_ratio=max([wc0,wc1])\/min([wc0,wc1])\n  else:\n    abs_ratio=min_ratio\n  fd_sorted['abs_ratio'].loc[fd_sorted['word']==word]=abs_ratio\n#locating the stop words with high ratio between desaster and not desaster and sagnificant\nimportent_sw=fd_sorted.loc[(fd_sorted['abs_ratio']>=min_ratio) & ( (fd_sorted['wordcount_0']>min_nuber_of_words) | (fd_sorted['wordcount_1']>min_nuber_of_words))]\nprint(len(importent_sw['word']))\nimportent_sw.head(len(importent_sw['word']))\n","11084ab0":"train1_df = tw_train[tw_train[\"target\"]==1]\ntrain0_df = tw_train[tw_train[\"target\"]==0]\n#stopwords = set(STOPWORDS)\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in stopwords]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(20), 'green')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(20), 'green')\n\n# Creating two subplots\nfig = make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent words - tweet is not real\", \n                                          \"Frequent words - tweet is real\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=400, width=1000, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Count Plots\")\nfig.show(renderer=\"kaggle\")","e781febb":"train1_df = tw_train[tw_train[\"target\"]==1]\ntrain0_df = tw_train[tw_train[\"target\"]==0]\n#stopwords = set(STOPWORDS)\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in stopwords]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"clean_text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(20), 'green')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"clean_text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(20), 'green')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent words if tweet is not real disaster\", \n                                          \"Frequent words if tweet is real disaster\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=400, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Count Plots\")\nfig.show(renderer=\"kaggle\")","01d595f2":"train1_df = tw_train[tw_train[\"target\"]==1]\ntrain0_df = tw_train[tw_train[\"target\"]==0]\n#stopwords = set(STOPWORDS)\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in stopwords]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(20), 'green')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(20), 'green')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent words if tweet is not real disaster\", \n                                          \"Frequent words if tweet is real disaster\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=400, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Count Plots\")\nfig.show(renderer=\"kaggle\")","d9888bee":"freq_dict = defaultdict(int)\nfor sent in train0_df[\"text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(20), 'orange')\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(20), 'orange')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04, horizontal_spacing=0.2,\n                          subplot_titles=[\"Frequent words if tweet is not real disaster\", \n                                          \"Frequent words if tweet is real disaster\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=400, width=1000, paper_bgcolor='rgb(233,233,233)', title=\"Trigram Count Plots\")\nfig.show(renderer=\"kaggle\")","2521a33a":"freq_dict = defaultdict(int)\nfor sent in train0_df[\"clean_text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(20), 'orange')\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"clean_text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(20), 'orange')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04, horizontal_spacing=0.2,\n                          subplot_titles=[\"Frequent words if tweet is not real disaster\", \n                                          \"Frequent words if tweet is real disaster\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=400, width=1000, paper_bgcolor='rgb(233,233,233)', title=\"Trigram Count Plots\")\nfig.show(renderer=\"kaggle\")","935d7185":"#tw_test=pd.read_csv('\/content\/drive\/My Drive\/Final NLP\/df_tw_test.csv')","28f973a4":"train1_df = tw_train[tw_train[\"target\"]==1]\ntrain0_df = tw_train[tw_train[\"target\"]==0]","b11409a6":"#Ngrams function to find stop wordes\n\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token in stopwords]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\ndef sw_freq(tw_train):\n  train1_df = tw_train[tw_train[\"target\"]==1]\n  train0_df = tw_train[tw_train[\"target\"]==0]\n\n  #stop wordes frq at 0 and at 1\n  freq_dict = defaultdict(int)\n  for sent in train0_df[\"text_semi_clean\"]:\n      for word in generate_ngrams(sent):\n          freq_dict[word] += 1\n  fd_sorted_0 = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n  fd_sorted_0.columns = [\"word\", \"wordcount\"]\n\n\n  freq_dict = defaultdict(int)\n  for sent in train1_df[\"text_semi_clean\"]:\n      for word in generate_ngrams(sent):\n          freq_dict[word] += 1\n  fd_sorted_1 = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n  fd_sorted_1.columns = [\"word\", \"wordcount\"]\n\n  #concat to 0 and 1 freq of words\n\n  fd_sorted_0.rename(columns={'wordcount': 'wordcount_0'},inplace=True)\n  fd_sorted_1.rename(columns={'wordcount': 'wordcount_1'},inplace=True)\n  fd_sorted=fd_sorted_0.merge(fd_sorted_1,how='outer',on='word')\n\n  #criating stop words data frame with abs ratio\n  fd_sorted['wordcount_0'].loc[fd_sorted['wordcount_0'].isnull()] = 0\n  fd_sorted['wordcount_1'].loc[fd_sorted['wordcount_1'].isnull()] = 0\n  fd_sorted['abs_ratio']=0.0\n  dif_ratio=2.5 #ratio if no stop word at 0 or at 1\n  for word in fd_sorted['word']:\n    wc0=fd_sorted['wordcount_0'].loc[fd_sorted['word']==word].values\n    wc1=fd_sorted['wordcount_1'].loc[fd_sorted['word']==word].values                                \n    if (wc0>0) and (wc1>0):\n      abs_ratio=max([wc0,wc1])\/min([wc0,wc1])\n    else:\n      abs_ratio=dif_ratio\n    fd_sorted['abs_ratio'].loc[fd_sorted['word']==word]=abs_ratio\n  return fd_sorted","ac9f2d1d":"def importent_stop_words(fd_sorted,nw,mr):\n  return fd_sorted['word'].loc[(fd_sorted['abs_ratio']>=mr) & \\\n                               ((fd_sorted['wordcount_0']>nw) \\\n                                | (fd_sorted['wordcount_1']>nw))].tolist()","cf88514c":"def update_SW_list(stopwords,list_words_to_remouve=[\"\"]):\n  return stopwords-set(list_words_to_remouve)","491ee218":"def final_clean_including_imp_stopwords(tw_train,tw_test,nw,mr):\n  fd_sorted=sw_freq(tw_train) # generating the sorted stop wordf frequncy df \n  list_words_to_remouve=importent_stop_words(fd_sorted,nw,mr)\n  print(list_words_to_remouve)\n  stopwords.difference_update(set(list_words_to_remouve)) #updating stop words list\n  tw_train=final_clean_text(tw_train) \n  tw_test=final_clean_text(tw_test)\n  #tw_train.sample(20)\n  return tw_train, tw_test","64b77520":"tw_train, tw_test = final_clean_including_imp_stopwords(tw_train,tw_test,nw=10,mr=2.5)","174e599d":"tw_train.to_csv (r'df_tw_train.csv', index = True, header=True)\ntw_test.to_csv (r'df_tw_test.csv', index = True, header=True)\n# tw_test =pd.read_csv('..\/input\/realtweetclean\/df_tw_test.csv')\n# tw_train =pd.read_csv('..\/input\/realtweetclean\/df_tw_train.csv')","c5d1ea5b":"def generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in stopwords]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]","c234961f":"def trigram_bag_of_words(TEXT,n3_words,n3_times):\n  freq_dict = defaultdict(int)\n  for sent in TEXT[\"clean_text\"]:\n      for word in generate_ngrams(sent,3):\n          freq_dict[word] += 1\n  \n  \n  fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n  fd_sorted.columns = [\"word\", \"wordcount\"]\n  \n  return fd_sorted['word'].head(n3_words).loc[fd_sorted['wordcount']>=n3_times].tolist()\n","f9214319":"def bigram_bag_of_words(TEXT,n2_words,n2_times):\n  freq_dict = defaultdict(int)\n  for sent in TEXT[\"clean_text\"]:\n      for word in generate_ngrams(sent,2):\n          freq_dict[word] += 1\n  \n  \n  fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n  fd_sorted.columns = [\"word\", \"wordcount\"]\n  \n  return fd_sorted['word'].head(n2_words).loc[fd_sorted['wordcount']>=n2_times].tolist()","45c2a4f4":"def word_bag_of_words(TEXT,n1_words,n1_times):\n  freq_dict = defaultdict(int)\n  for sent in TEXT[\"clean_text\"]:\n      for word in generate_ngrams(sent,1):\n          freq_dict[word] += 1\n  \n  \n  fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n  fd_sorted.columns = [\"word\", \"wordcount\"]\n  \n  return fd_sorted['word'].head(n1_words).loc[fd_sorted['wordcount']>=n1_times].tolist()","5524ac29":"def list_bag_of_words(TEXT,n1_words,n1_times,n2_words,n2_times,n3_words,n3_times):\n  lbow=trigram_bag_of_words(TEXT,n3_words,n3_times)\n  lbow+=bigram_bag_of_words(TEXT,n2_words,n2_times)\n  lbow+=word_bag_of_words(TEXT,n1_words,n1_times)\n  return lbow\n","30d76618":"lbow=list_bag_of_words(pd.concat([tw_test,tw_train]),n1_words=200,n1_times=100,n2_words=50,n2_times=20,n3_words=50,n3_times=20)\nprint(lbow[0:10])","50e859df":"n1_words = [1000, 2000]\nn1_times = [10, 20]\nn2_words = [100, 200]\nn2_times = [10, 20]\nn3_words = [100, 200]\nn3_times = [10, 20]\ni=0\nlbow_mat=[]\nfor n1w in n1_words:\n  for n1t in n1_times:\n    for n2w in n2_words:\n      for n2t in n2_times:\n        for n3w in n3_words:\n          for n3t in n3_times:\n            lbow_mat.append(list_bag_of_words(pd.concat([tw_test,tw_train]),n1w,n1t,n2w,n2t,n3w,n3t))\n\n\nprint(\"number of bages of words \"+str(len(lbow_mat)))","38c11248":"#X = tw_train['text']\nX = tw_train['clean_text'] # X = tw_data.drop('target', axis = 1)\ny = tw_train['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 53)","882f20ff":"\nvectorizer = CountVectorizer(stop_words=list(stopwords), ngram_range=(1,3), vocabulary=lbow, binary=True)\n\nvec_train = vectorizer.fit_transform(X_train)\n\nvec_test = vectorizer.transform(X_test)\n\nprint(vectorizer.get_feature_names()[0:10])\n\nprint(vec_train.A[0])\nprint(\"train_shape {}\".format(vec_train.shape))\nprint(\"train_shape {}\".format(vec_test.shape))","8cf60aa6":"count_df = pd.DataFrame(vec_train.A, columns = vectorizer.get_feature_names())\ncount_df.sample(10)","7544fb5c":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import  accuracy_score, confusion_matrix, f1_score\nnb_classifier = MultinomialNB()\nnb_classifier.fit(vec_train, y_train)\npred = nb_classifier.predict(vec_test)\nprint(\"Accuracy Score: \", accuracy_score(y_test, pred))\nprint(\"F1 Score: \", f1_score(y_test, pred))\nprint(confusion_matrix(y_test, pred, labels=[0,1]))","9b7a7747":"vectorizer = CountVectorizer(stop_words=list(stopwords), ngram_range=(1,3), vocabulary=lbow, binary=True)\nnb_classifier = MultinomialNB()","cc066054":"steps = [('vectorizer', vectorizer), \n         ('nb_classifier', nb_classifier)]\n\nmy_pipeline = Pipeline(steps)","d55c1923":"my_pipeline.fit(X_train, y_train)","0dce5e26":"y_pred = my_pipeline.predict(X_test)","f2d858d5":"f1_score(y_true=y_test, y_pred=y_pred)","a157b967":"from sklearn.model_selection import train_test_split, cross_val_score\nmy_param_grid = [{'vectorizer__vocabulary':lbow_mat}]\nmy_pipelin_gs = GridSearchCV(my_pipeline, my_param_grid, cv=5, scoring='f1')\nmy_pipelin_gs.fit(X_train, y_train)\nprint(\"Best Bag of words size: \"+str(len(my_pipelin_gs.best_params_['vectorizer__vocabulary'])))\npred = my_pipelin_gs.predict(X_test)\nprint(\"Accuracy Score: \", accuracy_score(y_test, pred))\nprint(\"F1 Score: \", f1_score(y_test, pred))\nprint(confusion_matrix(y_test, pred, labels=[0,1]))","b8de5250":"len(my_pipelin_gs.best_params_['vectorizer__vocabulary'])","8dcbabb7":"tw_train.info()","84e7bfa7":"tw_train['keyword'] = tw_train['keyword'].fillna(value = 'none')","d128d5e2":"X = tw_train['clean_text_1']  \ny = tw_train['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 53)\n\n","a8a994a9":"\ncount_vectorizer = CountVectorizer(max_df = 0.2, min_df = 0.003, ngram_range=(1, 2))\n\ncount_train = count_vectorizer.fit_transform(X_train)\n\ncount_test = count_vectorizer.transform(X_test)\n\n\nprint(count_vectorizer.get_feature_names()[:50])","bfedc7df":"tfidf_vectorizer = TfidfVectorizer(max_df = 0.2, min_df = 0.003)\n\ntfidf_train = tfidf_vectorizer.fit_transform(X_train)\n\ntfidf_test = tfidf_vectorizer.transform(X_test)\n\nprint(tfidf_vectorizer.get_feature_names()[:10])\n\nprint(tfidf_train.A[:10])","226fd2ff":"# create DataFrame\ncount_df = pd.DataFrame(count_train.A, columns = count_vectorizer.get_feature_names())\ncount_df.head()","b82230d6":" tfidf_df = pd.DataFrame(tfidf_train.A, columns =  tfidf_vectorizer.get_feature_names())\n tfidf_df.head()","b3a84c7a":"with nlp.disable_pipes():\n    list_docs_train = list(nlp.pipe(X_train))\n    doc_vectors_train = np.array([doc.vector for doc in list_docs_train])\n    \n    list_docs_test = list(nlp.pipe(X_test))\n    doc_vectors_test = np.array([doc.vector for doc in list_docs_test])\n                            \nprint(\"Train shape: \", doc_vectors_train.shape)\nprint(\"Test shape: \", doc_vectors_test.shape)","81f40fe8":"nb_classifier = MultinomialNB()\n\nnb_classifier.fit(count_train, y_train)\n\npred = nb_classifier.predict(count_test)\n\nprint(\"Accuracy Score: \", accuracy_score(y_test, pred))\nprint(\"F1 Score: \", f1_score(y_test, pred))\nprint(confusion_matrix(y_test, pred, labels=[0,1]))","25b39f04":"nb_classifier_tf = MultinomialNB()\n\nnb_classifier_tf.fit(tfidf_train, y_train)\n\npred = nb_classifier_tf.predict(tfidf_test)\n\nprint(\"Accuracy Score: \", accuracy_score(y_test, pred))\nprint(\"F1 Score: \", f1_score(y_test, pred))\nprint(confusion_matrix(y_test, pred, labels=[0,1]))","042d4a89":"param_grid_NB = {'alpha': [0.01,0.05,0.1,0.5] } \n\nMNB = MultinomialNB()\n\ngs_cv = GridSearchCV(MNB, param_grid=param_grid_NB, cv=5, scoring='f1')\n\ngs_cv.fit(tfidf_train, y_train)\n\ndf_results_train = pd.DataFrame(gs_cv.cv_results_)[['param_alpha', 'mean_test_score']]\n\ndf_results_train","cbde9692":"from sklearn.svm import LinearSVC\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score","b86412fd":"svc = LinearSVC(random_state=1, dual=False, max_iter=1000)\n\nsvc.fit(doc_vectors_train, y_train)","7576c05e":"y_pred = svc.predict(doc_vectors_test)\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}%\", )\nprint(f\"f1 Score: {f1_score(y_test, y_pred):.3f}%\", )","f3d0e6b4":"param_grid = {'C': [0.01,0.05, 0.1,0.5, 1] } \n\nsvc = LinearSVC(random_state=1, dual=False)\n\ngs_cv = GridSearchCV(svc, param_grid=param_grid, cv=5, scoring = 'f1')\n\ngs_cv.fit(tfidf_train, y_train)\n\ndf_results_train = pd.DataFrame(gs_cv.cv_results_)[['param_C', 'mean_test_score']]\n\ndf_results_train","070996da":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\nimport tokenization\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow import keras\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling1D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback","1a785638":"class ClassificationReport(Callback):\n    \n    def __init__(self, train_data=(), validation_data=()):\n        super(Callback, self).__init__()\n        \n        self.X_train, self.y_train = train_data\n        self.train_precision_scores = []\n        self.train_recall_scores = []\n        self.train_f1_scores = []\n        \n        self.X_val, self.y_val = validation_data\n        self.val_precision_scores = []\n        self.val_recall_scores = []\n        self.val_f1_scores = [] \n               \n    def on_epoch_end(self, epoch, logs={}):\n        train_predictions = np.round(self.model.predict(self.X_train, verbose=0))        \n        train_precision = precision_score(self.y_train, train_predictions, average='macro')\n        train_recall = recall_score(self.y_train, train_predictions, average='macro')\n        train_f1 = f1_score(self.y_train, train_predictions, average='macro')\n        self.train_precision_scores.append(train_precision)        \n        self.train_recall_scores.append(train_recall)\n        self.train_f1_scores.append(train_f1)\n        \n        val_predictions = np.round(self.model.predict(self.X_val, verbose=0))\n        val_precision = precision_score(self.y_val, val_predictions, average='macro')\n        val_recall = recall_score(self.y_val, val_predictions, average='macro')\n        val_f1 = f1_score(self.y_val, val_predictions, average='macro')\n        self.val_precision_scores.append(val_precision)        \n        self.val_recall_scores.append(val_recall)        \n        self.val_f1_scores.append(val_f1)\n        \n        print('\\nEpoch: {} - Training Precision: {:.6} - Training Recall: {:.6} - Training F1: {:.6}'.format(epoch + 1, train_precision, train_recall, train_f1))\n        print('Epoch: {} - Validation Precision: {:.6} - Validation Recall: {:.6} - Validation F1: {:.6}'.format(epoch + 1, val_precision, val_recall, val_f1))  ","7d0f0fa2":"\nbert_layer = hub.KerasLayer('https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1', trainable=True)","8b9749ea":"class DisasterDetector:\n    \n    def __init__(self, bert_layer, max_seq_length=128, lr=0.0001, epochs=15, batch_size=32):\n        \n        # BERT and Tokenization params\n        self.bert_layer = bert_layer\n        \n        self.max_seq_length = max_seq_length        \n        vocab_file = self.bert_layer.resolved_object.vocab_file.asset_path.numpy()\n        do_lower_case = self.bert_layer.resolved_object.do_lower_case.numpy()\n        self.tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n        \n        # Learning control params\n        self.lr = lr\n        self.epochs = epochs\n        self.batch_size = batch_size\n        \n        self.models = []\n        self.scores = {}\n        \n        \n    def encode(self, texts):\n                \n        all_tokens = []\n        all_masks = []\n        all_segments = []\n\n        for text in texts:\n            text = self.tokenizer.tokenize(text)\n            text = text[:self.max_seq_length - 2]\n            input_sequence = ['[CLS]'] + text + ['[SEP]']\n            pad_len = self.max_seq_length - len(input_sequence)\n\n            tokens = self.tokenizer.convert_tokens_to_ids(input_sequence)\n            tokens += [0] * pad_len\n            pad_masks = [1] * len(input_sequence) + [0] * pad_len\n            segment_ids = [0] * self.max_seq_length\n\n            all_tokens.append(tokens)\n            all_masks.append(pad_masks)\n            all_segments.append(segment_ids)\n\n        return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n    \n    \n    def build_model(self):\n        \n        input_word_ids = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='input_word_ids')\n        input_mask = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='input_mask')\n        segment_ids = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='segment_ids')    \n        \n        pooled_output, sequence_output = self.bert_layer([input_word_ids, input_mask, segment_ids])   \n        clf_output = sequence_output[:, 0, :]\n        out = Dense(1, activation='sigmoid')(clf_output)\n        \n        model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n        optimizer = SGD(learning_rate=self.lr, momentum=0.8)\n        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n        \n        return model\n    \n    \n    def train(self, X):\n        \n        for fold, (trn_idx, val_idx) in enumerate(skf.split(X['clean_text_1'], X['keyword'])):\n            \n            print('\\nFold {}\\n'.format(fold))\n        \n            X_trn_encoded = self.encode(X.loc[trn_idx, 'clean_text_1'].str.lower())\n            y_trn = X.loc[trn_idx, 'target']\n            X_val_encoded = self.encode(X.loc[val_idx, 'clean_text_1'].str.lower())\n            y_val = X.loc[val_idx, 'target']\n        \n            # Callbacks\n            metrics = ClassificationReport(train_data=(X_trn_encoded, y_trn), validation_data=(X_val_encoded, y_val))\n            \n            # Model\n            model = self.build_model()        \n            model.fit(X_trn_encoded, y_trn, validation_data=(X_val_encoded, y_val), callbacks=[metrics], epochs=self.epochs, batch_size=self.batch_size)\n            \n            self.models.append(model)\n            self.scores[fold] = {\n                'train': {\n                    'precision': metrics.train_precision_scores,\n                    'recall': metrics.train_recall_scores,\n                    'f1': metrics.train_f1_scores                    \n                },\n                'validation': {\n                    'precision': metrics.val_precision_scores,\n                    'recall': metrics.val_recall_scores,\n                    'f1': metrics.val_f1_scores                    \n                }\n            }\n                    \n                \n    def plot_learning_curve(self):\n        \n        fig, axes = plt.subplots(nrows=K, ncols=2, figsize=(20, K * 6), dpi=100)\n    \n        for i in range(K):\n            \n            # Classification Report curve\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[i].history.history['val_accuracy'], ax=axes[i][0], label='val_accuracy')\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['precision'], ax=axes[i][0], label='val_precision')\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['recall'], ax=axes[i][0], label='val_recall')\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['f1'], ax=axes[i][0], label='val_f1')        \n\n            axes[i][0].legend() \n            axes[i][0].set_title('Fold {} Validation Classification Report'.format(i), fontsize=14)\n\n            # Loss curve\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[0].history.history['loss'], ax=axes[i][1], label='train_loss')\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[0].history.history['val_loss'], ax=axes[i][1], label='val_loss')\n\n            axes[i][1].legend() \n            axes[i][1].set_title('Fold {} Train \/ Validation Loss'.format(i), fontsize=14)\n\n            for j in range(2):\n                axes[i][j].set_xlabel('Epoch', size=12)\n                axes[i][j].tick_params(axis='x', labelsize=12)\n                axes[i][j].tick_params(axis='y', labelsize=12)\n\n        plt.show()\n        \n        \n    def predict(self, X):\n        \n        X_test_encoded = self.encode(X['clean_text_1'].str.lower())\n        y_pred = np.zeros((X_test_encoded[0].shape[0], 1))\n\n        for model in self.models:\n            y_pred += model.predict(X_test_encoded) \/ len(self.models)\n\n        return y_pred","c827ea9a":"SEED = 1337\nK = 2\nskf = StratifiedKFold(n_splits=K, random_state=SEED, shuffle=True)","cf717e43":"#  Training, Evaluation and Prediction\nclf = DisasterDetector(bert_layer, max_seq_length=128, lr=0.0001, epochs=9, batch_size=32)\n\nclf.train(tw_train)","bade81fa":"clf.plot_learning_curve()","717647cc":"y_bert_pred = clf.predict(tw_test)\n\nmodel_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nmodel_submission['target'] = np.round(y_bert_pred).astype('int')\nmodel_submission.to_csv('model_submission.csv', index=False)\nmodel_submission.describe()","45394a18":"with nlp.disable_pipes():\n    list_docs = list(nlp.pipe(tw_train['clean_text_1']))\n    vectors = np.array([doc.vector for doc in list_docs])\n    \nvectors.shape","8dbc14a2":"text = \"\"\"There was a wild fire in the forest. it was a disaster.\"\"\"\n#text = \"\"\"someone drowned in the sea.\"\"\"\nprint(text)\n\ndef my_cosine_similarity(a, b):\n    return np.dot(a, b)\/np.sqrt(a.dot(a)*b.dot(b))\n\ntext_vec = nlp(text).vector\n\n## Center the document vectors\n# Calculate the mean for the document vectors, should have shape (300,)\nvec_mean = vectors.mean(axis=0)\n# Subtract the mean from the vectors\ncentered = vectors - vec_mean\n\n# Calculate similarities for each document in the dataset\n# Make sure to subtract the mean from the text vector\nsims = np.array([my_cosine_similarity(text_vec - vec_mean, vec) for vec in centered])\n\n# Get the index for the most similar document\nmost_similar = sims.argmax()\nprint(\"most_similar\", most_similar)\nprint(tw_train.iloc[most_similar].text)","cb223ead":"!pip install textstat","2a467e96":"\n# applying the automated readability index\ntw_train[\"a_read_ind\"] = tw_train[\"text\"].apply(textstat.automated_readability_index)\ntw_test[\"a_read_ind\"] = tw_test[\"text\"].apply(textstat.automated_readability_index)\n# applying the ensemble algorithm of readability index\ntw_train[\"e_read_ind\"] = tw_train[\"text\"].apply(lambda x: textstat.text_standard(x,float_output=True))\ntw_test[\"e_read_ind\"] = tw_test[\"text\"].apply(lambda x: textstat.text_standard(x,float_output=True))","5ed5ec9b":"train1_df = tw_train[tw_train[\"target\"]==1]\ntrain0_df = tw_train[tw_train[\"target\"]==0]\n\n# fig = go.Figure()\n# fig.add_trace(go.Histogram(x=train1_df['a_read_ind'],name = 'Tweets about real disaster'))\n# fig.add_trace(go.Histogram(x=train0_df['a_read_ind'],name = 'Tweets other than real disaster'))\n# # Overlay both histograms\n# fig.update_layout(title_text='automated_readability_index')\n# # Reduce opacity to see both histograms\n# fig.update_traces(opacity=0.75)\n# fig.show()\n\nfig = make_subplots(rows=2, cols=1,subplot_titles=(\"Automated readability index\", \"Ensemble algorithm readability index\"))\n\n#fig = go.Figure()\nfig.add_trace(go.Histogram(x=train1_df['a_read_ind'],name = 'Tweets about real disaster'),row=1,col=1)\nfig.add_trace(go.Histogram(x=train0_df['a_read_ind'],name = 'Tweets other than real disaster'),row=1,col=1)\nfig.add_trace(go.Histogram(x=train1_df['e_read_ind'],name = 'Tweets about real disaster'),row=2,col=1)\nfig.add_trace(go.Histogram(x=train0_df['e_read_ind'],name = 'Tweets other than real disaster'),row=2,col=1)\n\n# Overlay both histograms\nfig.update_layout(height=800, width=1100, title_text='Readability Index on Original Text')\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.75)\nfig.show()","378bc11c":"with nlp.disable_pipes():\n    list_docs_train = list(nlp.pipe(X))\n    doc_vectors_train = np.array([doc.vector for doc in list_docs_train])\n    \n\n                            \nprint(\"Train shape: \", doc_vectors_train.shape)","75c4004f":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\npca_transformer = PCA(n_components=4)\nWtV_transformer = pca_transformer.fit(doc_vectors_train)\nXtrain_pca = WtV_transformer.transform(doc_vectors_train)","48a27126":"train_pca_df = pd.DataFrame(Xtrain_pca, columns = ['PCA_x','PCA_y','PCA_z','PCA_c'], index=tw_train.index )\ntrain_pca_df['PCA_c'].loc[train_pca_df['PCA_c']>1]=1\ntrain_pca_df['PCA_c'].loc[train_pca_df['PCA_c']<-1]=-1\ntrain_pca_df['PCA_c']+=1\ntrain_pca_df['target']=tw_train['target']\ntrain_pca_df['size']=0.5\ntrain_pca_df.sample(10)","df4fee89":"import plotly.express as px\n\n\nfig = px.scatter_3d(train_pca_df.sample(100), x=\"PCA_x\", y=\"PCA_y\", z=\"PCA_z\",color=\"target\", opacity=0.6,symbol=\"target\")\nfig.update_layout({\"legend_orientation\":\"h\"})\nfig.show()","78dccb34":"## Readability\nRedability is the ease with which a reader can understand a written text.\n\nWe use two algorithems:\n\nAutomated Readability Index\n\nFormula to calculate ARI\n\n\nEnsaemble Readability Consensus\n\nBased upon all the available redability tests at Textstat API.\n","9d61e377":"Bag of words matrix for grid-serch","65967ead":"## Additional features - Number of tweets according to location","b68e5e30":"## Pipeline set-up","1746e788":"## \"Bag of Words\"","15c2f438":"Trigram","4c76b939":"## Cleaning","b64b357f":"This model uses the implementation of BERT from the TensorFlow Models repository on GitHub at tensorflow\/models\/official\/nlp\/bert.  \nIt uses L=12 hidden layers (Transformer blocks), a hidden size of H=768, and A=12 attention heads.\n\nThis model has been pre-trained for English on the Wikipedia and BooksCorpus. \nInputs have been \"uncased\", meaning that the text has been lower-cased before tokenization into word pieces, and any accent markers have been stripped. In order to download this model, Internet must be activated on the kernel.","c6ff2752":"## Grid serch for bag of words size","dc743188":"## Additional features - Keyword Frequency plots","c2e36d10":"Befor cleaning","da178018":"Word","7d84e4e4":"# Hasgtags\ndef hashtags(doc):\n  matcher = Matcher(nlp.vocab)\n  hash_pattern = [{'IS_SPACE': False},{'ORTH': '#'},\n           {'IS_SPACE': False}]\n  matcher.add(\"hash_pattern\", None, hash_pattern)\n  matches = matcher(doc)\n  hash_list = []\n  for match_id, start, end in matches:\n    \n    hash_list = doc[start:end].text\n  return hash_list\n  ","44976da3":"After cleaning","102615d6":"# Miscellaneous","096e6a5c":"## Word Frequency plots","22c8d53a":"Out of the 110,  18 tweets were labeled differently in their duplicates. ","bd12e909":"##  Vectorization  \n","a85e0d6d":"# First Model Naive Bayes classifier","30094081":"## Grid Search with 5-Fold Cross Validation and LinearSVC model\n","5575594f":"### Spacy vectorizer","b3476f20":"## Naive Bayes Classifier \n**Using countvactorizer\/TF-IDF**","fd032dcf":"### Sklearn CountVectorizer","e03ef1ba":"# Preliminary EDA","6c33e29d":"## Common Words - Pre cleaning, with worldcloud","2c922f6c":"# Fast cleaning and modeling using SpaCy","9f620989":"## BERT \nthanks to this [notebook](https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert#7.-Model)\n","b6dac42f":"Bigram","aba46e32":"**Is the data balanced?**  \nyes","8a32c3b2":"## Split the Data","2ff2db6e":"## Text Categorizer","8d80629c":"Thire are 110 not unique (duplicated) tweets at train data\n","2cc01020":"## Bigrams","3c6974c3":"Words frequancy - Word - Bigram - Trigram","a81550a1":"# EDA & Adjusted Preprocessing","21b7db69":"# Classification Models\n\n","bf78175f":"# Importing necessary modules","7fe094c6":"The keywords given as a column in the source data file, defined by who wrote the twit","fd772545":"Ngrams function","2e5f58a0":"This notebook is a joint project of Practicle Data Science course\nyou can find a presentation [here](https:\/\/docs.google.com\/presentation\/d\/168T8c3pTYYlwy5jJtZLzCWQjqcA4VSeArUgzMHoY3mY\/edit?usp=sharing)","27a3d144":"## cleaning including importent stop word","289cf1ba":"Cleaning function that cleanes Twits texts based on the great meticulous work in this [Kaggle NB](https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert)\n","b91206af":"### Categorizer based tag correction","c3506879":"## Word Cloud after cleaning","77a890d3":"## From text to \"bag of words\" based vectores  ","18d6a5ee":"* ## Using PCA for vector visualization","6174d542":"# EDA after cleaning","d70034e8":"## Linear SVC\nUsing spacy vectorizer","da5e99f6":"## Mislabeled Samples","3c8a3560":"# Preprocesing pipline","e1eba6ce":"## Stop Word Analasys","fceba51e":"## Text Similarity\nGiven an example text below, find the most similar document within the disaster dataset using the cosine similarity.","f38d03cd":"### Sklearn Tfidf","9c41c8f5":"Naive Bayes classifier"}}