{"cell_type":{"dc18e9c2":"code","e7369949":"code","164b9d6a":"code","9f121411":"code","b4f7f1c8":"code","7281c88c":"code","0fcd14ce":"code","42c98223":"code","0891f46e":"code","99aa00d4":"code","e0368e9f":"code","63d3f0ff":"code","78df0db5":"code","1875ef92":"code","29490fef":"code","5ab54be5":"code","1dc3ce17":"code","967ddd13":"code","8ba839d8":"code","98b10d47":"code","9702b13e":"code","68720a74":"code","e7d97e03":"code","43d15f54":"code","f746794c":"code","9b265cde":"code","154e16f2":"code","ff84e6e8":"code","0e0d22e4":"code","0ffb14ba":"code","5a74f358":"code","31fc5823":"code","660fd990":"code","e5637e39":"code","6154daf2":"code","af04b47b":"code","315eef4d":"code","e65e6460":"code","f28dbf46":"code","8b6e5637":"code","6c55a05f":"code","b46e321f":"code","e5bb06a2":"code","3e881f52":"code","2e05f643":"code","c1d1a62b":"code","e211a3e1":"code","388ec408":"code","9183e52e":"code","d329eff8":"code","06f701fd":"code","2e2f6130":"code","89f4888d":"code","64350552":"markdown","25491235":"markdown","7460ce53":"markdown","2fb4b602":"markdown","eaeaa841":"markdown","2e9a52b5":"markdown","e1f66a78":"markdown","8e1929dc":"markdown","2627acd9":"markdown","c690e38a":"markdown","6f5b5353":"markdown","32f9cabe":"markdown","64a6d67b":"markdown","c46ed112":"markdown","0331d539":"markdown","86f61bf3":"markdown","09043989":"markdown","c143f470":"markdown","9fa3b41b":"markdown","c53fac2c":"markdown","ccb799d5":"markdown","48c19af8":"markdown","75f9ca9f":"markdown","63ad8bd2":"markdown","007fd5a2":"markdown","f8c1b4c4":"markdown","d983c333":"markdown","ebe530df":"markdown","1da2b139":"markdown","a6e8fdf1":"markdown","47ce23da":"markdown","342714ba":"markdown","a07d69da":"markdown","862e64e5":"markdown","69d22b91":"markdown","0f63ee96":"markdown","4a359dd2":"markdown","919fd1b6":"markdown","3e49e930":"markdown","f3be526e":"markdown","b354d15b":"markdown","94f6a055":"markdown","2bd1efe4":"markdown","950a411a":"markdown","8fb737e1":"markdown","c2f0491e":"markdown","a249103c":"markdown","658c808f":"markdown","2f84bcaa":"markdown","9580fadb":"markdown","fe5ade12":"markdown","50a17f4a":"markdown","892c5079":"markdown","70876020":"markdown","10f9fa24":"markdown","2fd5c26d":"markdown"},"source":{"dc18e9c2":"# Data analysis libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n# to make the plots appear in the notebook\n%matplotlib inline \nimport matplotlib.pyplot as plt\n\n# Models from scikit-learn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Model Evaluation\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve\n","e7369949":"\nheart_disease = pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")\n\n# Displaying the first 10 data in the dataset\nheart_disease.head(10)","164b9d6a":"# How many class we have\nheart_disease[\"target\"].value_counts()","9f121411":"heart_disease[\"target\"].value_counts().plot(kind = \"bar\", color = [\"red\",\"green\"]);","b4f7f1c8":"# Checking whether our data has any missing values\nheart_disease.isna().sum()","7281c88c":"# Describing our data\nheart_disease.describe()","0fcd14ce":"heart_disease.sex.value_counts()","42c98223":"pd.crosstab(heart_disease.target,heart_disease.sex)","0891f46e":"# ploting the cross tab\n\npd.crosstab(heart_disease.target,heart_disease.sex).plot(kind = \"bar\",\n                                                         figsize = (10,6),\n                                                         color=[\"hotpink\",\"blue\"]);\nplt.title(\"Heart Disease Frequency VS Sex\")\nplt.xlabel(\"1 - affected    0 - not affected\")\nplt.ylabel(\"no.of people\")\nplt.legend([\"Female\",\"Male\"]);","99aa00d4":"heart_disease[\"thalach\"].value_counts()","e0368e9f":"# Scatter plot\nplt.figure(figsize = (10,6))\n\n# Scatter with positive examples\nplt.scatter(heart_disease.age[heart_disease.target == 1],\n            heart_disease.thalach[heart_disease.target == 1],\n            c=\"red\")\n\n#Scatter with negative examples\nplt.scatter(heart_disease.age[heart_disease.target == 0],\n            heart_disease.thalach[heart_disease.target == 0],\n            c=\"green\")\n\n# Labeling\nplt.title(\"Affected VS Not affected: AGE and MAX HEART RATE\")\nplt.xlabel(\"AGE\")\nplt.ylabel(\"HEART RATE\")\nplt.legend([\"Affected\",\"Not affected\"]);","63d3f0ff":"# Check how the data has been spread over\nheart_disease.age.plot.hist();","78df0db5":"pd.crosstab(heart_disease.cp,heart_disease.target)","1875ef92":"# Plot the crosstab\n\npd.crosstab(heart_disease.cp,heart_disease.target).plot(kind = \"bar\",\n                                                        figsize = (10,6),\n                                                        color = [\"green\",\"red\"])\n\nplt.title(\"Heart disease frequency for different chest pain\")\nplt.xlabel(\"Chest pain type\")\nplt.ylabel(\"Frequency\")\nplt.legend([\"No Disease\",\"Disease\"]);","29490fef":"# Tabular format\nheart_disease.corr()","5ab54be5":"# Visual format\ncorr_mat = heart_disease.corr()\nfig, ax = plt.subplots(figsize = (15,10))\nax = sns.heatmap(corr_mat,\n                 annot=True,\n                 linewidths=0.5,\n                 fmt=\".2f\",\n                 cmap=\"YlGnBu\")","1dc3ce17":"# Splitting the data\n\nx = heart_disease.drop(\"target\",axis=1)\ny = heart_disease[\"target\"]","967ddd13":"x","8ba839d8":"y","98b10d47":"# Spliting the data into train and test dataset\n\n# to reproduce the exact data chosen\nnp.random.seed(42) \n\n# Actual splitting\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)\n","9702b13e":"x_train","68720a74":"y_train,len(y_train)","e7d97e03":"# Put models in a dictionary\nmodels = {\n          \"Logistic Regression\": LogisticRegression(solver='liblinear'), \n          \"KNN\": KNeighborsClassifier(),\n          \"Random Forest\": RandomForestClassifier()\n          }\n\n# Create function to fit and score models\ndef fit_and_score(models, x_train, x_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluates given machine learning models.\n    models : a dict of different Scikit-Learn machine learning models\n    X_train : training data\n    X_test : testing data\n    y_train : labels assosciated with training data\n    y_test : labels assosciated with test data\n    \"\"\"\n    # Random seed for reproducible results\n    np.random.seed(42)\n    # Make a list to keep model scores\n    model_scores = {}\n    # Loop through models\n    for name, model in models.items():\n        # Fit the model to the data\n        model.fit(x_train, y_train)\n        # Evaluate the model and append its score to model_scores\n        model_scores[name] = model.score(x_test, y_test)\n    return model_scores","43d15f54":"model_scores = fit_and_score(models = models,\n                             x_train = x_train,\n                             x_test = x_test,\n                             y_train = y_train,\n                             y_test = y_test)\n\nmodel_scores","f746794c":"model_compare = pd.DataFrame(model_scores, index = [\"accuracy\"])\nmodel_compare.T.plot.bar();","9b265cde":"# Tuning KNN\n\ntrain_score = []\ntest_score = []\n\n# Create a list of different values for n_neighbors\n\nneighbors = range(1,21)\n\n# Setup KNN instance\n\nknn = KNeighborsClassifier()\n\n# Loop through differnt n_neighbors\n\nfor i in neighbors:\n    knn.set_params(n_neighbors = i)\n    \n    # fit the algo\n    knn.fit(x_train,y_train)\n    \n    # Update the training score and test scores\n    \n    train_score.append(knn.score(x_train,y_train))\n    \n    test_score.append(knn.score(x_test,y_test))","154e16f2":"train_score","ff84e6e8":"test_score","0e0d22e4":"plt.plot(neighbors, train_score, label = \"Train Score\")\nplt.plot(neighbors, test_score, label = \"Test Score\")\nplt.xticks(np.arange(1,21,1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model Score\")\nplt.legend()\n\nprint(f\"Maximum KNN score on the test data: {max(test_score)*100:.2f}%\")","0ffb14ba":"# Create a hyperparameter grid for logostic regression\n\nlog_reg_grid = {\"C\" : np.logspace(-4, 4, 20),\n                \"solver\" : [\"liblinear\"]}\n\n# Create a hyperparameter for RandomForest Classifier\n\nrf_grid = {\"n_estimators\" : np.arange(10, 1000, 50),\n           \"max_depth\" : [None, 3, 5, 10],\n           \"min_samples_split\" : np.arange(2, 20, 2),\n           \"min_samples_leaf\" : np.arange(1, 20, 2)}","5a74f358":"# Tune using RandomizedSearchCV\n\nnp.random.seed(42)\n\n# LogisticRegression\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                                param_distributions = log_reg_grid,\n                                cv = 5,\n                                n_iter = 20,\n                                verbose = True)\n# Fit the model\n\nrs_log_reg.fit(x_train, y_train)","31fc5823":"# Check the best params\nrs_log_reg.best_params_","660fd990":"rs_log_reg.score(x_test,y_test)","e5637e39":"# Set the random seed\nnp.random.seed(42)\n\n#RandomForest\n\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                           param_distributions = rf_grid,\n                           cv = 5,\n                           n_iter = 20,\n                           verbose = True)\n\n# Fit the model\nrs_rf.fit(x_train,y_train)","6154daf2":"# Best parameters\n\nrs_rf.best_params_","af04b47b":"rs_rf.score(x_test,y_test)","315eef4d":"# Set up hyperparameter\nlog_reg_grid = {\"C\" : np.logspace(-4, 4, 20),\n                \"solver\" : [\"liblinear\"]}\n\n# Set the grid \ngs_log_reg = GridSearchCV(LogisticRegression(),\n                          param_grid = log_reg_grid,\n                          cv = 5,\n                          verbose = True)\n\n# Fit the model\ngs_log_reg.fit(x_train,y_train)","e65e6460":"# Check the best parameters\ngs_log_reg.best_params_","f28dbf46":"# Evaluate\n\ngs_log_reg.score(x_test,y_test)","8b6e5637":"y_pred = gs_log_reg.predict(x_test)\n\ny_pred","6c55a05f":"plot_roc_curve(gs_log_reg, x_test, y_test);","b46e321f":"def plot_conf_mat(y_test,y_preds):\n    fig, ax = plt.subplots(figsize=(3,3))\n    ax = sns.heatmap(confusion_matrix(y_test,y_preds),\n                     annot = True,\n                     cbar = False)\n    \n    plt.xlabel(\"Predicted label\")\n    plt.ylabel(\"True label\")\n    \nplot_conf_mat(y_test,y_pred)","e5bb06a2":"print(classification_report(y_test,y_pred))","3e881f52":"# Create a new classifier using best params\n\nclf  = LogisticRegression(C=0.23357214690901212,\n                          solver = \"liblinear\")","2e05f643":"# Cross-validated Accuracy\n\ncv_acc = cross_val_score(clf,x,y,cv=5,scoring=\"accuracy\")\n\ncv_acc\n\nacc_mean = np.mean(cv_acc)\nacc_mean","c1d1a62b":"# Cross-validated Precision\n\ncv_pre = cross_val_score(clf,x,y,cv=5,scoring=\"precision\")\n\ncv_pre\n\npre_mean = np.mean(cv_pre)\npre_mean","e211a3e1":"# Cross-validated Recall\n\ncv_re = cross_val_score(clf,x,y,cv=5,scoring=\"recall\")\n\ncv_re\n\nre_mean = np.mean(cv_re)\nre_mean","388ec408":"# Cross-validated F1 score\n\ncv_f1 = cross_val_score(clf,x,y,cv=5,scoring=\"f1\")\n\ncv_f1\n\nf1_mean = np.mean(cv_f1)\nf1_mean","9183e52e":"# Visualizing cross validated matrix\n\ncv_metrics = pd.DataFrame({\"Accuracy\":acc_mean,\n                           \"Precision\":pre_mean,\n                           \"Recall\":re_mean,\n                           \"F1\":f1_mean},\n                            index = [0])\ncv_metrics.T.plot.bar(title = \"Crovalidated Classification Report\",legend = False);","d329eff8":"clf  = LogisticRegression(C=0.23357214690901212,\n                          solver = \"liblinear\")\n\n# fit the model\n\nclf.fit(x_train,y_train);","06f701fd":"# Coefficient : how the attributes contribute for prediction\nclf.coef_","2e2f6130":"# Match the coefficient to the columns\nfeature_dict = dict(zip(heart_disease.columns,list(clf.coef_[0])))\nfeature_dict","89f4888d":"# Visualize it\nfeature_df = pd.DataFrame(feature_dict, index = [0])\nfeature_df.T.plot.bar(title=\"Feature Importance\",legend = False);","64350552":"if we see in the above result Logistic Regression has higher value. ","25491235":"the model didnt improverd it remained the same, let leave it as it is for now","7460ce53":"Visualizing X an Y","2fb4b602":"## Hyperparameter with RandomizedSearchCV","eaeaa841":"why we need to split our data into test and train : https:\/\/docs.microsoft.com\/en-us\/analysis-services\/data-mining\/training-and-testing-data-sets?view=asallproducts-allversions#:~:text=Separating%20data%20into%20training%20and,of%20evaluating%20data%20mining%20models.&text=Because%20the%20data%20in%20the,the%20model's%20guesses%20are%20correct.","2e9a52b5":"> [NOTE]: The upcoming steps can be performed with an attributes but here we are going to use some columns which gets our eyes","e1f66a78":"## Correlation between the Attributes and the target","8e1929dc":"# Predicting heart disease using ML\n\nIn this notebook we are going to use various ML libraries in an attempt to build a ML model which can predict whether some one has heart disease or not","2627acd9":"> ROC curve & AUC score\n\n* Check out this link : https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/roc-and-auc#:~:text=An%20ROC%20curve%20","c690e38a":"bye bye RandomForestRegression...","6f5b5353":"Here we are looking for outliers but we dont have any here","32f9cabe":"These are calculates only with single split","64a6d67b":"> This is different for different models\n\nfor LogisticRegression","c46ed112":"### Train and Test the set by Fitting it into a Model","0331d539":"to know more about it : https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html","86f61bf3":"> Logistic Regression\nAnd\n> RandomForest","09043989":"The above warning suggesting us an way to improve the logistic regression model","c143f470":"### Data Exploration\nThe goal here is to gain more information from the data\n\n1. What are you trying to solve ?\n2. What kind of data we have and how to treat different types ?\n3. What is missing from the data and how to deal with it ?\n4. Where are the outliers and why should you care about them?\n5. How can you add, change or remove features to get more out of your data ?","9fa3b41b":"We can analyse the above graph and improve our dataset much more","c53fac2c":"AUC score = 0.92","ccb799d5":"To know more about correlation : https:\/\/www.displayr.com\/what-is-a-correlation-matrix\/#:~:text=A%20correlation%20matrix%20is%20a,a%20diagnostic%20for%20advanced%20analyses.","48c19af8":"## Preparing the tools\n\n* pandas\n* numpy\n* seaborn\n* matplotlib\n\nfor data analysis and manipulation\n\n* Regression\n* Classification\n\nScikit-learn models\n\n* Spliting\n* Cross validation\n* Evaluation method libraries\n \nfor evaluation \n","75f9ca9f":"## Improving the model","63ad8bd2":"## Modeling \n* We have a classification problem\n\nTo know more about Supervised data Classification VS Regression:https:\/\/www.google.com\/search?q=classification+vs+regression&rlz=1C1ONGR_enIN973IN973&oq=classification+vs+&aqs=chrome.0.0i433i512j0i512j69i57j0i512l7.9790j1j15&sourceid=chrome&ie=UTF-8","007fd5a2":"## Approach followed\n\n1. Problem definition\n2. Data \n3. Evaluation\n4. Features\n5. Modelling\n6. Experimenting(will be followed in ever step)\n\n### 1. Problem Defenition\n\n> Given the required clinical data can we predict whether a person has heart disease or not\n\n### 2. DATA\n\n* refer UCI : https:\/\/archive.ics.uci.edu\/ml\/datasets\/heart+Disease\n* refer Kaggle : https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\n\nWe are going to user 14(Widely used) dataset out of 76(Original) attributes\n\n### 3. Evaluation\n\n> If we can get an accuracy of accuracy >= 95% .Then we can say it as a good model.\n\n### 4. Features\n\n*** Creating Data Dictionary ***\n\n \n#### Attributes (13) : (Independent variables)\n\n* age\n* sex\n* chest pain type (4 values)\n* resting blood pressure\n* serum cholestoral in mg\/dl\n* fasting blood sugar > 120 mg\/dl\n* resting electrocardiographic results (values 0,1,2)\n* maximum heart rate achieved\n* exercise induced angina\n* oldpeak = ST depression induced by exercise relative to rest\n* the slope of the peak exercise ST segment\n* number of major vessels (0-3) colored by flourosopy\n* thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n\n#### Final (1) : (Dependent on Attributes)\n\n* target (yes - 1 \/ no - 0)","f8c1b4c4":"Yes we can use Logistic\"Regression\" for classification for more details : https:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#logistic-regression","d983c333":"this based on our present dataset it may be different in the real world","ebe530df":"we don't have any missing data here ","1da2b139":"Still we did get the same as baseline and RandomizedSearchCV :\\","a6e8fdf1":"The heighest score is 75.41%\n* [note] we can still try different range of n_neighbors int this case i tried some and stil didn't get anything to the mark","47ce23da":"* LogisticRegression\n* KNeighbors\n* Ensembler(RandomForestClassifier)","342714ba":"## Heart Disease Frequency VS Sex\n\n### SEX\n* 1 - MALE\n* 0 - FEMALE\n\n### Target\n* 1 - YES\n* 0 - NO","a07d69da":"# Conclusion\n* We got accuracy of 88.5% which is low in this case\n* We cannot practically implement it\n\n> Want we can do?\n\n* Try better model like CatBoost or XGBoost\n* Try to collect more data","862e64e5":"A perfectly distributed data looks like : https:\/\/www.simplypsychology.org\/normal-distribution.html","69d22b91":"## Load the data","0f63ee96":"### Improving the Logistic model\n\nLet revisit what we did while improving the model\n* By hand - KNN eliminated\n* RandomizedSearchCv - RandomForestClassification eliminated\n* GridSearchCv - upcoming...","4a359dd2":"Good bye KNN....","919fd1b6":"as the length is 91(91 differnt values) we cannot use bar graph ","3e49e930":"### Using cross validation\n> Classification report\n\n* Accuracy\n* Precision\n* Recall\n* F1 score","f3be526e":"## Hear Disease vs Chest pain types\n Chest pain:\n* 0: Typical angina - related to heart\n* 1: Atypical angina - not related to heart\n* 2: Non-anginal - not related to heart\n* 3: Asymptomatic - not showing signs of disease","b354d15b":"The score has certainly increased by 0.32","94f6a055":" We have to make prediction inorder to compare the models","2bd1efe4":"perfect model AUC score = 1","950a411a":"to know more about GridSearchCv : https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html","8fb737e1":"> Classification report\n\n* Precision\n* Recall\n* F1 score","c2f0491e":"## Evaluating the models beyond score\n* ROC curve & AUC score \n* Confusion matrix\n* Classification report\n    * Precision\n    * Recall\n    * F1 score","a249103c":"## Tuning Hyperperameter using GridSearchCv\n> LogisticRegression\n","658c808f":"## Important Features","2f84bcaa":"As the finding the trend by ourself is dificult we will make the ML model to do the work for us","9580fadb":"> Confusion matrix","fe5ade12":"we are going to do the following:\n\n* Hyperparameter tuning\n* Feature importance\n* Confusion Matrix\n* Cross-validation\n* Precision (mean absolute error)\n* Recall (mean squared error)\n* F1 score (root mean squared error)\n* Classification report\n* ROC curve\n* Area Under the curve\n","50a17f4a":"## Model comparision","892c5079":"But still Logistic Regression holds upperhand here","70876020":"***RandomForestClassifier***","10f9fa24":"### Hyperparameter Tuning\n\n> KNN","2fd5c26d":"### Max heart rate(thalach)  VS  Age for heart disease"}}