{"cell_type":{"bc0a4a6c":"code","0b750efc":"code","a5e1c485":"code","d026b4f5":"code","6ed3de0a":"code","109da6e4":"code","1e942e4e":"code","cfbd6d8e":"code","f301f82f":"code","2ebb0be3":"code","58b3cee3":"code","0c52a242":"code","e8bcda8f":"code","12153fc3":"markdown","bc0c70b1":"markdown","c77faa1b":"markdown","ba6b5b04":"markdown","9ed84846":"markdown"},"source":{"bc0a4a6c":"import pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sklearn.datasets","0b750efc":"dt=sklearn.datasets.load_breast_cancer()\nprint(dt.DESCR)","a5e1c485":"breast_data=pd.DataFrame(dt.data,columns=dt.feature_names)\nbreast_data","d026b4f5":"dt.feature_names","6ed3de0a":"from sklearn.decomposition import PCA\nX=breast_data\ny=dt.target","109da6e4":"cls=PCA(n_components=2)\ncls.fit(X)\nX_pca=cls.transform(X)\nX_pca=pd.DataFrame(X_pca,columns=[\"PCA1\",\"PCA2\"])\nX_pca[\"y\"]=y\nX_pca","1e942e4e":"from collections import Counter\nCounter(y)","cfbd6d8e":"from  sklearn.ensemble import RandomForestClassifier as rfc\ncls=rfc(n_estimators=500,random_state=666)\ncls.fit(X,y)\nresult=pd.DataFrame(cls.feature_importances_,index=[dt.feature_names],columns=[\"Significance\"])\nresult_sort=result.sort_values(by=[\"Significance\"],ascending=False)\nresult_sort","f301f82f":"plt.bar(range(0,30),result_sort[\"Significance\"])\nplt.xticks(range(30), \n           result_sort.index, rotation=90)\nplt.xlim([-1, 30])\nplt.tight_layout()\nplt.title(\"Significance of each variable\")\nplt.show()","2ebb0be3":"sig=breast_data[[\"worst perimeter\",\"worst concave points\"]]\nreg_data=pd.concat([sig,X_pca],axis=1)\nreg_data","58b3cee3":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(reg_data.drop(columns=[\"y\"]),reg_data[\"y\"],\n                                               train_size=0.8,random_state=666)","0c52a242":"from sklearn.linear_model import LogisticRegression as LR\nX_train=pd.DataFrame(X_train,columns=[\"worst perimeter\",\"worst concave points\",\"PCA1\",\"PCA2\"])\nX_test=pd.DataFrame(X_test,columns=[\"worst perimeter\",\"worst concave points\",\"PCA1\",\"PCA2\"])\ncls=LR()\ncls.fit(X_train[[\"PCA1\",\"PCA2\"]],y_train) #Feature Extraction: PCA\ncls.score(X_test[[\"PCA1\",\"PCA2\"]],y_test)","e8bcda8f":"cls.fit(X_train[[\"worst perimeter\",\"worst concave points\"]],y_train) # Feature Selection: Random Forest\ncls.score(X_test[[\"worst perimeter\",\"worst concave points\"]],y_test)","12153fc3":"The two most significant variable is \"worst perimeter\" and \"worst concave points\".\n\n<br>\n<br>\n\n# Logistic Regression:\n\nIn this part, I will use the extracted and seleced variable above to construct the logistic regression model and observe the accuracy of each model.","bc0c70b1":"# Conclusion:\nIn this case, the final result shows that using feature selection to reduce the dimension of data is more effective and have a higher accuracy.","c77faa1b":"Confirm that this is not a sparse data.\n<br>\n<br>\n# Feature Selection: Random Forest","ba6b5b04":"<br>\n<br>\n\n# Feature Extraction: PCA","9ed84846":"# Introduction:\n### In this report, I am going to compare the accuracy of the prediction of breast_cancer of methods of feature engineering. Feature selection and feature extraction will be included in the process."}}