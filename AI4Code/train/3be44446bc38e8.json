{"cell_type":{"b4bdf770":"code","a96ee4df":"code","de9f7993":"code","e00b7207":"code","b949658b":"code","911136ec":"code","34e4eff1":"code","a3380d62":"code","4586b0f6":"code","784d3662":"code","118a3b99":"code","7f8cd759":"code","340b2feb":"code","66b39cf1":"code","76c2a1e9":"code","ca9ac993":"code","1822d402":"code","3ceced1d":"code","870503c5":"code","dfbc0ef9":"code","b640740b":"code","f55bddc8":"code","71754180":"markdown","ce7fb952":"markdown","89ec09bc":"markdown","9b658303":"markdown","e2f75d00":"markdown","cf5a2cf5":"markdown","f1bcc38f":"markdown","c2e4d6cd":"markdown","f5370e95":"markdown"},"source":{"b4bdf770":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output \n# when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a96ee4df":"from sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC","de9f7993":"full_train = pd.read_csv('\/kaggle\/input\/data-science-london-scikit-learn\/train.csv', header=None)\nfull_test = pd.read_csv('\/kaggle\/input\/data-science-london-scikit-learn\/test.csv', header=None)\ntrainLabels = pd.read_csv('\/kaggle\/input\/data-science-london-scikit-learn\/trainLabels.csv', names=['y'])","e00b7207":"full_train.head()","b949658b":"full_train.shape,full_test.shape,trainLabels.shape","911136ec":"# check out missing values\nfull_train.info()\nprint('-'*50)\nfull_test.info()","34e4eff1":"# check whether exist the unbalanced-problems\ntrainLabels.apply(pd.value_counts)","a3380d62":"models = {}\nmodels['LR'] = LogisticRegression()\nmodels['LDA'] = LinearDiscriminantAnalysis()\nmodels['KNN'] = KNeighborsClassifier()\nmodels['DT'] = DecisionTreeClassifier()\nmodels['RF'] = RandomForestClassifier()\nmodels['NB'] = GaussianNB()\nmodels['SVM'] = SVC()","4586b0f6":"baseline_mean_acc = {}\nresults = []\nfor model in models:\n    kfold = KFold(n_splits=5)\n    cv_results = cross_val_score(models[model],full_train, np.ravel(trainLabels),cv=kfold,scoring='accuracy')\n    results.append(cv_results)\n    baseline_mean_acc[model] = round(cv_results.mean(), 3)\n\nbaseline_mean_acc","784d3662":"from sklearn.preprocessing import StandardScaler, Normalizer\n\nstd = StandardScaler()\nstd_train_data = std.fit_transform(full_train)\n\nnorm = Normalizer()\nnorm_train_data = norm.fit_transform(full_train)","118a3b99":"std_mean_acc = {}\nresults_std = []\nfor model in models:\n    kfold = KFold(n_splits=5)\n    cv_results_std = cross_val_score(models[model],std_train_data, np.ravel(trainLabels),cv=kfold,scoring='accuracy')\n    results_std.append(cv_results_std)\n    std_mean_acc[model] = round(cv_results_std.mean(), 3)\n    \nstd_mean_acc","7f8cd759":"norm_mean_acc = {}\nresults_norm = []\nfor model in models:\n    kfold = KFold(n_splits=5)\n    cv_results_norm = cross_val_score(models[model], norm_train_data, np.ravel(trainLabels),cv=kfold,scoring='accuracy')\n    results_norm.append(cv_results_norm)\n    norm_mean_acc[model] = round(cv_results_norm.mean(), 3)\n    \nnorm_mean_acc","340b2feb":"from sklearn.decomposition import PCA\n\npca = PCA(0.85, whiten=True)\npca_train_data = pca.fit_transform(full_train)\nprint(pca_train_data.shape,'\\n')\n\nexplained_variance = pca.explained_variance_ratio_ \nprint(explained_variance)","66b39cf1":"pca_mean_acc = {}\nresults_pca = []\nfor model in models:\n    kfold = KFold(n_splits=5)\n    cv_results_pca = cross_val_score(models[model], pca_train_data, np.ravel(trainLabels),cv=kfold,scoring='accuracy')\n    results_pca.append(cv_results_pca)\n    pca_mean_acc[model] = round(cv_results_pca.mean(), 3)\n    \npca_mean_acc","76c2a1e9":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=[12,7])\n\n\nplt.plot(range(len(baseline_mean_acc)), list(baseline_mean_acc.values()), label='Baseline')\nplt.plot(range(len(std_mean_acc)), list(std_mean_acc.values()), label='Std_scale')\nplt.plot(range(len(norm_mean_acc)), list(norm_mean_acc.values()), label='Norm_scale')\nplt.plot(range(len(pca_mean_acc)), list(pca_mean_acc.values()), label='PCA')\n\n\nplt.legend(loc='lower right')\nplt.title('Approach comparison')\nplt.xlabel('Models')\nplt.ylabel('Accuracy')\nplt.xticks(range(len(baseline_mean_acc)), list(baseline_mean_acc.keys()))\nplt.yticks(np.arange(0.75, 0.93, 0.005))\nplt.grid()\nplt.show()","ca9ac993":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.mixture import GaussianMixture","1822d402":"X = np.r_[full_train,full_test]\nprint('X shape :',X.shape)","3ceced1d":"# USING THE GAUSSIAN MIXTURE MODEL \n# The Bayesian information criterion (BIC) can be used to select the number of components \n# in a Gaussian Mixture in an efficient way. As the AIC does.\n\nlowest_bic = np.infty\nbic = []\nn_components_range = range(1, 10)\n\n# The GaussianMixture comes with different options to constrain the covariance of the difference classes estimated: \n# spherical, diagonal, tied or full covariance.\n\ncv_types = ['spherical', 'tied', 'diag', 'full']\nfor cv_type in cv_types:\n    for n_components in n_components_range:\n        gmm = GaussianMixture(n_components=n_components,covariance_type=cv_type)\n        gmm.fit(X)\n        bic.append(gmm.bic(X))\n        if bic[-1] < lowest_bic:\n            lowest_bic = bic[-1]\n            best_gmm = gmm\n            \nbest_gmm.fit(X)\ngmm_train = best_gmm.predict_proba(full_train)\ngmm_test = best_gmm.predict_proba(full_test)","870503c5":"#KNN \nknn = KNeighborsClassifier()\n\n#USING GRID SEARCH\nparam_grid_knn = {\"n_neighbors\": range(1, 11, 2), \n              \"weights\": ['uniform', 'distance']}\n\ngrid_search_knn = GridSearchCV(estimator=knn, \n                               param_grid=param_grid_knn, \n                               cv = 5, n_jobs=-1,\n                               scoring='accuracy').fit(gmm_train, trainLabels.values.ravel())\n\nknn_best = grid_search_knn.best_estimator_\n\nprint('KNN Best Score', grid_search_knn.best_score_)\nprint('KNN Best Params',grid_search_knn.best_params_)\nprint('KNN Accuracy',cross_val_score(knn_best, gmm_train, trainLabels.values.ravel(), cv=5).mean())","dfbc0ef9":"#SVM\nsvc = SVC()\n\n#USING GRID SEARCH\nparam_grid_svm = {'C':[1,10,100,1000],\n              'gamma':[1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel':['linear','rbf']}\n\ngrid_search_svm = GridSearchCV(estimator=svc, \n                               param_grid=param_grid_svm, \n                               cv = 5, n_jobs=-1,\n                               scoring='accuracy').fit(gmm_train, trainLabels.values.ravel())\n\nsvm_best = grid_search_svm.best_estimator_\n\nprint('SVM Best Score',grid_search_svm.best_score_)\nprint('SVM Best Params',grid_search_svm.best_params_)\nprint('SVM Accuracy',cross_val_score(svm_best,gmm_train, trainLabels.values.ravel(), cv=5).mean())","b640740b":"# Fitting our model\nsvm_best.fit(gmm_train,trainLabels.values.ravel())\npred  = svm_best.predict(gmm_test)","f55bddc8":"submission = pd.DataFrame(pred)\nsubmission.columns = ['Solution']\nsubmission['Id'] = np.arange(1,submission.shape[0]+1)\nsubmission = submission[['Id', 'Solution']]\nsubmission.to_csv('submission_with_GMM.csv', index=False)","71754180":"# 1. **Some EDA**","ce7fb952":"# **Principal Component Analysis (PCA)**","89ec09bc":"# Gaussian Mixture and Grid Search\n\nLets take the above 2 algorithms (**KNN and SVM**) which gave maximum accuracy for the further analysis","9b658303":"# **3. Feature Engineering**","e2f75d00":"# **Visualisation**","cf5a2cf5":"# **4. Submission**","f1bcc38f":"# Feature Scaling\n\nTwo approaches are shown below:\n\n- The StandardScaler assumes your data is normally distributed within each ***feature*** and will scale them such that the distribution is now centred around 0, with a standard deviation of 1.\n\n- The normalizer scales each ***value*** by dividing each value by its magnitude in n-dimensional space for n number of features.","c2e4d6cd":"The predict_proba method take in new data points and predict the probability that this data point came from each Gaussian distribution.","f5370e95":"# 2. **Explore baseline models**"}}