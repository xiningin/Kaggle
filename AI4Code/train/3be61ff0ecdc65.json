{"cell_type":{"723afa22":"code","589e31b7":"code","dae64ae1":"code","06d8cc2a":"code","b7a2a7e1":"code","835ddc64":"code","60d68473":"code","799ae0a0":"code","95894351":"code","2e66c94d":"code","9d95ff9b":"code","cb835b9a":"code","f913019b":"code","79821e74":"code","6f66ebd9":"code","9b17b4a4":"code","8ce31f78":"code","617317e0":"code","e709e3b6":"code","e404234f":"code","66393f15":"code","967b46d8":"code","a1b2fe73":"code","5aa42758":"code","431eba83":"code","79242d77":"code","4007b8ca":"code","d9c5e83e":"markdown","1bca6ac5":"markdown","131a0805":"markdown","7d560198":"markdown","a79b0486":"markdown","45008b26":"markdown","dc07dba3":"markdown","ab413392":"markdown","dfb6131a":"markdown"},"source":{"723afa22":"# Basic Data manipulation and math functions\nimport pandas as pd\nimport numpy as np\nimport random\nimport scipy\n\n# File listing, creating directory paths etc. and memory management\nimport os\nimport gc\n\n# Garphing\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# NLP specific and string functionalities\nimport re, string\nfrom nltk.tokenize import RegexpTokenizer\nfrom stop_words import get_stop_words\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.tokenize import TweetTokenizer\n\n# Iporting fucntions from the popular sklearn ML module\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import GridSearchCV, train_test_split","589e31b7":"# Constants\nrun_gridsearch__=False\nnrows__ = 1000\nfit_sample_size__=1.1\nrf_n_estimators__=1000\ntrain_inloc_ = '..\/input\/train.csv'\ntest_inloc_ = '..\/input\/test.csv'\nlabels_ = []\nen_stop_ = get_stop_words('en')","dae64ae1":"p_stemmer = PorterStemmer()","06d8cc2a":"rsrc = pd.read_csv(\"..\/input\/resources.csv\")\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","b7a2a7e1":"train.head()","835ddc64":"test.head()","60d68473":"rsrc.head()","799ae0a0":"print(\"Train:\", train.shape)\nprint(\"Test:\", test.shape)\nprint(\"Resource:\", rsrc.shape)","95894351":"train.isnull().sum()[train.isnull().sum()>0]","2e66c94d":"test.isnull().sum()[test.isnull().sum()>0]","9d95ff9b":"rsrc.isnull().sum()[rsrc.isnull().sum()>0]","cb835b9a":"train = train.fillna('')\ntest = test.fillna('')\nrsrc = rsrc.fillna('')","f913019b":"# Combining the project essay 1, 2, 3, 4 \nfor df in [train, test]:\n    df[\"essays\"] = df[\"project_essay_1\"] + df[\"project_essay_2\"] + df[\"project_essay_3\"] + df[\"project_essay_4\"]\n    df.drop(['project_essay_1', 'project_essay_2', 'project_essay_3', 'project_essay_4'], axis=1, inplace=True)\n","79821e74":"def data_preview_str(df, name):\n    tot_feats = 0\n    if name=='train':\n        for col in [i for i in df.columns if (df[i].dtypes=='O') & (i.find('_id')<0)]:\n            tot_feats = tot_feats + len(df[col].unique())-1\n            print(\" %s \"% col, '# unique: ',len(df[col].unique()))\n            xdf = df.groupby(col).agg({'id':'count','project_is_approved':'sum'})\n            xdf.columns = ['#Records','Response_Rate']\n            xdf['Response_Rate'] = xdf['Response_Rate']\/sum(xdf['Response_Rate'])\n            xdf['%Records'] = xdf['#Records']*100\/sum(xdf['#Records'])\n            xdf.sort_values('#Records', ascending=[0], inplace=True)\n            print(xdf.loc[:,['#Records','%Records','Response_Rate']].head(10))\n            print(\"%d of %d\"%(min(10,xdf.shape[0]), xdf.shape[0]))\n            del xdf\n            print(\"-\"*50)        \n    else:\n        for col in [i for i in df.columns if (df[i].dtypes=='O') or (len(df[i].unique())<=15)]:\n            tot_feats = tot_feats + len(df[col].unique())-1\n            print(\" %s \"% col, '# unique: ',len(df[col].unique()))\n            xdf = pd.DataFrame(df[col].value_counts())\n            xdf['%Records'] = xdf[col]*100\/sum(xdf[col])\n            xdf.columns = ['#Records','%Records']\n            print(xdf.head())\n            del xdf\n            print(\"-\"*50)\n    ll=[i for i in df.columns if (df[i].dtypes!='O') and (len(df[i].unique())>15)]\n    tot_feats= tot_feats + len(ll)\n    print(\"Creating dummies will lead to a total of %d features\"% tot_feats)\n    return 1","6f66ebd9":"data_preview_str(train, 'train')","9b17b4a4":"data_preview_str(test, 'test')","8ce31f78":"# Combine Train and Test ( for easier submissions)\ntrain['train_flag'] = 1\ntest['train_flag'] = 0\nfull_data = pd.concat([train,test], sort=False)\nfull_data.groupby('train_flag')['id','project_is_approved'].count()","617317e0":"# Dummy or Flag Features\nfull_data = pd.get_dummies(full_data, columns =['teacher_prefix','school_state','project_grade_category'])\n# full_data.drop(['teacher_prefix','school_state','project_grade_category'], axis=1, inplace=True)\nfull_data.head()","e709e3b6":"# Date features\nfull_data['project_submitted_datetime'] = pd.to_datetime(full_data['project_submitted_datetime'])\nfull_data['day'] = full_data['project_submitted_datetime'].dt.day\nfull_data['dayofweek'] = full_data['project_submitted_datetime'].dt.dayofweek\nfull_data['month'] = full_data['project_submitted_datetime'].dt.month\nfull_data['year'] = full_data['project_submitted_datetime'].dt.year\nfull_data.drop('project_submitted_datetime', axis=1, inplace=True)\nfull_data[['day','dayofweek','month','year']].head()","e404234f":"# Project Title attributes : First impression is a Last-ing one\nfull_data['pt_caps'] = full_data['project_title'].str.findall(r'[A-Z]').str.len()\/full_data['project_title'].str.len()\nfull_data['pt_special_chars'] = full_data['project_title'].str.findall(r'[^A-Za-z0-9]').str.len()\/full_data['project_title'].str.len()\nfull_data['pt_len'] = full_data['project_title'].str.len()\nfull_data['pt_words'] = full_data['project_title'].str.findall(r'[\\s]').str.len()\/full_data['project_title'].str.len()\n\n# project_resource_summary attributes\nfull_data['prs_caps'] = full_data['project_resource_summary'].str.findall(r'[A-Z]').str.len()\/full_data['project_title'].str.len()\nfull_data['prs_special_chars'] = full_data['project_resource_summary'].str.findall(r'[^A-Za-z0-9]').str.len()\/full_data['project_title'].str.len()\nfull_data['prs_len'] = full_data['project_resource_summary'].str.len()\nfull_data['prs_words'] = full_data['project_resource_summary'].str.findall(r'[\\s]').str.len()\/full_data['project_title'].str.len()\n\n# essays\nfull_data['ess_caps'] = full_data['essays'].str.findall(r'[A-Z]').str.len()\/full_data['project_title'].str.len()\nfull_data['ess_special_chars'] = full_data['essays'].str.findall(r'[^A-Za-z0-9]').str.len()\/full_data['project_title'].str.len()\nfull_data['ess_len'] = full_data['essays'].str.len()\nfull_data['ess_words'] = full_data['essays'].str.findall(r'[\\s]').str.len()\/full_data['project_title'].str.len()","66393f15":"# aggregating it based on the project id\n\nrsrc[\"description\"].fillna(\"\", inplace = True)\n\nrsrc_grp = pd.DataFrame(rsrc.groupby(\"id\").agg({\"description\" : lambda x : \"\".join(x),\n                   \"quantity\": [\"sum\", \"mean\"],\n                   \"price\" : [\"sum\", \"mean\"]}))\n\nrsrc_grp.reset_index(inplace = True)\nrsrc_grp.columns.droplevel(0)\nrsrc_grp.columns= [\"id\", \"description\", \"quantity_sum\",\n                \"quantity_mean\", \"price_sum\", \"price_mean\"]\n# Merge the aggregated data with the combined Data frame\nprint(\"Before merge:\",full_data.shape)\nfull_data = pd.merge(full_data, rsrc_grp, on = \"id\", how = \"left\")\nprint(\"After merge:\",full_data.shape)\ndel rsrc_grp","967b46d8":"text_cols = [\n    'project_title', \n    'essays', \n    'project_resource_summary',\n    'description'\n]\nnfeats=5\n\nprint(full_data[text_cols].head(1).T)\nfrom tqdm import tqdm\n\nfor c in text_cols:\n    tfidf = TfidfVectorizer(\n        max_features=nfeats,\n        norm='l2',\n        sublinear_tf = True,\n        stop_words = 'english',\n        analyzer = 'word',\n        min_df = 5,\n        max_df = .9,\n        smooth_idf = False)\n    \n    print(\"*** %s ***\"% c)\n    print(tfidf)\n    \n    tfidf.fit(full_data[c])\n    tfidf_train = np.array(tfidf.transform(full_data[c]).toarray(), dtype=np.float16)\n    for i in range(nfeats):\n        full_data[c + '_tfidf_' + str(i)] = tfidf_train[:, i]\n    del tfidf, tfidf_train\n    gc.collect()\n    \nprint('Done.')\n","a1b2fe73":"remcols = [\n    'id',\n    'teacher_id',\n    'project_title', \n    'essays', \n    'project_resource_summary',\n    'description',\n    'project_subject_categories', 'project_subject_subcategories', 'project_is_approved'\n]\n\n\ntrain_idcol = full_data.loc[full_data['train_flag']==1,'id']\ntrain_teacher_idcol = full_data.loc[full_data['train_flag']==1,:'teacher_id']\n\ntest_idcol = full_data.loc[full_data['train_flag']==1,'id']\ntest_teacher_idcol = full_data.loc[full_data['train_flag']==1,:'teacher_id']\n\n\nfull_data.drop(remcols, axis = 1, inplace = True)\n\nX = full_data.loc[full_data['train_flag']==1, :].reset_index()\ny = train[\"project_is_approved\"].reset_index()\n\nprint(X.shape, y.shape)\n\nX_test = full_data.loc[full_data['train_flag']==0, :].reset_index()\n\ndel train, test\n\n#  Data Splitting for validation\n\nxTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.3)","5aa42758":"yTrain.drop('index', axis=1, inplace=True)\nyTest.drop('index', axis=1, inplace=True)","431eba83":"xTrain.columns","79242d77":"first_rf = RandomForestClassifier(n_estimators=1,random_state=1)\nfirst_rf.fit(xTrain, yTrain.values.ravel())\nimportance = first_rf.feature_importances_\nimportance = pd.DataFrame(importance, index=xTrain.columns, \n                          columns=[\"Importance\"])\n\nimportance[\"Std\"] = np.std([tree.feature_importances_\n                            for tree in first_rf.estimators_], axis=0)\n\nimportance.sort_values('Importance', ascending=[0], inplace=True)\n\nx = importance.index\ny = importance['Importance']\nyerr = importance['Std']\nplt.figure(figsize=(50,5))\nplt.bar(x, y, yerr=yerr, align=\"center\")\nplt.show()","4007b8ca":"score_dict = {'AUC': 'roc_auc'}\n# clf = RandomForestClassifier(max_depth=2, random_state=0)\nclf = GridSearchCV(\n    RandomForestClassifier(random_state=0),\n    param_grid={\n        'n_estimators':[1],\n        'max_depth':[2, 4, 8, 10, 12, 14, 16, 18, 20, 22, 24]},\n    scoring = score_dict,\n    cv=5,refit='AUC')\n\nclfs = clf.fit(xTrain,yTrain.values.ravel())\n\nresults = clfs.cv_results_\n\n\npd.DataFrame(results['mean_test_AUC']).rename(columns= {0:'AUC'}).plot()\n\n","d9c5e83e":"### Let us create TFiDF features from our texts\n**tf\u2013idf** or **TFIDF**, short for term frequency\u2013inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.[1] It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. Tf-idf is one of the most popular term-weighting schemes today; 83% of text-based recommender systems in digital libraries use tf-idf.","1bca6ac5":"### Let us build some simple features based on the EDA above","131a0805":"## Usually we perform a large amount of feature engineering to understand the problem and build better models. With the limited time we stop here and start fitting an RF model","7d560198":"### Let us now get rid of extra column sand prep the data for modeling","a79b0486":"### Let's do some quick data exploration","45008b26":"### There i smore to it, let us tune this model","dc07dba3":"### Treat misisng values\nThere are various ways in which missing values can be treated. Given that here values are missing for textual columns, we just replace it with '' (an empty string)\n","ab413392":"### Combine the 4 essays","dfb6131a":"### Check for Missing Values"}}