{"cell_type":{"4368ead4":"code","067b91ea":"code","4f2fdbb1":"code","b1552d5e":"code","9c03d4cd":"code","6f9e9c16":"code","4a2203dc":"code","7802cadf":"code","78ef653b":"code","d73dcbb6":"code","71d6fb0e":"code","e8a42109":"code","540dcfb3":"code","3d26827c":"code","9a2744ee":"code","c6edec35":"code","87518fa9":"code","c001bee8":"code","0cd5b84a":"code","edd14da5":"code","792ea782":"code","61990ead":"code","847a5374":"code","72a7c251":"code","a81d258f":"code","b379d978":"code","9839b836":"code","d8145281":"code","a1f64093":"code","0a441d82":"code","9b0d07b0":"code","07166766":"code","2f9b1fb2":"code","43edc169":"code","6c8d3794":"code","fd95fcef":"code","654d04ec":"code","9f6d6120":"code","8cd377ef":"code","4b2eacdc":"code","90e64e23":"code","9eac77ca":"code","2f30978d":"code","17ae6b2c":"code","6a7d5d61":"code","8ca727bf":"code","7c1095b7":"code","2dfc5ce5":"markdown","b5c08c21":"markdown","8c886558":"markdown","2b033a14":"markdown","0dd4b11c":"markdown","d4b15c5f":"markdown","7d8d3d2c":"markdown","76ecdf77":"markdown","87353362":"markdown","b0b959d6":"markdown","beb4c72d":"markdown","b2a4cc8f":"markdown","807b0ede":"markdown","597acd58":"markdown","ee0119ee":"markdown","c546418c":"markdown","1b5d9caa":"markdown","49e6cbb5":"markdown","d6a5d800":"markdown","20693250":"markdown","1fedf487":"markdown","02e6c470":"markdown","d26c0fa2":"markdown","0ea90088":"markdown","957da4f8":"markdown","aebe6f3d":"markdown","955e9e74":"markdown","f8e5e350":"markdown","e538fda5":"markdown","3c4342f0":"markdown","109b2d8c":"markdown","3a68c20d":"markdown","1d72fcd3":"markdown","71f6fc2f":"markdown","054ef3bf":"markdown","3725faf7":"markdown"},"source":{"4368ead4":"import numpy as np\nimport pandas as pd\nimport plotly as py\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import PolynomialFeatures\nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport seaborn as sns","067b91ea":"df = pd.read_csv('..\/input\/fish-market\/Fish.csv')\ndf.head()","4f2fdbb1":"df['Species'].nunique()","b1552d5e":"df.shape","9c03d4cd":"#checking for missing data\ndf.isnull().sum()","6f9e9c16":"df.info()","4a2203dc":"df['Species'].value_counts()","7802cadf":"sns.countplot(data = df, x = 'Species')","78ef653b":"corr=df.corr()\ncorr","d73dcbb6":"plt.figure()\nsns.heatmap(data=df.corr(),annot=True)","71d6fb0e":"sns.pairplot(df[[\"Weight\", \"Length1\", \"Length2\", \"Length3\", \"Height\", \"Width\",'Species']], diag_kind =\"kde\", hue='Species')","e8a42109":"df.describe().round(2)","540dcfb3":"#Box Plot and Distribution Plot for Dependent variable weight\nplt.figure(figsize=(20,3))\n\nplt.subplot(1,2,1)\nsns.boxplot(df.Weight,color='red')\nplt.title('Box Plot of Weight')\n\nplt.subplot(1,2,2)\nsns.distplot(a=df.Weight,color='green')\nplt.title('Distribution Plot of Weight')\nplt.show()","3d26827c":"Q1 = df['Weight'].quantile(0.25)\nQ3 = df['Weight'].quantile(0.75)\nIQR = Q3 - Q1\nprint(\"IQR: \",IQR)\nupper_limit = Q3 +(1.5*IQR)\nprint(\"upper_limit: \",upper_limit)\noutliers = df['Weight'][(df['Weight'] >upper_limit)]\noutliers","9a2744ee":"df = df.drop([142,143,144], axis=0)","c6edec35":"#Box Plot and Distribution Plot for Dependent variable weight\nplt.figure(figsize=(20,3))\n\nplt.subplot(1,2,1)\nsns.boxplot(df.Length1,color='red')\nplt.title('Box Plot of Length1')\n\nplt.subplot(1,2,2)\nsns.distplot(a=df.Length1,color='green')\nplt.title('Distribution Plot of Length1')\nplt.show()","87518fa9":"Q1 = df['Length1'].quantile(0.25)\nQ3 = df['Length1'].quantile(0.75)\nIQR = Q3 - Q1\nprint(\"IQR: \",IQR)\nupper_limit = Q3 +(1.5*IQR)\nprint(\"upper_limit: \",upper_limit)\noutliers = df['Length1'][(df['Length1'] >upper_limit)]\noutliers","c001bee8":"df = df.drop([141], axis=0)","0cd5b84a":"#Box Plot and Distribution Plot for Dependent variable weight\nplt.figure(figsize=(20,3))\n\nplt.subplot(1,2,1)\nsns.boxplot(df.Length2,color='red')\nplt.title('Box Plot of Length2')\n\nplt.subplot(1,2,2)\nsns.distplot(a=df.Length2,color='green')\nplt.title('Distribution Plot of Length2')\nplt.show()","edd14da5":"#Box Plot and Distribution Plot for Dependent variable weight\nplt.figure(figsize=(20,3))\n\nplt.subplot(1,2,1)\nsns.boxplot(df.Length3,color='red')\nplt.title('Box Plot of Length3')\n\nplt.subplot(1,2,2)\nsns.distplot(a=df.Length3,color='green')\nplt.title('Distribution Plot of Length3')\nplt.show()","792ea782":"#Box Plot and Distribution Plot for Dependent variable weight\nplt.figure(figsize=(20,3))\n\nplt.subplot(1,2,1)\nsns.boxplot(df.Height,color='red')\nplt.title('Box Plot of Height')\n\nplt.subplot(1,2,2)\nsns.distplot(a=df.Height,color='green')\nplt.title('Distribution Plot of Height')\nplt.show()","61990ead":"#Box Plot and Distribution Plot for Dependent variable weight\nplt.figure(figsize=(20,3))\n\nplt.subplot(1,2,1)\nsns.boxplot(df.Width,color='red')\nplt.title('Box Plot of Width')\n\nplt.subplot(1,2,2)\nsns.distplot(a=df.Width,color='green')\nplt.title('Distribution Plot of Width')\nplt.show()","847a5374":"df.describe().round(2)","72a7c251":"df_dummy = pd.get_dummies(df, columns=[\"Species\"])\nprint(df_dummy.shape)\ndf_dummy.head()","a81d258f":"#Now will split our dataset into Dependent variable and Independent variable\n# Dependant (Target) Variable:\ny = df_dummy['Weight']\n# Independant Variables:\nx = df_dummy.iloc[:,1:13]\n\nx[:5]","b379d978":"print(f\"Shape of Dependent Variable y = {y.shape}\")\nprint(f\"Shape of Independent Variable X = {x.shape}\")","9839b836":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state = 2)\n\nprint(f\"Shape of X_train = {X_train.shape}\")\nprint(f\"Shape of X_test = {X_test.shape}\")\nprint(f\"Shape of y_train = {y_train.shape}\")\nprint(f\"Shape of y_test = {y_test.shape}\")","d8145281":"lin_reg_mod = linear_model.LinearRegression() \nlin_reg_mod.fit(X_train, y_train)","a1f64093":"pred = lin_reg_mod.predict(X_test) #Make Prediction for test (unseen) data\ntest_set_rmse = (np.sqrt(mean_squared_error(y_test, pred))) #Create metrics for accuracy\ntest_set_r2 = r2_score(y_test, pred)\nprint(\"RMSE value:\",test_set_rmse)\nprint(\"R^2 value: \",test_set_r2)","0a441d82":"plt.figure()\nsns.regplot(y_test,pred)","9b0d07b0":"plt.scatter(X_test['Height'], y_test, color='red', label = 'Actual Weight')\nplt.scatter(X_test['Height'], lin_reg_mod.predict(X_test), color='green', label = 'Prdicted Weight')\nplt.xlabel('Height')\nplt.ylabel('Weight')\nplt.rcParams[\"figure.figsize\"] = (10,6) \nplt.title('Actual Vs Predicted Weight for Test Data')\nplt.legend()\nplt.show()","07166766":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.4,random_state = 2)\n\nprint(f\"Shape of X_train = {X_train.shape}\")\nprint(f\"Shape of X_test = {X_test.shape}\")\nprint(f\"Shape of y_train = {y_train.shape}\")\nprint(f\"Shape of y_test = {y_test.shape}\")","2f9b1fb2":"lin_reg_mod2 = linear_model.LinearRegression() \nlin_reg_mod2.fit(X_train, y_train)","43edc169":"pred = lin_reg_mod2.predict(X_test) #Make Prediction for test (unseen) data\ntest_set_rmse_2 = (np.sqrt(mean_squared_error(y_test, pred))) #Create metrics for accuracy\ntest_set_r2_2 = r2_score(y_test, pred)\nprint(\"RMSE value:\",test_set_rmse_2)\nprint(\"R^2 value: \",test_set_r2_2)","6c8d3794":"plt.figure()\nsns.regplot(y_test,pred)","fd95fcef":"plt.scatter(X_test['Height'], y_test, color='red', label = 'Actual Weight')\nplt.scatter(X_test['Height'], lin_reg_mod2.predict(X_test), color='green', label = 'Prdicted Weight')\nplt.xlabel('Height')\nplt.ylabel('Weight')\nplt.rcParams[\"figure.figsize\"] = (10,6) \nplt.title('Actual Vs Predicted Weight for Test Data')\nplt.legend()\nplt.show()","654d04ec":"y = df_dummy['Weight']\nx = df_dummy.iloc[:,1:13]\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(x)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled,y,test_size=0.4,random_state = 2)\n\nprint(f\"Shape of X_train = {X_train.shape}\")\nprint(f\"Shape of X_test = {X_test.shape}\")\nprint(f\"Shape of y_train = {y_train.shape}\")\nprint(f\"Shape of y_test = {y_test.shape}\")","9f6d6120":"def cost_function(X, Y, B):\n    m = len(Y)\n    J = np.sum((X.dot(B) - Y) ** 2)\/(2 * m)\n    return J","8cd377ef":"def batch_gradient_descent(X, Y, B, alpha, iterations):\n    cost_history = [0] * iterations\n    m = len(Y)\n\n    for iteration in range(iterations):\n        #print(iteration)\n        # Hypothesis Values\n        h = X.dot(B)\n        # Difference b\/w Hypothesis and Actual Y\n        loss = h - Y\n        # Gradient Calculation\n        gradient = X.T.dot(loss) \/ m\n        # Changing Values of B using Gradient\n        B = B - alpha * gradient\n        # New Cost Value\n        cost = cost_function(X, Y, B)\n        cost_history[iteration] = cost\n\n    return B, cost_history","4b2eacdc":"# Initial Coefficients\niter_ = 2000\nB1 = np.zeros(X_train.shape[1])\nalpha1 = 0.005\nnewB1, cost_history1 = batch_gradient_descent(X_train, y_train, B1, alpha1, iter_)\n\nB2 = np.zeros(X_train.shape[1])\nalpha2 = 0.001\nnewB2, cost_history2 = batch_gradient_descent(X_train, y_train, B2, alpha2, iter_)\n\nB3 = np.zeros(X_train.shape[1])\nalpha3 = 0.003\nnewB3, cost_history3 = batch_gradient_descent(X_train, y_train, B3, alpha3, iter_)\n\nB4 = np.zeros(X_train.shape[1])\nalpha4 = 0.01\nnewB4, cost_history4 = batch_gradient_descent(X_train, y_train, B4, alpha4, iter_)\n\nB5 = np.zeros(X_train.shape[1])\nalpha5 = 0.03\nnewB5, cost_history5 = batch_gradient_descent(X_train, y_train, B5, alpha5, iter_)","90e64e23":"plt.figure(figsize=(8,5))\nplt.plot(cost_history1,label = 'alpha = 0.005')\nplt.plot(cost_history2,label = 'alpha = 0.001')\nplt.plot(cost_history3,label = 'alpha = 0.03')\nplt.plot(cost_history4,label = 'alpha = 0.01')\nplt.plot(cost_history5,label = 'alpha = 0.03')\nplt.title('Convergence of Gradient Descent for different values of alpha')\nplt.xlabel('No. of iterations')\nplt.ylabel('Cost')\nplt.legend()\nplt.show()","9eac77ca":"newB5","2f30978d":"def Predict(X,theta):\n    y_pred = X.dot(theta)\n    return y_pred","17ae6b2c":"y_pred = Predict(X_test,newB5)\ny_pred[:5]","6a7d5d61":"plt.scatter(x=y_test,y=y_pred,alpha=0.5)\nplt.xlabel('y_test',size=12)\nplt.ylabel('y_pred',size=12)\nplt.title('Predicited Values vs Original Values (Test Set)',size=15)\nplt.show()","8ca727bf":"from sklearn import metrics\nmse = metrics.mean_squared_error(y_test,y_pred)\nmae = metrics.mean_absolute_error(y_test,y_pred)\nrmse = np.sqrt(metrics.mean_squared_error(y_test,y_pred))\nprint(f'Mean Squared Error: {mse}',f'Mean Absolute Error: {mae}',f'Root Mean Squared Error: {rmse}',sep='\\n')","7c1095b7":"models = pd.DataFrame({\n    'Model Features': ['Model with 80-20 split','Model with 60-40 split'],\n    'RMSE Score': [ test_set_rmse , test_set_rmse_2 ],\n    'R-squared Score': [ test_set_r2 , test_set_r2_2]})\nmodels.sort_values(by='RMSE Score', ascending=True)","2dfc5ce5":"**Observations:**\n- We can see that for ***alpha = 0.03***, Gradient Descent algorithm converges to minimum much faster than for any other value of alpha (taken).\n\n\n- We can see that Gradient Descent algorithm converged to minimum Cost somewhere before 50 iterations for *alpha = 0.03*.\n\n\n- Gradient Descent convergenced fastest for *alpha = 0.03 -> 0.001-> 0.01 -> 0.003*.\n\n\n- Thus, the best value of *alpha = 0.03* and corrosponding to it we will get best *theta* which is equal to '*theta4*.","b5c08c21":"# Multiple Linear Regression","8c886558":"## Width","2b033a14":"# Linear Regression","0dd4b11c":"## Length2","d4b15c5f":"From above *Evaluation Metrices*, we can notice that Root Mean Squared Error is low for our Multiple Regression Model and that is good thing for us.","7d8d3d2c":"There is no missing value in the data","76ecdf77":"This data set has 7 features and 159 rows i.e. details of 159 fishes.","87353362":"## Removing outliers","b0b959d6":"- We know, the higher the R-squared value, the more accurately the regression equation models your data\n- ALso, RMSE measures how accurately the model predicts the response, hence it's an important criterion for fit if the main purpose of the model is prediction.\n- This is a good regression model as the R square score is near 1.0, and the RMSE error is not very large.","beb4c72d":"Above is the value of theta corrosponding to alpha = 0.03","b2a4cc8f":"First, we have divided our data into two sets:\n\n**X** contains all independent variables\n\n**y** contains dependent variable weight","807b0ede":"### Importing Libraries","597acd58":"**Observations**\n- The dependent variable-'weight' has linear relationship with all other variables.\n- Width and Height are quite normally distributed\n- There seems to have presence of some outliers in the dataset.","ee0119ee":"# Univariate Analysis","c546418c":"Above table displays measures of central tendency like Mean, Median (50%) etc. We can see number of entries for each variable which is same as 159.\n\n**Observations**\n- Maximum value in weight is much higher than 75% of data points\n\n\n- We will study each of the feature seprately and see how data is distributed and if there are any outliers or not.","1b5d9caa":"### Skimming through the datasets","49e6cbb5":"## 40% - 60% split","d6a5d800":"### Columns\n- <B>Species:<\/B> Species name of fish\n- <B>Weight:<\/B> Weight of fish in gram\n- <B>Length1:<\/B> Vertical length in cm\n- <B>Length2:<\/B> Diagonal length in cm\n- <B>Length3:<\/B> Cross length in cm\n- <B>Height:<\/B> Height in cm\n- <B>Width:<\/B> Diagonal width in cm\nOur dependent variable is 'Weight'. Independent variables are 'species', different lengths, 'height' and 'width'.\n\nI will use independent variables to estimate dependent variable (weight of the fish) using regression.","20693250":"<B>Perch<\/B> species of fish has the has highest value count whereas for <B>Whitefish<\/B> the data might be insufficient","1fedf487":"## Finding variables which are useful for prediction","02e6c470":"## Length1","d26c0fa2":"The Big colorful picture above which is called *Heatmap* helps us to understand how features are correlated to each other.\n- Postive sign implies postive correlation between two features whereas Negative sign implies negative correlation between two features.\n\n\n- I am here interested to know which features have good correlation with our dependent variable <b>weight<\/b> and can help in having good predictions.\n\n\n- I observed that <b>all<\/b> features show some good correaltion with <b>weight<\/b>","0ea90088":" **CONCLUSION:**\n- In this experiment we observe that when the dataset is split in 80:20 ratio, the models give better results than when it is split in 60:40 ratio to train the model on the Machine Learning Algorithm","957da4f8":"## Converting categorical data","aebe6f3d":"### Importing Datasets","955e9e74":"## Height","f8e5e350":"## Weight","e538fda5":"## Splitting Dataset into Train and Test Set","3c4342f0":"## Length3","109b2d8c":"We can see that all features except species in the dataset are numeric type float. <Br>\n<B>Species<\/B> is only categorical variable in the dataframe.","3a68c20d":"Predict fucntion predicts the weight of the new unseen data using the regression coefficients i.e. theta.","1d72fcd3":"From above two figures we can see observe that:\n- Weight is normally distributed\n- It contains some extreme values which could be potential outliers","71f6fc2f":"This dataset is a record of 7 common different fish species in fish market sales. With this dataset, a predictive model can be performed using machine friendly data and estimate the weight of fish can be predicted.","054ef3bf":"## Removing outliers","3725faf7":"## 80% - 20% split"}}