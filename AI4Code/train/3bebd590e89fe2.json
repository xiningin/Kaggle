{"cell_type":{"814ae844":"code","2babb967":"code","4b0d31c3":"code","60dca243":"code","c514c0b1":"code","da6205ef":"code","59f23273":"code","4e786405":"code","61298533":"markdown","dceb5f9a":"markdown","99972013":"markdown","38d0a408":"markdown","e37ec2a1":"markdown","b1df3523":"markdown","285b9cd5":"markdown","9255fcab":"markdown"},"source":{"814ae844":"import pandas as pd\nimport sqlite3\nimport requests\nfrom bs4 import BeautifulSoup\nimport numpy as np\nfrom datetime import datetime\nwebsite = 'http:\/\/covid.gov.pk\/intl_travellers\/flight_info'\ncxn = sqlite3.connect(\"flights.db\")\n","2babb967":"page = requests.get(website)\nif page.status_code == 200:\n    soup = soup = BeautifulSoup(page.content, 'html.parser')\nelse:\n    print(\"Error Page status:\", page.status_code)","4b0d31c3":"scheduled_flights = soup.find_all('table')[1]\ncomplted_flights = soup.find_all('table')[2]\n","60dca243":"data = []\nfor idx,tr in enumerate(scheduled_flights.find('tbody').find_all('tr')):\n    if idx == 0:\n        continue\n    row = [td.text.replace('\\n','').replace('\\t','').replace('\\r','').replace(' ','') for td in tr.find_all('td')]\n    row.append('open' if 'open' in row[1] else 'close' if 'close' in row[1] else np.nan)\n    data.append(row)\n\ncols = ['sr_no', 'from_place', 'departure_airport', 'to_place', 'arrival_date', 'passengers', 'airline', 'status']\nschedule = pd.DataFrame(data, columns=cols)\n# date format setting for the database\nschedule['arrival_date'] = pd.to_datetime(schedule['arrival_date']).dt.date\nschedule.to_sql('schedule', cxn, if_exists='replace', index=False)\n","c514c0b1":"data = []\nfor idx,tr in enumerate(complted_flights.find('tbody').find_all('tr')):\n    if idx == 0:\n        continue\n    row = [td.text.replace('\\n','').replace('\\t','').replace('\\r','').replace(' ','') for td in tr.find_all('td')]\n    data.append(row)\n\ncols = ['sr_no', 'from_place', 'to_place', '_date', 'passengers', 'airline',]\ncompleted = pd.DataFrame(data, columns=cols)\n# date format setting for the database\ncompleted['_date'] = pd.to_datetime(completed['_date']).dt.date\ncompleted.to_sql('completed', cxn, if_exists='replace', index=False)\n\n","da6205ef":"pak_loc = 'https:\/\/en.wikipedia.org\/wiki\/List_of_cities_in_Pakistan'\npage = requests.get(pak_loc)\nif page.status_code == 200:\n    soup = soup = BeautifulSoup(page.content, 'html.parser')\nelse:\n    print(\"Error Page status:\", page.status_code)","59f23273":"data = []\nfor x in soup.find_all('table', {'class': 'wikitable'}):\n    provice = ''\n    _type = ''\n    if 'Balochistan' in x.find('th').text:\n        provice = 'Balochistan'\n        if 'municipalities' in x.find('th').text:\n            _type = 'municipalities'\n        elif 'districts' in x.find('th').text:\n            _type = 'districts'\n    elif 'Khyber' in x.find('th').text:\n        provice = 'Khyber'\n        if 'municipalities' in x.find('th').text:\n            _type = 'municipalities'\n        elif 'districts' in x.find('th').text:\n            _type = 'districts'\n    elif 'Punjab' in x.find('th').text:\n        provice = 'Punjab'\n        if 'municipalities' in x.find('th').text:\n            _type = 'municipalities'\n        elif 'districts' in x.find('th').text:\n            _type = 'districts'\n    elif 'Sindh' in x.find('th').text:\n        provice = 'Sindh'\n        if 'municipalities' in x.find('th').text:\n            _type = 'municipalities'\n        elif 'districts' in x.find('th').text:\n            _type = 'districts'\n    elif 'Kashmir' in x.find('th').text:\n        provice = 'Kashmir'\n        if 'municipalities' in x.find('th').text:\n            _type = 'municipalities'\n        elif 'districts' in x.find('th').text:\n            _type = 'districts'\n    elif 'Capital' in x.find('th').text:\n        provice = 'Punjab'\n        _type = 'municipalities'\n    elif 'Gilgit' in x.find('th').text:\n        provice = 'Gilgit'\n        if 'municipalities' in x.find('th').text:\n            _type = 'municipalities'\n        elif 'districts' in x.find('th').text:\n            _type = 'districts'\n    \n    population = x.find('tbody').find_all('tr')[3]\n    names = x.find('tbody').find_all('tr')[2]\n    for name, pop in zip(names.find_all('td'), population.find_all('td')):\n        data.append({\n            'provice': provice,\n            '_type': _type,\n            'city': name.text[:-6],\n            'population': pop.text.replace('\\n','').replace(',','')\n        })\n\npak = pd.DataFrame(data)\npak.to_sql('pakistan', cxn, if_exists='replace', index=False)","4e786405":"def sql_fetch(con):\n\n    cursorObj = con.cursor()\n\n    cursorObj.execute('SELECT name from sqlite_master where type= \"table\"')\n\n    return cursorObj.fetchall()\n\nfor x in sql_fetch(cxn):\n    filename = f\"{x[0]}.csv\"\n    df = pd.read_sql(f\"select * from {x[0]}\", cxn)\n    df.to_csv(filename, index=False)\n    del df\n    print(filename, 'created')","61298533":"# Scraping data from the Offical Covid Pakistan Goverment website.\nThe scraper won't work since kaggle won't let request make a call for obivious reasons. But I have uploaded a data set that I scrapped on the 15-06-2020. You can get the latest version of the data till this website is live. You get a Sqlite3 database with 2 table.\n\n1. schedule.\n2. completed.\n\n## schedule flights table\nThis table has all the scheduled flights depended on the latest info on the website.\n\n## completed flights table\nThis table has all the information of the flights complete till that. For me that is 15-06-2020. You can run this scrapper in the future to get the latest complete flights infromations.\n\n## If you want CSV\nI have a section at the end of of this notebook, which will provide you with the CSV if you don't want the power of Sqlite, or you don't know how to use the Sqlite.\n\n## Missing\nWe can motify the script to get the latest information without purging the current information. At the moment when you run the scrapper it replaces that data in the database with the latest data.","dceb5f9a":"## Getting Provinces and cities of pakistan","99972013":"## Completed Flights\nWe follow the same logic for completed flights table. We skip the same 1st row of the table as we did with the scheduled flights. But we have different number of columns for this table. Make sure you look at them.","38d0a408":"## Tables\nBases on the struct of the website we get the 2 table that we need. They are named accordingly.","e37ec2a1":"## Get CSVs","b1df3523":"## Getting the Province, cities and population info from Wikipedia for Pakistan\nI got this information because I want to see the flights based on provinces. This information is not avilabile in the orignal table.","285b9cd5":"## Checking the response from the website\nOn successful return we will have soup object which will provide us with the data from the website.","9255fcab":"## Scheduled Flights table loop\nWe skip the 1st row in the loop. This is the first complete in the data. It give use the following column names\n```\n['Sr #', 'From', 'Departure Airport', 'To', 'Arrival Date', 'Passengers', 'Airline']\n```\nPersonal if you ask me they are kind of ugly. That is why you see an order write of the column with the varabile name `cols`\n\n## Status column\nSince we have open and close status in the name of the from_place. I wanted to put it in it's own column. If you some need's or if we might want to fliter the status of the closed scheduled flights"}}