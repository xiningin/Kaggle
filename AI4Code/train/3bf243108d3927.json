{"cell_type":{"bcebcba7":"code","6020a813":"code","87c88d7d":"code","d0786681":"code","43b24242":"code","3be34d79":"code","b0b69b1d":"code","221f8fda":"code","d0d93dd4":"code","ec34643a":"code","3c39b985":"code","794a5aff":"code","bab683d7":"code","abd8d364":"code","7847a68f":"code","aabe3d91":"code","c8566f3a":"code","daffe199":"code","c372c81b":"code","b6499bb4":"code","7c3384d4":"code","5913ef36":"code","80909b99":"code","35671122":"code","f6c5ffe0":"code","1f1d7c3d":"code","0d8ca31e":"code","6895b4e3":"markdown","84deeda5":"markdown","3ed880e8":"markdown","eec21777":"markdown","45fd760a":"markdown","9b39d599":"markdown","f6d906bb":"markdown","1e6796c4":"markdown","24131d6e":"markdown","4e03f8f8":"markdown","3febbe99":"markdown","9a22d7e6":"markdown","d2e4910a":"markdown","af2b2c0c":"markdown","7ec70aff":"markdown","0b683c02":"markdown","770ef6a3":"markdown","e06f7dc3":"markdown","cd747720":"markdown","5aa37b33":"markdown","6b57f86c":"markdown","2f0a18b0":"markdown","5f2f7193":"markdown","46d6aa8e":"markdown","80af9b30":"markdown"},"source":{"bcebcba7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.offline as pyo\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\npyo.init_notebook_mode()\nfrom imblearn.over_sampling import SMOTE\nimport scikitplot as skplt\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder\nfrom sklearn.model_selection import train_test_split,cross_val_score\n\n\nfrom sklearn.linear_model import LinearRegression,LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nimport os\n\nfrom sklearn.metrics import classification_report,confusion_matrix,f1_score\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rc('figure',figsize=(17,13))\nsns.set_context('paper',font_scale=2)\n\ndef set_seed(seed=31415):\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed()","6020a813":"s_data = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ns_data.head(3)","87c88d7d":"plt.title('Missing Value Status',fontweight='bold')\nax = sns.heatmap(s_data.isna().sum().to_frame(),annot=True,fmt='d',cmap='vlag')\nax.set_xlabel('Amount Missing')\nplt.show()","d0786681":"DT_bmi_pipe = Pipeline( steps=[ \n                               ('scale',StandardScaler()),\n                               ('lr',DecisionTreeRegressor(random_state=42))\n                              ])\nX = s_data[['age','gender','bmi']].copy()\nX.gender = X.gender.replace({'Male':0,'Female':1,'Other':-1}).astype(np.uint8)\n\nMissing = X[X.bmi.isna()]\nX = X[~X.bmi.isna()]\nY = X.pop('bmi')\nDT_bmi_pipe.fit(X,Y)\npredicted_bmi = pd.Series(DT_bmi_pipe.predict(Missing[['age','gender']]),index=Missing.index)\ns_data.loc[Missing.index,'bmi'] = predicted_bmi","43b24242":"fig = ex.pie(s_data,names='stroke')\nfig.update_layout(title='<b>Proportion Of Stroke Samples<b>')\nfig.show()","3be34d79":"fig = make_subplots(\n    rows=2, cols=2,subplot_titles=('','<b>Distribution Of Female Ages<b>','<b>Distribution Of Male Ages<b>','Residuals'),\n    vertical_spacing=0.09,\n    specs=[[{\"type\": \"pie\",\"rowspan\": 2}       ,{\"type\": \"histogram\"}] ,\n           [None                               ,{\"type\": \"histogram\"}]            ,                                      \n          ]\n)\n\nfig.add_trace(\n    go.Pie(values=s_data.gender.value_counts().values,labels=['<b>Female<b>','<b>Male<b>','<b>Other<b>'],hole=0.3,pull=[0,0.08,0.3],marker_colors=['pink','lightblue','green'],textposition='inside'),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Histogram(\n        x=s_data.query('gender==\"Female\"').age,marker= dict(color='pink'),name='Female Ages'\n    ),\n    row=1, col=2\n)\n\n\nfig.add_trace(\n    go.Histogram(\n        x=s_data.query('gender==\"Male\"').age,marker= dict(color='lightblue'),name='Male Ages'\n    ),\n    row=2, col=2\n)\n\n\nfig.update_layout(\n    height=800,\n    showlegend=True,\n    title_text=\"<b>Age-Sex Infrence<b>\",\n)\n\nfig.show()","b0b69b1d":"plt.subplot(2,1,1)\nplt.title('Stroke Sample Distribution Based On Bmi And Glucose Level')\nsns.scatterplot(x=s_data['avg_glucose_level'],y=s_data['bmi'],hue=s_data['stroke'])\nplt.subplot(2,1,2)\nplt.title('Stroke Sample Distribution Based On Bmi And Age')\nsns.scatterplot(x=s_data['age'],y=s_data['bmi'],hue=s_data['stroke'])\nplt.tight_layout()\nplt.show()","221f8fda":"stroke_population = s_data.query('stroke ==1').copy()\n\nfig = make_subplots(\n    rows=2, cols=2,subplot_titles=('','<b>Distribution Of Female Ages<b>','<b>Distribution Of Male Ages<b>','Residuals'),\n    vertical_spacing=0.09,\n    specs=[[{\"type\": \"pie\",\"rowspan\": 2}       ,{\"type\": \"histogram\"}] ,\n           [None                               ,{\"type\": \"histogram\"}]            ,                                      \n          ]\n)\n\nfig.add_trace(\n    go.Pie(values=stroke_population.gender.value_counts().values,labels=['<b>Female<b>','<b>Male<b>','<b>Other<b>'],hole=0.3,pull=[0,0.08,0.3],marker_colors=['pink','lightblue','green'],textposition='inside'),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Histogram(\n        x=stroke_population.query('gender==\"Female\"').age,marker= dict(color='pink'),name='Female Ages'\n    ),\n    row=1, col=2\n)\n\n\nfig.add_trace(\n    go.Histogram(\n        x=stroke_population.query('gender==\"Male\"').age,marker= dict(color='lightblue'),name='Male Ages'\n    ),\n    row=2, col=2\n)\n\n\nfig.update_layout(\n    height=800,\n    showlegend=True,\n    title_text=\"<b>Age-Sex Infrence Of Stroke Positive Samples<b>\",\n)\n\nfig.show()","d0d93dd4":"stroke_population = s_data.query('stroke ==1').copy()\n\nfig = make_subplots(\n    rows=2, cols=2,subplot_titles=('<b>Proportion Of Different Work Types<b>','<b>Proportion Of Married Individuals<b>','<b>Proportion Of Residence Type<b>','Residuals'),\n    vertical_spacing=0.09,\n    specs=[[{\"type\": \"pie\",\"rowspan\": 2}       ,{\"type\": \"pie\"}] ,\n           [None                               ,{\"type\": \"pie\"}]            ,                                      \n          ]\n)\n\nfig.add_trace(\n    go.Pie(values=stroke_population.work_type.value_counts().values,labels=['<b>Private<b>','<b>Self-employed<b>','<b>Govt_job<b>','<b>children<b>','<b>Never_worked<b>'],hole=0.3,pull=[0,0.08,0.03,0.2],marker_colors=['orange','green','blue','brown','purple'],textposition='inside'),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Pie(values=stroke_population.ever_married.value_counts().values,labels=['<b>Yes<b>','<b>No<b>'],hole=0.3,pull=[0,0.08],marker_colors=['wheat','black'],textposition='inside'),\n    row=1, col=2\n)\nfig.add_trace(\n    go.Pie(values=stroke_population.Residence_type.value_counts().values,labels=['<b>Urban<b>','<b>Rural<b>'],hole=0.3,pull=[0,0.08],marker_colors=['pink','gray'],textposition='inside'),\n    row=2, col=2\n)\n\nfig.update_layout(\n    height=800,\n    showlegend=True,\n    title_text=\"<b>Different Categorical Attributes Of Stroke Samples<b>\",\n)\n\nfig.show()","ec34643a":"fig = ex.pie(stroke_population,names='smoking_status')\nfig.update_layout(title='<b>Proportion Of Different Smoking Categories Among Stroke Population<b>')\nfig.show()","3c39b985":"s_data.gender = s_data.gender.replace({'Male':0,'Female':1,'Other':-1}).astype(np.uint8)\ns_data.Residence_type = s_data.Residence_type.replace({'Rural':0,'Urban':1}).astype(np.uint8)\ns_data.work_type = s_data.work_type.replace({'Private':0,'Self-employed':1,'Govt_job':2,'children':-1,'Never_worked':-2}).astype(np.uint8)\n","794a5aff":"fig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Perason Correaltion',  'Spearman Correaltion'))\ncolorscale=     [[1.0              , \"rgb(165,0,38)\"],\n                [0.8888888888888888, \"rgb(215,48,39)\"],\n                [0.7777777777777778, \"rgb(244,109,67)\"],\n                [0.6666666666666666, \"rgb(253,174,97)\"],\n                [0.5555555555555556, \"rgb(254,224,144)\"],\n                [0.4444444444444444, \"rgb(224,243,248)\"],\n                [0.3333333333333333, \"rgb(171,217,233)\"],\n                [0.2222222222222222, \"rgb(116,173,209)\"],\n                [0.1111111111111111, \"rgb(69,117,180)\"],\n                [0.0               , \"rgb(49,54,149)\"]]\n\ns_val =s_data.corr('pearson')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,name='pearson',showscale=False,xgap=1,ygap=1,colorscale=colorscale),\n    row=1, col=1\n)\n\n\ns_val =s_data.corr('spearman')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,xgap=1,ygap=1,colorscale=colorscale),\n    row=2, col=1\n)\n\nfig.update_layout(height=700, width=900, title_text=\"Locations That Contribute The Most To Our Cut-Offs\")\nfig.show()","bab683d7":"import pymc3 as pm\nimport theano.tensor as tt\nwith pm.Model():\n    p_s = pm.Uniform('p_stroke',0,1)\n    \n    \n    stroke = pm.Bernoulli('pstroke',p_s,observed=s_data.stroke)\n    \n    \n    step = pm.Metropolis()\n    trace = pm.sample(50000,step=step)\n    burned_trace = trace[40000:]","abd8d364":"beta_samples = burned_trace['p_stroke']\n\nplt.subplot(211)\nplt.title(r\"Posterior distributions of the Probability Of A Stroke After Observing Our Data\")\n_hist = plt.hist(beta_samples, histtype='stepfilled', bins=35, alpha=0.85,\n         label=r\"posterior of $P_{Stroke}$\", color=\"#7A68A6\")\nplt.vlines(beta_samples.mean(),0,1.1*np.max(_hist[0]),label='Posterior Mean',linestyle='-.',linewidth=3,color='red')\nplt.vlines(beta_samples.mean()-2*beta_samples.std(),0,1.1*np.max(_hist[0]),label='95% Confidence Bound',linestyle='-.',linewidth=3,color='tab:green')\nplt.vlines(beta_samples.mean()+2*beta_samples.std(),0,1.1*np.max(_hist[0]),linestyle='-.',linewidth=3,color='tab:green')\n\nplt.legend()\nplt.show()\n","7847a68f":"with pm.Model() as t:\n    p_s = pm.Uniform('p_stroke',0.04,0.06)\n    p_ns = 1-p_s\n    p = tt.stack([p_s,p_ns])\n    assigment = pm.Categorical('assigment',p,shape=s_data.shape[0],testval=np.random.randint(0,1,s_data.shape[0]))\n    \n    sd = pm.Uniform('sd',0,100,shape=2)\n    center = pm.Normal('center',mu=np.array([40,20]),sd=np.array([30,30]),shape=2)\n    \n    center_i= pm.Deterministic('center_i',center[assigment])\n    sd_i= pm.Deterministic('sd_i',sd[assigment])\n    \n    observation = pm.Normal('obs',mu=center_i,sd=sd_i,observed=s_data.age)\n    \n    \n    step = pm.Metropolis(vars=[p,sd,center])\n    step_c = pm.ElemwiseCategorical(vars=[assigment])\n    trace = pm.sample(6000,step=[step,step_c])","aabe3d91":"center_trace = trace['center'][8000:]\nstd_trace = trace[\"sd\"][8000:]\ncolors = [\"#348ABD\", \"#A60628\"] if center_trace[-1, 0] > center_trace[-1, 1] \\\n    else [\"#A60628\", \"#348ABD\"]\n\n_i = [1, 2, 3, 4]\ntitles = [\"Posterior of Stroke Prone Ages $\\mu$\",\"Posterior of Stroke Prone Ages $\\sigma$\",\"Posterior of Stroke Resistant Ages $\\mu$\",\"Posterior of Stroke Resistant Ages $\\sigma$\"]\nfor i in range(2):\n    plt.subplot(2, 2, _i[2 * i])\n    plt.title(titles[2 * i])\n    plt.hist(center_trace[:, i], color=colors[i], bins=10,\n             histtype=\"stepfilled\")\n\n    plt.subplot(2, 2, _i[2 * i + 1])\n    plt.title(titles[2 * i + 1])\n    plt.hist(std_trace[:, i], color=colors[i], bins=25,\n             histtype=\"stepfilled\")\n    # plt.autoscale(tight=True)\n\nplt.tight_layout()","c8566f3a":"oversample = SMOTE()\neval_df = s_data[['gender','age','hypertension','heart_disease','work_type','avg_glucose_level','bmi','stroke']].sample(int(s_data.shape[0]*0.2),random_state=42)\ntrain_df = s_data.drop(index=eval_df.index)\n\nX_eval,y_eval = eval_df[['gender','age','hypertension','heart_disease','work_type','avg_glucose_level','bmi']], eval_df['stroke']\nX,y = train_df[['gender','age','hypertension','heart_disease','work_type','avg_glucose_level','bmi']], train_df['stroke']\n\n\nX, y = oversample.fit_resample(X,y)\nusampled_df = X.assign(Stroke = y)\n\nX_eval,y_eval = oversample.fit_resample(X_eval,y_eval)\nusampled_eval_df = X_eval.assign(Stroke = y_eval)","daffe199":"fig = ex.pie(usampled_df,names='Stroke')\nfig.update_layout(title='<b>Stroke Proportion After SMOTE Upsampling<b>')\nfig.show()","c372c81b":"DT_PIPE = Pipeline(steps = [('scale',StandardScaler()),('DT',DecisionTreeClassifier(random_state=42))])\nRF_PIPE = Pipeline(steps = [('scale',StandardScaler()),('DT',RandomForestClassifier(random_state=42))])\nSVM_PIPE = Pipeline(steps = [('scale',StandardScaler()),('DT',SVC(random_state=42))])\nLR_PIPE = Pipeline(steps = [('scale',StandardScaler()),('DT',LogisticRegression(random_state=42))])\n\nX = usampled_df.iloc[:,:-1]\nY = usampled_df.iloc[:,-1]\n\nX_EVAL = usampled_eval_df.iloc[:,:-1]\nY_EVAL = usampled_eval_df.iloc[:,-1]\n\nholdout_x = X.sample(100)\nholdout_y = Y.loc[X.index]\n\nX = X.drop(index=holdout_x.index)\nY = Y.drop(index=holdout_x.index)","b6499bb4":"DT_CROSS_VAL = cross_val_score(DT_PIPE,X,Y,cv=10,scoring='f1')\nRF_CROSS_VAL = cross_val_score(RF_PIPE,X,Y,cv=10,scoring='f1')\nSVM_CROSS_VAL = cross_val_score(SVM_PIPE,X,Y,cv=10,scoring='f1')\nLR_CROSS_VAL = cross_val_score(LR_PIPE,X,Y,cv=10,scoring='f1')","7c3384d4":"fig = make_subplots(rows=4, cols=1,shared_xaxes=True,subplot_titles=('Decision Tree Cross Val Scores',\n                                                                     'RandomForest Cross Val Scores',\n                                                                    'SVM Cross Val Scores','Logistic Regression Cross Val Scores'))\n\nfig.add_trace(\n    go.Scatter(x=list(range(0,len(DT_CROSS_VAL))),y=DT_CROSS_VAL,name='Decision Tree'),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Scatter(x=list(range(0,len(DT_CROSS_VAL))),y=RF_CROSS_VAL,name='RandomForest'),\n    row=2, col=1\n)\nfig.add_trace(\n    go.Scatter(x=list(range(0,len(DT_CROSS_VAL))),y=SVM_CROSS_VAL,name='SVM'),\n    row=3, col=1\n)\nfig.add_trace(\n    go.Scatter(x=list(range(0,len(DT_CROSS_VAL))),y=LR_CROSS_VAL,name='Logistic Regression'),\n    row=4, col=1\n)\n\nfig.update_layout(height=700, width=900, title_text=\"Different Model 5 Fold Cross Validation\")\nfig.update_yaxes(title_text=\"F1 Score\")\nfig.update_xaxes(title_text=\"Fold #\")\n\nfig.show()","5913ef36":"DT_PIPE.fit(X,Y)\nRF_PIPE.fit(X,Y)\nSVM_PIPE.fit(X,Y)\nLR_PIPE.fit(X,Y)\n\nX = s_data.loc[:,X.columns]\nY = s_data.loc[:,'stroke']\n\nDT_PRED   = DT_PIPE.predict(X_EVAL)\nRF_PRED   =RF_PIPE.predict(X_EVAL)\nSVM_PRED  = SVM_PIPE.predict(X_EVAL)\nLR_PRED   = LR_PIPE.predict(X_EVAL)\n\nDT_CM  = confusion_matrix(Y_EVAL,DT_PRED )\nRF_CM  = confusion_matrix(Y_EVAL,RF_PRED )\nSVM_CM = confusion_matrix(Y_EVAL,SVM_PRED)\nLR_CM  = confusion_matrix(Y_EVAL,LR_PRED )\n\nDT_F1  = f1_score(Y_EVAL,DT_PRED )\nRF_F1  = f1_score(Y_EVAL,RF_PRED )\nSVM_F1 = f1_score(Y_EVAL,SVM_PRED)\nLR_F1  = f1_score(Y_EVAL,LR_PRED )","80909b99":"fig = go.Figure()\nfig.add_trace(go.Bar(x=['Decision Tree','Random Forest','SVM','Logistic Regression'],y=[DT_F1,RF_F1,SVM_F1,LR_F1]))\nfig.update_layout(title='F1 Score Of Our Model On Original Data',xaxis_title='Model',yaxis_title='F1 Score')\nfig.show()","35671122":"z=RF_CM\nfig = ff.create_annotated_heatmap(z, x=['Not Stroke','Stroke'], y=['Predicted Not Stroke','Predicted Stroke'], colorscale='Fall',xgap=3,ygap=3)\nfig['data'][0]['showscale'] = True\nfig.update_layout(title='Prediction On Original Data With Random Forest Model Confusion Matrix')\nfig.show()","f6c5ffe0":"fig = go.Figure()\nfig.add_trace(go.Bar(x=X.columns,y=RF_PIPE['DT'].feature_importances_))\nfig.update_layout(title='The Importance Of The Original Attributes On Our Prediction',xaxis_title='Model',yaxis_title='F1 Score')\nfig.show()","1f1d7c3d":"import pymc3 as pm\n\nstroke_population = s_data.query('stroke==1')\nregular_population = s_data.query('stroke==0')\n\nwith pm.Model() as model:\n    p_A = pm.Uniform(\"p_A\", 0, 1)\n    p_B = pm.Uniform(\"p_B\", 0, 1)\n    \n    delta = pm.Deterministic(\"delta\", p_A - p_B)\n\n    \n    obs_A = pm.Bernoulli(\"obs_A\", p_A, observed=stroke_population.heart_disease)\n    obs_B = pm.Bernoulli(\"obs_B\", p_B, observed=regular_population.heart_disease)\n\n    step = pm.Metropolis()\n    trace = pm.sample(20000, step=step)\n    burned_trace=trace[1000:]","0d8ca31e":"p_A_samples = burned_trace[\"p_A\"]\np_B_samples = burned_trace[\"p_B\"]\ndelta_samples = burned_trace[\"delta\"]\n#histogram of posteriors\n\nplt.xlim(0, .2)\nax = plt.subplot(311)\nplt.hist(p_A_samples, histtype='stepfilled', bins=25, alpha=0.85,\n         label=\"posterior of $p_A$\", color=\"#A60628\")\nplt.legend(loc=\"upper right\")\nplt.title(\"Posterior distributions of $p_A$, $p_B$, and delta unknowns\")\n\nax = plt.subplot(312)\n\nplt.xlim(0, .3)\nplt.hist(p_B_samples, histtype='stepfilled', bins=25, alpha=0.85,\n         label=\"posterior of $p_B$\", color=\"#467821\")\nplt.legend(loc=\"upper right\")\n\nax = plt.subplot(313)\nplt.hist(delta_samples, histtype='stepfilled', bins=30, alpha=0.85,\n         label=\"posterior of delta\", color=\"#7A68A6\")\n\nplt.vlines(0, 0, 60, color=\"black\", alpha=0.2)\nplt.legend(loc=\"upper right\");","6895b4e3":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>It seems that we have two underlying populations based on the first scatter plot where the x-axis is the glucose level.<br>What it visible straight away is the fact the in both scatterplots the individuals who had a stroke are located in the BMI value region under 60 and in high glucose levels as well as old age. <\/span><\/p>","84deeda5":"<a id=\"1\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Conclusions<\/h1>\n","3ed880e8":"<a id=\"1\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Analysis of Stroke Positve Population<\/h1>\n","eec21777":"<a id=\"1\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Probabilistic Infrence<\/h1>\n","45fd760a":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Next, we will make the following assumptions: The age of the population prone to strokes is normally distributed, meaning we can model that distribution of stroke-prone individuals using a Normal distribution with some unknown mu and sigma.<\/span><\/p>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">The same logic is applied to the population of those who are less prone to strokes; we will refer to them as the &quot;resistant&quot; population.<\/span><\/span><\/p>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">Our goal in the following cells will be to model these Normal distributions, which we will denote by $$N_{prone} \\sim (\\mu_{prone},\\sigma_{prone})$$,$$N_{resistant} \\sim (\\mu_{resistant},\\sigma_{resistant})$$.<\/span><\/span><\/p>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">We will create a prior landscape where all the possible parameters for our Normal distribution may reside and produce a posterior landscape using the observed data we have; of course, the probability of belonging to the stroke-prone population, as we saw in the figure above, is between 0.04 and 0.06 with 99% confidence interval.<\/span><\/span><\/p>\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>We will use Monte Carlo Markov Chain to traverse our posterior landscape and gather samples from it which we will examine to decide what parameters suffices are Normal models and how confident we are in them.<\/span><\/p>\n<p><br><\/p>","9b39d599":"<a id=\"1\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Model Evaluation On Original Data (Before Upsampling)<\/h1>\n","f6d906bb":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>It seems from our cross-validation test that the random forest and decision tree models did the best on the average of overfitting the sample data, to test that those models indeed learned to predict stroke-prone individuals we will test those models once again on the original data before we performed upsampling.<\/span><\/p>","1e6796c4":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>As for numeric correlation, it seems that there is no clear numeric correlation between the stroke attribute and other numeric attributes in our dataset.<\/span><\/p>","24131d6e":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>A very interesting inference to be made here is what attributes had the most important in predicting stroke-prone individuals and we see that the BMI, age and glucose level of the individuals are by far the most significant predictors for a stroke-prone individual.<\/span><\/p>","4e03f8f8":"<a id=\"1\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Exploratory Data Analysis<\/h1>\n","3febbe99":"<a id=\"1\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Analysis of Entire Sample Space<\/h1>\n","9a22d7e6":"<a id=\"1\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Libraires And Utilities<\/h1>","d2e4910a":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Now that we have an upsampled version of our dataset where both sample outcomes are uniformly distributed, the models we will try to train will have a better chance of picking up on the details that define stroke individuals from those who are negative for a stroke.<\/span><\/p>\n<p><br><\/p>","af2b2c0c":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>It seems that the marriage status and the residence type are distributed in a way that doesn&apos;t tell us anything confounding about stroke-positive individuals, the residence type is close to a uniform distribution and the marriage status is almost completely dominated by one of the values.<\/span><\/p>","7ec70aff":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>After constructing a sampling space of a Bernoulli model and sampling from a uniform distribution of probability values for each group, we examine the results and see that we can say with great confidence that the probability of an individual from the stroke-negative group is much likely to have heart disease wheres the stroke-positive group. However, with less certainty, we can say that the probability for a stroke-positive individual with heart disease is around 0.15-0.21.If it seems less trivial, the delta distribution seen in the third plot is a distribution of the difference between the groups' two probabilities. If there were no difference, we would see a very thin distribution around zero in contrast to what we see that the distribution is far from zero, which indicates there is a definite difference in which we have lower confidence.\n<\/span><\/p>","0b683c02":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>We are clearly dealing with an imbalanced dataset, and later in our pipeline, we will have to deal with this problem potentially with upscaling; that way, we give our models a better chance of learning the small details which define potential stroke individuals.<\/span><\/p>\n<p><br><\/p>","770ef6a3":"<a id=\"1\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Data Preprocessing<\/h1>\n","e06f7dc3":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Next, we will try to construct a belief that there is a difference between the two groups in our dataset, i.e., the stroke positive and the negative stroke individuals.\nWe will define two probabilities, $P(A)$ = the probability for heart disease in a  stroke positive individual, and $P(B)$ = the probability a negative stroke individual has heat disease.\nHaving a more concrete belief that there are differences between the two groups which did not show up on our decision tree model will confirm that there may be more factors that differentiate between the groups but maybe less indicative of a stroke.<\/span><\/p>","cd747720":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>There are slightly more females in our sample data than males and from a visual perspective, the ages seem to be normally distributed but with a high variance measure due to the visible fat tail.<\/span><\/p>","5aa37b33":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>When looking at the inner distribution of our different attributes among stroke positive samples, we see a couple of interesting points to be noted; the first is that females, although appearing more than males in our dataset, also surpass the males in the stroke sample space, the second point to be noted is that males are more prone to strokes in their early 50\/60 where the median of the women stroke age is around 75-79.<\/span><\/p>","6b57f86c":"<a id=\"1\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Model Selection<\/h1>\n","2f0a18b0":"<a id=\"1\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Upsampling The Dataset<\/h1>\n","5f2f7193":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Initially, the dataset had 201 samples with absent BMI value; rather than imputing it naively with the mean or the median, we used a simple decision tree model which based on the age and gender of all other samples gave us a fair prediction for the missing values.<\/span><\/p>\n<p><br><\/p>","46d6aa8e":"<ol>\n    <li><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>It seems that there two underlying groups based on the joint distribution of BMI and glucose level attributes, in a further analysis it may be of interest to understand what distinguishes those two groups from one another.<\/span><\/li>\n    <li><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">We saw that a random forest model trained on an upsampled version of the original dataset yielded satisfiable results as for predicting stroke-prone individuals, further analysis should be conducted to ensure that such results are not due to chance or any bais raising during the modeling stage.<\/span><\/span><\/li>\n    <li><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">We saw that age, BMI, and glucose level are the most important features when it comes to predicting stroke-prone individuals, based on the current dataset.<\/span><\/span><\/li>\n    <li><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">We observed that women are prone to stroke on average at a much older age (74-79) in comparison to males which experiences strokes on average as soon as their mid 50&apos;s and 60, ages which are much rarer for women to have strokes.<\/span><\/span><\/li>\n    <li><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">Surprisingly when looking at the smoking status of our stroke-positive samples we saw that the majority of the samples have never smoked! and the smallest proportion of the store postive samples are smorkes, indeed a peculiar situation, one would think that smoking will increase the probability of an individual to experience a stroke.<\/span><\/span><\/li>\n<\/ol>\n<p><br><\/p>","80af9b30":"<a id=\"1\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Model Evaluation<\/h1>\n"}}