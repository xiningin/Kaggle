{"cell_type":{"7627e21e":"code","eb0829bb":"code","f780a84a":"code","f9e375c5":"code","fa3552cc":"code","44873145":"code","207f421f":"code","df0fcab8":"code","eb960433":"code","3933336d":"code","479c780c":"code","1db091b8":"code","c927f6b5":"code","bd2d398d":"code","792319fb":"code","d836f9d1":"code","60b5c64d":"code","0bd793d9":"markdown","743913a5":"markdown","536b1668":"markdown","c0c47a7c":"markdown","395e6ced":"markdown","4d66454e":"markdown"},"source":{"7627e21e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n!pip install category_encoders\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # visualization library\nimport seaborn as sns # visualization library\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","eb0829bb":"data = pd.read_csv(\"..\/input\/advertsuccess\/Train.csv\", index_col='id')","f780a84a":"data.info()","f9e375c5":"data.netgain.value_counts().plot(kind='barh')\nplt.title('Net gain ( \\'True\\' or \\'False\\' )', fontsize=20)\nplt.show()","fa3552cc":"data.head()","44873145":"plt.figure(figsize=(15,4))\nplt.title('Relationship Status',fontsize=20)\nsns.countplot(data.realtionship_status)\nplt.show()","207f421f":"plt.figure(figsize=(15,4))\nplt.title('Industry',fontsize=20)\nsns.countplot(data.industry)\nplt.show()","df0fcab8":"plt.figure(figsize=(15,4))\nplt.title('Genre',fontsize=20)\nsns.countplot(data.genre)\nplt.show()","eb960433":"plt.figure(figsize=(15,4))\nplt.hist(data['average_runtime(minutes_per_week)'],bins=25)\nplt.title('average_runtime(minutes_per_week)',fontsize=20)\nplt.show()","3933336d":"plt.figure(figsize=(15,4))\nplt.title('Gender',fontsize=20)\nsns.countplot(data.targeted_sex)\nplt.show()","479c780c":"plt.figure(figsize=(15,4))\nplt.title('Air time',fontsize=20)\nsns.countplot(data.airtime)\nplt.show()","1db091b8":"plt.figure(figsize=(5,10))\nplt.title('Air Location',fontsize=20)\ndata.airlocation.value_counts().plot(kind='barh')\nplt.show()","c927f6b5":"plt.figure(figsize=(15,4))\nplt.title('Expensive',fontsize=20)\nsns.countplot(data.expensive)\nplt.show()","bd2d398d":"plt.figure(figsize=(15,4))\nplt.title('Money back Guarantee - Yes or No',fontsize=20)\nsns.countplot(data.money_back_guarantee)\nplt.show()","792319fb":"#Preprocessing\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom category_encoders import BinaryEncoder\nfrom sklearn.metrics import precision_score\n#expensive is an ordinal categorical variable\nexp_dict = {'Low':0,'Medium':1,'High':2}\ndata['expensive'] = data.expensive.map(exp_dict)\n\n#Binary Categorical\nBin_columns = ['targeted_sex','money_back_guarantee']\nBin_Encoder = BinaryEncoder()\n\n#Multi class nominal categorical\ncat_columns = ['realtionship_status', 'industry', 'genre', 'airtime', 'airlocation' ]\nOHE = OneHotEncoder(sparse=False)\n\nencoding = ColumnTransformer(transformers=[('cat',OHE,cat_columns),\n                                               ('bin',Bin_Encoder,Bin_columns)])\n\nclf = Pipeline(steps=[('encoder',encoding),('Std',StandardScaler()),('LR',LogisticRegression())])\n\ny, X = data['netgain'],data.drop('netgain',axis=1)\n\nX_train, X_test, y_train,  y_test = train_test_split(X,y)\n\nclf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)\n\nprecision_score(y_pred,y_test)","d836f9d1":"# Applying SMOTE\nfrom imblearn.over_sampling import SMOTE \n\n#SMOTE have to be applied for training set only\n\npreprocessor = Pipeline(steps=[('encoder',encoding),('Std',StandardScaler())])\n\nX_train = preprocessor.fit_transform(X_train)\nX_test = preprocessor.transform(X_test)\n\nprint(\"Shape of train dataset before applying SMOTE:\",X_train.shape)\n\nX_train, y_train = SMOTE().fit_resample(X_train,y_train)\n\nprint(\"Shape of train dataset after applying SMOTE:\",X_train.shape)","60b5c64d":"lr2 = LogisticRegression()\nlr2.fit(X_train,y_train)\ny_pred2 = lr2.predict(X_test)\nprecision_score(y_pred2,y_test)","0bd793d9":"# Notebook Description:\n   In this Notebook we will analyze the Ad success with and without oversampling(SMOTE). Then we will see the influence of oversampling.\n  1. Imabalanced dataset and SMOTE\n  2. Preprocessing\n  3. Logistic Regression without SMOTE\n  4. Logistic Regression with SMOTE\n    \n# How to measure an imbalanced dataset?\n\n    When the dependent variable is categorical and the classes are not in equal proportion in the data, then such datasets can be termed as imbalanced dataset. Mostly in case of anomaly detection   the dependent variables will be imbalanced.\n    \n    How to select the metrics? Accuracy is not a right metrics, because if we mark every results to the majority  class (No Succes in case of ad's sucess) then we will have accuracy, but the results are not as expected. So in such cases it is better to know how well the minority class (Success) was evaluated. So we can determine the precision.\n    \n   precision = TP \/ (TP + FP) \n    \n# SMOTE\n\n    We have a metrics to measure, still what if the minority class is  very less (very less success). In that case we want the model to learn more about the minority class. (Like you concentrate on the topic you are not strong in your school exam). This process is called over sampling. Sythetic Minority Oversampling Technique (SMOTE). \n    \n   How SMOTE works? \n             \n    We need to be aware how much samples we need for oversampling.\n       * Choose the Minority Class\n       * Calculate the no of samples from minority class required.\n       * For each sample to be synthesized,\n          - Randomly choose a minor class instance in the feature space.(say pt 'a')\n          - Select k_nearest neighbours to the instance.\n          - Choose one of the k nearest neighbours at random ('b').\n          - Join 'a' & 'b' in the feature space by a line.\n          - select a random point on the line. This will be the sample sythesized.\n       \n   For more refer: https:\/\/www.youtube.com\/watch?v=U3X98xZ4_no\n          \n   How do we implement SMOTE?\n       \n       SMOTE can be applied to a dataset in python using imbalanced_learn library.\n   ","743913a5":"*Here the Precision of our classification has improved by nearly 25%*\n\n\n* Feedback, upvotes are most welcomed! *","536b1668":"Now we have 10000 more samples! These are synthesized from the minority class.\n\nLets make logistic regression for the resampled data.","c0c47a7c":"REFERENCE:\n\n  https:\/\/scikit-learn.org\/stable\/auto_examples\/compose\/plot_column_transformer_mixed_types.html\n  https:\/\/machinelearningmastery.com\/smote-oversampling-for-imbalanced-classification\/","395e6ced":"# Preprocessing\n     \n   There are 11 columns of which 9 are categorical. You expand the cells to find visualization of distribution of columns. There are 3 binary columns which will be binary encoded and other categoricals will be One Hot Encoded. Then the dataset will be standardized. In second case we will apply SMOTE before standardizing.\n   \n   1) Encoding -> Standardization -> Training -> Precision\n   \n   2) Encoding -> **SMOTE** -> Standardization -> Training -> Precision","4d66454e":"    We see that total data we have 26,048 entries, of which 6195 have a net gain. This is 23% and so the dataset is imbalanced. Here we can construct a model and evaluate using prceision. We will  do preprocessing and construct a simple logistic regression.\n    \n    Then we can apply SMOTE to our data and analyze our data in the similar fashion."}}