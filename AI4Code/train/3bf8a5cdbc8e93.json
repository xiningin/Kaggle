{"cell_type":{"dfac493d":"code","29dbcbe2":"code","1f2f0b7c":"code","b802c10e":"code","fe736e0d":"code","d4e1111f":"code","455cd19d":"code","a9fba597":"code","f919d7e1":"code","29950156":"code","e337f476":"code","9667ba38":"code","d126ef4b":"code","b858d6e7":"code","75c32c91":"code","1d736295":"code","21aead01":"code","2984852d":"code","faae0181":"markdown","b4bd850a":"markdown","d1a7fc9c":"markdown","b8891d92":"markdown","0aca7d16":"markdown","f4019eb8":"markdown","a2603f15":"markdown"},"source":{"dfac493d":"import pandas as pd\nimport torch\nimport matplotlib.pyplot as plt","29dbcbe2":"df = pd.read_csv(r'\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')","1f2f0b7c":"df.head()","b802c10e":"X = df[df.columns[:-1]]\ny = df['Outcome']\nprint('*********X*********')\nprint(X)\nprint('*********y*********')\nprint(y)","fe736e0d":"X = torch.tensor(X.values)\ny = torch.tensor(y.values)\nprint('*********X*********')\nprint(X)\nprint('*********y*********')\nprint(y)","d4e1111f":"def log_loss(y, y_pred): ##log loss error (binary cross entropy)\n  return -torch.sum((y*torch.log(y_pred) + (1-y)*torch.log(1-y_pred)))\/y.shape[0]\n\ndef sigmoid(linear):\n    return 1\/(1+torch.exp(-linear))","455cd19d":"X_mean = torch.mean(X, axis=0)\nX_std = torch.std(X, axis=0)\nX = (X-X_mean)\/X_std\nX = torch.cat((X, torch.ones(X.shape[0], dtype=float).reshape(-1,1)), dim=1)","a9fba597":"epoch_loss = []\n\nweights = torch.zeros(9, dtype=float, requires_grad=True)\nlearning_rate = 1e-1 \nn = X.shape[0]\n\nfor epoch in range(1000+1): \n    linear = weights.reshape(1,-1)@X.T ##one equation for all\n    y_pred =  sigmoid(linear) ##logistic\n    loss = log_loss(y, y_pred)\n    epoch_loss.append(loss.item())\n\n    ###backpropagation###\n    loss.backward()\n\n    ###step###\n    with torch.no_grad():\n        weights -= learning_rate * weights.grad\n\n    weights.grad.zero_()\n\n    if(epoch % 100 == 0):\n        acc = torch.sum(((y_pred>=0.5)+0 == y)+0).item()\/y.shape[0]\n        print('epoch: {0} - loss: {1:.5f}; acc: {2:.3f}'.format(epoch, epoch_loss[-1], acc))","f919d7e1":"plt.plot(epoch_loss)","29950156":"def accuracy(y_pred, y):\n    return torch.sum((((y_pred>=0.5)+0).reshape(1,-1)==y)+0).item()\/y.shape[0]","e337f476":"hidden_nodes = 16\ntorch.random.manual_seed(1234)\nhidden_weights = torch.rand((hidden_nodes, 9), dtype=float, requires_grad=True)\ntorch.random.manual_seed(1234)\noutput_weights = torch.rand((1, hidden_nodes), dtype=float, requires_grad=True)","9667ba38":"epoch_loss = []\n\nlearning_rate = 1e-1\nn = X.shape[0]\n\nfor epoch in range(100000+1): \n    ##hidden layer##\n    hidden_linear = hidden_weights@X.T\n    hidden_output = sigmoid(hidden_linear)\n    ################\n    output_linear = output_weights@hidden_output\n    y_pred = sigmoid(output_linear)\n    loss = log_loss(y, y_pred)\n    epoch_loss.append(loss.item())\n\n    ###backpropagation###\n    loss.backward()\n\n    ###step###\n    with torch.no_grad():\n        hidden_weights -= learning_rate * hidden_weights.grad\n        output_weights -= learning_rate * output_weights.grad\n\n    hidden_weights.grad.zero_()\n    output_weights.grad.zero_()\n\n    if(epoch % 10000 == 0):\n        print('epoch: {0} - loss: {1:.5f}; acc: {2:.3f}'.format(epoch, epoch_loss[-1], accuracy(y_pred, y)))","d126ef4b":"plt.plot(epoch_loss)","b858d6e7":"import torch.nn as nn","75c32c91":"class Model(nn.Module):\n    \n    def __init__(self, nodes):\n        super().__init__()\n        self.hidden_linear = nn.Linear(8, nodes)\n        self.output_linear = nn.Linear(nodes, 1)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, X):\n        hidden_output = self.sigmoid(self.hidden_linear(X))\n        out = self.sigmoid(self.output_linear(hidden_output))\n        return out","1d736295":"hidden_nodes = 16\nmodel = Model(hidden_nodes)\nlog_loss = torch.nn.BCELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr = 1e-1)","21aead01":"model.train()\nepoch_loss = []\n\nfor epoch in range(100000+1):\n    y_pred = model(X[:,:-1].float())\n    loss = log_loss(y_pred, y.reshape(-1,1).float())\n    epoch_loss.append(loss)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if(epoch % 10000 == 0):\n        print('epoch: {0} - loss: {1:.5f}; acc: {2:.3f}'.format(epoch, epoch_loss[-1], accuracy(y_pred, y)))","2984852d":"plt.plot(epoch_loss)","faae0181":"![image.png](attachment:image.png)","b4bd850a":"<h1>Vectorization<\/h1>","d1a7fc9c":"<h1>Normalizing the Data<\/h1>","b8891d92":"<h1>Diabetes Dataset<\/h1>","0aca7d16":"<h1>Quick Data Cleaning<\/h1>","f4019eb8":"<h1>Using torch.nn<\/h1>","a2603f15":"<h1>Neural Network Architecture<\/h1>"}}