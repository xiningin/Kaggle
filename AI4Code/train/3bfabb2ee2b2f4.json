{"cell_type":{"02165214":"code","aafd31e5":"code","8d79beba":"code","363c0760":"code","f47110e7":"code","0583b1c3":"code","70941a64":"code","32f7cf04":"code","2b031f1e":"code","d7de1cc7":"code","ef392782":"code","e9c1dc97":"code","5f07582c":"code","18990ddb":"code","58cf1e3c":"code","1123116f":"code","e8b6689f":"code","4bf5e056":"code","1a99e919":"code","de0ae2d3":"code","b178730d":"code","4b771138":"code","ffe58abb":"code","aecdc93b":"code","725b75d6":"code","fc4834a3":"code","147d1e61":"code","eefd20f7":"code","0f1da672":"markdown","f16b4b42":"markdown","36a3356d":"markdown","eb46445a":"markdown","9c2d518b":"markdown","278709cd":"markdown","24de0b9b":"markdown","90b8d456":"markdown","cbe0435a":"markdown","3031a355":"markdown","9b70ee96":"markdown","01c74e29":"markdown"},"source":{"02165214":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aafd31e5":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,classification_report\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV","8d79beba":"df = pd.read_csv('\/kaggle\/input\/dementia-prediction-dataset\/dementia_dataset.csv')","363c0760":"df.head()","f47110e7":"df.shape ","0583b1c3":"(df.isna().sum()\/df.shape[0])*100 #Amount of null values in each dataset","70941a64":"df.describe()","32f7cf04":"sns.countplot(df.Group) #No. of classes and Count for each","2b031f1e":"sns.boxplot(df.SES)","d7de1cc7":"df.SES.fillna(df.SES.median(),inplace=True) #Done..","ef392782":"df.isna().sum() #Lets check after the treatment..","e9c1dc97":"df.dropna(inplace=True)","5f07582c":"df.isna().sum() #Now we dont have any null values in any of the columns..","18990ddb":"#Lets check for outliers for each numerical column..\n\ncolnames = [i for i in df.columns if df[i].dtypes !='O'] #List of all the numerical columns\n\nfor i in colnames:\n    sns.boxplot(df[i])\n    plt.show()\n","58cf1e3c":"Q1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3-Q1\nlower = Q1 - 1.5 * IQR\nupper = Q3 + 1.5 * IQR\n","1123116f":"df_out = df[~((df[colnames] < (lower)) |(df[colnames] > (upper))).any(axis=1)]","e8b6689f":"df_out #After Removing outliers","4bf5e056":"df.drop(['Subject ID','MRI ID'],axis = 1,inplace = True)","1a99e919":"x = df.drop('Group',axis=1)\ny = df.Group","de0ae2d3":"#Data Cleaning\ny.replace('Demented',0,inplace=True)\ny.replace('Nondemented',1,inplace=True)\ny.replace('Converted',2,inplace=True)","b178730d":"x = pd.get_dummies(x)","4b771138":"x.head()","ffe58abb":"#Lets define every model with multiple parameters for the selection of best model with best parameters.\nmodel_params = {\n    'svm':{\n        'model': SVC(),\n        'params':{\n            'C' : [1,2,3],\n            'kernel' : ['linear', 'poly', 'rbf', 'sigmoid']\n        }\n    },\n    'logistic':{\n        'model':LogisticRegression(),\n        'params':{\n            'penalty' : ['l1', 'l2', 'elasticnet'],\n            'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n            \n        }\n    },\n    'RF':{\n        'model':RandomForestClassifier(n_jobs = -1),\n        'params':{\n            'n_estimators':[100,200,300,400,500],\n            'criterion' : [\"gini\", \"entropy\"]\n            \n        }\n    },\n    'DT':{\n        'model':DecisionTreeClassifier(),\n        'params':{\n            'criterion' : [\"gini\", \"entropy\"],\n            'splitter':[\"best\", \"random\"]\n        }\n    }\n}","aecdc93b":"scores = [] #List to append all the best scores\nfor model_name, modelp in model_params.items():\n    clf = GridSearchCV(modelp['model'],modelp['params'],cv = 5,return_train_score=False)\n    clf.fit(x,y)\n    scores.append({\n        'model': model_name,\n        'best_score': clf.best_score_,\n        'best_params': clf.best_params_\n    })\nscoreddf = pd.DataFrame(scores,columns=['model','best_score','best_params'])\nscoreddf","725b75d6":"xtrain,xtest,ytrain,ytest = train_test_split(x,y,random_state=42)","fc4834a3":"model = RandomForestClassifier(n_estimators=300,criterion='gini',n_jobs = -1) #From the above insights.\nmodel.fit(xtrain,ytrain)\nypred = model.predict(xtest)\n","147d1e61":"print(f'Models Accuracy = {round(accuracy_score(ytest,ypred),3)*100}%')","eefd20f7":"target_names = ['Demented', 'Non Demented', 'Converted']\nprint(classification_report(ytest, ypred, target_names=target_names))","0f1da672":"***Training and Testing Split***","f16b4b42":"\n***We can drop : 1) Subject ID  2) MRI ID***\n\n\n***These are the columns with no impact on output variable.***","36a3356d":"***Lets treat null values***","eb46445a":"**Time to convert each categorical feature into numerical one**","9c2d518b":"**Lets Select best model for this use case**","278709cd":"***As we can see from the above graphs, There are outliers present in some of the features***\n\n**We can treat the outliers based on IQR, and cap the outliers on the basis of lower visker and upper visker.**","24de0b9b":"**From the above results, We can see that Logistic and RandomForest are performing the best in this particular use case**","90b8d456":"**Lets create a Classification Report for the overall model performance**","cbe0435a":"**Time to build a model**","3031a355":"***From the above description we can observe that columns 'MR Delay' might be containing outliers..***","9b70ee96":"***Very less in terms of outliers... Still will Fill null values for this columns on the basis of median..***\n","01c74e29":"**We can drop the 2 rows from MMSE column , If wont affect the overall dataset**"}}