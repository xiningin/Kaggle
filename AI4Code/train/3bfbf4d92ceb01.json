{"cell_type":{"aa6862c5":"code","cec9dd7c":"code","70f982d2":"code","90af21ef":"code","5ee3adea":"code","cadbd73e":"code","c37e194b":"code","bd1c1b67":"code","486aafb5":"code","17ca03b8":"code","af1c9f64":"code","25521b5f":"code","ee8421ce":"code","168a35bd":"code","a56ab99b":"code","7aa9c327":"code","ef081fa3":"code","23c57235":"code","2a5651a6":"code","d15f84e8":"code","0c196c21":"code","f0903f65":"code","32ef3888":"code","f618cc4c":"code","10fe2f8e":"code","2165ea5f":"code","d3b3f293":"code","4427596f":"code","5142901f":"code","873b7a8d":"code","6778569b":"code","38008e1e":"code","7f06a242":"code","984b9880":"code","21b1f807":"code","1d2f53e7":"code","fcc3a9e3":"code","04450a4a":"code","fbdbb2f3":"code","545696e9":"code","6174fad0":"code","3d61b79c":"code","ca46de9c":"code","6a65b947":"code","a281070c":"code","e3ff8a54":"code","3e924a7e":"markdown","78ed9095":"markdown","c2228cb0":"markdown","92bc0874":"markdown","44459f79":"markdown","61071872":"markdown","6b5a780e":"markdown","1040a49d":"markdown","64436c48":"markdown","460043ec":"markdown","e04c802c":"markdown","6850d2d7":"markdown","e15bd60b":"markdown","e41510ff":"markdown","69224fa9":"markdown","db193ea5":"markdown","2c6838fe":"markdown","2d698238":"markdown","3222caf2":"markdown","bf71d2f4":"markdown","032fb5a2":"markdown"},"source":{"aa6862c5":"import sys\nsys.path.append('..\/input\/pytorch-image-models\/pytorch-image-models-master')\n\nimport collections\nimport gc\nimport json\nimport os\nimport random\nimport time\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nfrom albumentations import *\nfrom albumentations.pytorch import ToTensor\nimport cv2\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageFilter\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nimport tifffile as tiff\nimport timm\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader, Dataset, sampler\nfrom tqdm import tqdm_notebook as tqdm\n\n%matplotlib inline","cec9dd7c":"!ls ..\/input\/256-x-256-cropped-images","70f982d2":"DATASET = \"..\/input\/iwildcam2021-fgvc8\"\nCROPED_DATA = \"..\/input\/256-x-256-cropped-images\/\"\n\nTRAIN_CROPED_DATA = \"croped_images_train\/\"\nTEST_CROPED_DATA = \"croped_images_test\/\"","90af21ef":"BATCH_SIZE = 32\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 5000\nNUM_WORKERS = 4\nSEED = 2021","5ee3adea":"def set_seed(seed=2**3):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\nset_seed(SEED)","cadbd73e":"df_croped_img_ids_train = pd.read_csv(CROPED_DATA + \"croped_train.csv\")\ndf_croped_img_ids_test = pd.read_csv(CROPED_DATA + \"croped_test.csv\")","c37e194b":"df_croped_img_ids_train.head()","bd1c1b67":"df_croped_img_ids_test.head()","486aafb5":"with open('..\/input\/iwildcam2021-fgvc8\/metadata\/iwildcam2021_train_annotations.json', encoding='utf-8') as json_file:\n    train_annotations =json.load(json_file)\ndf_train_annotation = pd.DataFrame(train_annotations[\"annotations\"])","17ca03b8":"train = df_croped_img_ids_train[[\"id\", \"idx\"]].merge(df_train_annotation[[\"image_id\", \"category_id\"]], \n                                      left_on='id', right_on='image_id')[[\"id\", \"idx\", \"category_id\"]]","af1c9f64":"df_categories = pd.DataFrame(train_annotations[\"categories\"])","25521b5f":"cat_idxs = df_categories[\"id\"]\n\ndef convert_cat_to_index(x):\n    return np.where(cat_idxs==x)[0][0]","ee8421ce":"train[\"category_id\"] = train[\"category_id\"].map(lambda x: convert_cat_to_index(x))","168a35bd":"train.head()","a56ab99b":"! unzip ..\/input\/256-x-256-cropped-images\/croped_images_train.zip ","7aa9c327":"! unzip ..\/input\/256-x-256-cropped-images\/croped_images_test.zip","ef081fa3":"# ====================================================\n# Dataset for train\n# ====================================================\n\nmean = np.array([0.37087523, 0.370876, 0.3708759] )\nstd = np.array([0.21022698, 0.21022713, 0.21022706])\n\ndef img2tensor(img,dtype:np.dtype=np.float32):\n    if img.ndim==2 : img = np.expand_dims(img,2)\n    img = np.transpose(img,(2,0,1))\n    return torch.from_numpy(img.astype(dtype, copy=False))\n\nclass IWildcamTrainDataset(Dataset):\n    def __init__(self, df, tfms=None):\n        self.ids = df[\"id\"]\n        self.idxs = df[\"idx\"]\n        self.categories = df[\"category_id\"]\n        self.tfms = tfms\n        \n    def __len__(self):\n        return len(self.ids)\n    \n    def __getitem__(self, idx):\n        size = (256, 256)\n        image_id = self.ids[idx]\n        image_idx = self.idxs[idx]\n        iamge_categorie = self.categories[idx]\n        \n        image_path = TRAIN_CROPED_DATA + f\"{image_id}_{image_idx}.jpg\"\n        img = cv2.resize(cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB),size)\n\n        if self.tfms is not None:\n            augmented = self.tfms(image=img)\n            img = augmented['image']\n            \n        # we should normalize here\n        return img2tensor((img\/255.0  - mean)\/std), torch.tensor(iamge_categorie)","23c57235":"def get_aug(p=1.0):\n    return Compose([\n        HorizontalFlip(),\n        ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=0.9, \n                         border_mode=cv2.BORDER_REFLECT),\n        VerticalFlip(),\n        RandomBrightnessContrast(p=0.9),\n    ], p=p)","2a5651a6":"# ====================================================\n# EfficientNet Model\n# ====================================================\n\nclass enet_v2(nn.Module):\n\n    def __init__(self, backbone, out_dim, pretrained=False):\n        super(enet_v2, self).__init__()\n        self.enet = timm.create_model(backbone, pretrained=pretrained)\n        in_ch = self.enet.classifier.in_features\n        self.myfc = nn.Linear(in_ch, out_dim)\n        self.enet.classifier = nn.Identity()\n\n    def forward(self, x):\n        x = self.enet(x)\n        x = self.myfc(x)\n        return x","d15f84e8":"model = enet_v2(backbone=\"tf_efficientnet_b0\", out_dim=205)\nmodel.to(DEVICE)","0c196c21":"# ====================================================\n# Optimizer and Loss\n# ====================================================\n\noptimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n#optimizer = torch.optim.SGD([\n#                {'params': model.parameters()},\n#               {'params': model.classifier.parameters(), 'lr': 1e-4}\n#            ], lr=1e-3, momentum=0.9)\ncriterion = nn.CrossEntropyLoss()","f0903f65":"rus = RandomUnderSampler(random_state=SEED, replacement=True)\n\ndef generate_dataloders(train):\n    \n    train_resampled, _ = rus.fit_resample(train, train[\"category_id\"])\n    test_resampled, _ = rus.fit_resample(train, train[\"category_id\"])\n\n    train_resampled = train_resampled.reset_index(drop=True)\n    test_resampled = test_resampled.reset_index(drop=True)\n    \n    ds_train = IWildcamTrainDataset(train_resampled, tfms=get_aug())\n    dl_train = DataLoader(ds_train,batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n    ds_test = IWildcamTrainDataset(test_resampled)\n    dl_test = DataLoader(ds_test,batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n    \n    return dl_train, dl_test","32ef3888":"# ====================================================\n# Train\n# ====================================================\n\nfor epoch in tqdm(range(EPOCHS)):\n    \n    dl_train, dl_test = generate_dataloders(train)\n    \n    ###Train\n    model.train()\n    train_loss = 0\n    \n    for data in dl_train:\n        optimizer.zero_grad()\n        imgs, categories = data\n        imgs = imgs.to(DEVICE)\n        categories = categories.to(DEVICE)\n        \n        outputs = model(imgs)\n    \n        loss = criterion(outputs, categories)\n        loss.backward()\n        optimizer.step()\n            \n        train_loss += loss.item()\n    train_loss \/= len(dl_train)\n        \n    print(f\"EPOCH: {epoch + 1}, train_loss: {train_loss}\")\n        \n    ###Validation\n    model.eval()\n    valid_loss = 0\n        \n    for data in dl_test:\n        imgs, categories = data\n        imgs = imgs.to(DEVICE)\n        categories = categories.to(DEVICE)\n        \n        outputs = model(imgs)\n    \n        loss = criterion(outputs, categories)\n        \n        valid_loss += loss.item()\n    valid_loss \/= len(dl_test)\n        \n    print(f\"EPOCH: {epoch + 1}, valid_loss: {valid_loss}\")\n        \n    \n    if (epoch+1)%50 == 0 or (epoch+1)%EPOCHS == 0:\n        ###Save model\n        torch.save(model.state_dict(), f\"{epoch+1}_.pth\")","f618cc4c":"# ====================================================\n# Dataset for test\n# ====================================================\n\nmean = np.array([0.37087523, 0.370876, 0.3708759] )\nstd = np.array([0.21022698, 0.21022713, 0.21022706])\n\nclass IWildcamTestDataset(Dataset):\n    def __init__(self, df, tfms=None):\n        self.ids = df[\"id\"]\n        self.idx = df[\"idx\"]\n        self.tfms = tfms\n        \n    def __len__(self):\n        return len(self.ids)\n    \n    def __getitem__(self, idx):\n        size = (256, 256)\n        image_id = self.ids[idx]\n        image_idx = self.idx[idx]\n        \n        image_path = TEST_CROPED_DATA + f\"{image_id}_{image_idx}.jpg\"\n        \n        img = cv2.resize(cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB),size)\n\n        if self.tfms is not None:\n            augmented = self.tfms(image=img)\n            img = augmented['image']\n            \n        # we should normalize here\n        return img2tensor((img\/255.0 - mean)\/std), image_id","10fe2f8e":"ds_test = IWildcamTestDataset(df_croped_img_ids_test)\ndl_test = DataLoader(ds_test,batch_size=32,shuffle=False,num_workers=NUM_WORKERS)","2165ea5f":"model = enet_v2(backbone=\"tf_efficientnet_b0\", out_dim=205)\nmodel.to(DEVICE)\nmodel.load_state_dict(torch.load(f\"{epoch+1}_.pth\"))\nmodel.eval()","d3b3f293":"pred_categories = []\npred_img_ids = []","4427596f":"with torch.no_grad():\n    for imgs, img_ids in tqdm(dl_test):\n        imgs = imgs.to(DEVICE)\n        \n        outputs = model(imgs)\n        output_labels = torch.argmax(outputs, dim=1).tolist()\n        pred_categories += output_labels\n        pred_img_ids += img_ids","5142901f":"pred = collections.defaultdict(list)\nfor category, img_id in zip(pred_categories, pred_img_ids):\n    pred[img_id].append(category)","873b7a8d":"pred","6778569b":"sub = pd.read_csv(\"..\/input\/iwildcam2021-fgvc8\/sample_submission.csv\")\ncol_Predicted = [col for col in sub.columns if \"Predicted\" in col]","38008e1e":"with open('..\/input\/iwildcam2021-fgvc8\/metadata\/iwildcam2021_train_annotations.json', encoding='utf-8') as json_file:\n    train_annotations =json.load(json_file)\ndf_categories = pd.DataFrame.from_records(train_annotations[\"categories\"])","7f06a242":"results = []\n\nfor key in pred.keys():\n    c = collections.Counter(pred[key])\n    \n    res = []\n    cnts = [ 0 for i in range(205)]\n    for category, cnt in c.items():\n        cnts[category] = cnt\n    res += [key] + cnts[1:]\n    results.append(res)","984b9880":"sub_tmp = pd.DataFrame(results, columns=sub.columns)","21b1f807":"sub_tmp.head()","1d2f53e7":"sub_tmp.to_csv(\".\/sub_tmp.csv\", index=False)","fcc3a9e3":"with open('..\/input\/iwildcam2021-fgvc8\/metadata\/iwildcam2021_test_information.json', encoding='utf-8') as json_file:\n    test_information =json.load(json_file)\n    \ndf_test_info = pd.DataFrame(test_information[\"images\"])[[\"id\", \"seq_id\"]]\ndf_test_info.head()","04450a4a":"sub_tmp = sub_tmp.merge(df_test_info, left_on=\"Id\", right_on=\"id\", how=\"right\")","fbdbb2f3":"sub_tmp.head()","545696e9":"sum_counts = []\nfor i in range(len(sub_tmp)):\n    sum_counts.append(sum(sub_tmp.iloc[i][col_Predicted]))","6174fad0":"sub_tmp[\"total\"] =  sum_counts\nsub_tmp = sub_tmp.sort_values('total', ascending=False)\nsub_tmp = sub_tmp[~sub_tmp.duplicated(keep='first', subset='seq_id')].fillna(\"0\")","3d61b79c":"sub_tmp","ca46de9c":"# Since it was difficult to join the pandas series, I intentionally created an extra column.\nsub = sub.reset_index()\nsub = sub[[\"index\", \"Id\"]].merge(sub_tmp, left_on=\"Id\", right_on=\"seq_id\")","6a65b947":"sub = sub[[\"Id_x\"] + col_Predicted].rename(columns={\"Id_x\": \"Id\"})\nsub.to_csv(\"sub.csv\", index=False)","a281070c":"sub.head()","e3ff8a54":"#If we don't delete them, csv files are buried and cannot be retrieved.\n!rm -r croped_images_train\n!rm -r croped_images_test","3e924a7e":"## Create model","78ed9095":"## Train\n\nSince we know that [the training data is imbalanced](https:\/\/www.kaggle.com\/nayuts\/iwildcam-2021-overviewing-for-start#EDA), I undersampled it.","c2228cb0":"Convert to pandas dataframe.","92bc0874":"## Load trained model","44459f79":"## inference","61071872":"## Create dataset for training","6b5a780e":"## Create dataset for test","1040a49d":"Add seq_id information to the counted results. iwildcam2021_test_information.json contains the mapping between the id of the image and the id of the sequence.","64436c48":"Since there are multiple lines for the same sequence ID. We should aggregate them to single line. In this case, we will choose the image with the highest number of animals shown and submit the animal species and the number of animals shown in that image.","460043ec":"### create train dataframe","e04c802c":"### unzip croped data","6850d2d7":"# Create submit file","e15bd60b":"I'll match the result to the sample submission format. I was told that the order of the rows is not related to the score, but we will match it just in case.","e41510ff":"# Train","69224fa9":"For each image, count the number of each animal species and store them in the corresponding column.","db193ea5":"## train setting","2c6838fe":"Many thanks to @nayuts for sharing his hard work with the community. All credit for this work goes to below notebooks and @nayuts. Kindly upvote and appreciate the original work.\n\n1. https:\/\/www.kaggle.com\/nayuts\/256-x-256-cropped-images\n2. https:\/\/www.kaggle.com\/nayuts\/efficientnet-with-undersampling\n\nI have increased the number of epochs and tuned the model optimizer paramaters to get slightly better results.","2d698238":"Take right join on the image id.","3222caf2":"<img src=\"https:\/\/raw.githubusercontent.com\/tasotasoso\/kaggle_media\/main\/iwildcam2021\/model_image.png\" width=\"***300***\">","bf71d2f4":"# Inference","032fb5a2":"### setting"}}