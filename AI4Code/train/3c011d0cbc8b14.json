{"cell_type":{"60a2035a":"code","833ced12":"code","aaf4098f":"code","81a51737":"code","60f72a4d":"code","46fb8f0a":"code","8d5ba268":"code","5b92c9be":"code","719e5655":"code","6fda77d1":"code","ccede302":"markdown","8a7483f4":"markdown","da74fc7a":"markdown","0a1b3771":"markdown","d5bf8106":"markdown","dff8cd25":"markdown","0f36c3bd":"markdown","9de16bcf":"markdown","20fae512":"markdown"},"source":{"60a2035a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","833ced12":"data = pd.read_csv('..\/input\/dataset-badmintn-alwi\/Dataset_alwi_bdm.csv')\ndata","aaf4098f":"#library -------------\nimport re\nimport nltk\nimport string\nimport sklearn\nimport pandas as pd\n\n#modulstemming------\n!pip install Sastrawi\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n\n#fitur_nltk-------\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \nfrom nltk.probability import FreqDist\n\n#fitur_sklearn--------\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn.feature_extraction.text import TfidfVectorizer","81a51737":"df = data [['Tweet']]\ndf['Tweet'] = df['Tweet'].astype(str)\n\ndf = pd.DataFrame(data[['Tweet']])\n\ndf[\"Case_Folding_Tweet\"] = df['Tweet'].str.lower()\n\ndf = pd.DataFrame(df[['Case_Folding_Tweet']])\n\ndf","60f72a4d":"def remove_df_special(text):\n    text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n    text = text.encode('ascii', 'replace').decode('ascii')\n    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\\/\\\/\\S+)\",\" \", text).split())\n    return text.replace(\"http:\/\/\", \" \").replace(\"https:\/\/\", \" \")\n                \ndf['Case_Folding_Tweet'] = df['Case_Folding_Tweet'].apply(remove_df_special)\n\ndef remove_df_number(text):\n    return  re.sub(r\"\\d+\", \"\", text)\n\ndf['Case_Folding_Tweet'] = df['Case_Folding_Tweet'].apply(remove_df_number)\n\ndef remove_df_punctuation(text):\n    return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n\ndf['Case_Folding_Tweet'] = df['Case_Folding_Tweet'].apply(remove_df_punctuation)\n\ndef remove_df_whitespace(text):\n    return text.strip()\n\ndf['Case_Folding_Tweet'] = df['Case_Folding_Tweet'].apply(remove_df_whitespace)\n\ndef remove_df_whitespace_multiple(text):\n    return re.sub('\\s+',' ',text)\n\ndf['Case_Folding_Tweet'] = df['Case_Folding_Tweet'].apply(remove_df_whitespace_multiple)\n\ndef remove_df_singl_char(text):\n    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n\ndf['Case_Folding_Tweet'] = df['Case_Folding_Tweet'].apply(remove_df_singl_char)\n\ndef word_df_tokenize(text):\n    return word_tokenize(text)\n\ndf['df_tokenizing'] = df['Case_Folding_Tweet'].apply(word_df_tokenize)\n\nprint('Tokenizing Isi Tweet : \\n') \nprint(df['df_tokenizing'])","46fb8f0a":"def freqDist_wrapper(text):\n    return FreqDist(text)\n\ndf['df_jumlah_token'] = df['df_tokenizing'].apply(freqDist_wrapper)\n\nprint('Jumlah Token di Isi Tweet : \\n') \nprint(df['df_jumlah_token'].apply(lambda x : x.most_common()))","8d5ba268":"rex_stopwords = stopwords.words('indonesian')\n\nrex_stopwords.extend([\"mn\", \"dg\", \"jg\", \"jga\", \"ny\", \"d\", 'kli', \n                       'masasi', 'kalo', 'biar', 'iya', 'bikin', \n                       'gak', 'ga', 'krn', 'bilang', 'nih', 'sih', \n                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n                       'jd', 'jgn', 'sdh', 'nya', 'n', 't', \n                       'nyg', 'hehe', 'pen', 'u', 'aja', 'loh', 'nan',\n                       '&amp', 'yah'])\n\nrex_stopwords = set(rex_stopwords)\n\ndef df_stopwords_removal(words):\n    return [word for word in words if word not in rex_stopwords]\n\ndf['df_stopword'] = df['df_tokenizing'].apply(df_stopwords_removal) \n\n\nprint('Stopword Isi Tweet : \\n') \nprint(df['df_stopword'])","5b92c9be":"vectorizer = TfidfVectorizer(stop_words='english')\nresponse = vectorizer.fit_transform(data['Tweet'])\nprint(response)","719e5655":"response.todense()","6fda77d1":"Data = pd.DataFrame(response.todense().T,\n                   index = vectorizer.get_feature_names(),\n                   columns = [f'Teks{i+1}' for i in range(len(data))])\n\nData","ccede302":"# Library Yang Digunakan","8a7483f4":"Perhitungan jumlah kata","da74fc7a":"> **Stopword \/ Filtering**","0a1b3771":"# Tahap Pre-Processing","d5bf8106":"# Tahap Pembobotan TF-IDF","dff8cd25":"> **Case Folding**","0f36c3bd":"**> Sekian dan Terimakasih**","9de16bcf":"> **Tokenizing**","20fae512":"# Dataset Yang Digunakan"}}