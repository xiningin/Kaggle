{"cell_type":{"27894318":"code","e98e895e":"code","115551cf":"code","cca711b7":"code","df0bd960":"code","f4b5cfbd":"code","50caf03a":"code","99b3dfd7":"code","9f8ffea1":"code","3618ab01":"code","27beb927":"code","9e594398":"code","dc33545b":"code","3cf4409b":"code","089712e0":"code","2edd38cf":"code","ea41c72b":"code","13610500":"code","7eebdb31":"code","2b292bcd":"code","ead43d3d":"code","a0587d3d":"code","22a201c4":"markdown"},"source":{"27894318":"!pip install pandas_path\n!pip install fasttext\n!pip install pytorch-lightning","e98e895e":"%matplotlib inline\n\nimport json\nimport logging\nfrom pathlib import Path\nimport random\nimport tarfile\nimport tempfile\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pandas_path  \nfrom tqdm import tqdm\nimport zipfile\n\nimport torch                    \nimport torchvision\nimport fasttext\nimport pytorch_lightning as pl","115551cf":"!wget -O Lnmwdnq3YcF7F3YsJncp.zip --no-check-certificate --no-proxy \"https:\/\/drivendata-competition-fb-hateful-memes-data.s3.amazonaws.com\/Lnmwdnq3YcF7F3YsJncp.zip?AWSAccessKeyId=AKIAJYJLFLA7N3WRICBQ&Signature=PLVOjg3fmVHcb8Qvuiasj3ZJG7o%3D&Expires=1596241398\"\nprint(\"Dowmloading  Done\")\n!unzip -P KexZs4tn8hujn1nK Lnmwdnq3YcF7F3YsJncp.zip","cca711b7":"data_dir = Path.cwd().parent \/ \"data\" \n\nimg_tar_path = data_dir \/ \"img.tar.gz\"\ntrain_path = data_dir \/ \"train.jsonl\"\ndev_path = data_dir \/ \"dev.jsonl\"\ntest_path = data_dir \/ \"test.jsonl\"\n\nprint(data_dir)\nprint(img_tar_path)","df0bd960":"if not (data_dir \/ \"img\").exists():\n    with tarfile.open(img_tar_path) as tf:\n        tf.extractall(data_dir)","f4b5cfbd":"print(train_path)","50caf03a":"train_samples_frame = pd.read_json(train_path, lines=True)\ntrain_samples_frame.head()","99b3dfd7":"# class imblances check\ntrain_samples_frame.label.value_counts()","9f8ffea1":"# Text exploration\ntrain_samples_frame.text.map(\n    lambda text: len(text.split(\" \"))\n).describe()","3618ab01":"from PIL import Image\nimages = [\n    Image.open(\n        data_dir \/ train_samples_frame.loc[i, \"img\"]\n    ).convert(\"RGB\")\n    for i in range(5)\n]\n\nfor image in images:\n    print(image.size)\n\n","27beb927":"# define a callable image_transform with Compose\nimage_transform = torchvision.transforms.Compose(\n    [\n        torchvision.transforms.Resize(size=(224, 224)),\n        torchvision.transforms.ToTensor()\n    ]\n)\n\n# convert the images and prepare for visualization.\ntensor_img = torch.stack(\n    [image_transform(image) for image in images]\n)\ngrid = torchvision.utils.make_grid(tensor_img)\n\n# plot\nplt.rcParams[\"figure.figsize\"] = (20, 5)\nplt.axis('off')\n_ = plt.imshow(grid.permute(1, 2, 0))\n\n","9e594398":"class HatefulMemesDataset(torch.utils.data.Dataset):\n    \"\"\"Uses jsonl data to preprocess and serve \n    dictionary of multimodal tensors for model input.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_path,\n        img_dir,\n        image_transform,\n        text_transform,\n        balance=False,\n        dev_limit=None,\n        random_state=0,\n    ):\n\n        self.samples_frame = pd.read_json(\n            data_path, lines=True\n        )\n        self.dev_limit = dev_limit\n        if balance:\n            neg = self.samples_frame[\n                self.samples_frame.label.eq(0)\n            ]\n            pos = self.samples_frame[\n                self.samples_frame.label.eq(1)\n            ]\n            self.samples_frame = pd.concat(\n                [\n                    neg.sample(\n                        pos.shape[0], \n                        random_state=random_state\n                    ), \n                    pos\n                ]\n            )\n        if self.dev_limit:\n            if self.samples_frame.shape[0] > self.dev_limit:\n                self.samples_frame = self.samples_frame.sample(\n                    dev_limit, random_state=random_state\n                )\n        self.samples_frame = self.samples_frame.reset_index(\n            drop=True\n        )\n        self.samples_frame.img = self.samples_frame.apply(\n            lambda row: (img_dir \/ row.img), axis=1\n        )\n\n        # https:\/\/github.com\/drivendataorg\/pandas-path\n        if not self.samples_frame.img.path.exists().all():\n            raise FileNotFoundError\n        if not self.samples_frame.img.path.is_file().all():\n            raise TypeError\n            \n        self.image_transform = image_transform\n        self.text_transform = text_transform\n\n    def __len__(self):\n        \"\"\"This method is called when you do len(instance) \n        for an instance of this class.\n        \"\"\"\n        return len(self.samples_frame)\n\n    def __getitem__(self, idx):\n        \"\"\"This method is called when you do instance[key] \n        for an instance of this class.\n        \"\"\"\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_id = self.samples_frame.loc[idx, \"id\"]\n\n        image = Image.open(\n            self.samples_frame.loc[idx, \"img\"]\n        ).convert(\"RGB\")\n        image = self.image_transform(image)\n\n        text = torch.Tensor(\n            self.text_transform.get_sentence_vector(\n                self.samples_frame.loc[idx, \"text\"]\n            )\n        ).squeeze()\n\n        if \"label\" in self.samples_frame.columns:\n            label = torch.Tensor(\n                [self.samples_frame.loc[idx, \"label\"]]\n            ).long().squeeze()\n            sample = {\n                \"id\": img_id, \n                \"image\": image, \n                \"text\": text, \n                \"label\": label\n            }\n        else:\n            sample = {\n                \"id\": img_id, \n                \"image\": image, \n                \"text\": text\n            }\n\n        return sample","dc33545b":"class LanguageAndVisionConcat(torch.nn.Module):\n    def __init__(\n        self,\n        num_classes,\n        loss_fn,\n        language_module,\n        vision_module,\n        language_feature_dim,\n        vision_feature_dim,\n        fusion_output_size,\n        dropout_p,\n        \n    ):\n        super(LanguageAndVisionConcat, self).__init__()\n        self.language_module = language_module\n        self.vision_module = vision_module\n        self.fusion = torch.nn.Linear(\n            in_features=(language_feature_dim + vision_feature_dim), \n            out_features=fusion_output_size\n        )\n        self.fc = torch.nn.Linear(\n            in_features=fusion_output_size, \n            out_features=num_classes\n        )\n        self.loss_fn = loss_fn\n        self.dropout = torch.nn.Dropout(dropout_p)\n        \n    def forward(self, text, image, label=None):\n        text_features = torch.nn.functional.relu(\n            self.language_module(text)\n        )\n        image_features = torch.nn.functional.relu(\n            self.vision_module(image)\n        )\n        combined = torch.cat(\n            [text_features, image_features], dim=1\n        )\n        fused = self.dropout(\n            torch.nn.functional.relu(\n            self.fusion(combined)\n            )\n        )\n        logits = self.fc(fused)\n        pred = torch.nn.functional.softmax(logits)\n        loss = (\n            self.loss_fn(pred, label) \n            if label is not None else label\n        )\n        return (pred, loss)","3cf4409b":"\n\n\n# for the purposes of this post, we'll filter\n# much of the lovely logging info from our LightningModule\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger().setLevel(logging.WARNING)\n\n\nclass HatefulMemesModel(pl.LightningModule):\n    def __init__(self, hparams):\n        for data_key in [\"train_path\", \"dev_path\", \"img_dir\",]:\n            # ok, there's one for-loop but it doesn't count\n            if data_key not in hparams.keys():\n                raise KeyError(\n                    f\"{data_key} is a required hparam in this model\"\n                )\n        \n        super(HatefulMemesModel, self).__init__()\n        self.hparams = hparams\n        \n        # assign some hparams that get used in multiple places\n        self.embedding_dim = self.hparams.get(\"embedding_dim\", 300)\n        self.language_feature_dim = self.hparams.get(\n            \"language_feature_dim\", 300\n        )\n        self.vision_feature_dim = self.hparams.get(\n            # balance language and vision features by default\n            \"vision_feature_dim\", self.language_feature_dim\n        )\n        self.output_path = Path(\n            self.hparams.get(\"output_path\", \"model-outputs\")\n        )\n        self.output_path.mkdir(exist_ok=True)\n        \n        # instantiate transforms, datasets\n        self.text_transform = self._build_text_transform()\n        self.image_transform = self._build_image_transform()\n        self.train_dataset = self._build_dataset(\"train_path\")\n        self.dev_dataset = self._build_dataset(\"dev_path\")\n        \n        # set up model and training\n        self.model = self._build_model()\n        self.trainer_params = self._get_trainer_params()\n    \n    ## Required LightningModule Methods (when validating) ##\n    \n    def forward(self, text, image, label=None):\n        return self.model(text, image, label)\n\n    def training_step(self, batch, batch_nb):\n        preds, loss = self.forward(\n            text=batch[\"text\"], \n            image=batch[\"image\"], \n            label=batch[\"label\"]\n        )\n        \n        return {\"loss\": loss}\n\n    def validation_step(self, batch, batch_nb):\n        preds, loss = self.eval().forward(\n            text=batch[\"text\"], \n            image=batch[\"image\"], \n            label=batch[\"label\"]\n        )\n        \n        return {\"batch_val_loss\": loss}\n\n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack(\n            tuple(\n                output[\"batch_val_loss\"] \n                for output in outputs\n            )\n        ).mean()\n        \n        return {\n            \"val_loss\": avg_loss,\n            \"progress_bar\":{\"avg_val_loss\": avg_loss}\n        }\n\n    def configure_optimizers(self):\n        optimizers = [\n            torch.optim.AdamW(\n                self.model.parameters(), \n                lr=self.hparams.get(\"lr\", 0.001)\n            )\n        ]\n        schedulers = [\n            torch.optim.lr_scheduler.ReduceLROnPlateau(\n                optimizers[0]\n            )\n        ]\n        return optimizers, schedulers\n    \n    @pl.data_loader\n    def train_dataloader(self):\n        return torch.utils.data.DataLoader(\n            self.train_dataset, \n            shuffle=True, \n            batch_size=self.hparams.get(\"batch_size\", 4), \n            num_workers=self.hparams.get(\"num_workers\", 16)\n        )\n\n    @pl.data_loader\n    def val_dataloader(self):\n        return torch.utils.data.DataLoader(\n            self.dev_dataset, \n            shuffle=False, \n            batch_size=self.hparams.get(\"batch_size\", 4), \n            num_workers=self.hparams.get(\"num_workers\", 16)\n        )\n    \n    ## Convenience Methods ##\n    \n    def fit(self):\n        self._set_seed(self.hparams.get(\"random_state\", 42))\n        self.trainer = pl.Trainer(**self.trainer_params)\n        self.trainer.fit(self)\n        \n    def _set_seed(self, seed):\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(seed)\n\n    def _build_text_transform(self):\n        with tempfile.NamedTemporaryFile() as ft_training_data:\n            ft_path = Path(ft_training_data.name)\n            with ft_path.open(\"w\") as ft:\n                training_data = [\n                    json.loads(line)[\"text\"] + \"\/n\" \n                    for line in open(\n                        self.hparams.get(\"train_path\")\n                    ).read().splitlines()\n                ]\n                for line in training_data:\n                    ft.write(line + \"\\n\")\n                language_transform = fasttext.train_unsupervised(\n                    str(ft_path),\n                    model=self.hparams.get(\"fasttext_model\", \"cbow\"),\n                    dim=self.embedding_dim\n                )\n        return language_transform\n    \n    def _build_image_transform(self):\n        image_dim = self.hparams.get(\"image_dim\", 224)\n        image_transform = torchvision.transforms.Compose(\n            [\n                torchvision.transforms.Resize(\n                    size=(image_dim, image_dim)\n                ),        \n                torchvision.transforms.ToTensor(),\n                # all torchvision models expect the same\n                # normalization mean and std\n                # https:\/\/pytorch.org\/docs\/stable\/torchvision\/models.html\n                torchvision.transforms.Normalize(\n                    mean=(0.485, 0.456, 0.406), \n                    std=(0.229, 0.224, 0.225)\n                ),\n            ]\n        )\n        return image_transform\n\n    def _build_dataset(self, dataset_key):\n        return HatefulMemesDataset(\n            data_path=self.hparams.get(dataset_key, dataset_key),\n            img_dir=self.hparams.get(\"img_dir\"),\n            image_transform=self.image_transform,\n            text_transform=self.text_transform,\n            # limit training samples only\n            dev_limit=(\n                self.hparams.get(\"dev_limit\", None) \n                if \"train\" in str(dataset_key) else None\n            ),\n            balance=True if \"train\" in str(dataset_key) else False,\n        )\n    \n    def _build_model(self):\n        # we're going to pass the outputs of our text\n        # transform through an additional trainable layer\n        # rather than fine-tuning the transform\n        language_module = torch.nn.Linear(\n                in_features=self.embedding_dim,\n                out_features=self.language_feature_dim\n        )\n        \n        # easiest way to get features rather than\n        # classification is to overwrite last layer\n        # with an identity transformation, we'll reduce\n        # dimension using a Linear layer, resnet is 2048 out\n        vision_module = torchvision.models.resnet152(\n            pretrained=True\n        )\n        vision_module.fc = torch.nn.Linear(\n                in_features=2048,\n                out_features=self.vision_feature_dim\n        )\n\n        return LanguageAndVisionConcat(\n            num_classes=self.hparams.get(\"num_classes\", 2),\n            loss_fn=torch.nn.CrossEntropyLoss(),\n            language_module=language_module,\n            vision_module=vision_module,\n            language_feature_dim=self.language_feature_dim,\n            vision_feature_dim=self.vision_feature_dim,\n            fusion_output_size=self.hparams.get(\n                \"fusion_output_size\", 512\n            ),\n            dropout_p=self.hparams.get(\"dropout_p\", 0.1),\n        )\n    \n    def _get_trainer_params(self):\n        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n            filepath=self.output_path,\n            monitor=self.hparams.get(\n                \"checkpoint_monitor\", \"avg_val_loss\"\n            ),\n            mode=self.hparams.get(\n                \"checkpoint_monitor_mode\", \"min\"\n            ),\n            verbose=self.hparams.get(\"verbose\", True)\n        )\n\n        early_stop_callback = pl.callbacks.EarlyStopping(\n            monitor=self.hparams.get(\n                \"early_stop_monitor\", \"avg_val_loss\"\n            ),\n            min_delta=self.hparams.get(\n                \"early_stop_min_delta\", 0.001\n            ),\n            patience=self.hparams.get(\n                \"early_stop_patience\", 3\n            ),\n            verbose=self.hparams.get(\"verbose\", True),\n        )\n\n        trainer_params = {\n            \"checkpoint_callback\": checkpoint_callback,\n            \"early_stop_callback\": early_stop_callback,\n            \"default_root_dir\": self.output_path,\n            \"accumulate_grad_batches\": self.hparams.get(\n                \"accumulate_grad_batches\", 1\n            ),\n            \"gpus\": self.hparams.get(\"n_gpu\", 1),\n            \"max_epochs\": self.hparams.get(\"max_epochs\", 100),\n            \"gradient_clip_val\": self.hparams.get(\n                \"gradient_clip_value\", 1\n            ),\n        }\n        return trainer_params\n            \n    @torch.no_grad()\n    def make_submission_frame(self, test_path):\n        test_dataset = self._build_dataset(test_path)\n        submission_frame = pd.DataFrame(\n            index=test_dataset.samples_frame.id,\n            columns=[\"proba\", \"label\"]\n        )\n        test_dataloader = torch.utils.data.DataLoader(\n            test_dataset, \n            shuffle=False, \n            batch_size=self.hparams.get(\"batch_size\", 4), \n            num_workers=self.hparams.get(\"num_workers\", 16))\n        for batch in tqdm(test_dataloader, total=len(test_dataloader)):\n            preds, _ = self.model.eval().to(\"cpu\")(\n                batch[\"text\"], batch[\"image\"]\n            )\n            submission_frame.loc[batch[\"id\"], \"proba\"] = preds[:, 1]\n            submission_frame.loc[batch[\"id\"], \"label\"] = preds.argmax(dim=1)\n        submission_frame.proba = submission_frame.proba.astype(float)\n        submission_frame.label = submission_frame.label.astype(int)\n        return submission_frame","089712e0":"print(\"[DataDir]: \", data_dir)\nprint(\"[TrainDir]: \", train_path)\nprint(\"[DevDir]: \", dev_path)","2edd38cf":"hparams = {\n    \n    # Required hparams\n    \"train_path\": train_path,\n    \"dev_path\": dev_path,\n    \"img_dir\": data_dir,\n    \n    # Optional hparams\n    \"embedding_dim\": 150,\n    \"language_feature_dim\": 300,\n    \"vision_feature_dim\": 300,\n    \"fusion_output_size\": 256,\n    \"output_path\": \"model-outputs\",\n    \"dev_limit\": None,\n    \"lr\": 0.00005,\n    \"max_epochs\": 10,\n    \"n_gpu\": 1,\n    \"batch_size\": 4,\n    # allows us to \"simulate\" having larger batches \n    \"accumulate_grad_batches\": 16,\n    \"early_stop_patience\": 3,\n}\n\nhateful_memes_model = HatefulMemesModel(hparams=hparams)\nhateful_memes_model.fit()","ea41c72b":"# we should only have saved the best checkpoint\ncheckpoints = list(Path(\"model-outputs\").glob(\"*.ckpt\"))\nassert len(checkpoints) == 1\ncheckpoints\n","13610500":"checkpoints[0]","7eebdb31":"hateful_memes_model = HatefulMemesModel.load_from_checkpoint(\"\/content\/data\/model-outputs\/epoch=0.ckpt\")\nsubmission = hateful_memes_model.make_submission_frame(test_path)\nsubmission.head()","2b292bcd":"submission.groupby(\"label\").proba.mean()","ead43d3d":"submission.label.value_counts()","a0587d3d":"submission.to_csv((\"model-outputs\/submission.csv\"), index=True)","22a201c4":"DrivenData Hateful memes competition by facebook.\nbaseline version\nrunnable in colab\/local machine as kaggle have limited storage support."}}