{"cell_type":{"a189d3d3":"code","19c753a4":"code","1fae8990":"code","750f584e":"code","77ce2d62":"code","838f000b":"code","f1edffe7":"code","c5d1072f":"code","c72a89a7":"code","50449807":"code","81f9a142":"code","8bf45c06":"code","a56eefd2":"code","4dd822f2":"code","7141f5fa":"code","2de28f1b":"code","f3dcb4f8":"code","a313ac5b":"code","587e41b4":"code","57028b3e":"code","98a68fd7":"code","8a7d190b":"markdown","cec16353":"markdown","ddd4b4ac":"markdown","ca5068c5":"markdown","a788732b":"markdown","ec873986":"markdown","7eadbdd2":"markdown","b6d4032c":"markdown","a8c58bed":"markdown","39051128":"markdown","5dbfe6f7":"markdown","f111d2a0":"markdown","5e1fa819":"markdown","2adf74ee":"markdown","b4cac856":"markdown","5fc6f881":"markdown","a34a44a0":"markdown","3e2a7dfd":"markdown","c5f71f4f":"markdown","13135bc7":"markdown","7f24fc89":"markdown","aef498d0":"markdown"},"source":{"a189d3d3":"import matplotlib\nmatplotlib.rcParams['figure.figsize'] = (12, 10)","19c753a4":"!pip install ensemble_boxes","1fae8990":"import os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import rcParams\nsns.set(rc={\"font.size\":9,\"axes.titlesize\":15,\"axes.labelsize\":9,\n            \"axes.titlepad\":11, \"axes.labelpad\":9, \"legend.fontsize\":7,\n            \"legend.title_fontsize\":7, 'axes.grid' : False})\nimport cv2\nimport json\nimport pandas as pd\nimport glob\nimport os.path as osp\nfrom path import Path\nimport datetime\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport random\nimport shutil\nfrom sklearn.model_selection import train_test_split\n\nfrom ensemble_boxes import *\nimport warnings\nfrom collections import Counter\n","750f584e":"df = pd.read_csv(\"..\/input\/vinbigdata-competition-jpg-data-3x-downsampled\/train_downsampled.csv\")\ndf.head(5)","77ce2d62":"#Ref : https:\/\/www.kaggle.com\/sreevishnudamodaran\/vinbigdata-fusing-bboxes-coco-dataset\/data\ndef plot_img(img, size=(18, 18), is_rgb=True, title=\"\", cmap='gray'):\n    plt.figure(figsize=size)\n    plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    plt.show()\n\ndef plot_imgs(imgs, cols=2, size=10, is_rgb=True, title=\"\", cmap='gray', img_size=None):\n    rows = len(imgs)\/\/cols + 1\n    fig = plt.figure(figsize=(cols*size, rows*size))\n    for i, img in enumerate(imgs):\n        if img_size is not None:\n            img = cv2.resize(img, img_size)\n        fig.add_subplot(rows, cols, i+1)\n        plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    \ndef draw_bbox(image, box, label, color):   \n    alpha = 0.1\n    alpha_box = 0.4\n    thickness = 1\n    overlay_bbox = image.copy()\n    overlay_text = image.copy()\n    output = image.copy()\n    label = str(label)\n    text_width, text_height = cv2.getTextSize(label.upper(), cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)[0]\n    cv2.rectangle(overlay_bbox, (box[0], box[1]), (box[2], box[3]),\n                color, -1)\n    cv2.addWeighted(overlay_bbox, alpha, output, 1 - alpha, 0, output)\n    cv2.rectangle(overlay_text, (box[0], box[1]-7-text_height), (box[0]+text_width+2, box[1]),\n                (0, 0, 0), -1)\n    cv2.addWeighted(overlay_text, alpha_box, output, 1 - alpha_box, 0, output)\n    cv2.rectangle(output, (box[0], box[1]), (box[2], box[3]),\n                    color, thickness)\n    cv2.putText(output, label.upper(), (box[0], box[1]-5),\n            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n    return output","838f000b":"from random import randint","f1edffe7":"image_id = \"d3637a1935a905b3c326af31389cb846\"","c5d1072f":"\n\nlabel2Color = {i:(randint(1, 255), randint(1, 255), randint(1, 255)) for i in df.class_name.unique().tolist()}\n\n","c72a89a7":"label2Color","50449807":"df[df.image_id==image_id]\n","81f9a142":"# draw_bbox(image, box, label, color)\n# image_id = \"d3637a1935a905b3c326af31389cb846\"\ndef getBboxOnImage(image_id):\n    img = cv2.imread(f\"..\/input\/vinbigdata-competition-jpg-data-3x-downsampled\/train\/train\/{image_id}.jpg\")\n    for i, row in df[df.image_id==\"d3637a1935a905b3c326af31389cb846\"].iterrows():\n        bbox = row[['x_min', 'y_min', 'x_max', 'y_max']].astype(int)\n        _class = row['class_id']\n        _class_name = row['class_name']\n        img = draw_bbox(img, bbox, _class_name, label2Color[_class_name])\n    return img","8bf45c06":"def runBoxFusion(image_id):\n    df_image_id = df[df.image_id==image_id]\n    disease_image_id = df[df.image_id==image_id]['class_name'].unique().tolist()\n    img_cp = cv2.imread(f\"..\/input\/vinbigdata-competition-jpg-data-3x-downsampled\/train\/train\/{image_id}.jpg\")\n    for d in disease_image_id:\n        height, width, _ = img.shape\n        rows = df_image_id[df_image_id.class_name==d][['x_min', 'y_min', 'x_max', 'y_max']]\n        bboxes = rows.to_numpy()\n        bboxes = bboxes\/(height, width, height, width)\n        labels = np.ones(bboxes.shape[0])\n        scores = np.ones(bboxes.shape[0])\n        boxes, scores, labels = weighted_boxes_fusion([bboxes], [scores], [labels], iou_thr=0.4, skip_box_thr=0.4)\n        boxes = boxes*(height, width, height, width)\n        for bbox in boxes:\n            img_cp = draw_bbox(img_cp, bbox.astype(int), d, label2Color[d])\n    return img_cp\n","a56eefd2":"def imshow(path, bbox):\n\n    img = cv2.imread(path)\n\n    bbox1 = bbox[:, :4].astype(int)\n\n    min_point = [tuple(x) for x in bbox1[:, :2].tolist()]\n\n    max_point = [tuple(x) for x in bbox1[:, 2:4].tolist()]\n\n    for i in range(len(bbox)):\n        cv2.rectangle(img, min_point[i], max_point[i], (0, 255, 0), 2)\n    plt.figure(figsize= (12, 10))\n    plt.imshow(img)\n\n    plt.show()","4dd822f2":"\ndef norm(bbox1, bbox2):\n    x1min, y1min, x1max, y1max = bbox1[0], bbox1[1], bbox1[2], bbox1[3]\n    x2min, y2min, x2max, y2max = bbox2[0], bbox2[1], bbox2[2], bbox2[3]\n\n    lx = max(x1min, x1max, x2min, x2max)-min(x1min, x1max, x2min, x2max)\n    ly = max(y1min, y1max, y2min, y2max)-min(y1min, y1max, y2min, y2max)\n\n    xmin = min(x1min, x1max, x2min, x2max)\n    ymin = min(y1min, y1max, y2min, y2max)\n\n    x1min = (x1min-xmin)\/lx\n    y1min = (y1min-ymin)\/ly\n    x2min = (x2min-xmin)\/lx\n    y2min = (y2min-ymin)\/ly\n    x1max = (x1max-xmin)\/lx\n    y1max = (y1max-ymin)\/ly\n    x2max = (x2max-xmin)\/lx\n    y2max = (y2max-ymin)\/ly\n\n    p = np.absolute(x1min-x2min)+np.absolute(x1max-x2max) + \\\n        np.absolute(y1min-y2min)+np.absolute(y1max-y2max)\n\n    return p","7141f5fa":"def confluence(bbox,threshold):\n    \n   \n    print(30*'-')\n    \n    bbox_num=[i for i in range(len(bbox))]#Number each bbox\n    \n    keep = []\n\n    while len(bbox_num) > 0:\n        \n        score_min = []#Store the minimum WP of each bbox in each round\n        loc_set=[]#Store the bbox number where P<threshold of each bbox\n        \n        for i in bbox_num:\n            score = []#Store all WPs of bbox[i], and then select the smallest\n            loc=[]#Store the bbox number of P<threshold\n            for j in bbox_num:\n                P = norm(bbox[i], bbox[j])\n                if P<threshold:\n                    loc.append(j)\n                print(f\"P = {P}\")\n                if P < 2 and P != 0:\n                    WP = P\/bbox[i][4]\n                    score.append(WP)\n                    \n            if len(score):       \n                score_min.append(round(min(score), 3))\n                loc_set.append(loc)\n                \n        \n        if len(score_min)==0:\n            return bbox\n        print('Minimum WP for each bbox:', score_min)\n        print('P<treshold of each bbox Another bbox number:',loc_set)\n       \n        \n        index = score_min.index(min(score_min))\n        bbox_index=bbox_num[index]\n        print('Lowest score bbox number:',bbox_index)\n        \n        keep.append(bbox_index)\n        print('Final bbox collection:',keep)\n        print('Bbox number to delete',loc_set[index])\n        \n        for i in loc_set[index]:\n            bbox_num.remove(i)\n            \n        \n        print('The remaining bbox:',bbox_num)\n        \n        \n        \n        if len(bbox_num)==1:\n            keep.append(bbox_num[0])\n            print('1 left, merged into the final bbox',keep)\n            break\n        \n        print(30*'-')\n   \n    bbox_process = bbox[keep]\n    \n    return bbox_process","2de28f1b":"def getConfluencedBBox(image_id):\n    labels_per_box = []\n    df_image_id = df[df.image_id==image_id]\n    confluenced_bbox = []\n    disease_image_id = df[df.image_id==image_id]['class_name'].unique().tolist()\n    path = f\"..\/input\/vinbigdata-competition-jpg-data-3x-downsampled\/train\/train\/{image_id}.jpg\"\n    img_cp = cv2.imread(f\"..\/input\/vinbigdata-competition-jpg-data-3x-downsampled\/train\/train\/{image_id}.jpg\")\n    for d in disease_image_id:\n        height, width, _ = img.shape\n        rows = df_image_id[df_image_id.class_name==d][['x_min', 'y_min', 'x_max', 'y_max']]\n        bboxes = rows.to_numpy()\n        # add confidence to each bbox\n        bboxes = np.insert(bboxes, 4, 1, axis=1)\n        if bboxes.shape[0]:\n            confluenced_bbox.append(confluence(bboxes, 0.8))\n            labels_per_box.append([d]*len(bboxes))\n    confluenced_bbox = np.concatenate(confluenced_bbox)\n    labels_per_box = np.concatenate(np.array(labels_per_box))\n    return confluenced_bbox, labels_per_box","f3dcb4f8":"def getBboxOnImageViaConfluence(image_id):\n    confluenced_bbox, labels_per_box = getConfluencedBBox(image_id)\n    img_cp = cv2.imread(f\"..\/input\/vinbigdata-competition-jpg-data-3x-downsampled\/train\/train\/{image_id}.jpg\")\n    for bbox, d in zip(confluenced_bbox, labels_per_box):\n        img_cp = draw_bbox(img_cp, bbox.astype(int), d, label2Color[d])\n    return img_cp","a313ac5b":"def plotComparisonImages(img, img_wbf, img_confluenced):\n    plt.figure(figsize = (22, 20))\n    ax = plt.subplot(1,3,1)\n    ax.set_title(\"Diseases Marked by All Radiologist\")\n    plt.imshow(img)\n    ax = plt.subplot(1,3,2)\n    ax.set_title(\"Fused BBox - WBF\")\n    plt.imshow(img_wbf)\n    ax = plt.subplot(1,3, 3)\n    ax.set_title(\"Box Fusion - Confluence\")\n    plt.imshow(img_confluenced)\n    plt.show()","587e41b4":"image_id = \"d3637a1935a905b3c326af31389cb846\"\nimg = getBboxOnImage(image_id)\nimg_wbf = runBoxFusion(image_id)\nimg_confluenced = getBboxOnImageViaConfluence(image_id)\nplotComparisonImages(img, img_wbf, img_confluenced)\n\n","57028b3e":"image_id = \"afb6230703512afc370f236e8fe98806\"\nimg = getBboxOnImage(image_id)\nimg_wbf = runBoxFusion(image_id)\nimg_confluenced = getBboxOnImageViaConfluence(image_id)\nplotComparisonImages(img, img_wbf, img_confluenced)\n\n","98a68fd7":"image_id = \"7c1add6833d5f0102b0d3619a1682a64\"\nimg = getBboxOnImage(image_id)\nimg_wbf = runBoxFusion(image_id)\nimg_confluenced = getBboxOnImageViaConfluence(image_id)\nplotComparisonImages(img, img_wbf, img_confluenced)\n\n","8a7d190b":"P expanded figure.png![image.png](attachment:image.png)","cec16353":"The paper link is: https:\/\/arxiv.org\/pdf\/2012.00257.pdf","ddd4b4ac":"# Confluence Code","ca5068c5":"- A small value of P would denote highly confluent boxes while the high value of P would indicate that the boxes are not attributable to same objects.\n- Thus a bounding box surrounded by dense cluster of boxes will be characterized by low values of P and in effect provides a object detector's confidence in presence of the object.\n- the bounding box b with the lowest intra-cluster P values represents the most confident detection for a given object.\n- Normalization is applied before finding the P value and the bbox can be of different sizes. Normalization allows the difference between the intraobject(same object represented by slightly misplaced objects) and interobjects(two different objects) to be distinguised.\n- overcomes an issue faced by NMS and its alternatives - in situations where the highest scoring bounding box is sub-optimal in comparison with another lower scoring bounding box, NMS returns the sub-optimal bounding box, as illustrated by confluence_1.png. In contrast, the P measure allows for the bounding box which is most confluent with all other bounding boxes assigned a given object to be favoured, making it more robust.","a788732b":"final P value is calculated:","ec873986":"A proximity score is calculated, P. Each box is compared to every other boxes and the proximity scores is used to filter the confluent boxes.","7eadbdd2":"A graphical view of how proximity score is calculated.","b6d4032c":"# Sample 3","a8c58bed":"# Now comparison it with wbf ","39051128":"# Reference: https:\/\/github.com\/lmk123568\/OD_Confluence\nThe code for confluence has been taken from this repo","5dbfe6f7":"inter and intra object p values.png![image.png](attachment:image.png)","f111d2a0":"Weighted P by Confidence.png![image.png](attachment:image.png)","5e1fa819":"# Weighted Box Fusion","2adf74ee":"proximity equ.png![image.png](attachment:image.png)","b4cac856":"The aim of this notebook is to demostrate the working of confluence, a new way for merging bboxes in object detection. Notes have been added from the paper to make it more clear.  ","5fc6f881":"confluence_1.png![image.png](attachment:image.png)","a34a44a0":"# How does this algorithm work?\n- Uses manhattan distance to find the closest boxes and removes the highly confluent bounding boxes.\n- mAP improvement of 0.3-0.5% and recall improvement of 1.4-2.5%\n- Confluence outperforms Greedy NMS in both mAP and recall, using the challenging 0.50:0.95 mAP evaluation metric. Superior non-IoU alternative to Greedy NMS which does not rely on IoU or the maxima confidence score in bounding box retention and suppression.","3e2a7dfd":"mh equ.png![image.png](attachment:image.png)","c5f71f4f":"# Sample 1","13135bc7":"# Plotting func","7f24fc89":"# Sample 2","aef498d0":"adding confidences to each bbox at last = [xmin, ymin, xmax, ymax, confidence]"}}