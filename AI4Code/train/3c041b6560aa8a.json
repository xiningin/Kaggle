{"cell_type":{"0aa0c933":"code","36a135c2":"code","cc4249c9":"code","88412d37":"code","9f3aff0b":"code","a2f90f84":"code","0bcc60fd":"code","41297c45":"code","fbc27601":"code","12d310b4":"code","e7fba870":"code","c74d7b3e":"code","f2dfc56f":"code","66c47067":"code","4d9cc214":"code","e0f92f5d":"code","66f06c65":"code","11c6426a":"code","cc7f4a5f":"code","a41451d6":"code","4b4ccd05":"code","6699c912":"code","80024149":"code","048e057e":"code","0b8e8e65":"code","0383adfa":"code","ddaff3e4":"code","6dcf1130":"code","573a2410":"code","b1aaf82c":"code","e4eb0e0d":"code","e83806a2":"code","c33cdbff":"code","272c8187":"code","59441774":"code","1d9c9892":"code","ac575baa":"code","4d99c0db":"code","40e3ebdd":"code","52eae2aa":"code","e0dee798":"code","aadee459":"code","5aa18b4e":"code","7af03458":"code","b625cd86":"code","3ddea212":"code","9ce42db2":"code","670f7df6":"code","7dd9586b":"code","f2c4c18b":"code","ba3d0e8c":"code","d6fd8baf":"code","f94d07ec":"code","14f4873e":"code","23b07906":"code","2ab3c056":"code","13f2f9af":"code","d93bd69e":"markdown","1f87e8b5":"markdown","edeb8438":"markdown","f027ac70":"markdown","b3a938f8":"markdown","4df276e6":"markdown","10c5c29a":"markdown","4e818ab3":"markdown","3c3330e4":"markdown","425e1654":"markdown","ec90a477":"markdown","758c3cee":"markdown"},"source":{"0aa0c933":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","36a135c2":"#Read the train dataset\ndf = pd.read_csv('\/kaggle\/input\/mobile-price-classification\/train.csv')","cc4249c9":"#first few rows\ndf.head()","88412d37":"#length of dataset\nlen(df)","9f3aff0b":"#check for null values\ndf.isnull().sum()","a2f90f84":"#confirm with heatmap\nsns.heatmap(df.isnull(), yticklabels = False, cbar = False, cmap = 'viridis')","0bcc60fd":"#check the datatypes\ndf.dtypes","41297c45":"#report\ndf.describe().T","fbc27601":"sns.catplot(x = 'blue', data = df, kind = 'count')","12d310b4":"sns.catplot(x = 'four_g', data = df, kind = 'count')","e7fba870":"sns.catplot(x = 'three_g', data = df, kind = 'count')","c74d7b3e":"sns.catplot(x = 'touch_screen', data = df, kind = 'count')","f2dfc56f":"sns.catplot(x = 'price_range', data = df, kind = 'count')","66c47067":"#number of 3G phones with respect to number of cores\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (10,6))\nsns.countplot(x = \"n_cores\", hue = 'three_g', data = df)\nplt.title(\"3G with respect to number of cores\")\nplt.show()","4d9cc214":"#number of 4G phones with respect to number of cores\nplt.figure(figsize = (10,6))\nsns.countplot(x = \"n_cores\", hue = 'four_g', data = df)\nplt.title(\"4G with respect to number of cores\")\nplt.show()","e0f92f5d":"#number of touch screen phones with respect to price range\nplt.figure(figsize = (10,6))\nsns.countplot(x = \"price_range\", hue = 'touch_screen', data = df)\nplt.title(\"Count of touch screen phones for each price range\")\nplt.show()","66f06c65":"#number of dual sim phones for each price range\nplt.figure(figsize = (10,6))\nsns.countplot(x = \"price_range\", hue = 'dual_sim', data = df)\nplt.title(\"Count of dual sim phones for each price range\")\nplt.show()","11c6426a":"#Choose price range as the desired label. Check correlation of the table.\ndf.corr()['price_range'].sort_values()","cc7f4a5f":"#heatmap for correlation\nf, ax = plt.subplots(figsize = (20,12))\nsns.heatmap(df.corr(), vmax = 0.8, square = True)","a41451d6":"#Divide the dataset into X and y\nX = df.iloc[:,:-1].values\ny = df.iloc[:,-1].values","4b4ccd05":"#scale the value of X\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit_transform(X)","6699c912":"#split into train and test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 0)","80024149":"#Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0, multi_class = 'auto', solver = 'lbfgs')\nclassifier.fit(X_train, y_train)","048e057e":"#predicting the values\ny_pred = classifier.predict(X_test)","0b8e8e65":"#confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_Log = confusion_matrix(y_test, y_pred)","0383adfa":"cm_Log","ddaff3e4":"#Accuracy and report of the classifier\nfrom sklearn.metrics import accuracy_score, classification_report\naccuracy_score(y_test,y_pred)","6dcf1130":"#report\nreport_Log = classification_report(y_test, y_pred)\nprint(report_Log)","573a2410":"#KNNeighbours\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 10, metric = 'minkowski', p =2)\nclassifier.fit(X_train, y_train)","b1aaf82c":"#predicting the values\ny_pred = classifier.predict(X_test)","e4eb0e0d":"#confusion matrix\ncm_KNN = confusion_matrix(y_test, y_pred)\nprint(cm_KNN)","e83806a2":"#Accuracy\naccuracy_score(y_test,y_pred)","c33cdbff":"#Report\nreport_KNN = classification_report(y_test, y_pred)\nprint(report_KNN)","272c8187":"#SVM\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'linear', random_state = 0, C = 100)\nclassifier.fit(X_train,y_train)","59441774":"#predicting the values\ny_pred = classifier.predict(X_test)","1d9c9892":"#confusion matrix\ncm_SVC = confusion_matrix(y_test, y_pred)\nprint(cm_SVC)","ac575baa":"#Accuracy\naccuracy_score(y_test,y_pred)","4d99c0db":"#Report\nreport_SVC = classification_report(y_test, y_pred)\nprint(report_SVC)","40e3ebdd":"#NaiveBayes\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train,y_train)","52eae2aa":"#predicting the values\ny_pred = classifier.predict(X_test)","e0dee798":"#confusion matrix\ncm_NB = confusion_matrix(y_test, y_pred)\nprint(cm_NB)","aadee459":"#Accuracy\naccuracy_score(y_test,y_pred)","5aa18b4e":"#Report\nreport_NB = classification_report(y_test, y_pred)\nprint(report_NB)","7af03458":"#Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train,y_train)","b625cd86":"#predicting the values\ny_pred = classifier.predict(X_test)","3ddea212":"#confusion matrix\ncm_RF = confusion_matrix(y_test, y_pred)\nprint(cm_RF)","9ce42db2":"#Accuracy\naccuracy_score(y_test,y_pred)","670f7df6":"#Report\nreport_RF = classification_report(y_test, y_pred)\nprint(report_RF)","7dd9586b":"#Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nclassifier = GradientBoostingClassifier()\nclassifier.fit(X_train,y_train)","f2c4c18b":"#predicting the values\ny_pred = classifier.predict(X_test)","ba3d0e8c":"#confusion matrix\ncm_GBC = confusion_matrix(y_test, y_pred)\nprint(cm_GBC)","d6fd8baf":"#Accuracy\naccuracy_score(y_test,y_pred)","f94d07ec":"#Report\nreport_GBC = classification_report(y_test, y_pred)\nprint(report_GBC)","14f4873e":"#Select SVM for classification\nTest = pd.read_csv('\/kaggle\/input\/mobile-price-classification\/test.csv')\ndf_test = Test.drop(['id'], axis = 1)\ndf_test = sc.fit_transform(df_test)","23b07906":"#classify\ny_Test_values = classifier.predict(df_test)","2ab3c056":"#join the predicted values to the test dataset\nTest['price_range'] = y_Test_values","13f2f9af":"#display Test \nTest.head()","d93bd69e":"**RANDOM FOREST CLASSIFIER**","1f87e8b5":"**GRADIENT BOOSTING CLASSIFIER**","edeb8438":"**I hope this notebook was helpful :)**\n\n*** Classification DONE!***\n","f027ac70":"From the above heatmap it can be seen that the feature \"ram\" has the highest correlation with price_range.","b3a938f8":"**SVM CLASSIFICATION**","4df276e6":"**LOGISTIC CLASSIFICATION**","10c5c29a":"Considering **price_range** as the label and the remaining as features. We have **four classes** as seen above, that is 0 1 2 and 3.\n\nLets begin creating a **classification model**, each models accuracy will be checked to determine the best model for classification.\n1. Logistic Regression Classifier\n1. KNN\n1. SVM\n1. Naive Bayes\n1. Random Forest Classifier\n1. Gradient Boosting Classifier","4e818ab3":"![TbFEdJb6E7cJ5EonZB9eSS-1024-80.jpg](attachment:TbFEdJb6E7cJ5EonZB9eSS-1024-80.jpg)\n\nThis notebook concentrates in understanding the train data that is provided and training diffrent classification models in order to get the model with the best accuracy.\nOnce the model with the best accuracy is obtained the model is used to classify the data in the test.csv file and then appended to the file for further use.\n\nSo, lets begin!","3c3330e4":"From the above models it can be notices that their accuracy is as follows,\n\n* Logistic Regression Classifier - 95.5%\n* KNN Classifier - 56.75%\n* SVM Classifier - 95.75%\n* Naive Bayes Classifier - 83.5%\n* Random Forest Classifier - 86.5%\n* Gradient Boost Classifier - 91.25%\n\nThus we can choose the **SVM Classifier** to classify the data in the test set provided as it has the highest accuracy among the models that we compared.","425e1654":"**NAIVE BAYES CLASSIFIER**","ec90a477":"**This gives us the new dataset with the added price_range column based on the SVM Classifier.**","758c3cee":"**KNN CLASSIFIER**"}}