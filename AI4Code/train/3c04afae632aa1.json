{"cell_type":{"e8b0bde9":"code","99eed8bb":"code","e1ec3e69":"code","68d9cf6d":"code","402ba31e":"code","7bb3234c":"code","6cdbccf5":"code","e2337f10":"code","8fba9e35":"code","7a6cb7e5":"code","81b8fcf0":"code","4ab39d7d":"code","c96c0728":"code","2c049cc3":"code","c8ffcc0c":"code","0ccd5435":"code","e908395d":"code","f5677ee7":"code","8374d441":"code","bbf680cc":"code","86375288":"code","be5244ab":"code","cfbc66d5":"code","05d79558":"code","d0b14f08":"code","dd249c84":"markdown","2d176b23":"markdown","bec264a0":"markdown","7ca3c69d":"markdown","2c9f6c74":"markdown","fead9ce2":"markdown","ea78b723":"markdown","b3e96e7f":"markdown","5fc4eec6":"markdown","d30559b0":"markdown","2e0cadb8":"markdown","be089977":"markdown","d4749bf6":"markdown","e3303665":"markdown","6392202e":"markdown","a86b5e08":"markdown"},"source":{"e8b0bde9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction import DictVectorizer as DV\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn import model_selection\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom catboost import CatBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import linear_model\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn import tree\nfrom sklearn import svm\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom lightgbm import LGBMClassifier\nfrom mlxtend.classifier import StackingCVClassifier\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nimport scikitplot as skplt\nfrom scikitplot.plotters import plot_learning_curve\nfrom mlxtend.plotting import plot_learning_curves\nfrom sklearn.model_selection import learning_curve\nfrom xgboost import XGBClassifier","99eed8bb":"# GradientBoostingClassifier\ncv_method = StratifiedKFold(n_splits=3, random_state=42)\n\nparams_GBC = {\"loss\": [\"deviance\"],\n              \"learning_rate\": [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1], \n              \"n_estimators\": [200],\n              \"max_depth\": [3, 5, 8],\n              \"min_samples_split\": np.linspace(0.1, 1.0, 10, endpoint=True),\n              \"min_samples_leaf\": np.linspace(0.1, 0.5, 5, endpoint=True),\n              \"max_features\": [\"log2\",\"sqrt\"],\n              \"criterion\": [\"friedman_mse\", \"mae\"]\n              }\n\nGridSearchCV_GBC = GridSearchCV(estimator=GradientBoostingClassifier(), \n                                param_grid=params_GBC, \n                                cv=cv_method,\n                                verbose=1, \n                                n_jobs=2,\n                                scoring=\"accuracy\", \n                                return_train_score=True\n                                )\n\nparams_XGBoost = {\"learning_rate\": [0.01, 0.02, 0.05, 0.1], \n              \"max_depth\": [1, 3, 5],\n              \"min_child_weight\": [1, 3, 5],\n              \"subsample\": [0.6, 0.7, 1],\n              \"colsample_bytree\": [1]\n              }\n\nGridSearchCV_XGBoost = GridSearchCV(estimator=XGBClassifier(), \n                                param_grid=params_XGBoost, \n                                cv=cv_method,\n                                verbose=1, \n                                n_jobs=2,\n                                scoring=\"accuracy\", \n                                return_train_score=True\n                                )\n\nparams_adaboost = {\"learning_rate\": [0.001, 0.01, 0.5, 0.1], \n              \"n_estimators\": [750],\n              \"algorithm\": [\"SAMME.R\"]\n              }\nGridSearchCV_adaboost = GridSearchCV(estimator=AdaBoostClassifier(), \n                                param_grid=params_adaboost, \n                                cv=cv_method,\n                                verbose=1, \n                                n_jobs=2,\n                                scoring=\"accuracy\", \n                                return_train_score=True\n                                )\n\nparams_LR = {\"tol\": [0.0001,0.0002,0.0003],\n            \"C\": [0.01, 0.1, 1, 10, 100],\n            \"intercept_scaling\": [1, 2, 3, 4]\n              }\n\nGridSearchCV_LR = GridSearchCV(estimator=linear_model.LogisticRegression(), \n                                param_grid=params_LR, \n                                cv=cv_method,\n                                verbose=1, \n                                n_jobs=2,\n                                scoring=\"accuracy\", \n                                return_train_score=True\n                                )\n\nparams_SGD = {\"alpha\": [0.001, 0.005, 0.01, 0.05, 0.1, 1], \n             \"max_iter\": [1, 3, 5, 7], \n             \"tol\": [0.0001,0.0002,0.0003]\n              }\n\nGridSearchCV_SGD = GridSearchCV(estimator=linear_model.SGDClassifier(), \n                                param_grid=params_SGD, \n                                cv=cv_method,\n                                verbose=1, \n                                n_jobs=2,\n                                scoring=\"accuracy\", \n                                return_train_score=True\n                                )\n\n\nparams_RF = {\"min_samples_split\": [2, 6, 20],\n              \"min_samples_leaf\": [1, 4, 16],\n              \"n_estimators\" :[100,200,300,400],\n              \"criterion\": [\"gini\"]             \n              }\n\nGridSearchCV_RF = GridSearchCV(estimator=RandomForestClassifier(), \n                                param_grid=params_RF, \n                                cv=cv_method,\n                                verbose=1, \n                                n_jobs=2,\n                                scoring=\"accuracy\", \n                                return_train_score=True\n                                )\n\nparams_SVC = {\"C\": [0.1, 1, 10, 100, 1000],  \n              \"gamma\": [1, 0.1, 0.01, 0.001, 0.0001], \n              \"kernel\": [\"rbf\"]\n              }\n\nGridSearchCV_SVC = GridSearchCV(estimator=SVC(), \n                                param_grid=params_SVC, \n                                cv=cv_method,\n                                verbose=1, \n                                n_jobs=-1,\n                                refit = True,\n                                scoring=\"accuracy\", \n                                return_train_score=True\n                                )\n\nparams_DT = {\"criterion\": [\"gini\", \"entropy\"],\n             \"max_depth\": [1, 2, 3, 4, 5, 6, 7, 8],\n             \"min_samples_split\": [2, 3]}\n\nGridSearchCV_DT = GridSearchCV(estimator=DecisionTreeClassifier(), \n                                param_grid=params_DT, \n                                cv=cv_method,\n                                verbose=1, \n                                n_jobs=-1,\n                                scoring=\"accuracy\", \n                                return_train_score=True\n                                )\n\nparams_GNB = {\"C\": [0.1,0.25,0.5,1],\n              \"gamma\": [0.1,0.5,0.8,1.0],\n              \"kernel\": [\"rbf\",\"linear\"]}\n\n\nGridSearchCV_GNB = GridSearchCV(estimator=svm.SVC(), \n                                param_grid=params_GNB, \n                                cv=cv_method,\n                                verbose=1, \n                                n_jobs=-1,\n                                scoring=\"accuracy\", \n                                return_train_score=True\n                                )\n\n# Plot learning curve\ndef plot_learning_curve(estimator, title, x, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n        \n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, x, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"#80CBC4\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"#00897B\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ngbc_mod = 0\nXGBoost_classifier_mod = 0\nadaboost_mod = 0\nlogistic_regression_mod = 0\nsgd_mod = 0\nrandom_forest_mod = 0\nsvc_mod = 0\ndecision_tree_mod = 0\ngnb_mod = 0\n\n\nmodel  = 0\n\ndef AlgoTrain (X_train, Y_train, X_test, Y_test, Algo):\n    if (Algo ==  'GBC'):\n        print(\"\\n----------------------Gradient Boosting----------------------\\n\")\n        GridSearchCV_GBC.fit(X_train, Y_train)\n        model = GridSearchCV_GBC\n    \n    elif (Algo ==  'XGB'):\n        print(\"\\n----------------------XGBoost----------------------\\n\")\n        GridSearchCV_XGBoost.fit(X_train, Y_train)\n        model = GridSearchCV_XGBoost\n    \n    elif (Algo ==  'ADA'):\n        print(\"\\n----------------------ADA Boost----------------------\\n\")\n        GridSearchCV_adaboost.fit(X_train, Y_train)\n        model = GridSearchCV_adaboost\n    \n    elif (Algo ==  'LOG'):\n        print(\"\\n----------------------Logistic Regression----------------------\\n\")\n        GridSearchCV_LR.fit(X_train, Y_train)\n        model = GridSearchCV_LR\n        \n    elif (Algo ==  'SGD'):\n        print(\"\\n----------------------Stochastic Gradient----------------------\\n\")\n        GridSearchCV_SGD.fit(X_train, Y_train)\n        model = GridSearchCV_SGD\n        \n    elif (Algo ==  'RFC'):\n        print(\"\\n----------------------Random Forest----------------------\\n\")\n        GridSearchCV_RF.fit(X_train, Y_train)\n        model = GridSearchCV_RF\n        \n    elif (Algo ==  'SVC'):\n        print(\"\\n----------------------SVC----------------------\\n\")\n        GridSearchCV_SVC.fit(X_train, Y_train)\n        model = GridSearchCV_SVC\n    \n    elif (Algo ==  'DTC'):\n        print(\"\\n----------------------Decision Tree----------------------\\n\")\n        GridSearchCV_DT.fit(X_train, Y_train)\n        model = GridSearchCV_DT\n\n    elif (Algo ==  'GNB'):\n        print(\"\\n----------------------Naive Bayes----------------------\\n\")\n        GridSearchCV_GNB.fit(X_train, Y_train)\n        model = GridSearchCV_GNB\n        \n    else:\n        print(\"\\n Wrong Algo\")\n    \n    best_estimator = model.best_estimator_\n    print(f\"Best estimator values for model:\\n{best_estimator}\")\n    best_params = model.best_params_\n    print(f\"Best parameter values for model:\\n{best_params}\")\n    best_score = model.best_score_\n    print(f\"Best score value for model: {round(best_score, 3)}\")\n  \n    return best_estimator, best_params, best_score\n\n\ndef AlgoTrainWIP (X_train, Y_train, X_test, Y_test, Algo):\n    if (Algo ==  'GBC'):\n        GridSearchCV_GBC.fit(X_train, Y_train)\n        model = GridSearchCV_GBC\n        gbc = GradientBoostingClassifier(**model.best_params_)\n        gbc_mod = gbc.fit(X_train, Y_train)\n        Y_predicted = gbc_mod.predict(X_test)\n        score_train = gbc_mod.score(X_train, Y_train)\n        score_test = gbc_mod.score(X_test, Y_test)\n    \n    elif (Algo ==  'XGB'):\n        GridSearchCV_XGBoost.fit(X_train, Y_train)\n        model = GridSearchCV_XGBoost\n        XGBoost_classifier = XGBClassifier(**model.best_params_)\n        XGBoost_classifier_mod = XGBoost_classifier.fit(X_train, Y_train)\n        Y_predicted = XGBoost_classifier_mod.predict(X_test)\n        score_train = XGBoost_classifier_mod.score(X_train, Y_train)\n        score_test = XGBoost_classifier_mod.score(X_test, Y_test)          \n    \n    elif (Algo ==  'ADA'):\n        GridSearchCV_adaboost.fit(X_train, Y_train)\n        model = GridSearchCV_adaboost\n        adaboost = AdaBoostClassifier(**model.best_params_)\n        adaboost_mod = adaboost.fit(X_train, Y_train)\n        Y_predicted = adaboost_mod.predict(X_test)\n        score_train = adaboost_mod.score(X_train, Y_train)\n        score_test = adaboost_mod.score(X_test, Y_test)           \n    \n    elif (Algo ==  'LOG'):\n        GridSearchCV_LR.fit(X_train, Y_train)\n        model = GridSearchCV_LR\n        logistic_regression = linear_model.LogisticRegression(**model.best_params_)\n        logistic_regression_mod = logistic_regression.fit(X_train, Y_train)\n        Y_predicted = logistic_regression_mod.predict(X_test)\n        score_train = logistic_regression_mod.score(X_train, Y_train)\n        score_test = logistic_regression_mod.score(X_test, Y_test)\n        \n    elif (Algo ==  'SGD'):\n        GridSearchCV_SGD.fit(X_train, Y_train)\n        model = GridSearchCV_SGD\n        sgd = linear_model.SGDClassifier(**model.best_params_)\n        sgd_mod = sgd.fit(X_train, Y_train)\n        Y_predicted = sgd_mod.predict(X_test)\n        score_train = sgd_mod.score(X_train, Y_train)\n        score_test = sgd_mod.score(X_test, Y_test)          \n        \n    elif (Algo ==  'RFC'):\n        GridSearchCV_RF.fit(X_train, Y_train)\n        model = GridSearchCV_RF\n        random_forest = RandomForestClassifier(**model.best_params_)\n        random_forest_mod = random_forest.fit(X_train, Y_train)\n        Y_predicted = random_forest_mod.predict(X_test)\n        score_train = random_forest_mod.score(X_train, Y_train)\n        score_test = random_forest_mod.score(X_test, Y_test)     \n        \n        \n    elif (Algo ==  'SVC'):\n        GridSearchCV_SVC.fit(X_train, Y_train)\n        model = GridSearchCV_SVC\n        svc = SVC(**model.best_params_)\n        svc_mod = svc.fit(X_train, Y_train)\n        Y_predicted = svc_mod.predict(X_test)\n        score_train = svc_mod.score(X_train, Y_train)\n        score_test = svc_mod.score(X_test, Y_test)           \n    \n    elif (Algo ==  'DTC'):\n        GridSearchCV_DT.fit(X_train, Y_train)\n        model = GridSearchCV_DT\n        decision_tree = DecisionTreeClassifier(**model.best_params_)\n        decision_tree_mod = decision_tree.fit(X_train, Y_train)\n        Y_predicted = decision_tree_mod.predict(X_test)\n        score_train = decision_tree_mod.score(X_train, Y_train)\n        score_test = decision_tree_mod.score(X_test, Y_test)  \n\n    elif (Algo ==  'GNB'):\n        GridSearchCV_GNB.fit(X_train, Y_train)\n        model = GridSearchCV_GNB\n        gnb = GaussianNB(**model.best_params_)\n        gnb_mod = gnb.fit(X_train, Y_train)\n        Y_predicted = gnb_mod.predict(X_test)\n        score_train = gnb_mod.score(X_train, Y_train)\n        score_test = gnb_mod.score(X_test, Y_test)\n        \n    else:\n        print(\"\\n Wrong Algo\")\n    \n    \n    best_estimator = model.best_estimator_\n    print(f\"Best estimator values for model:\\n{best_estimator}\")\n    best_params = model.best_params_\n    print(f\"Best parameter values for model:\\n{best_params}\")\n    best_score = model.best_score_\n    print(f\"Best score value for model: {round(best_score, 3)}\")\n\n    plot_learning_curve(model.best_estimator_, title = \"Learning curve\", x = X_train, y = Y_train, cv = cv_method)\n    plt.show()\n    \n    print(\"Model Accuracy with best parameters\")\n    mse = mean_squared_error(Y_test, Y_predicted)\n    rmse = np.sqrt(mean_squared_error(Y_test, Y_predicted))\n    print(f\"Mean Square Error for Gradient Boosting Classifier = {round(mse, 3)}\")\n    print(f\"Root Mean Square Error for Gradient Boosting Classifier = {round(rmse, 3)}\")\n    print(f\"R^2(coefficient of determination) on training set = {round(score_train, 3)}\")\n    print(f\"R^2(coefficient of determination) on testing set = {round(score_test, 3)}\")\n    print(\"Classification Report\")\n    print(classification_report(Y_test, Y_predicted))\n    print(\"Confusion Matrix:\")\n    print(confusion_matrix(Y_test, Y_predicted))\n    ax= plt.subplot()\n    sns.heatmap(confusion_matrix(Y_test, Y_predicted), annot=True, ax = ax, cmap = \"BuGn\");\n\n    # labels, title and ticks\n    ax.set_xlabel(\"Predicted labels\");\n    ax.set_ylabel(\"True labels\"); \n    ax.set_title(\"Confusion Matrix\"); \n\n    \n    return best_params\n    \n\ndef Clean_data(Data):\n    df = pd.DataFrame(Data['Ticket'].str.split(' ',0).tolist())\n    for i in range(len(df)):  \n        if df.loc[i,2] == None:\n            if df.loc[i,1] == None:\n                if df.loc[i,0].isnumeric(): \n                    df.loc[i,0] = int(df.loc[i,0])\n                else:\n                    df.loc[i,0] = 0\n            else:\n                if df.loc[i,1].isnumeric(): \n                    df.loc[i,0] = int(df.loc[i,1])\n                else:\n                    df.loc[i,0] = 0\n        else:\n            if df.loc[i,2].isnumeric(): \n                df.loc[i,0] = int(df.loc[i,2])\n            else:\n                df.loc[i,0] = 0\n\n    Data['Ticket_num'] = df.iloc[:,0]\n    Data['Ticket_num'] = Data['Ticket_num'].astype('int32')\n    Data['Ticket_num'].replace(to_replace=0, value = np.median(Data['Ticket_num']), inplace = True)\n    \n    Data[['Last Name','FName']] = Data['Name'].str.split(', ',expand=True)\n    Data1 = Data.groupby(['Last Name']).count().sort_values(by='PassengerId',ascending=False)\n    Data1 = Data1.reset_index()\n    Data2 = Data1[Data1['PassengerId']>1]['Last Name'].to_numpy()\n\n    Data['Family'] = np.zeros(Data.shape[0])\n    Data.loc[Data['Last Name'].isin(Data2),['Family']] = 1\n\n    Data['Family_Size'] = Data['Parch'] + Data['SibSp'] + 1 # Sibling + Parents + me\n    Data['Alone'] = np.zeros(Data.shape[0])\n    Data.loc[Data['Family_Size']>1,['Alone']] = 1\n\n    bin_labels_5 = ['Bronze', 'Silver', 'Gold', 'Platinum', 'Diamond']\n    bin_labels_4 = ['Bronze', 'Silver', 'Gold', 'Diamond']\n\n    Data['FareBin'] = pd.qcut(Data['Fare'], q=4, labels=bin_labels_4)\n    Data['Age'] = Data['Age'].fillna(Data['Age'].mean())\n    Data['AgeBin'] = pd.cut(Data['Age'].astype(int),5,labels=bin_labels_5)\n\n    Data['Title'] = Data['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n    title_names = (Data['Title'].value_counts() < 10)\n    Data['Title'] = Data['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n\n    return Data\n\ndef Data_clean_func (Data, Drop_col, Date_col, Y_column, Data_Variable_fill=True, Encoder = 'OHE'):\n\n    Data = Clean_data(Data)\n    Data = Data.drop(Drop_col, axis = 1)\n\n    for col in Date_col:\n        Data[col + '_year'] = pd.DatetimeIndex(Data[col]).year\n        Data[col + '_month'] = pd.DatetimeIndex(Data[col]).month\n        Data[col + '_day'] = pd.DatetimeIndex(Data[col]).day\n        Data.drop(col, axis=1, inplace=True)\n\n    Columns_integer_data = Data.describe().columns\n    Columns_variable_data = Data.columns[~Data.columns.isin(Data.describe().columns)]\n    Data1=Data[Columns_integer_data]\n    Data1=Data1.fillna(Data1.mean())\n\n    for col in Date_col:\n        Data1[col + '_year'] = Data1[col + '_year'].astype('int32')\n        Data1[col + '_month'] = Data1[col + '_month'].astype('int32')\n        Data1[col + '_day'] = Data1[col + '_day'].astype('int32')\n\n    if Data_Variable_fill == True:\n        for col in Columns_variable_data:\n            Data[col] = Data[col].fillna(Data[col].value_counts().index[0])\n\n    Result = pd.concat([Data1, Data[Columns_variable_data]], axis=1)\n    Result = Result.dropna()\n    Columns_integer_data = Data.describe().columns\n    Columns_variable_data = Data.columns[~Data.columns.isin(Data.describe().columns)]\n\n    if Encoder == 'Label':\n        labelencoder = LabelEncoder()\n        for col in Columns_variable_data:\n            Result[col] = labelencoder.fit_transform(Result[col])\n    elif Encoder == 'OHE':\n        Result = pd.get_dummies(Result, prefix_sep='_', drop_first=True)\n    elif Encoder == 'Dict_Vect':\n        vectorizer = DV(sparse = False)\n        Result1 = Result[Columns_variable_data].T.to_dict().values()\n        Result1 = pd.DataFrame(vectorizer.fit_transform(Result1))\n        Result = pd.concat([Result[Columns_integer_data], Result1], axis=1)\n    else:\n        print('Wrong Encoder option')\n\n\n      \n    X_column = Result.columns[~Result.columns.isin(Y_column)]\n    X_train, X_test, Y_train, Y_test  = train_test_split(Result[X_column], Result[Y_column], test_size=0.3,random_state=42)\n    \n    return X_train, X_test, Y_train, Y_test\n\n\ndef Data_clean_func_with_TrainTest (Data_train, Data_test, Drop_col, Date_col, Y_column, Data_Variable_fill=True, Encoder = 'OHE'):\n    \n    # adding column with constant value \n    Data_test[Y_column] = pd.Series([1 for x in range(Data_test.shape[0])]) \n\n\n    frames = [Data_train, Data_test]\n    Data = pd.concat(frames)\n\n    Data = Clean_data(Data)\n    Data = Data.drop(Drop_col, axis = 1)\n\n    for col in Date_col:\n        Data[col + '_year'] = pd.DatetimeIndex(Data[col]).year\n        Data[col + '_month'] = pd.DatetimeIndex(Data[col]).month\n        Data[col + '_day'] = pd.DatetimeIndex(Data[col]).day\n        Data.drop(col, axis=1, inplace=True)\n\n    Columns_integer_data = Data.describe().columns\n    Columns_variable_data = Data.columns[~Data.columns.isin(Data.describe().columns)]\n    Data1=Data[Columns_integer_data]\n    Data1=Data1.fillna(Data1.mean())\n\n    for col in Date_col:\n        Data1[col + '_year'] = Data1[col + '_year'].astype('int32')\n        Data1[col + '_month'] = Data1[col + '_month'].astype('int32')\n        Data1[col + '_day'] = Data1[col + '_day'].astype('int32')\n\n    if Data_Variable_fill == True:\n        for col in Columns_variable_data:\n            Data[col] = Data[col].fillna(Data[col].value_counts().index[0])\n\n    Result = pd.concat([Data1, Data[Columns_variable_data]], axis=1)\n    Result = Result.dropna()\n    Columns_integer_data = Data.describe().columns\n    Columns_variable_data = Data.columns[~Data.columns.isin(Data.describe().columns)]\n\n    if Encoder == 'Label':\n        labelencoder = LabelEncoder()\n        for col in Columns_variable_data:\n            Result[col] = labelencoder.fit_transform(Result[col])\n    elif Encoder == 'OHE':\n        Result = pd.get_dummies(Result, prefix_sep='_', drop_first=True)\n    elif Encoder == 'Dict_Vect':\n        vectorizer = DV(sparse = False)\n        Result1 = Result[Columns_variable_data].T.to_dict().values()\n        Result1 = pd.DataFrame(vectorizer.fit_transform(Result1))\n        Result = pd.concat([Result[Columns_integer_data], Result1], axis=1)\n    else:\n        print('Wrong Encoder option')\n\n\n    Data_train1 = Result.iloc[:Data_train.shape[0],:] \n    Data_test1 = Result.iloc[Data_train.shape[0]:,:] \n  \n    X_column = Result.columns[~Result.columns.isin(Y_column)]\n    \n    X_train = Data_train1[X_column]\n    Y_train = Data_train1[Y_column]\n    \n    X_test = Data_test1[X_column]\n    Y_test = Data_test1[Y_column]\n    \n    \n    return X_train, X_test, Y_train, Y_test","e1ec3e69":"Data = pd.read_csv('..\/input\/titanic\/train.csv')","68d9cf6d":"Data.describe()","402ba31e":"def Get_Deck(cabin):\n    return np.where(pd.notnull(cabin), str(cabin)[0].upper(), \"Z\")\n\ndef Titanic_clean(Data):\n    df = pd.DataFrame(Data['Ticket'].str.split(' ',0).tolist())\n    for i in range(len(df)):  \n        if df.loc[i,2] == None:\n            if df.loc[i,1] == None:\n                if df.loc[i,0].isnumeric(): \n                    df.loc[i,0] = int(df.loc[i,0])\n                else:\n                    df.loc[i,0] = 0\n            else:\n                if df.loc[i,1].isnumeric(): \n                    df.loc[i,0] = int(df.loc[i,1])\n                else:\n                    df.loc[i,0] = 0\n        else:\n            if df.loc[i,2].isnumeric(): \n                df.loc[i,0] = int(df.loc[i,2])\n            else:\n                df.loc[i,0] = 0\n\n    Data.loc[Data[\"Cabin\"] == \"T\", \"Cabin\"] = np.NaN\n    Data[\"Deck\"] = Data[\"Cabin\"].map(lambda x : Get_Deck(x))\n    \n    \n    Data['Ticket_num'] = df.iloc[:,0]\n    Data['Ticket_num'] = Data['Ticket_num'].astype('int32')\n    Data['Ticket_num'].replace(to_replace=0, value = np.median(Data['Ticket_num']), inplace = True)\n\n    Data[['Last Name','FName']] = Data['Name'].str.split(', ',expand=True)\n\n    #complete embarked with mode\n    Data['Embarked'].fillna(Data['Embarked'].mode()[0], inplace = True)\n\n    #complete missing fare with median\n    Data['Fare'].fillna(Data['Fare'].median(), inplace = True)\n\n\n    Data1 = Data.groupby(['Last Name']).count().sort_values(by='PassengerId',ascending=False)\n    Data1 = Data1.reset_index()\n    Data2 = Data1[Data1['PassengerId']>1]['Last Name'].to_numpy()\n\n    Data['Family'] = np.zeros(Data.shape[0])\n    Data.loc[Data['Last Name'].isin(Data2),['Family']] = 1\n\n    Data['Family_Size'] = Data['Parch'] + Data['SibSp'] + 1 # Sibling + Parents + me\n    Data['Family_Size'] = Data['Parch'] + Data['SibSp'] + 1\n    Data['Alone'] = np.zeros(Data.shape[0])\n    Data.loc[Data['Family_Size']>1,['Alone']] = 1\n\n    bin_labels_5 = ['Bronze', 'Silver', 'Gold', 'Platinum', 'Diamond']\n    bin_labels_4 = ['Bronze', 'Silver', 'Gold', 'Diamond']\n\n    Data['FareBin'] = pd.qcut(Data['Fare'], q=4, labels=bin_labels_4)\n\n    Data['Ticket_numBin'] = pd.cut(Data['Ticket_num'].astype(int),5,labels=bin_labels_5)\n\n    Data['Age'] = Data['Age'].fillna(Data['Age'].mean())\n    Data['AgeBin'] = pd.cut(Data['Age'].astype(int),5,labels=bin_labels_5)\n\n    Data['Title'] = Data['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n    title_names = (Data['Title'].value_counts() < 10)\n    Data['Title'] = Data['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n\n    return Data\n\nData  = Titanic_clean(Data)","7bb3234c":"Data.head()","6cdbccf5":"Data.describe()","e2337f10":"data1_x = ['Pclass','AgeBin','SibSp','Parch','FareBin','Ticket_numBin','Family','Family_Size','Alone']\nTarget = ['Survived']\n\nfor x in data1_x:\n    if Data[x].dtype != 'float64' :\n        print('Survival Correlation by:', x)\n        print(Data[[x, Target[0]]].groupby(x, as_index=False).mean())\n        print('-'*10, '\\n')\n        \n\n#using crosstabs: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.crosstab.html\nprint(pd.crosstab(Data['Title'],Data[Target[0]]))","8fba9e35":"print(\"\\nData[Data['Ticket_numBin'] == 'Silver']['Survived']\")\nprint(Data[Data['Ticket_numBin'] == 'Silver']['Survived'])\n\nprint(\"\\nData[Data['Ticket_numBin'] == 'Gold']['Survived']\")\nprint(Data[Data['Ticket_numBin'] == 'Gold']['Survived'])\n\nprint(\"\\nData[Data['Ticket_numBin'] == 'Platinum']['Survived']\")\nprint(Data[Data['Ticket_numBin'] == 'Platinum']['Survived'])\n\n","7a6cb7e5":"plt.figure(figsize=(20,10))\nsns.heatmap(Data.corr(), cmap=\"viridis\",annot=True,linewidths=0.1)\nplt.show()","81b8fcf0":"print (Data.info())","4ab39d7d":"Data.columns","c96c0728":"Data.head(3).T","2c049cc3":"Drop_col = ['PassengerId','Name','Ticket','Cabin','FName','Fare','Ticket_num','Age','Last Name']\nDate_col = []\nY_column = ['Survived']\n\nX_train, X_test, Y_train, Y_test = Data_clean_func(Data,Drop_col,Date_col,Y_column,True,'Label')","c8ffcc0c":"#base model\nframes = [X_train, X_test]\nX = pd.concat(frames)\n\nframes = [Y_train, Y_test]\nY = pd.concat(frames)","0ccd5435":"Data_train = pd.read_csv('..\/input\/titanic\/train.csv')\nData_test = pd.read_csv('..\/input\/titanic\/test.csv')","e908395d":"frames = [Data_train, Data_test]\nData = pd.concat(frames)\n\nData  = Titanic_clean(Data)\n\nData_train = Data.iloc[:Data_train.shape[0],:] \nData_test = Data.iloc[Data_train.shape[0]:,:] ","f5677ee7":"Drop_col = ['PassengerId','Name','Ticket','Cabin','FName','Fare','Ticket_num','Age','Last Name']\nDate_col = []\nY_column = ['Survived']\n\nX_train, X_test, Y_train, Y_test = Data_clean_func_with_TrainTest(Data_train,Data_test,Drop_col,Date_col,Y_column,True,'Label')","8374d441":"BestAlgo = pd.DataFrame()\nbest_p_array = list()\nbest_est_array = list()\n\nAlgo_Array = ['XGB', 'GBC', 'ADA', 'LOG', 'SGD','RFC','DTC','GNB','SVC']\n\ncounter = 0\n\nfor index in Algo_Array:\n    best_estimator, best_params, best_score = AlgoTrain(X_train, Y_train, X_test, Y_test,index)\n    best_p_array.append(best_params)  \n    best_est_array.append(best_estimator) \n    BestAlgo.loc[counter,['Algo']] = index\n    BestAlgo.loc[counter,['best_score']] = best_score\n    counter  = counter + 1","bbf680cc":"print (\"XGB\\n\" + str(best_p_array[BestAlgo[BestAlgo['Algo'] == 'XGB'].index[0]]))\nprint (\"GBC\\n\" + str(best_p_array[BestAlgo[BestAlgo['Algo'] == 'GBC'].index[0]]))\nprint (\"ADA\\n\" + str(best_p_array[BestAlgo[BestAlgo['Algo'] == 'ADA'].index[0]]))\nprint (\"LOG\\n\" + str(best_p_array[BestAlgo[BestAlgo['Algo'] == 'LOG'].index[0]]))\nprint (\"SGD\\n\" + str(best_p_array[BestAlgo[BestAlgo['Algo'] == 'SGD'].index[0]]))\nprint (\"RFC\\n\" + str(best_p_array[BestAlgo[BestAlgo['Algo'] == 'RFC'].index[0]]))\nprint (\"DTC\\n\" + str(best_p_array[BestAlgo[BestAlgo['Algo'] == 'DTC'].index[0]]))\nprint (\"GNB\\n\" + str(best_p_array[BestAlgo[BestAlgo['Algo'] == 'GNB'].index[0]]))\nprint (\"SVC\\n\" + str(best_p_array[BestAlgo[BestAlgo['Algo'] == 'SVC'].index[0]]))\n\n","86375288":"XGBoost_classifier = XGBClassifier(**{'colsample_bytree': 1, 'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.7})\nXGBoost_classifier_mod = XGBoost_classifier.fit(X_train, Y_train)\n\ngbc = GradientBoostingClassifier(**{'criterion': 'friedman_mse', 'learning_rate': 0.5, 'loss': 'deviance', 'max_depth': 8, 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 0.4, 'n_estimators': 200})\ngbc_mod = gbc.fit(X_train, Y_train)\n        \nadaboost = AdaBoostClassifier(**{'algorithm': 'SAMME.R', 'learning_rate': 0.01, 'n_estimators': 750})\nadaboost_mod = adaboost.fit(X_train, Y_train)\n\nlogistic_regression = linear_model.LogisticRegression(**{'C': 10, 'intercept_scaling': 1, 'tol': 0.0001})\nlogistic_regression_mod = logistic_regression.fit(X_train, Y_train)\n\nsgd = linear_model.SGDClassifier(**{'alpha': 0.005, 'max_iter': 7, 'tol': 0.0001})\nsgd_mod = sgd.fit(X_train, Y_train)\n   \nrandom_forest = RandomForestClassifier(**{'criterion': 'gini', 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 400})\nrandom_forest_mod = random_forest.fit(X_train, Y_train)\n    \ndecision_tree = DecisionTreeClassifier(**{'criterion': 'entropy', 'max_depth': 4, 'min_samples_split': 2})\ndecision_tree_mod = decision_tree.fit(X_train, Y_train)\n        \ngnb = GaussianNB()\ngnb_mod = gnb.fit(X_train, Y_train)\n\nsvc = SVC(**{'C': 100, 'gamma': 0.01, 'kernel': 'rbf'})\nsvc_mod = svc.fit(X_train, Y_train)\n","be5244ab":"results = pd.DataFrame({\n                        \"Model\": [\"Gradient Boosting Classifier\",\n                                    \"XGBoost Classifier\",\n                                    \"Adaptive Boosting\",\n                                    \"Logistic Regression\",\n                                    \"Stochastic Gradient Descent\",\n                                    \"Random Forest\",\n                                    \"Support Vector Machine\",\n                                    \"Decision Tree\",\n                                    \"Gaussian Naive Bayes\"],\n                        \"Score\": [  gbc_mod.score(X_train, Y_train),\n                                    XGBoost_classifier_mod.score(X_train, Y_train),\n                                    adaboost_mod.score(X_train, Y_train),\n                                    logistic_regression_mod.score(X_train, Y_train),\n                                    sgd_mod.score(X_train, Y_train),\n                                    random_forest_mod.score(X_train, Y_train),\n                                    svc_mod.score(X_train, Y_train),\n                                    decision_tree_mod.score(X_train, Y_train),\n                                    gnb_mod.score(X_train, Y_train)]\n                        })\nresult_df = results.sort_values(by=\"Score\", ascending=False)\nresult_df = result_df.set_index(\"Score\")\nresult_df.head(10)","cfbc66d5":"stack = StackingCVClassifier(classifiers=[random_forest, svc, gbc],\n                            meta_classifier=XGBoost_classifier,\n                            random_state=42)\n\nstack_mod = stack.fit(X_train, Y_train)\nY_pred = stack_mod.predict(X_test)\n\nprint(f\"Root Mean Square Error test for STACKING CV Classifier: {round(np.sqrt(mean_squared_error(Y_test, Y_pred)), 3)}\")","05d79558":"Y_pred  = pd.DataFrame(Y_pred)\nY_pred.columns = ['Survived']\nY_pred['PassengerId'] = Data_test['PassengerId']","d0b14f08":"submission = Y_pred\nsubmission.to_csv('submission.csv', index=False)","dd249c84":"![](https:\/\/static0.thetravelimages.com\/wordpress\/wp-content\/uploads\/2018\/11\/titanic-movie-3.jpg?q=50&fit=crop&w=960&h=500&dpr=1.5)","2d176b23":"<center><h1> Loading Data for Idetifying the right model <\/h1><\/center>","bec264a0":"<center><h1> Loading Library <\/h1><\/center>","7ca3c69d":"![](https:\/\/www.egypttoday.com\/siteimages\/Larg\/25427.jpg)","2c9f6c74":"![](https:\/\/miro.medium.com\/max\/1024\/0*KfHijq1bO1nDV5Dl.jpg)","fead9ce2":"<h1> Decision Tree Clasifier ML function turns out to be the best model. Now its time to fly <\/h1>","ea78b723":"<center><h1> Cleaning data  <\/h1><\/center>","b3e96e7f":"<center><h1> Cleaning and Splitting data for Finding the best ML model <\/h1><\/center>","5fc4eec6":"* GBC\n* XGB\n* ADA\n* LOG\n* SGD\n* RFC\n* SVC\n* DTC\n* GNB","d30559b0":"**References**\n\n* https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\n\n* https:\/\/www.kaggle.com\/pariaagharabi\/titanic-survival-scaling-tuning-stacking\n","2e0cadb8":"<center><h1> Saving the Result <\/h1><\/center>","be089977":"<center><h1> Some survived but many didnt make it. Rest in peace <\/h1><\/center>","d4749bf6":"![](https:\/\/media.vanityfair.com\/photos\/5dd6e6580ffcd40008f572d8\/7:3\/w_1994,h_854,c_limit\/titanic-production.jpg)","e3303665":"<center><h1> Fixing the Result Dataframe <\/h1><\/center>","6392202e":"<center><h1> Loading Training and Test data <\/h1><\/center>","a86b5e08":"<center><h1> All Analysis Functions <\/h1><\/center>"}}