{"cell_type":{"3b224cad":"code","0cdf349f":"code","e82e8d1f":"code","4fba44c2":"code","9f0b20ac":"code","a5f31340":"code","7740b26b":"code","29205f92":"code","cd600528":"code","2bf30b9e":"code","90f87953":"code","9687943a":"code","3acbae61":"code","e7ba5002":"code","a95c207a":"code","89d91396":"code","556c68cd":"code","f0f5f11e":"code","aad6665b":"code","c517daf5":"code","73777f64":"code","d78c7088":"code","f2187003":"code","080ca9c4":"code","e2ccb440":"code","7ba4cf53":"code","586a02c1":"code","f2e04991":"code","09d20fac":"code","0d00f1cb":"code","862f0cb7":"code","5e75ed74":"code","c5fd95b3":"code","65e070dc":"code","139926ef":"code","722c1738":"code","ac9f2615":"code","598614f8":"code","38a61324":"code","7f2f36d7":"code","0f6ac3a4":"code","3fa0fb1d":"code","c820040b":"code","dd15f5f1":"code","38a497d2":"code","270e1565":"code","fe7ac9c9":"code","7da01365":"code","a53140a2":"code","beac703d":"code","9c682024":"code","755167a1":"code","cad9b269":"code","a1d619cb":"code","24cbfcaf":"code","faad242b":"code","c530d72a":"code","f9063a81":"code","26c99d27":"code","e0ca3561":"code","a6e3d349":"code","fc6929ec":"code","496901b0":"code","fd94731b":"code","91873b1a":"code","ea146fed":"code","8df84fff":"code","9a3041bd":"code","54db094a":"code","d2f117cf":"markdown","74f59112":"markdown","8307066d":"markdown","f991fee1":"markdown","a87eed4c":"markdown","dbb5f00a":"markdown","3bb205de":"markdown","234966cf":"markdown","11b07a20":"markdown","cb0b5816":"markdown","a334e34f":"markdown","7324af32":"markdown","cdb98884":"markdown","4a271e5e":"markdown","7c50f585":"markdown","72872409":"markdown","a5655a17":"markdown","dff4ab26":"markdown","279b1362":"markdown","614c3bcc":"markdown","286ba16f":"markdown","e41b2974":"markdown","ab4d376b":"markdown","89d16531":"markdown","54504a9a":"markdown","6246417f":"markdown","ea2dd33c":"markdown","42069e16":"markdown","26c216f2":"markdown","7939c768":"markdown","752c9fcd":"markdown","140139bb":"markdown","2ff4cdd5":"markdown","fd13fad3":"markdown","7ac8468a":"markdown","ac7faecf":"markdown","61f31e9c":"markdown","d5f58f50":"markdown","3280f147":"markdown","a77c6a91":"markdown","72ed7ba9":"markdown","9e4873d1":"markdown","8aca4721":"markdown","8c966bde":"markdown","dea864aa":"markdown","063c1225":"markdown","9ea10fec":"markdown","0f3174cb":"markdown","b191cfea":"markdown","ce7f8d37":"markdown","15649147":"markdown","8284bea9":"markdown","b089f6be":"markdown","9ef1bf3d":"markdown","4fab8a56":"markdown","d4447b17":"markdown","490ab0c7":"markdown","4b4e628f":"markdown","3f6d4d5f":"markdown","1c2cfaeb":"markdown","7d8580b6":"markdown","1128ed96":"markdown","d9a61b66":"markdown","06bb8241":"markdown","c48991fc":"markdown","c7ddee33":"markdown","0911258d":"markdown","7c19d4fb":"markdown","118aea1f":"markdown","14951ef3":"markdown","41abd505":"markdown","ce66a4f5":"markdown","2db1187f":"markdown","afe231e3":"markdown","003959e1":"markdown","22350a8c":"markdown","9136932b":"markdown","ca92ff25":"markdown","677985d4":"markdown","8ae8a194":"markdown","50f6a38f":"markdown","2e0e73b1":"markdown","3ffb7e2f":"markdown","e4a13587":"markdown","c803364d":"markdown","e86d84bc":"markdown","1767c130":"markdown","d454b2c5":"markdown","94844c3b":"markdown","6c0c95b2":"markdown","200d727b":"markdown","a1e940d5":"markdown","3e052eb3":"markdown","732d59b2":"markdown"},"source":{"3b224cad":"#import some necessary librairies\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n\n\nfrom subprocess import check_output\n# print(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\")) #check the files available in the directory","0cdf349f":"# \ub370\uc774\ud130 \ubd88\ub7ec\uc624\uae30\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n","e82e8d1f":"##display the first five rows of the train dataset.\ntrain.head(5)","4fba44c2":"##display the first five rows of the test dataset.\ntest.head(5)","9f0b20ac":"# check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n# Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n# Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n# check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","a5f31340":"\nfig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()\n","7740b26b":"#Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","29205f92":"sns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","cd600528":"from sklearn.preprocessing import StandardScaler\n\ntrain_salePrice = train['SalePrice']\n\nscaler = StandardScaler()\ntrain_salePrice_std = scaler.fit_transform(train_salePrice.values.reshape(-1,1))\nprint(\"\ud45c\uc900\ud3b8\ucc28 : \" ,np.std(train_salePrice_std))\ntrain_salePrice_std\n\nsns.distplot(train_salePrice_std)","2bf30b9e":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()\n\nprint(\"\ud45c\uc900\ud3b8\ucc28 : \" ,np.std(train['SalePrice']))\n","90f87953":"ntrain = train.shape[0]\nntest = test.shape[0]\n\ny_train = train.SalePrice.values\n\nall_data = pd.concat((train, test)).reset_index(drop=True)\n\nall_data.drop(['SalePrice'], axis=1, inplace=True)\n\nprint(\"all_data size is : {}\".format(all_data.shape))","9687943a":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","3acbae61":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","e7ba5002":"# Correlation map to see how features are correlated with SalePrice\ncorrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","a95c207a":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")","89d91396":"all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")","556c68cd":"all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")","f0f5f11e":"all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")","aad6665b":"all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")","c517daf5":"# Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","73777f64":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')","d78c7088":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)","f2187003":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)","080ca9c4":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')","e2ccb440":"all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)","7ba4cf53":"all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])","586a02c1":"all_data = all_data.drop(['Utilities'], axis=1)","f2e04991":"all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")","09d20fac":"all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])","0d00f1cb":"all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])","862f0cb7":"all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","5e75ed74":"all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","c5fd95b3":"all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")","65e070dc":"#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","139926ef":"# MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n# Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n# Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","722c1738":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","ac9f2615":"# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","598614f8":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index # \uc22b\uc790\ud615 feature \ub4e4\ub9cc \uc120\ud0dd\ud568\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)\n","38a61324":"# \uac00\uc7a5 \uce58\uc6b0\uce68\uc774 \uc2ec\ud55c feature\ub97c \uc2dc\uac01\ud654\ud574\ubd04..\nsns.distplot(all_data['MiscVal'])","7f2f36d7":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)\n    \n#all_data[skewed_features] = np.log1p(all_data[skewed_features])","0f6ac3a4":"sns.distplot(all_data['MiscVal'])","3fa0fb1d":"pd.get_dummies(all_data['Electrical']).head(5)","c820040b":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","dd15f5f1":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","38a497d2":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\n\n","270e1565":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","fe7ac9c9":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","7da01365":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","a53140a2":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","beac703d":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","9c682024":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n\n","755167a1":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","cad9b269":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","a1d619cb":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","24cbfcaf":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","faad242b":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","c530d72a":"\nscore = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","f9063a81":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","26c99d27":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   # \uc785\ub825\ud55c \ubaa8\ub4e0 \ubaa8\ub378\uc758 \uc608\uce21\uac12 \ud3c9\uade0\uc73c\ub85c \uacc4\uc0b0","e0ca3561":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","a6e3d349":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","fc6929ec":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","496901b0":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","fd94731b":"stacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","91873b1a":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","ea146fed":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","8df84fff":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","9a3041bd":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15","54db094a":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)","d2f117cf":"\ubaa8\ub378\ub4e4\uc744 \uac1c\ubcc4\uc801\uc744 \uc0ac\uc6a9\ud588\ub358\uac83 \ubcf4\ub2e4 \ud720\uc52c(?) \uc88b\uc740 \uc131\ub2a5\uc744 \ub0c4!!","74f59112":"## Ensembling StackedRegressor, XGBoost and LightGBM","8307066d":"- **PoolQC** : data description says NA means \"No  Pool\". That make sense, given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general. ","f991fee1":"- **BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2** : For all these categorical basement-related features, NaN means that there is no  basement.\n","a87eed4c":"![Faron](http:\/\/i.imgur.com\/QBuDOjs.jpg)\n\n(Image taken from [Faron](https:\/\/www.kaggle.com\/getting-started\/18153#post103381))","dbb5f00a":"- **Fence** : data description says NA means \"no fence\"","3bb205de":"**\uc2e4\uc81c\ub85c\ub294 \ubc94\uc8fc\ud615(categorical) \ub370\uc774\ud130\uc778\ub370 \uc22b\uc790\ub85c \ud45c\uae30\ub418\uc5b4 \uc788\ub294 \uacbd\uc6b0 \uc774\ub97c \ubb38\uc790\uc5f4\uc73c\ub85c \ubcc0\ud658\ud568**\n\n\uc704 \ub0b4\uc6a9\uc740 data_description.txt \ub97c \ucc38\uace0\ud574\uc11c \uc218\ud589","234966cf":"\uba54\ud0c0 \ubaa8\ub378\uc744 \ucd94\uac00\ud568\uc73c\ub85c\uc368 \ub354 \uc88b\uc740 \uc131\ub2a5\uc758 \ubaa8\ub378\uc774 \ub418\uc5c8\uc74c (0.1091 -> 0.1084)","11b07a20":"### Simplest Stacking approach : Averaging base models","cb0b5816":"[Documentation][1] for the Ames Housing Data indicates that there are outliers present in the training data\n[1]: http:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/Decock\/DataDocumentation.txt","a334e34f":"\uac00\uc7a5 \ucc98\uc74c \ud560 \uc77c\uc740 \uc608\uce21\ud560 \ub370\uc774\ud130\uc778 SalePrice \ubcc0\uc218\ub97c \uc0b4\ud3b4\ubcf4\ub294 \uac83\uc774\ub2e4.","7324af32":"StandardScaler \ub294 \ub2e4\uc74c\uacfc \uac19\uc740 \uc2dd\uc744 \uc0ac\uc6a9\ud55c\ub2e4. \n\n\nz = (x - u) \/ s","cdb98884":"- **FireplaceQu** : data description says NA means \"no fireplace\"","4a271e5e":"**Submission**","7c50f585":"### Note : \n Outliers removal is note always safe.  We decided to delete these two as they are very huge and  really  bad ( extremely large areas for very low  prices). \n\nThere are probably others outliers in the training data.   However, removing all them  may affect badly our models if ever there were also  outliers  in the test data. That's why , instead of removing them all, we will just manage to make some of our  models robust on them. You can refer to  the modelling part of this notebook for that. ","72872409":"**Import librairies**","a5655a17":"**StackedRegressor:**","dff4ab26":"\uc704\uc5d0\uc11c \uc815\uc758\ud55c 4\uac1c\uc758 \ubaa8\ub378\ub4e4 **ENet, GBoost,  KRR and lasso** \ub9cc\uc744 \uac00\uc9c0\uace0 \ud3c9\uade0\uc744 \ub0b4\uc5b4 \uc0ac\uc6a9\ud574 \ubd05\uc2dc\ub2e4.\n\n\ub2f9\uc5f0\ud788 \ub2e4\ub978 \ubaa8\ub378\ub4e4\uc744 \uc27d\uac8c \ucd94\uac00\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. (models \ud30c\ub77c\uba54\ud130\ub85c \ub118\uaca8\uc8fc\uba74 \ub428)","279b1362":"- **LotFrontage** : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can **fill in missing values by the median LotFrontage of the neighborhood**.\n\nNeighborhood : \uc9c0\uc5ed \ub370\uc774\ud130\ub97c \uac00\uc9c0\uace0 \uc788\uc74c\n\n\uc989, LotFrontage\ub294 \ud55c \uc9c0\uc5ed\uc5d0\uc11c \ubaa8\ub450 \ube44\uc2b7\ud560 \uac83\uc774\ubbc0\ub85c \ud574\ub2f9 \uc9c0\uc5ed\uc758 \uc911\uc559\uac12 \uc73c\ub85c NaN \uc744 \ub300\uccb4\uac00\ub2a5!","614c3bcc":"- **BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath** : missing values are likely zero for having no basement","286ba16f":"\uc774\uc81c \ub370\uc774\ud130\uc5d0 NaN \uac12\uc774 \uc5c6\ub2e4..!","e41b2974":"# Stacked Regressions to predict House Prices \n","ab4d376b":"- **Elastic Net Regression** :\n\nElastic Net Regression \ub3c4 \uc704\uc640 \uac19\uc774 RobustScaler() \ub97c \uc0ac\uc6a9\ud574\uc11c \ubaa8\ub378\uc744 \ub9cc\ub4e4\uc790.","89d16531":"train set \uacfc test set \uc744 \ud558\ub098\uc758 \ub370\uc774\ud130\ud504\ub808\uc784\uc73c\ub85c \ubb36\uae30\n\n(\uc804\ucc98\ub9ac\ub97c \ud55c\ubc88\uc5d0 \ud558\uae30 \uc704\ud574\uc11c)","54504a9a":"- **GarageYrBlt, GarageArea and GarageCars** : Replacing missing data with 0 (Since No garage = no cars in such garage.)\n","6246417f":"Is there any remaining missing value ? ","ea2dd33c":"**XGBoost:**","42069e16":"## <font color=\"blue\"> Summary <\/font>\n\noutlier \ub97c \uc81c\uac70\ud558\ub294\uac83\uc740 \uc77c\ubc18\ud654\ub41c \ubaa8\ub378\uc744 \ub9cc\ub4dc\ub294\ub370 \ub9e4\uc6b0 \uc911\uc694\ud55c\uc77c. \n\nhttps:\/\/towardsdatascience.com\/understanding-the-68-95-99-7-rule-for-a-normal-distribution-b7b7cbf760c2\n\n\uc704 \uc815\uaddc\ubd84\ud3ec\ub97c \ucc38\uace0\ud558\uba74.. std \uac00 3 \ubcf4\ub2e4 \ud070 \ub370\uc774\ud130\ub4e4\uc744 outlier \ub85c \uaddc\uc815\ud55c\ub2e4\uba74, 0.27 % \uc758 \uadf9\ub2e8\uc801\uc778 \ub370\uc774\ud130\ub97c \uc0ad\uc81c \ud560 \uc218 \uc788\uc744\uac83.\n\n\ud558\uc9c0\ub9cc \uc5ec\uae30(\uce90\uae00)\uc5d0\uc11c\ub294 test set\uc5d0\ub3c4 outlier \uac00 \uc11e\uc5ec \uc788\uc744 \uc218 \uc788\uc73c\ubbc0\ub85c \uc608\uce21\ub960\uc744 \ub192\uc774\uae30 \uc704\ud574 outlier \ub97c \uc9c0\uc6b0\ub294\uac83\uc774 \uc544\ub2c8\ub77c outlier \uc5d0\ub3c4 \uc601\ud5a5\uc774 \uc801\uc740 \ubaa8\ub378\uc744 \ub9cc\ub4e4\uc790.","26c216f2":"- **LightGBM** :","7939c768":"\uc624\ub978\ucabd \uc544\ub798 2\uac1c\uc758 \uc774\uc0c1 \uce58\ub97c \uac80\ucd9c\ud560 \uc218 \uc788\uc5c8\ub2e4. \ub9e4\uc6b0 \ud070 \ud3c9\uc218\uc758 \uc9d1\uc774 \ub9e4\uc6b0 \uc801\uc740 \uac00\uaca9\uc5d0 \ud314\ub9b0 \uacbd\uc6b0\n\n\ud574\ub2f9 2\uac1c\uc758 \uc774\uc0c1 \uce58\ub97c \uc0ad\uc81c\ud558\uc790.","752c9fcd":"- **MSSubClass** : Na most likely means No building class. We can replace missing values with None\n","140139bb":"**Averaged base models class**","2ff4cdd5":"== \uc6d0\ud558\ub358 \uacb0\uacfc\uac00 \uc544\ub2c8\ub2e4...!\n\n\n### ---------------------------------------------","fd13fad3":"\uc704\uc5d0\uc11c \uacc4\uc0b0\ud55c Averaged model \uacfc \ube44\uad50\ud558\uae30 \uc704\ud574 \uac19\uc740 \uc218\uc758 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud574\uc11c \uacc4\uc0b0\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. (**Enet KRR Gboost** \uc758 \ud3c9\uade0\uc744 \uc0ac\uc6a9\ud574 \uba54\ud0c0 \ubaa8\ub378\uc778 Lasso \ub97c \ucd94\uac00\ud568)","7ac8468a":"**Box Cox Transformation of (highly) skewed features**\n\n**\ubc15\uc2a4 \ucf55\uc2a4 \ubcc0\ud658**","ac7faecf":"## Base models","61f31e9c":"# Data Processing","d5f58f50":"**Define a cross validation strategy**","3280f147":"**7 : 1.5 : 1.5 \ube44\uc728\ub85c ensemble \ud55c \uc774\uc720...?**","a77c6a91":"On this gif, the base models are algorithms 0, 1, 2 and the meta-model is algorithm 3. The entire training dataset is \nA+B (target variable y known) that we can split into train part (A) and holdout part (B). And the test dataset is C. \n\nB1 (which is the prediction from the holdout part)  is the new feature used to train the meta-model 3 and C1 (which\nis the prediction  from the test dataset) is the meta-feature on which the final prediction is done. ","72ed7ba9":"- **Alley** : data description says NA means \"no alley access\"","9e4873d1":"### Less simple Stacking : Adding a Meta-model","8aca4721":"- **XGBoost** :","8c966bde":"### ------------------------------------------------","dea864aa":"\uba74\uc801(\ud3c9\uc218) \uad00\ub828\ub41c feature\ub294 \uc8fc\ud0dd \uac00\uaca9\uc744 \uacb0\uc815\ud558\ub294 \ub370 \ub9e4\uc6b0 \uc911\uc694\ud558\uae30 \ub54c\ubb38\uc5d0 \uac01 \uc8fc\ud0dd\uc758 \uc9c0\ud558, 1 \uce35 \ubc0f 2 \uce35 \uba74\uc801\uc758 \ucd1d \uba74\uc801\uc744 \ud558\ub098 \ub354 \ucd94\uac00\ud569\ub2c8\ub2e4","063c1225":"- **Electrical** : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.","9ea10fec":"### Imputing missing values ","0f3174cb":"**Skewed features**\n\n\uc22b\uc790\ud615 \ub370\uc774\ud130\ub4e4\uc758 Skewness (\uce58\uc6b0\uce68) \uc0b4\ud3b4\ubd04 (\ub192\uc744\uc218\ub85d \ub9ce\uc774 \uce58\uc6b0\uce68)","b191cfea":"-  **LASSO  Regression**  : \n\n\ub77c\uc3d8 \ud68c\uadc0 \ubaa8\ub378\uc740 \uc774\uc0c1 \uce58(outliers)\uc5d0 \ub9e4\uc6b0 \ubbfc\uac10\ud558\uac8c \ubc18\uc751\ud55c\ub2e4. \uadf8\ub7ec\ubbc0\ub85c Robustscaler() \ub97c \uc0ac\uc6a9\ud574\uc11c outlier\uc758 \uc601\ud5a5\uc744 \ucd5c\uc18c\ud654 \ud558\uc790\n\n\nRobustscaler : [https:\/\/rfriend.tistory.com\/269]\n\n> Summary:\n\nRobust Scaler\ub294 \uc774\uc0c1\uce58\uac00 \ud3ec\ud568\ub41c \ub370\uc774\ud130\uc758 \uc911\uc559\uac12\uacfc IQR \ub97c \uc774\uc6a9\ud55c \ud45c\uc900\ud654\n\nStandard Scaler \ubcf4\ub2e4 \ub3d9\uc77c\ud55c \uac12\uc744 \ub354 \ub113\uac8c \ubd84\ud3ec\uc2dc\ud0a4\uace0 \uc788\uc74c\n\n\uc989, \ubaa9\ud45c\ubcc0\uc218 y\uac12\uc744 \ubd84\ub958\ub098 \uc608\uce21\ud558\ub294\ub370 \uc788\uc5b4 \uc0b0\ud3ec\uac00 \ub354 \ud06c\uae30 \ub54c\ubb38\uc5d0 \uc124\uba85\ubcc0\uc218 x\ubcc0\uc218\ub85c\uc11c \ub354 \uc720\uc6a9\ud560 \uc218 \uc788\ub2e4\uace0 \ucd94\uc815\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n---------------------------------------------------------\n\n\ud30c\uc774\ud504\ub77c\uc778 \uae30\ubc95\uc744 \uc0ac\uc6a9\ud560 \uac83. (make_pipeline)\n\n\uc804\ucc98\ub9ac\uae30 + \ubaa8\ub378\uc744 \uc5ee\uc5b4\uc11c \ud30c\uc774\ud504\ub77c\uc778\uc73c\ub85c \ub9cc\ub4e4\uba74 \ub370\uc774\ud130\uac00 \uc804\ucc98\ub9ac\uae30\ub97c \ud1b5\ud574 \ubcc0\ud658\ub418\uace0 \uadf8 \uc774\ud6c4\uc5d0 \ud574\ub2f9 \ubaa8\ub378\ub85c \ud559\uc2b5\uc774 \uc9c4\ud589\ub428","ce7f8d37":"- **KitchenQual**: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent)  for the missing value in KitchenQual.\n","15649147":"- **SaleType** : Fill in again with most frequent which is \"WD\"","8284bea9":"**Averaged base models score**","b089f6be":"# Modelling","9ef1bf3d":"## Outliers","4fab8a56":"\uc774\uc0c1 \uce58(Outlier)\ub97c \ucc3e\uc544\ubcf8\ub2e4.","d4447b17":"**Stacking averaged Models Class**","490ab0c7":"## Target Variable","4b4e628f":"\ub2e4\uc2dc all_data \ub97c train \uacfc test\ub85c \ub098\ub208\ub2e4 (\uc704\uc5d0\uc11c \ud569\uccd0\uc11c \ud55c\ubc88\uc5d0 feature\ub4e4\uc744 \uc870\uc815\ud588\uc73c\ubbc0\ub85c)","3f6d4d5f":"SKlearn \uc758 cross_val_score \ud568\uc218\ub97c \uc0ac\uc6a9\ud574\uc11c \uad50\ucc28\uac80\uc99d\uc744 \uc218\ud589\ud55c\ub2e4.\n\n\ud558\uc9c0\ub9cc \uc774 \ud568\uc218\ub294 shuffle(\ub370\uc774\ud130 \uc11e\uae30) \uae30\ub2a5\uc774 \uc5c6\uc73c\ubbc0\ub85c KFold \ud568\uc218\ub97c \uc0ac\uc6a9\ud574\uc11c \ub370\uc774\ud130 shuffle \uae4c\uc9c0 \uc218\ud589\ud55c\ub2e4.","1c2cfaeb":"### Base models scores","7d8580b6":"- **Utilities** : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\"  and 2 NA . Since the house with 'NoSewa' is in the training set, **this feature won't help in predictive modelling**. We can then safely  remove it.\n\nUtilities \ub294 \ubaa8\ub4e0 \uac12\uc774 100% AllPub \uc784\n\nUtilities \ub294 \ubaa8\ub378\uc5d0 \ub3c4\uc6c0\uc774 \ub418\uc9c0 \uc54a\uc744\uac83\uc774\ubbc0\ub85c \uc0ad\uc81c\ud568\n","1128ed96":"### Missing Data\n\n\uac01\uac01\uc758 column \ub9c8\ub2e4 null \uac12\uc774 \uc5bc\ub9c8\ub098 \uc788\ub294\uc9c0 \uacc4\uc0b0\ud574 \ubcf4\uc558\ub2e4.","d9a61b66":"**Data Correlation**\n\nfeature \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \uc0b4\ud3b4\ubcf4\uc790","06bb8241":"**numpy.expm1 : exp(x) - 1**\n\n* **\uc704\uc5d0\uc11c \uc804\ucc98\ub9ac\ud560\ub54c SalePrice\ub97c log(1+x) \uc801\uc6a9\ud588\uae30 \ub54c\ubb38\uc5d0 \ucd5c\uc885 \ucd9c\ub825\uac12\uc740 \uc5ed\ud568\uc218\uc778 e^x -1 \ub97c \uc801\uc6a9\ud574\uc900\ub2e4.**","c48991fc":"\ubc94\uc8fc\ud615 \ub370\uc774\ud130\ub4e4\uc744 \ub354\ubbf8\ub85c \ubcc0\ud658\uc2dc\ud0b4\n\n\uc608\ub97c\ub4e4\uc5b4..\n\n","c7ddee33":"**Stacking Averaged models Score**","0911258d":"## <font color=\"blue\"> Summary <\/font>\n\ntrain \ub370\uc774\ud130\uc640 test \ub370\uc774\ud130\ub97c import \ud558\uace0, id \uceec\ub7fc\uc744 \uc0ad\uc81c\ud558\uc600\ub2e4. (\ud544\uc694 \uc5c6\uc73c\ubbc0\ub85c)\n\ntrain set\uc740 target(SalePrice) \ub370\uc774\ud130\ub97c \ud3ec\ud568\ud558\uc5ec 79 + 1 \uac1c\uc758 feature\ub97c \uac00\uc9c0\uba70,\n\ntest set\uc740 target \ub370\uc774\ud130\uac00 \uc5c6\ub294 79 \uac1c\uc758 feature\ub97c \uac00\uc9c4\ub2e4.","7c19d4fb":"- **MiscFeature** : data description says NA means \"no misc feature\"\n","118aea1f":"## Stacking  models","14951ef3":"## Features engineering","41abd505":"**LightGBM:**","ce66a4f5":"### Final Training and Prediction","2db1187f":"**Getting dummy categorical features**","afe231e3":"- **Exterior1st and Exterior2nd** : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string\n","003959e1":"- **Gradient Boosting Regression** :\n\nWith **huber**  loss that makes it robust to outliers\n    ","22350a8c":"We use the scipy  function boxcox1p which computes the Box-Cox transformation of 1 + x \n\nNote that setting \\\\( \\lambda = 0 \\\\) is equivalent to log1p used above for the target variable.  \n\nSee [this page][1] for more details on Box Cox Transformation as well as [the scipy function's page][2]\n[1]: http:\/\/onlinestatbook.com\/2\/transformations\/box-cox.html\n[2]: https:\/\/docs.scipy.org\/doc\/scipy-0.19.0\/reference\/generated\/scipy.special.boxcox1p.html","9136932b":"### heat map \ubcf4\ub294\ubc95:\n\n\ub300\uac01\uc120\uc740 \uc790\uae30 \uc790\uc2e0\uacfc\uc758 correltion(\uc0c1\uad00\uad00\uacc4) \uc774\ubbc0\ub85c 1.0 \uc758 \uac12\uc744 \uac00\uc9c4\ub2e4. (\uac00\uc7a5 \ubc1d\uc740 \uc0c9)\n\n\uadf8 \uc678\uc5d0 \ubc1d\uc740 \uc0c9\uc744 \ub744\ub294 \uce78\uc758 feature \ub4e4\uc740 \ub192\uc740 \uc0c1\uad00\uad00\uacc4\ub97c \uac00\uc9c4\ub2e4. \n\n\uc608\ub97c\ub4e4\uc5b4 GarageCars \uc640 GarageArea \ub294 \uac70\uc758 \ub3d9\uc77c\ud55c \uc758\ubbf8\uc758 \ub370\uc774\ud130\uc774\ub2e4. \n\nGarageCars: Size of garage in car capacity\n\nGarageArea: Size of garage in square feet\n\n\ub458 \ub2e4 \ucc28\uace0\uc758 \ud06c\uae30\ub97c \ub098\ud0c0\ub0c4\n\n\n### SalePrice \uc640 \uc758 \uc0c1\uad00 \uad00\uacc4\n\n\ub9e8 \ubc11 \uc904 \ud639\uc740 \ub9e8 \uc624\ub978\ucabd \uc904\uc744 \ubcf4\uba74 SalePrice\uc640\uc758 \uc0c1\uad00\uad00\uacc4\uac00 \ub192\uc740 feature\ub4e4\uc744 \ud655\uc778\ud574 \ubcfc \uc218 \uc788\ub2e4.\n\n\uc608\ub97c\ub4e4\uc5b4 OverallQual(Overall material and finish quality) \ub294 \ub9e4\uc6b0 \uc911\uc694\ud55c feature \ub77c\uace0 \ud310\ub2e8\ud560 \uc218 \uc788\ub2e4.","ca92ff25":"\uc704\uc5d0\uc11c \uc0dd\uc131\ud55c base \ubaa8\ub378\ub4e4\uc774 \uc6b0\ub9ac\uac00 \uac00\uc9c4 \ub370\uc774\ud130\uc5d0 \uc5bc\ub9cc\ud07c \uc131\ub2a5\uc744 \ub0b4\ub294\uc9c0 \ud655\uc778\ud574 \ubd05\ub2c8\ub2e4. (rmsle error\ub97c \uae30\uc900\uc73c\ub85c)","677985d4":"- **Functional** : data description says NA means typical","8ae8a194":"### More features engeneering","50f6a38f":"\uccab\uc9f8\ub85c \uc131\ub2a5\uc744 \uce21\uc815\ud560 root-mean-square-logarithmic-error \ub97c \uc815\uc758 \ud569\ub2c8\ub2e4.\n\n[https:\/\/dacon.io\/user1\/41382]\n\n> summary:\n\nRMSLE\ub294 \uc608\uce21\uac12\uacfc \uc2e4\uc81c\uac12\uc758 \ucc28\uc774\uc5d0 \ub300\ud55c \ube44\uc728\uc744 \uce21\uc815\ud558\ub294 \uac83\uc73c\ub85c \uc54c\uace0 \uc788\uc2b5\ub2c8\ub2e4.\n\n\ud070 \uac12\uc5d0 \ub300\ud574\uc11c \ud328\ub110\ud2f0\ub97c \uc8fc\uc9c0 \uc54a\uace0\uc790 \ud560 \ub54c \uc0ac\uc6a9\ud558\uc9c0\ub9cc,\n\n\uc2e4\uc81c\uac12\ubcf4\ub2e4 \uc608\uce21\uac12\uc774 \ub0ae\uc740 \uacbd\uc6b0\uc5d0 \ud328\ub110\ud2f0\ub97c \uc8fc\uace0\uc790 \ud560 \ub54c\ub3c4 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n\nCase a) : Pi = 600, Ai = 1000\n\nRMSE = 400, RMSLE = 0.5108\n\nCase b) : Pi = 1400, Ai = 1000\n\nRMSE = 400, RMSLE = 0.3365","2e0e73b1":"target variable(SalePrice) \ub294 \uc624\ub978\ucabd\uc73c\ub85c \uce58\uc6b0\uc838\uc788\ub2e4 (right skewed) \uc120\ud615 \ubaa8\ub378\ub4e4\uc740 \uc815\uaddc\ubd84\ud3ec(normal distribution)\uc744 \uc798 \ud559\uc2b5\ud558\ubbc0\ub85c \uc815\uaddc\ud654 \uc791\uc5c5\uc774 \ud544\uc694\n\n**Right-Skewed \ub780 \ubd84\ud3ec\uc758 tail(\uaf2c\ub9ac\ubd80\ubd84)\uc774 \uc624\ub978\ucabd\uc73c\ub85c \ucb49 \uc774\uc5b4\uc9c4 \ubaa8\uc591\uc744 \ub73b\ud568 (peak\uac00 \uc67c\ucabd\uc5d0 \uc704\uce58\ud568)**","3ffb7e2f":"\uc774 \uc811\uadfc \ubc29\uc2dd\uc5d0\uc11c\ub294 \ud3c9\uade0 \uae30\ubc18 \ubaa8\ub378(Averaged model)\uc5d0 \uba54\ud0c0 \ubaa8\ub378(Meta model)\uc744 \ucd94\uac00\ud558\uace0 \uc774\ub7ec\ud55c \uae30\ubcf8 \ubaa8\ub378\uc758 out-of-folds \uc608\uce21\uc744 \uc0ac\uc6a9\ud558\uc5ec \uba54\ud0c0 \ubaa8\ub378\uc744 \ud559\uc2b5\uc2dc\ud0b5\ub2c8\ub2e4.\n\n\ud559\uc2b5 \uc808\ucc28\ub294 \uc544\ub798 4\uac1c\uc758 step\uc744 \ub2e4\ub985\ub2c8\ub2e4:\n\n1. Split the total training set into two disjoint sets (here **train** and .**holdout** )\n\n2. Train several base models on the first part (**train**)\n\n3. Test these base models on the second part (**holdout**)\n\n4. Use the predictions from 3)  (called  out-of-folds predictions) as the inputs, and the correct responses (target variable) as the outputs  to train a higher level learner called **meta-model**.\n\nThe first three steps are done iteratively . If we take for example a 5-fold stacking , we first split the training data into 5 folds. Then we will do 5 iterations. In each iteration,  we train every base model on 4 folds and predict on the remaining fold (holdout fold). \n\nSo, we will be sure, after 5 iterations , that the entire data is used to get out-of-folds predictions that we will then use as \nnew feature to train our meta-model in the step 4.\n\nFor the prediction part , We average the predictions of  all base models on the test data  and used them as **meta-features**  on which, the final prediction is done with the meta-model.\n","e4a13587":"\ub204\ub77d\ub41c \ub370\uc774\ud130\ub4e4(Nan)\uc744 \ucc44\uc6cc\ub123\uc790. \n\n\ubb38\uc790\uc5f4\ub85c \uc800\uc7a5\ub41c column \ub4e4\uc740 None \uc73c\ub85c \ucc44\uc6b0\uace0, (\ub610\ub294 \uac00\uc7a5 \ube48\ub3c4\uc218\uac00 \ub9ce\uc740 \uac12 \ub4f1..)\n\n\uc22b\uc790\ub85c \uc800\uc7a5\ub41c column \ub4e4\uc740 0 or \uc911\uc559\uac12, \ud3c9\uade0\uac12 \ub4f1\uc73c\ub85c \ucc44\uc6c0","c803364d":"- **GarageType, GarageFinish, GarageQual and GarageCond** : Replacing missing data with None","e86d84bc":"![kaz](http:\/\/5047-presscdn.pagely.netdna-cdn.com\/wp-content\/uploads\/2017\/06\/image5.gif)\n\nGif taken from [KazAnova's interview](http:\/\/blog.kaggle.com\/2017\/06\/15\/stacking-made-easy-an-introduction-to-stacknet-by-competitions-grandmaster-marios-michailidis-kazanova\/)","1767c130":"- **Kernel Ridge Regression** :","d454b2c5":"**XGBoost, LightGBM** \uc744 \uc704\uc5d0\uc11c \ub9cc\ub4e0 Stacked Averaged model \uc5d0 \ucd94\uac00\ud569\ub2c8\ub2e4. ","94844c3b":"**\ubc94\uc8fc\ud615\uc73c\ub85c \uc800\uc7a5\ub418\uc5b4 \uc788\ub294 \ub370\uc774\ud130\uc778\ub370, \uc2e4\uc81c\ub85c\ub294 \ud574\ub2f9 category \uc758 \uc21c\uc11c\uac00 \uc911\uc694\ud55c \uacbd\uc6b0 \uc774\ub97c \ub77c\ubca8\uc778\ucf54\ub529\uc73c\ub85c \uc22b\uc790\ub85c \ubc14\uafc8(0,1,2...)**\n\n**\uc989, \ub4f1\uae09,\ub808\ubca8 \ub4f1\uc758 \uc21c\uc11c\uac00 \uc758\ubbf8 \uc788\ub294 \ub370\uc774\ud130\ub97c \ubc94\uc8fc\ud615\uc774 \uc544\ub2cc \uc22b\uc790\ub85c \ub098\ud0c0\ub0c4 **","6c0c95b2":"**Ensemble prediction:**","200d727b":"- **MasVnrArea and MasVnrType** : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type. \n","a1e940d5":"\uc774\ubc88\uc5d0\ub294 z = log(1+x) \uc2dd\uc744 \uc0ac\uc6a9\ud574\uc11c Log-transformation \uc744 \ud574\ubcf4\uc790.\n\nRight-Skewed \ub370\uc774\ud130\ub294 \ub85c\uadf8\ub97c \uc0ac\uc6a9\ud574 \uc815\uaddc\ubd84\ud3ec \ubaa8\uc591\uc73c\ub85c \ub9cc\ub4e4 \uc218 \uc788\ub2e4.\n\n[https:\/\/stats.stackexchange.com\/questions\/107610\/what-is-the-reason-the-log-transformation-is-used-with-right-skewed-distribution]\n\n### <font color=\"blue\"> Summary <\/font> : \n\ub370\uc774\ud130\uc5d0 log\ub97c \ucde8\ud558\ub294 \uac83\uc740 \uadf9\ub2e8\uc801\uc778 \uac12\ub4e4\uc744 PULL \ud558\ub294 \ud6a8\uacfc\uac00 \uc788\ub2e4. \uc989. \uc57d\uac04 \ud070 \uac12\ub4e4\uc740 \uc870\uae08 \uac10\uc18c\uc2dc\ud0a4\uace0, \ub108\ubb34 \ud070 \uac12\ub4e4\uc740 \ub9ce\uc774 \uac10\uc18c \uc2dc\ud0a8\ub2e4.\n\nhttps:\/\/www.google.com\/search?ei=mXXNXMjGKIX88AWmnq3IAQ&q=log%28x%2B1%29+and+x&oq=log%28x%2B1%29+and+x&gs_l=psy-ab.3...72927.78248..78512...3.0..0.128.1218.0j11......0....1..gws-wiz.......0i30j0i20i263j0i203j0i13i30j0i8i13i30j0i8i30j33i160.nDOPeD89dQU\n\n\ucd94\uac00\ub85c Left-Skewed \ub370\uc774\ud130\uc758 \uacbd\uc6b0\ub294 \ub85c\uadf8\ub97c \uc0ac\uc6a9\ud558\uba74 \ub354 \uc2ec\ud558\uac8c left-skewed \ub41c\ub2e4. \uc774\ub54c\ub294 power\ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uc744 \uace0\ub824\ud574\uc57c \ud560 \uac83\uc774\ub2e4.","3e052eb3":"\uae30\ubcf8 \ubaa8\ub378\uc744 \ud3c9\uade0\ud654 \ud558\ub294 \uac04\ub2e8\ud55c \ubc29\ubc95\uc73c\ub85c \ubaa8\ub378\uc744 Stacking \ud569\ub2c8\ub2e4. \ubaa8\ub378\uc744 scikit-learn \uc5d0\uc11c \ud655\uc7a5\ud558\uace0 \ucea1\uc290\ud654\uc640 \ucf54\ub4dc \uc7ac\uc0ac\uc6a9\uc744 \uc704\ud574 \uc0c8\ub85c\uc6b4  **\ud074\ub798\uc2a4**\ub97c \ube4c\ub4dc\ud569\ub2c8\ub2e4. (\uc77c\uc885\uc758 Wrapper \ud074\ub798\uc2a4)\n\n\uc989, \uc5ec\ub7ec \ubaa8\ub378\uc744 \ub3d9\uc2dc\uc5d0 \uc0ac\uc6a9\ud558\uace0 \ud574\ub2f9 \ubaa8\ub378\ub4e4\uc758 \uc608\uce21\uac12(prediction)\uc744 \ud3c9\uade0\ub0b4\uc5b4 \ucd5c\uc885 \uc608\uce21\uac12\uc73c\ub85c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.","732d59b2":"- **MSZoning (The general zoning classification)** :  'RL' is by far  the most common value.  So we can fill in missing values with 'RL'\n\nRL\uc774 \uac00\uc7a5 \ub9ce\uc774 \ub098\uc624\ub294 \uac12\uc774\ubbc0\ub85c \ube48\uac12\uc740 \ubaa8\ub450 RL\ub85c \ub300\uccb4\ud558\uae30\ub85c \uacb0\uc815"}}