{"cell_type":{"f17cb606":"code","d4a67230":"code","e93e5d5d":"code","b59cd842":"code","bfd50a9b":"code","1555e394":"code","9ea8f3e0":"code","4da7c478":"code","5f427be4":"code","8cd0aed0":"code","ccad131a":"code","7f8b75eb":"code","5898a5e7":"code","6519a153":"code","1c925044":"code","b10d087f":"code","f42d3444":"code","1f18711b":"code","7a67bd1f":"code","22446fec":"code","bf2f069b":"code","5e51203d":"code","31d23814":"code","c9855669":"code","519beae3":"code","fc8da532":"code","3ef2721c":"code","ee5cb5df":"code","410e4c6e":"code","a198fff0":"code","afaa08de":"code","70568014":"code","e74a66b6":"code","20138894":"code","fc722976":"code","a484609a":"code","0b9c184e":"code","78186649":"code","6a4582da":"code","eb9e717f":"code","06bf2e31":"code","b35910c6":"code","bb0a2f30":"code","b7a6f24c":"code","b6e6c838":"markdown","95658fac":"markdown","9ba46c3c":"markdown","752f01c1":"markdown","a829304b":"markdown","777b3902":"markdown","c0b2505b":"markdown","eb8505a0":"markdown","9f851d12":"markdown","9f637c63":"markdown","93f8091e":"markdown","e279c98f":"markdown"},"source":{"f17cb606":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\n%matplotlib inline\npd.pandas.set_option(\"display.max_columns\", None)\n# pd.pandas.set_option(\"display.max_rows\", None)","d4a67230":"train_data = pd.read_csv('..\/input\/income-qualification\/train.csv')","e93e5d5d":"train_data.head()","b59cd842":"train_data.info()","bfd50a9b":"# checking for the columns that has the null values in them\ncolumn_nan = [feature for feature in train_data.columns if train_data[feature].isnull().any() == True]\nprint(column_nan)","1555e394":"# checking for numerical features\nnumerical_features = [feature for feature in train_data.columns if train_data[feature].dtype != 'object']\ntrain_data[numerical_features].head()","9ea8f3e0":"# checking for categorical features\ntrain_data.select_dtypes('object').head()","4da7c478":"print(train_data.groupby('Target')['Target'].count())\nsns.countplot(train_data.Target)","5f427be4":"# since the data seems to be baised so we will first we will do over sampling of the data so to get the balanced data\n# minority specifies the sampler to resmaple those data which are less in count and \n# to reach them at the same count with the other majority class\n\noversampler = RandomOverSampler() #sampling_strategy='minority')","8cd0aed0":"X_over, y_over = oversampler.fit_resample(train_data.drop('Target', axis=1), train_data['Target'])\nX_over.shape, y_over.shape","ccad131a":"dataset_over = X_over.merge(y_over, left_index=True, right_index=True)","7f8b75eb":"train_data.shape, dataset_over.shape","5898a5e7":"# Let's again check the biasnes of data\nsns.countplot(dataset_over.Target)","6519a153":"same_poverty = 0\nno_family_head = 0\n\nfor idhogar in dataset_over['idhogar'].unique():\n    if len(dataset_over[dataset_over['idhogar'] == idhogar]['Target'].unique()) == 1:\n        same_poverty += 1\n    if (dataset_over[dataset_over['idhogar'] == idhogar]['parentesco1'] == 0).all():\n        no_family_head += 1","1c925044":"print('Family with the same poverty level:', same_poverty)\nprint('Family with the diff poverty level:', len(dataset_over['idhogar'].unique()) - same_poverty)\nprint('House without a Family head:', no_family_head)","b10d087f":"dataset_over[column_nan].isnull().sum() \/ dataset_over.shape[0]","f42d3444":"zero_var_col = [feature for feature in dataset_over.columns if len(dataset_over[feature].unique()) == 1]\nprint('columns with zero variance', zero_var_col)","1f18711b":"# removing columns with zero variance\ndataset_over.drop(['elimbasu5'], axis=1, inplace=True)","7a67bd1f":"# We will be performing Feature Engineering on the copy of the original data\ntrain_df = dataset_over.copy()","22446fec":"# Since dependency column decimal values so that we will convert into integer value using ceil value. \n# But before that we will have to convert the yes and no values to integer value\n# print(train_df['dependency'].unique())\ntrain_df.loc[train_df['dependency'] == 'no', 'dependency'] = 0\n# print(train_df['dependency'].unique())\ntrain_df.loc[train_df['dependency'] == 'yes', 'dependency'] = train_df[train_df['dependency'] != 'yes']['dependency'].astype('float').mean()\n# print(train_df['dependency'].unique())\ntrain_df['dependency'] = train_df['dependency'].astype('float').apply(np.ceil)\n# print(train_df['dependency'].unique())\ntrain_df['dependency'].plot.box()\nplt.show()","bf2f069b":"# Since we have some outliers in the dependency column so we will treat them by updating those rows from mean(including non-zero values only) which have outliers\n# train_df = train_df[train_df['dependency'] < 4]\ntrain_df.loc[train_df['dependency']>3, 'dependency'] = np.ceil(train_df[train_df['dependency']>0]['dependency'].mean())\ntrain_df['dependency'].plot.box()","5e51203d":"train_df['v2a1'].fillna(0, inplace=True)\ntrain_df[train_df['v2a1'] > 0]['v2a1'].plot.box()\nplt.show()\ntrain_df[(train_df['v2a1'] < 330000) & (train_df['v2a1'] > 0)]['v2a1'].plot.box()\nplt.show()","31d23814":"train_df.loc[train_df['v2a1'] > 350000, 'v2a1'] = np.round(train_df[train_df['v2a1'] > 0]['v2a1'].mean())\ntrain_df[train_df['v2a1'] > 0]['v2a1'].plot.box()","c9855669":"# Removing uneccessary features from the dataset\n\nfeature_remove = ['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin',\n       'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq']\ntrain_df.drop(feature_remove, axis=1, inplace=True)","519beae3":"train_df.head()","fc8da532":"for idhogar in train_df['idhogar'].unique():\n    train_df.loc[train_df['idhogar'] == idhogar, 'Target'] = stats.mode(train_df[train_df['idhogar'] == idhogar]['Target']).mode[0]","3ef2721c":"column_null = [feature for feature in train_df.columns if train_df[feature].isnull().any() == True]\nprint(column_null)","ee5cb5df":"train_df.drop(['v18q1', 'rez_esc'], axis=1, inplace=True)","410e4c6e":"train_df.head()","a198fff0":"train_df['meaneduc'].fillna(0, inplace=True)\ntrain_df.drop(['Id', 'idhogar'], axis=1, inplace=True)","afaa08de":"train_df.select_dtypes('object').head()","70568014":"print(train_df['edjefe'].unique())\ntrain_df.loc[train_df['edjefe'] == 'no', 'edjefe'] = 0\ntrain_df.loc[train_df['edjefe'] == 'yes', 'edjefe'] = train_df[train_df['edjefe'] != 'yes']['edjefe'].astype('float').mean()\nprint(train_df['edjefe'].unique())\ntrain_df['edjefe'] = train_df['edjefe'].astype('float').apply(np.ceil)\nsns.boxplot(train_df['edjefe'])\nplt.show()\ntrain_df.loc[train_df['edjefe'] > 15, 'edjefe'] = np.ceil(train_df[train_df['edjefe']>0]['edjefe'].mean())\nsns.boxplot(train_df['edjefe'])\nplt.show()","e74a66b6":"print(train_df['edjefa'].unique())\ntrain_df.loc[train_df['edjefa'] == 'no', 'edjefa'] = 0\ntrain_df.loc[train_df['edjefa'] == 'yes', 'edjefa'] = train_df[train_df['edjefa'] != 'yes']['edjefa'].astype('float').mean()\nprint(train_df['edjefa'].unique())\ntrain_df['edjefa'] = train_df['edjefa'].astype('float').apply(np.ceil)\nprint(train_df['edjefa'].unique())\nsns.boxplot(train_df['edjefa'])\nplt.show()\ntrain_df.loc[train_df['edjefa'] > 15, 'edjefa'] = np.ceil(train_df[train_df['edjefa']>0]['edjefa'].mean())\nsns.boxplot(train_df['edjefa'])\nplt.show()","20138894":"sns.boxplot(train_df['meaneduc'])\nplt.show()\ntrain_df.loc[train_df['meaneduc'] > 17, 'meaneduc'] = np.ceil(train_df[train_df['meaneduc']>0]['meaneduc'].mean())\nsns.boxplot(train_df['meaneduc'])\nplt.show()","fc722976":"pca = PCA()","a484609a":"pca.fit(train_df)\npca.explained_variance_ratio_","0b9c184e":"X = train_df.drop(['Target'], axis=1)\ny = train_df['Target'].values\npca = PCA(n_components=10)\nX_pca = pca.fit_transform(X)","78186649":"X_pca[:1]","6a4582da":"X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2)","eb9e717f":"rfc_model = RandomForestClassifier(n_estimators = 100)","06bf2e31":"rfc_model.fit(X_train, y_train)","b35910c6":"y_pred = rfc_model.predict(X_test)","bb0a2f30":"print(accuracy_score(y_test, y_pred))\nprint(precision_score(y_test, y_pred, average='weighted'))\nprint(recall_score(y_test, y_pred, average='weighted'))\nprint(f1_score(y_test, y_pred, average='weighted'))","b7a6f24c":"from sklearn.model_selection import cross_val_score\ncv_score = cross_val_score(rfc_model, X, y, cv=10)\nprint(cv_score)\nprint('Accuracy after cross validation: ', cv_score.mean())","b6e6c838":"##### checking the biases in the DATA\n\nsince it is classification problem so we can check the biasness in the data by grouping them all on the target variable","95658fac":"## Feature Engineering","9ba46c3c":"from above bar chart we can clearly observe that the is biased towards a single class 4","752f01c1":"##### Set poverty level of the members and the head of the house within a family.\n\nupdating the value of target from the most probable poverty level of the house.","a829304b":"apart from the meaneduc rest og the columns are not creating too much significance on the target so we will delete those columns from the dataset","777b3902":"Since the rent paid has nan values as well as 0. This might be the case where the household people are the owner of that house. So we will be updating the nan value from 0\n\nAnd the dataset is also having some outliers so we will treat them by updating them from mean value of that column","c0b2505b":"Treating outliers from the mean value of that column excluding zeros and null values","eb8505a0":"columns with zero variance","9f851d12":"## EDA","9f637c63":"* Checking if all members of the family has the same poverty level\n* Checking if there is a house without a family head.","93f8091e":"Count how many null values are existing in columns.","e279c98f":"##### Oversampling on the biased data"}}