{"cell_type":{"8b6f9732":"code","0ef3a72f":"code","4834759c":"code","4f03ffab":"code","26d4b84b":"code","f800d33a":"code","4fa1c483":"code","3536678c":"code","2e4ea14a":"code","04d839f8":"code","ca26335a":"code","20fc951f":"code","3dbe0518":"code","5b924386":"code","0c4f48db":"code","7421559c":"markdown","915af381":"markdown","f90b0200":"markdown","797b43c0":"markdown","373952b7":"markdown","bb50fba6":"markdown","9971ab9b":"markdown","9793c1cb":"markdown","13d209fb":"markdown","fd8a3815":"markdown","c41ec18c":"markdown"},"source":{"8b6f9732":"!pip install welford --user","0ef3a72f":"import numpy as np\nimport pandas as pd\nfrom welford import Welford\nimport glob\nimport json\nfrom tqdm import tqdm\nimport tensorflow as tf\nimport tensorflow_datasets as tfds","4834759c":"train_files = glob.glob('..\/input\/g2net-gravitational-wave-detection\/train\/*\/*\/*\/*.npy')","4f03ffab":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nBATCH_SIZE = 1\n\ndef _parse_function1(filename):\n    np_data = tf.io.read_file(filename)\n    np_data = tf.strings.substr(np_data, 128, 98304) # header is 128 bytes (skip)\n    np_data = tf.reshape(tf.io.decode_raw(np_data, tf.float64), (3, 4096))\n    return np_data\n\ntrain_ds = tf.data.Dataset.from_tensor_slices(train_files)\ntrain_ds = train_ds.map(_parse_function1, num_parallel_calls=AUTOTUNE)\ntrain_ds = train_ds.batch(BATCH_SIZE)","26d4b84b":"w0 = Welford()\nw1 = Welford()\nw2 = Welford()\n\nCHK_COUNT = 25000\n\nfor i in tqdm(range(CHK_COUNT)):\n    d = np.load(train_files[i])\n    w0.add_all(np.expand_dims(d[0], axis = 1))\n    w1.add_all(np.expand_dims(d[1], axis = 1))\n    w2.add_all(np.expand_dims(d[2], axis = 1))","f800d33a":"def tf_welford(ds, cnt_limit=-1):\n    ds_numpy = tfds.as_numpy(ds)\n    w_mean = np.zeros(3, dtype=np.float64)\n    w_var = np.zeros(3, dtype=np.float64)\n    sumsq = np.zeros(3, dtype=np.float64)\n    cnt = 0.0\n    for da in tqdm(ds_numpy):\n        cnt += 1.0\n        for j in range(3):\n            x = da[0,j]\n            delta = tf.math.reduce_mean(x - w_mean[j]).numpy()\n            w_mean[j] += delta \/ cnt\n            # variance calculation deviates a little from Welford as it uses a batch of 4096 \n            sumsq[j] += tf.math.reduce_sum(tf.math.multiply(x, x)).numpy()\n            w_var[j] = (sumsq[j]\/(cnt*4096.)) - w_mean[j]*w_mean[j]\n    \n        if cnt == float(cnt_limit):\n            break \n    return w_mean, w_var","4fa1c483":"w1_mean, w1_var = tf_welford(train_ds, CHK_COUNT)","3536678c":"stats = [[w0.mean[0], w1_mean[0], w0.mean[0]\/w1_mean[0], w0.var_s[0], w1_var[0], w0.var_s[0]\/w1_var[0]],\n         [w1.mean[0], w1_mean[1], w1.mean[0]\/w1_mean[1], w1.var_s[0], w1_var[1], w1.var_s[0]\/w1_var[1]],\n         [w2.mean[0], w1_mean[2], w2.mean[0]\/w1_mean[2], w2.var_s[0], w1_var[2], w2.var_s[0]\/w1_var[2]]]\ndfs = pd.DataFrame(stats, columns=['PyPI mean', 'TF mean', 'PyPi\/TF mean ratio', 'PyPI var', 'TF var', 'PyPi\/TF var ratio'])\ndfs","2e4ea14a":"w1_mean, w1_var = tf_welford(train_ds)","04d839f8":"train_stats = {}\ntrain_stats['detector'] = []\nfor i in range(3):\n    train_stats['detector'].append({\n                'idx': i,\n                'mean': w1_mean[i],\n                'std': np.sqrt(w1_var[i])})\n    \nwith open('train_stats.json', 'w') as fp:\n    json.dump(train_stats, fp, indent=4)","ca26335a":"test_files = glob.glob('..\/input\/g2net-gravitational-wave-detection\/test\/*\/*\/*\/*.npy')","20fc951f":"test_ds = tf.data.Dataset.from_tensor_slices(test_files)\ntest_ds = test_ds.map(_parse_function1, num_parallel_calls=AUTOTUNE)\ntest_ds = test_ds.batch(BATCH_SIZE)","3dbe0518":"w2_mean, w2_var = tf_welford(test_ds)","5b924386":"test_stats = {}\ntest_stats['detector'] = []\nfor i in range(3):\n    test_stats['detector'].append({\n                'idx': i,\n                'mean': w2_mean[i],\n                'std': np.sqrt(w2_var[i])})\n    \nwith open('test_stats.json', 'w') as fp:\n    json.dump(test_stats, fp, indent=4)","0c4f48db":"stats = []\nfor j in range(3):\n    stats.append([w1_mean[j], w2_mean[j], w1_mean[j]\/w2_mean[j], w1_var[j], w2_var[j], w1_var[j]\/w2_var[j]])\ndfs = pd.DataFrame(stats, columns=['Train mean', 'Test mean', 'Train\/test mean ratio', 'Train var', 'Test var', 'Train\/test var ratio'])\ndfs","7421559c":"The Welford library is actually super slow, much faster to use a TF implementation here. But we first calculate mean and variance for a subset with the slow and the fast implementation as a quality check of the TF version.","915af381":"Store results to json as well:","f90b0200":"Yes, happy with that - good enough for this purpose! Now run through the entire train dataset:","797b43c0":"# Summary\nSo is there a difference between then train and test data sets when it comes to mean and variance?","373952b7":"Yes - mean values vary quite a bit actually, while variance is very similar.","bb50fba6":"Let's compare the values computed by the two implementations:","9971ab9b":"# Train dataset\nWe create a tensorflow dataset here, not because we need to - just to practice.","9793c1cb":"# Calculate mean and std for entire dataset\nWhen scaling the input data during preprocessing, it is common practice to normalize data with a fixed mean and std (rather than individual on file level). So here we will simply calculate mean and std for each detector using [Welford\u2019s algorithm](https:\/\/pypi.org\/project\/welford\/) across the entire dataset.","13d209fb":"Then the TF implementation:","fd8a3815":"# Test dataset\nRepeat for the test dataset.","c41ec18c":"Store the mean and std in a json file for use in other notebooks:"}}