{"cell_type":{"5feb2686":"code","7162948e":"code","b9dc65d7":"code","5d01a3be":"code","d6f3dc6d":"code","c28986b6":"code","bb1c3dc1":"code","f686e4fd":"code","f9cd552f":"code","82905053":"code","f2e012e7":"code","1faa34d9":"code","163fc2d3":"code","eb86ef95":"code","cca9590a":"code","a6717479":"code","923b6edc":"markdown","98ffd88f":"markdown","641068dc":"markdown","e9ca7f60":"markdown","2bb8241a":"markdown","832c8204":"markdown","19782028":"markdown","35855489":"markdown"},"source":{"5feb2686":"#import these\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd \nimport numpy as np\nfrom sklearn import linear_model\n","7162948e":"#NoW suppose we have a dataset\nx=[1,2,3,4,5]\ny=[5,7,9,11,13]\n\n","b9dc65d7":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\nimg = mpimg.imread('..\/input\/linear-regression-analysis\/cost_function.png')\nimgplot = plt.imshow(img)","5d01a3be":"#Let's plot it\nplt.scatter(x,y,marker='o',color='r',label='skitscat')\nplt.xlabel('X')\nplt.ylabel('Y')\n#plt.grid(False,color='k')\nplt.title('Scatter')\nplt.legend()\nplt.show()","d6f3dc6d":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\nimg = mpimg.imread('..\/input\/linear-regression-analysis\/learningrate.png')\nimgplot = plt.imshow(img)","c28986b6":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\nimg = mpimg.imread('..\/input\/linear-regression-analysis\/MSEvsb.png')\nimgplot = plt.imshow(img)","bb1c3dc1":"import matplotlib.pyplot as plt \nimport matplotlib.image as mpimg \n%matplotlib inline \nimg = mpimg.imread('..\/input\/linear-regression-analysis\/learningrate.png') \nimgplot = plt.imshow(img)","f686e4fd":"#now let's create a gradient descent function\nn=len(x)\nx=np.array(x)\ny=np.array(y)\nlearningrate=0.08\ndef gradient_descent(x,y):\n    m_curr=b_curr=0\n    iterations=10000\n    \n    for i in range(iterations):\n        y_predic=m_curr*x + b_curr\n        cost=(1\/n)*sum([val**2 for val in (y-y_predic)])\n        md=-(2\/n)*sum(x*(y-y_predic))\n        bd=-(2\/n)*sum(y-y_predic)\n        m_curr=m_curr-learningrate*md\n        b_curr=b_curr-learningrate*bd\n        print(cost,m_curr,b_curr)\ngradient_descent(x,y)\n        \n    ","f9cd552f":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\nimg = mpimg.imread('..\/input\/linear-regression-analysis\/Learningr.png')\nimgplot = plt.imshow(img)","82905053":"reg=linear_model.LinearRegression()\n\ndf=pd.DataFrame(columns=['x','y'])\ndf['x']=x\ndf['y']=y\ndf.head()\n\n","f2e012e7":"df[['x']]","1faa34d9":"df['y']","163fc2d3":"reg.fit(df[['x']],df.y)\nreg.predict([[2.5]])\n","eb86ef95":"print(reg.coef_,reg.intercept_)","cca9590a":"homeprices=pd.read_csv('..\/input\/linear-regression-analysis\/homeprices.csv')\nhomeprices\ndf=homeprices\ndf.fillna(3.0,inplace=True)\ndf\n#You need to fill the nan values with exploratory data analytics\n#WE are using 3.0 here\n","a6717479":"reg=linear_model.LinearRegression()\nreg.fit(df[['area','bedrooms','age']],df.price)\nreg.predict([[3000,2000,3.0]])","923b6edc":" ##NOW LET'S USE SKLEARN\n","98ffd88f":"Now we want to reduce the m and b and we do that using learning rate and derivatives of m and b","641068dc":"Here's what our cost function looks like-","e9ca7f60":"# LINEAR REGRESSION IN ONE VARIABLE (GRADIENT DESCIENT AND COST FUNCTION","2bb8241a":"WE WILL NOT BE USING THE SKKIT LIBRARY INITIALLY ,BUT WILL CALCULATE THE GRADIENT DESCENT AND COST FUNCTION OURSELVES AND TRY TO PREDICT THE HOUSING PRICES OR ANY OTHER SINGLE VARIABLE REGRESSION PROBLEM BY REDUCING OUR COST FUNCTION AND ARRIVING AT OUR PREDICTED FUNCTION . WE WILL USE THE SKKIT LIBRARY IN THE END TO  PERFORM LINEAR REGRESSION IN ONE VARIABLE AND MULTIPLE VARIABLE AS WELL. LET'S GET STARTED :)\n","832c8204":"As you can see the cost function reduces , we have to alter the iterations and the learning rate ourselves.","19782028":"Here's what the MEAN SQUARED ERROR FUNCTION(COST FUNCTION) is and also derivative of m and b ,so that we minimize it","35855489":"# LINEAR REGRESSION WITH TWO VARIABLES"}}