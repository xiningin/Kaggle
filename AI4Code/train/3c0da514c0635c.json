{"cell_type":{"a9648f31":"code","4eeb84d5":"code","73f0f3ee":"code","a3c471ae":"code","d2dcb07d":"code","8b03e2bc":"code","4c85029e":"code","b65de2d1":"code","969943bc":"code","e6b04887":"code","c349131c":"code","4818ba68":"code","7b16482b":"code","33e09f2a":"code","f6853bcc":"code","8e1ec409":"code","7617eb50":"code","e5f37dd9":"code","db3e01ed":"code","7785c85a":"code","3d58b178":"code","b33bb3bb":"code","e88c2684":"code","603c443b":"code","90d00b28":"code","012aeb0a":"code","5d17f25b":"code","2e4f9ccf":"code","dec67dda":"code","6b7bf71a":"code","59ff6b4b":"code","abfcd168":"code","c7b78eff":"code","6f2c9e37":"code","5fb78392":"code","76285be2":"code","a20c5523":"code","024eab1b":"code","dcc96bc4":"code","ec0b3221":"code","99463ceb":"code","a8a1a069":"code","f3699fac":"code","22e91712":"code","14368fe0":"code","f94e5146":"code","265e564a":"code","da3ce291":"code","46ba2d92":"code","7f3d6e9a":"code","388556df":"code","f2fe6869":"code","38ec6556":"code","9a063fe7":"code","7903aeb1":"code","94bbc105":"code","3d5f213b":"code","279f28a2":"code","3161d0cf":"code","99a13906":"code","74f29154":"code","afc9e1a0":"code","1fa99c1f":"code","c3723bbf":"code","749cf215":"code","04e224b9":"code","6ec7b38f":"code","98f5fddb":"code","a56defc6":"code","847b041e":"code","e1a6a0e0":"code","ebce8855":"code","2e1edf44":"code","ac78bdb5":"code","1fd1b863":"code","76323835":"code","7796e6e6":"code","08e81cc6":"code","4cf8cd49":"code","369b130e":"code","61f2e113":"code","1567995c":"code","b56717ea":"code","87c32cc8":"code","6e09be1e":"code","8e047676":"code","586e002f":"code","58422363":"code","b4b875e2":"code","2b8a91e1":"code","f0ed3209":"code","ddf9e31d":"code","0479ad9b":"code","69d18be6":"code","2827859c":"code","8eb4b0e7":"code","9ab56e29":"markdown","8028e094":"markdown","f9c7bf10":"markdown","92f5a5e3":"markdown","67a09b64":"markdown","9d85bef7":"markdown","6ec43b2a":"markdown","61f5b97b":"markdown","6f957378":"markdown","2a3014e6":"markdown","ca4acc3f":"markdown","5385c89d":"markdown","78ef2b1a":"markdown","8ca95edb":"markdown","c19c8a19":"markdown","b60880cc":"markdown","44dc8fbe":"markdown","e1245d30":"markdown","f98d6e31":"markdown","c4fa239e":"markdown","bcecfe5b":"markdown","265abec7":"markdown"},"source":{"a9648f31":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nimport plotly.express as px\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder","4eeb84d5":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntrain_data.head()","73f0f3ee":"train_data.describe(include='all')","a3c471ae":"train_data.isnull().sum()","d2dcb07d":"#Extracting the title from the name using regex. Since names are unique, it would be better to work with just titles.\ntrain_data['Title'] = train_data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\ntrain_data['Title'].unique()","8b03e2bc":"#Rather than replacing the null values with the median age across the dataset, i choose to replace the null values with the\n#median age for each title.\ntrain_data.groupby(['Title'])['Age'].median()","4c85029e":"train_data['Age'] = train_data['Age'].fillna(train_data.groupby(['Title'])['Age'].transform('median'))\ntrain_data['Age'].isnull().sum()","b65de2d1":"train_data['Embarked'] = train_data['Embarked'].fillna(train_data['Embarked'].mode()[0])\ntrain_data['Embarked'].isnull().sum()","969943bc":"train_data['Title'].replace(to_replace=['Don', 'Rev', 'Dr', 'Major', 'Lady', 'Sir', 'Col', 'Capt', \n                                        'Countess','Jonkheer'], value='Rare', inplace=True)","e6b04887":"#Mlle representing Mademoiselle, according to Google is a title for an unmarried woman, so i am replacing it with 'Miss' which is \n#commonly used for an unmarried woman.\ntrain_data['Title'].replace(to_replace=['Mlle','Ms'], value='Miss', inplace=True)","c349131c":"train_data['Title'].replace({'Master':'Mr'}, inplace=True)\ntrain_data['Title'].replace({'Mme':'Mrs'}, inplace=True) #Madame refers to married woman. Replacing it with 'Mrs' which is more\n#common.\ntrain_data['Title'].unique()","4818ba68":"train_data['Embarked'].replace({'C':'Cherbourg', 'Q':'Queenstown', 'S':'Southampton'}, inplace=True)","7b16482b":"#By adding the SibSp and Parch columns i can get the family size. 1 is added to represent those traveling without a sibling, \n#spouse, parent or child.\ntrain_data['Family_Size'] = train_data['SibSp'] + train_data['Parch'] + 1","33e09f2a":"#People traveling alone would be represented by 1 and those who are traveling with others will be represented by 0.\ntrain_data['Traveling_Alone'] = np.where(train_data['Family_Size'] > 1, 0, 1)","f6853bcc":"#Creating age bins. This would make it easier to perform analysis and it would also be used in my machine learning model.\nbins= [0, 12, 19, 59, np.inf]\n\n#This categorizes children as 0-12, Teenagers as 13-19, Adults 20-59 and Seniors as 60 and above\nlabels= ['Children', 'Teenagers', 'Adults', 'Seniors']\n\ntrain_data['Age_Group'] = pd.cut(train_data['Age'], bins=bins, labels=labels)","8e1ec409":"#Running a check to get the titles in the children age group which should either be Mr or Miss.\ntrain_check1 = train_data[train_data['Age_Group']=='Children']\ntrain_check1['Title'].unique()","7617eb50":"#Running a check to get the titles in teenagers age group which should either be Mr or Miss.\ntrain_check2 = train_data[train_data['Age_Group']=='Teenagers']\ntrain_check2['Title'].unique()","e5f37dd9":"#Checking to see the rows that have title as Mrs. This could be a data collection or data entry error.\ntrain_check2[train_check2['Title']=='Mrs']","db3e01ed":"#Replacing the Title for Teenagers that have 'Mrs' with 'Miss'\ntrain_data['Title'] = np.where((train_data.Age_Group.values == 'Teenagers') & \n                               (train_data.Title.values == 'Mrs'),'Miss', train_data.Title.values)","7785c85a":"#Running a another check to ensure that the title for the teenagers age group is either Mr or Miss\ntrain_check3 = train_data[train_data['Age_Group']=='Teenagers']\ntrain_check3['Title'].unique()","3d58b178":"train_data.rename({'Sex':'Gender'}, axis = 1, inplace = True)","b33bb3bb":"#Dropping columns that won't be used in building the ML model and predicting\ntrain_data.drop(['Cabin','Ticket','Name','SibSp','Parch'], axis=1, inplace=True)","e88c2684":"test_data = pd.read_csv('..\/input\/titanic\/test.csv')\ntest_data.head()","603c443b":"test_data.describe(include='all')","90d00b28":"test_data.isnull().sum()","012aeb0a":"#Extracting the title from the name using regex. Since names are unique, it would be better to work with just titles.\ntest_data['Title'] = test_data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\ntest_data['Title'].unique()","5d17f25b":"test_data['Age'] = test_data['Age'].fillna(test_data.groupby(['Title'])['Age'].transform('median'))\ntest_data['Age'].isnull().sum()","2e4f9ccf":"#Getting the index for the age that still has a null values\nage_with_nan = [index for index, row in test_data[['Age']].iterrows() if row.isnull().any()]\nprint(age_with_nan)","dec67dda":"test_data.iloc[88]","6b7bf71a":"#I'll be imputing the null values with the median age of Ms from the train data which is 28.\ntest_data['Age'].fillna(28,inplace=True)\ntest_data['Age'].isnull().sum()","59ff6b4b":"test_data.groupby(['Pclass'])['Fare'].median()","abfcd168":"#Replacing the null values with the median fare for each Pclass.\ntest_data['Fare'] = test_data['Fare'].fillna(test_data.groupby(['Pclass'])['Fare'].transform('median'))\ntest_data['Fare'].isnull().sum()","c7b78eff":"test_data['Title'].replace(to_replace=['Col', 'Rev', 'Dr', 'Dona'], value='Rare', inplace=True)","6f2c9e37":"test_data['Title'].replace({'Master':'Mr'}, inplace=True)\ntest_data['Title'].replace({'Ms':'Miss'}, inplace=True)\ntest_data['Title'].unique()","5fb78392":"test_data['Embarked'].replace({'C':'Cherbourg', 'Q':'Queenstown', 'S':'Southampton'}, inplace=True)","76285be2":"#By adding the SibSp and Parch columns i can get the family size. 1 is added to represent those traveling without a sibling, \n#spouse, parent or child.\ntest_data['Family_Size'] = test_data['SibSp'] + test_data['Parch'] + 1","a20c5523":"#People traveling alone would be represented by 1 and those who are traveling with others will be represented by 0.\ntest_data['Traveling_Alone'] = np.where(test_data['Family_Size'] > 1, 0, 1)","024eab1b":"#Creating age bins. This would make it easier to perform analysis and it would also be used in my machine learning model.\nbins= [0, 12, 19, 59, np.inf]\n\n#This categorizes children as 0-12, Teenagers as 13-19, Adults 20-59 and Seniors as 60 and above\n\ntest_data['Age_Group'] = pd.cut(test_data['Age'], bins=bins, labels=labels)","dcc96bc4":"#Running a check to get the titles in the children age group which should either be Mr or Miss.\ntest_check1 = test_data[test_data['Age_Group']=='Children']\ntest_check1['Title'].unique()","ec0b3221":"#Running a check to get the titles in the teenagers age group which should either be Mr or Miss.\ntest_check2 = test_data[test_data['Age_Group']=='Teenagers']\ntest_check2['Title'].unique()","99463ceb":"#Checking to see the rows that have title as Mrs. This could be a data collection or data entry error.\ntest_check2[test_check2['Title']=='Mrs']","a8a1a069":"#Replacing the Title for Teenagers that have 'Mrs' with 'Miss'\ntest_data['Title'] = np.where((test_data.Age_Group.values == 'Teenagers') & \n                               (test_data.Title.values == 'Mrs'),'Miss', test_data.Title.values)","f3699fac":"#Running a another check to ensure that the title for the teenager age group is either Mr or Miss\ntest_check3 = test_data[test_data['Age_Group']=='Teenagers']\ntest_check3['Title'].unique()","22e91712":"test_data.rename({'Sex':'Gender'}, axis = 1, inplace = True)","14368fe0":"#Dropping columns that won't be used in building the ML model and predicting\ntest_data.drop(['Cabin','Ticket','Name','SibSp','Parch'], axis=1, inplace=True)","f94e5146":"fig = plt.figure(figsize=(8,6))\nsns.countplot(data= train_data, hue = 'Gender', x = 'Survived')\nplt.xlabel('Dead or Survived', fontsize = 14)\nplt.ylabel('Count', fontsize = 14)\nplt.title('Survival Rate by Gender', fontsize = 18)\nplt.show()\n#Men had a significant death count than females, so definitely women had a higher survival count than men","265e564a":"fig = plt.figure(figsize=(8,6))\nsns.countplot(data= train_data, hue = 'Pclass', x = 'Survived')\nplt.xlabel('Dead or Survived', fontsize = 14)\nplt.ylabel('Count', fontsize = 14)\nplt.title('Survival Rate by Ticket Class', fontsize = 18)\nplt.show()\n#People who had a Pclass 1, that is, a first class ticket had a higher survival count and lower death count than those who had \n# a Pclass 2 or 3, second or third class ticket. Pclass 3 had a significant death toll compared to other Pclass groups.","da3ce291":"fig = plt.figure(figsize=(8,6))\nsns.countplot(data= train_data, hue= 'Traveling_Alone', x = 'Survived')\nplt.xlabel('Dead or Survived', fontsize = 14)\nplt.ylabel('Count', fontsize = 14)\nplt.title('Traveling Alone - Survival Rate', fontsize = 18)\nplt.show()\n#People traveling alone(1) had a higher death count than those traveling with other people. People who traveled with others(0)\n#had a slightly higher chance or surviving that those who traveled alone","46ba2d92":"age_group_survival = train_data[['Age_Group','Survived']].groupby(['Age_Group','Survived']).agg({'Survived':'count'})\nage_group_survival.columns = ['Survival_Rate']\nage_group_survival.reset_index(inplace=True)\nage_group_survival","7f3d6e9a":"fig = px.bar(age_group_survival, x= 'Survived', y='Survival_Rate', \n             color='Age_Group',\n             title='Survival Rate by Age Group',\n             labels = {'Survived':'Dead or Survived', 'Survival_Rate':'Count'},\n             hover_name='Age_Group',\n             hover_data=['Survival_Rate','Survived'],\n             barmode='group',\n             template='plotly_dark',\n             width = 800,\n             height = 400)\n\n#aligning the title position to center\nfig.update(layout = dict(title = dict(x = 0.5)))\n\nfig.show()\n# HOVER OVER THE BARS TO GET MORE INFORMATION","388556df":"train_data['Age_Group'].value_counts()","f2fe6869":"poe_survival = train_data[train_data['Survived']==1]\npoe_survival = train_data.groupby(['Embarked','Pclass']).agg({'Survived':'count'})\npoe_survival.columns = ['Count_of_Survivors']\npoe_survival.reset_index(inplace=True)\npoe_survival","38ec6556":"fig = px.bar(poe_survival, x= 'Pclass', y='Count_of_Survivors', \n             color='Embarked',\n             title='Survival Rate by Point of Embarkation',\n             labels = {'Pclass':'Ticket Class', 'Count_of_Survivors':'Count of Survivors'},\n             hover_name='Embarked',\n             hover_data=['Pclass','Count_of_Survivors'],\n             barmode='group',\n             template='plotly_dark',\n             width = 800,\n             height = 400)\n\n#aligning the title position to center\nfig.update(layout = dict(title = dict(x = 0.5)))\n\nfig.show()\n# HOVER OVER THE BARS TO GET MORE INFORMATION","9a063fe7":"poe_deaths = train_data[train_data['Survived']==0]\npoe_deaths = poe_deaths.groupby(['Embarked','Pclass']).agg({'Survived':'count'})\npoe_deaths.columns = ['Death_Count']\npoe_deaths.reset_index(inplace=True)\npoe_deaths","7903aeb1":"fig = px.bar(poe_deaths, x= 'Pclass', y='Death_Count', \n             color='Embarked',\n             title='Death Rate by Point of Embarkation',\n             labels = {'Pclass':'Ticket Class', 'Death_Count':'Death Count'},\n             hover_name='Embarked',\n             hover_data=['Pclass','Death_Count'],\n             barmode='group',\n             template='plotly_dark',\n             width = 800,\n             height = 400)\n\n#aligning the title position to center\nfig.update(layout = dict(title = dict(x = 0.5)))\n\nfig.show()\n# HOVER OVER THE BARS TO GET MORE INFORMATION","94bbc105":"train_data['Embarked'].value_counts()","3d5f213b":"train_data.head()","279f28a2":"X = train_data.drop(['Survived','PassengerId'], axis=1)\nY = train_data['Survived']","3161d0cf":"#Machine Learning models take just numbers so any string values we have in our data will have to be converted to numbers.\n\n#Using Column Transformer and One Hot Encoder rather than Label Encoder and One Hot Encoder as both give the same results.\n#Using this method is however more effcient since i use just two lines of code.\n\n#One Hot Encoder sorts the values for each column in ascending order and encodes each category based on this order. Eg male and \n#female, female will have a value of 1, 0 and male 0, 1. The output from One Hot Encoding puts the encoded columns first and \n#then the other columns that were not encoded.\n\n#Since i'm dropping passenger id and survived columns for my X, the number of columns will reduce.\n#I will be encoding the gender, embarked, title, age group columns which is represented by [1,4,5,8]\n\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1, 4, 5, 8])], remainder='passthrough')\nX = np.array(ct.fit_transform(X))","99a13906":"print(X[:1])","74f29154":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)","afc9e1a0":"#Scaling the data so that all values are on the same scale\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","1fa99c1f":"log_classifier = LogisticRegression(random_state = 0)\nlog_classifier.fit(X_train, y_train)","c3723bbf":"log_pred = log_classifier.predict(X_test)","749cf215":"log_cm = confusion_matrix(y_test, log_pred)\nprint(log_cm)\naccuracy_score(y_test, log_pred)","04e224b9":"knn_classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nknn_classifier.fit(X_train, y_train)","6ec7b38f":"knn_pred = knn_classifier.predict(X_test)","98f5fddb":"knn_cm = confusion_matrix(y_test, knn_pred)\nprint(knn_cm)\naccuracy_score(y_test, knn_pred)","a56defc6":"svm_classifier = SVC(kernel = 'linear', random_state = 0)\nsvm_classifier.fit(X_train, y_train)","847b041e":"svm_pred = svm_classifier.predict(X_test)","e1a6a0e0":"svm_cm = confusion_matrix(y_test, svm_pred)\nprint(svm_cm)\naccuracy_score(y_test, svm_pred)","ebce8855":"ksvm_classifier = SVC(kernel = 'rbf', random_state = 0)\nksvm_classifier.fit(X_train, y_train)","2e1edf44":"ksvm_pred = ksvm_classifier.predict(X_test)","ac78bdb5":"ksvm_cm = confusion_matrix(y_test, ksvm_pred)\nprint(ksvm_cm)\naccuracy_score(y_test, ksvm_pred)","1fd1b863":"nb_classifier = GaussianNB()\nnb_classifier.fit(X_train, y_train)","76323835":"nb_pred = nb_classifier.predict(X_test)","7796e6e6":"nb_cm = confusion_matrix(y_test, nb_pred)\nprint(nb_cm)\naccuracy_score(y_test, nb_pred)","08e81cc6":"dt_classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\ndt_classifier.fit(X_train, y_train)","4cf8cd49":"dt_pred = dt_classifier.predict(X_test)","369b130e":"dt_cm = confusion_matrix(y_test, dt_pred)\nprint(dt_cm)\naccuracy_score(y_test, dt_pred)","61f2e113":"rf_classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nrf_classifier.fit(X_train, y_train)","1567995c":"rf_pred = rf_classifier.predict(X_test)","b56717ea":"rf_cm = confusion_matrix(y_test, rf_pred)\nprint(rf_cm)\naccuracy_score(y_test, rf_pred)","87c32cc8":"xgb_classifier = XGBClassifier()\nxgb_classifier.fit(X_train, y_train)","6e09be1e":"xgb_pred = xgb_classifier.predict(X_test)","8e047676":"xgb_cm = confusion_matrix(y_test, xgb_pred)\nprint(xgb_cm)\naccuracy_score(y_test, xgb_pred)","586e002f":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,xgb_pred))","58422363":"test_data_X = test_data.copy()\ntest_data_X = test_data_X.drop(['PassengerId'], axis = 1)\ntest_data_X.head()","b4b875e2":"ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1, 4, 5, 8])], remainder='passthrough')\ntest_data_X = np.array(ct.fit_transform(test_data_X))","2b8a91e1":"print(test_data_X[:1])","f0ed3209":"test_data_X = sc.transform(test_data_X)","ddf9e31d":"xgb_test_pred = xgb_classifier.predict(test_data_X)","0479ad9b":"predicted_values = pd.DataFrame(xgb_test_pred , columns=['Survived'])","69d18be6":"titanic_prediction = test_data.merge(predicted_values, left_index = True, right_index = True)","2827859c":"titanic_submission = pd.DataFrame(titanic_prediction[['PassengerId','Survived']])\ntitanic_submission.head()","8eb4b0e7":"titanic_submission.to_csv('my_titanic_submission.csv', index = False)\nprint('Your submission was successfully saved!')","9ab56e29":"## Naive Bayes","8028e094":"## Preprocessing the Train Data","f9c7bf10":"From the two graphs above, Survival Rate by Point of Embarkation and Death Rate by Point of Embarkation, people who embarked at Southampton had the highest deaths and survivors across all ticket class. This can be atrributed to the fact that over 600 people embarked from Southampton. This is a significant number when compared to those who embarked at Cherbourg or Queenstown.","92f5a5e3":"## Import the Libraries","67a09b64":"## Splitting the data and applying feature scaling","9d85bef7":"## Kernel SVM","6ec43b2a":"## KNN ","61f5b97b":"Variable\tDefinition\t                                Key\nsurvival\tSurvival\t                                0 = No, 1 = Yes\npclass\t    Ticket class\t                            1 = 1st, 2 = 2nd, 3 = 3rd\nsex\t        Sex\t\nAge         Age in years\t\nsibsp\t    # of siblings \/ spouses aboard the Titanic\t\nparch\t    # of parents \/ children aboard the Titanic\t\nticket\t    Ticket number\t\nfare\t    Passenger fare\t\ncabin\t    Cabin number\t\nembarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton\n\nVariable Notes\npclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.","6f957378":"# Content\n1. Import the Libraries\n2. Preprocessing the train data\n3. Preprocessing the test data\n4. Exploratory data analysis on the train data\n5. Training the train data with maching learning models\n6. Predicting the test data","2a3014e6":"## Preprocessing the Test Data","ca4acc3f":"# Training the train data with Machine Learning Models","5385c89d":"## Predicting the test dataset","78ef2b1a":"## SVM","8ca95edb":"## Random Forest Classification","c19c8a19":"## XGBoost","b60880cc":"Adults had both a higher count of survivors and deaths. This most likely due to the fact that they had a higher number of individuals in their age_group.","44dc8fbe":"## Logisitc Regression","e1245d30":"## Decision Tree Classification","f98d6e31":"## Encoding categorical variables","c4fa239e":"XGBoost had the highest accuracy (86.59% OR 0.8659) compared to other machine learning models used. It was able to predict 101 people who did not survive(0) accurately and 54 people who survived(1) accurately. It correctly predicted 155 incorrectly predicted 24 in total.","bcecfe5b":"In addition to accuracy, precison, recall, f1-score and support can also be used to measure the classification model\nPrecison measure the ability of the model to not label positive values as negative.\nRecall is the ability of the model to find positive vales.\nF1-score is the weighted mean of precision and recall. The closer to 1, the better.\nSupport is the sum of each class. Eg 0 has a value of 110 which is a sum of (101 + 9)","265abec7":"## Exploratory Data Analysis on Train Data"}}