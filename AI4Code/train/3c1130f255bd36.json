{"cell_type":{"0a804f3d":"code","9f2d5119":"code","32bd2469":"code","f05495aa":"code","51807638":"code","90cdf45a":"code","41b29598":"code","616e672a":"code","a16cba92":"code","7ff6d444":"code","3a9c4cc8":"code","20314bf2":"code","0407290d":"code","7838380f":"code","f8e056b3":"code","eb762fa4":"code","e28365d9":"code","ee37019e":"code","d04980af":"code","49aefe40":"code","2aa65488":"markdown","0c77cf4f":"markdown","0c6fe685":"markdown","48e7217c":"markdown","8c197800":"markdown","038e1a4f":"markdown","5323af65":"markdown","4da3ea33":"markdown","5373eae5":"markdown","6f971620":"markdown","69df34f9":"markdown","65149de4":"markdown","6516f621":"markdown","b901d98e":"markdown","95fa2a8f":"markdown","786049ec":"markdown","2b178a06":"markdown","5538113e":"markdown"},"source":{"0a804f3d":"# important dependencies\nfrom sklearn.model_selection import train_test_split\nimport keras\nfrom keras.applications.vgg16 import VGG16,preprocess_input,decode_predictions\nfrom keras.preprocessing import image\nfrom keras.layers import Flatten,Dropout,Dense\nfrom keras.optimizers import Adam,SGD\nfrom keras import Model\nimport numpy as np\nimport random\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline","9f2d5119":"TRAIN_DIR = '..\/input\/train\/train\/'\nprint ('This is a sample of the dataset file names')\nprint( os.listdir(TRAIN_DIR)[:5])","32bd2469":"sample = plt.imread(os.path.join(TRAIN_DIR,random.choice(os.listdir(TRAIN_DIR))))\nprint ('Visualize a sample of the image')\nprint ('Image shape:',sample.shape)\nplt.imshow(sample)","f05495aa":"f_train, f_valid = train_test_split(os.listdir(TRAIN_DIR), test_size=0.7, random_state=42)","51807638":"# Network input size\nPATCH_DIM = 32","90cdf45a":"# src: https:\/\/stanford.edu\/~shervine\/blog\/keras-how-to-generate-data-on-the-fly\nclass DataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, files, batch_size=32, dim=(224,224), n_channels=3,n_classes=2, shuffle=True):\n        'Initialization'\n        self.dim = dim\n        self.batch_size = batch_size\n        self.files = files\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.shuffle = shuffle\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.files) \/ self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = np.random.choice(len(self.files), self.batch_size)\n        \n        # Find files of IDs\n        batch_files = self.files[indexes]\n\n        # Generate data\n        X, y = self.__data_generation(batch_files)\n\n        return X, y\n\n    def __data_generation(self, batch_files):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n        y = np.empty((self.batch_size), dtype=int)\n\n        # Generate data\n        for i, f in enumerate(batch_files):\n            # Store sample\n            img_path = os.path.join(TRAIN_DIR,f)\n            img = image.load_img(img_path, target_size=self.dim)\n            x = image.img_to_array(img)\n            x = np.expand_dims(x, axis=0)\n            x = preprocess_input(x)\n            x = np.squeeze(x)\n            X[i,:,:,:] = x\n\n            # Store class\n            if 'dog' in f:\n                y[i]=1\n            else:\n                y[i]=0\n                \n        return X, y","41b29598":"training_generator = DataGenerator(np.array(f_train),dim=(PATCH_DIM,PATCH_DIM))","616e672a":"initial_model = VGG16(weights=\"imagenet\", include_top=False ,input_shape = (PATCH_DIM,PATCH_DIM,3))\nlast = initial_model.output\n\nx = Flatten()(last)\nx = Dense(4096, activation='relu')(x)\nx = Dropout(0.4)(x)\nx = Dense(4096, activation='relu')(x)\nx = Dropout(0.4)(x)\npreds = Dense(1, activation='sigmoid')(x)\n\nmodel = Model(initial_model.input, preds)\nmodel.compile(Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])","a16cba92":"model.fit_generator(generator=training_generator,\n                    use_multiprocessing=True,\n                    epochs=4,\n                    workers=8)","7ff6d444":"X_val = np.empty((len(f_valid), PATCH_DIM, PATCH_DIM ,3))\ny_val = np.empty((len(f_valid)), dtype=int)\n\nfor i, f in enumerate(f_valid):\n    # Store sample\n    img_path = os.path.join(TRAIN_DIR,f)\n    img = image.load_img(img_path, target_size=(PATCH_DIM,PATCH_DIM))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n    x = np.squeeze(x)\n    X_val[i,:,:,:] = x\n\n    # Store class\n    if 'dog' in f:\n        y_val[i]=1\n    else:\n        y_val[i]=0","3a9c4cc8":"y_pred = model.predict(X_val)","20314bf2":"y_pred = [(y[0]>=0.5).astype(np.uint8) for y in y_pred]","0407290d":"print('Accuracy without TTA:',np.mean((y_val==y_pred)))","7838380f":"from edafa import ClassPredictor","f8e056b3":"class myPredictor(ClassPredictor):\n    def __init__(self,model,*args,**kwargs):\n        super().__init__(*args,**kwargs)\n        self.model = model\n\n    def predict_patches(self,patches):\n        return self.model.predict(patches)","eb762fa4":"# use orignal image and flipped Left-Right images\n# use arithmetic mean for averaging\nconf = '{\"augs\":[\"NO\",\\\n                \"FLIP_LR\"],\\\n        \"mean\":\"ARITH\"}'","e28365d9":"p = myPredictor(model,conf)","ee37019e":"y_pred_aug = p.predict_images(X_val)","d04980af":"y_pred_aug = [(y[0]>=0.5).astype(np.uint8) for y in y_pred_aug ]","49aefe40":"print('Accuracy with TTA:',np.mean((y_val==y_pred_aug)))","2aa65488":"### Let's explore the dataset!","0c77cf4f":"### Build and train the model\nOur model is reusing VGG16 architecture without the fully connected layers. So we used the weights from imagenet and add our head as shown","0c6fe685":"Now we train the model","48e7217c":"Step 4: Predict images","8c197800":"## A proof of concept on Dogs vs. Cats competition","038e1a4f":"Step 3: Instantiate your class with configuration and whatever parameters needed","5323af65":"![pipeline](https:\/\/preview.ibb.co\/kH61v0\/pipeline.png)","4da3ea33":"TTA is simply to apply different transformations to **test** image like: rotations, flipping and translations. Then feed these different transformed images to the trained model and average the results to get more confident answer. For example, the image shown below applies two transformations (Left-Right flipping and Contrast change) together with the original image. All of these images are passed to the same model and the results are averaged.","5373eae5":"### Now we use edafa (TTA package)","6f971620":"Step 1: Import the predictor suitable for your problem (`ClassPredictor` for Classification and `SegPredictor` for Segmentation)","69df34f9":"## What is Test Time Augmentation ?","65149de4":"In this kernel we will explore the concept of Test Time Augmentation (TTA) and will run an experiment on **Dogs vs. Cats** competition. Compare the results with and without TTA.","6516f621":"[Edafa](https:\/\/github.com\/andrewekhalel\/edafa) is a ready-to-use package for TTA which can be used directly as we will show in the example","b901d98e":"Split data into train and validation sets (70% and 30% respectively)","95fa2a8f":"## Conclusion\nWe can see that the accuracy improved using the exact same model. Thanks to TTA!","786049ec":"### Read validation images and evaluate the model","2b178a06":"### Build data generator that reads batch by batch from disk when needed","5538113e":"Step 2: Inherit predictor class and implement the main function `predict_patches(self,patches)`"}}