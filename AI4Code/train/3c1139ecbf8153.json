{"cell_type":{"b09645c0":"code","dfd01912":"code","88c8722d":"code","a267f24a":"code","58c0f874":"code","4aceff93":"code","b66eb1e9":"code","a8ceca0f":"code","b366396b":"code","0292485d":"code","78f7fbdc":"code","11e74114":"code","4eacb4d9":"code","63c2ede3":"code","f3d9ff39":"code","86d7a259":"markdown","14a5be4a":"markdown","f11891b8":"markdown","543debc5":"markdown","16e272fe":"markdown","daa3c355":"markdown","41ede4b1":"markdown","94764569":"markdown","b4e2f999":"markdown","15343b83":"markdown","c0e0521a":"markdown","88580103":"markdown"},"source":{"b09645c0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport math\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dfd01912":"pd.set_option('display.max_rows', 5)\npd.set_option('display.max_colwidth', -1)\n\nantam = pd.read_csv('..\/input\/antam-stock-market-by-kitto\/ANTM.JK.csv', parse_dates=True)\nantam['Date'] = pd.to_datetime(antam['Date'])\nantam.index = antam['Date']\n\nantam","88c8722d":"pd.set_option('display.max_rows', None)\n\nantam_desc = pd.DataFrame()\nantam_desc['isna'] = antam.isna().sum()\nantam_desc['isnull'] = antam.isnull().sum()\nantam_desc['nunique'] = antam.nunique()\n\nprint(antam.info(verbose=True), '\\n','-'*80,'\\n','-'*80,'\\n', antam_desc)","a267f24a":"antam['Open'] = antam['Open'].interpolate()\nantam['Close'] = antam['Close'].interpolate()\nantam['High'] = antam['High'].interpolate()\nantam['Low'] = antam['Low'].interpolate()\nantam['Adj Close'] = antam['Adj Close'].interpolate()\nantam['Volume'] = antam['Volume'].interpolate()\n\nantam.isna().sum()","58c0f874":"antam.describe()","4aceff93":"fig, ax = plt.subplots(figsize=(10, 5))\n\nax.plot(antam['Date'], antam['Close'])\nax.set_title('Close Prices of ANTM.JK', fontsize = 20)","b66eb1e9":"from tensorflow.keras.layers import Bidirectional, Dropout, Activation, Dense, LSTM\nfrom tensorflow.python.keras.layers import CuDNNLSTM\nfrom tensorflow.keras.models import Sequential\nimport tensorflow as tf\nfrom tensorflow import keras\n\nprint(\"Done\")","a8ceca0f":"antam[\"Close\"]['2020-01-30':'2021'].plot(figsize=(10,5),legend=True)\nantam[\"Close\"]['2021':].plot(figsize=(10,5),legend=True)\nplt.legend(['Training set','Test set'])\nplt.show()","b366396b":"train_set = antam.Close[(antam.index > '2020-01-30') & (antam.index < '2021-01-01')].values.reshape(-1, 1)\nall_set = antam.Close[antam.index > '2020-01-30'].values.reshape(-1, 1)\n\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled_all = scaler.fit_transform(all_set)\nscaled_train = scaler.transform(train_set)\n\nprint(\"Train shape = {}\".format(scaled_train.shape))\nprint(\"All shape = {}\".format(scaled_all.shape))\n\nwindow_size = 10    # Window size = number of previous values to predict the next value\n\ndef generateSequence(sequence, backward):\n    x_train, y_train = list(), list()\n    for i in range(sequence.shape[0]-backward):\n        seq_x, seq_y = sequence[i:i+backward], sequence[i+backward]\n        x_train.append(seq_x)\n        y_train.append(seq_y)\n    x_train = np.array(x_train)\n    y_train = np.array(y_train)\n    x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n    return x_train, y_train\n    \nx_train, y_train = generateSequence(scaled_train, window_size)\nprint(\"x shape = {}\".format(x_train.shape))\nprint(\"y shape = {}\".format(y_train.shape))","0292485d":"model_lstm = Sequential()\nmodel_lstm.add(LSTM(units=50, return_sequences=True, activation='relu', input_shape=(x_train.shape[1], 1)))\nmodel_lstm.add(LSTM(units=50))\nmodel_lstm.add(Dense(1))\n\nmodel_lstm.compile(loss='mean_squared_error', optimizer='adam')\nepoch_history = model_lstm.fit(x_train, y_train, epochs=100, batch_size=36, verbose=2, validation_split=0.1)","78f7fbdc":"train_score = model_lstm.evaluate(x_train, y_train)\nprint('Train Score: %.6f MSE (%.6f RMSE)' % (train_score, math.sqrt(train_score)))","11e74114":"plt.figure(figsize=(15, 5))\nplt.plot(epoch_history.history['loss'])\nplt.plot(epoch_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","4eacb4d9":"antam[\"Close\"]['2020-01-30':'2021'].plot(figsize=(10,5),legend=True)\nantam[\"Close\"]['2021':].plot(figsize=(10,5),legend=True)\nplt.legend(['Training set','Test set'])\nplt.show()\n\ny_train_predicted = model_lstm.predict(x_train)\ny_inverse = scaler.inverse_transform(y_train)\ny_train_predicted_inverse = scaler.inverse_transform(y_train_predicted)\n\nplt.figure(figsize=(10, 5))\nplt.plot(y_inverse.ravel(), label=\"Price\", color='black')\nplt.plot(y_train_predicted_inverse.ravel(), label=\"Predicted Price\", color='blue')\nplt.legend(loc=2)\nplt.show()","63c2ede3":"x_train, y_train = generateSequence(scaled_all, window_size)\n\ny_predicted = model_lstm.predict(x_train)\ny_inverse = scaler.inverse_transform(y_train)\ny_predicted_inverse = scaler.inverse_transform(y_predicted)\n\nplt.figure(figsize=(10, 5))\nplt.plot(y_inverse.ravel(), label=\"Close Price\", color='black')\nplt.plot(pd.Series(y_predicted_inverse[:211].ravel(),index=range(0,211)), label=\"Train Predicted Close Price\", color='blue')\nplt.plot(pd.Series(y_predicted_inverse[211:].ravel(),index=range(211,234)), label=\"Test Predicted Close Price\", color='red')\nplt.legend(loc=2)\nplt.title(\"All data - Prediction at 1 day based on the previous {} days\".format(window_size))","f3d9ff39":"antam['delta'] = antam['Date'] - antam['Date'].shift(1)\n#antam[['Date', 'delta']].head()\nantam['delta'].sum(), antam['Date'].count(), antam['delta'].nunique(), antam['delta'].value_counts()","86d7a259":"## Data Info and Description","14a5be4a":"# Import and Read the Raw Data\n## Import Libraries","f11891b8":"# Data Visualization","543debc5":"## Split Data into Train and Test Sets","16e272fe":"Still on progress and further analysis","daa3c355":"Let's say because of pandemic, the data should only contain on when pandemic started until today. A 11 months from pandemic started is train set, on 30 January 2020, declared the outbreak of COVID-19 to be a Public Health Emergency of International Concern. A month later until end of the data as the test set.","41ede4b1":"# Predict","94764569":"## Import and Read the Data","b4e2f999":"## Fill NaN value","15343b83":"# Predict all data when on Pandemic","c0e0521a":"Still don't know should to handle this missing date or not","88580103":"# Additional"}}