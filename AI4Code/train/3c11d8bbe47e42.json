{"cell_type":{"d1ff0330":"code","9b7bc965":"code","d84c9564":"code","c2f38f29":"code","d68d58d0":"code","e4b553a2":"code","9a93f1bb":"code","f5509216":"code","2c8a774f":"code","11917767":"code","0c912a04":"code","429fe01f":"code","0344db86":"code","4cc36161":"code","2b11bf0f":"code","e1ef0422":"code","a3728621":"code","73e31f11":"code","0b5168f4":"code","3873d168":"code","9ed0e396":"code","2f825448":"code","128297fb":"code","bfb9b21e":"code","24ff4560":"code","cd1e9ac9":"code","662c1bbb":"code","3231bd32":"code","f204ba41":"code","ef2a3d9b":"code","a5aff19d":"code","f5b620a3":"code","779d9d5b":"code","837f7950":"code","32e81cbc":"code","835fea10":"code","f92b5e97":"code","ca8fe1eb":"code","b650cc90":"code","d65937e1":"code","5f666f5b":"code","44f02c56":"code","b5586063":"code","63f9d49d":"code","ac2c14d6":"code","b7a0e25d":"code","8c2fe63e":"code","89897564":"code","db4598b3":"code","ae5daa82":"code","934074a7":"code","70336ebe":"code","42ca560f":"code","0778b6bc":"code","f5772f48":"code","1e1c869a":"code","8fa36edc":"code","27b31781":"code","6a9b92b2":"code","1b4f9237":"code","b6ab06e4":"code","7a5f637a":"code","cc589c2a":"code","ca9e3e6e":"code","eaaef54a":"code","10b70ca0":"code","30efec7f":"markdown","c32641ae":"markdown","f43a7660":"markdown","fc8d7b67":"markdown","82528c16":"markdown","686e1180":"markdown"},"source":{"d1ff0330":"import numpy as np # linear algebra\nimport pandas as pd\nimport cv2\nimport os\nimport math\nimport copy\nimport imageio\nimport seaborn as sns\nfrom glob import glob\nfrom pathlib import Path\nfrom collections import deque\nimport matplotlib.pyplot as plt\nfrom keras.utils import np_utils\nfrom IPython.display import Image\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split","9b7bc965":"import seaborn as sns\nimport plotly.express as px\nfrom IPython.display import SVG\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\n%matplotlib inline","d84c9564":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.datasets import mnist\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.losses import categorical_crossentropy\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.applications import VGG16,inception_v3\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import RMSprop,Adam,Optimizer,Optimizer, SGD\nfrom tensorflow.keras.layers import Input, Lambda,Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,MaxPooling2D,BatchNormalization,\\\n                                    Permute, TimeDistributed, Bidirectional,GRU, SimpleRNN,\\\n                                    LSTM, GlobalAveragePooling2D, SeparableConv2D, ZeroPadding2D, Convolution2D, ZeroPadding2D,Reshape,\\\n                                    Conv2DTranspose, LeakyReLU, Conv1D, AveragePooling1D, MaxPooling1D,Activation, Conv3D,MaxPooling3D\n\n\n","c2f38f29":"Main_Video_Path = Path(\"..\/input\/real-life-violence-situations-dataset\/Real Life Violence Dataset\")\nVideo_Path = list(Main_Video_Path.glob(r\"*\/*.mp4\"))\nVideo_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],Video_Path))\nVideo_Path_Series = pd.Series(Video_Path,name=\"MP4\").astype(str)\nVideo_Labels_Series = pd.Series(Video_Labels,name=\"CATEGORY\")\nMain_MP4_Data = pd.concat([Video_Path_Series,Video_Labels_Series],axis=1)","d68d58d0":"Violence_Data = Main_MP4_Data[Main_MP4_Data[\"CATEGORY\"] == \"Violence\"]\nNonViolence_Data = Main_MP4_Data[Main_MP4_Data[\"CATEGORY\"] == \"NonViolence\"]\n\nViolence_Data = Violence_Data.reset_index()\nNonViolence_Data = NonViolence_Data.reset_index()","e4b553a2":"Violence_Data","9a93f1bb":"Main_Video_Path = Path(\"..\/input\/violencedetectionsystem\")\nVideo_Path = list(Main_Video_Path.glob(r\"*\/*.mp4\"))\nVideo_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],Video_Path))\nVideo_Path_Series = pd.Series(Video_Path,name=\"MP4\").astype(str)\nVideo_Labels_Series = pd.Series(Video_Labels,name=\"CATEGORY\")\nMain_MP4_Data = pd.concat([Video_Path_Series,Video_Labels_Series],axis=1)","f5509216":"Main_MP4_Data[\"CATEGORY\"].replace({'fight':'Violence','noFight':'NonViolence'}, inplace=True)\nVD = Main_MP4_Data[Main_MP4_Data[\"CATEGORY\"] == \"Violence\"]\nNVD = Main_MP4_Data[Main_MP4_Data[\"CATEGORY\"] == \"NonViolence\"]\n\n","2c8a774f":"Violence_Data = Violence_Data.append(VD,ignore_index=True, sort=False)\nNonViolence_Data = NonViolence_Data.append(NVD,ignore_index=True, sort=False)","11917767":"Violence_Data","0c912a04":"NonViolence_Data","429fe01f":"Main_Video_Path3 = Path(\"..\/input\/ucf-crime-full\/Normal_Videos_for_Event_Recognition\")\nVideo_Path = list(Main_Video_Path3.glob(r\"*.mp4\"))\n# Video_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],Video_Path))\nVideo_Path_Series = pd.Series(Video_Path,name=\"mp4\").astype(str)\nVideo_Labels_Series = pd.Series('NonViolence',name=\"CATEGORY\")\nMain_MP4_Data = pd.concat([Video_Path_Series,Video_Labels_Series],axis=1)","0344db86":"Main_MP4_Data['CATEGORY'] = Main_MP4_Data['CATEGORY'].fillna(\"NonViolence\")","4cc36161":"Main_MP4_Data.rename(columns = {'mp4':'MP4'}, inplace = True)","2b11bf0f":"Main_MP4_Data","e1ef0422":"NonViolence_Data = NonViolence_Data.append(Main_MP4_Data,ignore_index=True, sort=False)","a3728621":"NonViolence_Data","73e31f11":"Main_Video_Path3 = Path(\"..\/input\/ucf-crime-full\/Fighting\")\nVideo_Path = list(Main_Video_Path3.glob(r\"*.mp4\"))\n# Video_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],Video_Path))\nVideo_Path_Series = pd.Series(Video_Path,name=\"mp4\").astype(str)\nVideo_Labels_Series = pd.Series('Violence',name=\"CATEGORY\")\nMain_MP4_Data = pd.concat([Video_Path_Series,Video_Labels_Series],axis=1)","0b5168f4":"Main_MP4_Data['CATEGORY'] = Main_MP4_Data['CATEGORY'].fillna(\"Violence\")","3873d168":"Main_MP4_Data.rename(columns = {'mp4':'MP4'}, inplace = True)","9ed0e396":"Main_MP4_Data","2f825448":"Violence_Data = Violence_Data.append(Main_MP4_Data,ignore_index=True, sort=False)","128297fb":"Violence_Data","bfb9b21e":"NonViolence_Data","24ff4560":"\nCdata = NonViolence_Data","cd1e9ac9":"Cdata = Cdata.append(Violence_Data,ignore_index=True, sort=False)","662c1bbb":"Cdata","3231bd32":"Cdata[\"CATEGORY\"].replace({'Violence':1,'NonViolence':0}, inplace=True)","f204ba41":"# Cdata.CATEGORY[Cdata.CATEGORY == 'Violence']","ef2a3d9b":"Violence_Data.MP4[1]","a5aff19d":"! mkdir .\/Frames\n","f5b620a3":"v_train,v_valid = train_test_split(Violence_Data.MP4, test_size=0.1,random_state=17)\nprint(\"v_train {} samples , v_valid {} samples\".format(len(v_train),len(v_valid)))","779d9d5b":"nv_train,nv_valid = train_test_split(NonViolence_Data.MP4, test_size=0.1,random_state=17)\nprint(\"nv_train {} samples , nv_valid {} samples\".format(len(nv_train),len(nv_valid)))","837f7950":"os.mkdir(\".\/Frames\/train\")\nos.mkdir(\".\/Frames\/test\")\nos.mkdir(\".\/Frames\/train\/nonviolence\")\nos.mkdir(\".\/Frames\/train\/violence\")\nos.mkdir(\".\/Frames\/test\/nonviolence\")\nos.mkdir(\".\/Frames\/test\/violence\")","32e81cbc":"def make_frames(df,tag,sset):\n    v = 0\n    for file_video in df:\n        Video_File_Path = file_video\n\n        Video_Caption = cv2.VideoCapture(Video_File_Path)\n        Frame_Rate = 15\n        count = 0\n        temp = []\n        if Video_Caption.isOpened():\n            os.mkdir('.\/Frames\/{}\/{}\/v{}'.format(sset,tag,v)) \n        while Video_Caption.isOpened():\n\n            Current_Frame_ID = Video_Caption.get(1)\n\n            ret,frame = Video_Caption.read()\n\n            if ret != True:\n                break\n\n            if Current_Frame_ID % math.floor(Frame_Rate) == 0:\n                image = cv2.resize(frame,(256,256))\n                cv2.imwrite(\".\/Frames\/{}\/{}\/v{}\/frame{}.jpg\".format(sset,tag,v,count), image)\n                count += 1\n        v += 1\n\n        Video_Caption.release()\n    \n    \n        \n        \n        \n    ","835fea10":"make_frames(v_train,tag='violence',sset='train')","f92b5e97":"make_frames(v_valid,tag='violence',sset='test')","ca8fe1eb":"make_frames(nv_train,tag='nonviolence',sset='train')","b650cc90":"make_frames(nv_valid,tag='nonviolence',sset='test')","d65937e1":"# violence_frame_list = []\n# v = 0\n# for file_video in Violence_Data.MP4:\n#     Video_File_Path = file_video\n    \n#     Video_Caption = cv2.VideoCapture(Video_File_Path)\n#     Frame_Rate = 15\n#     count = 0\n#     temp = []\n#     if Video_Caption.isOpened():\n#         os.mkdir('.\/Frames\/violence\/v{}'.format(v)) \n#     while Video_Caption.isOpened():\n        \n#         Current_Frame_ID = Video_Caption.get(1)\n        \n#         ret,frame = Video_Caption.read()\n        \n#         if ret != True:\n#             break\n            \n#         if Current_Frame_ID % math.floor(Frame_Rate) == 0:\n#             image = cv2.resize(frame,(256,256))\n#             cv2.imwrite(\".\/Frames\/violence\/v{}\/frame{}.jpg\".format(v,count), image)\n#             violence_frame_list.append([\".\/Frames\/violence\/v{}\/frame{}.jpg\".format(v,count),1])\n#             count += 1\n#     v += 1\n        \n#     Video_Caption.release()\n    \n# len(violence_frame_list)","5f666f5b":"sorted(os.listdir(\".\/Frames\/test\"))","44f02c56":"# non_violence_frame_list = []\n# v = 0\n# for file_video in NonViolence_Data.MP4:\n#     Video_File_Path = file_video\n    \n#     Video_Caption = cv2.VideoCapture(Video_File_Path)\n#     Frame_Rate = 10\n#     count = 0\n#     if Video_Caption.isOpened():\n#         os.mkdir('.\/Frames\/nonviolence\/n{}'.format(v)) \n#     while Video_Caption.isOpened():\n        \n#         Current_Frame_ID = Video_Caption.get(1)\n        \n#         ret,frame = Video_Caption.read()\n        \n#         if ret != True:\n#             break\n            \n#         if Current_Frame_ID % math.floor(Frame_Rate) == 0:\n#             image = cv2.resize(frame,(256,256))\n#             cv2.imwrite(\".\/Frames\/nonviolence\/n{}\/frame{}.jpg\".format(v,count), image)\n#             non_violence_frame_list.append([\".\/Frames\/nonviolence\/n{}\/frame{}.jpg\".format(v,count),0])\n#             count += 1\n#     v += 1\n#     Video_Caption.release()\n    \n# len(non_violence_frame_list)","b5586063":"# non_violence_frame_list","63f9d49d":"class Config():\n    def __init__(self):\n        pass\n    \n    num_classes=2\n    labels_to_class = {0:'nonviolence',1:'violence'}\n    class_to_labels = {'nonviolence':0,'violence':1}\n    resize = 224\n    num_epochs =10\n    batch_size =10","ac2c14d6":"train_data_path = '.\/Frames\/train'\ntest_data_path = \".\/Frames\/test\"","b7a0e25d":"if not os.path.exists('data_files'):\n    os.mkdir('data_files')\nif not os.path.exists('data_files\/train'):\n    os.mkdir('data_files\/train') \nif not os.path.exists('data_files\/test'):\n    os.mkdir('data_files\/test') ","8c2fe63e":"num_classes = 2\nlabels_name={'nonviolence':0,'violence':1}","89897564":"data_dir_list = os.listdir(train_data_path)\ndata_dir_list","db4598b3":"def make_train(train_data_path):\n    data_dir_list = os.listdir(train_data_path)\n    for data_dir in data_dir_list: # looping over every activity\n        label = labels_name[str(data_dir)]\n        video_list = os.listdir(os.path.join(train_data_path,data_dir))\n        for vid in video_list: # looping over every video within an activity\n            train_df = pd.DataFrame(columns=['FileName', 'Label', 'ClassName'])\n            img_list = os.listdir(os.path.join(train_data_path,data_dir,vid))\n            for img in img_list:# looping over every frame within the video\n                img_path = os.path.join(train_data_path,data_dir,vid,img)\n                train_df = train_df.append({'FileName': img_path, 'Label': label,'ClassName':data_dir },ignore_index=True)\n            file_name='{}_{}.csv'.format(data_dir,vid)\n            train_df.to_csv('data_files\/train\/{}'.format(file_name))\n            \nmake_train(train_data_path)","ae5daa82":"def make_test(test_data_path):\n    data_dir_list = os.listdir(test_data_path)\n    for data_dir in data_dir_list: # looping over every activity\n        label = labels_name[str(data_dir)]\n        video_list = os.listdir(os.path.join(test_data_path,data_dir))\n        for vid in video_list: # looping over every video within an activity\n            test_df = pd.DataFrame(columns=['FileName', 'Label', 'ClassName'])\n            img_list = os.listdir(os.path.join(test_data_path,data_dir,vid))\n            for img in img_list: # looping over every frame within the video\n                img_path = os.path.join(test_data_path,data_dir,vid,img)\n                test_df = test_df.append({'FileName': img_path, 'Label': label,'ClassName':data_dir },ignore_index=True)\n            file_name='{}_{}.csv'.format(data_dir,vid)\n            test_df.to_csv('data_files\/test\/{}'.format(file_name))\n\nmake_test(test_data_path)","934074a7":"class ActionDataGenerator(object):\n    \n    def __init__(self,root_data_path,temporal_stride=1,temporal_length=16,resize=224):\n        \n        self.root_data_path = root_data_path\n        self.temporal_length = temporal_length\n        self.temporal_stride = temporal_stride\n        self.resize=resize\n    def file_generator(self,data_path,data_files):\n        '''\n        data_files - list of csv files to be read.\n        '''\n        for f in data_files:       \n            tmp_df = pd.read_csv(os.path.join(data_path,f))\n            label_list = list(tmp_df['Label'])\n            total_images = len(label_list) \n            if total_images>=self.temporal_length:\n                num_samples = int((total_images-self.temporal_length)\/self.temporal_stride)+1\n                print ('num of samples from vid seq-{}: {}'.format(f,num_samples))\n                img_list = list(tmp_df['FileName'])\n            else:\n                print ('num of frames is less than temporal length; hence discarding this file-{}'.format(f))\n                continue\n            \n            start_frame = 0\n            samples = deque()\n            samp_count=0\n            for img in img_list:\n                samples.append(img)\n                if len(samples)==self.temporal_length:\n                    samples_c=copy.deepcopy(samples)\n                    samp_count+=1\n                    for t in range(self.temporal_stride):\n                        samples.popleft() \n                    yield samples_c,label_list[0]\n\n    def load_samples(self,data_cat='train'):\n        data_path = os.path.join(self.root_data_path,data_cat)\n        csv_data_files = os.listdir(data_path)\n        file_gen = self.file_generator(data_path,csv_data_files)\n        iterator = True\n        data_list = []\n        while iterator:\n            try:\n                x,y = next(file_gen)\n                x=list(x)\n                data_list.append([x,y])\n            except Exception as e:\n                print ('the exception: ',e)\n                iterator = False\n                print ('end of data generator')\n        return data_list\n    \n    def shuffle_data(self,samples):\n        data = shuffle(samples,random_state=2)\n        return data\n    \n    def preprocess_image(self,img):\n        img = cv2.resize(img,(self.resize,self.resize))\n        img = img\/255\n        return img\n    \n    def data_generator(self,data,batch_size=10,shuffle=True):              \n        \"\"\"\n        Yields the next training batch.\n        data is an array [[img1_filename,img2_filename...,img16_filename],label1], [image2_filename,label2],...].\n        \"\"\"\n        num_samples = len(data)\n        if shuffle:\n            data = self.shuffle_data(data)\n        while True:   \n            for offset in range(0, num_samples, batch_size):\n                #print ('startring index: ', offset) \n                # Get the samples you'll use in this batch\n                batch_samples = data[offset:offset+batch_size]\n                # Initialise X_train and y_train arrays for this batch\n                X_train = []\n                y_train = []\n                # For each example\n                for batch_sample in batch_samples:\n                    # Load image (X)\n                    x = batch_sample[0]\n                    y = batch_sample[1]\n                    temp_data_list = []\n                    for img in x:\n                        try:\n                            img = cv2.imread(img)\n                            #apply any kind of preprocessing here\n                            #img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n                            img = self.preprocess_image(img)\n                            temp_data_list.append(img)\n    \n                        except Exception as e:\n                            print (e)\n                            print ('error reading file: ',img)  \n    \n                    # Read label (y)\n                    #label = label_names[y]\n                    # Add example to arrays\n                    X_train.append(temp_data_list)\n                    y_train.append(y)\n        \n                # Make sure they're numpy arrays (as opposed to lists)\n                X_train = np.array(X_train)\n                #X_train = np.rollaxis(X_train,1,4)\n                y_train = np.array(y_train)\n                y_train = np_utils.to_categorical(y_train, 2)\n\n                # The generator-y part: yield the next training batch            \n                yield X_train, y_train\n","70336ebe":"root_data_path='data_files'\n\ndata_gen_obj=ActionDataGenerator(root_data_path,temporal_stride=2,temporal_length=5)","42ca560f":"train_data = data_gen_obj.load_samples(data_cat='train')\n\ntest_data = data_gen_obj.load_samples(data_cat='test')","0778b6bc":"\nprint('num of train_samples: {}'.format(len(train_data)))\n\nprint('num of train_samples: {}'.format(len(test_data)))\n","f5772f48":"\ntrain_generator = data_gen_obj.data_generator(train_data,batch_size=10,shuffle=True)\n\ntest_generator = data_gen_obj.data_generator(test_data,batch_size=10,shuffle=True)","1e1c869a":"Callback_Stop_Early = tf.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=3,mode='auto')","8fa36edc":"def get_model():\n    # Define model\n    model = Sequential()\n    model.add(Conv3D(32, kernel_size=(3, 3, 3), input_shape=(\n        5,224,224,3), padding='same'))\n    model.add(Activation('relu'))\n    model.add(Conv3D(32, kernel_size=(3, 3, 3), padding='same'))\n    model.add(Activation('relu'))\n    model.add(MaxPooling3D(pool_size=(3, 3, 3), padding='same'))\n    model.add(Dropout(0.25))\n\n    model.add(Conv3D(64, kernel_size=(3, 3, 3), padding='same'))\n    model.add(Activation('relu'))\n    model.add(Conv3D(64, kernel_size=(3, 3, 3), padding='same'))\n    model.add(Activation('relu'))\n    model.add(MaxPooling3D(pool_size=(3, 3, 3), padding='same'))\n    model.add(Dropout(0.25))\n\n    model.add(Flatten())\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(2))\n    model.add(Activation(\"softmax\"))\n    model.compile(loss=categorical_crossentropy,\n                  optimizer=Adam(), metrics=['accuracy'])\n    model.summary()\n    #plot_model(model, show_shapes=True,\n    \n    \n    #           to_file='model.png')\n    return model","27b31781":"\nmodel = get_model()","6a9b92b2":"history = model.fit_generator(train_generator, \n                steps_per_epoch=len(train_data)\/\/10,epochs=10,callbacks=[Callback_Stop_Early],\n                             validation_data = test_generator ,\n                              validation_steps = len(test_data) \/\/ 10,)","1b4f9237":"# datagen = ImageDataGenerator(validation_split=0.2, \n#                              rescale=1.\/255,\n#                              preprocessing_function=preprocess_input)","b6ab06e4":"# train_generator = datagen.flow_from_directory(\".\/Frames\",\n#                                               target_size=(224, 224), color_mode='rgb',\n#                                               class_mode='binary', batch_size=32,)","7a5f637a":"# video = Input(shape=(224,224,3))\n# cnn_base = VGG16(input_shape=(224,224,3),\n#                  weights=\"imagenet\",\n#                  include_top=False)\n# # cnn_out = GlobalAveragePooling2D()(cnn_base.output)\n# cnn_out = cnn_base.output\n# cnn = Model(cnn_base.input,cnn_out)\n# cnn.trainable = False\n\n# model = keras.Sequential()\n# model.add(keras.Input(shape=(224,224,3)))\n# model.add(cnn)\n# model.add(TimeDistributed(Flatten()))\n# model.add(Bidirectional(LSTM(256,return_sequences=True,\n#                                   dropout=0.5,\n#                                   recurrent_dropout=0.5)))\n# model.add(LSTM(256,return_sequences=True,))\n# model.add(Dense(128,\"relu\"))\n# model.add(Dense(32,\"relu\"))\n# model.add(Dense(1, activation=\"sigmoid\"))\n\n# model.summary()","cc589c2a":"# train = datagen.flow_from_directory(\".\/Frames\",\n#                                               target_size=(224, 224), color_mode='rgb',\n#                                               class_mode='binary',shuffle=True, batch_size=32,subset=\"training\" )\n# valid = datagen.flow_from_directory(\".\/Frames\",\n#                                               target_size=(224, 224), color_mode='rgb',\n#                                               class_mode='binary',shuffle=True, batch_size=8,subset=\"validation\")","ca9e3e6e":"# Callback_Stop_Early = tf.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=3,mode='auto')\n\n# model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n\n# history = model.fit(train,\n#                     validation_data=valid,\n#                       callbacks=[Callback_Stop_Early],\n#                       epochs=25)","eaaef54a":"model.save(\".\/vgg16_lstmraks.hdf5\")\n","10b70ca0":"print(history.history.keys())\nsns.set()\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","30efec7f":"## **Video Preprocessing**","c32641ae":"##  **Import Libraries**","f43a7660":"<a href=\".\/vgg16_lstm.hdf5\"> Download File <\/a>","fc8d7b67":"## **Data Importing from different datasets**","82528c16":"### **Merging all different Data**","686e1180":"# Part-3  VDG + VGG16 + LSTM"}}