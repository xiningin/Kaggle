{"cell_type":{"87a9a207":"code","56c90f97":"code","52b44107":"code","041dccda":"code","9eaf6124":"code","2558a175":"code","f372eb22":"code","0b4778c5":"code","cf69e6be":"code","ce3aa586":"code","ba122b49":"code","6727427f":"code","9b7cc5e2":"code","5b4cb95e":"code","00e4c58b":"code","eba4eaab":"code","ec2de806":"code","8dc1e752":"code","c55c2bc3":"code","c04d39fe":"code","07cc5c3f":"code","c1eee8ac":"code","57417237":"code","31072acb":"code","4bb124af":"code","6b939cbc":"code","4a21c86c":"code","8ec5c5b3":"code","6b38515f":"code","7e2857b7":"code","c7bf4057":"code","12684503":"code","20eda88b":"markdown","963c213d":"markdown","837e858f":"markdown","d30ec144":"markdown","ca5ad80a":"markdown","089fbf60":"markdown","790eaa32":"markdown","b8aecf6a":"markdown","af624993":"markdown","d0bee637":"markdown","f1d0c79d":"markdown","a6f8d6e4":"markdown","edd8e9b7":"markdown","6a430052":"markdown","fc096812":"markdown","f3f2f18e":"markdown","932a4a16":"markdown","b30b3169":"markdown","476a8d32":"markdown","6678be3b":"markdown"},"source":{"87a9a207":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","56c90f97":"import pandas as pd\ndf=pd.read_csv('\/kaggle\/input\/reviews\/Restaurant_Reviews.tsv', sep='\\t')","52b44107":"df.isna().count()","041dccda":"blanks = []  # start with an empty list\n\nfor i,lb,rv in df.itertuples():  # iterate over the DataFrame\n    if type(rv)==str:            # avoid NaN values\n        if rv.isspace():         # test 'review' for whitespace\n            blanks.append(i)     # add matching index numbers to the list\n        \nprint(len(blanks), 'blanks: ', blanks)","9eaf6124":"df.head(5)","2558a175":"def clean_reviews(review):\n    \n    # 1. Removing html tags\n    review_text = BeautifulSoup(review).get_text()\n    \n    # 2. Retaining only alphabets.\n    review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n    \n    # 3. Converting to lower case and splitting\n    word_tokens= review_text.lower().split()\n    \n    # 4. Remove stopwords\n    stop_words= set(stopwords.words(\"english\"))     \n    word_tokens= [w for w in word_tokens if not w in stop_words]\n    \n    cleaned_review=\" \".join(word_tokens)\n    return cleaned_review","f372eb22":"import re\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\ndf['Review']=df['Review'].apply(clean_reviews)","0b4778c5":"df.head(5)","cf69e6be":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\n\n# Na\u00efve Bayes:\n#text_clf_nb = Pipeline([('tfidf', TfidfVectorizer()),('clf', MultinomialNB())])\nfrom sklearn.model_selection import train_test_split\ndf1=pd.read_csv('\/kaggle\/input\/reviews\/Restaurant_Reviews.tsv', sep='\\t')\nX1 = df1['Review']\ny1 = df1['Liked']\n\nX1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.33, random_state=42)\n# Linear SVC:\ntext_clf_lsvc = Pipeline([('tfidf', TfidfVectorizer()), ('clf', LinearSVC())])\ntext_clf_lsvc.fit(X1_train, y1_train)","ce3aa586":"#cleaning the text\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\ncorpus = []\nfor i in range(0,1000):\n    review = re.sub('[^a-zA-Z]', ' ', df['Review'][i])\n    review = review.lower()\n    review = review.split()\n    ps = PorterStemmer()\n    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n    review = ' '.join(review)\n    corpus.append(review)\n","ba122b49":"master=[]\nfor e in df['Review']:\n  master.append(e)\nwords=[]\nfor review in master:\n  tok=review.split()\n  print(tok)\n  for r in tok:\n    words.append(r)\n    \nprint(len(set(words)))\n  \nvocab_size=len(set(words))","6727427f":"# Basic packages\nimport pandas as pd \nimport numpy as np\nimport re\nimport collections\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\n# Packages for data preparation\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\n\n# Packages for modeling\nfrom keras import models\nfrom keras import layers\nfrom keras import regularizers","9b7cc5e2":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 1500)\nZ= cv.fit_transform(corpus).todense()\np = df.iloc[:,1].values","5b4cb95e":"df.shape","00e4c58b":"import nltk\nmaxi=-1\nfor str in df['Review']:\n  tokens=nltk.word_tokenize(str)\n  if(maxi<len((tokens))):\n    maxi=len(tokens)\n    \nprint(maxi)","eba4eaab":"import pandas as pd\ndf3=pd.DataFrame(Z)","ec2de806":"df3.head(6)","8dc1e752":"df3.shape","c55c2bc3":"\nfrom keras.layers import Input,Dense\nfrom keras.models import Model\nfrom keras import regularizers\nencoding_dim = 32\ninput_img = Input(shape=(1500,))\n# add a Dense layer with a L1 activity regularizer\nencoded = Dense(encoding_dim, activation='relu',\n                activity_regularizer=regularizers.l1(10e-5))(input_img)\ndecoded = Dense(1500, activation='sigmoid')(encoded)\n\nautoencoder = Model(input_img, decoded)","c04d39fe":"autoencoder.compile(optimizer='adadelta',loss='mse')","07cc5c3f":"autoencoder.fit(df3, df3,epochs=100)","c1eee8ac":"from keras.models import Model, Sequential\nhidden_representation = Sequential()\nhidden_representation.add(autoencoder.layers[0])\n#hidden_representation.add(autoencoder.layers[1])\n#hidden_representation.add(autoencoder.layers[2])\n#hidden_representation.add(autoencoder.layers[3])\n","57417237":"\nperished_hid_rep = hidden_representation.predict(df3)\nprint(len(perished_hid_rep))","31072acb":"from sklearn.model_selection import train_test_split\n\nX = perished_hid_rep[:]\ny = df['Liked']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","4bb124af":"len(perished_hid_rep)","6b939cbc":"from sklearn.model_selection import train_test_split\n\nX1 = df1['Review']\ny1 = df1['Liked']\n\nX1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.33, random_state=42)\n","4a21c86c":"predictions = text_clf_lsvc.predict(X1_test)\nfrom sklearn import metrics\nprint(metrics.confusion_matrix(y1_test,predictions))","8ec5c5b3":"print(metrics.classification_report(y1_test,predictions))\nprint(metrics.accuracy_score(y1_test,predictions))","6b38515f":"import seaborn as sns\nsns.set(font_scale=1.4)\nsns.heatmap(metrics.confusion_matrix(y1_test,predictions), annot=True,annot_kws={\"size\": 16})","7e2857b7":"from sklearn.linear_model import LogisticRegression\n#from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import classification_report, accuracy_score\n#train_x, val_x, train_y, val_y = train_test_split(rep_x, rep_y, test_size=0.2)\nclf = LogisticRegression().fit(X_train, y_train)\npred_y = clf.predict(X_test)\ncm=confusion_matrix(y_test, y_pred)\nprint (classification_report(y_test, pred_y))\nprint (accuracy_score(y_test, pred_y))","c7bf4057":"feedback = \"\"\nnewReview = \"\"\nnewReview = \"Wow The food was good\"\ndef predict(new_review):   \n        new_review = re.sub(\"[^a-zA-Z]\", \" \", new_review)   \n        new_review = new_review.lower().split()\n        new_review = [ps.stem(word) for word in new_review if word not in set(stopwords.words(\"english\"))]   \n        new_review = \" \".join(new_review)   \n        new_review = [new_review]   \n        new_review = cv.transform(new_review).toarray()   \n        if clf.predict(new_review)[0] == 1:\n            return \"Positive\"   \n        else:       \n            return \"Negative\"\n\nfeedback = predict(newReview)\n\nprint(\"This review is: \", feedback) ","12684503":"feedback = \"\"\nnewReview = \"\"\nnewReview = \"The food was very bad\"\ndef predict(new_review):   \n        new_review = re.sub(\"[^a-zA-Z]\", \" \", new_review)   \n        new_review = new_review.lower().split()\n        new_review = [ps.stem(word) for word in new_review if word not in set(stopwords.words(\"english\"))]   \n        new_review = \" \".join(new_review)   \n        new_review = [new_review]   \n        new_review = cv.transform(new_review).toarray()   \n        if clf.predict(new_review)[0] == 1:\n            return \"Positive\"   \n        else:       \n            return \"Negative\"       \n\nfeedback = predict(newReview)\n\nprint(\"This review is: \", feedback) ","20eda88b":"df3 = pd.DataFrame(X_train_seq_trunc,index=X_train_seq_trunc[:,1])\ndf3.head(6)","963c213d":"import pandas as pd\n\ndf3=pd.DataFrame(Z)","837e858f":"import numpy as np\nfrom sklearn.datasets.samples_generator import make_blobs\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nfrom sklearn.svm import LinearSVC\nfrom sklearn import svm\nfrom sklearn.metrics import confusion_matrix\n#svc = LinearSVC()\nsvc=svm.SVC()\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\nprint (classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint (accuracy_score(y_test, y_pred))","d30ec144":"from sklearn.model_selection import train_test_split\n\nX = df3[:]\ny = df['Liked']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","ca5ad80a":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 1500)\nZ= cv.fit_transform(corpus).todense()\np = df.iloc[:,1].values","089fbf60":"X_train_seq_trunc = pad_sequences(encoded_train, maxlen=19,padding='post')\n","790eaa32":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\nZ= vectorizer.fit_transform(corpus).todense()\np=df.iloc[:,1].values\n#print(vectorizer.get_feature_names())\nprint(Z.shape)","b8aecf6a":"from sklearn.metrics import confusion_matrix\nimport seaborn as sn\nimport pandas as pd\nimport matplotlib.pyplot as plt  \ndf_cm = pd.DataFrame(cm, range(2),range(2))\n#plt.figure(figsize = (10,7))\nsn.set(font_scale=1.4)#for label size\nsn.heatmap(df_cm, annot=True,annot_kws={\"size\": 16})# font size\nprint(\"Accuracy Score is :\", accuracy_score(y_test, pred_y))","af624993":"import keras\nfrom keras.preprocessing.text import one_hot,Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\ntokenizer=Tokenizer(num_words=vocab_size)   \na=tokenizer.fit_on_texts(list(df['Review']))\nencoded_train=tokenizer.texts_to_sequences(list(df['Review']))\n\nprint(a)","d0bee637":"X_train_seq_trunc = X_train_seq_trunc \/ np.max(X_train_seq_trunc)\n","f1d0c79d":"Z[0]","a6f8d6e4":"import numpy as np\nfrom sklearn.datasets.samples_generator import make_blobs\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nfrom sklearn.svm import LinearSVC\nfrom sklearn import svm\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import svm\n\n#Create a svm Classifier\n#clf = svm.SVC(kernel='linear')\nsvc = LinearSVC()\nsvc=svm.SVC()\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\nprint (classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint (accuracy_score(y_test, y_pred))","edd8e9b7":"#from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import classification_report, accuracy_score\n#train_x, val_x, train_y, val_y = train_test_split(rep_x, rep_y, test_size=0.2)\nclf = GaussianNB().fit(X_train, y_train)\npred_y = clf.predict(X_test)\nprint (classification_report(y_test, pred_y))\nprint (accuracy_score(y_test, pred_y))","6a430052":"import nltk\nmaxi=-1\nfor str in df['Review']:\n  tokens=nltk.word_tokenize(str)\n  if(maxi<len((tokens))):\n    maxi=len(tokens)\n    \nprint(maxi)","fc096812":"master=[]\nfor e in df['Review']:\n  master.append(e)\nwords=[]\nfor review in master:\n  tok=review.split()\n  print(tok)\n  for r in tok:\n    words.append(r)\n    \nprint(len(set(words)))\n  \nvocab_size=len(set(words))","f3f2f18e":"input_img = Input(shape=(19,))\nencoded = Dense(512, activation='relu')(input_img)\nencoded = Dense(256, activation='relu')(encoded)\nencoded = Dense(128, activation='relu')(encoded)\nencoded = Dense(64, activation='relu')(encoded)\nencoded = Dense(32, activation='relu')(encoded)\n\ndecoded = Dense(64, activation='relu')(encoded)\ndecoded = Dense(128, activation='relu')(encoded)\ndecoded = Dense(256, activation='relu')(encoded)\ndecoded = Dense(512, activation='relu')(decoded)\ndecoded = Dense(19, activation='sigmoid')(decoded)","932a4a16":"print(X_train_seq_trunc[0])","b30b3169":"\nfrom keras.layers import Input,Dense\nfrom keras.models import Model\nencoding_dim=32\ninput_layer = Input(shape=(19, ))\n\nencoder = Dense(encoding_dim, activation=\"tanh\")(input_layer)\n\ndecoder = Dense(19, activation='relu')(encoder)\n\nautoencoder = Model(input_layer,decoder)\n","476a8d32":"type(encoded_train)","6678be3b":"X_train_seq_trunc[0]"}}