{"cell_type":{"01112eab":"code","dd0b9a6b":"code","d7fefaad":"code","e8d3ba68":"code","787c352e":"code","1b308c47":"code","2d8ab149":"code","d49c6cf4":"code","b8d4d7ed":"code","a6587233":"code","5325cce8":"code","f40fbf80":"code","9b59f7f1":"code","32258872":"code","987f53ed":"code","c2631217":"code","4c98ab6f":"code","476b2fe6":"code","13ece52f":"code","2718b9e9":"code","69c6e584":"code","04194453":"markdown","925cda39":"markdown","bd3dce1a":"markdown","831d692e":"markdown","0774f177":"markdown","ec9f03a2":"markdown"},"source":{"01112eab":"import pandas as pd\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\nimport scipy as sp \nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n\nimport os\nprint(os.listdir(\"..\/input\"))","dd0b9a6b":"# Load the data\ntrain_df = pd.read_csv('..\/input\/train.csv', header=0)\ntest_df = pd.read_csv('..\/input\/test.csv', header=0) \nprint('Train:')\nprint(train_df.tail())\nprint()\nprint('Test:')\nprint(test_df.tail())","d7fefaad":"# We'll impute missing values using the median for numeric columns and the most\n# common value for string columns.\n# This is based on some nice code by 'sveitser' at http:\/\/stackoverflow.com\/a\/25562948\nfrom sklearn.base import TransformerMixin\nclass DataFrameImputer(TransformerMixin):\n    def fit(self, X, y=None):\n        self.fill = pd.Series([X[c].value_counts().index[0]\n            if X[c].dtype == np.dtype('O') else X[c].median() for c in X],\n            index=X.columns)\n        return self\n    def transform(self, X, y=None):\n        return X.fillna(self.fill)","e8d3ba68":"feature_columns_to_use = ['Pclass','Sex','Age','Fare','Parch']\nnonnumeric_columns = ['Sex']","787c352e":"# Join the features from train and test together before imputing missing values,\n# in case their distribution is slightly different\nbig_X = train_df[feature_columns_to_use].append(test_df[feature_columns_to_use])\nbig_X_imputed = DataFrameImputer().fit_transform(big_X)","1b308c47":"# XGBoost doesn't (yet) handle categorical features automatically, so we need to change\n# them to columns of integer values.\n# See http:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html#preprocessing for more\n# details and options\nle = LabelEncoder()\nfor feature in nonnumeric_columns:\n    big_X_imputed[feature] = le.fit_transform(big_X_imputed[feature])","2d8ab149":"print(big_X.shape)\nbig_X.tail()","d49c6cf4":"print(big_X_imputed.shape)\nbig_X_imputed.tail()","b8d4d7ed":"# Prepare the inputs for the model\n# train_df.shape[0] is 891. So... \n# train_X = big_X_imputed[0:891].values\ntrain_X = big_X_imputed[0:train_df.shape[0]].values\ntrain_y = train_df['Survived']\n\n# train.df.shape[0] is 891, so set test equal to the rest of the df....\n# test_X = big_X_imputed[891::]\ntest_X = big_X_imputed[train_df.shape[0]::].values","a6587233":"print('tail of train_X (values from big_X_imputed)')\nprint(train_X[-5:])\nprint('shape: ', train_X.shape)\nprint()\n\nprint('tail of train_y:')\nprint(train_y[-5:])\nprint('shape: ', train_y.shape)\nprint()\n\nprint('tail of test_X:')\nprint(test_X[-5:])\nprint('shape: ', test_X.shape)\nprint()\n\nprint('There is no \"Survived\" column in the test data as this is a contest and they did not want to disclose that.')\nprint('So we will not be able to calculate any accuracy stats on the predicted test data.')\nprint(test_df.columns)","5325cce8":"# This example uses the current build of XGBoost, from https:\/\/github.com\/dmlc\/xgboost\nxgb_model = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05).fit(train_X, train_y)\ntrain_pred = xgb_model.predict(train_X)\npredictions = xgb_model.predict(test_X)","f40fbf80":"print('original big_X_imputed:')\nprint(big_X_imputed[-10:])\nprint('shape:', big_X_imputed.shape)\nprint()\n\nprint('train_X (which has the values of the big_X_imputed):')\nprint(train_X[-10:])\nprint('shape:', train_X.shape)\nprint()\n\nprint('train_y:')\nprint(train_y[-10:])\nprint('shape:', train_y.shape)\nprint()\n\nprint('test_X:')\nprint(test_X[-10:])\nprint('shape:', test_X.shape)\nprint()\n\nprint('train_pred:')\nprint(train_pred[-10:])\nprint('shape:', train_pred.shape)\nprint()\n\nprint('predictions:')\nprint(predictions[-10:])\nprint('shape:', predictions.shape)","9b59f7f1":"accuracy = accuracy_score(train_y, train_pred)\nprint(\"Accuracy on Training Data: %.2f%%\" % (accuracy * 100.0))","32258872":"confusion_df = pd.DataFrame(confusion_matrix(train_y, train_pred),\n             columns=['Predicted to Die', 'Predicted to Survive'], index=['Actual Died', 'Actual Survived'])\n\nconfusion_df","987f53ed":"print(classification_report(train_y, train_pred))","c2631217":"dtrain = xgb.DMatrix(train_X)\ndtrain.feature_names = ['Pclass', 'Sex', 'Age', 'Fare', 'Parch']\ndtrain.feature_names","4c98ab6f":"xgb_model.get_booster().get_score()\nmapper = {'f{0}'.format(i): v for i, v in enumerate(dtrain.feature_names)}\nmapped = {mapper[k]: v for k, v in xgb_model.get_booster().get_score().items()}\nmapped","476b2fe6":"# set size of all fonts (but after specifying the ones at the bottom, this will only apply to the f-scores)\nimport seaborn as sns\nsns.set(font_scale = 2)\n\n# set size of chart\nfig, ax = plt.subplots(figsize=(25,8))\n\n# create chart using useful parameters\nxgb.plot_importance(mapped, ax=ax, color='blue', height=0.4, # importance_type='weight', (doesn't change value)\n                    max_num_features=None, grid=True, show_values=True)\n\n# need to use the following (rather than parameters) in order to set fontsize for these \n# as different than the font_scale\nplt.title('Feature Importance by \"weight\"', fontsize = 30)\nplt.yticks(fontsize = 25)\nplt.ylabel('Features', fontsize = 30)\nplt.xticks(fontsize = 25)\nplt.xlabel('f-score', fontsize = 30)","13ece52f":"# plot the output tree via matplotlib, specifying the ordinal number of the target tree\n# converts the target tree to a graphviz instance\nxgb.to_graphviz(xgb_model, num_trees=xgb_model.get_booster().best_iteration)","2718b9e9":"# Kaggle needs the submission to have a certain format;\n# see https:\/\/www.kaggle.com\/c\/titanic-gettingStarted\/download\/gendermodel.csv\n# for an example of what it's supposed to look like.\nsubmission = pd.DataFrame({ 'PassengerId': test_df['PassengerId'],\n                            'Survived': predictions })\nsubmission.to_csv(\"submission.csv\", index=False)","69c6e584":"submission.tail()","04194453":"# Documentation notes:\n# xgboost.plot_importance(booster, ax=None, height=0.2, xlim=None, ylim=None, title='Feature importance', xlabel='F score', ylabel='Features', importance_type='weight', max_num_features=None, grid=True, show_values=True, **kwargs)\n\nPlot importance based on fitted trees.\n\nParameters\n- booster (Booster, XGBModel or dict) \u2013 Booster or XGBModel instance, or dict taken by Booster.get_fscore()\n\n- ax (matplotlib Axes, default None) \u2013 Target axes instance. If None, new figure and axes will be created.\n\n- grid (bool, Turn the axes grids on or off. Default is True (On)) \u2013\n\n\n- importance_type (str, default \"weight\") - How the importance is calculated: either \u201cweight\u201d, \u201cgain\u201d, or \u201ccover\u201d\n    - \u201dweight\u201d is the number of times a feature appears in a tree\n    - \u201dgain\u201d is the average gain of splits which use the feature\n    - \u201dcover\u201d is the average coverage of splits which use the feature where coverage is defined as the number of samples affected by the split\n\n\n- max_num_features (int, default None) \u2013 Maximum number of top features displayed on plot. If None, all features will be displayed.\n\n- height (float, default 0.2) \u2013 Bar height, passed to ax.barh()\n\n- xlim (tuple, default None) \u2013 Tuple passed to axes.xlim()\n\n- ylim (tuple, default None) \u2013 Tuple passed to axes.ylim()\n\n- title (str, default \"Feature importance\") \u2013 Axes title. To disable, pass None.\n\n- xlabel (str, default \"F score\") \u2013 X axis title label. To disable, pass None.\n\n- ylabel (str, default \"Features\") \u2013 Y axis title label. To disable, pass None.\n\n- show_values (bool, default True) \u2013 Show values on plot. To disable, pass False.\n\n- kwargs \u2013 Other keywords passed to ax.barh()","925cda39":"# change feature names back from f0 to Pclass, etc.","bd3dce1a":"# plot the output tree, use to_graphviz to increase size","831d692e":"# Using XGBClassifier to predict survivors of the Titanic.","0774f177":"# Plot feature importance.","ec9f03a2":"# Note that the test data does not have the survived column."}}