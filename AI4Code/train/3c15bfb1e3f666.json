{"cell_type":{"4a1ae4f3":"code","a221a2d0":"code","3297c1b7":"code","74198245":"code","87fd86c2":"code","dcb61f1a":"code","88d112ab":"code","68053b42":"code","531b8fc2":"code","03d0421e":"code","5d627747":"code","7649d0ae":"code","a3b75fc5":"code","db2dce77":"code","1aa58cdc":"code","374d63b7":"code","3180f108":"code","06de31b2":"code","6bb7e49c":"code","ff2aa52b":"code","e75fcf3b":"code","4a90b382":"code","0814026f":"code","e63258be":"code","ae954457":"code","fc3c4160":"code","11d90962":"code","2dc1a75c":"code","efb22718":"code","ee428c87":"code","1d80b563":"code","baef5dda":"code","bdc6fcdc":"code","1f979ea6":"code","c024d3a9":"code","6d524ac9":"code","eb64b88b":"code","9f8f7d48":"code","c0a5fd9c":"code","26c8d7a2":"code","d571a6e7":"code","38f6de1b":"code","1eea3608":"code","82e6f103":"code","f6396bad":"code","f3fc4622":"code","e40609d1":"code","1123e9d7":"code","986c5726":"code","8033caa5":"code","f288c234":"code","b87e48f7":"code","db4079af":"code","3743f7e3":"code","ec498d8a":"code","7082e92d":"code","919ec60c":"code","4e7de0be":"code","8575c318":"code","7ac4faf0":"code","ef0008fe":"code","73e03881":"code","573feaa3":"code","cd79143f":"code","8dd9c374":"code","8a044cea":"code","b9b9f207":"code","170a0f49":"code","68cea711":"code","ad7eba35":"code","9b0193fc":"code","f4970747":"code","c05615e6":"code","56bced34":"code","64db7dde":"code","de89b0da":"code","5470107a":"code","035b86d9":"code","781d7896":"code","166a63af":"code","a3d964c6":"code","8fe74bf0":"code","de81bcc9":"code","0fd40792":"code","dbab39f1":"code","f31fad33":"code","2183a2cd":"code","a69fd863":"code","6ec758c7":"code","6fa213c5":"code","d24a16da":"code","3a4c33c4":"code","77cca1a0":"code","8d391724":"code","a0fa03e2":"code","2c3f4db9":"code","9c1a148f":"code","b4b179d4":"code","21f124cd":"code","051d33a5":"code","ee762c11":"code","8c02bc0b":"code","306bfff3":"code","320f138c":"code","91bd9f58":"code","7ff5f4d5":"code","e478fbf6":"code","83004be0":"code","d713437c":"code","c59aefa1":"code","199ad398":"code","82093e9c":"code","b459a426":"code","04787aaf":"code","b26e25f0":"code","a35b5bc0":"code","4aba4a2d":"code","2270e212":"code","7945dcaf":"code","c0e7b7dc":"code","bcbfc9b5":"code","f633f63b":"code","d97a6479":"code","93691d49":"code","ce118625":"code","0625ac45":"code","75be2429":"code","8024c68e":"code","02fabb52":"code","fe5f16c8":"code","69293c66":"code","17a2196a":"code","334fe0d2":"code","f099db61":"code","a240c9d2":"code","70fe8b1d":"code","8ca95fa6":"markdown","8b7658a1":"markdown","700967f0":"markdown","8125f2b3":"markdown","1198f905":"markdown","664c04bb":"markdown","33679614":"markdown","48201a03":"markdown","90b6bb95":"markdown","0997a7ec":"markdown","dd52372e":"markdown","57bcf38b":"markdown","7c6775a4":"markdown","4e046f9e":"markdown","84fe6411":"markdown","e0c02cbe":"markdown","59f57be6":"markdown","85acd339":"markdown","b676e3fc":"markdown","eb5e0039":"markdown","a2a595ce":"markdown","c6b7b1bd":"markdown","b9f4e3d0":"markdown","9d581750":"markdown","db19e413":"markdown","0a4db28b":"markdown","d67496cf":"markdown","c2ab7d5c":"markdown","5598e8f7":"markdown","36499008":"markdown","959e44bf":"markdown","e1123098":"markdown","d7debcff":"markdown","b34d434b":"markdown","32687af3":"markdown","f8a64d59":"markdown","17fde66e":"markdown","050ea9c3":"markdown","b43256f6":"markdown","5523199a":"markdown","76e910f8":"markdown","7e4664ef":"markdown","6861b9df":"markdown","5b83eb36":"markdown","1628ea34":"markdown","ce223bb1":"markdown","f07a5976":"markdown","5e759ee6":"markdown","dd178b58":"markdown","f400ce0e":"markdown","b2a9574f":"markdown","689b8385":"markdown","a5fe93b0":"markdown","2b4dafa8":"markdown","0493d0e4":"markdown","b1c76ae8":"markdown","9b7d69e4":"markdown","84cfcaa0":"markdown","0397d593":"markdown","a465d118":"markdown","df7cbb85":"markdown","9fcbe891":"markdown","c7b6370e":"markdown","8b099618":"markdown","c3021165":"markdown","6f3dad96":"markdown","6c7ceed8":"markdown","83afcd75":"markdown","1ac2f15a":"markdown","1dac985f":"markdown","284154a5":"markdown","cba2f0ac":"markdown","b4cdc95e":"markdown","5343c12d":"markdown","41e681bf":"markdown","c0585637":"markdown","287afae5":"markdown","5b7612b8":"markdown","f8304d1f":"markdown","1bd94563":"markdown","e34e01ad":"markdown","055430fa":"markdown","fbd84dd9":"markdown","9030e793":"markdown","4157cc8a":"markdown","a33e5901":"markdown","89a1db97":"markdown","3ceff579":"markdown","6d6f6022":"markdown","afd3cc02":"markdown","247fe996":"markdown","9cc699f0":"markdown","91c00259":"markdown","547dcc29":"markdown","bdcd0cf1":"markdown","67fa6df0":"markdown","84adbfa8":"markdown","14b735a2":"markdown"},"source":{"4a1ae4f3":"# importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, binarize\nfrom imblearn.over_sampling import SMOTE\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, InputLayer\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\n# dataframe display settings\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\n# ignoring warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# plotting style\nsns.set_style('darkgrid')\nplt.style.use('ggplot')","a221a2d0":"# reading data from csv and creating a dataframe\ndf = pd.read_csv(\"..\/input\/paysim1\/PS_20174392719_1491204439457_log.csv\")\n\n# dataframe dimensions\nprint(f\"This dataframe has {df.shape[0]} rows and {df.shape[1]} columns.\")","3297c1b7":"# columns and data types\ndf.info()","74198245":"# renaming column\ndf.rename({'oldbalanceOrg':'oldBalanceOrig',\n           'newbalanceOrig':'newBalanceOrig',\n           'oldbalanceDest':'oldBalanceDest',\n           'newbalanceDest':'newBalanceDest'}, axis='columns', inplace=True)","87fd86c2":"# sample rows\ndf.sample(5)","dcb61f1a":"# missing values summary\npd.DataFrame(zip(df.columns,\n                 df.isna().any(),\n                (df.isna().sum() \/ df.shape[0]) * 100),\n             columns=['Column', 'Has Missing Values?', '% Missing Values'])\\\n    .sort_values('% Missing Values', ascending=False)","88d112ab":"# visual summary of % of missing values in each column\n(df.isna().sum() \/ df.shape[0] * 100)\\\n    .sort_values()\\\n    .plot(kind='barh', figsize=(12,8))\nplt.xticks(np.arange(0,110,10), fontsize=15, fontweight='bold')\nplt.xlabel(\"% of missing values\", fontsize=15, fontweight='bold')\nplt.yticks(fontsize=15, fontweight='bold')\nplt.title(\"Summary of Missing Values\", fontsize=20, fontweight='bold')\nplt.show()","68053b42":"# transaction hour\ndf['transactionHour'] = df['step'] % 24\n\n# converting into object type\ndf['transactionHour'] = df['transactionHour'].astype('object')","531b8fc2":"# transaction amount in thousands\ndf['th_amount'] = df['amount'] \/ 1000","03d0421e":"# transaction type\ndf['transactionBetween'] = df['nameDest'].apply(lambda x: 'Customer2Customer' if x[0] == \"C\" else 'Customer2Merchant')","5d627747":"# transaction period\ndf['transactionPeriod'] = df['transactionHour'].apply(lambda x: 'Peak' if x in [2,3,4,5,6] else\n                                                                ('Mid' if x in [22,23,0,1,7,8] else 'Safe'))","7649d0ae":"# origin balance error\ndf['errorBalanceOrig'] = df['newBalanceOrig'] + df['amount'] - df['oldBalanceOrig']","a3b75fc5":"# destination balance error\ndf['errorBalanceDest'] = df['oldBalanceDest'] + df['amount'] - df['newBalanceDest']","db2dce77":"# flag to indicate whether both old and new balance in origin account are zero\ndf['zeroBalanceOrig'] = df['oldBalanceOrig'] + df['newBalanceOrig']\ndf['zeroBalanceOrig'] = df['zeroBalanceOrig'].apply(lambda x: 1 if x == 0 else 0)","1aa58cdc":"# flag to indicate whether both old and new balance in destination account are zero\ndf['zeroBalanceDest'] = df['oldBalanceDest'] + df['newBalanceDest']\ndf['zeroBalanceDest'] = df['zeroBalanceDest'].apply(lambda x: 1 if x == 0 else 0)","374d63b7":"# transaction type\nplt.subplots(1,2, figsize=(18,6))\n\n# countplot to visualize the no. of observations under each class\nplt.subplot(1,2,1)\nax = sns.countplot(df['type'])\nplt.xlabel('Transaction Type', fontsize=15, fontweight='bold')\nplt.xticks(fontsize=12, fontweight='bold')\nplt.ylabel('No. of observations (Millions)', fontsize=15, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.title('No. of obervations in each transaction type', fontsize=18, fontweight='bold')\nfor i in ax.patches:\n    # get_x pulls left or right; get_height pushes up or down\n    ax.text(i.get_x()+0.1, i.get_height(), str(round(i.get_height(), 2)), fontsize=15, color='black')\n\n# pie chart to visualize the percentage distribution of each class\nplt.subplot(1,2,2)\nplt.pie(df['type'].value_counts(), labels=['CASH_OUT','PAYMENT','CASH_IN','TRANSFER','DEBIT'], autopct='%.2f')\nplt.title('Percentage distribution of each transaction type', fontsize=18)\n\n# display plot\nplt.show()\n","3180f108":"# least transaction amount\nprint(\"Least amount transacted:\", df['amount'].min());print()\n\n# highest transaction amount\nprint(\"Highest amount transacted:\", df['amount'].max()); print()\n\n# highest transaction amount for each transaction type\nprint(\"Highest amount transacted in each transaction type:\")\nfor t_type in df['type'].unique():\n    print(f\"\\t* Highest amount transacted in {t_type} type is {df.loc[(df['type'] == t_type), 'amount'].max()}\")","06de31b2":"# number of transactions in each hour\nplt.figure(figsize=(18,6))\nax = (df['transactionHour'].value_counts(sort=False, normalize=True) * 100).round(2).plot(kind='bar')\nfor i in ax.patches:\n    # get_x pulls left or right; get_height pushes up or down\n    ax.text(i.get_x(), i.get_height(), str(round(i.get_height(), 2)) + \"%\", fontsize=12, color='black')\nplt.xlabel('Transaction hour', fontsize=15, fontweight='bold')\nplt.xticks(fontsize=15, fontweight='bold', rotation=0)\nplt.ylabel('% of observations', fontsize=15, fontweight='bold')\nplt.yticks(fontsize=15, fontweight='bold')\nplt.title('No. of obervations at each hour', fontsize=22, fontweight='bold')\nplt.show()","6bb7e49c":"# avg. transaction amount at each hour for each transaction type\nplt.figure(figsize=(18,6))\nsns.lineplot(data=df.groupby(['transactionHour','type']).agg({'amount' : 'mean'}).round(2).reset_index(),\n             x='transactionHour',\n             y='amount',\n             hue='type')\nplt.xlabel('Transaction hour', fontsize=15, fontweight='bold')\nplt.xticks(range(24), range(24),fontsize=15, fontweight='bold', rotation=0)\nplt.ylabel('Avg. transaction amount (millions)', fontsize=15, fontweight='bold')\nplt.yticks(fontsize=15, fontweight='bold')\nplt.title('Avg. transaction amount at each hour', fontsize=22, fontweight='bold')\nplt.show()","ff2aa52b":"# avg. transaction amount at each hour for each transaction type\nplt.figure(figsize=(18,6))\nsns.lineplot(data=df[df['type'].isin(['DEBIT','PAYMENT'])].groupby(['transactionHour','type']).agg({'amount' : 'mean'}).round(2).reset_index(),\n             x='transactionHour',\n             y='amount',\n             hue='type')\nplt.xlabel('Transaction hour', fontsize=15, fontweight='bold')\nplt.xticks(range(24), range(24),fontsize=15, fontweight='bold', rotation=0)\nplt.ylabel('Avg. transaction amount', fontsize=15, fontweight='bold')\nplt.yticks(fontsize=15, fontweight='bold')\nplt.title('Avg. transaction amount at each hour', fontsize=22, fontweight='bold')\nplt.show()","e75fcf3b":"# avg. transaction amount at each hour for each transaction type\nplt.figure(figsize=(18,6))\nsns.barplot(data=df.groupby(['transactionHour','transactionBetween']).size().reset_index(),\n            x='transactionHour',\n            y=0,\n            hue='transactionBetween',\n            dodge=False,\n            alpha=0.75)\nplt.xlabel('Transaction hour', fontsize=15, fontweight='bold')\nplt.xticks(range(24), range(24),fontsize=15, fontweight='bold', rotation=0)\nplt.ylabel('No. of transactions', fontsize=15, fontweight='bold')\nplt.yticks(fontsize=15, fontweight='bold')\nplt.title('Transaction type at each hour', fontsize=22, fontweight='bold')\nplt.show()","4a90b382":"# transaction type vs transaction between\nplt.figure(figsize=(8,6))\npd.crosstab(df['type'], df['transactionBetween'], normalize=0).plot(kind='barh', stacked=True, ax=plt.gca())\nplt.xlabel('Proportion of observations', fontsize=15, fontweight='bold')\nplt.xticks(fontsize=15, fontweight='bold')\nplt.ylabel('', fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.title('Transaction Type vs Transaction Between', fontsize=18, fontweight='bold')\nplt.show()","0814026f":"# subsetting fraudulent transactions\ndf_fraud = df[df['isFraud'] == 1].drop(columns=['isFlaggedFraud'])\n\n# subsetting non-fraudulent transactions\ndf_nonfraud = df[(df['isFraud'] == 0) & (df['type'].isin(['CASH_OUT','TRANSFER']))].drop(columns=['isFlaggedFraud'])\n\n# creating a new dataframe from sub-sampled data\ndf_new = pd.concat([df_fraud, df_nonfraud], axis=0)\n\n# dataframe dimensions\nprint(f\"The new sub-sampled dataframe has {df_new.shape[0]} rows and {df_new.shape[1]} columns.\")","e63258be":"# fraudulent transactions\nplt.subplots(1,2, figsize=(18,6))\n\n# countplot to visualize the no. of observations under each class\nplt.subplot(1,2,1)\nax = df['isFraud'].value_counts().plot(kind='bar')\nplt.xlabel('Transaction Type', fontsize=15, fontweight='bold')\nplt.xticks(ticks=[0,1], labels=['Non-Fraud','Fraud'], rotation=0, fontsize=12, fontweight='bold')\nplt.ylabel('No. of observations (Millions)', fontsize=15, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.title('No. of obervations in each class', fontsize=18, fontweight='bold')\nfor i in ax.patches:\n    # get_x pulls left or right; get_height pushes up or down\n    ax.text(i.get_x()+0.1, i.get_height()+3, str(round(i.get_height(), 2)), fontsize=15, color='black')\n\n# pie chart to visualize the percentage distribution of each class\nplt.subplot(1,2,2)\nplt.pie(df['isFraud'].value_counts(), labels=['Fraud','Non-Fraud'], autopct='%.2f')\nplt.title('Percentage distribution of each class', fontsize=18, fontweight='bold')\n\n# display plot\nplt.show()","ae954457":"# min and max fraud transaction amount\nprint(\"Min fraud transaction amount:\", df_fraud['amount'].min());print()\nprint(\"Max fraud transaction amount:\", df_fraud['amount'].max())","fc3c4160":"# where does fraud happen?\nplt.subplots(1,2, figsize=(18,6))\n\n# transaction type\nplt.subplot(1,2,1)\nax=df_fraud.groupby('type').size().plot(kind='bar')\nplt.xlabel('Transaction Type', fontsize=15, fontweight='bold')\nplt.xticks(rotation=0, fontsize=12, fontweight='bold')\nplt.ylabel('No. of observations', fontsize=15, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.title('No. of obervations in each transaction type', fontsize=18, fontweight='bold')\nfor i in ax.patches:\n    # get_x pulls left or right; get_height pushes up or down\n    ax.text(i.get_x()+0.17, i.get_height()+3, str(round(i.get_height(), 2)), fontsize=15, color='black')\n\n# transaction between\nplt.subplot(1,2,2)\nax=df_fraud.groupby('transactionBetween').size().plot(kind='bar')\nplt.xlabel('Transaction Between', fontsize=15, fontweight='bold')\nplt.xticks(rotation=0, fontsize=12, fontweight='bold')\nplt.ylabel('No. of observations', fontsize=15, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.title('No. of obervations in each transaction type', fontsize=18, fontweight='bold')\nfor i in ax.patches:\n    # get_x pulls left or right; get_height pushes up or down\n    ax.text(i.get_x()+0.22, i.get_height()+3, str(round(i.get_height(), 2)), fontsize=15, color='black')\n\nplt.show()","11d90962":"# when does fraud happen? (time of day)\nplt.subplots(2,1, figsize=(18,12))\n\n# actual count of fraud cases\nplt.subplot(2,1,1)\nax=df_fraud.groupby('transactionHour').size().plot(kind='bar')\nplt.xlabel('Transaction hour', fontsize=15, fontweight='bold')\nplt.xticks(range(24), range(24),fontsize=15, fontweight='bold', rotation=0)\nplt.ylabel('No. of fraudulent transactions', fontsize=15, fontweight='bold')\nplt.yticks(fontsize=15, fontweight='bold')\nplt.title('Fraudelent transaction at each hour', fontsize=22, fontweight='bold')\nfor i in ax.patches:\n    # get_x pulls left or right; get_height pushes up or down\n    ax.text(i.get_x(), i.get_height()+3, str(round(i.get_height(), 2)), fontsize=14, color='black')\n    \n# proportion of fraud\nplt.subplot(2,1,2)\nax=(df_fraud.groupby('transactionHour').size() \/ df_new.groupby('transactionHour').size() * 100).plot(kind='bar')\nplt.xlabel('Transaction hour', fontsize=15, fontweight='bold')\nplt.xticks(range(24), range(24),fontsize=15, fontweight='bold', rotation=0)\nplt.ylabel('% of fraudulent transactions', fontsize=15, fontweight='bold')\nplt.yticks(fontsize=15, fontweight='bold')\nplt.title('Fraudelent transaction at each hour', fontsize=22, fontweight='bold')\nfor i in ax.patches:\n    # get_x pulls left or right; get_height pushes up or down\n    ax.text(i.get_x(), i.get_height()+1, str(round(i.get_height(), 2))+\"%\", fontsize=12, color='black')\n\nplt.show()","2dc1a75c":"# when does fraud happen? (time of day)\nplt.figure(figsize=(18,6))\nax=sns.barplot(data=df_fraud.groupby(['transactionHour','type']).size().reset_index(),\n               x='transactionHour',\n               y=0,\n               hue='type')\nplt.xlabel('Transaction hour', fontsize=15, fontweight='bold')\nplt.xticks(range(24), range(24),fontsize=15, fontweight='bold', rotation=0)\nplt.ylabel('No. of fraudulent transactions', fontsize=15, fontweight='bold')\nplt.yticks(fontsize=15, fontweight='bold')\nplt.title('Fraudelent transaction at each hour', fontsize=22, fontweight='bold')\nplt.show()","efb22718":"# fraudulent transaction amount last digit\nplt.subplots(2,1, figsize=(12,12))\n\n# actual counts\nplt.subplot(2,1,1)\nax=np.floor(df_fraud['amount']).astype('str').str[-3].value_counts().sort_index().plot(kind='bar')\nplt.xlabel('Amount ending digit', fontsize=15, fontweight='bold')\nplt.xticks(fontsize=15, fontweight='bold', rotation=0)\nplt.ylabel('No. of observations', fontsize=15, fontweight='bold')\nplt.yticks(fontsize=15, fontweight='bold')\nplt.title('Fraudelent transaction amount ending digit', fontsize=22, fontweight='bold')\nfor i in ax.patches:\n    # get_x pulls left or right; get_height pushes up or down\n    ax.text(i.get_x(), i.get_height(), str(round(i.get_height(), 2)), fontsize=14, color='black')\n\n# proportion of fraud\nplt.subplot(2,1,2)\nax=(np.floor(df_fraud['amount']).astype('str').str[-3].value_counts().sort_index() \/ np.floor(df_new['amount']).astype('str').str[-3].value_counts().sort_index() * 100).plot(kind='bar')\nplt.xlabel('Amount ending digit', fontsize=15, fontweight='bold')\nplt.xticks(fontsize=15, fontweight='bold', rotation=0)\nplt.ylabel('% of observations', fontsize=15, fontweight='bold')\nplt.yticks(fontsize=15, fontweight='bold')\nplt.title('Fraudelent transaction amount ending digit', fontsize=22, fontweight='bold')\nfor i in ax.patches:\n    # get_x pulls left or right; get_height pushes up or down\n    ax.text(i.get_x(), i.get_height(), str(round(i.get_height(), 2))+\"%\", fontsize=14, color='black')\n\nplt.show()","ee428c87":"# no. of fraud transactions per destination account\ndf_fraud.groupby('nameDest').size().value_counts().reset_index().rename(columns={'index':'No. of Fraudulent Transactions per Destination Account',\n                                                                                 0:'No. of Unique Accounts'})","1d80b563":"# zero old and new destination balance, but with non-zero transaction amount\nprint(\"Proportion of fraudulent transactions where transaction amount is non-zero, but both old balance and new balance in destination account are zero:\",\n      sum((df_fraud['oldBalanceDest'] == 0) & (df_fraud['newBalanceDest'] == 0) & (df_fraud['amount'] != 0)) \/ df_fraud.shape[0])\nprint()\nprint(\"Proportion of non fraudulent transactions where transaction amount is non-zero, but both old balance and new balance in destination account are zero:\",\n      sum((df_nonfraud['oldBalanceDest'] == 0) & (df_nonfraud['newBalanceDest'] == 0) & (df_nonfraud['amount'] != 0)) \/ df_nonfraud.shape[0])","baef5dda":"# zero old and new destination balance, but with non-zero transaction amount\nprint(\"Proportion of fraudulent transactions where transaction amount is non-zero, but both old balance and new balance in origin account are zero:\",\n      sum((df_fraud['oldBalanceOrig'] == 0) & (df_fraud['newBalanceOrig'] == 0) & (df_fraud['amount'] != 0)) \/ df_fraud.shape[0])\nprint()\nprint(\"Proportion of non fraudulent transactions where transaction amount is non-zero, but both old balance and new balance in origin account are zero:\",\n      sum((df_nonfraud['oldBalanceOrig'] == 0) & (df_nonfraud['newBalanceOrig'] == 0) & (df_nonfraud['amount'] != 0)) \/ df_nonfraud.shape[0])","bdc6fcdc":"# error in origin account balance\nprint(\"Proportion of fraudulent transactions where transaction amount is non-zero, and there is an error in origin account balance amount:\",\n      sum((df_fraud['errorBalanceOrig'] != 0) & (df_fraud['amount'] != 0)) \/ df_fraud.shape[0])\nprint()\nprint(\"Proportion of non fraudulent transactions where transaction amount is non-zero, and there is an error in origin account balance amount:\",\n      sum((df_nonfraud['errorBalanceOrig'] != 0) & (df_nonfraud['amount'] != 0)) \/ df_nonfraud.shape[0])","1f979ea6":"# error in destination account balance\nprint(\"Proportion of fraudulent transactions where transaction amount is non-zero, and there is an error in destination account balance amount:\",\n      sum((df_fraud['errorBalanceDest'] != 0) & (df_fraud['amount'] != 0)) \/ df_fraud.shape[0])\nprint()\nprint(\"Proportion of non fraudulent transactions where transaction amount is non-zero, and there is an error in destination account balance amount:\",\n      sum((df_nonfraud['errorBalanceDest'] != 0) & (df_nonfraud['amount'] != 0)) \/ df_nonfraud.shape[0])","c024d3a9":"# average error in origin account balance\nprint(\"Average absolute error in origin account balance amount in fraudulent data:\", round(abs(df_fraud['errorBalanceOrig']).mean(),2))\nprint()\nprint(\"Average absolute error in origin account balance amount in non fraudulent data:\", round(abs(df_nonfraud['errorBalanceOrig']).mean(), 2))","6d524ac9":"# average error in destination account balance\nprint(\"Average absolute error in destination account balance amount in fraudulent data:\", round(abs(df_fraud['errorBalanceDest']).mean(), 2))\nprint()\nprint(\"Average absolute error in destination account balance amount in non fraudulent data:\", round(abs(df_nonfraud['errorBalanceDest']).mean(), 2))","eb64b88b":"# summary statistics\nprint(\"Summary statistics:\\n\", df_new['amount'].describe().round(2))\nprint(\"-\"*100)\n\n# skewness\nprint(\"Skew: \", df_new['amount'].skew().round(2))\n\n# shape of distribution\nplt.subplots(1,2, figsize=(15,6))\n# distribution using histogram\nplt.subplot(1,2,1)\nsns.distplot(df_new['amount'])\nplt.axvline(df_new['amount'].mean(), color=\"g\", label=\"Mean\")\nplt.axvline(df_new['amount'].median(), color=\"r\", label=\"Median\")\nplt.legend()\n\n# boxplot\nplt.subplot(1,2,2)\nsns.boxplot(df_new['amount'], whis=3)\n\n# display plot\nplt.show()","9f8f7d48":"# summary statistics\nprint(\"Summary statistics:\\n\", np.log1p(df_new['th_amount']).describe().round(2))\nprint(\"-\"*100)\n\n# skewness\nprint(\"Skew: \", np.log1p(df_new['th_amount']).skew().round(2))\n\n# shape of distribution\nplt.subplots(1,2, figsize=(15,6))\n# distribution using histogram\nplt.subplot(1,2,1)\nsns.distplot(np.log1p(df_new['th_amount']))\nplt.axvline(np.log1p(df_new['th_amount']).mean(), color=\"g\", label=\"Mean\")\nplt.axvline(np.log1p(df_new['th_amount']).median(), color=\"r\", label=\"Median\")\nplt.legend()\n\n# boxplot\nplt.subplot(1,2,2)\nsns.boxplot(np.log1p(df_new['th_amount']), whis=3)\n\n# display plot\nplt.show()","c0a5fd9c":"# log transformed amount\ndf_new['ln_amount'] = np.log1p(df_new['th_amount'])","26c8d7a2":"# summary statistics\nprint(\"Summary statistics:\\n\", df_new['errorBalanceOrig'].describe().round(2))\nprint(\"-\"*100)\n\n# skewness\nprint(\"Skew: \", df_new['errorBalanceOrig'].skew().round(2))\n\n# shape of distribution\nplt.subplots(1,2, figsize=(15,6))\n# distribution using histogram\nplt.subplot(1,2,1)\nsns.distplot(df_new['errorBalanceOrig'])\nplt.axvline(df_new['errorBalanceOrig'].mean(), color=\"g\", label=\"Mean\")\nplt.axvline(df_new['errorBalanceOrig'].median(), color=\"r\", label=\"Median\")\nplt.legend()\n\n# boxplot\nplt.subplot(1,2,2)\nsns.boxplot(df_new['errorBalanceOrig'], whis=3)\n\n# display plot\nplt.show()","d571a6e7":"# correcting few negative errors\ndf_new['errorBalanceOrig'] = df_new['errorBalanceOrig'].apply(lambda x: 0 if x < 0 else x)","38f6de1b":"# summary statistics\nprint(\"Summary statistics:\\n\", np.log1p(df_new['errorBalanceOrig']).describe().round(2))\nprint(\"-\"*100)\n\n# skewness\nprint(\"Skew: \", np.log1p(df_new['errorBalanceOrig']).skew().round(2))\n\n# shape of distribution\nplt.subplots(1,2, figsize=(15,6))\n# distribution using histogram\nplt.subplot(1,2,1)\nsns.distplot(np.log1p(df_new['errorBalanceOrig']))\nplt.axvline(np.log1p(df_new['errorBalanceOrig']).mean(), color=\"g\", label=\"Mean\")\nplt.axvline(np.log1p(df_new['errorBalanceOrig']).median(), color=\"r\", label=\"Median\")\nplt.legend()\n\n# boxplot\nplt.subplot(1,2,2)\nsns.boxplot(np.log1p(df_new['errorBalanceOrig']), whis=3)\n\n# display plot\nplt.show()","1eea3608":"# log transformed origin account balance error\ndf_new['ln_errorBalanceOrig'] = np.log1p(df_new['errorBalanceOrig'])","82e6f103":"# summary statistics\nprint(\"Summary statistics:\\n\", df_new['errorBalanceDest'].describe().round(2))\nprint(\"-\"*100)\n\n# skewness\nprint(\"Skew: \", df_new['errorBalanceDest'].skew().round(2))","f6396bad":"# summary statistics\nprint(\"Summary statistics:\\n\", np.log1p(df_new['errorBalanceDest']).describe().round(2))\nprint(\"-\"*100)\n\n# skewness\nprint(\"Skew: \", np.log1p(df_new['errorBalanceDest']).skew().round(2))","f3fc4622":"# log transformed origin account balance error\ndf_new['ln_errorBalanceDest'] = np.log1p(df_new['errorBalanceDest'])\n\n# correcting errors\ndf_new['ln_errorBalanceDest'] = df_new['ln_errorBalanceDest'].apply(lambda x: 0 if x < 0 else x)\ndf_new['ln_errorBalanceDest'].fillna(0, inplace=True)","e40609d1":"# heat map of correlation among variables\nplt.figure(figsize=(8,8))\nsns.heatmap(df_new.loc[:, 'zeroBalanceOrig':].corr(),\n            fmt='.2f', annot=True, square=True, cmap='coolwarm', cbar=False)\nplt.xticks(fontsize=15, fontweight='bold', rotation=45)\nplt.yticks(fontsize=15, fontweight='bold')\nplt.title('Correlation among variables', fontsize=22, fontweight='bold')\nplt.show()","1123e9d7":"# subsetting variables for modelling\ndf_model = df_new[['type', 'transactionPeriod',\n                   'zeroBalanceOrig', 'zeroBalanceDest',\n                   'ln_errorBalanceOrig', 'ln_errorBalanceDest', 'ln_amount',\n                   'isFraud']]\n\n# dataframe dimensions\nprint(f\"The dataframe for modelling has {df_model.shape[0]} rows and {df_model.shape[1]} columns.\")\nprint()\nprint(\"Features that are finally selected for our model:\\n\", list(df_model.columns)[:-1])","986c5726":"# class imbalance\nplt.subplots(1,2, figsize=(18,6))\n\n# countplot to visualize the no. of observations under each class\nplt.subplot(1,2,1)\nax = df['isFraud'].value_counts().plot(kind='bar')\nplt.xlabel('Transaction Type', fontsize=15, fontweight='bold')\nplt.xticks(ticks=[0,1], labels=['Non-Fraud','Fraud'], rotation=0, fontsize=12, fontweight='bold')\nplt.ylabel('No. of observations (Millions)', fontsize=15, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.title('No. of obervations in each transaction type', fontsize=18, fontweight='bold')\nfor i in ax.patches:\n    # get_x pulls left or right; get_height pushes up or down\n    ax.text(i.get_x()+0.1, i.get_height()+3, str(round(i.get_height(), 2)), fontsize=15, color='black')\n\n# pie chart to visualize the percentage distribution of each class\nplt.subplot(1,2,2)\nplt.pie(df['isFraud'].value_counts(), labels=['Fraud','Non-Fraud'], autopct='%.2f')\nplt.title('Percentage distribution of each transaction type', fontsize=18, fontweight='bold')\n\n# display plot\nplt.show()","8033caa5":"# identifying dependent and independent features\nX = df_model.loc[:, :'ln_amount']\ny = df_model['isFraud']\n\n# splitting data into train (80%) and test (20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)","f288c234":"# creating train and test dataframes\ndf_train = X_train\ndf_train['isFraud'] = y_train\n\ndf_test = X_test\ndf_test['isFraud'] = y_test\n\n# dataframe dimensions\nprint(f\"Dimensions of original train set: {df_train.shape[0]} rows and {df_train.shape[1]} columns\")\nprint(f\"Dimensions of original test set: {df_test.shape[0]} rows and {df_test.shape[1]} columns\")","b87e48f7":"# down-sampling majority class\ndf_train_downsampled = df_train[df_train['isFraud'] == 0].sample(250000, random_state=1)","db4079af":"# down sampling majority class\ndf_train_downsampled_bootstrap = pd.concat([df_train_downsampled, df_train[df_train['isFraud'] == 1].sample(250000, replace=True, random_state=1)],\n                                           axis=0)\n \n# dataframe dimensions\nprint(f\"After downsampling the majority class and upsampling the minority class using bootstrap, the balanced train set has {df_train_downsampled_bootstrap.shape[0]} rows and {df_train_downsampled_bootstrap.shape[1]} columns.\")","3743f7e3":"# appending the fraud data\ndf_train_downsampled = pd.concat([df_train_downsampled, df_train[df_train['isFraud'] == 1]])\n\n# dataframe dimensions\nprint(f\"After downsampling the majority class, imbalanced train set has {df_train_downsampled.shape[0]} rows and {df_train_downsampled.shape[1]} columns.\")","ec498d8a":"# preprocessing data for SMOTE\nX_train_smote = df_train_downsampled[['zeroBalanceOrig', 'zeroBalanceDest', 'ln_errorBalanceOrig', 'ln_errorBalanceDest', 'ln_amount']].round(3)\n\nX_train_smote['type_Transfer'] = df_train_downsampled['type'].replace({'CASH_OUT' : 0,\n                                                                       'TRANSFER' : 1})\n\nX_train_smote['transactionPeriod_Mid'] = df_train_downsampled['transactionPeriod'].apply(lambda x: 1 if x == 'Mid' else 0)\nX_train_smote['transactionPeriod_Peak'] = df_train_downsampled['transactionPeriod'].apply(lambda x: 1 if x == 'Peak' else 0)\n\n# initializing SMOTE object\nsmote = SMOTE(random_state=33)\n\n# fitting data with SMOTE and creating synthetic over-sampled data set\nX, y = smote.fit_sample(X_train_smote, df_train_downsampled['isFraud'])\n\n# concatnating data and creating train df\ndf_train_downsampled_smote = pd.concat([X, y], axis=1)\n\n# shuffling data\ndf_train_downsampled_smote = df_train_downsampled_smote.sample(frac=1, random_state=1)\n\n# dataframe dimensions\nprint(f\"After downsampling the majority class and upsampling the minority class using SMOTE, the balanced train set has {df_train_downsampled_smote.shape[0]} rows and {df_train_downsampled_smote.shape[1]} columns.\")","7082e92d":"# initializing standard scaler\nss = StandardScaler()\n\n# fitting scaler on train data\nss.fit(df_train_downsampled[['ln_errorBalanceOrig','ln_errorBalanceDest','ln_amount']])\n\n# standardizing test data\nX_test = pd.DataFrame(ss.transform(df_test[['ln_errorBalanceOrig','ln_errorBalanceDest','ln_amount']]),\n                      columns=['ln_errorBalanceOrig','ln_errorBalanceDest','ln_amount'])\n\n# creating dummies for test data\nX_test['zeroBalanceOrig'] = df_test['zeroBalanceOrig'].reset_index(drop=True)\nX_test['zeroBalanceDest'] = df_test['zeroBalanceDest'].reset_index(drop=True)\n\nX_test['type_Transfer'] = df_test['type'].replace({'CASH_OUT' : 0,\n                                                   'TRANSFER' : 1})\\\n                                         .reset_index(drop=True)\n\nX_test['transactionPeriod_Mid'] = df_test['transactionPeriod'].apply(lambda x: 1 if x == 'Mid' else 0)\\\n                                                              .reset_index(drop=True)\nX_test['transactionPeriod_Peak'] = df_test['transactionPeriod'].apply(lambda x: 1 if x == 'Peak' else 0)\\\n                                                               .reset_index(drop=True)\n\n# target variable\ny_test = df_test['isFraud'].reset_index(drop=True)\n\n# dataframe shape\nprint(f\"After standardizing and encoding the test data, this dataframe has {X_test.shape[0]} rows and {X_test.shape[1]} columns.\")\n\n# df_train_downsampled_bootstrap\n# standardizing train data\nX_train_downsampled_bootstrap = pd.DataFrame(ss.transform(df_train_downsampled_bootstrap[['ln_errorBalanceOrig','ln_errorBalanceDest','ln_amount']]),\n                                             columns=['ln_errorBalanceOrig','ln_errorBalanceDest','ln_amount'])\n\n# creating dummies for test data\nX_train_downsampled_bootstrap['zeroBalanceOrig'] = df_train_downsampled_bootstrap['zeroBalanceOrig']\\\n                                                        .reset_index(drop=True)\nX_train_downsampled_bootstrap['zeroBalanceDest'] = df_train_downsampled_bootstrap['zeroBalanceDest']\\\n                                                        .reset_index(drop=True)\n\nX_train_downsampled_bootstrap['type_Transfer'] = df_train_downsampled_bootstrap['type'].replace({'CASH_OUT' : 0,\n                                                                                                 'TRANSFER' : 1})\\\n                                                                                       .reset_index(drop=True)\n\nX_train_downsampled_bootstrap['transactionPeriod_Mid'] = df_train_downsampled_bootstrap['transactionPeriod']\\\n                                                            .apply(lambda x: 1 if x == 'Mid' else 0)\\\n                                                            .reset_index(drop=True)\nX_train_downsampled_bootstrap['transactionPeriod_Peak'] = df_train_downsampled_bootstrap['transactionPeriod']\\\n                                                            .apply(lambda x: 1 if x == 'Peak' else 0)\\\n                                                            .reset_index(drop=True)\n\n# target variable\ny_train_downsampled_bootstrap = df_train_downsampled_bootstrap['isFraud'].reset_index(drop=True)\n\n# dataframe shape\nprint(f\"After standardizing and encoding the down-sampled and bootstrapped train data, this dataframe has {X_train_downsampled_bootstrap.shape[0]} rows and {X_train_downsampled_bootstrap.shape[1]} columns.\")\n\n# df_train_downsampled_smote\n# standardizing train data\nX_train_downsampled_smote = pd.DataFrame(ss.transform(df_train_downsampled_smote[['ln_errorBalanceOrig','ln_errorBalanceDest','ln_amount']]),\n                                         columns=['ln_errorBalanceOrig','ln_errorBalanceDest','ln_amount'])\n\n# dummies\nX_train_downsampled_smote['zeroBalanceOrig'] = df_train_downsampled_smote['zeroBalanceOrig'].reset_index(drop=True)\nX_train_downsampled_smote['zeroBalanceDest'] = df_train_downsampled_smote['zeroBalanceDest'].reset_index(drop=True)\nX_train_downsampled_smote['type_Transfer'] = df_train_downsampled_smote['type_Transfer'].reset_index(drop=True)\nX_train_downsampled_smote['transactionPeriod_Mid'] = df_train_downsampled_smote['transactionPeriod_Mid'].reset_index(drop=True)\nX_train_downsampled_smote['transactionPeriod_Peak'] = df_train_downsampled_smote['transactionPeriod_Peak'].reset_index(drop=True)\n\n# target variable\ny_train_downsampled_smote = df_train_downsampled_smote['isFraud'].reset_index(drop=True)\n\n# dataframe shape\nprint(f\"After standardizing and encoding the down-sampled and synthetic over-sampled train data, this dataframe has {X_train_downsampled_smote.shape[0]} rows and {X_train_downsampled_smote.shape[1]} columns.\")","919ec60c":"# shuffling data\nX_train_downsampled_bootstrap = X_train_downsampled_bootstrap.sample(frac=1, random_state=33)\ny_train_downsampled_bootstrap = y_train_downsampled_bootstrap.sample(frac=1, random_state=33)\n\nX_train_downsampled_smote = X_train_downsampled_smote.sample(frac=1, random_state=33)\ny_train_downsampled_smote = y_train_downsampled_smote.sample(frac=1, random_state=33)","4e7de0be":"# correlated features\nplt.figure(figsize=(8,8))\nsns.heatmap(X_train_downsampled_bootstrap.corr(),\n            annot = True, square = True, fmt = \".2f\",\n            cmap = \"coolwarm\", cbar = False, mask = np.triu(X_train_downsampled_bootstrap.corr(), 1))\nplt.title(\"Feature correlation\", fontsize=20, fontweight='bold')\n# display plot\nplt.show()","8575c318":"# calculating vif values\nvif = [variance_inflation_factor(X_train_downsampled_bootstrap.values,i)\n       for i in range(X_train_downsampled_bootstrap.shape[1])]\n\n# displaying results as dataframe\npd.DataFrame({\"VIF\":vif}, index=X_train_downsampled_bootstrap.columns)\\\n        .sort_values(\"VIF\", ascending=False)","7ac4faf0":"# dropping ln_errorBalanceDest\nX_train_downsampled_bootstrap.drop(columns=['ln_errorBalanceDest'], inplace=True)\nX_train_downsampled_smote.drop(columns=['ln_errorBalanceDest'], inplace=True)\nX_test.drop(columns=['ln_errorBalanceDest'], inplace=True)","ef0008fe":"# calculating vif values\nvif = [variance_inflation_factor(X_train_downsampled_bootstrap.values,i)\n       for i in range(X_train_downsampled_bootstrap.shape[1])]\n\n# displaying results as dataframe\npd.DataFrame({\"VIF\":vif}, index=X_train_downsampled_bootstrap.columns)\\\n        .sort_values(\"VIF\", ascending=False)","73e03881":"# results matrix\ndf_results = pd.DataFrame(columns=['Description',\n                                   'Misclassifications',\n                                   'Type I errors',\n                                   'Type II errors',\n                                   'Precision',\n                                   'Recall',\n                                   'Accuracy',\n                                   'F1-score',\n                                   'ROC AUC'])","573feaa3":"# defining function to implement stratified K-fold cross validation\ndef StratifiedKFoldCV(model, X_train, y_train, folds=5, threshold=0.5):\n    \n    # initializing dictionary to store cross validation results\n    scores = {\n        'train_accuracy' : [],\n        'train_f1' : [],\n        'train_roc_auc' : [],\n        'test_accuracy' : [],\n        'test_misclassification' : [],\n        'test_type1' : [],\n        'test_type2' : [],\n        'test_precision' : [],\n        'test_recall' : [],\n        'test_f1' : [],\n        'test_roc_auc' : []\n    }\n\n    # initializing the stratified K-fold splitter with n splits\n    skf = StratifiedKFold(n_splits=folds)\n\n    # accessing each train and test fold\n    for train_index, test_index in skf.split(X_train, y_train):\n        X1_train, X1_test = X_train.iloc[train_index], X_train.iloc[test_index]\n        y1_train, y1_test = y_train.iloc[train_index], y_train.iloc[test_index]\n\n        # fitting the model with train data\n        model.fit(X1_train, y1_train)\n\n        # predictions on training data\n        scores['train_accuracy'].append(accuracy_score(y1_train, model.predict(X1_train)).round(2))\n        scores['train_f1'].append(f1_score(y1_train, model.predict(X1_train)).round(2))\n        scores['train_roc_auc'].append(roc_auc_score(y1_train, model.predict(X1_train)).round(2))\n\n        # predicting the test data\n        y1_pred = model.predict_proba(X1_test)\n        y1_pred = binarize(y1_pred, threshold)[:,1]\n\n        # predictions on test data\n        tn, fp, fn, tp = confusion_matrix(y1_test, y1_pred).ravel()\n        scores['test_type1'].append(fp)\n        scores['test_type2'].append(fn)\n        scores['test_misclassification'].append(fp + fn)\n        scores['test_accuracy'].append(accuracy_score(y1_test, y1_pred).round(2))\n        scores['test_precision'].append(precision_score(y1_test, y1_pred).round(2))\n        scores['test_recall'].append(recall_score(y1_test, y1_pred).round(2))\n        scores['test_f1'].append(f1_score(y1_test, y1_pred).round(2))\n        scores['test_roc_auc'].append(roc_auc_score(y1_test, y1_pred).round(2))\n        \n    # returning the scores\n    return scores","cd79143f":"# logit model with default hyperparameters\nmodel_base = LogisticRegression(penalty='none')\nmodel_base","8dd9c374":"# evaluating model using K-fold cross validation\nscores = StratifiedKFoldCV(model_base, X_train_downsampled_bootstrap, y_train_downsampled_bootstrap, 5)","8a044cea":"# checking for overfitting\nprint(f\"Train accuracy: {np.array(scores['train_accuracy']).mean().round(2)}\\tTest accuracy: {np.array(scores['test_accuracy']).mean().round(2)}\")\nprint(f\"Train F1 score: {np.array(scores['train_f1']).mean().round(2)}\\tTest F1 score: {np.array(scores['test_f1']).mean().round(2)}\")\nprint(f\"Train ROC AUC: {np.array(scores['train_roc_auc']).mean().round(2)}\\tTest ROC AUC: {np.array(scores['test_roc_auc']).mean().round(2)}\")","b9b9f207":"# itereation results\ndescription = \"Base logit model on bootstrapped data\"\nmisclassifications = int(np.array(scores['test_misclassification']).mean())\ntype1 = int(np.array(scores['test_type1']).mean().round())\ntype2 = int(np.array(scores['test_type2']).mean().round())\nprecision = np.array(scores['test_precision']).mean().round(2)\nrecall = np.array(scores['test_recall']).mean().round(2)\naccuracy = np.array(scores['test_accuracy']).mean().round(2)\nf1 = np.array(scores['test_f1']).mean().round(2)\nauc = np.array(scores['test_roc_auc']).mean().round(2)\n\n# storing the scores in results dataframe\ndf_results = pd.concat([df_results,\n                        pd.DataFrame(np.array([description,\n                                     misclassifications,\n                                     type1,\n                                     type2,\n                                     precision,\n                                     recall,\n                                     accuracy,\n                                     f1,\n                                     auc]).reshape(1,-1),\n                                     columns=['Description','Misclassifications','Type I errors','Type II errors','Precision','Recall','Accuracy','F1-score','ROC AUC'])\n                                  ], axis=0)\n\n# displaying results\ndf_results","170a0f49":"# evaluating model using K-fold cross validation\nscores = StratifiedKFoldCV(model_base, X_train_downsampled_smote, y_train_downsampled_smote, 5)","68cea711":"# checking for overfitting\nprint(f\"Train accuracy: {np.array(scores['train_accuracy']).mean().round(2)}\\tTest accuracy: {np.array(scores['test_accuracy']).mean().round(2)}\")\nprint(f\"Train F1 score: {np.array(scores['train_f1']).mean().round(2)}\\tTest F1 score: {np.array(scores['test_f1']).mean().round(2)}\")\nprint(f\"Train ROC AUC: {np.array(scores['train_roc_auc']).mean().round(2)}\\tTest ROC AUC: {np.array(scores['test_roc_auc']).mean().round(2)}\")","ad7eba35":"# itereation results\ndescription = \"Base logit model on SMOTE data\"\nmisclassifications = int(np.array(scores['test_misclassification']).mean())\ntype1 = int(np.array(scores['test_type1']).mean().round())\ntype2 = int(np.array(scores['test_type2']).mean().round())\nprecision = np.array(scores['test_precision']).mean().round(2)\nrecall = np.array(scores['test_recall']).mean().round(2)\naccuracy = np.array(scores['test_accuracy']).mean().round(2)\nf1 = np.array(scores['test_f1']).mean().round(2)\nauc = np.array(scores['test_roc_auc']).mean().round(2)\n\n# storing the scores in results dataframe\ndf_results = pd.concat([df_results,\n                        pd.DataFrame(np.array([description,\n                                     misclassifications,\n                                     type1,\n                                     type2,\n                                     precision,\n                                     recall,\n                                     accuracy,\n                                     f1,\n                                     auc]).reshape(1,-1),\n                                     columns=['Description','Misclassifications','Type I errors','Type II errors','Precision','Recall','Accuracy','F1-score','ROC AUC'])\n                                  ], axis=0)\n\n# displaying results\ndf_results","9b0193fc":"# hyper-parameter tuning\n# creating hyperparameter grid\ndict_params = {\"penalty\" : ['l1','l2'],\n               \"solver\" : ['lbfgs', 'saga'],\n               \"C\" : list(np.linspace(0.1, 2, 20))}\n\n# hyperparameter tuning using grid search CV\nmodel_tuning = GridSearchCV(model_base, param_grid=dict_params, scoring='recall', cv=5, return_train_score=True, n_jobs=-1)\nmodel_tuning.fit(X_train_downsampled_smote, y_train_downsampled_smote)","f4970747":"# hyperparameter tuning results\ncv_results = pd.DataFrame(model_tuning.cv_results_)\ncv_results['train_test_diff'] = cv_results['mean_train_score'] - cv_results['mean_test_score']\ncv_results.sort_values('mean_test_score', ascending=False)[[\"param_penalty\",\"param_C\",\"param_solver\",\"mean_train_score\",\"mean_test_score\",'train_test_diff']].head()","c05615e6":"# visualizing training and testing accuracy\nplt.figure(figsize=(12,8))\nplt.plot(cv_results.index, cv_results[\"mean_test_score\"], label=\"test score\", marker='o')\nplt.plot(cv_results.index, cv_results[\"mean_train_score\"], label=\"train score\", marker='o')\nplt.title(\"Training vs. Test score\")\nplt.ylabel(\"Recall Score\")\nplt.xlabel(\"Iteration\")\nplt.legend()\nplt.show()","56bced34":"# optimized logit model\nmodel_optimized = LogisticRegression(penalty='l2', C=0.1, solver='saga')\nmodel_optimized","64db7dde":"# evaluating model using K-fold cross validation\nscores = StratifiedKFoldCV(model_optimized, X_train_downsampled_bootstrap, y_train_downsampled_bootstrap, 5)","de89b0da":"# checking for overfitting\nprint(f\"Train accuracy: {np.array(scores['train_accuracy']).mean().round(2)}\\tTest accuracy: {np.array(scores['test_accuracy']).mean().round(2)}\")\nprint(f\"Train F1 score: {np.array(scores['train_f1']).mean().round(2)}\\tTest F1 score: {np.array(scores['test_f1']).mean().round(2)}\")\nprint(f\"Train ROC AUC: {np.array(scores['train_roc_auc']).mean().round(2)}\\tTest ROC AUC: {np.array(scores['test_roc_auc']).mean().round(2)}\")","5470107a":"# itereation results\ndescription = \"Optimized logit model\"\nmisclassifications = int(np.array(scores['test_misclassification']).mean())\ntype1 = int(np.array(scores['test_type1']).mean().round())\ntype2 = int(np.array(scores['test_type2']).mean().round())\nprecision = np.array(scores['test_precision']).mean().round(2)\nrecall = np.array(scores['test_recall']).mean().round(2)\naccuracy = np.array(scores['test_accuracy']).mean().round(2)\nf1 = np.array(scores['test_f1']).mean().round(2)\nauc = np.array(scores['test_roc_auc']).mean().round(2)\n\n# storing the scores in results dataframe\ndf_results = pd.concat([df_results,\n                        pd.DataFrame(np.array([description,\n                                     misclassifications,\n                                     type1,\n                                     type2,\n                                     precision,\n                                     recall,\n                                     accuracy,\n                                     f1,\n                                     auc]).reshape(1,-1),\n                                     columns=['Description','Misclassifications','Type I errors','Type II errors','Precision','Recall','Accuracy','F1-score','ROC AUC'])\n                                  ], axis=0)\n\n# displaying results\ndf_results","035b86d9":"# fitting base model with training data\nmodel_base.fit(X_train_downsampled_bootstrap, y_train_downsampled_bootstrap)\n\n# let's predict the probabilities on train data\ny_pred = model_base.predict_proba(X_train_downsampled_bootstrap)","781d7896":"# let's see how lower threshold value affects our model\nlist_threshold, list_type2_errors, list_recall_score, list_precision_score, list_rocauc_score = [], [], [], [], []\n\n# trying with different threshold values\nfor i in range(1,11):\n    y_pred_iter = binarize(y_pred, i\/20)[:,1]\n    list_threshold.append(i\/20)\n    list_type2_errors.append(confusion_matrix(y_train_downsampled_bootstrap, y_pred_iter)[1,0])\n    list_recall_score.append(recall_score(y_train_downsampled_bootstrap, y_pred_iter))\n    list_precision_score.append(precision_score(y_train_downsampled_bootstrap, y_pred_iter))\n    list_rocauc_score.append(roc_auc_score(y_train_downsampled_bootstrap, y_pred_iter))\n\n# # comparing the results\n# pd.DataFrame(zip(list_threshold, list_type2_errors, list_recall_score, list_precision_score, list_rocauc_score),\n#              columns=[\"Threshold\",\"Type II errors\",\"Recall\",\"Precision\",\"ROC_AUC\"])\n\n# optimal threshold\nplt.figure(figsize=(10,6))\nplt.plot(list_recall_score[::-1], marker='o', label='Recall Score')\nplt.plot(list_precision_score[::-1], marker='o', label='Precision Score')\nplt.plot(list_rocauc_score[::-1], marker='o', label='AUC ROC')\nplt.axvline(x=2, c='black')\nplt.xticks(ticks=range(10), labels=[(i\/20) for i in range(1,11)][::-1], fontsize=15, fontweight='bold')\nplt.xlabel('Threshold', fontsize=15, fontweight='bold')\nplt.yticks(fontsize=15, fontweight='bold')\nplt.ylabel('Scores', fontsize=15, fontweight='bold')\nplt.title('Finding optimal threshold', fontsize=20, fontweight='bold')\nplt.legend()\nplt.show()","166a63af":"# evaluating model using K-fold cross validation\nscores = StratifiedKFoldCV(model_base, X_train_downsampled_bootstrap, y_train_downsampled_bootstrap, 5, 0.4)","a3d964c6":"# checking for overfitting\nprint(f\"Train accuracy: {np.array(scores['train_accuracy']).mean().round(2)}\\tTest accuracy: {np.array(scores['test_accuracy']).mean().round(2)}\")\nprint(f\"Train F1 score: {np.array(scores['train_f1']).mean().round(2)}\\tTest F1 score: {np.array(scores['test_f1']).mean().round(2)}\")\nprint(f\"Train ROC AUC: {np.array(scores['train_roc_auc']).mean().round(2)}\\tTest ROC AUC: {np.array(scores['test_roc_auc']).mean().round(2)}\")","8fe74bf0":"# itereation results\ndescription = \"Base logit model with 0.4 threshold\"\nmisclassifications = int(np.array(scores['test_misclassification']).mean())\ntype1 = int(np.array(scores['test_type1']).mean().round())\ntype2 = int(np.array(scores['test_type2']).mean().round())\nprecision = np.array(scores['test_precision']).mean().round(2)\nrecall = np.array(scores['test_recall']).mean().round(2)\naccuracy = np.array(scores['test_accuracy']).mean().round(2)\nf1 = np.array(scores['test_f1']).mean().round(2)\nauc = np.array(scores['test_roc_auc']).mean().round(2)\n\n# storing the scores in results dataframe\ndf_results = pd.concat([df_results,\n                        pd.DataFrame(np.array([description,\n                                     misclassifications,\n                                     type1,\n                                     type2,\n                                     precision,\n                                     recall,\n                                     accuracy,\n                                     f1,\n                                     auc]).reshape(1,-1),\n                                     columns=['Description','Misclassifications','Type I errors','Type II errors','Precision','Recall','Accuracy','F1-score','ROC AUC'])\n                                  ], axis=0)\n\n# displaying results\ndf_results","de81bcc9":"# model hyper-parameters\nmodel_base","0fd40792":"# fitting model on train data\nmodel_base.fit(X_train_downsampled_bootstrap, y_train_downsampled_bootstrap)","dbab39f1":"# predicting probablities on test data\ny_pred_prob = model_base.predict_proba(X_test)\n\n# binarizing the probablities using threshold value of 0.4\ny_pred = binarize(y_pred_prob, 0.4)[:,1]","f31fad33":"# confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n# normalizing\ncm = cm \/ cm.sum(axis=1).reshape(-1,1)\n# plotting\ncm = pd.DataFrame(cm,\n                  columns=['Genuine','Fraudulent'],\n                  index=['Genuine','Fraudulent'])\ncm = cm.round(2)\nplt.figure(figsize=(6,6))\nsns.heatmap(cm, cmap=\"RdYlGn\", annot=True, cbar=False, square=True, annot_kws={\"fontsize\":18})\nplt.xlabel(\"Predicted values\", fontsize=15, fontweight='bold')\nplt.xticks(fontsize=15, fontweight='bold')\nplt.ylabel(\"Actual values\", fontsize=15, fontweight='bold')\nplt.yticks(fontsize=15, fontweight='bold')\nplt.title(\"Confusion Matrix\", fontsize=20, fontweight='bold')\nplt.show()","2183a2cd":"# itereation results\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\ndescription = \"Best logit model - Test data\"\nmisclassifications = fp+fn\ntype1 = fp\ntype2 = fn\nprecision = precision_score(y_test, y_pred).round(2)\nrecall = recall_score(y_test, y_pred).round(2)\naccuracy = accuracy_score(y_test, y_pred).round(2)\nf1 = f1_score(y_test, y_pred).round(2)\nauc = roc_auc_score(y_test, y_pred).round(2)\n\n# storing the scores in results dataframe\ndf_results = pd.concat([df_results,\n                        pd.DataFrame(np.array([description,\n                                     misclassifications,\n                                     type1,\n                                     type2,\n                                     precision,\n                                     recall,\n                                     accuracy,\n                                     f1,\n                                     auc]).reshape(1,-1),\n                                     columns=['Description','Misclassifications','Type I errors','Type II errors','Precision','Recall','Accuracy','F1-score','ROC AUC'])\n                                  ], axis=0)\n\n# displaying results\ndf_results","a69fd863":"# model coefficients\nmodel_coef = pd.DataFrame(zip(X_train_downsampled_bootstrap.columns,\n                              model_base.coef_[0]),\n                          columns=[\"Feature\", \"Coefficient\"])\n\n# visualizing the values\nplt.figure(figsize=(6,6))\nsns.barplot(data = model_coef,\n            y = \"Feature\",\n            x = \"Coefficient\",\n            color = \"green\")\nplt.title(\"Model Coefficients\", fontsize=20, fontweight=\"bold\")\nplt.xlabel(\"Coefficient Value\", fontsize=15, fontweight=\"bold\")\nplt.xticks(fontsize=15)\nplt.ylabel(\"\")\nplt.yticks(fontsize=15, fontweight=\"bold\")\nplt.show()","6ec758c7":"# logit model with default hyperparameters\nmodel_base = DecisionTreeClassifier()\nmodel_base","6fa213c5":"# evaluating model using K-fold cross validation\nscores = StratifiedKFoldCV(model_base, X_train_downsampled_bootstrap, y_train_downsampled_bootstrap, 5)","d24a16da":"# checking for overfitting\nprint(f\"Train accuracy: {np.array(scores['train_accuracy']).mean().round(2)}\\tTest accuracy: {np.array(scores['test_accuracy']).mean().round(2)}\")\nprint(f\"Train F1 score: {np.array(scores['train_f1']).mean().round(2)}\\tTest F1 score: {np.array(scores['test_f1']).mean().round(2)}\")\nprint(f\"Train ROC AUC: {np.array(scores['train_roc_auc']).mean().round(2)}\\tTest ROC AUC: {np.array(scores['test_roc_auc']).mean().round(2)}\")","3a4c33c4":"# itereation results\ndescription = \"Base decision tree clf model on bootstrapped data\"\nmisclassifications = int(np.array(scores['test_misclassification']).mean())\ntype1 = int(np.array(scores['test_type1']).mean().round())\ntype2 = int(np.array(scores['test_type2']).mean().round())\nprecision = np.array(scores['test_precision']).mean().round(2)\nrecall = np.array(scores['test_recall']).mean().round(2)\naccuracy = np.array(scores['test_accuracy']).mean().round(2)\nf1 = np.array(scores['test_f1']).mean().round(2)\nauc = np.array(scores['test_roc_auc']).mean().round(2)\n\n# storing the scores in results dataframe\ndf_results = pd.concat([df_results,\n                        pd.DataFrame(np.array([description,\n                                     misclassifications,\n                                     type1,\n                                     type2,\n                                     precision,\n                                     recall,\n                                     accuracy,\n                                     f1,\n                                     auc]).reshape(1,-1),\n                                     columns=['Description','Misclassifications','Type I errors','Type II errors','Precision','Recall','Accuracy','F1-score','ROC AUC'])\n                                  ], axis=0)\n\n# displaying results\ndf_results","77cca1a0":"# hyper-parameter tuning\n# creating hyperparameter grid\ndict_params = {\"max_depth\" : [3, 4, 5, 6, 8, 10],\n               \"criterion\" : ['gini', 'entropy'],\n               \"max_features\" : ['sqrt', None]}\n\n# hyperparameter tuning using grid search CV\nmodel_tuning = GridSearchCV(model_base, param_grid=dict_params, scoring='recall', cv=5, return_train_score=True, n_jobs=-1)\nmodel_tuning.fit(X_train_downsampled_smote, y_train_downsampled_smote)","8d391724":"# hyperparameter tuning results\ncv_results = pd.DataFrame(model_tuning.cv_results_)\ncv_results['train_test_diff'] = cv_results['mean_train_score'] - cv_results['mean_test_score']\ncv_results.sort_values('mean_test_score', ascending=False)[[\"param_max_depth\",\"param_criterion\",\"param_max_features\",\"mean_train_score\",\"mean_test_score\",'train_test_diff']].head()","a0fa03e2":"# visualizing training and testing accuracy\nplt.figure(figsize=(12,8))\nplt.plot(cv_results.index, cv_results[\"mean_test_score\"], label=\"test score\", marker='o')\nplt.plot(cv_results.index, cv_results[\"mean_train_score\"], label=\"train score\", marker='o')\nplt.title(\"Training vs. Test score\")\nplt.ylabel(\"Recall Score\")\nplt.xlabel(\"Iteration\")\nplt.legend()\nplt.show()","2c3f4db9":"# optimized decision tree classifier model\nmodel_optimized = DecisionTreeClassifier(max_depth=3)\nmodel_optimized","9c1a148f":"# evaluating model using K-fold cross validation\nscores = StratifiedKFoldCV(model_optimized, X_train_downsampled_bootstrap, y_train_downsampled_bootstrap, 5)","b4b179d4":"# checking for overfitting\nprint(f\"Train accuracy: {np.array(scores['train_accuracy']).mean().round(2)}\\tTest accuracy: {np.array(scores['test_accuracy']).mean().round(2)}\")\nprint(f\"Train F1 score: {np.array(scores['train_f1']).mean().round(2)}\\tTest F1 score: {np.array(scores['test_f1']).mean().round(2)}\")\nprint(f\"Train ROC AUC: {np.array(scores['train_roc_auc']).mean().round(2)}\\tTest ROC AUC: {np.array(scores['test_roc_auc']).mean().round(2)}\")","21f124cd":"# itereation results\ndescription = \"Optimized decision tree clf\"\nmisclassifications = int(np.array(scores['test_misclassification']).mean())\ntype1 = int(np.array(scores['test_type1']).mean().round())\ntype2 = int(np.array(scores['test_type2']).mean().round())\nprecision = np.array(scores['test_precision']).mean().round(2)\nrecall = np.array(scores['test_recall']).mean().round(2)\naccuracy = np.array(scores['test_accuracy']).mean().round(2)\nf1 = np.array(scores['test_f1']).mean().round(2)\nauc = np.array(scores['test_roc_auc']).mean().round(2)\n\n# storing the scores in results dataframe\ndf_results = pd.concat([df_results,\n                        pd.DataFrame(np.array([description,\n                                     misclassifications,\n                                     type1,\n                                     type2,\n                                     precision,\n                                     recall,\n                                     accuracy,\n                                     f1,\n                                     auc]).reshape(1,-1),\n                                     columns=['Description','Misclassifications','Type I errors','Type II errors','Precision','Recall','Accuracy','F1-score','ROC AUC'])\n                                  ], axis=0)\n\n# displaying results\ndf_results","051d33a5":"# model hyper-parameters\nmodel_optimized","ee762c11":"# fitting model on train data\nmodel_optimized.fit(X_train_downsampled_bootstrap, y_train_downsampled_bootstrap)","8c02bc0b":"# predicting probablities on test data\ny_pred = model_optimized.predict(X_test)","306bfff3":"# confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n# normalizing\ncm = cm \/ cm.sum(axis=1).reshape(-1,1)\n# plotting\ncm = pd.DataFrame(cm,\n                  columns=['Genuine','Fraudulent'],\n                  index=['Genuine','Fraudulent'])\ncm = cm.round(2)\nplt.figure(figsize=(6,6))\nsns.heatmap(cm, cmap=\"RdYlGn\", annot=True, cbar=False, square=True, annot_kws={\"fontsize\":18})\nplt.xlabel(\"Predicted values\", fontsize=15, fontweight='bold')\nplt.xticks(fontsize=15, fontweight='bold')\nplt.ylabel(\"Actual values\", fontsize=15, fontweight='bold')\nplt.yticks(fontsize=15, fontweight='bold')\nplt.title(\"Confusion Matrix\", fontsize=20, fontweight='bold')\nplt.show()","320f138c":"# itereation results\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\ndescription = \"Best decision tree clf - Test data\"\nmisclassifications = fp+fn\ntype1 = fp\ntype2 = fn\nprecision = precision_score(y_test, y_pred).round(2)\nrecall = recall_score(y_test, y_pred).round(2)\naccuracy = accuracy_score(y_test, y_pred).round(2)\nf1 = f1_score(y_test, y_pred).round(2)\nauc = roc_auc_score(y_test, y_pred).round(2)\n\n# storing the scores in results dataframe\ndf_results = pd.concat([df_results,\n                        pd.DataFrame(np.array([description,\n                                     misclassifications,\n                                     type1,\n                                     type2,\n                                     precision,\n                                     recall,\n                                     accuracy,\n                                     f1,\n                                     auc]).reshape(1,-1),\n                                     columns=['Description','Misclassifications','Type I errors','Type II errors','Precision','Recall','Accuracy','F1-score','ROC AUC'])\n                                  ], axis=0)\n\n# displaying results\ndf_results","91bd9f58":"# model coefficients\nmodel_coef = pd.DataFrame(zip(X_train_downsampled_bootstrap.columns,\n                              model_optimized.feature_importances_),\n                          columns=[\"Feature\", \"Coefficient\"])\n\n# visualizing the values\nplt.figure(figsize=(6,6))\nsns.barplot(data = model_coef,\n            y = \"Feature\",\n            x = \"Coefficient\",\n            color = \"green\")\nplt.title(\"Feature Importance\", fontsize=20, fontweight=\"bold\")\nplt.xlabel(\"Feature Importance Value\", fontsize=15, fontweight=\"bold\")\nplt.xticks(fontsize=15)\nplt.ylabel(\"\")\nplt.yticks(fontsize=15, fontweight=\"bold\")\nplt.show()","7ff5f4d5":"# plotting the tree\nplt.figure(figsize=(25,12))\nplot_tree(model_optimized, feature_names=X_train_downsampled_bootstrap.columns, class_names=['Non-Fraud','Fraud'], filled=True, fontsize=13)\nplt.title(\"Decision Tree\", fontsize=25, fontweight=\"bold\")\nplt.show()","e478fbf6":"# normalizing train data\n# initializing min_max scaler\nmms = MinMaxScaler()\n\n# fitting scaler with train data\nmms.fit(df_train[['ln_errorBalanceOrig','ln_errorBalanceDest','ln_amount']])\n\n# transforming train data\nX_train = pd.DataFrame(mms.transform(df_train[['ln_errorBalanceOrig','ln_errorBalanceDest','ln_amount']]),\n                       columns=['ln_errorBalanceOrig','ln_errorBalanceDest','ln_amount'])\n\n# creating dummies for test data\nX_train['zeroBalanceOrig'] = df_train['zeroBalanceOrig'].reset_index(drop=True)\nX_train['zeroBalanceDest'] = df_train['zeroBalanceDest'].reset_index(drop=True)\n\nX_train['type_Transfer'] = df_train['type'].replace({'CASH_OUT' : 0,\n                                                     'TRANSFER' : 1})\\\n                                           .reset_index(drop=True)\n\nX_train['transactionPeriod_Mid'] = df_train['transactionPeriod'].apply(lambda x: 1 if x == 'Mid' else 0)\\\n                                                                .reset_index(drop=True)\nX_train['transactionPeriod_Peak'] = df_train['transactionPeriod'].apply(lambda x: 1 if x == 'Peak' else 0)\\\n                                                                 .reset_index(drop=True)\n\n\n# target variable\ny_train = df_train['isFraud'].reset_index(drop=True)\n\n# normalizing test data\nX_test = pd.DataFrame(mms.transform(df_test[['ln_errorBalanceOrig','ln_errorBalanceDest','ln_amount']]),\n                      columns=['ln_errorBalanceOrig','ln_errorBalanceDest','ln_amount'])\n\n# creating dummies for test data\nX_test['zeroBalanceOrig'] = df_test['zeroBalanceOrig'].reset_index(drop=True)\nX_test['zeroBalanceDest'] = df_test['zeroBalanceDest'].reset_index(drop=True)\n\nX_test['type_Transfer'] = df_test['type'].replace({'CASH_OUT' : 0,\n                                                   'TRANSFER' : 1})\\\n                                         .reset_index(drop=True)\n\nX_test['transactionPeriod_Mid'] = df_test['transactionPeriod'].apply(lambda x: 1 if x == 'Mid' else 0)\\\n                                                              .reset_index(drop=True)\nX_test['transactionPeriod_Peak'] = df_test['transactionPeriod'].apply(lambda x: 1 if x == 'Peak' else 0)\\\n                                                               .reset_index(drop=True)\n\n# target variable\ny_test = df_test['isFraud'].reset_index(drop=True)","83004be0":"# down sampling majority class\nX_train = pd.concat([X_train[y_train == 0].sample(1000000, random_state=1),\n                     X_train[y_train == 1]])\ny_train = y_train.iloc[X_train.index]\n\n# dataframe dimensions\nprint(f\"After downsampling the majority class, the imbalanced train set has {X_train.shape[0]} rows and {X_train.shape[1]} columns.\")","d713437c":"# up sampling minority class using SMOTE\n# initializing SMOTE object\nsmote = SMOTE(random_state=33)\n\n# fitting data with SMOTE and creating synthetic over-sampled data set\nX_train, y_train = smote.fit_sample(X_train, y_train)\n\n# dataframe dimensions\nprint(f\"After downsampling the majority class and upsampling the minority class using SMOTE, the balanced train set has {X_train.shape[0]} rows and {X_train.shape[1]} columns.\")","c59aefa1":"# shuffling data\nX_train = X_train.sample(frac=1, random_state=33)\ny_train = y_train.sample(frac=1, random_state=33)","199ad398":"# creating dummies for target variable\ny_train_binary = pd.get_dummies(y_train)","82093e9c":"# checking for GPU\ntf.test.is_gpu_available()","b459a426":"# below are hyperparameters used for the experiments\nbatch_size=40\nnum_epochs=5\nvalidate_pct=0.1\nlearning_rate = 1e-3\nloss_function='binary_crossentropy'\noptimization_function=keras.optimizers.RMSprop(0.001)\nmetric=[keras.metrics.BinaryAccuracy()]","04787aaf":"# simple feed forward NN\nmodel_ann = Sequential()\nmodel_ann.add(InputLayer(input_shape=(8,)))\nmodel_ann.add(Dense(32, activation='relu'))\n# model_ann.add(Dropout(0.3))\nmodel_ann.add(Dense(16, activation='relu'))\n# model_ann.add(Dropout(0.3))\n# model_ann.add(Dense(8, activation='relu'))\n# model_ann.add(Dropout(0.3))\nmodel_ann.add(Dense(2, activation='softmax'))\n\n# model framework\nmodel_ann.summary()","b26e25f0":"# training ANN\nmodel_ann.compile(loss=loss_function, optimizer=optimization_function, metrics=metric)\nmodel_history = model_ann.fit(X_train, y_train_binary,\n                              batch_size=batch_size,\n                              epochs=num_epochs,\n                              validation_split=validate_pct,\n                              shuffle=True,\n                              verbose=True)","a35b5bc0":"# validation loss and accuracy\nplt.subplots(1,2, figsize=(18,6))\n\n# validation loss\nplt.subplot(1,2,1)\nplt.plot(model_history.history['loss'], marker='o', label='train loss')\nplt.plot(model_history.history['val_loss'], marker='o', label='val loss')\nplt.xlabel(\"Epoch\", fontsize=15, fontweight='bold')\nplt.xticks(ticks=range(5), labels=range(1,6), fontsize=15, fontweight='bold')\nplt.ylabel(\"Cross-entropy loss\", fontsize=15, fontweight='bold')\nplt.yticks(fontsize=15, fontweight='bold')\nplt.title(\"Validation loss\", fontsize=20, fontweight='bold')\nplt.legend()\n\n# validation accuracy\nplt.subplot(1,2,2)\nplt.plot(model_history.history['binary_accuracy'], marker='o', label='train accuracy')\nplt.plot(model_history.history['val_binary_accuracy'], marker='o', label='val accuracy')\nplt.xlabel(\"Epoch\", fontsize=15, fontweight='bold')\nplt.xticks(ticks=range(5), labels=range(1,6), fontsize=15, fontweight='bold')\nplt.ylabel(\"Accuracy score\", fontsize=15, fontweight='bold')\nplt.yticks(fontsize=15, fontweight='bold')\nplt.title(\"Validation loss\", fontsize=20, fontweight='bold')\nplt.legend()\n\nplt.show()","4aba4a2d":"# predicting using train data\ny_pred = pd.DataFrame(model_ann.predict(X_train))[1].apply(lambda x: 1 if x > 0.5 else 0)","2270e212":"# itereation results\ntn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\ndescription = \"Base ANN (32+16)\"\nmisclassifications = fp+fn\ntype1 = fp\ntype2 = fn\nprecision = precision_score(y_train, y_pred).round(2)\nrecall = recall_score(y_train, y_pred).round(2)\naccuracy = accuracy_score(y_train, y_pred).round(2)\nf1 = f1_score(y_train, y_pred).round(2)\nauc = roc_auc_score(y_train, y_pred).round(2)\n\n# storing the scores in results dataframe\ndf_results = pd.concat([df_results,\n                        pd.DataFrame(np.array([description,\n                                     misclassifications,\n                                     type1,\n                                     type2,\n                                     precision,\n                                     recall,\n                                     accuracy,\n                                     f1,\n                                     auc]).reshape(1,-1),\n                                     columns=['Description','Misclassifications','Type I errors','Type II errors','Precision','Recall','Accuracy','F1-score','ROC AUC'])\n                                  ], axis=0)\n\n# displaying results\ndf_results","7945dcaf":"# simple feed forward NN\nmodel_ann = Sequential()\nmodel_ann.add(InputLayer(input_shape=(8,)))\nmodel_ann.add(Dense(64, activation='relu'))\n# model_ann.add(Dropout(0.3))\nmodel_ann.add(Dense(32, activation='relu'))\n# model_ann.add(Dropout(0.3))\n# model_ann.add(Dense(8, activation='relu'))\n# model_ann.add(Dropout(0.3))\nmodel_ann.add(Dense(2, activation='softmax'))\n\n# model framework\nmodel_ann.summary()","c0e7b7dc":"# training ANN\nmodel_ann.compile(loss=loss_function, optimizer=optimization_function, metrics=metric)\nmodel_history = model_ann.fit(X_train, y_train_binary,\n                              batch_size=batch_size,\n                              epochs=num_epochs,\n                              validation_split=validate_pct,\n                              shuffle=True,\n                              verbose=True)","bcbfc9b5":"# validation loss and accuracy\nplt.subplots(1,2, figsize=(18,6))\n\n# validation loss\nplt.subplot(1,2,1)\nplt.plot(model_history.history['loss'], marker='o', label='train loss')\nplt.plot(model_history.history['val_loss'], marker='o', label='val loss')\nplt.xlabel(\"Epoch\", fontsize=15, fontweight='bold')\nplt.xticks(ticks=range(5), labels=range(1,6), fontsize=15, fontweight='bold')\nplt.ylabel(\"Cross-entropy loss\", fontsize=15, fontweight='bold')\nplt.yticks(fontsize=15, fontweight='bold')\nplt.title(\"Validation loss\", fontsize=20, fontweight='bold')\nplt.legend()\n\n# validation accuracy\nplt.subplot(1,2,2)\nplt.plot(model_history.history['binary_accuracy'], marker='o', label='train accuracy')\nplt.plot(model_history.history['val_binary_accuracy'], marker='o', label='val accuracy')\nplt.xlabel(\"Epoch\", fontsize=15, fontweight='bold')\nplt.xticks(ticks=range(5), labels=range(1,6), fontsize=15, fontweight='bold')\nplt.ylabel(\"Accuracy score\", fontsize=15, fontweight='bold')\nplt.yticks(fontsize=15, fontweight='bold')\nplt.title(\"Validation loss\", fontsize=20, fontweight='bold')\nplt.legend()\n\nplt.show()","f633f63b":"# predicting using train data\ny_pred = pd.DataFrame(model_ann.predict(X_train))[1].apply(lambda x: 1 if x > 0.5 else 0)","d97a6479":"# itereation results\ntn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\ndescription = \"ANN (64+32)\"\nmisclassifications = fp+fn\ntype1 = fp\ntype2 = fn\nprecision = precision_score(y_train, y_pred).round(2)\nrecall = recall_score(y_train, y_pred).round(2)\naccuracy = accuracy_score(y_train, y_pred).round(2)\nf1 = f1_score(y_train, y_pred).round(2)\nauc = roc_auc_score(y_train, y_pred).round(2)\n\n# storing the scores in results dataframe\ndf_results = pd.concat([df_results,\n                        pd.DataFrame(np.array([description,\n                                     misclassifications,\n                                     type1,\n                                     type2,\n                                     precision,\n                                     recall,\n                                     accuracy,\n                                     f1,\n                                     auc]).reshape(1,-1),\n                                     columns=['Description','Misclassifications','Type I errors','Type II errors','Precision','Recall','Accuracy','F1-score','ROC AUC'])\n                                  ], axis=0)\n\n# displaying results\ndf_results","93691d49":"# simple feed forward NN\nmodel_ann = Sequential()\nmodel_ann.add(InputLayer(input_shape=(8,)))\nmodel_ann.add(Dense(64, activation='relu'))\nmodel_ann.add(Dropout(0.2))\nmodel_ann.add(Dense(32, activation='relu'))\nmodel_ann.add(Dropout(0.2))\nmodel_ann.add(Dense(8, activation='relu'))\nmodel_ann.add(Dropout(0.1))\nmodel_ann.add(Dense(2, activation='softmax'))\n\n# model framework\nmodel_ann.summary()","ce118625":"# training ANN\nmodel_ann.compile(loss=loss_function, optimizer=optimization_function, metrics=metric)\nmodel_history = model_ann.fit(X_train, y_train_binary,\n                              batch_size=batch_size,\n                              epochs=num_epochs,\n                              validation_split=validate_pct,\n                              shuffle=True,\n                              verbose=True)","0625ac45":"# validation loss and accuracy\nplt.subplots(1,2, figsize=(18,6))\n\n# validation loss\nplt.subplot(1,2,1)\nplt.plot(model_history.history['loss'], marker='o', label='train loss')\nplt.plot(model_history.history['val_loss'], marker='o', label='val loss')\nplt.xlabel(\"Epoch\", fontsize=15, fontweight='bold')\nplt.xticks(ticks=range(5), labels=range(1,6), fontsize=15, fontweight='bold')\nplt.ylabel(\"Cross-entropy loss\", fontsize=15, fontweight='bold')\nplt.yticks(fontsize=15, fontweight='bold')\nplt.title(\"Validation loss\", fontsize=20, fontweight='bold')\nplt.legend()\n\n# validation accuracy\nplt.subplot(1,2,2)\nplt.plot(model_history.history['binary_accuracy'], marker='o', label='train accuracy')\nplt.plot(model_history.history['val_binary_accuracy'], marker='o', label='val accuracy')\nplt.xlabel(\"Epoch\", fontsize=15, fontweight='bold')\nplt.xticks(ticks=range(5), labels=range(1,6), fontsize=15, fontweight='bold')\nplt.ylabel(\"Accuracy score\", fontsize=15, fontweight='bold')\nplt.yticks(fontsize=15, fontweight='bold')\nplt.title(\"Validation loss\", fontsize=20, fontweight='bold')\nplt.legend()\n\nplt.show()","75be2429":"# predicting using train data\ny_pred = pd.DataFrame(model_ann.predict(X_train))[1].apply(lambda x: 1 if x > 0.5 else 0)","8024c68e":"# itereation results\ntn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\ndescription = \"ANN (64+32+8+dropout 0.2)\"\nmisclassifications = fp+fn\ntype1 = fp\ntype2 = fn\nprecision = precision_score(y_train, y_pred).round(2)\nrecall = recall_score(y_train, y_pred).round(2)\naccuracy = accuracy_score(y_train, y_pred).round(2)\nf1 = f1_score(y_train, y_pred).round(2)\nauc = roc_auc_score(y_train, y_pred).round(2)\n\n# storing the scores in results dataframe\ndf_results = pd.concat([df_results,\n                        pd.DataFrame(np.array([description,\n                                     misclassifications,\n                                     type1,\n                                     type2,\n                                     precision,\n                                     recall,\n                                     accuracy,\n                                     f1,\n                                     auc]).reshape(1,-1),\n                                     columns=['Description','Misclassifications','Type I errors','Type II errors','Precision','Recall','Accuracy','F1-score','ROC AUC'])\n                                  ], axis=0)\n\n# displaying results\ndf_results","02fabb52":"# predicting using train data\ny_pred = pd.DataFrame(model_ann.predict(X_train))[1].apply(lambda x: 1 if x > 0.4 else 0)","fe5f16c8":"# itereation results\ntn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\ndescription = \"ANN (64+32+8+dropout 0.2) with 0.4 threshold\"\nmisclassifications = fp+fn\ntype1 = fp\ntype2 = fn\nprecision = precision_score(y_train, y_pred).round(2)\nrecall = recall_score(y_train, y_pred).round(2)\naccuracy = accuracy_score(y_train, y_pred).round(2)\nf1 = f1_score(y_train, y_pred).round(2)\nauc = roc_auc_score(y_train, y_pred).round(2)\n\n# storing the scores in results dataframe\ndf_results = pd.concat([df_results,\n                        pd.DataFrame(np.array([description,\n                                     misclassifications,\n                                     type1,\n                                     type2,\n                                     precision,\n                                     recall,\n                                     accuracy,\n                                     f1,\n                                     auc]).reshape(1,-1),\n                                     columns=['Description','Misclassifications','Type I errors','Type II errors','Precision','Recall','Accuracy','F1-score','ROC AUC'])\n                                  ], axis=0)\n\n# displaying results\ndf_results","69293c66":"# network architecture\nmodel_ann.summary()","17a2196a":"# predicting using test data\ny_pred = pd.DataFrame(model_ann.predict(X_test))[1].apply(lambda x: 1 if x > 0.4 else 0)","334fe0d2":"# confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n# normalizing\ncm = cm \/ cm.sum(axis=1).reshape(-1,1)\n# plotting\ncm = pd.DataFrame(cm,\n                  columns=['Genuine','Fraudulent'],\n                  index=['Genuine','Fraudulent'])\ncm = cm.round(2)\nplt.figure(figsize=(6,6))\nsns.heatmap(cm, cmap=\"RdYlGn\", annot=True, cbar=False, square=True, annot_kws={\"fontsize\":18})\nplt.xlabel(\"Predicted values\", fontsize=15, fontweight='bold')\nplt.xticks(fontsize=15, fontweight='bold')\nplt.ylabel(\"Actual values\", fontsize=15, fontweight='bold')\nplt.yticks(fontsize=15, fontweight='bold')\nplt.title(\"Confusion Matrix\", fontsize=20, fontweight='bold')\nplt.show()","f099db61":"# itereation results\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\ndescription = \"Best ANN model - Test data\"\nmisclassifications = fp+fn\ntype1 = fp\ntype2 = fn\nprecision = precision_score(y_test, y_pred).round(2)\nrecall = recall_score(y_test, y_pred).round(2)\naccuracy = accuracy_score(y_test, y_pred).round(2)\nf1 = f1_score(y_test, y_pred).round(2)\nauc = roc_auc_score(y_test, y_pred).round(2)\n\n# storing the scores in results dataframe\ndf_results = pd.concat([df_results,\n                        pd.DataFrame(np.array([description,\n                                     misclassifications,\n                                     type1,\n                                     type2,\n                                     precision,\n                                     recall,\n                                     accuracy,\n                                     f1,\n                                     auc]).reshape(1,-1),\n                                     columns=['Description','Misclassifications','Type I errors','Type II errors','Precision','Recall','Accuracy','F1-score','ROC AUC'])\n                                  ], axis=0)\n\n# displaying results\ndf_results","a240c9d2":"# dataframe\nmodels = ['Logistic Regression', 'ANN', 'Decision Tree']\ntype1 = [42564, 47416, 52906]\ntype2 = [35,20,6]\n\ndf_tradeoff = pd.DataFrame(zip(models, type1, type2), columns=['Model','Type I Errors', 'Type II Errors'])\ndf_tradeoff","70fe8b1d":"# visualizing\nplt.figure(figsize=(8,6))\nmms=MinMaxScaler()\nplt.plot(mms.fit_transform(df_tradeoff.iloc[:, [1,2]])[:,0], color='r', label='Type I', marker='o')\nplt.plot(mms.fit_transform(df_tradeoff.iloc[:, [1,2]])[:,1], color='g', label='Type II', marker='o')\nplt.xlabel(\"\", fontsize=15, fontweight='bold')\nplt.xticks(ticks=[0,1,2], labels=models, fontsize=15, fontweight='bold')\nplt.ylabel(\"No. of errors (Normalize)\", fontsize=15, fontweight='bold')\nplt.yticks(fontsize=15, fontweight='bold')\nplt.title(\"Type I vs Type II\", fontsize=20, fontweight='bold')\nplt.legend()\nplt.show()","8ca95fa6":"**Observations:**\n\n* ANN sits in the middle between Logit and Decision Tree Clf models.","8b7658a1":"### Model interpretation","700967f0":"### Feed forward neural network (64+32)","8125f2b3":"**Observations:**\n\n* Base ANN model is performing well, but not as good as the best logit model or best decision tree classifier.\n* Let's try to improve the complexity.","1198f905":"**Observations:**\n\n* Fraudulent transactions, in general, are having lower error in origin account balance and higher error in destination account balance at the end of transaction.","664c04bb":"**Observations:**\n\n* Train and test scores are similar, there is no model over-fit.","33679614":"### Log transformed destination account error balance\n\nApplying log transformation on the destination account error balance variable.","48201a03":"### Encoding and scaling variables","90b6bb95":"### Base model on bootstrapped data","0997a7ec":"### Optimizing threshold","dd52372e":"### Log transformed amount\n\nApplying log transformation on the amount variable.","57bcf38b":"### Setting parameters","7c6775a4":"## Feed-forward Nueral Network\n\n* Non-liner model\n* Parametric model\n* Complex model","4e046f9e":"**Observations:**\n\n* We can see that almost 50% of the fraudulent transactions had a zero old balance and new balance in destination account even through transaction amount was non-zero.\n* But, less than 0.01% of the non fraudulent transactions had a zero old balance and new balance in destination account even through transaction amount was non-zero.","84fe6411":"### Hyper-parameter tuning","e0c02cbe":"# Feature engineering","59f57be6":"**Observations:**\n\n* There is some serious multicollinearity in the model features.\n* From the EDA, we decide to drop `ln_errorBalanceDest` variable.","85acd339":"**Observations:**\n\n* From the CV results, there is no significant difference or improvement in model performance.\n* Nevertheless, best model is model with `l2` penalty, `C` = 0.1, `solver` = saga.","b676e3fc":"### Down sampling majority class and up sampling minority class","eb5e0039":"## TypeI vs TypeII errors trade-off","a2a595ce":"## Decision Tree Classifier\n\n* Non-linear model\n* Non-parametric model\n* Simple model","c6b7b1bd":"**Observations:**\n\n* Train and test scores are similar, there is no model over-fit.","b9f4e3d0":"### Missing values","9d581750":"# Exploratory data analysis","db19e413":"**Observations:**\n\n* Majority of the transactions are of `Payment` and `Cash-Out` types (~34%).\n* `Debit` transactions are the least, less than 1%.","0a4db28b":"**Observations:**\n\n* Applying log transformation on the amount is removing skewness.\n* There are still some outliers after applying log transformation.","d67496cf":"**Observations:**\n\n* Fraudulent transaction observations are only 8,213 (0.13%). Fraud is rare.","c2ab7d5c":"### Testing on unseen data","5598e8f7":"**Observations:**\n\n* Model performance on test data is very good.","36499008":"**Observations:**\n\n* Fraudulent transactions were recored only in `Cash-Out` and `Transfer` transaction types. They are equally distributed.\n* Also, fraudulent transactions were recored only in Customer to Customer transactions.","959e44bf":"**Observations:**\n\n* Number of transactions gradually increase from 9 hrs, peaking at 19 hrs, and then gradually decrease.\n* Majority of transactions happen during day.","e1123098":"**Observations:**\n\n* `ln_errorBalanceOrig` is the most important feature.\n* `ln_amount` and `zeroBalanceDest` also have some importance.","d7debcff":"# Modelling\n\n**What is our objective?**\n\nWe want to build some classification models to predict whether or not a transaction is fraudulent or not, based on the transaction data.\n\n**What is our cost (Type I\/Type II)?**\n\nIn this scenario, we do not want to incorrectly flag fraudulent transaction as non-fraudulent, i.e. incorrectly label (classify) a positive class as negative. Which is a false negative, i.e. Type II errors.\n\n**What are reliable validation metrics?**\n\nType II errors: A lower number of Type II errors is desirable.\n\nRecall: Recall is number of true positives out of actual positives. In general terms, it states that how well the model is able to differentiate between the two classes. A high recall value is desirable.\n\n**What is our optimizing criteria?**\n\nKeeping the accuracy and ROC AUC value as high as possible, we need to lower Type II errors or improve recall score.","b34d434b":"**Observations:**\n\n* This dataset is clean, no missing values.","32687af3":"### Testing in unseen data","f8a64d59":"### Transaction between\n\nDeriving a label to denote whether the transfer happened from Customer to Merchant or from Customer to Customer.","17fde66e":"**Observations:**\n\n* Majority of fraudulent transactions have amount ending with digit `0`.\n* That's probably because, human beings tend to make up round numbers (10, 50, 100, etc) when they make up some numbers.\n* Not much significant difference. We can ignore this feature.","050ea9c3":"### Transaction amount in thousands (units)\n\nMaking the transaction amount units to thousands.","b43256f6":"**Observations:**\n\n* Adding dropout layers did improve the model performance.\n* Let's test it on unseen (test) data.","5523199a":"### Hyper-parameter tuning","76e910f8":"### Correlation plot","7e4664ef":"**Observations:**\n\n* Top 3 important features are: `zeroBalanceDest`, `transactionPeriod_Peak`, `ln_errorBalanceOrig`.\n* `ln_errorBalanceOrig` and `type_Transfer` have negative effect on the likelihood of fraud, rest of the features are having positive effect on likelihood of fraud. ","6861b9df":"### Correlated features","5b83eb36":"**Observations:**\n\n* During day, Customer to Customer transactions are greater than Customer to Merchant transactions.\n* During mid-night, Customer to Merchant transactions are greater than Customer to Customer transactions.","1628ea34":"# Conclusions\n\nFraud is a very rare event, approximately only 1 in 1,000 transactions is a fraudulent. Identifying fraudulent transactions could be challenging due to its highly rare nature.\n\n**Where does fraud happen?**\n\n* Fraud is observed in `Cash Out` and `Transfer` type transactions only.\n* Also, fraud is observed in Customer 2 Customer transactions only.\n\n**When does fraud happend?**\n\n* Majority of fraudulent transactions happen during night.\n* Fraudulent transactions peak between 3am and 6am, and goes as high as 58% of transactions.\n* Less than 1% of transactions during day are fraudulent.\n\n**Characteristics of fraudulent transaction amount:**\n\n* Proportion of fraudulent transactions in case of transaction amount ending in 0 is little higher than rest of the digits.\n* Human beings tend to come up with round numbers when they make-up numbers.\n\n**Cues to identify fraudulent transactions:**\n\n* Proportion of fraudulent transactions in case of transaction amount ending in 0 is little higher than rest of the digits.\n* Human beings tend to come up with round numbers when they make-up numbers.\n\n**Important features for fraud detection model:**\n\n* `zeroBalanceDest`: Whether the destination account balance amount after the transaction was zero.\n* `ln_errorBalanceOrig`: Log of error in the origin account balance amount after the transaction.\n* `transactionPeriod_Peak`: Whether the transaction was made during peak fraudulent transaction period (midnight).","ce223bb1":"**Observations:**\n\n* Using threshold value of 0.4, there is slight improvement in model performance (`Recall` and `Type II` errors) at the expense of some `Precision` and `Type I` errors.","f07a5976":"### Model interpretation","5e759ee6":"**Observations:**\n\n* ANN did slightly outperform logit model, but it could not outperform decision tree classifier.\n* Decision tree classifier remains the best model.","dd178b58":"**Observations:**\n\n* Train and test scores are similar, there is no model over-fit.","f400ce0e":"## Logistic regression\n\n* Linear model\n* Parametric model\n* Simple model","b2a9574f":"**Observations:**\n\n* Train and test scores are similar, there is no model over-fit.","689b8385":"### Feed forward neural network (64+32+8+dropout)","a5fe93b0":"### Transaction hour\n\nExtracting the hour information.","2b4dafa8":"**Observations:**\n\n* After dropping `X_train_downsampled_smote`, we have removed multi-collinearity from input variables.","0493d0e4":"# Reading and understanding data","b1c76ae8":"**Observations:**\n\n* We can see that majority of the classification happens one the first and second (right side) nodes.","9b7d69e4":"### Down sampling majority class","84cfcaa0":"### Subsetting final set of variables for modelling","0397d593":"### Transaction amount","a465d118":"**Observations:**\n\n* There is no visible difference in frequency of fraud between transaction types.","df7cbb85":"### Origin balance error","9fcbe891":"### Preparing data","c7b6370e":"**Observations:**\n\n* From the above plot, we choose the best threshold value to be 0.4.","8b099618":"### Destination account balance error","c3021165":"**Observations:**\n\n* Customer to Merchant transactions are `Payment` type only.\n* Remaining transaction types are between Customer to Customer only.","6f3dad96":"**Observations:**\n\n* Comparing the results, training model on SMOTE data gave us slightly better results. That's probably because SMOTE technique induces some variation in data.\n* Let's use SMOTE data for rest of our modelling.","6c7ceed8":"**Observations:**\n\n* Average amount for `Transfer` type transactions peak at 3 hrs and 16 hrs. That's strange.\n* Average amount for `Cash-Out` type transactions peak between 3 hrs and 5 hrs.\n* Average amount for `Cash-In` type transactions is stable through out the day.\n* Average amount for `Debit` and `Payment` type transactions peak between 9 hrs and 13 hrs.","83afcd75":"### Flag to indicate zero origin account balance","1ac2f15a":"### Transaction period (time of day)\n\nCategorizing the hour value into peak hour, mid hour, and least hour based on the proportion of fraud happening.","1dac985f":"**Observations:**\n\n* Pruning the tree and setting the max depth to 3 had degraded model performance. \n* But, the pruned tree performance is definitely better than best logit model performance.","284154a5":"**Observations:**\n\n* `Transfer` transactions are having highest transactions amounts.\n* `Debit` transactions are having the least highest transaction amount.","cba2f0ac":"**Observations:**\n\n* Adding layers improved model performance by a little.\n* Let's try adding dropout layers.","b4cdc95e":"### Results matrix and user defined functions","5343c12d":"### Log transformed origin account error balance\n\nApplying log transformation on the origin account error balance variable.","41e681bf":"**Observations:**\n\n* Applying log transformation on the amount is removing skewness.\n* Outliers also reduced after applying log transformation.","c0585637":"### Base model","287afae5":"**Obseravations:**\n\n* Proportion of fraudulent transactions is gradually increasing from 9:00 pm, peaks at 5:00 am (~58%, thats huge!) and then gradually decreases.","5b7612b8":"### Feed forward neural network (32+16)","f8304d1f":"**Observations:**\n\n* Applying log transformation on the amount is removing skewness.","1bd94563":"**Observations:**\n\n* There are no visible correlations among variables, except for some correlation between `amount` and `errorBalanceOrig`.","e34e01ad":"**Observations:**\n\n* Train and test scores are similar, there is no model over-fit.","055430fa":"**Observations:**\n\n* Base decision tree classifier with default hyperparameters model performance is 100%, zero Type II errors. WOW!!\n* Now let's try to prune the tree and reduce the model complexity, and see if we can still get similar results with a simpler tree.","fbd84dd9":"### Error in destination account balance amount\n\nManually calculating the error in destination account balance amount after transaction.\n\nError = (Old Balance + Transaction Amount) - New Balance\n\nIf,\n<br>* Transaction Amount = (New Balance - Old Balance), then zero error value.\n<br>* Transaction Amount > (New Balance - Old Balance), then positive value.\n<br>* Transaction Amount < (New Balance - Old Balance), then negative value.\n\nNOTE: This applies only for outgoing transactions.","9030e793":"**Observations:**\n\n* Looking at the results, our base logit model with default hyper parameters is performing well on training data.\n* Examining the metrics of our interest, `Recall` score is 0.97, which is very high and is satisfactory.\n* Will have to see how this model works on unseen (test) data.","4157cc8a":"### Error in origin account balance amount\n\nManually calculating the error in origin account balance amount after transaction.\n\nError = (New Balance + Transaction Amount) - Old Balance\n\nIf,\n<br>* Transaction Amount = (Old Balance - New Balance), then zero error value.\n<br>* Transaction Amount > (Old Balance - New Balance), then negative value.\n<br>* Transaction Amount < (Old Balance - New Balance), then positive value.\n\nNOTE: This applies only for outgoing transactions.","a33e5901":"### Over-sampling using SMOTE\n\nCreating synthetic up-sampled dataset using SMOTE technique.","89a1db97":"# Data preprocessing","3ceff579":"### Splitting into train and test data","6d6f6022":"### Up sampling minority class - Simple bootstrapping\n\nUp sampling minority class using sampling with replacement technique.","afd3cc02":"### Base model on SMOTE data","247fe996":"**Observations:**\n\n* From the iteration results, optimized logit model did not outperform base model on train data.\n* Overall, the results are similar and stable.","9cc699f0":"**Observations:**\n\n* Setting threshold to 0.4 did improve recall a little at the expense of precision and type I errors.","91c00259":"**Observations:**\n\n* Compared to logit model, decision tree classifier has improved performance on test data. Reduced type II errors and improved recall score.\n* Let's see if we can reduce the number of type I errors keeping the type II errors similar to decision tree classifier model performance using complex models.","547dcc29":"**Observations:**\n\n* Train and test scores are similar, there is no model over-fit.\n* But, wait a minute, a perfect model..? Let's look at other metrics too.","bdcd0cf1":"### Testing on unseen data","67fa6df0":"### Flag to indicate zero destination account balance","84adbfa8":"**Observations:**\n\n* From the CV results, there is no difference in train and test performance.\n* Best model parameters: `max_depth` = 3, `criterion` = 'gini', `max_features` = None.","14b735a2":"### Finding optimal threshold"}}