{"cell_type":{"4ee5cc99":"code","92e8306a":"code","c6a45e6d":"code","4879a6f0":"code","f8e183be":"code","8e45bc47":"code","b4daf45b":"code","836f798d":"code","e5f5e796":"code","01e45d43":"code","63c90dbd":"code","1bf85766":"code","58c3e584":"code","33f75fb3":"code","bdf81ad2":"code","2be56c45":"code","31a7ced3":"markdown","cb1704ec":"markdown","03e2cf80":"markdown","34e5c442":"markdown","3897340c":"markdown","9239fcfd":"markdown","4b893aaa":"markdown","c9da188d":"markdown","151d3083":"markdown"},"source":{"4ee5cc99":"import numpy as np\nimport pandas as pd\nimport nltk\nimport re\nfrom bs4 import BeautifulSoup\n\nfrom tqdm.auto import tqdm","92e8306a":"TRAIN_DATA_PATH = \"\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\"\nVALID_DATA_PATH = \"\/kaggle\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\"\nTEST_DATA_PATH = \"\/kaggle\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\"","c6a45e6d":"df_train = pd.read_csv(TRAIN_DATA_PATH)\ndf_valid = pd.read_csv(VALID_DATA_PATH)\ndf_test = pd.read_csv(TEST_DATA_PATH)","4879a6f0":"cat_mtpl = {'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, \n            'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\n\nfor category in cat_mtpl:\n    df_train[category] = df_train[category] * cat_mtpl[category]\n\ndf_train['score'] = df_train.loc[:, 'toxic':'identity_hate'].mean(axis=1)\n\ndf_train['y'] = df_train['score']\n\nmin_len = (df_train['y'] > 0).sum()  # len of toxic comments\ndf_y0_undersample = df_train[df_train['y'] == 0].sample(n=min_len, random_state=41)  # take non toxic comments\ndf_train_new = pd.concat([df_train[df_train['y'] > 0], df_y0_undersample])  # make new df\ndf_train_new","f8e183be":"from tokenizers import (\n    decoders,\n    models,\n    normalizers,\n    pre_tokenizers,\n    processors,\n    trainers,\n    Tokenizer,\n)\n\nraw_tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\nraw_tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\nraw_tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\nspecial_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\ntrainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)","8e45bc47":"from datasets import Dataset\n\ndataset = Dataset.from_pandas(df_train_new[['comment_text']])\n\ndef get_training_corpus():\n    for i in range(0, len(dataset), 1000):\n        yield dataset[i : i + 1000][\"comment_text\"]","b4daf45b":"raw_tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)","836f798d":"from transformers import PreTrainedTokenizerFast\n\ntokenizer = PreTrainedTokenizerFast(\n    tokenizer_object=raw_tokenizer,\n    unk_token=\"[UNK]\",\n    pad_token=\"[PAD]\",\n    cls_token=\"[CLS]\",\n    sep_token=\"[SEP]\",\n    mask_token=\"[MASK]\",\n)","e5f5e796":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import Ridge","01e45d43":"def dummy_fun(doc):\n    return doc","63c90dbd":"labels = df_train_new['y']\ncomments = df_train_new['comment_text']\ntokenized_comments = tokenizer(comments.to_list())['input_ids']\n\nvectorizer = TfidfVectorizer(\n    analyzer = 'word',\n    tokenizer = dummy_fun,\n    preprocessor = dummy_fun,\n    token_pattern = None)\n\ncomments_tr = vectorizer.fit_transform(tokenized_comments)\ncomments_tr","1bf85766":"regressor = Ridge(random_state=42, alpha=0.8)\nregressor.fit(comments_tr, labels)","58c3e584":"# preprocess val data\nless_toxic_comments = df_valid['less_toxic']\nmore_toxic_comments = df_valid['more_toxic']\n\nless_toxic_comments = tokenizer(less_toxic_comments.to_list())['input_ids']\nmore_toxic_comments = tokenizer(more_toxic_comments.to_list())['input_ids']\n\nless_toxic = vectorizer.transform(less_toxic_comments)\nmore_toxic = vectorizer.transform(more_toxic_comments)\n\n# make predictions\ny_pred_less = regressor.predict(less_toxic)\ny_pred_more = regressor.predict(more_toxic)\n\n(y_pred_less < y_pred_more).mean()\n","33f75fb3":"texts = df_test['text']\ntexts = tokenizer(texts.to_list())['input_ids']\ntexts = vectorizer.transform(texts)","bdf81ad2":"df_test['prediction'] = regressor.predict(texts)\ndf_test = df_test[['comment_id','prediction']]\n\ndf_test['score'] = df_test['prediction']\ndf_test = df_test[['comment_id','score']]","2be56c45":"df_test.to_csv('.\/submission.csv', index=False)\ndf_test","31a7ced3":"# Imports","cb1704ec":"* Tokenizer (deberta-v3): 0.6699880430450379\n* Tokenizer (trained): 0.6674970107612594\n* Tokenizer (trained + dirty): 0.6716819449980072\n\n** Be careful, this results suggest that the 0.86 LB score is not reliable!!! Use at your own risk!","03e2cf80":"<h1><center>How to train a Huggingface Tokenizer + TFIDF + RIDGE<\/center><\/h1>     \n\n<center><img src = \"https:\/\/i.imgur.com\/iRX7hwu.png\" width = \"1000\" height = \"400\"\/><\/center>           \n\nThis notebook was inspided on the following other two notebooks:\n* https:\/\/www.kaggle.com\/vitaleey\/tfidf-ridge\n* https:\/\/www.kaggle.com\/pablorosa01\/naive-bayes-modeling-base-line\n\n<h3 style='background:orange; color:black'><center>Consider upvoting this notebook if you found it helpful.<\/center><\/h3>","34e5c442":"# Validation","3897340c":"# Predictions and load submission.csv","9239fcfd":"# Train the Model","4b893aaa":"# Scoring training data","c9da188d":"# Train the tokenizer","151d3083":"## Load Datasets"}}