{"cell_type":{"6ab6dd18":"code","20162498":"code","67ee62b3":"code","f55c8af8":"code","37751720":"code","70fa9b9d":"code","b335d9ae":"code","87089f6d":"code","c7c9bd9b":"code","db874915":"code","fe3b3887":"code","120f6ff2":"code","b9d6c574":"code","98004a6d":"code","a498751d":"code","28b863e0":"code","591f1794":"code","914382a1":"code","7233725c":"code","fde4f3c0":"code","521f6b22":"code","c79b7013":"code","4b322e82":"code","47dae4a3":"markdown","7561af23":"markdown","849ac30a":"markdown","e5708a9d":"markdown","3e566c30":"markdown","be91c078":"markdown","1c29ecdd":"markdown","64d60ae9":"markdown","34b06046":"markdown","2b768158":"markdown"},"source":{"6ab6dd18":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#load the csv file data into dataframe\ndf = pd.read_csv(\"..\/input\/zomato.csv\",encoding=\"ISO-8859-1\")\ncountry = pd.read_excel('..\/input\/Country-Code.xlsx') # load the country values from excel file\ndf = pd.merge(df, country, on='Country Code')\ndf.head()","20162498":"# how many data points for India?\ndf = df[(df['Country']=='India')]\ndf.shape # (row,column)","67ee62b3":"#Let's convert the boolean columns into integers \ndf['Has Table booking'].replace({'Yes':1,'No':0},inplace=True)\ndf['Has Online delivery'].replace({'Yes':1,'No':0},inplace=True)\ndf['Switch to order menu'].replace({'Yes':1,'No':0},inplace=True)\ndf.head()","f55c8af8":"# The average cost for two can be dependent on the cuisine of the restaurant\n# Continental or Italian will be more costly than NorthIndian :) \n# Let's find out the the number of unique cuisines.\ncuisines = list(set(df['Cuisines'].str.cat(sep=',').replace(\" \",\"\").split(',')))\ncuisines.sort()\ncuisines","37751720":"# Add one column for each cuisine and set value 1 for the column if restaurant \n# serves that cuisine else 0 \n# Ultimately ,we are doing conversion of categorical columns into numerical columns.\nfor cuisine in cuisines:\n    df[cuisine] = df['Cuisines'].str.contains(cuisine)\n    df[cuisine].replace({True:1,False:0},inplace=True)\ndf.head()\n# All cuisines added as column!","70fa9b9d":"# Let's find out how each column is linearly related with Cost for two\ncorr = df.corr()[['Average Cost for two']].sort_values('Average Cost for two', ascending=False)\ncorr[corr['Average Cost for two']>0.2] # min 20% correlation !","b335d9ae":"# We are going to pick the top 5 features for building our regression model. \ndf = df[['Price range','Has Table booking','Aggregate rating','Continental','Votes','Average Cost for two']]\ndf.head()\n#Our final dataset ! :) ","87089f6d":"# scaling data - all have different units for measurement. \n# We want all values to be across the same scale because cost for two\n# is in rupees whereas aggregate rating is simple numbers from 0 to 5.\n# Let's scale using min max scaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaler.fit(df)\ndf = pd.DataFrame(scaler.transform(df),columns=df.columns)\ndf.head()","c7c9bd9b":"# Let's visualize how each columns is related to all other columns of the dataframe.\npd.scatter_matrix(df,figsize=(16,9),diagonal='kde',alpha=0.2)","db874915":"# split data into training and testing set\nfrom sklearn.model_selection import train_test_split\n\ntrain,test = train_test_split(df,random_state=50)\nX_train = train.iloc[:,df.columns!='Average Cost for two']\nX_test = test.iloc[:,df.columns!='Average Cost for two']\ny_train = train['Average Cost for two']\ny_test = test['Average Cost for two']\nprint('Training set size - ' , X_train.shape)\nprint('Testing set size - ' , X_test.shape)","fe3b3887":"# We are going to start with the simplest type of regression - LinearRegression :) \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\nreg = LinearRegression()\nreg.fit(X_train, y_train)\ny_pred = reg.predict(X_test)\nscore = r2_score(y_test,y_pred)\nscore\n# Not that impressive :( ","120f6ff2":"# Function to plot validation curve\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nimport matplotlib.pyplot as plt\n\ndef plot_validation_curve(model,param_name,x_label,param_range=np.arange(1,7)):\n    train_scores,validation_scores = validation_curve(model,X_train, y_train,\n                                                     param_name = param_name,param_range=param_range,\n                                                     scoring='r2',cv=3)\n    validation_scores[validation_scores < 0] = 0 # we are not going to plot any negative numbers!\n    print('Training scores  ',train_scores.mean(axis=1))\n    print('Validation scores  ',validation_scores.mean(axis=1))\n    \n    plt.figure(figsize=(6, 4))\n    plt.plot(param_range,validation_scores.mean(axis=1),lw=2, label='validation')\n    plt.plot(param_range,train_scores.mean(axis=1),lw=2, label='training')\n    plt.xlabel(x_label)\n    plt.ylabel('Score')\n    plt.title('Validation curve')\n    plt.legend(loc='best')\n    plt.show()","b9d6c574":"model = make_pipeline(PolynomialFeatures(),LinearRegression())\nplot_validation_curve(model,'polynomialfeatures__degree',x_label='Degree of polynomial')","98004a6d":"polynomial_features= PolynomialFeatures(degree=3)\nX_train_poly = polynomial_features.fit_transform(X_train)\nX_test_poly = polynomial_features.fit_transform(X_test)\nreg = LinearRegression()\nreg.fit(X_train_poly, y_train)\ny_pred = reg.predict(X_test_poly)\nscore = r2_score(y_test,y_pred)\nscore","a498751d":"from sklearn.tree import DecisionTreeRegressor\nplot_validation_curve(DecisionTreeRegressor(random_state=42),'max_depth','Max Depth')","28b863e0":"reg = DecisionTreeRegressor(max_depth=3,random_state=42)\nreg.fit(X_train, y_train)\ny_pred = reg.predict(X_test)\nscore = r2_score(y_test,y_pred)\nscore\n","591f1794":"from sklearn.ensemble import RandomForestRegressor\nplot_validation_curve(RandomForestRegressor(random_state=50,n_estimators=10),'max_depth','Max Depth',np.arange(1,10))","914382a1":"# From the graph, we can see that 4 is the best value for max_depth after which \n# the cross validation error starts to increase. \n# How about we confirm that using GridSearch Cross validation ;) \nfrom sklearn.model_selection import GridSearchCV\nrfr_cv = GridSearchCV(RandomForestRegressor(random_state=50, n_estimators=100),\n                     param_grid={'max_depth': np.arange(1,10)},\n                     scoring='r2', cv=3)\nrfr_cv.fit(X_train, y_train)\nrfr_cv.best_params_","7233725c":"from sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.metrics import r2_score\n\nreg = RandomForestRegressor(max_depth=4,n_estimators=10,random_state=50)\nreg.fit(X_train, y_train)\ny_pred = reg.predict(X_test)\nscore = r2_score(y_test,y_pred)\nscore","fde4f3c0":"from sklearn.model_selection import learning_curve\nunderfitting_max_depth = 3\nbest_max_depth = 4\noverfitting_max_depth = 20\n\ndef plot_learning_curve(max_depth,title):\n    train_sizes, train_scores, validation_scores = learning_curve(RandomForestRegressor\n                                                                  (max_depth=max_depth,n_estimators=18,\n                                                                   random_state=50),\n                                                                  X_train, y_train, cv = 5,\n                                                                  train_sizes = np.linspace(.1, 1.0, 5))\n    plt.plot(train_sizes, validation_scores.mean(axis=1), 'o-', color=\"r\", label=\"Cross-validation score\")\n    plt.plot(train_sizes, train_scores.mean(axis=1), 'o-', color=\"g\", label=\"Training score\")\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    plt.legend(loc='best') \n    plt.title(title)","521f6b22":"plot_learning_curve(1,'Under-fitting')\n# Meet at a lower point. Low training and testing score as number of samples increases.\n# Performs bad on both training and cross-validation data.","c79b7013":"plot_learning_curve(4,'Best fit')\n# Meet at some higher point as both training and testing scores are high.","4b322e82":"plot_learning_curve(50,'Over-fitting')\n# The lines are far apart. High training score(memorizes the data) but low cross-validation score. \n# So, the lines never meet!","47dae4a3":"As we can see that LinearRegression gives us r2_score of 0.7 but we should try to find out if any other model can give us better results. Let's try to build a model using **Polynomial Regression.** To help us find the best degree of fit for polynomial regression, we will be using **validation curve.** ","7561af23":"The score above is not better than polynomial regression but definitely better than Linear Regression.\nWell, there's another algorithm - **RandomForestRegressor** which I am going to try next!\nIt fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.","849ac30a":"Our r2_score increased from 0.7 to 0.82 ! Not bad ;) \nDo you think **Decision Tree** can give better results? Let's find out ! :) \nWe are going to use validation curve to help us find the best number for max_depth.","e5708a9d":"Learning Curves can give a clear indication of under-fitting\/over-fitting.\nLet's visualize it wrt RandomForestRegressor :) ","3e566c30":"Following the validation curve, our best pick for **max_depth should be 3.  **","be91c078":"**Learning Curve**","1c29ecdd":"In this kernel, I am going to build a regression model that can predict the ***Average Cost for two***.\nLet's get started ! :) ","64d60ae9":"This score is very close to polynomial regression! \nI think Polynomial Regression is the winner wrt r2_score :) ","34b06046":"Both the training and cross validation scores increase till 3rd degree of polynomial after which the cross validation score starts to decrease while the training score keeps increasing i.e our model starts to overfit for higher degree of polynomial. So, our best pick for polynomial **degree is 3**. Let's build the model and test it on our test data!","2b768158":"Thank you for going through this kernel. Please upvote if you found this uselful :) "}}