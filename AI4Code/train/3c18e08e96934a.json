{"cell_type":{"c6b8ebf9":"code","202600cb":"code","03cc9c9e":"code","38e21271":"code","77e387fa":"code","ee1dd427":"code","df5c3d4e":"code","4649c3d9":"code","a9937351":"code","7fc1a7c4":"code","7241e907":"code","75e9398f":"code","f1359cc3":"code","9d6d13d0":"code","dc8a97f4":"code","5e16ccc6":"code","efea8785":"code","21518c6a":"code","49dd7df0":"code","ccf0ed45":"code","2fd9818a":"code","2f45529e":"code","2d54ada5":"code","d7e09b46":"code","98809f3e":"code","7baf6b9e":"code","fdb7fd5b":"code","f8be4445":"code","8470f8e3":"code","c2b2ae62":"code","14071e32":"code","54caa919":"code","0579eade":"code","f1025850":"code","a93f7d8c":"code","00d7e53d":"code","d41f70b5":"code","8bbd18e5":"code","e6a0967c":"code","3f705424":"markdown","f777bfd7":"markdown","d1f41a92":"markdown","6d262226":"markdown","969dc286":"markdown","1f7c8055":"markdown","a4896074":"markdown","ce09fd8d":"markdown","fe888fb2":"markdown","582de7da":"markdown","66c3aca7":"markdown","8017a1b2":"markdown","e60eec58":"markdown","8bfee60c":"markdown","ee5d4676":"markdown","596e78c9":"markdown"},"source":{"c6b8ebf9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directo\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","202600cb":"#!pip install lightgbm\n#!pip install catboost","03cc9c9e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import *\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler,QuantileTransformer,RobustScaler,PowerTransformer\nfrom sklearn.model_selection import  RandomizedSearchCV\nfrom sklearn import model_selection\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Ridge, RidgeCV,Lasso,LinearRegression\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.tree import DecisionTreeRegressor\nimport lightgbm as lgbm\nimport xgboost as xgb\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import (\n    AdaBoostRegressor,\n    GradientBoostingRegressor,\n    RandomForestRegressor,\n    BaggingRegressor\n)\nfrom mlxtend.regressor import StackingCVRegressor\nimport warnings\nwarnings.filterwarnings('ignore') # To supress warnings\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.max_rows', 300)\npd.set_option('display.max_columns', 90)\npd.set_option('display.max_colwidth',200)","38e21271":"df_train=pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/train.csv\")\ndf_test=pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/test.csv\")\n","77e387fa":"df_train.head()","ee1dd427":"df_test.head()","df5c3d4e":"print(\"#\"*40,\"\\nTrain\",)\nprint(f'There are {df_train.shape[0]} rows and {df_train.shape[1]} columns') # fstring \n#missing_df = pd.DataFrame({\n#    \"Missing\": df_train.isnull().sum(),\n#    \"Missing %\": round((df_train.isnull().sum()\/ df_train.isna().count()*100), 2)\n#})\n#display(missing_df.sort_values(by='Missing', ascending=False))\nprint(f'There are {df_train.isnull().sum().mean()} missing values')","4649c3d9":"print(\"#\"*40,\"\\nTest\",)\nprint(f'There are {df_test.shape[0]} rows and {df_test.shape[1]} columns') # fstring \nmissing_df = pd.DataFrame({\n    \"Missing\": df_test.isnull().sum(),\n    \"Missing %\": round((df_test.isnull().sum()\/ df_test.isna().count()*100), 2)\n})\ndisplay(missing_df.sort_values(by='Missing', ascending=False))\n\nprint(f'There are {df_test.isnull().sum().mean()} missing values')","a9937351":"#### Check the data types of the columns for the dataset.\ndf_train.info()","7fc1a7c4":"#dropping id column\ndf_train.drop(['id'],inplace=True,axis=1)","7241e907":"intfeatures=df_train.select_dtypes(include='int64')\nintfeatures.columns","75e9398f":"floatfeatures=df_train.select_dtypes(include='float')\nfloatfeatures.columns","f1359cc3":"floatfeatures.head(10)","9d6d13d0":"floatfeatures.tail(10)","dc8a97f4":"#### Check the data types of the columns for the dataset.\ndf_test.info()","5e16ccc6":"# Colors to be used for plots\ncolors = [\"#f72585\",\"#b5179e\",\"#7209b7\",\"#560bad\", \"#3a0ca3\", \"#3f37c9\", \"#3f37c9\",\n          \"#4361ee\", \"#4895ef\", \"#4895ef\", \"#4cc9f0\"\n          ]","efea8785":"df_train.describe().T.style.bar(subset=['mean'], color='#f72585')\\\n                           .background_gradient(subset=['50%'], cmap='PiYG') # highlight median","21518c6a":"#checking f60 , it has very larges values .\ndf_train['f60']","49dd7df0":"df = df_train.drop([\"loss\"],axis=1)\ncolumns = df.columns.values\ncols = 4\nrows = len(columns) \/\/ cols + 1\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(19,100), sharex=False) #subplot with all rows\nplt.subplots_adjust(hspace = 0.4)\ni=0\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            axs[r,c].hist(df_train[columns[i]].values,\n                                   color=\"#f72585\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Train Dataset\",bins=40)\n            axs[r, c].set_title(columns[i], fontsize=17, pad=4)\n            axs[r, c].tick_params(axis=\"y\", labelsize=11)\n            axs[r, c].tick_params(axis=\"x\", labelsize=11)\n           \n        i+=1\n\nplt.show();","ccf0ed45":"sns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.distplot(df_train['loss'], color=\"purple\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"Loss\")\nax.set(title=\"Loss distribution\")\nsns.despine(trim=True, left=True)\nplt.show()","2fd9818a":"# Skew and kurt\nprint(\"Skewness: %f\" % df_train['loss'].skew())\nprint(\"Kurtosis: %f\" % df_train['loss'].kurt())","2f45529e":"# Find skewed numerical features\nkurt_features = df_train[columns].kurt().sort_values(ascending=False)\n\nhigh_kurt = kurt_features[kurt_features >3]\nkurt_index = high_kurt.index\n\nprint(\"There are {} numerical features with kurtosis > 3 :\".format(high_kurt.shape[0]))\nskewness = pd.DataFrame({'Skew' :high_kurt})\nkurt_features.head(10)","2d54ada5":"# Find skewed numerical features\nskew_features = df_train[columns].skew().sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nprint(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\nskewness = pd.DataFrame({'Skew' :high_skew})\nskew_features.head(10)","d7e09b46":"#checking intgers and some feature with bimodal  distrbution\nplt.figure(figsize=(15,25))\ncolumns=['f1','f3', 'f11','f12','f16','f18' ,'f27', 'f55', 'f86','f60','f76','f57','f84','f93','loss']\nsns.pairplot(df_train[columns],corner=True)\n","98809f3e":"corr = df_train.corr()\nmask = np.triu(np.ones_like(corr, dtype = bool))\n\nplt.figure(figsize = (10, 10))\nplt.title('Correlation matrix for Train data')\nsns.heatmap(corr, mask = mask,  linewidths = .5,square=True,cbar_kws={\"shrink\": .60})\nplt.show()","7baf6b9e":"#check correlation with target varaiable\nloss_corr=corr['loss'].sort_values(ascending=False).head(10).to_frame()\n\ncm = sns.light_palette(\"pink\", as_cmap=True)\n\nloss = loss_corr.style.background_gradient(cmap=cm)\nloss","fdb7fd5b":"X = df_train.drop(['loss'], axis=1).values\ny = df_train['loss'].values\n","f8be4445":"# Setup cross validation folds\nkf = KFold(n_splits=5, random_state=1, shuffle=True)\n\n# Define error metrics\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X_pass,y_pass):\n    rmse = np.sqrt(-cross_val_score(model, X_pass, y_pass, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","8470f8e3":"X_train, X_valid, y_train, y_valid = train_test_split(\n   X, y, test_size=0.3, random_state=42\n)","c2b2ae62":"# Initialize model using pipeline\npipe_lr = make_pipeline( PowerTransformer(), (LinearRegression()))\n\npipe_lasso = make_pipeline( PowerTransformer(), (Lasso(alpha=0.1)))\n\npipe_elastic = make_pipeline( PowerTransformer(), (ElasticNet(alpha=0.2,l1_ratio=0.6,random_state=1)))\n","14071e32":"\nresults = []  # Empty list to store all model's CV scores\nnames = []  # Empty list to store name of the models\nscores={}\nscore = cv_rmse(pipe_lr,X_train,y_train)\nresults.append(score)\nnames.append(\"linear\")\nprint(\"Linear: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['Linear'] = (score.mean(), score.std())\nscore = cv_rmse(pipe_lasso,X_train,y_train)\nresults.append(score)\nnames.append(\"lasso\")\nprint(\"Lasso: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['lasso'] = (score.mean(), score.std())\nresults.append(score)\nnames.append(\"elastic\")\nscore = cv_rmse(pipe_elastic,X_train,y_train)\nprint(\"Elastic: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['Elastic'] = (score.mean(), score.std())\n","54caa919":"# Plotting boxplots for CV scores of all models defined above\nfig = plt.figure(figsize=(10, 7))\n\nfig.suptitle(\"Algorithm Comparison\")\nax = fig.add_subplot(111)\n\nplt.boxplot(results)\nax.set_xticklabels(names)\n\nplt.show()","0579eade":"model_lr = pipe_lr.fit(X_train, y_train)\nmodel_lr.score(X_valid, y_valid)\ny_pred_lr = model_lr.predict(X_valid)\nprint('mean_absolute_error: ', mean_absolute_error(y_valid, y_pred_lr))\nprint('root_mean_squared_error: ', rmsle(y_valid, y_pred_lr))","f1025850":"lasso_model = pipe_lasso.fit(X_train, y_train)\nlasso_model.score(X_valid, y_valid)\ny_pred_lasso = lasso_model.predict(X_valid)\nprint('mean_absolute_error: ', mean_absolute_error(y_valid, y_pred_lasso))\nprint('root_mean_squared_error: ', rmsle(y_valid, y_pred_lasso))","a93f7d8c":"elasticmod = pipe_elastic.fit(X_train, y_train)\nelasticmod.score(X_valid,y_valid)","00d7e53d":"y_pred_ela = elasticmod.predict(X_valid)\nprint('mean_absolute_error: ', mean_absolute_error(y_valid, y_pred_ela))\nprint('root_mean_squared_error: ', rmsle(y_valid, y_pred_ela))","d41f70b5":"#fit on complete train data\nfinal = pipe_lr.fit(X, y)","8bbd18e5":"finpreds = final.predict(df_test.drop('id', axis=1))","e6a0967c":"df_sub = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')\ndf_sub['loss'] = finpreds\ndf_sub.to_csv('Linearsubmission.csv' , index = False)\ndf_sub.head(10)","3f705424":"\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color:  #852aa8; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\">Exploratory Data Analysis<\/h2> ","f777bfd7":"\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color:  #852aa8; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\">Read and Understand Data<\/h2> ","d1f41a92":"<h2 style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: #852aa8; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Tabular Playground Series - Aug 2021<\/h2> \n","6d262226":"Checking distribution of Target Varaible `Loss`","969dc286":"**Skewness**\n\nIt is the degree of distortion from the symmetrical bell curve or the normal distribution. It measures the lack of symmetry in data distribution.\nIt differentiates extreme values in one versus the other tail. A symmetrical distribution will have a skewness of 0.\n \n**when is the skewness too much?**\n\nThe rule of thumb seems to be:\nIf the skewness is between -0.5 and 0.5, the data are fairly symmetrical.\nIf the skewness is between -1 and -0.5(negatively skewed) or between 0.5 and 1(positively skewed), the data are moderately skewed.\nIf the skewness is less than -1(negatively skewed) or greater than 1(positively skewed), the data are highly skewed.\n\n**Kurtosis**\nKurtosis is all about the tails of the distribution \u2014 not the peakedness or flatness. It is used to describe the extreme values in one versus the other tail. It is actually the measure of outliers present in the distribution.Kurtosis > 3: Distribution is longer, tails are fatter. Peak is higher and sharper than Mesokurtic, which means that data are heavy-tailed or profusion of outliers.","1f7c8055":"<h2 style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: #852aa8; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\">Introduction<\/h2> \n","a4896074":"**Just starting with simple model , and then will try feature selection ,converting few features in categorical, and some other models. If you like my Notebook please upvote,Thank you.**","ce09fd8d":"<h4 style = \"font-size:30px; font-family:Garamond ; font-weight : medium;  color :#852aa8   ; text-align: left; \"><b>Observation <\/b><\/h4>\n- Most of the features are right skewed with outliers on higher end\n\n- Target variable loss feature is rightskewed with outliers on higher end\n\n- Some feautures are bimodal, trimodal.\n\n- very few features have normal disturbution.\n- few features have negative values\n- f26, f86 can be converted to categorical features\n- There is no significant correlation between features\n- Instead of standardscaling which is not very robut at handling outliers we can trying using robust scaler \/ quantile transformer \/ power transform so data follows a normal distribution.\n\n","fe888fb2":"<h2 style = \"font-family:TimesNewRoman;color:black;font-weight:bold\">Table of Contents<\/h2>\n\n- [Introduction](#Introduction) \n- [Libraries](#Import-Libraries)\n- [Read and Understand Data](#Read-and-Understand-Data)\n- [Exploratory Data Analysis](#Exploratory-Data-Analysis) \n- [Data Preparation](#Data-Preparation)\n- [Model Building ](#Model-Building)\n","582de7da":"## Linear Regression,Lasso,ElasticNet","66c3aca7":"\n\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color:  #852aa8; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\">Model Building<\/h2> ","8017a1b2":"\n**Here I am building different models using KFold and cross_val_score with pipelines and scaling with powertransform**","e60eec58":"Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, Kaggle have launched many Playground competitions that are more approachable than Featured competition, and thus more beginner-friendly.\n\nThe goal of these competitions is to provide a fun, but less challenging, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition.\n\nThe dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with calculating the loss associated with a loan defaults. Although the features are anonymized, they have properties relating to real-world features.\n\n**Task:Calculating the loss associated with a loan defaults.The features are anonymized, but they have properties relating to real-world features.**\n\n**What is loan default?**\n\nDefault is a failure to repay a debt\/loan on time. It can occur when a borrower fails to make timely payments on loans such as mortgage, bank loans, car leases, etc.\n\n**Metric**\n\n Submissions are scored on the root mean squared error.","8bfee60c":"<h4 style = \"font-size:30px; font-family:Garamond ; font-weight : medium;  color :#852aa8   ; text-align: left; \"><b>Observation <\/b><\/h4>\n\n- Training set has 250K observations with 102 features .\n- Testing set has 150K observations with 101 features\n- `Loss` column is the target variable which is only available in the train dataset.\n- There are no missing values in both sets\n- In train dataset, 95 features are float64 and 7('id', 'f1', 'f16', 'f27', 'f55', 'f86', 'loss') are of  int64 type\n","ee5d4676":"<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color:  #852aa8; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\">Import Libraries<\/h2> ","596e78c9":"\nThis is my first time participating in Tabular Playground series.I looked at the dataset and was compeltely lost as to where do I start , I had never worked with dataset with 100 columns. Second probelm since the features are anonymous , its very diffcult to make sense of data , relationship between features, deriving new features. Appreciate any kind of feedback."}}