{"cell_type":{"dd62069f":"code","0ac88457":"code","8043fe24":"code","019c6e38":"code","00226c7e":"code","63a08bf3":"code","a8db8ccc":"code","5fca7603":"code","061cb30f":"code","de61acb8":"code","59d04e76":"code","242c12d0":"code","ff8ffbac":"code","c5d1a3ad":"code","2063c1ca":"code","f3ab174c":"code","8251893c":"code","d242d3f9":"code","0e9ed7e5":"code","9a032183":"code","8efc40fe":"code","2db28f22":"code","6cc5cfca":"code","f4dee4e4":"code","4f22e890":"markdown","e0a02600":"markdown","4c1cdf8d":"markdown","f7b81865":"markdown","5e04368c":"markdown","40f65a0a":"markdown","0933841a":"markdown","ac684d08":"markdown","f38d7923":"markdown","671d0c00":"markdown","d89e4df2":"markdown","50434cee":"markdown","6214f1a5":"markdown"},"source":{"dd62069f":"import tensorflow as tf\nimport random\nimport sklearn.model_selection\nfrom tensorflow import keras\nfrom keras.layers import Dense, Flatten, Dropout\nfrom keras.models import Sequential\nimport numpy as np\nimport pandas as pd\nimport os\nfrom numpy import loadtxt\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport xgboost as xgb\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0ac88457":"path = '\/kaggle\/input\/titanic\/'\ntrain_data = pd.read_csv(path + 'train.csv')","8043fe24":"random.seed(1912)\ntrain_data.sample(frac=1)\ntrain_data.shape\n# splitting AFTER feature extraction pevents problems with one-hot encoding of features","019c6e38":"def featureExtractWrapper(data):\n    # some people likely have more \"missingness\" than others - maybe especially if they died an couldn't be questioned afterwards?\n    # for each row sum the NaNs\n    missings = np.array(data.isnull().sum(axis=1))\n    missings = np.array(pd.get_dummies(missings))\n    parch = np.expand_dims(np.array(data.Parch\/4), axis=1)\n    sibs = np.expand_dims(np.array(data.SibSp), axis=1)\n    pclass = pd.get_dummies(data.Pclass)\n    pclass.columns=['class_1', 'class_2', 'class_3']\n    pclass = np.array(pclass)\n    name_length = np.array([len(name) for name in data.Name])\n    name_length  =  np.expand_dims(name_length, axis=1)\n    mister = [name  for name in data.Name if 'Mr.' in name]\n    mister = np.array([int(name in mister) for name in data.Name])\n    mister = np.expand_dims(mister, axis=1)\n    miss = [name  for name in data.Name if 'Miss' in name]\n    miss = np.array([int(name in miss) for name in data.Name])\n    miss = np.expand_dims(miss, axis=1)\n    missus = [name  for name in data.Name if 'Mrs' in name]\n    missus = np.array([int(name in missus) for name in data.Name])\n    missus = np.expand_dims(missus, axis=1)\n    sex_frame = np.array(pd.get_dummies(data.Sex))[:,1]\n    sex_frame = np.expand_dims(sex_frame, axis=1)\n    sex_frame = np.array(pd.get_dummies(data.Sex))[:,1]\n    sex_frame = np.expand_dims(sex_frame, axis=1)\n    fares =  np.array(( data.Fare -min(data.Fare) )\/ (max(data.Fare)-min(data.Fare)))\n    fares = np.expand_dims(fares, axis=1)\n    # perhaps people with smaller ticket numbers booked in advance and were better off or had better cabins?\n    ticketlen = np.array([len(ticket) for ticket in data.Ticket])\n    ticketlen = ( max(ticketlen) - ticketlen )\/   (max(ticketlen) - min(ticketlen))\n    ticketlen = np.expand_dims(ticketlen, axis=1)\n    embarked = np.array(pd.get_dummies(data.Embarked))\n    age = np.array(data.Age - data.Age.mean())\/data.Age.std()\n    age = np.expand_dims(age, axis=1)\n    adult = np.array((data.Age >= 21).astype(int))\n    adult = np.expand_dims(adult, axis=1)\n    older = np.array((data.Age >= 50).astype(int)) # old as in 1912 old\n    older = np.expand_dims(older, axis=1)\n    array_list = [pclass, name_length, mister, ticketlen,\n                  missus, miss, sex_frame, \n                  fares, sibs, parch, embarked, adult, \n                  older]\n    [print(x.shape) for x in array_list]\n    x_train = np.concatenate(array_list, axis=1)\n    if 'Survived' in data.columns:\n        y_train = np.array(data.Survived)\n        return x_train, y_train\n    else:\n        return  x_train","00226c7e":"def getModel(input_shape): # I trained it first with dropout but actually if the NN \n    #is only one arm of an ensemble of methods, you WANT it to overfit\n    model = Sequential([\n        Dense(320, activation='relu', input_shape = input_shape),\n        Dense(320, activation='relu'),\n        Dense(160, activation='relu'),\n        Dense(40, activation='relu'),\n        Dense(10, activation='relu'),\n        Dense(4, activation='relu'),\n        Dense(1,  activation='sigmoid')    \n    ])\n    return model","63a08bf3":"x_train, y_train = featureExtractWrapper(train_data)\nx_train.shape, y_train.shape","a8db8ccc":"x_train, x_valid, y_train, y_valid =  sklearn.model_selection.train_test_split(x_train, y_train,  shuffle=False, test_size=0.2)","5fca7603":"x_train.shape, x_valid.shape, y_train.shape, y_valid.shape","061cb30f":"model = getModel(x_train[0].shape)","de61acb8":"model.summary()","59d04e76":"model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.1e-2), loss='binary_crossentropy', metrics=['accuracy'])\n\n# Learnign raets less than this get stuck at local minima\n","242c12d0":"history = model.fit(x_train, y_train, batch_size=128, epochs=1000, validation_data=(x_valid, y_valid), verbose=0)","ff8ffbac":"test_data = pd.read_csv('..\/input\/titanic\/test.csv')\nx_test = featureExtractWrapper(test_data) ","c5d1a3ad":"nn_predict_survived = (model.predict(x_test)>0.5).astype(int)","2063c1ca":"from sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nrf_model = RandomForestClassifier(n_estimators=1000, max_depth=5, random_state=2)\nrf_model.fit(X, y)\nrf_predictions = rf_model.predict(X_test)\n\nrf_predictions.shape\n","f3ab174c":"npa = np.array(nn_predict_survived)\nnn_predictions= np.squeeze(npa)","8251893c":"sum(nn_predictions == rf_predictions)\/len(test_data) # so they agree in 80% of cases... that's reasonable","d242d3f9":"xgb_model = XGBClassifier(max_depth=10, n_estimators=1000)\nxgb_model.fit(x_train, y_train)","0e9ed7e5":"print(xgb_model)","9a032183":"# Make test set predictions\n\ny_pred = xgb_model.predict(x_test)\nxgb_predictions = [round(value) for value in y_pred]# evaluate predictions","8efc40fe":"a = sum(rf_predictions == xgb_predictions)\/len(test_data) # so they agree in 82% of cases... that's reasonable\nb = sum(nn_predictions == xgb_predictions)\/len(test_data) # so they agree in 82% of cases... that's .... reasonable\na, b","2db28f22":"y_hat = nn_predictions + xgb_predictions + rf_predictions # so they agree in 90% of cases... that's reasonable\npredictions = [int(x) for x in y_hat>=2]","6cc5cfca":"submission = {'PassengerId' : test_data.PassengerId, 'Survived' : predictions}","f4dee4e4":"pd.DataFrame(submission).to_csv('submission.csv', index=False)","4f22e890":"28\/28 [==============================] - 0s 2ms\/step - loss: 0.3463 - accuracy: 0.8608\n[0.34633705019950867, 0.860830545425415]","e0a02600":"<center><img src=\"https:\/\/i.insider.com\/5085ae166bb3f73174000004?width=1300&format=jpeg&auto=webp\"><\/img><\/center>","4c1cdf8d":"<center><img src='https:\/\/newsong.us\/wp-content\/uploads\/2018\/10\/maxresdefault.jpg'><\/center>","f7b81865":"No Idea why I'm choosing this particular weighting - if anyone has good ideas I'm all ears","5e04368c":"## Get the data","40f65a0a":"## Imports","0933841a":"#  <center>Build a (deliberately overfitted) NN model<\/center>","ac684d08":"#  <center>Neural network being overfitted and dumped into an ensemble<\/center>\n<center><img src='https:\/\/qph.fs.quoracdn.net\/main-qimg-233a5cb74db1a6c3aa0e1b9f8c2cf7bd')><\/img><\/center>","f38d7923":"## Develop some extracted features for the Neural Network Model","671d0c00":"\n<center><img src=\"https:\/\/i.insider.com\/5085ae1aecad04f711000000?width=1300&format=jpeg\" width=\"500\" height=\"500\" align=\"center\"\/><\/center>\n","d89e4df2":"## Feature extract","50434cee":"# Let's ensemble the RF, shallow neural network, and a  gradient boosted tree model","6214f1a5":"## Shuffle the training data and then take out validation dataset for the ensemble"}}