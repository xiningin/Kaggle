{"cell_type":{"6463abee":"code","1ac3212e":"code","3027edbe":"code","9ed227b4":"code","7008747b":"code","a66e6b13":"code","500a8806":"code","eeded791":"code","9582eab0":"code","53442878":"code","753a6fd0":"code","f827a085":"code","0f371b74":"code","045e04b7":"code","516dd40c":"code","3d1243c3":"code","8759ae8d":"code","b8d1bb28":"code","64200efb":"code","0bc12b8c":"code","d840de6f":"code","4b9980ac":"code","fd59b888":"code","f179d113":"code","66e72e04":"code","ed722d5a":"code","fd0d1073":"code","19183f03":"code","34bab268":"code","d5537d9b":"code","02ad19c6":"code","91fbf298":"code","ff45c588":"code","d10bebc3":"code","ef53bd89":"code","646b01d5":"code","f36cd8e3":"code","ddf05bb4":"code","b145a3f7":"code","d9b7dfeb":"code","333a1ac2":"code","d2b44544":"code","752f528c":"code","8ad9fa73":"code","76427581":"code","b5dd5b8a":"code","01cf8fad":"code","1f2792cf":"code","a0caaa2e":"code","d51644f2":"code","7d312e42":"code","4a3d8bb6":"code","d93b034e":"code","eebbcdb7":"code","3a3cc06d":"code","a96e6692":"code","7d9729da":"code","05e16c4d":"code","96c390f6":"code","e2817c8d":"code","502752e7":"code","18d8a5b0":"code","ac6ca69f":"code","a56fea6c":"code","8116bb5d":"code","8b0b6445":"code","2b2fdcfb":"code","a91052ec":"markdown","cc6c57e6":"markdown","e664a9f7":"markdown","cb9417e7":"markdown","c4e8df04":"markdown","498b6675":"markdown","6317748b":"markdown","5736ca56":"markdown","df2b5003":"markdown","51c3f385":"markdown","3a37a43e":"markdown","a04de142":"markdown","e1e372a2":"markdown","0ece68cb":"markdown","dcf4babb":"markdown","04cc957b":"markdown","61390777":"markdown","dda33148":"markdown","2fe5185b":"markdown","225da468":"markdown"},"source":{"6463abee":"#import necessary packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nimport seaborn as sb\nimport sklearn\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport os\nfrom datetime import datetime\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV\nfrom sklearn.metrics import mean_squared_error\n\nfrom scipy import stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p","1ac3212e":"#Read dataset\n# df_train = pd.read_csv('data\/train.csv')\n# df_test = pd.read_csv('data\/test.csv')\n\n#Reading datasets train and test\ndf_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","3027edbe":"#Review dataset\ndf_train.head()","9ed227b4":"df_test.head()","7008747b":"#Review dataset size and shapes\ndf_train.shape, df_test.shape","a66e6b13":"#take a back up copy of dataset\ndf_train_copy = df_train.copy()\ndf_test_copy = df_test.copy()","500a8806":"#Plot the outliers\nplt.figure(figsize=(5,4))\nplt.scatter(x=df_train['GrLivArea'], y=df_train['SalePrice'])\nplt.xlabel('Ground Live Area')\nplt.ylabel('SalePrice')\nplt.show();","eeded791":"df_train.shape","9582eab0":"#Delete the outliers\ndelidx = df_train[df_train['GrLivArea']>4000].index\ndf_train = df_train.drop(delidx, axis=0)\ndf_train.shape","53442878":"#review outliers removal\nplt.figure(figsize=(5,4))\nplt.scatter(x=df_train['GrLivArea'], y=df_train['SalePrice'])\nplt.xlabel('Ground Live Area')\nplt.ylabel('SalePrice')\nplt.show();","753a6fd0":"ntrain = df_train.shape[0]\nntest = df_test.shape[0]\nntrain, ntest","f827a085":"#Target column is identified as House SalePrice\ny_train = df_train['SalePrice'].values","0f371b74":"#Drop column id as that do not contribute to calculate SalePrice\nsrs_testid = df_test['Id'] #Take backup of testid for final submission file\n\ndf_train.drop('Id', inplace=True, axis=1)\ndf_test.drop('Id', inplace=True, axis=1)\n\n#Validate column reduction\ndf_train.shape, df_test.shape","045e04b7":"#Visualize target data\nplt.figure(figsize=(8,5))\nsb.distplot(y_train, fit=norm)\nplt.show();","516dd40c":"#Since its not normalized taking log1p to normalize it\nplt.figure(figsize=(8,5))\nsb.distplot(np.log1p(y_train), fit=norm)\nplt.show();","3d1243c3":"#As the graph now shown to be normalized, Hence transforming yvalue to log1p\ny_train = np.log1p(y_train)","8759ae8d":"#Concate both train and test dataset for data transformation\ndf_alldata = pd.concat([df_train, df_test], axis=0).reset_index(drop=True)\n\n#Review all data\ndf_alldata.shape","b8d1bb28":"#drop target column SalePrice from all data\ndf_alldata.drop('SalePrice', \n               inplace=True,\n               axis=1)","64200efb":"df_alldata.head(3)","0bc12b8c":"#Find null values in dataset\nalldata_na = df_alldata.isnull().sum()\nalldata_na = alldata_na[alldata_na>0]\nalldata_na = alldata_na.sort_values(ascending=False)\nprint('No of columns with nulls: ', len(alldata_na))","d840de6f":"#For selected columns below impute missing values with 'None'\nnonecols=['PoolQC','MiscFeature','Alley','Fence','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','MasVnrType','MSSubClass']\n\nfor col in nonecols:\n    df_alldata[col] = df_alldata[col].fillna('None')","4b9980ac":"#For selected columns below impute missing values with 0\nzerocols = ['GarageYrBlt','GarageArea','GarageCars', 'BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath','MasVnrArea']\n\nfor col in zerocols:\n    df_alldata[col] = df_alldata[col].fillna(0)","fd59b888":"#For selected columns below fill null with mode\nmodecols=['MSZoning', 'Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType']\n\nfor col in modecols:\n    df_alldata[col] = df_alldata[col].fillna(df_alldata[col].mode()[0])","f179d113":"#Drop the Utilities column\ndf_alldata.drop('Utilities', inplace=True, axis=1)","66e72e04":"#Impute value 'Typ'\ndf_alldata['Functional'] = df_alldata.fillna('Typ')","ed722d5a":"#Impute lotfrontage null values with Neighbour hood median\ndf_alldata['LotFrontage'] = df_alldata.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","fd0d1073":"#Lookout for missing null values again\nnas = df_alldata.isnull().sum()\nnas = nas[nas>0]\nnas","19183f03":"#Add new column TotalSF\ndf_alldata['TotalSF'] = df_alldata['TotalBsmtSF'] + df_alldata['1stFlrSF'] + df_alldata['2ndFlrSF']","34bab268":"#Drop noise columns\ndropcols = ['TotalBsmtSF', '1stFlrSF', '2ndFlrSF']\ndf_alldata.drop(dropcols, inplace=True, axis=1)","d5537d9b":"#Convert these cols to numerical\ndf_alldata['Functional'] = df_alldata['Functional'].astype(float)","02ad19c6":"#Selecting numerical features\nnum_feats = df_alldata.select_dtypes(exclude='object').columns\nnum_feats","91fbf298":"catg_feats = df_alldata.dtypes[df_alldata.dtypes == 'object'].index\ncatg_feats","ff45c588":"#Before onehot encoding\ndf_beforeonehot = df_alldata.copy()","d10bebc3":"df_alldata.shape","ef53bd89":"df_alldata_copy = df_alldata.copy()","646b01d5":"#Actual onehot encoding avoiding dummy variable trap\nfor col in catg_feats:\n    df_temp = df_alldata[col]\n    df_temp = pd.DataFrame(df_temp)\n    df_temp = pd.get_dummies(df_temp, prefix = col)\n    temp = df_temp.columns[0] #Delete one dummy variable\n    df_temp.drop(temp, inplace=True, axis=1)\n    df_alldata = pd.concat([df_alldata, df_temp], axis=1).reset_index(drop=True)\n    df_alldata.drop(col, inplace=True, axis=1) #Delete actual column from dataframe","f36cd8e3":"df_alldata.shape","ddf05bb4":"skew_feats = df_alldata[num_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskew_feats = skew_feats[skew_feats>0.5]\nprint('No of Features to be Skewed: ', len(skew_feats))\nprint(skew_feats[:10])","b145a3f7":"plt.figure(figsize=(8,5))\nskew_feats.plot(kind='bar')\nplt.title('Skewed Features')\nplt.xlabel('Features')\nplt.ylabel('Skewed Value')\nplt.show()","d9b7dfeb":"#Perform Box Cox Transformation on selected features having skew value > 0.5\nLambda=0.15\nfor col in skew_feats.index:\n    df_alldata[col] = boxcox1p(df_alldata[col], Lambda)\n    \nprint('No of Features Skewed: ',skew_feats.shape[0])","333a1ac2":"df_train = df_alldata[:ntrain]\ndf_test = df_alldata[ntrain:]","d2b44544":"df_train.shape","752f528c":"y_train.shape","8ad9fa73":"#Defining cross validation strategy\ncross_val = KFold(n_splits=10, shuffle=True, random_state=42)","76427581":"#Define function to calculate rmse during training\ndef rmse_train(model, x, y):\n    rmse = np.sqrt(-cross_val_score(model, x, y, scoring='neg_mean_squared_error', cv=cross_val, n_jobs=-1))\n    return rmse.mean()","b5dd5b8a":"#Find rmse for prediction\ndef rmse_pred(y, y_pred):\n    rmse = np.sqrt(mean_squared_error(y, y_pred))\n    return rmse","01cf8fad":"def Submission(prediction):\n    df_pred = pd.DataFrame({'Id':srs_testid, 'SalePrice':prediction})\n    print('Sample Prediction:', prediction[:5])\n    \n    #Defining file name\n    tday = datetime.today()\n    tm = str(tday.date().day)+str(tday.date().month)+str('_')+str(tday.time().hour)+str(tday.time().minute)+str(tday.time().second)\n    fn = 'Submission_'\n    fn = str(fn)+str(tm)+str('.csv')\n    \n    #Saving prediction to csv\n    df_pred.to_csv(fn, index=False)\n    print('Submission file saved to', os.path.realpath(fn))","1f2792cf":"from sklearn.linear_model import LassoCV, RidgeCV, Lasso, Ridge, ElasticNet, ElasticNetCV\nfrom sklearn.model_selection import KFold, train_test_split, ShuffleSplit, cross_val_score\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport xgboost as xgb\nimport lightgbm as lgbm","a0caaa2e":"#Defining training inputs\nX_train = df_train.values\ny_train = y_train\n\n#Define test inputs\nX_test = df_test.values","d51644f2":"alphas=[0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 2, 10, 20, 50]","7d312e42":"#Building Lasso Model\nlcv = LassoCV(alphas=alphas, random_state=42, cv=cross_val, n_jobs=-1, max_iter=10000)","4a3d8bb6":"lcv.fit(X_train, y_train)","d93b034e":"#Optimum alpha value for lasso model\nlcv.alpha_","eebbcdb7":"regressor_lasso = Lasso(alpha=0.0001, random_state=42)\nregressor_lasso.fit(X_train,y_train)","3a3cc06d":"#Review RMSE values for Lasso\nprint('Training RMSE:',rmse_train(regressor_lasso, X_train, y_train))","a96e6692":"#Making prediction and review Test RMSE\nprint('Testing RMSE:',rmse_pred(y_train, regressor_lasso.predict(X_train)))","7d9729da":"#Scoring Lasso prediction\npred = regressor_lasso.predict(X_test)\npred = np.expm1(pred)\nSubmission(pred)","05e16c4d":"alphas2 = [10, 12, 16, 12.5, 17, 10.001]","96c390f6":"rcv = RidgeCV(alphas=alphas, scoring='neg_mean_squared_error', cv=cross_val)","e2817c8d":"rcv.fit(X_train, y_train)","502752e7":"rcv.alpha_","18d8a5b0":"regressor_ridge = Ridge(alpha=10, max_iter=10000, random_state=42)","ac6ca69f":"regressor_ridge.fit(X_train,y_train)","a56fea6c":"pred_ridge = regressor_ridge.predict(X_train)","8116bb5d":"#Review Train RMSE values\nprint('Training RMSE:',rmse_train(regressor_ridge, X_train, y_train))\n\n#Making prediction and review Test RMSE\nprint('Testing RMSE:',rmse_pred(y_train, pred_ridge))","8b0b6445":"#Make Test Prediction\npred_ridge = regressor_ridge.predict(X_test)\npred_ridge = np.expm1(pred_ridge)\nSubmission(pred_ridge)","2b2fdcfb":"#Test both lasso and ridge\npred = regressor_lasso.predict(X_test)*0.5 + regressor_ridge.predict(X_test)*0.5\npred = np.expm1(pred)\nSubmission(pred)","a91052ec":"### Building Function for Submission","cc6c57e6":"## <center> Predicting House Price","e664a9f7":"### Building Model","cb9417e7":"## Next Best Score - 0.11792","c4e8df04":"### Data Back Up","498b6675":"### Target Setup","6317748b":"### Building Model Validation Functions","5736ca56":"### Capture Total Number of rows in Train and Test","df2b5003":"## Secured score - 0.11976","51c3f385":"---","3a37a43e":"### Skew data to normalize feature values","a04de142":"### Building Ridge Model","e1e372a2":"### Building All Data","0ece68cb":"### Secured score - 0.11814 (not an improvement)","dcf4babb":"## Building Training and Test Set","04cc957b":"---","61390777":"### Handling Outliers","dda33148":"## Data Munging","2fe5185b":"## Handling Missing Values","225da468":"## Perform One hot Encoding"}}