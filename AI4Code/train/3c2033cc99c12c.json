{"cell_type":{"54c0bfb9":"code","145cd42a":"code","73cf2ae2":"code","ba4662d2":"code","606f62dc":"code","72f40697":"code","2e4793b9":"code","a5fd3634":"code","2cf864d8":"code","ded991ad":"code","f8d2f9e2":"code","71e5647d":"code","c5263e2b":"code","941ab54e":"code","058f108b":"code","12dace42":"code","a69fefac":"code","7ad24d19":"code","c40f138e":"code","ecbce9e4":"code","d5cc94ed":"code","c4dd1573":"code","3151191c":"code","61ac347a":"code","3976fe52":"code","e887c2f7":"code","79a0f817":"code","56c79199":"code","0325a2d9":"code","23be8e60":"code","7e21f27c":"code","659731f2":"code","6a72b093":"code","030b19f1":"code","f265457d":"code","70c12eb7":"code","6921ab5f":"code","373d6ba6":"code","a6d76a36":"code","558c8600":"code","c0d43820":"code","cffcbd61":"code","403f309b":"code","6118c22c":"code","935c8b2d":"code","5534dc10":"code","6c99574b":"code","5bd47f0d":"code","63618e9f":"code","d7225b94":"code","5be66628":"code","caf1b4d4":"code","4cd6de93":"code","0a22fc26":"code","5262aee2":"code","88d92834":"code","42462f61":"code","3721216a":"code","75377cd1":"code","0db60af0":"markdown","e2bc5c28":"markdown","c7e8fd22":"markdown","f154cf82":"markdown","48ce8160":"markdown","e6487c8d":"markdown","e3e05b10":"markdown","1943c3bb":"markdown","52e8aa4e":"markdown","d0510cb9":"markdown","3fbf0ebe":"markdown","ec4831eb":"markdown","3b900e2d":"markdown","060e80bd":"markdown","115a861f":"markdown","6ecdaa86":"markdown","74ced391":"markdown","c3268804":"markdown","d1a5749f":"markdown","56d03052":"markdown","de13147a":"markdown","f366536f":"markdown","d560684c":"markdown","dc47414a":"markdown","09d5d984":"markdown","22364dba":"markdown","d88a5651":"markdown","0d3d59a0":"markdown","c0dacf20":"markdown","f11d66fd":"markdown","97c827f3":"markdown","232fc424":"markdown","e8bb090e":"markdown","3ed6370f":"markdown","9509235c":"markdown","93ba0724":"markdown","19bc357d":"markdown","7a3596e4":"markdown","197f215d":"markdown","8ea30306":"markdown","623ed1bd":"markdown","76f526e7":"markdown","9784a22e":"markdown","57866ab2":"markdown","3074c912":"markdown","09636595":"markdown","537f7c9a":"markdown","698a06c7":"markdown","ae7ee398":"markdown","26de6179":"markdown","76dfa044":"markdown","f8fcb0db":"markdown","ad5afa9a":"markdown","882cbbbd":"markdown","cd019baf":"markdown","f19a0e17":"markdown","a3ecdcb6":"markdown","1b073c8d":"markdown","b6a2100a":"markdown","9d9a629c":"markdown","ceb090e0":"markdown","5aca3092":"markdown","375f2dae":"markdown","2f509b2c":"markdown","8158e0a3":"markdown","7f508263":"markdown","730e874a":"markdown","744b84e4":"markdown","a5f88465":"markdown","2074309b":"markdown","75aeb716":"markdown","13abb390":"markdown","bee30b3a":"markdown","05cbb20d":"markdown","21674d22":"markdown","c11d5874":"markdown","3b83f39c":"markdown","78643021":"markdown"},"source":{"54c0bfb9":"#basic scientific libraries \nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\n#classification libraries \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\n\n#dimension reduction packages \nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib.patches as mpatches\nimport time\n\n# Other Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import precision_recall_curve\n\n\n#animation and dynamic visualization \nimport plotly\nimport plotly.graph_objects as go\nimport plotly.express as px              #the ploty function enable us to realize some interacting function in the visualization \nfrom plotly.offline import iplot,init_notebook_mode\nimport cufflinks as cf                   #the visualizzation tool \nfrom scipy import stats                  #the package can be used to apply some basic regression method into application\n\n#Deep learning part\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch import nn,optim\nfrom torch.nn import functional as F \nfrom torch.utils.data import Dataset,DataLoader,TensorDataset\nfrom sklearn.preprocessing import StandardScaler\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\nimport plotly.offline as py\nimport plotly.graph_objs as go\n#plotly.offline.iplot()\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=True, world_readable=True)","145cd42a":"data = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndisplay(data.info())\ndisplay(data.describe())","73cf2ae2":"data.head()","ba4662d2":"print(data.isnull().sum())","606f62dc":"data.dropna(axis = 0)[data[\"Class\"]==1].describe()","72f40697":"data = data.dropna(axis = 0)\ndata = data.drop_duplicates()","2e4793b9":"Fraud = data[data[\"Class\"] == 1]\nNormal = data[data[\"Class\"] == 0]\nprint(Fraud.shape)\nprint(Normal.shape)","a5fd3634":"plt.figure(figsize = (20,12))\nj=1;\nfor i in range(1,13):\n    plt.subplot(4,3,j)\n    sns.distplot(Fraud[\"V\"+str(i)],hist = False,color = 'red',label = \"Fraud\")\n    sns.distplot(Normal[\"V\"+str(i)],hist = False, color = 'blue',label = \"Normal\")\n    plt.legend(fontsize = \"medium\",loc = \"best\")\n    j = j+1    ","2cf864d8":"import copy \ndata = data.drop([\"Amount\",\"Time\"],axis = 1)\ndf = copy.deepcopy(data)\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range = (-1,1))\nfor i in range(1,29):\n    df[\"V\"+str(i)] = scaler.fit_transform(df[\"V\"+str(i)].values.reshape(-1,1))","ded991ad":"f, ax = plt.subplots(figsize=(16,6))\nplt.style.use('ggplot') # Using ggplot2 style visuals \nax.set_facecolor('#fafafa')\nax = sns.boxplot(data = df, palette = 'Set3',whis = 2.5)\nplt.title(\"The box distribution of V1 - V28 after min-max normalization\")\nplt.show()","f8d2f9e2":"#create an outtlier-detection function \nimport copy\ncollist = [\"V\"+ str(i) for i in range(1,29)]\ndf2 = df\ndf2fraud = df2[df2[\"Class\"] == 1]\ndf2normal = df2[df2[\"Class\"] == 0]\nfor column in collist:\n    Q1 = np.percentile(df2normal[column],25)    # calculate the 25 percentile and 75 percentile \n    Q3 = np.percentile(df2normal[column],75)\n    IQR = Q3-Q1                          # calculate the interval \n    threshold = IQR * 2.5                # set the threshold of 2.5 times of IQR\n    lower, upper = Q1 - threshold, Q3 + threshold                  \n    df2normal= df2normal[(df2normal[column]>lower) & (df2normal[column]<upper)]\ndf2 = pd.concat([df2normal,df2fraud])\n\nfrom sklearn.utils import shuffle\ndf2 = shuffle(df2)                        #shuffle the dataset ","71e5647d":"df3 = data\ndf3fraud = df3[df3[\"Class\"] == 1]\ndf3normal = df3[df3[\"Class\"] == 0]\nfor column in collist:\n    Q1 = np.percentile(df3normal[column],25)    # calculate the 25 percentile and 75 percentile \n    Q3 = np.percentile(df3normal[column],75)\n    IQR = Q3-Q1                          # calculate the interval \n    threshold = IQR * 2.5                # set the threshold of 2.5 times of IQR\n    lower, upper = Q1 - threshold, Q3 + threshold                  \n    df3normal= df3normal[(df3normal[column]>lower) & (df3normal[column]<upper)]\ndf3 = pd.concat([df3normal,df3fraud])\n\nfrom sklearn.utils import shuffle\ndf3 = shuffle(df3) ","c5263e2b":"#plt.figure(figsize = (16,4))\n\n#plt.subplot(1,2,1)\nf, ax = plt.subplots(figsize=(16,5))\nplt.style.use('ggplot') # Using ggplot2 style visuals \nax.set_facecolor('#fafafa')\nax = sns.boxplot(data = df2normal, palette = 'Set3',whis = 2.5)\nplt.title(\"Normal Class after outlier detection\")\nplt.show()\n\n#plt.subplot(1,2,2)\nf, ax = plt.subplots(figsize=(16,5))\nplt.style.use('ggplot') # Using ggplot2 style visuals \nax.set_facecolor('#fafafa')\nax = sns.boxplot(data = df2fraud, palette = 'Set3',whis = 2.5)\nplt.title(\"Fraud Class after outlier detection\")\nplt.show()","941ab54e":"Fraud = df3[df3[\"Class\"] == 1]\nNormal = df3[df3[\"Class\"] == 0]\nprint(Fraud.shape)\nprint(Normal.shape)","058f108b":"plt.figure(figsize = (12,3),dpi = 100)\n\nplt.subplot(1,2,1)\nsns.countplot('Class',data = df3, palette = \"Set2\")\nplt.title('Class Distributions \\n (0: Normal || 1: Fraud)', fontsize=14)\n\nplt.subplot(1,2,2)\ncountdata = [df3[df3[\"Class\"] == 0][\"Class\"].count(),df3[df3[\"Class\"] == 1][\"Class\"].count()]\nlabelsdata = [\"Normal\",\"Fraud\"]\ncolors = sns.color_palette('pastel')[0:2]\nplt.pie(countdata, labels = labelsdata, colors = colors,autopct='%.2f%%')\nplt.title(\"The Ratio between two classes\")\nplt.show()","12dace42":"Normal = df3[df3[\"Class\"] == 0]\nFraud = df3[df3[\"Class\"] == 1]\n\nRUSNormal = Normal.iloc[:df3[df3[\"Class\"] == 1][\"Class\"].count()]\nRUSFraud = Fraud\nRUSdata = pd.concat([RUSNormal,RUSFraud])\nRUSdata = shuffle(RUSdata)\nscaler = MinMaxScaler(feature_range = (-1,1))\nfor i in range(1,29):\n    RUSdata[\"V\"+str(i)] = scaler.fit_transform(RUSdata[\"V\"+str(i)].values.reshape(-1,1))\nRUSdata.head()","a69fefac":"plt.figure(figsize = (12,3),dpi = 100)\n\nplt.subplot(1,2,1)\nsns.countplot('Class',data = RUSdata, palette = \"Set2\")\nplt.title('Class Distributions after RUS \\n (0: Normal || 1: Fraud)', fontsize=14)\n\nplt.subplot(1,2,2)\ncountdata = [RUSdata[RUSdata[\"Class\"] == 0][\"Class\"].count(),RUSdata[RUSdata[\"Class\"] == 1][\"Class\"].count()]\nlabelsdata = [\"Normal\",\"Fraud\"]\ncolors = sns.color_palette('pastel')[0:2]\nplt.pie(countdata, labels = labelsdata, colors = colors,autopct='%.2f%%')\nplt.title(\"The Ratio between two classes after RUS\")\nplt.show()","7ad24d19":"# choose a few columns to see the distribution\nCopydata = RUSdata[collist]\nlayout = go.Layout(\n    autosize=False,\n    width=800,\n    height=750,\n    xaxis= go.layout.XAxis(linewidth = 1),\n    yaxis= go.layout.YAxis(linewidth = 1),\n#margin=go.layout.Margin(l=50, r=50,b=100,t=100,pad = 4)\n)\nCopydata.iplot(kind='hist',              # choose the kind of histogram     \n           subplots=True,               # plot a few subplots \n           shape = (7,4),\n           horizontal_spacing=.03,       # set the horizontal space \n           fill=True,\n           layout = layout,\n           shared_yaxes = True,\n           subplot_titles=True,        # set the subtitle \n           title='Data Distribution')\n# using the Cufflinks package to plot a dynamic chart to visualize the distribution of some variables","c40f138e":"plt.figure(figsize = (18,8),dpi = 60)   # set the canvas \ncorr = RUSdata.corr()                   # instantiate the correlation matrix\nmask =  np.zeros_like(corr, dtype = np.bool)          # set the mask of the heap map \nmask[np.triu_indices_from(mask)] = True \nsns.set_style(\"whitegrid\")                            # set the seaborn style \nplt.title(\"The heat map of the data\")\nplt.subplot(1,2,1)\nsns.heatmap((RUSdata.loc[RUSdata['Class'] ==1]).corr(), vmax = .8, cmap = \"RdBu_r\" , mask=mask);  # the fraud part \nplt.subplot(1,2,2)\nsns.heatmap((RUSdata.loc[RUSdata['Class'] ==0]).corr(), vmax = .8,  cmap = \"GnBu\", mask=mask);    # the noraml part \nplt.suptitle(\"The heat map of the data\",fontsize = 20)\nplt.show()","ecbce9e4":"plt.figure(figsize = (10,4),dpi = 100)\nc = pd.DataFrame(RUSdata.corr().sort_values('Class',ascending=True)['Class'])\nsns.barplot(x = c.index, y = c.values.flatten(), palette = \"Blues\")\nplt.title(\"The correlation between features and class\")\nplt.xlabel(\"Features\")\nplt.ylabel(\"Correlation\")\nplt.show()","d5cc94ed":"plt.figure(figsize = (20,30))\nfor i in range(1,29):\n    sns.set_style(\"ticks\")  \n    plt.subplot(7,4,i)                  # create the subplot canvas \n                       \n    data_select = \"V\" + str(i)             \n    sns.kdeplot(x = data_select,data = RUSdata , hue = \"Class\",shade= True,alpha = 0.3 )  # plot the KDE plot in seaborn \n    plt.suptitle('The data distribution of different features',y = 0.9,fontsize = 20)","c4dd1573":"def NormalMean(a):\n    feature = 'V'+str(a)\n    return RUSdata[RUSdata[\"Class\"] == 0].loc[:,feature].mean()\ndef FraudMean(a):\n    feature = 'V'+str(a)\n    return RUSdata[RUSdata[\"Class\"] == 1].loc[:,feature].mean()\ndef NormalStd(a):\n    feature = 'V'+str(a)\n    return RUSdata[RUSdata[\"Class\"] == 0].loc[:,feature].std()\ndef FraudStd(a):\n    feature = 'V'+str(a)\n    return RUSdata[RUSdata[\"Class\"] == 1].loc[:,feature].std()","3151191c":"import plotly.graph_objects as go\nimport plotly.figure_factory as ff\ntable_data = [['Features', 'Normal Mean', 'Fraud Mean','Normal Std','Fraud Std'],\n              ['V10', NormalMean(10), FraudMean(10),NormalStd(10),FraudStd(10)],\n              ['V11', NormalMean(11), FraudMean(11),NormalStd(11),FraudStd(11)],\n              ['V16', NormalMean(16), FraudMean(16),NormalStd(16),FraudStd(16)],\n              ['V17', NormalMean(17), FraudMean(17),NormalStd(17),FraudStd(17)],\n              ['V20', NormalMean(20), FraudMean(20),NormalStd(20),FraudStd(20)],\n              ['V23', NormalMean(23), FraudMean(23),NormalStd(23),FraudStd(23)],\n              ['V28', NormalMean(28), FraudMean(28),NormalStd(28),FraudStd(28)]]\nfig = ff.create_table(table_data, height_constant=20)\nteams = [\"V10\",\"V11\",\"V16\",\"V17\",\"V20\",\"V23\",\"V28\"]\n\n# create four different bars \nnm = [NormalMean(10),NormalMean(11),NormalMean(16),NormalMean(17),NormalMean(20),NormalMean(23),NormalMean(28)]\nfm = [FraudMean(10),FraudMean(11),FraudMean(16),FraudMean(17),FraudMean(20),FraudMean(23),FraudMean(28)]\nns = [NormalStd(10),NormalStd(11),NormalStd(16),NormalStd(17),NormalStd(20),NormalStd(23),NormalStd(28)]\nfs = [FraudStd(10),FraudStd(11),FraudStd(16),FraudStd(17),FraudStd(20),FraudStd(23), FraudStd(28)]\n\n# Make traces for graph\ntrace1 = go.Bar(x=teams, y=nm, xaxis='x2', yaxis='y2',\n                marker=dict(color='#0099ff'),\n                name='NormalMean')\ntrace2 = go.Bar(x=teams, y=fm, xaxis='x2', yaxis='y2',\n                marker=dict(color='#404040'),\n                name='FraudMean')\ntrace3 = go.Bar(x=teams, y=ns, xaxis='x2', yaxis='y2',\n                marker=dict(color='red'),\n                name='NormalStd')\ntrace4 = go.Bar(x=teams, y=fs, xaxis='x2', yaxis='y2',\n                marker=dict(color='green'),\n                name='FraudStd')\n\n# Add trace data to figure\nfig.add_traces([trace1, trace2,trace3,trace4])\n\n# initialize xaxis2 and yaxis2\nfig['layout']['xaxis2'] = {}\nfig['layout']['yaxis2'] = {}\n\n# Edit layout for subplots\nfig.layout.yaxis.update({'domain': [0, .55]})\nfig.layout.yaxis2.update({'domain': [.6, 1]})\n\n# The graph's yaxis2 MUST BE anchored to the graph's xaxis2 and vice versa\nfig.layout.yaxis2.update({'anchor': 'x2'})\nfig.layout.xaxis2.update({'anchor': 'y2'})\nfig.layout.yaxis2.update({'title': 'Comparisons'})\n\n# Update the margins to add a title and see graph x-labels.\nfig.layout.margin.update({'t':50, 'l':50})\nfig.layout.update({'title': 'The Comparison of statistical values'})\n\n# Update the height because adding a graph vertically will interact with the plot height calculated for the table\nfig.layout.update({'height':400})\nfig.show()","61ac347a":"DATA = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\nDATA['hour'] = DATA['Time'].apply(lambda x: np.ceil(float(x)\/3600) % 24)\ntimedata = pd.concat([shuffle(DATA[DATA[\"Class\"] == 0 ]).iloc[:500],  DATA[DATA[\"Class\"] == 1]])\ntimedata.head()","3976fe52":"sns.set_style(\"white\")\nbins = np.arange(timedata['hour'].min(),timedata['hour'].max()+2)\nplt.figure(figsize=(15,4))\n# plot a distribution plot according to the hour\nsns.distplot(timedata[timedata['Class']==0.0]['hour'],bins=bins,kde=True,hist_kws={'alpha':.5}, label='Normal')\nsns.distplot(timedata[timedata['Class']==1.0]['hour'],bins=bins,kde=True,label='Fraud',hist_kws={'alpha':.5})\n\nplt.xticks(range(0,24))\nplt.legend()\nplt.title(\"The distribution of amount according to hour\")\nplt.xlabel(\"Hour\")\nplt.ylabel(\"Density\")\nplt.show()","e887c2f7":"from sklearn.decomposition import PCA\nfeatures = [\"V\"+ str(i) for i in range(1,29)]   # create of list of V1-V28\ntstart = time.time()\nXpca = PCA(2).fit_transform(RUSdata[features].values)  # instanciate the pca \nYpca = RUSdata[\"Class\"]\ntend = time.time()\nPCAtime = tend - tstart   # record the running time of PCA ","79a0f817":"# draw a scatter plot of the PCA method\nplt.figure(figsize = (8,6),dpi = 60)\nplt.scatter(Xpca[:,0], Xpca[:,1], c = (Ypca==0),cmap='coolwarm', label='No Fraud', linewidths=2)\nplt.scatter(Xpca[:,0], Xpca[:,1], c = (Ypca==1),cmap='coolwarm', label='Fraud', linewidths=2)\nplt.legend()\nplt.title('PCA', fontsize=14)\nplt.show()","56c79199":"import plotly.express as px\nfrom sklearn.decomposition import PCA\n# utilize the plotly package to create the dynamic canvas \npca = PCA(4)  \nRUSdata = shuffle(RUSdata)\nhhh = copy.deepcopy(RUSdata)\n\ncomponents = pca.fit_transform(RUSdata[features])\nlabels = {\n    str(i): f\"PC {i+1} ({var:.0f}%)\"\n    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n}\nfig = px.scatter_matrix(\n    components,\n    labels=labels,\n    dimensions=range(4),\n    color=(hhh[\"Class\"]).astype(bool),\n    color_discrete_sequence = [\"blue\",\"red\"],\n    title = \"Different combinations of PCA\"\n)\nfig.update_traces(diagonal_visible=False)\nfig.update_layout(title_x = 0.5)\nfig.show()","0325a2d9":"from sklearn.decomposition import TruncatedSVD\ntstart = time.time()\n# instanciate the svd \nXsvd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(RUSdata[features].values)\nYsvd = RUSdata[\"Class\"]\ntend = time.time()\nSVDtime = tend - tstart   # record the running time of SVD ","23be8e60":"# draw a scatter plot of the SVD \nplt.figure(figsize = (8,6),dpi = 60)\nplt.scatter(Xsvd[:,0], Xsvd[:,1], c = (Ysvd==0),cmap='coolwarm', label='No Fraud', linewidths=2)\nplt.scatter(Xsvd[:,0], Xsvd[:,1], c = (Ysvd==1),cmap='coolwarm', label='Fraud', linewidths=2)\nplt.legend()\nplt.title('Truncated SVD', fontsize=14)\nplt.show()","7e21f27c":"from sklearn.manifold import TSNE # import the manifold learning package from the Sklearn \nsns.set_style(\"whitegrid\")\nfig, axes = plt.subplots(1, 2,figsize = (16,4))\ntstart = time.time()\ntsne = TSNE(n_components=2) \nX_tsne = tsne.fit_transform(RUSdata[features]) \ny = RUSdata[\"Class\"].values\ntend = time.time()\nTSNEtime = tend - tstart\nX_tsne_data = np.vstack((X_tsne.T, y)).T \ndf_tsne = pd.DataFrame(X_tsne_data, columns=['Dim1', 'Dim2','Class']) \nsns.scatterplot(data=df_tsne, hue='Class', x='Dim1', y='Dim2',ax = axes[0]) \n#plt.scatter(df_tsne[\"Dim1\"],df_tsne[\"Dim2\"],c = df_tsne[\"Class\"])\naxes[0].set_title(\"0.0:Normal 1.0:Fraud\")\n\n\ntsne = TSNE(n_components=2) \nX_tsne = tsne.fit_transform(RUSdata[features]) \ny = RUSdata[\"Class\"].values\nX_tsne_data = np.vstack((X_tsne.T, y)).T \ndf_tsne = pd.DataFrame(X_tsne_data, columns=['Dim1', 'Dim2','Class']) \nsns.scatterplot(data=df_tsne, hue='Class', x='Dim1', y='Dim2',ax = axes[1]) \n#plt.scatter(df_tsne[\"Dim1\"],df_tsne[\"Dim2\"],c = df_tsne[\"Class\"])\naxes[1].set_title(\"0.0:Normal 1.0:Fraud\")\n\nplt.suptitle(\"The distibution of two classes after T-SNE\")\nplt.show()","659731f2":"tsne = TSNE(n_components=3)   # instanciate the TSNE dimenesion reduction method\nX_tsne = tsne.fit_transform(RUSdata[features])      \ny = RUSdata[\"Class\"].values   # transform the dataframe sturcture into the numpy array \nX_tsne_data = np.vstack((X_tsne.T, y)).T  \ndf_tsne = pd.DataFrame(X_tsne_data, columns=['Dim1', 'Dim2','Dim3','Class']) \ndf_tsne[\"Class\"] = df_tsne[\"Class\"].astype(bool)\nimport plotly.express as px\nfig = px.scatter_3d(df_tsne, x='Dim1', y='Dim2', z='Dim3',color='Class',color_discrete_sequence = [\"yellow\",\"blue\"], \n                    title = \"The visualization of T-SNE methods in 3D dynamic plot\")\nfig.update_layout(width = 500, height = 500)\nfig.show()","6a72b093":"print(\"PCA method used a total of {:.2}s\".format(PCAtime))\nprint(\"SVD method used a total of {:.2}s\".format(SVDtime))\nprint(\"T-SNE method used a total of {:.2}s\".format(TSNEtime))","030b19f1":"normal2 = shuffle(df3[df3[\"Class\"] == 0]).iloc[:427,]\nfraud2 = shuffle(df3[df3[\"Class\"] == 1])\nRUSdata2 = pd.concat([normal2, fraud2])\nRUSdata2 = shuffle(RUSdata2)\nRUSdata2.head()","f265457d":"test = copy.deepcopy(RUSdata)\ntest.head()\n\n# Set the train and test data\nX = RUSdata2.drop(\"Class\",axis = 1)\nY = RUSdata2[\"Class\"]\nXTEST = test.drop(\"Class\",axis = 1)\nYTEST = test[\"Class\"]\n\nX_train = X.values           # get the numpy array from the dataframe \nY_train = Y.values         \nX_test = XTEST.values \nY_test = YTEST.values","70c12eb7":"from sklearn.linear_model import LogisticRegression \nfrom sklearn.metrics import accuracy_score\nlr = LogisticRegression()         # instanciate a logistic regression model named lr\nlr.fit(X_train,Y_train)           # fit the model \npred = lr.predict(X_test)              ","6921ab5f":"from sklearn.metrics import classification_report\nprint(classification_report(Y_test,pred))    # get a brief report of the prediction performance ","373d6ba6":"y_pred_logit_proba = lr.predict_proba(X_test)[::,1]\nfpr_logit, tpr_logit, _ = metrics.roc_curve(Y_test,  y_pred_logit_proba)\nauc_logit = metrics.roc_auc_score(Y_test, y_pred_logit_proba)\nplt.figure(figsize = (12,4))\n\nplt.subplot(1,2,1)\nplt.plot(fpr_logit,tpr_logit,label=\"Logistic Regression, auc={:.3f})\".format(auc_logit))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('Logistic Regression ROC curve')\nplt.legend()\n\nplt.subplot(1,2,2)\nlogit_precision, logit_recall, _ = precision_recall_curve(Y_test, y_pred_logit_proba)\nno_skill = len(Y_test[Y_test==1]) \/ len(Y_test)\nplt.plot([0, 1], [no_skill, no_skill], linestyle='--', color='black', label='No Skill')\nplt.plot(logit_recall, logit_precision, color='orange', label='Logistic')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall curve')\nplt.legend()\nplt.suptitle(\"The ROC and PR curve of Logistic Regression model\",fontsize = 20,y = 1.02)\nplt.show()","a6d76a36":"visualdata = pd.DataFrame(Xpca.copy())\nvisualdata[\"Class\"] = Ypca.values \nvisualdata.columns = [\"PC1\",\"PC2\",\"Class\"]\nvisualdata.head()","558c8600":"lr2 = LogisticRegression()\nvisualx = visualdata.drop(\"Class\",axis = 1)\nvisualy = visualdata[\"Class\"]\nlr2.fit(visualx, visualy)\ny_hat = lr2.predict(visualx)","c0d43820":"def x2(x1):\n    return (-lr2.coef_[0][0] * x1 - lr2.intercept_[0]) \/ lr2.coef_[0][1]\n\nplt.figure(figsize = (10,6))\nsns.set_style('ticks')\nx = np.linspace(-1,0.5,50)\ny = x2(x)\nvisualdata = visualdata.iloc[:400,]\nmarkers = {1: \"s\", 0: \"X\"}\nsns.scatterplot(x=\"PC1\",y = \"PC2\",hue = \"Class\",style = \"Class\",markers = markers,data = visualdata, palette = \"Set1\")\nsns.lineplot(x = x,y = y,color ='g')\nplt.title(\"The visualization of the logistic regression PC1&PC2\")\nplt.legend(fontsize = 14)\nplt.show()","cffcbd61":"from sklearn.model_selection import cross_val_score\n#import the package from the sklearn to evaluate the corss validation score\ntraining_score = cross_val_score(lr,X_train,Y_train, cv=5)\nprint(\"The prediction accuracy of the Linear Regression model after cross validation is {:.2f}%\".format(100*training_score.mean()))","403f309b":"from sklearn import svm\nclassifier = svm.SVC(kernel='linear',probability= True)          #instantiate the SVM classifier \nclassifier.fit(X_train,Y_train)                #fit the model \npred = classifier.predict(X_test)                ","6118c22c":"from sklearn.metrics import classification_report\nprint(classification_report(Y_test,pred))    # get a brief report of the prediction performance ","935c8b2d":"y_pred_svm_proba = classifier.predict_proba(X_test)[::,1]\nfpr_svm, tpr_svm, _ = metrics.roc_curve(Y_test,  y_pred_svm_proba)\nauc_svm = metrics.roc_auc_score(Y_test, y_pred_svm_proba)\nplt.figure(figsize = (12,4))\n\nplt.subplot(1,2,1)\nplt.plot(fpr_svm,tpr_svm,label=\"SVM, auc={:.3f})\".format(auc_svm))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('SVM ROC curve')\nplt.legend()\n\nplt.subplot(1,2,2)\nsvm_precision, svm_recall, _ = precision_recall_curve(Y_test, y_pred_svm_proba)\nno_skill = len(Y_test[Y_test==1]) \/ len(Y_test)\nplt.plot([0, 1], [no_skill, no_skill], linestyle='--', color='black', label='No Skill')\nplt.plot(svm_recall, svm_precision, color='orange', label='SVM')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall curve')\nplt.legend()\nplt.suptitle(\"The ROC and PR curve of SVM\",fontsize = 20,y = 1.02)\nplt.show()","5534dc10":"from sklearn.model_selection import cross_val_score\ntraining_score = cross_val_score(classifier,X_train,Y_train, cv=5)   # calculate the crossvalidation score\nprint(\"The prediction accuracy of the SVM model after cross validation is {:.2f}%\".format(100*training_score.mean()))","6c99574b":"Xpca = PCA(n_components=2).fit_transform(RUSdata[features].values)  # instanciate the pca \nYpca = RUSdata[\"Class\"]\nvisualdata = pd.DataFrame(Xpca.copy())\nvisualdata[\"Class\"] = Ypca.values \nvisualdata.columns = [\"PC1\",\"PC2\",\"Class\"]\nvisualdata= shuffle(visualdata)\nvisualdata.head()","5bd47f0d":"def make_meshgrid(x, y, h=.02):\n    x_min, x_max = x.min() - 1, x.max() + 1\n    y_min, y_max = y.min() - 1, y.max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n    return xx, yy\ndef plot_contours(ax, clf, xx, yy, **params):\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    out = ax.contour(xx, yy, Z, **params)\n    return out\nX = visualdata.drop(\"Class\",axis=1).values\ny = visualdata[\"Class\"].values\nC = 1.5\nmodels = (svm.SVC(kernel='linear', C=C),svm.LinearSVC(C=C),svm.SVC(kernel='rbf', gamma=0.7, C=C),svm.SVC(kernel='poly', degree=3, C=C))\nmodels = (clf.fit(X, y) for clf in models)\ntitles = ('SVC(Linear Kernal)','LinearSVC (Linear Kernal)','SVC(RBF)','SVC(Polynomial)')\nfig, sub = plt.subplots(2, 2,figsize=(12,8))\nplt.subplots_adjust(wspace=0.2, hspace=0.2)\nX0, X1 = X[:, 0], X[:, 1]\nxx, yy = make_meshgrid(X0, X1)\nfor clf, title, ax in zip(models, titles, sub.flatten()):\n    plot_contours(ax, clf, xx, yy,alpha=1)\n    ax.scatter(X0, X1, c=y, cmap = \"rainbow\", s=20, edgecolors='k',alpha = 0.8)\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xlabel('PC1')\n    ax.set_ylabel('PC2')\n    ax.set_xticks(()) \n    ax.set_yticks(()) \n    ax.set_title(title)","63618e9f":"from sklearn.metrics import confusion_matrix\nlr = LogisticRegression()         # instanciate a logistic regression model named lr\nlr.fit(X_train,Y_train)  \ny_pred_log = lr.predict(X_test)\ny_pred_svc = classifier.predict(X_test)\nlog_reg_cf = confusion_matrix(Y_test, y_pred_log)\nsvc_cf = confusion_matrix(Y_test, y_pred_svc)       # instantiate the confusion matrix \n\nfig, (ax1,ax2) = plt.subplots(1, 2,figsize=(22,6))\nsns.heatmap(log_reg_cf, ax=ax1, annot=True, cmap = \"BuGn\")            # visualize the confusion matrix vai the heap map \nax1.set_title(\"Logistic Regression \\n Confusion Matrix\", fontsize=14)\n\nsns.heatmap(svc_cf, ax=ax2, annot=True)                # visualize the confusion matrix vai the heap map \nax2.set_title(\"Suppor Vector Classifier \\n Confusion Matrix\", fontsize=14)\n\nplt.show()","d7225b94":"from sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator1,estimator2, X, y, ylim=None, cv=None, n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \n    f,  (ax3, ax1) = plt.subplots(1,2, figsize=(20,7), sharey=True)\n    \n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax3.fill_between(train_sizes, train_scores_mean - train_scores_std,train_scores_mean + train_scores_std, alpha=0.1)\n    ax3.fill_between(train_sizes, test_scores_mean - test_scores_std,test_scores_mean + test_scores_std, alpha=0.1)\n    ax3.plot(train_sizes, train_scores_mean, 'o-',label=\"Training score\")\n    ax3.plot(train_sizes, test_scores_mean, 'o-',label=\"Cross-validation score\")\n    ax3.set_title(\"Support Vector Classifier Learning Curve\", fontsize=14)\n    ax3.set_xlabel('Training size (m)')\n    ax3.set_ylabel('Score')\n    ax3.grid(True)\n    ax3.legend(loc=\"best\")\n    \n    train_sizes, train_scores, test_scores = learning_curve(estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,train_scores_mean + train_scores_std, alpha=0.1)\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,test_scores_mean + test_scores_std, alpha=0.1)\n    ax1.plot(train_sizes, train_scores_mean, 'o-', label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-',label=\"Cross-validation score\")\n    ax1.set_title(\"Logistic Regression Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\") ","5be66628":"plot_learning_curve(classifier, lr, X_train, Y_train, (0.87, 1.01), n_jobs=4)","caf1b4d4":"import torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch import nn,optim\nfrom torch.nn import functional as F \nfrom torch.utils.data import Dataset,DataLoader,TensorDataset\nfrom sklearn.preprocessing import StandardScaler\nimport torch.utils.data as data_utils","4cd6de93":"class DuJunyeNet(nn.Module):\n    def __init__(self):\n        super(DuJunyeNet,self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(28,18),           # hidden layer \n            nn.Sigmoid(),                  # activation function of ReLU\n            nn.Linear(18,8),            # hidden layer \n            nn.Sigmoid(),                  # activation funvtion of ReLU\n            nn.Linear(8,1),             # hidden layer \n            nn.Sigmoid(),               # sigmoid function \n        )\n    def forward(self, x):\n        x = self.model(x)\n        return x ","0a22fc26":"model = DuJunyeNet()                    # instantiate the network into the model          \ncriteon= nn.BCELoss()                   # BCE loss function could play a better performance when combined with the sigmoid function\noptimizer = optim.Adam(model.parameters(),lr = 0.001)    # set the hyper parameter manually ","5262aee2":"print(model)","88d92834":"device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nif(device):\n    print(torch.cuda.get_device_name())          # check whether the GPU device could be used \n#model=model.to(device)                           # transfer the model into GPU device if possible\nprint(device)","42462f61":"# Adjust the data sructure of numpy array into tensor\ntrain_x = torch.from_numpy(RUSdata2.drop(\"Class\",axis =1).values).float()\ntrain_y = torch.from_numpy(RUSdata2[\"Class\"].values).float()\n\ntrain_dataset=TensorDataset(train_x,train_y)  \n# create a datset set via the tool of tensor dataset loader \ntrain_loader = torch.utils.data.DataLoader(dataset = train_dataset,batch_size = 32, shuffle = True )","3721216a":"def plot_curve(data):\n    fig= plt.figure()\n    plt.plot(range(len(data)),data,color= \"blue\")\n    plt.legend([\"value\"],loc= \"upper right\")\n    plt.xlabel(\"Step\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"The loss value according to the steps\")\n    plt.show()","75377cd1":"correct = 0\ntotal = 0\nfor data in train_loader:\n    inputs, labels = data\n    outputs = model(inputs).detach().numpy()\n    #print(outputs)\n    labels = labels.detach().numpy().astype(int)\n    outputs = (outputs>0.5).astype(int).flatten()\n    total += labels.size\n    correct += (outputs == labels).sum().item()\n    \nprint('Accuracy of the network on the inputs: {:.2f}%'.format( 100 * correct\/total))","0db60af0":"### Comparison between two machine learning method ","e2bc5c28":"**Brief Intro:** *In the first part of the project, I will conduct the process of data exploration and data cleaning, which could removes major errors and inconsistencies that are inevitable when multiple sources of data are getting pulled into one dataset. The process pave the way for the prediction and anlysis.*","c7e8fd22":"**Findings:** *From the above visualization, we could find that under the situation of logistic regression, if we only consider the PC1 & PC2 columns, the algorithm is still efective but may not have better performance.*","f154cf82":"*After the data preprocessing and cleaning part, the data distribution of the dataset become relatively smooth, so in this data visualization part, i will aggregate the data and visualize the distribution.*","48ce8160":"*From the heap map, we could find that the color of the normal part of the dataset seems much smooth while the fraud dataset seems much chaotic, especially between the column of V16-V18, as such we can refer that the column between V10-V12 may have a greater importance in the detection of credit fraud, which in means in PCA, those columns may contribute to a larger variance.*","e6487c8d":"*As is shown in the above, after cleaning the outliers bigger than Q3+2.5IQR and lower than Q1-2.5IQR would be removed from the dataset.*","e3e05b10":"### Classify two classes into two catogories and see their distribution \n","1943c3bb":"+ A system with high recall but low precision returns many results, but most of its predicted labels are incorrect when compared to the training labels. A system with high precision but low recall is just the opposite, returning very few results, but most of its predicted labels are correct when compared to the training labels. An ideal system with high precision and high recall will return many results, with all results labeled correctly.","52e8aa4e":" **Findings:** *From the above experiment, we could find that the accuracy on the testing set is a bit lower than the validation set, I think that the main reason lies on that the data distribution of the validation set is more similar to the training set, which may cause the occurence of overfitting, the solutions contains methods like data augmentation, shuffling and regularization.*","d0510cb9":"+ As the task is only a classification problem, so i choos to directly use the linear neural network together with the RELU function, in the last layer, i use a signoid function to map the result into a probabilty.","3fbf0ebe":"*Design a neural network structure*","ec4831eb":"#### Model Establishment ","3b900e2d":"**Note:** *In this part i will create a neural network to evaluate the performance*  \n<b>Steps of applying deep learning model into prediction<b>  \n+ *Design the network structure*  \n+ *Choose the proper activation function and loss function*  \n+ *Prepare the feature and label and transform them into tensor array*  \n+ *Check the device and whether CUDA could be used*  \n+ *Train the model* \n+ *Evaluate and test the accuracy*","060e80bd":"### T-SNE method of the dimension reduction \n+ The Similarity Measure \n+ The analysis  of the  T-SNE method ","115a861f":"**Note:** *In this part, i will test a few models on this dataset and have a brief view of the accuracy of the model*   \n\n<b>The models will be divided into a few parts<b><br> \n+ The Logistic Regression   \n+ Support Vector Machine  \n+ Deep Learning Method ","6ecdaa86":"## Data Visualization ","74ced391":"#### The heap map of the data ","c3268804":"## Model Training ","d1a5749f":"**Findings:** *From the above table and barchart, we could find that those with similar distribution usually do not conribute much to the difference between classes.*","56d03052":"As is shown above the columns of V22 and V23 have Nan values. Since the data of Credit Fraud Detection is highly imbalanced, so we should fisrt have a brief view of whether directly dropping the Nan value will have a great influence on the Fraud data, and we found that the Nan value dosen't have any impact on the Fraud class data, so I choose drop the whol row of Nan value.","de13147a":"**Note:** *In this part, I will aggregate the data of amount and transfrom the format of time into hour to see whether there exists any distribution features.*","f366536f":"![image-2.png](attachment:image-2.png)","d560684c":"*Transform the numpy array into the torch tensor*","dc47414a":"**Note:**  *In this part, I will use the two PCA features to train the model in order to visualize the result*","09d5d984":"![image.png](attachment:image.png)","22364dba":"### Deep Learning Method ","d88a5651":"### Nan value processing ","0d3d59a0":"**Note:** *The algorithm of support vector machine a kind of traditional statistical method that focus on seeking a higher dimensinal flat so that it could maximize the margin between two classes.*","c0dacf20":"### A brief comparison between three dimension reduction methods  ","f11d66fd":"#### Plot about time ","97c827f3":"##### Outliers detection  \nAs we can seen from the above box plot, there exits a lot of outliers, which may do harm to the result of the prediction, so I decide to make use of the IQR method to deal with the situation. Since the dataset is highly imbalanced, so i choose to only drop those outliers in the Normal set. \n","232fc424":"According to the requirement,we should remove samples that are smaller than Q1 - 2.5 * IQR or larger than Q3 + 2.5 * IQR.","e8bb090e":"### Support Vector Machine ","3ed6370f":"#### Using the Cross Validation to test the accuracy","9509235c":"**Note:** *In this part I will further process the data due to the imbalance, I choose the method of Under-sampling*  \n**Steps:**  \n+ Have a brief view of the ratio between two classes  \n+ Randomly choose the same size of data of Normal class \n\n**Attention:** *The use of Under-Sampling will bring a huge amount of information loss, which may have a impact on the accuracy!!!*","93ba0724":"**Note:** *In this part I will compare the training score and the validation score during the process and trying to seek any correlation between the two curves.*","19bc357d":"#### Features distribution","7a3596e4":"*Print the model and check the parameter of the network*","197f215d":"#### Randomly select a few columns and draw a distribution plot of two classes ","8ea30306":"In this part, I will plot the density graph of different fatures and compare them between different classes.","623ed1bd":"### SVD(Singular Value Decomposition) \n+ The mathematical principle of the SVD \n+ The design of loss function ","76f526e7":"## Dimension Reduction ","9784a22e":"# <h1 align=\"center\"> EDA and Classification Training on credit card fraud detection <\/h1>  \n","57866ab2":"### Logistic Regression In Sklearn ","3074c912":"*In this part I will select a few unsupervised learning method to have a brief view of the dimension reduction algorithm*","09636595":"**Findings:** *From the above visualization, under the situation of using SVM on PC1 & PC2, Linear Kernel and RBF have relatively better performance.*","537f7c9a":"![image.png](attachment:image.png)","698a06c7":"#### Evaluate the performance of the Logistic Regression Model ","ae7ee398":"#### Have a brief view of the distribution and deal with the outliers ","26de6179":"<b>Summary:<b>  \n*As seen above, the PCA and SVD method's effeciency is much shorter than T-sne method, but the performance of T-SNE method is a bit better. In reality, the T-sne method is mainly used in the field of visualization, for T-SNE determines the local neighborhood size of each data point based on the local density of the data (by forcing each conditional probability distribution to have the same degree of confusion). The PCA and SVD method is of great similarities in algorithm, both utilizing the eign vectors related decomposition. In most of the time, the running time of SVD is less than PCA, but since there are only 800 rows of data after the under sampling, so the ruuning time are approximative*","76dfa044":"#### The Learning Curve of two models","f8fcb0db":"## Summary ","ad5afa9a":"**Findings:** *From the experiment above, we can find that there seems no difference between the accuracy of deep learning method and traditional statistical learning method, I think that the reasons are mmainly due to the amount of data. Although there are more than 200000 rows of data, there only exist about 400 fraud credit card record, so if I put the whole dataset into the neural network, it will set all the parameters 0 so that it can achieve an accuracy of 99.8%. As such, I also implement the method of undersampling in the dataset, the average testing accuray in the test dataset is between 88% - 94%, which is similar to the statistical method. If possible, I would also use the method of Data Augmentation to manually combat the limit of data.*","882cbbbd":"<b>Before we Begin:  <\/b>   \nIn order to run the code, necessary pckages are required,make sure the following packages have already been installed in your computer: \n+ Basic scientific calculation libraries: Numpy, Pandas, Matplotlib  \n+ Advanced Visualization libraies: Seaborn  \n+ Packages for machine learning and deep learning: Sklearn, Pytorch(GPU version)  \n+ Packages for dynamic graphs: Cufflinks, Plotly\n\n<b> Introduction <\/b> \n<b> Our Goals: <\/b>  \nThe goal of the project is to analyze the data distribution and build a classification model of the credit card fraud detection. \n\n<b> Outline: <\/b>  \n\n\n\n>I. <b>Data Exploration and Data Cleaning<\/b><br>\n>> <b>Brief view of the dataset<\/b><br>\n>> <b>Nan value processing<\/b><br>\n>> <b>Outlier processing<\/b><br>\n>> <b>Combat Imbalanced Classes in the Dataset<\/b><br>  \n\n>II. <b>Data Visualization<\/b><br>   \n>> <b> Heat map<b>  \n>> <b> Visualization of data distribution<b>  \n>> <b> Table and barchart of statistical values<b>    \n    \n>III. <b>Dimension Reduction<\/b><br>  \n>> <b>Principle Component Analysis<b><br>\n>> <b>Singular Value Decomposition<b><br>\n>> <b>T-distributed Stochastic Neighbor Embedding visualization<b><br> \n>> <b>Comparison between different methods<b><br>\n    \n>IV. <b>Classification<\/b><br> \n>> <b>Sampling Method to deal with the imbalanced data<b><br>\n>> <b>Logistic Regression in Scikit Learn<\/b><br>\n>> <b>Support Vector Machine<\/b><br>\n>> <b>Deep Learning Method in Pytorch<\/b><br> \n>> <b>The accuracy analysis and model evaluation<b><br>  \n    \n>V.<b>Summary<b><br>  \n>> <b>The findings during the process<b><br>\n>>\n\n\n\n<b> References: <\/b>\n<ul> \n<li>Hands on Machine Learning with Scikit-Learn & TensorFlow by Aur\u00e9lien G\u00e9ron (O'Reilly). CopyRight 2017 Aur\u00e9lien G\u00e9ron  <\/li>\n<li>Reference Lecture Note of SDSC2001 \n","cd019baf":"## Import relevant packages ","f19a0e17":"**Findings:** *From the above distribution plot we could find that the fraud class time are more likely to lie in the scope of [2,9]*","a3ecdcb6":"#### The confusion matrix of two models","1b073c8d":"<b>From the distribution plot, we could obviously found that the variance of the Fraud part of the dataset is much larger than the normal part, lefr or right skewed than the normal part. <b><br>","b6a2100a":"## Data Exploration and Data Cleaning","9d9a629c":"*In general, the PC1 together with PC2, PC1 with PC4 have relatively better performance.*","ceb090e0":"#### Plot the table of statistical values and barchart of specified features ","5aca3092":"\n**Note:** *The course project of SDSC2001--Python for Data Science*   \n\n*SID: 56641800&emsp; &emsp; Name: Du Junye*   \n\n*This project is only for the use of course requirement of sdsc2001*  \n\n","375f2dae":"<b>*The process of EDA and Fraud Detection on the dataset is enjoyble and meaningful, during which I get some insight in data processing and analysis, in the following part I will summarize the points and finish my analysis.*<b> \n+ **Data Exploration:** \n> In this part, i have done the Null_value detection, Duplicate_value dropping, Outlier_detection, Min-Max scaler, Balancing data combating. As the most significant part before the machine learning algorithm, the data cleaning and preprocessing process make the data distribution more smooth and achieve the purpose of removing duplicate information, correcting existing errors, and providing data consistency.  \n>>**Outlier detection with imbalanced data:** From previous experiemnt, I found that in most case the Fraud data has relatively larger variance and distribution range, so I choose to not apply the outlier dectection function(IQR standard) to the Fraud part of the data. If I do this, the amount of data will be cut to only 240 rows, one point I should admit is that the outlier feature of data are more likely to contribute more to the Fraud probability.  \n>>**Under Sampling:** The UnderSampling method is a way to cambat the data imbalance, but the cost is that the Under Sampling method would loss a large amount of Normal data, so for those deep learning method, the UnderSampling is a disaster to information loss. In comparison, the SMOTE method could be better.\n+ **Data Visualization:**\n> In this part, I visualize the distribution of all the features and seek the correlation between the features and Class, besides, i also summarize the statistical values of some features.\n>> **Distribution difference between classes:** In general, I found that the fraud data has ralatively larger variance and the range of the min and max in most cases larger than the normal part, that is mainly due to that there exits some unnormal feature values causing the variance larger.  \n>> **The connection between distribution and correlation:** From the experiment, I found that those features having similar distribution usually not so significant in the correlation.   \n>> **Time data distribution:** From the experiment, we could find that fraud class data are more likely to lie in the range from 2 to 9, which is the dawn time in day, so maybe this time period makes it more easy for credit card fraud.\n\n+ **Dimension Reduction:**\n> In this part, I used three famous dimension reduction algorithm to test the performace--PCA, SVD ,T-SNE\n>>**T-SNE's best performance:**Among nearly all the dimension reduction algorithms, the T-SNE methods generally has the best performance, with randomly sampling from stochastic t-distribution, the running time of T-SNE is usually much longer than PCA, so this algorithm is mainly used in the visualization, which could better help us get the insight of potential correlated features.\n\n+ **Prediction with machine learning methods:** \n> In this part, I make use of some machine learning algorithm to predict the reuslt of credit cards, the Logistic Regression method and Support Vector Machine method both have an accuracy of 93.3%.\n>>**Validation Curve and Training Performance:** From the above experiment we could find that the validation set initially sperate from training score and gradually approaching it, which means after iteration, the validation set could become increasingly accurate in representing the performance.  \n>>**The difference between validation and testing:** From the experiement we could find that in general the result of validation set is usually better than the result of testing set, I think it mainly lies on that the distribution of validation set is similar to the training set, which may cause a liitle overfitting problem, but even in this case, the validation set plays an important role in modifying the model and choose those with better performance without bias.  \n    >>**Comparison between two algorithms:** Although the performance of two statistical method in this case is similar, I am also interested in the different applying fields of these two. The principle of SVM is to only consider support vectors, that is, the few points most relevant to classification, to learn the classifier. Logistic regression greatly reduces the weight of points far away from the classification plane through nonlinear mapping, and relatively increases the weight of the data points most relevant to the classification.  \n\n+ **Choice of deep learning and statistical learning**\n>In this part, I design a nerual network to test its adaptility to imbalanced data, however in this case the neural network did not perform perfectly as I expected, in this case, If we put all the data into the neural network, it will set all the parameters 0 so that it could satisfy the requirement of most data, however, when encountered with fraud data, it could not change the weight too much. So I can only use undersampling method to combat with imbalance. The deep learning method finally get a result of 92%, a little lower than statistical learning method. \n>> **In what situation DL could defeat traditional algorithms:** Deep learning could perform better when the dataset has large amount of data and different classes of data are balanced. Besides, deep learning could better simulate the humanbeing behaviour like images recognition and self-driving, where traditional algorithm could not be applied.  \n>>**The disadvantages of DL:** The training of Deep Learning method is much more difficult than traditional algorithms, it usually requires high-performance GPU to do the multi-thread calculation, also, it is also time-consuming in tunning the hyperparameters like learning rate, momentum or batch-features. ","2f509b2c":"#### The visualization of SVM  ","8158e0a3":"As is shown from the above three methods, T-SNE has the best performance. For better visualization,I choose to create a 3D dynamic plot.","7f508263":"![image.png](attachment:image.png)","730e874a":"**Note:** *The above part is about the data cleaning and preprocessing, I have dealt with the null values and drop the outliers in the normal class*","744b84e4":"## Random Under-Sampling ","a5f88465":"**Note:** *In this part i will do a brief summary of the data analysis of the prediction of the credit card fraud detection*","2074309b":"**Note:** *As the pca could explain the main variance of the features, I choose to use a dynamic table to represent the ratio of different compnent contributes*","75aeb716":"### PCA(Principle Component Analysis)  \n*Principal component analysis (PCA) is an unsupervised algorithm that creates a linear combination of original features. The new features are spatially orthogonal, which means they are not related. In addition, they are arranged according to the magnitude of the \"explainable variance\" value. The first principal component (PC1) explains the largest variance in the data set, PC2 explains the second largest variance, and so on.*\n+ The mathematical principle of the dimension reduction \n+ The advantage of the PCA method \n+ The disadvantages of this kind of method ","13abb390":"##### Data Preprocessing \n+ Min-Max Normalization: \n+ Outlier Detection ","bee30b3a":"Confusion matrix : also known as the error matrix, allows visualization of the performance of an algorithm :\n+ true positive (TP) : Diabetic correctly identified as diabetic\n+ true negative (TN) : Healthy correctly identified as healthy\n+ false positive (FP) : Healthy incorrectly identified as diabetic\n+ false negative (FN) : Diabetic incorrectly identified as healthy","05cbb20d":"*The Heap map could describe the density, distribution and changes of the data group on the page, we can use different colors to correspond to different data intervals, visually present the amount of data, and clearly describe the information of correlation.*","21674d22":"**Findings:** *From the barchart and the heatmap V12 & V4 have highest correlation to Class, which conform to the result of heat map.*","c11d5874":"#### Prepare the data of the network ","3b83f39c":"*In this part, i will use the ROC curve and the Precision-Recall curve to evaluate the performance of the model.*","78643021":"<b>The advantages and disadvantages of T-SNE<b>  \n+ The dimension reduction method with best performance \n+ The t-sne costs large memory and long running time \n+ It could not plays a good role when we are not clear whether the dataset is separabl \n"}}