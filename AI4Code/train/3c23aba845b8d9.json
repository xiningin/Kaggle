{"cell_type":{"b6cc8c47":"code","3b1b54c7":"code","dc260c0d":"code","d6e36670":"code","08656fc5":"code","b7dc3cfb":"code","fc997fe8":"code","e16b2509":"code","f6bf7a14":"code","3a12d827":"code","72df6ec7":"code","013de6a3":"code","cf4f7fcc":"code","5051eb82":"code","a422cc12":"code","3082cb14":"code","082aacf8":"code","18e4e28c":"code","ccccba2c":"code","81c1d196":"code","7b34d78f":"code","45de570a":"code","6480cb64":"code","4192ef01":"code","6a0a8896":"code","0c8cc062":"code","55c25994":"code","a814f189":"code","91cd87ea":"code","804807e9":"code","03ff766b":"markdown","dd81f5ef":"markdown","2a311c30":"markdown","4676ba96":"markdown","3bef7541":"markdown","5d96fd74":"markdown","2bca939c":"markdown","e3c0dbc8":"markdown","3198b540":"markdown","e4925ed6":"markdown","bad1f835":"markdown","ce3235c7":"markdown","c5a1819f":"markdown","7f537492":"markdown","2fcd0edb":"markdown","3029fb20":"markdown","aeba9b9b":"markdown","f54ad610":"markdown","f0e90455":"markdown"},"source":{"b6cc8c47":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3b1b54c7":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom nltk.stem import PorterStemmer,WordNetLemmatizer\nfrom wordcloud import WordCloud\nimport re\nimport gensim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.utils import resample\nfrom sklearn.metrics import accuracy_score,f1_score","dc260c0d":"df=pd.read_csv('\/kaggle\/input\/twitter-sentiment-analysis\/train_E6oV3lV.csv')\ntest=pd.read_csv('\/kaggle\/input\/twitter-sentiment-analysis\/test_tweets_anuFYb8.csv')\ndf.head(4)","d6e36670":"df.info()","08656fc5":"print('shape of train dataset',df.shape)\ndf.label.value_counts()","b7dc3cfb":"# \nsns.countplot(df.label,)\nplt.xlabel('class label')\nplt.ylabel('number of tweets')\nplt.show()","fc997fe8":"plt.hist(df[df['label']==1].tweet.str.len(),bins=20,label='class 1')\nplt.legend()\nplt.xlabel('length of tweets')\nplt.ylabel('number of tweets')\nplt.show()\nplt.hist(df[df['label']==0].tweet.str.len(),color='yellow',bins=20,label='class 0')\nplt.legend()\nplt.xlabel('length of tweets')\nplt.ylabel('number of tweets')\nplt.show()","e16b2509":"text=df['tweet'].values.tolist()\ntext_test=test['tweet'].values.tolist()\ntext+=text_test\nprint(len(text))","f6bf7a14":"import nltk\nstopword=nltk.corpus.stopwords.words('english')\nstopword.remove('not')\nfor index,text_ in enumerate(text):\n    text_=re.sub(r'@[\\w]*','',text_) #Removing Twitter Handles (@user)\n    text_=re.sub(r'http\/S+','',text_) #Removing urls from text \n    text_=re.sub(r'[^A-Za-z#]',' ',text_) #Removing Punctuations, Numbers, and Special Characters\n    text_=\" \".join(i.lower() for i in text_.split() if i.lower() not in stopword) #Removing stopword\n    text[index]=text_","3a12d827":"#Stemming the word\npt=PorterStemmer()\nwordnet=WordNetLemmatizer()\nfor index,text_ in enumerate(text):\n    text_=\" \".join(pt.stem(i) for i in text_.split())\n    text_=\" \".join(wordnet.lemmatize(i) for i in text_.split())  \n    text[index]=text_\n","72df6ec7":"df['preprocess_tweet']=text[:len(df)]\ndf['length_tweet']=df['preprocess_tweet'].str.len()\ntest['preprocess_tweet']=text[len(df):]\ndf.head()","013de6a3":"train=df.copy()\ntrain.drop(columns=['id','tweet','preprocess_tweet'],inplace=True)\n","cf4f7fcc":"bow=CountVectorizer( min_df=2, max_features=1000)\nbow.fit(df['preprocess_tweet'])\nbow_df=bow.transform(df['preprocess_tweet']).toarray()\nprint('feature name==',bow.get_feature_names()[:10])\nprint('number of uniqe words',bow_df.shape[1])\nprint('shape',bow_df.shape)\nbow_train=pd.DataFrame(bow_df)\nbow_train['length_tweet']=df['length_tweet']\nbow_train['label']=df['label']\nbow_train.head()","5051eb82":"tfidf=TfidfVectorizer(ngram_range=(1, 2),min_df=2,max_features=1000)\ntfidf.fit(df['preprocess_tweet'])\ntfidf_df=tfidf.transform(df['preprocess_tweet']).toarray()\nprint('number of uniqe words',bow_df.shape[1])\nprint('shape',tfidf_df.shape)\ntfidf_train=pd.DataFrame(tfidf_df)\ntfidf_train['length_tweet']=df['length_tweet']\ntfidf_train['label']=df['label']\ntfidf_train.head()","a422cc12":"tokenize=df['preprocess_tweet'].apply(lambda x: x.split())\nw2vec_model=gensim.models.Word2Vec(tokenize,min_count = 1, size = 100, window = 5, sg = 1)\nw2vec_model.train(tokenize,total_examples= len(df['preprocess_tweet']),epochs=20)","3082cb14":"w2vec_model.most_similar('father')","082aacf8":"w2v_words = list(w2vec_model.wv.vocab)\nprint(\"number of words that occured minimum 5 times \",len(w2v_words))\nprint(\"sample words \", w2v_words[0:50])","18e4e28c":"vector=[]\nfrom tqdm import tqdm\nfor sent in tqdm(tokenize):\n  sent_vec=np.zeros(100)\n  count =0\n  for word in sent: \n    if word in w2v_words:\n      vec = w2vec_model.wv[word]\n      sent_vec += vec \n      count += 1\n  if count != 0:\n    sent_vec \/= count #normalize\n  vector.append(sent_vec)\nprint(len(vector))\nprint(len(vector[0]))        ","ccccba2c":"#example\nl='father dysfunct selfish drag kid dysfunct'\ncount=0\nvcc=np.zeros(100)\nfor word in l:\n  if word in w2v_words:\n    v=w2vec_model.wv[word]\n    vcc+=v\n    count+=1\nvcc","81c1d196":"\nprint('number of uniqe words',len(vector[1]))\nw2v_train=pd.DataFrame(vector)\nw2v_train['length_tweet']=df['length_tweet']\nw2v_train['label']=df['label']\nw2v_train.head()","7b34d78f":"major_class_0,major_class_1=bow_train.label.value_counts()\ndf_major=bow_train[bow_train['label']==0]\ndf_minor=bow_train[bow_train['label']==1]\ndf_minor_upsampled = resample(df_minor, \n                                 replace=True,     # sample with replacement\n                                 n_samples=major_class_0)\ndf_bow_upsampled = pd.concat([df_major, df_minor_upsampled])\nprint('shape',df_bow_upsampled.shape)\nsns.countplot(df_bow_upsampled.label)","45de570a":"major_class_0,major_class_1=tfidf_train.label.value_counts()\ndf_major=tfidf_train[tfidf_train['label']==0]\ndf_minor=tfidf_train[tfidf_train['label']==1]\ndf_minor_upsampled = resample(df_minor, \n                                 replace=True,     # sample with replacement\n                                 n_samples=major_class_0)\ndf_tfidf_upsampled = pd.concat([df_major, df_minor_upsampled])\nprint('shape',df_tfidf_upsampled.shape)\nsns.countplot(df_tfidf_upsampled.label)","6480cb64":"major_class_0,major_class_1=w2v_train.label.value_counts()\ndf_major=w2v_train[w2v_train['label']==0]\ndf_minor=w2v_train[w2v_train['label']==1]\ndf_minor_upsampled = resample(df_minor, \n                                 replace=True,     # sample with replacement\n                                 n_samples=major_class_0)\ndf_w2v_upsampled = pd.concat([df_major, df_minor_upsampled])\nprint('shape',df_w2v_upsampled.shape)\nsns.countplot(df_w2v_upsampled.label)","4192ef01":"x=df_bow_upsampled.iloc[:,0:-1]\ny=df_bow_upsampled['label']\nx_train_bow,x_test_bow,y_train_bow,y_test_bow=train_test_split(x,y,test_size=0.2)","6a0a8896":"x=df_tfidf_upsampled.iloc[:,0:-1]\ny=df_tfidf_upsampled['label']\nx_train_tfidf,x_test_tfidf,y_train_tfidf,y_test_tfidf=train_test_split(x,y,test_size=0.2)","0c8cc062":"x=df_w2v_upsampled.iloc[:,0:-1]\ny=df_w2v_upsampled['label']\nx_train_w2v,x_test_w2v,y_train_w2v,y_test_w2v=train_test_split(x,y,test_size=0.2)","55c25994":"def f1_score_(y_proba,y_test):\n  proba = y_proba[:,1] >= 0.3\n  proba = proba.astype(np.int) \n  return f1_score( proba,y_test)   \n","a814f189":"#use Bow\nfrom sklearn.neighbors import KNeighborsClassifier\nk=[3,5,7,11]\naccuracy=[]\nfor i in tqdm(k):\n  model=KNeighborsClassifier(n_neighbors=i)\n  model.fit(x_train_bow,y_train_bow)\n  y_pred=model.predict(x_test_bow)\n  acc=accuracy_score(y_pred,y_test_bow)\n  print('for k=',i,'Accuracy Score',acc)\n  accuracy.append(acc)\n  y_proba=model.predict_proba(x_test_bow)\n  f1_scor=f1_score_(y_proba,y_test_bow)\n  print('for k=',i,'f1 score ',f1_scor)","91cd87ea":"#use tfidf\nk=[3,5,11]\naccuracy_tfidf=[]\nfor i in k:\n  model=KNeighborsClassifier(n_neighbors=i)\n  model.fit(x_train_tfidf,y_train_tfidf)\n  y_pred=model.predict(x_test_tfidf)\n  acc=accuracy_score(y_pred,y_test_tfidf)\n  print('for k=',i,'Accuracy Score',acc)\n  accuracy_tfidf.append(acc)\n  y_proba=model.predict_proba(x_test_tfidf)\n  f1_scor=f1_score_(y_proba,y_test_tfidf)\n  print('for k=',i,'f1 score ',f1_scor)","804807e9":"#use word2vec\nfrom sklearn.neighbors import KNeighborsClassifier\nk=[3,5,11]\naccuracy_w2v=[]\nfor i in tqdm(k):\n  model=KNeighborsClassifier(n_neighbors=i)\n  model.fit(x_train_w2v,y_train_w2v)\n  y_pred=model.predict(x_test_w2v)\n  acc=accuracy_score(y_pred,y_test_w2v)\n  print('for k=',i,'Accuracy Score',acc)\n  accuracy_w2v.append(acc)\n  y_proba=model.predict_proba(x_test_w2v)\n  f1_scor=f1_score_(y_proba,y_test_w2v)\n  print('for k=',i,'f1 score ',f1_scor)","03ff766b":"## Upsampling BOW","dd81f5ef":"__What is Sentiment Analysis?__\n\nSentiment analysis is a process of identifying an attitude of the author on a topic that is being written about. ","2a311c30":"## Upsampling TF-IDF","4676ba96":"## Upsampling  word2vec","3bef7541":"# Split Dataset","5d96fd74":"# Featurization","2bca939c":"###Word2vec\n__size:__ The number of dimensions of the embeddings and the default is 100.\n\n__window:__ The maximum distance between a target word and words around the target word. The default window is 5.\n\n__min_count:__ The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5.\n\n__workers:__ The number of partitions during training and the default workers is 3.\n\n__sg:__ The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.","e3c0dbc8":"#   Preprocessing Tweet Text\n\n1. Removing Twitter Handles (@user)\n2. Removing urls from text \n3. Removing Punctuations, Numbers, and Special Characters\n\n5. Convert the word to lowercase\n6. Remove Stopwords\n7. Stemming the word\n8. Lemmatization<br>\n\nAfter which we collect the words used to describe positive and negative reviews","3198b540":"<a href=\"https:\/\/colab.research.google.com\/github\/ujjalkumarmaity\/Twitter_Sentiment_Analysis\/blob\/master\/Twitter_Sentiment_Analysis_word2vec.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","e4925ed6":"# Summary\n<table>\n<tr>\n<td>K<\/td>\n<td colspan=2>BOW<\/td>\n<td colspan=2>TF-IDF<\/td>\n<td colspan=2>WORD2VEC<\/td>\n<\/tr>\n<tr><td>    <\/td>\n<td>Accuray<\/td>\n<td>f1_score<\/td>\n<td>Accuray<\/td>\n<td>f1_score<\/td>\n<td>Accuray<\/td>\n<td>f1_score<\/td>\n<\/tr>\n<tr><td>  3 <\/td>\n<td>0.8681864064602961<\/td>\n<td>0.8384620752215477<\/td>\n<td>0.856914535666218<\/td>\n<td>8520794626268402<\/td>\n<td>0.9531460296096904<\/td>\n<td>0.9388906298965841<\/td>\n<\/tr>\n<tr><td>  5 <\/td>\n<td>0.8405114401076716<\/td>\n<td>0.8299329137561381<\/td>\n<td>0.84185733512786<\/td>\n<td>0.8429833863556027<\/td>\n<td>0.9362382234185733<\/td>\n<td>0.9263401465483996<\/td>\n<\/tr>\n<tr><td>  7 <\/td>\n<td>0.8201547779273217<\/td>\n<td>0.8190749079001228<\/td>\n<td> <\/td>\n<td> <\/td>\n<td> <\/td>\n<td> <\/td>\n<\/tr>\n<tr><td>  11 <\/td>\n<td>0.7850773889636609<\/td>\n<td>0.7800259403372244<\/td>\n<td>0.8036675639300135<\/td>\n<td> 0.8097859327217125<\/td>\n<td>0.8855989232839838<\/td>\n<td>0.8777883419878594<\/td>\n<\/tr>\n<\/table>\n\n\n","bad1f835":"###BOW","ce3235c7":"## KNN","c5a1819f":"### TF-IDF Features (Bi-Grams)","7f537492":"# Load the libraries","2fcd0edb":"# Resample","3029fb20":"#  EDA","aeba9b9b":"# Load Dataset","f54ad610":"#### Table of contents\n\n1. Load the libraries\n1. Load Dataset\n1. EDA\n1. Preprocessing Tweet Text\n1. Featurization\n    1. Bag-of-Words\n    1. TF-IDF\n    1. Word2vec\n1. Resample\n    1. Upsampling BOW\n    2. Upsampling TF-IDF\n    1. Upsampling word2vec\n1. Split Dataset\n1. Model Selection\n    1. KNN\n1. Summary\n","f0e90455":"# Model Selection"}}