{"cell_type":{"c07e64f7":"code","31d592b7":"code","d45189b5":"code","d4185f4f":"code","c59a47f5":"code","4f412652":"code","b88af6d4":"code","7d51877f":"code","a3601323":"code","0a793b80":"code","c41f5738":"code","0ddbe329":"code","cae92069":"code","4e06924a":"code","e0e7a284":"code","4683a7f4":"code","07942fe0":"code","4870632b":"code","b9a80659":"code","283365b7":"code","da04ecbb":"code","2bd9695e":"code","82f6d6f0":"code","ad52ee11":"code","692429eb":"code","46faf57c":"code","21113fc1":"code","ce09b262":"code","a2d4e7f2":"code","de790fda":"code","ddb0c433":"code","1d49dbe6":"code","4cade921":"code","24222a73":"code","74f24a5d":"code","1559bfc7":"code","37e7ee06":"code","37b516e2":"code","15632694":"code","3fb4afe9":"code","69a6d3fc":"code","4e574c6f":"code","0b93b108":"code","0ccf6857":"code","903b5501":"code","50f743ab":"code","9aaf26d5":"code","a4bccc55":"code","e5eefe4f":"code","66c09f20":"code","6492deb2":"code","171e6db4":"code","df419528":"code","586ecdd9":"code","4aa12047":"code","d5fd6d38":"code","a3981f61":"code","ad46d89d":"code","126732c3":"code","cd686b34":"code","c9d45157":"code","ddaad3b1":"code","557cc015":"code","5a8024d5":"code","e1c59413":"markdown","d31e5a69":"markdown","12619ec8":"markdown","132ebeae":"markdown","d682b864":"markdown","7a870062":"markdown","90c567b9":"markdown","ece10d38":"markdown","9ea76ac4":"markdown","2838be2b":"markdown","2b9fc4fe":"markdown","8082bef1":"markdown","5b0b76e0":"markdown","79ee66c4":"markdown","934fb1e6":"markdown","39b94bde":"markdown","23bee933":"markdown","4e752d6e":"markdown","bd963c3f":"markdown","1a225803":"markdown","02669113":"markdown","b46b2513":"markdown","bc1e963f":"markdown"},"source":{"c07e64f7":"# Importing necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier,ExtraTreeClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.model_selection import RandomizedSearchCV\nimport category_encoders as ce\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder,PowerTransformer,StandardScaler\nfrom scipy.stats import chi2_contingency\n%matplotlib inline\npd.pandas.set_option('display.max_columns',None)\npd.pandas.set_option('display.max_rows',None)","31d592b7":"# Reading training and testing files\ntrain_data = pd.read_csv('..\/input\/churn-risk-rate-hackerearth-ml\/train.csv')\ntest_data  = pd.read_csv('..\/input\/churn-risk-rate-hackerearth-ml\/test.csv')","d45189b5":"train_data.head()","d4185f4f":"test_data.head()","c59a47f5":"# Displaying total recorfds in training and testing data\nprint(\"There are {} number of rows and {} number of columns in training data\".format(train_data.shape[0],train_data.shape[1]))\nprint(\"There are {} number of rows and {} number of columns in testing data\".format(test_data.shape[0],test_data.shape[1]))","4f412652":"# Describing training data\ntrain_data.describe()","b88af6d4":"train_data.info()","7d51877f":"def check_null(train,test):\n  ''' Checking null values in dataset using heatmap''' \n  plt.figure(figsize=(16,9))\n  plt.subplot(1,2,1)\n  train_visual = sns.heatmap(train.isnull(),yticklabels=False,cmap='viridis')\n  plt.subplot(1,2,2)\n  test_visual  = sns.heatmap(test.isnull(),yticklabels=False,cmap='viridis')\n  plt.show()\n\ncheck_null(train_data,test_data)","a3601323":"def visualize_null_relationship(train):\n    '''visualize the relationship of null values with target variables'''\n    features_with_nan = [features for features in train.columns if train[features].isnull().sum()>=1]\n    for feature in features_with_nan:\n      data = train.copy()\n      data[feature] = np.where(data[feature].isnull(), 1,0)\n      data.groupby(feature)['churn_risk_score'].median().plot.bar()\n      plt.title(feature)\n      plt.show()\n\nvisualize_null_relationship(train_data)","0a793b80":"# Analysing numerical variables\nnumerical_features = [feature for feature in train_data.columns if train_data[feature].dtypes!='O']\nprint(\"The number of numerical features in training data is {}.\".format(len(numerical_features)))\ntrain_data[numerical_features].head()","c41f5738":"# Discrete Features\ndiscrete_features = [features for features in numerical_features if len(train_data[features].unique())<=25 and features not in ['churn_risk_score']]\nprint(\"The number of discrete features are {} \".format(len(discrete_features)))\ntrain_data[discrete_features].head()","0ddbe329":"# Relationship between cont. and target features\nfor feature in numerical_features:\n  data = train_data.copy()\n  data[feature].hist(bins=45)\n  plt.xlabel(feature)\n  plt.ylabel('Churn_Risk')\n  plt.show()\n","cae92069":"#Detecting Outliers with boxplot\nfor feature in numerical_features:\n  data_copy = train_data.copy()\n  data_copy.boxplot(column=feature)\n  plt.title(feature)\n  plt.show()","4e06924a":"#Further, visualizing outliers with scatter plot\nfor feature in numerical_features:\n    data_copy = train_data.copy()\n    plt.scatter(data_copy[feature],data_copy['churn_risk_score'])\n    plt.xlabel(feature)\n    plt.ylabel('Churn_Risk')\n    plt.title(feature)\n    plt.show()","e0e7a284":"# Computing the correlation b\/w the features\nplt.figure(figsize=(10,9))\nsns.heatmap(train_data[numerical_features].corr(),annot=True)","4683a7f4":"# Analysing Categorical Features \ncategorical_features = [feature for feature in train_data.columns if feature not in numerical_features]\nprint(\"Total number of categorical features are {}\".format(len(categorical_features)))\ntrain_data[categorical_features].head()","07942fe0":"# Determining the cadinality of features\nfor features in categorical_features:\n  print(\"The name of the features is {} and its cardinality is {} \".format(features,len(train_data[features].unique())))","4870632b":"# Visualizing relationship of categorical variables with target\nfor feature in categorical_features:\n    data_copy = train_data.copy()\n    print(feature,chi2_contingency(pd.crosstab(data_copy[feature],train_data['churn_risk_score'])))","b9a80659":"# Plotting a countplot for columns having cardinality less than 10\ncard_less_than_10 = [feature for feature in categorical_features if len(train_data[feature].unique())<=10]\nplt.figure(figsize=(30,9))\nfor i in range(0,len(card_less_than_10)):\n    plt.subplot(4,3,i+1)\n    sns.countplot(y=train_data[card_less_than_10[i]])","283365b7":"train_data['churn_risk_score'].value_counts()","da04ecbb":"# Converting records having churn rate == -1 to 1 .\ntraining_data = train_data.copy()\ntraining_data['churn_risk_score'] = training_data['churn_risk_score'].apply(lambda x:1 if x==-1 else 0 if x==5 else x)\ntraining_data.shape","2bd9695e":"training_data['churn_risk_score'].value_counts().plot.bar()","82f6d6f0":"training_data.head()","ad52ee11":"# Handling Misssing values in Numerical Variables\ndef missing_numerical(train,test,feature):\n  median_value_train = train[feature].median()\n  median_value_test  = test[feature].median()\n  train[feature+'_nan'] = np.where(train[feature].isnull(),1,0)\n  train[feature] = np.where(train[feature].isnull(),median_value_train,train[feature])\n  test[feature+'_nan'] = np.where(test[feature].isnull(),1,0)\n  test[feature]  = np.where(test[feature].isnull(),median_value_test,test[feature])\n  return train,test\n\ntraining_data,test_data = missing_numerical(training_data,test_data,'points_in_wallet')","692429eb":"# Dropping Id Column as it doesn't contributes in prediction\ntraining_data = training_data.drop(['customer_id'],axis=1)\ntesting_data = test_data.copy()\ntesting_data = testing_data.drop(['customer_id'],axis=1)","46faf57c":"# Handling Categorical Variables in dataset\ncategorical_var = ['region_category', 'preferred_offer_types','joined_through_referral','medium_of_operation']\n\ndef calc_mode(data,feature):\n  return data[feature].mode()\n\ndef categorical_null(train,test,features):\n  for feature in features[0:2]:\n    train[feature+\"_nan\"] = np.where(train[feature].isnull(),1,0) \n    train[feature] = np.where(train[feature].isnull(),calc_mode(train,feature),train[feature])\n    test[feature+\"_nan\"] = np.where(test[feature].isnull(),1,0)  \n    test[feature] = np.where(test[feature].isnull(),calc_mode(test,feature),test[feature]) \n\n\n  for feature in features[3:]:\n    train[feature+\"_missing\"] = np.where(train[feature]==\"?\",1,0)  \n    train[feature] = np.where(train[feature]=='?',calc_mode(train,feature),train[feature])\n    test[feature+\"_missing\"] = np.where(test[feature]==\"?\",1,0) \n    test[feature] = np.where(test[feature]=='?',calc_mode(test,feature),test[feature]) \n\n  return train,test\n\ntraining_data,testing_data = categorical_null(training_data,testing_data,categorical_var)","21113fc1":"training_data.isnull().any()","ce09b262":"testing_data.isnull().any()","a2d4e7f2":"# Performing Feature Engineering on Numerical Variables [age\tdays_since_last_login\tavg_time_spent\tavg_transaction_value\tpoints_in_wallet]\nnumerical_features = [feature for feature in training_data.columns if training_data[feature].dtypes!='O' and feature not in ['churn_risk_score']]\nfeatures = [\"age\",\"days_since_last_login\",\"avg_time_spent\"]\ndef encode_neg_val(data,feature):\n  '''Handle negative values'''\n  data[feature] = np.where(data[feature]<0,0,data[feature])\n  return data[feature]\n\ndef dsl_eng(train,test,feature):\n    ''' Feature Engineering Days Since Last Login '''\n    train[feature] = encode_neg_val(train,feature)\n    test[feature] = encode_neg_val(test,feature)\n    return train,test\n\ndef ats_eng(train,test,feature):\n    ''' Feature Engineering Avg. time spent '''\n    train[feature] = encode_neg_val(train,feature)\n    test[feature] =  encode_neg_val(test,feature)\n    return train,test\n\ndef feature_eng_numerical(train,test,features):\n\n  ''' Feature Engineering Numerical Columns'''\n  train,test = dsl_eng(train,test,features[1])  #Days Since Last Login\n  train,test = ats_eng(train,test,features[2])  # avg_time_spent\n  return train,test\n\ntraining_data,testing_data = feature_eng_numerical(training_data,testing_data,features)\n  \n","de790fda":"training_data.head()","ddb0c433":"testing_data.head()","1d49dbe6":"# Performing Feature Engineering on Categorical Variables \ncategorical_feat = ['joining_date','avg_frequency_login_days',\"referral_id\"]\n\n\ndef jd_eng(train,test,feature):\n  ''' Feature Eng. Joining date'''\n  present_yr = 2021\n  train[feature] = train[feature].str.split(\"-\",expand=True)[0].astype(int)\n  train[feature] = present_yr-train[feature]\n  test[feature]  = test[feature].str.split(\"-\",expand=True)[0].astype(int)\n  test[feature]  = present_yr-test[feature]\n  return train,test\n\ndef calc_login_act(data,feature):\n \n  data[feature] = np.where(data[feature].str.contains('Error'),\n                                             0.0,data[feature])\n  return data[feature]\n\ndef alg_eng(train,test,feature):\n  ''' Feature Eng. Avg Login Days'''\n  train[feature] = calc_login_act(train,feature)\n  test[feature] =  calc_login_act(test,feature)\n  return train,test\n\ndef rid_eng(train,test,feature):\n  ''' Feature Eng. Referral Id'''\n  encoder = ce.CountEncoder()\n  train[feature] = encoder.fit_transform(train[feature])\n  test[feature] = encoder.transform(test[feature])\n  train[feature] = train[feature].apply(lambda x:\"Not_Referred\" if x==1 else \"Referred\" if x<20 else \"Unknown\")\n  test[feature] = test[feature].apply(lambda x:\"Not_Referred\" if x==1 else \"Referred\" if x<20 else \"Unknown\")\n  return train,test\n\ndef categorical_eng(train,test,features):\n  ''' Feature Engineering Categorical Variables'''\n\n  train,test = jd_eng(train,test,features[0]) # joining_date\n  train,test = alg_eng(train,test,features[1]) #avg_login_days\n  train,test = rid_eng(train,test,features[2]) #referral_id\n  return train,test\n\ntraining_data,testing_data = categorical_eng(training_data,testing_data,categorical_feat)","4cade921":"training_data.head()","24222a73":"testing_data.head()","74f24a5d":"# Removing columns with high cardinality and less contribution\ncols_to_remove = ['Name','security_no','last_visit_time']\ntraining_data = training_data.drop(columns=cols_to_remove,axis=1)\ntesting_data = testing_data.drop(columns=cols_to_remove,axis=1)","1559bfc7":"# Seperating independent and dependent features\ny = training_data['churn_risk_score']\nX_train = training_data.drop(columns=['churn_risk_score'],axis=1)\nX_test = testing_data.copy()","37e7ee06":"# Encoding Categorical Variables\n\ncols_to_encode = ['membership_category',\n              'gender','region_category','joined_through_referral',\n               'preferred_offer_types','medium_of_operation','internet_option',\n               'used_special_discount','offer_application_preference','past_complaint',\n               'complaint_status','feedback',\"referral_id\"]\n               \n\nX_train_le = X_train.copy()\nX_test_le  = X_test.copy()\n\ndef label_encode(data,features):\n  dummies = data.copy()\n  dummies = pd.get_dummies(dummies[features])\n  data = pd.concat([data,dummies],axis=1)\n  data = data.drop(columns=features,axis=1)\n\n  return data\n\nX_train_le = label_encode(X_train_le,cols_to_encode)\nX_test_le  = label_encode(X_test_le,cols_to_encode)\n","37b516e2":"X_train_le.head()","15632694":"X_test_le.head()","3fb4afe9":"# First transforming numerical variables to gaussian curve using power transformer\npt = PowerTransformer(method='yeo-johnson',standardize=False)\ncols_to_pt = ['age','points_in_wallet','avg_time_spent','avg_transaction_value']\nX_train_pt = X_train_le.copy()\nX_test_pt  = X_test_le.copy()\nX_train_pt = pd.DataFrame(pt.fit_transform(X_train_pt[cols_to_pt]),columns=cols_to_pt)\nX_train_transformed = X_train_le.drop(columns=cols_to_pt)\nX_train_ptransformed = pd.concat([X_train_pt,X_train_transformed],axis=1)\nX_test_pt  = pd.DataFrame(pt.transform(X_test_pt[cols_to_pt]),columns=cols_to_pt)\nX_test_transformed = X_test_le.drop(columns=cols_to_pt)\nX_test_ptransformed = pd.concat([X_test_transformed,X_test_pt],axis=1)","69a6d3fc":"# Standardizing using Standard Scaler\nsc = StandardScaler()\nX_train_scaled = pd.DataFrame(sc.fit_transform(X_train_ptransformed),columns= X_train_ptransformed.columns)\nX_test_scaled = pd.DataFrame(sc.fit_transform(X_test_ptransformed),columns = X_test_ptransformed.columns)","4e574c6f":"X_train_scaled.head()","0b93b108":"X_test_scaled.head()","0ccf6857":"from sklearn.ensemble import ExtraTreesClassifier\nmodel=ExtraTreesClassifier()\nmodel.fit(X_train_scaled,y)","903b5501":"feat_importances=pd.Series(model.feature_importances_,index=X_train_scaled.columns)\nfeat_importances.nlargest(66).plot(kind='barh')","50f743ab":"# Model Splitting\nx_train,x_test,y_train,y_test = train_test_split(X_train_scaled,y,test_size=0.1,random_state=1,stratify=y)\nx_train,x_valid,y_train,y_valid = train_test_split(x_train,y_train,test_size=0.1,random_state=1,stratify = y_train)","9aaf26d5":"model_1 = LogisticRegression(max_iter=400)\nmodel_1.fit(x_train,y_train.values.ravel())\npredictions_1 = model_1.predict(x_valid)\nscore_1 = f1_score(y_valid,predictions_1,average='macro')\nscore_1","a4bccc55":"predictions_1 = model_1.predict(x_test)\nscore_1 = f1_score(y_test,predictions_1,average='macro')\nscore_1","e5eefe4f":"model_2 = KNeighborsClassifier()\nmodel_2.fit(x_train,y_train)\npredictions_2 = model_2.predict(x_valid)\nscore_2 = f1_score(y_valid,predictions_2,average='macro')\nscore_2","66c09f20":"predictions_2 = model_2.predict(x_test)\nscore_2 = f1_score(y_test,predictions_2,average='macro')\nscore_2","6492deb2":"model_3 = SVC()\nmodel_3.fit(x_train,y_train.values.ravel())\npredictions_3 = model_3.predict(x_valid)\nscore_3 = f1_score(y_valid,predictions_3,average='macro')\nscore_3","171e6db4":"predictions_3 = model_3.predict(x_test)\nscore_3 = f1_score(y_test,predictions_3,average='macro')\nscore_3","df419528":"model_4 = DecisionTreeClassifier()\nmodel_4.fit(x_train,y_train.values.ravel())\npredictions_4 = model_4.predict(x_valid)\nscore_4 = f1_score(y_valid,predictions_4,average='macro')\nscore_4","586ecdd9":"predictions_4 = model_4.predict(x_test)\nscore_4 = f1_score(y_test,predictions_4,average='macro')\nscore_4","4aa12047":"model_5 = RandomForestClassifier()\nmodel_5.fit(x_train,y_train.values.ravel())\npredictions_5 = model_5.predict(x_valid)\nscore_5 = f1_score(y_valid,predictions_5,average='macro')\nscore_5","d5fd6d38":"predictions_5 = model_5.predict(x_test)\nscore_5 = f1_score(y_test,predictions_5,average='macro')\nscore_5","a3981f61":"from xgboost import XGBClassifier\nmodel_6 = XGBClassifier()\nmodel_6.fit(x_train,y_train.values.ravel())\npredictions_6 = model_6.predict(x_valid)\nscore_6 = f1_score(y_valid,predictions_6,average='macro')\nscore_6","ad46d89d":"predictions_6 = model_6.predict(x_test)\nscore_6 = f1_score(y_test,predictions_6,average='macro')\nscore_6","126732c3":"import catboost as cb\ncat_model = cb.CatBoostClassifier(verbose=2,iterations=90,depth=3,learning_rate=0.2,bagging_temperature=0.8,border_count=236,l2_leaf_reg=2)\ncat_model.fit(x_train,y_train,eval_set=(x_valid,y_valid))\nprint(cat_model.best_score_)","cd686b34":"predictions_8 = cat_model.predict(x_test)\nscore_8 = f1_score(y_test,predictions_8,average='macro')\nscore_8","c9d45157":"# Classification Report\nfrom sklearn.metrics import classification_report\ncr = classification_report(y_test,predictions_8)\nprint(cr)","ddaad3b1":"final_predictions = pd.DataFrame(cat_model.predict(X_test_scaled))\nfinal_predictions.columns = ['churn_risk_score']\nfinal_predictions = pd.concat([test_data[\"customer_id\"],final_predictions],axis=1)\nfinal_predictions.head()\n","557cc015":"final_predictions['churn_risk_score'].value_counts()","5a8024d5":"final_predictions['churn_risk_score'] = final_predictions['churn_risk_score'].apply(lambda x:5 if x==0 else x)","e1c59413":"From the above, following observations can be deduced:\nGender: Both number of Males and Females are equal.\n\n    There is an 'unknown' category in the column\nJoined through referral : Equally people joined\/not joined.\n\n    ? represents about status of 4500 unknown people.\nInternet Option : Contains equal no. of subscribers.\n\nPast Complaints: Contains equal no. of people.\n\nRegion_category :\n\n              Shows about 14000 people living in Town.\n              Shows about 4500 people living in Villages.\nPreferred offer types: Mopstly Contains equal no. of people distribution.\n\nUsed Special Discout :\n\n         Shows 20000+ people applied for discount\n         Shows about 16500 people didn't applied for \nComplaint_status:\n\n             Shows most complaints are not applicable\nMembership category :\n\nShows max. count of people with no and basic membership.\nShows least count for people with premium\/platinum membership\nMode of Operation:\n\nShows most people use Desktop and smartfone for operation\nShows ? about 5800 unknown no of people.\nPreferes offers: Shows max. count for people preferes offers.\n\nFeedback:\n\n      Most of the people had given negative comments ","d31e5a69":"**1. Logistic Regression.**","12619ec8":"**Since,Catbbost Algorithm gives the higest f1-score, therefore we choose it as the final model for the prediction.**","132ebeae":"**It appears that no columns have a relationship.**","d682b864":"**We will keep all the features in our training and testing data since all the features are contributing in prediction.**","7a870062":"**2. KNN**","90c567b9":"**3. SVM**","ece10d38":"**5. Random Forest**","9ea76ac4":"**From visualization, it appears that some of the numerical features are skewed. Also, the target variable contains outliers.**","2838be2b":"**From statistics, features: region_category,membership_category,joined_through_reference, preferred_off_types,medium_of_operation, avg_frequency_login_days,offer_application_preference and feedback are related to target.**","2b9fc4fe":"# Feature Scaling","8082bef1":"**6. XGBOOST**","5b0b76e0":"# Prediction on Test Data","79ee66c4":"7. CatBoost Classifier","934fb1e6":"# Data Pre-processing","39b94bde":"**4. Decision Tree**","23bee933":"# Model Building","4e752d6e":"# Data Cleaning","bd963c3f":"# Performing Feature Engineering","1a225803":"**There are most of the columns with high cardinality.**","02669113":"**From the above heatmap, it appears that there are three columns with null values i.e. 'region_category', 'preferred_offer_types', 'points_in_wallet'. Also, it appears that 'region_category' column have higher null values in both dataset while 'preferred_offer_types' column has the least.**","b46b2513":"# Feature Selection","bc1e963f":"**From the scatter plot and boxplot it appears that most of the columns contains outliers even the target variable with churn rate= -1.**"}}