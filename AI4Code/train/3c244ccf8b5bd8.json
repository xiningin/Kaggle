{"cell_type":{"50391544":"code","c5c79149":"code","6f4692a5":"code","d2f76e1a":"code","e322c037":"code","2a4e3fe1":"code","f9f4f61a":"code","6b48e11c":"code","227bd8c8":"code","b55d786e":"code","d4252eb8":"code","32c30688":"code","8fdbf8b5":"code","bbe5b5e2":"code","f3ecc30a":"code","98aabbd4":"code","961bc627":"code","b2eb21a1":"code","6508ace9":"markdown","ead82887":"markdown","658f007b":"markdown","72b5b16d":"markdown","c9ea8e13":"markdown","e41b339b":"markdown","de91b9c4":"markdown","2d1ffbdf":"markdown","ecac539c":"markdown","ffabee84":"markdown","0709ec57":"markdown","2fe6dcc3":"markdown","9c8b75d7":"markdown","b5df166a":"markdown","a409dc8d":"markdown","f995eea4":"markdown","ec8e7aba":"markdown","6e648e14":"markdown","561d8ecc":"markdown","a6babae6":"markdown","10be5fc1":"markdown","befef91b":"markdown","54fac914":"markdown","24b1c9ab":"markdown"},"source":{"50391544":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c5c79149":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntrain.head(10)","6f4692a5":"train.count()","d2f76e1a":"test.count()","e322c037":"from nltk.corpus import stopwords\nimport re\nimport string","2a4e3fe1":"def change_contraction_verb(text):\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    \n    # specific\n    text = re.sub(r\"won\\'t\", \"will not\", text)\n    text = re.sub(r\"can\\'t\", \"can not\", text)\n    return text\n\ntrain['text'] = train['text'].apply(lambda x : change_contraction_verb(x))\ntest['text'] = test['text'].apply(lambda x : change_contraction_verb(x))\n\ntrain['text'].head(10)","f9f4f61a":"def custom_preprocessor(text):\n    '''\n    Make text lowercase, remove text in square brackets,remove links,remove special characters\n    and remove words containing numbers.\n    '''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub(\"\\\\W\",\" \",text) # remove special chars\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\ntrain['text'] = train['text'].apply(lambda x : custom_preprocessor(x))\ntest['text'] = test['text'].apply(lambda x : custom_preprocessor(x))\n\ntrain['text'].head(10)","6b48e11c":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\ntrain['text'] = train['text'].apply(lambda x : remove_emoji(x))\ntest['text'] = test['text'].apply(lambda x : remove_emoji(x))","227bd8c8":"train.head(10)","b55d786e":"test.head(10)","d4252eb8":"#Stopwords\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nstopwords = stopwords.words('english')\n\nprint(stopwords)","32c30688":"#CountVectorizer\n\ncount_vectorizer = CountVectorizer(token_pattern=r'\\w{1,}', ngram_range=(1, 2), stop_words = stopwords)\n\ntrain_vector = count_vectorizer.fit_transform(train['text'])\ntest_vector = count_vectorizer.transform(test['text'])","8fdbf8b5":"train_vector.toarray()","bbe5b5e2":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import model_selection\n\nclf = LogisticRegression()\n\n#Check accuracy score from K-Fold cross validation\nscores = model_selection.cross_val_score(clf, train_vector, train[\"target\"], cv=5, scoring=\"accuracy\")\nprint(scores)","f3ecc30a":"#get the mean of each fold \nprint(\"Accuracy of Model with Cross Validation is: \",scores.mean() * 100)","98aabbd4":"# Fitting a simple Logistic Regression on Counts\nclf.fit(train_vector, train[\"target\"])","961bc627":"# Submission\nsubmission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\nsubmission[\"target\"] = clf.predict(test_vector)\nsubmission.to_csv(\"submission.csv\", index=False)","b2eb21a1":"submission.head(20)","6508ace9":"# **Reference**\n\n* https:\/\/www.kaggle.com\/parulpandey\/getting-started-with-nlp-a-general-intro\n* https:\/\/www.kaggle.com\/parulpandey\/getting-started-with-nlp-feature-vectors\n* http:\/\/as.nida.ac.th\/gsas\/wp-content\/uploads\/2020\/07\/6110422045_%E0%B8%9B%E0%B8%8F%E0%B8%B4%E0%B8%8D%E0%B8%8D%E0%B8%B2-%E0%B8%AB%E0%B8%B1%E0%B8%AA%E0%B8%81%E0%B8%B8%E0%B8%A5.pdf (Thai)","ead82887":"\u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e17\u0e33\u0e01\u0e32\u0e23\u0e40\u0e0a\u0e47\u0e04\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e02\u0e2d\u0e07\u0e17\u0e31\u0e49\u0e07 2 datasets \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 <code>pd.head()<\/code> \u0e43\u0e19\u0e01\u0e32\u0e23\u0e41\u0e2a\u0e14\u0e07\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e0b\u0e36\u0e48\u0e07\u0e08\u0e30\u0e01\u0e33\u0e2b\u0e19\u0e14\u0e40\u0e23\u0e34\u0e48\u0e21\u0e15\u0e49\u0e19\u0e17\u0e35\u0e48 5 \u0e41\u0e16\u0e27\u0e41\u0e23\u0e01\n\nThen, we checked data from 2 datasets by using <code>pd.head()<\/code> to show data. Which starts at the first 5 rows.","658f007b":"# **Members**\n\n1. \u0e19\u0e32\u0e22\u0e1b\u0e0f\u0e34\u0e20\u0e32\u0e13 \u0e23\u0e31\u0e15\u0e19\u0e32\u0e27\u0e34\u0e19 | Patipan Rattanawin | 6209656187\n2. \u0e19\u0e32\u0e22\u0e1e\u0e07\u0e28\u0e4c\u0e1e\u0e25 \u0e19\u0e34\u0e23\u0e32\u0e21\u0e31\u0e22 | PhongPhon Niramai | 6209656120\n3. \u0e19\u0e32\u0e07\u0e2a\u0e32\u0e27\u0e13\u0e31\u0e0f\u0e10\u0e19\u0e34\u0e0a \u0e14\u0e27\u0e07\u0e2d\u0e32\u0e17\u0e34\u0e15\u0e22\u0e4c | Natthanit Duangarthit | 6209656492","72b5b16d":"\u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e17\u0e32\u0e07\u0e01\u0e25\u0e38\u0e48\u0e21\u0e44\u0e14\u0e49\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2b\u0e32\u0e04\u0e48\u0e32\u0e04\u0e27\u0e32\u0e21\u0e41\u0e21\u0e48\u0e19\u0e22\u0e33\u0e02\u0e2d\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25\u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 <code>cross_val_score<\/code> \u0e0b\u0e36\u0e48\u0e07\u0e21\u0e35 parameters \u0e14\u0e31\u0e07\u0e19\u0e35\u0e49\n\n* clf : \u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23\u0e17\u0e35\u0e48\u0e40\u0e01\u0e47\u0e1a\u0e1f\u0e31\u0e07\u0e01\u0e4c\u0e0a\u0e31\u0e19 <code>LogisticRegression<\/code>\n* train_vector : \u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23 list \u0e17\u0e35\u0e48\u0e08\u0e30\u0e17\u0e33\u0e01\u0e32\u0e23 fit \n* train\\['target'\\] : \u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23 list \u0e17\u0e35\u0e48\u0e08\u0e30\u0e17\u0e33\u0e19\u0e32\u0e22\u0e08\u0e32\u0e01\u0e01\u0e32\u0e23 learning\n* cv : \u0e15\u0e31\u0e27\u0e01\u0e33\u0e2b\u0e19\u0e14\u0e01\u0e32\u0e23\u0e41\u0e1a\u0e48\u0e07\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e04\u0e33\u0e19\u0e27\u0e13\u0e04\u0e48\u0e32\u0e04\u0e27\u0e32\u0e21\u0e41\u0e21\u0e48\u0e19\u0e22\u0e33\u0e2b\u0e23\u0e37\u0e2d\u0e04\u0e48\u0e32\u0e04\u0e27\u0e32\u0e21\u0e1c\u0e34\u0e14\u0e1e\u0e25\u0e32\u0e14\u0e08\u0e33\u0e19\u0e27\u0e19 K \u0e23\u0e2d\u0e1a (\u0e43\u0e19\u0e17\u0e35\u0e48\u0e19\u0e35\u0e49\u0e43\u0e0a\u0e49 5 \u0e23\u0e2d\u0e1a)\n* scoring : \u0e15\u0e31\u0e27\u0e41\u0e2a\u0e14\u0e07\u0e1c\u0e25\u0e25\u0e31\u0e1e\u0e18\u0e4c\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e08\u0e32\u0e01\u0e01\u0e32\u0e23 learning \u0e41\u0e25\u0e30\u0e17\u0e33\u0e19\u0e32\u0e22\u0e2d\u0e2d\u0e01\u0e21\u0e32 \u0e0b\u0e36\u0e48\u0e07\u0e08\u0e30\u0e43\u0e0a\u0e49 accuracy \u0e43\u0e19\u0e01\u0e32\u0e23\u0e41\u0e2a\u0e14\u0e07\u0e1c\u0e25\u0e25\u0e31\u0e1e\u0e18\u0e4c\u0e41\u0e15\u0e48\u0e25\u0e30 fold\n\n\u0e41\u0e2a\u0e14\u0e07\u0e1c\u0e25\u0e14\u0e31\u0e07\u0e42\u0e04\u0e49\u0e14\u0e02\u0e49\u0e32\u0e07\u0e25\u0e48\u0e32\u0e07\n\nThen, we calculated the accuracy value by using <code>cross_val_score<\/code> which has parameters\n\n* clf : a variable collected a function <code>LogisticRegression<\/code>\n* train_vector : a list variable to be fit\n* train\\['target'\\] : a list variable to predict from learning\n* cv : divide data to learn and calculate the accuracy value or the error value of K cycles (In this case, we use 5 times)\n* scoring : Indicator of learning and prediction results, which will use the accuracy.\n\nShow the result as the code below.","c9ea8e13":"------------------------------------------------------------------------------------------------------","e41b339b":"# **Dataset**\n\nDataset \u0e17\u0e35\u0e48\u0e43\u0e0a\u0e49 \u0e1b\u0e23\u0e30\u0e01\u0e2d\u0e1a\u0e14\u0e49\u0e27\u0e22 \n\n1. train.csv \u0e0b\u0e36\u0e48\u0e07\u0e21\u0e35 Instances (\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e08\u0e32\u0e01 Twitter) \u0e08\u0e33\u0e19\u0e27\u0e19 7,613 \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e41\u0e25\u0e30\u0e21\u0e35 Attribute \u0e14\u0e31\u0e07\u0e15\u0e48\u0e2d\u0e44\u0e1b\u0e19\u0e35\u0e49\n    * id : \u0e40\u0e25\u0e02\u0e25\u0e33\u0e14\u0e31\u0e1a id \u0e02\u0e2d\u0e07\u0e1c\u0e39\u0e49\u0e43\u0e0a\u0e49\n    * keyword : \u0e04\u0e33\u0e40\u0e09\u0e1e\u0e32\u0e30\u0e43\u0e19\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22\u0e17\u0e35\u0e48\u0e40\u0e08\u0e2d\u0e08\u0e32\u0e01 Tweet\n    * location : \u0e1e\u0e34\u0e01\u0e31\u0e14\u0e02\u0e2d\u0e07\u0e1c\u0e39\u0e49\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\n    * text : \u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e17\u0e35\u0e48\u0e1c\u0e39\u0e49\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\u0e17\u0e33\u0e01\u0e32\u0e23\u0e42\u0e1e\u0e2a\u0e15\u0e4c\n    * target : \u0e1c\u0e25\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22\u0e27\u0e48\u0e32\u0e40\u0e1b\u0e47\u0e19\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e17\u0e35\u0e48\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e02\u0e49\u0e2d\u0e07\u0e01\u0e31\u0e1a\u0e20\u0e31\u0e22\u0e1e\u0e34\u0e1a\u0e31\u0e15\u0e34\u0e2b\u0e23\u0e37\u0e2d\u0e44\u0e21\u0e48 (0 : \u0e44\u0e21\u0e48\u0e40\u0e1b\u0e47\u0e19, 1 : \u0e40\u0e1b\u0e47\u0e19)\n\n2. test.csv \u0e0b\u0e36\u0e48\u0e07\u0e21\u0e35 Instances (\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e08\u0e32\u0e01 Twitter) \u0e08\u0e33\u0e19\u0e27\u0e19 3,263 \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e41\u0e25\u0e30\u0e21\u0e35 Attribute \u0e14\u0e31\u0e07\u0e15\u0e48\u0e2d\u0e44\u0e1b\u0e19\u0e35\u0e49\n    * id : \u0e40\u0e25\u0e02\u0e25\u0e33\u0e14\u0e31\u0e1a id \u0e02\u0e2d\u0e07\u0e1c\u0e39\u0e49\u0e43\u0e0a\u0e49\n    * keyword : \u0e04\u0e33\u0e40\u0e09\u0e1e\u0e32\u0e30\u0e43\u0e19\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22\n    * location : \u0e1e\u0e34\u0e01\u0e31\u0e14\u0e02\u0e2d\u0e07\u0e1c\u0e39\u0e49\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\n    * text : \u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e17\u0e35\u0e48\u0e1c\u0e39\u0e49\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\u0e17\u0e33\u0e01\u0e32\u0e23\u0e42\u0e1e\u0e2a\u0e15\u0e4c\n\n3. sample_submission.csv \u0e0b\u0e36\u0e48\u0e07\u0e08\u0e30\u0e17\u0e33\u0e01\u0e32\u0e23\u0e40\u0e01\u0e47\u0e1a\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25 id \u0e41\u0e25\u0e30 target \u0e17\u0e35\u0e48\u0e44\u0e27\u0e49\u0e23\u0e30\u0e1a\u0e38\u0e27\u0e48\u0e32\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21 \u0e13 id \u0e19\u0e31\u0e49\u0e19\u0e40\u0e1b\u0e47\u0e19\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e17\u0e35\u0e48\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e02\u0e49\u0e2d\u0e07\u0e01\u0e31\u0e1a\u0e20\u0e31\u0e22\u0e1e\u0e34\u0e1a\u0e31\u0e15\u0e34\u0e2b\u0e23\u0e37\u0e2d\u0e44\u0e21\u0e48\n\nThe dataset used contains\n\n1. train.csv that has instances (texts from Twitter) 7,613 rows and atrributes\n    * id : a unique identifier of user\n    * keyword : a particular word from the tweet\n    * location : the location the tweet was sent from\n    * text : the text of the tweet\n    * target : this denotes whether a tweet is about a real disaster (1) or not (0)\n    \n2. test.csv that has instances (texts from Twitter) 3,263 rows and attributes\n    * id : a unique identifier of user\n    * keyword : a particular word from the tweet\n    * location : the location the tweet was sent from\n    * text : the text of the tweet\n    \n3. submission.csv : This will collect the id and target that indicates whether the message at the id is a disaster-related message or not.","de91b9c4":"\u0e19\u0e2d\u0e01\u0e08\u0e32\u0e01\u0e19\u0e35\u0e49\u0e17\u0e32\u0e07\u0e01\u0e25\u0e38\u0e48\u0e21\u0e22\u0e31\u0e07\u0e44\u0e14\u0e49\u0e28\u0e36\u0e01\u0e29\u0e32\u0e01\u0e32\u0e23\u0e19\u0e33 emojis \u0e2d\u0e2d\u0e01\u0e08\u0e32\u0e01\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21 \u0e0b\u0e36\u0e48\u0e07\u0e08\u0e30\u0e43\u0e0a\u0e49 <code>re.compile()<\/code> \u0e43\u0e19\u0e01\u0e32\u0e23\u0e23\u0e27\u0e1a\u0e23\u0e27\u0e21 pattern \u0e17\u0e35\u0e48\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e02\u0e49\u0e2d\u0e07\u0e01\u0e31\u0e1a emoji \u0e41\u0e25\u0e30 flags \u0e40\u0e01\u0e47\u0e1a\u0e40\u0e02\u0e49\u0e32\u0e43\u0e19\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23 <code>emoji_pattern<\/code> \u0e42\u0e14\u0e22\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e40\u0e1b\u0e47\u0e19\u0e1f\u0e31\u0e07\u0e01\u0e4c\u0e0a\u0e31\u0e19 <code>remove_emoji<\/code> \u0e0b\u0e36\u0e48\u0e07\u0e08\u0e30 return \u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e17\u0e35\u0e48\u0e21\u0e35\u0e01\u0e32\u0e23\u0e15\u0e31\u0e14 emojis \u0e41\u0e25\u0e30 flags \u0e2d\u0e2d\u0e01\u0e41\u0e25\u0e49\u0e27 \u0e14\u0e31\u0e07\u0e42\u0e04\u0e49\u0e14\u0e02\u0e49\u0e32\u0e07\u0e25\u0e48\u0e32\u0e07\n\nIn addition, we also studied emojis removal from text, which uses <code>re.compile()<\/code> to compile emoji-related patterns and flags into a variable name <code>emoji_pattern<\/code>. And we create a function <code>remove_emoji<\/code> that will return the text with the emojis and flags removed, as in the code below.","2d1ffbdf":"\u0e2b\u0e32\u0e01\u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e1c\u0e34\u0e14\u0e1e\u0e25\u0e32\u0e14\u0e1b\u0e23\u0e30\u0e01\u0e32\u0e23\u0e43\u0e14 \u0e17\u0e32\u0e07\u0e01\u0e25\u0e38\u0e48\u0e21\u0e02\u0e2d\u0e2d\u0e20\u0e31\u0e22\u0e21\u0e32 \u0e13 \u0e17\u0e35\u0e48\u0e19\u0e35\u0e49\u0e14\u0e49\u0e27\u0e22\u0e04\u0e23\u0e31\u0e1a \u0e02\u0e2d\u0e1a\u0e04\u0e38\u0e13\u0e04\u0e23\u0e31\u0e1a\n\nIf there is any mistake, we apologize here too. Thank you.","ecac539c":"# **Data Cleaning**\n\n\u0e40\u0e21\u0e37\u0e48\u0e2d\u0e14\u0e36\u0e07\u0e44\u0e1f\u0e25\u0e4c\u0e40\u0e01\u0e47\u0e1a\u0e40\u0e02\u0e49\u0e32\u0e43\u0e19\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23\u0e41\u0e25\u0e49\u0e27 \u0e08\u0e36\u0e07\u0e17\u0e33\u0e01\u0e32\u0e23 cleaning \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 module \u0e15\u0e48\u0e2d\u0e44\u0e1b\u0e19\u0e35\u0e49\n\nWhen the files are stored in variables, we performed data cleaning using the following modules.","ffabee84":"-----------------------------------------------------------------------------------------------------------","0709ec57":"# **Data Analyzing**\n\n\u0e17\u0e32\u0e07\u0e01\u0e25\u0e38\u0e48\u0e21\u0e44\u0e14\u0e49\u0e17\u0e33\u0e01\u0e32\u0e23 import module \u0e0a\u0e37\u0e48\u0e2d CountVectorizer \u0e0b\u0e36\u0e48\u0e07\u0e40\u0e1b\u0e47\u0e19\u0e27\u0e34\u0e18\u0e35\u0e17\u0e35\u0e48\u0e07\u0e48\u0e32\u0e22\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14\u0e17\u0e35\u0e48\u0e43\u0e0a\u0e49\u0e43\u0e19\u0e01\u0e32\u0e23\u0e17\u0e33 feature extraction \u0e42\u0e14\u0e22\u0e41\u0e1b\u0e25\u0e07\u0e0a\u0e38\u0e14\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e43\u0e2b\u0e49\u0e01\u0e25\u0e32\u0e22\u0e40\u0e1b\u0e47\u0e19\u0e40\u0e25\u0e02\u0e41\u0e25\u0e49\u0e27\u0e43\u0e2a\u0e48\u0e25\u0e07\u0e43\u0e19 vector \u0e42\u0e14\u0e22\u0e01\u0e32\u0e23\u0e19\u0e31\u0e1a\u0e02\u0e2d\u0e07\u0e04\u0e33\u0e17\u0e35\u0e48\u0e1b\u0e23\u0e32\u0e01\u0e0f\u0e2d\u0e22\u0e39\u0e48\u0e1a\u0e19\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e43\u0e2a\u0e48\u0e25\u0e07\u0e44\u0e1b\u0e43\u0e19\u0e41\u0e15\u0e48\u0e25\u0e30\u0e40\u0e27\u0e01\u0e40\u0e15\u0e2d\u0e23\u0e4c \u0e08\u0e30\u0e41\u0e22\u0e01\u0e15\u0e32\u0e21\u0e15\u0e33\u0e41\u0e2b\u0e19\u0e48\u0e07\u0e02\u0e2d\u0e07\u0e04\u0e33 \u0e0b\u0e36\u0e48\u0e07\u0e27\u0e34\u0e18\u0e35\u0e01\u0e32\u0e23\u0e19\u0e35\u0e49\u0e22\u0e31\u0e07\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e43\u0e2a\u0e48\u0e08\u0e33\u0e19\u0e27\u0e19\u0e04\u0e33\u0e17\u0e35\u0e48\u0e19\u0e31\u0e1a\u0e44\u0e14\u0e49\u0e25\u0e07\u0e44\u0e1b\u0e43\u0e19\u0e40\u0e27\u0e01\u0e40\u0e15\u0e2d\u0e23\u0e4c\u0e42\u0e14\u0e22\u0e15\u0e23\u0e07 \u0e2b\u0e23\u0e37\u0e2d\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e16\u0e48\u0e27\u0e07\u0e19\u0e49\u0e33\u0e2b\u0e19\u0e31\u0e01\u0e42\u0e14\u0e22\u0e08\u0e33\u0e19\u0e27\u0e19\u0e04\u0e33\u0e17\u0e31\u0e49\u0e07\u0e2b\u0e21\u0e14\u0e1a\u0e19\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e17\u0e31\u0e49\u0e07\u0e2b\u0e21\u0e14\u0e01\u0e48\u0e2d\u0e19\u0e43\u0e2a\u0e48\u0e25\u0e07\u0e44\u0e1b\u0e43\u0e19\u0e40\u0e27\u0e01\u0e40\u0e15\u0e2d\u0e23\u0e4c\u0e01\u0e47\u0e44\u0e14\u0e49\n\n\u0e42\u0e14\u0e22\u0e43\u0e19\u0e1f\u0e31\u0e07\u0e01\u0e4c\u0e0a\u0e31\u0e19 CountVectorizer \u0e21\u0e35 parameter \u0e17\u0e35\u0e48\u0e15\u0e49\u0e2d\u0e07\u0e43\u0e0a\u0e49\u0e43\u0e19\u0e01\u0e32\u0e23\u0e27\u0e34\u0e40\u0e04\u0e23\u0e32\u0e30\u0e2b\u0e4c\u0e14\u0e31\u0e07\u0e15\u0e48\u0e2d\u0e44\u0e1b\u0e19\u0e35\u0e49\n\n* token_pattern : \u0e23\u0e39\u0e1b\u0e41\u0e1a\u0e1a\u0e02\u0e2d\u0e07 token \u0e17\u0e35\u0e48\u0e08\u0e30\u0e17\u0e33\u0e01\u0e32\u0e23\u0e27\u0e34\u0e40\u0e04\u0e23\u0e32\u0e30\u0e2b\u0e4c\n* ngram_range : \u0e01\u0e33\u0e2b\u0e19\u0e14\u0e02\u0e2d\u0e1a\u0e40\u0e02\u0e15\u0e01\u0e32\u0e23\u0e40\u0e25\u0e37\u0e2d\u0e01\u0e04\u0e33\u0e17\u0e35\u0e48\u0e15\u0e34\u0e14\u0e01\u0e31\u0e19 n \u0e04\u0e33 \u0e43\u0e19\u0e17\u0e35\u0e48\u0e19\u0e35\u0e49\u0e43\u0e0a\u0e49\u0e17\u0e31\u0e49\u0e07 unigrams \u0e41\u0e25\u0e30 bigrams (\u0e14\u0e39\u0e2b\u0e19\u0e36\u0e48\u0e07\u0e04\u0e33 \u0e41\u0e25\u0e30 \u0e2a\u0e2d\u0e07\u0e04\u0e33\u0e17\u0e35\u0e48\u0e15\u0e34\u0e14\u0e01\u0e31\u0e19) \u0e41\u0e25\u0e30\u0e43\u0e2b\u0e49\u0e04\u0e48\u0e32\u0e04\u0e27\u0e32\u0e21\u0e19\u0e48\u0e32\u0e08\u0e30\u0e40\u0e1b\u0e47\u0e19\u0e02\u0e2d\u0e07\u0e0a\u0e38\u0e14\u0e04\u0e33\u0e17\u0e35\u0e48\u0e40\u0e01\u0e34\u0e14\u0e02\u0e36\u0e49\u0e19\u0e23\u0e48\u0e27\u0e21\u0e01\u0e31\u0e19\n* stop_words : \u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23\u0e43\u0e19\u0e01\u0e32\u0e23\u0e15\u0e31\u0e14\u0e04\u0e33\u0e17\u0e35\u0e48\u0e1e\u0e1a\u0e1a\u0e48\u0e2d\u0e22\u0e43\u0e19\u0e1b\u0e23\u0e30\u0e42\u0e22\u0e04\u0e41\u0e15\u0e48\u0e44\u0e21\u0e48\u0e2a\u0e37\u0e48\u0e2d\u0e16\u0e36\u0e07\u0e2d\u0e30\u0e44\u0e23 \u0e0b\u0e36\u0e48\u0e07\u0e43\u0e19\u0e17\u0e35\u0e48\u0e19\u0e35\u0e49\u0e43\u0e0a\u0e49\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23 stopwords \u0e17\u0e35\u0e48\u0e40\u0e01\u0e47\u0e1a list \u0e02\u0e2d\u0e07\u0e04\u0e33\u0e20\u0e32\u0e29\u0e32\u0e2d\u0e31\u0e07\u0e01\u0e24\u0e29\u0e17\u0e35\u0e48\u0e1e\u0e1a\u0e1a\u0e48\u0e2d\u0e22\n\n\u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e17\u0e33\u0e01\u0e32\u0e23 fit \u0e41\u0e25\u0e30 transform \u0e43\u0e2b\u0e49\u0e01\u0e31\u0e1a\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e43\u0e19 attribute \"text\" \u0e14\u0e31\u0e07\u0e42\u0e04\u0e49\u0e14\u0e02\u0e49\u0e32\u0e07\u0e25\u0e48\u0e32\u0e07\n\nWe imported a module name CountVectorizer that is the simplest way for use to make the feature extraction. This method converts from text to numbers and put into vectors. By counting the words that appear on the text to enter into each vector, separated by word position.  This method can also insert the countable word count directly into the vector, or it can be weighted by the total number of words on all of the text before adding it to the vector.\n\nIn this function has parameters used to analyze, which contains\n\n* token_pattern\n* ngram_range : In this case, we used unigrams and bigrams\n* stop_words : a variable used to wrap words often in sentences, but not to convey anything. In this case, we used a variable names <code>stopwords<\/code> that holds a list of common English words.\n\nThen, fit and transform data in the attribute <code>\"text\"<\/code> as the code below.","2fe6dcc3":"# **Submission data**\n\n\u0e40\u0e21\u0e37\u0e48\u0e2d fit \u0e40\u0e02\u0e49\u0e32\u0e01\u0e31\u0e1a <code>train_vector<\/code> \u0e41\u0e25\u0e49\u0e27\u0e08\u0e36\u0e07\u0e17\u0e33\u0e01\u0e32\u0e23 predict \u0e01\u0e31\u0e1a <code>test_vector<\/code> \u0e40\u0e01\u0e47\u0e1a\u0e04\u0e48\u0e32\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e25\u0e07\u0e43\u0e19\u0e15\u0e32\u0e23\u0e32\u0e07 submission \u0e43\u0e19\u0e04\u0e2d\u0e25\u0e31\u0e21\u0e21\u0e4c target \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e2d\u0e31\u0e1e\u0e42\u0e2b\u0e25\u0e14\u0e40\u0e02\u0e49\u0e32\u0e44\u0e1f\u0e25\u0e4c submission.csv \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e48\u0e07\u0e1c\u0e25\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22\n\nOnce it fits into <code>train_vector<\/code>. we predict to <code>test_vector<\/code>, store the result in the column 'target' of <code>submission<\/code> table. Then, upload it to the file <code>submission.csv<\/code> to submit predictions.","9c8b75d7":"\u0e17\u0e32\u0e07\u0e01\u0e25\u0e38\u0e48\u0e21\u0e44\u0e14\u0e49\u0e2a\u0e33\u0e23\u0e27\u0e08\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e08\u0e32\u0e01 dataset \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 <code>pd.count()<\/code> \u0e43\u0e19\u0e01\u0e32\u0e23\u0e14\u0e39\u0e08\u0e33\u0e19\u0e27\u0e19\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e08\u0e32\u0e01\u0e41\u0e15\u0e48\u0e25\u0e30 Attribute \u0e0b\u0e36\u0e48\u0e07\u0e08\u0e30\u0e40\u0e2b\u0e47\u0e19\u0e27\u0e48\u0e32\u0e43\u0e19\u0e2a\u0e48\u0e27\u0e19\u0e02\u0e2d\u0e07 keyword \u0e41\u0e25\u0e30 location \u0e08\u0e30\u0e21\u0e35\u0e08\u0e33\u0e19\u0e27\u0e19\u0e19\u0e49\u0e2d\u0e22\u0e01\u0e27\u0e48\u0e32 attribute \u0e15\u0e31\u0e27\u0e2d\u0e37\u0e48\u0e19\u0e46 \u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01\u0e21\u0e35\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e1a\u0e32\u0e07\u0e2a\u0e48\u0e27\u0e19\u0e44\u0e21\u0e48\u0e21\u0e35\u0e01\u0e32\u0e23\u0e01\u0e33\u0e2b\u0e19\u0e14\u0e04\u0e48\u0e32\u0e2b\u0e23\u0e37\u0e2d\u0e43\u0e2a\u0e48\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e41\u0e15\u0e48\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e43\u0e14\n\nWe surveyed data from dataset by using <code>pd.count()<\/code> to look at the amount of data from each attribute. Which saw that the keyword and location parts were lower than other attributes because some of the data was not configured, or enter text in any way.","b5df166a":"\u0e0b\u0e36\u0e48\u0e07\u0e08\u0e32\u0e01\u0e27\u0e34\u0e18\u0e35\u0e01\u0e32\u0e23\u0e02\u0e49\u0e32\u0e07\u0e15\u0e49\u0e19\u0e08\u0e30\u0e44\u0e14\u0e49\u0e04\u0e48\u0e32\u0e04\u0e27\u0e32\u0e21\u0e41\u0e21\u0e48\u0e19\u0e22\u0e33\u0e02\u0e2d\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25\u0e02\u0e2d\u0e07\u0e41\u0e15\u0e48\u0e25\u0e30\u0e23\u0e2d\u0e1a\u0e01\u0e32\u0e23\u0e04\u0e33\u0e19\u0e27\u0e13 \u0e0b\u0e36\u0e48\u0e07\u0e08\u0e32\u0e01\u0e1c\u0e25\u0e25\u0e31\u0e1e\u0e18\u0e4c\u0e08\u0e30\u0e40\u0e2b\u0e47\u0e19\u0e27\u0e48\u0e32\u0e43\u0e19\u0e01\u0e32\u0e23 learning \u0e23\u0e2d\u0e1a\u0e17\u0e35\u0e48 5 \u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e43\u0e19\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22\u0e42\u0e21\u0e40\u0e14\u0e25\u0e2a\u0e39\u0e07\u0e01\u0e27\u0e48\u0e32\n\u0e23\u0e2d\u0e1a\u0e2d\u0e37\u0e48\u0e19\u0e46 \u0e41\u0e25\u0e30\u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e2b\u0e32\u0e04\u0e48\u0e32\u0e40\u0e09\u0e25\u0e35\u0e48\u0e22\u0e04\u0e27\u0e32\u0e21\u0e41\u0e21\u0e48\u0e19\u0e22\u0e33\u0e02\u0e2d\u0e07\u0e17\u0e38\u0e01\u0e01\u0e25\u0e38\u0e48\u0e21 (fold) \u0e0b\u0e36\u0e48\u0e07\u0e1c\u0e25\u0e25\u0e31\u0e1e\u0e18\u0e4c\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49 \u0e40\u0e17\u0e48\u0e32\u0e01\u0e31\u0e1a 70.76% \u0e14\u0e31\u0e07\u0e1c\u0e25\u0e02\u0e49\u0e32\u0e07\u0e25\u0e48\u0e32\u0e07 \u0e2d\u0e32\u0e08\u0e40\u0e1b\u0e47\u0e19\u0e44\u0e1b\u0e44\u0e14\u0e49\u0e27\u0e48\u0e32\u0e01\u0e32\u0e23\u0e2a\u0e38\u0e48\u0e21\u0e41\u0e25\u0e30\u0e17\u0e33\u0e19\u0e32\u0e22\u0e04\u0e23\u0e31\u0e49\u0e07\u0e19\u0e35\u0e49\u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e41\u0e21\u0e48\u0e19\u0e22\u0e33\u0e43\u0e19\u0e23\u0e30\u0e14\u0e31\u0e1a\u0e2b\u0e19\u0e36\u0e48\u0e07\n\nUsing the above method, the accuracy of the model for each calculation cycle is obtained. Which from the results, it can be seen that the fifth round of learning has a higher model prediction capability than the others. And calculating the mean of the accuracy of each fold that the result is 70.76%. It is possible that the fifth round of stunts and predictions have some degree of accuracy.","a409dc8d":"\u0e17\u0e32\u0e07\u0e01\u0e25\u0e38\u0e48\u0e21\u0e44\u0e14\u0e49\u0e17\u0e33\u0e01\u0e32\u0e23 import module \u0e2a\u0e33\u0e04\u0e31\u0e0d\u0e2d\u0e22\u0e48\u0e32\u0e07 <code>pandas<\/code> \u0e17\u0e35\u0e48\u0e0a\u0e48\u0e27\u0e22\u0e43\u0e19\u0e01\u0e32\u0e23\u0e08\u0e31\u0e14\u0e01\u0e32\u0e23\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e01\u0e31\u0e1a\u0e15\u0e32\u0e23\u0e32\u0e07 \u0e41\u0e25\u0e30 <code>numpy<\/code> \u0e17\u0e35\u0e48\u0e0a\u0e48\u0e27\u0e22\u0e43\u0e19\u0e01\u0e32\u0e23\u0e08\u0e31\u0e14\u0e01\u0e32\u0e23\u0e04\u0e48\u0e32\u0e15\u0e48\u0e32\u0e07\u0e46 \u0e23\u0e27\u0e21\u0e16\u0e36\u0e07\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e1f\u0e31\u0e07\u0e01\u0e4c\u0e0a\u0e31\u0e19\u0e04\u0e33\u0e19\u0e27\u0e13\u0e15\u0e48\u0e32\u0e07\u0e46 \u0e41\u0e25\u0e30\u0e17\u0e33\u0e01\u0e32\u0e23\u0e40\u0e0a\u0e47\u0e04 dataset \u0e27\u0e48\u0e32\u0e21\u0e35\u0e44\u0e1f\u0e25\u0e4c\u0e2d\u0e30\u0e44\u0e23\u0e1a\u0e49\u0e32\u0e07 \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 module <code>os<\/code> \u0e14\u0e31\u0e07\u0e42\u0e04\u0e49\u0e14\u0e02\u0e49\u0e32\u0e07\u0e25\u0e48\u0e32\u0e07\n\nWe have imported key modules like <code>Pandas<\/code> to help manage tables and <code>Numpy<\/code> to help manage values, including using various computational functions. And checking dataset what files are available, by using module <code>os<\/code> as code below.","f995eea4":"\u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19 \u0e17\u0e33\u0e01\u0e32\u0e23 fit \u0e40\u0e02\u0e49\u0e32\u0e01\u0e31\u0e1a <code>train_vector<\/code> \u0e41\u0e25\u0e30 \u0e04\u0e2d\u0e25\u0e31\u0e21\u0e21\u0e4c target \u0e43\u0e19 <code>train<\/code>\n\nAfter that, fit to <code>train_vector<\/code> and the column 'target' in <code>train<\/code>","ec8e7aba":"\u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e1f\u0e31\u0e07\u0e01\u0e4c\u0e0a\u0e31\u0e19 <code>change_contraction_verb<\/code> \u0e43\u0e19\u0e01\u0e32\u0e23\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19\u0e41\u0e1b\u0e25\u0e07\u0e15\u0e31\u0e27\u0e2d\u0e31\u0e01\u0e02\u0e23\u0e30\u0e2b\u0e23\u0e37\u0e2d\u0e04\u0e33\u0e1a\u0e32\u0e07\u0e04\u0e33\u0e17\u0e35\u0e48\u0e40\u0e1b\u0e47\u0e19\u0e15\u0e31\u0e27\u0e22\u0e48\u0e2d\u0e43\u0e2b\u0e49\u0e01\u0e25\u0e32\u0e22\u0e40\u0e1b\u0e47\u0e19\u0e04\u0e33\u0e23\u0e39\u0e1b\u0e41\u0e1a\u0e1a\u0e40\u0e15\u0e47\u0e21 \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 Regular Expression \u0e43\u0e19\u0e01\u0e32\u0e23\u0e14\u0e33\u0e40\u0e19\u0e34\u0e19\u0e01\u0e32\u0e23\u0e01\u0e31\u0e1a\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e43\u0e19\u0e23\u0e39\u0e1b\u0e41\u0e1a\u0e1a string (\u0e43\u0e19\u0e17\u0e35\u0e48\u0e19\u0e35\u0e49\u0e43\u0e0a\u0e49 <code>re.sub()<\/code> \u0e43\u0e19\u0e01\u0e32\u0e23\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19\u0e04\u0e33)\n\n\u0e40\u0e21\u0e37\u0e48\u0e2d\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e1f\u0e31\u0e07\u0e01\u0e4c\u0e0a\u0e31\u0e19\u0e41\u0e25\u0e49\u0e27 \u0e08\u0e36\u0e07\u0e17\u0e33\u0e01\u0e32\u0e23 apply \u0e01\u0e31\u0e1a\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23 train \u0e41\u0e25\u0e30 test \u0e17\u0e35\u0e48\u0e40\u0e01\u0e47\u0e1a dataset \u0e19\u0e31\u0e49\u0e19 \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 <code>lambda<\/code> \u0e40\u0e02\u0e49\u0e32\u0e21\u0e32\u0e0a\u0e48\u0e27\u0e22\u0e43\u0e19\u0e01\u0e32\u0e23\u0e17\u0e33\u0e1f\u0e31\u0e07\u0e01\u0e4c\u0e0a\u0e31\u0e19\n\nThen, we created a function <code>change_contraction_verb<\/code> to convert some abbreviated characters or words to full words, use Regular Expression to manipulate text in the form of strings (In this case, we use <code>re.sub()<\/code>).\n\nOnce the function is created, we applied to the variables (train and test) that contain the dataset, and used <code>lambda<\/code> to perform the function.","6e648e14":"![1_IjKy-Zc9zVOHFzMw2GXaQw.png](attachment:1_IjKy-Zc9zVOHFzMw2GXaQw.png)","561d8ecc":"![2020-11-24%20%281%29.png](attachment:2020-11-24%20%281%29.png)","a6babae6":"\u0e17\u0e32\u0e07\u0e01\u0e25\u0e38\u0e48\u0e21\u0e44\u0e14\u0e49\u0e28\u0e36\u0e01\u0e29\u0e32 Notebook code \u0e0a\u0e37\u0e48\u0e2d Getting started with NLP-Feature Vectors \u0e02\u0e2d\u0e07 parulpandey \u0e43\u0e19\u0e2a\u0e48\u0e27\u0e19\u0e17\u0e35\u0e48\u0e40\u0e1b\u0e47\u0e19 custom preprocessor \u0e0b\u0e36\u0e48\u0e07\u0e0a\u0e48\u0e27\u0e22\u0e43\u0e19\u0e01\u0e32\u0e23\u0e15\u0e31\u0e14\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e1a\u0e32\u0e07\u0e2a\u0e48\u0e27\u0e19\u0e17\u0e35\u0e48\u0e44\u0e21\u0e48\u0e08\u0e33\u0e40\u0e1b\u0e47\u0e19\u0e2d\u0e2d\u0e01\u0e08\u0e32\u0e01 dataset \u0e17\u0e32\u0e07\u0e01\u0e25\u0e38\u0e48\u0e21\u0e08\u0e36\u0e07\u0e19\u0e33 code \u0e08\u0e32\u0e01\u0e41\u0e2b\u0e25\u0e48\u0e07\u0e17\u0e35\u0e48\u0e21\u0e32\u0e14\u0e31\u0e07\u0e01\u0e25\u0e48\u0e32\u0e27\u0e21\u0e32\u0e43\u0e0a\u0e49\u0e20\u0e32\u0e22\u0e43\u0e15\u0e49\u0e1f\u0e31\u0e07\u0e01\u0e4c\u0e0a\u0e31\u0e19 <code>custom_preprocessor<\/code> \u0e14\u0e31\u0e07\u0e42\u0e04\u0e49\u0e14\u0e02\u0e49\u0e32\u0e07\u0e25\u0e48\u0e32\u0e07\n\nWe studied the Parulpandey's Getting started with NLP-Feature Vectors Notebook code as a custom preprocessor that eliminated some unnecessary text from the dataset.\n\nWe used it under the function <code>custom_preprocessor<\/code> as the code below.","10be5fc1":"\u0e42\u0e14\u0e22\u0e17\u0e32\u0e07\u0e01\u0e25\u0e38\u0e48\u0e21\u0e44\u0e14\u0e49\u0e43\u0e0a\u0e49\u0e2b\u0e25\u0e31\u0e01\u0e01\u0e32\u0e23 K-Fold Cross Validation \u0e0b\u0e36\u0e48\u0e07\u0e40\u0e1b\u0e47\u0e19\u0e01\u0e32\u0e23\u0e41\u0e1a\u0e48\u0e07\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e2d\u0e2d\u0e01\u0e40\u0e1b\u0e47\u0e19 K \u0e0a\u0e38\u0e14\u0e40\u0e17\u0e48\u0e32 \u0e46 \u0e01\u0e31\u0e19 \u0e41\u0e25\u0e30\u0e17\u0e33\u0e01\u0e32\u0e23\u0e04\u0e33\u0e19\u0e27\u0e13\u0e2b\u0e32\u0e04\u0e48\u0e32\u0e04\u0e27\u0e32\u0e21\u0e41\u0e21\u0e48\u0e19\u0e22\u0e33\u0e2b\u0e23\u0e37\u0e2d\u0e04\u0e48\u0e32\u0e04\u0e27\u0e32\u0e21\u0e1c\u0e34\u0e14\u0e1e\u0e25\u0e32\u0e14\u0e08\u0e33\u0e19\u0e27\u0e19 K \u0e23\u0e2d\u0e1a \u0e42\u0e14\u0e22\u0e41\u0e15\u0e48\u0e25\u0e30\u0e23\u0e2d\u0e1a\u0e01\u0e32\u0e23\u0e04\u0e33\u0e19\u0e27\u0e13 \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e0a\u0e38\u0e14\u0e2b\u0e19\u0e36\u0e48\u0e07\u0e08\u0e32\u0e01\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25 K \u0e0a\u0e38\u0e14\u0e08\u0e30\u0e16\u0e39\u0e01\u0e40\u0e25\u0e37\u0e2d\u0e01\u0e2d\u0e2d\u0e01\u0e21\u0e32\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e40\u0e1b\u0e47\u0e19\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e14\u0e2a\u0e2d\u0e1a \u0e41\u0e25\u0e30\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e2d\u0e35\u0e01 K-1 \u0e0a\u0e38\u0e14\u0e08\u0e30\u0e16\u0e39\u0e01\u0e43\u0e0a\u0e49\u0e40\u0e1b\u0e47\u0e19\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e40\u0e23\u0e35\u0e22\u0e19\u0e23\u0e39\u0e49\n\nWe used the K-Fold Cross Validation principle, which divides data into K sets evenly and calculate the accuracy value or the error values of K cycles. For each calculation cycles, One of the K-data sets is selected for testing. And the other K-1 sets used for learning.","befef91b":"\u0e41\u0e25\u0e30\u0e17\u0e32\u0e07\u0e01\u0e25\u0e38\u0e48\u0e21\u0e44\u0e14\u0e49\u0e40\u0e25\u0e37\u0e2d\u0e01\u0e43\u0e0a\u0e49\u0e2b\u0e25\u0e31\u0e01\u0e01\u0e32\u0e23 Logistic Regression \u0e0b\u0e36\u0e48\u0e07\u0e40\u0e1b\u0e47\u0e19\u0e40\u0e17\u0e04\u0e19\u0e34\u0e04\u0e2a\u0e16\u0e34\u0e15\u0e34\u0e40\u0e0a\u0e34\u0e07\u0e04\u0e38\u0e13\u0e20\u0e32\u0e1e \u0e43\u0e0a\u0e49\u0e43\u0e19\u0e01\u0e32\u0e23\u0e2b\u0e32\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e31\u0e21\u0e1e\u0e31\u0e19\u0e18\u0e4c\u0e23\u0e30\u0e2b\u0e27\u0e48\u0e32\u0e07\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23\u0e2d\u0e34\u0e2a\u0e23\u0e30\u0e01\u0e31\u0e1a\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23\u0e15\u0e32\u0e21 \u0e41\u0e25\u0e30\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e42\u0e2d\u0e01\u0e32\u0e2a\u0e17\u0e35\u0e48\u0e08\u0e30\u0e40\u0e01\u0e34\u0e14\u0e40\u0e2b\u0e15\u0e38\u0e01\u0e32\u0e23\u0e13\u0e4c\u0e17\u0e35\u0e48\u0e2a\u0e19\u0e43\u0e08 \u0e43\u0e19\u0e17\u0e35\u0e48\u0e19\u0e35\u0e49\u0e40\u0e1b\u0e47\u0e19\u0e41\u0e1a\u0e1a\u0e17\u0e27\u0e34 (Binary) \u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23\u0e15\u0e32\u0e21 (target) \u0e21\u0e35\u0e40\u0e1e\u0e35\u0e22\u0e07 2 \u0e04\u0e48\u0e32\u0e04\u0e37\u0e2d 0 \u0e41\u0e25\u0e30 1 \u0e42\u0e14\u0e22 logistic regression \u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e04\u0e33\u0e19\u0e27\u0e13\u0e44\u0e14\u0e49\u0e15\u0e32\u0e21\u0e2a\u0e21\u0e01\u0e32\u0e23\u0e02\u0e49\u0e32\u0e07\u0e25\u0e48\u0e32\u0e07\n\nWe used the Logistic Regression principle that is a qualitative statistic technique. It used to find the relationship between independent and dependent variables and predict the likelihood of an event of interest. In this case, it is binary. Since the dependent varaiable (target) has 2 values, 0 and 1. The logistic regression can be calculated according to the equation below.","54fac914":"<center><h1><b>Real Or Not? NLP with disaster tweets<\/b><\/h1><\/center>\n\n<center><h3>DSI206 Multimedia Representation Management<\/h3><\/center>\n\n<center>Data Science and Innovation, College of Interdisciplinary Studies<\/center>\n\n<center>Thammasat University, Thailand<\/center>\n\n<br>\n\n<center><h3>Kids With Code<\/h3><\/center>\n<br><br>\n\n# **Introduction**\n\n\u0e1b\u0e31\u0e08\u0e08\u0e38\u0e1a\u0e31\u0e19 Twitter \u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e33\u0e04\u0e31\u0e0d\u0e43\u0e19\u0e01\u0e32\u0e23\u0e01\u0e23\u0e30\u0e08\u0e32\u0e22\u0e02\u0e48\u0e32\u0e27\u0e2a\u0e32\u0e23\u0e15\u0e48\u0e32\u0e07\u0e46 \u0e43\u0e2b\u0e49\u0e04\u0e19\u0e17\u0e31\u0e48\u0e27\u0e44\u0e1b\u0e44\u0e14\u0e49\u0e23\u0e31\u0e1a\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e31\u0e19\u0e40\u0e27\u0e25\u0e32 \u0e42\u0e14\u0e22\u0e40\u0e09\u0e1e\u0e32\u0e30\u0e40\u0e23\u0e37\u0e48\u0e2d\u0e07\u0e20\u0e31\u0e22\u0e1e\u0e34\u0e1a\u0e31\u0e15\u0e34\u0e18\u0e23\u0e23\u0e21\u0e0a\u0e32\u0e15\u0e34\u0e17\u0e35\u0e48\u0e40\u0e01\u0e34\u0e14\u0e02\u0e36\u0e49\u0e19\u0e1a\u0e48\u0e2d\u0e22\u0e04\u0e23\u0e31\u0e49\u0e07 \u0e41\u0e15\u0e48\u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e14\u0e49\u0e27\u0e22\u0e04\u0e33\u0e1a\u0e32\u0e07\u0e04\u0e33\u0e17\u0e35\u0e48\u0e41\u0e21\u0e49\u0e08\u0e30\u0e40\u0e1b\u0e47\u0e19\u0e04\u0e33\u0e17\u0e35\u0e48\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e02\u0e49\u0e2d\u0e07\u0e01\u0e31\u0e1a\u0e20\u0e31\u0e22\u0e1e\u0e34\u0e1a\u0e31\u0e15\u0e34\u0e41\u0e15\u0e48\u0e2d\u0e32\u0e08\u0e19\u0e33\u0e21\u0e32\u0e43\u0e0a\u0e49\u0e43\u0e19\u0e40\u0e0a\u0e34\u0e07\u0e40\u0e1b\u0e23\u0e35\u0e22\u0e1a\u0e40\u0e17\u0e35\u0e22\u0e1a \u0e0b\u0e36\u0e48\u0e07\u0e17\u0e33\u0e43\u0e2b\u0e49\u0e1a\u0e32\u0e07\u0e42\u0e1e\u0e2a\u0e15\u0e4c\u0e2d\u0e32\u0e08\u0e21\u0e2d\u0e07\u0e27\u0e48\u0e32\u0e40\u0e1b\u0e47\u0e19\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e17\u0e35\u0e48\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e02\u0e49\u0e2d\u0e07\u0e01\u0e31\u0e1a\u0e20\u0e31\u0e22\u0e1e\u0e34\u0e1a\u0e31\u0e15\u0e34 \u0e44\u0e21\u0e48\u0e43\u0e0a\u0e48\u0e2a\u0e34\u0e48\u0e07\u0e17\u0e35\u0e48\u0e21\u0e19\u0e38\u0e29\u0e22\u0e4c\u0e40\u0e2b\u0e47\u0e19\u0e08\u0e32\u0e01\u0e42\u0e1e\u0e2a\u0e15\u0e4c\u0e19\u0e31\u0e49\u0e19 \u0e40\u0e21\u0e37\u0e48\u0e2d\u0e44\u0e14\u0e49\u0e40\u0e2b\u0e47\u0e19\u0e16\u0e36\u0e07\u0e1b\u0e31\u0e0d\u0e2b\u0e32\u0e17\u0e35\u0e48\u0e40\u0e01\u0e34\u0e14\u0e02\u0e36\u0e49\u0e19 \u0e08\u0e36\u0e07\u0e44\u0e14\u0e49\u0e2a\u0e19\u0e43\u0e08\u0e40\u0e02\u0e49\u0e32\u0e23\u0e48\u0e27\u0e21\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19\u0e19\u0e35\u0e49\u0e43\u0e19\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e08\u0e32\u0e01 twitter \u0e27\u0e48\u0e32\u0e40\u0e1b\u0e47\u0e19\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e17\u0e35\u0e48\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e02\u0e49\u0e2d\u0e07\u0e01\u0e31\u0e1a\u0e20\u0e31\u0e22\u0e1e\u0e34\u0e1a\u0e31\u0e15\u0e34\u0e2b\u0e23\u0e37\u0e2d\u0e44\u0e21\u0e48 \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 dataset \u0e08\u0e32\u0e01 kaggle \u0e17\u0e35\u0e48\u0e40\u0e15\u0e23\u0e35\u0e22\u0e21\u0e44\u0e27\u0e49\u0e43\u0e2b\u0e49\n<br><br>\nToday, Twitter is important in the distribution of news. Let the general public receive timely information. Especially about natural disasters that occur frequently. But as some terms, although they are related to disaster, may be used metaphorically. This makes some posts look like disaster-related messages, not what humans saw from that post.\n<br><br>\nWhen seeing the problems that arise. Therefore, I interested in participating in this competition to predict messages from twitter whether they are related to disaster or not, by using the dataset from Kaggle provided.","24b1c9ab":"![s3-news-tmp-140656-s3-news-tmp-980-twitter1_0-2x1-828--2x1--828.jpg](attachment:s3-news-tmp-140656-s3-news-tmp-980-twitter1_0-2x1-828--2x1--828.jpg)"}}