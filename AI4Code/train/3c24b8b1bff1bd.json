{"cell_type":{"6b025e72":"code","f904546d":"code","5685b323":"code","ca149aab":"code","aa976f34":"code","58af35aa":"code","8447eba8":"code","c811bd57":"code","1800836f":"code","56dae898":"code","360dbc9a":"code","fc42e467":"code","9a6e2dc7":"code","d97d88e5":"code","db7a56de":"code","27151616":"code","b999add8":"code","4f4e1039":"code","4ddafb26":"code","0f758331":"code","43becfe2":"code","697a83e7":"code","63dcd2bd":"code","e4e55be5":"code","3de5ba54":"markdown","cc1ab901":"markdown","3d8c1f2f":"markdown","8b883bc8":"markdown","b7b3547d":"markdown","6cce43f0":"markdown","e8dbf092":"markdown","81046fda":"markdown","a92baa28":"markdown","e2cf3ec4":"markdown","e429d23f":"markdown","8ccbaec7":"markdown","cebf1acc":"markdown","cfc74ec4":"markdown","9a679786":"markdown","d413d178":"markdown","2094f397":"markdown","7e833f7e":"markdown","2920c356":"markdown","2981c0ab":"markdown","9e4d62b7":"markdown","29a722dd":"markdown","0fe3eabc":"markdown","2091a9b1":"markdown"},"source":{"6b025e72":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f904546d":"data = pd.read_csv(\"\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\")","5685b323":"data.head()","ca149aab":"data.columns","aa976f34":"data.info()","58af35aa":"data.describe()","8447eba8":"data.corr()","c811bd57":"f,ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\nplt.title(\"Corrolation Map\")\nplt.show()","1800836f":"data[\"class\"].value_counts()","56dae898":"sns.countplot(data[\"class\"], palette=\"Set2\")\nplt.title(\"Amount\")\nplt.show()","360dbc9a":"sns.pairplot(data=data, hue=\"class\")\nplt.show()","fc42e467":"sns.violinplot(x=data[\"class\"], y = data[\"pelvic_incidence\"])\nplt.show()","9a6e2dc7":"data[\"class\"] = [0 if i == \"Abnormal\" else 1 for i in data[\"class\"]]\ny = data[\"class\"].values","d97d88e5":"y","db7a56de":"x_data = data.drop([\"class\"], axis=1)","27151616":"x = (x_data - np.min(x_data))\/(np.max(x_data) - np.min(x_data)).values\nx","b999add8":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=11)","4f4e1039":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 5) #k value\nknn.fit(x_train, y_train)\nprint(\" {} nn score: {} \".format(5,knn.score(x_test,y_test)))","4ddafb26":"score_list = []\nfor i in range(1,20):\n    knn2 = KNeighborsClassifier(n_neighbors = i)\n    knn2.fit(x_train, y_train)\n    score_list.append(knn2.score(x_test, y_test))\nscore_list","0f758331":"current_max = score_list[0]\nfor i in range(len(score_list)):\n    if current_max < score_list[i]:\n        current_max = score_list[i]\ncurrent_max","43becfe2":"plt.plot(range(1,20),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"Accuracy\")\nplt.show()","697a83e7":"knn3 = KNeighborsClassifier(n_neighbors = 8) #k value\nknn3.fit(x_train, y_train)\nprint(\" {} nn score: {} \".format(8,knn3.score(x_test,y_test)))","63dcd2bd":"y_pred_knn = knn3.predict(x_test)\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred_knn)\nprint(\"KNN result confusion matrix : \\n\", cm)","e4e55be5":"#confusion metrics visualization\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm, annot=True, linewidths=0.5, linecolor=\"red\", fmt=\".0f\", ax=ax)\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Real\")\nplt.show()","3de5ba54":"<a id = '5'><\/a><br>\n## KNN with Sklearn and Evaluation","cc1ab901":"#### KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification). KNN works on a principle assuming every data point falling in near to each other is falling in the same class. In other words it classifies a new data point based on similarity.","3d8c1f2f":"#### As you see, the model predicted 56 true out of 62 and 6 wrong (Predicted 2 abnormal as normal, 4 normal as abnormal).","8b883bc8":"#### Columns are non null this and except for class other ones are float","b7b3547d":"<a id = '6'><\/a><br>\n## Conclusion","6cce43f0":"#### After finding the optimum point, lets check confusion matrix.","e8dbf092":"#### We need to make normalization on data because in this data there are some values like 63.02, 114.40 and there are binary values like 0 and 1. This may cause overtower between datas on features. To prevent this, we are doing normalization","81046fda":"#### After convertion, we must seperate them to x and y","a92baa28":"#### The target value is class feature (dependent veriable) but as you see, class feature's columns defined as Abnormal and Normal. Machines can not understand that. So, we need to convert a int or float format","e2cf3ec4":"<a id = '1'><\/a><br>\n## Read Data","e429d23f":"# Content\n1. [Read Data](#1)    \n1. [Visualization](#2)\n1. [Data Preproccesing](#3)\n1. [KNN Model](#4) \n1. [KNN with Sklearn and Evaluation](#5) \n1. [Conclusion](#6) ","8ccbaec7":"#### Obviously, some datas are clearly seperable","cebf1acc":"#### These are the equations for finding distances between data points. Euclidian is the most popular one and sklearn uses it by default in KNN.","cfc74ec4":"<a id = '2'><\/a><br>\n## Visulization","9a679786":"<a id = '3'><\/a><br>\n## Data Preproccesing","d413d178":"#### Now its time to split to train and test datas.","2094f397":"![knn.png](attachment:ca69a810-1823-4677-90e9-8ae28870936e.png)","7e833f7e":"![knn math.png](attachment:f1ced9be-b911-46f3-acf1-d1f977e3ce33.png)","2920c356":"#### In this tutorial, first we read the data and had ideas about it, then visualize it after that we constituted the KNN model, lastly evaluated it with accuracy and confusion matrix.","2981c0ab":"<a id = '4'><\/a><br>\n## KNN model","9e4d62b7":"#### You might ask what is the optimum n_neighbors or k value for KNN :","29a722dd":"#### Finding maximum accuracy and plotting in which values we get it :","0fe3eabc":"#### As an illustration, if K = 5 : The algorithm would calculate the the distance of 5 nearest neigbhors for the data that will be predicted then according to result, for example 4 category A and 1 category B, the classification will be made as category A","2091a9b1":"### X_normalized = (x - x minimum)\/(x maximum - x minimum)\n"}}