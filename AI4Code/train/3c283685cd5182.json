{"cell_type":{"d09b0602":"code","8e71e101":"code","41a8ef91":"code","a08bb831":"code","07e181c0":"code","690341b9":"code","71bea105":"code","10fa9b34":"code","ad2ac9ec":"code","af6d711a":"code","ad2aac98":"code","605a6359":"code","1f94498c":"code","b340685a":"code","94da1ced":"code","2cc63c88":"code","93f2682d":"code","75f209b6":"code","46028980":"code","e3d462fb":"code","a208663d":"code","5f5c43fb":"code","4fbbcd97":"code","8f0c2d86":"code","2e968a41":"code","ee57c0af":"code","5301493c":"code","450035b4":"code","d0eb9ea6":"code","6ba6cf22":"code","c2d8d44a":"code","02d5cb24":"code","80dd3d2b":"code","3f1093ab":"code","a34f2d27":"code","f37494dd":"code","92a3c2a8":"code","2b187ff6":"code","8fe77026":"code","8dab32a7":"code","317f9faf":"code","e597be6f":"code","37f28f63":"code","861a13a6":"code","d0114622":"code","db60499b":"code","614e3147":"code","9cb29321":"code","550c087b":"code","e5c784e7":"code","c79d4052":"code","cdc1aba2":"code","df506d23":"code","f901c3d6":"code","cc93eb4b":"code","5d876eac":"code","a4c06df5":"code","5ec69053":"code","76c6d158":"code","46c4048d":"markdown","4a14f9e0":"markdown","f574d0ad":"markdown","c0e1e192":"markdown","612836ea":"markdown","3ccb103a":"markdown","81e33a4d":"markdown","a4b39184":"markdown"},"source":{"d09b0602":"#1. Choose 3 Features\n#2. Plot them to see how the distribution are with respect to the label (diagnosis)\n#3. Fit a appropriate clustering on the Features\n#4. For each sample replace the 3 feature with the centroid of the cluster the 3 feature belongs to\n#5. Fit a classifier with original features and features modified with k means \n#6. see the performance difference","8e71e101":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\nfrom sklearn import linear_model, tree, ensemble\n\n\n","41a8ef91":"# Read and upload data\ndata = pd.read_csv(\"..\/input\/data.csv\")","a08bb831":"data","07e181c0":"data.head(5)","690341b9":"data.columns","71bea105":"data.info()","10fa9b34":"# We don't need id and NaN data.\ndata.drop([\"Unnamed: 32\", \"id\"], axis = 1, inplace = True)\ndata.head()","ad2ac9ec":"data.info()","af6d711a":"data[\"diagnosis\"].value_counts()\n\n# We have 357 B and 212 M labelled data","ad2aac98":"# For clustering we do not need labels. Because we'll identify the labels.\n\ndataWithoutLabels = data.drop([\"diagnosis\"], axis = 1)\ndataWithoutLabels.head()","605a6359":"dataWithoutLabels.info()","1f94498c":"# radius_mean and texture_mean features will be used for clustering. Before clustering process let's check  how our data looks.\n\nsns.pairplot(data.loc[:,['perimeter_mean','area_mean', 'diagnosis']], hue = \"diagnosis\", height = 5)\nplt.show()","b340685a":"# Our data looks like below plot without diagnosis label\n\nplt.figure(figsize = (10, 10))\nplt.scatter(dataWithoutLabels[\"perimeter_mean\"], dataWithoutLabels[\"area_mean\"])\nplt.xlabel('perimeter_mean')\nplt.ylabel('area_mean')\nplt.show()","94da1ced":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples,silhouette_score\nwcss = [] # within cluster sum of squares\n\nfor k in range(1, 15):\n    kmeansForLoop = KMeans(n_clusters = k)\n    kmeansForLoop.fit(dataWithoutLabels)\n    wcss.append(kmeansForLoop.inertia_)\n\nplt.figure(figsize = (10, 10))\nplt.plot(range(1, 15), wcss)\nplt.xlabel(\"K value\")\nplt.ylabel(\"WCSS\")\nplt.show()","2cc63c88":"#centroids.shape","93f2682d":"%matplotlib inline\nfor i,k in enumerate([2,5,7,10,14]):\n    fig, ax = plt.subplots(1,2,figsize=(15,5))\n    \n    # Run the kmeans algorithm\n    km = KMeans(n_clusters=k)\n    y_predict = km.fit_predict(dataWithoutLabels)\n    centroids  = km.cluster_centers_\n    \n    \n    y_ticks = []\n    y_lower = y_upper = 0\n    silhouette_vals = silhouette_samples(dataWithoutLabels,y_predict)\n    for i,cluster in enumerate(np.unique(y_predict)):\n        cluster_silhouette_vals = silhouette_vals[y_predict ==cluster]\n        cluster_silhouette_vals.sort()\n        y_upper += len(cluster_silhouette_vals)\n\n        ax[0].barh(range(y_lower,y_upper),\n        cluster_silhouette_vals,height =1)   \n        ax[0].text(-0.03,(y_lower+y_upper)\/2,str(i+1))\n        y_lower += len(cluster_silhouette_vals)       \n        # Get the average silhouette score    \n        avg_score = np.mean(silhouette_vals)\n        ax[0].axvline(avg_score,linestyle ='--',linewidth =2,color = 'green')\n        ax[0].set_yticks([])\n        ax[0].set_xlim([-0.1, 1])\n        ax[0].set_xlabel('Silhouette coefficient values')\n        ax[0].set_ylabel('Cluster labels')\n        ax[0].set_title('Silhouette plot for the various clusters');\n\n\n        # scatter plot of data colored with labels\n\n        ax[1].scatter(dataWithoutLabels['perimeter_mean'],dataWithoutLabels['area_mean'] , c = y_predict)    \n        ax[1].scatter(centroids[:,0],centroids[:,1],marker = '*' , c= 'r',s =250);\n        ax[1].set_xlabel('Eruption time in mins')\n        ax[1].set_ylabel('Waiting time to next eruption')\n        ax[1].set_title('Visualization of clustered data', y=1.02)\n\n        plt.suptitle(f' Silhouette analysis using k = {k}',fontsize=16,fontweight = 'semibold')","75f209b6":"from sklearn.metrics import silhouette_samples,silhouette_score\nsilhouette_vals = silhouette_samples(dataWithoutLabels,y_predict)\n    #silhouette_vals# silhouette ploty_ticks = []\n","46028980":"dataWithoutLabels.shape","e3d462fb":"silhouette_vals.shape","a208663d":"dataWithoutLabels.columns","5f5c43fb":"import matplotlib.pyplot as plt\n\n","4fbbcd97":"# Elbow point starting from 2 \n\ndataWithoutLabels = data.loc[:,['perimeter_mean','area_mean']]\nkmeans = KMeans(n_clusters = 2)\nclusters = kmeans.fit_predict(dataWithoutLabels)\ndataWithoutLabels[\"type\"] = clusters\ndataWithoutLabels[\"type\"].unique()","8f0c2d86":"# Plot data after k = 2 clustering\n\nplt.figure(figsize = (15, 10))\nplt.scatter(dataWithoutLabels[\"perimeter_mean\"][dataWithoutLabels[\"type\"] == 0], dataWithoutLabels[\"area_mean\"][dataWithoutLabels[\"type\"] == 0], color = \"red\")\nplt.scatter(dataWithoutLabels[\"perimeter_mean\"][dataWithoutLabels[\"type\"] == 1], dataWithoutLabels[\"area_mean\"][dataWithoutLabels[\"type\"] == 1], color = \"green\")\nplt.xlabel('perimeter_mean')\nplt.ylabel('area_mean')\nplt.show()","2e968a41":"# Data centroids middle of clustered scatters\n\nplt.figure(figsize = (15, 10))\nplt.scatter(dataWithoutLabels[\"perimeter_mean\"], dataWithoutLabels[\"area_mean\"], c = clusters, alpha = 0.5)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], color = \"red\", alpha = 1)\nplt.xlabel('perimeter_mean')\nplt.ylabel('area_mean')\nplt.show()","ee57c0af":"dataWithoutDiagnosis = data.drop([\"diagnosis\"], axis = 1)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nscalar = StandardScaler()\nkmeans = KMeans(n_clusters = 2)\npipe = make_pipeline(scalar, kmeans)\npipe.fit(dataWithoutDiagnosis)\nlabels = pipe.predict(dataWithoutDiagnosis)\ndf = pd.DataFrame({'labels': labels, \"diagnosis\" : data['diagnosis']})\nct = pd.crosstab(df['labels'], df['diagnosis'])\nprint(ct)","5301493c":"#dataWithoutTypes.info()","450035b4":"dataWithoutTypes = dataWithoutLabels.drop([\"type\"], axis = 1)\ndataWithoutTypes.head()","d0eb9ea6":"from scipy.cluster.hierarchy import linkage,dendrogram\nmerg = linkage(dataWithoutTypes, method = \"ward\")\ndendrogram(merg, leaf_rotation = 90)\nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidean distance\")\nplt.show()\n","6ba6cf22":"from sklearn.cluster import AgglomerativeClustering\nhc = AgglomerativeClustering(n_clusters = 2, affinity = \"euclidean\", linkage = \"ward\")\ncluster = hc.fit_predict(dataWithoutTypes)\ndataWithoutTypes[\"label\"] = cluster","c2d8d44a":"dataWithoutTypes.label.value_counts()","02d5cb24":"# Data after hierarchical clustering\n\nplt.figure(figsize = (15, 10))\nplt.scatter(dataWithoutTypes[\"perimeter_mean\"][dataWithoutTypes.label == 0], dataWithoutTypes[\"area_mean\"][dataWithoutTypes.label == 0], color = \"red\")\nplt.scatter(dataWithoutTypes[\"perimeter_mean\"][dataWithoutTypes.label == 1], dataWithoutTypes[\"area_mean\"][dataWithoutTypes.label == 1], color = \"blue\")\nplt.xlabel(\"perimeter_mean\")\nplt.ylabel(\"area_mean\")\nplt.show()","80dd3d2b":"y= df.iloc[:,-1] #class variable\nX = df.iloc[:,:-1]\n","3f1093ab":"# Lets split the data into 5 folds.  \n# We will use this 'kf'(KFold splitting stratergy) object as input to cross_val_score() method\nkf =KFold(n_splits=5, shuffle=True, random_state=42)\n\ncnt = 1\n# split()  method generate indices to split data into training and test set.\nfor train_index, test_index in kf.split(X, y):\n    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n    cnt += 1","a34f2d27":"from sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=0,stratify=y)\n","f37494dd":"print('Total count for each class:\\n', y.value_counts())\nprint(\"\\nCount of each class in train data:\\n\",y_train.value_counts())\nprint(\"\\nCount of each class in test data:\\n\",y_test.value_counts())","92a3c2a8":"# Function to plot ROC curve and classification score which will be used for each model\n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\n\ndef plot_roc(fpr,tpr):\n    plt.plot(fpr, tpr, color='green', label='ROC')\n    plt.plot([0, 1], [0, 1], color='yellow', linestyle='--')\n    plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.legend()\n    plt.show()\n\ndef clf_score(clf):\n    prob = clf.predict_proba(X_test)\n    prob = prob[:, 1]\n    auc = roc_auc_score(y_test, prob)    \n    print('AUC: %.2f' % auc)\n    fpr, tpr, thresholds = roc_curve(y_test,prob, pos_label='Non_Fraudulent')\n    plot_roc(fpr,tpr)\n    predicted=clf.predict(X_test)\n    report = classification_report(y_test, predicted)\n    print(report)\n    return auc","2b187ff6":"# Logistic Regression\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression #import the package\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report","8fe77026":"num_C = [0.001,0.01,0.1,1,10,100] #--> list of values\nfor cv_num in num_C:\n  clf = LogisticRegression(penalty='l2',C=cv_num,random_state = 0)\n  clf.fit(X_train, y_train)\n  print('C:', cv_num)\n  print('Training metric:\\n'+ classification_report(y_train, clf.predict(X_train)))\n  print('Test metric:\\n'+ classification_report(y_test, clf.predict(X_test)))\n  print('')","8dab32a7":"#perform cross validation\n\ngrid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l2\"]}  # l2 ridge\n\nlsr = LogisticRegression()\nclf_lsr_cv = GridSearchCV(lsr,grid,cv=3,scoring='roc_auc')\nclf_lsr_cv.fit(X_train,y_train)\n\nprint(\"tuned hyperparameters :(best parameters) \",clf_lsr_cv.best_params_)\nprint(\"accuracy :\",clf_lsr_cv.best_score_)\n\n#perform hyperparameter tuning\n\nprint('Training metric:\\n'+ classification_report(y_train, clf_lsr_cv.best_estimator_.predict(X_train)))\nprint('Test metric:\\n'+ classification_report(y_test, clf_lsr_cv.best_estimator_.predict(X_test)))\n\n#print the optimum value of hyperparameters","317f9faf":"# Fitting the model with best parameters .\n\nlsr_best = LogisticRegression(penalty='l2',C=0.01,random_state = 0)\nlsr_clf = lsr_best.fit(X_train,y_train)\nclf_score(lsr_clf)","e597be6f":"#K-Nearest Neighbor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection  import cross_val_score\nfrom sklearn.metrics import accuracy_score, mean_squared_error\n\n# Taking only odd integers as K values to apply the majority rule. \nk_range = np.arange(1, 20, 2)\nscores = [] #to store cross val score for each k\nk_range","37f28f63":"# Finding the best k with stratified K-fold method. \n# We will use cv=3 in cross_val_score to specify the number of folds in the (Stratified)KFold.\n\nfor k in k_range:\n  knn_clf = KNeighborsClassifier(n_neighbors=k)\n  knn_clf.fit(X_train,y_train)\n  score = cross_val_score(knn_clf, X_train, y_train, cv=3, n_jobs = -1)\n  scores.append(score.mean())\n\n#Storing the mean squared error to decide optimum k\nmse = [1-x for x in scores]","861a13a6":"print(mse)","d0114622":"#Plotting a line plot to decide optimum value of K\n\nplt.figure(figsize=(20,8))\nplt.subplot(121)\nsns.lineplot(k_range,mse,markers=True,dashes=False)\nplt.xlabel(\"Value of K\")\nplt.ylabel(\"Mean Squared Error\")\nplt.subplot(122)\nsns.lineplot(k_range,scores,markers=True,dashes=False)\nplt.xlabel(\"Value of K\")\nplt.ylabel(\"Cross Validation Accuracy\")\n\nplt.show()","db60499b":"#Fitting the best parameter to the model\n# 3 fold cross validation with K=3\n\nknn = KNeighborsClassifier(n_neighbors=3)\n\nknn_clf = knn.fit(X_train,y_train)","614e3147":"# Checking AUC \n\nclf_score(knn_clf)\n","9cb29321":"from sklearn import tree\nfrom pprint import pprint","550c087b":"# 5 fold cross validation for getting best parameter\n\ndepth_score=[]\ndep_rng = [x for x in range(1,20)]\nfor i in dep_rng:\n  clf = tree.DecisionTreeClassifier(max_depth=i)\n  score_tree = cross_val_score(estimator=clf, X=X_train, y=y_train, cv=5, n_jobs=-1)\n  depth_score.append(score_tree.mean())\nprint(depth_score)","e5c784e7":"#Plotting depth against score\n\nplt.figure(figsize=(8,6))\nsns.lineplot(dep_rng,depth_score,markers=True,dashes=False)\nplt.xlabel(\"Depth\")\nplt.ylabel(\"Cross Validation Accuracy\")\n\nplt.show()","c79d4052":"#Fitting the model with depth=5 and plotting ROC curve\n\ndt = tree.DecisionTreeClassifier(max_depth = 5)\ndt_clf = dt.fit(X_train,y_train)\n\n#Plotting ROC\nclf_score(dt_clf)","cdc1aba2":"#Import libraries\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV","df506d23":"# Using grid search cv to find the best parameters.\n\nparam = {'n_estimators': [50, 60, 30], 'max_depth': [5,4, 3]}\nrfc = RandomForestClassifier()\nclf_rfc_cv = GridSearchCV(rfc, param, cv=5,scoring='roc_auc', n_jobs=-1)\nclf_rfc_cv.fit(X_train,y_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",clf_rfc_cv.best_params_)\nprint(\"accuracy :\",clf_rfc_cv.best_score_)\nprint('Training metric:\\n'+ classification_report(y_train, clf_rfc_cv.best_estimator_.predict(X_train)))\nprint('Test metric:\\n'+ classification_report(y_test, clf_rfc_cv.best_estimator_.predict(X_test)))\n","f901c3d6":"from sklearn.ensemble import RandomForestClassifier\n\n\nrf = RandomForestClassifier(max_depth=5, n_estimators=30)\nRFC_clf = rf.fit(X_train,y_train)\n\n#Plotting ROC\nprint('Training metric:\\n'+ classification_report(y_train, rf.predict(X_train)))\nprint('Test metric:\\n'+ classification_report(y_test, rf.predict(X_test)))\n\n","cc93eb4b":"#Plotting ROC\nclf_score(dt_clf)","5d876eac":"#import libraries\n\nfrom xgboost import XGBClassifier\nfrom scipy import stats","a4c06df5":"# Using grid search cv to find the best parameters.\n\nxgbst = XGBClassifier()\n\nparam_xgb = {'n_estimators': [50,60],\n              'max_depth': [5, 7]\n               \n             } \n\nclf_xgb_cv = GridSearchCV(xgbst, param_xgb, cv=3,scoring='roc_auc', n_jobs=-1)\nclf_xgb_cv.fit(X_train,y_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",clf_xgb_cv.best_params_)\nprint(\"accuracy :\",clf_xgb_cv.best_score_)\n\nprint('Training metric:\\n'+ classification_report(y_train, clf_xgb_cv.best_estimator_.predict(X_train)))\nprint('Test metric:\\n'+ classification_report(y_test, clf_xgb_cv.best_estimator_.predict(X_test)))","5ec69053":"from xgboost import XGBClassifier\n\nxgbst = XGBClassifier(n_estimators=150,max_depth=5,min_child_weight=3)\n\nxgb_clf = xgbst.fit(X_train,y_train)\n\n#Plotting ROC\nprint('Training metric:\\n'+ classification_report(y_train, xgb_clf.predict(X_train)))\nprint('Test metric:\\n'+ classification_report(y_test, xgb_clf.predict(X_test)))\n","76c6d158":"clf = XGBClassifier(n_estimators=150,max_depth=5,min_child_weight=3)  #initialise the model with optimum hyperparameters\nclf.fit(X_train, y_train)\n\n# print the evaluation score on the X_test by choosing the best evaluation metric\nclf_score(clf)","46c4048d":"# K-Means and Hierarchical Clustering Implementation\n\n* Clustering algorithms is being used for unlabelled datasets.\n* This is an implementation example of clustering algorithms.  We'll use K-Means an Hierarchical clustering algorithms for seperate the cancer data by \"radius_mean\" and \"texture_mean\"\n\n## Index of contents\n\n* [DATA EXPLORATION](#1)\n* [K-MEANS CLUSTERING](#2)\n* [HIERARCHICAL CLUSTERING](#3)","4a14f9e0":"I got All the classification  models of same roc and auc score  as 0.89\n","f574d0ad":"# Conclusion","c0e1e192":"<a id=\"1\"><\/a> \n**DATA EXPLORATION**","612836ea":"# KNN","3ccb103a":"# Decision Tree","81e33a4d":"# logistic Regression","a4b39184":"# Random Forest"}}