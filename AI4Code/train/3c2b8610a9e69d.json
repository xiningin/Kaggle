{"cell_type":{"08d88a5a":"code","7a160fb9":"code","32596a97":"code","a3f5034d":"code","4c683faf":"code","dd17cc9c":"code","ad6748b1":"code","1f71e73b":"code","10ca77d7":"code","4680148a":"code","be1a7619":"code","f92c9ae3":"code","5ab277be":"code","1521ab31":"code","133c5301":"markdown","c76fe223":"markdown","4a7a12a7":"markdown","52512a0b":"markdown"},"source":{"08d88a5a":"import datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, train_test_split","7a160fb9":"# Read in data which was created by PreProcessing.py\nX_df = pd.read_csv(\"..\/input\/puntanalytics\/x.csv\")\ny_df = pd.read_csv(\"..\/input\/puntanalytics\/y.csv\", names=[\"target\"])","32596a97":"#define variables for diff in orientation and direction. Using cos function so that diff between 360 degrees and 0 degrees is the same\n\nX_df['o_diff'] = abs(np.cos(np.radians(X_df.o - X_df.o_partner)))\nX_df['dir_diff'] = abs(np.cos(np.radians(X_df.dir - X_df.dir_partner)))\nX_df['self_diff'] = abs(np.cos(np.radians(X_df.o - X_df.dir)))\nX_df['partner_diff'] = abs(np.cos(np.radians(X_df.o_partner - X_df.dir_partner)))","a3f5034d":"# Convert Start_Time variable to a categorical variable that denotes the hour of game start\nX_df['Start_Time'] = pd.to_datetime(X_df['Start_Time'])\nX_df['start_hour'] = X_df.Start_Time.apply(lambda x: x.hour)","4c683faf":"# Coerce boolean to integer to create a dummy variable for high vs. low playing temperature\nX_df[\"high_temp\"] = (X_df[\"Temperature\"] >= X_df[\"Temperature\"].median())\n\n# Coerce boolean to integer to create a dummy variable for mid\/late week\nX_df[\"late_week\"] = (X_df[\"Week\"] >= X_df[\"Week\"].median())\n\n# Coerce boolean to integer to create a dummy variable for middle of field\nmaxY = 53.3\nX_df[\"average_y\"] = X_df[[\"y\",\"y_partner\"]].mean(axis = 1)\nX_df[\"middle_field\"] = (X_df.average_y > maxY * 0.25) & (X_df.average_y < maxY * 0.75)","dd17cc9c":"# Remove unneeded columns\nX_df = X_df.drop([\"Season_Year\",\"GameKey\",\"PlayID\",\"GSISID\",\"Time\",\"Event\",\n                  \"GamePlayKey\",\"GamePlayTimeKey\",\"GSISID_partner\",\"Game_Date\",\n                  \"PlayPlayerPartnerID\", \"Start_Time\",\"Temperature\",\"Week\", \"y\",\n                 \"y_partner\", \"average_y\"], axis=1)","ad6748b1":"# Create boolean vectors to indicate any missing values\nfor column in X_df.columns:\n    if X_df[column].isnull().sum() > 0:\n        col_str = str(column) + \"_isnull\"\n        X_df[col_str] = X_df[column].isnull()","1f71e73b":"# Grouping all non-sunday games together\nX_df.loc[X_df.Game_Day != \"Sunday\", \"Game_Day\"] = \"Not_Sunday\"\n\n# Create dummies for categorical features\nfor column in [\"Season_Type\",\"Game_Day\"]:\n    # Create dummy variable columns, drop one dummy column to avoid multicollinearity, and remove the original, non-binary column\n    X_df = pd.concat([X_df.drop(column, axis=1), pd.get_dummies(X_df[column], drop_first=True)], axis=1)\n    \nX_df = pd.concat([X_df.drop(\"start_hour\", axis=1), pd.get_dummies(X_df[\"start_hour\"], drop_first=True, prefix='hour')], axis=1)","10ca77d7":"#normalizing data from 0 to 1 to see most important components in logistic regression\nX_df = X_df.astype(float)\nX_df = X_df.apply(lambda x: (x - x.min()) \/ (x.max() - x.min()))","4680148a":"# Split into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X_df, y_df.values.ravel(), test_size=0.2, random_state = 53)","be1a7619":"# Instantiate logistic regression object and fit to training data\nlogreg = LogisticRegression(random_state = 53, solver='lbfgs').fit(X_train,y_train)","f92c9ae3":"# Training and Test set accuracy of logistic regression model\ntrainset_acc = logreg.score(X_train,y_train)\ntestset_acc = logreg.score(X_test,y_test)\nprint('logreg training set accuracy: {:.3f}'.format(trainset_acc))\nprint('logreg testing set accuracy: {:.3f}'.format(testset_acc))","5ab277be":"# Five-fold cross-validation accuracy of the logistic regression model on the training set\nscores = cross_val_score(logreg, X_train, y_train, cv=5)\nprint('logreg mean cv accuracy: {:.3f}'.format(np.mean(scores)))","1521ab31":"logRegCoef = sorted(list(zip(X_train.columns, logreg.coef_[0])), key=lambda x: x[1])\n\nfor i in range(len(logRegCoef)):\n    #formatting print for easier reading\n    print(logRegCoef[i][0], \":\", ' '*(25 - len(logRegCoef[i][0]) - int(logRegCoef[i][1] < 0)), \"{:.4f}\".format(logRegCoef[i][1]))","133c5301":"## Section 2: Logistic Regression\nThe data will be put into a trian test split and then tested for accuracy.","c76fe223":"## Section 1: Transforming Data\n\nFirst, features based on differences between columns will be added. Next, logistic variables based on the game data will be created. Finally, dummy variables will be added for the categorical data.","4a7a12a7":"# Classification of Data\n## Tom Bliss and Connor Daly\n\nIn this notebook, we use logistic regression to determine the most important components of the data and which have the greatest effect on whether or not a penalty occurs.","52512a0b":"Coefficients that are relatively large and negative imply that when the data associated with these features have relatively high values, an injury is not likely to occur according to the data. On the otherhand, when coefficients are relatively large and positive they imply that when the data associated with these features have relatively high values, an injury is likely to occur according to the data. When the coefficients are close to 0, that implies that the feature has little affect on the outcome according to the data."}}