{"cell_type":{"0e2d39f4":"code","dbcc47b6":"code","1b6abb6f":"code","6d7d3bbd":"code","cc00879e":"code","dfb558c4":"code","5c9fde60":"code","8ab065ea":"code","e6f03232":"code","aa74da25":"code","c15e9666":"code","b31991a7":"code","2c79bf2a":"code","cf0ba889":"code","8cacd2ae":"code","6c8ad0e7":"code","192c8eb1":"code","1d2b3bdc":"code","39d1fb2b":"code","863b4cb0":"code","e26e0be0":"code","f3cde32e":"code","d3dc8da3":"markdown","153661b2":"markdown","fc1ec6f4":"markdown","2a557adc":"markdown","4222f64f":"markdown","258d0634":"markdown","48135f39":"markdown","6b6b9834":"markdown","ce84573b":"markdown","346863ac":"markdown","9fce172d":"markdown","6d3a9409":"markdown","4d2e0916":"markdown","e8a33e2b":"markdown","2afabe55":"markdown","670b1e89":"markdown","303bbba9":"markdown"},"source":{"0e2d39f4":"from collections import Counter\nimport json\nimport os\nfrom pprint import pprint\nimport string\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom tqdm import tqdm\n\nsns.set()\nsns.set_context('talk')\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","dbcc47b6":"NUM_KLAT_LINES = 5_343_564\nkdwd_path = os.path.join(\"\/kaggle\/input\", \"kensho-derived-wikimedia-data\")\nvocab_path = os.path.join(\"\/kaggle\/input\", \"hugging-face-tokenizer-vocabs\")","1b6abb6f":"!pip install tokenizers\n!pip install transformers","6d7d3bbd":"import tokenizers    # Rust implementations\nimport transformers  # Python implementations","cc00879e":"vocab_file = os.path.join(vocab_path, \"bert-base-uncased-vocab.txt\")\nrust_bert_wp = tokenizers.BertWordPieceTokenizer(vocab_file)\npyth_bert_wp = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\npprint(\"Rust tokenizer class: {}\".format(rust_bert_wp))\nprint()\npprint(\"Python tokenizer class: {}\".format(pyth_bert_wp))","dfb558c4":"encoded = rust_bert_wp.encode(\"Do you feel like I feel?\")\npprint(\"encoded={}\".format(encoded))\npprint(\"encoded.tokens={}\".format(encoded.tokens))","5c9fde60":"tokens = pyth_bert_wp.convert_ids_to_tokens(pyth_bert_wp.encode(\"Do you feel like I feel?\"))\npprint(\"tokens={}\".format(tokens))","8ab065ea":"class KdwdLinkAnnotatedText:\n    def __init__(self, file_path, max_pages):\n        self.num_lines = NUM_KLAT_LINES\n        self.file_path = file_path\n        self.max_pages = max_pages\n        self.pages_to_parse = min(self.num_lines, self.max_pages)\n    def __iter__(self):\n        with open(self.file_path) as fp:\n            for ii_line, line in enumerate(fp):\n                if ii_line == self.pages_to_parse:\n                    break\n                yield json.loads(line)","e6f03232":"NUM_PAGES = 500","aa74da25":"table = str.maketrans('', '', string.punctuation)\ndef simple_tokenizer(text):\n    tokens = [tok.lower().strip() for tok in text.split()]\n    tokens = [tok.translate(table) for tok in tokens]\n    tokens = [tok for tok in tokens if tok != \"\"]\n    return tokens","c15e9666":"file_path = os.path.join(kdwd_path, \"link_annotated_text.jsonl\")\nklat = KdwdLinkAnnotatedText(file_path, max_pages=NUM_PAGES)","b31991a7":"t0 = time.time()\nfor page in tqdm(klat, total=klat.pages_to_parse, desc='just iteration'):\n    for section in page['sections']:\n        first = section['text'][0]\ndt_iter = time.time() - t0\nprint(\"dt: {}\".format(dt_iter))","2c79bf2a":"unigrams_simple = Counter()\nunigrams_hf_rust = Counter()\nunigrams_hf_pyth = Counter()","cf0ba889":"t0 = time.time()\nfor page in tqdm(klat, total=klat.pages_to_parse, desc='simple tokenizer'):\n    for section in page['sections']:\n        tokens = simple_tokenizer(section['text'])\n        unigrams_simple.update(tokens)\ndt_simple = time.time() - t0\nprint(\"dt: {}\".format(dt_simple))","8cacd2ae":"t0 = time.time()\nfor page in tqdm(klat, total=klat.pages_to_parse, desc='hugging face Rust tokenizer'):\n    for section in page['sections']:\n        encoded = rust_bert_wp.encode(section['text'])\n        unigrams_hf_rust.update(encoded.tokens)\ndt_hf_rust = time.time() - t0\nprint(\"dt: {}\".format(dt_hf_rust))","6c8ad0e7":"t0 = time.time()\nfor page in tqdm(klat, total=klat.pages_to_parse, desc='hugging face Python tokenizer'):\n    for section in page['sections']:\n        tokens = pyth_bert_wp.convert_ids_to_tokens(pyth_bert_wp.encode(section['text']))\n        unigrams_hf_pyth.update(tokens)\ndt_hf_pyth = time.time() - t0\nprint(\"dt: {}\".format(dt_hf_pyth))","192c8eb1":"labels = [\"just iteration\", \"simple\", \"hugging rust\", \"hugging python\"]\ntimes = np.array([dt_iter, dt_simple, dt_hf_rust, dt_hf_pyth])\nrates = np.array([\n    sum(unigrams_simple.values()) \/ dt_simple,\n    sum(unigrams_hf_rust.values()) \/ dt_hf_rust,\n    sum(unigrams_hf_pyth.values()) \/ dt_hf_pyth,\n])\nyy = np.arange(len(labels)) \n\nwidth = 0.5\nfigsize = (16, 8)\nfig, axes = plt.subplots(1, 2, figsize=figsize, sharey=True)\n\nax = axes[0]\nrects1 = ax.barh(yy, times, width) \nax.set_yticks(yy)\nax.set_yticklabels(labels)\nax.set_xlabel('seconds')\nax.set_ylabel('Tokenizer')\nax.set_title('Total Parse Time')\n\nax = axes[1]\nrects2 = ax.barh(yy[1:], rates\/1000, width, color=\"orange\") \nax.set_xlabel('Thousands of Tokens \/ s')\nax.set_title('Token Parse Rate')\n\nfig.suptitle('Tokenizer Performance on {} Wikipedia Pages'.format(NUM_PAGES));","1d2b3bdc":"print(\"times: {}\".format(list(zip(labels, times))))\n\n# normalize by Hugging Face Python\nprint(\"times normalized by Hugging Face Python: {}\".format(times\/times[3]))","39d1fb2b":"print(\"rates: {}\".format(list(zip(labels[1:], rates))))\n\n# normalize by Hugging Face Python\nprint(\"rates normalized by Hugging Face Python: {}\".format(rates\/rates[2]))","863b4cb0":"unigrams_simple.most_common(25)","e26e0be0":"unigrams_hf_rust.most_common(25)","f3cde32e":"unigrams_hf_pyth.most_common(25)","d3dc8da3":"# Install the Hugging Face tokenizers and transformers packages","153661b2":"# Hugging Face - Python Tokenizer","fc1ec6f4":"# Results\nNote execution time may vary between runs, but we can get a sense of how large the differences are. Iteration takes a negligible amount of time compared to any of the tokenizers.  In all experiments the simple parser is the fastest but does the least (e.g. no unicode normalization, cant recover original string ...) and  the Hugging Face Rust implementation is more than a factor of 10 faster that the Python implementation. ","2a557adc":"Lets see how long it takes to tokenize some pages from our Wikipedia sample. We'll use both huggingface tokenizers and a simple function that splits on whitespace, lowercases, and removes punctuation. ","4222f64f":"# Hugging Face - Rust Tokenizer","258d0634":"All of the KDWD files have one \"thing\" per line.  We'll hard code the number of lines in the files we're going to use so we can have nice progress bars when streaming through them.","48135f39":"# Example Usage of Tokenizers","6b6b9834":"Lets create some tokenizer classes and see how they work.  We'll use Bert Word Pieces as our benchmark.","ce84573b":"Next we'll count unigrams produced by our 3 tokenizers. ","346863ac":"# Simple Tokenizer","9fce172d":"# Kensho Derived Wikimedia Dataset - Checking out Hugging Face Tokenizers\nHugging Face [recently announced](https:\/\/twitter.com\/huggingface\/status\/1215746098201014272?lang=en) fast [Rust](https:\/\/www.rust-lang.org\/) implementations of its tokenizers. Lets see what kind of performance we can get out of the new [huggingface tokenizers package](https:\/\/github.com\/huggingface\/tokenizers) compared to the tokenizers included in the [huggingface transformers package](https:\/\/github.com\/huggingface\/transformers).","6d3a9409":"# Speed Test \nLets create a class to iterate through the link annotated text of the Kensho Derived Wikimedia Dataset (KDWD). ","4d2e0916":"The `BertTokenizer` class from the [huggingface transformers package](https:\/\/github.com\/huggingface\/transformers) works like this,  ","e8a33e2b":"To begin we'll see how long it take to simply iterate through the pages. ","2afabe55":"# Check Tokens","670b1e89":"The `BertWordPieceTokenizer` class from the [huggingface tokenizers package](https:\/\/github.com\/huggingface\/tokenizers) works like this,  ","303bbba9":"# Plot Results"}}