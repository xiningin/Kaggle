{"cell_type":{"db387488":"code","32feba81":"code","a71f5579":"code","6cf6088f":"code","3cdcddf6":"code","21820c37":"code","f6a4ca05":"code","e8e0036e":"code","56b7618b":"code","5f8b2a3c":"code","e1d34805":"code","650f2a3c":"code","c0d206fe":"code","766d30ca":"code","059c133c":"code","5b4ef0b0":"code","096ce2b7":"code","d7c9b27b":"code","6830cd0d":"code","8f10d010":"code","0511e4d1":"code","c61de649":"code","a2e6e152":"code","ffacba25":"code","6284f133":"code","bd53670e":"markdown","d7510679":"markdown","7b3e13ba":"markdown","22f15338":"markdown","e40cc12f":"markdown","b67f76a7":"markdown","af7dda9a":"markdown","a5f2a28d":"markdown","f216863b":"markdown","c168607c":"markdown","b5906856":"markdown","7d773f8a":"markdown","0fa7748b":"markdown","6bf92521":"markdown","76981d72":"markdown","d240d212":"markdown","2e9dd013":"markdown","7b38d15c":"markdown","6f113281":"markdown","036eac1b":"markdown","2b52020b":"markdown","140dbdc8":"markdown","85817aff":"markdown","66b1dad9":"markdown","f331851f":"markdown","0a418d17":"markdown","54f1b3e0":"markdown"},"source":{"db387488":"# Load libraries\nimport numpy\nfrom numpy import arange\nfrom numpy import set_printoptions\nfrom pandas import read_csv\nfrom pandas import set_option\nfrom pandas.plotting import scatter_matrix\nfrom matplotlib import pyplot\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LassoLars\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.linear_model import ARDRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import OrthogonalMatchingPursuit\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings\nwarnings.filterwarnings('ignore')","32feba81":"# Load dataset\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfilename = '\/kaggle\/input\/auto-insurance-in-sweden-small-dataset\/insurance.csv'\ndata = read_csv(filename, skiprows=list(range(0,5)), header=None, names=['claims','payment'])","a71f5579":"peek = data.head()\nprint(peek)","6cf6088f":"shape = data.shape\nprint(shape)","3cdcddf6":"types = data.dtypes\nprint(types)","21820c37":"# Statistical Summary\nset_option('display.width', 100)\nset_option('precision', 3)\ndescription = data.describe()\nprint(description)","f6a4ca05":"# Pairwise Pearson correlations\ncorrelations = data.corr(method='pearson')\nprint(correlations)","e8e0036e":"# Skew for each attribute\nskew = data.skew()\nprint(skew)","56b7618b":"# Univariate Histograms\ndata.hist()\npyplot.show()","5f8b2a3c":"# Univariate Density Plots\ndata.plot(kind='density', subplots=True, layout=(3,3), sharex=False)\npyplot.show()","e1d34805":"# Box and Whisker Plots\ndata.plot(kind='box', subplots=True, layout=(3,3), sharex=False, sharey=False)\npyplot.show()","650f2a3c":"# Correction Matrix Plot\ncorrelations = data.corr()\n# plot correlation matrix\nfig = pyplot.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(correlations, vmin=-1, vmax=1)\nfig.colorbar(cax)\nticks = numpy.arange(0,9,1)\nax.set_xticklabels(data.columns)\nax.set_yticklabels(data.columns)\npyplot.show()","c0d206fe":"# Scatterplot Matrix\ng = sns.PairGrid(data, diag_sharey=False)\ng.map_upper(sns.scatterplot)\ng.map_lower(sns.kdeplot, colors=\"C0\")\ng.map_diag(sns.kdeplot, lw=2)","766d30ca":"# Split-out validation dataset\narray = data.values\nX = array[:,0:1]\nY = array[:,1]\nvalidation_size = 0.20\nseed = 7\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=seed)","059c133c":"# Test options and evaluation metric\nnum_folds = 10\nseed = 7\nscoring = 'neg_mean_squared_error'","5b4ef0b0":"# Spot-Check Algorithms\nmodels = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('LASSO', Lasso()))\nmodels.append(('EN', ElasticNet()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('SVR', SVR()))","096ce2b7":"# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","d7c9b27b":"models.append(('Ridge', Ridge()))\nmodels.append(('RidgeCV', RidgeCV(alphas=numpy.logspace(-6, 6, 13))))\nmodels.append(('LassoLars', LassoLars(alpha=0.1)))\nmodels.append(('BayesianRidge', BayesianRidge()))\nmodels.append(('ARDRegression', ARDRegression()))\nmodels.append(('OrthogonalMatchingPursuit', OrthogonalMatchingPursuit()))\n\nfor name, model in models:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","6830cd0d":"# Standardize the dataset\npipelines = []\npipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LinearRegression())])))\npipelines.append(('ScaledLASSO', Pipeline([('Scaler', StandardScaler()),('LASSO', Lasso())])))\npipelines.append(('ScaledEN', Pipeline([('Scaler', StandardScaler()),('EN', ElasticNet())])))\npipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())])))\npipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeRegressor())])))\npipelines.append(('ScaledSVR', Pipeline([('Scaler', StandardScaler()),('SVR', SVR())])))\npipelines.append(('ScaledRidge', Pipeline([('Scaler', StandardScaler()),('Ridge', Ridge())])))\npipelines.append(('ScaledRidgeCV', Pipeline([('Scaler', StandardScaler()),('RidgeCV', RidgeCV(alphas=numpy.logspace(-6, 6, 13)))])))\npipelines.append(('ScaledLassoLars', Pipeline([('Scaler', StandardScaler()),('LassoLars', LassoLars(alpha=0.1))])))\npipelines.append(('ScaledBayesianRidge', Pipeline([('Scaler', StandardScaler()),('BayesianRidge', BayesianRidge())])))\npipelines.append(('ScaledARDRegression', Pipeline([('Scaler', StandardScaler()),('ARDRegression', ARDRegression())])))\npipelines.append(('ScaledOrthogonalMatchingPursuit', Pipeline([('Scaler', StandardScaler()),('OrthogonalMatchingPursuit', OrthogonalMatchingPursuit())])))\nresults_standardized = []\nnames_standardized = []\nfor name, model in pipelines:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results_standardized.append(cv_results)\n    names_standardized.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","8f10d010":"# Compare Algorithms\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results) # Baseline\nax.set_xticklabels(names) # Baseline\npyplot.show()","0511e4d1":"# ensembles\nensembles = []\nensembles.append(('ScaledAB', Pipeline([('Scaler', StandardScaler()),('AB', AdaBoostRegressor())])))\nensembles.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingRegressor())])))\nensembles.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()),('RF', RandomForestRegressor())])))\nensembles.append(('ScaledET', Pipeline([('Scaler', StandardScaler()),('ET', ExtraTreesRegressor())])))\nresults_ensembles = []\nnames_ensembles = []\nfor name, model in ensembles:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results_ensembles.append(cv_results)\n    names_ensembles.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","c61de649":"# prepare the model\nmodel = RidgeCV(alphas=numpy.logspace(-6, 6, 13))\nmodel.fit(X_train, Y_train)","a2e6e152":"predictions = model.predict(X_validation)\nprint(mean_squared_error(Y_validation, predictions))","ffacba25":"# prepare the model\nmodel = LinearRegression()\nmodel.fit(X_train, Y_train)\npredictions = model.predict(X_validation)\nprint(mean_squared_error(Y_validation, predictions))","6284f133":"# prepare the model\nmodel = Lasso() # ElasticNet(), KNeighborsRegressor(), DecisionTreeRegressor(), SVR(), Ridge(), RidgeCV(), LassoLars(), BayesianRidge(), ARDRegression(), OrthogonalMatchingPursuit()\nmodel.fit(X_train, Y_train)\npredictions = model.predict(X_validation)\nprint(mean_squared_error(Y_validation, predictions))","bd53670e":"## 2.2 Load Dataset","d7510679":"# 1. Problem Definition\n\nThe Swedish Auto Insurance Dataset involves predicting the total payment for all claims in thousands of Swedish Kronor, given the total number of claims.\n\nIt is a <u>regression<\/u> problem. It is comprised of 63 observations with 1 input variable and one output variable. The variable names are as follows:\n\n- Number of claims.\n- Total payment for all claims in thousands of Swedish Kronor.\n\nWe are  going to cover the following steps:\n1. Problem Definition\n2. Load data\n3. Understand our data with descriptive statistics\n4. Understand our data with visualization\n5. Validation dataset\n6. Evaluate Algorithms: Baseline\n7. Evaluate Algorithms: Standardization\n8. Run Models on Validation dataset\n9. References and Credits\n10. Thoughts\n\n# 2. Load data\n\nLet's start off by loading the libraries required for this project.\n\n## 2.1 Import libraries\n\nFirst, let's import all of the modules, functions and objects we are going to use in this project.","7b3e13ba":"# 9. References and Credits\n- Thank you to Jason Brownlee https:\/\/machinelearningmastery.com\/\n- Thanks to https:\/\/www.kaggle.com\/tanejaa3\/auto-insurance-linear-regression which was referenced for loading the dataset without any issues.\n- For Ridge Regression https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Ridge.html\n- For RidgeCV Regression https:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#ordinary-least-squares\n- https:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#bayesian-regression\n- https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.OrthogonalMatchingPursuit.html#sklearn.linear_model.OrthogonalMatchingPursuit\n- Regarding Mean Squared Error https:\/\/en.wikipedia.org\/wiki\/Mean_squared_error","22f15338":"- On validation data, the MSE is as follows:\n    - LinearRegression (859.209)\n    - Lasso (859.716)\n    - ElasticNet (860.319)\n    - KNeighborsRegressor (1155.381)\n    - DecisionTreeRegressor (1310.374)\n    - SVR (3187.732)\n    - Ridge (859.243)\n    - RidgeCV (862.653)\n    - LassoLars (975.414)\n    - BayesianRidge (863.474)\n    - ARDRegression (863.474)\n    - OrthogonalMatchingPursuit (859.209)\n- LinearRegression (859.209) and OrthogonalMatchingPursuit (859.209) have the least MSE that we have managed to get so far.","e40cc12f":"- We can see the distribution for each attribute is clearer than the histograms.\n\n### 4.1.3 Univariate Plots (Box and Whisker Plots)","b67f76a7":"We can see that the dataset has 63 rows and 2 columns\n\n## 3.3 Data Type for Each Attribute","af7dda9a":"Let's generate predictions on the validation dataset.","a5f2a28d":"What if used any model apart from RidgeCV? Let's try other models as well.","f216863b":"- We can see that claims is of type integer and payment is of type floating point\n\n## 3.4 Descriptive Statistics","c168607c":"## 3.2 Dimensions of our data","b5906856":"- <u>Inference<\/u>: Let's not standardize, because standardizing is leading to poor performance of our best algorithm\n    - RidgeCV (-1476)\n    - StandardizedRidgeCV (-1486)\n    \nLet's take a look at the distribution of the scores across the cross validation folds.","7d773f8a":"# 8. Runs Models on Validation dataset\n\nIn this section, we will evaluate the models on our hold out validation dataset.","0fa7748b":"- The LASSO (-1497), EN (-1497) and LR (-1498) seem to be worth further study.\n- We can see that Linear algorithms (LR, LASSO, EN) are performing better on this data compared to Non-Linear algorithms\n- Why don't we try more regression algorithms such as Ridge Regression?\n\nRidge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients.\n\n- Ridge Regression - Setting the regularization parameter: generalized Cross-Validation\n    - RidgeCV implements ridge regression with built-in cross-validation of the alpha parameter. The object works in the same way as GridSearchCV except that it defaults to Generalized Cross-Validation (GCV), an efficient form of leave-one-out cross-validation.\n    - Specifying the value of the cv attribute will trigger the use of cross-validation with GridSearchCV, for example cv=10 for 10-fold cross-validation, rather than Generalized Cross-Validation","6bf92521":"# 6. Evaluate Algorithms: Baseline\n\nWe have no idea what algorithms will do well on this problem. Gut feel suggests regression algorithms like Linear Regression and ElasticNet may do well. It is also possible that decision trees and even SVM may do well. We have no idea. Let's design our test harness. We will use 10-fold cross validation. The dataset is small and this is a good standard test harness configuration. We will evaluate algorithms using the Mean Squared Error (MSE) metric. MSE will give a gross idea of how wrong all predictions are (0 is perfect).","76981d72":"- claims and payment are highly positively correlated (i.e. 0.913) with each other\n\n## 3.7 Skew of Univariate Distributions","d240d212":"# 3. Understand our data with descriptive statistics\n\nWe are going to cover the following steps:\n\n1. Take a peek at our raw data.\n2. Review the dimensions of our dataset.\n3. Review the data types of attributes in our data.\n4. Summarize the distribution of instances across classes in our dataset. (Since this is not a classification problem, we will skip this step)\n5. Summarize our data using descriptive statistics.\n6. Understand the relationships in our data using correlations.\n7. Review the skew of the distributions of each attribute.\n\n## 3.1 Peek at our data\n\nLet's review the first five rows of the data.","2e9dd013":"Let's create a baseline of performance on this problem and spot-check a number of different algorithms. We will select a suite of different algorithms capable of working on this regression problem. The six algorithms selected include:\n- Linear Algorithms: Linear Regression (LR), Lasso Regression (LASSO) and ElasticNet (EN).\n- Nonlinear Algorithms: Classification and Regression Trees (CART), Support Vector Regression (SVR) and k-Nearest Neighbors (KNN).","7b38d15c":"- It appears that the spread of the attributes is similar\n- claims range between 0 to 100\n- payment ranges between 0 to 400\n\n### 4.2.1 Multivariate Plots (Correlation Matrix Plot)","6f113281":"# 10. Thoughts\n- I tried to rescale, standardize and normalize the training data separately, but the MSE of all the algorithms deteriorated, hence we had to use the raw data\n- I need to figure out a way to tune the hyper-parameters of the linear algorithms using http:\/\/pavelbazin.com\/post\/linear-regression-hyperparameters\/#linear-regression-batch-gradient-descent ","036eac1b":"The algorithms all use default tuning parameters. Let's compare the algorithms. We will display the mean and standard deviation of MSE for each algorithm as we calculate it and collect the results for use later.","2b52020b":"- claims and payment are positively correlated\n- There are <u>no negative correlations<\/u>.\n\n### 4.2.2 Multivariate Plots (Scatter Plot Matrix)","140dbdc8":"- What is happening?\n- On training data, the MSE of RidgeCV was better than LR\n- On validation data, the MSE of LR is better than RidgeCV\n- Why don't we try other algorithms as well on the validation data? Let's do that.","85817aff":"- We can see that RidgeCV (-1476) is the best MSE we have so far. It also has the minimum standard deviation of 695\n\n# 7. Evaluate Algorithms: Standardization\n\nLet's evaluate the same algorithms with a standardized copy of the dataset. This is where the data is transformed such that our attribute has a mean value of zero and a standard deviation of 1. We also need to avoid data leakage when we transform the data. A good way to avoid leakage is to use pipelines that standardize the data and build the model for each fold in the cross validation test harness. That way we can get a fair estimation of how each model with standardized data might perform on unseen data.","66b1dad9":"- Both, claims and payment have an exponential-like distribution\n\n### 4.1.2 Univariate Plots (Density Plots)","f331851f":"- the correlation between claims and payment is also displayed in the scatter matrix (top right) displayed above\n\n# 5. Validation Dataset\n\nIt is a good idea to use a validation hold-out set. This is a sample of the data that we hold back from our analysis and modeling. We use it right at the end of our project to confirm the accuracy of our final model. It is a smoke test that we can use to see if we messed up and to give us confidence on our estimates of accuracy on unseen data. We will use 80% of the dataset for modeling and hold back 20% for validation.","0a418d17":"- There are no missing\/NA values, hence we do not need to handle missing values (i.e data imputation is not required).\n\n## 3.6 Correlations Between Attributes","54f1b3e0":"- The skew result show a positive (right) skew. Values closer to zero show less skew.\n- claims and payment are positively skewed\n\n# 4. Understand our data with visualization\n\nWe are going to cover the following visualizations:\n1. Univariate Plots (Histograms, Density Plots, Box and Whisker Plots)\n2. Multivariate Plots (Correlation Matrix Plot, Scatter Plot Matrix)\n\n### 4.1.1 Univariate Plots (Histograms)"}}