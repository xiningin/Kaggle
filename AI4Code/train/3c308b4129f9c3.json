{"cell_type":{"707fa83f":"code","a396b049":"code","6ce7bc40":"code","aa330044":"code","f237c485":"code","9a19d24b":"markdown","eb8615bb":"markdown","36f8930f":"markdown"},"source":{"707fa83f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev\n\n\n\nimport csv\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport torchvision.transforms as transforms\nimport time\n\nimport torch_xla\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a396b049":"train = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\/train.csv\", dtype = np.float32)\nprint(train.shape)\n\n# splitting data into features and labels\nlabels_numpy = train.label.values\nfeatures_numpy = train.loc[:,train.columns != \"label\"].values\/255 # normalization\n\n# splitting data in train data and validation data 80% - 20%\nfeatures_train, features_valid, labels_train, labels_valid = train_test_split(features_numpy, labels_numpy, test_size = 0.2, random_state = 1)\n\n# converting to tensor\nfeatures_train_tensor = torch.from_numpy(features_train)\nlabels_train_tensor = torch.from_numpy(labels_train).type(torch.LongTensor)\n\nfeatures_valid_tensor = torch.from_numpy(features_valid)\nlabels_valid_tensor = torch.from_numpy(labels_valid).type(torch.LongTensor)\n\n# # data loader\n# train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = False)\n# test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = False)","6ce7bc40":"# defining the model\nclass NeuralNet(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(NeuralNet, self).__init__()\n        self.l1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.l2 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        out = self.l1(x)\n        out = self.relu(out)\n        out = self.l2(out)\n        return out\n    \ninput_size = 28*28 # the size of an image\nhidden_size = 100 # number of neurons in the hidden layer\noutput_size = 10 # number of label classes, from 0 to 9\n\nmodel = NeuralNet(input_size, hidden_size, output_size)","aa330044":"def _run(model):\n    \n    def train_model(train_dataloader, device, optimizer, criterion):\n\n        model.train()\n\n        for i, (images, labels) in enumerate(train_dataloader): # enumerate gives us our actual index\n            # 100, 1, 28, 28 the input we have to resize\n            # 100, 784\n            images = images.reshape(-1, 28*28)\n            images = images.to(device)\n            labels = labels.to(device)\n\n            # forward pass\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n#             xm.master_print(f'loss = {loss.item():4f}')\n\n            # backward pass\n            optimizer.zero_grad() # clear the gradients from the previous iteration\n            loss.backward() # calculates gradients\n            xm.optimizer_step(optimizer) # updates parameters\n\n\n    def valid_model(valid_dataloader, device):\n        \n        # calculate accuracy\n        correct = 0\n        total = 0\n        # Predict test dataset\n        \n        model.eval()\n        \n        for images, labels in valid_dataloader:\n            images = images.reshape(-1, 28 * 28)\n            labels = labels\n\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n\n            # total number of labels\n            total += len(labels)\n            # total number of correct predictions\n            correct += (predicted == labels).sum()\n\n        accuracy = 100 * correct \/ float(total)\n        xm.master_print(f'Accuracy: {accuracy}')\n        \n    # batch size and epoch\n    batch_size = 64\n    num_epochs = 20\n    \n    train_dataset = torch.utils.data.TensorDataset(features_train_tensor, labels_train_tensor)\n    valid_dataset = torch.utils.data.TensorDataset(features_valid_tensor, labels_valid_tensor)\n    \n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n          train_dataset,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=True)\n    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n          valid_dataset,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=False)\n    \n    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=1)\n    valid_dataloader = DataLoader(valid_dataset, batch_size=32, sampler=valid_sampler, num_workers=1)\n    \n    device = xm.xla_device()\n    model = model.to(device)\n    \n    # loss and optimizer\n    learning_rate = 1e-3 * xm.xrt_world_size()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    \n    \n    train_begin = time.time()\n    for epoch in range(num_epochs):\n        \n        para_loader = pl.ParallelLoader(train_dataloader, [device])\n        \n        start = time.time()\n        xm.master_print('*'*15)\n        xm.master_print(f'EPOCH: {epoch+1}')\n        xm.master_print('*'*15)\n\n        xm.master_print('Training.....')\n        \n        train_model(train_dataloader=para_loader.per_device_loader(device),\n                   device=device,\n                   optimizer=optimizer,\n                   criterion=criterion)\n        xm.master_print(f'Epoch completed in {(time.time() - start)\/60} minutes')\n        \n    with torch.no_grad():\n        para_loader = pl.ParallelLoader(valid_dataloader, [device])\n            \n        xm.master_print('Validating...')\n        valid_model(valid_dataloader=para_loader.per_device_loader(device), device=device)\n            \n    xm.master_print(f'Training completed in {(time.time() - train_begin)\/60} minutes') ","f237c485":"# Start training processes\ndef _mp_fn(rank, flags):\n    torch.set_default_tensor_type('torch.FloatTensor')\n    a = _run(model)\n\nFLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')","9a19d24b":"Hello, I made this notebook to learn how to use TPUs.\nI am just a beginner, so if you notice mistakes or things that I can do better and simpler, please tell me.","eb8615bb":"Preparing data","36f8930f":"References:\n1. https:\/\/www.kaggle.com\/abhiswain\/pytorch-tpu-efficientnet-b5-tutorial-reference"}}