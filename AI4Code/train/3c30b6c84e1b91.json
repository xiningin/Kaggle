{"cell_type":{"57d32ecb":"code","b4af64cc":"code","f1fee3a1":"code","0bc04c18":"code","537c0286":"code","287f07b7":"code","2d97ae58":"code","a3385295":"code","3b51ebe4":"code","30fbe774":"code","83cc9887":"code","123c91b1":"code","e4007edd":"code","fb5cf159":"code","839b8004":"code","e8e0727d":"code","875fba7b":"code","a21ba4fe":"code","5aacf2a7":"code","76eda099":"code","ae47fb81":"code","983651ba":"code","2395131c":"code","be512451":"code","95871038":"code","12408047":"code","14924004":"code","75cd46f2":"code","361abc63":"code","bcff2331":"code","ce12b721":"code","f98bfdf0":"code","31ca8e40":"code","fda4cbdd":"code","8726c322":"code","ee067154":"code","0cbb5ec1":"code","43c66b45":"code","944b833f":"code","a750001b":"code","b487c971":"code","f188b480":"code","e38ac75f":"code","ee25ebb3":"code","ee10c3c8":"code","4072f809":"code","5c672d8a":"code","73fac22a":"code","c4296096":"code","2d101967":"code","4105bab2":"code","bc36223c":"code","cf4f74d0":"code","2b36dc70":"code","82908239":"code","bada1f99":"code","a56e1127":"code","61751e8f":"code","7b660d41":"code","16031143":"code","0a0fe442":"code","561dbdbf":"markdown","8c662846":"markdown"},"source":{"57d32ecb":"#we shall import dependencies\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib\nmatplotlib.rcParams['figure.figsize']=(20,10)","b4af64cc":"#importing the dataset\ndf1=pd.read_csv('\/kaggle\/input\/bengaluru-house-price-data\/Bengaluru_House_Data.csv')\ndf1.head()                ","f1fee3a1":"#we will check what is in dataset\ndf1.shape","0bc04c18":"#now we will group the dataset by area type\ndf1.groupby('area_type')['area_type'].agg('count')","537c0286":"# we will drop certain column from the datafreme, like availability, society,balcony,availability \n#as they are not important in predicting the price\ndf2=df1.drop(['area_type','society','balcony','availability'],axis='columns')\ndf2.head()","287f07b7":"#now we will find the number of null values in the dataframe\ndf2.isnull().sum()","2d97ae58":"#now we will drop the null values in the dataframe as they are very less\ndf3=df2.dropna()\ndf3.isnull().sum()","a3385295":"df3.shape","3b51ebe4":"#the size colums has both categorical as well as numerical data, we will see unique values\ndf3['size'].unique()","30fbe774":"#now we will create a separate column BHK containing number of rooms\ndf3['bhk']=df3['size'].apply(lambda x:int(x.split(' ')[0]))","83cc9887":"df3.head()","123c91b1":"\ndf3['bhk'].unique()\n#here rooms like 27,43 and others seems astronomical","e4007edd":"#now we will find the outliers\ndf3[df3.bhk>20]\n#the outplut seems like an error","fb5cf159":"#now we will find the unique values in the total_sqft column\ndf3.total_sqft.unique()\n\n#the total_sqft has range values, this is a hindrance, we will try to remove that in future codes","839b8004":"def is_float(x):\n    try:\n        float(x)\n    except:\n        return False\n    return True","e8e0727d":"#now we will see slice the total_sqft column based on the float\ndf3[~df3['total_sqft'].apply(is_float)].head(10)","875fba7b":"#now we will convert float into range by averaging the sqft ranges\n","a21ba4fe":"def convert_sqft_to_num(x):\n    tokens=x.split('-')\n    if len(tokens)==2:\n        return(float(tokens[0])+float(tokens[1]))\/2\n    try:\n        return float(x)\n    except:\n        return None\n    ","5aacf2a7":"#now we will taste the function\nconvert_sqft_to_num('2166')","76eda099":"convert_sqft_to_num('2100-3000')\n#here the output is average of the range","ae47fb81":"#now we will apply this function to the sqft column and creat a new dataframe\ndf4=df3.copy()\ndf4['total_sqft']=df4['total_sqft'].apply(convert_sqft_to_num)\ndf4.head()","983651ba":"#now we will start with feature engineering techniques and dimensionality reduction techniques\ndf5=df4.copy()\n#now we will create price per sqft\ndf5['price_per_sqft']=df5['price']*100000\/df5['total_sqft']\ndf5.head()","2395131c":"#now we will explore location column\nlen(df5.location.unique())","be512451":"# there are 1304 features, means this is too much feature\n#this is called dimensionality curse\n#we shall reduce this dimensionality as low as possible\ndf5.location=df5.location.apply(lambda x:x.strip())\nlocation_stats=df5.groupby('location')['location'].agg('count').sort_values(ascending=False)\nlocation_stats","95871038":"#now we will find the location with less than 10 datapoints\nlen(location_stats[location_stats<=10])","12408047":"location_stats_less_than_10=location_stats[location_stats<=10]\nlocation_stats_less_than_10","14924004":"len(df5.location.unique())","75cd46f2":"df5.location=df5.location.apply(lambda x: 'other'if x in location_stats_less_than_10 else x )\nlen(df5.location.unique())","361abc63":"#all the locations with less than 10 properties are converted to other location \ndf5.head(10)","bcff2331":"#now we will start outlier detections and removal\ndf5[df5.total_sqft\/df5.bhk<300].head()","ce12b721":"df5.shape","f98bfdf0":"#now we will negate the criteria from above dataframe\ndf6=df5[~(df5.total_sqft\/df5.bhk<300)]\ndf6.shape","31ca8e40":"#now we will chaek the outliers in price per sqft\ndf6.price_per_sqft.describe()","fda4cbdd":"# we will filter the data beyond (mean +1std deviation).\ndef remove_pps_outliers(df):\n    df_out=pd.DataFrame()\n    for key,subdf in df.groupby('location'):\n        m=np.mean(subdf.price_per_sqft)\n        st=np.std(subdf.price_per_sqft)\n        reduced_df=subdf[(subdf.price_per_sqft>(m-st))&(subdf.price_per_sqft<=(m+st))]\n        df_out=pd.concat([df_out,reduced_df],ignore_index=True)\n    return df_out\ndf7=remove_pps_outliers(df6)\ndf7.shape","8726c322":"def plot_scatter_chart(df,location):\n    bhk2=df[(df.location==location)&(df.bhk==2)]\n    bhk3=df[(df.location==location)&(df.bhk==3)]\n    matplotlib.rcParams['figure.figsize']=(15,10)\n    plt.scatter(bhk2.total_sqft,bhk2.price,color='blue',label='2 BHK',s=50)\n    plt.scatter(bhk3.total_sqft,bhk3.price,marker='+',color='green',label='3 BHK',s=50)\n    plt.xlabel('total square feet area')\n    plt.ylabel('price')\n    plt.title(location)\n    plt.legend()\n    \nplot_scatter_chart(df7,'Hebbal')","ee067154":"def remove_bhk_outliers(df):\n    exclude_indices = np.array([])\n    for location, location_df in df.groupby('location'):\n        bhk_stats = {}\n        for bhk, bhk_df in location_df.groupby('bhk'):\n            bhk_stats[bhk] = {\n                'mean': np.mean(bhk_df.price_per_sqft),\n                'std': np.std(bhk_df.price_per_sqft),\n                'count': bhk_df.shape[0]\n            }\n        for bhk, bhk_df in location_df.groupby('bhk'):\n            stats = bhk_stats.get(bhk-1)\n            if stats and stats['count']>5:\n                exclude_indices = np.append(exclude_indices, bhk_df[bhk_df.price_per_sqft<(stats['mean'])].index.values)\n    return df.drop(exclude_indices,axis='index')\ndf8 = remove_bhk_outliers(df7)\n# df8 = df7.copy()\ndf8.shape","0cbb5ec1":"plot_scatter_chart(df8,'Hebbal')","43c66b45":"#now we will pot a histogram with price per squarefeet\nimport matplotlib\nmatplotlib.rcParams['figure.figsize']=(20,10)\nplt.hist(df8.price_per_sqft,rwidth=0.8)\nplt.xlabel('price per squarefoot')\nplt.ylabel('count')\n","944b833f":"#now we will explore bathroom feature\ndf8.bath.unique()","a750001b":"df8[df8.bath>10]","b487c971":"plt.hist(df8.bath,rwidth=0.8)\nplt.xlabel('number of bathrooms')\nplt.ylabel('count')","f188b480":"#now we will remove any time we have bathrooms greater than number of bedrooms +2 is an outlier\ndf8[df8.bath>df8.bhk+2]\n","e38ac75f":"df9=df8[df8.bath<df8.bhk+2]\ndf9.shape","ee25ebb3":"#now we will prepare our dataframe for ML training\n#we shall remove price per squarefoot and size\ndf10=df9.drop(['size','price_per_sqft'],axis='columns')\ndf10.head(3)","ee10c3c8":"#we will perform one hot encoding for location column\ndummies=pd.get_dummies(df10.location)\ndummies.head()","4072f809":"df11=pd.concat([df10,dummies],axis='columns')","5c672d8a":"#to avoid dummy trap, we will drop last column\ndf11=df11.drop(['other'],axis='columns')","73fac22a":"df11.head()","c4296096":"df12=df11.drop('location',axis='columns')\ndf12.head(2)","2d101967":"df12.shape","4105bab2":"#will define dependent variable for training\nX=df12.drop('price',axis='columns')\nX.head()","bc36223c":"#will define independent variable for training\ny=df12.price\ny.head()","cf4f74d0":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=10)\n","2b36dc70":"from sklearn.linear_model import LinearRegression\nlr_clf=LinearRegression()\nlr_clf.fit(x_train,y_train)\nlr_clf.score(x_test,y_test)","82908239":"#now we will perform k fold cross validation\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import cross_val_score\ncv=ShuffleSplit(n_splits=5,test_size=0.2,random_state=0)\ncross_val_score(LinearRegression(),X,y,cv=cv)","bada1f99":"#now we will perform grid search cv, here we will try multiple model and select best one with good score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef find_best_model_using_gridsearchcv(X,y):\n    algos = {\n        'linear_regression' : {\n            'model': LinearRegression(),\n            'params': {\n                'normalize': [True, False]\n            }\n        },\n        'lasso': {\n            'model': Lasso(),\n            'params': {\n                'alpha': [1,2],\n                'selection': ['random', 'cyclic']\n            }\n        },\n        'decision_tree': {\n            'model': DecisionTreeRegressor(),\n            'params': {\n                'criterion' : ['mse','friedman_mse'],\n                'splitter': ['best','random']\n            }\n        }\n    }\n    scores = []\n    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n    for algo_name, config in algos.items():\n        gs =  GridSearchCV(config['model'], config['params'], cv=cv, return_train_score=False)\n        gs.fit(X,y)\n        scores.append({\n            'model': algo_name,\n            'best_score': gs.best_score_,\n            'best_params': gs.best_params_\n        })\n\n    return pd.DataFrame(scores,columns=['model','best_score','best_params'])\n\nfind_best_model_using_gridsearchcv(X,y)\n    \n","a56e1127":"X.columns","61751e8f":"np.where(X.columns=='2nd Phase Judicial Layout')[0][0]","7b660d41":"#here the winner is Linear Regression\n#Now we will use linear regression for property price prediction\ndef predict_price(location,sqft,bath,bhk):\n    loc_index=np.where(X.columns==location)[0][0]\n    x=np.zeros(len(X.columns))\n    x[0]=sqft\n    x[1]=bath\n    x[2]=bhk\n    if loc_index >=0:\n        x[loc_index]=1\n        \n    return lr_clf.predict([x])[0]","16031143":"predict_price('1st Phase JP Nagar',1000,2,2)","0a0fe442":"predict_price('1st Phase JP Nagar',1000,3,3)","561dbdbf":"# Now you can tryout with your imagination","8c662846":"# Predictions"}}