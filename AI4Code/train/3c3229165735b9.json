{"cell_type":{"d472b60e":"code","9ad6ce05":"code","a75b7633":"code","44dff82b":"code","7eb7742b":"code","d2e910f8":"code","87cfe5f9":"code","c48fe68c":"code","e8d33581":"code","9e1e5864":"code","596ad066":"code","16c2552c":"code","550fe2fe":"code","149f2e74":"code","d5a483a1":"code","c3401932":"code","36de3337":"code","4a7db6f6":"code","8d1462d2":"markdown","8b389a57":"markdown"},"source":{"d472b60e":"import numpy as np\nimport pandas as pd\nimport librosa \nimport librosa.display\nimport matplotlib.pyplot as plt\nimport IPython.display as ipd\nimport os\nimport pywt\nfrom tqdm.notebook import tqdm\nfrom tqdm._tqdm_notebook import tqdm_notebook\ntqdm_notebook.pandas(desc=\"Processing:\")\nimport soundfile as sf\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nimport scipy.stats\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import LSTM, Dense, RepeatVector, Dropout, TimeDistributed\n#\u30c7\u30fc\u30bf\u30d1\u30b9\u306e\u6307\u5b9a\u3092\u3057\u307e\u3059\nINPUT_DIR = '\/kaggle\/input\/hah-data-science-challenge'","9ad6ce05":"def meta_define():\n    \"\"\"\n    input : none\n    output : Corrected metadata\n    \"\"\"\n    import pandas as pd\n    import os \n    os.chdir(\"\/kaggle\/input\/hah-data-science-challenge\/\")\n    df_train = pd.read_csv(\"train.csv\", index_col=False)\n    df_test = pd.read_csv(\"test.csv\", index_col=False)\n    \n    ##################################################\n    #\u4ee5\u4e0b\u8f9e\u66f8\u3084\u5909\u6570\u306e\u5b9a\u7fa9\n    #\u5404\u30c7\u30fc\u30bf\u4fee\u6b63\u7528\u306e\u8f9e\u66f8\n    bolt_dict = {\n        '\u5927':\"big\",\n        '\u5c0f':\"small\"\n    }\n\n    plate_dict = {\n        '\u5927':\"big\",\n        '\u5c0f':\"small\"\n    }\n\n    record_dict = {\n        'PC\u5185\u81d3':\"pc_built_in\",\n        'PC\u5185\u8535':\"pc_built_in\",\n        'USB1':\"usb1\", \n        'USB2':\"usb2\", \n        'USB3':\"usb3\", \n        'USB4':\"usb4\", \n        '\u30b9\u30de\u30db':\"smart_phone\",\n        '\u30b9\u30de\u30db\u306e\u30dc\u30a4\u30b9\u30ec\u30b3\u30fc\u30c0':\"smart_phone\",\n        '\u5185\u8535\u30de\u30a4\u30af':\"pc_built_in\",\n        }\n\n    distance_dict = {\n        '10cm': 0.1, \n        '10\u339d': 0.1, \n        '1M': 1.0, \n        '20cm': 0.2, \n        '20\u339d': 0.2, \n        '2M': 2.0, \n        '2m': 2.0, \n        '30cm': 0.3, \n        '30cn': 0.3, \n        '30\u339d': 0.3, \n        '3m': 3.0, \n        '40cm': 0.4, \n        '40\u339d': 0.4, \n        '50cm': 0.5, \n        '50\u339d': 0.5, \n        '5cm': 0.05,\n        '8cm': 0.08, \n        '\uff11\uff2d': 1.0   \n    }\n\n    cvt_dict = {\n        \"\u306d\u3058\" : bolt_dict, \n        '\u30d7\u30ec\u30fc\u30c8' : plate_dict, \n        '\u9332\u97f3\u65b9\u6cd5' : record_dict, \n        '\u30de\u30a4\u30af\u8ddd\u96e2' : distance_dict\n    }\n    \n    #df_train\u65e5\u672c\u8a9e\u30ab\u30e9\u30e0\u540d : ['ID', '\u306d\u3058', '\u30d7\u30ec\u30fc\u30c8', '\u9332\u97f3\u65b9\u6cd5', '\u30de\u30a4\u30af\u8ddd\u96e2', '\u30d5\u30a1\u30a4\u30eb', 'Target']\n    col_train = ['id', 'bolt', 'plate', 'record', 'mic_dist', 'file', 'target']\n    #df_test\u65e5\u672c\u8a9e\u30ab\u30e9\u30e0\u540d : ['ID', '\u306d\u3058', '\u30d7\u30ec\u30fc\u30c8', '\u9332\u97f3\u65b9\u6cd5', '\u30de\u30a4\u30af\u8ddd\u96e2', '\u30d5\u30a1\u30a4\u30eb', 'Target']\n    col_test = ['id', 'bolt', 'plate', 'record', 'mic_dist', 'file']\n    \n    tgt_col = [\"\u306d\u3058\", '\u30d7\u30ec\u30fc\u30c8', '\u9332\u97f3\u65b9\u6cd5', '\u30de\u30a4\u30af\u8ddd\u96e2']\n    ##################################################\n    \n    for col in tgt_col:#Target\u306f\u5909\u63db\u5bfe\u8c61\u5916\n        df_train[col] = df_train[col].map(cvt_dict[col])\n        df_test[col] = df_test[col].map(cvt_dict[col])\n        \n    df_train.columns = col_train\n    df_test.columns = col_test\n    \n    return df_train, df_test","a75b7633":"#read data\ndf_train, df_test = meta_define()","44dff82b":"# meta_data\u306b\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u3092\u8ffd\u52a0\ndf_train['filepath'] = INPUT_DIR + '\/train\/train\/' + df_train['file']\ndf_test['filepath'] = INPUT_DIR + '\/test\/test\/' + df_test['file']\n\n# train\u3068test\u3092\u3072\u3068\u3064\u306b\ndf = pd.concat([df_train, df_test]).reset_index(drop=True)\n# \u4e00\u6253\u97f31\u30ec\u30b3\u30fc\u30c9\u306e\u30e1\u30bf\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\u3059\u308b\nresults = []\nfilelist = df['filepath'].to_list()\n\nfor i in tqdm(range(len(df))):\n    filepath = df.loc[i, 'filepath']\n    y, sr = librosa.load(filepath)\n    onset_envelope = librosa.onset.onset_strength(y, sr)\n    onset_times = librosa.onset.onset_detect(y=y, sr=sr, onset_envelope=onset_envelope, units='time')\n    \n    for j, onset_time in enumerate(onset_times):\n        tmp = pd.DataFrame([df.loc[i]])\n        tmp['split_id'] = j\n        tmp['onset_time'] = onset_time\n        tmp['play_time'] = y.size \/ sr\n        results.append(tmp)\ndf_split = pd.concat(results)","7eb7742b":"# onset_detect\u3067\u691c\u51fa\u3055\u308c\u305f\u6642\u9593\u3092\u5143\u306b\u5207\u308a\u51fa\u3059\u6642\u9593\u3092\u8a2d\u5b9a\n# onset_detect\u306f\u3001\u59cb\u70b9\u304c\u82e5\u5e72\u9045\u308c\u3066\u3044\u308b\u306e\u3067\u30de\u30a4\u30ca\u30b9\u306e\u30aa\u30d5\u30bb\u30c3\u30c8\u3092\u4ed8\u4e0e\n# \u30aa\u30d5\u30bb\u30c3\u30c8\u639b\u3051\u305f\u59cb\u70b9\u304b\u3089\u56fa\u5b9a\u6642\u9593\u5206\u3092\u8db3\u3057\u305f\u5024\u3092\u7d42\u70b9\u3068\u3059\u308b\n\nst_offset = 0.05 # \u59cb\u70b9\u306e\u30aa\u30d5\u30bb\u30c3\u30c8\nduration = 0.2 # \u5207\u308a\u51fa\u3059\u6642\u9593\ndf_split['st_time'] = df_split['onset_time'] - st_offset\ndf_split['ed_time'] = df_split['st_time'] + duration\ndf_split['duration'] = duration\n\n# \u7d42\u70b9\u3067\u56fa\u5b9a\u6642\u9593\u306b\u6e80\u305f\u306a\u3044\u5834\u5408\u306f\u9664\u5916\ndf_split = df_split[df_split['ed_time'] <= df_split['play_time']].reset_index(drop=True)","d2e910f8":"# \u5207\u308a\u51fa\u3057\u305f\u6253\u97f3\u3054\u3068\u306esignal\u306e\u6700\u5927\u5024\u3092\u30e1\u30bf\u30c7\u30fc\u30bf\u306b\u8ffd\u52a0\u3059\u308b\nfor i in tqdm(range(len(df_split))):\n    filepath = df_split.loc[i, 'filepath']\n    st_time = df_split.loc[i, 'st_time']\n    duration = df_split.loc[i, 'duration']\n    \n    # load\u3059\u308b\u3068\u304d\u306boffset\u3067\u59cb\u70b9\u3001duration\u3067\u671f\u9593\u3092\u6307\u5b9a\u3059\u308b\n    y, sr = librosa.load(filepath, offset=st_time, duration=duration)\n    df_split.loc[i, 'signal_max'] = max(abs(y))","87cfe5f9":"df = df_split.copy()","c48fe68c":"# \u97f3\u306e\u5927\u304d\u3055\u304c\u4e00\u5b9a\u4ee5\u4e0b\u306e\u30c7\u30fc\u30bf\u306f\u7121\u97f3\u3068\u3057\u3066\u3001\u5bfe\u8c61\u304b\u3089\u9664\u5916\nth = 0.5\ndf = df[df['signal_max']>0.5].reset_index(drop=True)","e8d33581":"# librosa\u3067.wav\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u7279\u5fb4\u62bd\u51fa\u3057\u307e\u3059\u3002\nfeatures=[]\nfor i in tqdm(range(len(df))):\n    filepath = df.loc[i, 'filepath']\n    st_time = df.loc[i, 'st_time']\n    duration = df.loc[i, 'duration']\n    \n    y, sr = librosa.load(filepath, offset=st_time, duration=duration)\n    #\u5916\u308c\u5024\u3092\u8003\u616e\u3057\u3066\n    y=scipy.stats.zscore(y)\n    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n    features.append(mfcc)\n","9e1e5864":"features=np.array(features)","596ad066":"norm_idx = list(df[df['target']==0].index)\nanorm_idx = list(df[df['target']==1].index)\n\ntr_idx = norm_idx[:int(len(norm_idx)\/2)]\nval_idx = norm_idx[int(len(norm_idx)\/2):] + anorm_idx","16c2552c":"tr = features[tr_idx]\nval = features[val_idx]","550fe2fe":"n_steps = tr.shape[1]\nn_features = tr.shape[2]\n\nLSTM_units = 64\nmodel = keras.Sequential()\nmodel.add(LSTM(LSTM_units, input_shape=(n_steps,n_features), return_sequences=False,name='encoder_lstm'\n              ))\nmodel.add(Dropout(0.2, name='encoder_dropout'))\nmodel.add(RepeatVector(n_steps, name='decoder_repeater'))\nmodel.add(LSTM(LSTM_units, return_sequences=True, name='decoder_lstm'))\nmodel.add(Dropout(rate=0.2, name='decoder_dropout'))\nmodel.add(TimeDistributed(Dense(n_features,name='decoder_dense_output')))\n\nmodel.compile(loss='mae', optimizer='adam')\nmodel.summary()","149f2e74":"%time history = model.fit(tr, tr, epochs=12000, batch_size=200, validation_split=0.1, shuffle=False)","d5a483a1":"plt.plot(history.history['loss'], label='training_loss')\nplt.plot(history.history['val_loss'], label='validation_loss')\nplt.legend()\nplt.show()","c3401932":"df_valid = df.copy()\npred = model.predict(val) \n# \u5024\u304c\u5c0f\u3055\u3044\u307b\u3069\u7570\u5e38\u5ea6\u304c\u9ad8\u3044\u306e\u3067\u3001\u7b26\u53f7\u3092\u3072\u3063\u304f\u308a\u8fd4\u3057\u30660\u304b\u30891\u306b\u304a\u3055\u3081\u308b\npred =pred.mean(axis=1)\npred=pred.mean(axis=1)\nscaler = MinMaxScaler()\npred = scaler.fit_transform(-pred.reshape(-1,1))\ndf_valid.loc[val_idx, 'pred']=pred\ndf_valid = df_valid.dropna(subset=['target', 'pred'])\nscore = roc_auc_score(df_valid['target'], df_valid['pred'])\nprint(f'roc_auc_socre={score}')","36de3337":"tr_idx = norm_idx\npred_idx = list(df[df['file'].str.contains('test')].index)\ntr = features[tr_idx]\npred = features[pred_idx]\ndf_pred = df.copy()\npred = model.predict(pred) \n# \u5024\u304c\u5c0f\u3055\u3044\u307b\u3069\u7570\u5e38\u5ea6\u304c\u9ad8\u3044\u306e\u3067\u3001\u7b26\u53f7\u3092\u3072\u3063\u304f\u308a\u8fd4\u3057\u30660\u304b\u30891\u306b\u304a\u3055\u3081\u308b\npred =pred.mean(axis=1)\npred=pred.mean(axis=1)\npred = scaler.fit_transform(-pred.reshape(-1,1))\ndf_pred.loc[pred_idx, 'target'] = pred\ndf_pred = df_pred[df_pred['file'].str.contains('test')]\n# 1\u6253\u97f3\u3054\u3068\u306e\u7d50\u679c\u306e\u5e73\u5747\u53d6\u3063\u3066\u30d5\u30a1\u30a4\u30eb\u5358\u4f4d\u306e\u4e88\u6e2c\u5024\u3092\u7b97\u51fa\u3059\u308b\nmean_pred = df_pred.groupby('id')['target'].mean().reset_index()","4a7db6f6":"# sample_sub\u306b\u7d50\u679c\u3092merge\nsub = pd.read_csv('sample_submission.csv', usecols=['ID']).rename(columns={\"ID\":\"id\"})\nsub = sub.merge(mean_pred, on='id', how='left')\nsub=sub.rename(columns={\"id\":\"ID\",\"target\":\"Target\"})\n# \u3059\u3079\u3066\u7121\u97f3\u3060\u3063\u305f\u5834\u5408\u3001\u4e88\u6e2c\u5024\u304c\u306a\u3044\u306e\u3067\u5e73\u5747\u5024\u3067\u57cb\u3081\u308b\nsub = sub.fillna(sub['Target'].mean())\nsub = sub.set_index('ID')\nsub.to_csv('\/kaggle\/working\/submission.csv')\nsub","8d1462d2":"\u524d\u56de\u3001\u30a6\u30a7\u30fc\u30d6\u30ec\u30c3\u30c8\u89e3\u6790\u3092\u884c\u3063\u3066\u3001\u3053\u308c\u3092\u7279\u5fb4\u91cf\u3068\u3059\u308b\u3053\u3068\u3067\u9ad8\u3044\u7cbe\u5ea6\u3092\u51fa\u305d\u3046\u3068\u8a66\u307f\u305f\u3002\n\u6b8b\u5ff5\u306a\u304c\u3089\u3001\u9ad8\u3044\u7cbe\u5ea6\u3092\u51fa\u3059\u3053\u3068\u306f\u3067\u304d\u306a\u3044\u306e\u3067\n\u7d50\u8ad6\u3068\u3057\u3066\u30e1\u30eb\u5468\u6ce2\u6570\u30b9\u30da\u30af\u30c8\u30b0\u30e9\u30e0\u3092\u3092\u63a1\u7528\u3002\u97f3\u58f0\u30c7\u30fc\u30bf\u306e\u305f\u3081\u3001\u30e2\u30c7\u30eb\u306fLSTM\uff0dAutoencoder\u3092\u63a1\u7528\u3057\u305f\u3002\n\u306a\u304a\u3001\u7de0\u3081\u5207\u308a\u524d\u65e5\u306e\u305f\u3081\u3001submit\u307e\u3067\u3044\u3051\u306a\u304b\u3063\u305f\u3053\u3068\u3092\u5831\u544a\u3059\u308b\u3002\uff08\u305d\u306e\u305f\u3081V1\u306e\u30b9\u30b3\u30a2\u3092\u53c2\u8003\u306b\u3057\u306a\u3044\u3067\u304f\u3060\u3055\u3044\u3002\uff09","8b389a57":"#### **\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u306b\u3064\u3044\u3066**"}}