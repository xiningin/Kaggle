{"cell_type":{"25b3a259":"code","c97a7df2":"code","27b0fa45":"code","9133153f":"code","bf646160":"code","f6c57a98":"code","4f97478d":"code","4089cfc9":"code","2bf2d2ba":"code","216b9c73":"code","9e9774d5":"code","2d6069f3":"code","b652dac5":"code","b9f96e71":"code","ad037bce":"code","8ca88259":"code","f69aa287":"code","2b848ee1":"code","84c8de1d":"code","1052a102":"code","735bec0e":"code","9414fe1f":"code","6921f211":"code","0828c41a":"code","42758ea4":"code","4ebaca34":"code","20c9ecc2":"markdown","621e347c":"markdown","fec599ba":"markdown","332a052f":"markdown","04b0912d":"markdown","a04c2736":"markdown","b1e76935":"markdown","0aecf497":"markdown","66a4b083":"markdown","bbdf0e10":"markdown","2afe9c1a":"markdown","faf0eb3d":"markdown","679422d5":"markdown"},"source":{"25b3a259":"import warnings\nwarnings.simplefilter('ignore')","c97a7df2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport xml.etree.ElementTree as ET\n\nfrom tqdm.notebook import tqdm\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nfrom multiprocessing import cpu_count\nfrom pympler.asizeof import asizeof\n\nimport os\nimport sys\nimport nltk\nimport string\nimport math\nimport spacy\nimport gc\nimport re\nimport requests\nimport time\nimport joblib\nimport sys\n\n# Download Stopword Corpus\nnltk.download('stopwords')\n\n# NLP stats\nnlp = spacy.load('en_core_web_sm')\n        \n# Stop Words\nSTOP_WORDS = set(stopwords.words())\n\ntqdm.pandas()\n\nprint(f'python version: P{sys.version}')","27b0fa45":"# Preload extracted titles\nif os.path.exists('\/kaggle\/input\/wikipedia-abstracts\/simple_titles.npy'):\n    simple_titles = np.load('\/kaggle\/input\/wikipedia-abstracts\/simple_titles.npy')\nelse:\n    root = ET.parse('..\/input\/wikipedia-abstracts\/simplewiki-20170820-pages-meta-current.xml').getroot()\n    ns = { 'ns': 'http:\/\/www.mediawiki.org\/xml\/export-0.10\/' }\n    simple_titles = []\n\n    # Simple english\n    for idx, child in enumerate(tqdm(root)):\n        if child.tag == '{http:\/\/www.mediawiki.org\/xml\/export-0.10\/}page':\n            # Only use articles\n            if child.find('ns:ns', ns).text == '0':\n                title = child.find('ns:title', ns).text\n                simple_titles.append(title)\n    \nprint(f'Found {len(simple_titles)} valid simple English Wikipedia pages')","9133153f":"# titles need to be split in sets of 20 as this is the limit of per API call\nsimple_titles_chunks = np.array_split(simple_titles, math.ceil(len(simple_titles) \/ 20))\nprint(f'Created {len(simple_titles_chunks)} title chunks')","bf646160":"# Crawl urlURL's\nsimple_wiki_url = 'https:\/\/simple.wikipedia.org\/w\/api.php?exintro&explaintext'\nwiki_url = 'https:\/\/en.wikipedia.org\/w\/api.php?exintro&explaintext'","f6c57a98":"# Function to clean the abstracts\ndef clean_abstract(s):\n    # replace all combination of line breaks and spaces with a single space\n    for m in re.findall('[\\s|\\\\n]+', s):\n        s = s.replace(m, ' ')\n        \n    # replace missing spaces after sentence ending\n    for (a, b, c) in re.findall('([a-zA-Z])([\\.|!|?])([a-zA-Z])', s):\n        s = s.replace(f'{a}{b}{c}', f'{a}{b} {c}')\n    \n    return s\n\ndef process_page(page_data, target_dict):\n    title = page_data['title']\n    \n    abstract_original = page_data['extract']\n    # Filter out abstract with styling\n    if (\n        # Abstract is missing\n        len(abstract_original) == 0 or\n        # Abstract is a referal page to other pages\n        any([s in sent_tokenize(abstract_original)[0].lower() for s in [\n                'displaystyle', 'may refer to', 'could mean', 'might mean', 'may mean', 'can mean', 'may be', 'can refer to',\n                'might refer to', 'has several meanings', 'can mean different things', 'refers to', 'may mean the following',\n                'can mean several things', 'may stand for',\n            ]]) or\n        # Abstract is a referal page to other pages\n        any([s in title.lower() for s in ['list of', 'Lists of', 'disambiguation']])\n    ):\n        return\n    else:\n        abstract_clean = clean_abstract(abstract_original)\n        target_dict[f'{title}'] = {\n            'original': abstract_original,\n            'clean': abstract_clean,\n        }\n        # Add title to valid titles\n        GLOBAL_TITLES.add(title)","4f97478d":"# Result Dictionaries\nwiki_dict = dict()\nsimple_wiki_dict = dict()\nGLOBAL_TITLES = set()","4089cfc9":"# Used to automatically stop after predefined timeout\nCRAWL_TIMEOUT = 60\nT_START = time.time()","2bf2d2ba":"for idx, title_chunk in enumerate(tqdm(simple_titles_chunks)):\n    # Titles seperated with \"|\"\n    titles = '|'.join(title_chunk)\n\n    params = dict({\n        'format': 'json',\n        'action': 'query',\n        'prop': 'extracts',\n        'exlimit': 'max',\n        'redirects': 1,\n        'titles': titles,\n    })\n\n    # Make Requests\n    resp_wiki = requests.get(url=wiki_url, params=params)\n    resp_simple_wiki = requests.get(url=simple_wiki_url, params=params)\n    # Convert response to JSON\n    data_wiki = resp_wiki.json()\n    data_simple_wiki = resp_simple_wiki.json()\n    \n    # SIMPLE WIKIPEDIA\n    for page_idx, page_data in data_simple_wiki['query']['pages'].items():\n        if all([s in page_data.keys() for s in ['title', 'extract']]):\n            process_page(page_data, simple_wiki_dict)\n            \n    # WIKIPEDIA\n    for page_idx, page_data in data_wiki['query']['pages'].items():\n        if all([s in page_data.keys() for s in ['title', 'extract']]):\n            process_page(page_data, wiki_dict)\n            \n    # Automatically stop after predifined time\n    if time.time() - T_START > CRAWL_TIMEOUT:\n        break","216b9c73":"# Some statistics of the retrieved abstracts\nintersection_len = len(set(simple_wiki_dict.keys()).intersection(set(wiki_dict.keys())))\nsimple_len = len(simple_wiki_dict.keys())\n\nprint(f'N Simple Wikipedia abstract: {len(simple_wiki_dict)}, N Normal Wikipedia Articles: {len(wiki_dict)}')\nprint(f'N Unique Titles: {len(GLOBAL_TITLES)}')\nprint(f'Intersection percentage: {intersection_len\/simple_len*100:.2f}%')","9e9774d5":"def get_linguistic_features(s):\n    # Only NLP stats for cleaned abstract\n    s_doc = nlp(s)\n    s_nlp = dict({\n        'lemma': [],\n        'pos': [],\n        'tag': [],\n        'dep': [],\n        'shape': [],\n        'is_alpha': [],\n        'is_stop': [],\n    })\n\n    for token in s_doc:\n        s_nlp['lemma'].append(token.lemma_)\n        s_nlp['pos'].append(token.pos_)\n        s_nlp['tag'].append(token.tag_)\n        s_nlp['dep'].append(token.dep_)\n        s_nlp['shape'].append(token.shape_)\n        s_nlp['is_alpha'].append(token.is_alpha)\n        s_nlp['is_stop'].append(token.is_stop)\n\n    return s_nlp\n\ndef remove_stopwords(words):\n    return [w for w in words if not w in STOP_WORDS]","2d6069f3":"def process_wiki_abstracts(title, abstracts, label):\n    DEL = chr(0)\n    # Abstracts\n    abstract_original = abstracts['original']\n    abstract_clean = abstracts['clean']\n    # Words\n    clean_words = word_tokenize(abstract_clean)\n    words_wo_stopwords = remove_stopwords(clean_words)\n    # Word Count\n    n_words = len(clean_words)\n    n_words_wo_stopwords = len(words_wo_stopwords)\n    # Sentences\n    sentences = sent_tokenize(abstract_clean)\n    n_sentences = len(sentences)\n\n    # Only NLP stats for cleaned abstract\n    linguistic_features = get_linguistic_features(abstract_clean)\n\n    row = {\n        'title': title,\n        'abstract_original': abstract_original,\n        'abstract_clean': abstract_clean,\n        # Words\n        'clean_words': DEL.join(clean_words),\n        'words_wo_stopwords': DEL.join(words_wo_stopwords),\n        # Word Count\n        'n_words': n_words,\n        'n_words_wo_stopwords': n_words_wo_stopwords,\n        # Sentences\n        'sentences': sentences,\n        'n_sentences': n_sentences,\n        # Simple Wiki Linguistic Features\n        'lemma': DEL.join(linguistic_features['lemma']),\n        'pos': DEL.join(linguistic_features['pos']),\n        'tag': DEL.join(linguistic_features['tag']),\n        'dep': DEL.join(linguistic_features['dep']),\n        'shape': DEL.join(linguistic_features['shape']),\n        'is_alpha': linguistic_features['is_alpha'],\n        'is_stop': linguistic_features['is_stop'],\n        # label\n        'label': label,\n        'label_int': 0 if label == 'Simple Wikipedia' else 1,\n    }\n    \n    return row","b652dac5":"# SIMPLE WIKIPEDIA\njobs = [joblib.delayed(process_wiki_abstracts)(title, abstracts, 'Simple Wikipedia') for title, abstracts in simple_wiki_dict.items()]\nsimple_wiki_rows = joblib.Parallel(\n    n_jobs=cpu_count(),\n    verbose=1,\n    batch_size=8,\n    require='sharedmem',\n)(jobs)\n# NORMAL WIKIPEDIA\njobs = [joblib.delayed(process_wiki_abstracts)(title, abstracts, 'Normal Wikipedia') for title, abstracts in wiki_dict.items()]\nnormal_wiki_rows = joblib.Parallel(\n    n_jobs=cpu_count(),\n    verbose=1,\n    batch_size=8,\n    require='sharedmem',\n)(jobs)","b9f96e71":"df = pd.DataFrame.from_dict(simple_wiki_rows + normal_wiki_rows)","ad037bce":"display(df.info())","8ca88259":"display(df.head())","f69aa287":"# All lists are saved as NULL seprated strings to save memory.\nprint(df.loc[0, 'clean_words'].split(chr(0)))","2b848ee1":"pd.options.display.max_colwidth = 64\ndisplay(df.sort_values('n_words').head(25))","84c8de1d":"# Error bar configuration\nerrorbar_config = {\n    'capsize': 10, 'ecolor': 'black', 'capthick': 2, 'elinewidth': 2, 'markersize': 10, 'fmt': 'o',\n}","1052a102":"display(df['n_words'].describe())","735bec0e":"plt.figure(figsize=(12,8))\nplt.title(f'Distribution of number of words per abstract', size=18)\ndf['n_words'].plot(kind='hist', bins=64)\nplt.errorbar(df['n_words'].mean(), plt.gca().get_ylim()[1] * 0.25, xerr=df['n_words'].std(), color='red', **errorbar_config)\nplt.show()","9414fe1f":"display(df['n_sentences'].describe())","6921f211":"plt.figure(figsize=(12,8))\nplt.title(f'Distribution of number of sentences per abstract', size=18)\ndf['n_sentences'].plot(kind='hist', bins=32)\nplt.errorbar(df['n_sentences'].mean(), plt.gca().get_ylim()[1] * 0.25, xerr=df['n_sentences'].std(), color='red', **errorbar_config)\nplt.show()","0828c41a":"display(df['label'].value_counts().to_frame())","42758ea4":"plt.figure(figsize=(8,8))\nplt.title(f'Distribution of class occurances', size=18)\ndf['label'].value_counts().plot(kind='pie', legend=True, autopct='%1.1f%%')\nplt.show()","4ebaca34":"df.to_pickle('wikipedia_abstracts.pkl')","20c9ecc2":"This loop retrieves the abstracts for simple and normal Wikipedia for each title chunk. Take note the each loop can take between 1-20 seconds and is generally quite fast for the first few minutes after which it significantly slows down. This loop took ~15 hours when running locally.","621e347c":"# Class distribution","fec599ba":"# Simple English Titles","332a052f":"Show the shortest abstracts to check if filtering process was succesful.","04b0912d":"Hello fellow Kagglers,\n\nThis notebook demonstrates the method used to create the [Simple\/Normal Wikipedia Abstracts V1](https:\/\/www.kaggle.com\/markwijkhuizen\/simplenormal-wikipedia-abstracts-v1) dataset.\n\nAn example of using this dataset to pretrain for the CommonLit competition dataset can be found [here](https:\/\/www.kaggle.com\/markwijkhuizen\/simple-normal-wikipedia-abstracts-pretraining).\n\nIn short, all titles on simple Wikipedia pages are extracted and for each title both the simple and normal abstracts is acquired using the public Wikipedia API.\n\nDocumentation for the Wikipedia API can be found [here](https:\/\/www.mediawiki.org\/wiki\/API:Main_page).\n\nI will keep on working to improve the dataset by adding page content next to abstracts.\n\n***Before rerunning or editing this notebook, read the [Wikipedia API Etiquette](https:\/\/www.mediawiki.org\/wiki\/API:Etiquette)***","a04c2736":"Extract all page titles from the simple Wikipedia XML dump, this dump and many other dumps can be found [here](https:\/\/meta.wikimedia.org\/wiki\/Data_dump_torrents#Simple_English_Wikipedia).","b1e76935":"# Crawl Abstract","0aecf497":"# Linguistic Features","66a4b083":"# Sentences","bbdf0e10":"# Save DataFrame","2afe9c1a":"# Words","faf0eb3d":"# Create DataFrame","679422d5":"# Statistics"}}