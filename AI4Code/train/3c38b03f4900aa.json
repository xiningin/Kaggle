{"cell_type":{"c58a1138":"code","cdfc8e4b":"code","9f1010e0":"code","43aa1861":"code","0d0b1b5c":"code","b9918477":"code","026f54a1":"code","7f50213a":"code","4950f76c":"code","74b1124a":"code","d643235c":"code","3a99bb67":"code","c9f90b76":"code","66d88ba2":"code","a6957c11":"code","5ca9b93c":"code","608be10e":"code","6d27d4dd":"code","2041c53c":"code","8f9466e0":"code","49d6287a":"code","82286c9e":"code","4a596089":"code","6812389d":"code","f15e245d":"code","b79bf9b4":"code","4af6a452":"code","9e1cc1fb":"code","ee0e3f8f":"markdown","040ea252":"markdown","592b1289":"markdown","600f5d07":"markdown","67609e25":"markdown","efa542dd":"markdown","9abefb14":"markdown","21ddc42e":"markdown","d9f64197":"markdown","e3aa91f6":"markdown","9f7871db":"markdown","b2f26bbd":"markdown","fe993fd8":"markdown","feacaf37":"markdown","4cf66e37":"markdown"},"source":{"c58a1138":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cdfc8e4b":"import matplotlib.pyplot as plt\nimport re\nimport string\nimport time\npd.set_option('display.max_rows', 50)\n\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import cross_validate\n\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier","9f1010e0":"df = pd.read_csv(\"\/kaggle\/input\/data-cleaning-enron-email-dataset\/cleaned_data.csv\")\n\n# view first 5 rows of the dataframe\ndf.head()","43aa1861":"def remove_folders(emails, n):\n    # returns the number of folders containing more than 'n' number of emails\n    email_count = dict(df['X-Folder'].value_counts())\n    small_folders = [key for key, val in email_count.items() if val<=n]\n    emails = df.loc[~df['X-Folder'].isin(small_folders)]\n    return emails","0d0b1b5c":"n = 150\ndf = remove_folders(df, n)","b9918477":"print(\"Total folders: \", len(df['X-Folder'].unique()))\nprint(\"df.shape: \", df.shape)","026f54a1":"df['text'] = df['subject'] + \" \" + df['body']","7f50213a":"# drop the columns 'subject' and 'body'\ndf.drop(['subject','body'], axis=1, inplace=True)","4950f76c":"def preprocess(x):\n    # lowercasing all the words\n    x = x.lower()\n    \n    # remove extra new lines\n    x = re.sub(r'\\n+', ' ', x)\n    \n    # removing (replacing with empty spaces actually) all the punctuations\n    x = re.sub(\"[\"+string.punctuation+\"]\", \" \", x)\n    \n    # remove extra white spaces\n    x = re.sub(r'\\s+', ' ', x)\n    \n    return x","74b1124a":"start = time.time()\ndf.loc[:,'text'] = df.loc[:, 'text'].map(preprocess)\n\n# remove stopwords\ndf.loc[:, 'text'] = df.loc[:, 'text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))\nend = time.time()\nprint(\"Execution time (sec): \",(end - start))","d643235c":"start = time.time()\nfolders_dict = dict(df['X-Folder'].value_counts().sort_values()[50:70])\ndata = df[df['X-Folder'].isin(folders_dict.keys())]\nend = time.time()\nprint(\"Execution time (sec): \",(end - start))","3a99bb67":"# check number of rows in the 'data' dataframe\nprint(\"Number of instances: \", data.shape[0])\ndata.to_csv('preprocessed.csv', index=False)","c9f90b76":"data = pd.read_csv(\"preprocessed.csv\")","66d88ba2":"data['X-Folder'].value_counts()","a6957c11":"def label_encoder(data):\n    class_le = LabelEncoder()\n    # apply label encoder on the 'X-Folder' column\n    y = class_le.fit_transform(data['X-Folder'])\n    return y","5ca9b93c":"y = label_encoder(data)\ninput_data = data['text']","608be10e":"start = time.time()\nvectorizer = CountVectorizer(min_df=5, max_features=5000)\nX = vectorizer.fit_transform(input_data)\nend = time.time()\nprint(\"Execution time (sec): \",(end - start))","6d27d4dd":"start = time.time()\nX = X.toarray()\nprint(\"X.shape: \",X.shape)\nend = time.time()\nprint(\"Execution time (sec): \",(end - start))","2041c53c":"# create dataframe to store results\nf1_data = {\n    'Algorithm': ['Gaussian NB', 'Multinomial NB','Decision Tree','SVM','AdaBoost','ANN'],\n    'BoW': ''\n}\nf1_df = pd.DataFrame(f1_data)\n\njaccard_data = {\n    'Algorithm': ['Gaussian NB', 'Multinomial NB', 'Decision Tree','SVM','AdaBoost','ANN'],\n    'BoW': ''\n}\njacc_df = pd.DataFrame(jaccard_data)\n\nacc_data = {\n    'Algorithm': ['Gaussian NB', 'Multinomial NB','Decision Tree','SVM','AdaBoost','ANN'],\n    'BoW': ''\n}\nacc_df = pd.DataFrame(acc_data)\nacc_df","8f9466e0":"models = [GaussianNB(), MultinomialNB(), DecisionTreeClassifier(), LinearSVC(), \n          AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=5),\n         MLPClassifier(hidden_layer_sizes=(10,))]\n\nnames = [\"Gaussian NB\", \"Multinomial NB\", \"Decision Tree\", \"SVM\", \"AdaBoost\", \"ANN\"]\n\njacc_scores = []\nacc_scores = []\nf1_scores = []\nexec_times = []\n\nfor model, name in zip(models, names):\n    print(name)\n    start = time.time()\n    scoring = {\n        'acc': 'accuracy',\n        'f1_mac': 'f1_macro',\n        'jacc_mac': 'jaccard_macro'\n    }\n    scores = cross_validate(model, X, y, cv=10, n_jobs=4, scoring=scoring)\n    training_time = (time.time() - start)\n    print(\"accuracy: \", scores['test_acc'].mean())\n    print(\"f1_score: \", scores['test_f1_mac'].mean())\n    print(\"Jaccard_index: \", scores['test_jacc_mac'].mean())\n    print(\"time (sec): \", training_time)\n    print(\"\\n\")\n    \n    jacc_scores.append(scores['test_jacc_mac'].mean())\n    acc_scores.append(scores['test_acc'].mean())\n    f1_scores.append(scores['test_f1_mac'].mean())\n    exec_times.append(training_time)\n    \nacc_df['BoW'] = acc_scores\njacc_df['BoW'] = jacc_scores\nf1_df['BoW'] = f1_scores\nacc_df['time'] = exec_times\nacc_df","49d6287a":"# save the results\nacc_df.to_csv(\"accuracy.csv\", index=False)\nf1_df.to_csv(\"f1_score.csv\", index=False)\njacc_df.to_csv(\"jacc_score.csv\", index=False)","82286c9e":"start = time.time()\nvectorizer = CountVectorizer(min_df=5, max_features=5000, ngram_range=(2,2))\nX = vectorizer.fit_transform(input_data)\n\nX = X.toarray()\nprint(\"X.shape: \",X.shape)\n\nend = time.time()\nprint(\"Execution time (sec): \",(end - start))","4a596089":"models = [GaussianNB(), MultinomialNB(), DecisionTreeClassifier(), LinearSVC(), \n          AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=5),\n         MLPClassifier(hidden_layer_sizes=(10,))]\n\nnames = [\"Gaussian NB\", \"Multinomial NB\", \"Decision Tree\", \"SVM\", \"AdaBoost\", \"ANN\"]\n\njacc_scores = []\nacc_scores = []\nf1_scores = []\nexec_times = []\n\nfor model, name in zip(models, names):\n    print(name)\n    start = time.time()\n    scoring = {\n        'acc': 'accuracy',\n        'f1_mac': 'f1_macro',\n        'jacc_mac': 'jaccard_macro'\n    }\n    scores = cross_validate(model, X, y, cv=10, n_jobs=4, scoring=scoring)\n    training_time = (time.time() - start)\n    print(\"accuracy: \", scores['test_acc'].mean())\n    print(\"f1_score: \", scores['test_f1_mac'].mean())\n    print(\"Jaccard_index: \", scores['test_jacc_mac'].mean())\n    print(\"time (sec): \", training_time)\n    print(\"\\n\")\n    \n    jacc_scores.append(scores['test_jacc_mac'].mean())\n    acc_scores.append(scores['test_acc'].mean())\n    f1_scores.append(scores['test_f1_mac'].mean())\n    exec_times.append(training_time)\n    \nacc_df['BoWBi'] = acc_scores\njacc_df['BoWBi'] = jacc_scores\nf1_df['BoWBi'] = f1_scores\nacc_df['BoWBi_time'] = exec_times\nacc_df","6812389d":"# save the results\nacc_df.to_csv(\"accuracy.csv\", index=False)\nf1_df.to_csv(\"f1_score.csv\", index=False)\njacc_df.to_csv(\"jacc_score.csv\", index=False)","f15e245d":"start = time.time()\nvectorizer = TfidfVectorizer(min_df=5, max_features=5000)\nX = vectorizer.fit_transform(input_data)\n\nX = X.toarray()\nprint(\"X.shape: \",X.shape)\n\nend = time.time()\nprint(\"Execution time (sec): \",(end - start))","b79bf9b4":"models = [GaussianNB(), MultinomialNB(), DecisionTreeClassifier(), LinearSVC(), \n          AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=5),\n         MLPClassifier(hidden_layer_sizes=(10,))]\n\nnames = [\"Gaussian NB\", \"Multinomial NB\", \"Decision Tree\", \"SVM\", \"AdaBoost\", \"ANN\"]\n\njacc_scores = []\nacc_scores = []\nf1_scores = []\nexec_times = []\n\nfor model, name in zip(models, names):\n    print(name)\n    start = time.time()\n    scoring = {\n        'acc': 'accuracy',\n        'f1_mac': 'f1_macro',\n        'jacc_mac': 'jaccard_macro'\n    }\n    scores = cross_validate(model, X, y, cv=10, n_jobs=4, scoring=scoring)\n    training_time = (time.time() - start)\n    print(\"accuracy: \", scores['test_acc'].mean())\n    print(\"f1_score: \", scores['test_f1_mac'].mean())\n    print(\"Jaccard_index: \", scores['test_jacc_mac'].mean())\n    print(\"time (sec): \", training_time)\n    print(\"\\n\")\n    \n    jacc_scores.append(scores['test_jacc_mac'].mean())\n    acc_scores.append(scores['test_acc'].mean())\n    f1_scores.append(scores['test_f1_mac'].mean())\n    exec_times.append(training_time)\n    \nacc_df['TfIdf'] = acc_scores\njacc_df['TfIdf'] = jacc_scores\nf1_df['TfIdf'] = f1_scores\nacc_df['TfIdf_time'] = exec_times\nacc_df","4af6a452":"# save the results\nacc_df.to_csv(\"accuracy.csv\", index=False)\nf1_df.to_csv(\"f1_score.csv\", index=False)\njacc_df.to_csv(\"jacc_score.csv\", index=False)","9e1cc1fb":"jacc_df","ee0e3f8f":"### Training and Evaluation","040ea252":"## 1. Bag-of-Words","592b1289":"- Randomly select any 20 folders which we would like to categorize.\n- Only 20 folders have been selected because of very high training time and computational cost","600f5d07":"**Combine subject and body columns**","67609e25":"## 2. Bag-of-Words Bigram","efa542dd":"### Data Pre-processing","9abefb14":"### Load Data","21ddc42e":"### Training and Evaluation","d9f64197":"### Training and Evaluation","e3aa91f6":"## 3. Tf-Idf (Term Frequency - Inverse Document Frequency)","9f7871db":"### Import necessary libraries","b2f26bbd":"#### Remove Folders\nRemove folders that do not contain enough e-mails because such folders would not be significant for training our classifier. Also, we can infer that some folders with very little e-mails in them were created but unused.","fe993fd8":"## Enron Email Classification using Machine Learning\n\nyou can find data cleaning notebook of enron email dataset at:\n\n[https:\/\/www.kaggle.com\/ankur561999\/data-cleaning-enron-email-dataset](https:\/\/www.kaggle.com\/ankur561999\/data-cleaning-enron-email-dataset)","feacaf37":"**Encode class labels**","4cf66e37":"Now, do the following to preprocess text:\n- lowercasing all words\n- Remove extra new lines\n- Remove extra tabs, punctuations, commas\n- Remove extra white spaces\n- Remove stopwords"}}