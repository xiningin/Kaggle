{"cell_type":{"da469378":"code","2f3b2f62":"code","62df6b4b":"code","b8e69b7f":"code","1042c69a":"code","92b44e90":"code","8065effd":"code","b1456a4c":"code","7f789dcb":"code","5e155307":"code","99a52942":"code","b568f1ca":"code","241d03ce":"code","2e15a6b7":"code","6e692e4e":"markdown","5ca62df1":"markdown","89e9b03a":"markdown"},"source":{"da469378":"import numpy as np\nimport pandas as pd \nimport tensorflow as tf\nfrom keras.regularizers import l1_l2\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\nfrom keras import models, layers\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support, multilabel_confusion_matrix, f1_score\nfrom sklearn.preprocessing import normalize\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import metrics,optimizers\n\nimport matplotlib.pyplot as plt\n\nfrom keras.backend import clear_session\nimport re","2f3b2f62":"X = pd.read_csv('\/kaggle\/input\/movie-plot-title-data\/data.csv', index_col = 'Unnamed: 0')['0']\ny = pd.read_csv('\/kaggle\/input\/movie-plot-title-data\/labels.csv', index_col = 'Unnamed: 0')\ny = y.iloc[:,np.where(y.sum(axis = 0) > 800)[0]]\n\nX.drop(np.where(y.sum(axis = 1) == 0)[0], inplace = True)\ny.drop(np.where(y.sum(axis = 1) == 0)[0], inplace =True)\ngenre_list =list(y.columns)\n","62df6b4b":"y.sum(axis = 0)","b8e69b7f":"# def preprocess_text(sen):\n#     # Remove punctuations and numbers\n#     sentence = re.sub('[^a-zA-Z.]', ' ', sen)\n\n#     # Single character removal\n#     sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n\n#     # Removing multiple spaces\n#     sentence = re.sub(r'\\s+', ' ', sentence)\n\n#     return sentence\n# X = X.apply(preprocess_text)","1042c69a":"def data_generator(data, label, batch_size = 32):\n    while True:\n        rows = np.random.randint(0, len(data), size = batch_size)\n        yield data[rows,:], np.array(label)[rows,:]\ndef nn_preds_to_classes(preds, threshold = 0.3):\n    classes_preds = np.zeros(preds.shape)\n    rows, columns = np.where(preds > threshold)\n    for i in range(len(rows)):\n        classes_preds[rows[i],columns[i]] = 1\n    return classes_preds\ndef eval_nn_model(model, xtest, ytest):\n    preds = nn_preds_to_classes(model.predict(xtest))\n    \n    test_score = f1_score(ytest, preds, average = None)\n    return test_score, preds\n\n","92b44e90":"max_words = 10000\n\nmaxlen = 5000\n\nxtrain, xval, ytrain, yval = train_test_split(X,y, random_state = 0, train_size = 0.8)\n\ntokenizer = Tokenizer(num_words = max_words)\ntokenizer.fit_on_texts(xtrain)\nxtrain = pad_sequences(tokenizer.texts_to_sequences(xtrain), maxlen = maxlen)\nxval = pad_sequences(tokenizer.texts_to_sequences(xval), maxlen = maxlen)\n\n","8065effd":"embeddings = {}\nwith open('\/kaggle\/input\/glove-global-vectors-for-word-representation\/glove.6B.50d.txt') as f : \n    for line in f.readlines():\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings[word] = coefs\nf.close()\n\nembeddings_dim = 50\nembeddings_matrix = np.zeros((max_words, embeddings_dim))\nfor word, i in tokenizer.word_index.items():\n    if i < max_words:\n        embedding_vec = embeddings.get(word)\n        if embedding_vec is not None:\n            embeddings_matrix[i] = embedding_vec\n    ","b1456a4c":"import keras\nlr_schedule = keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=1e-1,\n    decay_steps=10000,\n    decay_rate=0.9)","7f789dcb":"tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","5e155307":"with tpu_strategy.scope():    \n    model = models.Sequential()\n    model.add(layers.Embedding(max_words, embeddings_dim,  input_length = xtrain.shape[1]))\n    model.add(layers.LSTM(64))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(64,kernel_regularizer = l1_l2(l1 = 0, l2 = 1e-3)))\n    model.add(layers.Dropout(0.2))\n    model.add(layers.Dense(len(genre_list), activation = 'sigmoid'))\n    model.compile(loss = 'binary_crossentropy',\n                  optimizer = optimizers.RMSprop(),\n                  metrics = [metrics.AUC()])\n\n    model.summary()\n\n    model.layers[0].set_weights([embeddings_matrix])\n    model.layers[0].trainable = False\n\nhistory = model.fit(xtrain,ytrain,batch_size = 16,\n                    epochs = 20,\n                    validation_data = (xval,yval)\n                  )\nval_preds = model.predict(xval)\n#     train_preds = model.predict(xtrain)\npreds_classes = nn_preds_to_classes(val_preds, threshold = 0.5)\nval_score = precision_recall_fscore_support( yval, preds_classes, zero_division = 1)\nprint(val_score)\nprint(val_score[0].mean())\nprint(val_score[1].mean())\n\nprint(val_score[2].mean())\n","99a52942":"clear_session()","b568f1ca":"history_dict = history.history\nfig = plt.figure(figsize = (10,10))\nplt.plot([i for i in range(20)], history_dict['loss'], label = 'training loss')\n\nplt.plot([i for i in range(20)], history_dict['val_loss'],label = 'validation loss')\nplt.legend()","241d03ce":"val_preds = model.predict(xval)\n#     train_preds = model.predict(xtrain)\npreds_classes = nn_preds_to_classes(val_preds, threshold = 0.5)\nval_score = precision_recall_fscore_support( yval, preds_classes, zero_division = 1)\nprint(val_score)\nprint(val_score[0].mean())\nprint(val_score[1].mean())\n\nprint(val_score[2].mean())","2e15a6b7":"fig = plt.figure(figsize = (10,10))\nplt.bar(y.columns, val_score[2])","6e692e4e":"training loss , val loss curve\nplotting the distribution\nclean data even more\nvisualize the tree ? \ntree pruning ? \nshow results after every technique","5ca62df1":"31 - 33 : 64 lstm, 128 dense, rmsprop ( 37%)\nLSTM 64, dropout 0.2, dense 64 1e-3 l2 dropout 0.2 16 epochs : 48%","89e9b03a":"'sigmoid' activaation and 'binary_crossentropy' loss\n\nUsing count_vec or tfidf with normalization"}}