{"cell_type":{"ce1aa944":"code","b886091a":"code","3b13f3a5":"code","33011fa4":"code","ab725066":"code","e3b8ac1f":"code","ad688843":"code","7dfccc73":"code","7b98efba":"code","8b9a50d9":"code","a3eeebea":"code","c75867e9":"code","daa03cc5":"code","9d762e6e":"code","92744565":"code","6805eded":"code","f220703e":"code","7197eece":"code","f9796f6d":"code","a845a220":"code","34ab185a":"code","4961036e":"code","e63161a7":"code","49c5c4fb":"code","dcfe4d47":"code","2a57ab49":"code","f820d8ff":"code","e562449d":"code","1f89dcee":"code","77dc9e01":"code","d97c3f64":"code","3cc7ec44":"code","d8710c7e":"code","8df889b5":"code","6a066e77":"code","ef04bdc8":"code","80ef377c":"code","22b435fa":"code","ae988fee":"code","9d14c1b4":"code","8e72efb5":"code","205f649e":"code","946634ee":"code","5dbf42d3":"code","a7a6246c":"code","07c06f71":"code","f80af0b7":"code","3cea7ad5":"code","bff5a5e0":"code","5a66afa2":"code","37a434a9":"code","5af43fea":"code","bcf57040":"code","3859cecc":"code","e7aa777f":"code","59b3de1e":"markdown","3e6f1b8e":"markdown","734492d3":"markdown","0d0add05":"markdown","ca4e6b77":"markdown","9061546d":"markdown","89edd20b":"markdown","3566432a":"markdown","2e7a0f3c":"markdown","eaf18909":"markdown","c4a6d8a8":"markdown","63ebaf0c":"markdown","4f910ca4":"markdown","e40b27cc":"markdown","cc5c0a40":"markdown","3df72c3f":"markdown","759b850a":"markdown","f7cf0c8c":"markdown","818d2571":"markdown","197936f3":"markdown","3ea603ec":"markdown","4f727b7b":"markdown","7dee15da":"markdown","956e3a54":"markdown","46992347":"markdown","b9117730":"markdown","e9c3e246":"markdown","ad8d134e":"markdown","6031ce00":"markdown","e74cf0e9":"markdown","28a59780":"markdown","6e86ec4d":"markdown","09e6b938":"markdown","f698ff5d":"markdown","2dd0be3a":"markdown","d53f2aa4":"markdown"},"source":{"ce1aa944":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set()","b886091a":"\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import linear_model\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn import svm","3b13f3a5":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","33011fa4":"df = pd.read_csv(\"\/kaggle\/input\/reader-wonky-machines-21sensors\/reader.csv\")\ndf.head()\n","ab725066":"df[' label']= df[' label'].fillna(0)","e3b8ac1f":"df.info()\n","ad688843":"df.isnull().sum()","7dfccc73":"df.head()\n","7b98efba":"df['setting1'] = df['setting1'].fillna(df['setting1'].mean())\ndf['setting2'] = df['setting2'].fillna(df['setting2'].mean())\ndf['setting03'] = df['setting03'].fillna(df['setting03'].mean())\ndf['sensor1'] = df['sensor1'].fillna(df['sensor1'].mean())\ndf['sensor2'] = df['sensor2'].fillna(df['sensor2'].mean())\ndf['sensor3'] = df['sensor3'].fillna(df['sensor3'].mean())\ndf['sensor4'] = df['sensor4'].fillna(df['sensor4'].mean())\ndf['sensor5'] = df['sensor5'].fillna(df['sensor5'].mean())\ndf['sensor6'] = df['sensor6'].fillna(df['sensor6'].mean())\ndf['sensor7'] = df['sensor7'].fillna(df['sensor7'].mean())\ndf['sensor8'] = df['sensor8'].fillna(df['sensor8'].mean())\ndf['sensor9'] = df['sensor9'].fillna(df['sensor9'].mean())\ndf['sensor10'] = df['sensor10'].fillna(df['sensor10'].mean())\ndf['sensor11'] = df['sensor11'].fillna(df['sensor11'].mean())\ndf['sensor12'] = df['sensor12'].fillna(df['sensor12'].mean())\ndf['sensor13'] = df['sensor13'].fillna(df['sensor13'].mean())\ndf['sensor14'] = df['sensor14'].fillna(df['sensor14'].mean())\ndf['sensor15'] = df['sensor15'].fillna(df['sensor15'].mean())\ndf['sensor16'] = df['sensor16'].fillna(df['sensor16'].mean())\ndf['sensor17'] = df['sensor17'].fillna(df['sensor17'].mean())\ndf['sensor18'] = df['sensor18'].fillna(df['sensor18'].mean())\ndf['sensor19'] = df['sensor19'].fillna(df['sensor19'].mean())\ndf['sensor20'] = df['sensor20'].fillna(df['sensor20'].mean())\ndf['sensor21'] = df['sensor21'].fillna(df['sensor21'].mean())","8b9a50d9":"df = df.drop('timestamp', axis = 1)","a3eeebea":"df = df.dropna()","c75867e9":"df.isnull().sum()","daa03cc5":"dtrack = pd.read_csv(\"\/kaggle\/input\/reader-failures-wonky-machines-ltd\/failures.csv\")\ndtrack.tail()\n","9d762e6e":"df[' label'].value_counts()","92744565":"target_var = ['Target_Remaining_Useful_Life']\nsensor_columns =[\"sensor\"+str(i) for i in range(1,22)]\nprint(sensor_columns)","6805eded":"max_cycle = df.groupby('id')['cycle'].max().reset_index()\nmax_cycle.columns = ['id', 'MaxOfCycle']","f220703e":"df_merged = df.merge(max_cycle, left_on='id', right_on='id', how='inner')","7197eece":"Target_Remaining_Useful_Life = df_merged[\"MaxOfCycle\"] - df_merged[\"cycle\"]\ntrain_with_target = df_merged[\"Target_Remaining_Useful_Life\"] = Target_Remaining_Useful_Life\ntrain_with_target = df_merged.drop(\"MaxOfCycle\", axis=1)\ntrain_with_target[train_with_target['id'] == 1].head(5)","f9796f6d":"explore = sns.PairGrid(data=train_with_target.query('id == 11 | id == 15 | id == 18 | id == 25| id == 27 | id == 34| id == 36| id == 41 | id == 42| id == 44 | id == 50') ,\n                 x_vars=target_var,\n                 y_vars=sensor_columns,\n                 hue=\"id\", size=3, aspect=2.5)\nexplore = explore.map(plt.scatter, alpha=0.5)\nexplore = explore.set(xlim=(175,0))\nexplore = explore.add_legend()","a845a220":"explore = sns.PairGrid(data=train_with_target.query('id < 50') ,\n                 x_vars=target_var,\n                 y_vars=sensor_columns,\n                 hue=\"id\", size=3, aspect=2.5)\nexplore = explore.map(plt.scatter, alpha=0.5)\nexplore = explore.set(xlim=(175,0))\nexplore = explore.add_legend()","34ab185a":"# operational setting 3 is stable, let's visualize op setting 1 and 2 against some of the most active sensors\ng = sns.pairplot(data=train_with_target.query('id == 11 | id == 15 | id == 18 | id == 25| id == 27 | id == 34| id == 36| id == 41 | id == 42| id == 44 | id == 50'),\n                 x_vars=[\"setting1\",\"setting2\",\"setting03\"],\n                 y_vars=[\"sensor2\", \"sensor5\", \"sensor6\", \"sensor7\", \"sensor8\", \"sensor9\", \"sensor11\", \"sensor12\", \"sensor13\", \"sensor14\", \"sensor15\", \"sensor17\", \"sensor20\", \"sensor21\"],\n                 hue=\"id\", aspect=1)\n","4961036e":"ax = train_with_target[['setting2', ' label']].boxplot(by=' label', figsize=(10,6))\nax.set_ylabel('setting2')","e63161a7":"\nplt.figure(figsize=(20,50))\nax=train_with_target.groupby('id')['cycle'].max().plot(kind='barh',width=0.8, stacked=True,align='center',rot=0)\nplt.title('Engine LifeTime',fontweight='bold',size=35)\nplt.xlabel('Cycle Time',fontweight='bold',size=30)\nplt.xticks(size=25)\nplt.ylabel('Engine ID',fontweight='bold',size=30)\nplt.yticks(size=25)\nplt.grid(True)\nplt.tight_layout(True)\nplt.show()","49c5c4fb":"cycletimereduced =train_with_target.query('id == 11 | id == 15 | id == 18 | id == 25| id == 27 | id == 34| id == 36| id == 41 | id == 42| id == 44 | id == 50')","dcfe4d47":"cycletimereduced","2a57ab49":"plt.figure(figsize=(20,10))\nax=cycletimereduced.groupby('id')['cycle'].max().plot(kind='barh',width=0.4, stacked=True,align='center',rot=0)\nplt.title('Engine LifeTime',fontweight='bold',size=35)\nplt.xlabel('Cycle Time',fontweight='bold',size=30)\nplt.xticks(size=25)\nplt.ylabel('Engine ID',fontweight='bold',size=30)\nplt.yticks(size=25)\nplt.grid(True)\nplt.tight_layout(True)\nplt.show()","f820d8ff":"print(train_with_target.shape)\nleakage_to_drop = ['id', 'cycle', 'setting1', 'setting2', 'setting03']  \ntrain_no_leakage = train_with_target.drop(leakage_to_drop, axis = 1)\nprint(train_no_leakage.shape)\n# set up features and target variable \ny = train_no_leakage['Target_Remaining_Useful_Life']\nX = train_no_leakage.drop(['Target_Remaining_Useful_Life'], axis = 1)\n# I like to use a simple random forest to determine some of the most important\/meaningful features. Can be used as feature selection\n# create an exhuastive random forest (200 trees up to 15 levels deep)\nfrom sklearn import ensemble\nrf = ensemble.RandomForestRegressor()\nsingle_rf = ensemble.RandomForestRegressor(n_estimators = 200, max_depth = 15)\nsingle_rf.fit(X, y)\ny_pred = single_rf.predict(X)\nprint(\"complete\")","e562449d":"import matplotlib.pyplot as plt\nimportances = single_rf.feature_importances_\nindices = np.argsort(importances)[::-1]\nfeature_names = X.columns    \nf, ax = plt.subplots(figsize=(11, 9))\nplt.title(\"Feature ranking\", fontsize = 20)\nplt.bar(range(X.shape[1]), importances[indices], color=\"b\", align=\"center\")\nplt.xticks(range(X.shape[1]), indices) #feature_names, rotation='vertical')\nplt.xlim([-1, X.shape[1]])\nplt.ylabel(\"importance\", fontsize = 18)\nplt.xlabel(\"index of the feature\", fontsize = 18)\nplt.show()\n# list feature importance\nimportant_features = pd.Series(data=single_rf.feature_importances_,index=X.columns)\nimportant_features.sort_values(ascending=False,inplace=True)\nprint(important_features.head(10))","1f89dcee":"corr = train_with_target.corr() #Correlation matrix for CB player\ncorr","77dc9e01":"fig = plt.figure(figsize=(8,8))\nplt.matshow(corr, cmap='RdBu', fignum=fig.number)\nplt.xticks(range(len(corr.columns)), corr.columns, rotation='vertical');\nplt.yticks(range(len(corr.columns)), corr.columns);","d97c3f64":"train_with_target['newlabel'] = 0","3cc7ec44":"train_with_target","d8710c7e":"train_with_target['newlabel'] = train_with_target['Target_Remaining_Useful_Life'].apply(lambda x: '1' if x < 15 else '0')","8df889b5":"train_with_target","6a066e77":"train_with_target = train_with_target.drop('id', axis = 1)\ntrain_with_target = train_with_target.drop(' label', axis = 1)\ntrain_with_target = train_with_target.drop('Target_Remaining_Useful_Life', axis = 1)\n\n","ef04bdc8":"# Reminder of the list of the features : 9, 0, 17, 15, 4, 18","80ef377c":"train_with_target = train_with_target.drop('sensor10', axis = 1)\ntrain_with_target = train_with_target.drop('sensor1', axis = 1)\ntrain_with_target = train_with_target.drop('sensor16', axis = 1)\ntrain_with_target = train_with_target.drop('sensor5', axis = 1)\ntrain_with_target = train_with_target.drop('sensor19', axis = 1)","22b435fa":"train_with_target","ae988fee":"all_inputs = train_with_target[['setting1', 'setting2', 'setting03', 'sensor2', 'sensor3', 'sensor4', 'sensor6', 'sensor7', 'sensor8', 'sensor9',  'sensor11',\n                   'sensor12', 'sensor13', 'sensor14', 'sensor15', 'sensor17', 'sensor18' , 'sensor20', 'sensor21']].values\n# extracting quality labels\nall_labels = train_with_target['newlabel'].values\n# a test to see what the inputs look like\nall_inputs[:2]","9d14c1b4":"#Train and split the data\nX_train, X_test, y_train, y_test = train_test_split(all_inputs, all_labels, test_size= 0.2, random_state=42)","8e72efb5":"#Test of firsts values\nX_train[:1]","205f649e":"#trying decision tree classfier \nfrom sklearn.tree import DecisionTreeClassifier\n\n\n# Create the classifier\ndecision_tree_classifier = DecisionTreeClassifier()\n\n\n# Train the classifier on the training set\ndecision_tree_classifier.fit(X_train, y_train)\n\n\n# Validate the classifier on the testing set using classification accuracy\ndecision_tree_classifier.score(X_test, y_test)","946634ee":"rfc = RandomForestClassifier(n_estimators = 200)\nrfc.fit(X_train, y_train)\npred_rfc = rfc.predict(X_test)","5dbf42d3":"pred_rfc[:20]","a7a6246c":"X_test[:2]","07c06f71":"# Let's see  the RFC efficients \nprint(classification_report(y_test, pred_rfc))\nprint(confusion_matrix(y_test,pred_rfc))","f80af0b7":"\n# Validate the classifier on the testing set using classification accuracy\nrfc.score(X_test, y_test)","3cea7ad5":"Clf = svm.SVC()\nClf.fit(X_train,y_train)\npref_clf = Clf.predict(X_test)","bff5a5e0":"# Let's see  the SVM efficients \nprint(classification_report(y_test, pred_rfc))\nprint(confusion_matrix(y_test,pred_rfc))","5a66afa2":"\n# Validate the classifier on the testing set using classification accuracy\nClf.score(X_test, y_test)","37a434a9":"#selecting the models and the model names in an array\nmodels=[LogisticRegression(),\n        LinearSVC(),\n        SVC(kernel='rbf'),\n        KNeighborsClassifier(),\n        RandomForestClassifier(),\n        DecisionTreeClassifier(),\n        GradientBoostingClassifier(),\n        GaussianNB()]\nmodel_names=['Logistic Regression',\n             'Linear SVM',\n             'rbf SVM',\n             'K-Nearest Neighbors',\n             'Random Forest Classifier',\n             'Decision Tree',\n             'Gradient Boosting Classifier',\n             'Gaussian NB']\n\n\n# creating an accuracy array and a matrix to join the accuracy of the models\n# and the name of the models so we can read the results easier\nacc=[]\nm={}\n\n\n# next we're going to iterate through the models, and get the accuracy for each\nfor model in range(len(models)):\n     clf=models[model]\n     clf.fit(X_train,y_train)\n     pred=clf.predict(X_test)\n     acc.append(accuracy_score(pred,y_test))\n\n\nm={'Algorithm':model_names,'Accuracy':acc}\n\n\n# just putting the matrix into a data frame and listing out the results\nacc_frame=pd.DataFrame(m)\nacc_frame","5af43fea":"random_forest_classifier = RandomForestClassifier()\n\n\n# setting up the parameters for our grid search\n# You can check out what each of these parameters mean on the Scikit webiste!\n# http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\nparameter_grid = {'n_estimators': [10, 25, 50, 100, 200],\n'max_features': ['auto', 'sqrt', 'log2'],\n'criterion': ['gini', 'entropy'],\n'max_features': [1, 2, 3, 4]}\n\n\n# Stratified K-Folds cross-validator allows us mix up the given test\/train data per run\n# with k-folds each test set should not overlap across all shuffles. This allows us to \n# ultimately have \"more\" test data for our model\ncross_validation = StratifiedKFold(n_splits=10)\n\n\n# running the grid search function with our random_forest_classifer, our parameter grid\n# defineda bove, and our cross validation method\ngrid_search = GridSearchCV(random_forest_classifier,\nparam_grid=parameter_grid,\ncv=cross_validation)\n\n\n# using the defined grid search above, we're going to test it out on our\n# data set\ngrid_search.fit(all_inputs, all_labels)\n\n\n\n# printing the best scores, parameters, and estimator for our Random Forest classifer\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\n\n\ngrid_search.best_estimator_","bcf57040":"RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n\n          max_depth=None, max_features=1, max_leaf_nodes=None,\n\n          min_impurity_decrease=0.0, min_impurity_split=None,\n\n          min_samples_leaf=1, min_samples_split=2,\n\n          min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n\n          oob_score=False, random_state=None, verbose=0,\n\n          warm_start=False)","3859cecc":"random_forest_classifier = grid_search.best_estimator_\n\n\nrf_df = pd.DataFrame({'accuracy': cross_val_score(random_forest_classifier, all_inputs, all_labels, cv=10),\n                      'classifier': ['Random Forest'] * 10})\nrf_df.mean()","e7aa777f":"sns.boxplot(x='classifier', y='accuracy', data=rf_df)\nsns.stripplot(x='classifier', y='accuracy', data=rf_df, jitter=True, color='black')","59b3de1e":"### The dataframe is now ready  to serve the ML model","3e6f1b8e":"### We see that we have 11 failures in both sources, the data is well labeled","734492d3":"## compare labeled data with effective failures","0d0add05":"# Librairies","ca4e6b77":"![image.png](attachment:image.png)","9061546d":" ## Preparing the ML model: ","89edd20b":"# SVM Classifier","3566432a":"## Data cleaned","2e7a0f3c":"## We now see again an accuracy of 94 % with the SVM classifier","eaf18909":"# Test of the new optimised Random forest classifier","c4a6d8a8":"![image.png](attachment:image.png)","63ebaf0c":"### It allows us to create a ML model able to say if given the information, a specific machine will have an issue in the following 15 cycles.","4f910ca4":"# At the End, we delivered a ML model able to predict with an accuracy of 95% if the tested machine will have an issue in the following 15 Cycles. \n# Thanks to this algorithm, Wonky Machines Ltd will improve their maintenance process in order to save money","e40b27cc":"# Testing other ML methods using a loop and comparing results!","cc5c0a40":"## Features Ranking :","3df72c3f":"### First : we change the label defined to get : 1 if the machine will have an issue in the following 15 Cycles.  0 if not.\n","759b850a":"![image.png](attachment:image.png)","f7cf0c8c":"## Random Forest Classifier\u00b6","818d2571":"# Data Cleaning","197936f3":"![image.png](attachment:image.png)","3ea603ec":"# Basic info on the Database","4f727b7b":"# The goal of this study is to improve the maintenance process of  Wonky Machines Ltd in order to reduce production costs and therefore save money. \n","7dee15da":"## Data ready to be compiled into ML tools - Test of  a few ML tools","956e3a54":"## But we also drop the features that the EDA showed to be not significant","46992347":"# Data import","b9117730":"## Labelisation of the data ; 1 for defect detection and 0 for no defect","e9c3e246":"## Data Exploration and Analysis","ad8d134e":"## We now see an accuracy of 94 % with the Random Forest classifier","6031ce00":"## Thanks to the last plot, we now know that the sensors 1,5,10,16,19,18 are insignificant compared to the sensors 15,11 etc..","e74cf0e9":"### The EDA shows the the machines seem to all be of the same kind. Knowing that we will anonymise the ID of every machine in ML models. We drop ID. \n","28a59780":"### We already see an accuracy of 90 % with the Decision tree classifier","6e86ec4d":"![image.png](attachment:image.png)","09e6b938":"## Machine Learning models","f698ff5d":"## Adding a new feature : The current lifetime of a machine","2dd0be3a":"# How to optimise parameters for a specific algorithm. Random forest classifier case","d53f2aa4":"## The EDA shows the machines seem to all be of the same kind. Knowing that we will anonymise the ID of every machine in ML models. We drop ID. we also drop unnecessary columns"}}