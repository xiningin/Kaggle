{"cell_type":{"65f9efa1":"code","09184312":"code","622c8eed":"code","5d2c6a53":"code","27cfa44e":"code","27adaa49":"code","7e386814":"code","54a0dc3d":"code","a22fc609":"code","22c6ef8a":"code","7e06bf95":"markdown","4cc75eb9":"markdown","700ac6e5":"markdown","98ad573a":"markdown","5f081d4e":"markdown","15914fb3":"markdown","7f343da4":"markdown","da83cf64":"markdown","84c209ac":"markdown"},"source":{"65f9efa1":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py\nimport tokenization","09184312":"#setting a seed for reproducability\nSEED = 2718\ndef seed_everything(seed):\n    np.random.seed(seed)\n    tf.random.set_seed(seed) \n    \nseed_everything(SEED) ","622c8eed":"#reading input data with pandas\ntrain = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\n\n#visualizing some of the tweets\nfor i, val in enumerate(train.iloc[:2][\"text\"].to_list()):\n    print(\"Tweet {}: {}\".format(i+1, val))","5d2c6a53":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","27cfa44e":"def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    #could be pooled_output, sequence_output yet sequence output provides for each input token (in context)\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    \n    #specifying optimizer\n    model.compile(Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","27adaa49":"#load uncased bert model\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","7e386814":"#vocab file from pre-trained BERT for tokenization\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n\n#returns true\/false depending on if we selected cased\/uncased bert layer\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n\n#Create the tokenizer\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n\n#tokenizing the training and testing data\ntrain_input = bert_encode(train.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\ntrain_labels = train.target.values","54a0dc3d":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","a22fc609":"checkpoint = ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.1,\n    epochs=3,\n    callbacks=[checkpoint],\n    batch_size=16\n)","22c6ef8a":"test_pred = model.predict(test_input)\n\nsubmission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('submission.csv', index=False)","7e06bf95":"## <center>build_model function<\/center>\n\nThe first three components of the function are basically preprocessing plain text inputs into the input format expected by the roBERTa model.\n\n### input_word_ids\n- Basically maps each word to its token id. There can be multiple different values that correspond with the same word. For example, \"smell\" could be encoded both as 883 and 789.\n<br><\/br>\n```\ntext = \"I love this notebook. It is Great.\"\ninput_word_ids = [10, 235, 123, 938, 184, 301, 567]\n```\n\n### the input_mask\n- Shows where the sentence begins, and where it ends using an array. All input tokens that are not padding are given a value of 1, and all values that are padding are given 0. If the sentence exceeds that max_length, then the entire vector will be of 1's.\n<br><\/br>\n```\ntext = \"I love this notebook. It is Great.\"\ninput_mask = [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n```\n\n### segment_ids\n- This component is still a little vague for me, but from my understanding, it is recognizing segments of the text. The start of each segment has a 1 in the array, and other components and padding all have a zero. I am unsure as to whether this corresponds to the end of sentences or paragraphs, but if you can explain this better please do so in the comments below!\n<br><\/br>\n```\ntext = \"I love this notebook. It is Great.\"\nsegment_ids = [1, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n```","4cc75eb9":"In the next cell, we are setting up the tokenizer that will be used to preprocess our input data to what BERT understands. We have to specify a vocab file so that the tokenizer knows what number to encode each word as then we have to specify whether we want uncased or cased text. We will use the same vocab_file that the pre-trained model was trained on (Google's SentencePiece in this case) and we will also use the same case that the model was built for (uncased).\n\nFinally, once we have these two variables, we create the tokenizer and tokenize the training and testing data using the bert_encode function that we created above. ","700ac6e5":"## <center>bert_encode function<\/center>\n\n\n### tokenizer\nWe are using the [Tensorflow Research's BERT tokenization method](https:\/\/github.com\/tensorflow\/models\/blob\/master\/official\/nlp\/bert\/tokenization.py). This tokenization method can be thought of as three steps.\n\n- Text Normalization\n    - The first part of the tokenizer converts the text to lowercase (given that we are using the uncased version of roBERTa), converts whitespace to spaces, and strips out accent markers.\n    ```\n    \"Alex P\u00e4ttason's, \"  -> \"alex pattason's,\"\n    ```\n    <br><\/br>\n- Punctuation splitting\n    - This next step adds spaces on each side of all \"punctuation\". Note that this includes any non-letter\/number\/space ASCII characters (ie including \\$, \\@). See more of this in the Docs. \n    ```\n    \"Alex P\u00e4ttason's, \"  -> \"alex pattason ' s ,\"\n    ```\n    <br><\/br>\n- WordPiece tokenization\n    - This step applies what is called whitespace tokenization to the output of the process above, and apply's WordPiece tokenization to each word separately. See the example below.\n    ```\n    \"Alex P\u00e4ttason's, \"  -> \"alex pat ##ta ##son ' s ,\"\n    ```\n   \n### tags\nThe next part of the function reduces the length of the text by the max_length that we have specified and adds [CLS] and [SEP] tags to the end of the array. The [CLS] tag is short for classification and indicates the start of the sentence. Similarly, the [SEP] tag indicates the end of the sentence.\n\n### convert_tokens_to_ids + pad_masks\n\nWe then use the tokenizer method to replace the string representation of words with integers. We also create the input mask (AKA pad_masks), and the segment id's. Note that we are not fulfilling the segment_ids full benefits below as we are only passing an array of zeros. More on the tokens, pad_masks, and segment_ids further in the notebook.","98ad573a":"## Make Prediction\n\nUsing the model to make predictions on the testing set. We round the prediction to 1 or 0. 1 is a disaster tweet, and 0 is a regular tweet.","5f081d4e":"## Build Model + Preprocess Data\n\nThe first cell below is basically loading in the version of the roBERTa model that we want to use. We are using a Large uncased model. The most simple way to use a roBERTa model and modify it to a specific use case is to set it as a KerasLayer.\n\nNote there are many different variations of BERT models that you can look through here --> [TFhub Bert](https:\/\/tfhub.dev\/google\/collections\/bert\/1)\n","15914fb3":"## <center>NLP Disaster Tweet Classification w\/ roBERTa<\/center>\n\nThis notebook implements a roBERTa model in Tensorflow to evaluate whether a tweet is about a disaster or not. I have provided explanations throughout to provide a better understanding of what the roBERTa model is actually doing.\n\nI got most of my understanding for this notebook from a good discussion thread about the roBERTa model from @Chris Deotte explaining how the components of the model work and his starter notebook on the roBERTa model. These are the top two links below. I also found the Tensorflow documentation quite informative as well (third and fourth links).\n\n### Useful Links\n\nThis is a collection of links that I found helpful in understanding the structure of the roBERTa model, how it works, and more.\n\n- [TensorFlow roBERTa Explained Discussion](https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/143281#807401)\n- [tensorflow-roberta-0-705 Notebook](https:\/\/www.kaggle.com\/cdeotte\/tensorflow-roberta-0-705)\n- [Bert_en_uncased Docs](https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/4)\n- [bert_en_uncased_preprocess](https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3)\n- [How to get meaning from text with language model BERT](https:\/\/www.youtube.com\/watch?v=-9vVhYEXeyQ)\n- [TF Bert Tokenizer](https:\/\/github.com\/google-research\/bert)","7f343da4":"## Training Model\n\nThe next cell is a simple way of training the model using Keras. We have included the built-in ModelCheckpoint callback to only save the model that has the highest validation accuracy. This ensures we are only saving the best models. \n\n\nWe could decrease the randomness of the split by doing some sort of a stratified split, or cross-validation, but this will do for now.","da83cf64":"Having a look at the model summary. We can see the three input layers that we created followed by the roBERTa model which is in the keras_layer. We have the final dense layer which predicts the sentiment of the tweet on a scale of 0-1. ","84c209ac":"Reading in the data using pandas. We will tokenize the text later in the notebook."}}