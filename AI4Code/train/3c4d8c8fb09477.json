{"cell_type":{"798b833f":"code","1f76195b":"code","c29e4476":"code","c404bbc2":"code","93568d63":"code","6cc89e2f":"code","3e18d5d3":"code","dd58898f":"code","3b065cbd":"code","da887b62":"code","b9c7f8b2":"code","874106d2":"code","09b3ca8b":"code","cb45db0e":"code","c84067e6":"markdown","67732ff3":"markdown","d5348808":"markdown","535c4e81":"markdown","e57eef39":"markdown","70bc202d":"markdown","856f136d":"markdown","52246e1e":"markdown","d7216d90":"markdown","fba2ec26":"markdown","5958964e":"markdown","7993cd8c":"markdown","cd046f2d":"markdown","7757e816":"markdown"},"source":{"798b833f":"import pandas as pd\nimport numpy as np\nimport datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import Image\nimport math\nfrom scipy.stats import kendalltau\n\nfrom IPython.display import clear_output\nimport timeit\n\nimport warnings\nwarnings.filterwarnings('ignore')","1f76195b":"#kills = pd.read_csv('C:\\\\Users\\\\Phil\\\\Documents\\\\LoL Model\\\\kills.csv')\n#matchinfo = pd.read_csv('C:\\\\Users\\\\Phil\\\\Documents\\\\LoL Model\\\\matchinfo.csv')\n#monsters = pd.read_csv('C:\\\\Users\\\\Phil\\\\Documents\\\\LoL Model\\\\monsters.csv')\n#structures = pd.read_csv('C:\\\\Users\\\\Phil\\\\Documents\\\\LoL Model\\\\structures.csv')\n\nkills = pd.read_csv('..\/input\/kills.csv')\nmatchinfo = pd.read_csv('..\/input\/matchinfo.csv')\nmonsters = pd.read_csv('..\/input\/monsters.csv')\nstructures = pd.read_csv('..\/input\/structures.csv')\n\n#gold = pd.read_csv('C:\\\\Users\\\\Phil\\\\Documents\\\\LoL Model\\\\gold.csv')\ngold = pd.read_csv('..\/input\/gold.csv')","c29e4476":"# Add ID column based on last 16 digits in match address for simpler matching\ngold = gold[gold['Type']==\"golddiff\"]\n\nmatchinfo['id'] = matchinfo['Address'].astype(str).str[-16:]\nkills['id'] = kills['Address'].astype(str).str[-16:]\nmonsters['id'] = monsters['Address'].astype(str).str[-16:]\nstructures['id'] = structures['Address'].astype(str).str[-16:]\ngold['id'] = gold['Address'].astype(str).str[-16:]\n\n\n\n# Dragon became multiple types in patch v6.9 (http:\/\/leagueoflegends.wikia.com\/wiki\/V6.9) \n# so we remove and games before this change occured and only use games with the new dragon system\n\nold_dragon_id = monsters[ monsters['Type']==\"DRAGON\"]['id'].unique()\nold_dragon_id\n\nmonsters = monsters[ ~monsters['id'].isin(old_dragon_id)]\nmonsters = monsters.reset_index()\n\nmatchinfo = matchinfo[ ~matchinfo['id'].isin(old_dragon_id)]\nmatchinfo = matchinfo.reset_index()\n\nkills = kills[ ~kills['id'].isin(old_dragon_id)]\nkills = kills.reset_index()\n\nstructures = structures[ ~structures['id'].isin(old_dragon_id)]\nstructures = structures.reset_index()\n\ngold = gold[ ~gold['id'].isin(old_dragon_id)]\ngold = gold.reset_index()\n\n#Transpose Gold table, columns become matches and rows become minutes\n\ngold_T = gold.iloc[:,3:-1].transpose()\n\n\ngold2 = pd.DataFrame()\n\nstart = timeit.default_timer()\nfor r in range(0,len(gold)):\n    clear_output(wait=True)\n    \n    # Select each match column, drop any na rows and find the match id from original gold table\n    gold_row = gold_T.iloc[:,r]\n    gold_row = gold_row.dropna()\n    gold_row_id = gold['id'][r]\n    \n    # Append into table so that each match and event is stacked on top of one another    \n    gold2 = gold2.append(pd.DataFrame({'id':gold_row_id,'GoldDiff':gold_row}))\n    \n    \n    stop = timeit.default_timer()\n   \n                 \n    print(\"Current progress:\",np.round(r\/len(gold) *100, 2),\"%\")        \n    print(\"Current run time:\",np.round((stop - start)\/60,2),\"minutes\")\n        \n\ngold3 = gold2[['id','GoldDiff']]\n\n### Create minute column with index, convert from 'min_1' to just the number\ngold3['Minute'] = gold3.index.to_series()\ngold3['Minute'] = np.where(gold3['Minute'].str[-2]==\"_\", gold3['Minute'].str[-1],gold3['Minute'].str[-2:])\ngold3[gold3['Minute']==\"pe\"]\n\ngold3 = gold3.iloc[1:,]\ngold3['Minute'] = gold3['Minute'].astype(int)\ngold3 = gold3.reset_index()\ngold3 = gold3.sort_values(by=['id','Minute'])\n\n\n\n# Gold difference from data is relative to blue team's perspective,\n# therefore we reverse this by simply multiplying amount by -1\ngold3['GoldDiff'] = gold3['GoldDiff']*-1\n\n\ngold4 = gold3\n\nmatchinfo2 = matchinfo[['id','rResult','gamelength']]\nmatchinfo2['gamlength'] = matchinfo2['gamelength'] + 1\nmatchinfo2['index'] = 'min_'+matchinfo2['gamelength'].astype(str)\nmatchinfo2['rResult2'] =  np.where(matchinfo2['rResult']==1,999999,-999999)\nmatchinfo2 = matchinfo2[['index','id','rResult2','gamelength']]\nmatchinfo2.columns = ['index','id','GoldDiff','Minute']\n\n\ngold4 = gold4.append(matchinfo2)\n\n\nkills = kills[ kills['Time']>0]\n\nkills['Minute'] = kills['Time'].astype(int)\n\nkills['Team'] = np.where( kills['Team']==\"rKills\",\"Red\",\"Blue\")\n\n# For the Kills table, we need decided to group by the minute in which the kills took place and averaged \n# the time of the kills which we use later for the order of events\n\nf = {'Time':['mean','count']}\n\nkillsGrouped = kills.groupby( ['id','Team','Minute'] ).agg(f).reset_index()\nkillsGrouped.columns = ['id','Team','Minute','Time Avg','Count']\nkillsGrouped = killsGrouped.sort_values(by=['id','Minute'])\n\n\nstructures = structures[ structures['Time']>0]\n\nstructures['Minute'] = structures['Time'].astype(int)\nstructures['Team'] = np.where(structures['Team']==\"bTowers\",\"Blue\",\n                        np.where(structures['Team']==\"binhibs\",\"Blue\",\"Red\"))\nstructures2 = structures.sort_values(by=['id','Minute'])\n\nstructures2 = structures2[['id','Team','Time','Minute','Type']]\n\n\nmonsters['Type2'] = np.where( monsters['Type']==\"FIRE_DRAGON\", \"DRAGON\",\n                    np.where( monsters['Type']==\"EARTH_DRAGON\",\"DRAGON\",\n                    np.where( monsters['Type']==\"WATER_DRAGON\",\"DRAGON\",       \n                    np.where( monsters['Type']==\"AIR_DRAGON\",\"DRAGON\",   \n                             monsters['Type']))))\n\nmonsters = monsters[ monsters['Time']>0]\n\nmonsters['Minute'] = monsters['Time'].astype(int)\n\nmonsters['Team'] = np.where( monsters['Team']==\"bDragons\",\"Blue\",\n                   np.where( monsters['Team']==\"bHeralds\",\"Blue\",\n                   np.where( monsters['Team']==\"bBarons\", \"Blue\", \n                           \"Red\")))\n\nmonsters = monsters[['id','Team','Time','Minute','Type2']]\nmonsters.columns = ['id','Team','Time','Minute','Type']\n\n\nGoldstackedData = gold4.merge(killsGrouped, how='left',on=['id','Minute'])\n \nmonsters_structures_stacked = structures2.append(monsters[['id','Team','Minute','Time','Type']])\n\nGoldstackedData2 = GoldstackedData.merge(monsters_structures_stacked, how='left',on=['id','Minute'])\n\nGoldstackedData2 = GoldstackedData2.sort_values(by=['id','Minute'])\n\nGoldstackedData3 = GoldstackedData2\nGoldstackedData3['Time2'] = GoldstackedData3['Time'].fillna(GoldstackedData3['Time Avg']).fillna(GoldstackedData3['Minute'])\nGoldstackedData3['Team'] = GoldstackedData3['Team_x'].fillna(GoldstackedData3['Team_y'])\nGoldstackedData3 = GoldstackedData3.sort_values(by=['id','Time2'])\n\nGoldstackedData3['EventNum'] = GoldstackedData3.groupby('id').cumcount()+1\n\nGoldstackedData3 = GoldstackedData3[['id','EventNum','Team','Minute','Time2','GoldDiff','Count','Type']]\n\nGoldstackedData3.columns = ['id','EventNum','Team','Minute','Time','GoldDiff','KillCount','Struct\/Monster']\n\n\n# We then add an 'Event' column to merge the columns into one, where kills are now\n# simple labelled as 'KILLS'\n\nGoldstackedData3['Event'] = np.where(GoldstackedData3['KillCount']>0,\"KILLS\",None)\nGoldstackedData3['Event'] = GoldstackedData3['Event'].fillna(GoldstackedData3['Struct\/Monster'])\n\nGoldstackedData3['Event'] = GoldstackedData3['Event'].fillna(\"NONE\")\n\nGoldstackedData3['GoldDiff2'] = np.where( GoldstackedData3['GoldDiff']== 999999,\"WIN\",\n                                np.where( GoldstackedData3['GoldDiff']==-999999, 'LOSS',\n                                         \n    \n                                np.where((GoldstackedData3['GoldDiff']<1000) & (GoldstackedData3['GoldDiff']>-1000),\n                                        \"EVEN\",\n                                np.where( (GoldstackedData3['GoldDiff']>=1000) & (GoldstackedData3['GoldDiff']<2500),\n                                         \"SLIGHTLY_AHEAD\",\n                                np.where( (GoldstackedData3['GoldDiff']>=2500) & (GoldstackedData3['GoldDiff']<5000),\n                                         \"AHEAD\",\n                                np.where( (GoldstackedData3['GoldDiff']>=5000),\n                                         \"VERY_AHEAD\",\n                                         \n                                np.where( (GoldstackedData3['GoldDiff']<=-1000) & (GoldstackedData3['GoldDiff']>-2500),\n                                         \"SLIGHTLY_BEHIND\",\n                                np.where( (GoldstackedData3['GoldDiff']<=-2500) & (GoldstackedData3['GoldDiff']>-5000),\n                                         \"BEHIND\",\n                                np.where( (GoldstackedData3['GoldDiff']<=-5000),\n                                         \"VERY_BEHIND\",\"ERROR\"\n                                        \n                                        )))))))))\n\nGoldstackedData3['Next_Min'] = GoldstackedData3['Minute']+1\n\n\nGoldstackedData4 = GoldstackedData3.merge(gold4[['id','Minute','GoldDiff']],how='left',left_on=['id','Next_Min'],\n                                         right_on=['id','Minute'])\n\nGoldstackedData4['GoldDiff2_Next'] =  np.where( GoldstackedData4['GoldDiff_y']== 999999,\"WIN\",\n                                np.where( GoldstackedData4['GoldDiff_y']==-999999, 'LOSS',\n                                         \n    \n                                np.where((GoldstackedData4['GoldDiff_y']<1000) & (GoldstackedData4['GoldDiff_y']>-1000),\n                                        \"EVEN\",\n                                np.where( (GoldstackedData4['GoldDiff_y']>=1000) & (GoldstackedData4['GoldDiff_y']<2500),\n                                         \"SLIGHTLY_AHEAD\",\n                                np.where( (GoldstackedData4['GoldDiff_y']>=2500) & (GoldstackedData4['GoldDiff_y']<5000),\n                                         \"AHEAD\",\n                                np.where( (GoldstackedData4['GoldDiff_y']>=5000),\n                                         \"VERY_AHEAD\",\n                                         \n                                np.where( (GoldstackedData4['GoldDiff_y']<=-1000) & (GoldstackedData4['GoldDiff_y']>-2500),\n                                         \"SLIGHTLY_BEHIND\",\n                                np.where( (GoldstackedData4['GoldDiff_y']<=-2500) & (GoldstackedData4['GoldDiff_y']>-5000),\n                                         \"BEHIND\",\n                                np.where( (GoldstackedData4['GoldDiff_y']<=-5000),\n                                         \"VERY_BEHIND\",\"ERROR\"\n                                        \n                                        )))))))))\nGoldstackedData4 = GoldstackedData4[['id','EventNum','Team','Minute_x','Time','Event','GoldDiff2','GoldDiff2_Next']]\nGoldstackedData4.columns = ['id','EventNum','Team','Minute','Time','Event','GoldDiff2','GoldDiff2_Next']\n\nGoldstackedData4['Event'] = np.where( GoldstackedData4['Team']==\"Red\", \"+\"+GoldstackedData4['Event'],\n                                np.where(GoldstackedData4['Team']==\"Blue\", \"-\"+GoldstackedData4['Event'], \n                                         GoldstackedData4['Event']))\n\n\n\n\n\n# Errors are caused due to game ending in minute and then there is no 'next_min' info for this game but our method expects there to be\nGoldstackedData4 = GoldstackedData4[GoldstackedData4['GoldDiff2_Next']!=\"ERROR\"]\nGoldstackedData4[GoldstackedData4['GoldDiff2_Next']==\"ERROR\"]\n\n\nGoldstackedDataFINAL = GoldstackedData4\nGoldstackedDataFINAL['Min_State_Action_End'] = ((GoldstackedDataFINAL['Minute'].astype(str)) + \"_\"\n                                       + (GoldstackedDataFINAL['GoldDiff2'].astype(str)) + \"_\"\n                                       + (GoldstackedDataFINAL['Event'].astype(str)) + \"_\"  \n                                       + (GoldstackedDataFINAL['GoldDiff2_Next'].astype(str))\n                                      )\n\nGoldstackedDataFINAL['MSAE'] = ((GoldstackedDataFINAL['Minute'].astype(str)) + \"_\"\n                                       + (GoldstackedDataFINAL['GoldDiff2'].astype(str)) + \"_\"\n                                       + (GoldstackedDataFINAL['Event'].astype(str)) + \"_\"  \n                                       + (GoldstackedDataFINAL['GoldDiff2_Next'].astype(str))\n                                      )\n\n\ngoldMDP = GoldstackedDataFINAL[['Minute','GoldDiff2','Event','GoldDiff2_Next']]\ngoldMDP.columns = ['Minute','State','Action','End']\ngoldMDP['Counter'] = 1\n\ngoldMDP2 = goldMDP.groupby(['Minute','State','Action','End']).count().reset_index()\ngoldMDP2['Prob'] = goldMDP2['Counter']\/(goldMDP2['Counter'].sum())\n\ngoldMDP3 = goldMDP.groupby(['Minute','State','Action']).count().reset_index()\ngoldMDP3['Prob'] = goldMDP3['Counter']\/(goldMDP3['Counter'].sum())\n\n\n\ngoldMDP4 = goldMDP2.merge(goldMDP3[['Minute','State','Action','Prob']], how='left',on=['Minute','State','Action'] )\n\ngoldMDP4['GivenProb'] = goldMDP4['Prob_x']\/goldMDP4['Prob_y']\ngoldMDP4 = goldMDP4.sort_values('GivenProb',ascending=False)\ngoldMDP4['Next_Minute'] = goldMDP4['Minute']+1\n    \n\n","c404bbc2":"goldMDP4.sort_values(['Minute','State']).head(20)","93568d63":"def MCModelv6(data, alpha, gamma, epsilon, reward, StartState, StartMin, StartAction, num_episodes, Max_Mins):\n    \n    # Initiatise variables appropiately\n    \n    data['V'] = 0\n    data_output = data\n    \n    outcomes = pd.DataFrame()\n    episode_return = pd.DataFrame()\n    actions_output = pd.DataFrame()\n    V_output = pd.DataFrame()\n    \n    \n    Actionist = [\n       'NONE',\n       'KILLS', 'OUTER_TURRET', 'DRAGON', 'RIFT_HERALD', 'BARON_NASHOR',\n       'INNER_TURRET', 'BASE_TURRET', 'INHIBITOR', 'NEXUS_TURRET',\n       'ELDER_DRAGON'] \n        \n    for e in range(0,num_episodes):\n        clear_output(wait=True)\n        \n        action = []\n        \n        current_min = StartMin\n        current_state = StartState\n        \n        \n        \n        data_e1 = data\n    \n    \n        actions = pd.DataFrame()\n\n        for a in range(0,100):\n            \n            action_table = pd.DataFrame()\n       \n            # Break condition if game ends or gets to a large number of mins \n            if (current_state==\"WIN\") | (current_state==\"LOSS\") | (current_min==Max_Mins):\n                continue\n            else:\n                if a==0:\n                    data_e1=data_e1\n                   \n                elif (len(individual_actions_count[individual_actions_count['Action']==\"+RIFT_HERALD\"])==1):\n                    data_e1_e1 = data_e1[(data_e1['Action']!='+RIFT_HERALD')|(data_e1['Action']!='-RIFT_HERALD')]\n                    \n                elif (len(individual_actions_count[individual_actions_count['Action']==\"-RIFT_HERALD\"])==1):\n                    data_e1 = data_e1[(data_e1['Action']!='+RIFT_HERALD')|(data_e1['Action']!='-RIFT_HERALD')]\n                \n                elif (len(individual_actions_count[individual_actions_count['Action']==\"+OUTER_TURRET\"])==3):\n                    data_e1 = data_e1[data_e1['Action']!='+OUTER_TURRET']\n                elif (len(individual_actions_count[individual_actions_count['Action']==\"-OUTER_TURRET\"])==3):\n                    data_e1 = data_e1[data_e1['Action']!='-OUTER_TURRET']\n                    \n                elif (len(individual_actions_count[individual_actions_count['Action']==\"+INNER_TURRET\"])==3):\n                    data_e1 = data_e1[data_e1['Action']!='+INNER_TURRET']\n                elif (len(individual_actions_count[individual_actions_count['Action']==\"-INNER_TURRET\"])==3):\n                    data_e1 = data_e1[data_e1['Action']!='-INNER_TURRET']\n                    \n                elif (len(individual_actions_count[individual_actions_count['Action']==\"+BASE_TURRET\"])==3):\n                    data_e1 = data_e1[data_e1['Action']!='+BASE_TURRET']\n                elif (len(individual_actions_count[individual_actions_count['Action']==\"-BASE_TURRET\"])==3):\n                    data_e1 = data_e1[data_e1['Action']!='-BASE_TURRET']\n                    \n                elif (len(individual_actions_count[individual_actions_count['Action']==\"+INHIBITOR\"])==3):\n                    data_e1 = data_e1[data_e1['Action']!='+INHIBITOR']\n                elif (len(individual_actions_count[individual_actions_count['Action']==\"-INHIBITOR\"])==3):\n                    data_e1 = data_e1[data_e1['Action']!='-INHIBITOR']\n                elif (len(individual_actions_count[individual_actions_count['Action']==\"+NEXUS_TURRET\"])==2):\n                    data_e1 = data_e1[data_e1['Action']!='+NEXUS_TURRET']\n                elif (len(individual_actions_count[individual_actions_count['Action']==\"-NEXUS_TURRET\"])==2):\n                    data_e1 = data_e1[data_e1['Action']!='-NEXUS_TURRET']\n                \n                       \n                else:\n                    data_e1 = data_e1\n                    \n                # Break condition if we do not have enough data    \n                if len(data_e1[(data_e1['Minute']==current_min)&(data_e1['State']==current_state)])==0:\n                    continue\n                else:             \n\n                    \n                    # Greedy Selection:\n                    # If this is our first action and start action is non, select greedily. \n                    # Else, if first actions is given in our input then we use this as our start action. \n                    # Else for other actions, if it is the first episode then we have no knowledge so randomly select actions\n                    # Else for other actions, we randomly select actions a percentage of the time based on our epsilon and greedily (max V) for the rest \n                    \n                    \n                    if   (a==0) & (StartAction is None):\n                        random_action = data_e1[(data_e1['Minute']==current_min)&(data_e1['State']==current_state)].sample()\n                        random_action = random_action.reset_index()\n                        current_action = random_action['Action'][0]\n                    elif (a==0):\n                        current_action =  StartAction\n                    \n                    elif (e==0) & (a>0):\n                        random_action = data_e1[(data_e1['Minute']==current_min)&(data_e1['State']==current_state)].sample()\n                        random_action = random_action.reset_index()\n                        current_action = random_action['Action'][0]\n                    \n                    elif (e>0) & (a>0):\n                        epsilon = epsilon\n                        greedy_rng = np.round(np.random.random(),2)\n                        if (greedy_rng<=epsilon):\n                            random_action = data_e1[(data_e1['Minute']==current_min)&(data_e1['State']==current_state)].sample()\n                            random_action = random_action.reset_index()\n                            current_action = random_action['Action'][0]\n                        else:\n                            greedy_action = (\n                            \n                                data_e1[(data_e1['Minute']==current_min)&(data_e1['State']==current_state)][\n                                    \n                                    data_e1[(data_e1['Minute']==current_min)&(data_e1['State']==current_state)]['V']==data_e1[(data_e1['Minute']==current_min)&(data_e1['State']==current_state)]['V'].max()\n                                \n                                ])\n                                \n                            greedy_action = greedy_action.reset_index()\n                            current_action = greedy_action['Action'][0]\n                            \n                  \n                    \n                        \n\n                    data_e = data_e1[(data_e1['Minute']==current_min)&(data_e1['State']==current_state)&(data_e1['Action']==current_action)]\n\n                    data_e = data_e[data_e['GivenProb']>0]\n\n\n\n\n\n                    data_e = data_e.sort_values('GivenProb')\n                    data_e['CumProb'] = data_e['GivenProb'].cumsum()\n                    data_e['CumProb'] = np.round(data_e['CumProb'],4)\n\n\n                    rng = np.round(np.random.random()*data_e['CumProb'].max(),4)\n                    action_table = data_e[ data_e['CumProb'] >= rng]\n                    action_table = action_table[ action_table['CumProb'] == action_table['CumProb'].min()]\n                    action_table = action_table.reset_index()\n\n\n                    action = current_action\n                    next_state = action_table['End'][0]\n                    next_min = current_min+1\n\n\n                    if next_state == \"WIN\":\n                        step_reward = 10*(gamma**a)\n                    elif next_state == \"LOSS\":\n                        step_reward = -10*(gamma**a)\n                    else:\n                        step_reward = action_table['Reward']*(gamma**a)\n\n                    action_table['StepReward'] = step_reward\n\n\n                    action_table['Episode'] = e\n                    action_table['Action_Num'] = a\n\n                    current_action = action\n                    current_min = next_min\n                    current_state = next_state\n\n\n                    actions = actions.append(action_table)\n\n                    individual_actions_count = actions\n                    \n        print(\"Current progress:\", np.round((e\/num_episodes)*100,2),\"%\")\n\n        actions_output = actions_output.append(actions)\n                \n        episode_return = actions['StepReward'].sum()\n\n                \n        actions['Return']= episode_return\n                \n        data_output = data_output.merge(actions[['Minute','State','Action','End','Return']], how='left',on =['Minute','State','Action','End'])\n        data_output['Return'] = data_output['Return'].fillna(0)    \n             \n            \n        data_output['V'] = np.where(data_output['Return']==0,data_output['V'],data_output['V'] + alpha*(data_output['Return']-data_output['V']))\n        \n        data_output = data_output.drop('Return', 1)\n\n        \n        for actions in data_output[(data_output['Minute']==StartMin)&(data_output['State']==StartState)]['Action'].unique():\n            V_outputs = pd.DataFrame({'Index':[str(e)+'_'+str(actions)],'Episode':e,'StartMin':StartMin,'StartState':StartState,'Action':actions,\n                                      'V':data_output[(data_output['Minute']==StartMin)&(data_output['State']==StartState)&(data_output['Action']==actions)]['V'].sum()\n                                     })\n            V_output = V_output.append(V_outputs)\n        \n        if current_state==\"WIN\":\n            outcome = \"WIN\"\n        elif current_state==\"LOSS\":\n            outcome = \"LOSS\"\n        else:\n            outcome = \"INCOMPLETE\"\n        outcome = pd.DataFrame({'Epsiode':[e],'Outcome':[outcome]})\n        outcomes = outcomes.append(outcome)\n\n        \n   \n\n\n    return(outcomes,actions_output,data_output,V_output)\n    ","6cc89e2f":"alpha = 0.3\ngamma = 0.9\nnum_episodes = 1000\nepsilon = 0.2\n\n\ngoldMDP4['Reward'] = np.where(goldMDP4['Action']==\"+KILLS\",5,-0.005)\nreward = goldMDP4['Reward']\n\nStartMin = 15\nStartState = 'EVEN'\nStartAction = None\ndata = goldMDP4\n\nMax_Mins = 50\nstart_time = timeit.default_timer()\n\n\nMdl6 = MCModelv6(data=data, alpha = alpha, gamma=gamma, epsilon = epsilon, reward = reward,\n                StartMin = StartMin, StartState=StartState,StartAction=StartAction, \n                num_episodes = num_episodes, Max_Mins = Max_Mins)\n\nelapsed = timeit.default_timer() - start_time\n\nprint(\"Time taken to run model:\",np.round(elapsed\/60,2),\"mins\")\nprint(\"Avg Time taken per episode:\", np.round(elapsed\/num_episodes,2),\"secs\")","3e18d5d3":"final_output = Mdl6[2]\nV_episodes = Mdl6[3]\n\nfinal_output2 = final_output[(final_output['Minute']==StartMin)&(final_output['State']==StartState)]\nfinal_output3 = final_output2.groupby(['Minute','State','Action']).sum().sort_values('V',ascending=False).reset_index()\nfinal_output3[['Minute','State','Action','V']]\n\nsingle_action1 = final_output3['Action'][0]\nsingle_action2 = final_output3['Action'][len(final_output3)-1]\n\nplot_data1 = V_episodes[(V_episodes['Action']==single_action1)]\nplot_data2 = V_episodes[(V_episodes['Action']==single_action2)]\n\nplt.plot(plot_data1['Episode'],plot_data1['V'], label = single_action1, color = 'C2')\nplt.plot(plot_data2['Episode'],plot_data2['V'], label = single_action2, color = 'C1')\nplt.xlabel(\"Epsiode\")\nplt.ylabel(\"V\")\nplt.legend()\nplt.title(\"V by Episode for the Best\/Worst Actions given the Current State\")\nplt.show()","dd58898f":"alpha = 0.3\ngamma = 0.9\nnum_episodes = 1000\nepsilon = 0.2\n\n\ngoldMDP4['Reward'] = np.where(goldMDP4['Action']==\"+KILLS\",-5,-0.005)\nreward = goldMDP4['Reward']\n\nStartMin = 15\nStartState = 'EVEN'\nStartAction = None\ndata = goldMDP4\n\nMax_Mins = 50\nstart_time = timeit.default_timer()\n\n\nMdl6_2 = MCModelv6(data=data, alpha = alpha, gamma=gamma, epsilon = epsilon, reward = reward,\n                StartMin = StartMin, StartState=StartState,StartAction=StartAction, \n                num_episodes = num_episodes, Max_Mins = Max_Mins)\n\nelapsed = timeit.default_timer() - start_time\n\nprint(\"Time taken to run model:\",np.round(elapsed\/60,2),\"mins\")\nprint(\"Avg Time taken per episode:\", np.round(elapsed\/num_episodes,2),\"secs\")","3b065cbd":"final_output_2 = Mdl6_2[2]\nV_episodes_2 = Mdl6_2[3]\n\n\nfinal_output_22 = final_output_2[(final_output_2['Minute']==StartMin)&(final_output_2['State']==StartState)]\nfinal_output_23 = final_output_22.groupby(['Minute','State','Action']).sum().sort_values('V',ascending=False).reset_index()\nfinal_output_23[['Minute','State','Action','V']]\n\nsingle_action1_2 = final_output_23['Action'][0]\nsingle_action2_2 = final_output_23['Action'][len(final_output_23)-1]\n\nplot_data1_2 = V_episodes_2[(V_episodes_2['Action']==single_action1_2)]\nplot_data2_2 = V_episodes_2[(V_episodes_2['Action']==single_action2_2)]\n\nplt.plot(plot_data1_2['Episode'],plot_data1_2['V'], label = single_action1_2, color = 'C1')\nplt.plot(plot_data2_2['Episode'],plot_data2_2['V'], label = single_action2_2, color = 'C2')\nplt.xlabel(\"Epsiode\")\nplt.ylabel(\"V\")\nplt.legend()\nplt.title(\"V by Episode for the Best\/Worst Actions given the Current State\")\nplt.show()","da887b62":"goldMDP4['Reward'] = np.where(goldMDP4['Action']=='NONE',-0.05,        \n                     np.where(goldMDP4['Action']=='+OUTER_TURRET',(np.random.rand()*-0.1)+0.05,\n                     np.where(goldMDP4['Action']=='+DRAGON',(np.random.rand()*-0.1)+0.05,\n                     np.where(goldMDP4['Action']=='+RIFT_HERALD',(np.random.rand()*-0.1)+0.05,\n                     np.where(goldMDP4['Action']=='+BARON_NASHOR',(np.random.rand()*-0.1)+0.05,\n                     np.where(goldMDP4['Action']=='+INNER_TURRET',(np.random.rand()*-0.1)+0.05,\n                     np.where(goldMDP4['Action']=='+BASE_TURRET',(np.random.rand()*-0.1)+0.05,\n                     np.where(goldMDP4['Action']=='+INHIBITOR',(np.random.rand()*-0.1)+0.05,\n                     np.where(goldMDP4['Action']=='+NEXUS_TURRET',(np.random.rand()*-0.1)+0.05,    \n                     np.where(goldMDP4['Action']=='+ELDER_DRAGON',(np.random.rand()*-0.1)+0.05,\n                              -0.05))))))))))\n                              \nreward = goldMDP4['Reward']\ngoldMDP4[['Action','Reward']].drop_duplicates('Action').sort_values('Reward',ascending=False)","b9c7f8b2":"alpha = 0.3\ngamma = 0.9\nnum_episodes = 1000\nepsilon = 0.2\n\n\n\n\nStartMin = 15\nStartState = 'EVEN'\nStartAction = None\ndata = goldMDP4\n\nMax_Mins = 50\nstart_time = timeit.default_timer()\n\n\nMdl7 = MCModelv6(data=data, alpha = alpha, gamma=gamma, epsilon = epsilon, reward = reward,\n                StartMin = StartMin, StartState=StartState,StartAction=StartAction, \n                num_episodes = num_episodes, Max_Mins = Max_Mins)\n\nelapsed = timeit.default_timer() - start_time\n\nprint(\"Time taken to run model:\",np.round(elapsed\/60,2),\"mins\")\nprint(\"Avg Time taken per episode:\", np.round(elapsed\/num_episodes,2),\"secs\")","874106d2":"final_output = Mdl7[2]\n\n\nfinal_output2 = final_output[(final_output['Minute']==StartMin)&(final_output['State']==StartState)]\nfinal_output3 = final_output2.groupby(['Minute','State','Action']).sum().sort_values('V',ascending=False).reset_index()\nfinal_output3[['Minute','State','Action','V']]","09b3ca8b":"final_output = Mdl7[2]\nV_episodes = Mdl7[3]\n\nfinal_output2 = final_output[(final_output['Minute']==StartMin)&(final_output['State']==StartState)]\nfinal_output3 = final_output2.groupby(['Minute','State','Action']).sum().sort_values('V',ascending=False).reset_index()\nfinal_output3[['Minute','State','Action','V']]\n\nsingle_action1 = final_output3['Action'][0]\nsingle_action2 = final_output3['Action'][len(final_output3)-1]\n\nplot_data1 = V_episodes[(V_episodes['Action']==single_action1)]\nplot_data2 = V_episodes[(V_episodes['Action']==single_action2)]\n\nplt.plot(plot_data1['Episode'],plot_data1['V'], label = single_action1)\nplt.plot(plot_data2['Episode'],plot_data2['V'], label = single_action2)\nplt.xlabel(\"Epsiode\")\nplt.ylabel(\"V\")\nplt.legend()\nplt.title(\"V by Episode for the Best\/Worst Actions given the Current State\")\nplt.show()","cb45db0e":"plt.figure(figsize=(20,10))\n\nfor actions in V_episodes['Action'].unique():\n    plot_data = V_episodes[V_episodes['Action']==actions]\n    plt.plot(plot_data['Episode'],plot_data['V'])\nplt.xlabel(\"Epsiode\")\nplt.ylabel(\"V\")\nplt.title(\"V for each Action by Episode\")\nplt.show()\n","c84067e6":"![MDP Example](https:\/\/i.imgur.com\/JRfOiyf.png)","67732ff3":"### AI Model II: Introducing Gold Difference\n\nI then realised from the results of our first model attempts that we have nothing to take into account the cumulative impact negative and positive events have on the likelihood in later states. In other words, the current MDP probabilities are just as likely to happen whether you are ahead or behind at that point in time. In the game this simply isn\u2019t true; if you are behind then kills, structures and other objectives are much harder to obtain and we need to account for this. \nTherefore, we introduce gold difference between the teams as a way to redefine our states. We now aim to have a MDP defining the states to be both the order events occurred but also whether the team is behind, even or ahead in gold. We have categorised the gold difference to the following:\n-\tEven: 0-999 gold difference (0-200 per player avg.)\n-\tSlightly Behind\/Ahead: 1,000-2,499 gold difference (200-500 per player avg.)\n-\tBehind\/Ahead: 2,500-4,999 gold difference (500-1,000 per player avg.)\n-\tVery Behind\/Ahead: 5,000 gold difference (1,000+ per player avg.) \nWe also now consider no events to be of interest and include this as \u2018NONE\u2019 event so that each minute has at least one event. This \u2018NONE\u2019 event represents if a team decided to try stalling game and helps differentiate teams that are better at obtaining a gold lead in the early game without kills or objectives (through minion kills). However, doing this also massively stretches our data thin as we have now added 7 categories to fit the available matches into but if we had access to more normal matches the amount of data would be sufficient. \nAs before, we can outline each step by the following:\n\n","d5348808":"### Motivations and Objectives\nLeague of Legends is a team oriented video game where on two team teams (with 5 players in each) compete for objectives and kills. Gaining an advantage enables the players to become stronger (obtain better items and level up faster) than their opponents and, as their advantage increases, the likelihood of winning the game also increases. We therefore have a sequence of events dependent on previous events that lead to one team destroying the other\u2019s base and winning the game. \n\nSequences like this being modelled statistically is nothing new; for years now researchers have considered how this is applied in sports, such as basketball (https:\/\/arxiv.org\/pdf\/1507.01816.pdf), where a sequence of passing, dribbling and foul plays lead to a team obtaining or losing points. The aim of research such as this one mentioned is to provide more detailed insight beyond a simple box score (number of points or kill gained by player in basketball or video games respectively) and consider how teams perform when modelled as a sequence of events connected in time. \n\nModelling the events in this way is even more important in games such as League of Legends as taking objectives and kills lead towards both an item and level advantage. For example, a player obtaining the first kill of the game nets them gold that can be used to purchase more powerful items. With this item they are then strong enough to obtain more kills and so on until they can lead their team to a win. Facilitating a lead like this is often referred to as \u2018snowballing\u2019 as the players cumulatively gain advantages but often games are not this one sided and objects and team plays are more important. \n\n#### The aim of this is project is simple; can we calculate the next best event given what has occurred previously in the game so that the likelihood of eventually leading to a win increases based on real match statistics?\n\nHowever, there are many factors that lead to a player\u2019s decision making in a game that cannot be easily measured. No how matter how much data collected, the amount of information a player can capture is beyond any that a computer can detect (at least for now!). For example, players may be over or underperforming in this game or may simply have a preference for the way they play (often defined by the types of characters they play). Some players will naturally be more aggressive and look for kills while others will play passively and push for objectives instead.\nTherefore, we further develop our model to allow the player to adjust the recommended play on their preferences.\n\n","535c4e81":"# AI in Video Games: Improving Decision Making in League of Legends using Real Match Statistics and Personal Preferences\n\n## Part 3: Introducing Personal Preferences and Concept of Final Output\n\nThis is the final part of a short series where I transformed the competitive match statistics into an MDP and applied reinforcement learning to find the optimal play given a current state.\n\nWrite up can be found on Medium page or my website by following the links:\n\nhttps:\/\/medium.com\/@philiposbornedata\n\nor \n\nhttps:\/\/www.philiposbornedata.com\/\n\n\nPlease let me know if you have any questions.\n\nThanks\nPhil\n","e57eef39":"![Voting Example](https:\/\/i.imgur.com\/ytz8RRJ.png)","70bc202d":"### Import Packages and Data","856f136d":"### Create MDP from Match Statistics (see part 1 or 2 for more details)","52246e1e":"#### Pre-processing\n\n1.\tImport data for kills, structures, monsters and gold difference.\n2.\tConvert \u2018Address\u2019 into an id feature.\n3.\tRemove all games with old dragon.\n4.\tStart with gold difference data and aggregate this by minute of event, match id and team that made event as before\n5.\tAppend (stack) the kills, monsters and structures data onto the end of this creating a row for each event and sort by time event occurred (avg. for kills).\n6.\tAdd and \u2018Event Number\u2019 feature that shows the order of events in each of the matches.\n7.\tCreate a consolidated \u2018Event\u2019 feature with either kills, structures, monsters or \u2018NONE\u2019 for each event on the row.\n8.\tTransform this into one row per match with columns now denoting each event.\n9.\tOnly consider red team\u2019s perspective so merge columns and where blue gains become negative red gains. Also add on game length and outcome for red team.\n10.\tReplace all blank values (i.e. game ended in earlier step) with the game outcome for the match so that the last event in all rows is the match outcome.\n11.\tTransform into MDP where we have P( X_t | X_t-1 ) for all event types in between each event number and state defined by gold difference.\n","d7216d90":"### Introducing Preferences with Rewards\n\nFirst, we adjust our model code to include the reward in our Return calculation. Then, when we run the model, instead of simply having a reward equal to zero, we now introduce a bias towards some actions.\n\nIn our first example, we show what happens if we heavily weight an action positively and then, in our second, if we weight an action negatively. \n\n#### Model v6 Pseudocode in Plain English\n\nOur very final version of the model can be simply summarised by the following:\n\n\n1.\tIntroduce parameters\n2.\tInitialise start state, start event and start action\n3.\tSelect actions based on either first provided or randomly over their likelihood of occurring as defined in MDP\n4.\tWhen action reaches win\/loss, end episode\n5.\tTrack the actions taken in the episode and final outcome (win\/loss)\n6.\tRepeat for x number of episodes\n","fba2ec26":"![Probability of Winning Given Outcome of First Two Events](https:\/\/i.imgur.com\/p3gb0GC.png)","5958964e":"### More realistic player preferences\n\nSo let us attempt to approximately simulate a player's actual preferences. In this case, I have randomised some of the rewards to follow the two rules:\n\n1. The player doesn't want to give up any objectives\n2. The player prioritises gaining objectives over kills\n\n\nTherefore, our rewards for kills and losing objects are all the minimum of -0.05 whereas the other actions are randomised between -0.05 and 0.05.","7993cd8c":"### Conclusion and Collecting Feedback from Players for Rewards\n\n\nI have vastly oversimplified some of the features (such as \u2018kills\u2019 not representing the actual amount of kills) and the data is likely not representative of normal matches. However, I hope that this demonstrates an interesting concept clearly and encourages discussion to start about how this could be developed further. \n\nFirst, I will list the main improvements that need to be made before this could be viable for implementation:\n\n1. Calculate the MDP using more data that represents the whole player population, not just competitive matches.\n2. Improve the efficiency of the model so that it can calculate in a more resonable time. Monte Carlo is known for being time consuming so would explore more time efficient algorithms.\n3. Apply more advanced parameter optimisation to further improve the results.\n4. Prototype player feedback capture and mapping for a more realistic reward signal.\n\nWe have introduced rewards for influencing the model output but how is this obtained? Well there are a few ways we could consider but, based on my previous research, I think the best way is to consider a reward that considers both the individual quality of the action AND the quality of transitioning. \n\nThis becomes more and more complex and not something I will cover here but, in short, we would like to match a player's decision making in which the optimal next decision is dependent on what just occured. For example, if the team kills all players on the enemy team, then they may push to obtain Baron. Our model already takes in to account the probability of events occuring in a sequence so we should also consider a player's decision making in the same way. This idea is drawn from the following research which explains how the feedback can be mapped in more detail (https:\/\/www.researchgate.net\/publication\/259624959_DJ-MC_A_Reinforcement-Learning_Agent_for_Music_Playlist_Recommendation). \n\nHow we collect this feedback defines how successful our model will be. In my eyes, the end goal of this would be to have real time recommendations for players on the next best decision to make. The player would then be able to select from the top few decisions (ranked in order of success) given the match statistics. This player\u2019s choice can be tracked over multiple games to further learn and understand that player\u2019s preferences. This would also mean that not only could we track the outcome of decisions but would also know what that player attempted to achieve (e.g. tried to take tower but was killed instead) and would open up information for even more advanced analytics.  \n\nOf course, an idea like this may cause complications with team mates disagreeing and perhaps take an element out of the game that makes it exciting. But I think something like this could greatly benefit players at a lower or normal skill level where decision making between the players are difficult to communicate clearly. It could also help identify players that are being \u2018toxic\u2019 by their actions as teams would look to agree the play via a vote system and it can then be seen whether the toxic player consistently ignores their team mates by their movements instead of following the agreed plans.","cd046f2d":"### What makes our model 'Artifical Intellegence'?\n\nIn out first part, we performed some introductory statistical analysis. For example, we were able to calculate the probability of winning given the team obtains the first and second objective in a match as shown in the image below.\n\nThere are two components that make takes our project beyond simple statistics into AI:\n\n- First, the model learns which actions are best with no pre-conceived notion of the game and \n- Secondly, it attempts to learn a player's preference for decisions that will influence the model's output.\n\nHow we define our Markov Decision Process and collect a player's preference will define what our model learns and therefore outputs. \n","7757e816":"## Pre-Processing and Creating Markov Decision Process from Match Statistics"}}