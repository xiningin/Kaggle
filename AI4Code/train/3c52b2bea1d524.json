{"cell_type":{"5ac04c6d":"code","cddff969":"code","34a9dceb":"code","defe3895":"code","d709f5a5":"code","3865dbc3":"code","40abef1e":"code","865d32c4":"code","86d5b998":"code","c03f06c3":"code","912996b5":"code","769786d2":"code","58f285ce":"code","14c8e141":"code","4b805546":"code","884e96db":"code","2d718c29":"code","3d711aae":"code","2e7d2531":"code","2d947b6b":"code","477af364":"code","b028bc79":"code","39a0f377":"code","74d99749":"code","da44f944":"code","e44be46a":"code","91228613":"code","bf0255ab":"code","43256d4f":"code","49718779":"code","0277c642":"code","64f9b5f4":"code","fad847eb":"code","9f1baabe":"code","07b102f1":"code","9284e83e":"code","0e397353":"code","e6c00264":"code","95d2e728":"code","28143eb4":"code","eee4d562":"code","16e7e896":"code","0e19bee2":"code","94ba73f6":"code","704e8218":"code","6c64f46b":"code","a8f275f3":"code","3e456fc9":"code","4aae057f":"code","2a08a86d":"code","ad32160c":"code","3498bbf0":"code","3aa1086b":"code","b72087b4":"code","f4ec910a":"code","aaa746be":"markdown","d201fcbc":"markdown","2c5ff53c":"markdown","22bb2b93":"markdown","e38dafba":"markdown","2dc5e6c2":"markdown","d64b41f9":"markdown","08d880f2":"markdown","c90d35ee":"markdown","43787f05":"markdown","dc153cf5":"markdown","1e3db57d":"markdown","2e026027":"markdown","4d3905e1":"markdown","03acc3cd":"markdown","d244426d":"markdown","818d335c":"markdown","9bbf7692":"markdown","63e7bf7d":"markdown","04d2335f":"markdown","4bda1b66":"markdown","cbd40e1d":"markdown","023c9366":"markdown","458cf7b4":"markdown","f2a5e98e":"markdown","0108f560":"markdown","69cb702b":"markdown","42a43f5d":"markdown","11b9b30b":"markdown","c7d0de9f":"markdown","d5d284b2":"markdown","80ae7297":"markdown","f18b86cc":"markdown","06d485db":"markdown"},"source":{"5ac04c6d":"# Bibliotecas Necess\u00e1rias \nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","cddff969":"# Criando o DataFrame a partir da base de dados\ndf = pd.read_csv('..\/input\/quality-prediction-in-a-mining-process\/MiningProcess_Flotation_Plant_Database.csv', decimal=\",\") #substituir a v\u00edlgula por ponto\ndf.date = pd.to_datetime(df.date) #Transformar a data do registro para o formato Pandas Datetime, que facilita a manipula\u00e7\u00e3o\nprint('N\u00famero de Linhas: ', df.shape[0])\nprint('N\u00famero de Colunas: ', df.shape[1])\ndf.head()","34a9dceb":"# Avaliar se o formato dos dados est\u00e1 correto\ndf.dtypes","defe3895":"# Avaliar a exist\u00eancia de dados nulos\ndf.isnull().sum()","d709f5a5":"# Avaliar os valores de % Iron Concentrate e % Silica Concentrate durante certo dia\ndf.loc[(df['date'].dt.day==10) & (df['date'].dt.month==3)].head()","3865dbc3":"# Testar se o registro dos valores de % Iron Concentrate e % Silica Concentrate ocorre a cada hora\ndf.loc[172:175]","40abef1e":"# Avaliar quantos valores s\u00e3o atribu\u00eddos ao par\u00e2metro '% Silica Concentrate' dentro de uma mesma hora\ndf.loc[0:173][['% Silica Concentrate', '% Iron Concentrate']].value_counts()","865d32c4":"# Avaliar quantos valores s\u00e3o atribu\u00eddos ao par\u00e2metro '% Iron Feed' e '% Silica Feed' no dia 10\/03\ndf.loc[(df['date'].dt.day==10) & (df['date'].dt.month==3)][['% Iron Feed', '% Silica Feed']].value_counts()","86d5b998":"# Avaliar quantos valores s\u00e3o atribu\u00eddos ao par\u00e2metro '% Iron Feed' e '% Silica Feed' no dia 11\/03\ndf.loc[(df['date'].dt.day==11) & (df['date'].dt.month==3)][['% Iron Feed', '% Silica Feed']].value_counts()","c03f06c3":"# Avaliar quantos valores s\u00e3o atribu\u00eddos ao par\u00e2metro '% Iron Feed' e '% Silica Feed' no dia 12\/03\ndf.loc[(df['date'].dt.day==12) & (df['date'].dt.month==3)][['% Iron Feed', '% Silica Feed']].value_counts()","912996b5":"# Avaliar quantos valores s\u00e3o atribu\u00eddos ao par\u00e2metro '% Iron Feed' e '% Silica Feed' no dia 13\/03\ndf.loc[(df['date'].dt.day==13) & (df['date'].dt.month==3)][['% Iron Feed', '% Silica Feed']].value_counts()","769786d2":"# Deletar todos os registros onde as features '% Iron Feed' e '% Silica Feed' s\u00e3o replicados\ndf.drop_duplicates(subset=['% Iron Feed', '% Silica Feed']).head(15)","58f285ce":"# Avaliar a quantidade de medi\u00e7\u00f5es no per\u00edodo considerado na base\nsns.scatterplot(x=df.date,y=df['% Silica Concentrate'])","14c8e141":"# Avaliar os dias sem registro dos dados, no m\u00eas 03\ndf3 = df.loc[df['date'].dt.month==3]\nsns.scatterplot(x=df3.date,y=df3['% Silica Concentrate'])","4b805546":"# Selecionar apenas os 3 hor\u00e1rios de interesse para cada dia (onde todas as features foram de fato mensuradas)\nconditions = [(df.date.dt.hour == 1), (df.date.dt.hour == 7), (df.date.dt.hour == 16)]\nchoices = [1, 1, 1]\ndf['Medi\u00e7\u00e3oV\u00e1lida'] = np.select(conditions, choices, default= 0)\ndf.head()","884e96db":"# Criar novo DataFrame apenas com os registros considerados v\u00e1lidos, segundo as premissas adotadas acima\ndf_valid = df[df.Medi\u00e7\u00e3oV\u00e1lida == 1]\ndf_valid = df_valid.drop_duplicates(subset='date')\ndf_valid = df_valid.drop(labels=['Medi\u00e7\u00e3oV\u00e1lida'], axis=1).reset_index(drop=True)\nprint('N\u00famero de Linhas: ', df_valid.shape[0])\nprint('N\u00famero de Colunas: ', df_valid.shape[1])\ndf_valid.head()","2d718c29":"# Distribui\u00e7\u00e3o de frequ\u00eancia da Target\nplt.hist(df_valid['% Silica Concentrate'], label='% Silica Concentrate')\nplt.axvline(df_valid['% Silica Concentrate'].mean(), color='red', linestyle='dashed', linewidth=2, label='M\u00e9dia')\nplt.axvline(df_valid['% Silica Concentrate'].median(), color='black', linewidth=2, label='Mediana')\nplt.legend()\nplt.show()","3d711aae":"# Avaliar a distribui\u00e7\u00e3o da target no tempo\nm = df_valid['% Silica Concentrate'].mean()\nprint(f'M\u00e9dia da target no per\u00edodo: {m}')\nsns.pairplot(df_valid, y_vars='% Silica Concentrate', x_vars=['date'], height=6, aspect=1.5)","2e7d2531":"# Pairplot para avaliar visualmente a rela\u00e7\u00e3o entre as features e a target\ncol = ['% Iron Feed', '% Silica Feed', 'Starch Flow', 'Amina Flow', 'Ore Pulp Flow', 'Ore Pulp pH', 'Ore Pulp Density', 'Flotation Column 01 Air Flow', \n       'Flotation Column 02 Air Flow', 'Flotation Column 03 Air Flow', 'Flotation Column 04 Air Flow', 'Flotation Column 05 Air Flow', 'Flotation Column 06 Air Flow',\n       'Flotation Column 07 Air Flow', 'Flotation Column 01 Level', 'Flotation Column 02 Level', 'Flotation Column 03 Level', 'Flotation Column 04 Level', \n       'Flotation Column 05 Level', 'Flotation Column 06 Level', 'Flotation Column 07 Level']\n\nax = sns.pairplot(df_valid, y_vars='% Silica Concentrate', x_vars=col)","2d947b6b":"# An\u00e1lise inicial de Correla\u00e7\u00e3o entre as Features\ncorr = df_valid.corr()\nfig, ax = plt.subplots(figsize=(12,9))\nsns.heatmap(corr, \n           yticklabels=corr.columns,\n           xticklabels=corr.columns,\n           annot=True,\n           cmap='YlGnBu')","477af364":"# Plotar correla\u00e7\u00e3o entre Silica Concentrate e Iron Feed\nleft_data = df.groupby(df.date)['% Silica Concentrate'].mean()\nright_data = df.groupby(df.date)['% Iron Feed'].mean()\nfig, ax_left = plt.subplots(figsize=(15,5))\nax_right = ax_left.twinx()\nax_left.plot(left_data, label = \"S\u00edlica Final\", color='black')\nax_right.plot(right_data, label = \"Ferro Inicial\", color='red')\nfig.legend()","b028bc79":"# Plotar correla\u00e7\u00e3o entre Silica Concentrate e Starch Flow\nleft_data = df.groupby(df.date)['% Silica Concentrate'].mean()\nright_data = df.groupby(df.date)['Starch Flow'].mean()\nfig, ax_left = plt.subplots(figsize=(15,5))\nax_right = ax_left.twinx()\nax_left.plot(left_data, label = \"S\u00edlica Final\", color='black')\nax_right.plot(right_data, label = \"Amido\", color='green')\nfig.legend()","39a0f377":"# Plotar correla\u00e7\u00e3o entre Silica Concentrate e Amina Flow\nleft_data = df.groupby(df.date)['% Silica Concentrate'].mean()\nright_data = df.groupby(df.date)['Amina Flow'].mean()\nfig, ax_left = plt.subplots(figsize=(15,5))\nax_right = ax_left.twinx()\nax_left.plot(left_data, label = \"S\u00edlica Final\", color='black')\nax_right.plot(right_data, label = \"Amina\", color='orange')\nfig.legend()","74d99749":"# Vou deletar a colunas que n\u00e3o ser\u00e3o consideradas na an\u00e1lise\ndf_valid = df_valid.drop(labels=['date', '% Iron Concentrate'], axis=1)\nprint('N\u00famero de Linhas: ', df_valid.shape[0])\nprint('N\u00famero de Colunas: ', df_valid.shape[1])\ndf_valid.head()","da44f944":"# Criar a matriz com as features e o vetor com a classe target\nX = df_valid.drop('% Silica Concentrate', axis = 1)\ny = df_valid['% Silica Concentrate']  \n\n# Dividir a base em Treino e Teste\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=25)","e44be46a":"# Baseline com Decision Tree Regressor\nfrom sklearn.tree import DecisionTreeRegressor\nbase_model = DecisionTreeRegressor(random_state=25)\nbase_model.fit(X_train, y_train)\ny_pred = base_model.predict(X_test)\n\n# RMSE - M\u00e9trica base da avalia\u00e7\u00e3o e que ser\u00e1 alvo da minimiza\u00e7\u00e3o\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import mean_absolute_error as mae\nerro_mean = mse(y_test, y_pred, squared=False)\nerro_mean","91228613":"# Median Absolute Error - M\u00e9trica de compara\u00e7\u00e3o para identifica\u00e7\u00e3o de outliers\nfrom sklearn.metrics import median_absolute_error as mdae\nerro_median = mdae(y_test, y_pred)\nerro_median","bf0255ab":"# Boxplot da features target para avalia\u00e7\u00e3o visual\ndf_valid['% Silica Concentrate'].plot(kind='box')","43256d4f":"# Avaliar quantos registros maiores que 5 existem na base\ndf_valid[df_valid['% Silica Concentrate'] > 5]","49718779":"df_valid2 = df_valid[df_valid['% Silica Concentrate'] < 5.1]\n\n# Criar a matriz com as features e o vetor com a classe target\nX2 = df_valid2.drop('% Silica Concentrate', axis = 1)\ny2 = df_valid2['% Silica Concentrate']  \n\n# Dividir a base em Treino e Teste\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.20, random_state=30)","0277c642":"# Baseline com Decision Tree Regressor\nbase_model2 = DecisionTreeRegressor(random_state=29)\nbase_model2.fit(X_train2, y_train2)\ny_pred2 = base_model2.predict(X_test2)\n\n# RMSE - M\u00e9trica base da avalia\u00e7\u00e3o e que ser\u00e1 alvo da minimiza\u00e7\u00e3o\nerro_mean2 = mse(y_test2, y_pred2, squared=False)\nerro_mean2","64f9b5f4":"# Median Absolute Error - M\u00e9trica de compara\u00e7\u00e3o para identifica\u00e7\u00e3o de outliers\nerro_median2 = mdae(y_test2, y_pred2)\nerro_median2","fad847eb":"# M\u00e9dia e Desvio padr\u00e3o da feature target\nmean = np.mean(y2)\nstd = np.std(y2)\nprint(mean, std)","9f1baabe":"# Plotar Feature Importances \nimportances = base_model2.feature_importances_\nindices = np.argsort(importances)[::-1]\nf, ax = plt.subplots(figsize=(10, 6))\nplt.bar(range(X_train2.shape[1]), importances[indices],\n    color=\"b\", \n    align=\"center\")\nfeature_names = X_train2.columns\nplt.xticks(range(X_train2.shape[1]), feature_names, fontsize = 8, rotation=90)\nplt.xlim([-1, X_train2.shape[1]])\nplt.ylabel(\"importance\", fontsize = 14)\nplt.xlabel(\"feature\", fontsize = 14)","07b102f1":"# Confrontar valores Previstos com valores Reais\nplt.scatter(range(len(y_pred2)), y_pred2, color='blue', label='Previsto')\nplt.scatter(range(len(y_test2)), y_test2, color='orange', label='Real')\nplt.legend(loc='best', frameon=True)\nplt.show()","9284e83e":"# Tuning do Modelo Decision Tree\n\nfrom sklearn.model_selection import GridSearchCV\ntun_tree = DecisionTreeRegressor()\nparametros = {'min_samples_leaf':[2, 3, 4, 5, 6], 'max_depth':[3, 4, 5, 6, 7], 'max_leaf_nodes':[3, 4, 5, 6]}\ngrid = GridSearchCV(estimator=tun_tree, param_grid=parametros, scoring='neg_root_mean_squared_error', cv=4)\n\n# Validando o Modelo\ngrid.fit(X_train2, y_train2)\nprint(pd.DataFrame(grid.cv_results_)[['rank_test_score','mean_test_score']].sort_values('rank_test_score').head(5))\nprint(f'Melhores par\u00e2metros: {grid.best_params_}')","0e397353":"# Modelo Tunado\ntun_tree = DecisionTreeRegressor(max_depth=3, max_leaf_nodes=5, min_samples_leaf=6, random_state=12)\ntun_tree.fit(X_train2, y_train2)\ny_pred_tun = tun_tree.predict(X_test2)\n\n# RMSE \nerro_mean_tun = mse(y_test2, y_pred_tun, squared=False)\nerro_mean_tun","e6c00264":"# Median Absolute Error - M\u00e9trica de compara\u00e7\u00e3o para identifica\u00e7\u00e3o de outliers\nerro_median2 = mdae(y_test2, y_pred_tun)\nerro_median2","95d2e728":"# Plotar Feature Importances \nimportances = tun_tree.feature_importances_\nindices = np.argsort(importances)[::-1]\nf, ax = plt.subplots(figsize=(10, 6))\nplt.bar(range(X_train2.shape[1]), importances[indices],\n    color=\"b\", \n    align=\"center\")\nfeature_names = X_train2.columns\nplt.xticks(range(X_train2.shape[1]), feature_names, fontsize = 8, rotation=90)\nplt.xlim([-1, X_train2.shape[1]])\nplt.ylabel(\"importance\", fontsize = 14)\nplt.xlabel(\"feature\", fontsize = 14)","28143eb4":"# Confrontar valores Previstos com valores Reais\nplt.scatter(range(len(y_pred_tun)), y_pred_tun, color='green', label='Previsto')\nplt.scatter(range(len(y_test2)), y_test2, color='orange', label='Real')\nplt.legend(loc='upper left', frameon=True)\nplt.show()","eee4d562":"# Distribui\u00e7\u00e3o de frequ\u00eancia da feature '% Iron Feed'\nplt.hist(df_valid['% Iron Feed'], label='% Iron Feed')\nplt.axvline(df_valid['% Iron Feed'].mean(), color='red', linestyle='dashed', linewidth=2, label='M\u00e9dia')\nplt.axvline(df_valid['% Iron Feed'].median(), color='black', linewidth=2, label='Mediana')\nplt.axvline(x=df_valid['% Iron Feed'].median() - df_valid['% Iron Feed'].std(), color='yellow', linewidth=2, label='Desvio Padr\u00e3o')\nplt.axvline(x=df_valid['% Iron Feed'].median() + df_valid['% Iron Feed'].std(), color='yellow', linewidth=2)\nplt.legend()\nplt.show()","16e7e896":"# Distribui\u00e7\u00e3o de frequ\u00eancia da feature '% Silica Feed'\nplt.hist(df_valid['% Silica Feed'], label='% Silica Feed')\nplt.axvline(df_valid['% Silica Feed'].mean(), color='red', linestyle='dashed', linewidth=2, label='M\u00e9dia')\nplt.axvline(df_valid['% Silica Feed'].median(), color='black', linewidth=2, label='Mediana')\nplt.axvline(x=df_valid['% Silica Feed'].median() - df_valid['% Silica Feed'].std(), color='yellow', linewidth=2, label='Desvio Padr\u00e3o')\nplt.axvline(x=df_valid['% Silica Feed'].median() + df_valid['% Silica Feed'].std(), color='yellow', linewidth=2)\nplt.legend()\nplt.show()","0e19bee2":"# Criar novo dataframe mantendo a primeira medi\u00e7\u00e3o de cada hora, segundo an\u00e1lise laboratorial da '% Silica Concentrate'\ndf_test = df.drop_duplicates(subset=['date']) #por padr\u00e3o mant\u00e9m a primeira inst\u00e2ncia\ndf_test = df_test.drop(labels=['date', '% Iron Feed','% Silica Feed', '% Iron Concentrate', 'Medi\u00e7\u00e3oV\u00e1lida'], axis=1)\ndf_test = df_test[df_test['% Silica Concentrate'] < 5.1]\nprint('N\u00famero de Linhas: ', df_test.shape[0])\nprint('N\u00famero de Colunas: ', df_test.shape[1])\ndf_test.head()","94ba73f6":"# Distribui\u00e7\u00e3o de frequ\u00eancia da Target\nplt.hist(df_test['% Silica Concentrate'], label='% Silica Concentrate')\nplt.axvline(df_test['% Silica Concentrate'].mean(), color='red', linestyle='dashed', linewidth=2, label='M\u00e9dia')\nplt.axvline(df_test['% Silica Concentrate'].median(), color='black', linewidth=2, label='Mediana')\nplt.legend()\nplt.show()","704e8218":"# Criar a matriz com as features e o vetor com a classe target\nX_new = df_test.drop('% Silica Concentrate', axis = 1)\ny_new = df_test['% Silica Concentrate']  \n\n# Dividir a base em Treino e Teste\nX_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y_new, test_size=0.30, random_state=42)\n\n# Baseline com Decision Tree Regressor\nmodel_new = DecisionTreeRegressor(random_state=40)\nmodel_new.fit(X_train_new, y_train_new)\ny_pred_new = model_new.predict(X_test_new)\n\n# RMSE - M\u00e9trica base da avalia\u00e7\u00e3o e que ser\u00e1 alvo da minimiza\u00e7\u00e3o\nerro_mean_new = mse(y_test_new, y_pred_new, squared=False)\nerro_mean_new","6c64f46b":"# Tuning New Modelo Decision Tree\n\nnew_tree = DecisionTreeRegressor()\nparametros = {'min_samples_leaf':[2, 3, 4, 5, 6], 'max_depth':[3, 4, 5, 6, 7], 'max_leaf_nodes':[3, 4, 5, 6]}\ngrid = GridSearchCV(estimator=new_tree, param_grid=parametros, scoring='neg_root_mean_squared_error', cv=5)\n\n# Validando o Modelo\ngrid.fit(X_train_new, y_train_new)\nprint(pd.DataFrame(grid.cv_results_)[['rank_test_score','mean_test_score']].sort_values('rank_test_score').head(5))\nprint(f'Melhores par\u00e2metros: {grid.best_params_}')","a8f275f3":"# Tuning Decision Tree Regressor\ntun_new_model = DecisionTreeRegressor(max_depth=4, max_leaf_nodes=6, min_samples_leaf=2, random_state=20)\ntun_new_model.fit(X_train_new, y_train_new)\ny_tun_new = tun_new_model.predict(X_test_new)\n\n# RMSE \nerro_mean_new_tun = mse(y_test_new, y_tun_new, squared=False)\nerro_mae_new_tun = mae(y_test_new, y_tun_new)\nprint('RMSE: {} | MAE: {}'.format(erro_mean_new_tun, erro_mae_new_tun))","3e456fc9":"# Confrontar valores Previstos com valores Reais\nplt.scatter(range(len(y_tun_new)), y_tun_new, color='blue', label='Previsto')\nplt.scatter(range(len(y_test_new)), y_test_new, color='orange', label='Real')\nplt.legend(loc='lower right')\nplt.show()","4aae057f":"# Plotar Feature Importances \nimportances = tun_new_model.feature_importances_\nindices = np.argsort(importances)[::-1]\nf, ax = plt.subplots(figsize=(10, 6))\nplt.bar(range(X_train_new.shape[1]), importances[indices],\n    color=\"b\", \n    align=\"center\")\nfeature_names = X_train_new.columns\nplt.xticks(range(X_train_new.shape[1]), feature_names, fontsize = 8, rotation=90)\nplt.xlim([-1, X_train_new.shape[1]])\nplt.ylabel(\"importance\", fontsize = 14)\nplt.xlabel(\"feature\", fontsize = 14)","2a08a86d":"# Valida\u00e7\u00e3o Random Forest model\nfrom sklearn.ensemble import RandomForestRegressor\nrf_model = RandomForestRegressor()\nparam = {'n_estimators':[100, 400, 500, 700], \n         'max_depth':[3, 4, 5], \n         'min_samples_leaf':[2, 3, 4], \n         'max_features':['auto', 'sqrt']}\ngrid = GridSearchCV(estimator=rf_model, param_grid=param, scoring='neg_root_mean_squared_error', cv=5)\n\n# Rodar a valida\u00e7\u00e3o\ngrid.fit(X_train_new, y_train_new)\nprint(pd.DataFrame(grid.cv_results_)[['rank_test_score','mean_test_score']].sort_values('rank_test_score').head(5))\nprint(f'Melhores par\u00e2metros: {grid.best_params_}')","ad32160c":"# Avaliar o Random Forest model\ntun_rf = RandomForestRegressor(max_depth=5, max_features='auto', min_samples_leaf=3, n_estimators=700)\ntun_rf.fit(X_train_new, y_train_new)\ny_pred_rf = tun_rf.predict(X_test_new)\n\n# Compara\u00e7\u00e3o entre as M\u00e9tricas \nrmse_rf = mse(y_test_new, y_pred_rf, squared=False)\nmae_rf = mae(y_test_new, y_pred_rf)\nmedian_rf = mdae(y_test_new, y_pred_rf)\n\nprint('RMSE: {} | MAE: {} | MEAE: {}'.format(rmse_rf, mae_rf, median_rf))","3498bbf0":"# Plotar Feature Importances \nimportances = tun_rf.feature_importances_\nindices = np.argsort(importances)[::-1]\nf, ax = plt.subplots(figsize=(10, 6))\nplt.bar(range(X_train_new.shape[1]), importances[indices],\n    color=\"b\", \n    align=\"center\")\nfeature_names = X_train_new.columns\nplt.xticks(range(X_train_new.shape[1]), feature_names, fontsize = 8, rotation=90)\nplt.xlim([-1, X_train_new.shape[1]])\nplt.ylabel(\"importance\", fontsize = 14)\nplt.xlabel(\"feature\", fontsize = 14)","3aa1086b":"# Confrontar valores Previstos com valores Reais\nplt.scatter(range(len(y_pred_rf)), y_pred_rf, color='blue', label='Previsto')\nplt.scatter(range(len(y_test_new)), y_test_new, color='orange', label='Real')\nplt.legend(loc='lower right')\nplt.show()","b72087b4":"# Gr\u00e1fico Erro de Predi\u00e7\u00e3o\nfrom yellowbrick.regressor import PredictionError, residuals_plot\nfig, ax = plt.subplots(figsize=(6,6))\npev = PredictionError(tun_rf)\npev.fit(X_train_new, y_train_new)\npev.score(X_test_new,y_test_new)\npev.show()","f4ec910a":"# Gr\u00e1fico de Res\u00edduos\nresiduals_plot(tun_rf, X_train_new, y_train_new, X_test_new, y_test_new, train_color=\"grey\", test_color=\"lime\", line_color=\"dodgerblue\")","aaa746be":"- Todos os dados est\u00e3o preenchidos com valores num\u00e9ricos.\n- Em uma primeira vista me pareceu que os valores de '% Iron Concentrate' e\t'% Silica Concentrate' est\u00e3o se repetindo.","d201fcbc":"## Abordagem Alternativa\n- Como os maiores limitadores da disponibilidade de dados s\u00e3o '% Iron Feed' e '% Silica Feed', preciso avaliar uma abordagem sem essas features.\n- Preciso avaliar qual a distribui\u00e7\u00e3o dessas features, e ver se elas seguem o padr\u00e3o aproximado da normal com um pequeno desvio padr\u00e3o.","2c5ff53c":"## Discuss\u00e3o\n- Ao contr\u00e1rio da primeira abordagem, nesse caso as features tiveram grande relev\u00e2ncia para o modelo (> 0.35).\n- Basta encontrar o menor intervalo onde todas as features tenham medi\u00e7\u00f5es, e ser\u00e1 poss\u00edvel fazer previs\u00f5es a cada intervalo deste tempo.\n- Considerando que as caracter\u00edsticas de solo n\u00e3o sofram muita varia\u00e7\u00e3o, e o min\u00e9rio de determinada mina tenha um valor m\u00e9dio de qualidade de ferro e s\u00edlica, torna-se vi\u00e1vel o ajuste do modelo sem as 2 prmeiras features: **'% Iron Feed'** e **'% Silica Feed'**. Isto proporciona uma maior quantidade de inst\u00e2ncias v\u00e1lidas, o que permite a gera\u00e7\u00e3o de um **modelo mais est\u00e1vel**.\n- Caso as caracter\u00edstcas do solo tenham uma varia\u00e7\u00e3o muito grande, a primeira proposta \u00e9 a mais indicada, ainda que muitas inst\u00e2ncias sejam perdidas. \n- Pode-se avaliar a exclus\u00e3o de algumas features com quase nenhuma relev\u00e2ncia para o modelo, segundo gr\u00e1fico acima. Isto porque, al\u00e9m de serem altamente correlacionadas entre si, estas features assumem - em grande parte do tempo - valores constantes e acabam se tornando irrelevantes para o modelo (j\u00e1 que s\u00e3o muito bem controladas).\n- A partir desse ponto \u00e9 poss\u00edvel o ajuste de modelos mais robustos, como Random Forest, XGBoost ou Ensembles de modelos, buscando minimizar a m\u00e9trica escolhida. ","22bb2b93":"- Claramente temos uma distribui\u00e7\u00e3o Assim\u00e9trica \u00e0 Direita, o que \u00e9 confirmado visualmente - pelo histograma - e tamb\u00e9m com a mediana sendo menor que a m\u00e9dia.\n- Alguns algor\u00edtimos n\u00e3o s\u00e3o bons nesse tipo de caso, como \u00e9 o exemplo da regress\u00e3o linear. Logo, ser\u00e3o testados modelos que lidem bem com esta caracter\u00edstica.","e38dafba":"- Este modelo conseguiu capturar melhor a complexidade dos dados.\n- A utiliza\u00e7\u00e3o do RMSE como m\u00e9trica faz com que o modelo penalize mais erros maiores.\n- \u00c9 prov\u00e1vel que mesmo o modelo estando com um bom ajuste, as m\u00e9tricas cl\u00e1ssicas de regress\u00e3o demonstrem um modelo com p\u00e9ssimo desempenho.\n- Isto ocorre pela exist\u00eancia de **correla\u00e7\u00f5es n\u00e3o lineares** entre praticamente todas as features e a target.\n- Vamos avaliar o modelo segundo as m\u00e9tricas cl\u00e1ssicas de regress\u00e3o, abaixo.","2dc5e6c2":"- O formato dos dados est\u00e1 correto e da melhor forma para serem trabalhados. Vou fazer agora o dicion\u00e1rio dos dados para colocar um pouco de contexto sobre a an\u00e1lise e conseguir avaliar o impacto inicial de cada feature.","d64b41f9":"- Considerando o valor da m\u00e9dia, o desempenho do modelo est\u00e1 aceit\u00e1vel, ainda mais sendo um simples modelo de decision tree. \n- Mas algumas coisas podem ser feitas para tentar melhorar a generaliza\u00e7\u00e3o do modelo: Reduzir a dimensionalidade e Tuning do modelo, buscando os melhor hiper par\u00e2metros.\n- Algumas features n\u00e3o explicam quase nada para o modelo, de modo que praticamente todas as features foram utilizadas com uma pequena contribui\u00e7\u00e3o.","08d880f2":"- De acordo com as duas \u00faltimas an\u00e1lises, fica claro que o valor laboratorial de '% Iron Concentrate' e '% Silica Concentrate' \u00e9 medido a cada hora. Dessa forma, todos os registros onde estas features s\u00e3o simplesmente replicadas n\u00e3o podem ser considerados como dados v\u00e1lidos. Assim sendo, dever\u00e3o ser considerados como registros v\u00e1lidos, em termos de '% Iron Concentrate' e '% Silica Concentrate', apenas o primeiro registro de cada hora.\n\n- Uma \u00faltima quest\u00e3o em rela\u00e7\u00e3o ao registro dos dados \u00e9 o registro das features '% Iron Feed' e '% Silica Feed'. Estes valores parecem n\u00e3o ser medidos a cada hora, mas em um intervalo diferente. Isso precisa ser avaliado, e caso seja identificado uma frequ\u00eancia diferente, esta deve ser a frequ\u00eancia utilizada para todo a base.","c90d35ee":"- A segunda abordagem apresentou um n\u00edvel de erro aceit\u00e1vel (+- 1), ainda mais considerando que foi utilizado apenas um modelo simples de Decision Tree.\n- O modelo sem as 2 primeiras features p\u00f4de ser treinado com um n\u00famero muito maior de inst\u00e2ncias e conseguiu generalizar melhor.\n- Resta avaliar a qualidade da previs\u00e3o e quais as features tiveram maior impacto nesse resultado.","43787f05":"- Resultado melhorou em rela\u00e7\u00e3o \u00e0 abordagem anterior.\n- Vamos avaliar com o ajuste dos hiperpar\u00e2metros. ","dc153cf5":"- As distribui\u00e7\u00f5es n\u00e3o s\u00e3o exatamente normais, mas tem um perfil parecido com normal.\n- Os valores de m\u00e9dia e mediana s\u00e3o praticamente iguais, al\u00e9m do pefil da distribui\u00e7\u00e3o ter apar\u00eancia pr\u00f3xima \u00e0 normal.\n- Isto posto, vou considerar que essas 2 features tem uma distribui\u00e7\u00e3o normal, com baixa vari\u00e2ncia, e retir\u00e1-las da base.\n- Como essas features limitam muito a disponibilidade dos dados, vou testar a regra de delimitar as **medi\u00e7\u00f5es a cada hora**, deletando as 2 features em quest\u00e3o. \n- Isso vai aumentar de forma substancial a disponibilidade de dados e pode ajudar o modelo a generalizar melhor, al\u00e9m de permitir o ajuste de um modelo de maior capacidade.","1e3db57d":"- Ao que parece as features '% Iron Feed' e '% Silica Feed' s\u00e3o medidas 3 vezes ao dia, nos hor\u00e1rios de: 01h, 07h e 16h. \n- Como este \u00e9 o M\u00ednimo M\u00faltiplo Comum entre todas as features, este padr\u00e3o de hor\u00e1rio ser\u00e1 utilizado como regra para determina\u00e7\u00e3o dos registros que efetivamente mostram a rela\u00e7\u00e3o entre todas as features e a Target (% Silica Concentrate), o que \u00e9 fundamental na vis\u00e3o do aprendizado supervisionado.","2e026027":"- A maioria das correla\u00e7\u00f5es s\u00e3o n\u00e3o lineares!\n\n- As colunas de ar 1, 2 e 3 parecem trabalhar em valores constantes: 250 Nm\u00b3\/h e 300 Nm\u00b3\/h, apresentando muito pouca varia\u00e7\u00e3o. Esse comportamento acontece tamb\u00e9m nas colunas de ar 6 e 7, que parece assumir basicamente 3 valores: 250 Nm\u00b3\/h, 300 Nm\u00b3\/h e 350 Nm\u00b3\/h.\n\n- Buscando avaliar o real impacto dessas features, e tamb\u00e9m de outras features que n\u00e3o apresentam uma clara rela\u00e7\u00e3o com a target, vou realizar o ajuste de um modelo Decision Tree para determinar se algumas features podem ser retiradas da base, tornando mais claro o entendimento do comportamento da target para o modelo.","4d3905e1":"- Fica claro, atrav\u00e9s dos gr\u00e1ficos acima, que faltam dados de alguns dias do m\u00eas 03. Isso vai causar um n\u00famero menor de observa\u00e7\u00f5es v\u00e1lidas ao final do processo de tratamento da base.","03acc3cd":"- Parace haver uma clara correla\u00e7\u00e3o inversa entre '% Silica Concentrate' a as features 'Starch Flow' e 'Amina Flow'.\n- Vou deletar 2 colunas que n\u00e3o ser\u00e3o utilizadas na previs\u00e3o da qualidade do min\u00e9rio: '% Iron Concentrate' **j\u00e1 que esta feature \u00e9 altamente correlacionada com a target** e n\u00e3o ser\u00e1 alvo da nossa previs\u00e3o, e tamb\u00e9m a coluna 'Date' j\u00e1 que n\u00e3o se trata de um problema de s\u00e9rie temporal.","d244426d":"- \u00c9 percept\u00edvel, atrav\u00e9s da avalia\u00e7\u00e3o do **Gr\u00e1fico de Erro de Predi\u00e7\u00e3o** e do **Gr\u00e1fico de Res\u00edduos**, que os erros n\u00e3o t\u00eam distribui\u00e7\u00e3o normal.\n- Este \u00e9 o motivo pelo qual a regress\u00e3o linear (e suas m\u00e9tricas) n\u00e3o s\u00e3o bons nesse tipo de situa\u00e7\u00e3o. Pois, ainda que os Gr\u00e1ficos e o R\u00b2 indiquem um modelo com p\u00e9ssimo desempenho, n\u00e3o \u00e9 isto que est\u00e1 acontecendo na realidade.","818d335c":"## Ajuste com Random Forest\n- Agora que o entendimento dos dados e da import\u00e2ncia das features est\u00e1 bastante claro, \u00e9 poss\u00edvel ajustar o modelo Random Forest, sem que o overfitting seja um problema n\u00e3o percebido. \n- A escolha pelo Random Forest segue a mesma ideia de que os modelos de \u00e1rvore s\u00e3o bons com rela\u00e7\u00f5es n\u00e3o lineares e resistentes \u00e0 overfitting, al\u00e9m de n\u00e3o ser necess\u00e1rio normaliza\u00e7\u00e3o dos dados. ","9bbf7692":"- Parece existir uma clara correla\u00e7\u00e3o entre **'% Silica Concentrate'** e os n\u00edveis de **'% Iron Feed'**, **'Starch Flow'** e **'Amina Flow'**. \n- Vou avaliar plotando as 3 features juntas com o n\u00edvel de S\u00edlica.","63e7bf7d":"- Parece que as features '% Iron Feed' e '% Silica Feed' s\u00e3o medidas 3 vezes ao dia, e o \u00faltimo valor do dia anterior \u00e9 replicado para o primeiro valor do dia atual. \n- Vou deletar os valores duplicados para essas features, a fim de tentar encontrar um padr\u00e3o (os hor\u00e1rios nos quais essas features s\u00e3o de fato medidas).","04d2335f":"- Ainda que o modelo Decision Tree tenha apresentado bons resultados, parece que ele n\u00e3o foi capaz de capturar toda a complexidade dos dados.\n- Ainda assim, ele serve como bom baseline e tamb\u00e9m para ajudar na Redu\u00e7\u00e3o de Dimensionalidade.\n- \u00c9 necess\u00e1rio, portanto, ajustar um modelo com maior capacidade para capturar melhor a complexidade dos dados.","4bda1b66":"- Vou tratar os valores **maiores ou iguais a 5.1 como outliers** e retir\u00e1-los da base. \n- Vou avaliar novamente os 2 erros, a fim de entender se o impacto desses outliers foi minimizado.","cbd40e1d":"## Tratamento dos Dados","023c9366":"- Quando avaliamos a distribui\u00e7\u00e3o dos Dados de Previs\u00e3o, fica percept\u00edvel que o modelo n\u00e3o conseguiu capturar as correla\u00e7\u00f5es dos dados.\n- Como as correla\u00e7\u00f5es n\u00e3o s\u00e3o lineares, \u00e9 necess\u00e1rio um n\u00famero maior de inst\u00e2ncias para que o modelo consiga generalizar melhor.\n- N\u00e3o \u00e9 indicado ajustar um modelo muito complexo em uma base t\u00e3o pequena, foi seria muito f\u00e1cil cair em overfitting. \n- \u00c9 necess\u00e1rio aumentar a disponibilidade de dados.","458cf7b4":"- Na mudan\u00e7a da primeira para a segunda hora ocorreu a mudan\u00e7as nas features '% Iron Concentrate' e\t'% Silica Concentrate'.\n- Parece que estes valores s\u00e3o medidos em laborat\u00f3rio a cada hora.","f2a5e98e":"- Considerando a defini\u00e7\u00e3o de cada feature e como elas s\u00e3o medidas, temos um poss\u00edvel problema aqui.\n\n- J\u00e1 que algumas features s\u00e3o amostradas a cada 20 segundos e outras somente a cada hora, como s\u00e3o registrados os dados dessas \u00faltimas, a cada 20 segundos? Ser\u00e1 que os \u00faltimos valores dispon\u00edveis s\u00e3o repetidos at\u00e9 a pr\u00f3xima hora?\n\n- E em rela\u00e7\u00e3o \u00e0s 2 \u00faltimas features, as quais o valor depende de uma medi\u00e7\u00e3o em laborat\u00f3rio. Em qual intervalo essa an\u00e1lise de laborat\u00f3rio \u00e9 feita? Os \u00faltimos dados dispon\u00edveis s\u00e3o repetidos at\u00e9 que o pr\u00f3ximo valor de medi\u00e7\u00e3o esteja dispon\u00edvel?","0108f560":"- A distribui\u00e7\u00e3o da feature target mostrou-se assim\u00e9trica \u00e0 direita e, segundo o gr\u00e1fico acima, SEM correla\u00e7\u00e3o clara com a data. \n- Nesta abordagem, portanto, o problema n\u00e3o ser\u00e1 tratado como uma s\u00e9rie temporal.\n- Em todo o per\u00eddo os valores tendem a ficar abaixo de 3 (mostrando mais uma vez a assimetria), mas de forma constante em todo no intervalo de tempo. ","69cb702b":"## Entendimento Inicial dos Dados","42a43f5d":"## Dicion\u00e1rio de Dados\n\n**date:** Data e hora da medi\u00e7\u00e3o, que cobre o per\u00edodo de 10 de Mar\u00e7o de 2017 \u00e0 9 de Setembro de 2017. Algumas vari\u00e1veis s\u00e3o amostradas a cada 20 segundos, enquanto outras s\u00e3o amostradas a cada 1 hora. \n\n**% Iron Feed:** Percentual de Ferro presente no min\u00e9rio de ferro, antes de entrar na planta de flota\u00e7\u00e3o.\n\n**% Silica Feed:** Percentual de S\u00edlica (impureza) presente no min\u00e9rio de ferro, antes de entrar na planta de flota\u00e7\u00e3o.\n\n**Starch Flow:** Fluxo do reagente **Amido** (m\u00b3\/h).\n\n**Amina Flow:** Fluxo do reagente **Amina** (m\u00b3\/h).\n\n**Ore Pulp Flow:** Fluxo da polpa de Min\u00e9rio (t\/h).\n\n**Ore Pulp pH:** PH da polpa de Min\u00e9rio (0 a 14).\n\n**Ore Pulp Density:** Densidade da polpa de Min\u00e9rio (1 a 3 kg\/cm\u00b3).\n\n**Flotation Column 01 Air Flow:** Fluxo de ar que vai para a c\u00e9lula de flota\u00e7\u00e3o (Nm\u00b3\/h).\n\n**Flotation Column 02 Air Flow:** Fluxo de ar que vai para a c\u00e9lula de flota\u00e7\u00e3o (Nm\u00b3\/h).\n\n**Flotation Column 03 Air Flow:** Fluxo de ar que vai para a c\u00e9lula de flota\u00e7\u00e3o (Nm\u00b3\/h).\n\n**Flotation Column 04 Air Flow:** Fluxo de ar que vai para a c\u00e9lula de flota\u00e7\u00e3o (Nm\u00b3\/h).\n\n**Flotation Column 05 Air Flow:** Fluxo de ar que vai para a c\u00e9lula de flota\u00e7\u00e3o (Nm\u00b3\/h).\n\n**Flotation Column 06 Air Flow:** Fluxo de ar que vai para a c\u00e9lula de flota\u00e7\u00e3o (Nm\u00b3\/h).\n\n**Flotation Column 07 Air Flow:** Fluxo de ar que vai para a c\u00e9lula de flota\u00e7\u00e3o (Nm\u00b3\/h).\n\n**Flotation Column 01 Level:** N\u00edvel de Espuma na c\u00e9lula de flota\u00e7\u00e3o (mm).\n\n**Flotation Column 02 Level:** N\u00edvel de Espuma na c\u00e9lula de flota\u00e7\u00e3o (mm).\n\n**Flotation Column 03 Level:** N\u00edvel de Espuma na c\u00e9lula de flota\u00e7\u00e3o (mm).\n\n**Flotation Column 04 Level:** N\u00edvel de Espuma na c\u00e9lula de flota\u00e7\u00e3o (mm).\n\n**Flotation Column 05 Level:** N\u00edvel de Espuma na c\u00e9lula de flota\u00e7\u00e3o (mm).\n\n**Flotation Column 06 Level:** N\u00edvel de Espuma na c\u00e9lula de flota\u00e7\u00e3o (mm).\n\n**Flotation Column 07 Level:** N\u00edvel de Espuma na c\u00e9lula de flota\u00e7\u00e3o (mm).\n\n**% Iron Concentrate:** Percentual de Ferro presente no min\u00e9rio de ferro, ap\u00f3s o processo de flota\u00e7\u00e3o (medi\u00e7\u00e3o de laborat\u00f3rio).\n\n**% Silica Concentrate:** Percentual de S\u00edlica (impureza) presente no min\u00e9rio de ferro, ap\u00f3s o processo de flota\u00e7\u00e3o (medi\u00e7\u00e3o de laborat\u00f3rio).","11b9b30b":"## An\u00e1lise Explorat\u00f3ria dos Dados","c7d0de9f":"- A distribui\u00e7\u00e3o da feature target continua assim\u00e9trica \u00e0 direita, mas um pouco menos que na abordagem anterior.\n- Resta agora ajustar o modelo e avaliar a generaliza\u00e7\u00e3o.","d5d284b2":"- Os valores de erros est\u00e3o aceit\u00e1veis para o problema em quest\u00e3o.\n- A proximidade entre essas m\u00e9tricas indica boa generaliza\u00e7\u00e3o do modelo.\n- Resta saber da consist\u00eancia e distribui\u00e7\u00e3o do erro.","80ae7297":"## Avalia\u00e7\u00e3o da Qualidade das Previs\u00f5es","f18b86cc":"## Ajuste do Modelo\n- Como a maioria das correla\u00e7\u00f5es se mostraou n\u00e3o linear, al\u00e9m da vari\u00e1vel target apresentar distribui\u00e7\u00e3o assim\u00e9trica, vou utilizar um modelo mais robusto: Decision Tree Regressor. Dessa forma, n\u00e3o ser\u00e1 necess\u00e1rio fazer a Normaliza\u00e7\u00e3o dos dados.\n- Vou avaliar o erro com o **RMSE**, pois este caso **penaliza os erros maiores**, que \u00e9 exatamente o que queremos evitar no caso da concentra\u00e7\u00e3o de Silica.","06d485db":"- Essa diferen\u00e7a grande entre o **RMSE** e o erro **Median Absolute Error** pode indicar a presen\u00e7a de outliers na base. Vou avaliar a vari\u00e1vel target com um boxplot e tentar identificar quais s\u00e3o os outliers."}}