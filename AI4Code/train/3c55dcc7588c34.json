{"cell_type":{"0c2eac06":"code","f8fdd0fb":"code","5fd1577d":"code","0fb2edc8":"code","37390e3e":"code","64ff27b1":"code","f3549313":"code","df1a0efb":"code","9e27f91a":"code","297d41c7":"code","22c92368":"code","f9fd67ed":"code","e13df2f4":"code","f259b055":"code","298f4c2f":"code","47d4ef0a":"code","bac827f5":"code","2c4a93a4":"code","554c87b7":"code","b3a0c203":"code","fafc9cab":"code","30f78f75":"code","28602c61":"code","7dbf67e3":"code","acf37a23":"code","eb2cbeca":"code","5f5ca841":"code","bff2bb2d":"code","0b9960c3":"markdown","061ee4d5":"markdown","e4c99610":"markdown","7b6cabe0":"markdown","2c8612c5":"markdown","79292766":"markdown","dd82e023":"markdown","c6c48b06":"markdown","764b691c":"markdown"},"source":{"0c2eac06":"!pip install nltk\n!python -m nltk.downloader stopwords\n!pip install textacy\n!pip install spacy\n!pip install Pattern\n!python -m spacy download en_core_web_sm","f8fdd0fb":"!pip install gensim ","5fd1577d":"import en_core_web_sm\nimport gensim                \nimport nltk\nimport os                  # dealing with directories\nimport re\nimport string\nimport spacy\nimport random \nimport textacy\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd \nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom gensim.models import LdaModel, LdaMulticore, HdpModel\nfrom gensim.utils import simple_preprocess\nfrom nltk.corpus import stopwords, wordnet\nfrom gensim.test.utils import common_texts\nfrom gensim.models import KeyedVectors\nfrom gensim.corpora.dictionary import Dictionary\nfrom gensim.models import CoherenceModel\nfrom gensim.utils import simple_preprocess\nfrom random import shuffle # mixing up or currently ordered data that might lead our network astray in training.4\nfrom tqdm import tqdm      # a nice pretty percentage bar for tasks. \\\nfrom wordcloud import WordCloud","0fb2edc8":"# globals \n\nnum_topics_list = [5,10]\nalpha_list = [0.4,0.6]\nbeta_list = [0.4,0.6]\n\nDIR = ''\n\nnlp = en_core_web_sm.load()","37390e3e":"train_df = pd.read_csv('\/kaggle\/input\/hackerearthericsson\/train.csv')\ntrain_df.head(2)","64ff27b1":"# creating an anchor columns which will be useful for us to merge all our R&D work in the end.\n\nN = 12\ntrain_df['anchor_column'] = train_df.apply(\n    lambda x: ''.join(random.choices(string.ascii_uppercase + string.digits, k = N)),\n    axis=1\n)","f3549313":"# we will have to save this train df with anchor id's if we want to save other df's in this experiment. if we don't save then new anchor id's\n# created while reading the train df will not be compatible with the already saved ones\n\ntrain_df.to_csv(DIR+'anchor_train_df.csv', index=False)","df1a0efb":"anchor_train_df = pd.read_csv(DIR+'anchor_train_df.csv')","9e27f91a":"\ndef spacy_preprocessing(text_logs):\n    lemmatized_logs = [\n        [token.lemma_.lower() for token in nlp(doc) if not token.is_stop and \n        not token.is_punct and \n        len(token.text)>3 and \n        token.pos_ in ['NOUN', 'ADJ', 'VERB']\n        ] \n        for doc in text_logs\n    ]\n    return lemmatized_logs\n\ndef n_grams(lemma_unigrams, ngrams = 2, min_freq = 2):\n    bigrams = [list(textacy.extract.basics.ngrams(nlp(' '.join(unigram_list)), ngrams , min_freq=min_freq)) for unigram_list in lemma_unigrams] \n    uni_and_bigrams = [unigram_list + [token.text for token in bigram_list] for unigram_list, bigram_list in zip(lemma_unigrams, bigrams)]\n    return uni_and_bigrams\n\npositive_reviews_df = anchor_train_df[['anchor_column', 'positives']]\npositive_reviews_df = positive_reviews_df.rename(\n    columns = {'positives': 'reviews'}\n)\nprocessed_text_logs = spacy_preprocessing(list(positive_reviews_df['reviews']))\npositive_reviews_df['preprocessed_reviews'] = processed_text_logs\nbigram_text_logs = n_grams(processed_text_logs, ngrams=2)\npositive_reviews_df['preprocessed_reviews'] = bigram_text_logs","297d41c7":"def get_bag_of_words(text_logs):\n    unique_words_dictionary = Dictionary(text_logs)\n    # filter words which appear in more than 70 percent of the documents, and in less than 15 documents\n    unique_words_dictionary.filter_extremes(no_below=15, no_above=0.7, keep_n=10000)\n    bag_of_words_corpus = [\n      unique_words_dictionary.doc2bow(text) for text in text_logs\n    ]\n    return bag_of_words_corpus, unique_words_dictionary\n\ndef get_tfidf(text_logs):\n    bag_of_words_corpus, _ = get_bag_of_words(text_logs)\n    tfidf_model = TfidfModel(bag_of_words_corpus)\n    tdidf_corpus = [tfidf_model[corpus] for corpus in bag_of_words_corpus]\n    return tdidf_corpus\n\n\nbag_of_words_corpus, unique_words_dictionary = get_bag_of_words(list(positive_reviews_df['preprocessed_reviews']))","22c92368":"def build_lda_model(\n    vectorized_corpus, unique_words_dictionary, n_topics = 2, \n    alpha = 1, beta=1\n):\n    lda = LdaMulticore(\n      vectorized_corpus, \n      id2word = unique_words_dictionary,\n      num_topics=n_topics,\n      alpha=alpha,\n      eta = beta,\n      passes = 60\n    )\n    return lda\n\ndef get_coherance_score(lda_model, text_logs, unique_words_dictionary):\n    coherence_model_lda = CoherenceModel(\n      model=lda_model, \n      texts=text_logs, \n      dictionary=unique_words_dictionary, \n      coherence='c_v'\n    )\n    coherence_score = coherence_model_lda.get_coherence()\n    return coherence_score\n\ndef lda_grid_search(\n    vectorized_corpus, unique_words_dictionary, num_topics_list, \n    alpha_list, beta_list, processed_reviews_df\n):\n    '''\n    output: returns a df with num_topics, alpba, beta and corresponding coherence score\n    '''\n    params_df = pd.DataFrame(columns=['n_topics', 'alpha', 'beta', 'coherence_score'])\n    for k in num_topics_list:\n        for alpha in alpha_list:\n            for beta in beta_list:\n                lda_model = build_lda_model(\n                    vectorized_corpus, unique_words_dictionary, n_topics = k, \n                    alpha = alpha, beta = beta\n                )\n                coherence_score = get_coherance_score(\n                    lda_model, list(processed_reviews_df['preprocessed_reviews']), unique_words_dictionary\n                )\n                params_df = params_df.append(\n                    {\n                        'n_topics': k,\n                        'alpha': alpha,\n                        'beta': beta,\n                        'coherence_score': coherence_score\n                    },\n                    ignore_index=True\n                )\n    return params_df\n\npositive_reviews_params_df = lda_grid_search(\n    bag_of_words_corpus, unique_words_dictionary, num_topics_list, \n    alpha_list, beta_list, positive_reviews_df\n)\npositive_reviews_params_df.to_csv(DIR+'positive_reviews_params_df.csv', index=False)\npositive_reviews_params_df = pd.read_csv(DIR+'positive_reviews_params_df.csv')\npositive_reviews_params_df = positive_reviews_params_df.sort_values(['coherence_score'], ascending=False)\npositive_reviews_params_df.reset_index(inplace=True, drop=True)\nn_topics = positive_reviews_params_df.loc[0,'n_topics']\nalpha = positive_reviews_params_df.loc[0,'alpha']\nbeta = positive_reviews_params_df.loc[0,'beta']\nlda_model = lda_model = build_lda_model(\n    bag_of_words_corpus, unique_words_dictionary, n_topics = int(n_topics), alpha = alpha, beta = beta\n)","f9fd67ed":"def get_predictions(df, lda_model, n_topics=2):\n    '''\n    params: df which has preprocessed , anchor column\n    returns a df with the probability of topic belongingness\n    '''\n    topic_prob_df = pd.DataFrame(\n      columns = ['anchor_column'] + list(range(n_topics))\n    ) \n    for row in df.itertuples():\n        topic_probs_dict = {'anchor_column':row.anchor_column}\n        lda_op = lda_model.get_document_topics(\n            unique_words_dictionary.doc2bow(row.preprocessed_reviews)\n        )\n        for topic_tuple in lda_op:\n            topic_probs_dict.update({\n              topic_tuple[0]: topic_tuple[1]\n        })\n        topic_prob_df = topic_prob_df.append(topic_probs_dict, ignore_index=True)\n    return topic_prob_df\n\ntopic_prob_df = get_predictions(\n  positive_reviews_df[['anchor_column','preprocessed_reviews']], \n  lda_model, n_topics=int(n_topics)\n)\ncoherence_score = get_coherance_score(\n  lda_model, list(positive_reviews_df['preprocessed_reviews']), \n  unique_words_dictionary\n)\nprint('coherance score:', coherence_score)","e13df2f4":"def get_df_for_frequent_roles(topic_prob_df, most_frequent = 20):\n    employee_role_topic_prob_df = pd.merge(\n      anchor_train_df[['anchor_column','job_title']],\n      topic_prob_df,\n      how='inner',\n      on='anchor_column'\n    )\n    job_title_frequency = anchor_train_df['job_title'].value_counts().to_dict()\n    job_title_frequency = dict(\n      sorted(job_title_frequency.items(), key=lambda x: x[1], reverse=True)\n    )\n    employee_role_topic_prob_df = employee_role_topic_prob_df.loc[\n      employee_role_topic_prob_df['job_title'].isin(\n          list(job_title_frequency.keys())[:most_frequent]\n      )\n    ]\n    return employee_role_topic_prob_df\n\ndef get_top_words(lda_model, job_title, assigned_topic, num_words = 10):\n    '''\n    params: lda_model, mean prob dict of each topic, top n topics to consider\n    output: unique words acroos top topics considered. top 5 words per topic\n    '''\n    unique_words_across_topics = set()\n    for idx, topic in lda_model.show_topics(formatted=False, num_words= num_words):\n        if idx == assigned_topic:\n            [unique_words_across_topics.add(word[0]) for word in topic]\n    return unique_words_across_topics\n\nemployee_role_topic_prob_df = get_df_for_frequent_roles(topic_prob_df, most_frequent = 10)\nemployee_role_topic_prob_df['assigned_topic'] = employee_role_topic_prob_df.apply(\n    lambda x: x[list(range(int(n_topics)))].astype('float64').argmax(),\n    axis=1\n)\nemployee_role_topic_prob_df_grouped = employee_role_topic_prob_df.groupby(['job_title'])['assigned_topic'].agg(pd.Series.median).to_frame()\nplt.figure(figsize = (16, 16))\ni = 1\nfor job_title,row in employee_role_topic_prob_df_grouped.iterrows():\n    unique_words_across_topics = get_top_words(lda_model, job_title, row['assigned_topic'], num_words = 12)\n    wordcloud = WordCloud(\n      max_font_size=30, background_color=\"white\"\n    ).generate(\" \".join(list(unique_words_across_topics)))\n    # Display the generated image:\n    plt.subplot(4,3,i)\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.title(job_title)\n    i+=1\nplt.show()","f259b055":"negative_reviews_df = anchor_train_df[['anchor_column', 'negatives']]\nnegative_reviews_df = negative_reviews_df.rename(\n    columns = {'negatives': 'reviews'}\n)\nprocessed_text_logs = spacy_preprocessing(list(negative_reviews_df['reviews']))","298f4c2f":"negative_reviews_df['preprocessed_reviews'] = processed_text_logs","47d4ef0a":"bigram_text_logs = n_grams(processed_text_logs, ngrams=2)\nnegative_reviews_df['preprocessed_reviews'] = bigram_text_logs\nnegative_reviews_df.head(2)","bac827f5":"bag_of_words_corpus, unique_words_dictionary = get_bag_of_words(\n    list(negative_reviews_df['preprocessed_reviews'])\n)","2c4a93a4":"negative_reviews_params_df = lda_grid_search(\n    bag_of_words_corpus, unique_words_dictionary, num_topics_list,\n    alpha_list, beta_list, negative_reviews_df\n)","554c87b7":"negative_reviews_params_df.to_csv(DIR+'negative_reviews_params_df.csv', index=False)\nnegative_reviews_params_df = pd.read_csv(DIR+'negative_reviews_params_df.csv')","b3a0c203":"negative_reviews_params_df = negative_reviews_params_df.sort_values(['coherence_score'], ascending=False)\nnegative_reviews_params_df.reset_index(inplace=True, drop=True)","fafc9cab":"n_topics = negative_reviews_params_df.loc[0,'n_topics']\nalpha = negative_reviews_params_df.loc[0,'alpha']\nbeta = negative_reviews_params_df.loc[0,'beta']","30f78f75":"lda_model = lda_model = build_lda_model(\n    bag_of_words_corpus, unique_words_dictionary, \n    n_topics = int(n_topics), alpha = alpha, beta = beta\n)","28602c61":"topic_prob_df = get_predictions(\n    negative_reviews_df[['anchor_column','preprocessed_reviews']], \n    lda_model, \n    n_topics=int(n_topics)\n)","7dbf67e3":"coherence_score = get_coherance_score(lda_model, list(negative_reviews_df['preprocessed_reviews']), unique_words_dictionary)\nprint('coherance score:', coherence_score)","acf37a23":"employee_role_topic_prob_df = get_df_for_frequent_roles(topic_prob_df, most_frequent = 12)","eb2cbeca":"employee_role_topic_prob_df['assigned_topic'] = employee_role_topic_prob_df.apply(\n    lambda x: x[list(range(int(n_topics)))].astype('float64').argmax(),\n    axis=1\n)\nemployee_role_topic_prob_df.head(2)","5f5ca841":"employee_role_topic_prob_df_grouped = employee_role_topic_prob_df.groupby(['job_title'])['assigned_topic'].agg(pd.Series.mode).to_frame()\nemployee_role_topic_prob_df_grouped.head(-1)","bff2bb2d":"plt.figure(figsize = (16, 16))\ni = 1\nfor job_title,row in employee_role_topic_prob_df_grouped.iterrows():\n    unique_words_across_topics = get_top_words(lda_model, job_title, row['assigned_topic'], num_words = 12)\n    wordcloud = WordCloud( \n      max_font_size=30, background_color=\"white\"\n    ).generate(\" \".join(list(unique_words_across_topics)))\n    # Display the generated image:\n    plt.subplot(4,3,i)\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.title(job_title)\n    i+=1\nplt.show()","0b9960c3":"# Words to vectors\nNow, we have a list of selected words for each document. We now have to convert these words into vectors. We can do this by bag-of-words or by TfIdf. Both of the methods will not preserve the order of the words in the document, also we are not interested in the order of the words, we are mainly concerned with the term frequencies here. Since we have a huge dataset  most of the words will be repeated across the corpus. If we consider TfIdf, the rarity of the words will not be reflected as due to repetitive words. So, we will use bag-of-words here.","061ee4d5":"We can see that the nontechnical roles such as Fulfilment Associate, Warehouse Associate were positive about the health benefits the company was offering, discounts, working hours, stocks, and other benefits. The technical roles were positive about the career opportunities, great team, product on which they were working, their managers, work culture, etc. The topics assigned to the technical roles differ slightly, but the crux of our experiment's output and the hypothesis that we have defined at the beginning of this article are the same.","e4c99610":"Running the same pipeline as above for negative reviews. ","7b6cabe0":"# NEGATIVE REVIEWS ANALYSIS","2c8612c5":"I'am be using Gensim's LDA model in this experiment.\nGensim's LDA takes bag of words\/tfidf  as input to it's model. So, before converting the words to bow, remove the stopwords, lemmatize and do other necessary preprocessing such as removing numerical data. \n\nGensim has a predifined doc2bow functionality which takes in the list of words from each text\/doc and then converts it into the bow. This is done row wise. For this the doc2bow functinality needs the prior knowledge of all the unique words that we have in the collection of documents\/ texts. For this we pass the list of lists (where each inner list consists of words converted to their root form) into a class called Dictionary and we will get a list of all the unique words in the corpora.\n\nSince we need a list for each document\/text, we create it while preprocessing itself.\n\nFor preprocessing I have experimented with custom preprocessing with nltk, gensim's simple preprocessing functionality, and preprocessing with spacy. Out of all of these,I chose spacy because:\n    Spacy gave a good results because spacy's lemmatization model is better than the other two. \n    Also, for generating the bigrams I personally liked spacy because using it was quite straight forward.","79292766":"In this notebook we will be creating a topic model for the employee reviews which we have in the dataset. The aim of the experiment is to find out what do employee across different domains\/ roles experience, because a person will generally write a review what they actually experience in the office. \n\nMy hypothesis: I think that there will be two types of employees in every office. The first kind of employees  will be the technical people such as SDE, Data Scientist who will generally comment on technical aspects of the company. The other kind of employees will be the non technical persons who will talk about the culture and other oppurtunities of the company. ","dd82e023":"# Hyperparameter tuning and model building\nWe will now tune the LDA model for a set of hyperparameters:\nThe number of topics.\nAlpha:- Represents document-topic density. With a higher alpha, documents are made up of more topics, and with a lower alpha, documents contain fewer topics.\nBeta:- Represents topic-word density, with high beta, topics are made up of more words, vice-versa.","c6c48b06":"# Evaluating the model\nThe best hyperparameters that we got after tuning are:\nnumber of topics:- 5\nalpha:- 0.4\nbeta:- 0.6\n\nAs we have trained the model now, we will predict the most probable topic for every document in the corpus. We will consider the 12 most frequent roles, take the most repeated topic for each role. Now, with the topic-word distributions, we can know what words each role has expressed the most.","764b691c":"# Text preprocessing\nIn every NLP project, it's very important to understand what syntactic and lexical filters to apply to the corpus, this would be an add-on to the performance of the model. In this project, we are only concerned with the views of employees about the company. So, we will only use NOUNS, VERBS, ADJECTIVES as these will capture most of the information that we want for the analysis. We will use both unigrams and bigrams as there are few bigrams in the dataset such as work-life, software engineer, work culture, etc."}}