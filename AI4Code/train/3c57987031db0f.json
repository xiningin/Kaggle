{"cell_type":{"7f9c14e3":"code","9348e5ec":"code","2c89ecdb":"code","ec883e84":"code","c9a3894c":"code","83fb1898":"markdown","c6dd418d":"markdown","e94d0c31":"markdown","7e1f740b":"markdown","cbc12c63":"markdown","e7c94f18":"markdown","16f97ffe":"markdown"},"source":{"7f9c14e3":"import pandas as pd\n\ncars = pd.read_csv('\/kaggle\/input\/craigslist-cars-in-british-columbia\/cars.csv', index_col=0)\n\n#filling missing data\nfill_lib = { #this library specifies how to fill missing values based on column\n    'condition':'unknown',\n    'odometer':cars['odometer'].mean(),\n    'paint color':'unknown',\n    ' ':'unknown'\n}\ncars.fillna(fill_lib, inplace=True) #fill the missing values\ncars = cars.replace(' ', 'unknown') #replace blank space with 'unknown'\n\n#### type ####\n#combing 'pickup' and 'truck' types\ncars['type'] = cars['type'].replace(' pickup', ' truck')\n\n#### price #####\n#convert price column to number\ncars['price'] = cars['price'].str.replace('$', '').str.replace(',', '').astype('int')\n\n#### latlong ####\n#split latlong\ncars['latitude'] = cars['latlong'].map(lambda x: x.split(';')[0]).astype('float32')\ncars['longitude'] = cars['latlong'].map(lambda x: x.split(';')[1]).astype('float32')\n\n####location#####\n#clean up location column - we will just take the first word to clean up data\n#make the location column all lower case avoid separating the same place into different categories\ncars['location'] = cars['location'].str.lower()\n\n#make the reaplcements below so when we take the first word the town names still make sense\nloc_lib = {\n    'port ':'port-',\n    'new ':'new-',\n    'north ':'north-',\n    'maple ':'maple-',\n    'campbell ':'campbell-', \n    'fraser ':'fraser-',\n    'langley city':'langley-city',\n    'langley township':'langley-township',\n    'pitt ':'pitt-',\n    'white ':'white-',\n    ',':'',\n    'powell ':'powell-'\n    }\n\nlocation_cleaned = cars['location'].map(lambda x: x.lower())\nfor a, b in loc_lib.items(): #make the replacements in the library above\n    location_cleaned = location_cleaned.map(lambda x: x.replace(a, b))\n\n#take the first word of the location column only as it is too specific\ncars['location_simple'] = location_cleaned.map(lambda x: x.split(' ')[0])\n\n#### year, make, model####\n#get year, make, and model from make column\n#clean up common variations or makes\nmake_lib = {\n    'mercedes-benz':'mercedes','mercedes benz':'mercedes',\n    'smart car':'smart-car',\n    'land ':'land-','range ':'range-',\n    'vw':'volkswagen',\n    'grand ':'grand-',\n    'chevy':'chevrolet',\n    'model ':'model-',\n    'f150':'f-150', 'f250':'f-250', 'f350':'f-350',\n    'cx9':'cx-9', 'cx5':'cx-5','cx3':'cx-3',\n    'mazda3':'3',\n    'santa fe':'santa-fe'\n    }\n\nmake_cleaned = cars['make'].map(lambda x: x.lower())\nfor a, b in make_lib.items(): #make the replacements in the library above\n    make_cleaned = make_cleaned.map(lambda x: x.replace(a, b))\n\ncars['year'] = make_cleaned.map(lambda x: x.split(' ', 2)[0]).astype('int')\ncars['age'] = 2021 - cars['year']\ncars['make_'] = make_cleaned.map(lambda x: x.split(' ', 2)[1])\n\ndef get_third(x):\n    try:\n        return(x.split(' ', 2)[2])\n    except:\n        return('unknown') #if there is no more text then the model must be missing\ncars['model'] = make_cleaned.map(get_third)\n\n##### text body #####\n#get a column from the body text\n#positive = cars['body text'].str.find('lady').map(lambda x: 0 if x==-1 else 1)\npos_words = ['lady', 'off road', 'winter', 'lift', 'vintage']\nneg_words = ['crack', 'torn', 'damage', 'leak', 'missing']\n\ncars['pos_words'] = cars['body text'].map(lambda x: 1 if any(txt in x for txt in pos_words) else 0).astype(bool)\ncars['neg_words'] = cars['body text'].map(lambda x: 1 if any(txt in x for txt in neg_words) else 0).astype(bool)\ncars['low_text'] = cars['body text'].map(lambda x: 1 if len(x.split()) < 30 else 0).astype(bool) #text has less than 30 words\n\n#convert the odometer reading to an integer to save space\ncars['odometer'] = cars['odometer'].astype('int')\n\n#drop extraneous columns\ncars = cars.drop(['body text', 'latlong', 'make'], axis=1)\n\n#many of the model values ended up as a year because people put the year in twice. Deal with that here.\nyears = [str(y) for y in range(1900, 2020)]#check for these years in make column\nmake = cars[cars.make_.isin(years)]['model'].map(lambda x: x.split(' ')[0]) #get the actual make as the first word of the next column\n\ndef get_second(x): #function to get the second word of text when it might be missing\n    try:\n        return(x.split(' ', 1)[1])\n    except:\n        return('None') #if there is no more text then the model must be missing\n\ncars.loc[make.index, 'make_'] = make.values #replace the years make with the actual make\ncars.loc[make.index, 'model'] = cars.loc[make.index, 'model'].map(get_second) #remove the year from the model column\n\ndef get_second_3_parts(x): #function to get the second word of text when it might be missing\n    try:\n        return(x.split(' ', 2)[1])\n    except:\n        return('None') #if there is no more text then the model must be missing\n    \ndef get_third_3_parts(x): #function to get the second word of text when it might be missing\n    try:\n        return(x.split(' ', 2)[2])\n    except:\n        return('None') #if there is no more text then the model must be missing\n\n\n#split the model out into one column for each word\ncars['model0'] = cars['model'].map(lambda x: x.split(' ')[0])\ncars['model1'] = cars['model'].map(get_second_3_parts)\ncars['model2'] = cars['model'].map(get_third_3_parts)\n\n#fill unknown values in some columns by the mode of the model\nfill_mode_cols = ['type', 'size', 'drive', 'cylinders'] #for these columns we, will replace 'unknown' with the mode of that make of vehicle\n\ndef fill_func(x, m):\n    try:\n        fill_val = m.loc[x['model0']].values[0]\n        if type(fill_val) != str: return(x[col])\n        elif pd.isnull(x[col]):return(fill_val)\n        else: return(x[col])\n    except:\n        return(x[col])\n\nfor col in fill_mode_cols:\n    modes = cars.groupby('model0')[col].agg([pd.Series.mode])\n    cars[col] = cars.apply(fill_func, args=(modes,), axis=1)\n\n#filling remaining missing data\nfill_lib = { #this library specifies how to fill missing values based on column\n    'drive':'unknown',\n    'type':'unknown',\n    'cylinders':'unknown',\n    'size':'unknown',\n}\n\ncars.fillna(fill_lib, inplace=True) #fill the missing values\ncars.head()","9348e5ec":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer, TransformedTargetRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import KFold, cross_val_score, GridSearchCV\nfrom sklearn.impute import SimpleImputer\n\n#read in data\ndata = cars.copy()\n\n#### missing values #####\ndata.dropna(inplace=True)#there are only a few dozen missing values so drop those rows\n\n#cleaning\ndata = data[data.odometer<1e6] #drop rows where the odometer is very high\ndata = data[data.price<1e5] #drop rows where the price is very high\n#drop unneeded columns\ndata.drop(['year', 'model', 'latitude', 'longitude', 'pos_words', 'neg_words', 'low_text', 'location', 'model1', 'model2', 'model0', 'location_simple'], axis=1, inplace=True)\n\n#only consider the top 12 makes\ntop_makes = data.make_.value_counts()[:15].index\ndata = data[data['make_'].isin(top_makes)]\n\n#get rid of negative age values (for model years that are in the future)\ndata['age'] = data['age'].map(lambda x: x if x>=0 else 0)\n\n#add yes or no columns\ndata['4wd'] = data['drive'].map(lambda x: 1 if x==' 4wd' else 0)\ndata['diesel'] = data['fuel'].map(lambda x: 1 if x==' diesel' else 0)\ndata['electric'] = data['fuel'].map(lambda x: 1 if x==' electric' else 0)\ndata['hybrid'] = data['fuel'].map(lambda x: 1 if x==' hybrid' else 0)\ndata['clean title'] = data['title status'].map(lambda x: 1 if x==' clean' else 0)\ndata['new'] = data['condition'].map(lambda x: 1 if x==' new' else 0)\ndata['manual transmission'] = data['transmission'].map(lambda x: 1 if x==' manual' else 0)\ndata['sale by owner'] = data['sale type'].map(lambda x: 1 if x=='owner' else 0)\ndata['type'] = data['type'].replace(['unknown', ' other'], value='unknown\/other').replace(' offroad', value=' truck')\n\n#label encode the size column\nsize_lib = {' sub-compact':0, ' compact':1, ' mid-size':2, ' full-size':3 , 'unknown':np.nan}\ndata['size'] = data['size'].replace(size_lib)\n\n#convert cylinders to number\ndef get_cyls(s):\n    try:\n        x = s.split(' ')[1]\n        if x == 'other':\n            return(np.nan)\n        else:\n            return(x)\n    except:\n        return(np.nan)\ndata['cylinders'] = data['cylinders'].map(get_cyls)\n\n#drop excell columns\ndata.drop(['fuel', 'title status', 'transmission', 'condition', 'drive', 'sale type'], axis=1, inplace=True)\n\ndata.head()","2c89ecdb":"#### create dataframes for cross-validation ####\nX = data.copy()\ny = X.pop('price')\n\n#### pipeline ###\ncat_cols = ['paint color', 'type', 'make_'] #categorical columns\nnumeric_cols = [col for col in X.columns if col not in cat_cols] #the rest are numeric\n\n# Preprocessing for categorical data\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')), \n    ('num_standardizer', StandardScaler(with_mean=False))])#standardization helps with linear models so certain features are not more heavily weighted simply because of their scale\ncategorical_transformer = Pipeline(steps=[\n    ('cat_encoder', OneHotEncoder(handle_unknown='ignore')), \n    ('cat_standardizer', StandardScaler(with_mean=False))\n    ])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numeric_cols),\n        ('cat', categorical_transformer, cat_cols)])\n\ndef unit(x): #unit transformation\n    return(x)\n\nmodel = TransformedTargetRegressor(\n        regressor=Lasso(alpha=250, max_iter=1e5, tol=1e-12), \n        func=unit,#np.log10,\n        inverse_func=unit,#sp.special.exp10\n    )\n\n# Bundle preprocessing and modeling code in a pipeline\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\n\n#cross validation\nscores = cross_val_score(pipeline, X, y, cv=5, scoring = 'neg_mean_absolute_error')\nprint('MAE (5-fold cross validation):', -scores.mean())","ec883e84":"from sklearn.metrics import median_absolute_error\nimport matplotlib.pyplot as plt\n\n#split training and test data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=41)\n\n#thanks to the tutorial following tutorial for the code below\n#https:\/\/scikit-learn.org\/stable\/auto_examples\/inspection\/plot_linear_model_coefficient_interpretation.html\n\n#fit on training data\npipeline.fit(X_train, y_train)\ny_pred = pipeline.predict(X_train)\nmae = mean_absolute_error(y_train, y_pred)\nstring_score = f'MAE on training set: {mae:.2f} $'\ny_pred = pipeline.predict(X_test)\nmae = mean_absolute_error(y_test, y_pred)\nstring_score += f'\\nMAE on testing set: {mae:.2f} $'\nfig, ax = plt.subplots(figsize=(8, 8))\nplt.scatter(y_test, y_pred)\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\"red\")\nplt.text(1000, 60000, string_score)\nplt.title('Lasso model')\nplt.ylabel('Model predictions')\nplt.xlabel('Truths')\nplt.xlim([0, 1e5])\n_ = plt.ylim([0, 1e5])","c9a3894c":"import seaborn as sns\n\n#get the feature names from the model\nfeature_names = pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['cat_encoder'].get_feature_names(input_features=cat_cols)\nfeature_names = np.concatenate([numeric_cols, feature_names])\n\n#get the standard deveations of the data from the pipeline preprocessor (which we divided by to standardize the data)\nstds_cat = pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['cat_standardizer'].scale_\nstds_num = pipeline.named_steps['preprocessor'].named_transformers_['num'].named_steps['num_standardizer'].scale_\nstds = np.concatenate([stds_num, stds_cat])\n\ndevs = [1 for x in range(len(feature_names))] #we will scale each coefficient of the linear model by this value (most will be a change of 1)\ndevs[0]= 25000 #for the odometer coefficient we will show for a change of 25000km\nprint(feature_names[0], 'shown for an increase of', devs[0])\ndevs[3]= 5 #for the age coefficient we will show for a change of 5 years\nprint(feature_names[3], 'shown for an increase of', devs[3])\n\ncoefs = pd.DataFrame(\n    pipeline.named_steps['model'].regressor_.coef_ * devs \/ stds, #divide by stds to get the untransformed coefficeint, multiply by devs to get the dollar value change for the given deviation\n    columns=['Coefficient importance'], index=feature_names)\n\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import RepeatedKFold\n\ncv_model = cross_validate(\n    pipeline, X, y, cv=RepeatedKFold(n_splits=5, n_repeats=5),\n    return_estimator=True, n_jobs=-1)\ncoefs = pd.DataFrame(#coefficients of the linear model\n    [est.named_steps['model'].regressor_.coef_ \/ stds * devs\n     for est in cv_model['estimator']],\n    columns=feature_names)\n\nordered_cols = coefs.mean(axis=0).sort_values(ascending=False).index#order the coefficients from largest to smallest\ncoefs_ordered = pd.DataFrame()\n\nimp_coefs = abs(coefs.mean(axis=0))>500 #remove the coefficients that have an impact less than $500\nimp_coefs = imp_coefs[imp_coefs.values==True]\n\nfor col in ordered_cols:\n    if col in imp_coefs.index:\n        coefs_ordered[col] = coefs[col]\n\ncoefs_ordered.rename(columns = {'odometer': 'odometer (per {} km)'.format(devs[0]),\n                               'age': 'age (per {} years)'.format(devs[3]),\n                               'cylinders': 'cylinders (per each additional)'.format(devs[3]),\n                               'size': 'size (per step up)'},\n                    inplace=True)        \n\nplt.figure(figsize=(16, 8))\nsns.boxplot(data=coefs_ordered, orient='h', color='cyan', saturation=0.5)\nplt.axvline(x=0, color='.5')\nplt.xlabel('$ Value of Trait')\nplt.title('Value of Most Important Vehicle Traits')\nplt.subplots_adjust(left=.3)\nplt.xticks(range(-4000, 11000, 1000))\nplt.grid()","83fb1898":"Again, we can see that the model does not fit the data particularly well, though it clearly captures some trends. We could also log transform the target variable to deal with the 'bend' seen in the scattered data, but it would make the model harder to interpret.\n\nFinally, we can look at the coefficients of the linear model to see what dollar value it puts on the features provided.","c6dd418d":"We begin with some data cleaning and feature generation.","e94d0c31":"The MAE is fairly high, but here we are most concerned with interpretability of the model. Next we will split the data into training and testing sets and plot the models predictions of the test set vs the actual values.","7e1f740b":"Define a processing pipeline and run cross-validation over the whole dataset. This was done to determine a best value for the Lasso regularization term.","cbc12c63":"In this notebook I am going to fit a linear model (Lasso) to the vehicle data to determine which vehicle traits might be the most important in determining its price. While the accuracy of the model is not as high as it could be with a more complex model, it is straightforward to interpret the model's linear coefficients as a dollar value.","e7c94f18":"We next drop the few rows that still have missing data and simplify the columns for interpretability. Also only include data for the top 12 makes of vehicle.","16f97ffe":"This plot displays the linear coefficients of the linear model, and can be used to get an idea of the value of each trait. Keep in mind that the linear model is not particularly accurate, but it does capture the general trend of the data.\n\nValues shown are averages over 5 cross-validation data sets repeated 5 times for a total of 25 models. The error bars show the extend of the data with outliers shown as separate dots.\n\nMost traits are 'yes\/no'. The exceptions are cyilnders (\\\\$ per cylinder), age (\\\\$ per 5 years), odometer (\\\\$ per 25,000 km), and size (\\\\$ per step up between: sub-compact, compact, mid-size, full-size)\n\nNote that only the top 12 most popular brans were considered in this linear analysis. They are (in order of popuularity): Ford, Toyota, Honda, Chevrolet, Mazda, Nissan, Dodge, BMW, Hyundai, Volkswagen, Mercedes, Kia, Jeep, Audi, and GMC.\n\n(Tesla was not included. When added to the plot above, it has a value trait of around $25,000, similar to porsche.)"}}