{"cell_type":{"6360f85c":"code","c10da56b":"code","ca2d8a76":"code","79bff68b":"code","118a224a":"code","4b686cf1":"code","38ff7f31":"code","e9a70d72":"code","6275ccb8":"code","5e02cbf8":"code","827205dd":"code","90131307":"code","6989856e":"code","50411229":"code","85ea1beb":"code","19e22f69":"code","d59bfb51":"code","d8464b88":"code","eda256c3":"code","de1a694e":"code","d233fdcc":"markdown","c78b481a":"markdown","e933783c":"markdown","01189691":"markdown","e04caf44":"markdown","4ab7d30c":"markdown","cca266fd":"markdown","63a6eff9":"markdown","d0e6146f":"markdown","17b824e6":"markdown","313a938a":"markdown","eaa60348":"markdown","4da78bdc":"markdown","ddd207d8":"markdown","fde48230":"markdown","0bc466a3":"markdown"},"source":{"6360f85c":"import pandas as pd \nimport numpy as np\nfrom collections import Counter\n\nimport sklearn\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\nimport xgboost\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nOVERSAMPLE = False\nPERFORM_PCA = False","c10da56b":"all_cat_vars = ['Profession', 'Status', 'edu', 'Irregular', 'residence', 'prev_diagnosed', 'communication', 'Month', 'side_effects']\ncategorical_vars = ['Profession', 'Status', 'edu', 'communication', 'Month', 'side_effects']\ncontinuous_vars = ['age', 'Money']\n\ndef load_data(csv_file, impute=False):\n    df = pd.read_csv(csv_file)\n    df = df.drop(columns=['ID'])\n\n    # impute missing values\n    if impute:\n        for col in all_cat_vars:\n            df[col] = df[col].fillna(df[col].value_counts().index[0])\n        imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n        df[continuous_vars] = imputer.fit_transform(df[continuous_vars].values)\n\n    # encode categorical variables\n    df = df.replace({'no': 0, 'yes': 1})\n    categorical_vars = ['Profession', 'edu', 'Status', 'communication', 'Month', 'side_effects']\n    df = pd.get_dummies(df, columns=categorical_vars)\n    return df\n\ntrain = load_data('..\/input\/pasc-data-quest-20-20\/doctor_train.csv', impute=True)","ca2d8a76":"train.head()","79bff68b":"# classes distribution\nratio = list(train['Y'].value_counts())\nratio","118a224a":"def split(df):\n    y = df['Y']\n    X = df[[x for x in df.columns if x != 'Y']]\n    return X, y\n\nX, y = split(train)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nprint('train:', X_train.shape, y_train.shape)\nprint('test:', X_test.shape, y_test.shape)","4b686cf1":"# oversampling\nif OVERSAMPLE:\n    from imblearn.over_sampling import RandomOverSampler\n    oversample = RandomOverSampler(sampling_strategy='minority', random_state=0)\n    X_over, y_over = oversample.fit_resample(X, y)","38ff7f31":"if OVERSAMPLE:\n    print(Counter(y_train))\n    print(Counter(y_over))","e9a70d72":"# principal component analysis\nif PERFORM_PCA:\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    X_train = scaler.transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    pca = PCA(.95)\n    pca.fit(X_train)\n    print(pca.n_components_)","6275ccb8":"if PERFORM_PCA:\n    X_train = pca.transform(X_train)\n    X_test = pca.transform(X_test)","5e02cbf8":"# feature selection\n\ndtrain = xgboost.DMatrix(X, y)\n\nparams = {\n    'scale_pos_weight': max(ratio)\/min(ratio),\n    'n_estimators': 400\n}\nnum_rounds = 5\n\nbst = xgboost.train(params, dtrain, num_rounds)\nax = xgboost.plot_importance(bst, \n                             importance_type='gain', \n                             max_num_features=len(X_train.columns), \n                             xlabel='Gain')\nfig = ax.figure\nfig.set_size_inches(8, 8)","827205dd":"# Evaluation\ndef results(model):\n    pred = model.predict(X_test)\n    print('Accuracy:', accuracy_score(y_test, pred))\n    print('f1 score:', f1_score(y_test, pred))\n    print('ROC-AUC:', roc_auc_score(y_test, pred))\n    print('\\nConfusion matrix:')\n    return confusion_matrix(y_test, pred)","90131307":"# LGBM\n\n# Tuned LGBM model\n# lgbm_model = LGBMClassifier(scale_pos_weight=10,\n                            # n_estimators=140,\n                            # learning_rate=0.17,\n                            # max_depth=8,\n                            # num_leaves=30,\n                            # reg_alpha=0.2,\n                            # reg_lambda=0.2,\n                            # subsample=0.1,\n                            # gamma=0\n                            # )\n\n# This configuration gives the best results on this dataset \nlgbm_model = LGBMClassifier(scale_pos_weight=10,\n                            n_estimators=140\n                            )\n\nGRID_SEARCH = False\nif GRID_SEARCH:\n    param_dist = {\n        'scale_pos_weight': [7, 7.1, 7.2, 7.3, max(ratio)\/min(ratio), 8],\n        }\n    grid_search = GridSearchCV(lgbm_model, \n                            n_jobs=-1, \n                            param_grid=param_dist, \n                            cv = 3, \n                            scoring='f1', \n                            verbose=5)\n    grid_search.fit(X_train, y_train)\n\n    print(grid_search.best_params_, grid_search.best_score_)\n    results = pd.DataFrame(grid_search.cv_results_)\n    results[['mean_test_score', 'std_test_score', 'params']]","6989856e":"lgbm_model.fit(X_train, y_train)","50411229":"results(lgbm_model)","85ea1beb":"# XGBoost\n\n# Tuned XGBoost model\n# xgb_model = XGBClassifier(scale_pos_weight=max(ratio)\/min(ratio),\n                        #   n_estimators=439,\n                        #   gamma=0.28,\n                        #   reg_alpha=0.8,\n                        #   reg_lambda=1.5,\n                        #   learning_rate=0.117,\n                        #   max_depth=5,\n                        #   colsample_bytree=0.565,\n                        #   subsample=0.8\n                        #   )\n\n# This configuration gives the best results on this dataset \nxgb_model = XGBClassifier(scale_pos_weight=max(ratio)\/min(ratio),\n                          n_estimators=439\n                          )\nGRID_SEARCH = False\nif GRID_SEARCH:\n    params = {\n        'scale_pos_weight': [max(ratio)\/min(ratio)+0.1, max(ratio)\/min(ratio)+0.2, max(ratio)\/min(ratio)+0.3]\n        }\n\n    grid_search = GridSearchCV(model, \n                            param_grid = params, \n                            scoring='f1',\n                            n_jobs=-1,\n                            cv=3,\n                            verbose=5)\n    grid_search.fit(X, y)\n    print(grid_search.best_params_, grid_search.best_score_)\n    results = pd.DataFrame(grid_search.cv_results_)\n    results[['mean_test_score', 'std_test_score', 'params']]","19e22f69":"xgb_model.fit(X_train, y_train)","d59bfb51":"results(xgb_model)","d8464b88":"# combined xgb + lgbm\nxgb_pred = xgb_model.predict(X_test)\nlgbm_pred = lgbm_model.predict(X_test)\npred = []\nfor xgb, lgbm in zip(xgb_pred, lgbm_pred):\n    pred.append(round(np.mean([xgb, lgbm])))\n\nprint('Accuracy:', accuracy_score(y_test, pred))\nprint('f1 score:', f1_score(y_test, pred))\nprint('ROC-AUC:', roc_auc_score(y_test, pred))\nprint('\\nConfusion matrix:')\nconfusion_matrix(y_test, pred)","eda256c3":"# comparing all models\ndef get_results(y_true, y_pred):\n    acc = accuracy_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    roc_auc = roc_auc_score(y_true, y_pred)\n    return acc, f1, roc_auc\n\nxgb_acc, xgb_f1, xgb_roc_auc = get_results(y_test, xgb_model.predict(X_test))\nlgbm_acc, lgbm_f1, lgbm_roc_auc = get_results(y_test, lgbm_model.predict(X_test))\n\nxgb_pred = xgb_model.predict(X_test)\nlgbm_pred = lgbm_model.predict(X_test)\nstacked_pred = []\nfor xgb, lgbm in zip(xgb_pred, lgbm_pred):\n    stacked_pred.append(round(np.mean([xgb, lgbm])))\nstacked_acc, stacked_f1, stacked_roc_auc = get_results(y_test, stacked_pred)\n\nprint('\\t\\t\\tStacked\\t\\t\\tXGBoost\\t\\t\\tLGBM\\n')\nprint('Accuracy\\t', stacked_acc, '\\t', xgb_acc, '\\t', lgbm_acc)\nprint('F1 Score\\t', stacked_f1, '\\t', xgb_f1, '\\t', lgbm_f1)\nprint('ROC AUC Score\\t', stacked_roc_auc, '\\t', xgb_roc_auc, '\\t', lgbm_roc_auc)","de1a694e":"# submission\n\ndef submit(model, output_file='submission'):\n    test = load_data('..\/input\/pasc-data-quest-20-20\/doctor_test.csv')\n    pred = model.predict(test.values)\n    pred.tolist()\n\n    submission = pd.DataFrame()\n    submission['ID'] = range(0, len(test))\n\n    submission['Y'] = pred\n    submission['Y'] = submission['Y'].replace({0:'no', 1:'yes'})\n    submission.to_csv(output_file + '.csv', index=False)\n\n    print(submission.shape)\n    print(output_file + '.csv saved!')\n    return submission.head()\n\nsubmit(lgbm_model, 'submission_lgbm')","d233fdcc":"In order to handle imbalanced classes oversampling is performed. It duplicates randomly selected rows from the dataset as per the strategy used.\n\nThe models we'll be using have a different parameter to handle imbalanced distribution.","c78b481a":"## Imports","e933783c":"### XGBoost Model","01189691":"GridSearch used for Hyperparamter Tuning. Tuning one parameter at a time is convenient, but multiple parameters can be passed too.","e04caf44":"## Models","4ab7d30c":"### LGBM Model","cca266fd":"Dropping columns \/ Dimensionality reduction for this dataset reduced the performance, but be sure to experiment with it.","63a6eff9":"## Data Preprocessing","d0e6146f":"Stacking is a popular technique which is the combining of two or more machine learning models. Here I've taken the mean of the results predicted by both the models.","17b824e6":"### Comparison Table","313a938a":"The LGBM Model has the highest ROC-AUC score and thus outperforms the others.","eaa60348":"The most important hyperparamter in both the models is 'scale_pos_weight'. This hyperparameter is responsible for handling the imbalanced distribution of classes.\n\nAn optimum value is the count of the majority class examples divided by the count of the minority class examples. Experimenting with this value can provide major differences in performance.","4da78bdc":"### Stacked LGBM + XGBoost","ddd207d8":"## Feature Selection\n","fde48230":"LGBM & XGBoost, the models I'll be using, handle missing values implicitly. Although in order to perform PCA (Principal Component Analysis) scaling and imputing is required.","0bc466a3":"## Submission"}}