{"cell_type":{"ff4904c8":"code","2fbd4054":"code","b5a5b8f7":"code","23a2a801":"code","4584c83e":"code","c75a35ae":"code","36f7ccd2":"code","b9616d78":"code","14b68acb":"code","5c4af549":"code","dbc3e0c6":"code","5aa6caee":"code","54ab1eba":"code","d9f09a5f":"code","e7ce8d76":"code","18ee1aca":"code","eda84854":"code","7adc94b1":"code","642ed98b":"code","2724db09":"code","81b193aa":"code","ab5270a6":"code","512a22ed":"code","dbf7328a":"code","b311c5a9":"code","a4da4a8e":"code","7a28c87b":"code","4c0641e9":"code","02190e6f":"code","8bb9a899":"code","16b3a0fc":"code","a5d142df":"markdown","a186dcd2":"markdown","bf4c9940":"markdown","32d02659":"markdown","17f5b4a5":"markdown","5e2bc5a4":"markdown","2630d19f":"markdown","cbd178ad":"markdown","0f25d27d":"markdown","10d79e42":"markdown","a2c4127e":"markdown"},"source":{"ff4904c8":"from time import time\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport cv2\n\nfrom sklearn.model_selection import train_test_split, GroupKFold, StratifiedKFold\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nimport torchvision\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.ops import nms\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\n\ntorch.cuda.empty_cache() \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2fbd4054":"DATA_PATH = '..\/input\/lisa-traffic-light-dataset'\nos.listdir(DATA_PATH)","b5a5b8f7":"DAY_TRAIN_PATH = '..\/input\/lisa-traffic-light-dataset\/Annotations\/Annotations\/dayTrain\/'\nNIGHT_TRAIN_PATH = '..\/input\/lisa-traffic-light-dataset\/Annotations\/Annotations\/nightTrain\/'","23a2a801":"train_day = []\nfor clipName in tqdm(sorted(os.listdir(DAY_TRAIN_PATH))):\n    df = pd.read_csv(os.path.join(DAY_TRAIN_PATH,clipName,'frameAnnotationsBOX.csv'),sep=';')\n    train_day.append(df)\n    \ntrain_day_df = pd.concat(train_day,axis=0)","4584c83e":"df = df.drop(['Origin file','Origin track','Origin track frame number'],axis=1)","c75a35ae":"def changeFilename(x):\n    filename = x.Filename\n    \n    splitted = filename.split('\/')\n    clipName = splitted[-1].split('--')[0]\n   \n    return os.path.join(DATA_PATH,f'dayTrain\/dayTrain\/{clipName}\/frames\/{splitted[-1]}')\n\ndf['Filename'] = df.apply(changeFilename,axis=1)","36f7ccd2":"df['Annotation tag'].unique()","b9616d78":"label_to_idx = {'go':1, 'warning':2, 'stop': 3}\nidx_to_label = {v:k for k,v in label_to_idx.items()}\n\ndef changeAnnotation(x):\n    if 'go' in x['Annotation tag']:\n        return label_to_idx['go']\n    elif 'warning' in x['Annotation tag']:\n        return label_to_idx['warning']\n    elif 'stop' in x['Annotation tag']:\n        return label_to_idx['stop']\n    \ndf['Annotation tag'] = df.apply(changeAnnotation,axis=1)\n\nannotation_tags = df['Annotation tag'].unique()\nannotation_tags","14b68acb":"df.columns = ['image_id','label','x_min','y_min','x_max','y_max','frame']","5c4af549":"fig, ax = plt.subplots(len(annotation_tags),1,figsize=(15,10*len(annotation_tags)))\n\nfor i, tag in enumerate(annotation_tags):\n    sample = df[df['label']==tag].sample(1)\n    bbox = sample[['x_min','y_min','x_max','y_max']].values[0]\n    \n    image = cv2.imread(sample.image_id.values[0])\n    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n    \n    cv2.rectangle(image,(bbox[0],bbox[1]),(bbox[2],bbox[3]),(220, 0, 0), 2)\n    \n    ax[i].set_title(idx_to_label[tag])\n    ax[i].set_axis_off()\n    ax[i].imshow(image)","dbc3e0c6":"df['clipNames'] = df[['image_id']].applymap(lambda x: x.split('\/')[5])\ndf['clipNames'].unique()","5aa6caee":"def split(df,p=0.25):\n    clipNames = sorted(df['clipNames'].unique())\n\n    dayClips = [name for name in clipNames if 'day' in name]\n\n    testDayClipNames = list(np.random.choice(dayClips,int(len(dayClips)*p)))\n    testClipNames =  testDayClipNames\n\n    trainDayClipNames = list(set(dayClips) - set(testDayClipNames))\n    trainClipNames =  trainDayClipNames\n    \n    train_df = df[df.clipNames.isin(trainClipNames)]\n    test_df = df[df.clipNames.isin(testClipNames)]\n    \n    return train_df, test_df\n","54ab1eba":"train_df, test_df = split(df)","d9f09a5f":"train_df, val_df = split(train_df)","e7ce8d76":"EPOCHS = 1\nBATCH_SIZE = 2","18ee1aca":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(device)","eda84854":"class TrafficLightsDataset:\n    def __init__(self, df, transforms=None):\n        super().__init__()\n\n        self.image_ids = df.image_id.unique()\n        self.df = df\n        self.transforms = transforms\n        \n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n\n    def __getitem__(self, index: int):\n\n        image_id = self.image_ids[index]\n        records = self.df[self.df.image_id == image_id]\n\n        image = cv2.imread(image_id)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n        \n        boxes = records[['x_min','y_min','x_max','y_max']].values\n        boxes = torch.as_tensor(boxes,dtype=torch.float32)\n        \n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        area = torch.as_tensor(area, dtype=torch.float32)\n\n        labels = torch.as_tensor(records.label.values, dtype=torch.int64)\n        \n        iscrowd = torch.zeros_like(labels, dtype=torch.int64)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        target['image_id'] = torch.tensor([index])\n        target['area'] = area\n        target['iscrowd'] = iscrowd\n\n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n            target['boxes'] = torch.as_tensor(sample['bboxes'],dtype=torch.float32)\n            target['labels'] = torch.as_tensor(sample['labels'])\n            \n        return image, target, image_id","7adc94b1":"class LossAverager:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","642ed98b":"def collate_fn(batch):\n    return tuple(zip(*batch))","2724db09":"def getTrainTransform():\n    return A.Compose([\n        A.Resize(height=256, width=256, p=1),\n        A.Flip(0.5),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\ndef getValTransform():\n    return A.Compose([\n        A.Resize(height=256, width=256, p=1),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\ndef getTestTransform():\n    return A.Compose([\n        A.Resize(height=256, width=256, p=1),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})","81b193aa":"trainDataset = TrafficLightsDataset(train_df,getTrainTransform())\nvalDataset = TrafficLightsDataset(val_df,getValTransform())\ntestDataset = TrafficLightsDataset(test_df,getTestTransform())","ab5270a6":"trainDataLoader = DataLoader(\n    trainDataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=0,\n    collate_fn=collate_fn\n)\n\nvalDataLoader = DataLoader(\n    valDataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=0,\n    collate_fn=collate_fn\n)\n\ntestDataLoader = DataLoader(\n    testDataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=0,\n    collate_fn=collate_fn\n)","512a22ed":"images, targets, image_ids = next(iter(trainDataLoader))\n\nboxes = targets[0]['boxes'].numpy().astype(np.int32)\nimage = images[0].permute(1,2,0).numpy()","dbf7328a":"def displayImage(image, boxes):\n    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\n    for box in boxes:\n        cv2.rectangle(image,\n                      (box[0], box[1]),\n                      (box[2], box[3]),\n                      (220, 0, 0), 1)\n\n    ax.set_axis_off()\n    ax.imshow(image)\n\n    plt.show()","b311c5a9":"displayImage(image,boxes)","a4da4a8e":"model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\nN_CLASS = 4 \n\nINP_FEATURES = model.roi_heads.box_predictor.cls_score.in_features\n\nmodel.roi_heads.box_predictor = FastRCNNPredictor(INP_FEATURES, N_CLASS)","7a28c87b":"model.to(device)\n\nparams = [p for p in model.parameters() if p.requires_grad]\n\noptimizer = torch.optim.Adam(params)\n\nlr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)","4c0641e9":"lossHist = LossAverager()\nvalLossHist = LossAverager()\n\nfor epoch in range(EPOCHS):\n    \n    start_time = time()\n    model.train()\n    lossHist.reset()\n    \n    for images, targets, image_ids in tqdm(trainDataLoader):\n        \n        images = torch.stack(images).to(device)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        \n        bs = images.shape[0]\n        \n        loss_dict = model(images, targets)\n        \n        totalLoss = sum(loss for loss in loss_dict.values())\n        lossValue = totalLoss.item()\n        \n        lossHist.update(lossValue,bs)\n\n        optimizer.zero_grad()\n        totalLoss.backward()\n        optimizer.step()\n    \n    if lr_scheduler is not None:\n        lr_scheduler.step(totalLoss)\n\n    print(f\"[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}]\")\n    print(f\"Epoch {epoch}\/{EPOCHS}\")\n    print(f\"Train loss: {lossHist.avg}\")\n    \n    if(epoch == 10):\n        torch.save(model.state_dict(), 'fasterrcnn_resnet{}_fpn.pth'.format(epoch))","02190e6f":"torch.save(model.state_dict(), 'fasterrcnn_resnet{}_fpn.pth'.format(epoch))","8bb9a899":"model.load_state_dict(torch.load('..\/input\/weights\/fasterrcnn_resnet50_fpn.pth'))","16b3a0fc":"preprocess = transforms.Compose([\n    transforms.ToTensor()\n])\n\ncap = cv2.VideoCapture(\"project.avi\")\n\nwhile(True):\n    ret, input = cap.read()\n    image = input.copy()\n    input = preprocess(input).float()\n    input = input.unsqueeze_(0)\n    input = input.type(torch.cuda.FloatTensor)\n\n    result = model(input)\n\n    boxes = result[0]['boxes'].type(torch.cuda.FloatTensor)\n    scores = result[0]['scores'].type(torch.cuda.FloatTensor)\n    labels = result[0]['labels'].type(torch.cuda.FloatTensor)\n\n    mask = nms(boxes,scores,0.3)\n    boxes = boxes[mask]\n    scores = scores[mask]\n    labels = labels[mask]\n\n    boxes = boxes.data.cpu().numpy().astype(np.int32)\n    scores = scores.data.cpu().numpy()\n    labels = labels.data.cpu().numpy()\n\n    mask = scores >= 0.5\n    boxes = boxes[mask]\n    scores = scores[mask]\n    labels = labels[mask]\n\n    colors = {1:(0,255,0), 2:(255,255,0), 3:(255,0,0)}\n\n    for box,label in zip(boxes,labels):\n        image = cv2.rectangle(image,\n                          (box[0], box[1]),\n                          (box[2], box[3]),\n                          (0,0,255), 1)\n\n    cv2.imshow(\"image\", image)\n    \n    if cv2.waitKey(0):\n        break\n    ","a5d142df":"<a id = \"1\"><\/a><br>\n# Importing Libraries","a186dcd2":"<a id = \"5\"><\/a><br>\n# Defining Dataset and Dataloader","bf4c9940":"<a id = \"2\"><\/a><br>\n# Reading CSV File and Preprocessing","32d02659":"<a id = \"3\"><\/a><br>\n# Hyperparameters","17f5b4a5":"<a id = \"8\"><\/a><br>\n# Saving Models","5e2bc5a4":"<a id = \"10\"><\/a><br>\n# Testing on Video with OpenCV","2630d19f":"<a id = \"9\"><\/a><br>\n# Loading Models","cbd178ad":"<a id = \"6\"><\/a><br>\n# Display Frames ","0f25d27d":"<a id = \"7\"><\/a><br>\n# Training FasterRCNN Model","10d79e42":"<a id = \"4\"><\/a><br>\n# Selecting Device (GPU - CPU)","a2c4127e":"<font color = 'red'>\nContent: \n\n1. [Importing Libraries](#1)\n1. [Reading CSV File and Preprocessing](#2)\n1. [Hyperparameters](#3)\n1. [Selecting Device (GPU - CPU)](#4)\n1. [Defining Dataset and Dataloader](#5)\n1. [Display Frames](#6)\n1. [Training FasterRCNN Model](#7)\n1. [Saving Models](#8)\n1. [Loading Model](#9)\n1. [Testing on Video with OpenCV](#10)\n"}}