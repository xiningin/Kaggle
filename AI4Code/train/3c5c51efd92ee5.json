{"cell_type":{"d06d06ea":"code","17f3aa30":"code","1aa47ae7":"code","896e7a4d":"code","b1797ade":"code","868ac5e2":"code","d205738f":"code","928acf87":"code","cfcb5024":"code","7a7cdca5":"code","cffa5702":"code","208ea847":"code","b681df0b":"code","7ccfd167":"code","cbc463fd":"code","54acd0a9":"code","c2628604":"code","def78a5d":"code","700a3f6c":"code","56a0d89b":"markdown","ff31ca9b":"markdown","cbc0508a":"markdown","f0129a6d":"markdown","2fcaed31":"markdown","7786c167":"markdown","5e5315d3":"markdown","1f57fcd6":"markdown","1d6a39fa":"markdown","d325c77d":"markdown","84e5c656":"markdown"},"source":{"d06d06ea":"import re\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n    \nprint(tf.__version__)","17f3aa30":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nGCS_PATH = KaggleDatasets().get_gcs_path(\"covid-19-x-ray-10000-images\")\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nIMAGE_SIZE = [180, 180]\nEPOCHS = 25","1aa47ae7":"filenames = tf.io.gfile.glob(str(GCS_PATH + '\/dataset\/covid\/*'))\nfilenames.extend(tf.io.gfile.glob(str(GCS_PATH + '\/dataset\/normal\/*')))\nrandom.shuffle(filenames)","896e7a4d":"train_filenames, test_filenames = train_test_split(filenames, test_size=0.1)\ntrain_filenames, val_filenames = train_test_split(train_filenames, test_size=0.1)","b1797ade":"COUNT_NORMAL = len([filename for filename in train_filenames if \"normal\" in filename])\nprint(\"Normal images count in training set: \" + str(COUNT_NORMAL))\n\nCOUNT_COVID = len([filename for filename in train_filenames if \"covid\" in filename])\nprint(\"COVID-19 images count in training set: \" + str(COUNT_COVID))","868ac5e2":"train_list_ds = tf.data.Dataset.from_tensor_slices(train_filenames)\nval_list_ds = tf.data.Dataset.from_tensor_slices(val_filenames)\ntest_list_ds = tf.data.Dataset.from_tensor_slices(test_filenames)\n\nfor f in train_list_ds.take(5):\n    print(f.numpy())","d205738f":"TRAIN_IMG_COUNT = tf.data.experimental.cardinality(train_list_ds).numpy()\nprint(\"Training images count: \" + str(TRAIN_IMG_COUNT))\n\nVAL_IMG_COUNT = tf.data.experimental.cardinality(val_list_ds).numpy()\nprint(\"Validating images count: \" + str(VAL_IMG_COUNT))","928acf87":"def get_label(file_path):\n    # convert the path to a list of path components\n    parts = tf.strings.split(file_path, os.path.sep)\n    # The second to last is the class-directory\n    return parts[-2] == \"covid\"","cfcb5024":"def decode_img(img):\n  # convert the compressed string to a 3D uint8 tensor\n  img = tf.image.decode_jpeg(img, channels=3)\n  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n  img = tf.image.convert_image_dtype(img, tf.float32)\n  # resize the image to the desired size.\n  return tf.image.resize(img, IMAGE_SIZE)","7a7cdca5":"def process_path(file_path):\n    label = get_label(file_path)\n    # load the raw data from the file as a string\n    img = tf.io.read_file(file_path)\n    img = decode_img(img)\n    return img, label","cffa5702":"train_ds = train_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\nval_ds = val_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\ntest_ds = test_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)","208ea847":"def prepare_for_training(ds, cache=True):\n    # This is a small dataset, only load it once, and keep it in memory.\n    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n    # fit in memory.\n    if cache:\n        if isinstance(cache, str):\n            ds = ds.cache(cache)\n        else:\n            ds = ds.cache()\n\n    ds = ds.shuffle(buffer_size=1000)\n    ds = ds.batch(BATCH_SIZE)\n\n    if cache:\n        ds = ds.prefetch(buffer_size=AUTOTUNE)\n\n    return ds","b681df0b":"train_ds = prepare_for_training(train_ds)\nval_ds = prepare_for_training(val_ds)\ntest_ds = prepare_for_training(test_ds, False)","7ccfd167":"def show_batch(image_batch, label_batch):\n    plt.figure(figsize=(10,10))\n    for n in range(25):\n        ax = plt.subplot(5,5,n+1)\n        plt.imshow(image_batch[n])\n        if label_batch[n]:\n            plt.title(\"COVID-19\")\n        else:\n            plt.title(\"NORMAL\")\n        plt.axis(\"off\")","cbc463fd":"image_batch, label_batch = next(iter(train_ds))\nshow_batch(image_batch.numpy(), label_batch.numpy())","54acd0a9":"early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=5,\n                                                     restore_best_weights=True)","c2628604":"with strategy.scope():\n    reconstructed_model = tf.keras.models.load_model(\"..\/input\/test-model\/xray_model.h5\")\n    \n    METRICS = [\n        'accuracy',\n        tf.keras.metrics.Precision(name=\"precision\"),\n        tf.keras.metrics.Recall(name=\"recall\")\n    ]\n    \n    reconstructed_model.compile(\n        optimizer=\"adam\",\n        loss=\"binary_crossentropy\",\n        metrics=METRICS,\n    )","def78a5d":"history = reconstructed_model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=EPOCHS\n)","700a3f6c":"reconstructed_model.evaluate(test_ds, return_dict=True)","56a0d89b":"# Visualize our images","ff31ca9b":"# Load in our saved model\n\nWe want to load in the model from the [pneumonia notebook](https:\/\/www.kaggle.com\/amyjang\/tensorflow-pneumonia-classification-on-x-rays). Keras has an easy API to just load in previously trained models. The weights of the model will be preserved as well. First, let's define an early stopping callback.","cbc0508a":"# Evaluate our model","f0129a6d":"The following functions will help us format our dataset into the necessary (image, label) tuple for easy training.","2fcaed31":"# Introduction and Set-up\n\nAlthough there are many applications for machine learning, many times, the applications are limited because there is not enough data available. The dataset that is going to be used for this notebook is a collection of X-rays from COVID-19 patients.\n\nThere are only 70 COVID-19 X-rays in this dataset; this is not enough to train a model. However, this [notebook](https:\/\/www.kaggle.com\/amyjang\/tensorflow-pneumonia-classification-on-x-rays) has been trained on thousands of images and had a similar classification problem. Let's see if it will help us build a classifier for our small dataset.\n\nSpoiler alert: the model correctly classifies ALL of the images.\n\nRemember to change the accelerator to TPU for quick training.","7786c167":"We achieved 100% precision, 100% recall, and 100% accuracy on our training and validation dataset. Will it do the same for our testing dataset?","5e5315d3":"Because our data is not balanced, we can't only use accuracy as our metric. Let's look at precision and recall as well.","1f57fcd6":"Divide the set into training, validation, and testing sets.","1d6a39fa":"We see that we don't have many images to work with.","d325c77d":"We correctly classify all of our testing images as well! Even though we had a very limited number of images, we could build a great model by loading in a pre-trained model.","84e5c656":"# Load the images"}}