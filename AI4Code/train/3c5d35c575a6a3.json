{"cell_type":{"381b4249":"code","db84673b":"code","5a534dc0":"code","3ae8451b":"code","a3a9d443":"code","be8bb68b":"code","33679804":"code","653375ff":"code","8800960b":"code","4cef0fb5":"code","9b65d4e1":"code","c776fede":"code","0bff2380":"code","dc5ab142":"code","cda3349c":"code","613e2007":"code","70838ed9":"code","89e7531f":"code","d7ced9a8":"code","8a9a77fd":"code","b7eecaef":"code","1f4d9f3d":"code","cbfcf46b":"code","5a9767b2":"code","f579a138":"code","ab5f258a":"code","068b746e":"code","3d01dad4":"code","631306c9":"code","5228cd59":"code","c0fb21ac":"code","7f6e175e":"code","5782396c":"code","d3a83ea0":"code","1f580e1e":"code","7a49af27":"code","2b49bf52":"code","40defe69":"code","81d7c111":"code","d8208797":"code","ed5ca8aa":"code","38a560da":"code","389fe698":"markdown","af5d4677":"markdown","ac96c0c6":"markdown","8f3b11c6":"markdown","d5787561":"markdown","42be9367":"markdown","6b81c4ca":"markdown","c213372c":"markdown","115c6126":"markdown","ecd978a0":"markdown","819baf5b":"markdown","0ad81ddb":"markdown","ba98bd8f":"markdown","ef8f739c":"markdown","55fd5662":"markdown","9c3daa11":"markdown","bb0a2567":"markdown","8f44b4fa":"markdown","525996c4":"markdown"},"source":{"381b4249":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","db84673b":"california_housing_DF = pd.read_csv(\"\/kaggle\/input\/california-housing-prices\/housing.csv\")\ncalifornia_housing_DF.head()","5a534dc0":"california_housing_DF.info()","3ae8451b":"california_housing_DF.describe()","a3a9d443":"%matplotlib inline\nimport matplotlib.pyplot as plt\ncalifornia_housing_DF.hist(bins=50, figsize = (20, 15))\nplt.show()","be8bb68b":"california_housing_DF[\"income_category\"] = pd.cut(california_housing_DF[\"median_income\"], bins= [0., 1.5, 3.0, 4.5, 6., np.inf], labels = [1,2,3,4,5])\ncalifornia_housing_DF['income_category'].hist()","33679804":"from sklearn.model_selection import StratifiedShuffleSplit\nsss = StratifiedShuffleSplit(n_splits=1, test_size =0.2, random_state = 42)\nfor train_idx, test_idx in sss.split(california_housing_DF,california_housing_DF['income_category']):\n    train_set = california_housing_DF.loc[train_idx]\n    test_set =  california_housing_DF.loc[test_idx]\n\n# proportion of income category in full set \nprint(\"Proportion of income category in full set\")\nprint(\"------------------------------------------\")\nprint(california_housing_DF['income_category'].value_counts()\/len(california_housing_DF))\n\n# proportion of income category in test set\nprint(\"\\n Proportion of income category in test set\")\nprint(\"------------------------------------------\")\nprint(test_set['income_category'].value_counts()\/len(test_set))\n\n# proportion of income category in train set\nprint(\"\\n Proportion of income category in train set\")\nprint(\"------------------------------------------\")\nprint(train_set['income_category'].value_counts()\/len(train_set))\n\nfor set_ in (train_set, test_set):\n    set_.drop(\"income_category\", axis = 1, inplace = True)","653375ff":"housing = train_set.copy()","8800960b":"# s - radius of each circle represents district population\n# c - color represents price\n# alpha - visualize places with high density\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n    s=housing[\"population\"]\/100, label=\"population\", figsize=(10,7),\n    c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True)","4cef0fb5":"from pandas.plotting import scatter_matrix\nscatter_matrix(housing[['median_house_value', 'median_income', 'total_rooms','housing_median_age']], figsize=(12,8))","9b65d4e1":"housing[\"rooms_per_household\"] = housing[\"total_rooms\"]\/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]\/housing[\"total_rooms\"]\nhousing[\"population_per_household\"]=housing[\"population\"]\/housing[\"households\"]\n\n\ncorr_matrix = housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","c776fede":"print(corr_matrix[\"total_bedrooms\"].sort_values(ascending=False))","0bff2380":"housing_features = train_set.drop(\"median_house_value\", axis=1)\nhousing_labels = train_set['median_house_value'].copy()","dc5ab142":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")","cda3349c":"housing_f_numeric = housing_features.drop(\"ocean_proximity\",axis=1)\nimputer.fit(housing_f_numeric)","613e2007":"print(imputer.statistics_)\nX = imputer.transform(housing_f_numeric)\nX_train_numeric = pd.DataFrame(X, columns = housing_f_numeric.columns, index = housing_f_numeric.index)","70838ed9":"X_train_numeric[\"rooms_per_household\"] = X_train_numeric[\"total_rooms\"]\/X_train_numeric[\"households\"]\nX_train_numeric[\"bedrooms_per_room\"] = X_train_numeric[\"total_bedrooms\"]\/X_train_numeric[\"total_rooms\"]\nX_train_numeric[\"population_per_household\"]= X_train_numeric[\"population\"]\/X_train_numeric[\"households\"]","89e7531f":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train_numeric)\nprint(scaler.mean_)\nX = scaler.transform(X_train_numeric)\nX_train_numeric_scaled = pd.DataFrame(X, columns = X_train_numeric.columns, index = X_train_numeric.index)","d7ced9a8":"from sklearn.preprocessing import OneHotEncoder\ncat_encoder = OneHotEncoder()\nX_train_category = pd.DataFrame(cat_encoder.fit_transform(housing_features[['ocean_proximity']]).toarray(), columns = cat_encoder.categories_,index = housing_features.index)","8a9a77fd":"X_train_category","b7eecaef":"X_train = pd.concat([X_train_numeric_scaled, X_train_category], axis=1, sort=False)\nX_train.head()","1f4d9f3d":"y_train = housing_labels\nfrom sklearn.model_selection import cross_val_score","cbfcf46b":"def print_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard Deviation:\", scores.std())","5a9767b2":"from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\n\nlin_scores = cross_val_score(lin_reg, X_train, y_train, scoring = \"neg_mean_squared_error\",cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\nprint_scores(lin_rmse_scores)","f579a138":"from sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor()\ntree_scores = cross_val_score(tree_reg, X_train, y_train, scoring = \"neg_mean_squared_error\",cv=10)\ntree_rmse_scores = np.sqrt(-tree_scores)\nprint_scores(tree_rmse_scores)","ab5f258a":"from sklearn.ensemble import RandomForestRegressor\nforest_reg = RandomForestRegressor(n_estimators=10, random_state=42)\nforest_reg.fit(X_train, y_train)\nforest_scores = cross_val_score(forest_reg, X_train, y_train, scoring = \"neg_mean_squared_error\",cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\nprint_scores(forest_rmse_scores)","068b746e":"from sklearn import svm\nsvm_reg = svm.SVR()\nsvm_reg.fit(X_train, y_train)\nsvm_scores = cross_val_score(svm_reg, X_train, y_train, scoring = \"neg_mean_squared_error\",cv=10)\nsvm_rmse_scores = np.sqrt(-svm_scores)\nprint_scores(svm_rmse_scores)","3d01dad4":"from sklearn.model_selection import GridSearchCV\nparam_grid = [\n    {'n_estimators':[1000],\n     'max_features': [2,4,6,8,10]},\n    {'bootstrap':[False],\n     'n_estimators':[3,10],\n     'max_features': [2,3,4]}\n]\n\nforest_reg = RandomForestRegressor(random_state=42)\n# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error', return_train_score=True)\ngrid_search.fit(X_train, y_train)","631306c9":"grid_search.best_params_","5228cd59":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"],cvres['params']):\n    print(np.sqrt(-mean_score), params)","c0fb21ac":"grid_search.best_estimator_","7f6e175e":"from sklearn.model_selection import RandomizedSearchCV\nparam_dist = [\n    {'n_estimators':[3,10,30],\n     'max_features': [2,4,6,8,10]},\n    {'bootstrap':[False],\n     'n_estimators':[3,10],\n     'max_features': [2,3,4]}\n]\nrnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_dist,\n                                n_iter=50, cv=5, scoring='neg_mean_squared_error',\n                                verbose=2, n_jobs=4, random_state=42)\nrnd_search.fit(X_train, y_train)","5782396c":"rnd_search.best_params_","d3a83ea0":"housing_X_test = test_set.drop(\"median_house_value\", axis = 1)\ny_test = test_set['median_house_value'].copy()","1f580e1e":"housing_test_numeric = housing_X_test.drop(\"ocean_proximity\",axis=1)\nX = imputer.transform(housing_test_numeric)\nX_test_numeric = pd.DataFrame(X, columns = housing_test_numeric.columns, index = housing_test_numeric.index)","7a49af27":"X_test_numeric[\"rooms_per_household\"] = X_test_numeric[\"total_rooms\"]\/X_test_numeric[\"households\"]\nX_test_numeric[\"bedrooms_per_room\"] = X_test_numeric[\"total_bedrooms\"]\/X_test_numeric[\"total_rooms\"]\nX_test_numeric[\"population_per_household\"]= X_test_numeric[\"population\"]\/X_test_numeric[\"households\"]","2b49bf52":"X_test_numeric.head()","40defe69":"from sklearn.preprocessing import StandardScaler\n#scaler = StandardScaler()\n#scaler.fit(X_test_numeric)\nprint(scaler.mean_)\nX = scaler.transform(X_test_numeric)\nX_test_numeric_scaled = pd.DataFrame(X, columns = X_test_numeric.columns, index = X_test_numeric.index)","81d7c111":"cat_encoder = OneHotEncoder()\nX_test_category = pd.DataFrame(cat_encoder.fit_transform(housing_X_test[['ocean_proximity']]).toarray(), columns = cat_encoder.categories_,index = housing_X_test.index)","d8208797":"X_test = pd.concat([X_test_numeric_scaled, X_test_category], axis=1, sort=False)\nX_test.head()","ed5ca8aa":"from sklearn.metrics import mean_squared_error\nfinal_model = grid_search.best_estimator_\nfinal_predictions = final_model.predict(X_test)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nprint(final_rmse)","38a560da":"from scipy import stats\nconfidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nmean = squared_errors.mean()\nm = len(squared_errors)\n\nnp.sqrt(stats.t.interval(confidence, m - 1,\n                         loc=np.mean(squared_errors),\n                         scale=stats.sem(squared_errors)))","389fe698":"#### Randomized Search\n- When the hyperparameter space is large, it is prefereable to use RandomSearchCV instead on GridSearch CV. \n- Unlike GridSeach that evaluates all combination of hyperparameter values, this evaluates random combinations of values at each iteration. If you let the randomized search run for 1000 iterarions, it evaluates 1000 different value combinations.","af5d4677":"### Select and train a model\n- Let's try the following models:\n\n    - **Linear Regression**\n    - **Decision Tree Regressor**\n    - **Random Forest Regressor**\n    - **Support Vector Machine**\n- Let's also do the evaluation using cross-validation. This way we can test our models on multiple validation sets","ac96c0c6":"### Fine tune your model\n#### Grid Search\n- When you have no idea what values to use for hyperparameter tuning, you can try out consecutive powers of 10 (1,10,100,1000,.....)","8f3b11c6":"### Create train and test set\n- To avoid **Data Snooping** lets keep the test set aside before doing any exploratory data analysis.\n- If your dataset is large enough **Random Sampling method** would suffice but if not you run the risk of introducing **sampling bias**.\n- **Stratified sampling** is when you divide the population into homogeneous subgroups calles *strata* and you include the right number of instances from each *stratum* into the test set.\n- Let's do stratified sampling based on median income.","d5787561":"### Preparing the data for machine learning\n- Let' start by seperating thr predictors and labels. Since we don't want to apply the same transformation to the predictors and target.\n\n#### Data Cleaning\n- Let's fill in the missing values in **total_bedrooms**. For that we use scikitlearn's SimpleImputer. It computes the median of each attribute and stores it in **statistics_** instance.We fit the imputer on the train set and transform the train and test set.**Note:** We don't fit the imputer again on the test set.\n- Let's also add the 3 additional combined attributes that we discovered in the EDA part had more correlation to the target variable.\n- Next, we perform **feature scaling** using scikitlearn's StandarScalar to ensure every attribute has the same scale.We fit the standard scale on the train set and transform the train and test set.**Note:** We don't fit it again on the test set.\n- Finally we perform **one hot encoding** on the single categorical feature **ocean_proximity** and then combine all the features together. After this, we are ready to select and train a model!","42be9367":"> \ud83d\udcca **My insight**\n> * The median **age** of a houses within blocks(districts) is between 1 and 52. From the histogram we see there are a lot of houses aged 52.The number of houses aged 52 are unusally higher than the other counts. This is because the age value was capped. Any house that was older than 52 years was considered 52 years.\n> * Similarly **median house value** is also capped at 500,001 dollars. This maybe a serious problem because this is the target variable. The ML model may not learn to predict house values beyond 500,001 dollars\n> * The max and min value for **median income** is 1 and 15. From feature reference we understand that 1 actually means 10,000 dollars\n> * Notice how attributes all have different scales. We need to perform **feature scaling** to fix this.\n> * Finally, we notice that attributes like **total_rooms, total_bedrooms, households** tend to be tail heavy: they extend much farther to the right.This may mean that that the attributes have a lot of outliers. For example, we see 75% of the blocks have a total of 3148 rooms or less. We see there is one block that has a total of 39320 rooms. Having outliers will make it difficult for the Machine Learning model to detect patterns. We will transform them later to more bell-shaped attributes.\n","6b81c4ca":"> \ud83d\udcca **My insight**\n> * Out of all the models tested, the SVM model produced the worst results.The Decision Tree model peformed worse than Linear Regression.\n> * Random Forest looks the most promising model so let's try to fine tune the model by performing Hyperparameter Tuning.","c213372c":"> \ud83d\udcca **My insight**\n> * **Bedrooms per room** is negatively correlated to **median house value**. Houses with lower bedrooms to room ratio tend to be more expensive. Interesting!\n> * Also, **rooms per household** attribute tends to be more informative than **total_rooms**\n> * **total bedrooms** is positively correlated with **households, total_rooms and population**.Would we need all 4 features?","115c6126":"- We have a 95% confidence interval that the generalization error will fall between 44833.57672874, 48674.0419568","ecd978a0":"## Read the Data","819baf5b":"> \ud83d\udcca My insight\n> * The RMSE score for the tuned RandomForest is better than the score we got using the default hyperparameter values. We now have a fined tuned best model!\n> * Before we move on, let's also try to implement Randomized Search.","0ad81ddb":"> \ud83d\udcca **My insight**\n> * The houses close to the ocean are more expensive. Also, housing prices is related to population density.\n> * The correlation between **median_income** and **median_house_value** is very strong and it looks like they are positively correlated.\n> * The scatter plots of other attributes seem to be more dispersed. Let's try combining the attributes to see if they give us a better correlation","ba98bd8f":"### Exploratory Data Analysis\n- When doing EDA, create a copy of the training set so you can play with it without harming it \n\n#### 1. Visualize Geographical data\n- Let's do a scatterplot to visualize the districts in California","ef8f739c":"> \ud83d\udcca **My insight**\n> * It is important to note that **describe()** ignores null values\n> * **Standard Deviation** measures how dispered the values are.\n> * **Percentile** indicates the value below which a percentage of the datapoints fall. Example, 25% of the districts have houses that are 18 years old or lesser\n","55fd5662":"### Feature Reference\n- **longitude:** A measure of how far west a house is; a higher value is farther west\n- **latitude:** A measure of how far north a house is; a higher value is farther north\n- **housingMedianAge:** Median age of a house within a block; a lower number is a newer building\n- **totalRooms:** Total number of rooms within a block\n- **totalBedrooms:** Total number of bedrooms within a block\n- **population:** Total number of people residing within a block\n- **households:** Total number of households, a group of people residing within a home unit, for a block\n- **medianIncome:** Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n- **medianHouseValue:** Median house value for households within a block (measured in US Dollars)\n- **oceanProximity:** Location of the house w.r.t ocean\/sea","9c3daa11":"### Evaluate your model on the test set\n- Since we did not automate the cleaning and transformation step, we need to do it once more to the testing set in order for us to pass the testing set into the ML model for prediction.\n- You should use the trained Imputer and Standard Scaler to transform the test set.","bb0a2567":"> \ud83d\udcca **My insight**\n> * The dataset consists of 20640 instances. This is a small dataset by Machine Learning standards.\n> * The attribute **total_bedrooms** has only 20433 instances meaning there are few values that are missing \n> * There is 1 categorical features - **ocean_proximity**\n> * There are 7 numeric features - **longitude, latitude, housing_median_age, total_rooms,total_bedrooms, population, households, median_income, median_house_value, ocean_proximity**","8f44b4fa":"# California Housing Prices Prediction\n###  Prediction of district's median house price\n![houses4-01_cropped.png](attachment:houses4-01_cropped.png)","525996c4":"### What exactly is the business objective?\nBefore we go about exploring the data, it is always important to state what the goal of the analysis is.\n- To predict the **median house prices** in the state of California\n\n### Frame the problem\n- **Supervised Learning Task** - This is a superised learning task because we are given labels (median house value) for the training set\n- **Regression Task** - The target(median house value) is a continous value\n- **Multiple Regression** - We will be using multiple features to make the prediction\n- **Univariate Regression** - We will be predicting only a single value(median housing value) for each district\n- **Batch Learning** - Since there is no continous flow of data and the data is small enough to fit in the memory, we do batch or offline learning\n\n### Select the perfomance measure\n- A typical performance measure for regression problems is **RMSE**. RMSE corresponds to l2 norm and so is more sensitive to outliers so if there are many outlier districts **MAE** (corresponds to l1 norm) may be a better performance measure"}}