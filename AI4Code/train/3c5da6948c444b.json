{"cell_type":{"859b01ac":"code","d672fa9f":"code","641fc9b6":"code","06e3c50c":"code","2fb7f377":"code","b618f7ee":"code","e73b0ed5":"code","9f69e0bd":"code","cd0ea3cc":"code","1ab6c739":"code","3d67888c":"code","9f598d0d":"code","0fa985f5":"code","e1deff7c":"code","42253868":"code","e464ed2f":"code","d2a21b24":"markdown","6ee1dd0f":"markdown","843a112c":"markdown","d4e85e94":"markdown","c7451cee":"markdown","cbe3b824":"markdown","4f50f3d7":"markdown","354d65d6":"markdown","59f46fb8":"markdown","290ee602":"markdown","7d62fa65":"markdown"},"source":{"859b01ac":"import numpy as np\n\nimport torch\ntorch.__version__","d672fa9f":"#Just a check for gpu availability\nprint(\"GPU Details: \",torch.cuda.get_device_properties(0)) if torch.cuda.is_available() else print(\"No GPU\")","641fc9b6":"# creating the tensors with 3 random values\n\nx1 = torch.randn(3)   # sets requires_grad = False by default\nprint(x1)\n\nx2 = torch.randn(3, requires_grad = True)\nprint(x2)","06e3c50c":"# Forward Pass 1\ny1 = x1 + 2\nprint(y1)  # simple output\n\ny2 = x2 + 2\nprint(y2)  # pytorch adds the gradient function to this since requires_grad was true","2fb7f377":"# Forward Pass 2\nz1 = y1 * y1 * 2\nz1 = z1.mean()\nprint(z1)\n\nz2 = y2 * y2 * 2\nz2 = z2.mean()\nprint(z2)","b618f7ee":"# Backward Pass \n\ntry:\n    z1.backward()   # this wiil throw an error since this operations are done on just a tensor with requires_grad = False\nexcept:\n    print(\"RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\")\n    print(\"So requires_grad must be set to True for back propogations\")\n    print(\"so for now ignore the tensor x1 y1 & z1 \\n\")\n\nz = z2\nz.backward()  # dz\/dx\nprint(\"Printing all Grads of tensor x2: \",x2.grad)","e73b0ed5":"# we can also use backward() directly on y \nz2 = y2.sum().backward()\nprint(x2.grad)\nprint(\"backward() function does not work with multiple values it only needs a scalar value such as mean or sum of all etc\")\nprint(\"\\nfor example: \")\ntry:\n    print(y2)\n    y2.backward()\nexcept:\n    print(\"RuntimeError: grad can be implicitly created only for scalar outputs\")\n\nprint(\"\\nfeel free to play with this!!!\")","9f69e0bd":"# Vector-Jacobian product calculation\n\nz2 = y2 * y2 * 2\nprint(\"Non Scalar outputs: \",z2)\n\nv = torch.tensor([0.1, 0.01, 0.001], dtype = torch.float32)\n\nz2.backward(v)\nprint(x2.grad)","cd0ea3cc":"# 1. set requires_grad to False\nx3 = torch.randn(3, requires_grad = True)   #we can also set it to False here\nprint(x3)\nx3.requires_grad_(False)\nprint(x3)","1ab6c739":"# 2. tensor.detach()\nx3 = torch.randn(3, requires_grad = True)   #we can also set it to False here\nprint(x3)\ny3 = x3.detach()  # since detach() function creates a new tensor with same values but without grads\nprint(y3)","3d67888c":"# 3. with torch.no_grad():\n\nx3 = torch.randn(3, requires_grad = True)   #we can also set it to False here\nprint(x3)\n\ny3 = x3 + 2\nprint(\"Outside with: \",y3)\n\nwith torch.no_grad():\n    y3 = x3 + 2\n    print(\"Inside: \",y3)","9f598d0d":"weights = torch.ones(4, requires_grad = True)\nprint(\"Whenever we call the backward function then gradients for this tensor will be accumilated into .grad attribute and the values will be sumed up. \\n\")\nfor epoch in range(3):\n    model_output = (weights * 3).sum()\n    model_output.backward()\n    print(weights.grad)\n\nprint(\"\\nzero_grad() restarts looping without losses from the last step if you use the gradient method for decreasing the error (or losses). \\n\")\nweights = torch.ones(4, requires_grad = True)\nfor epoch in range(3):\n    model_output = (weights * 3).sum()\n    model_output.backward()\n    print(weights.grad)\n    weights.grad.zero_()","0fa985f5":"from torch import nn, optim\nimport torchvision\n\nmodel = torchvision.models.resnet18(pretrained = True)\n\n# Freeze all the parameters in the network\nfor param in model.parameters():\n    param.requires_grad = False","e1deff7c":"print(model.fc)\nmodel.fc = nn.Linear(512, 10)\nprint(model.fc)","42253868":"# Optimize only the classifier\noptimizer = optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9)","e464ed2f":"print(optimizer)","d2a21b24":"In case to make such scenario work: \n\n# Vector-Jacobian product calculation\n\nMathematically, if you have a vector valued function y\u20d7 =f(x\u20d7 ), then the gradient of y\u20d7  with respect to x\u20d7  is a Jacobian matrix J:\n![image.png](attachment:image.png)\n\nGenerally speaking, torch.autograd is an engine for computing vector-Jacobian product. That is, given any vector v\u20d7 , compute the product JT\u22c5v\u20d7 \n\nIf v happens to be the gradient of a scalar function,\nthen by the chain rule, the vector-Jacobian product would be the gradient of l with respect to x\u20d7 :\n![image.png](attachment:image.png)\n\nThis characteristic of vector-Jacobian product is what we use in the above example; external_grad represents v\u20d7 .\n","6ee1dd0f":"## Stop Pytorch from creating the grad fn and tracking the history of our computation\n> 1. set requires_grad to False\n> 2. tensor.detach()\n> 3. with torch.no_grad():","843a112c":"# tensor.grad.zero_()\n\nIn PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes. This is convenient while training RNNs. So, the default action is to accumulate (i.e. sum) the gradients on every loss.backward() call.\n\nBecause of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly. Else the gradient would point in some other direction than the intended direction towards the minimum (or maximum, in case of maximization objectives).","d4e85e94":"# Chain Rule\n![chain%20rule.png](attachment:chain%20rule.png)","c7451cee":"Notice although we register all the parameters in the optimizer, the only parameters that are computing gradients (and hence updated in gradient descent) are the weights and bias of the classifier.","cbe3b824":"## Tensors ##\nA torch.Tensor is a multi-dimensional matrix containing elements of a single data type with capability to run on GPU unlike numpy.\nA tensor can be created with requires_grad=True so that torch.autograd records operations on them for automatic differentiation.","4f50f3d7":"# Forward and Backward Pass for Propogations\n\n![](https:\/\/miro.medium.com\/max\/3040\/1*q1M7LGiDTirwU-4LcFq7_Q.png)\n\nThe forward pass on the left calculates z as a function f(x,y) using the input variables x and y. The right side of the figures shows the backward pass. Receiving dL\/dz, the gradient of the loss function with respect to z from above, the gradients of x and y on the loss function can be calculate by applying the chain rule, as shown in the figure (borrowed from [this post](https:\/\/kratzert.github.io\/2016\/02\/12\/understanding-the-gradient-flow-through-the-batch-normalization-layer.html))","354d65d6":"Now all parameters in the model, except the parameters of model.fc, are frozen. The only parameters that compute gradients are the weights and bias of model.fc.","59f46fb8":"Let\u2019s say we want to finetune the model on a new dataset with 10 labels. In resnet, the classifier is the last linear layer model.fc. We can simply replace it with a new linear layer (unfrozen by default) that acts as our classifier.","290ee602":"# Important Notes for Gradients in NN\n\nIn a NN, parameters that don\u2019t compute gradients are usually called frozen parameters. It is useful to \u201cfreeze\u201d part of your model if you know in advance that you won\u2019t need the gradients of those parameters (this offers some performance benefits by reducing autograd computations).\n\nAnother common usecase where exclusion from the DAG is important is for finetuning a pretrained network\n\nIn finetuning, we freeze most of the model and typically only modify the classifier layers to make predictions on new labels. Let\u2019s walk through a small example to demonstrate this. As before, we load a pretrained resnet18 model, and freeze all the parameters.","7d62fa65":"# **Pytorch Autograd (torch.autograd)**\n\ntorch.autograd is PyTorch\u2019s automatic differentiation engine that powers neural network training."}}