{"cell_type":{"ee51ce3b":"code","f887034d":"code","0bea60cb":"code","19b6d85e":"code","aba43101":"code","b6eaba27":"code","c7d80af3":"code","ffa49bd0":"code","90f0a344":"code","63775e8e":"code","47a849ac":"code","c2d33ab5":"code","987327eb":"code","a47e5acd":"code","9649ddef":"code","83ed71d3":"code","67552ba6":"code","78781472":"code","7a071de7":"code","45ce2443":"markdown","6dce15bb":"markdown","fff71e34":"markdown","5cb04dc3":"markdown","934c0a4c":"markdown","52d9e16c":"markdown","ee2d7905":"markdown","fefe14fd":"markdown","4294e206":"markdown","25be80c8":"markdown","e3b0e94b":"markdown","58a9eb93":"markdown","ffc2252f":"markdown","5212cfc3":"markdown","8b5c0688":"markdown","135009b2":"markdown","c6e8d368":"markdown","521e8bf5":"markdown","8ed0561d":"markdown","22c39023":"markdown"},"source":{"ee51ce3b":"from keras.models import Model\nfrom keras_preprocessing.image import ImageDataGenerator\nfrom keras.layers import Dense, Dropout, BatchNormalization\nfrom keras.layers import Input, Conv2D, multiply, LocallyConnected2D, Lambda, Flatten, concatenate\nfrom keras.layers import GlobalAveragePooling2D, AveragePooling2D, MaxPooling2D\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom keras import optimizers\nfrom keras.metrics import mean_absolute_error\nfrom keras.applications import Xception\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns","f887034d":"# hyperparameters\nEPOCHS = 15\nLEARNING_RATE = 0.0006\nBATCH_SIZE_TRAIN = 32\nBATCH_SIZE_VAL = 256\n\n#image parameters \nPIXELS = 299 #default for Xception\nCHANNELS = 3\nIMG_SIZE = (PIXELS, PIXELS)\nIMG_DIMS = (PIXELS, PIXELS, CHANNELS)\nVALIDATION_FRACTION = 0.25\nSEED = 7834","0bea60cb":"path = '..\/input\/boneage-training-dataset\/boneage-training-dataset\/'\npath = '..\/input\/'\ntrain_path = path + 'boneage-training-dataset\/boneage-training-dataset\/'\ntest_path = path + 'boneage-test-dataset\/boneage-test-dataset\/'\n\ndf = pd.read_csv(path + 'boneage-training-dataset.csv')\nfiles = [train_path + str(i) + '.png' for i in df['id']]\ndf['file'] = files\ndf['exists'] = df['file'].map(os.path.exists)","19b6d85e":"fig, ax = plt.subplots()\nax = sns.distplot(df['boneage'], bins=10)\nax.set(xlabel='Boneage (months)', ylabel='Density',\n    title='Boneage distribution');","aba43101":"boneage_mean = df['boneage'].mean()\nboneage_div = 2 * df['boneage'].std()\ndf['boneage_zscore'] = df['boneage'].map(lambda x:\n    (x - boneage_mean) \/ boneage_div)\ndf.dropna(inplace=True)\n\ndf['gender'] = df['male'].map(lambda x: 1 if x else 0)\n\ndf['boneage_category'] = pd.cut(df['boneage'], 10)\nraw_train_df, raw_valid_df = train_test_split(df, test_size=VALIDATION_FRACTION,\n  random_state=2018, stratify=df['boneage_category'])\ntrain_df = raw_train_df.groupby(['boneage_category', 'male']).apply(\n  lambda x: x.sample(500, replace=True)).reset_index(drop=True)\nvalid_df, test_df = train_test_split(raw_valid_df,\n  test_size=VALIDATION_FRACTION, random_state=2019)","b6eaba27":"fig, ax = plt.subplots()\nax = sns.distplot(train_df['boneage'], bins=10)\nax.set(xlabel='Boneage (months)', ylabel='Density',\n    title='Boneage training distribution');","c7d80af3":"optim = optimizers.Nadam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.0003)","ffa49bd0":"weight_path = \"{}_weights.best.hdf5\".format('bone_age3')\n\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1,\n    save_best_only=True, mode='min', save_weights_only=True)\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8,\n    patience=10, verbose=1, mode='auto', min_delta=0.0001, cooldown=5,\n    min_lr=0.00006)\nearly = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=8)\ncallbacks_list = [checkpoint, early, reduceLROnPlat]","90f0a344":"BATCH_SIZE_TEST = len(test_df) \/\/ 3\nSTEP_SIZE_TEST = 3\nSTEP_SIZE_TRAIN = len(train_df) \/\/ BATCH_SIZE_TRAIN\nSTEP_SIZE_VALID = len(valid_df) \/\/ BATCH_SIZE_VAL","63775e8e":"def gen_2inputs(imgDatGen, df, batch_size, seed, img_size):\n    gen_img = imgDatGen.flow_from_dataframe(dataframe=df,\n        x_col='file', y_col='boneage_zscore',\n        batch_size=batch_size, seed=seed, shuffle=True, class_mode='other',\n        target_size=img_size, color_mode='rgb',\n        drop_duplicates=False)\n    \n    gen_gender = imgDatGen.flow_from_dataframe(dataframe=df,\n        x_col='file', y_col='gender',\n        batch_size=batch_size, seed=seed, shuffle=True, class_mode='other',\n        target_size=img_size, color_mode='rgb',\n        drop_duplicates=False)\n    \n    while True:\n        X1i = gen_img.next()\n        X2i = gen_gender.next()\n        yield [X1i[0], X2i[1]], X1i[1]","47a849ac":"def test_gen_2inputs(imgDatGen, df, batch_size, img_size):\n    gen_img = imgDatGen.flow_from_dataframe(dataframe=df,\n        x_col='file', y_col='boneage_zscore',\n        batch_size=batch_size, shuffle=False, class_mode='other',\n        target_size=img_size, color_mode='rgb',\n        drop_duplicates=False)\n    \n    gen_gender = imgDatGen.flow_from_dataframe(dataframe=df,\n        x_col='file', y_col='gender',\n        batch_size=batch_size, shuffle=False, class_mode='other',\n        target_size=img_size, color_mode='rgb',\n        drop_duplicates=False)\n    \n    while True:\n        X1i = gen_img.next()\n        X2i = gen_gender.next()\n        yield [X1i[0], X2i[1]], X1i[1]","c2d33ab5":"train_idg = ImageDataGenerator(zoom_range=0.2,\n                               fill_mode='nearest',\n                               rotation_range=25,  \n                               width_shift_range=0.25,  \n                               height_shift_range=0.25,  \n                               vertical_flip=False, \n                               horizontal_flip=True,\n                               shear_range = 0.2,\n                               samplewise_center=False, \n                               samplewise_std_normalization=False)\n\nval_idg = ImageDataGenerator(width_shift_range=0.25, height_shift_range=0.25, horizontal_flip=True)\n\ntrain_flow = gen_2inputs(train_idg, train_df, BATCH_SIZE_TRAIN, SEED, IMG_SIZE)\n\nvalid_flow = gen_2inputs(val_idg, valid_df, BATCH_SIZE_VAL, SEED, IMG_SIZE)\n\ntest_idg = ImageDataGenerator()\n\ntest_flow = test_gen_2inputs(test_idg, test_df, 789, IMG_SIZE)","987327eb":"def mae_months(in_gt, in_pred):\n    return mean_absolute_error(boneage_div * in_gt, boneage_div * in_pred)","a47e5acd":"# Two inputs. One for gender and one for images\nin_layer_img = Input(shape=IMG_DIMS, name='input_img')\nin_layer_gender = Input(shape=(1,), name='input_gender')\n\n# Pretrained neural network\nbase = Xception(input_shape=IMG_DIMS, weights='imagenet', include_top=False)\n\npt_depth = base.get_output_shape_at(0)[-1]\npt_features = base(in_layer_img)\nbn_features = BatchNormalization()(pt_features)\n\n# Attention layer\nattn_layer = Conv2D(64, kernel_size=(1,1), padding='same', activation='relu')(bn_features)\nattn_layer = Conv2D(16, kernel_size=(1,1), padding='same', activation='relu')(attn_layer)\nattn_layer = LocallyConnected2D(1, kernel_size=(1,1), padding='valid',\n    activation = 'sigmoid')(attn_layer)\n\n# Applying attention to all features coming out of bn_features\nup_c2_w = np.ones((1, 1, 1, pt_depth))\nup_c2 = Conv2D(pt_depth, kernel_size=(1,1), padding='same',\n    activation='linear', use_bias=False, weights=[up_c2_w])\nup_c2.trainable = False\nattn_layer = up_c2(attn_layer)\n\nmask_features = multiply([attn_layer, bn_features])\n\n# Global Average Pooling 2D\ngap_features = GlobalAveragePooling2D()(mask_features)\ngap_mask = GlobalAveragePooling2D()(attn_layer)\ngap = Lambda(lambda x: x[0]\/x[1], name='RescaleGAP')([gap_features, gap_mask])\ngap_dr = Dropout(0.5)(gap)\ndr_steps = Dropout(0.25)(Dense(1024, activation = 'elu')(gap_dr))\n\n# This is where gender enters in the model\nfeature_gender = Dense(32, activation='relu')(in_layer_gender)\nfeature = concatenate([dr_steps, feature_gender], axis=1)\n\no = Dense(1000, activation='relu')(feature)\no = Dense(1000, activation='relu')(o)\no = Dense(1, activation='linear')(o)\n\nmodel = Model(inputs=[in_layer_img, in_layer_gender], outputs=o)\n\nmodel.compile(loss='mean_absolute_error', optimizer=optim, metrics=[mae_months])\n\nmodel.summary()","9649ddef":"model_history = model.fit_generator(generator=train_flow,\n    steps_per_epoch=STEP_SIZE_TRAIN, validation_data=valid_flow,\n    validation_steps=STEP_SIZE_VALID, epochs=EPOCHS,\n    callbacks = callbacks_list)","83ed71d3":"loss_history = model_history.history['loss']\nfrom keras.utils import plot_model\nplot_model(model, to_file='model.png')\nhistory_df = pd.DataFrame.from_dict(model_history.history)\nhistory_df.to_csv('loss_history.csv')","67552ba6":"test_X, test_Y = next(test_flow)","78781472":"plt.style.use(\"dark_background\")\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.sans-serif'] = 'DejaVu Sans'\npred_Y = boneage_div*model.predict(test_X, batch_size = 263, verbose = True)+boneage_mean\ntest_Y_months = boneage_div*test_Y+boneage_mean\n\nord_idx = np.argsort(test_Y)\nord_idx = ord_idx[np.linspace(0, len(ord_idx)-1, 4).astype(int)] # take 8 evenly spaced ones\nfig, m_axs = plt.subplots(2, 2, figsize = (8, 8))\nfor (idx, c_ax) in zip(ord_idx, m_axs.flatten()):\n    cur_img = test_X[0][idx:(idx+1)]\n    c_ax.imshow(cur_img[0, :,:,0], cmap = 'bone')\n    \n    c_ax.set_title('Age: %2.1fY\\nPredicted Age: %2.1fY' % (test_Y_months[idx]\/12.0, \n                                                           pred_Y[idx]\/12.0))\n    c_ax.axis('off')\nfig.savefig('trained_img_predictions.png', dpi = 300)","7a071de7":"from sklearn.metrics import mean_absolute_error as mean_abs\ntest_error = mean_abs(test_Y, pred_Y)\ntest_error = boneage_div * test_error + boneage_mean\ntest_error = str(test_error)\n\nOutfile=open('test_error.txt','w')","45ce2443":"We convert the bone age to a z-score.\nWe also convert the male column (True \/ False) to integers 1 and 0.\n\nThe bone ages are cut into 10 bins (categories), and the full set is then split between a \"raw\" training set and a \"raw\" validation set, to be further processed afterwards. The split is stratified according to the bone age category, to ensure that we get entries from each category in the sets.\n\nThe raw training set is then converted to the final training set by randomly sampling 500 entries according to the bone age category, with replacement. This means that some images will appear more than once, but as we apply augmentations to the images (ie. we rotate, flip, etc.) this should have no impact.\n\nLastly, the raw validation set is split into a final validation set and a test set.\n\nThis process is from https:\/\/www.kaggle.com\/kmader\/attention-on-pretrained-vgg16-for-bone-age.","6dce15bb":"We want to be able to keep an eye on the loss while training. A sensible metric is the mean absolute error, but we need to convert it from a z-score to months, which is the job of the following function.","fff71e34":"The optimizer used during training.","5cb04dc3":"We save the model as a png file, and the loss history as a csv file. I'm not sure what the best way is to download it after the notebook has run; I personally commit the code, and the use the Kaggle API (see https:\/\/github.com\/Kaggle\/kaggle-api), which offers the `$ kaggle kernels output` command, that fetches the files from the latest commit.","934c0a4c":"As callbacks we save the weights, create checkpoints, reduce the learning rate if the loss plateaues, and (of course!) early stopping.","52d9e16c":"Plotting four images with test result.","ee2d7905":"For the test images, we do not want shuffling, as we need to be able to compare with the test dataframe.","fefe14fd":"The files are read in programmaticallly, with the ID given in the csv file used as pointer.\nWe do a check to see if the file is actually available in the folder; if not, it is removed.","4294e206":"The test set is retrieved.","25be80c8":"Saving test error.","e3b0e94b":"The step sizes are used for Keras' fit generator.","58a9eb93":"We process images with the `ImageDataGenerator`. With the `ImageDataGenerator` we randomnly rotate, flip, shift, and zoom the images. This is a way to make sure every picture is different but in a random way. \n\nNext we define generators for both the training and validation set. The generator feeds the CNN model with the images in batches as we can't handle all the images at once. Our generator has to give two outputs: the images and the gender respectively for each image, in order for us to train on that feature as well, and therefore these use the generators previously defined.","ffc2252f":"The new plot of the training set shows the distribution to be uniform over 10 bins, as expected.","5212cfc3":"The hyperparameters are set with gef\u00fchl, because searching a grid for optimal values is not feasable when the training takes around six hours.\n\nThe image size is set to `299x299`, as that is what the pretrained network we use, Xception, expects.\nAlso, even though the images are grayscale, we use three channels (each just a copy of the grayscale values), again because Xception demands it to be so.\n\nThe validation fraction is 0.25; this fraction is again used when splitting the validation set into validation and testing. We unfortunately need to split the validation set, as we do not have any test set available with truth values.\n\nNote that we run for 15 epochs, as that is about the range of computing time allowed.","8b5c0688":"Now we fit the model, feeding it both the generators, the constants and the callback list.","135009b2":"The optimizer used during training.","c6e8d368":"Because we want to use the gender information as input to the neural network, we need the generator to take in two inputs; the image data and the gender information. This functionality is not given out of the box by Keras, and so the following wrapper ensures that both inputs are used, and handed over to the generator in the correct format.\n\nNote that we use the same seed for both image and gender, to ensure that the generator fetches the same file for both.\n\nThis process is from https:\/\/www.kaggle.com\/sinkie\/keras-data-augmentation-with-multiple-inputs.","521e8bf5":"The following code is glued together with input from several sources, mainly Kevin Mader's excellent \"Attention on Pretrained-VGG16 for Bone Age\" (https:\/\/www.kaggle.com\/kmader\/attention-on-pretrained-vgg16-for-bone-age) and sinkie's \"Keras data augmentation with multiple inputs\" (https:\/\/www.kaggle.com\/sinkie\/keras-data-augmentation-with-multiple-inputs).\n\nThe code also includes the original work of Alisa Agafonova, Jon Raunkj\u00e6r S\u00f8ndergaard,\nYifan Liu Vestergaard & Mads Ehrhorn Kj\u00e6r, masters students at NBI 2019, prepared for the course Big Data Analysis.","8ed0561d":"The distribution of the bone age is plotted, and we can see that it is imbalanced. This is corrected in the next cell.","22c39023":"Creating the CNN: \nAs is seen we have two inputs, one for images and one for gender. The images is run through a pretrained neural network Xception, attention is applied, and this is then fed to a layer that merges the information with the gender info.\n\nAfterwards, this combined information is fed through some dense nodes, which finally gives the output.\n\nThe user may uncomment `model.summary()` to see the network layout; this is also saved as a png file later in the notebook."}}