{"cell_type":{"957d1f93":"code","d2b153f5":"code","c88868c8":"code","a413c76d":"code","c36974eb":"code","93c5ccbc":"code","15bffe3f":"code","b4456050":"code","240e401b":"code","76a9eae9":"code","182d7709":"code","bf8bc09c":"code","c4008477":"code","cbd76b06":"markdown","d982472b":"markdown","e764f5d4":"markdown","48247841":"markdown","610e6b21":"markdown","b1f291f7":"markdown","6f224298":"markdown","4c37c357":"markdown","7ed3bd62":"markdown","cbd30317":"markdown","2ea654fd":"markdown","cefa8689":"markdown","d51a4b03":"markdown"},"source":{"957d1f93":"import pandas as pd\ntest_data = pd.read_csv ('\/kaggle\/input\/titanic\/test.csv')\ntrain_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')","d2b153f5":"from sklearn.tree import DecisionTreeClassifier\n\n##Fill in missing values, convert features to numbers where applicable\nfor df in [train_data,test_data]:\n    df['Sex_boolean']=df['Sex'].map({'male':1,'female':0})\n    df['Fare'].fillna(train_data['Fare'].mean(), inplace=True)\n    df['Age'].fillna(train_data['Age'].mean(), inplace=True)\n\n##Generate model and predict\nmodel=DecisionTreeClassifier().fit(train_data[['Pclass','Sex_boolean']],train_data['Survived'])\npredictions=model.predict(test_data[['Pclass', 'Sex_boolean']])\n\n##Append the predictions to the PassengerID and convert to CSV\npd.DataFrame({'PassengerId':test_data['PassengerId'],'Survived':predictions}).to_csv('KaggleOutput', index=False)\n\n##A detailed explanation of the above code can be found at:\n##https:\/\/www.kaggle.com\/allohvk\/titanic-simplest-tutorial-ever-code-as-a-story","c88868c8":"from sklearn import tree\nimport matplotlib.pyplot as plt\n\n##Plot the decision tree\nplt.figure(figsize=(40,20))  \n_ = tree.plot_tree(model, feature_names = ['Pclass', 'Sex_boolean'], filled=True, fontsize=30, rounded = True)","a413c76d":"##Meanwhile you can even print the final rules that go into taking a decision\n##This is a new feature of Scikit and shows exactly how the machines 'make the\n##program from the data'\nfrom sklearn.tree.export import export_text\n\ntree_rules = export_text(model, feature_names=['Pclass', 'Sex_boolean'])\nprint(tree_rules)","c36974eb":"from sklearn.model_selection import train_test_split\n\n#Split data into training features and labels\nX, y = train_data[['Pclass', 'Sex_boolean', 'Age', 'SibSp', 'Parch', 'Fare']], train_data['Survived']\n\nX_train, X_validate, y_train, y_validate = train_test_split(X, y, stratify=y, test_size = 0.2, random_state = 200)","93c5ccbc":"model=DecisionTreeClassifier(max_depth=4).fit(train_data[['Pclass', 'Sex_boolean', 'Age', 'SibSp', 'Parch', 'Fare']],train_data['Survived'])\n\nplt.figure(figsize=(40,20))  \n_ = tree.plot_tree(model, feature_names = ['Pclass', 'Sex_boolean', 'Age', 'SibSp', 'Parch', 'Fare'], filled=True, fontsize=24, rounded = True)\n","15bffe3f":"display(model.get_params())","b4456050":"from sklearn.metrics import roc_curve, auc\nfrom matplotlib.legend_handler import HandlerLine2D\n\n\n##Let us compare difference in results for depth=3 and depth=12\nfor max_depth in [3,12]:\n\n    ##Train on training data. We call the model dt henceforth\n    dt = DecisionTreeClassifier(max_depth=max_depth).fit(X_train, y_train)\n\n    ##Predict on training data\n    train_prediction = dt.predict(X_train)\n\n    ##Get the results - false +ve and true +ves. We will discuss this in a minute\n    false_positive_rate,true_positive_rate,thresholds=roc_curve(y_train,train_prediction)\n\n    ##Generate the roc. We will discuss this also\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    print(roc_auc)\n\n    ##Plot the ROC in a graph\n    plt.plot(false_positive_rate, true_positive_rate)\n    plt.axis([0,1,0,1])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.show()","240e401b":"for max_depth in [3,12]:\n\n    ##Train on training data\n    dt = DecisionTreeClassifier(max_depth=max_depth).fit(X_train, y_train)\n\n    ##Predict on validation data\n    validate_prediction = dt.predict(X_validate)\n\n    ##Get the results - false +ve and true +ves\n    false_positive_rate,true_positive_rate,thresholds=roc_curve(y_validate,validate_prediction)\n\n    ##Generate the roc\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    print(roc_auc)\n\n    ##Plot the ROC in a graph\n    plt.plot(false_positive_rate, true_positive_rate)\n    plt.axis([0,1,0,1])\n    plt.xlabel('False Positive Rate - Validation')\n    plt.ylabel('True Positive Rate - Validation')\n    plt.show()","76a9eae9":"train_results = []\nvalidate_results = []\n\nfor max_depth in range(1,20):\n    dt = DecisionTreeClassifier(max_depth=max_depth).fit(X_train, y_train)\n    \n    train_prediction = dt.predict(X_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_prediction)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    train_results.append(roc_auc)\n    print('For Depth = ', max_depth, ' for training data, the AUC = ', roc_auc)\n    \n    validate_prediction = dt.predict(X_validate)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_validate, validate_prediction)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    validate_results.append(roc_auc)\n    print('For Depth = ', max_depth, ' for validation data, the AUC = ', roc_auc, '\\n')\n\n\n\nline1 = plt.plot(range(1,20), train_results, label=\"Training data AUC\")\nline2 = plt.plot(range(1,20), validate_results, label=\"Validation data AUC\")\nplt.show()","182d7709":"from sklearn.model_selection import GridSearchCV \nimport numpy as np\n\nparams = {'max_depth': range(1,15),\n          'min_samples_leaf': np.arange(1,25,2), \n          'max_features': range(1,6),\n          'criterion' : ['gini', 'entropy'],\n          'splitter' : ['random', 'best'],\n          'random_state' : [1] }\n\ngrid_dt = GridSearchCV(estimator=dt, param_grid=params, scoring='accuracy', cv=8, n_jobs=-1)\ngrid_dt.fit(X, y)\n##Note above we are using the full trainig data. We dont need to split.\n##Gridsearch automatically splits this into 8 sets, trains on 7 and tests on the 8th \n##for each combination of hyperparameters\n\nprint('Best hyerparameters:\\n', grid_dt.best_params_)\nprint('Best CV roc aus', grid_dt.best_score_)\n##print('Score on validation data', grid_dt.best_estimator_.score(X_validate,y_validate))","bf8bc09c":"imp_features = pd.DataFrame({'feature':X_train.columns,'importance':np.round(grid_dt.best_estimator_.feature_importances_,2)})\nprint(imp_features.sort_values('importance',ascending=False))\n\n##You can even display the individual scores. I commented this because it is a lengthy o\/p\n##for mean_score, params in zip(grid_dt.cv_results_['mean_test_score'], grid_dt.cv_results_['params']):\n##display(mean_score, params)","c4008477":"predictions=grid_dt.best_estimator_.predict(test_data[['Pclass', 'Sex_boolean', 'Age', 'SibSp', 'Parch', 'Fare']])\n\npd.DataFrame({'PassengerId':test_data['PassengerId'],'Survived':predictions}).to_csv('KaggleOutput', index=False)","cbd76b06":"I had the recent previlige of being in the audience of a small group of people interacting with a retired Silicon valley specialist who had once worked with Steve Jobs. The topic was 'Explainable AI' and how there was a need to understand what is going on behind the scenes. The specialist lamented that better insight is needed into the decision making process of ML and in some cases this could be a legal or compliance requirement as well. Who takes the bottomline in case something goes seriously wrong? While explainable AI is an evloving concept and much progress is expected in coming years, but as far as rank beginners are concerned, thankfully there is at least one model which gives a nice visual display of what is going on inside it - the decision trees.\n\nWhen Arthur Samuel defined ML in 1959 as as the \u201ca field of study that gives computers the ability to learn without being explicitly programmed\u201d, little did he know that a few decades later, it takes just 1 line of code to understand exactly what he meant.\n\nWhat we will do here is:\n- Understands how decision trees are built \n- Understand how they take a decision\n- Learn how to tune the hyperparameters manually.. Understand a bit of ROC AUC in the process\n- Learn auto-tuning","d982472b":"X which is the 'feature' list is set to train_data.loc[:, train_data.columns != 'Survived']. Basically all columns excluding 'Survived'. \n\ny which is the 'label' is set to train_data['Survived'].\n\nNext we call the train_test_split funciton of Scikit. Two points to be noted here: (a) Use the stratify option. This splits the data wisely based on the ratios in the original dataset. (b) Use a random seed so that in subsequent re-runs, you can get similar split.\n\nNow we will 'tune' on the X_train dataframe and will 'validate' on the X_validation dataframe. The most important parameter to tune would be the 'depth' of the tree. How many levels should it go? In the last DT, the depth was 3 though we didnt specify anything. Based on the 2 features we provided as input, the tree went to 3 levels. This time we will give all the features to the model (instead of just Sex and Pclass) and keep max depth as 4 and see what it does.","e764f5d4":"Did you notice some interesting things right away? Now that we gave the entire list of features to the model, it chose Sex as the top and most important parameter linked to Survival. But see what happens next. For a female, the next level is Pclass but for a male, looks like Age has a major say esp for children. Is this model better or worse that the prev model? Difficult to say until we compare the data.\n\nWhat is the optimum set of features to provide? This is goverened by a hyperparameter called max_features. What is the optimum depth to provide? This is of course goverened by max_depth. Also notice certain things. The Fare<88.775 box on the extreme left. It has just 2 samples. It then gives way to 2 leaf nodes each having 1 sample. In general it is not too good to go into a very minute level of breakdown else there is risk of overfitting. So another very good hyper-parameter to adjust is 'min-sample-split'. It should have some decent value - 10, 20, 30 whatever.. definitely not 2 right? Related to this concept is a bunch of hyperparameters - min-samples-leaf...definitely should not be 1 like in the above DT. We also have min-weight-fraction-leaf which is same as min-samples-leaf but expressed as a fraction. We also have something called the max_leaf_nodes and the name suggests what it limits. Last but not the least we have the error measurement parameter. Instead of 'gini' we can use 'entropy' which is another way to measure error. Most of the time they give similar results. So it may not matter match but you may want to experiment. We also have another interesting hyperparameter - random splitter or best splitter. This decides on how to select the next feature to split on. While it may seem intuitive that best splitter gives the best results, sometimes randomness can be a virtue. There are a couple more hyperparameters but these are the main ones.\n\nLet us try getting the entire list of hyperparameters","48247841":"Let us now try tuning some hyperparamers. We dont want to keep using the test data again and again else it loses it independence. So we will split the training data itself into training and validation data. ","610e6b21":"Couple of quick points. \n\nTry to use Python, np features like: np.arange(1,25,2). This feeds the min-samples-leaf a range from 1 to 25 by adding 2 each time. This results in clean elegant and readable code\n\nThe GridsearchCV takes in a couple of parameters. cv is the count of the number of cross folds. We have given the value as 8. The Gridsearch breaks the data into 8 pieces and for each hyperparameter combination, the model is trained 7 times and tested on the 8th one. This reduces overfitting. We also have the n_jobs parameter which indicates the amount of parallel processing that happens. '-1' means utilize as many cores as possible. There are diff scoring mechanisms. We can choose roc_auc like before or accuracy. I chose accuracy because it gave a slightly higher score.\n\nThis job could take a few mins. You see a 'star' in the Jupyter screen In [*] indicating that something is running. Be patient. \n\nFinally you get a list of fine tuned hyper-parameters. There are some interesting values. For e.g. it shows a larger depth this time. Rest of the hyperparameters are on expected lines. We get 83% on the train dataset. Let us see how this model works on the test dataset. But before that we can check the important features that the model is using.","b1f291f7":"There are 13 hyperparameters and above are the default values (other than max depth which we had explicitly set as 12). We discussed almost all the important ones. \n\nLet us start manually tuning the a couple of this. Let us start with max_depth.","6f224298":"Aah. Did you see something interesting? The model where depth=3 is actually performing slightly better than the model with depth=12 on the 'validation' data. That means that increasing the depth beyond 3-4 is not helping. It is worsening the score. It fitted the training data perfectly (overfitted as a matter of fact) but when tested against validation data it gave a lower score. It will definitely give a lower score against the test data also. \n\nIn fact let us go ahead and test for the whole range of depth values from 1 to 20","4c37c357":"There you go! You can see that the optimized depth is somewhere around 3 (given the current set of features..If you add more features like 'Title' or do a better feature engg e.g. converting fare into fare per person, this may change). So for the above case, we can freeze the depth to around 3. At this depth, we get max AUC for validation data and max AUC means this is as perfect a model that you can get based on the feature engg you have done.\n\nYou can use this same technique to fine tune various hyperparameters that we discussed above and find out the optimal value of each. If course there could be interdepencies and so there is a fine-tuning that must be done considering all inputs. If you have ever manually tuned a amplifier and speakers you may know what I am talking of. You could even write a small program that takes in a range of values for a range of hyperparameters and generates AUC for all. But Sci-kit has done one better, they already have an API for that. Not only does the API fine tune the entire hyperparameter range for entire set of possible values, but it tests the validation on different (random) sets of validation data each time to avoid overfitting. This is the GridSearch cross validation API. You just tell this API what all hyperparameters you want to experiment with and what values to try out. It breaks the training data into much smaller sets or 'folds' and trains and validates using these folds and spits out the best combination of hyperparameter values.\n\nThere is also a randomized Search as an alternative to Gridsearch where the hyperparameter search space is large. For the above simple Titanic example we will stick to Gridsearch and see how much we can tune.","7ed3bd62":"What we have done here is to compare the results of two models. One model with depth of only 3 and another with depth of 12. We first train the model. Then we predict the results on the training data. Let us ignore the validation and test data for now. We will come to that later. Let us first see how the model fits the training data instances. The robustness of the model can be measured by many means and one such main one is the ROC curve. Very simply it gives a measure of how correct the model is **AND** how wrong the model is in one neat graph. The correctness of the model is the count of true positives (model says survived and actually person survives) **AND** the false positives (model says survived but actually the person expired). A perfect model would have all True positives and 0 false positives. In that case the graph would be a straight vertical line. \n\nIn real life, we know things are not perfect. So we maybe happy with mild imperfections. So we may be OK with some minimal False positives (called FPR - False Positive rate henceforth) as long as the True Positives (TPR) are always correct. In other words we want a model that gives a HIGH TPR and a low FPR. \n\nLet us now look at the graphs above. With a depth =3, we see that upto somewhere around TPR=0 to 70%, the FPR stays at reasonably low levels of 0 to 10%. If we want to increase TPR beyind that, it comes at a cost - the FPR begins to rise. We can get a 100% TPR by just saying all folks survive, but this will also give a 100% FPR. So the optimal value of TPR to FPR ratio is somewhere around 70%, but based on the circumstance we may be want a more stringent FPR or a more lenient FPR. This depends on the problem. An ideal graph would be a straight vertical line where we have 100% TPR and 0% FPR. Now let us leave this model and all its troubles and complexities aside for a moment. \n\nLet us say, we want to compare this model to another model, then what would be the simplest way? One very simple way would be to compute the 'area of the region under the blue line in the square'. In a perfect case where TPR=100% and FPR=0%, this is the area of the square itself. A perfectly imperfect model (which will be as difficult to build) will give perfectly horizontal line with 0% TPR and a 100% FPR and the area of the graph would be 0. A perfectly random model (which will also be as difficult to build as a perfectly perfect model or a perfectly imperfect model) would be a diaganol line starting from bottomleft and going to top right of the square. Its area would be the area of the triangle - 0.5 * base * height square which is =0.5. A perfect model would have an area (and we will start referring to this area as AUC  - Area Under Blue Line or more precisely Area Under the Curve) of 1. SO A REAL MODEL WOULD have AUC somewhere between 0.5 to 1 and we can tell if a model is good or bad by comparing the AUC's. The one with the greater AUC is a good model. This is a decent metric of comparision.\n\nWith this in mind, let us compare the AUC's above of a model where depth=3 and one where depth=12. You need not even calculate the AUC. You can just see and guess depth=12 outperforms the depth=3 model by a great deal. In fact the second model gives a score of almost 90% with very little FPR of less than 5% and now you feel you can break Chris Deotte's record comfortably and you rub your hands in glee and get ready for the submission. But there is a small step you need to do before that and that is to test the model on the validation data that you have kept aside for precisely this purpose.\n\nLet us now use both the models on the validation data.","cbd30317":"Up Next:\n- Creating a small ensemble of models in minimal lines of code\n- Bagging, Boosting, Stacking - An intuitive explanation\n- XGBoosting - An easy approach\n\n- Advanced data exploration and visualization - https:\/\/www.kaggle.com\/allohvk\/captivating-conversations-with-the-titanic-dataset\n- Advanced Strategy to fill missing ages - https:\/\/www.kaggle.com\/allohvk\/titanic-missing-age-imputation-tutorial-advanced\n- Advanced feature engineering\n- Final ensemble\n\n","2ea654fd":"As you can see, this is on expected lines. Now let us try to make a submission ","cefa8689":"OOps it gives a miserable 73.2%. This is bad. Again the model has badly overfit. It is predicting high scores on training data and unnecessarily misguiding us that we are going to be champions and top scorers. What could be the reason here? It may be possible that the 'test data' and the 'train data' of Titanic are not split correctly and any model will perform as badly? A more likely explanation is the model we have chosen is not the best fit for this particular dataset. We can still give one last try though and we can 'regularize' or 'constrain' this model to reduce overfitting. We saw earlier during manual tuning that depth does not give much benefit beyond 3-4. So that is an option we could pursue. Another option is to reduce the number of features. If you reduce the depth range from 1 to 5 in the gridsearch you get an improvement in score back to around 76%. But in general, the model itself seems to be not completely appropriate for the dataset.\n\nDecision trees are sensitive to variations in training data. Small variations in input can cause a different tree to get created altogether..it is so sensitive. So one way to eliminate this sensitivity is to grow a lot of trees and convert it into a forest. We also bring an element of randomness and as we discussed, randomness can be a virtue in modelling. And so we must turn to random forests which will hopefully give us better and more stable results. \n\nPlease upvote if you like this style of narration. The code is usually done in 10% of the time it takes to document. So let me know if the extra effort in documentation is helping anybody ","d51a4b03":"Let us see the decision tree image to understand how the machine made up the rules. Ignore the gini business for now. Let us look at the rest of the data. For now let us not worry about HOW the model is built. We will come to that later. Let us assume that the model building is done and look at what happemns when a new observation comes in...say a 'male' and he belongs to 3rd class. Let us see how the model now decides whether he survives or not survives. \n\nStart from the top node. There are 891 samples reaching this decision point (basically the entire training set). The 'value' is nothing but the label. 549 expired and 342 survived out of these 891. Now let us see the condition itself - Is Sex_boolen<=0.5? No it is male and sex=1. True is left and False is right so we move to the below level to the node to the right. \n\nIn next node, we have 577 samples which corresponds to the 577 males in the dataset. Out of this 468 expired and 109 survived. The question at this decision point is: Is Pclass<=1.5. Since our man's Pclass is =3 so it moves below to the node on the right side. There it checks for Pclass<=2.5  and then moves to the last level on the right. There are 347 samples here which means in the training data set there are 347 rows which contain Pclass=3 and Sex=Male. Now check the value column. It shows [300, 47]. This means out of 347 people, 300 expired and 47 survive. This is a leaf node. There is nowhere else to go to. So the model has to give a decision now. What decision do you think it will give? Obviously at this node 300 expire and 47 survive out of 347. Naturally the model or any rational being would assume that if somebody reaches this node it is more likely that he will expire. So the answer returned by model is 'Expire'. But you do know that there are 47 people who survive. They will be predicted WRONGLY by the model. There is nothing you can do about it..other than improving the model or trying a new model. In fact this gini business is all about giving an idea of 'by how much the model could be wrong' at that node. It is like an 'error' factor. There is a simple formula for it but we will not bother about that right now.\n\nNow do the same for a Pclass1 female. You will end up with SURVIVED. You end up on the left bottom most node (leaf). The samples there is 94. This means there are 94 females who travelled in Pclass 1. The value field shows that 3 expire and 91 survive. Naturally the node predicts 'Survival'. Look at the gini value here. As we discussed it is a simple measure of error. What do you think it should be - high or low? Obviously the possibility of error here is very very low. So the gini is .062 compared to the previous flow (Pclass3 Male) where gini was .234.\n\nThis explains the whole magical genie business. It is nothing but a measure of the error at that node. Another way to look at it is to say that it is a (inverse) measure of confidence of the model at that node. \n\nBy the way, Scikit uses a binary decision tree called CART (because it can be used for both Classification and Regression Trees). In other words, every level has only 2 decisions - Yes or No. if a sample satisfies the condition, then it goes to the LEFT branch (if exists), otherwise it goes RIGHT. There are other models available. \n\nFor any model to be good, the ginis have to be as low as possible. Let us quickly see how the model tries to keep the gini low and this will help us understand how the decision tree is built. When creating the first node, the model tries to analyse all features.  In this case we have only 2 features - Sex and Pclass. The model takes each feature individually, calculates the Gini for every possible split in that feature and then selects the feature\/split which leads to lowest Gini score. Between Sex and Pclass which do you think will have a better Gini score? If we split by Sex, and predict that females live and males die, we will not be widely off the mark right? The gini would be decent here. Now instead of Sex, let us consider Pclass. If we blindly say Pclass 1 survives and Pclass2,3 expires, we may not get a very high gini score. This is because there are fair number of females in Pclass 2,3 who survive. Even if we predict that Pclass 1,2 survive and all Pclass 3 expire, the gini is still lower than the earlier case (the one with Sex feature where we assume all males die and females live). So the model chooses the feature and the split that MAKES MOST SENSE and in this case it is Sex - m or f. This is the first level. \n\nNow the model moves on to the second level. Obviously there is only one feature left - Pclass. But what should be the binary split that will yield the best gini? Interestingly the answer is different in case of males and females. In case of females that split is Pclass3 versus Pclass 1,2. In case of males it is Pclass 1 versus Pclass2,3. This is a clever model because Pclass 2 females almost always survive (like Pclass 1 females) and Pclass 2 males have expiry rates similar to Pclass 3 males.\n\nThe way the tree is contructed is not too different from the way we construct our questions when we played the 10 question game as kids. The first question should be the one from which we glean the max information possible, then the next question and so on..\n\nAbove is a simple intuitive explanation of a decision tree and gives a feel of how an ML model generates code from data. You may not have the luxury of seeing this type of inner working for every model. But hopefully this gives you an idea of how *any* model works. There is an error function and the model works to minimize this error function. Another thing both man and machine try to minimize (esp in these times) is Cost. So this error minimization is more popularly called Cost minimization or 'Cost function' and there are several interesting ways to do it but that is a story for another day\n\nCART is a bit of a carefree (greedy?) model. It does not pay too much attention to the future. At each stage, it considers only the present situation and goes for the best fit. It does not worry about what happens down the line. For e.g. let us say at top level it chooses feature 'f1' because it is the best fit. It is possible however that as the tree is subsequently built, we come to know that this was not a good decision. The model should have gone with 'f2' which though was not a great choice at the top level (its gini would have been higher than f1 at that level), however would have resulted in a better tree and a better scoring model. The model is too lazy to introspect itself and correct itself at a later stage.\n\nOne beauty of DTs are that there is no feature 'scaling' needed. For other models, you may need to bring all feature values within similar ranges. For e.g. (say) we use the AGE feature and we use the Pclass feature. Pclass values vary from 1 to 3. Age varies from 0-100. The scale is vastly different and many models underperform or at least take more time with such different scales. There are various techniques to bring them into the same numerical range without losing any informaiton. Nothing of that sort for DTs. "}}