{"cell_type":{"c66bbb71":"code","6725d4aa":"code","0f7ba7eb":"code","eac9d2b6":"code","ec96d7a9":"code","2e63c55c":"code","3469cfac":"code","bac804f3":"code","310d1774":"code","7cc70def":"code","ae223e80":"code","aa62f5c2":"code","268232a5":"code","ae0494bf":"code","860add6f":"code","a583ebd2":"code","4c0a88a2":"code","7c016f65":"code","f2016d27":"code","d6555bfc":"code","5dcb462e":"code","eed4ecb9":"code","3673aaad":"code","c3eac22c":"code","c711aabf":"code","f534815d":"code","aabd6425":"code","d317d6ea":"code","21057207":"code","860dff1f":"code","58abca56":"code","50112136":"code","6b4c25b1":"code","bd43895c":"code","9e89258d":"code","da92abf1":"code","8827aeb3":"code","6f429be5":"code","87ad3a13":"code","1135dde0":"code","2d813797":"code","ebd908dc":"code","69b8e7cf":"code","231245e1":"code","ed5e1fd2":"code","78561335":"code","3c83ef6d":"code","131d6d8a":"code","0e1f15fe":"markdown","9ccfc952":"markdown","2fa5df53":"markdown","83cd57ab":"markdown","3df2418c":"markdown","4b945aff":"markdown","a1c73f0a":"markdown","0b9cefa3":"markdown","fbb38dd1":"markdown","32d33619":"markdown","182a5677":"markdown","37431128":"markdown","d5a7660d":"markdown","caf5649a":"markdown","44701dc1":"markdown","100a613f":"markdown","659fa67f":"markdown","dca0ba78":"markdown","9de7192d":"markdown","aafcd6a5":"markdown","8dd0f95f":"markdown","fcda4ac5":"markdown","a59954fe":"markdown","67fb8f3e":"markdown","30f648f0":"markdown","a1ea7214":"markdown","4a0b625f":"markdown","92208779":"markdown"},"source":{"c66bbb71":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6725d4aa":"import matplotlib.pylab as plt","0f7ba7eb":"from sklearn import metrics\nmetrics.homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0])","eac9d2b6":"print(\"%.3f\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2]))","ec96d7a9":"print(\"%.3f\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3]))","2e63c55c":"print(\"%.3f\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1]))\nprint(\"%.3f\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 0, 0, 0]))\nprint (metrics.completeness_score([0, 0, 1, 1], [1, 1, 0, 0]))\nprint(metrics.completeness_score([0, 0, 1, 1], [0, 0, 0, 0]))\nprint(metrics.completeness_score([0, 1, 2, 3], [0, 0, 1, 1]))\nprint(metrics.completeness_score([0, 0, 1, 1], [0, 1, 0, 1]))\nprint(metrics.completeness_score([0, 0, 0, 0], [0, 1, 2, 3]))","3469cfac":"print (metrics.v_measure_score([0, 0, 1, 1], [0, 0, 1, 1]))\nprint (metrics.v_measure_score([0, 0, 1, 1], [1, 1, 0, 0]))","bac804f3":"print(\"%.3f\" % metrics.completeness_score([0, 1, 2, 3], [0, 0, 0, 0]))\nprint(\"%.3f\" % metrics.homogeneity_score([0, 1, 2, 3], [0, 0, 0, 0]))\nprint(\"%.3f\" % metrics.v_measure_score([0, 1, 2, 3], [0, 0, 0, 0]))\nprint(\"%.3f\" % metrics.v_measure_score([0, 0, 1, 2], [0, 0, 1, 1]))\nprint(\"%.3f\" % metrics.v_measure_score([0, 1, 2, 3], [0, 0, 1, 1]))","310d1774":"print(\"%.3f\" % metrics.v_measure_score([0, 0, 1, 1], [0, 0, 1, 2]))\nprint(\"%.3f\" % metrics.v_measure_score([0, 0, 1, 1], [0, 1, 2, 3]))","7cc70def":"print(\"%.3f\" % metrics.v_measure_score([0, 0, 0, 0], [0, 1, 2, 3]))","ae223e80":"print(\"%.3f\" % metrics.v_measure_score([0, 0, 1, 1], [0, 0, 0, 0]))","aa62f5c2":"import numpy as np\n\n#Create some data\nMAXN=40\nX = np.concatenate([1.25*np.random.randn(MAXN,2), 5+1.5*np.random.randn(MAXN,2)])\nX = np.concatenate([X,[8,3]+1.2*np.random.randn(MAXN,2)])\nX.shape","268232a5":"#Just for visualization purposes, create the labels of the 3 distributions\ny = np.concatenate([np.ones((MAXN,1)),2*np.ones((MAXN,1))])\ny = np.concatenate([y,3*np.ones((MAXN,1))])\n\nplt.subplot(1,2,1)\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\nplt.title('Data as were generated')\n\nplt.subplot(1,2,2)\nplt.scatter(X[:,0],X[:,1],color='r')\nplt.title('Data as the algorithm sees them')\n\nplt.savefig(\"\/kaggle\/working.png\",dpi=300, bbox_inches='tight')\n\nfrom sklearn import cluster\n\nK=3 # Assuming to be 3 clusters!\n\nclf = cluster.KMeans(init='random', n_clusters=K)\nclf.fit(X)","ae0494bf":"print (clf.labels_) # or\nprint (clf.predict(X)) # equivalent","860add6f":"print (X[(y==1).ravel(),0]) #numpy.ravel() returns a flattened array\nprint (X[(y==1).ravel(),1])","a583ebd2":"plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n\nfig = plt.gcf()\nfig.set_size_inches((6,5))","4c0a88a2":"x = np.linspace(-5,15,200)\nXX,YY = np.meshgrid(x,x)\nsz=XX.shape\ndata=np.c_[XX.ravel(),YY.ravel()]\n# c_ translates slice objects to concatenation along the second axis.","7c016f65":"Z=clf.predict(data) # returns the labels of the data\nprint (Z)","f2016d27":"# Visualize space partition\nplt.imshow(Z.reshape(sz), interpolation='bilinear', origin='lower',\nextent=(-5,15,-5,15),alpha=0.3, vmin=0, vmax=K-1)\nplt.title('Space partitions', size=14)\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n\nfig = plt.gcf()\nfig.set_size_inches((6,5))\n\nplt.savefig(\"\/kaggle\/working.png\",dpi=300, bbox_inches='tight')","d6555bfc":"clf = cluster.KMeans(n_clusters=K, random_state=0)\n#initialize the k-means clustering\nclf.fit(X) #run the k-means clustering\n\ndata=np.c_[XX.ravel(),YY.ravel()]\nZ=clf.predict(data) # returns the clustering labels of the data","5dcb462e":"plt.title('Final result of K-means', size=14)\n\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n\nplt.imshow(Z.reshape(sz), interpolation='bilinear', origin='lower',\nextent=(-5,15,-5,15),alpha=0.3, vmin=0, vmax=K-1)\n\nx = np.linspace(-5,15,200)\nXX,YY = np.meshgrid(x,x)\nfig = plt.gcf()\nfig.set_size_inches((6,5))\n\nplt.savefig(\"\/kaggle\/working.png\",dpi=300, bbox_inches='tight')","eed4ecb9":"clf = cluster.KMeans(init='random', n_clusters=K, random_state=0)\n#initialize the k-means clustering\nclf.fit(X) #run the k-means clustering\nZx=clf.predict(X)\n\nplt.subplot(1,3,1)\nplt.title('Original labels', size=14)\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b') # b\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g') # g\nfig = plt.gcf()\nfig.set_size_inches((12,3))\n\nplt.subplot(1,3,2)\nplt.title('Data without labels', size=14)\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='r') # b\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='r') # g\nfig = plt.gcf()\nfig.set_size_inches((12,3))\n\nplt.subplot(1,3,3)\nplt.title('Clustering labels', size=14)\nplt.scatter(X[(Zx==1).ravel(),0],X[(Zx==1).ravel(),1],color='r')\nplt.scatter(X[(Zx==2).ravel(),0],X[(Zx==2).ravel(),1],color='b')\nplt.scatter(X[(Zx==0).ravel(),0],X[(Zx==0).ravel(),1],color='g')\nfig = plt.gcf()\nfig.set_size_inches((12,3))","3673aaad":"from sklearn import metrics\n\nclf = cluster.KMeans(n_clusters=K, init='k-means++', random_state=0,\nmax_iter=300, n_init=10)\n#initialize the k-means clustering\nclf.fit(X) #run the k-means clustering\n\nprint ('Final evaluation of the clustering:')\n\nprint('Inertia: %.2f' % clf.inertia_)\n\nprint('Adjusted_rand_score %.2f' % metrics.adjusted_rand_score(y.ravel(),\nclf.labels_))\n\nprint('Homogeneity %.2f' % metrics.homogeneity_score(y.ravel(),\nclf.labels_))\n\nprint('Completeness %.2f' % metrics.completeness_score(y.ravel(),\nclf.labels_))\n\nprint('V_measure %.2f' % metrics.v_measure_score(y.ravel(), clf.labels_))\n\nprint('Silhouette %.2f' % metrics.silhouette_score(X, clf.labels_,\nmetric='euclidean'))\n\nclf1 = cluster.KMeans(n_clusters=K, init='random', random_state=0,\nmax_iter=2, n_init=2)\n#initialize the k-means clustering\nclf1.fit(X) #run the k-means clustering\n\nprint ('Final evaluation of the clustering:')\n\nprint ('Inertia: %.2f' % clf1.inertia_)\n\nprint ('Adjusted_rand_score %.2f' % metrics.adjusted_rand_score(y.ravel(),\nclf1.labels_))\n\nprint ('Homogeneity %.2f' % metrics.homogeneity_score(y.ravel(),\nclf1.labels_))\n\nprint ('Completeness %.2f' % metrics.completeness_score(y.ravel(),\nclf1.labels_))\n\nprint ('V_measure %.2f' % metrics.v_measure_score(y.ravel(),\nclf1.labels_))\n\nprint ('Silhouette %.2f' % metrics.silhouette_score(X, clf1.labels_,\nmetric='euclidean'))","c3eac22c":"#Read and check the dataset downloaded from the EuroStat\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import cluster\n\nedu=pd.read_csv('..\/input\/ense32020ac\/files\/ch07\/educ_figdp_1_Data.csv',na_values=':')\nedu.head()","c711aabf":"edu.tail(20)","f534815d":"#Pivot table in order to get a nice feature vector representation with dual indexing by TIME and GEO\npivedu=pd.pivot_table(edu, values='Value', index=['TIME', 'GEO'], columns=['INDIC_ED'])\npivedu.head()","aabd6425":"print ('Let us check the two indices:\\n')\nprint ('\\nPrimary index (TIME): \\n' + str(pivedu.index.levels[0].tolist()))\nprint ('\\nSecondary index (GEO): \\n' + str(pivedu.index.levels[1].tolist()))","d317d6ea":"#Extract 2010 set of values\nedu2010=pivedu.loc[2010]\nedu2010.head()","21057207":"#Store column names and clear them for better handling. Do the same with countries\nedu2010 = edu2010.rename(index={'Euro area (13 countries)': 'EU13',\n'Euro area (15 countries)': 'EU15',\n'European Union (25 countries)': 'EU25',\n'European Union (27 countries)': 'EU27',\n'Former Yugoslav Republic of Macedonia, the': 'Macedonia',\n'Germany (until 1990 former territory of the FRG)': 'Germany'\n})\nfeatures = edu2010.columns.tolist()\n\ncountries = edu2010.index.tolist()\n\nedu2010.columns=range(12)\nedu2010.head(50)","860dff1f":"#Check what is going on in the NaN data\nnan_countries=np.sum(np.where(edu2010.isnull(),1,0),axis=1)\nprint(nan_countries)\nplt.bar(np.arange(nan_countries.shape[0]),nan_countries)\nplt.xticks(np.arange(nan_countries.shape[0]),countries,rotation=90,horizontalalignment='left',\nfontsize=12)\nfig = plt.gcf()\nfig.set_size_inches((12,5))","58abca56":"#Remove non info countries\nwrk_countries = nan_countries<4\neduclean=edu2010.loc[wrk_countries] #.loc - Construct an open mesh from multiple sequences.\n\n#Let us check the features we have\nna_features = np.sum(np.where(educlean.isnull(),1,0),axis=1)\n\nplt.bar(np.arange(na_features.shape[0]),na_features)\nplt.xticks(np.arange(nan_countries.shape[0]),countries,rotation=90,horizontalalignment='left',fontsize=12)\nfig = plt.gcf()\nfig.set_size_inches((12,5))","50112136":"#Option A fills those features with some value, at risk of extracting wrong information\n#Constant filling : edufill0=educlean.fillna(0)\nedufill=educlean.fillna(educlean.mean())\nprint ('Filled in data shape: ' + str(edufill.shape))\n\n#Option B drops those features\nedudrop=educlean.dropna(axis=1)\n#dropna: Return object with labels on given axis omitted where alternately any or\n# all of the data are missing\nprint ('Drop data shape: ' + str(edudrop.shape))","6b4c25b1":"scaler = StandardScaler() #Standardize features by removing the mean and scaling to unit variance\n\nX_train_fill = edufill.values\nX_train_fill = scaler.fit_transform(X_train_fill)\n\nclf = cluster.KMeans(init='k-means++', n_clusters=3, random_state=42)\n\nclf.fit(X_train_fill) #Compute k-means clustering.\n\ny_pred_fill = clf.predict(X_train_fill)\n#Predict the closest cluster each sample in X belongs to.\n\nidx=y_pred_fill.argsort()","bd43895c":"plt.plot(np.arange(35),y_pred_fill[idx],'ro')\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n\nplt.xticks(np.arange(len(wrk_countries_names)),[wrk_countries_names[i] for i in idx],\nrotation=90,horizontalalignment='left',fontsize=12)\nplt.title('Using filled in data', size=15)\nplt.yticks([0,1,2])\nfig = plt.gcf()\n\nfig.set_size_inches((12,5))","9e89258d":"X_train_drop = edudrop.values\nX_train_drop = scaler.fit_transform(X_train_drop)\n\nclf.fit(X_train_drop) #Compute k-means clustering.\ny_pred_drop = clf.predict(X_train_drop) #Predict the closest cluster of each sample in X.","da92abf1":"idx=y_pred_drop.argsort()\nplt.plot(np.arange(35),y_pred_drop[idx],'ro')\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n\nplt.xticks(np.arange(len(wrk_countries_names)),[wrk_countries_names[i] for i in idx],\nrotation=90,horizontalalignment='left',fontsize=12)\nplt.title('Using dropped missing values data',size=15)\nfig = plt.gcf()\nplt.yticks([0,1,2])\nfig.set_size_inches((12,5))","8827aeb3":"plt.plot(y_pred_drop+0.2*np.random.rand(35),y_pred_fill+0.2*np.random.rand(35),'bo')\nplt.xlabel('Predicted clusters for the filled in dataset.')\nplt.ylabel('Predicted clusters for the dropped missing values dataset.')\nplt.title('Correlations')\nplt.xticks([0,1,2])\nplt.yticks([0,1,2])\nplt.savefig(\"\/kaggle\/working.png\",dpi=300, bbox_inches='tight')","6f429be5":"print ('Cluster 0: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_fill)\nif item==0]))\nprint ('Cluster 0: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==0]))\nprint ('\\n')\nprint ('Cluster 1: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_fill)\nif item==1]))\nprint ('Cluster 1: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==1]))\nprint ('\\n')\nprint ('Cluster 2: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_fill)\nif item==2]))\nprint ('Cluster 2: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==2]))\nprint ('\\n')","87ad3a13":"width=0.3\np1 = plt.bar(np.arange(8),scaler.inverse_transform(clf.cluster_centers_[1]),width,color='b')\n# Scale back the data to the original representation\np2 = plt.bar(np.arange(8)+width,scaler.inverse_transform(clf.cluster_centers_[2]),\nwidth,color='yellow')\np0 = plt.bar(np.arange(8)+2*width,scaler.inverse_transform(clf.cluster_centers_[0]),\nwidth,color='r')\n\nplt.legend( (p0[0], p1[0], p2[0]), ('Cluster 0', 'Cluster 1', 'Cluster 2') ,loc=9)\nplt.xticks(np.arange(8) + 0.5, np.arange(8),size=12)\nplt.yticks(size=12)\nplt.xlabel('Economical indicators')\nplt.ylabel('Average expanditure')\nfig = plt.gcf()\n\nplt.savefig(\"\/kaggle\/working.png\",dpi=300, bbox_inches='tight')","1135dde0":"edudrop.tail()","2d813797":"from scipy.spatial import distance\np = distance.cdist(X_train_drop[y_pred_drop==1,:],[clf.cluster_centers_[0]],'euclidean')\n#the distance of the elements of cluster 0 to the center of cluster 1\n\nfx = np.vectorize(np.int)\n\nplt.plot(np.arange(p.shape[0]),\nfx(p)\n)\n\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\nzero_countries_names = [wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==1]\nplt.xticks(np.arange(len(zero_countries_names)),zero_countries_names,rotation=90,\nhorizontalalignment='left',fontsize=12)","ebd908dc":"from scipy.spatial import distance\np = distance.cdist(X_train_drop[y_pred_drop==1,:],[clf.cluster_centers_[1]],'euclidean')\npown = distance.cdist(X_train_drop[y_pred_drop==1,:],[clf.cluster_centers_[0]],'euclidean')\n\nwidth=0.45\np0=plt.plot(np.arange(p.shape[0]),fx(p),width)\np1=plt.plot(np.arange(p.shape[0])+width,fx(pown),width,color = 'red')\n\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\nzero_countries_names = [wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==1]\nplt.xticks(np.arange(len(zero_countries_names)),zero_countries_names,rotation=90,\nhorizontalalignment='left',fontsize=12)\nplt.legend( (p0[0], p1[0]), ('d -> 1', 'd -> 0') ,loc=1)\nplt.savefig(\"\/kaggle\/working.png\",dpi=300, bbox_inches='tight')","69b8e7cf":"X_train = edudrop.values\nclf = cluster.KMeans(init='k-means++', n_clusters=4, random_state=0)\nclf.fit(X_train)\ny_pred = clf.predict(X_train)\n\nidx=y_pred.argsort()\nplt.plot(np.arange(35),y_pred[idx],'ro')\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n\nplt.xticks(np.arange(len(wrk_countries_names)),[wrk_countries_names[i] for i in idx],rotation=90,\nhorizontalalignment='left',fontsize=12)\nplt.title('Using drop features',size=15)\nplt.yticks([0,1,2,3])\nfig = plt.gcf()\nfig.set_size_inches((12,5))","231245e1":"width=0.2\np0 = plt.bar(np.arange(8)+1*width,clf.cluster_centers_[0],width,color='r')\np1 = plt.bar(np.arange(8),clf.cluster_centers_[1],width,color='b')\np2 = plt.bar(np.arange(8)+3*width,clf.cluster_centers_[2],width,color='yellow')\np3 = plt.bar(np.arange(8)+2*width,clf.cluster_centers_[3],width,color='pink')\n\nplt.legend( (p0[0], p1[0], p2[0], p3[0]), ('Cluster 0', 'Cluster 1', 'Cluster 2',\n'Cluster 3') ,loc=9)\nplt.xticks(np.arange(8) + 0.5, np.arange(8),size=12)\nplt.yticks(size=12)\nplt.xlabel('Economical indicator')\nplt.ylabel('Average expenditure')\nfig = plt.gcf()\nfig.set_size_inches((12,5))\nplt.savefig(\"\/kaggle\/working.png\",dpi=300, bbox_inches='tight')","ed5e1fd2":"print ('Cluster 0: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred) if item==0]))\n\nprint ('Cluster 1: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred) if item==1]))\n\nprint ('Cluster 2: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred) if item==2]))\n\nprint ('Cluster 3: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred) if item==3]))\n\n#Save data for future use.\nimport pickle\nofname = open('edu2010.pkl', 'wb')\ns = pickle.dump([edu2010, wrk_countries_names,y_pred ],ofname)\nofname.close()","78561335":"from scipy.cluster.hierarchy import linkage, dendrogram\nfrom scipy.spatial.distance import pdist\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.metrics import euclidean_distances\n\nX = StandardScaler().fit_transform(edudrop.values)\n\ndistances = euclidean_distances(edudrop.values)\n\nspectral = cluster.SpectralClustering(n_clusters=4, affinity=\"nearest_neighbors\")\nspectral.fit(edudrop.values)\n\ny_pred = spectral.labels_.astype(np.int)","3c83ef6d":"idx=y_pred.argsort()\n\nplt.plot(np.arange(35),y_pred[idx],'ro')\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n\nplt.xticks(np.arange(len(wrk_countries_names)),[wrk_countries_names[i]\nfor i in idx],rotation=90,horizontalalignment='left',fontsize=12)\n\nplt.yticks([0,1,2,3])\n\nplt.title('Applying Spectral Clustering on the drop features',size=15)\nfig = plt.gcf()\nfig.set_size_inches((12,5))","131d6d8a":"X_train = edudrop.values\ndist = pdist(X_train,'euclidean')\nlinkage_matrix = linkage(dist,method = 'complete');\nplt.figure() # we need a tall figure\nfig = plt.gcf()\nfig.set_size_inches((12,12))\ndendrogram(linkage_matrix, orientation=\"right\", color_threshold = 3,labels = wrk_countries_names, leaf_font_size=20);\n\nplt.savefig(\"\/kaggle\/working.png\",dpi=300, bbox_inches='tight')\nplt.show()\n\n#plt.tight_layout() # fixes margins","0e1f15fe":"Spain is one of the country in the cluster 1 to be the closest to cluster 0.","9ccfc952":"We now have two dataset. The first one with the 12 indicators and the NaN values have been modified with the mean of values.\nThe second one with only 8 indicators for all countries.","2fa5df53":"This time, it's the cluster 3 which invest the most in education and the cluster 2 which invest the less. The cluster of Spain, the cluster 0 is the second one to invest the less.","83cd57ab":"To simplify the reading of the table we reduced some texts. For example, rather than having the explaination of indicators, we have a number between 0 and 11. Some country name have been reduced also.","3df2418c":"We still have 3 clusters with different number of country. The first one has less number of countries. We can notice also that the countries of each groupe are not the same than when we clustered the dataset with filled in na values by mean.","4b945aff":"Question: Labelings that assign all classes members to the same clusters are: **complete**, but not **homogeneous**","a1c73f0a":"Question: Labelings that have pure clusters with members coming from the same classes are homogeneous but un-necessary splits harm completeness and thus penalise V-measure as well","0b9cefa3":"We now have droped the values for the 3 countries with two much missing values. Now the worst cases are : Croatia, Japan, Switzerland","fbb38dd1":"With 4 clusters, Spain is now in cluster 0","32d33619":"On this graph, we can see the different cluster which are obtained from the Kmeans++ method. We can see the link bewtween country and this link change color when the distance between nodes is bigger than the distance in the cluster.","182a5677":"Questions: How many \u201cmisclusterings\u201d do we have? \n\nIt seems that we have two miscluster. The blue and green one are mixed while they should be separated with iteration on the creation of cluster.","37431128":"We separated datas in 3 clusters, we can see 3 different colors. Nevertheless, we can see that some green point are in the group of blue point. It means that there was no iteration to adjust the clusters.","d5a7660d":"We see that Spain is further away to cluster 0, than it is from centroid of cluster 1. Spain is in the good cluster, as the method should do, so we can conclude that Spain spend the less fund in education in comparaison of other european countries.","caf5649a":"We can see that the two groups of clusters are different but a number of country are in the same predicted clusters for the both datasets.","44701dc1":"Question: Clusters that include samples from totally different classes totally destroy the homogeneity, so also the V measure of the labelling, hence:","100a613f":"Finally, the Kmeans++ method gave us clusters which are more accurate than the random one. \nThen, we tried the spectral method which gave more equilibrated clusters in number of country. \nTo finish, we used agglomerate method which provide the same result as Kmeans++ method in our case.","659fa67f":"The study on clusters seems to be based on the data without indicators when there was NaN values.\nIt is easy to notice that the first cluster, \"cluster 0\" globally invests more in education while the second cluster, \"cluster 1\", invests the less. There is here only one exception for the third indicator.\nWe know that Spain is in the Cluster 1, so it seems that Spain is one of the countries which invest the less in education.","dca0ba78":"Question: Shall the centroids belong to the original set of points?\n\nNo because at the beginning the group are made randomly, and then the algorithm need to adjust groups by including and ejecting samples. A cluster can totally change depending on the random first situation.","9de7192d":"Spain is now in the cluster 3 and we can notice that all clusters changed","aafcd6a5":"The graph shows us the number of missing values of each country in the table. There is only 7 countries with missing values, but the worst case is for Turkey and Luxembourg for which 9 values are missing and then Liechtenstein with 6 missing values.","8dd0f95f":"3. CASE STUDY: EUROSTAT data analysis","fcda4ac5":"[](http:\/\/)We can see that we have 4 information as expected : time, country, indicators and value. The dataset is order by years and within a year by country.\nAbout the quality of the dataset, we can already notice that there are missing values \"NaN\".","a59954fe":"We changed the table to be more readable. In vertical axis we have time and countries and in horizontal axis we have the indicators. Then in the table we have the values.","67fb8f3e":"Finally, we can conclude that the Kmeans++ method is better than the random one because all indicators are better in the first situation. The V_measure is better so globally the clusters are better designed. Nevertheless, for the both the Silhouette is 0.5 so the both are dense and well separated.\n\nThen, as expected, the inertia is smaller in the Kmeans++ method because this one aims to reduce it.","30f648f0":"A new clustering method is used, the spectral clustering method.","a1ea7214":"We can see that we have 3 clusters with different number of country in each group. The second one has the less number of country.","4a0b625f":"We focused on 2010 to simplify the case, so there is only one index now, which is country.","92208779":"As we can see, the value for Spain are really close to the mean value of the cluster 1."}}