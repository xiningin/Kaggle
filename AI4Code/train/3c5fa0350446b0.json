{"cell_type":{"f756eefe":"code","66e589ab":"code","77b5aa92":"code","54948243":"code","82a08062":"code","7df0f708":"code","b37d1a6a":"code","d160ad50":"code","84358248":"code","4bf946ab":"code","b611937b":"code","fc60ec38":"code","418b6e25":"code","39488275":"code","b7de0f37":"code","5eae88e9":"code","8f6b9eb9":"code","e1f3a042":"code","9583523a":"code","822ff964":"code","c3aab795":"code","62c79fc2":"code","f1bb6efd":"code","ba98a664":"code","0d716809":"code","5c819044":"code","0c8e935c":"code","b7a7f6f7":"code","93205114":"code","247baa83":"code","60faec19":"code","2b87aa48":"code","b076db80":"markdown","8bf130f1":"markdown","341427df":"markdown","685e67f2":"markdown","b41c750e":"markdown","b913b852":"markdown","96017efd":"markdown","d17840a0":"markdown","9b22b992":"markdown","4f509686":"markdown","7293244a":"markdown","16738d54":"markdown","265f3580":"markdown","422146c6":"markdown","3f08c5c6":"markdown","71f1b5b9":"markdown","1e6083bc":"markdown","aa214f15":"markdown","7a934bb4":"markdown"},"source":{"f756eefe":"import gym\nfrom gym import envs\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib as mpl\nfrom matplotlib import cm\nfrom collections import defaultdict\nfrom IPython.display import clear_output\n%matplotlib inline","66e589ab":"env = gym.make('Blackjack-v0')","77b5aa92":"env.observation_space","54948243":"env.action_space.n","82a08062":"env.reset()","7df0f708":"visible = \"\" if env._get_obs()[2] else \"no\"\nprint(\"The above shows that the player's hand has a total sum of \" \n      + str(env._get_obs()[0]) + \" while the dealer visible hand is \" \n      + str(env._get_obs()[1]) + \" and that the player has \" + visible + \" usable ace\")","b37d1a6a":"env.step(1)","d160ad50":"env.step(0)","84358248":"env.dealer","4bf946ab":"def draw_till_17_pol(obs):\n    return [1,0] if obs[0]<17 else [0,1]\n\ndef calc_payoffs(env,rounds,players,pol):\n    \"\"\"\n    Calculate Payoffs.\n    \n    Args:\n        env: environment\n        rounds: Number of rounds a player would play\n        players: Number of players \n        pol: Policy used\n        \n    Returns:\n        Average payoff\n    \"\"\"\n    average_payouts = []\n    for player in range(players):\n        rd = 1\n        total_payout = 0 # to store total payout over 'num_rounds'\n\n        while rd <= rounds:\n            action = np.argmax(pol(env._get_obs()))\n            obs, payout, is_done, _ = env.step(action)\n            if is_done:\n                total_payout += payout\n                env.reset() # Environment deals new cards to player and dealer\n                rd += 1\n        average_payouts.append(total_payout)\n\n    plt.plot(average_payouts)                \n    plt.xlabel('num_player')\n    plt.ylabel('payout after ' + str(rounds) + 'rounds')\n    plt.show()    \n    print (\"Average payout of a player after {} rounds is {}\".format(rounds, sum(average_payouts)\/players))","b611937b":"env = gym.make('Blackjack-v0')\nenv.reset()\ncalc_payoffs(env,1000,1000,draw_till_17_pol)","fc60ec38":"from mpl_toolkits.axes_grid1 import make_axes_locatable\nfrom mpl_toolkits.mplot3d import Axes3D\ndef plot_policy(policy):\n\n    def get_Z(player_hand, dealer_showing, usable_ace):\n        if (player_hand, dealer_showing, usable_ace) in policy:\n            return policy[player_hand, dealer_showing, usable_ace]\n        else:\n            return 1\n\n    def get_figure(usable_ace, ax):\n        x_range = np.arange(1, 11)\n        y_range = np.arange(11, 22)\n        X, Y = np.meshgrid(x_range, y_range)\n        Z = np.array([[get_Z(player_hand, dealer_showing, usable_ace) for dealer_showing in x_range] for player_hand in range(21, 10, -1)])\n        surf = ax.imshow(Z, cmap=plt.get_cmap('Accent', 2), vmin=0, vmax=1, extent=[0.5, 10.5, 10.5, 21.5])\n        plt.xticks(x_range, ('A', '2', '3', '4', '5', '6', '7', '8', '9', '10'))\n        plt.yticks(y_range)\n        ax.set_xlabel('Dealer Showing')\n        ax.set_ylabel('Player Hand')\n        ax.grid(color='black', linestyle='-', linewidth=1)\n        divider = make_axes_locatable(ax)\n        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n        cbar = plt.colorbar(surf, ticks=[0, 1], cax=cax)\n        cbar.ax.set_yticklabels(['0 (STICK)','1 (HIT)'])\n        cbar.ax.invert_yaxis() \n            \n    fig = plt.figure(figsize=(12, 12))\n    ax = fig.add_subplot(121)\n    ax.set_title('Usable Ace', fontsize=16)\n    get_figure(True, ax)\n    ax = fig.add_subplot(122)\n    ax.set_title('No Usable Ace', fontsize=16)\n    get_figure(False, ax)\n    plt.show()","418b6e25":"def plot_value_function(V, title=\"Value Function\"):\n    \"\"\"\n    Plots the value function as a surface plot.\n    \"\"\"\n    min_x = min(k[0] for k in V.keys())\n    max_x = max(k[0] for k in V.keys())\n    min_y = min(k[1] for k in V.keys())\n    max_y = max(k[1] for k in V.keys())\n\n    x_range = np.arange(min_x, max_x + 1)\n    y_range = np.arange(min_y, max_y + 1)\n    X, Y = np.meshgrid(x_range, y_range)\n\n    # Find value for all (x, y) coordinates\n    Z_noace = np.apply_along_axis(lambda _: V[(_[0], _[1], False)], 2, np.dstack([X, Y]))\n    Z_ace = np.apply_along_axis(lambda _: V[(_[0], _[1], True)], 2, np.dstack([X, Y]))\n\n    def plot_surface(X, Y, Z, title):\n        fig = plt.figure(figsize=(16,8))\n        ax = fig.add_subplot(111, projection='3d')\n        surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n                               cmap=matplotlib.cm.coolwarm, vmin=-1.0, vmax=1.0)\n        ax.set_xlabel('Player Sum')\n        ax.set_ylabel('Dealer Showing')\n        ax.set_zlabel('Value')\n        ax.set_title(title)\n        ax.view_init(ax.elev, -120)\n        fig.colorbar(surf)\n        plt.show()\n\n    plot_surface(X, Y, Z_noace, \"{} (No Usable Ace)\".format(title))\n    plot_surface(X, Y, Z_ace, \"{} (Usable Ace)\".format(title))\n","39488275":"def create_epsilon_greedy_action_policy(env,Q,epsilon):\n    \"\"\" Create epsilon greedy action policy\n    Args:\n        env: Environment\n        Q: Q table\n        epsilon: Probability of selecting random action instead of the 'optimal' action\n    \n    Returns:\n        Epsilon-greedy-action Policy function with Probabilities of each action for each state\n    \"\"\"\n    def policy(obs):\n        P = np.ones(env.action_space.n, dtype=float) * epsilon \/ env.action_space.n  #initiate with same prob for all actions\n        best_action = np.argmax(Q[obs])  #get best action\n        P[best_action] += (1.0 - epsilon)\n        return P\n    return policy","b7de0f37":"def On_pol_mc_control_learn(env, episodes, discount_factor, epsilon):\n    \"\"\"\n    Monte Carlo Control using Epsilon-Greedy policies.\n    Finds an optimal epsilon-greedy policy.\n    \n    Args:\n        env: Environment.\n        episodes: Number of episodes to sample.\n        discount_factor: Gamma discount factor.\n        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n    \n    Returns:\n        A tuple (Q, policy).\n        Q is a dictionary mapping state to action values.\n        Policy is the trained policy that returns action probabilities\n    \"\"\"\n    # Keeps track of sum and count of returns for each state\n    # An array could be used to save all returns but that's memory inefficient.\n    # defaultdict used so that the default value is stated if the observation(key) is not found\n    returns_sum = defaultdict(float)\n    returns_count = defaultdict(float)\n    \n    # The final action-value function.\n    # A nested dictionary that maps state -> (action -> action-value).\n    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n    \n    # The policy we're following\n    pol = create_epsilon_greedy_action_policy(env,Q,epsilon)\n    \n    for i in range(1, episodes + 1):\n        # Print out which episode we're on\n        if i% 1000 == 0:\n            print(\"\\rEpisode {}\/{}.\".format(i, episodes), end=\"\")\n            clear_output(wait=True)\n\n        # Generate an episode.\n        # An episode is an array of (state, action, reward) tuples\n        episode = []\n        state = env.reset()\n        for t in range(100):\n            probs = pol(state)\n            action = np.random.choice(np.arange(len(probs)), p=probs)\n            next_state, reward, done, _ = env.step(action)\n            episode.append((state, action, reward))\n            if done:\n                break\n            state = next_state\n\n        # Find all (state, action) pairs we've visited in this episode\n        # We convert each state to a tuple so that we can use it as a dict key\n        sa_in_episode = set([(tuple(x[0]), x[1]) for x in episode])\n        for state, action in sa_in_episode:\n            sa_pair = (state, action)\n            #First Visit MC:\n            # Find the first occurance of the (state, action) pair in the episode\n            first_occurence_idx = next(i for i,x in enumerate(episode)\n                                       if x[0] == state and x[1] == action)\n            # Sum up all rewards since the first occurance\n            G = sum([x[2]*(discount_factor**i) for i,x in enumerate(episode[first_occurence_idx:])])\n            # Calculate average return for this state over all sampled episodes\n            returns_sum[sa_pair] += G\n            returns_count[sa_pair] += 1.0\n            Q[state][action] = returns_sum[sa_pair] \/ returns_count[sa_pair]\n    \n    return Q, pol","5eae88e9":"env = gym.make('Blackjack-v0')\nenv.reset()\nQ_on_pol,On_MC_Learned_Policy = On_pol_mc_control_learn(env, 500000, 0.9, 0.05)","8f6b9eb9":"V = defaultdict(float)\nfor state, actions in Q_on_pol.items():\n    action_value = np.max(actions)\n    V[state] = action_value\nplot_value_function(V, title=\"Optimal Value Function for On-Policy Learning\")","e1f3a042":"on_pol = {key: np.argmax(On_MC_Learned_Policy(key)) for key in Q_on_pol.keys()}\nprint(\"On-Policy MC Learning Policy\")\nplot_policy(on_pol)","9583523a":"#Payoff for On-Policy MC Trained Policy\nenv.reset()\ncalc_payoffs(env,1000,1000,On_MC_Learned_Policy)","822ff964":"def create_random_policy(nA):\n    \"\"\"\n    Creates a random policy function.\n    \n    Args:\n        nA: Number of actions in the environment.\n    \n    Returns:\n        A function that takes an observation state as input and returns a vector\n        of action probabilities\n    \"\"\"\n    A = np.ones(nA, dtype=float) \/ nA\n    def policy_fn(obs):\n        return A\n    return policy_fn\ndef create_greedy_action_policy(env,Q):\n    \"\"\" Create greedy action policy\n    Args:\n        env: Environment\n        Q: Q table\n    \n    Returns:\n        Greedy-action Policy function \n    \"\"\"\n    def policy(obs):\n        P = np.zeros_like(Q[obs], dtype=float)\n        best_action = np.argmax(Q[obs])  #get best action\n        P[best_action] = 1\n        return P\n    return policy\n\ndef Off_pol_mc_control_learn(env, num_episodes, policy, discount_factor):\n    \"\"\"\n    Monte Carlo Control Off-Policy Control using Weighted Importance Sampling.\n    Finds an optimal greedy policy.\n    \n    Args:\n        env: Environment.\n        num_episodes: Number of episodes to sample.\n        policy: The policy to follow while generating episodes.\n            A function that given an observation returns a vector of probabilities for each action.\n        discount_factor: Gamma discount factor.\n    \n    Returns:\n        A tuple (Q, policy).\n        Q is a dictionary mapping state -> action values.\n        policy is a function that takes an observation as an argument and returns\n        action probabilities. This is the optimal greedy policy.\n    \"\"\"\n    \n    # The final action-value function.\n    # A dictionary that maps state -> action values\n    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n    # The cumulative denominator of the weighted importance sampling formula\n    # (across all episodes)\n    C = defaultdict(lambda: np.zeros(env.action_space.n))\n    \n    # Our greedy policy \n    target_policy = create_greedy_action_policy(env,Q)\n        \n    for i_episode in range(1, num_episodes + 1):\n        if i_episode % 1000 == 0:\n            print(\"\\rEpisode {}\/{}.\".format(i_episode, num_episodes), end=\"\")\n            clear_output(wait=True)\n\n        # Generate an episode.\n        # An episode is an array of (state, action, reward) tuples\n        episode = []\n        state = env.reset()\n        for t in range(100):\n            # Sample an action from our policy\n            probs = target_policy(state)\n            action = np.random.choice(np.arange(len(probs)), p=probs)\n            next_state, reward, done, _ = env.step(action)\n            episode.append((state, action, reward))\n            if done:\n                break\n            state = next_state\n        \n        # Sum of discounted returns\n        G = 0.0\n        # The importance sampling ratio (the weights of the returns)\n        W = 1.0\n        # For each step in the episode, backwards\n        for t in range(len(episode))[::-1]:\n            state, action, reward = episode[t]\n            # Update the total reward since step t\n            G = discount_factor * G + reward\n            # Update weighted importance sampling formula denominator\n            C[state][action] += W\n            # Update the action-value function using the incremental update formula \n            # This also improves our target policy which holds a reference to Q\n            Q[state][action] += (W \/ C[state][action]) * (G - Q[state][action])\n            # If the action taken by the policy is not the action \n            # taken by the target policy the probability will be 0 and we can break\n            if action !=  np.argmax(target_policy(state)):\n                break\n            W = W * 1.\/policy(state)[action]\n        \n    return Q, target_policy","c3aab795":"env = gym.make('Blackjack-v0')\nenv.reset()\nrand = create_random_policy(env.action_space.n)\nQ_off_Pol,off_MC_Learned_Policy = Off_pol_mc_control_learn(env, 500000, rand,0.9)","62c79fc2":"#Payoff for Off-Policy MC Trained Policy\nenv.reset()\ncalc_payoffs(env,1000,1000,off_MC_Learned_Policy)","f1bb6efd":"V = defaultdict(float)\nfor state, actions in Q_off_Pol.items():\n    action_value = np.max(actions)\n    V[state] = action_value\nplot_value_function(V, title=\"Optimal Value Function for Off-Policy Learning\")","ba98a664":"pol_test = {key: np.argmax(off_MC_Learned_Policy(key)) for key in Q_off_Pol.keys()}\nprint(\"Off-Policy MC Learning Policy\")\nplot_policy(pol_test)","0d716809":"def create_epsilon_greedy_action_policy(env,Q,epsilon):\n    \"\"\" Create epsilon greedy action policy\n    Args:\n        env: Environment\n        Q: Q table\n        epsilon: Probability of selecting random action instead of the 'optimal' action\n    \n    Returns:\n        Epsilon-greedy-action Policy function with Probabilities of each action for each state\n    \"\"\"\n    def policy(obs):\n        P = np.ones(env.action_space.n, dtype=float) * epsilon \/ env.action_space.n  #initiate with same prob for all actions\n        best_action = np.argmax(Q[obs])  #get best action\n        P[best_action] += (1.0 - epsilon)\n        return P\n    return policy\ndef SARSA(env, episodes, epsilon, alpha, gamma):\n    \"\"\"\n    SARSA Learning Method\n    \n    Args:\n        env: OpenAI gym environment.\n        episodes: Number of episodes to sample.\n        epsilon: Probability of selecting random action instead of the 'optimal' action\n        alpha: Learning Rate\n        gamma: Gamma discount factor\n        \n    \n    Returns:\n        A tuple (Q, policy).\n        Q is a dictionary mapping state -> action values.\n        policy is a function that takes an observation as an argument and returns\n        action probabilities. \n    \"\"\"\n    \n    # Initialise a dictionary that maps state -> action values\n    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n    # The policy we're following\n    pol = create_epsilon_greedy_action_policy(env,Q,epsilon)\n    for i in range(1, episodes + 1):\n        # Print out which episode we're on\n        if i% 1000 == 0:\n            print(\"\\rEpisode {}\/{}.\".format(i, episodes), end=\"\")\n            clear_output(wait=True)\n        curr_state = env.reset()\n        probs = pol(curr_state)   #get epsilon greedy policy\n        curr_act = np.random.choice(np.arange(len(probs)), p=probs)\n        while True:\n            next_state,reward,done,_ = env.step(curr_act)\n            next_probs = create_epsilon_greedy_action_policy(env,Q,epsilon)(next_state)\n            next_act = np.random.choice(np.arange(len(next_probs)),p=next_probs)\n            td_target = reward + gamma * Q[next_state][curr_act]\n            td_error = td_target - Q[curr_state][curr_act]\n            Q[curr_state][curr_act] = Q[curr_state][curr_act] + alpha * td_error\n            if done:\n                break\n            curr_state = next_state\n            curr_act = next_act\n    return Q, pol","5c819044":"env = gym.make('Blackjack-v0')\nenv.reset()\nQ_SARSA,SARSA_Policy = SARSA(env, 500000, 0.1, 0.1,0.95)","0c8e935c":"#Payoff for Off-Policy MC Trained Policy\nenv.reset()\ncalc_payoffs(env,1000,1000,SARSA_Policy)","b7a7f6f7":"pol_sarsa = {key: np.argmax(SARSA_Policy(key)) for key in Q_SARSA.keys()}\nprint(\"SARSA Learning Policy\")\nplot_policy(pol_sarsa)","93205114":"def off_pol_TD_Q_learn(env, episodes, epsilon, alpha, gamma):\n    \"\"\"\n    Off-Policy TD Q-Learning Method\n    \n    Args:\n        env: OpenAI gym environment.\n        episodes: Number of episodes to sample.\n        epsilon: Probability of selecting random action instead of the 'optimal' action\n        alpha: Learning Rate\n        gamma: Gamma discount factor\n        \n    \n    Returns:\n        A tuple (Q, policy).\n        Q is a dictionary mapping state -> action values.\n        policy is a function that takes an observation as an argument and returns\n        action probabilities. \n    \"\"\"\n    # Initialise a dictionary that maps state -> action values\n    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n    # The policy we're following\n    pol = create_epsilon_greedy_action_policy(env,Q,epsilon)\n    for i in range(1, episodes + 1):\n        if i% 1000 == 0:\n            print(\"\\rEpisode {}\/{}.\".format(i, episodes), end=\"\")\n            clear_output(wait=True)\n        curr_state = env.reset()\n        while True:\n            probs = pol(curr_state)   #get epsilon greedy policy\n            curr_act = np.random.choice(np.arange(len(probs)), p=probs)\n            next_state,reward,done,_ = env.step(curr_act)\n            next_act = np.argmax(Q[next_state])\n            td_target = reward + gamma * Q[next_state][next_act]\n            td_error = td_target - Q[curr_state][curr_act]\n            Q[curr_state][curr_act] = Q[curr_state][curr_act] + alpha * td_error\n            if done:\n                break\n            curr_state = next_state\n    return Q, pol","247baa83":"env = gym.make('Blackjack-v0')\nenv.reset()\nQ_QLearn,QLearn_Policy = off_pol_TD_Q_learn(env, 500000, 0.1, 0.1,0.95)","60faec19":"#Payoff for Off-Policy Q-Learning Trained Policy\nenv.reset()\ncalc_payoffs(env,1000,1000,QLearn_Policy)","2b87aa48":"pol_QLearn = {key: np.argmax(QLearn_Policy(key)) for key in Q_QLearn.keys()}\nprint(\"Off-Policy Q Learning Policy\")\nplot_policy(pol_QLearn)","b076db80":"# **Optimising Blackjack Strategy using Model-Free Learning**\n\nIn Reinforcement learning, there are 2 kinds of approaches, model-based learning and model-free learning. Model-Based Learning can be applied if we have full information of the transition probabilitiies and rewards, but it would be too computationally expensive if the game gets too complex. \n\nModel-Free Learning is the more practical approach as it doesn't need to have information on the full transition probabilities and rewards as it focus on figuring out value function directly from the interactions with the environment. \n\nWe would attempt to train an agent to play blackjack using model-free learning approach. ","8bf130f1":"Assume the next action would be to stay.","341427df":"The agent only has 2 options: Hit(1) or Stay(0).","685e67f2":"**1. Basics of the OpenAi Blackjack Environment**","b41c750e":"**3.2 MC (Off-Policy) --> Learn from the tail**","b913b852":"Let's first define some function to plot our policy and value function.","96017efd":"**2. Basic Naive Strategy**\n\nLet's start by testing out a basic strategy which is to **Draw as long as the score is below 17** and calculate the average payoff.","d17840a0":"**5. Summary**","9b22b992":"**4.1 SARSA Learning**","4f509686":"**4. TD Learning**","7293244a":"The current Dealer strategy is to draw cards as long as he has a score of below 17.","16738d54":"The second element in the tuple above shows the reward. The reward for winning is +1, drawing is 0, and losing is -1. We can also see the actual dealer hand in:","265f3580":"**4.2 Q-Learning: Off-Policy TD**","422146c6":"Let's assume the player's first action would be to \"hit\" (Action step would be 1). The following would occur.","3f08c5c6":"**3.1 MC (On-Policy) --> Learn from the top**","71f1b5b9":"Let's view one scenario.","1e6083bc":"The states are stored in this tuple format:\n\n(Agent's score , Dealer's visible score, and whether or not the agent has a usable ace)","aa214f15":"We can see the different policies and payoffs for the basic naive strategy, the Monte-Carlo On-Policy trained policy, Monte-Carlo Off-Policy trained policy, TD SARSA (On-Policy) trained policy and TD Q-Learn (Off-Policy) trained policy.\n\nThe \"best\" policy we trained would be the Monte-Carlo On-Policy policy, with an 'highest' average amount, although it is still negative. So Well, of course the best policy should be to not play at all...","7a934bb4":"**3. Monte Carlo Method**"}}