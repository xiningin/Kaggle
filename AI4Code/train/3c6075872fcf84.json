{"cell_type":{"a4ad36c0":"code","67f54d0f":"code","cd6dfcb7":"code","5552c098":"code","bacc9269":"code","53d5ec27":"code","760c182b":"code","c1033400":"code","022355ef":"code","9e3cd919":"code","fea096dd":"code","abb01d25":"code","634fbbb9":"code","7fee8fea":"code","515ec1a9":"code","169fdcaf":"code","1fe25d41":"code","5579ca36":"code","3585e8b3":"code","fe60cf3d":"code","a67aa11f":"code","1c4af46d":"code","9d6d2fac":"code","a3c9ee39":"code","b27d662f":"code","69e2fff9":"code","e9e037f7":"code","18de26c6":"code","0854bd5a":"code","92eb8b70":"code","638fbef4":"code","6aa2ef0b":"code","bd945335":"code","5ecffe48":"code","f9f7966a":"code","451761de":"code","f1d3cff8":"code","c0c3c60a":"code","fdc34dd1":"code","4ea6a2a2":"code","6ee65739":"code","4ca2f45e":"code","37097466":"code","aa379ea4":"code","2f8c507c":"markdown","14ad52f3":"markdown","a024e6d1":"markdown","6710eb56":"markdown","3da00972":"markdown","1296fc37":"markdown","ecfdb2fc":"markdown","fa845f79":"markdown"},"source":{"a4ad36c0":"!pip install -q efficientnet","67f54d0f":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport math\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.utils import Sequence\nimport tensorflow.keras.layers as L\nimport efficientnet.tfkeras as efn","cd6dfcb7":"# limit the GPU memory growth\ngpu = tf.config.list_physical_devices('GPU')\nprint(\"Num GPUs Available: \", len(gpu))\nif len(gpu) > 0:\n    tf.config.experimental.set_memory_growth(gpu[0], True)","5552c098":"# detect and init the TPU\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# instantiate a distribution strategy\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","bacc9269":"data_dir = Path('..\/input\/seti-breakthrough-listen\/')\ntrain_data_dir = data_dir \/ 'train'\ntest_data_dir = data_dir \/ 'test'\n\ntrain_label_file = data_dir \/ 'train_labels.csv'\nsample_file = data_dir \/ 'sample_submission.csv'","53d5ec27":"id_col = 'id'\ntarget_col = 'target'\n\nlabel = pd.read_csv(train_label_file, index_col=id_col)\nsub = pd.read_csv(sample_file, index_col=id_col)","760c182b":"label['target'].value_counts()","c1033400":"# sample_df = label.sample(frac=1).reset_index(drop=True)\nsample_df_0 = label[label['target'] == 0]\nsample_df_0_reduced = sample_df_0.sample(n=4694)\n\nsample_df_1 = label[label['target'] == 1]\n\nnew_label = pd.concat([sample_df_1, sample_df_0_reduced])\n\nnew_label['target'].value_counts()","022355ef":"def id_to_path(s, train=True):\n    data_dir = train_data_dir if train else test_data_dir\n    return data_dir \/ s[0] \/ f'{s}.npy'","9e3cd919":"input_size = (273, 256, 3)\nbatch_size = 32\nn_epoch = 5\nseed = 42","fea096dd":"class SETISequence(Sequence):\n    def __init__(self, x_set, y_set=None, batch_size=32):\n        self.x, self.y = x_set, y_set\n        self.batch_size = batch_size\n        self.is_train = False if y_set is None else True\n    \n    def __len__(self):\n        return math.ceil(len(self.x) \/ self.batch_size)\n    \n    def __getitem__(self, idx):\n        batch_ids = self.x[idx * self.batch_size: (idx + 1) * self.batch_size]\n        if self.y is not None:\n            batch_y = self.y[idx * self.batch_size: (idx + 1) * self.batch_size]\n        \n        # taking channels 1, 3, and 5 only\n        list_x = [np.load(id_to_path(x, self.is_train))[::2] for x in batch_ids]\n        batch_x = np.moveaxis(list_x,1,-1)\n        batch_x = batch_x.astype(\"float\") \/ 255\n        \n        if self.is_train:\n            return batch_x, batch_y\n        else:\n            return batch_x","abb01d25":"import matplotlib.pyplot as plt\n\ndef plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.show()","634fbbb9":"x0 = label.index.values\ny0 = label[target_col].values\n\nx1 = sub.index.values\n\nx_train, x_val, y_train, y_val = train_test_split(x0, y0, test_size=.2, random_state=seed)\n\ntrain_complete = SETISequence(x_train, y_train, batch_size=batch_size)\nval_complete = SETISequence(x_val, y_val, batch_size=batch_size)\ntest_complete = SETISequence(x1, batch_size=batch_size)\n","7fee8fea":"x0 = new_label.index.values\ny0 = new_label[target_col].values\n\nx1 = sub.index.values\n\nx_train, x_val, y_train, y_val = train_test_split(x0, y0, test_size=.2, random_state=seed, stratify=y0)\n\ntrain_reduced = SETISequence(x_train, y_train, batch_size=batch_size)\nval_reduced = SETISequence(x_val, y_val, batch_size=batch_size)\ntest_reduced = SETISequence(x1, batch_size=batch_size)","515ec1a9":"def create_model2():\n    input_size = (273, 256, 3)\n    model = tf.keras.Sequential([\n            L.Conv2D(16,(3,3), strides=(1,1), padding=\"same\", activation='relu', input_shape=(273,256,3)),\n            L.Conv2D(32,(3,3), strides=(1,1), padding=\"same\", activation='relu', input_shape=(273,256,3)),\n            L.Conv2D(64,(3,3), strides=(1,1), padding=\"same\", activation='relu', input_shape=(273,256,3)),\n            L.Conv2D(3,(3,3), strides=(1,1), padding=\"same\", activation='relu', input_shape=(273,256,3)),\n            efn.EfficientNetB1(input_shape=(273, 256, 3),weights='imagenet',include_top=False),\n            L.GlobalAveragePooling2D(),\n            L.Dense(1, activation='sigmoid')\n            ])\n\n    #model.summary\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=5e-4),\n                  loss='binary_crossentropy', metrics=[keras.metrics.AUC(), 'mae', 'accuracy'])\n    \n    return model","169fdcaf":"model2 = create_model2()\nmodel2.summary()","1fe25d41":"ckpt = tf.keras.callbacks.ModelCheckpoint(\n    \"history2.h5\", save_best_only=True, use_multiprocessing=True, workers=4, save_weights_only=True,\n)\n\nhistory2 = model2.fit(train_reduced, validation_data=val_reduced, epochs=n_epoch, callbacks=[ckpt])","5579ca36":"# Epoch 4: val_loss: 0.4185 - val_auc: 0.9245 - val_mae: 0.1706 - val_accuracy: 0.8530\n# Epoch 5: val_loss: 0.3305 - val_auc: 0.9350 - val_mae: 0.1807 - val_accuracy: 0.8653\n\nplot_graphs(history2, 'loss')\nplot_graphs(history2, 'auc')\nplot_graphs(history2, 'mae')\nplot_graphs(history2, 'accuracy')","3585e8b3":"y_pred = model2.predict(\n    test, use_multiprocessing=True, workers=4, verbose=1\n)\n\nsub = pd.read_csv('..\/input\/seti-breakthrough-listen\/sample_submission.csv')\nsub['target'] = y_pred\nsub.to_csv('history2.csv', index=False)\nsub['target'].hist()","fe60cf3d":"def create_model3():\n    input_size = (273, 256, 3)\n    model = tf.keras.Sequential([\n            L.Conv2D(3,(3,3), strides=(1,1), padding=\"same\", activation='relu', input_shape=(273,256,3)),\n            efn.EfficientNetB1(input_shape=(273, 256, 3),weights='imagenet',include_top=False),\n            #L.GlobalAveragePooling2D(),\n            #L.Flatten(),\n            L.Conv2D(16,(3,3), strides=(1,1), padding=\"same\", activation='relu'),\n            L.Conv2D(32,(3,3), strides=(1,1), padding=\"same\", activation='relu'),\n            L.Conv2D(64,(3,3), strides=(1,1), padding=\"same\", activation='relu',),\n            L.Conv2D(64,(3,3), strides=(1,1), padding=\"same\", activation='relu'),\n            L.Conv2D(64,(3,3), strides=(1,1), padding=\"same\", activation='relu'),\n            L.Flatten(),\n            # 512 neuron hidden layer\n            L.Dense(512, activation='relu'),\n            L.Dense(1, activation='sigmoid')\n            ])\n\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=5e-4),\n                  loss='binary_crossentropy', metrics=[keras.metrics.AUC(), 'mae', 'accuracy'])\n    \n    return model","a67aa11f":"model3 = create_model3()\nmodel3.summary()","1c4af46d":"model3 = create_model3()\n\nckpt = tf.keras.callbacks.ModelCheckpoint(\n    \"history3.h5\", save_best_only=True, use_multiprocessing=True, workers=4, save_weights_only=True,\n)\n\nhistory3 = model3.fit(train_reduced, validation_data=val_redcued, epochs=5, callbacks=[ckpt])","9d6d2fac":"# Epoch 4: val_loss: 0.3570 - val_auc_4: 0.9291 - val_mae: 0.1731 - val_accuracy: 0.8541\n# Epoch 5: val_auc_4: 0.9237 - val_mae: 0.1782 - val_accuracy: 0.8461\n\nplot_graphs(history3, 'loss')\nplot_graphs(history3, 'auc_4')\nplot_graphs(history3, 'mae')\nplot_graphs(history3, 'accuracy')","a3c9ee39":"y_pred = model3.predict(\n    test, use_multiprocessing=True, workers=4, verbose=1\n)\n\nsub = pd.read_csv('..\/input\/seti-breakthrough-listen\/sample_submission.csv')\nsub['target'] = y_pred\nsub.to_csv('history3.csv', index=False)\nsub['target'].hist()","b27d662f":"def create_model4():\n    model = tf.keras.Sequential([\n            L.Conv2D(3,(3,3), strides=(1,1), padding=\"same\", activation='relu', input_shape=input_size),\n            efn.EfficientNetB1(input_shape=input_size,weights='imagenet',include_top=False),\n            L.TimeDistributed(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, return_sequences=True, recurrent_dropout=0.2)), name=\"bi_gru_1\"),\n            L.TimeDistributed(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, return_sequences=True, recurrent_dropout=0.2)), name=\"bi_gru_2\"),\n            L.TimeDistributed(L.GlobalAveragePooling1D()),\n            L.Dropout(.2),\n            L.Flatten(),\n            L.Dropout(.2),\n            L.Dense(129, activation=\"sigmoid\"),\n            L.Dropout(.2),\n            L.Dense(66, activation=\"sigmoid\"),\n            L.Dropout(.2),\n            L.Dense(33, activation=\"sigmoid\"),\n            L.Dropout(.2),\n            L.Dense(18, activation=\"sigmoid\"),\n            L.Dropout(.2),\n            L.Dense(9, activation=\"sigmoid\"),\n            L.Dense(1, activation=\"sigmoid\"),\n            ])\n\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=5e-4),\n                  loss='binary_crossentropy', metrics=[keras.metrics.AUC(), 'mae', 'accuracy'])\n    \n    return model","69e2fff9":"def create_model5():\n    model = tf.keras.Sequential([\n            L.Conv2D(3,(3,3), strides=(1,1), padding=\"same\", activation='relu', input_shape=input_size),\n            efn.EfficientNetB1(input_shape=input_size,weights='imagenet',include_top=False),\n            L.TimeDistributed(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, return_sequences=True)), name=\"bi_gru_1\"),\n            L.TimeDistributed(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, return_sequences=True)), name=\"bi_gru_2\"),\n            L.TimeDistributed(L.GlobalAveragePooling1D()),\n            L.Dropout(.2),\n            L.Flatten(),\n            L.Dropout(.2),\n            L.Dense(129, activation=\"sigmoid\"),\n            L.Dropout(.2),\n            L.Dense(66, activation=\"sigmoid\"),\n            L.Dropout(.2),\n            L.Dense(33, activation=\"sigmoid\"),\n            L.Dropout(.2),\n            L.Dense(18, activation=\"sigmoid\"),\n            L.Dropout(.2),\n            L.Dense(9, activation=\"sigmoid\"),\n            L.Dense(1, activation=\"sigmoid\"),\n            ])\n\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=5e-4),\n                  loss='binary_crossentropy', metrics=[keras.metrics.AUC(), 'mae', 'accuracy'])\n    \n    return model","e9e037f7":"model4 = create_model4()\n\nckpt = tf.keras.callbacks.ModelCheckpoint(\n    \"history4.h5\", save_best_only=True, use_multiprocessing=True, workers=4, save_weights_only=True,\n)\n\nhistory4 = model4.fit(train_reduced, validation_data=val_reduced, epochs=5, callbacks=[ckpt])","18de26c6":"# Epoch 4: val_loss: 0.6268 - val_auc_6: 0.6879 - val_mae: 0.4273 - val_accuracy: 0.6794\n# Epoch 5: val_loss: 0.6949 - val_auc_6: 0.6065 - val_mae: 0.4546 - val_accuracy: 0.5980\n\nplot_graphs(history4, 'loss')\nplot_graphs(history4, 'auc_6')\nplot_graphs(history4, 'mae')\nplot_graphs(history4, 'accuracy')","0854bd5a":"y_pred = model4.predict(\n    test, use_multiprocessing=True, workers=4, verbose=1\n)\n\nsub = pd.read_csv('..\/input\/seti-breakthrough-listen\/sample_submission.csv')\nsub['target'] = y_pred\nsub.to_csv('history4.csv', index=False)\nsub['target'].hist()","92eb8b70":"model5 = create_model5()\n\nckpt = tf.keras.callbacks.ModelCheckpoint(\n    \"history5.h5\", save_best_only=True, use_multiprocessing=True, workers=4, save_weights_only=True,\n)\n\nhistory5 = model5.fit(train, validation_data=val, epochs=5, callbacks=[ckpt])","638fbef4":"# Epoch 4:  val_loss: 0.5886 - val_auc_7: 0.7669 - val_mae: 0.4186 - val_accuracy: 0.7242\n# Epoch 5: val_loss: 0.5529 - val_auc_7: 0.7893 - val_mae: 0.3814 - val_accuracy: 0.7572\n\nplot_graphs(history5, 'loss')\nplot_graphs(history5, 'auc_7')\nplot_graphs(history5, 'mae')\nplot_graphs(history5, 'accuracy')","6aa2ef0b":"y_pred = model5.predict(\n    test, use_multiprocessing=True, workers=4, verbose=1\n)\n\nsub = pd.read_csv('..\/input\/seti-breakthrough-listen\/sample_submission.csv')\nsub['target'] = y_pred\nsub.to_csv('history5.csv', index=False)\nsub['target'].hist()","bd945335":"import tensorflow_addons as tfa\nloss = tf.keras.losses.BinaryCrossentropy(\n                from_logits=False,\n                label_smoothing=0,\n                reduction='auto',\n                name='binary_crossentropy'\n    )\n    \noptimizer = tfa.optimizers.AdamW(\n        weight_decay=1e-7,\n        learning_rate=1e-4,\n        beta_1=0.9,\n        beta_2=0.999,\n        epsilon=1e-07,\n        amsgrad=True,\n        name='AdamW',\n    )\n\ndef create_model6():\n    input_size = (273, 256, 3)\n    model = tf.keras.Sequential([\n            L.Conv2D(64,(3,3), strides=(1,1), padding=\"same\", activation='relu', input_shape=(273,256,3)),\n            L.MaxPooling2D(2, 2),\n            L.Conv2D(32,(3,3), strides=(1,1), padding=\"same\", activation='relu'),\n            L.MaxPooling2D(2, 2),\n            L.Conv2D(16,(3,3), strides=(1,1), padding=\"same\", activation='relu'),\n            L.MaxPooling2D(2, 2),\n            L.Conv2D(8,(3,3), strides=(1,1), padding=\"same\", activation='relu'),\n            L.MaxPooling2D(2, 2),\n            L.Conv2D(3,(3,3), strides=(1,1), padding=\"same\", activation='relu'),\n            efn.EfficientNetB1(weights='imagenet',include_top=False),\n            L.GlobalAveragePooling2D(),\n            L.Dense(1, activation='sigmoid')\n            ])\n\n    model.compile(optimizer=optimizer,\n                  loss=loss, metrics=[keras.metrics.AUC(), 'mae', 'accuracy'])\n    \n    return model\n\ndef create_model7():\n    input_size = (273, 256, 3)\n    model = tf.keras.Sequential([\n            L.Conv2D(3,(3,3), strides=(1,1), padding=\"same\", activation='relu', input_shape=(273,256,3)),\n            efn.EfficientNetB1(weights='imagenet',include_top=False),\n            L.GlobalAveragePooling2D(),\n            L.Dense(1, activation='sigmoid')\n            ])\n\n    model.compile(optimizer=optimizer,\n                  loss=loss, metrics=[keras.metrics.AUC(), 'mae', 'accuracy'])\n    \n    return model\n\n\ndef create_model7_original():\n    input_size = (273, 256, 3)\n    model = tf.keras.Sequential([\n            L.Conv2D(3,(3,3), strides=(1,1), padding=\"same\", activation='relu', input_shape=(273,256,3)),\n            efn.EfficientNetB1(weights='imagenet',include_top=False),\n            L.GlobalAveragePooling2D(),\n            L.Dense(1, activation='sigmoid')\n            ])\n\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=5e-4),\n                  loss='binary_crossentropy', metrics=[keras.metrics.AUC(), 'mae', 'accuracy'])\n    \n    return model","5ecffe48":"scheduler_cb = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=2,\n    verbose=0,\n    mode='auto',\n    min_delta=0.0001,\n    cooldown=0,\n    min_lr=0\n)\n\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    min_delta=0,\n    patience=5,\n    verbose=1,\n    mode='auto',\n    baseline=None,\n    restore_best_weights=True\n)\n\nmodel7 = create_model7()\nmodel7_original = create_model7_original()\n\nckpt = tf.keras.callbacks.ModelCheckpoint(\n    \"create_model7.h5\", save_best_only=True, use_multiprocessing=True, workers=4, save_weights_only=True,\n)","f9f7966a":"history7_original = model7_original.fit(train_reduced, validation_data=val_reduced, epochs=20, callbacks=[ckpt,scheduler_cb,early_stopping_cb])","451761de":"#Model 7 Base Line SETI Simple Code for Beginners(TensorFlow): \n'''\nData Reduced: New Label\nEpoch 5\/20\n235\/235 [==============================] - 119s 506ms\/step - loss: 0.1987 - auc_1: 0.9772 - mae: 0.1388 - accuracy: 0.9140 - val_loss: 0.4910 - val_auc_1: 0.8824 - val_mae: 0.2274 - val_accuracy: 0.7977\nEpoch 6\/20\n235\/235 [==============================] - 119s 505ms\/step - loss: 0.1460 - auc_1: 0.9883 - mae: 0.1022 - accuracy: 0.9406 - val_loss: 0.6574 - val_auc_1: 0.8663 - val_mae: 0.2298 - val_accuracy: 0.7849\nEpoch 7\/20\n235\/235 [==============================] - 118s 504ms\/step - loss: 0.1084 - auc_1: 0.9935 - mae: 0.0756 - accuracy: 0.9600 - val_loss: 0.6538 - val_auc_1: 0.8736 - val_mae: 0.2174 - val_accuracy: 0.7939\nEpoch 8\/20\n235\/235 [==============================] - 120s 509ms\/step - loss: 0.0698 - auc_1: 0.9977 - mae: 0.0508 - accuracy: 0.9749 - val_loss: 0.6684 - val_auc_1: 0.8763 - val_mae: 0.2098 - val_accuracy: 0.8009\nEpoch 9\/20\n235\/235 [==============================] - 120s 509ms\/step - loss: 0.0517 - auc_1: 0.9987 - mae: 0.0379 - accuracy: 0.9867 - val_loss: 0.6982 - val_auc_1: 0.8794 - val_mae: 0.2098 - val_accuracy: 0.7961\nRestoring model weights from the end of the best epoch.\nEpoch 00009: early stopping\n'''\n\n'''\n\nEpoch 1\/20\n1255\/1255 [==============================] - 775s 603ms\/step - loss: 0.2767 - auc_1: 0.7383 - mae: 0.1621 - accuracy: 0.9081 - val_loss: 0.1418 - val_auc_1: 0.9262 - val_mae: 0.0608 - val_accuracy: 0.9599\nEpoch 2\/20\n1255\/1255 [==============================] - 773s 615ms\/step - loss: 0.1288 - auc_1: 0.9408 - mae: 0.0675 - accuracy: 0.9604 - val_loss: 0.1248 - val_auc_1: 0.9399 - val_mae: 0.0530 - val_accuracy: 0.9651\nEpoch 3\/20\n1255\/1255 [==============================] - 781s 622ms\/step - loss: 0.0895 - auc_1: 0.9691 - mae: 0.0469 - accuracy: 0.9738 - val_loss: 0.1269 - val_auc_1: 0.9370 - val_mae: 0.0470 - val_accuracy: 0.9665\nEpoch 4\/20\n1255\/1255 [==============================] - 773s 615ms\/step - loss: 0.0665 - auc_1: 0.9850 - mae: 0.0366 - accuracy: 0.9784 - val_loss: 0.1372 - val_auc_1: 0.9345 - val_mae: 0.0428 - val_accuracy: 0.9663\nEpoch 5\/20\n1255\/1255 [==============================] - 771s 614ms\/step - loss: 0.0405 - auc_1: 0.9941 - mae: 0.0240 - accuracy: 0.9861 - val_loss: 0.1538 - val_auc_1: 0.9233 - val_mae: 0.0421 - val_accuracy: 0.9667\nEpoch 6\/20\n1255\/1255 [==============================] - 776s 618ms\/step - loss: 0.0280 - auc_1: 0.9979 - mae: 0.0173 - accuracy: 0.9898 - val_loss: 0.1667 - val_auc_1: 0.9159 - val_mae: 0.0400 - val_accuracy: 0.9647\nEpoch 7\/20\n1255\/1255 [==============================] - 773s 616ms\/step - loss: 0.0172 - auc_1: 0.9985 - mae: 0.0111 - accuracy: 0.9945 - val_loss: 0.1853 - val_auc_1: 0.9081 - val_mae: 0.0378 - val_accuracy: 0.9659\nRestoring model weights from the end of the best epoch.\nEpoch 00007: early stopping\n'''\n\nplot_graphs(history7_original, 'loss')\nplot_graphs(history7_original, 'auc_1')\nplot_graphs(history7_original, 'mae')\nplot_graphs(history7_original, 'accuracy')","f1d3cff8":"y_pred = model.predict(\n    test, use_multiprocessing=True, workers=4, verbose=1\n)\n\nsub = pd.read_csv('..\/input\/seti-breakthrough-listen\/sample_submission.csv')\nsub['target'] = y_pred\nsub.to_csv('seti-breakthrough-listen_sample_submission_model_11_06.csv', index=False)\nsub['target'].hist()","c0c3c60a":"history7 = model7.fit(train_reduced, validation_data=val_reduced, epochs=20, callbacks=[ckpt,scheduler_cb,early_stopping_cb])","fdc34dd1":"plot_graphs(history7, 'loss')\nplot_graphs(history7, 'auc_1')\nplot_graphs(history7, 'mae')\nplot_graphs(history7, 'accuracy')","4ea6a2a2":"def create_model8():\n    input_size = (273, 256, 3)\n    model = tf.keras.Sequential([\n        L.Conv2D(3,(3,3), strides=(1,1), padding=\"same\", activation='relu', input_shape=input_size),\n        efn.EfficientNetB1(input_shape=input_size,weights='imagenet',include_top=False),\n    \n        L.TimeDistributed(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, return_sequences=True, recurrent_dropout=0.1)), name=\"bi_gru_1\"),\n        #L.Dropout(.2),\n        L.TimeDistributed(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, return_sequences=True, recurrent_dropout=0.1)), name=\"bi_gru_2\"),\n        #L.Dropout(.2),\n    \n        #L.TimeDistributed(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, return_sequences=True)), name=\"bi_gru_1\"),\n        #L.Dropout(.2),\n        \n        #L.TimeDistributed(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, return_sequences=True)), name=\"bi_gru_2\"),\n        #L.Dropout(.2),\n        \n        L.TimeDistributed(L.GlobalAveragePooling1D()),\n        L.Flatten(),\n        \n        L.Dense(512, activation=\"sigmoid\"),\n        L.Dropout(.2),\n        L.Dense(256, activation=\"sigmoid\"),\n        L.Dropout(.2),\n        L.Dense(128, activation=\"sigmoid\"),\n        L.Dropout(.1),\n        L.Dense(64, activation=\"sigmoid\"),\n        L.Dropout(.1),\n        L.Dense(1, activation=\"sigmoid\"),\n        ])\n\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=5e-4),\n                  loss='binary_crossentropy', metrics=[keras.metrics.AUC(), 'mae', 'accuracy'])\n    \n    return model\n\n\nmodelBiGRU.summary()","6ee65739":"model8 = create_model8()\n\nckpt = tf.keras.callbacks.ModelCheckpoint(\n    \"create_model8.h5\", save_best_only=True, use_multiprocessing=True, workers=4, save_weights_only=True,\n)\n\nhistory8 = model8.fit(train_reduced, validation_data=val_redcued, epochs=5, callbacks=[ckpt])","4ca2f45e":"plot_graphs(history8, 'loss')\nplot_graphs(history8, 'auc')\nplot_graphs(history8, 'accuracy')\nplot_graphs(history8, 'mae')\n\n# For modelBiGRU best epoch was between 6 and 7 or 14 - 15 -> (ok)\n# For modelBiGRU with 7 epochs dropout 0.2 included in GRU -> val_loss: 0.4577 - val_auc_2: 0.9130 (todo)\n# For modelBiGRU with 7 epochs dropout 0.2 not included in GRU ->  val_loss: 0.4853 - val_auc_2: 0.9147 (todo)\n# For modelBiGRU with 20 epochs dropout 0.2 with dropout between GRU layers -> val_loss: 0.3848 - val_auc_6: 0.9116 - val_mae: 0.1733 - val_accuracy: 0.8637\n# For modelBiGRU with 19 epochs dropout 0.2 with dropout between GRU layers -> val_loss: 0.3848 - val_auc_6: 0.9116 - val_mae: 0.1733 - val_accuracy: 0.8637\n# For modelBiGRU with 7 epochs dropout 0.2 with dropout between GRU layers -> val_loss: 0.4139 - val_auc_6: 0.8921 - val_mae: 0.2656 - val_accuracy: 0.8253\n# For modelBiGRU with 40 epochs dropout 0.2 with dropout between GRU layers -> Stoped in epoch 22: val_loss: 0.5288 - val_auc_10: 0.8153 - val_mae: 0.3297 - val_accuracy: 0.7609\n# For modelBiGRU with 20 epochs dropout 0.2 without dropout between GRU layers -> val_loss: 0.3120 - val_auc_7: 0.9250 - val_mae: 0.1711 - val_accuracy: 0.8845 *\n# For modelBiGRU with 18 epochs dropout 0.2 without dropout between GRU layers -> val_loss: 0.4750 - val_auc_9: 0.8621 - val_mae: 0.3139 - val_accuracy: 0.7902\n# For modelBiGRU with 20 epochs dropout 0.2 without dropout between GRU layers and without drop after Flatten -> val_loss: 0.3904 - val_auc_8: 0.9042 - val_mae: 0.2221 - val_accuracy: 0.8371\n# For modelBiGRU with 26 epochs dropout 0.2 without GRU recurrent_dropout -> val_loss: 0.3266 - val_auc: 0.9309 - val_mae: 0.1732 - val_accuracy: 0.8663\n# For modelBiGRU with 30 epochs dropout 0.2 without dropout between GRU layers and recurrent_dropout -> val_loss: 0.3781 - val_auc: 0.9121 - val_mae: 0.1751 - val_accuracy: 0.8578\n# For modelBiGRU with 50 epochs dropout 0.2 without dropout between GRU layers and recurrent_dropout -> val_loss: 0.3781 - val_auc: 0.9121 - val_mae: 0.1751 - val_accuracy: 0.8578","37097466":"y_pred = model8.predict(\n    test, use_multiprocessing=True, workers=4, verbose=1\n)\n\nsub = pd.read_csv('..\/input\/seti-breakthrough-listen\/sample_submission.csv')\nsub['target'] = y_pred\nsub.to_csv('submission_model_50_epochs_with_dropout_gru_and_recurrent_dropout.csv', index=False)\nsub['target'].hist()","aa379ea4":"sub = pd.read_csv('..\/input\/seti-breakthrough-listen\/sample_submission.csv')\nsub['target'] = y_pred\nsub.to_csv('submission_model_50_epochs_with_dropout_gru_and_recurrent_dropout.csv', index=False)\nsub['target'].hist()","2f8c507c":"# Completed Dataset","14ad52f3":"# Model 3 Performance in reduced dataset","a024e6d1":"# Model 2 Performance in reduced dataset","6710eb56":"# Model 4 Performance in reduced dataset","3da00972":"# Model 5 Performance in reduced dataset","1296fc37":"# Model 7 Original Performance in reduced dataset","ecfdb2fc":"# Model 7 Tuned Performance in reduced dataset","fa845f79":"# Reduced Dataset"}}