{"cell_type":{"b348f5b9":"code","e86b2df4":"code","1eb7dc12":"code","774cd7d5":"code","3ba25ce3":"code","3f3fa24e":"code","fb852919":"code","8eccdc62":"code","e3679b22":"code","7e54a7b1":"code","6376ca50":"code","db9bbc86":"code","818d68d0":"code","abecc84d":"code","c3a72324":"code","c6fb114f":"code","7d0552e5":"code","a378d831":"code","afc86677":"code","281d533e":"code","ab715410":"code","070beec7":"code","7b8990f1":"code","7ec26b50":"code","5e098804":"code","956e3019":"code","3cd8de8c":"code","9dc94035":"code","624a5561":"code","60f39a6d":"code","a73cfaf9":"code","b8b34761":"code","9b75f321":"code","d19c163f":"code","ef2264e8":"code","53fb5c31":"code","51028303":"code","81edabcc":"code","dcb3d763":"code","aed7e185":"code","ee638ba7":"code","88e0badf":"code","c9e485a1":"code","45165bf5":"code","6b35f61a":"code","a3f75f71":"code","be3269d7":"code","4dcce0eb":"code","97c7a737":"code","19e34010":"code","3db63a7b":"markdown","6413f61c":"markdown","a14e40bd":"markdown","c0d3ac43":"markdown","3b0f387e":"markdown","8f5db316":"markdown","16fbc73c":"markdown","ddb6c0ce":"markdown","72870961":"markdown","1b09f70e":"markdown","4e624a3b":"markdown","795b67cc":"markdown","06f67462":"markdown","7a0be8c7":"markdown","67586cae":"markdown","7500fbb4":"markdown","dbbe1e92":"markdown","0a94cee6":"markdown","3906c2df":"markdown","5f4c746d":"markdown","ee2af9ac":"markdown","c14db761":"markdown","143c260a":"markdown","aef2c0da":"markdown","0a7a1d31":"markdown","955d972b":"markdown","8ae45423":"markdown","32889796":"markdown","ca373187":"markdown","dc0dc445":"markdown","0acf29d6":"markdown","b85fd198":"markdown","811bca12":"markdown","6f2f64f4":"markdown","295492c6":"markdown","ec0d770f":"markdown","eb76ce84":"markdown","84906e1c":"markdown","4132ae45":"markdown","d8c71de8":"markdown","93145af9":"markdown","60c5c90a":"markdown","0d7b5d02":"markdown","1cc3bef0":"markdown","5b0b425e":"markdown","cc447f2b":"markdown","374acd46":"markdown","cf49d843":"markdown","83d7d8ee":"markdown","7a43334d":"markdown","44847225":"markdown","6dfc6826":"markdown","d3ac3f4d":"markdown","60393383":"markdown","7a6a9f57":"markdown","96a0921d":"markdown","1d0540cc":"markdown"},"source":{"b348f5b9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n'''for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))'''\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e86b2df4":"#! pip freeze","1eb7dc12":"#Other necessary imports\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nimport keras\nfrom keras.models import load_model\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')","774cd7d5":"#Function to Evaluate Final Model\ndef evaluate_final_model(model, x_test, y_test):\n    '''\n    Source for plotting confusion matrix:\n    https:\/\/stackoverflow.com\/questions\/69875073\/confusion-matrix-valueerror-classification-metrics-cant-handle-a-mix-of-binary\n    \n    Evaluate model by displaying confusion matrix of testing data, \n    and also by displaying a DataFrame with Loss, Accuracy, Recall, and AUC-ROC for testing data\n    \n    Input: \n    -final model to be evaluated\n    -x_test: numpy array of images from testing data\n    -y_test: numpy array of target values from testing data\n    \n    Output:\n    -Confusion matrix of y_test and model predictions\n    -DataFrame with Loss, Accuracy, Recall, and AUC-ROC for test dataset\n    '''\n    #Calculated Prediction\n    y_preds = model.predict(x_test)\n    y_preds = y_preds.flatten()\n    y_preds = np.where(y_preds > 0.5, 1, 0)\n    \n    #Plot Confusion Matrix of with Counts\n    cm_counts = confusion_matrix(y_test, y_preds)\n    sns.heatmap(cm_counts, annot=True, fmt = 'd', cmap = 'summer', xticklabels=['Predicted Benign', 'Predicted Malignant'],\n                yticklabels=['Is Benign', 'Is Malignant'])\n    plt.yticks(rotation=0)\n    plt.title(\"Confusion Matrix w\/ Counts\")\n    plt.show()\n    \n    #Plot Confusion Matrix normalized\n    cm_norm = confusion_matrix(y_test, y_preds, normalize = 'all')\n    sns.heatmap(cm_norm, annot=True, fmt = '.2%', cmap = 'summer', xticklabels=['Predicted Benign', 'Predicted Malignant'],\n                yticklabels=['Is Benign', 'Is Malignant'])\n    plt.yticks(rotation=0)\n    plt.title(\"Confusion Matrix w\/ Normalization\")\n    plt.show()\n    \n    #Plot Confusion Matrix based on True\n    cm_true = confusion_matrix(y_test, y_preds, normalize = 'true')\n    sns.heatmap(cm_true, annot=True, fmt = '.2%', cmap = 'summer', xticklabels=['Predicted Benign', 'Predicted Malignant'],\n                yticklabels=['Is Benign', 'Is Malignant'])\n    plt.yticks(rotation=0)\n    plt.title(\"Confusion Matrix based on True Label\")\n    plt.show()\n    \n    #Create a DataFrame with the results \n    model_test = model.evaluate(x_test, y_test, verbose=0)\n    model_results = {\"Test Dataset\":[]};\n    for i in range(4):\n        model_results['Test Dataset'].append(round(model_test[i],3))\n    model_results_df = pd.DataFrame.from_dict(data = model_results,\n                                                   orient = 'index', \n                                                   columns = ['Loss', 'Accuracy', 'Recall', 'AUC-ROC'])\n    display(model_results_df)","3ba25ce3":"#Find path for images and csv files\nroot_path = '..\/input\/jpeg-melanoma-512x512\/'\nprint(list(os.listdir(root_path)))","3f3fa24e":"#Read in training csv\ndf = pd.read_csv(root_path + 'train.csv')\ndf.head()","fb852919":"#Look at DataFrame information\ndf.info()","8eccdc62":"#Check the number of nulls in each column\ndf.isna().sum()","e3679b22":"#Check and see if any image names are duplicates\nprint(f\"Does Image Name have any duplicates?: {df['image_name'].nunique() != len(df['image_name'])}\")","7e54a7b1":"#Get diagnosis value counts\ndf['diagnosis'].value_counts()","6376ca50":"#Check if target only includes melanoma diagnosis\ndf[df['diagnosis']=='melanoma']['target'].value_counts()","db9bbc86":"df[df['diagnosis']!='melanoma']['target'].value_counts()","818d68d0":"#Add a column with the path to each images jpeg\ndf['path_jpeg'] = root_path + 'train\/' + df['image_name'] + '.jpg'\nprint(df['image_name'][0])\nprint(df['path_jpeg'][0])","abecc84d":"#Check target distribution\ndf['target'].value_counts()","c3a72324":"#Check target distribution normalized\ndf['target'].value_counts(normalize=True)","c6fb114f":"#Check benign_malignant distribution\ndf['benign_malignant'].value_counts()","7d0552e5":"#Double check that whenever target = 0, benign_malignant = 'benign', and vice versa\nprint(df[df['target']==0]['benign_malignant'].value_counts())\nprint(df[df['target']==1]['benign_malignant'].value_counts())","a378d831":"#Visualize Melanoma lesion distribution\nfig, ax = plt.subplots(figsize=(10,5))\nsns.countplot(data = df, x = 'benign_malignant')\nax.set_xlabel(xlabel = 'Diagnosis', fontsize = 20)\nax.set_ylabel(ylabel = 'Count', fontsize = 20)\nax.tick_params(axis='y', labelsize=15)\nax.tick_params(axis='x', labelsize=15)\nax.set_title(\"Melanoma Lesions in Original Dataset\", fontsize = 30)\nplt.show()","afc86677":"#Read in 2019 dataset\ndf_2019 = pd.read_csv(\"..\/input\/jpeg-isic2019-512x512\/train.csv\")\n\n#Create a column with path of image\ndf_2019['path_jpeg'] = \"..\/input\/jpeg-isic2019-512x512\/train\/\" + df_2019['image_name'] + \".jpg\"\nprint(df_2019['path_jpeg'][0]) #Sanity Check","281d533e":"#Check information from 2019 dataset\ndf_2019.info()","ab715410":"#Print out the target distribution\nprint(df_2019['target'].value_counts())\nprint(df_2019['benign_malignant'].value_counts())","070beec7":"#Check that only melanoma skin cancer is considered malignant\ndf_2019[df_2019['diagnosis']=='MEL']['benign_malignant'].value_counts()","7b8990f1":"#Create a new dataframe with only malignant images\ndf_2019_malignant = df_2019[df_2019['benign_malignant']=='malignant']\ndf_2019_malignant","7ec26b50":"#Add more malignant images that weren't used in 2020, 2019, 2018, or 2017 but were still on the ISIM website\ndf_more = pd.read_csv(\"..\/input\/malignant-v2-512x512\/train_malig_2.csv\")\n\n#Create a column with the path to each image\ndf_more['path_jpeg'] = \"..\/input\/malignant-v2-512x512\/jpeg512\/\" + df_more['image_name'] + \".jpg\"\nprint(df_more['path_jpeg'][0]) #Sanity Check","5e098804":"#View information of DataFrame\ndf_more.info()","956e3019":"#Check that target column and benign_malignant column have the same value counts\nprint(df_more['target'].value_counts())\nprint(df_more['benign_malignant'].value_counts())","3cd8de8c":"#Combine the 3 DataFrames into 1\ndf_combined = pd.concat([df, df_2019_malignant, df_more])\ndf_combined.info()","9dc94035":"#Check the value counts for benign or malignant images\ndf_combined['benign_malignant'].value_counts(normalize=True)","624a5561":"#Visualize the new distribution\nfig, ax = plt.subplots(figsize=(10,5))\nsns.countplot(data = df_combined, x = 'benign_malignant')\nax.set_xlabel(xlabel = 'Diagnosis', fontsize = 20)\nax.set_ylabel(ylabel = 'Count', fontsize = 20)\nax.tick_params(axis='y', labelsize=15)\nax.tick_params(axis='x', labelsize=15)\nax.set_title(\"Diagnosis in Combined Datasets\", fontsize = 30)\nplt.show()","60f39a6d":"#Visualize the 6 benign lesions in the Dataframe\nbenign_df = df_combined[df_combined['benign_malignant']=='benign']\n#I handpicked a couple images for illustration purposes\nbenign_images = benign_df['path_jpeg'].iloc[[0,1,4,6,8,9]] \nplt.figure(figsize=(15,10))\nfor i in range(6):\n    plt.suptitle('Examples of Benign Skin Lesions', fontsize = 16)\n    plt.subplot(2,3,i+1)\n    img = plt.imread(benign_images.iloc[i])\n    plt.imshow(img)","a73cfaf9":"#Visualize 6 malignant lesions in the DataFrame\nmalignant_df = df_combined[df_combined['benign_malignant']=='malignant']\n#I handpicked a couple images for illustration purposes\nmalignant_images = malignant_df['path_jpeg'].iloc[[8,12,13,15,16,18]] \nplt.figure(figsize=(15,10))\nfor i in range(6):\n    plt.suptitle('Examples of Malignant Skin Lesions', fontsize = 16)\n    plt.subplot(2,3,i+1)\n    img = plt.imread(malignant_images.iloc[i])\n    plt.imshow(img)","b8b34761":"#Make 2 Train test splits so that we are left with:\n#X_train -> 67.5% of data\n#X_val -> 22.5% of data\n#X_test -> 10% of data\nX = df_combined[['path_jpeg']]\ny = df_combined['benign_malignant']\nX_use, X_test, y_use, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\nX_train, X_val, y_train, y_val = train_test_split(X_use, y_use, test_size=0.25, random_state=42, stratify=y_use)","9b75f321":"#Combine the training data back together into 1 DataFrame\ntrain_df = pd.concat([X_train, y_train], axis=1)\ntrain_df","d19c163f":"#Combine the validation data back together into 1 DataFrame\nval_df = pd.concat([X_val, y_val], axis=1)\nval_df","ef2264e8":"#Combine the test data back together into 1 DataFrame\ntest_df = pd.concat([X_test, y_test], axis=1)\ntest_df","53fb5c31":"#Double check target distribution in train DataFrame\ntrain_df['benign_malignant'].value_counts(normalize=True)","51028303":"#Double check target distribution in train DataFrame\nval_df['benign_malignant'].value_counts(normalize=True)","81edabcc":"#Load processed image data data\ntrain_images = np.load('..\/input\/eda-and-data-preprocessing\/train_images.npy')\ntrain_labels = np.load('..\/input\/eda-and-data-preprocessing\/train_labels.npy')\n\n#A third of the training set \ntrain_images_third = np.load('..\/input\/eda-and-data-preprocessing\/train_images_third.npy')\ntrain_labels_third = np.load('..\/input\/eda-and-data-preprocessing\/train_labels_third.npy')\n\nval_images = np.load('..\/input\/eda-and-data-preprocessing\/val_images.npy')\nval_labels = np.load('..\/input\/eda-and-data-preprocessing\/val_labels.npy')\n\ntest_images = np.load('..\/input\/eda-and-data-preprocessing\/test_images.npy')\ntest_labels = np.load('..\/input\/eda-and-data-preprocessing\/test_labels.npy')","dcb3d763":"#Checking classes\nprint(\"Train data set classes:\",np.unique(train_labels))\nprint(\"Validation data set classes:\",np.unique(val_labels))\nprint(\"Test data set classes:\",np.unique(test_labels))","aed7e185":"#Explore the dataset again\nprint (\"Number of training samples: \" + str(train_images.shape[0]))\nprint (\"A third of training samples: \" + str(train_images_third.shape[0]))\nprint (\"Number of validation samples: \" + str(val_images.shape[0]))\nprint (\"Number of testing samples: \" + str(test_images.shape[0]))\nprint (\"===\")\nprint (\"train_images shape: \" + str(train_images.shape))\nprint (\"train_labels shape: \" + str(train_labels.shape))\nprint (\"A third of train_images shape: \" + str(train_images_third.shape))\nprint (\"A third of train_labels shape: \" + str(train_labels_third.shape))\nprint (\"val_images shape: \" + str(val_images.shape))\nprint (\"val_labels shape: \" + str(val_labels.shape))\nprint (\"test_images shape: \" + str(test_images.shape))\nprint (\"test_labels shape: \" + str(test_labels.shape))","ee638ba7":"#Baseline Understanding\n(unique, counts) = np.unique(val_labels, return_counts=True)\nfrequencies = np.asarray((unique, counts\/val_labels.shape[0])).T\nprint(frequencies)","88e0badf":"#Load baseline model\nbaseline_model = load_model('..\/input\/baseline-model\/best_baseline_model.h5')\n\n#View summary of baseline model\nbaseline_model.summary()","c9e485a1":"#Load first cnn model\nfirst_cnn_model = load_model('..\/input\/first-cnn-model\/best_cnn_model.h5')\n\n#View summary of CNN model\nfirst_cnn_model.summary()","45165bf5":"#Load second cnn model with dropouts\nsecond_cnn_model = load_model('..\/input\/second-cnn-model\/best_cnn2_model.h5')\n\n#View summary of previous CNN model but with Dropout layers\nsecond_cnn_model.summary()","6b35f61a":"#Load VGG16 model\nvgg16_model = load_model('..\/input\/vgg16-model\/best_vgg16_model.h5')\n\n#View summary of VGG16 model\nvgg16_model.summary()","a3f75f71":"#Load ResNet50 model\nresnet50_model = load_model('..\/input\/resnet50-model\/best_resnet50_model.h5')\n\n#View summary of ResNet50 model\nresnet50_model.summary()","be3269d7":"#Load InceptionResNet model\ninception_resnet_model = load_model('..\/input\/inceptionresnetv2-model\/best_inceptionResNet_model.h5')\n\n#View summary of InceptionResNetV2 Model\ninception_resnet_model.summary()","4dcce0eb":"#Instantiate Final Model\nfinal_model = vgg16_model\n\n#View summary of Final Model\nfinal_model.summary()","97c7a737":"#Evaluate Final Model on Test Set\nevaluate_final_model(final_model, test_images, test_labels)","19e34010":"#Save Final Model\nfinal_model.save(\"\/kaggle\/working\/final_model.h5\")","3db63a7b":"The target is heavily unbalanced. Let's add more malignant images from past competitions to help combat this.","6413f61c":"This model is almost identical to the previous but we also add some dropout layers after the 2 hidden convolutional layers. For memory purposes, I'm only importing the model from the notebook I created it in which can be found on [Github](https:\/\/github.com\/garrettwilliams90\/MelanomaClassification\/blob\/main\/Notebooks\/Model_Iterations\/second-cnn-model.ipynb) and [Kaggle](https:\/\/www.kaggle.com\/garrettwilliams90\/second-cnn-model).","a14e40bd":"The final model will be the VGG16 because it predicted the highest accuracy and recall score, while not overfitting too much to the training data. Again, recall is the second evaluation metric because a low score would mean our model is predicting a skin lesion is benign when it's actually malignant. This implies you're healthy when you actually aren't and need to seek medical assistance. Now, I'll take our VGG16 model and evaluate it on the testing set, which I held out at the beginning. This is so I can truly evaluate our model on unseen images.","c0d3ac43":"## 6th Iteration: InceptionResNetV2 Model","3b0f387e":"I ran 6 different models, fitting to the training set and evaluating on the validation set:\n- Simple Baseline Model\n- Convolutional Neural Network\n- Convolutional Neural Network with Dropout Layers\n- Pretrained VGG16\n- Pretrained ResNet50\n- Pretrained InceptionResNetV2\n\nThe key metrics I focused on were Accuracy, Recall, and AUC-ROC. I focused on Recall because having False Negatives are more costly than having False Positives. False Negatives is having a malignant skin lesion but predicting that it's benign. This would result in people thinking they're healthy when they aren't, which could result in lives being lost. False Positives on the other hand are benign lesions being classified as malignant. This would result in people seeing a Dermatologist when they don't need to. This could make it a lot harder for someone who needs to see a doctor to book an appointment. It could also lead to an unnecessary amount of money being spent to visit the Dermatologist when you don't need to.","8f5db316":"A pretrained network consists of layers that have already been trained on general data. For images, these layers have already learned general patterns, textures, colors, etc. such that when you feed in your training data, certain features can immediately be detected. VGG16 is a simple and widely used CNN, a great starting point for our pretrained models. I'm only importing the model from the notebook I created it in which can be found on [Github](https:\/\/github.com\/garrettwilliams90\/MelanomaClassification\/blob\/main\/Notebooks\/Model_Iterations\/vgg16-model.ipynb) and [Kaggle](https:\/\/www.kaggle.com\/garrettwilliams90\/vgg16-model).","16fbc73c":"# Data Preparation","ddb6c0ce":"# Data Understanding","72870961":"## 2nd Iteration: Convolutional Neural Network Model","1b09f70e":"For the second iteration, I used a Convolutional Neural Network (CNN) because they are widely considered the gold standard for image recognition. This model 3 hidden layers, 2 of which are convolutional. For now, I only loaded the model and displayed the evaluation visuals. You can view the entire process in notebook on [Github](https:\/\/github.com\/garrettwilliams90\/MelanomaClassification\/blob\/main\/Notebooks\/Model_Iterations\/first-cnn-model.ipynb) or [Kaggle](https:\/\/www.kaggle.com\/garrettwilliams90\/first-cnn-model). ","4e624a3b":"## 4th Iteration: VGG16 Model","795b67cc":"The model performed surprisingly well for such a simply model. There is a 7% difference in Recall scores but at least validation didn't have a 0.0 score. It doesn't seem to be overfitting and I think 93% accuracy is pretty good but there is no way to know without running a few more model iterations. ","06f67462":"## Baseline Understanding","7a0be8c7":"## 5th Iteration: ResNet50 Model","67586cae":"## Load More Unused Malignant Images","7500fbb4":"![First-CNN-Confusion-Matrix.png](attachment:58eb0b54-e5cd-47e8-8b96-6a8371e44e89.png)","dbbe1e92":"A function was defined at the top of each modeling iteration notebook to visualize the training and validation results. The source of inspiration for my code was [Lindsey Berlin](https:\/\/github.com\/flatiron-school\/DSLE-083021-Phase4-NN-Review\/blob\/main\/Phase4Review-NNs-Text-Images.ipynb).","0a94cee6":"In order to truely evaluate my model on unseen data, I performed 2 `train_test_split()`. This prevents data leakage. The first split resulted in the 10% of the data going to the testing set. I will hold this data until the very end, allowing me to evaluate the final model on unseen data. The second split resulted in 67.5% going to the training set, and 22.5% going to the validation set. These 2 sets is what I will use to evaluate all my model iterations and decide what model should be my final model.\n\nFor each dataset, I used the `ImageDataGenerator` class, imported from `Keras`, to generate batches of normalized tensor image data, rescaling the values so that they're between 0 and 1. With this class, I used the `flow_from_dataframe()` method to locate the images. Using the `target_size` parameter inside this method, I resized the images down to 128x128 for memory purposes.\n\nAnother consequence of having limited memory capactiy on Kaggle was that I was forced to divide the batch size of my training data by 3. The implications of this meant that my models were training on only 8,601 images, 7,321 were benign and 1,279 were malignant. This means over 17,000 weren't seen which may result in a bias toward this first batch of training images. For this current summary notebook, I loaded the processed data from my EDA-and-Data-Preprocessing notebook, found on [Github](https:\/\/github.com\/garrettwilliams90\/MelanomaClassification\/blob\/main\/Notebooks\/eda-and-data-preprocessing.ipynb) and [Kaggle](https:\/\/www.kaggle.com\/garrettwilliams90\/eda-and-data-preprocessing).  ","3906c2df":"## Train\/Test Split","5f4c746d":"The Australian Department of Health can develop and market a mobile app for the public that uses my model to classify if a person has melanoma or not. This would result in quicker reactions to seek out professionally-trained Dermatologists, which could save lives. ","ee2af9ac":"## 3rd Iteration: Convolutional Neural Network with Dropout Layers","c14db761":"Wow! The VGG16 model has given us our highest accuracy and recall score yet. I believe this is the best model yet but let's see if another pretrained model can beat it.","143c260a":"# Business Understanding","aef2c0da":"## Combine 3 DataFrames into 1","0a7a1d31":"Read in more jpg images that weren't from 2020,2019,2018, or 2017 <br> Source: https:\/\/www.kaggle.com\/cdeotte\/malignant-v2-1024x1024","955d972b":"![Second-CNN-Evaluation.png](attachment:3b3346d1-5b16-4723-a6ab-883efd11ea80.png)","8ae45423":"## Load Malignant Images From 2019 Competition","32889796":"Last but not least, I chose InceptionResNetV2 as the final model iteration. According to keras's [list](https:\/\/keras.io\/api\/applications\/) of pretrained neural networks, InceptionResNetV2 had one of the highest accuracy scores in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes. I loaded the model and results from another one of my notebooks, found on [GitHub](https:\/\/github.com\/garrettwilliams90\/MelanomaClassification\/blob\/main\/Notebooks\/Model_Iterations\/inceptionresnetv2-model.ipynb) and [Kaggle](https:\/\/www.kaggle.com\/garrettwilliams90\/inceptionresnetv2-model). ","ca373187":"![Baseline-Evaluation-Confusion-Matrix.png](attachment:ca5ddaba-5697-4b8d-a61a-e2eb625f6de8.png)","dc0dc445":"ResNet50 is another popular pretrained neural network. I'm loading the model from the notebook I created it in which can be found on [Github](https:\/\/github.com\/garrettwilliams90\/MelanomaClassification\/blob\/main\/Notebooks\/Model_Iterations\/resnet50-model.ipynb) and [Kaggle](https:\/\/www.kaggle.com\/garrettwilliams90\/resnet50-model).","0acf29d6":"## Image Processing","b85fd198":"## 1st Iteration: Simple Baseline Model","811bca12":"Well, that's upsetting. All of our scores decreased, especially recall which went below 50%. ","6f2f64f4":"# Future Work","295492c6":"Source for displaying value counts of numpy array: https:\/\/www.kite.com\/python\/answers\/how-to-count-frequency-of-unique-values-in-a-numpy-array-in-python","ec0d770f":"![First-CNN-DataFrame.png](attachment:14baaad6-81ed-4aaf-84c1-9d6bb81fb84f.png)","eb76ce84":"I started working on some future steps but didn't have time to include everything in this final summary notebook. To combat class imbalance, I trained a VGG16 model with `class_weight=\"balanced\"` in this [Kaggle notebook](https:\/\/www.kaggle.com\/garrettwilliams90\/vgg16-model-with-balanced-class-weight), also found on [GitHub](https:\/\/github.com\/garrettwilliams90\/MelanomaClassification\/blob\/main\/Notebooks\/Future_Work\/vgg16-model-with-balanced-class-weight.ipynb). I then evaluated that model in this [Kaggle notebook](https:\/\/www.kaggle.com\/garrettwilliams90\/evaluate-7-vgg16-with-balanced-class-weights) which can also be found on [GitHub](https:\/\/github.com\/garrettwilliams90\/MelanomaClassification\/blob\/main\/Notebooks\/Future_Work\/evaluate-7-vgg16-with-balanced-class-weights.ipynb). The recall increased on the validation set by about the same amount that the accuracy decreased. While this means more people with Melanoma are being correctly predicted to have melanoma, the downside is more people are being incorrectly predicted as having (more False Positives). This could result in making it a lot harder for someone who needs to see a doctor to book an appointment. This would require further investigating the consequences (i.e. maybe AB testing the app).\n\nI also did some preliminary work on what would happen if I resized the images to 64x64 instead of 128x128. You can look at my test notebook on [Kaggle](https:\/\/www.kaggle.com\/garrettwilliams90\/testing-vgg16-model-on-64pixel-images) or [Github](https:\/\/github.com\/garrettwilliams90\/MelanomaClassification\/blob\/main\/Notebooks\/Future_Work\/testing-vgg16-model-on-64pixel-images.ipynb). Even though I was able to use the whole training set (instead of dividing by 3), the validation scores weren't better than the final original VGG16 validation scores. This means that for this project, decreasing the image size from 128 pixels to 64 pixels isn't worth it.\n\nThere are some future steps that I haven't had time to get started on. With the ultimate goal of improving the model, I'd try image augmentation including removing hairs, increasing the model epochs, and increasing the patience to allow the model to run longer. \n\nLastly, I would be interested in modeling only on darker skin tones because theoretically, it would be harder to classify if someone has Melanoma or not.","84906e1c":"![ResNet50-Evaluation.png](attachment:d198c1f2-8270-4404-9deb-25be8ce9dae2.png)","4132ae45":"Source for calculating predictions for confusion matrix: <br>\nhttps:\/\/stackoverflow.com\/questions\/69875073\/confusion-matrix-valueerror-classification-metrics-cant-handle-a-mix-of-binary","d8c71de8":"Create a column for each jpeg path<br>\nSource: https:\/\/www.kaggle.com\/andradaolteanu\/siim-melanoma-competition-eda-augmentations#4.-Preprocess-.csv-files-%F0%9F%93%90","93145af9":"# Evaluation","60c5c90a":"![VGG16-Evaluation.png](attachment:b17f33a2-addf-4000-bc68-5d15aaff54ea.png)","0d7b5d02":"# Modeling","1cc3bef0":"I saved the data on my EDA-and-Data-Preprocessing notebook, found on [Github](https:\/\/github.com\/garrettwilliams90\/MelanomaClassification\/blob\/main\/Notebooks\/eda-and-data-preprocessing.ipynb) and [Kaggle](https:\/\/www.kaggle.com\/garrettwilliams90\/eda-and-data-preprocessing). Personally, I could only use a third of the training data because of memory limitations. I loaded the full processed training data as well for reference.","5b0b425e":"My model accurately predicts the diagnosis of a skin lesion 93.8% of the time and incorrectly labels the lesion as benign 16.3% of the time. As a reminder, my baseline understanding had an accuracy of 85.1% and a recall score of 0.","cc447f2b":"![Capstone-Baseline-Evaluation-Dataframe.png](attachment:7a4fd9e2-c8f6-44b1-91c7-3217f5dfbb57.png)","374acd46":"Our results are not too different than our baseline model and first CNN model, but it's not our best model.","cf49d843":"The data was created by the [Society for Imaging Informatics in Medicine (SIIM)](https:\/\/siim.org\/) and [International Skin Imaging Collaboration (ISIC)](https:\/\/www.isic-archive.com\/#!\/topWithHeader\/wideContentTop\/main). SIIM is the leading healthcare organization for informatics in medical imaging whose mission is to advance medical imaging informatics through education, research, and innovation in a multi-disciplinary community. ISIC is an international effort to improve melanoma diagnosis. The ISIC Archive contains the largest publicly available collection of quality-controlled dermoscopic images of skin lesions. I was able to use the data through the [SIIM-ISIC Melanoma Classification competition](https:\/\/www.kaggle.com\/c\/siim-isic-melanoma-classification\/overview) on Kaggle.\n\nThe following is the citation of the original dataset under CC BY-NC 4.0:\n\n> The ISIC 2020 Challenge Dataset https:\/\/doi.org\/10.34970\/2020-ds01 (c) by ISDIS, 2020\n> \n> Creative Commons Attribution-Non Commercial 4.0 International License.\n> \n> The dataset was generated by the International Skin Imaging Collaboration (ISIC) and images are from the following sources: Hospital Cl\u00ednic de Barcelona, Medical University of Vienna, Memorial Sloan Kettering Cancer Center, Melanoma Institute Australia, Sydney Melanoma Diagnostic Centre, University of Queensland, and the University of Athens Medical School.\n> \n> You should have received a copy of the license along with this work.\n> \n> If not, see https:\/\/creativecommons.org\/licenses\/by-nc\/4.0\/legalcode.txt.\n\nOne limitation of the dataset is the size of images. Since the images had different sizes and were too large for my code to run, I used resized images, found [here](https:\/\/www.kaggle.com\/cdeotte\/jpeg-melanoma-512x512). Another major limitation of this dataset is how imbalanced the target is. 98% of the over 33,000 images were classified as benign. To combat this imbalance, I added only the malignant images from the 2019 SIIM-ISIC Melanoma Classification competition, found [here](https:\/\/www.kaggle.com\/cdeotte\/jpeg-isic2019-512x512), and the malignant images that weren't used in the 2019 or 2020 competitions, found [here](https:\/\/www.kaggle.com\/cdeotte\/malignant-v2-512x512). Since the images came from the same source, [ISIC](https:\/\/challenge.isic-archive.com\/data\/), and there was no fundamental difference between the way they were taken, I'm not worried about creating a bias toward these newly added malignant images.\n\nThe final dataset after combining the 3 had over 38,000 images. As a baseline understanding. 85.1% of the validation images are benign and 14.9% of the validation images are malignant. This means that the model's accuracy would be 85% if it always predicted 'Benign'. All code was run using Kaggle and can be found [here](https:\/\/www.kaggle.com\/garrettwilliams90\/code).","83d7d8ee":"I used `keras` to create a neural network model with an input layer of 12 nodes and 2 hidden dense layers with 7 and 5 nodes respectively. You can visit my notebook on [Github](https:\/\/github.com\/garrettwilliams90\/MelanomaClassification\/blob\/main\/Notebooks\/Model_Iterations\/baseline-model.ipynb) or [Kaggle](https:\/\/www.kaggle.com\/garrettwilliams90\/baseline-model) to view the whole notebook. I only imported the model and final evaluations on the validation dataset.","7a43334d":"The results from this model are slightly worse than the baseline model. Maybe our model will improve if we add some dropout layers. At the very least it should eliminate the overfitting. ","44847225":"Even though the model is not overfit anymore and Recall improved by about 6%, Accuracy and AUC-ROC declined by close to 10%. Instead of spending too much time trying to figure how many layers, nodes, or features to add in order to improve all metrics, let's try some iterations on pretrained neural networks.","6dfc6826":"# Conclusion","d3ac3f4d":"## Visualize Images","60393383":"![InceptionResNetV2-Evaluation.png](attachment:57ea4324-23c2-4d67-a5d3-945dab14340c.png)","7a6a9f57":"85.1% of the validation images are benign and 14.9% of the validation images are malignant. This means that the models accuracy would be 85% if it always predicted 0. And if these were the case, the recall score would equal 0 because there are no True Positives. This is another reason why I focused on increasing Recall.","96a0921d":"Read in the 2019 data (includes images from 2018 and 2017 competitions) <br> Source: https:\/\/www.kaggle.com\/cdeotte\/jpeg-isic2019-512x512","1d0540cc":"Skin cancer is one of the most common types of cancer in the world. Melanoma, specifically, is responsible for 75% of skin cancer deaths, despite being the least common skin cancer. According to the [World Cancer Research Fund](https:\/\/www.wcrf.org\/dietandcancer\/skin-cancer-statistics\/), Australia has the highest rate of Melanoma per capita. [Cancer Australia](https:\/\/www.canceraustralia.gov.au\/cancer-types\/melanoma\/statistics) estimates close to 17,000 new cases of Melanoma have been diagnosed in 2021, resulting in over 1,000 deaths this year. \n\nUnlike other cancers though, skin cancer can be visibly seen. By using image classification tools, my work aims to accurately predict if a skin lesion is malignant or benign. By identifying if the skin has Melanoma early on, lives can be saved. I hope that you, the Australian Department of Health, would use your resources in conjunction with my model to develop and market an app for the benefit of your citizens. Using their mobile phone, they can easily take a picture and recognize whether they need to go to a dermatologist for further diagnosis."}}