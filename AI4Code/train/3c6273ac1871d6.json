{"cell_type":{"68da3f5d":"code","eafb029c":"code","2648c3d2":"code","713995b1":"code","4b699205":"code","2d87d4aa":"code","7f1b3828":"code","57b5a43b":"code","54b7932e":"code","ff7646fb":"code","40a0b962":"code","8994befc":"code","3d3657b8":"code","960984ab":"code","70455c0c":"code","3e78ca1e":"code","1d847ff6":"code","836732d2":"code","1d5441bb":"code","a8777615":"code","f835af66":"code","14e308ab":"code","72713bd9":"code","554ef50a":"code","0e584a44":"code","3543e84e":"code","a1cc5099":"code","d4bb7735":"code","ab796a84":"code","5b2d926f":"code","ab202239":"code","2ddfd1c0":"code","5834a7a4":"code","3c90f777":"code","51a3af25":"code","d6c75b1f":"code","f9756343":"code","1911be0c":"code","12a39190":"code","691f18bd":"code","4851e0a1":"code","3f4d3061":"code","46825250":"code","7e3e1175":"code","dd082873":"code","a65e8bc6":"code","dadd6909":"code","40c09f51":"code","131e241a":"code","077a03b8":"code","e6eb80ad":"code","3fe4fbe9":"code","d75475eb":"code","15af0106":"code","2e343865":"code","c16f2cb0":"code","5b3ac293":"code","99439d21":"code","29526110":"code","edfad972":"code","3a5a6fc0":"markdown","fb7362f6":"markdown","af845fe8":"markdown","ddf5c9ae":"markdown","850bc6d2":"markdown","946a51a3":"markdown","c0d20be4":"markdown","849670e6":"markdown","4bf5c2ac":"markdown","69dad48f":"markdown","f99c17e4":"markdown","e213636c":"markdown","be415770":"markdown","3a4379d4":"markdown","abec4d41":"markdown"},"source":{"68da3f5d":"!pip install git+git:\/\/github.com\/kvoyager\/GmdhPy.git","eafb029c":"from gmdhpy.gmdh import Regressor\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional, BatchNormalization, SimpleRNN, RepeatVector, TimeDistributed\nfrom keras.optimizers import SGD\nimport math\nfrom sklearn.metrics import mean_squared_error","2648c3d2":"from sklearn.metrics import mean_absolute_error, r2_score","713995b1":"# Some functions to help out with\ndef plot_predictions(test,predicted):\n    plt.plot(test, color='red',label='Real Coin Price')\n    plt.plot(predicted, color='blue',label='Predicted Coin Price')\n    plt.title('Coin Price Prediction')\n    plt.xlabel('Time')\n    plt.ylabel('Coin Price')\n    plt.legend()\n    plt.figure(figsize=(16,4))\n    plt.show()\n\ndef return_rmse(test,predicted):\n    rmse = math.sqrt(mean_squared_error(test, predicted))\n    print(\"The root mean squared error is {}.\".format(rmse))\n    print(\"The MSE is {}\".format(mean_squared_error(test, predicted)))\n    print(\"The MAE is {}\".format(mean_absolute_error(test, predicted)))\n    print(\"The R2_Score is {}\".format(r2_score(test, predicted)))\n    print(\"The MAPE is {}\".format(mean(MAPE(test, predicted))))","4b699205":"# First, we get the data\ndataset = pd.read_csv('..\/input\/crypto-data1\/coin_Litecoin.csv', index_col='Date', parse_dates=['Date'])\ndataset","2d87d4aa":"# Checking for missing values\ntraining_set = dataset[:'2020'].iloc[:,6:7].values\ntest_set = dataset['2021':].iloc[:,6:7].values\n\n# print(training_set)\n# print(test_set)","7f1b3828":"# We have chosen 'Close' attribute for prices. Let's see what it looks like\ndataset[\"Close\"]['2017':'2020'].plot(figsize=(16,4),legend=True)\ndataset[\"Close\"]['2021':].plot(figsize=(16,4),legend=True)\nplt.legend(['Training set (From 2017 till 2021)','Test set (2021 and beyond)'])\nplt.title('LTC coin')\nplt.show()","57b5a43b":"# Scaling the training set\nsc = MinMaxScaler(feature_range=(0,1))\ntraining_set_scaled = sc.fit_transform(training_set)\n# print(training_set_scaled.shape)\nprint(training_set.shape)\nprint(test_set.shape)","54b7932e":"window = 7","ff7646fb":"training_set.shape[0]","40a0b962":"X_train = []\ny_train = []\nfor i in range(window,training_set.shape[0]):\n    X_train.append(training_set[i-window:i,0])\n    y_train.append(training_set[i,0])\nX_train, y_train = np.array(X_train), np.array(y_train)\nprint(X_train.shape)\nprint(y_train.shape)","8994befc":"gmdh = Regressor(ref_functions=('cubic','quadratic','linear','linear_cov'),\n                      criterion_type='validate',\n                      criterion_minimum_width=7,\n                      stop_train_epsilon_condition=0.0001,\n                      layer_err_criterion='top',\n                      l2=0.5,\n                      verbose=1,\n                      n_jobs='max')","3d3657b8":"# Now to get the test set ready in a similar way as the training set.\n# The following has been done so first 60 entires of test set have 60 previous values which is impossible to get unless we take the whole \n# 'High' attribute data for processing\ndataset_total = pd.concat((dataset[\"Close\"]['2017':'2020'],dataset[\"Close\"]['2021':]),axis=0)\ninputs = dataset_total[len(dataset_total)-len(test_set) - window:].values\n# print(inputs.shape)\ninputs = inputs.reshape(-1,1)\nprint(inputs.shape)\n\n# print(inputs.shape)\n# inputs  = sc.transform(inputs)\n# print(inputs.shape)\nprint(test_set.shape)","960984ab":"# Preparing X_test and predicting the prices\nX_test = []\ny_test = []\nfor i in range(window,inputs.shape[0]):\n    X_test.append(inputs[i-window:i,0])\n    y_test.append(inputs[i,0])\nX_test, y_test = np.array(X_test), np.array(y_test)\nprint(X_test.shape)\nprint(y_test.shape)\n# X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n","70455c0c":"gmdh.fit(X_train,y_train)","3e78ca1e":"predicted_stock_price = gmdh.predict(X_test)\nprint(predicted_stock_price.shape)\n# predicted_stock_price = regressor.predict(X_test)\n# predicted_stock_price.reshape(-1,1)\n# predicted_stock_price = sc.inverse_transform(predicted_stock_price)","1d847ff6":"plot_predictions(test_set,predicted_stock_price)\nreturn_rmse(y_test,predicted_stock_price)","836732d2":"y_test[:10], predicted_stock_price[:10]","1d5441bb":"from keras.callbacks import *\nfrom keras.losses import MAPE\nfrom numpy import mean","a8777615":"from sklearn.metrics import r2_score\ny_test, predicted_stock_price = list(y_test),list(predicted_stock_price)\n# gmdh.score(y_test, predicted_stock_price)\nr2_score(y_test, predicted_stock_price)","f835af66":"k=133\nprint(y_test[k],predicted_stock_price[k])\n","14e308ab":"windowLSTM = 30","72713bd9":"# Since LSTMs store long term memory state, we create a data structure with 60 timesteps and 1 output\n# So for each element of training set, we have 60 previous training set elements \nX_train = []\ny_train = []\nfor i in range(windowLSTM,2804):\n    X_train.append(training_set_scaled[i-windowLSTM:i,0])\n    y_train.append(training_set_scaled[i,0])\nX_train, y_train = np.array(X_train), np.array(y_train)","554ef50a":"X_train.shape, y_train.shape","0e584a44":"# Reshaping X_train for efficient modelling\nX_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))\nX_train.shape","3543e84e":"# The LSTM architecture\nregressor = Sequential()\n# First LSTM layer with Dropout regularisation\nregressor.add(LSTM(units=30, return_sequences=True, input_shape=(X_train.shape[1],1)))\nregressor.add(Dropout(0.1))\n# Second LSTM layer\nregressor.add(LSTM(units=30, return_sequences=True))\nregressor.add(Dropout(0.1))\n# Third LSTM layer\nregressor.add(LSTM(units=30, return_sequences=True))\nregressor.add(Dropout(0.1))\n# Fourth LSTM layer\nregressor.add(LSTM(units=30))\nregressor.add(Dropout(0.1))\n# The output layer\nregressor.add(Dense(units=1))\n\n# Compiling the RNN\nregressor.compile(optimizer='rmsprop',loss='mean_squared_error')\n# Fitting to the training set\ncsv_logger = CSVLogger('trainingLSTM.log')\nregressor.fit(X_train,y_train,epochs=10,batch_size=8,callbacks=[csv_logger])","a1cc5099":"# Now to get the test set ready in a similar way as the training set.\n# The following has been done so forst 60 entires of test set have 60 previous values which is impossible to get unless we take the whole \n# 'High' attribute data for processing\ndataset_total = pd.concat((dataset[\"Close\"][:'2020'],dataset[\"Close\"]['2021':]),axis=0)\ninputs = dataset_total[len(dataset_total)-len(test_set) - windowLSTM:].values\ninputs = inputs.reshape(-1,1)\ninputs  = sc.transform(inputs)","d4bb7735":"# Preparing X_test and predicting the prices\nX_test = []\nfor i in range(windowLSTM,187+windowLSTM):\n    X_test.append(inputs[i-windowLSTM:i,0])\nX_test = np.array(X_test)\nX_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n\n\npredicted_stock_price = regressor.predict(X_test)\npredicted_stock_price = sc.inverse_transform(predicted_stock_price)\n\n# Visualizing the results for LSTM\nplot_predictions(test_set,predicted_stock_price)\n\n# Evaluating our model\nreturn_rmse(test_set,predicted_stock_price)","ab796a84":"from numpy import mean\nmean(MAPE(test_set, predicted_stock_price))","5b2d926f":"windowGRU = 30","ab202239":"# Since LSTMs store long term memory state, we create a data structure with 60 timesteps and 1 output\n# So for each element of training set, we have 60 previous training set elements \nX_train = []\ny_train = []\nfor i in range(windowGRU,2804):\n    X_train.append(training_set_scaled[i-windowGRU:i,0])\n    y_train.append(training_set_scaled[i,0])\nX_train, y_train = np.array(X_train), np.array(y_train)","2ddfd1c0":"X_train.shape, y_train.shape","5834a7a4":"# Reshaping X_train for efficient modelling\nX_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))\nX_train.shape","3c90f777":"# The GRU architecture\nregressorGRU = Sequential()\n# First GRU layer with Dropout regularisation\nregressorGRU.add(GRU(units=120, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\nregressorGRU.add(Dropout(0.2))\n# Second GRU layer\n# regressorGRU.add(GRU(units=120, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\n# regressorGRU.add(Dropout(0.2))\n# Third GRU layer\n# regressorGRU.add(GRU(units=120, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\n# regressorGRU.add(Dropout(0.2))\n# Fourth GRU layer\nregressorGRU.add(GRU(units=120, activation='tanh'))\nregressorGRU.add(Dropout(0.2))\n# The output layer\nregressorGRU.add(Dense(units=1))\n# Compiling the RNN\nregressorGRU.compile(optimizer=SGD(lr=0.01, decay=1e-7, momentum=0.9, nesterov=False),loss='mean_squared_error',metrics=['accuracy'])\n# Fitting to the training set\ncsv_logger = CSVLogger('trainingGRU.log')\nregressorGRU.fit(X_train,y_train,epochs=10,batch_size=16,callbacks=[csv_logger])\n","51a3af25":"# Now to get the test set ready in a similar way as the training set.\n# The following has been done so forst 60 entires of test set have 60 previous values which is impossible to get unless we take the whole \n# 'High' attribute data for processing\ndataset_total = pd.concat((dataset[\"Close\"][:'2020'],dataset[\"Close\"]['2021':]),axis=0)\ninputs = dataset_total[len(dataset_total)-len(test_set) - windowGRU:].values\ninputs = inputs.reshape(-1,1)\ninputs  = sc.transform(inputs)","d6c75b1f":"# Preparing X_test and predicting the prices\nX_test = []\nfor i in range(windowGRU,187+windowGRU):\n    X_test.append(inputs[i-windowGRU:i,0])\n    \nprint(len(X_test), windowGRU)\nX_test = np.array(X_test)\nX_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\nGRU_predicted_stock_price = regressorGRU.predict(X_test)\nGRU_predicted_stock_price = sc.inverse_transform(GRU_predicted_stock_price)","f9756343":"# Visualizing the results for GRU\nplot_predictions(test_set,GRU_predicted_stock_price)\n\n# Evaluating GRU\nreturn_rmse(test_set,GRU_predicted_stock_price)","1911be0c":"windowRNN = 30","12a39190":"# Since LSTMs store long term memory state, we create a data structure with 60 timesteps and 1 output\n# So for each element of training set, we have 60 previous training set elements \nX_train = []\ny_train = []\nfor i in range(windowRNN,2804):\n    X_train.append(training_set_scaled[i-windowRNN:i,0])\n    y_train.append(training_set_scaled[i,0])\nX_train, y_train = np.array(X_train), np.array(y_train)","691f18bd":"X_train.shape, y_train.shape","4851e0a1":"# Reshaping X_train for efficient modelling\nX_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))\nX_train.shape","3f4d3061":"# Now to get the test set ready in a similar way as the training set.\n# The following has been done so forst 60 entires of test set have 60 previous values which is impossible to get unless we take the whole \n# 'High' attribute data for processing\ndataset_total = pd.concat((dataset[\"Close\"][:'2020'],dataset[\"Close\"]['2021':]),axis=0)\ninputs = dataset_total[len(dataset_total)-len(test_set) - windowRNN:].values\ninputs = inputs.reshape(-1,1)\ninputs  = sc.transform(inputs)\n\n# Preparing X_test and predicting the prices\nX_test = []\nfor i in range(windowRNN,187+windowRNN):\n    X_test.append(inputs[i-windowRNN:i,0])\nX_test = np.array(X_test)\nX_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n","46825250":"# The LSTM architecture\nregressorRNN = Sequential()\n# First LSTM layer with Dropout regularisation\nregressorRNN.add(SimpleRNN(units=30, return_sequences=True, input_shape=(X_train.shape[1],1)))\nregressorRNN.add(Dropout(0.1))\n# Second LSTM layer\nregressorRNN.add(SimpleRNN(units=30, return_sequences=True))\nregressorRNN.add(Dropout(0.1))\n# Third LSTM layer\nregressorRNN.add(SimpleRNN(units=30, return_sequences=True))\nregressorRNN.add(Dropout(0.1))\n# Fourth LSTM layer\nregressorRNN.add(SimpleRNN(units=30))\nregressorRNN.add(Dropout(0.1))\n# The output layer\nregressorRNN.add(Dense(units=1))\n\n# Compiling the RNN\nregressorRNN.compile(optimizer='rmsprop',loss='mean_squared_error', metrics=['accuracy'])\n# Fitting to the training set\ncsv_logger = CSVLogger('trainingRNN.log')\nregressorRNN.fit(X_train,y_train,epochs=10,batch_size=32,callbacks=[csv_logger])\n","7e3e1175":"predicted_stock_price = regressorRNN.predict(X_test)\npredicted_stock_price = sc.inverse_transform(predicted_stock_price)\n\n# Visualizing the results for LSTM\nplot_predictions(test_set,predicted_stock_price)\n\n# Evaluating our model\nreturn_rmse(test_set,predicted_stock_price)","dd082873":"windowBiLSTM = 60","a65e8bc6":"# Since LSTMs store long term memory state, we create a data structure with 60 timesteps and 1 output\n# So for each element of training set, we have 60 previous training set elements \nX_train = []\ny_train = []\nfor i in range(windowBiLSTM,2804):\n    X_train.append(training_set_scaled[i-windowBiLSTM:i,0])\n    y_train.append(training_set_scaled[i,0])\nX_train, y_train = np.array(X_train), np.array(y_train)\n\n# Reshaping X_train for efficient modelling\nX_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))\n\n# Now to get the test set ready in a similar way as the training set.\n# The following has been done so forst 60 entires of test set have 60 previous values which is impossible to get unless we take the whole \n# 'High' attribute data for processing\ndataset_total = pd.concat((dataset[\"Close\"][:'2020'],dataset[\"Close\"]['2021':]),axis=0)\ninputs = dataset_total[len(dataset_total)-len(test_set) - windowBiLSTM:].values\ninputs = inputs.reshape(-1,1)\ninputs  = sc.transform(inputs)\n\n# Preparing X_test and predicting the prices\nX_test = []\nfor i in range(windowBiLSTM,187+windowBiLSTM):\n    X_test.append(inputs[i-windowBiLSTM:i,0])\nX_test = np.array(X_test)\nX_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))","dadd6909":"# The LSTM architecture\nBi_regressor = Sequential()\n# First LSTM layer with Dropout regularisation\nBi_regressor.add(Bidirectional(LSTM(units=150, return_sequences=True), input_shape=(X_train.shape[1],1)))\nBi_regressor.add(Dropout(0.3))\n# Second LSTM layer\nBi_regressor.add(Bidirectional(LSTM(units=150, return_sequences=True)))\nBi_regressor.add(Dropout(0.3))\n# Third LSTM layer\n# Bi_regressor.add(Bidirectional(LSTM(units=100, return_sequences=True)))\n# Bi_regressor.add(Dropout(0.5))\n# Fourth LSTM layer\nBi_regressor.add(LSTM(units=150))\nBi_regressor.add(Dropout(0.3))\n# The output layer\nBi_regressor.add(Dense(units=1))\n\n# Compiling the RNN\nBi_regressor.compile(optimizer='adam',loss='mean_squared_error', metrics=['accuracy'])\n# Fitting to the training set\n\ncsv_logger = CSVLogger('trainingBiLSTM.log')\nBi_regressor.fit(X_train,y_train,epochs=10,batch_size=32,callbacks=[csv_logger])\n","40c09f51":"predicted_stock_price = Bi_regressor.predict(X_test)\npredicted_stock_price = sc.inverse_transform(predicted_stock_price)\n\n# Visualizing the results for LSTM\nplot_predictions(test_set,predicted_stock_price)\n\n# Evaluating our model\nreturn_rmse(test_set,predicted_stock_price)","131e241a":"windowBiGRU = 30","077a03b8":"# Since LSTMs store long term memory state, we create a data structure with 60 timesteps and 1 output\n# So for each element of training set, we have 60 previous training set elements \nX_train = []\ny_train = []\nfor i in range(windowBiGRU,2804):\n    X_train.append(training_set_scaled[i-windowBiGRU:i,0])\n    y_train.append(training_set_scaled[i,0])\nX_train, y_train = np.array(X_train), np.array(y_train)\n\n# Reshaping X_train for efficient modelling\nX_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))\n\n# Now to get the test set ready in a similar way as the training set.\n# The following has been done so forst 60 entires of test set have 60 previous values which is impossible to get unless we take the whole \n# 'High' attribute data for processing\ndataset_total = pd.concat((dataset[\"Close\"][:'2020'],dataset[\"Close\"]['2021':]),axis=0)\ninputs = dataset_total[len(dataset_total)-len(test_set) - windowBiGRU:].values\ninputs = inputs.reshape(-1,1)\ninputs  = sc.transform(inputs)\n\n# Preparing X_test and predicting the prices\nX_test = []\nfor i in range(windowBiGRU,187+windowBiGRU):\n    X_test.append(inputs[i-windowBiGRU:i,0])\nX_test = np.array(X_test)\nX_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))","e6eb80ad":"# The LSTM architecture\nBi_regressorGRU = Sequential()\n# First LSTM layer with Dropout regularisation\nBi_regressorGRU.add(Bidirectional(GRU(units=100, return_sequences=True), input_shape=(X_train.shape[1],1)))\nBi_regressorGRU.add(Dropout(0.4))\n# Second LSTM layer\nBi_regressorGRU.add(Bidirectional(GRU(units=100, return_sequences=True)))\nBi_regressorGRU.add(Dropout(0.4))\n# Third LSTM layer\nBi_regressorGRU.add(Bidirectional(GRU(units=100, return_sequences=True)))\nBi_regressorGRU.add(Dropout(0.4))\n# Fourth LSTM layer\nBi_regressorGRU.add(GRU(units=100))\nBi_regressorGRU.add(Dropout(0.4))\n# The output layer\nBi_regressorGRU.add(Dense(units=1))\n\n# Compiling the RNN\nBi_regressorGRU.compile(optimizer='rmsprop',loss='mean_squared_error', metrics=['accuracy'])\n# Fitting to the training set\ncsv_logger = CSVLogger('trainingBiGRU.log')\nBi_regressorGRU.fit(X_train,y_train,epochs=10,batch_size=16,callbacks=[csv_logger])","3fe4fbe9":"predicted_stock_price = Bi_regressorGRU.predict(X_test)\npredicted_stock_price = sc.inverse_transform(predicted_stock_price)\n\n# Visualizing the results for LSTM\nplot_predictions(test_set,predicted_stock_price)\n\n# Evaluating our model\nreturn_rmse(test_set,predicted_stock_price)","d75475eb":"from tensorflow.keras.layers import Layer\nfrom tensorflow.keras import backend as K\n\nclass Attention(Layer):\n    \n    def __init__(self, return_sequences=True):\n        self.return_sequences = return_sequences\n        super(Attention,self).__init__()\n        \n    def build(self, input_shape):\n        \n        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1),\n                               initializer=\"normal\")\n        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1),\n                               initializer=\"zeros\")\n        \n        super(Attention,self).build(input_shape)\n        \n    def call(self, x):\n        \n        e = K.tanh(K.dot(x,self.W)+self.b)\n        a = K.softmax(e, axis=1)\n        output = x*a\n        \n        if self.return_sequences:\n            return output\n        \n        return K.sum(output, axis=1)","15af0106":"windowBiGRU = 30","2e343865":"# Since LSTMs store long term memory state, we create a data structure with 60 timesteps and 1 output\n# So for each element of training set, we have 60 previous training set elements \nX_train = []\ny_train = []\nfor i in range(windowBiGRU,2804):\n    X_train.append(training_set_scaled[i-windowBiGRU:i,0])\n    y_train.append(training_set_scaled[i,0])\nX_train, y_train = np.array(X_train), np.array(y_train)\n\n# Reshaping X_train for efficient modelling\nX_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))\n\n# Now to get the test set ready in a similar way as the training set.\n# The following has been done so forst 60 entires of test set have 60 previous values which is impossible to get unless we take the whole \n# 'High' attribute data for processing\ndataset_total = pd.concat((dataset[\"Close\"][:'2020'],dataset[\"Close\"]['2021':]),axis=0)\ninputs = dataset_total[len(dataset_total)-len(test_set) - windowBiGRU:].values\ninputs = inputs.reshape(-1,1)\ninputs  = sc.transform(inputs)\n\n# Preparing X_test and predicting the prices\nX_test = []\nfor i in range(windowBiGRU,187+windowBiGRU):\n    X_test.append(inputs[i-windowBiGRU:i,0])\nX_test = np.array(X_test)\nX_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))","c16f2cb0":"# The LSTM architecture\nBi_regressorAtt = Sequential()\n# First LSTM layer with Dropout regularisation\nBi_regressorAtt.add(Bidirectional(LSTM(units=50, return_sequences=True), input_shape=(X_train.shape[1],1)))\nBi_regressor.add(Dropout(0.2))\n# Second LSTM layer\n# Bi_regressorAtt.add(Bidirectional(LSTM(units=100, return_sequences=True)))\n# Bi_regressorAtt.add(Dropout(0.1))\n\nBi_regressorAtt.add(Attention(return_sequences=True))\n\n# Third LSTM layer\n# Bi_regressorAtt.add(Bidirectional(LSTM(units=32, return_sequences=True)))\n# Bi_regressorAtt.add(Dropout(0.2))\n# Fourth LSTM layer\n\n\nBi_regressorAtt.add(LSTM(units=50))\nBi_regressorAtt.add(Dropout(0.2))\n# The output layer\nBi_regressorAtt.add(Dense(units=1))\n\n# Compiling the RNN\nBi_regressorAtt.compile(optimizer='adam',loss='mean_squared_error', metrics=['accuracy'])\n# Fitting to the training set\ncsv_logger = CSVLogger('trainingAtt.log')\nBi_regressorAtt.fit(X_train,y_train,epochs=10,batch_size=16,callbacks=[csv_logger])","5b3ac293":"predicted_stock_price = Bi_regressorAtt.predict(X_test)\npredicted_stock_price = sc.inverse_transform(predicted_stock_price)\n\n# Visualizing the results for LSTM\nplot_predictions(test_set,predicted_stock_price)\n\n# Evaluating our model\nreturn_rmse(test_set,predicted_stock_price)","99439d21":"# Preparing sequence data\ninitial_sequence = X_train[2743,:]\nsequence = []\nfor i in range(187):\n    new_prediction = regressorGRU.predict(initial_sequence.reshape(initial_sequence.shape[1],initial_sequence.shape[0],1))\n    initial_sequence = initial_sequence[1:]\n    initial_sequence = np.append(initial_sequence,new_prediction,axis=0)\n    sequence.append(new_prediction)\nsequence = sc.inverse_transform(np.array(sequence).reshape(187,1))","29526110":"# Visualizing the sequence\nplot_predictions(test_set,sequence)","edfad972":"# Evaluating the sequence\nreturn_rmse(test_set,sequence)","3a5a6fc0":"# ***START OF Bi-LSTM***","fb7362f6":"The current version version uses a dense GRU network with 100 units as opposed to the GRU network with 50 units in previous version","af845fe8":"#### I was going to cover text generation using LSTM but already an excellent kernel by [Shivam Bansal](https:\/\/www.kaggle.com\/shivamb) on the mentioned topic exists. Link for that kernel here: https:\/\/www.kaggle.com\/shivamb\/beginners-guide-to-text-generation-using-lstms","ddf5c9ae":"# ***START LSTM***","850bc6d2":"#### This is certainly not the end. Stay tuned for more stuff!","946a51a3":"# ***START OF SIMPLERNN***","c0d20be4":"# ***END OF GRU***","849670e6":"Truth be told. That's one awesome score. \n\nLSTM is not the only kind of unit that has taken the world of Deep Learning by a storm. We have **Gated Recurrent Units(GRU)**. It's not known, which is better: GRU or LSTM becuase they have comparable performances. GRUs are easier to train than LSTMs.\n\n## Gated Recurrent Units\nIn simple words, the GRU unit does not have to use a memory unit to control the flow of information like the LSTM unit. It can directly makes use of the all hidden states without any control. GRUs have fewer parameters and thus may train a bit faster or need less data to generalize. But, with large data, the LSTMs with higher expressiveness may lead to better results.\n\nThey are almost similar to LSTMs except that they have two gates: reset gate and update gate. Reset gate determines how to combine new input to previous memory and update gate determines how much of the previous state to keep. Update gate in GRU is what input gate and forget gate were in LSTM. We don't have the second non linearity in GRU before calculating the outpu, .neither they have the output gate.\n\nSource: [Quora](https:\/\/www.quora.com\/Whats-the-difference-between-LSTM-and-GRU-Why-are-GRU-efficient-to-train)\n\n<img src=\"https:\/\/cdnpythonmachinelearning.azureedge.net\/wp-content\/uploads\/2017\/11\/GRU.png?x31195\">","4bf5c2ac":"# **ATTENTION LAYER**","69dad48f":"## Sequence Generation\nHere, I will generate a sequence using just initial 60 values instead of using last 60 values for every new prediction. **Due to doubts in various comments about predictions making use of test set values, I have decided to include sequence generation.** The above models make use of test set so it is using last 60 true values for predicting the new value(I will call it a benchmark). This is why the error is so low. Strong models can bring similar results like above models for sequences too but they require more than just data which has previous values. In case of stocks, we need to know the sentiments of the market, the movement of other stocks and a lot more. So, don't expect a remotely accurate plot. The error will be great and the best I can do is generate the trend similar to the test set.\n\nI will use GRU model for predictions. You can try this using LSTMs also. I have modified GRU model above to get the best sequence possible. I have run the model four times and two times I got error of around 8 to 9. The worst case had an error of around 11. Let's see what this iterations.\n\nThe GRU model in the previous versions is fine too. Just a little tweaking was required to get good sequences. **The main goal of this kernel is to show how to build RNN models. How you predict data and what kind of data you predict is up to you. I can't give you some 100 lines of code where you put the destination of training and test set and get world-class results. That's something you have to do yourself.**","f99c17e4":"# ***BI GRU***","e213636c":"# ***END OF GMDH***","be415770":"# ***GRU MODEL***","3a4379d4":"# ***END OF LSTM***","abec4d41":"So, GRU works better than LSTM in this case. Bidirectional LSTM is also a good way so make the model stronger. But this may vary for different data sets. **Applying both LSTM and GRU together gave even better results.** "}}