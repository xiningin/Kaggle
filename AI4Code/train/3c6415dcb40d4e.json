{"cell_type":{"9be5eca9":"code","4724a488":"code","123c711c":"code","01160bfe":"code","cdf4535a":"code","90b2c767":"code","1dfb1c65":"code","406a0053":"code","0a77c4fb":"code","1f4d1981":"code","601f84d5":"code","20c82f4f":"code","f778f6f1":"code","dfa207ca":"code","d3ce926a":"code","2df5fe2e":"code","7d906e53":"code","154e4c80":"code","267660aa":"code","6f03eb6d":"code","f204e1b4":"code","c01c7f33":"code","596fcd51":"code","b8220d4c":"code","be1e2a39":"markdown","b5f215c5":"markdown"},"source":{"9be5eca9":"import numpy as np\nimport pandas as pd\nimport os\nimport random\nimport matplotlib.pyplot as plt\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import text\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense, LSTM, Dropout, Bidirectional, Conv1D, MaxPooling1D\nfrom keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\nfrom scipy.stats import entropy","4724a488":"random.seed(42)","123c711c":"df_train=pd.read_csv('\/kaggle\/input\/dbpedia-classes\/DBPEDIA_train.csv').sample(5000, random_state=42)\ndf_val=pd.read_csv('\/kaggle\/input\/dbpedia-classes\/DBPEDIA_val.csv').sample(5000, random_state=42)","01160bfe":"df_train[\"l1\"].unique()","cdf4535a":"tokenizer=Tokenizer(oov_token=\"'oov'\")\ntokenizer.fit_on_texts(df_train['text'])","90b2c767":"maxlen = 200\ntrain_X = pad_sequences(tokenizer.texts_to_sequences(df_train['text']), maxlen=maxlen)\nval_X = pad_sequences(tokenizer.texts_to_sequences(df_val['text']), maxlen=maxlen)","1dfb1c65":"enc = LabelEncoder()\nenc.fit(df_train[\"l1\"])\ntrain_Y = to_categorical(enc.transform(df_train[\"l1\"]))\nval_Y = to_categorical(enc.transform(df_val[\"l1\"]))","406a0053":"glove_dir=\"\/kaggle\/input\/glove-global-vectors-for-word-representation\/\"\n\nembedding_index = {}\nf = open(os.path.join(glove_dir,'glove.6B.100d.txt'),encoding='utf8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:],dtype='float32')\n    embedding_index[word] = coefs\nf.close()\nprint('Found %s word vectors ' % len(embedding_index))","0a77c4fb":"max_words = len(tokenizer.word_index) + 1\nembedding_dim = 100\nembedding_matrix = np.zeros((max_words,embedding_dim))\n\nfor word, idx in tokenizer.word_index.items():\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[idx]=embedding_vector","1f4d1981":"def train_model(X, Y, pool):\n    model=Sequential()\n    model.add(Embedding(max_words, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=False))\n    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(Flatten())\n    model.add(Dense(9, activation=\"softmax\"))\n    model.compile(optimizer=\"Adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n    model.fit(X, Y, epochs=20, batch_size=64, verbose=0)\n    \n    val_acc = accuracy_score([np.argmax(p) for p in val_Y], [np.argmax(p) for p in model.predict(val_X)])\n    pool_predictions = model.predict(pool)\n    return val_acc, pool_predictions","601f84d5":"class Dataset:\n    def __init__(self, X, Y):\n        self._X = X\n        self._Y = Y\n        self._labeled = np.array([False for _ in range(0, len(self._X))])\n    \n    @property\n    def pool(self):\n        return self._X\n    \n    @property\n    def X(self):\n        return self._X[self._labeled]\n    \n    @property\n    def Y(self):\n        return self._Y[self._labeled]\n    \n    def random_sampling(self, batch_size):\n        not_labeled = np.where(self._labeled == False)[0]\n        new_labels = []\n        while len(new_labels) < batch_size:\n            r = random.randrange(0, len(not_labeled))\n            if not_labeled[r] not in new_labels:\n                new_labels.append(not_labeled[r])\n        self._labeled[new_labels] = True\n    \n    def lc_sampling(self, batch_size, predictions):\n        lc = sorted([(1 - p[np.argmax(p)], i) for i, p in enumerate(predictions)], reverse=True)\n        self._label_batch(lc, batch_size)\n                \n    def margin_sampling(self, batch_size, predictions):\n        ms = sorted([(p[np.argsort(p)[-1]] - p[np.argsort(p)[-2]], i) for i, p in enumerate(predictions)])\n        self._label_batch(ms, batch_size)\n    \n    def entropy_sampling(self, batch_size, predictions):\n        es = sorted([(entropy(p), i) for i, p in enumerate(predictions)], reverse=True)\n        self._label_batch(es, batch_size)\n        \n    def _label_batch(self, sorted_candidates, batch_size):\n        i = 0\n        for _, j in sorted_candidates:\n            if not self._labeled[j]: #if not already labeled\n                self._labeled[j] = True\n                i += 1\n            if i >= batch_size:\n                break","20c82f4f":"def active_learning(query_strategy, seed_size, batch_size, num_steps):\n    \"\"\"\n    query_strategy - 'lc' for Least confidence sampling\n                   - 'ms' for Margin sampling\n                   - 'es' for Entropy sampling\n                   - 'rs' for Random sampling\n    \"\"\"\n    assert query_strategy in [\"lc\", \"ms\", \"es\", \"rs\"], \"Unknown query strategy\"\n    accuracies = []\n    d = Dataset(train_X, train_Y)\n    d.random_sampling(seed_size)\n    acc, predictions = train_model(d.X, d.Y, d.pool)\n    accuracies.append(acc)\n    for _ in tqdm(range(0, num_steps)):\n        if query_strategy == \"lc\":\n            d.lc_sampling(batch_size, predictions)\n        elif query_strategy == \"ms\":\n            d.margin_sampling(batch_size, predictions)\n        elif query_strategy == \"es\":\n            d.entropy_sampling(batch_size, predictions)\n        elif query_strategy == \"rs\":\n            d.random_sampling(batch_size)\n        acc, predictions = train_model(d.X, d.Y, d.pool)\n        accuracies.append(acc)\n    return accuracies","f778f6f1":"seed_size=100\nbatch_size=50\nnum_steps=98","dfa207ca":"random_accuracies = active_learning(\"rs\", seed_size, batch_size, num_steps)","d3ce926a":"lc_accuracies = active_learning(\"lc\", seed_size, batch_size, num_steps)","2df5fe2e":"ms_accuracies = active_learning(\"ms\", seed_size, batch_size, num_steps)","7d906e53":"es_accuracies = active_learning(\"es\", seed_size, batch_size, num_steps)","154e4c80":"plt.figure(figsize=(10, 8))\nplt.plot(np.arange(seed_size, seed_size + (num_steps + 1) * batch_size, batch_size), random_accuracies, color=\"b\", label=\"Random Sampling\")\nplt.plot(np.arange(seed_size, seed_size + (num_steps + 1) * batch_size, batch_size), lc_accuracies, color=\"g\", label=\"Least Confidence Sampling\")\nplt.plot(np.arange(seed_size, seed_size + (num_steps + 1) * batch_size, batch_size), ms_accuracies, color=\"r\", label=\"Margin Sampling\")\nplt.plot(np.arange(seed_size, seed_size + (num_steps + 1) * batch_size, batch_size), es_accuracies, color=\"y\", label=\"Entropy Sampling\")\nplt.legend(loc=\"lower right\")\nplt.title(\"Active Learning on DBPedia Classes Dataset\")\nplt.ylabel('Accuracy')\nplt.xlabel('Labeled data')\nplt.grid()","267660aa":"idx = np.where(np.arange(seed_size, seed_size + (num_steps + 1) * batch_size, batch_size) == 2000)[0][0]","6f03eb6d":"random_accuracies[idx]","f204e1b4":"lc_accuracies[idx]","c01c7f33":"def find_nearest(array, value):\n    array = np.asarray(array)\n    idx = (np.abs(array - value)).argmin()\n    return seed_size + idx * batch_size","596fcd51":"find_nearest(random_accuracies, 0.85) #random sampling","b8220d4c":"find_nearest(lc_accuracies, 0.85) #least confidence","be1e2a39":"### **Accuracy after labeling 2000 data points**","b5f215c5":"### **Number of labeled data points required to the reach accuracy of 85%**"}}