{"cell_type":{"b9fc1fac":"code","8b99fb9d":"code","2b449da9":"code","94d72b05":"code","f9cac737":"code","b6ecfc38":"code","3d7a8db9":"code","0897fb6a":"code","2d7679c0":"code","76824e6f":"code","6aa4524b":"code","3238d553":"code","fa3efd38":"code","6b322aeb":"code","9bbcf7bf":"code","33b4bde5":"code","9b4aaa00":"code","5e51cd6f":"code","34f05f25":"code","0948f30c":"code","010cb593":"code","188a709c":"code","ab015af0":"code","a051a934":"code","39b0788e":"code","b3705429":"code","5565dea0":"code","c071639a":"code","a0a5b05f":"code","c8ab0a6f":"code","02d9ac45":"code","896c2fda":"code","b695f550":"code","870c9b6c":"code","de46b113":"markdown","b20ab0df":"markdown","c1b79cf1":"markdown","07f7b48d":"markdown","c7b15ebc":"markdown","1144f0ba":"markdown","c2dcf4b0":"markdown","4a938f80":"markdown","1a23ac51":"markdown","3451724f":"markdown","dea37f78":"markdown","2fb5f65e":"markdown","9cbbe87b":"markdown","b32465af":"markdown","799f631e":"markdown"},"source":{"b9fc1fac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8b99fb9d":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import  DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import classification_report,confusion_matrix, roc_auc_score\n\nimport warnings\nwarnings.filterwarnings('ignore')","2b449da9":"train= pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv') ","94d72b05":"train.head()","f9cac737":"train.shape","b6ecfc38":"train.isnull().sum().sort_values(ascending=False)","3d7a8db9":"train.describe().transpose()","0897fb6a":"train['Outcome'].value_counts()","2d7679c0":"sns.countplot(train['Outcome']);","76824e6f":"plt.figure(figsize=(20,10))\nsns.countplot(train[train['Outcome']==1]['Age']);","6aa4524b":"plt.figure(figsize=(20,10))\nsns.histplot(train[train['Outcome']==1]['Age'], bins=3);","3238d553":"def plotGraph(col):\n    \n    for ele in col:\n        \n        print('Plots for : ',ele)\n        plt.figure(figsize=(30,10))\n        \n        # Distribution plot\n        plt.subplot(1,3,1)\n        sns.distplot(train[ele])\n        plt.title('Distribution Plot')\n        \n        # Histogram\n        plt.subplot(1,3,2)\n        sns.histplot(train[ele])\n        plt.title('Histogram plot')\n        \n        # Box plot\n        plt.subplot(1,3,3)\n        sns.boxplot(train[ele])\n        plt.title('Box Plot')\n        \n        plt.show()","fa3efd38":"#plotGraph(train.columns)","6b322aeb":"plt.figure(figsize=(9,9))\nsns.heatmap(train.corr(), annot=True, mask=np.triu(train.corr()))\nplt.ylim(9,0);","9bbcf7bf":"train.drop(['Pregnancies','SkinThickness'], axis=1, inplace=True)","33b4bde5":"y = train['Outcome']","9b4aaa00":"train.drop('Outcome', axis=1, inplace=True)","5e51cd6f":"x_train, x_test, y_train, y_test = train_test_split(train,y, test_size=0.2, random_state=15)","34f05f25":"scaler= StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)","0948f30c":"def print_performance(yt,clf):\n    y_pred=clf.predict(x_test)\n    print('ROC_AUC value : ',roc_auc_score(yt,y_pred),'\\n')\n    print('classification_report : ','\\n',classification_report(yt,y_pred))\n    print('Confusion_matrics : ','\\n',confusion_matrix(yt,y_pred))\n    \n    confu_matric(yt,y_pred)","010cb593":"# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\n\ndef confu_matric(y_test, y_pred):\n    \n    cm = confusion_matrix(y_test, y_pred)\n    p = sns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n    plt.title('Confusion matrix', y=1.1)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')","188a709c":"# Random forest\n\nclassifier = RandomForestClassifier(n_estimators=250,random_state=15)\nclassifier.fit(x_train, y_train)\nprint_performance(y_test,classifier)","ab015af0":"# decision tree \n\nclassifier = DecisionTreeClassifier()\nclassifier.fit(x_train, y_train)\nprint_performance(y_test,classifier)","a051a934":"#Logistic regression\n\nclassifier = LogisticRegression(random_state = 15)\nclassifier.fit(x_train, y_train)\nprint_performance(y_test,classifier)","39b0788e":"# K Neighbour\n\nclassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclassifier.fit(x_train, y_train)\nprint_performance(y_test,classifier)","b3705429":"# SVC rbf\n\nclassifier = SVC(kernel = 'rbf', random_state = 15)\nclassifier.fit(x_train, y_train)\nprint_performance(y_test,classifier)","5565dea0":"#Gaussian\n\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)\nprint_performance(y_test,classifier)","c071639a":"# SVC linear\n\nclassifier = SVC(kernel = 'linear', random_state = 15)\nclassifier.fit(x_train, y_train)\nprint_performance(y_test,classifier)","a0a5b05f":"# Gradient Boosting\n\nclassifier = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=13, random_state=15)\nclassifier.fit(x_train, y_train)\nprint_performance(y_test,classifier)","c8ab0a6f":"# XGBClassifier\n\nclassifier = XGBClassifier(n_estimators=200,random_state = 15)\nclassifier.fit(x_train, y_train)\nprint_performance(y_test,classifier)","02d9ac45":"import tensorflow as tf","896c2fda":"# Initialize ANN\nann = tf.keras.models.Sequential()\n\n#Adding input layer and first hidden layer\nann.add(tf.keras.layers.Dense(units=6, activation='relu'))\n\n# Adding 2nd hidden layer\nann.add(tf.keras.layers.Dense(units=6, activation='relu'))\n\n# Adding output layer\nann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n\n#Compile ANN\nann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n#Fit the model\nann.fit(x_train, y_train, batch_size = 16, epochs = 50)\n\n# make prediction on x_test\ny_pred = ann.predict(x_test)\n#y_pred = (y_pred > 0.5)\ny_pred = np.where(y_pred>0.5, 1,0)","b695f550":"# Predicting the Test set results\nscore, acc = ann.evaluate(x_test, y_test,\n                            batch_size=16)\nprint('Test score:', score)\nprint('Test accuracy:', acc)","870c9b6c":"# Making the Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\n\np = sns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","de46b113":"So, We can drop these two columns('Pregnancies','SkinThickness')","b20ab0df":"Lets analyze the diabetes patients distribution over age.","c1b79cf1":"Lets see some other plots for all the features","07f7b48d":"Let's try Neural Network","c7b15ebc":"Thank you.....!","1144f0ba":"Some of the features are skewed and have few outliers. \nBut i am gonna use RandomForest for training so we are good.","c2dcf4b0":"OK So there is no null value.\n\nLets see some Statistical info about the features.","4a938f80":"##### Ok So, Dataset is very small.\n\nLets check if there is any null value present","1a23ac51":"So, from above plots it seems patients are mostly of age between 20 to 40 yrs","3451724f":"Now lets prepare the model","dea37f78":"### Import all the important libraries","2fb5f65e":"### So, we can see there is really good positive correlation between Age and Pregnancies\n\n### Also there is good Negative correlation between Age and SkinThickness","9cbbe87b":"#### Read the dataset","b32465af":"#### Dataset look like kind of balanced dataset.","799f631e":"That's it for now."}}