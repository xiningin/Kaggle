{"cell_type":{"9ee79334":"code","17e3d8de":"code","f90d2be7":"code","f699e40f":"code","409c3406":"code","780a9652":"code","e84c2c57":"code","b42322fa":"code","337d8d44":"code","b55712ca":"code","a3f7b165":"code","5bd90be2":"markdown","586c4294":"markdown","f64f95c6":"markdown","91fc94d0":"markdown","27af3f3c":"markdown","3085fc6b":"markdown","fbe1d167":"markdown"},"source":{"9ee79334":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pylab as plt\ntrain = pd.read_csv('..\/input\/train.csv', parse_dates=[0])\ntest = pd.read_csv('..\/input\/test.csv', parse_dates=[0])\nsubmit = pd.read_csv('..\/input\/sample_submission.csv')\nplt.style.use('ggplot')","17e3d8de":"fake_store_names = {1: 'Albertsons', 2: 'Ahold', 3: 'Food_Lion', 4: 'Hannaford',\n                    5: 'Giant', 6: 'Stop_n_Shop', 7: 'Kroger', 8: 'SpartanNash',\n                    9: 'SuperValu', 10: 'Walmart'}\n\nfake_items = {1:'Apples', 2:'Bacon', 3:'Bagles', 4:'Beans', 5:'Beer', 6:'Bread',\n              7:'Carrots', 8:'Cheese', 9:'Chips', 10:'Coffee', 11:'Cream', \n              12:'Egg', 13:'Fish', 14:'Foil', 15:'Granola Bars', 16:'Grapes',\n              17:'Ham', 18:'Honey', 19:'Ice Cream', 20:'Ketchup', 21:'Kielbasa',\n              22:'Lemons', 23:'Lettuce', 24:'Margarine', 25:'Mayonnaise', 26:'Milk',\n              27:'Mushrooms', 28:'Mustard', 29:'Oranges', 30:'Paper Towles',\n              31:'Pasta', 32:'Peanut Butter', 33:'Pears', 34:'Pizza', 35:'Plastic Wrap',\n              36:'Potatoes', 37:'Pretzels', 38:'Ribs', 39:'Rice', 40:'Salami', \n              41:'Salsa', 42:'Salt', 43:'Sausage', 44:'Soda', 45:'Soup', 46:'Sugar',\n              47:'Tuna', 48:'Turkey', 49:'Waffles', 50:'Yoghurt'}\n\ntrain['store_name'] = train['store'].replace(fake_store_names)\ntrain['item_name'] = train['item'].replace(fake_items)","f90d2be7":"grouped = train.groupby(by=['item_name'])\nfor i, d in grouped:\n    myplot = d.set_index('date').groupby('store_name')['sales'] \\\n        .plot(figsize=(15,2), style='.', title=str(i), legend=False)\n    plt.show()","f699e40f":"def plot_year_over_year(item, store):\n    sample = train.loc[(train['store'] == store) & (train['item'] == item)].set_index('date')\n    pv = pd.pivot_table(sample, index=sample.index.month, columns=sample.index.year,\n                        values='sales', aggfunc='sum')\n    ax = pv.plot(figsize=(15,3), title=fake_store_names[store] + ' - ' + fake_items[item])\n    ax.set_xlabel(\"Month\")\nplot_year_over_year(1, 1)\nplot_year_over_year(1, 2)\nplot_year_over_year(20, 5)\nplot_year_over_year(20, 6)","409c3406":"def plot_year_over_year_dow(item, store):\n    sample = train.loc[(train['store'] == store) & (train['item'] == item)].set_index('date')\n    pv = pd.pivot_table(sample, index=sample.index.weekday, columns=sample.index.year,\n                        values='sales', aggfunc='sum')\n    ax = pv.plot(figsize=(15,3), title=fake_store_names[store] + ' - ' + fake_items[item])\n    ax.set_xlabel(\"Day of Week\")\nplot_year_over_year_dow(1, 1)\nplot_year_over_year_dow(1, 2)\nplot_year_over_year_dow(20, 5)\nplot_year_over_year_dow(20, 6)","780a9652":"# Data prep\ntrain['store_item'] = train['store_name'] + '-' + train['item_name']\ntrain['store_item_mean'] = train.groupby('store_item')['sales'].transform('mean')\ntrain['deviation_from_storeitem_mean'] = train['sales'] - train['store_item_mean']\ntrain['dev_rolling'] = train.groupby('store_item')['deviation_from_storeitem_mean'].rolling(30).mean().reset_index()['deviation_from_storeitem_mean']\ntrain_pivoted = train.pivot(index='store_item', columns='date', values='sales')\ntrain_pivoted.head()","e84c2c57":"deviation_pivot = train.pivot(index='store_item', columns='date', values='dev_rolling')\ndeviation_pivot = deviation_pivot.dropna(axis=1)\ndeviation_pivot.head()","b42322fa":"# Example from here:\n# https:\/\/stackoverflow.com\/questions\/34940808\/hierarchical-clustering-of-time-series-in-python-scipy-numpy-pandas\n    \nimport scipy.cluster.hierarchy as hac\nfrom scipy import stats\n# Here we use spearman correlation\ndef my_metric(x, y):\n    r = stats.pearsonr(x, y)[0]\n    return 1 - r # correlation to distance: range 0 to 2\n\nZ = hac.linkage(deviation_pivot, method='single', metric=my_metric)","337d8d44":"from scipy.cluster.hierarchy import fcluster\n\ndef print_clusters(deviation_pivot, Z, k, plot=False):\n    # k Number of clusters I'd like to extract\n    results = fcluster(Z, k, criterion='maxclust')\n\n    # check the results\n    s = pd.Series(results)\n    clusters = s.unique()\n\n    for c in clusters:\n        cluster_indeces = s[s==c].index\n        print(\"Cluster %d number of entries %d\" % (c, len(cluster_indeces)))\n        if plot:\n            deviation_pivot.T.iloc[:,cluster_indeces].plot()\n            plt.show()\n\nprint_clusters(deviation_pivot, Z, 5, plot=False)","b55712ca":"from sklearn.cluster import KMeans\nclust = KMeans()\ndeviation_pivot['cluster'] = clust.fit_predict(deviation_pivot)","a3f7b165":"deviation_pivot.T","5bd90be2":"# Plot Day of Week","586c4294":"# EDA with Fake Names","f64f95c6":"## Rename from number to something more fun\nNumbers are boring. Lets change the names of the items to make them more fun to refer to - and easier to remember!\n\n**WARNING** THESE ARE NOT THE REAL ITEM AND\/OR STORE NAMES (unless I got extremely lucky). These names are just for fun.\n\n- Store Names found here: https:\/\/en.wikipedia.org\/wiki\/List_of_supermarket_chains_in_the_United_States\n    1. Albertsons\n    2. Ahold\n    3. Food Lion\n    4. Hannaford\n    5. Giant\n    6. Stop & Shop\n    7. Kroger\n    8. SpartanNash\n    9. SuperValu\n    10. Walmart\n\n- Items taken from here: https:\/\/www.webmd.com\/food-recipes\/guide\/grocery-list#1\n","91fc94d0":"# Time Series Clustering","27af3f3c":"# Plot Year over Year","3085fc6b":"## Plot each item, scroll through and visually inspect for trends","fbe1d167":"# KMeans Clustering"}}