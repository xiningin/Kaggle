{"cell_type":{"434fdd7d":"code","c6d49c99":"code","e6c47698":"code","fe198887":"code","26199bcb":"code","224a1bd2":"code","ee03e945":"code","17142278":"code","3b9cd518":"code","0a50bc03":"code","2b9befa3":"code","a20e3885":"code","b2ad4a4b":"code","58efed3d":"code","1c34c65f":"code","461dad4b":"code","27046674":"code","178265ab":"code","f14e2d3a":"code","ac3047dd":"code","a4082bf8":"code","fb7bfca9":"code","14ac05cb":"code","3f05349d":"code","dd36aada":"code","0c042f98":"code","6149391c":"code","a0e30f27":"code","1eb7f0ec":"code","c601bd0f":"markdown","fb386e09":"markdown"},"source":{"434fdd7d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c6d49c99":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n%matplotlib inline\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV","e6c47698":"cancer = load_breast_cancer()\ncancer.keys()","fe198887":"print(cancer.DESCR[27:3130])","26199bcb":"\ndf_features = pd.DataFrame(cancer.data, columns = cancer.feature_names)\ncancer.target_names","224a1bd2":"df_target = pd.DataFrame(cancer.target, columns=['target'])\ndf_target['target'].value_counts()","ee03e945":"df = pd.concat([df_features, df_target], axis=1)\ndf['target'] = df['target'].apply(lambda x: \"Benign\"\nif x == 1 else \"Malignant\")\ndf.head(4)","17142278":"df.describe()","3b9cd518":"df.info()","0a50bc03":"sns.set_style('darkgrid')\ndf['target'].value_counts()","2b9befa3":"scaler = StandardScaler()\nscaler.fit(df_features)\nfeatures_scaled = scaler.transform(df_features)\nfeatures_scaled = pd.DataFrame(data=features_scaled,\ncolumns=df_features.columns)\ndf_scaled = pd.concat([features_scaled, df['target']], axis=1)\ndf_scaled_melt = pd.melt(df_scaled, id_vars='target',\nvar_name='features', value_name='value')\ndf_scaled_melt.head(3)","a20e3885":"def violin_plot(features, name):\n    \"\"\"\n    This function creates violin plots of features given in the argument.\n    \"\"\"\n    # Create query\n    query = ''\n    for x in features:\n        query += \"features == '\" + str(x) + \"' or \"\n    query = query[0:-4]\n    # Create data for visualization\n    data = df_scaled_melt.query(query)\n    # Plot figure\n    plt.figure(figsize=(12, 6))\n    sns.violinplot(x='features',\n    y='value',\n    hue='target',\n    data=data,\n    split=True,\n    inner=\"quart\")\n    plt.xticks(rotation=45)\n    plt.title(name)\n    plt.xlabel(\"Features\")\n    plt.ylabel(\"Standardize Value\")\ndef swarm_plot(features, name):\n    \"\"\"\n    This function creates swarm plots of features given in the argument.\n    \"\"\"\n    # Create query\n    query = ''\n    for x in features:\n        query += \"features == '\" + str(x) + \"' or \"\n    query = query[0:-4]\n    # Create data for visualization\n    data = df_scaled_melt.query(query)\n    # Plot figure\n    plt.figure(figsize=(12, 6))\n    sns.swarmplot(x='features', y='value', hue='target', data=data)\n    plt.xticks(rotation=45)\n    plt.title(name)\n    plt.xlabel(\"Features\")\n    plt.ylabel(\"Standardize Value\")\ndef box_plot(features, name):\n    \"\"\"\n    This function creates box plots of features given in the argument.\n    \"\"\"\n    # Create query\n    query = ''\n    for x in features:\n        query += \"features == '\" + str(x) + \"' or \"\n    query = query[0:-4]\n    # Create data for visualization\n    data = df_scaled_melt.query(query)\n    # Plot figure\n    plt.figure(figsize=(12, 6))\n    sns.boxplot(x='features', y='value', hue='target', data=data)\n    plt.xticks(rotation=45)\n    plt.title(name)\n    plt.xlabel(\"Features\")\n    plt.ylabel(\"Standardize Value\")\n","b2ad4a4b":"violin_plot(df.columns[0:10], \"Violin Plot of the First 10 Features\")","58efed3d":"swarm_plot(df.columns[10:20], \"Swarm Plot of the Next 10 Features\")","1c34c65f":"box_plot(df.columns[20:30], \"Box Plot of the Last 10 Features\")","461dad4b":"def correlation(var):\n    \"\"\"\n    1. Print correlation\n    2. Create jointplot\n    \"\"\"\n    # Print correlation\n    print(\"Correlation: \", df[[var[0], var[1]]].corr().iloc[1, 0])\n    # Create jointplot\n    plt.figure(figsize=(6, 6))\n    sns.jointplot(df[(var[0])], df[(var[1])], kind='reg')","27046674":"correlation(['mean perimeter', 'mean area'])","178265ab":"correlation(['mean concavity', 'mean concave points'])","f14e2d3a":"# Create correlation matrix\ncorr_mat = df.corr()\n# Create mask\nmask = np.zeros_like(corr_mat, dtype=np.bool)\nmask[np.triu_indices_from(mask, k=1)] = True\n# Plot heatmap\nplt.figure(figsize=(15, 10))\nsns.heatmap(corr_mat, annot=True, fmt='.1f',\ncmap='RdBu_r', vmin=-1, vmax=1,\nmask=mask)","ac3047dd":"from sklearn.feature_selection import SelectKBest, chi2\nfeature_selection = SelectKBest(chi2, k=5)\nfeature_selection.fit(df_features, df_target)\nselected_features = df_features.columns[feature_selection.get_support()]\nprint(\"The five selected features are: \", list(selected_features))","a4082bf8":"X = pd.DataFrame(feature_selection.transform(df_features),\ncolumns=selected_features)","fb7bfca9":"sns.pairplot(pd.concat([X, df['target']], axis=1), hue='target')","14ac05cb":"from sklearn.model_selection import train_test_split\ny = df_target['target']\nX_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size=0.33, random_state=42)","3f05349d":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=200)\nrfc.fit(X_train, y_train)\ny_pred = rfc.predict(X_test)","dd36aada":"from sklearn.metrics import confusion_matrix, classification_report\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\nprint(\"\\n\")\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))","0c042f98":"X_scaled = features_scaled\npca = PCA(n_components=2)\npca.fit(X_scaled)\nX_pca = pca.transform(X_scaled)\nplt.figure(figsize=(8, 8))\nsns.scatterplot(X_pca[:, 0], X_pca[:, 1], hue=df['target'])\nplt.title(\"PCA\")\nplt.xlabel(\"First Principal Component\")\nplt.xlabel(\"Second Principal Component\")\n","6149391c":"X = X_pca\ny = df_target['target']\nX_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size=0.33, random_state=42)\nparam_grid = {'C': [0.1, 1, 10, 100, 1000],\n'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n'kernel': ['rbf']}\ngrid = GridSearchCV(SVC(), param_grid, refit=True, cv=5)\ngrid.fit(X_train, y_train)","a0e30f27":"y_pred = grid.predict(X_test)","1eb7f0ec":"print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\nprint(\"\\n\")\nprint(\"Classification Report:\\n\",classification_report(y_test, y_pred))","c601bd0f":"**Model Evaluation**","fb386e09":"**Random Forest Classifier**"}}